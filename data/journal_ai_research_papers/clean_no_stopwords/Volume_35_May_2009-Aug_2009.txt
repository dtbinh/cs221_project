Journal Artificial Intelligence Research 35 (2009) 593621

Submitted 02/09; published 07/09

Bounds Arc Consistency Weighted CSPs
Matthias Zytnicki

Matthias.Zytnicki@versailles.inra.fr
INRA, Unite de Recherche en Genomique et Informatique
UR 1164, Versailles, France

Christine Gaspin
Simon de Givry
Thomas Schiex

Christine.Gaspin@toulouse.inra.fr
Simon.DeGivry@toulouse.inra.fr
Thomas.Schiex@toulouse.inra.fr

INRA, Unite de Biometrie et Intelligence Artificielle
UR 875, Toulouse, France

Abstract
Weighted Constraint Satisfaction Problem (WCSP) framework allows representing
solving problems involving hard constraints cost functions. applied various problems, including resource allocation, bioinformatics, scheduling, etc.
solve problems, solvers usually rely branch-and-bound algorithms equipped
local consistency filtering, mostly soft arc consistency. However, techniques
well suited solve problems large domains. Motivated resolution
RNA gene localization problem inside large genomic sequences, spirit bounds
consistency large domains crisp CSPs, introduce soft bounds arc consistency,
new weighted local consistency specifically designed WCSP large domains.
Compared soft arc consistency, BAC provides significantly improved time space
asymptotic complexity. paper, show semantics cost functions
exploited improve time complexity BAC. also compare
theory practice efficiency BAC WCSP bounds consistency enforced
crisp CSP using cost variables. two different real problems modeled WCSP,
including RNA gene localization problem, observe maintaining bounds arc consistency outperforms arc consistency also improves bounds consistency enforced
constraint model cost variables.

1. Introduction
Weighted Constraint Satisfaction Problem (WCSP) extension crisp Constraint Satisfaction Problem (CSP) allows direct representation hard constraints
cost functions. WCSP defines simple optimization (minimization) framework
wide range applications resource allocation, scheduling, bioinformatics (Sanchez,
de Givry, & Schiex, 2008; Zytnicki, Gaspin, & Schiex, 2008), electronic markets (Sandholm,
1999), etc. also captures fundamental AI statistical problems Maximum
Probability Explanation Bayesian nets Markov Random Fields (Chellappa & Jain,
1993).
crisp CSP, two main approaches solve WCSP inference search.
last approach usually embodied branch-and-bound algorithm. algorithm
estimates node search tree lower bound cost solutions
sub-tree.
c
2009
AI Access Foundation. rights reserved.

fiZytnicki, Gaspin, de Givry & Schiex

One successful approaches build lower bounds obtained
extending notion local consistency WCSP (Meseguer, Rossi, & Schiex, 2006).
includes soft AC (Schiex, 2000), AC* (Larrosa, 2002), FDAC* (Larrosa & Schiex,
2004), EDAC* (Heras, Larrosa, de Givry, & Zytnicki, 2005), OSAC (Cooper, de Givry,
& Schiex, 2007) VAC (Cooper, de Givry, Sanchez, Schiex, & Zytnicki, 2008) among
others. Unfortunately, worst case time complexity bounds associated enforcing
algorithms least cubic domain size use amount space
least linear domain size. makes consistencies useless problems
large domains.
motivation designing local consistency enforced efficiently
problems large domains follows interest RNA gene localization problem. Initially modeled crisp CSP, problem tackled using bounds consistency (Choi, Harvey, Lee, & Stuckey, 2006; Lhomme, 1993) dedicated propagators
using efficient pattern matching algorithms (Thebault, de Givry, Schiex, & Gaspin, 2006).
domain sizes related size genomic sequences considered reach
hundreds millions values. order enhance tool scoring capabilities
improved quality localization, shift crisp weighted CSP natural step
requires extension bounds consistency WCSP. Beyond direct motivation,
extension also useful domains large domains occur naturally temporal reasoning scheduling.
local consistencies define combine principles bounds consistency
principles soft local consistencies. definitions general restricted
binary cost functions. corresponding enforcing algorithms improve time
space complexity AC* factor also nice rare property,
WCSP local consistencies, confluent.
done AC-5 Van Hentenryck, Deville, Teng (1992) functional
monotonic constraints, show different forms cost functions (largely captured
notion semi-convex cost functions) processed efficiently. also
show powerful bounds arc consistencies strictly stronger
application bounds consistency reified representation WCSP proposed
Petit, Regin, Bessiere (2000).
conclude, experimentally compare efficiency algorithms maintain
different local consistencies inside branch-and-bound agile satellite scheduling problems (Verfaillie & Lematre, 2001) RNA gene localization problems (Zytnicki et al.,
2008) observe clear speedups compared different existing local consistencies.

2. Definitions Notations
section introduce main notions used throughout paper.
define (Weighted) Constraint Satisfaction Problems, well local consistency
property frequently used solving Weighted Constraint Satisfaction Problem: arc
consistency (AC*).
594

fiBounds Arc Consistency Weighted CSPs

2.1 Constraint Networks
Classic weighted constraint networks share finite domain variables one components. paper, domain variable xi denoted D(xi ). denote value
D(xi ), use index vi , vi ,. . . variable xi , assume domain xi
totally ordered denote inf(xi ) sup(xi ) minimum (resp. maximum)
values domain D(xi ). assignment tS set variables = {xi1 , . . . , xir }
function maps variables elements domains: tS = (xi1 vi1 , . . . , xir vir )
{i1 , . . . , ir }, tS (xi ) = vi D(xi ). given assignment tS xi S,
simply say value vi D(xi ) belongs tS mean tS (xi ) = vi . denote
, set possible assignments S.
Definition 2.1 constraint network (CN) tuple P = hX , D, Ci, X = {x1 , . . . , xn }
set variables = {D(x1 ), . . . , D(xn )} set finite domains
variable. C set constraints. constraint cS C defines set authorized
combinations values variables subset . called scope cS .
|S| called arity cS . simplicity, unary (arity 1) binary (arity 2) constraints
may denoted ci cij instead c{xi } c{xi ,xj } respectively. denote
maximum domain size, n, number variables network e, number
constraints. central problem constraint networks find solution, defined
assignment tX variables constraint cS C, restriction tX
authorized cS (all constraints satisfied). Constraint Satisfaction
Problem (CSP).
Definition 2.2 Two CNs variables equivalent set
solutions.
CN said empty one variables empty domain.
may happen following local consistency enforcement. CN large domains, use
bounds consistency usual approach. Historically, different variants bounds
consistency introduced, generating confusion. Using terminology introduced Choi et al. (2006), bounds consistency considered paper
bounds(D) consistency. consider large domains defining intervals,
actually equivalent bounds(Z) consistency. simplicity, rest paper
denote bounds consistency.
Definition 2.3 (Bounds consistency) variable xi bounds consistent iff every constraint cS C xi contains pair assignments (t, )
inf(xi ) sup(xi ) . case, called supports two bounds
xi domain.
CN bounds consistent iff variables bounds consistent.
enforce bounds consistency given CN, domain bound satisfy
properties deleted fixed point reached.
595

fiZytnicki, Gaspin, de Givry & Schiex

2.2 Weighted Constraint Networks
Weighted constraint networks obtained using cost functions (also referred soft
constraints) instead constraints.
Definition 2.4 weighted constraint network (WCN) tuple P = hX , D, W, ki,
X = {x1 , . . . , xn } set variables = {D(x1 ), . . . , D(xn )} set finite
domains variable. W set cost functions. cost function wS W associates
integer cost wS (tS ) [0, k] every assignment tS variables S. positive
number k defines maximum (intolerable) cost.
cost k, may finite infinite, cost associated forbidden assignments. cost used represent hard constraints. Unary binary cost functions
may denoted wi wij instead w{xi } w{xi ,xj } respectively. usually
WCNs, assume existence zero-arity cost function, w [0, k], constant cost
whose initial value usually equal 0. cost assignment tX variables
obtained combining costs cost functions wS W applied restriction
tX S. combination done using function defined b = min(k, + b).
Definition 2.5 solution WCN assignment tX variables whose cost
less k. optimal assignment X strictly lower cost.
central problem WCN find optimal solution.
Definition 2.6 Two WCNs variables equivalent give
cost assignments variables.
Initially introduced Schiex (2000), extension arc consistency WCSP
refined Larrosa (2002) leading definition AC*. decomposed
two sub-properties: node arc consistency itself.
Definition 2.7 (Larrosa, 2002) variable xi node consistent iff:
vi D(xi ), w wi (vi ) < k.
vi D(xi ) wi (vi ) = 0. value vi called unary support xi .
WCN node consistent iff every variable node consistent.
enforce NC WCN, values violate first property simply deleted.
Value deletion alone capable enforcing second property. shown Cooper
Schiex (2004), fundamental mechanism required ability move costs
different scopes. cost b subtracted greater cost function
defined b = (a b) 6= k k otherwise. Using , unary support
variable xi created subtracting smallest unary cost minvi D(xi ) wi (vi )
wi (vi ) adding (using ) w. operation shifts costs variables
w, creating unary support, called projection wi w.
cancel out, defining fair valuation structure (Cooper & Schiex, 2004), obtained WCN
equivalent original one. equivalence preserving transformation (Cooper
Schiex) precisely described ProjectUnary() function Algorithm 1.
able define arc AC* consistency WCN.
596

fiBounds Arc Consistency Weighted CSPs

Algorithm 1: Projections unary binary levels
1
2
3
4
5
6
7
8
9
10

Procedure ProjectUnary(xi )
min minvi D(xi ) {wi (vi )} ;
(min = 0) return;
foreach vi D(xi ) wi (vi ) wi (vi ) min ;
w w min ;

[ Find unary support xi ]

Procedure Project(xi , vi , xj )
[ Find support vi w.r.t. wij ]
min minvj D(xj ) {wij (vi , vj )} ;
(min = 0) return;
foreach vj D(xj ) wij (vi , vj ) wij (vi , vj ) min ;
wi (vi ) wi (vi ) min ;

Definition 2.8 variable xi arc consistent iff every cost function wS W
xi S, every value vi D(xi ), exists assignment vi
wS (t) = 0. assignment called support vi wS . WCN AC* iff
every variable arc node consistent.
enforce arc consistency, support given value vi xi cost function wS
created subtracting (using ) cost mintS ,vi wS (t) costs
assignments containing vi adding wi (vi ). cost movements, applied
values vi D(xi ), define projection wS wi . Again, transformation
preserves equivalence problems. precisely described (for simplicity,
case binary cost functions) Project() function Algorithm 1.

Example 2.9 Consider WCN Figure 1(a). contains two variables (x1 x2 ),
two possible values (a b, represented vertices). unary cost function
associated variable, cost value represented inside corresponding
vertex. binary cost function two variables represented weighted edges
connecting pairs values. absence edge two values represents zero cost.
Assume k equal 4 w equal 0.
Since cost w1 (x1 a) equal k, value deleted domain
x1 (by NC, first property). resulting WCN represented Figure 1(b). Then,
since x2 unary support (second line definition NC), project cost
1 w (cf. Figure 1(c)). instance NC. enforce AC*, project 1
binary cost function w12 value x1 since value support w12
(cf. Figure 1(d)). Finally, project 1 w1 w, seen Figure 1(e). Ultimately,
note value b x2 support. enforce AC*, project binary cost
1 value remove since unary cost 2 which, combined w
reaches k = 4.

597

fiZytnicki, Gaspin, de Givry & Schiex

w = 0, k = 4
x1
x2

w = 0, k = 4
x2
x1


1

4



1



1

0

2

b

2

b


1

0

2

2

b

b

0





1

b

b

0

1

0

x1

0





1

b

b

(e) find unary support using
ProjectUnary(x1 )

(d) find support (x1
b) using Project(x1 , b, x2 )
(AC*)

0



2

1

b

(c) find unary support using
ProjectUnary(x2 ) (NC*)

w = 2, k = 4
x2
x1

w = 1, k = 4
x1
x2

1


1

(b) prune forbidden values
(NC*)

(a) original instance

b



1

1

b

w = 1, k = 4
x2
x1

w = 2, k = 4
x2
0

0


b

(f) Arc consistency enforced

Figure 1: Enforcing Arc Consistency.

3. Bounds Arc Consistency (BAC)
crisp CSP, bounds consistency enforcing process deletes bounds
supported one constraint. weighted CSP, enforcement complex. similar
value deletion process exists based first node consistency property violation (whenever
w wi (vi ) reaches k), additional cost movements performed enforce node arc
consistency.
shown AC*, projections require ability represent arbitrary unary
cost function wi every variable xi . requires space O(d) general since projections
lead arbitrary changes original wi cost function (even efficient
internal representation). prevent this, therefore avoid move cost cost functions
arity greater one unary constraints. Instead projections, keep
value deletion mechanism applied bounds current domain takes
account cost functions involving variable considered. given variable xi
involved cost function wS , choice given value vi least induce cost
increase mintS ,vi tS wS (tS ). minimum costs, combined cost functions
involving xi , together w, reach intolerable cost k, value deleted.
bounds consistency, done two bounds domain. leads
following definition BAC (bounds arc consistency) WCSP:
Definition 3.1 WCN P = hX , D, W, ki, variable xi bounds arc consistent iff:


X
wS (tS ) < k
w
min
tS ,inf(xi )tS

wS W,xi

w

X

wS W,xi



min

tS ,sup(xi )tS


wS (tS ) < k

WCN bounds arc consistent every variable bounds arc consistent.
598

fiBounds Arc Consistency Weighted CSPs

One note definition proper generalization bounds consistency since
k = 1, actually equivalent definition bounds(D) consistency crisp
CSP (Choi et al., 2006) (also equivalent bounds(Z) consistency since domains defined
intervals).
algorithm enforcing BAC described Algorithm 2. enforcing BAC
uses value deletion, similar structure bounds consistency enforcement.
maintain queue Q variables whose domain modified (or untested).
better efficiency, use extra data-structures efficiently maintain combined cost associated domain bound inf(xi ), denoted winf (xi ). cost function wS involving
xi , contribution wS combined cost equal mintS ,inf(xi )tS wS (tS ).
contribution maintained data-structure inf (xi , wS ) updated whenever minimum cost may change value removals. Notice that, Algorithm 2, line 14
concise way denote hidden loops initialize winf , wsup , inf sup
data-structures zero.
Domain pruning achieved function PruneInf() also resets data-structures
associated variable line 35 data-structures recomputed
variable extracted queue. Indeed, inside loop line 20, contributions
inf (xi , wS ) cost winf (xi ) cost functions wS involving xj reset.
Function pop removes element queue returns it.
Proposition 3.2 (Time space complexity) WCN maximum arity r
constraints, enforcing BAC Algorithm 2 time O(er2 dr ) space O(n + er).
Proof: Regarding time, every variable pushed Q + 1 times:
beginning, one values removed. consequence,
foreach loop line 18 iterates O(erd) times, foreach loop line 20 iterates
O(er2 d) times. min computation line 22 takes time O(dr1 ) thus, overall
time spent line takes time O(er2 dr ). PruneInf() called O(er2 d) times.
condition line 32 true O(nd) times so, line 35 takes time O(ed)
(resetting inf (xi , ) line 35 hides loop cost functions involving xi ). total
time complexity thus O(er2 dr ).
Regarding space, used winf , wsup data-structures. space complexity
thus O(n + er).

Note exploiting information last supports AC2001 (Bessiere & Regin,
2001) reduce worst-case time complexity minimum cost cost
function must recomputed scratch time domain reduced
last support lost (Larrosa, 2002). However, using last supports helps practice
reduce mean computation time done implementation.
Compared AC*, enforced O(n2 d3 ) time O(ed) space binary
WCN, BAC enforced times faster, space complexity becomes independent
requirement problems large domains.
Another interesting difference AC* BAC confluent bounds
consistency is. Considering AC*, known may exist several different AC*
closures possibly different associated lower bounds w (Cooper & Schiex, 2004). Note
although OSAC (Cooper et al., 2007) able find optimal w (at much higher
599

fiZytnicki, Gaspin, de Givry & Schiex

Algorithm 2: Algorithm enforcing BAC.
11
12
14
15
16
18
20
22
23
24
25
26
27
28
29

30
32
33
35
36
37
38
39
40
41
42
43

Procedure BAC(X , D, W, k)
QX ;
winf () 0 ; wsup () 0 ; inf (, ) 0 ; sup (, ) 0 ;
(Q 6= )
xj pop(Q) ;
foreach wS W, xj
foreach xi
mintS ,inf(xi )tS wS (tS ) ;
winf (xi ) winf (xi ) inf (xi , wS ) ;
inf (xi , wS ) ;
PruneInf(xi ) Q Q {xi } ;
mintS ,sup(xi )tS wS (tS ) ;
wsup (xi ) wsup (xi ) sup (xi , wS ) ;
sup (xi , wS ) ;
PruneSup(xi ) Q Q {xi } ;
Function PruneInf(xi ) : boolean
(w winf (xi ) = k)
delete inf(xi ) ;
winf (xi ) 0 ; inf (xi , ) 0 ;
return true;
else return false;
Function PruneSup(xi ) : boolean
(w wsup (xi ) = k)
delete sup(xi ) ;
wsup (xi ) 0 ; sup (xi , ) 0 ;
return true;
else return false;

600

fiBounds Arc Consistency Weighted CSPs

computational cost), still confluent. following property shows BAC
confluent.
Proposition 3.3 (Confluence) Enforcing BAC given problem always leads
unique WCN.
Proof: prove proposition follows. first define set problems
contains problems reached original WCN BAC
enforcement. Notice that, step BAC enforcement, general case, several
operations performed specific order imposed. Therefore, set problems
reached step. show set problems lattice structure
ultimately show closure BAC lower bound lattice,
therefore unique, proves property. proof technique usual proving
convergence chaotic iteration collection suitable functions used
characterizing CSP local consistency Apt (1999).
enforcement BAC, original problem P = hX , D, W, ki iteratively
transformed set different problems equivalent P, obtained
deleting values violating BAC. problems obtained value removals,
belong set 1 (P ) defined by: {hX , , W, ki : D}.
define relation, denoted , set 1 (P ):
(P1 , P2 ) 21 (P), P1 P2 [1, n], D1 (xi ) D2 (xi )
easy see relation defines partial order. Furthermore, pair
elements greatest lower bound glb least upper bound lub 1 (P), defined by:
(P1 , P2 ) 21 (P),
glb(P1 , P2 ) = hX , {D1 (xi ) D2 (xi ) : [1, n]}, W, ki 1 (P)
lub(P1 , P2 ) = hX , {D1 (xi ) D2 (xi ) : [1, n]}, W, ki 1 (P)
h1 (P), thus complete lattice.
BAC filtering works removing values violating BAC properties, transforming
original problem succession equivalent problems. transformation
described application dedicated functions 1 (P) 1 (P). precisely,
two functions variable, one minimum bound inf(xi )
domain xi symmetrical one maximum bound. inf(xi ), associated
function keeps instance unchanged inf(xi ) satisfies condition Definition 3.1
otherwise returns WCN inf(xi ) alone deleted. collection
functions defines set functions 1 (P ) 1 (P ) denote FBAC .
Obviously, every function f FBAC order preserving:
(P1 , P2 ) 21 (P), P1 P2 f (P1 ) f (P2 )
application Tarski-Knaster theorem (Tarski, 1955), known every
function f FBAC (applied quiescence BAC enforcement) least one
fixpoint, set fixed points forms lattice . Moreover, intersection lattices fixed points functions f FBAC , denoted 1 (P), also
601

fiZytnicki, Gaspin, de Givry & Schiex

lattice. 1 (P) empty since problem hX , {, . . . , }, Wi fixpoint every
filtering function FBAC . 1 (P) exactly set fixed points FBAC .
show that, algorithm reaches fixpoint, reaches greatest element
1 (P). prove induction successive application elements FBAC
P yields problems greater element 1 (P) order . Let
us consider fixpoint P 1 (P). Initially, algorithm applies P,
greatest element 1 (P), thus P P. base case induction. Let
us consider problem P1 obtained execution algorithm. have,
induction, P P1 . Since order preserving, know that, function f
FBAC , f (P ) = P f (P1 ). therefore proves induction.
conclude, algorithm terminates, gives maximum element 1 (P).
Since proposition 3.2 showed algorithm actually terminates, conclude
confluent.

enforcing BAC may reduce domains, never increases lower bound w.
important limitation given increase w may generate value deletions
possibly, failure detection. Note even cost function becomes totally assigned,
cost corresponding assignment projected w BAC enforcement.
simply done maintaining form backward checking simple
WCSP branch-and-bound algorithm (Freuder & Wallace, 1992). go beyond simple
approach, consider combination BAC another WCSP local consistency which,
similarly AC*, requires cost movements enforced avoids modification
unary cost functions keep reasonable space complexity. achieved directly
moving costs w.

4. Enhancing BAC
many cases, BAC may weak compared AC* situations seems
possible infer decent w value. Consider example following cost function:

D(x1 ) D(x2 )
E
D(x1 ) = D(x2 ) = [1, 10]
w12 :
(v1 , v2 )
7 v1 + v2
AC* increase w 2, projecting cost 2 w12 unary constraint w1
every value, projecting costs w1 w enforcing NC. However,
w = w1 = w2 = 0 k strictly greater 11, BAC remains idle here.
however simply improve BAC directly taking account minimum possible cost
cost function w12 possible assignments given current domains.
Definition 4.1 cost function wS -inverse consistent (-IC) iff:
tS , wS (tS ) = 0
tuple tS called support wS . WCN -IC iff every cost function (except
w) -IC.
Enforcing -IC always done follows: every cost function wS non
empty scope, minimum cost assignment wS given current variable domains
602

fiBounds Arc Consistency Weighted CSPs

computed. cost assignment subtracted tuple costs wS
added w. creates least one support wS makes cost function
-IC. given cost function wS , done Project() function Algorithm 3.
order strengthen BAC, natural idea combine -IC. call BAC
resulting combination BAC -IC. enforce BAC, previous algorithm
modified first adding call Project() function (see line 53 Algorithm 3).
Moreover, maintain BAC whenever w modified projection, every variable tested
possible pruning line 66 put back Q case domain change. Note
subtraction applied constraint tuples line 75 done constant time
without modifying constraint using additional wS data-structure, similar
data-structure introduced Cooper Schiex (2004). data-structure keeps
track cost projected wS w. feature makes possible
leave original costs unchanged enforcement local consistency.
example, tS , wS (t) refers wS (t) wS , wS (t) denotes original
cost. Note wS , later used confluence proof, precisely contains
amount cost moved wS w. whole algorithm described
Algorithm 3. highlighted black parts different Algorithm 2
whereas unchanged parts gray.
Proposition 4.2 (Time space complexity) WCN maximum arity r
constraints, enforcing BAC Algorithm 3 enforced O(n2 r2 dr+1 ) time
using O(n + er) memory space.
Proof: Every variable pushed O(d) times Q, thus foreach line 51
(resp. line 55) loops O(erd) (resp. O(er2 d)) times. projection line 53 takes
O(dr ) time. operation line 57 carried O(dr1 ) time. overall time
spent inside PruneInf() function bounded O(ed). Thus overall time
spent loop line 51 (resp. line 55) bounded O(er2 dr+1 ) (resp. O(er2 dr )).
flag line 66 true w increases, cannot true k times
(assuming integer costs). flag true, spend O(n) time check bounds
variables. Thus, time complexity bounded O(min{k, nd} n).
sum up, overall time complexity O(er2 dr+1 + min{k, nd} n), bounded
O(n2 r2 dr+1 ).
space complexity given , winf , wsup wS data-structures sums
O(n + re) WCN arity bounded r.

time complexity algorithm enforcing BAC multiplied compared
BAC without -IC. usual trade-off strength local property
time spent enforce it. However, space complexity still independent d.
Moreover, like BAC, BAC confluent.
Proposition 4.3 (Confluence) Enforcing BAC given problem always leads
unique WCN.
Proof: proof similar proof Proposition 3.3. However,
possible cost movements induced projections, BAC transforms original problem P
complex ways, allowing either pruning domains (BAC) moving costs cost
603

fiZytnicki, Gaspin, de Givry & Schiex

Algorithm 3: Algorithm enforcing BAC
44
45
46
47
48
49
51
53
55
57
58
59
60
61
62
63
64
66
67
68
69

70
71
72
73
75
76
77

Procedure BAC(X , D, W, k)
QX ;
winf () 0 ; wsup () 0 ; inf (, ) 0 ; sup (, ) 0 ;
(Q 6= )
xj pop(Q) ;
flag false ;
foreach wS W, xj
Project(wS ) flag true ;
foreach xi
mintS ,inf(xi )tS wS (tS ) ;
winf (xi ) winf (xi ) inf (xi , wS ) ;
inf (xi , wS ) ;
PruneInf(xi ) Q Q {xi } ;
mintS ,sup(xi )tS wS (tS ) ;
wsup (xi ) wsup (xi ) sup (xi , wS ) ;
sup (xi , wS ) ;
PruneSup(xi ) Q Q {xi } ;
(flag)
foreach xi X
PruneInf(xi ) Q Q {xi } ;
PruneSup(xi ) Q Q {xi } ;
Function Project(wS ) : boolean
mintS wS (tS ) ;
( > 0)
w w ;
wS () wS () ;
return true;
else return false;

604

fiBounds Arc Consistency Weighted CSPs

functions w. set problems considered needs therefore take
account. Instead defined domains, WCN reached BAC
also characterized amount cost moved cost function wS
w. quantity already denoted wS Section 4, page 603. therefore
consider set 2 (P) defined by:


(hX , , W, ki, {w : w W}) : [1, n], (xi ) D(xi ), w W, w [0, k]
define relation 2 (P):

w
P1 P2 ((w W, w
1 2 ) (xi X , D1 (xi ) D2 (xi )))

relation reflexive, transitive antisymmetric. first two properties
easily verified. Suppose (P1 , P2 ) 22 (P) (P1 P2 ) (P2 P1 ).
thus (w W, w = w )(xi X , D(xi ) = (xi )). ensures domains,
well amounts cost projected cost function, same. Thus,
problems antisymmetric.
Besides, h2 (P), complete lattice, since:
(P1 , P2 ) 22 (P),
w
glb(P1 , P2 ) = (hX , {D1 (xi ) D2 (xi ) : [1, n]}, W, ki, {max{w
1 , 2 } : w W})
w
lub(P1 , P2 ) = (hX , {D1 (xi ) D2 (xi ) : [1, n]}, W, ki, {min{w
1 , 2 } : w W})

2 (P).
Every enforcement BAC follows application functions set functions FBAC may remove maximum minimum domain bound (same definition
BAC) may project cost cost functions w. given cost function
w W, function keeps instance unchanged minimum w 0
possible tuples. Otherwise, > 0, problem returned derived P projecting
amount cost w w. functions easily shown order preserving
.
proof Proposition 3.3, define lattice 2 (P), intersection sets fixed points functions f FBAC . 2 (P) empty, since
(hX , {, . . . , }, W, ki, {k, . . . , k}) it. proof proposition 3.3, since Algorithm 3 terminates, conclude algorithm confluent, results
lub(2 (P)).


5. Exploiting Cost Function Semantics BAC
crisp AC, several classes binary constraints make possible enforce AC significantly
faster (in O(ed) instead O(ed2 ), shown Van Hentenryck et al., 1992). Similarly,
possible exploit semantics cost functions improve time complexity
BAC enforcement. proof Proposition 4.2 shows, dominating factors
complexity comes complexity computing minimum cost functions
projection lines 53 57 Algorithm 3. Therefore, cost function property
605

fiZytnicki, Gaspin, de Givry & Schiex

makes computations less costly may lead improvement overall time
complexity.
Proposition 5.1 binary WCN, cost function wij W subintervals Ei D(xi ), Ej D(xj ), minimum wij Ei Ej found time
O(d), time complexity enforcing BAC O(n2 d2 ).
Proof: follows directly proof Proposition 4.2. case, complexity
projection line 53 O(d) instead O(d2 ). Thus overall time spent
loop line 51 bounded O(ed2 ) overall complexity O(ed2 + n2 d) O(n2 d2 ).

Proposition 5.2 binary WCN, cost function wij W subintervals Ei D(xi ), Ej D(xj ), minimum wij Ei Ej found
constant time, time complexity enforcing BAC O(n2 d).
Proof: follows proof Proposition 4.2. case, complexity
projection line 53 O(1) instead O(d2 ). Moreover, operation line 57
carried time O(1) instead O(d). Thus, overall time spent loop
line 51 bounded O(ed) overall complexity O(ed + n2 d) = O(n2 d).

two properties quite straightforward one may wonder non
trivial usage. actually directly exploited generalize results presented
Van Hentenryck et al. (1992) functional, anti-functional monotonic constraints.
following sections, show functional, anti-functional semi-convex cost functions (which include monotonic cost functions) indeed benefit O(d) speedup
factor application Proposition 5.1. monotonic cost functions generally
convex cost function, stronger speedup factor O(d2 ) obtained Proposition 5.2.
5.1 Functional Cost Functions
notion functional constraint extended cost functions follows:
Definition 5.3 cost function wij functional w.r.t. xi iff:
(vi , vj ) D(xi ) D(xj ), wij (vi , vj ) {0, } [1, k]
vi D(xi ), one value vj D(xj ) wij (vi , vj ) = 0.
exists, value called functional support vi .
assume rest paper functional
support computed constant
(
0 xi = xj
= =
time. example, cost function wij
functional. case,
1 otherwise
functional support vi itself. Note k = 1, functional cost functions represent
functional constraints.
Proposition 5.4 minimum functional cost function wij w.r.t. xi always
found O(d).
606

fiBounds Arc Consistency Weighted CSPs

Proof: every value vi xi , one check functional support vi belongs
domain xj . requires O(d) checks. never case, minimum
cost function known . Otherwise, 0. result follows.

5.2 Anti-Functional Semi-Convex Cost Functions
Definition 5.5 cost function wij anti-functional w.r.t. variable xi iff:
(vi , vj ) D(xi ) D(xj ), wij (vi , vj ) {0, } [1, k]
vi D(xi ), one value vj D(xj ) wij (vi , vj ) = .
exists, value called anti-support vi .
(
0 xi 6= xj
6=
cost function wij =
example anti-functional cost function.
1 otherwise
case, anti-support vi itself. Note k = 1, anti-functional cost functions
represent anti-functional constraints.
Anti-functional cost functions actually specific case semi-convex cost functions,
class cost functions appear example temporal constraint networks
preferences (Khatib, Morris, Morris, & Rossi, 2001).
Definition 5.6 Assume domain D(xj ) contained set Dj totally ordered
order <j .
function wij semi-convex w.r.t. xi iff [0, k], vi Di , set {vj Dj :
wij (vi , vj ) }, called -support vi , defines interval Dj according <j .
Semi-convexity relies definition intervals defined totally ordered discrete
set denoted Dj , ordered <j . Even may identical, important avoid
confusion order j D(xj ), used define interval domains bounds
arc consistency, order <j Dj used define intervals semi-convexity.
order guarantee constant time access minimum maximum elements D(xj )
according <j (called <j -bounds domain), assume <j =j <j =j 1 .
case, <j -bounds domain bounds identical.
One simply check anti-functional cost functions indeed semi-convex:
case, -support value either whole domain ( = 0), reduced one
point (0 < ) empty set (otherwise). Another example cost function
wij = x2i x2j semi-convex w.r.t. xi .
Proposition 5.7 minimum cost function wij semi-convex w.r.t. one
variables always found O(d).
Proof: first show that, wij semi-convex w.r.t. one variables (let
say xi ), value vi xi , cost function wij must minimum one
<j -bounds Dj .
1. restriction could removed using example doubly-linked list data-structure values
D(xj ), keeping domain sorted according <j allowing constant time access deletion
would cost linear space cannot afford context BAC.

607

fiZytnicki, Gaspin, de Givry & Schiex

Assume xi set vi . Let b lowest cost reached either two <j -bounds
domain. Since wij semi-convex, {vj Dj : wij (vi , vj ) b } interval,
thus every cost wij (vi , vj ) less b every value Dj . Therefore, least
one two <j -bounds minimum cost.
order find global minimum wij , restrict <j -bounds
domain xj every value xi . Therefore, 2d costs need checked.
Proposition 5.1, conclude
Corollary 5.8 binary WCN, cost functions functional, anti-functional
semi-convex, time complexity enforcing BAC O(n2 d2 ) only.
5.3 Monotonic Convex Cost Functions
Definition 5.9 Assume domain D(xi ) (resp. D(xj )) contained set Di (resp.
Dj ) totally ordered order <i (resp. <j ).
cost function wij monotonic iff:
(vi , vi , vj , vj ) Di2 Dj2 , vi vi vj j vj wij (vi , vj ) wij (vi , vj )
(

0 xi xj
example monotonic cost function.
1 otherwise
Monotonic cost functions actually instances larger class functions called convex
functions.
cost function


wij

=

Definition 5.10 function wij convex iff semi-convex w.r.t. variables.
example, wij = xi + xj convex.
Proposition 5.11 minimum convex cost function always found constant
time.
Proof: Since cost function semi-convex w.r.t. variable, know
proof Proposition 5.7 must reach minimum cost one <j -bounds
domain xj similarly xi . therefore four costs check order
compute minimum cost.

Proposition 5.2, conclude
Corollary 5.12 binary WCN, cost functions convex, time complexity
enforcing BAC O(n2 d) only.
One interesting example convex cost function wij = max{xi xj + cst, 0}.
type cost function, efficiently filtered BAC, may occur temporal
reasoning problems also used RNA gene localization problem specifying
preferred distances elements gene.
608

fiBounds Arc Consistency Weighted CSPs

6. Comparison Crisp Bounds Consistency
Petit et al. (2000) proposed transform WCNs crisp constraint networks
extra cost variables. transformation, every cost function reified constraint,
applies original cost function scope augmented one extra variable representing assignment cost. reification costs domain variables transforms WCN
crisp CN variables augmented arities. proposed Petit et al.,
achieved using meta-constraints, i.e. logical operators applied constraints. Given
relation WCNs crisp CNs relation BAC bounds
consistency, natural wonder BAC enforcing relates enforcing bounds
consistency reified version WCN.
section show BAC precise sense stronger enforcing
bounds consistency reified form. natural consequence fact
domain filtering BAC based combined cost several cost functions instead
taking constraint separately bounds consistency. first define reification process precisely. show BAC stronger reified bounds consistency
one example conclude proving never weaker.
following example introduces cost reification process.
Example 6.1 Consider WCN Figure 2(a). contains two variables x1 x2 , one
binary cost function w12 , two unary cost functions w1 w2 . sake clarity,
every variable constraint reified hard model, described Figure 2(b),
indexed letter R.
First all, model every cost function hard constraint, express assigning
b x1 yields cost 1. create new variable x1 C
R , cost variable w1 , stores
cost assignment x1 . Then, replace unary cost function w1 binary
constraint c1R involves x1 x1 C
R , value v1 assigned x1 ,
x1 C

take

value
w
(v
).

unary cost function w2 .
1
1
R
idea binary cost function w12 : create new variable x12 C
R ,
replace w12 ternary constraint c12R , makes sure assignment x1
x2 v1 v2 respectively, x12 C
R takes value w12 (v1 , v2 ). Finally, global cost
C
constraint cR states sum cost variables less k added:
C
C
x1 C
R + x2 R + x12 R < k. completes description reified cost hard constraint
network.
define formally reification process WCN.
Definition 6.2 Consider WCN P = hX , D, W, ki. Let reify(P) = hXR , DR , WR
crisp CN that:
set XR contains one variable xi R every variable xi X , augmented
extra cost variable xS C
R per cost function wS W {w}.
domains DR are:
DR (xiR ) = D(xi ) xiR variables, domain bounds lbiR ubiR ,
C
C
[lbS C
R , ubS R ] = [0, k 1] xS R variables.

609

fiZytnicki, Gaspin, de Givry & Schiex

x2 C
R

x1 C
R
x2R

x1R

0

0




b

b

1

1

2

2

k=3
x1

x2



0

1



b

1

0

b

x12 C
R
0

1

2

cC
R

(a) small cost function
network

(b) reified constraint network

Figure 2: small cost function network reified counterpart.
set WR constraints contains:
cS R = {(t, wS (t)) : , w wS (t) < k}, scope {xS C
R }, every cost
function wS W,
P
C
cC
wS W xS R < k), extra constraint makes sure
R defined (w
sum cost variables strictly less k.
simple check problem reify(P) solution iff P solution
sum cost variables solution cost corresponding solution (defined
values xiR variables) original WCN.
Definition 6.3 Let P problem, two local consistency properties. Let (P)
problem obtained filtering P . said weaker iff (P)
emptiness implies (P) emptiness.
said stronger iff weaker , exists problem
P (P) empty (P) empty.
definition practically significant since emptiness filtered problem
event generates backtracking tree search algorithms used solving CSP
WCSP.
Example 6.4 Consider WCN defined three variables (x1 , x2 x3 ) two binary
cost functions (w12 w13 ). D(x1 ) = {a, b, c, d}, D(x2 ) = D(x3 ) = {a, b, c} (we assume
b c d). costs binary cost functions described Figure 3.
Assume k = 2 w = 0.
One check associated reified problem already bounds consistent
obviously empty. example, support minimum bound domain
x1 R w.r.t. c12 R (a, a, 1), support maximum bound (d, a, 1). Supports
maximum minimum bounds domain x12 C
R w.r.t. c12R (b, a, 0) (a, a, 1)
respectively. Similarly, one check variable bounds also supported
constraints involve them.
610

fiBounds Arc Consistency Weighted CSPs


1
(x2 ) b 1
c 1

(x1 )
b c
0 2
0 2
0 2


1
(x3 ) b 1
c 1


1
1
1

(x1 )
b c
2 0
2 0
2 0


1
1
1

Figure 3: Two cost matrices.
However, original problem BAC since example, value a, minimum
bound domain x1 , satisfy BAC property:
w

X

wS W,x1



min

tS ,atS


wS (tS ) < k

means value deleted BAC filtering. symmetry, applies
maximum bound x1 ultimately, problem inconsistency proved
BAC. shows bounds consistency reified problem cannot stronger
BAC original problem.
show BAC actually stronger bounds consistency applied
reified WCN. BAC consistency implies non-emptiness (since requires
existence assignments cost 0 every cost function) start BAC
consistent WCN P (therefore empty) prove filtering reified problem reify(P)
bounds consistency lead empty problem.
Lemma 6.5 Let P BAC consistent binary WCN. filtering reify(P) bounds
consistency produce empty problem.
Proof: prove bounds consistency reduce maximum bounds
domains cost variables xS C
R non empty set leave domains
unchanged.
precisely, final domain xS C
R become [0, max{wS (t) : , w wS (t) <
k}]. Note interval empty network BAC consistent
means every cost function assignment cost 0 (by -IC) w < k (or else
bounds domains could supports problem would BAC).
prove bounds consistency reduce problem this,
simply prove problem defined domain reductions actually bounds
consistent.
bounds consistency required properties apply bounds domains
variables reify(P). Let us consider every type variable reified reduced problem:
reified variables xiR . Without loss generality, assume minimum bound lbiR
xiR bounds consistent (the symmetrical reasoning applies maximum
bound). means would support respect given reified constraint
611

fiZytnicki, Gaspin, de Givry & Schiex

cS R , xi S. However, BAC,
w

min

tS ,lbiR

wS (t) < k

, lbiR t,

wS (t) max{wS (t) : , w wS (t) < k}

means lbi R supported w.r.t. cS R .
cost variables. minimum bound cost variables always bounds consistent
w.r.t. global constraint cC
R constraint less inequality.
Moreover, since minimum bounds cost variables set 0, also
consistent w.r.t. reified constraints, definition -inverse consistency.
Consider maximum bound ubS C
R cost variable reduced reified problem.
Remember defined max{wS (t) : , w wS (t) < k}, w ubS C
R < k.
minimum bounds cost variables reified problem, 0,
C
C
form support ubS C
R w.r.t. global constraint cR . ubS R cannot removed
bounds consistency.

prove final assertion:
Proposition 6.6 BAC stronger bounds consistency.
Proof: Lemma 6.5 shows BAC weaker bounds consistency. Then,
example 6.4 instance BAC, therefore BAC actually stronger
bounds consistency reification.

filtering related BAC could achieved reified approach extra shaving
process variable assigned one domain bounds bound deleted
inconsistency found enforcing bounds consistency (Lhomme, 1993).

7. Related Works
Definition 3.1 BAC closely related notion arc consistency counts introduced Freuder Wallace (1992) Max-CSP processing. Max-CSP seen
simplified form WCN cost functions generate costs 0 1 (when
associated constraint violated). definition BAC seen extension
AC counts allowing dealing arbitrary cost functions, including usage w k,
applied domain bounds bounds consistency. addition -IC makes
BAC powerful.
Dealing large domains Max-CSP also considered Range-Based
Algorithm, designed Max-CSP Petit, Regin, Bessiere (2002). algorithm uses reversible directed arc consistency (DAC) counts exploits fact
Max-CSP, several successive values domain may DAC counts.
algorithm intimately relies fact problem Max-CSP problem, defined
set constraints actively uses bounds consistency dedicated propagators
constraints Max-CSP. case number different values reachable
DAC counters variable bounded degree variable, much
612

fiBounds Arc Consistency Weighted CSPs

smaller domain size. Handling intervals values DAC cost one
value allows space time savings. arbitrary binary cost functions, translation
constraints could generate d2 constraints single cost function makes
scheme totally impractical.
Several alternative definition bounds consistency exist crisp CSPs (Choi et al.,
2006). extension WCSP based bounds(D) bounds(Z) consistencies (which
equivalent intervals). numerical domains, another possible weaker definition
bounds consistency bounds(R) consistency, obtained relaxation real
numbers. shown Choi et al. bounds(R) consistency checked
polynomial time constraints whereas bounds(D) bounds(Z) NP-hard (eg.
linear equality). use relaxed version WCSP context together
intentional description cost functions would side effect extending cost
domain integer real numbers. extensional algorithmical description
integer cost functions general frequent problems, possibility
considered. Since cost comparison fundamental mechanism used pruning
WCSP, shift real numbers costs would require safe floating number implementation
local consistency enforcing algorithms branch bound algorithm.

8. Experimental Results
experimented bounds arc consistency two benchmarks translated weighted CSPs.
first benchmark AI planning scheduling. mission management
benchmark agile satellites (Verfaillie & Lematre, 2001; de Givry & Jeannin, 2006).
maximum domain size temporal variables 201. reasonable size fact
binary cost functions allows us compare BAC strong local
consistencies EDAC*. Additionally, benchmark also modeled using
reified version WCN, thus allowing experimental counterpart theoretical
comparison Section 6.
second benchmark comes bioinformatics models problem localization non-coding RNA molecules genomes (Thebault et al., 2006; Zytnicki et al.,
2008). aim mostly confirm bounds arc consistency useful practical
real complex problem huge domains, reach several millions.
8.1 Mission Management Benchmark Agile Satellites
solved simplified version described de Givry Jeannin (2006) problem
selecting scheduling earth observations agile satellites. complete description
problem given Verfaillie Lematre (2001). satellite pool candidate
photographs take. must select schedule subset pass
certain strip territory. satellite take one photograph time (disjunctive
scheduling). photograph taken time window depends
location photographed. Minimal repositioning times required two consecutive
photographs. physical constraints (time windows repositioning times) must
met, sum revenues selected photographs must maximized.
equivalent minimizing rejected revenues non selected photographs.
613

fiZytnicki, Gaspin, de Givry & Schiex

Let N number candidate photographs. define N decision variables representing acquisition starting times candidate photographs. domain
variable defined time window corresponding photograph plus extra domain
value represents fact photograph selected. proposed de Givry
Jeannin (2006), create binary hard constraint every pair photographs (resulting complete constraint graph) enforces minimal repositioning times
photographs selected (represented disjunctive constraint). photograph,
unary cost function associates rejected revenue corresponding extra value.
order better filtering, moved costs unary cost functions inside
binary hard constraints preprocessing step. allows bounds arc consistency
filtering exploit revenue information repositioning times jointly, possibly
increasing w starting times photographs. achieve this, variable
xi , unary cost function wi successively combined (using ) binary hard
defined
constraint wij involves xi . yields N 1 new binary cost functions wij

wij (t) = wij (t) wi (t[xi ]), hard (+) soft weights. binary
replace unary cost function w N 1 original binary hard
cost functions wij

constraints wij . Notice transformation side effect multiplying soft
weights N 1. preserve equivalence original problem since
finite weights multiplied constant (N 1).
search procedure exact depth-first branch-and-bound dedicated scheduling
problems, using schedule postpone strategy described de Givry Jeannin (2006)
avoids enumeration possible starting time values. initial upper bound
provided (k = +).
generated 100 random instances different numbers candidate photographs
(N varying 10 30)2 . compared BAC (denoted BAC0 experimental
results) EDAC* (Heras et al., 2005) (denoted EDAC*). Note FDAC* VAC
(applied preprocessing search, addition EDAC*) also tested
instances, improve EDAC* (FDAC* slightly faster EDAC*
developed search nodes VAC significantly slower EDAC*, without
improving w preprocessing). OSAC practical benchmark (for N = 20,
solve linear problem 50, 000 variables 4 million constraints).
algorithms using search procedure. implemented
toulbar2 C++ solver3 . Finding minimum cost previously-described binary cost
functions (which convex consider extra domain values rejected photographs
separately), done constant time BAC. done time O(d2 ) EDAC*
(d = 201).
also report results obtained maintaining bounds consistency reified
problem using meta-constraints described de Givry Jeannin (2006), using
claire/Eclair C++ constraint programming solver (de Givry, Jeannin, Josset, Mattioli,
Museux, & Saveant, 2002) developed THALES (denoted B-consistency).
results presented Figure 4, using log-scale. results obtained
3 GHz Intel Xeon 4 GB RAM. Figure 4 shows mean CPU time seconds
mean number backtracks performed search procedure find optimum
2. instances available http://www.inra.fr/mia/ftp/T/bep/.
3. See http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro.

614

fiBounds Arc Consistency Weighted CSPs

Satellite benchmark
10000

EDAC*
B consistency
BAC0

Cpu time seconds

1000
100
10
1
0.1
0.01
0.001
1e-04
10

15
20
25
Number candidate photographs

30

Satellite benchmark
1e+08

B consistency
BAC0
EDAC*

Number backtracks

1e+07
1e+06
100000
10000
1000
100
10
10

15

20

25

30

Number candidate photographs

Figure 4: Comparing various local consistencies satellite benchmark. Cpu-time (top)
number backtracks (bottom) given.

615

fiZytnicki, Gaspin, de Givry & Schiex

prove optimality problem size increases. legends, algorithms sorted
increasing efficiency.
analysis experimental results shows BAC 35 times faster
EDAC* 25% backtracks EDAC* (for N = 30, backtrack results reported EDAC* solve instance within time limit 6 hours).
shows bounds arc consistency prune almost many search nodes stronger
local consistency much less time temporal reasoning problems semantic cost functions exploited, explained Section 5. second fastest
approach bounds consistency reified representation least 2.3 worse
BAC terms speed number backtracks N 25. practical confirmation comparison Section 6. reified approach used bounds
consistency introduces Boolean decision variables representing photograph selection
uses criteria defined linear function variables. Contrarily BAC, bounds
consistency definition unable reason simultaneously combination several
constraints prune starting times.
8.2 Non-coding RNA Gene Localization
non-coding RNA (ncRNA) gene functional molecule composed smaller molecules,
called nucleotides, linked together covalent bonds. four types nucleotides, commonly identified single letter: A, U, G C. Thus, RNA
represented word built four letters. sequence defines called
primary structure RNA molecule.
RNA molecules ability fold back developing interactions
nucleotides, forming pairs. frequently interacting pairs are: G interacts
C, U interacts A. sequence interactions forms structure
called helix. Helices fundamental structural element ncRNA genes
basis complex structures. set interactions often displayed graph
vertices represent nucleotides edges represent either covalent bonds linking successive nucleotides (represented plain lines Figure 5) interacting nucleotide pairs
(represented dotted lines). representation usually called molecules secondary
structure. See graph helix Figure 5(a).
set ncRNAs common biological function called family.
signature gene family set conserved elements either sequence
secondary structure. expressed collection properties must satisfied
set regions occurring sequence. Given signature family, problem
interested involves searching new members gene family existing genomes,
members fact set regions appearing genome satisfy
signature properties. Genomic sequences long texts composed nucleotides.
thousand nucleotides long simplest organisms several hundred
million nucleotides complex ones. problem searching occurrence
gene signature genomic sequence NP-complete complex combinations helix
structures (Vialette, 2004).
order find ncRNAs, build weighted constraint network scans
genome, detects regions genome signature elements present
616

fiBounds Arc Consistency Weighted CSPs

U
C

G C G U C

G

U C G C G

helix

C
G U

G C UA U U
xi
xj

loop

(cost: 1)
(b)
pattern(xi , xj , ACGUA)
cost function.

(a) helix loop.

G

xi

U C

U G

G

xl

cost

xj

G C

k

xk
(cost: 2)

(c) helix(xi , xj , xk , xl , 6) cost
function.



0

xj xi
d1 d2

d3

d4

(d)

cost
profile
spacer(xi , xj , d1 , d2 , d3 , d4 )
cost
tion.


func-

Figure 5: Examples signature elements cost functions.
correctly positioned. variables positions signature elements
sequence. size domains size genomic sequence. Cost functions
enforce presence signature elements positions taken variables
involved. Examples cost functions given Figure 5.
pattern(xi , xj , p) function states fixed word p, given parameter,
found positions indicated variables xi xj . cost given
function edit distance word found xi :xj word p
(see cost function pattern word ACGUA Figure 5(b)).
helix(xi , xj , xk , xl , m) function states nucleotides positions xi
xj able bind nucleotides xk xl . Parameter
specifies minimum helix length. cost given number mismatches
nucleotides left unmatched (see helix function 5 interacting nucleotide pairs
Figure 5(c)).
Finally, function, spacer(xi , xj , d1 , d2 , d3 , d4 ) specifies favorite range distances
positions xi xj using trapezoidal cost function shown Figure 5(d).
See work Zytnicki et al. (2008) complete description cost functions.
sheer domain size, given complex pattern matching oriented
cost functions specific property could speedup filtering, BAC alone
used filtering cost functions (Zytnicki et al., 2008). exception
617

fiZytnicki, Gaspin, de Givry & Schiex

piecewise linear spacer cost function: minimum computed constant time
BAC enforcement. resulting C++ solver called DARN!4 .
Size
# solutions
AC*
Time
# backtracks
BAC
Time (sec.)
# backtracks

10k
32

50k
33

100k
33

500k
33

1M
41

4.9M
274

1hour 25min.
93

44 hours
101

-

-

-

-

0.016
93

0.036
101

0.064
102

0.25
137

0.50
223

2.58
1159

Table 1: Searching solutions tRNA motif Escherichia coli genome.
typical benchmark ncRNA localization problem transfer RNA (tRNA)
localization. tRNA signature (Gautheret, Major, & Cedergren, 1990) modelled
22 variables, 3 nucleotide words, 4 helices, 7 spacers. DARN! searched
solutions cost strictly lower maximum cost k = 3. illustrate
absolute necessity using bounds arc consistency problem, compared bounds
arc consistency enforcement AC* (Larrosa, 2002) sub-sequences genome
Escherichia coli, 4.9 million nucleotides long. identical space
complexity defined implemented non-binary cost
functions (helix quaternarycost function), DAC, FDAC EDAC tested
(see work Sanchez et al., 2008, however extension FDAC ternary cost
functions).
results displayed Table 1. different beginning sub-sequences complete sequence, report size sub-sequence signature searched
(10k sequence 10,000 nucleotides), well number solutions found.
also show number backtracks time spent 3 GHz Intel Xeon
2 GB. - means instance could solved due memory reasons, despite
memory optimizations. BAC solved complete sequence less 3 seconds. BAC
approximately 300, 000 (resp. 4, 400, 000) times faster AC* 10k (resp. 50k)
sub-sequence. results genomes ncRNA signatures found
work Zytnicki et al. (2008).
reason superiority BAC AC* twofold. First, AC* needs store
unary costs every variable projects costs binary cost functions unary
cost functions. Thus, space complexity AC* least O(nd). large domains
(in experiments, greater 100,000 values), computer cannot allocate sufficient
memory program aborted. kind projection, BAC needs
store costs bounds domains, leading space complexity O(n).
Second, BAC care interior values focuses bounds
domains only. hand, AC* projects binary costs interior values,
4. DARN!,

several
genomic
http://carlit.toulouse.inra.fr/Darn/.

sequences

618



family

signatures



available



fiBounds Arc Consistency Weighted CSPs

takes lot time, remove values detect inconsistencies earlier.
However, Table 1 shows number backtracks performed AC* BAC
same. explained follows. Due nature cost functions used
problems, supports bounds domains variables usually
bounds variables. Thus, removing values inside domains,
AC* does, help removing bounds variables. consequence, bounds
founds BAC found AC*. explains enforcing AC*
generally lead new domain wipe compared BAC, finding support
inside bounds domains useless.
Notice spacer cost functions dramatically reduce size domains.
single variable assigned, domain sizes dramatically reduced,
instance becomes quickly tractable. Moreover, helix constraint extra knowledge
maximum distance djk variables xj xk (see Fig. 5(c)) bounds
time complexity finding minimum cost w.r.t. djk length sequence.

9. Conclusions Future Work
presented new local consistencies weighted CSPs dedicated large domains well algorithms enforce properties. first local consistency, BAC,
time complexity easily reduced semantics cost function
appropriate. possible enhancement property, -IC, also presented.
experiments showed maintaining bounds arc consistency much better AC*
problems large domains, ncRNA localization scheduling Earth observation satellites. due fact AC* cannot handle problems large
domains, especially high memory complexity, also BAC behaves
particularly well specific classes cost functions.
Similarly bounds consistency, implemented almost state-of-the-art
CSP solvers, new local property implemented open source toulbar2
WCSP solver.5
BAC, BAC -inverse consistency allowed us transfer bounds consistency CSP
weighted CSP, including improved propagation specific classes binary cost functions.
implementation RNA gene finding also able filter non-binary constraints.
would therefore quite natural try define efficient algorithms enforcing BAC,
BAC -inverse consistency specific cost functions arbitrary arity soft
global constraints derived All-Diff, GCC regular (Regin, 1994; Van Hoeve, Pesant,
& Rousseau, 2006). line research recently explored Lee Leung
(2009).
Finally, another interesting extension work would better exploit connection BAC bounds consistency exploiting idea Virtual Arc Consistency
introduced Cooper et al. (2008). connection established Virtual AC crisp
CNs WCNs much finer grained reification approach considered Petit
et al. (2000) could provide strong practical theoretical results.
5. Available http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro.

619

fiZytnicki, Gaspin, de Givry & Schiex

References
Apt, K. (1999). essence constraint propagation. Theoretical computer science, 221 (12), 179210.
Bessiere, C., & Regin, J.-C. (2001). Refining basic constraint propagation algorithm.
Proc. IJCAI01, pp. 309315.
Chellappa, R., & Jain, A. (1993). Markov Random Fields: Theory Applications. Academics Press.
Choi, C. W., Harvey, W., Lee, J. H. M., & Stuckey, P. J. (2006). Finite domain bounds
consistency revisited. Proc. Australian Conference Artificial Intelligence, pp.
4958.
Cooper, M., & Schiex, T. (2004). Arc consistency soft constraints. Artificial Intelligence,
154, 199227.
Cooper, M. C., de Givry, S., Sanchez, M., Schiex, T., & Zytnicki, M. (2008). Virtual arc
consistency weighted CSP.. Proc. AAAI2008.
Cooper, M. C., de Givry, S., & Schiex, T. (2007). Optimal soft arc consistency. Proc.
IJCAI07, pp. 6873.
de Givry, S., & Jeannin, L. (2006). unified framework partial hybrid search
methods constraint programming. Computer & Operations Research, 33 (10), 2805
2833.
de Givry, S., Jeannin, L., Josset, F., Mattioli, J., Museux, N., & Saveant, P. (2002).
THALES constraint programming framework hard soft real-time applications.
PLANET Newsletter, Issue 5 ISSN 1610-0212, pages 5-7.
Freuder, E., & Wallace, R. (1992). Partial constraint satisfaction. Artificial Intelligence,
58, 2170.
Gautheret, D., Major, F., & Cedergren, R. (1990). Pattern searching/alignment RNA
primary secondary structures: effective descriptor tRNA. Comp. Appl.
Biosc., 6, 325331.
Heras, F., Larrosa, J., de Givry, S., & Zytnicki, M. (2005). Existential arc consistency:
Getting closer full arc consistency weighted CSPs. Proc. IJCAI05, pp.
8489.
Khatib, L., Morris, P., Morris, R., & Rossi, F. (2001). Temporal constraint reasoning
preferences. Proc. IJCAI01, pp. 322327.
Larrosa, J. (2002). Node arc consistency weighted CSP. Proc. AAAI02, pp.
4853.
Larrosa, J., & Schiex, T. (2004). Solving weighted CSP maintaining arc-consistency.
Artificial Intelligence, 159 (1-2), 126.
Lee, J., & Leung, K. (2009). Towards Efficient Consistency Enforcement Global Constraints Weighted Constraint Satisfaction. Proc. IJCAI09.
Lhomme, O. (1993). Consistency techniques numeric CSPs. Proc. IJCAI93, pp.
232238.
620

fiBounds Arc Consistency Weighted CSPs

Meseguer, P., Rossi, F., & Schiex, T. (2006). Soft constraints. Rossi, F., van Beek, P.,
& Walsh, T. (Eds.), Handbook constraint programming, Foundations Artificial
Intelligence, chap. 9, pp. 281328. Elsevier.
Petit, T., Regin, J.-C., & Bessiere, C. (2000). Meta-constraints violations constrained problems. Proc. ICTAI00, pp. 358365.
Petit, T., Regin, J. C., & Bessiere, C. (2002). Range-based algorithm Max-CSP.
Proc. CP02, pp. 280294.
Regin, J.-C. (1994). filtering algorithm constraints difference CSPs. Proc.
AAAI94, pp. 362367.
Sanchez, M., de Givry, S., & Schiex, T. (2008). Mendelian error detection complex
pedigrees using weighted constraint satisfaction techniques. Constraints, 13 (1-2), 130
154.
Sandholm, T. (1999). Algorithm Optimal Winner Determination Combinatorial
Auctions. Proc. IJCAI99, pp. 542547.
Schiex, T. (2000). Arc consistency soft constraints. Proc. CP00, pp. 411424.
Tarski, A. (1955). lattice-theoretical fixpoint theorem applications. Pacific Journal
Mathematics, 5 (2), 285309.
Thebault, P., de Givry, S., Schiex, T., & Gaspin, C. (2006). Searching RNA motifs
intermolecular contacts constraint networks. Bioinformatics, 22 (17), 207480.
Van Hentenryck, P., Deville, Y., & Teng, C.-M. (1992). generic arc-consistency algorithm
specializations. Artificial Intelligence, 57 (23), 291321.
Van Hoeve, W., Pesant, G., & Rousseau, L. (2006). global warming: Flow-based soft
global constraints. Journal Heuristics, 12 (4), 347373.
Verfaillie, G., & Lematre, M. (2001). Selecting scheduling observations agile satellites: lessons constraint reasoning community point view. Proc.
CP01, pp. 670684.
Vialette, S. (2004). computational complexity 2-interval pattern matching problems. Theoretical Computer Science, 312 (2-3), 223249.
Zytnicki, M., Gaspin, C., & Schiex, T. (2008). DARN! soft constraint solver RNA
motif localization. Constraints, 13 (1-2), 91109.

621

fiJournal Artificial Intelligence Research 35 (2009) 717-773

Submitted 12/08; published 08/09

Complexity Circumscription Description Logic
Piero A. Bonatti

bonatti@na.infn.it

Section Computer Science, Department Physics
University Naples Federico II, Italy

Carsten Lutz

clu@informatik.uni-bremen.de

Department Mathematics Computer Science
University Bremen, Germany

Frank Wolter

wolter@liverpool.ac.uk

Department Computer Science
University Liverpool, UK

Abstract
fragments first-order logic, Description logics (DLs) provide nonmonotonic
features defeasible inheritance default rules. Since many applications would
benefit availability features, several families nonmonotonic DLs
developed mostly based default logic autoepistemic logic.
paper, consider circumscription interesting alternative approach nonmonotonic
DLs that, particular, supports defeasible inheritance natural way. study DLs
extended circumscription different language restrictions different
constraints sets minimized, fixed, varying predicates, pinpoint exact
computational complexity reasoning DLs ranging ALC ALCIO ALCQO.
minimized fixed predicates include concept names role names,
reasoning complete NExpNP . becomes complete NPNExp number
minimized fixed predicates bounded constant. roles minimized
fixed, complexity ranges NExpNP undecidability.

1. Introduction
Early knowledge representation (KR) formalisms semantic networks frames
included wealth features order provide much expressive power possible
(Quillian, 1968; Minsky, 1975). particular, formalisms usually admitted structured representation classes objects similar modern description logics (DLs),
also mechanisms defeasible inheritance, default rules, features nowadays studied area nonmonotonic logics (NMLs). KR theory developed
further, all-embracing approaches largely given favour specialized
ones due unfavourable computational properties problems semantics.
process caused DLs NMLs develop two independent subfields. Consequently,
modern description logics SHIQ lack expressive power represent defeasible
inheritance nonmonotonic features (Horrocks, Sattler, & Tobies, 2000).
Despite (or due to) development, continuous interest
(re)integration nonmonotonic features description logics. recent years, advent several new applications DLs increased interest even further. briefly
discuss two them. First, DLs become popular tool formalization biomedc
2009
AI Access Foundation. rights reserved.

fiBonatti, Lutz, & Wolter

ical ontologies GALEN (Rector & Horrocks, 1997) SNOMED (Cote, Rothwell,
Palotay, Beckett, & Brochu, 1993). argued example Rector (2004) Stevens
et al. (2005), important ontologies represent exceptions form
humans, heart usually located left-hand side body; humans
situs inversus, heart located right-hand side body. Modelling
situations requires defeasible inheritance, i.e., properties transfer instances class
default, explicitly overridden special cases (McCarthy, 1986; Horty, 1994;
Brewka, 1994; Baader & Hollunder, 1995b). second application use DLs security policy languages (Uszok, Bradshaw, Johnson, Jeffers, Tate, Dalton, & Aitken, 2004;
Kagal, Finin, & Joshi, 2003; Tonti, Bradshaw, Jeffers, Montanari, Suri, & Uszok, 2003).
formalizing access control policies, one must deal situation given request
neither explicitly allowed explicitly denied. Then, default decision taken
open closed policies, authorizations respectively granted denied
default (Bonatti & Samarati, 2003). Moreover, policies often formulated incrementally, i.e., start general authorizations large classes subjects, objects, actions,
progressively refine introducing exceptions specific subclasses.
approach clearly incarnation defeasible inheritance.
applications illustrate integrating nonmonotonic features DLs
worthwhile, actual engineering computationally well-behaved nonmonotonic DL
provides sufficient expressive power turns non-trivial task. particular,
combinations DLs nonmonotonic logics typically involve subtle interactions
two component logics easily leads undecidability. appears
one optimal way circumnavigate difficulties, thus many different combinations
DLs nonmonotonic logics proposed literature, individual
strengths limitations (we provide survey Section 7). However, striking
gap: almost existing approaches based default logic autoepistemic logic,
circumscription received little attention connection DLs, computational properties DLs circumscription almost completely unknown.
surprising since circumscription known one weakest forms
nonmonotonic reasoningsee work Janhunen (1999) one recent surveys, paper Bonatti Eiter (1996) expressiveness analysis terms
queries. Therefore, natural idea use circumscription defining computationally
well-behaved, yet expressive DL nonmonotonic features.
paper, study circumscription (McCarthy, 1980) alternative approach
defining nonmonotonic DLs. particular, define family DLs circumscription enable natural modelling defeasible inheritance. general approach
generalize standard DL knowledge bases circumscribed knowledge bases (cKBs) which,
additionally TBox representing terminological knowledge ABox representing knowledge individuals, equipped circumscription pattern.
pattern lists predicates (i.e., concept role names) minimized sense that,
admitted models cKB, extension listed predicates minimal
w.r.t. set inclusion. Following McCarthy (1986), minimized predicates used
abnormality predicates identify instances typical class. Circumscription patterns require predicates fixed minimization, allow
vary freely (McCarthy, 1986). main feature DLs family
718

fiThe Complexity Circumscription DLs

come built-in mechanism defeasible inheritance: default, properties
class (humans first example above) transfer subclass (humans situs
inversus), exceptions specified based priority mechanism. well-known
defeasible inheritance priority cannot modularly encoded pure default
autoepistemic logic (Horty, 1994), workarounds explicit listing exceptions
lead serious maintainability problems. Circumscription lends naturally priorities,
based circumscription patterns express preferences minimized predicates terms partial ordering. argued Baader Hollunder (1995b),
approach well-suited ensure smooth interplay defeasible inheritance DL
subsumption, thus prefer traditional prioritized circumscription.
achieve decidability, nonmonotonic DLs usually adopt suitable restrictions
expressive power DL component, non-monotonic features, interaction. case default logic autoepistemic logic, typical restriction concerns
different treatment individuals explicitly denoted constant,
not. goes back reasoning first-order default logic (Reiter, 1980)
autoepistemic logic (Moore, 1985), also involve tricky technical issues related
denotation individuals. make reasoning decidable DLs based default logic, default rules applied individuals denoted constants occur explicitly
knowledge base (Baader & Hollunder, 1995a), unnamed individuals.
consequence, named unnamed individuals treated uniformly. approaches
based autoepistemic logic (Donini, Lenzerini, Nardi, Nutt, & Schaerf, 1998; Donini,
Nardi, & Rosati, 1997, 2002), alternative solution restrict domain fixed, denumerable set constants. approach overcomes different treatment named
unnamed individuals since individuals named. flipside ad-hoc encodings
required domain finite unique name assumption enforced, i.e.,
different constants allowed denote individual. respect, DLs
circumscription pose difficulty all, named individuals treated exactly
way unnamed ones without assumptions domain.
time, able base nonmonotonic DLs rather expressive DL components
ALCIO ALCQO without losing decidability. However, cannot without
restrictions either: allow fix minimize concept names circumscription
require role names vary.
main contribution paper detailed analysis computational properties
reasoning cKBs. show that, expressive DLs ALCIO ALCQO, instance
checking, satisfiability subsumption decidable concept-circumscribed KBs
concept names (and role names) minimized fixed. precisely,
prove reasoning problems NExpNP -complete, lower bound applies
already concept-circumscribed KBs ALC empty preference relation without
fixed concept names (1) empty TBox (2) empty ABox acyclic TBox. addition,
show constant bound imposed number minimized fixed concept
names, complexity drops NPNExp .
situation completely different role names minimized fixed. First,
complexity reasoning cKBs formulated ALC single fixed role name, empty
TBox, empty preference relation, minimized role names turns outside
analytic hierarchy, thus highly undecidable. result shown reduction
719

fiBonatti, Lutz, & Wolter

Name

Syntax
r

inverse role
nominal
negation
conjunction
disjunction
at-least restriction
at-most restriction

{a}
C
C uD
C tD
(> n r C)
(6 n r C)

Semantics
(rI )` = {(d, e) | (e, d) rI }
{aI }
\ C
C DI
C DI
{d | #{e C | (d, e) rI } n}
{d | #{e C | (d, e) rI } n}

Figure 1: Syntax semantics ALCQIO.

satisfiability monadic second-order logic (MSO) binary predicates arbitrary
(i.e., necessarily tree-shaped) structures. reduction apply cKBs
role names minimized, fixed. Surprisingly, find case
reasoning empty TBoxes becomes decidable (and NExpNP -complete) DLs
ALC ALCQO, ALCI extensions undecidable.
logics, however, adding acyclic TBoxes leads undecidability. reader find
table summarising complexity results Section 7.
interesting note results somewhat unusual perspective
NMLs. First, arity predicates impact decidability: fixing concept names
(unary predicates) impair decidability, whereas fixing single role name (binary
predicate) leads strong undecidability result. Second, number predicates
minimized fixed (bounded vs. unbounded) affects computational complexity
reasoning. Although (as note passing) similar effect observed propositional
logic circumscription, has, best knowledge, never explicitly
noted.
paper organized follows. next section introduce syntax, semantics,
reasoning problems circumscribed KBs, provide examples. Section 3
provides basic results polynomial simulation fixed concepts means
minimized concepts, polynomial reduction reasoning general TBoxes reasoning acyclic TBoxes, polynomial reduction simultaneous satisfiability
multiple cKBs standard satisfiability. Then, Section 4 proves decidability complexity results concept-circumscribed knowledge bases. Fixed minimized roles
considered Sections 5 6, respectively. Section 7 discusses related work, Section 8
concludes paper summarizing main results pointing interesting
directions research. improve readability, many proof details deferred
appendix. paper extended version article Bonatti, Lutz, Wolter
(2006).

2. Description Logics Circumscription
DLs, concepts inductively defined help set constructors, starting
set NC concept names, set NR role names, (possibly) set NI individual
720

fiThe Complexity Circumscription DLs

names (all countably infinite). use term predicates refer elements NC NR .
concepts expressive DL ALCQIO formed using constructors shown
Figure 1.
There, inverse role constructor role constructor, whereas remaining six
constructors concept constructors. Figure 1 throughout paper, use #S
denote cardinality set S, b denote individual names, r denote
roles (i.e., role names inverses thereof), A, B denote concept names, C,
denote (possibly complex) concepts. usual, use > abbreviation arbitrary
(but fixed) propositional tautology, >, usual Boolean abbreviations,
r.C (existential restriction) (> 1 r C), r.C (universal restriction) (6 0 r C).
paper, concerned ALCQIO itself, several
fragments.1 basic fragment allows negation, conjunction, disjunction,
universal existential restrictions, called ALC. availability additional
constructors indicated concatenation corresponding letter: Q stands number restrictions, stands inverse roles, nominals. explains name
ALCQIO, also allows us refer fragments ALCIO, ALCQO, ALCQI.
semantics ALCQIO-concepts defined terms interpretation =

( , ). domain non-empty set individuals interpretation function maps concept name NC subset AI , role name r NR
binary relation rI , individual name NI individual aI .
extension inverse roles arbitrary concepts inductively defined shown
third column Figure 1. interpretation called model concept C C 6= .
model C, also say C satisfied I.
(general) TBox finite set concept implications (CIs) C v C
.
concepts. usual, use C = abbreviation C v v C. ABox
finite set concept assertions C(a) role assertions r(a, b), a, b individual
names, r role name, C concept. interpretation satisfies (i) CI C v
C DI , (ii) assertion C(a) aI C , (iii) assertion r(a, b) (aI , bI ) rI .
Then, model TBox satisfies implications , model ABox
satisfies assertions A.
important class TBoxes acyclic TBoxes: call TBox acyclic set
.
concept definitions = C, concept name following two conditions
hold:
concept name occurs left hand side definition ;
.
relation , defined setting B iff = C B occurs C,
well-founded.
1. reason consider ALCQIO paper finite model
property; i.e., satisfiable concepts satisfiable finite models. proofs
complexity upper bounds assume finite model property and, therefore, work ALCQIO. Investigating circumscription description logics without finite model property remains interesting
open problem.

721

fiBonatti, Lutz, & Wolter

2.1 Circumscription, Varying Predicates, Partial Priority Ordering
Circumscription logical approach suitable modelling normally typically
holds, thus admits modeling defeasible inheritance (McCarthy, 1986; Lifschitz,
1993). idea define, standard first-order language, domain knowledge
so-called abnormality predicates identify instances class violate normal
typical properties class. capture intuition abnormality exceptional,
inference based set models resulting theory classical logic,
rather restricted models extension abnormality predicates
minimal respect set inclusion. Intuitively, means reasoning based
models normal possible. Since models classical models
given knowledge base, classical first-order inferences valid circumscription (but
additional inferences may become possible).
Since description logics fragments first-order logic, circumscription readily
applied. Using ALC syntax, assert mammals normally inhabitate land,
whales live land:
Mammal v habitat.Land AbMammal
Whale v Mammal u habitat.Land
upper inclusion states mammal inhabitating land abnormal mammal,
thus satisfying abnormality predicate AbMammal . applying circumscription
TBox, thus consider models extension AbMammal minimal.
However, one way defining preferred models nonminimized predicate treated two different ways minimization: may
either fix extension let vary freely.
Intuitively, fixed predicates retain classical semantics varying predicates may
affected minimization. concrete example, consider TBox
assume non-minimized predicates fixed. derive following
subsumptions:
Whale v AbMammal
()
.
AbMammal = Mammal u habitat.Land.
Here, Whale v AbMammal AbMammal w Mammal u habitat.Land classical consequences
TBox. minimization AbMammal adds inclusion AbMammal v Mammal u
habitat.Land.
analyze fixed predicates, suppose explicitly introduce concrete
mammal whale adding ABox assertion
Mammal u Whale(flipper)
might expect derive habitat.Land(flipper), actually
case. see this, observe classical model knowledge base falsifies
habitat.Land(flipper); extension fixed predicates habitat Land
affected minimization, habitat.Land(flipper) must still false minimization.
argument applied negation habitat.Land(flipper),
thus also derivable. seen sentence uses fixed
722

fiThe Complexity Circumscription DLs

predicates, consequence circumscribed knowledge base if, if,
classical consequence knowledge base.
assume let role habitat concept name Land vary freely, fix
Mammal Whale. view concept inclusion Mammal original TBox,
setup may interpreted expressing unlikely mammal
live land: willing modify extension habitat Land order avoid
abnormality. obtain additional consequence, namely:
.
Whale = AbMammal .
()
see indeed consequence note that, minimization, (i) make
Land non-empty (ii) mammal whale, ensure
AbMammal linking via habitat generated instance Land.2 Intuitively,
equality () seen reflecting unlikeliness abnormal: mammal
abnormal reason, reason captured knowledge
base whale.
Let us return assertion Mammal u Whale(flipper). applying classical
reasoning () (), derive Whale w Mammal u habitat.Land (i.e., whales
mammals live land). Thus derive expected conclusion habitat.Land(flipper). summary, turning habitat Land varying
predicates, obtained natural modelling habitat attribute
mammals forced default value.
Driving example further, might consider whales abnormal degree
believe exist unless evidence do. should,
additionally, let Whale vary freely. result () () still derived,
additionally obtain consequence
.
.
Whale = AbMammal = .
use ABox add evidence whales exist, e.g. assertion
Whale(mobydick). expected, result change
.
.
Whale = AbMammal = {mobydick}.
Evidence existence another, anonymous whale could generated adding
ABox assertion Male(mobydick) TBox statement
Whale v mother.(Whale u Male)
mother Male varying freely. knowledge base classically entails exist
two whales, satisfying Male Male, respectively. former denoted mobydick,
latter denoted ABox individual (which corresponds first-order
constant). minimization, Whale contains exactly two individuals.
general, appropriate combination fixed varying predicates depends
application. Therefore, adhere standard circumscription give users freedom
choose predicates minimized, fixed, varying.
2. Indeed, reason let Land vary: ensure made non-empty
minimization.

723

fiBonatti, Lutz, & Wolter

another example, consider sentences: humans, heart usually located
left-hand side body; humans situs inversus, heart located
right-hand side body. axiomatized follows:
Human v heart.has position.{Left} AbHuman
Situs Inversus v heart.has position.{Right}
heart.has position.{Left} u heart.has position.{Right} v .
predicate AbHuman represents abnormal humans minimized. humans
situs inversus restricted individuals explicitly declared
property, analogy previous example roles specifying heart
position class exceptional individuals Situs Inversus allowed vary
Human fixed retain classical semantics. result absence
axioms, AbHuman Situs Inversus empty minimized models.
additional axiom friend.Situs Inversus(John) turns AbHuman Situs Inversus
singleton set containing anonymous individual (though models, may
John himself). example nonclassical consequence, consider:
Human u Situs Inversus v heart.has position.{Left} ,
is, humans default heart position exception
explicitly declared situs inversus.
extensively argued (McCarthy, 1986; Horty, 1994; Brewka, 1994; Baader &
Hollunder, 1995b) interplay subsumption abnormality predicates addressed nonmonotonic DLs. Consider, example, following
TBox:
User
Staff
Staff
BlacklistedStaff

v
v
v
v

hasAccessTo.{ConfidentialFile} AbUser
User
hasAccessTo.{ConfidentialFile} AbStaff
Staff u hasAccessTo.{ConfidentialFile}

get models normal possible, first attempt could minimize
two abnormality predicates AbUser AbStaff parallel. Assume hasAccessTo
varying, User, Staff, BlacklistedStaff fixed. Then, result parallel
minimization staff members may may access confidential files,
equal preference. first case, abnormal users, second case,
abnormal staff. However, one may argue first option preferred: since
Staff v User (but way round), normality information staff
specific normality information users higher priority. effects well-known also propositional/first-order case indeed, circumscription
soon introduction extended priorities address issues specificity
(McCarthy, 1986).
formalism, users specify priorities minimized predicates. Typically,
priorities reflect subsumption hierarchy (as computed w.r.t. class
models). Since subsumption hierarchy general partial order, priorities
minimized predicates may form partial order, too. approach analogous partially
724

fiThe Complexity Circumscription DLs

ordering priorities default rules, proposed Brewka (1994). general
standard prioritized circumscription, assumes total ordering (McCarthy, 1986;
Lifschitz, 1985), special case nested circumscription (Lifschitz, 1995).
2.2 Circumscribed Knowledge Bases
define DLs circumscription, start introducing circumscription patterns.
describe individual predicates treated minimization.
Definition 1 (Circumscription pattern, <CP ) circumscription pattern tuple CP
form (, M, F, V ), strict partial order , , F , V
mutually disjoint subsets NC NR , minimized, fixed, varying predicates,
respectively. , denote reflexive closure . Define preference relation <CP
interpretations setting <CP J iff following conditions hold:
1. = J and, NI , aI = aJ ,
2. p F , pI = pJ ,
3. p , pI 6 pJ exists q , q p, q q J ,
4. exists p pI pJ q q p, q = q J .
F NC (i.e., minimized fixed predicates concepts) call
(, M, F, V ) concept circumscription pattern.
4
use term concept circumscription concept circumscription patterns admitted. Based circumscription patterns, define circumscribed DL knowledge
bases models.
Definition 2 (Circumscribed KB) circumscribed knowledge base (cKB) expression CircCP (T , A), TBox, ABox, CP = (, M, F, V ) circumscription pattern M, F, V partition predicates used A. interpretation
model CircCP (T , A) model exists model 0
0 <CP I.
cKB CircCP (T , A) called concept-circumscribed KB CP concept circumscription pattern.
4
Note partially ordered circumscription becomes standard parallel circumscription
empty relation used .
main reasoning tasks (non-circumscribed) KBs satisfiability concepts
w.r.t. KBs, subsumption w.r.t. KBs, instance checking w.r.t. KBs. reasoning
tasks fundamental circumscribed KBs well. provide precise definitions
tasks. Throughout following section, DL denotes set DLs
introduced previous section; i.e., ALC, ALCI, ALCO, ALCQ, ALCQI, ALCIO,
ALCQO, ALCQIO.
Definition 3 (Reasoning tasks)
725

fiBonatti, Lutz, & Wolter

concept C satisfiable w.r.t. cKB CircCP (T , A) model CircCP (T , A)
satisfies C 6= . Let L DL. satisfiability problem w.r.t. cKBs L defined
follows: given concept C L cKB CircCP (T , A) L, decide whether C
satisfiable w.r.t. CircCP (T , A).
concept C subsumed concept w.r.t. cKB CircCP (T , A), symbols
CircCP (T , A) |= C v D, C DI models CircCP (T , A). Let L DL.
subsumption problem w.r.t. cKBs L defined follows: given concepts C
L cKB CircCP (T , A) L, decide whether CircCP (T , A) |= C v D.
individual name instance concept C w.r.t. cKB CircCP (T , A),
symbols CircCP (T , A) |= C(a), aI C models CircCP (T , A). Let
L DL. instance problem w.r.t. cKBs L defined follows: given
concept C L, individual name a, cKB CircCP (T , A) L, decide whether
CircCP (T , A) |= C(a).
4
reasoning problems polynomially reduced one another: first, C satisfiable
w.r.t. CircCP (T , A) iff CircCP (T , A) 6|= C v , CircCP (T , A) |= C v iff C u
satisfiable w.r.t. CircCP (T , A). second, C satisfiable w.r.t. CircCP (T , A) iff
CircCP (T , A) 6|= C(a), individual name appearing A; conversely,
CircCP (T , A) |= C(a) iff AuC satisfiable w.r.t. CircCP0 (T , A{A(a)}),
concept name occurring A, CP0 obtained CP adding
(and leaving is). paper, use satisfiability w.r.t. cKBs basic
reasoning problem.

3. Basic Reductions
present three basic reductions reasoning problems circumscribed knowledge
bases interesting right and, additionally, useful establishing
main results paper later on. precisely, replay well-known reduction
fixed predicates minimized predicates context DLs, reduce reasoning w.r.t. cKBs
general TBoxes reasoning w.r.t. cKBs acyclic TBoxes, show that,
certain conditions, simultaneous satisfiability w.r.t. collection cKBs reducible
satisfiability w.r.t. single cKB.
3.1 Fixed minimized concepts
circumscription, folklore fixed predicates simulated terms minimized
predicates, see e.g. de Kleer (1989). case DLs, simulation possible
concept names. see this, let C0 concept CircCP (T , A) circumscribed
KB CP = (, M, F, V ) F0 = {A1 , . . . , Ak } = F NC . Define new pattern
CP0 = (, 0 , F \ F0 , V )
0 = {A1 , . . . , Ak , A01 , . . . , A0k }, A01 , . . . , A0k concept names
occur C0 , , F , V , , A;
.
0 = {A0i = Ai | 1 k}.
726

fiThe Complexity Circumscription DLs

difficult see C0 satisfiable w.r.t. CircCP (T , A) iff satisfiable w.r.t.
CircCP0 (T 0 , A). Thus, get following result.
Lemma 4 Let L DL. satisfiability w.r.t. (concept-)circumscribed KBs L
polynomially reduced satisfiability w.r.t. (concept-)circumscribed KBs L
fixed concept names.
contrast concept names, fixed role names cannot reduced minimized role names
since Boolean operators roles available standard DLs ALCQIO.
proof given Section 6, show that, cases, reasoning minimized
role names decidable, whereas corresponding reasoning task cKBs fixed role
names undecidable.
reduction clearly relies TBoxes. However, paper sometimes
work circumscribed KBs TBox empty. following lemma, proved
appendix, shows cKBs ALC without fixed role names empty TBox,
one simulate fixed concept names using minimized concept names without introducing
TBox. proof, may viewed much careful version proof
Lemma 4, adapted yield analogous result logics DL.
Lemma 5 ALC, satisfiability w.r.t. (concept-)circumscribed KBs empty TBox
without fixed roles polynomially reduced satisfiability w.r.t. (concept-)circumscribed
KBs empty TBox without fixed predicates.
3.2 Acyclic General TBoxes
many DLs, satisfiability w.r.t. (non-circumscribed) KBs general TBoxes harder
satisfiability w.r.t. (non-circumscribed) KBs acyclic TBoxes. case ALC,
ALCI, ALCQ, ALCQO, latter problem PSpace-complete (Baader, McGuiness,
Nardi, & Patel-Schneider, 2003; Baader, Milicic, Lutz, Sattler, & Wolter, 2005b; Y. Ding
& Wu, 2007) former ExpTime-complete (Baader et al., 2003).
DLs considered paper satisfiability ExpTime-hard already acyclic
TBoxes ALCIO extensions (Areces, Blackburn, & Marx, 2000). show that,
circumscribed KBs, difference computational complexity acyclic
general TBoxes.
Let C0 concept CircCP (T , A) cKB CP = (, M, F, V ). may assume
.
without loss generality = {> = C} concept C. (To see this, observe
.
axioms C v equivalent > = C D). Define
.
.
.
.
acyclic TBox 0 = {A = C, B = u.A, A0 = A, B 0 = B}, A, B, A0 , B 0 , u
new concept role names occurring , A, , F , V , C0 .
circumscription pattern CP0 = (, 0 , F, V 0 ), 0 = {A0 , B 0 } V 0 =
V {A, B, u}.
ad B 0 conjunctively C0 thus interested models CircCP0 (T 0 , A)
(B 0 )I 6= . models, AI = (and thus C = ) since, otherwise,
turn instance B 0 instance B 0 making instance B
727

fiBonatti, Lutz, & Wolter

linking via role u instance A, thus obtaining preferred model w.r.t.
<CP0 . basis proof following lemma, given appendix.
Lemma 6 C0 satisfiable w.r.t. CircCP (T , A) iff C0 uB 0 satisfiable w.r.t. CircCP0 (T 0 , A).
Thus, obtained following result.
Proposition 7 Let L DL. Satisfiability w.r.t. (concept-)circumscribed KBs L
polynomially reduced satisfiability w.r.t. (concept-)circumscribed KBs L acyclic
TBoxes without changing ABox.
shows satisfiability w.r.t. cKBs acyclic TBoxes complexity
satisfiability w.r.t. cKBs general TBoxes. many cases considered paper,
even true cKBs empty TBoxes, c.f. Section 4. However, also identify
cases cKBs non-empty TBoxes higher complexity (see Theorems 24 28),
thus general reduction one underlying Proposition 7 cannot exist case
empty TBoxes.
3.3 Simultaneous Satisfiability
applications, often necessary merge TBoxes, ABoxes, whole knowledge bases
taking union. show that, certain conditions, reasoning w.r.t. union
several circumscribed KBs reduced reasoning w.r.t. component cKBs.
concept C simultaneously satisfiable w.r.t. cKBs CircCP1 (T1 , A1 ), . . . , CircCPk (Tk , Ak )
exists interpretation model cKBs satisfies C 6= .
following lemma says simultaneous satisfiability polynomially reduced
satisfiability w.r.t. single cKB two cKBs share role name.
proof idea case k = 2 follows. Given CircCP1 (T1 , A1 ) CircCP2 (T2 , A2 ),
first take union two cKBs, replacing CircCP2 (T2 , A2 ) concept name
also used CircCP1 (T1 , A1 ) fresh concept name A0 . introduce
additional concept name P (for problem) make sure P satisfied ABox
individual whenever point model interpretation A0
disagrees. look model P satisfied ABox. Intuitively,
additional concept name P satisfies purpose decoupling A0 , important
e.g. case A/A0 minimized CircCP1 (T1 , A1 ) CircCP2 (T2 , A2 ). Details
given appendix.
Lemma 8 L DL, simultaneous satisfiability w.r.t. (concept-)circumscribed KBs
CircCP1 (T1 , A1 ), . . . CircCPk (Tk , Ak ), CircCPi (Ti , Ai ) CircCPj (Tj , Aj ) share
role names 1 < j k, reduced polynomial time satisfiability w.r.t. single
(concept-)circumscribed KBs.

4. Complexity Reasoning Concept-Circumscribed KBs
main contributions paper consist (i) showing that, many cases, reasoning
circumscribed knowledge bases decidable; (ii) performing detailed analysis
728

fiThe Complexity Circumscription DLs

computational complexity decidable cases. section, show
satisfiability w.r.t. concept-circumscribed KBs NExpNP -complete DL ALC
extensions ALCIO ALCQO. also show NPNExp -complete number
fixed minimized concept names bounded constant. first present proofs
upper bounds establish matching lower bounds.
4.1 Upper Bounds
start general case bound number fixed
minimized predicates.
4.1.1 General Case
prepare upper bound proof showing concept satisfiable w.r.t.
concept-circumscribed KB, satisfiable model bounded size. use |C|
denote length concept C, i.e.,
P number (occurrences of) symbols needed
write C. size |T | TBox CvDT |C| + |D|, size |A| ABox
sum sizes assertions A, size role assertion 1
size concept assertions C(a) |C|.
Lemma 9 Let C0 concept, CircCP (T , A) concept-circumscribed KB, n := |C0 | +
|T | + |A|. C0 satisfiable w.r.t. CircCP (T , A), following holds:
(i) , C0 formulated ALCIO, C0 satisfied model
CircCP (T , A) #I 22n .
(ii) , C0 formulated ALCQO maximal parameter occurring number restriction , A, C0 , C0 satisfied model
CircCP (T , A) #I 22n (m + 1) n.
Proof. Let CP, , A, C0 Lemma 9. may assume = every
assertion C(a) expressed implication {a} v C, every assertion r(a, b)
expressed {a} v r.{b}. Denote cl(C, ) smallest set concepts contains
subconcepts C, subconcepts concepts appearing , closed single
negations (i.e., cl(C, ) start , cl(C, )).
Let common model C0 CircCP (T , A), let d0 C0I . Define equivalence relation setting d0 iff
{C cl(C0 , ) | C } = {C cl(C0 , ) | d0 C }.
use [d] denote equivalence class w.r.t. relation. Pick
equivalence class [d] exactly one member denote resulting subset 0 .
first prove Point (i). Thus, assume C0 formulated ALCIO.
define new interpretation J follows:
J

:= 0

AJ

:= {d 0 | AI }

rJ

:= {(d1 , d2 ) 0 0 | d01 [d1 ], d02 [d2 ] : (d01 , d02 ) rI }

aJ

:= 0 aI [d].
729

fiBonatti, Lutz, & Wolter

following claim easily proved using induction structure C.
Claim: C cl(C0 , ) , C iff d0 C J element
d0 [d] J .
Thus, J model satisfying C0 . show J model CircCP (T , A), thus
remains show model J 0 J 0 <CP J . Assume contrary
J 0 . define interpretation 0 follows:


0

AI

0

:=
[
[d]
:=
dAJ 0

r

I0

[

:=

(d1 ,d2
0
aI

[d1 ] [d2 ]

)rJ 0

:= aI .

matter routine show following:
0

0

Claim: concepts C cl(C0 , ) , C iff d0 C J
element d0 [d] J .
0

0

follows 0 model . Observe AI fi AI iff AJ fi AJ concept
name fi {, }. Therefore since CP concept circumscription pattern
0 <CP follows J 0 <CP J . derived contradiction conclude J
model CircCP (T , A). Thus done since size J bounded 22n .
Point (ii). Pick, 0 concept (> k r C) cl(C0 , )
(> k r C)I , k elements {d0 | d0 C , (d, d0 ) rI }. Also pick,
concept (6 k r C) cl(C0 , ) ((6 k r C))I , k + 1 elements
{d0 | d0 C , (d, d0 ) rI }. Denote 00 collection elements picked. Take
00 \ 0 element ds 0 ds define interpretation J
J

:= 0 00

AJ

:= {d 0 00 | AI }

rJ

:= {(d1 , d2 ) 0 (0 00 ) | (d1 , d2 ) rI }
{(d1 , d2 ) (00 \ 0 ) (0 00 ) | (ds1 , d2 ) rI }

aJ

:= aI [d].

following claim easily proved.
Claim: C cl(C0 , ), following:
(i) d, d0 J , d0 , C J iff d0 C J ;
(ii) , C iff d0 C J element d0 [d] J .
Thus, J model satisfying C0 . show J model CircCP (T , A),
thus remains show model J 0 J 0 <CP J . Assume
contrary J 0 . define interpretation 0 . end, take
730

fiThe Complexity Circumscription DLs

\ J dp 0 dp . define 0 follows



0

I0

:=
0

0

:= AJ {d \ J | dp AJ }

rI

0

:= rJ {(d1 , d2 ) (I \ J ) | (dp1 , d2 ) rJ }

aI

0

:= aI .

0

0

Again, matter routine show:
0

Claim: concepts C cl(C0 , ) , C J iff C J
0
0
C (I \ J ) iff dp C J element dp [d] 0 .
0

0

0

follows 0 model . Observe AI fi AI iff AJ fi AJ concept
name fi {, }. Therefore since CP concept circumscription pattern
0 <CP follows J 0 <CP J . derived contradiction conclude
J model CircCP (T , A). Thus done since size J clearly bounded
22n (m + 1) n.

interesting note proof Lemma 9 go role names
minimized fixed. problem cannot overcome, proved undecidability
results presented Sections 5 6.
Using bounded model property established, prove decidability
reasoning concept-circumscribed KBs formulated ALCIO ALCQO.
precisely, Lemma 9 suggests non-deterministic decision procedure satisfiability w.r.t.
concept circumscription patterns: simply guess interpretation bounded size
check whether model. turns procedure shows containment satisfiability complexity class NExpNP , contains problems solved
non-deterministic exponentially time-bounded Turing machine access
NP oracle. known NExp NExpNP ExpSpace.
Theorem 10 ALCIO ALCQO, NExpNP decide whether concept
satisfiable w.r.t. concept-circumscribed KB CircCP (T , A).
Proof. hard see exists NP algorithm takes input
cKB CircCP (T , A) finite interpretation I, checks whether model
CircCP (T , A): algorithm first verifies polynomial time whether model
A, answering yes case. Otherwise, algorithm guesses interpretation
J domain interprets individual names way,
checks whether (i) J model A, (ii) J <CP I. answers yes
checks succeed, otherwise. Clearly, checking whether J <CP done
time polynomial w.r.t. size J I.
NP algorithm may used oracle NExp-algorithm deciding
satisfiability concept C0 w.r.t. cKB CircCP (T , A): Lemma 9, suffices guess
interpretation size 24k k = |C0 | + |T | + |A|,3 use NP algorithm
check whether model CircCP (T , A). proves concept satisfiability
NExpNP .

3. bound 24k clearly dominates two bounds given Parts (i) (ii) Lemma 9.

731

fiBonatti, Lutz, & Wolter

reductions given Section 2, Theorem 10 yields co-NExpNP upper bounds
subsumption instance problem. show Section 4.2 upper
bounds tight.
4.1.2 Bounded Number Minimized Fixed Predicates
Since NExpNP rather large complexity class, natural question whether
impose restrictions concept circumscription reasoning becomes simpler.
following, identify case considering concept-circumscribed KBs
number minimized fixed concept names bounded constant. case,
complexity satisfiability w.r.t. concept-circumscribed KBs drops NPNExp . readers
uninitiated oracle complexity classes, recall NExp NPNExp NExpNP ,
NPNExp believed much less powerful NExpNP , see example work
Eiter et al. (2004).
prove NPNExp upper bound, first introduce counting formulas common
generalization TBoxes ABoxes.
Definition 11 (Counting Formula) counting formula Boolean combination
concept implications, ABox assertions C(a), cardinality assertions (C = n) C
concept n non-negative integer. use , , denote Boolean
operators counting formulas. interpretation satisfies cardinality assertion (C = n)
#C = n. satisfaction relation |= models counting formulas
defined obvious way.
4
following, assume integers occurring cardinality assertions coded
binary. NPNExp algorithm devised use algorithm satisfiability
(non-circumscribed) counting formulas oracle. Therefore, first determine
computational complexity latter. follows results Tobies (2000) that,
ALC, satisfiability counting formulas NExp-hard. matching upper bound
DLs ALCIO ALCQO obtained facts (i) polynomial
translation counting formulas formulated languages C2, two-variable
fragment first-order logic extended counting quantifiers (Gradel, Otto, & Rosen,
1997; Pacholski, Szwast, & Tendera, 2000), (ii) satisfiability C2 NExp even
numbers counting quantifiers coded binary (Pratt-Hartmann, 2005).
Theorem 12 (Tobies, Pratt) ALC, ALCIO ALCQO, satisfiability counting
formulas NExp-complete even numbers number restrictions coded binary.
establish improved upper bound.
Theorem 13 Let c constant. ALCIO ALCQO, NPNExp decide satisfiability w.r.t. concept-circumscribed KBs CircCP (T , A), CP = (, M, F, V )
#M c #F c.

732

fiThe Complexity Circumscription DLs

Proof. Assume want decide satisfiability concept C0 w.r.t. cKB
CircCP (T , A), CP = (, M, F, V ) #M c #F c. Lemma 4,
may assume F = (we may increase constant c appropriately).
may assume without loss generality cardinality exactly c. Thus, let
= {A0 , . . . , Ac }. Lemma 9, C0 satisfiable w.r.t. CircCP (T , A) iff exists
model C0 CircCP (T , A) size 24k , k = |C0 | + |T | + |A|. Consider,
, concept
CS :=
Au
A.

u



u

A{A1 ,...,Ac }\S

c constant, number 2c concepts constant well. Clearly, sets CSI ,
, form partition domain model I. Introduce, concept
name B role name r A, fresh concept name B 0 fresh role name r0 ,
respectively. concept C, denote C 0 result replacing C concept name
B role name r B 0 r0 , respectively. primed versions A0 0
defined analogously. Denote N set individual names {C0 }.
NExp-oracle going use algorithm checks whether counting formula
satisfiable not. Now, NPNExp -algorithm follows (we use C @
abbreviation counting formula (C v D) (D v C)):
1. Guess
sequence (nS | ) numbers nS 24k coded binary;
individual name N , exactly one set Sa ;
subset E N N .
2. calling oracle, check whether counting formula 1 satisfiable, 1
conjunction
{(C0 = 0)};
(CS = nS ), ;
CSa (a), N ;
{({a} v {b}) | (a, b) E} {({a} v {b}) | (a, b) N E}.
3. calling oracle, check whether counting formula 2 satisfiable, 2
conjunction
0 A0 ;
(CS = nS ), (note use unprimed versions);
CSa (a), individual name N (we use unprimed versions);
{({a} v {b}) | (a, b) E} {({a} v {b}) | (a, b) N E};
,
(A0 v A)

_
BM,BA

733

(B 0 @ B);

fiBonatti, Lutz, & Wolter

and, finally,
_

^

(A0 @ A)




.
(B = B 0 ) .

BM,BA

4. algorithm states C0 satisfiable model CircCP (T , A) if, if,
1 satisfiable 2 satisfiable.
Using fact c fixed, hard verify NPNExp -algorithm.
remains show correctness completeness.
Suppose exists model CircCP (T , A) satisfying C0 .
model size bounded 24k . Let algorithm guess
numbers nS = #CSI , ,
sets Sa aI CSIa ,
set E = {(a, b), (b, a) | aI = bI , a, b N }.
Clearly, 1 satisfied I. remains show 2 unsatisfiable. suppose
exists model J satisfying 2 . definitions 1 2 , may assume
= J ;
AI = AJ ;
aI = aJ individual names a.
Moreover, unprimed role names occur 2 unprimed concept names
2 , may assume interpretation unprimed concept role
names J coincide. Thus, J model CircCP (T , A) satisfying C0 .
define model J 0 domain J setting
0

aJ = aJ , individual names a;
0

rJ = (r0 )J , role names r;
0

AJ = (A0 )J , concept names A.
Then, conjunct Item 1 definition 2 , J 0 model .
Items 5 6 definition 2 , J 0 <CP J , derived contradiction.
Conversely, suppose algorithm says exists model CircCP (T , A) satisfying C0 . take model 1 . conjunct Item 1 1 , model
satisfying C0 . follows unsatisfiability 2 model
CircCP (T , A).


734

fiThe Complexity Circumscription DLs

corollary, obtain co-NPNExp upper bounds subsumption instance problem. similar drop complexity occurs propositional logic, satisfiability w.r.t.
circumscribed theories complete NPNP difficult see bounding
minimized fixed predicates allows us find PNP algorithm.
4.2 Lower Bounds
prove lower complexity bounds reasoning concept-circumscribed KBs
match upper bounds given Section 4.1.
4.2.1 General Case
Section 4.1, start general case number fixed minimized
predicates bounded. aim establish two NExpNP -lower bounds
match upper bound established Theorem 10. first bound satisfiability w.r.t.
concept-circumscribed KBs formulated ALC empty TBox, nonempty ABox. second bound also satisfiability w.r.t. concept-circumscribed KBs
formulated ALC, assumes acyclic TBox empty ABox. reductions work
already case empty preference relation, without fixed predicates. Note
considering satisfiability concept C w.r.t. concept-circumscribed KB CircCP (T , A)
empty interesting: seen C satisfiable w.r.t.
CircCP (T , A) iff C satisfiable (without reference KB), C concept
obtained C replacing minimized concept names .
proof first result reduction succinct version problem coCERT3COL, NExpNP -complete (Eiter, Gottlob, & Mannila, 1997), satisfiability
w.r.t. concept-circumscribed KBs empty TBox. Let us first introduce regular (nonsuccinct) version co-CERT3COL:
Instance size n: undirected graph G vertices {0, 1, . . . , n 1} every
edge labelled disjunction two literals Boolean variables {Vi,j | i, j < n}.
Yes-Instance size n: instance G size n that, truth value assignment
Boolean variables, graph t(G) obtained G including edges
whose label evaluates true 3-colorable.
shown Stewart (1991), co-CERT3COL complete NPNP . obtain problem
complete NExpNP , Eiter et al. use complexity upgrade technique: encoding
input succinct form using Boolean circuits, complexity raised one exponential
NExpNP (Eiter et al., 1997). precisely, succinct version co-CERT3COLS
co-CERT3COL obtained representing input graph G nodes {0, . . . , 2n 1}
4n + 3 Boolean circuits 2n inputs (and one output) each. Boolean circuits
(1)
(2)
(i)
named cE , cS , cS , cj , {1, 2, 3, 4} j < n. circuits, 2n inputs
bits binary representation two nodes graph. purpose
circuits follows:
circuit cE outputs 1 edge two input nodes, 0 otherwise;
(1)

edge input nodes, circuit cS outputs 1 first literal
(2)
disjunction labelling edge positive, 0 otherwise; circuit cS
735

fiBonatti, Lutz, & Wolter

second literal; edge input nodes, output
arbitrary;
(i)

edge input nodes, circuits cj compute labelling
Vk1 ,k2 Vk3 ,k4 edge input nodes generating numbers k1 , . . . , k4 :
(i)
circuit cj outputs j-th bit ki ; edge input nodes,
output arbitrary.
reduce co-CERT3COLS satisfiability w.r.t. concept-circumscribed KBs formulated ALC whose TBox preference relation empty. remains
apply Lemma 5 eliminate fixed concept names (we note construction
proof lemma leaves preference relation untouched). Let
(1)

(2)

(i)

G = (n, cE , cS , cS , {cj }i{1,..,4},j<n )
(succinct representation the) input graph 2n nodes. construct ABox
AG = {C0 u Root(a0 )}, circumscription pattern CPG , concept CG G
yes-instance co-CERT3COLS iff CG satisfiable w.r.t. CircCPG (, AG ).
concept C0 used AG conjunction whose presentation split two parts.
Intuitively, purpose first group conjuncts fix truth assignment
variables {Vi,j | i, j < n}, construct (an isomorphic image of) graph t(G)
obtained G including edges whose label evaluates true t. Then,
purpose second group make sure t(G) 3-colorable.
formulating C0 , use several binary counters counting modulo 2n (the
number nodes input graph). main counters X use concept names
X0 , . . . , Xn1 Y0 , . . . , Yn1 bits, respectively. Additionally, introduce concept
(i)
(i)
names K0 , . . . , Kn1 , {1, 2, 3, 4}, serve four additional counters K (1) , . . . , K (4) .
first group conjuncts C0 found Figure 2, following abbreviations
used:
ri .C denotes n-fold nesting r. .r.C;
r.(K (i) = X) abbreviation

u

j<n

(i)
(i)
(Xj r.Kj ) u (Xj r.Kj )

similarly r.(K (i) = );
abbreviations Wc , c Boolean circuit, explained later on.
intuition behind Figure 2 follows. Lines (1) (5) build binary tree depth
2n whose edges labeled role name r. 22n leaves tree instances
concept name Leaf, labeled possible values counters X
. Since minimize Leaf via circumscription pattern CPG , concept name
denotes precisely leaves tree. Due use counters X , leaves
distinct.
leaves tree established satisfy number purposes. start with,
leaf counter values X = = j corresponds variable Vi,j co3CERTCOLS determines truth value variable via truth/falsity concept
736

fiThe Complexity Circumscription DLs

ri .(r.Xi u r.Xi )
j

r .((Xi r.Xi ) u (Xi r.Xi ))
r
r

n+j

n+i

.(r.Yi u r.Yi )

.((Yi r.Yi ) u (Yi r.Yi ))

< n

(1)

< n, j < 2n

(2)

i<n

(3)

< j < n

(4)

2n

r .Leaf

(5)

r2n .(WcE u Wc(1) u Wc(2) )

(6)





r2n .(Wc(1) u u Wc(4) )
j

j < n

r2n .(var1.LeafFix u var1.(K (1) =X) u var1.(K (2) =Y ))
2n

r .(var2.LeafFix u var2.(K

(3)

(7)

j

=X) u var2.(K

(4)

(8)

=Y ))

(9)

2n

P r .var1.Leaf

(10)

2n

P r .var2.Leaf

(11)

r2n .(S1 (Tr1 var1.Tr))

(12)

2n

r .(S1 (Tr1 var1.Tr))
2n

r .(S2 (Tr2 var2.Tr))
2n

(13)
(14)

r .(S2 (Tr2 var1.Tr))

(15)

2n

(16)

r .(Elim (E (Tr1 Tr2 )))
Figure 2: first group conjuncts C0 .

name Tr. Thus, leaves jointly describe truth assignment instance G co3CERTCOLS . second purpose leaves represent potential edges G:
additionally representing variable, leaf X = = j corresponds
potential edge nodes j. explain properly, must first
discuss abbreviations Wc used Lines (6) (7) Figure 2.
concept Wc , c Boolean circuit 2n inputs, result converting c
concept uses constructors , u, following condition satisfied:
instance Wc , output c upon input b0 , . . . , b2n1 b, truth
value concept names X0 , . . . , Xn1 , Y0 , . . . , Yn1 described b0 , . . . , b2n1 ,
truth value concept name described b. introducing one
auxiliary concept name every inner gate c, translation done
size Wc linear size c. following concept names used output:
WcE uses concept name E output;
Wc(i) uses concept name Si output, {1, 2};


(i)

Wc(i) uses concept name Kj

output, {1, . . . , 4} j < n.

j

737

fiBonatti, Lutz, & Wolter

r2n .(col1.LeafFix u col2.LeafFix)
2n

r .(col1.(X = X) u col1.(Y = 0))
2n

r .(col2.(Y = X) u col2.(Y = 0))

(17)
(18)
(19)

2n

(20)

2n

P r .col2.Leaf

(21)

r2n .((Y = 0) (R B G))

(22)

r .((Y = 0) ((R u B) u (R u G) u (B u G)))

(23)

r2n .((Elim u col1.R u col2.R) Clash)

(24)

r2n .((Elim u col1.G u col2.G) Clash)

(25)

r2n .((Elim u col1.B u col2.B) Clash)

(26)

P r .col1.Leaf

2n

Figure 3: second group conjuncts C0 .
Lines (6) (7) ensure concepts propagated leaves. next aim
ensure leaf represents potential edge (i, j) connected via role var1
leaf represents variable first disjunct label (i, j), analogously
role var2 variable second disjunct edge label. replaced
concept name LeafFix Leaf Lines (8) (9), lines would apparently
encode properties. However, careful mentioned replacement
would interact minimization Leaf. fix problem, resort trick:
use concept name LeafFix instead Leaf. way, may may reach
instance Leaf. not, force concept name P true root
tree Lines (10) (11). use CG rule models P true. Finally,
fix LeafFix via CPG eliminate interaction minimization Leaf.
remaining Lines (12) (16) ensure leaf instance Elim iff potential
edge represents present graph t(G) induced truth assignment
described leaves.
second group conjuncts C0 found Figure 3. Here, (Y = 0) stands
concept (Y0 u u Yn1 ). already mentioned, purpose conjuncts
ensure graph t(G) described leaves 3-coloring.
strategy ensuring follows: use 2n leaves = 0 store colors
nodes, i.e., leaf X = = 0 stores color node i. Lines (22)
(23), unique coloring. Then, Lines (17) (21) ensure leaf (viewed
edge) connected via role col1 leaf stores color first node
edge, analogously role col2 second node edge. LeafFix
P role before. Lines (24) (26) guarantee concept name Clash
identifies problems coloring: leaf Clash represents edge exists
G, dropped t(G), endpoints color. idea
Clash minimized R, G, B vary. additional concept
names fixed, corresponds universal quantification possible colorings.
738

fiThe Complexity Circumscription DLs

Set CG = Root u P u r2n .Clash, recall AG = {C0 u Root(a0 )}. following
lemma proved appendix.
Lemma 14 G yes-instance co-3CERTCOLS iff CG satisfiable respect
CircCPG (, AG ), CPG = (, M, F, V ) = , = {Root, Leaf, Clash},
F = {LeafFix, Tr, X0 , . . . , Xn1 , Y0 , . . . , Yn1 , },
V set remaining predicates AG .
Since size AG polynomial n, get following result applying Lemma 5.
Theorem 15 ALC, satisfiability w.r.t. concept-circumscribed KBs NExpNP -hard even
TBox preference relation empty fixed predicates.
rather straightforward establish announced second NExpNP lower bound
reduction satisfiability w.r.t. concept-circumscribed KBs special case formulated
Theorem 15. Details given appendix.
Corollary 16 ALC, satisfiability w.r.t. concept-circumscribed KBs NExpNP -hard
even TBox acyclic, ABox preference relations empty,
fixed predicates.
Corresponding lower bounds subsumption instance problems follow
reduction given Section 2.
4.2.2 Bounded Number Minimized Fixed Predicates
establish matching lower bound Theorem 13 showing that, ALC, satisfiability w.r.t. concept-circumscribed KBs NPNExp -hard even constant number
predicates allowed minimized fixed. contrast previous section,
ignore case empty TBoxes directly establish lower bound case
non-empty TBoxes empty ABoxes. allows us demonstrate usefulness
Lemma 8 separating different parts lower bound proof: main reduction
previous section, two parts reduction shown Figure 2 3 truly
independent, forced us implement technical trick involves concept
names LeafFix P . using Lemma 8, contrast, achieve true separation
concerns. general, though, conjecture lower bound proved section
also established case empty TBoxes adapting mentioned technical trick.
leave problem interested reader.
Recall (non-deterministic) k-tape Turing machine described tuple
(Q, , q0 , , qacc , qrej ),
Q finite set states, finite alphabet, q0 Q starting state,
Q k Q k {L, R}k
transition relation, qacc , qrej Q accepting rejecting states. purposes,
oracle Turing machine 2-tape Turing machine that, additionally, equipped

739

fiBonatti, Lutz, & Wolter

1-tape Turing machine M0 (the oracle) whose alphabet identical M,
query state q? ,
two answer states qyes qno .
second tape called oracle tape. enters q? , oracle determines
next state M: content oracle tape contained language accepted
oracle, next state qyes . Otherwise, qno . transition, head
moved symbols written. state q? cannot occur left-most component
tuple Ms transition relation.
Let = (Q, , q0 , , qacc , qrej , M0 , q? , qyes , qno ) oracle Turing machine
following holds:
solves NPNExp -complete problem;
time consumption bounded polynomial p (where oracle calls contribute single clock tick);
0 , q 0 ) bounded 2q(n) , q
time consumption M0 = (Q0 , , q00 , 0 , qacc
rej
polynomial.

assume without loss generality M0 never attempt move left
left-most position tape (neither right right-most position). NPNExp hardness proof uses reduction word problem M. Thus, let w input
length n, let = p(n) m0 = q(p(n)). construct three TBoxes
Tw , Tw0 , Tw00 , circumscription patterns CP, CP0 , CP00 , concept Cw
accepts w iff Cw simultaneously satisfiable w.r.t. CircCP (Tw , ), CircCP0 (Tw0 , ),
CircCP00 (Tw00 , ). Then, Lemma 8 yields reduction (non-simultaneous) satisfiability w.r.t.
concept-circumscribed cKBs. Intuitively, purpose first TBox Tw impose
basic structure domain, Tw0 describes computations M, Tw00 describes
computations M0 . use general TBoxes rather acyclic ones since, Lemma 6,
done without loss generality.
TBox Tw shown Figure 4. previous reduction, use concept
names X0 , . . . , Xm0 1 Y0 , . . . , Ym0 1 implement two binary counters counting
0
modulo 2m . also use abbreviations previous reduction. Additionally,
r.(X++) states value counter X0 , . . . , Xm0 1 incremented going
r-successors, i.e.,


Xj (Xk r.Xk ) u (Xk r.Xk )
0

u
u

k=0..m 1

k=0..m0 1

u


j=0..k1

j=0..k1



Xj (Xk r.Xk ) u (Xk r.Xk )

purpose Lines (27) (30) ensure that, possible value (i, j)
counters X , least one instance NExp satisfies (X = i) (Y = j).
0
0
minimize NExp, thus enforce NExp exactly 2m 2m elements.
0
0
elements interconnected via roles r (for right) u (for up) form 2m 2m grid. Later on, use grid encode computations oracle machine M0 . Observe
740

fiThe Complexity Circumscription DLs

> v aux.NExp
m0

NExp v ((X = 2

(27)
m0

1) r.NExp) u ((Y = 2

1) u.NExp)

(28)

NExp v r.(Y =Y ) u r.(X++)

(29)

NExp v u.(X=X) u u.(Y ++)

(30)

> v
Result v

u aux.(Result u R )
u (R u R )

(31)



i<m



i<j<m

(32)

j

> v aux.NP

(33)
Figure 4: TBox Tw .

that, since working simultaneous satisfiability, minimization NExp
interact anything going put TBoxes Tw0 Tw00 .
also minimize concept name Result, thus Lines (31) (32) guarantee
exactly instances Result, identified concept names R0 , . . . , Rm1 .
makes call oracle i-th step, result call stored
(unique) instance Result u Ri , i.e., instance satisfy concept name Rej
M0 rejected input falsify otherwise. Finally, also minimize NP, thus
Line (33) guarantees exactly one instance NP. instance used
represent computation M. Summing up, circumscription pattern Tw
CP := (, {NExp, Result, NP}, , V ),
V containing remaining predicates used Tw .
purpose Tw0 describe computations M. use following concept
names:
, i, j < m, k {1, 2}, introduce concept name Sai,j,k . Intuitively,
Sai,j,k expresses symbol j-th cell k-th tape i-th step
Ms computation. start numbering tape cells steps 0.
q Q < m, Qiq concept name expressing state q
i-th step computation.
q Q, i, j < m, k {1, 2}, Hji,k concept name expressing
k-th head cell j i-th step computation.
q Q, , i, j < m, k {1, 2}, {L, R}, concept names Aiq , Ai,j,k
,
i,k

serve markers. precisely, Aq means that, time point i,
executed transition switches state q. Similarly, Ai,j,k
describes symbol

i,k
written transition tape k, describes move tape k.
details Tw0 given Figure 5. One copy concept inclusions figure
needed every i, j, j 0 < every k {1, 2}. assume w = a0 an1
741

fiBonatti, Lutz, & Wolter

0,n,1
0,m1,1
NP v Q0q0 u H00,1 u Sa0,0,1
u Sa0,n1,1
u SB
u u SB
0
n1

(34)

0,0,2
0,m1,2
NP v H00,2 u SB
u u SB

(35)

NP v

u

i,j 0 ,2

a,b,qQ\{q? }

(Sai,j,1 u Sb

NP v
NP v
NP v
NP v
NP v

uA
uA

qQ


Ai,k
L
i,k
AR


q

i,j,k


0 ,2
i,2
Aiq u Ai,j,1
u Ai,j
u Ai,1

u 0 ) (37)
b

< 1

(38)

(q,a,b,q 0 ,a0 ,b0 ,M,M 0 )

Sai+1,j,k

i+1,k
Hj1

< 1

(39)

< 1 j > 0

(40)

i+1,k
Hj+1

< 1 j < 1

i+1,j,k
< 1 j 6= j 0
(Sai,j,k u Hji,k
0 ) Sa

u
u



Qi+1
q

a,b,a6=b

(Sai,j,k u Sbi,j,k ) u

NP v (Hji,k u Hji,k
0 )

(36)



(
NP v

u Qiq u Hji,1 u Hji,2
0 )

u

q,q 0 Q,q6=q 0

(Qiq u Qiq0 )

j 6= j 0

(41)
(42)
(43)
(44)

NP v resi .(Result u Ri ) u resi .(Result u Ri )

(45)

NP v (Qiq? u resi .Rej) Qi+1
qno

(46)

NP v

(Qiq?

u resi .Rej)

Qi+1
qyes

NP v (Qiq? u Hji,k ) Hji+1,k
NP v

u



< 1
< 1

< 1

i+1,j,k

(Qiq? u Sai,j,k ) Sa

(47)
(48)
(49)

Figure 5: TBox Tw0 .
use B denote (shared) blank symbol M0 . Lines (34) (43) describe
behaviour Turing machine usual way: transitions follow transition table,
computation starts initial configuration, etc. Line (45) ensures instance
NP reach (unique) instance Result u Ri via role resi , < m. Lines (46)
(47) deal transitions query state looking result oracle
call corresponding instance Result. Finally, Lines (48) (49) ensure head
position tape symbols change querying oracle. circumscription
pattern Tw0 simply CP0 := (, , , V ), V set predicates used Tw0 .
purpose Tw00 describe computations oracle Turing machine M0 . Note
describe single computation M0 (but polynomially many) since may visit state q? once. computations
represented NExp grid, different computations untangled use
different concept names computation. use counter X0 , . . . , Xm0 1 count
742

fiThe Complexity Circumscription DLs

configurations counter Y0 , . . . , Ym0 1 count tape cells configuration.
Moreover, use following concept names:
< m, concept name Sai . Sai satisfied instance
NExp X0 , . . . , Xm0 1 value j Y0 , . . . , Ym0 1 value k, i-th
computation M0 has, j-th step, symbol k-th cell.
q Q < m, concept name Qiq . purpose concept name
two-fold: first, represents current state M0 i-th computation.
second, indicates head position i-th computation.
, q Q, {L, R} < m, concept name Aiq,a,M marker.
meaning marker Aiq,a,M that, reach current configuration, M0
switched q, written a, moved head direction . Additionally, marker
indicates head position previous configuration.
additional concept name NH (for nohead) helps us enforce M0
single head position.
details Tw00 shown Figure 6, require one copy line every
< m. purpose Lines (50) (51) regenerate grid structure NExp using
roles r0 und u0 . necessary since roles r u used Tw , and, use
Lemma 8, TBoxes cannot share role names. Lines (52) (53) ensure every
instance NExp reaches (only) unique instance NP via role toNP, (only)
unique instance Result u Ri via role res0i , < m. Lines (54) (64) describe
computation M0 straightforward way. precisely, Lines (54) (56) set
initial configuration reading contents Ms oracle tape instance NP.
Lines (57) (61) implement transitions, Lines (62) (64) enforce unique label
tape, unique state, unique head position. Finally, Line (65) ensures that,
i-th computation M0 rejecting, Rej true instance Result u Ri .
Note M0 non-deterministic machine may one computation.
storing Rej Result u Ri , need know computations rejecting.
deal issue, Rej minimized predicates varying: exists
accepting computation M0 i-th input, represent computation
NExp make Rej false instance Result u Ri . Hence, Rej holds Result u Ri
iff exists accepting computation. Note cannot fix concept names
X0 , . . . , Xm0 1 , Y0 , . . . , Ym0 1 minimizing Rej since would get unbounded number
fixed concept names. means elements NExp change position
minimization, roles r0 u0 . harmful since Tw
Lines (50) (51) ensure structure (NExpI , (r0 )I , (u0 )I ) always isomorphic
grid, rest Tw00 ensures elements NExp always encode computations
M0 . thus use circumscription pattern CP00 := (, {Rej}, , V 0 ), V 0 contains
predicates used Tw00 except Rej.
proof following lemma left reader. formulation, union
Qiqacc imposes least one state computation accepting.
743

fiBonatti, Lutz, & Wolter

0

0

NExp v ((X = 2m 1) r0 .NExp) u ((Y = 2m 1) u0 .NExp)
0

0

0

0

(50)

NExp v r .(Y =Y ) u r .(X++) u u .(X=X) u u .(Y ++)

(51)

NExp v res0i .(Result u Ri ) u res0i .(Result u Ri )

(52)

NExp v toNP.NP u toNP.NP

(53)

NExp v

uu



j<m



(X = 0) u (Y = j) u toNP.Sai,j,2 Sai

(54)


NExp v ((X = 0) u (Y m)) SB

(55)

Qiq0
0

(56)

NExp v ((X = 0) u (Y = 0))
NExp v

uu



(Sai u Qiq )

qQ0



(q,a,q 0 ,a0 ,M )0

r0 .Aiq0 ,a0 ,M



(57)

NExp v Aiq,a,R (Sai u u0 .Qiq )

(58)

Aiq,a,L Sai
u0 .Aiq,a,L

(59)

NExp v
NExp v

NExp v
NExp v

(60)

Q u (S r .S )
u (S u ) u u
Q u .NH

NH Q u u .NH

NExp v
NExp v

Qiq

qQ0


q

a,b,a6=b
qQ0




rej


b




(61)
(Qiq u Qiq0 )

q,q 0 Q0 ,q6=q 0

0


q

qQ0

NExp v Qiq0

0







q

(62)
(63)

0

(64)

res0i .Rej

(65)

Figure 6: TBox Tw00 .
Lemma 17 accepts w iff Cw := NP u

tQ

i<m


qacc

simultaneously satisfiable w.r.t.

CircCP (Tw , ), CircCP0 (Tw0 , ), CircCP00 (Tw00 , ).

remains apply Lemmas 6, 4, 8 obtain following result.
Theorem 18 ALC, satisfiability w.r.t. concept-circumscribed cKBs NPNExp -hard
even TBox acyclic, ABox preference relations empty,
fixed predicates, number minimized predicates bounded constant.
already mentioned, conjecture result proved empty TBoxes
(but non-empty ABoxes). Corresponding lower bounds subsumption instance
problems follow reduction given Section 2.
744

fiThe Complexity Circumscription DLs

5. Circumscription Fixed Roles
preceeding sections, analyzed computational complexity reasoning
w.r.t. concept-circumscribed KBs and, particular, established decidability. current
section, extend concept-circumscribed KBs call role-fixing cKBs, differ
former allowing role names fixed (but minimized). Interestingly,
result seemingly harmless modification reasoning becomes highly undecidable.
start defining cKBs studied section.
Definition 19 cKB CircCP (T , A) CP = (, M, F, V ) called role-fixing contains role names.
4
pinpoint exact complexity reasoning role-fixing cKBs, present reduction
logical consequence problem monadic second-order logic binary relation symbols
(over unrestricted structures, trees) instance problem w.r.t. role-fixing cKBs
formulated ALC. follows latter problem harder problem definable
second-order arithmetic thus outside analytical hierarchy. Analogous results
satisfiability subsumption follow reductions given Section 2. reduction
applies already case TBox preference relation empty.
finite set R binary relation symbols, denote MSO(R) set formulas
constructed countably infinite set P1 , P2 , . . . variables sets, countable infinite
set x1 , x2 , . . . individual variables, binary relation symbols R, using Boolean
connectives, first-order quantification, monadic second-order quantification.
hard see reasoning role-fixing cKBs corresponds reasoning tiny fragment MSO(R). specifically, consider standard translation ALC-concepts C
FO-formulas (and thus MSO(R)-formulas) C ] (x) one free individual variable x e.g.
given Borgida (1996) take cKB CircCP (T , A) CP = (, M, F, V ), = V = ,
= {A}, F = {r}. Translate (T , A) MSO(R)-sentence
^
^
^
=
x(C ] (x) D] (x))
C ] (xa )
r(xa , xb ),
CvDT

C(a)A

r(a,b)A

xa individual variables.
ALC-concept C satisfiable w.r.t. CircCP (T , A) if, if, MSO(R)formula
xC ] (x) P (P A] [P/A] ])
satisfiable, P A] stands x (P (x) A] (x))x (A] (x)P (x)) [P/A] ]
denotes A] replaced P . translation easily extended case
arbitrary number concept names minimized arbitrary number concept
role names fixed varies.
prove logical consequence MSO(R) reducible instance problem w.r.t. role-fixing cKBs, thus establish surprising result reasoning
sKBs correspond tiny fragment MSO(R), hard
MSO(R). reduction indirect: instead directly reducing logical consequence
MSO(R), reduce semantic consequence problem modal logic exploit Thomasons result logical consequence MSO(R) reduced latter problem, see
745

fiBonatti, Lutz, & Wolter

works Thomason (1975b, 1975a) survey articles Wolter et al. (2007)
Goldblatt (2003) details.
first define semantic consequence problem modal logic (in framework
description logic) present Thomasons result, starting notation.
Let R finite set role names. R-frame structure F = (F , RF ), F
non-empty domain rF F F r R. interpretation = (I , )
based R-frame F iff F = rI = rF r R. say concept C
valid F write F |= C C = every interpretation based F. semantic
consequence modal logic defined follows. Let C ALC concepts
roles R. semantic consequence C, symbols C D, every
R-frame F, F |= C follows F |= D. Note since validity R-frame F
involves quantification possible interpretations symbols contained R,
relation C invariant uniform renamings atomic concepts (this
used later on).
simplicity, consider MSO(r), monadic second-order logic one binary
relation symbol r. straighforward extend result arbitrary finite sets R
relation symbols. Given set role names R, ALC-concept called ALC R -concept
uses role names R. following theorem follows
results Thomason (1975b, 1975a).
Theorem 20 exist effective translation : 7 () MSO(r) sentences
ALC {r} -concepts ALC {r} -concept C0 MSO(r) sentences ,
following conditions equivalent:
logical consequence MSO(r);
C0 u () ().
thus establish reduction MSO(r) instance problem w.r.t. role-fixing
cKBs reducing instead semantic consequence problem. fact, reduction
implemented transparent way extend ALC universal role, whereas
reduction ALC requires number rather technical intermediate steps.
reason, defer ALC case appendix give proof universal
role.
Let u new role name, called universal role. every interpretation I, u
fixed interpretation uI = . Since interpretation u fixed anyway,
allow use circumscription patterns.
suppose C ALC {r} -concepts. establish reduction, construct role-fixing cKB CircCP (, {C0 (a)}) concept C1 C if, if,
instance C1 w.r.t. CircCP (, {C0 (a)}). noted above, may assume C,
share concept names (otherwise, simply replace concept names fresh
ones). Let concept name occur C D, let CP = (, M, {r}, V ),
= , consists set concept names C, V consists
concept names D. Set = {(u.C u.
B)(a)}.

u

BM

Lemma 21 following conditions equivalent:
746

fiThe Complexity Circumscription DLs

C D;
instance (u.C) w.r.t. CircCP (, A).
Proof. prove Point 1 implies Point 2, assume Point 2 hold. Let
model CircCP (, A) aI ((u.C) u D)I . Let based F. prove
Point 1 hold, show F |= C F 6|= D. latter easy
witnessed interpretation I. show former, let J interpretation based
F. show C J = . First note that, since model aI (u.C)I ,
B = , B . distinguish two cases:
B J = , B . case, B interpretation
J . Thus, since concept names C J based
frame, obtain C J = C = .
B J 6= , least one B . J <CP I. Assume C J 6= .
aI (u.C)J J model A. Thus, derived contradiction
assumption model CircCP (, A).
prove Point 2 implies Point 1, assume Point 1 hold. Consider
frame F F |= C, F 6|= D. Let interpretation based F
aI (D)I . may also assume B = B (since B occurs
D). aI ((u.C) u D)I model A. remains show
exist 0 <CP 0 model A. straightforward: F |= C,
0
0
obtain exist 0 aI (u.C)I . Moreover, clearly
0
0
0
exist 0 B ( B B aI (u.
B)I .


u

BM

thus proved logical consequence problem MSO(r) effectively reducible
instance problem w.r.t. role-fixing cKBs formulated ALC extended universal role. reduction, TBox preference relation empty. appendix,
show reduction modified prove result ALC
without universal role.
Theorem 22 logical consequence problem MSO(r) effectively reducible
instance problem w.r.t. role-fixing cKBs formulated ALC. even holds
TBox preference relation empty.

6. Circumscription Minimized Roles
Unlike fixed concept names, fixed role names cannot simulated using minimized role
names. due fact Boolean operators roles available standard
DLs. Thus, Theorem 22 imply undecidability reasoning cKBs role
names allowed minimized, fixed. section, investigate cKBs
type. formal definition follows.
Definition 23 cKB CircCP (T , A) CP = (, M, F, V ) called role-minimizing F
contains role names.
4
747

fiBonatti, Lutz, & Wolter

show role-minimizing cKBs behave rather differently concept-circumscribed
KBs role-fixing cKBs. First, turns reasoning role-minimizing
cKBs empty TBox NExpNP -complete ALCQO, undecidable ALCI.
Thus, contrast circumscribed KBs considered far, observe difference
complexity ALCQO ALCI, even ALC ALCI. second
difference results obtained far NExpNP -lower bound, applies
cKBs formulated ALC empty TBox, even holds role-minimizing cKBs
single role minimized predicate fixed minimized. result
interest shows complexity drop NPNExp number
minimized predicates constant. Finally, show that, non-empty TBoxes, reasoning
role-minimizing cKBs becomes undecidable already ALC.
6.1 Role-minimizing cKBs Empty TBox ALCQO
first prove NExpNP -completeness result discussed DLs without inverse
roles. start upper bound. prove it, first establish bounded model
property using selective filtration-style argument, see e.g. Blackburn et al. (2001).
difference bounded model property proof given concept-circumscribed KBs
that, here, build quotient model given model identifying nodes using
equivalence relation, construct submodel given model selecting relevant nodes.
contrast forming quotient models, technique works empty TBoxes since
TBox force us select infinitely many nodes. Similarly, selection technique
work DLs inverse role because, shall see below, inverse roles
used simulate TBoxes.
Recall role depth rd(C) concept C defined nesting depth
constructors (> k r D) (6 k r D) C.
Theorem 24 ALCQO, satisfiability w.r.t. role-minimizing cKBs empty TBox
NExpNP .
Proof. Let CircCP (, A) role-minimizing cKB CP = (, M, F, V ), let C0
concept satisfiable w.r.t. CircCP (, A). Let m0 maximal parameter occurring
number restrictions C0 . Set
n := max({rd(C0 )} {rd(C) | C(a) A}) := ((m0 + 1) (|A| + |C0 |))n+1 ,
show exists model J CircCP (, A) satisfying C0 #J m.
Let model CircCP (, A) exists d0 C0I . , fix
minimal set D(d) that,
every concept (> k r C) occurs C0 (> k r C)I exist
least k distinct e D(d) (d, e) rI e C
every concept (6 k r C) occurs C0 6 (6 k r C)I exist
least k + 1 distinct e D(d) (d, e) rI e C .
Clearly, #D(d) (m0 + 1) (|C0 | + |A|) . Next, define set D0
setting
D0 := {d0 } {aI | NI occurs C0 }.
748

fiThe Complexity Circumscription DLs

Define sets Di , 1 n, inductively
[
Di+1 := (
D(d))
dDi

set n :=



0in Di .

Define interpretation 0 domain follows:

0

aI = aI , individual names a;
0

r V , (d, e) rI n \ Dn , e D(d), (d, e) rI ;
0

V , AI = AI n ;
0

F , AI = AI .
0

straightforward inductive argument shows 0 model d0 C0I .
0
Note change interpretation F . Moreover, pI pI
every p . Together fact 0 model 0 6<CP I, even get
0
pI = pI every p . follows 0 model CircCP (, A) J <CP 0
would imply J <CP I.
0
Note rI n n , every role r. define interpretation J domain
J = n putting
0

AJ = AI n , every concept name A;
0

rJ = rI , every role name r;
aJ = aI , every individual name C0 .
still J model satisfying C0 . Moreover, interpretation J 0 <CP J
satisfying easily extended interpretation 00 <CP 0 satisfying A. Hence,
interpretation exists J model CircCP (, A). #n derive
#J m.
proof NExpNP -upper bound exactly proof Theorem
10; suffices replace bound 24k size interpretations bound m.

give lower bound matching upper bound Theorem 24.
Theorem 25 ALC, satisfiability w.r.t. role-minimizing cKBs empty TBox NExpNP hard. holds even one minimized role name fixed prediates
Proof. Theorem 15, ALC NExpNP -hard decide whether concept C0
satisfiable w.r.t. CircCP (, A), CP = (, M, , V ) contains concept names only.
Clearly, still NExpNP -hard decide whether exists common model C0
CircCP (, A) size least #M . Thus, sufficient provide polynomial reduction
problem satisfiability problem w.r.t. cKBs ALC single minimized
role remaining predicates varying. Suppose C0 CircCP (, A) given. Let
= {A1 , . . . , Ak } take
two fresh role names r0 , r1 ;
749

fiBonatti, Lutz, & Wolter

Boolean concepts C1 , . . . , Ck built using fresh concept names B1 , . . . , Bk
every Ci , k, satisfiable every Ci u Cj , 6= j, unsatisfiable. One take,
example, Ci = B1 u u Bi1 u Bi u Bi+1 u u Bk , k.
Let CP0 = (, {r0 }, , V {B1 , . . . , Bk , r1 }) define A0 C00 result replacing,
C0 , every occurrence Ai r0 .Ci , k. Finally, set = A0 {r1 .Ci (a) |
k}. show following:
() C0 satisfiable w.r.t. CircCP (, A) model size least #M if, if, C00
satisfiable w.r.t. CircCP0 (, ).
Let model CircCP (, A) C0 size least #M . Define interpretation 0
domain extending follows: take mutually distinct d1 , . . . , dk
interpret B1 , . . . , Bk , r0 , r1 , way
0

CiI = {di }, k,
0

r0I = {(d, di ) | AIi , k},
aI = d1 ,
0

r1I = {(d1 , d1 ), . . . , (d1 , dk )}.
readily checked 0 model C00 CircCP0 (, ).
Conversely, let model CircCP0 (, ) C00 . Define interpretation 0
0
extending AIi = (r0 .Ci )I , k. readily checked 0 model C0
CircCP (, A).

6.2 Role-minimizing cKBs Nonempty TBox
bounded model property proof above, important selection nodes
stops n iterations set n , n maximum role depths
concepts ABox concept C want satisfy. bound selection
nodes exist TBox non-empty, show reasoning w.r.t.
role-minimizing cKBs indeed undecidable case. proof reduction
-tiling problem (Berger, 1966).

N N

Definition 26 tiling problem quadruple triple P = (T, H, V ), finite
set tile types H, V horizontal vertical matching conditions.
solution P mapping :

N N

( (i, j), (i + 1, j)) H i, j 0;
( (i, j), (i, j + 1)) V i, j 0.
4
750

fiThe Complexity Circumscription DLs

Let P = (T, H, V ) instance tiling problem. define TBox TP follows:
> v x.> u y.>
> v
> v



u

u



tT

tT

u

(66)
At0
0



(67)

t0 T,t6=t



(t,t0 )H


x.At0 u

u

tT





(t,t0 )V

y.At0



> v N (x.y.B u y.x.B)
N

v

(68)
(69)
(70)

w x.D y.D

(71)

v x.D u y.D

(72)

Let CP = (, M, , V ) circumscription pattern V = {B, D} consists
remaining concept role names.
Lemma 27 CircCP (TP , ) 6|= D(a) iff P solution.
Proof. Assume P solution . Define interpretation follows:

xI
yI
AIt
NI
BI
DI
aI

:=
:=
:=
:=
:=
:=
:=
:=

NN

{((i, j), (i + 1, j)) | i, j 0}
{((i, j), (i, j + 1)) | i, j 0}
{(i, j) | (i, j) = t}

{(i, j) | > 0 j > 0}

(0, 0)

straightforward verify model TP . Additionally, clearly aI
/ DI .
thus remains show model J TP J <CP I. Assume
J . Since concept role names except B minimized, follows
1. xI = xJ = J J model (66);
2. AIt = AJ
Point 1 J model (67);
3. N = N J because, matter B J is, Point 1
(x.y.B u y.x.B)J = .
Thus J model (69), N = N J .
Thus, J differ interpretation concept names B,
varying. contradiction J <CP I.
Conversely, assume CircCP (TP , ) 6|= D(a), let model CircCP (TP , )
aI 6 DI . induction + j, define mapping assigns (i, j)
element (i, j) i, j 0,

N N

751

fiBonatti, Lutz, & Wolter

1. ((i, j), (i + 1, j)) xI ;
2. ((i, j), (i, j + 1)) ;
start, set (0, 0) = aI . `-th step, following:
Select ((0, ` 1), d) put (0, `) := d. exists
since model (66).
Select ((` 1, 0), d) xI put (`, 0) := d. Again,
exists since model (66).
let i, j > 0 + j = `. Since model (66), d, d0
((i 1, j), d) xI ((i, j 1), d0 ) . show = d0 ,
set (i, j) := d.
Assume contrary 6= d0 . (70)(72) since aI
/ DI ,

(i 1, j 1) N . Define new interpretation J obtained
following modifications:
(i 1, j 1) removed N ;
let d0 B J 6 B J ;
let DJ = .
Clearly, J <CP I. obtain contradiction model CircCP (TP , ),
thus remains show J model TP . suffices consider (69) (72),
concept inclusions referring N , B, D. axioms (70) (72) hold
DJ = . show (69), let e . show e C J C concept
right hand side (69). Clearly, e C J since e C , e {x, y, x , }reachable aI . e N e {x, y, x , }-reachable aI ,
otherwise would aI DI axioms (70) (72). Thus, e N J C J
e 6= (i 1, j 1). Finally, (i 1, j 1) C J , definition B J .

N N

define mapping : setting (i, j) := (i, j) . (67),
mapping well-defined. (68), satisfies horizontal vertical matching conditions.
Thus, P solution.

Thus, shown following result.
Theorem 28 ALC, satisfiability w.r.t role-minimizing cKBs undecidable.
6.3 Reasoning Role-minimizing cKBs Empty TBox ALCI
prove undecidability reasoning role-minimizing cKBs empty TBox ALCI.
proof uses spy-point technique (Areces, Blackburn, & Marx, 1999); namely,
show ABoxes simulate TBox reasoning employing inverse roles simulation
nominals circumscription. Using idea proof rather similar proof
Theorem 28.
752

fiThe Complexity Circumscription DLs

Let P instance tiling problem consider cKB CircCP (TP , ) defined
proof Lemma 27. simulate TBox axioms C v C 0 ABox assertions
((C C 0 ) u r0 .(C C 0 ))(a) enforcing role r0 connect relevant points a.
achieved forcing relevant points domain satisfy r0 .{a}. Since
nominals language use concept name instead {a} ensure
behaves like nominal. present details.
sake readability, write concept assertions C(a) form : C
set 1 {r}.C = C u r.C. Let A, B 0 , N 0 fresh concept names r0 fresh role
name occurring TP . AP consists assertions
: 1 {r0 }.(C C 0 ),

(73)

C v C 0 TP ,
: A,

: 1 {r0 }.

u s.r

s=x,y




: 1 {r0 }. N 0 (A u B 0 ) r0 .(A u B 0 ) u
: 1 {r0 }.(N 0 D),


0 .A,

(74)

s.r

s=x,y


0 .(A

u B 0 )



: r0 .D

(75)

(76)

let CP = (, M, , {D, B, B 0 }), consists concept role names distinct
D, B, B 0 .
Lemma 29 CircCP (, AP ) 6|= D(a) iff P solution.
Proof. Assume P solution . Take interpretation proof Lemma 27
expanded
AI = {(0, 0)},

N 0I = ,

B 0I = ,

r0I = {(aI , d) | }.

show model CircCP (, AP ). Clearly, model AP . Thus remains
show model J AP J <CP I. Assume exists J <CP
model AP . minimized (74), AJ = {(0, 0)}. follows axiom
(66) encoded (73) ((0, 0), (1, 0)) xJ ((0, 0), (0, 1)) J . one prove
induction ` > 0 using axiom (66) encoded (73) (74) (i, j)
+ j = `, ((0, 0), (i, j)) r0J ((i, j), (i + 1, j)) xJ , ((i, j), (i, j + 1)) J .
follows xJ = xI J = . Also observe N 0J = because, matter
B 0 interpreted,

J

(A u B 0 ) r0 .(A u B 0 ) u
s.r0 .(A u B 0 )
= .



s=x,y

one prove similarly proof Lemma 27 J differ
interpretation B, B 0 , D, contradiction.
Conversely, suppose model CircCP (, AP ) aI 6 DI . first show
(aI , d) r0I whenever 6= aI {x, y}-reachable aI finite number
steps. Assume case. exist d, d0
753

fiBonatti, Lutz, & Wolter

= aI (aI , d) r0I ,
(d, d0 ) xI (d, d0 ) ,
(a, d0 ) 6 r0I .
(74), exists d00 aI 6= d00 , (d00 , d0 ) r0I , d00 AI . Observe
N 0I (76) aI 6 DI . Define new interpretation J modifying follows:
removed N 0I ;
let aI B 0J d00 6 B 0J ;
let DJ = .
Clearly J <CP I. obtain contradiction thus sufficient show J model
AP . Clearly, assertion AP containing neither D, B 0 , N 0 satisfied J .
remaining assertions except (75), follows DJ = satisfied J .
Finally, (75), observe N 0I {aI } {e | (aI , e) r0I } 6 DI assertions
(76). Thus, definition N 0J , consider point removed N 0I .

J

s.r0 .(A u B 0 )
definition J .
(A u B 0 ) r0 .(A u B 0 ) u



s=x,y

one use assertions (73) construct solution P way
proof Lemma 27.

thus proved following result.
Theorem 30 ALCI, satisfiability w.r.t. role-minimizing cKBs empty TBox undecidable.

7. Related Work
already pointed introduction circumscription one several
possible approaches nonmonotonic DLs that, order achieve decidability,
approaches adopt suitable restriction expressive power DL
component, non-monotonic features, interaction DL nonmonotonic features. section, survey existing approaches, discuss adopted
restrictions, relate DLs circumscription whenever possible. However,
point full-fledged formal comparison different approaches serious research endeavor outside scope paper. main approaches
nonmonotonic DLs (excluding relying integration DLs logic programming) summarized Table 1, n.a. stands analyzed specificity
column states whether formalism equipped priority mechanism based
specificity (i.e., subsumption) concepts.
start two early approaches based circumscription. work Brewka
(1987), frame system given nonmonotonic semantics based circumscription.
focus showing appropriateness proposed semantics, decidability
complexity reasoning analyzed. Cadoli et al. (1990), apply circumscription DL
754

fiThe Complexity Circumscription DLs

Ref
(Brewka, 1987)
(Cadoli, Donini, &
Schaerf, 1990)
(Padgham & Zhang,
1993)
(Straccia, 1993)
(Baader & Hollunder, 1995a)
(Baader & Hollunder, 1995b)
(Lambrix,
Shahmehri, & Wahlloef,
1998)
(Donini et al., 1997)

DL
frame lang.
< ALE

NM features
Circ
Circ

Complexity
n.a.
p2

Specificity

N

AL concrete domains
ALC

inheritance
networks
prioritized
default logic
default logic

n.a.



decidable



decidable

N

prioritized
default logic
prioritized
default logic

decidable



n.a.



MKNF
restrictions
MKNF
restrictions
maximized
typicality

depends DL

N

3-ExpTime

N

co-NExpNP

N

ALCF
ALC
ALQO+feature
agreement
decidable DL

(Donini et al., 2002)

ALC

(Giordano, Gliozzi,
Olivetti, & Pozzato,
2008)

ALC

Table 1: approaches nonmonotonic DLs

way here. authors consider non-prioritized circumscription
apply fragment description logic ALE. Decidability reasoning shown
reduction propositional reasoning Extended Closed World Assumption
(ECWA), p2 . best knowledge, first effective reasoning
method nonmonotonic description logic.
another early approach Padgham Zhang (1993), nonmonotonic DL obtained adaptation inheritance networks approach (Horty, 1994)
underlying DL essentially AL extended concrete data values. main contribution definition formalism discussion applications, decidability
complexity analyzed.
recent approach similar spirit circumscription taken Giordano
et al. (2008). authors extend ALC modal operator representing typicality,
maximize extension achieve nonmonotonic inferences. Decidability proved via
tableau algorithm also establishes co-NExpNP upper bound subsumption.
lower bound given.
turn approaches based default logic (Reiter, 1980). nonmonotonic
DLs Baader Hollunder (1995a, 1995b), Straccia (1993), Lambrix et al. (1998)
share common restriction: default rules applied individual
name, is, denoted individual constant occurs explicitly
knowledge base. restriction motivated observation that, defaults also
applied implicit (anonymous) individuals, reasoning becomes undecidable (Baader &
755

fiBonatti, Lutz, & Wolter

Hollunder, 1995a). Since models DL knowledge bases usually include large number
anonymous individuals due existential quantification, restriction introduces
strong asymmetry treatment individuals.
Another line nonmonotonic DLs (Donini et al., 1998, 1997, 2002) based first-order
autoepistemic logics whose interpretation domains restricted fixed denumerable
set constants. use unique domain resolves several issues related quantification
modal logics (such whether Barcan formula hold, whether different
worlds Kripke structure allowed different domains). also
avoids asymmetry approaches based default logic because, definition,
individuals name. side coin domains finite varying
cardinality knowledge bases satisfy unique name assumption
dealt using rather technical encodings (such explicit axiomatization
finite domain represented concept name D).
first paper mentioned (Donini et al., 1998), decidability results apply
monotonic knowledge bases4 autoepistemic operator used nonmonotonic
fashion queries. restriction lifted subsequent publications.
make use logic minimal knowledge negation failure (MKNF),
equipped two (auto)epistemic operators K (Donini et al., 1997, 2002).
former paper (Donini et al., 1997), underlying monotonic fragment
description logic decidable instance checking problem. price payed
generality decidability results apply so-called simple knowledge bases,
quantifying-in (that is, quantification across modal operators, R.K C) allowed.
Nonetheless, KBs expressive enough model default rules. different restriction
explored Donini et al. (2002). underlying DL restricted ALC limited forms
quantifying-in allowed, so-called subjectively quantified ALCKN F knowledge bases.
Decidability results obtained subclass simple subjectively quantified knowledge
bases, whose nonmonotonic part restricted inclusions form KC v
> v C inferred knowledge base. restriction incompatible
traditional embedding (priority-free) circumscription autoepistemic logic,
based prerequisite-free default rules would equivalent inclusions
form K> v C.
recent line research integrating DLs logic programming rules introduces
nonmonotonic extensions DLs based negation-as-failure. approaches
(Eiter et al., 2004) use loosely coupled integration logic programs DLs,
interpretations DL component restricted logic program variables
range named DL individuals. Thus, approaches somewhat similar
classical extensions DLs based defaults nonmonotonic inferences
limited named individuals. recent approach (Motik & Rosati, 2007) based
MKNF shares MKNF-DLs discussed pros cons adopting fixed
denumerable domain. complexity reasoning underlying DL C 6 NP,
data complexity entailment bounded C C . Finally, mention 3-valued variant
approach (Knorr, Alferes, & Hitzler, 2007) based well-founded semantics.
4. autoepistemic operator used restricted contexts suffice encode so-called
procedural rules, monotonic.

756

fiThe Complexity Circumscription DLs

ALC

Concept circ.
Minim. roles
Fixed roles

#M n, #F n
(unrestricted)
TBox=
TBox6=

ALCQO
ALCI ALCIO
NExp
NP
NExpNP even = , either TBox= ABox=
NExpNP even #M 1, #F 0
Undecidable
Undecidable
Highly undecidable, even TBox= , =

Table 2: Summary complexity results satisfiability w.r.t. cKBs
common limitation nonmonotonic extensions DLs based MKNF
provide support specificity priorities. particular, defeasible inheritance
mentioned expressiveness analysis autoepistemic approaches (Donini et al.,
1997, 2002) appear goal MKNF-based approach. pointed
introduction, well-known that, propositional case, nonmonotonic logics
cannot modularly encode specificity-based priorities needed inheritance
mechanisms (Horty, 1994).

8. Conclusions Perspectives
shown circumscription provides elegant approach defining nonmonotonic
DLs, resulting formalisms appealing expressive power decidable
appropriate restrictions adopted. main restriction, leads rather
robust decidability results, concept names minimized fixed whereas
role names vary. empty TBoxes, decidability retained roles allowed
minimized, fixed. decidability complexity results obtained
paper listed detail Table 2. results Section 3, bounds
TBox 6= apply general acyclic TBoxes.
view paper promising step towards establishing circumscribed DLs
major family nonmonotonic DLs used practical applications. reach goal,
additional research topics addressed, mention two. First,
algorithms presented paper based massive non-deterministic guessing
thus admit efficient implementation. Ideally, one would like algorithms
well-behaved extensions tableau algorithms underly state-of-the-art
DL reasoners (Baader & Sattler, 2000). crucial issue whether sophisticated optimization techniques implemented reasoners (tableaux caching, dependency-directed
backtracking etc.; cf. Baader et al., 2003, Chap. 9) adapted circumscribed DLs.
first steps made Grimm Hitzler (2008). Second, seems necessary
develop design methodology modelling defeasible inheritance. examples given
paper indicate main challenge find rules thumb determine
predicates fixed, varied, minimized. may appealing hide abnormality
predicates behind explicit syntax defeasible inclusions, trade generality
simplicity usability.
Also theoretical perspective, initial investigation leave open least
interesting questions. example, current techniques limited circumscribed DLs
757

fiBonatti, Lutz, & Wolter

finite model property. would desirable overcome limitation
obtain decidability results even expressive DLs SHIQ OWL. also
possible follow opposite approach consider circumscribed versions inexpressive
DLs EL DL-Lite family (Baader, Brandt, & Lutz, 2005a; Calvanese,
Giacomo, Lembo, Lenzerini, & Rosati, 2007), currently popular large
number applications. First steps made Bonatti, Faella, Sauro (2009),
investigated circumscribed versions EL DL-lite.
Finally, worth mentioning complexity results circumscription
used prove complexity bounds other, seemingly unrelated, reasoning problems
description logic. example, certain reasoning services conservative extensions
modularity description logic satisfiability problem w.r.t. concept-circumscribed
knowledge bases mutually reducible polynomial time (Konev, Lutz,
Walther, & Wolter, 2008). many problems known NExpNP complete, circumscription thus provides new class problems potentially
employed give NExpNP lower bound proofs.

Acknowledgments
first author partially supported network excellence REWERSE, IST-2004506779. third author partially supported UK EPSRC grant no. GR/S63182/01.

Appendix A. Missing Proofs Section 3
Lemma 5. ALC, satisfiability w.r.t. (concept-)circumscribed KBs empty TBox
without fixed roles polynomially reduced satisfiability w.r.t. (concept-)circumscribed KBs empty TBox without fixed predicates.
Proof. proof Lemma 4, used TBox axioms state fresh
concept names interpreted complement fixed concept names. general,
cannot done using ABox assertions only. Instead, construct ABox assertions
force case objects relevant truth given ABox.
care required using ABox assertions polynomial size. first part
proof deals problem. second part straighforward modification
proof Lemma 4.
first part proof consists introducing notation proving central
technical claim. w = r1 rn NR , interpretation I, d, e , write

(d, e) wI iff d0 , . . . , dn = d0 , e = dn , (di , di+1 ) ri+1

< n.
Let N set individual names Paths mapping N powerset NR .
interpretation well-behaved mapping Paths every ,
N w Paths(a) (aI , d) wI . ALC-concept C, associate
set P(C) pairs (w, D) w NR subconcept C follows:
C NC , P(C) = {(, C)};
C = D, P(C) = {(, C)} P(D);
758

fiThe Complexity Circumscription DLs

C = D1 u D2 C = D1 D2 , P(C) = {(, C)} P(D1 ) P(D2 );
C = r.D C = r.D, P(C) = {(, C)} {(rw, E) | (w, E) P(D)}.
set ABox assertions individual name a, use P(S, a) denote set

C(a)S P(C). write Paths(S, a) {w | : (w, D) P(S, a)}. formulate
announced claim.
Claim 1. Suppose CircCP (, A) 6|= C0 (a0 ), CP contain fixed role names
C0 (a) formulated ALC. Let = {C0 (a0 )} let N set individual
names S. Let 0 restriction that, N
w Paths(S, a), (aI , d) wI . 0 model CircCP (, A)
C0 (a0 ) well-behaved mapping Paths(a) = Paths(S, a), N .
prove claim. Let model CircCP (, A) aI0 (C0 )I . Note
0
N , Paths(S, a) thus aI . Clearly, 0 well-behaved
Paths. One prove induction C
0

() N , (w, C) P(S, a), (aI , d) wI , C iff
0
CI .
show case C = r.D, leave cases reader. Let C .
e DI (d, e) rI . Since (w, C) P(S, a), (wr, D) P(S, a).
0
Since (aI , d) wI , (aI , e) (wr)I . Thus, e induction hypothesis
0
0
0
yields e DI . definition 0 semantics, C . let C .
0
0
e DI (d, e) rI . definition 0 , (d, e) rI . Since (w, C) P(S, a),
(wr, D) P(S, a). Since (aI , d) wI , (aI , e) (wr)I . Thus, induction
hypothesis yields e DI .
Thus, () established. Using () facts aI0 (C0 )I model
0
0
A, hard verify aI0 (C0 )I 0 model A. show
0 also model CircCP (, A). Assume contrary model J 0
J 0 <CP 0 . Define interpretation J follows:
J = ;
AJ = AI F ;
0

AJ = AJ V ;
0

rJ = rJ r NC ;
bJ = bI b NI .
Using assumption CP contain fixed role names, hard verify
J <CP I. obtain contradiction fact model CircCP (, A),
thus remains show J model A. end, prove induction C
0

() N , (w, C) P(S, a), J (aJ , d) wJ , C J iff
0
CJ .
Since induction step proof (), induction start. Thus, let
C = NC . V , done definition J . let F . Since 0
759

fiBonatti, Lutz, & Wolter

0

0

0

0

restriction AJ = AI , definition J yields AJ J = AJ , required.
finishes proof claim.
prove Lemma 5, consider instance checking instead satisfiability. Since
provided polynomial reductions satisfiability (non)-instance checking vice
versa Section 2, nevertheless obtain desired result. Let CircCP (, A) cKB
CP = (, M, F, V ) F = {A1 , . . . , Ak }. Take concept assertion C0 (a0 ). Let
= {C0 (a0 )} N set individual names S. Define
0 = {A1 , . . . , Ak , A01 , . . . , A0k }, A0i fresh concept names;
A0 = {w.(A0i Ai )(a) | w Paths(S, a), N , k};
CP0 = (, 0 , , V ).
CircCP (, A) |= C0 (a0 ) iff CircCP0 (, A0 ) |= C0 (a0 ) follows immediately Claim 1
fact Paths(S, a) = Paths(S 0 , a), 0 = A0 C0 (a0 ).

Lemma 6 C0 satisfiable w.r.t. CircCP (T , A) iff C0 u B 0 satisfiable w.r.t. CircCP0 (T 0 , A).
Proof. Suppose model C0 CircCP (T , A). Expand interpretation 0
setting
0
0
0
0
0
AI = B 0I = , B = A0I = , uI = .
Clearly, 0 model C0 u B 0 (T 0 , A). show 0 model CircCP0 (T 0 , A).
Assume contrary model J (T 0 , A) J <CP0 0 . A0J = ,
AJ = , B J = , B 0 J = . Since u varying J <CP0 0 , thus easy
show J <CP I. contradicts fact model CircCP (T , A).
Conversely, suppose model C0 u B 0 CircCP0 (T 0 , A). show also
model C0 CircCP (T , A). First observe AI = . suppose
case. Define new interpretation J way except uJ = ,
B J = , B 0J = . J <CP0 (since B 0I 6= ) J model (T 0 , A). Thus
derived contradiction. follows C = hence model (T , A)
C0 . remains show J <CP J model (T , A).
Assume J exists. C J = . Define model J 0 expanding J follows:
0

0

AJ = B 0J = ,

0

0

B J = A0J = ,

0

uJ = .

Note A, B, A0 , B 0 , u interpreted way, J 0 <CP0 I.
Moreover, J 0 model (T 0 , A). derived contradiction.

Lemma 8 L DL, simultaneous satisfiability w.r.t. (concept) circumscribed KBs
CircCP1 (T1 , A1 ), . . . CircCPk (Tk , Ak ), CircCPi (Ti , Ai ) CircCPj (Tj , Aj ) share
role names 1 < j k, reduced satisfiability w.r.t. single (concept)
circumscribed KBs polynomial time.
Proof. suffices reduce simultaneous satisfiability without shared role names complement instance checking w.r.t. single cKBs. generalization straightforward,
give proof case k = 2. Thus, let L DL CircCP1 (T1 , A1 ), CircCP2 (T2 , A2 )
760

fiThe Complexity Circumscription DLs

cKBs formulated L share role names, C0 L-concept. Moreover, let
A0 , . . . , An1 concept names shared two cKBs, R role names used
least one two cKBs together fresh role name r0 , CPi = (i , Mi , Fi , Vi )
{1, 2}. obtain new TBox T20 T2 replacing concept name Ai , < n,
new concept name A0i . Let A02 obtained A2 CP02 = (02 , M20 , F20 , V20 )
CP2 way. Define TBox follows, P fresh concept name:
Ai u A0i v P < n
Ai u A0i v P < n
P
r.P

v r.P r R
v P r R

set:


:= T1 T20

:= A1 A02 {r0 (b1 , b2 ) | b1 , b2 occur A1 A2 T1 T2 }
:= 1 02


:= M1 M20

F

:= F1 F20

V

:= V1 V20 {P, r0 }

CP := (, M, F, V )
Let a0 individual name A1 (clearly, may assume A1 6= ). remains
show following:
Claim. C0 simultaneously satisfiable w.r.t. CircCP1 (T1 , A1 ) CircCP2 (T2 , A2 ) iff a0
instance (P u r0 .C0 ) w.r.t. CircCP (T , A).
If. Assume a0 instance (P u r0 .C0 ) w.r.t. CircCP (T , A).
model CircSCP (T , A) aI0 (P u r0 .C0 )I . call connected directed
graph GI = (I , rR rI (r )I ) connected. connected component 0
0
0
restriction domain (maximal) connected component GI .
may assume without loss generality chosen model connected:
not, use role r0 ensures connected component 0
contains bI individual names b A1 A2 T1 T2 . easy see 0
0
0
model CircCP (T , A) aI0 (P u r0 .C0 )I , thus may simply replace 0 .
show C0 simultaneously satisfiable respect CircCP1 (T1 , A1 )
CircCP2 (T1 , A2 ). Clearly, model C0 . construction CircCP (T , A),
model T1 , A1 . show also model CircCP1 (T1 , A1 ), assume
contrary case. exists model J T1 A1
J <CP1 I. Define model J 0 follows:
0

J = J ;
predicates used T1 A1 interpreted J ;
predicates used T20 A02 interpreted I.
761

fiBonatti, Lutz, & Wolter

(
P

J0

:=

0



((Ai u A0i ) (Ai u A0i ))J 6= < n



otherwise

0

0

r0J = J J .
readily checked J 0 model A, J 0 <CP I. Thus,
derived contradiction fact model CircCP (T , A), follows
model CircCP (T1 , A1 ).
remains show model CircCP (T2 , A2 ). Since connected, model
, satisfies aI0
/ P , AIi = (A0i )I < n. Therefore, also
model T2 A2 . Analogously case CircCP1 (T1 , A1 ), show
model CircCP2 (T2 , A2 ).
if. Assume C0 simultaneously satisfiable w.r.t. cKBs CircCP1 (T1 , A1 )
CircCP2 (T2 , A2 ). exists model C0 model CircCP1 (T1 , A1 )
CircCP2 (T2 , A2 ). modify new model 0 setting
0

(A0i )I := AIi < n;
0

P := ;
0

r0I := .
0

0

easy see 0 model A, aI (P u r0 .C0 )I . show
a0 instance (P u r0 .C0 ) w.r.t. CircCP (T , A), thus remains prove
0 also model CircCP (T , A). this, first show following:
(a) 0 model CircCP1 (T1 , A1 ). case since model J T1 A1
J <CP1 0 satisfies J <CP1 I. Thus, existence model contradicts
fact model CircCP1 (T1 , A1 ).
(b) 0 model CircCP02 (T20 , A02 ). Assume contrary model J T20

:= (A0i )J
A02 J <CP02 0 . Convert J interpretation J setting AJ



< n. Then, J model T2 A2 satisfies J <CP2 I.
contradiction fact model CircCP2 (T2 , A2 ).
Now, assume contrary shown model J 0
J 0 <CP 0 . definition CP, J 0 <CP 0 implies either J 0 <CP1 0
J 0 <CP02 0 hold. Since J 0 clearly satisfies T1 , A1 , T20 , A02 , obtain contradiction
(a) (b).


Appendix B. Missing Proofs Section 4
Lemma 14 G yes-instance co-3CERTCOLS iff CG satisfiable w.r.t. CircCPG (, AG ),
CPG = (, M, F, V ) = , = {Root, Leaf, Clash},
F = {LeafFix, Tr, X0 , . . . , Xn1 , Y0 , . . . , Yn1 , },
V set remaining predicates AG .
762

fiThe Complexity Circumscription DLs

Proof. If. Suppose CG satisfiable w.r.t. CircCPG (, AG ) let model
6= . show G yes-instance co-CERT3COL .
CircCPG (, AG ) CG

start, show
I.
(I) aI0 CG
6= aI .
Assume contrary case. CG
0
, RootI . Let J interpretation obtained setting
Since CG
RootJ = {aI0 }. definition AG , aI0 RootI , thus J CPG I. Moreover,
easily seen J model AG . thus established contradiction
fact model CircCPG (, AG ), proves (I).
Lines (1)-(4) (Fig. 2), elements di,w (the nodes tree) 2n
w {0, 1}i

aI0 = d0, ;
di,w XjI iff j + 1-st bit w 1, 2n j min{i, n 1};
di,w YjI iff n + j + 1-st bit w 1, i, j n < 2n j n;
(di,w , di+1,w0 ), (di,w , di+1,w1 ) rI , < 2n;
d2n,w Leaf , w {0, 1}2n .
i, j < 2n , use `i,j denote element d2n,w w {0, 1}2n denotes
binary encoding followed j. show
(II) Leaf = {`i,j | i, j < 2n }.
Assume contrary case, i.e., Leaf
6= `i,j i, j < 2n . Let id , jd > 0 integers truth values X0 , . . . , Xn1
encode id truth values Y0 , . . . , Yn1 encode jd . Starting I, construct
interpretation J setting
Leaf J
rJ

:= Leaf \ {d}
:= (rI \ (I d)) {(e, `id ,jd ) | (e, d) rI }

modify J J 0 setting
PJ

0

:= (r2n .var1.Leaf)J
(r2n .var2.Leaf)J
(r2n .col1.Leaf)J
(r2n .col2.Leaf)J

going Lines (1) (26), straightforward check J 0 model AG .
Moreover, clearly J 0 <CPG I, thus obtain contradiction fact
model CircCPG (, AG ). thus shown (II).
following easy consequence (I), fact P conjunct CG ,
Lines (8), (9), (20), (21):
(III) (`i,j , d) rI implies Leaf i, j < 2n , , r {var1, var2, col1, col2}.
763

fiBonatti, Lutz, & Wolter

suppose contrary aim prove G yes-instance. Then,
truth assignments t, subgraph t(G) 3-colorable. particular, holds
assignment defined setting, i, j < 2n ,

t(Vij ) := 1 iff `i,j TrI .

Let c : {0, . . . , 2n 1} {R, G, B} 3-coloring t(G) construct interpretation
J starting applying following modifications:

CJ
ClashJ

= {`i,0 | < 2n , c(i) = C} C {R, G, B}
= .

Clearly, J <CPG I: minimized predicate Clash empty J , non-empty since
non-empty. obtain contradiction, thus suffices show J model
CG
AG .
Since J agree predicates R, G, B, Clash, inclusions
mention concepts satisfied J . Lines (1) (21). Lines (22) (23)
2n J
satisfied construction J since, due Line (5) (II), (aJ
0 , d) (r )
n
implies = `i,j i, j < 2 . thus remains consider Lines (24) (26). first
show
(IV) i, j < 2n , (i, j) edge t(G) iff `i,j
/ ElimJ .
Let i, j < 2n let potential edge (i, j) labeled Vk1 ,k2 Vk3 ,k4 (since
(i)
(i)
circuits cS cj deliver output input, assume also potential,
non-existing edges label). (II) (III) together Lines (8) (9),
var1J = var1I = {`k1 ,k2 } var2J = var2I = {`k3 ,k4 }. Thus, definition together
Lines (12) (16) yields (i, j) edge t(G) iff `i,j
/ ElimI . prove (IV),
remains note J interpret concept name Elim way.
2n J
Now, prove (24) (26) satisfied J . Let (aJ
0 , d) (r ) . Line 5
J
n
(II), = `i,j i, j < 2 . Let `i,j
/ Elim . (IV) since c 3-coloring
t(G), get c(i) 6= c(j). Thus, construction J , `i,0
/ C J `j,0
/ C J
C {R, G, B}. (II) (III) together Lines (18) (19), col1J = col1I = {`i,0 }
col2J = col2I = {`j,0 }. Therefore, `i,j
/ (col1.C u col2.C)J C {R, G, B}.
J
Since holds `i,j
/ Elim , preconditions implications Lines (24)
(26) never satisfied. Thus, implications satisfied.

if. Suppose G yes-instance let truth assignment
t(G) 3-colorable. Let c : {0, . . . , 2n 1} {R, G, B} color assignment
minimizes (w.r.t. set inclusion) set {(i, j) | (i, j) edge G c(i) = c(j)}. Define
interpretation follows (here following, distinguish
number binary encoding string):
764

fiThe Complexity Circumscription DLs


RootI

= {di,w | 2n , w {0, 1}i }
= {d0, }



= {d2n,w | w {0, 1}2n }

LeafFixI

= {d2n,w | w {0, 1}2n }

Leaf

XjI

= {di,w | j + 1-st bit w 1, 2n, j min{i, n 1}};

YjI


= {di,w | n + j + 1-st bit w 1, n < 2n, j n};

Tr

= {d2n,ij | t(Vij ) = 1}

TrI`

= {d2n,ij | t(Vij ) cS (i, j)}

(`)

(` = 1, 2)

Elim



= {d2n,ij | (i, j) edge t(G)}

C



= {d2n,i0 | c(i) = C} (C = R, G, B)



= {d2n,ij | (i, j) edge t(G) c(i) = c(j)}

r



= {(di,w , di+1,w0 ), (di,w , di+1,w1 ) | < 2n}

var1



= {(d2n,ij , d2n,kl ) | first variable label (i, j) Vkl }

var2



= {(d2n,ij , d2n,kl ) | second variable label (i, j) Vkl }



col1

= {(d2n,ij , d2n,i0 ) | < 2n }

col2I

= {(d2n,ij , d2n,j0 ) | < 2n }

Clash

PI

=

aI0

= d0, .

Boolean circuit c, corresponding output concept name Outc interpreted
OutIc := {d2n,ij | i, j < 2n c(i, j) true}.

show CG satisfiable w.r.t. CircCPG (, AG ), suffices show aI0 CG
model CircCPG (, AG ). former easy: recall CG = RootuP ur2n .Clash.
definition I, aI0 (Root u P )I . Since c 3-coloring, aI0 (r2n .Clash)I .
thus remains show model CircCPG (, AG ). Since easy verify
model AG , boils showing model J AG J <CPG I.
Assume contrary J . Lines (1)(5) since J model
AG , #Leaf J 22n . Since #Leaf = 22n J <CPG I, get Leaf J = Leaf .
similar simpler reasons, RootJ = RootI . Thus, J <CPG implies ClashJ ( ClashI .
Lines (1) (5) since J models AG Leaf J = Leaf #Leaf =
#Leaf J = 22n ,

J
2n J
(I) {d | (aI0 , d) (r2n )I } = {d J | (aJ
0 , d) (r ) } = Leaf = Leaf

Thus, Lines (24) (26) fact J model AG ensure

(II) C{R,G,B} (Leaf u Elim u col1.C u col2.C)J ClashJ .
Define coloring c0 setting
c0 (i) = C iff d2n,i0 C J
Suppose
765

(C = R, G, B).

fiBonatti, Lutz, & Wolter

(III) (a) ElimI Leaf = ElimJ Leaf ,
(b) col1I (Leaf Leaf ) = col1J (Leaf J Leaf J ),
(c) col2I (Leaf Leaf ) = col2J (Leaf J Leaf J ).
(II) guarantees (i, j) edge G c0 (i) = c0 (j), d2n,ij ClashJ .
Since ClashJ ( ClashI , get
1. c0 (i) = c0 (j), c(i) = c(j);
2. i, j c0 (i) 6= c0 (j), c(i) = c(j).
contradicts initial minimality assumption coloring c.
thus remains prove (III). start (a). Assume
(d) var1I (Leaf Leaf ) = var1J (Leaf J Leaf J )
(e) var2I (Leaf Leaf ) = var2J (Leaf J Leaf J ).
Then, Lines (12) (16) together (I) fact TrI = TrJ implies (a). thus
remains prove (b) (e). concentrate (b) cases analogous. Take
(d, d0 ) col1I (Leaf Leaf ). = d2n,ij d0 = d2n,i0 j 0 i, j, i0 , j 0 < 2n .
Line (18), i0 = j 0 = 0. (I) Lines (17) (18) since J
agree interpretation X0 , . . . , Xn1 , Y0 , . . . , Yn1 , e LeafFixJ
(d2n,ij , e) col1J , value encoded X0 , . . . , Xn1 e J i, value
encoded Y0 , . . . , Yn1 e J 0. Since LeafFixI = LeafFixJ , LeafFixJ =
Leaf J . However, single element Leaf J X0 , . . . , Xn1 encodes
Y0 , . . . , Yn1 encodes 0 d2n,i0 = d0 . converse direction analogous.

Corollary 16 ALC, satisfiability w.r.t. concept-circumscribed KBs NExpNP -hard
even TBox acyclic, ABox preference relations empty,
fixed predicates.
Proof. ABox AG reduction given Section 4.2.1 form {C0 (a0 )}
circumscription pattern CPG empty preference relation. thus suffices
show polynomial reduction satisfiability w.r.t. concept-circumscribed KBs
form satisfiability w.r.t. concept-circumscribed KBs acyclic TBox, empty
ABox preference relation, fixed predicates.
Let CircCP (, A) concept-circumscribed KB = {C0 (a0 )} CP = (
, M, V, F ) = , let C ALC concept. Define = {A v u.C0 },
concept name occur C u role name
occur C. Also define CP0 = (, M, V {u}, F {A}). C satisfiable w.r.t.
CircCP (, A) iff u C satisfiable w.r.t. CircCP0 (T , ):
If. u C satisfiable w.r.t. CircCP0 (T , ), model CircCP0 (T , )
d0 (A u C)I . Thus, e0 C0I . Modify obtain new interpretation
J setting aJ
0 = e0 . Clearly, J model C. show also model
CircCP (, A), assume contrary model J 0 J 0 <CP J . Modify
0
0
J 0 interpretation 0 setting AI = AI uI = . readily checked
0 model 0 <CP0 I, thus obtained contradiction fact
model CircCP0 (T , ).
766

fiThe Complexity Circumscription DLs

if. C satisfiable w.r.t. CircCP (, A), model CircCP (, A)
d0 C . Let J defined I, except AJ = {d0 } uJ = . Clearly,
J model u C. show J also model CircCP (T , ), assume
0
contrary model J 0 J 0 <CP0 J . Since fixed CP0 , d0 AI ,
0
0
thus d0 (u.C0 )J e0 C0J . Modify J 0 new interpretation 0
0
setting aI0 = e0 . readily checked 0 model 0 <CP I, thus
obtained contradiction fact model CircCP0 (, A). get rid fixed
predicates, suffices apply Lemma 5.


Appendix C. Missing Proofs Section 5
show semantic consequence problem reduced instance checking
w.r.t. role-fixing cKBs ALC. already proved ALC extended
universal role. fact, remaining problem approximate concepts u.C using ALC concepts state extension C contains points within certain,
sufficiently large neighbourhood.
construct approximation, first introduce local version notion
frame validity concepts. pointed R-frame pair (F, d) F F
R-frame. concept C valid pointed R-frame (F, d), symbols (F, d) |= C, iff
C every interpretation based F. R finite set role names,
R.C denotes C = 0,

N

m1 R.C u

u r.

m1

rR

R.C

> 0. follows, use concepts form R.C approximations
u.C. remind reader correspondence results modal logic. Let transA =
s.A s.s.A contA = s.A r.A. well known (Blackburn & van
Benthem, 2007) easy prove every {r, s}-frame F, following holds:
transA valid F if, if, sF transitive;
contA valid F if, if, rF sF .
Say d0 F {r, s}-reachable F (d, d0 ) (rF sF ) call F
root F every d0 F {r, s}-reachable F. F {r, s}-frame root
d, following conditions equivalent:
1 {r, s}.(transA u contA ) valid (F, d);
sF transitive rF sF .
observations used proof Lemma 31 below. before, sometimes write
concept assertions C(a) form : C. Recall role depth rd(C) concept C
defined nesting depth constructors r.D r.D, r R, C.
Lemma 31 Let C ALC {r} -concepts sharing concept names let
fresh concept name. Let CP = (, M, {r, s}, V ) circumscription pattern,
consists concept names C V consists concept names D. Let
individual name. following conditions equivalent:
767

fiBonatti, Lutz, & Wolter

1. {r, s}-frames F rF sF sF transitive: F |= C, F |= D;
2. pointed {r, s}-frames (F, d):
(F, d) |= 1 {r, s}.(transA u contA u C)



(F, d) |=

3. instance 1 {r, s}.(transA u contA u C) w.r.t. CircCP (, A),
= {a : (1 {r, s}.(transA u contA u C) 1+max {2,rd(C)} {r, s}.

u B)}.

BM

Proof. Point 1 implies Point 2. Suppose Point 2 hold. Let (F, d) pointed
{r, s}-frame (F, d) |= 1 {r, s}.(transA u contA u C) (F, d) 6|= D. may
assume root F. (F, d) |= 1 {r, s}.(transA u contA ) obtain rF sF
sF transitive. Therefore, (F, d) |= 1 {s}.C obtain F |= C. follows F
frame refuting Point 1.
follows use, every , d,I denote set e
{r, s}-reachable 1 + max {2, rd(C)} steps.
Point 2 implies Point 3. Suppose Point 3 hold. Let model CircCP (, A)

aI (1 {r, s}.(transA u contA u C) u D)I .

(77)

Let based F set := aI . show (F, d) |= 1 {r, s}.(transA u contA u C)
(F, d) 6|= D. latter easy witnessed interpretation I. show former,
let J interpretation based F. show (1 {r, s}.(transA u contA u C))J .
(77) since model CircCP (, A), aI (1+max {2,rd(C)} {r, s}.
B)I .

u

BM

follows immediately
B = d,I ,

(78)

B . distinguish two cases:
B J d,I , B . Since J based frame concept
names contA , transA , C , truth 1 {r, s}.(transA u contA u C)
depends truth value concept names d,I only. (78)
obtain B d,I = B J d,I , B . Hence, (77), (1 {r, s}.(transA u
contA u C))J , required.
B J 6 d,I , least one B .
0

Let J 0 modification J B J = B J d,I , B . (78), J 0 <CP
0
I. (1 {r, s}.(transA u contA u C))J , J 0 model
contradiction fact model CircCP (, A). Thus, (1 {r, s}.(transA u
0
contA u C))J . Since, again, truth 1 {r, s}.(transA u contA u C) depends
truth value B d,I only, (1 {r, s}.(transA u contA u C))J ,
required.
768

fiThe Complexity Circumscription DLs

Point 3 implies Point 1. Suppose Point 1 hold. Consider frame F sF
transitive, rF sF , F |= C, F 6|= D. follows F |= transA u contA . Let
interpretation based F (D)I . may assume root F.
may also assume B = d,I B (since B occurs D) aI = d.
aI (1 {r, s}.(transA u contA u C) u D))I model A. remains show
exist 0 <CP
0

aI (1 {r, s}.(transA u contA u C) 1+max {2,rd(C)} {r, s}.

u B)

I0

BM

.

straightforward: (F, d) |= 1 {r, s}.C, obtain exist
0
0
0 (1 {r, s}.C)I clearly exist B B B
0
(1+max {2,rd(C)} {r, s}.B)I .

position prove reduction ALC.
Theorem 22 logical consequence problem MSO(r) effectively reducible
instance problem w.r.t. role-fixing cKBs formulated ALC. even holds
TBox preference relation empty.
Proof. Theorem 20, logical consequence problem MSO(r) effectively reducible
modal consequence problem ALC {r} -concepts. Hence, suffices reduce
modal consequence problem ALC {r} -concepts. Let ALC {r} -concepts C given.
may assume C concept names common (if have, replace
every concept name B new concept name B 0 denote resulting concept
D0 ; noted above, C iff C D0 .) Let CP = (, M, {s, r}, V ) = ,
consists concept names C, V consists concept names D. Let
= {a : (1 {r, s}.(transA u contA u C) 1+max {2,rd(C)} {r, s}.

u B)}

BM

C0 = 1 {r, s}.(transA u contA u C) D. equivalence Point 1 Point 3
Lemma 31, CircCP (, A) |= C0 (a) if, if, frames F rF sF sF
transitive, F |= C follows F |= D. C contain s,
CircCP (, A) |= C0 (a)



C D.


References
Areces, C., Blackburn, P., & Marx, M. (1999). road-map complexity hybrid logics.
Proceedings Eighth Annual Conference EACSL (CSL99), No. 1683
Lecture Notes Computer Science, pp. 307321. Springer-Verlag.
Areces, C., Blackburn, P., & Marx, M. (2000). computational complexity hybrid
temporal logics. Logic Journal IGPL, 8 (5), 653679.
Baader, F., Brandt, S., & Lutz, C. (2005a). Pushing EL envelope. Kaelbling, L. P.,
& Saffiotti, A. (Eds.), Proceedings Nineteenth International Joint Conference
Artificial Intelligence (IJCAI05), pp. 364369. Professional Book Center.
769

fiBonatti, Lutz, & Wolter

Baader, F., Milicic, M., Lutz, C., Sattler, U., & Wolter, F. (2005b). Integrating description logics action formalisms reasoning web services. LTCSreport LTCS-05-02, Chair Automata Theory, Institute Theoretical Computer Science, Dresden University Technology, Germany. See http://lat.inf.tudresden.de/research/reports.html.
Baader, F., & Hollunder, B. (1995a). Embedding defaults terminological knowledge
representation formalisms.. Journal Automated Reasoning, 14 (1), 149180.
Baader, F., & Hollunder, B. (1995b). Priorities defaults prerequisites, application treating specificity terminological default logic.. Journal Automated
Reasoning, 15 (1), 4168.
Baader, F., McGuiness, D. L., Nardi, D., & Patel-Schneider, P. (2003). Description
Logic Handbook: Theory, implementation applications. Cambridge University
Press.
Baader, F., & Sattler, U. (2000). Tableau algorithms description logics. Dyckhoff,
R. (Ed.), Proceedings International Conference Automated Reasoning
Tableaux Related Methods (Tableaux2000), Vol. 1847 Lecture Notes Artificial
Intelligence, pp. 118. Springer-Verlag.
Berger, R. (1966). undecidability dominoe problem. Memoirs American
Mathematical Society, 66.
Blackburn, P., & van Benthem, J. (2007). Modal logic: semantic perspective. Handbook
Modal Logic. Elsevier.
Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal Logic. Cambridge University
Press.
Bonatti, P., Faella, M., & Sauro, L. (2009). Defeasible inclusions low-complexity DLs:
Preliminary notes. Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI09). AAAI Press.
Bonatti, P., Lutz, C., & Wolter, F. (2006). Expressive non-monotonic description logics
based circumscription. Proceedings Tenth International Conference
Principles Knowledge Representation Reasoning (KR06), pp. 400410. AAAI
Press.
Bonatti, P. A., & Eiter, T. (1996). Querying disjunctive databases nonmonotonic
logics.. Theoretical Computer Science, 160 (1&2), 321363.
Bonatti, P. A., & Samarati, P. (2003). Logics authorization security. Logics
Emerging Applications Databases, pp. 277323. Springer-Verlag.
Borgida, A. (1996). relative expressiveness description logics predicate logics.
Artificial Intelligence, 82 (1 - 2), 353367.
Brewka, G. (1987). logic inheritance frame systems. Proceedings 10th
International Joint Conference Artificial Intelligence (IJCAI87), pp. 483488. Morgan Kaufmann.
770

fiThe Complexity Circumscription DLs

Brewka, G. (1994). Adding priorities specificity default logic.. Proceedings
Logics Artificial Intelligence (JELIA94), Vol. 838 Lecture Notes Computer
Science, pp. 247260. Springer-Verlag.
Cadoli, M., Donini, F., & Schaerf, M. (1990). Closed world reasoning hybrid systems.
Proceedings 6th International Symposium Methodologies Intelligent
Systems (ISMIS90), pp. 474481. Elsevier.
Calvanese, D., Giacomo, G. D., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning efficient query answering description logics: DL-lite family. Journal Automated Reasoning, 39 (3), 385429.
Cote, R., Rothwell, D., Palotay, J., Beckett, R., & Brochu, L. (1993). systematized
nomenclature human veterinary medicine. Tech. rep., SNOMED International,
Northfield, IL: College American Pathologists.
de Kleer, J., & Konolige, K. (1989). Eliminating fixed predicates circumscription.
Artificial Intelligence, 39 (3), 391398.
Donini, F. M., Lenzerini, M., Nardi, D., Nutt, W., & Schaerf, A. (1998). epistemic
operator description logics.. Artificial Intelligence, 100 (1-2), 225274.
Donini, F. M., Nardi, D., & Rosati, R. (1997). Autoepistemic description logics. Proceedings Fifteenth International Joint Conference Artificial Intelligence (IJCAI97), pp. 136141. Morgan Kaufmann.
Donini, F. M., Nardi, D., & Rosati, R. (2002). Description logics minimal knowledge
negation failure. ACM Transactions Computational Logic, 3 (2), 177225.
Eiter, T., Gottlob, G., & Mannila, H. (1997). Disjunctive Datalog. ACM Transactions
Database Systems, 22 (3), 364418.
Eiter, T., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2004). Combining answer set
programming description logics semantic web. Proceedings
Ninth International Conference Principles Knowledge Representation
Reasoning (KR 2004), pp. 141151.
Giordano, L., Gliozzi, V., Olivetti, N., & Pozzato, G. L. (2008). Reasoning typicality
preferential description logics. Proceedings Logics Artificial Intelligence
(JELIA08), Vol. 5293 Lecture Notes Computer Science, pp. 192205. SpringerVerlag.
Goldblatt, R. (2003). Mathematical modal logic: view evolution. Journal Applied
Logic, 1, 309392.
Gradel, E., Otto, M., & Rosen, E. (1997). Two-Variable Logic Counting Decidable.
Proceedings Twelfth IEEE Symposium Logic Computer Science (LICS97),
pp. 306317. IEEE Computer Society Press.
Grimm, S., & Hitzler, P. (2008). Defeasible inference circumscriptive OWL ontologies.
Proceedings Workshop Advancing Reasoning Web: Scalability
Commonsense, No. 350 CEUR-WS (http://ceur-ws.org/).
Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning expressive description logics. Logic Journal IGPL, 8 (3), 239264.
771

fiBonatti, Lutz, & Wolter

Horty, J. F. (1994). direct theories nonmonotonoc inheritance. Handbook
Logic Artificial Intelligence Logic Programming-Nonmonotonic Reasoning
Uncertain Reasoning(Volume 3), pp. 111187. Clarendon Press.
Janhunen, T. (1999). intertranslatability non-monotonic logics. Annals Mathematics Artificial Intelligence, 27 (1-4), 79128.
Kagal, L., Finin, T., & Joshi, A. (2003). policy language pervasive computing
environment. Fourth IEEE International Workshop Policies Distributed
Systems Networks (POLICY2003).
Knorr, M., Alferes, J. J., & Hitzler, P. (2007). well-founded semantics hybrid MKNF
knowledge bases. Proceedings 2007 International Workshop Description
Logics (DL2007), No. 250 CEUR-WS (http://ceur-ws.org/).
Konev, B., Lutz, C., Walther, D., & Wolter, F. (2008). Semantic modularity module extraction description logics. Proceedings 18th European Conference
Artificial Intelligence (ECAI), pp. 5559.
Lambrix, P., Shahmehri, N., & Wahlloef, N. (1998). default extension description
logics use intelligent search engine. : Proceedings Thirty-First
Annual Hawaii International Conference System Sciences (HICSS98)-Volume 5,
p. 2835. IEEE Computer Society.
Lifschitz, V. (1985). Computing circumscription. Proceedings Ninth International Joint Conference Artificial Intelligence (IJCAI85), pp. 121127. Morgan
Kaufmann.
Lifschitz, V. (1993). Circumscription. Handbook Logic AI Logic Programming 3, pp. 298352. Oxford University Press.
Lifschitz, V. (1995). Nested abnormality theories. Artificial Intelligence, 74 (2), 351365.
McCarthy, J. (1980). Circumscription: form nonmonotonic reasoning. Artificial Intelligence, 13, 2739.
McCarthy, J. (1986). Applications circumscription formalizing common sense knowledge. Artificial Intelligence, 28, 89116.
Minsky, M. (1975). framework representating knowledge. Winston, P. H. (Ed.),
Psychology Computer Vision, pp. 211277. McGraw-Hill.
Moore, R. C. (1985). Semantical considerations nonmonotonic logics. Artificial Intelligence, 25, 7594.
Motik, B., & Rosati, R. (2007). Faithful Integration Description Logics Logic
Programming. Proceedings Twentieth International Joint Conference
Artificial Intelligence (IJCAI2007), pp. 477482. Morgan Kaufmann.
Pacholski, L., Szwast, W., & Tendera, L. (2000). Complexity results first-order twovariable logic counting. SIAM Journal Computing, 29 (4), 10831117.
Padgham, L., & Zhang, T. (1993). terminological logic defaults: definition
application. Proceedings Thirteenth International Joint Conference
Artificial Intelligence (IJCAI93), pp. 662668. Morgan Kaufmann.
772

fiThe Complexity Circumscription DLs

Pratt-Hartmann, I. (2005). Complexity two-variable fragment counting quantifiers. Journal Logic, Language, Information, 14 (3), 369395.
Quillian, M. R. (1968). Semantic memory. Semantic Information Processing, pp. 227270.
MIT Press.
Rector, A. (2004). Defaults, context, knowledge: Alternatives OWL-indexed knowledge bases. Proceedings Pacific Symposium Biocomputing (PSB04), pp.
226237. World Scientific.
Rector, A., & Horrocks, I. (1997). Experience building large, re-usable medical ontology
using description logic transitivity concept inclusions. Proceedings
Workshop Ontological Engineering, AAAI Spring Symposium. AAAI Press.
Reiter, R. (1980). logic default reasoning. Artificial Intelligence, 13, 81132.
Stevens, R., Aranguren, M. E., Wolstencroft, K., Sattler, U., Drummond, N., Horridge,
M., & Rector, A. (2005). Using OWL model biological knowledge. International
Journal Man-Machine Studies, 65 (7), 583594.
Stewart, I. A. (1991). Complete problems involving Boolean labelled structures projection transactions. Journal Logic Computation, 1 (6), 861882.
Straccia, U. (1993). Default inheritance reasoning hybrid KL-ONE-style logics..
Proceedings Thirteenth International Joint Conference Artificial Intelligence
(IJCAI93), pp. 676681. Morgan Kaufmann.
Thomason, S. (1975a). logical consequence relation propositional tense logic.
Zeitschrift fur Mathematische Logik und Grundlagen der Mathematik, 21, 2940.
Thomason, S. (1975b). Reduction second-order logic modal logic. Zeitschrift fur
Mathematische Logik und Grundlagen der Mathematik, 21, 107114.
Tobies, S. (2000). complexity reasoning cardinality restrictions nominals
expressive description logics. Journal Artificial Intelligence Research, 12, 199217.
Tonti, G., Bradshaw, J. M., Jeffers, R., Montanari, R., Suri, N., & Uszok, A. (2003). Semantic web languages policy representation reasoning: comparison KAoS,
Rei, Ponder. Proceedings Second International Semantic Web Conference
(ISWC03), Vol. 2870 Lecture Notes Computer Science, pp. 419437. SpringerVerlag.
Uszok, A., Bradshaw, J. M., Johnson, M., Jeffers, R., Tate, A., Dalton, J., & Aitken, S.
(2004). KAoS policy management semantic web services. IEEE Intelligent Systems,
19 (4), 3241.
Wolter, F., & Zakharyaschev, M. (2007). Modal decision problems. Handbook Modal
Logic. Elsevier.
Y. Ding, V. H., & Wu, J. (2007). new mapping ALCI ALC. Proceedings
2007 International Workshop Description Logics (DL2007), No. 250 CEURWS (http://ceur-ws.org/).

773

fiJournal Artificial Intelligence Research 35 (2009) 193-234

Submitted 4/08; published 6/09

Transductive Rademacher Complexity Applications
Ran El-Yaniv
Dmitry Pechyony

rani@cs.technion.ac.il
pechyony@cs.technion.ac.il

Department Computer Science
Technion - Israel Institute Technology
Haifa, 32000, Israel

Abstract
develop technique deriving data-dependent error bounds transductive
learning algorithms based transductive Rademacher complexity. technique based
novel general error bound transduction terms transductive Rademacher complexity, together novel bounding technique Rademacher averages particular
algorithms, terms unlabeled-labeled representation. technique relevant
many advanced graph-based transductive algorithms demonstrate effectiveness deriving error bounds three well known algorithms. Finally, present new
PAC-Bayesian bound mixtures transductive algorithms based Rademacher
bounds.

1. Introduction
Alternative learning models utilize unlabeled data received considerable attention
past years. Two prominent models semi-supervised transductive learning. main attraction models theoretical empirical evidence (Chapelle,
Scholkopf, & Zien, 2006) indicating often allow efficient significantly faster learning terms sample complexity. paper support theoretical
evidence providing risk bounds number state-of-the-art transductive algorithms.
bounds utilize labeled unlabeled examples much tighter
bounds relying labeled examples alone.
focus distribution-free transductive learning. setting given
labeled training sample well unlabeled test sample. goal guess labels
given test points accurately possible1 . Rather generating general hypothesis
capable predicting label point, inductive learning, advocated
Vapnik (1982) aim transduction solve easier problem transferring
knowledge directly labeled points unlabeled ones.
Transductive learning already proposed briefly studied thirty years
ago Vapnik Chervonenkis (1974), lately empirically recognized
transduction often facilitate efficient accurate learning traditional supervised learning approach (Chapelle et al., 2006). recognition motivated
flurry recent activity focusing transductive learning, many new algorithms
1. Many papers refer model semi-supervised learning. However, setting semi-supervised
learning different transduction. semi-supervised learning learner given randomly drawn
training set consisting labeled unlabeled examples. goal learner generate
hypothesis providing accurate predictions unseen examples.
c
2009
AI Access Foundation. rights reserved.

fiEl-Yaniv & Pechyony

heuristics proposed. Nevertheless, issues identification universally effective learning principles transduction remain unresolved. Statistical learning
theory provides principled approach attacking questions study
error bounds. example, inductive learning bounds proven instrumental
characterizing learning principles deriving practical algorithms (Vapnik, 2000).
paper consider classification setting transductive learning. far,
several general error bounds transductive classification developed Vapnik (1982), Blum Langford (2003), Derbeko, El-Yaniv, Meir (2004), El-Yaniv
Pechyony (2006). continue fruitful line research develop new technique
deriving explicit data-dependent error bounds. bounds less tight implicit ones, developed Vapnik Blum Langford. However explicit bounds
may potentially used model selection guide development new learning
algorithms.
technique consists two parts. first part develop novel general error
bound transduction terms transductive Rademacher complexity. bound
syntactically similar known inductive Rademacher bounds (see, e.g., Bartlett & Mendelson, 2002), fundamentally different sense transductive Rademacher complexity computed respect hypothesis space chosen observing
unlabeled training test examples. opportunity unavailable inductive
setting hypothesis space must fixed example observed.
second part bounding technique generic method bounding Rademacher
complexity transductive algorithms based unlabeled-labeled representation (ULR).
representation soft-classification vector generated algorithm product
U , U matrix depends unlabeled data vector may
depend given information, including labeled training set. transductive algorithm infinite number ULRs, including trivial ULR, U identity
matrix. show many state-of-the-art algorithms non-trivial ULR leading
non-trivial error bounds. Based ULR representation bound Rademacher complexity
transductive algorithms terms spectrum matrix U ULR.
bound justifies spectral transformations, developed Chapelle, Weston, Scholkopf
(2003), Joachims (2003), Johnson Zhang (2008), commonly done improve
performance transductive algorithms. instantiate Rademacher complexity
bound consistency method Zhou et al. (2004), spectral graph transducer
(SGT) algorithm Joachims (2003) Tikhonov regularization algorithm Belkin,
Matveeva, Niyogi (2004). bounds obtained algorithms explicit
easily computed.
also show simple Monte-Carlo scheme bounding Rademacher complexity
transductive algorithm using ULR. demonstrate efficacy scheme
consistency method Zhou et al. (2004). final contribution PAC-Bayesian bound
transductive mixture algorithms. result, stated Theorem 4, obtained
consequence Theorem 2 using techniques Meir Zhang (2003). result
motivates use ensemble methods transduction yet explored
setting.
paper following structure. Section 1.1 survey results
closely related work. Section 2 define learning model transductive
2

fiTransductive Rademacher Complexity Applications

Rademacher complexity. Section 3 develop novel concentration inequality functions partitions finite set points. inequality transductive Rademacher
complexity used Section 4 derive uniform risk bound, depends transductive Rademacher complexity. Section 5 introduce generic method bounding
Rademacher complexity transductive algorithm using unlabeled-labeled representation. Section 6 exemplify technique obtain explicit risk bounds several
known transductive algorithms. Finally, Section 7 instantiate risk bound
transductive mixture algorithms. discuss directions future research Section 8.
technical proofs results presented Appendices A-I.
Preliminary (and shorter) version paper appeared Proceedings
20th Annual Conference Learning Theory, page 157171, 2007.
1.1 Related Work
Vapnik (1982) presented first general 0/1 loss bounds transductive classification.
bounds implicit sense tail probabilities specified bound
outcome computational routine. Vapniks bounds refined include prior
beliefs noted Derbeko et al. (2004). Similar implicit somewhat tighter bounds
developed Blum Langford (2003) 0/1 loss case. Explicit PAC-Bayesian
transductive bounds bounded loss function presented Derbeko et al. (2004).
Catoni (2004, 2007) Audibert (2004) developed PAC-Bayesian VC dimensionbased risk bounds special case size test set multiple
size training set. Unlike PAC-Bayesian bound, published transductive PACBayesian bounds hold deterministic hypotheses Gibbs classifiers. bounds
Balcan Blum (2006) semi-supervised learning also hold transductive setting,
making conceptually similar transductive PAC-Bayesian bounds. General
error bounds based stability developed El-Yaniv Pechyony (2006).
Effective applications general bounds mentioned particular algorithms
learning principles automatic. case PAC-Bayesian bounds several
successful applications presented terms appropriate priors promote
various structural properties data (see, e.g., Derbeko et al., 2004; El-Yaniv & Gerzon,
2005; Hanneke, 2006). Ad-hoc bounds particular algorithms developed Belkin
et al. (2004) Johnson Zhang (2007). Unlike bounds (including ours)
bound Johnson Zhang depend empirical error
properties hypothesis space. size training test set increases
bound converges zero2 . Thus bound Johnson Zhang effectively proves
consistency transductive algorithms consider. However bound holds
hyperparameters algorithms chosen w.r.t. unknown test labels.
Hence bound Johnson Zhang cannot computed explicitly.
Error bounds based Rademacher complexity introduced Koltchinskii (2001)
well-established topic induction (see Bartlett & Mendelson, 2002, references
therein). first Rademacher transductive risk bound presented Lanckriet et al.
(2004, Theorem 24). bound, straightforward extension inductive
2. known explicit bounds increase training test sets decreases slack term
empirical error.

3

fiEl-Yaniv & Pechyony

Rademacher techniques Bartlett Mendelson (2002), limited special case
training test sets equal size. bound presented overcomes
limitation.

2. Definitions
Section 2.1 provide formal definition learning model. Section 2.2
define transductive Rademacher complexity compare inductive counterpart.
2.1 Learning Model
paper use distribution-free transductive model, defined Vapnik (1982,

Section 10.1, Setting 1). Consider fixed set Sm+u = {(xi , yi )}m+u
i=1 + u points xi
space together labels yi . learner provided (unlabeled)

full-sample Xm+u = {xi }m+u
i=1 . set consisting points selected Xm+u uniformly
random among subsets size m. points together labels given
learner training set. Re-numbering points denote unlabeled training


set points Xm = {x1 , . . . , xm } labeled training set Sm = {(xi , yi )}m
i=1 .

set unlabeled points Xu = {xm+1 , . . . , xm+u } = Xm+u \ Xm called test set.
learners goal predict labels test points Xu based Sm Xu .
Remark 1 learner model example xi unique label yi . However allow
6= j, xi = xj yi 6= yj .
choice set points described viewed three equivalent
ways:
1. Drawing points Xm+u uniformly without replacement. Due draw,
points training test sets dependent.
2. Random permutation full sample Xm+u choosing first points
training set.
3. Random partitioning + u points two disjoint sets u points.
emphasize different aspects transductive learning model, throughout paper
use interchangeably three views generation training test sets.
paper focuses binary learning problems labels {1}. learning
algorithms consider generate soft classification vectors h = (h(1), . . . h(m + u))
Rm+u , h(i) (or h(xi )) soft, confidence-rated, label example xi given
hypothesis h. actual (binary) classification xi algorithm outputs sgn(h(i)).
denote Hout Rm+u set possible soft classification vectors (over possible
tranining/test partitions) generated algorithm.
Based full-sample Xm+u , algorithm selects hypothesis space H Rm+u
soft classification hypotheses. Note Hout H. Then, given labels training
points algorithm outputs one hypothesis h Hout H classification. goal

transductive learner find hypothesis h minimizing test error Lu (h) =
4

fiTransductive Rademacher Complexity Applications

1 Pm+u
i=m+1 `(h(i), yi ) w.r.t.
u
1 Pm
i=1 `(h(i), yi )



0/1 loss function `. empirical error h Lbm (h) =
P

m+u
1
full sample error h Lm+u (h) = m+u
i=1 `(h(i), yi ).
work also use margin loss function ` . positive real , ` (y1 , y2 ) = 0
y1 y2 ` (y1 , y2 ) = min{1, 1 y1 y2 /} otherwise. empirical (margin) error
1 Pm

h Lbm (h) =
i=1 ` (h(i), yi ). denote Lu (h) margin error test set

Lm+u (h) margin full sample error.
denote Irs , r < s, set natural numbers {r, r + 1, . . . , s}. Throughout
paper assume vectors column ones. mark vectors boldface.

2.2 Transductive Rademacher Complexity
adapt inductive Rademacher complexity transductive setting generalize
bit also include neutral Rademacher values.
Definition 1 (Transductive Rademacher complexity) Let V Rm+u p [0, 1/2].
Let = (1 , . . . , m+u )T vector i.i.d. random variables


1

= 1


0

probability
probability
probability

p;
p;
1 2p.

(1)

transductive Rademacher complexity parameter p




1
1


Rm+u (V, p) =
+
E sup v .
u
vV
need novel definition Rademacher complexity technical. Two main
issues lead new definition are:
P
1. need bound test error Lu (h) = u1 m+u
i=m+1 `(h(i), yi ). Notice inductive risk bounds standard definition Rademacher complexity (see Definition 2
below), binary values , used bound generalization
error,
1 Pm+u
inductive analogue full sample error Lm+u (h) = m+u
`(h(i),
yi ).
i=1
2. Different sizes (m u respectively) training test set.
See Section 4.1 technical details lead definition Rademacher
complexity.
sake comparison also state inductive definition Rademacher complexity.
Definition 2 (Inductive Rademacher complexity, Koltchinskii, 2001) Let
probability distribution X . Suppose examples Xn = {xi }ni=1 sampled independently X according D. Let F class functions mapping X R. Let
= {i }ni=1 independent uniform {1}-valued random variables, = 1 probability 1/2 = 1 probability. empirical Rademacher complex5

fiEl-Yaniv & Pechyony

(ind)
Rn (F)



Pn
2
f (xi )
nnE supf F
i=1

(ind)
bn (F) .
= EXn Dn R


bn(ind) (F) =
ity is3 R



Rademacher complexity F


b(ind) (V).
case p = 1/2, = u n = + u Rm+u (V) = 2R
m+u
Whenever p < 1/2, Rademacher variables attain (neutral) zero values reduce
complexity (see Lemma 1). use property tighten bounds.
Notice transductive complexity empirical quantity depend
underlying distribution, including one choices training set. Since
distribution-free transductive model unlabeled full sample training test points
fixed, transductive Rademacher complexity dont need outer expectation,
appears inductive definition. Also, transductive complexity depends
(unlabeled) training test points whereas inductive complexity depends
(unlabeled) training points.
following lemma, whose proof appears Appendix A, states Rm+u (V, p)
monotone increasing p. proof based technique used proof
Lemma 5 paper Meir Zhang (2003).

Lemma 1 V Rm+u 0 p1 < p2 1/2, Rm+u (V, p1 ) < Rm+u (V, p2 ).
forthcoming results utilize transductive Rademacher complexity


mu
p0 = (m+u)
2 . abbreviate Rm+u (V) = Rm+u (V, p0 ). Lemma 1, bounds also

apply Rm+u (V, p) p > p0 . Since p0 < 21 , Rademacher complexity involved
results strictly smaller standard inductive Rademacher complexity defined
Xm+u . Also, transduction approaches induction, namely fixed u ,
b(ind) (V) 2Rm+u (V).
R
m+u

3. Concentration Inequalities Functions Partitions
section develop novel concentration inequality functions partitions
compare several known ones. concentration inequality utilized
derivation forthcoming risk bound.


Let Z = Zm+u
= (Z1 , . . . , Zm+u ) random permutation vector variable
1
Zk , k I1m+u , kth component permutation I1m+u chosen uniformly
random. Let Zij perturbed permutation vector obtained exchanging values Zi
Zj Z. function f permutations I1m+u called (m, u)-permutation symmetM
ric f (Z) = f (Z1 , . . . , Zm+u ) symmetric Z1 , . . . , Zm well Zm+1 , . . . , Zm+u .
section present novel concentration inequality (m, u)-permutation symmetric functions. Note (m, u)-permutation symmetric function essentially function partition + u items sets sizes u. Thus, forthcoming
inequalities Lemmas 2 3, stated (m, u)-permutation symmetric functions, also hold exactly form functions partitions. Conceptually
3. original definition Rademacher complexity,P
given Koltchinskii
(2001),P
slightly different

n

one presented here, contains supf F n
i=1 f (xi ) instead supf F
i=1 f (xi ). However, conceptual point view, Definition 2 one given Koltchinskii equivalent.

6

fiTransductive Rademacher Complexity Applications

convenient view results concentration inequalities functions partitions. However, technical point view find convenient consider
(m, u)-permutation symmetric functions.
following lemma (that utilized proof Theorem 1) presents concentration inequality extension Lemma 2 El-Yaniv Pechyony (2006).
proof (appearing Appendix B) relies McDiarmids inequality (McDiarmid, 1989,
Corollary 6.10) martingales.
m+u
Lemma 2 Let Z random permutation
(m, u) vector
I1 . Let f (Z)
m+u
ij


permutation symmetric function satisfying f (Z) f (Z ) I1m , j Im+1
.




22 (m + u 1/2)
1
PZ {f (Z) EZ {f (Z)} } exp
1
.
(2)
mu 2
2 max(m, u)



2 1
1
right hand side (2) approximately exp 2
+
. similar, less tight
2

u

inequality obtained reduction draw random permutation draw
min(m, u) independent random variables application bounded difference inequality McDiarmid (1989):
Lemma 3 Suppose conditions Lemma 2 hold.

PZ {f (Z) EZ {f (Z)} } exp

22
2 min(m, u)


.

(3)

proof Lemma 3 appears Appendix C.
Remark 2 inequalities developed Section 5 Talagrand (1995) imply concentration inequality similar (3), worse constants.
inequality (2) defined (m, u)-permutation symmetric function f .
specializing f obtain following two concentration inequalities:
P
1 Pm
Remark 3 g : I1m+u {0, 1} f (Z) = u1 m+u
i=m+1 g(Zi )
i=1 g(Zi ),
m+u
1
EZ {f (Z)} = 0. Moreover, I1m , j Im+1
, |f (Z) f (Zij )|
+ u1 . Therefore,
specializing (2) f obtain
(
PZ

m+u

1 X
1 X
g(Zi )
g(Zi )
u

i=m+1

i=1

)

2

mu(m + u 1/2) 2 max(m, u) 1
exp

.
(m + u)2
max(m, u)

(4)

right hand side (4) approximately exp
. inequality (4) explicit (and looser) version Vapniks absolute bound (see El-Yaniv & Gerzon, 2005).
note using (2) unable obtain explicit version Vapniks relative bound
(inequality 10.14 Vapnik, 1982).


7

2 mu
2m+u

fiEl-Yaniv & Pechyony

1 Pm
1 Pm+u
Remark 4 g : I1m+u {0, 1}, f (Z) =
i=1 g(Zi ), EZ {f (Z)} = m+u
i=1 g(Zi ).
m+u
1

ij
Moreover, I1 , j Im+1 , |f (Z) f (Z )| . Therefore, specializing (2)
f obtain
(
)
2


m+u
X
1 X
1
(m + u 1/2)m 2 max(m, u) 1
g(Zi )
g(Zi ) exp

PZ
.

m+u
u
max(m, u)
i=1
i=1
(5)


22 (m+u)m
. bound asymptotically
right hand side (5) approximately exp
u
following bound, developed Serfling (1974):
(
)



m+u
X
1 X
1
22 (m + u)m
PZ
g(Zi )
g(Zi ) exp
.

m+u
u+1
i=1

i=1

4. Uniform Rademacher Error Bound
section develop transductive risk bound, based transductive
Rademacher complexity (Definition 1). derivation follows standard two-step scheme,
induction4 :
1. Derivation uniform concentration inequality set vectors (or functions).
inequality depends Rademacher complexity set. substituting
vectors (or functions) values loss functions, obtain error bound
depending Rademacher complexity values loss function. step
done Section 4.1.
2. order bound Rademacher complexity terms properties hypothesis space, Rademacher complexity translated, using contraction property (Ledoux & Talagrand, 1991, Theorem 4.12), domain loss function
values domain soft hypotheses hypothesis space. step done
Section 4.2.
show Sections 4.1 4.2, adaptation steps transductive
setting immediate involves several novel ideas. Section 4.3 combine
results two steps obtain transductive Rademacher risk bound. also
provide thorough comparison risk bound corresponding inductive bound.
4.1 Uniform Concentration Inequality Set Vectors
induction (Koltchinskii & Panchenko, 2002), derivation uniform concentration
inequality set vectors consists three steps:
1. Introduction ghost sample.
2. Bounding supremum suphH g(h), g(h) random real-valued function, expectation using concentration inequality functions random
variables.
4. scheme introduced Koltchinskii Panchenko (2002). examples uses
technique found papers Bartlett Mendelson (2002) Meir Zhang (2003).

8

fiTransductive Rademacher Complexity Applications

3. Bounding expectation supremum using Rademacher variables.
follow three steps induction, establishment steps
achieved using inductive techniques. Throughout section, performing
derivation step transductive context discuss differences inductive
counterpart.
introduce several new definitions. Let V set vectors [B1 , B2 ]m+u , B1

0, B2 0 set B = B2 B1 , Bmax = max(|B1 |, |B2 |). Consider two independent
permutations I1m+u , Z Z0 . v V denote


v(Z) = (v(Z1 ), v(Z2 ), . . . , v(Zm+u )) ,
vector v permuted according Z. use following abbreviations averages
1 Pm+u
1 Pk
v subsets components: Hk {v(Z)} =
i=1 v(Zi ), Tk {v(Z)} = u
i=k+1 v(Zi )
(note H stands head T, tail). special case k = set


H{v(Z)} = Hm {v(Z)}, T{v(Z)} = Tm {v(Z)}. uniform concentration inequality
develop shortly states > 0, probability least 1 random
permutation Z I1m+u , v V,

!
1
1
T{v(Z)} H{v(Z)} + Rm+u (V) +
ln
.
min(m, u)

Step 1: Introduction ghost sample.

1 Pm+u
denote v = m+u
i=1 v(i) average component v. v V
m+u
permutation Z I1


T{v(Z)} = H{v(Z)} + T{v(Z)} H{v(Z)}
h

H{v(Z)} + sup T{v(Z)} v + v H{v(Z)}
vV
h

= H{v(Z)} + sup T{v(Z)} EZ0 T{v(Z0 )} + EZ0 H{v(Z0 )} H{v(Z)}
vV
h

H{v(Z)} + EZ0 sup T{v(Z)} T{v(Z0 )} + H{v(Z0 )} H{v(Z)} . (6)
vV
|
{z
}


=(Z)

Remark 5 derivation ghost sample permutation Z0 + u elements
drawn distribution Z. inductive Rademacher-based risk bounds
ghost sample new training set size m, independently drawn original one.
Note transductive setting ghost sample corresponds independent draw
training/test set partition, equivalent independent draw random permutation Z0 .
Remark 6 principle could avoid introduction ghost sample Z0 consider
elements H{v(Z)} ghosts u elements T{v(Z)}. approach would lead
9

fiEl-Yaniv & Pechyony

new definition Rademacher averages (with = 1/m probability m/(m + u)
1/u probability u/(m + u)). definition obtain Corollary 1. However,
since distribution alternative Rademacher averages symmetric around zero,
technically know prove Lemma 5 (the contraction property).
Step 2: Bounding supremum expectation.


m+u
Let = (m+u1/2)(11/(2
max(m,u))) . sufficiently large u, value almost 1. function (Z) (m, u)-permutation symmetric Z. verified
1

1


|(Z) (Zij )| B
+ u1 . Therefore, apply Lemma 2 = B
+ u1
(Z). obtain, probability least 1 random permutation Z I1m+u ,
v V:


1
1
1
T{v(Z)} H{v(Z)} + EZ {(Z)} + B
+
ln .
(7)
2 u


Remark 7 induction step performed using application McDiarmids bounded
difference inequality (McDiarmid, 1989, Lemma 1.2). cannot apply inequality
setting since function supremum (i.e. (Z)) function independent variables, rather permutations. Lemma 2 replaces bounded
difference inequality step.
Step 3: Bounding expectation supremum using Rademacher random variables.
goal bound expectation EZ {(Z)}. done following lemma.
Lemma 4 Let Z random permutation

I1m+u .


EZ {(Z)} Rm+u (V) + c0 Bmax

q



Let c0 =

1
1
+
u



p

32 ln(4e)
3

< 5.05.

min(m, u) .

Proof: proof based ideas proof Lemma 3 Bartlett Mendelson
(2002). technical convenience use following definition pairwise Rademacher
variables.
Definition 3 (Pairwise Rademacher variables) Let v = (v(1), . . . , v(m + u)) Rm+u .
m+u
Let V set vectors Rm+u . Let = {i }i=1
vector i.i.d. random variables
defined as:
1

mu
, u1
probability (m+u)

2 ;





m2
1, 1
probability (m+u)2 ;
(8)
= (i,1 , i,2 ) = 1 m1
mu

,

probability
;

2
u
(m+u)



1
1
u2
,


probability
.
u
u
(m+u)2
obtain Definition 3 Definition 1 (with p =
Rademacher variable = 1 split =
10

mu
2)
(m+u)
1 1
u, .

following way.
Rademacher variable

fiTransductive Rademacher Complexity Applications

1

1
= 1 split
m,
u . Rademacher variable = 0
it1 to1 =

split randomly , u1 , u1 . first component indicates ith
component v first elements v(Z) last u elements v(Z).
1
former case value
latter case value u1 . second
component meaning first one, Z replaced Z0 .
1
values
u1 exactly coefficients appearing inside T{v(Z)}, T{v(Z0 )},
0
H{v(Z )} H{v(Z)} (6). coefficients random distribution
induced uniform distribution permutations. course proof
1
establish precise relation distribution
u1 coefficients
distribution (8) pairwise Rademacher variables.
easy verify
(
)
m+u
X
Rm+u (V) = E sup
(i,1 + i,2 )v(i) .
(9)
vV i=1

1

Let n1 , n2 n3 number random variables realizing value
, u1 ,
1 1 1 1


, , u , , respectively. Set N1 = n1 + n2 N2 = n2 + n3 . Note ni
Ni random variables. Denote Rad distribution defined (8)
Rad(N1 , N2 ), distribution Rad conditioned events n1 +n2 = N1 n2 +n3 = N2 .
define
(
)
m+u
X

s(N1 , N2 ) = ERad(N1 ,N2 ) sup
(i,1 + i,2 ) v(i) .
vV i=1

rest proof based following three claims:
Claim 1. Rm+u (V) = EN1 ,N2 {s(N1 , N2 )}.
Claim 2. EZ {(Z)} = (E N1 , E N2 ).
Claim 3. (E N1 , E N2 ) EN1 ,N2 {s(N1 , N2 )} c0 Bmax

1

u

+

1



m.

established three claims immediately obtain


1
1
EZ {g(Z)} Rm+u (V) + c0 Bmax
+
.
u

(10)

entire development symmetric u and, therefore, also obtain


result u instead m. taking minimum (10) symmetric

bound (with u) establish theorem.
proof three claims appears Appendix D.

Remark 8 technique use bound expectation supremum complicated technique Koltchinskii Panchenko (2002) commonly used
induction. caused structure function supremum (i.e.,
g(Z)). conceptual point view, step utilizes novel definition transductive Rademacher complexity.
combining (7) Lemma 4 obtain next concentration inequality,
main result section.
11

fiEl-Yaniv & Pechyony

Theorem 1 Let B1 0, B2 0 V (possibly infinite) set real-valued vectors





1
[B1 , B2 ]m+u . Let B = B2 B1 Bmax = max(|B1 |, |B2 |). Let Q = u1 +
, =
q

32 ln(4e)
m+u
< 5.05. probability least 1
3
(m+u1/2)(11/2(max(m,u))) c0 =
m+u
random permutation Z I1 , v V,
p
T{v(Z)} H{v(Z)} + Rm+u (V) + Bmax c0 Q min(m, u) + B

r

1

Q ln .
(11)
2

q
p
defer analysis slack terms Bmax c0 Q min(m, u) B S2 Q ln 1 Section 4.3. instantiate inequality (11) obtain first risk bound. idea
apply Theorem 1 appropriate instantiation set V T{v(Z)}
correspond test error H{v(Z)} empirical error. true (unknown)
labeling full-sample h Hout define


`y (h) = (`(h(1), y1 ), . . . , `(h(m + u), ym+u ))
set LH = {v : v = `y (h), h Hout }. Thus `y (h) vector values
0/1 loss full sample examples, transductive algorithm operated
training/test partition. set LH set possible vectors `y (h), possible


training/test partitions. apply Theorem 1 V = LH , v = `(h), Bmax = B = 1
obtain following corollary:
Corollary 1 Let Q, c0 defined Theorem 1. > 0, probability
least 1 choice training set Xm+u , h Hout ,
r
p
1

Q ln .
(12)
Lu (h) Lbm (h) + Rm+u (LH ) + Bmax c0 Q min(m, u) +
2

q
p
defer analysis slack terms Bmax c0 Q min(m, u) B S2 Q ln 1 Section 4.3.
bound (12) obtained straightforward application concentration
inequality (11), convenient deal with. Thats clear bound
Rademacher complexity Rm+u (LH ) 0/1 loss values terms properties
transductive algorithm. next sections eliminate deficiency utilizing margin
loss function.
4.2 Contraction Rademacher Complexity
following lemma version well-known contraction principle theory
Rademacher averages (see Theorem 4.12 Ledoux & Talagrand, 1991, Ambroladze,
Parrado-Hernandez, & Shawe-Taylor, 2007). lemma adaptation, accommodates transductive Rademacher variables, Lemma 5 Meir Zhang (2003).
proof provided Appendix E.
Lemma 5 Let V Rm+u set vectors. Let f g real-valued functions. Let
= {i }m+u
i=1 Rademacher variables, defined (1). 1 + u
12

fiTransductive Rademacher Complexity Applications

v, v0 V, |f (vi ) f (vi0 )| |g(vi ) g(vi0 )|,
"m+u
#
"m+u
#
X
X
E sup
f (vi ) E sup
g(vi ) .
vV

vV

i=1

i=1

Let = (y1 , . . . , ym+u ) Rm+u true (unknown) labeling full-sample. Similarly done derivation Corollary 1, h Hout define

`y (h(i)) = ` (h(i), yi )


`y (h) = (`y (h(1)), . . . , `Y (h(m + u)))
set LH = {v : v = `y (h), h Hout }. Noting `y satisfies Lipschitz condition


|`y (h(i)) `y (h0 (i))| 1 |h(i) h0 (i)|, apply Lemma 5 V = LH , f (vi ) = `y (h(i))


g(vi ) = h(i)/, get
(
E

sup

m+u
X

hHout i=1

)
`y (h(i))

follows (13)
Rm+u (LH )

1
E


(
sup

m+u
X

hHout i=1

1
Rm+u (Hout ) .


)
h(i)

.

(13)

(14)

4.3 Risk Bound Comparison Related Results




Applying Theorem 1 V = LH , v = ` (h), Bmax = B = 1, using inequality
(14) obtain5 :
Theorem 2 Let Hout set full-sample soft labelings algorithm, generated
operating possible training/test q
set partitions. choice Hout de

1
32 ln(4e)
1
pend full-sample Xm+u . Let c0 =
<
5.05,
Q
=
+
3
u
=
m+u
(m+u1/2)(11/(2 max(m,u))) . fixed , probability least 1 choice
training set Xm+u , h Hout ,
r
p
Rm+u (Hout )
SQ 1


b
+ c0 Q min(m, u) +
ln .
(15)
Lu (h) Lu (h) Lm (h) +

2

large enough valuesqof u value close
1. Therefore slack
p
p

1
term c0 Q min(m, u) + 2 Q ln order 1/ min(m, u) . convergence rate

p
1/ min(m, u) slow small u m. Slow rate small
surprising, latter case u somewhat surprising. However note
u mean u elements, drawn + u elements, large variance.
Hence, case high-confidence interval estimation large.
confidence interval reflected slack term (15).
5. bound holds fixed margin parameter . Using technique proof Theorem 18
Bousquet Elisseeff (2002), also obtain bound uniform .

13

fiEl-Yaniv & Pechyony

compare bound (15) Rademacher-based inductive risk bounds.
use following variant Rademacher-based inductive risk bound Meir Zhang
(2003):
Theorem 3 Let probability distribution X . Suppose set examples
Sm = {(xi , yi )}m
i=1 sampled i.i.d. X according D. Let F class functions
(ind)
bm
maps X R R
(F) empirical Rademacher
complexity F (Defini1 Pm
tion 2). Let L(f ) = E(x,y)D {`(f (x), y)} Lb (f ) =
`
i=1 (f (xi ), yi ) respectively
0/1 generalization error empirical margin error f . > 0
> 0, probability least 1 random draw Sm , f F,
r
(ind)
bm
R
(F)
2 ln(2/)

L(f ) Lb (f ) +
+
.
(16)



slack term bound (16) order O(1/ m). bounds (15) (16)
quantitatively comparable. inductive bound holds high probability
random selection examples distribution D. bound average
(generalization) error, examples D. transductive bound holds high
probability random selection training/test partition. bound test
error hypothesis particular set u points.
kind meaningful comparison obtained follows. Using given full
(transductive) sample Xm+u , define corresponding inductive distribution Dtrans
uniform distribution Xm+u ; is, training set size generated
sampling Xm+u times replacements. Given inductive hypothesis space
F = {f } function define transductive hypothesis space HF projection F
full sample Xm+u : HF = {h Rm+u : f F, 1 + u, h(i) = f (xi )}.
definition HF , L(f ) = Lm+u (h).
final step towards meaningful comparison would translate transductive
bound form Lu (h) Lbm (h)+slack bound average error hypothesis6
h:


bm (h) + u Lbm (h) + slack



L
b
mLm (h) + uLu (h)

Lm+u (h) Lm+u (h) =
m+u
m+u
u

slack
= Lbm (h) +
(17)
m+u
instantiate (17) bound (15) obtain
Lm+u (h)

Lbm (h)

"
#
r
p
u Rm+u (HF )
u
SQ 1
+
+
c0 Q min(m, u) +
ln
. (18)
m+u

m+u
2


6. Alternatively, compare (15) (16), could try express bound (16) bound
error f Xu (the randomly drawn subset u examples). bound (16) holds setting
random draws replacement. setting number unique training examples smaller
thus number remaining test examples larger u. Hence draw
training examples replacement imply draw subset u test examples,
transductive setting. Thus cannot express bound (16) bound randomly drawn Xu

14

fiTransductive Rademacher Complexity Applications

given transductive problem consider corresponding inductive bound obtained
(16) distribution Dtrans compare bound (18).
Note inductive bound (16) sampling training set done
replacement, transductive bound (18) done without replacement. Thus,
inductive case actual number distinct training examples may smaller m.
bounds (16) (18) consist three terms: empirical error term (first summand
(16) (18)), term depending Rademacher complexity (second summand
(16) (18)) slack term (third summand (16) third fourth summands
(18)). empirical error terms bounds. hard compare
analytically Rademacher complexity terms. inductive bound
derived setting sampling replacement transductive bound derived
setting sampling without replacement. Thus, transductive Rademacher
complexity example xi Xm+u appears Rm+u (Hout ) multiplied
. contrast, due sampling replacement, inductive Rademacher
b(ind) (F), multiplied different
term example xi Xm+u appear several times R
m+u
values Rademacher variables.
Nevertheless, transduction full control Rademacher complexity
(since choose Hout observing full sample Xm+u ) choose hypothesis space Hout arbitrarily small Rademacher complexity. induction choose F
b(ind) (F)
observing data. Hence, lucky full sample Xm+u R
m+u
b(ind) (F) large. Thus,
small, unlucky Xm+u R
m+u
provisions argue transductive Rademacher term larger
inductive counterpart.
Finally, compare slack terms (16) (18). u u slack

term (18) order (1/ m), corresponding term (16).

u slack term (18) order (1/(m u)), much smaller

O(1/ m) slack term (16).
Based comparison corresponding terms (16) (18) conclusion
regime u transductive bound significantly tighter
inductive one.7

5. Unlabeled-Labeled Representation (ULR) Transductive Algorithms
Let r natural number let U (m + u) r matrix depending Xm+u .
Let r 1 vector may depend Sm Xu . soft classification
output h transductive algorithm represented
h=U .

(19)

refer (19) unlabeled-labeled representation (ULR). section develop
bounds Rademacher complexity algorithms based ULRs. note
transductive algorithm trivial ULR, example, taking r = + u, setting U
7. regime u occurs following class applications. Given large library tagged
objects, goal learner assign tags small quantity newly arrived objects.
example application organization daily news.

15

fiEl-Yaniv & Pechyony

identity matrix assigning desired (soft) labels. interested
non-trivial ULRs provide useful bounds representations.8
vanilla ULR, U (m + u) (m + u) matrix = (1 , . . . , m+u ) simply
specifies given labels Sm (where = yi labeled points, = 0 otherwise).
point view vanilla ULR trivial encode
final classification algorithm. example, algorithm Zhou et al. (2004)
straightforwardly admits vanilla ULR. hand, natural (non-trivial) ULR
algorithms Zhu et al. (2003) Belkin Niyogi (2004) vanilla
type. algorithms necessarily obvious find non-trivial ULRs.
Sections 6 consider two cases particular, algorithms Joachims (2003)
Belkin et al. (2004).
rest section organized follows. Section 5.1 present generic
bound Rademacher complexity transductive algorithm based ULR.
Section 5.2 consider case matrix U kernel matrix. case
develop another bound transductive Rademacher complexity. Finally, Section 5.3
present method computing high-confidence estimate transductive Rademacher
complexity.
5.1 Generic Bound Transductive Rademacher Complexity
present bound transductive Rademacher complexity transductive
algorithm based
ULR. Let {i }ri=1 singular values U . use well-known
qP
qP

r
2 , kU k
2

=
fact kU kFro =
Fro
i=1
i,j (U (i, j)) Frobenius norm


U . Suppose kk2 1 1 . Let Hout = Hout (U ) set possible
outputs algorithm operated possible training/test set partitions
1
+ u1 . Using abbreviation U (i, ) ith row U
full-sample Xm+u . Let Q =
following proof idea Lemma 22 Bartlett Mendelson (2002),
(
)
(
)
m+u
m+u
X
X
Rm+u (Hout ) = Q E
sup
h(xi ) = Q E
sup
h, U (i, )i
(
= Q E

hHout i=1

sup
:kk2 1

h,

m+u
X

)

:kk2 1 i=1

U (i, )i

i=1

)
(m+u
X



= Q1 E
U (i, )


i=1
2
v

u
m+u
u X

= Q1 E
j hU (i, ), U (j, )i


i,j=1
v
u m+u
uX
Q1
E {i j hU (i, ), U (j, )i}

(20)

(21)

i,j=1

8. trivial representation U identity matrix multiplied constant show Lemma 6
risk bound (15), combined forthcoming Rademacher complexity bound (22), greater
1.

16

fiTransductive Rademacher Complexity Applications

v
v
r
um+u
u
r
uX 2
u 2 X
2
2

hU (i, ), U (i, )i = 1
kU kFro = 1
2i .
= 1
mu
mu
mu
i=1

(22)

i=1

(20) (21) obtained using, respectively, Cauchy-Schwarz Jensen inequalities. Using bound (22) conjunction Theorem 2 immediately get
data-dependent error bound algorithm, computed derive
upper bound maximal length possible values vector, appearing
ULR. Notice vanilla ULR (and thus consistency method Zhou

et al. (2004)), 1 = m. Section 6 derive tight bound 1 non-trivial ULRs
SGT Joachims (2003) consistency method Zhou et al. (2004).
bound (22) syntactically similar form corresponding inductive Rademacher
bound kernel machines (Bartlett & Mendelson, 2002). However, noted above,
fundamental difference induction, choice kernel (and therefore Hout )
must data-independent sense must selected training examples
observed. transductive setting, U Hout selected unlabeled
full-sample observed.
Rademacher bound (22), well forthcoming Rademacher bound (25), depend spectrum matrix U . see Section 6, non-trivial ULRs
transductive algorithms (the algorithms Zhou et al., 2004 Belkin et al.,
2004) spectrum U depends spectrum Laplacian graph used
algorithm. Thus transforming spectrum Laplacian control Rademacher
complexity hypothesis class. exists strong empirical evidence (see Chapelle
et al., 2003; Joachims, 2003; Johnson & Zhang, 2008) spectral transformations
improve performance transductive algorithms.
next lemma (proven Appendix F) shows trivial ULRs resulting
risk bound vacuous.


Lemma 6 Let Rm+u vector depending Sm Xu . Let c R, U = c
transductive algorithm generating soft-classification vector h = U . Let {i }ri=1
singular values U 1 upper bound kk2 . algorithm
bound (22) conjunction bound (15) vacuous; namely, (0, 1)
h generated holds
v
r
u
k
p
1
1 u
2 X 2



b
Q ln 1 .
Lm (h) +
+ c0 Q min(m, u) +

mu
2

i=1

5.2 Kernel ULR
r = + u matrix U kernel matrix (this holds U positive semidefinite),
say decomposition kernel-ULR. Let G Rm+u reproducing
kernel Hilbert space (RKHS), corresponding U . denote h, iG inner product
G. Since U kernel matrix, reproducing property9 G, U (i, j) = hU (i, ), U (j, )iG .
9. means h G I1m+u , h(i) = hU (i, ), hiG .

17

fiEl-Yaniv & Pechyony


Suppose vector satisfies U 2 2 . Let {i }m+u
i=1 eigenvalues U . similar arguments used derive (22) have:


m+u


X
X m+u
h(xi ) = Q E sup

j U (i, j)
Q E
sup


hHout i=1
j=1
i=1


m+u


X
X m+u

j hU (i, ), U (j, )iG
Q E sup


j=1
i=1

*m+u
+
m+u


X
X
U (i, ),
j U (j, )
Q E sup


i=1
j=1
G







m+u
m+u
X
X



Q E sup
U (i, )
j U (j, )
(23)




i=1
j=1
G
G



m+u

X

Q2 E
U (i, )


i=1
G
v*
+
u
m+u
u m+u

X
X
U (i, ),
j U (j, )
Q2 E


i=1
j=1
G

v
v
u m+u

u
X
uX
u m+u

j U (i, j) Q2
E {i j U (i, j)}
(24)
Q2 E


i,j=1
i,j=1
v
v
r
um+u
u
X
uX 2
u 2 m+u
2 trace(U)

2
U (i, i) = 2
= 2
.
(25)
mu
mu
mu
(

Rm+u (Hout ) =

=

=



=

=

=

=

)

m+u
X

i=1

i=1

inequalities (23) (24) obtained using, respectively, Cauchy-Schwarz Jensen
inequalities. Finally, first equality (25) follows definition Rademacher
variables (see Definition 1).
transductive algorithm kernel-ULR use (25) (22) bound
Rademacher complexity. kernel bound (25) tighter non-kernel
counterpart (22) kernel matrix eigenvalues larger one and/or 2 < 1 .
Section 6 derive tight bound 1 non-trivial ULRs consistency method
Zhou et al. (2004) Tikhonov regularization method Belkin et al. (2004).
5.3 Monte-Carlo Rademacher Bounds
show compute Monte-Carlo Rademacher bounds high confidence
transductive algorithm using ULR. empirical examination bounds
(see Section 6.3) shows tighter analytical bounds (22) (25).
technique, based simple application Hoeffdings inequality, made
particularly simple vanilla ULRs.
18

fiTransductive Rademacher Complexity Applications



1
Let V Rm+u set vectors, Q =
+ u1 , Rm+u Rademacher vector
(1), g() = supvV v. Definition 1, Rm+u (V) = Q E {g()}. Let 1 , . . . , n
i.i.d. sample Rademacher vectors.
P estimate Rm+u (V) high confidence
applying Hoeffding inequality ni=1 n1 g(i ). apply Hoeffding inequality
need bound sup |g()|, derived case V = Hout . Namely
assume V set possible outputs algorithm (for fixed Xm+u ).
Specifically, suppose v V output algorithm, v = U , assume
kk2 1 .

Definition 1, , kk2 b = + u. Let 1 . . . k singular
values U u1 , . . . , uk w1 , . . . , wk corresponding unit-length right left
singular vectors10 .


k
X



sup |g()| =
sup
| U | =
sup
ui wiT b1 k .



kk2 b, kk2 1
kk2 b, kk2 1
i=1

Applying one-sided Hoeffding inequality n samples g() have, given ,
probability least 1 random i.i.d. choice vectors 1 , . . . , n ,





n
1
X

2 ln
1
1
1
.
Rm+u (V)
+

sup U + 1 k + u
(26)
u
n
n
:kk2 1
i=1

use bound (26), value sup:kk2 1 U computed randomly drawn . computation algorithm-dependent Section 6.3 show
compute algorithm Zhou et al. (2004).11 cases compute
supremum exactly (as vanilla ULRs; see below) also get lower bound using
symmetric Hoeffding inequality.

6. Applications: Explicit Bounds Specific Algorithms
section exemplify use Rademacher bounds (22), (25) (26)
particular transductive algorithms. Section 6.1 instantiate generic ULR bound
(22) SGT algorithm Joachims (2003). Section 6.2 instantiate kernel-ULR
bound (25) algorithm Belkin et al. (2004). Finally, Section 6.3 instantiate
three bounds (22), (25) (26) algorithm Zhou et al. (2004) compare
resulting bounds numerically.
6.1 Spectral Graph Transduction (SGT) Algorithm Joachims (2003)
start description simplified version SGT captures essence
algorithm.12 Let W symmetric (m + u) (m + u) similarity matrix full-sample
10. vectors found singular value decomposition U .
11. application approach induction seems hard, impossible. example,
case RBF kernel machines need optimize (typically) infinite-dimensional vectors
feature space.
12. omit heuristics optional SGT. exclusion affect error bound
derive.

19

fiEl-Yaniv & Pechyony

Xm+u . (i, j)th entry W represents similarity xi xj . matrix W
constructed various ways, example, k-nearest neighbors graph.
graph vertex represents example full sample Xm+u . edge
pair vertices one corresponding examples among k similar
examples other. weights edges proportional similarity
adjacent vertices (points). examples commonly used measures similarity cosine
similarity RBF kernel. Let diagonal matrix, whose (i, i)th entry sum
ith row W . unnormalized Laplacian W L = W .
Let r {1, . . . , + u 1} fixed, {i , vi }m+u
eigenvectors eigenvalues L
Pr+1 2i=1
e
0 = 1 . . . m+u L = i=2 vv . Let = (1 , . . . , m+u ) vector
specifies given labels Sm ; is, {1} labeled points, = 0
otherwise. Let c fixed constant 1 (m + u) 1 vector whose entries 1
let C diagonal matrix C(i, i) = 1/m iff example training set (and
zero otherwise). soft classification h produced SGT algorithm solution
following optimization problem:
min

hRm+u

e + c(h ~ )T C(h ~ )
hT Lh

s.t. hT 1 = 0,

hT h = + u.

(27)
(28)

shown Joachims (2003) h = U , U (m + u) r matrix whose
columns vi s, 2 r + 1, r 1 vector. depends
training test sets, matrix U depends unlabeled full-sample. Substituting
h = U second constraint (28) using orthonormality
columns

U , weget + u = hT h = U U = . Hence, kk2 = + u take
1 = + u. Since U (m + u) r matrix orthonormal columns, kU k2Fro = r.
conclude (22) following bound transductive Rademacher complexity SGT


1
1
+
,
(29)
Rm+u (Hout ) 2r
u
r number non-zero eigenvalues L. Notice bound (29) oblivious
magnitude eigenvalues. small value r bound (29) small,
but, shown Joachims (2003) test error SGT bad. r increases bound
(29) increases test error improves. Joachims shows empirically smallest
value r achieving nearly optimal test error 40.
6.2 Kernel-ULR Algorithm Belkin et al. (2004)
defining RKHS induced graph (unnormalized) Laplacian, done
Herbster, Pontil, Wainer (2005), applying generalized representer theorem
Scholkopf, Herbrich, Smola (2001), show algorithm Belkin et al. (2004)
kernel-ULR. Based kernel-ULR derive explicit risk bound this.
also derive explicit risk bound based generic ULR. show former (kernel)
bound tighter latter (generic) one. Finally, compare kernel bound
risk bound Belkin et al. (2004). proofs lemmas section appear
Appendix G.
20

fiTransductive Rademacher Complexity Applications

algorithm Belkin et al. (2004) similar SGT algorithm, described
Section 6.1. Hence appendix use notation description SGT
(see Section 6.1). algorithm Belkin et al. formulated follows.
min

hRm+u

hT Lh + c(h ~ )T C(h ~ )

(30)

hT 1 = 0

(31)

s.t.

difference (30)-(31) (27)-(28) constraint (28), may change
resulting hard classification. Belkin et al. developed stability-based error bound
algorithm based connected graph. analysis follows also assume
underlying graph connected, shown end section, argument
also extended unconnected graphs.
represent full-sample labeling vector Reproducing Kernel Hilbert Space
(RKHS) associated graph Laplacian (as described Herbster et al., 2005)
derive transductive version generalized representer theorem Scholkopf et al.
(2001). Considering (30)-(31) set H = {h | hT 1 = 0, h Rm+u }. Let h1 , h2 H
two soft classification vectors. define inner product


hh1 , h2 iL = hT1 Lh2 .

(32)

denote HL set H along inner product (32). Let 1 , . . . , m+u
eigenvalues L increasing order. Since L Laplacian connected graph,
1 = 0 2 + u, 6= 0. Let ui eigenvector corresponding .
Since L symmetric, vectors {ui }m+u
i=1 orthogonal. assume also w.l.o.g.
1
1. Let
vectors {ui }m+u

orthonormal

u1 = m+u
i=1


U=

m+u
X
i=2

1
ui uTi .


(33)

Note matrix U depends unlabeled full-sample.
Lemma 7 (Herbster et al., 2005) space HL RKHS reproducing kernel
matrix U .
consequence Lemma 7 algorithm (30)-(31) performs regularization
RKHS HL regularization term khk2L = hT Lh (this fact also noted Herbster
et al., 2005). following transductive variant generalized representer theorem
Scholkopf et al. (2001) concludes derivation kernel-ULR algorithm
Belkin et al. (2004).
Lemma 8 Let h H solution optimization problem (30)-(31), let U
defined above. Then, exists Rm+u h = U .
Remark 9 consider case unconnected graph. Let number
connected components underlying graph. zero eigenvalue Laplacian
L multiplicity t. Let u1 , . . . , ut eigenvectors corresponding zero eigenvalue L. Let ut+1 , . . . , um+u eigenvectors corresponding non-zero eigenvalues
21

fiEl-Yaniv & Pechyony

t+1 , . . . , m+u L. replace constraint (31) constraints hT ui = 0 define
P
1

kernel matrix U = m+u
i=t+1 ui ui . rest analysis case
connected graph.
obtain explicit bounds transductive
Rademacher complexity algo

rithm Belkin et al. remains bound U kk2 . start bounding

U .
substitute h = U (30)-(31). P
Since u2 , . . . , um+u orthogonal u1 =
m+u 1





1
1, h 1 = U 1 =
i=2 ui ui 1 = 0. Moreover,
m+u


1
hT Lh = U LU = m+u
1 1T U = U . Thus (30)-(31) equivalent
solving
min U + c(U ~ )T C(U ~ )
(34)
Rm+u

outputting h = U , solution (34). Let 0 (m + u) 1
vector consisting zeros.
Tout U Tout U + c(U ~ )T C(U ~ )
0T U 0 + c(U 0 ~ )T C(U 0 ~ ) = c .
Thus

q
Tout U


c = 2 .

(35)

Let 1 , . . . , m+u eigenvalues U , sorted increasing order. follows
1
, 1 , . . . , m+u
(33) 1 = 0 2 + u, = m+ui+2
eigenvalues L sorted increasing order.
substitute bound (35) (25), obtain kernel bound
v
v
u
u
X
X 1
u 2c m+u
u 2c m+u

.
=
mu
mu

i=2

i=2

P
1
Suppose that13 m+u
i=2 = O(m + u). substitute kernel bound (15)
obtain probability least 1 random training/test partition,

!
1

b
.
(36)
Lu (h) Lm (h) + p
min(m, u)
briefly compare bound risk bound algorithm (30)-(31) given
Belkin et al. (2004). Belkin et al. provide following bound algorithm14 .
probability least 1 random draw training examples Xm+u ,


1

b
Lm+u (h) Lm (h) +
.
(37)

13. assumption restricting since define matrix L spectrum observing
unlabeled full-sample. Thus set L way assumption hold.
14. original bound Belkin et al. terms squared loss. equivalent bound terms 0/1
margin loss obtained derivation paper Belkin et al. (2004).

22

fiTransductive Rademacher Complexity Applications

Similarly done Section 4.3, bring bounds common denominator,
rewrite bound (36)

!
u
1
Lu (h) Lbm (h) +
p
.
(38)
m+u
min(m, u)
u u bounds (37) (38) convergence rate. However

u convergence rate (38) (which O(1/(m u))) much faster

one (37) (which O(1/ m)).
6.3 Consistency Method Zhou et al. (2004)
section instantiate bounds (22), (25) (26) consistency method
Zhou et al. (2004) provide numerical comparison.
start brief description Consistency Method (CM) algorithm Zhou
et al. (2004). algorithm natural vanilla ULR (see definition beginning
Section 5), matrix U computed follows. Let W matrices

SGT (see Section 6.1). Let L = D1/2 W D1/2 parameter (0, 1). Then,

U = (1 )(I L)1 output CM h = U , specifies given

labels. Consequently kk2 = m. following lemma, proven Appendix H, provides
characterization eigenvalues U :
Lemma 9 Let max min be, respectively, largest smallest eigenvalues U .
max = 1 min > 0.
follows Lemma 9 U positive definite matrix hence also kernel
matrix. Therefore, decomposition
U kernel-ULR. apply kernel
bound (25) compute bound 2 U . Rayleigh-Ritz theorem (Horn

& Johnson, 1990), TU max . Since definition vanilla ULR,
p


=
= m, obtain U max

max m.


obtained 1 = 2 = max m, max maximal eigenvalue
U . Since Lemma 9 max = 1, CM algorithm bound (22) always tighter
(25).
turns CM, exact value supremum (26) analytically
derived. Recall vectors , induce CM hypothesis space particular
U , exactly components values {1}; rest components zeros.

Let set possible s. Let t(i ) = (t1 , . . . , tm+u ) = U R1(m+u)

|t(i )| = (|t1 |, . . . , |tm+u |). Then, fixed , sup U sum
largest elements |t(i )|. derivation holds vanilla ULR.
demonstrate Rademacher bounds discussed paper present empirical
comparison bounds two datasets (Voting, Pima) UCI repository15 .
dataset took + u size dataset (435 768 respectively)
took 1/3 full-sample size. matrix W 10-nearest neighbors graph
computed cosine similarity metric. applied CM algorithm = 0.5.
Monte-Carlo bounds (both upper lower) computed = 0.05 n = 105 .
15. also obtained similar results several UCI datasets.

23

fiEl-Yaniv & Pechyony

Pima Dataset
Bound Transductive Rademacher

Bound Transductive Rademacher

Voting Dataset
1.4
1.2
1
0.8
0.6
Kernel ULR bound

0.4

Generic ULR bound
Upper Monte Carlo bound

0.2
0
0

Lower Monte Carlo
50

100

150

200

250

300

350

1.4
1.2
1
0.8
0.6
Kernel ULR bound
0.4

Upper Monte Carlo bound
0.2
0
0

400

Generic ULR bound

Lower Monte Carlo
100

200

300

400

500

600

700

Number Eigenvalues/Singular values

Number Eigenvalues/Singular values

Figure 1: comparison transductive Rademacher bounds.
compared upper lower Mote-Carlo bounds generic ULR bound (22)
kernel-ULR bound (25). graphs Figure 1 compare four bounds
datasets function number non-zero eigenvalues U . Specifically,
point x-axis corresponds bounds computed matrix Ut approximates U using smallest eigenvalues U . examples lower upper
Monte-Carlo bounds tightly sandwich true Rademacher complexity. striking
generic-ULR bound close true Rademacher complexity. principle,
simple Monte-Carlo method approximate true Rademacher complexity
desired accuracy (with high confidence) cost drawing sufficiently many
Rademacher vectors.

7. PAC-Bayesian Bound Transductive Mixtures
section adapt part results Meir Zhang (2003) transduction.
proofs results presented section appear Appendix I.
|B|
Let B = {hi }i=1 finite set base-hypotheses. class B formed
observing full-sample Xm+u , obtaining training/test set partition
P|B|
labels. Let q = (q1 , . . . , q|B| ) R|B| probability vector, i.e. i=1 qi = 1 qi 0
1 |B|. vector q computed observing training/test partition
training labels. goal find posterior vector
P q
mixture
P
P

|B|
|B|
m+u
1
eq =
e
hypothesis h
j=m+1 `
i=1 qi hi (j), yj .
i=1 qi hi minimizes Lu (hq ) = u
section derive uniform risk bound set qs. bound depends
KL-divergence (see definition below) q prior probability vector
p R|B| , vector p defined based unlabeled full-sample. Thus
forthcoming bound (see Theorem 4) belongs family PAC-Bayesian bounds
(McAllester, 2003; Derbeko et al., 2004), depend prior posterior information.
Notice bound, different
PAC-Bayesian bounds Gibbs classifiers
1 Pm+u
bound EhB(q) Lu (h) = u j=m+1 EhB(q) `(h(j), yj ), h B(q) random draw
base hypothesis B according distribution q.
24

fiTransductive Rademacher Complexity Applications

e q ) EhB(q) Lu (h).
Remark 10 one reviewers noted, Jensen inequality Lu (h
Hence risk bound transductive Gibbs classifier holds true also transductive mixture classifier. Currently known risk bound transductive Gibbs classifiers (Theorem 18
paper Derbeko et al., 2004) diverges u . forthcoming risk bound
(41) deficiency.
assume q belongs domain g,A = {q | g(q) A}, g : R|B|
R predefined function R constant. domain g,A set B


e q . Recalling Q =
induce class Beg,A possible mixtures h
(1/m + 1/u), =
p

m+u
32 ln(4e)/3 < 5.05, apply Theorem 2 Hout =
(m+u0.5)(10.5/ max(m,u)) c0 =
Beg,A obtain probability least 1 training/test partition
e q Beg,A ,
Xm+u , h
r
p
Rm+u (Beg,A )

1
e
e
b
Lu (hq ) Lm (hq ) +
+ c0 Q min(m, u) +
Q ln .
(39)

2

q

Let Q1 = S2 Q (ln(1/) + 2 ln logs (sg(q)/g0 )). straightforward apply technique
used proof Theorem 10 Meir Zhang (2003) obtain following bound,
eliminates dependence A.
Corollary 2 Let g0 > 0, > 1 g(q) = max(g(q), g0 ). fixed g > 0,
eq,
probability least 1 training/test set partition, all16 h
e q ) Lb (h
e
Lu (h
q) +

p
Rm+u (Beg,g(q) )
+ c0 Q min(m, u) + Q1 .


(40)

instantiate Corollary 2 g(q)
KL-divergence derive PAC-Bayesian
P|B|

bound. Let g(q) = D(qkp) = i=1 qi ln pqii KL-divergence p q. Adopting
Lemma 11 Meir Zhang (2003) transductive Rademacher variables, defined
(1), obtain following bound.
Theorem 4 Let g0 > 0, > 1, > 0. Let p q prior posterior distribution


B, respectively. Set g(q) = D(qkp) g(q) = max(g(q), g0 ). Then, probability
eq,
least 1 training/test set partition, h
r
p
e q ) Lb (h
e q ) + Q 2g(q) sup khk2 + c0 Q min(m, u) + Q1 .
Lu (h
(41)

2

hB
Theorem 4 PAC-Bayesian result, prior p depend Xm+u posterior optimized adaptively, based
p also Sm . general bound (15), bound
(41) convergence rate O(1/ min(m, u)). bound (41) syntactically similar
inductive PAC-Bayesian bound mixture hypothesis (see Theorem 10 Lemma 11

paper Meir & Zhang, 2003), similar convergence rate O(1/ m). However
conceptual difference inductive transductive bounds transduction
define prior vector p observing unlabeled full-sample induction
define p observing data.
eg,eg(q) ) follows:
16. bound (40) meaning Rm+u (B

Rm+u (Beg,g(q) ) = Rm+u (Beg,A ).

25

q, let = ge(q)

fiEl-Yaniv & Pechyony

8. Concluding Remarks
studied use Rademacher complexity analysis transductive setting.
results include first general Rademacher bound soft classification algorithms,
unlabeled-labeled representation (ULR) technique bounding Rademacher complexity
transductive algorithm bound Bayesian mixtures. demonstrated
usefulness results and, particular, effectiveness ULR framework
deriving error bounds several advanced transductive algorithms.
would nice improve bounds using, example, local Rademacher
approach Bartlett, Bousquet, Mendelson (2005). However, believe main
advantage transductive bounds possibility selecting hypothesis space
based full-sample. clever data-dependent choice space provide sufficient
flexibility achieve low training error low Rademacher complexity. opinion
opportunity explored exploited much further. particular, would
interesting develop efficient procedure choice hypothesis space learner
knows properties underlying distribution (e.g., clustering assumption holds).
work opens new avenues future research. example, would interesting
optimize matrix U ULR explicitly (to fit data) constraint low
Rademacher complexity. Also, would nice find low-Rademacher approximations
particular U matrices. PAC-Bayesian bound mixture algorithms motivates
development use transductive mixtures, area yet investigated.
Finally, would interesting utilize bounds model selection process.

Acknowledgments
grateful anonymous reviewers insightful comments. also thank
Yair Wiener Nati Srebro fruitful discussions. Dmitry Pechyony supported
part IST Programme European Community, PASCAL Network
Excellence, IST-2002-506778.

Appendix A. Proof Lemma 1
proof based technique used proof Lemma 5 paper Meir
Zhang (2003). Let = (1 , . . . , m+u )T Rademacher random variables
Rm+u (V, p1 ) = (1 , . . . , m+u )T Rademacher random variables Rm+u (V, p2 ).
real-valued function g(v), n I1m+u v0 V,

)


sup [g(v)] = En
n 6= 0 .

vV
(42)


useP
abbreviation 1s = 1 , . . . , . apply (42) fixed 1n1 g(v) =
f (v) + n1
i=1 vi , obtain

)
(


n vn0 + sup [g(v)] n 6= 0 En sup [n vn + g(v)]

vV
vV

(

sup
vV

"n1
X
i=1

#
vi + f (v) En

(
sup
vV

26

" n
X
i=1

)
#


vi + f (v) n 6= 0 .


(43)

fiTransductive Rademacher Complexity Applications

complete proof lemma, prove general claim: real-valued
function f (v), 0 n + u,
(
" n
#)
(
" n
#)
X
X
E sup
vi + f (v)
E sup
vi + f (v)
.
(44)
vV

vV

i=1

i=1

proof induction n. claim trivially holds n = 0 (in case (44) holds
equality). Suppose claim holds k < n functions f (v). use

abbreviation 1s = 1 , . . . , . function f 0 (v)
" n
#
X
0
vi + f (v)
E1n sup
vV

(

i=1

"n1
#
"n1
#)
X
X
1
1
= 2p1
vi + vn + f 0 (v) + En1 sup
vi vn + f 0 (v)
E n1 sup
2 1 vV
2 1 vV
i=1
i=1
"n1
#
X
+ (1 2p1 ) En1 sup
vi + f 0 (v)
1

(
2p1

vV

"n1
X

1
E n1 sup
2 1 vV

1
+ E n1 sup
2 1 vV
(
(

i=1

#

vi + vn + f 0 (v)

i=1
"n1
X

(45)
#)

0

vi vn + f (v)

i=1

#


vi + f 0 (v)
= E n1 2p1 En sup
1

vV i=1
" n
#
(

(
X
vi + f 0 (v)
= E n1 2p1 En sup
1

" n
X

vV

+ sup
vV

"n1
X
i=1

i=1

+ (1 2p1 ) E n1 sup
1

)
n 6= 0

vV

+ (1 2p1 ) sup
vV

"n1
X

#
vi + f (v)

i=1

"n1
X

#)
vi + f 0 (v)

i=1


)
"n1
#!

X

0
vi + f (v)
n 6= 0 sup

vV
i=1

#)

vi + f 0 (v)

#
)
"n1
#!

X

0
2p2 En sup
vi + f (v) n =
6 0 sup
vi + f (v)

vV i=1
vV i=1
"n1
#)
X
+ sup
vi + f 0 (v)

(
E n1
1

1

= E1n sup
vV

vV

(

i=1

" n
X

0

)
#


0
2p2 En sup
vi + f (v) n 6= 0

vV i=1
"n1
#)
X
+ (1 2p2 ) sup
vi + f 0 (v)

(
= E n1



" n
X

(

0

" n
X

vV

#

i=1

vi + f 0 (v) .

i=1

27

(46)

fiEl-Yaniv & Pechyony

inequality (45) follow inductive hypothesis, applied thrice f (v) = vn +
f 0 (v), f (v) = vn + f 0 (v) f (v) = f 0 (v). inequality (46) follows (43)
fact p1 < p2 .

Appendix B. Proof Lemma 2


require following standard definitions facts martingales.17 Let Bn1 =

(B1 , . . . , Bn ) sequence random variables bn1 = (b1 , . . . , bn ) respective

w.r.t.
values. sequence W0n = (W0 , W1 , . . . , Wn ) called martingale

underlying
sequence Bn1 I1n , Wi function Bi1 EBi Wi |B1i1 = Wi1 .


Let f (Xn1 ) = f (X1 , . . . , Xn ) arbitrary function n (possibly dependent) random




variables. Let W0 = EXn1 {f (Xn1 )} Wi = EXn1 f (Xn1 )|Xi1 I1n . elementary fact W0n martingale w.r.t. underlying sequence Xn1 . Thus obtain
martingale function (possibly dependent) random variables. routine
obtaining martingale arbitrary function called Doobs martingale process.
definition Wn Wn = EXn1 {f (Xn1 )|Xn1 } = f (Xn1 ). Consequently, bound
deviation f (Xn1 ) mean sufficient bound difference Wn W0 .
fundamental inequality, providing bound, McDiarmids inequality (McDiarmid,
1989).
Lemma 10 (McDiarmid, 1989, Corollary 6.10) Let W0n martingale w.r.t. Bn1 .
Let bn1 = (b1 , . . . , bn ) vector possible values random variables B1 , . . . , Bn .
Let





i1
i1
ri (bi1
= bi1
= b1i1 , Bi = bi .
1 ) = sup Wi : B1
1 , Bi = bi inf Wi : B1
bi

bi



Let r2 (bn1 ) =

Pn

i1 2
i=1 (ri (b1 ))



rb2 = supbn1 r2 (bn1 ). Then,



22
PBn1 {Wn W0 > } < exp 2
.
rb

(47)

inequality (47) improved version Hoeffding-Azuma inequality (Hoeffding,
1963; Azuma, 1967).
proof Lemma 2 inspired McDiarmids proof bounded difference
inequality permutation graphs (McDiarmid, 1998, Section 3). Let W0m+u martingale



)
obtained f (Z) Doobs martingale process, namely W0 = EZm+u f (Zm+u
1
1



. compute upper bound r
2 apply Lemma 10.
)|Z
b
Wi = EZm+u f (Zm+u
1
1
1

Fix i, I1m . Let m+u
= 1 , . . . , m+u specific permutation I1m+u i0
1






m+u
mi

{i+1 , . . . , m+u }. Let p1 = PjI m+u j Ii+1
= m+ui
p2 = PjI m+u j Im+1
=
i+1

i+1

17. See, e.g., Chapter 12 Grimmett Stirzaker (1995), Section 9.1 Devroye et al. (1996)
details.

28

fiTransductive Rademacher Complexity Applications

1 p1 =

u
m+ui .







1i1 ) = sup Wi : Bi1
Wi : B1i1 = 1i1 , Bi =
ri (
= i1
1
1 , Bi = inf







i1
i1
= sup EZ f (Z) | Z1 = 1 , Zi = EZ f (Z) | Zi1
= 1i1 , Zi = i0
1
,i0

n


sup EjI m+u EZ f (Z) | Zi1
= 1i1 , Zi = , Zj = i0
1

=

=
=

,i0

i+1

,i0

i+1



EjI m+u EZ f (Zij ) | Zi1
= i1
, Zi = , Zj = i0
1
1
i+1
n


0
sup EjI m+u EZ f (Z) f (Zij ) | Z1i1 = i1
1 , Zi = , Zj =

(48)

n


0

sup p1 EZ,jIi+1
f (Z) f (Zij ) | Z1i1 = i1
1 , Zi = , Zj =

(49)

,i0



0
+ p2 EZ,jI m+u f (Z) f (Zij ) | Z1i1 = i1
1 , Zi = , Zj =
m+1

Since f (Z) (m, u)-permutation symmetric function, expectation (49) zero. Therefore,
i1
ri (
1 ) =


n


sup p2 EZ,jI m+u f (Z) f (Zij ) | Z1i1 = 1i1 , Zi = , Zj = i0
m+1

,i0

u
.
m+ui

m+u
Since f (Z) (m, u)-permutation symmetric, also follows (48) Im+1
,
R
j+1/2 1
i1
1
1 ) = 0. verified j > 1/2, j 2 j1/2 t2 dt, therefore,
ri (

2

rb

=

sup

m+u
X

m+u
i=1
1

Z

u2 2


X


i1 2
1 )
ri (

m+u1/2

u1/2

i=1



u
m+ui

2

2 2

=u

m+u1
X
j=u

1
j2

mu2 2

1
dt =
.
t2
(u 1/2)(m + u 1/2)

(50)

applying Lemma 10 bound (50) obtain


22 (u 1/2)(m + u 1/2)
PZ {f (Z) EZ {f (Z)} } exp
.
mu2 2

(51)

entire derivation symmetric u. Therefore, also


22 (m 1/2)(m + u 1/2)
PZ {f (Z) EZ {f (Z)} } exp
.
m2 u 2

(52)

taking tightest bound (51) (52) obtain statement lemma.
29

fiEl-Yaniv & Pechyony

Appendix C. Proof Lemma 3
consider following algorithm18 (named RANDPERM) drawing first elements
m+u
{Zi }m
:
i=1 random permutation Z I1
Let Zi = I1m+u .
2: = 1
3:
Draw di uniformly Iim+u .
4:
Swap values Zi Zdi .
5: end
Algorithm 1: RANDPERM - draw first elements random permutation + u
elements.
1:

algorithm RANDPERM abridged version procedure drawing random
permutation n elements drawing n1 non-identically distributed independent random
variables, presented Section 5 paper Talagrand (1995) (which according
Talagrand due Maurey, 1979).
Lemma 11 algorithm RANDPERM performs uniform draw first elements
Z1 , . . . , Zm random permutation Z.
Proof: proof induction m. = 1, single random variable d1
uniformly drawn among Im+u , therefore, Z1 uniform distribution I1m+u . Let

dm
1 = d1 , . . . , dm . Suppose claim holds m1 < m. two possible values

0m 0
0

1 = 1 , . . . , 1 = 1 , . . . , Z1 , . . . , Zm ,
m1

Pdm
{Zm
= m1
} Pdm {Zm = | Zm1
= m1
}
1 = 1 } = Pdm1 {Z1
1
1
1
1
1

1
(53)
u+1
0
= 0m1
} Pdm {Zm =
| Zm1
= 0m1
}
1
1
1

= Pdm1 {Zm1
= 0m1
}
1
1
1

= Pdm1 {Zm1
1
1

0m
= Pdm
{Zm
1 = 1 } .
1

equality (53) follows inductive assumption definition dm .



Consider (m, u)-permutation symmetric function f = f (Z) random permutations Z. Using algorithm RANDPERM represent random permutation Z
function g(d) independent random variables. value function g(d)
output algorithm RANDPERM operated values random draws given d.
next lemma relates Lipschitz constant function f (g(d)) Lipschitz
constant f (Z):
18. Another algorithm generating random permutation independent draws presented Appendix B Lanckriet et al. (2004). algorithm draws random permutation means drawing
+ u independent random variables. Since deal (m, u)-permutation symmetric functions,
interested first elements random permutation. algorithm Lanckriet
et al. needs + u draws independent random variables define elements. algorithm RANDPERM, presented section, needs draws. use algorithm Lanckriet
et al. instead RANDPERM, forthcoming bound (55) would term + u instead m.
change, turn, would result non-convergent risk bound derived using techniques.

30

fiTransductive Rademacher Complexity Applications

Lemma 12 Let f (Z) (m, u)-permutation symmetric function random permutation
m+u
Z. Suppose I1m , j Im+1
, |f (Z) f (Zij )| . Let d0i independent
draw random variable di . I1m ,
|f (g(d1 , . . . , di1 , di , di+1 , . . . , dm )) f (g(d1 , . . . , di1 , d0i , di+1 , . . . , dm ))| .


(54)



Proof: values = (d1 , . . . , di , . . . , dm ) d0 = (d1 , . . . , d0i , . . . , dm ) induce, re0m
0
0
spectively, first values19 Zm
1 = {Z1 , . . . , Zm } Z1 = {Z1 , . . . , Zm } two
m+u
dependent permutations I1 . Since f (m, u)-permutation symmetric, value
0
uniquely determined value Zm
1 . prove change di di results

ij
change single element Z1 . Combined property |f (Z) f (Z )| ,
conclude proof (54).
refer d0 as, respectively, old new draws. Consider operation
RANDPERM draws d0 . Let , di d0i values of, respectively,
Zi , Zdi Zd0i ith iteration RANDPERM. Note di d0i i.
old permutation, ith iteration Zi = di , Zdi = Zd0i = d0i .
new permutation, ith iteration Zi = d0i , Zdi = di Zd0i = . ith
iteration RANDPERM value Zi remains intact. However values Zdi Zd0i
may change. particular values di may among Zi+1 , . . . , Zm end
run RANDPERM. four cases:
0m

/ Z0m
/ Z0m
Case 1 d0i
/ Zm
/ Zm
1 ,
1 Z1 = Z1 \{di } {d0i }.
1
1 di

0m
0m
0m

Case 2 d0i Zm
1 Z1 di Z1 , Z1 Z1 = Z1 .
0m
0m

Case 3 Zm
/ Zm
/ Z0m
1 d0i
1 di Z1 ,
1 Z1 = Z1 \{i } {d0i }.
0m

0m
/ Z0m
Case 4 d0i Zm
/ Zm
1 Z1 = Z1 \{di } {i }.
1
1 Z1 , di


apply bounded difference inequality McDiarmid (1989) f (g(d)) obtain


22
Pd {f (g(d)) Ed {f (g(d))} } exp 2
.
(55)

Since f (Z) (m, u)-permutation symmetric, follows (55)


22
.
PZ {f (Z) EZ {f (Z)} } exp 2

Since entire derivation symmetric u also


22
PZ {f (Z) EZ {f (Z)} } exp 2
.
u

(56)

(57)

proof Lemma 3 completed taking minimum bounds (56) (57).
19. notational convenience section, refer Zm
1 set values vector values
(as done sections).

31

fiEl-Yaniv & Pechyony

Appendix D. Proof Claims Lemma 4
Proof Claim 1. Note N1 N2 random variables whose distribution
induced distribution . (9)
Rm+u (V) = EN1 ,N2 ERad(N1 ,N2 ) sup

m+u
X

vV i=1

(i,1 + i,2 ) v(i) = EN1 ,N2 s(N1 , N2 ) .

Proof Claim 2. definitions Hk Tk (appearing start Section 4.1),
N1 , N2 I1m+u
h

EZ,Z0 sup TN1 {v(Z)} TN2 {v(Z0 )} + HN2 {v(Z0 )} HN1 {v(Z)} =
vV

#
N2
N1
m+u
m+u
X
X
1 X
1
1
1 X
v(Zi )
v(Zi0 ) +
v(Zi0 )
v(Zi ) . (58)
EZ,Z0 sup
u


vV u i=N +1
i=1
i=1
i=N
+1
1
2
|
{z
}
"



=r(v,Z,Z0 ,N1 ,N2 )

values N1 N2 , distribution Z Z0 , respect
take

1 1
1
expectation (58), induce distribution assignments coefficients
,
, u , u1
components v. N1 , N2 realizations Z Z0 , component v(i),
I1m+u , assigned exactly two coefficients, one two permutations (Z


Z0 ). Let = (a1 , . . . , am+u ), ai = (ai,1 , ai,2 ) pair coefficients.
m+u
I1 , pair (ai,1 , ai,2 ) takes values coefficients v(i), first
1
u1 ) second
component induced realization Z (i.e., ai,1 either
1
1
0
component realization Z (i.e., ai,2 either u ).
Let A(N1 , N2 ) distribution vectors a, induced distribution Z Z0 ,
particular N1 , N2 . Using definition write
" m+u
#
X
(58) = EaA(N1 ,N2 ) sup
(ai,1 + ai,2 )v(i) .
(59)
vV

i=1

Let Par(k) uniform distribution partitions m+u elements two subsets,

k

+ u k elements, respectively. Clearly, Par(k) uniform distribution
m+u
elements. distribution random vector (a1,1 , a2,1 , . . . , am+u,1 )
k
first elements pairs equivalent Par(N1 ). is, vector obtained
1
corresponding
taking first N1 indices realization Z assigning
1
components. components assigned u . Similarly, distribution
random vector (a1,2 , a2,2 , . . . , am+u,2 ) equivalent Par(N2 ). Therefore, distribution
A(N1 , N2 ) entire vector equivalentto product
Par(N1 )
distribution

m+u
m+u
Par(N2 ), uniform distribution

elements,
N1
N2
element pair independent permutations.
show distributions Rad(N1 , N2 ) A(N1 , N2 ) identical. Given N1
N2 setting = (m+u)2 , probability drawing specific realization (satisfying
32

fiTransductive Rademacher Complexity Applications

n1 + n2 = N1 n2 + n3 = N2 )


m2


n2

mu N1 n2 mu N2 n2





u2


m+uN1 N2 +n2
=

mN1 +N2 u2(m+u)N1 N2
. (60)
(m + u)2(m+u)

Since (60) independent ni s, distribution Rad(N1 , N2 ) uniform
possible Rademacher assignments satisfying constraints N1 N2 . easy see
support size Rad(N1 , N2 ) support size A(N1 , N2 ). Moreover,
support sets distributions identical; hence distributions identical.
Therefore, follows (59)
(
(58) = ERad(N1 ,N2 )

" m+u
#)
X
sup
(i,1 + i,2 )v(i)
= s(N1 , N2 ) .

vV

i=1

easy see E N1 = E {n1 + n2 } = E N2 = E {n2 + n3 } = m. Since
EZ {(Z)} (58) N1 = N2 = m,
(
EZ {(Z)} = ERad(m,m)

sup
vV

"m+u
X

#)
(i,1 + i,2 ) v(i)

= (E N1 , E N2 ) .

i=1

Proof Claim 3.
bound differences |s(N1 , N2 ) (N10 , N2 ) | |s(N1 , N2 ) (N1 , N20 ) |
1 N1 , N2 , N10 , N20 + u. Suppose w.l.o.g. N10 N1 . Recalling definition
r() (58)
"
#
s(N1 , N2 ) = EZ,Z0 sup r(v, Z, Z0 , N1 , N2 )
vV

"



s(N10 , N2 ) = EZ,Z0 sup r(v, Z, Z0 , N1 , N2 ) +
vV

1
1
+
u

X
N1

#
v(Zi ) .

(61)

i=N10 +1

expressions supremums s(N1 , N2 ) s(N10 , N2 ) differ two
terms (61). Therefore, N1 N10 ,






s(N1 , N2 ) s(N10 , N2 ) Bmax N1 N10 1 + 1
.
(62)
u
Similarly N2 N20 ,




s(N1 , N2 ) s(N1 , N20 ) Bmax N2 N20



1
1
+
u


.

(63)

use following Bernstein-type concentration inequality (see Devroye et al., 1996,
Problem
8.3)
binomial random variable X Bin(p, n): P
X {|X EX|
> t} <
1

3t2
1
2 exp 8np . Abbreviate Q = + u . Noting N1 , N2 Bin m+u , + u , use
33

fiEl-Yaniv & Pechyony





(62), (63) Bernstein-type inequality (applied n = + u p =
obtain


m+u )



PN1 ,N2 {|s(N1 , N2 ) s(E {N1 } , E {N2 })| }
PN1 ,N2 {|s(N1 , N2 ) s(N1 , E N2 )| + |s(N1 , E N2 ) s(E N1 , E N2 )| }
n

PN1 ,N2 |s(N1 , N2 ) s(N1 , E N2 )|
2
n

+PN1 ,N2 |s(N1 , E N2 ) s(E N1 , E N2 )|
2
n
n


PN2 |N2 E N2 | Bmax Q
+ PN1 |N1 E N1 | Bmax Q
2 !
2



2
2
3
3
4 exp
= 4 exp
.

2
2
2 Q2
32(m + u) m+u Bmax Q
32mBmax
Next use following fact (see Devroye et al., 1996, Problem 12.1): nonnegative
random variable X satisfies P{X > t} c exp(kt2 ) c 1 k > 0,
p


EX ln(ce)/k. Using fact, along c = 4 k = 3/(32mQ2 ),
|EN1 ,N2 {s(N1 , N2 )} s(E N1 , E N2 )| EN1 ,N2 |s(N1 , N2 ) s(E N1 , E N2 )|



32 ln(4e)
1
1 2
2

mBmax
+
.
3
u

Appendix E. Proof Lemma 5
proof straightforward extension proof Lemma 5 Meir Zhang
(2003) also similar proof Lemma 1 Appendix A. prove stronger
claim: I1m+u v, v0 V, |f (vi )f (vi0 )| |g(vi )g(vi0 )|, function
e
c : Rm+u R.
"m+u
#
"m+u
#
X
X
f (vi ) + e
c(v) E sup
g(vi ) + e
c(v) .
E sup
vV

vV

i=1

i=1



use abbreviation 1n = 1 , . . . , n . proof induction n,
0 n + u. lemma trivially holds n = 0. Suppose lemma holds n 1.
words, function c(v),
"
En1 sup c(v) +
1



Let p =

mu
.
(m+u)2

vV

n1
X

#

"

f (vi ) En1 sup c(v) +
1

i=1

vV

n1
X

#
g(vi )

.

i=1


"



= E1n sup c(v) +
vV

n
X

#

"

f (vi ) = En En1 sup c(v) +
1

i=1

34

vV

n
X
i=1

#
f (vi )

(64)

fiTransductive Rademacher Complexity Applications

(
= pEn1

"
sup c(v) +

1

vV

n1
X

#

"

f (vi ) + f (vn ) + sup c(v) +
vV

i=1

n1
X

#)
f (vi ) f (vn )

i=1

"

+(1 2p)En1 sup c(v) +
1

n1
X

vV

(65)
#

f (vi )

.

(66)

i=1

apply inductive hypothesis three times: first second summands (65)



c(v) = c(v)+f (vn ) c(v) = c(v)f (vn ), respectively, (66) c(v) = c(v).
obtain
(
"
#
"
#)
n1
n1
X
X
pEn1 sup c(v) +
g(vi ) + f (vn ) + sup c(v) +
g(vi ) f (vn )
1

vV

|

i=1

vV

{z

i=1

}



=B

"
+ (1 2p)En1 sup c(v) +
1

|

vV

#

n1
X

g(vi )

i=1

{z

.
}



=C

expression B written follows.
"
#)
"
#
(
n1
n1
X
X
0
0
0
g(vi ) f (vn )
g(vi ) + f (vn ) + sup c(v ) +
B = pEn1 sup c(v) +
1

vV

c(v) + c(v0 ) +

= pEn1 sup
1

v,v0 V

"
0

= pEn1 sup
1

v0 V

i=1

"

c(v) + c(v ) +

v,v0 V

i=1

n1
Xh


(g(vi ) + g(vi0 )) + (f (vn ) f (vn0 ))

i=1
n1
Xh



(g(vi ) +

g(vi0 ))



+ f (vn ) f (vn0 )

#
#
.

(67)

i=1

P
0
equality (67) holds since expression c(v)+c(v0 )+ n1
i=1 (g(vi )+g(vi )) symmetric
0
0
v v . Thus, f (v) < f (v ) exchange values v v0
increase value expression supremum. Since |f (vn ) f (vn0 )|
|g(vn ) g(vn0 )|
"
#
n1

Xh
0
0
0
B pEn1 sup c(v) + c(v ) +
(g(vi ) + g(vi )) + |g(vn ) g(vn )|
1

v,v0 V

"
c(v) + c(v0 ) +

= pEn1 sup
1

v,v0 V

(
= pEn1
1

n1
X

#



(g(vi ) + g(vi0 )) + (g(vn ) g(vn0 ))

i=1

"

sup c(v) +
vV

i=1
n1
Xh

#

"

n1
X

g(vi ) + g(vn ) + sup c(v) +
vV

i=1

Therefore, using reverse argument (64)-(66),
"
C + = E1n sup c(v) +
vV

35

n
X
i=1

i=1

#
g(vi )

.

#)
g(vi ) g(vn )



= D.

fiEl-Yaniv & Pechyony

Appendix F. Proof Lemma 6


Let c R, U = c I. c = 0, soft classification generated constant zero.
case, h generated A, Lbm (h) = 1 lemma holds.
Suppose c 6= 0.
1
= h .
(68)
c
Since (m + u) (m + u) matrix U + u singular values, one precisely c,
(22) Rademacher complexity trivial ULR bounded

r

2
1
1
2
.
(69)
1
(m + u)c = c1 2
+
mu
u
assume w.l.o.g. training points indices 1 m. Let = {i
I1m | yi h(i) > 0 |h(i)| > } set indices training examples zero margin
loss. Let B = {i I1m | |h(i)| [, ]} C = {i I1m | yi h(i) < 0 |h(i)| > }.
(68) definition sets A, C, C, |i | > c . Similarly,
B, |i | = |h(i)|
c . obtain bound (69) least

r
2 X h(i)2 1
c (|A| + |C|) 2 +
.
c
c2

iB

Therefore, risk bound (15) bounded
r

X
2
2
2
(|A| + |C|) +
h(i)

iB

r
P
X h(i)2
2
iB (1 |h(i)|/) + |C|
+ |A| + |C| +

2



iB
r
P

X
|B| + |C| iB ri
2
+ |A| + |C| +
ri2


iB
r
P

X
|A| iB ri
2
2
+ |A| + |C| +
ri


1
Lbm (h) +




=
=


= ,

iB

ri = |h(i)|
. prove 1. Equivalently, sufficient prove
ri1 , . . . , ri|B| [0, 1]|B| holds


f ri1 , . . . , ri|B| =

P
(|A| + iB ri )2
P
.
|A| + |C| + iB ri2

claim stronger statement holds:

(|A| + |C| + P
ri )2
PiB 2 .
f ri1 , . . . , ri|B| =
|A| + |C| + iB ri
36

(70)

fiTransductive Rademacher Complexity Applications

prove (70) use Cauchy-Schwarz inequality, stating two vectors a, b

Rm , ha, bi kak2 kbk2 . set bi = 1 I1m . vector set follows: ai = ri
B ai = 1 otherwise. definition b, ha, bi 0
thus (ha, bi)2 kak22 kbk22 . application inequality defined vectors
b results inequality (70).

Appendix G. Proofs Section 6.2
Proof Lemma 7: Let ei (m + u) 1 vector whose ith entry equals 1
entries zero. According definition RKHS, need show
1 + u, h(i) = hU (i, ), hiL .
hU (i, ), hiL = U (i, )Lh = ei U Lh
! m+u
!
m+u
!
m+u
X
X
X 1





ui ui
ui ui h = ei
ui ui h
= ei

i=2
i=1
i=2


1
= eTi (I u1 uT1 )h = eTi
1 1T h = h(i) .
m+u

Lemma 13 1 + u, U (i, ) HL .
Proof: Since L Laplacian matrix, u1 = 1. Since vectors {ui }m+u
i=1 orthonormal
Pm+u 1

1 = 0. Therefore, 1 + u,
u1 = 1, U 1 =
i=2 ui ui
U (i, ) 1 = 0.


p

Proof Lemma 8: Let khkL = hh, hiL = hT Lh norm GL . optimization
problem (30)-(31) stated following form:
min

hHL

khk2L + c(h ~ )T C(h ~ ) .

(71)


Let U HL vector space spanned vectors {U (i, )}m+u
i=1 . Let hk =
(i,)iL
+ u, = hh,U
kU (i,)kL .
verified h

projection h onto U. 1
part h perpendicular U.
1 + u, hh , U (i, )iL = 0. 1 + u
m+u
X

h(i) = hh, U (i, )iL = h

Pm+u
i=1

U (i, )

Let h = h hk
HL

j U (j, ), U (i, )iL + hh , U (i, )iL

j=1

=

m+u
X

j hU (j, ), U (i, )iL =

j=1

m+u
X

j U (i, j) = hk (i) .

(72)

j=1

second equation (72) holds Lemma 13. consequence (72), empirical
error (the second term (71)) depends hk . Furthermore,
hT Lh = hh, hiL = khk2L = k

m+u
X

U (i, )k2L + kh k2L k

i=1

m+u
X
i=1

37

U (i, )k2L .

fiEl-Yaniv & Pechyony

Therefore, h H minimizes (71), h = 0 h = hk =


Pm+u
i=1

U (i, ) = U .

Appendix H. Proof Lemma 9


Let LN = L = D1/2 W D1/2 normalized Laplacian W . eigenvalues
0
{0i }m+u
i=1 LN non-negative smallest eigenvalue LN , denoted min ,
zero (Chung, 1997). eigenvalues matrix L = (1 )I + LN
{1 + 0i }m+u
L strictly positive.
i=1 . Since 0 < < 1, eigenvalues
om+u
n
1
. Finally,
Hence matrix L invertible eigenvalues 1+
0
i=1
n
om+u
1
eigenvalues matrix U 1+
. Since 0min = 0, largest eigenvalue
0
i=1
U 1. Since eigenvalues LN non-negative, min > 0.

Appendix I. Proofs Section 7

Proof Corollary 2: Let {Ai }
i=1 {pi }i=1 set positive numbers
P

i=1 pi 1. weighted union bound argument (39) probability least 1 training/test set partitions, Ai q g,Ai ,

eq)
Lu (h

eq)
Lbm (h

p
Rm+u (Beg,Ai )
+
+ c0 Q min(m, u) +




1
Q ln
.
2
pi

(73)

P


1
set Ai = g0 si pi = i(i+1)
. verified
i=1 pi 1. q let iq
smallest index Aiq g(q). two cases:
Case 1 iq = 1. case iq = logs (g(q)/g0 ) = 1.
Case 2 iq 2. case Aiq 1 = g0 siq 1 < g(q) g(q)s1 , therefore, iq
logs (g(q)/g0 ).
Thus always iq logs (g(q)/g0 ). follows definition Aiq g(q)
Aiq g(q). ln(1/piq ) 2 ln(iq + 1) 2 ln logs (sg(q)/g0 ). Substituting
bounds (73) taking account monotonicity Rm+u (Beg,Ai ) (in Ai ),
probability least 1 , q, bound (40) holds.

Proof Theorem 4: require several definitions facts convex analysis
(Rockafellar, 1970). function f : Rn R conjugate function f : Rn R
defined f (z) = supxRn (hz, xi f (x)). domain f consists values z
value supremum finite. consequence definition f
so-called Fenchel inequality:
hx, zi f (x) + f (z) .
(74)
P|B|
verified conjugate function g(q) = D(qkp) g (z) = ln j=1 pj ezj .

e =
Let h(i)
(h1 (i), . . . , h|B| (i)). derivation follows use following inequality
38

fiTransductive Rademacher Complexity Applications

(Hoeffding, 1963): X random variable X b c constant,

EX exp(cX) exp

c2 (b a)2
8


.

(75)

> 0 have,
*
e q = QE sup
Rm+u (Beg,A ) = QE sup h, h
qg,A

=







=

q,

qg,A

*

m+u
X

m+u
X

+
e
h(i)

i=1

+

Q
e
E sup
q,
h(i)

qg,A
i=1
!!

m+u
X
Q

e
sup g(q) + E g
h(i)
qg,A
i=1

" m+u
#
|B|
X
X
Q
+ E ln
pj exp
hj (i)

j=1
i=1

" m+u
#!
X
Q
+ sup E ln exp
h(i)

hB
i=1

" m+u
#!
X
Q
+ sup ln E exp
h(i)

hB
i=1

" m+u
#!
2
X

Q
+ sup ln exp
h(i)2

2
hB
i=1



Q
+ sup khk22 .

2 hB


(76)

(77)

(78)
(79)
(80)



Inequality (76) obtained applying (74) f = g f = g . Inequality (77) follows
definition g g . Inequality (78) obtained application Jensen
inequality inequality (79) obtained applying + u times (75). minimizing
(80) w.r.t. obtain
r
Rm+u (Beg,A ) Q 2A sup khk2 .
hB

2

Substituting bound (39) get fixed A, probability least 1,
q Bg,A
p
Qr
e q ) Lb (h
e
Lu (h
2A sup khk22 + c0 Q min(m, u) +
q) +

hB

r

1

Q ln .
2


Finally, applying weighted union bound technique, proof Corollary 2,
obtain statement theorem.


39

fiEl-Yaniv & Pechyony

References
Ambroladze, A., Parrado-Hernandez, E., & Shawe-Taylor, J. (2007). Complexity pattern
classes Lipschitz property. Theoretical Computer Science, 382 (3), 232246.
Audibert, J.-Y. (2004). better variance control PAC-Bayesian classification. Tech.
rep. 905, Laboratoire de Probabilites et Modeles Aleatoires, Universites Paris 6
Paris 7.
Azuma, K. (1967). Weighted sums certain dependent random variables. Tohoku Mathematical Journal, 19, 357367.
Balcan, M., & Blum, A. (2006). augmented PAC model semi-supervised learning.
Chapelle, O., Scholkopf, B., & Zien, A. (Eds.), Semi-Supervised Learning, chap. 22,
pp. 383404. MIT Press.
Bartlett, P., Bousquet, O., & Mendelson, S. (2005). Local Rademacher complexities. Annals
Probability, 33 (4), 14971537.
Bartlett, P., & Mendelson, S. (2002). Rademacher Gaussian complexities: risk bounds
structural results. Journal Machine Learning Research, 3, 463482.
Belkin, M., Matveeva, I., & Niyogi, P. (2004). Regularization semi-supervised learning
large graphs. Shawe-Taylor, J., & Singer, Y. (Eds.), Proceedings 17th
Annual Conference Learning Theory, pp. 624638. Springer-Verlag.
Belkin, M., & Niyogi, P. (2004). Semi-supervised learning Riemannian manifolds. Machine Learning, 56, 209239.
Blum, A., & Langford, J. (2003). PAC-MDL bounds. Scholkopf, B., & Warmuth, M.
(Eds.), Proceedings 16th Annual Conference Learning Theory, pp. 344357.
Springer-Verlag.
Bousquet, O., & Elisseeff, A. (2002). Stability generalization. Journal Machine
Learning Research, 2, 499526.
Catoni, O. (2004). Improved Vapnik-Cervonenkis bounds. Tech. rep. 942, Laboratoire de
Probabilites et Modeles Aleatoires, Universites Paris 6 Paris 7.
Catoni, O. (2007). PAC-Bayesian supervised classification, Vol. 56 IMS Lecture Notes Monograph Series. Institute Mathematical Statistics.
Chapelle, O., Scholkopf, B., & Zien, A. (Eds.). (2006). Semi-Supervised Learning. MIT
Press, Cambridge, MA.
Chapelle, O., Weston, J., & Scholkopf, B. (2003). Cluster kernels semi-supervised learning. Becker, S., Thrun, S., & Obermayer, K. (Eds.), Advances Neural Information
Processing Systems 15, pp. 585592. MIT Press, Cambridge, MA.
Chung, F. R. (1997). Spectral graph theory, Vol. 92 CBMS Regional Conference Series
Mathematics. American Mathematical Society.
Derbeko, P., El-Yaniv, R., & Meir, R. (2004). Explicit learning curves transduction
application clustering compression algorithms. Journal Artificial Intelligence
Research, 22, 117142.
40

fiTransductive Rademacher Complexity Applications

Devroye, L., Gyorfi, L., & Lugosi, G. (1996). Probabilistic Theory Pattern Recognition.
Springer-Verlag.
El-Yaniv, R., & Gerzon, L. (2005). Effective transductive learning via objective model
selection. Pattern Recognition Letters, 26, 21042115.
El-Yaniv, R., & Pechyony, D. (2006). Stable transductive learning. Lugosi, G., & Simon,
H. (Eds.), Proceedings 19th Annual Conference Learning Theory, pp. 3549.
Springer-Verlag.
Grimmett, G., & Stirzaker, D. (1995). Probability Random Processes. Oxford Science
Publications. Second edition.
Hanneke, S. (2006). analysis graph cut size transductive learning. Proceedings
23rd International Conference Machine Learning, pp. 393399. ACM Press.
Herbster, M., Pontil, M., & Wainer, L. (2005). Online learning graphs. Proceedings
22nd International Conference Machine Learning, pp. 305312. ACM Press.
Hoeffding, W. (1963). Probability inequalities sums bounded random variables. Journal American Statistical Association, 58, 1330.
Horn, R., & Johnson, C. (1990). Matrix Analysis. Cambridge University Press.
Joachims, T. (2003). Transductive learning via spectral graph partitioning. Proceedings
20th International Conference Machine Learning, pp. 290297. ACM Press.
Johnson, R., & Zhang, T. (2007). effectiveness Laplacian normalization graph
semi-supervised learning. Journal Machine Learning Research, 8, 14891517.
Johnson, R., & Zhang, T. (2008). Graph-based semi-supervised learning spectral kernel
design. IEEE Transactions Information Theory, 54.
Koltchinskii, V. (2001). Rademacher penalties structural risk minimization. IEEE
Transactions Information Theory, 47 (5), 19021915.
Koltchinskii, V., & Panchenko, D. (2002). Empirical margin distributions bounding
generalization error combined classifiers. Annals Statistics, 30 (1), 150.
Lanckriet, G., Cristianini, N., Bartlett, P., Ghaoui, L. E., & Jordan, M. (2004). Learning
kernel matrix semidefinite programming. Journal Machine Learning Research,
5, 2772.
Ledoux, M., & Talagrand, M. (1991). Probability Banach spaces. Springer-Verlag.
Maurey, B. (1979). Construction de suites symetriques. Comptes Rendus Acad. Sci. Paris,
288, 679681.
McAllester, D. (2003). PAC-Bayesian stochastic model selection. Machine Learning, 51 (1),
521.
McDiarmid, C. (1989). method bounded differences. Siemons, J. (Ed.), Surveys
Combinatorics, pp. 148188. London Mathematical Society Lecture Note Series 141,
Cambridge University Press.
McDiarmid, C. (1998). Concentration. Habib, M., McDiarmid, C., Ramirez, J., & Reed,
B. (Eds.), Probabilistic methods algorithmic discrete mathematics, pp. 195248.
Springer-Verlag.
41

fiEl-Yaniv & Pechyony

Meir, R., & Zhang, T. (2003). Generalization error bounds Bayesian mixture algorithms.
Journal Machine Learning Research, 4, 839860.
Rockafellar, R. (1970). Convex Analysis. Princeton University Press, Princeton, N.J.
Scholkopf, B., Herbrich, R., & Smola, A. (2001). generalized representer theorem.
Helmbold, D., & Williamson, B. (Eds.), 14th Annual Conference Computational
Learning Theory 5th European Conference Computational Learning Theory,
pp. 416426. Springer-Verlag.
Serfling, R. (1974). Probability inequalities sum sampling without replacacement.
Annals Statistics, 2 (1), 3948.
Talagrand, M. (1995). Concentration measure isoperimetric inequalities product
spaces. Publications Mathematiques de lI.H.E.S., 81, 73203.
Vapnik, V. (1982). Estimation Dependences Based Empirical Data. Springer-Verlag.
Vapnik, V. (2000). nature statistical learning theory (2nd edition). Springer-Verlag.
Vapnik, V., & Chervonenkis, A. (1974). Theory Pattern Recognition. Moscow:
Nauka.
Zhou, D., Bousquet, O., Lal, T., Weston, J., & Scholkopf, B. (2004). Learning local
global consistency. Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Advances
Neural Information Processing Systems 16. MIT Press, Cambridge, MA.
Zhu, X., Ghahramani, Z., & Lafferty, J. (2003). Semi-supervised learning using gaussian
fields harmonic functions. Proceedings 20th International Conference
Machine Learning, pp. 912919. ACM Press.

42

fiJournal Artificial Intelligence Research 35 (2009) 485-532

Submitted 08/08; published 07/09

Hard Bribery Elections?
Piotr Faliszewski

faliszew@agh.edu.pl

Department Computer Science
AGH University Science Technology
Krakow, Poland

Edith Hemaspaandra

eh@cs.rit.edu

Department Computer Science
Rochester Institute Technology
Rochester, NY 14623 USA

Lane A. Hemaspaandra

lane@cs.rochester.edu

Department Computer Science
University Rochester
Rochester, NY 14627 USA

Abstract
study complexity influencing elections bribery: computationally complex external actor determine whether paying certain voters
change preferences specified candidate made elections winner?
study problem election systems varied scoring protocols Dodgson voting,
variety settings regarding homogeneous-vs.-nonhomogeneous electorate bribability, bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted voters,
succinct-vs.-nonsuccinct input specification. obtain polynomial-time bribery algorithms proofs intractability bribery, indeed results show
complexity bribery extremely sensitive setting. example, find settings
bribery NP-complete manipulation (by voters) P, find settings bribing weighted voters NP-complete bribing voters individual
bribe thresholds P. broad class elections (including plurality, Borda, kapproval, veto) known scoring protocols, prove dichotomy result bribery
weighted voters: find simple-to-evaluate condition classifies every case either
NP-complete P.

1. Introduction
paper studies complexity bribery elections, is, complexity computing whether possible, modifying preferences given number voters,
make preferred candidate winner.
Election systems provide framework aggregating voters preferencesideally
(though truly ideal voting system, see Duggan & Schwartz, 2000; Gibbard,
1973; Satterthwaite, 1975) way satisfying, attractive, natural. Societies use
elections select leaders, establish laws, decide policies, practical
applications elections restricted people politics. Many parallel algorithms
start electing leaders. Multiagent systems sometimes use voting purpose
planning (Ephrati & Rosenschein, 1997). Web search engines aggregate results using
methods based elections (Dwork, Kumar, Naor, & Sivakumar, 2001). wide
c
2009
AI Access Foundation. rights reserved.

fiFaliszewski, Hemaspaandra, & Hemaspaandra

range applications, surprising elections vary tremendously. example,
one might think first typical elections many voters candidates.
However, fact, may wide range voter-to-candidate proportions:
typical presidential elections relatively candidates may millions
voters. context web, one may consider web pages voting pages
linking them, may consider humans voting pages site time
spend each. setting may large number voters
large number candidates. hand, Dwork et al. (2001) suggest designing
meta search engine treats search engines voters web pages candidates.
yields voters many candidates. summarize paragraph, elections
great variety sizes terms numbers candidates numbers voters. So,
surely, one simply say Elections tend small, always solve
brute-force issues related them.
principles democracy mind, also tend think vote equally
important. However, scenarios make much sense setting
voter different voting power. example, U.S. presidential elections
sense weighted (different states different voting powers Electoral College);
shareholders company votes weighted number shares own;
search engines example could weighted quality. Weighted voting
natural choice many settings well.
importance election systems naturally inspired questions regarding resistance abuse, several potential dangers identified studied. example,
elections organizers make attempts control outcome elections procedural tricks adding deleting candidates encouraging/discouraging people
voting. Classical social choice theory concerned possibility impossibility procedural control. However, recently realized even control
possible, may still difficult find actions needed effect control, e.g., computational problem NP-complete. complexity controlling wins
election studied first Bartholdi, Tovey, Trick (1992) later many
authors (Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2007; Hemaspaandra,
Hemaspaandra, & Rothe, 2007; Erdelyi, Nowak, & Rothe, 2008a, 2008b; Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2008; Meir, Procaccia, Rosenschein, & Zohar, 2008).
Elections endangered organizers also voters (manipulation),
might tempted vote strategically (that is, according true preferences)
obtain preferred outcome. desirable skew result
elections way arguably best interest society. Gibbard
Satterthwaite/DugganSchwartz Theorems (Gibbard, 1973; Satterthwaite, 1975; Duggan &
Schwartz, 2000) show essentially election systems manipulated,
important discover systems manipulation computationally difficult execute.
line research started Bartholdi, Tovey, Trick (1989a) continued
many researchers (as varied examples, mention work Elkind & Lipmaa, 2005b, 2005a; Conitzer, Sandholm, & Lang, 2007; Hemaspaandra & Hemaspaandra,
2007; Procaccia & Rosenschein, 2007; Brelsford, Faliszewski, Hemaspaandra, Schnoor, &
Schnoor, 2008; Faliszewski, Hemaspaandra, & Schnoor, 2008; Zuckerman, Procaccia, &
Rosenschein, 2008; readers interested manipulation able reach broader collec486

fiHow Hard Bribery Elections?

tion papers standard process recursive bibliography search). Faliszewski,
Hemaspaandra, Hemaspaandra, Rothe (2009b) provide relatively nontechnical survey
complexity issues related altering outcomes elections.
Surprisingly, nobody seems addressed issue (the complexity of) bribery,
i.e., attacks person interested success particular candidate picks
group voters convinces vote says. Bribery seems strongly
motivated real-life computational agent-based settings, shares
flavor manipulation (changing voters (reported) preferences) control (deciding
voters influence). paper (in version conference precursor) initiates
study complexity bribery elections.
many different settings bribery studied. simplest one
interested least number voters need bribe make favored
candidate win. natural extension consider prices voter. setting,
voter willing change true preferences anything say,
meet price. even complicated setting conceivable
voters would different prices depending want affect vote (however,
clear succinctly encode voters price scheme). mainly focus
previous two scenarios point reader results approval voting
work Faliszewski (2008) discussion bribery prices represented
flexibly.
classify election systems respect bribery seeking case either (a)
prove complexity low giving polynomial-time algorithm (b) argue intractability
via proving NP-completeness discovering whether bribery affect given case.
obtain broad range results showing complexity bribery depends closely
setting. example, weighted plurality elections bribery P (Theorem 3.3)
jumps NP-complete voters price tags (Theorem 3.2). another example,
approval voting manipulation problem easily seen P, contrast
prove bribery problem NP-complete (Theorem 4.2). Yet also prove
bribery cost function made local, complexity approval voting falls back
P (Theorem 4.4). constraints added, problem goes NP-complete
(Theorem 4.5). scoring protocols obtain complete characterizations
complexity bribery possible voter types, i.e., without weights
without price tags. particular, via dichotomy theorems (Theorems 4.8 4.9)
algorithmic constructions (Theorems 4.13 4.15) provide voter type simple
condition partitions scoring protocols ones NP-complete bribery problems
ones P bribery problems. point reader Tables 1 2 (they appear near
end paper) summary complexity results regarding bribery
elections.
paper organized follows. Section 2 describe election systems
bribery problems interested cover complexity background preliminaries. Section 3, provide detailed study plurality elections.
study connections manipulation bribery, obtain dichotomy results
bribery scoring protocols Section 4. Section 5, study case succinctly
represented elections fixed number candidates.

487

fiFaliszewski, Hemaspaandra, & Hemaspaandra

2. Preliminaries
section introduces notions notations use paper.
2.1 Election Systems
describe elections providing set C = {c1 , . . . , cm } candidates, set V n voters
specified preferences, rule selecting winners. voter vs preferences
represented list ci1 > ci2 > . . . > cim , {i1 , i2 , . . . , im } = {1, 2, . . . , m}, ci1
preferred candidate cim despised one. assume preferences
transitive, complete (for every two candidates voter knows one
prefers), strict (no ties). Sometimes authors also allow ties preference lists,
ties clear interpretation election rules simplicity
uniformity consider them.
Given list votes (i.e., voters preference lists), election rule determines
candidates winners elections. briefly describe election systems
analyze paper, standard literature social choice theory.1
Winners plurality elections candidate(s) top choice largest
number voters (of course, different voters different winners). approval
voting voter selects candidates approves of; candidate(s) approvals
win. Unlike systems discussed paper, approval voting input
preference order rather bit-vector approvals/disapprovals. scoring
protocol candidates described vector = (1 , . . . , ) nonnegative integers
1 2 . . . . (We require 1 > , wish classify
broadest class cases possible, including usually easy boundary case
equal.) time candidate appears ith position voters preference list,
candidate gets points; candidate(s) receive points win. Wellknown examples scoring protocols include Borda count, plurality, k-approval,
veto voting systems, m-candidate elections Borda uses = (m 1, 2, . . . ,
k

mk

z }| { z }| {
0), plurality uses = (1, 0, . . . , 0, 0), k-approval uses = (1, . . . , 1, 0, . . . , 0), veto uses
= (1, 1, . . . , 1, 0). Note selecting scoring protocol automatically select
number candidates within elections. Though scoring protocols easily
naturally generalized arbitrary candidate sets, formally individual scoring
protocol deals fixed number candidates. Thus results regarding scoring
protocols automatically talk fixed number candidates.
Condorcet winner candidate (strictly) beats candidates pairwise
contests, is, Condorcet winner beats everyone else pairwise plurality elections.
Clearly, one Condorcet winner, sometimes none (as
case Condorcet Paradox, Condorcet, 1785). many voting systems
1. social choice literature, often voting systems assumed least one winner, exactly
one winner, least terms basic definition voting system, require
restriction, since one imagine wanting study elections whichperhaps due tie effects
symmetry effects perhaps even due zero candidatesthere always exactly one winner.
Indeed, practice, elections Hall Fame induction worthiness
hired given academic department, quite possible real-world election system might give
answer one year.

488

fiHow Hard Bribery Elections?

choose Condorcet winner one exists use compatible rule otherwise. One
systemdeveloped 1800sis Charles Lutwidge Dodgson (a.k.a. Lewis
Carroll). Dodgsons system winner person(s) become Condorcet
winner smallest number switches voters preference lists. (A switch changes
order two adjacent candidates list.2 ) Thus, Condorcet winner exists,
also unique winner Dodgsons election. See work Dodgson (1876)and also
work Bartholdi, Tovey, Trick (1989b)for details regarding Dodgsons voting
rule, known winner testing complete parallel access NP
(Hemaspaandra, Hemaspaandra, & Rothe, 1997). different election rule introduced
recently Young (1977). Young elections winner person become
Condorcet winner removing smallest number voters. way contrast, note
plurality rule property elects candidates who, removing
least number votes, preferred everyone. work Rothe, Spakowski,
Vogel (2003), see also expository presentation Rothe (2005), proves winner
problem Young elections extremely difficultcomplete parallel access NP.
Another election rule Kemeny (1959), see also work Kemeny Snell
(1960). Kemeny consensus preference order maximizes number agreements
voters preference lists, voter two candidates b
say preference order agrees voters preference list place b
place b a. Naturally, many different Kemeny consensuses may possible.
candidate winner Kemeny election preferred candidate
Kemeny consensus election. (The original work Kemeny allowed voters
nonstrict preference orders, like, e.g., Saari & Merlin, 2000, use Kemeny
elections refer case input orderings strict.) Note winner
testing problem Kemeny elections known complete parallel access NP,
known hold case input preference orders must strict,
case nonstrict input preference orders allowed (Hemaspaandra, Spakowski,
& Vogel, 2005, see particular comments footnote page 383
paper). Kemeny rule might first sound Dodgson rule,
fact different. Dodgsons elections based making minimum
number local changes, Kemenys elections hinge overall closeness voters
preference orders certain consensus orderingswhich possibly may
preferences voters.
2.2 Bribery Problems
Informally, bribery problem following: Given description election (i.e.,
set candidates, preferences voters, etc.), number k, distinguished
candidate p, make p winner changing preference lists k voters.
formally, election rule (i.e., election system) E define E-bribery problem
following. assume standard encoding mathematical objects finite
2. mention, since source confusion, seminal paper Dodgson explicitly
state switches limited adjacent candidates. However, mathematics examples
consistent reading, clear intended meaning.

489

fiFaliszewski, Hemaspaandra, & Hemaspaandra

sets lists (e.g., see Garey & Johnson, 1979). Also, numbers nonnegative
integers and, unless otherwise specified, represented binary.
Name: E-bribery.
Given: set C candidates, collection V voters specified via preference lists.
distinguished candidate p C nonnegative integer k.
Question: possible make p winner E election changing preference
lists k voters?
speak unweighted case (all voters equal; paper always
holds unless weighted problem name) weighted case (voters weighted).
Essentially results apply case want make preferred
candidate winner case want make preferred candidate
unique winner, explicitly put nonunique/unique setting
problem names. clarity specificity, focus nonunique case discussions
proofs, problem statements theorems default refer nonunique
case. However, settings differences proofs unique case
nonunique case minor amount couple small tweaks, e.g., changing
weak inequalities strong ones, adding special voter already prefers p, etc.,
often end proof briefly note theorem also holds unique case.
E-$bribery family problems assume voter price changing
preference list. case ask whether bribe k people,
whether make p winner spending k dollars. example,
plurality-weighted-$bribery problem described follows.
Name: plurality-weighted-$bribery.
Given: set C candidates. collection V voters specified via preference
lists (prefs 1 , . . . , prefs ), (nonnegative, integer) weights (w1 , . . . , wm ),
(nonnegative, integer) prices (p1 , . . . , pm ). distinguished candidate p C
nonnegative integer k (which sometimes refer budget).
P
Question: set B {1, . . . , m} iB pi k way bribe
voters B way p becomes winner?
Regarding fact models voters assumed vote bribes dictate,
stress using term bribery intend necessarily imply moral
failure part bribe recipients bribe givers: Bribes simply payments. Although human, political elections, payments typically considered morally wrong
(perhaps voter supposed thinking overall social welfare),
electronic/web/multiagent systems settings, morality issues often apply.
voter may simply electronic entity trying maximize utility, bribe price
entity may crisply reflect fact.
Throughout paper use term bribery regular sense
nonstandard sense collection bribes. using latter sense often
speak bribery, thus mean collection bribes. So, example,
490

fiHow Hard Bribery Elections?

Alice end winner bribery ask anyone vote Alice, mean
pattern/scheme bribes (e.g., give two dollars Bob make change
FOO vote give five dollars Carol make change BAR vote) none
asks anyone vote Alice yet make Alice overall winner.
dealing variety settings, need common format speak
instances bribery problems. adopt following convention (and view
already specified problems implicitly recast form): instance bribery
problem 4-tuple E = (C, V, p, k),
1. C list candidates,
2. V list voters (see below),
3. p C candidate want make winner (for problems making
candidate unique winner, winner replaced unique winner),

4. k bribe limit (either amount money spend bribing
maximum number voters bribe, depending flavor bribery
problem).
list voters contains tuples describing votes cast. voter 3-tuple
(prefs, , ),
1. prefs preference list voter (or, case approval voting,
preference vector),
2. price changing voters preference list,
3. weight voter.
tuple V describes precisely one voter. drop price and/or weight field
given election voters prices/weights. (However, assume
dropped prices weights unit values, refer them.
proofs handle two cases, one priced voters one weighted voters,
time need able uniformly refer weights prices.) v V voter
refer price weight (v) (v). manner, U V

X
(U ) =
(v)
vU

(U ) =

X

(v).

vU

often refer (U ) either vote weight U total weight U .
Note throughout paper V , though input list, typically functions multiset, summations additive term appropriate
occurrence multisetthe multiplicities carry sums, also set-like
491

fiFaliszewski, Hemaspaandra, & Hemaspaandra

operations, e.g., {v V | . . .} multiset, multiplicities appropriately
preserved. dealing V use set/subset mean multiset/submultiset.
Section 5 deal succinct representations. dealing succinct
representations, V consist 4-tuples (prefs, , , m), multiplicity
vote, is, number voters identical preferences, price, weight entry
V standing for. m(v) denote value v V . Note single entry
V often represents multiple voters.
notation help us speak bribery problems uniform fashion. Note
addition specifying E = (C, V, p, k) always need explicitly state election rule
using.
Positive results regarding demanding bribery problems imply positive results
weaker ones. example, weighted bribery P election system E clearly
unweighted bribery also easy E. Conversely, hardness results regarding
simpler models imply hardness results involved ones. often mention
implied results separately interesting (e.g., algorithm simpler
case provides insights understanding complicated case), omit
enlightening.
2.3 Reductions NP-completeness
proceed study bribery, let us briefly review notions computational complexity standard NP-complete problems use
proofs.
usual, kSk denotes cardinality set S. fix alphabet = {0, 1}
assume standard encodings mathematical entities involved problems.
particular, integers represented binary unless specified otherwise. (For example,
see Garey & Johnson, 1979, discussion issues.) NP-completeness
standard mean completeness respect many-one (polynomial-time) reductions.
Definition 2.1 pm B (A many-one reduces B) polynomial-time computable function f
(x )[x f (x) B].
one results relating manipulation bribery also need disjunctive truth-table
reductions.
Definition 2.2 say pdtt B (A disjunctively truth-table reduces B)
polynomial-time procedure input x outputs list strings y1 , . . . , ym
x least one yi , 1 m, B.
definitions standard commonly used within field complexity theory. Detailed treatment various reduction types including found,
e.g., work Ladner, Lynch, Selman (1975).
standard way showing problem NP-complete proving NP
reducing known NP-complete problem it. former easy bribery
problems deal with: compute winners elections polynomial
time, nondeterministically guess bribe test whether yields
492

fiHow Hard Bribery Elections?

desired outcome. latter issue use reductions either partition problem
exact cover 3-sets problem (e.g., see Garey & Johnson, 1979; Papadimitriou, 1994,
general background problems proving NP-completeness).
problem Partition asks whether possible split sequence integers two
subsequences equal sums.
Name: Partition.
P
Given: sequence s1 , . . . , sn nonnegative integers satisfying ni=1 si 0 (mod 2).3
P
P
Question: set {1, . . . , n} iA si = i{1,...,n}A si ?

prove main dichotomy result Section 4 need restrictive version


Pn partition problem. Let s1 , . . . , sn sequence nonnegative integers
i=1 si 0 (mod 2). Partition assume i, 1 n, holds
n

si

1 X
si
2+n

(1)

i=1

P
(reminder: footnote 3 courseP
applies regarding handling ni=1 si 0 (mod 2)
n
1
(i {1, . . . , n})[si 2+n
i=1 si ]), ask whether exists {1, . . . , n}
P
P
n
1

i=1 si . sake completeness include proof
iA si = 2
Partition remains NP-complete.
Lemma 2.3 Partition NP-complete.

Proof. Clearly, Partition NP. show, reduction standard
partition problem, Partition also NP-hard.
P
Let q = s1 , . . . , sn sequence nonnegative integers let 2S = ni=1 si . First,
construct sequence q = s1 , o1 , . . . , sn , 2n nonnegative integers following
two properties. (1) q partitioned q be. (2) partition q
splits q two sequences cardinality. define si oi , 1 n,
follows.
si = 3i1 + 3n si .
oi = 3i1 .
partition s1 , o1 , . . . , sn , splits q two subsequences sum ,
defined
n
n
X
1X
3n 1


n
=
.
3i1 = 3n +
(si + oi ) = 3 +
2
2
i=1

i=1

Pn

3. given input holds i=1 si 6 0 (mod 2), consider input syntactically illegal
thus consider input member Partition. rest paper assume,
reducing Partition problem (Q), syntactic constraint violated
input, reduction whatever reduction give states, rather instantly
map fixed element Q. (often tacitly) make assumptionthat syntactically (by
mean true conditions syntax polynomial-time constraints problem via
Given assumes apply inputs) illegal inputs handled via reductions operation
inputs components, rather mapped fixed element complement set
reduced to.

493

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Clearly, partition s1 , o1 , . . . , sn , splits q two halves si belongs
one oi belongs other. also immediate q partitioned
q can.
satisfy condition (1) add constant si oi . Define qb sequence
numbers sb1 , ob1 , . . . , sbn , obn i, 1 n,
sbi = si +

obi = oi + .

Clearly, partition q still partition qb, since partition q splits q
two subsequences cardinality. converse holds partition qb
split subsequences sum Sb = + nS possible
subsequence contains exactly n elements. (A sum n elements would
greater (n + 1)S would subsequence could sum
to.) remains show (1) holds qb. case sbi obi
2 b
S. (Note sequence qb 2n elements.) Since qb
greater = 2+2n
computed polynomial time, proof completed.

exact cover 3-sets problem (X3C) asks way pick, given list,
three-element subsets set B cover whole set without ever introducing
element once.
Name: X3C.
Given: set B = {b1 , . . . , b3t } family three-element subsets B, =
{S1 , . . . , Sm }.

Question: set {1, . . . , m} kAk = iA Si = B?

two problemsPartition X3Chave useful tools proving NPcompleteness control manipulation problems, paper see
powerful used bribery problems. Specifically, Partition
useful dealing weighted elections X3C particularly useful
unweighted cases.

3. Plurality
section determine complexity bribery plurality-rule elections. Plurality
rule perhaps popular election system practical use; point view
democracy natural appealing make decision many people prefer.
However, also downsides plurality rule. Plurality rule may slight voices
minorities take account full information voters preferences.
particular, candidate voters rank second best
candidate top choice many rankings, might seem natural elect second
best person. However, plurality blind this. fact, typically view vote
plurality-rule elections vote particular candidate, namely, preferred
candidate according preference order actual vote; purposes
paper thing matters voter, though mention
494

fiHow Hard Bribery Elections?

contexts, control problems allowing deletion candidates (Bartholdi et al.,
1992; Hemaspaandra et al., 2007; Hemaspaandra, Hemaspaandra, & Rothe, 2009), full
ordering might important. simplicity widespread use plurality-rule elections
make results section particular relevance.
previous section somewhat carelessly mentioned plurality scoring rule
vector = (1, 0, . . . , 0). course, formally speaking, really holds
number candidates k notion plurality-of-k-candidates scoring-rule
election, vector 1 followed k 1 0s. one also consider
system takes input number candidates applies plurality scoring
rule (for inputs number candidates). people think
speaking plurality elections, throughout section, results plurality
course fixed numbers candidates, case.
simplest bribery scenario voters unweighted voter
expensive bribe voter. surprisingly, bribery easy setting.
Theorem 3.1 plurality-bribery P.
Proof.
proof theorem simple, describe detail simple
introduction proofs regarding bribery. give polynomial-time algorithm
given instance bribery E = (C, V, p, k) decides whether possible make p
winner bribing k voters.
algorithm works following way. Initially bribed zero voters.
check whether p currently winner. so, accept. Otherwise,
exceed bribe limit, pick current winner, bribe one voters (recall,
mentioned earlier section, [i.e., selected winners] voters
mean voters particular selected winner preferred candidate)
vote p, jump back testing whether p winner. reach bribe limit
(i.e., exceed bribe limit break us
loop) without making p winner reject.
algorithm accepts obviously bribery possible. show
possible ensure p winner via k bribes algorithm accepts.
proof follows induction k. base case enough note
algorithm works correctly k = 0. induction step, let us assume
input E = (C , V , p, k ) k < k, k positive integer, algorithm
accepts exactly possible ensure p winner via k bribes. Now, let
E = (C, V, p, k) arbitrary input p become winner via k bribes.
show algorithm accepts input. consider two cases.
bribery k voters ensures ps victory never involves voters
current winners election (C, V ) clear algorithm accepts. (Let
Vp V set voters vote p. case bribery min(k, ||Vp ||)
voters ensures p becomes winner.) Thus, let us assume briberies
make p winner involve bribing least one voter one current winners. Let c1
one winners (C, V ) (note c1 6= p) let E = (C, V , p, k 1)
instance plurality-bribery obtained E bribing one c1 voters. Clearly,
executing single iteration loop, algorithm transforms input one

495

fiFaliszewski, Hemaspaandra, & Hemaspaandra

isomorphic E . assumptions inductive hypothesis algorithm
accepts transformed input.
algorithm works polynomial time kV k bribes suffice make p
winner iterations executed polynomial time. theorem
proven. mention approach clearly also works unique case.
ease obtaining algorithm might fool us thinking bribery
within plurality system always easy. However, case.
Theorem 3.2 plurality-weighted-$bribery NP-complete, even two candidates.
Proof. Recall nonunique version problem default case,
addressing here.
plurality-weighted-$bribery NP: guess voters bribe test whether
bribe makes designated candidate winner exceed budget.
remains show problem NP-hard.
show NP-hardness, construct
Pa reduction Partition. Let s1 , . . . , sn
sequence nonnegative integers let ni=1 si = 2S. goal design election
E = (C, V, p, k) p become P
winner bribery cost k
set {1, . . . , n} iA si = S. define election two
candidates, p c, exactly n voters, v1 , . . . , vn , vi weight
price equal si . voters prefer c p. budget k set S. claim p
become winner s1 , . . . , sn partitioned two
P equal-sum groups.
Let us assume set {1, . . . , n} iA si = S. means
bribe vi vote p get p total vote weight (in
natural sense, defined Section 2) S. makes p winner. hand,
assume p made winner bribes total cost k = S. weight
voter equal price p obtain vote weight k = S.
fact, p must obtain exactly vote weight S, since setup clear p gains
strictly less vote weight c unique winner. means
way picking voters whose weights sum exactly S, thus sequence
s1 , . . . , sn partitioned two subsequences sum S.
reduction carried polynomial time proof complete.
course regards default case, namely nonunique case. unique case also follows,
namely, observing enough add one voter weight 1 price 0 votes
p. arguments show correct reduction.

theorems show bribery easy basic case becomes intractable
allow voters prices weights. natural ask additional
features (prices? weights?) responsible making problem difficult. turns
neither sole reason combination yields enough power
make problem NP-complete.4
Theorem 3.3 plurality-$bribery plurality-weighted-bribery P.
4. However, interesting compare Theorems 4.8, 4.9, 4.13, 4.15, suggest high
weights often feature responsible making problem NP-complete.

496

fiHow Hard Bribery Elections?

Theorem 3.3 special case result prove later (namely, Theorem 3.8)
thus, instead giving proof, provide informal discussion polynomial-time
algorithms plurality-$bribery plurality-weighted-bribery.
direct greedy algorithm, like one underpinning Theorem 3.1, fails prove
Theorem 3.3: problem one judge whether better bribe voters
currently prefer one winners bribe voters highest weights (or
lowest prices). (To see former may sometime make sense, consider election
two weight-4 voters, b one weight-5 voter, p one weight-2 voter.
Bribing one weight-4 voter winning bribery bribing one weight-5 voter not.)
approach Theorem 3.3s proof follows. Assume p r votes
bribery (or weighted case, vote weight r), r number specified
later. make p winner, need make sure everyone else gets r
votes. Thus carefully choose enough cheapest (heaviest) voters candidates defeat
p bribing vote p candidate p r votes.
simply make sure p gets least r votes bribing cheapest (the
heaviest) remaining voters. process p ever becomes winner without
exceeding budget (the bribe limit) know bribery possible.
pick value r? case plurality-$bribery, simply run
procedure possible values r, i.e., 0 r kV k, accept exactly
succeeds least one them. plurality-weighted-bribery slightly trickier approach
works. Namely, enough try values r obtained vote weight
candidate (other p) via bribing number heaviest voters.
polynomially many values whole algorithm works polynomial
time. intuition using values r following: (a) bribing voters
candidate one always limit oneself heaviest ones, (b) successful
bribery value r ps vote weight least r , candidates
vote weight r , candidate c 6= p cs vote weight
exactly r . algorithm, essence, performs exhaustive search (within heavily
limited search space) value r .
Note algorithms assume bribe people vote p.
reasonable method bribing one wants p become winner, also
potential real-world downsides: people bribe, likely may
malicious attempts detected work p. minimize chances
happening might instead bribe voters vote p
candidate(s). way p get extra votes might able take away enough
popular candidates become winner.
Definition 3.4 plurality-weighted-negative-bribery defined pluralityweighted-bribery, except restriction illegal bribe people vote
designated candidate.
problem plurality-negative-$bribery defined analogously. call setting
negative-bribery motivation p get votes him- herself,
take away others. Unlike Theorem 3.3, version problem draws
sharp line complexity bribing weighted priced voters.

497

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Theorem 3.5 plurality-weighted-negative-bribery NP-complete, plurality-negative$bribery P.
Proof. first give polynomial-time algorithm plurality-negative-$bribery. Let
E = (C, V, p, k) bribery instance want solve. need make p winner
taking votes away popular candidates distributing among less popular
ones. (The previous sentence said winner since usual addressing nonunique
case. However, clear similar approach works unique case, i.e., case
goal make p winner.)
partition set candidates three sets: candidates defeat p,
votes need taken away, candidates defeated p,
give extra votes, candidates score p. unweighted case,
score E (c) mean number voters within E prefer candidate c.
weighted case, score E (c) means total vote weight voters within E prefer c.
Cabove = {c | c C, score E (c) > score E (p)}.
Cbelow = {c | c C, score E (c) < score E (p)}.
Cequal = {c | c C, score E (c) = score E (p)}.
Since voters weight (weight 1) current case, plurality-negative$bribery, hard see successful negative bribery
successful negative bribery bribe voters Cequal
also wont bribe voters move within group, e.g., bribing voter shift
one Cbelow candidate another. (However, weights case, crazy bribes
sometimes needed; see footnote 5.) make sure p becomes winner,
candidate c Cabove need bribe many cs voters needed
reduce
P
score score E (p). Thus, altogether, need bribe cCabove (score E (c)
score E (p)) voters. number
P votes candidate c Cbelow accept without
preventing p winning cCbelow (score E (p) score E (c)). Thus, hard see
negative bribery possible exactly following inequality holds.
X
X
(score E (c) score E (p))
(score E (p) score E (c)).
(2)
cCabove

cCbelow

inequality (2) hold immediately reject. Otherwise, remains check
whether cost negative bribery within budget: every candidate c Cabove
let
P bc cost bribing cs score E (c) score E (p) cheapest voters. holds
cCabove bc k accept, negative bribery possible. Otherwise reject.
Clearly, algorithm works polynomial time. correctness follows fact
need make candidates Cabove score score E (p)
c Cabove bc lowest possible cost achieving that. Equation (2) guarantees
votes taken candidates Cabove distributed among Cbelow without
preventing p winning.
let us turn showing NP-hardness plurality-weighted-negative-bribery.
must careful here. plurality-negative-$bribery, argued one could without
loss generality ignore Cequal , i.e., one never needs bribe voters Cequal ,
ignore bribing voters one candidate group (Cbelow , Cequal ,
498

fiHow Hard Bribery Elections?

Cabove three groups) another candidate within group.
hard see claim false weights case, essentially due fact that,
example, members Cequal Cbelow useful making changethat is,
splitting large weights small ones.5 However, image reduction
construct, Cequal contain p, bribing votes change p forbidden
negative setting, bribing votes change away p clearly never required
success; setting Cequal fact play interesting role.
similarly, kCabove k = kCbelow k = 1 image reduction,
worry within-a-group bribes.
Now, start construction show NP-hardness plurality-weighted-negativebribery. particular, construct reduction Partition. Let s1 , . . . , sn
sequence nonnegative integers. design instance pluralityweighted-negative-bribery bribery possible s1P
, . . . , sn split
two parts sum value. Let ni=1 si = 2S.
elections three candidates: p, c1 , c2 , n + 1 weighted voters:
1. v0 weight S, whose preferences p > c1 > c2 ,
2. v1 , . . . , vn weights s1 , . . . , sn , preferences c1 > c2 > p.
goal briber ensure ps victory via bribing k = n + 1 voters (i.e.,
voters). Note reasonable bribes ones transfer votes vi ,
1 n, c1 c2 . (Strictly speaking, v0 could legally bribed vote c1 c2 ,
safely ignored.) set {1, . . . , n}
X
si = S,
(3)
iA

could bribe voters vi , A, vote c2 candidates would winners.
hand, p end winner bribery ask anyone vote
p, set satisfies Equation (3): p winner election
c1 c2 vote weight exactly S. However, beginning c1 holds 2S
vote weight successful bribery needs transfer exactly vote weight c1
c2 . possible (3) holds A.
finish proof, observe reduction computed polynomial
time.

Theorems 3.2 3.3 state plurality-weighted-$bribery NP-complete
attempt make simpler immediately pushes back realm P. fact,
5. see this, consider setting candidate Big preferred candidate one weight-10 voter
one weight-2 voter, candidate p preferred candidate one weight-10 voter, candidate
MakeChange preferred candidate ten weight-1 voters, candidate SmallOne
preferred candidate one weight-9 voter, candidate SmallTwo preferred candidate one
weight-9 voter, limit number bribes 3. Cabove = {Big}, Cequal = {p, MakeChange},
Cbelow = {SmallOne, SmallTwo}. Note successful negative bribery leaves
MakeChange uninvolved. However, moving Big MakeChange weight-2 voter,
moving one weight-1 voter SmallOne SmallTwo MakeChange, successful
negative bribery. example uses Cequal make change, one construct similar examples
require one bribe votes one member Cbelow another member Cbelow .

499

fiFaliszewski, Hemaspaandra, & Hemaspaandra

situation even dramatic. NP-complete problem plurality-weighted-$bribery
assume prices weights encoded binary. However, either prices
weights encoded unary, problem, again, becomes easy.
proceed formal proof fact, let us discuss issue informal manner.
unary encoding either one weights prices matter? reason
that, example, weights encoded unary trivially linearly
many (with respect size input problem) different total weights subsets
voters. Together additional tricks allows us use dynamic programming
obtain solution.
Definition 3.6 plurality-weighted-$briberyunary defined exactly pluralityweighted-$bribery, except prices encoded unary. plurality-weightedunary $bribery plurality-weighted-$bribery except weights encoded unary.
tempting use exactly proof approach one hinted
discussion Theorem 3.3, i.e., split bribery two parts: demoting others
promoting p. However, would correct. Sometimes optimal way
getting scores candidates certain threshold r prevents one
getting optimal bribe complete problem. Consider elections three
candidates, c, d, p, three voters v1 , v2 , v3 v1 price weight
equal 10, v2 price weight equal 7, v3 price 1, 000, 000 weight
10. v1 v2 prefer c v3 prefers d. Clearly, optimal bribe problem
requires threshold 10. optimal way getting c vote weight 10
bribing v2 . However, point making p winner requires bribing v1 well. Yet,
bribing v1 cheaper way making p winner getting c below-or-equal-to
10 threshold.
refer plurality-weighted-$briberyunary unary prices case,
plurality-weightedunary -$bribery unary weights case. give overview
algorithm works unary prices case, input E = (C, V, p, k). unary
weights case handled analogously. main idea that, using fact
linearly many possible prices paid, argue exists polynomialtime computable function Heaviest(E, C , , r)where C subset candidates,
integer price, r integer thresholdthat gives maximum vote
weight obtain bribing voters candidates C
1. cost bribery ,
2. bribery every candidate C vote weight r.
test whether possible make p winner spending k dollars,
need find threshold r score E (p) + Heaviest(E, C {p}, k, r) r, i.e.,
weight p originally via bribed voters least great post-bribery
weight candidates. Unfortunately, case plurality-weighted$briberyunary cannot try thresholds since may exponentially many
them. Instead use strategy similar one hinted discussing
Theorem 3.3. every successful bribery (in elections least two candidates)
candidate c 6= pnamely, candidate(s) p greatest
500

fiHow Hard Bribery Elections?

post-bribery total weightthat either tied-with-p winner loses p.
use after-bribery vote weight candidate threshold bribery
voters candidates. course, neither know candidate
vote weight would successful bribery. Nonetheless, try
candidates c 6= p candidate possible sub-budget b k
ask maximum amount additional weight get p bribing cs
voters allowed spend b (this require solving certain instances
knapsack problem). Then, using thus obtained threshold, bribe voters
rest candidates. (at most) linearly many candidates (at most)
linearly many prices yields (at most) polynomially many combinations.
Let us describe plan implemented. longer limit
unary prices case, describe cases parallel. Let E = (C, V, p, k)
input. candidate c C define
VEc = {v V | c preferred candidate v}.
Since additional restrictions makes sense bribe voters
support p. given candidate c C, describe bribing options either
function gives highest weight cs voters bribe b dollars function
gives lowest price needed gain vote weight least w bribing cs voters.
heaviest(E, c, b) = max{(U ) | U VEc (U ) b}.
cheapest(E, c, w) = min{(U ) | U VEc (U ) w}.
c candidate E, functions undefined. rest
proof, take max min empty set undefined. Note c
candidate E, heaviest(E, c, b) defined b 0 cheapest(E, c, w) defined
w (VEc ). Also note heaviest easily computed polynomial time
unary prices case cheapest easily computed polynomial time
unary weights case. cases simply use dynamic programming solutions
appropriate optimization variant knapsack problem.6 generalize
functions give us information best bribes regarding sets candidates.
U V , define bribed (E, U ) bribery problem exactly like E
voters U bribed vote p. define
fi



c
fi (U
cC ) ((U ) b)
,
Heaviest(E, C , b, r) = max (U ) fifi
(c C )[score bribed(E,U ) (c) r]
fi



c
fi (U
) ((U ) w)

cC
.
Cheapest(E, C , w, r) = min (U ) fifi
(c C )[score bribed(E,U ) (c) r]
C subset Es candidate set, functions undefined.

6. knapsack problem following. Given set items, price weight ,
possible select items total weight least W , without exceeding total price K? well
known knapsack problem polynomial-time dynamic programming algorithm either
prices encoded unary weights encoded unary. (See work Martello & Toth,
1990, background/reference knapsack problem.)

501

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Lemma 3.7 consider elections voter price
weight. prices encoded unary algorithm computes Heaviest
polynomial time. weights encoded unary algorithm computes
Cheapest polynomial time.
Proof. Note unary prices case linearly many sub-budgets b
need compute value Heaviest, namely 0 b (V ), unary
weights case linearly many weights w need evaluate Cheapest,
namely 0 w (V ). Using fact provide dynamic programming algorithms
computing functions. base case following: c candidate
E, functions undefined. Otherwise,

heaviest(E, c, b) score E (c) heaviest(E, c, b) r,
Heaviest(E, {c}, b, r) =
undefined
otherwise.

cheapest(E, c, w)
score E (c) w r,
Cheapest(E, {c}, w, r) =
cheapest(E, c, score E (c) r) otherwise.
following observation allows us compute Cheapest Heaviest larger sets.
assume C contain c. candidates C {c} candidates
E, functions undefined. Otherwise,
Heaviest(E, C {c}, b, r) = max{Heaviest(E, C , b , r) + Heaviest(E, {c}, b b , r) |
0 b b Heaviest(E, C , b , r) Heaviest(E, {c}, b b , r) defined}.
Cheapest(E, C {c}, w, r) = min{Cheapest(E, C , w , r) + Cheapest(E, {c}, w w , r) |
0 w w Cheapest(E, C , w , r) Cheapest(E, {c}, w w , r) defined}.
Thus, unary prices case compute Heaviest(E, C , b, r) using dynamic programming polynomial time. applies Cheapest(E, C , w, r) unary
weights case.

Theorem 3.8
$bribery P.

plurality-weighted-$briberyunary



plurality-weightedunary -

Proof. Algorithms problems similar describe
(nonunique) unary prices case detail. provide pseudocode (nonunique)
unary weights case, omit proof correctness, analogous proof
unary prices case. mention passing two unique cases easily
obtained well, via natural modifications algorithm.
Figure 1 shows procedure unary prices case. idea algorithm
following: Suppose set B voters bribe members
B vote p p becomes winner. assume candidate c,
cs voters bribed optimally, i.e., cheaper way getting (or
greater) vote weight bribing different subset cs voters. candidate
c votes among non-p candidates bribery. Thus, decide
bribery possible enough test whether candidate c 6= p sub-budget
b, 0 b k, bribing cs voters optimally, spending b dollars, still
possible bribe (without, overall, exceeding budget) voters candidates
way
502

fiHow Hard Bribery Elections?

procedure UnaryPricesBribery(E = (C, V, p, k))
begin
C = C {p};
k (V )
return(accept);
c C
b 0 b k
begin
w = heaviest(E, c, b);
r = score E (c) w ;
w = Heaviest(E, C {c}, k b, r);
w defined score E (p) + w + w r
return(accept);
end
return(reject);
end
Figure 1: main procedure plurality-weighted-$briberyunary .
1. candidate ends vote weight higher c,
2. enough voters bribed p becomes winner.
algorithm tests exactly case accepts so. (Though if-then line
might first seem focus candidates C {c} beat p, note cs
post-bribery score r, line handles c also.) reasoning, bribery
possible algorithm accepts. also clear algorithm accepts
bribery indeed possible. Since functions heaviest Heaviest computed
polynomial time, whole algorithm runs polynomial time. Thus,
plurality-weighted-$briberyunary P.
analogous algorithm works unary weights case, see Figure 2. proof
correctness analogous unary prices case.

Theorem 3.8 particularly interesting says plurality-weighted-$bribery
difficult choose weights bribe prices high. However,
prices set voters, many cases one could assume would set
fairly low, sense rendering bribery problem easy.
Another possible attack complexity plurality-weighted-$bribery
approximation algorithms. fact, using Theorem 3.8, Faliszewski (2008) obtained fullypolynomial approximation scheme plurality-weighted-$bribery. Many researchers ask
typical-case complexity practically encountered NP-complete problems (see
work Conitzer & Sandholm, 2006; Procaccia & Rosenschein, 2007; Erdelyi, Hemaspaandra, Rothe, & Spakowski, 2007, discussions issue context voting problems;
see also Erdelyi, Hemaspaandra, Rothe, & Spakowski, appear; Erdelyi, Hemaspaandra,

503

fiFaliszewski, Hemaspaandra, & Hemaspaandra

procedure UnaryWeightsBribery(E = (C, V, p, k))
begin
C = C {p};
c C
w 0 w (VEc )
begin
b = cheapest(E, c, w );
r = score E (c) w ;
b = Cheapest(E, C {c}, r (score E (p) + w ), r);
b defined b + b k
return(accept);
end
return(reject);
end
Figure 2: main procedure plurality-weightedunary -$bribery.
Rothe, & Spakowski, 2009), clearly important direction.7 However, often
difficult come distribution inputs realistic simple enough
study. hand, providing good polynomial-time approximation algorithm
would worst-case result: matter difficult instance would given,
could compute decent answer. Recent papers Brelsford et al. (2008), Faliszewski (2008),
Zuckerman et al. (2008) take steps interesting direction.
7. much excitement recent paper Elections Manipulated Often (Friedgut,
Kalai, & Nisan, 2008, see also Dobzinski & Procaccia, 2008; Xia & Conitzer, 2008), indeed referee
commented us paper proves (under certain assumptions) elections manipulable
time. However, one fact bit careful one claims. lower
bound paper (given assumptions) establishes frequency manipulation is, type
problem related unpriced bribery (but involves choosing single manipulator drawing
votes ones distribution), (1/kV k), typical approach manipulation
focusing single voter, gives lower bound frequency manipulation (1/kV k2 ).
isnt most, time, rather goes zero asymptotically. Even lower bound could
tremendously raised (1), still might mean manipulation frequency 0.00001 percent
timenot frequency means election methods frequently open manipulation.
exciting, active research direction suspect future work much clarify
upper lower bounds hold frequency manipulation (both single-manipulator
case coalition manipulation case), assumptions election systems needed
obtain results. Changing topics, mention cases time detours around
NP-hardness election-complexity results already obtained. example, although winner
problem Dodgson elections known complete parallel access NP, two recent papers,
McCabe-Dansted, Pritchard, Slinko (2008) Homan Hemaspaandra (2009), shown
(under particular assumptions distributions relationship numbers
candidates voters) heuristic algorithms rigorous sense correct
time.

504

fiHow Hard Bribery Elections?

4. Bribery Versus Manipulation, Two Dichotomy Theorems
previous section provided detailed discussion complexity bribery plurality
voting. obtain results carefully hand-crafted altered various algorithms
reductions. Designing algorithms reductions specific bribery problems certainly
reasonable approach, even better would find general tools establishing
complexity bribery elections. general tools would especially interesting
allowed one inherit results already existent literature election systems.
section implement plan studying relationships bribery
manipulation, showing obtain results using relationships find.
next section, studying ways integer programming employed solve
bribery problems continue emphasis exploring flexible tools establishing
complexity bribery. There, using theorem Lenstra show many bribery problems
regarding elections fixed-size candidate sets P, even voters
succinctly represented. (Regarding coming results studying relationship
bribery manipulation, generally commend reader issue finding
even extensive links problems bribery, manipulation, control.
find natural important direction.)
Manipulation flavor somewhat similar bribery, difference manipulation set voters may change preference lists specified input.
Formally, E election rule E-manipulation following problem (e.g., see
Bartholdi et al., 1989a; Conitzer et al., 2007).
Name: E-manipulation.
Given: set C candidates, collection V voters specified via preference lists, set
manipulative voters (without loss generality, including members V ),
candidate p C.
Question: way set preference lists voters election
rule E voters V together choose p winner?
Instances manipulation problems described tuples (C, V, S, p), C
list candidates, V list voters (in format bribery problems),
list manipulative voters, p designated candidate voters
want winner (a unique winner, unique case).
Manipulation, like bribery, comes many flavors. may asked make p
unique winner winner, voters may weights (in case specified
together weights voters S), etc. Bribery viewed manipulation
set manipulators fixed advance deciding manipulate
part challenge. Note check whether bribery successful given input
simply try possible manipulations k voters, k number bribes
willing allow. way, fixed k, disjunctively truth-table reduce
bribery problem analogous manipulation problem. (Note prices
limit bribing k voters effect unit prices budget k.)
Theorem 4.1 Let k arbitrary positive integer. Let B bribery problems,
following constraints: Voters prices (i.e., consider $bribery
505

fiFaliszewski, Hemaspaandra, & Hemaspaandra

problems) bribing k voters forbidden (that is, require
yes-instance (C, V, p, k ) B k k). Let analogous manipulation
problem, i.e., manipulation problem election system, weighted voters
B allows that, allowing manipulating set contain number voters 0
k. holds B pdtt M.
Proof. show B pdtt need give polynomial-time procedure
input x outputs list strings y1 , . . . , ym x B least one yi ,
1 m, M. describe procedure.
Let x input string. first check whether x parsed instance
B (reminder: is, x meets syntactic constraints B).
output empty list terminate; otherwise decode V , voter set, k k,
maximum number voters bribed, string x. every subset W
k = min(k , kV k) elements V form instance manipulation problem
voter set V W manipulating set equal W . go k -element
subsets output list manipulation instances formed.
procedure clearly works polynomial time kVk k = O(kV kk )
sets test form instances manipulation polynomial time.
manipulation instances output bribery possible; enough bribe
exactly voters selected manipulating group. hand, bribery
possible, least one instances output belongs M, namely one
includes voters would bribe.

simple, result still powerful enough allow inheritance results
previous papers. Bartholdi et al. (1989a) discuss manipulation single voters
Theorem 4.1 translates results bribery case. particular, translation says
bribery k = 1 P plurality, Borda count, many systems.
strengthen Theorem 4.1 constant-bounded bribery general bribery?
answer no: election systems bribery NP-complete manipulation
P. particular, manipulation approval voting (both weighted
unweighted case) P size manipulating set: manipulating group simply
approves favorite candidate nobody else.8 However, following
theorem, bribery approval voting NP-complete.
Theorem 4.2 approval-bribery NP-complete.
Proof. Clearly, approval-bribery NP. NP-completeness follows reduction
X3C.
Let B = {b1 , . . . , b3t } let = {S1 , . . . , Sm } family three-element subsets
B. Without loss generality, assume t; otherwise exact cover impossible.
i, 1 3t, let number sets Sj contain bi . input (B, S)
construct approval-bribery instance E = (C, V, p, k), k = t, set candidates C
equal B {p}, following voters.
1. Si voter vi approves exactly members Si .
8. Meir et al. (2008) somewhat different flexible setting previously noted approvalmanipulation P one manipulator.

506

fiHow Hard Bribery Elections?

2. bi + 1 voters approve bi .
3. voters approve p.
Note p gets approvals bi , 1 3t, gets + 1 approvals.
claim p made winner bribing voters B exact
cover sets S.

First assume set kAk = iA Si = B. make p
winner, bribe vi approve p. result p gets approvals
bi loses exactly one approval. Thus, candidates winners. hand,
assume bribery voters makes p winner. bribed voter
contributes one additional approval p. Thus, p get approvals.
candidate B + 1 approvals, bribery needs take away least one
approval candidate B. Since bribe voters, happen
bribe voters vi correspond cover B.
reduction computed polynomial time.

course, number bribes bounded fixed constant then,
Theorem 4.1, approval-bribery solved polynomial time.
mention bribery approval elections actually easy look
slightly different model. bribery problems allow us completely modify approval
vector voter, may demanding. voter might willing change
approval vectors entries change completely.
Definition 4.3 approval-bribery problem takes input description
approval election along designated candidate p nonnegative integer k, asks
whether possible make p winner k entry changes (total) approval
vectors. approval-$bribery defined analogously, extra twist changing
entry approval vector may different price.9
different prices flipping different entries approval-$bribery models possibility voter might willing change approval candidates
candidates. modified problems turn easy. fact,
easy even weights prices, provided one encoded unary.
Theorem 4.4
$bribery P.

approval-weighted-$briberyunary



approval-weightedunary -

Proof. polynomial-time algorithm provide based observation
approval-weighted-$briberyunary approval-weightedunary -$bribery getting vote
weight favorite candidate (carefully) treated separately demoting
candidates. (This basically approval voting bribery model costs
linked entries voters approval vectors candidates point total weighted
addition, voters, candidates 0-or-1 entry voter.)
9. referee points out, technical perspective one view voters approval-bribery (and
variants) broken multiple plurality voters turned off. Partially due
similarity, various flavors approval-bribery computational properties similar
corresponding variants plurality-bribery.

507

fiFaliszewski, Hemaspaandra, & Hemaspaandra

divide bribery two phases: First, bribe voters approve p,
favorite candidate, second, bribe enough voters retract approvals
candidates still defeat p. polynomially many relevant vote weights
p may obtain bribery, try all.
Let E = (C, V, p, k) bribery instance need solve. candidate c, price
b, subset voters V , define heaviest(V , c, b) highest vote weight
voters V whose approval c switched spending b dollars. Similarly,
candidate c, vote weight w, subset voters V , define cheapest(V , c, w)
lowest price switch approval-of-c voters V total
weight least w. proof use sets V either voters approve c
voters disapprove c. Note heaviest(V , c, b) defined b 0
cheapest(V , c, w) defined w (V ). Section 3, heaviest easily
computed polynomial time unary prices case cheapest easily computed
polynomial time unary weights case. addition, cheapest computed
polynomial time unary prices case. Note
cheapest(V , c, w) = min{b | heaviest(V , c, b) w}.
Since polynomially many prices try, done polynomial time.
Figure 3 gives pseudocode procedure UnaryPricesApproval, decides
approval-weighted-$briberyunary . score E (c) denotes number approvals candidate c election E. procedure simply tries relevant weights p could obtain
bribery tests whether possible, them, bring candidates vote weight p without exceeding budget. procedure
correct separation achieved (as discussed above, applied within
proof framework trying thresholds) issue bribing voters approve p issue bribing approve candidate. Also,
cheapest heaviest computable polynomial time, procedure works polynomial time. analogous procedure decides unary weights case: Simply change line
b = 0 k w = 0 (V ) line w = heaviest(V , p, b)
b = cheapest(V , p, w).

prices weights encoded binary, approval-weighted-$bribery becomes
NP-complete.
Theorem 4.5 approval-weighted-$bribery NP-complete.
Proof. immediate approval-weighted-$bribery NP. show NP-hardness,
constructPa reduction Partition. Let s1 , . . . , sn sequence nonnegative
integers let ni=1 si = 2S. construct election E candidates p c
n + 1 voters, v0 , . . . , vn , following properties.
1. v0 weight S, approves p, changing v0 approvals costs 2S + 1.
2. vi , 1 n, weight si , approves c, changing vi approval p costs
si , changing vi approval c costs 2S + 1.
claim p made winner
bribery cost
P
set {1, . . . , n} iA si = S.
508

fiHow Hard Bribery Elections?

procedure UnaryPricesApproval(E = (C, V, p, k))
begin
k (V )
return(accept);
V = {v | v V v approve p};
b = 0 k
begin
w = heaviest(V , p, b);
r = score E (p) + w;
k = k b;
c C {p}
begin
Vc = {v | v V v approves c};
score E (c) > r
k = k cheapest(Vc , c, score E (c) r);
end
k 0 return(accept);
end
return(reject);
end
Figure 3: procedure UnaryPricesApproval.
First suppose p made winner bribery cost S.
bribe voters v1 , . . . , vn approve p. election E, p approvals c
2S approvals, bribery needs give p least extra approvals. Since changing
vi approval p costs si , weight vi also si , follows p gains exactly
approvals, weights bribed voters v1 , . . . , vn add exactly S.
implies sequence s1 , . . . , sn partitioned two subsequences sum
S.
P
hand, assume set {1, . . . , n} iA si = S.
bribe voters vi , A, approve p. result, p c vote
weight 2S winners. reduction computed polynomial
time thus theorem proved.

above-discussed bribery models approval appropriate depends
setting. example, bribery seems natural look web treat
web pages voting linking pages. certainly easier ask webmaster
add/remove link completely redesign page. point reader work
Faliszewski (2008) discussion bribery scenarios similar bribery .
somewhat lengthy discussion approval bribery, let us return
central goal relating bribery manipulation. Theorem 4.1 managed disjunctively truth-table reduce restricted version bribery manipulation. discussion
theorems follow show working opposite direction, reducing manipulation
bribery, first might seem difficult, fact fruitful.
509

fiFaliszewski, Hemaspaandra, & Hemaspaandra

reason reducing manipulation bribery appears difficult
bribery allows freedom person interested affecting elections. embed
manipulation within bribery, find way expressing fact
certain group voters bribed (or, least, expressing fact
successful bribery also one bribes manipulators). fairly
easily implement plan, though cost reducing stronger bribery model,
namely bribery prices.
Theorem 4.6 Let manipulation problem let B analogous $bribery
problem (for election system). holds pm B.
Proof. Let = (C, V, S, p) instance M. design instance B B
B = (C, V , p, 0),
1. V equal V , except voter price 1,
2. equal S, except voter price 0 fixed arbitrary preference
list.
Since bribery budget set zero, voters may possibly bribe
. preference lists voters bribery directly correspond
manipulation . reduction carried polynomial time.

Clearly, Theorem 4.6 holds even $bribery problems prices represented
unary required come set {0, 1}. Theorem 4.6 useful allows us
inherit powerful results theory manipulation. Hemaspaandra
Hemaspaandra proved following dichotomy result (see also Procaccia & Rosenschein,
2007; Conitzer et al., 2007).
Theorem 4.7 (Hemaspaandra & Hemaspaandra, 2007) Let = (1 , . . . , )
scoring protocol. case 2 = 3 = = , -weightedmanipulation NP-complete; otherwise, P. result holds unique
nonunique variants.
Combining two theorems Theorem 3.2 immediately classify
complexity weighted-$bribery scoring protocols.
Theorem 4.8 scoring protocol = (1 , . . . , ), 1 = -weighted$bribery P; otherwise NP-complete.
Proof. consider three cases.
1. 1 = = .
2. 1 > 2 = = .
3. settings.
first case, 1 = = , -weighted-$bribery trivially P candidates
always tied. remaining two cases, note -weighted-$bribery clearly NP.
remains show NP-hardness.
510

fiHow Hard Bribery Elections?

second case, 1 > 2 = = , employ proof Theorem 3.2.
Theorem 3.2 shows NP-hardness (1, 0)-weighted-$bribery. easy see
2 pad reduction 2 candidates never ranked first
m1
z }| {
obtain NP-hardness (1, 0, . . . , 0)-weighted-$bribery. Note describes elections
equivalent plurality (i.e., candidate winner election
m1

z }| {
would also winner (1, 0, . . . , 0) election voters candidates;
see Observation 2.2 paper Hemaspaandra & Hemaspaandra, 2007). Thus,
get NP-completeness -weighted-$bribery case since least two
candidates.
third case follows combining Theorem 4.6 Theorem 4.7. Since -weightedmanipulation many-one reduces -weighted-$bribery -weighted-manipulation
NP-complete -weighted-$bribery NP-hard. exhausts cases.
Theorem 4.8 applies $bribery, course also interesting ask happens
case voters prices. bribery remain NP-complete?
express constraints bribery without using direct embedding above?
following dichotomy theorem shows answer Yes, fewer cases.
Theorem 4.9 scoring protocol = (1 , 2 , . . . , ), 2 = 3 = =
-weighted-bribery P; otherwise NP-complete.
2 = 3 = = either -weighted-bribery trivially P (if 1 =
= ) solved using algorithm plurality-weighted-bribery. core
proof show NP-hardness. would nice reducing
corresponding manipulation problems (which share characterizations boundary line
regarding s). seems work, Lemma 4.11 construct
reduction right properties whenever inputs satisfy additional condition,
namely, weight lightest manipulating voter least double
heaviest nonmanipulator. would suffice thus-restricted manipulation problem
NP-hard. Lemma 4.12 shows thus-restricted manipulation problem NPhard. examining manipulation-dichotomy proof Hemaspaandra
Hemaspaandra (2007) noting apply papers reduction Partition (see
Section 2.3) rather Partition guarantee restriction mentioned above.
Definition 4.10 -weighted-manipulation mean manipulation problem weighted-manipulation restriction manipulative voter weight
least twice high weight heaviest nonmanipulative voters. instance restriction violated considered element -weightedmanipulation .
Lemma 4.11 Let = (1 , . . . , ) scoring protocol. -weighted-manipulation pm
-weighted-bribery.
Proof. Without loss generality assume = 0. 6= 0
consider scoring protocol = (1 , 2 , . . . , ) instead. Given
instance = (C, V, S, p) manipulation problem, construct B = (C, V , p, kSk),
511

fiFaliszewski, Hemaspaandra, & Hemaspaandra

bribery instance, successful manipulation within
successful bribery within B. assume fulfills -weighted-manipulation
requirements regarding relative weights voters V S. not, output fixed
B successful briberies.
reduction works constructing V = V , set voters
fixed arbitrary preference list p least preferred candidate. Clearly,
manipulation possible within bribery works B. show
direction also holds arguing successful bribery within B exists,
successful bribery affects voters . implies viewed
manipulative group.
Let us assume way bribing kSk voters V p
becomes winner. bribed voters theorem proven. Otherwise,
select bribed voter v V . bribing v, p gains (1 + 1 ) (v) points
candidate c 6= p. (The first 1 p get 1 additional points
bribery, second 1 c lose 1 votes.) However,
instead bribing v would bribe voter v , p would gain least 1 (v ) points
c. (We would bribe v put p preferred candidate shift
candidates back.) Since holds (v ) 2(v), might well bribe
v instead v, p would still winner. Thus, p made winner, p
made winner bribing voters .
reduction easily computed polynomial time.

remains show restricted version manipulation NP-complete
scoring protocols nonrestricted version is.
Lemma 4.12 = (1 , . . . , ) scoring protocol case
2 = 3 = = , -weighted-manipulation NP-complete.
Proof. Let = (1 , . . . , ) scoring protocol 2 6= . use Hemaspaandra Hemaspaandras (2007) proof result called Theorem 4.7
show NP-completeness -weighted-manipulation . Clearly, -weighted-manipulation
NP need prove NP-hardness.
Hemaspaandra Hemaspaandras (2007) proof Theorem 4.7 reduces Partition (restricted positive integers) -weighted-manipulation. close inspection proof10
shows exist constants c 2 depend
evP
ery sequence positive integers s1 , . . . , sn ni=1 si = 2S, Hemaspaandra
Hemaspaandra reduction outputs manipulation problem following properties.
1. nonmanipulative voter weight cS,
2. weights manipulative voters ds1 , ds2 , . . . , dsn .
use facts provide reduction Partition -weighted-manipulation .
reduction
works follows. Let s1 , . . . , sn input sequence nonnegative
P
2
integers, ni=1 si = 2S, i, 1 n, holds si 2+n
S. (As per
footnote 3, conditions hold return fixed string -weighted10. repeat proof here. Interested readers referred paper Hemaspaandra
Hemaspaandra (2007).

512

fiHow Hard Bribery Elections?

manipulation .) Without loss generality, assume > 0, thus s1 , . . . , sn
positive integers. Let f reduction given proof Theorem 4.7 paper
Hemaspaandra Hemaspaandra (2007). compute f ((s1 , . . . , sn )) = (C, V, T, p).
(Reduction f works general Partition so, since already checked special properties required Partition , work correctly input.) is, s1 , . . . , sn
partitioned successful manipulation (C, V, T, p). Unfortunately, cannot output (C, V, T, p) necessarily fulfill condition
voters weights. Recall ensure manipulative voter weight
least twice high weight heaviest nonmanipulative voters. Let
smin = min{sj | 1 j n}. (C, V, T, p), least weight voter exactly dsmin ,
highest weight voter V cS. However, split voter v
V . weights voters participate manipulation irrelevant
long total weight voters given preference order change. Thus,
replace voter high weight several voters preference
order lower weights. case, need make sure nonmanipulative
voter weight 21 dsmin . Since heaviest nonmanipulative voters
weight cS, need replace voter v V
&
'
cS
(4)
21 dsmin
voters, weight 12 dsmin . Since 2, > 0, smin positive integer,
2
2+n smin , bound (4)
&

cS
1
2 dsmin

'



cS

smin





&

cS
2S
2+n

'

=




c(n + 2)
,
2

clearly polynomially bounded n. Thus, splitting voters easily
performed polynomial time, since change result manipulation,
theorem proven.

proof Theorem 4.9 simply combines Lemmas 2.3, 4.11, 4.12.
Theorem 4.9 shows bribery within weighted scoring protocols is, cases,
difficult. Though weighted bribery light Theorem 4.9 easy trivial elections (1 =
), plurality, even pluralitys equivalent clones (all scoring systems 1 > 2 =
= ), voters weighted also prices (by Theorem 3.2)
bribery also becomes difficult case plurality pluralitys equivalent clones.
interesting ask whether voters prices weighted also yields
dichotomy result. Theorem 4.13 shows, behavior scoring protocols respect
priced voters different respect weighted ones.
Theorem 4.13 Let = (1 , . . . , ) scoring protocol. -$bribery P.
Proof. give polynomial-time algorithm -$bribery. Let E = (C, V, p, k)
instance problem. First, observe considering scoring protocol =
(1 , . . . , ) we, definition, limit scenario candidates,
fixed constant. implies constant number different preference
513

fiFaliszewski, Hemaspaandra, & Hemaspaandra

orders, o1 , . . . , om! , voters might have. partition V sets V1 , V2 , . . . , Vm!
Vi contains exactly voters preference order oi . Vi might
empty Vi n elements, n = kV k.
bribery within E described giving two sequences integers, b1 , . . . , bm!
d1 , . . . , dm! , 0 bi kVi k 0 di n, 1 m!,
m!
X
i=1

bi =

m!
X

di .

i=1

bi says many voters Vi bribing. sufficient give
numbers biPsince want bribe cheapest members Vi . bribe
b = m!
i=1 bi voters, need decide preferences assign them.
described sequence d1 , . . . , dm! : di says many b voters assigned
preferences oi . Since voters indistinguishable, specifying numbers
enough.
remains observe nm! sequences b1 , . . . , bm!
nm! sequences d1 , . . . , dm! b. Thus, n2(m!) sequences try
out. pair sequences easy check whether performing described
bribery p becomes winner whether budget exceeded. Thus, -$bribery
P.

algorithm given proof Theorem 4.13, almost changes, used
prove following corollary.
Corollary 4.14 Let E election system (a) fixed number candidates outcome computable polynomial time (b) outcome depend
order votes. fixed number candidates E-$bribery P.
Theorem 4.13 stands sharp contrast Theorem 4.9. natural ask prices
weights exhibit differing behavior. One answer weighted case
voters retain individualitytheir weightsthroughout whole process bribery.
hand, priced case voters disassociated prices
soon decide bribe them. decide bribe particular priced voter
simply need add price total budget, voter
indistinguishable bribed ones. Precisely observation facilitated
proof Theorem 4.13.
algorithm given proof Theorem 4.13 rather disappointing running
time. nO(m!) polynomial setting, one would certainly prefer
algorithm whose time complexity depend way. particular, would
nice algorithm running time polynomial n + m. However,
algorithm exists P = NP. follows proof fact approval-bribery
NP-complete. proof showed reduce X3C approval-bribery
way voter approves 3 candidates. polynomial p
algorithm ran time p(kCk + kV k) every scoring protocol , could
solve X3C reducing approval-bribery embedding approval-bribery
problem -bribery problem = (1, 1, 1, 0, . . . , 0), possibly adding
dummy candidates. embedding straightforward describe detail.
514

fiHow Hard Bribery Elections?

Let = (1 , . . . , ) scoring protocol case 2 =
= . Theorem 4.9 know -weighted-bribery NP-complete. also
know, Theorem 4.13, -$bribery P. clearly holds -weighted-$bribery
NP-complete, interesting ask whether NP-completeness -weightedbribery -weighted-$bribery holds possibly exponentially large values
weights, problems remain NP-complete even weights encoded
unary? turns out, following theorem, high weight values necessary
NP-completeness.
Theorem 4.15 Let = (1 , . . . , ) scoring protocol. -weightedunary -$bribery
P.
Proof. Let = (1 , . . . , ) scoring protocol. proof theorem cashes
observation made proof Theorem 4.13:
finitely many different preference orders, polynomially many substantially
different ways bribing.
Let E = (C, V, p, k) bribery problem let o1 , . . . , om! different possible
preference orders C. partition V m! disjoint sets V1 , . . . , Vm! Vi
contains exactly voters preference order oi . bribery within E described
sequence m! vectors bi = (bi,1 , bi,2 , . . . , bi,m! ), 1 m!, i, j,
1 i, j m!, bi,j nonnegative integer i, 1 m!,
m!
X

bi,j = (Vi ).

j=1

interpretation vector bi voters Vi partitioned m! sets
Vi,1 , . . . , Vi,m! (Vi,j ) = bi,j , intention bribing voters Vi,j change
preference lists oj (recall Vi multiset, course multiset
partition Vi,j multisets). 6= j bribery price,
= j free nothing really needs done. Note vectors
realizable; every splitting vote weight (Vi ) achieved. rest proof
devoted developing method evaluating whether given split possible
minimal cost is. ((V )m! )m! ways selecting vectors b1 , . . . , bm!
test whether given vector realizable (and compute minimal price realization),
simply try sequences vectors test whether makes
p winner (the winner, unique case) total cost fall within budget.
describe algorithm checks particular vector w = (w1 , . . . , wm! ),
wi {0, . . . , (V )} {1, . . . , m!}, realizable computes minimal
price ws realization. Vi (w1 , . . . , wm! ) mean following set m!-element
sequences subsets Vi :

Vi (w) = {(Vi,1 , . . . , Vi,m! ) | (Vi = m!
j=1 Vi,j ) (1 j m!)[(Vi,j ) = wj ]}.
w define

P
min{ | ((Vi,1 , . . . , Vi,m! ) Vi (w))[ = j6=i (Vi,j )]}
gi (w) =

515

Vi (w) 6= ,
otherwise.

fiFaliszewski, Hemaspaandra, & Hemaspaandra

is, gi (w) gives lowest price bribing voters Vi according weight vector
(w1 , . . . , wm! ). compute gi (w) polynomial time using dynamic programming
techniques. Let us rename candidates Vi = {v1 , . . . , vt } let gi, (w)
gi (w) except restricted voters v , . . . , vt . Thus, gi,1 exactly gi . Naturally,
following boundary condition holds gi,t+1 .

0
w1 = w2 = = wm! = 0,
gi,t+1 (w1 , . . . , wm! ) =

otherwise.
compute values gi, (w1 , . . . , wm! ) using dynamic programming observation
gi, (w1 , . . . , wm! ) equal minimum following:
gi,+1 (w1 (v ), w2 , . . . , wm! ) + (v ),
gi,+1 (w1 , w2 (v ), w3 , . . . , wm! ) + (v ),
...
gi,+1 (w1 , . . . , wm!1 , wm! (v )) + (v ),
gi,+1 (w1 , . . . , wi1 , wi (v ), wi+1 , . . . , wm! ).
Note last values handles fact bribe v report preference
order oi actually need pay her; v already preference order
oi . Otherwise, need decide m! 1 preference orders ask v
report, need pay change. Clearly, using rule boundary
condition compute gi,1 (w), thus gi (w), time polynomial (V ). Since (V )
polynomial size input, completes proof.

Corollary 4.16 Let E election system (a) fixed number candidates outcome computable polynomial time (b) outcome depend
order votes. fixed number candidates E-weightedunary -$bribery
P.
Note that, Theorem 4.6, holds scoring protocol ,
-weightedunary -manipulation pm -weightedunary -$bribery, latter P,
following corollary.
Corollary 4.17 scoring protocol , -weightedunary -manipulation P.
Certain scoring protocols natural generalizations arbitrary number candidates, e.g., plurality rule, Borda count, veto rule. results
imply easiness bribery election systems, need single P algorithm
work cases. example, case Borda count, recently Brelsford et al. (2008)
shown even Borda-bribery NP-complete. cases, easiness results
easily obtained hand. example, Theorem 4.9 immediately implies vetoweighted-bribery NP-complete even 3 candidates, yet following result shows
difficulty bribery veto voting comes purely weighted votes.
Theorem 4.18 veto-bribery P.
516

fiHow Hard Bribery Elections?

Proof. proof theorem essentially Theorem 3.1.
view veto elections elections every voter vetos one candidate, candidate
least number vetoes wins. (In unique case, candidate winner
candidate vetoes has.)
Thus, given instance E = (C, V, p, k), keep bribing voters veto p ask
veto candidate that, time, least number vetoes.
k bribes p winner accept; otherwise reject. simple inductive argument
shows correct strategy, algorithm clearly runs polynomial time.

Interestingly, Brelsford et al. (2008) showed veto-weightedunary -manipulation
NP-complete. immediately gives, Theorem 4.6, veto-weightedunary -$bribery
NP-complete. Using techniques similar used proof Theorem 4.9
(but much simpler) modify reduction show even veto-weightedunary bribery NP-complete. hand, main result paper Faliszewski (2008)
implies veto-$bribery P. whether Theorem 4.18 follows immediately
Theorem 4.13, not. Why? Recall 4.13 covers veto fixed number
candidates, Theorem 4.18 covering protocol handles veto numbers
candidateswhat people commonly think think veto voting system.
spirit obtained Theorem 4.9 reducing (with much work adjustment)
manipulation bribery, scoring protocols. work settings?
answer no; designed artificial voting system checking manipulability
even one voter NP-complete, checking bribability easy.
Theorem 4.19 exists voting system E manipulation NP-complete,
bribery P.
Proof. Let NP-complete set let B P
1. = {x | (y )[hx, yi B]},
2. (x, )[hx, yi B |x| = |y|].
sets easily constructed NP-complete set padding. idea
proof embed verifier within election rule E. way forces
manipulation solve arbitrary instances, allowing bribery still easy.
First, observe preference lists used encode arbitrary binary strings.
use following encoding. C set candidates, let c1 , c2 , . . . , cm
candidates lexicographical order. view preference list
ci1 > ci2 > ci3 > > cim
encoding binary string b1 b2 bm/2 ,
bj =



0
1

i2j1 > i2j ,
otherwise.

encoding course efficient one, given binary string may
many preference lists encode it. However, encoding easy
properties need construction.
517

fiFaliszewski, Hemaspaandra, & Hemaspaandra

reduction, binary strings starting 1 encode instances, binary strings
starting 0 encode witnesses. Given setup, describe election system
E. Let (C, V ) election. c C, c winner election
kV k = 3
Rule 1: preference lists encode strings starting 1 preference lists encode
strings starting 0,
Rule 2: exactly one preference list encodes string starts 1, say 1x, least
one preference list encodes string 0y hx, yi B.
Thus, either candidates winners none winners. Note testing
whether candidate c winner E election easily done polynomial time.
following polynomial-time algorithm shows perform optimal bribery.
implies E-bribery P.
1. c winner, nothing.
2. Otherwise, kV k =
6 3, bribery impossible.
3. Otherwise, exactly one voter whose preference list encodes string
starts 1, bribe voter encode string starts 0.
Rule 1, c winner election.
4. Otherwise, exactly one voter whose preference list encodes string starts
0 bribe voter preference list encodes string
starts 1. Rule 1, c winner election.
hand, ability solve manipulation problem E implies ability
solve A. construct reduction E-manipulation. Given string x ,
first check whether hx, 0|x| B. so, clearly x output fixed
member E-manipulation. Otherwise, output manipulation problem candidates
{1, 2, . . . , 2(|x| + 1)} three voters, v0 , v1 , v2 , v0 preference list encodes
1x, v1 preference list encodes 00|x| , v2 manipulative voter. claim
candidate 1 made winner x A.
Since hx, 0|x| 6 B, way v2 make 1 winner v2 encodes
string 0y hx, yi B case x A. converse, x A,
exists string |x| hx, yi B. encode string 0y preference list
{1, 2, . . . , 2(|x| + 1)}, let preference list v2 . ensures 1
winner election.
Since reduction computed polynomial time, E-manipulations
membership NP clear, E-manipulation NP-complete.
result holds case unique winners. case modify E
lexicographically smallest candidate win election reduction
define distinguished candidate lexicographically smallest candidate.

election system natural, show unless restrict
election rules somehow prove P = NP, obtaining general reduction manipulation
bribery seems precluded.
518

fiHow Hard Bribery Elections?

5. Succinct Elections
far discussed nonsuccinct electionsones voters
preference lists (and weights, voters weighted) given listing one
time (as given stack ballots). also natural consider case
preference list frequency conveyed via count (in binary), refer
succinct input. succinct representation particularly relevant case
number candidates bounded constant. many candidates,
natural expect lot voters preferences vary insignificant
ways. hand, candidates then, naturally,
large numbers voters preferences, using succinct representation
save lot space.
section provide P membership results (and due proofs, fact
even FPT membership results11 ) regarding bribery succinctly represented elections
fixed number candidates. main tool Lenstras (1983) extremely
powerful result integer programming feasibility problem P number
variables fixed.
Theorem 5.1 (Lenstra, 1983) Let k fixed nonnegative integer.
polynomial-time algorithm given k integer matrix vector b Zm
determines whether
{x Zk | Ax b} =
6
holds. is, integer linear programming P fixed number variables.
mention Lenstras polynomial-time algorithm attractive practically
speaking. particular, although algorithm uses linear number arithmetic
operations linear-sized integersand thus theoretically attractive, low-degree
polynomial run-timethe multiplicative constant large. us
critical issue since mostly interested polynomial-time computability results
general tools obtaining them, rather actual optimized optimal algorithms.
Although Lenstras result applies integer linear programming problem
number variables fixed achieves P-time case, section fact
11. Regarding natural issue P results strengthened FPT results, mention
passing every P membership result section clearly (although implicitly), via proof, even
FPT membership result. (A problem parameter j FPT, class capturing notion
fixed-parameter tractable, exists algorithm whose running time instances size n
bounded f (j)nO(1) , f function depending j; see Niedermeier, 2006, detailed
coverage parameterized complexity.) Essentially, Lenstras method well known
use linear number arithmetic operations linear-sized variables (Lenstra, 1983, see also Downey,
2003; Niedermeier, 2002). Although fact voting problems FPT implicit
seminal work Bartholdi et al., 1989b (e.g., see Christian, Fellows, Rosamond, & Slinko, 2007; Betzler,
Guo, & Niedermeier, 2008b), mention work Christian et al. (2007, Section 4),
bribery-like flavor results, explicitly addresses issue FPT, particular mentioning
well known (nonsuccinct) winner score problems Kemeny Dodgson
FPT, indebted earlier version paper motivated us mention
sections P results (even succinct elections) FPT results. Among papers
addressing FPT results election problems (although regarding bribery problems), mention
examples work Betzler, Fellows, Guo, Niedermeier, Rosamond (2008a) Kemeny voting
Faliszewski et al. (2008) Llull Copeland voting.

519

fiFaliszewski, Hemaspaandra, & Hemaspaandra

typically need special case result number variables
number constraints fixed (and parameter changing
constants within constraints).
P membership results regarding succinctly represented elections naturally imply analogous results nonsuccinct representation. express fact succinctness
representation optional cases, put phrase succinct curly braces
names problems. example, say plurality-{succinct}-bribery P,
mean plurality-bribery plurality-succinct-bribery P. (By way,
Theorem 3.1, similar careful algorithm one proof also holds
succinct case.)
proceed results, let us introduce notation. Throughout
section assume bribery problems dealing exactly
candidates, arbitrary fixed constant. Thus, E = (C, V, p, k) bribery
problem may assume C = {1, . . . , m}, o1 , o2 , . . . , om!
possible preference orders C. Given set voters V , Vi , 1 m!, mean
set voters v V preference order oi . given i, define wh(c, i)
index candidate c within preferences oi (informally, c oi ). notation
assumed proofs section.
Using integer programming approach obtain polynomial-time algorithms
bribery scoring protocols succinct nonsuccinct cases.
approach yields similar result manipulation. (The nonsuccinct case manipulation
already obtained work Conitzer et al., 2007.)
Theorem 5.2 every scoring protocol = (1 , . . . , ), -{succinct}-bribery
-{succinct}-manipulation P.
Proof. Let = (1 , . . . , ) scoring protocol let E = (C, V, p, k) bribery
problem want solve, C = {1, . . . , m}. bribery described providing
numbers si,j , 1 i, j m!, saying many people switch preference order oi
preference order oj . (The values si,i simply say many voters preference order
oi switch anything else. allow superfluous exchanging, e.g.,
legal, even 6= j, si,j sj,i strictly greater zero. However,
note proof, example, solution happens
another solution holds that, 6= j, least one si,j
sj,i zero.) may express integer program fact si,j describe
successful bribery, follows. Here, variables, si,j s, required
integers. (m!)2 variables. constants (they constantsi.e.,
coefficientsfrom integer linear programming perspective, constants
complexity perspective, effect k, ||V1 ||, . . . , ||Vm! || inputs problem)
k, 1 , 2 , . . . , , kV1 k, kV2 k, . . . , kVm! k.
1. number bribed voters nonnegative. i, j, 1 i, j m!,

si,j 0.
2. cannot bribe voters given preference are. i,
1 m!, constraint (keeping mind si,i pick
520

fiHow Hard Bribery Elections?

leftover, thus state equality)
m!
X

si,j = kVi k.

j=1

3. Altogether, bribe k people.
m! X
m!
X

si,j

i=1 j=1

m!
X

s, k.

=1

4. score p least high anybody elses. h, 1 h m,
constraint says candidate h defeat p:
!
!
m!
m!
m!
m!
X
X
X
X
wh(h,j)
si,j .
wh(p,j)
si,j
j=1

j=1

i=1

i=1

constant number variables, (m!)2 , constant number constraints.
(Of course, hand size integer linear programs constants
particular k, kV1 k, . . . , kVm! kmay increase number voters.) Thus using
Lenstras algorithm polynomial time test whether set constraints
satisfied legal si,j s. clear constraints satisfied
bribery k voters leads making p winner. Also, note
make program test whether p become unique winner simply make
inequalities final set constraints strict.
case manipulation proved similarly, would variables
si would say many manipulators decide report preference order oi .
omit detailed description clear given above.

power integer programming approach limited case scoring
protocols. fact, seminal paper Bartholdi et al. (1989b) shows applying
method computing Dodgson score nonsuccinct elections fixed number
candidates yields polynomial-time score algorithm (and though paper Bartholdi
et al., 1989b, address issue succinct elections, one see
method works perfectly; is, implicit Lenstra approach paper
Bartholdi et al., 1989b, Dodgson score elections fixed number candidates
is, even succinct case, FPT, see also footnote 11).12 similar program
used compute scores within Young elections. Let us recall definition
Dodgson Young scores.
Definition 5.3 Given set candidates C set voters V specified via preferences, Dodgson score candidate c C minimum number adjacent switches
within preferences voters make c Condorcet winner.
Young score candidate c C minimum number voters need
removed elections make c Condorcet winner.
12. mention passing recent work responds theoretical complexity Dodgson scores
different direction, namely, studying success rate simple heuristics problem (McCabe-Dansted et al., 2008; Homan & Hemaspaandra, 2009).

521

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Applying integer programming attack case bribery within Dodgson-like
election systems, i.e., Dodgson system Young system, complicated.
systems involve intricate interaction bribing voters
changing preferences. Dodgson elections, bribery, still need worry
adjacent switches within voters preference lists make particular candidate
Condorcet winner. Young elections, need consider voters removed
elections. interaction seems complicated captured
integer linear program, building flavor Bartholdi et al. (1989b) integer
programming attack achieve following: Instead making p winner,
attempt make p given Dodgson Young score.
Formally, DodgsonScore-bribery (and succinctly encoded variant,
DodgsonScore-succinct-bribery) mean problem takes input Dodgsonbribery instance (C, V, p, k) (with voters encoded succinctly succinct variant)
nonnegative integer t, asks possible ensureby bribing k votersthat
ps Dodgson score t. define YoungScore-bribery YoungScore-succinctbribery analogously.
Theorem 5.4 fixed number candidates, DodgsonScore-{succinct}-bribery
P restricted number candidates.
Proof. Given nonnegative integer bribery problem instance E = (C, V, p, k)
Dodgson elections, C = {1, . . . , m}, give integer program tests
whether possible bribe k voters way that, bribery, p
Dodgson score t. program constant number variables
constant number constraints. Thus Lenstras algorithm solved polynomial
time.
process bribery has, case Dodgson elections, two phases: bribery phase
decide bribe voters, swapping phase (in effect) allow
adjacent swaps occur. model first phase integer variables bi,j
second phase integer variables si,j : i, j, 1 i, j m!, interpretation
bi,j si,j follows.
bi,j number voters preference order oi bribed report preference
order oj .
si,j number voters who, bribery, change preference order oi
oj .
values bi,i say many voters preferences oi bribed. values si,i
say many voters preferences oi change preferences.
variables bi,i si,i make equations neater.
Recall Dodgson score number adjacent switches within preference lists
needed make given candidate Condorcet winner. However, variables si,j
talk much complicated operations, namely transfers preference order oi
preference order oj . i, j, 1 i, j m!, define constant switches i,j gives
minimum number adjacent switches lead preference order oi preference

522

fiHow Hard Bribery Elections?

order oj .13 every preference order oi every two candidates r q define
who(r, q, i) 1 r strictly defeats q preference order oi 1 otherwise.
who(r, r, i) = 1, never invoke fact. integer linear program
following constraints.
1. number bribes switches nonnegative. i, j, 1 i, j m!,

bi,j

0,

si,j

0.

2. cannot bribe voters are. i, 1 m!, require
m!
X

bi,j = kVi k.

j=1

3. Altogether, bribe k people.
m!
m! X
X

bi,j

i=1 j=1

m!
X

b, k.

=1

4. number voters switch preference order oi swapping phase
needs equal number voters who, bribery, preference order
oi . i, 1 m!,
m!
m!
X
X
si,k .
bj,i =
j=1

k=1

5. swapping phase, p Condorcet winner. every q C {p},
m!
m! X
X

who(p, q, j ) si,j > 0.

i=1 j=1

6. swapping phase involves adjacent switches within preference lists.
m!
m! X
X

switches i,j si,j t.

i=1 j=1

13. astute reader note seek meet beat given score p given amount
bribery, one would never need Dodgson score calculation invoke exchanges anything
except move p ahead number slots. true, thus rather (m!)2 variables si,j ,
one could define integer linear program replaced si,j (m!)(m1) variables capture
shifting. define things current general way since current approach removes
dependence get away shifts sort argument type made
(which would work fine might hold sharply different settings), current approach
also leads quite simple, uncluttered constraint equations.

523

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Clearly, program contains constant number variables constant number
constraints. Thus light Lenstras algorithm theorem proven.

Theorem 5.5 fixed number candidates, YoungScore-{succinct}-bribery P
restricted number candidates.
Proof. proof similar proof Theorem 5.4. Let nonnegative
integer let E = (C, V, p, k) bribery instance C = {1, . . . , m}. want
provide algorithm tests whether possible ensure p Young score
bribing k voters. providing integer linear program.
workings integer linear program divided two phases: bribery
phase removal phase. bribery described variables bi,j , 1 i, j m!,
say many voters preferences oi bribed preferences oj .
removal described variables ri , 1 m!, say many voters
preferences oi bribery removed. enforce above, use
following constraints:
1. number bribes removals nonnegative. i, j, 1 i, j m!,

0,

bi,j

ri 0.
2. cannot bribe voters are. i, 1 m!,
m!
X

bi,j = kVi k.

j=1

3. Altogether, bribe k people.
m!
m! X
X

bi,j

i=1 j=1

m!
X

b, k.

=1

4. number voters preference order oi removed election
removal phase bounded number voters
bribery preference order oi . i, 1 m!,
ri

m!
X

bj,i .

j=1

5. removal phase, p Condorcet winner. every q C {p},
!
!
m!
m!
X
X
bi,j rj who(p, q, j ) > 0.
j=1

i=1

524

fiHow Hard Bribery Elections?

6. removal phase removes voters.
m!
X

ri t.

i=1

Clearly, constant number variables constraints, integer linear
program solved using Lenstras algorithm polynomial time.

two theorems say test polynomial time whether given bribe
suffices obtain beat given Dodgson Young score. Thus using binary search
fact find optimal bribe obtaining particular score.
issue actually making candidate p winner (a unique winner, studying
unique winner case) Dodgson elections is, already indicated, much difficult
direct attack using integer linear programming seems fail. Nonetheless, combining
integer programming method brute-force algorithm resolves issue
nonsuccinct case.
Theorem 5.6 fixed number candidates, Dodgson-bribery, Dodgson-$bribery,
Young-bribery, Young-$bribery P.
Proof. Theorem 4.13, polynomially many briberies need check.
test whether favorite candidate becomes winner, using Bartholdi
et al.s (1989b) integer linear program Dodgson score-testing similar one Young
score-testing.

discussions bribery respect Dodgson elections lead observation
small change voting system allow us resolve natural bribery-related
winner problem. Note bribes allow us completely change given voters preference
listand goes far beyond switches allowed Dodgson score-counting. interesting observe one define Dodgson-like voting system based bribes: Instead
counting many switches needed make given candidate Condorcet winner,
count many bribes (where bribe complete overwrite, unit cost, one
voters preference list) suffice guarantee outcome. call election system
Dodgson . comments, fixed number candidates computing winners
Dodgson elections done polynomial time.
Theorem 5.7 fixed number candidates, winner problem succinct
Dodgson elections P.
Proof. follows immediately Theorem 5.4. candidate c simply need
binary search smallest bribe makes Condorcet winner (i.e.,
gives c Dodgson score zero). winners candidates least number
bribes needed.

Clearly, like Dodgson, Dodgson elects Condorcet winner whenever one exists.
Theorem 5.7 shows fixed number candidates winner problem succinct
Dodgson elections P, Definition 5.3 noted holds
succinct Dodgson elections. attractive, depend
one feels natural model counting distancecounting adjacent switch
counting voter change. settings latter seems
525

fiFaliszewski, Hemaspaandra, & Hemaspaandra

attractive, course Dodgson seems preferable Dodgson. Nonetheless, jumping
Dodgson bandwagon, one probably first carefully study properties
new election system. Note even though computing Dodgson winners fixed
number candidates polynomial-time procedure, immediately imply
bribery problem easy Dodgson , conjecture not.
light discussion, might seem Dodgson-like election rules getting
polynomial-time bribery results (in succinct model) difficult using integer linear
programming. However, always case. particular, following theorem
states bribery Kemeny system easy fix number candidates. Recall
candidate c winner Kemeny elections exists preference order oh
lists c top agrees strongly votes. (See Section 2.1.)
Theorem 5.8 fixed number candidates, Kemeny-{succinct}-bribery P
restricted number candidates.
Proof. proof employs integer linear programming, time need
one program. informally put, integer linear programs seemingly
express conjunctions, disjunctions, case Kemeny elections
need express fact least one preference orders lists favorite
candidate top disagrees least number voters preferences.14
Let E = (C, V, p, k) bribery instance Kemeny elections, C = {1, . . . , m}.
preference order oh , 1 h m!, p top candidate oh , construct
separate integer linear program feasible solution bribery
k candidates oh ordering maximizes (compared
orders) number agreements voters reported preferences. agree i,j
mean number agreements preference orders oi oj (see Section 2.1).
Let us consider arbitrary h p top candidate preference order oh .
describe bribery using variables bi,j , 1 i, j m!, saying many voters
preference order oi bribed preference order oj . employ following
constraints.
1. number bribes nonnegative. i, j, 1 i, j m!,
bi,j 0.
2. cannot bribe voters are. i, 1 m!,
m!
X

bi,j = kVi k.

j=1

3. Altogether, bribe k people.
m!
m! X
X

bi,j

i=1 j=1

m!
X

b, k.

=1

14. natural way expressing disjunction within single integer program use boolean variables
indicating preference order concentrating on. However, leads integer quadratic
program.

526

fiHow Hard Bribery Elections?

bribery problem
E-bribery
E-$bribery
E-weightedunary -$bribery
E-weighted-bribery
E-weighted-$briberyunary
E-weighted-$bribery

plurality
P
P
P
P
P
NP-complete

election system E
approval
veto
NP-complete
P
NP-complete
P (Faliszewski, 2008)
NP-complete NP-complete (Brelsford et al., 2008)
NP-complete
NP-complete
NP-complete
NP-complete
NP-complete
NP-complete

Table 1: complexity bribery plurality, approval, veto (in setting
number candidates bounded). results attributed work
Brelsford et al. (2008) Faliszewski (2008) follow via simple arguments
results papers.

4. preference order disagrees voters preferences least many times
oh . , 1 m!,




m!
m!
m!
m!
X
X
X
X
agree i,h
bj,i
agree i,
bj,i .
i=1

j=1

i=1

j=1

Clearly, integer program constant number constraints constant
number variables. Thus solved separately, using Lenstras algorithm,
polynomial time. since constant numberm!of integer linear
programs regarding given input, m! applications Lenstras algorithm solve
them. one feasible solution bribery possible otherwise
not.

interesting consider features Kemeny elections allow us employ
attack, given approach seem work either Dodgson
Young elections. One reasons universal quantification implicit Dodgson
Young elections exponentially large search space, quantification
Kemeny is, case fixed candidate set, fixed number options.

6. Conclusions
paper provides study bribery respect plurality rule provides tools
results regarding many election systems, scoring protocols, approval voting,
Condorcet-winner based elections. Bribery seems important issue manipulation control; paper addresses gap knowledge complexity
voting systems. Tables 1 2 collect main results paper regarding
complexity bribery scoring protocols related election systems. (However,
course, paper contains many results cannot easily presented form
table, e.g., Theorem 4.19 results Section 5.)
One important contributions paper pointing out, concrete examples,
NP-completeness results may guarantee difficulty natural problem
instances. particular, Theorem 3.2 says plurality-weighted-$bribery NP-complete,
527

fiFaliszewski, Hemaspaandra, & Hemaspaandra

bribery problem
-bribery
-$bribery
-weightedunary -$bribery
-weighted-bribery
-weighted-$briberyunary
-weighted-$bribery

Scoring protocol = (1 , . . . , ).
1 > 2
true
1 = = 2 = = 2 = =
P
P
P
P
P
P
P
P
P
P
P
NP-complete
P
P
NP-complete
P
NP-complete
NP-complete

Table 2: complexity bribery within scoring protocols.
Theorem 3.8 observes either weights prices small enough,
problem solved efficiently.
Another contribution paper relate manipulation bribery, thus making result transfer former latter reasonable line attackand one already
exploited spirit proof approach central dichotomy result (Theorem 4.9).
suggested future work, believe studying approximation algorithms
control (by voter/candidate addition/deletion) bribery problems currently known
NP-complete would attractive next step point reader recent papers
regarding approximation manipulation, bribery, control (Brelsford, 2007; Brelsford
et al., 2008; Faliszewski, 2008; Zuckerman et al., 2008). would also interesting study
complexity bribery settings, incomplete information, multiple
competing bribers, complicated bribe structures (see work Faliszewski, 2008,
preliminary results bribery involved pricing schemes).

Acknowledgments
grateful Samir Khuller helpful conversations Bartholdi et al.
(1989b) integer programming attack fixed-candidate Dodgson elections. also
grateful anonymous referees Preetjot Singh helpful comments. work
supported part grants NSF-CCR-0311021, NSF-CCF-0426761, NSF-IIS-0713061,
AGH-UST 11.11.120.777, Friedrich Wilhelm Bessel Research Awards Edith Hemaspaandra Lane A. Hemaspaandra, Alexander von Humboldt Foundations
TransCoop program. work done part Piotr Faliszewski University Rochester. early version paper, titled Complexity Bribery
Elections, appeared proceedings AAAI-06 (Faliszewski, Hemaspaandra, & Hemaspaandra, 2006) also presented COMSOC-06 NESCAI-07.

References
Bartholdi, III, J., Tovey, C., & Trick, M. (1989a). computational difficulty manipulating election. Social Choice Welfare, 6 (3), 227241.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989b). Voting schemes
difficult tell election. Social Choice Welfare, 6 (2), 157165.

528

fiHow Hard Bribery Elections?

Bartholdi, III, J., Tovey, C., & Trick, M. (1992). hard control election?
Mathematical Computer Modeling, 16 (8/9), 2740.
Betzler, N., Fellows, M., Guo, J., Niedermeier, R., & Rosamond, F. (2008a). Fixedparameter algorithms Kemeny scores. Proceedings 4th International Conference Algorithmic Aspects Information Management, pp. 6071. SpringerVerlag Lecture Notes Computer Science #5034.
Betzler, N., Guo, J., & Niedermeier, R. (2008b). Parameterized computational complexity
Dodgson Young elections. Proceedings 11th Scandinavian Workshop
Algorithm Theory, pp. 402413. Springer-Verlag Lecture Notes Computer Science
#5124.
Black, D. (1958). Theory Committees Elections. Cambridge University Press.
Brelsford, E. (2007). Approximation elections. Masters thesis, Rochester Institute
Technology, Rochester, NY.
Brelsford, E., Faliszewski, P., Hemaspaandra, E., Schnoor, H., & Schnoor, I. (2008). Approximability manipulating elections. Proceedings 23rd AAAI Conference
Artificial Intelligence, pp. 4449. AAAI Press.
Christian, R., Fellows, M., Rosamond, F., & Slinko, A. (2007). complexity lobbying
multiple referenda. Review Economic Design, 11 (3), 217224.
Condorcet, J. (1785). Essai sur lApplication de LAnalyse la Probabilite des Decisions
Rendues la Pluralite des Voix. Facsimile reprint original published Paris, 1972,
Imprimerie Royale.
Conitzer, V., & Sandholm, T. (2006). Nonexistence voting rules usually hard
manipulate. Proceedings 21st National Conference Artificial Intelligence,
pp. 627634. AAAI Press.
Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidates
hard manipulate? Journal ACM, 54 (3), Article 14.
Dobzinski, S., & Procaccia, A. (2008). Frequent manipulability elections: case two
voters. Proceedings 4th International Workshop Internet Network
Economics, pp. 653664. Springer-Verlag Lecture Notes Computer Science #5385.
Dodgson, C. (1876). method taking votes two issues. Pamphlet printed
Clarendon Press, Oxford, headed yet published (see discussions
(McLean & Urken, 1995; Black, 1958), reprint paper).
Downey, R. (2003). Parameterized complexity skeptic. Proceedings 18th Annual IEEE Conference Computational Complexity, pp. 147168. IEEE Computer
Society Press.
Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness shared
beliefs: GibbardSatterthwaite generalized. Social Choice Welfare, 17 (1), 8593.
Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methods
web. Proceedings 10th International World Wide Web Conference, pp.
613622. ACM Press.

529

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Elkind, E., & Lipmaa, H. (2005a). Hybrid voting protocols hardness manipulation.
Proceedings 16th Annual International Symposium Algorithms Computation, pp. 206215. Springer-Verlag Lecture Notes Computer Science #3872.
Elkind, E., & Lipmaa, H. (2005b). Small coalitions cannot manipulate voting. Proceedings
9th International Conference Financial Cryptography Data Security, pp.
285297. Springer-Verlag Lecture Notes Computer Science #3570.
Ephrati, E., & Rosenschein, J. (1997). heuristic technique multi-agent planning.
Annals Mathematics Artificial Intelligence, 20 (14), 1367.
Erdelyi, G., Hemaspaandra, L., Rothe, J., & Spakowski, H. Generalized juntas NP-hard
sets. Theoretical Computer Science. appear.
Erdelyi, G., Hemaspaandra, L., Rothe, J., & Spakowski, H. (2007). approximating optimal weighted lobbying, frequency correctness versus average-case polynomial
time. Proceedings 16th International Symposium Fundamentals Computation Theory, pp. 300311. Springer-Verlag Lecture Notes Computer Science
#4639.
Erdelyi, G., Hemaspaandra, L., Rothe, J., & Spakowski, H. (2009). Frequency correctness
versus average polynomial time. Information Processing Letters, 109 (16), 946949.
Erdelyi, G., Nowak, M., & Rothe, J. (2008a). Sincere-strategy preference-based approval
voting broadly resists control. Proceedings 33rd International Symposium
Mathematical Foundations Computer Science, pp. 311322. Springer-Verlag Lecture
Notes Computer Science #5162.
Erdelyi, G., Nowak, M., & Rothe, J. (2008b). Sincere-strategy preference-based approval
voting fully resists constructive control broadly resists destructive control. Tech.
rep. arXiv:0806.0535 [cs.GT], arXiv.org. precursor appears (Erdelyi et al., 2008a).
Faliszewski, P. (2008). Nonuniform bribery (short paper). Proceedings 7th International Conference Autonomous Agents Multiagent Systems, pp. 15691572.
International Foundation Autonomous Agents Multiagent Systems.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2006). complexity bribery
elections. Proceedings 21st National Conference Artificial Intelligence,
pp. 641646. AAAI Press.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Llull
Copeland voting broadly resist bribery control. Proceedings 22nd AAAI
Conference Artificial Intelligence, pp. 724730. AAAI Press. Journal version available (Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009a).
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2008). Copeland voting
fully resists constructive control. Proceedings 4th International Conference
Algorithmic Aspects Information Management, pp. 165176. Springer-Verlag
Lecture Notes Computer Science #5034.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009a). Llull
Copeland voting broadly resist bribery control. Journal Artificial Intelligence
Research, 35, 275341.
530

fiHow Hard Bribery Elections?

Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009b). richer understanding complexity election systems. Ravi, S., & Shukla, S. (Eds.), Fundamental Problems Computing: Essays Honor Professor Daniel J. Rosenkrantz,
pp. 375406. Springer.
Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: Ties matter.
Proceedings 7th International Conference Autonomous Agents Multiagent Systems, pp. 983990. International Foundation Autonomous Agents
Multiagent Systems.
Friedgut, E., Kalai, G., & Nisan, N. (2008). Elections manipulated often. Proceedings 49rd IEEE Symposium Foundations Computer Science, pp. 243249.
IEEE Computer Society.
Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory
NP-Completeness. W. H. Freeman Company.
Gibbard, A. (1973). Manipulation voting schemes. Econometrica, 41 (4), 587601.
Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy voting systems. Journal
Computer System Sciences, 73 (1), 7383.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (1997). Exact analysis Dodgson
elections: Lewis Carrolls 1876 voting system complete parallel access NP.
Journal ACM, 44 (6), 806825.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Anyone him: complexity
precluding alternative. Artificial Intelligence, 171 (5-6), 255285.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). Hybrid elections broaden
complexity-theoretic resistance control. Mathematical Logic Quarterly, 55 (4), 397
424.
Hemaspaandra, E., Spakowski, H., & Vogel, J. (2005). complexity Kemeny elections.
Theoretical Computer Science, 349 (3), 382391.
Homan, C., & Hemaspaandra, L. (2009). Guarantees success frequency algorithm finding Dodgson-election winners. Journal Heuristics, 15 (4), 403423.
Kemeny, J. (1959). Mathematics without numbers. Daedalus, 88, 577591.
Kemeny, J., & Snell, L. (1960). Mathematical Models Social Sciences. Ginn.
Ladner, R., Lynch, N., & Selman, A. (1975). comparison polynomial time reducibilities.
Theoretical Computer Science, 1 (2), 103124.
Lenstra, Jr., H. (1983). Integer programming fixed number variables. Mathematics
Operations Research, 8 (4), 538548.
Martello, S., & Toth, P. (1990). Knapsack Problems: Algorithms Computer Implementations. John Wiley Sons.
McCabe-Dansted, J., Pritchard, G., & Slinko, A. (2008). Approximability Dodgsons
rule. Social Choice Welfare, 31 (2), 311330.
McLean, I., & Urken, A. (1995). Classics Social Choice. University Michigan Press.

531

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Meir, R., Procaccia, A., Rosenschein, J., & Zohar, A. (2008). complexity strategic
behavior multi-winner elections. Journal Artificial Intelligence Research, 33,
149178.
Niedermeier, R. (2002). Invitation fixed-parameter algorithms. Habilitation thesis, University Tubingen.
Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford University Press.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.
Procaccia, A., & Rosenschein, J. (2007). Junta distributions average-case complexity
manipulating elections. Journal Artificial Intelligence Research, 28, 157181.
Rothe, J. (2005). Complexity Theory Cryptology: Introduction Cryptocomplexity.
Springer-Verlag.
Rothe, J., Spakowski, H., & Vogel, J. (2003). Exact complexity winner problem
Young elections. Theory Computing Systems, 36 (4), 375386.
Saari, D., & Merlin, V. (2000). geometric examination Kemenys rule. Social Choice
Welfare, 17 (3), 403438.
Satterthwaite, M. (1975). Strategy-proofness Arrows conditions: Existence correspondence theorems voting procedures social welfare functions. Journal
Economic Theory, 10 (2), 187217.
Xia, L., & Conitzer, V. (2008). sufficient condition voting rules frequently
manipulable. Proceedings 9th ACM Conference Electronic Commerce,
pp. 99108. ACM Press.
Young, H. (1977). Extending Condorcets rule. Journal Economic Theory, 16 (2), 335
353.
Zuckerman, M., Procaccia, A., & Rosenschein, J. (2008). Algorithms coalitional
manipulation problem. Artificial Intelligence, 173 (2), 392412.

532

fiJournal Artificial Intelligence Research 35 (2009) 623-675

Submitted 10/08; published 08/09

Compiling Uncertainty Away Conformant Planning
Problems Bounded Width
Hector Palacios

hlp@ldc.usb.ve

Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona, SPAIN

Hector Geffner

hector.geffner@upf.edu

ICREA & Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona, SPAIN

Abstract
Conformant planning problem finding sequence actions achieving goal
presence uncertainty initial state action effects. problem
approached path-finding problem belief space good belief representations
heuristics critical scaling up. work, different formulation introduced
conformant problems deterministic actions automatically converted
classical ones solved off-the-shelf classical planner. translation maps
literals L sets assumptions initial situation, new literals KL/t
represent L must true initially true. lay general translation scheme
sound establish conditions translation also complete.
show complexity complete translation exponential parameter
problem called conformant width, benchmarks bounded. planner
based translation exhibits good performance comparison existing planners,
basis T0 , best performing planner Conformant Track 2006
International Planning Competition.

1. Introduction
Conformant planning form planning goal achieved initial
situation fully known actions may non-deterministic effects (Goldman &
Boddy, 1996; Smith & Weld, 1998). Conformant planning computationally harder
classical planning, even polynomial restrictions plan length, plan verification
remains hard (Haslum & Jonsson, 1999; Baral, Kreinovich, & Trejo, 2000; Turner, 2002;
Rintanen, 2004). practical problems purely conformant, ability find
conformant plans needed contingent planning conformant situations special
case relaxations conformant planning yield useful heuristics (Hoffmann &
Brafman, 2005).
problem conformant planning formulated path-finding problem
belief space sequence actions map given initial belief state target
belief sought (Bonet & Geffner, 2000). belief state represents set states
deemed possible, actions, whether deterministic not, map one belief state
c
2009
AI Access Foundation. rights reserved.

fiPalacios & Geffner

another. formulation, underlies current conformant planners (Hoffmann &
Brafman, 2006; Bryce, Kambhampati, & Smith, 2006; Cimatti, Roveri, & Bertoli, 2004)
must address two problems: problem representing beliefs compact way,
problem obtaining effective heuristics beliefs. first problem approached
logical representations make use SAT OBDD technology,
intractable worst case, scale better plain state representations. second
problem, hand, complex, heuristics searching belief
space successful far heuristics developed classical planning (Bonet
& Geffner, 2001; Hoffmann & Nebel, 2001).
work, introduce different approach conformant planning problems
automatically compiled classical problems solved classical planner.
translation maps sets literals initial situation literals L new literals
KL/t express true initial situation, L must true. lay first
general translation scheme sound establish conditions
translation also complete. Also, show complexity complete translation
exponential parameter problem call conformant width,
benchmark domains bounded, implying complete translation
cases polynomial. planner based translation exhibits good performance
comparison existing conformant planners basis T0 , best performing
planner Conformant Track 2006 International Planning Competition.
translation-based approach provides solution two problems faced conformant planners search belief space: belief representation heuristic
beliefs. translation-based approach, beliefs represented literals KL/t
stand conditionals, representation polynomial complete conformant problems bounded width. addition, since belief states represented
plain states, heuristic beliefs classical heuristic. computational point
view, though, explicit search belief-space: conformant problems P
converted classical problems K(P ) knowledge-level (Petrick & Bacchus, 2002),
whose solutions, computed classical planner, encode conformant solutions P .
formulation limited conformant problems deterministic
uncertainty lies initial situation. address nonetheless issues must
handled order generalize translation-based approach non-deterministic domains
report empirical results non-deterministic domains well.
paper organized follows. define first syntax semantics conformant
planning problems P (Section 2), consider simple sound incomplete translation
K0 (Section 3). consider general translation scheme KT,M
two parameters, set tags encoding assumptions initial situation,
set merges encoding valid disjunctions tags (Section 4), analyze several
instances scheme follow particular choices sets tags merges:
complete exponential translation KS0 tags associated possible
initial states problem (Section 5), polynomial translation Ki fixed integer
0 complete problems conformant width bounded (Section 6).
provide alternative explanation compact complete translation
showing problems bounded width, exponential number possible initial
states S0 includes always polynomial number critical initial states S00 plans
624

fiCompiling Uncertainty Away Conformant Planning Problems

conform S00 conform also S0 (Section 7). finally present conformant
planner T0 (Section 8), empirical evaluation planner (Section 9), extension
non-deterministic actions (Section 10), discussion related work (Section 11).
followed brief summary (Section 12) formal proofs (Appendix).
work revision extension formulation presented Palacios
Geffner (2007), turn based ideas first sketched also Palacios Geffner
(2006).

2. Conformant Problem P
define next syntax semantics conformant planning problems considered.
2.1 Syntax
Conformant planning problems P represented tuples form P = hF, I, O, Gi
F stands fluent symbols problem, set clauses F defining
initial situation, stands set (ground) operators actions a, G set
literals F defining goal. Every action precondition P re(a) given set
fluent literals, set conditional effects C L C set fluent literals
L fluent literal.
actions assumed deterministic hence uncertainty lies initial
situation. Thus, language conformant problem P excluding uncertainty
initial situation, Strips extended conditional effects negation. Moreover,
uncertainty initial situation, fluents appear unit clauses
I, P equivalent classical planning problem.
refer conditional effects C L action rules associated
a, sometimes write : C L. convenient, also join several effects
associated action condition : C L L0 write C L
true L C empty. Finally, literal L, L denotes complement L.
2.2 Semantics
state truth assignment fluents F P = hF, I, O, Gi possible initial
state P state satisfies clauses I.
state s, write I(s) refer set atoms (positive literals) true
s, write P/s refer classical planning problem P/s = hF, I(s), O, Gi
like conformant problem P except initial state fixed s.
action sequence = {a0 , a1 , . . . , } classical plan P/s action sequence
executable state results goal state sG ; i.e., = 0, . . . , n,
preconditions action ai true si , si+1 state results action
ai state si , goal literals true sn+1 .
Finally, action sequence conformant plan P iff classical plan P/s
every possible initial state P .
Conformant planning computationally harder classical planning, plan verification remains hard even polynomial restrictions plan length (Haslum & Jonsson,
1999; Baral et al., 2000; Turner, 2002; Rintanen, 2004). common approach
625

fiPalacios & Geffner

conformant planning based belief state formulation (Bonet & Geffner, 2000).
belief state b non-empty set states deemed possible given situation,
every action executable b, maps b new belief state ba . conformant
planning task becomes path-finding problem graph nodes belief states
b, source node b0 belief state corresponding initial situation, target
belief states bG goals true.
assume throughout logically consistent, set possible initial
states empty, P consistent, bodies C C 0 conflicting
effects : C L : C 0 L associated action mutually exclusive
mutex. details this; see Part B Appendix.

3. Basic Translation K0
simple translation conformant problem P classical problem K(P )
obtained replacing literals L literals KL KL aimed capturing whether
L known true known false respectively.
Definition 1 (Translation K0 ). conformant planning problem P = hF, I, O, Gi,
translation K0 (P ) = hF 0 , 0 , O0 , G0 classical planning problem
F 0 = {KL, KL | L F }
0 = {KL | L unit clause I}
G0 = {KL | L G}
O0 = precondition L replaced KL, conditional
effect : C L replaced : KC KL : KC KL,
expressions KC KC C = L1 , L2 . . . abbreviations formulas
KL1 , KL2 . . . KL1 , KL2 . . . respectively.
intuition behind translation simple: first, literal KL true initial
state 0 L known true I; otherwise false. removes uncertainty
K0 (P ), making classical planning problem. addition, soundness,
rule : C L P mapped two rules: support rule : KC KL, ensures
L known true condition known true, cancellation
rule : KC KL guarantees KL deleted (prevented persist)
action applied C known false. use support cancellation rules
encoding original rules knowledge-level subtlety translation.
translation K0 (P ) sound every classical plan solves K0 (P ) conformant
plan P , incomplete, conformant plans P classical plans K(P ).
meaning KL literals follows similar pattern: plan achieves KL K0 (P ),
plan achieves L certainty P , yet plan may achieve L certainty
P without making literal KL true K0 (P ).1
Proposition 2 (Soundness K0 (P )). classical plan K0 (P ),
conformant plan P .
1. Formal proofs found appendix.

626

fiCompiling Uncertainty Away Conformant Planning Problems

illustration, consider conformant problem P = hF, I, O, Gi F = {p, q, r},
= {q}, G = {p, r}, actions = {a, b} effects
: q r , : p p , b : q p .
problem, action sequence = {a, b} conformant plan P action
sequence 0 = {a} not. Indeed, classical plan P/s possible initial state
s, 0 classical plan possible initial state s0 p true (recall
possible initial state P satisfies neither p r assumed
initially false problem).
Definition 1, translation K0 (P ) = hF 0 , 0 , O0 , G0 classical planning problem
fluents F 0 = {Kp, Kp, Kq, Kq, Kr, Kr}, initial situation 0 = {Kq}, goals G0 =
{Kp, Kr}, actions O0 = {a, b} effects
: Kq Kr , : Kp Kp , b : Kq Kp,
encode supports, effects
: Kq Kr , : Kp Kp , b : Kq Kp,
encode cancellations.
Proposition 2 implies, example, 0 = {a}, conformant plan
P , cannot classical plan K(P ) either. easy verify, support
: Kq Kr achieves goal Kr Kq true 0 , cancellation : Kp Kp
associated action, preserves Kp false goal p.
translation K0 complete, meaning fails capture conformant plans P classical plans, completeness assessed terms weaker
semantics. so-called 0-approximation semantics (Baral & Son, 1997), belief states b
represented 3-valued states fluents true, false, unknown. incomplete belief representation, checking whether action applicable belief state b,
computing next belief state ba , verifying polynomial length plans polynomial
time operations. particular, literal L true next belief state ba iff a) action
effect C L literals C true b, b) L true b
effects C 0 L action a, complement literal L0 C 0 true b. action
sequence conformant plan P according 0-approximation semantics
belief sequence generated according 0-approximation semantics makes
action sequence applicable terminates belief state goals true.
possible prove that:
Proposition 3 (K0 (P ) 0-Approximation). action sequence classical plan
K0 (P ) iff conformant plan P according 0-approximation semantics.
correspondence surprising though 0-approximation semantics
K0 (P ) translation throw away disjunctive information restrict plans
make use uncertain knowledge. Indeed, states s0 , s1 , . . . generated
action sequence = {a0 , a1 , . . .} classical problem K0 (P ) encode precisely
627

fiPalacios & Geffner

literals known true according 0-approximation; namely, L true
time according 0-approximation iff literal KL true state si .
Proposition 3 mean basic translation K0 0-approximation
semantics equivalent rather rely equivalent belief representations.
translation K0 delivers also way get valid conformant plans using classical
planner. translation-based approach thus addresses representational
heuristic issues arise conformant planning.
illustration Proposition 3, given conformant problem P = {p, r}
actions b effects : p q, : r v, b : q v, plan = {a, b} valid
achieving goal G = {q, v} according K0 (P ) 0-approximation,
plan = {b} valid according either. time, initial situation
changed = {p q}, neither approach sanctions plan = {a} G = {q}, even
valid conformant plan. this, ability reason disjunctions needed.
extension basic translation K0 allows limited form disjunctive reasoning presented Palacios Geffner (2006). extension based introduction
new literals L/Xi used encoding conditionals Xi L. Below, basic translation
K0 extended different manner ensures tractability completeness
large class problems.

4. General Translation Scheme KT,M
basic translation K0 extended general translation scheme KT,M
two parameters: set tags set merges m. show
suitable choices two parameters, translation KT,M , unlike translation K0 ,
sound complete.
tag set (conjunction) literals L P whose truth value initial
situation known. tags used introduce new class literals KL/t
classical problem KT,M (P ) represent conditional true initially, L
true, assertion could written K(t0 L) temporal modal logic. use
notation KL/t rather L/t used Palacios Geffner (2006),
distinction KL/t KL/t: roughly KL/t means conditional
K(t0 L) true, KL/t means conditional K(t0 L) true.
Likewise, merge non-empty
W collection tags stands Disjunctive Normal Form (DNF) formula tm t. merge valid one tags
must true I; i.e.,
_
|=
.
tm

merge literal L P translate merge action single effect
^
KL/t KL
tm

captures simple form reasoning cases.
valid merge used reasoning literal L P , computationally
convenient (although logically necessary) specify certain merges
used literals L others. Thus, formally, collection pairs
628

fiCompiling Uncertainty Away Conformant Planning Problems

(m, L), merge L literal P . pair means merge
L. group merges literal L set ML , thus, understood
collection sets ML L P . simplicity, however, except
may cause confusion, keep referring plain set merges.
assume collection tags always includes tag stands
empty collection literals, call empty tag denote . empty
tag, denote KL/t simply KL.
translation KT,M (P ) basic translation K0 (P ) conditioned tags
extended actions capture merges :
Definition 4 (Translation KT,M ). Let P = hF, I, O, Gi conformant problem,
KT,M (P ) classical planning problem KT,M (P ) = hF 0 , 0 , O0 , G0
F 0 = {KL/t, KL/t | L F }
0 = {KL/t | I, |= L}
G0 = {KL | L G}
O0 = {a V
: KC/t KL/t, : KC/t KL/t | : C L P }
{am,L : [ tm KL/t] KL XL | L P, ML }
KL precondition action KT,M (P ) L precondition P , KC/t
KC/t stand KL1 /t, KL2 /t,
V . . . , KL1 /t, KL2 /t, . . . respectively,
C = L1 , L2 , . . ., XL stands L0 KL0 L0 ranging literals L0 mutex
L P .
translation KT,M (P ) reduces basic translation
K0 (P ) empty
V
contains empty tag. extra effects XL = L0 KL0 merge actions am,L
needed ensure translation KT,M (P ) consistent P consistent,
otherwise ignored. Indeed, L L0 mutex consistent P , invariant
KL/t KL0 /t holds KT,M (P ) non-empty tags t, hence successful merge
L always followed successful merge L0 . rest paper
thus assume P KT,M (P ) consistent, ignore extra merge effects,
come back Appendix B proving consistency KT,M (P )
consistency P .
suitable choices , translation KT,M (P ) sound complete.
establishing results, however, let us make notions precise.
Definition 5 (Soundness). translation KT,M (P ) sound classical plan
solves classical planning problem KT,M (P ), plan 0 results dropping
merge actions conformant plan P .
Definition 6 (Completeness). translation KT,M (P ) complete conformant
plan 0 solves conformant problem P , classical plan solves
classical problem KT,M (P ) 0 equal merge actions removed.
general translation scheme KT,M sound provided merges valid
tags consistent (literals tag true possible initial state):
629

fiPalacios & Geffner

Theorem 7 (Soundness KT,M (P )). translation KT,M (P ) sound provided
merges valid tags consistent.
Unless stated otherwise, assume merges valid tags consistent,
call translations, valid translations.
convention keeping notation simple, singleton tags like = {p}, curly
brackets often dropped. Thus, literals KL/t = {p} written KL/p,
merges = {t1 , t2 } singleton tags t1 = {p} t2 = {q}, written = {p, q}.
Example. illustration, consider problem moving object origin
destination using two actions: pick(l), picks object location hand
empty object location, drop(l), drops object location
object held. making problem interesting, let us also assume
action pick(l) drops object held l hand empty.
conditional effects action preconditions. Assuming single
object, effects written as:
pick(l) : hold, at(l) hold at(l)
pick(l) : hold hold at(l)
drop(l) : hold hold at(l) .
Consider instance P domain, hand initially empty
object, initially either l1 l2 , must moved l3 ; i.e., P = hF, I, O, Gi
= {hold , at(l1 ) at(l2 ) , at(l1 ) at(l2 ) , at(l3 )}

G = {at(l3 )} .
action sequence
1 = {pick(l1 ), drop(l3 ), pick(l2 ), drop(l3 )}
conformant plan problem, attempt pick object location
l1 followed drop target location l3 , ensuring object ends l3
originally l1 . followed attempt pick object l2
drop l3 .
hand, action sequence 2 results 1 removing first
drop action
2 = {pick(l1 ), pick(l2 ), drop(l3 )}
conformant plan, since object originally l1 , would end l2
action pick(l2 ). notation introduced above, 1 classical plan classical
problem P/s two possible initial states s, 2 classical plan problem
P/s state object initially l2 .
630

fiCompiling Uncertainty Away Conformant Planning Problems

Consider classical problem KT,M (P ) = hF 0 , 0 , O0 , G0 obtained P
= {at(l1 ), at(l2 )}2 contains merge = {at(l1 ), at(l2 )} literals
hold at(l3 ). definition, fluents F 0 KT,M (P ) form KL/t
KL/t L {at(l), hold}, l {l1 , l2 }, , initial situation 0
0 = {Khold, Khold/at(l), Kat(l3 ), Kat(l3 )/at(l), Kat(l)/at(l), Kat(l0 )/at(l)}
l, l0 {l1 , l2 } l0 6= l, goal G0
G0 = {Kat(l3 )} .
effects associated actions pick(l) drop(l) O0 support rules
pick(l) : Khold, Kat(l) Khold Kat(l)
pick(l) : Khold Khold Kat(l)
drop(l) : Khold Khold Kat(l)
one three locations l = li , condition rule empty tag,
along support rules:
pick(l) : Khold/at(l0 ), Kat(l)/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )
pick(l) : Khold/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )
drop(l) : Khold/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )
condition rule tags at(l0 ) , l0 {l1 , l2 }. corresponding
cancellation rules are:
pick(l) : Khold, Kat(l) Khold Kat(l)
pick(l) : Khold Khold Kat(l)
drop(l) : Khold Khold Kat(l)

pick(l) : Khold/at(l0 ), Kat(l)/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )
pick(l) : Khold/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )
drop(l) : Khold/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 ) .
addition, actions O0 include merge actions am,hold am,at(l3 ) follow
merge = {at(l1 ), at(l2 )} literals hold at(l3 ):
am,hold : Khold/at(l1 ), Khold/at(l2 ) Khold
am,at(l3 ) : Kat(l3 )/at(l1 ), Kat(l3 )/at(l2 ) Kat(l3 ) .
2. empty tag assumed every thus mentioned explicitly.

631

fiPalacios & Geffner

shown plan
10 = {pick(l1 ), drop(l3 ), pick(l2 ), drop(l3 ), am,at(l3 ) }
solves classical problem KT,M (P ) hence, Theorem 7, plan 1 obtained
10 dropping merge action, valid conformant plan P (shown above).
see literals KT,M (P ) evolve actions 10 executed:
0:
1:
2:
3:
4:
5:

Kat(l1 )/at(l1 ), Kat(l2 )/at(l2 )
Khold/at(l1 ), Kat(l2 )/at(l2 )
Kat(l3 )/at(l1 ), Kat(l2 )/at(l2 )
Kat(l3 )/at(l1 ), Khold/at(l2 )
Kat(l3 )/at(l1 ), Kat(l3 )/at(l2 )
Kat(l3 )

true
true
true
true
true
true

0






pick(l1 )
drop(l3 )
pick(l2 )
drop(l3 )
merge am,at(l3 ) .

also verify manner action sequence 20
20 = {pick(l1 ), pick(l2 ), am,hold , drop(l3 )}
classical plan KT,M (P ), reason atom Khold/at(l1 ) holds
first pick action second. due cancellation rule:
pick(l2 ) : Khold/at(l1 ) Khold/at(l1 ) Kat(l2 )/at(l1 )
expresses assumption at(l1 ) initial situation, hold at(l2 )
known true action pick(l2 ), assumption, hold
known true action.

5. Complete Translation: KS0
complete instance translation scheme KT,M obtained simple manner
setting tags possible initial states problem P merge
precondition goal literal L includes tags. call resulting
exhaustive translation KS0 :
Definition 8 (Translation KS0 ). conformant problem P , translation KS0 (P )
instance translation KT,M (P )
set union empty tag set S0 possible initial states P
(understood maximal sets literals consistent I),
set contain single merge = S0 precondition goal literal L
P.
translation KS0 valid hence sound, complete due correspondence
tags possible initial states:
Theorem 9 (Completeness KS0 ). conformant plan P ,
classical plan 0 KS0 (P ) result dropping merge actions 0 .

632

fiCompiling Uncertainty Away Conformant Planning Problems

#S0
Problem
adder-01
blocks-02
blocks-03
bomb-10-1
bomb-10-5
bomb-10-10
bomb-20-1
coins-08
coins-09
coins-10
coins-11
comm-08
comm-09
comm-10
corners-square-16
corners-square-24
corners-square-28
corners-square-116
corners-square-120
square-center-16
square-center-24
log-2-10-10
log-3-10-10
ring-5
ring-6
safe-50
safe-70
safe-100
sortnet-07
sortnet-08
sortnet-09
sortnet-10
uts-k-08
uts-k-10

18
18
231
1k
1k
1k
1M
1k
1k
1k
1M
512
1k
2k
4
4
4
4
4
256
576
1k
59k
1,2k
4,3k
50
70
100
256
512
1k
2k
16
20

KS0
time
len
> 2h
0,2
23
59,2
80
5,9
19
11,3
15
18,3
10
> 2.1GB
20,2
27
19,9
25
21,5
31
> 2.1GB
18,3
61
77,7
68
> 2.1GB
0,2
102
0,7
202
1,2
264
581,4 3652
> 2.1GB
13,1
102
> 2.1GB
183,5
85
> 2h
12,6
17
> 2.1GB
0,5
50
1,4
70
6
100
2,9
28
9,8
36
77,7
45
> 2.1GB
0,6
46
1,2
58

POND
time len
0,4
26
0,4
26
126,8 129
1
19
3
15
8
10
4139
39
2
28
5
26
5
28
> 2h
1
53
1
59
1
65
1131
67
> 2h
> 2h
> 2h
> 2h
1322
61
> 2h
> 2h
> 2h
6
20
33
27
9
50
41
70
> 2.1GB
480
25
> 2h
> 2h
> 2h
24
47
2219
67

CFF
time
len
> 2h
> 2h
> 2h
0
19
0
15
0
10
0
39
0
28
0
26
0,1
38
1
78
0
53
0
59
0
65
13,1
140
321
304
> 2h
> 2h
> 2h
> 2h
> 2h
1,6
83
4,7
108
4,3
31
93,6
48
29,4
50
109,9
70
1252,4 100
SNH
SNH
SNH
SNH
4,4
46
16,5
58

Table 1: KS0 translation fed FF planner compared POND Conformant FF
(CFF) along times reported plan lengths. #S0 stands number
initial states, SNH means goal syntax handled (by CFF). Times reported
seconds rounded closest decimal.

633

fiPalacios & Geffner

problems P whose actions preconditions, argument simple:
conformant plan P must classical plan P/s possible initial
state s, achieves (goal) literal Gi P/s s, must achieve
literal KGi /s KS0 (P ) well, followed merge action Gi ,
must achieve literal KGi . presence action preconditions, argument must
applied inductively plan length, idea remains (see proof
appendix details): correspondence established evolution
fluents L problem P/s evolution fluents KL/s problem
KS0 (P ).
significance exhaustive KS0 translation theoretical.
plenty conformant problems quite hard current planners even involve
handful possible initial states. example Square-Center-n task (Cimatti
et al., 2004), agent reach center empty square grid certainty,
knowing initial location. four actions move agent one unit
direction, except border grid, effects. standard
version problem, initial position fully unknown resulting n2 possible initial
states, yet problem remains difficult, actually beyond reach planners,
small values n, even uncertainty reduced pair possible initial states.
reason agent must locate heading goal. domain
Corners-Square-n Table 1 variation Square-Center-n possible initial
states four corners grid.
Table 1 shows results conformant planner based KS0 (P ) translation
uses FF (Hoffmann & Nebel, 2001) solving resulting classical problem, compares
two planners entered Conformant track 2006 Int. Planning
Competition (Bonet & Givan, 2006): POND (Bryce et al., 2006) Conformant FF
(Hoffmann & Brafman, 2006) (the two planners competition translationbased: T0 , based formulation developed paper, K(P ), based earlier
restricted formulation due Palacios & Geffner, 2006). Clearly, approach
based KS0 (P ) translation scale problems many possible initial
states, yet number states small, quite well.

6. Complete Translations May Compact
order complete translations polynomial, certain assumptions
formulas initial situation need made. Otherwise, checking whether
goal true intractable itself, therefore polynomial complete translation
would impossible (unless P = NP). thus assume prime implicate (PI)
form (Marquis, 2000), meaning includes inclusion-minimal clauses
entails tautologies. known checking whether clause follows logically
formula PI form reduces checking whether clause subsumed clause
tautology, hence polynomial operation. initial situations
benchmarks P form easily cast PI form normally specified
means set non-overlapping oneof (X1 , . . . , Xn ) expressions translate
clauses X1 Xn binary clauses Xi Xj 6= j resolvent
tautology.
634

fiCompiling Uncertainty Away Conformant Planning Problems

6.1 Conformant Relevance
translation KS0 (P ) complete introduces number literals KL/t exponential worst case: one possible initial state s0 . raises question:
possible complete translations exhaustive sense? answer
yes section provide simple condition ensures translation
KT,M (P ) complete. makes use notion relevance:3
Definition 10 (Relevance). conformant relevance relation L L0 P , read L
relevant L0 , defined inductively
1. L L
2. L L0 : C L0 P L C action P
3. L L0 L L00 L00 L0
4. L L0 L L00 L00 L0 .
first clause stands reflexivity, third transitivity, second captures conditions relevant effect, fourth, conditions L preempts
conditional effects may delete L0 . replace 4
4 L L0 L L0
equivalent 4 context 13, resulting definition one Son
Tu (2006), notion relevance used generate limited set possible partial
initial states 0-approximation complete (see Section 11 discussion
relation tags partial initial states).
Notice according definition, precondition p action taken
relevant effect q. reason want relation L L0 capture
conditions uncertainty L relevant uncertainty L0 .
say relation conformant relevance. Preconditions must known
true order action applied, introduce propagate uncertainty
effects action.
let CI stand set clauses representing uncertainty initial situation, namely, non-unit clauses along tautologies L L complementary
literals L L appearing unit clauses I, notion (conformant) relevance
extended clauses follows:
Definition 11 (Relevant Clauses). clause c CI relevant literal L P
literals L0 c relevant L. set clauses CI relevant L denoted CI (L).
representation uncertainty initial situation relevant
literal L, possible analyze completeness translation KT,M terms
relation merges literals L, one hand, sets clauses CI (L)
relevant L other.
3. follow earlier account (Palacios & Geffner, 2007), many definitions theorems
differ number details (for example, notion relevance depends rules P
clauses initial situation). changes aimed making resulting formulation simpler
cleaner.

635

fiPalacios & Geffner

6.2 Covering Translations
may appear translation KT,M would complete
W merges precondition goal literals L, understood DNF formulas tm t, contain much
information, thus equivalent CNF formula CI (L) captures fragment
initial situation relevant L. intuition partially correct, misses
one important point; namely every DNF formula equivalent CI (L) do:
DNF representation captured merges must vivid enough. example, CI (L)
single clause x x, completeness requires tag x, tag x, merge
= {x, x} L containing two tags, even clause x x tautology
thus equivalent DNF formula true.
defining types tags merges required completeness then,
let us first define closure set literals S, relative conformant problem
P = hF, I, O, Gi, set literals follow I:
= {L | I, |= L} .
Let us also say consistent contain pair complementary literals.
type merges required precondition goal literals L
imply CI (L) satisfy well. notion satisfaction associates
consistent set literals partial truth assignment implicit closure
S, extended account conditions DNF formula (e.g.,
merge L) satisfies CNF formula (e.g., CI (L)).
Definition 12 (Satisfaction).
1. consistent set literals satisfies clause L1 L2

Lm contains one literals Li , = 1, . . . , m.
2. consistent set literals satisfies collection clauses C satisfies clause
C.
3. collection consistent sets literals satisfies collection clauses C set
satisfies C.
type merges required completeness simply valid merges
satisfy set clauses CI (L). call covering merges:
Definition 13 (Covering Merges). valid merge translation KT,M (P ) covers
literal L satisfies CI (L).
example, CI (L) given clauses result oneof (x1 , . . . , xn ) expression, i.e. x1 x2 xn xi xj j, 1 i, j n, 6= j,
merge = {x1 , . . . , xn } covers literal L, xi includes xi also xj
j 6= i, thus xi satisfies CI (L).
W
merge = {t1 , . . . , tn }, denote DNF formula ti ti ,
tag ti replaced closure ti , simple prove covers literal L,
entails CI (L). merge covers L thus DNF formula strong enough
imply CNF formula CI (L) (through closure), weak enough entailed I,
vivid enough satisfy CI (L).
636

fiCompiling Uncertainty Away Conformant Planning Problems

illustration, CI (L) given tautologies p p q q,
= CI (L), merge m1 = {p, p} implies CI (L) satisfy CI (L). Likewise,
merge m2 = {{p, q}, {p, q}} satisfies CI (L) entailed I. Finally, merge
m3 = {{p, q}, {p, q}, {p, q}, {p, q}} satisfies CI (L) entailed I, thus
valid merge covers L.
valid translation KT,M (P ) contains merge covers L precondition
goal literal L P , say translation covers P covering
translation:
Definition 14 (Covering Translation). covering translation valid translation
KT,M (P ) includes one merge covers L, precondition goal literal L
P .
central result paper covering translations complete:
Theorem 15 (Completeness). Covering translations KT,M (P ) complete; i.e.,
conformant plan P , classical plan 0 KT,M (P ) 0
merge actions removed.
words, complete translations KT,M (P ) result tags merges
capture information initial situation relevant precondition
goal literal suitable manner.
Theorem 15 used two ways: proving completeness translation,
checking covering condition holds, constructing complete translations,
enforcing covering condition. addition, interest paper conformant planning optimality guarantees, theorem useful optimal conformant
planning well, whether cost plans defined length (action costs equal
1) sum non-uniform action costs. cases, theorem ensures
problem optimal conformant planning gets mapped problem optimal classical
planning provided cost merge actions KT,M (P ) made sufficiently small.
illustration Theorem 15, consider conformant problem P initial situation = {x1 xm }, goal G = L, actions ai , = 1, . . . , m, effect xi L.
number possible initial states problem exponential m, disjunction
among xi exclusive. So, translation KS0 (P ) complete exponential
size. hand, consider translation KT,M (P ) = {x1 , . . . , xm }
contains single valid merge = {x1 , . . . , xm } L. simple verify
merge covers goal L (satisfies CI (L) = I), hence translation KT,M (P )
covering, Theorem 15, complete, polynomial m.
Notice testing whether valid translation KT,M (P ) covering translation
done polynomial time, particular, computing set literals every tag
tractable operation provided PI form; indeed, I, |= L0 iff |= L0
iff L0 tautology subsumed clause I.
6.3 Translation Kmodels
straightforward show exponential translation KS0 considered Section 3,
(non-empty) tags stand possible initial states, covering hence complete
637

fiPalacios & Geffner

according Theorem 15. possible, however, take advantage Theorem 15
devising complete translation usually compact. call Kmodels.
Definition 16. translation Kmodels(P ) obtained general scheme KT,M (P )
defining
contain one merge precondition goal literal L given models
CI (L) consistent I,4
contain tags merges along empty tag.
translation Kmodels equivalent KS0 precondition goal
literals L, CI (L) = I; i.e., clauses relevant L. Yet, cases,
first translation exponential number variables appearing one CI (L)
set (the one largest number variables), second exponential
number unknown variables I. example, n precondition goal
literals Li , = 1, . . . , n P one, CI (Li ) unique oneof (xi1 , . . . , xim )
expression, merge literal Li KS0 (P ) contain mn models n one-of
expressions I, merge Li Kmodels(P ) contain models
single oneof (xi1 , . . . , xim ) expression CI (Li ). translation Kmodels thus
exponentially compact exhaustive KS0 translation remaining sound
complete:
Theorem 17. translation Kmodels(P ) sound complete.
worst case, however, Kmodels also exponential translation. thus consider
next polynomial translations conditions complete.
6.4 Conformant Width
address conditions compact, covering translation constructed polynomial time. this, define structural parameter call
conformant width problem P , analogy notion width used graphical
models (Dechter, 2003), provide upper bound time space complexity
required generating covering translation. precisely, complexity construction exponential conformant width problem P cannot exceed
number fluents P much lower.
principle, would like define width w(P ) maximum tag size required
translation KT,M (P ) covering translation. definition, however, would
give us complexity bounds want, checking validity merge
tags bounded size intractable operation, whether initial situation
prime implicate form not.5 need define width different way. First, let
cover set clauses defined follows:
4. models CI (L) understood conjuntions literals.
5. problem checking whether entails DNF formula whose terms may 2 literals
coNP-hard even equivalent true. Indeed, 3-CNF formula; contradictory iff
negation (which 3-DNF) valid, turn true iff implied I. Actually,
general prime implicate form, problem remains coNP-hard even terms DNF formula
contain 2 literals. thank Pierre Marquis pointing results us.

638

fiCompiling Uncertainty Away Conformant Planning Problems

Definition 18 (Cover). cover c(C) set clauses C, relative conformant
problem P initial situation I, collection minimal sets literals consistent
contains literal clause C.
Two important properties cover c(C) set clauses C c(C) stands
DNF formula logically equivalent CNF formula C given I, c(C)
computed polynomial time size C bounded constant. Moreover, c(C)
implies C satisfies C well. Thus particular, C collection clauses
CI (L) relevant literal L, cover c(CI (L)) CI (L) valid merge
covers L. completeness covering translations, follows complete
translation KT,M (P ) constructed polynomial time size |CI (L)| sets
clauses CI (L) precondition goal literals L P bounded. Unfortunately,
condition rarely seems hold, yet weaker sufficient condition does: namely,
often possible find subset C clauses either CI (L) tautologies
c(C) satisfies CI (L) thus covers literal L. thus define width
literal L size smallest set (cardinality-wise). this, denote
CI (L) set clauses CI (L) extended tautologies form p p fluents p
either p p appears CI (L) (if appear CI (L) p p CI (L)
definition).
Definition 19 (Width Literal). conformant width literal L P , written w(L),
size smallest (cardinality-wise) set clauses C CI (L) c(C) satisfies
CI (L).
consequence definition width literal must lie interval
0 w(L) n, n number fluents P whose status initial situation
known. Indeed, CI (L) empty, w(L) = 0, set clauses CI (L),
cover c(C) set C tautologies CI (L) must satisfy CI (L), thus w(L) |C| n.
Similarly, CI (L) contains single clause x1 xm clauses x1 xm
xi xj correspond oneof (x1 , . . . , xm ) expression, simple prove
w(L) = 1 singleton C = {x1 xm } generating cover c(C) = {{x1 }, . . . , {xn }}
satisfies CI (L). Finally, CI (L) contains two tautologies pp qq, w(L) = 2
smallest C CI (L) whose cover satisfies CI (L) CI (L) itself.
width problem width precondition goal literal maximum
width:
Definition 20 (Width Problem). conformant width problem P , written
w(P ), w(P ) = maxL w(L), L ranges precondition goal literals P .
show problems bounded width, complete translations
constructed polynomial time, moreover, almost existing conformant benchmarks bounded width, precisely, width equal 1. case,
resulting translations use tags never greater size w(P ),
problems width 1, tags single literals.
Like (tree)width graphical models, computing width problem P
exponential w(P ), recognition problems small width carried
quite efficiently:
639

fiPalacios & Geffner

Proposition 21 (Determining Width). width w(P ) P determined time
exponential w(P ).
particular, test w(P ) = 1 considering one one sets C
includes single clause CI (L), verifying whether c(C) satisfies CI (L) not.
w(P ) 6 1, verification must carried setting C set
clauses CI (L) increasing values i. fixed value i, polynomial
number clause sets C verification one done polynomial
time. Moreover, arguments regarding w(L), w(P ) never exceed
number unknown fluents problem:
Proposition 22 (Bounds Width). width P 0 w(P ) n, n
number fluents whose value initial situation known.
6.5 Polynomial Translation Ki
translation Ki , parameter non-negative integer, instance
general KT,M scheme designed sound, polynomial fixed i, complete
problems width w(P ) i. Thus, example, translation K1 sound, polynomial,
complete problems width 1.
Definition 23 (Translation Ki ). translation Ki (P ) obtained general
scheme KT,M (P )
set contain one merge = c(C) precondition goal literal L P
set C clauses CI (L) covers L. set
exists, one merge = c(C) L created set C clauses CI (L),
merges created L CI (L) empty;
collection tags appearing merges empty tag.
translation Ki (P ) applies problems P width, remaining cases exponential polynomial number fluents, actions, clauses P . addition,
translation Ki (P ) sound, problems width bounded i, complete.
Theorem 24 (Properties Ki ). fixed i, translation Ki (P ) sound, polynomial,
w(P ) i, covering complete.
Soundness result merges valid construction, covers c(C)
C CI (L) entailed C hence I. complexity polynomial fixed
i, polynomial number clause sets C size CI (L), constructing
cover c(C) one them, polynomial operation. Finally, completeness follows
definition width: w(P ) i, set clauses C CI (L)
size |C| greater whose cover satisfies CI (L), thus Ki (P ) must contain
merge = c(C) L covers L.
Notice = 0, translation Ki (P ) reduces basic K0 (P ) translation
introduced Section 3 tags (other empty tag) merges. Before,
assessed completeness translation terms 0-approximation semantics.
Theorem 24 provides alternative interpretation: translation K0 (P ) complete
640

fiCompiling Uncertainty Away Conformant Planning Problems

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Domain-Parameter
Safe-n combinations
UTS-n locs
Ring-n rooms
Bomb-in-the-toilet-n bombs
Comm-n signals
Square-Center-n n grid
Cube-Center-n n n cube
Grid-n shapes n keys
Logistics n pack locs
Coins-n coins locs
Block-Tower-n Blocks
Sortnet-n bits
Adder n pairs bits
Look-and-Grab objs n n locs
1-dispose objs n n locs

# Unknown Fluents
n
n
4n
n
n
2n
3n
nm
nm
nm
n (n 1) + 3n + 1
n
2n
nnm
nnm

Width
1
1
1
1
1
1
1
1
1
1
n (n 1) + 3n + 1
n
2n



Table 2: Width parameterized domains
problems P zero width. problems set clauses CI (L)
relevant precondition goal literal L empty. makes precise intuition
mentioned K0 (P ) translation complete problems uncertain
information relevant. cases, none clauses initial situation
make sets relevant clauses CI (L) preconditions goal literals L.
illustration Theorem 24, consider conformant problem P initial
situation = {x1 xm }, goal G = {L}, actions ai , = 1, . . . , m,
effect xi L. problem, singleton set clauses C = CI (L) =
c(C) = {{x1 }, . . . , {xm }} covers CI (L). Then, since precondition goal
literal, K1 (P ) includes single merge = c(C) L singleton tags ti = {xi },
write simply = {x1 , . . . , xm }. translation K1 (P ) polynomial m,
since w(P ) = 1, Theorem 24 complete. Notice example,
translations KS0 (P ) Kmodels(P ) identical exponential (the number
models CI (L)).
6.6 Width Conformant Benchmarks
practical value notion width becomes apparent width existing
benchmarks considered. Table 2 summarizes width many existing benchmark
domains conformant planning. domains depend certain parameters n
capture size instances (e.g., size grid, number objects, etc).6 domain
bounded width width grow size instances,
width equal instances width regardless parameter values.
seen table, width existing benchmarks 1.
cases, means sets CI (L) clauses relevant precondition
6. names parameterized domains table coincide names instances
currently used. E.g. Comm-n IPC5 refers Communication instance necessarily
instance n signals.

641

fiPalacios & Geffner

goal literal L contain single clause (often tautology p p disjunction x1 . . . xm )
single oneof (x1 , . . . , xm ) expression (that translates disjunction x1 xm
clauses xi xk ). shown above, w(L), therefore, w(P ), equal 1 theses
cases.
extreme domains Blocks, Sortnet, Adder,
maximal widths; i.e., widths equivalent number fluents whose status initial situation known. fluents interact
action conditions (not preconditions). numbers Blocks Table 2, thus follow
number fluents involved; namely, fluents on(x, y), clear(x), ontable(x),
holding(x).
Finally, domains 1-dispose Look-and-Grab (Palacios & Geffner, 2006, 2007)
objects unknown locations grid n n must collected robot
whose gripper hold one object time, width equal m, meaning
width domains grows number objects size grid.
case, clauses possible locations objects
relevant condition hand empty pick actions.
Let us point completeness translation Ki (P ) problems P
width w(P ) bounded i, establishes correspondence conformant plans
P classical plans KT,M (P ). solving P , however, correspondence
needed; suffices Ki (P ) solvable; plan Ki (P ) encode
conformant plan P , even Ki (P ) capture conformant plans P .
perspective, makes sense refer smallest value parameter
classical problem Ki (P ) solvable, effective width P , denoted (P ). turns
(P ) cannot larger w(P ), may much smaller. interesting
example comes Sortnet-n domain (Bonet & Geffner, 2000). Sortnet-n
considered challenging domain conformant planning planners able
scale even small values n (the number entries sorted sorting network).
domain width n, compact encoding used IPC5, input vector
represented set bits, exploiting fact sorting vectors numbers reduces
sorting vector bits (0s 1s). domain cannot solved K1 translation
FF reports correctly unsolvable brief unsuccessful search. hand,
possible reformulate domain, replacing unary high(i) low(i) predicates
binary predicates less(i, j) compare two vector entries. call reformulation
Sort-2-n. encoding Sort-n linear n, encoding Sort-2-n quadratic n,
cases, problem width maximum, given number fluents whose
status initial situation unknown. Yet, compact Sort-n encoding
solvable K1 translation, K1 suffices solve problem expanded Sort2-n encoding actually also solved K0 . Thus effective width Sort-2-n
0. Interestingly, provided K0 translation Sort-2-n, instances solved
20 entries. hand, conformant planners Conformant-FF POND
solve Sort-2-n instances n greater 3.
642

fiCompiling Uncertainty Away Conformant Planning Problems

7. Tags Initial States
deeper understanding results obtained relating tags possible
initial states. looking closely relation context covering translations,
able answer question polynomial number contexts (tags)
play role exponential number possible initial states problems bounded
width.
this, let us first recall notation introduced Section 2.2, state s,
wrote I(s) refer set atoms encoding (i.e, p I(s) iff p true s) P/s
refer classical planning problem P/s = hF, I(s), O, Gi like conformant
problem P = hF, I, O, Gi initial state fixed s.
Let us extend notation say action sequence conforms set
states given conformant problem P iff plan classical problem P/s
S. Clearly, conformant plan P nothing else action sequence
conforms set S0 possible initial states P , yet notion conforms allows
us abstract away initial situation make precise notion basis:
Definition 25 (Basis P ). set states 0 basis conformant problem P =
hF, I, O, Gi 0 subset set S0 possible initial states P every plan
conforms 0 conforms set possible initial states S0 .
words, 0 basis P , necessary consider states S0
computing conformant plans P ; suffices consider states 0 . aim
show width P bounded, P polynomial basis 0 even S0
exponential size. Moreover, states basis close correspondence
tags appearing covering translation.
illustration, consider problem P actions ai , = 1, . . . , n, effects
ai : xi L. Let G = {L} goal = {x1 xn } initial situation.
set S0 possible initial states truth valuations xi atoms least
one atoms true. 2n 1 states. hand, one show
set S00 n valuations exactly one atoms true provides basis
P ; i.e., plans conform n possible initial states, exactly plans
conform complete set 2n 1 possible initial states S0 .
reduction number possible initial states must considered computing conformant plans results two monotonicity properties formulate using
notation rel(s, L) refer set literals L0 true state
relevant literal L:
rel(s, L) = {L0 | L0 L0 relevant L} .
Proposition 26 (Monotonicity 1). Let s0 two states let action sequence
applicable classical problems P/s P/s0 . achieves literal L P/s0
rel(s0 , L) rel(s, L), achieves literal L P/s.
Proposition 27 (Monotonicity 2). 0 two collections states
every state every precondition goal literal L P , state s0 0
rel(s0 , L) rel(s, L), plan P conforms 0 , plan
P conforms S.
643

fiPalacios & Geffner

properties, follows
Proposition 28. 0 basis P every possible initial state P every
precondition goal literal L P , 0 contains state s0 rel(s0 , L) rel(s, L).
proposition allows us verify claim made example set S00 ,
contains number states linear n, basis P exponential
number possible initial states. Indeed, problem precondition single
goal literal L, every state makes one atom xi true (these
literals relevant L), state s0 S00 makes one atoms true,
hence relation rel(s0 , L) rel(s, L) holds.
question address build basis complies condition
Proposition 28 given covering translation KT,M (P ). this, let = {t1 , . . . , tn }
merge covers precondition goal literal L, let S[ti , L] denote set
possible initial states P rel(s, L) ti ; i.e., S[ti , L] contains possible initial
states P make literals L0 relevant L false, except
closure ti ti . show first prime implicate form, S[ti , L] non-empty
set:7
Proposition 29. initial situation prime implicate form = {t1 , . . . , tn }
valid merge covers literal L P , set S[ti , L] possible initial states
P rel(s, L) ti non-empty.
Let s[ti , L] stand arbitrary state S[ti , L]. obtain following result:
Theorem 30. Let KT,M (P ) covering translation problem P initial
situation PI form, let 0 stand collection states s[ti , L] L
precondition goal literal P ti tag merge covers L. 0 basis
P .
important result three reasons. First, tells us build basis P
given tags ti covering translation KT,M (P ). Second, tells us size
resulting basis linear number precondition goal literals L tags ti .
third, makes role tags ti covering translation KT,M (P ) explicit, providing
intuition works: tag ti merge covers literal L represents one
possible initial state; namely, state s[ti , L] makes false literals L0
relevant L except ti . plan conforms critical states,
conform possible initial states monotonicity (Proposition 27). follows
particular that:
Theorem 31. P conformant planning problem bounded width, P admits
basis polynomial size.
Namely, conformant problems P width bounded non-negative integer admit
polynomial translations complete, plans conform possibly
exponential number initial states P correspond plans conform
7. Recall assuming throughout initial situation logically consistent
tags consistent I.

644

fiCompiling Uncertainty Away Conformant Planning Problems

subset critical initial states polynomial number (namely,
polynomial basis). Thus, one complete polynomial translation problems
Ki translation; another one, KS0 translation tags associated
critical initial states rather initial states.
illustration, problem P actions ai effects ai : xi L,
goal G = {L}, initial situation = {x1 xn }, K1 (P ) translation tags xi ,
= 1, . . . , n, merge = {x1 , . . . , xn } goal literal L, covering translation.
Theorem 30 states basis 0 P results collection states si
make tag xi true, literals relevant L xi false (i.e.,
xk atoms k 6= i). precisely basis P includes
states make single atom xi true = 1, . . . , n: plans conform
basis exactly plans conform whole collection possible initial
states P . basis size polynomial though, number
possible initial states P exponential m.

8. Planner T0
current version conformant planner T0 based two instances general
translation scheme KT,M (P ) whose outputs fed classical planner FF v2.3.8 One
instance polynomial necessarily complete; complete necessarily
polynomial. incomplete translation, T0 uses K1 complete problems
width greater 1, argued above, result solvable instances problems
larger widths. complete translation, Kmodels translation used instead
simple optimization: K1 translation produces single merge covers L,
merge used L instead potentially complex one determined
Kmodels. mere optimization resulting translation remains complete.
merges Kmodels, result models set clauses CI (L)
consistent I, computed using SAT solver relsat v2.20 (Bayardo Jr. & Schrag,
1997). current default mode T0 , one used experiments below,
two translations K1 Kmodels used sequence: FF called first upon
output K1 fails, called upon output Kmodels. experiments
below, indicate cases Kmodels invoked.
translations used T0 accommodate certain simplifications two additional
actions capture types deductions. simplifications fact
translations considered uniform sense literals L P
rules C L conditioned tags . practical point
view, however, needed. simplifications address source inefficiency.
particular:
literals KL/t created closure contains literal relevant L.
case, invariance KL/t KL holds, thus, every occurrence
literal KL/t KT,M (P ) replaced KL.
8. conformant planner T0 along benchmarks considered paper available
http://www.ldc.usb.ve/hlp/software.

645

fiPalacios & Geffner

support rules : KC/t KL/t non-empty tags created L
relevant literal L0 merge contains t, case, literal
KL/t cannot contribute establish precondition goal. Similarly, cancellation
rules : KC/t KL/t non-empty tags created L
relevant literal L0 merge contains t.
support cancellation rules : KC/t KL/t : KC/t KL/t
grouped : KC/t KL/t KL/t every fluent L0 relevant L, either
L0 L0 entailed t. case, incomplete information
L given initial situation, thus invariant KL/t KL/t holds,
KC/t equivalent KC/t.
Two types sound deductive rules included translations:
rule : KC KL added : C, L L rule P action a,
rule P form : C 0 L,
rules KL1 , . . . , KLi1 , KLi+1 , . . . , KLn KLi = 1, . . . , n added
new unique action precondition, L1 Ln static clause P (a
clause P static true initial situation provably true action).
rules versions action compilation static disjunctions rules (Palacios &
Geffner, 2006, 2007), appear help certain domains without hurting others.
version T0 reported assume initial situation P
prime implicate form rather renders PI form running version Tisons
algorithm (1967), computation none benchmarks solved took 48
seconds.
translators T0 written OCaml code parsing PDDL files
written C++.

9. Experimental Results
considered instances three sources: Conformant-FF distribution, conformant track 2006 International Planning Competition (IPC5), relevant publications (Palacios & Geffner, 2006, 2007; Cimatti et al., 2004). instances run
cluster Linux boxes 2.33 GHz 8GB. experiment cutoff 2h 2.1GB
memory. Times T0 include steps, particular, computation prime implicates, translation, search (done FF). also include results Conformant
Track recent 2008 International Planning Competition (IPC6).
Goals sets literals sets clauses transformed T0 standard
way: goal clause C : L1 Lm modeled new goal atom GC , new
action executed added rules Li GC , = 1, . . . , m.9
9. alternative way represent CNF goals converting DNF first
action End map non-mutex terms dummy goal LG . alternative encoding pays
cases, Adder-01 instance get solved default CNF goal
encoding (see below).

646

fiCompiling Uncertainty Away Conformant Planning Problems

Problem
bomb-100-100
square-center-96
sortnet-09
blocks-03
dispose-16-1
look-and-grab-8-1-1
sgripper-30

P
#Acts #Atoms #Effects
10100
404
40200
4
196
760
46
68
109
32
30
152
1217
1479
2434
352
358
2220
487
239
1456

Time
2
35,1
8,3
4
163,6
6,9
21,5

K1 (P )
PDDL
#Acts #Atoms #Effects Size
10201
1595
50500
2,9
7
37248
75054
3,8
56
29707
154913
5,1
37
11370
35232
0,7
1218 133122
3458
0,3
353
8708
118497
7,8
860
1127
12769
1

Table 3: Translation data selected instances: #Acts, #Atoms, #Effects stand
number actions, fluents, conditional effects. Time translation
time seconds rounded closest decimal, PDDL Size size
PDDL file Megabytes.

Table 3 shows data concerning translation group selected instances.
seen, number conditional effects grows considerably cases, sometimes
translation may take several seconds.
Tables 4, 5, 6, 7, 8, show plan times lengths obtained number
benchmarks T0 , POND 2.2 (Bryce et al., 2006), Conformant FF (Hoffmann & Brafman,
2006), MBP (Cimatti et al., 2004) KACMBP (Bertoli & Cimatti, 2002). last
two planners accept problems standard syntax (based PDDL),
limited number experiments performed them. general picture T0
scales well domains, exceptions Square-Center Cube-Center
Table 5, KACMBP scales better, Sortnet Table 6, KACMBP MBP
scale better; Adder Table 6, POND planner able solve one
instance.
problems Table 4 encodings Conformant-FF repository: Bomb-x-y
refers Bomb-in-the-toilet problem x packages, toilets, clogging; Logistics-ij-k variation classical version uncertainty initial location packages;
Ring-n closing locking windows ring n rooms without knowing
current room; Safe-n opening safe n possible combinations.
problems width 1. T0 clearly best last two domains, first two
domains, Conformant-FF well too.
Table 5 reports experiments four grid domains: Cube-Center-n refers problem
reaching center cube size n3 completely unknown location; SquareCenter-n similar involves square n2 possible locations; Corners-Cube-n
Corners-Square-n variations problems set possible initial locations
restricted Cube Square corners respectively. MBP KACMBP appear
effective domains, although KACMBP doesnt scale well corner versions.
T0 solves problems, corner versions, quality plans poor.
problems also width 1.
Table 6 reports experiments problems 2006 International Planning Competition (Bonet & Givan, 2006). domains Coins, Comm UTS width 1.
others max width given number unknown fluents initial situation.
647

fiPalacios & Geffner

Problem
bomb-20-1
bomb-20-5
bomb-20-10
bomb-20-20
bomb-100-1
bomb-100-5
bomb-100-10
bomb-100-60
bomb-100-100
logistics-4-3-3
logistics-2-10-10
logistics-3-10-10
logistics-4-10-10
ring-4
ring-5
ring-6
ring-7
ring-8
ring-30
safe-10
safe-30
safe-50
safe-70
safe-100

T0
time
0,1
0,1
0,1
0,1
0,5
0,7
1,1
4,25
9,4
0,1
1
1,5
2,5
0,1
0,1
0,1
0,1
0,1
13,4
0,1
0,1
0,4
1,12
2,5

len
49
35
30
20
199
195
190
140
100
35
84
108
125
13
17
20
30
39
121
10
30
50
70
100

POND
time len
4139 39
> 2h
> 2h
> 2h





56
40
> 2h
> 2h
> 2h
1
18
6
20
33
27
444
33
> 2h

0
10
2
30
9
50
41
70
> 2.1GB

CFF
time
len
0
39
0
35
0
30
0
20
56,7
199
52,9
195
46,8
190
9,4
140
1
100
0
37
1,6
83
4,7
108
4,4
121
0,4
18
4,3
31
93,6
48
837
71
> 2h

0
10
1,4
30
29,4
50
109,9
70
1252,4 100

MBP
time len
> 2h
> 2h
> 2h
> 2h





> 2h
> 2h
> 2h
> 2h
0
11
0,1
14
0,6
17
3,8
20
40
23
> 2h
0,1
10
> 2h
> 2h
> 2h
> 2h

KACMBP
time len
0
40
0,2
40
0,5
40
2
40
1,9
200
4,3
200
16,4 200
> 2h
> 2h
> 2.1GB
> 2.1GB
> 2.1GB
> 2.1GB
0
26
0,1
58
0,2
99
0,5
204
2
432
> 2.1GB
0
10
0,2
30
0,7
50
2,4
70
8,6
100

Table 4: Experiments well known benchmarks. Times reported seconds rounded
closest decimal. means time memory smaller instances.

648

fiCompiling Uncertainty Away Conformant Planning Problems

Problem
square-center-8
square-center-12
square-center-16
square-center-24
square-center-92
square-center-96
square-center-100
square-center-120
cube-center-5
cube-center-7
cube-center-9
cube-center-11
cube-center-15
cube-center-19
cube-center-63
cube-center-67
cube-center-87
cube-center-91
cube-center-119
corners-square-12
corners-square-16
corners-square-20
corners-square-24
corners-square-28
corners-square-36
corners-square-40
corners-square-72
corners-square-76
corners-square-80
corners-square-120
corners-cube-15
corners-cube-16
corners-cube-19
corners-cube-20
corners-cube-23
corners-cube-24
corners-cube-27
corners-cube-52
corners-cube-55

T0
time
len
0,2
21
0,2
33
0,3
44
0,8
69
45,3
273
50,2
285
> 2.1GB
> 2.1GB
0,1
18
0,1
27
0,2
33
0,3
45
0,5
63
0,8
81
28,5
279
41,6
297
137,5 387
> 2.1GB
> 2.1GB
0,1
64
0,2
102
0,3
148
0,5
202
0,7
264
1,7
412
2,5
498
26,1 1474
30,5 1632
38,2 1798
223,6 3898
0,8
147
0,9
174
2,5
225
2,7
258
6,3
319
6,7
358
14,6
429
448
1506
> 2.1GB

POND
time len
2
41
12
52
1322 61
> 2h
> 2h



1
22
2
43
3
47
29
87
880 109
> 2h
> 2h




11
44
1131 67
> 2h
> 2h







907 105
3168 115
> 2h
> 2h






CFF
time
len
70,6
50
> 2h
> 2h





8,2
45
> 2h
> 2h








1,7
82
13,1
140
73,7
214
321
304
MPL






134,5 284
439,4 214
868,4 456
3975,6 332
MPL





MBP
time
len
0
24
0
36
0
48
0
72
0,9
276
0,9
288
1,1
300
1,9
360
0
28
0
33
0,1
54
0,2
59
0,2
69
1,6
111
28
285
> 2.1GB
> 2.1GB


0
36
0
48
0,3
60
0,6
72
1,1
84
1,5
108
7,8
120
118,8 216
371
228
649,6 240
> 2.1GB
3,7
69
12,5
72
549,5 111
1061,9 90
> 2h
> 2h




KACMBP
time
len
0
28
0
42
0
56
0
84
0,3
322
0,3
336
0,3
350
0,4
420
0
25
0
35
0
45
0
55
0
75
0,1
95
0,5
315
0,7
335
1,2
435
1,2
455
2,1
595
0,2
106
0,6
158
3
268
7,5
346
20,7
502
3308,8 808
> 2h
> 2h



174,1 391
270,5 316
1503,1 488
2759
625
6265,9 899
> 2h
> 2h



Table 5: Experiments grid problems. Times reported seconds rounded
closest decimal. MPL CFF means plan exceeds maximal plan length
(500 actions). means time memory smaller instances.

649

fiPalacios & Geffner

Problem
adder-01
adder-02
blocks-01
blocks-02
blocks-03
coins-10
coins-12
coins-15
coins-16
coins-17
coins-18
coins-19
coins-20
coins-21
comm-07
comm-08
comm-09
comm-10
comm-15
comm-16
comm-20
comm-25
sortnet-06
sortnet-07
sortnet-08
sortnet-09
sortnet-10
sortnet-11
uts-k-04
uts-k-05
uts-k-06
uts-k-07
uts-k-08
uts-k-09
uts-k-10
uts-l-07
uts-l-08
uts-l-09
uts-l-10

T0
time len
> 2h
> 2h
0,1
5
0,3
23
82,6
80
0,1
26
0,1
67
0,1
79
0,3
113
0,2
96
0,2
97
0,2
105
0,2
107
> 2h
0,1
54
0,1
61
0,1
68
0,1
75
0,1
110
0,2
138
0,8
278
2,3
453
0,6
21
2,5
28
9,6
36
76,8
45
> 2.1GB
> 2.1GB
0,1
23
0,1
29
0,2
35
0,4
41
0,6
47
0,9
53
1,3
59
0,2
70
0,3
80
0,6
93
0,7
97

POND
time len
1591
5
> 2h
0,1
4
0,4
26
126,8 129
5
28
> 2h
> 2h






0
47
1
53
1
59
1
65
6
95
> 2h
> 2.1GB

18
20
480
25
> 2h
> 2h


2
22
4
28
10
34
13
40
24
47
> 2h
2219
67
201
58
937
67
> 2h
> 2h

CFF
time len
SNH
SNH
0
6
> 2h
> 2h
0,1
38
0,8
72
3
89
33,3 145
1,4
94
6,2
118
16,5 128
20,6 143
> 2h
0
47
0
53
0
59
0
65
0,2
95
0,4
119
6,4
239
56,1 389
SNH
SNH
SNH
SNH
SNH
SNH
0,1
22
0,3
28
0,8
34
1,9
40
4,4
46
8,6
52
16,5
58
0,2
41
0,4
47
0,8
53
1,6
59

MBP
time
len
NR
NR
NR
NR
NR
> 2h
> 2h







0,2
55
0,2
71
0,2
77
0,3
85
0,9
115
1,6
151
50,9
340
> 2h
0
17
0
20
0
28
0
36
0,1
37
0,1
47
5,4
32
1247,3 38
1704,8 50
> 2h
> 2h


10,5
89
41,1
106
1176
137
> 2h

KACMBP
time
len
NR
NR
NR
NR
NR
4,2
106
3654,7 674
> 2h
> 2h





63,6
53
1966,8 53
> 2h
> 2h




0
21
0
28
0
36
0
45
0,1
55
0,1
66
1,5
30
195,4
42
> 2h
> 2h



> 2h
> 2h



Table 6: Experiments problems IPC5. Times reported seconds rounded
closest decimal. SNH CFF means goal syntax handled,
NR MBP KACMBP planners run due lack
translations PDDL. means time memory smaller instances.

650

fiCompiling Uncertainty Away Conformant Planning Problems

Problem
dispose-4-1
dispose-4-2
dispose-4-3
dispose-8-1
dispose-8-2
dispose-8-3
dispose-12-1
dispose-12-2
dispose-12-3
dispose-16-1
dispose-16-2
look-and-grab-4-1-1
look-and-grab-4-1-2
look-and-grab-4-1-3
look-and-grab-4-2-1
look-and-grab-4-2-2
look-and-grab-4-2-3
look-and-grab-4-3-1
look-and-grab-4-3-2
look-and-grab-4-3-3
look-and-grab-8-1-1
look-and-grab-8-1-2
look-and-grab-8-1-3
look-and-grab-8-2-1
look-and-grab-8-2-2
look-and-grab-8-2-3
look-and-grab-8-3-1
look-and-grab-8-3-2
look-and-grab-8-3-3

T0
time
len
0,1
59
0,1
110
0,3
122
2,7
426
18,4
639
197,1 761
78
1274
2555 1437
> 2.1GB
382
1702
> 2.1GB
0,3
30
0,5
4
0,61
4
35
12
49,41
4
60,02
4
> 2.1GB
213,3
4
> 2.1GB
58,2
242
75,3
90
55,89
58
> 2h
> 2h
> 2h
> 2h
> 2h
> 2h

POND
time len
9
55
36
70
308 102
> 2.1GB
> 2.1GB






3098 16
> 2h
> 2h
> 2.1GB
> 2h
> 2h
> 2.1GB












CFF
time
len
0,1
39
0,2
56
0,6
73
339,1 227
2592,1 338
> 2h

> 2.1GB



> 2h
Mcl
Mcl
> 2h
Mcl
Mcl
> 2h
> 2h
> 2h










MBP
time len
> 2h
> 2h









> 2h
0,02
5
0,01
5
> 2h
0,02
5
0,02
5
> 2h
0,02
5
0,02
5
> 2h
> 2h
> 2h
> 2h
> 2h
> 2h
> 2h
> 2h
> 2h

KACMBP
time len
17,1
81
> 2h
> 2h








0,6
54
0,0
6
0,0
6
0,63
40
0,01
6
0,01
6
0,98
60
0,02
6
0,01
6
> 2h
> 2h
> 2h
> 2h
> 2h
1195 178
> 2h
> 2h
17,9
58

Table 7: Problems Palacios Geffner (2006, 2007): Times reported seconds
rounded closest decimal. means time memory smaller instances.
Mcl mean many edges many clauses respectively.

T0 dominates domains except Adder POND planner able
solve instance, Sortnet, MBP KACMBP well, possibly due
use cardinality heuristic OBDD representations. T0 fails Adder FF
gets lost search. Looking problem closely, found FF could solve
(translation the) first instance less minute provided CNF goal
problem encoded DNF explained footnote 9, page 646. domains Adder,
Blocks, Sortnet table, along domain Look-and-Grab next table,
domains considered FF run K1 translation reports solution
brief search, triggering use complete Kmodels translation.
cases Kmodels used, K1 translation unreachable goal fluent
need try FF it.
651

fiPalacios & Geffner

Problem
push-to-4-1
push-to-4-2
push-to-4-3
push-to-8-1
push-to-8-2
push-to-8-3
push-to-12-1
push-to-12-2
push-to-12-3
1-dispose-8-1
1-dispose-8-2
1-dispose-8-3

T0
time
len
0,2
78
0,3
85
0,6
87
81,8
464
457,9
423
1293,1 597
> 2h
> 2h
> 2.1GB
82,2
1316
> 2.1GB
> 2.1GB

POND
time len
5
50
171
58

> 2h
> 2h
> 2h



> 2.1GB
> 2.1GB


CFF
time len
0,3
46
0,7
47
1,6
48
> 2.1GB
> 2.1GB
> 2.1GB



> 2h
> 2h


Table 8: problems Palacios Geffner (2006, 2007). MBP KACMBP
tried problems use different syntax. Times reported
seconds rounded closest decimal. means time memory
smaller instances.

problems reported Table 7 Table 8 variations family grid problems
(Palacios & Geffner, 2006, 2007). Dispose retrieving objects whose initial location
unknown placing trash given, known location; Push-to variation
objects picked two designated positions grid
objects pushed to: pushing object cell contiguous cell moves
object cell. 1-Dispose variation Dispose robot hand
empty condition pick actions work. result, plan 1-Dispose
scan grid, performing pick ups every cell, followed excursions trash can,
on. plans get long (a plan reported 1316 actions). Look-and-Grab
action picks objects sufficiently close any, pickup must dump objects collected trash continuing. problem
P-n-m table, n grid size number objects. Look-n-Grab,
third parameter radius action: 1 means hand picks
objects 8 surrounding cells, 2 hand picks objects 15
surrounding cells, on. domains Tables 7 8 width 1 except 1-Dispose
Look-n-Grab. because, hand empty fluent relevant
goal, clauses location objects relevant hand empty.
domains T0 appears better planners. Kmodels translation
triggered instances Look-and-Grab-n-m-r > 1 (the width
instances, mentioned Section 6.6, m, independent grid size).
also report additional data Table 9, comparing search results
use FF planner classical translations T0 , search carried
Conformant-FF original conformant problems. Conformant-FF conformant
planner built top FF searches explicitly belief space. table illustrates
two problems faced belief-space planners mentioned introduction handle
652

fiCompiling Uncertainty Away Conformant Planning Problems

Problem
bomb-100-1
bomb-100-100
Safe-100
logistics-4-10-10
square-center-8
square-center-12
cube-center-5
cube-center-7
blocks-01
blocks-02
coins-20
comm-25
uts-k-10
dispose-8-1
dispose-8-2
dispose-8-3
look-and-grab-4-1-1

Nodes
5149
100
100
356
4634
39000
2211
81600
46
1420
1235
517
58
1107
1797
2494
4955

CFF
Time
32,9
0,8
1747,4
4,42
59,3
>5602,5
8,2
>5602,5
0,0
>5602,5
20,6
56,1
16,5
339,1
2592,1
>5602,5
>5602,5

Nodes/sec
156,5
125
0,1
80,5
78,1
7
269,6
14,6
4600
0,3
60
9,2
3,5
3,3
0,7
0,4
0,9

Nodes
5250
201
102
774
46
72
74
105
47
86
783
1777
62
11713
87030
580896
79

FF T0
Time Nodes/sec
0,41
12804,9
7,53
26,7
0
25500
0,47
1646,8
0,05
920
0,03
2400
0,01
7400
0,0
5250
0
11750
0,0
4300
0,04
19575
0,43
4132,6
0,34
182,4
0,78
15016,7
14,32
6077,5
190,2
3054,1
0,1
790

Table 9: CFF Conformant Problems vs. FF Translations: Nodes stand number
nodes evaluated, Time expressed seconds, Nodes/sec stands average
number nodes per second. Numbers shown bold either CFF FF
evaluate significantly less nodes (an order-of-magnitude reduction more). Times
preceded > time outs.

653

fiPalacios & Geffner

results translation-based approach. belief representation
update problem appears overhead maintaining evaluating beliefs,
shows number nodes evaluated per second: CFF evaluates
hundred nodes per second; FF evaluates several thousands. time, heuristic
used CFF conformant setting, appears less informed heuristic used
FF classical translations. domains like Square-Center-n, Cube-Center-n,
Blocks, Look-and-Grab, FF needs orders-of-magnitude less nodes CFF find
plan, oppositive true Dispose-n-m FF evaluates many nodes
CFF. Nonetheless, even then, due overhead involved carrying beliefs, FF
manages solve problems CFF cannot solve. example, instance Dispose-8-3
solved T0 evaluating half million nodes, times CFF
evaluating less three thousand nodes.
Tables 10 11 provide details results Conformant Track 2008
International Planning Competition (IPC6) (Bryce & Buffet, 2008), held almost time
original version paper submitted, planner binaries submitted
organizers months before. version T0 IPC6 different
version T0 used IPC5, winning entry, different also
version reported paper. relation, former, T0 IPC6 cleaner
complete reimplementation; relation latter, T0 IPC6 handled problems width
greater 1 different way. explained previous section, current version
T0 , uses K1 basic translation regardless width problem, switching
Kmodels search K1 fails. version T0 IPC6, basic translation
combination K0 K1 ; precisely, merges literals L width w(L) = 1,
generated according K1 , merges literals L width w(L) 6= 1
generated all. result basic translation T0 IPC6 lighter
basic translation current version T0 could fail problems width
higher 1 latter solve. Retrospectively, good choice,
didnt much impact results. however bug program
prevented two width-1 domains, Forest Dispose, recognized such,
thus resulted use Kmodels translation, complete widths,
scale well.
two conformant planners entered IPC6 CpA(H) CpA(C);
belief-space planners represent beliefs DNF formulas, use simple
belief-state heuristics guiding search (Tran, Nguyen, Pontelli, & Son, 2008, 2009).
belief progression planners done quite effectively, progressing term
turn, according 0-approximation semantics. potential blow comes
number terms DNF formula encoding initial belief state. Rather choosing
terms initial belief state possible initial states, planners limit
terms DNF formula collection partial initial states assign
truth value literals deemed irrelevant. resulting belief representation
complete may still result exponential number terms (Son & Tu, 2006). order
reduce number terms initial DNF formula, independent one-of
expressions combined. example, two independent one-of clauses oneof (x1 , x2 )
oneof (y1 , y2 ) would give rise 4 possible initial states DNF terms, combined
single one-of expression oneof (x1 y1 , x2 y2 ), results 2 possible initial
654

fiCompiling Uncertainty Away Conformant Planning Problems

Domain
Blocks
Adder
UTS Cycle
Forest
Raos keys
Dispose

# Instances
4
4
27
9
29
90

CpA(H)
4
1
2
1
2
76

CpA(C)
3
1
2
1
2
59

T0 IPC6
3
1
3
8
1
20

Table 10: Data Conformant Track recent IPC6 Competition: Number
problems solved conformant planners, time 20 mins.
bold, entry planner performed best domain. data
Bryce Buffet (2008)

states terms. one-of expressions independent shown
interact problem. technique appears related notion critical
initial states considered Section 7, shown plans conform
critical initial states must conform also possible initial states. heuristics used
CpA(H) CpA(C) combinations cardinality heuristic, measures
number states belief state, total sum heuristic, adds heuristic distances
goal possible state, number satisfied goals, counts
number top goals achieved. heuristics simple, yet work well
benchmarks.
Tables 10 11 show data obtained IPC6 organizers planner logs.
first table appears IPC6 report (Bryce & Buffet, 2008), new domains
Forest Raos keys explained, shows number problems solved
planner, displaying bold planner best domain. planner CpA(H),
declared winner, declared best three domains (Blocks, Raos keys,
Dispose), T0 best two domains (UTS Cycle Forest), CpA(C)
best one (Adder).
Table 11 shows additional details instances; particular, total time
taken solve instance length plans three planners.
terms domain coverage, planners similarly domains, except
Forest, T0 solved instances CPA(H) solved (8/9 vs. 1/9),
Dispose, CPA(H) solved instances T0 solved (76/90 vs. 20/90).
terms time plan quality, CpA(H) CpA(C) appear slightly faster
T0 Blocks, produce much longer plans. Dispose, T0 scales better
CpA(H) CpA(C) size grids, worse number objects.
Indeed, T0 manages solve largest grid single object (Dispose-10-01),
CpA(H) CpA(C) solve instances 2 objects largest grids.
cases, plan lengths produced T0 shorter; e.g., plan Dispose-04-03
contains 125 actions T0 , 314 CpA(H), 320 CpA(C).
Dispose actually domain cardinality heuristic well generation plans, even plans tend rather long. discussed above, domain,
agent scan grid collecting set objects unknown locations, time
655

fiPalacios & Geffner

action picking object cell may contain object made (except
first time), cardinality belief state reduced. Indeed, initially object
may positions p1 , p2 , . . . , pn , pick p1 , object positions
p2 , . . . , pn gripper, pick p2 , object positions p3 , . . . , pn
gripper, on, pick action decreasing cardinality belief state,
becoming singleton belief object must gripper certainty.
problem version T0 used IPC6 Dispose domain,
FF explores many states search, explained above, used
expensive Kmodels translation instead lighter K1 translation complete
domain width 1. bug fixed, T0 solves 60 rather 20 90
Dispose instances, still failing larger grids many objects, producing
much shorter plans. example, Dispose-06-8 solved plan 470 actions,
CpA(H) CpA(C) solve plans 2881 3693 actions respectively.
bug surfaced Forest domain, prevented solution one instance
only. Forest, Dispose, UTS Cycle conformant widths equal 1,
domains larger widths (see Table 2 widths Blocks Adder).
second domain IPC6 FF got lost search Adder, indeed, T0
solve instance. instance shown solved T0 competition
report, appears mistake. Similarly, fourth instance blocks, reported
solved CPA(H), may mistake too; indeed, plan instance found
logs, T0 reports goal unreachable Kmodels translation
complete. According T0 , instance four Raos key unsolvable too.
hand, T0 failed larger UTS Cycle Raos key instances translation.
first, resulting PDDLs large cant loaded FF;
second, number init clauses turns quite large (above 300), giving rise
still larger set prime implicates (above 5000) caused translator run
memory. second instance Raos keys, however, rather small T0 didnt solve
due different bug. bug fixed, T0 solves 0.3 seconds, producing plan
53 actions, compares well solutions produced CpA(H) CpA(C)
0.7 1.9 seconds, 85 99 steps, respectively.

10. Non-Deterministic Actions
translation schemes considered limited problems deterministic actions
only. Nonetheless, illustrate below, schemes applied non-deterministic
actions well provided suitable transformations included. cover transformations briefly matter illustration only.
Consider conformant problem P non-deterministic action effects : C oneof (S1 ,
S2 , . . . , Sm ), Si set (conjunction) literals, transformed problem
P 0 , effects mapped deterministic rules form : C, hi Si ,
expression oneof (h1 , . . . , hm ) added initial situation P 0 . P 0 , hidden hi
variables used encoding uncertainty possible outcomes Si action a.
easy show non-deterministic conformant problem P deterministic conformant problem P 0 equivalent provided plans P P 0
considered non-deterministic action P executed once. Namely,
656

fiCompiling Uncertainty Away Conformant Planning Problems

Problem

Instance

Blocks

1
2
3
4
1
1
2
3
1
2
3
4
5
6
7
8
1
2
4,1
4,2
4,3
4,4
6,1
6,2
6,3
6,4
8,1
8,2
8,3
10,1
10,2

Adder
UTS Cycle

Forest

Raos keys
Dispose

CpA(H)
time
len
0
4
0,1
28
5,9
411
143,9 257
8,5
3
0,8
3
25,3
6

CpA(C)
time
len
0
7
0,1
35
6,3
157
8,3
0,6
24,7

3
3
6

3,6

24

11,6

18

0,1
0,7
0,3
0,7
1,3
2
4,7
10,4
17,7
27,6
40,1
86,7
86,7

28
85
80
197
314
431
270
643
1016
1389
753
1851
1851

0
1,9
0,4
0,9
1,8
2,8
4,5
42,2
97,9
172,5
40,3
524,6

29
99
88
206
320
434
187
735
1228
1721
518
1962

T0 IPC6
time len
0,1
5
0,1
23
17,8
83

0,1
0,7
5,4
0,2
1,3
2,2
12,1
14,4
69,7
355,1

3
7
10
16
45
78
129
115
200
256

0

16

0,1
3,6
528,3

77
110
125

0,9
217,7

204
329

7,4

326

45

683

Table 11: Running time plan length IPC6 logs. Time seconds. Blanks stand
time memory out. 13 90 Dispose-n-m instances shown,
IPC6, size n grid ranged 2 10, number objects, 1
10. T0 scales best n worst m.

657

fiPalacios & Geffner

correspondence exists conformant plans P use actions
conformant plans P 0 use actions too.
hand, conformant plan P 0 actions done many times
necessarily represent conformant plan P . Indeed, non-deterministically moves
agent right square grid n n, starting bottom left corner, n actions
row would leave agent either top left corner bottom right corner P 0 ,
anywhere Manhattan distance n origin P. divergence P
P 0 , however, arise non-deterministic actions executed once.
Building idea, non-deterministic conformant planner obtained
deterministic conformant planner following way. non-deterministic problem
P , let P1 problem P 0 above, additional constraint actions
P1 arising non-deterministic actions P executed once.
easily achieved adding precondition enabled(a) true initially
sets false. Let P2 represent deterministic conformant problem
non-deterministic action P mapped 2 deterministic actions, executable
once, hidden fluents h1 , . . . , hm oneof (h1 , . . . , hm )
expression initial situation. Similarly, let Pi deterministic problem results
encoding non-deterministic action P deterministic copies.
encoding, simple iterative conformant planner non-deterministic problems P defined terms conformant planner deterministic problems
invoking latter upon P1 , P2 , P3 , on, solution reported. reported
solution uses copy non-deterministic action once, thus encodes
solution original problem.
implemented strategy top T0 additional refinement
takes advantage nature KT,M translation, assumptions initial
situation maintained explicitly tags. Basically, non-deterministic actions Pi
allowed executed provided literals KL/hi depend
particular outcome actions (Si ) erased. implemented means
additional reset(a) action Pi whose unconditional effect enabled(a) (i.e.,
action done again) whose conditional effects KL KL/hi
KL KL/hi = 1, . . . , m. Namely, literals KL/hi truth L depends
particular non-deterministic outcome (Si ) erased, except L true
assumptions; i.e. KL true. non-deterministic actions executed
plan provided occurrence a, except first one, preceded
reset(a) action.
Table 12 compares resulting non-deterministic planner MBP KACMBP
number non-deterministic problems considered MBP KACMBP papers.
added additional domain, Slippery Gripper (sgripper), similar
classical Gripper number balls moved room B, except
robot cannot move B directly, non-deterministic move action
move(A, C, D) moves robot either C D. typical plan moving
two balls B pick A, move C D, move C B,
B, finally dropping balls B.
deterministic conformant planner (T0 ) used non-deterministic setting
added following modification: merges introduced precondition goal
658

fiCompiling Uncertainty Away Conformant Planning Problems

Problem
sgripper-10
sgripper-20
sgripper-30
btuc-100
btuc-150
btuc-200
btuc-250
btuc-300
bmtuc-10-10
bmtuc-20-10
bmtuc-20-20
bmtuc-50-10
bmtuc-50-50
bmtuc-100-10
bmtuc-100-50
bmtuc-100-100
nondet-ring-5
nondet-ring-10
nondet-ring-15
nondet-ring-20
nondet-ring-50
nondet-ring-1key-5
nondet-ring-1key-10
nondet-ring-1key-15
nondet-ring-1key-20
nondet-ring-1key-25
nondet-ring-1key-30

T0
time len
1,4
48
16,7
93
90
138
2,9
200
9,2
300
23
400
44,6 500
82
600
0,1
20
0,1
40
0,3
40
0,9
100
3,3
100
4,9
200
14,9 200
30,2 200
18,3
19
> 2h
> 2h


> 2h
> 2.1GB





MBP
time
len
> 2h
> 2h

> 2h
> 2h



65,9
29
> 2h
> 2h





0
18
2,1
38
1298,9 58
> 2h

0,1
33
11,2
122
5164,4 87
> 2.1GB



KACMBP
time
len
0,6
68
5,4
148
23,3
228
2
200
7,9
300
16,9
400
33,2
500
62,1
600
0,2
20
0,6
40
2,2
40
3,6
100
2722,4 100
25,1
200
> 2h
> 2h
0,1
32
0,5
112
2,4
242
7,3
422
603,1 2552
0,2
42
4
197
33,7
375
246,5 1104
1417,5 2043
> 2h

Table 12: Non-deterministic problems. problems except sgripper MBP
KACMBP. problems modified render simple translation
PDDL; particular, complex preconditions moved conditions. Times
reported seconds rounded closest decimal. means time memory
smaller instances.

literals literals. reason setting pays remove uncertainty
literals reset mechanism used. Indeed, provided simple change
reset mechanism, none problems move beyond P1 (a single copy
non-deterministic action) even domains non-deterministic actions
required many times plans (e.g., 2 balls room A).
seen table, T0 better MBP collection nondeterministic domains, although well KACMBP, particular, NonDetRing Non-Det-Ring-1Key domains. case, results obtained T0
domains quite meaningful. cases T0 failed solved problem, reason
classical planner (FF) got lost search plans, something may
improve advances classical planning technology.

659

fiPalacios & Geffner

11. Related Work
recent conformant planners CFF, POND, MBP cast conformant planning
heuristic search problem belief space (Bonet & Geffner, 2000). Compact belief
representations informed heuristic functions, however, critical making approach work. effective belief representation, planners use SAT OBDDs
techniques intractable worst case often exhibit good behavior average.
heuristics, hand, use fixed cardinality heuristics count number
states possible given belief state (a tractable operation OBDD representations) heuristics obtained relaxed planning graph suitably extended take
uncertain information account. heuristics appear work well domains
others. perspective, translation-based approach provides handle
two problems: belief states P become plain states translation KT,M (P ),
solved using classical heuristics. also established conditions
belief representation compact complete.
sound incomplete approach planning incomplete information advanced
Petrick Bacchus (2002) represent belief states formulas. order make
belief updates efficient though, several approximations introduced, particular,
existing disjunctions carried one belief next, new disjunctions
added. imposes limitation type problems handled.
two limitations approach domains must crafted hand,
control information derived domains search plans blind.
approach understood providing solution two problems too:
one hand, move knowledge-level done automatically, other,
problem lifted knowledge-level solved classical planners able search
control information derived automatically new representation.
third thread work related approach arises so-called 0-approximation
semantics (Baral & Son, 1997). 0-approximation semantics, belief states b represented sets states single 3-valued state fluents true, false,
unknown. Proposition 3 above, correspondence established plans
P conformant according 0-approximation semantics classical plans
translation K0 (P ), turns instance general translation
Ki (P ) complete problems width = 0. semantics translation
K0 thus related 0-approximation semantics, yet K0 translation delivers something more: computational method obtaining conformant plans comply
0-approximation semantics using classical planner.
0-approximation basic K0 translation weak dealing
existing benchmarks. translations Ki extend K0 problems higher width
replacing set fluents KL fluents KL/t tags encode assumptions
initial situation. extensions 0-approximation semantics context
conformant planning taken different form: switching single 3-valued state
representing beliefs sets 3-valued states, 3-valued state progressed efficiently
independently others (Son, Tu, Gelfond, & Morales, 2005). initial set
3-valued states obtained forcing states assign boolean truth-value (true false)
number fluents. Crucial approach work number fluents;
660

fiCompiling Uncertainty Away Conformant Planning Problems

belief representation update exponential it. conditions ensure
completeness extension 0-approximation semantics expressed terms
relevance analysis similar one underlying analysis width (Son & Tu,
2006): fluents must set true false initial 3-valued state
appearing clause CI (L) precondition goal literal L. particular,
initial situation n tautologies pi pi , relevant precondition goal literal
L, number initial 3-valued states required completeness exponential n,
make fluent pi true false. difference approach seen
tautologies pi pi relevant unique precondition goal literal Li .
case, number 3-valued partial states required completeness remains
exponential n, resulting problem width 1 thus solved
K1 translation involves tags single literal. words, tags used
translation scheme encode local contexts required different literals
problem, initial 3-valued states (Son & Tu, 2006) encode possible combinations
form global contexts. global contexts correspond consistent combinations
local contexts, may thus exponential number even problem
bounded width. planners CpA(H) CpA(C), discussed context
Conformant Track recent 2008 Int. Planning Competition (IPC6), build
approach, reduce number partial initial states required using technique
replace many one-of expressions single one (Tran et al., 2008, 2009); simplification
related notion critical initial states discussed Section 7.
Another difference 3-valued approach (Son et al., 2005; Son & Tu, 2006),
translation approach addresses representation beliefs also
computation conformant plans: conformant problem P translated problem
KT,M (P ), solved classical planner. approaches defined
top 0-approximation semantics, like knowledge-level approach planning
incomplete information Petrick Bacchus (2002), need way guide search
plans simplified belief space. search Petrick Bacchus (2002) blind
(iterative deepening), search Son et al. (2005), Son Tu (2006) guided
combination simple heuristics cardinality subgoal counting.

12. Summary
practical problems purely conformant, ability find conformant plans
needed contingent settings conformant situations special case. paper,
introduced new approach conformant planning conformant problems P
converted classical planning problems KT,M (P ) solved classical
planner. also studied conditions general translation sound
complete. translation depends two parameters: set tags, referring local
contexts initial situation, set merges stand valid disjunctions
tags. seen different translations, KS0 Kmodels, obtained
suitable choices tags merges, introduced measure complexity
conformant planning called conformant width, translation scheme Ki polynomial fixed complete problems width bounded i. also shown
conformant benchmarks width 1, developed conformant planner T0
661

fiPalacios & Geffner

based translations, shown planner exhibits good performance
comparison existing conformant planners. Recently, explored use
ideas general setting contingent planning (Albore, Palacios, & Geffner,
2009).

Acknowledgments
thank Alex Albore help syntax MBP KACMBP, Pierre Marquis
kindly answering question complexity deductive task. also thank
anonymous reviewers useful comments. H. Geffner partially supported grant
TIN2006-15387-C03-03.

Appendix A. Proofs
P stands conformant planning problem P = hF, I, O, Gi KT,M (P ) =
hF 0 , 0 , O0 , G0 translation. Propositions theorems body paper
appear appendix numbers; new lemmas propositions
numbers preceded letters B (for Appendix B). conformant problem
P classical problems P/s KT,M (P ) arise P assumed
consistent. Consistency issues important, addressed detail
second part appendix shown P consistent, KT,M (P ) consistent
(Appendix B). consistent classical problem P 0 , standard progression lemma
applies; namely, literal L achieved applicable action sequence +1 = , a,
action sequence action iff A) achieves C rule : C L P 0 ,
B) achieves L negation L0 literal L0 body C 0 rule P 0
form : C 0 L (see Theorem B.2 below).
Lemma A.1. Let action sequence applicable P K0 (P ).
achieves KL K0 (P ), achieves L P .
Proof. induction length . empty achieves KL K0 (P ),
KL must 0 , hence L must I, achieves L P .
Likewise, +1 = , achieves KL K0 (P ) A) rule : KC KL
K0 (P ), achieves KC K0 (P ); B) achieves KL K0 (P )
rule : KC 0 KL K0 (P ), achieves KL0 K0 (P ) L0 C 0 .
A) true, P must contain rule : C L, inductive hypothesis,
must achieve C P , therefore, +1 = , must achieve L P . B) true,
inductive hypothesis, must achieve L P along L0 literal L0 body
C 0 rule : C 0 L, thus +1 = , must achieve L P too.
Lemma A.2. action sequence applicable K0 (P ), applicable P .
Proof. empty, trivial. Likewise, +1 = , applicable K0 (P ),
applicable K0 (P ), thus inductive hypothesis, applicable P . Also since,
, applicable K0 (P ), must achieve literals KL K0 (P ) precondition
L a, Lemma A.1, must achieve literals L preconditions
P , thus, sequence +1 = , applicable P .
662

fiCompiling Uncertainty Away Conformant Planning Problems

Proposition 2 classical plan K0 (P ), conformant plan P .
Proof. Direct Lemma A.2 consider problem P 0 similar P new
dummy action aG whose preconditions goals G P . plan K0 (P ),
, aG applicable K0 (P 0 ), Lemma A.2, , aG applicable P 0 , implies
applicable P achieves G, thus, plan P .
Proposition 3 action sequence classical plan K0 (P ) iff conformant
plan P according 0-approximation semantics.
Proof. Let us say action sequence = a0 , . . . , 0-applicable P 0-achieves
literal L P belief sequence b0 , . . . , bn+1 generated according 0-approximation
semantics preconditions actions ai true bi , goals
true bn+1 respectively. definition 0-approximation semantics (and
consistency P ), applicable action sequence thus 0-achieves literal L P iff
empty L I, = 0 , A) : C L effect P 0 0-achieves
literal L0 C, B) 0 0-achieves L effects : C 0 L P , 0 0-achieves L0
L0 C 0 . These, however, conditions achieves literal KL
K0 (P ) sequence 0-achieving literal L P replaced sequence achieving
literal KL K0 (P ). Thus, action sequence applicable K0 (P )
0-applicable P achieves literal KL K0 (P ) iff 0-achieves literal L P ,
applicable K0 (P ) iff 0-applicable P , last part following first
using induction plan length.
Definition A.3. action P , define action sequence
followed merges KT,M (P ) arbitrary order. Similarly, = a0 , . . . , ai
action sequence P , define action sequence = a0 , . . . , KT,M (P ).
Lemma A.4. Let action sequence applicable P applicable
valid translation KT,M (P ). achieves KL/t KT,M (P ), achieves L
P/s possible initial states satisfy t.
Proof. empty , achieves KL/t, definition KT,M (P ) since
|= L, L must s, thus must achieve L P/s.

Likewise, +1 = , empty tag, +1
= , achieves KL/t

KT,M (P ) iff A) achieves KC/t KT,M (P ) rule : KC/t KL/t KT,M (P ),
B) achieves KL/t, rule : KC 0 /t KL/t, achieves KL0 /t
KT,M (P ) L0 C 0 (merge actions delete positive literals KL/t).
A, inductive hypothesis, achieves C P/s possible initial state
satisfies t, hence +1 = , achieves L P/s rule : C L must
P . B, inductive hypothesis, achieves L L0 P/s, L0 body
rule : C 0 L P , thus +1 = , achieves L P/s.
V
empty tag = , third case must considered: merge action t0 KL/t0
= , achieving KL K
KL may cause action sequence +1
T,M (P ).



case, sequence , a, hence , , must achieve KL/t0 (nonempty) t0 KT,M (P ), hence inductive hypothesis two cases
above, sequence , must achieve L P/s possible initial state satisfies
663

fiPalacios & Geffner

t0 . Yet, since merge valid, possible initial states must satisfy one
t0 , thus must achieve L P/s possible initial states s, initial
states satisfy = .
Lemma A.5. applicable valid translation KT,M (P ), applicable P .
= , applicable K
Proof. empty, direct. +1 = , a, +1
T,M (P ),

applicable KT,M (P ), achieving KL precondition L a, hence
inductive hypothesis, applicable P , Lemma A.4, must achieve
L precondition L a, thus +1 = , applicable P .

Theorem 7 translation KT,M (P ) sound provided merges valid
tags consistent.
Proof. Consider problem P 0 similar P new dummy action aG
whose preconditions goals G P . plan KT,M (P ) iff
1 , aG applicable KT,M (P 0 ), Lemma A.5 implies , aG applicable
P 0 , means plan P .
Lemma A.6. Let action sequence applicable P applicable
KS0 (P ). achieves L P/s possible initial state s, achieves KL/s
KS0 (P ).
Proof. empty achieves L P/s, L s, since |= L, KL/s must
0 thus achieves KL/s KS0 (P ).
Likewise, +1 = , achieves L P/s A) rule : C L
achieves C P/s; B) achieves L rule : C 0 L, achieves L0
KS0 (P ) L0 C 0 .
A), inductive hypothesis, achieves KC/s KS0 (P ) and, rule : KC/s
= , achieves KL/s (merges
KL/s, , must achieve KL/s, thus, +1
delete positive literals KL/t).
B), inductive hypothesis, achieves KL/s KL0 /s KS0 (P ) L0
body rule : C 0 L P , therefore , achieves KL/s,
= , .
+1
Lemma A.7. applicable P , applicable KS0 (P ).
Proof. empty, trivial. +1 = , applicable P , must
applicable P must achieve precondition L P/s every possible initial
state s, S0 . inductive hypothesis, must applicable KS0 (P ),
Lemma A.6, V
must achieve literals KL/s S0 , then, last
merge action effect sS0 KL/s KL must achieve KL, ,
therefore, , applicable KS0 (P ).
Theorem 9 conformant plan P , classical plan 0 KS0 (P )
result dropping merge actions 0 .
664

fiCompiling Uncertainty Away Conformant Planning Problems

Proof. Direct Lemma A.7 consider problem P 0 similar P new
action aG whose preconditions goals G P . plan P , sequence , aG
applicable P 0 , Lemma A.7, , aG applicable KS0 (P 0 ), thus
plan KS0 (P ).
Definition A.8. rel(s, L) stands set literals L0 relevant L P :
rel(s, L) = {L0 | L0 L0 relevant L} .
Definition A.9. stands deductive closure I:
= { L | I, |= L} .
Theorem A.10. Let = {t1 , . . . , tn } covering merge literal L valid translation KT,M (P ) problem P whose initial situation prime implicate form.
tag ti must possible initial state P rel(s, L) ti .
Proof. Assume otherwise state satisfying makes true literal Ls relevant
L Ls 6 ti . take c disjunction literals Ls
states satisfy I, obtain entails c, since prime implicate form,
means c contains tautology c0 subsumed clause c00 I. But, either case,
contradiction, literals c0 c00 relevant L, hence ti , ti
part covering merge m, must contain literal either c0 c00 , hence c.
Lemma A.11. Let action sequence applicable P applicable covering translation KT,M (P ). Then, achieves L P/s possible
initial state tag rel(s, L) , achieves KL/t
KT,M (P ).
Proof. empty achieves L P/s, L thus, rel(s, L). Since
rel(s, L) , L , thus KL/t initial situation 0 KT,M (P ),
achieves KL/t KT,M (P ). Likewise, +1 = , achieves L P/s, A)
rule : C L P achieves C P/s, B) achieves L P/s
rule : C 0 L, achieves L0 P/s L0 C 0 . A, inductive
hypothesis, achieves KC/t, support rule : KC/t KL/t KT,M (P ),
= , , merges cannot
, must achieve KL/t KT,M (P ), must +1
delete positive literal KL/t. B, inductive hypothesis, achieves KL/t,
cancellation rule : KC 0 /t KL/t arising rule : C 0 L P , must
= , ,
achieve KL0 /t literal L0 C 0 . means , a, therefore, +1
must achieve KL/t.
Lemma A.12. Let KT,M (P ) covering translation P . applicable P ,
applicable KT,M (P ).
Proof. empty, direct. Else, +1 = , applicable P , must
applicable P must achieve literal L P re(a), therefore, inductive
hypothesis must applicable KT,M (P ). Then, let = {t1 , . . . , tn } covering
merge L P re(a) KT,M (P ). Theorem A.10, ti must
665

fiPalacios & Geffner

possible initial state rel(s, L) ti , Lemma A.11, achieving L
P/s implies achieving KL/ti KT,M (P ). Since true ti achieves
L P re(a) P/s possible initial states s, follows achieves KL/ti
ti KT,M (P ), therefore achieves KL KT,MV(P ) ends
sequence merges include action merge am,L effect ti KL/ti KL.
= , applicable K
result, +1
T,M (P ).
Theorem 15 Covering translations KT,M (P ) complete; i.e., conformant plan
P , classical plan 0 KT,M (P ) 0 merge actions
removed.
Proof. theorem follows trivially Lemma A.12 problem P 0 like
P additional, dummy action aG goals G P preconditions
aG . action sequence plan P iff action sequence , aG applicable P 0 ,
due Lemma A.12 implies action sequence , aG applicable KT,M (P 0 )
turn true iff action sequence plan KT,M (P ). sequence ,
turn, sequence merge actions removed.
Theorem 17 translation Kmodels(P ) sound complete.
Proof. Direct merges generated Kmodels precondition goal
literals L. Clearly merges valid, tags consistent I,
cover L (the models CI (L) satisfy CI (L)). Thus result follows Theorems 7
15.
Proposition 21 width w(P ) P determined time exponential
w(P ).
Proof. number clauses CI (L), mi sets clauses C
CI (L) |C| = i. clause one set must n literals, n
number fluents P , hence, one literal clause C collected, end
ni sets literals size greater i, inconsistent
consistent minimal (no consistent set collection
properly included); tests polynomial given prime implicate form.
Thus constructing cover c(C) set clauses C |C| = exponential i,
checking whether one cover satisfies CI (L) polynomial operation provided
prime implicate form. Indeed, c(C) = {t1 , . . . , tn }, computing closures
ti ti c(C), PI, testing whether ti intersects clause
CI (L) polynomial operations (the former reducing checking literal L0
whether |= ti L0 ). Thus computing width(L), generate sets C clauses
CI (L) |C| = i, starting = 0, increasing one one one set,
c(C) satisfies CI (L). computation exponential w(L), computation
preconditions goal literals P exponential w(P ).
Proposition 22 width P 0 w(P ) n, n number
fluents whose value initial situation known.
666

fiCompiling Uncertainty Away Conformant Planning Problems

Proof. inequality 0 w(P ) direct w(L) defined size |C| minimal set
clauses C CI (L) c(C) satisfies CI (L), w(P ) = w(L) precondition
goal literal L. inequality w(P ) n follows noticing set C clauses
given tautologies L0 L0 CI (L), c(C) must satisfy clause c CI (L),
c(C) must assign truth value literal c, inconsistent c,
inconsistent thus pruned c(C). Finally, max number tautologies
CI (L) number fluents L0 neither L0 L0 unit clauses I.
Theorem 24 fixed i, translation Ki (P ) sound, polynomial, w(P ) i,
covering complete.
Proof. soundness, need prove merges Ki (P ) valid
tags Ki (P ) consistent. soundness follows Theorem 7. merges
literal L Ki (P ) given covers c(C) collections C less clauses
Ci (L) clearly since
model must satisfy CI (L), must satisfy
W
c(C) |= tm = c(C). time, definition
cover c(C), tags must consistent I.
proving Ki polynomial fixed i, follow ideas similar ones used
proof Proposition 21 above, shown width P
determined time exponential w(P ) polynomial number clauses
fluents P . fixed i, number sets clauses C CI (L) size |C|
polynomial, complexity computing covers c(C) sets, hence,
merges L Ki (P ) polynomial too. Thus, whole translation Ki (P ) fixed
polynomial number clauses, fluents, rules P .
Finally, proving completeness, w(P ) i, w(L) precondition
goal literal L P . Therefore, literal L, set C clauses CI (L)
c(C) satisfies CI (L). translation Ki (P ) generate unique merge
L covers L. Since Ki (P ) valid translation, means Ki (P ) covering
translation, complete, virtue Theorem 15.
Lemma A.13. L0 relevant L rel(s, L) rel(s0 , L), rel(s, L0 ) rel(s0 , L0 ).
Proof. L00 rel(s, L0 ), L00 relevant L0 , since L0 relevant L
relevance relation transitive, L00 relevant L. Thus, L00 rel(s, L) therefore,
since rel(s, L) rel(s0 , L), L00 rel(s0 , L). L00 s0 since relevant
L0 , L00 rel(s0 , L0 ).
Proposition 26 Let s0 two states let action sequence applicable
classical problems P/s P/s0 . achieves literal L P/s0 rel(s0 , L)
rel(s, L), achieves literal L P/s.
Proof. induction length . empty, achieves literal L P/s0 , L
must s0 , since L relevant itself, L rel(s0 , L). rel(s0 , L) rel(s, L),
L must s, thus achieves L P/s.
667

fiPalacios & Geffner

Likewise, +1 = , achieves L P/s0 A) rule : C L
achieves C P/s0 ; B) achieves L P/s0 rule : C 0 L, achieves
L0 P/s0 L0 C 0 .
A, must achieve literal Li C P/s0 . Since Li relevant L rel(s0 , L)
rel(s, L), Lemma A.13, rel(s0 , Li ) rel(s, Li ). Then, inductive hypothesis, plan
must achieve Li P/s Li C, thus +1 = , must achieve L P/s
B, since L0 relevant L (as L0 relevant L), rel(s0 , L)
rel(s, L), Lemma A.13, rel(s0 , L0 ) rel(s, L0 ), thus inductive hypothesis,
must achieve L0 P/s also L, +1 = , must achieve L P/s.
Lemma A.14. 0 two collection states every state
every precondition goal literal L P , state s0 0 rel(s0 , L)
rel(s, L), applicable P/S 0 , applicable P/S.
Proof. induction length . empty, obvious. +1 = , applicable
P/S 0 , applicable P/S 0 and, inductive hypothesis, applicable P/S.
need prove achieves preconditions action P/S.
L P rec(a) S, hypothesis, state s0 0
rel(s0 , L) rel(s, L). Proposition 26, since achieves L P/s0 , must
achieve L P/s. Since argument applies S, achieves L P/S, thus
+1 = , must applicable P/S.
Proposition 27 0 two collections states every state
every precondition goal literal L P , state s0 0 rel(s0 , L)
rel(s, L), plan P conforms 0 , plan P conforms
S.
Proof. Lemma A.14, consider problem P 0 similar P new action
aG whose preconditions goals G P . plan P conforms 0 ,
action sequence , aG applicable P 0 /S 0 , lemma, , aG
applicable P 0 /S, thus must plan P/S
Proposition 28 0 basis P every possible initial state P every
precondition goal literal L P , 0 contains state s0 rel(s0 , L) rel(s, L).
Proof. Direct Proposition 27, considering set possible initial states
P .
Proposition 29 initial situation prime implicate form = {t1 , . . . , tn }
merge covers literal L P , set S[ti , L] possible initial states P
rel(s, L) ti non-empty.
Proof. Direct Theorem A.10.
Theorem 30 Let KT,M (P ) covering translation let 0 stand collection
states s[ti , L] L precondition goal literal P ti tag merge
covers L. 0 basis P .
668

fiCompiling Uncertainty Away Conformant Planning Problems

Proof. show every possible initial state precondition goal literal
L, 0 theorem contains state s0 rel(s0 , L) rel(s, L). result
follows Proposition 28. Indeed, state must satisfy tag ti covering
merge = {t1 , . . . , tn } L, merges valid. Theorem A.10, must
possible initial state s0 rel(s0 , L) ti , therefore, rel(s0 , L) rel(s, L)
must satisfy ti possibly literals L0 relevant L.
Theorem 31 P conformant planning problem bounded width, P admits
basis polynomial size.
Proof. w(P ) fixed i, Ki (P ) covering translation polynomial number
merges tags, case, basis 0 P defined Theorem 30 contains
polynomial number states, regardless number possible initial states.

Appendix B. Consistency
assuming throughout paper conformant planning problems P
translations KT,M (P ) consistent. section make notion precise,
explain needed, prove KT,M (P ) consistent P is. proof,
take account heads KL merge actions am,L KT,M (P ), extended
literals KL0 literals L0 mutex L P (see Definition 4).
start beginning assuming states truth-assignments sets
literals fluents language. state complete every literal L, L L
s, consistent literal L L s. Complete consistent
states represent truth-assignments fluents F consistency P
translation KT,M (P ) ensures applicable action sequences map complete
consistent states complete consistent states s0 . guaranteed, complete
consistent states referred simply states done
paper.
Given complete state action applicable s, next state sa
sa = (s \ Del(a, s)) Add(a, s)

Add(a, s) = {L | : C L P C s}

Del(a, s) = {L | L Add(a, s)} .
follows sa complete state complete state, action
deletes literal L L added s. hand, may consistent
sa inconsistent, example, rules : C L : C 0 L
C C 0 s. order exclude possibility, ensuring reachable
states complete consistent, thus represent genuine truth assignments
fluents F , consistency condition P needed:
Definition B.1 (Consistency). classical conformant problem P = hF, I, O, Gi consistent initial situation logically consistent every pair complementary
literals L L mutex P .
669

fiPalacios & Geffner

consistent classical problem P , reachable states complete consistent,
standard progression lemma used preceding proofs holds:
Theorem B.2 (Progression). action sequence +1 = , applicable complete
consistent state achieves literal L consistent classical problem P iff A) achieves
body C rule : C L P , B) achieves L every rule : C 0 L,
achieves L0 literal L0 C 0 .
see conformant problem P consistent sense,
valid translation KT,M (P ). tested benchmarks considered paper
consistency found consistent except two domains
introduced elsewhere: 1-Dispose Look-and-Grab. cases, since consistency
classical problem KT,M (P ) cannot inferred consistency P ,
checked explicitly using Definition B.1, similarly, plans obtained
KT,M (P ) checked consistency indicated Section 8: soundness
plans ensured provided never trigger conflicting effects KL/t KL/t.10
Proof. proof Theorem B.2 rest particular definition mutexes,
mutex atoms true reachable state. consistent problem P ,
applicable action sequence maps complete consistent state s0 represents
truth assignment. Then, action sequence +1 = , achieves L iff C) L Add(a, s0 )
D) L s0 L 6 Del(a, s0 ). Condition theorem, however, equivalent C,
Condition B theorem, equivalent D. Indeed, L 6 Del(a, s0 ) iff rule
: C 0 L literal L0 C 0 L0 6 s0 , which, given s0 complete
consistent, true iff L0 s0 (this precisely consistency needed; else L0 s0
would imply L0 6 s0 ).
notion mutex used definition consistency expresses guarantee
pair literals true reachable state. Sufficient polynomial conditions
mutual exclusivity type invariants defined various papers,
follow definition Bonet Geffner (1999).
Definition B.3 (Mutex Set). mutex set collection R unordered literals pairs
(L, L0 ) classical conformant problem P that:
1. pair (L, L0 ) R, L L0 possible initial state s,
2. : C L : C 0 L0 two rules action (L, L0 ) pair
R, P re(a) C C 0 mutex R,
3. : C L rule P literal L pair (L, L0 ) R, either a) L0 = L,
b) P re(a) C mutex L0 R, c) P re(a) C implies C 0 R rule
: C 0 L0 P ;
10. consistency two domains, 1-Dispose Look-and-Grab, established however
definition mutexes slightly stronger one used. actually suffices change
expression P re(a) C clause 3c) definition mutex sets P re(a) C {L0 }.

670

fiCompiling Uncertainty Away Conformant Planning Problems

definition, pair said mutex R belongs R, set literals
said mutex R contains pair R, set literals said imply set
literals 0 R mutex R complement L literal L 0 \ S.
easy verify R1 R2 mutex sets, union R1 R2 mutex set,
thus maximal mutex set P denote R . pairs R
called mutexes.
simplicity without loss generality, assume preconditions P re(a)
empty. Indeed, simple show mutexes problem P remain
preconditions pushed conditions. also assume condition C rule
C L P mutex, rules simply pruned. addition, assume
literal L mutex pair complementary literals L0 L0 , L cannot
true reachable state, thus, pruned well.
definition mutexes sound, meaning pair mutex set true
reachable state:
Theorem B.4. (L, L0 ) pair mutex set R classical conformant problem
P , reachable state P , {L, L0 } s.
Proof. proceed inductively. Clearly, L L0 cannot part possible initial state,
ruled definition mutex sets. Thus, let us assume inductive hypothesis
L L0 part state reachable less steps, let us prove
true states s0 = sa reachable one step. Clearly
L L0 belong s0 , either A) L L0 belong Add(a, s), B) L belongs
Add(a, s) L0 belongs Del(a, s). show possible.
A, P must comprise rules : C L : C 0 L0 C C 0 s, yet
definition mutex sets, C C 0 must mutex, inductive hypothesis
C C 0 6 s. B, must rule : C L C s, L0
inductive hypothesis, follows L0 mutex C R, thus,
mutex set definition, either L0 = L C implies C 0 rule : C 0 L0 .
first case, however, due rule : C L C s, L0 Del(a, s),
second case, completeness reachable states, must C 0 s, hence
L0 Del(a, s), contradicting B cases.
Provided initial situation conformant planning problem P prime
implicate form, computing largest mutex set R testing consistency P
polynomial time operations. former, one starts set literal pairs
iteratively drops set pairs comply definition
reaching fixed point (Bonet & Geffner, 1999).
move prove conformant problem P consistent, valid
translation KT,M (P ). consistency classical problems P/s possible initial
states direct, set mutexes P subset set mutexes P/s
initial situation constrained.
Proposition B.5 (Mutex Set RT ). valid translation KT,M (P ) consistent conformant problem P , define RT set (unordered) literals pairs (KL/t, KL0 /t0 )
(KL/t, KL0 /t) (L, L0 ) mutex P , t0 two tags jointly satisfiable
(I 6|= (t t0 )). RT mutex set KT,M (P ).
671

fiPalacios & Geffner

follows KT,M (P ) consistent P consistent, L0 = L
mutex L P , (KL/t, KL/t) must mutex RT .
Theorem B.6 (Consistency KT,M (P )). valid translation KT,M (P ) consistent P
consistent.
consistency translation K0 (P ) follows special case, K0 (P ) KT,M (P )
empty set merges set tags containing empty tag.
left prove Proposition B.5.
Proof Proposition B.5. must show set RT comprised pairs (KL/t, KL0 /t0 )
(KL/t, KL0 /t) L0 mutex L P , tags t0 jointly satisfiable
I, set complies clauses 1, 2, 3 Definition B.3. go one clause
time.
1. pair RT true initially KT,M (P ) = hF 0 , 0 , O0 , G0 jointly satisfiable
I, t, t0 . Indeed, KL/t KL0 /t0 0 must possible initial
state satisfying t0 L L0 true contradiction L L0
mutex P . Similarly, KL/t 0 KL0 /t not, must case
|= L 6|= L0 , must possible initial state P
t, L, L0 hold, contradiction L L0 mutex P too.
2. action rules KL/t KL0 /t0 rules must support
rules form : KC/t KL/t : KC 0 /t0 KL0 /t0 arising rules
: C L : C 0 L0 P .11 since L L0 mutex P , C C 0
must contain literals L1 C L2 C 0 (L1 , L2 ) mutex P ,
hence (KL1 /t, KL2 /t0 ) belongs RT , KC/t KC 0 /t0 mutex RT
well.
Similarly, action rules KL/t KL0 /t literal L0
mutex L P , rules must support cancellation rules form
: KC/t KL/t : KC 0 /t KL0 /t, arising rules : C L
: C 0 L0 P . Since L L0 mutex P , C C 0 must contain literals
L1 C L2 C 0 mutex P , hence RT must contain pair
(KL1 /t, KL2 /t), KC/t KC 0 /t must mutex RT .
3. left show set RT given pairs (KL/t, KL0 /t0 ) (KL/t,
KL0 /t) complies clause 3 definition mutex sets well. Consider
first class pairs (KL/t, KL0 /t0 ) rule : KC/t KL/t KL/t arising
rule : C L P . Since L mutex L0 P , one conditions 3a,
3b, 3c must hold rule : C L L0 . 3a, L0 = L, KC/t
must imply body KC/t0 cancellation rule : KC/t0 KL/t0 ,
literal L1 C, RT must contain pair (KL1 /t, KL1 /t0 ) KL1 /t
implies KL1 /t0 , KC/t implies KC/t0 (case 3c). 3b, C L0
11. action cannot merge literal L00 mutex L L0 , case, L00 implies
L L0 mutex. Similarly, cannot merge L case, L mutex
L0 L0 . reason, cannot merge L0 either. Thus, action
cannot merge must action P .

672

fiCompiling Uncertainty Away Conformant Planning Problems

mutex P , thus C contains literal L1 mutex L0 P . means
pair (KL1 /t, KL0 /t0 ) RT hence KC/t mutex KL0 /t0 RT
(case 3b). Last, 3c, C implies C 0 P rule : C 0 L0 , KC/t must
imply body KC 0 /t0 cancellation rule : KC 0 /t0 KL0 /t0 . Indeed,
literal L1 C C 0 , KL1 /t implies KL1 /t0 ,
L2 literal C 0 C, literal L3 C must mutex
L2 P , hence pair (KL3 /t, KL2 /t0 ) must RT KL3 /t implies
KL2 /t0 (case 3c)
0 0
Consider
V pair (KL/t, KL /t ) along merge action am,L
rule ti KL/ti KL KL/t = KL (thus empty tag). case, since
merge valid t0 consistent, must ti ti
t0 jointly consistent I. follows (KL/ti , KL0 /t0 ) pair RT
thus body merge mutex KL0 /t0 RT (case 3b).

need consider pair (KL/t, KL0 /t0 ) along rules KL0 /t0 ,
literals KL/t KL0 /t0 structure, thus argument
applies, replacing t0 L L0 .
switch second class pairs (KL/t, K/L0 /t) rules :
KC/t KL/t KL/t. Since L L0 mutex P , conditions 3a, 3b,
3c must hold. a, L0 = L, case, condition 3c holds KT,M (P )
KC/t implies body KC/t rule : KC/t KL0 (L0 = L). b, C
mutex L0 , thus literal L1 C L1 L0 mutex
P , therefore KC/t KL0 /t mutex RT (case 3b). Finally, c, C
implies C 0 rule : C 0 L0 P , KC/t must imply KC 0 /t RT
rule : KC 0 /t KL0 /t (case 3c).
empty tag t, rule KL/t may also merge, due extra
effects KL0 merge action L, merge KL also merge KL0 ,
case 3c holds.
Last, class pairs, rules KL0 /t cancellation rules
form : KC 00 /t KL0 /t rule : C 00 L0 P . Since L0 mutex
L P , conditions 3a, 3b, 3c must hold rule : C 00 L0 L0 P .
a, L = L0 , cancellation rule : KC 00 /t KL (case 3c).
b, C 00 mutex L, thus literal L2 C 00 (L2 , L) mutex
P , therefore KL/t implies KL2 /t RT , hence KL2 /t KC 00 /t
imply KL/t RT (case 3b). Finally, c, C 00 implies C 0 rule : C 0 L P ,
KC 00 /t must imply KC 0 /t rule : KC 0 /t KL/t RT .
Indeed, LA implies LB P , LB implies LA P , KLB /t implies KLA /t
RT , KLA /t implies KLB /t.

References
Albore, A., Palacios, H., & Geffner, H. (2009). translation-based approach contingent
planning. Proc. 21st Int. Joint Conference AI (IJCAI-09), pp. 16231628.
673

fiPalacios & Geffner

Baral, C., Kreinovich, V., & Trejo, R. (2000). Computational complexity planning
approximate planning presence incompleteness. Artificial Intelligence, 122 (12), 241267.
Baral, C., & Son, T. C. (1997). Approximate reasoning actions presence sensing
incomplete information. Proc. ILPS 1997, pp. 387401.
Bayardo Jr., R., & Schrag, R. (1997). Using CSP look-back techniques solve real-world
sat instances. Proc. AAAI, pp. 203208.
Bertoli, P., & Cimatti, A. (2002). Improving heuristics planning search belief space.
Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proc. AIPS-2002, pp. 143152.
AAAI Press.
Bonet, B., & Geffner, H. (1999). Planning heuristic search: New results. Proceedings
ECP-99, pp. 359371. Springer.
Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic search
belief space. Proc. AIPS-2000, pp. 5261. AAAI Press.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (1
2), 533.
Bonet, B., & Givan, B. (2006). Results conformant track 5th int. planning
competition. http://www.ldc.usb.ve/bonet/ipc5/docs/results-conformant.pdf.
Bryce, D., & Buffet, O. (2008). International planning competition uncertainty part: Benchmarks results. http://ippc-2008.loria.fr/wiki/images/0/03/Results.pdf.
Bryce, D., Kambhampati, S., & Smith, D. E. (2006). Planning graph heuristics belief
space search. Journal Artificial Intelligence Research, 26, 3599.
Cimatti, A., Roveri, M., & Bertoli, P. (2004). Conformant planning via symbolic model
checking heuristic search. Artificial Intelligence, 159, 127206.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Goldman, R. P., & Boddy, M. S. (1996). Expressive planning explicit knowledge.
Proc. AIPS-1996, pp. 110117.
Haslum, P., & Jonsson, P. (1999). results complexity planning incomplete information. Proc. ECP-99, Lect. Notes AI Vol 1809, pp. 308318.
Springer.
Hoffmann, J., & Brafman, R. (2005). Contingent planning via heuristic forward search
implicit belief states. Proc. 15th Int. Conf. Automated Planning Scheduling
(ICAPS 2005), pp. 7180. AAAI.
Hoffmann, J., & Brafman, R. (2006). Conformant planning via heuristic forward search:
new approach. Artificial Intelligence, 170 (6-7), 507541.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Marquis, P. (2000). Consequence finding algorithms. Gabbay, D., & Smets, P. (Eds.),
Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 5, pp.
41145. Kluwer.
674

fiCompiling Uncertainty Away Conformant Planning Problems

Palacios, H., & Geffner, H. (2006). Compiling uncertainty away: Solving conformant planning problems using classical planner (sometimes). Proc. AAAI-06, pp. 900905.
Palacios, H., & Geffner, H. (2007). conformant classical planning: Efficient translations may complete too. Proc. ICAPS-07, pp. 264271.
Petrick, R., & Bacchus, F. (2002). knowledge-based approach planning incomplete
information sensing. Proc. AIPS-02, pp. 212221.
Rintanen, J. (2004). Complexity planning partial observability. Proc. ICAPS2004, pp. 345354.
Smith, D., & Weld, D. (1998). Conformant graphplan. Proceedings AAAI-98, pp. 889
896. AAAI Press.
Son, T. C., Tu, P. H., Gelfond, M., & Morales, A. (2005). Conformant planning domains
constraints: new approach. Proc. AAAI-05, pp. 12111216.
Son, T. C., & Tu, P. H. (2006). completeness approximation based reasoning
planning action theories incomplete information.. Proc. 10th Int. Conf.
Principles KR Reasoning (KR-06), pp. 481491.
Tison, P. (1967). Generalized consensus theory applications minimization
boolean circuits. IEEE Transactions Computers, EC-16 (4), 446456.
Tran, D., Nguyen, H., Pontelli, E., & Son, T. C. (2008). CPA(C)/(H): Two approximationbased conformant planners. http://ippc-2008.loria.fr/wiki/images/5/57/Team2CPA.pdf.
Tran, D., Nguyen, H., Pontelli, E., & Son, T. C. (2009). Improving performance conformant planners: Static analysis declarative planning domain specifications. Practical Aspects Declarative Languages, 11th International Symposium, PADL 2009,Proceedings, Vol. 5418 Lecture Notes Computer Science, pp. 239253. Springer.
Turner, H. (2002). Polynomial-length planning spans polynomial hierarchy. JELIA
02: Proc. European Conference Logics AI, pp. 111124. Springer-Verlag.

675

fiJournal Artificial Intelligence Research 35 (2009) 449-484

Submitted 1/09; published 7/09

Efficient Markov Network Structure Discovery
Using Independence Tests
Facundo Bromberg

fbromberg@frm.utn.edu.ar

Departamento de Sistemas de Informacion,
Universidad Tecnologica Nacional,
Mendoza, Argentina

Dimitris Margaritis
Vasant Honavar

dmarg@cs.iastate.edu
honavar@cs.iastate.edu

Dept. Computer Science,
Iowa State University,
Ames, IA 50011

Abstract
present two algorithms learning structure Markov network data:
GSMN GSIMN. algorithms use statistical independence tests infer structure successively constraining set structures consistent results
tests. recently, algorithms structure learning based maximum likelihood estimation, proved NP-hard Markov networks due
difficulty estimating parameters network, needed computation
data likelihood. independence-based approach require computation
likelihood, thus GSMN GSIMN compute structure efficiently
(as shown experiments). GSMN adaptation Grow-Shrink algorithm
Margaritis Thrun learning structure Bayesian networks. GSIMN extends GSMN additionally exploiting Pearls well-known properties conditional
independence relation infer novel independences known ones, thus avoiding performance statistical tests estimate them. accomplish efficiently GSIMN uses
Triangle theorem, also introduced work, simplified version set
Markov axioms. Experimental comparisons artificial real-world data sets show
GSIMN yield significant savings respect GSMN , generating Markov
network comparable cases improved quality. also compare GSIMN
forward-chaining implementation, called GSIMN-FCH, produces possible conditional independences resulting repeatedly applying Pearls theorems known
conditional independence tests. results comparison show GSIMN,
sole use Triangle theorem, nearly optimal terms set independences
tests infers.

1. Introduction
Graphical models (Bayesian Markov networks) important subclass statistical models possess advantages include clear semantics sound widely
accepted theoretical foundation (probability theory). Graphical models used
represent efficiently joint probability distribution domain. used
numerous application domains, ranging discovering gene expression pathways
bioinformatics (Friedman, Linial, Nachman, & Peer, 2000) computer vision (e.g. Geman

c
2009
AI Access Foundation. rights reserved.

fiBromberg, Margaritis, & Honavar

Figure 1: Example Markov network. nodes represent variables domain V =
{0, 1, 2, 3, 4, 5, 6, 7}.

& Geman, 1984, Besag, York, & Mollie, 1991, Isard, 2003, Anguelov, Taskar, Chatalbashev,
Koller, Gupta, Heitz, & Ng, 2005). One problem naturally arises construction
models data (Heckerman, Geiger, & Chickering, 1995, Buntine, 1994). solution
problem, besides theoretically interesting itself, also holds potential
advancing state-of-the-art application domains models used.
paper focus task learning Markov networks (MNs) data
domains variables either discrete continuous distributed according
multidimensional Gaussian distribution. MNs graphical models consist two
parts: undirected graph (the model structure), set parameters. example
Markov network shown Figure 1. Learning models data consists two interdependent tasks: learning structure network, and, given learned structure,
learning parameters. work focus problem learning structure
MN domain data.
present two algorithms MN structure learning data: GSMN (Grow-Shrink
Markov Network learning algorithm) GSIMN (Grow-Shrink Inference-based Markov
Network learning algorithm). GSMN algorithm adaptation Markov networks
GS algorithm Margaritis Thrun (2000), originally developed learning
structure Bayesian networks. GSMN works first learning local neighborhood
variable domain (also called Markov blanket variable),
using information subsequent steps improve efficiency. Although interesting
useful itself, use GSMN point reference performance regard
time complexity accuracy achieved GSIMN, main result work.
GSIMN algorithm extends GSMN using Pearls theorems properties
conditional independence relation (Pearl, 1988) infer additional independences
set independences resulting statistical tests previous inferences, thus avoiding
execution tests data. allows savings execution time and, data
distributed, communication bandwidth.
rest paper organized follows: next section present previous
research related problem. Section 3 introduces notation, definitions presents
intuition behind two algorithms. Section 4 contains main algorithms, GSMN
GSIMN, well concepts practical details related operation. evaluate
GSMN GSIMN present results Section 5, followed summary
450

fiEfficient Markov Network Structure Discovery Using Independence Tests

work possible directions future research Section 6. Appendices B contain
proofs correctness GSMN GSIMN.

2. Related Work
Markov networks used physics computer vision communities (Geman &
Geman, 1984, Besag et al., 1991, Anguelov et al., 2005) historically
called Markov random fields. Recently interest use spatial
data mining, applications geography, transportation, agriculture, climatology,
ecology others (Shekhar, Zhang, Huang, & Vatsavai, 2004).
One broad popular class algorithms learning structure graphical models
score-based approach, exemplified Markov networks Della Pietra, Della Pietra,
Lafferty (1997), McCallum (2003). Score-based approaches conduct search
space legal structures attempt discover model structure maximum score.
Due intractable size search space i.e., space legal graphs,
super-exponential size, score-based algorithms must usually resort heuristic search.
step structure search, probabilistic inference step necessary evaluate
score (e.g., maximum likelihood, minimum description length, Lam & Bacchus, 1994,
pseudo-likelihood, Besag, 1974). Bayesian networks inference step tractable
therefore several practical score-based algorithms structure learning developed
(Lam & Bacchus, 1994, Heckerman, 1995, Acid & de Campos, 2003). Markov networks
however, probabilistic inference requires calculation normalizing constant (also
known partition function), problem known NP-hard (Jerrum & Sinclair, 1993,
Barahona, 1982). number approaches considered restricted class graphical
models (e.g. Chow & Liu, 1968, Rebane & Pearl, 1989, Srebro & Karger, 2001). However,
Srebro Karger (2001) prove finding maximum likelihood network NP-hard
Markov networks tree-width greater 1.
work area structure learning undirected graphical models concentrated learning decomposable (also called chordal) MNs (Srebro & Karger,
2001). example learning non-decomposable MNs presented work Hofmann Tresp (1998), approach learning structure continuous domains
non-linear relationships among domain attributes. algorithm removes edges
greedily based leave-one-out cross validation log-likelihood score. non-score based
approach work Abbeel, Koller, Ng (2006), introduces new class efficient algorithms structure parameter learning factor graphs, class graphical
models subsumes Markov Bayesian networks. approach based new
parameterization Gibbs distribution potential functions forced
probability distributions, supported generalization Hammersley-Clifford
theorem factor graphs. promising theoretically sound approach may
lead future practical efficient algorithms undirected structure learning.
work present algorithms belong independence-based constraintbased approach (Spirtes, Glymour, & Scheines, 2000). Independence-based algorithms exploit fact graphical model implies set independences exist distribution domain, therefore data set provided input algorithm (under
assumptions, see next section); work conducting set conditional independence
451

fiBromberg, Margaritis, & Honavar

tests data, successively restricting number possible structures consistent
results tests singleton (if possible), inferring structure
possible one. desirable characteristic independence-based approaches fact
require use probabilistic inference discovery structure.
Also, algorithms amenable proofs correctness (under assumptions).
Bayesian networks, independence-based approach mainly exemplified
SGS (Spirtes et al., 2000), PC (Spirtes et al., 2000), algorithms learn
Markov blanket step learning Bayesian network structure Grow-Shrink
(GS) algorithm (Margaritis & Thrun, 2000), IAMB variants (Tsamardinos, Aliferis,
& Statnikov, 2003a), HITON-PC HITON-MB (Aliferis, Tsamardinos, & Statnikov,
2003), MMPC MMMB (Tsamardinos, Aliferis, & Statnikov, 2003b), max-min hill
climbing (MMHC) (Tsamardinos, Brown, & Aliferis, 2006), widely used
field. Algorithms restricted classes trees (Chow & Liu, 1968) polytrees
(Rebane & Pearl, 1989) also exist.
learning Markov networks previous work mainly focused learning Gaussian
graphical models, assumption continuous multivariate Gaussian distribution
made; results linear dependences among variables Gaussian noise (Whittaker, 1990, Edwards, 2000). recent approaches included works Dobra,
Hans, Jones, Nevins, Yao, West (2004), (Castelo & Roverato, 2006), Pena (2008),
Schafer Strimmer (2005), focus applications Gaussian graphical models
Bioinformatics. make assumption continuous Gaussian variables
paper, algorithms present applicable domains use
appropriate conditional independence test (such partial correlation). GSMN
GSIMN algorithms presented apply case arbitrary faithful distribution assumed probabilistic conditional independence test distribution
available. algorithms first introduced Bromberg, Margaritis, Honavar
(2006); contributions present paper include extending results conducting
extensive evaluation experimental theoretical properties. specifically,
contributions include extensive systematic experimental evaluation proposed algorithms (a) data sets sampled artificially generated networks varying
complexity strength dependences, well (b) data sets sampled networks
representing real-world domains, (c) formal proofs correctness guarantee
proposed algorithms compute correct Markov network structure domain,
stated assumptions.

3. Notation Preliminaries
denote random variables capitals (e.g., X, Y, Z) sets variables bold
capitals (e.g., X, Y, Z). particular, denote V = {0, . . . , n 1} set n
variables domain. name variables indices V; instance,
refer third variable V simply 3. denote data set size
(number data points) |D| N . use notation (XY | Z) denote
proposition X independent conditioned Z, disjoint sets variables X,
Y, Z. (X 6Y | Z) denotes conditional dependence. use (XY | Z) shorthand
({X}{Y } | Z) improve readability.
452

fiEfficient Markov Network Structure Discovery Using Independence Tests

Markov network undirected graphical model represents joint probability
distribution V. node graph represents one random variables
domain, absences edges encode conditional independences among them.
assume underlying probability distribution graph-isomorph (Pearl, 1988) faithful
(Spirtes et al., 2000), means faithful undirected graph. graph G
said faithful distribution graph connectivity represents exactly
dependencies independences existent distribution. detail, means
disjoint sets X, Y, Z V, X independent given Z set
vertices Z separates set vertices X set vertices graph G (this
sometimes called global Markov property, Lauritzen, 1996). words, means
that, removing vertices Z G (including edges incident them),
exists (undirected) path remaining graph variable X
variable Y. example, Figure 1, set variables {0, 5} separates set {4, 6}
set {2}. generally, shown (Pearl, 1988; Theorem 2, page 94 definition
graph isomorphism, page 93) necessary sufficient condition distribution
graph-isomorph set independence relations satisfy following axioms
disjoint sets variables X, Y, Z, W individual variable :

(Symmetry)
(Decomposition)
(Intersection)
(Strong Union)
(Transitivity)

(XY | Z)
(XY W | Z)
(XY | Z W)
(XW | Z Y)
(XY | Z)
(XY | Z)




(YX | Z)
(XY | Z) (XW | Z)

=
=
=

(XY W | Z)
(XY | Z W)
(X | Z) (Y | Z)

(1)

operation algorithms also assume existence oracle
answer statistical independence queries. standard assumptions needed
formally proving correctness independence-based structure learning algorithms
(Spirtes et al., 2000).
3.1 Independence-Based Approach Structure Learning
GSMN GSIMN independence-based algorithms learning structure
Markov network domain. approach works evaluating number statistical
independence statements, reducing set structures consistent results
tests singleton (if possible), inferring structure possible one.
mentioned above, theory assume existence independence-query oracle
provide information conditional independences among domain variables.
viewed instance statistical query oracle (Kearns & Vazirani, 1994).
practice oracle exist; however, implemented approximately
statistical test evaluated data set D. example, discrete data
Pearsons conditional independence chi-square (2 ) test (Agresti, 2002), mutual
information test etc. continuous Gaussian data statistical test used
measure conditional independence partial correlation (Spirtes et al., 2000). determine
conditional independence two variables X given set Z data,
453

fiBromberg, Margaritis, & Honavar

statistical test returns p-value. p-value test equals probability obtaining
value test statistic least extreme one actually observed
given null hypothesis true, corresponds conditional independence
case. Assuming p-value test p(X, | Z), statistical test concludes
dependence p(X, | Z) less equal threshold i.e.,
(X 6Y | Z) p(X, | Z) .
quantity 1 sometimes referred tests confidence threshold. use
standard value = 0.05 experiments, corresponds confidence
threshold 95%.
faithful domain, shown (Pearl & Paz, 1985) edge exists
two variables X 6= V Markov network domain
dependent conditioned remaining variables domain, i.e.,
(X, ) edge iff (X 6Y | V {X, }).
Thus, learn structure, theoretically suffices perform n(n 1)/2 tests i.e.,
one test (X, | V {X, }) pair variables X, V, X 6= . Unfortunately,
non-trivial domains usually involves test conditions large number
variables. Large conditioning sets produce sparse contingency tables (count histograms)
result unreliable tests. number possible configurations
variables grows exponentially size conditioning setfor example,
2n cells test involving n binary variables, fill table one data point
per cell would need data set least exponential size i.e., N 2n . Exacerbating
problem, one data point per cell typically necessary reliable test:
recommended Cochran (1954), 20% cells contingency table
less 5 data points test deemed unreliable. Therefore GSMN
GSIMN algorithms (presented below) attempt minimize conditioning set size;
choosing order examining variables irrelevant variables
examined last.

4. Algorithms Related Concepts
section present main algorithms, GSMN GSIMN, supporting concepts required description. purpose aiding understanding
reader, discussing first describe abstract GSMN algorithm next
section. helps showing intuition behind algorithms laying foundation
them.
4.1 Abstract GSMN Algorithm
sake clarity exposition, discussing first algorithm GSMN ,
describe intuition behind describing general structure using abstract GSMN
algorithm deliberately leaves number details unspecified; filled-in
concrete GSMN algorithm, presented next section. Note choices

454

fiEfficient Markov Network Structure Discovery Using Independence Tests

Algorithm 1 GSMN algorithm outline: G = GSMN (V, D).
1: Initialize G empty graph.
2: variables X domain V
3:
/* Learn Markov Blanket BX X using GS algorithm. */
4:
BX GS (X, V, D)
5:
Add undirected edge G X variable BX .
6: return G

Algorithm 2 GS algorithm. Returns Markov Blanket BX variable X V: BX =
GS (X, V, D).
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

BX
/* Grow phase. */
variable V {X}
(X 6Y | BX ) (estimated using data D)
BX BX {Y }
goto 3 /* Restart grow loop. */
/* Shrink phase. */
variable BX
(XY | BX {Y }) (estimated using data D)
BX BX {Y }
goto 8 /* Restart shrink loop. */
return BX

details source optimizations reduce algorithms computational cost.
make explicit discuss concrete GSMN GSIMN algorithms.
abstract GSMN algorithm shown Algorithm 1. Given input data set
set variables V, GSMN computes set nodes (variables) BX adjacent
variable X V; completely determine structure domain MN.
algorithm consists main loop learns Markov blanket BX node
(variable) X domain using GS algorithm. constructs Markov network
structure connecting X variable BX .
GS algorithm first proposed Margaritis Thrun (2000) shown
Algorithm 2. consists two phases, grow phase shrink phase. grow phase
X proceeds attempting add variable current set hypothesized
neighbors X, contained BX , initially empty. BX grows variable
iteration grow loop X found dependent X
given current set hypothesized neighbors BX . Due (unspecified) ordering
variables examined (this explicitly specified concrete GSMN algorithm,
presented next section), end grow phase variables BX
might true neighbors X underlying MNthese called false positives.
justifies shrink phase algorithm, removes false positive BX
testing independence X conditioned BX {Y }. found independent
X shrink phase, cannot true neighbor (i.e., cannot edge X ),
GSMN removes BX . Assuming faithfulness correctness independence
query results, end shrink phase BX contains exactly neighbors X
underlying Markov network.
455

fiBromberg, Margaritis, & Honavar

next section present concrete implementation GSMN, called GSMN .
augments GSMN specifying concrete ordering variables X examined
main loop GSMN (lines 25 Algorithm 1), well concrete order
variables examined grow shrink phases GS algorithm (lines 36
811 Algorithm 2, respectively).
4.2 Concrete GSMN Algorithm
section discuss first algorithm, GSMN (Grow-Shrink Markov Network
learning algorithm), learning structure Markov network domain. Note
reason introducing GSMN addition main contribution, GSIMN
algorithm (presented later Section 4.5), comparison reasons. particular, GSIMN
GSMN identical structure, following order examination variables,
difference use inference GSIMN (see details subsequent
sections). Introducing GSMN therefore makes possible measure precisely (through
experimental results Section 5) benefits use inference performance.
GSMN algorithm shown Algorithm 3. structure similar abstract
GSMN algorithm. One notable difference order variables examined
specified; done initialization phase so-called examination order
grow order X variable X V determined. X priority
queues initially permutation V (X permutation V {X})
position variable queue denotes priority e.g., = [2, 0, 1] means
variable 2 highest priority (will examined first), followed 0 finally 1.
Similarly, position variable X determines order examined
grow phase X.
initialization phase algorithm computes strength unconditional
dependence pair variable X , given unconditional p-value
p(X, | ) independence test pair variables X 6= , denoted
pXY algorithm. (In practice logarithm p-values computed, allows
greater precision domains dependencies may strong weak.)
particular, algorithm gives higher priority (examines earlier) variables
lower average log p-value (line 5), indicating stronger dependence. average defined
as:
X
1
avg log(pXY ) =
log(pXY ).
|V| 1

6=X

grow order X variable X, algorithm gives higher priority variables
whose p-value (or equivalently log p-value) variable X small (line 8).
ordering due intuition behind folk-theorem (as Koller & Sahami, 1996,
puts it) states probabilistic influence association attributes tends
attenuate distance graphical model. suggests pair variables X
high unconditional p-value less likely directly linked. Note ordering
heuristic guaranteed hold general. example, may hold
underlying domain Bayesian network e.g., two spouses may independent
unconditionally dependent conditional common child. Note however
example apply faithful domains i.e., graph-isomorph Markov network. Also
456

fiEfficient Markov Network Structure Discovery Using Independence Tests

Algorithm 3 GSMN , concrete implementation GSMN: G = GSMN (V, D).
1:
2:
3:
4:
5:

Initialize G empty graph.
/* Initialization. */
X, V, X 6=
pXY p(X, | )


Initialize i, {0, . . . , n 1}, < avg log(pi j ) < avg log(pi j ) .
j

j

6: X V
7:
BX


8:
Initialize X j, j {0, . . . , n 1}, j < j pXX < pXX .
j

j

9:
Remove X X .
10: /* Main loop. */
11: empty
12:
X dequeue()
13:
/* Propagation phase. */
14:
{Y : examined X }
15:
F {Y : examined X
/ }
16:
T, move end X .
17:
F, move end X .
18:
/* Grow phase. */
19:

20:
X empty
21:
dequeue(X )
22:
pXY
23:
IGSMN (X, Y, S, F, T)
24:
{Y }
25:
/* Change grow order . */
26:
Move X beginning .
27:
W = S|S|2 S0
28:
Move W beginning .
29:
/* Change examination order. */
30:
W = S|S|1 S0
31:
W
32:
Move W beginning .
33:
break line 34
34:
/* Shrink phase. */
35:
= S|S|1 S0
36:
IGSMN (X, Y, {Y } , F, T)
37:
{Y }
38:
BX
39:
Add undirected edge G X variable BX .
40: return G

note correctness algorithms present depend holding i.e.,
prove Appendices B, GSMN GSIMN guaranteed return
correct structure assumptions stated Section 3 above. Also note
computational cost calculation pXY low due empty conditioning set.
remaining GSMN algorithm contains main loop (lines 1039)
variable V examined according examination order , determined
457

fiBromberg, Margaritis, & Honavar

Algorithm 4 IGSMN (X, Y, S, F, T): Calculate independence test (X, | S) propagation, possible, otherwise run statistical test data.
1:
2:
3:
4:
5:
6:
7:
8:
9:

/* Attempt infer dependence propagation. */

return false
/* Attempt infer independence propagation. */
F
return true
/* Else statistical test data. */
1(p(X,Y |Z)>) /* = true iff p-value statistical test (X, | S) > . */
return

initialization phase. main loop includes three phases: propagation phase (lines
1317), grow phase (lines 1833), shrink phase (lines 3437). propagation
phase optimization variables already computed
(i.e., variables already examined) collected two sets F T. Set F (T) contains
variables X
/ (X ). sets passed independence
procedure IGSMN , shown Algorithm 4, purpose avoiding execution
tests X algorithm. justified fact that, undirected
graphs, Markov blanket X X Markov blanket .
Variables already found contain X blanket (set F) cannot members
BX exists set variables rendered conditionally
independent X previous step, independence therefore inferred easily.
Note experiments section paper (Section 5) evaluate GSMN
without propagation phase, order measure effect propagation
optimization performance. Turning propagation accomplished simply setting
sets F (as computed lines 14 15, respectively) empty set.
Another difference GSMN abstract GSMN algorithm use condition pXY (line 22). additional optimization avoids independence test
case X found (unconditionally) independent initialization
phase, since case would imply X independent given conditioning
set axiom Strong Union.
crucial difference GSMN abstract GSMN algorithm GSMN
changes examination order grow order every variable X . (Since
X
/ X , excludes grow order X itself.) changes ordering proceed
follows: end grow phase variable X, new examination order (set
lines 3033) dictates next variable W examined X last
added growing phase yet examined (i.e., W still ).
grow order variables found dependent X also changed; done
maximize number optimizations GSIMN algorithm (our main contribution
paper) shares algorithm structure GSMN . changes grow order
therefore explained detail Section 4.5 GSIMN presented.
final difference GSMN abstract GSMN algorithm restart
actions grow shrink phases GSMN whenever current Markov blanket
modified (lines 6 11 Algorithm 2), present GSMN . restarting

458

fiEfficient Markov Network Structure Discovery Using Independence Tests

Figure 2: Illustration operation GSMN using independence graph. figure
shows growing phase variable 5. Variables examined according
grow order 5 = [3, 4, 1, 6, 2, 7, 0].

loops necessary GS algorithm due original usage learning
structure Bayesian networks. task, possible true member
blanket X found initially independent grow loop conditioning
set found dependent later conditioned superset S.
could happen unshielded spouse X i.e., one common
children X existed direct link X underlying Bayesian
network. However, behavior impossible domain distribution faithful
Markov network (one assumptions): independence X given
must hold superset axiom Strong Union (see Eqs. (1)).
restart grow shrink loops therefore omitted GSMN order save
unnecessary tests. Note that, even though possible behavior impossible
faithful domains, possible unfaithful ones, also experimentally evaluated
algorithms real-world domains assumption Markov faithfulness may
necessarily hold (Section 5).
proof correctness GSMN presented Appendix A.
4.3 Independence Graphs
demonstrate operation GSMN graphically concept independence
graph, introduce. define independence graph undirected
graph conditional independences dependencies single variables
represented one annotated edges them. solid (dotted) edge
variables X annotated Z represents fact X found
dependent (independent) given Z. conditioning set Z enclosed parentheses
edge represents independence dependence inferred Eqs. (1) (as
opposed computed statistical tests). Shown graphically:
459

fiBromberg, Margaritis, & Honavar

X
X
X
X

Z


(X 6Y | Z)



(XY | Z)



(X 6Y | Z) (inferred)



(XY | Z) (inferred)

Z
(Z)
(Z)

instance, Figure 2, dotted edge 5 1 annotated 3, 4 represents
fact (51 | {3, 4}). absence edge two variables indicates
absence information independence dependence variables
conditioning set.
Example 1. Figure 2 illustrates operation GSMN using independence graph
domain whose underlying Markov network shown Figure 1. figure shows
independence graph end grow phase variable 5, first examination
order . (We discuss example initialization phase GSMN ; instead,
assume examination () grow () orders shown figure.) According
vertex separation underlying network (Figure 1), variables 3, 4, 6, 7 found
dependent 5 growing phase i.e.,
I(5, 3 | ),
I(5, 4 | {3}),
I(5, 6 | {3, 4}),
I(5, 7 | {3, 4, 6})
therefore connected 5 independence graph solid edges annotated sets
, {3}, {3, 4} {3, 4, 6} respectively. Variables 1, 2, 0 found independent i.e.,
I(5, 1 | {3, 4}),
I(5, 2 | {3, 4, 6}),
I(5, 0 | {3, 4, 6, 7})
thus connected 5 dotted edges annotated {3, 4}, {3, 4, 6} {3, 4, 6, 7}
respectively.
4.4 Triangle Theorem
section present prove theorem used subsequent GSIMN
algorithm. seen, main idea behind GSIMN algorithm attempt decrease number tests done exploiting properties conditional independence
relation faithful domains i.e., Eqs. (1). properties seen inference rules
used derive new independences ones know true. careful
study axioms suggests two simple inference rules, stated Triangle
theorem below, sufficient inferring useful independence information
inferred systematic application inference rules. confirmed
experiments Section 5.
460

fiEfficient Markov Network Structure Discovery Using Independence Tests

Figure 3: Independence graph depicting Triangle theorem. Edges graph
labeled sets represent conditional independences dependencies. solid
(dotted) edge X labeled Z means X dependent
(independent) given Z. set label enclosed parentheses means edge
inferred theorem.

Theorem 1 (Triangle theorem). Given Eqs. (1), every variable X, , W sets Z1
Z2 {X, Y, W } Z1 = {X, Y, W } Z2 = ,
(X 6W | Z1 ) (W 6Y | Z2 )

=

(X 6Y | Z1 Z2 )

(XW | Z1 ) (W 6Y | Z1 Z2 )

=

(XY | Z1 ).

call first relation D-triangle rule second I-triangle rule.
Proof. using Strong Union Transitivity Eqs. (1) shown contrapositive form.
(Proof D-triangle rule):
Strong Union (X 6W | Z1 ) get (X 6W | Z1 Z2 ).
Strong Union (W 6Y | Z1 ) get (W 6Y | Z1 Z2 ).
Transitivity, (X 6W | Z1 Z2 ), (W 6Y | Z1 Z2 ), get (X 6Y | Z1 Z2 ).
(Proof I-triangle rule):
Strong Union (W 6Y | Z1 Z2 ) get (W 6Y | Z1 ).
Transitivity, (XW | Z1 ) (W 6Y | Z1 ) get (XY | Z1 ).

represent Triangle theorem graphically using independence graph construct Section 4.2. Figure 3 depicts two rules Triangle theorem using two
independence graphs.
Triangle theorem used infer additional conditional independences
tests conducted operation GSMN . example shown Figure 4, illustrates application Triangle theorem example presented
Figure 2. independence information inferred Triangle theorem shown
curved edges (note conditioning set edge enclosed parentheses).

461

fiBromberg, Margaritis, & Honavar

Figure 4: Illustration use Triangle theorem example Figure 2. set
variables enclosed parentheses correspond tests inferred Triangle
theorem using two adjacent edges antecedents. example, result
(17 | {3, 4}), inferred I-triangle rule, independence (51 | {3, 4})
dependence (5 67 | {3, 4, 6}).

example, independence edge (4, 7) inferred D-triangle rule adjacent edges (5, 4) (5, 7), annotated {3} {3, 4, 6} respectively. annotation
inferred edge {3}, intersection annotations {3} {3, 4, 6}.
example application I-triangle rule edge (1, 7), inferred edges
(5, 1) (5, 7) annotations {3, 4} {3, 4, 6} respectively. annotation
inferred edge {3, 4}, intersection annotations {3, 4, 6} {3, 4}.
4.5 GSIMN Algorithm
previous section saw possibility using two rules Triangle
theorem infer result novel tests grow phase. GSIMN algorithm
(Grow-Shrink Inference-based Markov Network learning algorithm), introduced section, uses Triangle theorem similar fashion extend GSMN inferring value
number tests GSMN executes, making evaluation unnecessary. GSIMN
GSMN work exactly way (and thus GSIMN algorithm shares exactly
algorithmic description i.e., follow Algorithm 3), differences
concentrated independence procedure use: instead using independence
procedure IGSMN GSMN , GSIMN uses procedure IGSIMN , shown Algorithm 5. Procedure IGSIMN , addition attempting propagate blanket information obtained
examination previous variables (as IGSMN does), also attempts infer
value independence test provided input either Strong Union
axiom (listed Eqs. (1)) Triangle theorem. attempt successful, IGSIMN
returns value inferred (true false), otherwise defaults statistical test
data set (as IGSMN does). purpose assisting inference process, GSIMN
462

fiEfficient Markov Network Structure Discovery Using Independence Tests

Algorithm 5 IGSIMN (X, Y, S, F, T): Calculate independence test result inference (including propagation), possible. Record test result knowledge base.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:

/* Attempt infer dependence propagation. */

return false
/* Attempt infer independence propagation. */
F
return true
/* Attempt infer dependence Strong Union. */
(A, false) KXY
return false
/* Attempt infer dependence D-triangle rule. */
W
(A, false) KXW (B, false) KW B
Add (A B, false) KXY KY X .
return false
/* Attempt infer independence Strong Union. */
(A, true) KXY
return true
/* Attempt infer independence I-triangle rule. */
W
(A, true) KXW s.t. (B, false) KW s.t. B
Add (A, true) KXY KY X .
return true
/* Else statistical test data. */
1(p(X,Y |Z)>) /* = true iff p-value statistical test (X, | S) > . */
Add (S, t) KXY KY X .
return

IGSIMN maintain knowledge base KXY pair variables X , containing
outcomes tests evaluated far X (either data inferred).
knowledge bases empty beginning GSIMN algorithm (the initialization step shown algorithm since GSMN use it), maintained
within test procedure IGSIMN .
explain IGSIMN (Algorithm 5) detail. IGSIMN attempts infer independence value input triplet (X, | S) applying single step backward
chaining using Strong Union Triangle rules i.e., searches knowledge base
K = {KXY : X, V} antecedents instances rules input triplet
(X, | S) consequent. Strong Union rule used direct shown
Eqs. (1) also contrapositive form. direct form used infer independences, therefore refer I-SU rule on. contrapositive form,
I-SU rule becomes (X 6Y | W) = (X 6Y | S), referred D-SU rule
since used infer dependencies. According D-Triangle D-SU rules,
dependence (X 6Y | S) inferred knowledge base K contains
1. test (X 6Y | A) S,
2. tests (X 6W | A) (W 6Y | B) variable W , B S,
463

fiBromberg, Margaritis, & Honavar

Figure 5: Illustration operation GSIMN. figure shows grow phase two
consecutively examined variables 5 7. figure shows variable
examined second 3 7, according change examination order
lines 3033 Algorithm 3. set variables enclosed parentheses
correspond tests inferred Triangle theorem using two adjacent edges
antecedents. results (7 63 | ), (7 64 | {3}), (7 66 | {3, 4}), (7 65 |
{3, 4, 6}) (b), shown highlighted, executed inferred tests
done (a).

respectively. According I-Triangle I-SU rules, independence (XY | S)
inferred knowledge base contains
3. test (XY | A) S,
4. tests (XW | A) (W 6Y | B) variable W , B A,
respectively.
changes grow orders variables occur inside grow phase
currently examined variable X (lines 2528 GSIMN i.e., Algorithm 3 IGSMN replaced IGSIMN .). particular, if, variable , algorithm reaches line 24,
i.e., pXY IGSIMN (X, Y, S) = false, X variables found
dependent X (i.e., variables currently S) promoted beginning
grow order . illustrated Figure 5 variable 7, depicts grow
phase two consecutively examined variables 5 7. figure, curved edges
show tests inferred IGSIMN grow phase variable 5. grow
order 7 changes 7 = [2, 6, 3, 0, 4, 1, 5] 7 = [3, 4, 6, 5, 2, 0, 1] grow phase
variable 5 complete variables 5, 6, 4 3 promoted (in order)
beginning queue. rationale observation increases
number tests inferred GSIMN next step: change examination
grow orders described chosen inferred tests learning
blanket variable 7 match exactly required algorithm future step.
464

fiEfficient Markov Network Structure Discovery Using Independence Tests

particular, note example set inferred dependencies variable
found dependent 5 7 exactly required initial part grow
phase variable 7, shown highlighted Figure 5(b) (the first four dependencies).
independence tests inferred (not conducted), resulting computational savings.
general, last dependent variable grow phase X maximum number
dependences independences inferred provides rationale change
grow order selection algorithm examined next.
shown assumptions GSMN , structure returned
GSIMN correct one i.e., set BX computed GSIMN algorithm equals
exactly neighbors X. proof correctness GSIMN based correctness
GSMN presented Appendix B.
4.6 GSIMN Technical Implementation Details
section discuss number practical issues subtly influence accuracy
efficiency implementation GSIMN. One order application I-SU, D-SU,
I-Triangle D-Triangle rules within function IGSIMN . Given independence-query
oracle, order application matterassuming one rules
inferring value independence, guaranteed produce
value due soundness axioms Eqs. (1) (Pearl, 1988). practice however,
oracle implemented statistical tests conducted data incorrect,
previously mentioned. particular importance observation false independences
likely occur false dependencies. One example case
domain dependencies weakin case pair variables connected (dependent)
underlying true network structure may incorrectly deemed independent paths
long enough. hand, false dependencies much rare
confidence threshold 1 = 0.95 statistical test tells us probability
false dependence chance alone 5%. Assuming i.i.d. data test, chance
multiple false dependencies even lower, decreasing exponentially fast. practical
observation i.e., dependencies typically reliable independences, provide
rationale way IGSIMN algorithm works. particular, IGSIMN prioritizes
application rules whose antecedents contain dependencies first i.e., D-Triangle
D-SU rules, followed I-Triangle I-SU rules. effect, uses statistical results
typically known greater confidence ones usually less reliable.
second practical issue concerns efficient inference. GSIMN algorithm uses onestep inference procedure (shown Algorithm 5) utilizes knowledge base K = {KXY }
containing known independences dependences pair variables X .
implement inference efficiently utilize data structure K purpose
storing retrieving independence facts constant time. consists two 2D arrays,
one dependencies another independencies. array n n size, n
number variables domain. cell array corresponds pair
variables (X, ), stores known independences (dependences) X
form list conditioning sets. conditioning set Z list, knowledge
base KXY represents known independence (XY | Z) (dependence (X 6Y | Z)).
important note length list 2, two

465

fiBromberg, Margaritis, & Honavar

tests done variable X execution GSIMN (done
growing shrinking phases). Thus, always takes constant time retrieve/store
independence (dependence), therefore inferences using knowledge base
constant time well. Also note uses Strong Union axion IGSIMN
algorithm constant time well, accomplished testing (at
two) sets stored KXY subset superset inclusion.

5. Experimental Results
evaluated GSMN GSIMN algorithms artificial real-world data sets.
experimental results presented show simple application
Pearls inference rules GSIMN algorithm results significant reduction number
tests performed compared GSMN without adversely affecting quality
output network. particular report following quantities:
Weighted number tests. weighted number tests computed
summation weight test executed, weight test (X, | Z)
defined 2+|Z|. quantity reflects time complexity algorithm (GSMN
GSIMN) used assess benefit GSIMN using inference instead
executing statistical tests data. standard method comparison
independence-based algorithms justified observation running
time statistical test triplet (X, | Z) proportional size N data
set number variables involved i.e., O(N (|Z|+2)) (and exponential
number variables involved nave implementation might assume).
one construct non-zero entries contingency table used
test examining data point data set exactly once, time proportional
number variables involved test i.e., proportional |{X, }Z| = 2+|Z|.
Execution time. order assess impact inference running time
(in addition impact statistical tests), report execution time
algorithm.
Quality resulting network. measure quality two ways.
Normalized Hamming distance. Hamming distance output
network structure underlying model another measure
quality output network, actual network used generate
data known. Hamming distance defined number reversed
edges two network structures, i.e., number times actual
edge true network missing returned network edge absent
true network exists algorithms output network. value zero
means output network correct structure. able compare
domains
different dimensionalities (number variables n) normalize

n
2 , total number node pairs corresponding domain.
Accuracy. real-world data sets underlying network unknown,
Hamming distance calculation possible. case impossible know
true value independence. therefore approximate statistical
test entire data set, use limited, randomly chosen subset (1/3
data set) learn network. measure accuracy compare result
466

fiEfficient Markov Network Structure Discovery Using Independence Tests

(true false) number conditional independence tests network
output (using vertex separation), tests performed full data
set.
experiments involving data sets used 2 statistical test estimation
conditional independences. mentioned above, rules thumb exist deem certain
tests potentially unreliable depending counts contingency table involved;
example, one rule Cochran (1954) deems test unreliable 20%
cells contingency table less 5 data points test. Due requirement
answer must obtained independence algorithm conducting test, used
outcomes tests well experiments. effect possibly unreliable
tests quality resulting network measured accuracy measures, listed
above.
next section present results domains underlying probabilistic
model known. followed real-world data experiments model structure
available.
5.1 Known-Model Experiments
first set experiments underlying model, called true model true network,
known Markov network. purpose set experiments conduct controlled
evaluation quality output network systematic study algorithms
behavior varying conditions domain size (number variables) amount
dependencies (average node degree network).
true network contains n variables generated randomly follows:
network initialized n nodes edges. user-specified parameter
network structure average node degree equals average number neighbors
per node. Given , every node set neighbors determined randomly
uniformly selecting first n2 pairs random permutation possible pairs.
factor 1/2 necessary edge contributes degree two nodes.
conducted two types experiments using known network structure: Exact learning
experiments sample-based experiments.
5.1.1 Exact Learning Experiments
set known-model experiments, assume result statistical queries
asked GSMN GSIMN algorithms available, assumes existence
oracle answer independence queries. underlying model known,
oracle implemented vertex separation. benefits querying
true network independence two: First, ensures faithfulness correctness
independence query results, allows evaluation algorithms
assumptions correctness. Second, tests performed much faster actual
statistical tests data. allowed us evaluate algorithms large networkswe
able conduct experiments domains containing 100 variables.
first report weighted number tests executed GSMN without
propagation GSIMN. results summarized Figure 6, shows ratio
weighted number tests GSIMN two versions GSMN . One
467

fiWC(GSIMN) / WC(GSMN* propagation)

Ratio weighted cost GSIMN vs. GSMN* without propagation
1
0.9
0.8
0.7
0.6
=1
=2
=4
=8

0.5
0.4
0.3
0.2
0.1
0
0

10

20

30
40
50
60
70
80
Domain size (number variables)

90

100

WC(GSIMN) / WC(GSMN* without propagation)

Bromberg, Margaritis, & Honavar

Ratio weighted cost GSIMN vs. GSMN* propagation
1
0.9
0.8
0.7
0.6
0.5
0.4
=1
=2
=4
=8

0.3
0.2
0.1
0
0

10

20

30
40
50
60
70
80
Domain size (number variables)

90

100

Figure 6: Ratio weighted number tests GSIMN GSMN without propagation (left plot) propagation (right plot) network sizes (number
nodes) n = 100 average degree = 1, 2, 4, 8.
Algorithm 6 IFCH (X, Y, S, F, T). Forward-chaining implementation independence test
IGSIMN (X, Y, S, F, T).
1:
2:
3:
4:
5:
6:
7:

/* Query knowledge base. */
(S, t) KXY
return
result test (X, | S) /* = true iff test (X, | S) returns independence. */
Add (S, t) KXY KY X .
Run forward-chaining inference algorithm K, update K.
return

hundred true networks generated randomly pair (n, ), figure shows
mean value. see limiting reduction (as n grows large) weighted
number tests depends primarily average degree parameter . reduction
GSIMN large n dense networks ( = 8) approximately 40% compared GSMN
propagation 75% compared GSMN without propagation optimization,
demonstrating benefit GSIMN vs. GSMN terms number tests executed.
One reasonable question performance GSIMN extent inference
procedure complete i.e., tests GSIMN needs operation,
number tests infers (by applying single step backward chaining
Strong Union axiom Triangle theorem, rather executing statistical test
data) compare number tests inferred (for example using complete
automated theorem prover Eqs. (1))? measure this, compared number tests
done GSIMN number done alternative algorithm, call GSIMNFCH (GSIMN Forward Chaining). GSIMN-FCH differs GSIMN function
IFCH , shown Algorithm 6, replaces function IGSIMN GSIMN. IFCH exhaustively
produces independence statements inferred properties Eqs. (1)
using forward-chaining procedure. process iteratively builds knowledge base K
containing truth value conditional independence predicates. Whenever outcome
test required, K queried (line 2 IFCH Algorithm 6). value test
468

fiEfficient Markov Network Structure Discovery Using Independence Tests

Ratio Number tests GSIMN-FCH GSIMN

=1
=2
=4
=8

1.4
1.2

Ratio

1
0.8
0.6
0.4
0.2
0

2

3

4

5

6

7

8

9

10 11 12

Number variables (n)

Figure 7: Ratio number tests GSIMN-FCH GSIMN network sizes (number
variables) n = 2 n = 13 average degrees = 1, 2, 4, 8.

found K, returned (line 3). not, GSIMN-FCH performs test uses result
standard forward-chaining automatic theorem prover subroutine (line 6) produce
independence statements inferred test result K, adding new
facts K.
comparison number tests executed GSIMN vs. GSIMN-FCH presented
Figure 7, shows ratio number tests GSIMN GSIMN-FCH.
figure shows mean value four runs, corresponding network generated
randomly pair (n, ), = 1, 2, 4 8 n 12. Unfortunately, two
days execution GSIMN-FCH unable complete execution domains containing
13 variables more. therefore present results domain sizes 12 only.
figure shows n 9, every ratio exactly 1 i.e., tests inferable
produced use Triangle theorem GSIMN. smaller domains, ratio
0.95 exception single case, (n = 5, = 1).
5.1.2 Sample-based Experiments
set experiments evaluate GSMN (with without propagation) GSIMN
data sampled true model. allows realistic assessment
performance algorithms. data sampled true (known) Markov
network using Gibbs sampling.
exact learning experiments previous section structure true
network required, generated randomly fashion described above. sample data
known structure however, one also needs specify network parameters.
random network, parameters determine strength dependencies among connected
variables graph. Following Agresti (2002), used log-odds ratio measure
strength probabilistic influence two binary variables X , defined

Pr(X = 0, = 0) Pr(X = 1, = 1)
XY = log
.
Pr(X = 0, = 1) Pr(X = 1, = 0)

469

fiBromberg, Margaritis, & Honavar

Hamming distance sampled data
n = 50, = 1, = 1.5

GSMN* without propagation
GSMN* propagation
GSIMN

0.6
0.4
0.2
0
4

6

8

10

12

14

16

18

0.4
0.2
0

20

0

2

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

0.6
0.4
0.2
0
2

4

6

8

10

12

14

16

18

0.2
0
10

12

14

16

18

2

0.2
0
10

12

14

16

18

Data set size (thousands data points)

8

10

12

14

16

18

0.2
0
6

8

10

12

14

16

18

20

0.4
0.2
0
4

6

8

10

12

14

16

18

Data set size (thousands data points)

16

18

20

0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.6
0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

Hamming distance sampled data
n = 50, = 8, = 2.0

0.6

2

14

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

0

12

0.6

20

1
0.8

10

Hamming distance sampled data
n = 50, = 4, = 2.0

0.4

4

8

Data set size (thousands data points)

0.6

2

6

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

20

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

0

Normalized Hamming distance

Normalized Hamming distance

0.4

8

6

4

1

Hamming distance sampled data
n = 50, = 8, = 1.5

0.6

6

2

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

4

4

1

Hamming distance sampled data
n = 50, = 8, = 1.0

2

0

Hamming distance sampled data
n = 50, = 2, = 2.0

0

20

1

0

0

Data set size (thousands data points)

0.2

Data set size (thousands data points)

0.8

0.2

20

0.4

0

Normalized Hamming distance

Normalized Hamming distance

0.4

8

18

0.4

Hamming distance sampled data
n = 50, = 4, = 1.5

0.6

6

16

0.6

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

4

14

0.6

Hamming distance sampled data
n = 50, = 4, = 1.0

2

12

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

20

1

0

10

1

Data set size (thousands data points)

0.8

8

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

Hamming distance sampled data
n = 50, = 2, = 1.5
Normalized Hamming distance

Normalized Hamming distance

Hamming distance sampled data
n = 50, = 2, = 1.0

0

6

1

Data set size (thousands data points)

1
0.8

4

Normalized Hamming distance

2

0.6

Normalized Hamming distance

0

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

Normalized Hamming distance

0.8

Hamming distance sampled data
n = 50, = 1, = 2.0

1

Normalized Hamming distance

Normalized Hamming distance

Normalized Hamming distance

Hamming distance sampled data
n = 50, = 1, = 1.0
1

20

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.6
0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

Data set size (thousands data points)

Figure 8: Normalized Hamming distances true network network output
GSMN (with without propagation) GSIMN domain size n = 50
average degrees = 1, 2, 4, 8.

network parameters generated randomly log-odds ratio every
pair variables connected edge graph specified value. set
experiments, used values = 1, = 1.5 = 2 every pair variables
network.
Figures 8 9 show plots normalized Hamming distance true
network output GSMN (with without propagation) GSIMN
domain sizes n = 50 n = 75 variables, respectively. plots show
Hamming distance GSIMN comparable ones GSMN algorithms

470

fiEfficient Markov Network Structure Discovery Using Independence Tests

Hamming distance sampled data
n = 75, = 1, = 1.5

GSMN* without propagation
GSMN* propagation
GSIMN

0.6
0.4
0.2
0
4

6

8

10

12

14

16

18

0.4
0.2
0

20

0

2

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

0.6
0.4
0.2
0
2

4

6

8

10

12

14

16

18

0.2
0
10

12

14

16

18

2

0.2
0
10

12

14

16

18

Data set size (thousands data points)

8

10

12

14

16

18

0.2
0
6

8

10

12

14

16

18

20

0.4
0.2
0
4

6

8

10

12

14

16

18

Data set size (thousands data points)

16

18

20

0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.6
0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

Hamming distance sampled data
n = 75, = 8, = 2.0

0.6

2

14

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

0

12

0.6

20

1
0.8

10

Hamming distance sampled data
n = 75, = 4, = 2.0

0.4

4

8

Data set size (thousands data points)

0.6

2

6

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

20

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

0

Normalized Hamming distance

Normalized Hamming distance

0.4

8

6

4

1

Hamming distance sampled data
n = 75, = 8, = 1.5

0.6

6

2

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

4

4

1

Hamming distance sampled data
n = 75, = 8, = 1.0

2

0

Hamming distance sampled data
n = 75, = 2, = 2.0

0

20

1

0

0

Data set size (thousands data points)

0.2

Data set size (thousands data points)

0.8

0.2

20

0.4

0

Normalized Hamming distance

Normalized Hamming distance

0.4

8

18

0.4

Hamming distance sampled data
n = 75, = 4, = 1.5

0.6

6

16

0.6

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

4

14

0.6

Hamming distance sampled data
n = 75, = 4, = 1.0

2

12

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

20

1

0

10

1

Data set size (thousands data points)

0.8

8

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

Hamming distance sampled data
n = 75, = 2, = 1.5
Normalized Hamming distance

Normalized Hamming distance

Hamming distance sampled data
n = 75, = 2, = 1.0

0

6

1

Data set size (thousands data points)

1
0.8

4

Normalized Hamming distance

2

0.6

Normalized Hamming distance

0

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

Normalized Hamming distance

0.8

Hamming distance sampled data
n = 75, = 1, = 2.0

1

Normalized Hamming distance

Normalized Hamming distance

Normalized Hamming distance

Hamming distance sampled data
n = 75, = 1, = 1.0
1

20

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.6
0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

Data set size (thousands data points)

Figure 9: Normalized Hamming distance results Figure 8 domain size n = 75.

domain sizes n = 50 n = 75, average degrees = 1, 2, 4, 8 log-odds ratios = 1,
= 1.5 = 2. reinforces claim inference done GSIMN small
impact quality output networks.
Figure 10 shows weighted number tests GSIMN vs. GSMN (with without
propagation) sampled data set 20,000 points domains n = 50, n = 75,
average degree parameters = 1, 2, 4, 8 log-odds ratios = 1, 1.5 2. GSIMN
shows reduced weighted number tests respect GSMN without propagation
cases compared GSMN propagation cases (with exceptions
( = 4, = 2) ( = 8, = 1.5)). sparse networks weak dependences i.e.,
= 1, reduction larger 50% domain sizes, reduction much larger

471

fiBromberg, Margaritis, & Honavar

Weighted cost sampled data
= 1, = 1.0, 20,000 data points

Weighted cost sampled data
= 1, = 1.5, 20,000 data points

200000
150000
100000
50000
0

250000

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

50

75

Weighted cost sampled data
= 2, = 1.0, 20,000 data points

100000
50000
0

250000

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000

75

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000

75

50

Weighted cost sampled data
= 4, = 1.5, 20,000 data points

Weighted cost sampled data
= 4, = 2.0, 20,000 data points

200000
150000
100000
50000
0

250000

300000
GSMN* without propagation
GSMN* propagation
GSIMN

Weighted number tests

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

75

250000

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

50

Number variables

75

50

Number variables

Weighted cost sampled data
= 8, = 1.0, 20,000 data points

Weighted cost sampled data
= 8, = 2.0, 20,000 data points

300000
Weighted number tests

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

250000

300000
GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

75

75
Number variables

Weighted cost sampled data
= 8, = 1.5, 20,000 data points

300000

75
Number variables

300000
Weighted number tests

Weighted number tests

250000

Number variables

Weighted cost sampled data
= 4, = 1.0, 20,000 data points

Weighted number tests

Weighted cost sampled data
= 2, = 2.0, 20,000 data points

0
50

300000

75

300000

Number variables

Number variables

50000

Number variables

0

50

100000

50

Weighted number tests

150000

250000

150000

Weighted cost sampled data
= 2, = 1.5, 20,000 data points
Weighted number tests

Weighted number tests

GSMN* without propagation
GSMN* propagation
GSIMN

50

200000

75

300000

200000

250000

GSMN* without propagation
GSMN* propagation
GSIMN

Number variables

300000

50

250000

0
50

Number variables

250000

Weighted number tests

GSMN* without propagation
GSMN* propagation
GSIMN

300000

Weighted number tests

250000

Weighted cost sampled data
= 1, = 2.0, 20,000 data points

300000
Weighted number tests

Weighted number tests

300000

250000

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

50

75
Number variables

50

75
Number variables

Figure 10: Weighted number tests executed GSMN (with without propagation)
GSIMN |D| = 20, 000, domains sizes n = 50 75, average degree
parameters = 1, 2, 4, 8, log-odds ratios = 1, 1, 5, 2.

one observed exact learning experiments. actual execution times
various data set sizes network densities shown Figure 11 largest domain
n = 75, = 1, verifying reduction cost GSIMN various data set sizes.
Note reduction proportional number data points; reasonable
test executed must go entire data set construct contingency table.
confirms claim cost inference GSIMN small (constant time per
test, see discussion Section 4.6) compared execution time tests themselves,
indicates increasing cost benefits use GSIMN even large data sets.

472

fiEfficient Markov Network Structure Discovery Using Independence Tests

Execution times sampled data sets
n = 75 variables, = 1, = 1

Execution times sampled data sets
n = 75 variables, = 2, = 1

300

300
GSMN* without propagation
GSMN* propagation
GSIMN

GSMN* without propagation
GSMN* propagation
GSIMN

250
Execution time (sec)

Execution time (sec)

250
200
150
100
50

200
150
100
50

0

0
0

2000 4000 6000 8000 10000 12000 14000 16000 18000 20000

0

Execution times sampled data sets
n = 75 variables, = 4, = 1

Execution times sampled data sets
n = 75 variables, = 8, = 1

300

300
GSMN* without propagation
GSMN* propagation
GSIMN

GSMN* without propagation
GSMN* propagation
GSIMN

250
Execution time (sec)

250
Execution time (sec)

2000 4000 6000 8000 10000 12000 14000 16000 18000 20000

200
150
100
50

200
150
100
50

0

0
0

2000 4000 6000 8000 10000 12000 14000 16000 18000 20000

0

2000 4000 6000 8000 10000 12000 14000 16000 18000 20000

Figure 11: Execution times sampled data experiments = 1, = 1, 2 (top row)
= 4, 8 (bottom row) domain n = 75 variables.

5.1.3 Real-World Network Sampled Data Experiments
also conducted sampled data experiments well-known real-world networks.
known repository Markov networks drawn real-world domains, instead
utilized well-known Bayesian networks widely used Bayesian network research
available number repositories.1 generate Markov networks
Bayesian network structures used process moralization (Lauritzen, 1996)
consists two steps: (a) connect pair nodes Bayesian network
common child undirected edge (b) remove directions edges.
results Markov network local Markov property valid i.e., node
conditionally independent nodes domain given direct neighbors.
procedure conditional independences may lost. This, however, affect
accuracy results compare independencies output network
moralized Markov network (as opposed Bayesian network).
conducted experiments using 5 real-world domains: Hailfinder, Insurance, Alarm,
Mildew, Water. domain sampled varying number data points
corresponding Bayesian network using logic sampling (Henrion, 1988), used input
GSMN (with without propagation) GSIMN algorithms. compared
network output algorithms original moralized network using
normalized Hamming distance metric previously described. results shown
1. used http://compbio.cs.huji.ac.il/Repository/. Accessed December 5, 2008.

473

fiBromberg, Margaritis, & Honavar

Hamming distance hailfinder data set

Hamming distance insurance data set

Hamming distance alarm data set

GSMN* without propagation
GSMN* propagation
GSIMN

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

2

4

6

8

10 12 14 16

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

18 20 22

0

Data set size (thousands data points)

2

4

6

8

10 12 14 16

Normalized Hamming distance

Normalized Hamming distance

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

18 20 22

0

2

4

6

8

10 12 14 16

18 20 22

Data set size (thousands data points)

Hamming distance Water data set

GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.7

GSMN* without propagation
GSMN* propagation
GSIMN

0.9
0.8

Data set size (thousands data points)

Hamming distance Mildew data set
1
0.9

Normalized Hamming distance

1
Normalized Hamming distance

Normalized Hamming distance

1

0.6
0.5
0.4
0.3
0.2
0.1
0

1
0.9

GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

2

4

6

8

10 12 14 16

18 20 22

0

Data set size (thousands data points)

2

4

6

8

10 12 14 16

18 20 22

Data set size (thousands data points)

Figure 12: Normalized Hamming distance network output GSMN (with
without propagation) GSIMN true Markov networks network using
varying data set sizes sampled Markov networks various real-world
domains modeled Bayesian networks.

Fig. 12 indicate distances produced three algorithms similar.
cases (e.g., Water Hailfinder) network resulting use GSIMN
actually better (of smaller Hamming distance) ones output GSMN
algorithms.
also measured weighted cost three algorithms domains,
shown Fig. 13. plots show significant decrease weighted number tests
GSIMN respect GSMN algorithms: cost GSIMN 66% cost
GSMN without propagation average, savings 34%, cost GSIMN 28%
cost GSMN without propagation average, savings 72%.
5.2 Real-World Data Experiments
artificial data set studies previous section advantage allowing
controlled systematic study performance algorithms, experiments
real-world data necessary realistic assessment performance. Real data
challenging may come non-random topologies (e.g., possibly
irregular lattice many cases spatial data) underlying probability distribution
may faithful.
conducted experiments number data sets obtained UCI machine
learning data set repository (Newman, Hettich, Blake, & Merz, 1998). Continuous variables
data sets discretized using method widely recommended introductory statistics texts (Scott, 1992); dictates optimal number equally-spaced discretization
bins continuous variable k = 1 + log2 N , N number points
474

fiEfficient Markov Network Structure Discovery Using Independence Tests

GSMN* without propagation
GSMN* propagation
GSIMN

70000
60000
50000
40000
30000
20000

GSMN* without propagation
GSMN* propagation
GSIMN

8000

Weighted cost tests

Weighted cost tests

80000

Weighted cost tests insurance data set
9000
7000
5000
4000
3000
2000

14000
12000
10000
8000
6000
4000
2000

1000

0

0
0

5

10

15

20

0
0

5

Data set size (thousands data points)

10

15

0

5

10

15

GSMN* without propagation
GSMN* propagation
GSIMN

25000

8000
6000
4000
2000

20000
15000
10000
5000

0

0
0

5

10

15

20

0

Data set size (thousands data points)

5

10

15

20

Data set size (thousands data points)

Figure 13: Weighted cost tests conducted GSMN (with without propagation)
GSIMN algorithms various real-world domains modeled Bayesian
networks.
Weighted cost accuracy real-world data sets
1
acc(GSIMN) - acc(GSMN* without propagation)
acc(GSIMN) - acc(GSMN* propagation)
wc(GSIMN) / wc(GSMN* without propagation)
wc(GSIMN) / wc(GSMN* propagation)

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
-0.1
-0.2

19 1

2 14 6

7 12 8

3 13 15 4
Data set index

5 10 11 18 17 9 16

Figure 14: Ratio weighted number tests GSIMN versus GSMN difference
accuracy GSIMN GSMN real data sets. Ratios smaller
1 positive bars indicate advantage GSIMN GSMN .
numbers x-axis indices data sets shown Table 1.

data set. data set algorithm, report weighted number conditional independence tests conducted discover network accuracy, defined
below.

475

20

Data set size (thousands data points)

Weighted cost tests Water data set

GSMN* without propagation
GSMN* propagation
GSIMN

Weighted cost tests

Weighted cost tests

20

Data set size (thousands data points)

Weighted cost tests Mildew data set
10000

GSMN* without propagation
GSMN* propagation
GSIMN

16000

6000

10000

Weighted cost tests alarm data set

Weighted cost tests

Weighted cost tests hailfinder data set

fiBromberg, Margaritis, & Honavar

Table 1: Weighted number tests accuracy several real-world data sets.
evaluation measure, best performance GSMN (with without
propagation) GSIMN indicated bold. number variables
domain denoted n number data points data set N .

#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

Data set
Name
echocardiogram
ecoli
lenses
hayes-roth
hepatitis
cmc
balance-scale
baloons
flag
tic-tac-toe
bridges
car
monks-1
haberman
nursery
crx
imports-85
dermatology
adult

n

N

14
9
5
6
20
10
5
5
29
10
12
7
7
5
9
16
25
35
10

61
336
24
132
80
1473
625
20
194
958
70
1728
556
306
12960
653
193
358
32561

Weighted number tests
GSMN
GSMN
GSIMN
(w/o prop.) (w/ prop.)
1311
1050
604
425
309
187
60
40
20
102
72
30
1412
980
392
434
292
154
82
47
29
60
40
20
5335
2787
994
435
291
119
520
455
141
194
140
67
135
93
42
98
76
42
411
270
123
1719
999
305
4519
3064
1102
9902
6687
2635
870
652
418

GSMN
(w/o prop.)
0.244
0.353
0.966
0.852
0.873
0.746
0.498
0.932
0.300
0.657
0.814
0.622
0.936
0.308
0.444
0.279
0.329
0.348
0.526

Accuracy
GSMN
(w/ prop.)
0.244
0.394
0.966
0.852
0.912
0.767
0.797
0.932
0.674
0.657
0.635
0.677
0.936
0.308
0.793
0.556
0.460
0.541
0.537

GSIMN
0.244
0.411
0.966
0.852
0.968
0.794
0.698
0.932
0.929
0.704
0.916
0.761
0.936
0.308
0.755
0.892
0.847
0.808
0.551

real-world data structure underlying Bayesian network (if any)
unknown, impossible measure Hamming distance resulting network
structure. Instead, measured estimated accuracy network produced GSMN
GSIMN comparing result (true false) number conditional independence
tests network learned (using vertex separation) result tests
performed data set (using 2 test). approach similar estimating accuracy
classification task unseen instances inputs triplets (X, Y, Z)
class attribute value corresponding conditional independence test.
used 1/3 real-world data set (randomly sampled) input GSMN GSIMN
entire data set 2 test. corresponds hypothetical scenario
much smaller data set available researcher, approximates true value
test outcome entire data set. Since number possible tests
exponential, estimated independence accuracy sampling 10,000 triplets (X, Y, Z)
randomly, evenly distributed among possible conditioning set sizes {0, . . . , n 2}
(i.e., 10000/(n 1) tests m). triplets constructed follows:
First, two variables X drawn randomly V. Second, conditioning set
determined picking first variables random permutation V {X, }.
Denoting set 10,000 triplets, triplet, Idata (t) result test
performed entire data set Inetwork (t) result test performed

476

fiEfficient Markov Network Structure Discovery Using Independence Tests

network output either GSMN GSIMN, estimated accuracy defined as:
fi
ofifi
1 fifin
| Inetwork (t) = Idata (t) fifi.
accuracy
\ =
|T | fi

data sets, Table 1 shows detailed results accuracy weighted
number tests GSMN GSIMN algorithms. results also plotted
Figure 14, horizontal axis indicating data set index appearing first column
Table 1. Figure 14 plots two quantities graph real-world data sets:
ratio weighted number tests GSIMN versus two GSMN algorithms
difference accuracies. data set, improvement GSIMN
GSMN corresponds number smaller 1 ratios positive histogram bar
accuracy differences. observe GSIMN reduced weighted number
tests every data set, maximum savings 82% GSMN without propagation
(for crx data set) 60% GSMN propagation (for crx data set
well). Moreover, 11 19 data sets GSIMN resulted improved accuracy, 6 tie
2 somewhat reduced accuracy compared GSMN propagation (for
nursery balance-scale data sets).

6. Conclusions Future Research
paper presented two algorithms, GSMN GSIMN, learning efficiently
structure Markov network domain data using independence-based
approach (as opposed NP-hard algorithms based maximum likelihood estimation)
evaluated performance measurement weighted number tests
require learn structure network quality networks learned
artificial real-world data sets. GSIMN showed decrease vast majority
artificial real-world domains output network quality comparable
GSMN , cases showing improvement. addition, GSIMN shown
nearly optimal number tests executed compared GSIMN-FCH, uses
exhaustive search produce independence information inferred Pearls
axioms. directions future research include investigation way topology
underlying Markov network affects number tests required quality
resulting network, especially commonly occurring topologies grids. Another
research topic impact number tests examination grow orderings
variables.

Acknowledgments
thank Adrian Silvescu insightful comments accuracy measures general advice
theory undirected graphical models.

Appendix A. Correctness GSMN
variable X V examined main loop GSMN algorithm (lines
1039), set BX variable X V constructed growing shrinking set S,
477

fiBromberg, Margaritis, & Honavar

starting empty set. X connected member BX produce
structure Markov network. prove procedure returns actual Markov
network structure domain.
proof correctness make following assumptions.
axioms Eqs. (1) hold.
probability distribution domain strictly positive (required Intersection
axiom hold).
Tests conducted querying oracle, returns true value underlying model.
algorithm examines every variable X inclusion (and thus BX )
grow phase (lines 18 33) and, added grow phase,
considers removal shrinking phase (lines 34 37). Note
one test executed X growing phase X; call grow
test X (line 23). Similarly, one tests executed X
shrinking phase; test (if executed) called shrink test X (line
36).
general idea behind proof show that, learning blanket X,
variable end shrinking phase dependence (X 6Y |
V {X, }) X holds (which, according Theorem 2 end
Appendix, implies edge X ). immediately prove one
direction.
Lemma 1.
/ end shrink phase, (XY | V {X, }).
Proof. Let us assume
/ end shrink phase. Then, either
added set grow phase (i.e., line 24 never reached), removed
shrink phase (i.e., line 37 reached). former true
(pXY > ) line 22 (indicating X unconditionally independent) found
independent X line 23. latter true found independent X
line 36. cases V {X, } (XY | A), Strong Union
(XY | V {X, }).
opposite direction proved Lemma 6 below. However, proof involved,
requiring auxiliary lemmas, observations, definitions. two main auxiliary
Lemmas 4 5. use lemma presented next (Lemma 2) inductively extend
conditioning set dependencies found grow shrink tests X ,
remaining variables V{X, }. Lemma shows that, certain independence
holds, conditioning set dependence increased one variable.
Lemma 2. Let X, V, Z V {X, }, Z Z. W V,
(X 6Y | Z) (XW | Z {Y }) = (X 6Y | Z {W }).

478

fiEfficient Markov Network Structure Discovery Using Independence Tests

Proof. prove contradiction, make use axioms Intersection (I), Strong
Union (SU), Decomposition (D). Let us assume (X 6Y | Z) (XW | Z {Y })
(XY | Z {W }).
(XY | Z {W }) (XW | Z {Y })
SU

=

(XY | Z {W }) (XW | Z {Y })



(X{Y, W } | Z)



=

(XY | Z) (XW | Z)

=

(XY | Z).

=

contradicts assumption (X 6Y | Z).
introduce notation definitions prove auxiliary lemmas.
denote SG value end grow phase (line 34) i.e., set
variables found dependent X grow phase, SS value end
shrink phase (line 39). also denote G set variables found independent
X grow phase U = [U0 , . . . , Uk ] sequence variables shrunk
BX , i.e., found independent X shrink phase. sequence U assumed
ordered follows: < j variable Ui found independent X Uj
shrinking phase. prefix first variables [U0 , . . . , Ui1 ] U denoted
Ui . test performed algorithm, define k(t) integer
Uk(t) prefix U containing variables found independent X
loop t. Furthermore, abbreviate Uk(t) Ut .
definition U fact grow phase conditioning set
increases dependent variables only, immediately make following observation:
Observation 1. variable Ui U, denotes shrink test performed
X Ui Ut = Ui1 .
relate conditioning set shrink test Ut follows:
Lemma 3. SS = (X, | Z) shrink test , Z = SG Ut {Y }.
Proof. According line 36 algorithm, Z = {Y }. beginning shrink
phase (line 34) = SG , variables found independent afterward conducted
removed line 37. Thus, time performed, = SG Ut
conditioning set becomes SG Ut {Y }.
Corollary 1. (XUi | SG Ui ).
Proof. proof follows immediately Lemma 3, Observation 1, fact
Ui = Ui1 {Ui }.
following two lemmas use Lemma 2 inductively extend conditioning set
dependence X variable SS . first lemma starts shrink
test X (a dependence), extends conditioning set SS {Y } (or
equivalently SG {Y } Ut according Lemma 3) SG {Y }.
479

fiBromberg, Margaritis, & Honavar

Lemma 4. SS shrink test , (X 6Y | SG {Y }).
Proof. proof proceeds proving
(X 6Y | SG {Y } Ui )
induction decreasing values i, {0, 1, . . . , k(t)}, starting = k(t).
lemma follows = 0 noticing U0 = .
Base case (i = k(t)): Lemma 3, = (X, | SG {Y } Ut ), equals
(X, | SG {Y } Uk(t) ) definition Ut . Since SS , must case
found dependent, i.e., (X 6Y | SG {Y } Uk(t) ).
Inductive step: Let us assume statement true = m, 0 < k(t)1:
(X 6Y | SG {Y } Um ).

(2)

need prove also true = 1:
(X 6Y | SG {Y } Um1 ).
Corollary 1,
(XUm | SG Um )
Strong Union,
(XUm | (SG Um ) {Y })

(XUm | (SG Um {Y }) {Y }).

(3)

Eqs. (2), (3) Lemma 2 get desired relation:
(X 6Y | (SG {Y } Um ) {Um }) = (X 6Y | SG {Y } Um1 ).

Observation 2. definition SG , every test = (X, | Z) performed
grow phase, Z SG .
following lemma completes extension conditioning set dependence
X SS universe variables V {X, }, starting SG {Y }
(where Lemma 4 left off) extending SG G {Y }.
Lemma 5. SS , (X 6Y | SG G {Y }).
Proof. proof proceeds proving
(X 6Y | SG Gi {Y })
induction increasing values 0 |G|, Gi denotes first elements
arbitrary ordering set G.
480

fiEfficient Markov Network Structure Discovery Using Independence Tests

Base Case (i = 0): Follows directly Lemma 4 = 0, since G0 = .
Inductive Step: Let us assume statement true = m, 0 < |G|:
(X 6Y | SG Gm {Y }).

(4)

need prove also true = + 1:
(X 6Y | SG Gm+1 {Y }).

(5)

Observation 2 grow test Gm results independence:
(XGm | Z), Z SG .
Strong Union axiom become:
(XGm | Z {Y }), Z SG

(6)

(XGm | (Z {Y }) {Y }), Z SG .

(7)

equivalently
Since Z SG SG Gm , Z {Y } SG Gm , Eq. (4)
Lemma 2 get desired relation:
(X 6Y | (SG Gm {Y }) Gm ) = (X 6Y | SG Gm+1 {Y }).

Finally, prove X dependent every variable SS given universe
V {X, }.
Lemma 6. SS , (X 6Y | V {X, }).
Proof. Lemma 5,
(X 6Y | SG G {Y })
suffices prove SG G {Y } = V {X, }. loop 69 GSMN ,
queue X populated elements V {X}, then, line 21, removed
X . grow phase partitions X variables dependent X (set SG )
independent X (set G).
Corollary 2. SS (X 6Y | V {X, }).
Proof. Follows directly Lemmas 1 6.
Corollary immediately show graph returned
connecting X member BX = SS exactly Markov network domain
using following theorem, first published Pearl Paz (1985).
Theorem 2. (Pearl & Paz, 1985) Every dependence model satisfying symmetry, decomposition, intersection (Eqs. (1)) unique Markov network G = (V, E) produced
deleting complete graph every edge (X, ) (XY | V {X, }) holds
, i.e.,
(X, )
/ E (XY | V {X, }) .
481

fiBromberg, Margaritis, & Honavar

Appendix B. Correctness GSIMN
GSIMN algorithm differs GSMN use test subroutine IGSIMN
instead IGSMN (Algorithms 5 4, respectively), turn differs number
additional inferences conducted obtain independencies (lines 8 22).
inferences direct applications Strong Union axiom (which holds assumption)
Triangle theorem (which proven hold Theorem 1). Using correctness
GSMN (proven Appendix A) therefore conclude GSIMN algorithm
correct.

References
Abbeel, P., Koller, D., & Ng, A. Y. (2006). Learning factor graphs polynomial time
sample complexity. Journal Machine Learning Research, 7, 17431788.
Acid, S., & de Campos, L. M. (2003). Searching Bayesian network structures
space restricted acyclic partially directed graphs. Journal Artificial Intelligence
Research, 18, 445490.
Agresti, A. (2002). Categorical Data Analysis (2nd edition). Wiley.
Aliferis, C. F., Tsamardinos, I., & Statnikov, A. (2003). HITON, novel Markov blanket
algorithm optimal variable selection. Proceedings American Medical
Informatics Association (AMIA) Fall Symposium.
Anguelov, D., Taskar, B., Chatalbashev, V., Koller, D., Gupta, D., Heitz, G., & Ng, A.
(2005). Discriminative learning Markov random fields segmentation 3D range
data. Proceedings Conference Computer Vision Pattern Recognition
(CVPR).
Barahona, F. (1982). computational complexity Ising spin glass models. Journal
Physics A: Mathematical General, 15 (10), 32413253.
Besag, J. (1974). Spacial interaction statistical analysis lattice systems. Journal
Royal Statistical Society, Series B, 36, 192236.
Besag, J., York, J., & Mollie, A. (1991). Bayesian image restoration two applications
spatial statistics.. Annals Institute Statistical Mathematics, 43, 159.
Bromberg, F., Margaritis, D., & Honavar, V. (2006). Efficient Markov network structure discovery independence tests. Proceedings SIAM International Conference
Data Mining.
Buntine, W. L. (1994). Operations learning graphical models. Journal Artificial
Intelligence Research, 2, 159225.
Castelo, R., & Roverato, A. (2006). robust procedure Gaussian graphical model search
microarray data p larger n. Journal Machine Learning Research,
7, 26212650.
Chow, C., & Liu, C. (1968). Approximating discrete probability distributions dependence trees. IEEE Transactions Information Theory, 14 (3), 462 467.

482

fiEfficient Markov Network Structure Discovery Using Independence Tests

Cochran, W. G. (1954). methods strengthening common 2 tests. Biometrics,
10, 417451.
Della Pietra, S., Della Pietra, V., & Lafferty, J. (1997). Inducing features random fields.
IEEE Transactions Pattern Analysis Machine Intelligence, 19 (4), 390393.
Dobra, A., Hans, C., Jones, B., Nevins, J. R., Yao, G., & West, M. (2004). Sparse graphical
models exploring gene expression data. Journal Multivariate Analysis, 90, 196
212.
Edwards, D. (2000). Introduction Graphical Modelling (2nd edition). Springer, New
York.
Friedman, N., Linial, M., Nachman, I., & Peer, D. (2000). Using Bayesian networks
analyze expression data. Computational Biology, 7, 601620.
Geman, S., & Geman, D. (1984). Stochastic relaxation, gibbs distributions, bayesian
relation images.. IEEE Transactions Pattern Analysis Machine Intelligence,
6, 721741.
Heckerman, D. (1995). tutorial learning bayesian networks. Tech. rep. MSR-TR-95-06,
Microsoft Research.
Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks:
combination knowledge statistical data. Machine Learning, 20, 197243.
Henrion, M. (1988). Propagation uncertainty probabilistic logic sampling Bayes
networks. Lemmer, J. F., & Kanal, L. N. (Eds.), Uncertainty Artificial Intelligence 2. Elsevier Science Publishers B.V. (North Holland).
Hofmann, R., & Tresp, V. (1998). Nonlinear Markov networks continuous variables.
Neural Information Processing Systems, Vol. 10, pp. 521529.
Isard, M. (2003). Pampas: Real-valued graphical models computer vision. IEEE
Conference Computer Vision Pattern Recognition, Vol. 1, pp. 613620.
Jerrum, M., & Sinclair, A. (1993). Polynomial-time approximation algorithms Ising
model. SIAM Journal Computing, 22, 10871116.
Kearns, M. J., & Vazirani, U. V. (1994). Introduction Computational Learning Theory.
MIT Press, Cambridge, MA.
Koller, D., & Sahami, M. (1996). Toward optimal feature selection. International Conference Machine Learning, pp. 284292.
Lam, W., & Bacchus, F. (1994). Learning Bayesian belief networks: approach based
MDL principle. Computational Intelligence, 10, 269293.
Lauritzen, S. L. (1996). Graphical Models. Oxford University Press.
Margaritis, D., & Thrun, S. (2000). Bayesian network induction via local neighborhoods.
Solla, S., Leen, T., & Muller, K.-R. (Eds.), Advances Neural Information Processing
Systems 12, pp. 505511. MIT Press.
McCallum, A. (2003). Efficiently inducing features conditional random fields. Proceedings Uncertainty Artificial Intelligence (UAI).

483

fiBromberg, Margaritis, & Honavar

Newman, D. J., Hettich, S., Blake, C. L., & Merz, C. J. (1998). UCI repository machine
learning databases. Tech. rep., University California, Irvine, Dept. Information
Computer Sciences.
Pena, J. M. (2008). Learning Gaussian graphical models gene networks false discovery rate control. Proceedings 6th European Conference Evolutionary
Computation, Machine Learning Data Mining Bioinformatics, pp. 165176.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers, Inc.
Pearl, J., & Paz, A. (1985). Graphoids: graph-based logic reasoning releveance
relations. Tech. rep. 850038 (R-53-L), Cognitive Systems Laboratory, University
California.
Rebane, G., & Pearl, J. (1989). recovery causal poly-trees statistical data.
Kanal, L. N., Levitt, T. S., & Lemmer, J. F. (Eds.), Uncertainty Artificial
Intelligence 3, pp. 175182, Amsterdam. North-Holland.
Schafer, J., & Strimmer, K. (2005). empirical bayes approach inferring large-scale
gene association networks. Bioinformatics, 21, 754764.
Scott, D. W. (1992). Multivariate Density Estimation. Wiley series probability
mathematical statistics. John Wiley & Sons.
Shekhar, S., Zhang, P., Huang, Y., & Vatsavai, R. R. (2004) Kargupta, H., Joshi, A.,
Sivakumar, K., & Yesha, Y. (Eds.), Trends Spatial Data Mining, chap. 19, pp.
357379. AAAI Press / MIT Press.
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, Prediction, Search (2nd
edition). Adaptive Computation Machine Learning Series. MIT Press.
Srebro, N., & Karger, D. (2001). Learning Markov networks: Maximum bounded tree-width
graphs. ACM-SIAM Symposium Discrete Algorithms.
Tsamardinos, I., Aliferis, C. F., & Statnikov, A. (2003a). Algorithms large scale Markov
blanket discovery. Proceedings 16th International FLAIRS Conference, pp.
376381.
Tsamardinos, I., Aliferis, C. F., & Statnikov, A. (2003b). Time sample efficient discovery Markov blankets direct causal relations. Proceedings 9th ACM
SIGKDD International Conference Knowledge Discovery Data Mining, pp.
673678.
Tsamardinos, I., Brown, L. E., & Aliferis, C. F. (2006). max-min hill-climbing Bayesian
network structure learning algorithm. Machine Learning, 65, 3178.
Whittaker, J. (1990). Graphical Models Applied Multivariate Statistics. John Wiley &
Sons, New York.

484

fiJournal Artificial Intelligence Research 35 (2009) 275-341

Submitted 09/08; published 06/09

Llull Copeland Voting Computationally Resist
Bribery Constructive Control
Piotr Faliszewski

FALISZEW @ AGH . EDU . PL

Department Computer Science, AGH University Science Technology
Krakow, Poland

Edith Hemaspaandra

EH @ CS . RIT. EDU

Department Computer Science, Rochester Institute Technology
Rochester, NY 14623 USA

Lane A. Hemaspaandra

LANE @ CS . ROCHESTER . EDU

Department Computer Science, University Rochester
Rochester, NY 14627 USA

Jorg Rothe

ROTHE @ CS . UNI - DUESSELDORF. DE

Institut fur Informatik, Heinrich-Heine-Universitat Dusseldorf
40225 Dusseldorf, Germany

Abstract
Control bribery settings external agent seeks influence outcome
election. Constructive control elections refers attempts agent to, via actions
addition/deletion/partition candidates voters, ensure given candidate wins. Destructive
control refers attempts agent to, via actions, preclude given candidates victory.
election system agent sometimes affect result determined
polynomial time inputs agent succeed said vulnerable given type
control. election system agent sometimes affect result, yet
NP-hard recognize inputs agent succeed, said resistant given
type control.
Aside election systems NP-hard winner problem, systems previously
known resistant standard control types highly artificial election systems created hybridization. paper studies parameterized version Copeland voting, denoted
Copeland , parameter rational number 0 1 specifies ties
valued pairwise comparisons candidates. every previously studied constructive destructive control scenario, determine resistance vulnerability holds Copeland
rational , 0 1. particular, prove Copeland0.5 , system commonly
referred Copeland voting, provides full resistance constructive control, prove
Copeland , rational , 0 < < 1. Among systems polynomial-time
winner problem, Copeland voting first natural election system proven full resistance
constructive control. addition, prove Copeland0 Copeland1 (interestingly,
Copeland1 election system developed thirteenth-century mystic Llull) resistant
standard types constructive control one variant addition candidates. Moreover, show rational , 0 1, Copeland voting fully resistant bribery
attacks, establish fixed-parameter tractability bounded-case control Copeland .
also study Copeland elections flexible models microbribery extended control, integrate potential irrationality voter preferences many results,
prove results unique-winner model nonunique-winner model.
vulnerability results microbribery proven via novel technique involving min-cost network
flow.
c
2009
AI Access Foundation. rights reserved.

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

1. Introduction
section gives history outline results.
1.1 Historical Remarks: Llulls Copelands Election Systems
Elections played important role human societies thousands years. example,
elections central importance democracy ancient Athens. citizens typically
could agree (vote yes) disagree (vote no) speaker, simple majority-rule
effect. mathematical study elections, give take discussions ancient Greeks
Romans, recently thought initiated hundred years ago, namely
breakthrough work Borda Condorcetlater part reinvented Dodgson (see, e.g.,
McLean Urken, 1995, reprints classic papers). One interesting results
early work Condorcets (1785) observation one conducts elections two
alternatives even voters rational (i.e., transitive) preferences, society aggregate
irrational (indeed, cycles strict preference). Nonetheless, Condorcet believed
exists candidate c c defeats candidate head-to-head contests
c win election (see, e.g., page 114 McLean Urken, 1995). candidate
called Condorcet winner. Clearly, one Condorcet winner election,
might none.
understanding history reconsidered past decades,
discovered study elections considered deeply early thirteenth century (see
Hagele Pukelsheim, 2001, citations therein regarding Ramon Llull fifteenthcentury figure Cusanus, especially citations Hagele Pukelsheim, 2001, numbered
3, 5, 2427). Ramon Llull (b. 1232, d. 1315), Catalan mystic, missionary, philosopher
developed election system (a) efficient winner-determination procedure (b) elects
Condorcet winner whenever one exists otherwise elects candidates are, certain sense,
closest Condorcet winners.
Llulls motivation developing election system obtain method choosing
abbesses, abbots, bishops, perhaps even pope. election ideas never gained public acceptance medieval Europe long forgotten.
interesting note Llull allowed voters so-called irrational preferences. Given
three candidates, c, d, e, perfectly acceptable voter prefer c d, e, e
c. hand, modern studies voting election systems voters preferences
typically modeled linear order candidates. (In paper, common
discussing elections, linear order implies strictness, i.e., tie ordering; is,
linear order mean strict, complete order, i.e., irreflexive, antisymmetric, complete,
transitive relation.) Yet allowing irrationality tempting natural. Consider Bob,
likes eat often hurry. Bob prefers diners fast food willing
wait little longer get better food. Also, given choice fancy restaurant diner
prefers fancy restaurant, willing wait somewhat longer get better
quality. However, given choice fast-food place fancy restaurant Bob might
reason willing wait much longer served fancy restaurant
choose fast food instead. Thus regarding catering options, Bobs preferences irrational
sense, i.e., intransitive. voters make choices based multiple criteriaa
common natural occurrence among humans software agentssuch irrationalities
276

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

occur. Another example irrationality might naturally occur, suggested referee,
case voter delegate group (having odd number members),
pair alternatives delegate votes whichever alternative majority
constituents prefers among pair.
Llulls election system remarkably similar known Copeland elections (Copeland, 1951), half-century old voting procedure based pairwise
comparisons candidates: winner (by majority votesin paper majority always,
standard, means strict majority) head-to-head contest awarded one point
loser awarded zero points; ties, parties (in common interpretation
Copelands meaning) awarded half point; whoever collects points contests (including tie-related points) elections winner. fact, point value awarded ties
head-to-head majority-rule contests treated two ways literature speaking
Copeland elections: half point (most common) zero points (less common). provide
framework capture notions, well capturing Llulls system whole
family systems created choices value ties, propose introduce parameterized version Copeland elections, denoted Copeland , parameter rational
number, 0 1, case tie candidates receive points. system widely
referred literature Copeland elections Copeland0.5 , tied candidates receive
half point (see, e.g., Saari Merlin, 1996, Merlin Saari, 1997; definition used
Conitzer, Sandholm, & Lang, 2007, scaled equivalent Copeland0.5 ). Copeland0 ,
tied candidates come away empty-handed, sometimes also referred Copeland
elections (see, e.g., Procaccia, Rosenschein, Kaminka, 2007, Faliszewski, Hemaspaandra,
Hemaspaandra, Rothe, 2007, early version paper). above-mentioned election
system proposed Ramon Llull thirteenth century notation Copeland1 , tied
candidates awarded one point each, like winners head-to-head contests.1 group stage
1
FIFA World Cup finals essence collection Copeland 3 tournaments.
first glance, one might tempted think definitional perturbation due parameter Copeland elections negligible. However, fact make dynamics Llulls
system quite different of, instance, Copeland0.5 Copeland0 . Specific examples
witnessing claim, regarding complexity results regarding proofs, given
end Section 1.3.
Finally, mention probabilistic variant Copeland voting (known Jech method)
defined already 1929 Zermelo (1929) later reintroduced several researches (see, e.g., Levin Nalebuff, 1995, references description Jech
method). note passing Jech method applicable even fed incomplete information. present paper, however, consider incomplete-information probabilistic
scenarios, although commend settings interesting future work.
1. Page 23 Hagele Pukelsheim 2001 indicates way find deeply convincing (namely direct quote
Llulls in-this-case-very-clear words Artifitium Electionis Personarumwhich rediscovered
authors year 2000) least one Llulls election systems Copeland1 , paper refer
both-candidates-score-a-point-on-a-tie variant Llull voting.
settings Llull required candidate voter sets identical elaborate two-stage tiehandling rule ending randomization. disregard issues cast system modern idiom
election systems. (However, note passing exist modern papers voter
candidate sets taken identical, see example work references Altman Tennenholtz, 2007.)

277

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

1.2 Computational Social Choice
general impossible design perfect election system. Arrow (1963) famously showed
social choice system satisfies certain small set arguably reasonable requirements,
later Gibbard (1973), Satterthwaite (1975), Duggan Schwartz (2000) showed
natural election system sometimes manipulated strategic voting, i.e., voter revealing
different preferences true ones order affect elections result
favor. Also, natural election system polynomial-time winner-determination procedure
yet shown resistant types control via procedural changes. Control refers
attempts external agent (called chair) to, via actions addition/deletion/partition
candidates voters, make given candidate win election (in case constructive control,
Bartholdi, Tovey, Trick, 1992) preclude given candidates victory (in case destructive
control, Hemaspaandra, Hemaspaandra, Rothe, 2007a).
obstacles discouraging, field computational social choice theory grew
part realization computational complexity provides potential shield manipulation/control/etc. particular, around 1990, Bartholdi, Tovey, Trick (1989a), Bartholdi
Orlin (1991), Bartholdi et al. (1992) brilliantly observed perhaps might
able make manipulation (i.e., strategic voting) control elections impossible, could
least try make manipulation control computationally difficult neither voters
election organizers attempt it. example, way committees chair set
election within committee way favorite option guaranteed
win, chairs computational task would take million years, practical purposes
may feel chair prevented finding set-up.
Since seminal work Bartholdi, Orlin, Tovey, Trick, large body research
dedicated study computational properties election systems. topics received much attention complexity manipulating elections (Conitzer & Sandholm, 2003,
2006; Conitzer et al., 2007; Elkind & Lipmaa, 2005; Hemaspaandra & Hemaspaandra, 2007; Procaccia & Rosenschein, 2007; Meir, Procaccia, Rosenschein, & Zohar, 2008) controlling
elections via procedural changes (Hemaspaandra et al., 2007a; Hemaspaandra, Hemaspaandra, &
Rothe, 2007b; Meir et al., 2008; Erdelyi, Nowak, & Rothe, 2008b). Recently, Faliszewski, Hemaspaandra, Hemaspaandra (2006a) introduced study complexity bribery elections.
Bribery shares features manipulation features control. particular, briber
picks voters wants affect (as voter control problems) asks vote
wishes (as manipulation). (For additional citations pointers, see recent survey Faliszewski, Hemaspaandra, Hemaspaandra, Rothe, 2009.)
paper study Copeland elections respect computational complexity
bribery procedural control; see Faliszewski, Hemaspaandra, Schnoor 2008 study
manipulation within Copeland .
study election systems computational properties, complexity
manipulation, control, bribery problems, important topic multiagent systems.
Agents/voters may different, often conflicting, individual preferences given alternatives (or candidates) voting rules (or, synonymously, election systems) provide useful method
agents come reasonable decision alternative choose. Thus elections
employed multiagent settings also contexts solve many practical problems.
examples, mention Ephrati Rosenschein (1997) use elections planning, Ghosh,

278

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

Mundhe, Hernandez, Sen (1999) develop recommender system movies based
voting, Dwork, Kumar, Naor, Sivakumar (2001) use elections aggregate results
multiple web-search engines. multiagent setting may hundreds elections happening
every minute cannot hope carefully check case whether party organized
election attempted procedural change skew results. However, computationally
hard find procedural changes hope practically infeasible organizers
undertake them.
standard technique showing particular election-related problem (for example,
problem deciding whether chair make favorite candidate winner influencing
k voters cast votes) computationally intractable show NP-hard.
approach taken almost papers computational social choice cited above,
approach take paper. One justifications using NP-hardness
barrier manipulation control elections multiagent settings attempts
influence elections outcome made computationally bounded software agents
neither human intuition computational ability solve NP-hard problems.
Recently, Conitzer Sandholm (2006), Procaccia Rosenschein (2007), Homan
Hemaspaandra (to appear), McCabe-Dansted, Pritchard, Slinko (2008) studied
frequency (or sometimes, probability weight) correctness heuristics voting problems. Although fascinating important direction, point remove need study
worst-case hardness. Indeed, view worst-case study natural prerequisite frequency-ofhardness attack: all, point seeking frequency-of-hardness results problem
hand P begin with. one cannot even prove worst-case hardness problem,
proving average-case hardness even beyond reach. Also, current frequency results
debilitating limitations (for example, locked specific distributions; depending unproven assumptions; adopting tractability notions declare undecidable problems tractable
robust even linear-time reductions). models arguably ready
prime time and, contrary peoples impression, imply (and goal
implying, since studying frequency hardness) average-case polynomial runtime claims.
Erdelyi, Hemaspaandra, Rothe, Spakowski (2007) Homan Hemaspaandra (to appear)
provide discussions issues. Regarding recent work Friedgut, Kalai,
Nisan (2008) (see also Xia Conitzer, 2008a, 2008b), interesting work control, lower bounds proven show one manipulate time,
rather work provides lower bounds unfortunately go zero number voters
increases, case studied. course, limitations current results frequency
hardness surely mean direction interesting; clearly, field best
go beyond limitations.
1.3 Outline Results
goal paper study Copeland elections point view computational social
choice theory, setting voters rational setting voters allowed
irrational preferences. (Note: henceforward say irrational voters, mean
voters may irrational preferences, must.) study issues bribery
control point reader Faliszewski et al. 2008 work manipulation. (Very
briefly summarized, work Faliszewski et al., 2008, manipulation Copeland elections

279

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

shows rational , 0 < < 1, 6= 12 , coalitional manipulation problem unweighted
Copeland elections, even coalitions two manipulators, NP-complete.
constructions present paper adopted adapted paper order prove
results manipulation.)
Bribery control problems natural real-life interpretations. example,
presidential elections candidate might want encourage many supporters
possible vote (get-out-the-vote efforts): control via addition voters; elections held
inconvenient date group voters (e.g., holiday) hard-to-reach location (e.g.,
requiring one car, getting location involves passing dangerous areas):
control via deleting voters; one choose voting districts way favorable particular candidate party (gerrymandering): control via partitioning voters; one introduce new candidate
election hope steal votes away opponents ones favorite
candidate without affecting favorite candidates performance: control via adding candidates.
control scenarios study also natural interpretations.
Similarly, bribery natural important issue context elections. stress, however, bribery problems necessarily need correspond cheating sort illegal
action. One could view bribery problems as, example, problems finding minimum number voters switch result election and, thus, problems finding coalitions,
especially one assigns prices voters measure difficulty convincing particular voter
join coalition (see, e.g., Faliszewski, 2008, example bribery problem
interpretation natural).
quite natural study control bribery constructive settings (where want
make favorite candidate winner) destructive settings (where try prevent candidate winning). context real-life elections, one often hears voters speaking
candidate hope win, one also often hears voters expressing sentiment Anyone
him. constructive destructive settings correspond actions agents belonging
groups might interested in.
One main achievements paper classify resistance vulnerability holds Copeland every previously studied control scenario rational value ,
0 1. so, provide example control problem complexity
Copeland0.5 (which system commonly referred Copeland) differs
Copeland0 Copeland1 : latter two problems vulnerable constructive control
adding (an unlimited number of) candidates, Copeland0.5 resistant control type (see
Section 2 definitions Theorem 4.10 result).
fact, Copeland (i.e., Copeland0.5 ) first natural election system (with polynomial-time
winner problem) proven resistant every type constructive control proposed
literature date. result closes 15-year quest natural election system fully resistant
constructive control.2
2. referee wondered whether (and speculated that) virtually every common rule (other plurality Condorcet,
said referee, although actually plurality displays breathtakingly many resistances itself, albeit constructive resistances) would display broad resistance control Copeland, one obtain results
rules. course open issue, see reason think case (and approval voting
already provides counterexample, see Hemaspaandra et al., 2007a). even case
rules resisted many control types, suspect pattern types resisted differ among
rules, although case four quadrants (of constructive/destructive voter/candidate seem
often stand fall block). pattern seems us something natural importance, since ones choice

280

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

also show Copeland resistant constructive destructive bribery,
case rational voters case irrational voters. hardness proofs work case
unweighted voters without price tags (see Faliszewski et al., 2006a) thus, naturally, apply
well involved scenarios weighted unpriced voters, unweighted priced voters,
weighted priced voters.
prove bribery results, introduce method controlling relative performances
certain voters way that, one sets restrictions appropriately, legal possibilities
bribery actions sharply constrained. call approach UV technique, since
based dummy candidates u v. proofs Theorems 3.2 3.4 particular applications
method. feel UV technique useful, even beyond scope paper,
analysis bribery election systems based head-to-head contests.
also study Copeland elections flexible models microbribery (see Section 3.2) extended control (see Section 4.3). show Copeland (with irrational voters
allowed) vulnerable destructive microbribery destructive candidate control via providing
fairly simple greedy algorithms. contrast, polynomial-time algorithms constructive microbribery proven via technique involving min-cost network flows. best knowledge,
first application min-cost flows election problems. believe range applicability flow networks election problems extends well beyond microbribery Copeland
elections point reader recent, independent paper Procaccia, Rosenschein,
Zohar (2008)3 paper Faliszewski (2008) examples applications.
also mention study Copeland control noticed proof
important result Bartholdi et al. (1992, Theorem 12), namely, Condorcet voting resistant
constructive control deleting voters, invalid. invalidity due proof centrally
using nonstrict voters, violation Bartholdi et al.s (1992) (and our) model, invalidity
seems potentially daunting impossible fix proof approach taken there. note also
Theorem 14 paper similar flaw. Section 5 validly reprove claimed
results using techniques.
mentioned Section 1.1, Copeland elections may behave quite differently depending
value tie-rewarding parameter . give concrete examples make case.
Specifically, proofs results Copeland occasionally differ considerably distinct values
, cases even computational complexity various control manipulation
problems (for manipulation case see Faliszewski et al., 2008) may jump P membership
NP-completeness depending . Regarding control, already noted Theorem 4.10
shows control problem (namely, control adding unlimited number candidates)
Copeland NP-complete rational 0 < < 1, yet Theorem 4.11 shows
control problem P {0, 1}. give another example involving different
election rule probably (along many factors influence rule choice) shaped
strength rule respect resisting types attacks one expects system faced with. example, Copeland exceedingly strongin fact, perfectwith respect constructive control types studied here.
contrast, plurality, Condorcet, approval (Bartholdi et al., 1992; Hemaspaandra et al., 2007a), cant
speak issue yet unstudied systems. holds rules, suspect dream-case
path would find broad results one stroke reveal control-resistance patterns whole classes election
systems. example, see Hemaspaandra Hemaspaandra 2007, essentially manipulation
scoring systems.
3. Procaccia et al. (2008) independently work Faliszewski et al. 2007 used similar technique work
regarding complexity achieving proportional representation.

281

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

control problem, namely control partition candidates ties-eliminate tie-handling rule
(see Section 2), note proofs Theorem 4.15 (which applies = 1 control
problem within Copeland ) Theorem 4.16 (which applies rational 0 < 1
problem) differ substantially. Regarding constructive microbribery, vulnerability
constructions = 0 (see Lemma 3.13) = 1 (see Lemma 3.15) significantly differ
other, neither works tie-rewarding values 0 1.
remarks notwithstanding, results show possible obtain unified
though due uniformity sometimes rather involvedconstruction works Copeland
every rational , 0 1.
1.4 Organization
paper organized follows. Section 2, formalize notion elections particular Copeland elections, introduce useful notation, formally define control
bribery problems interested in. Section 3, show rational , 0 1,
Copeland elections fully resistant bribery, case rational voters case
irrational voters. hand, one changes bribery model allow microbribes
voters (a fine-grained approach bribery, one changes voters vote,
one pay voter), prove vulnerability rational , 0 1, irrationalvoters destructive case specific values irrational-voters constructive case.
Sections 4.1 4.2, present results procedural control Copeland elections
rational 0 1. see broad resistance holds constructive-control
cases. Section 4.3 presents results fixed-parameter tractability bounded-case control
Copeland . Section 5 provides valid proofs several control problems Condorcet elections
(studied Bartholdi et al., 1992) whose original proofs invalid due odds
model elections used Bartholdi et al. 1992. conclude paper brief summary
Section 6 stating open problems.
every proof included paper, would extremely long difficult read.
Nonetheless, course important make proofs available claims. handled follows. made available Faliszewski, Hemaspaandra, Hemaspaandra,
Rothe 2008b full technical report version paper, complete detailed proofs essentially every result. current article, proofs would repetitive tedious relative
proofs include here, simply included proofs
instead included place pointer detailed proof result full technical report
version.

2. Preliminaries
section defines many notions use paper, various election systems,
election problems, hardness notions.
2.1 Elections: Systems Llull Copeland
election E = (C,V ) consists finite candidate set C = {c1 , . . . , cm } finite collection V
voters, voter represented (individually, except later discuss succinct inputs)

282

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

via preferences candidates. election system (or election rule) rule
determines winner(s) given election, i.e., mapping pairs (C,V ) subsets C.
consider two ways voters express preferences. rational case (our
default case), voters preferences represented linear order set C,4 i.e.,
voter vi preference list ci1 > ci2 > > cim , {i1 , i2 , . . . , im } = {1, 2, . . . , m}. irrational case, voters preferences represented preference table every unordered
pair distinct candidates ci c j C indicates whether voter prefers ci c j (i.e., ci > c j )
prefers c j ci (i.e., c j > ci ).
well-known election rules case rational voters plurality, Borda count,
Condorcet. Plurality elects candidate(s) ranked first largest number voters.
Borda count elects candidate(s) receive points, voter vi gives
candidate c j many points number candidates c j preferred respect vi
preferences. candidate ci Condorcet winner every candidate c j holds ci
preferred c j majority voters. Note election instance one
Condorcet winner.
paper, introduce parameterized version Copelands (1951) election system,
denote Copeland , parameter rational number 0 1 specifies
ties rewarded head-to-head majority-rule contests two distinct candidates.
Definition 2.1 Let , 0 1, rational number. Copeland election, head-tohead contest two distinct candidates, candidate preferred majority voters
obtains one point candidate obtains zero points, tie occurs
candidates obtain points. Let E = (C,V ) election. c C, scoreE (c) (by
definition) sum cs Copeland points E. Every candidate c maximum scoreE (c) (i.e.,
every candidate c satisfying (d C)[scoreE (c) scoreE (d)]) wins.
Let CopelandIrrational denote election system voters allowed irrational.
mentioned earlier, literature term Copeland elections often used
system Copeland0.5 (e.g., Saari Merlin, 1996, Merlin Saari, 1997, rescaled version
Conitzer et al., 2007), occasionally used Copeland0 (e.g., Procaccia et al., 2007,
Faliszewski et al., 2007, early version paper). mentioned earlier, system
Copeland1 proposed Llull thirteenth century (see literature pointers given
introduction) called Llull voting.
define notation help discussion Copeland elections. Informally put,
E = (C,V ) election ci c j two candidates C vsE (ci , c j ) mean
surplus votes candidate ci c j . Formally, define notion follows.
Definition 2.2 Let E = (C,V ) election let ci c j two arbitrary candidates C.
Define relative vote-score ci respect c j

0
ci = c j
vsE (ci , c j ) =
k{v V | v prefers ci c j }k k{v V | v prefers c j ci }k otherwise.
4. paper, take linear order mean strict total order. common convention within voting theory,
see, e.g., book Austen-Smith Banks 2000. However, mention field mathematics term
linear order typically taken allow nonstrictness, i.e., allow ties.

283

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

So, ci defeats c j head-to-head contest E vsE (ci , c j ) > 0, tied
vsE (ci , c j ) = 0, c j defeats ci vsE (ci , c j ) < 0. (Throughout paper, defeats excludes
possibility tie, i.e., defeats means (strictly) defeats. say ties-or-defeats
wish allow tie suffice.) Clearly, vsE (ci , c j ) = vsE (c j , ci ). often speak, plural,
relative vote-scores mean group results head-to-head contests particular
candidates.
Let , 0 1, rational number. Definition 2.1 introduced scoreE (c), Copeland
score candidate c election E. Note candidate ci C,
scoreE (ci ) = k{c j C | ci 6= c j vsE (ci , c j ) > 0}k
+ k{c j C | ci 6= c j vsE (ci , c j ) = 0}k.
particular, score0E (ci ) = k{c j C | ci 6= c j vsE (ci , c j ) > 0}k, score1E (ci ) = k{c j
C | ci 6= c j vsE (ci , c j ) 0}k. Note highest possible Copeland score
election E = (C,V ) kCk 1.
Recall candidate ci C Copeland winner E = (C,V ) c j C holds
scoreE (ci ) scoreE (c j ). (Clearly, elections one Copeland winner.)
candidate ci Condorcet winner E score0E (ci ) = kCk 1, is, ci defeats
candidates head-to-head contests.
many constructions presented upcoming proofs, use following
notation rational voters.
Notation 2.3 Within every election fix arbitrary order candidates. occurrence
subset candidates preference list means candidates listed respect


fixed order. Occurrences mean except candidates listed
reverse order.
example, C = {a, b, c, d, e}, alphabetical order used, = {a, c, e}


b > > means b > > c > e > d, b > > means b > e > c > > d.
2.2 Bribery Control Problems
describe computational problems study paper. problems come two
flavors: constructive destructive. constructive version goal determine whether,
via bribery control action type study, possible make given candidate winner
election. destructive case goal determine whether possible prevent
given candidate winner election.
Let E election system. case, E either Copeland CopelandIrrational ,
, 0 1, rational number. bribery problem E rational voters defined
follows (Faliszewski et al., 2006a).
Name: E -bribery E -destructive-bribery.
Given: set C candidates, collection V voters specified via preference lists C,
distinguished candidate p C, nonnegative integer k.
Question (constructive): possible make p winner E election resulting (C,V )
modifying preference lists k voters?
284

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

Question (destructive): possible ensure p winner E election resulting
(C,V ) modifying preference lists k voters?
version problem elections irrational voters allowed defined exactly like
rational one, difference voters represented via preference tables
rather preference lists, briber may completely change voters preference table unit
cost. end present section, Section 2.2, describe variants based seeking
make p (or preclude p being) unique winner. Later paper study another
variant bribery problemsa variant one allowed perform microbribes: bribes
cost depends preference-table entry change, briber pays separately
change.
Bribery problems seek change outcome elections via modifying reported preferences voters. contrast, control problems seek change outcome election
modifying elections structure via adding/deleting/partitioning either candidates voters.
formally defining control types, use following naming conventions corresponding control problems. name control problem starts election system used
(when clear context, may omitted), followed CC constructive control DC
destructive control, followed acronym type control: AC adding (a limited
number of) candidates, ACu adding (an unlimited number of) candidates, DC deleting
candidates, PC partition candidates, RPC run-off partition candidates, AV
adding voters, DV deleting voters, PV partition voters. partitioning
cases (PC, RPC, PV) two-stage elections, use tie-handling rules Hemaspaandra et al. (2007a) first-stage subelections two-stage elections. particular,
partitioning cases, acronym PC, RPC, PV, respectively, followed acronym
tie-handling rule used first-stage subelections, namely TP ties promote (i.e., winners
first-stage subelections promoted final round election) TE ties eliminate
(i.e., unique winners first-stage subelections promoted final round election,
one winner given first-stage subelection winner given
first-stage subelection subelection move candidates forward).
formally define control problems. definitions due Bartholdi et al.
(1992) constructive control Hemaspaandra et al. (2007a) destructive control.
Let E election system. Again, E either Copeland CopelandIrrational ,
, 0 1, rational number. describe control problems case
rational preferences, irrational cases perfectly analogous, except replacing preference
lists preference tables.
C ONTROL VIA DDING C ANDIDATES
start two versions control via adding candidates. unlimited version goal
election chair introduce candidates pool spoiler candidates make
favorite candidate winner election (in constructive case) prevent despised
candidate winner (in destructive case). suggested name problem,
unlimited version chair introduce subset spoiler candidates (none, some,
legal options) election.
Name: E -CCACu E -DCACu (control via adding unlimited number candidates).
285

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

Given: Disjoint sets C candidates, collection V voters specified via preference
lists candidates set C D, distinguished candidate p C.
Question (E -CCACu ): subset E p winner E election voters
V candidates C E?
Question (E -DCACu ): subset E p winner E election
voters V candidates C E?
definition E -CCACu (using different notation) introduced Bartholdi et al. (1992).
contrast control problems involving adding deleting candidates voters,
adding candidates problem Bartholdi, Tovey, Trick introduce nonnegative integer k
bounds number candidates (from set D) chair allowed add. feel
asymmetry definitions well justified,5 thus define with-change-parameter
version control-by-adding-candidates problems, denote ACl (where l
stands fact part problem instance limit number candidates
added, contrast model Bartholdi et al., 1992, denote ACu u
standing fact number added candidates unlimited, least sense
limited via separately input integer). with-parameter version long-studied case
AV, DV, DC, paper use AC synonymous ACl , thus
use notation AC rest paper speaking ACl . suggest natural
regularization definitions hope version become normal version
adding-candidates problem study. However, caution reader earlier papers
AC used mean ACu .
present paper, obtain results ACl also ACu case, order
allow comparisons results paper earlier works.
Turning mean AC (equivalently, ACl ), per definition E -CCAC
(i.e., E -CCACl ) ask whether possible make distinguished candidate p winner
E election obtained adding k candidates spoiler candidate set D. (Note
k part input.) define destructive version, E -DCAC (i.e., E -DCACl ), analogously.
Name: E -CCAC E -DCAC (control via adding limited number candidates).
Given: Disjoint sets C candidates, collection V voters specified via preference
lists candidates set C D, distinguished candidate p C, nonnegative
integer k.
Question (E -CCAC): subset E kEk k p winner E election
voters V candidates C E?
Question (E -DCAC): subset E kEk k p winner E
election voters V candidates C E?
5. Bartholdi et al. (1992) aware asymmetry. write: certain extent exact formalization
problem matter taste. [. . . ] could equally well formalized [the problem control via adding
candidates] whether K fewer candidates added [. . . ] much matter problems
discuss, since versions complexity (Bartholdi et al., 1992). contrast, complexity
problems studied crucially hinges formalization used, thus define versions formally.

286

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

C ONTROL VIA ELETING C ANDIDATES
constructive control via deleting candidates, chair seeks ensure favorite
candidate p winner election suppressing k candidates. destructive variant
problem, chairs goal block p winning suppressing k candidates
p.6
Name: E -CCDC E -DCDC (control via deleting candidates).
Given: set C candidates, collection V voters represented via preference lists C,
distinguished candidate p C, nonnegative integer k.
Question (E -CCDC): possible deleting k candidates ensure p winner
resulting E election?
Question (E -DCDC): possible deleting k candidates p ensure p
winner resulting E election?
C ONTROL VIA PARTITION RUN -O FF PARTITION C ANDIDATES
Bartholdi et al. (1992) gave two types control elections via partition candidates.
cases candidate set C partitioned two groups, C1 C2 (i.e., C1 C2 = C C1 C2 = 0),
/
election conducted two stages. control via run-off partition candidates,
elections first stage conducted separately group candidates, C1 C2 , group
winners survive tie-handling rule compete second stage. control
via partition candidates, first-stage election performed candidate set C1
elections winners survive tie-handling rule compete candidates C2
second stage.
ties-promote (TP) model, first-stage winners within group promoted final
round. ties-eliminate (TE) model, first-stage winner within group promoted final
round unique winner within group.
Although loosely correspond real-world settings, let us give rough example regarding case run-off partition candidates. Consider department, powerful director,
trying decide among collection alternatives. certainly plausible director might announce divided candidates two groups, entire department
would vote separately among candidates group, candidates
moved forward votes (under whatever tie-handling rule used,
ties) would compete final election, entire department would vote. (How
6. referee asked whether control adding candidates, redefined require adding certain number
candidates instead least certain number candidates, cover forthcoming notion (which
standard notion) control deleting (at certain number of) candidates. answer seems
case. Consider election thirty candidates ask whether certain constructive control goal
reached via deleting five candidates. Note reframing twenty-candidate election
one tries reach goal adding least five candidates ten-candidate spoiler set doesnt make sense,
one particular twenty-candidate election start; far many possibilities.
referee similarly asked representing addition candidates new notion deleting candidates put
lower bound number deletions, attempt seems also fail, case different reason
deletion case one might delete originally spoiler candidates one might delete candidates
core election addition case, allowed.

287

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

convincingly director could would course depend directors power
well director could think justification partition candidates. Clearly partitions may easy justify, e.g., Lets regarding hire academic computer science
department first vote separately among fresh-Ph.D. candidates among senior hiring
candidates, may harder justify except executive fiat.)
Name: E -CCRPC E -DCRPC (control via run-off partition candidates).
Given: set C candidates, collection V voters represented via preference lists C,
distinguished candidate p C.
Question (E -CCRPC): partition C C1 C2 p winner twostage election winners subelection (C1 ,V ) survive tie-handling rule
compete winners subelection (C2 ,V ) survive tie-handling rule?
subelection (in stages) conducted using election system E .
Question (E -DCRPC): partition C C1 C2 p winner
two-stage election winners subelection (C1 ,V ) survive tie-handling rule
compete winners subelection (C2 ,V ) survive tie-handling rule?
subelection (in stages) conducted using election system E .
description defines four computational problems given election system E :
E -CCRPC-TE, E -CCRPC-TP, E -DCRPC-TE, E -DCRPC-TP. Note concept possible TE case candidates, due ties, eliminated first round here,
case overall election would winner.
Name: E -CCPC E -DCPC (control via partition candidates).
Given: set C candidates, collection V voters represented via preference lists C,
distinguished candidate p C.
Question (E -CCPC): partition C C1 C2 p winner two-stage
election winners subelection (C1 ,V ) survive tie-handling rule compete
candidates C2 ? subelection (in stages) conducted using election
system E .
Question (E -DCPC): partition C C1 C2 p winner
two-stage election winners subelection (C1 ,V ) survive tie-handling rule
compete candidates C2 ? subelection (in stages) conducted using
election system E .
description defines four computational problems given election system E :
E -CCPC-TE, E -CCPC-TP, E -DCPC-TE, E -DCPC-TP.
C ONTROL VIA DDING VOTERS
scenario control via adding voters, chairs goal either ensure p winner (in
constructive case) ensure p winner (in destructive case) via causing k
288

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

additional voters participate election. chair draw voters add election
prespecified collection voters (with given preferences).
loosely model real-world situations get-out-the-vote efforts. example,
suppose campaign enough money volunteers drive one hundred set
thousand car-less elderly people polling place, decide ones choose.
Name: E -CCAV E -DCAV (control via adding voters).
Given: set C candidates, two disjoint collections voters, V W , represented via preference lists C, distinguished candidate p, nonnegative integer k.
Question (E -CCAV): subset Q, kQk k, voters W voters V Q
jointly elect p C winner according system E ?
Question (E -DCAV): subset Q, kQk k, voters W voters V Q
elect p winner according system E ?
reason unlimited control notion here, anywhere else except
ACu , ACu historically special case. seminal paper Bartholdi et al. 1992 defined
addition/deletion problems (only) limited version, number k limiting
additions/deletions, except paper, describing matter individual taste, defined
addition candidates (only) unlimited version. consider limited versions
addition/deletion problems far natural, study those, Bartholdi, Tovey,
Trick every case addition candidates. However, allow comparison earlier
papers, keep defined control type case ACu .
C ONTROL VIA ELETING VOTERS
control via deleting voters case chair seeks either ensure p winner (in
constructive case) prevent p winner (in destructive case) via blocking k
voters participating election.
loosely models vote suppression. example, consider case given campaign afford send doors k voters smooth-talking operative
demoralize wont bother vote.
Name: E -CCDV E -DCDV (control via deleting voters).
Given: set C candidates, collection V voters represented via preference lists C,
distinguished candidate p C, nonnegative integer k.
Question (E -CCDV): possible deleting k voters ensure p winner
resulting E election?
Question (E -DCDV): possible deleting k voters ensure p winner
resulting E election?

289

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

C ONTROL VIA PARTITION VOTERS
case control via partition voters, following two-stage election performed. First,
voter set V partitioned two subcommittees, V1 V2 . winners election (C,V1 )
survive tie-handling rule compete winners (C,V2 ) survive tie-handling
rule. Again, tie-handling rules TE TP (ties-eliminate ties-promote).
control type bit harder others imagine real world, somewhat
contrived example, consider following case. given organization, director splits
workers two study groups (and let us say choose partition likes, either
powerful director, good enough manager make justification
division) study problem propose thinks best alternative.
entire organization comes together vote among alternatives chosen first
round (that survive tie-handling rule case ties).
Name: E -CCPV E -DCPV (control via partition voters).
Given: set C candidates, collection V voters represented via preference lists C,
distinguished candidate p C.
Question (E -CCPV): partition V V1 V2 p winner twostage election winners election (C,V1 ) survive tie-handling rule compete
winners (C,V2 ) survive tie-handling rule? subelection (in
stages) conducted using election system E .
Question (E -DCPV): partition V V1 V2 p winner twostage election winners election (C,V1 ) survive tie-handling rule compete
winners (C,V2 ) survive tie-handling rule? subelection (in
stages) conducted using election system E .
U NIQUE W INNERS

RRATIONALITY

bribery control problems defined rational voters
nonunique-winner model, i.e., asking whether given candidate made, prevented
being, winner. Nonetheless, proven control results case nonunique
winners (to able fairly compare existing control results, uniquewinner model) unique winners (a candidate unique winner winner
winner). Similarly, bribery results proven unique-winner model (to
able fairly compare existing bribery results literature) nonunique-winner
model. addition rational-voters case, also study problems case voters
allowed irrational. mentioned earlier, case irrational voters, voters
represented via preference tables rather preference lists.
2.3 Graphs
undirected graph G pair (V (G), E(G)), V (G) set vertices E(G)
set edges edge unordered pair distinct vertices.7 directed graph defined
7. paper, symbols E V generally reserved elections voters, except just-introduced overloading mean sets edges vertices given graph. intended meaning E V clear
context, even proofs involve multiple elections graphs.

290

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

analogously, except edges represented ordered pairs. example, u v
distinct vertices undirected graph G G either edge e = {u, v} connects u v
doesnt. hand, G directed graph G either edge e = (u, v)
u v, edge e = (v, u) v u, e e , neither e e .
directed graph G, indegree vertex u V (G) number Gs edges enter u
(i.e., number edges form (v, u) E(G)). Similarly, outdegree u V (G)
number edges leave u (i.e., number edges form (u, v) E(G)).
2.4 NP-Complete Problems Reductions
Without loss generality, assume problems consider encoded natural,
efficient way alphabet = {0, 1}. use standard notion NP-completeness, defined
via polynomial-time many-one reductions. say computational problem polynomial-time
many-one reduces problem B exists polynomial-time computable function f
(x )[x f (x) B].
problem NP-hard members NP polynomial-time many-one reduce it. Thus,
NP-hard problem polynomial-time many-one reduces problem B, B NP-hard
well. problem NP-complete NP-hard member NP. clear context
use reduce reduction shorthands polynomial-time many-one reduce
polynomial-time many-one reduction.
NP-hardness results typically follow via reduction either exact-cover-by-3-sets
problem vertex cover problem (see, e.g., Garey Johnson, 1979). wellknown NP-complete problems, define sake completeness.
Name: X3C (exact cover 3-sets).
Given: set B = {b1 , . . . , b3k }, k 1, family sets = {S1 , . . . , Sn } i,
1 n, holds Si B kSi k = 3.
Question: set {1, . . . , n}, kAk = k,



iA Si

= B?

set ask problem called exact cover B. cover
every member B belongs Si A, exact
member B belongs exactly one Si A.
Whenever consider instances X3C problem, assume well-formed,
is, assume follow syntactic requirements stated Given field (e.g.,
cardinality set B indeed multiple three). apply convention considering
syntactically correct inputs problems well. Let computational problem
let x instance A. consider algorithm A, input x malformed,
immediately reject. building reduction problem B,
whenever hit malformed input x output fixed B. (In reductions B never
, always possible.)
Copeland elections often considered terms appropriate graphs. representation
particularly useful face control problems modify structure candidate
set, since case operations election directly translate suitable operations
291

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

corresponding graph. candidate control problems, weinstead using reductions X3C
construct reductions vertex cover problem. vertex cover undirected graph G
subset Gs vertices edge G adjacent least one vertex subset.
Name: VertexCover.
Given: undirected graph G nonnegative integer k.
Question: set W W V (G), kW k k, every edge e E(G) holds
e W 6= 0?
/
2.5 Resistance Vulnerability
election systems affected control type; not, system said immune
type control. example, candidate c Condorcet winner impossible
make Condorcet winner adding candidates (see Bartholdi et al., 1992, Hemaspaandra et al., 2007a, immunity results). However, Copeland elections easy
see type control defined Section 2.2 scenario outcome
election indeed changed via conducting corresponding control action. election
system immune type control (as witnessed scenario), election system
said susceptible control type.
Proposition 2.4 rational number , 0 1, Copeland susceptible type
control defined Section 2.2.
say election system (Copeland CopelandIrrational , case) resistant
particular attack (be type control bribery) appropriate computational problem
NP-hard susceptibility holds.8 hand, computational problem P
susceptibility holds, say system vulnerable attack. bribery
control problems defined, vulnerability definition merely requires exist
polynomial-time algorithm determines whether successful bribe control action exists
given input. However, every single one vulnerability proofs provide something
far stronger. provide polynomial-time algorithm actually finds successful bribe
control action input successful bribe control action exists, input
successful bribe control action exists announce fact.
notions resistance vulnerability (and immunity susceptibility) control
problems election systems introduced Bartholdi et al. (1992), follow
definition alteration Hemaspaandra et al. (2007b) resistance NP-complete NPhard, change compelling (because old definition, NP-completeness, things could
8. true unnatural election systems immunity bribery holds, e.g., election system Every candidate winner immune types bribery. However, Copeland -type systems susceptible
bribery types look paper, wont explicitly discuss state susceptibility bribery
cases.
referee asked whether definition resistance could equivalently stated simply requiring appropriate computational problem NP-hard. seems yield notion, P 6= NP yet
known result, one doesnt know NP-hard problems cannot possibly P, subtly susceptibility defined terms changing outcomes corresponding control problems NP-hardness (which
part determines resistance) related outcome (regardless started as).

292

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

actually become nonresistant hard, natural). However, resistance
claims paper NP-membership clear, NP-completeness fact hold.

3. Bribery
section present results complexity bribery Copeland election systems, rational number 0 1. main result, presented
Section 3.1, system resistant bribery, regardless voters rationality
mode operation (constructive versus destructive). Section 3.2, provide vulnerability results Llull Copeland0 respect microbribery.
3.1 Resistance Bribery
Theorem 3.1 rational , 0 1, Copeland CopelandIrrational resistant
constructive destructive bribery, nonunique-winner model unique-winner
model.
prove Theorem 3.1 via Theorems 3.2, 3.4, 3.5 below. proofs employ approach
call UV technique. constructive cases, technique proceeds constructing
bribery instances briberies could possibly ensure favorite candidate p
winner involve voters rank group special candidates (often group contain
exactly two candidates, u v) p. remaining voters, bystanders speak,
used create appropriate padding structure within election. destructive cases follow
via cute observation regarding dynamics constructive cases.
remainder section devoted proving Theorem 3.1. start case
rational voters Theorems 3.2 3.4 argue analogous results case
irrational voters follow via, essentially, proof.
Theorem 3.2 rational number , 0 1, Copeland resistant constructive
bribery unique-winner model destructive bribery nonunique-winner model.
Proof. Fix arbitrary rational number 0 1. proof provides reductions
X3C problem to, respectively, unique-winner variant constructive bribery
nonunique-winner variant destructive bribery. reductions differ regarding specification goal (i.e., regarding candidate attempt make unique winner
candidate prevent winner) thus describe jointly as, essentially, single
reduction.
reduction produce instance appropriate bribery problem odd number
voters, never ties head-to-head contests. Thus proof works regardless
rational number 0 1 chosen.
Let (B, ) instance X3C, B = {b1 , b2 , . . . , b3k }, collection {S1 , S2 , . . . , Sn }

three-element subsets B nj=1 j = B, k 1. input meet conditions output fixed instance bribery problem negative answer.
Construct Copeland election E = (C,V ) follows. candidate set C {u, v, p} B,
none u, v, p B. voter set V contains 2n + 4k + 1 voters following types.

293

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

1. Si , introduce one voter type (i) one voter type (ii):
(i) u > v > Si > p > B Si ,



(ii) B Si > p > u > v > Si .
2. introduce k voters types (iii)-1, (iii)-2, (iv)-1, (iv)-2:
(iii)-1
(iii)-2
(iv)-1
(iv)-2

u > v > p > B,
v > u > p > B,


u > B > p > v,


v > B > p > u.

3. introduce single type (v) voter:
(v) B > p > u > v.
following relative vote-scores:
1. vsE (u, v) = 2n + 1 2k + 1, inequality follows assumption
(which implies n kBk/3 = k),

Sn

j=1 j

=B

2. vsE (u, p) = vsE (v, p) = 2k 1,
3. {1, 2, . . . , 3k}, vsE (u, bi ) = vsE (v, bi ) 2k + 1,
4. {1, 2, . . . , 3k}, vsE (bi , p) = 1,
5. i, j {1, 2, . . . , 3k} 6= j, |vsE (bi , b j )| = 1.
example, see vsE (u, bi ) 2k + 1 {1, 2, . . . , 3k}, note bi

least one j nj=1 j = B, voters types (i) (ii) give u advantage
least two votes bi . Furthermore, voters types (iii)-1, (iii)-2, (iv)-1, (iv)-2 give u
advantage 2k additional votes bi , single type (v) voter gives bi one-vote
advantage u. Summing up, obtain vsE (u, bi ) 2 + 2k 1 = 2k + 1. relative
vote-scores similarly easy verify.
relative vote-scores yield following Copeland scores upper bounds scores:
1. scoreE (u) = 3k + 2,
2. scoreE (v) = 3k + 1,
3. {1, 2, . . . , 3k}, scoreE (bi ) 3k,
4. scoreE (p) = 0.
prove theorem, need following claim.
Claim 3.3 following three statements equivalent:
1. (B, ) X3C.
294

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

2. Candidate u prevented winning via bribing k voters E.
3. Candidate p made unique winner via bribing k voters E.
Proof Claim 3.3. (1) implies (2): easy see (B, ) X3C bribe
involving k fewer voters prevents u winner: enough bribe type (i)
voters correspond cover size k report p top choice (while changing
anything else preference lists): p > u > v > Si > B Si . Call resulting election E . E
following relative vote-scores change: vsE (p, u) = vsE (p, v) = n + k (n k) 2k + 1 = 1
vsE (p, bi ) 1 {1, 2, . . . , 3k}, relative vote-scores remain unchanged.
Thus scoreE (p) = 3k + 2, scoreE (u) = 3k + 1, scoreE (v) = 3k, scoreE (bi ) < 3k
{1, 2, . . . , 3k}, p defeats candidates unique winner. particular, bribe
(of k voters E) ensures u winner.
(2) implies (3): Suppose bribe involving k fewer voters prevents u
winner. Note u defeats everyone except p 2k votes E. means
via bribery k voters us score decrease one. Thus, prevent u
winner via bribery, need ensure u receives Copeland score 3k + 1
candidate u gets Copeland score 3k + 2, is, candidate defeats everyone.
Neither v bi possibly obtain Copeland score 3k + 2 via bribery,
since bribery k voters affect head-to-head contests relative vote-scores
participants 2k. Thus, via bribery, u prevented winning
p made (in fact, unique) winner election.
(3) implies (1): Let W set k voters whose bribery ensures p unique
winner election. Thus know kW k = k W contains voters rank
u v p (as otherwise p would defeat u v), case
voters types (i), (iii)-1, (iii)-2. Furthermore, bribery makes p unique winner
ensure p defeats members B; note type (iii)-1 (iii)-2 voters E already rank
p B. Thus, via simple counting argument, W must contain exactly k type (i) voters
correspond size-k cover B.
Claim 3.3
Since reduction computable polynomial time, Claim 3.3 completes proof
Theorem 3.2.

Theorem 3.4 rational , 0 1, Copeland resistant constructive bribery
nonunique-winner model destructive bribery unique-winner model.
proof Theorem 3.4, follows general structure proof
Theorem 3.2,9 reasons space nonrepetitiveness included found
full TR version (Faliszewski et al., 2008b).
9. Since proof Theorem 3.4 slightly involved, let us briefly mention key differences proof
Theorem 3.2. Starting X3C instance (B, ) kBk = 3k, case construct election E = (C,V )
two candidates (i.e., C = {p, s,t, u, v} B) V having, addition voter types similar
proof Theorem 3.2, 20k normalizing voters eight subtypes. unique winner E s,
candidate able prevent unique winner via k voters bribed p.
construction ensures (B, ) X3C exactly k voters bribed p tie winner,
simultaneously handles nonunique-winner constructive case unique-winner destructive case.

295

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

proofs theorems interesting feature. discuss bribery,
never rely fact voters rational. Thus allow voters irrational
form CopelandIrrational -bribery CopelandIrrational -destructive-bribery instances simply deriving voters preference tables voters preference lists given proofs.
easy see proofs remain valid change; proofs assume bribed
voter, bribery, prefers p candidates, make assumptions
(and, particular, use linearity preferences). Thus following corollary
proofs Theorems 3.2 3.4.
Theorem 3.5 rational number , 0 1, CopelandIrrational resistant constructive bribery destructive bribery, nonunique-winner model uniquewinner model.
Theorems 3.2, 3.4, Theorem 3.5 together constitute proof Theorem 3.1 establish
rational , 0 1, Copeland CopelandIrrational possess broadessentially
perfectresistance bribery regardless whether interested constructive destructive
results. However, next section shows perfect picture is, fact, near-perfect
consider microbribes, dont allow changing complete preferences voters
rather change results head-to-head contests candidates voters preferences.
show efficient way finding optimal microbriberies case irrational
voters Copeland elections.
3.2 Vulnerability Microbribery Irrational Voters
section explore problems related microbribery irrational voters. standard
bribery problems, considered Section 3.1, ask whether possible ensure
designated candidate p winner (or, destructive case, ensure p winner)
via modifying preference tables k voters. is, unit cost completely
redefine preference table voter bribed. model, pay service (namely,
modification reported preference table) pay bulk (when buy voter,
secured total obedience). However, sometimes may far reasonable
adopt local approach pay separately preference-table entry
flipto pay alter vote.
Throughout remainder section use term microbribe refer flipping
entry preference table, use term microbribery refer bribing possibly
irrational voters via microbribes. Recall irrational voters simply mean
allowed have, must have, irrational preferences.
study microbribery, consider irrational voters clearly natural model
study. all, one changing (and measuring overall change terms number
changes in) pairwise preferences, changes easily move one rational preference
irrational preference. (We mention passing one could define versions problem
case rational voters various ways, e.g., allowing changes stay rational
profiles. seems far less natural model use microbribery problem.)
rational , 0 1, define following two problems.
Name: CopelandIrrational -microbribery CopelandIrrational -destructive-microbribery.
296

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

Given: set C candidates, collection V voters specified via preference tables C,
distinguished candidate p C, nonnegative integer k.
Question (constructive): possible, flipping k entries preference tables
voters V , ensure p winner resulting election?
Question (destructive): possible, flipping k entries preference tables voters
V , guarantee p winner resulting election?
flip multiple entries preference table voter, pay separately flip. microbribery problems CopelandIrrational similar flavor
so-called bribery problems approval voting studied Faliszewski et al. (2006a),
unit cost flipping approvals disapprovals voters paid. However, proofs
CopelandIrrational seem much involved counterparts approval voting.
reason CopelandIrrational elections allow subtle complicated interactions
candidates scores.
proceed results, let us define notation useful throughout
section. Let E election candidate set C = {c1 , c2 , . . . , cm } voter collection V =
{v1 , v2 , . . . , vn }. define two functions, wincostE tiecostE , describe costs ensuring
victory tie given candidate particular head-to-head contest.
Definition 3.6 Let E = (C,V ) election let ci c j two distinct candidates C.
1. wincostE (ci , c j ) mean minimum number microbribes ensure ci defeats
c j head-to-head contest. ci already wins contest wincostE (ci , c j ) = 0.
2. tiecostE (ci , c j ) mean minimum number microbribes ensure ci ties
c j head-to-head contest, E odd number voters thus ties
impossible.
first result regarding microbribery destructive microbribery easy
CopelandIrrational . Since papers first vulnerability proof, take opportunity
remind reader (recall Section 2.5) although definition vulnerability requires
polynomial-time algorithm determine whether successful action (in present case,
destructive microbribery) exists, vulnerability proof provide something far stronger,
namely polynomial-time algorithm determines whether successful action exists
that, so, finds successful action (e.g., flow algorithms later on, successful action
implicit flow computed).
Theorem 3.7 rational , 0 1, CopelandIrrational vulnerable destructive microbribery nonunique-winner model unique-winner model.
Proof.
Fix arbitrary rational number 0 1. give algorithm
CopelandIrrational , destructive microbribery nonunique-winner model. (We omit analogous algorithm unique-winner case.)
Let E = (C,V ) input election C = {p, c1 , c2 , . . . , cm } V = {v1 , v2 , . . . , vn },
let k number microbribes allowed make. define predicate
297

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

M(E, p, ci , k) true microbribery cost k ensures ci
score higher p. algorithm computes M(E, p, ci , k) ci C accepts
true least one them. describe compute M(E, p, ci , k).10
applying appropriate minimum-cost microbriberies E, obtain elections E1 , E2 , E3
identical E except
1. E1 , p defeats ci head-to-head contest,
2. E2 , p loses ci head-to-head contest,
3. E3 , p ties ci head-to-head contest (we disregard E3 number voters odd
thus ties impossible).
Let k1 , k2 , k3 minimum costs microbriberies transform E E1 , E E2 , E
E3 , respectively. microbriberies involve head-to-head contest p ci .
define predicate (E , p, ci , k ), E {E1 , E2 , E3 } k integer, true
microbribery cost k involve head-to-head contest
p ci ensures ci CopelandIrrational score higher ps. easy see


M(E, p, ci , k) (E1 , p, ci , k k1 ) (E2 , p, ci , k k2 ) (E3 , p, ci , k k3 ) .

Thus enough focus problem computing (E , p, ci , k ).
Let (E , k ) one (E1 , k k1 ), (E2 , k k2 ), (E3 , k k3 ). Define promoteE (ci , w , w ,t),
ci C candidate w , w , nonnegative integers, minimum cost
microbribery that, applied E , increases ci CopelandIrrational score w + (1 )w +
via ensuring
1. ci wins additional w head-to-head contests candidates C {p} used
defeat ci originally,
2. ci wins additional w head-to-head contests candidates C {p} ci
used tie originally,
3. ci ties additional head-to-head contests candidates C {p} used defeat ci
originally.

microbribery exist set promoteE (ci , w , w ,t) . easy
exercise see promoteE computable polynomial time simple greedy algorithm.
define demoteE (ci , , ,t) minimum cost microbribery that, applied
election E , decreases ps score + + (1 )t via ensuring
1. p loses additional head-to-head contests candidates C {ci } p used defeat
originally,
2. p loses additional head-to-head contests candidates C {ci } p used
tie originally,
10. stress optimized algorithm simplicity rather performance.

298

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

3. p ties additional head-to-head contests candidates C {ci } p used defeat
originally.
microbribery exist set demoteE (ci , , ,t) . Note demoteE
computed polynomial time using algorithm similar promoteE .
Naturally, microbriberies used implicitly within promoteE (ci , w , w ,t ), within
demoteE (ci , , ,t ), within transforming E E disjoint, i.e., never involve
pair candidates. Thus (E , p, ci , k ) true exist integers
w , w , , ,t ,t {0, 1, . . . , m}
scoreE (ci ) + (w + + (1 )(t + w ) + (t + )) scoreE (p) > 0

promoteE (ci , w , w ,t ) + demoteE (ci , , ,t ) k.
polynomially many combinations w , w , , ,t , , try
all. Thus given polynomial-time algorithm (E , p, ci , k ). Via observations given
beginning proof implies M(E, p, ci , k) computable polynomial time
proof complete.

destructive-case algorithm approach fairly straightforward; destructive
case need worry side effects promoting c demoting p. constructive
case complicated, still able obtain polynomial-time algorithms via fairly
involved use flow networks model particular points shift candidates.
remainder section restrict values {0, 1} settings number
voters odd ties never happen. remind reader Copeland1 Copeland1Irrational ,
respectively, refer Llull voting.
flow network network nodes directed edges want transport
amount flow source sink (these two designated nodes). edge e
carry c(e) units flow, transporting unit flow e costs a(e).
min-cost-flow problem target flow value F, goal find way transporting F
units flow source sink, minimizing cost. (If way achieving
target flow F, cost effect infinite.)
define notions related flow networks formally. Let N = {0, 1, 2, . . .}
Z = {. . . , 2, 1, 0, 1, 2, . . .}.
Definition 3.8
1. flow network quintuple (K, s,t, c, a), K set nodes
includes source sink t, c : K 2 N capacity function, : K 2 N
cost function. assume c(u, u) = a(u, u) = 0 node u K,
one c(u, v) c(v, u) nonzero pair distinct nodes u, v K. also assume
c(u, v) = 0 a(u, v) = 0 well.
2. Given flow network (K, s,t, c, a), flow function f : K 2 Z satisfies following
conditions:
(a) u, v K, f (u, v) c(u, v), i.e., capacities limit flow.

299

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

(b) u, v K, f (u, v) = f (v, u).11
(c) u K {s,t}, vK f (u, v) = 0, i.e., flow conserved nodes
except source sink.
3. value flow f is:
flowvalue( f ) =

f (s, v).

vK

particular flow network mind always clear context
indicate explicitly (we write explicitly subscript function flowvalue).
4. cost flow f defined as:
flowcost( f ) =



a(u, v) f (u, v).

u,vK

is, pay price a(u, v) unit flow passes node u node v.
Given flow network (K, s,t, c, a) often use term edges refer pairs distinct
nodes (u, v) K 2 c(u, v) > 0.
define min-cost-flow problem, well known literature.
definition employ general one suffice needs. (Readers
seeking broader discussion problem may wish see, example, monograph Ahuja,
Magnanti, Orlin, 1993.)
Definition 3.9 define min-cost-flow problem follows: Given flow network (K, s,t, c, a)
target flow value F, find flow f value F (if one exists) minimum cost among
flows, otherwise indicate flow f exists.
min-cost-flow problem polynomial-time algorithm.12 large body work
devoted flow problems even attempt provide complete list references
here. Instead, point reader excellent monograph Ahuja et al. 1993,
provides descriptions polynomial-time algorithms, theoretical analysis, numerous references
previous work flow-related problems. also mention issue flows prevalent
study algorithms textbook Cormen, Leiserson, Rivest, Stein 2001, page 787,
contains exposition min-cost-flow problem.
Coming back study constructive microbribery Llull Copeland0 , irrational
voters allowed, present following result.
Theorem 3.10 {0, 1}, CopelandIrrational vulnerable constructive microbribery,
nonunique-winner model unique-winner model.
11. Note flow fully defined via nonnegative values. Whenever speak flow (e.g., defining
particular flows) speak nonnegative part.
12. min-cost-flow problem often defined terms capacity cost functions necessarily limited
nonnegative integer values corresponding flows restricted integer values either. However,
crucially us, known capacity cost functions integral values (as assumed)
exist optimal solutions min-cost-flow problem use integer-valued flows found
polynomial time.

300

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

Edge
e = (s, ci ),
ci C
e = (ci , c j ),
ci , c j C vsE (ci , c j ) > 0
e = (c0 ,t)
e = (ci ,t),
> 0 ci C
every edge e

Parameters
c(e) = scoreE (ci )
a(e) = 0
c(e) = 1
a(e) = wincostE (c j , ci )
c(e) =
a(e) = 0
c(e) =
a(e) = B
c(e) = 0
a(e) = 0

Figure 1: Edge capacities costs min-cost-flow instance I(T ), built election E.
prove Theorem 3.10 via Lemmas 3.11 3.16 below, cover three cases: (a)
odd number voters, CopelandIrrational elections 0 1 identical due
lack ties, (b) Copeland1Irrational even number voters, (c) Copeland0Irrational
even number voters. lemmas discuss nonunique-winner model case
easy see change algorithms proofs make work unique-winner
model.
Lemma 3.11 rational 0 1, polynomial-time algorithm solves
constructive microbribery problem CopelandIrrational elections odd number voters
(in nonunique-winner model).
Proof. input nonnegative integer k (the budget) election E = (C,V ),
candidate set C {c0 , c1 , . . . , cm }, number voters odd, p = c0 candidate whose
victory want ensure via k microbribes. Note interchangeably use p c0
refer candidate, since sometimes convenient able speak p
candidates uniformly. number voters odd, ties never occur. Thus candidate ci
CopelandIrrational score rational value , 0 1. Fix arbitrary .
give polynomial-time algorithm constructive microbribery problem. high-level
overview try find threshold value microbribery cost
k transforms E E (a) p scoreE exactly , (b) every candidate
scoreE .
Let B number greater cost possible microbribery within E (e.g.,
B = kV k kCk2 + 1). possible threshold , consider min-cost-flow instance I(T )
node set K = C {s,t}, source sink, edge capacities costs
specified Figure 1, target flow value
F=

scoreE (ci ) =

ci C

301

kCk(kCk 1)
.
2

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

Voter 1 :
Voter 2 :
Voter 3 :

vsE (ci , c j )
c0
c1
c2
c3

c0 > c1 > c2 > c3
c3 > c2 > c1 > c0
c2 > c0 > c3 > c1

c0
0
1
1
1

c1
1
0
1
1

c2
1
1
0
1

c3
1
1
1
0

Figure 2: Sample election E Example 3.12 proof Lemma 3.11.

(2, 0)
(0, 0)


c0

(T, 0)

(1, 1)
(1, 1)

(1, 1)
c1

(T, 49)
(1, 1)

(3, 0)


(T, 49)
c2

(1, 1)
(1, 0)

(1, 1)
c3

(T, 49)

Figure 3: Flow network I(T ) corresponding instance (E, c0 , k) Example 3.12.
Example 3.12 illustration, consider following example. Suppose given election E
four candidates three voters, preference tables voters (who happen
rational example) obtained preference orders shown Figure 2,
also gives corresponding values vsE (ci , c j ) pair candidates. Thus
scoreE (c0 ) = 2, scoreE (c1 ) = 0, scoreE (c2 ) = 3, scoreE (c3 ) = 1. Suppose
allowed perform one microbribe, k = 1. Clearly, one microbribe changes preference
third voter c2 > c0 c0 > c2 flip outcome head-to-head contest c2
winning c0 winning, enough reach goal making c0 win election,
course cheapest possible successful microbribery. Finally, note example
B = 49.
threshold 0 3, flow network I(T ) corresponding instance
(E, c0 , k) constructive microbribery problem shown Figure 3, target flow
value F = 6. Every edge e flow network labeled pair (c(e), a(e)) numbers
give capacity cost edge e, respectively.
continue proof Lemma 3.11, note odd number voters, constructive
microbribery CopelandIrrational simply requires us choose pairs distinct candidates
want flip outcome head-to-head contest order ensure ps victory. Thus
sufficient represent microbribery collection pairs (ci , c j ) distinct candidates
need flip result head-to-head contest ci winning c j winning. Clearly,
302

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

given collection M, cheapest way implement costs



wincostE (c j , ci ).

(ci ,c j )M

crucial observation algorithm directly translate flows microbriberies
using following interpretation. Let f flow (as per Definition 3.8 edge flows integers) value F within instance I(T ). units flow travel network correspond
CopelandIrrational points. ci C, interpret amount flow goes directly
ci number CopelandIrrational points ci microbribery attempted,13
amount flow goes directly ci number CopelandIrrational points
ci microbribery (defined flow). units flow travel distinct ci
(i.e., edges form (ci , c j ), 6= j) correspond microbribes exerted: unit flow
traveling node ci c j corresponds changing result head-to-head contest
ci c j ci winning c j winning. case, CopelandIrrational point moves ci
c j cost flow increases a(ci , c j ) = wincost(c j , ci ), exactly minimum cost
microbribery flips contests result. Let f microbribery defined, described,
flow f . easy see
flowcost( f ) = B (F f (c0 ,t)) +



wincostE (c j , ci ).

(ci ,c j )M f

Thus easily extract cost microbribery f cost flow f .
algorithm crucially depends correspondence flows microbriberies.
(Also, proofs Lemmas 3.14 3.16 cover case even number voters
simply show modify instances I(T ) handle ties, show correspondences
new networks microbriberies; rest proofs here.)
Note small values flow value F exists I(T ). reason
edges coming sink might enough capacity hold flow value F.
situation impossible guarantee every candidate gets points;
many CopelandIrrational points distribute.
Figure 4 gives algorithm constructive microbribery CopelandIrrational . algorithm
runs polynomial time since, already mentioned, min-cost-flow problem solvable
polynomial time.
Let us prove algorithm correct. presented flow f value
F within flow network I(T ) (with 0 F) defines microbribery. Based this, clear
algorithm accepts microbribery cost k ensures ps victory.
hand, suppose exists microbribery cost k ensures
ps victory election. show algorithm accepts case.
Let minimum-cost bribery (of cost k) ensures ps victory. pointed
above, represented collection pairs (ci , c j ) distinct candidates flip
result head-to-head contest ci winning c j winning. cost



wincostE (c j , ci ).

(ci ,c j )M

13. Note ci C flow value F within I(T ) needs send exactly scoreE (ci ) units ci .

303

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

procedure CopelandIrrational -odd-microbribery(E = (C,V ), k, p)
begin
p winner E accept;
;
F = ci C scoreE (ci ) = kCk(kCk1)
2
= 0 kCk 1
begin
build instance I(T ) min-cost-flow;
I(T ) flow value F
restart loop next value ;
f = minimum-cost flow I(T );
f (c0 ,t) < restart loop;
= flowcost( f ) B (F );
k accept;
end;
reject;
end

Figure 4: constructive microbribery algorithm CopelandIrrational elections odd number voters.

Since applying microbribery ensures p winner, candidate among
c1 , c2 , . . . , cm many CopelandIrrational points p does. Let E election
results E applying microbribery E (i.e., flipping results contests
specified optimal way, given wincostE ). Let scoreE (p), ps CopelandIrrational
score implementing M. Clearly, 0 kCk 1.
Consider instance I(T ) let fM flow corresponds microbribery M.
flow edge form (s, ci ) carries flow maximum capacity, scoreE (ci ), edge
form (ci , c j ) carries one unit flow exactly e listed carries zero units flow
otherwise, edge form (ci ,t) carries scoreE (ci ) units flow. easy see
legal flow. cost fM
flowcost( fM ) = B (F ) +



wincostE (c j , ci ).

(ci ,c j )M

applying M, p gets CopelandIrrational points travel sink via edge (c0 ,t) cost
a(c0 ,t) = 0, remaining F points travel via edges (ci ,t), {1, 2, . . . , m}, cost
a(ci ,t) = B. remaining part flowcost( fM ) cost units flow traveling
edges (ci , c j ) directly correspond cost microbribery M.
consider minimum-cost flow fmin I(T ). Since fM exists, minimum-cost flow
must exist well. Clearly,
flowcost( fmin ) flowcost( fM ).


Let number units flow fmin assigns travel edge (c0 ,t), i.e.,
= fmin (c0 ,t). edges nonzero cost sending flow
304

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

set {(ci , c j ) | ci , c j C vsE (ci , c j ) > 0} {(ci ,t) | {1, . . . , m}} thus cost fmin
expressed (recall vsE (ci , c j ) > 0 implies 6= j)



flowcost( fmin ) = B (F ) +

fmin (ci , c j ) wincostE (c j , ci ).

ci ,c j CvsE (ci ,c j )>0

holds (1) B > i, j,i6= j wincostE (ci , c j ), (2) ci , c j C vsE (ci , c j ) > 0
fmin (ci , c j ) {0, 1}, (3) flowcost( fmin ) flowcost( fM ). = must hold14 must
hold





fmin (ci , c j ) wincostE (c j , ci )

wincostE (c j , ci ).

(ci ,c j )M

ci ,c j CvsE (ci ,c j )>0

Thus flow fmin corresponds microbribery guarantees ps victory cost
high M. Since chosen minimum cost among microbriberies, flow
fmin corresponds microbribery minimum cost algorithm correctly accepts within
loop Figure 4, latest loop set .

turn algorithms showing Llull Copeland0 , irrational voters allowed,
vulnerable constructive microbribery number voters even. need
take account sometimes desirable candidates tie
head-to-head contest one win contest. handle cases Llull
Copeland0 separately, case proofs follow general structure. case
first provide lemma restricts set microbriberies model, use slightly
modified version algorithm Theorem 3.11, modified set min-cost-flow instances,
solve thus limited microbribery problem. omit proofs remaining four lemmas
section somewhat lengthy repetitive. However, proofs found
full TR version paper (Faliszewski et al., 2008b).
Lemma 3.13 Let E = (C,V ) election candidate set C = {c0 , c1 , . . . , cm }
even number voters, specified via preference tables C. election conducted using Copeland0Irrational minimum-cost microbribery ensures victory c0 involves either
(a) flipping result head-to-head contest two distinct candidates ci , c j C {c0 }
ci winning c j winning, (b) changing result head-to-head contest two
distinct candidates C {c0 } tie one winning.
14. Let us explain = . I(T ), definition, c(c0 ,t) = , know = fmin (c0 ,t) .
show = . sake contradiction, let us assume < .
flowcost( fmin )

=



B (F ) +

fmin (ci , c j ) wincostE (c j , ci )

ci ,c j CvsE (ci ,c j )>0



B (F ) + B +



fmin (ci , c j ) wincostE (c j , ci )

ci ,c j CvsE (ci ,c j )>0

>

B (F ) +



wincostE (c j , ci )

(ci ,c j )M

=

flowcost( fM ),

last inequality follows fact B greater cost microbribery within E.
reached contradiction, since fmin minimum-cost flow I(T ). Thus = .

305

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

Lemma 3.14 polynomial-time algorithm solves constructive microbribery problem Copeland0Irrational elections even number voters (in nonunique-winner model).
Lemma 3.15 Let E = (C,V ) election candidate set C = {c0 , c1 , . . . , cm }
even number voters, specified via preference tables C. election conducted using
Copeland1Irrational minimum-cost microbribery ensures victory c0 involves obtaining
tie head-to-head contest two distinct candidates C {c0 }.
Lemma 3.16 polynomial-time algorithm solves constructive microbribery problem Copeland1Irrational elections even number voters (in nonunique-winner model).
Together, Theorem 3.7 Lemmas 3.11, 3.14, 3.16 show that, particular,
Copeland1Irrational Copeland0Irrational vulnerable microbribery, constructive
destructive settings. interesting note microbribery proofs would work
well considered slight twist definition microbribery problem, namely, instead
saying flip voters preference table unit cost would allow voter
possibly different price flipping separate entry preference table. change
would affect computing values functions wincost tiecost (or, strictly speaking, analogues priced setting). (Technically, would also modify Lemmas 3.13
3.15, unit-cost setting say optimal microbribery never involves certain specified pairs candidates, whereas priced setting would need rephrase state
exist optimal microbriberies involve specified pairs candidates.)
interesting direction study complexity bribery within Copeland systems
consider version microbribery problem case rational voters. There, one would
pay unit cost switch two adjacent candidates given voters preference list.
CopelandIrrational , would also like know complexity constructive microbribery
rational number strictly 0 1. network-flow-based approach
seem generalize easily values strictly 0 1 (when number voters even)
flow network hard split unit flow tie. promising approach would
several units flow model one CopelandIrrational point (e.g., case = 12 could
try use two units flow model single Copeland0.5 point), seems difficult
(if impossible) find edge costs appropriately model microbribery. (It possible
restricted setting, namely = 12 exactly two voters
bribed.) Also, results regarding hardness manipulation Faliszewski et al. (2008) suggest
microbribery strictly 0 1 might NP-hard. However, again, nontrivial
translate reduction world microbribery.
related note, Kern Paulusma (2001) shown following problem,
call SC(0, , 1), NP-complete. Let rational number 0 < < 1 6= 21 .
given undirected graph G = (V (G), E(G)), vertex u V (G) assigned
rational value cu form + j , nonnegative integers j. question,
rephrased state terms (a variant of) notion Copeland , whether possible
(possibly partially) orient edges G vertex u V (G) holds us
Copeland score cu . Here, Copeland score vertex u mean, natural,
number vertices u defeats (i.e., number vertices v directed edge
u v) plus times number vertices u ties (i.e., number vertices
undirected edge u v).
306

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

Problem SC(0, , 1) closely related microbribery problem. However,
see immediate reduction SC(0, , 1) microbribery. natural approach would
embed graph G election (in sense explored Section 4) way
preferred candidate p become winner, via microbribery, possible
orient edges G way respecting constraints defined values cu (for u
V (G)). would, course, set budget microbribery high enough allow
modifying edges G none edges outside G. However, difficult.
proof Kern Paulusma uses values cu implemented via using tied headto-head contests. agent performing microbribery could, potentially, affect head-to-head
contests, thus spoiling reduction.

4. Control
section focus complexity control Copeland elections. control problems
trying ensure preferred candidate p winner (or, destructive case,
despised candidate winner) given election via affecting elections structure (namely,
via adding, deleting, partitioning either candidates voters). contrast bribery problems,
control problems never allowed change votes and, consequently, issues
encounter proof techniques use quite different presented
previous section. reason previously standard type control resistance
result rational-voters case implies analogous resistance result irrational-voters case,
vulnerability result irrational-voters case implies analogous vulnerability result
rational-voters case.
literature regarding complexity control problems large. best
knowledge, election systems comprehensive analysis conducted previously plurality, Condorcet, (variants of) approval voting (see Bartholdi et al., 1992; Hemaspaandra et al., 2007a, 2007b; Betzler Uhlmann, 2008; Erdelyi et al., 2008b; see also Meir et al.,
2008, results (variants of) approval voting, single nontransferable vote, cumulative
voting respect constructive control via adding voters). Among plurality, Condorcet, (the
standard variant of) approval voting, plurality appears least vulnerable control
natural compare new results plurality. However, mention passing
Hemaspaandra et al. (2007b) show construct hybrid election systems resistant
standard types control (including AC ACu ; AC discussed proven Hemaspaandra et al., 2007bthe AC ACu mention techniques clearly
handle without problem). (It also noted hybrid systems
designed natural systems applied real-world elections rather purpose
prove certain impossibility theorem impossible.)
main result section Theorem 4.1.
Theorem 4.1 Let rational number 0 1. Copeland elections resistant
vulnerable control types indicated Table 1 nonunique-winner model
unique-winner model, rational irrational voter model.
particular, prove section notion widely referred literature
simply Copeland elections, clarity call Copeland0.5 , possesses ten
basic types (see Table 1) constructive resistance (and addition, even constructive ACu
307

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

Control type
ACu
AC
DC
RPC-TP
RPC-TE
PC-TP
PC-TE
PV-TE
PV-TP
AV
DV

Copeland
=0
0< <1
CC DC CC DC
V
V
R
V
R
V
R
V
R
V
R
V
R
V
R
V
R
V
R
V
R
V
R
V
R
V
R
V
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R

Plurality

=1
CC DC
V
V
R
V
R
V
R
V
R
V
R
V
R
V
R
R
R
R
R
R
R
R

CC
R
R
R
R
R
R
R
V
R
V
V

DC
R
R
R
R
R
R
R
V
R
V
V

Table 1: Comparison control results Copeland elections, 0 1
rational number, plurality-rule elections. R means resistance particular control
type V means vulnerability. results regarding plurality due Bartholdi et al.
(1992) Hemaspaandra et al. (2007a). (Note CCAC DCAC resistance results
plurality, handled explicitly Bartholdi et al., 1992, Hemaspaandra et al., 2007a,
follow immediately respective CCACu DCACu results.)

resistance). (As consider AC basic ACu , see discussion Control via
Adding Candidates subpart Section 2.2. Nonetheless, obtain ACu results, fans
naturalness ACu know things fare control type.) establish
notion literature occasionally referred Copeland elections, namely
Copeland0 , well Llull elections, denoted Copeland1 , possess ten
basic types constructive resistance. However, show Copeland0 Copeland1
vulnerable eleventh type constructive control, incongruous historically resonant
notion constructive control adding unlimited number candidates (i.e., CCACu ).
Note Copeland0.5 higher number constructive resistances, three, even
plurality, paper reigning champ among natural election systems
polynomial-time winner-determination procedure. (Although results regarding plurality
Table 1 stated unique-winner version control, tables Copeland cases,
0 1, results hold cases unique winners nonunique winners,
regardless two winner models one finds natural, one know holds
model.) Admittedly, plurality perform better respect destructive candidate control
problems, still study Copeland makes significant steps forward quest fully
control-resistant natural election system easy winner problem.
Among systems polynomial-time winner problem, Copeland0.5 indeed
Copeland , 0 < < 1have resistances currently known natural election system whose voters vote giving preference lists. mention work, Erdelyi et al.
(2008b) shown variant variant approval voting proposed Brams Sanver (2006)a certain rather subtle election system richer voter preference type (each voter
308

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

specifies permutation set) combines approval preference-based votinghas
nineteen (out possible twenty-two) control resistances.
section organized follows. next two sections devoted proving Theorem 4.1,
Section 4.3 considers case control elections bounded number candidates
voters. particular, Section 4.1 focuses upper part Table 1 studies control problems
affect candidate structure. Section 4.2 devoted voter control covers lower part
Table 1. Finally, Section 4.3 study fixed-parameter complexity control problems.
particular, take role someone tries solve in-general-resistant control problems
devise efficient algorithms case number candidates number
voters bounded.
resistance results regarding candidate control follow via reductions vertex cover
vulnerability results follow via greedy algorithms. resistance results case
control modifying voter structure follow reductions X3C problem.
4.1 Candidate Control
start discussion candidate control Copeland results destructive control.
somewhat disappointing rational , 0 1, Copeland vulnerable type
destructive candidate control. positive side, vulnerability proofs follow via natural
greedy algorithms allow us smoothly get spirit candidate-control problems.
4.1.1 ESTRUCTIVE C ANDIDATE C ONTROL
results destructive control adding deleting candidates use following observation.
Observation 4.2 Let (C,V ) election, let rational number 0 1.
every candidate c C holds
score(C,V ) (c) =



score({c,d},V ) (c).

dC{c}

Theorem 4.3 rational number 0 1, Copeland vulnerable destructive
control via adding candidates (both limited unlimited, i.e., DCAC DCACu ),
nonunique-winner model unique-winner model, rational irrational
voter model.
Proof. input set C candidates, set spoiler candidates, collection V voters
preferences (either preference lists preference tables) C D, candidate p C,
nonnegative integer k (for unlimited version problem let k = kDk). ask whether
subset kD k k p winner (is unique winner)
Copeland election E = (C ,V ). Note k = 0, amounts determining whether p
winner (is unique winner) election E, easily done polynomial time.
remainder proof assume k > 0. Let c candidate (C D)
{p}. define a(c) maximum value expression
score(CD ,V ) (c) score(CD ,V ) (p)

309

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

conditions D, c C , kD k k. Observation 4.2 follows
a(c) maximum value


score(C{c},V ) (c) score(C{c},V ) (p) +
score({c,d},V ) (c) score({p,d},V ) (p)
dD {c}

conditions D, c C , kD k k.
Clearly, p prevented winner (a unique winner) exists
candidate c (C D) {p} a(c) > 0 (such a(c) 0).
Given candidate c (C D) {p}, easy construct polynomial time set D,

kD k k, yields value a(c). start = 0.
/ c D, add c . add
candidates score({c,d},V ) (c) score({p,d},V ) (p) positive, starting
value highest, kD k = k candidates exist.

Theorem 4.4 rational number 0 1, Copeland vulnerable destructive control via deleting candidates (DCDC), nonunique-winner model uniquewinner model, rational irrational voter model.
proof Theorem 4.4 similar Theorem 4.3, include
instead refer full TR version (Faliszewski et al., 2008b).
Destructive control via partitioning candidates (with without run-off) also easy. Since
arguments proof involved, present here.
Theorem 4.5 rational number 0 1, Copeland vulnerable destructive
control via partitioning candidates via partitioning candidates run-off (in
TP TE model, i.e., DCPC-TP, DCPC-TE, DCRPC-TP, DCRPC-TE), nonuniquewinner model unique-winner model, rational irrational voter model.
Proof. easy see TP model, p prevented winner via partitioning candidates (with without run-off) set C C p C
p winner (C ,V ). follows p prevented winner
p prevented winner deleting kCk 1 candidates,
determined polynomial time Theorem 4.4. show handle unique-winner
variants DCPC-TP DCRPC-TP later proof.
TE model, easy see set C C p C p
unique winner (C ,V ) p prevented unique winner via partitioning
candidates (with without run-off). One simply partitions candidates C C C
thus p fails advance final stage. hand, p prevented
winner (a unique winner) via partitioning candidates (with without run-off) TE model,
exists set C C p C p unique winner (C ,V ).
either p advance final stage (and means p unique
winner first-stage election) p winner (not unique winner) final stage
(note winner implies unique winner).
Thus, p prevented winner (a unique winner) via partitioning candidates
(with without run-off) TE model set C C p C p
310

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

unique winner (C ,V ). Clearly, set exists p prevented
unique winner via deleting kCk 1 candidates, Theorem 4.4 tested
polynomial time.
remains show Copeland vulnerable destructive control via partitioning candidates (with without run-off), rational irrational voter model, uniquewinner model TP tie-handling rule. argument focus DCRPC-TP
case easy see essentially reasoning works DCPC-TP.
First determine whether p precluded winner current control
scenario. done polynomial time explained above. p precluded
winner, p certainly precluded unique winner, done.
remainder proof, suppose p cannot precluded winner current
control scenario, i.e., every set C p D, p winner (D,V ). Let
D1 = {c C {p} | p defeats c head-to-head contest}
let D2 = (D1 {p}). Note c D2 , p ties c head-to-head contest, since
otherwise p would winner ({c, p},V ). D2 = 0,
/ p Condorcet winner
partition (with without run-off) prevent p unique winner (Hemaspaandra
et al., 2007a). remainder proof, assume D2 6= 0.
/ show p
precluded unique winner current control scenario.
< 1, let first subelection (D1 {p},V ). Note p unique winner
subelection. final stage election involves p one candidates D2 . Note
every pair candidates D2 {p} tied head-to-head election (since c would defeat
head-to-head election, c would unique winner ({c, d, p},V ), contradicts
assumption p winner every subelection participates in). follows candidates
participate final stage election winners, p unique winner.
Finally, consider case = 1. score(C,V ) (p) = kCk 1. candidate
C {p} score(C,V ) (d) = kCk 1, always (i.e., every subelection containing d) winner, thus p unique winner final stage election,
regardless partition C chosen. suppose score(C,V ) (d) < kCk 1
C {p}. score(C,V ) (d) kCk 2 C {p}. Let c candidate D2
let first subelection (C {c},V ). Let C set winners (C {c},V ). Since
score(C{c},V ) (p) = kCk 2, holds p C every C {p}, score(C{c},V ) (d) =
kCk 2. Since score(C,V ) (d) kCk 2, follows c defeats head-to-head election.
final stage election involves candidates C {c}. Note score(C {c},V ) (c) = kC k, thus
c winner election, precluded p unique winner.

vulnerability results case destructive candidate control contrasted
essentially perfect resistance constructive candidate control (with exception constructive control via adding unlimited number candidates Copeland {0, 1})
shown Section 4.1.3. first, Section 4.1.2, provide technical prerequisites.

311

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

4.1.2 C ONSTRUCTING NSTANCES E LECTIONS
Many proofs next section require constructing fairly involved instances Copeland
elections. section provide several lemmas observations simplify building
instances.
first note election E = (C,V ) induces directed graph G(E) whose vertices
Es candidates whose edges correspond results head-to-head contests E.
is, two distinct vertices G(E) (i.e., two distinct candidates), b,
edge b defeats b head-to-head contest (i.e.,
vsE (a, b) > 0). Clearly, G(E) depend value . following fundamental result
due McGarvey. result allows us basically identify elections election graphs
proofs resistance candidate control. effect, Copeland candidate-control problems
often viewed (with care regarding ties) graph-theoretic problems.
Lemma 4.6 (McGarvey, 1953) polynomial-time algorithm given input antisymmetric directed graph G outputs election E G = G(E).
Proof. sake completeness, give algorithm. Let G antisymmetric directed
graph. algorithm computes election E = (C,V ), C = V (G) edge (a, b)
G exactly two voters, one preference list > b > C {a, b} one preference

list C {a, b} > > b. Since G antisymmetric, easy see G = G(E).

basic construction McGarvey improved upon Stearns (1959). McGarveys construction requires twice many voters edges G, construction
Stearns needs kV (G)k + 2 voters. Stearns also provides lower bound number
voters needed represent arbitrary graph via election. (It easy see
graph modeled via two irrational voters lower bound case rational votes
somewhat harder.)
often construct complicated elections via combining simpler ones (see, particular,
rather involved proofs Theorems 4.12 4.16 found full TR version,
Faliszewski et al., 2008b). Whenever speak combining two elections, say E1 = (C1 ,V1 )
E2 = (C2 ,V2 ), mean building, via algorithm Lemma 4.6, election E = (C,V ) whose
election graph disjoint union election graphs E1 E2 with, possibly, edges
added vertices G(E1 ) G(E2 ) (in case explicitly state edges,
any, added). particular, often want add padding candidates election,
without affecting original election much. order so, typically combine
main election one following padding elections. Note construction,
originally developed use study control Copeland voting, also proven useful
study manipulation Copeland (Faliszewski et al., 2008).
Lemma 4.7 Let rational number 0 1. positive integer n,
polynomial-time (in n) computable election Padn = (C,V ) kCk = 2n + 1
candidate ci C holds scorePadn (c) = n.
Proof. Fix positive integer n. Lemma 4.6 enough construct (in polynomial time n)
directed, antisymmetric graph G 2n + 1 vertices, indegree outdegree equal
312

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

n. set Gs vertex set {0, 1, . . . , 2n} put edge vertex vertex j (i 6= j)
( j i) mod (2n + 1) n. result exactly one directed edge every
two distinct vertices vertex edges going exactly vertices
(i + 1) mod (2n + 1), (i + 2) mod (2n + 1), . . . , (i + n) mod (2n + 1). Thus, indegree
outdegree vertex equal n proof complete.

Lemma 4.6 (McGarvey, 1953) useful building election need direct
control results head-to-head contests. However, many cases explicitly specifying results head-to-head contests would tedious. Instead would easier
specify results important head-to-head contests require candidates
certain suitable scores. next lemma show construct elections specified
way via combining small election containing important head-to-head contest large
padding election. mention generalized version lemma since used study
manipulation Copeland (Faliszewski et al., 2008).
Lemma 4.8 Let E = (C,V ) election C = {c1 , . . . , cn }, let rational number
0 1, let n n integer. candidate ci denote number headto-head ties ci E ti . Let k1 , . . . , kn sequence n nonnegative integers
ki 0 ki n. algorithm polynomial time n outputs election
E = (C ,V ) that:
1. C = C D, = {d1 , . . . , d2n2 },
2. E restricted C E,
3. ties head-to-head contests E candidates C,
4. i, 1 n , scoreE (ci ) = 2n2 ki + ti ,
5. i, 1 2n2 , scoreE (di ) n2 + 1.
Proof. build E via combining E padding election F (see Lemma 4.7 paragraph
it). F = (D,W ), = {d1 , . . . , d2n2 }, essentially election Padn2 one
arbitrary candidate removed. partition candidates n groups, D1 , . . . , Dn ,
exactly 2n candidates set results head-to-head contests ci C
candidates according following scheme. j {1, . . . , n } 6= j, ci
defeats members j ci defeats exactly many candidates Di (and loses
remaining ones) needed ensure
scoreE (ci ) = 2n2 ki + ti .
easy see possible: ci score (C Di ,V ) 2n2 2n + k + ti k
0 k n ti . 2n candidates Di ci reach score form
2n2 k + ti , k integer 0 n, via defeating head-to-head contests
appropriate number candidates Di losing remaining ones.
Finally, since F Padn2 one candidate removed, di gets n2 points
defeating members one point possibly defeating member C.

Thus, di D, holds scoreE (di ) n2 + 1. completes proof.
313

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

Instead invoking Lemma 4.8 directly, often simply describe election terms
results important head-to-head contests scores important candidates
mention election built, possibly adding extra padding candidates
affect general structure election, using Lemma 4.8. case clear
Lemma 4.8 indeed used build election describe.
4.1.3 C ONSTRUCTIVE C ANDIDATE C ONTROL
Let us turn case constructive candidate control. show resistance holds
Copeland cases (i.e., rational values 0 1 constructive candidate control scenarios), except CCACu {0, 1} vulnerability holds (see
Theorem 4.11).
resistance proofs section follow via reductions vertex cover problem.
Recall vertex cover problem input (G, k) G undirected graph k
nonnegative integer accept G vertex cover size k. Without
loss generality, assume V (G) = {1, . . . , n} E(G) = {e1 , . . . , em }. Note
either = 0, n = 0, k min(n, m) instance trivial solution proofs
always assume n nonzero k less min(n, m).
case, input reduction meet requirements (or otherwise malformed)
reduction outputs fixed yes instance fixed instance depending (easily obtained)
solution (G, k) malformation input. Also note every input (G, k) meets
requirements, G vertex cover size less equal k G vertex
cover size k.
Theorem 4.9 Let rational number 0 1. Copeland resistant constructive control via adding candidates (CCAC), nonunique-winner model uniquewinner model, rational irrational voter model.
Proof. give reduction vertex cover problem. Let (G, k) instance vertex
cover problem, G undirected graph, k nonnegative integer, V (G) = {1, . . . , n},
E(G) = {e1 , . . . , em }, n 6= 0, 6= 0, k < min(n, m). construct instance CCAC
Copeland designated candidate p become winner adding k candidates
G vertex cover size k.
reduction works follows. Via Lemma 4.8, build election E = (C ,V ) that:
1. {p, e1 , . . . , em } C ,
2. scoreE (p) = 22 1 nonunique-winner case (scoreE (p) = 22 unique-winner
case); sufficiently large (but polynomially bounded) integer takes role
Lemma 4.8s n,
3. ei C , scoreE (ei ) = 22 ,
4. scores candidates C {p, e1 , . . . , em } 22 n 2.
form election E = (C,V ) combining E candidates = {1, . . . , n} (corresponding
vertices G). results head-to-head contests within set arbitrarily,
head-to-head contests members C members set follows:
314

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

candidates C {e1 , . . . , em } defeat members D, e j {e1 , . . . , em },
candidate defeats e j e j edge incident loses otherwise. reduction outputs
instance (C, D,V, p, k) CCAC question whether possible choose subset D,
kD k k, p winner (the unique winner) Copeland election (C ,V ). clear
reduction computable polynomial time. show correct.
G vertex cover size k add candidates correspond cover.
Adding candidates increases score p k, scores ei increase
k 1 each, since edge incident least one member vertex cover. Clearly,
candidates C {p, e1 , . . . , em } never become winners adding k candidates D,
thus p becomes winner (the unique winner).
converse, assume p become winner (the unique winner) via adding k
candidates set D. order p become winner (the unique winner), must
case via adding candidates ei gets least one point less p. However, possible
add candidates correspond cover.

Interestingly, parameter strictly 0 1 (i.e., 0 < < 1) Copeland
resistant constructive control via adding candidates even allow adding unlimited number candidates (the CCACu case). reason rational strictly
0 1 construction ensure, via structure, add k candidates.
hand, Copeland0 Copeland1 vulnerable constructive control via adding
unlimited number candidates (CCACu , see Theorem 4.11).
Theorem 4.10 Let rational number 0 < < 1. Copeland resistant constructive control via adding unlimited number candidates (CCACu ), nonunique-winner
model unique-winner model, rational irrational voter model.
Proof. give reduction vertex cover problem.
unique-winner case, need specify one candidates scores terms
number > 0 1 . Let t1 t2 two positive integers = tt12
greatest common divisor 1. Clearly, two numbers exist rational
greater 0. set t12 . elementary number-theoretic arguments, two positive
integer constants, k1 k2 , k1 = k2 .
Let (G, k) instance vertex cover problem, G undirected graph k
nonnegative integer. Let {e1 , . . . , em } Gs edges let {1, . . . , n} Gs vertices. before,
assume n nonzero k < min(n, m). Using Lemma 4.8, build
election E = (C,V ) following properties:
1. {p, r, e1 , . . . , em } C (the remaining candidates C used padding),
2. scoreE (p) = 22 1,
3. scoreE (r) = 22 1k +k nonunique-winner case (scoreE (r) = 22 1k +k
unique-winner case15 ),
15. Note via second paragraph proof easy build election r score form.
obtain part rs score could, example, r tie k1 padding candidates obtain k2 points.
k2 points could accounted part 22 1.

315

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

4. ei C, scoreE (ei ) = 22 1 + nonunique-winner case (scoreE (ei ) = 22 1
unique-winner case),
5. scores candidates C {p, r, e1 , . . . , em } 22 n 2.
form election E = (C D,V ) via combining E candidates = {1, . . . , n} appropriate voters results head-to-head contests are:
1. p ties candidates D,
2. e j , e j incident candidate defeats candidate e j , otherwise tie,
3. candidates C defeat candidates D.
show G contains vertex cover size k set
p winner (the unique winner) Copeland election (C ,V ). easy
see corresponds vertex cover size k p winner (the unique winner)
Copeland election (C ,V ). reason adding member increases ps score
increases rs score one, e j , adding increases e j score
e j incident i. Thus, via simple calculation scores candidates,
easy see p winner (the unique winner) election.
hand, assume p become winner (the unique winner) Copeland
election (C ,V ) via adding subset candidates D. First, note kD k k,
since otherwise r would end points (at least many points as) p p would
winner (would unique winner). claim corresponds vertex cover
G. sake contradiction, assume edge e j incident vertices u v
neither u v . However, case candidate e j would
points (at least many points as) p p would winner (would unique
winner). Thus, must form vertex cover size k.

Note proof crucial neither 0 1. 0 proof
would fall apart would able ensure vertex cover,
1 would able limit size . fact, show, Theorem 4.11,
Copeland0 Copeland1 vulnerable control via adding unlimited number
candidates (CCACu ).
Theorem 4.11 Let {0, 1}. Copeland vulnerable constructive control via adding unlimited number candidates (CCACu ), nonunique-winner model unique-winner
model, rational irrational voter model.
Proof. input candidate set C, spoiler candidate set D, collection voters preferences
(either preference lists preference tables) C D, candidate p C. goal check
whether subset p winner (the unique winner) (C ,V )
within Copeland . show find set , exists, following simple
algorithm.

316

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

Let D1 = {d | score({p,d},V ) (p) = 1}. Initialize D1 , delete every
score(CD ,V ) (p) < score(CD ,V ) (d). unique-winner problem, delete
every score(CD ,V ) (p) score(CD ,V ) (d).
Clearly, algorithm runs polynomial time. show algorithm works, first note
b D, p winner (the unique winner) (C D,V
b ), p winner (the unique

b
winner) (C (D D1 ),V ). because, Observation 4.2,

score(CD,V
b ) (p) = score(C(DD
b

1 ),V )

=

(p) +

score(C(DD
b 1 ),V ) (p).



score({p,d},V ) (p)

b
dDD
1

b D1 , p winner (the unique winner) (C D,V
b ),
suppose

algorithm computes set p winner (not unique winner) (C ,V ). first
b . Since p winner (not unique winner) (C ,V ), follows
consider case
construction exists candidate C {p} score(CD ,V ) (p) <
score(CD ,V ) (d) (such score(CD ,V ) (p) score(CD ,V ) (d)). However, nonunique-winner
model

b
score(CD ,V ) (p) = score(CD,V
b ) (p) + kD k kDk



b
score(CD,V
b ) (d) + kD k kDk score(CD ,V ) (d),

contradiction. unique-winner model, first inequality becomes
> reach contradiction well.
b 6 . Let first candidate
b deleted
Finally, consider case
b D1 score (p) < score (d)
algorithm. set
(CD ,V )
(CD ,V )
nonunique-winner case (score(CD ,V ) (p) score(CD ,V ) (d) unique-winner case).
b D1 ,
Since




b
b
1. score(CD,V
b ) (p) = score(CD ,V ) (p) (kD k kDk) < score(CD ,V ) (d) (kD k kDk)

score(CD,V
b ) (d) nonunique-winner case,





b
b
2. score(CD,V
b ) (p) = score(CD ,V ) (p) (kD k kDk) score(CD ,V ) (d) (kD k kDk)

score(CD,V
b ) (d) unique-winner case.

b ). contradicIt follows p winner (not unique winner) (C D,V
tion.


remainder section dedicated showing rational 0 1,
Copeland resistant constructive control via deleting candidates constructive control via
partitioning candidates (with without run-off TE TP model). reasons
space nonrepetitiveness, proofs results included found
full TR version (Faliszewski et al., 2008b), first handle case constructive control
via deleting candidates (CCDC) then, using proof CCDC case building block,
handle constructive partition-of-candidates cases.
317

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

Theorem 4.12 Let rational number 0 1. Copeland resistant constructive control via deleting candidates (CCDC), nonunique-winner model
unique-winner model, rational irrational voter model.
proof Theorem 4.13 (which, mentioned above, presented Faliszewski et al.,
2008b) employs construction used proving Theorem 4.12 construction combines suitable elections combined election properties useful proving various
partition-of-candidates cases (with without run-off). particular, construction
applied proof Theorem 4.13, also designed general enough serve key
ingredient proving Theorems 4.14, 4.15, 4.16 below.
Theorem 4.13 Let rational number 0 1. Copeland resistant constructive control via run-off partition candidates ties-promote model (CCRPC-TP)
ties-eliminate model (CCRPC-TE), nonunique-winner model uniquewinner model, rational irrational voter model.
Copeland also resistant constructive control via partition candidates (without run-off)
rational value (and including) 0 1. However, proofs TP TE
cases (which, again, found full TR version, Faliszewski et al., 2008b) uniform
CCRPC scenario soto stay sync structure Faliszewski et al. 2008b,
proofs arewe treat cases separately Theorems 4.14, 4.15, 4.16.
Theorem 4.14 Let rational number 0 1. Copeland resistant constructive control via partition candidates ties-promote tie-handling rule (CCPC-TP),
nonunique-winner model unique-winner model, rational irrational
voter model.
Theorem 4.15 Copeland1 resistant constructive control via partition candidates
ties-eliminate tie-handling rule (CCPC-TE), nonunique-winner model uniquewinner model, rational irrational voter model.
Theorem 4.16 Let rational number, 0 < 1. Copeland resistant constructive
control via partition candidates ties-eliminate tie-handling rule (CCPC-TE),
nonunique-winner model unique-winner model, rational irrational
voter model.
4.2 Voter Control
section, show rational , 0 1, Copeland resistant types
voter control. Table 2 lists type voter control, rational , 0 1,
winner model (i.e., nonunique-winner model unique-winner model) theorem
given case handled. start control via adding voters.
Theorem 4.17 Let rational number 0 1. Copeland resistant
constructive destructive control via adding voters (CCAV DCAV), nonuniquewinner model unique-winner model, rational irrational voter model.

318

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

unique
CCAV
DCAV
CCDV
DCDV
CCPV-TP
DCPV-TP
CCPV-TE
DCPV-TE

=0
nonunique

0< <1
unique
nonunique

unique

=1
nonunique

Thm. 4.17
Thm. 4.19
Thm. 4.20

Thm. 4.20
Thm. 4.19

Thm. 4.19
Thm. 4.20

Thm. 4.20
Thm. 4.19

Thm. 4.19
Thm. 4.18

Thm. 4.18
Thm. 4.19

Thm. 4.21
Thm. 4.23
Thm. 4.26

Thm. 4.24
Thm. 4.25

Table 2: Table theorems covering resistance results voter control Copeland .
theorem covers case rational voters case irrational voters.

Proof. result follows via reductions X3C problem. first show handle
nonunique-winner constructive case later argue construction easily
modified remaining cases.
Let (B, ) X3C instance B = {b1 , . . . , b3k } = {S1 , . . . , Sn } finite collection three-element subsets B. Without loss generality, assume k odd (if even,
simply add b3k+1 , b3k+2 , b3(k+1) B Sn+1 = {b3k+1 , b3k+2 , b3(k+1) } , add 1 k).

question whether one pick k sets Sa1 , . . . , Sak B = kj=1 Sa j .
build Copeland election E = (C,V ) follows. candidate set C contains candidates p
(the preferred candidate), r (ps rival), s, members B, number padding candidates.
select voter collection V head-to-head contests, defeats p, r defeats
bi , following Copeland scores candidates,
sufficiently large (but polynomially bounded n) nonnegative integer:
1. scoreE (p) = 1,
2. scoreE (r) = + 3k,
3. candidates Copeland scores 1.
easy see E constructed polynomial time Lemma 4.8. addition, ensure
following results head-to-head contests candidates C:
1. vsE (s, p) = k 1,
2. {1, . . . , k}, vsE (r, bi ) = k 3,
3. pairs candidates c, d, |vsE (c, d)| k + 1.
done since add 2 vsE (c, d) leave relative vote scores

adding two voters, c > > C {c, d} C {c, d} > c > (see Lemma 4.6). Since k odd
number voters even (see Lemma 4.8), easy see fulfill requirements.

319

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

also specify set W voters chair potentially add. set Si
single voter wi W preference list
p > B Si > r > Si >
(all unmentioned candidates follow fixed arbitrary order). claim contains kelement cover B p become winner election via adding k
voters selected W .
contains k-element cover B, say Sa1 , . . . , Sak , make p winner via adding
voters U = {wa1 , . . . , wak }. Adding k voters increases ps score one, since p
defeats head-to-head contest. Since voters U correspond cover, score r goes
3k points. Why? bi B, adding k 1 voters U correspond
sets cover containing bi increases relative performance bi versus r k 1 votes,
thus giving bi two votes advantage r. Adding remaining voter U decreases
advantage 1, still bi wins head-to-head contest r.
show make p winner adding k voters contains kelement cover B. Note p candidate possibly become winner adding
k voters, p best obtain Copeland score , p obtain score
add exactly k voters, r lose 3k points via losing head-to-head contests
bi s. Thus way p become winner adding k voters
W add exactly k voters r loses head-to-head contest bi .
Assume U W set voters correspond cover B. means
candidate bi least two voters U prefer r bi . However,
case bi cannot defeat r head-to-head contest p winner. U corresponds
cover. completes proof nonunique-winner constructive case theorem.
constructive unique-winner case, modify election E scoreE (p) = .
listed properties relative vote scores absolute Copeland scores unchanged.
previous case, easy see p become unique winner via adding k voters
correspond cover B. converse, show still need add exactly k voters
p become unique winner.
added fewer k 1 voters p would get extra points would
impossible p become unique winner. Let us show adding exactly k 1 voters
cannot make p unique winner. added exactly k 1 voters p would get points extra
tie s. consider candidate bi j , j corresponds one added
voters, w j . Since w j prefers r bi , adding w j election increases relative performance r
versus bi k 2. Thus adding remaining k 2 voters result bi either tieing losing
head-to-head contest r. either case p would high enough score become
unique winner. Thus know exactly k candidates must added want p become
unique winner and, via argument previous case, know
correspond cover.
destructive cases suffices note proof constructive nonunique-winner
case works also proof destructive unique-winner case (where preventing r
unique winner) constructive unique-winner case works also proof
destructive nonunique-winner case (where preventing r winner).

Let us turn case control via deleting voters. Unfortunately, proofs
uniform need cases handle = 1 separately case
320

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

0 < 1. Also, cannot use construction lemma (Lemma 4.8) anymore conveniently
build elections. case deleting voters (or partitioning voters) need
clear understanding voter affects election whole point introducing
construction lemma abstract away low-level details.
Analogously case candidate control, resistance proofs deleting voters reused
within resistance proofs partitioning voters. reasons space nonrepetitiveness,
include proofs. particular, proofs Theorems 4.18 4.20
included found full TR version (Faliszewski et al., 2008b). proofs
Theorems 4.19 4.21, however, presented here. mention construction given
proof Theorem 4.19 used later proof Theorem 5.1, construction
given proof Theorem 4.21 used later proof Theorem 5.2.
Theorem 4.18 Copeland1 resistant constructive control via deleting voters (CCDV)
nonunique-winner model destructive control via deleting voters (DCDV) uniquewinner model, rational irrational voter model.
Theorem 4.19 Let rational number 0 1. Copeland resistant constructive control via deleting voters (CCDV) unique-winner model destructive control
via deleting voters (DCDV) nonunique-winner model, rational irrational
voter model.
Proof. Let (B, ) instance X3C, B = {b1 , . . . , b3k } = {S1 , . . . , Sn }
finite family three-element subsets B. Without loss generality, assume n k
k > 2 (if n < k contain cover B, k 2 solve problem
brute force). build election E = (C,V ) that:
1. contains k-element cover B, preferred candidate p become unique
Copeland winner E deleting k voters,
2. r become nonwinner deleting k voters, contains k-element cover
B.
Let candidate set C {p, r, b1 , . . . , b3k } let V following collection 4n k + 1
voters:
1. n 1 voters preference B > p > r,
2. n k + 2 voters preference p > r > B,
3. Si two voters, vi vi ,
(a) vi preference r > B Si > p > Si ,
(b) vi preference r > Si > p > B Si .
easy see bi B, vsE (r, bi ) = 2n k + 3, vsE (bi , p) = k 3, vsE (r, p) = k 1.
contains k-element cover B, say {Sa1 , . . . , Sak }, delete voters va1 , . . . , vak .
resulting election, p defeats every candidate head-to-head contests, thus p
unique winner.
321

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

prove second statement, suppose subset W k voters
r winner Eb = (C,V W ). Since vsE (r, bi ) = 2n k + 3 n k, immediate
b order r winner E,
b
r defeats every bi B head-to-head contests E.
p must certainly defeat r tie-or-defeat every bi B head-to-head contests. p
defeat r head-to-head contest kW k = k every voter W prefers r p. follows
W size-k subset {v1 , v1 , . . . , vn , vn }.
Let bi B. Recall vsE (bi , p) = k 3 p needs least tie bi head-to-head
b Since kW k = k, follows W contain one voter prefers p bi .
contest E.
Since k > 2, follows W contains voters set {v1 , . . . , vn } voters W
correspond k-element cover B.

Theorem 4.20 Let rational number 0 < 1. Copeland resistant constructive control via deleting voters (CCDV) nonunique-winner model destructive control
via deleting voters (DCDV) unique-winner model, rational irrational
voter model.
Theorem 4.21 Let rational number 0 1. Copeland resistant
constructive destructive control via partitioning voters TP model (CCPV-TP DCPVTP), nonunique-winner model unique-winner model, rational
irrational voter model.
Proof. Let (B, ) instance X3C, B = {b1 , . . . , b3k } = {S1 , . . . , Sn }
finite family three-element subsets B. Without loss generality, assume n k
k > 2 (if n < k contain cover B, k 2 solve problem
brute force). build election E = (C,V ) that:
1. contains k-element cover B, preferred candidate p become unique
Copeland winner E via partitioning voters TP model,
2. r made uniquely win E via partitioning voters TP model,
contains k-element cover B.
Note implies Copeland resistant constructive destructive control via
partitioning voters TP model, nonunique-winner model unique-winner
model.
construction extension construction Theorem 4.19. let candidate
set C {p, r, s, b1 , . . . , b3k } let V following collection voters:
1. k + 1 voters preference > r > B > p,
2. n 1 voters preference B > p > r > s,
3. n k + 2 voters preference p > r > B > s,
4. Si two voters, vi vi ,
(a) vi preference r > B Si > p > Si > s,
322

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

(b) vi preference r > Si > p > B Si > s.
Let Vb V collection voters V except k + 1 voters preference > r >
B > p. Note Vb exactly voter collection used proof Theorem 4.19 candidate
added least desirable candidate. Since influence differences scores
candidates, following claim follows immediately proof Theorem 4.19.
Claim 4.22 r become nonwinner (C, Vb ) deleting k voters, contains
k-element cover B.
Recall need prove contains k-element cover B, p made
unique Copeland winner E via partitioning voters TP model, r made
uniquely win E via partitioning voters TP model, contains k-element cover
B.
contains k-element cover B, say {Sa1 , . . . , Sak }, let second subelection
consist k + 1 voters preference > r > B > p voters va1 , . . . , vak . p
unique winner first subelection, unique winner second subelection, p
uniquely wins final run-off p s.
prove second statement, suppose partition voters r unique
winner resulting election model TP. Note least one subelections, without
loss generality say second subelection, majority voters prefers r candidates
{p, b1 , . . . , b3k }. Since r unique winner every run-off participates in, r cannot
winner either subelection. Since r defeats every candidate {p, b1 , . . . , b3k } head-tohead contests second subelection, order r winner second subelection,
must certainly case defeats r head-to-head contest second subelection.
implies k voters Vb part second subelection.
consider first subelection. Note r cannot winner first subelection. Then,
clearly, r cannot winner first subelection restricted voters Vb .16 Claim 4.22
follows contains k-element cover B.

turn TE variant control via partitioning voters. None remaining proofs
Section 4.2 (i.e., none proofs Theorems 4.23 4.26) included
found full TR version (Faliszewski et al., 2008b). particular, proof
Theorem 4.23 uses exact construction proof Theorem 4.21 proofs
Theorems 4.24 4.25 use modifications thereof. stay sync structure Faliszewski
et al. 2008b, proof-providing full TR (where proof structure, mentioned above, depends
value ), state Theorems 4.23 4.26 separately.
Theorem 4.23 Let rational number 0 < 1. Copeland resistant constructive control via partitioning voters TE model (CCPV-TE), nonunique-winner
model unique-winner model, rational irrational voter model.
16. r winner first subelection restricted voters Vb r would certainly winner first
subelection without restrictions: voters V Vb prefer r everyone except s, (by discussion
proof) cannot winner first subelection. (Note winner one two subelections
winner second subelection.)

323

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

Theorem 4.24 Copeland1 resistant constructive control via partitioning voters TE model
(CCPV-TE), nonunique-winner model unique-winner model, rational
irrational voter model.
Theorem 4.25 Copeland1 resistant destructive control via partitioning voters TE model
(DCPV-TE), nonunique-winner model unique winner model, rational
irrational voter model.
Finally, Theorem 4.26 states resistance Copeland , rational number
0 < 1, destructive control partition voters TE model. proof result
(see Faliszewski et al., 2008b) extends construction proof Theorem 4.20 (see also
Faliszewski et al., 2008b) way proof Theorem 4.21 extended construction
proof Theorem 4.19.
Theorem 4.26 Let rational number 0 < 1. Copeland resistant destructive control via partitioning voters TE model (DCPV-TE), nonunique-winner
model unique-winner model, rational irrational voter model.
4.3 FPT Algorithm Schemes Bounded-Case Control
Resistance control generally viewed desirable property system design. However, suppose one trying solve resistant control problems. hope?
Bartholdi, Tovey, Trick (1989b), seminal paper NP-hard winner-determination
problems, suggested considering hard election problems cases bounded number candidates bounded number voters, obtained efficient-algorithm results cases.
Within study elections, approachseeking efficient fixed-parameter algorithms
has, example, also used (although somewhat tacitlysee coming discussion second paragraph Footnote 17) within study bribery (Faliszewski et al., 2006a; Faliszewski,
Hemaspaandra, & Hemaspaandra, 2006b). best knowledge, bounded-case approach finding limits resistance results previously used study control problems. section precisely that.
particular, obtain resistant-in-general control problems broad range efficient algorithms case number candidates voters bounded. algorithms
merely polynomial time. Rather, give algorithms prove membership FPT (fixedparameter tractability, i.e., problem merely individually P fixed value
parameter interest (voters candidates), indeed single P algorithm degree
bounded independently value fixed number voters candidates)
number candidates bounded, also number voters bounded. prove
FPT claims hold even succinct input modelin voters input via
(preference-list, binary-integer-giving-frequency-of-that-preference-list) pairsand even
case irrational voters. (One imagine succinct-representation case holding initial preprocessing elections ballots compute number people casting preference
occurred.)
obtain algorithms voter-control cases, bounded candidates
bounded voters, candidate-control cases bounded candidates.
hand, show resistant-in-general irrational-voter, candidate-control cases, resistance
still holds even number voters limited two.
324

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

structure section follows. first start briefly stating notions notations.
next state, prove, fixed-parameter tractability results. Regarding those, first
address FPT results (standard) constructive destructive cases. show
many cases assert FPT results general stillin particular, look
extended control: completely pinpointing whether given type control ensure
least one specified collection Copeland Outcome Tables (to defined later)
obtained. Finally, give resistance results.
4.3.1 N OTIONS



N OTATIONS

study fixed-parameter complexity (see, e.g., Niedermeier, 2006) expanding explosively since parented field Downey, Fellows, others late 1980s 1990s.
Although area built rich variety complexity classes regarding parameterized problems,
purpose current paper need focus one important class, namely,
class FPT. Briefly put, problem parameterized value j said fixed-parameter
tractable (equivalently, belong class FPT) algorithm problem whose
running time f ( j)nO(1) . (Note particular particular constant big-oh
holds inputs, regardless j value particular input has.)
context, consider two parameterizations: bounding number candidates
bounding number voters. use notations used throughout paper
describe problems, except postpend -BV j problem name state number
voters may j, postpend -BC j problem name state number
candidates may j. case, bound applies full number items
involved problem. example, case control adding voters, j must bound
total number voters election added together number voters pool
voters available adding.
Typically, viewing input votes coming ballot. However, one
also consider case succinct inputs, algorithm given votes (preferencelist, binary-integer-giving-frequency-of-that-preference-list) pairs. (We mention passing
adding voter cases, speak succinctness require always-voting
voters specified succinctly also pool voters-available-to-be-added specified
succinctly.) Succinct inputs studied extensively case bribery (Faliszewski et al.,
2006a, 2006b), speaking broadly, succinctness-of-input issues often germane
complexity classification (see, e.g., Wagner, 1986). Note proving FPT result succinct
case problem immediately implies FPT result problem (without requirement
succinct inputs place), indeed stronger result, since succinctness potentially
exponentially compress input.
Finally, would like able concisely express many results single statement.
so, borrow notational approach transformational grammar,
h iuse square brackets
runs
independent choice notation. So, example, claim walks shorthand


six assertions: runs; runs; runs; walks; walks; walks. special case
symbol 0
/ which, appearsin bracket, means unwound
viewed text all. example, Succinct
Copeland fun asserts Succinct Copeland
0/
fun Copeland fun.

325

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

4.3.2 F IXED -PARAMETER RACTABILITY R ESULTS
immediately state main results, show voter-control cases FPT schemes
hold bounded-voter bounded-candidate cases, candidate-control
cases FPT schemes hold bounded-candidate cases.
Theorem 4.27 rational , 0 1, choice independent choice brackets below, specified parameterized (as j varies N) problem FPT:


AV





DV
C
BV j
succinct
Copeland


.
C
PV-TE BC j
0/
CopelandIrrational
PV-TP
Theorem 4.28 rational , 0 1, choice independent choice brackets below, specified parameterized (as j varies N) problem FPT:


ACu
AC




DC



succinct
Copeland
C
C PC-TE

-BC j .
0/

CopelandIrrational

PC-TP


RPC-TE
RPC-TP
Readers interested discussion results proofs point safely
skip next labeled section header.
proving theorems, let us first make observations them. First,
cases particular set choices case known (e.g., due results
Sections 4.1 4.2) P even unbounded case, results uninteresting
follow earlier results (such cases include succinct cases, since
treated earlier). However, small minority cases. Also, clarity
cases covered, included items formally needed. example,
since FPT succinct case implies FPT no-succinctness-restriction case, since FPT
irrationality-allowed case implies FPT rational-only case, first two choice brackets
theorems could, without decreasing results strength, removed eliminating
0
/ Copeland choices.
turn proofs. Since proving every case would uninterestingly repetitive,
times (after carefully warning reader) prove cases one two control types
enough make clear omitted cases proofs go.
Let us start cases done simply appropriately applied brute force.
first prove Theorem 4.28.
Proof Theorem 4.28. limited j candidates,
cases mentioned, total number ways adding/deleting/partitioning candidates simply
(large) constant. example, (at rather exactly since j
merely upper bound number candidates) 2 j possible run-off partitions
2 j1 relevant ways deleting candidates (since cant (destructive case) would
326

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

never (constructive case) delete distinguished candidate). brute-force try ways
adding/deleting/partitioning candidates, way see whether get desired
outcome. works polynomial time (with fixed degree independent j ) even
succinct case, even irrationality allowed.
Theorem 4.28
brute-force approach similarly works case voter control number voters
fixed. particular, prove following subcase Theorem 4.27.
Lemma 4.29 rational , 0 1, choice independent choice brackets
below, specified parameterized (as j varies N) problem FPT:


AV



DV
C
succinct
Copeland
-BV j .
C


CopelandIrrational
PV-TE

0/
PV-TP
considering BV j casesnamely proof resistance section starting
page 334we even discuss succinctness. reason number voters
bounded, say j, succinctness doesnt asymptotically change input sizes interestingly,
since succinctness best would compress vote description factor jwhich
case fixed constant (relative value parameterization, j).
Proof Lemma 4.29. limited j voters, note can,
four types control, brute-force check possible approaches type control.
example, case control deleting voters, clearly 2 j possible vote
deletion choices, case control partitioning voters, 2 j
partitions (into V1 V V1 ) consider. 2 j (large) constant. direct brute-force
check yields polynomial-time algorithm, inspection one see run-times degree
bounded independently j.
Lemma 4.29
come interesting cluster FPT cases: voter-control cases number
candidates bounded. Now, first, one might think handle this, cases,
via brute-force approach. almost correct: One get polynomial-time algorithms
cases via brute-force approach. However, succinct cases, degrees
algorithms huge, independent bound, j, number candidates.
example, even rational case, one would approach obtain run-times terms
nkCk! . is, one would obtain family P-time algorithms, one would
FPT algorithm.
overcome obstacle, employ Lenstras (1983) algorithm bounded-variablecardinality integer programming. Although Lenstras algorithm truly amazing power, even
enough accomplish goal. Rather, use scheme involves fixed
(though large) number Lenstra-type programs focused different resolution
path regarding given problem.
need prove, complete proof Theorem 4.27, following lemma.
Lemma 4.30 rational , 0 1, choice independent choice brackets
below, specified parameterized (as j varies N) problem FPT:
327

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE



AV





succinct
Copeland
DV
C
-BC j .
C

0/
PV-TE
CopelandIrrational
PV-TP
Let us start recalling that, regarding first choice bracket, succinct case implies
0
/ case, need address succinct case. Recall also that, regarding second choice
bracket, rational , 0 1, CopelandIrrational case implies Copeland case,
need address CopelandIrrational case.
remains handle pair choices third forth choice brackets.
prove every case would repetitive. simply prove detail difficult, relatively
representative case, cases either mention type adjustment needed
obtain proofs, simply leave simple tedious exercise clear,
do, anyone reads section.
So, particular, let us prove following result.
Lemma 4.31 rational , 0 1, following parameterized (as j varies N)
problem FPT: succinct-CopelandIrrational -CCPV-TP-BC j .
Proof. Let , 0 1, arbitrary, fixed rational number. particular, suppose
expressed b/d, b N, N+ , b share common integer divisor greater
1, b = 0 = 1. wont explicitly invoke b algorithm, time
speak evaluating certain set pairwise outcomes respect , one think
evaluating respect strict pairwise win giving points, pairwise tie giving b points,
strict pairwise loss giving 0 points.
need method specifying pairwise outcomes among set candidates. this,
use notion Copeland outcome table set candidates. actually
table, rather function (a symmetric oneit affected order
two arguments) that, given pair distinct candidates inputs, say
three possible outcomes allegedly happened: Either tie, one candidate won,
candidate won. Note COT simply representation election graph (see Section 4.1.2).
j
So, j-candidate election, exactly 3(2) functions. (We care names
candidates, assume tables simply use names 1 j,
match names actual candidates integers linking lexicographically, i.e.,
lexicographically first candidate associated integer 1 on.) Let us call
j-candidates Copeland outcome table j-COT.

need

build

algorithm

shows


problem
succinct-CopelandIrrational -CCPV-TP-BC j , j N, FPT. So, let j fixed integer
bound number candidates.17
17. seem specify algorithm merely bound. However, important note enough
establish exists single algorithm fulfills requirements definition FPT. particular,
specification give sufficiently uniform one simply consider single algorithm that,
given input, notes value j, number candidates, j algorithm
specify does.
take moment mention passing earlier work, Faliszewski et al. 2006a (this expanded, full version that) Faliszewski et al. 2006b, gives P-time algorithms fixed parameter (fixed

328

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

j -COT, T1 ,
j -COT, T2 ,


CopelandIrrational election (involving input voters), respect , candidates win T1 respect ,
candidates win T2 respect , preferred candidate
input problem winner,

create run integer linear program constraint feasibility problem checks
whether exists partition voters first subelection j COT T1 second subelection j -COT T2 , so, accept.

Figure 5: top-level code case succinct-CopelandIrrational -CCPV-TP-BC j .
Let us suppose given input instance. Let j j number candidates
instance (recall j number candidates, rather upper bound number
candidates).
top level algorithm specified pseudocode Figure 5. (Although algorithm seemingly trying tell whether given control possible given case, rather
telling partition achieve control, note iteration double
loop accepts precise values variables inside integer linear program constraint feasibility problem made iteration satisfied fact tell us precisely partition
makes preferred candidate win.)
Now, note total number j -COTs exist (we need care whether
j
j
realized via actual votes) 3( 2 ) . code inside two loops executes 9( 2 ) times,
constant-bounded since j j, fixed j.
remains give integer linear program constraint feasibility problem mentioned
inside inner loop. setting sometimes confusing, e.g., speak constants
grow without limit. important keep mind integer linear program
constraint feasibility problem, number variables constraints constant (over inputs),
integer linear program constraint feasibility problems constants (one may prefer word
candidate fixed voters) cases fact, claims work, implicitly giving FPT algorithms, even though papers dont explicitly note that. reason generally true
papernamely, Lenstra technique powerful also ideally suited FPT algorithms
used inside algorithms FPT algorithms. interestingly, Lenstra approach tends work even
succinct inputs, FPT comment made applies even results abovementioned earlier papers
succinct-inputs case fixed-number-of-candidates fixed-number-of-voters claims. (The fixednumber-of-candidates fixed-number-of-voters Dodgson winner/score work Bartholdi et al., 1989b, known
FPT algorithmsdue proof Bartholdi et al., 1989b, itself, see discussion Faliszewski et al.,
2006a, see also Betzler, Guo, Niedermeier, 2008. Although paper Bartholdi et al., 1989b, doesnt address
succinct input model, Faliszewski et al., 2006a, notes approach works fine even succinct cases
winner problem. true P-ness algorithms even succinct case, also
FPT-ness algorithms even succinct case.)

329

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

coefficients, makes things clearer) things change respect input.
framework allows us invoke Lenstras powerful algorithm.
first specify set constants integer linear program constraint feasibility problem.
j
particular, i, 1 2( 2 ) , constant, n ,18 number input voters




j
whose vote ith type (among 2( 2 ) possible vote possibilities; keep mind voters
j
allowed irrational, thus value 2( 2 ) correct). Note number constants

constant-bounded (for fixed j), though course values constants
(of integer linear program constraint feasibility problem) take grow without limit.
addition, let us define constants vary input rather simply
notational shorthand use describe integer linear program constraint feasibility
problem defined (what constraints occur it). particular, 1 j ,
1 j , 6= , let val1i, 1 T1 asserts (in head-to-head contest) ties defeats
, let 0 T1 asserts (in head-to-head contest) loses . Let val2i, identically
defined, except respect T2 . Informally put, values used let integer linear
program constraint feasibility problem seek enforce win/loss/tie pattern respect
given input vote numbers given type allowed control action.
integer linear program constraint feasibility problems variables, course
j
j
integer variables, following 2( 2 ) variables. i, 1 2( 2 ) , variable,


j
mi , represents many ni voters ith among 2( 2 ) possible vote types go
first subelection.
Finally, must specify constraints integer linear program constraint feasibility problem. three groups constraints.
first constraint group enforcing plausible numbers put first partition.
j
particular, i, 1 2( 2 ) , constraints 0 n .







second constraint group enforcing partitioning really first
subelection situation pairwise contests come exactly specified T1 .
particular, 1 j , 1 j , 6= , following. Consider
equation
(




) OP (


j
{a | 1 2( 2 ) votes type
holds preferred }



),

(4.a)

j
{a | 1 2( 2 ) votes type
holds preferred i}



j
sum varies 2( 2 ) possible preferences. val1(i, ) = 1
constraint form OP set . val1(, i) = 1 constraint
form OP set . Note means val1(i, ) = val1(, i) = 1, i.e.,
two voters purported tie, add two constraints.
third constraint group function second constraint group, except regards
second subelection rather first subelection. particular,

18. Again, discussed immediately previous paragraph, say that, example, ni constants
integer linear program constraint feasibility problem, mean constants complexity sense,
rather constantsin sense coefficientsof integer linear program constraint
feasibility problem. saying that, mean imply number voters bounded global
value cases.

330

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

1 j , 1 j , 6= , following. Consider equation (4.a) above,
except two occurrences replaced na . val2(i, ) = 1
constraint form OP set . val2(, i) = 1 constraint
form OP set . above, means val2(i, ) = val2(, i) = 1, add two
constraints.
completes specification integer linear programming constraint feasibility problem.
Note top-level code, Figure 5, clearly runs within polynomial time relative
even succinct-case input original CCPV-TP problem, polynomials degree
bounded independently j. Note particular algorithm constructs
large constant (for j fixed) number integer linear programming constraint feasibility problems,
polynomial-sized relative even succinct-case input original
CCPV-TP problem, polynomial sizes degree bounded independently j. Further, note integer linear programming constraint feasibility problems clearly test
supposed testmost importantly, test subelections match pairwise
outcomes specified j -COTs T1 T2 . Finally crucially, Lenstras (1983) algorithm (see
also Downey, 2003, Niedermeier, 2002, clear regarding linears later
sentence), since integer linear programming constraint feasibility problem fixed number
constraints (and case fact also fixed number variables), solvedrelative
size (which includes filled-in constants, ni example, effect inputs
integer programs specification)via linear number arithmetic operations linear-sized
integers. So, overall, polynomial time even relative succinctly specified input,
polynomials degree bounded independently j. Thus established membership
class FPT.

describe briefly proof Lemma 4.31
adjusted
cases
Lemma 4.30, namely, cases

handle
partition


succinct
Copeland
C
PV-TE
C
-BC j . noted before, first two brack0/
CopelandIrrational

PV-TP
ets ignored, chosen demanding choice each. Let us discuss
variations. Regarding changing constructive destructive, Figure 5 change winner
winner. Regarding changing PV-TP PV-TE, block Figure 5
change candidates win candidate wins (if unique candidate
wins).



C
AV
succinct
Copeland
C
-BC j .
remaining cases cases
CopelandIrrational

DV
0/
However, cases even straightforward partition cases covered,
space reasons write out, rather briefly comment cases. Basically,
j
ones top-level code cases loops j -COTs, (there 3( 2 ) ) checks
whether right outcome happens j -COT (i.e., distinguished candidate either
(constructive case) (destructive case) winner), so, runs Lenstras algorithm
integer linear programming constraint feasibility problem see whether allowed
action (adding/deleting) get state particular j -COT matches (after addition
deletion voters) election. integer program, variables obvious ones, namely,
j
i, 1 2( 2 ) , variable, , describes many voters type


331

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

add/delete. key constants (of integer linear program constraint feasibility problem),
j
have, i, 1 2( 2 ) , value, n , number type voters input. Also,




j
problem addition voters, additional constants, nbi , 1 2( 2 ) ,
representing number type voters among pool, W , voters available addition.
problem internal k (a limit number additions deletions), enforce
natural constraints, also natural constraints enforce obvious relationships mi , ni , nbi , on. critically, constraints ensuring
additions/deletions specified mi , pairwise outcome specified j -COT realized.
Finally, although everything Section 4.3 (both part far part come) written
case nonunique-winner model, results hold analogously unique-winner
model, natural, minor proof modifications. (Also, mention passing due
connection, found Footnote 5 Hemaspaandra et al., 2007a, unique-winner destructive
control nonunique-winner constructive control, one could use nonunique-winner
constructive-case results indirectly prove unique-winner destructive-case results.)

4.3.3 FPT



E XTENDED C ONTROL

section, look extended control. mean changing ten standard
control notions adding/deleting/partitioning candidates/voters. Rather, mean generalizing past
merely looking constructive (make distinguished candidate winner) destructive
(prevent distinguished candidate winner) cases. particular, interested
control goal far flexibly specified, example (though partition cases
even flexible this), allow goal region (reasonablethere
time-related conditions) subcollection Copeland outcome tables (specifications
won/lost/tied head-to-head contest). Since Copeland outcome table, concert
current , one read CopelandIrrational scores candidates, allows us
tremendous range descriptive flexibility specifying control goals, e.g., specify
linear order desired candidates respect CopelandIrrational scores, specify
linear-order-with-ties desired candidates respect CopelandIrrational scores,
specify exact desired CopelandIrrational scores one candidates, specify
want ensure candidate certain subgroup CopelandIrrational score ties
defeats CopelandIrrational score candidate certain subgroup, etc.19 Later
section give list repeating examples adding new examples.
FPT algorithms given previous section regard, surface, standard control
problem, tests whether given candidate made winner (constructive case)
precluded winner (destructive case). note general approaches used
section fact yield FPT schemes even far flexible notions control mentioned
19. mention front initial example list applies additional minor technical caveats.
examples speaking final election candidates receiving CopelandIrrational scores
final election. fact partition cases (necessarily) so, cases focus
Copeland outcome tables natural given case. example, control partition voters,
focus subcollections pairs Copeland outcome tables two subelections. Also, though Copeland
outcome tables defined explicitly labeled candidate names, rather use lexicographical
correspondence involved candidates, cases wouldthough dont repeat discussion
belowneed allow inclusion goal specification names candidates play given
table tables, particularly, cases addition deletion candidates, partition cases.

332

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

above. fact, one gets, FPT cases covered previous section, FPT algorithms
extended-control problem casesvery loosely put, FPT algorithms test, virtually
natural collection outcome tables (as long collection recognized way
doesnt take much running time, i.e., checking time polynomial degree
bounded independently j), whether given type control one reach one
outcome tables.
Let us discuss bit detail. key concept used inside proof Lemma 4.31
Copeland outcome tablea function distinct pair candidates specifies
either tie specifies (not tied) winner pairwise contest. Let us consider
control algorithm given proof lemma, particular let us consider top-level
code specified Figure 5. code double-loops size j Copeland outcome tables (a.k.a. j COTs), regarding subpartitions, case outcome tables subelection cases,
followed final election imply, correspond desired type constructive (the
distinguished person wins) destructive (the distinguished person win) outcome, check
whether two j -COTs made hold via current type control (for case
discussed, PV-TP).
However, note simply easily varying top-level code obtain natural FPT
algorithm (a single algorithm, see Footnote 17 analogue applies here) question
whether via allowed type control one reach run-time-quick-to-recognize collection
pairs j -COTs (in subelection), even whether given candidate collection one
given (run-time-quick-to-recognize) j -COT collection candidate collection ( j
size final-round candidate collection) reached final election. true
partition cases (where, informally put, would by, Figure 5, changing
condition inside instead look membership collection j -COTs20 ) also
cases attacked via Lenstras method (though nonpartition cases typically
single-loop Copeland outcome tables may represent outcome control exerted;
also, cases, caveat end Footnote 19 apply). even easier
notice cases attacked direct brute force also holds.
So, examples (some echoing start section, new), following (with caveats mentioned needed names attached, e.g., cases candidate
addition/deletion/partition, regarding partition cases focusing necessarily directly
20. Let us discuss bit formally, using PV-TP example. Consider family boolean functions
Fj , j N, Fj computable, even first argument succinctly specified, polynomial time
polynomial degree bounded independently j. Now, consider changing Figure 5s code to:
j -COT, T1 ,
j -COT, T2 ,
(Fj (input, T1 , T2 ))
.
Note change gives FPT control scheme certain extended control problem. particular,
extended control problem whose goal ensure realize least one set (T1 , T2 )
Fj ( j number candidates particular input), given inputs problems input, T1 , T2
evaluates true. is, Fj functions recognizing (viewed bit differently, defining) goal set
extended control problem.
input, T1 , T2 easily tell scores final election. approach used
choose extended-control goals natural features final election.

333

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

final table) FPT extended control algorithms types control boundedness cases
FPT results previous section stated.
1. Asking whether stated action one obtain final election (simply
election case partitioning) outcome CopelandIrrational system scores candidates precisely match relations lexicographic names
candidates.
2. generally that, asking whether stated action one obtain final
election (simply election case partitioning) certain linearorder-without-ties regarding CopelandIrrational -system scores candidates.
3. generally still, asking whether stated action one obtain final election
(simply election case partitioning) certain linear-order-withties regarding CopelandIrrational -system scores candidates.
4. Asking whether stated action one obtain final election (simply
election case partitioning) situation exactly 1492 candidates
tie winner regarding CopelandIrrational -system scores.
5. Asking whether stated action one obtain final election (simply
election case partitioning) situation two candidates
CopelandIrrational -system scores other.
Again, examples. point previous section flexible enough
address constructive/destructive control, also address far general control issues.
4.3.4 R ESISTANCE R ESULTS
Theorems 4.27 4.28 give FPT schemes voter-control cases bounded voters,
voter-control cases bounded candidates, candidate-control cases bounded candidates. might lead one hope cases admit FPT schemes. However, remaining
type case, candidate-control cases bounded voters, follow pattern. fact,
note CopelandIrrational candidate-control cases showed earlier paper (i.e., without bounds number voters) resistant remain resistant even case
bounded voters. resistance holds even input succinct format,
certainly also holds input succinct format.
reason that, case irrational voters, two voters (with preferences
j candidates) given j-COT achieved. this, distinct pair candidates
, preferred pairwise contest voters prefer , preferred
pairwise contest voters prefer i, tie pairwise contest
one voter prefer one voter prefer . Since proofs resistance candidate
control, identified elections election graphs, i.e., COTs, hard see
resistance proofs carry even case two irrational voters.
open cases remaining regard rational-voter, candidate-control, bounded-voter
cases. note Betzler Uhlmann (2008) recently resolved open issues.

334

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

5. Control Condorcet Elections
section show Condorcet elections resistant constructive control via deleting
voters (CCDV) via partition voters (CCPV). results originally claimed
seminal paper Bartholdi et al. (1992), proofs based assumption
voter indifferent several candidates. model elections allow
(and neither ours). show one obtain results case voters
preference lists linear orderswhich model ours.
Recall candidate c election E = (C,V ) Condorcet winner E defeats
candidates head-to-head contests. Alternatively, one could say candidate c
Condorcet winner election E Copeland0 score kCk 1. Since
election one Condorcet winner, doesnt make sense differentiate
unique-winner nonunique-winner models. (We pass reader referees
comment different system known weak Condorcet elections, whose winners
candidates beat tie candidate head-to-head elections, one
one winner.)
Theorem 5.1 Condorcet elections resistant constructive control via deleting voters.
Proof. follows immediately proof Theorem 4.19. Note Condorcet winner
always unique Copeland winner, rational 0 1, note proof
Theorem 4.19, contains k-element cover B, delete k voters
resulting election p defeats every candidate head-to-head contest, i.e., p Condorcet
winner resulting election.

proceed proof resistance case constructive control via partition
voters (CCPV), mention slight quirk Bartholdi, Tovey, Tricks model voter
partition. one reads paper carefully, becomes apparent quiet assumption
given set voters partitioned subelections elect exactly one
winner, thus severely restricting chairs partitioning possibilities. Hemaspaandra
et al. (2007a) replaced Bartholdi, Tovey, Tricks convention natural ties-promote
ties-eliminate rules (see discussion Hemaspaandra et al., 2007a), current
section paper go back Bartholdi, Tovey, Tricks model, since goal
reprove results without breaking model.
Theorem 5.2 Condorcet elections resistant constructive control via partitioning voters
(CCPV) Bartholdi, Tovey, Tricks model (see paragraph above).
Proof. proof follows via reduction X3C problem. fact, use exactly
construction proof Theorem 4.21. Let E = (C,V ) election constructed
proof.
Since candidate p defeats head-to-head contest, way p
become winner via partitioning voters guarantee p wins within subelection
wins within one. (Note since p Condorcet winner, p cannot win
subelections.)
contains k-element cover, say, {Sa1 , . . . , Sak }, letting Vp = Vb {va1 , . . . , vak }
Vs = V Vp make p Condorcet winner CCPV scenario.
335

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

converse, let (Vp ,Vs ) partition collection voters p global
Condorcet winner CCPV scenario use two subelections, one voters Vp
one voters Vs . Via paragraph assume, without loss generality, p
Condorcet winner (C,Vp ) Condorcet winner (C,Vs ). Since k + 1 voters
V Vb rank first rank p last, assume Vs contains k + 1 voters (i.e., voters
preference > r > B > p). Also, Vs contains k voters Vb , otherwise would
certainly Condorcet winner (C,Vs ).
result, p made Condorcet winner (C, Vb ) deleting k voters.
follows Claim 4.22 contains k-element cover B.


6. Conclusions
shown computational point view election systems Llull Copeland
(i.e., Copeland0.5 ) broadly resistant bribery constructive procedural control, regardless
whether voters required rational preferences. rather charming Llulls 700year-old system shows perfect resistance bribery resistances (constructive) control
natural system (even far modern ones) easy winner-determination
procedureother Copeland , 0 < < 1is known possess, even remarkable one considers Llulls system defined long control elections even
explicitly studied. Copeland0.5 voting matches Llulls perfect resistance bribery addition
perfect resistance (constructive) control.
natural open direction would study complexity control additional election
systems. would particularly interesting find existing, natural voting systems
polynomial-time winner determination procedures resistant standard types
constructive destructive control. would also extremely interesting find single results
classify, broad families election systems, precisely makes control easy hard,
i.e., obtain dichotomy meta-results control (see Hemaspaandra Hemaspaandra, 2007,
discussion regarding work flavor manipulation).

Acknowledgments
thank Nadja Betzler, Felix Brandt, Preetjot Singh, Frieder Stolzenburg, Dietrich Stoyan,
anonymous AAAI-07, AAIM-08, COMSOC-08, JAIR referees, JAIR handling editor Jeff
Rosenschein helpful comments, suggestions, guidance. work supported part
AGH-UST grant 11.11.120.777, DFG grants RO-1202/{9-3, 11-1, 12-1}, NSF grants CCR0311021, CCF-0426761, IIS-0713061, Alexander von Humboldt Foundations TransCoop
program, European Science Foundations EUROCORES program LogICCC, Friedrich Wilhelm Bessel Research Awards Edith Hemaspaandra Lane A. Hemaspaandra. work
done part visits Piotr Faliszewski, Edith Hemaspaandra, Lane A. Hemaspaandra
Heinrich-Heine-Universitat Dusseldorf, visits Jorg Rothe University Rochester,
Piotr Faliszewski University Rochester. paper combines extends
University Rochester Computer Science Department Technical Reports TR-913 TR-923,
papers results presented 22nd AAAI Conference Artificial
Intelligence (AAAI-07) Faliszewski et al. 2007, October 2007 Dagstuhl Seminar Computational Issues Social Choice, 4th International Conference Algorithmic Aspects
336

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

Information Management (AAIM-08) Faliszewski, Hemaspaandra, Hemaspaandra,
Rothe 2008a, 2nd International Workshop Computational Social Choice (COMSOC08).

References
Ahuja, R., Magnanti, T., & Orlin, J. (1993). Network Flows: Theory, Algorithms, Applications.
Prentice-Hall.
Altman, A., & Tennenholtz, M. (2007). axiomatic approach personalized ranking systems.
Proceedings 20th International Joint Conference Artificial Intelligence, pp. 1187
1192. AAAI Press.
Arrow, K. (1951 (revised editon, 1963)). Social Choice Individual Values. John Wiley
Sons.
Austen-Smith, D., & Banks, J. (2000). Positive Political Theory I: Collective Preference. University
Michigan Press.
Bartholdi, III, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social Choice
Welfare, 8(4), 341354.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989a). computational difficulty manipulating
election. Social Choice Welfare, 6(3), 227241.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989b). Voting schemes difficult tell
election. Social Choice Welfare, 6(2), 157165.
Bartholdi, III, J., Tovey, C., & Trick, M. (1992). hard control election? Mathematical
Computer Modeling, 16(8/9), 2740.
Betzler, N., Guo, J., & Niedermeier, R. (2008). Parameterized computational complexity Dodgson Young elections. Proceedings 11th Scandinavian Workshop Algorithm
Theory, pp. 402413. Springer-Verlag Lecture Notes Computer Science #5124.
Betzler, N., & Uhlmann, J. (2008). Parameterized complexity candidate control elections
related digraph problems. Proceedings 2nd Annual International Conference
Combinatorial Optimization Applications, pp. 4353. Springer-Verlag Lecture Notes
Computer Science #5156.
Brams, S., & Sanver, R. (2006). Critical strategies approval voting: gets ruled
ruled out. Electoral Studies, 25(2), 287305.
Condorcet, J. (1785). Essai sur lApplication de LAnalyse la Probabilite des Decisions Rendues
la Pluralite des Voix. Facsimile reprint original published Paris, 1972, Imprimerie
Royale.
Conitzer, V., & Sandholm, T. (2003). Universal voting protocol tweaks make manipulation hard.
Proceedings 18th International Joint Conference Artificial Intelligence, pp. 781
788. Morgan Kaufmann.
Conitzer, V., & Sandholm, T. (2006). Nonexistence voting rules usually hard manipulate. Proceedings 21st National Conference Artificial Intelligence, pp. 627634.
AAAI Press.
337

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidates hard
manipulate? Journal ACM, 54(3), Article 14.
Copeland, A. (1951). reasonable social welfare function. Mimeographed notes Seminar
Applications Mathematics Social Sciences, University Michigan.
Cormen, T., Leiserson, C., Rivest, R., & Stein, C. (2001). Introduction Algorithms (second
edition). MIT Press/McGraw Hill.
Downey, R. (2003). Parameterized complexity skeptic. Proceedings 18th Annual
IEEE Conference Computational Complexity, pp. 147168. IEEE Computer Society Press.
Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness shared beliefs:
GibbardSatterthwaite generalized. Social Choice Welfare, 17(1), 8593.
Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methods web.
Proceedings 10th International World Wide Web Conference, pp. 613622. ACM
Press.
Elkind, E., & Lipmaa, H. (2005). Small coalitions cannot manipulate voting. Proceedings
9th International Conference Financial Cryptography Data Security, pp. 285297.
Springer-Verlag Lecture Notes Computer Science #3570.
Ephrati, E., & Rosenschein, J. (1997). heuristic technique multi-agent planning. Annals
Mathematics Artificial Intelligence, 20(14), 1367.
Erdelyi, G., Hemaspaandra, L., Rothe, J., & Spakowski, H. (2007). approximating optimal
weighted lobbying, frequency correctness versus average-case polynomial time.
Proceedings 16th International Symposium Fundamentals Computation Theory,
pp. 300311. Springer-Verlag Lecture Notes Computer Science #4639.
Erdelyi, G., Nowak, M., & Rothe, J. (2008a). Sincere-strategy preference-based approval voting
broadly resists control. Proceedings 33rd International Symposium Mathematical
Foundations Computer Science, pp. 311322. Springer-Verlag Lecture Notes Computer
Science #5162.
Erdelyi, G., Nowak, M., & Rothe, J. (2008b). Sincere-strategy preference-based approval voting fully resists constructive control broadly resists destructive control. Tech. rep.
arXiv:0806.0535 [cs.GT], arXiv.org. precursor appears (Erdelyi, Nowak, & Rothe,
2008a). Journal version appear Mathematical Logic Quarterly.
Faliszewski, P. (2008). Nonuniform bribery (short paper). Proceedings 7th International
Conference Autonomous Agents Multiagent Systems, pp. 15691572. International
Foundation Autonomous Agents Multiagent Systems.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. complexity bribery elections.
Journal Artificial Intelligence Research. appear.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2006a). complexity bribery elections. Proceedings 21st National Conference Artificial Intelligence, pp. 641646.
AAAI Press. Journal version appear (Faliszewski, Hemaspaandra, & Hemaspaandra,
appear).

338

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2006b). hard bribery elections?
Tech. rep. TR-895, Department Computer Science, University Rochester, Rochester,
NY. Revised, September 2006.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Llull Copeland
voting broadly resist bribery control. Proceedings 22nd AAAI Conference
Artificial Intelligence, pp. 724730. AAAI Press.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2008a). Copeland voting fully
resists constructive control. Proceedings 4th International Conference Algorithmic Aspects Information Management, pp. 165176. Springer-Verlag Lecture Notes
Computer Science #5034.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2008b). Llull Copeland
voting computationally resist bribery control. Tech. rep. arXiv:0809.4484 [cs.GT], Computing Research Repository, http://www.acm.org/repository/.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). richer understanding
complexity election systems. Ravi, S., & Shukla, S. (Eds.), Fundamental Problems
Computing: Essays Honor Professor Daniel J. Rosenkrantz, pp. 375406. Springer.
Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: Ties matter. Proceedings 7th International Conference Autonomous Agents Multiagent Systems,
pp. 983990. International Foundation Autonomous Agents Multiagent Systems.
Friedgut, E., Kalai, G., & Nisan, N. (2008). Elections manipulated often. Proceedings
49rd IEEE Symposium Foundations Computer Science, pp. 243249. IEEE Computer Society.
Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory NPCompleteness. W. H. Freeman Company.
Ghosh, S., Mundhe, M., Hernandez, K., & Sen, S. (1999). Voting movies: anatomy
recommender systems. Proceedings 3rd Annual Conference Autonomous Agents,
pp. 434435. ACM Press.
Gibbard, A. (1973). Manipulation voting schemes. Econometrica, 41(4), 587601.
Hagele, G., & Pukelsheim, F. (2001). electoral writings Ramon Llull. Studia Lulliana,
41(97), 338.
Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy voting systems. Journal Computer System Sciences, 73(1), 7383.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007a). Anyone him: complexity
precluding alternative. Artificial Intelligence, 171(5-6), 255285.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007b). Hybrid elections broaden complexitytheoretic resistance control. Proceedings 20th International Joint Conference
Artificial Intelligence, pp. 13081314. AAAI Press. Journal version appear Mathematical Logic Quarterly.
Homan, C., & Hemaspaandra, L. Guarantees success frequency algorithm finding
Dodgson-election winners. Journal Heuristics. appear. Full version available (Homan
& Hemaspaandra, 2005).
339

fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE

Homan, C., & Hemaspaandra, L. (2005). Guarantees success frequency algorithm
finding Dodgson-election winners. Tech. rep. TR-881, Department Computer Science,
University Rochester, Rochester, NY. Revised, June 2007.
Kern, W., & Paulusma, D. (2001). new FIFA rules hard: Complexity aspects sports
competitions. Discrete Applied Mathematics, 108(3), 317323.
Lenstra, Jr., H. (1983). Integer programming fixed number variables. Mathematics
Operations Research, 8(4), 538548.
Levin, J., & Nalebuff, B. (1995). introduction vote-counting schemes. Journal Economic Perspectives, 9(1), 326.
McCabe-Dansted, J., Pritchard, G., & Slinko, A. (2008). Approximability Dodgsons rule. Social
Choice Welfare, 31(2), 311330.
McGarvey, D. (1953). theorem construction voting paradoxes. Econometrica, 21(4),
608610.
McLean, I., & Urken, A. (1995). Classics Social Choice. University Michigan Press.
Meir, R., Procaccia, A., Rosenschein, J., & Zohar, A. (2008). complexity strategic behavior
multi-winner elections. Journal Artificial Intelligence Research, 33, 149178.
Merlin, V., & Saari, D. (1997). Copeland method II: Manipulation, monotonicity, paradoxes.
Journal Economic Theory, 72(1), 148172.
Niedermeier, R. (2002). Invitation fixed-parameter algorithms. Habilitation thesis, University
Tubingen.
Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford University Press.
Procaccia, A., & Rosenschein, J. (2007). Junta distributions average-case complexity
manipulating elections. Journal Artificial Intelligence Research, 28, 157181.
Procaccia, A., Rosenschein, J., & Kaminka, G. (2007). robustness preference aggregation noisy environments. Proceedings 6th International Joint Conference
Autonomous Agents Multiagent Systems, pp. 416422. ACM Press.
Procaccia, A., Rosenschein, J., & Zohar, A. (2008). complexity achieving proportional
representation. Social Choice Welfare, 30(3), 353362.
Saari, D., & Merlin, V. (1996). Copeland method I: Relationships dictionary. Economic
Theory, 8(1), 5176.
Satterthwaite, M. (1975). Strategy-proofness Arrows conditions: Existence correspondence theorems voting procedures social welfare functions. Journal Economic
Theory, 10(2), 187217.
Stearns, R. (1959). voting problem. American Mathematical Monthly, 66(9), 761763.
Wagner, K. (1986). complexity combinatorial problems succinct input representations.
Acta Informatica, 23(3), 325356.
Xia, L., & Conitzer, V. (2008a). Generalized scoring rules frequency coalitional manipulability. Proceedings 9th ACM Conference Electronic Commerce, pp. 109118.
ACM Press.
340

fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL

Xia, L., & Conitzer, V. (2008b). sufficient condition voting rules frequently manipulable.
Proceedings 9th ACM Conference Electronic Commerce, pp. 99108. ACM Press.
Zermelo, E. (1929). Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der Wahrscheinlichkeitsrechnung. Mathematische Zeitschrift, 29(1), 436460.

341

fiJournal Artificial Intelligence Research 35 (2009) 557-591

Submitted 11/08; published 07/09

Optimal Value Information Graphical Models
Andreas Krause

KRAUSEA @ CALTECH . EDU

California Institute Technology,
1200 E California Blvd.,
Pasadena, CA 91125 USA

Carlos Guestrin

GUESTRIN @ CS . CMU . EDU

Carnegie Mellon University,
5000 Forbes Ave.,
Pittsburgh, PA 15213 USA

Abstract
Many real-world decision making tasks require us choose among several expensive observations. sensor network, example, important select subset sensors
expected provide strongest reduction uncertainty. medical decision making tasks, one
needs select tests administer deciding effective treatment.
general practice use heuristic-guided procedures selecting observations. paper,
present first efficient optimal algorithms selecting observations class probabilistic
graphical models. example, algorithms allow optimally label hidden variables Hidden
Markov Models (HMMs). provide results selecting optimal subset observations,
obtaining optimal conditional observation plan.
Furthermore prove surprising result: graphical models tasks, one designs
efficient algorithm chain graphs, HMMs, procedure generalized polytree graphical models. prove optimizing value information NPPP -hard even
polytrees. also follows results computing decision theoretic value information objective functions, commonly used practice, #P-complete problem even
Naive Bayes models (a simple special case polytrees).
addition, consider several extensions, using algorithms scheduling observation selection multiple sensors. demonstrate effectiveness approach several
real-world datasets, including prototype sensor network deployment energy conservation
buildings.

1. Introduction
probabilistic reasoning, one choose among several possible expensive observations,
often central issue decide variables observe order effectively increase
expected utility (Howard, 1966; Howard & Matheson, 1984; Mookerjee & Mannino, 1997;
Lindley, 1956). medical expert system, example, multiple tests available, test
different cost (Turney, 1995; Heckerman, Horvitz, & Middleton, 1993). systems,
thus important decide tests perform order become certain
patients condition, minimum cost. Occasionally, cost testing even exceed value
information possible outcome, suggesting discontinue testing.
following running example motivates research empirically evaluated Section 6.
Consider temperature monitoring task, wireless temperature sensors distributed across

c
2009
AI Access Foundation. rights reserved.

fiK RAUSE & G UESTRIN

building. task become certain temperature distribution, whilst minimizing
energy expenditure, critically constrained resource (Deshpande, Guestrin, Madden, Hellerstein, &
Hong, 2004). fine-grained building monitoring required obtain significant energy savings
(Singhvi, Krause, Guestrin, Garrett, & Matthews, 2005).
Many researchers suggested use myopic (greedy) approaches select observations (Scheffer, Decomain, & Wrobel, 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen,
1997; Bayer-Zubek, 2004; Kapoor, Horvitz, & Basu, 2007). Unfortunately, general, heuristic provide performance guarantees. paper, present efficient algorithms,
guarantee optimal nonmyopic value information chain graphical models. example,
algorithms used optimal active labeling hidden states Hidden Markov Models (HMMs, Baum & Petrie, 1966). address two settings: subset selection, optimal
subset observations obtained open-loop fashion, conditional plans, sequential,
closed-loop plan observation strategy depends actual value observed variables (c.f., Figure 1). knowledge, first optimal efficient algorithms
observation selection diagnostic planning based value information class graphical models. settings, address filtering smoothing versions: Filtering
important online decision making, decisions utilize observations made
past. Smoothing arises example structured classification tasks, temporal
dimension data, hence observations taken account. call approach
VO IDP algorithms use Dynamic Programming optimize Value Information. evaluate VO IDP algorithms empirically three real-world datasets, also show
well-suited interactive classification sequential data.
inference problems graphical models, computing marginal distributions
finding probable explanation, solved efficiently chain-structured graphs,
also solved efficiently polytrees. prove problem selecting best k
observations maximizing decision theoretic value information NPPP -complete even
discrete polytree graphical models, giving complexity theoretic classification core artificial
intelligence problem. NPPP -complete problems believed significantly harder NPcomplete even #P-complete problems commonly arising context graphical models.
furthermore prove evaluating decision-theoretic value information objective functions
#P-complete even case Naive Bayes models, simple special case polytree graphical
models frequently used practice (c.f., Domingos & Pazzani, 1997).
Unfortunately, hardness results show that, problem scheduling single sensor optimally solved using algorithms, problem scheduling multiple, correlated
sensors wildly intractable. Nevertheless, show VO IDP algorithms single sensor
scheduling used approximately optimize multi-sensor schedule. demonstrate
effectiveness approach real sensor network testbed building management.
summary, provide following contributions:
present first optimal algorithms nonmyopically computing optimizing value
information chain graphical models.
show optimizing decision theoretic value information NPPP -hard discrete
polytree graphical models. computing decision theoretic value information #Phard even Naive Bayes models.

558

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS



Tmorn =high?

Tnoon =high?

yes

Teve =high?

Figure 1: Example conditional plan.
present several extensions algorithms, e.g., tree graphical models
leaves, multiple correlated chains (for multi-sensor scheduling).
extensively evaluate algorithms several real-world problems, including sensor
scheduling real sensor testbed active labeling bioinformatics Natural Language Processing.

2. Problem Statement
assume state world described collection random variables
XV = (X1 , . . . , Xn ), V index set. example, V could denote set locations, Xi
models temperature reading sensor placed location V. subset
= {i1 , . . . , ik } V, use notation XA refer random vector XA = (Xi1 , . . . , Xik ).
algorithms extend continuous distributions, generally assume variables XV discrete. take Bayesian approach, assume prior probability distribution
P (XV ) outcomes variables. Suppose select subset variables, XA (for
V), observe XA = xA . example, set locations place sensors,
set medical tests decide perform. observing realization variables
XA = xA , compute posterior distribution variables P (XV | XA = xA ). Based
posterior probability obtain reward R(P (XV | XA = xA )). example, reward function could depend uncertainty (e.g., measured entropy) distribution
P (XV | XA = xA ). describe several examples detail below.
general, selecting observation, know ahead time observations
make. Instead, distribution possible observations. Hence,
interested expected reward, take expectation possible observations.
optimizing selection variables, consider different settings: subset selection, goal pick subset V variables, maximizing
X
= argmax
P (XA = xA )R(P (XV | XA = xA )),
(1)


xA

impose constraints set allowed pick (e.g., number
variables selected, etc.). subset selection setting, commit selection
variables get see realization.
Instead, also sequentially select one variable other, letting choice depend
observations made past. setting, would like find conditional plan

559

fiK RAUSE & G UESTRIN

maximizes
= argmax


X

P (xV )R(P (XV | X(xV ) = x(xV ) )).

(2)

xV

Hereby, conditional plan select different set variables possible state
world xV . use notation (xV ) V refer subset variables selected
conditional plan state XV = xV . Figure 1 presents example conditional plan
temperature monitoring example. define notion conditional planning formally
Section 4.2.
general setup selecting observations goes back decision analysis literature
notion value information Howard (1966) statistical literature notion
Bayesian Experimental Design Lindley (1956). paper, refer Problems (1)
(2) problems optimizing value information.
paper, show complexity solving value information problems depend properties probability distribution P . give first algorithms optimally
solving value information interesting challenging class distributions including Hidden Markov Models. also present hardness results showing optimizing value information
wildly intractable (NPPP -complete) even probability distributions efficient inference possible (even Naive Bayes models discrete polytrees).
2.1 Optimization Criteria
paper, consider class local reward1 functions Ri , defined
marginal probability distributions variables Xi . class computational advantage
local rewards evaluated using probabilistic inference techniques. total reward
sum local rewards.
Let subset V. P (Xj | XA = xA ) denotes marginal distribution variable Xj conditioned observations XA = xA . example, temperature monitoring
application, Xj models temperature location j V. conditional marginal distribution
P (Xj = xj | XA = xA ) models conditional distribution temperature location j
observing temperature locations V.
classification purposes, appropriate consider max-marginals
P max (Xj = xj | XA = xA ) = max P (XV = xV , Xj = xj | XA = xA ),
xV

is, Xj set value xj , probability probable assignment XV = xV
random variables (including Xj simplicity notation) conditioned observations
XA = xA .
local reward Rj functional probability distribution P P max Xj .
is, Rj takes entire distribution variable Xj maps reward value. Typically,
reward functions chosen certain peaked distributions obtain higher reward.
simplify notation, write
Rj (Xj | xA ) , Rj (P (Xj | XA = xA ))
1. Local reward functions also widely used additively independent utility models, (c.f., Keeney & Raiffa, 1976).

560

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

denote reward variable Xj upon observing XA = xA ,
X
Rj (Xj | XA ) ,
P (XA = xA )Rj (Xj | xA )
xA

refer expected local rewards, expectation taken assignments xA
observations A. Important local reward functions include:
Residual entropy. set
Rj (Xj | xA ) = H(Xj | xA ) =

X

P (xj | xA ) log2 P (xj | xA ),

xj

objective optimization problem becomes minimize sum residual entropies. Optimizing reward function attempts reduce uncertainty predicting marginals Xi .
choose reward function running example measure uncertainty temperature distribution.
P
Joint entropy. Instead minimizing sum residual entropies H(Xi ), also attempt minimize joint entropy entire distribution,
X
H(XV ) =
P (xV ) log2 P (xV ).
xV

Note, joint entropy depends full probability distribution P (XV ), rather
marginals P (Xi ), hence local. Nevertheless, exploit chain rule joint
entropy H(XB ) set random variables B = {1, . . . , m} (c.f., Cover & Thomas, 1991),
H(XB ) = H(X1 ) + H(X2 | X1 ) + H(X3 | X1 , X2 ) + + H(Xm | X1 , . . . , Xm1 ).
Hence, choose local reward functions Rj (Xj | XA ) = H(Xj | X1 , . . . , Xj1 , XA ),
optimize non-local reward function (the joint entropy) using local reward functions.
Decision-theoretic value information. concept local reward functions also includes
concept decision theoretic value information. notion value information widely
used (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), formalized, e.g.,
context influence diagrams (Howard & Matheson, 1984) Partially Observable Markov Decision Processes (POMDPs, Smallwood & Sondik, 1973). variable Xj , let Aj finite set
actions. Also, let Uj : Aj dom Xj R utility function mapping action Aj
outcome x dom Xj real number. maximum expected utility principle states actions
selected maximize expected utility,
X
EUj (a | XA = xA ) =
P (xj | xA )Uj (a, xj ).
xj

certain Xj , economically choose action. idea
captured notion value information, choose local reward function
Rj (Xj | xA ) = max EUj (a | xA ).


561

fiK RAUSE & G UESTRIN

Margin structured prediction. also consider margin confidence:
Rj (Xj | xA ) = P max (xj | xA ) P max (xj | xA ),

xj = argmax P max (xj | xA ) xj = argmax P max (xj | xA ),
xj 6=xj

xj

describes margin likely outcome closest runner up. reward
function useful structured classification purposes, shown Section 6.
Weighted mean-squared error. variables continuous, might want minimize
mean squared error prediction. choosing
Rj (Xj | xA ) = wj Var(Xj | xA ),

Z
Var(Xj | xA ) =


P (xj | xA ) xj

Z

x0j P (x0j

|

xA )dx0j

2
dxj

conditional variance Xj given XA = xA , wj weight indicating importance
variable Xj .
Monitoring critical regions (Hotspot sampling). Suppose want use sensors detecting fire. generally, want detect, j, whether Xj Cj , Cj dom Xj
critical region variable Xj . local reward function
Rj (Xj | xA ) = P (Xj Cj | xA )
favors observations maximize probability detecting critical regions.
Function optimization (Correlated bandits). Consider setting collection
random variables XV taking numerical
P values interval [m, m], and, selecting
variables, get reward xi . setting arises want optimize unknown
(random) function, evaluating function expensive. setting, encouraged
evaluate function likely obtain high values. maximize expected
total reward choose local reward function
Z
Rj (Xj | xA ) = xj P (xj | xA )dxj ,
i.e., expectation variable Xj given observations xA . setting optimizing random
function also considered version classical k-armed bandit problem correlated
arms. details relationship bandit problems given Section 8.
examples demonstrate generality notion local reward. Note examples apply continuous distributions well discrete distributions.

562

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

2.2 Cost Selecting Observations
also want capture constraint observations expensive. mean
observation Xj associated positive penalty Cj effectively decreases reward.
example, might interested trading accuracy sensing energy expenditure. Alternatively, also possible define budget B selecting observations, one associated
integer cost j . Here, want select observations whose sum cost within budget,
costs decrease reward. running example, sensors could powered
solar power, regain certain amount energy per day, allows certain amount
sensing. formulation optimization
penalties budgets.
P problems allows P
simplify notation also write C(A) = jA Cj (A) = jA j extend C sets.
Instead fixed penalties costs per observation, also depend state
world. example, medical domain, applying particular diagnostic test bear different
risks health patient, depending patients illness. algorithms develop
adapted accommodate dependencies straight-forward manner.
present details conditional planning algorithm Section 4.2.

3. Decomposing Rewards
section, present key observation allows us develop efficient algorithms
nonmyopically optimizing value information class chain graphical models.
algorithms presented Section 4.
set random variables XV = {X1 , . . . , Xn } forms chain graphical model (a chain),
Xi conditionally independent XV\{i1,i,i+1} given Xi1 Xi+1 . Without loss generality
assume joint distribution specified prior P (X1 ) variable X1
conditional probability distributions P (Xi+1 | Xi ). time series model temperature
measured one sensor example formulated chain graphical model. Note
transition probabilities P (Xi+1 | Xi ) allowed depend index (i.e., chain models
allowed nonstationary). Chain graphical models extensively used machine
learning signal processing.
Consider example Hidden Markov Model unrolled n time steps, i.e., V partitioned hidden variables {X1 , . . . , Xn } emission variables {Y1 , . . . , Yn }. HMMs,
Yi always observed variables Xi form chain. many applications,
discussed Section 6, observe hidden variables Xi well, e.g., asking
expert, addition observing emission variables. cases, problem selecting
expert labels also belongs class chain graphical models addressed paper, since
variables Xi form chain conditional observed values emission variables Yi . idea
generalized class Dynamic Bayesian Networks separators time
slices size one, separators selected observation. formulation
also includes certain conditional random fields (Lafferty, McCallum, & Pereira, 2001) form
chains, conditional emission variables (the features).
Chain graphical models originating time series additional, specific properties:
system online decision making, observations past present time steps
taken account, observations made future. generally referred
filtering problem. setting, notation P (Xi | XA ) refer distribution
Xi conditional observations XA prior including time i. structured classification
563

fiK RAUSE & G UESTRIN

Figure 2: Illustration decomposing rewards idea. reward chain 1:7 observing
variables X1 , X4 X7 decomposes sum chain 1:4 plus reward chain
4:7 plus immediate reward observing XP
4 minus cost observing X4 . Hereby
brevity use notation Rew(a : b) = bj=a Rj (Xj | X1 , X4 , X7 ).

problems discussed Section 6, general observations made anywhere chain must
taken account. situation usually referred smoothing problem. provide
algorithms filtering smoothing.
describe key insight, allows efficient optimization chains. Consider
set observations V. j variable observed, i.e., j A, local reward
simply R(Xj | XA ) = R(Xj | Xj ). consider j
/ A, let Aj subset
containing closest ancestor (and smoothing problem also closest descendant) Xj
XA . conditional independence property graphical model implies that, given XAj , Xj
independent rest observed variables, i.e., P (Xj | XA ) = P (Xj | XAj ). Thus,
follows R(Xj | XA ) = R(Xj | XAj ).
observations imply expected reward set observations decomposes
along chain. simplicity notation, add two independent dummy variables X0 Xn+1 ,
R0 = C0 = 0 = Rn+1 = Cn+1 = n+1 = 0. Let = {i0 , . . . P
, im+1 } il < il+1 ,
i0 = 0 im+1 = n + 1. Using notation, total reward R(A) = j Rj (Xj | XA )
smoothing case given by:


iv+1 1

X
X
Riv (Xiv | Xiv ) Civ +
Rj (Xj | Xiv , Xiv+1 ) .
v=0

j=iv +1

filtering settings, simply replace Rj (Xj | Xiv , Xiv+1 ) Rj (Xj | Xiv ). Figure 2 illustrates
decomposition.

4. Efficient Algorithms Optimizing Value Information
section, present algorithms efficiently nonmyopically optimizing value information chain graphical models.
4.1 Efficient Algorithms Optimal Subset Selection Chain Models
subset selection problem, want find informative subset variables observe
advance, i.e., observations made. running example, would,
deploying sensors, identify k time points expected provide informative
sensor readings according model.

564

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

First, define objective function L subsets V
L(A) =

n
X

Rj (Xj | XA ) C(A).

(3)

j=1

subset selection problem find optimal subset
=

argmax L(A)
AV,(A)B

maximizing sum expected local rewards minus penalties, subject constraint
total cost must exceed budget B.
solve optimization problem using dynamic programming algorithm, chain
broken sub-chains using insight Section 3. Consider sub-chain variable Xa
Xb . define Lsm
a:b (k) represent expected total reward sub-chain Xa , . . . , Xb ,
lt
smoothing setting Xa Xb observed, budget level k. Lfa:b
(k) represents
expected reward filtering setting Xa observed. formally:
lt
(k)
Lfa:b

=

b1
X

max

A{a+1...b1}
j=a+1
(A)k

Rj (Xj | XA , Xa ) C(A),

filtering version,
Lsm
a:b (k) =

b1
X

max

Rj (Xj | XA , Xa , Xb ) C(A),

A{a+1...b1}
j=a+1
(A)k

smoothing version. Note cases, L0:n+1 (B) = maxA:(A)B L(A), Equation (3), i.e., computing values La:b (k), compute maximum expected total reward
entire chain.
f lt
compute Lsm
a:b (k) La:b (k) using dynamic programming. base case simply:
lt
Lfa:b
(0) =

b1
X

Rj (Xj | Xa ),

j=a+1

filtering,
b1
X

Lsm
a:b (0) =

Rj (Xj | Xa , Xb ),

j=a+1

smoothing. recursion La:b (k) two cases: choose spend
budget, reaching base case, break chain two sub-chains, selecting optimal
observation Xj , < j < b. filtering smoothing


La:b (k) = max La:b (0),
max
{Rj (Xj | Xj ) Cj + La:j (0) + Lj:b (k j )} .
j:a<j<b,j k

565

fiK RAUSE & G UESTRIN

Input: Budget B, rewards Rj , costs j penalties Cj
Output: Optimal selection observation times
begin
0 < b n + 1 compute La:b (0);
k = 1 B
0 < b n + 1
sel(1) := La:b (0);
j = + 1 b 1 sel(j) := Rj (Xj | Xj ) Cj + La:j (0) + Lj:b (k j );
La:b (k) = maxj{a+1,...,b1,1} sel(j);
a:b (k) = argmaxj{a+1,...,b1,1} sel(j);
end
end
:= 0; b := n + 1; k := B; := ;
repeat
j := a:b (k);
j 0 := {j}; := j; k := k j ;
j = 1 ;
end
Algorithm 1: VO IDP algorithm optimal subset selection (for filtering smoothing).
first, may seem recursion consider optimal split budget
two sub-chains. However, since subset problem open-loop order observations
irrelevant, need consider split points first sub-chain receives zero budget.
pseudo code implementation dynamic programming approach, call VO IDP
subset selection given Algorithm 1. algorithm fills dynamic programming tables
two loops, inner loop ranging pairs (a, b), < b, outer loop increasing k. Within
inner loop, computing best reward sub-chain b, fills table sel,
sel(j) reward could obtained making observation j, sel(1)
reward observation made.
addition computing optimal rewards La:b (k) could achieved sub-chain : b
budget k, algorithm also stores choices a:b (k) realize maximum score. Here,
a:b (k) index next variable selected sub-chain : b budget
k, 1 variable selected. order recover optimal subset budget k,
Algorithm 1 uses quantities a:b recover optimal subset tracing maximal values
occurring dynamic programming equations. Using induction proof, obtain:
Theorem 1 (Subset Selection). dynamic programming algorithm described computes
optimal subset budget B ( 61 n3 + O(n2 ))B evaluations expected local rewards.
Note consider different costs variable, would simply choose j =
1 variables compute La:b (N ). note variables Xi continuous,
algorithm still applicable integrations inferences necessary computing
expected rewards performed efficiently. case, example, Gaussian linear
model (i.e., variables Xi normally distributed) local reward functions residual
entropies residual variances variable.

566

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

4.2 Efficient Algorithms Optimal Conditional Planning Chain Models
conditional plan problem, want compute optimal sequential querying policy :
observe variable, pay penalty, depending values observed past, select next
query, proceeding long budget suffices. objective find plan highest
expected reward, where, possible sequence observations, budget B exceeded.
filtering, select observations future, whereas smoothing case, next
observation anywhere chain. running example, filtering algorithm would
appropriate: sensors would sequentially follow conditional plan, deciding
informative times sense based previous observations. Figure 1 shows example
conditional plan.
4.2.1 F ROM UBSET ELECTION C ONDITIONAL P LANNING
Note contrast subset selection setting considered Section 4.1, conditional planning, set variables depends state world XV = xV . Hence,
state, conditional plan could select different set variables, (xV ) V. example, consider Figure 1, set possible observations V = {morn, noon, eve},
XV = {Tmorn , Tnoon , Teve }. world state xV = (high, low, high), conditional
plan presented Figure 1 would select (xV ) = {morn, eve}, whereas,
xV = (low, low, high), would select (xV ) = {morn, noon}. Since conditional plan
function (random) state world, set-valued random variable. order optimize
Problem (2), define objective function2
J() =

X

P (xV )

xV

n
X



Rj (Xj | x(xV ) ) C((xV )) ,

j=1

i.e., expected sum local rewards given observations made plan (xV ) state XV = xV
minus penalties selected variables, expectation taken respect
distribution P (XV ). addition defining value policy J(), also define cost ()
() = max ((xV )),
xV

maximum cost (A) (as defined Section 2.2) set = (xV ) could selected
policy , state world XV = xV .
Based notation, goal find policy
= argmax J() () B,


i.e., policy maximum value, guaranteed never cost exceeding budget
B. Hereby class sequential policies (i.e., those, observations chosen
sequentially, based observations previously made).
useful introduce following notation:
J(xA ; k) = max J( | XA = xA ) () k,


(4)

2. Recall that, filtering setting, R(Xj | x(xV ) ) , R(Xj | xA0 ), A0 = {t (xV ) s.t. j}, i.e.,
observations past taken account.

567

fiK RAUSE & G UESTRIN


J( | XA = xA ) =

X

n
X


P (xV | XA = xA )
Rj (Xj | x(xV ) ) C((xV )) .

xV

j=1

Hence, J(xA ; k) best possible reward achieved sequential policy cost
k, observing XA = xA . Using notation, goal find optimal plan
reward J(; B).
value function J satisfies following recursion. base case considers exhausted
budget:
X
J(xA ; 0) =
Rj (Xj | xA ) C(A).
jV

recursion, holds





X
J(xA ; k) = max J(xA ; 0), max
,
P (xj | xA )J(xA , xj ; k j )


j
/

(5)

xj

i.e., best one state XA = xA budget k either stop selecting variables,
chose best next variable act optimally thereupon.
Note easily allow cost j depend state xj variable Xj . case,
would simply replace j j (xj ), define J(XA , r) = whenever r < 0. Equivalently,
let penalty C(A) depend state replacing C(A) C(xA ).
Relationship finite-horizon Markov Decision Processes (MDPs). Note function
J(xA ; k) defined (4) analogous concept value function Markov Decision Processes (c.f., Bellman, 1957): finite-horizon MDPs, value function V (s; k) models maximum expected reward obtainable starting state performing k actions. value
function holds
X
V (s; k) = R(s, k) + max
P (s0 | s, a)V (s0 ; k 1),


s0

P (s0 | s, a) probability transiting state s0 performing action state s,
R(s, k) immediate reward obtained state k steps still left. recursion,
similar Eq. (5), exploited value iteration algorithm solving MDPs.
conditional planning problem unit observation cost (i.e., (A) = |A|) could modeled
finite-horizon MDP, states correspond observed evidence XA = xA , actions correspond
observing variables (or making observation) transition probabilities given
probability observing particular instantiation selected variable. immediate reward
R(s, k) = 0 k > 0, R(s, 0) expected reward (in value information problem)
observing assignment (i.e., R(P (XV | s)) C(s)). observations unit cost,
MDP, holds V (xA ; k) = J(xA ; k). Unfortunately, conditional planning problem, since
state MDP uniquely determined observed evidence XA = xA , state space
exponentially large. Hence, existing algorithms solving MDPs exactly (such value iteration)
cannot applied solve large value information problems. Section 4.2.2, develop
efficient dynamic programming algorithm conditional planning chain graphical models
avoids exponential increase complexity.
568

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

4.2.2 DYNAMIC P ROGRAMMING PTIMAL C ONDITIONAL P LANNING C HAINS
propose dynamic programming algorithm obtaining optimal conditional plan
similar subset algorithm presented Section 4.1. Again, utilize decomposition
rewards described Section 3. difference observation selection budget
allocation depend actual values observations. order compute value
function J(xA ; k) entire chain, compute value functions Ja:b (xA ; k) subchains Xa , . . . , Xb .
base case dynamic programming approach deals zero budget setting:
f lt
Ja:b
(xa ; 0)

=

b1
X

Rj (Xj | Xa = xa ),

j=a+1

filtering,
sm
(xa , xb ; 0) =
Ja:b

b1
X

Rj (Xj | Xa = xa , Xb = xb ),

j=a+1

smoothing. recursion defines Ja:b (xa ; k) (or Ja:b (xa , xb ; k) smoothing), expected
reward problem restricted sub-chain Xa , . . . , Xb conditioned values Xa = xa
(and Xb = xb smoothing), budget limited k. compute quantity,
iterate possible split points j, < j < b. observe notable difference
filtering smoothing case. smoothing, must consider possible
splits budget two resulting sub-chains, since observation time j might
require us make additional, earlier observation:
X

n
sm
sm
P (Xj = xj | Xa = xa , Xb = xb )
Ja:b (xa ,xb ; k) = max Ja:b (xa , xb ; 0), max
a<j<b

Rj (Xj | xj ) Cj (xj ) +

max
0lkj (xj )

xj



sm
Ja:j
(xa , xj ; l)

+

sm
(xj , xb ; k
Jj:b



.
l j (xj ))

Looking back time possible filtering case, hence recursion simplifies

X
n
f lt
f lt
Ja:b (xa ; k) = max Ja:b (xa ; 0),
max
P (Xj = xj | Xa = xa )
a<j<b:j (xj )k

Rj (Xj | xj ) Cj (xj ) +

xj

f lt
Ja:j
(xa ; 0)

+

f lt
Jj:b
(xj ; k


j (xj ))
.

J f lt J sm , optimal reward obtained J0:n+1 (; B) = J(; B) = J( ).
Algorithm 2 presents pseudo code implementation smoothing version filtering case
straight-forward modification. call Algorithm 2 VO IDP algorithm conditional planning. algorithm fill dynamic programming tables using three loops, inner loop
ranging assignments xa , xb , middle loop ranging pairs (a, b) < b,
outer loop covers increasing values k B. Within innermost loop, algorithm
computes table sel sel(j) optimal reward achievable selecting variable j next.
569

fiK RAUSE & G UESTRIN

value expectation possible observation variable Xj make. Note
every possible instantiation Xj = xj different allocation remaining budget k j (xj )
left right sub-chain (a : j j : b respectively) chosen. quantity (j, xj )
tracks optimal budget allocation.
Input: Budget B, rewards Rj , costs j penalties Cj
Output: Optimal conditional plan (a:b , a:b )
begin
sm (x , x ; 0);
0 < b n + 1, xa dom Xa , xb dom Xb compute Ja:b
b
k = 1 B
0 < b n+1, xa dom Xa , xb dom Xb
sm (0);
sel(1) := Ja:b
< j < b
sel(j) := 0;
xj dom Xj
0 l k j (xj )
sm (x , x ; l) + J sm (x , x ; k l (x ));
bd(l) := Ja:j
j
j j
j
b
j:b
end
sel(j) := sel(j) + P (xj | xa , xb ) [Rj (Xj | xj ) Cj (xj ) + maxl bd(j)];
(j, xj ) = argmaxl bd(j);
end
end
sm (k) = max
Ja:b
j{a+1,...,b1,1} sel(j);
a:b (xa , xb ; k) = argmaxj{a+1,...,b1,1} sel(j);
xj dom Xa:b (k) a:b (xa , xb , xj ; k) = (a:b (k), xj );
end
end
end
Algorithm 2: VO IDP algorithm computating optimal conditional plan (for smoothing
setting).
Input: Budget k, observations Xa = xa , Xb = xb , ,
begin
j := a:b (xa , xb ; k);
j 0
Observe Xj = xj ;
l := a:b (xa , xb , xj ; k);
Recurse k := l, Xa = xa Xj = xj instead Xb = xb ;
Recurse k := k l j , Xj = xj instead Xa = xa , Xb = xb ;
end
end
Algorithm 3: Observation selection using conditional planning.
plan compactly encoded quantities a:b a:b . Hereby, a:b (xa , xb ; k)
determines next variable query observing Xa = xa Xb = xb , remaining budget k. a:b (xa , xb , xj ; k) determines allocation budget new observation
Xj = xj made. Considering exponential number possible sequences observations,
570

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

remarkable optimal plan represented using polynomial space. Algorithm 3
indicates computed plan executed. procedure recursive, requiring parameters := 0, xa := 1, b := n + 1, xb := 1 k := B initial call. temperature
monitoring example, could first collect temperature timeseries training data,
learn chain model data. Offline, would compute conditional plan (for
filtering setting), encode quantities a:b a:b . would deploy computed
plan actual sensor node, together implementation Algorithm 3. computation optimal plan (Algorithm 2) fairly computationally expensive, execution plan
(Algorithm 3) efficient (selecting next timestep observation requires single lookup
a:b a:b tables) hence well-suited deployment small, embedded device.
summarize analysis following Theorem:
Theorem 2 (Conditional Planning). algorithm smoothing presented computes
optimal conditional plan d3 B 2 ( 61 n3 + O(n2 )) evaluations local rewards,
maximum domain size random variables X1 , . . . , Xn . filtering case, optimal plan computed using d3 B ( 61 n3 + O(n2 )) evaluations, or, budget used,
d3 ( 16 n4 + O(n3 )) evaluations.
faster computation filtering / no-budget case obtained observing
require third maximum computation, distributes budget sub-chains.
Also, note contrary algorithm computing optimal subsets Section 4.1, Algorithm 2 requires evaluations form R(Xj | XA = xA ), general computed
d2 times faster expectations R(Xj | XA ). consideration, subset selection
algorithm general factor B faster, even though conditional planning algorithm
nested loops.
4.3 Efficient Algorithms Trees Leaves
Sections 4.1 4.2 presented dynamic programming-based algorithms optimize value information chain graphical models. fact, key observations Section 3
local rewards decompose along chains holds chain graphical models, also trees.
formally, tree graphical model joint probability distribution P (XV ) collection
random variables XV P (XV ) factors
P (XV ) =

1
i,j (Xi , Xj ),
Z
(i,j)E

i,j nonnegative potential function, mapping assignments xi xj nonnegative
real numbers, E V V set edges form undirected tree index set V, Z
normalization constant enforcing valid probability distribution.
dynamic programming algorithms presented previous sections extended
tree models straightforward manner. Instead identifying optimal subsets conditional
plans sub-chains, algorithms would select optimal subsets plans sub-trees
increasing size. Note however number sub-trees grow exponentially number
leaves tree: star n leaves example number subtrees exponential
n. fact, counting number subtrees arbitrary tree n vertices believed
intractable (#P-complete, Goldberg & Jerrum, 2000). However, trees contain
571

fiK RAUSE & G UESTRIN

small (constant) number leaves, number subtrees polynomial, optimal subset
conditional plans computed polynomial time.

5. Theoretical Limits
Many problems solved efficiently discrete chain graphical models also efficiently solved discrete polytrees3 . Examples include probabilistic inference probable explanation (MPE).
Section 4.3 however seen complexity dynamic programming algorithms chains increases dramatically extended trees: complexity increases exponentially number leafs tree.
prove that, perhaps surprisingly, problem optimizing value information,
exponential increase complexity cannot avoided, reasonable complexity theoretic assumptions. making statement formal, briefly review complexity classes
used results.
5.1 Brief Review Relevant Computational Complexity Classes
briefly review complexity classes used following statements presenting complete
problem class. details see, e.g., references Papadimitriou (1995)
Littman, Goldsmith, Mundhenk (1998). class NP contains decision problems
polynomial-time verifiable proofs. well-known complete problem 3SAT
instances Boolean formulas conjunctive normal form containing three literals per
clause (3CNF form). complexity class #P contains counting problems. complete problem
class #P #3SAT counts number satisfying instances 3CNF formula.
PP decision version class #P: complete problem AJSAT , decides
whether given 3CNF formula satisfied majority, i.e., half
possible assignments. B Turing machine based complexity classes, AB
complexity class derived allowing Turing machines deciding instances oracle calls
Turing machines B. intuitively think problems class AB
solved Turing Machine class A, special command solves problem B.
PP similar #P PPP = P#P , i.e., allow deterministic polynomial time Turing
machine access counting oracle, cannot solve complex problems give
access majority oracle. Combining ideas, class NPPP class problems
solved nondeterministic polynomial time Turing machines access majority
(or counting) oracle. complete problem NPPP EM AJSAT which, given 3CNF
variables X1 , . . . , X2n , decides whether exists assignment X1 , . . . , Xn
satisfied majority assignments Xn+1 , . . . , X2n . NPPP introduced
found natural class modeling AI planning problems seminal work Littman et al.
(1998). example, MAP assignment problem NPPP -complete general graphical
models, shown Park Darwiche (2004).
complexity classes satisfy following set inclusions (where inclusions assumed,
known strict):
P NP PP PPP = P#P NPPP .
3. Polytrees Bayesian Networks form trees edge directions dropped.

572

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

5.2 Complexity Computing Optimizing Value Information
order solve optimization problems, likely evaluate objective
function, i.e., expected local rewards. first result states that, even specialize decision theoretic value information objective functions defined Section 2.1, problem
intractable even Naive Bayes models, special case discrete polytrees. Naive Bayes models
often used classification tasks (c.f., Domingos & Pazzani, 1997), class variable
predicted noisy observations (features), assumed conditionally independent given
class variable. sense, Naive Bayes models next simplest (from perspective
inference) class Bayesian networks chains. Note Naive Bayes models correspond
stars referred Section 4.3, number subtrees exponential number
variables.
Theorem 3 (Hardness computation Naive Bayes models). computation decision
theoretic value information functions #P-complete even Naive Bayes models. also
hard approximate factor unless P = NP.
immediate corollary subset selection problem PP-hard Naive Bayes
models:
Corollary 4 (Hardness subset selection Naive Bayes models). problem determining,
given Naive Bayes model, constants c B, cost function set decision-theoretic value
information objective functions Ri , whether subset variables V
L(A) c (A) B PP-hard.
fact, show subset selection arbitrary discrete polytrees (that general
Naive Bayes models, inference still tractable) even NPPP -complete, complexity
class containing problems believed significantly harder NP #P complete
problems. result provides complexity theoretic classification value information, core
AI problem.
Theorem 5 (Hardness subset selection computation polytrees). problem determining, given discrete polytree, constants c B, cost function set decision-theoretic
value information objective functions Ri , whether subset variables V
L(A) c (A) B NPPP -complete.
running example, implies generalized problem optimally selecting k sensors
network correlated sensors likely computationally intractable without resorting
heuristics. corollary extends hardness subset selection hardness conditional plans.
Corollary 6 (Hardness conditional planning computation polytrees). Computing conditional plans PP-hard Naive Bayes models NPPP -hard discrete polytrees.
proofs results section stated Appendix. rely reductions complete
problems NP, #P NPPP involving boolean formulae problems computing / optimizing value information. reductions inspired works Littman et al. (1998) Park
Darwiche (2004), require development novel techniques, new reductions
Boolean formulae Naive Bayes polytree graphical models associated appropriate reward
functions, ensuring observation selections lead feasible assignments Boolean formulae.
573

fiK RAUSE & G UESTRIN

Percent improvement

10

1

Optimal conditional plan

8

Mean margin optimal subset
Mean margin greedy heuristic

1
0.98

0.9

Mean F1 score

0.96

6

0.94

0.8

4
2

0.7
Optimal subset
Greedy heuristic

0
1

4

8
12
16
20
Number observations

24

(a) Sensor scheduling

0.6

0.9

Mean accuracy
greedy heuristic
1

2
3
4
5
Number observations

(b) CpG island detection

Mean margin

0.92

Mean accuracy
optimal subset

6

0.88
0.86
0

10
20
30
40
Number observations

50

(c) Part Speech Tagging

Figure 3: Experimental results. (a) Temperature data: Improvement uniform spacing
heuristic. (b) CpG island data set: Effect increasing number observations
margin classification accuracy. (c) Part-of-Speech tagging data set: Effect increasing number observations margin F1 score.

6. Experiments
section, evaluate algorithms several real world data sets. special focus
comparison optimal methods greedy heuristic heuristic methods selecting observations, algorithms used interactive structured classification.
6.1 Temperature Time Series
first data set consists temperature time series collected sensor network deployed
Intel Research Berkeley (Deshpande et al., 2004) described running example. Data
continuously collected 19 days, linear interpolation used case missing samples.
temperature measured every 60 minutes, discretized 10 bins 2 degrees
Kelvin. avoid overfitting, used pseudo counts = 0.5 learning model. Using
parameter sharing, learned four sets transition probabilities: 12 - 7am, 7 - 12 pm,
12 pm - 7 pm 7 pm - 12 am. Combining data three adjacent sensors, got 53 sample
time series.
goal task select k 24 time points day, sensor
readings informative. experiment designed compare performance
optimal algorithms, greedy heuristic, uniform spacing heuristic, distributed k
observations uniformly day. Figure 3(a) shows relative improvement optimal algorithms greedy heuristic uniform spacing heuristic. performance measured
decrease expected entropy, zero observations baseline. seen k less
half possible observations, optimal algorithms decreased expected uncertainty several percent heuristics. improvement gained optimal plan
subset selection algorithms appears become drastic large number observations
(over half possible observations) allowed. Furthermore, large number observations,
optimal subset subset selected greedy heuristic almost identical.

574

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

6.2 CpG-Island Detection
studied bioinformatics problem finding CpG islands DNA sequences. CpG islands
regions genome high concentration cytosine-guanine sequence. areas
believed mainly located around promoters genes, frequently expressed
cell. experiment, considered gene loci HS381K22, AF047825 AL133174,
GenBank annotation listed three, two one CpG islands each. ran algorithm
50 base window beginning end island, using transition emission
probabilities Durbin, Eddy, Krogh, Mitchison (1999) Hidden Markov Model,
used sum margins reward function.
goal experiment locate beginning ending CpG islands
precisely asking experts, whether certain bases belong CpG region not. Figure 3(b) shows mean classification accuracy mean margin scores increasing number
observations. results indicate that, although expected margin scores similar
optimal algorithm greedy heuristic, mean classification performance optimal algorithm still better performance greedy heuristic. example, making 6
observations, mean classification error obtained optimal algorithm 25% lower
error obtained greedy heuristic.
6.3 Part-of-Speech Tagging
third experiment, investigated structured classification task part-of-speech (POS)
tagging (CoNLL, 2003). Problem instances sequences words (sentences), word
part entity (e.g., European Union), entity belongs one five categories:
Location, Miscellaneous, Organization, Person Other. Imagine application, automatic
information extraction guided expert: algorithms compute optimal conditional plan
asking expert, trying optimize classification performance requiring little expert
interaction possible.
used conditional random field structured classification task, node corresponds word, joint distribution described node potentials edge potentials.
sum margins used reward function. Measure classification performance F1
score, geometric mean precision recall. goal experiment analyze
addition expert labels increases classification performance, indirect, decomposing reward function used algorithms corresponds real world classification performance.
Figure 3(c) shows increase mean expected margin F1 score increasing number observations, summarized ten 50 word sequences. seen classification
performance effectively enhanced optimally incorporating expert labels. Requesting
three 50 labels increased mean F1 score five percent. following
example illustrates effect: one scenario words entity, sportsman P. Simmons,
classified incorrectly P. Simmons Miscellaneous. first request
optimal conditional plan label Simmons. Upon labeling word correctly, word P.
automatically labeled correctly also, resulting F1 score 100 percent.

575

fiK RAUSE & G UESTRIN

7. Applying Chain Algorithms General Graphical Models
Section 4 seen algorithms used schedule single sensor, assuming time
series sensor readings (e.g., temperature) form Markov chain. natural assumption
sensor networks (Deshpande et al., 2004). deploying sensor networks however, multiple
sensors need scheduled. time series sensors independent, could use
algorithms schedule sensors independently other. However, practice,
measurements correlated across different sensors fact, dependence essential
allow generalization measurements locations sensor placed. following, describe approach using single-sensor scheduling algorithm coordinate
multiple sensors.
formally, interested monitoring spatiotemporal phenomenon set locations = {1, . . . , m}, time steps = {1, . . . , }. locationtime pair s, t,
associate random variable Xs,t describes state phenomenon location
time. random vector XS,T fully describes relevant state world vector XS,t
describes state particular time step t. before, make Markov assumption, assuming
conditional independence XS,t XS,t0 given XS,t1 t0 < 1.
Similarly single-chain case, consider reward functions Rs,t associated
variable Xs,t . goal select, timestep, set sensors activate,
order maximize sum expected rewards. Letting A1:t = A1 , expected total
reward given
X
Rs,t (Xs,t | XA1:t )
s,t

filtering setting (i.e., observations past taken account evaluating
rewards),
X
Rs,t (Xs,t | XA1:T )
s,t

smoothing setting (where observations taken account). generalization
conditional planning done described Section 2.
Note case single sensor (` = 1), problem optimal sensor scheduling
solved using Algorithm 1. Unfortunately, optimization problem wildly intractable even
case two sensors, ` = 2:
Corollary 7 (Hardness sensor selection two chains). Given model two dependent
chains, constants c B, cost function set decision theoretic value information
functions Rs,t , NPPP -complete determine whether subset A1:T variables
L(A1:T ) c (A1:T ) B.
following, develop approximate algorithm uses optimal single-chain algorithms performs well practice.
7.1 Approximate Sensor Scheduling Lower Bound Maximization
reason sudden increase complexity case multiple chains decomposition rewards along sub-chains (as described Section 3) extend case multiple

576

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

S(1)1

S(1)2

S(1)3

S(1)4

S(1)5

S(2)1

S(2)2

S(2)3

S(2)4

S(2)5

Figure 4: Scheduling multiple correlated sensors dynamic processes.
sensors, since influence flow across chains. Figure 4 visualizes problem there, distri(1)
(1)
(2)
bution sensor (2) depends three observations S1 S4 sensor (1) S2
sensor (2).
address complexity issue using (approximate) extension decomposition approach used single chains. focus decision-theoretic value information objective (as described Section 2.1), local reward functions, residual entropy,
used well.
Considering recent observations. first approximation, allow sensor take
account recent observations. Intuitively, appears reasonable approximation,
especially potential scheduling times reasonably far apart. Formally, evaluating
local rewards time t, replace set observations time t, A1:t subset
A01:t A1:t


A01:t = (s, t) A1:t : t0 (s, t0 ) A1:t ,
i.e, sensor s, last observation (with largest time index t) kept.
approximate Rs,t (Xs,t | A1:t ) Rs,t (Xs,t | A01:t ). Figure 4 example, A1:5 =
{(s1 , 1), (s2 , 2), (s1 , 4)}, total expected utility time t5 would computed using observations A01:5 = {(s2 , 2), (s1 , 4)}, i.e., using time t4 sensor one, time t2 sensor two,
(1)
ignoring influence originating observation S1 flowing chains indicated
dashed arrow. following proposition proves approximation lower bound
true value information:
Proposition 8 (Monotonicity value information). decision-theoretic value information Rs,t (A) set sensors monotonic A,
Rs,t (A0 ) Rs,t (A)
A0 A.
Proposition 8 proves conditioning recent observations decrease
objective function, hence maximizing approximate objective implies maximizing lower bound
true objective.
coordinate ascent approach. propose following heuristic maximizing lower
bound expected utility. Instead jointly optimizing schedules (timesteps selected
sensor), algorithm repeatedly iterate sensors. sensors s,
optimize selected observations As1:T , holding schedules sensors fixed.
577

fiK RAUSE & G UESTRIN

procedure resembles coordinate ascent approach, coordinate ranges possible
schedules fixed sensor s.
optimizing sensor s, algorithm finds schedule As1:T


[
X
As1:T = argmax
Rs,t Xs,t | XA01:t
XA0s0 (As1:T ) B,
(6)
A1:T

s0 6=s

s,t

1:t

i.e., maximizes, schedules A1:T , sum expected rewards time steps
0
sensors, given schedules As1:T non-selected sensors s0 .
Solving single-chain optimization problem. order solve maximization problem
(6) individual sensors, use dynamic programming approach introduced
lt
Section 4. recursive case Lfa:b
(k) k > 0 exactly same. However, base case
computed
b1 X


X
[
f lt
La:b (0) =
Rs,j Xs,j | Xa
XA0s0 ,
s0 6=s

j=a+1

1:j

i.e., takes account recent observation non-selected sensors s0 .
lt
(0). First all,
Several remarks need made computation base case Lfa:b
naive implementation, computation expected utility


[
Rs,j Xs,j | Xa
XA0s0
s0 6=s

1:j

requires time exponential number chains. case since, order compute
reward Rs,t , chain, possible observations XA0s
= xA0s
could made need
1:t
1:t
taken account. computation requires computing expectation joint distribution
P (XA01:t ), exponential size. increase complexity avoided using sampling
approximation: Hoeffdings inequality used derive polynomial bounds sample complexity approximating value information arbitrarily small additive error , similarly
done approach Krause Guestrin (2005a)4 . practice, small number samples
appears provide reasonable performance. Secondly, inference becomes intractable
increasing number sensors. Approximate inference algorithms algorithm proposed
Boyen Koller (1998) provide viable way around problem.
Analysis. Since sensors maximize global objective L(A1:T ), coordinated ascent
approach guaranteed monotonically increase global objective every iteration (ignoring
possible errors due sampling approximate inference). Hence must converge (to local
optimum) finite number steps. procedure formalized Algorithm 4.
Although cannot general provide performance guarantees procedure, building algorithm provides optimal schedule sensor isolation,
benefit observations provided remaining sensors. Also, note sensors
independent, Algorithm 4 obtain optimal solution. Even sensors correlated,
obtained solution least good solution obtained scheduling sensors independently other. Algorithm 4 always converge, always compute lower bound
4. absolute error evaluating reward Rs,t accumulate total error |T ||S|
variables hence error optimal schedule.

578

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

Input: Budget B
Output: Selection A1 , . . . , A` observation times sensor
begin
Select Ai , 1 ` random;
repeat
= 1 `
Use Algorithm 1 select observations Ai sensor i, conditioning current
sensor scheduling Aj , j 6= i, remaining sensors;
end
Compute improvement total expected utility;
small enough ;
end
Algorithm 4: Multi-Sensor scheduling.
expected total utility. Considering intractability general problem even two chains
(c.f., , Corollary 7), properties reassuring. experiments, coordinated sensor
scheduling performed well, discussed Section 7.2.
7.2 Proof Concept Study Real Deployment
work Singhvi et al. (2005), presented approach optimizing light control
buildings, purpose satisfying building occupants preferences lighting conditions,
simultaneously minimizing energy consumption. approach, wireless sensor network
deployed monitors building environmental conditions (such sunlight intensity
etc.). sensors feed measurements building controller actuates lighting system
(lamps, blinds, etc.) accordingly. every timestep , building controller choose
action affects lighting conditions locations building. Utility functions
Ut (a, xS,t ) specified map chosen actions current lighting levels utility
value. utility chosen capture users preferences light levels, well
energy consumption lighting system. Details utility functions described detail
Singhvi et al..
evaluated multi-sensor scheduling approach real building controller testbed,
described detail Singhvi et al.. experiments, used Algorithm 4 schedule three
sensors, allowing sensor choose subset ten time steps (in one-hour intervals
daytime). varied number timesteps sensor activated, computed
total energy consumption total user utility (as defined Singhvi et al.). Figure 5(a) shows
mean user utility energy savings achieved, number observations varying
observations continuous sensing (10 observations discretization)5 . results imply
using predictive model active sensing strategy, even small number observations
achieves results approximately good results achieved continuous sensing.
Figure 5(b) presents mean total utility achieved using observations, one observation ten
observations per sensor day. seen even single observation per sensor increases
total utility close level achieved continuous sensing. Figure 5(c) shows mean energy
5. Note Figure 5(a), energy cost utility plotted different units directly compared.

579

fiK RAUSE & G UESTRIN

6

12
Energy cost

10

1 Observ./
sensor

15

10 Observ./
sensor
Energy cost

8

Total utility

User utility energy cost

14

4
observ.

2

8

observ.

10

5

0

1 Observ./
sensor

Measured user utility
6

0

1

2 3
Number observations

10

2

(a) Sensing scheduling evaluation

10

12
14
Hour day

16

18

0

(b) Total utility

10

10 Observ./
sensor

12
14
Hour day

16

18

(c) Energy cost

Figure 5: Active sensing results.
consumption required experiment. Here, single sensor observation strategy comes
even closer power savings achieved continuous sensing.
Since sensor network battery lifetime general inversely proportional amount
power expended sensing communication, conclude sensor scheduling strategy
promises lead drastic increases sensor network lifetime, deployment permanence reduced maintenance cost. testbed, network lifetime could increased factor 3
without significant reduction user utility increase energy cost.

8. Related Work
section, review related work number different areas.
8.1 Optimal Experimental Design
Optimal experimental design general methodology selecting informative experiments infer
aspects state world (such parameters particular nonlinear function,
etc.). large literature different approaches experimental design (c.f., Chaloner &
Verdinelli, 1995; Krause, Singh, & Guestrin, 2007).
Bayesian experimental design, prior distribution possible states world assumed, experiments chosen, e.g., reduce uncertainty posterior distribution.
general form, Bayesian experimental design pioneered Lindley (1956). users encode
preferences utility function U (P (), ? ), first argument, P (), distribution
states world (i.e., parameters) second argument, ? , true state
world. Observations xA collected, change expected utility prior P ()
posterior P ( | XA = xA ) used design criterion. sense, value observation problems considered paper considered instances Bayesian experimental design
problems. Typically, Bayesian Experimental Design employed continuous distributions, often
multivariate normal distribution. choosing different utility functions, different notions
optimality defined, including A- D- optimality developed (Chaloner & Verdinelli,
1995). posterior covariance matrix |A , whose
maximum

eigenvalue max ,
Bayesian A-, D-, E- optimality minimizes tr |A , det |A , max |A , respectively. terminology Section 2.1, D-optimality corresponds choosing total entropy,
A-optimality corresponds (weighted) mean-squared error criteria.

580

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

Even multivariate normal distributions, optimal Bayesian Experimental design NP-hard
(Ko, Lee, & Queyranne, 1995). applications experimental design, number experiments selected often large compared number design choices. cases, one
find fractional design (i.e., non-integral solution defining proportions experiments
performed), round fractional solutions. fractional formulation, A-, D-,
E-optimality criteria solved exactly using semi-definite program (Boyd & Vandenberghe,
2004). however known bounds integrality gap, i.e., loss incurred
rounding process.
algorithms presented Section 4.1 used optimally solve non-fractional Bayesian
Experimental Design problems chain graphical models, even continuous distributions,
long inference distributions tractable (such normal distributions). paper hence
provides new class combinatorial algorithms interesting class Bayesian experimental
design problems.
8.2 Value Information Graphical Models
Decision-theoretic value information frequently used principled information gathering (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), popularized decision
analysis context influence diagrams (Howard & Matheson, 1984). sense, value
information problems special cases Bayesian experimental design problems, prior
distribution particular structure, typically given graphical model considered
paper.
Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen,
1997; Kapoor et al., 2007) suggested myopic, i.e., greedy approaches selectively gathering
evidence graphical models, considered paper, which, unlike algorithms presented
paper. algorithms applicable much general graphical models,
theoretical guarantees. Heckerman et al. (1993) propose method compute
maximum expected utility specific sets observations. work considers general
graphical models paper (Naive Bayes models certain extensions), provide
large sample guarantees evaluation given sequence observations, use heuristic
without guarantees select sequences. Bilgic Getoor (2007) present branch bound
approach towards exactly optimizing value information complex probabilistic models.
contrast algorithms described paper however, approach running time
worst-case exponential. Munie Shoham (2008) present algorithms hardness results
optimizing special class value information objective functions motivated optimal
educational testing problems. algorithms apply different class graphical models
chains, apply specific objective functions, rather general local reward functions
considered paper. Radovilsky, Shattah, Shimony (2006) extended previous version
paper (Krause & Guestrin, 2005a) obtain approximation algorithms guarantees
case noisy observations (i.e., selecting subset emission variables observe, rather
selecting among hidden variables considered paper).
8.3 Bandit Problems Exploration / Exploitation
important class sequential value information problems class Bandit problems.
classical k-armed bandit problem, formalized Robbins (1952), slot machine given
581

fiK RAUSE & G UESTRIN

k arms. draw arm results reward success probability pi fixed
arm, different (and independent) across arm. selecting arms pull, important
problem trade exploration (i.e., estimation success probabilities arms)
exploitation (i.e., repeatedly pulling best arm known far). celebrated result Gittins
Jones (1979) shows fixed number draws, optimal strategy computed
polynomial time, using dynamic programming based algorithm. similar sense
optimal sequential strategy computed polynomial time, Gittins algorithm however
different structure dynamic programming algorithms presented paper.
Note using function optimization objective function described Section 2.1,
approach used solve particular instance bandit problems, arms
required independent, but, contrary classical notion bandit problems,
chosen repeatedly.
8.4 Probabilistic Planning
Optimized information gathering also extensively studied planning community.
Bayer-Zubek (2004) example proposed heuristic method based Markov Decision Process framework. However, approach makes approximations without theoretical guarantees.
problem optimizing decision theoretic value information naturally formalized
(finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik,
1973). Hence, principle, algorithms planning POMDPs, anytime algorithm
Pineau, Gordon, Thrun (2006), employed optimizing value information. Unfortunately, state space grows exponentially number variables considered
selection problem. addition, complexity planning POMDPs grows exponentially
cardinality state space, hence doubly-exponentially number variables selection. steep increase complexity makes application black-box POMDP solvers infeasible.
Recently, Ji, Parr, Carin (2007) demonstrated use POMDP planning multi-sensor
scheduling problem. presenting promising empirical results, approach however uses
approximate POMDP planning techniques without theoretical guarantees.
robotics literature, Stachniss, Grisetti, Burgard (2005), Sim Roy (2005)
Kollar Roy (2008) presented approaches information gathering context Simultaneous Localization Mapping (SLAM). None approaches however provide guarantees
quality obtained solutions. Singh, Krause, Guestrin, Kaiser, Batalin (2007)
present approximation algorithm theoretical guarantees problem planning informative path environmental monitoring using Gaussian Process models. contrast
algorithms presented paper, dealing complex probabilistic models
complex cost functions arising path planning, approach requires submodular objective
functions (a property hold value information show Proposition 9).
8.5 Sensor Selection Scheduling
context wireless sensor networks, sensor nodes limited battery hence
enable small number measurements, optimizing value information selected
sensors plays key role. problem deciding selectively turn sensors order
conserve power first discussed Slijepcevic Potkonjak (2001) Zhao, Shin, Reich
(2002). Typically, assumed sensors associated fixed sensing region, spatial
582

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

domain needs covered regions associated selected sensors. Abrams, Goel,
Plotkin (2004) present efficient approximation algorithm theoretical guarantees
problem. Deshpande, Khuller, Malekian, Toossi (2008) present approach problem
based semidefinite programming (SDP), handling general constraints providing tighter
approximations. approaches described apply problem optimizing sensor schedules complex utility functions as, e.g., increase prediction accuracy
objectives considered paper. address shortcomings, Koushanfary, Taft,
Potkonjak (2006) developed approach sensor scheduling guarantees specified prediction accuracy based regression model. However, approach relies solution
Mixed Integer Program, intractable general. Zhao et al. (2002) proposed heuristics
selectively querying nodes sensor network order reduce entropy prediction. Unlike algorithms presented paper, approaches performance
guarantees.
8.6 Relationship Machine Learning
Decision Trees (Quinlan, 1986) popularized value information criterion creating
conditional plans. Unfortunately, guarantees performance greedy method.
subset selection problem instance feature selection central issue machine
learning, vast amount literature (see Molina, Belanche, & Nebot, 2002 survey).
However, aware work providing similarly strong performance guarantees
algorithms considered paper.
problem choosing observations also strong connection field active learning
(c.f., Cohn, Gharamani, & Jordan, 1996; Tong & Koller, 2001) learning system designs
experiments based observations. sample complexity bounds derived
active learning problems (c.f., Dasgupta, 2005; Balcan, Beygelzimer, & Langford, 2006),
aware active learning algorithms perform provably optimal (even restricted
classes problem instances).
8.7 Previous Work Authors
previous version paper appeared work Krause Guestrin (2005b).
contents Section 7 appeared part work Singhvi et al. (2005). present version
much extended, new algorithmic hardness results detailed discussions.
light negative results presented Section 5, cannot expect able optimize value information complex models chains. However, instead attempting
solve optimal solution, one might wonder whether possible obtain good approximations. authors showed (Krause & Guestrin, 2005a; Krause et al., 2007; Krause, Leskovec,
Guestrin, VanBriesen, & Faloutsos, 2008) large number practical objective functions satisfy intuitive diminishing returns property: Adding new observation helps
observations far, less already made many observations. intuition formalized using combinatorial concept called submodularity. fundamental result Nemhauser
et al. proves optimizing submodular utility function, myopic greedy algorithm
fact provides near-optimal solution, within constant factor (11/e) 63% optimal.
Unfortunately, decision theoretic value information satisfy submodularity.

583

fiK RAUSE & G UESTRIN

Proposition 9 (Non-submodularity value information). Decision-theoretic value information submodular, even Naive Bayes models.
Intuitively, value information non-submodular, need make several observations
order convince need change action.

9. Conclusions
described novel efficient algorithms optimal subset selection conditional plan computation chain graphical models (and trees leaves), including HMMs. empirical
evaluation indicates algorithms improve upon commonly used heuristics decreasing expected uncertainty. algorithms also effectively enhance performance interactive
structured classification tasks.
Unfortunately, optimization problems become wildly intractable even slight generalization chains. presented surprising theoretical limits, indicate even class
decision theoretic value information functions (as widely used, e.g., influence diagrams
POMDPs) cannot efficiently computed even Naive Bayes models. also identified optimization value information new class problems intractable (NPPP -complete)
polytrees.
hardness results, along recent results polytree graphical models, NPcompleteness maximum posteriori assignment (Park & Darwiche, 2004) NP-hardness
inference conditional linear Gaussian models (Lerner & Parr, 2001), suggest possibility
developing generalized complexity characterization problems hard polytree graphical
models.
light theoretical limits computing optimal solutions, natural question ask
whether approximation algorithms non-trivial performance guarantees found. Recent
results Krause Guestrin (2005a), Radovilsky et al. (2006) Krause et al. (2007) show
case interesting classes value information problems.

Acknowledgments
would like thank Ben Taskar providing part-of-speech tagging model, Reuters
making news archive available. would also like thank Brigham Anderson Andrew Moore helpful comments discussions. work partially supported NSF
Grants No. CNS-0509383, CNS-0625518, ARO MURI W911NF0710287 gift Intel.
Carlos Guestrin partly supported Alfred P. Sloan Fellowship, IBM Faculty Fellowship ONR Young Investigator Award N00014-08-1-0752 (2008-2011). Andreas Krause
partially supported Microsoft Research Graduate Fellowship.

Appendix
Proof Theorem 3. Membership #P arbitrary discrete polytrees straightforward since
inference models P. Let instance #3SAT , count
number assignments X1 , . . . , Xn satisfying . Let C = {C1 , . . . , Cm } set
clauses. create Bayesian network 2n + 1 variables, X1 , . . . , Xn , U1 , . . . , Un Y,
Xi conditionally independent given Y. Let uniformly distributed values
584

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS


U1

U2

Un


X1

X2

Xn

Figure 6: Graphical model used proof Theorem 3.
{n, (n 1), . . . , 1, 1, . . . , 1, m}, Ui Bernoulli prior p = 0.5. Let
observed variables Xi CPTs defined following way:

1, Xi = u satisfies clause Cj ;
Xi | [Y = +j, Ui = u]
0, otherwise.

0, = j;
Xi | [Y = j, Ui = u]
u, otherwise.
model, presented Figure 6, holds X1 = X2 = = Xn = 1 iff U1 , . . . , Un
encode satisfying assignment , > 0. Hence, observe X1 = X2 = = Xn = 1,
know > 0 certainty. Furthermore, least one Xi = 0, know
P (Y > 0 | X = x) < 1. Let nodes zero reward, except Y, assigned
reward function following properties (we show model local
reward function using decision-theoretic value information):
(n+m)2n
, P (Y > 0 | XA = xA ) = 1;

R(Y | XA = xA ) =
0,
otherwise.
argument, expected reward
X
R(Y | X1 , . . . , Xn ) =
P (Y = y)P (U = u)P (x| u)R(Y | X = x)
u,y,x

=

X

P (Y > 0)P (u)

u sat

X
(n + m)2n
=
1

u sat

exactly number satisfying assignments . Note model defined yet
Naive Bayes model. However, easily turned one marginalizing U.
show realize reward function properties maximum expected utility sense. Let = {d1 , d2 } set two decisions. Define utility function
property:

(n+m)2n

,
= d1 > 0;


(n+m)22n+1
u(y, d) =
, = d1 < 0;

0, n
otherwise.
reward R(Y | XA ) given decision-theoretic value information:
X
X
R(Y | XA ) =
P (xA ) max
P (y | xA )u(y, d).
xA



585



fiK RAUSE & G UESTRIN

Figure 7: Graphical model used proof Theorem 5.
utility function u based following consideration. Upon observing particular instantiation variables X1 , . . . , Xn make decision variable Y. goal achieve
number times action d1 chosen exactly corresponds number satisfying assignments . accomplished following way. Xi 1, know Ui
encoded satisfying assignment, > 0 probability 1. case, action d1 chosen.
need make sure whenever least one Xi = 0 (which indicates either < 0
U satisfying assignment) decision d2 chosen. Now, least one Xi = 0, either
= j > 0 clause j satisfied, < 0. utilities designed unless
n
P (Y > 0 | XA = xA ) 1 n22m , action d2 gives higher expected reward 0. Hereby,
n2n
2m lower bound probability misclassification P (Y < 0 | XA = xA ).
Note construction immediately proves hardness approximation: Suppose
polynomial time algorithm computes approximation R within
factor > 1 (which depend problem instance) R = R(Y | X1 , . . . , Xn ). R > 0
implies R > 0, R = 0 implies R = 0. Hence, approximation R used
decide whether satisfiable not, implying P = NP.
Proof Corollary 4. Let 3CNF formula. convert Naive Bayes model variables X1 , . . . , Xn construction Theorem 3. function L(V)
V = {1, . . . , n} set variables Xi counts number satisfying assignments .
Note function L(A) V = {1, . . . , n} monotonic, i.e., L(A) L(V)
V, shown Proposition 8. Hence majority assignments satisfies
L(V) > 2n1 .
Proof Theorem 5. Membership follows fact inference polytrees P discrete polytrees: nondeterministic Turing machine #P oracle first guess selection
variables, compute value information using Theorem 3 (since computation
#P-complete arbitrary discrete polytrees), compare constant c.
show hardness, let instance EM AJSAT , find instantiation
X1 , . . . , Xn (X1 , . . . , X2n ) true majority assignments Xn+1 , . . . , X2n .
Let C = {C1 , . . . , Cm } set 3CNF clauses. Create Bayesian network shown Figure 7,
nodes Ui , uniform Bernoulli prior. Add bivariate variables Yi = (seli , pari ),
0 2n, seli takes values {0, . . . , m} pari parity bit. CPTs Yi

586

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

defined as: sel0 uniformly varies {1, . . . , m}, par0 = 0, Y1 , . . . , Y2n :

0, j = 0, ui satisfies Cj ;
seli | [seli1 = j, Ui = ui ]
j, otherwise;
pari | [pari1 = bi1 , Ui ] bi1 Ui ,
denotes parity (XOR) operator. add variables ZiT ZiF 1 n
let

Uniform({0, 1}), ui = 1;

Zi | [Ui = ui ]
0,
otherwise;
Uniform denotes uniform distribution. Similarly, let

Uniform({0, 1}), ui = 0;
ZiF | [Ui = ui ]
0,
otherwise.
Intuitively, ZiT = 1 guarantees us Ui = 1, whereas ZiT = 0 leaves us uncertain Ui .
case ZiF symmetric.
use subset selection algorithm choose Zi encode solution EM AJSAT .
ZiT chosen, indicate Xi set true, similarly ZiF indicates false assignment
Xi . parity function going used ensure exactly one {ZiT , ZiF } observed
i.
first assign penalties nodes except ZiT , ZiF 1 n, Uj
n + 1 j 2n, assigned zero penalty. Let nodes zero reward, except
Y2n , assigned following reward:
n
4 , P (sel2n = 0 | XA = xA ) = 1
[P (par2n = 1 | XA = xA ) = 1 P (par2n = 0 | XA = xA ) = 1];
R(Y2n | XA = xA ) =

0,
otherwise.
Note sel2n = 0 probability 1 iff U1 , . . . , U2n encode satisfying assignment . Furthermore, get positive reward certain sel2n = 0, i.e., chosen observation
set must contain proof satisfied, certain par2n . parity certainty
occur certain assignment U1 , . . . , U2n . possible infer
value Ui certainty observing one Ui , ZiT ZiF . Since, = 1, . . . , n, cost
observing Ui , receive reward must observe least one ZiT ZiF . Assume
compute optimal subset budget 2n, receive positive reward
observing exactly one ZiT ZiF .
interpret selection ZiT ZiF assignment first n variables EM AJSAT .
Let R = R(Y2n | O). claim EM AJSAT R > 0.5. First let
EM AJSAT , assignment x1 , . . . , xn first n variables. add Un+1 , . . . , U2n
add ZiT iff xi = 1 ZiF iff xi = 0. selection guarantees R > 0.5.
assume R > 0.5. call assignment U1 , . . . , U2n consistent 1 n,
ZiT O, Ui = 1 ZiF Ui = 0. consistent assignment, chance
observations Zi prove consistency 2n . Hence R > 0.5 implies majority
provably consistent assignments satisfy hence EM AJSAT . proves subset
selection NPPP complete.
Note realize local reward function R sense maximum expected utility
similarly described Proof Theorem 3.
587

fiK RAUSE & G UESTRIN

Proof Corollary 6. constructions proof Theorem 4 Theorem 5 also prove
computing conditional plans PP-hard NPPP -hard respectively, since, instances,
plan positive reward must observe variables corresponding valid instantiations (i.e.,
X1 , . . . , Xn Corollary 4, Un+1 , . . . , U2n one Z1 , . . . , Zn satisfy
parity condition Theorem 5). cases, order selection irrelevant, and, hence,
conditional plan effectively performs subset selection.
Proof Corollary 7. proof follows observation polytree construction
proof Theorem 5 arranged two dependent chains. transformation, revert
arc ZiT Ui applying Bayes rule. make sure number
nodes sensor timeslice, triple variables Yi , calling copies Yi0 Yi00 .
conditional probability tables given equality constraints, Yi0 = Yi Yi00 = Yi0 .
transformation, variables associated timesteps 3i 2 (for 1) given sets
00 , Z }. timesteps 3i 1 associated sets {U , }, timesteps 3i associated
{Yi1



{ZiF , Yi0 }.
Proof Proposition 8. bound follows fact maximization convex,
application Jensens inequality. Using induction argument, simply need show
L(A) L().
!
X
X
L(A) =
P (XA = xA )
max EU (a, t, x | XA1:t = xA1:t )
xA



tV

!


X
tV

=

X
tV

max


X

P (XA = xA )EU (a, t, x | XA1:t = xA1:t )

xA

max EU (a, t, x) = L()



EU (a, t, x | XA1:t = xA1:t ) =

X

P (xt | XA1:t = xA1:t )Ut (a, xt )

xt

expected utility action time observing XA1:t = xA1:t .
Proof Proposition 9. Consider following binary classification problem assymetric cost.
one Bernoulli random variable (the class label) P (Y = 1) = 0.5
P (Y = 1) = 0.5. also two noisy observations X1 , X2 , conditionally independent given Y. Let P (Xi = Y) = 3/4 (i.e., observations agree class label
probability 3/4, disagree probability 1/4. three actions, a1 (classifying 1),
a1 (classifying -1) a0 (not assigning label). define utility functon U
gain utility 1 assign label correctly (U (a1 , 1) = U (a1 , 1) = 1), 3 misassign
label (U (a1 , 1) = U (a1 , 1) = 3), 0 choose a0 , i.e., assign label. Now,
2
2
6
> 0.
verify L() = L({X1 }) = L({X2 }) = 0, L({X1 , X2 }) = 43 3 14 = 16
Hence, adding X2 X1 increases utility adding X2 empty set, contradicting
submodularity.

588

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

References
Abrams, Z., Goel, A., & Plotkin, S. (2004). Set k-cover algorithms energy efficient monitoring
wireless sensor networks.. IPSN.
Balcan, N., Beygelzimer, A., & Langford, J. (2006). Agnostic active learning. ICML.
Baum, L. E., & Petrie, T. (1966). Statistical inference probabilistic functions finite state
Markov chains. Ann. Math. Stat, 37, 15541563.
Bayer-Zubek, V. (2004). Learning diagnostic policies examples systematic search. UAI.
Bellman, R. (1957). Markovian decision process. Journal Mathematics Mechanics, 6.
Bilgic, M., & Getoor, L. (2007). Voila: Efficient feature-value acquisition classification.
Twenty-Second Conference Artificial Intelligence (AAAI).
Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge UP.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes. Uncertainty Artificial Intelligence (UAI).
Chaloner, K., & Verdinelli, I. (1995). Bayesian experimental design: review. Statistical Science,
10(3), 273304.
Cohn, D. A., Gharamani, Z., & Jordan, M. I. (1996). Active learning statistical models. J AI
Research, 4, 129145.
CoNLL (2003).
Conference computational natural language learning shared task.
http://cnts.uia.ac.be/conll2003/ner/.
Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. Wiley Interscience.
Dasgupta, S. (2005). Coarse sample complexity bounds active learning. NIPS.
Deshpande, A., Guestrin, C., Madden, S., Hellerstein, J., & Hong, W. (2004). Model-driven data
acquisition sensor networks. VLDB.
Deshpande, A., Khuller, S., Malekian, A., & Toossi, M. (2008). Energy efficient monitoring
sensor networks. LATIN.
Dittmer, S., & Jensen, F. (1997). Myopic value information influence diagrams. UAI, pp.
142149, San Francisco.
Domingos, P., & Pazzani, M. (1997). optimality simple Bayesian classifier
zero-one loss. Machine Learning, 29, 103137.
Durbin, R., Eddy, S. R., Krogh, A., & Mitchison, G. (1999). Biological Sequence Analysis : Probabilistic Models Proteins Nucleic Acids. Cambridge University Press.
Gittins, J. C., & Jones, D. M. (1979). dynamic allocation index discounted multiarmed
bandit problem. Biometrika, 66(3), 561565.
Goldberg, L. A., & Jerrum, M. (2000). Counting unlabelled subtrees tree #p-complete. LMS
J Comput. Math., 3, 117124.
Heckerman, D., Horvitz, E., & Middleton, B. (1993). approximate nonmyopic computation
value information. IEEE Trans. Pattern Analysis Machine Intelligence, 15, 292298.

589

fiK RAUSE & G UESTRIN

Howard, R. A. (1966). Information value theory. IEEE Transactions Systems Science
Cybernetics (SSC-2).
Howard, R. A., & Matheson, J. (1984). Readings Principles Applications Decision
Analysis II, chap. Influence Diagrams, pp. 719762. Strategic Decision Group, Menlo Park.
Reprinted 2005 Decision Analysis 2(3) 127-143.
Ji, S., Parr, R., & Carin, L. (2007). Non-myopic multi-aspect sensing partially observable
Markov decision processes. IEEE Transactions Signal Processing, 55(6), 27202730.
Kapoor, A., Horvitz, E., & Basu, S. (2007). Selective supervision: Guiding supervised learning
decision-theoretic active learning. International Joint Conference Artificial Intelligence
(IJCAI).
Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences Value
Trade-offs. Wiley.
Ko, C., Lee, J., & Queyranne, M. (1995). exact algorithm maximum entropy sampling.
Operations Research, 43(4), 684691.
Kollar, T., & Roy, N. (2008). Efficient optimization information-theoretic exploration slam.
AAAI.
Koushanfary, F., Taft, N., & Potkonjak, M. (2006). Sleeping coordination comprehensive sensing
using isotonic regression domatic partitions. Infocom.
Krause, A., & Guestrin, C. (2005a). Near-optimal nonmyopic value information graphical
models. Proc. Uncertainty Artificial Intelligence (UAI).
Krause, A., & Guestrin, C. (2005b). Optimal nonmyopic value information graphical models
- efficient algorithms theoretical limits. Proc. IJCAI.
Krause, A., Leskovec, J., Guestrin, C., VanBriesen, J., & Faloutsos, C. (2008). Efficient sensor
placement optimization securing large water distribution networks. Journal Water Resources Planning Management, 136(6).
Krause, A., Singh, A., & Guestrin, C. (2007). Near-optimal sensor placements Gaussian processes: Theory, efficient algorithms empirical studies. JMLR.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic models
segmenting labeling sequence data. ICML.
Lerner, U., & Parr, R. (2001). Inference hybrid networks: Theoretical limits practical algorithms. UAI.
Lindley, D. V. (1956). measure information provided experiment. Annals
Mathematical Statistics, 27, 9861005.
Littman, M., Goldsmith, J., & Mundhenk, M. (1998). computational complexity probabilistic
planning. Journal Artificial Intelligence Research, 9, 136.
Molina, L., Belanche, L., & Nebot, A. (2002). Feature selection algorithms: survey experimental evaluation. ICDM.
Mookerjee, V. S., & Mannino, M. V. (1997). Sequential decision models expert system optimization. IEEE Trans. Knowl. Data Eng., 9(5), 675687.

590

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

Munie, M., & Shoham, Y. (2008). Optimal testing structured knowledge. Twenty-Third Conference Artificial Intelligence (AAAI).
Papadimitriou, C. H. (1995). Computational Complexity. Addison-Wesley.
Park, J. D., & Darwiche, A. (2004). Complexity results approximation strategies map
explanations. Journal Aritificial Intelligence Research, 21, 101133.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations large pomdps.
JAIR, 27, 335380.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81106.
Radovilsky, Y., Shattah, G., & Shimony, S. E. (2006). Efficient deterministic approximation algorithms non-myopic value information graphical models. IEEE International
Conference Systems, Man Cybernetics (SMC), Vol. 3, pp. 25592564.
Robbins, H. (1952). aspects sequential design experiments. Bulletin American
Mathematical Society, 58, 527535.
Scheffer, T., Decomain, C., & Wrobel, S. (2001). Active learning partially hidden Markov models
information extraction. ECML/PKDD Workshop Instance Selection.
Sim, R., & Roy, N. (2005). Global a-optimal robot exploration slam. IEEE International
Conference Robotics Automation (ICRA).
Singh, A., Krause, A., Guestrin, C., Kaiser, W. J., & Batalin, M. A. (2007). Efficient planning
informative paths multiple robots. International Joint Conference Artificial Intelligence (IJCAI), pp. 22042211, Hyderabad, India.
Singhvi, V., Krause, A., Guestrin, C., Garrett, J., & Matthews, H. (2005). Intelligent light control
using sensor networks. Proc. 3rd ACM Conference Embedded Networked Sensor
Systems (SenSys).
Slijepcevic, S., & Potkonjak, M. (2001). Power efficient organization wireless sensor networks.
ICC.
Smallwood, R., & Sondik, E. (1973). optimal control partially observable Markov decision
processes finite horizon. Operations Research, 21, 10711088.
Stachniss, C., Grisetti, G., & Burgard, W. (2005). Information gain-based exploration using raoblackwellized particle filters. Robotics Science Systems (RSS).
Tong, S., & Koller, D. (2001). Active learning parameter estimation Bayesian networks.
NIPS.
Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation hybrid genetic decision
tree induction algorithm. Journal Artificial Intelligence Research, 2, 369409.
van der Gaag, L., & Wessels, M. (1993). Selective evidence gathering diagnostic belief networks.
AISB Quart., 86, 2334.
Zhao, F., Shin, J., & Reich, J. (2002). Information-driven dynamic sensor collaboration tracking
applications. IEEE Signal Processing, 19(2), 6172.

591

fiJournal Artificial Intelligence Research 35 (2009) 1-47

Submitted 01/09; published 05/09

Complex Question Answering: Unsupervised Learning
Approaches Experiments
Yllias Chali

chali@cs.uleth.ca

University Lethbridge
Lethbridge, AB, Canada, T1K 3M4

Shafiq R. Joty

rjoty@cs.ubc.ca

University British Columbia
Vancouver, BC, Canada, V6T 1Z4

Sadid A. Hasan

hasan@cs.uleth.ca

University Lethbridge
Lethbridge, AB, Canada, T1K 3M4

Abstract
Complex questions require inferencing synthesizing information multiple
documents seen kind topic-oriented, informative multi-document summarization goal produce single text compressed version set
documents minimum loss relevant information. paper, experiment
one empirical method two unsupervised statistical machine learning techniques:
K-means Expectation Maximization (EM), computing relative importance
sentences. compare results approaches. experiments show
empirical approach outperforms two techniques EM performs better
K-means. However, performance approaches depends entirely feature
set used weighting features. order measure importance
relevance user query extract different kinds features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic shallow-semantic)
document sentences. use local search technique learn weights
features. best knowledge, study used tree kernel functions
encode syntactic/semantic information complex tasks computing
relatedness query sentences document sentences order generate
query-focused summaries (or answers complex questions). methods
generating summaries (i.e. empirical, K-means EM) show effects syntactic
shallow-semantic features bag-of-words (BOW) features.

1. Introduction
vast increase amount online text available demand access different types information led renewed interest broad range Information
Retrieval (IR) related areas go beyond simple document retrieval. areas
include question answering, topic detection tracking, summarization, multimedia retrieval, chemical biological informatics, text structuring, text mining, genomics, etc.
Automated Question Answering (QA)the ability machine answer questions, simple
complex, posed ordinary human languageis perhaps exciting technological development past six seven years (Strzalkowski & Harabagiu, 2008).
c
2009
AI Access Foundation. rights reserved.

fiChali, Joty, & Hasan

expectations already tremendous, reaching beyond discipline (a subfield Natural
Language Processing (NLP)) itself.
tool finding documents web, search engines proven adequate.
Although limitation expressiveness user terms query formulation, certain limitations exist search engine query. Complex
question answering tasks require multi-document summarization aggregated
search, faceted search, represents information need cannot answered
single document. example, look comparison average number
years marriage first birth women U.S., Asia, Europe, answer
likely contained multiple documents. Multi-document summarization useful
type query currently tool market designed meet
kind information need.
QA research attempts deal wide range question types including: fact, list,
definition, how, why, hypothetical, semantically-constrained, cross-lingual questions.
questions, call simple questions, easier answer. example,
question: president Bangladesh? asks persons name. type
question (i.e. factoid) requires small snippets text answer. Again, question:
countries Pope John Paul II visited? sample list question, asking
list small snippets text.
made substantial headway factoid list questions, researchers
turned attention complex information needs cannot answered
simply extracting named entities (persons, organizations, locations, dates, etc.) documents. Unlike informationally simple factoid questions, complex questions often seek multiple different types information simultaneously presuppose one single
answer meet information needs. example, factoid question like:
accurate HIV tests? safely assumed submitter question looking number range numbers. However, complex questions like:
causes AIDS? wider focus question suggests submitter
may single well-defined information need therefore may amenable
receiving additional supporting information relevant (as yet) undefined informational goal (Harabagiu, Lacatusu, & Hickl, 2006). questions require inferencing
synthesizing information multiple documents.
well known QA systems Korean Navers Knowledge search1 ,
pioneers community QA. tool allows users ask question get
answers users. Navers Knowledge roughly 10 times entries
Wikipedia. used millions Korean web users given day. people
say Koreans addicted internet Naver. January 2008 Knowledge Search database included 80 million pages user-generated information.
Another popular answer service Yahoo! Answers community-driven knowledge market website launched Yahoo!. allows users submit questions
answered answer questions users. People vote best answer. site
gives members chance earn points way encourage participation based
Naver model. December 2006, Yahoo! Answers 60 million users 65
1. http://kin.naver.com/

2

fiComplex Question Answering: Unsupervised Approaches

million answers. Google QA system2 based paid editors launched
April 2002 fully closed December 2006.
However, computational linguistics point view information synthesis
seen kind topic-oriented informative multi-document summarization. goal
produce single text compressed version set documents minimum loss
relevant information. Unlike indicative summaries (which help determine whether
document relevant particular topic), informative summaries must attempt find
answers.
paper, focus extractive approach summarization subset
sentences original documents chosen. contrasts abstractive summarization information text rephrased. Although summaries produced
humans typically extractive, state art summarization systems
based extraction achieve better results automated abstraction. Here,
experimented one empirical two well-known unsupervised statistical machine
learning techniques: K-means EM evaluated performance generating topicoriented summaries. However, performance approaches depends entirely
feature set used weighting features. order measure importance
relevance user query extract different kinds features (i.e. lexical, lexical
semantic, cosine similarity, basic element, tree kernel based syntactic shallow-semantic)
document sentences. used gradient descent local search technique
learn weights features.
Traditionally, information extraction techniques based BOW approach augmented language modeling. task requires use complex semantics, approaches based BOW often inadequate perform fine-level textual
analysis. improvements BOW given use dependency trees syntactic parse trees (Hirao, , Suzuki, Isozaki, & Maeda, 2004; Punyakanok, Roth, & Yih, 2004;
Zhang & Lee, 2003b), adequate dealing complex questions
whose answers expressed long articulated sentences even paragraphs. Shallow
semantic representations, bearing compact information, could prevent sparseness
deep structural approaches weakness BOW models (Moschitti, Quarteroni,
Basili, & Manandhar, 2007). pinpointing answer question relies deep understanding semantics both, attempting application syntactic semantic
information complex QA seems natural. best knowledge, study used
tree kernel functions encode syntactic/semantic information complex tasks
computing relatedness query sentences document sentences
order generate query-focused summaries (or answers complex questions).
methods generating summaries (i.e. empirical, K-means EM) show effects
syntactic shallow-semantic features BOW features.
past three years, complex questions focus much attention
automatic question-answering Multi Document Summarization (MDS) communities. Typically, current complex QA evaluations including 2004 AQUAINT
Relationship QA Pilot, 2005 Text Retrieval Conference (TREC) Relationship QA Task,
TREC definition (and others) require systems return unstructured lists can2. http://answers.google.com/

3

fiChali, Joty, & Hasan

didate answers response complex question. However recently, MDS evaluations (including 2005, 2006 2007 Document Understanding Conference (DUC)) tasked
systems returning paragraph-length answers complex questions responsive,
relevant, coherent.
experiments based DUC 2007 data show including syntactic semantic features improves performance. Comparison among approaches also shown.
Comparing DUC 2007 participants, systems achieve top scores
statistically significant difference results system results DUC
2007 best system.
paper organized follows: Section 2 focuses related work, Section 3
gives brief description intended final model, Section 4 describes features
extracted, Section 5 discusses learning issues presents learning approaches,
Section 6 discusses remove redundant sentences adding final
summary, Section 7 describes experimental study. conclude discuss future
directions Section 8.

2. Related Work
Researchers world working query-based summarization trying different
directions see methods provide best results.
number sentence retrieval systems based IR (Information Retrieval)
techniques. systems typically dont use lot linguistic information, still
deserve special attention. Murdock Croft (2005) propose translation model specifically
monolingual data, show significantly improves sentence retrieval query
likelihood. Translation models train parallel corpus used corpus question/answer pairs. Losada (2005) presents comparison multiple-Bernoulli models
multinomial models context sentence retrieval task shows multivariate Bernoulli model really outperform popular multinomial models retrieving
relevant sentences. Losada Fernandez (2007) propose novel sentence retrieval method
based extracting highly frequent terms top retrieved documents. results reinforce idea top retrieved data valuable source enhance retrieval systems.
specially true short queries usually query-sentence matching terms. argue method improves significantly precision top ranks
handling poorly specified information needs.
LexRank method addressed Erkan Radev (2004) successful
generic multi-document summarization. topic-sensitive LexRank proposed Otterbacher, Erkan, Radev (2005). LexRank, set sentences document cluster
represented graph nodes sentences, links nodes induced similarity relation sentences. system ranks sentences
according random walk model defined terms inter-sentence similarities
similarities sentences topic description question.
Concepts coherence cohesion enable us capture theme text. Coherence represents overall structure multi-sentence text terms macro-level
relations clauses sentences (Halliday & Hasan, 1976). Cohesion, defined
Halliday Hasan (1976), property holding text together one single grammat4

fiComplex Question Answering: Unsupervised Approaches

ical unit based relations (i.e. ellipsis, conjunction, substitution, reference, lexical
cohesion) various elements text. Lexical cohesion defined cohesion
arises semantic relations (collocation, repetition, synonym, hypernym, hyponym, holonym, meronym, etc.) words text (Morris & Hirst, 1991).
Lexical cohesion among words represented lexical chains sequences
semantically related words. summarization methods based lexical chain first extract nouns, compound nouns named entities candidate words (Li, Sun, Kit, &
Webster, 2007). using WordNet3 systems find semantic similarity
nouns compound nouns. lexical chains built two steps:
1. Building single document strong chains disambiguating senses words.
2. Building multi-chain merging strongest chains single documents
one chain.
systems rank sentences using formula involves a) lexical chain, b) keywords query c) named entities. example, Li et al. (2007) uses following
formula:
Score = P (chain) + P (query) + P (namedEntity)
P (chain) sum scores chains whose words come
candidate sentence, P (query) sum co-occurrences key words topic
sentence, P (namedEntity) number name entities existing topic
sentence. three coefficients , set empirically. top ranked
sentences selected form summary.
Harabagiu et al. (2006) introduce new paradigm processing complex questions
relies combination (a) question decompositions; (b) factoid QA techniques;
(c) Multi-Document Summarization (MDS) techniques. question decomposition
procedure operates Markov chain. is, following random walk mixture
model bipartite graph relations established concepts related topic
complex question subquestions derived topic-relevant passages manifest
relations. Decomposed questions submitted state-of-the-art QA system
order retrieve set passages later merged comprehensive answer MDS system. show question decompositions using method
significantly enhance relevance comprehensiveness summary-length answers
complex questions.
approaches based probabilistic models (Pingali, K., & Varma,
2007; Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, & Vanderwende, 2007). Pingali
et al. (2007) rank sentences based mixture model component
model statistical model:
Score(s) = QIScore(s) + (1 ) QF ocus(s, Q)

(1)

3. WordNet (http://wordnet.princeton.edu/) widely used semantic lexicon English language.
groups English words (i.e. nouns, verbs, adjectives adverbs) sets synonyms called synsets,
provides short, general definitions (i.e. gloss definition), records various semantic relations
synonym sets.

5

fiChali, Joty, & Hasan

Score(s) score sentence s. Query-independent score (QIScore)
query-dependent score (QFocus) calculated based probabilistic models. Toutanova
et al. (2007) learns log-linear sentence ranking model maximizing three metrics
sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, (c) Model Frequency.
scoring function learned fitting weights set feature functions sentences
document set trained optimize sentence pair-wise ranking criterion.
scoring function adapted apply summaries rather sentences take
account redundancy among sentences.
Pingali et al. (2007) reduce document-sentences dropping words
contain important information. Toutanova et al. (2007), Vanderwende, Suzuki,
Brockett (2006), Zajic, Lin, Dorr, Schwartz (2006) heuristically decompose
document-sentences smaller units. apply small set heuristics parse
tree create alternatives original sentence (possibly multiple)
simplified versions available selection.
approaches multi-document summarization try cluster sentences
together. Guo Stylios (2003) use verb arguments (i.e. subjects, times, locations
actions) clustering. sentence method establishes indices information
based verb arguments (subject first index, time second, location third
action fourth). sentences closest subjects index put
cluster sorted according temporal sequence earliest
latest. Sentences spaces/locations index value cluster
marked out. clusters ranked based sizes top 10 clusters
chosen. Then, applying cluster reduction module system generates compressed
extract summaries.
approaches Recognizing Textual Entailment, Sentence Alignment,
Question Answering use syntactic and/or semantic information order measure
similarity two textual units. indeed motivated us include syntactic
semantic features get structural similarity document sentence query
sentence (discussed Section 4.1). MacCartney, Grenager, de Marneffe, Cer, Manning
(2006) use typed dependency graphs (same dependency trees) represent text
hypothesis. try find good partial alignment typed dependency
graphs representing hypothesis (contains n nodes) text (graph contains
nodes) search space O((m + 1)n). use incremental beam search combined
node ordering heuristic approximate global search space possible
alignments. locally decomposable scoring function chosen score
alignment sum local node edge alignment scores. scoring measure
designed favor alignments align semantically similar subgraphs, irrespective
polarity. reason, nodes receive high alignment scores words represent
semantically similar. Synonyms antonyms receive highest score unrelated
words receive lowest. Alignment scores also incorporate local edge scores based
shape paths nodes text graph correspond adjacent
nodes hypothesis graph. final step make decision whether
hypothesis entailed text conditioned typed dependency graphs
well best alignment them. make decision use supervised
6

fiComplex Question Answering: Unsupervised Approaches

statistical logistic regression classifier (with feature space 28 features) Gaussian
prior parameter regularization.
Hirao et al. (2004) represent sentences using Dependency Tree Path (DTP) incorporate syntactic information. apply String Subsequence Kernel (SSK) measure
similarity DTPs two sentences. also introduce Extended String
Subsequence Kernel (ESK) incorporate semantics DTPs. Kouylekov Magnini
(2005) use tree edit distance algorithms dependency trees text
hypothesis recognize textual entailment. According approach, text entails
hypothesis H exists sequence transformations (i.e. deletion, insertion
substitution) applied obtain H overall cost certain
threshold. Punyakanok et al. (2004) represent question sentence containing
answer dependency trees. add semantic information (i.e. named entity,
synonyms related words) dependency trees. apply approximate
tree matching order decide similar given pair trees are. also use
edit distance matching criteria approximate tree matching. methods
show improvement BOW scoring methods.

3. Approach
accomplish task answering complex questions extract various important features sentences document collection measure relevance
query. sentences document collection analyzed various levels
document sentences represented vector feature-values. feature set
includes lexical, lexical semantic, statistical similarity, syntactic semantic features,
graph-based similarity measures (Chali & Joty, 2008b). reimplemented many
features successfully applied many related fields NLP.
use simple local search technique fine-tune feature weights. also use
statistical clustering algorithms: EM K-means select relevant sentences
summary generation. Experimental results show systems perform better
include tree kernel based syntactic semantic features though summaries based
syntactic semantic feature achieve good results. Graph-based cosine
similarity lexical semantic features also important selecting relevant sentences.
find local search technique outperforms two EM performs
better K-means based learning. later sections describe subparts
systems details.

4. Feature Extraction
section, describe features used score sentences.
provide detailed examples4 show get feature values. first describe
syntactic semantic features introducing work. follow
detailed description features commonly used question answering
summarization communities.
4. query document sentences used examples taken DUC 2007 collection.

7

fiChali, Joty, & Hasan

4.1 Syntactic Shallow Semantic Features
task like query-based summarization requires use complex syntactic
semantics, approaches BOW often inadequate perform fine-level
textual analysis. importance syntactic semantic features context
described Zhang Lee (2003a), Moschitti et al. (2007), Bloehdorn Moschitti
(2007a), Moschitti Basili (2006) Bloehdorn Moschitti (2007b).
effective way integrate syntactic semantic structures machine learning algorithms use tree kernel functions (Collins & Duffy, 2001; Moschitti & Quarteroni,
2008) successfully applied question classification (Zhang & Lee, 2003a;
Moschitti & Basili, 2006). Syntactic semantic information used effectively measure similarity two textual units MacCartney et al. (2006). best
knowledge, study used tree kernel functions encode syntactic/semantic
information complex tasks computing relatedness query
sentences document sentences. Another good way encode shallow syntactic
information use Basic Elements (BE) (Hovy, Lin, Zhou, & Fukumoto, 2006)
uses dependency relations. experiments show including syntactic semantic
features improves performance sentence selection complex question answering
task (Chali & Joty, 2008a).
4.1.1 Encoding Syntactic Structures
Basic Element (BE) Overlap Measure Shallow syntactic information based dependency relations proved effective finding similarity two textual
units (Hirao et al., 2004). incorporate information using Basic Elements
defined follows (Hovy et al., 2006):
head major syntactic constituent (noun, verb, adjective adverbial phrases),
expressed single item.
relation head-BE single dependent, expressed triple:
(head|modifier|relation).
triples encode syntactic information one decide whether two units
match not- easily longer units (Hovy et al., 2006). extracted BEs
sentences (or query) using package distributed ISI5 .
get BEs sentence, computed Likelihood Ratio (LR)
following Zhou, Lin, Hovy (2005). Sorting BEs according LR scores produced
BE-ranked list. goal generate summary answer users questions.
ranked list BEs way contains important BEs top may may
relevant users questions. filter BEs checking whether contain
word query word QueryRelatedWords (defined Section 4.3).
example, consider following sentence get score 0.77314.
Query: Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
5. website:http://www.isi.edu/ cyl/BE

8

fiComplex Question Answering: Unsupervised Approaches

Sentence: Frankfurt-based body said annual report released today
decided two themes new currency: history European civilization
abstract concrete paintings.
Score: 0.77314
Here, decided|themes|obj considered contain word
query words query relevant words report|annual|mod taken
contains query word report. way, filter BEs related
query. score sentence sum scores divided number BEs
sentence. limiting number top BEs contribute calculation
sentence scores remove BEs little importance sentences
fewer important BEs. set threshold 100 topmost 100 BEs ranked
list contribute normalized sentence score computation. paper,
set threshold took BEs counted calculating scores
sentences.
Tree Kernels Approach order calculate syntactic similarity query
sentence first parse sentence well query syntactic tree
(Moschitti, 2006) using parser like Charniak (1999). calculate similarity
two trees using tree kernel. reimplemented tree kernel model
proposed Moschitti et al. (2007).
build trees, next task measure similarity trees.
this, every tree represented dimensional vector v(T ) = (v1 (T ), v2 (T ), vm (T )),
i-th element vi (T ) number occurrences i-th tree fragment tree
. tree fragments tree sub-trees include least one production
restriction production rules broken incomplete parts (Moschitti
et al., 2007). Figure 1 shows example tree portion subtrees.

Figure 1: (a) example tree (b) sub-trees NP covering press.
Implicitly enumerate possible tree fragments 1, 2, , m. fragments
axis m-dimensional space. Note could done implicitly since
number extremely large. this, Collins Duffy (2001) define tree
kernel algorithm whose computational complexity depend m.
tree kernel two trees T1 T2 actually inner product v(T1 ) v(T2 ):
9

fiChali, Joty, & Hasan

K(T1 , T2 ) = v(T1 ).v(T2 )

(2)

define indicator function Ii (n) 1 sub-tree seen rooted node n
0 otherwise. follows:

vi (T1 ) =

X

X

Ii (n1 ), vi (T2 ) =

n1 N1

Ii (n2 )

(3)

n2 N2

N1 N2 set nodes T1 T2 respectively. So, derive:
K(T1 , T2 ) = v(T1 ).v(T2 ) =

X

vi (T1 )vi (T2 )



X

=

X X

n1 N1 n2 N2

X

=

X

Ii (n1 )Ii (n2 )



C(n1 , n2 )

(4)

n1 N1 n2 N2

define C(n1 , n2 ) = Ii (n1 )Ii (n2 ). Next, note C(n1 , n2 )
computed polynomial time due following recursive definition:
P

1. productions n1 n2 different C(n1 , n2 ) = 0
2. productions n1 n2 same, n1 n2 pre-terminals,
C(n1 , n2 ) = 1
3. Else productions n1 n2 pre-terminals,
nc(n1 )

C(n1 , n2 ) =



(1 + C(ch(n1 , j), ch(n2 , j)))

(5)

j=1

nc(n1 ) number children n1 tree; productions n1
n2 nc(n1 ) = nc(n2 ). i-th child-node n1 ch(n1 , i).
cases query composed two sentences compute similarity
document sentence (s) query-sentences (qi ) take
average scores syntactic feature value.
Syntactic similarity value =

Pn

i=1 K(qi , s)

n

n number sentences query q sentence consideration. TK similarity value (tree kernel) sentence query
sentence q based syntactic structure. example, following sentence
query q get score:
10

fiComplex Question Answering: Unsupervised Approaches

Figure 2: Example semantic trees
Query (q): Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence (s): Europes new currency, euro, rival U.S. dollar international
currency long term, Der Spiegel magazine reported Sunday.
Scores: 90, 41
Average Score: 65.5
4.1.2 Semantic Features
Though introducing syntactic information gives improvement BOW, use
syntactic parses, adequate dealing complex questions whose answers expressed long articulated sentences even paragraphs. Shallow semantic
representations, bearing compact information, could prevent sparseness deep
structural approaches weakness BOW models (MacCartney et al., 2006; Moschitti
et al., 2007).
Initiatives PropBank (PB) (Kingsbury & Palmer, 2002) made design
accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu, Pradhan, Ward, Martin, & Jurafsky, 2003) possible. Hence, attempting application SRL
QA seems natural pinpointing answer question relies deep understanding
semantics both. example, consider PB annotation:
[ARG0 all][TARGET use][ARG1 french franc][ARG2 currency]
annotation used design shallow semantic representation
matched semantically similar sentences, e.g.
[ARG0 Vatican][TARGET use][ARG1 Italian lira][ARG2 currency]
order calculate semantic similarity sentences first represent
annotated sentence (or query) using tree structures like Figure 2 called Semantic Tree
(ST) proposed Moschitti et al. (2007). semantic tree arguments replaced
important wordoften referred semantic head. look noun
first, verb, adjective, adverb find semantic head argument.
none present take first word argument semantic head.
11

fiChali, Joty, & Hasan

Figure 3: Two STs composing STN
However, sentences rarely contain single predicate, rather typically propositions contain one subordinate clauses. instance, let us consider slight modification
second sentence: Vatican, located wholly within Italy uses Italian lira
currency. Here, main predicate uses subordinate predicate located.
SRL system outputs following two annotations:
(1) [ARG0 Vatican located wholly within Italy][TARGET uses][ARG1 Italian
lira][ARG2 currency]
(2) [ARG0 Vatican][TARGET located] [ARGM-LOC wholly][ARGM-LOC within
Italy] uses Italian lira currency
giving STs Figure 3. see Figure 3(A), argument node
corresponds entire subordinate clause label leaf ST (e.g. leaf
ARG0). ST node actually root subordinate clause Figure 3(B).
taken separately, STs express whole meaning sentence. Hence,
accurate define single structure encoding dependency two
predicates Figure 3(C). refer kind nested STs STNs.
Note tree kernel (TK) function defined Section 4.1.1 computes number
common subtrees two trees. subtrees subject constraint
nodes taken none children original tree. Though
definition subtrees makes TK function appropriate syntactic trees, well
suited semantic trees (ST). instance, although two STs Figure 2 share
subtrees rooted ST node, kernel defined computes match.
critical aspect steps (1), (2), (3) TK function productions
two evaluated nodes identical allow match descendants.
means common substructures cannot composed node
children effective ST representation would require. Moschitti et al. (2007) solve
problem designing Shallow Semantic Tree Kernel (SSTK) allows portions
ST match.
Shallow Semantic Tree Kernel (SSTK) reimplemented SSTK according
model given Moschitti et al. (2007). SSTK based two ideas: first, changes
12

fiComplex Question Answering: Unsupervised Approaches

ST, shown Figure 4 adding SLOT nodes. accommodate argument labels
specific order fixed number slots, possibly filled null arguments
encode possible predicate arguments. Leaf nodes filled wildcard character *
may alternatively accommodate additional information. slot nodes used
way adopted TK function generate fragments containing one
children like example shown frames (b) (c) Figure 4. previously
pointed out, arguments directly attached root node kernel function
would generate structure children (or structure children, i.e.
empty) (Moschitti et al., 2007).

Figure 4: Semantic tree fragments
Second, original tree kernel would generate many matches slots filled
null label set new step 0 TK calculation:
(0) n1 (or n2 ) pre-terminal node child label null, C(n1 , n2 ) = 0;
subtract one unit C(n1 , n2 ), step 3:
nc(n1 )

(3) C(n1 , n2 ) =



j=1

(1 + C(ch(n1 , j), ch(n2 , j))) 1

(6)

changes generate new C which, substituted (in place original C )
Eq. 4, gives new SSTK.
example, following sentence query q get semantic score:
Query (q): Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence (s): Frankfurt-based body said annual report released today
decided two themes new currency history European civilization
abstract concrete paintings.
Scores: 6, 12
Average Score: 9
13

fiChali, Joty, & Hasan

4.2 Lexical Features
Here, discuss lexical features commonly used QA
summarization communities. reimplemented research.
4.2.1 N-gram Overlap
N-gram overlap measures overlapping word sequences candidate document
sentence query sentence. view measure overlap scores, query pool
sentence pool created. order create query (or sentence) pool, took
query (or document) sentence created set related sentences replacing
content words6 first-sense synonyms using WordNet. example, given stemmed
document-sentence: John write poem, sentence pool contains: John compose
poem, John write verse form along given sentence.
measured recall based n-gram scores sentence P using following formula:
N gramScore(P ) = maxi (maxj N gram(si , qj ))
P
gram Countmatch (gramn )
N gram(S, Q) = P n
gramn Count (gramn )

(7)
(8)

n stands length n-gram (n = 1, 2, 3, 4), Countmatch (gramn )
number n-grams co-occurring query candidate sentence, qj j-th
sentence query pool, si i-th sentence sentence pool sentence P .
1-gram Overlap Measure
1-gram overlap score measures number words common sentence hand
query related words. computed follows:
1gram Overlap Score =

P

Countmatch (w1 )
w1 Count (w1 )

w1

P

(9)

set content words candidate sentence Countmatch
number matches sentence content words query related words. Count (gramn )
number w1 .
Note order measure 1-gram score took query related words instead
exact query words. motivation behind sentence word(s)
exactly query words synonyms, hypernyms, hyponym gloss
words, get counted.
Example:
Query Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence Frankfurt-based body said annual study released today
decided two themes new currency: history European civilization
abstract concrete paintings.
6. hence forth content words nouns, verbs, adverbs adjectives.

14

fiComplex Question Answering: Unsupervised Approaches

1-gram Score 0.06666 (After normalization7 ).
Note sentence 1-gram overlap score 0.06666 even though
exact word common query words. got score sentence word
study synonym query word report.
N-gram Overlap Measures
above, calculate n-gram overlap scores. example, considering
following query sentence document sentence (From DUC 2007 collection), 4
matching 2-grams: (1 1999,of Euro, January January 1). Hence, employing
formula given above, get following 2-gram score normalization. 3-gram score
also found accordingly.
Query Sentence: Describe steps taken worldwide reaction prior introduction
Euro January 1, 1999. Include predictions expectations reported
press.
Document Sentence: Despite skepticism actual realization single European currency scheduled January 1, 1999, preparations design
Euro note already begun.
2-gram: 0.14815
3-gram: 0.0800
4.2.2 LCS WLCS
sequence W = [w1 , w2 , ..., wn ] subsequence another sequence X = [x1 , x2 , ..., xm ] ,
exists strict increasing sequence [i1 , i2 , ..., ] indices X
j = 1, 2, ..., n xij = wj (Cormen, Leiserson, & Rivest, 1989). Given two sequences
S1 S2 , longest common subsequence (LCS) S1 S2 common subsequence
maximum length (Lin, 2004).
longer LCS two sentences is, similar two sentences are.
used LCS-based F-measure estimate similarity document sentence
length query sentence Q length n follows:
LCS(S, Q)

LCS(S, Q)
Plcs (S, Q) =
n
Flcs (S, Q) = (1 ) Plcs (S, Q) + Rlcs (S, Q)
Rlcs (S, Q) =

(10)
(11)
(12)

LCS(S, Q) length longest common subsequence Q,
constant determines importance precision recall. computing
LCS measure document sentence query sentence viewed sequence words.
7. normalize feature values corresponding sentence respect entire context
particular document.

15

fiChali, Joty, & Hasan

intuition longer LCS two similar are.
recall (Rlcs (S, Q)) ratio length longest common subsequence
Q document sentence length measures completeness. Whereas precision
(Plcs (S, Q)) ratio length longest common subsequence Q
query sentence length measure exactness. obtain equal importance
precision recall set value 0.5. Equation 12 called LCS-based
F-measure. Notice Flcs 1 when, S=Q; Flcs 0 nothing
common Q.
One advantage using LCS require consecutive matches insequence matches reflect sentence level word order n-grams. advantage
automatically includes longest in-sequence common n-grams. Therefore, predefined n-gram length necessary. Moreover, property value less
equal minimum unigram (i.e. 1-gram) F-measure Q. Unigram recall
reflects proportion words also present Q; unigram precision
proportion words Q also S. Unigram recall precision count
co-occurring words regardless orders; LCS counts in-sequence co-occurrences.
awarding credit in-sequence unigram matches, LCS measure also captures
sentence level structure natural way. Consider following example:
S1 John shot thief
S2 John shot thief
S3 thief shot John
Using S1 reference sentence, S2 S3 sentences consideration S2
S3 would 2-gram score since one bigram (i.e. thief)
common S1. However, S2 S3 different meanings. case LCS S2
score 3/4=0.75 S3 score 2/4=0.5 = 0.5. Therefore, S2 better
S3 according LCS.
However, LCS suffers one disadvantage counts main in-sequence
words; therefore, alternative LCSes shorter sequences reflected
final score. example, given following candidate sentence:
S4 thief John shot
Using S1 reference, LCS counts either thief John shot both;
therefore, S4 LCS score S3 2-gram would prefer S4 S3.
order measure LCS score sentence took similar approach previous section using WordNet (i.e. creation sentence pool query pool). calculated
LCS score using following formula:

LCS score = maxi (maxj Flcs (si , qj ))

(13)

qj j-th sentence query pool, si i-th sentence
sentence pool.
16

fiComplex Question Answering: Unsupervised Approaches

basic LCS problem differentiate LCSes different spatial
relations within embedding sequences (Lin, 2004). example, given reference
sequence two candidate sequences Y1 Y2 follows:
S: B C E F G
Y1 : B C H K
Y2 : H B K C
Y1 Y2 LCS score. However, Y1 better choice Y2
Y1 consecutive matches. improve basic LCS method store length
consecutive matches encountered far regular two dimensional dynamic program table
computing LCS. call weighted LCS (WLCS) use k indicate length
current consecutive matches ending words xi yj . Given two sentences X Y,
WLCS score X computed using similar dynamic programming
procedure stated Lin (2004). use WLCS advantage measuring
similarity taking words higher dimension like string kernels indeed
reduces time complexity. before, computed WLCS-based F-measure
way using query pool sentence pool.
W LCS score = maxi (maxj Fwlcs (si , qj ))

(14)

Example:
Query Sentence: Describe steps taken worldwide reaction prior introduction
Euro January 1, 1999. Include predictions expectations reported
press.
Document Sentence: Despite skepticism actual realization single European currency scheduled January 1, 1999, preparations design
Euro note already begun.
find 6 matching strings: (of 1 Euro 1999 January) longest common
subsequence considering sentence related sentences. WLCS set
weight 1.2. normalization, get following LCS WLCS scores
sentence applying formula.
LCS Score: 0.27586
WLCS Score: 0.15961
4.2.3 Skip-Bigram Measure
skip-bigram pair words sentence order allowing arbitrary gaps. Skipbigram measures overlap skip-bigrams candidate sentence query
sentence (Lin, 2004). rely query pool sentence pool using
WordNet. Considering following sentences:
17

fiChali, Joty, & Hasan

S1 John shot thief
S2 John shoot thief
S3 thief shoot John
S4 thief John shot
get sentence C(4,2)=6 skip-bigrams8 . example, S1 following
skip-bigrams: (John shot, John the, John thief, shot the, shot thief
thief) S2 three skip bi-gram matches S1 (John the, John thief, thief),
S3 one skip bi-gram match S1 (the thief), S4 two skip bi-gram matches
S1 (John shot, thief).
skip bi-gram score document sentence length query
sentence Q length n computed follows:
SKIP2 (S, Q)
C(m, 2)
SKIP2 (S, Q)
Pskip2 (S, Q) =
C(n, 2)
Fskip2 (S, Q) = (1 ) Pskip2 (S, Q) + Rskip2 (S, Q)
Rskip2 (S, Q) =

(15)
(16)
(17)

SKIP2 (S, Q) number skip bi-gram matches Q,
constant determines importance precision recall. set value
0.5 associate equal importance precision recall. C combination
function. call equation 17 skip bigram-based F-measure. computed skip
bigram-based F-measure using formula:

SKIP BIGRAM = maxi (maxj Fskip2 (si , qj ))

(18)

example, given following query sentence, get 8 skip-bigrams: (on 1,
January 1, January 1999, Euro, 1 1999, 1999, January on).
Applying equations above, get skip bi-gram score 0.05218 normalization.
Query Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence Despite skepticism actual realization single European currency
scheduled January 1, 1999, preparations design Euro note
already begun.
Skip bi-gram Score: 0.05218
8. C(n, r) =

n!
r!(nr)!

18

fiComplex Question Answering: Unsupervised Approaches

Note skip bi-gram counts in-order matching word pairs LCS counts
one longest common subsequence. put constraint maximum skip distance,
dskip , two in-order words form skip bi-gram avoids spurious matches
like from. example, set dskip 0 equivalent bi-gram
overlap measure (Lin, 2004). set dskip 4 word pairs 4 words
apart form skip bi-grams. experiment set dskip = 4 order ponder
4 words apart get skip bi-grams.
Modifying equations: 15, 16, 17 allow maximum skip distance limit
straightforward: following Lin (2004) count skip bi-gram matches, SKIP2 (S, Q),
within maximum skip distance replace denominators equations
actual numbers within distance skip bi-grams reference sentence
candidate sentence respectively.
4.2.4 Head Head Related-words Overlap
number head words common two sentences indicate much
relevant other. order extract heads sentence (or query),
sentence (or query) parsed Minipar9 dependency tree extract
heads call exact head words. example, head word sentence: John
eats rice eat.
take synonyms, hyponyms, hypernyms10 query-head words
sentence-head words form set words call head-related words.
measured exact head score head-related score follows:
P

w1 HeadSet Countmatch (w1 )

(19)

w1 HeadRelSet Countmatch (w1 )

(20)

ExactHeadScore =
HeadRelatedScore =

P

P

P

w1 HeadSet Count (w1 )

w1 HeadRelSet Count (w1 )

HeadSet set head words sentence Countmatch number
matches HeadSet query sentence. HeadRelSet set
synonyms, hyponyms, hypernyms head words sentence Countmatch
number matches head-related words query sentence.
example, list head words query sentence measures:
Query: Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Heads Query: include, reaction, step, take, describe, report, Euro, introduction, press,
prediction, 1999, expectation
Sentence: Frankfurt-based body said annual report released today
decided two themes new currency: history European civilization
abstract concrete paintings.
9. http://www.cs.ualberta.ca/ lindek/minipar.htm
10. hypernym hyponym levels restricted 2 3 respectively.

19

fiChali, Joty, & Hasan

Heads Sentence: history, release, currency, body, report,painting, say, civilization,
theme, decide.
Exact Head Score:

1
11

= 0.09

Head Related Score: 0
4.3 Lexical Semantic Features
form set words call QueryRelatedWords taking content words
query, first-sense synonyms, nouns hypernyms/hyponyms, nouns
gloss definitions using WordNet.
4.3.1 Synonym Overlap
synonym overlap measure overlap list synonyms content
words extracted candidate sentence query related words. computed
follows:
Synonym Overlap Score =

P

w1 SynSet Countmatch (w1 )
w1 SynSet Count (w1 )

P

(21)

SynSet synonym set content words sentence Countmatch
number matches SynSet query related words.
4.3.2 Hypernym/Hyponym Overlap
hypernym/hyponym overlap measure overlap list hypernyms (level
2) hyponyms (level 3) nouns extracted sentence consideration
query related words. computed follows:
Hypernym/hyponym overlap score =

P

h1 HypSet Countmatch (h1 )
h1 HypSet Count (h1 )

P

(22)

HypSet hyponym/hyponym set nouns sentence Countmatch
number matches HypSet query related words.
4.3.3 Gloss Overlap
gloss overlap measure overlap list content words extracted
gloss definition nouns sentence consideration query related
words. computed follows:
Gloss Overlap Score =

P

g1 GlossSet Countmatch (g1 )

P

g1 GlossSet Count (g1 )

(23)

GlossSet set content words (i.e. nouns, verbs adjectives) taken
gloss definition nouns sentence Countmatch number matches
GlossSet query related words.
20

fiComplex Question Answering: Unsupervised Approaches

Example:
example, given query following sentence gets synonym overlap score
0.33333, hypernym/hyponym overlap score 0.1860465 gloss overlap score 0.1359223.
Query Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence Frankfurt-based body said annual report released today
decided two themes new currency: history European civilization
abstract concrete paintings.
Synonym Overlap Score: 0.33333
Hypernym/Hyponym Overlap Score: 0.1860465
Gloss Overlap Score: 0.1359223
4.4 Statistical Similarity Measures
Statistical similarity measures based co-occurrence similar words corpus.
Two words termed similar belong context. used thesaurus
provided Dr. Dekang Lin11 purpose. used two statistical similarity
measures:
Dependency-based similarity measure
method uses dependency relations among words order measure similarity (Lin, 1998b). extracts dependency triples uses statistical approach
measure similarity. Using given corpus one retrieve similar words
given word. similar words grouped clusters.
Note word one cluster. cluster represents
sense word similar words sense. So, selecting right cluster
word problem. goals are: i) create bag similar words query
words ii) get bag similar words (dependency based) query words
measure overlap score sentence words.
Creating Bag Similar Words:
query-word extract clusters thesaurus. order
determine right cluster query word measure overlap score
query related words (i.e. exact words, synonyms, hypernyms/hyponyms gloss)
clusters. hypothesis cluster words common query
related words right cluster assumption first synonym correct
sense. choose cluster word highest overlap score.

Overlap scorei =

P

w1 QueryRelatedW ords Countmatch (w1 )

(24)

Cluster = argmaxi (Overlap Scorei )

(25)

w1 QueryRelatedW ords Count (w1 )

P

11. http://www.cs.ualberta.ca/ lindek/downloads.htm

21

fiChali, Joty, & Hasan

QueryRelatedWords set exact words, synonyms, hyponyms/hypernyms,
gloss words words query (i.e query words) Countmatch number
matches query related words ith cluster similar words.
Measuring Overlap Score:
get clusters query words measured overlap
cluster words sentence words call dependency based similarity measure:

DependencyM easure =

w1 SenW ords Countmatch (w1 )

P

P

w1 SenW ords Count (w1 )

(26)

SenWords set words sentence Countmatch number
matches sentence words cluster similar words.
Proximity-based similarity measure
similarity computed based linear proximity relationship words
(Lin, 1998a). uses information theoretic definition similarity measure
similarity. similar words grouped clusters. took similar approach
measure feature previous section except used different thesaurus.
Example:
Considering following query sentence get following measures:
Query: Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence: Frankfurt-based body said annual report released today
decided two themes new currency: history European civilization
abstract concrete paintings.
Dependency-based Similarity Score: 0.0143678
Proximity-based Similarity Score: 0.04054054
4.5 Graph-based Similarity Measure
Erkan Radev (2004) used concept graph-based centrality rank set sentences
producing generic multi-document summaries. similarity graph produced
sentences document collection. graph node represents sentence.
edges nodes measure cosine similarity respective pair sentences.
degree given node indication important sentence is. Figure 5
shows example similarity graph 4 sentences.
similarity graph constructed, sentences ranked according
eigenvector centrality. LexRank performed well context generic summarization. apply LexRank query-focused context topic-sensitive version LexRank
proposed Otterbacher et al. (2005). followed similar approach order calculate
feature. score sentence determined mixture model relevance
sentence query similarity sentence high-scoring sentences.
22

fiComplex Question Answering: Unsupervised Approaches

Figure 5: LexRank similarity
Relevance Question
first stem sentences collection compute word IDFs (Inverse
Document Frequency) using following formula:
N +1
idfw = log
0.5 + sfw




(27)

N total number sentences cluster, sfw number
sentences word w appears in.
also stem questions remove stop words. relevance sentence
question q computed by:
rel(s|q) =

X

wq

log (tfw,s + 1) log (tfw,q + 1) idfw

(28)

tfw,s tfw,q number times w appears q, respectively.
Mixture Model
previous section measured relevance sentence question
sentence similar high scoring sentences cluster also high
score. instance, sentence gets high score based question relevance
model likely contain answer question related sentence, may
similar question itself, also likely contain answer (Otterbacher et al.,
2005).
capture idea following mixture model:

p(s|q) =

(

X
sim(s, v)
rel(s|q)
P
+ (1 d)
P
zC rel(z|q)
zC sim(z, v)
vC

)

p(v|q)

(29)

p(s|q), score sentence given question q, determined sum
relevance question similarity sentences collection.
C set sentences collection. value parameter call
23

fiChali, Joty, & Hasan

bias trade-off two terms equation set empirically. higher
values prefer relevance question similarity sentences.
denominators terms normalization. Although computationally
expensive, equation 29 calculates sum entire collection since required
model sense global impact voting sentences. measure
cosine similarity weighted word IDFs similarity two sentences cluster:

sim(x, y) = qP

P

wx,y

tfw,x tfw,y (idfw )2

2
xi x (tfxi ,x idfxi )

qP

2
yi (tfyi ,y idfyi )

(30)

Equation 29 written matrix notation follows:
p = [dA + (1 d)B]T p

(31)

square matrix given index i, elements i-th column
proportional rel(i|q). B also square matrix entry B(i,j)
proportional sim(i,j). matrices normalized row sums add 1.
Note result normalization rows resulting square matrix Q =
[dA + (1 d)B] also add 1. matrix called stochastic defines Markov
chain. view sentence state Markov chain Q(i,j) specifies
transition probability state state j corresponding Markov chain.
vector p looking Eq. 31 stationary distribution Markov chain.
intuitive interpretation stationary distribution understood concept
random walk graph representation Markov chain. probability
transition made current node nodes similar query.
probability (1-d) transition made nodes lexically similar current
node. Every transition weighted according similarity distributions. element
vector p gives asymptotic probability ending corresponding state
long run regardless starting state. stationary distribution Markov chain
computed simple iterative algorithm called power method (Erkan & Radev,
2004). starts uniform distribution. iteration eigenvector updated
multiplying transpose stochastic matrix. Since Markov chain
irreducible aperiodic algorithm guaranteed terminate.

5. Ranking Sentences
use several methods order rank sentences generate summaries applying
features described Section 4. section describe systems detail.
5.1 Learning Feature-weights: Local Search Strategy
order fine-tune weights features, used local search technique. Initially set feature-weights, w1 , , wn , equal values (i.e. 0.5) (see Algorithm 1).
train weights using DUC 2006 data set. Based current weights
score sentences generate summaries accordingly. evaluate summaries using
24

fiComplex Question Answering: Unsupervised Approaches

Input: Stepsize l, Weight Initial Value v
Output: vector w
~ learned weights
Initialize weight values wi v.
1 n
rg1 = rg2 = prev = 0
(true)
scoreSentences(w)
~
generateSummaries()
rg2 = evaluateROUGE()
rg1 rg2
prev = wi
wi + = l
rg1 = rg2
else
break
end
end
end
return w
~
Algorithm 1: Tuning weights using Local Search technique
automatic evaluation tool ROUGE (Lin, 2004) (described Section 7) ROUGE
value works feedback learning loop. learning system tries maximize
ROUGE score every step changing weights individually specific step size (i.e.
0.01). means, learn weight wi change value wi keeping weight
values (wj j6=i ) stagnant. weight wi algorithm achieves local maximum
(i.e. hill climbing) ROUGE value.
learned feature-weights compute final scores sentences
using formula:
scorei = x~i .w
~

(32)

x~i feature vector i-th sentence, w
~ weight vector, scorei
score i-th sentence.
5.2 Statistical Machine Learning Approaches
experimented two unsupervised statistical learning techniques features
extracted previous section sentence selection problem:
1. K-means learning
2. Expectation Maximization (EM) learning
5.2.1 K-means Learning
K-means hard clustering algorithm defines clusters center mass
members. start set initial cluster centers chosen randomly go
25

fiChali, Joty, & Hasan

several iterations assigning object cluster whose center closest.
objects assigned recompute center cluster centroid
) members. distance function use squared Euclidean distance
mean (
instead true Euclidean distance.
Since square root monotonically growing function squared Euclidean distance
result true Euclidean distance computation overload smaller
square root dropped.
learned means clusters using K-means algorithm next
task rank sentences according probability model. used Bayesian
model order so. Bayes law says:

x|qk , )P (qk |)
p(x
x|)
p(x
x|qk , )P (qk |)
p(x
PK
x|qk , )p(qk |)
k=1 p(x

x, ) =
P (qk |x
=

(33)

qk cluster, x feature vector representing sentence, parameter
set class models. set weights clusters equiprobable (i.e. P (qk |) =
x|qk , ) using gaussian probability distribution. gaussian
1/K). calculated p(x
probability density function (pdf) d-dimensional random variable x given by:

x) =
p(,
) (x

e

1
x
)T 1 (x
x
)
(x
2

dp
)
2 det(

(34)

, mean vector, , covariance matrix, parameters
) K-means algorithm calculate
gaussian distribution. get means (
covariance matrix using unbiased covariance estimation procedure:

j =

N
1 X
xi j )(x
x j )T
(x
N 1 i=1

(35)

5.2.2 EM Learning
EM algorithm gaussian mixture models well known method cluster analysis.
useful outcome model produces likelihood value clustering model
likelihood values used select best model number different
models providing number parameters (i.e. number
clusters).
26

fiComplex Question Answering: Unsupervised Approaches

x) represented feature vector length
Input: sample n data-points (x
L
Input: Number Clusters K
Output: array K-means-based Scores
Data: Array dnK , K , K
Data: Array C K , nK
Randomly choose K data-points K initial means: k , k = 1, , K.
repeat
1 n
j 1 K
xi j k2 = (x
xi j )T (x
xi j )
ij = kx
end
ik < il , l 6= k
assign x C k .
end
end
P
1 K
C
x C

xj

j

=
C i|
|C
end
change occurs ;
/* calculating covariances cluster
1 K
C i|
= |C
j 1
C ij ) (C
C ij )T
+ = (C
end
= (1/(m 1))
end
/* calculating scores sentences
1 n
j 1 K
1
1


2 (x j ) j (x j )
yij = e

2

*/

*/

j )
det(

end
j 1 K
P
// where, wj = 1/K
zij = (yij wj )/ K
j=1 yij wj ;
end
k ) k
= max(
Push zim
end
return
Algorithm 2: Computing K-means based similarity measure

27

fiChali, Joty, & Hasan

significant problem EM algorithm converges local maximum
likelihood function hence quality result depends initialization.
problem along method improving initialization discussed later
section.
EM soft version K-means algorithm described above. K-means
start set random cluster centers c1 ck . iteration soft assignment
data-points every cluster calculating membership probabilities. EM
iterative two step procedure: 1. Expectation-step 2. Maximization-step.
expectation step compute expected values hidden variables hi,j cluster
membership probabilities. Given current parameters compute likely
object belongs clusters. maximization step computes likely
parameters model given cluster membership probabilities.
data-points considered generated mixture model k-gaussians
form:

P (x) =

k
X

P (C = i)P (x|C = i) =

i=1

k
X

, )
P (C = i)P (x|

(36)

i=1

total likelihood model k components, given observed data points
X = x 1 , , x n , is:

L(|X)

=


n X
k


i=1 j=1

x |j ) =
P (C = j)P (x

n
X

k
X

i=1

log

j=1

n X
k


i=1 j=1

xi |
j , j )
wj P (x

xi |
j , j ) ( taking log likelihood )
wj P (x

(37)
(38)

P probability density function (i.e. eq 34). j j mean
covariance matrix component j, respectively. component contributes proportion,
P
wj , total population that: K
j=1 wj = 1.
Log likelihood used instead likelihood turns product sum.
describe EM algorithm estimating gaussian mixture.
Singularities covariance matrix must non-singular invertible.
EM algorithm may converge position covariance matrix becomes singular
| = 0) close singular, means invertible anymore. covariance
(|
matrix becomes singular close singular EM may result wrong clusters.
restrict covariance matrices become singular testing cases iteration
algorithm follows:
q

| > 1e9 ) update
( |
else update
28

fiComplex Question Answering: Unsupervised Approaches

Discussion: Starting values EM algorithm
convergence rate success clustering using EM algorithm degraded
poor choice starting values means, covariances, weights components. experimented one summary (for document number D0703A DUC
2007) order test impact initial values EM algorithm. cluster
means initialized
p heuristic spreads randomly around ean(DAT A)
standard deviation Cov(DAT A) 10. initial covariance set Cov(DAT A)
initial values weights wj = 1/K K number clusters.
is, d-dimensional data-points parameters j th component follows:

~j = rand(1, , d)
j = (DAT A)
wj

q

(DAT A) 10 + ~(DAT A)

= 1/K

highly variable nature results tests reflected inconsistent values total log likelihood results repeated experiments indicated
using random starting values initial estimates means frequently gave poor
results. two possible solutions problem. order get good results
using random starting values (as specified algorithm) run EM algorithm several times choose initial configuration get maximum
log likelihood among configurations. Choosing best one among several runs
computer intensive process. So, improve outcome EM algorithm gaussian
mixture models, necessary find better method estimating initial means
components.
best starting position EM algorithm, regard estimates means,
would one estimated mean per cluster closer true mean
cluster.
achieve aim explored widely used K-means algorithm cluster
(means) finding method. is, means found K-means clustering
utilized initial means EM calculate initial covariance matrices
using unbiased covariance estimation procedure (Equation 35).
Ranking Sentences
sentences clustered EM algorithm, identify sentences
xi , ) qr denotes clusare question-relevant checking probabilities, P (qr |x
x , ) > 0.5 x considered
ter question-relevant. sentence x , P (qr |x
question-relevant. cluster mean values greater one
considered question-relevant cluster.
next task rank question-relevant sentences order include
summary. done easily multiplying feature vector x~i weight
vector w
~ learned applying local search technique (Equation 32).
29

fiChali, Joty, & Hasan

Input: Sample n data-points ( x ) represented feature vector length
L
Input: Number Clusters K
Output: array EM-based Scores
k , k ) k = 1, , K, equal priors set
Start K initial Gaussian models: N (
P (qk ) = 1/K.
repeat
(i)
x j , (i) )
/* Estimation step: compute probability P (qk |x
(i)

data point xj , j = 1, , n, belong class qk
j 1 n
k 1 K
(i)
x j , (i) ) =
P (qk |x

(i)

(i)

xj |qk , (i) )
P (qk |(i) )p(x
xj |(i) )
p(x
(i)

=
end
end
/* Maximization step:
k 1 K
j 1 n
// update means:
i+1
k

=

(i)

(i)

x j |
k , k )
P (qk |(i) )p(x

(i)
(i)
(i)
x j |
(i)
k=1 P (qk | )p(x
k , k )

PK

*/

=

// update variances:
(i+1)
k

*/

(i)
xj , (i) )
j=1 x j P (qk |x
Pn
(i)
xj , (i) )
j=1 P (qk |x

Pn

(i)
xj
xj , (i) )(x
xj (i+1)
)(x
j=1 P (qk |x
k
PN
(i)
xj , (i) )
j=1 P (qk |x

Pn

(i+1)
)

k

// update priors:

P (qk (i + 1)|(i+1) ) =

n
1X
(i)
x j , (i) )
P (qk |x
n j=1

end
end
total likelihood increase falls desired threshold ;
return
Algorithm 3: Computing EM-based similarity measure

30

fiComplex Question Answering: Unsupervised Approaches

6. Redundancy Checking Generating Summary
sentences scored easiest way create summaries output
topmost N sentences required summary length reached. case,
ignoring factors: redundancy coherence.
know text summarization clearly entails selecting salient information putting together coherent summary. answer summary consists
multiple separately extracted sentences different documents. Obviously,
selected text snippets individually important. However, many competing sentences included summary issue information overlap parts
output comes mechanism addressing redundancy needed. Therefore,
summarization systems employ two levels analysis: first content level every
sentence scored according features concepts covers, second textual level,
when, added final output, sentences deemed important
compared similar candidates included final answer summary. Goldstein, Kantrowitz, Mittal, Carbonell (1999)
observed authors called Maximum-Marginal-Relevance (MMR). Following Hovy et al. (2006) modeled overlap intermediate summary
to-be-added candidate summary sentence.
call overlap ratio R, R 0 1 inclusively. Setting R = 0.7
means candidate summary sentence, s, added intermediate summary,
S, sentence overlap ratio less equal 0.7.

7. Experimental Evaluation
section describes results experiments conducted using DUC12 2007 dataset
provided NIST 13 . questions experiments address include:
different features affect behavior summarizer system?
one algorithms (K-means, EM Local Search) performs better
particular problem?
used main task DUC 2007 evaluation. task was:
Given complex question (topic description) collection relevant documents,
task synthesize fluent, well-organized 250-word summary documents
answers question(s) topic.
documents DUC 2007 came AQUAINT corpus comprising newswire
articles Associated Press New York Times (1998-2000) Xinhua News
Agency (1996-2000). NIST assessors developed topics interest choose set
25 documents relevant (document cluster) topic. topic document
cluster given 4 different NIST assessors including developer topic.
assessor created 250-word summary document cluster satisfies information
12. http://www-nlpir.nist.gov/projects/duc/
13. National Institute Standards Technology

31

fiChali, Joty, & Hasan

need expressed topic statement. multiple reference summaries used
evaluation summary content.
purpose experiments study impact different features. accomplish generated summaries 45 topics DUC 2007 seven
systems defined below:
LEX system generates summaries based lexical features (Section 4.2):
n-gram (n=1,2,3,4), LCS, WLCS, skip bi-gram, head, head synonym overlap.
LEXSEM system considers lexical semantic features (Section 4.3): synonym, hypernym/hyponym, gloss, dependency-based proximity-based similarity.
SYN system generates summary based syntactic feature (Section 4.1.1).
COS system generates summary based graph-based method (Section 4.5).
SYS1 system considers features except syntactic semantic features
(All features except section 4.1).
SYS2 system considers features except semantic feature (All features
except section 4.1.2)
system generates summaries taking features (Section 4) account.
7.1 Automatic Evaluation
ROUGE carried automatic evaluation summaries using ROUGE (Lin,
2004) toolkit, widely adopted DUC automatic summarization evaluation. ROUGE stands Recall-Oriented Understudy Gisting Evaluation.
collection measures determines quality summary comparing reference summaries created humans. measures count number overlapping units
n-gram, word-sequences, word-pairs system-generated summary
evaluated ideal summaries created humans. available ROUGE measures
are: ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W ROUGE-S. ROUGE-N n-gram
recall candidate summary set reference summaries. ROUGE-L measures
longest common subsequence (LCS) takes account sentence level structure
similarity naturally identifies longest co-occurring insequence n-grams automatically.
ROUGE-W measures weighted longest common subsequence (WLCS) providing improvement basic LCS method computation credit sentences
consecutive matches words. ROUGE-S overlap skip-bigrams candidate summary set reference summaries skip-bigram pair words
sentence order allowing arbitrary gaps. ROUGE measures
applied automatic evaluation summarization systems achieved promising
results (Lin, 2004).
systems, report widely accepted important metrics: ROUGE-2
ROUGE-SU. also present ROUGE-1 scores since never shown
correlate human judgement. ROUGE measures calculated running
32

fiComplex Question Answering: Unsupervised Approaches

ROUGE-1.5.5 stemming removal stopwords. ROUGE run-time parameters
set DUC 2007 evaluation setup. are:
ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -a
also show 95% confidence interval important evaluation metrics systems
report significance meaningful comparison. use ROUGE tool
purpose. ROUGE uses randomized method named bootstrap resampling compute
confidence interval. used 1000 sampling points bootstrap resampling.
report evaluation scores one baseline system (The BASE column)
tables order show level improvement systems achieve. baseline
system generates summaries returning leading sentences (up 250 words)
hT EXT field recent document(s).
presenting results highlight top two F-scores bottom one F-score
indicate significance glance.
7.1.1 Results Discussion
K-means Learning Table 1 shows ROUGE-1 scores different combinations
features K-means learning. noticeable K-means performs best
graph-based cosine similarity feature. Note including syntactic feature
improve score. Also, including syntactic semantic features increases score
significant amount. Summaries based lexical features give us good
ROUGE-1 evaluation.
Scores
Recall
Precision
F-score

LEX
0.366
0.397
0.381

LEXSEM
0.360
0.393
0.376

SYN
0.346
0.378
0.361

COS
0.378
0.408
0.393

SYS1
0.376
0.403
0.389

SYS2
0.365
0.415
0.388


0.366
0.415
0.389

BASE
0.312
0.369
0.334

Table 1: ROUGE-1 measures K-means learning

Table 2 shows ROUGE-2 scores different combinations features K-means
learning. like ROUGE-1 graph-based cosine similarity feature performs well here.
get significant improvement ROUGE-2 score include syntactic feature
features. Semantic features affect score much. Lexical Semantic features
perform well here.

Scores
Recall
Precision
F-score

LEX
0.074
0.080
0.077

LEXSEM
0.076
0.084
0.080

SYN
0.063
0.069
0.065

COS
0.085
0.092
0.088

SYS1
0.074
0.080
0.077

SYS2
0.077
0.107
0.090


0.076
0.109
0.090

Table 2: ROUGE-2 measures K-means learning

33

BASE
0.060
0.072
0.064

fiChali, Joty, & Hasan

Table 3 shows: ROUGE-SU scores best features without syntactic
semantic. Including syntactic/semantic features features degrades scores.
Summaries based lexical features achieve good scores.
Scores
Recall
Precision
F-score

LEX
0.131
0.154
0.141

LEXSEM
0.127
0.152
0.138

SYN
0.116
0.139
0.126

COS
0.139
0.162
0.149

SYS1
0.135
0.176
0.153

SYS2
0.134
0.174
0.152


0.134
0.174
0.152

BASE
0.105
0.124
0.112

Table 3: ROUGE-SU measures K-means learning
Table 4 shows 95% confidence interval (for F-measures K-means learning)
important ROUGE evaluation metrics systems comparison confidence
interval baseline system. seen systems performed significantly
better baseline system cases.
Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


ROUGE-1
0.326680 - 0.342330
0.362976 - 0.400498
0.357154 - 0.395594
0.345512 - 0.377525
0.372804 - 0.413440
0.367817 - 0.408390
0.358237 - 0.400000
0.350756 - 0.404275

ROUGE-2
0.060870 - 0.068840
0.064983 - 0.090981
0.069909 - 0.091376
0.056041 - 0.076337
0.075127 - 0.104377
0.063284 - 0.095170
0.065219 - 0.093733
0.066281 - 0.095393

ROUGE-SU
0.108470 - 0.116720
0.128390 - 0.157784
0.126157 - 0.151831
0.116191 - 0.136799
0.134971 - 0.164885
0.132061 - 0.162509
0.123703 - 0.153165
0.124157 - 0.159447

Table 4: 95% confidence intervals K-means system

EM learning Table 5 Table 7 show different ROUGE measures feature
combinations context EM learning. easily noticed
measures get significant amount improvement ROUGE scores include
syntactic semantic features along features. get 3-15% improvement
SYS1 F-score include syntactic feature 2-24% improvement include
syntactic semantic features. cosine similarity measure perform well
K-means experiments. Summaries considering lexical features achieve
good results.
Table 8 shows 95% confidence interval (for F-measures EM learning) important ROUGE evaluation metrics systems comparison confidence
interval baseline system. see systems performed significantly
better baseline system cases.
Local Search Technique ROUGE scores based feature combinations
given Table 9 Table 11. Summaries generated including features perform
34

fiComplex Question Answering: Unsupervised Approaches

Scores
Recall
Precision
F-score

LEX
0.383
0.415
0.398

LEXSEM
0.357
0.390
0.373

SYN
0.346
0.378
0.361

COS
0.375
0.406
0.390

SYS1
0.379
0.411
0.395

SYS2
0.399
0.411
0.405


0.398
0.399
0.399

BASE
0.312
0.369
0.334


0.090
0.138
0.109

BASE
0.060
0.072
0.064


0.143
0.185
0.161

BASE
0.105
0.124
0.112

Table 5: ROUGE-1 measures EM learning

Scores
Recall
Precision
F-score

LEX
0.088
0.095
0.092

LEXSEM
0.079
0.087
0.083

SYN
0.063
0.069
0.065

COS
0.087
0.094
0.090

SYS1
0.084
0.091
0.088

SYS2
0.089
0.116
0.100

Table 6: ROUGE-2 measures EM learning

Scores
Recall
Precision
F-score

LEX
0.145
0.171
0.157

LEXSEM
0.128
0.153
0.139

SYN
0.116
0.139
0.126

COS
0.138
0.162
0.149

SYS1
0.143
0.167
0.154

SYS2
0.145
0.186
0.163

Table 7: ROUGE-SU measures EM learning

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


ROUGE-1
0.326680 - 0.342330
0.382874 - 0.416109
0.352610 - 0.395048
0.345512 - 0.377525
0.366364 - 0.410020
0.378068 - 0.413658
0.360319 - 0.414068
0.378177 - 0.412705

ROUGE-2
0.060870 - 0.068840
0.075084 - 0.110454
0.070816 - 0.095856
0.056041 - 0.076337
0.076088 - 0.104243
0.077480 - 0.099739
0.073661 - 0.112157
0.077515 - 0.115231

ROUGE-SU
0.108470 - 0.116720
0.144367 - 0.172449
0.125276 - 0.154562
0.115713 - 0.136599
0.133251 - 0.164110
0.141550 - 0.168759
0.130022 - 0.171378
0.141345 - 0.164849

Table 8: 95% confidence intervals EM system

35

fiChali, Joty, & Hasan

best scores measures. get 7-15% improvement SYS1 F-score
include syntactic feature 8-19% improvement SYS1 F-score include
syntactic semantic features. case also lexical features (LEX) perform well
better features (ALL).
Scores
Recall
Precision
F-score

LEX
0.379
0.411
0.394

LEXSEM
0.358
0.390
0.373

SYN
0.346
0.378
0.361

COS
0.375
0.406
0.390

SYS1
0.382
0.414
0.397

SYS2
0.388
0.434
0.410


0.390
0.438
0.413

BASE
0.312
0.369
0.334

Table 9: ROUGE-1 measures local search technique

Scores
Recall
Precision
F-score

LEX
0.085
0.092
0.088

LEXSEM
0.079
0.087
0.083

SYN
0.063
0.069
0.065

COS
0.087
0.094
0.090

SYS1
0.086
0.093
0.090

SYS2
0.095
0.114
0.104


0.099
0.116
0.107

BASE
0.060
0.072
0.064

Table 10: ROUGE-2 measures local search technique

Scores
Recall
Precision
F-score

LEX
0.143
0.168
0.155

LEXSEM
0.128
0.153
0.139

SYN
0.116
0.139
0.126

COS
0.138
0.162
0.149

SYS1
0.145
0.170
0.157

SYS2
0.148
0.195
0.169


0.150
0.196
0.170

BASE
0.105
0.124
0.112

Table 11: ROUGE-SU measures local search technique
Table 12 shows 95% confidence interval (for F-measures local search technique)
important ROUGE evaluation metrics systems comparison confidence interval baseline system. find systems performed significantly
better baseline system cases.
7.1.2 Comparison
results reported see three algorithms systems clearly outperform baseline system. Table 13 shows F-scores reported ROUGE measures
Table 14 reports 95% confidence intervals baseline system, best system
DUC 2007, three techniques taking features (ALL) consideration.
see method based local search technique outperforms two
EM algorithm performs better K-means algorithm. analyze deeply, find
cases ROUGE-SU local search confidence intervals overlap
best DUC 2007 system.
36

fiComplex Question Answering: Unsupervised Approaches

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


ROUGE-1
0.326680 - 0.342330
0.380464 - 0.409085
0.353458 - 0.394853
0.345512 - 0.377525
0.366364 - 0.410020
0.381544 - 0.414534
0.370310 - 0.415768
0.384897 - 0.416301

ROUGE-2
0.060870 - 0.068840
0.078002 - 0.100107
0.070845 - 0.096261
0.056041 - 0.076337
0.076088 - 0.104243
0.079550 - 0.101246
0.078760 - 0.114175
0.084181 - 0.114753

ROUGE-SU
0.108470 - 0.116720
0.143851 - 0.166648
0.125342 - 0.154729
0.115713 - 0.136599
0.133251 - 0.164110
0.144551 - 0.170047
0.141043 - 0.174575
0.146302 - 0.171736

Table 12: 95% confidence intervals local search system

Algorithms
Baseline
Best System
K-means
EM
Local Search

ROUGE-1
0.334
0.438
0.389
0.399
0.413

ROUGE-2
0.064
0.122
0.089
0.109
0.107

ROUGE-SU
0.112
0.174
0.152
0.161
0.170

Table 13: ROUGE F-scores different systems

Algorithms
Baseline
Best System
K-means
EM
Local Search

ROUGE-1
0.326680 - 0.342330
0.431680 - 0.445970
0.350756 - 0.404275
0.378177 - 0.412705
0.384897 - 0.416301

ROUGE-2
0.060870 - 0.068840
0.118000 - 0.127680
0.066281 - 0.095393
0.077515 - 0.115231
0.084181 - 0.114753

ROUGE-SU
0.108470 - 0.116720
0.169970 - 0.179390
0.124157 - 0.159447
0.141345 - 0.164849
0.146302 - 0.171736

Table 14: 95% confidence intervals different systems

37

fiChali, Joty, & Hasan

7.2 Manual Evaluation
sample 105 summaries14 drawn different systems generated summaries
conduct extensive manual evaluation order analyze effectiveness
approaches. manual evaluation comprised Pyramid-based evaluation contents
user evaluation get assessment linguistic quality overall responsiveness.
7.2.1 Pyramid Evaluation
DUC 2007 main task, 23 topics selected optional community-based
pyramid evaluation. Volunteers 16 different sites created pyramids annotated
peer summaries DUC main task using given guidelines15 . 8 sites among
created pyramids. used pyramids annotate peer summaries
compute modified pyramid scores16 . used DUCView.jar17 annotation tool
purpose. Table 15 Table 17 show modified pyramid scores systems
three algorithms. baseline systems score also reported. peer summaries
baseline system generated returning leading sentences (up 250 words)
hT EXT field recent document(s). results see
systems perform better baseline system inclusion syntactic semantic
features yields better scores. three algorithms also notice lexical
semantic features best terms modified pyramid scores.
7.2.2 User Evaluation
10 university graduate students judged summaries linguistic quality overall
responsiveness. given score integer 1 (very poor) 5 (very good)
guided consideration following factors: 1. Grammaticality, 2. Non-redundancy,
3. Referential clarity, 4. Focus 5. Structure Coherence. also assigned
content responsiveness score automatic summaries. content score
integer 1 (very poor) 5 (very good) based amount information
summary helps satisfy information need expressed topic narrative.
measures used DUC 2007. Table 18 Table 20 present average linguistic
quality overall responsive scores systems three algorithms.
baseline systems scores given meaningful comparison. closer look
results, find systems perform worse baseline system terms
linguistic quality achieve good scores case overall responsiveness. also
obvious tables exclusion syntactic semantic features often causes
lower scores. hand, lexical lexical semantic features show good overall
responsiveness scores three algorithms.
14. 7 systems 3 algorithms, cumulatively 21 systems. Randomly chose
5 summaries 21 systems.
15. http://www1.cs.columbia.edu/ becky/DUC2006/2006-pyramid-guidelines.html
16. equals sum weights Summary Content Units (SCUs) peer summary matches,
normalized weight ideally informative summary consisting number contributors
peer.
17. http://www1.cs.columbia.edu/ ani/DUC2005/Tool.html

38

fiComplex Question Answering: Unsupervised Approaches

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Modified Pyramid Scores
0.13874
0.44984
0.51758
0.45762
0.50368
0.42872
0.41666
0.49900

Table 15: Modified pyramid scores K-means system

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Modified Pyramid Scores
0.13874
0.51894
0.53226
0.45058
0.48484
0.47758
0.44734
0.49756

Table 16: Modified pyramid scores EM system

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Modified Pyramid Scores
0.13874
0.49760
0.53912
0.43512
0.49510
0.46976
0.46404
0.47944

Table 17: Modified pyramid scores local search system

39

fiChali, Joty, & Hasan

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Linguistic Quality
4.24
3.08
4.08
3.24
4.00
2.72
3.12
3.56

Overall Responsiveness
1.80
3.20
3.80
3.60
3.60
2.20
2.80
3.80

Table 18: Linguistic quality responsive scores K-means system

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Linguistic Quality
4.24
4.08
3.56
4.20
3.80
3.68
4.20
3.36

Overall Responsiveness
1.80
4.40
3.40
3.80
4.00
3.80
3.60
3.40

Table 19: Linguistic quality responsive scores EM system

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Linguistic Quality
4.24
3.24
3.12
2.64
3.40
3.40
3.12
3.20

Overall Responsiveness
1.80
2.40
4.20
2.00
3.40
3.60
3.80
3.20

Table 20: Linguistic quality responsive scores local search system

40

fiComplex Question Answering: Unsupervised Approaches

8. Conclusion Future Work
paper presented works answering complex questions. extracted eighteen important features sentences document collection. Later used
simple local search technique fine-tune feature weights. weight, wi ,
algorithm achieves local maximum ROUGE value. way, learn
weights rank sentences multiplying feature-vector weight-vector.
also experimented two unsupervised learning techniques: 1) EM 2) K-means
features extracted. assume two clusters sentences: 1. queryrelevant 2. query-irrelevant. learned means clusters using K-means
algorithm used Bayesian model order rank sentences. learned means
K-means algorithm used initial means EM algorithm. applied EM algorithm cluster sentences two classes : 1) query-relevant 2)
query-irrelevant. take query-relevant sentences rank using learned
weights (i.e. local search). methods generating summaries filter
redundant sentences using redundancy checking module generate summaries
taking top N sentences.
also experimented effects different kinds features. evaluated
systems automatically using ROUGE report significance results
95% confidence intervals. conducted two types manual evaluation: 1) Pyramid
2) User Evaluation analyze performance systems. experimental
results mostly show following: (a) approaches achieve promising results, (b)
empirical approach based local search technique outperforms two learning
techniques EM performs better K-means algorithm, (c) systems achieve
better results include tree kernel based syntactic semantic features,
(d) cases ROUGE-SU local search confidence intervals overlap
best DUC 2007 system.
experimenting supervised learning techniques (i.e. SVM, MAXENT, CRF etc) analyzing perform problem. Prior that, produced huge amount labeled data automatically using similarity measures ROUGE
(Toutanova et al., 2007).
future plan decompose complex questions several simple questions
measuring similarity document sentence query sentence.
certainly serve create limited trees subsequences might increase
precision. Thus, expect decomposing complex questions sets
subquestions entail systems improve average quality answers returned
achieve better coverage question whole.

Acknowledgments
thank anonymous reviewers useful comments earliest version
paper. Special thanks go colleagues proofreading paper. also grateful
graduate students took part user evaluation process. research
reported supported Natural Sciences Engineering Research Council
(NSERC) research grant University Lethbridge.
41

fiChali, Joty, & Hasan

Appendix A. Stop Word List

reuters
may
nov
tue

accordingly

alone

another
anyway
appropriate
ask
awfully
becomes

better

cant
certainly
comes
containing
currently
didnt
dont

else
etc
everyone
except
followed
forth
get
goes
h
hasnt

ap
jun
dec
wed

across
aint
along
amid

anyways

asking
b
becoming
believe

c
cannot
changes
concerning
contains

different
done
edu
elsewhere
etc.
everything
f
following
four
gets
going



jan
jul
tech
thu
able
actually

already
among
anybody
anywhere
arent
associated



beyond
cmon
cant
clearly
consequently
corresponding
definitely


eg
enough
even
everywhere
far
follows

getting
gone
hadnt
havent

42

feb
aug
news
fri


allow
also
amongst
anyhow
apart
around

became

beside

cs
cause
co
consider
could
described

downwards
e.g.
entirely
ever
ex



given
got
happens


mar
sep
index
sat

afterwards
allows
although

anyone
appear

available

beforehand
besides
brief
came
causes
com
considering
couldnt
despite
doesnt

eight
especially
every
exactly
fifth
former
furthermore
gives
gotten
hardly


apr
oct
mon

according

almost
always

anything
appreciate
aside
away
become
behind
best


certain
come
contain
course


e
either
et
everybody
example
five
formerly
g
go
greetings

hes

fiComplex Question Answering: Unsupervised Approaches

hello
hereafter
hi

im
immediate
indicated
inward

keep
l
less
likely

mean


nearly
nevertheless
non
nothing

old
onto

overall
perhaps
probably
r
regarding

help
hereby

howbeit
ive

indicates


keeps
lately
lest
little
mainly
meanwhile
mostly

necessary
new
none
novel





placed
provides
rather
regardless

hence
herein

however
ie
inasmuch
inner
isnt

kept
later
let
look
many
merely
mr.
n
need
next
noone

often



p
please
q
rd
regards


hereupon


i.e.
inc
insofar

j
know
latter
lets
looking
may
might
ms.
namely
needs
nine

nowhere
oh
one
others

particular
plus
que

relatively

43



hither
id

indeed
instead
itd

knows
latterly
like
looks
maybe

much
nd
neither

normally

ok
ones
otherwise
outside
particularly
possible
quite
really
respectively

heres

hopefully
ill
ignored
indicate

itll
k
known
least
liked
ltd

moreover
must
near
never
nobody

obviously
okay

ought

per
presumably
qv
reasonably
right

fiChali, Joty, & Hasan


says
seemed
sensible
shall

sometime
specified
sup
tell
thanx

theres
thereupon
theyve

thus
towards
twice
unless
us
usually
via


werent
whenever
wherein
whither
whose
within
wouldnt
youd


said
second
seeming
sent


sometimes
specify
sure
tends


thereafter

think
though

tried
two
unlikely
use
uucp
viz
wasnt
weve


whereupon


without
x
youll



secondly
seems
serious

somebody
somewhat
specifying

th
thats

thereby

third
three
together
tries
u

used
v
vs
way
welcome
whats
wheres
wherever
whos

wont

youre
z

saw
see
seen
seriously
shouldnt
somehow
somewhere
still
ts

thats

therefore
theyd



truly
un
unto
useful
value
w

well
whatever
whereafter
whether
whoever
willing
wonder
yes
youve
zero

44

say
seeing
self
seven
since
someone
soon
sub
take
thank

thence
therein
theyll
thorough
throughout
took
try


uses
various
want
wed
went

whereas

whole
wish
would
yet


saying
seem
selves
several
six
something
sorry

taken
thanks


theres
theyre
thoroughly
thru
toward
trying
unfortunately
upon
using

wants
well

whence
whereby



would



fiComplex Question Answering: Unsupervised Approaches

References
Bloehdorn, S., & Moschitti, A. (2007a). Combined syntactic semantic kernels text
classification. 29th European Conference IR Research, ECIR 2007, pp. 307318
Rome, Italy.
Bloehdorn, S., & Moschitti, A. (2007b). Structure semantics expressive text kernels.
CIKM-2007, pp. 861864.
Chali, Y., & Joty, S. R. (2008a). Improving performance random walk model
answering complex questions.. Proceedings 46th Annual Meeting
ACL-HLT. Short Paper Section, pp. 912 OH, USA.
Chali, Y., & Joty, S. R. (2008b). Selecting sentences answering complex questions.
Proceedings EMNLP, pp. 304313 Hawaii, USA.
Charniak, E. (1999). Maximum-Entropy-Inspired Parser. Technical Report CS-99-12
Brown University, Computer Science Department.
Collins, M., & Duffy, N. (2001). Convolution Kernels Natural Language. Proceedings
Neural Information Processing Systems, pp. 625632 Vancouver, Canada.
Cormen, T. R., Leiserson, C. E., & Rivest, R. L. (1989). Introduction Algorithms.
MIT Press.
Erkan, G., & Radev, D. R. (2004). LexRank: Graph-based Lexical Centrality Salience
Text Summarization. Journal Artificial Intelligence Research, 22, 457479.
Goldstein, J., Kantrowitz, M., Mittal, V., & Carbonell, J. (1999). Summarizing Text Documents: Sentence Selection Evaluation Metrics. Proceedings 22nd International ACM Conference Research Development Information Retrieval,
SIGIR, pp. 121128 Berkeley, CA.
Guo, Y., & Stylios, G. (2003). New Multi-document Summarization System. Proceedings Document Understanding Conference. NIST.
Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., & Jurafsky, D. (2003). Shallow
Semantic Parsing Using Support Vector Machines. Technical Report TR-CSLR2003-03 University Colorado.
Halliday, M., & Hasan, R. (1976). Cohesion English. Longman, London.
Harabagiu, S., Lacatusu, F., & Hickl, A. (2006). Answering complex questions random
walk models. Proceedings 29th annual international ACM SIGIR conference
Research development information retrieval, pp. 220 227. ACM.
Hirao, T., , Suzuki, J., Isozaki, H., & Maeda, E. (2004). Dependency-based sentence
alignment multiple document summarization. Proceedings Coling 2004, pp.
446452 Geneva, Switzerland. COLING.
45

fiChali, Joty, & Hasan

Hovy, E., Lin, C. Y., Zhou, L., & Fukumoto, J. (2006). Automated Summarization Evaluation Basic Elements. Proceedings Fifth Conference Language
Resources Evaluation Genoa, Italy.
Kingsbury, P., & Palmer, M. (2002). Treebank PropBank. Proceedings
international conference Language Resources Evaluation Las Palmas, Spain.
Kouylekov, M., & Magnini, B. (2005). Recognizing textual entailment tree edit distance
algorithms. Proceedings PASCAL Challenges Workshop: Recognising Textual
Entailment Challenge.
Li, J., Sun, L., Kit, C., & Webster, J. (2007). Query-Focused Multi-Document Summarizer Based Lexical Chains. Proceedings Document Understanding
Conference Rochester. NIST.
Lin, C. Y. (2004). ROUGE: Package Automatic Evaluation Summaries. Proceedings Workshop Text Summarization Branches Out, Post-Conference Workshop
Association Computational Linguistics, pp. 7481 Barcelona, Spain.
Lin, D. (1998a). Information-Theoretic Definition Similarity. Proceedings
International Conference Machine Learning, pp. 296304 Madison, Wisconsin.
Lin, D. (1998b). Automatic Retrieval Clustering Similar Words. Proceedings
International Conference Computational Linguistics Association
Computational Linguistics, pp. 768774 Montreal, Canada.
Losada, D. (2005). Language modeling sentence retrieval: comparison
multiple-bernoulli models multinomial models. Information Retrieval Theory Workshop Glasgow, UK.
Losada, D., & Fernandez, R. T. (2007). Highly frequent terms sentence retrieval.
Proc. 14th String Processing Information Retrieval Symposium, SPIRE07, pp.
217228 Santiago de Chile.
MacCartney, B., Grenager, T., de Marneffe, M., Cer, D., & Manning, C. D. (2006). Learning recognize features valid textual entailments. Proceedings Human
Language Technology Conference North American Chapter ACL, p. 4148
New York, USA.
Morris, J., & Hirst, G. (1991). Lexical cohesion computed thesaural relations
indicator structure text. Computational Linguistics, 17 (1), 2148.
Moschitti, A. (2006). Efficient convolution kernels dependency constituent syntactic
trees. Proceedings 17th European Conference Machine Learning Berlin,
Germany.
Moschitti, A., & Basili, R. (2006). Tree Kernel approach Question Answer Classification Question Answering Systems. Proceedings 5th international
conference Language Resources Evaluation Genoa, Italy.
46

fiComplex Question Answering: Unsupervised Approaches

Moschitti, A., & Quarteroni, S. (2008). Kernels linguistic structures answer extraction. Proceedings 46th Conference Association Computational
Linguistics (ACL08). Short Paper Section Columbus, OH, USA.
Moschitti, A., Quarteroni, S., Basili, R., & Manandhar, S. (2007). Exploiting Syntactic
Shallow Semantic Kernels Question/Answer Classificaion. Proceedings
45th Annual Meeting Association Computational Linguistics, pp. 776783
Prague, Czech Republic. ACL.
Murdock, V., & Croft, W. B. (2005). translation model sentence retrieval. HLT 05:
Proceedings conference Human Language Technology Empirical Methods
Natural Language Processing, pp. 684691 Morristown, NJ, USA. ACL.
Otterbacher, J., Erkan, G., & Radev, D. R. (2005). Using Random Walks Questionfocused Sentence Retrieval. Proceedings Human Language Technology Conference
Conference Empirical Methods Natural Language Processing, pp. 915922
Vancouver, Canada.
Pingali, P., K., R., & Varma, V. (2007). IIIT Hyderabad DUC 2007. Proceedings
Document Understanding Conference Rochester. NIST.
Punyakanok, V., Roth, D., & Yih, W. (2004). Mapping dependencies trees: application
question answering. Proceedings AI & Math Florida, USA.
Strzalkowski, T., & Harabagiu, S. (2008). Advances Open Domain Question Answering.
Springer.
Toutanova, K., Brockett, C., Gamon, M., Jagarlamudi, J., Suzuki, H., & Vanderwende,
L. (2007). pythy summarization system: Microsoft research duc 2007.
proceedings Document Understanding Conference Rochester. NIST.
Vanderwende, L., Suzuki, H., & Brockett, C. (2006). Microsoft Research DUC2006:
Task-Focused Summarization Sentence Simplification Lexical Expansion.
Proceedings Document Understanding Conference Rochester. NIST.
Zajic, D. M., Lin, J., Dorr, B. J., & Schwartz, R. (2006). Sentence Compression Component Multi-Document Summarization System. Proceedings Document
Understanding Conference Rochester. NIST.
Zhang, A., & Lee, W. (2003a). Question Classification using Support Vector Machines.
Proceedings Special Interest Group Information Retrieval, pp. 2632 Toronto,
Canada. ACM.
Zhang, D., & Lee, W. S. (2003b). Language Modeling Approach Passage Question
Answering. Proceedings Twelfth Text REtreival Conference, pp. 489495
Gaithersburg, Maryland.
Zhou, L., Lin, C. Y., & Hovy, E. (2005). BE-based Multi-dccument Summarizer
Query Interpretation. Proceedings Document Understanding Conference Vancouver, B.C., Canada.

47

fiJournal Artificial Intelligence Research 35 (2009) 813857

Submitted 03/09; published 08/09

Modularity Aspects Disjunctive Stable Models
Tomi Janhunen
Emilia Oikarinen

Tomi.Janhunen@tkk.fi
Emilia.Oikarinen@tkk.fi

Helsinki University Technology
Department Information Computer Science
P.O. Box 5400, FI-02015 TKK, Finland

Hans Tompits
Stefan Woltran

tompits@kr.tuwien.ac.at
woltran@dbai.tuwien.ac.at

Technische Universitt Wien
Institut fr Informationssysteme
Favoritenstrae 911, A-1040 Vienna, Austria

Abstract
Practically programming languages allow programmer split program
several modules brings along several advantages software development.
paper, interested area answer-set programming fully declarative
nonmonotonic languages applied. context, obtaining modular structure
programs means straightforward since output entire program cannot
general composed output components. better understand effects
disjunctive information modularity restrict scope analysis case
disjunctive logic programs (DLPs) subject stable-model semantics. define notion
DLP-function, well-defined input/output interface provided, establish
novel module theorem indicates compositionality stable-model semantics
DLP-functions. module theorem extends well-known splitting-set theorem
enables decomposition DLP-functions given strongly connected components
based positive dependencies induced rules. setting, also possible split
shared disjunctive rules among components using generalized shifting technique.
concept modular equivalence introduced mutual comparison DLP-functions
using generalization translation-based verification method.

1. Introduction
Practically programming languages used software development allow programmer
split program several modules interact well-dened input/output
interfaces. Given this, entire program viewed composition component modules typically linked together respective run-time environment.
expected benets modular program development manifold. First, imposes
good programming style followed programmer. complex software system
much easier develop set interacting components rather monolithic program. Second, modular architecture allows additional exibility regards delegating
programming tasks amongst team programmers. setting, goal programmer implement desired input/output behavior(s) terms concrete module(s)
together implement software system developed. Third, modular program
c
2009
AI Access Foundation. rights reserved.

fiJanhunen, Oikarinen, Tompits & Woltran

design also exploited order boost execution programs. Program optimization also facilitated structural information encompassed module interfaces.
Answer-set programming (ASP) (Marek & Truszczyski, 1999; Niemel, 1999; Gelfond
& Leone, 2002) paradigm declarative problem solving solutions problems
described terms rules subject nonmonotonic semantics based stable models
(Gelfond & Lifschitz, 1988). typical problem representations, tight correspondence
solutions stable models sought for, default negation fully exploited
order obtain concise encodings relations involved problem descriptions.
Furthermore, recursive denitions enable, e.g., representation closures relations
natural way. Due ecient implementations emerging applications,
paradigm received increasing attention past two decades.1 meantime,
number extensionssuch disjunctions, weight constraints, aggregateshave
proposed basic syntax normal logic programs. paper, concentrate
class disjunctive logic programs (DLPs) appropriate solving search problems
residing second level polynomial-time hierarchy. semantical account
DLPs based respective generalization stable-model semantics (Gelfond &
Lifschitz, 1991).
paper, goal investigate modularity context DLPs stablemodel semantics. Since stable models dened complete programs,
lend modular programming prima facie. Perhaps reason, concept
module yet raised much attention realm answer-set programming.
Except dedicated papers (Gaifman & Shapiro, 1989; Eiter, Gottlob, & Veith, 1997b;
Baral, Dzifcak, & Takahashi, 2006), modules mostly appeared by-product studies
formal properties like stratication, splitting, or, lately, work equivalence
relations programs (Lifschitz & Turner, 1994; Eiter, Gottlob, & Mannila, 1997a;
Eiter, Ianni, Lukasiewicz, Schindlauer, & Tompits, 2008). recent approach Oikarinen
Janhunen (2008a), modular architecture put forth Gaifman Shapiro (1989)
accommodated classes normal smodels programs. main result
module theorem links stable models associated individual modules
composition. result signicant indicates stable models
compositional much sense classical models propositional logic.
major restriction implied module theorem denition set
positively interdependent atoms must given within module.
Besides general benets modular program development discussed above,
also looking potential computational advantages modularizing reasoning tasks ASP.
context, search stable models probably central reasoning task.
Results like module theorem discussed provide basis modularizing
search task. Extra care, however, required computation stable models
modules separation necessarily ecient. sophisticated methods,
identifying cones influence Boolean circuits (Junttila & Niemel, 2000), devised
identify modules relevant search stable modelsthe rest used
expand qualied stable model one entire program. strategy alleviates
treatment extremely large program instances also amenable query evaluation.
1. 20th anniversary stable-model semantics celebrated ICLP08 held Udine,
Italy, December 2008.

814

fiModularity Aspects Disjunctive Stable Models

Unfortunately, contemporary disjunctive answer-set solvers, claspd (Drescher
et al., 2008), cmodels (Giunchiglia, Lierler, & Maratea, 2006), dlv (Leone et al., 2006),
gnt (Janhunen, Niemel, Seipel, Simons, & You, 2006), exhibit little support modular
reasoning although related techniques like strongly connected components exploited internally. also reasoning tasks boosted modular approach.
instance, optimization answer-set programs gives rise problem verifying
whether dierent versions programs answer sets. demonstrated
Oikarinen Janhunen (2009), verication tasks may benet modularization,
and, particular, approximation techniques based modular equivalence introduced.
Following idea, rst modular o-line optimizer answer-set programs, called modopt, recently implemented (Janhunen, 2008b).
also interesting applications modules sight: Gebser et al. (2008a)
propose incremental technique answer-set solving. idea gradually extend
program instance terms additional modules, e.g., solving AI planning problems.
Moreover, theoretical results like splitting-set theorem (Lifschitz & Turner, 1994)
module theorem directly exploited correctness proofs. instance, proved
Oikarinen Janhunen (2008b) models prioritized circumscription
captured disjunctive stable models using particular translation. similar proof
strategy adopted Theorem 8.5 paper.
anticipate compositional semantics also prove useful one tries boost
search stable models via parallelization, e.g., computing stable models modules
parallel. However, order avoid excessive communication costs, extra caution needed
stable models computed separation linked together potentially rejected.
One possibility identify mutually independent modules basis distribution.
Besides aspect, modularization may also lead novel methods (non-parallelized)
computation stable models, traditional ones.
Structure Preview Results paper, concentrate formal underpinnings modular programming context disjunctive logic programs
stable-model semantics. proceed follows. rst goal generalize theory
developed normal programs smodels programs (Oikarinen & Janhunen, 2008a)
case disjunctive programs. end, rst introduce notion DLPfunction Section 2. term goes back Gelfond Gabaldon (1999) introduced
LP-functions (partial) denitions new relations terms old, known ones. enable functional view disjunctive programs, endowed well-dened
input/output interface. idea partition signature program encapsulated
way input atoms, output atoms, hidden (or local ) atoms. distinctions
provide basis systematic composition larger disjunctive logic programs
program modules. However, arbitrary combinations program modules meaningful and, rst all, adopt syntactic restrictions introduced Gaifman Shapiro
(1989) context negation/disjunction-free logic programs. interplay default negation disjunctions brings along new factors lead relaxation
restrictions sense program modules allowed share rules. Then,
basic syntactic issues DLP-functions laid out, concentrate semantics
Section 3. respect, follow strict model-theoretic approach and, particular,
815

fiJanhunen, Oikarinen, Tompits & Woltran

address role input atoms comes viewing DLP-functions mathematical
functions. proceed step step assign three dierent classes models
DLP-function, viz. classical models, minimal models, stable models. last provides
appropriate generalization disjunctive stable models (Gelfond & Lifschitz, 1991)
presence input atoms.
second objective establish adequacy concept DLP-function
view compositional semantics. witnessed main result paper,
viz. module theorem shows stable models DLP-function, ,
alternatively obtained unions compatible stable models modules constituting
. proof theorem based notions completion (Clark, 1978) loop
formulas (Lin & Zhao, 2004; Lee & Lifschitz, 2003) rst lifted case DLPfunctions Section 4 preparatory step. proof module theorem follows
main topic Section 5. result non-trivial underlying semantics based
stable models inherently nonmonotonic. feature already recognized Gaifman
Shapiro (1989) much simpler setting denite programsneither involving default
negation disjunctions. observed them, too, syntactic restrictions program
composition necessary order guarantee compositionality properties semantics
based Herbrand models.2 current paper, strive analogous results
case programs permitting default negation disjunctions. turns
strongly connected components positive dependency graphs provide key criterion
comes conning program composition. compositionality properties disjunctive
programs stable-model semantics also arisen context so-called
splitting-set theorem (Lifschitz & Turner, 1994; Eiter et al., 1997a, 2008). fact,
module theorem established herein proper generalization predecessor (Oikarinen &
Janhunen, 2008a). illustrate potential modular architecture evaluation
quantied Boolean formulas (QBFs), serve canonical representatives classes
polynomial-time hierarchy (PH). Due basic complexity results established Eiter
Gottlob (1995), natural perspective concentrate second level
PH case disjunctive programs.
third aim paper look particular applications module theorem disjunctive logic programming. Section 6, take opposite view
modular construction DLP-functions consider possibilities decomposition
even absence structural information. turns strongly connected
components also exploited respect but, addition, occurrences hidden
atoms must taken account splitting DLP-function components.
demonstrated Section 7, results open new prospects regards unwinding disjunctions
using principle shifting (Gelfond, Przymusinska, Lifschitz, & Truszczyski, 1991; Dix,
Gottlob, & Marek, 1996; Eiter, Fink, Tompits, & Woltran, 2004). proper generalization
principle partially covers also programs involving head-cycles formulated
proved correct. Moreover, due modular nature DLP-functions, makes perfect
sense compare modules. notion modular equivalence introduced
purpose Section 8. Interestingly, modular equivalence supports substitutions equivalent
programs also lends translation-based verification put forth Oikarinen
2. main concern Gaifman Shapiro (1989) modularity respect logical consequences
definite program hence intersection Herbrand models.

816

fiModularity Aspects Disjunctive Stable Models

Janhunen (2004, 2009) related cases ordinary equivalence smodels programs. Section 9 contrasts approach related work. Finally, Section 10 provides
brief summary results concludes paper.

2. Class DLP-Functions
topic section syntax DLP-functions well syntactic restrictions
imposed composition DLP-functions. disjunctive rule expression form
a1 b1 , . . . , bm , c1 , . . . , ck ,

(1)

n, m, k 0, a1 , . . . , , b1 , . . . , bm , c1 , . . . , ck propositional atoms. Since
order atoms considered insignicant, write B, C shorthand rules
form (1), = {a1 , . . . , }, B = {b1 , . . . , bm }, C = {c1 , . . . , ck } respective
sets atoms. basic intuition behind rule B, C atom
positive body B inferred none atoms negative body C,
atom head inferred. B C empty, disjunctive
fact, written . empty, constraint, written B, C.
disjunctive logic program (DLP) conventionally formed nite set disjunctive
rules. Additionally, want distinguished input output interface DLP.
end, extend denition originally proposed Gaifman Shapiro (1989) case
disjunctive programs.3 natural interface imposes certain restrictions
rules allowed module. Given set R disjunctive rules, write At(R)
signature R, i.e., set (ground) atoms eectively appearing rules R.
Definition 2.1 DLP-function, , quadruple hR, I, O, Hi, I, O, H
pairwise distinct sets input atoms, output atoms, hidden atoms, respectively, R
DLP rule B, C R,
1. B C H,
2. 6= , (O H) 6= .
DLP-function = hR, I, O, Hi occasionally identied R and, slight abuse
notation, write B, C denote B, C R. rst condition
Denition 2.1, rules DLP-function must obey interface specication ,
i.e., At(R) H. regards sets atoms I, O, H involved module
interface, atoms considered visible hence accessible
DLP-functions conjoined ; either produce input utilize output
. hand, hidden atoms H used formalize auxiliary concepts
may make sense context DLP-functions may save space
substantially demonstrated, e.g., Janhunen Oikarinen (2007, Example 4.5).
second condition Denition 2.1 concerned set atoms H defined
rules R. principle non-empty disjunctive head must involve least one
atom H. ensure DLP-function must interfere
3. Similar approaches within area ASP previously introduced Gelfond Gabaldon
(1999), Janhunen (2006), Oikarinen Janhunen (2008a).

817

fiJanhunen, Oikarinen, Tompits & Woltran

denitions input atoms terms rules B, C satisfying I.
otherwise, rules may conditioned input atoms.4 Given set atoms,
distinguish set rules dene atoms R, i.e., set defining rules
Def R (S) = {A B, C R | 6= }.

(2)

next objective specify conditions composition DLP-functions
may take place. Roughly speaking, idea larger DLP-functions formed
modular fashion using smaller DLP-functions components. observed already
Gaifman Shapiro (1989), syntactic restrictions program composition necessary
order guarantee compositionality properties semantics based Herbrand models, even simple case denite programs. Thus, program union operator
composition without restrictions satisfactory respect compositionality.
start adapting construction Gaifman Shapiro (1989) case
disjunctive programs.
Definition 2.2 Two DLP-functions 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 respect input/output interfaces
1. (I1 O1 H1 ) H2 = ,
2. (I2 O2 H2 ) H1 = ,
3. O1 O2 = ,
4. Def R1 (O1 ) = Def R1 R2 (O1 ),
5. Def R2 (O2 ) = Def R1 R2 (O2 ).
rst three conditions due Gaifman Shapiro (1989)
imply sets O1 , H1 , O2 , H1 mutually pairwise distinct. Violations
respect rst two conditions circumvented renaming strategy.
instance, atom H1 appears I2 O2 H2 , hence violating second condition,
possible replace occurrences 1 new atom 6 I2 O2 H2
appearing 2 . removes conict respect forth.5
hand, last two conditions Denition 2.2 concern distribution
rules involved definitions (2) sets atoms O1 O2 , i.e., sets rules
Def R1 (O1 ) Def R2 (O2 ), R1 R2 , respectively. regards disjunctive rules,
principle sets dening rules must remain intact union R1 R2
formed means module supposed copies rules form
denition output atoms. spite this, two modules 1 2 subject
conditions Denition 2.2 may eectively share disjunctive rules B, C
non-empty head O1 6= O2 6= , demonstrated next.
4. particular, input atoms head rule act much like atoms negative body C.
5. opposite view program composition considered Section 6, possibilities decomposing disjunctive program smaller DLP-functions studied. counterpart renaming,
revealing operator introduced Definition 7.3 used circumventing first two conditions
Definition 2.2.

818

fiModularity Aspects Disjunctive Stable Models

Example 2.3 Consider following two DLP-functions:6
{b}
b c;
a,
{a, c}



{a}
b c;
e a, e
{b, c}

formally, 1 = hR1 , {a, c}, {b}, {d}i 2 = hR2 , {b, c}, {a}, {e}i
R1 R2 = {a b c}. show 1 2 respect input/output interfaces
other: First, hidden atoms e occur exactly one two programs
thus first two conditions Definition 2.2 satisfied. Second, disjoint output
atoms, viz. atom b 1 atom 2 . Finally, Def R1 ({b}) = Def R1 R2 ({b}) =
Def R2 ({a}) = Def R1 R2 ({a}) = {a b c}, shows also final two conditions
Definition 2.2 satisfied, far syntax concerned, makes sense compose
larger DLP-function obtained kind union 1 2 ; see (4) below.
contrast disjunctive programs, shared rules arise context normal
logic programs since one head atom allowed rule. stated
smodels programs (Simons, Niemel, & Soininen, 2002) although programs
may contain, among rule types, choice rules form
{a1 , . . . , } B, C

(3)

heads cardinality greater one. observed Oikarinen Janhunen (2008a),
heads choice rules possessing multiple atoms freely split without aecting
semantics. splitting rules n dierent rules {ai } B, C
1 n, concern creation n copies rule body B, C could
reserve quadratic space worst case. new atom introduced circumvent
this. nature proper disjunctive rules (1), subject study paper,
somewhat dierent. Unlike choice rules, disjunctive rules may interact rule heads.
Example 2.3, denition depends b vice versa. However, given choice rule
{a, b} c instance, choices regarding b independent other: c
true, atoms truth value. quite dierent interpretation
ab c makes either b true given c true. grasp interaction
b natural b input denition and, conversely, input
b. demonstrated Section 7, shared rules rewritten input atoms
removed rule head drawback rewriting technique, compactness
representation partly lost. Therefore, appreciate extra exibility provided
shared rules interpret reect true nature disjunctive rules.
general, DLP-functions composed according following principle:

6. henceforth make use tabular format represent DLP-functions: output signature
given top, input signature bottom, rules listed between. Thus,
declaration hidden signature remains implicit.

819

fiJanhunen, Oikarinen, Tompits & Woltran

O2
L

I1
H1

H2
I2

=

O1

I1 O2

O2

H2

I1

I1 I2

I2

H1

O1

O1 I2

Figure 1: Treatment signatures composition operator .
Definition 2.4 (Composition) Let 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2
two DLP-functions respect input/output interfaces other. Then, composition 1 2 defined determined
1 2 = hR1 R2 , (I1 \ O2 ) (I2 \ O1 ), O1 O2 , H1 H2 i.

(4)

treatment atom types Denitions 2.2 2.4 summarized Figure 1.
two symmetric gures left-hand side illustrate signatures DLP-functions
1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 subject composition. Input signatures
output signatures emphasized light gray dark gray shadings, respectively.
superposition two gures yields diagram given right represents
resulting nine categories atoms. three may involve shared atoms
originate 1 2 . interface conditions introduced intuitive
readers acquainted principles object-oriented programming:
1. Although 1 2 must share hidden atoms, may share input atoms, i.e.,
I1 I2 6= allowed. Output atoms treated dierently O1 O2 = assumed.
2. input atom 1 becomes output atom 1 2 appears output
atom 2 , i.e., 2 provides input 1 setting. input atoms 2
treated symmetric fashion.
3. hidden atoms 1 2 retain status 1 2 .
Example 2.5 Recall Example 2.3 showed DLP-functions 1 2 respect
input/output interfaces other. Thus, composition 1 2 defined,
1 2 hR1 R2 , I, O, Hi set input atoms ({a, c} \ {a}) ({b, c} \ {b}) =
{c}, set output atoms {a} {b} = {a, b}, set H hidden atoms
{d} {e} = {d, e}, i.e., using tabular format represent modules,
{b}
b c;
a,
{a, c}



{a}
b c;
e a, e
{b, c}

=

{a, b}
b c;
a, d;
e a, e
{c}

definitions b 1 2 share rule b c. Thanks flexibility
Definition 2.4, also able split 1 2 components whenever appropriate.
820

fiModularity Aspects Disjunctive Stable Models

Following previous approaches (Gelfond & Gabaldon, 1999; Oikarinen & Janhunen,
2008a), dene signature At() DLP-function = hR, I, O, Hi H.7
notational convenience, distinguish visible hidden parts At() setting
Atv () = Ath () = H = At() \ Atv (), respectively. Moreover, Ati ()
Ato () used refer sets input output atoms , respectively.
notations provide us way access module interface left implicit, e.g.,
neglect internal structure modules. Lastly, set At() atoms, denote projections Ati (), Ato (), Atv (), Ath () Si , , Sv , Sh ,
respectively.
formal terms, DLP-function = hR, I, O, Hi designed provide mapping
subsets set subsets H analogy LP-functions formalized
Gelfond Gabaldon (1999). However, exact denition mapping deferred
Section 3 semantics DLP-functions anchored. sequel,
(syntactic) class DLP-functions denoted D. assumed, sake simplicity,
spans xed (at denumerable) signature At(D)8 At() At(D)
holds DLP-function D. Given DLP-functions 1 , 2 , 3 pairwise
respect input/output interfaces other, holds
1 2 (closure),
1 = 1 = 1 , empty DLP-function = h, , , (identity),
1 2 = 2 1 (commutativity),
1 (2 3 ) = (1 2 ) 3 (associativity).
theory modules put forth Oikarinen Janhunen (2008a) based
restrictive operator program composition, viz. join . idea behind operator
forbid positive dependencies programs explicated next.
Technically speaking, dene positive dependency graph DG+ () DLP-function
= hR, I, O, Hi using positive dependenciesfollowing denition Ben-Eliyahu
Dechter (1994). However, exclude input atoms graph denitions
external anyway. Thus, let DG+ () = hO H, 1 b 1 holds
pair atoms a, b H rule B, C R
b B. reexive transitive closure 1 gives rise dependency relation
Ato () Ath (). strongly connected component (SCC) graph DG+ ()
maximal set Ato () Ath () b every pair a, b atoms. Given
1 2 dened, say 1 2 mutually dependent DG+ (1 2 )
SCC Ato (1 ) 6= Ato (2 ) 6= (Oikarinen & Janhunen,
2008a), i.e., component shared DLP-functions 1 2 way. 1
2 mutually dependent, also call mutually independent.
Definition 2.6 (Joins) Given two DLP-functions 1 2 , composition 1 2
defined 1 2 mutually independent, join, 1 2 , 1 2
defined coincides 1 2 .
7. Consequently, length symbols, denoted kk, gives upper bound |At()|
important one considers computational cost translating programs (Janhunen, 2006).
8. practice, set could set identifiers (names propositions similar objects).

821

fiJanhunen, Oikarinen, Tompits & Woltran

case 1 2 dened, thus 1 2 mutually independent, exactly
one following conditions holds SCC DG+ (1 2 ):
Ato (1 ) Ath (1 );

(5)

Ato (2 ) Ath (2 ).

(6)

Example 2.7 Recall programs 1 2 Example 2.5 obtain
positive dependency graph DG+ (1 2 ) = h{a, b, d, e}, {ha, di, ha, ei}i. Hence, SCCs
graph simply singletons {a}, {b}, {d}, {e}. Together observation
Ato (1 ) Ato (2 ) disjoint, derive 1 2 mutually dependent.
Thus, join 1 2 = 1 2 defined since composition 1 2 defined
basis analysis performed Example 2.5.
Example 2.8 example two DLP-functions composition defined
yet ineligible join, consider following situation:
{b}
b c;
b a, c
{a, c}



{a}
b c;
b, c
{b, c}

=

{a, b}
b c;
b, c;
b a, c
{c}

Here, result composition involves SCC = {a, b} respective positive dependency graph, non-empty intersection output signatures programs
subject composition. Hence, respective join modules question defined.

3. Model Theory Stable-Model Semantics
syntax DLP-functions dened, turn semantics. proceed
three steps introduce, correspondingly, three kinds models, viz. classical models,
minimal models, and, nally, stable models DLP-function. last provide
intended semantics DLP-function whereas rst two serve auxiliary concepts.
usual, interpretation DLP-function dened arbitrary subset
At(). Given particular interpretation At(), atom At() true ,
denoted |= a, , otherwise false , denoted 6|= a. negative
literal a, dene |= 6|= a. set L literals satisfied , denotedWby
|= L, |= l, everyWliteral l L. also dene disjunctive interpretation L
set L literals: |= L |= l literal l L.
begin with, cover DLP-functions pure classical semantics, treats
disjunctive rules classical implications. emphasized classical models
DLP-function specic interpretations dened hence subsets At().
Definition 3.1 interpretation At() (classical) model DLP-function
= hR, I, O, Hi, denoted |= , iff |= R, i.e., every rule B, C R,
W
|= B C implies |= A.
822

fiModularity Aspects Disjunctive Stable Models

set classical models denoted CM().
Classical models provide appropriate level abstraction address role input
atoms DLP-functions. Given DLP-function interpretation At(),
projection Mi viewed actual input may (or may not) produce
respective output Mo , depending semantics assigned . treatment input
atoms sequel based partial evaluation: idea pre-interpret input
atoms appearing respect Mi .
Definition 3.2 DLP-function = hR, I, O, Hi actual input Mi ,
instantiation respect Mi , denoted /Mi , quadruple hR , , O, Hi
R contains reduced rule
(A \ I) (B \ I), (C \ I)

(7)

rule B, C R Mi |= Ai Bi Ci .
Example 3.3 Consider following DLP-function :
{a, b}
b c;
c, b;
b c,
{c}
actual input {c} Ati (), reduct /{c} DLP-function
h{a b; b a}, , {a, b}, i.
hand, actual input Ati (), obtain reduct
/ = h{a b}, , {a, b}, i.
rules form (7) free input atoms indicates reduct /Mi
DLP-function without input. Atoms Ato () Ath () aected /Mi .
Proposition 3.4 Let DLP-function At() interpretation defines
actual input Mi Ati () . interpretations N At() Ni = Mi ,
N |= Nh |= /Mi .
Proof. Consider N At() Ni = Mi .
(=) Suppose N |= . Assume Nh satisfy (7) rule
B, C . follows Mi |= Ai Bi Ci , therefore Ni |= Ai Bi Ci .
Thus, N 6|= B, C, contradiction. follows Nh |= /Mi .
(=) Let Nh |=W/Mi hold. Assuming N 6|= B, C rule implies
N |= B C N 6|= A. follows Ni |= Ai Bi Ci corresponding
rule (7) included /Mi Ni = Mi . rule satised Nh since
823

fiJanhunen, Oikarinen, Tompits & Woltran

W
N 6|= B, C implies Nh |= (B \ I) (C \ I) Nh 6|= (A \ I),
contradiction. Hence, N |= .

Thus, input reduction, given Denition 3.2, fully compatible classical
semantics characterize semantic operator CM also terms equation
[
{Mi N | N CM(/Mi )}.
(8)
CM() =
Mi Ati ()

Recall models DLP-function subsets At(). Hence,
N CM(/Mi ) subset At(/Mi ) thus Mi N = Mi Ati ()
since atom Ati () occurs /Mi denition.
Handling input atoms slightly complicated case minimal models
primitives parallel circumscription (Lifschitz, 1985; McCarthy, 1986) provide us
straightforward way address them. rough idea keep interpretation input
atoms fixed minimizing (i.e., falsifying) others far possible.
Definition 3.5 Let = hR, I, O, Hi DLP-function. model At()
I-minimal iff model N Ni = Mi N .
sequel, set I-minimal models = hR, I, O, Hi denoted MM()
treat input atoms stipulating I-minimality models. Using idea, Proposition 3.4
lifts minimal models given fact Ati (/Mi ) = .
Proposition 3.6 Let DLP-function At() interpretation defines
actual input Mi Ati () . interpretations N At() Ni = Mi ,
N MM() Nh MM(/Mi ).
Proof. Consider N At() Ni = Mi .
(=) Let N MM(). follows Proposition 3.4 Nh |= /Mi . Assume
Nh
/ MM(/Mi ). Recall Ati (/Mi ) = . Thus, interpretation
Nh |= /Mi . follows Proposition 3.4 N |=
interpretation N = Mi S. Ni = Ni N N jointly contradict N MM().
(=) Suppose Nh MM(/Mi ). So, Nh |= /Mi , N |= follows
Proposition 3.4. Let us assume N 6 MM(), i.e., model N |=
Ni = Ni N N . Thus, (No Nh ) (No Nh ), since Ni = Ni = Mi
follows Nh |= /Mi Proposition 3.4. Then, however, Nh |= /Mi
contradiction Nh MM(/Mi ).

set MM() Ati ()-minimal models sucient determine semantics
positive DLP-function , i.e., whose rules form B. Recall
rules \ Ati () 6= holds whenever 6= . order cover arbitrary DLP-functions,
interpret negative body literals way proposed Gelfond Lifschitz (1991).
Definition 3.7 Given DLP-function = hR, I, O, Hi interpretation At(),
reduct respect positive DLP-function = hRM , I, O, Hi
RM = {A B | B, C R |= C}.
824

(9)

fiModularity Aspects Disjunctive Stable Models

Definition 3.8 interpretation At() stable model DLP-function
input signature Ati () iff MM(M ), i.e., Ati ()-minimal model .
Hidden atoms play special role Denition 3.8. contrast this, aect
possibilities program decomposition, presented Section 6, status
nally explicated notion modular equivalence introduced Section 8.
Denition 3.8 covers also case ordinary disjunctive logic program, simply
DLP-function = hR, , O, i: model At() = stable
minimal model RM . denition stable models gives rise semantic operator
At(D)
SM : 22
DLP-functions:
SM() = {M At() | MM(M )}.

(10)

Proposition 3.6 provides us way dismiss Ati ()-minimality denition stable
models desirable. Given stable model , projection N = Mo Mh minimal
model (/Mi )N hence stable model /Mi . words,
(/Mi )M = (/Mi )Mo Mh = /Mi .
Thus, derive following result:
Corollary 3.9 DLP-function ,
SM() = {M At() | Mo Mh SM(/Mi )}.
Example 3.10 Recall DLP-function Example 3.3, hidden atoms,
given follows:
{a, b}
b c;
c, b;
b c,
{c}
four stable models total: M1 = {a}, M2 = {b}, M3 = {a, c}, M4 = {b, c},
{c}-minimal models respective reducts :
M1
M2
M3
M4

= h{a b ; c}, {c}, {a, b}, i,
= h{a b ; b c}, {c}, {a, b}, i,
= h{a c}, {c}, {a, b}, i,
= h{b c}, {c}, {a, b}, i.

Now, easy verify Mj {c}-minimal model reduct Mj .
illustrating Corollary 3.9, recall reducts
/{c} = h{a b; b a}, , {a, b},
/ = h{a b}, , {a, b}, i.
Then, SM(/{c}) = {{a}, {b}} SM(/) = {{a}, {b}}.
825

fiJanhunen, Oikarinen, Tompits & Woltran

immediate observation loose general antichain property stable
models input signatures introduced. instance, M1 M3 M2 M4
Example 3.10. However, since interpretation input atoms xed semantics,
perceive antichains locally, i.e., set {N SM() | Ni = Mi } stable models forms
antichain, input Mi Ati (). Example 3.10, sets stable models associated
actual inputs {c} {M1 , M2 } {M3 , M4 }, respectively.

4. Characterizations using Classical Logic
well known set stable models ordinary disjunctive logic program, i.e.,
DLP-function form hR, , O, i, characterized via classical propositional logic,
using concepts completion (Clark, 1978) loop formulas (Lin & Zhao, 2004; Lee &
Lifschitz, 2003). section, generalize concepts arbitrary DLP-functions.
end, main concern role input atoms incorporate
concepts. Furthermore, extend tightness property programs (Erdem & Lifschitz,
2003) DLP-functions introducing notion I-tightness Section 4.2.
4.1 Program Completion Loop Formulas
Given DLP-function , loop non-empty subset strongly connected
component positive dependency graph DG+ (). Recall DG+ ()
atoms Ato () Ath () nodes. particular, singleton {a} Ato ()
Ath () thus loop.
Example 4.1 Consider DLP-functions 1 2 defined follows:

1 :

{b, c}
c b;
ba
{a}

2 :

{a, b}
c b;
ba
{c}

Here, 1 singleton loops {b} {c}. particular, {a, b} loop
contains input atom a. hand, 2 loops {a}, {b}, {a, b}.
follows,
use,Wfor set propositional
W formulas (or atoms), denote
V
conjunction sS shorthand sS s. Moreover, appearing within
formula, set implicitly understood conjunction elements. DLP-function
atom Ato () Ath (), dene set supporting formulas
SuppF(a, ) = {B C (A \ {a}) | B, C A}
loop L Ato () Ath () , set externally supporting formulas
ESuppF(L, ) = {B C (A \ L) | B, C , L 6= , B L = }.
Clarks completion procedure (conjunctive) loop formulas generalized DLPfunctions following way:
Definition 4.2 DLP-function , completion set formulas
826

fiModularity Aspects Disjunctive Stable Models

W
Comp() = {B C
W | B, C }
{a SuppF(a, ) | Ato () Ath ()}

set loop formulas
W
LF() = {L ESuppF(L, ) | L Ato () Ath () loop }.9

Observe case Ati () = , i.e., Ato () Ath () = At(), completion
Comp() reduces denition provided Lee Lifschitz (2003) holds
set LF() loop formulas. Generally speaking, propositional theories Comp()
LF() characterize set SM() stable models following sense:

Theorem 4.3 DLP-function interpretation At(),
SM() |= Comp() |= LF().
Proof. rst relate sets SuppF(a, ) ESuppF(L, ), introduced
DLP-function , respective sets complementary rules
SuppCR(a, ) = {A \ {a} B, C | B, C A}
ESuppCR(L, ) = {A \ L B, C | B, C , L 6= , B L = }.
First, straightforward that, interpretation At(), |= Comp()
jointly |= (Ato () Ath ()), 6|= SuppCR(a, ). Quite
similarly, holds |= LF() i, loop L (Ato () Ath ()) ,
6|= ESuppCR(L, ). hand, viewing SuppCR(a, ) ESuppCR(L, )
DLP-functions signatures , apply Proposition 3.4 order
evaluate input atoms. Thus, obtain following relationships DLP-function
, interpretation At(), atom Ato () Ath (), loop L Ato () Ath ()
:
1. |= Mo Mh |= /Mi ,
2. |= SuppCR(a, ) Mo Mh |= SuppCR(a, /Mi ),
3. |= ESuppCR(L, ) Mo Mh |= ESuppCR(L, /Mi ).
Finally, recall interpretation At(), Ato () = Ato (/Mi )
Ath () = Ath (/Mi ). Inspecting denition Comp() LF() again,
conclude interpretation At() |= Comp() LF() Mo Mh |=
Comp(/Mi ) LF(/Mi ). turn, know Mo Mh |= Comp(/Mi ) LF(/Mi )
Mo Mh stable model program /Mi results Lee Lifschitz (2003);
recall /Mi ordinary disjunctive program without input atoms. Finally,
SM() = {M At() | Mo Mh SM(/Mi )} Corollary 3.9. equality shows
claim.

Example 4.4 Let us demonstrate functioning program completion loop formulas DLP-functions Example 4.1, i.e., 1 = hR, {a}, {b, c}, 2 =
hR, {c}, {a, b}, i, R = {a c b; b a}. completions
9. Although may seem case singleton loop L = {a} somewhat redundant, so,
since tautological rules b make difference.

827

fiJanhunen, Oikarinen, Tompits & Woltran

Comp(1 ) = {b c, b} {b a, c b a}
Comp(2 ) = {b c, b} {b a, b c}.
Furthermore, sets loop formulas
W
W
LF(1 ) = {b ESuppF({b}, 1 ), c ESuppF({c}, 1 )}
= {b a,
W c b a}
W
LF(2 ) = {b ESuppF({b},
2 ), ESuppF({a}, 2 ),
W
b ESuppF({a, b}, 2 )}
= {b a, b c, b }.
last formula, W
occurrence view ESuppF({a, b}, 2 ) = ,
yields empty disjunction ESuppF({a, b}, 2 ) = usual.
Computing classical models Comp(1 ) LF(1 ) = Comp(1 ) yields two
models, M1 = {a, b} M2 = . One check indeed stable models
1 recalling Ati (1 ) = {a}. Thus, M1 relates actual input M1 Ati (1 ) = {a}
whereas M2 based M2 Ati (1 ) = . hand, classical models
Comp(2 ) LF(2 ) M1 = {c} M2 = , relate two possible
inputs Ati (2 ) = {c}. Finally, note {a, b} also model Comp(2 )
ruled LF(2 ).
4.2 Tight DLP-functions
extend well-known concept tightness (Erdem & Lifschitz, 2003) DLPfunctions. interest since exploit fact positive dependency
graph DG+ () reduced modulo input atoms. words, since dependency graph
DG+ () atoms Ato () Ath () nodes, tightness DLP-functions
dened respect input signature.
beginning Section 4, loops dened arbitrary non-empty subsets
strongly connected components DG+ (). Thus, DG+ () acyclic
singleton loops. However, converse necessarily true, since, program
singleton loops, DG+ () may edges ha, ai, i.e., cycles length one.
Definition 4.5 DLP-function Ati ()-tight (or tight, short), positive dependency graph DG+ () acyclic.
Example 4.6 Recall DLP-functions 1 = hR, {a}, {b, c}, 2 = hR, {c}, {a, b},
based R = {a c b; b a} Example 4.1. Here, 1 {a}-tight since potential
non-singleton loop {a, b} contains input atom a. hand, 2 {c}-tight.
worth mentioning ordinary variant 1 , viz. DLP-function hR, , {a, b, c}, i,
-tightin particular, since R tight usual sense.
note last observation, viz. DLP-function hR, I, O, Hi may I-tight
although R tight program, relies use disjunctions program. fact,
DLP-functions hR, I, O, Hi, R set normal rules form B, C,
DLP-function = hR, I, O, Hi I-tight R tight. verify this, note
second item Denition 2.1 implies head atom normal rule B, C
must appear I, thus loop may involve atoms I.
828

fiModularity Aspects Disjunctive Stable Models

show notion tightness introduced Denition 4.5 enables us
characterize stable models DLP-function classical models completion.
Since ordinary program represented DLP-function, thus properly
generalize well-known completion semantics (Clark, 1978). following lemma
already sucient result view Denition 4.2 Theorem 4.3.
Lemma 4.7 tight DLP-function , LF() Comp().
W
Proof. Recall Ato () Ath (), SuppF(a, ) contained
Comp(). Moreover,Wsince tight, singleton loops, thus LF() contains
formulas ESuppF({a}, ), Ato () Ath (). remains
show that, atom a, SuppF(a, ) equivalent ESuppF({a}, ) whenever
positive dependency graph DG+ () acyclic. repeat denition SuppF(a, )
give denition ESuppF(L, ), simplied case L = {a}:
SuppF(a, ) = {B C (A \ {a}) | B, C A};
ESuppF({a}, ) = {B C (A \ {a}) | B, C , A, B {a} = }.
easy see acyclic dependency graph DG+ (), implies B{a} =
every rule B, C . Thus, conclude SuppF(a, ) = ESuppF({a}, )
holds Ato () Ath (). Hence, claim follows.

Example 4.8 Recalling DLP-function 1 = hR, {a}, {b, c}, Example 4.4
R = {a c b; b a}, obtain
Comp(1 ) = {b
W c, b} {b a, cW b a}
LF(1 ) = {b ESuppF({b}, 1 ), c ESuppF({c}, 1 )}
= {b a, c b a}.
Now, 1 tight observe LF(1 ) Comp(1 ) expected.
observations presented far lead us following result:
Theorem 4.9 tight DLP-function interpretation At(),
SM() |= Comp().
particular, result compatible existing characterization stable models
case Ati () = , i.e., Ato () Ath () = At(). Then, notion Ati ()tightness coincides ordinary tightness, denition completion Comp()
reduces one provided Lee Lifschitz (2003).

5. Compositional Semantics
follows, objective establish main result paper, i.e., show
stable-model semantics, given Denition 3.8, fully compositional larger
DLP-functions formed joins 1 . . . n DLP-functions. precisely,
interconnection SM() SM(1 ), . . . , SM(n ) explicated Section 5.1. analogy
829

fiJanhunen, Oikarinen, Tompits & Woltran

Section 3, follow quite rigorous approach consider relationship classical
models rst, minimal models, eventually cover case stable models
comprises module theorem. Then, Section 5.2, use quantied Boolean formulas
second level polynomial hierarchy modular representation terms
DLP-functions illustrate module theorem. Finally, devote Section 5.3
comparison splitting set theorem proven Lifschitz Turner (1994).
5.1 Module Theorem
begin with, formalize criteria combining interpretations well models.
Definition 5.1 Given two DLP-functions 1 2 , interpretations M1 At(1 )
M2 At(2 ) mutually compatible (with respect 1 2 ), compatible,
M1 Atv (2 ) = M2 Atv (1 ).

(11)

According (11), two compatible interpretations M1 M2 1 2 , respectively, agree truth values joint visible atoms Atv (1 ) Atv (2 ).
quick inspection Figure 1 reveals three cases may arise join = 1 2
dened joint output atoms 1 2 thereafter disallowed: may exist
1. joint input atoms Ati () = Ati (1 ) Ati (2 ),
2. atoms Ato (1 ) Ati (2 ) output atoms 1 input atoms 2 ,
3. symmetry, atoms Ati (1 ) Ato (2 ).
Recall according Denition 2.6, atoms last two categories end Ato ()
= 1 2 formed. Atoms Atv (1 ) Atv (2 ) provide basis combine
compatible interpretations 1 2 .
Definition 5.2 Let 1 2 two DLP-functions = 1 2 defined.
Given sets interpretations A1 2At(1 ) A2 2At(2 ) , natural join A1
A2 respect Atv (1 ) Atv (2 ), denoted A1
A2 , set interpretations
A1
A2 = {M1 M2 | M1 A1 , M2 A2 , M1 M2 compatible}.

(12)

rst modularity result formulated DLP-functions classical semantics
dened Section 3. combination classical models understood (12).
Proposition 5.3 positive DLP-functions 1 2 1 2 defined,
CM(1 2 ) = CM(1 )
CM(2 ).

(13)

Proof. Consider interpretation At(1 2 ) projections M1 = At(1 )
M2 = At(2 ) respect 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 i.
follows M1 M2 compatible = M1 M2
830

fiModularity Aspects Disjunctive Stable Models

CM(1 2 )




|= R1 R2
M1 |= R1 M2 |= R2
M1 CM(1 ) M2 CM(2 )
CM(1 )
CM(2 ).



Generalizing Proposition 5.3 stable models DLP-functions much elaborate.
cover case positive DLP-functions minimal models rst. proof
Theorem 5.5 exploits program completion, loop formulas, well characterization
stable minimal models Section 4 follows:
Lemma 5.4 DLP-functions 1 2 1 2 defined, following
conditions hold:
Comp(1 2 ) = Comp(1 ) Comp(2 );
LF(1 2 ) = LF(1 ) LF(2 ).

(14)
(15)

Proof. begin proof analyzing formulas introduced Clarks completion
loop formulas related joins DLP-functions. end, establish
sets formulas associated 1 2 directly obtained unions sets
formulas associatedWwith 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 i: First,
implication B C belongs Comp(1 2 ) belongs Comp(1 ),
Comp(2 ), case shared rule. Second, let us consider atom H,
= O1 O2 H = H1 H2 disjoint 1 2 dened.
reason, either O1 H1 O2 H2 , i.e., atom dened either 1 2 . Thus,
either Def R1 (a) = Def R1 R2 (a) Def R2 (a) = Def R1 R2 (a) Denition 2.2,
implies either SuppF(a, 1 2 )W= SuppF(a, 1 ) SuppF(a, 1 2 ) = SuppF(a, 2 ).
follows implication
1 2 )
W SuppF(a, 1 2 ) member Comp(
W
either (i) SuppF(a, 1 ) belongs Comp(1 ) (ii) SuppF(a, 2 )
belongs Comp(2 ). Thus, may conclude (14) completions involved.
Third, recall loop L At(1 2 ) 1 2 contained SCC
1 2 . follows (5), (6), Denition 2.2 either
1. L O1 H1 loop 1 Def R1 (L) = Def R1 R2 (L),
2. L O2 H2 loop 2 Def R2 (L) = Def R1 R2 (L).
cases above, either ESuppF(L, 1 2 ) = ESuppF(L,W
1 ) ESuppF(L, 1
2 ) = ESuppF(L, 2 ). Thus, respective loop formula L
ESuppF(L, 1 2 )
belongs LF(1 2 ) contained LF(1 ) LF(2 ).

Theorem 5.5 positive DLP-functions 1 2 1 2 defined,
MM(1 2 ) = MM(1 )
MM(2 ).

(16)

Proof. Consider At(1 2 ) respective projections M1 = At(1 )
M2 = At(2 ) compatible and, moreover, = M1 M2 . obtain
following chain equivalences:
831

fiJanhunen, Oikarinen, Tompits & Woltran

MM(1 2 )
|= Comp(1 2 ) |= LF(1 2 )
M1 |= Comp(1 ) M1 |= LF(1 )

M2 |= Comp(2 ) M2 |= LF(2 )
M1 MM(1 ) M2 MM(2 )
MM(1 )
MM(2 ).

[Theorem 4.3]
[(14) (15)]
[Theorem 4.3]
[Denition 5.2]


Example 5.6 Let us demonstrate result Theorem 5.5 practical setting using DLPfunctions 1 2 visualized composition = hR, , {a, b, c, d, e}, i.

1 :

{a, b, c}
b ;
b;
b a;
c;
c e a, b
{d, e}

2 :

{d, e}
c;
e d;
e;
c e a, b
{a, b, c}

join 1 2 defined SCCs composition S1 = {a, b, c}
S2 = {d, e}. Ati (1 )-minimal models 1 {a, b, c}, {a, b, d}, {a, b, e},
{a, b, d, e}. Likewise, calculating MM(2 ), get
MM(2 ) = {, {a}, {b}, {c, d, e}, {a, b, d, e}, {a, c, d, e}, {b, c, d, e}, {a, b, c, d, e}}.
Hence, minimal model = {a, b, d, e} compatibility condition
underlying (16) correctly excludes N = {a, b, c, d, e} 6 MM(). Note support
c true 1 e true. Accordingly, c e a, b active.
prepared present central result:
Theorem 5.7 (Module Theorem) DLP-functions 1 2 1 2
defined,
SM(1 2 ) = SM(1 )
SM(2 ).
(17)
Proof. Again, take interpretation At(1 2 ) respective compatible
projections M1 = At(1 ) M2 = At(2 ) consideration. proof (17)
based (16) number preliminary facts established:
M2
1
1. composition
dened.
1 2

Since 1 2 dened, know 1 2 dened. indicates 1
1

2 respect input/output interfaces other. construction
1
M2
M1
M2
2 aect property implies 1 2 dened.
M2
1
dened.
2. join
1 2
M2
1
preceding item, positive dependency graph DG+ (M
1 2 ) dened.
M1
M2
Let us assume 1 2 mutually dependent, i.e., SCC
M2
1
graph Ato (M
1 ) 6= Ato (2 ) 6= . Since
dependency graph potentially fewer dependencies respective graph

832

fiModularity Aspects Disjunctive Stable Models

DG+ (1 2 ) 1 2 , follows contained SCC latter.
M2

1
Since Ato (M
1 ) = Ato (1 ) Ato (2 ) = Ato (2 ), obtain Ato (1 ) 6=
Ato (2 ) 6= . Thus, 1 2 mutually dependent, contradiction.
M2
1
3. reduct (1 2 )M coincides
1 2 .

rule B belongs (1 2 )M rule B, C 1 ,
2 , C = . Equivalently, rule B, C 1
C M1 = , rule B, C 2 C M2 = , i.e.,
1
2
B
B
1
2 .
therefore get following chain equivalences:
SM(1 2 )






MM((1 2 )M )
M2
1
MM(M
1 2 )
1
2
MM(M
MM(M
1 )
2 )
M2
1
M1 MM(M
1 ) M2 MM(2 )
M1 SM(1 ) M2 SM(2 )
SM(1 )
SM(2 ).

[Denition 3.8]
[Item 3 above]
[Theorem 5.5]
[Denition 5.2]
[Denition 3.8]
[Denition 5.2]



moral Theorem 5.7 Denition 2.6 stable semantics supports modularization long positively interdependent atoms enforced module.
Example 5.8 Let 1 2 DLP-functions defined = 1 2
join (which clearly defined):
{b}
b ;
bc
{a, c}



{c}
c ;
bc
{a, b}

=

{b, c}
b ;
c ;
bc
{a}

straightforward verify SM(1 ) = {{b}, {a, b}, {a, c}, {b, c}} SM(2 ) =
{{c}, {a, b}, {a, c}, {b, c}}. Since Atv (1 ) Atv (2 ) = {a, b, c}, obtain
SM(1 )
SM(2 ) = SM(1 ) SM(2 ) = {{a, b}, {a, c}, {b, c}}.
simple cross-check confirms SM() indeed given set.
Example 5.9 Consider DLP-functions 1 2 Example 2.8. Then, SM(1 ) =
{, {a, b}, {b, c}} SM(2 ) = {, {a, b}, {a, c}}. shown Example 2.8, join 1
2 undefined. Thus, Theorem 5.7 applicable. Concerning composition
1 2 , note SM(1 2 ) = {, {a, c}, {b, c}} =
6 {, {a, b}} = SM(1 )
SM(2 ).
Theorem 5.7 easily extended DLP-functions consisting two
modules. view this, say nite sequence M1 , . . . , Mn stable models
modules 1 , . . . , n , respectively, compatible, Mi Mj pairwise compatible,
1
i, j n. property guarantees Mi recovered union
= ni=1 Mi taking respective projection At(i ) = Mi .
833

fiJanhunen, Oikarinen, Tompits & Woltran

Corollary 5.10 Let 1 , . . . , n sequence DLP-functions join 1
n defined. Then,
SM(1 n ) = SM(1 )

SM(n ).

(18)

Example 5.11 following example simply extends Example 5.8:
{b}
b ;
bc
{a, c}



{c}
c ;
bc
{a, b}

{a}
b ;
ac
{b, c}



=

{a, b, c}
b ;
c ;
bc


SM(1 ) = {{b}, {a, b}, {a, c}, {b, c}}, SM(2 ) = {{c}, {a, b}, {a, c}, {b, c}},
SM(3 ) = {{a}, {a, b}, {a, c}, {b, c}}. Thus, learn Corollary 5.10
SM(1 2 3 ) = SM(1 )
SM(2 )
SM(3 ) = {{a, b}, {a, c}, {b, c}}.
5.2 Modular Representation Quantified Boolean Formulas
next objective illustrate theory developed far terms extensive
unsat depicted
example. end, consider pair DLP-functions sat
n n
Figure 2. purpose evaluation quantified Boolean formulas (QBFs)
form
n
_
XY
(Ai Bi Ci Di ),
(19)
i=1

Aj , Bj , Cj , Dj set Boolean variables, parameter n gives
number disjuncts matrix Boolean formula inSdisjunctive normal
form (DNF).10 Without loss generality, may assume X = ni=1 (Ai Bi ), =

n
i=1 (Ci Di ), X = hold sets X Boolean variables (19).
important point general evaluation QBFs form (19) constitutes p2 -complete decision problem perfectly matches complexity checking
existence stable models disjunctive program. Given completeness property,
follows principle decision problem p2 turned QBF form
(19), albeit direct representations obtained particular problem domains.
respect, let us address three specic domains prior detailing generic approach.
1. strategic companies domain identied Leone et al. (2006) one rst
practical domains involving decision problems second level polynomialtime hierarchy solved using ASP techniques. simplied encoding provided
Koch, Leone, Pfeifer (2003) based two kinds disjunctive rules:
strat(x1 ) strat(x2 ) strat(x3 ) strat(x4 ) prod(y, x1 , x2 , x3 , x4 ),

(20)

strat(x) ctrl(x, x1 , x2 , x3 , x4 ), strat(x1 ), strat(x2 ), strat(x3 ), strat(x4 ),

(21)

10. Also, recall shorthands =

V

sS

=

V

834

sS

introduced right Example 4.1.

fiModularity Aspects Disjunctive Stable Models

Function sat
n :

Function unsat
:
n

X
1 n x Ai : x, act(i);
1 n x Bi : x act(i);
1 n: Ai Bi , act(i)
{act(1), . . . , act(n)}

Ci {u} Di , act(i);
u;
u u
{act(1), . . . , act(n)}

1 n:
:

unsat
Figure 2: DLP-functions sat
n n Wfor evaluation quantied Boolean formula
XY matrix = ni=1 (Ai Bi Ci Di ).

predicates strat(x), prod(y, x1 , x2 , x3 , x4 ), ctrl(x, x1 , x2 , x3 , x4 ), respectively,
denote company x strategic, product produced companies x1 , . . . , x4 ,
company x controlled companies x1 , . . . , x4 . Obviously, instances
predicate strat arising rules forms (20) (21) create positive dependencies program . resulting SCCs used split program
modules 1 , . . . , n = 1 . . . n dened. Theorem 5.7, status
specic company x decided using module denes strat(x)
rather entire encoding .
2. model-based diagnosis digital circuitry provides another interesting application
area. Quite recently, Oikarinen Janhunen (2008b) presented ecient encoding
prioritized circumscription disjunctive program (and thus, special case,
parallel circumscription well)enabling concise representation minimal diagnoses sense Reiter (1987). resulting disjunctive rules involve head-cycles
(see Section 7 details) typically pre-empt polynomial-time translation
computationally easier normal logic program. observation suggests completeness second level polynomial-time hierarchy although aware
exact hardness result. correctness proof encoding exploits two modules
module theorem.
3. Finally, let us mention Gebser, Schaub, Thiele, Usadel, Veber (2008b) identify
minimal inconsistent cores large biological networks disjunctive programs.
decision problem question Dp -complete also indicates appropriateness
disjunctive logic programs representation domain. Since Dp complete decision problem described independent combination NPcomplete decision problem P1 coNP-complete decision problem P2 , foresee
representation form join sat unsat , sat stable model P1
succinct certicate, unsat unique stable model P2 succinct
835

fiJanhunen, Oikarinen, Tompits & Woltran

{x1 , x2 }
x1 act(1);
x1 , act(1);
x2 , act(2); x2 act(2);
x1 act(3);
x1 , act(3);
x1 act(4);
x2 , act(4);
x2 x1 , act(4)
{act(1), act(2), act(3), act(4)}


u y1 , y2 , act(1);
u y2 y1 , act(2);
u y1 y2 , act(3);
u y1 y2 act(4);
y1 u; y2 u; u u
{act(1), act(2), act(3), act(4)}

unsat .
Figure 3: Particular instances sat
4 4

certicates. required DLPs worked via reductions propositional
(un)satisability. particular, test unsatisability realized analogy
unsat
analyzed below.
n
general case, use Boolean variables propositional atoms interchangeably
order describe validity problem (19) captured DLP-functions
unsat based explanatory approach Janhunen
Figure 2. design sat
n n
et al. (2006), (19) equivalently viewed formula XY matrix
conjunctive normal form (CNF). clause 11 Ai Bi Ci Di active
whenever Ai Bi false truth clause becomes dependent Ci Di ;
put dually, Ai Bi true truth Ai Bi Ci Di depends Ci Di .
validity formula XY captured follows: Given input interpretation
Mi {act(1), . . . , act(n)}, upper DLP-function sat
n Figure 2 tries explain
activation statuses clauses checking respective theory {Ai Bi |
act(i) Mi } {Ai Bi | act(i) 6 Mi } satisable. lower DLP-function, unsat
, plays
n
role coNP-oracle: captures test theory {Ci Di | act(i) Mi }
unsatisable. correctness representation provided DLP-functions
addressed soon, enough understand syntax intuitive meaning
moment. concrete QBF instance evaluated follows.
unsat Figure 2 case QBF
Example 5.12 Consider DLP-functions sat
n n

x1 x2 y1 y2 [(x1 y1 y2 )(x2 y1 y2 )(x1 y1 y2 )(x1 x2 y1 y2 )]. (22)
Thus, parameter instance n = 4, input signature {act(1), . . . , act(4)}
unsat , illustrated Figure 3. output signature former DLPfor sat
4 4
function {x1 , x2 } atoms, i.e., y1 , y2 , u, remain hidden latter.
joint input signature used specify active part matrix (22). DLPfunction sat
provides explanation, i.e., assignment variables x1 x2
4
output, whereas unsat
responsible respective unsatisfiability check. regards
4
validity QBF given (22), input interpretation {act(1), act(2), act(3), act(4)}
yields positive answer. respective explanation, i.e., output interpretation found
sat
4 , {x1 }. easy check x1 true x2 false remainder
matrix true whatever values assigned y1 y2 . Hence, QBF (22) valid.
11. purposes section, interpret disjunctions B sets B = {b | b B}
positive negative literals, respectively, disjunctions elements.

836

fiModularity Aspects Disjunctive Stable Models

unsat Figure 2, identical
regards general DLP-functions sat
n n
sat
input signatures, n output atoms, hidden atoms unsat
fully
n
sat
unsat
respected. Hence, composition n n
dened. Moreover, atoms appearing
rules involve positive dependencies belong disjoint sets X {u}.
unsat ) cannot SCC X 6=
therefore clear DG+ (sat
n n
unsat dened regardless QBF (19)
(Y {u}) 6= . implies sat
n n
question. Let us exploit fact context specic DLP-functions Example 5.12.

Example 5.13 four stable models DLP-function sat
4 :
{act(1), act(2), act(3), act(4), x1 }, {act(1), act(3), x1 , x2 }, {act(2)}, {x2 },
listed decreasing level activation. hand, DLP-function unsat

4
unique stable model {act(1), act(2), act(3), act(4), y1 , y2 , u}, i.e., interpretation {y1 , y2 , u}
/{act(1), act(2), act(3), act(4)} set rules
unique stable model unsat
4
given
{ u y1 , y2 ; u y2 y1 ; u y1 y2 ; u y1 y2 ; y1 u; y2 u; u u },
/Mi stable models input interpretation Mi . Moreover,
unsat
4
unsat ) combining compatible pairs
may apply module theorem calculate SM(sat
4 4
models. one pair:
{act(1), act(2), act(3), act(4), x1 } SM(sat
4 )
{act(1), act(2), act(3), act(4), y1 , y2 , u} SM(unsat
).
4
Thus, {act(1), act(2), act(3), act(4), x1 , y1 , y2 , u} unique stable model join sat
4
sat unsat ) non-empty, conclude (22) indeed valid.
unsat
.
Since
SM(
4
4
4
natural ask stated stable models general DLPfunctions unsat
sat
n
n associated QBF XY given (19). stable
sat
model n , respective projection MX = X determines , i.e., holds
1 n matrix act(i) MX |= Ai Bi . Moreover, model
MX minimal sense strictly smaller interpretation N MX
property. additional feature brought along minimality stable models.
consequence, DLP-function sat
n capture possible truth assignments
variables X relevant truth assignments lost. hand, stable
indicates respective theory
model unsat
n
{Ci Di | 1 n, act(i) }
W
inconsistent, alternatively, formula 1in,act(i)M Ci Di valid.
Concerning correctness representation given Figure 2, due existing
proof Janhunen et al. (2006), present main stepsfully exploiting benets
modular approach.
unsat ) non-empty.
Theorem 5.14 QBF XY form (19) valid iff SM(sat
n n

Proof sketch. Consider QBF XY form (19). following equivalent:
837

fiJanhunen, Oikarinen, Tompits & Woltran

1. formula XY valid.
2. minimal interpretation N X that, set = {1 n |
N 6|= Ai Bi } indices determined N N |= {Ai Bi | I}{Ai Bi |
6 I}, theory {Ci Di | I} unsatisable.
unsat compatible stable models = N {act(i) |
3. DLP-functions sat
1
n n
I} M2 = {act(i) | I} {u}, respectively.
unsat stable model
4. DLP-function sat
n n

= M1 M2 = N {act(i) | I} {u}.
second item, minimality N means N N {1 n |
N 6|= Ai Bi } = I. assumed without loss generality.

Theorem 5.14 module theorem suggest approximation strategy verifying
unsat ) empty, know
validity QBFs form (19). either SM(sat
n ) SM(n
unsat ) = .
directly formula valid. Otherwise, check whether SM(sat
n n
5.3 Splitting Sets
sake comparison, formulate splitting-set theorem (Lifschitz & Turner,
1994) DLP-function = hR, , O, i, essentially forms ordinary disjunctive
program. Splitting sets sets atoms closed following sense:
Definition 5.15 Given DLP-function = hR, , O, i, set U atoms splitting set if, every rule B, C R,
U 6= implies B C U .
Denitions 2.1 5.15, sets always splitting sets . However,
one mostly interested non-trivial splitting sets U , sets
need exist. Nevertheless, splitting set U divides respective set rules R two
parts. bottom, bU (R), R respect U contains rules B, C R
B C U , whereas top, tU (R), R R \ bU (R). splitting R
bU (R) tU (R) becomes proper one, i.e., bU (R) 6= tU (R) 6= ,
1. U non-trivial
2. every atom least one dening rule B, C R A.
According Lifschitz Turner (1994), solution R respect U pair
hX, X U , \U , X SM(bU (R)), SM(tU (R)/X). Here, tU (R)/X
denotes partial evaluation tU (R) sense Denition 3.2 using X U
input interpretation. Using similar idea, let us introduce DLP-functions corresponding
bU (R) tU (R). Given splitting set U , join = B ,
B = hbU (R), , U, = htU (R), U, \ U,
dened. Then, following result implied Theorem 5.7.
838

fiModularity Aspects Disjunctive Stable Models

Corollary 5.16 (Splitting-Set Theorem Lifschitz & Turner, 1994) every
DLP-function = hR, , O, corresponding set R disjunctive rules, every splitting
set U , every interpretation At() = O, following conditions
equivalent:
1. stable model .
2. U SM(B ) SM(T ).
3. hM U , \ U solution R respect U .
fact, Theorem 5.7 strictly stronger splitting-set theorem. previously
demonstrated Oikarinen Janhunen (2008a), splitting sets applicable DLPfunctions like = h{a b; b a}, , {a, b}, trivial way, i.e., U1 =
U2 = {a, b} splitting sets . contrast, Theorem 5.7 applies preceding
DLP-function versatile ways, i.e., 1 2 dened 1 = h{a b}, {b}, {a},
2 = h{b a}, {a}, {b}, i. consequence 1 2 dened, possible
determine sets stable models SM(1 ) = {{a}, {b}} = SM(2 ) separation,
appropriate, conclude SM() = SM(1 )
SM(2 ) = {{a}, {b}} holds
well. Yet another generality aspect splitting concerns role input atomsthey
assumed nonexistent above. Theorem 5.7, however, enables us treat well.

6. Decomposing DLP-Functions
objectives section contrary construction DLP-function join
modules. idea exploit strongly connected components DG+ (), DLPfunction , order decompose smaller components, e.g., priori
information internal structure . simplicity, rst consider DLPfunctions hidden atoms, i.e., Ath () = . eects hidden atoms
decomposition DLP-functions addressed thereafter. dened conjunction
Denition 2.6, SCCs DG+ () induced positive dependency relation
reexive transitive, i.e., preorder denition. sequel, set
SCCs DG+ () denoted SCC+ (). positive dependency relation lifts
elements SCC+ () follows: S1 S2 atoms a1 S1 a2 S2
a1 a2 . end, matter pair atoms inspected.
Lemma 6.1 DLP-function components S1 , S2 SCC+ (), S1 S2
a1 a2 every a1 S1 a2 S2 .
Proof. (=) S1 S2 , b1 S1 b2 S2 b1 b2 . Consider
a1 S1 a2 S2 . follows a1 b1 b2 a2 denition SCCs. Thus,
a1 a2 transitive.
(=) holds trivially SCCs non-empty.

Proposition 6.2 relation SCC+ () reflexive, transitive, antisymmetric.
839

fiJanhunen, Oikarinen, Tompits & Woltran

Proof. relation SCC+ () reexive transitive denition. antisymmetry, consider S1 , S2 SCC+ () S1 S2 S2 S1 . follows
Lemma 6.1 that, every a1 S1 a2 S2 , a1 a2 a2 a1 . Thus, S1 = S2
maximality components SCC+ ().

+
Consequently, may conclude hSCC (), partially ordered set. Since
nite denition, hSCC+ (), maxima minima elements need
unique. particular, SCC+ () minimum element S1 SCC+ ()
S1 S2 S1 implies S2 = S1 , S2 SCC+ (). Thus, may
apply principle well-founded induction using minima hSCC+ (), basis.
Given structure hSCC+ (), i, DLP-function = hR, I, O, decomposed following way: set rules associated SCC+ () Def R (S)
(2), i.e., set defining rules R. general, head arbitrary rule
B, C R may coincide sense (2) several SCCs, implies
rule included Def R (S) several SCC+ (). However, distribution
rules perfect harmony last two conditions Denition 2.2. must also
bear mind integrity constraints B, C included Def R (S)
SCC+ (). access integrity constraints set R rules, dene
IC(R) = {A B, C R | = }.

(23)

ready present decomposition based SCC+ ().
Definition 6.3 Given DLP-function = hR, I, O, i, decomposition induced
SCC+ () includes DLP-function
0 = hIC(R), At(IC(R)) (I \ At(R)), ,

(24)

and, SCC+ (), DLP-function
= hDef R (S), At(Def R (S)) \ S, S, i.

(25)

purpose extra module 0 keep track integrity constraints well
input atoms mentioned rules R. modules involved
decomposition induced SCCs. refers modules using
At(Def R (S)) \ input signature provides dening rules (if any) every
atom S. Recall output atom dening rules falsied default.
Proposition 6.4 DLP-function = hR, I, O, decomposition based
SCC+ (), join
F
(26)
0 ( SSCC+ () )
defined equal .

Proof. Let us consider 0 SCC+ (). composition 0
dened modules involve hidden atoms, Ato (0 ) = ,
Def R1 () = = Def R1 R2 () Def R2 (S) = Def R (S) = Def R1 R2 (S) sets rules
R1 = IC(R) R2 = Def R (S). join 0 dened respective composition
integrity constraints 0 create dependencies DG+ (0 ).
840

fiModularity Aspects Disjunctive Stable Models

Let us perform similar analysis S1 S2 based two dierent components
S1 , S2 SCC+ (). clear S1 S2 dened since modules involve
hidden atoms, S1 S2 = , Def R1 (S1 ) = Def R (S1 ) = Def R1 R2 (S1 )
Def R2 (S2 ) = Def R (S2 ) = Def R1 R2 (S2 ), R1 = Def R (S1 ) R2 = Def R (S2 ).
Since pairwise joins dened, also overall join (26) dened. Denition 2.4
denition SCC+ (), outcome equal

1. IC(R) SSCC+ () Def R (S) = R,

2. SSCC+ () = O,


3. (At(IC(R)) \ O) ((I \ At(R)) \ O) SSCC+ () (At(Def R (S)) \ O) = I.
Corollary 6.5 DLP-function Ath () = decomposition based
SCC+ (),
SM() = SM(0 )
( SSCC+ () SM(S )).




Example 6.6 Consider following DLP-function :
{a, b, c, d}
b c ;
a, c; b, c;
a, d; b, d;
b;
c d;
b a;
c.

So, Ati () = , Ato () = {a, b, c, d}, Ath () = . two SCCs DG+ (),
viz. S1 = {a, b} S2 = {c, d}. resulting decomposition consists
0 = h{ a, c; a, d; b, c; b, d}, {a, b, c, d}, , i,
S1 = h{a b c ; b; b a}, {c, d}, {a, b}, i,
S2 = h{a b c ; c d; c}, {a, b}, {c, d}, i.
respective sets stable models
SM(0 )
SM(S1 )
SM(S1 )
SM()

=
=
=
=

{{a, b}, {c, d}, {a}, {b}, {c}, {d}, },
{{a, b}, {c}, {d}, {c, d}},
{{c, d}, {a}, {b}, {a, b}},
{{a, b}, {c, d}}.

Next, address case DLP-functions involving hidden atoms, i.e.,
Ath () 6= holds. Then, components DG+ () subsets Ato () Ath ()
revise (25) accordingly. DLP-function = hR, I, O, Hi SCC+ (),
= hDef R (S), At(Def R (S)) \ S, O, Hi.

(27)

Unfortunately, decomposition based modules form (27) likely negrained. certain components S1 , S2 SCC+ () S1 6= S2 , respective
841

fiJanhunen, Oikarinen, Tompits & Woltran

modules S1 S2 conforming (27) might respect hidden atoms other.
similar setting may arise 0 individual module based SCC+ ()
integrity constraints refer hidden atoms . problem would disappear
hidden atoms revealed hardly appropriatethere good reasons
hide certain atoms knowledge representation perspective.
way approach problem distinguish components S1 SCC+ ()
S2 SCC+ () respective modules S1 S2 would respect hidden
atoms other, i.e., hidden atom dened one would referred othereither
positively negatively. Similar conicts could also arise due integrity constraints packed
module 0 distinguished Denition 6.3. rst sight, amalgamate 0
module whose hidden atoms occur integrity constraints 0 . But,
order avoid fusions kind far possible, worth redistributing integrity
constraints referring hidden atoms. clearly possible integrity constraints referring hidden atoms involved single component only. formalize ideas presented
far, distinguish precise relation among components SCC+ () follows.
Definition 6.7 Given DLP-function , components S1 , S2 SCC+ () respect
hidden atoms other, denoted S1 !h S2 , S1 6= S2
1. hidden atom h Ath (S1 ) h Ati (S2 ),
2. hidden atom h Ath (S2 ) h Ati (S1 ),
3. hidden atoms h1 Ath (S1 ) h2 Ath (S2 ) occurrence integrity constraint B, C .
clear !h irreexive symmetric components SCC+ ()
DLP-function . Moreover, transitive closure !h , denoted !+
h , gives rise
+
repartitioning SCC (). maximal block S1 , . . . , Sn components
Si !+
h Sj holds every 6= j induces module determined (27) union
= S1 . . . Sn . key observation modules associated dierent blocks
components respect hidden atoms makes Theorem 5.7 applicable
level abstraction. summarize treatment DLP-functions involving hidden
atoms rules, revise Denition 6.3 accordingly.
Definition 6.8 Given DLP-function = hR, I, O, Hi, decomposition induced
SCC+ () !+
h includes DLP-function
0 = hIC0 (R), At(IC0 (R)) (I \ At(R)), ,

(28)

IC0 (R) = { B, C R | (B C) H = } and, maximal block S1 , . . . , Sn
components SCC+ () Si !+
h Sj every 6= j, DLP-function
= hDef R (S) ICS (R), At(Def R (S) ICS (R)) \ S, O, Hi

(29)

= S1 . . . Sn ICS (R) = { B, C R | (B C) (S H) 6= }.
regards Example 6.6, Denitions 6.3 6.8 yield identical decompositions
DLP-function question. eects hiding demonstrated following example:
842

fiModularity Aspects Disjunctive Stable Models

Example 6.9 Consider DLP-function = hR, , O, Hi,
R = { a, c; b ; b c ; c d; c, b}
H = {a, b, c, d}, exact partitioning atoms H varies case
case analyzed below. SCCs SCC+ () S1 = {a}, S2 = {b}, S3 = {c, d}.
1. take atoms visible , i.e., H = , decomposition yields three modules, S1 = h{a b }, {b}, {a}, i, S2 = h{a b ; b c }, {a, c, d}, {b}, i,
S3 = h{b c ; c d; c, b}, {b}, {c, d}, i, addition module
0 = h{ a, c}, {a, c}, , encompassing integrity constraints.
2. hide H = {a} , obtain S1 !h S2 disjunctive rule b . Therefore, components S1 S2 must placed block also maximal
giving rise module = h{ a, c; b ; b c }, {c, d}, {b}, {a}i
= S1 S2 = {a, b}. modules 0 = S3 listed above.
3. Finally, set H = {a, c} , obtain S2 !h S3 b c S1 !h S3
a, c addition S1 !h S2 stated above. Since 0 = , decomposition
effectively collapses single module = = S1 S2 S3 .
note non-trivial modules mentioned
SM(S1 )
SM(S2 )
SM(S3 )
SM(0 )
SM(S )

=
=
=
=
=

{{a}, {b}},
{{b}, {a, b}, {b, c}, {a, c}, {b, d}, {a, d}, {b, c, d}, {a, c, d}},
{{b}, {c, d}},
{, {c}, {a, c}},
{{b}, {a, c}, {b, c}, {b, d}, {a, c, d}, {b, c, d}}.

But, regardless decomposition obtained, holds respective joins
SM() =
=
=
=

SM(S1 )
SM(S2 )
SM(S3 )
SM(0 )
SM(S )
SM(S3 )
SM()
SM(S )
SM()
{{a, c, d}, {b}}.

calculations involving
important notice allowed combinations
stable models determined terms joint visible atoms modules involved.
instance, Atv (S1 ) Atv (S3 ) = {a, b} {b, c, d} = {b} SM(S1 )

SM(S3 ) {{a} {c, d}, {b} {b}} = {{a, c, d}, {b}} Denition 5.2. Thus, interestingly,
role remaining two modules S2 0 merely approve upon two
models. Recalling discussion introduction, suggests strategy gives
precedence
1. evaluation modules stable models,
2. combination stable models modules visible atoms common.
843

fiJanhunen, Oikarinen, Tompits & Woltran

7. Shifting Disjunctions
section, continue pursuit applications module theorem established
Section 5. generalize principle shifting disjunctive rules (Gelfond et al., 1991;
Dix et al., 1996) applying results paper. Roughly speaking, idea behind
shifting translate disjunctive rule B, C several normal (non-disjunctive)
rules shifting head atoms h negative literals h body. instance,
simple disjunctive rule b c captured normal rules
b, c,

b a, c,



c a, b.

shown Eiter et al. (2004), local shifting transformation preserves ordinary
equivalence, i.e., stable models.12 application technique is, however, pre-empted
presence head-cycles (Ben-Eliyahu & Dechter, 1994). cycle provided
SCC intersects head disjunctive rule B, C
|S A| > 1. instance, local shifting longer applicable rule b c
presence b b create strongly connected component = {a, b}.
consequence, respective DLP-functions
1 = h{a b c ; b; b a}, , {a, b, c}, i,

(30)

2 = h{a b, c; b a, c; c a, b; b; b a}, , {a, b, c},

(31)

dierent stable models: SM(1 ) = {{a, b}, {c}} SM(2 ) = {{c}}. discrepancy stable models settled applying decomposition technique
Section 6. fact, leads proper generalization local shifting transformation
formalized DLP-functions strongly connected components.
Definition 7.1 Let = hR, I, O, Hi DLP-function SCC+ () respective set
SCCs. general shifting DLP-function GSH() = hIC(R) R , I, O, Hi,
R set rules
{(A S) B, C, (A \ S) | B, C R, SCC+ () 6= }.

(32)

Hence, idea project head rule respect component S,
atoms dierence \ shifted negative body. viewed
contribution disjunctive rule B, C particular component S.
Example 7.2 1 (30), SCC+ (1 ) = {{a, b}, {c}},
GSH(1 ) = h{a b c; c a, b; b; b a}, , {a, b, c}, i.
importantly, SM(GSH(1 )) = {{a, b}, {c}} = SM(1 ), contrast set
SM(2 ) = {{c}} stable models 2 (31).
12. addition ordinary equivalence, also uniform equivalence (Eiter & Fink, 2003) preserved local
shifting strong equivalence (Lifschitz, Pearce, & Valverde, 2001).

844

fiModularity Aspects Disjunctive Stable Models

prove correctness general shifting principle Denition 7.1.
aim exploit decomposition Denition 6.3 together modular
reconstruction Proposition 6.4 compositionality stable semantics
Corollary 6.5. extend coverage Corollary 6.5, introduce explicit operators
revealing hiding atoms DLP-functions follows:
Definition 7.3 Let = hR, I, O, Hi DLP-function. Then,
1. Reveal(, A) = hR, I, A, H \ Ai, set H hidden atoms,
2. Hide(, A) = hR, I, \ A, H Ai, set output atoms.
Since denition stable models make dierence output atoms
hidden atoms, following properties easy verify. role hidden atoms
becomes important Section 8 DLP-functions compared other.
Proposition 7.4 Let DLP-function.
1. Ath (), SM() = SM(Reveal(, A)).
2. Ato (), SM() = SM(Hide(, A)).
Lemma 7.5 Let DLP-function Ath () = , component SCC+ (),
respective module decomposition according Definition 6.3. Then,
SM(S ) = SM(GSH(S )).

(33)

Proof. Recall = hDef R (S), I, S, i, input signature = At(Def R (S)) \
S. Notice component SCC+ (S ) hence GSH(S ) set rules
R = {(A S) B, C, (A \ S) | B, C Def R (S)}.
Consider interpretation S, input output signatures
, respectively. Thus, Mi = Mo = S. Then, following equivalences
hold:






B (Def R (S)/Mi )Mo
B, C Def R (S)/Mi Mo |= C
B , C Def R (S) = Ao , B = Bo , C = Co ,
Mi |= Ai Bi Ci , Mo |= Co
B , C , Ai R = Ao , B = Bo , C = Co ,
Mi |= Ai Bi Ci , Mo |= Co
B, C R /Mi Mo |= C
B (R /Mi )Mo .

Thus, conclude (Def R (S)/Mi )Mo coincides (R /Mi )Mo , and, consequently, Mo
MM((Def R (S)/Mi )Mo ) Mo MM((R /Mi )Mo ). Therefore, SM(S /Mi ) =
SM(GSH(S )/Mi ). Since and, particular, Mi arbitrarily chosen beginning,
obtain equality stable models stated (33) directly Corollary 3.9.

845

fiJanhunen, Oikarinen, Tompits & Woltran

Theorem 7.6 DLP-function = hR, I, O, Hi, SM() = SM(GSH()).
Proof. Since may hidden atoms, Corollary 6.5 applicable decomposition based SCC+ (). Thus, start = Reveal(, H) = hR, I, H,
rather itself. Since SCCs independent hiding, SCC+ ( ) = SCC+ ()
GSH( ) = Reveal(GSH(),
H). Since Ath ( ) = construction, know
F

Proposition 6.4 0 ( SSCC+ () ) = . Applying GSH() equation yields
F
GSH( ) = 0 ( SSCC+ () GSH(S )).

(34)

regards respective sets stable models, obtain
SM( )

=



(
SM( )


=

SM(GSH( )).

=

SM(0 )
(

0

SSCC+ ( )

SM(S ))

[Corollary 6.5]

SSCC+ ( )

SM(GSH(S )))

[Lemma 7.5]
[Corollary 6.5 (34)]

follows Proposition 7.4 SM(Hide( , H)) = SM( ) = SM(GSH( )) =
SM(Hide(GSH( ), H)). Since Hide( , H) = Hide(GSH( ), H) = GSH(),
established SM() = SM(GSH()) desired.

According Denition 6.3, decompositions DLP-functions create multiple copies
disjunctive rules whose heads intersect several SCCs. introduction copies
circumvented applying general shifting technique Denition 7.1.
Example 7.7 DLP-function Example 6.6, obtain R1 = {ab c, d;
b; b a} R2 = {c a, b; c d; c} sets rules associated 1 = hR1 , {c, d}, {a, b}, 2 = hR2 , {a, b}, {c, d}, i, 1 2 =
hR1 R2 , , {a, b, c, d}, defined.

observations enable us view disjunctive rules shared modules
associated SCCs syntactic sugar. However, clever implementation save space
using shared rules. worst case, unwinding rule a1 B, C coincides
respective SCCs S1 , . . . , Sn a1 S1 , . . . , Sn may create n copies
body B C. quadratic blow-up partly alleviated introducing new
atom b name body. Thus result shifting a1 S1 , . . . , Sn becomes
a1 b, a2 , . . . , ;
..
.
ai b, a1 , . . . , ai1 , ai+1 , . . . , ;
..
.
b, a1 , . . . , an1
together dening rule b B, C b. implementation general
shifting principle called dencode.13 requested so, calculates beforehand whether
pays introduce new atom body disjunctive rule not.
13. Available http://www.tcs.hut.fi/Software/asptools/ experimenting.

846

fiModularity Aspects Disjunctive Stable Models

8. Equivalence DLP-Functions
concept visible equivalence originally introduced order neglect hidden atoms
logic programs, theories interest, compared basis models (Janhunen, 2006). Oikarinen Janhunen (2008a) extended idea level
logic program modulesgiving rise notion modular equivalence logic programs.
section, generalize concept modular equivalence DLP-functions
introduce translation-based method checking modular equivalence DLP-functions
following analogous approaches Oikarinen Janhunen (2004, 2009).
8.1 Modular Equivalence
Module interfaces must taken properly account DLP-functions compared.
reason, consider two DLP-functions 1 2 compatible
Ati (1 ) = Ati (2 ) Ato (1 ) = Ato (2 ).
Definition 8.1 DLP-functions 1 2 modularly equivalent, denoted 1 2 ,

1. 1 2 compatible
2. bijection f : SM(1 ) SM(2 ) interpretations
SM(1 ), Atv (1 ) = f (M ) Atv (2 ).
proof congruent lifts case normal programs (Oikarinen
& Janhunen, 2008a) disjunctive case using Theorem 5.7.
Proposition 8.2 Let 1 , 2 , DLP-functions. 1 2 1
2 defined, 1 2 .
Proof. Let 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 DLP-functions
1 2 , = hR, I, O, Hi DLP-function acting arbitrary context 1
2 1 2 dened. Consider SM(1 ). Theorem 5.7
implies M1 = At(1 ) SM(1 ) N = At() SM(). Since 1 2 ,
I1 = I2 , O1 = O2 , bijection f : SM(1 ) SM(2 )
M1 (I1 O1 ) = f (M1 ) (I2 O2 )

(35)

holds M1 . Dene M2 = f (M1 ). Since M1 N compatible denition (35)
holds, models M2 N compatible I1 = I2 O1 = O2 . Thus, M2 N
SM(2 ) Theorem 5.7 eectively described mapped model
SM(2 ) function g : SM(1 ) SM(2 ) dened
g(M ) = f (M At(1 )) (M At()).
Clearly, g maps set visible atoms itself, is,
(I1 O1 O) = g(M ) (I2 O2 O).
justications g bijection follows:
847

fiJanhunen, Oikarinen, Tompits & Woltran

g injection: 6= N implies g(M ) 6= g(N ) M, N SM(1 ), since
f (M At(1 )) 6= f (N At(1 )) At() 6= N At().
g surjection: N SM(2 ), = f 1 (N At(2 )) (N At())
SM(1 ) g(M ) = N , since f surjection.
inverse function g 1 : SM(2 ) SM(1 ) g dened setting
= f 1 (N At(2 )) (N At()). Thus, 1 2 .

Note GSH() follows directly Theorem 7.6. Applying Proposition 8.2
context Theorem 7.6 indicates shifting localized particular component
1 larger DLP-function 1 since 1 GSH(1 ) .

g 1 (N )

8.2 Verifying Modular Equivalence
Oikarinen Janhunen (2004) proposed translation-based method verication
weak equivalence disjunctive logic programs. Two logic programs weakly equivalent exactly set stable models. Thus, weak equivalence
seen special case modular equivalence DLP-functions 1 2
Ati (1 ) Ath (1 ) = Ati (2 ) Ath (2 ) = . motivates us adjust translationbased technique verication modular equivalence. observed previous work
(Janhunen & Oikarinen, 2007; Oikarinen & Janhunen, 2008a), verication visible/modular equivalence involves counting problem general. reduction computational time complexity achieved programs enough visible atoms,
referred EVA property short, (Janhunen & Oikarinen, 2007). DLPfunction = hR, I, O, Hi, dene hidden part restricted DLP-function
h = hDef R (H), O, H, enables evaluation hidden atoms H given
arbitrary truth values atoms O. Recalling Denition 3.2, use
instantiation h respect interpretation Mv Ati (h ), i.e., h /Mv , dene
EVA property DLP-function .
Definition 8.3 DLP-function = hR, I, O, Hi enough visible atoms iff h /Mv
unique stable model Mv Atv () = Ati (h ).
idea behind translation-based method Oikarinen Janhunen (2004)
ordinary disjunctive programs R1 R2 weakly equivalent translations
TR(R1 , R2 ) TR(R2 , R1 ) stable models. following, propose modied
version translation function adjusted verication modular equivalence. order
able verify modular equivalence, need take semantics atoms
input signature account well role hidden atoms modular equivalence
programs consideration. case DLP-functions, transform pair 1
2 compatible DLP-functions DLP-function EQT(1 , 2 ) stable
model stable model SM(1 ) stable model
N SM(2 ) Atv (1 ) = N Atv (2 ). form translation composition
DLP-functions order fully exploit compositionality stable model semantics
justifying correctness method.
follows, use new atoms , , appearing At(1 ) At(2 )
atom a, use shorthand = {a | A} set atoms,
848

fiModularity Aspects Disjunctive Stable Models

analogously dened shorthands . Moreover, diff, unsat, unsat , ok new
atoms appearing At(1 ) At(2 ). translation EQT(1 , 2 ),
summarized Denition 8.4 below, consists following three parts:
(i) DLP-function 1 naturally captures stable model SM(1 ).
(ii) DLP-function hidden(2 ) = hRh , O, H , provides representation
hidden part 2 = hR, I, O, Hi evaluated respect visible part .
input signature hidden(2 ) consists visible atoms Atv (2 ) = Atv (1 ) =
O. set Rh contains rule Ah Bv Bh , (Av Cv Ch ) B, C R
Ah 6= , i.e., B, C Def R (H). hidden parts rules renamed
systematically using atoms Ath (2 ) . capture unique stable model
N (2 )h /Mv expressed Ath (2 ) rather Ath (2 ). Note existence
uniqueness N guaranteed EVA property.
(iii) Finally, DLP-function
TR(2 ) = hRTR , H , H {unsat, unsat , diff, ok}, H
provides minimality check. set RTR contains
1. rule unsat Bv Bh , (Av Ah Cv Ch ) rule B, C R,
2. rules a, , unsat a, , unsat O, rules
, , unsat , , unsat H,
3. rule unsat Bi Bo Bh , (Ai Ao Ah Cv Ch ), unsat rule
B, C R,
4. rule diff a, , unsat O, rule diff , , unsat
H,
5. following rules:
ok unsat,

ok diff, unsat, unsat ,

ok.

intuition behind translation TR(2 ) follows. rules rst
item check whether interpretation L At(2 ) corresponding actual input
K = (L (I O)) {a | L H} Atv (2 ) Ath (2 ) TR(2 ) satises
rules 2 . rules 2 satised, rules items 24 activated
literals unsat bodies. rules second item used
generate subset L L L Ati (2 ) = L Ati (2 ). achieved
introducing new atom Ato (2 ) Ath (2 ). rules third
item check whether representation L Ati (2 ) Ato (2 ) Ath (2 ) , i.e.,
K = (L I) {a | L (O H)}, satises rules L
2 . rules
fourth item check whether L proper subset L. Finally, rules fth
item summarize reasons L cannot stable model 2 , i.e., either
rules 2 satised L, L minimal model L
2 . net eect
construction, TR(2 )/K stable model L stable model 2 .
849

fiJanhunen, Oikarinen, Tompits & Woltran

Definition 8.4 Let 1 2 = hR, I, O, Hi compatible DLP-functions enough
visible atoms. Then, translation EQT(1 , 2 ) given 1 hidden(2 ) TR(2 ).
translation TR(2 ) minimality check essentially contains rules
TR(R1 , R2 ) \ R1 , TR(R1 , R2 ) translation dened Oikarinen Janhunen
(2004) sets R1 R2 disjunctive rules. two aspects, however.
First, occurrences hidden atoms H additionally represented using counterparts H . Second, need renamed versions atoms H
interpretation atoms input signature kept xed. Finally, note
DLP-functions 1 2 correspond ordinary disjunctive logic programs, i.e.,
1 = hR1 , , O, 2 = hR2 , , O, i, translation EQT(1 , 2 ) coincides
TR(R1 , R2 ).
Theorem 8.5 Let 1 2 compatible DLP-functions enough visible atoms.
Then, 1 2 iff SM(EQT(1 , 2 )) = SM(EQT(2 , 1 )) = .
Proof sketch. Let 1 2 = hR, I, O, Hi compatible DLP-functions enough
visible atoms. Theorem 5.7, given compatible interpretations M1 At(1 ), M2
At(hidden(2 )), M3 At(TR(2 )), = M1 M2 M3 stable model translation EQT(1 , 2 ) M1 SM(1 ), M2 SM(hidden(2 )), M3 SM(TR(2 )). Given
interpretation M1 At(1 ), unique stable model M2 SM(hidden(2 ))
compatible M1 , since 2 EVA property. Hence, hidden(2 ) constrain
stable models composition EQT(1 , 2 ). Whenever M3 compatible M1
M2 , holds M3 (I OH ) = (M1 M2 )(I OH ) M3 SM(TR(2 ))
interpretation M3 (I O){a H | M3 } stable model 2 established
Oikarinen Janhunen (2004, Theorem 1).

verifying modular equivalence DLP-functions forms 1 2 ,
possible streamline translations involved verication task.
Theorem 8.6 Let 1 2 compatible DLP-functions enough visible atoms,
DLP-function 1 2 defined. Then, 1 2
iff SM(EQT(1 , 2 ) ) = SM(EQT(2 , 1 ) ) = .
context arbitrary DLP-function, i.e., necessary
EVA property, long 1 2 dened. prove Theorem 8.6, notice
due structure translation, EQT(1 , 2 ) dened whenever 1
dened, Theorems 5.7 8.5 applied.

9. Related Work
Eiter et al. (1997a) consider use disjunctive datalog programs query programs
relational databases. approach, query programs formalized triples h, R, Si
set disjunctive rules R signatures input output
relations, respectively, whereas auxiliary (hidden) predicates left implicit. Hence,
propositional case, notable dierence respect Denition 2.1 input
atoms allowed occur heads disjunctive rules. regards semantics,
850

fiModularity Aspects Disjunctive Stable Models

program reduced respect complete input database specied terms
R, yielding instantiation [D], and, among others, stable-model semantics applied
[D] analogy Denition 3.2. However, contrast modular architecture, Eiter
et al. (1997a) take positive negative dependencies account recursion
modules tolerated. resulting hierarchy complete components admits
straightforward generalization splitting sequences (Lifschitz & Turner, 1994).
essential dierence partial order rather total order modules assumed.
respect, worth pointing partial orders DLP-functions permitted
.
Modularity gained attention context conventional (monotonic) logic
programming; see work Bugliesi, Lamma, Mello (1994) survey. Two mainstream approaches identied: rst called programming-in-the-large algebraic operators introduced construction logic programs modules.
approach paper falls categorythe join example operators.
other, quite dierent programming-in-the-small approach, extend underlying logical language terms abstraction mechanisms. approach Eiter et al.
(1997b), instance, logic program modules viewed generalized quantifiers
allowed nest hierarchical fashion. give idea approach, consider
module formalizes transitive closure relation denoted predicate rel(, ):
tclo(x, y) rel(x, y);

tclo(x, y) tclo(x, z), rel(z, y).

Here, tclo(, ) acts output predicate module tclo[rel] whereas rel(, )
input predicate. module invoked create transitive closure binary
relation substituted rel(, ) above. Consider, instance, rule
loop(x) tclo[edge](x, y), tclo[edge](y, x)
captures nodes involved loops directed graph whose edges supposed
represented predicate edge(, ). approach, call tclo[edge] would result
one module part respective ground program input output signatures
= {edge(x, y) | 1 x, n} = {tclo(x, y) | 1 x, n}
case n vertices. However, architecture Eiter et al. (1997b), module
tclo[rel] invoked several times form transitive closures dierent relations.
eectively propositional approach, invocation tclo[rel] would map new module.
Although modules could obtained straightforward renaming predicates,
aspect illustrates power programming-in-the-small approach. Here, tclo[rel] acts
new parameterized connective programmer concisely refer new
relation, viz. transitive closure rel case. But, spite succinctness
point, relations may unwound actual implementation. aspect
made explicit modular action description (MAD) language proposed Lifschitz
Ren (2006): modular action description turned single-module description
recursive fashion. outcome determines meaning modular description via
embedding ASP (Lifschitz & Turner, 1999).
Faber, Greco, Leone (2007) apply magic-set method evaluation datalog
programs negation. notion module based concept independent
851

fiJanhunen, Oikarinen, Tompits & Woltran

set. non-disjunctive logic program = hR, , O, i, set satises,
S, following two conditions:
1. rule h B, C R h = a, B C S,
2. B C dangerous rule h B, C R, {h} B C S.
skip exact denition dangerous rules which, roughly speaking, may interfere
existence stable models. clear independent sets splitting sets sense
Denition 5.15, vice versa general. Hence, module theorem provided
Faber et al. (2007) viewed special case splitting-set theorem and, therefore,
observations presented Section 5.3 apply independent sets well.

10. Conclusion Discussion
paper, introduced formal framework modular programming context
disjunctive logic programs stable-model semantics. framework based
notion DLP-function puts eect appropriate input/output interfacing
disjunctive logic programs. Analogous module concepts already studied
cases normal logic programs smodels programs (Oikarinen & Janhunen, 2008a)
even propositional theories (Janhunen, 2008a), special characteristics disjunctive
rules properly taken account syntactic semantic denitions DLPfunctions presented herein. respect, would like draw readers attention
Denition 2.1 (item 2), Denition 2.2 (items 45), well Denition 3.2.
Undoubtedly, main result paper module theorem, i.e., Theorem 5.7,
proved DLP-functions generalthus covering class disjunctive programs. module theorem important provides compositional semantics
disjunctive programs generalizes existing approaches based splitting sets (Lifschitz & Turner, 1994) magic sets (Faber et al., 2007). Although
approach based number design decisions, e.g., regards denition module
composition, nevertheless brings limits modular programming context
nonmonotonic declarative language. module theorem exploited number ways ASP based disjunctive logic programs. demonstrated Section 6,
provides basis decomposing disjunctive programs components hence
localization reasoning tasks. Moreover, established Section 7, technique
shifting disjunctive rules generalized disjunctive programs involving head-cycles.
Actually, generalized form enables us remove shared disjunctive rules altogether
might desirable due higher space requirements. Finally, theory modular
equivalence fully applicable DLP-functions demonstrated Section 8.
addition results discussed above, anticipate applications module theorem future. strongly believe research direction yields
results theoretical interest also leads development practicably useful software
engineering methods ASP. fact, rst tools decomposing linking programs
already implemented context smodels system.14 results Section 6 enable development analogous tools used disjunctive solvers
14. See modlist lpcat ASP tools collection http://www.tcs.hut.fi/Software/asptools/.

852

fiModularity Aspects Disjunctive Stable Models

claspD, cmodels, dlv, GnT. also implementation general shifting
principle, called dencode, ASP tool collection. results Section 8 pave way
extending translation-based verication tool, dlpeq (Janhunen & Oikarinen, 2004),
verication modular equivalence. extension already available
respective tool, lpeq, smodels programs (Oikarinen & Janhunen, 2009).15
Acknowledgments work partially supported Academy Finland projects #211025 (Advanced Constraint Programming Techniques Large Structured Problems) #122399 (Methods Constructing Solving Large Constraint
Models), Austrian Science Foundation (FWF) projects P18019 (Formal Methods Comparing Optimizing Nonmonotonic Logic Programs) P21698
(Methods Methodologies Developing Answer-Set Programs). authors would
like thank anonymous referees constructive comments well Martin Gebser Torsten Schaub suggestion exploit program completion loop formulas
proof module theorem. preliminary version paper appeared
proceedings 9th International Conference Logic Programming Nonmonotonic
Reasoning (LPNMR07), Vol. 4483 LNCS, pp. 175187, Tempe, AZ, USA, Springer.

References
Baral, C., Dzifcak, J., & Takahashi, H. (2006). Macros, macro calls use ensembles
modular answer set programming. Etalle, S., & Truszczyski, M. (Eds.), Proceedings
22nd International Conference Logic Programming (ICLP06 ), Vol. 4079
LNCS, pp. 376390, Seattle, WA, USA. Springer.
Ben-Eliyahu, R., & Dechter, R. (1994). Propositional semantics disjunctive logic programs. Annals Mathematics Artificial Intelligence, 12 (12), 5387.
Bugliesi, M., Lamma, E., & Mello, P. (1994). Modularity logic programming. Journal
Logic Programming, 19/20, 443502.
Clark, K. L. (1978). Negation failure. Gallaire, H., & Minker, J. (Eds.), Logic
Data Bases, pp. 293322. Plenum Press, New York.
Dix, J., Gottlob, G., & Marek, V. W. (1996). Reducing disjunctive non-disjunctive
semantics shift-operations. Fundamenta Informaticae, 28 (1-2), 87100.
Drescher, C., Gebser, M., Grote, T., Kaufmann, B., Knig, A., Ostrowski, M., & Schaub,
T. (2008). Conict-driven disjunctive answer set solving. Brewka, G., & Lang, J.
(Eds.), Proceedings 11th International Conference Principles Knowledge
Representation Reasoning, pp. 170176, Sydney, Australia. AAAI Press.
Eiter, T., & Fink, M. (2003). Uniform equivalence logic programs stable model
semantics. Palamidessi, C. (Ed.), Proceedings 19th International Conference
Logic Programming (ICLP03), Vol. 2916 LNCS, pp. 224238, Mumbay, India.
Springer.
15. Verification tools mentioned available http://www.tcs.hut.fi/Software/lpeq/.

853

fiJanhunen, Oikarinen, Tompits & Woltran

Eiter, T., Fink, M., Tompits, H., & Woltran, T. (2004). Simplifying logic programs
uniform strong equivalence. Lifschitz, V., & Niemel, I. (Eds.), Proceedings
7th International Conference Logic Programming Nonmonotonic Reasoning
(LPNMR04 ), Vol. 2923 LNAI, pp. 8799, Fort Lauderdale, FL, USA. Springer.
Eiter, T., & Gottlob, G. (1995). computational cost disjunctive logic programming:
Propositional case. Annals Mathematics Artificial Intelligence, 15 (3-4), 289
323.
Eiter, T., Gottlob, G., & Mannila, H. (1997a). Disjunctive datalog. ACM Transactions
Database Systems, 22 (3), 364418.
Eiter, T., Gottlob, G., & Veith, H. (1997b). Modular logic programming generalized
quantiers. Dix, J., Furbach, U., & Nerode, A. (Eds.), Proceedings 4th
International Conference Logic Programming Nonmonotonic Reasoning (LPNMR97 ), Vol. 1265 LNCS, pp. 290309, Dagstuhl, Germany. Springer.
Eiter, T., Ianni, G., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2008). Combining
answer set programming description logics Semantic Web. Artificial
Intelligence, 172 (1213), 14951539.
Erdem, E., & Lifschitz, V. (2003). Tight logic programs. Theory Practice Logic
Programming, 3 (4-5), 499518.
Faber, W., Greco, G., & Leone, N. (2007). Magic sets application data integration. Journal Computer System Sciences, 73, 584609.
Gaifman, H., & Shapiro, E. (1989). Fully abstract compositional semantics logic programs. Proceedings 16th Annual ACM Symposium Principles Programming Languages, pp. 134142, Austin, TX, USA. ACM Press.
Gebser, M., Kaminski, R., Kaufmann, B., Ostrowski, M., Schaub, T., & Thiele, S. (2008a).
Engineering incremental ASP solver. de la Banda, M., & Pontelli, E. (Eds.),
Proceedings 24th International Conference Logic Programming (ICLP08),
Vol. 5366 LNCS, pp. 190205, Udine, Italy. Springer.
Gebser, M., Schaub, T., Thiele, S., Usadel, B., & Veber, P. (2008b). Detecting inconsistencies large biological networks answer set programming. de la Banda,
M., & Pontelli, E. (Eds.), Proceedings 24th International Conference Logic
Programming (ICLP08), Vol. 5366 LNCS, pp. 130144, Udine, Italy. Springer.
Gelfond, M., & Gabaldon, A. (1999). Building knowledge base: example. Annals
Mathematics Artificial Intelligence, 25 (3-4), 165199.
Gelfond, M., & Leone, N. (2002). Logic programming knowledge representation
A-Prolog perspective. Artificial Intelligence, 138 (1-2), 338.
Gelfond, M., & Lifschitz, V. (1988). stable model semantics logic programming.
Kowalski, R. A., & Bowen, K. A. (Eds.), Proceedings 5th International Conference Logic Programming (ICLP88), pp. 10701080, Seattle, WA, USA. MIT
Press.
854

fiModularity Aspects Disjunctive Stable Models

Gelfond, M., & Lifschitz, V. (1991). Classical negation logic programs disjunctive
databases. New Generation Computing, 9, 365385.
Gelfond, M., Przymusinska, H., Lifschitz, V., & Truszczyski, M. (1991). Disjunctive defaults. Allen, J. F., Fikes, R., & Sandewall, E. (Eds.), Proceedings 2nd International Conference Principles Knowledge Representation Reasoning, pp.
230237, Cambridge, MA, USA. Morgan Kaufmann.
Giunchiglia, E., Lierler, Y., & Maratea, M. (2006). Answer set programming based
propositional satisability. Journal Automated Reasoning, 36 (4), 345377.
Janhunen, T. (2006). (in)translatability results normal logic programs propositional theories. Journal Applied Non-Classical Logics, 16 (12), 3586.
Janhunen, T. (2008a). Modular equivalence general. Ghallab, M., Spyropoulos, C.,
Fakotakis, N., & Avouris, N. (Eds.), Proceedings 18th European Conference
Artificial Intelligence (ECAI08), pp. 7579, Patras, Greece. IOS Press.
Janhunen, T. (2008b). Removing redundancy answer set programs. de la Banda,
M., & Pontelli, E. (Eds.), Proceedings 24th International Conference Logic
Programming (ICLP08), Vol. 5366 LNCS, pp. 729733, Udine, Italy. Springer.
Janhunen, T., Niemel, I., Seipel, D., Simons, P., & You, J.-H. (2006). Unfolding partiality
disjunctions stable model semantics. ACM Transactions Computational
Logic, 7 (1), 137.
Janhunen, T., & Oikarinen, E. (2004). lpeq dlpeq translators automated equivalence testing logic programs. Lifschitz, V., & Niemel, I. (Eds.), Proceedings
7th International Conference Logic Programming Nonmonotonic Reasoning
(LPNMR04 ), Vol. 2923 LNAI, pp. 336340, Fort Lauderdale, FL, USA. Springer.
Janhunen, T., & Oikarinen, T. (2007). Automated verication weak equivalence within
smodels system. Theory Practice Logic Programming, 7 (6), 697744.
Junttila, T., & Niemel, I. (2000). Towards ecient tableau method boolean circuit
satisability checking. Lloyd, J. W., et al. (Eds.), Proceedings First International Conference Computational Logic (CL 2000), Vol. 1861 LNCS, pp. 553567,
London, UK. Springer.
Koch, C., Leone, N., & Pfeifer, G. (2003). Enhancing disjunctive logic programming systems
SAT checkers. Artificial Intelligence, 151 (1-2), 177212.
Lee, J., & Lifschitz, V. (2003). Loop formulas disjunctive logic programs. Palamidessi,
C. (Ed.), Proceedings 19th International Conference Logic Programming
(ICLP03 ), Vol. 2916 LNCS, pp. 451465, Mumbay, India. Springer.
Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., & Scarcello, F. (2006). DLV
system knowledge representation reasoning. ACM Transactions Computational Logic, 7 (3), 499562.
855

fiJanhunen, Oikarinen, Tompits & Woltran

Lifschitz, V. (1985). Computing circumscription. Joshi, A. K. (Ed.), Proceedings
9th International Joint Conference Artificial Intelligence (IJCAI85 ), pp. 121127,
Los Angeles, CA, USA. Morgan Kaufmann.
Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. ACM
Transactions Computational Logic, 2 (4), 526541.
Lifschitz, V., & Ren, W. (2006). modular action description language. Proceedings
21st National Conference Artificial Intelligence (AAAI06), pp. 853859,
Boston, MA, USA. AAAI Press.
Lifschitz, V., & Turner, H. (1994). Splitting logic program. Hentenryck, P. V. (Ed.),
Proceedings 11th International Conference Logic Programming (ICLP94 ),
pp. 2337, Santa Margherita Ligure, Italy. MIT Press.
Lifschitz, V., & Turner, H. (1999). Representing transition systems logic programs.
Gelfond, M., Leone, N., & Pfeifer, G. (Eds.), Proceedings 6th International
Conference Logic Programming Nonmonotonic Reasoning, (LPNMR99 ), Vol.
1730 LNAI, pp. 92106, El Paso, TX, USA. Springer.
Lin, F., & Zhao, Y. (2004). ASSAT: computing answer sets logic program SAT
solvers. Artificial Intelligence, 157 (1-2), 115137.
Marek, V. W., & Truszczyski, M. (1999). Stable models alternative logic programming paradigm. Apt, K. R., Marek, V. W., Truszczyski, M., & Warren,
D. S. (Eds.), Logic Programming Paradigm: 25-Year Perspective, pp. 375398.
Springer.
McCarthy, J. (1986). Applications circumscription formalizing commonsense knowledge. Artificial Intelligence, 28, 89116.
Niemel, I. (1999). Logic programs stable model semantics constraint programming
paradigm. Annals Mathematics Artificial Intelligence, 25 (34), 241273.
Oikarinen, E., & Janhunen, T. (2004). Verifying equivalence logic programs
disjunctive case. Lifschitz, V., & Niemel, I. (Eds.), Proceedings 7th International Conference Logic Programming Nonmonotonic Reasoning (LPNMR04 ),
Vol. 2923 LNAI, pp. 180193, Fort Lauderdale, FL, USA. Springer.
Oikarinen, E., & Janhunen, T. (2008a). Achieving compositionality stable model
semantics smodels programs. Theory Practice Logic Programming, 8 (56),
717761.
Oikarinen, E., & Janhunen, T. (2008b). Implementing prioritized circumscription computing disjunctive stable models. Dochev, D., Pistore, M., & Traverso, P. (Eds.),
Artificial Intelligence: Methodology, Systems, Applications, 13th International
Conference (AIMSA08), Vol. 5253 LNCS, pp. 167180, Varna, Bulgaria. Springer.
Oikarinen, E., & Janhunen, T. (2009). translation-based approach verication
modular equivalence. Journal Logic Computation, 19 , 591613.
856

fiModularity Aspects Disjunctive Stable Models

Reiter, R. (1987). theory diagnosis rst principles. Artificial Intelligence, 32 (1),
5795.
Simons, P., Niemel, I., & Soininen, T. (2002). Extending implementing stable
model semantics. Artificial Intelligence, 138 (12), 181234.

857

fiJournal Artificial Intelligence Research 35 (2009) 677-716

Submitted 11/08; published 08/09

Variable Forgetting Reasoning Knowledge
Kaile Su

sukl@pku.edu.cn

School Electronics Engineering Computer Science
Peking University
Beijing, P.R. China

Abdul Sattar

a.sattar@griffith.edu.au

Institute IIS
Griffith University
Brisbane, Qld 4111, Australia

Guanfeng Lv

lvgf@yahoo.com

School Computer Science
Beijing University Technology
Beijing, P.R. China

Yan Zhang

yan@cit.uws.edu.au

School Computing Information Technology
University Western Sydney
Penrith South DC NSW 1797, Australia

Abstract
paper, investigate knowledge reasoning within simple framework called
knowledge structure. use variable forgetting basic operation one agent reason
agents knowledge. framework, two notions namely agents
observable variables weakest sufficient condition play important roles knowledge
reasoning. Given background knowledge base set observable variables Oi
agent i, show notion agent knowing formula defined
weakest sufficient condition Oi . Moreover, show capture
notion common knowledge using generalized notion weakest sufficient condition.
Also, show public announcement operator conveniently dealt via
notion knowledge structure. Further, explore computational complexity
problem whether epistemic formula realized knowledge structure.
general case, problem PSPACE-hard; however, interesting subcases,
reduced co-NP. Finally, discuss possible applications framework
interesting domains automated analysis well-known muddy
children puzzle verification revised Needham-Schroeder protocol. believe
many scenarios natural presentation available information
knowledge form knowledge structure. makes valuable
compared corresponding multi-agent S5 Kripke structure much
succinct.

1. Introduction
Epistemic logics, logics knowledge usually recognized originated
work Jaakko Hintikka - philosopher showed certain modal logics could
used capture intuitions nature knowledge early 1960s (Hintikka,
c
2009
AI Access Foundation. rights reserved.

fiSu, Sattar, Lv, & Zhang

1962). mid 1980s, Halpern colleagues discovered S5 epistemic logics
could given natural interpretation terms states processes (commonly called
agents) distributed system. model known interpreted system model
(Fagin, Halpern, Moses, & Vardi, 1995). found model plays important
role theory distributed systems applied successfully reasoning
communication protocols (Halpern & Zuck, 1992). However, work epistemic
logic mainly focused theoretical issues variants modal logic, completeness,
computational complexity, derived notions like distributed knowledge common
knowledge.
paper, explore knowledge reasoning within concrete model knowledge. framework reasoning knowledge simple powerful enough
analyze realistic protocols widely used security protocols.
illustrate problem investigated paper, let us consider communication
scenario Alice sends Bob message Bob sends Alice acknowledgement
receiving message. assume Alice Bob commonly following background
knowledge base CS :
Bob recv msg Alice send msg
Bob send ack Bob recv msg
Alice recv ack Bob send ack
Bob recv msg Bob send ack observable variables Bob, Alice send msg
Alice recv ack observable Alice.
problem concerned verify Alice Bob knows statement
. Intuitively, able prove statement observable Alice (Bob),
Alice (Bob) knows statement statement holds.
knowledge non-observable statements, following hold:
1. Alice knows Bob recv msg Alice recv ack holds; hand, Alice knows
Bob recv msg, Alice recv ack holds, means that, context
example, way Alice gets know Bob recv msg Alice receives
acknowledgement Bob.
2. Bob knows Alice send msg Bob recv msg holds; moreover, Bob knows Alice send msg,
Bob recv msg holds. latter indicates way Bob gets
know Alice send msg Bob receives message Alice.
3. Finally, Bob know Alice recv ack.
idea behind presented knowledge model scenarios demonstrated
agents knowledge agents observations logical consequences
agents observations background knowledge base.
One key notions introduced paper agents observable variables.
notion shares similar spirit local variables work van der Hoek
Wooldridge (2002) local propositions work Engelhardt, van der Meyden
Moses (1998) work Engelhardt, van der Meyden Su (2003). Informally
speaking, local propositions depending upon agents local information;
agent always determine whether given local proposition true. Local variables
678

fiVariable Forgetting Reasoning Knowledge

primitive propositions local. Nevertheless, notion local propositions
(Engelhardt et al., 1998, 2003) semantics property truth assignment function
Kripke structure, notion local variables (van der Hoek & Wooldridge, 2002)
property syntactical variables. paper, prefer use term observable variable order avoid confusion term local variable used programming,
non-local variables global variables may often observable.
knowledge model also closely related notion weakest sufficient condition,
first formalized Lin (2001). Given background knowledge base set
observable variables Oi agent i, show notion agent knowing
formula defined weakest sufficient condition Oi ,
computed via operation variable forgetting (Lin & Reiter, 1994) eliminations
middle terms (Boole, 1854). Moreover, generalize notion weakest sufficient
condition capture notion common knowledge.
briefly discuss role variable forgetting knowledge model. Let us
examine scenario described again. Consider question: Alice figure
Bobs knowledge receives acknowledgement Bob? Note Alices
knowledge conjunction background knowledge base CS observations
Alice recv ack etc. Moreover, Alice knows Bobs knowledge conjunction
background knowledge base CS knows Bobs observations. Thus,
Alice gets Bobs knowledge computing knows Bobs observations.
setting, Alice gets knowledge Bobs observations simply forgetting Bobs nonobservable variables knowledge.
recent trend extending epistemic logics dynamic operators
evolution knowledge expressed (van Benthem, 2001; van Ditmarsch, van der Hoek,
& Kooi, 2005a). basic extension public announcement logic (PAL),
obtained adding operator truthful public announcements (Plaza, 1989; Baltag,
Moss, & Solecki, 1998; van Ditmarsch, van der Hoek, & Kooi, 2005b). show public
announcement operator conveniently dealt via notion knowledge structure. makes notion knowledge structure genuinely useful applications
like automated analysis well-known muddy children puzzle.
discussion above, see framework reasoning knowledge appropriate situations every agent specified set observable
variables. show significance framework, investigate
interesting applications automated analysis well-known muddy children puzzle
verification revised Needham-Schroeder protocol (Lowe, 1996).
believe many scenarios natural presentation available
information knowledge form knowledge structure. makes
valuable compared corresponding multi-agent S5 Kripke structure
much succinct. course, price pay determining whether formula
holds knowledge structure PSPACE-hard general case, PTIME
corresponding S5 Kripke structure taken input. However, achieved
trade-off time space prove computationally valuable. particular,
validity problem knowledge structure addressed instances
generating corresponding Kripke structure would unfeasible. muddy children
puzzle shows point clearly: generating corresponding Kripke structure impossible
679

fiSu, Sattar, Lv, & Zhang

practical point view, even least number children considered
experiments.
organization paper follows. next section, briefly introduce
concept forgetting notion weakest sufficient strongest necessary conditions.
Section 3, define framework reasoning knowledge via variable forgetting.
Section 4, generalize notion weakest sufficient condition strongest necessary
condition capture common knowledge within framework. Section 5, show
public announcement operator also conveniently dealt via notion
knowledge structure. Section 6 discusses computational complexity issue
problem whether epistemic formula realized knowledge structure.
general case, problem PSPACE-hard; however, interesting subcases,
reduced co-NP. Section 7, consider case study applying framework
model well known muddy children puzzle; security protocol
verification Section 8. Finally, discuss related work conclude paper
remarks.

2. Preliminaries
section, provide preliminaries notions variable forgetting
weakest sufficient condition, epistemic logic.
2.1 Forgetting
Given set propositional variables P , identify truth assignment P subset
P . say formula formula P propositional variable occurring
P . convenience, define true abbreviation fixed valid propositional
formula, say p p, p primitive proposition P . abbreviate true false.
also use |= denote usual satisfaction relation truth assignment
formula. Moreover, set formulas formula , use |= denote
every assignment , |= , |= .
p
)
Given propositional formula , propositional variable p, denote ( true
p
result replacing every p true. define ( false ) similarly.
notion variable forgetting (Lin & Reiter, 1994), eliminations middle terms
(Boole, 1854), defined follows:
Definition 1 Let formula P , V P . forgetting V , denoted
V , quantified formula P , defined inductively follows:
1. = ;
2. {p} =



p
true





p
false ;

3. (V {p}) = V ({p}).
convenience, use V denote V ().
Example 2: Let = (p q) (p r). {p} (q r) {q} (p r).
2
680

fiVariable Forgetting Reasoning Knowledge

Many characterizations variable forgetting, together complexity results, reported work Lang Marquis (1998). particular, notion variable
forgetting closely related formula-variable independence (Lang, Liberatore, &
Marquis, 2003).
Definition 3 Let propositional formula, V set propositional variables.
say independent V logically equivalent formula
none variables V appears.
following proposition given work Lang, Liberatore Marquis (2003).
Proposition 4 Let propositional formula, V set propositional variables.
V logically strongest consequence independent V (up
logical equivalence).
2.2 Weakest Sufficient Conditions
formal definitions weakest sufficient conditions strongest necessary conditions
first formalized via notion variable forgetting Lin (2001), turn play
essential role approach.
Definition 5 Let V set propositional variables V 0 V . Given set formulas
V background knowledge base formula V .
formula V 0 called sufficient condition V 0 |= .
called weakest sufficient condition V 0 sufficient
condition V 0 , sufficient condition 0 V 0
, |= 0 .
formula V 0 called necessary condition V 0 |= .
called strongest necessary condition V 0 necessary
condition V 0 , necessary condition 0 V 0
, |= 0 .
notions given closely related theory abduction. Given observation,
may one abduction conclusion draw. useful
find weakest one conclusions, i.e., weakest sufficient condition
observation (Lin, 2001). notions strongest necessary weakest sufficient conditions
proposition also many potential applications areas reasoning
actions. following proposition, due Lin (2001), shows compute
two conditions.
Proposition 6 Given background knowledge base {} V , formula V ,
subset V 0 V . Let SN C W SC strongest necessary condition weakest
sufficient condition V 0 {} respectively.
W SC equivalent (V V 0 )( );
SN C equivalent (V V 0 )( ).
681

fiSu, Sattar, Lv, & Zhang

2.3 Epistemic Logic Kripke Structure
recall standard concepts notations related modal logics multiagents knowledge.
Given set V propositional variables. Let L(V ) set propositional formulas
V . language epistemic logic, denoted Ln (V ), L(V ) augmented modal
operator Ki agent i. Ki read agent knows . Let LC
n (V )
language Ln (V ) augmented modal operator C set agents . formula
C indicates common knowledge among agents holds. omit
argument V write Ln LC
n , clear context.
According paper Halpern Moses (1992), semantics formulas
given means Kripke structure (Kripke, 1963), formalizes intuition behind
possible worlds. Kripke structure tuple (W, , K1 , , Kn ), W set
worlds, associates world truth assignment propositional variables,
(w)(p) {true, false} world w propositional variable p, K1 , , Kn
binary accessibility relations. convention, W , KiM used refer
set W possible worlds, Ki relation function Kripke structure ,
respectively. omit superscript clear context. Finally, let C

transitive closure Ki .
situation pair (M, w) consisting Kripke structure world w .
using situations, inductively give semantics formulas follows: primitive
propositions p,
(M, w) |= p iff (w)(p) = true.
Conjunctions negations dealt standard way. Finally,
(M, w) |= Ki iff w0 W wKiM w0 , (M, w0 ) |= ;
w 0 , (M, w 0 ) |= .
(M, w) |= C iff w0 W wC

say formula satisfiable Kripke structure (M, w) |= possible
world w Kripke structure .
Kripke structure called S5n Kripke structure if, every i, KiM equivalence relation. Kripke structure called finite Kripke structure set possible
worlds finite. According work Halpern Moses (1992), following
lemma.

Lemma 7 formula satisfiable S5n Kripke structure, finite S5n
Kripke structure.

3. Knowledge Weakest Sufficient Conditions
framework, knowledge structure simple model reasoning knowledge.
advantage model is, shown later, agents knowledge
computed via operation variable forgetting.
682

fiVariable Forgetting Reasoning Knowledge

3.1 Knowledge Structure
Definition 8 knowledge structure F n-agents (n + 2)-tuple (V, , O1 , , )
(1) V set propositional variables; (2) consistent set propositional
formulas V ; (3) agent i, Oi V .
variables Oi called agent observable variables. assignment satisfies
called state knowledge structure F. Given state F, define agent local
state state Oi . Two knowledge structures said equivalent
set propositional variables, set states and, agent i,
set agent observable variables.
pair (F, s) knowledge structure F state F called scenario.
Given knowledge structure (V, , O1 , , ) set V subsets V , use EV
denote relation two assignments s, s0 V satisfying (s, s0 ) EV
iff exists P V P = s0 P . use EV denote transitive closure
EV .
Let V = {Oi | }. (s, s0 ) EV iff exists
Oi = s0 Oi .
simple instance knowledge structure F0 = ({p, q}, {p q}, {p}, {q}), p, q
propositional variables. two agents knowledge structure F0 . Variables p
q observable agents 1 2, respectively. V{1,2} = {{p}, {q}};
two subsets s0 {p, q} satisfy p q, (s, s0 ) EV{1,2} .
give semantics language LC
n based scenarios.
Definition 9 satisfaction relationship |= scenario (F, s) formula
defined induction structure .
1. propositional variable p, (F, s) |= p iff |= p.
2. formulas , (F, s) |= iff (F, s) |= (F, s) |= ;
(F, s) |= iff (F, s) |= .
3. (F, s) |= Ki iff s0 F s0 Oi = Oi , (F, s0 ) |= .
4. (F, s) |= C iff (F, s0 ) |= s0 F (s, s0 ) EV .
say proposition formula L(V ) i-local Oi . Clearly, agent
knows i-local formula F iff |= .
Let F = (V, , O1 , , ) knowledge structure. say formula realized
knowledge structure F, every state F, (F, s) |= . convenience, F |= ,
denote formula realized knowledge structure F.
conclude subsection following lemma, used remains
paper.
Lemma 10 Let V finite set variables, F = (V, , O1 , , ) knowledge structure, state F. Also suppose {1, , n}, V = {Oi | }.

683

fiSu, Sattar, Lv, & Zhang

1. objective formula (i.e., propositional formula V ), (F, s) |= iff |= ;
2. formula , (F, s) |= ;
3. i-local formula , (F, s) |= Ki ;
4. formula , exists, , i-local formula logically equivalent
, (F, s) |= C ;
5. formulas 1 2 , (F, s) |= Ki (1 2 ) (Ki 1 Ki 2 );
6. formulas 1 2 , (F, s) |= C (1 2 ) (C 1 C 2 );
7. formula , (F, s) |= C Ki C .
Proof:
1. first item proposition proved induction structure .
primitive proposition, done first item Definition 9.
form negation conjunction, conclusion also follows immediately
first item Definition 9.
2. second item proposition proved first item fact
satisfies .
3. Given i-local formula , suffices show (F, s) |= Ki iff (F, s) |= .
first item proposition, (F, s) |= iff |= . Moreover,
i-local Oi , assignments s0 s0 Oi = Oi , s0 |=
iff |= . Therefore, get following three iffs: (F, s) |= Ki iff, state
s0 F s0 Oi = Oi , (F, s0 ) |= iff, state s0 F
s0 Oi = Oi , s0 |= iff |= . Thus, (F, s) |= Ki iff (F, s) |= .
4. Suppose that, , exists i-local formula logically equivalent
. need show (F, s) |= C . First, (s, s) EV EV ,
formula , (F, s) |= C implies (F, s) |= . Therefore, suffices
prove (F, s) |= C . Assume (F, s) |= . prove (F, s) |= C ,
need show every assignment s0 (s, s0 ) EV , (F, s0 ) |= .
definition EV , suffices show every finite sequence assignments
s0 , , sk s0 = (sj , sj+1 ) EV (0 j < k), every j k,
(F, sj ) |= . show induction j. j = 0, result clearly true.
Assume (F, sj ) |= . prove (F, sj+1 ) |= . (sj , sj+1 ) EV ,
Oi sj = Oi sj+1 . hand, sj |= iff
sj+1 |= equivalent i-local formula. Hence, (F, sj+1 ) |=
desired.
5. suffice show (F, s) |= Ki (1 2 ) (F, s) |= Ki 1 , (F, s) |=
Ki 2 . Assume (F, s) |= Ki (1 2 ) (F, s) |= Ki 1 , item 3 Definition 9 get that, s0 F s0 Oi = Oi , (F, s0 ) |= (1 2 )
(F, s0 ) |= 1 . However, item 2 Definition 9, get (F, s0 ) |= 2
(F, s0 ) |= (1 2 ) (F, s0 ) |= 1 . Therefore, get that, s0 F
s0 Oi = Oi , (F, s0 ) |= 2 . follows immediately (F, s) |= Ki 2 .
684

fiVariable Forgetting Reasoning Knowledge

6. item shown way proof item 5.
7. suffices prove state s00 state s0 Oi =
s0 Oi s0 EV s00 , get sEV s00 , follows immediately fact
EV transitive closure EV . 2
3.2 Relationship S5 Kripke Structure
Given knowledge structure F = (V, , O1 , , ), let (F) Kripke structure
(W, , K1 , , Kn ),
1. W set states F;
2. w W , assignment (w) w;
3. agent assignments w, w0 W , wKi w0 iff w Oi = w0 Oi .
following proposition indicates knowledge structure viewed specific Kripke structure.
Proposition 11 Given knowledge structure F, state F, formula LC
n (V ),
(F, s) |= iff (M (F), s) |= .
Proof: Immediately definition satisfaction relationship scenario
formula situation formula. 2
Proposition 11, conclude formula LC
n satisfiable knowledge
structure, formula also satisfiable Kripke structure. following
proposition Lemma 7, get formula LC
n satisfiable Kripke
structure, formula also satisfiable knowledge structure.
Proposition 12 finite S5n Kripke structure propositional variable set V
possible world w , exists knowledge structure FM state sw F
that, every formula LC
n (V ), (FM , sw ) |= iff (M, w) |= .
Proof: Let = (W, , R1 , , Rn ), W finite set R1 , , Rn equivalence
relations. Let O1 , , sets new propositional variables
1. O1 , , finite pairwise disjoint;
2. (0 < n), number subsets Oi less
equivalence classes Ri .
latter condition, is, i, function gi : W 7 2Oi
w1 , w2 W , gi (w1 ) gi (w2 ) subset Oi iff w1 w2
equivalence class Ri .

0
Let V 0 = V 0<in Oi . define function g : W 7 2V follows. possible
world w W ,
[
g(w) = {v V | (w)(v) = true}
gi (w).
0<in

following two claims hold:
685

fiSu, Sattar, Lv, & Zhang

C1 w1 , w2 W , (0 < n), g(w1 ) Oi = g(w2 ) Oi iff
w1 Ri w2 .
C2 w W v V , v g(w) iff (w)(v) = true.
Let
= { | V 0 , g(w) |= w W }.
get knowledge structure
FM = (V 0 , , O1 , , ).
show following claim:
C3 every V 0 , state FM iff = g(w) w W.
part claim C3 easy prove. = g(w0 ) w0 W ,
definition , g(w0 ) |= hence g(w0 ) state FM . show
part, assume every w W , 6= g(w). Then, every w W ,
V
exists w V 0 |= w g(w) |= w . Therefore, |= wW w .
W
W
Moreover, that, every w0 W , g(w0 ) |= wW w , hence wW w
. Consequently, 6|= hence state FM .
complete proof, suffices show, every LC
n (V ), (FM , g(w)) |=
iff (M, w) |= . conditions C1, C2 C3, induction .
base case, assume propositional variable, say p. Then, condition C2,
(FM , g(w)) |= p iff p g(w) iff (w)(p) = true iff (M, w) |= p.
Suppose propositional variable claim holds every subformula
. three cases:
1. form . case dealt definitions satisfaction
relations directly.
2. form Ki . case, (FM , g(w)) |= Ki iff (FM , s) |= states
FM g(w) Oi = Oi . condition C3, (FM , g(w)) |= Ki
iff (FM , g(w0 )) |= w0 W g(w) Oi = g(w0 ) Oi . condition C1,
(FM , g(w)) |= Ki iff (FM , g(w0 )) |= w0 W wRi w0 .
Therefore, induction assumption, (FM , g(w)) |= Ki iff (M, w0 ) |=
w0 W wRi w0 . right part (M, w) |= Ki .
3. form C . Recall that, arbitrary two states s0 FM , (s, s0 ) EV
iff exists Oi = s0 Oi . condition C1, w1 , w2 W ,
(g(w1 ), g(w2 )) EV iff (w1 , w2 )

[

Ri .






EV transitive closure EV , C
Ri , condition C3
get

(g(w1 ), g(w2 )) EV iff (w1 , w2 ) C

w1 , w2 W .
686

fiVariable Forgetting Reasoning Knowledge

want show (FM , g(w)) |= C iff (M, w) |= C . one hand, (FM , g(w)) |=
C iff states FM (g(w), s) EV , (FM , s) |= . condition C3,
(FM , g(w)) |= C iff w0 W (g(w), g(w0 )) EV .
. Therefore,
hand, (M, w) |= C iff w0 W (w, w0 ) C
conclude (FM , g(w)) |= C iff (M, w) |= C discussion. 2
Propositions 11 12 show satisfiability issue formula language
multi-agent S5 common knowledge modality whatever satisfiability
meant w.r.t. standard Kripke structure w.r.t. knowledge structure.
3.3 Knowledge Weakest Sufficient Conditions
following theorem establishes bridge notion knowledge notion
weakest sufficient strongest necessary conditions.
Theorem 13 Let V finite set variables, F = (V, , O1 , , ) knowledge structure, propositional formula L(V ), agent i, W SCi SN Ci weakest
sufficient condition strongest necessary condition Oi respectively.
Then, state F,
(F, s) |= Ki W SCi

(F, s) |= Ki SN Ci .
Proof: show (F, s) |= Ki W SCi , part comes straightforward way duality WSCs SNCs. W SCi sufficient condition
, |= W SCi . Let conjunction formulas
, |= (W SCi ), leads (F, s) |= Ki W SCi Ki (by
item 5 Lemma 10.) W SCi i-local, Lemma 10 (item 3) again,
(F, s) |= W SCi Ki W SCi . Hence, (F, s) |= W SCi Ki .
show direction (F, s) |= Ki W SCi , consider formula (V
Oi )( ), above. Proposition 6, |= (V Oi )(
) W SCi . hand, know (F, s) |= Ki (V Oi )( )
definition Ki . proves (F, s) |= Ki W SCi . 2
following corollary characterizes subjective formulas Ki (where objective)
satisfied given knowledge structure.
Corollary 14 Let V finite set variables, F = (V, {}, O1 , , ) knowledge
structure n agents, formula V . Then, every state F,
(F, s) |= Ki (V Oi )( ).
Proof:

Immediately Theorem 13 Proposition 6. 2

consider communication scenario Alice Bob
Example 15 :
addressed section 1 again. show system deal knowledge
reasoning issue scenario, define knowledge structure F follows:
F = (V, {}, OA , OB ),
687

fiSu, Sattar, Lv, & Zhang


OA = {Alice send msg, Alice recv ack},
OB = {Bob recv msg, Bob send ack},
V = OA OB ,
conjunction following three formulas:
Bob recv msg Alice send msg,
Bob send ack Bob recv msg,
Alice recv ack Bob send ack,
given state F
s=




Alice send msg,



Alice recv ack,


Bob recv msg,




Bob send ack






,

would like know whether Alice knows Bob received message. Consider
formula
(
)
Bob recv msg,

( Bob recv msg).
Bob send ack
Definition 1, formula simplified Alice recv ack, which, obviously,
satisfied scenario (F, s), i. e. ,
(F, s) |= Alice recv ack.
Corollary 14,
(F, s) |= KA Bob recv msg.
item 3 lemma 10, follows
(F, s) |= KA Alice send msg

(F, s) |= KA Alice recv ack,
indicates Alice knows sent message knows received
acknowledgement Bob. 2
Given set states knowledge structure F formula , (F, S) |= ,
mean S, (F, s) |= . following proposition presents alternative
way compute agents knowledge.
688

fiVariable Forgetting Reasoning Knowledge

Proposition 16 Let V finite set variables, F = (V, , O1 , , ) knowledge

structure n agents, formula V , formula LC
n . Suppose SN Ci
strongest necessary condition Oi , denotes set states
F (F, s) |= , SSN C denotes set states (F, s) |=


SN Ci . Then, agent i,
(F, ) |= Ki iff (F, SSN C ) |= .


Proof: Let S1 set states satisfying (F, s) |= (V Oi )( ).
|= SN Ci (V Oi )( ), S1 = SSN C . Also easy see state


F, S1 iff state s0 F s0 |= Oi = s0 Oi . Therefore
(F, ) |= Ki iff S1 {s | (F, s) |= }. leads (F, ) |= Ki iff (F, S1 ) |=
iff (F, SSN C ) |= . 2


intuitive meaning behind Proposition 16 know current
state , know agent knowledge (or agent observations)
strongest necessary condition Oi .
following proposition provides method determined whether formula
nested depth knowledge operators (like Ki1 Kik , propositional formula)
always true states, given proposition formula true.
Proposition 17 Let V finite set variables, F = (V, {}, O1 , , ) knowledge
structure n agents, two formulas V , denotes set states
F (F, s) |= . Then, group agents i1 , , ik , (F, ) |=
Ki1 Kik holds iff
|= k
k defined inductively follows:
1 = (V Oi1 )( );
j < k,
j+1 = (V Oij+1 )( j ).
Proof: show proposition induction nested depth knowledge operations. base case implied directly Proposition 16. Assume claim holds
cases nested depth k, want show also holds nested depth
k + 1, i. e. ,
(F, ) |= Ki1 Kik+1 iff |= k+1 .
Proposition 16,
(F, ) |= Ki1 Kik+1 iff (F, S1 ) |= Ki2 Kik+1 .
inductive assumption,
(F, S1 ) |= Ki2 Kik+1 iff |= k+1 .
689

fiSu, Sattar, Lv, & Zhang

Combining two assertions above, get
(F, ) |= Ki1 Kik+1 iff |= k+1 .
2
consider case nested depth knowledge operators
2, get following corollary.
Corollary 18 Let V, F, , Proposition 17. Then, agent
agent j,
1. (F, ) |= Ki holds iff
|= ( (V Oi )( )) ;
2. (F, ) |= Kj Ki holds iff
|= ( (V Oi )( (V Oj )( ))) .
Proof:

Immediately Proposition 17. 2

illustrated analysis security protocols (i.e. Section 6), part 2
Corollary 18 useful verifying protocol specifications nested knowledge operators.
Given background knowledge base , face task testing whether Kj Ki holds
states satisfying , part 2 Corollary 18, first get 1 = (V Oj )( ),
strongest necessary condition Oj . know agent
j observes . compute 2 = (V Oi )( 1 ), i. e. , strongest necessary
condition 1 Oi is, viewpoint agent j, agent observes.
way, task checking Kj Ki reduced task checking 2 .
following corollary gives two methods check truth Ki (where
propositional formula) states given formula true. One via
strongest necessary condition via weakest sufficient condition
.
Corollary 19 Let V finite set propositional variables F = (V, {}, O1 , , )
knowledge structure n agents, two formulas V . Suppose denotes
set states F (F, s) |= , SN Ci W SCi strongest
necessary condition Oi weakest sufficient condition Oi {}
respectively.
1. (F, ) |= Ki iff |= ( ) W SCi ;
2. (F, ) |= Ki iff |= ( SN Ci ) .
Proof: first part corollary follows Theorem 13 Lemma 10,
second part follows immediately Proposition 16. 2
analysis security protocols, observe often, seems efficient
check agents knowledge via second part Corollary 19 rather via first
part. may always true applications (e.g. see example
muddy children puzzle next section).
690

fiVariable Forgetting Reasoning Knowledge

4. Common Knowledge
Common knowledge special kind knowledge group agents, plays
important role reasoning knowledge (Fagin et al., 1995). group agents
commonly know agents know , know know ,
know know know , ad infinitum. recall
common knowledge characterized terms Kripke structures. Given Kripke
structure = (W, , K1 , , Kn ), group agents commonly know ( modal
logic language, C true ) world w iff true worlds w0 (w, w0 ) C ,

C denotes transitive closure Ki .
section, generalize concept weakest sufficient strongest necessary
conditions used compute common knowledge.
4.1 Generalized Weakest Sufficient Strongest Necessary Conditions
following gives generalized notion weakest sufficient conditions strongest necessary conditions.
Definition 20 Given set formulas V background knowledge base. Let
formula V , V nonempty set subsets V .
formula called V-definable (or simply called V-definable
confusion context), P V, formula P P
|= P .
formula called V-sufficient condition V-definable
|= . called weakest V-sufficient condition
V-sufficient condition , V-sufficient condition 0
, |= 0 .
Similarly, formula called V-necessary condition V-definable
|= . called strongest V-necessary condition
V-necessary condition , V-necessary condition 0
, |= 0 .
notice notion V-definability introduced simple elaboration
notion V-definability given work Lang Marquis (1998): V-definable
iff V -definable V V. Moreover, easy see
formulas implied inconsistent exactly formulas -definable ,
definability exhibits monotonicity property: V -definable ,
V 0 -definable superset V 0 V (Lang & Marquis, 1998). Observe also
V -definable iff V -definable , extends trivially
V-definability.
following lemma says notions weakest V-sufficient conditions strongest
V-necessary ones dual other.
Lemma 21 Given set formulas V background knowledge base, V
set subsets V . Let formulas V . Then, weakest
691

fiSu, Sattar, Lv, & Zhang

V-sufficient condition iff strongest V-necessary condition
.
Proof:

Straightforward duality WSCs SNCs. 2

give intuition motivation definition, let us consider following example.
Example 22: Imagine two babies, say Marry Peter, playing
dog. Suppose propositions dog moderately satisfied (denoted m, short)
dog full(f ) understandable Marry, propositions dog
hungry (h) dog unhappy(u) understandable Peter.
Let = {h u, (m f ), (m f ) h}, V1 = {m, f }, V2 = {h, u}, V = {V1 , V2 }.
show
1. h V-definable ;
2. h weakest V-sufficient condition u ;
3. h strongest V-necessary condition u .
first claim easy check definition. last two claims follow immediately
prove V-definable propositions f alse, true, h h (up
logical equivalence ). 8 propositions V1 logical equivalence.
8 propositions are: true, f alse, m, m, f, f, f, f . Similarly, 8
propositions V2 logical equivalence , i.e., true, f alse, h, h, u, u, h
u, h u. However, find, two classes propositions, 4 pairs
equivalence relations , i.e., |= true true, |= f alse f alse, |= (m f )
h, |= (m f ) h. Therefore, V-definable propositions f alse,
true, h h (up logical equivalence ). 2
Example 23: recall background knowledge CS communication
scenario Alice Bob introduction section. CS set following
three formulas:
Bob recv msg Alice send msg
Bob send ack Bob recv msg
Alice recv ack Bob send ack
Let
OA = {Alice send msg, Alice recv ack},
OB = {Bob recv msg, Bob send ack},
VAB = {OA , OB }.
Clearly, formula logically implied CS inconsistent CS ,
VAB -definable CS . Moreover, Example 22, able check
VAB -definable formulas implied CS inconsistent CS .
Therefore, given formula , weakest VAB -sufficient condition CS implied
CS CS |= , inconsistent CS . 2
692

fiVariable Forgetting Reasoning Knowledge

Let set formulas, V set propositional variables, V set subsets
V . following proposition gives existence weakest V-sufficient strongest
V-necessary conditions. given formula V , weakest V-sufficient condition 1
strongest V-necessary condition 2 obtained proposition. Indeed,
set assignments satisfying 1 assignments satisfying 2 given
terms relation EV .
Proposition 24 Given finite set V propositional variables, set formulas V
background knowledge base, formula V , set V subsets V . Denote

0
SW
SC set assignments V |= , assignments satisfying
0

0

(s, ) EV , |= . Also denote SSN C set assignments V
|= , exists s0 s0 |= , s0 |= (s, s0 ) EV . Then, following
two points hold.

formula satisfied exactly assignments SW
SC , formula
weakest V-sufficient condition ;

formula satisfied exactly assignments SSN
C , formula
strongest V-necessary condition .

Proof: first prove former point, show Lemma 21. Let 1

propositional formula V that, assignments s, |= 1 iff SW
SC .


Then, every assignment SW SC , |= (s, s) EV . Thus, 1 |= .
remark arbitrarily given formula V assignment V , |=
(V P ) iff assignments s0 V P = s0 P , s0 |= .
prove 1 V-definable, show that, P V, 1 |= (V P )1 ,
implies 1 equivalent formula (V P )1 P . prove 1 |= (V P )1 ,

0
semantical way, suffices show that, every assignment SW
SC |= ,
0
0

0
P = P , SW SC . Let given suppose P = s0 P .
Then, (s, s0 ) EV . Given assignment |= , (s0 , t) EV , (s, t) EV

(s, s0 ) EV . Thus, s0 SW
SC . proves 1 V-definable.
show 1 weakest V-sufficient condition . Suppose Vdefinable sufficient condition , want prove |= 1 .
semantical argument proof follows. Let assignment |=

0
0
, must show SW
SC , i.e., every assignment |=
0

0
0
(s, ) EV , |= . |= , suffices show |= . condition
(s, s0 ) EV , finite sequence assignments s0 , , sk sj |= s0 =
sk = s0 , every j < k, (sj , sj+1 ) EV . V-definability , know
every j < k, sj |= implies sj+1 |= . Thus, s0 |= induction.
prove second point proposition Lemma 21. Let 2 proposi
tional formula V that, assignments s, |= 2 iff SSN
C . Let
conjunction formulas . Then, |= 2 iff assignments s0 s0 |=
sEV s0 , s0 |= . Thus, first point proposition,
2 weakest V-sufficient condition . Thus, 2 hence 2 strongest
V-necessary condition according Lemma 21. 2
proposition thought semantical characterization weakest
V-sufficient strongest V-necessary conditions.
693

fiSu, Sattar, Lv, & Zhang

4.2 Characterizations Least Greatest Fixed Points
investigate computation weakest V-sufficient strongest V-necessary conditions using notions least greatest fixed points operator,
introduced follows. Let V set propositional variables, operator (or
mapping) set propositional formulas V set propositional formulas
V . say fixed point , |= () . say 0 greatest fixed
point , 0 fixed point every fixed point , |= 0 .
Clearly, two greatest fixed points logically equivalent other. Thus, denote
greatest fixed point gfpZ(Z). Similarly, say 0 least fixed point ,
0 fixed point every fixed point , |= 0 . denote
least fixed point lfpZ(Z). say monotonic, every two formulas 1
2 |= 1 2 , |= (1 ) (2 ). finite set V propositional
variables monotonic, exists least fixed point greatest fixed point
(Tarski, 1955).
Theorem 25 Let V finite set variables, F = (V, {}, O1 , , ) knowledge
structure, formula V , {1, , n}, V = {Oi | }. Assume 1
2 two operators
1 (Z) =

^

(x Oi )( Z)




2 (Z) =

_

(x Oi )( Z).



Then,
weakest V -sufficient condition {} equivalent gfp Z( 1 (Z));

strongest V -necessary condition {} equivalent lfp Z( 2 (Z)).
weakest V -sufficient condition {}. Note
Proof: Let W SC

operator ( 1 (Z)) monotonic thus exists greatest fixed point it. Let
1 = gfp Z( 1 (Z)). prove first point theorem, must show
.
|= W SC
1
. purpose, need prove
first show |= W SC
1
( (true));
1. |= W SC
1
, |= W SC ( ()).
2. formulas V , |= W SC
1


first point trivially true 1 (true) equivalent true W SC

sufficient condition {}. show second point, suppose |= W SC .
. Then, |= .
, let formula Oi |= W SC


follows |= ( ) hence |= (V Oi )( )

depend variables (V Oi ). So, that, , |= W SC
(V Oi )( ). conclusion second point follows immediately.
, |= ( ) W SC . suffices show
show |= 1 W SC
1

1 V -sufficient condition {}, is,

694

fiVariable Forgetting Reasoning Knowledge

1. 1 V -definable;
2. |= ( 1 ) .
fact 1 fixed point operator ( 1 (Z)),
|= 1 (

^

(x Oi )( 1 )).



follows |= 1 , hence |= ( 1 ) . show point,
, need prove 1 equivalent formula Oi . above,
1 (V Oi )( 1 ). follows |= ( 1 ) (V Oi )( 1 ),
hence
|= ( 1 ) (V Oi )( 1 )
|= (V Oi )( 1 ) ( 1 ) holds trivially. Thus ( 1 ) equivalent
(V Oi )( 1 ), Oi . completes first point
conclusion theorem.
show second point theorem using first point Lemma 21.
strongest V -necessary condition {}. Lemma 21,
Let SN C


SN C weakest V -sufficient condition {}. Thus, first point
equivalent gfp Z( (Z)) . Hence, SN C
theorem, SN C
1

equivalent gfp Z( 1 (Z)) . However, gfp Z( 1 (Z)) logically
equivalent lfp Z(( 1 (Z))), turn equivalent lfp Z( 2 (Z)).
completes second point theorem. 2
4.3 Common Knowledge Weakest V-sufficient Conditions
Given set agents family V observable variable sets agents,
investigate relationship common knowledge weakest V -sufficient
strongest V -necessary conditions.
Theorem 26 Let V finite set variables, F = (V, , O1 , , ) knowledge struc SN C
ture, {1, , n}, V = {Oi | }, formula V , W SC

weakest V -sufficient condition strongest V -necessary condition
respectively. Then, every state F,

(F, s) |= C W SC



(F, s) |= C SN C
.
,
Proof: show first part theorem, i.e., (F, s) |= C W SC
sufficient
Lemma 21 get part immediately. W SC

condition , |= W SC . Let conjunction formulas
), leads (F, s) |= C W SC C (by
, |= (W SC



V -definable, have, point 4 Lemma 10,
point 6 Lemma 10). W SC

C W SC . Hence, (F, s) |= W SC C .
(F, s) |= W SC





695

fiSu, Sattar, Lv, & Zhang

, consider formula
show direction (F, s) |= C W SC
1
proof Theorem 25, i.e., greatest fixed point operator

(Z) =

^

(V Oi )( Z).


Theorem 25, suffices show (F, s) |=
already (F, s) |= 1 W SC
C 1 . greatest fixed point 1 operator obtained
finite iteration operator starting point (true), need prove

1. F |= C (true);
2. arbitrary propositional formula V , F |= C , F |= C
().
first point trivially true (true) equivalent . prove second,
suppose F |= C . Then, , F |= Ki (C ). Thus,
F |= C Ki points 5 7 Lemma 10. Hence, F |= C (V Oi )( )
V
(by Corollary 14). follows F |= C (V Oi )( ) hence F |=
C (). thus get F |= C 1 . completes proof. 2
Proposition 27 Given V , F, , V , defined Theorem 26. Let formula

. Denote
V . Assume strongest V -necessary condition SN C
set states F (F, s) |= , SSN C set states



. Then,
(F, s) |= SN C

(F, ) |= C iff (F, SSN C ) |= .


Proof: Let S1 set states state s0 s0 |=
(s0 , s) V . (F, ) |= C iff every S1 , (F, s) |= . leads
(F, ) |= C iff (F, S1 ) |= . hand, Proposition 24,
S1 = SSN C . conclusion proposition follows immediately. 2


Note that, Proposition 27, propositional formula, (F, ) |= C

iff |= SN C
. Moreover, Theorem 26, (F, ) |= C iff |=

weakest V -sufficient .
W SC , W SC


5. Adding Public Announcement Operator
recent trend extending epistemic logic dynamic operators evolution knowledge expressed. basic extension public announcement
logic (PAL), obtained adding operator truthful public announcements.
original version PAL proposed Plaza (1989). section, show
public announcement operator conveniently dealt via notion knowledge
structure.
696

fiVariable Forgetting Reasoning Knowledge

5.1 Public Announcement Logic
Given set agents = {1, . . . , n} set V propositional variables. language
public announcement logic (P ALn ) inductively defined
::= p|| |Ki |C |[]
p V , A.
words, P ALn obtained epistemic logic LC
n (V ) adding public announcement operator [] formula . Formula [] means public announcement , formula true.
give semantics public announcement logic Kripke model. Given
Kripke structure = (W, , K1 , . . . , Kn ), semantics new operators defined
follows.
M, w |= [] iff M, w |= implies | , w |= , | Kripke structure
| = (W 0 , 0 , K10 , . . . , Kn0 )
W 0 = {w W |M, w |= },
0 (w0 )(p) = (w0 )(p) w0 W 0 p V ,
Ki0 = Ki (W 0 W 0 ) A.
sentences become false immediately announcement
them. Consider, example, sentence p true commonly known
true . announcement sentence agents learn p therefore p
commonly known. modelled public announcement logic valid formula
[], = p C p. see validity, let (M, w) arbitrary situation.
M, w |= ,then M, w |= p, implies | , w |= C p, therefore | , w |= .
5.2 Semantics Knowledge Structure
semantics public announcement logic conveniently characterized notion
knowledge structure. define satisfaction relationship |= scenario (F, s)
formula P ALn . need consider formulas form []; cases
Definition 9.
Let V finite set propositional variables F = (, V, O1 , , ). semantics
definition new operators follows. First, let F| knowledge structure
({}, V, O1 , , ), propositional formula V (F, s) |= iff
satisfies . V finite set, propositional formula always exists.
Then, set (F, s) |= [] iff (F, s) |= implies (F| , s) |= .
remark formula equivalent propositional one 0 knowledge structure
F, i.e., F |= 0 propositional formula 0 , simply define F|
( {0 }, V, O1 , , ).
following proposition indicates semantics public announcement logic
knowledge structure coincides Kripke model.

697

fiSu, Sattar, Lv, & Zhang

Proposition 28 (1) Let V finite set propositional variables F = (, V, O1 , , ).
every state F every formula P ALn , (F, s) |= iff situation (M (F), s) |= . (2) finite S5n Kripke structure possible world w
, knowledge structure FM state sw F that, every formula
P ALn , (FM , sw ) |= iff (M, w) |= .
Proof: (1) Let us proceed induction structure formula . consider
case form []; cases straightforward definitions.
definition, (F, s) |= [] iff (F, s) |= implies (F| , s) |= .
Thus, inductive assumption, (F, s) |= [] iff (M (F), s) |= implies
(M (F| ), s) |= . want show (F, s) |= [] iff (M (F), s) |= []. suffices
show (F| ) equals (F)| (M (F), s) |= [] iff (M (F), s) |= implies
(M (F)| , s) |= .
First, set possible states (F| ) equals set states s0 F
(F, s0 ) |= . inductive assumption, (F, s0 ) |= iff (M (F), s0 ) |= . Thus, set
possible states (F| ) equals set states s0 F (M (F), s0 ) |= ,
hence equals set possible states (F)| . Second, s0 F
(M (F), s0 ) |= , (F| ) (s0 ) = s0 (F )| (s0 ) = (F ) (s0 ) = s0 . Hence (F | ) =
(F )| . Finally, states s1 s2 F (M (F), s1 ) |= (M (F), s2 ) |= ,
(F| )
(F )
(s1 , s2 ) Ki
iff (s1 , s2 ) Ki
iff s1 Oi = s2 Oi . Moreover,
(F )|
(F | )
(F )|
. completes
(s1 , s2 ) Ki
iff s1 Oi = s2 Oi . Therefore, Ki
= Ki
proof (F| ) = (F)| .
(2) Suppose = (W0 , 0 , R1 , , Rn ), W0 finite set R1 , , Rn
equivalence relations. assume also set propositional variables V0 .
Let O1 , , sets new propositional variables
1. O1 , , finite pairwise disjoint;
2. (0 < n), number subsets Oi less
equivalence classes Ri .
latter condition, is, i, function gi : W0 7 2Oi
w1 , w2 W0 , gi (w1 ) gi (w2 ) subset Oi iff w1 w2
equivalence class Ri .

Let V = V0 0<in Oi . define function g : W0 7 2V follows. possible
world w W0 ,
[
g(w) = {v V | (w)(v) = true}
gi (w).
0<in

following two claims hold:
C1 w1 , w2 W0 , (0 < n), g(w1 ) Oi = g(w2 ) Oi iff
w1 Ri w2 .
C2 w W0 v V0 , v g(w) iff (w)(v) = true.
W W0 , let
W = { | V, g(w) |= w W }.
698

fiVariable Forgetting Reasoning Knowledge

get knowledge structure
FW = (V, W , O1 , , ).
show following claim:
C3 every V , state FW iff = g(w) w W.
part claim C3 easy prove. = g(w0 ) w0 W ,
definition W , g(w0 ) |= W hence g(w0 ) state FM . show
part, assume every w W , 6= g(w). Then, every w W , exists
V
w V |= w g(w) |= w . Therefore, |= wW w . Moreover,
W
W
that, every w0 W , g(w0 ) |= wW w , hence wW w W . Consequently,
6|= W hence state FW .
complete proof second part, suffices show, every P ALn ,
(FW , g(w)) |= iff (M |W , w) |= , |W Kripke structure | =
(W, , R10 , . . . , Rn0 )
(w)(p) = 0 (w)(p) w W p V0 ,
Ri0 = Ri (W 0 W 0 ) 0 < n.
claims C1, C2 C3, induction . Again, consider
case form []; cases dealt way
proof Proposition 12.
first show knowledge structure FW | equivalent FW 0 ,
W 0 = {w0 W | MW , w |= }.
two knowledge structures set V propositional variables and,
agent i, set Oi observable variables agent i, need prove
set states. assignment V state FW | iff state
FW FW , |= . Thus, claim C3, state FW | iff = g(w0 ) w0 W
FW , g(w0 ) |= . hand, have, claim C3 again, assignment
state FW 0 iff = g(w0 ) w0 W 0 , i.e., w0 W MW , w0 |= . However,
induction assumption, FW , g(w0 ) |= iff MW , w0 |= . Therefore, knowledge structures
FW | FW 0 set states.
show (FW , g(w)) |= [] iff (M |W , w) |= [], have, induction assumption, (FW , g(w)) |= iff (M |W , w) |= . Also, claim proved
above, (FW | , g(w)) |= iff (FW 0 , g(w)) |= . induction assumption again, (FW 0 , g(w)) |= iff MW 0 , w |= . definition W 0 ,
MW | , w |= . Hence, (FW | , g(w)) |= iff MW | , w |= . Therefore, semantics
announcement operators Kripke structure knowledge structure,
(FW , g(w)) |= [] iff (M |W , w) |= []. 2
proposition generalization Propositions 11 12 PALn ,
shows satisfiability issue formula language multi-agent S5
announcement operators whatever satisfiability meant w.r.t. standard
Kripke structure w.r.t. knowledge structure.
Notice that, every formula P ALn , get equivalent propositional formula.
specifically, following:
699

fiSu, Sattar, Lv, & Zhang

Remark 29 Let V finite set propositional variables F = ({}, V, O1 , , ).
Given formula P ALn , define propositional formula induction
structure :
propositional formula, = .
b e = .
bKi e = (V Oi )( ).
Let {1, , n}, V = {Oi | }.


bC e = W SC


W SC

weakest V -sufficient condition .

b[]e = bebe



Then, every P ALn , F |= .

6. Complexity Results
interested following problem: given knowledge structure F formula
language epistemic logic, whether formula realized structure F. kind
problem called realization problem. section, examine inherent difficulty
realization problem terms computational complexity. general case,
problem PSPACE-Complete; however, interesting subset language,
reduced co-NP.
Let L epistemic logic (or language). realization problem L is, given
knowledge structure F formula L, determine whether F |= holds.
realization problem closely related model checking problem: given
epistemic formula Kripke structure , determine whether |= . checking
definition Kripke structure semantics epistemic logic, see model
checking problem solved polynomial time (with respect input size (| |
+ | |). determine whether formula realized knowledge structure F
first translating knowledge structure F Kripke structure checking |= .
However, resulting algorithm exponential space. size
corresponding Kripke structure exponential respect knowledge structure
F.
number algorithms model checking epistemic specifications computational complexity related realization problems studied (van der Meyden,
1998). However, like Kripke structure, semantics framework adopt list
global states explicitly. result, size input concerned decision problem
large.
Proposition 30 realization problem Ln PSPACE-complete.
700

fiVariable Forgetting Reasoning Knowledge

Proof: proposition two parts: PSPACE-easiness PSPACE-hardness.
PSPACE-easiness part means algorithm determines polynomial
space whether epistemic formula Ln realized knowledge structure F.
PSPACE-completeness indicates PSPACE-hard problem, say satisfiability
problem quantified propositional formulas (QBF) (Stockmeyer & Meyer, 1973),
effectively reduced realization problem consider.
difficult see PSPACE-easiness. Given knowledge structure epistemic
formula , Corollary 14, replace knowledge modalities propositional quantifiers
formula . So, problem whether realized F reduced determine
whether quantified Boolean formulas valid. latter done polynomial space
(Stockmeyer & Meyer, 1973).
PSPACE-hardness, suffices show every QBF formula
p1 q2 p2 q3 pm1 qm A(p1 , q2 , p2 , q3 , pm1 , qm ),
construct knowledge structure F
` p1 q2 p2 q3 pm1 qm A(p1 , q2 , p2 , pm1 , qm )
iff
F |= d1 d2 (K1 K2 )m1 (dm A(p1 , q2 , p2 , q3 , pm1 , qm )).
Let F = (V, {}, O1 , O2 ),
1. V = {c} {d1 , , dm } {d01 , , d0m } {p1 , , pm } {q1 , , qm }
2. conjunction following formulas
(a)

^

(dj+1 dj ) (d0j+1 d0j )

j<m

(b)


^


^

dj dj+1

j<m

(pi qi )

i6=j

(c)

^

c

(dj d0j )

j<m+1

(d)



c (dm1 dm ) d0m

^
j<m1

3. O1 = {c} {d1 , , dm } {q1 , , qm }
4. O2 = {d01 , , d0m } {p1 , , pm }
701







(dj dj+1 ) (d0j+1 d0j+2 )

fiSu, Sattar, Lv, & Zhang

picture, two agents: agents 1 2. assign every state
integer number, called depth state convenience. every j, dj expresses
depth state least j. Propositions d1 , , dm observable agent
1, agent 2. Nevertheless, agent 2 observe d01 , , d0m , closely
related d1 , , dm . formula item 2c indicates d01 , , d0m
d1 , , dm c holds, formula item 2d says that, c hold, depth
expressed d1 , , dm less d01 , , d0m difference 1. formula
item 2b implies that, condition depth state exactly j,
pj unobservable agent 1 qj unobservable agent 2.
order show
` p1 q2 p2 q3 pm1 qm A(p1 , q2 , p2 , pm1 , qm )
implies
F |= d1 d2 (K1 K2 )m1 (dm A(p1 , q2 , p2 , q3 , pm1 , qm )),
suffices prove that, every j propositional formula p1 , , pm ,
q1 , , q ,
F |= dj dj+1 pj qj+1 K1 K2 (dj+1 dj+2 )
so, need show
F |= dj dj+1 pj K1 (dj dj+1 )

F |= dj dj+1 qj+1 K2 (dj+1 dj+2 ).
direction, notice that, l < 1,
F |= d1 d2 (K1 K2 )l dl+2 .
also notice that, 1 < m0 m,
F |= K1 K2 dm0 dm0 1

F |= dm0 1 dm0 K1 K2 (dm0 ) pm0 1 qm0 .
applying three claims repeatedly, obtain
F |= d1 d2 (K1 K2 )m1 (dm ) p1 q2 p2 q3 pm1 qm .
Therefore,
F |= d1 d2 (K1 K2 )m1 (dm )
p1 q2 p2 q3 pm1 qm satisfiable F d1 d2 .
However, QBF formula p1 q2 p2 q3 pm1 qm contain free variable, immediately conclude QBF formula valid QBF formula
satisfiable F. 2
702

fiVariable Forgetting Reasoning Knowledge

Remark 29, see that, language formulas P ALn without common
knowledge operators, realization problem reduced problem validness
problem quantified Boolean formulas, hence PSPACE-complete Proposition 30.
conjecture realization problem also PSPACE-complete LC
n P ALn .
Proposition 30 indicates realization problem general case hard
computer solve. Thus, interesting give special cases lower computational
complexity. Let L+K
fragment positive formulas Ln . consists formulas
n
negation applied propositional formulas modalities
restricted K1 , , Kn . instance, formula K1 K2 pK1 K2 p (where p propositional
formula) belongs L+K
n , formula K1 K2 p K1 K2 p not.
sublanguage L+K
interesting sufficient represent important
n
security properties security protocols. Moreover, shown following proposition,
complexity realization problem L+K
co-NP-complete.
n
Proposition 31 realization problem L+K
co-NP-complete.
n
Proof: well-known validity problem propositional formulas co-NPcomplete. easily get co-NP-hardness realization problem L+K
n ,
validity problem propositional formulas reduced realization problem
propositional formulas (considering case background knowledge base
tautology).
co-NP, show
hand, show realization problem L+K
n
reduced validity problem propositional formulas. Given knowledge structure
F formula L+K
n , translate propositional formula kkF (which
define below), realized F iff kkF valid, background
knowledge base knowledge structure F.
Suppose F = (V, {}, O1 , , ). every subformula Ki , introduce set
Vi new propositional variables | Vi |=| V Oi |.
propositional translation kkF inductively given follows.
1. propositional formula, kkF = .
2. conjunction form 1 2 ,
kkF = k1 kF k2 kF .
3. disjunction form 1 2 ,
kkF = k1 kF k2 kF .
4. form Ki ,
kkF = ( kkF )(

V Oi
),
Vi


( kkF )( V VO
) formula obtained ( kkF ) replacing


variables V Oi new ones Vi .
703

fiSu, Sattar, Lv, & Zhang

idea behind translation first translate formula quantified
propositional formula, quantifiers universal ones, eliminate
universal quantifiers introducing new variables.
Let V set new variables kkF . show correctness translation,
suffices show F |= V kkF .
prove claim induction .
trivial, propositional formula.
form 1 2 , claim obtained immediately induction
assumption.
form 1 2 , V (k1 kF k2 kF ) logically equivalent
V1 k1 kF V2 k2 kF , variables V1 appear V2 k2 kF
V2 V1 k1 kF . Thus, claim holds induction assumption.
Finally, form Ki ,
kkF = ( kkF )(

V Oi
).
Vi


Therefore, V = V Vi V kkF logically equivalent ( Vi V kkF )( V VO
).


Thus, induction assumption,
F |= V kkF ( Vi (

V Oi
)
Vi

hence
F |= V kkF ( (V Oi )).
Therefore, F |= V kkF Ki . 2
Proposition 31 implies that, arbitrary formula L+K
knowledge structure
n
F background knowledge base ,
F |= iff kkF unsatisfiable.
Thus, solve realization problem formulas L+K
using propositional
n
satisfiability solver.

7. Case Study: Muddy Children Puzzle
section, demonstrate framework applied practical problems
using example muddy children puzzle.
704

fiVariable Forgetting Reasoning Knowledge

7.1 Muddy Children Puzzle
muddy children puzzle well-known variant wise men puzzle. story goes
follows (Fagin et al., 1995): Imagine n children playing together. children,
say k them, get mud foreheads. see mud others
his/her forehead. Along comes father, says, least one mud
forehead. father asks following question, over:
know whether mud forehead?
Assuming children perceptive, intelligent, truthful, answer simultaneously, want show first (k 1) times father asks question,
say k th time children muddy foreheads answer Yes.
7.2 Modeling Muddy Children Puzzle
model muddy children puzzle, let mi propositional variable, means
child muddy (i < n). Denote V set {mi | < n}. Suppose assignment
s0 = {mi | < k} represents actual state: child 0, , child k 1 mud
foreheads; children not. captured scenario (F0 , s0 ),
F0 = (V, 0 , O0 , , On1 )
V = {mi | < n};
0 = ;
Oi = V {mi } < n.
V

Let = i<n Ki mi , indicates every child know whether
mud forehead. convenience, introduce, natural number l,
notations []l []0 = []l+1 = [][]l . properties want show
formally expressed P ALn :
W

[

W

[

j
i<n mi ][]

every 0 j < k 1,

k1 V
i<n mi ][]
i<k

Ki mi .

W

Formula [ i<n mi ][]j means children say j + 1th time
father asks question. particular, j = 0, condition 0 j < k 1 simplified
W
W
k > 1; resulting formula [ i<n mi ] says father announces i<n mi
W
V
every child says No. Formula [ i<n mi ][]k1 i<k Ki mi indicates k th time
children muddy foreheads answer Yes.
Therefore, want prove


(F0 , s0 ) |=


^

[

_





mi ][]j [

0j<k1 i<n

_

i<n

mi ][]k1

^

K mi .

i<k

check above, basically follow definition P AL semantics knowledge
structure. checking process, series Fj (0 < j k) knowledge structures
constructed F1 = F0 |W mi and, every j (0 < j < k), Fj+1 = Fj | .
i<n

705

fiSu, Sattar, Lv, & Zhang

Figure 1: Performances two algorithms muddy children puzzle
Specifically, that, step j k, get
Fj = (V, j , O0 , , On1 )
Oi = V {mi } < n, j defined follows:
W

step 1: 1 = {

i<n mi }.

V

step j + 1: Let b = i<n mi (j mi ). < n, Fj |=nKi
oi
b
b
mi (j mi ), Fj |= . Thus, may set j+1 = j .
Therefore, suffices verify, 0 < j < k < n, (Fj , s0 ) |= Ki mi , < k,
(Fk , s0 ) |= Ki mi .
7.3 Experimental Results
framework knowledge structure implemented using BDD library
(CUDD) developed Fabio Somenzi Colorado University. Notice BDD-based
QBF solvers satisfiability problems among best solvers nowadays. However,
experiments need compute represent serial Boolean functions
(say j ), decision problems solved general QBF solver.
check agents knowledge, implemented two different algorithms terms Part
1 2 Corollary 19 Section 3, respectively. Algorithm 1, based part
1 Corollary 19, seems much efficient Algorithm 2, based part 2
Corollary 19, particular example. reason follows. clear
main task algorithms check whether (Fj , s0 ) |= Ki mi . However, Algorithm 1s
method compute s0 |= mi (j mi ), Algorithm 2 compute |= mi (j
s0 ) mi . main reason Algorithm 1 much efficient particular

problem clear: mi (j mi ) simply equivalent j ( f
alse ). Assuming half
children muddy, Fig. 1 gives performances Pentium IV PC 2.4GHz,
512RAM. figure, x-axis number children, y-axis CPU
run time seconds.
706

fiVariable Forgetting Reasoning Knowledge

muddy children puzzle famous benchmark problem reasoning knowledge resolved proof-theoretic semantical approaches (Baltag et al., 1998;
Gerbrandy, 1999; Lomuscio, 1999). Proof-theoretic approaches depend efficient provers
multi-modal logics; semantical ones may suffer state-explosion problem.
approach essentially semantic one, give syntactical compact way
represent Kripke structures using knowledge structures, hence may avoid
state-explosion problem extent.

8. Application Verification Security Protocols
section, apply knowledge model security protocol verification. Security
protocols set credits parties deal distribution cryptographic
keys essential communication vulnerable networks. Authentication plays key
role security protocols. Subtle bugs lead attack often found protocols
used many years. presents challenge prove correctness
security protocol. Formal methods introduced establish prove whether
secure protocol satisfies certain authentication specification.
8.1 Background Authentication Protocols
Authentication protocols aim coordinate activity different parties (usually referred
principals) network. generally consist sequence message exchanges
whose format fixed advance must conformed to. Usually, principal take
part protocol run different ways, initiator responder ; often call
principal different roles. often principal take part several protocol
runs simultaneously different roles.
designers authentication protocols must conscious mind
message may intercepted someone malicious intention impersonate
honest principal. One key issues authentication ensure confidentiality,
is, prevent private information disclosed unauthorized entities. Another
issue avoid intruder impersonating principals. general, principal
ensure message receives created recently sent principal
claims sent it.
Cryptography fundamental element authentication. message transmitted
channel without cryptographic converting called plaintext. intention cryptography transform given message form unrecognizable anyone
except intended receiver. procedure called encryption corresponding
parameter known encryption key. encoded message referred ciphertext.
reverse procedure called decryption uses corresponding decryption key.
symmetric-key cryptography, also called secret-key cryptography, uses key
encryption decryption. asymmetric-key cryptography, also called
public-key cryptography, uses different keys encryption decryption. one
encryption public key generally available anyone. Corresponding
public key private key, decryption owned one principal.
707

fiSu, Sattar, Lv, & Zhang

8.2 Dolev-Yao Intruder Model
standard adversary model analysis security protocols introduced
Dolev Yao 1983 commonly known Dolev-Yao model (Dolev & Yao, 1983).
According model, set conservative assumptions made follows:
1. Messages considered indivisible abstract values instead sequences bits.
2. messages one principal principals must pass
adversary adversary acts general router communication.
3. adversary read, alter redirect message.
4. adversary decrypt message right keys,
compose new messages keys messages already possesses.
5. adversary perform statistical cryptanalytic attacks.
Although model drawback finding implementation dependent attacks,
simplifies protocol analysis. proved powerful modeling
adversary (Cervesato, 2001) simulate possible attackers.
8.3 Revised Needham-Schroeder Protocol
Lowe (1996) pointed out, Needham-Schroeder protocol problem lacking
identity responder fixed small modification. However,
clear revised version correct. approach provides method automatically
prove correctness security protocols instead finding bugs usual analysis
tools security protocols.
cryptography literature, revised Needham-Schroeder protocol described
follows:
1. B: {N a, A}Kb
2. B A: {B, N a, N b}Ka
3. B: {N b}Kb
B : notation sends B message B receives message
A. notation {M }K means encryption key K. Also, A, B
denote principal identifiers; Ka, Kb indicate, respectively, Bs public keys.
Moreover, N N b nonces newly generated unguessable values
B, respectively, guarantee freshness messages.
Two informal goals specifications protocol knows B knows said
N N fresh, B knows knows B said N b N b fresh .
analyze protocol, introduce B local histories protocol:
plays role initiator protocol, assumes B responsor,
local history
1. said {N a, A}KbA
708

fiVariable Forgetting Reasoning Knowledge

2. sees {B , N a, N bA }Ka
3. said {N bA }KbA
said means sent message , message containing ;
sees indicates receives got received messages; B
responsor protocol local view; KbA N bA are, local view,
responsors public key nonce, respectively.
B plays role responsor protocol, assumes initiator,
local history
1. B sees {N aB , AB }Kb
2. B said {B, N aB , N b}Ka
3. B sees {N b}Kb
AB initiator protocol Bs local observations; KaB N aB are,
Bs local view, initiators public key nonce, respectively.
main point analysis agent involved protocol,
agents real observations compatible so-called local history. example,
initiator protocol, sees {B, N aB , N b}Ka , according local
history protocol assumes B responsor protocol,
responsors nonce N b, responsors view, initiators nonce N (see
4th formula background knowledge below).
Let us see framework reasoning knowledge applied
protocol.
variable set VRN consists following atoms:
f resh(N a): Nonce N fresh.
f resh(N b): Nonce N b fresh.
role(Init, A): plays role initiator protocol.
role(Resp, B): B plays role responder protocol.
RespA = B: assumes responder protocol B.
InitB = A: B assumes initiator protocol A.
N aB = N a: B assumes partners nonce execution protocol
N a.
N bA = N b: assumes partners nonce execution protocol
N b.
said(B, N a): B said N sending message containing N a.
said(A, N b): said N b.
709

fiSu, Sattar, Lv, & Zhang

sees(B, {N a, A}Kb ): B sees {N a, A}Kb (possibly decrypting messages received.)
sees(A, {B, N aB , N b}Ka ): sees {B, N aB , N b}Ka .
background knowledge RN consists following formulas:




sees(B, {N a, A}Kb )


1. said(B, N a)
role(Resp, B)
f resh(N a)




sees(A, {B, N aB , N b}Ka )


2. said(A, N b)
role(Init, A)
f resh(N b)



3.




4.



5.


6.
7.

role(Resp, B)
sees(B, {N a, A}Kb )
said(B, N a)
f resh(N a)









InitB =
N aB = N

!




role(Init, A)
RespA = B
sees(A, {B, N aB , N b}Ka )
N aB = N

said(A, N b)
N bA = N b
f resh(N b)
!

!

role(Init, A)
RespA = B
role(Resp, B)
InitB =

sees(B, {N a, A}Kb )
said(B, N a)








sees(A, {B, N aB , N b}Ka )
said(A, N b)



(role(Init, A) f resh(N a))
(role(Resp, B) f resh(N b))

Notice first two formulas required rationality agents B.
formulas obtained automatically fixed set meta rules.
obtain third fourth formulas comparing local history protocols
conditions appearing formulas. get fifth formula informally, consider
local history conditions role(Init, A) RespA = B,
1. said {N a, A}Kb
2. sees {B, N a, N bA }Ka
3. said {N bA }Kb .
According local history, sees nonce N generated itself. N
said message {N a, A}Kb , thus B, inverse key Kb, must see
message said N a. Similarly, see sixth formula holds. last formula
follows immediately definition protocol.
710

fiVariable Forgetting Reasoning Knowledge

set OA observable variables
{f resh(N a), role(Init, A), RespA = B}.
set OB observable variables B
{f resh(N b), role(Resp, B), InitB = A}.
consider knowledge structure
F = (VRN , RN , OA , OB ).
Let SpecA formal specification:





!
f resh(N a)
said(A, N a)


role(Init, A) KA KB
f resh(N a)
RespA = B

SpecB formal specification:





!
f resh(N b)
said(B, N b)


.
role(Resp, B) KB KA
f resh(N b)
InitB =

easy show that, states F,
(F, s) |= SpecA SpecB
desired.
mention that, original Needham-Schroeder protocol (Needham &
Schroeder, 1978), second message B A: {N a, N b}Ka instead B A: {B, N a, N b}Ka .
Therefore, fourth formula would changed



role(Init, A)


sees(A, {N aB , N b}Ka )
N aB = N


said(A, N b)

N bA = N b
f resh(N b)

Thus, RespA = B necessarily hold condition
role(Init, A) sees(A, {N aB , N b}Ka ) said(A, N b) f resh(N b).
specifications SpecA SpecB hold original NeedhamSchroeder protocol.
8.4 Discussion
BAN logic (Burrows, Abadi, & Needham, 1990) one successful logical tools
reason security protocols. However, semantics BAN always arguable,
clear assumption rules BAN logic sound complete.
711

fiSu, Sattar, Lv, & Zhang

motivated research seeking adequate frameworks (models). Providing modeltheoretic semantics BAN logic central idea development BAN-like
logics (Abadi & Tuttle, 1991) SVO (Syversion & van Oorschot, 1996).
advantage approach use knowledge structures semantic models verify
correctness epistemic goals security protocols.
important problem that, given security protocol, corresponding knowledge structure comes from. get knowledge structure corresponding
security protocol, developed semantic model, background knowledge base
corresponding knowledge structure consists formulas valid semantic
model. Moreover, generate background knowledge systematically. ongoing
work implement approach promising automatic security protocol verifier.

9. Related Work
number approaches dealing concept variable forgetting eliminations middle terms (Boole, 1854) several contexts. notion variable forgetting
formally defined propositional first order logics Lin Reiter (1994).
recent years, theories forgetting answer set programming semantics proposed
(Zhang & Foo, 2006; Eiter & Wang, 2008). Forgetting also generalized description
logics (Kontchakov, Wolter, & Zakharyaschev, 2008; Wang, Wang, Topor, & Pan, 2008;
Kontchakov, Walther, & Wolter, 2009).
context epistemic logic, notion forgetting studied number
ways. Baral Zhang (2006) treated knowledge forgetting special form update
effect K K: knowledge forgetting , agent would neither know
. Ditmarsch, Herzig, Lang Marquis (2008) proposed dynamic epistemic
logic epistemic operator K dynamic modal operator [F g(p)] formula
[F g(p)] means agent forgets knowledge p, true. (Zhang &
Zhou, 2008) modeled forgetting via bisimulation invariance except forgotten variable.
notion variable forgetting closely related quantified modal logics,
existential variable quantification modeled via bisimulation invariance except
quantified variable (Engelhardt et al., 2003).
notion variable forgetting various applications knowledge representation
reasoning. example, Weber (1986) applied updating propositional knowledge
bases. Lang Marquis (2002) used merging set knowledge bases simply
taking union may result inconsistency. notion variable forgetting also
closely related formula-variable independence, result forgetting
set variables V formula defined strongest consequence
independent V (Lang et al., 2003). recently, Liu Lakemeyer (2009) applied
notion forgetting situation calculus, obtained interesting results
first-order definability computability progression local-effect actions.

10. Conclusion
main contribution paper follows. First, investigated knowledge
reasoning within simple framework called knowledge structure, consists global
712

fiVariable Forgetting Reasoning Knowledge

knowledge base set observable variables agent. notion knowledge
structure used semantic model multi-agent logic knowledge common
knowledge. model, computation knowledge common knowledge
reduced operation variable forgetting; moreover, objective formula known
agent state weakest sufficient condition Oi holds state s.
Second, capture notion common knowledge framework, generalized notion weakest sufficient conditions obtained, set V sets propositional variables, notion weakest V-sufficient conditions. Given set agents
family V observable variable sets agents, shown objective
formula common knowledge agents iff weakest {Oi | }-sufficient
holds. Also, shown public announcement operator conveniently dealt
via notion knowledge structure.
Third, relationship S5 Kripke structure knowledge structure
explored. Specifically, satisfiability issue formula language multi-agent
S5 public announcement operator satisfiability meant w.r.t.
standard Kripke structure w.r.t. knowledge structure.
Fourth, examined computational complexity problem whether
formula realized structure F. general case, problem PSPACE-hard;
however, interesting subcases reduced co-NP.
Finally, shown strength concept knowledge structure practical side empirical results satisfiability problem knowledge structures
based instances muddy children puzzle, since even smallest instances
considered experiments generating corresponding S5 Kripke structure would
reach. also discussed automated analysis verification corrected
Needham-Schroeder protocol via knowledge structures.
work presented paper extended several directions. First,
investigate whether knowledge structures extended used basis
knowledge based programming (Fagin et al., 1995). Secondly, current framework
knowledge structures, considered issue knowing
extensively studied knowledge reasoning models (Halpern & Lakemeyer, 1996;
van der Hock, Jaspars, & Thijsse, 2003; Levesque, 1990). interesting topic
knowledge model handles knowing reasoning knowledge. Thirdly,
notions methods work extended investigate extension
variable forgetting operator multi-agent logics beliefs. Finally, recent research
shown knowledge update many important applications reasoning actions
plans dynamic modeling multi-agent systems (Zhang, 2003). first step
direction (in mono-agent S5) found work Herzig, Lang Marquis
(2003). Baral Zhang proposed general model performing knowledge update
based standard single agent S5 modal logic (Baral & Zhang, 2001). believe
work extended multi-agent modal logics using knowledge structure
defined paper therefore develop general system knowledge update.
Along direction, interesting research issue explore underlying relationship
knowledge forgetting - specific type knowledge update, variable forgetting
addressed paper.
713

fiSu, Sattar, Lv, & Zhang

Acknowledgments
authors thank Ron van der Meyden, Fangzheng Lin anonymous reviewers
valuable comments earlier version paper. work partially supported Australian Research Council grant DP0452628, National Basic Research
973 Program grants (Nos. 2010CB328103, 2009CB320701 2005CB321902), National Natural Science Foundation China grants (Nos. 60725207 60763004).
paper revised extended version paper appeared Proceedings KR
2004 (Su, Lv, & Zhang, 2004)

References
Abadi, M., & Tuttle, M. (1991). semantics logic authentication. Proceedings
Tenth Annual ACM Symposium Principles Distributed Computing, pp.
201216.
Baltag, A., Moss, L., & Solecki, S. (1998). logic public announcements common
knowledge distributed applications (extended abstract). Proceedings TARKVII, pp. 4356.
Baral, C., & Zhang, Y. (2001). semantics knowledge update. Proceedings
17th International Joint Conference Artificial Intelligence (IJCAI-01), pp.
97102.
Baral, C., & Zhang, Y. (2006). Knowledge updates: semantic complexity issues. Artificial Intelligence, 164, 209243.
Boole, G. (1854). Investigation Laws Thought. Walton, London.
Burrows, M., Abadi, M., & Needham, R. M. (1990). logic authentication. ACM
Transactions Computer Systems, 8 (1).
Cervesato, I. (2001). Dolev-Yao intruder powerful attacker. Proc. 16th
Annual Int. Symp Logic Computer Science.
Dolev, D., & Yao, A. (1983). security public-key protocols. Communications
ACM, 29 (8), 198208.
Eiter, T., & Wang, K. (2008). Semantic forgetting answer set programming. Artificial
Intelligence, 172, 16441672.
Engelhardt, K., van der Meyden, R., & Moses, Y. (1998). Knowledge logic local
propositions. Proceedings TARK VII.
Engelhardt, K., van der Meyden, R., & Su, K. (2003). Modal logics hierarchy local
propositional quantifiers. Advance Modal Logic, Vol. 4, pp. 930. Kings College
Publications.
Fagin, R., Halpern, J., Moses, Y., & Vardi, M. (1995). Reasoning knowledge. MIT
Press, Cambridge, MA.
Gerbrandy, J. (1999). Bisimulation Plant Kripke. Ph.D thesis, Institute Logic,
Language Computation, University Amsterdam.
714

fiVariable Forgetting Reasoning Knowledge

Halpern, J., & Moses, Y. (1992). guide completeness complexity modal logics
knowledge belief. Artificial Intelligence, 54, 319379.
Halpern, J., & Zuck, L. (1992). little knowledge goes long way: Simple knowledge based
derivations correctness proofs family protocols. Journal ACM,
39 (3), 449478.
Halpern, J. Y., & Lakemeyer, G. (1996). Multi-agent knowing. Proceedings TARK
VI, pp. 251265.
Herzig, A., Lang, J., & Marquis, P. (2003). Action representation partially observable
planning using epistemic logic. Proceedings IJCAI-03, pp. 10671072.
Hintikka, J. (1962). Knowledge Belief. Cornell University Press, Ithaca, NY.
Kontchakov, R., Walther, D., & Wolter, F. (2009). Forgetting uniform interpolation
large-scale description logic terminologies. Proc. IJCAI09.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2008). tell difference
dl-lite ontologies. Proc. KR08.
Kripke, S. (1963). semantical analysis modal logic. i: Normal modal propositional
calculi. Z. Math. Logik Grundl. Math., 9, 6796.
Lang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence: Formulavariable independence forgetting. Journal Artificial Intelligence Research, 18,
391443.
Lang, J., & Marquis, P. (1998). Complexity results independence definability.
Proc. 6th International Conference Knowledge Representation Reasoning,
pp. 356367.
Lang, J., & Marquis, P. (2002). Resolving inconsistencies variable forgetting. Proc.
KR2002, pp. 239250.
Levesque, H. (1990). know: study autoepistemic logic. Artificial Intelligence, 42,
263309.
Lin, F. (2001). strongest necessary weakest sufficient conditions. Artificial
Intelligence, 128, 143159.
Lin, F., & Reiter, R. (1994). Forget it!. Greiner, R., & Subramanian, D. (Eds.), Working
Notes AAAI Fall Symposium Relevance, pp. 154159, New Orleans.
Liu, Y., & Lakemeyer, G. (2009). first-order definability computability progression
local-effect actions beyond. Proc. IJCAI09.
Lomuscio, A. (1999). Knowledge Sharing among Ideal Agents. Ph.D thesis, School
Computer Science, University Birmingham.
Lowe, G. (1996). Breaking fixing Needham-Schroeder public-key protocol using
FDR. Margaria, & Steffen (Eds.), Tools Algorithms Construction
Analysis Systems, Vol 1055 Lecture Notes Computer Science, pp. 147166.
Springer Verlag.
Needham, R. M., & Schroeder, M. D. (1978). Using encryption authentication large
networks computers. Communication ACM, 21 (12), 993999.
715

fiSu, Sattar, Lv, & Zhang

Plaza, J. (1989). Logics public communications. Proceedings 4th International
Symposium Methodologies Intelligent Systems, pp. 201216346.
Stockmeyer, L., & Meyer, A. (1973). Word problem requiring exponential time: prelimnary
report. Proc. 5th ACM Symp. Theory Computing, pp. 19.
Su, K., Lv, G., & Zhang, Y. (2004). Reasoing knowledge variable forgetting.
Proceedings KR-04, pp. 576586.
Syversion, P. F., & van Oorschot, P. (1996). unified cryptographic protocol logic. Tech.
rep. NRL Publication 5540-227, Naval Research Lab.
Tarski, A. (1955). lattice-theoretical fixpoint theorem ans applications. Pacific J.
Math., 5, 285309.
van Benthem, J. (2001). Logics information update. Proceedings TARK-VIII, pp.
5158.
van der Hock, W., Jaspars, J., & Thijsse, E. (2003). Theories knowledge ignorance.
S. Rahman, J. Symons, D. G., & van Bendegem, J. (Eds.), Logic, Epistemology
Unity Science. Kluwer.
van der Hoek, W., & Wooldridge, M. (2002). Model checking knowledge time. Proc.
19th Workshop SPIN (Model Checking Software), pp. 95111, Grenoble.
van der Meyden, R. (1998). Common knowledge update finite environments. Information Computation, 140 (2), 115157.
van Ditmarsch, H., Herzig, A., Lang, J., & Marquis, P. (2008). Introspective forgetting.
Wobcke, W., & Zhang, M. (Eds.), AI 2008: Advances Artificial Intelligence, Vol.
5360.
van Ditmarsch, H., van der Hoek, W., & Kooi, B. (2005a). Dynamic epistemic logic
assignment. Proceedings AAMAS-05, pp. 141148.
van Ditmarsch, H., van der Hoek, W., & Kooi, B. (2005b). Public announcements belief
expansion. Advances Modal Logic, Volume 5, pp. 335346.
Wang, Z., R., Wang, K., Topor, R., & Pan, J. (2008). Forgetting dl-lite. Proc.
ESWC08.
Weber, A. (1986). Updating propositional formulas. Proc. First Conference Expert
Database Systems, pp. 487500.
Zhang, Y. (2003). Minimal change maximal coherence epistemic logic program
updates. Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI-03), pp. 112117.
Zhang, Y., & Foo, N. (2006). Solving logic program conflict strong weak
forgettings. Artificial Intelligence, 170, 739778.
Zhang, Y., & Zhou, Y. (2008). Properties knowledge forgetting.. Proceedings
20th International Workshop Non-monoronic Reasoning ( NMR08), pp. 6875.

716

fiJournal Artificial Intelligence Research 35 (2009) 775-811

Submitted 03/09; published 08/09

Enhancing QA Systems Complex Temporal
Question Processing Capabilities
Estela Saquete
Jose L. Vicedo
Patricio Martnez-Barco
Rafael Munoz
Hector Llorens

stela@dlsi.ua.es
vicedo@dlsi.ua.es
patricio@dlsi.ua.es
rafael@dlsi.ua.es
hllorens@dlsi.ua.es

Natural Language Processing Information System Group
Department Software Computing Systems
University Alicante
Apartado de Correos 99, E-03080 Alicante, Spain

Abstract
paper presents multilayered architecture enhances capabilities current
QA systems allows different types complex questions queries processed.
answers questions need gathered factual information scattered
throughout different documents. Specifically, designed specialized layer process
different types temporal questions. Complex temporal questions first decomposed
simple questions, according temporal relations expressed original question.
way, answers resulting simple questions recomposed, fulfilling
temporal restrictions original complex question. novel aspect approach
resides decomposition uses minimal quantity resources, final
aim obtaining portable platform easily extensible languages.
paper also present methodology evaluation decomposition questions
well ability implemented temporal layer perform multilingual level.
temporal layer first performed English, evaluated compared with:
a) general purpose QA system (F-measure 65.47% QA plus English temporal layer
vs. 38.01% general QA system), b) well-known QA system. Much better
results obtained temporal questions multilayered system. system
therefore extended Spanish good results obtained evaluation
(F-measure 40.36% QA plus Spanish temporal layer vs. 22.94% general QA
system).

1. Introduction
Nowadays, fact huge amount digital information available (mainly
textual form) also large number users want easiest possible access
information. situation continuously fosters research development
information systems make possible analyze, locate, manage, access process
information automatically. Commonly, systems referred search
engines.
search engine especially useful obtain specific piece information without
need manually go available documentation related search
topic. Search engines currently evolving towards new generation engines capable
c
2009
AI Access Foundation. rights reserved.

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

understanding user needs better (the necessity behind every query) offering specific
services interfaces, depending domain context. new generation search
engines able offer list ordered web pages, also discover pieces
information scattered throughout different information sources even summaries (Barzilay,
Elhadad, & McKeown, 2002). is, integrate information text search (web
pages, documents), multimedia search (images, video, audio) database search (tourist,
biomedicine, etc.) comprehensible answers delivered users. addition,
correctly process questions formulated free natural language opposed keyword
queries fixed templates, information extraction scenarios (Michelson & Knoblock,
2008). Question answering systems (QA) one best examples new generation
search engines, allowing users formulate questions free natural language (NL)
providing exactly information required, also NL form.
However, QA mature technology current systems mainly focused
treatment questions require specific items data answer dates,
names entities quantities. capital Brazil? example
called factual questions. case, answer name city.
long road towards next generation systems, work presented takes
new step forward. defines layer that, installed top current NL-based search
engines QA systems, enhances capabilities processing different types complex
temporal questions.
specific case temporal QA trivial task due potential complexity
temporal questions. Current search engines, operational QA systems deal
simple factual temporal questions, is, questions requiring date answer
(When Bob Marley die?) questions involve simple temporal expressions
formulation (Who U.S. Open 1999?). Processing kinds questions
usually accomplished identifying explicit temporal expressions questions
relevant documents contain temporal expressions order answer questions.
However, system described paper also processes complex temporal questions.
is, questions whose complexity related temporal properties entities
enquired relative ordering events mentioned question. following
examples complex temporal questions:
spokesman Soviet Embassy Baghdad invasion
Kuwait?
Bill Clinton currently President United States?
approach present work tries imitate temporal reasoning human
solving types questions. example, person trying answer question:
spokesman Soviet Embassy Baghdad invasion Kuwait?
would proceed follows:
1. First, complex question would decomposed two simple ones:
spokesman Soviet Embassy Baghdad? invasion
Kuwait occur?.
776

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

2. He/She would look possible answers first simple question:
spokesman Soviet Embassy Baghdad?.
3. that, he/she would look answer second question:
invasion Kuwait occur?
4. Finally, he/she would give final answer one answers first question (if
any) temporal compatibility answer second question.
case, answer first question must temporally compatible
period dates associated invasion Kuwait (during).
Therefore, logical approach treatment complex questions based
decomposition questions simple ones resolved using conventional
QA systems. Finally, answers simple questions, fulfilling temporal constraints, would
used construct answer original complex question.
study presents development evaluation tool processes complex
NL-temporal questions information retrieval purposes. Apart fact tool
capable processing type complex questions, following advantages:
incorporated layer top one already existing QA systems.
contain integrate answer different data obtained different types
information sources (web pages, databases, documents, etc.) retrieved
using different types search engines (QA, NLIDB1 , etc.).
layer portable platform since language-dependent features process
easily extended languages.
information necessary process complex question obtained directly
it, extra auxiliary questions annotations required.
paper, main aim demonstrate temporal layer improve
general purpose QA system questions simple factual, higher
degree complexity. Specifically, implemented temporal layer order deal
questions different levels temporal complexity. Furthermore, proposed
treatment questions uses minimum quantity linguistic resources order obtain
portable platform, easily extended different languages.
paper structured following way: first all, section 2 briefly introduces current situation temporal reasoning QA; section 3 depicts proposal
classifying temporal questions four groups, depending features question; section 4 explains concept Multilayered QA system; section 5 describes
different modules temporal layer detail; section 6, decomposition
question Multilayered QA system evaluated English. portability
system languages described, procedure repeated evaluated
Spanish. Finally, conclusions comments future work made.
1. Natural Language Interfaces Databases

777

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

2. Background
explained introduction, one aims paper process complex questions. Complex questions general dealt previous studies using different
approaches decompose them. Harabagiu, Lacatusu Hickl (2006) presented procedure question produces lots queries semantically related original
question, main aim obtaining information answers. approach requires significant amount semantic information. question decomposition
presented Katz, Borchardt Felshin (2005) involves three decomposition techniques:
a) syntactic decomposition using linguistic knowledge language-based descriptions
resource content, b) semantic decomposition using domain-motivated explanation patterns language-based annotations resource content, c) semantic decomposition
questions resource content lower-level assertions. approach makes use
considerable amount linguistic knowledge order move new domains, new
sets parameterized language-based annotations need composed. addition
studies dealing single focus complex questions, Lin Lui (2008) propose processing
complex questions multiple foci obtaining one subquestion focus
original question. approach determines four possible relations subquestions derived original question. However, temporal relation considered
approach.
Apart complex questions treatment, motivation temporal aspect
work due great importance question answering field relating questions
information temporal dimension order find correct answer. Take,
example, following two similar questions:
president USA?
president USA 1975?
obvious dependency answers time, order obtain right answer
two questions, temporal information needs extracted processed,
first question refers current president USA (the exact point time
question formulated), whereas second one refers president 1975.
temporal information explicit, questions considered complex temporal
questions.
importance temporal dimension data information search processes
corroborated recent interest shown major evaluation forums QA like Text
REtrieval Conference - TREC (2008) Cross Language Evaluation Forum - CLEF (2008),
see also works Voorhees (2002) Magnini et al. (2005), including different types
temporal questions part evaluation benchmarks.
Furthermore, CLEF explicitly fostered research complex temporal questions
organizing specific pilot task questions (Herrera, Penas, & Verdejo, 2005)
including CLEF (Magnini et al., 2006) temporal dimension questions answers
part main QA task.
temporal question appropriately processed by: (1) relating available information temporal dimension (2) adapting search link temporal
information information search process.
778

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

Concerning first task, analysis time challenging problem, needs
applications based information extraction techniques expand include varying degrees
time stamping (identification reasoning) events expressions within narrative
question. Interest temporal representation reasoning evolving throughout years resulted growing number meetings related topic.
present here, descending chronological order, important ones: TIME (2008)
annual symposium Temporal Representation Reasoning (Demri & Jensen, 2008),
involves different areas including Time Natural Language; TempEval 2007 (Verhagen
et al., 2007) workshop held within SemEval-2007 evaluation systems performing Time-Event Temporal Relation Identification; ARTE 2006 new workshop focused
Annotating Reasoning Time Events (Ahn, 2006; Dalli & Wilks, 2006; Mani &
Wellner, 2006) part relevant conference COLING-ACL (2006) (Pan, Mulkar,
& Hobbs, 2006a); Dagstuhl 2005 seminar annotating, extracting reasoning
time events (Katz, Pustejovsky, & Schilder, 2005); TERN (2004) international
competition different systems identify normalize temporal expressions
evaluated compared; TANGO 2003 specialized developing appropriate infrastructure annotation (Pustejovsky & Mani, 2008); LREC (2002) dedicated
workshop Annotation Standards Temporal Information Natural Language (Mani
& Wilson, 2002; Setzer & Gaizauskas, 2002; Saquete, Martnez-Barco, & Munoz, 2002);
ACL (2001) included Temporal Spatial Information Processing workshop (Setzer &
Gaizauskas, 2001; Filatova & Hovy, 2001; Katz & Arosio, 2001; Moia, 2001; Schilder & Habel, 2001; Wilson, Mani, Sundheim, & Ferro, 2001) finally, COLING (2000),
papers related temporal expression identification temporal databases.
important emphasize meetings led development standard
specification language events temporal expressions ordering (TimeML,
2008). Nowadays, also growing number automatic systems extracting temporal
expression information2 , as: ATEL (2008), Chronos (Negri, 2007), TempEx (2008),
GUTime (Mani & Wilson, 2000a), DANTE (Mazur & Dale, 2007), TimexTag (Ahn, 2006)
TERSEO (Saquete, Munoz, & Martnez-Barco, 2006).
Regarding second task, significant progress made temporal analysis applied IE QA tasks presented TERQAS workshop (Pustejovsky, 2002; Radev
& Sundheim, 2002). purpose TERQAS workshop address problem
enhance natural language question answering systems answer temporally-based
questions events entities news articles. Besides, temporal question corpus
developed. far know, one first systems treated temporal information QA purposes described Breck et al. (2000) used temporal expression
identification applying temporal tagger developed Mani Wilson (2000b). Another important study related temporal constraints question answering presented
Prager, Chu-Carroll Czuba (2004). presented method improve accuracy
QA system asking auxiliary questions related original question whose answers used temporally verify restrict original answer. method called
QA-by-Dossier Constraints suitable TREC-style factoid questions,
inconvenience requiring generation set auxiliary questions. Besides,
2. http://timexportal.wikidot.com/systems

779

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

recently, researchers also focused important features temporal reasoning
final applications, as: a) event detection: Evita (Saur, Knippen, Verhagen, &
Pustejovsky, 2005) application recognizing events natural language texts,
recognition applied QA, b) event extension: Pan, Mulkar Hobbs (2006b) describe method automatically learn durations event descriptions, c) temporal
relations temporal expressions events, described Lapata Lascarides
(2006).
However, strategies implied complex temporal processing question,
using information extracted original question small amount linguistic
resources temporal reasoning beyond scope investigations.
proposal focused temporal reasoning complex temporal questions
necessary add new layer existing systems, thereby allowing complex questions
processed (Saquete, Martnez-Barco, Munoz, & Vicedo, 2004). decomposition
performed temporal layer based temporal relation events
original question, linguistic information required decomposition.
addition, system identifies normalizes temporal expressions used part
processing layer (Negri, Saquete, Martnez-Barco, & Munoz, 2006), taking advantage
multilingual feature system order use cross-lingual tasks.
However, temporal questions need treated way since
may different characteristics, reason, classification different types
temporal questions also proposed.

3. Temporal Questions Taxonomy
explaining answer temporal questions, must classified different
categories since way solve differ depending type question involved.
temporality question depends two levels complexity: a) number events
question: Questions formed single event whose answers found
document (simple questions), questions formed one event
temporally related whose answers could found multiple documents (complex
questions), b) temporal information appearing question, like implicit
explicit temporal expressions, needs recognized normalized. combination
two features results four different types temporal questions.
Simple Temporal Questions:
Type 1: Single event temporal questions without temporal expression (TE).
questions require temporal expression answer contain temporal
expression formulation. questions formed single event
temporal reasoning required, resolved QA system directly without
pre postprocessing question. example: Jordan close port
Aqaba Kuwait?. However, since taxonomy temporal question taxonomy,
type basic temporal questions need considered.
Type 2: Single event temporal questions temporal expression. questions
require temporal reasoning temporal expression contained formulation
question. single event question, one temporal
expressions need identified, normalized annotated. temporal infor780

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

mation necessary search correct answer, due fact establishing
temporal constraints candidate answers. example: 1988 New
Hampshire Republican primary?. Temporal Expression: 1988
Complex Temporal Questions:
Type 3: Multiple event temporal questions temporal expression. Questions
contain one event, related temporal signal. signal establishes order
events question. Moreover, one temporal expressions
question. temporal expressions need identified, normalized annotated,
establish temporal constraints answers question. example:
George Bush U.N. Security Council ordered global embargo trade
Iraq August 90? example, temporal signal temporal
constraint 8/1/1990 8/31/1990. question consists two events:
Event 1: George Bush something
Event 2: U.N. Security Council ordered global embargo trade Iraq
(Temporal constraint: August 1990)
Type 4: Multiple event temporal questions without temporal expression. Like Type 3,
questions consist one event, related temporal signal,
case, questions contain temporal expressions. temporal signal establishes
order events question. example: president
US AARP founded?. example, temporal signal
question would decomposed into:
Event 1: someone president US
Event 2: AARP foundation
process type question explained detail following sections.

4. Architecture Multilayered QA System
order process special types questions beyond scope currently QA
systems, work proposes multilayered architecture increases functionality
QA systems, allowing solve type complex question. work,
temporal layer implemented. Moreover, architecture enables different layers
added cope questions need kinds complex processing
temporally oriented, script questions (How assemble bicycle?)
template-based questions (What main biographical data Nelson Mandela?).
Complex questions common need additional processing question
order solve adequately. architecture presented paper enables different
types complex questions dealt superposing additional processing layers,
one type, top existing general purpose QA system, shown Figure
1. layers will:
decompose question simple events generate simple questions (sub-questions)
ordered according original question,
781

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

send simple questions general purpose QA system,
receive answers simple questions general purpose QA system,
filter, compare validate sub-answers, according relation detected sub-questions, order construct final complex answer.

Complex Question

Complex Answer

INTERFACE
TEMPORAL
QUESTION
LAYER

SCRIPT
QUESTION
LAYER

TEMPLATE
QUESTION
LAYER

Simple Questions

...

Simple Answers
SEARCH ENGINE

Text

Multimedia

Databases

Figure 1: Multi-layered Architecture QA system
architecture large number advantages, following
mentioned:
allows researchers use existing general purpose QA system.
Since complex questions processed superior layer, necessary modify
current QA system want deal complex questions. layer
enhances capabilities existing QA system without changing way.
additional processing layer works independently others processes questions accepted layer.
possible one type QA system working parallel,
specialized searching specific type information (text,multimedia,databases).
Next, layer oriented processing temporal questions according taxonomy shown
section 3 presented.
782

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

5. Temporal Layer
temporal layer proposed consists two units, Question Decomposition Unit
Answer Recomposition Unit, superposed general purpose QA
system, shown Figure 2.
Complex
Question

Complex
Answer
INTERFACE
TEMPORAL LAYER

QUESTION
DECOMPOSITION UNIT

ANSWER
RECOMPOSITION UNIT

TE
tags

TE Identification
Normalization

Individual Answer
Filtering

Type Identification
Answer Comparison
Composition

Signal
Question Splitter

Q-Focus

Q-Restriction

Q-Focus
Answers

Q-Rest.
Answer

SEARCH ENGINE

Text

Multimedia

Databases

Figure 2: Architecture temporal layer
components work together order obtain final answer follows:
Question Decomposition Unit preprocessing unit performs three main tasks.
First all, temporal expressions question identified normalized. Secondly, following taxonomy shown section 3, different types questions
type must treated different way. reason, type needs
identified. that, complex questions (Type 3 4) split simple
ones using temporal signal reference. first sub-question defined
question focus (Q-Focus) specifies type information user needs
find. second sub-question called question restriction (Q-Restriction)
answer sub-question establishes temporal restrictions list
answers Q-Focus. Q-Focus Q-Restriction input QA
system. example, question Bill Clinton study going Oxford University?, divided two sub-questions related temporal
signal before: Q-Focus: Bill Clinton study? Q-Restriction:When
Bill Clinton go Oxford University?.
783

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

General purpose QA system. simple questions generated processed general purpose QA system. QA system could used (QA systems, Multimedia
search engines NLIDB). example above, current QA system returns
following answers:
Q-Focus Answers:
Georgetown University (1964-68)
Oxford University (1968-70)
Yale Law School (1970-73)
Q-Restriction Answer: 1968
Answer Recomposition Unit. unit constructs answer original question answers Q-Focus Q-Restriction using temporal
constraints, temporal signals (which fully explained later) temporal
expressions, available original question. temporal signal establishes
appropriate order answers Q-Focus Q-Restriction
question. Finally, unit returns appropriate answer analyzing temporal compatibility list possible Q-Focus answers Q-Restriction
answer.
example temporal layer operates shown Figure 3.
Bill Clinton study going Oxford University?
Q-Focus

Q-Restriction

Bill Clinton study?

Bill Clinton go
Oxford University?

ANSWERS:
Georgetown University
(1964-1968)
Oxford University
(1968-1970)
Yale Law School
(1970-1973)

ANSWER:

Temporal
Signal

1968-1970

<

Temporal Compatible
Answer
Georgetown University

Figure 3: Example performance Temporal Layer
important emphasize temporal layer language dependent platform
(it uses lexical syntactic patterns) English language chosen initially
develop layer; however, easily extended languages, seen
section 6.3. units integrate temporal layer described detail
following sections.
784

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

5.1 Question Decomposition Unit
main task unit, divided three main modules, temporal
reasoning temporal information question decomposition question
(only required Type 3 4 questions). temporal expression identification
normalization module detects resolves temporal expressions question.
type identification module classifies question according taxonomy proposed
section 3. Finally, question splitter module splits complex question simple
ones.
Thus, output Question Decomposition unit consists of:
two sub-questions (Q-Focus Q-Restriction), processed QA
system order obtain answer them,
temporal tags, containing concrete dates returned TERSEO system (Saquete
et al., 2006), tags part input Answer Recomposition Unit
used unit temporal constraints order filter individual
answers,
temporal signal, part input Answer Recomposition Unit
well, information needed order compose final answer
determine temporal compatibility answers Q-Focus
answer Q-Restriction.
modules decomposition unit fully explained following subsections.
5.1.1 Temporal Expression Identification Normalization
module uses TERSEO system (Saquete et al., 2006) identify, annotate
normalize temporal expressions question.
system, implicit explicit temporal expressions annotated. Expressions like 12/06/1975 explicit, like two days implicit
need location another complete temporal expression (TE) understood.
specific purposes temporal layer, TERSEO simply returns text temporal
expression string normalization resolution value temporal expression
using ISO standard format concrete dates periods.
work, TERSEO use complete text input, question.
temporal tags (TE tag value attribute) obtained questions output
module used Answer Recomposition Unit order filter
individual answers obtained QA system. TE tag necessary order
determine temporal compatibility answers Q-Focus answer
Q-Restriction. example, question like: U.S. ship attacked
Israeli forces Six Day war sixties?, temporal constraint must
fulfilled is: date Q-Focus answers 1960-01-01 1969-12-31
(196 ISO format). means answers whose dates within range
dates question temporally compatible.
important emphasize that, initially, TERSEO developed Spanish,
platform automatically extend system languages developed
785

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

well. Therefore, system evaluated three different languages: Spanish, English
Italian. Spanish results 91% precision 73% recall. system
evaluated English using TERN (2004) corpus results obtained Fmeasure 86% identification 65% normalization. Italian
evaluation, I-CAB corpus used. corpus consists 525 news documents taken
local newspaper LAdige 3 . Ita-TERSEO obtained F-measure around 77%
identification. results quite good extension English Italian
completely automatic therefore, also fast.
multilingual capabilities TERSEO interesting various NLP fields,
particular application Crosslingual QA systems, therefore Temporal Layer
well.
5.1.2 Type Identification
Type Identification module classifies question one four types
taxonomy proposed above. identification necessary type question
produces different behavior (scenario) system. Type 1 Type 2 questions
classified simple, answer obtained without splitting original question.
hand, Type 3 Type 4 questions need split set simple
sub-questions. types sub-questions always Type 1, Type 2 non-temporal
question, considered simple questions.
question type established according rules Figure 4. four
possibilities: (a) Temporal Expression Temporal Signal, question
classified Type 1 ; (b) Temporal Expression Temporal Signal,
question classified Type 4 ; (c) Temporal Expression Temporal
Signal, question classified Type 2 ; (d) Temporal Expression
Temporal signal, question classified Type 3.
5.1.3 Question Splitter
task necessary when, according type identification module, question
Type 3 Type 4. questions considered complex questions need
divided simple ones (Type 1, Type 2 non-temporal questions). decomposition
complex question based identification temporal signals, link simple
events form complex questions (see Table 1).
explained before, using temporal signal referent, two events related
transformed two simple questions: Question-Focus (Q-Focus) QuestionRestriction (Q-Restriction).
Q-Focus question specifies information user searching
for. question simple obtain, syntactic changes required
construct it, question mark must added. Q-Focus processed
QA system, system return list possible answers.
Q-Restriction constructed using part complex question follows
temporal signal. question always transformed question using set
3. http://www.adige.it

786

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

QUESTION
(Q)

QUESTION
ANALYSIS





YES

TEMPORAL
EXPRESSION?

YES



TEMPORAL
SIGNAL?

TYPE 1

YES
TEMPORAL
SIGNAL?

TYPE 4

TYPE 2

TYPE 3

Figure 4: Decision tree Type Identification
lexical syntactic patterns defined layer. Q-Restriction processed
QA system, one appropriate answer expected.
addition, temporal signals denote ordering events linked. Assuming F1 date associated answers Q-Focus F2 date
associated answer Q-Restriction4 , signal establish certain order
answers, called ordering key. example ordering keys
shown Table 1.
Table 1: Example signals ordering keys
SIGNAL




F2 F3
F2 F3
/


time
Since

ORDERING KEY
F1 > F2
F1 = F2
F1 < F2
F2i <= F1 <= F2f
F2 <= F1 <= F3
F2 <= F1 <= F3
F1 = F2
F2i <= F1 <= F2f
F2i <= F1 <= F2f
F1 = F2
F1 > F2

Using list answers Q-Focus, answer Q-Restriction
temporal signal, Answer Recomposition Unit determines temporal compatibility
4. F2:Q-Restriction concrete date / [F2i-F2f]:Q-Restriction period dates

787

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

answers composes final answer original complex question.
process fully explained following subsection.
5.2 Answer Recomposition Unit
main task Answer Recomposition Unit obtain final answer
complex question using available inputs Decomposition Unit answers
obtained QA system. Recomposition Unit divided two modules.
Individual Answer Filtering module filters possible answers Q-Focus, avoiding
non-temporally compatible ones, Answer Comparison Composition module,
composes answer original question using ordering key established
temporal signal.
complex questions split Decomposition Unit Q-Focus
Q-Restriction answers questions obtained QA
system, Recomposition Unit determines list potential answers
Q-Focus one compatible temporal constraints obtained process:
temporal expressions, temporal signal answer Q-Restriction. answers
Q-Focus fulfill temporal constraints considered answer initial
complex question.
5.2.1 Individual Answer Filtering
list possible answers Q-Focus answer Q-Restriction given
QA system input Individual Answer Filtering module. Q-Focus
Q-Restriction temporal expression, selects answers satisfy
temporal constraints obtained TE Identification Normalization Unit. date
answer temporally compatible temporal tag, is, date
answer must lie within date values tag. not, rejected.
answers fulfill constraints go Answer Comparison Composition module.
5.2.2 Answer Comparison Composition
Finally, answers filtered using signals ordering key,
results Q-Focus compared answer Q-Restriction order
determine temporally compatible. Temporal signals denote relationship
order date answer Q-Focus date answer
Q-Restriction.
answers fulfill compatibility established temporal signal
possible answers original question. answer selected considered module
answer complex question. Hence, system able solve complex
temporal questions.

6. Evaluation Experiments
evaluation experiments performed paper done initially English,
porting system Spanish, evaluation procedure carried
new language.
788

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

evaluation dual aim: one hand, determine Decomposition
Unit able process type question properly order obtain appropriate simple
factual questions answered kind general purpose QA system,
hand, show extent general purpose QA system could improved
techniques applied.
6.1 Test Environment
First all, corpus English questions contains many simple complex temporal questions possible necessary. first idea use already existing resources,
TREC (2008) CLEF (2008) question corpora, due large number
questions contain. Unfortunately, studying corpora, discarded
contain complex temporal questions. Thus, using initial TERQAS
question corpus proposal (Radev & Sundheim, 2002; Pustejovsky, 2002) model, new
question corpus manually developed collecting questions group volunteers
unacquainted work. instructions given volunteers were: 1) answers
questions proposed must found Internet, 2) questions must constructed
according temporal question taxonomy described Section 3, 3) questions
must expect fact answer (factual questions). case complex questions, two
factual questions must related temporal signal5 . last instruction necessary
order make evaluation procedure straightforward since open-ended questions
usually require long answers, makes difficult judge. order
balanced corpus, questions discarded finally corpus developed contains balanced number (50) type temporal question (Types 1,2,3 4),
resulted 200 temporal questions English6 . Spanish evaluation, English
question corpus manually translated language. Therefore, distribution
questions type English.
question corpus English Spanish developed, following step
construct testbeds languages order allow rigorous, transparent
replicable evaluation tests. testbed annotation performed using XML
schema developed three independent annotators. case doubts
disagreement, annotation reviewed referee, made final decision.
interannotator agreement calculated every attribute, resulting 100% cases
except temporal signal (98%) due complexity temporal signals.
testbed annotation, every question annotated Q tag, tag
id attribute identifies every question. question string annotated using
QUESTION tag. Furthermore, every question, five items must annotated:
1. Identification Normalization temporal expressions question.
annotation item done using TE tag, content stores string text
TE. tag also attribute value, stores resolution
expression using ISO format,
5. important emphasize Type 3 questions contain two events temporal expression,
used extra time constraint answering procedure, limiting number potential answers,
speeding answering step, refining final answer
6. http://gplsi.dlsi.ua.es/corpus/CTQ

789

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

2. type question according classification proposed paper. type
annotated using tag TYPE must value 1 4. Since
questions manually built using temporal question taxonomy reference,
3. temporal signal. tag used annotate item called SIGNAL stores
exact string text temporal signal question,
4. two possible sub-questions case complex questions (Type 3 4):
first sub-question annotated using tag Q-FOCUS second using tag
Q-REST, and,
5. answer complex question: answer annotated using ANSWER tag
contains correct answer complex question. One example annotation
format question is:
<Q id="107">
<QUESTION>Who best actress Oscar award James Dean died 50s?< /QUESTION>
<TE value="195">the 50s< /TE>
<TYPE>3< /TYPE>
<SIGNAL>when< /SIGNAL>
<Q-FOCUS>Who best actress Oscar award?< /Q-FOCUS>
<Q-REST>When James Dean die 1950s?< /Q-REST>
<ANSWER>Anna Magnani< /ANSWER>
< /Q>

case Decomposition Unit, following five aspects evaluated:
TE Identification Normalization: temporal expressions question
correctly detected normalized?
Type Identification: type question correctly identified according classification presented previously paper?
Signal Detection: temporal signals question correctly detected?
Question Splitter: complex questions correctly split simple factual questions, answered general purpose QA system?
DECOMPOSITION UNIT whole: system correctly undertaken
sub-tasks previously defined, since sub-tasks whole compose decomposition unit?
However, evaluation aspects explained need considered
questions. Table 2 determines aspect must evaluated ignored particular
type question. decomposition unit whole taking consideration
evaluated sub-tasks every question.
determined aspects evaluated decomposition cases
must evaluated, depending type question, next step establish
aspects evaluated. purpose, criteria matrix, containing rules
followed evaluation order determine elements treated (ACT)
correct (CORR), shown Table 3.
790

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

Table 2: Aspects evaluated Decomposition Unit depending Q-type
Type
1
2
3
4

TE Id.Norm.

Yes
Yes


Type
Yes
Yes
Yes
Yes

Signal


Yes
Yes

Q Splitter


Yes
Yes

DECOMP.
Yes
Yes
Yes
Yes

Table 3: ACT CORR criteria matrix Decomposition Unit evaluation
TE
Ident.Norm.

ACT
TE annotated system

Type
Signal
Q Splitter

type returned system
temporal signal annotated system
Complex Q divided two sub-Qs

DECOMP.

previous aspects ACT

CORR
-Exact agreement TE tag content
-Exact agreement value attribute content
within TE tag
Exact agreement TYPE tag content
Exact agreement SIGNAL tag content
Sub-Qs agreement Q-FOCUS Q-REST tag
in:
-Interrogative particle
-Main verb correctly detected tensed
-All keywords appear keywords
original Q, except stopwords
previous aspect CORR

case QA evaluation, using current CLEF evaluation criteria7
starting point, determining correct inexact answers. use evaluation
criteria possible since corpus contains factual questions. Therefore, determining
correctness answer straightforward. CLEF judgments
specified evaluation measure incorrectness may calculated directly
subtracting number correct answers total number questions.
addition, unknown judgement also omitted since two human assessors must evaluate
answers. finally, consider unsupported judgement neither, since
corpus consisted data obtained Internet, correct answers found.
criteria matrix QA, shown Table 4, describes rules followed evaluation order determine treated (ACT), correct (CORR) inexact (INE) answers.
Table 4: ACT CORR criteria matrix QA system
QA

ACT
anwer returned
system

CORR (CLEF R)
-Exact agreement one
answers contained
ANSWER tag content

INE (CLEF X)
answer contains correct answer, incomplete longer
minimum amount information required

evaluations performed work, following measures used:
POS:Total number items
7. http://www.clef-campaign.org/

791

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

ACT: Number items treated system
CORR: Number items properly treated (Correct)(CLEF R)
PREC: Precision ( CORR
ACT ) percentage items output system
properly treated
REC: Recall ( CORR
P OS ) percentage items treated system (CLEF Accuracy)
2

)(P R)
) Combination precision recall single value. = 1
F: ( (1+
( 2 P +R)

QA evaluation purposes:
1
MRR: ordered list possible answers question ( CorrectAnswerP
osition ).
final MRR average every individual MRR obtained.

INE: number answers judged inexact human assessors (CLEF X).
6.2 Evaluation Results
section presents results decomposition unit analysis influence
QA systems English.
6.2.1 Evaluating Decomposition Unit English
section, decomposition unit8 processing simple complex temporal
questions English evaluated, based testbed defined previously. evaluation,
addition decomposition unit efficiency, aspects temporal expressions
influence complex questions analyzed.
evaluation results good F-measure 89.5%. results
shown Table 5. evaluation, 176 total 200 questions correctly
preprocessed. Since decomposition unit divides complex questions also
determines type question performs temporal reasoning necessary,
whole set questions considered global measure decomposition unit.
obvious case Type 1 questions decomposition unit simply determines
type question, interested evaluating performance unit
respect.
Table 5: Evaluation decomposition unit English
POS

ACT

CORR

PREC

REC

F

TE Identification Normalization

100

93

80

86.0%

80.0%

82.9%

Type Identification

200

200

194

97.0%

97.0%

97.0%

Signal Detection

100

100

96

96.0%

96.0%

96.0%

Question Splitter

100

100

92

92.0%

92.0%

92.0%

200

193

176

91.1%

88.0%

89.5%

DECOMPOSITION UNIT

8. http://gplsi.dlsi.ua.es/demos/TQA/

792

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

Next, detailed analysis results evaluation aspect shown (see Appendix
detailed error examples):
Identification normalization Temporal Expressions: corpus,
100 temporal expressions system detected 93, 80 correctly
resolved. said previously, module uses TERSEO system identify
normalize temporal expressions question. three types
errors: (1) expressions treated TERSEO temporal expressions
fact temporal; (2) expressions identified wrongly because: a)
expression outside scope TERSEO system, b) identification
extent exact; (3) expressions that: a) normalized wrongly
normalization rule TERSEO appropriate expressions, b)
normalized normalization rule exist.
Type Identification: 200 temporal questions corpus,
processed module, 194 correctly identified according taxonomy
proposed section 3. errors module due fact TE
detected TERSEO, shown Appendix A. However, type error
usually affect question splitting cases complex question
split correctly.
Signal Detection: corpus, 100 questions considered complex (Type 3 Type 4). system able treat recognize correctly
temporal signal 96 questions. main error detected module arose
temporal expression part signal, denoting complex signal, as:
EVENT1 year EVENT2. type complex signal outside scope
system. system also fails preposition, classified signal
system, part TE, like 18 century therefore wrongly
considered temporal signal.
Question Splitter: set 100 complex questions, system able
process split 92 properly. errors unit due to: a)
wrong signal identification; b) syntactic problems, obtaining tensed verb
subject Q-Focus construct Q-Restriction properly. instance,
question language invented Berliner patented Gramophone?,
POSTagger identify patented past tense verb Q-Restriction
wrongly generated as: Berliner patented Gramophone happen?.
One possible problem appear complex questions, yet treated
system, questions contain anaphoric co-references. Therefore, splitting
complex question two separate simple questions, question contains
anaphoric co-reference treated directly QA system needs processed
module performs anaphora resolution first. example: studies
Ms. Whitman graduate got MBA? . Q-Restriction obtained is:
get MBA?. case, referring Ms. Whitman. case,
type question outside scope. However, adding module adapts
anaphora resolution techniques dialogs texts (Palomar et al., 2001; Palomar &
793

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

Martnez-Barco, 2001) questions, situation solved. Moreover, applying
module question affect decomposition process case.
6.2.2 Evaluating influence temporal processing QA systems
English
QA system used evaluation general purpose one uses Internet data
corpus (Moreda, Llorens, Saquete, & Palomar, 2008a). simple opendomain QA system, whose main feature Answer Extraction Unit able look
possible answers two ways: performing mapping type name entity
question requires (NE-based), type semantic role question needs
answer (SR-based)9 .
Due modularity QA system, evaluation, NE-based answer extraction used. baseline, using subset factual questions, extracted
TREC1999 TREC2000 NE oriented, authors evaluated system
found 87.50% precision, 84% recall, 85.70% F 87.25% MRR (Moreda, Llorens, Saquete,
& Palomar, 2008b).
evaluation performed work divided two experiments:
1. Base QA system evaluation: First QA system evaluated without using
temporal layer.
2. Multilayered QA system evaluation: QA system evaluated performs temporal layer.
main aim evaluation compare results two experiments
determine temporal layer enhances general purpose QA system like one used
case. Besides, experiments, 200 temporal question corpus created
purpose containing simple (Type 1 Type 2) complex (Type 3 Type 4)
questions used.
results obtained general purpose QA system without temporal layer
shown Table 6.
Table 6: Evaluation QA system English temporal questions
POS

ACT

CORR

INE

PREC

REC

F

MRR

Type 1

50

50

35

0

70.00%

70.00%

70.00%

77.60%

Type 2

50

45

23

1

51.11%

46.00%

48.42%

48.00%

Type 3

50

8

1

0

12.50%

2.00%

3.45%

3.00%

Type 4

50

18

2

0

11.11%

4.00%

5.88%

5.00%

GLOBAL

200

121

32

1

50.41%

30.50%

38.01%

33.40%

results obtained system enhanced temporal layer shown Table
7.
9. http://gplsi.dlsi.ua.es/demos/TMQA/

794

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

Table 7: Evaluation QA system plus temporal layer English temporal questions
POS

ACT

CORR

INE

PREC

REC

F

MRR

Type 1

50

50

35

0

70.00%

70.00%

70.00%

77.60%

Type 2

50

47

38

1

80.85%

76.00%

78.35%

78.00%

Type 3

50

48

29

2

60.42%

58.00%

59.18%

63.66%

Type 4

50

46

26

2

56.52%

52.00%

54.17%

55.66%

GLOBAL

200

191

128

5

67.02%

64.00%

65.47%

68.73%

shown tables, QA system enhanced temporal layer offers better
results measures (72.24% improvement F 33.58% error reduction F).
outstanding improvements occur complex temporal questions, due extra
reasoning temporal layer applies find candidate answer. Moreover, extra
experiment, manually corrected temporal expression identification normalization,
performed. Obviously, questions Type 3 4 affected improved. Results
shown Table 8.
Table 8: Evaluation QA system plus temporal layer English temporal questions
manually corrected TERN
POS

ACT

CORR

INE

PREC

REC

F

MRR

Type 1

50

50

35

0

70.00%

70.00%

70.00%

77.60%

Type 2

50

48

40

1

83.33%

80.00%

81.63%

82.00%

Type 3

50

48

30

2

62.50%

60.00%

61.22%

65.66%

Type 4

50

46

26

2

56.52%

52.00%

54.17%

55.66%

GLOBAL

200

192

131

5

68.22%

65.50%

66.83%

70.23%

graphical comparison results type question shown Figure 5.
clear Multilayered QA system enhances performance QA system
types questions except Type 1 (simple factual temporal questions), since type
question processed way systems. types, precision,
recall, F-measure MRR improved, especially case Type 3 Type 4
questions, base QA system almost incapable answering questions
properly. system gave inexact answers since questions need short answers
consisting NE TE.
interesting examples analyzed shown Figure 6.
first example, question Type 2 question, contains implicit
temporal expression 16 years ago. question processed systems,
important difference Multilayered QA system able process temporal
expression normalize expression concrete date, case 1992.
preprocessing temporal expression done, question processed Base QA
system Olympics held 1992?, allowing system find correct
answer. Without preprocessing temporal layer, Base QA system returns
795

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

F

MRR

80%

80%

70%

70%

60%

60%

50%

50%

40%

Q.A.

40%

Q.A.

30%

M.Q.A

30%

M.Q.A.

20%

20%

10%

10%

0%

0%
Type 1

Type 2

Type 3

Type 4

Type 1

(a) F-measure comparison

Type 2

Type 3

Type 4

(b) MRR comparison

Figure 5: Comparative graphics Base QA system Multilayered QA system
Indian Prime Minister
Black Panthers founded?

Olympics held 16 years ago?
Type 2

Base Q.A.
system

Type 4

Multilayered
Q.A. system

Base Q.A.
system

16 years ago
= 1992

Beijing
(Wrong)

Multilayered
Q.A. system
Q-R=1996
T.S.=when

Barcelona
(OK)

H.Rap Brown
(Wrong)

(a) Example 1

Indira Gandhi
(OK)

(b) Example 2

Figure 6: Examples Multilayered QA system performance
popular answer, corresponds last Olympic games Beijing, therefore
fails answer question correctly.
second example, question Type 4 complex question processed
systems. Since Base QA system able reason second part
question simply uses keywords question, Multilayered QA system
returns correct answer, taking restriction date event second part
questions occurred.
conclude, study demonstrates including type layer help general purpose QA systems resolve questions complex simple factual
questions, without changing implementation general QA system.
6.2.3 Comparison QA systems
order compare results another QA system, carried
test widely known START QA system (Katz, 1990, 1997), available
Internet10 . results obtained START system obtained
10. http://start.csail.mit.edu/

796

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

QA system enhanced temporal layer shown compared Table 9.
general purpose QA systems using Internet corpus.
Table 9: QA system plus temporal layer compared START QA system
QA + temp layer

START

PREC

REC

F

MRR

PREC

REC

F

MRR

Type 1

70.00%

70.00%

70.00%

77.60%

85.71%

24.00%

37.50%

24.00%

Type 2

80.85%

76.00%

78.35%

78.00%

75.00%

6.00%

11.11%

7.00%

Type 3

60.42%

58.00%

59.18%

63.66%

00.00%

00.00%

00.00%

00.00%

Type 4

56.52%

52.00%

54.17%

55.66%

00.00%

00.00%

00.00%

00.00%

GLOBAL

67.02%

64.00%

65.47%

68.73%

83.33%

7.50%

13.76%

7.75%

START QA system able answer Types 1 Type 2 questions. Although, precision achieved system types questions high, recall
lower, specially Type 2 questions (6.00%). Focusing complex temporal questions
(Types 2, 3 4), QA system, uses temporal information, seen obtain
better results START QA, use temporal layer. conclusion,
results show application temporal layer improves QA results complex
temporal questions. Concretely, considering overall results, QA system using
temporal layer exceeds START system 375.79% regards F-measure (48.36%
error reduction).
6.3 Portability Languages: Spanish Approach
said before, system initially developed English extended Spanish
well. Since task performed layer processes complex questions language
dependent, adaptation system required: (1) TERSEO Spanish
used, (2) temporal signals stored system translated Spanish, (3)
question splitter module adapted build grammatically correct Spanish Cuando
(When) questions.
6.3.1 Decomposition Unit Evaluation Spanish
results evaluation shown Table 10.

Table 10: Evaluation system Spanish
POS

ACT

TE Identification Normalization

100

90

82

91.1%

82.0%

86.3%

Type Identification

200

200

189

94.5%

94.5%

94.5%

Signal Detection

100

99

97

97.9%

97.0%

97.4%

Question Splitter

100

100

93

93.0%

93.0%

93.0%

200

190

174

91.5%

87.0%

89.2%

DECOMPOSITION UNIT

797

CORR

PREC

REC

F

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

Briefly, evaluation Spanish, 174 total 200 questions properly
processed decomposed aspects (TE identification, type identification, temporal
signal detection splitting, necessary), means F-measure 89.2%
whole decomposition process. best results obtained Signal Detection module (F-measure almost 100%), results Question Splitting Type Identification
(F-measure around 93-94%) TE Identification Normalization also quite good
(F-measure around 86%).
main errors similar English ones. However, new problems
appeared Spanish (see Appendix B details), principally produced by:
Grammatical errors transformation second question due ambiguity
words produces incorrect POS-tagging. example: expression
el cometa Hale (the Hale comet), POSTAGGER classifies cometa (comet)
verb rather noun, would appropriate tag case.
Temporal expressions like el ano 99 el 99 (year 99), Spanish refer
1999 case, detected resolved. problem appears
expressions containing non explicit numeric temporal expressions, i.e. el siglo XIX
(XIX century), el segundo milenio (second millennium) less common
word-spelled dates mil novecientos noventa ocho (one thousand nine hundred
ninety eight) successfully processed temporal layer.
Finally, questions temporal signal complex, as: un ano despues
de que...(a year after...), signal detection question splitting wrong
type complex signal outside scope system.
6.3.2 Evaluating influence temporal processing QA systems
Spanish
evaluation, QA system described (Moreda et al., 2008a) adapted
Spanish language. English, used NE-based answer extraction module.
divided Spanish evaluation two experiments, English evaluation:
1. Base QA system evaluation: First adapted QA system evaluated without using
temporal layer.
2. Multilayered QA system evaluation: adapted QA system evaluated
performs temporal layer.
main aim evaluation analyze whether temporal layer successfully extended languages deal language QA system, like
Spanish-adapted QA system case. 200 temporal question corpus created
English test manually translated Spanish used experiments.
results obtained general purpose Spanish QA system without temporal
layer shown Table 11.
results obtained system enhanced temporal layer
shown Table 12.
798

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

Table 11: Evaluation QA system Spanish temporal questions
POS

ACT

CORR

INE

PREC

REC

F

MRR

Type 1

50

35

20

1

57.14%

40.00%

47.06%

45.34%

Type 2

50

37

12

0

32.43%

24.00%

27.59%

29.06%

Type 3

50

3

0

0

0.00%

0.00%

0.00%

0.00%

Type 4

50

4

0

0

0.00%

0.00%

0.00%

0.00%

GLOBAL

200

79

32

1

40.51%

16.00%

22.94%

18.60%

Table 12: Evaluation QA system plus temporal layer Spanish temporal questions
POS

ACT

CORR

INE

PREC

REC

F

MRR

Type 1

50

35

20

1

57.14%

40.00%

47.06%

45.34%

Type 2

50

40

19

0

47.50%

38.00%

42.22%

45.96%

Type 3

50

31

15

1

48.39%

30.00%

37.04%

37.00%

Type 4

50

31

14

1

45.16%

28.00%

34.57%

34.00%

GLOBAL

200

137

68

3

49.64%

34.00%

40.36%

40.58%

results Spanish show, expected already proven English case,
that: a) QA system enhanced temporal layer gives better results measures (79.42% improvement F 22.60% error reduction F), b) temporal layer
easily extensible languages. final global results worst English
approach QA system, due fact baseline results
Spanish QA system also worst compared English QA system (F-English 38%
compared F-Spanish 23%). addition, English experiments, extra experiment manually corrected Spanish temporal expression identification normalization
performed results shown Table 13.
Table 13: Evaluation QA system plus temporal layer Spanish temporal questions
manually corrected TERN
POS

ACT

CORR

INE

PREC

REC

F

MRR

Type 1

50

35

20

1

57.14%

40.00%

47.06%

45.34%

Type 2

50

43

22

0

51.16%

44.00%

47.31%

51.96%

Type 3

50

31

15

1

48.39%

30.00%

37.04%

37.00%

Type 4

50

31

14

1

45.16%

28.00%

34.57%

34.00%

GLOBAL

200

140

71

3

50.71%

35.50%

41.76%

42.08%

Despite fact, Multilayered QA system enhances performance Spanish
QA system types questions, even case complex questions,
Base Spanish QA system unable find correct answer. inexact
799

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

answers Spanish well, thus proving system usually obtains exact answers
languages.
conclude second evaluation analysis, extension evaluation Spanish
corroborates conclusions obtained English evaluation also demonstrates
temporal layer improves system way regardless language.

7. Conclusions Work
study presents multilayered temporal QA architecture performs multilingual level, case English Spanish. system processes complex temporal
questions splitting simple questions answered different types
general purpose QA systems. addition, system performs temporal reasoning
questions temporal information.
proposal consists adding new layer, top current QA system,
two main features:
Complex question decomposition. Questions decomposed simple events
generate simple questions (sub-questions) using temporal signal relates
events. first sub-question (Q-Focus) specifies type information user
needs find. answer second sub-question (Q-Restriction) establishes
temporal restrictions list answers Q-Focus. Q-Focus
Q-Restriction input QA system (any type QA system could used
here).
Question recomposition. Answers Q-Focus Q-Restriction, obtained
QA system, filtered compared order determine temporal compatibility construct final complex answer.
Since layer processes complex questions uses lexical syntactic rules (a
grammar), task language dependent. Initially, decomposition unit prepared
English, general way. Extension system Spanish therefore
simple (only small changes required), applies languages.
addition, temporal reasoning system performed TERSEO,
multilingual system (now working Spanish, English, Catalan Italian) easily
extensible European language.
evaluation purposes, two aims: a) determine decomposition unit
processes type question properly order obtain appropriate simple factual
questions, b) show techniques enhance general purpose QA system. order
accomplish aims, test bed English Spanish constructed, annotating
question corpus correct results decomposition QA tasks,
determining criteria establishing question properly decomposed
answered.
decomposition unit evaluation results English Spanish good
complex questions (F-measure 89.5% English 89.2% Spanish). evaluating
performance whole multilayered architecture, results compared
obtained base QA system without temporal layer. Great improvement
800

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

found, especially case complex questions (Type 3 4), base
system able answer (4.66% average F-measure English 0.00%
Spanish). multilayered QA system obtained overall F-measure approximately
65% English 40% Spanish types questions. Besides, temporal layer
QA system also compared online general purpose QA system called START,
demonstrating difficulty encountered general purpose QA systems answering
questions complex temporal information temporal relations.
work done along three main lines research: 1) resolving problems
detected temporal layer evaluation process, 2) adding module resolve
anaphoric co-reference questions, 3) integrating event link information
TIMEML schema (TimeML, 2008) system order extract deeper understanding
complex questions, 4) taking consideration techniques determine event durations
case open-ended questions, as: happened world oil prices Iraqi
annexation Kuwait?. task, previous work field considered (Pan
et al., 2006b), 5) applying layer procedure types complex questions,
well studying new features need added system enable
perform languages like Chinese.

Acknowledgments
paper partially supported Spanish government, project TIN-200615265-C06-01, framework project QALL-ME, 6th Framework
Research Programme European Union (EU), contract number: FP6-IST-033860.

Appendix A. Question Decomposition Error Analysis English
appendix gives detailed information decomposition errors detected test
English language. shown Table 5 distinguish TE identification normalization, type identification, signal detection question splitter errors. Table 14
specify questions English testbed correspond error types.
questions bold correspond one type error.
Table 14: Question Decomposition Error Analysis English
Error type
TE Identification Normalization

testbed question
81, 83, 89, 92, 97, 98, 99, 102, 108, 112, 114, 115, 116, 117,
126, 129, 133, 135, 142, 148

Type Identification

97, 98, 108, 129, 135, 148

Signal Detection

101, 114, 116, 129

Question Splitter

101, 110, 114, 116, 129, 142, 179, 192

questions implied listed below. erroneous elements listed correct
values indicated brackets.
<Q id=81> (ACT: Yes CORR: No)

801

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

<QUESTION>Who Nobel Peace Prize 91?</QUESTION>
<TE value=>91</TE> (CORR value=1991)
</Q>
<Q id=83> (ACT: Yes CORR: No)
<QUESTION>What tennis player win Wimbledon women singles second millennium year?</QUESTION>
<TE value=>year</TE>(CORR <TE value=2000>second millennium year</TE>)
</Q>
<Q id=89> (ACT: Yes CORR: No)
<QUESTION>How many planes crashed Twin Towers 01?</QUESTION>
<TE value=>01</TE> (CORR value=2001)
</Q>
<Q id=92> (ACT: Yes CORR: No)
<QUESTION>What organization founded 75 Bill Gates?</QUESTION>
<TE value=>75 by</TE> (CORR <TE value=1975>75</TE>)
</Q>
<Q id=97> (ACT: CORR: No)
<QUESTION>What city capital Nicaragua eighteen fifty-five?</QUESTION>
<TE value=></TE> (CORR <TE value=1855>eighteen fifty-five</TE>)
<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)
</Q>
<Q id=98> (ACT: CORR: No)
<QUESTION>What largest city Italy 17th century?</QUESTION>
<TE value=></TE> (CORR <TE value=16>the 17th century</TE>)
<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)
</Q>
<Q id=99> (ACT: Yes CORR: No)
<QUESTION>Where Eurovision held 68?</QUESTION>
<TE value=>68</TE> (CORR value=1968)
</Q>
<Q id=101> (ACT: Yes CORR: No)
<QUESTION>Who Prime Minister Spain four years Jose Maria Aznar presided Spain
2000 2004?</QUESTION>
<SIGNAL>after</SIGNAL> (CORR <SIGNAL>four years after</SIGNAL>)
<Q-FOCUS>Who Prime Minister Spain four years?</Q-FOCUS>
(CORR <Q-FOCUS>Who Prime Minister Spain?</Q-FOCUS>)
</Q>
<Q id=102> (ACT: CORR: No)
<QUESTION>Who king Spain Charles III died 1780s?</QUESTION>
<TE value=></TE> (CORR <TE value=178>the 1780s</TE>)
</Q>
<Q id=108> (ACT: CORR: No)
<QUESTION>Who president US AARP founded five decades ago?</QUESTION>
<TE value=></TE>(CORR <TE value=195>five decades ago</TE>)
<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)
</Q>
<Q id=110> (ACT: Yes CORR: No)
<QUESTION>Who Prime Minister Spain Columbia first flight 1980s?</QUESTION>
<Q-FOCUS>Who Prime Minister Spain just?</Q-FOCUS>
(CORR <Q-FOCUS>Who Prime Minister Spain?</Q-FOCUS>)
</Q>
<Q id=112> (ACT: Yes CORR: No)
<QUESTION>How many members European Union Gladiator released 00?</QUESTION>
<TE value=>00</TE> (CORR <TE value=2000>00</TE>)

802

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

</Q>
<Q id=114> (ACT: Yes CORR: No)
<QUESTION>What company introduced onto market seat adjustable shoulder support year
Mariah Carey born 1960s?</QUESTION>
<TE value=>the 1960s</TE> (CORR <TE value=196>the 1960s</TE>)
<SIGNAL>before</SIGNAL> (CORR <SIGNAL>a year before</SIGNAL>)
<Q-FOCUS>What company introduced onto market seat adjustable shoulder support year?</QFOCUS>
(CORR <Q-FOCUS>What company introduced onto market seat adjustable shoulder support?</QFOCUS>)
</Q>
<Q id=115> (ACT: Yes CORR: No)
<QUESTION>Which language forbidden Spain Francos Dictatorship period 1939-1975?</QUESTION>
<TE value=1975>1939-1975</TE>
(CORR <TE value=1939-1975>1939-1975</TE>)
</Q>
<Q id=116> (ACT: Yes CORR: No)
<QUESTION>When Indurain win Tour year Shawshank Redemption film released
1990s?</QUESTION>
<TE value=>the 1990s</TE> (CORR <TE value=199>the 1990s</TE>)
<SIGNAL>after</SIGNAL> (CORR <SIGNAL>a year after</SIGNAL>)
<Q-FOCUS>When Indurain win Tour year?</Q-FOCUS>
(CORR <Q-FOCUS>When Indurain win Tour?</Q-FOCUS>)
</Q>
<Q id=117> (ACT: Yes CORR: No)
<QUESTION>When Vesuvius erupt Sinclair Lewis Literature Nobel Prize 1930s?</QUESTION>
<TE value=>1930s</TE> (CORR <TE value=193>1930s</TE>)
</Q>
<Q id=126> (ACT: Yes CORR: No)
<QUESTION>Who died plane crash Vietnam war started late 1960s?</QUESTION>
<TE value=>1960s</TE> (CORR <TE value=1965-1969>late 1960s</TE>)
</Q>
<Q id=129> (ACT: CORR: No)
<QUESTION>Who king Spain Charles IV reigned Spain eighteenth century?</QUESTION>
<TE value=></TE> (CORR <TE value=17>eighteenth century</TE>)
<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)
<SIGNAL>during</SIGNAL> (CORR <SIGNAL>after</SIGNAL>)
<Q-FOCUS>Who king Spain Charles IV reigned Spain?</Q-FOCUS>
(CORR <Q-FOCUS>Who king Spain?</Q-FOCUS>)
<Q-REST>When eighteenth century happen?</Q-REST>
(CORR <Q-REST>When Charles IV reign Spain eighteenth century?</Q-REST>)
</Q>
<Q id=133> (ACT: Yes CORR: No)
<QUESTION>What person Literature Nobel Prize James Dean born 31?</QUESTION>
<TE value=>31</TE> (CORR value=1931)
</Q>
<Q id=135> (ACT: CORR: No)
<QUESTION>Who prime minister United Kingdom AARP founded five decades
ago?</QUESTION>
<TE value=></TE> (CORR <TE value=195>five decades ago</TE>)
<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)
</Q>
<Q id=142> (ACT: Yes CORR: No)
<QUESTION>Which language invented Zamenhof Berliner patented Gramophone 1880s?</QUESTION>
<TE value=>the 1880s</TE> (CORR <TE value=188>the 1880s</TE>)

803

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

<Q-REST>When Berliner patented Gramophone 1880s happen?</Q-REST>
(CORR <Q-REST>When Berliner patent Gramophone 1880s?</Q-REST>)
</Q>
<Q id=148> (ACT: CORR: No)
<QUESTION>Where Woodstock Festival held August 15 Unix developed?</QUESTION>
<TE value=></TE> (CORR <TE value=XXXX-08-15>August 15</TE>)
<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)
</Q>
<Q id=179> (ACT: Yes CORR: No)
<QUESTION>Who king Spain Charles IV reigned Spain?</QUESTION>
<Q-REST>When Charles IV reign Spain happen?</Q-REST>
(CORR <Q-REST>When Charles IV reign Spain?</Q-REST>)
</Q>
<Q id=192> (ACT: Yes CORR: No)
<QUESTION>Which language invented Zamenhof Berliner patented Gramophone?</QUESTION>
<Q-REST>When Berliner patented Gramophone happen?</Q-REST>
(CORR <Q-REST>When Berliner patent Gramophone?</Q-REST>)
</Q>

Appendix B. Question Decomposition Error Analysis Spanish
appendix gives detailed information decomposition errors detected test
Spanish language (see table 10)
. Table 15 specify questions correspond error types. questions
bold correspond one type error.
Table 15: Question decomposition error analysis Spanish
Error type

testbed question

TE Identification Normalization

81, 83, 89, 92, 97, 98, 99, 108, 114, 116, 129, 130, 133, 135,
142, 143, 145, 148

Type Identification

2, 6, 9, 31, 45, 81, 97, 98, 108, 129, 135

Signal Detection

114, 116, 129

Question Splitter

105, 110, 114, 116, 129, 133, 155

questions implied listed below. erroneous elements listed correct
values indicated brackets.
<Q id=2> (ACT: Yes CORR: No)
<QUESTION>Durante que decada fue inventado el test del polgrafo?</QUESTION>
<TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)
</Q>
<Q id=6> (ACT: Yes CORR: No)
<QUESTION>En que ano fue lanzado el submarino Nautilus?</QUESTION>
<TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)
</Q>
<Q id=9> (ACT: Yes CORR: No)

804

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

<QUESTION>En que ano entro en vigor la enmienda 18?</QUESTION>
<TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)
</Q>
<Q id=31> (ACT: Yes CORR: No)
<QUESTION>Que ano volaron los Wright Brothers por primera vez?</QUESTION>
<TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)
</Q>
<Q id=45> (ACT: Yes CORR: No)
<QUESTION>Que ano fue el gran Incendio de Londres?</QUESTION>
<TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)
</Q>
<Q id=81> (ACT: CORR: No)
<QUESTION>Quien gano el Nobel de la Paz en el 91?</QUESTION>
<TE value=></TE> (CORR <TE value=1991>el 91</TE>)
<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)
</Q>
<Q id=83> (ACT: Yes CORR: No)
<QUESTION>Que jugador de tenis gano Wimbledon mujeres individuales en el ano del segundo milenio?</QUESTION>
<TE value=>el ano</TE>
(CORR <TE value=2000>en el ano del segundo milenio</TE>)
</Q>
<Q id=89> (ACT: Yes CORR: No)
<QUESTION>Cuantos aviones chocaron en las Torres Gemelas en el 01?</QUESTION>
<TE value=>el 01</TE> (CORR value=2001)
</Q>
<Q id=92> (ACT: Yes CORR: No)
<QUESTION>Que empresa fue fundada en el 75 por Bill Gates?</QUESTION>
<TE value=2008>el 75</TE> (CORR value=1975)
</Q>
<Q id=97> (ACT: CORR: No)
<QUESTION>Que ciudad fue la capital de Nicaragua en mil ochocientos cincuenta cinco?</QUESTION>
<TE value=></TE> (CORR <TE value=1855>mil ochocientos cincuenta cinco</TE>)
<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)
</Q>
<Q id=98> (ACT: CORR: No)
<QUESTION>Cual fue la ciudad mas grande de Italia en el siglo XVII?</QUESTION>
<TE value=></TE> (CORR <TE value=16>el siglo XVII</TE>)
<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)
</Q>
<Q id=99> (ACT: Yes CORR: No)
<QUESTION>Donde se celebro Eurovision en el ano 68?</QUESTION>
<TE value=2008>el ano 68</TE> (CORR value=1968)
</Q>
<Q id=105> (ACT: Yes CORR: No)
<QUESTION>Quien gano el Nobel de Fsica cuando el cometa Hale Bopp fue descubierto hace 13 anos?</QUESTION>
<Q-REST>Cuando cometio el Hale Bopp fue descubierto hace 13 anos?</Q-REST>
(CORR <Q-REST>Cuando fue descubierto el cometa Hale Bopp hace 13 anos?</Q-REST>)
</Q>
<Q id=108> (ACT: CORR: No)
<QUESTION>Quien fue el presidente de los Estados Unidos cuando se fundo AARP hace cinco decadas?</QUESTION>
<TE value=></TE> (CORR <TE value=195>hace cinco decadas </TE>)
<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)
</Q>

805

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

<Q id=110> (ACT: Yes CORR: No)
<QUESTION>Quien fue el Presidente de Espana justo despues de que se produjera el primer vuelo del Columbia
en los anos 80?</QUESTION>
<Q-REST>Cuando se produjo se el primer vuelo del Columbia en los anos 80?</Q-REST>
(CORR <Q-REST>Cuando se produjo el primer vuelo del Columbia en los anos 80?</Q-REST>)
</Q>
<Q id=114> (ACT: CORR: No)
<QUESTION>Que empresa introdujo en el mercado el primer asiento con respaldo regulable un ano antes de
que naciera Mariah Carey en los anos 60?</QUESTION>
<TE value=>un ano antes</TE> (CORR <TE value=196>los anos 60</TE>)
<SIGNAL>antes de que</SIGNAL>
(CORR <SIGNAL>un ano antes de que</SIGNAL>)
<Q-FOCUS>Que empresa introdujo en el mercado el primer asiento con respaldo regulable un ano?</Q-FOCUS>
(CORR <Q-FOCUS>Que empresa introdujo en el mercado el primer asiento con respaldo regulable?</QFOCUS>)
</Q>
<Q id=116> (ACT: CORR: No)
<QUESTION>Cuando gano Indurain el Tour un ano despues de que se estrenara Cadena Perpetua en los anos
90?</QUESTION>
<TE value=>un ano despues</TE> (CORR <TE value=199>los anos 90</TE>)
<SIGNAL>despues de que</SIGNAL>
(CORR <SIGNAL>un ano despues de que</SIGNAL>)
<Q-FOCUS>Cuando gano Indurain el Tour un ano?</Q-FOCUS>
(CORR <Q-FOCUS>Cuando gano Indurain el Tour?</Q-FOCUS>)
</Q>
<Q id=129> (ACT: CORR: No)
<QUESTION>Quien fue el Rey de Espana despues de que Carlos IV reinara Espana durante el siglo XVIII?</QUESTION>
<TE value=></TE> (CORR <TE value=17>el siglo XVIII</TE>)
<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)
<SIGNAL>durante</SIGNAL> (CORR <SIGNAL>despues</SIGNAL>)
<Q-REST>Cuando fue el siglo XVIII?</Q-REST>
(CORR <Q-REST>Cuando reino Carlos IV Espana durante el siglo XVIII?</Q-REST>)
</Q>
<Q id=130> (ACT: Yes CORR: No)
<QUESTION>Quien gano Wimbledon femenino individuales antes de que Rafa Nadal ganara Wimbledon este
ano?</QUESTION>
<TE value=>este ano</TE> (CORR <TE value=2008>este ano</TE>)
</Q>
<Q id=133> (ACT: Yes CORR: No)
<QUESTION>Que persona gano el premio Nobel de Literatura cuando James Dean nacio en el ano 31?</QUESTION>
<TE value=>el ano 31</TE> (CORR value=1931)
<Q-REST>Cuando jamo Dean nacio en el ano 31?</Q-REST>
(CORR <Q-REST>Cuando nacio James Dean en el ano 31?</Q-REST>)
</Q>
<Q id=135> (ACT: CORR: No)
<QUESTION>Quien fue el Presidente de Reino Unido cuando AARP fue fundada hace cinco decadas?</QUESTION>
<TE value=></TE> (CORR <TE value=195>hace cinco decadas</TE>)
<TYPE>3</TYPE> (CORR <TYPE>4</TYPE>)
</Q>
<Q id=142> (ACT: Yes CORR: No)
<QUESTION>Que lengua fue inventada por Zamenhof cuando Berliner patento el disco de vinilo en la decada
de 1880?</QUESTION>
<TE value=1880>1880</TE>
(CORR <TE value=188>la decada de 1880</TE>)
</Q>

806

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

<Q id=143> (ACT: CORR: No)
<QUESTION>Donde se celebraran las Olimpiadas cuando Polonia adopte el Euro en la decada de 2010?</QUESTION>
<TE value=></TE> (CORR <TE value=201>la decada de 2010</TE>)
</Q>
<Q id=145> (ACT: Yes CORR: No)
<QUESTION>Cuando gano Gary Becker el premio Nobel de Economa antes de que Zapatero fuera elegido
Presidente de Espana en los ultimos anos?</QUESTION>
<TE value=>los ultimos anos</TE> (CORR value=[2003-2008])
</Q>
<Q id=148> (ACT: CORR: No)
<QUESTION>Donde se celebro el Festival de Woodstock el 15 de agosto cuando el Unix fue desarrollado?</QUESTION>
<TE value=></TE> (CORR <TE value=XXXX-08-15>el 15 de agosto</TE>)
</Q>
<Q id=155> (ACT: Yes CORR: No)
<QUESTION>Quien gano el Nobel de Fsica cuando el cometa Hale Bopp fue descubierto?</QUESTION>
<Q-REST>Cuando cometio el Hale Bopp fue descubierto?</Q-REST>
(CORR <Q-REST>Cuando fue descubierto el cometa Hale Bopp?</Q-REST>)
</Q>

References
ACL (2001). Association computational linguistics. http://www.aclweb.org/.
Ahn, D. (2006). stages event extraction. Computational Linguistics, A.
(Ed.), ARTE: Workshop 44th Annual Meeting Association Computational
Linguistics, pp. 18, Sydney, Australia.
ATEL (2008). Computational Language Education Research. University Colorado.
http://timex2.mitre.org/taggers/timex2 taggers.html.
Barzilay, R., Elhadad, N., & McKeown, K. (2002). Inferring strategies sentence ordering
multidocument news summarization. J. Artif. Intell. Res. (JAIR), 17, 3555.
Breck, E., Burger, J., Ferro, L., Greiff, W., Light, M., Mani, I., & Rennie, J. (2000). Another
Sys Called Quanda. Ninth Text REtrieval Conference, Vol. 500-249 NIST Special
Publication, pp. 369378. National Institute Standards Technology.
CLEF (2008). Cross Language Evaluation Forum. http://www.clef-campaign.org/.
COLING (2000).
18th international
http://coling.dfki.de/.

conference



computational

linguistics.

COLING-ACL (2006). 44th annual meeting association computational linguistics.
http://www.aclweb.org/mirror/acl2006/.
Dalli, A., & Wilks, Y. (2006). Annotating Dating Documents Temporal Text Classification. Computational Linguistics, A. (Ed.), ARTE: Workshop 44th Annual
Meeting Association Computational Linguistics, pp. 1122, Sydney, Australia.
Demri, S., & Jensen, C. S. (Eds.). (2008). 15th International Symposium Temporal
Representation Reasoning, Vol. 15 TIME symposium. IEEE Computer Society.
807

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

Filatova, E., & Hovy, E. (2001). Assigning Time-Stamps Event-Clauses. Proceedings
2001 ACL Workshop Temporal Spatial Information Processing, pp. 8895.
Harabagiu, S., Lacatusu, F., & Hickl, A. (2006). Answering complex questions random
walk models. Proceedings 29th Annual International ACM SIGIR Conference
Research Development Information Retrieval, pp. 220227.
Herrera, J., Penas, A., & Verdejo, F. (2005). Question answering pilot task CLEF 2004.
Peters, C., Clough, P., Gonzalo, J., Jones, G. J., Kluck, M., & Magnini, B. (Eds.),
Berlin Heidelberg New York, Vol. 3491 Lecture Notes Computer Science, pp.
581590. Springer-Verlag.
Katz, B. (1990). Using English indexing retrieving. Artificial intelligence MIT
expanding frontiers, 1, 134165.
Katz, B. (1997). Annotating World Wide Web using Natural Language. Proceedings
5th RIAO Conference Computer Assisted Information Searching
Internet.
Katz, B., Borchardt, G., & Felshin, S. (2005). Syntactic semantic decomposition strategies question ansering multiple resources. Proceedings AAAI 2005
Workshop Inference Textual Question Answering, pp. 3541.
Katz, G., & Arosio, F. (2001). Annotation Temporal Information Natural Language Sentences. Proceedings 2001 ACL Workshop Temporal Spatial
Information Processing, pp. 104111.
Katz, G., Pustejovsky, J., & Schilder, F. (2005). Annotating, Extracting Reasoning

Time

Events.
http://www.dagstuhl.de/en/programm/kalender/semhp/?semnr=05151.
Lapata, M., & Lascarides, A. (2006). Learning Sentence-internal Temporal Relations. Journal Artificial Intelligence Research, 27, 85117.
Lin, C.-J., & Liu, R.-R. (2008). Analysis Multi-Focus Questions. Proceedings
SIGIR 2008 Workshop Focused Retrieval, pp. 3036.
LREC (2002). Proceedings LREC Workshop Temporal Annotation Standards.
http://www.lrec-conf.org/lrec2002/.
Magnini, B., Giampiccolo, D., Forner, P., Ayache, C., Jijkoun, V., Osenova, P., Penas,
A.,
Rocha,
P.,
Sacaleanu,
B.,
& Sutcliffe, R. (2006). Overview CLEF 2006 Multilingual Question Answering
Track. http://www.clef-campaign.org/2006/working notes/workingnotes2006/
magniniOCLEF2006.pdf.
Magnini, B., Vallin, A., Ayache, C., Erbach, G., Penas, A., de Rijke, M., Rocha, P., Simov,
K., & Sutcliffe, R. (2005). Overview CLEF 2004 Multilingual Question Answering Track. Peters, C., Clough, P., Gonzalo, J., Jones, G. J., Kluck, M., & Magnini,
B. (Eds.), Berlin Heidelberg New York, Vol. 3491 Lecture Notes Computer Science, pp. 371391. Springer-Verlag.
Mani, I., & Wellner, B. (2006). Pilot Study Acquiring Metric Temporal Constraints
Events. Computational Linguistics, A. (Ed.), ARTE: Workshop 44th
808

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

Annual Meeting Association Computational Linguistics, pp. 2329, Sydney,
Australia.
Mani, I., & Wilson, G. (2000a). Processing news. Proceedings 38th Annual
Meeting Association Computational Linguistics (ACL2000), pp. 6976.
Mani, I., & Wilson, G. (2000b). Robust temporal processing news. ACL (Ed.),
Proceedings 38th Meeting Association Computational Linguistics (ACL
2000), Hong Kong.
Mani, I., & Wilson, G. (2002). Annotation Standards Temporal Information Natural
Language. Proceedings LREC Workshop Temporal Annotation Standards.
Mazur, P., & Dale, R. (2007). DANTE temporal expression tagger. Proceedings
3rd Language Technology Conference.
Michelson, M., & Knoblock, C. A. (2008). Creating Relational Data Unstructured
Ungrammatical Data Sources. J. Artif. Intell. Res. (JAIR), 31, 543590.
Moia, T. (2001). Telling apart temporal locating adverbials time-denoting expressions.
Proceedings 2001 ACL Workshop Temporal Spatial Information Processing.
Moreda, P., Llorens, H., Saquete, E., & Palomar, M. (2008a). Automatic generalization
QA answer extraction module based semantic roles. 11th edition
Ibero-American Conference Artificial Intelligence (IBERAMIA 2008), Advances
Artificial Intelligence. Springer-Verlag LNAI.
Moreda, P., Llorens, H., Saquete, E., & Palomar, M. (2008b). influence Semantic
Roles QA: comparative analysis. XXIV edicion del Congreso Anual de la
Sociedad Espanola para el Procesamiento del Lenguaje Natural 2008 (SEPLN 08) .
Negri, M. (2007). Dealing italian temporal expressions: ITA-Chronos system.
Proceedings EVALITA 2007, Workshop held conjunction AI*IA.
Negri, M., Saquete, E., Martnez-Barco, P., & Munoz, R. (2006). Evaluating Knowledgebased Approaches Multilingual Extension Temporal Expression Normalizer.
Computational Linguistics, A. (Ed.), ARTE: Workshop 44th Annual Meeting
Association Computational Linguistics, pp. 3037, Sydney, Australia.
Palomar, M., Ferrandez, A., Moreno, L., Martnez-Barco, P., Peral, J., Saiz-Noeda, M., &
Munoz, R. (2001). algorithm anaphora resolution Spanish text. Computational Linguistics, 27 (4), 545567.
Palomar, M., & Martnez-Barco, P. (2001). Computational approach anaphora resolution
Spanish dialogues. Journal Artificial Intelligence Research, 15 (4), 263287.
Pan, F., Mulkar, R., & Hobbs, J. (2006a). Learning Event Durations Event Descriptions. 44th Annual Meeting Association Computational Linguistics, pp.
393400.
Pan, F., Mulkar-Mehta, R., & Hobbs, J. R. (2006b). Learning Event Durations Event
Descriptions. ACL. Association Computer Linguistics.
Prager, J. M., Chu-Carroll, J., & Czuba, K. (2004). Question Answering Using Constraint
Satisfaction: QA-By-Dossier-With-Contraints. ACL, pp. 574581.
809

fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens

Pustejovsky, J. (2002). TERQAS: Time Event Recognition Question Answering
Systems. www.timeml.org/terqas/.
Pustejovsky, J., & Mani, I. (2008). TANGO: TimeML Annotation Graphical Organizer.
http://www.timeml.org/site/tango/index.html.
Radev,
D.,
&
Sundheim,
B. (2002). Using TimeML question answering. http://www.cs.brandeis.edu/~
jamesp/arda/time/documentation/ TimeML-use-in-qa-v1.0.pdf.
Saquete, E., Martnez-Barco, P., & Munoz, R. (2002). Recognising Tagging Temporal
Expressions Spanish. Proceedings LREC Workshop Temporal Annotation Standards, pp. 4451.
Saquete, E., Martnez-Barco, P., Munoz, R., & Vicedo, J. (2004). Splitting Complex Temporal Questions Question Answering systems. ACL (Ed.), 42nd Annual Meeting
Association Computational Linguistics, pp. 566573, Barcelona, Espana.
Saquete, E., Munoz, R., & Martnez-Barco, P. (2006). Event Ordering using TERSEO
system. Data Knowledge Engineering Journal, 58 (1), 7089.
Saur, R., Knippen, R., Verhagen, M., & Pustejovsky, J. (2005). EVITA: robust event
recognizer QA systems. HLT 05: Proceedings conference Human
Language Technology Empirical Methods Natural Language Processing, pp.
700707, Morristown, NJ, USA. Association Computational Linguistics.
Schilder, F., & Habel, C. (2001). Temporal Expressions Temporal Information:
Semantic Tagging News Messages. Proceedings 2001 ACL Workshop
Temporal Spatial Information Processing, pp. 6572.
Setzer, A., & Gaizauskas, R. (2001). Pilot Study Annotating Temporal Relations
Text. Proceedings 2001 ACL Workshop Temporal Spatial Information
Processing.
Setzer, A., & Gaizauskas, R. (2002). Importance Annotating Event-Event Temporal Relations Text. Proceedings LREC Workshop Temporal Annotation
Standards, pp. 5260.
TempEx
(2008).
MITRE
http://timex2.mitre.org/taggers/timex2 taggers.html.
TERN
(2004).
Time
Expression
http://timex2.mitre.org/tern.html.

Recognition

Corporation.


Normalization.

TIME (2008). International Symposium Temporal Representation Reasoning.
http://time.dico.unimi.it/.
TimeML (2008).
Markup Language Temporal
http://www.timeml.org/site/index.html.



Event

Expressions.

TREC (2008). Text REtrieval Conference. http://trec.nist.gov/.
Verhagen, M., Gaizauskas, R., Schilder, F., Hepple, M., Katz, G., & Pustejosvky, J. (2007).
Temeval-2007 task 15: Tempeval temporal relation identification. Proceedings
4th International Workshop SemEval-2007, pp. 7580.
810

fiEnhancing QA Systems Complex Temporal Question Processing Capabilities

Voorhees, E. M. (2002). Overview TREC 2002 Question Answering Track. Eleventh
Text REtrieval Conference, Vol. 500-251 NIST Special Publication, pp. 115123.
National Institute Standards Technology.
Wilson, G., Mani, I., Sundheim, B., & Ferro, L. (2001). Multilingual Approach
Annotating Extracting Temporal Information. Proceedings 2001 ACL
Workshop Temporal Spatial Information Processing, pp. 8187.

811

fiJournal Artificial Intelligence Research 35 (2009) 391 447

Submitted 09/08; published 06/09

Learning Bayesian Network Equivalence Classes
Ant Colony Optimization
Rnn Daly

RDALY @ DCS . GLA . AC . UK

Department Computing Science
University Glasgow
Sir Alwyn Williams Building
Glasgow, G12 8QQ, UK

Qiang Shen

QQS @ ABER . AC . UK

Department Computer Science
Aberystwyth University
Penglais Campus
Aberystwyth, SY23 3DB, UK

Abstract
Bayesian networks useful tool representation uncertain knowledge. paper
proposes new algorithm called ACO-E, learn structure Bayesian network.
conducting search space equivalence classes Bayesian networks using Ant
Colony Optimization (ACO). end, two novel extensions traditional ACO techniques
proposed implemented. Firstly, multiple types moves allowed. Secondly, moves
given terms indices based construction graph nodes. results testing
show ACO-E performs better greedy search state-of-the-art metaheuristic
algorithms whilst searching space equivalence classes.

1. Introduction
task learning Bayesian networks data has, relatively short amount time, become
mainstream application process knowledge discovery model building (Aitken, JirapechUmpai, & Daly, 2005; Heckerman, Mamdani, & Wellman, 1995). reasons many.
one, model built process intuitive feel Bayesian network consists
directed acyclic graph (DAG), conditional probability tables annotating node.
node graph represents variable interest problem domain arcs (with
caveats) seen represent causal relations variables (Heckerman, Meek, & Cooper,
1999) nature causal relations governed conditional probability tables associated
node/variable. example Bayesian network shown Figure 1.
Another reason popularity Bayesian networks aside visual attractiveness
model, underlying theory quite well understood solid foundation. Bayesian
network seen factorization joint probability distribution, conditional
probability distributions node making factors graph structure making
method combination. equivalence, network answer probabilistic
question regarding variables modeled.
addition, popularity Bayesian networks increased accessibility methods query model learn structure parameters network
c
2009
AI Access Foundation. rights reserved.

fiDALY & HEN

Figure 1: example Bayesian network
(Daly, Shen, & Aitken, 2009). shown inference Bayesian networks NP-complete
(Dagum & Luby, 1993; Shimony, 1994), approximate methods found perform
operation acceptable amount time. Learning structure Bayesian networks also
NP-complete (Chickering, 1996a), too, methods found render operation
tractable. include greedy search, iterated hill climbing simulated annealing (Chickering,
Geiger, & Heckerman, 1996). Recently however, heuristics become popular
problem combinatorial optimization high dimensional spaces. include approaches
tabu search (Glover, 1989, 1990), genetic algorithms (Mitchell, 1996) approach
paper investigate Ant Colony Optimization (ACO).
ACO fairly recent, called metaheuristic, used solution combinatorially hard
problems (Dorigo & Sttzle, 2004). iterated, stochastic technique biased results
previous iterations (Birattari, Caro, & Dorigo, 2002). method modeled behavior
real-life ants foraging food.
Many ants secrete pheromone trail recognizable ants positively biases
follow trail, stronger trail meaning likely biased towards it.
time pheromone trail evaporates. hunting food, ants behavior randomly walk
about, perhaps following pheromone trail, finds food. returns direction
whence came. strength trail factor choosing follow it, ant
faced two pheromone trails choose from, tend choose trails
highest concentration pheromone.
characteristics, situation multiple paths food source, ants
generally follow shortest path. explained follows. Assuming ants start nest
pheromone trails present, randomly wander reach food source
return home, laying pheromone way back. ant chooses shortest path
food source return home quickest, means pheromone trail highest
concentration, pheromone laid per unit time. stronger trail cause ants

392

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

prefer longer trails. ants leave pheromone short trail, thereby
providing reinforcing behavior choose trail others.
computing technique, ACO roughly modeled behavior. Artificial ants walk around
graph nodes represent pieces solution. continue complete solution
found. node, choice next edge traverse made, depending pheromone value
associated edge problem specific heuristic. number ants performed
traversal graph, one best solutions chosen pheromone edges
taken increased, relative edges. biases ants towards choosing edges
future iterations. search stops problem specific criterion reached. could
stagnation quality solutions passage fixed amount time.
paper seek use ACO technique learning Bayesian networks. Specifically,
used learn equivalence class Bayesian network structures. end, rest
paper structured following fashion. Firstly, in-depth study
problem searching optimum Bayesian network, space Bayesian networks
equivalence classes Bayesian networks. Then, new method formulating
search Bayesian network structure terms ACO metaheuristic introduced.
method based part earlier work done topic (Chickering, 2002a; de Campos,
Fernndez-Luna, Gmez, & Puerta, 2002). Next, results tests previous techniques
discussed finally, conclusions possible future directions stated.

2. Searching Bayesian Network Structure
are, general, three different methods used learning structure Bayesian network
data. first finds conditional independencies data uses conditional
independencies produce structure (Spirtes, Glymour, & Scheines, 2000). Probably
well known algorithms use method PC algorithm Spirtes Glymour (1990)
CI FCI algorithms Spirtes, Meek, Richardson (1995) able identify latent
variables selection bias. second uses dynamic programming optionally, clustering,
construct DAG (Ott, Imoto, & Miyano, 2004; Ott & Miyano, 2003). third method
dealt defines search space Bayesian networks. method uses
scoring function defined implementer, says relatively good network compared
others.
Although classification three different methods noted useful differentiating
applicability, boundaries often clear may seem. E.g.
score search approach dynamic programming approach similar
use scoring functions. Indeed, view Cowell (2001) conditional independence
approach equivalent minimizing Kullback-Leibler (KL) divergence (Kullback & Leibler,
1951) using score search approach. discussing score search method
works, definitions notation introduced.
graph G given pair (V, E), V = {v1 , . . . , vn } set vertices nodes
graph E set edges arcs nodes V . directed graph graph
edges associated direction one node another. directed acyclic graph DAG,
directed graph without cycles, i.e. possible return node graph following
direction arcs. illustration, graph Figure 2 DAG. parents node vi ,
Pa (vi ), nodes v j arrow v j vi (v j vi ). descendants vi ,
393

fiDALY & HEN

Figure 2: directed acyclic graph

Figure 3: skeleton DAG Figure 2

(vi ), nodes (not including vi ) reachable vi following arrows forwards
direction repeatedly. non-descendants vi , ND (vi ), nodes (not including vi )
descendants vi .
Let graph G = (V, E) joint probability distribution P nodes V . Let
IP (X,Y |Z) mean variables set X conditionally independent
variables set probability distribution P given variables set Z. Say also
following true
v V. IP ({v} , ND (v) |Pa (v)) .
is, node conditionally independent non-descendants, given parents.
said G satisfies Markov condition P, (G, P) Bayesian network. Notice
conditional independencies implied Markov condition. allow joint distribution P
written product conditional distributions; P (v1 |Pa (v1 )) P (v2 |Pa (v2 )) P (vn |Pa (vn )) =
P (v1 , v2 , . . . , vn ). However, importantly, reverse also true. Given DAG G
either discrete conditional distributions certain types continuous conditional distributions (e.g.
Gaussians), form P (vi |Pa (vi )) exists joint probability distribution
P (v1 , v2 , . . . , vn ) = P (v1 |Pa (v1 )) P (v2 |Pa (v2 )) P (vn |Pa (vn )) .
means specify DAG known structure conditional probability distributions node given parents, often parameterised, Bayesian network,
representation joint probability distribution.
learning Bayesian network data, structure G parameters conditional
probability distributions must learned, normally separately. case complete multinomial
data, problem learning parameters easy given certain reasonable assumptions,
simple closed form formula (Heckerman, 1995). However, case learning structure,
formula exists methods needed. fact, learning optimal structure
discrete variables NP-hard problem almost circumstances consequently enumeration
test network structures likely succeed (Chickering, 1996a; Chickering, Heckerman,
& Meek, 2004). ten variables roughly 1018 possible DAGs. Whilst exist
dynamic programming methods handle roughly 30 variables discussed above, general,
non-exact heuristic methods possibly tractable solution anything this.
394

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

order create space search through, three components needed. Firstly
possible solutions must identified set states space. Secondly representation
mechanism state needed. Finally set operators must given, order move
state state space.
search space defined, two pieces needed complete search
algorithm, scoring function evaluates goodness fit structure set data
search procedure decides operator apply, normally using scoring function
see good particular operator application might be. example search procedure
greedy search, every stage applies operator produces best change structure,
according scoring function. scoring function, various formul found
see well DAG matches data sample.
One functions given computing relative posterior probability structure G
given sample data D, i.e.
S(G, D) = P(G, D) = P(D|G)P(G).
likelihood term take many forms. One popular method called Bayesian Dirichlet
(BD) metric. Here,
ri (N 0 + N )
n qi
(Ni0j )
jk
jk
P(D|G) =

(1)

0
0
(Ni jk )
i=1 j=1 (Ni j + Ni j ) k=1
formula, n variables graph, first product variable.
qi configurations parents node i, second product possible parent
configurations, i.e. Cartesian product number possible values parent variable
take. variable take one ri possible values. value Ni jk number times
variable = k parents configuration j data sample D. Ni j given
ri

Ni jk , i.e. sum Ni jk possible values take on. Ni0j = ri=1
Ni0jk ,
k=1
values Ni0jk given parameters give different variants BD metric. E.g. Ni0jk set 1
K2 metric results, given Cooper Herskovits (1992). Ni0jk set N 0 /(ri qi ) (where
N 0 , known equivalent sample size measure confidence prior network),
BDeu metric results proposed Buntine (1991) generalised Heckerman,
Geiger, Chickering (1995).
prior value P(G) measure probable particular structure data
seen. values often hard estimate massive numbers graphs,
needing probability. Therefore, values often given uniform possible
network structures possibly favouring structures less arcs.
forms used scoring function S(G, D) = log P(D|G, ) d2 log N, known
Bayesian information criterion (BIC) (Schwarz, 1978) S(G, D) = log P(D|G, ) d, known
Akaike Information Criterion (AIC) (Akaike, 1974). models, parameters give
maximum likelihood estimate likelihood, number free parameters structure
N number samples data D.
Traditionally, searching Bayesian network structure, set states set possible
Bayesian network structures, representation DAG set operators various small
local changes DAG, e.g. adding, removing reversing arc, illustrated Table 1. type

395

fiDALY & HEN

Operator





Insert_Arc(X,Y)

Delete_Arc(X,Y)

Reverse_Arc(X,Y)

Table 1: Basic modification operators
search convenient decomposition properties score functions,
n

S(G, D) = vi , PaG (vi ) , ,
i=1

scoring function takes node vi parents node graph G, PaG (vi ).
Popular scoring functions BD metric decomposable manner. Successful
application operators also dependent changed graph DAG, i.e. cycle
formed applying operator.

3. Searching Space Equivalence Classes
According many scoring criteria, DAGs equivalent one another, sense
produce score other. known time DAGs
equivalent one another, entail set independence constraints
other, even though structures different. According theorem Verma Pearl (1991),
two DAGs equivalent skeletons set v-structures.
skeleton undirected graph results undirecting edges DAG (see Figure
3) v-structure (sometimes referred morality), head-to-head meeting two arcs,
tails arcs joined. concepts illustrated Figure 4. notion
equivalence, class DAGs equivalent defined, notated
Class(G).
3.1 Representation Equivalence Classes
apparent redundancy space DAGs, attempts made conduct
search Bayesian network structures space equivalence classes DAGs (Acid &
de Campos, 2003; Chickering, 1996b, 2002a; Munteanu & Bendou, 2001). search set
space set equivalence classes DAGs referred E-space. represent
members equivalence class, different type structure used, known partially directed
acyclic graph (PDAG). PDAG (an example shown Figure 5) graph may
contain undirected directed edges contains directed cycles notated
396

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

(a) (X,Y, Z) v-structure

(b) (X,Y, Z) v-structure

Figure 4: V-Structures

Figure 5: partially directed acyclic graph

Figure 6: PDAG exists consistent extension

herein P. equivalence class DAGs corresponding PDAG denoted Class(P),
DAG G Class(P) G P skeleton set v-structures.
Related idea consistent extension. DAG G skeleton
set v-structures PDAG P said G consistent extension P.
PDAGs DAG consistent extension itself. consistent extension exists,
said PDAG admits consistent extension. PDAGs admit consistent extension
used represent equivalence class DAGs hence Bayesian network. example
PDAG consistent extension shown Figure 6. figure, directing
edge x either way create v-structure exist PDAG hence
consistent extension exist.
Directed edges PDAG either: compelled, made directed way; reversible,
could undirected PDAG would still represent equivalence class.
idea, completed PDAG (CPDAG) defined, every undirected edge reversible
equivalence class every directed edge compelled equivalence class. CPDAG
denoted P C . shown one-to-one mapping CPDAG
P C Class(P C ). Therefore, supplying CPDAG, one uniquely denote set conditional
independencies. in-depth look topic, see papers Andersson, Madigan,
Perlman (1997) Chickering (1995).

397

fiDALY & HEN

3.2 Techniques Searching Equivalence Classes
Note below, move referred application operator particular state
search space.
able conduct search space equivalence classes, method must able
find whether particular move valid valid, good move is. tasks
relatively easy whilst searching space DAGs check whether move valid
equivalent check whether move keeps DAG acyclic. goodness move found
using scoring function, rather scoring neighboring DAG search space,
decomposability scoring criteria taken advantage of, result
nodes whose parent sets changed need scored.
However, task checking move validity move score easy space
equivalence classes. classes often represented PDAGs, discussed previous
section. one, instead checking cycles, checks also made unintended
v-structures created consistent extension PDAG. Scoring move also creates
difficulties, hard know extension hence changes parent sets nodes
occur, without actually performing extension. Also, local change PDAG might make
non-local change corresponding consistent extension force unnecessary applications
score function.
problems voiced concerns Chickering (1996b). paper, validity checking
moves performed trying obtain consistent extension resulting PDAG none
exists move valid. Scoring move achieved scoring changed nodes
consistent extension given. methods generic, resulted significant slowdown
algorithm execution, compared search space DAGs.
alleviate problem, authors proposed improvements would allow move validity
move score computed without needing obtain consistent extension PDAG (Acid &
de Campos, 2003; Chickering, 2002a; Munteanu & Bendou, 2001). done defining
explicit set operators, operator validity test corresponding score change
function, could calculated PDAG. changes led speedup execution
time algorithm, result search space equivalence classes Bayesian
networks became competitive search space Bayesian networks. example one set
operators given Table 2. table, variables x refer nodes graph.
example, InsertU operator takes two nodes arguments, x y. seen
operators take two arguments, except MakeV, takes three arguments. operator also
set validity tests must passed order application operator particular
arguments valid. Finally, score difference old new PDAGs given
last column.
Note table:
x parent set node x, i.e. set nodes directed arcs going node x;
Nx neighbor set node x, i.e. set nodes undirected arcs going node x;
Nx,y set shared neighbors nodes x y, i.e. Nx Ny ;
x,y set parents x neighbors y, i.e. x Ny .

398

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Operator

Effect

InsertU
xy

Add
undirected arc
x

DeleteU
xy

Delete
undirected arc
x

Validity Tests

Change Score

1. Every undirected path x
contains node Nx,y

2. x =

Nx,y clique


+x
y, Nx,y

(y, Nx,y )

(y, Nx,y )

+x
y, Nx,y


1. Every semi-directed path
InsertD
xy

Add directed
arc x

x contains node
x,y

2. x,y clique


y, x,y +x

(y, x,y )

3. x 6=
DeleteD
xy

Delete directed
arc x

Ny clique

1. Every semi-directed path
ReverseD
xy

Reverse
directed arc
x

x
include edge x
contains node y,x Ny

2. y,x clique

MakeV
xzy

Direct undirected
arcs x
z

Every undirected path
x
contains node Nx,y


y, Ny x

(y, Ny )
y, x




+ x, +y
x y,x
(y, )
(x, x y,x )


z+x
z, +y
z Nx,y

z
+ y, Nx,y

z+x
z, z Nx,y
(y, Nx,y )

Table 2: Validity conditions change score operator

399



fiDALY & HEN

Also, convenience, +x notation {x} x notation \ {x}.
notation set operators Table 2 come proposed Chickering (2002a).
definitions include: undirected path path one node another follows
undirected edges; semi-directed path path one node another follows undirected
edges directed edges tail head; set nodes N clique, completely
connected subgraph graph, (i.e. every node connected every subgraph).
3.3 Advantages Searching E-space
representation equivalence classes Bayesian network structures set operators
modify CPDAGs represent (e.g. insert undirected arc, insert directed arc
etc.), search procedure proceed. However, reasons pursuing type
search? Chickering (2002a) gives list reasons, discussed here.
one, equivalence class represent many different DAGs single structure.
DAG representation, time wasted rescoring DAGs equivalence class.
search space DAGs, connectivity search space mean ability
move particular neighboring equivalence class constrained particular representation
given DAG. also problem given prior probability used scoring function.
Whilst searching space DAGs, certain equivalence classes represented
prior, many DAGs contained class. example given
case networks two nodes. B-space 3 possible structures, equal
priors give P (G) = 1/3, DAG G. However, two DAGs connected represent
equivalence class, giving effective prior 2/3. E-space 2 possible structures,
equal priors give P (P) = 1/2, PDAG P. necessarily problem
performing model selection, becomes much issue performing model averaging.
concerns motivated researchers. particular, recent implementations algorithms
search space equivalence classes produced results show marked
improvement execution time small improvement learning accuracy, depending type
data set (Chickering, 2002a,b).

4. Ant Colony Optimization
Ant colony optimization global optimization technique generally used area combinatorial
problems, i.e. problems set solutions discrete. Since inception present form
Dorigo (1992), ACO successfully applied many combinatorially hard problems
including sequential ordering problem (Gambardella & Dorgio, 2000), vehicle routing
problem (Bullnheimer, Hartl, & Strauss, 1999), bin-packing problem (Levine & Ducatelle, 2004)
many (Costa & Hertz, 1997; Gambardella & Dorgio, 2000; Maniezzo & Colorni, 1999;
Sttzle, 1998). diverse range applications must ask question nature
system solve them.
particular form ACO metaheuristic field swarm intelligence (Bonabeau,
Dorigo, & Theraulaz, 1999), based behavior real-life ants forage food.
metaheuristic general purpose heuristic guides other, problem specific heuristics,
whilst swarm intelligence may defined as:

400

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

algorithms distributed problem-solving devices inspired collective behaviour social insect colonies animal societies (Bonabeau, Dorigo et al.,
1999).
conceptual framework ACO defined.
4.1 Ant Colony Optimization
Ant colony optimization swarm intelligence technique based foraging behavior
real-life ants. particular, uses principle stigmergy (the indirect communication agents
environment) communication mechanism. Real-life ants leave chemical trail behind
explore environment. trail known pheromone. moving around, ants
likely follow path pheromone, path less (or no) pheromone.
behavior investigated Deneubourg, Aron, Goss, Pasteels (1990), designed
experiment nest Argentine ants, food source two trails could set
different length. Ants would leave nest, find food source return back food.
trails length, found ants would eventually settle single trail
travel nest. behavior explained follows.
experiment begins, ants initially choose one trails random. Whilst traversing
trail, deposit pheromone. causes following ants choose trail initial ants took
often, deposit pheromone trail. Again, causes ants choose
initially chosen trail, greater degree first set ants. Put another way, ant
chooses certain trail reinforces probability following ants choose trail. trail
initially gets chosen ants pheromone deposited per unit time hence
positive feedback autocatalytic process created, eventually ants converge single
trail.
trails start different lengths, found ants converge shorter trail
often longer. explained ants able traverse shorter
trail food source return nest amount time would take traverse
longer trail. ants traversing trail, pheromone deposited, ants eventually
converge path.
behavior ants faced trails different lengths ACO modeled upon.
Instead real-life ants, artificial ants conceived computing unit. Instead trails, ants
traverse construction graph. paths ants take graph solutions problem
looked idea reinforce pheromone better solutions. However, fundamental idea
laying pheromone kept, ants depositing arcs traverse node node.
Also, ants programmed follow arcs stronger pheromone often arcs weaker
pheromone.
Artificial ants useful real-life ants given memory.
stop ants looping around helps laying pheromone return journey. Also
programmed use problem dependent heuristics, guide search towards better
solutions. ideas discussed.
4.2 ACO Metaheuristic
Nowadays, ACO algorithms tend defined terms ACO metaheuristic (Dorigo & Di
Caro, 1999). metaheuristic general purpose heuristic guides other, problem specific
401

fiDALY & HEN

heuristics. Examples metaheuristics include simulated annealing (Kirkpatrick, Gelatt, & Vecchi,
1983), tabu search (Glover, 1989, 1990), evolutionary computation etc.
ACO metaheuristic, problem represented triple (S, f , ), set
candidate solutions, f : objective scoring function measures solutions quality
particular time : set constraints time , used solutions construction.
range f dependent particular instance metaheuristic. trying map
combinatorial optimization problem onto representation, following framework used.
finite set solution components C = {c1 , c2 , . . . , cNc }. building
blocks candidate solutions.


ff
problem states represented sequences solution components x = ci , c j , . . . .
set possible sequences given X .
set candidate solutions mentioned subset X , i.e. X .
set feasible states X , X X . feasible state x X state
possible add components C x create solution satisfying constraints .
candidate solution cost g(s,t). Normally g(s,t) f (s,t), S,
= X set feasible candidate solutions. However, might always
case; f expensive compute, g might easier compute function broadly
similar f used generation solutions.
set optimal solutions non-empty, S.
Sometimes may also possible associate cost J(x,t) state x X
candidate solution.
framework, solutions problem (S, f , ) generated artificial ants
perform random walk complete graph G defined components C. graph G
known construction graph. random walk graph series moves node node
graph, move random degree. walk Markovian, next
move always completely random; next move influenced previous moves.
Hence, using terminology ACO non-Markovian. walk ant makes generally
biased two things heuristic value (i heuristic associated individual nodes
G, j associated edges G) pheromone trail (again, pheromone
associated individual nodes G, j pheromone associated edges
G). way heuristic pheromone implemented problem dependent, general
heuristic measure goodness taking particular move construction graph
defined local measure. pheromone measure goodness taking
particular move defined aggregate behavior ants selecting move quality
solutions ants generate.
Finally, artificial ant k following properties order fully specify random
walk proceed:
Memory ant k memory Mk stores information path far followed.
Start State ant k start state xsk non-empty set termination conditions ek .
402

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Termination Criteria ant state x, checks one termination criteria ek
satisfied. not, moves node j N k (x). N k function returns neighborhood
node x, i.e. nodes construction graph G reached current
state, given constraints .
Decision Rule ant chooses node j according probabilistic decision rule,
function pheromone heuristic . specification rules problem
dependent, usually random choice biased towards moves higher heuristic
pheromone value.
Pheromone Update pheromone path modified ant traversing it,
return journey, returns start. Again, problem dependent, standard
formulation increase pheromone good solutions decrease pheromone
bad solutions, good bad given specific formulation.
terms algorithmic actions, ACO algorithm normally broken three parts.
are:
ConstructAntsSolutions part algorithm concerned sending ants around
construction graph according rules given above.
UpdatePheromones part concerned changing values pheromones,
depositing evaporating. Parts task might performed ants traversal
graph, ants traversal finished iteration ants traversals.
DaemonActions part algorithm performs tasks directly related ants. E.g.
local search procedure might performed ant finishes traversal.
Given framework, multiple artificial ants released perform random walk.
procedure repeated number times, pheromone gradually increasing best parts
solution.
many implementations metaheuristic. first original
ACO system designed Dorigo, Maniezzo, Colorni (1996) known Ant System. used
study traveling salesman problem, construction graph defined distances
cities. Another extension Ant System Ant Colony System (ACS) (Dorigo & Gambardella,
1997). Here, search biased towards best-so-far path, pseudo-random proportional
decision rule takes best solution component time normal random
proportional decision rule rest time. Also, best-so-far ant deposits pheromone.
ACS based system known ANT-Q designed Gambardella Dorigo (1995),
inspired reinforcement learning technique Q-learning (Sutton & Barto, 1998).
ACS particularly interesting context, system new work described
later sections modeled. work inspired previous approach
learning Bayesian networks using ACO (described Section 5.1) used ACS form
ACO.

5. Using Ant Colony Optimization Learning Equivalence Class
date, many state-based search algorithms create Bayesian network structure relied
simple techniques greedy-based searches. produce good results, ever
403

fiDALY & HEN

prevalent problem getting caught local minima. sophisticated heuristics applied,
iterated hill climbing simulated annealing (Chickering, Geiger et al., 1996), far,
none applied E-space. related approach, Acid de Campos (2003)
applied tabu search space restricted partially directed acyclic graphs (RPDAGs), halfway
house spaces given DAGs CPDAGs.
paper seeks apply ACO metaheuristic E-space, space equivalence classes
DAGs. end, two extensions made basic metaheuristic. first allow multiple
types moves. allow one operator used traversing state space.
needed, general, one type operator used whilst searching E-space.
second allow pheromone accessed arbitrary values normally accessed
single index two indices. needed operators used E-space
MakeV operator takes three nodes arguments.
proposed algorithm, ACO-E, based large part work de Campos, FernndezLuna et al. (2002), described next section.
5.1 ACO Algorithms Learning Bayesian Network Structures
Whilst ACO applied many problems area combinatorial optimization, date
much research using technique learn Bayesian network structures. Two
alternate methods defined de Campos, Fernndez-Luna et al. (2002) de Campos,
Gmez, Puerta (2002). first conducts search space orderings DAGs, whilst
second searches space DAGs. Since main topic work problem,
description given here, order examine early work done subject
see inform future studies.
5.1.1 ACO-K2SN
first technique, known ACO-K2SN, searching space orderings DAGs,
various problem components, taken Section 4.2 defined follows:
Construction Graph one node attribute data, extra dummy node
search starts.
Constraints constraints tour Hamiltonian path.
Pheromone Trails pheromone associated arc graph. arc graph
intialised initial small value.
Heuristic Information heuristic arc set inverse negative log likelihood
score explained below.
Solution Construction ants work system similar ACS system. Beginning
dummy node, ants construct complete path defines ordering nodes.
Pheromone Update works exactly ACS, local pheromone updates global update
best-so-far solution.
Local Search version local search orderings known HCSN (de Campos & Puerta, 2001a).
used last iteration run.
404

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Given components, search ordering proceeds follows. Starting dummy
node ant decides node go next. first node ordering. choose
node, heuristic information pheromone used. heuristic arc j given
1
fi,
j = fifi
f (x j, Pa (x j ))fi
f scoring metric used Pa (x j ), parents x j found K2 algorithm,
possible parents nodes already visited. initial pheromone value 0 given
0 =

1
,
n | f (SK2SN )|

SK2SN structure given K2SN algorithm de Campos Puerta (2001b).
update value pheromone given
j =

1
,
| f (S+ )|

S+ best-so-far structure.
5.1.2 ACO-B
second algorithm given de Campos, Fernndez-Luna et al. (2002) ACO-B algorithm.
components algorithm are:
Construction Graph one node possible directed arc pair attributes
(excluding self directed arcs). also dummy node ants start from.
Constraints constraints DAG must acyclic step.
Pheromone Trails pheromone associated node graph. pheromone
node (i, j) corresponds directed arc j i.
Heuristic Information heuristic node (i, j) gain score would occur
adding arc j i.
Solution Construction ants work system similar ACS system. Beginning
dummy node, ants construct path defines arcs added DAG.
process ends gain score.
Pheromone Update works exactly ACS, local pheromone updates global update
best far solution.
Local Search standard greedy search arc addition, deletion reversal carried
current candidate DAG. done every 10 iterations.
opposed ACO-K2SN algorithm given Section 5.1.1, search space
DAGs, orderings DAGs. Otherwise, similarities definitions parts
algorithm. heuristic given

j = f xi , Pa (xi ) x j f (xi , Pa (xi )) ,
405

fiDALY & HEN

is, change score adding arc j candidate DAG. initial pheromone
given
1
,
0 =
n | f (SK2SN )|
i.e. heuristic ACO-K2SN. Also, pheromone update value
ACO-K2SN, i.e.
1
j =
| f (S+ )|
5.1.3 P ERFORMANCE C OMPARISON
results given de Campos, Gmez et al. (2002) de Campos, Fernndez-Luna et al.
(2002), ACO-B algorithm performs slightly better terms accuracy ACO-K2SN across
ALARM (Beinlich, Suermondt, Chavez, & Cooper, 1989) INSURANCE (van der Putten &
van Someren, 2004) gold-standard networks. also contains order magnitude less statistical
tests always faster. comparisons ACO-B algorithms
de Campos, Fernndez-Luna et al. (2002). Here, compared ILS, iterative local
search algorithm random perturbations local maximum two estimation distribution
(EDA) genetic algorithms, univariate marginal distribution algorithm (UMDA) Mhlenbein
(1997) population-based incremental learning algorithm (PBIL) Baluja (1994). Compared
across ALARM, INSURANCE BOBLO (Rasmussen, 1995) networks, ACO-B performed
better methods.
5.2 Relation ACO-E ACO Metaheuristic
proposed algorithm, ACO-E, based large part, work de Campos, Fernndez-Luna
et al. (2002). work, ACO algorithm called ACO-B applied learning Bayesian
networks. current work differs searches E-space, uses one operator (add
arc) constrain using matrices store pheromone. algorithm shown
Algorithm 1.
section, relation various parts algorithm ACO framework
given. problem learning Bayesian network structure stated triple (S, f , ),

S, set candidate solutions, set CPDAGs nodes Bayesian
network. set massive cardinality, super-exponential number nodes.
f , objective function function used score candidate DAG. function would
generally one scoring criteria mentioned Section 2.
, set constraints, makes sure PDAGs consistent extensions
generated solutions. explanation idea consistent extension PDAG given
Section 3.1. formulation presented, constraints implicit operators
used move state state.
Given statement problem, ACO-E algorithm described following
properties. properties relate ACO metaheuristic described Section 4.2.

406

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Algorithm 1 ACO-E
Input: Operators O, tmax , tstep , m, , q0 , , n
Output: PDAG P +
(P + , Path+ ) GREEDY- E(P empty , Pathempty )
0 1/n |SCORE (P + )|
operator
possible move P empty
0
end
end
1 tmax
k 1
P k , Pathk ANT- E(O, q0 , , , 0 )
(t mod tstep = 0)

P k , Pathk GREEDY- E P k , Pathk
end
end

k
b arg maxm
SCORE
P
k=1

SCORE P b > SCORE (P + )
P+ Pb
Path+ Pathb
end
move Path+
(1 ) + / |SCORE (P + )|
end
end
return P +
5.2.1 C ONSTRUCTION G RAPH
construction graph ACO algorithm describes mechanism solutions
assembled. specified complete graph given solution components. such,
components play crucial part viability algorithm.
ACO-E algorithm, components C construction graph various moves
may made, i.e. move instantiation supplied operator; experiments presented
paper, six operators Table 2 used. operators used verified
work correctly effectively Chickering (2002a). Designing correct operators difficult,
Chickering showed finding counter examples validity operators Munteanu
Cau (2000). ant constructs solution walking construction graph. corresponds
applying sequence moves CPDAG. order procedure begin, starting state must
specified. ACO-E given empty graph.
usual, states problem sequences moves. However, every state
candidate solution, = X ACO metaheuristic framework. imply states
feasible candidate solutions, candidate solutions length. also
means = X . Another way view state ant consider empty graph P (the
407

fiDALY & HEN



ff
starting state) current state sequence moves (components) x = ci , . . . , c j . Applying
move c x order P give CPDAG another representation current state.
noted constraints implicitly taken care operators, i.e.
validity tests operators satisfy constraint state valid PDAG. also
stated usual definition
g(s,t) f (s,t),
applies, function J(x,t), since x candidate solutions adding solution
component decrease cost.
5.2.2 P ROBLEM H EURISTIC
ACO algorithm, heuristic used guide search good solutions. often
implicitly terms cost associated choosing particular component add current
state; adding component least cost often useful way proceeding constructing
solution.
ACO-E, heuristic used manner, addition cost adding
component negative, i.e. adding component current state improve cost function
g. heuristic dynamic depends current state ant. Also, associated
component c C opposed arcs ci c j components.
value heuristic given score gain move ci C possible given
current state. essence corresponds change score given performing particular
move current CPDAG. operators used article, means values
Table 2.
5.2.3 P ROBLEM P HEROMONE
pheromone ACO algorithm guides search based results previous searches.
many instances, associated arcs construction graph, ACO-E associated
nodes construction graph. gives pheromone values ci C.
pheromone initialised value 0 given
0 =

1
n |SCORE (P + )|

.

(2)

formula, n number variables data, SCORE objective function
f , defined Section 5.2 P + best-so-far solution. start algorithm,
initialised found greedy search starting empty graph.
order pheromone may change reflect tours ants, pheromone update rules
given. Similar ACS, local evaporation rule, whereby pheromone removed path
ant traverses
(1 ) + 0
shows effect parameter , pheromone evaporation deposition rate.
formula, implicit bounds high low pheromone component
get. Also similar ACS, global pheromone update rule deposits new pheromone
best-so-far path
fi
fi
(1 ) + / fiSCORE P + fi
408

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

occurs end run ants. Again, SCORE P + defined Equation 2. Also
again, formula implements implicit limits values pheromone take.
5.2.4 P ROBABILISTIC RANSITION RULE
choosing component visit next given particular state, ACO algorithm utilises
probabilistic transition rule. rule normally uses values given heuristic pheromone
inform choice node pick. actual choice random based distribution
given heuristic pheromone possible choice. ACO-E, probabilistic choice
rule given pseudo random proportional choice rule, similar one used ACS.
type rule allows balance exploration exploitation varied. able
change balance important, shown produce quite different results (Dorigo &
Sttzle, 2004). ant chooses component cm , given
(
arg maxmN (x) [m ] , q q0

random proportional,
otherwise.
formula, N (x) set components ant state x move to, given problem
constraints . rule pseudo-random proportional, sometimes behaves manner
random. random number q drawn uniformly range [0, 1]. number
less equal parameter q0 , rule behaves greedily; best move possible taken
dependent value [m ] component cm . Here, pheromone
heuristic explained previously parameter says much favour heuristic
pheromone.
number q greater q0 random proportional rule used select
component visit next. probability ant visit component cm given pm ,
pm =

[m ]
,
N (x)

N (x).

(3)

seen probability ant moves component cm directly given [m ] ,
normalised possible moves range [0, 1].
5.2.5 P ROPERTIES NTS
terms ants used construct solutions, following properties ant k noted:
memory Mk equated current state problem given ant k. this,
current CPDAG constructed order implement constraints , compute
heuristic values , evaluate current solution lay pheromone tour. practice,
current CPDAG normally kept order avoid recompute every step.
start state xsk given empty sequence hi, i.e. empty CPDAG.
single termination condition ek , stop tour improvement score
possible.
neighborhood N k (x) set valid moves given current CPDAG.
409

fiDALY & HEN

5.2.6 L OCAL EARCH P ROCEDURE
often case ACO algorithms, ACO-E use local search procedure intermediate
points throughout run algorithm end. local search procedure used
quickly bring solution local maximum. current heuristic standard local
search would used circumstances greedy search operators defined Table
2, known GREEDY-E local search would provide additional benefit solution
found ant. Nevertheless, local search put algorithm case problem
heuristic implemented differently. example would static heuristic obtained
scoring operations empty graph. Since invariant algorithm run, would
need calculated start run.
5.3 Description ACO-E
section focus giving algorithmic description ACO-E. done conjunction
pseudo code given Algorithms 1 2. ACO-E takes input number parameters
returns best PDAG found, according scoring criterion SCORE , defined objective
function f . assumed scoring criteria generally give negative values; higher value,
better model. case standard criteria discussed Section 2.
meaning parameters follows:
set operators modify current PDAG state search. Examples
ones given Table 2, e.g. InsertU, DeleteU, etc. However, operators could
used, e.g. Munteanu Cau (2000) Munteanu Bendou (2001).
tmax number iterations algorithm run. iteration, number ants
construct solutions. Pheromone deposition happens ants finished tours.
tstep gap, iterations, local search procedures run. set
tstep > tmax , local search happens end algorithm run.
number ants run iteration.
This, value [0, 1], rate pheromone evaporates deposited. used
pheromone evaporation pheromone deposition rules Section 5.2.3.
q0 This, value [0, 1], gives preference exploitation exploration. used
pseudo-random probabilistic transition rule explained Section 5.2.4.
exponent gives relative importance heuristic pheromone levels deciding
chance particular trail followed. used pseudo-random probabilistic
transition rule Section 5.2.4.
n number nodes PDAG.
also variables algorithm. include:
P + best-so-far PDAG;
Path+ best-so-far path;
410

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Algorithm 2 ANT-E
Input: Operators O, , q0 ,
Output: PDAG P, Path Path
Empty PDAG P, Empty path Path
true
possible moves P using
|M| = 0 maxlM TOTAL - SCORE(l, ) 0
return (P, Path)
end
q random number [0, 1)
q q0
l arg maxlM TOTAL - SCORE(l)
else
l random according Equation 3
end
(1 ) l + 0
P apply l P
Path append l Path
end
P empty empty PDAG;
Pathempty empty path, i.e. path entries.
starting algorithm, greedy search (called GREEDY-E) performed. search
space equivalence classes using framework operators given Chickering (2002a)
shown Table 2. gives starting best-so-far graph path search proceed.
Pheromone levels solution component initialised 0 = 1/n |SCORE (P + )|.
main loop algorithm begins tmax iterations. iteration, ants perform search,
given algorithm ANT-E, shown Algorithm 2. Also, every tstep iterations, local search
performed PDAGs returned ANT-E, try improve results. Using local search
part ACO algorithm common technique (Dorigo & Sttzle, 2004), easy way
obtain good results little effort. ants traversed graph, best graph
Algorithm 3 TOTAL - SCORE
Input: Move l,
Output: Score
(
l (l )
return =
0

l > 0
otherwise

path selected best-so-far graph path ones found ants
current iteration. Finally, global pheromone update lays evaporates pheromone
best-so-far path.
ANT-E algorithm creates PDAG examining various states may proceeded
current state, given set operators may act current PDAG. selects
411

fiDALY & HEN

Figure 7: Bayesian network used sample trace
new state based random-proportional choice rule. parameters function
description ones ACO-E function.
Starting out, algorithm constructs empty PDAG. stage move made
new PDAG, reached applying one operators O. Initially, number
given move TOTAL - SCORE, shown Algorithm 3. number represents weight given
move l depending current pheromone associated making move l ,
heuristic associated making move l . heuristic given increase score obtained
taking move, higher overall scores meaning better solutions. increase
score, ant stops returns solution P path followed. Otherwise possible
move ant decides make it. Firstly random number q obtained. less
specified value q0 , best move taken. greater q0 , random proportional
choice made, probability better moves higher. this, local pheromone
update applied path taken, path updated new location end
current state updated become new state given l. Note applying move CPDAG
change state implies resulting PDAG extended DAG suitable method
(e.g., Dor & Tarsi, 1992) DAG changed back CPDAG. Details found
article Chickering (2002a).
5.4 Trace Algorithm Execution
simple example execution ACO-E algorithm, trace behavior
actual execution given section. Consider Bayesian network Figure 7.
network fully specified, DAG structure parameters given form conditional
412

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION



q0



tmax



0.1

0.1

1.0

1

2

Table 3: Parameters sample trace
Move





1

InsertU(0,1)
InsertU(0,2)
InsertU(1,2)

0.00312
0.00312
0.00312

-0.21565
34.6527
11.2204

2

InsertU(0,1)
InsertU(1,2)
DeleteU(0,2)

0.00312
0.00312
0.00312

-0.21565
11.2204
-34.6527

3

InsertU(0,1)
DeleteU(0,2)
DeleteU(1,2)
MakeV(0,1,2)

0.00312
0.00312
0.00312
0.00312

0.37742
-34.6527
-11.2204
0.59307

Move





4 (Ant 1)

InsertU(0,1)
DeleteD(0,2)
DeleteD(1,2)
ReverseD(0,2)
ReverseD(1,2)

0.00312
0.00312
0.00312
0.00312
0.00312

-0.21565
-35.2457
-11.8134
-0.59306
-0.59307

4 (Ant 2)

DeleteU(0,1)
DeleteU(0,2)
DeleteU(1,2)

0.00312
0.00312
0.00312

-0.47742
-35.2457
-11.8134

Table 4: Values corresponding moves Figure 8
probability tables. seen, variable 0 take values b, variable 1
take values c variable 2 take values e, f g.
purposes demonstration, 90 data sampled Bayesian network.
ACO-E algorithm started parameters set Table 3. PDAG found
initial GREEDY-E run sample Bayesian network structure. P + set
PDAG. score PDAG 106.918. 0 set 0.00312. tmax set
1, one iteration algorithm. iteration, two ants constructed solutions
using ANT-E procedure. trace ants proceeded shown Figure 8 Table
4. diagram, sequence moves seen along value q step.
score final network ant also shown. table, possible moves point
ant shown, along pheromone heuristic value . noted
pheromone moves, start ACO-E algorithm
pheromone deposition occurred. move, pheromone evaporation occurs, more,
difference found pheromone values equal 0 .
two ants finish run, best solution chosen variable b. case
Ant 1, score -106.918. compared score P + . two structures
same, score difference hence change occurs. Pheromone deposition
occurs moves made P + , i.e. moves Path+ . case, pheromone
InsertU(0,2), InsertU(1,2) MakeV(0,1,2) got updated (1 0.1) 0.00312 + 0.1/ |106.918| =
0.00374. Since tmax set 1, iterations algorithm returns P + .

413

fiDALY & HEN

Figure 8: Trace progress ANT-E
414

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

5.5 Implementation Issues
implementing algorithms given paper, care must taken avoid long run times.
Firstly, caching score node given parents simple technique greatly improve
performance. Secondly, caching results validity tests needed check moves
applicable certain state, increase performance dramatically. However technique
easy implement might appear (Daly, Shen, & Aitken, 2006).
Care must also taken implementing pheromone moves. Traditionally, matrices
values used, allow fast access updating. However case MakeV operator,
takes three indices, three dimensional matrix would needed. would quickly become
infeasible problem size grew, especially entries would used. would
due algorithm never getting states. Instead structure map store
information. map scale linearly number elements actually used. map
implemented tree, entries accessed logarithmic time hash table used, access
constant time.

6. Experimental Methodology
section concerned testing ACO-E algorithm presented Section 5 evaluation
results produced. order facilitate understanding experimental methodology used,
section structured follows.
Firstly, account given objects testing performed.
objects six gold-standard Bayesian networks well known field. various
properties networks discussed. networks data sampled
data used input algorithms.
Then, experiments using ACO-E algorithm shown. methodology used running
experiments defined, along description various evaluation criteria.
involve criteria well known field. Two different sets experiments presented, one
focused comparison ACO-E similar algorithms, comparison ACO-E
state-of-the-art algorithms. Also, behavior ACO-E algorithm different parameters
shown.
6.1 Standard Bayesian Networks
section set six gold-standard Bayesian networks presented. networks
basis testing showcased later. Various properties networks given,
covering: number nodes structure, number edges structure, average
number edges etc.
6.1.1 IX G OLD -S TANDARD N ETWORKS
experiments shown next section, six gold-standard networks used.
ALARM (Beinlich, Suermondt et al., 1989), Barley (Kristensen & Rasmussen, 2002), Diabetes
(Andreassen, Hovorka, Benn, Olesen, & Carson, 1991), HailFinder (Abramson, Brown, Edwards,
Murphy, & Winkler, 1996), Mildew (Jensen, 1995) Win95pts networks (Microsoft Research,
1995). networks chosen covered wide range domains, easily

415

fiDALY & HEN

Nodes
Edges
Mean In-Degree
V-Structures
V-Struct/Nodes

Alarm

Barley

Diabetes

HailFinder

Mildew

Win95pts

37
46
1.24
26
0.70

48
84
1.75
66
1.38

36
48
1.33
21
0.58

56
66
1.18
37
0.66

35
46
1.31
37
1.06

76
112
1.47
135
1.78

Table 5: Bayesian network properties
available contained discrete attributes. last property important scoring
criterion would used experiments implemented multinomial random variables.
Various properties Bayesian networks shown Table 5. table, Nodes
Edges specify number nodes edges respectively graph. Mean In-Degree
average number arcs coming node graph. equal Mean Out-Degree
number edges divided number nodes. Finally, V-Structures V-Struct/Nodes show
amount v-structures graph amount v-structures divided number
nodes.
6.2 Methodology
section contains details experiments performed using ACO-E algorithm described
Section 5. Firstly, methodology used running experiments presented. includes
analysis needed outcomes, design five experimental conditions explanation
evaluation criteria.
6.2.1 E XPERIMENTAL ESIGN
designing experimental methodology test efficacy ACO-E algorithm, three different
outcomes desired.
first analyze behavior algorithm function parameters
test networks. needed order try understand range values
parameters might useful show effect ACO behavior outcomes.
next desired outcome test ACO-E similar algorithms. end,
ACO-E tested another ACO algorithm algorithms searched space
equivalence classes.
Finally last desired outcome test ACO-E state-of-the-art algorithms
literature. tests would show comparative usefulness ACO-E
well-known good-performing methods.
order obtain outcomes, various experimental conditions designed,
explained below.
Scoring Function experiments, decided use BDeu criterion invented
Buntine (1991) described Section 2. According study Shaughnessy Livingston
416

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

(2005), BDeu best tradeoff precision recall edges (confusingly BDeu
called BAYES study, BDeu study meaning K2 metric). criterion gives
fully Bayesian score, assumption Dirichlet parameter priors uniform prior
possible states joint distribution given prior network.To fully specify BDeu criterion,
two pieces information needed. First prior structures P (G). could uniform
prior, structures P (G). Another method shown Heckerman, Geiger
et al. (1995) expert specify structure, method penalises differences
experts structure candidate structure.
second piece information needed equivalent sample size, N 0 , parameter
encodes confidence prior parameters prior structure. Selecting value
troublesome (Silander, Kontkanen, & Myllymaki, 2007; Steck & Jaakkola, 2003), reasonable
values range [1, 10] often work well.
recognition simpler structures often appealing, prior specified
method shown Heckerman, Geiger et al. (1995). formulation, two objects specified;
prior structure G prior prior distribution given as:
P (G) = c ,
c normalisation constant ignored, parameter needs specified
given formula
n

= ,
i=1

symmetric difference parent set node G prior G.
Condition 1 Experimental condition 1 designed analyze behavior ACO-E across
different parameters compare similar algorithms. algorithms ACOB (de Campos, Fernndez-Luna et al., 2002), EPQ (Cotta & Muruzbal, 2004; Muruzbal &
Cotta, 2004) greedy search space equivalence classes using Chickerings operators
(Chickering, 2002a) (called GREEDY-E here). description given.
ACO-B ACO-E based part construction algorithm similarities. ACO-B ACO based algorithm provides search space DAGs,
moves addition directed arc current DAG. detailed
description given Section 5.1.2.
EPQ method uses evolutionary programming algorithm performs search
space equivalence classes DAGs. Like Chickering (2002a), explicitly use CPDAGs
(defined Section 3.1) represent individuals, i.e. equivalence classes DAGs.
generation, population P, members population selected using binary
tournament mutated using operators Chickering. best P 2P selected
put forward next round, rounds.
GREEDY-E algorithm uses operators Chickering perform greedy search space
CPDAGs. results tests performed Chickering showed search generally
performed better search space DAGs.

417

fiDALY & HEN

Parameter

Value

N0

4
0.2
200
5, 7, 10, 12, 15, 20
0.0, 0.1, 0.2, 0.3, 0.4, 0.5
0.7, 0.75, 0.8, 0.85, 0.9, 0.95
0.0, 0.5, 1.0, 1.5, 2.0, 2.5


tmax


q0


Table 6: Parameter values testing ACO-E
experiments section, testing involved six standard networks presented Section
6.1.1. BDeu scoring criterion used, suggested Kayaalp Cooper (2002)
Heckerman, Geiger et al. (1995), equivalent sample size 4 used parameter priors.
Also empty structure prior defined Heckerman, Geiger et al. (1995) used.
individual run, 10,000 data sampled network used construct scoring
function. combination values parameter settings , q0 , m, run
experiment made ACO-E ACO-B algorithms. range values
parameters taken shown Table 6.
total gave 1296 runs algorithm, network. consequence, gave
total 216 results setting parameter. order match number runs,
EPQ GREEDY-E algorithm also run 216 times each. stressed run
ACO-E using particular combination parameters run EPQ GREEDY-E
done different data set sampled network. technique guards overfitting
parameters particular data set. also noted algorithm, limit 5
parents allowed node, order speed algorithm execution.
Condition 2 Experimental condition 2 designed test ACO-E state-of-the-art
Bayesian network structure learning algorithms. purposes results found study
conducted Tsamardinos, Brown, Aliferis (2006) used. study produced thorough
comparison many different algorithms made results available, allows results
ACO-E compared algorithms used study. various parameters
used ACO-E (that equivalent parameters algorithms) kept close possible
used Tsamardinos, Brown et al. various algorithms compared
were: max-min hill-climbing algorithm (MMHC) (Tsamardinos, Brown et al., 2006), optimal
reinsertion algorithm (OR) (Moore & Wong, 2003), sparse candidate algorithm (SC) (Friedman,
Nachman, & Peer, 1999), greedy search using three standard operators Table 1 (GS),
PC algorithm (PC) (Spirtes, Glymour et al., 2000), three phase dependency analysis algorithm
(TPDA) (Cheng, Greiner, Kelly, Bell, & Liu, 2002) greedy equivalent search algorithm
(GES) (Chickering, 2002b).
experiments, testing involved four six standard networks presented Section
6.1.1; Alarm, Barley, HailFinder Mildew. networks used experiments
Tsamardinos, Brown et al. use two (Diabetes Win95pts). networks
shown paper Tsamardinos, Brown et al. used available usable

418

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Parameter
Value

N0
10


0.09

tmax
200


20


0.4

q0
0.75


0.75

Table 7: Parameter values testing ACO-E


q0


Alarm

Barley

Diabetes

HailFinder

Mildew

Win95pts

0.4
0.8
0.5

0.4
0.8
1.0

0.4
0.7
1.0

0.2
0.8
1.0

0.4
0.7
0.5

0.2
0.95
2.5

Table 8: Tuned parameters ACO-E
format. run algorithm, 5000 data generated sampling particular networks
question. chosen opposed 10,000 data Condition 1, amount
chosen Tsamardinos, Brown et al.
Condition 1, BDeu scoring function used. parameter values function
ACO-E parameters shown Table 7. ACO-E parameter values chosen
represented reasonable values perform well instances. experiment run
100 times network.
Condition 3 Condition 3 designed number objectives mind. were:
examine effect different sample sizes ACO-E output;
use separate test sample scoring networks output ACO-E;
examine complexity ACO-E noting number statistics computed run.
order achieve objectives, new experiments run. experiments, parameters
set examining output experiments Condition 1 outputs seen
Section 7.1. optimum value parameters chosen finding best combination
Condition 1 (note Table 10 shows average BDeu score parameter setting).
experiments performed across six standard networks, five different sample sizes 100,
500, 1000, 5000 10000. various parameters set Table 8.
combination network sample size run 100 times. various parameters
set = 20 tmax = 200. BDeu scoring criterion used, empty structure
prior, equivalent sample size N 0 4 value = 0.05. meaning BDeu parameters
described above.
Condition 4 Tuned Metaheuristics order able compare ACO-E metaheuristics described Condition 1, experiments run tuned parameters. experiments
performed across six standard networks, sample size 10000. ACO-B,
various parameters set Table 9. combination parameters gave best BDeu
score ACO-B Condition 1. GREEDY-E EPQ meaningful parameters tune.
experiment run 100 times. ACO-B, various parameters set = 20
tmax = 200. Similar Condition 3, BDeu scoring criterion used, empty structure
419

fiDALY & HEN


q0


Alarm

Barley

Diabetes

HailFinder

Mildew

Win95pts

0.1
0.85
2.0

0.5
0.7
2.0

0.5
0.8
2.0

0.4
0.9
2.0

0.4
0.7
2.5

0.1
0.95
2.5

Table 9: Tuned parameters ACO-B
prior, equivalent sample size N 0 4 value = 0.05. runs, limit 7 parents
allowed node, opposed 5 Condition 1.
Condition 5 Examining Applicability ACO-E Experimental Condition 5 designed
test applicability ACO-E given data sets. achieve this, simple procedure designed
indicate level ACO-E algorithm would perform better simple greedy search.
procedure based GREEDY-E algorithm mentioned Condition 1. procedure
follows.
original data set sampled replacement GREEDY-E algorithm run.
purposes experiments, original data set sampled Bayesian network.
algorithm terminates, number v-structures returned structure counted divided
number variables data set. statistic noted procedure starts again,
new set resampled data. whole procedure repeated confident prediction
normalized v-structure mean made. mean value obtained used measure
complexity search space. higher value indicates v-structures hence
complicated space.
purposes paper, BDeu scoring function equivalent sample size N 0 4
equal structure priors used. Test performed across six standard networks
sample sizes 100, 500, 1000, 5000 10000. 100 resamplings used case.
6.2.2 E VALUATION C RITERIA
running experiments, various scoring metrics picked ascertain well certain
algorithms behaved. were: scoring function used running experiments, test scoring
function based different sample, structural Hamming distance (SHD), number
scoring function evaluations number distinct scoring function evaluations.
explained below.
Scoring Function experiments, BDeu scoring function used differing
parameters, depending experimental condition. parameters uniform given
condition, score value Bayesian network structure could used compare results
different algorithms. terms BDeu score, means higher average score
achieved, better results.
Test Scoring Function well scoring function used running algorithm,
separate BDeu scoring function defined, using independent, same-size sample
network used.

420

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Structural Hamming Distance order provide objective measure network structure
reconstruction behavior compare results work Tsamardinos, Brown et al.
(2006), value structural Hamming distance (SHD) metric given. measures
difference learned network gold-standard generating network. networks
transformed DAG CPDAG (if already representation) penalties given
number missing extra edges incorrectly directed arcs.
Score Function Evaluations order estimate complexity running ACO-E algorithm,
two statistics measured. first statistic number times scoring function
evaluated particular point time.
Distinct Score Function Evaluations next statistic number times distinct scoring
function evaluation occurred, i.e. number times arguments scoring function
different. statistic often wildly different total number scoring function evaluations
often better measure complexity, caching evaluations standard technique speed
algorithm runs.

7. Experimental Results
section results experiments performed according methodologies given Section
6.2 presented. 6.2, five experimental conditions given. first dealt analyzing
behavior ACO-E respect parameters comparison metaheuristic
algorithms shared similar behavior. second condition dealt comparing ACO-E
state-of-the-art Bayesian network structure learning algorithms. third condition focused
effect sample sizes output quality, behavior scoring function defined separate
test set computational complexity algorithm. fourth looked behavior
metaheuristic algorithms tuned behavior. Finally fifth condition dealt situations
ACO-E used. results presented order, followed discussion
interpretation results.
7.1 Condition 1
results runs using experimental condition 1 shown two sets, reflect
analyzed later. Firstly, detailed results ACO-E shown Tables 10 11.
tables, figures given results parameters; e.g. figure = 0.1 given
calculating mean standard deviation results = 0.1. case, size
samples 216, calculated combinations parameters.
noted specific values = 0 = 0 special cases. = 0,
pheromone evaporation pheromone deposition graph; i.e. pheromone plays part
algorithm. = 0, heuristic used whilst ants traverse construction graph.
comparative results involving ACO-E, ACO-B, GREEDY-E EPQ shown Table
12 Figures 9 10. show behavior ACO-E algorithms,
function algorithm iteration final value. results, iterations figure
ACO-E ACO-B. EPQ iteration number three times shown iteration. such,
whilst ACO-E ACO-B run 200 iterations, EPQ run 600 results scaled
200. done, concept iteration one framework translate well
terms time another framework.
421

fiDALY & HEN

7.2 Condition 2
results experiments conducted experimental Condition 2 illustrated here. second
set comparisons involved ACO-E state-of-the-art Bayesian network structure learning
algorithms.
results comparison shown Table 13. acronyms specified given
Tsamardinos, Brown et al. (2006) discussed Section 6.2.1. results
supplied Tsamardinos, Brown et al. missing marked N/A Table 13. result
range others, represented number stating median.
7.3 Condition 3
results experiments conducted according Condition 3 shown here. set
experiments designed show effects sample size ACO-E output also provide
measure computational complexity algorithm.
SHD results runs 200 iterations seen Table 14, whilst score results
200 iterations Table 15. Table 16 shows score results different test sample.
remaining results experiments shown Figures 11 12. show
total number score evaluations distinct number score evaluations respectively, runs
ACO-E algorithm.
7.4 Condition 4
Experimental Condition 4 used compare ACO-E metaheuristic algorithms used
Condition 1 parameters tuned best combinations Condition 1.
algorithms ACO-B, EPQ GREEDY-E. results consolidated Table 17
show results runs finished.
7.5 Condition 5
results experiments Condition 5 shown Table 18. experiments,
multiple searches performed using GREEDY-E algorithm, data resampled
experiment. Experiments performed 100 times across combination test networks
sample sizes.

422

fi
Alarm
Barley (105 )
Diabetes (105 )
HailFinder (105 )
Mildew (105 )
Win95pts (104 )

0.0

0.1

0.2

0.3

0.4

0.5

1.0383 0.0037
5.0756 0.0136
1.9394 0.0032
4.9207 0.0039
4.5426 0.0096
9.4322 0.0448

1.0385 0.0036
5.0697 0.0039
1.9391 0.0034
4.9206 0.0038
4.5412 0.0091
9.4169 0.0433

1.0387 0.0035
5.0702 0.0096
1.9394 0.0029
4.9204 0.0042
4.5417 0.0101
9.4086 0.0452

1.0385 0.0037
5.0696 0.0039
1.9386 0.0034
4.9210 0.0034
4.5401 0.0094
9.4125 0.0454

1.0388 0.0037
5.0699 0.0041
1.9395 0.0034
4.9202 0.0040
4.5388 0.0083
9.4154 0.0457

1.0380 0.0038
5.0699 0.0041
1.9393 0.0035
4.9207 0.0037
4.5395 0.0090
9.4210 0.0468

q0
(105 )

423

Alarm
Barley (105 )
Diabetes (105 )
HailFinder (105 )
Mildew (105 )
Win95pts (104 )

0.7

0.75

0.8

0.85

0.9

0.95

1.0385 0.0035
5.0703 0.0066
1.9391 0.0035
4.9207 0.0039
4.5362 0.0069
9.4200 0.0453

1.0388 0.0035
5.0704 0.0065
1.9391 0.0033
4.9208 0.0035
4.5378 0.0074
9.4183 0.0441

1.0383 0.0035
5.0705 0.0060
1.9393 0.0032
4.9208 0.0038
4.5389 0.0084
9.4199 0.0462

1.0382 0.0035
5.0708 0.0057
1.9393 0.0035
4.9205 0.0038
4.5410 0.0098
9.4192 0.0481

1.0386 0.0039
5.0710 0.0073
1.9393 0.0035
4.9204 0.0040
4.5430 0.0097
9.4160 0.0468

1.0383 0.0040
5.0720 0.0126
1.9390 0.0030
4.9204 0.0040
4.5472 0.0092
9.4130 0.0439

0.0

0.5

1.0

1.5

2.0

2.5

1.0387 0.0036
5.0775 0.0121
1.9389 0.0033
4.9212 0.0039
4.53780.0075
9.4515 0.0505

1.0378 0.0038
5.0694 0.0043
1.9390 0.0034
4.9205 0.0037
4.5371 0.0075
9.4092 0.0404

1.0383 0.0036
5.0689 0.0035
1.9394 0.0034
4.9203 0.0040
4.5392 0.0090
9.4135 0.0420

1.0386 0.0035
5.0696 0.0040
1.9393 0.0032
4.9206 0.0038
4.5404 0.0094
9.4101 0.0385

1.0387 0.0040
5.0697 0.0055
1.9394 0.0032
4.9205 0.0038
4.5440 0.0102
9.4116 0.0444

1.0387 0.0034
5.0698 0.0097
1.9391 0.0035
4.9205 0.0040
4.5456 0.0090
9.4106 0.0427


(105 )

Alarm
Barley (105 )
Diabetes (105 )
HailFinder (105 )
Mildew (105 )
Win95pts (104 )


(105 )

Alarm
Barley (105 )
Diabetes (105 )
HailFinder (105 )
Mildew (105 )
Win95pts (104 )

5

7

10

12

15

20

1.0383 0.0036
5.0721 0.0120
1.9392 0.0031
4.9202 0.0041
4.5440 0.0097
9.4201 0.0452

1.0386 0.0036
5.0707 0.00061
1.9392 0.0036
4.9209 0.0037
4.5427 0.0097
9.4190 0.0452

1.0381 0.0037
5.0709 0.0083
1.9391 0.0032
4.9201 0.0039
4.5409 0.0093
9.4178 0.0457

1.0389 0.0037
5.0704 0.0066
1.9392 0.0032
4.9210 0.0040
4.5392 0.0092
9.4188 0.0458

1.0384 0.0039
5.0704 0.0061
1.9391 0.0033
4.9204 0.0036
4.5386 0.0082
9.4164 0.0462

1.0384 0.0036
5.07050.0060
1.9394 0.0034
4.9210 0.0037
4.5385 0.0085
9.4145 0.0467

Table 10: Mean standard deviation BDeu score ACO-E parameter setting

L EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

(105 )

fiDALY & HEN


0.0

0.1

0.2

0.3

0.4

0.5

Alarm

6.9 4.9

5.6 3.1

6.0 3.2

5.6 3.3

5.9 3.0

5.5 3.0

Barley

56.4 10.8

52.8 3.9

53.0 4.1

53.2 4.4

52.6 4.0

52.9 4.8

Diabetes

63.5 5.8

65.5 5.3

65.0 5.4

65.1 5.6

64.2 5.4

63.7 4.8

HailFinder

50.6 6.9

50.5 6.8

51.0 7.8

51.8 7.8

51.8 6.7

51.1 7.9

Mildew

25.7 5.8

22.6 5.0

22.8 5.1

22.0 4.9

21.4 4.7

21.9 4.8

Win95pts

94.6 27.7

83.3 24.3

81.7 20.3

81.9 22.9

81.9 24.2

83.5 21.9

0.85

0.9

0.95

q0
0.7

0.75

0.8

Alarm

5.2 2.5

5.6 3.1

5.4 3.0

6.1 3.7

6.4 4.2

6.7 4.1

Barley

53.9 5.9

53.6 5.8

53.35.4

53.4 5.9

53.1 5.8

53.6 7.2
68.1 4.7

Diabetes

62.1 4.8

62.6 4.9

63.4 4.9

64.7 5.4

66.1 5.3

HailFinder

52.1 7.3

51.9 7.5

51.5 8.2

50.7 7.1

50.6 7.7

50.0 5.9

Mildew

20.2 3.8

21.1 4.7

21.5 4.8

22.6 5.1

24.3 5.4

26.6 4.9

Win95pts

90.3 23.5

88.1 24.5

84.9 22.6

83.2 22.5

81.0 22.2

79.3 27.1

1.5

2.0

2.5


0.0

0.5

1.0

Alarm

7.4 5.1

4.3 1.1

4.9 2.4

5.4 2.8

6.2 3.6

7.2 3.7

Barley

61.4 9.0

52.0 3.9

51.93.0

51.7 3.3

51.9 3.3

52.0 3.7

Diabetes

64.3 4.9

64.3 5.5

64.2 5.5

64.7 5.1

64.6 5.9

64.8 5.5

HailFinder

52.2 7.0

52.0 6.5

51.5 6.7

50.1 8.0

50.3 8.0

50.5 7.5

Mildew

20.9 4.9

20.9 4.0

21.9 5.0

22.4 5.1

24.6 5.4

25.7 5.2

Win95pts

109.1 28.6

89.6 23.9

79.8 17.9

77.0 17.2

77.1 19.3

74.4 15.5

12

15

20


5

7

10

Alarm

7.1 4.3

6.6 3.8

5.9 3.5

5.2 2.8

5.7 3.6

5.1 2.4

Barley

54.3 7.3

52.8 5.3

53.8 6.6

53.2 5.7

53.7 5.8

53.15.1

Diabetes

66.2 5.1

65.8 5.9

64.1 5.4

64.5 5.6

63.5 5.4

62.8 4.7

HailFinder

50.5 7.2

50.5 6.7

51.0 6.5

51.8 7.0

51.4 8.5

51.6 7.9

Mildew

24.6 5.3

23.9 5.6

22.7 5.5

22.2 4.9

21.6 4.7

21.4 4.7

Win95pts

83.3 23.3

83.0 21.9

85.6 26.2

85.5 22.1

85.5 25.0

84.0 25.5

Table 11: Mean standard deviation SHD ACO-E parameter setting

424

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

ACO-E
Alarm

Barley

HailFinder

Mildew

Win95pts

ACO-B

EPQ

SHD

5.9 3.5

21.9 9.0

11.9 11.9

26.1 13.4

Score (105 )

1.0385 0.0037

1.0389 0.0039

1.0388 0.0038

1.0415 0.0045

SHD

53.5 6.0

104.8 9.7

67.3 21.6

101.4 14.4

5.0708 0.0078

5.2449 0.0124

5.0944 0.0423

5.2354 0.0628

64.5 5.4

69.2 3.0

70.7 8.5

77.2 7.1

1.9392 0.0033

1.9394 0.0033

1.9406 0.0041

1.9457 0.0048

Score

Diabetes

GREEDY-E

(105 )

SHD
Score

(105 )

SHD

51.1 7.3

49.1 0.8

74.1 19.7

82.8 18.2

Score (105 )

4.9206 0.0038

4.9213 0.0036

4.9248 0.0058

4.9481 0.0177

SHD

22.7 5.3

29.3 0.7

36.1 14.0

50.3 13.8

Score (105 )

4.5407 0.0093

4.5531 0.0039

4.5548 0.0170

4.6148 0.0369

SHD

84.5 24.1

104.9 15.5

178.9 58.8

220.1 31.6

Score (104 )

9.4178 0.0457

9.4649 0.0466

9.4589 0.0717

9.9181 0.0970

Table 12: Mean standard deviation metaheuristic algorithms Condition 1 results

ACO-E
MMHC
OR1 k = 5
OR1 k = 10
OR1 k = 20
OR2 k = 5
OR2 k = 10
OR2 k = 20
SC k = 5
SC k = 10
GS
PC
TPDA
GES

Alarm

Barley

HailFinder

Mildew

16.4 4.7
9.6 7.0
27.8 10.0
31.2 11.1
37.8 9.4
21.2 4.6
33.2 5.4
39.4 6.5
34.2 3.6
20.4 11.8
58.8 6.5
15.2 1.5
9.6 1.5
N/A

80.9 5.3
102.6 9.2
109.6 9.5
113.6 15.6
136.4 2.9
120.0 4.5
109.2 16.2
116.8 18.4
129.6 13.1
N/A
143.3 7.3
610.0 10.6
207.2 4.0
159.0 0.0

55.0 5.3
208.0 1.6
190.8 14.1
183.2 14.9
184.6 17.2
184.6 14.5
187.0 15.7
200.8 9.2
194.2 2.5
N/A
204.2 9.9
385.6 12.5
255.4 3.4
154.6 54.3

31.0 3.6
58.4 7.4
70.6 4.2
75.6 6.3
75.0 4.8
69.2 3.3
64.0 4.4
67.4 3.4
N/A
N/A
62.2 12.2
421.2 10.7
97.8 6.8
38.8 0.8

Table 13: SHD mean standard deviation state-of-the-art algorithms

425

fiDALY & HEN

5

x 10
5.05

5

x 10
1.038

5.1
1.04

5.15
1.042

5.2
1.044
1.046

Score

Score

5.25

1.048

5.3
5.35

1.05

5.4

1.052

5.45
ACOE
ACOB
EPQ
GREEDYE

1.054
1.056
0

20

40

60

80

100
120
Iterations

140

160

180

ACOE
ACOB
EPQ
GREEDYE

5.5
5.55
0

200

20

40

60

(a) Alarm

80

100
120
Iterations

140

160

180

200

(b) Barley
5

x 10
4.92

5

x 10
1.938

4.93
1.94

4.94
1.942

4.95

Score

Score

1.944

1.946

4.96
4.97
4.98

1.948

4.99
ACOE
ACOB
EPQ
GREEDYE

1.95

1.952
0

20

40

60

80

100
120
Iterations

140

160

180

ACOE
ACOB
EPQ
GREEDYE

5
5.01
0

200

20

40

(c) Diabetes
5

100
120
Iterations

140

160

180

200

5

x 10
0.94

4.55

0.96

4.6

0.98

Score

Score

80

(d) HailFinder

x 10
4.5

4.65

4.7

1

1.02

ACOE
ACOB
EPQ
GREEDYE

4.75

4.8
0

60

20

40

60

80

100
120
Iterations

140

160

180

ACOE
ACOB
EPQ
GREEDYE

1.04

1.06
0

200

(e) Mildew

20

40

60

80

100
120
Iterations

(f) Win95pts

Figure 9: Scores metaheuristic algorithm comparison
426

140

160

180

200

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

70

140
ACOE
ACOB
EPQ
GREEDYE

60

ACOE
ACOB
EPQ
GREEDYE

130

Structural Hamming Distance

Structural Hamming Distance

120
50

40

30

20

110
100
90
80
70

10
60
0
0

20

40

60

80

100
120
Iterations

140

160

180

50
0

200

20

40

60

(a) Alarm

80

100
120
Iterations

140

160

180

200

(b) Barley

82

110
ACOE
ACOB
EPQ
GREEDYE

80

ACOE
ACOB
EPQ
GREEDYE

100

Structural Hamming Distance

Structural Hamming Distance

78
76
74
72
70

90

80

70

60

68
50
66
64
0

20

40

60

80

100
120
Iterations

140

160

180

40
0

200

20

40

(c) Diabetes

80

100
120
Iterations

140

160

180

200

(d) HailFinder

80

300
ACOE
ACOB
EPQ
GREEDYE

ACOE
ACOB
EPQ
GREEDYE

280
260
Structural Hamming Distance

70

Structural Hamming Distance

60

60

50

40

240
220
200
180
160
140
120

30

100
20
0

20

40

60

80

100
120
Iterations

140

160

180

80
0

200

(e) Mildew

20

40

60

80

100
120
Iterations

(f) Win95pts

Figure 10: SHD metaheuristic algorithm comparison
427

140

160

180

200

fiNetwork

Sample Size

100
500
1000
5000
10000

Alarm

Barley

Diabetes

HailFinder

Mildew

Win95pts

49.32 8.37
23.30 5.32
17.73 4.58
6.45 2.71
4.33 1.74

145.62 2.62
132.25 5.32
106.05 3.85
66.30 5.15
51.49 2.82

78.24 5.28
70.95 5.92
68.13 7.65
67.15 4.30
61.01 3.18

98.54 7.28
79.99 11.60
69.61 8.00
58.50 5.99
52.64 6.85

84.49 1.34
74.84 2.46
55.53 2.78
36.68 5.21
18.96 0.79

164.34 17.49
91.45 21.75
77.10 15.77
56.29 14.53
50.84 11.19

Table 14: Structural Hamming distance different sample sizes


Barley 105

0.0138 0.0005
0.0568 0.0010
0.1092 0.0015
0.5228 0.0029
1.0388 0.0037

0.0774 0.0006
0.3263 0.0041
0.5833 0.0028
2.6028 0.0024
5.0695 0.0035

Network


Diabetes 105
HailFinder 105

Mildew 105



Win95pts 104

0.0316 0.0009
0.1135 0.0028
0.2102 0.0013
0.9810 0.0027
1.9399 0.0032

0.0667 0.0006
0.2946 0.0019
0.5576 0.0022
2.4178 0.0037
4.5338 0.0043

0.1507 0.0006
0.5446 0.0012
1.0198 0.0016
4.7468 0.0035
9.3794 0.0043

Network


Diabetes 105
HailFinder 105

Mildew 105



Win95pts 104

0.0316 0.0007
0.1136 0.0028
0.2104 0.0012
0.9811 0.0025
1.9394 0.0037

0.0668 0.0006
0.2947 0.0015
0.5581 0.0021
2.4181 0.0038
4.5350 0.0045

0.1669 0.0007
0.5564 0.0011
1.0238 0.0014
4.7535 0.0031
9.3883 0.0040

0.0596 0.0003
0.2687 0.0012
0.5189 0.0023
2.4807 0.0027
4.9205 0.0037



Table 15: Training score different sample sizes

Sample Size

100
500
1000
5000
10000


Alarm 105


Barley 105

0.0143 0.0005
0.0571 0.0009
0.1092 0.0014
0.5229 0.0027
1.0387 0.0039

0.0778 0.0007
0.3272 0.0042
0.5835 0.0026
2.6031 0.0030
5.0702 0.0030

0.0604 0.0004
0.2690 0.0012
0.5192 0.0019
2.4811 0.0030
4.9214 0.0036

Table 16: Test score different sample sizes



DALY & HEN

428

Sample Size

100
500
1000
5000
10000


Alarm 105

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

8

4

8

x 10

5
100 samples
500 samples
1000 samples
5000 samples
10000 samples

3.5

100 samples
500 samples
1000 samples
5000 samples
10000 samples

4.5
4
Score Function Evaluations

Score Function Evaluations

3

x 10

2.5

2

1.5

3.5
3
2.5
2
1.5

1
1
0.5

0

0.5

0

20

40

60

80

100
120
Iterations

140

160

180

0

200

0

20

40

60

(a) Alarm

140

160

180

200

140

160

180

200

140

160

180

200

8

x 10

6
100 samples
500 samples
1000 samples
5000 samples
10000 samples

14

x 10

100 samples
500 samples
1000 samples
5000 samples
10000 samples

5

Score Function Evaluations

12
Score Function Evaluations

100
120
Iterations

(b) Barley

7

16

80

10

8

6

4

3

2

4
1
2

0

0

20

40

60

80

100
120
Iterations

140

160

180

0

200

0

20

40

60

(c) Diabetes
9

x 10

2.5
100 samples
500 samples
1000 samples
5000 samples
10000 samples

x 10

100 samples
500 samples
1000 samples
5000 samples
10000 samples

2
Score Function Evaluations

Score Function Evaluations

2

1.5

1

0.5

0

100
120
Iterations

(d) HailFinder

8

2.5

80

1.5

1

0.5

0

20

40

60

80

100
120
Iterations

140

160

180

0

200

(e) Mildew

0

20

40

60

80

100
120
Iterations

(f) Win95pts

Figure 11: Score function evaluations different sample sizes

429

fiDALY & HEN

4

4

x 10

8
100 samples
500 samples
1000 samples
5000 samples
10000 samples

Distinct Score Function Evaluations

10

x 10

100 samples
500 samples
1000 samples
5000 samples
10000 samples

7
Distinct Score Function Evaluations

12

8

6

4

6

5

4

3

2

2
1

0

0

20

40

60

80

100
120
Iterations

140

160

180

0

200

0

20

40

60

(a) Alarm

10
100 samples
500 samples
1000 samples
5000 samples
10000 samples

2.5
2
1.5
1

200

140

160

180

200

140

160

180

200

7
6
5
4
3
2

0.5

1

0

20

40

60

80

100
120
Iterations

140

160

180

0

200

0

20

40

60

(c) Diabetes

80

100
120
Iterations

(d) HailFinder

4

5

x 10

3.5
100 samples
500 samples
1000 samples
5000 samples
10000 samples

x 10

100 samples
500 samples
1000 samples
5000 samples
10000 samples

3
Distinct Score Function Evaluations

2.5
Distinct Score Function Evaluations

180

100 samples
500 samples
1000 samples
5000 samples
10000 samples

8

3

2

1.5

1

0.5

0

160

x 10

9

Distinct Score Function Evaluations

Distinct Score Function Evaluations

3.5

3

140

4

x 10

4

0

100
120
Iterations

(b) Barley

4

4.5

80

2.5

2

1.5

1

0.5

0

20

40

60

80

100
120
Iterations

140

160

180

0

200

(e) Mildew

0

20

40

60

80

100
120
Iterations

(f) Win95pts

Figure 12: Distinct score function evaluations different sample sizes

430

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

SHD
Score
Alarm

Barley

Diabetes

Test Score (105 )

Mildew

Win95pts

GREEDY-E

ACO-B

EPQ

4.33 1.74

24.17 9.16

5.98 4.63

16.09 9.92

1.0388 0.0037

1.0396 0.0038

1.0388 0.0040

1.0389 0.0037

1.0387 0.0039

1.0395 0.0044

1.0391 0.0038

1.0396 0.0037

Score Eval.

3.7e8 2.7e7

6.8e4 7.3e3

1.7e7 1.9e5

3.2e7 1.3e6

Dist. Score Eval.

1.2e5 5.6e3

2.9e3 2.1e2

7.2e4 2.3e3

2.7e4 2.3e3

SHD

51.49 2.82

106.58 8.95

52.95 3.71

91.18 17.36

Score (105 )

5.0695 0.0035

5.2415 0.0114

5.0698 0.0033

5.1677 0.0740

Test Score (105 )

5.0702 0.0030

5.2413 0.0116

5.0702 0.0034

5.1673 0.0734

Score Eval.

4.7e8 1.5e7

1.3e5 5.7e3

3.1e7 1.5e5

4.5e7 2.1e6

Dist. Score Eval.

6.2e4 1.3e3

4.1e3 1.3e2

5.8e4 1.0e3

4.3e4 2.7e3

SHD

61.01 3.18

68.71 3.13

66.97 4.88

77.13 7.10

Score (105 )

1.9399 0.0032

1.9397 0.0033

1.9392 0.0039

1.9451 0.0047

Test Score (105 )

1.9394 0.0037

1.9395 0.0030

1.9398 0.0032

1.9449 0.0044

Score Eval.

1.4e8 7.5e6

2.7e4 1.6e3

1.6e7 1.0e5

2.7e7 2.7e6

Dist. Score Eval.

4.1e4 1.8e3

2.2e3 3.1e1

3.3e4 1.0e3

1.6e4 1.6e3

SHD

52.64 6.85

49.20 0.89

61.59 11.63

78.81 16.32

4.9205 0.0037

4.9212 0.0039

4.9213 0.0036

4.9293 0.0073

4.9214 0.0036

4.9209 0.0038

4.9214 0.0038

4.9296 0.0080

Score
HailFinder

(105 )

ACO-E

(105 )

Test Score (105 )
Score Eval.

5.3e8 3.3e7

1.0e5 2.7e3

4.0e7 2.9e5

6.1e7 3.7e6

Dist. Score Eval.

8.9e4 3.2e3

5.4e3 8.7e1

6.9e4 2.2e3

5.5e4 3.9e3

SHD

18.96 0.79

29.22 0.77

19.41 3.83

43.59 11.79

Score (105 )

4.5338 0.0043

4.5527 0.0038

4.5348 0.0058

4.5982 0.0292

Test Score (105 )

4.5350 0.0045

4.5526 0.0044

4.5350 0.0052

4.5989 0.0299

Score Eval.

2.1e8 9.8e6

4.2e4 1.1e3

1.6e7 1.6e5

2.8e7 2.4e6

Dist. Score Eval.

2.2e4 5.4e2

2.2e3 3.7e1

1.5e4 2.6e2

1.5e4 1.2e3

SHD

50.84 11.19

85.75 16.44

91.08 18.52

231.25 42.46

Score (104 )

9.3794 0.0043

9.4121 0.0043

9.3890 0.0036

9.6061 0.0075

Test Score (104 )

9.3883 0.0040

9.4153 0.0045

9.3897 0.0042

9.6058 0.0080

Score Eval.

2.2e9 2.4e8

5.2e5 8.9e4

8.5e7 8.6e5

1.2e8 4.6e6

Dist. Score Eval.

3.2e5 1.9e4

1.5e4 7.9e2

2.7e5 1.2e4

1.9e5 5.4e3

Table 17: Mean standard deviation tuned metaheuristic algorithms

Sample Size

100
500
1000
5000
10000

Alarm

Barley

Diabetes

HailFinder

Mildew

Win95pts

0.25
0.40
0.46
0.57
0.58

0.01
0.12
0.32
0.72
0.93

0.06
0.32
0.34
0.36
0.30

0.06
0.18
0.28
0.44
0.45

0.00
0.02
0.12
0.71
0.82

0.53
1.30
1.53
1.96
2.11

Table 18: Mean number v-structures divided number nodes greedy searches
431

fiDALY & HEN

8. Discussion
section discuss results presented previous section. general, discussion
involve looking score SHD values (as defined Section 6.2.2) obtained
algorithms. noted better score necessarily mean better SHD value
vice-versa. occur small sample sizes parameters given
scoring function (such equivalent sample size value), shown
produce differences scoring function behavior (Kayaalp & Cooper, 2002). general, different
data sets different parameter values behave optimally. seem
general method find optimum values. problem looked depth
Silander, Kontkanen et al. (2007).
first figures examined Tables 10 11 Condition 1.
presented results experiments varied parameter values ACO-E algorithm.
Looking figures, evidence ACO-E algorithm provides useful behavior
reasonable values parameters.
Next, results experimental Condition 5 examined, particularly Table 18
context Bayesian network properties given Table 5. Along results show
behavior ACO-E function sample size Condition 3 (Tables 14, 15 16), discussion
seek characterize ACO-E performance perspective generating network
sample. Evidence presented shows ACO-E performs better complicated
networks, i.e. networks v-structures.
previous discussion focuses behavior ACO-E function various parameters.
next results looked intended provide comparison Bayesian
network structure learning algorithms. include Figures 9 10 Table 12 Condition
1 Table 17 Condition 4. present ACO-E metaheuristic algorithms
similar. results strong evidence ACO-E performing well
algorithms.
Also comparative perspective, results given Table 13 discussed.
present series tests comparing ACO-E state-of-the-art Bayesian network structure
learning algorithms. Again, looking figures, strong evidence ACO-E competitive
performance.
Finally, complexity results Condition 3 shown form Figures 11 12.
order perform comparison, statistical tests needed. non-normality
distributions results, tests others ones rely normality
data used. mentioned below.
8.1 ACO-E Behavior
section behavior ACO-E parameters varied analyzed. shown
Tables 10 11 evidence difference behavior ACO-E algorithm
depending input parameters. differences analyzed using two-tailed MannWhitney U test Students test. particular test used depends normality data,
tested Jarque-Bera test.
order perform comparison, best figures Tables 10 11 compared
situation particular part ACO-E algorithm turned off. E.g. Table 10
Alarm row, best figure = 0.5. compared value = 0.0,
432

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

value pheromone deposition evaporation occurring. values various parts
ACO-E algorithm turned = 0.0, q0 = 1 = 0.0. value
q0 = 1, algorithm behaves purely greedy fashion. Therefore purposes testing,
value GREEDY-E algorithm Table 12 used comparison, results would
exactly case q0 = 1. results comparisons shown Table 19.
table shows p-values comparison.
8.1.1 B EHAVIOR
Looking Table 19 results seem certain Barley, Mildew
Win95pts. Looking Tables 10 11 networks, values 0.2 0.4 range.
Also looking features networks Table 5 correspondence = 0.2
76 nodes (Win95pts), = 0.3 48 nodes (Barley) = 0.4 35 nodes (Mildew). Whilst
conclusive, suggests behaves well region 0.2 0.4 (for data sets works
all). fact much variance range networks means range
quite robust. also suggestion datasets nodes would use smaller values .
makes sense, larger networks would probably need spend time following best
solutions, low value would provide.
8.1.2 B EHAVIOR q0
parameter q0 appears effect networks, possible exception
HailFinder. networks (Alarm Barley) parameter large effect
wide range, whereas others (Diabetes, Mildew Win95pts), effect depends large extent
value q0 . largest effects scoring function point view appear
Barley, Mildew Win95pts networks.
Looking networks, large variations behavior across different values q0 make
difficult predict best value parameter might particular data set. One rule
thumb might smaller values q0 create exploration might useful smaller
data sets, whereas larger data sets need exploitation order get reasonable answer.
8.1.3 B EHAVIOR
Table 19, networks parameter plays role appear Alarm,
Barley Win95pts. differences best values scoring function
SHD difficult predict best value . case Barley, behavior quite robust
values range 0.5 2.5. However, Alarm Win95pts, behavior depends
value parameter smaller value better Alarm larger value Win95pts.
rule thumb appears networks less numbers nodes need smaller values
help avoid local minima, whereas networks nodes need larger values order focus
search effectively.
8.1.4 B EHAVIOR
Looking Tables 10 11 seen value sometimes small effect
effectiveness ACO-E. case, effect pronounced Alarm, Diabetes
Mildew networks, higher values giving smaller SHD. Indeed cases, higher values

433

fiDALY & HEN

never produce statistically worse results, expected. However, important bear
mind increased running times larger values m.
8.1.5 G ENERAL ISCUSSION
reason strange behavior HailFinder results possibly explained examining
graphs score function SHD time (Figures 9 10). seen score
improving iterations, SHD value deteriorating. might lead one conclusion
problem scoring function HailFinder case, perhaps parameters.
Another plausible reason HailFinder Win95pts results sync others
larger networks, might favour aggressive exploitation best-so-far
solution smaller ones. case, would correspond lower values higher
values q0 . Also heuristic information might useful large numbers variables, leading
better results large values . Note problems HailFinder network
also seen de Campos Castellano (2007).
8.2 Behavior ACO-E Respect Test Network Sample
previous section, seen ACO-E useful algorithm learning structure
Bayesian networks. also seen values parameters produced best
behavior depended network tested. rules thumb consolidate
characteristics observed previous section were:
data variables, lower values , higher values q0 higher values
.
data less variables, higher values , lower values q0 lower values .
However, also seen ACO-E always successful learning.
little difference seen certain parameters turned certain networks. Looking
Table 19, seems networks effect felt Barley,
Mildew Win95pts networks. this?
Looking Table 5 seem discernible pattern network
properties suitability algorithm. However, values number v-structures
normalised number nodes graph show definite reason. networks
ACO-E performed well larger number nodes higher in-degree
hence larger number v-structures. result this, data sampled networks better
going match similar network scoring function, i.e. one similar standard
network. search starts empty graph, likely search would get
trapped local minimum trying add enough arcs get needed number. Due ACO-E
stochastic algorithm, able avoid local minima.
upshot ACO-E would good candidate algorithm data sampled
networks large number v-structures. However, real world generating network
exist. Therefore, experiments Condition 5 designed try estimate quantity.
results experiments shown Table 18. seen association
results number samples 10000 average number v-structures
per node. Indeed, value correlation coefficient values r = 0.94,

434

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

indicates linear relationship p-value 0.006. However, results quite
number samples decrease. example, 100 samples correlation visible.
makes sense, low number samples would able support many v-structures.
such, estimate might valid large sample limit.
However, procedure employed Condition 5 indicate way seeing effective
ACO-E algorithm would arbitrary set data. value calculated resampling
method low (towards 0), ACO-E would probably particularly effective simpler
algorithm would perform well. However, value rises, probable number v-structures also
rises hence ACO-E (and methods designed avoid local maxima) would fare better.
ideas seem borne examining Table 14. appears high expected values
v-structures per node imply good performance ACO-E algorithm, i.e. obvious large
improvement SHD. contrary, low expected values v-structures per node associated
small improvements SHD algorithm progresses.
8.3 Metaheuristic Algorithm Comparison
Figures 9 10 Table 12 show results comparing ACO-E metaheuristic
algorithms. seen ACO-E performs better algorithms shown except
case HailFinder network, GREEDY-E gives better result SHD. However,
case, ACO-E gives better score value. problem discussed Section 8.1,
gave better score worse structure.
statements backed looking Table 20 gives p-values two-tailed
unpaired Mann-Whitney-U test comparing ACO-E results algorithms runs
ended. statistic, smaller number, significant test. Since results
runs comes separate sample network, correct tests would unpaired.
data used tests metaheuristic algorithm comparison, i.e.
combinations parameters ACO-E ACO-B. seems cases, results
highly significant, supports assumption ACO-E performs well. cases
significance high (ACO-E score compared GREEDY-E score Alarm network
ACO-E score compared GREEDY-E score Diabetes network), noted tiny
changes score value lead large structural changes algorithm converges towards
optimum (generating) network. cases, SHD p-values show highly significant difference.
Comparisons also made variances results seen Table 21,
gives p-values Conovers (1999) Squared Ranks one-tailed test. table seen
ACO-E generally lower standard deviation results finishing run compared
ACO-B EPQ. Whilst standard deviation results compared GREEDY-E significantly
lower respect Alarm Barley networks, cases GREEDY-E seems
consistent regard final results.
noted non-parametric tests used, results general non-normal
distributions. also noted results tables might seem incorrect.
E.g. Table 20, Win95pts-Score row, test ACO-B significant
GREEDY-E, even though mean GREEDY-E ACO-E ACO-B
Table 12. larger sample size ACO-B test, 1296 samples,
compared 216 samples GREEDY-E.

435

fiDALY & HEN



q0



Alarm

SHD
Score

8.0 104
1.7 101

4.3 1091
2.1 102

1.3 1016
9.1 103

Barley

SHD
Score

1.3 106
1.3 109

4.2 10230
2.9 1072

6.9 1041
2.5 1026

Diabetes

SHD
Score

1.0 100
9.2 103

3.6 1042
2.8 101

8.7 101
1.0 100

HailFinder

SHD
Score

9.3 101
1.9 101

3.3 102
1.1 102

5.5 103
2.7 102

Mildew

SHD
Score

5.1 1016
1.4 105

3.6 10125
8.0 1063

1.0 100
3.5 101

Win95pts

SHD
Score

4.9 108
1.3 107

9.0 1041
2.0 1026

5.3 1044
2.2 1018

Table 19: Comparisons parameter behavior

GREEDY-E

ACO-B

EPQ

Alarm

SHD
Score

1.1 10113
4.3 102

1.6 1031
6.0 103

1.9 10118
1.8 1018

Barley

SHD
Score

9.6 10124
5.0 10123

1.8 1092
5.9 1096

9.3 10123
7.0 10123

Diabetes

SHD
Score

3.1 1034
2.5 101

4.7 10104
2.3 1018

1.5 1088
4.5 1069

HailFinder

SHD
Score

3.8 1012
4.4 103

2.7 10237
2.2 1093

1.8 1099
4.6 10113

Mildew

SHD
Score

2.3 1050
7.4 1062

4.7 10160
5.7 10114

7.5 10115
5.3 10118

Win95pts

SHD
Score

1.5 1042
3.0 1037

0
4.1 1053

4.8 10123
5.0 10123

Table 20: p-values Mann-Whitney U test, 10,000 samples

436

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

results Condition 4 confirm findings. experiments, algorithms
run parameters tuned. results experiments seen Table 17.
findings results similar ones discussed above, differences. ACO-E
outperforms algorithms SHD test score measures cases except
HailFinder network, GREEDY-E better, above. differences ACO-E
ACO-B, main competitor, pronounced, still exist. Alarm, Barley
Mildew networks practical difference quite small, whereas Diabetes, HailFinder
Win95pts networks still quite large. However, even this, ACO-E said perform
better three areas:
ACO-E robust respect parameter values input. Comparing tunedparameter-value results results across parameter values, seen ACO-E
sensitive values ACO-B. implies ACO-E could used learning
problem without long parameter optimization stage. Note reasonable parameter values
still important, discussed Section 8.1.
ACO-E converges faster optimum values ACO-B, terms number iterations.
example, Barley, Mildew Win95pts cases, ACO-E reaches best SHD value
20 iterations, whereas ACO-B takes 200 iterations.
ACO-E generally provides smaller variance output values algorithms.
important situations robust output needed.
8.4 State-of-the-Art Algorithm Comparison
section, comparison ACO-E state-of-the-art Bayesian network structure
learning algorithms analyzed. shown Table 13, ACO-E appears good performance algorithms. results statistical comparisons ACO-E
algorithms shown Table 22.
table shown p-values individual comparisons ACO-E algorithms.
test used comparisons Mann-Whitney U test. test used,
distributions found normal. foot table combined p-value found
individual p-values it. total p-value comparing ACO-E
algorithms. method combining values
n

pcombined = 1 1 pi ,
i=1

pi p-value entry table, n values total. method combining
p-values needed chance causing Type error otherwise. Type error
false positive result, i.e. null hypothesis rejected be. occur
case experiment small chance failing repeated enough times,
large chance least one fail. noted value foot Alarm
combine p-values it. Instead leaves SC k = 10, PC OR2
k = 5. median results figures close ACO-E would
pushed p-value high. Therefore, overall test valid tests
include three mentioned.
437

fiDALY & HEN

Alarm

Barley

Diabetes

HailFinder

Mildew

Win95pts

SHD
ESHD
Score
SHD
ESHD
Score
SHD
ESHD
Score
SHD
ESHD
Score
SHD
ESHD
Score
SHD
ESHD
Score

GREEDY-E
3.6 1084
4.1 10103
8.0 102
1.3 1061
6.2 1044
1.5 1066
1
1
4.8 101
1
1
9.1 101
1
1
1
1
1
5.8 101

ACO-B
2.4 10252
0
1.3 101
1.5 10274
7.6 10277
0
1.5 103
1.2 1013
2.1 104
2.0 10153
2.5 10169
4.3 1023
1.5 10111
1.3 1077
1.5 1085
5.7 10209
8.7 10210
2.6 1026

EPQ
5.3 10104
7.3 10117
5.0 105
2.3 1066
1.6 1069
4.6 10146
1.0 104
3.1 105
7.3 108
2.6 1062
2.0 1058
1.3 10143
9.3 1054
1.9 1049
8.3 1079
5.4 1012
1.4 1013
3.2 1038

Table 21: p-values Conovers squared ranks test, 10,000 samples

Alarm

Barley

HailFinder

Mildew

MMHC
OR1 k = 5
OR1 k = 10
OR1 k = 20
OR2 k = 5
OR2 k = 10
OR2 k = 20
SC k = 5
SC k = 10
GS
PC
TPDA
GES

3.2 102
5.9 104
1.9 105
4.1 108
3.5 102
1.4 107
2.1 108
2.1 108
8.1 101
2.1 108
4.0 101
6.5 104
N/A

2.1 108
2.1 108
2.1 108
2.1 108
2.1 108
1.1 105
3.8 106
2.1 108
N/A
4.3 107
2.1 108
2.1 108
2.1 108

2.1 108
2.1 108
2.1 108
2.1 108
2.1 108
2.1 108
2.1 108
2.1 108
N/A
2.1 108
2.1 108
2.1 108
2.1 108

2.1 108
2.1 108
2.1 108
2.1 108
2.1 108
2.1 108
2.1 108
N/A
N/A
2.1 108
2.1 108
2.1 108
2.1 106

Total

3.3 102

1.6 105

2.5 107

2.3 106

Table 22: p-values comparing ACO-E state-of-the-art algorithms

438

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

results given Table 13 appear indicative results given Section 8.1.
discussed there, ACO-E effect learning Alarm network, especially
straight greedy search. However, effectiveness appeared come randomness
search, make much use parameters.
networks ACO-E performed well, Barley Mildew, performance reflected
across current results also performed well here. results HailFinder network
seem odd, ACO-E algorithm better search using GREEDY-E. However,
figure, performance GES also seen well. GES works space
equivalence classes Bayesian networks, postulated ACO-E performs well
structure search space.
ever, comparisons must taken tentatively, especially case, results given
Tsamardinos, Brown et al. five samples.
8.5 Computational Complexity ACO-E
results experimental Condition 3 show two figures (11 12) related computational
complexity ACO-E. first shows total number score function evaluations
algorithm run, second shows number distinct score function evaluations.
counted, score function evaluations usually cached order improve running times.
surmized general, larger sample sizes imply evaluations. makes sense,
larger samples support networks arcs. Since algorithm starts empty graph,
would take moves hence evaluations get maximum. also seen
total number evaluations general, linear respect number iterations passed.
Looking plots Figure 12, seen number distinct function evaluations
many magnitudes less total number evaluations. also noted
distinct function evaluations take place within first twenty thirty iterations gradually tails
logarithmic fashion. expected, beginning, algorithm explore
many new paths pheromone evenly distributed. scores paths
cached computed again. means time, less less new score
function applications needed. However, worthwhile noticing many cases plots
level out. implies new paths taken algorithm stagnating.
finish up, worthwhile comparing complexity ACO-E metaheuristic algorithms tested. Looking Table 17, appears ACO-E much higher computational
complexity algorithms. However, seen evaluations
distinct. Since evaluations normally cached cache lookup proceed constant time,
total score evaluation results important. Focusing instead distinct score evaluation
results, seen much difference ACO-E algorithms
terms actual score function evaluations. Since often dominant factor algorithm
running time, complexity algorithms observed quite similar.

9. Conclusions Future Directions
main results paper development ACO-E algorithm implementation
ACO metaheuristic problem learning Bayesian network structure provides
good fit set data. nutshell, ACO-E performed well reconstructing test networks,

439

fiDALY & HEN

data sampled. detailed look behavior ACO-E depending parameters,
type test network compared algorithms given.
9.1 ACO-E Behavior Parameters Varied
analyzing behavior ACO-E function parameters, best worst performing
figures compared, across range parameter. best result found parameter
setting produced either highest score smallest difference test network. worst
result found parameter switched off, i.e. effect algorithms
behavior.
parameters, difference behavior best worst settings.
Whether difference significant depended particular network used
test; networks responded better algorithm others. networks ACO-E
worked well with, following trends noticed:
data features, lower values , higher values q0 higher values
worked better;
data less features, higher values , lower values q0 lower values worked
better;
rate pheromone deposition/evaporation, q0 balance exploration
exploitation power heuristic probabilistic transition rule.
9.2 Utility ACO-E Function Test Network Sample
noticed ACO-E performed better test networks others. networks
fared best Barley, Mildew Win95pts, described Section 6.1.1. closer
examination networks found large average v-structure (as discussed
Section 6.1.1) per node value.
reason might make difference nodes large number v-structures
imply possible local maxima search space. Greedy methods would run maxima,
whereas ACO-E able find way around stochastic nature always
choosing best move. Experiments run estimate average v-structure per node value
correspondence found large sample case. general, method used experiment
could used estimate usefulness ACO-E particular situations.
9.3 ACO-E Performance Compared Similar Algorithms
results Sections 7.1 7.4 show ACO-E performs well algorithms
similar nature. algorithms were:
GREEDY-E, performs greedy search space equivalence classes Bayesian
network structures (Chickering, 2002a);
EPQ, performs evolutionary programming search space equivalence classes
(Cotta & Muruzbal, 2004; Muruzbal & Cotta, 2004);
ACO-B, performs search using ACO space DAGs (de Campos, FernndezLuna et al., 2002).
440

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

cases, BDeu score ACO-E better score algorithms, every
iteration. case structural differences, better cases, except HailFinder
network, odd behavior scoring function meant better BDeu scores implied worse
structural differences. Concurring discussion 9.2, networks ACO-E
performed best Barley, Mildew Win95pts networks.
ACO-E also shown comparable computational complexity metaheuristic
algorithms.
9.4 ACO-E Performance Compared Alternative State-of-the-Art Algorithms
Similar section above, ACO-E performed well comparison state-of-the-art Bayesian
network structure learning algorithms, performing better 3 4 tested: Barley, Mildew
HailFinder. first two networks performed well self test. HailFinder
network postulated results good search space; good results also
shown greedy equivalent search (GES) algorithm, also searches space
equivalence classes.
Whilst ACO-E perform best Alarm network, perform badly either,
coming joint third rankings. reasons performance Alarm network
discussed Section 8.2.
9.5 Extending ACO-E Increase Performance Scalability
Since validity checking slowest part ACO-E algorithm, currently remains first issue
must dealt with, order improve running times. However, problem solved
focus turn back parts algorithm, particularly scoring function.
Reducing Number Scoring Function Evaluations One easy way cutting
number score evaluations would static heuristic defined could say, e.g.
would benefit adding arc empty graph. way, scoring functions would
evaluated per move hence lead speeding algorithm. situation
like this, local search would become important order finish traversals best
possible positions.
Pruning Search Space Recently, hybrid learning algorithms shown good success
learning Bayesian network structures, whilst cutting running time, sometimes dramatically.
generally work using conditional independence test discover nodes would likely
connected given node remove rest consideration. effect
requiring less scoring function evaluations, thus speeding algorithm requiring less memory
store results evaluations. bound number possible parents, number
cached values would grow least quadratically number variables eventually exhaust
computers memory.
Applying ACO-E Different Search Operators Better Avoid Local Maxima According
Castelo Kocka (2003), certain operators able avoid local maxima
search space, provided sample size tends infinity. example operators
given Chickering (2002b) used greedy search space equivalence classes
structures (GES).

441

fiDALY & HEN

However, small sample sizes guarantees strictly true search algorithms
still get caught maxima. example method tries avoid KES algorithm
Nielsen, Kocka, Pea (2003), uses operators GES, parameter controls
often algorithm acts greedily; algorithm act greedily, chooses move
necessarily best. Experiments show KES behaves better GES
time.
procedure bears similarities ACO-E. randomness augmented pheromone
heuristics, possibility performance would improve even more.

Acknowledgments
authors grateful Associate Editor reviewers comments,
helpful guiding revision research.

References
Abramson, B., Brown, J., Edwards, W., Murphy, A., & Winkler, R. L. (1996). Hailfinder:
Bayesian system forecasting severe weather. International Journal Forecasting, 12(1),
5771. doi:10.1016/0169-2070(95)00664-8.
Acid, S., & de Campos, L. M. (2003). Searching Bayesian network structures space
restricted acyclic partially directed graphs. Journal Artificial Intelligence Research, 18, 445490.
Aitken, S., Jirapech-Umpai, T., & Daly, R. (2005). Inferring gene regulatory networks classified
microarray data: Initial results. BMC Bioinformatics, 6(Suppl 3), S4. doi:10.1186/1471-2105-6S3-S4.
Akaike, H. (1974). new look statistical model identification. IEEE Transactions Automatic
Control, 19(6), 716723. doi:10.1109/TAC.1974.1100705.
Andersson, S. A., Madigan, D., & Perlman, M. D. (1997). characterization Markov
equivalence classes acyclic digraphs.
Annals Statistics, 25(2), 505541.
doi:10.1214/aos/1031833662.
Andreassen, S., Hovorka, R., Benn, J., Olesen, K. G., & Carson, E. (1991). model-based approach
insulin adjustment. Proceedings Third Conference Artificial Intelligence Medicine
(AIME 91), volume 44 Lecture Notes Medical Informatics, (pages 239249).
Baluja, S. (1994). Population-based incremental learning: method integrating genetic search
based function optimization competitive learning. Technical Report CMU-CS-94-163, School
Computer Science, Carnegie Mellon University.
Beinlich, I., Suermondt, H., Chavez, R., & Cooper, G. (1989). ALARM monitoring system:
case study two probabilistic inference techniques belief networks. Proceedings
Second European Conference Artificial Intelligence Medicine (AIME 89), volume 38
Lecture Notes Medical Informatics, (pages 247256). Springer.
Birattari, M., Caro, G. D., & Dorigo, M. (2002). Toward formal foundation ant programming.
Proceedings Third International Workshop Ant Algorithms, volume 2463 Lecture
442

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Notes Computer Science, (pages 188201). Springer-Verlag.
Bonabeau, E., Dorigo, M., & Theraulaz, G. (1999). Swarm Intelligence: Natural Artificial
Systems. Studies Sciences Complexity. Oxford University Press.
Bullnheimer, B., Hartl, R. F., & Strauss, C. (1999). improved ant system algorithm vehicle
routing problem. Annals Operations Research, 89, 319328. doi:10.1023/A:1018940026670.
Buntine, W. (1991). Theory refinement Bayesian networks. B. DAmbrosio, & P. Smets (Eds.),
Proceedings Seventh Annual Conference Uncertainty Artificial Intelligence (UAI 91),
(pages 5260). Morgan Kaufmann.
Castelo, R., & Kocka, T. (2003). inclusion-driven learning Bayesian networks. Journal
Machine Learning Research, 4, 527574.
Cheng, J., Greiner, R., Kelly, J., Bell, D., & Liu, W. (2002). Learning Bayesian networks data:
information-theory based approach. Artificial Intelligence, 137(12), 4390. doi:10.1016/S00043702(02)00191-1.
Chickering, D. M. (1995). transformational characterization equivalent Bayesian network structures. P. Besnard, & S. Hanks (Eds.), Proceedings Eleventh Conference Uncertainty
Artificial Intelligence (UAI-95), (pages 8798). Morgan Kaufmann.
Chickering, D. M. (1996a). Learning Bayesian networks NP-complete. D. Fisher, & H.-J. Lenz
(Eds.), Learning Data: Artificial Intelligence Statistics V, volume 112 Lecture Notes
Statistics, (pages 121130). Springer.
Chickering, D. M. (1996b). Learning equivalence classes Bayesian network structures.
E. Horvitz, & F. Jensen (Eds.), Proceedings Twelfth Conference Uncertainty Artificial
Intelligence (UAI-96), (pages 150157). Morgan Kaufmann.
Chickering, D. M. (2002a). Learning equivalence classes Bayesian-network structures. Journal
Machine Learning Research, 2, 445 498.
Chickering, D. M. (2002b). Optimal structure identification greedy search. Journal Machine
Learning Research, 3, 507554.
Chickering, D. M., Geiger, D., & Heckerman, D. (1996). Learning Bayesian networks: Search
methods experimental results. D. Fisher, & H.-J. Lenz (Eds.), Learning Data: Artificial
Intelligence Statistics V, volume 112 Lecture Notes Statistics, (pages 112128). Springer.
Chickering, D. M., Heckerman, D., & Meek, C. (2004). Large-sample learning Bayesian networks
NP-hard. Journal Machine Learning Research, 5, 12871330.
Conover, W. J. (1999). Practical Nonparametric Statistics. John Wiley & Sons, Third edition.
Cooper, G. F., & Herskovits, E. (1992). Bayesian method induction probabilistic networks
data. Machine Learning, 9(4), 309347. doi:10.1007/BF00994110.
Costa, D., & Hertz, A. (1997). Ants colour graphs. Journal Operational Research Society,
48(3), 295305. doi:10.1057/palgrave.jors.2600357.

443

fiDALY & HEN

Cotta, C., & Muruzbal, J. (2004). learning Bayesian network graph structures via
evolutionary programming. P. Lucas (Ed.), Proceedings Second European Workshop
Probabilistic Graphical Models, (pages 6572).
Cowell, R. (2001). Conditions conditional independence scoring methods lead
identical selection Bayesian network models. J. Breese, & D. Koller (Eds.), Proceedings
Seventeenth Conference Uncertainty Artificial Intelligence (UAI-01), (pages 9197).
Morgan Kaufmann.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference Bayesian belief networks
NP-hard. Artificial Intelligence, 60(1), 141154. doi:10.1016/0004-3702(93)90036-B.
Daly, R., Shen, Q., & Aitken, S. (2006). Speeding learning equivalence classes Bayesian
network structures. A. P. del Pobil (Ed.), Proceedings Tenth IASTED International
Conference Artificial Intelligence Soft Computing, (pages 3439). ACTA Press.
Daly, R., Shen, Q., & Aitken, S. (2009). Learning Bayesian networks: Approaches issues.
Knowledge Engineering Review. press.
de Campos, L. M., & Castellano, J. G. (2007). Bayesian network learning algorithms using structural restrictions. International Journal Approximate Reasoning, 45(2), 233254.
doi:10.1016/j.ijar.2006.06.009.
de Campos, L. M., Fernndez-Luna, J. M., Gmez, J. A., & Puerta, J. M. (2002). Ant colony
optimization learning Bayesian networks. International Journal Approximate Reasoning,
31(3), 291311. doi:10.1016/S0888-613X(02)00091-9.
de Campos, L. M., Gmez, J. A., & Puerta, J. M. (2002). Learning Bayesian networks ant colony
optimisation: Searching two different spaces. Mathware & Soft Computing, 9(23).
de Campos, L. M., & Puerta, J. M. (2001a). Stochastic local algorithms learning belief networks:
Searching space orderings. S. Benferhat, & P. Besnard (Eds.), Proceedings
Sixth European Conference Symbolic Quantitative Approaches Reasoning
Uncertainty (ECSQARU 2001), volume 2143 Lecture Notes Artificial Intelligence, (pages
228239). Springer. doi:10.1007/3-540-44652-4_21.
de Campos, L. M., & Puerta, J. M. (2001b). Stochastic local distributed search algorithms
learning belief networks. Proceedings Third International Symposium Adaptive Systems: Evolutionary Computation Probabilistic Graphical Models, (pages 109115).
ICIMAF.
Deneubourg, J. L., Aron, S., Goss, S., & Pasteels, J. M. (1990). self-organizing exploratory
pattern argentine ant. Journal Insect Behavior, 3(2), 159168. doi:10.1007/BF01417909.
Dor, D., & Tarsi, M. (1992). simple algorithm construct consistent extension partially
oriented graph. Technical Report R-185, Cognitive Systems Laboratory, Department Computer
Science, UCLA.
Dorigo, M. (1992). Ottimizzazione, apprendimento automatico, ed algoritmi basati su metafora
naturale. Ph.D. thesis, Politecnico di Milano, Italy.

444

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Dorigo, M., & Di Caro, G. (1999). ant colony optimization meta-heuristic. D. Corne,
M. Dorigo, & F. Glover (Eds.), New Ideas Optimization, (pages 1132). McGraw-Hill.
Dorigo, M., & Gambardella, L. M. (1997). Ant colony system: cooperative learning approach
traveling salesman problem. IEEE Transactions Evolutionary Computation, 1(1), 5366.
doi:10.1109/4235.585892.
Dorigo, M., Maniezzo, V., & Colorni, A. (1996). Ant System: Optimization colony
cooperating agents. IEEE Transactions Systems, Man, Cybernetics Part B: Cybernetics,
26(1), 2941. doi:10.1109/3477.484436.
Dorigo, M., & Sttzle, T. (2004). Ant Colony Optimization. MIT Press.
Friedman, N., Nachman, I., & Peer, D. (1999). Learning Bayesian network structure massive
datasets: Sparse Candidate algorithm. H. Prade, & K. Laskey (Eds.), Proceedings
Fifteenth Conference Uncertainty Artificial Intelligence (UAI-99), (pages 206215). Morgan
Kaufmann.
Gambardella, L. M., & Dorgio, M. (2000). ant colony system hybridized new local
search sequential ordering problem. INFORMS Journal Computing, 12(3), 237255.
doi:10.1287/ijoc.12.3.237.12636.
Gambardella, L. M., & Dorigo, M. (1995). Ant-Q: reinforcement learning approach travelling
salesman problem. A. Prieditis, & S. J. Russell (Eds.), Proceedings Twelfth International
Conference Machine Learning (ICML 1995), (pages 252260). Morgan Kaufmann.
Glover, F. (1989). Tabu searchPart I. ORSA Journal Computing, 1(3), 190206.
Glover, F. (1990). Tabu searchPart II. ORSA Journal Computing, 2(1), 432.
Heckerman, D. (1995). tutorial learning Bayesian networks. Technical Report MSR-TR95-06, Microsoft Research.
Heckerman, D., Geiger, D., & Chickering, D. M. (1995).
Learning Bayesian networks:
combination knowledge statistical data. Machine Learning, 20(3), 197243.
doi:10.1023/A:1022623210503.
Heckerman, D., Mamdani, A., & Wellman, M. P. (1995). Real-world applications Bayesian
networks. Communications ACM, 38(3), 2426. doi:10.1145/203330.203334.
Heckerman, D., Meek, C., & Cooper, G. (1999). Bayesian approach causal discovery.
C. Glymour, & G. F. Cooper (Eds.), Computation, Causation, & Discovery, (pages 141165).
AAAI Press.
Jensen, A. L. (1995). probabilistic model based decision support system mildew management
winter wheat. Ph.D. thesis, Dina Foulum, Research Center Foulum, Aalborg University.
Kayaalp, M., & Cooper, G. F. (2002). Bayesian network scoring metric based globally
uniform parameter priors. A. Darwiche, & N. Friedman (Eds.), Proceedings Eighteenth Conference Uncertainty Artificial Intelligence (UAI-02), (pages 251258). Morgan
Kaufmann.

445

fiDALY & HEN

Kirkpatrick, S., Gelatt, C. D., Jr., & Vecchi, M. P. (1983). Optimization simulated annealing.
Science, 220(4598), 671680. doi:10.1126/science.220.4598.671.
Kristensen, K., & Rasmussen, I. A. (2002). use Bayesian network design decision
support system growing malting barley without use pesticides. Computers Electronics
Agriculture, 33(3), 197217. doi:10.1016/S0168-1699(02)00007-8.
Kullback, S., & Leibler, R. A. (1951). information sufficiency. Annals Mathematical
Statistics, 22(1), 7986. doi:10.1214/aoms/1177729694.
Levine, J., & Ducatelle, F. (2004). Ant colony optimisation local search bin packing
cutting stock problems. Journal Operational Research Society, 55(7), 705716.
doi:10.1057/palgrave.jors.2601771.
Maniezzo, V., & Colorni, A. (1999). ant system applied quadratic assignment problem.
IEEE Transactions Knowledge Data Engineering, 11(5), 769778. doi:10.1109/69.806935.
Microsoft Research (1995). Win95pts. model printer troubleshooting Microsoft Windows
95.
Mitchell, M. (1996). Introduction Genetic Algorithms. MIT Press.
Moore, A., & Wong, W.-K. (2003). Optimal reinsertion: new search operator accelerated
accurate Bayesian network structure learning. T. Fawcett, & N. Mishra (Eds.), Proceedings
Twentieth International Conference Machine Learning, (pages 552559). AAAI Press.
Mhlenbein, H. (1997). equation response selection use prediction. Evolutionary
Computation, 5(3), 303346. doi:10.1162/evco.1997.5.3.303.
Munteanu, P., & Bendou, M. (2001). EQ framework learning equivalence classes Bayesian
networks. Proceedings 2001 IEEE International Conference Data Mining, (pages
417424). IEEE Computer Society. doi:10.1109/ICDM.2001.989547.
Munteanu, P., & Cau, D. (2000). Efficient score-based learning equivalence classes Bayesian
networks. D. A. Zighed, H. J. Komorowski, & J. M. Zytkow (Eds.), Proceedings Fourth
European Conference Principles Data Mining Knowledge Discovery (PKDD 2000),
volume 1910 Lecture Notes Computer Science, (pages 96105). Springer. doi:10.1007/3-54045372-5_10.
Muruzbal, J., & Cotta, C. (2004). primer evolution equivalence classes Bayesiannetwork structures. X. Yao, E. Burke, J. A. Lozano, J. Smith, J. J. Merelo-Guervs, J. A.
Bullinaria, J. Rowe, P. Tino, A. Kabn, & H.-P. Schwefel (Eds.), Proceedings 8th International Conference Parallel Problem Solving Nature - PPSN VIII, volume 3242 Lecture
Notes Computer Science, (pages 612621). Springer. doi:10.1007/b100601.
Nielsen, J. D., Kocka, T., & Pea, J. (2003). local optima learning Bayesian networks.
C. Meek, & U. Kjrulff (Eds.), Proceedings Ninteenth Conference Uncertainty
Artificial Intelligence, (pages 435444). Morgan Kaufmann.
Ott, S., Imoto, S., & Miyano, S. (2004). Finding optimal models small gene networks.
Proceedings Ninth Pacific Symposium Biocomputing, (pages 557567). World Scientific.
446

fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION

Ott, S., & Miyano, S. (2003). Finding optimal gene networks using biological constraints. Genome
Informatics, 14, 124133.
Rasmussen, L. K. (1995). Bayesian Network Blood Typing Parentage Verification Cattle.
Ph.D. thesis, Dina Foulum, Research Center Foulum.
Schwarz, G. (1978). Estimating dimension model. Annals Statistics, 6(2), 461464.
doi:10.1214/aos/1176344136.
Shaughnessy, P., & Livingston, G. (2005). Evaluating causal explanatory value Bayesian
network structure learning algorithms. Research Paper 2005-013, Department Computer Science,
University Massachusetts Lowell.
Shimony, S. E. (1994). Finding maps belief networks NP-hard. Artificial Intelligence, 68(2),
399410. doi:10.1016/0004-3702(94)90072-8.
Silander, T., Kontkanen, P., & Myllymaki, P. (2007). sensitivity MAP Bayesian network
structure equivalent sample size parameter. Proceedings Twenty-Third Conference
Uncertainty Artificial Intelligence (UAI-07).
Spirtes, P., & Glymour, C. (1990). algorithm fast recovery sparse causal graphs. Report
CMU-PHIL-15, Department Philosophy, Carnegie Mellon University.
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, Prediction, Search. Adaptive
Computation Machine Learning. MIT Press, 2nd edition.
Spirtes, P., Meek, C., & Richardson, T. (1995). Causal inference presence latent variables
selection bias. P. Besnard, & S. Hanks (Eds.), Proceedings Eleventh Conference
Uncertainty Artificial Intelligence (UAI-95), (pages 499506). Morgan Kaufmann.
Steck, H., & Jaakkola, T. S. (2003). Dirichlet prior Bayesian regularization.
S. Becker, S. Thrun, & K. Obermayer (Eds.), Advances Neural Information Processing Systems
15 (NIPS*2002), (pages 697704). MIT Press.
Sttzle, T. (1998). ant approach flow shop problem. Proceedings Sixth European
Congress Intelligent Techniques Soft Computing (EUFIT 98), volume 3, (pages 1560
1564). Aachen, Germany: ELITE Foundation.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press.
Tsamardinos, I., Brown, L. E., & Aliferis, C. F. (2006). max-min hill-climbing Bayesian network
structure learning algorithm. Machine Learning, 65(1), 3178. doi:10.1007/s10994-006-6889-7.
van der Putten, P., & van Someren, M. (2004). bias-variance analysis real world
learning problem: CoIL challenge 2000.
Machine Learning, 57(1-2), 177195.
doi:10.1023/B:MACH.0000035476.95130.99.
Verma, T., & Pearl, J. (1991). Equivalence synthesis causal models. P. Bonissone,
M. Henrion, L. Kanal, & J. Lemmer (Eds.), Uncertainty Artificial Intelligence 6, (pages 255
268). North-Holland.

447

fiJournal Artificial Intelligence Research 35 (2009) 119-159

Submitted 11/08; published 06/09

Trust-Based Mechanisms Robust Efficient Task Allocation
Presence Execution Uncertainty
Sarvapali D. Ramchurn

SDR @ ECS . SOTON . AC . UK

Intelligence, Agents, Multimedia
School Electronics Computer Science
University Southampton, Southampton, UK

Claudio Mezzetti

C . MEZZETTI @ WARWICK . AC . UK

Department Economics
University Warwick, Coventry, UK

Andrea Giovannucci

AGIOVANNUCCI @ IUA . UPF. EDU

SPECS Laboratory
Pompeu Fabra University
Barcelona, Spain

Juan A. Rodriguez-Aguilar

JAR @ IIIA . CSIC . ES

Artificial Intelligence Research Institute
Spanish Council Scientific Research
Barcelona, Spain

Rajdeep K. Dash
Nicholas R. Jennings

RKD @ ECS . SOTON . AC . UK
NRJ @ ECS . SOTON . AC . UK

Intelligence, Agents, Multimedia
School Electronics Computer Science
University Southampton, Southampton, UK

Abstract
Vickrey-Clarke-Groves (VCG) mechanisms often used allocate tasks selfish rational
agents. VCG mechanisms incentive compatible, direct mechanisms efficient (i.e., maximise social utility) individually rational (i.e., agents prefer join rather opt out). However, important assumption mechanisms agents always successfully complete allocated tasks. Clearly, assumption unrealistic many real-world applications,
agents can, often do, fail endeavours. Moreover, whether agent deemed
failed may perceived differently different agents. subjective perceptions
agents probability succeeding given task often captured reasoned using
notion trust. Given background, paper investigate design novel mechanisms
take account trust agents allocating tasks.
Specifically, develop new class mechanisms, called trust-based mechanisms,
take account multiple subjective measures probability agent succeeding given
task produce allocations maximise social utility, whilst ensuring agent obtains
negative utility. show mechanisms pose challenging new combinatorial
optimisation problem (that NP-complete), devise novel representation solving problem,
develop effective integer programming solution (that solve instances 2 105
possible allocations 40 seconds).
c
2009
AI Access Foundation. rights reserved.

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

1. Introduction
Task allocation important challenging problem within field multi-agent systems.
problem involves deciding assign number tasks set agents according
allocation protocol. example, number computational jobs may need allocated agents
run high performance computing data centres (Byde, 2006), number network maintenance
tasks may need performed communications companies number business clients
(Jennings, Faratin, Norman, OBrien, Odgers, & Alty, 2000), number transportation tasks
may need allocated number delivery companies (Sandholm, 1993). general case,
agents performing jobs asking jobs performed trying maximise
gains (e.g., companies owning data centres servers trying minimise
number servers utilised, communications companies try minimise number people
needed complete tasks demanded, transportation companies try use minimum
number vehicles). Given this, Mechanism Design (MD) techniques employed design
task allocation protocols since techniques produce solutions provable
desirable properties faced autonomous utility maximising actors (Dash, Parkes, &
Jennings, 2003). particular, Vickrey-Clarke-Groves (VCG) class mechanisms
advocated number problem domains (Walsh & Wellman, 1998; Hershberger & Suri, 2001;
Dash et al., 2003) maximise social welfare (i.e., efficient) guarantee nonnegative utility participating agents (i.e., individually rational). mechanisms,
agents typically reveal costs performing tasks valuation requested tasks
centre centre computes allocation tasks agent payments
need make receive. However, important underpinning assumption mechanisms
make agent always successfully completes every task assigned centre.
result assumption allocation (i.e., assignment tasks asked
requester agents executed task performer agents) selected centre based
costs valuations provided agents. ensures centre always chooses
performers cheapest requesters ready pay most. However,
agents chosen centre may ultimately successful completing assignment.
example, agent providing access data centre cost 10, success rate
100%, might preferable one providing service cheaper cost 5
10% chance successful. Thus, order make efficient allocations circumstances,
need design mechanisms consider task performers costs service
probability success (POS). Now, probability may perceived differently different agents
typically different standards means evaluating performance
counterparts. example, different customers might evaluate performance data centre
different ways timeliness, security, quality output. Given this, turn notion
trust capture subjective perceptions (Ramchurn, Huynh, & Jennings, 2004). take
account agents trust agents, well costs, allocating tasks requires
design new class mechanisms previously termed trust-based (Dash, Ramchurn,
& Jennings, 2004).
date, however, existing work trust-based mechanisms (TBMs) ignores number important aspects task allocation problem makes less robust uncertainty (see Section
2 details). First, Porter, Ronen, Shoham, Tennenholtz (2008) allow POS reports
come task performer, rather agent. means task requester
2

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

misled task performers opinion (even truthfully revealed) since task requester
may believe, times, task performer failed task performer believes succeeded. Second, previous work (Dash et al., 2004), presented trust-based mechanism
could result inefficient allocations agents strong incentives over-report POS.
Even importantly, however, existing trust-based mechanisms completely ignore computational cost associated including POS computing optimal allocation payments.
Thus, previous work highlights economic benefits, specify new problem effectively represented efficiently solved. ignoring issues, previous work
failed prove mechanisms actually implemented, solved, whether
scale reasonable numbers agents.
background, paper provides economically efficient individually rational
mechanisms scenarios exists uncertainty agents successfully completing
assigned tasks. execution uncertainty generally modelled follows. First, potential task performers assessed task requester uses individual experience
performance information gathered environment (such reports agents
performance) construct estimation POS. Often sources called confidence reputation respectively (Ramchurn et al., 2004; Dasgupta, 1998), combined
give notion trust agent performing particular task. combined view trust
used robust measure POS single estimate (especially one
originating task performer). evident fact agent likely
partial view performance task performer derived finite subset
interactions. example, task requester ten tasks performed agent may benefit
experience acquired another requesters fifty interactions agent. However, incorporating trust decision mechanism requester introduces two major issues.
First, agents use reports agents build trust, introduces possibility interdependent valuations. means value generated one agent system
affected another agents report mechanism (Jehiel & Moldovanu, 2001; Mezzetti, 2004).
This, turn, makes much harder standard VCG-based techniques incentivise agents
reveal private information truthfully. Second, using trust find optimal allocation involves
significant computational cost show solving optimisation problem trust-based
mechanisms NP-complete.
tackle issue interdependence, build upon work Mezzetti (2004, 2007)
construct novel mechanism incentivises agents reveal private information. Moreover, help combat computational complexity generated trust, go develop novel
representation optimisation problem posed trust-based mechanisms provide implementation based Integer Programming (IP). Given this, show main bottleneck
mechanism lies searching large set possible allocations, demonstrate
IP solution comfortably solve small medium instances within minutes (e.g., 6 tasks
50 agents) hours (e.g., 8 tasks 70 agents).1 doing, provide first benchmark
algorithms aim solve optimisation problems.
detail, paper advances state art following ways:

1. Though time taken find optimal solution grows exponentially number tasks, mechanism sets
baseline performance solving optimisation problem posed trust-based mechanisms.

3

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

1. design novel TBMs allocate tasks uncertainty completion. TBMs non-trivial extensions paper Porter et al. (2008)
first consider reputation task performer within system, addition
self-report. allows us build greater robustness mechanism since takes
account subjective perceptions agents (task requesters particular) POS
task performers.
2. prove TBMs incentive compatible, efficient individually rational.
3. develop novel representation optimisation problem posed TBMs and, given
this, cast problem special matching problem (Berge, 1973). show solving
generalised version TBMs NP-complete provide first Integer Programming
solution it. solution solve instances 50 agents 6 tasks within one minute
even larger instances within hours.
rest paper structured follows. start providing overview related
work Section 2. provide contributions listed step-wise manner. First,
simple task allocation model detailed Section 3, introduce TBM single
requester, single task scenario. Section 4 develops generalised TBM multiple requesters
multiple tasks prove economic properties. dealt economic aspects,
turn computational problem implementing TBMs Section 5. Specifically,
develop new representation optimisation problem posed generalised TBM, study
computational costs associated solving problem, provide IP-based solution it.
Section 6 discusses number broader issues related development future trust-based
mechanisms.

2. Related Work
associating uncertainty mechanism design, build upon work areas. regards
capturing uncertainty multi-agent interactions, work focused devising computational
models trust reputation (see papers Teacy, Patel, Jennings, & Luck, 2006, Ramchurn
et al., 2004, reviews). models mostly use statistical methods estimate reliability
opponent agents reports direct interactions opponent.
models also try identify false inaccurate reports checking closely report matches
agents direct experience opponent (Teacy et al., 2006; Jurca & Faltings, 2006). Now,
models help choosing successful agents, shown generate
efficient outcomes given mechanism. contrast, paper provide means use
models order this.
case MD, surprisingly little work achieving efficient, incentive compatible individually rational mechanisms take account uncertainty general.
approaches adopted separated work reputation mechanisms mechanisms
task resource allocation. former mainly aim eliciting honest feedback reputation
providers. Examples mechanisms include papers Dellarocas (2002), Miller, Resnick,
Zeckhauser (2005), Jurca Faltings (2003, 2006). particular, Miller et al. (2005) recently developed peer prediction model, incentivises agents report truthfully
experience. mechanism operates rewarding reporters according well reports
4

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

coincide experience peers. Specifically, assigns scores distance
given agents report selected reference reporters reports given task performer.
similar way, Jurca Faltings (2007) also attempted solve problem placing
importance repeated presence agents system order induce truthful reporting. However, given focus eliciting honest feedback, mechanism silent
feedback actually used for. particular, cannot employed task allocation
scenario study paper case objective maximise overall utility
society that, therefore, considers value POS agents. example, car repair
lower value building bridge. Hence, feedback car repairer less critical
feeback bridge builder terms impact social welfare. Interestingly, mechanism shown truth-telling (non-unique) Nash equilibrium budget balanced,
individually rational (see Section 6 social desiderata interplay).
terms MD task allocation, type uncertainty taken account Bayesian mechanisms dAGVA (dAspremont & Gerard-Varet, 1979; Arrow, 1979). considers
case payoffs agents determined via probability distribution types
common knowledge agents. However, mechanism cannot deal problem
uncertainty task completion, agent information POS
agents, common knowledge type distributions. Porter et al. (2008)
also considered task allocation problem mechanism one closely
related ours. However, limit case agents report
POS. serious drawback assumes agents measure POS
accurately consider case agents may different perceptions
POS (e.g., performer believes performs better worse requester perceives).
Moreover, consider single requester setting, mechanisms develop
deal multiple tasks multiple requesters. Thus, mechanisms considered
two-way generalisation theirs. First, allow multiple reports uncertainty need
fused appropriately give precise POS perceived requester. Second, generalise
mechanism case multiple requesters agents provide combinatorial valuations
multiple tasks. earlier work problem (Dash et al., 2004), proposed preliminary TBM agents could followed risky, potentially profitable strategy,
over-reporting costs under-reporting valuations since payments made according whether succeed fail allocated task (which new mechanism).
contrast, work, payment scheme ensures strategy viable thus
mechanism robust. Moreover, previous work assumed trust functions monotonically increasing POS reports (similar Porter et al.) develop algorithms
needed actually solve optimisation problem posed TBM. paper, present
mechanism applies general trust functions also develop algorithms solve TBMs.
Finally, work case interdependent, multidimensional allocation schemes. interdependent payoffs, Jehiel Moldovanu (2001) shown impossible achieve efficiency
one-stage mechanism. Mezzetti (2004), however, shown possible achieve efficiency elegant two-stage mechanism reasonable assumptions. mechanism
achieves efficiency without needing two reporting stages because, setting consider, payments contingent whether tasks successful agents derive
direct payoff allocation task another agent agents assessments
completion probabilities. setting, exists specific function captures interde5

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

pendence exists among agents assessments others POS. function
is, case, agents trust model.

3. Single Requester, Single Task Allocation Mechanisms
section, first present basic VCG mechanism simple task allocation model (a
single task requested single agent) allocated task guaranteed completed
(i.e., agents POS equal 1). briefly describe Porter et al.s (2008) extension
considers task performers privately known objective probability finish
assigned task. Finally, consider case POS task performer function
privately known variables held task performer system. ensures
choice made task requester better informed (drawing data various sources)
POS task performers. show Porter et al.s mechanism would fail produce
efficient allocation settings go provide non-trivial extension model
cater this. doing, define new trust-based mechanism single requester,
single task scenario (as prelude generalised mechanism develop next
section). go prove economic properties simple TBM. Throughout
section, running example task allocation problem employed demonstrate workings
mechanisms discussed.
3.1 Allocation Guaranteed Task Completion
task allocation scenario, single agent derives value certain task performed.
end, agent needs allocate task one available task performers,
charge certain amount execute task. start considering following simple example:
Example 1. MoviePictures.com, computer graphics company, image rendering task
wishes complete new movie. Hence, MoviePictures.com publicly announces intention
companies owning data centres execute task. Given interest shown many
companies, MoviePictures.com needs decide mechanism allocate contract
much pay chosen contractor, given MoviePictures.com know
contractors costs execute job (i.e., know much actually costs company
process images render required quality).
example captured following model. set agents (data
centre agents example), = {1, 2, . . . , i, . . . , I}, privately-known cost
ci ( ) R+ {0} performing rendering task . Furthermore, let MoviePictures.com
represented special agent 0, value v0 ( ) R+ {0} rendering task
cost c0 ( ) > v0 ( ) perform task (c0 ( ) = case agent 0 cannot execute task).
Hence, MoviePictures.com get task performed another agent set
cost ci ( ) v0 ( ).
Now, MoviePictures.com needs decide procedure award contract, hence,
acts centre invite offers agents perform task. devising
mechanism task allocation, focus incentive-compatible direct revelation mechanisms
(DRMs) invoking revelation principle states mechanism transformed
DRM (Krishna, 2002). context, direct revelation means strategy space (i.e.,
possible actions) agents restricted reporting type (i.e., private information,
6

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

example cost valuation task) incentive-compatible means equilibrium strategy
(i.e., best strategy certain equilibrium concept) truth-telling.
Thus, DRM, designer control two parts: 1) allocation rule determines wins contract, 2) payment rule determines transfer money centre (i.e., MoviePictures.com) agents (i.e., data centres). Let K denote
particular allocation within space possible allocations K i0 represent agent
gets allocated task agent 0. Then, setting, space possible allocations
K = {, 10 , 20 , . . . , I0 } denotes case task allocated.
Moreover, abuse notation slightly define cost allocation K agent i,
ci (K) = ci ( ) K = i0 ci (K) = 0 otherwise. Similarly, centre, value
non empty allocation simply value task, i.e., v0 (K) = v0 ( ) K 6=
v0 (K) = 0 K = . Finally, let ri () R payment centre agent i. case ri ()
negative, agent pay |ri ()| centre.
Within context task allocation, direct mechanisms take form sealed-bid auctions
task performers report costs centre (or auctioneer). Agents may wish report
true costs reporting falsely leads preferable outcome them. therefore
distinguish actual costs reported ones superscripting latter b.
task allocation problem consists choosing allocation payment rules
certain desirable system objectives (some detailed below) satisfied. allocation
rule mapping reported costs set allocations, K(b
ci , b
ci ) allocation
chosen agent reports b
ci agents report vector b
ci . Similarly, payment rule
mapping reported costs payments agent, ri (b
ci , b
ci ) payment
agent agent reports b
ci agents report vector b
ci .
Following task execution payments, agent derives utility given utility function ui : K R R. common domain, assume agent rational (expected
utility maximiser) quasi-linear utility function (MasColell, Whinston, & Green, 1995):
Definition 1. quasi-linear utility function one expressed as:
ui (K, ri ) = ri ci (K)

(1)

K K given allocation.
modelled problem above, MoviePictures.com would like use protocol
possesses desirable properties efficiency individual rationality. also needs make sure
protocol incentive compatible: agents must find optimal report true costs.
desiderata formally defined follows:
Definition 2. Efficiency: allocation mechanism said achieve efficiency outcome
generates maximises total utility agents system (without considering transfers).
is, vectors reports b
c, calculates K that:
"
#
X
b
ci (K)
(2)
K (b
c) = arg max v0 (K)
KK

iI

Definition 3. Individual Rationality: allocation mechanism said achieve individual rationality agents derive higher utility participating mechanism opting it.
7

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

Assuming utility agent obtains opting zero, individually rational
allocation K one (Krishna, 2002):
ui (K, ri ) 0 ,

(3)

Definition 4. Incentive compatibility: allocation mechanism said achieve incentive compatibility agents true type optimal report matter agents report.
is:
ri (ci , b
ci ) ci (K(ci , b
ci )) ri (b
ci , b
ci ) ci (K(b
ci , b
ci )) ci , b
ci , b
ci .

Note incentive compatibility implies vector reports agents b
ci
payments agent must depend report chosen allocation. Incentive
compatibility requires telling truth (weakly) dominant strategy. also important
note incentive compatibility dominant strategies strongest possible form incentive
compatibility. VCG mechanism property.
MoviePictures.com decides employ Vickrey auction (also known second-price
sealed bid auction) since protocol possesses desired properties incentive compatibility,
efficiency, individual rationality (Krishna, 2002). detail, received
sealed bids (reports b
c) agents, centre calculates allocation K (b
c) according
Equation (2), transfer ri () winner given by:


X
v0 (K )
ri (b
c) = v0 (K (b
c)) max
b
cj (K )
(4)

K Ki

jI\i

Ki set allocations involve task performer.
3.2 Allocation Execution Uncertainty

mechanism presented previous section, assumed allocation K
decided, value v0 (K ) obtained centre (either v0 ( ) task allocated
0 otherwise). Thus, implicit assumption allocated task, agent
always perform successfully. However, unrealistic, illustrated following example:
Example 2. Many previous rendering tasks required MoviePictures.com allocated
PoorRender Ltd competitive prices. Unfortunately, PoorRender Ltd could
complete task many cases lack staff technical problems (which
knew even bidding task). result, MoviePictures.com incurred severe
losses. Hence, MoviePictures.com decides alter allocation mechanism way
agents POS completing tasks factored selection cheapest agent.
MoviePictures.com assumes contractor knows POS cost privately needs
mechanism elicit information truthfully order choose best allocation.
problem studied Porter et al. (2008) briefly describe, terms,
mechanism order extend generalise later (see Sections 3.3 4). first introduce
boolean indicator variable denote whether task completed ( = 1)
( = 0). Thus, observable task allocated. Moreover, extend
8

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

notation capture centres valuation task execution v0 () = v0 (K )
= 1 v0 () = 0 = 0. setting, assume commonly observed (i.e.,
agent believes = 1, agents {0} believe same). rendering
example, might denote whether images rendered appropriate resolution
allow usage not. Furthermore, probability = 1 task allocated
agent dependent upon another privately known variable, pi ( ) [0, 1], POS
agent executing task . Note variable privately known task performer itself,
single observation within system, carried task performer,
POS. Also note task performer incurs cost ci ( ) soon attempts task
irrespective whether successful not.
seen, value centre (MoviePictures.com) derive, v0 (), known
allocation calculated. Hence, notions efficiency individual rationality introduced section 3.1 need adjusted new setting. Given probability task
executed given agent, consider expected value allocation, v 0 (K, p),
calculated as:
v 0 (K, p) = v0 (K) pi ( )

(5)

agent chosen perform task allocation K p = hp1 ( ), . . . , pI ( )i
vector POS values agents (the list assessments contractor
probability complete rendering task example). need require
b vector reported POS values
agents report POS, addition cost. denote p
hb
p1 ( ), . . . , pbI ( )i.
following modified desiderata need considered now:

Definition 5. Efficiency: mechanism said achieve efficiency chooses allocation
maximises sum expected utilities (without considering transfers):
#
"
X
b)
b) = arg max v 0 (K, p
b
ci (K)
(6)
K (b
c, p
KK

iI

b reported agents key computing efficient
Note b
ci (K) p
allocation.
Definition 6. Individual Rationality: mechanism achieves individual rationality participating
agent derives expected utility, ui , always non-negative:
ui (c, p) = ri (c, p) ci (K) 0

ri (c, p) expected payment agent receives.
order achieve desiderata, one could suppose nave extension standard
Vickrey mechanism presented would sufficient. mechanism, centre would
ask agents report extended types (b
ci , pbi ( )). allocation chosen would one
maximising expected utility agents payment rule would conditioned according
Equation (4) v 0 (K , p) replacing v0 (K ). However, mechanism would fail
settings, illustrated next section.
9

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

3.2.1 NA IVE PPLICATION V ICKREY AUCTION
Example 3. Consider case MoviePictures.com derives value v0 ( ) = 300
rendering task completed let three contractors whose costs ci ( ) render
images given (c1 ( ), c2 ( ), c3 ( )) = (100, 150, 200). Furthermore, assume contractor
POS given (p1 ( ), p2 ( ), p3 ( )) = (0.5, 0.9, 1). information represented Table
1.
efficient allocation case (shaded line Table 1) involves assigning task agent
2 expected social utility 300 0.9 150 = 120. payment agent 2 using
(reverse) Vickrey auction expected values 300 0.9 (300 200) = 170 (from Equation
(4)). However, mechanism incentive-compatible. example, agent 1 reveals
pb1 ( ) = 1, centre implement K = 10 pay agent 1, r1 = 300120 = 180.
Thus, agents mechanism always better reporting pbi ( ) = 1, matter
actual POS is! Hence, centre able implement efficient allocation.
Agent
1
2
3

ci ( )
100
150
200

pi ( )
0.5
0.9
1

Table 1: Costs performing task agents perceived probability successfully completing
task.

type extension (i.e., including POS) non-trivial POS report agent
affects social value expected centre, agents cost allocation.
result, reporting higher POS positively affect agents probability winning
allocation thus positively affect utility. rectify this, need means
gain utility balanced penalty truthfully reporting type, agent
maximise utility. achieved Porter et al.s (2008) mechanism, briefly detail
next section.
3.2.2 P ORTER ET AL . ECHANISM
mechanism based around payments applied completion tasks. Specifically,
mechanism finds marginal contribution agent made expected welfare
agents depending whether completes assigned task not. Intuitively, works since
payment scheme punishes agent assigned task complete (i.e., = 0).
result, agent incentivised reveal higher POS value real POS since
allocated task, likely reap punishment rather reward obtains
successfully completes task (i.e., = 1).
detail, allocation determined centre according Equation (6). payment rule agent task allocated similar VCG
marginal contribution agent system extracted comparing efficient allocation
b, ) = 0 allowith second best allocation, excluding agent (the agent gets ri (b
c, p
cated task). difference expected marginal contribution extracted (i.e.,
10

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

taking account agents real probability success). achieved follows:



P
(b
)
, p

b
b
v
(K
c
,
p
))

max
b
c
(K
, = 1
v
(K
)


0
0
jI\i j

K Ki

b, ) =
ri (b
c, p



P


b)
b
cj (K )
, = 0
max v 0 (K , p


(7)

jI\i

K Ki

Ki set allocations excluding agent i.
mechanism would work example provided Table 1 since if, example, agent
1 reports pb1 ( ) = 1, allocated task paid 300 120 = 180
probability 0.5 120 probability 0.5. Thus, average, agent 1 paid 30
time incur cost 100, thereby making expected utility 70. Clearly, then,
rational agent overstate POS. fact, incentive compatibility mechanism
arises agent expected utility, given allocated task, is:


b) = pi ( ) v0 (K (b
b)) ci (K (b
b)) max
ui (b
c, p
c, p
c, p


K Ki



b)) max
+ (1 pi ( )) ci (K (b
c, p


K Ki



b), p) ci (K (b
b)) max
c, p
c, p
= v 0 (K (b


K Ki



b)
v 0 (K , p

b)
v 0 (K , p



X

jI\i

b)
v 0 (K , p

X

jI\i



b
cj (K )



b
cj (K )

X

jI\i

(8)


b
cj (K )

Note expected utility within mechanism would
derived agents nave extension VCG truthful reporting p. However,
Porter et al.s mechanism, agents incentive lie. because, pbi ( ) > pi ( )
(i.e., agent over-reports POS), agent might allocated task even though:
h

6= arg max v0 (K x )px ( ) cx (K x )
xI



Kx

=

x0 ,

means could that:

b), p) ci (K (b
b)) < max
v 0 (K (b
c, p
c, p


K Ki



X
b)
b
cj (K )
v 0 (K , p
jI\i

results agent deriving negative utility per Equation (8). Hence, agent
report higher POS values. complete treatment proof incentive-compatibility
mechanism given paper Porter et al. (2008). Furthermore, mechanism also
proven individually rational efficient.
3.3 Allocation Multiple Reports Execution Uncertainty
previous section, considered mechanism agent privately known
estimation uncertainty task completion. mechanism considers centre
11

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

receive single estimate agents POS. turn attention previously
unconsidered, general, case several agents may estimate. example,
number agents may interacted given data centre provisioning company many
occasions past therefore acquired partial view POS company. Using
estimates, centre obtain accurate picture given agents likely performance
combines different estimates together. combination results better estimate
number reasons, including:
1. Accuracy estimation: accuracy estimation typically affected noise. Thus,
combining number observations lead refined estimate obtaining
single point estimate.
2. Personal Preferences: agent within system may different opinions
constitutes success attempting task. result, centre may willing assign
weight agents estimate believes agents perspective similar
own.
illustrate points considering following example:
Example 4. MoviePictures.com still satisfied solution chosen far.
PoorRender Ltd still reports high POS, even though MoviePictures.com noticed
failed task number occasions. PoorRender Ltd believes
images rendered high enough quality used feature film MoviePictures.com believed not. MoviePictures.com therefore cannot rely agents
perception POS decide allocation. Rather, MoviePictures.com wants ask
agents submit perception others POS. doing, MoviePictures.com aims
capture knowledge agents might either previous sub-contracted
tasks simple observations. end, MoviePictures.com needs devise mechanism
capture agents perceptions (including own) measures POS agent use
fused measures selection process.
example modelled introducing new variable, Expected Quality
Service (EQOS), noted ij ( ), perception agent POS agent j
task . Now, vector agent EQOS agents (including itself) within system
noted = hi1 ( ), . . . , iI ( )i. Furthermore, shall denote j EQOS agents
within system (including itself) agent j. Thus, image rendering example, ij ( )
might denote probability perceived agent rendering task completed according
certain level quality computer graphics (which perceived differently different
agents). Then, MoviePictures.com needs function order combine EQOS agents
give resultant POS movie rendered graphic requirements.
detail, given previous personal interaction j, compute, based frequency good bad interactions, probability, termed confidence, j POS. Second,
also take account agents (i) opinions j, known js reputation society, order compute POS j (Ramchurn et al., 2004). combination measures
generally captured concept trust, defined aggregate expectation, derived
history direct interactions information sources, j complete
12

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

task assigned it. aggregate trust agent j successfully complete task agent 0
function tr0j : [0, 1]|I| [0, 1].
multiple ways trust function could computed, often captured
follows:
tr0j () =

X

wl lj

(9)

lI

P
wl [0, 1] wl = 1. function generates trust weighted sum EQOS values.
cases, actually considered probability distributions trust function
expected value joint distribution constructed individually reported distributions
(Teacy et al., 2006; Jurca & Faltings, 2007). Much work exists literature deals
different ways combining distributions biases incompatibilities agents
perceptions taken account. Essentially, however, assign weights different reports
agents choose expected value reports trust agent. However,
date, none models actually studies get self-interested agents generate reports
truthfully along maximising social welfare.
Now, direct mechanism case elicits agent i, cost EQOS vector,
{ci ( ), }, centre decides allocation payments agents. computing expected utility mechanism, agent must evaluate trust, probability success,
agent allocated task. raises conceptual difficulty. agent
treat agents POS reports assessing probability task completion (as opposed
computing best response type reports)? approach take paper
agent assumes reported POS agents truthful computing trust another
agent; precisely, agent computes value trust function using true EQOS
reported EQOS agents. Thus, trust agent agent j able comb ). already seen, general payment agent depends
plete task tr0j ( ,
b)
reported types agents whether task succeeds fails. end, let i(b
c,
b ). Then, define
agent allocated task vector reported types (b
c,
b )
expected payment agent true types (c, ) reported types (b
c,
follows:
h

b ,
b)
b ,
b)
i(c
i(c
b ; c, ) = ri (b
b , = 1)tr0
b ) + ri (b
b , = 0) 1 tr0
b )
Eri (b
c,
c,
( ,
c,
( ,
point type agent (EQOS plus cost) multidimensional and,
common multidimensional world, could several type reports generate
expected payment agent. ready define modified notion incentive compatibility use.2

Definition 7. Incentive compatibility (in Dominant Strategies): allocation mechanism said
achieve incentive compatibility dominant strategies agents true type optimal report
matter agents report. is: c, , b
ci , b
, b
ci , b
,
b ; c, ) ci (K(ci , b
bi,
b ; c, ) ci (K(b
Eri (ci , b
ci , ,
ci )) Eri (b
ci , b
ci ,
ci , b
ci ))

2. agent uses reported POS agents computing value trust function seems natural
assumption agent rely agents truthfully reporting types. case, example,
history interactions POS reporters publicly known (e.g., eBay Amazon).

13

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

Now, case agents view EQOS reports agents
truthful, trust agent agent j able complete task may depend
true reported types agents; case could relax incentive compatibility
requirement dominant strategy (ex-post) Nash equilibrium (MasColell et al., 1995),
means agents report truthfully, optimal agent always report
true type, matter true types agents are. replacing new trust function
definition expected payment agent i, definition incentive compatibility would
change to:
Definition 8. Incentive compatibility (in Nash Equilibrium): allocation mechanism said
achieve incentive compatibility (ex-post) Nash equilibrium agents true type optimal
report provided agents report type truthfully. is: ci , b
ci , , b
, ci , ,
b , ; c, ) ci (K(b
Eri (ci , ci , , ; c, ) ci (K(ci , ci )) Eri (b
ci , ci ,
ci , ci ))

next demonstrate Porter et al.s mechanism would work setting extending
example 1.
3.3.1 FAILURE P ORTER ET.

AL

ECHANISM

Example 5. Two agents costs performing task requested centre formed
perceptions set agents given Table 2. Suppose tr0i () = [1i ( ) + 2i ( )]/2,
v0 ( ) = 1.
Agent
1
2

tr0 ()

ci ( )
0
0

i1 ( )
0.6
0.8
0.7

i2 ( )
1
0.6
0.8

Table 2: Costs EQOS reports agents single task scenario. trust requester calculated
assuming truthful reports.

Porter et al. specify procedure deals EQOS reports. However, natural
) instead pbi ( ), ignore
extension technique would allocate according tr0i (b
reports agent computation payment. implement example.
Agent 2 winner since generates expected social utility 0.8, agent 1 would
generate utility 0.7. expected utility agent allocated task (according
Equation (8)):
b ) ci (K (b
b )) max
b ) = v0 (K (b
b )) tr0i ( ,
c,
ui (b
c,
c,


K Ki

h



) b
cj (K )
v0 (K ) tr0j (b

(10)
b excludes reports agent i, Ki set allocations excluding agent i,

j agent allocated task allocation K . Unfortunately, extension breaks
incentive compatibility following way. Given efficient allocation computed using
b values agents (using tr0 (b
b Equation (6)), value best
reported
) instead p
14

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

allocation obtained removing one agent could arbitrarily lower. example above, agent
1 reports 12 = 0, efficient allocation becomes agent 1 expected social utility 0.7
agent 1 gets expected utility 0.1 systems utility drops 0.6 reports
removed allocation recomputed. agent 1 truthful obtain 0 utility since agent 2
would winner case. effect, removal agent system breaks
mechanism interdependence valuations introduced trust model.
elaborate issue show solve next section.
thus need develop mechanism incentive-compatible agents reporting
perceptions agents POS. order so, however, need additionally
consider effect reporting EQOS vector agents expected utility. Specifically,
need develop trust-based mechanism EQOS reports agent provide
way increasing overall expected utility (as per intuition behind VCG). Then,
true value EQOS, mechanism result selection optimal allocation
tasks.
3.3.2 INGLE R EQUESTER INGLE TASK RUST-BASED ECHANISM
Intuitively, following mechanism works ascertaining agent derives positive utility
successfully completes task EQOS report change allocation
favour (thus, mechanism develop regarded generalisation paper Porter
et al., 2008).
detail, let i(K) agent performing task allocation K; centre first
determines allocation according to:
#
X
i(K)
b ) = arg max v0 (K) tr0 (b
K (b
c,
)
b
ci (K)
KK

"

(11)

iI

computed efficient allocation above, adopt similar approach Porter et
al.s compute payments tasks executed (see section 3.2.2). However,
novelty mechanism lies use agents EQOS reports computation
efficient allocation (as showed above). Moreover, additional payments losers
incentivise agents select efficient allocation.
Thus, apply different payments cases agent winning allocation succeeds
(i.e., = 1) fails (i.e., = 0). agent allocated task (i.e., K = { i0 })
payment is:

b , ) =
ri (b
c,


b )) Bi (b
b ) , = 1
c,
ci ,
v0 (K (b


b )
Bi (b
ci ,

(12)

, = 0

Bi () 0 term independent report (a constant point view)
reduces payment needs made agent. briefly discuss value Bi ()
could set reduce payout made centre later section, provide greater
detail section 4.4.
addition paying winner, also reward losers k \ following way,
depending whether succeeds not:
15

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

b , ) =
rk (b
c,


b )) b
b )) Bk (b
b k ) , = 1
c,
ci (K (b
c,
ck ,
v0 (K (b


b )) Bk (b
b k )
b
ci (K (b
c,
ck ,

(13)

, = 0

Intuitively, payment scheme aims incentivise agents reveal type
efficient allocation chosen. Let K0i allocation assigning task agent i. Suppose
b ) agents report (b
b ).
agent type (ci , ) reports type (b
ci ,
ci ,
agent wins task, derive following expected utility:




b = v0 K0i tr0i ,
b ci K0i Bi (b
b )
ui K0i , ,
ci ,
(14)

b reflects true POS agent i. agent k 6= assigned task,
Note tr0i ,
agent obtains following expected utility participating mechanism:





b = v0 K0k tr0k ,
b b
b )
ui K0k , ,
ck K0k Bi (b
ci ,
(15)

difference Equations (14) (15) identity winner. Hence,
falsely reporting, agent influence identity winner. Agent expected utility
mechanism equal expected social utility system minus constant independent
report. Hence, agent rational report true type, efficient agent
(outcome) chosen. shows single task trust-based mechanism incentive compatible
efficient.3
Proposition 1. mechanism described Equations (11), (12), (13) incentive compatible.
Proposition 2. mechanism described Equations (11), (12), (13) efficient.

Proof. Since agent ks report k affects expected utility agents (see Equations
(14) (15)), interdependence agents payoffs, valuations. However, agent
influence transfer report, computation agent payment
b (and b
independent report
ci ) dependent actual execution task
therefore true value. feature permits implementation efficient
allocation single-stage mechanism.
exemplify payments mechanism, consider following extension Example 5.
Example 6. Two agents zero cost performing task requested centre
EQOS ij ( ) {0.6, 0.7, 0.8} i, j = 1, 2. Suppose tr0i () = [1i ( ) + 2i ( )]/2,
v0 ( ) = 1.
setting Bi = 0.6 example, payment agent
task completed successfully 0.4, payment task fails 0.6. Hence,
centre profits implementing mechanism. Agents incentive report truthfully,
agent likely succeed allocated task. Furthermore, agents willing
participate, probability success least 0.6 (it 0.6 worst case scenario)
3. provide detailed proof generalised case Section 4.3.

16

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

hence, agents expect obtain least zero participating: mechanism individually rational. Also note total expected payment centre agents
0.8 0.4 2 0.2 0.6 2 = 0.4, could low 0.6 0.4 2 0.4 0.6 2 = 0.
show, Bi always chosen individual rationality satisfied.
Proposition 3. appropriate choice Bi (), mechanism described Equations (11),
(12), (13) individually rational.
Proof. participating mechanism, agent obtain 0 utility. However,
agent decides participate, virtue selection efficient allocation (which returns
allocation social welfare generated less 0), guaranteed, winner, obtain
utility ui described Equation (14) or, loser, utility uk Equation (15). Since
cases ui Bi () efficient allocation chosen, Bi set 0, mechanism
individually rational.
Obviously, since agents utilities tied winning agent, also lose
winning agent fails but, expectation, agents make profit least 0 case Bi set
0. Example 6 shows, centre trying minimise payments (and increase profits),
could set Bi greater zero still satisfy individual rationality. Section 4.4,
show set Bi value maintains individual rationality minimising payments
general model.
note sometimes may preferable centre give individual rationality.
Consider, example, modify Example 6 allow additional EQOS value ij ( ) = 0.3
i, j = 1, 2. induce type ij ( ) = 0.3 participate, centre could set Bi () = 0.3,
payment following success 0.7 payment failure 0.3. worst
case scenario centre (i.e., centres profit lowest), total expected payment
mechanism 0.8 0.7 2 0.2 0.3 2 = 1 (in best case scenario, total
expected payment zero). shall see Section 4.4, centre could substantially reduce
payments making Bi () depend report agents (i.e., i). Still,
may preferable centre set Bi () = 0.6, giving participation agents
EQOS values ii ( ) = ij ( ) = 0.3. general, low EQOS types, centre faces
trade efficient task allocation payments minimisation. leave study
trade-off future work (see Section 6 initial thoughts).

4. Generalised Trust-Based Mechanism
mechanisms presented previous section dealt basic task allocation problem
one requester, one task, several performers. Here, aim efficiently solve
general problem trust-based interactions one agent requests
performs (or both) one task. end, extend single requester single task
setting general one multiple requesters multiple tasks Generalised TrustBased Mechanism (GTBM). extension needs consider number complex features
top dealt previously. First, need consider multiple requesters
make requests sets tasks task performers perform sets tasks well.
Thus, centre acts clearing house, determining allocation payments
17

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

multiple bids task requesters multiple asks task performers. significantly
complicates problem incentivising agents reveal types since make
sure agents reveal costs, valuations, EQOS truthfully one task.
Second, computation efficient allocation payments consider much larger
space previously. Thus, believe important show problem modelled,
implemented, solved demonstrate mechanism scales increasing numbers
agents tasks (the computability aspects dealt Section 5).
following example illustrates general setting.
Example 7. using trust-based mechanism months, MoviePictures.com made
significant profits expanded several independent business units, performing rendering
tasks rendering tasks performed certain clients. Now, MoviePictures.com would like
find ways business units efficiently allocate tasks amongst themselves. However,
companies uncertainties others performance rendering tasks.
example, business units, HighDefFilms.com, believe PoorRender Ltd (now part
MoviePictures.com) inefficient, others, GoodFilms.com, believe bad,
recently large set animations rendered well cheap price. cater
differences opinion maximising overall utility, MoviePictures.com needs extend
single task trust-based mechanism implement generalised mechanism efficiently.
order deal complex setting, extend task allocation model next
subsection, describing allocation rule payment scheme Section 4.2 proving
economic properties mechanism Section 4.3.
4.1 Extended Task Allocation Setting
Let = {1 , 2 , ..., } denote set tasks requested performed (compared
single task before). use notation .i specify subset tasks
performed specifically agent i.4 Similarly, adding superscript task, i. K
denotes subset tasks agent performs. Note nothing model restricts
agent task performer requester.
selected allocation K multiple task, multiple requester model generates matching problem involves finding agents perform tasks requested
II }). Let set possible allocations
agents (e.g., K = {111 , 112 , . . . , 1I1I , . . . ,
denoted K. Note requested tasks need allocated: is, matching K need
perfect.
multiple task case, agents may express valuations costs sets tasks well
subsets sets tasks. example, agent may vi (1 , 2 , 3 ) = 100 vi (1 , 2 ) =
10 vi (3 ) = 0. Then, agent gets 1 , 2 3 executed gets value 100,
1 2 get executed 3 fails, agent still obtains value 10. Similarly, agent may
task execution costs ci (4 , 5 , 6 ) = 100 ci (4 , 5 ) = 40 ci (6 ) = 10. capture
inter-relationships valuations, let Kij set tasks within allocation K
performed agent j agent (Kij could empty set). Note task
specific task requester. means agents 1 2 request task , task performer
4. paper, consider agents requesting performance multiple units tasks. Although model
easily extensible case, explanation much intricate.

18

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

(putting one bid ) matched agent 1, performs agent 1 agent
2. abuse notation slightly define K = {Ki , K }iI Ki = (Ki1 , ..., KiI )
K = (K1i , ..., KIi ). agent value (assuming tasks K completed) cost
allocation K, vi (K) + {0} ci (K) + {0} respectively, whereby:5
vi (K) = vi (Ki )
ci (K) = ci (K )
Kh K

Moreover, within model, agent EQOS vector, = {ij (Kh )}j,hI
represents belief successful agents within system completing tasks Kh
agent h. Thus, general level, agent type given = {v , ci , }.
j
e j K j EQOS
given set tasks Ki
thatfij mustperform i, subset tasks K


e j completed
e j fifi K j , trust exactly set tasks K
vector , let trj K








j. trust computed shown Section 3.3 simply replacing agent 0
agent replacing single task set tasks . single requester case, trust
function represents aggregate belief agents given task performer hence
task requesters form probability
(give agents EQOS reports) given

fi success
Q j e j fifi j
fi
e
tri Ki fi Ki , .
task performer. Finally, let tri Ki fi Ki , =
jI

ready present generalised trust-based mechanism.

4.2 Allocation Rule Payment Scheme
generalised mechanism (GTBM), task requesters first provide centre list
tasks require performed, along valuation vector associated set
tasks, whereas task performers provide costs performing sets tasks.6 agents
also submit EQOS vector centre. Thus, agent provides centre reports
bi = {b
b = (
b1 , ...,
bI ) report profile. Given this, centre applies
b },

vi, b
ci ,
rules mechanism order find allocation K net payments ri agent i.
detail:
1. centre computes allocation according following:

b =
K

arg max

X

K={Ki ,K }iI K iI




X

e Ki
K

e ) tri
vbi (K




fi

e fifi Ki ,
b b
ci (K)
K

(16)

Thus, centre uses reports agents order find allocation maximises
expected utility agents within system.

b .
2. agents carry tasks allocated allocation vector K

5. result setup, agent may want sets tasks performed may unable perform
tasks. cases, assign default value 0 cost sets tasks.
6. noted before, task performers also task requesters time (and vice versa).

19

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

3. centre computes payments agents, conditional completion tasks allocated. Let (Ki ) indicator function takes value one Ki set
tasks (requested agent agents) completed, takes value zero
otherwise. payment agent follows:

ri





X
b () =
,

jI\i

X

e j K (b)
K
j









ej b
ej K
cj
vbj K





b
bi )
K
Bi (

(17)

bi ) 0 constant point view (i.e., computed independently
Bi (
agent reports, may depend reports agents), used
reduce payout centre make.
discussed Section 3.3.2, centre faces trade-off. reducing value Bi ()
induces participation larger set types (i.e., types low EQOS), increases
centres payments agents, making mechanism less profitable centre. Thus,
scale payments one might expect application GTBM depends whether
centre decides satisfy individual rationality constraint, thus making sure every
type wants participate. shall see Section 4.4, centre decides satisfy
individual rationality constraint, scale payments agent increases lower
bound trust values could derived using EQOS report.
also noted computation payments requires solving several optimisation problems (i.e., finding optimal allocation without several reports).
number agents increases, difficulty computing payments increase
important show payments efficiently computed. elaborate
solution Section 5. so, however, detail prove economic
properties mechanism follows.
4.3 Economic Properties
Here, provide proofs incentive compatibility7 efficiency mechanism.
also prove values Bi make mechanism individually rational.
Proposition 4. GTBM incentive compatible.
Proof. order prove incentive-compatibility, analyse agent best response ( i.e.,
bi = {b
bi . first calculate expected
b } )) agents report
best report
vi, b
ci ,
utility agent derive given mechanism.
7. Again, place caveat notion incentive compatibility use Section 3.3 (i.e.,
Dominant Strategy (ex-post) Nash equilibrium depending whether agent computes trust functions
using agents POS reports true not).

20

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

by:

b true types given
expected utility agent reported types


b =
ui ;

X



b b
e K
K
,










bi ,
bi , ,
e |Ki
e ) tri K
b
vi (K



bi ,
bi
ci K





b
+ Eri ;



(18)

Eri expectation ri taken respect likelihood task completion.
e j ) equal one (i.e., tasks K
e j
probability attached indicator variable (K
completed), given
fi set
tasks requested j Kj agents different report
fi
b
e
b . Hence, use formula payments obtain:
, trj Kj fi Kj , ,




fi

bi ,
bi , ,
e j fifi Kj
e j trj K
b
vbj K





X

Ke j K b ,b
bi )
b
Eri ; =
j
Bi (






jI\i
bi ,
bi
b
cj K
(19)
replace expression formula ui observe agent
b key point note agent computes
affect utility report changing K ().
b ).
value trust function using true value ( rather reported value
Now, Equation (16) implies allocations K:




bi ,
bi ; ,
bi ; ui
(20)
ui ,


X

efficient allocation, computed taking account true type reported
bi better equal allocation.
types agents
Given condition since Equation (20) applies possible realisations ,
mechanism incentive compatible.
Proposition 5. GTBM efficient.

Proof. Given incentive compatibility mechanism, centre receive truthful reports
agents. result, compute allocation according Equation (16), thereby
leading efficient outcome.
Proposition 6. exist values Bi () GTBM individually rational.
Proof. begin making standard assumption agent derives ui = 0,
participating mechanism. Then, remains shown agent derives non-negative
utility mechanism. Since efficient allocation chosen (and worst null allocation),
expected utility agent always greater equal Bi () according Equation
(18). Since Bi () set 0, mechanism individually rational.
Note possibly many values Bi ( ), besides Bi = 0, guarantee individual
rationality.
21

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

Speaking generally, easily seen GTBM mechanism multiple task,
multiple requester scenario generalisation GTBM mechanism single requester
single task. also generalisation mechanism Porter et al. simply assume
agent EQOS probability success. Moreover, paper
Porter et al. example, Bi specified follows:


X

bi ) = max v0 (K ) pbi ( )
Bi (
b
c
(K
)
j

K Ki

jI\i

pbi ( ) reported probability completion agent assigned task allocation
K Ki set allocations excluding agent i.
4.4 Extracting Minimum Marginal Contribution

now, considered Bi ( ) could set arbitrary values try reduce
payments made centre agents. interestingly, possible,
standard VCG mechanism, pay agent marginal contribution system.
However, case, due interdependence valuations, simple comparing
social welfare without given agent system commonly done VCG-based
mechanisms (Porter et al., 2008 obvious example this). because, case,
agent removed domain used compute efficient allocation, remaining EQOS
reports arbitrarily change allocation value. could, turn, exploited agents
improve utility. example Section 3.3.1 showing failure simple extension
Porter et al.s mechanism illustrates point.
Assuming centre wants induce participation agent types, propose
novel approach extracting marginal contribution agent, taking account EQOS
reports agents possible reports agent could make. Let Ki set possible
allocations agent excluded society. value Bi () chosen
equivalent social utility mechanism agent excluded EQOS reports
chosen minimise social utility, is:



fi

X
X
bi ) =
e j fifi Kj , ,
e j trj K

b b
cj (K) (21)
Bi (
min
max
vbj K
|I||T
|
KKi
[0,1]
jI\i e
Kj Kj

noted Bi computed using lowest trust values could derived using
EQOS reports.
Then, generalised payment scheme is:







X X
b
b (.) =
ej b
ej K

K

c
K
v
b
ri ,
j
j



b
jI\i K
e j K
j


fi


X
X
e j fifi Kj , ,
e j trj K

b b
cj (K)
vbj K

min
max
[0,1]|I||T | KKi jI\i e
Kj Kj

(22)

22

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

point note incentive compatibility (and hence efficiency mechanism)
still holds given payment scheme still independent reports. fact, ri rewards
maximum difference agent could make setting elements different values
[0, 1]|I||T | .8
procedure reduces payments made centre, keeping individual rationality
since value efficient allocation (given incentive compatibility proven earlier) always
higher equal value Bi , means that:
ui (K (), ) =



X

jI



X

e j K ( )
K
j





e j trj
vj K
X






fi

e j fifi Kj () , cj (K ())
K



min
max
[0,1]|I||T | KKi jI\i e

X

Kj Kj





e j trj
vj K




fi

fi
e j fi Kj , , cj (K) 0;
K

also noted equation implies restriction placed
functional form trust function tr payment scheme work properties
mechanism hold. improvement previous mechanisms (see Section 2)
considered trust functions monotonically increasing i.
Now, choice Bi determines whether centre runs mechanism profit not.
Hence, understand scale payments may GTBM discussed section,
consider following example.
Example 8. n agents, = {1, ..., n}, requiring single task performed
them. agents value 1 task performed zero cost
performing tasks. EQOS agent h agent probability succeeding
task
P



agent j h (Kj ) [x, 1] h, i, j = 1, ..., n. Suppose trj () =
hI h (Kj ) /n.

example, EQOS agent interval [x, 1], x viewed
lower bound expected probability success task. Equation (21)
compute value Bi :
"P
#
(K ) + x
X

j
hI\i h
Bi ( ) =
max

n
jI\i

Note that, depending value , Bi ( ) could value (n 1)x
(n 1)(n 1 + x)/n; Bi increases lower bound x agents EQOS. actual payment
agent depend success failure task (e.g., payment Bi tasks
fail). Equation (19), calculate value expected payment agent as:
"
#
X (Kj )
X
h
Eri () =
max
Bi ( )

n
hI
jI\i
"
#
X (j) x


n
jI\i

8. minimisation takes place domain trust values could [0, 1] general case.

23

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

(j) agent allocated task agent j efficient allocation rule. Let EV
total expected value tasks:
"P
#
(j)
X
iI
EV () =
n
jI

Note total expected value tasks greater sum expected payments
agents, is:

" (j)
#
#
"P
(j)
X
X
X

x
iI

>
n
n
jI

iI

jI\i

Thus, centre always profits mechanism. lower bound difference total
P
(i)
+ (n 1)x]/n. Note also lower
expected value total expected payments [ iI
bound centres profit mechanism increases lower bound EQOS x.
pointed discussion Example 6 Section 3.3.2, centre trying minimise payments, could give individual rationality, increasing Bi , cost inducing
agent types participate mechanism. may appealing probability
task failure high; cases, centre may prefer avoid paying amount almost large
total value tasks. hand, number practical applications centre
may want use mechanism induces participation types, described section.
certainly case, example, lower bound EQOS (i.e., lower bound
probability tasks successful) high. Moreover, mechanism participation
types appropriate centre mainly seeks maximise social welfare. Consider, example, government trying boost economy major public infrastructure projects.
order so, may willing invest trust-based mechanism get best infrastructures built cheapest cost. Moreover, government may willing make low profit
order ensure survivability construction companies guaranteeing payoff
participate mechanism. Another example company might want involve
task performers would company trying acquire much information possible
task performers order maximise returns future decisions. Following running scenario, say MovePictures.com needs contract video editing company add computer
graphics movie may become blockbuster graphics well done. case task
successful, MoviePictures.com likely get many contracts future. therefore critical
available information collected agents order choose reliable video
editing company. case, MoviePictures.com may accept smaller short-run profit running
mechanism full participation, order guarantee selected agent best one
future contracts obtained.
summarise, section devised mechanism incentive compatible, individually rational efficient task allocation uncertainty multiple distributed reports
used order judge uncertainty. noted need two-stage mechanisms, work Mezzetti (2004), settings condition payments
completion tasks (the indicator function () captures dependence payments task
completion). far, considered economic properties mechanisms,
argued earlier, part picture. next section, report implementation.
24

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

5. Implementing Generalised Trust-Based Mechanism
shown above, addition trust basic task allocation problem complicates
payment scheme, also requires larger number important optimisation steps normal
VCG. detail, trust-based mechanisms require agents specify expected value set
tasks depending performer tasks which, turn, means space solutions
explored significantly larger common task allocation problems. Moreover, payment
scheme trust-based mechanisms requires finding efficient allocation multiple times
without agents reports. added level complexity, important show
mechanisms actually implementable solutions found usefully sized problems
reasonable time.9
background, section describe first formulation implementation GTBM. particular, GTBM, tackle main optimisation problem posed
Equation (16) (which repeated several times payment scheme). commonly
referred winner determination problem combinatorial auctions. order solve it,
take insight solutions combinatorial exchanges often map problem well
studied matching problem (Kalagnanam & Parkes, 2004; Engel, Wellman, & Lochner, 2006).
doing, develop novel representation optimisation problem using hypergraphs
describe relationships valuations, trust, bids task performers cast
problem special hypergraph matching problem. Given representation, able
solve problem using Integer Programming techniques concise formulation
objective function constraints.
5.1 Representing Search Space
important define search space way relationships valuations, bids,
trust, tasks clearly concisely captured. particular, representation aims map
GTBM optimisation problem matching problem well studied literature.
this, representation must allow us define whole space feasible task allocations,
and, subsequently, define select valid solutions GTBM optimisation problem.
Now, allow bidders (task performers) askers (task requesters) express bids valuations consistent implementable way, choose XOR bidding language. bidding
language requires auctioneer accept one bid XOR bid
XOR bid belong one agent. choose particular bidding language
shown valuation expressed using (Nisan, 2006).10 example XOR
bid context would {ci (1 , 2 ) XOR ci (1 , 3 ) XOR ci (1 , 2 , 3 )} means agent
would go one three bids tasks 1 , 2 3 (ci could also replaced
vi task requesters). terms running example, bid would express PoorRender
Ltds cost performing sound editing task (i.e., 1 ), movie production task (i.e., 2 ),
combination (i.e., 1 , 2 ).
9. already known computing efficient allocation payments VCG mechanisms NP-hard (Sandholm,
Suri, Gilpin, & Levine, 2002). Therefore, finding efficient solutions VCG mechanisms already significant
challenge right.
10. bidding languages (such describing Atomic bids, Nisan, 2006) could equally well used
model would require minor changes constraints need apply.

25

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS


"
V
H











+

fi
ff

$

fi
(

%

fi
)

'

&

"#"


(
(

!

H


!"
C


$

%



(
)

&


(*

'




Figure 1: Graphical representation GTBM search space. Nodes colour represent valuation
cost nodes belong agent (here nodes v1 belong agent 1 c4 belong
agent 4). Edges colour either originate node end node.

build overall representation problem, first focus representing expected valuations costs well relationships. depicted Figure 1. detail,
specify three types nodes: (1) valuations (along V column); (2) bids (under C column);
(3) task-per-bidder nodes (under column). node vi ( ) V column stands
valuation submitted agent set tasks . node cj ( ) C column stands
j.
bid issued agent j tasks . element represents allocation

single task task performer (bidder) j task requester yet determined (represented dot). words, elements represent patterns single-task allocations.
term elements task-per-bidder nodes.
Note possible different valuations come requester.
labelled subscript. Moreover, since opted XOR bidding language,
valuations belonging requester mutually exclusive.
26

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

5.1.1 EFINING R ELATIONSHIPS



VALUATIONS , TASKS ,



B IDS

j.
node vi (..., , ...) V
Given nodes defined A, V, C, relating node
define assignment task j specific valuation vi (..., , ...). Similarly,
j.
node cj (..., , ...) C define assignment task
relating node
j.
j.
specific bid cj (..., , ...) agent j. Therefore, triple (v,
, c) v V,
A, c C
fully characterises allocation task , namely single-task allocation. Hence, seen
Figure 1, define two types relationships: valuations task-per-bidder nodes
(noted edges e1 , e2 , ...), bids task-per-bidder nodes (noted edges e1 , e2 , ...).11
Using relationships, valuation related set task-per-bidder nodes
fully cover performance task(s) valuation. instance, relate
v1 (1 , 2 ) nodes 14. (agent 4 performs task 1 ) 22. (agent 2 performs task 2 )
guarantee performance tasks 1 2 . Similar valuation relationships, node C
related set task-per-bidder nodes bid splits. Thus, Figure 1,
bid c4 (1 ) related 14. , whereas bid c2 (2 , 3 ) related nodes 22. 32. .
Thus, identify task performers task given valuation. critical since
GTBM, contrary common task allocation mechanisms (such VCG Mth price auctions),
requires identify exactly performs task order determine POS task (by
virtue requesters trust performer) hence expected value task.
seen, representation allows us capture tasks performers tasks
since valuation node V potentially related multiple nodes A; and, likewise,
bid C column potentially related multiple nodes A. capture related relationships precisely, define special edges connect several nodes (e.g., ones depicted
e1 , e2 , ,e1 , e2 ,... Figure 1). edges termed hyperedges combine number
singleton edges. Hence, Figure 1 best described hypergraph (Berge, 1973). order
precisely define matching problem GTBM poses, elaborate formalism
hypergraphs since help concisely expressing problem later on. specifically,
formal notion hypergraphs, introduced paper Berge (1973), is:

Definition 9. Hypergraph. Let X = {x1 , x2 , . . . , xn } finite set n elements, let E =
{ej |j J} family subsets X J = {1, 2, ...}. family E said hypergraph
X if:
1. ej 6= (j J)
2. jJ ej = X.
pair H = (X, E) called hypergraph. elements x1 , x2 , . . . , xn called vertices
sets e1 , e2 , . . . , ej called hyperedges.
say hypergraph weighted associate hyperedge e E real number,
w(e), called weight e. used give less importance edges.
formal definition hypergraphs, observe Figure 1 results overlapping two separate hypergraphs: (i) valuation hypergraph occurs linking valuations
11. Figure 1 depicts sample possible relationships ease illustration.

27

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

task-per-bidder nodes; (ii) bid hypergraph occurs linking bid corresponding task-per-bidder nodes. follows, formally define hypergraphs valuations bids later structurally characterise notions feasible optimal
allocations.
5.1.2 VALUATION H YPERGRAPH
valuation hypergraph highlights main difference GTBM common combinatorial exchanges (e.g., based traditional VCG Mth -price auctions). particular,
GTBM valuations need take account trust task requester task performer
while, normal combinatorial exchanges, task requesters indifferent task performers.
means weight hyperedge valuation hypergraph dependent trust large
number edges need generated (one per task performer) case normal
combinatorial exchanges.
define valuation hypergraph, need define hyperedges emanate node
V one nodes A. end, let V = {vi ( ) 6= 0| , I} C = {cj ( ) 6=
| , j I} sets valuations bids respectively. Let j. = { |
: cj ( ) 6= } set tasks agent j submits bids. Hence,
= {kj. |k j. , j I, cj ( ) C} set containing tasks bid bidder.12
Furthermore, need define auxiliary sets follows. Given valuation set
tasks , set nodes fulfils if:
[

{kj. } = | | = |A|

kj.

instance, set nodes = {14. , 22. } fulfils valuation {1 , 2 }. Hence,
subsets fulfil valuation set tasks expressed using defined as:
[
= {A |
{kj. } = | | = |A|}
kj.

instance, considering example Figure 1,
A{1 ,2 } = {{14. , 24. }, {14. , 22. }, {14. , 25. }}
A{1 ,3 } = {{14. , 34. }, {14. , 32. }}
Given definitions, define set hyperedges connected valuation
vi ( ) V as:
Eiv ( ) = aA {{vi ( )} a}
instance, Figure 1:
E1v (1 , 2 ) = {e1 , e2 , e3 } E1v (3 ) = {e4 , e5 },
e1 = {v1 (1 , 2 ), 14. , 24. }, e2 = {v1 (1 , 2 ), 14. , 22. }, . . . , on.
12. Recall since mechanism proven incentive-compatible use agents true valuations
costs instead reported counterparts.

28

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

set hyperedges containing valuations agent defined as:
[
Eiv =
Eiv ( )

Then, set hyperedges connecting nodes V nodes defined as:
[
Ev =
Eiv
iI

Given this, define valuation hypergraph pair:
Hv = (V A, E v )
Thus, hyperedge Hv consists single valuation vertex corresponding element V
along complete task allocation valued tasks task-per-bidder nodes A.
valuation hypergraph Hv partly defines space within solution needs found.
However, order define quality solution found, important define weight
attached hyperedge hypergraph Hv . weight hyperedge actually equal
expected value allocation tasks set task performers (bidders). Consider,
instance, valuation v1 (1 , 2 ). possible matchings fulfil represented
pairs (1.1 , 2.1 ). example, hyperedge e2 involving pairing (141 , 221 ) denotes
agent 4 performs task 1 agent 1 agent 2 performs task 2 agent 1. expected valuation
associated allocation depends POS agents 4 2 performing tasks 1 2
respectively.
case, expected valuation associated e2 assessed as:
v 1 (141 , 221 ) = v1 (1 , 2 ) p4 (141 ) p2 (221 )+
v1 (1 ) p4 (141 ) (1 p2 (221 ))+
v1 (2 ) (1

p4 (141 ))



(23)

p2 (221 )

p function returns POS agent assigned given task (computed using
confidence, reputation, trust). Notice value (1 pi (kij )) represents probability
agent failing perform task k agent j. Since requests submitted 1 2 alone,
v(1 ) = v(2 ) = 0. Thus, expected valuation associated particular allocation represented
arc e2 becomes v 1 (141 , 221 ) = v1 (1 , 2 ) p4 (141 ) p2 (221 ). similar argument,
obtain v 1 (141 , 251 ) = v1 (1 , 2 ) p4 (141 ) p5 (251 ) 6= v 1 (141 , 221 ), corresponding
hyperedge e3 .
Generalising, given hyperedge e E v valuation vi ( ), readily build allocation
tasks elements e vi ( ). p function returns POS (be
confidence, reputation, trust) given task performer requesters point view,
compute expected valuation allocation defined hyperedge e follows:


X




v ( ) =
pj (lji )
1 pj (wji )
(24)
vi ( )

lj. e,l
lj. e,w \
29

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

words, given hyperedge e E v , weight assessed using Equation (24)
equivalent expected value computed Equation (16) (i.e., sum expected values
allocations agent i). Now, given edge valuation hypergraph assigned
weight, Hv termed weighted hypergraph.
5.1.3 B ID H YPERGRAPH
define bid hypergraph need determine hyperedges connect bids task-perbidder nodes. detail, given bid cj ( ) C, relate task-per-bidder nodes
constructing hyperedge Ejc ( ) = {cj ( )} {kj. |k }. hyperedge assigned weight
equal cost cj ( ). set hyperedges containing bids agent
defined as:
[
Eic =
Eci ( )

Given this, set hyperedges connecting nodes C nodes defined as:
[
Ec =
Eic
iI

Finally, define bid hypergraph pair:
Hc = (A C, E c )
words, hyperedge Hc consists single bid vertex corresponding element
C along corresponding task-per-bidder nodes A. Notice definitions valuation
bid hypergraphs ensure hyperedge H v contains single valuation V
hyperedge H c contains single bid C.
5.1.4 EFINING ATCHING P ROBLEM GTBM
defined valuation bid hypergraphs, structurally characterise notions
feasible optimal allocations (these needed determine computational complexity
problem define objective function particular). purpose, must firstly recall
notions hypergraph theory. hypergraph, two hyperedges said adjacent
intersection empty. Otherwise said disjoint. hypergraph H = (X, E),
family E E defined matching hyperedges E pairwise disjoint. respect
given matching E , vertex xi said matched covered hyperedge E
incident xi . vertex matched, said unmatched exposed. matching
leaves vertices exposed said complete.
Based definitions above, characterise feasible allocations GTBM follows.
First, must find matching valuation hypergraph necessarily complete (some
valuations may remain exposed). Second, must find another matching bid hypergraph
necessarily complete either. two matchings must related following manner:
task-per-bidder nodes matchings same. words, given task-perbidder node, must related valuation node bid node, else excluded
matchings. way, valuations bids linked create single-task allocations.
instance, Figure 1, e2 belongs matching valuation hypergraph, e4 must
30

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

part matching bid hypergraph ensure bid 22. either
e1 , e2 , e3 part matching bid hypergraph ensure bid 14. .
formally:




Definition 10. Feasible allocation. say pair (E v , E c ) defines feasible allocation iff:


E v matching Hv .


E c matching Hc .




A: ( matched E v ) ( matched E c ).




Given feasible allocation (E v , E c ) defined above, straightforward assess expected utility agents within system follows:
X

eE v

w(e)



X

w(e )


e E c

since weights hyperedges valuation hypergraph stand expected valuations
weights hyperedges bid hypergraph stand costs. Solving Equation (16)
GTBM amounts finding feasible allocation maximises expected utility agents
within system. Therefore, following definition naturally follows.
Definition 11. GTBM Task Allocation Problem problem assessing task allocation
maximises expected utility agents within system amounts solving:
arg max
(E


v


,E c )

X

eE

wv (e)

X

wc (e )

(25)


e E c

v



(E v , E c ) stands feasible allocation.
defined matching problem GTBM, next describe solution
problem using Integer Programming techniques commonly used solve problems
(Cerquides, Endriss, Giovannucci, & Rodrguez-Aguilar, 2007).13
5.2 Integer Programming Solution
section show map problem posed Equation (25) integer program
(Papadimitriou & Steiglitz, 1982) efficiently implemented solved. Given
translation, resulting program solved powerful commercial solvers ILOG
CPLEX14 LINGO.15
13. special purpose algorithms (e.g., using dynamic programming search trees) could also designed solve
combinatorial problem. However, understand magnitude problem compare difficulty
solving problem similar problems, believe better first attempt find solution using
standard techniques IP.
14. http://www.ilog.com
15. http://www.lindo.com

31

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

5.2.1 BJECTIVE F UNCTION



IDE C ONSTRAINTS

translation Equation (25) IP reasonably straightforward given representation.
Thus, solving GTBM task allocation problem amounts maximising following objective
function:
X

xe wv (e)

X

ye wc (e )

(26)

e E c

eE v

xe {0, 1} binary decision variable representing whether valuation hyperedge
e selected not, ye {0, 1} binary decision variable representing whether bid
hyperedge e selected not. Thus, xe decision variable selects given valuation
given task-bidder matching, ye selects given bid.
However, side constraints must fulfilled order obtain valid solution. First,
semantics bidding language must satisfied. Second, hyperedge containing set taskper-bidder nodes selected, must ensure bids covering nodes selected too.
Moreover, employ XOR bidding language, auctioneer centre case
select one bid per bidder one valuation per asker. Thus, bidders,
constraint translates into:
X
ye 1
(27)
e Eic

instance, Figure 1 constraint ensures auctioneer selects one hyperedge e1 , e2 ,
e3 , since belong agent 4 (they come nodes labelled subscript
c4 (.)).
valuations, XOR constraints involving collected following expression:
X
xe 1
(28)
eEiv

instance, Figure 1 constraint forces auctioneer select one hyperedge e1 , e2 ,
e3 , e4 , e5 since belong agent 1 (they come nodes labelled
subscript v1 (.)).
valuation hyperedge e E v selected, set task-per-bidder nodes connected
e must performed corresponding bidder agent. instance, Figure 1, hyperedge
e5 selected, task-per-bidder nodes 141 341 must covered bid agent 4.
case, bid c4 (1 , 3 ) one covering tasks. Thus, select hyperedge e5
forced select bid c4 (1 , 3 ) selecting hyperedge e3 . Thus, terms hyperedges, must
ensure number valuation hyperarcs containing given task-per-bidder node less
equal number bid hyperarcs containing it. Graphically, means number
incident valuation hyperedges given node must less number incident bid
hyperedges a.
X

eE v ,kj. e

xe

X

ye

kj.

(29)

e E c ,kj. e

case free-disposal (i.e., allow agents execute tasks without asked
for) simply replace =. summarise, solving GTBM task allocation problem
32

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

amounts maximising objective function defined expression (26) subject constraints
expressions (27), (28), (29). Next, determine complexity results problem.
5.2.2 C OMPLEXITY R ESULTS
represented GTBM task allocation problem defined corresponding IP formulation, analyse computational complexity order show difficulty solving GTBM.
also identify main parameters affect computational costs finding optimal allocation. parameters allow us determine settings GTBM
practically used.
Proposition 7. GTBM task allocation problem N P-complete cannot approximated
ratio n1 polynomial time unless P = ZPP, n total number bids
valuations.
Proof. Notice optimisation model, formalised Equation (26), naturally translates
combinatorial exchange (Kalagnanam, Davenport, & Lee, 2000). translation achieved
using representation taking goods (in combinatorial exchange) dummy tasks
, bids elements C, asks weights hyperedges Hv . Thus,
bids remain exchange, number valuations may significantly increase.
reason introduction trust theoretical model makes initial valuations (asks),
elements V, allocation-dependent. Hence, every single valuation V causes several asks
originated exchange considering bidder task may allocated
(see examples Section 5.1.2). shown Sandholm et al. (2002), decision problem
binary single-unit combinatorial exchange winner determination problem N P-complete
optimisation problem cannot approximated ratio n1 polynomial time unless P = ZPP,
n number bids. Therefore, optimisation problem N P-hard,
GTBM.
proof, understood search space GTBM task allocation
problem significantly larger traditional combinatorial exchanges dependency valuations bidders performing tasks. follows, provide formula
allows us calculate exactly big search space is. allows us determine whether
instance solved actually handled solver (which limits
memory requirements computation time).
detail, say Ak subset containing task-per-bidder nodes referring
tasks. formally, Ak = {kj. | j I}. example Figure 1,
A2 = {24. , 22. , 25. }. Thus, expression assess number feasible allocations is:
|E v | =

X X



|Ak |

(30)

iI vi ( )6=0 k

Observe number possible allocations computed cardinality E v (i.e.,
number valuation hyperarcs) since exactly determines number ways valuations
satisfied provided bids. total number decision variables Integer Program
thus |E v | + |E c |. Since number expected valuations several times larger number
bids, expect number decision variables associated bid hyperedges much less
33

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

number valuation hyperedges. Hence, assuming |E c | |E v |, number decision
variables order |E v |.
order understand implications parameters, consider case task
performers bid tasks requesters submit single valuation tasks. Specifically,
consider scenario 15 task performers, 20 requesters, 5 tasks. Given case
|Ak = 5|, number allocations |Ev | = 20 155 = 15187500. reality, agents may
able submit bids asks tasks would result significantly lower number
allocations (given possible matchings). Hence, see whether instances practically
solved, Appendix A, report running times solver, showing instances less
2 105 variables comfortably solved within 40 seconds (in worst case). taken
together, empirical results formula compute size input (i.e., Equation 30)
allow us affirm that, even computational cost associated GTBM potential
rather high, solution handle small medium sized problems reasonable time (see
table 3). However, seen, time complete grows exponentially number
Set
1
2
3

Tasks
5
8
10

Task Requesters
20
20
20

Task Performers
15
15
15

Worst Case Running Time
34
40 mins
3 days

Table 3: Average running times different numbers tasks agents (taken 300 sample runs set
1, 50 sample runs sets 2 3).

tasks. experimental analysis, also found impact increasing number
task performers task requesters significant increasing number tasks.
explained fact that, given setup, larger number tasks allows significantly
matchings bids asks larger number bids asks. Hence, many task
requesters performers accommodated small numbers tasks. also noted
expect worst case results occur fairly rarely average (much less half
instances generated parameters), shown Figure 2 Appendix A.
described complete picture GTBM implementation, next discuss
important issues may arise trying use GTBM task allocation.

6. Discussion
paper developed task allocation mechanisms operate effectively agents
cannot reliably complete tasks assigned them. Specifically, designed novel Generalised
Trust-Based Mechanism efficient individually rational. mechanism deals
case task requesters form opinions task performers using reports environment direct interactions performers. addition studying economic
properties allocation mechanisms, provided optimisation model generates solutions guarantee efficiency mechanism. optimisation model first solver
trust-based mechanisms (and mechanisms value allocation depends
performer allocation) based Integer Programming. result, shown
input explodes combinatorially due huge number possible allocations must
enumerated. Nevertheless, computational cost associated GTBM shown
34

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

rather high, given implementation, still able manage small medium-sized problems
reasonable time.
Speaking generally, work trust-based mechanisms number broader implications. First, GTBM shows explicitly blend work trust models work
mechanism design. Since mechanism guarantees certain properties hold task allocation
problems, used new, well-founded testbed within trust models evaluated. now, trust models mainly tested randomly generated scenarios
interactions obey somewhat ad hoc market rules used ART testbed (Fullam,
Klos, Muller, Sabater, Topol, Barber, Rosenschein, & Vercouter, 2005). Second, work
first single-stage interdependent valuations mechanism efficient individually rational (as
opposed Mezzettis two-stage mechanism). made achievable settings
consider capturing interdependence types trust function making
payments agents contingent actual execution tasks. Another novelty approach
able extract (maximum) marginal contribution agent despite valuations interdependent (as shown Section 4.4). Third, implementation GTBM
highlights importance considering computational aspects new mechanism, since
determine whether mechanism implementable realistic scenarios indeed
bring claimed benefits. work strong statement direction since provide
complete picture problem, starting representation, implementation
sample results, complexity analysis.
practical terms, GTBM step towards building robust multi-agent systems uncertain
environments. environments, important aggregate agents preferences,
taking account uncertainty order ensure solutions chosen result best
possible outcome whole system. Prior GTBM, possible come
efficient solution would maximise expected utility. Moreover, fact agents
express perception task performers POS new way building expressive
interactions buyers sellers services (Sandholm, 2007). believe
perceptions expressed, better ensuing matching buyers sellers
results proof gain efficiency better matching brings (see sections 3.2.1,
3.3.1, 4.3).
introducing GTBM new class mechanisms, work lays foundations several
areas inquiry. end, outline main areas below.
Budget Balance: important economic property mechanisms contexts budget
balance.16 However, mentioned Section 3.3.2, designed TBMs without
considering budget balance. fact, GTBM budget balanced similar VCG
Porter et al.s mechanism. Now, one possible way overcoming problem sacrifice
either efficiency individual rationality. fact, dAGVA mechanism counterpart
VCG indeed sacrifice individual rationality budget balance (see Section
2). Moreover, Parkes, Kalagnanam, Eso (2001) develop mechanisms number
budget balancing schemes proposed near-incentive compatibility attained
making payments agents close possible VCG ones.
16. mechanism budget balanced, computes transfers allocation overall transfer system
zero (MasColellP
et al., 1995). Thus, budget balanced mechanism, allocation K associated transfer
vector r, ri r ri = 0.

35

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

effective scheme, Threshold rule, results low loss incentive-compatibility
relatively high efficiency (around 80%). budget balance may useful
situations centre cannot run risk incurring loss generating efficient
outcome set agents system. example, MoviePictures.com may find
worth injecting money system find efficient outcome subunits
nearly equally competitive (both price POS). Instead MoviePictures.com might
prefer mechanism generates near-efficient outcome increasing Bi discussed
Section 4.4. this, set agents participate might reduced
individually rational participate mechanism, but, nevertheless,
MoviePictures.com may obtain better outcome. future, study trade-offs
efficiency achieved system profit made centre.
Trust Task Requesters: One potential criticism mechanisms
task requesters (and centre) must trusted reveal observed execution task
(Mezzetti, 2004). However, setting, task requesters strong incentive reveal
observations (in case publicly visible) since would prefer chosen
task performer available next time mechanism run. end, must
ensure task performer go bankrupt. noted Equations (12) (17),
task performer would pay significant amount centre case reported
fail task. Hence, task requester better revealing successful execution
task performer indeed successful.
Another issue trust function used weights given agents EQOS report
may uncertain. Thus, case, agents may learn weights multiple
interactions. Given this, important develop learning search techniques
able deal large number possible weights could used trust
functions. techniques take account fact agents may lose
significantly exploring search space.
Iterative Mechanisms: GTBM one-shot mechanism allocation
payments calculated given type agents {v, c, } using trust models.
However, cases participants may engaged repeated interactions
exploited trust models order build accurate trust values counterparts.
situations, introduction multiple rounds compromise properties
mechanism allowing greater range strategies (e.g., cornering market consistently offering low prices initial rounds accepting losses initial rounds providing
false damaging information competitors). However, explosion strategy
space also implies agents might able compute optimal strategy due
intractability process. Now, one way solving problem constrain
strategies agents myopic (i.e., best response current round) shown
Parkes Ungar (2000) using proxy bidding. Another allow agents learn trust
models without participating allocation problem. Then, agents accurate representation trust functions POS values, mechanism implemented
one-shot encounter. Note problem arises one-shot mechanism
implemented iterative context solely realm GTBM.
36

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

Computational Cost: discussed Section 5, algorithms developed compute
efficient allocation run multiple times compute individual payments
agents TBMs. Hence, time needed compute allocation pay agents
may impractical agents limited time find solution, put forward large
number bids, ask large number tasks performed. Hence, important
either less complex mechanisms described Nisan Ronen (2007)
approximate (and computationally less expensive) algorithms developed solve
problems (Archer, Papadimitriou, Talwar, & Tardos, 2003). require work
developing local approximation algorithms approximate mechanisms preserve
properties seek. vein, paper provides point departure
future mechanisms since provides efficient mechanisms approximate
ones compared.
Acknowledgments
thank anonymous reviewers highly valuable comments; allowed us
improve upon previous version paper, restrictive mechanism,
also helped rework proofs. grateful Juuso Valimaki initial comments
mechanism, Ioannis Vetsikas, Enrico Gerding, Archie Chapman checking proofs
discussing ideas. Juan A. Rodriguez-Aguilar thanks IEA (TIN2006-15662-C02-01), Agreement
Technologies (CONSOLIDER CSD2007-0022, INGENIO 2010) Jose Castillejo programme
(JC2008-00337) Spanish Ministry Science Innovation. Andrea Giovannucci funded
Juan De La Cierva Contract (JCI-2008-03006) EU funded Synthetic Forager project
(ICT-217148-SF). Claudio Mezzetti thanks Fondazione Cassa di Risparmio di Padova e Rovigo
support. research paper also undertaken part ALADDIN (Autonomous
Learning Agents Decentralised Data Information Systems) project jointly funded
BAE Systems EPSRC (Engineering Physical Research Council) strategic partnership
(EP/C548051/1).

Appendix A. Analysing Performance IP Solution
section analyse computational performance Integer Programming solution
detailed Section 5 order gauge sizes problems solved reasonable time.
end, important recall (as shown Section 5) number input variables
optimization problem nearly equal number valuation hyperedges |Ev |, since |Ec | |Ev |.
Given this, assume performance solver directly related number
possible allocations approximated |Ev |.
Therefore, test set composed several instances GTBM Task Allocation Problem
characterised number possible allocations. detail, produce allocations, bids
valuations generated number bids submitted single bidder number
valuations submitted single requester follow geometric distribution p parameter
set 0.23 (Milton & Arnold, 1998) (in order randomly generate relatively large numbers
bids/asks per agent).17 medium-sized problem set follows. number negotiated tasks
set 5. number task performers set 15 number task requesters set 20.
17. Setting p higher would result fewer bids/asks per agent.

37

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

average number generated valuations instance 88 average number bids
65. Finally, number runs experiments 300. experiments performed
Xeon dual processor machine 3Ghz CPUs, 2 GB RAM commercial software employed
solve Integer Program ILOG CPLEX 9.1.

35

Clock time find optimal solution (seconds)

30

25

20

15

10

5

0

0

0.2

0.4

0.6

0.8
1
1.2
No. possible allocations

1.4

1.6

1.8

2
5

x 10

Figure 2: Performance IP solution.

results shown Figure 2. Specifically, x-axis represents number allocations
given problem instance y-axis represents time seconds elapsed solving
corresponding problem instance. Notice dependence difficulty problem
number allocations quite clear. Moreover, seen, possible solve problem
less 2 105 variables within 40 seconds. important note performance
solver used critical case future advancements Mixed Integer Programming (MIP)
solvers CPU clock speeds improve results.
Given results since provide general formula (see Equation (30)) compute
priori number generated allocations, possible estimate feasibility general
problem performing it. means system designer ask task requesters
performers constrain number tasks ask number bids issue come
input solved program reasonable time. important,
however, design special purpose algorithms deal larger inputs left
future work.

References
Archer, A., Papadimitriou, C., Talwar, K., & Tardos, E. (2003). approximate truthful mechanism
combinatorial auctions single parameter agent. Internet Mathematics, 1(2), 129150.
38

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

Arrow, K. J. (1979). property rights doctrine demand revelation incomplete information. Boskin, M. (Ed.), Economics Human Welfare. Academic Press.
Berge, C. (1973). Graphs Hypergraphs. North-Holland Publishing Company.
Byde, A. (2006). comparison mechanisms sequential compute resource auctions.
Proceedings Fifth International Joint Conference Autonomous Agents MultiAgent systems (AAMAS-06), pp. 11991201. ACM Press.
Cerquides, J., Endriss, U., Giovannucci, A., & Rodrguez-Aguilar, J. A. (2007). Bidding languages
winner determination mixed multi-unit combinatorial auctions. Proceedings
Twentieth International Joint Conference Artificial Intelligence, pp. 12211226.
Dasgupta, P. (1998). Trust commodity. Gambetta, D. (Ed.), Trust: Making Breaking
Cooperative Relations, pp. 4972. Blackwell.
Dash, R. K., Parkes, D. C., & Jennings, N. R. (2003). Computational mechanism design: call
arms. IEEE Intelligent Systems, 18(6), 4047.
Dash, R. K., Ramchurn, S. D., & Jennings, N. R. (2004). Trust-based mechanism design.
Proceedings Third International Joint Conference Autonomous Agents MultiAgent Systems (AAMAS-04), Vol. 2, pp. 726753.
dAspremont, C., & Gerard-Varet, L. A. (1979). Incentives incomplete information. Journal
Public Economics, 11(1), 2545.
Dellarocas, C. (2002). Goodwill hunting: economically efficient online feedback mechanism
environments variable product quality. Proceedings (AAMAS-02) Workshop
Agent-Mediated Electronic Commerce, pp. 238252.
Engel, Y., Wellman, M. P., & Lochner, K. (2006). Bid expressiveness clearing algorithms
multi-attribute double auctions. Proceedings Seventh ACM Conference Electronic
Commerce (EC-06), pp. 110119.
Fullam, K., Klos, T., Muller, G., Sabater, J., Topol, Z., Barber, K. S., Rosenschein, J., & Vercouter,
L. (2005). agent reputation trust (ART) testbed architecture. Proceedings
(AAMAS-05) Workshop Trust Agent Societies, pp. 5062.
Hershberger, J., & Suri, S. (2001). Vickrey pricing network routing: Fast payment computation.
Proceedings Forty-Second IEEE Symposium Foundations Computer Science,
pp. 252259.
Jehiel, P., & Moldovanu, B. (2001). Efficient design interdependent valuations. Econometrica,
69(5), 123759.
Jennings, N. R., Faratin, P., Norman, T. J., OBrien, P., Odgers, B., & Alty, J. L. (2000). Implementing business process management system using adept: real-world case study. International Journal Applied Artificial Intelligence, 14(5), 421465.
Jurca, R., & Faltings, B. (2003). incentive compatible reputation mechanism. Proceedings
IEEE Conference E-Commerce (CEC-03), pp. 285292.
Jurca, R., & Faltings, B. (2006). Minimum payments reward honest reputation feedback.
Proceedings Seventh ACM conference Electronic commerce (EC-06), pp. 190199.
39

fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS

Jurca, R., & Faltings, B. (2007). Obtaining reliable feedback sanctioning reputation mechanisms.
Journal Artificial Intelligence Research (JAIR), 29, 391419.
Kalagnanam, J., Davenport, A. J., & Lee, H. S. (2000). Computational aspects clearing continuous double auctions assignment constraints indivisible demand. Tech. rep., IBM
Research RC21660(97613).
Kalagnanam, J., & Parkes, D. C. (2004). Auctions, bidding exchange design. Simchi-Levi,
D., Wu, S. D., & Shen, M. (Eds.), Handbook Quantitative Supply Chain Analysis: Modeling
E-Business Era, International Series Operations Research Management Science,
chap. 5. Kluwer.
Krishna, V. (2002). Auction Theory. Academic Press.
MasColell, A., Whinston, M., & Green, J. (1995). Microeconomic Theory. Oxford University Press.
Mezzetti, C. (2004). Mechanism design interdependent valuations: Efficiency. Econometrica,
72(5), 16171626.
Mezzetti, C. (2007). Mechanism design interdependent valuations: Surplus extraction. Economic Theory, 31(3), 473488.
Miller, N., Resnick, P., & Zeckhauser, R. (2005). Eliciting honest feedback: peer prediction
method. Management Science, 51(9), 13591373.
Milton, J., & Arnold, J. C. (1998). Introduction Probability Statistics. Principles Applications Engineering Computing Sciences. McGraw-Hill Inc.
Nisan, N. (2006). Bidding languages combinatorial auctions. Cramton, P., Shoham, Y., &
Steinberg, R. (Eds.), Combinatorial Auctions, pp. 215231. MIT Press.
Nisan, N., & Ronen, A. (2007). Computationally feasible VCG mechanisms. Journal Artificial
Intelligence Research (JAIR), 29, 1947.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial optimization: algorithms complexity. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance vickreybased payment schemes exchanges. Proceedings Seventeenth International Joint
Conference Artificial Intelligence (IJCAI-01), pp. 11611168.
Parkes, D. C., & Ungar, L. H. (2000). Preventing strategic manipulation iterative auctions: Proxy
agents price-adjustment. Proceedings Seventeenth National Conference Artificial Intelligence Twelfth Conference Innovative Applications Artificial Intelligence, pp. 8289.
Porter, R., Ronen, A., Shoham, Y., & Tennenholtz, M. (2008). Fault tolerant mechanism design.
Artificial Intelligence, 172(15), 17831799.
Ramchurn, S. D., Huynh, D., & Jennings, N. R. (2004). Trust multi-agent systems. Knowledge Engineering Review, 19, 125.
Sandholm, T. (2007). Expressive commerce application sourcing: conducted 35
billion generalized combinatorial auctions. AI Magazine, 28(3), 4558.
40

fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION

Sandholm, T., Suri, S., Gilpin, A., & Levine, D. (2002). Winner determination combinatorial
auction generalizations. Proceedings First International Joint Conference Autonomous Agents Multi-Agent Systems (AAMAS-02), pp. 6976.
Sandholm, T. W. (1993). implementation contract net protocol based marginal cost
calculations. Proceedings Twelfth International Workshop Distributed Artificial
Intelligence, pp. 295308.
Teacy, W. T. L., Patel, J., Jennings, N. R., & Luck, M. (2006). Travos: Trust reputation
context inaccurate information sources. Autonomous Agents Multi-Agent Systems,
12(2), 183198.
Walsh, W., & Wellman, M. (1998). market protocol decentralized task allocation. Proceedings Third International Conference Multi-Agent Systems (ICMAS-98).

41

fiJournal Artificial Intelligence Research 35 (2009) 235-274

Submitted 08/08; published 06/09

Bilinear Programming Approach Multiagent Planning
Marek Petrik

petrik@cs.umass.edu

Shlomo Zilberstein

shlomo@cs.umass.edu

Department Computer Science
University Massachusetts, Amherst, 01003, USA

Abstract
Multiagent planning coordination problems common known computationally hard. show wide range two-agent problems formulated
bilinear programs. present successive approximation algorithm significantly
outperforms coverage set algorithm, state-of-the-art method class
multiagent problems. algorithm formulated bilinear programs,
general simpler implement. new algorithm terminated time
andunlike coverage set algorithmit facilitates derivation useful online performance bound. also much efficient, average reducing computation time
optimal solution four orders magnitude. Finally, introduce automatic dimensionality reduction method improves effectiveness algorithm,
extending applicability new domains providing new way analyze subclass
bilinear programs.

1. Introduction
present new approach solving range multiagent planning coordination
problems using bilinear programming. problems focus represent various extensions Markov decision process (MDP) multiagent settings. success MDP
algorithms planning learning uncertainty motivated researchers extend
model cooperative multiagent problems. One possibility assume
agents share information underlying state. results multiagent
Markov decision process (Boutilier, 1999), essentially MDP factored
action set. complex alternative allow partial sharing information
among agents. settings, several agentseach different partial information
worldmust cooperate order achieve joint objective. problems common practice modeled decentralized partially
observable MDPs (DEC-POMDPs) (Bernstein, Zilberstein, & Immerman, 2000). refinements model studied, example making certain independence
assumptions (Becker, Zilberstein, & Lesser, 2003) adding explicit communication
actions (Goldman & Zilberstein, 2008). DEC-POMDPs closely related extensive
games (Rubinstein, 1997). fact, DEC-POMDP represents exponentially larger
extensive game common objective. Unfortunately, DEC-POMDPs two
agents intractable general, unlike MDPs solved polynomial time.
Despite recent progress solving DEC-POMDPs, even state-of-the-art algorithms
generally limited small problems (Seuken & Zilberstein, 2008). motivated
development algorithms either solve restricted class problems (Becker,
c
2009
AI Access Foundation. rights reserved.

fiPetrik & Zilberstein

Lesser, & Zilberstein, 2004; Kim, Nair, Varakantham, Tambe, & Yokoo, 2006) provide
approximate solutions (Emery-Montemerlo, Gordon, Schneider, & Thrun, 2004; Nair,
Roth, Yokoo, & Tambe, 2004; Seuken & Zilberstein, 2007). paper, introduce
efficient algorithm several restricted classes, notably decentralized MDPs
transition observation independence (Becker et al., 2003). sake simplicity,
denote model DEC-MDP, although usually used denote model
without independence assumptions. objective problems maximize
cumulative reward set cooperative agents finite horizon. agent
viewed single decision-maker operating local MDP. complicates
problem fact MDPs linked common reward function
depends states.
coverage set algorithm (CSA) first optimal algorithm solve efficiently
transition observation independent DEC-MDPs (Becker, Zilberstein, Lesser, & Goldman, 2004). exploiting fact interaction agents limited
compared individual local problems, CSA solve problems cannot solved
general exact DEC-POMDP algorithms. also exhibits good anytime behavior. However, anytime behavior limited applicability solution quality
known hindsight, algorithm terminates.
develop new approach solve DEC-MDPsas well range multiagent
planning problemsby representing bilinear programs. also present efficient
new algorithm solving kinds separable bilinear problems. algorithm
applied DEC-MDPs, improves efficiency several orders magnitude compared
previous state-of art algorithms (Becker, 2006; Petrik & Zilberstein, 2007a).
addition, algorithm provides useful runtime bounds approximation error,
makes useful anytime algorithm. Finally, algorithm formulated
general separable bilinear programs therefore easily applied range
problems.
rest paper organized follows. First, Section 2, describe basic
bilinear program formulation range multiagent planning problems
expressed within framework. Section 3, describe new successive approximation
algorithm bilinear programs. performance algorithm depends heavily
number interactions agents. address that, propose Section 4
method automatically reduces number interactions provides bound
degradation solution quality. Furthermore, able project computational
effort required solve given problem instance, develop offline approximation bounds
Section 5. Section 6, examine performance approach standard
benchmark problem. conclude summary results discussion future
work could improve performance approach.

2. Formulating Multiagent Planning Problems Bilinear Programs
begin formal description bilinear programs different types multiagent
planning problems formulated such. addition multiagent planning
problems, bilinear programs used solve variety problems robotic
manipulation (Pang, Trinkle, & Lo, 1996), bilinear separation (Bennett & Mangasarian,
236

fiA Bilinear Programming Approach Multiagent Planning

1992), even general linear complementarity problems (Mangasarian, 1995). focus
multiagent planning problems formulation turns particularly effective.
Definition 1. separable bilinear program normal form defined follows:
maximize
w,x,y,z





f (w, x, y, z) = sT
1 w + r1 x + x Cy + r2 + s2 z

subject A1 x + B1 w = b1

(1)

A2 + B2 z = b2
w, x, y, z 0

size program total number variables w, x, z. number
variables determines dimensionality program1 .
Unless otherwise specified, vectors column vectors. use boldface 0 1
denote vectors zeros ones respectively appropriate dimensions. program specifies two linear programs connected nonlinear objective
function term xT Cy. program contains two types variables. first type includes
variables x, appear bilinear term objective function. second
type includes additional variables w, z appear bilinear term.
show later, distinction important complexity algorithm propose
depends mostly dimensionality problem, number variables
involved bilinear term.
bilinear program Eq. (1) separable constraints x w
independent constraints z. is, variables participate
bilinear term objective function independently constrained. theory nonseparable bilinear programs much complicated corresponding algorithms
efficient (Horst & Tuy, 1996). Thus, limit discussion paper
separable bilinear programs often omit term separable. discussed later
detail, separable bilinear program may seen concave minimization problem
multiple local minima. shown solving problem NP-complete,
compared polynomial time complexity linear programs.
addition formulation bilinear program shown Eq. (1), also use
following formulation, stated terms inequalities:
maximize
x,y

xT Cy

subject A1 x b1

x0

A2 b2

y0

(2)

latter formulation easily transformed normal form using standard
transformations linear programs (Vanderbei, 2001). particular, introduce slack
1. possible define dimensionality terms x, minimum dimensions x y.
issue discussed Appendix B.

237

fiPetrik & Zilberstein

variables w, z obtain following identical bilinear program normal form:
xT Cy

maximize
w,x,y,z

subject A1 x w = b1
A2 z = b2

(3)

w, x, y, z 0
use following matrix block matrix notation paper. Matrices
denoted square brackets, columns separated commas rows separated
semicolons. Columns haveprecedence
rows. example, notation [A, B; C, D]

corresponds matrix


C

B
.


show later, presence variables w, z objective function may prevent
crucial function convex. Since unfavorable impact properties
bilinear program, introduce compact form problem.
Definition 2. say bilinear program Eq. (1) compact form s1
s2 zero vectors. semi-compact form s2 zero vector.
compactness requirement limiting bilinear program form
shown Eq. (1) expressed semi-compact form follows:
maximize
w,x,y,z,x,y

sT
1w

+

r1T x

+

xT

subject A1 x + B1 w = b1
x = 1 =



C 0

x
+ r2T

0 1
A2 + B2 z = b2

(4)

sT
2z

w, x, y, z 0
Clearly, feasible solutions Eq. (1) Eq. (4) objective value set
appropriately. Notice dimensionality bilinear term objective function
increases 1 x y. Hence, transformation increases dimensionality
program 1.
rest section describes several classes multiagent planning problems
formulated bilinear programs. Starting observation transition independent
DEC-MDPs, extend formulation allow different objective function (maximizing
average reward infinite horizon), handle interdependent observations, find
Nash equilibria competitive settings.
2.1 DEC-MDPs
mentioned previously, transition-independent observation-independent DECMDP (Becker et al., 2004) may formulated bilinear program. Intuitively, DECMDP transition independent agent influence agents transitions.
DEC-MDP observation independent agent observe states agents.
assumptions crucial since ensure lower complexity problem (Becker
238

fiA Bilinear Programming Approach Multiagent Planning

et al., 2004). remainder paper, use simply term DEC-MDP refer
transition observation independent DEC-MDP.
DEC-MDP model proved useful several multiagent planning domains. One
example use Mars rover planning problem (Bresina, Golden, Smith, & Washington, 1999), first formulated DEC-MDP Becker et al. (2003). domain
involves two autonomous rovers visit several sites given order may decide
perform certain scientific experiments site. overall activity must completed
within given time limit. uncertainty duration experiment modeled given discrete distribution. rovers operate independently receive
local rewards completed experiment, global reward function also depends
experiments completed rovers. interaction rovers thus
limited relatively small number overlapping tasks. return problem
describe detail Section 6.
DEC-MDP problem composed two MDPs state-sets S1 , S2 action sets
A1 , A2 . functions r1 r2 define local rewards action-state pairs. initial
state distributions 1 2 . MDPs coupled global reward function
defined matrix R. entry R(i, j) represents joint reward state-action
one agent j other. definition DEC-MDP based work
Becker et al. (2004), modifications discuss below.
Definition 3. two-agent transition observation independent DEC-MDP extended reward structure defined tuple hS, F, , A, P, Ri:
= (S1 , S2 ) factored set world states
F = (F1 S1 , F2 S2 ) factored set terminal states.
= (1 , 2 ) : Si 7 [0, 1] initial state distribution functions
= (A1 , A2 ) factored set actions
P = (P1 , P2 ), Pi : Si Ai Si 7 [0, 1] transition functions. Let Ai
action, Pia : Si Si 7 [0, 1] stochastic transition matrix
Pi (s, a, s0 ) = Pia (s, s0 ) probability transition state Si state
s0 Si agent i, assuming takes action a. transitions final states
0 probability; Pi (s, a, s0 ) = 0 Fi , s0 Si , Ai .
R = (r1 , r2 , R) ri : Si Ai 7 R local reward functions
R : (S1 A1 ) (S2 A2 ) 7 R global reward function. Local rewards ri
represented vectors, R matrix (s1 , a1 ) rows (s2 , a2 ) columns.
Definition 3 differs original definition transition observation independent DEC-MDP (Becker et al. 2004, Definition 1) two ways. modifications allow
us explicitly capture assumptions implicit previous work. First, individual MDPs model formulated stochastic shortest-path problems (Bertsekas &
Tsitsiklis, 1996). is, explicit time horizon, instead states
terminal. process stops upon reaching terminal state. objective maximize
cumulative reward received reaching terminal states.
second modification original definition Definition 3 generalizes
reward structure DEC-MDP formulation, using extended reward structure.
joint rewards original DEC-MDP defined joint states (s1 S1 , s2 S2 )
239

fiPetrik & Zilberstein

s11

s21

s12

s22

s13

s23

s31

t1

s1

s2

s3
t2

s32

t3

s33

Figure 1: MDP stochastic shortest path version time horizon 3. dotted
circles terminal states.

visited agents simultaneously. is, agent 1 visits states s11 , s12 agent 2
visits states s21 , s22 , reward defined joint states (s11 , s21 ) (s12 , s22 ).
However, DEC-MDP formulation extended reward structure also allows reward
depend (s11 , s22 ) (s12 , s21 ), even visited simultaneously. result,
global reward may depend history, current state. Note
reward structure general commonly used DEC-POMDPs.
prefer general definition already implicitly used
previous work. particular, extended reward structure arises introducing
primitive compound events work Becker et al. (2004). reward structure
necessary capture characteristics Mars rover benchmark. Interestingly,
extension complicate proposed solution methods way. Note
stochastic shortest path formulation (right side Figure 1) inherently eliminates
loops time always advances action taken. Therefore, every state
representation may visited once. property commonly used MDP
formulated linear program (Puterman, 2005).
solution DEC-MDP deterministic stationary policy = (1 , 2 ),
: Si 7 Ai standard MDP policy (Puterman, 2005) agent i. particular, (si )
represents action taken agent state si . define bilinear program, use
variables x(s1 , a1 ) denote probability agent 1 visits state s1 takes action a1
y(s2 , a2 ) denote agent 2. standard dual variables MDP
formulation. Given solution terms x agent 1, policy calculated S1
follows, breaking ties arbitrarily.
1 (s) = arg max x(s, a)
aA1

policy 2 similarly calculated y. correctness policy calculation follows
existence optimal policy deterministic depends local
states agent (Becker et al., 2004).
objective DEC-MDPs terms x maximize:
X
X
X X
r1 (s1 , a1 )x(s1 , a1 ) +
R(s1 , a1 , s2 , a2 )x(s1 , a1 )y(s2 , a2 ) +
r2 (s2 , a2 )y(s2 , a2 ).
s1 S1
a1 A1

s1 S1 s2 S2
a1 A1 a2 A2

s2 S2
a2 A2

stochastic shortest path representation general finite-horizon MDP
represented keeping track time part state, illustrated
240

fiA Bilinear Programming Approach Multiagent Planning

s11

s21

s12

r1

s13

s22

s14

s23
s24

s25

Figure 2: sample DEC-MDP.

Figure 1. modification allows us apply model directly Mars rover
benchmark problem. Actions Mars rover problem may different durations,
actions finite-horizon MDPs take amount time.
DEC-MDP problem extended reward structure formulated bilinear mathematical program follows. Vector variables x represent state-action
probabilities agent, used dual linear formulation MDPs. Given transition observation independence, feasible regions may defined linear equalities
A1 x = 1 x 0, A2 = 2 0. matrices A1 A2
dual formulation total expected reward MDPs (Puterman, 2005), representing
following equalities agent i:
X
a0 Ai

X X

x(s0 , a0 )

Pi (s, a, s0 )x(s, a) = (s0 ),

sSi aAi

every s0 Si . described above, variables x(s, a) represent probabilities visiting
state taking action appropriate agent plan execution. Note
agent 2, variables y(s, a) rather x(s, a). Intuitively, equalities ensure
probability entering non-terminal state, either initial step
states, probability leaving state. bilinear problem
formulated follows:
maximize
x,y

r1T x + xT Ry + r2T

subject A1 x = 1

x0

A2 = 2

y0

(5)

formulation, treat initial state distributions vectors, based fixed
ordering states. following simple example illustrates formulation.
Example 4. Consider DEC-MDP two agents, depicted Figure 2. transitions
problem deterministic, thus branches represent actions ai , ordered
state left right. states, one action available. shared reward
r1 , denoted dotted line, received agents visit state. local rewards
denoted numbers next states. terminal states omitted.
241

fiPetrik & Zilberstein

agents start states s11 s21 respectively. bilinear formulation problem is:
maximize
subject

x(s14 , a1 ) r1 y(s24 , a1 )
x(s11 , a1 )
1
1
x(s2 , a1 ) + x(s2 , a2 ) x(s11 , a1 )
x(s13 , a1 ) x(s12 , a1 )
x(s14 , a1 ) x(s12 , a2 )

=1
=0
=0
=0

y(s21 , a1 ) + y(s21 , a2 )
y(s22 , a1 ) y(s21 , a1 )
y(s23 , a1 ) y(s22 , a1 )
y(s24 , a1 ) y(s22 , a1 )
y(s25 , a1 ) y(s23 , a1 )

=1
=0
=0
=0
=0

results paper focus two-agent problems, approach extended DEC-MDPs two agents two ways. first approach requires
component global reward depends two agents. DEC-MDP
may viewed graph vertices representing agents edges representing
immediate interactions dependencies. formulate problem bilinear program, graph must bipartite. Interestingly, class problems previously
formulated (Kim et al., 2006). Let G1 G2 indices agents two
partitions bipartite graph. problem formulated follows:
X

maximize
riT xi + xT
Rij yj + rj yj
x,y

iG1 ,jG2

subject Ai xi = 1

x 0 G1

Aj yj = 2

yj 0 j G2

(6)

Here, Rij denotes global reward interactions agents j. program bilinear separable constraints variables G1 G2
independent.
second approach generalize framework represent DEC-MDP
multilinear program. case, restrictions reward structure necessary.
algorithm solve, say trilinear program, could almost identical algorithm
propose, except best response would calculated using bilinear, linear
programs. However, scalability approach agents doubtful.
2.2 Average-Reward Infinite-Horizon DEC-MDPs
previous formulation deals finite-horizon DEC-MDPs. average-reward problem may also formulated bilinear program (Petrik & Zilberstein, 2007b).
particularly useful infinite-horizon DEC-MDPs. example, consider infinitehorizon version Multiple Access Broadcast Channel (MABC) (Rosberg, 1983; Ooi &
Wornell, 1996). problem, used widely recent studies decentralized decision making, two communication devices share single channel, need
periodically transmit data. However, channel transmit single message
time. agents send messages time, leads collision,
transmission fails. memory devices limited, thus need send
messages sooner rather later. adapt model work Rosberg (1983),
particularly suitable assumes sharing local information among
devices.
242

fiA Bilinear Programming Approach Multiagent Planning

definition average-reward two-agent transition observation independent DECMDP Definition 3, exception terminal states, policy, objective. terminal states average-reward DEC-MDPs, policy (1 , 2 )
may stochastic. is, (s, a) 7 [0, 1] probability agent taking action
state s. objective find stationary infinite-horizon policy maximizes
average reward, gain, defined follows.
Definition 5. Let = (1 , 2 ) stochastic policy, Xt Yt random variables
represent probability distributions state-action pairs time two
agents respectively according . gain G policy defined states
s1 S1 s2 S2 as:
"N 1
#
X
1
G(s , ) = lim
E(s1 ,1 (s1 )),(s2 ,2 (s2 ))
r1 (Xt ) + R(Xt , Yt ) + r2 (Yt ) ,
N N
1

2

t=0

(si ) distribution actions state si . Note expectation
respect initial states action distributions (s1 , 1 (s1 )), (s2 , 2 (s2 )).
actual gain policy depends agents initial state distributions 1 , 2
may expressed 1T G2 , G represented matrix. Puterman (2005),
example, provides detailed discussion definition meaning policy gain.
simplify bilinear formulation average-reward DEC-MDP, assume
r1 = 0 r2 = 0. bilinear program follows.
maximize
p1 ,p2 ,q1 ,q2

(p1 , p2 , q1 , q2 ) = pT
1 Rp2

subject pX
1 , p2 0
s0 S1
p1 (s0 , a)
s0 S1
s0 S2
s0 S2

aA
X1
aA
X1
aA
X2
aA2

p1 (s0 , a) +
p2 (s0 , a)
p2 (s0 , a) +

X


p1 (s, a)P1a s, s0 = 0

sS
X1 ,aA1

q1 (s0 , a)

aAX
1

X


q1 (s, a)P1a s, s0 = 1 (s0 )

sS1 ,aA1

p2 (s, a)P2a s, s0

sS
X2 ,aA2

q2 (s0 , a)

aA2

X

(7)

=0


q2 (s, a)P2a s, s0 = 2 (s0 )

sS2 ,aA2

variables program come dual formulation average-reward MDP
linear program (Puterman, 2005). state sets MDPs divided recurrent
transient states. recurrent states expected visited infinitely many times,
transient states expected visited finitely many times. Variables p1
p2 represent limiting distributions MDP, non-zero recurrent
states. (possibly stochastic) policy agent defined recurrent states
probability taking action Ai state Si :
pi (s, a)
.
0
a0 Ai pi (s, )

(s, a) = P

243

fiPetrik & Zilberstein

a11

a12

b12

b11

b11

b12

a21

a22

a21

a22

a23

a24

a23

a24

r1

r2

r3

r4

r5

r1

r7

r8

Figure 3: tree form policy DEC-POMDP extensive game. dotted
ellipses denote information sets.

variables pi 0 transient states. policy transient states calculated
variables qi as:
qi (s, a)
(s, a) = P
.
0
a0 Ai qi (s, )
correctness constraints follows dual formulation optimal average
reward (Puterman 2005, Equation 9.3.4). Petrik Zilberstein (2007b) provide
details formulation.
2.3 General DEC-POMDPs Extensive Games
general DEC-POMDP problem extensive-form games two agents, players,
also formulated bilinear programs. However, constraints may separable
actions one agent influence agent. approach case may
similar linear complementarity problem formulation extensive games (Koller, Megiddo,
& von Stengel, 1994), integer linear program formulation DEC-POMDPs (Aras
& Charpillet, 2007). approach develop closely related event-driven DECPOMDPs (Becker et al., 2004), general efficient. Nevertheless, size
bilinear program exponential size DEC-POMDP. expected
since solving DEC-POMDPs NEXP-complete (Bernstein et al., 2000), solving
bilinear programs NP-complete (Mangasarian, 1995). general formulation
case somewhat cumbersome, illustrate using following simple example.
Aras (2008) provides details similar construction.
Example 6. Consider problem depicted Figure 3, assuming agents
cooperative. actions agent observable, denoted information
sets. approach generalized problem observable sets long
perfect recall condition satisfied. Agents satisfy perfect recall condition
remember set actions taken prior moves (Osborne & Rubinstein, 1994).
Rewards collected leaf-nodes case. variables edges represent
probability taking action. Here, variables denote actions one agent,
244

fiA Bilinear Programming Approach Multiagent Planning

variables b other. total common reward received end is:
r = a11 b11 a21 r1 + a11 b11 a22 r2 + a11 b12 a21 r3 + a11 b12 a22 r4 +
a12 b11 a23 r5 + a12 b11 a24 r6 + a12 b12 a23 r7 + a12 b12 a24 r8 .
constraints problem following form: a11 + a12 = 1.
DEC-POMDP problem represented using approach used above.
also straightforward extend approach problems rewards every node. However, formulation clearly bilinear. apply algorithm class
problems, need reformulate problem bilinear form. easily
accomplished way similar construction dual linear program MDP.
Namely, introduce variables:
c11 = a11
c12 = a12
c21 = a11 a21
c22 = a11 a22
every set variables path leaf node. Then, objective may
reformulated follows:
r = c21 b11 r1 + c22 b11 r2 + c23 b12 r3 + c24 b12 r4 +
c25 b11 r5 + c26 b11 r6 + c27 b12 r7 + c28 b12 r8 .
Variables bij replaced fashion. objective function clearly bilinear.
constraints may reformulated follows. constraint a21 + a22 = 1
multiplied a11 replaced c21 + c22 = c11 , on. is, variables
level sum variable least common parent level
agent.
2.4 General Two-Player Games
addition cooperative problems, competitive problems 2 players may
formulated bilinear programs. known problem finding equilibrium
bi-matrix game may formulated linear complementarity problem (Cottle, Pang,
& Stone, 1992). also shown linear complementarity problem may
formulated bilinear problem (Mangasarian, 1995). However, direct application
two reductions results complex problem large dimensionality. Below,
demonstrate general game directly formulated bilinear program.
many ways formulate game, thus take general approach. simply
assume agent optimizes linear program, follows.
maximize
x

maximize

d1 (x) = r1T x + xT C1

subject A1 x = b1



(8)

d2 (y) = r2T + xT C2

subject A2 = b2
y0

x0
245

(9)

fiPetrik & Zilberstein

Eq. (8), variable considered constant similarly Eq. (9)
variable x considered constant. normal form games, constraint matrices
A1 A2 simply rows ones, b1 = b2 = 1. competitive DEC-MDPs,
constraint matrices A1 A2 Section 2.1. Extensive games may
formulated similarly DEC-POMDPs, described Section 2.3.
game specified linear programs Eq. (8) Eq. (9) may formulated
bilinear program follows. First, define reward vectors agent, given policy
agent.
q1 (y) = r1 + C1
q2 (x) = r2 + C2T x.

values unrelated Eq. (7). complementary slackness values (Vanderbei, 2001) linear programs Eq. (8) Eq. (9) are:


k1 (x, y, 1 ) = q1 (y)T
1 A1 x


k2 (x, y, 2 ) = q2 (x)T

2 2 y,
1 2 dual variables corresponding linear programs.
primal feasible x y, dual feasible 1 2 , k1 (x, y, 1 ) 0
k2 (x, y, 2 ) 0. equality attained x optimal.
used write following optimization problem, implicitly assume
x,y,1 ,2 feasible appropriate primal dual linear programs:
0
=
=
=
=

min k1 (x, y, 1 ) + k2 (x, y, 2 )

x,y,1 ,2



min (q1 (y)T
1 A1 )x + (q2 (x) 2 A2 )y

x,y,1 ,2



min ((r1 + C1 y)T
1 A1 )x + ((r2 + C2 x) 2 A2 )y

x,y,1 ,2


min r1T x + r2T + xT (C1 + C2 )y xT
1 1 A2 2

x,y,1 ,2


min r1T x + r2T + xT (C1 + C2 )y bT
1 1 b2 2 .

x,y,1 ,2

Therefore, feasible x set right hand side 0 solve linear programs
Eq. (8) Eq. (9) optimally. Adding primal dual feasibility conditions
above, get following bilinear program:
minimize
x,y,1 ,2


r1T x + r2T + xT (C1 + C2 )y bT
1 1 b2 2

subject A1 x = b1

A2 = b2

r1 + C1
1 1 0
r2 + C2T x
2 2 0
x0

y0
246

(10)

fiA Bilinear Programming Approach Multiagent Planning

Algorithm 1: IterativeBestResponse(B)
x0 , w0 rand ;
i1;
yi1 6= yi xi1 6= xi
(yi , zi ) arg maxy,z f (wi1 , xi1 , y, z) ;
(xi , wi ) arg maxx,w f (w, x, yi , zi ) ;
6
ii+1

1
2
3
4
5

7

return f (wi , xi , yi , zi )

optimal solution Eq. (10) 0 corresponds Nash equilibrium.
primal variables x, dual variables 1 , 2 feasible
complementary slackness condition satisfied. open question example
interpretation approximate result formulation would select equilibrium.
clear yet whether possible formulate program optimal solution
Nash equilibrium maximizes certain criterion. approximate solutions
program probably correspond -Nash equilibria, remain open question.
algorithm case also relies number shared rewards small
compared size problem. even case, often possible
number shared rewards may automatically reduced described Section 4.
fact, easy show zero-sum normal form game automatically reduced
two uncoupled linear programs. follows dimensionality reduction procedure
Section 4.

3. Solving Bilinear Programs
One simple method often used solving bilinear programs iterative procedure shown
Algorithm 1. parameter B represents bilinear program. algorithm
often performs well practice, tends converge suboptimal solution (Mangasarian,
1995). applied DEC-MDPs, algorithm essentially identical JESP (Nair,
Tambe, Yokoo, Pynadath, & Marsella, 2003)one early solution methods.
following, use f (w, x, y, z) denote objective value Eq. (1).
rest section presents new anytime algorithm solving bilinear programs.
goal algorithm produce good solution quickly improve
solution remaining time. Along approximate solution, maximal approximation bound respect optimal solution provided. show below,
algorithm benefit results produced suboptimal algorithms, Algorithm 1,
quickly determine tight approximation bounds.
3.1 Successive Approximation Algorithm
begin overview successive approximation algorithm bilinear problems
takes advantage low number interactions agents. particularly
suitable input problem large comparison dimensionality, defined
Section 2. address issue dimensionality reduction Section 4.
247

fiPetrik & Zilberstein

begin simple intuitive explanation algorithm, show
formalized. bilinear program seen optimization game played
two agents, first agent sets variables w, x second one sets
variables y, z. general observation applies bilinear program.
practical application, feasible sets two sets variables may large
explore exhaustively. fact, method applied DEC-MDPs, sets
infinite continuous. basic idea algorithm first identify set best
responses one agents, say agent 1, policy agent.
simple variables agent 2 fixed, program becomes linear,
relatively easy solve. set best-response policies agent 1 identified,
assuming reasonable size, possible calculate best response agent 2.
general approach also used coverage set algorithm (Becker et al., 2004).
One distinction representation used CSA applies DEC-MDPs,
formulation applies bilinear programsa general representation. main
distinction algorithm CSA way variables y, z chosen.
CSA, values y, z calculated way simply guarantees termination
finite time. We, hand, choose values y, z greedily minimize
approximation bound optimal solution. possible establish bounds
optimality solution throughout calculation. result, algorithm
converges rapidly may terminated time guaranteed performance
bound. Unlike earlier version algorithm (Petrik & Zilberstein, 2007a), version
described paper calculates best response using subset values y, z.
show, possible identify regions y, z impossible improve
current best solution exclude regions consideration.
formalize ideas described above. simplify notation, define feasible
sets follows:
X = {(x, w) A1 x + B1 w = b1 }


= {(y, z) A2 + B2 z = b2 }.

use denote exists z (y, z) . addition, assume
problem semi-compact form. reasonable bilinear program
may converted semi-compact form increase dimensionality one,
shown earlier.
Assumption 7. sets X bounded, is, contained ball
finite radius.
Assumption 7 limiting, coordination problems uncertainty typically
bounded feasible sets variables correspond probabilities bounded [0, 1].
Assumption 8. bilinear program semi-compact form.
main idea algorithm compute set X X contains
elements satisfy necessary optimality condition. set X formally defined
follows:





X (x , w ) (y, z) f (w , x , y, z) = max f (w, x, y, z) .
(x,w)X

248

fiA Bilinear Programming Approach Multiagent Planning

described above, set may seen set best responses one agent
variable settings other. best responses easy calculate since bilinear
program Eq. (1) reduces linear program fixed w, x fixed y, z. algorithm,
assume X potentially proper subset necessary optimality points
focus approximation error optimal solution. Given set X, following
simplified problem solved.
maximize
w,x,y,z

f (w, x, y, z)

subject (x, w) X

(11)

A2 + B2 z = b2
y, z 0

Unlike original continuous set X, reduced set X discrete small. Thus
elements X may enumerated. fixed w x, bilinear program Eq. (11)
reduces linear program.
help compute approximation bound guide selection elements
X, use best-response function g(y), defined follows:
g(y) =

max

{w,x,z (x,w)X,(y,z)Y }

f (w, x, y, z) =

max

{x,w (x,w)X}

f (w, x, y, 0),

second equality semi-compact programs feasible . Note
g(y) also defined
/ , case choice z arbitrary since
influence objective function. best-response function easy calculate using
linear program. crucial property function g use calculate
approximation bound convexity. following proposition holds g(y) =
max{x,w (x,w)X} f (w, x, y, 0) maximum finite set linear functions.
Proposition 9. function g(y) convex program semi-compact form.
Proposition 9 relies heavily separability Eq. (1), means constraints variables one side bilinear term independent variables
side. separability ensures w, x valid solutions regardless values
y, z. semi-compactness program necessary establish convexity, shown
Example 23 Appendix C. example constructed using properties described
appendix, show f (w, x, y, z) may expressed sum convex
concave function.
ready describe Algorithm 2, computes set X bilinear
problem B approximation error 0 . algorithm iteratively adds
best response (x, w) selected pivot point X. pivot points selected
hierarchically. iteration j, algorithm keeps set polyhedra S1 . . . Sj
represent triangulation feasible space , possible based Assumption 7.
polyhedron Si = (y1 . . . yn+1 ), algorithm keeps bound maximal
difference optimal solution polyhedron best solution found
far. error bound polyhedron Si defined as:
= e(Si ) =

max

{w,x,y|(x,w)X,ySi }

f (w, x, y, 0)

max
{w,x,y|(x,w)X,ySi }

249

f (w, x, y, 0),

fiPetrik & Zilberstein

Algorithm 2: BestResponseApprox(B, 0 ) returns (w, x, y, z)
1

2
3
4
5
6
7

8
9
10
11
12

13

14

// Create initial polyhedron S1 .
S1 (y1 . . . yn+1 ), S1 ;
// Add best-responses vertices S1 X
X {arg max(x,w)X f (w, x, y1 , 0), ..., arg max(x,w)X f (w, x, yn+1 , 0)} ;
// Calculate error pivot point initial polyhedron
(1 , 1 ) P olyhedronError(S1 ) ;
// Section 3.2,Section 3.3
// Initialize number polyhedra 1
j1;
// Continue reaching predefined precision 0
maxi=1,...,j 0
// Find polyhedron largest error
arg maxk=1,...,j k ;
// Select pivot point polyhedron largest error
;
// Add best response pivot point set X
X X {arg max(x,w)X f (w, x, y, 0)} ;
// Calculate errors pivot points refined polyhedra
k = 1, . . . , n + 1
j j+1 ;
// Replace k-th vertex pivot point
Sj (y, y1 . . . yk1 , yk+1 , . . . yn+1 ) ;
(j , j ) P olyhedronError(Sj ) ;
// Section 3.2,Section 3.3
// Take smaller errors original refined
polyhedron. error may increase refinement,
although bound may
j min{i , j } ;
// Set error refined polyhedron 0, since region
covered refinements
0 ;

(w, x, y, z) arg max{w,x,y,z
16 return (w, x, y, z) ;

15

(x,w)X,(y,z)Y }

f (w, x, y, 0) ;

X represents current, final, set best responses.
Next, point y0 selected described n + 1 new polyhedra created
replacing one vertices y0 get: (y0 , y2 , . . .), (y1 , y0 , y3 , . . .), . . . , (y1 , . . . , yn , y0 ).
depicted 2-dimensional set Figure 4. old polyhedron discarded
procedure repeatedly applied polyhedron maximal
approximation error.
sake clarity, pseudo-code Algorithm 2 simplified address
efficiency issues. practice, g(yi ) could cached, errors could stored
prioritized heap least sorted array. addition, lower bound li upper
bound ui calculated stored polyhedron Si = (y1 . . . yn+1 ). function e(Si )
calculates maximal difference polyhedron Si point attained.
error bound polyhedron Si may tight, describe Remark 13.
result, polyhedron Si refined n polyhedra S10 . . . Sn0 online error
250

fiA Bilinear Programming Approach Multiagent Planning

y3

y0
y1

y2

Figure 4: Refinement polyhedron two dimensions pivot y0 .
bounds 01 . . . 0n , possible k: 0k > . Since S10 . . . Sn0 Si , true error
Sk0 less Si therefore 0k may set .
Conceptually, algorithm similar CSA, important differences.
main difference choice pivot point y0 bounds g. CSA
keep upper bound evaluates g(y) intersection points planes defined
current solutions X. guarantees g(y) eventually known precisely
(Becker et al., 2004). similar approach also taken POMDPs (Cheng, 1988).
|X|
upper bound number intersection points CSA dim
. principal problem
bound exponential dimension , experiments show
slower growth typical problems. contrast, choose pivot points minimize
approximation error. selective tends rapidly reduce error
bound. addition, error pivot point may used determine overall
error bound. following proposition states soundness triangulation, proved
Appendix A. correctness triangulation establishes iteration
approximation error equivalent maximum approximation errors
current polyhedra S1 . . . Sj .
Proposition 10. proposed triangulation, sub-polyhedra overlap
cover whole feasible set , given pivot point interior S.

3.2 Online Error Bound
selection pivot point plays key role performance algorithm,
calculating error bound speed convergence optimal solution.
section show exactly use triangulation algorithm calculate error
bound. compute approximation bound, define approximate best-response
function g(y) as:
g(y) =
max
f (w, x, y, 0).
{x,w (x,w)X}

Notice z considered expression, since assume bilinear program semi-compact form. value best approximate solution
execution algorithm is:
max

f (w, x, y, 0) = max g(y).
yY

{w,x,y,z (x,w)X,yY }

251

fiPetrik & Zilberstein

value calculated runtime new element X added.
maximal approximation error current solution optimal one may
calculated approximation error best-response function g(), stated
following proposition.
Proposition 11. Consider bilinear program semi-compact form. let w, x,
optimal solution Eq. (11) let w , x , optimal solution Eq. (1).
approximation error bounded by:
f (w , x , , 0) f (w, x, y, 0) max (g(y) g(y)) .
yY

Proof.
f (w , x , , 0) f (w, x, y, 0) = max g(y) max g(y) max g(y) g(y)
yY

yY

yY

Now, approximation error maxyY g(y) g(y), bounded difference
upper bound lower bound g(y). Clearly, g(y) lower bound
g(y). Given points g(y) best-response function g(y), use
Jensens inequality obtain upper bound. summarized following lemma.
Lemma
12.Let yi = 1, . . . , n + 1 g(yi ) = g(yi ).
P
Pn+1
Pn+1
n+1
g
i=1 ci yi
i=1 ci g(yi )
i=1 ci = 1 ci 0 i.
actual implementation bound relies choice pivot points. Next
describe maximal error calculation single polyhedron defined = (y1 . . . yn ).
Let matrix yi columns, let L = {x1 . . . xn+1 } set best responses
vertices. matrix used convert absolute coordinates relative
representation convex combination vertices. defined formally
follows:


...
= = y1 y2 . . .
...
1 = 1T
0
yi column vectors.
represent lower bound l(y) g(y) upper bound u(y) g(y) as:
l(y) = max rT x + xT Cy
xL

u(y) = [g(y1 ), g(y2 ), . . .]T = [g(y1 ), g(y2 ), . . .]T




1T

1

,
1

upper bound correctness follows Lemma 12. Notice u(y) linear function,
enables us use linear program determine maximal-error point.
252

fiA Bilinear Programming Approach Multiagent Planning

Algorithm 3: PolyhedronError(B, S)
P one Eq. (12), (13), (14), (20) ;
optimal solution P ;
optimal objective value P ;
// Coordinates relative vertices S, convert absolute
values
4 Tt ;
5 return (, ) ;

1
2
3

Remark 13. Notice use L instead X calculating l(y). Using X would
lead tighter bound, easy show three-dimensional examples. However,
also would substantially increase computational complexity.
Now, error polyhedron may expressed as:
e(S) max u(y) l(y) = max u(y) max rT x + xT Cy
yS

yS



xL



= max min u(y) r x x Cy.
yS xL

also


= 0 1T = 1 .
result, point maximal error bound may determined using following
linear program terms variables t, :
maximize
t,



subject u(T t) rT x xT CT

x L

(12)

1T = 1 0
x variable. formulation correct feasible solutions
bounded maximal error maximal-error solution feasible.
Proposition 14. optimal solution Eq. (12) equivalent maxyS |u(y) l(y)|.
thus select next pivot point greedily minimize error. maximal difference actually achieved points planes meet, Becker et al. (2004)
suggested. However, checking intersections similar running simplex
algorithm. general, simplex algorithm preferable interior point methods
program small size (Vanderbei, 2001).
Algorithm 3 shows general way calculate maximal error pivot point
polyhedron S. algorithm may use basic formulation Eq. (12),
advanced formulations Eqs. (13), (14), (20) defined Section 3.3.
following section, describe refined pivot point selection method
cases dramatically improve performance.
253

fiPetrik & Zilberstein

20

15

h10
5
6

Yh

4

2

0

2

4

Yh

6

Figure 5: reduced set Yh needs considered pivot point selection.
3.3 Advanced Pivot Point Selection
described above, pivot points chosen greedily determine maximal
error polyhedron minimize approximation error. basic approach
described Section 3.1 may refined, goal approximate function
g(y) least error, find optimal solution. Intuitively, ignore
regions guarantee improvement current solution, illustrated
Figure 5. show below, search maximal error point could limited
region well.
first define set Yh search maximal error, given
optimal solution f h.
Yh = {y g(y) h, }.
next proposition states maximal error needs calculated superset
Yh .
Proposition 15. Let w, x, y, z approximate optimal solution w , x , , z
optimal solution. Also let f (w , x , , z ) h assume Yh Yh . approximation error bounded by:
f (w , x , , z ) f (w, x, y, z) max g(y) g(y).
yYh

Proof. First, f (w , x , , z ) = g(y ) h thus Yh . Then:
f (w , x , , z ) f (w, x, y, z) = max g(y) max g(y)
yYh

yY

max g(y) g(y)
yYh

max g(y) g(y)
yYh

Proposition 15 indicates point maximal error needs selected
set Yh . question easily identify Yh . set convex
general, tight approximation set needs found. particular, use methods
254

fiA Bilinear Programming Approach Multiagent Planning

approximate intersection superset Yh polyhedron
refined, using following methods:
1. Feasibility [Eq. (13)]: Require pivot points feasible .
2. Linear bound [Eq. (14)]: Use linear upper bound u(y) h.
3. Cutting plane [Eq. (20)]: Use linear inequalities define YhC ,
YhC = R|Y | \ Yh complement Yh .
combination methods also possible.
Feasibility first method simplest, also least constraining. linear
program find pivot point maximal error bound follows:
maximize
,t,y,z



subject u(T t) rT x + xT CT

x L

1T = 1 0

(13)

= Tt
A2 + B2 z = b2
y, z 0

approach require bilinear program semi-compact form.
Linear Bound second method, using linear bound, also simple implement compute, selective requiring feasibility. Let:
Yh = {y u(y) h} {y g(y) h} = Yh .
set convex thus need approximated. linear program used
find pivot point maximal error bound follows:
maximize
,t



subject u(T t) rT x + xT CT

x L

1T = 1 0

(14)

u(T t) h
difference Eq. (12) last constraint. approach requires bilinear
program semi-compact form ensure u(y) bound total return.
Cutting Plane third method, using cutting plane elimination, computationally intensive one, also selective one. Using approach requires
additional assumptions parts algorithm, discuss below.
method based principle -extensions concave cuts (Horst & Tuy, 1996).
start set YhC convex may expressed as:





max sT
w
+
r
x
+

C
x
+
r

h
(15)
1
1
2
w,x

255

A1 x + B1 w = b1

(16)

w, x 0

(17)

fiPetrik & Zilberstein

y3

f1
y1
y2
f2

Yh

Figure 6: Approximating Yh using cutting plane elimination method.
use inequalities selecting pivot point, need make linear.
two obstacles: Eq. (15) contains bilinear term maximization.
issues addressed using dual formulation Eq. (15). corresponding linear program dual fixed y, ignoring constants h r2T y, are:
maximize
w,x



sT
1 w + r1 x + C x

subject A1 x + B1 w = b1

minimize
(18)



bT
1

subject
1 r1 + Cy

w, x 0

(19)

B1T s1

Using dual formulation, Eq. (15) becomes:




min b1 + r2
h



1 r1 + Cy
B1T s1
Now, use function value following holds:
min (x) (x) (x) .
x

Finally, leads following set inequalities.
r2T h bT
1
Cy
1 r1
s1 B1T
inequalities define convex set YhC . complement Yh
necessarily convex, need use convex superset Yh given polyhedron.
done projecting YhC , subset, onto edges polyhedron depicted
Figure 6 described Algorithm 4. algorithm returns single constraint
cuts part set YhC . Notice combination first n points fk
256

fiA Bilinear Programming Approach Multiagent Planning

Algorithm 4: PolyhedronCut({y1 , . . . , yn+1 }, h) returns constraint
1
2

3
4
5
6
7
8
9

// Find vertices polyhedron {y1 , . . . , yn+1 } inside YhC
{yi yi YhC } ;
// Find vertices polyhedron outside YhC
{yi yi Yh } ;
// Find least n points fk edge Yh intersects edge
polyhedron
k1;

j
fk yj + max { (yi yj ) (YhC )} ;
k k+1 ;
k n
break ;

Find , [f1 , . . . , fn ] = 1T = 1 ;
// Determine correct orientation constraint Yh
feasible
11 yj O, yj >
// Reverse constraint points wrong way
12
;
13
;
10

14

return

used. general, may n points, subset points fk size n
used define new cutting plane constraints Yh . lead significant
improvements experiments. linear program find pivot point
cutting plane option follows:
maximize
,t,y



subject u(T t) rT x + xT CT
1T = 1 0

x L
(20)

= Tt
Ty
Here, , obtained result running Algorithm 4.
Note approach requires bilinear program semi-compact form
ensure g(y) convex. following proposition states correctness
procedure.
Proposition 16. resulting polyhedron produced Algorithm 4 superset
intersection polyhedron complement Yh .
Proof. convexity g(y) implies YhC also convex. Therefore, intersection
Q = {y }
257

fiPetrik & Zilberstein

also convex. also convex hull points fk YhC . Therefore, convexity
YhC , Q YhC , therefore Q Yh .

4. Dimensionality Reduction
experiments show efficiency algorithm depends heavily dimensionality matrix C Eq. (1). section, show principles behind automatically
determining necessary dimensionality given problem. Using proposed procedure, possible identify weak interactions eliminate them. Finally, procedure
works arbitrary bilinear programs generalization method previously
introduced (Petrik & Zilberstein, 2007a).
dimensionality inherently part model, problem itself. may
equivalent models given problem different dimensionality. Thus, procedures
reducing dimensionality necessary modeler create model
minimal dimensionality. However, nontrivial many cases. addition,
dimensions may little impact overall performance. determine ones
discarded, need measure contribution computed efficiently.
define notions formally later section.
assume feasible sets bounded L2 norms, assume general formulation bilinear program, necessarily semi-compact form. Given Assumption 7,
achieved scaling constraints feasible region bounded.
Assumption 17. x X , norms satisfy kxk2 1 kyk2 1.
discuss implications problems assumption presenting Theorem 18. Intuitively, dimensionality reduction removes dimensions g(y)
constant, almost constant. Interestingly, dimensions may recovered based
eigenvectors eigenvalues C C. use eigenvectors C C instead
eigenvectors C, analysis based L2 norm x thus C.
L2 norm kCk2 bounded largest eigenvalue C C. addition, symmetric
matrix required ensure eigenvectors perpendicular span whole
space.
Given problem represented using Eq. (1), let F matrix whose columns
eigenvectors C C eigenvalues greater . Let G matrix
remaining eigenvectors columns. Notice together, columns matrices span
whole space real-valued, since C C symmetric matrix. Assume without
loss generality eigenvectors unitary. compressed version bilinear
program following:
maximize
w,x,y1 ,y2 ,z



f(w, x, y1 , y2 , z) = r1T x + sT
2 w + x CF y1 + r2 F

subject A1 x + B1 w = b

y1
A2 F G
+ B2 z = b2
y2
w, x, y1 , y2 , z 0
258


y1
G
+ sT
2z
y2
(21)

fiA Bilinear Programming Approach Multiagent Planning

Notice program missing element xT CGy2 , would make optimal
solutions identical optimal solutions Eq. (1). describe practical approach
reducing dimensionality Appendix B. approach based singular value
decomposition may directly applied bilinear program. following theorem
quantifies maximum error using compressed program.
Theorem 18. Let f f optimal solutions Eq. (1) Eq. (21) respectively. Then:
p
= |f f | .
Moreover, maximal linear dimensionality reduction possible error without
considering constraint structure.

Proof. first show indeed error linearly compressed
problem given error least f dimensions. Using mapping preserves
feasibility programs, error bounded by:
fi


fi fi
fi
fi
fi fi
y1

fi = fix CGy2 fifi .
fifif w, x, F G
, z f w, x, y1 , 2
fi
y2
z
Denote feasible region y2 Y2 . orthogonality [F, G],
ky2 k2 1 follows:

y1
= F G
y2


y1
F
=
y2
GT
GT = y2
kGT yk2 = ky2 k2
have:
fi
fi
fi
fi
max max fixT CGy2 fi max kCGy2 k2
y2 Y2 xX
y2 Y2
q
q
p

max y2T GT C CGy2 max y2T Ly2



y2 Y2

y2 Y2

result follows Cauchy-Schwartz inequality, fact C C symmetric,
Assumption 17. matrix L denotes diagonal matrix eigenvalues corresponding
eigenvectors G.
Now, let H arbitrary matrix satisfies preceding error inequality G.
Clearly, H F = , otherwise y, kyk2 = 1, kCHyk2 > . Therefore,
|H| n |F | |G|, |H| + |F | = |Y |. | | denotes number columns
matrix.
Alternatively, bound proved replacing equality A1 x + B1 w = b1
kxk2 = 1. bound obtained Lagrange necessary optimality conditions.
bounds use L2 -norm; extension different norm straightforward. Note
259

fiPetrik & Zilberstein





kyk2 1

Figure 7: Approximation feasible set according Assumption 17.
also dimensionality reduction technique ignores constraint structure.
constraints special structure, might possible obtain even tighter
bound. described next section, dimensionality reduction technique generalizes
reduction Becker et al. (2004) used implicitly.
result Theorem 18 based approximation feasible set kyk2 1,
Assumption 17 states. approximation may quite loose problems,
may lead significant multiplicative overestimation bound Theorem 18.
example, consider feasible set depicted Figure 7. bound may achieved
point y, far feasible region. specific problems, tighter bound could
obtained either appropriately scaling constraints, using weighted L2 better
precision. partially address issue considering structure constraints.
derive this, consider following linear program corresponding theorem:
maximize
x

cT x

subject Ax = b

(22)

x0

Theorem 19. optimal solution Eq. (22) objective function
modified
cT (I (AAT )1 A)x,
identity matrix.
Proof. objective function is:
max

{x Ax=b, x0}

cT x =
=

max

{x Ax=b, x0}

cT (I (AAT )1 A)x + cT (AAT )1 Ax

= cT (AAT )1 b +

max

{x Ax=b, x0}

cT (I (AAT )1 A)x.

first term may ignored depend solution x.
260

fiA Bilinear Programming Approach Multiagent Planning

following corollary shows theorem used strengthen
dimensionality reduction bound. example, zero-sum games, stronger dimensionality reduction splits bilinear program two linear programs.
Corollary 20. Assume variables w z Eq. (1). Let:
1
Qi = (I
(Ai Ai ) Ai )),

{1, 2},

Ai defined Eq. (1). Let C be:
C = Q1 CQ2 ,
C bilinear-term matrix Eq. (1). bilinear programs
identical optimal solutions either C C.
Proof. Using Theorem 19, modify original objective function Eq. (1) to:
1

1

f (x, y) = r1T x + xT (I
1 (A1 A1 ) A1 ))C(I A2 (A2 A2 ) A2 ))y + r2 y.

sake simplicity ignore variables w z, influence bilinear
1
term. (I
(Ai Ai ) Ai ) = 1, 2 orthogonal projection matrices,
none eigenvalues Theorem 18 increase.
dimensionality reduction presented section related idea compound events used CSA. Allen, Petrik, Zilberstein (2008a, 2008b) provide detailed
discussion issue.

5. Offline Bound
section develop approximation bound depends number
points g(y) evaluated structure problem. kind bound
useful practice provides performance guarantees without actually solving
problem. addition, bound reveals parameters problem influence
algorithms performance. bound derived based maximal slope g(y)
maximal distance among points.
Theorem 21. achieve approximation error , number points
evaluated regular grid k points every dimension must satisfy:

n
kCk2 n
n
k
,

n number dimensions .
theorem follows using basic algebraic manipulations following lemma.
Lemma 22. Assume y1 exists y2 ky1 y2 k2
g(y2 ) = g(y2 ). maximal approximation error is:
= max g(y) g(y) kCk2 .
yY

261

fiPetrik & Zilberstein

Proof. Let y1 point maximal error attained. point ,
set compact. Now, let y2 closest point y1 L2 norm. Let x1 x2
best responses y1 y2 respectively. definition solution optimality
derive:



r1T x1 + r2T y2 + xT
1 Cy2 r1 x2 + r2 y2 + x2 Cy2

r1T (x1 x2 ) (x1 x2 )T Cy2 .
error expressed, using fact kx1 x2 k2 1, as:



= r1T x1 + r2T y1 + xT
1 Cy1 r1 x2 r2 y1 x2 Cy1

= r1T (x1 x2 ) + (x1 x2 )T Cy1
(x1 x2 )T Cy2 + (x1 x2 )T Cy1
(x1 x2 )T C(y1 y2 )
(x1 x2 )T
(y1 y2 )
ky1 y2 k2
C
k(x1 x2 )k2 ky1 y2 k2
ky1 y2 k2

max

max

{x kxk2 1} {y kyk2 1}

xT Cy

kCk2
derivation follows Assumption 17, bound reduces matrix
norm using Cauchy-Schwartz inequality.
surprisingly, bound independent local rewards transition structure
agents. Thus fact shows complexity achieving fixed approximation
fixed interaction structure linear problem size. However, bounds
still exponential dimensionality space. Notice also bound additive.

6. Experimental Results
turn empirical analysis performance algorithm. purpose
use Mars rover problem described earlier. compared algorithm
original CSA mixed integer linear program (MILP), derived Eq. (1) Petrik
Zilberstein (2007b) describe. Although Eq. (1) also modeled linear complementarity problem (LCP) (Murty, 1988; Cottle et al., 1992), evaluate
option experimentally LCPs closely related MILPs (Rosen, 1986). expect
two formulations exhibit similar performance. also compare
methods described Horst Tuy (1996) Bennett Mangasarian (1992) due
different nature high complexity, algorithms
provide optimality guarantees.
experiments, applied algorithm randomly generated problem instances
parameters Becker et al. (2003, 2004) used. problem instance
includes 2 rovers 6 sites. site, rovers decide perform experiment
skip site. Performing experiments takes time, experiments must
performed 15 time units. time required perform experiment drawn
discrete normal distribution mean uniformly chosen 4.0-6.0. variance
262

fiA Bilinear Programming Approach Multiagent Planning

Algorithm 5: MPBP: Multiagent Planning Bilinear Programming

6

Formulate DEC-MDP bilinear program B ;
// [Section 2.1]
B 0 ReduceDimensionality(B) 104 ;
// [Section 4, Appendix B]
Convert B 0 semi-compact form ;
// [Definition 2]
h ;
// Presolve step: run Algorithm 1 times random initialization
{1 . . . }
h max{h, IterativeBestResponse(B 0 )} ;
// [Algorithm 1]

7

BestResponseApprox(B 0 , 0 ) ;

1
2
3
4

5

// [Algorithm 2]

0.4 mean. local reward performing experiment selected uniformly
interval [0.1,1.0] site identical rovers. global reward,
received rovers perform experiment shared site, super-additive
1/2 local reward. experiments performed sites {1, 2, 3, 4, 5} shared
sites. Typically, performance algorithm degrades number shared sites.
problem fewer 5 shared sitesas used original CSA paperwere
easy solve, present results problems 5 shared sites. Note CSA
used problem implicit dimensionality reduction due use
compound events.
experiments, naive dimensionality Eq. (5) 6 15 2 = 180.
dimensionality reduced one per shared site using automatic
dimensionality reduction procedure. dimension represents probability
experiment shared site performed regardless time. Therefore, dimension
represents sum individual probabilities. Becker et al. (2004) achieved
compression using compound events, compound event represents fact
experiment performed site regardless specific time.
complete algorithmMultiagent Planning Bilinear Programming (MPBP)is
summarized Algorithm 5. automatic dimensionality reduction reduces 5 dimensions. Then, reformulating problem semi-compact form increases dimensionality
6. experimented different configurations algorithm differ way
refinements pivot point selection performed. different methods, described
Section 3.3, used create six configurations shown Figure 8. configuration
C1 corresponds earlier version algorithm (Petrik & Zilberstein, 2007a).
executed algorithm 20 times configuration every problem, randomly
generated according distribution described above. results represent average
random instances. maximum number iterations algorithm 200.
Due rounding errors, considered error less 104 0. algorithm
implemented MATLAB release 2007a. linear solver used MOSEK version
5.0. hardware configuration Intel Core 2 Duo 1.6 GHz Low Voltage 2GB
RAM. time perform dimensionality reduction negligible included
result.
direct comparison CSA possible CSA cannot solve problems
dimensionality within reasonable amount time. However, similar
263

fiPetrik & Zilberstein

Configuration

Feasible
[Eq. (13)]

C1

Linear bound
[Eq. (14)]

Cutting plane
[Eq. (20)]

0



C2



C3

0





C4

0





C6

0





C5

Presolve []

10



10

Figure 8: six algorithm configurations evaluated. Feasible, linear bound,
cutting plane refer methods used determine optimal solution.

problem setup 4 shared sites, CSA solved 76% problems,
longest solution took approximately 4 hours (Becker et al., 2004). contrast, MPBP
solved 200 problems 4 shared sites optimally less 1 second average,
10000 times faster. addition, MPBP returns solutions guaranteed close
optimal first iterations. CSA also returns solutions close optimal
rapidly, takes long time confirm that.
Figure 9 shows average guaranteed ratio optimal solution, achieved
function number iterations, is, points g(y) evaluated. figure,
others, shows result online error bound. value guaranteed
based optimal solution. compares performance various configurations
algorithm, without using presolve step. optimal solution typically
discovered first iterations, takes significantly longer prove optimality.
average absolute errors linear log scale shown Figure 10.
results indicate methods proposed eliminate dominated region searching
pivot point dramatically improve performance. requiring new
pivot points feasible improves performance, much significant
1

Fraction Optimal

C1
0.95

C2

0.9

C3
C4

0.85
0.8
0.75
0.7
0.65
0.6
0

50

100
Iteration

150

200

Figure 9: Guaranteed fraction optimality according online bound.
264

fiA Bilinear Programming Approach Multiagent Planning

1

5

10

C1

C1

C2

4

C

2

0

10

C

C3

C4
3

2

Absolute Error

Absolute Error

3

4

2

10

3

1

0
0

C
1

10

10

4

50

100
Iteration

150

10

200

0

50

100
Iteration

150

200

Figure 10: Comparison absolute error various region elimination methods.

better approximation Yh . expected, cutting plane elimination efficient,
also complex.
evaluate tradeoffs implementation, also show average time per
iteration average total time Figure 11. figures show time per
iteration significantly larger cutting plane elimination used. Overall,
algorithm faster simpler linear bound used.

0.03

12

0.025

10

0.02

8

Total Seconds

Seconds per Iteration

trend likely problem specific. problems higher dimensionality,
precise cutting plane algorithm may efficient. Implementation issues play
significant role problem too, likely implementation Algorithm 4
improved.

0.015
0.01

4
2

0.005
0

6

C1

C2

C3

0

C4

C1

C2

C3

C4

Figure 11: Time per iteration total time solve. configurations C1 C2 ,
optimal value reached 200 iterations . figure shows
time compute 200 iterations.

265

fiPetrik & Zilberstein

1

10

C3
C

4

C5
Absolute Error

2

C6

10

3

10

4

10

0

10

20
Iteration

30

40

Figure 12: Influence presolve method.
Figure 12 shows influence using presolve method. plots C3 C4
identical plots C5 C6 respectively, indicating presolve method
significant influence. also indicates solution close
optimal obtained values initial points calculated.
also performed experiments CPLEXa state-of-the-art MILP solver direct MILP formulation DEC-MDP. CPLEX able solve problems
within 30 minutes, matter many sites shared. main reason
take advantage limited interaction. Nevertheless, possible
specialized MILP solvers may perform better.

7. Conclusion Work
present algorithm significantly improves state-of-the-art solving two-agent
coordination problems. algorithm takes input bilinear program representing
problem, solves problem using new successive approximation method. provides
useful online performance bound used decide approximation
good enough. algorithm take advantage limited interaction among agents,
translated small dimensionality bilinear program. Moreover, using
approach, possible reduce dimensionality problems automatically,
without extensive modeling effort. makes easy apply new method practice.
applied DEC-MDPs, algorithm much faster existing CSA method,
average reducing computation time four orders magnitude. also show
variety coordination problems treated within framework.
Besides multiagent coordination problems, bilinear programs previously used
solve problems operations research global optimization (Sherali & Shetty, 1980;
White, 1992; Gabriel, Garca-Bertrand, Sahakij, & Conejo, 2005). Global optimization
deals finding optimal solutions problems multi-extremal objective function.
Solution techniques often share idea based cutting plane methods.
main idea iteratively restrict set feasible solutions, improving incumbent
266

fiA Bilinear Programming Approach Multiagent Planning

solution. Horst Tuy (1996) provide excellent overview techniques.
algorithms different characteristics cannot directly compared algorithm
developed. Unlike traditional algorithms, focus providing quickly good
approximate solutions error bounds. addition, exploit small dimensionality
best-response space get tight approximation bounds.
Future work address several interesting open questions respect bilinear
formulation well improvement efficiency algorithm. regard
representation, yet determined whether anytime behavior exploited
applied games. is, necessary verify approximate solution
bilinear program also meaningful approximation Nash equilibrium. also
important identify classes extensive games efficiently formulated
bilinear programs.
algorithm present made efficient several ways. particular,
significant speedup could achieved reducing size individual linear programs.
programs solved many times constraints, different objective
function. objective function always small-dimensional space. Therefore,
problems solved similar. DEC-MDP domain, one option would
use procedure similar action elimination. addition, performance could
significantly improved starting tight initial triangulation. implementation,
simply use single large polyhedron covers whole feasible region. better
approach would start something approximates feasible region
tightly. tighter approximation feasible region could also improve precision
dimensionality reduction procedure. Instead naive ellipsis used Assumption 7,
possible use one approximates feasible region tightly possible.
however encouraging see even without improvements, algorithm
effective compared existing solution techniques.

Acknowledgments
thank Chris Amato, Raghav Aras, Alan Carlin, Hala Mostafa, anonymous
reviewers useful comments suggestions. work supported part
Air Force Office Scientific Research Grants No. FA9550-05-1-0254 FA955008-1-0181, National Science Foundation Grants No. IIS-0535061
IIS-0812149.

Appendix A. Proofs
Proof Proposition 10 proposition states proposed triangulation,
sub-polyhedra overlap cover whole feasible set , given pivot
point interior S.
Proof. prove theorem induction number polyhedron splits
performed. base case trivial: single polyhedron, covers
whole feasible region.
inductive case, show polyhedron sub-polyhedra induced
pivot point cover overlap. notation use following:
267

fiPetrik & Zilberstein

denotes original polyhedron = c pivot point, 1T c = 1 c 0.
Note matrix c, d, vectors, scalar.
show sub-polyhedra cover original polyhedron follows. Take
= 1T = 1 0. show exists sub-polyhedron
contains vertex. First, let


=
1T
matrix square invertible, since polyhedron non-empty. get representation contains y, show vector i,
o(i) = 0:



= = +
1
0,
> 0. ensure sub-polyhedron vertex
replaced y. value depends follows:

1
.
=
1
achieved setting:
= min


d(i)
(T 1 y)(i)

.

Since c = 1 non-negative. leaves us equation
sub-polyhedron containing point a. Notice resulting polyhedron may
smaller dimension n o(j) = 0 6= j.
show polyhedra overlap, assume exists point common interior least two polyhedra. is, assume convex
combination vertices:
= T3 c1 + h1 + 1 y1
= T3 c2 + h2 + 2 y2 ,
T3 represents set points common two polyhedra, y1 y2 represent
disjoint points two polyhedra. values h1 , h2 , 1 , 2 scalars,
c1 c2 vectors. Notice sub-polyhedra differ one vertex.
coefficients satisfy:
c1 0

c2 0

h1 0

h2 0

1 0

2 0





1 c1 + h1 + 1 = 1

1 c2 + h2 + 2 = 1
268

fiA Bilinear Programming Approach Multiagent Planning

Since interior polyhedron non-empty, convex combination unique.
First assume h = h1 = h2 . show following:
= T3 c1 + hy + 1 y1 = T3 c2 + hy + 2 y2
T3 c1 + 1 y1 = T3 c2 + 2 y2
1 y1 = 2 y2
1 = 2 = 0
holds since y1 y2 independent T3 polyhedron nonempty
y1 6= y2 . last equality follows fact y1 y2 linearly independent.
contradiction, since 1 = 2 = 0 implies point interior
two polyhedra, intersection.
Finally, assume WLOG h1 > h2 . let = T3 c + 1 y1 + 2 y2 , scalars
1 0 2 0 represent convex combination. get:
= T3 c1 + h1 + 1 y1 = T3 (c1 + h1 c) + (h1 1 + 1 )y1 + h1 2 y2
= T3 c2 + h2 + 2 y2 = T3 (c2 + h2 c) + h2 1 y1 + (h2 2 + 2 )y2 .
coefficients sum one shown below.
1T (c1 + h1 c) + (h1 1 + 1 ) + h1 2 = 1T c1 + 1 + h1 (1T c + 1 + 2 ) = 1T c1 + 1 + h1 = 1
1T (c2 + h2 c) + 1 + (h2 2 + 2 ) = 1T c2 + 2 + h2 (1T c + 1 + 2 ) = 1T c2 + 2 + h2 = 1
Now, convex combination unique, therefore coefficients associated
vertex two representations must identical. particular, equating coefficients y1 y2 results following:
h1 1 + 1 = h2 1

h1 2 = h2 2 + 2

1 = h2 1 h1 1

2 = h1 2 h2 2

1 = 1 (h2 h1 ) > 0

2 = 2 (h1 h2 ) < 0

1 > 0 2 > 0 fact interior polyhedron S.
Then, 2 0 contradiction convex combination vertices
S.

Appendix B. Practical Dimensionality Reduction
section describe approach dimensionality reduction easy implement. Note least two possible approaches take advantage reduced
dimensionality. First, possible use dimensionality information limit algorithm work significant dimensions . Second, possible modify
bilinear program small dimensionality. changing algorithm may
straightforward, limits use advanced pivot point selection methods described
Section 3.3. Here, show implement second option straightforward way
using singular value decomposition.
269

fiPetrik & Zilberstein

dimensionality reduction applied following bilinear program:
maximize
w,x,y,z




r1T x + sT
1 w + x Cy + r2 + s2 z

subject A1 x + B1 w = b1
A2 + B2 z = b2

(23)

w, x, y, z 0
Let C = SV singular value decomposition. Let = [T1 , T2 ],
singular value vectors ti T2 less required . Then, bilinear program
reduced dimensionality may defined follows:
maximize
w,x,y,y,z




r1T x + sT
1 w + x SV T1 + r2 + s2 z

subject T1 =
A1 x + B1 w = b1

(24)

A2 + B2 z = b2
w, x, y, z 0
Note constrained non-negative. One problematic aspect reducing
dimensionality define initial polyhedron needs encompass feasible
solutions. One option make large enough contain set {y kyk2 = 1},
may large. Often practice, may efficient first triangulate rough
approximation feasible region, execute algorithm triangulation.

Appendix C. Sum Convex Concave Functions
section show best-response function g(y) may convex
program semi-compact form. convexity best-response function
crucial bounding approximation error eliminating dominated regions.
show program semi-compact form, best-response
function written sum convex function concave function. show
consider following bilinear program.
maximize
w,x,y,z




f = r1T x + sT
1 w + x Cy + r2 + s2 z

subject A1 x + B1 w = b1
A2 + B2 z = b2
w, x, y, z 0
problem may reformulated as:
f

=
=

max

max

max

g 0 (y) + sT
2 z,

{y,z (y,z)Y } {x,w (x,w)X}
{y,z (y,z)Y }




r1T x + sT
1 w + x Cy + r2 + s2 z

270

(25)

fiA Bilinear Programming Approach Multiagent Planning


g 0 (y) =

max

{x,w (x,w)X}



r1T x + sT
1 w + x Cy + r2 y.

Notice function g 0 (y) convex, maximum set linear functions.
Since f = max{y (y,z)Y } g(y), best-response function g(y) expressed as:
g(y) =

max

{z (y,z)Y }
0

0
g 0 (y) + sT
2 z = g (y) +

max

{z (y,z)Y }

sT
2z

= g (y) + t(y),

t(y) =

max

{z A2 y+B2 z=b2 , y,z0}

sT
2 z.

Function g 0 (y) depend z, therefore could taken maximization.
function t(y) corresponds linear program, dual using variable q is:
(b2 A2 y)T q

minimize
q

subject B2T q s2

(26)

Therefore:
t(y) =

(b2 A2 y)T q,

min

{q B2T qs2 }

concave function, minimum set linear functions.
best-response function written as:
g(y) = g 0 (y) + t(y),
sum convex function concave function, also known d.c. function (Horst & Tuy, 1996). Using property, easy construct program
g(y) convex one part concave another part , following
example shows. Note semi-compact bilinear programs t(y) = 0, guarantees
convexity g(y).
Example 23. Consider following bilinear program:
maximize
x,y,z

x + xy 2z

subject 1 x 1
yz 2
z0
plot best response function program shown Figure 13.

271

(27)

fiPetrik & Zilberstein

2

maxx f(x,y)

1.5
1
0.5
0
0.5
1
1

0

1

2

3

4



Figure 13: plot non-convex best-response function g bilinear program,
semi-compact form.

References
Allen, M., Petrik, M., & Zilberstein, S. (2008a). Interaction structure dimensionality
decentralized problem solving. Conference Artificial Intelligence (AAAI), pp.
14401441.
Allen, M., Petrik, M., & Zilberstein, S. (2008b). Interaction structure dimensionality
decentralized problem solving. Tech. rep. 08-11, Computer Science Department,
University Massachussetts.
Aras, R. (2008). Mathematical programming methods decentralized POMDPs. Ph.D.
thesis, Universite Henri Poincare, Nancy, France.
Aras, R., & Charpillet, F. (2007). mixed integer linear programming method finitehorizon Dec-POMDP problem. International Conference Automated Planning
Scheduling (ICAPS), pp. 1825.
Becker, R. (2006). Exploiting Structure Decentralized Markov Decision Processes. Ph.D.
thesis, University Massachusetts Amherst.
Becker, R., Lesser, V., & Zilberstein, S. (2004). Decentralized Markov decision processes
event-driven interactions. International Joint Conference Autonomous
Agents Multi Agent Systems (AAMAS), pp. 302309.
Becker, R., Zilberstein, S., & Lesser, V. (2003). Transition-independent decentralized
Markov decision processes. International Joint Conference Autonomous Agents
Multi Agent Systems (AAMAS), pp. 4148.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independent decentralized Markov decision processes. Journal Artificial Intelligence
Research, 22, 423455.
Bennett, K. P., & Mangasarian, O. L. (1992). Bilinear separation two sets n-space.
Tech. rep., Computer Science Department, University Wisconsin.
Bernstein, D. S., Zilberstein, S., & Immerman, N. (2000). complexity decentralized control Markov decision processes. Conference Uncertainty Artificial
Intelligence (UAI), pp. 3237.
272

fiA Bilinear Programming Approach Multiagent Planning

Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic programming. Athena Scientific.
Boutilier, C. (1999). Sequential optimality coordination multiagent systems.
International Joint Conference Artificial Intelligence, pp. 478485.
Bresina, J. L., Golden, K., Smith, D. E., & Washington, R. (1999). Increased fexibility
robustness Mars rovers. International Symposium AI, Robotics,
Automation Space, pp. 167173.
Cheng, H. T. (1988). Algorithms Partially Observable Markov Decision Processes. Ph.D.
thesis, University British Columbia.
Cottle, R. W., Pang, J.-S., & Stone, R. E. (1992). Linear Complementarity Problem.
Academic Press.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions partially observable stochastic games common payoffs. International
Joint Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 136
143.
Gabriel, S. A., Garca-Bertrand, R., Sahakij, P., & Conejo, A. J. (2005). practical approach
approximate bilinear functions mathematical programming problems using
Schurs decomposition SOS type 2 variables. Journal Operational Research
Society, 57, 9951004.
Goldman, C. V., & Zilberstein, S. (2008). Communication-based decomposition mechanisms
decentralized MDPs. Journal Artificial Intelligence Research, 32, 169202.
Horst, R., & Tuy, H. (1996). Global optimization: Deterministic approaches. Springer.
Kim, Y., Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2006). Exploiting locality
interaction networked distributed POMDPs. AAAI Spring Symposium
Distributed Planning Scheduling, pp. 4148.
Koller, D., Megiddo, N., & von Stengel, B. (1994). Fast algorithms finding randomized
strategies game trees. ACM Symposium Theory Computing, pp. 750
759.
Mangasarian, O. L. (1995). linear complementarity problem separable bilinear
program. Journal Global Optimization, 12, 17.
Murty, K. G. (1988).
Helderman-Verlag.

Linear complementarity, linear nonlinear programming.

Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralized POMDPs: Towards efficient policy computation multiagent settings..
International Joint Conference Artificial Inteligence, pp. 705711.
Nair, R., Roth, M., Yokoo, M., & Tambe, M. (2004). Communication improving policy
computation distributed pomdps. International Joint Conference Agents
Multiagent Systems (AAMAS), pp. 10981105.
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcast
channel: performance bounds. Proceeding IEEE Conference Decision
Control, Vol. 1, pp. 293298.
273

fiPetrik & Zilberstein

Osborne, M. J., & Rubinstein, A. (1994). course game theory. MIT Press.
Pang, J.-S., Trinkle, J. C., & Lo, G. (1996). complementarity approach quasistatic
rigid body motion problem. Journal Computational Optimization Applications,
5 (2), 139154.
Petrik, M., & Zilberstein, S. (2007a). Anytime coordination using separable bilinear programs. Conference Artificial Intelligence, pp. 750755.
Petrik, M., & Zilberstein, S. (2007b). Average reward decentralized Markov decision processes. International Joint Conference Artificial Intelligence, pp. 19972002.
Puterman, M. L. (2005). Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, Inc.
Rosberg, Z. (1983). Optimal decentralized control multiaccess channel partial
information. IEEE Transactions Automatic Control, 28, 187193.
Rosen, J. B. (1986). Solution general LCP 0-1 mixed integer programming. Tech. rep.
Computer Science Tech. Report 8623, University Minnesota, Minneapolis.
Rubinstein, A. (1997). Modeling bounded rationality. MIT Press.
Seuken, S., & Zilberstein, S. (2007). Memory bounded dynamic programming DECPOMDPs. International Joint Conference Artificial Intelligence, pp. 20092016.
Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decision
making uncertainty. Autonomous Agents Multiagent Systems, 17 (2), 190
250.
Sherali, H. D., & Shetty, C. M. (1980). finitely convergent algorithm bilinear programming problems using polar cuts disjunctive face cuts. Mathematical Programming,
19 (1), 1431.
Vanderbei, R. J. (2001). Linear Programming: Foundations Extensions (2nd edition).
Springer.
White, D. J. (1992). linear programming approach solving bilinear programmes.
Mathematical Programming, 56 (1), 4550.

274

fiJournal Artificial Intelligence Research 35 (2009) 533-555

Submitted 12/08; published 07/09

Solving Weighted Constraint Satisfaction Problems
Memetic/Exact Hybrid Algorithms
Jose E. Gallardo
Carlos Cotta
Antonio J. Fernandez

pepeg@lcc.uma.es
ccottap@lcc.uma.es
afdez@lcc.uma.es

Dept. Lenguajes Ciencias de la Computacion
Universidad de Malaga, ETSI Informatica
Campus de Teatinos, 29071 Malaga, Spain

Abstract
weighted constraint satisfaction problem (WCSP) constraint satisfaction problem
preferences among solutions expressed. Bucket elimination complete
technique commonly used solve kind constraint satisfaction problem.
memory required apply bucket elimination high, heuristic method based
(denominated mini-buckets) used calculate bounds optimal solution.
Nevertheless, curse dimensionality makes techniques impractical large scale
problems. response situation, present memetic algorithm WCSPs
bucket elimination used mechanism recombining solutions, providing
best possible child parental set. Subsequently, multi-level model
exact/metaheuristic hybrid hybridized branch-and-bound techniques
mini-buckets studied. case study, applied algorithms resolution
maximum density still life problem, hard constraint optimization problem based
Conways game life. resulting algorithm consistently finds optimal patterns
date solved instances less time current approaches. Moreover, shown
proposal provides new best known solutions large instances.

1. Introduction
Many real problems formulated constraint satisfaction problems (CSPs)
solutions assignments set variables (each variable taking values certain
domain), exists collection constraints restrict assignment
particular values combination values; solving CSP means finding feasible assignment
values variables, i.e., one constraints satisfied. However, wide range
problems cannot posed way, either problem over-constrained (and
thus solution) problem multiple solutions objective
find best one according optimality criterion. cases, problem might
handled optimization point view associating preferences constraints.
kind CSP preferences among constraints/solutions expressed
called weighted constraint satisfaction problems (WCSPs) (Schiex, Fargier, & Verfaillie,
1995; Bistarelli, Montanari, & Rossi, 1997). Solving WCSP means optimally satisfying
set weighted constraints. clearly enlarges scope CSPs: many practical
problems modeled WCSPs, instance, radio frequency assignment,
scheduling cellular manufacturing, among others (Cabon, de Givry, Lobjois, Schiex, &
Warners, 1999; Khemmoudj & Bennaceur, 2007; Nonobe & Ibaraki, 2001).
c
2009
AI Access Foundation. rights reserved.

fiGallardo, Cotta, & Fernandez

Complete methods, like branch-and-bound (Lawler & Wood, 1966) bucket elimination (Dechter, 1999), technique originated early work Bertele
Brioschi (1972) nonserial dynamic programming, two popular ways
attack WCSPs. However, although picture CSP general, noted
inclusion preferences constraints makes particular WCSP specific
consequence WCSPs tackled using specialized algorithms
specifically designed (Freuder & Wallace, 1992; Verfaillie, Lematre, & Schiex, 1996;
Kask & Detcher, 2001; Lematre, Verfaillie, Bourreau, & Laburthe, 2001; Larrosa & Schiex,
2004; Gelain, Pini, Rossi, & Venable, 2007; Khemmoudj & Bennaceur, 2007; Marinescu
& Dechter, 2007). Moreover, general techniques require large computational effort
(in time, memory both) solve many WCSPs, due size complexity,
therefore impractical many cases. alleviated using heuristic methods,
e.g., beam search (BS) (Barr & Feigenbaum, 1981) mini-buckets (Dechter, 1997),
branch-and-bound bucket elimination respectively. However, large scale problems,
high computational cost still evident.
context use alternative techniques must considered overcome
limitations general techniques; instance, evolutionary algorithms (Back, 1996; Back,
Fogel, & Michalewicz, 1997) powerful heuristics optimization problems based
principles natural evolution, flexible enough deployed wide
range problems. However, generality reduces competitiveness, unless domain
knowledge also incorporated. need exploiting domain knowledge optimization
methods repeatedly shown (Wolpert & Macready, 1997; Culberson, 1998),
memetic algorithms (Moscato & Cotta, 2003, 2007; Krasnogor & Smith, 2005) represent
one successful responses need (Hart, Krasnogor, & Smith, 2005).
paper explores different ways hybridizing branch-and-bound/bucket elimination (and
corresponding heuristic methods) memetic algorithms, combining search
capabilities synergetic way.
hybrid techniques proposed used general problem solvers WCSPs.
Note essentially heuristic nature hence cannot provide optimality
proofs solutions obtain. Notice however probably provide optimal
near-optimal solutions wide range WCSPs. Furthermore, hybrid techniques
less time-consuming general methods involved them, thus applied
larger problem instances. order experimentally evaluate hybrid techniques,
tackled Maximum Density Still Life Problem, hard combinatorial optimization problem also prime example weighted constraint optimization problem.
polynomial-time algorithm known solve problem, although, best
knowledge, problem yet proven NP-hard. reasons,
surprising problem attracted interest constraint-programming
community, central development assessment sophisticated techniques bucket elimination. Indeed, constitutes excellent test bed different
optimization techniques, included CSPLib1 repository. web page2
keeps record up-to-date results.
1. http://www.csplib.org
2. http://www.ai.sri.com/~nysmith/life

534

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

2. Preliminaries
section, briefly introduce concepts techniques used rest
paper. end, first define weighted constraint satisfaction problems, well
techniques bucket elimination mini-buckets. Subsequently, describe beam
search, heuristic tree search algorithm derived branch-and-bound. Finally, memetic
algorithms presented. sake notational simplicity, appropriate stick
notation Larrosa et al. (2003, 2005).
2.1 Weighted Constraint Satisfaction Problems
weighted constraint satisfaction problem (WCSP) (Schiex et al., 1995; Bistarelli et al.,
1997) constraint satisfaction problem (CSP) preferences among solutions
expressed. Formally, WCSP defined tuple (X , D, F), = {D1 , , Dn }
set finite domains, X = {x1 , , xn } set variables taking values finite
domains (Di domain variable xi ) F set cost functions (also called soft
constraints weighted constraints) used declare preferences among possible solutions.
Variable correctly assigned receive finite costs express degree preference (the
lower value better preference) variables correctly assigned receive cost
. Note f F defined subset variables, var(f ) X , called
P scope.
objective function F defined sum functions F, i.e., F = f F f .
assignment value vi Di variable xi noted xi = vi . partial assignment
< n variables tuple = (xi1 = v1 , xi2 = v2 , , xim = vm ) ij {1, . . . , n}
different. complete assignment variables values domains
satisfies every soft constraint (i.e., finite valuation F ) represents solution
WCSP. optimization goal find solution minimizes objective function.
2.2 Bucket Elimination
Bucket elimination (BE) (Dechter, 1999) generic technique suitable many automated
reasoning optimization problems and, particular, solving WCSP. functioning
based upon following two operators functions (Larrosa et al., 2005):
sum two functions f g, denoted (f + g), new function scope
var(f )var(g) returns tuple sum costs f g, i.e., (f +g)(t) =
f (t) + g(t).
elimination variable xi f , denoted f xi , new function scope
var(f ) {xi } returns tuple minimum cost extension xi ,
(f xi )(t) = minvDi {f (t (xi = v))}, (xi = v) means extension
assignment assignment value v variable xi . Observe f
unary function (i.e., arity one), constant obtained upon elimination
variable scope.
Without losing generality, let us assume lexicographic ordering variables
X , i.e., = (x1 , x2 , , xn ). Figure 1 shows pseudo-code algorithm solving
WCSP instance, returns optimal cost F one optimal assignment
535

fiGallardo, Cotta, & Fernandez

Bucket Elimination WCSP (X , D, F )
1:
2:
3:
4:
5:
6:
7:
8:
9:
10 :
11 :

function BE(X , D, F)
:= n downto 1
Bi := {f
P F | xi var(f )}
gi := (
f Bi f ) xi
F := (F {gi }) Bi
end
:=
:= 1 n P
v := argminaDi {( f Bi f )(t (xi = a))}
:= (xi = v)
end
return(F, t)
end function

Figure 1: general template, adapted Larrosa Morancho (2003), bucket
elimination WCSP (X , D, F ).

t. Observe that, initially, eliminates decreasing order one variable xi X
iteration loop comprising lines 1-5. done computing firstly bucket Bi
variable xi set cost functions F xi scope. Then, new function
gi defined sum functions Bi variable xi eliminated.
Finally, F updated removing functions involving xi (i.e., Bi ) adding
new function contain xi . consequence xi exist F
value optimal cost preserved. elimination x1 produces function
empty scope (i.e., constant) optimal cost problem. Then,
lines 6-10, generates optimal assignment variables considering order
imposed o: done starting empty assignment assigning xi
best value extension t, respect sum functions Bi (argmina {f (a)}
represents value producing minimum f (a)).
Note exponential space complexity because, general, result summing functions eliminating variables cannot expressed intensionally algebraic expressions and, consequence, intermediate results collected extensionally
tables. precise, complexity depends problem structure (as captured
constraint graph G) ordering o. According Larrosa Morancho (2003),


complexity along ordering time (Q n dw (o)+1 ) space (n dw (o) ),
largest domain size, Q cost evaluating cost functions (usually assumed (1)), w (o) induced width graph along ordering o, describes
largest clique created graph bucket elimination, corresponds
largest scope function recorded algorithm. Although finding optimal
ordering NP-hard (Arnborg, 1985), heuristics approximation algorithms
developed task check work Dechter (1999) details.
536

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

2.3 Mini-Buckets
main drawback requires exponential space store functions extensionally. complexity high, solution approximated using
mini-bucket (MB) approach presented Dechter (1997) Detcher Rish (2003). Recall that, order eliminate variable xi ,
P corresponding bucket Bi = {fi1 , . . . , fim },
calculates new cost function gi = ( f Bi f ) xi , whose time space complexity increases cardinality gi , i.e., size set f Bi var(f ) {xi }.
complexity decreased approximating function gi set smallerarity functions. basic idea partition bucket Bi k called mini-buckets
Bi1 , . . . , Bik , number variables scope Bij bounded
parameter. Afterwards,
set k cost functions reduced arity sought
P
defined gij = ( f Bi f ) xi , j = 1 . . . k, required approximation gi
j
P
P
P
computed sum gi0 = 16j6k gij = 16j6k (( f Bi f ) xi ).
j
Note minimization computed gi operator migrated inside
sum. Since, general, two non-negative functions f1 (x) f2 (x), minx (f1 (x)+
f2 (x)) > minx f1 (x) + minx f2 (x), follows gi0 lower bound gi . Therefore,
variable elimination performed using approximated cost functions, provides lower
bound optimal cost requiring less computation BE. Notice described
approach provides family under-estimating heuristic functions whose complexity
accuracy parameterized maximum number variables allowed mini-bucket.
2.4 Beam Search
Branch-and-bound (BB) (Lawler & Wood, 1966) general tree search method solving
combinatorial optimization problems. Tree search methods constructive, sense
work partial solutions. way, tree search methods start empty
solution incrementally extended adding components it. way partial
solutions extended depends constraints imposed problem solved.
solution construction mechanism maps search space tree structure,
way path root tree leaf node corresponds construction
solution. order efficiently explore search tree, BB algorithms maintain
upper bound estimate lower bounds partially constructed solutions. Assuming
minimization problem, upper bound corresponds cost best solution found
far. search process, lower bound computed partial solution
generated, estimating cost best solution constructed extending
it. lower bound greater current upper bound, solutions constructed
extending lead improvement, thus nodes descending
pruned search tree. Clearly, capability algorithm pruning
search tree depends existence accurate lower bound, also
computationally inexpensive order practical.
Beam search (BS) (Barr & Feigenbaum, 1981) algorithms incomplete derivates BB
algorithms, thus heuristic methods. Essentially, BS works extending every partial
solution set B (called beam) kext possible ways. new partial
solution generated stored set B. solutions B processed,
algorithm constructs new beam selecting best kbw (called beam width)
537

fiGallardo, Cotta, & Fernandez

solutions B. Clearly, way estimating quality partial solutions,
lower bound, needed this.
interesting peculiarity BS way extends parallel set different partial
solutions several possible ways, making particularly suitable tree search method
used hybrid collaborative framework (it used provide periodically promising partial solutions population-based search method memetic algorithm).
Gallardo, Cotta, Fernandez (2007) shown kind hybrid algorithms
provide excellent results combinatorial optimization problems. subsequently present hybrid tree search/memetic algorithm WCSPs based idea.
2.5 Memetic Algorithms
Evolutionary algorithms (EAs) population-based metaheuristic optimization methods
inspired biological evolution (Back et al., 1997). order explore search space,
EA maintains set solutions known population individuals. usually
randomly initialized across search space, although heuristics may also used.
initialization, three different phases iteratively performed termination condition
reached: selection, reproduction (which encompasses recombination mutation)
replacement. context EAs, objective function assigning values solution
termed fitness function, used guide search.
Note EAs black box optimization procedures sense knowledge
problem (apart fitness function) used. need exploit problemknowledge repeatedly shown (Wolpert & Macready, 1997; Culberson, 1998) however. Different attempts made answer need; Memetic algorithms (Moscato
& Cotta, 2003, 2007; Krasnogor & Smith, 2005) (MAs) one successful approaches date (Hart et al., 2005). Like EAs, MAs also population based metaheuristics. main difference components population (sometimes termed
agents terminology) passive entities. Rather, active entities
cooperate compete order find improved solutions.
many possible ways implement MAs. common implementation
consists combining EA procedure perform local search
solutions population main generation loop (cf. Krasnogor & Smith, 2005).
Figure 2 shows general outline MA; pX , pm arity respectively refer
recombination probability, mutation probability recombination arity i.e., number
parents involved recombination. must noted however paradigm
simply reduce particular scheme different places (e.g.,
population initialization, genotype phenotype mapping, evolutionary operators, etc.)
problem specific knowledge incorporated. work, addition using
tabu search (Glover, 1989, 1990) (TS) local search procedure within MA,
designed intelligent recombination operator uses relaxation bucket elimination
order find best solution constructed set parents without
introducing implicit mutation (i.e., exogenous information).
538

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

Memetic Algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10 :
11 :
12 :
13 :
14 :
15 :
16 :
17 :
18 :
19 :
20 :
21 :
22 :
23 :

function (pX , pm , arity)
:= 1 popsize
pop[i] := Random solution(n)
pop[i] := Local Search(pop[i])
Evaluate(pop[i])
end
timeout
:= 1 offsize
recombination performed (under pX )
j := 1 arity
parentj := Select(pop)
end
offspring[i ] := Recombine(parent1 , parent2 , . . . , parentarity )
else
offspring[i ] := Select(pop)
end
mutation performed (under pm )
offspring[i ] := Mutate(offspring[i ])
end
offspring[i ] := Local Search(offspring[i ])
Evaluate(offspring[i ])
end
pop := Replace(pop, offspring)
end

Figure 2: Pseudo code memetic algorithm (MA). Although different variants possible respect scheme, broadly captures typical algorithmic structure
MAs.

3. Multi-Level Memetic/Exact Hybrid Algorithm WCSPs
WCSPs suitable tackled evolutionary metaheuristics. Obviously,
quality results greatly depend well knowledge problem
incorporated search mechanism. final goal present algorithmic model
based hybridization MAs exact techniques two levels: within (as
embedded operator), outside (in cooperative model). Firstly, focus
next subsection first level hybridization, incorporates exact technique
(namely BE) within embedded recombination operator. Subsequently,
proceed second level hybridization, cooperates branchand-bound based beam search algorithm uses technique mini-buckets
lower bound (see Figure 3).
539

fiGallardo, Cotta, & Fernandez



Promising Regions

BS
GA
TS
Local
Search



MB

Crossover

Lower
Bound

Upper Bound

Figure 3: Schematic description proposed hybrid algorithm.
3.1 Optimal recombination
previously mentioned, one phases constitutes typical recombination
(i.e., lines 9-14 Figure 2), individuals population combined
aim obtaining improved individuals. purpose, different standard recombination operators proposed literature (see Back et al., 1997). Although
blind operators feasible computational point view, would perform poorly,
problem knowledge used. context WCSPs, resort
order achieve sensible recombination information.
Even though performance exact method resolution WCSPs
may better basic search-based approaches, corresponding time space complexity still high, making technique unsuitable large instances.
following, explain used implement intelligent recombination operator WCSPs. operator implicitly explore possible children solutions
recombined, providing best solution constructed without introducing
implicit mutation, i.e., exogenous information (cf. Cotta & Troya, 2003). Note
use bucket elimination related usually referred Large Neighborhood
Search (Ahuja, Ergun, Orlin, & Punnen, 2002).
sake simplicity, let us assume variables WCSP (X , D, F )
domain (i.e., D1 = = Dn ), let x = (x1 , x2 , , xn ) = (y1 , y2 , , yn )
two solutions recombined, [zi ] value variable zi . operator calculate best solution obtained combining variables
x without introducing information present parents.
achieved restricting domain variables values appearing configurations recombined. recombination operator becomes BE(X , D, F),
= {[x1 ], , [xn ], [y1 ], , [yn ]}. Applying approach WCSP variables
may different domains would require previously separating set variables X
subsets variables sharing domain.
3.2 Beam Search/MA Hybrid Algorithm
subsection, describe hybrid tree search/memetic algorithm WCSPs.
algorithm combines, collaborative way (Puchinger & Raidl, 2005), BS algorithm
MA. noted previously, BS works extending parallel set different partial solutions
several possible ways, thus used provide promising partial solutions
540

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

Hybrid algorithm WCSP
1:
2:
3:
4:
5:
6:
7:
8:
9:
10 :
11 :
12 :
13 :
14 :
15 :
16 :
17 :

function BS-MA (X , D, kbw , kM )
sol :=
B := { () }
:= 1 n
B0 := {}
B
Di
B 0 := B 0 {s (xi = a)}
end
end
B := select best kbw nodes B 0
(i > kM )
initialize population best popsize nodes B 0
run
sol := min (sol, solution)
end
end
return sol
end function
Figure 4: Hybrid algorithm WCSP.

population based search method MA. goal exploit capability BS
identifying potentially good regions search space, also exploit
explore regions, synergistically combining two different approaches.
proposed hybrid algorithm, executes BS interleaved way,
depicted Figure 4. pseudo-code, (possibly partial) solution WCSP instance
represented vector variables = (x1 , x2 , . . . , xi ), 6 n, (xi = a) stands
extension partial solution assigning value i-th variable noted
previously. hybrid algorithm constructs search tree, leaves consist
complete solutions internal nodes level represent partially specified (up i-th
variable) solutions. tree heuristically traversed breadth first way using BS
algorithm beam width kbw (i.e., maintaining best kbw nodes level
tree). beam selection (line 10), heuristic quality measure defined
partial solutions, whose value must partial solution unfeasible. algorithm
starts (line 2) totally unspecified solution. Initially, BS part algorithm
executed. iteration BS (lines 3-17), new variable assigned every
solution beam (line 7). interleaved execution starts partial
solutions beam least kM variables (line 11). iteration BS,
best popsize solutions beam selected (using quality measure described above)
initialize population (line 12). Since partial solutions, must
first converted full solutions, e.g., completing remaining variables randomly.
541

fiGallardo, Cotta, & Fernandez

running MA, solution used update incumbent solution (sol),
process repeated search tree exhausted.
3.3 Computing Tight Bounds Mini-Buckets
performance BS component algorithm described previous section
depend quality heuristic function used estimate partial solutions (line
10 Figure 4). order compute tight, yet computationally inexpensive, lower bound
remanning part solution resort Mini-Buckets (MB). described
Kask Detcher (2001), intermediate functions created applying MB scheme
used general mechanism compute heuristic functions estimate best
cost yet unassigned variables partial solutions. end, MB must run
preprocessing stage, using reverse order search instantiate variables.
set augmented buckets computed process used estimations
best cost extension partial solutions (check work Kask & Detcher, 2001,
details).

4. Tackling Maximum Density Still Life Problem
Previously proposed algorithms general enough used many WCSPs
executed. section present application case study maximum
density still life problem (MDSLP). problem defined context game life
proposed John H. Conway 60s divulged Martin Gardner (Gardner, 1970),
let us first describe game. played infinite checkerboard
player places checkers squares. square board called cell
eight neighbors; eight cells share one two corners it. cell alive
checker it, dead otherwise. contents board evolve iteratively,
way state time determines state time + 1 according
simple rules: (1) live cell remains alive two three live neighbors, otherwise
dies, (2) dead cell becomes alive exactly three live neighbors.
simple rules game life nevertheless generate incredibly complex
dynamics. better understand MDSLP, let us define stable pattern (also called
still life) board configuration change time, let density
region percentage living cells. MDSLP nn grid consists finding still
life maximum density. Elkies (1998) shown that, infinite boards, maximum
density 1/2 (for finite size, exact formula known). paper, concerned
MDSLP finite patterns, is, finding maximal n n still lifes.
4.1 Related Work
MDSLP tackled literature using different approaches. Bosch Trick
(2002) compared different formulations MDSLP using integer programming (IP)
constraint programming (CP). best results obtained hybrid algorithm
mixing two approaches. able solve cases n = 14 n = 15
6 8 days CPU time respectively. Smith (2002) used pure constraint
programming approach address problem. However, instances n = 10
542

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

Table 1: Best experimental results reported Bosch Trick (2002) (CP/IP), Larrosa
Morancho (2003) (BE) Larrosa et al. (2005) (HYB-BE) solving
MDSLP. Time indicated seconds.

optimum
CP/IP

HYB-BE

12
68
11536
1638
1

13
79
12050
13788
2

14
92
5 105
105
2

15
106
7 105

16
120

17
137

18
153

19
171

20
190

58

7

1091

2029

56027

2 105

could solved. best results problem reported Larrosa Morancho
(2003) Larrosa et al. (2005), showing usefulness bucket elimination (BE),
exact technique based variable elimination commonly used solving constraint
satisfaction problems described detail Section 2.2. basic approach could solve
problem n = 14 105 seconds. improvements increased boundary
n = 20 twice much time. Recently, Cheng Yap (2005, 2006) tackled
problem via use ad-hoc global case constraints, results comparable
IP/CP hybrids, thus cannot compared ones obtained previously Larrosa
et al.
Table 1 summarizes experimental results current approaches used tackle MDSLP, reporting computational times hybrid IP/CP algorithm Bosch Trick
(2002), approach Larrosa Morancho (2003) BE/search hybrid
Larrosa et al. (2005). Although different computational platforms may used
experiments, trends clear give clear indication potential
different approaches. noted techniques applied MDSLP
exact approaches. inherently limited increasing problem sizes
capabilities anytime algorithms unclear. tackle problem, recently proposed
use hybrid methods combining exact metaheuristic approaches. considered
hybridization evolutionary algorithms (a stochastic population-based search
method) endowed tabu search (a local search method)(Gallardo, Cotta, & Fernandez,
2006a). resulting algorithm memetic algorithm (MA; see Section 2.5). used
mechanism recombining solutions, providing best possible child
parental set. Experimental tests indicated algorithm provided optimal nearoptimal results acceptable computational cost. Subsequently, studied extended
multi-level models previous hybrid algorithm hybridized
branch-and-bound derivative, namely beam search (BS)(Gallardo, Cotta, & Fernandez,
2006b). Studies influence variable clustering multi-parent recombination
performance algorithm also conducted. results indicated
variable clustering detrimental problem also multi-parent recombination improves performance algorithm. best knowledge,
heuristic approaches applied problem date.
section, previous research problem included extended.
new contributions, redone experiments using improved implementation
543

fiGallardo, Cotta, & Fernandez

bucket elimination crossover operator, described Section 3.1. Additionally,
present extensive experimental analysis BS/MA hybrid described (Gallardo
et al., 2006b), analyzing sensitivity parameters. also propose new hybrid
algorithm uses technique mini-buckets improve lower bounds
partial solutions considered BS part hybrid algorithm. new algorithm
obtained hybridization, different levels, complete solving techniques (BE),
incomplete deterministic methods (BS MB) stochastic algorithms (MAs).
experimental analysis shows new proposal consistently finds optimal solutions
MDSLP instances n = 20 considerably less time previous approaches
reported literature. Finally, order test scalability approach, novel
hybrid algorithm run large instances MDSLP optimal
solution currently unknown. results successful, algorithm performed
state-of-the-art level, providing solutions equal better best
ones reported date literature. readability reasons, many particular technical
details different algorithms MDSLP omitted, fully described
accompanying report (Gallardo, Cotta, & Fernandez, 2008). rate, model
MDSLP WCSP presented Appendix A.
4.2 Memetic Algorithm MDSLP
First all, develop MDSLP. MA, n n board represented
binary n n matrix. Based stratified gradient provided penalty based fitness
function measures number violated constraints distance feasibility
(prioritizing former latter), efficient local search strategy explores
set solutions obtained flipping exactly one cell configuration devised. order
escape local optima, tabu-search scheme used (line 19 Figure 2).
uses crossover operation described Section 3.1 (line 12 Figure 2).
One interesting property operator described limited recombining
two board configurations, instead generalized recombine number
considering domains consisting values variable parents.
multi-parental capability also explored rest paper.
evaluate usefulness described hybrid recombination operator, set experiments problem sizes n = 12 n = 20 realized (recall optimal
solutions MDSLP known n = 20). experiments performed
using steady-state evolutionary algorithm (popsize = 100, pm = 1/n2 , pX = 0.9, binary
tournament selection). aim maintaining diversity, duplicated individuals
allowed population. Algorithms run optimal solution found
time limit exceeded. time limit set 3 minutes problem instances size 12
gradually increased 60 seconds size increment. algorithm
instance size, 20 independent runs made. experiments paper
performed Pentium IV PC (2400MHz 512MB RAM) SuSE Linux.
base algorithm using two-dimensional version SPX (single-point
crossover) recombination, endowed tabu search local improvement.
algorithm termed MATS , shown capable finding feasible solutions systematically, solving optimality instances n < 15 (see MATS Figure 5a).
544

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

10
9.5
9
8.5
8
7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0

25
MABE
MABE1F
MABE2F
MATS

Arity=2
Arity=4
Arity=8
Arity=16

22.5
20

% distance optimum

% distance optimum

Although performance algorithm degrades larger instances, provides distributions solutions whose average relative distance optimum less 5.29%
cases. contrasts case plain EAs, incapable finding even
feasible solution runs (Gallardo et al., 2006a).

17.5
15
12.5
10
7.5
5
2.5

12

13

14

15

16

17

18

19

0

20

12

13

14

15

16

17

18

19

20

instance size

instance size

(a)

(b)

Figure 5: Relative distances optimum different (a) algorithms (b) arities sizes
ranging 12 20. subsequent figures, box summarizes 20 runs, boxes comprise second third quartiles distribution
(i.e., inner 50%), horizontal line marks median, plus sign indicates
mean, circles indicate results median 1.5 times
interquartile-distance.
MATS firstly compared MAs endowed performing recombination.
Since use recombination higher computational cost simple blind
recombination, guarantee recombining two infeasible solutions
result feasible solution, defined three variants MAs:
first one, called MA-BE, always used perform recombination.
second, termed MA-BE1F , require least one parents feasible
order apply BE; otherwise blind recombination used.
last variant, identified MA-BE2F , require two parents feasible,
thus restrictive application BE.
evaluating variants, intend explore computational tradeoffs involved
application embedded component MA. algorithms,
mutation performed prior recombination order take advantage good solutions
provided BE. Figure 5a shows empirical performance different algorithms.
Results show MA-BE returns significantly better results MATS . MA-BE2F
find slightly better solutions MA-BE smaller instances (n {13, 15, 16}),
545

fiGallardo, Cotta, & Fernandez

larger instances winner MA-BE. seems effort saved recombining
unfeasible solutions improve performance algorithm. Note also
that, larger instances, MA-BE1F better MA-BE2F . correlates well
fact used frequently former latter.
mentioned Section 3.1, optimal recombination scheme use readily extended multi-parent recombination (Eiben, Raue, & Ruttkay, 1994): arbitrary number
solutions contribute constituent rows constructing new solution. Additional experiments done explore effect capability MA-BE. Figure 5b
shows results obtained MA-BE different number parents recombined
(arities 2, 4, 8 16). arity = 2, algorithm able find optimum solution
instances except n = 18 n = 20 (the relative distance optimum
best solution found less 1.04% cases). Runs arity = 4 cannot find
optimum solutions remaining instances, note distribution improves
cases. Clearly, performance algorithm deteriorates combining
4 parents due higher computational cost BE. Variable clustering could
used alleviate higher computational cost, results performance degradation
since coarser granularity pieces information hinders information mixing (Cotta
& Troya, 2000; Gallardo et al., 2006b).
4.3 BS/MA Hybrid Algorithm MDSLP
section evaluate instantiation BS hybrid algorithm described
Section 3.2 MDSLP, called BS-MA-BE. beam selection (line 10 Figure 4),
simple quality measure defined partial solutions, whose value either
partial configuration unstable, number dead cells otherwise. methodology
Section 4.2 (20 executions performed algorithm instance
size), arities {2, 3, 4}. setting remaining parameters
kbw = 2000 (preliminary tests indicated value reasonable), kM
{0.3 n, 0.5 n, 0.75 n}, i.e., best 2000 nodes kept level BS algorithm,
30%, 50% 75% levels BS tree initially descended
run. respect termination conditions, execution within
hybrid algorithm consists 1000 generations, time limits imposed hybrid
algorithms, run n iterations BS.
Figure 6a shows results different values parameter kM . order better
compare distributions, number optimal solutions obtained algorithm
(out 20 executions) shown box plot. kM = 0.3 n, performance
resulting algorithm improves significantly original MA. Note BS-MA-BE,
using arity 2 parents, able find optimum cases except n = 18
(this instance solved arity = 4). distributions different instance sizes
significantly improved. n < 17 arity {2, 3, 4}, algorithm consistently finds
optimum runs. instances, solution provided algorithm
always within 1.05% optimum, except n = 18, relative distance
optimum worst solution 1.3%. two charts show that, general,
performance algorithm deteriorates increasing values kM parameter.
may due low quality bounds used BS part.
546

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

750
Arity=2
Arity=3
Arity=4

500

k



= 0.75 n

250

0
1500
kMA = 0.50 n

1250
Arity=2
Arity=3
Arity=4

2

0

1

kMA = 0.75 n

1.5

0 0 0
0

1

1 1 0
3 9 10

0.5

Time best solution(s)

2.5

1000

750

500

250

20 20 20 20 20 20 20 20 20 20 20 20 20 20 20

0

% distance optimum

2.5

0

2

2000
k

0 5

kMA = 0.50 n

1.5



0 0 0

= 0.30 n

1750
1

1

2 0 1

1500

10 7 10

0.5

1250

20 20 20 20 20 20 20 20 20 20 20 20 20 20 20

0
2.5

1000

2

750
kMA = 0.30 n

1.5

0 0 3
4 2 6

1

500

2 2 1
11 13 14

250

0.5
20 20 20 20 20 20 20 20 20 20 20 20 20 20 20

0

12

13

14

15

16

17

18

19

0

20

(a)

12

13

14

15

16

17

18

19

20

instance size

instance size

(b)

Figure 6: (a) Relative distances optimum (b) time best solution different
arities BS-MA-BE KM {0.3 n, 0.5 n, 0.75 n}, sizes ranging
12 20. numbers box indicate many times optimal
solution found.

Regarding execution times, Figure 6b shows time distributions (in seconds) reach
best solution needed algorithms. Although BS-MA-BE requires time
MA-BE, time needed remains reasonable instances, always less
2000 seconds. Note also execution time increases arity, time
needed perform crossover operator. hand, execution
time decreases larger values kM number executions decreases,
although, already remarked, quality solutions worsens.
4.4 Improving Lower Bound using MB MDSLP
simple quality measure beam selection used previous section depends solely
part solution already constructed. section, experimentally
study use MB technique compute tight, yet computationally inexpensive,
547

fiGallardo, Cotta, & Fernandez

lower bound remanning part configuration aim improving
performance BS part hybrid algorithm. Basically, idea cluster cells
row board metavariable. metavariables partitioned
columns n/M cells each. Finally, resort MB estimate best cost
extensions partial board configuration considering columns.
summing estimations column extensions, bound best board extension
partial solution obtained. section, experimented = 3 (i.e., three
columns row), although complexity still high, approach
used reduce further, considering columns.
2.5
2.5

% distance optimum

2
kMA = 0.30 n

1.5

16 18

1

18

13 19

6 13 19

0.5
20 20 20 20 20 20 20 20 20 20 20 20 20 20 20

0

12

13

14

15

20 20

16

17

20

18

20

19

2
kMA = 0.50 n

1.5

19

1

16

13 18 19

4 13 16

0.5

20

20 20 20 20 20 20 20 20 20 20 20 20 20 20 20

instance size

20 20

20 20

0

(a)

(b)

2.5

1.5

Arity=2
Arity=3
Arity=4

750

Time b. sol(s)

2

kMA = 0.75 n
19

1

16 14 16

14 13 18

3 12 19

Arity=2
Arity=3
Arity=4

500

k



= 0.75 n

250

0.5
20 20 20 20 20 20 20 20 20 20 20 20 20 20 20

20

0

20

(c)

12

13

14

15

16

17

18

19

20

instance size

0

(d)

Figure 7: (a)-(c) Relative distances optimum using different arities
BS-MA-BE-MB KM {0.3 n, 0.5 n, 0.75 n}, sizes ranging
12 20. (d) Time (in seconds) best solution different arities
BS-MA-BE-MB kM = 0.75 n, sizes ranging 12 20.
Experiments repeated hybrid algorithm equipped new lower bound,
BS-MA-BE-MB. Figure 7a-7c shows results experiments values kM
{0.3 n, 0.5 n, 0.75 n}. algorithm finds optimum instances arities
relative distance optimum worst solution found less 1.05%
cases. best results obtained arity = 4, although requires slightly
execution time. Note also BS-MA-BE-MB less sensitive setting parameter
kM , means execution times reduced considerably using large value
parameter (see Figure 7d). particular combination parameters kM = 0.75 n
arity = 4 provides excellent results lower computational cost, execution times
always 570 seconds n 6 20. comparison, recall approach
literature solve instances described Larrosa et al. (2005) requires
33 minutes n = 18, 15 hours n = 19 2 days n = 20,
approaches unaffordable n > 15. Note however times correspond
computational platform different ours. order make fairer comparison, executed
548

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

algorithm Larrosa et al. 3 platform. case, required 1867 seconds
(i.e., 31 minutes) order solve n = 18 instance, 1 day
18 hours solve n = 20 instance. values close times reported
Larrosa et al. (2005), hence indicate computational platforms fairly
comparable.
1.5

% distance

1

Arity=2
Arity=3
Arity=4

0.5
0
0.5
1

22

24

26

28

instance size

Figure 8: Relative distances best known solutions using different arities
BS-MA-BE-MB kM = 0.3 n, large instances (i.e., sizes 22,
24, 26, 28). Note improvement best known solutions sizes 24
26.

Figure 9: New best known maximum density still lifes n {24, 26}.

Table 2: Optimal solutions SMDLP.
n
opt

12 13
68 79

14
92

15
106

16
120

17
137

18
154

19
172

20
192

22
232

24 26
276 326

28
378

4.5 Results Large Instances
already mentioned, currently approach available tackle MDSLP
n > 20. Larrosa et al. (2005) tried algorithm n = 21 n = 22, could
3. Available http://www.lsi.upc.edu/~larrosa/publications/LIFE-SOURCE-CODE.tar.gz . Time
n = 19 could obtained code provided Larrosa et al. used even sized
instances.

549

fiGallardo, Cotta, & Fernandez

solve instances within week CPU. large instances,
solutions relaxations problem known. One relaxations, known
symmetrical maximum density still life problem (SMDSLP), proposed Bosch
Trick (2002), consists considering symmetric boards (either horizontally
2
vertically) reduces search space 2n 2ndn/2e .
alone find vertically symmetric still lifes, considering variable domains
sets contain symmetric rows. Larrosa Morancho (2003) Larrosa et al.
(2005) used algorithm solve SMDSLP instances considered far
paper (i.e., n {12 . . 20}), well large instances (i.e., n {22, 24, 26, 28}).
results summarized Table 2, shows instance size optimal
symmetrical solution (as number dead cells). Clearly, cost optimal symmetric
still lifes upper bounds MDSLP, additionally observed
tight n 6 20. Results n > 20 currently best known solutions
instances.
also run algorithm (BS-MA-BE-MB) large instances (i.e., n
{22, 24, 26, 28}), compare results symmetrical solutions instances. Results (displayed Figure 8) show algorithm able find two new best known
solutions MDSLP, namely n = 24 n = 26. 275 324 dead
cells respectively new solutions. solutions pictured Figure 9.
also worth noting algorithm could also find solution 325 dead cells
n = 26 instance. instances, algorithm could reach best known solutions consistently. computation mini-Buckets large instances done
considering four clustered cost functions variables row board,
complexity using three cost functions still high.

5. Conclusions
Many problems modeled WCSPs. One exact technique used
tackle problems BE. However, high space complexity exact technique,
makes approach impractical large instances. case, one resort minibuckets get approximate solution, although complexity large.
work, presented several proposals hybridization MB
memetic algorithms beam search order get effective heuristics shown
represent promising models.
experimentally evaluated model MDSLP, excellent example
WCSP. highly constrained nature typical many optimization scenarios. difficulty solving problem illustrates limitations classical optimization approaches,
highlights capabilities proposed approaches. Indeed, experimental results positive, solving large instances MDSLP optimality. Among
different models presented, must distinguish new algorithm resulting
hybridization, different levels, complete solving techniques (i.e., bucket elimination),
incomplete deterministic methods (i.e., beam search mini-buckets) stochastic algorithms (i.e., memetic algorithms). algorithm empirically produces good-quality results,
solving optimality large instances constrained problem relatively
short time, also providing new best known solutions large instances.
550

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

future work, plan consider complete versions hybrid algorithm.
involves use appropriate data structures store yet considered promising
branch-and-bound nodes. memory requirements course grow enormously
size problem instance considered, interesting analyze computational tradeoffs algorithm anytime technique.

Acknowledgments
would like thank Javier Larrosa valuable comments, helped us improve
significantly preliminary version paper. Thanks also due reviewers
constructive comments. work partially supported Spanish MCInn
grant TIN2008-05941 (Nemesis).

Appendix A. MDSLP WCSP
shown Larrosa Morancho (2003) Larrosa et al. (2005), MDSLP
well formulated WCSPs. end, n n board configuration represented
n-dimensional vector (r1 , r2 , . . . , rn ). vector component encodes (as binary
string) row, j-th bit row ri (noted rij ) represents state j-th cell
i-th row (a value 1 represents live cell value 0 dead cell).
Two functions rows useful describe constraints must satisfied
valid configuration. first one,
X
zeroes(a) =
(1 ai ),
(1)
1 6i6n

returns number dead cells row (i.e., number zeroes binary string a).
second one,
Adjs(a) = Adjs 0 (a, 1 , 0 )

l,
0
Adjs (a, , l ) = Adjs 0 (a, + 1 , l + 1 ),

max(l, Adjs 0 (a, + 1 , 0 )),

(2)
i>n
ai = 1
ai = 0,

computes maximum number adjacent living cells row a. also introduce ternary
predicate, Stable(ri1 , r , ri+1 ), takes three consecutive rows board configuration
satisfied if, if, cells central row stable (i.e., cells row r
remain unchanged next iteration game):
Stable(a, b, c) =

V

16i6n S(a, b, c, i)

2 6 (a, b, c, i) 6 3, bi = 1
S(a, b, c, i) =
(a, b, c, i) 6= 3,
bi = 0
P
(a, b, c, i) = max(1,i1)6j6min(n,i+1) (aj + bj + cj ) bi ,

(3)

(a, b, c, i) number living neighbors cell bi , assuming c rows
row b.
551

fiGallardo, Cotta, & Fernandez

MDSLP formulated WCSP using n cost functions fi , {1 . . n}.
Accordingly, fn binary scope last two rows board (var(fn ) = {rn1 , rn })
defined as:

,
Stable(a, b, 0 ) Adjs(b) > 2
fn (a, b) =
(4)
zeroes(b), otherwise.
first line checks cells row rn stable, whereas second one checks
new cells produced nn board. Note pair rows representing
unstable configuration assigned cost , whereas stable one assigned number
dead cells (to minimized).
{2 . . n 1}, corresponding fi cost functions ternary scope var(fi ) =
{ri1 , ri , ri+1 } defined as:

,
fi (a, b, c) =
zeroes(b),

Stable(a, b, c) (a1 = b1 = c1 = 1 ) (an = bn = cn = 1 )
(5)
otherwise.

case, boundary conditions checked left right board. regards
cost function f1 , binary scope first two rows board (var(f1 ) = {r1 , r2 })
specified similarly fn :

,
Stable(0 , b, c) Adjs(b) > 2
(6)
f1 (b, c) =
zeroes(b), otherwise.

References
Ahuja, R. K., Ergun, O., Orlin, J. B., & Punnen, A. P. (2002). survey large-scale
neighborhood search techniques. Discrete Appl. Math., 123 (1-3), 75102.
Arnborg, S. (1985). Efficient algorithms combinatorial problems graphs bounded
decomposability - survey. BIT, 2, 223.
Back, T. (1996). Evolutionary Algorithms Theory Practice. Oxford University Press,
New York NY.
Back, T., Fogel, D., & Michalewicz, Z. (1997). Handbook Evolutionary Computation.
Oxford University Press, New York NY.
Barr, A., & Feigenbaum, E. (1981). Handbook Artificial Intelligence. Morhan Kaufmann,
New York NY.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press, New
York NY.
Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint satisfaction
optimization. Journal ACM, 44 (2), 201236.
Bosch, R., & Trick, M. (2002). Constraint programming hybrid formulations three
life designs. International Workshop Integration AI Techniques
Constraint Programming Combinatorial Optimization Problems, CP-AI-OR02, pp.
7791.
552

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

Cabon, B., de Givry, S., Lobjois, L., Schiex, T., & Warners, J. P. (1999). Radio link
frequency assignment. Constraints, 4 (1), 7989.
Cheng, K. C. K., & Yap, R. H. C. (2005). Ad-hoc global constraints life. van Beek,
P. (Ed.), Principles Practice Constraint Programming CP2005, Vol. 3709
Lecture Notes Computer Science, pp. 182195, Berlin Heidelberg. Springer.
Cheng, K. C. K., & Yap, R. H. C. (2006). Applying ad-hoc global constraints case
constraint still-life. Constraints, 11, 91114.
Cotta, C., & Troya, J. (2000). influence representation granularity heuristic
forma recombination. Carroll, J., Damiani, E., Haddad, H., & Oppenheim, D.
(Eds.), ACM Symposium Applied Computing 2000, pp. 433439. ACM Press.
Cotta, C., & Troya, J. (2003). Embedding branch bound within evolutionary algorithms.
Applied Intelligence, 18(2), 137153.
Culberson, J. (1998). futility blind search: algorithmic view free lunch.
Evolutionary Computation, 6 (2), 109128.
Dechter, R. (1997). Mini-buckets: general scheme generating approximations automated reasoning. 15th International Joint Conference Artificial Intelligence,
pp. 12971303, Nagoya, Japan.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113 (1-2), 4185.
Detcher, R., & Rish, I. (2003). Mini-buckets: general scheme bounded inference.
Journal ACM, 50 (2), 107153.
Eiben, A., Raue, P.-E., & Ruttkay, Z. (1994). Genetic algorithms multi-parent recombination. Davidor, Y., Schwefel, H.-P., & Manner, R. (Eds.), Parallel Problem
Solving Nature III, Vol. 866 Lecture Notes Computer Science, pp. 7887,
Berlin Heidelberg. Springer.
Elkies, N. D. (1998). still-life problem generalizations. Engel, P., & Syta, H.
(Eds.), Voronois Impact Modern Science, Book 1, pp. 228253. Institute Math,
Kyiv.
Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58 (1-3), 2170.
Gallardo, J. E., Cotta, C., & Fernandez, A. J. (2008). Finding still lifes memetic/exact
hybrid algorithms. CoRR, Available http://arxiv.org/abs/0812.4170.
Gallardo, J., Cotta, C., & Fernandez, A. (2007). hybridization memetic algorithms branch-and-bound techniques. IEEE Transactions Systems, Man
Cybernetics, part B, 37 (1), 7783.
Gallardo, J. E., Cotta, C., & Fernandez, A. J. (2006a). memetic algorithm bucket
elimination still life problem. Gottlieb, J., & Raidl, G. (Eds.), Evolutionary
Computation Combinatorial Optimization, Vol. 3906 Lecture Notes Computer
Science, pp. 7385, Berlin Heidelberg. Springer.
553

fiGallardo, Cotta, & Fernandez

Gallardo, J. E., Cotta, C., & Fernandez, A. J. (2006b). multi-level memetic/exact hybrid
algorithm still life problem. Runarsson, T. P., et al. (Eds.), Parallel Problem
Solving Nature IX, Vol. 4193 Lecture Notes Computer Science, pp. 212221,
Berlin Heidelberg. Springer.
Gardner, M. (1970). fantastic combinations John Conways new solitaire game.
Scientific American, 223, 120123.
Gelain, M., Pini, M. S., Rossi, F., & Venable, K. B. (2007). Dealing incomplete preferences soft constraint problems. Bessiere, C. (Ed.), Principles Practice
Constraint Programming CP 2007, Vol. 4741 Lecture Notes Computer Science,
pp. 286300, Berlin Heidelberg. Springer.
Glover, F. (1989). Tabu search part I. ORSA Journal Computing, 1 (3), 190206.
Glover, F. (1990). Tabu search part II. ORSA Journal Computing, 2 (1), 432.
Hart, W., Krasnogor, N., & Smith, J. (2005). Recent Advances Memetic Algorithms, Vol.
166 Studies Fuzziness Soft Computing. Springer, Berlin Heidelberg.
Kask, K., & Detcher, R. (2001). general scheme automatic generation search
heuristics specification dependencies. Artificial Intelligence, 129, 91131.
Khemmoudj, M. O. I., & Bennaceur, H. (2007). Valid inequality based lower bounds
WCSP. Bessiere, C. (Ed.), Principles Practice Constraint Programming
CP 2007, Vol. 4741 Lecture Notes Computer Science, pp. 394408, Berlin
Heidelberg. Springer.
Krasnogor, N., & Smith, J. (2005). tutorial competent memetic algorithms: model,
taxonomy, design issues. IEEE Transactions Evolutionary Computation, 9 (5),
474488.
Larrosa, J., & Morancho, E. (2003). Solving still life soft constraints bucket
elimination. Principles Practice Constraint Programming CP2003, Vol.
2833 Lecture Notes Computer Science, pp. 466479, Berlin Heidelberg. Springer.
Larrosa, J., Morancho, E., & Niso, D. (2005). practical use variable elimination
constraint optimization problems: still life case study. Journal Artificial
Intelligence Research, 23, 421440.
Larrosa, J., & Schiex, T. (2004). Solving weighted CSP maintaining arc consistency.
Artificial Intelligence, 159 (1-2), 126.
Lawler, E., & Wood, D. (1966). Branch bounds methods: survey. Operations Research,
4 (4), 669719.
Lematre, M., Verfaillie, G., Bourreau, E., & Laburthe, F. (2001). Integrating algorithms
weighted CSP constraint programming framework. International Workshop
Modelling Solving Problems Soft Constraints, Paphos, Cyprus.
Marinescu, R., & Dechter, R. (2007). Best-first and/or search graphical models.
Twenty-Second AAAI Conference Artificial Intelligence, pp. 11711176, Vancouver, Canada. AAAI Press.
Moscato, P., & Cotta, C. (2003). gentle introduction memetic algorithms. Handbook
Metaheuristics, pp. 105144. Kluwer Academic Press, Boston, Massachusetts, USA.
554

fiSolving WCSPs Memetic/Exact Hybrid Algorithms

Moscato, P., & Cotta, C. (2007). Memetic algorithms. Gonzalez, T. (Ed.), Handbook
Approximation Algorithms Metaheuristics, chap. 27. Chapman & Hall/CRC
Press.
Nonobe, K., & Ibaraki, T. (2001). improved tabu search method weighted
constraint satisfaction problem. INFOR, 39 (2), 131151.
Puchinger, J., & Raidl, G. (2005). Combining metaheuristics exact algorithms
combinatorial optimization: survey classification. Mira, J., & Alvarez, J.
(Eds.), Artificial Intelligence Knowledge Engineering Applications: Bioinspired
Approach, Vol. 3562 Lecture Notes Computer Science, pp. 4153, Berlin Heidelberg. Springer.
Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued constraint satisfaction problems: hard
easy problems. 14th International Joint Conference Artificial Intelligence,
pp. 631637, Montreal, Canada.
Smith, B. M. (2002). dual graph translation problem life. Hentenryck, P. V.
(Ed.), Principles Practice Constraint Programming - CP2002, Vol. 2470
Lecture Notes Computer Science, pp. 402414, Berlin Heidelberg. Springer.
Verfaillie, G., Lematre, M., & Schiex, T. (1996). Russian doll search solving constraint
optimization problems. Thirteenth National Conference Artificial Intelligence
Eighth Innovative Applications Artificial Intelligence Conference, AAAI / IAAI
96, pp. 181187. AAAI Press / MIT Press.
Wolpert, D., & Macready, W. (1997). free lunch theorems optimization. IEEE
Transactions Evolutionary Computation, 1 (1), 6782.

555

fiJournal Artificial Intelligence Research 35 (2009) 343-389

Submitted 08/08; published 06/09

Automated Reasoning Modal Description Logics via
SAT Encoding: Case Study Km /ALC-Satisfiability
Roberto Sebastiani

roberto.sebastiani@disi.unitn.it

Michele Vescovi

michele.vescovi@disi.unitn.it

DISI, Universita di Trento
Via Sommarive 14, I-38123, Povo, Trento, Italy

Abstract
last two decades, modal description logics applied numerous
areas computer science, including knowledge representation, formal verification, database
theory, distributed computing and, recently, semantic web ontologies.
reason, problem automated reasoning modal description logics
thoroughly investigated. particular, many approaches proposed efficiently
handling satisfiability core normal modal logic Km , notational variant,
description logic ALC. Although simple structure, Km /ALC computationally
hard reason on, satisfiability PSpace-complete.
paper start exploring idea performing automated reasoning tasks
modal description logics encoding SAT, handled stateof-the-art SAT tools; previous approaches, begin investigation
satisfiability Km . propose efficient encoding, test extensive
set benchmarks, comparing approach main state-of-the-art tools available.
Although encoding necessarily worst-case exponential, experiments
notice that, practice, approach handle problems
reach approaches, performances comparable with, even
better than, current state-of-the-art tools.

1. Motivations Goals
last two decades, modal description logics provided essential framework
many applications numerous areas computer science, including artificial intelligence, formal verification, database theory, distributed computing and, recently, semantic web ontologies. reason, problem automated reasoning modal
description logics thoroughly investigated (e.g., Fitting, 1983; Ladner, 1977;
Baader & Hollunder, 1991; Halpern & Moses, 1992; Baader, Franconi, Hollunder, Nebel, &
Profitlich, 1994; Massacci, 2000). particular, research modal description logics
followed two parallel routes seminal work Schild (1991), proved
core modal logic Km core description logic ALC one notational variant
other. Since then, analogous results produced bunch logics,
that, nowadays two research lines mostly merged one research flow.
Many approaches proposed efficiently reasoning modal description
logics, starting problem checking satisfiability core normal modal
logic Km notational variant, description logic ALC (hereafter simply Km ).
classify follows.
c
2009
AI Access Foundation. rights reserved.

fiSebastiani & Vescovi

classic tableau-based approach (Fitting, 1983; Baader & Hollunder, 1991; Massacci, 2000) based construction propositional tableau branches,
recursively expanded demand generating successor nodes candidate Kripke
model. Kris (Baader & Hollunder, 1991; Baader et al., 1994), Crack (Franconi,
1998), LWB (Balsiger, Heuerding, & Schwendimann, 1998) among main
representative tools approach.
DPLL-based approach (Giunchiglia & Sebastiani, 1996, 2000) differs
previous one mostly fact Davis-Putnam-Logemann-Loveland (DPLL)
procedure, treats modal subformulas propositions, used instead
classic propositional tableaux procedure nesting level modal operators. KSAT (Giunchiglia & Sebastiani, 1996), ESAT (Giunchiglia, Giunchiglia,
& Tacchella, 2002) *SAT (Tacchella, 1999), representative tools
approach.
two approaches merged modern tableaux-based approach,
extended work expressive description logics provide sophisticate
reasoning functions. Among tools employing approach, recall FaCT/FaCT++
DLP (Horrocks & Patel-Schneider, 1999), Racer (Haarslev & Moeller, 2001). 1
translational approach (Hustadt & Schmidt, 1999; Areces, Gennari, Heguiabehere,
& de Rijke, 2000) modal formula encoded first-order logic (FOL),
encoded formula decided efficiently FOL theorem prover (Areces et al.,
2000). Mspass (Hustadt, Schmidt, & Weidenbach, 1999) representative
tool approach.
CSP-based approach (Brand, Gennari, & de Rijke, 2003) differs tableauxbased DPLL-based ones mostly fact CSP (Constraint Satisfaction
Problem) engine used instead tableaux/DPLL. KCSP representative
tool approach.
Inverse-method approach (Voronkov, 1999, 2001), search procedure based
inverted version sequent calculus (which seen modalized
version propositional resolution). KK (Voronkov, 1999) representative
tool approach.
Automata-theoretic approach, (a symbolic representation based BDDs
Binary Decision Diagrams of) tree automaton accepting tree models
input formula implicitly built checked emptiness (Pan, Sattler, & Vardi,
2002; Pan & Vardi, 2003). KBDD (Pan & Vardi, 2003) representative tool
approach.
1. Notice universal agreement terminology tableaux-based DPLL-based.
E.g., tools like FaCT, DLP, Racer often called tableau-based, although use
DPLL-like algorithm instead propositional tableaux handling propositional component
reasoning (Horrocks, 1998; Patel-Schneider, 1998; Horrocks & Patel-Schneider, 1999; Haarslev & Moeller,
2001).

344

fiAutomated Reasoning Modal Description Logics via SAT Encoding

Pan Vardi (2003) presented also encoding K-satisfiability QBF-satisfiability
(which PSpace-complete too), combined use state-of-the-art QBF
(Quantified Boolean Formula) solver. call approach QBF-encoding approach.
best knowledge, last four approaches far restricted satisfiability
Km only, whilst translational approach applied numerous modal
description logics (e.g. traditional modal logics like Tm S4m , dynamic modal
logics) relational calculus.
significant amount benchmarks formulas produced testing effectiveness different techniques (Halpern & Moses, 1992; Giunchiglia, Roveri, & Sebastiani, 1996; Heuerding & Schwendimann, 1996; Horrocks, Patel-Schneider, & Sebastiani,
2000; Massacci, 1999; Patel-Schneider & Sebastiani, 2001, 2003).
last two decades also witnessed impressive advance efficiency
propositional satisfiability techniques (SAT), brought large previouslyintractable problems reach state-of-the-art SAT solvers. success SAT
technologies motivated impressive efficiency reached current implementations
DPLL procedure, (Davis & Putnam, 1960; Davis, Longemann, & Loveland, 1962),
most-modern variants (Silva & Sakallah, 1996; Moskewicz, Madigan, Zhao, Zhang, &
Malik, 2001; Een & Sorensson, 2004). Current implementations handle formulas
order 107 variables clauses.
consequence, many hard real-world problems successfully solved
encoding SAT (including, e.g., circuit verification synthesis, scheduling, planning,
model checking, automatic test pattern generation , cryptanalysis, gene mapping). Effective
encodings SAT proposed also satisfiability problems quantifier-free
FOL theories interest formal verification (Strichman, Seshia, & Bryant,
2002; Seshia, Lahiri, & Bryant, 2003; Strichman, 2002). Notably, successful SAT encodings
include also PSpace-complete problems, like planning (Kautz, McAllester, & Selman, 1996)
model checking (Biere, Cimatti, Clarke, & Zhu, 1999).
paper start exploring idea performing automated reasoning tasks
modal description logics encoding SAT, handled state-ofthe-art SAT tools; previous approaches, begin investigation
satisfiability Km .
theory, task may look hopeless worst-case complexity issues: fact,
exceptions, satisfiability problem modal description logics
NP, typically PSpace-complete even harder PSpace-complete Km (Ladner,
1977; Halpern & Moses, 1992) encoding worst-case non polynomial. 2
practice, however, considerations allow discarding approach
may competitive state-of-the-art approaches. First, non-polynomial bounds
worst-case bounds, formulas may different behaviors
pathological formulas found textbooks. (E.g., notice exponentiality
based hypothesis unboundedness parameter like modal depth;
Halpern & Moses, 1992; Halpern, 1995.) Second, tricks encoding may allow
reducing size encoded formula significantly. Third, amount RAM
2. implicitly make assumption NP 6= PSpace.

345

fiSebastiani & Vescovi

memory current computers order GBytes current SAT solvers
successfully handle huge formulas, encoding many modal formulas (at least
hard solve also competitors) may reach SAT solver.
Finally, even PSpace-complete logics like Km , also state-of-the-art approaches
guaranteed use polynomial memory.
paper show that, least satisfiability Km , exploiting smart
optimizations encoding SAT-encoding approach becomes competitive practice
previous approaches. extent, contributions paper manyfold.
propose basic encoding Km formulas purely-propositional ones, prove
encoding satisfiability-preserving.
describe optimizations encoding, form preprocessing
on-the-fly simplification. techniques allow significant (and cases
dramatic) reductions size resulting Boolean formulas, performances
SAT solver thereafter.
perform extensive empirical comparison main state-of-the-art
tools available. show that, despite NP-vs.-PSpace issue, approach
handle problems reach approaches,
performances comparable with, sometimes even better than,
current state-of-the-art tools. perspective, surprising
contribution paper.
byproduct work, obtain empirical evaluation current tools Km satisfiability available, extensive terms amount variety
benchmarks number representativeness tools evaluated.
aware evaluation recent literature.
also stress fact approach encoder interfaced every
SAT solver plug-and-play manner, benefit free every improvement
technology SAT solvers made available.
Content. paper structured follows. Section 2 provide necessary
background notions modal logics SAT. Section 3 describe basic encoding
Km SAT. Section 4 describe discuss main optimizations, provide
many examples. Section 5 present empirical evaluation, discuss results.
Section 6 present related work current research trends. Section 7
conclude, describe possible future evolutions.
six-page preliminary version paper, containing basic ideas presented
here, presented SAT06 conference (Sebastiani & Vescovi, 2006). readers
convenience, online appendix provided, containing plots Section 5 full size.
Moreover, order make results reproducible, encoder, benchmarks
random generators seeds used also available online appendix.
346

fiAutomated Reasoning Modal Description Logics via SAT Encoding

2. Background
section provide necessary background modal logic Km (Section 2.1)
SAT DPLL procedure (Section 2.2).
2.1 Modal Logic Km
recall basic definitions properties Km . Given non-empty set primitive
propositions = {A1 , A2 , . . .}, set modal operators B = {21 , . . . , 2m }, constants True False (that denote respectively > ) language
Km least set formulas containing A, closed set propositional connectives {, , , , } set modal operators B {31 , . . . , 3m }. Notationally,
use Greek letters , , , , , denote formulas language Km (Km -formulas
hereafter). Notice consider {, } together B group primitive connectives/operators, defining remaining standard way, is: 3r
2r , 1 2 (1 2 ), 1 2 (1 2 ), 1 2
(1 2 ) (2 1 ). (Hereafter formulas like implicitly assumed
simplified V
,
.) Notationally,

V
W mean V
W
W that, ,
often write ( li ) jVlj Wfor clause j li j lj , ( li ) ( j lj )
conjunction clauses j ( li lj ). Further, often write 2r 3r meaning one
specific/generic modal operator, assumed r = 1, . . . , m; denote

2ir nested application 2r operator times: 20r := 2i+1
r := 2r 2r .
call depth , written depth(), maximum number nested modal operators .
call propositional atom every primitive proposition A, propositional literal
every propositional atom (positive literal) negation (negative literal). call modal
atom every formula either form 2r form 3r .
order make presentation uniform, avoid considering polarity
subformulas, adopt traditional representation Km -formulas (introduced, far
know, Fitting, 1983 widely used literature, e.g. Fitting, 1983; Massacci,
2000; Donini & Massacci, 2000) following table:

(1 2 )
(1 2 )
(1 2 )

1
1
1
1

2
2
2
2


(1 2 )
(1 2 )
(1 2 )

1
1
1
1

2
2
2
2

r
3r 1
2r 1

0r
1
1

r
2r 1
3r 1

0r
1
1

non-literal Km -formulas grouped four categories: (conjunctive),
(disjunctive), (existential), (universal). Importantly, formulas occur
main formula positive polarity only. allows disregarding issue polarity
subformulas.
semantic modal logics given means Kripke structures. Kripke structure
Km tuple = hU, L, R1 , . . . , Rm i, U set states, L function
L : U 7 {T rue, F alse}, Rr binary relation states U.
abuse notation write u instead u U. call situation pair M, u,
Kripke structure u M. binary relation |= modal formula
347

fiSebastiani & Vescovi

situation M, u defined follows:
M, u |= >;
M, u 6|= ;
M, u |= Ai , Ai
M, u |= Ai , Ai
M, u |=
M, u |=
M, u |= r
M, u |= r








L(Ai , u) = rue;
L(Ai , u) = F alse;
M, u |= 1 M, u |= 2 ;
M, u |= 1 M, u |= 2 ;
M, w |= 0r w U s.t. Rr (u, w) holds M;
M, w |= 0r every w U s.t. Rr (u, w) holds M.

M, u |= read M, u satisfy Km (alternatively, M, u Km -satisfies
). say Km -formula satisfiable Km (Km -satisfiable henceforth)
exist u s.t. M, u |= . (When causes ambiguity, sometimes
drop prefix Km -.) say w successor u Rr iff Rr (u, w) holds
M.
problem determining Km -satisfiability Km -formula decidable
PSPACE-complete (Ladner, 1977; Halpern & Moses, 1992), even restricting language
single Boolean atom (i.e., = {A1 }; Halpern, 1995); impose bound modal
depth Km -formulas, problem reduces NP-complete (Halpern, 1995).
detailed description Km including, e.g., axiomatic characterization, decidability
complexity results refer reader works Halpern Moses (1992),
Halpern (1995).
Km -formula said Negative Normal Form (NNF) written terms
symbols 2r , 3r , , propositional literals Ai , Ai (i.e., negations occur
propositional atoms A). Every Km -formula converted equivalent
one NNF () recursively applying rewriting rules: 2r =3r , 3r =2r ,
(1 2 )=(1 2 ), (1 2 )=(1 2 ), =.
Km -formula said Box Normal Form (BNF) (Pan et al., 2002; Pan & Vardi,
2003) written terms symbols 2r , 2r , , , propositional literals Ai ,
Ai (i.e., diamonds there, negations occur boxes
propositional atoms A). Every Km -formula converted equivalent one
BNF () recursively applying rewriting rules: 3r =2r , (1 2 )=(1
2 ), (1 2 )=(1 2 ), =.
2.2 Propositional Satisfiability DPLL Algorithm
state-of-the-art SAT procedures evolutions DPLL procedure (Davis &
Putnam, 1960; Davis et al., 1962). high-level schema modern DPLL engine, adapted
one presented Zhang Malik (2002), reported Figure 1. Boolean
formula CNF (Conjunctive Normal Form); assignment initially empty,
updated stack-based manner.
main loop, decide next branch(, ) chooses unassigned literal l
according heuristic criterion, adds . (This operation called decision,
l called decision literal number decision literals operation
called decision level l.) inner loop, deduce(, ) iteratively deduces literals l
348

fiAutomated Reasoning Modal Description Logics via SAT Encoding

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.

SatValue DPLL (formula , assignment ) {
(1) {
decide next branch(, );
(1) {
status = deduce(, );
(status == sat)
return sat;
else (status == conflict) {
blevel = analyze conflict(, );
(blevel == 0) return unsat;
else backtrack(blevel,, );
}
else break;
}}}

Figure 1: Schema modern SAT solver engine based DPLL.
deriving current assignment updates accordingly; step repeated
either satisfies , falsifies , literals deduced, returning sat,
conflict unknown respectively. (The iterative application Boolean deduction steps
deduce also called Boolean Constraint Propagation, BCP.) first case, DPLL returns
sat. second case, analyze conflict(, ) detects subset caused
conflict (conflict set) decision level blevel backtrack. blevel == 0,
conflict exists even without branching, DPLL returns unsat. Otherwise,
backtrack(blevel, , ) adds clause (learning) backtracks blevel
(backjumping), updating accordingly. third case, DPLL exits inner loop,
looking next decision.
Notably, modern DPLL implementations implement techniques, like two-watchedliteral scheme, allow extremely efficient handling BCP (Moskewicz et al., 2001;
Zhang & Malik, 2002). Old versions DPLL used implement also Pure-Literal Rule
(PLR) (Davis et al., 1962): one proposition occurs positively (resp. negatively)
formula, safely assigned true (resp. false). Modern DPLL implementations,
however, often implement anymore due computational cost. much
deeper description modern DPLL-based SAT solvers, refer reader literature
(e.g., Zhang & Malik, 2002).

3. Basic Encoding
borrow notation Single Step Tableau (SST) framework (Massacci, 2000;
Donini & Massacci, 2000). represent uniquely states labels , represented
non empty sequences integers 1.nr11 .nr22 . ... .nrkk , s.t. label 1 represents root state,
.nr represents n-th Rr -successor (where r {1, . . . , m}). little abuse
notation, hereafter may say state meaning state labeled . call
labeled formula pair h, i, state label Km -formula,
349

fiSebastiani & Vescovi

call labeled subformulas labeled formula h, labeled formulas h,
subformula .
Let Ah , injective function maps labeled formula h, i, s.t.
form , Boolean variable Ah, . conventionally assume Ah, >i
> Ah, . Let Lh, denote Ah, form , Ah, otherwise.
Given Km -formula , encoder Km 2SAT builds Boolean CNF formula follows: 3
def

Km 2SAT () = Ah1,



(1)

Def (1, )

def

(2)

def

(3)

def

(4)

def

(5)

Def (, >) = >
Def (, ) = >
Def (, Ai ) = >
Def (, Ai ) = >
def

Def (, ) = (Lh,



(Lh,

1

Lh,

2 ))

Def (, 1 ) Def (, 2 )

(6)



(Lh,

1

Lh,

2 ))

Def (, 1 ) Def (, 2 )

(7)

def

Def (, ) = (Lh,
r,j

0r,j )

def

Def (, ) = (Lh, r,j Lh.j, r,j ) Def (.j,
0
^
def
r
Def (, ) =
((Lh, r Lh, r,i ) Lh.i,


r
r
)

Def
(.i,

)
.
0
0

(8)
(9)

every
h, r,i

r,j mean r,j j-th distinct r formula labeled . Notice
Nn
(6) (7) generalize case n-ary obvious way:
i=1
V
N
N
def
s.t.
{, }, Def (, ) = (Lh, ni=1 Lh, ) ni=1 Def (, ). Although
conceptually
trivial, fact important practical consequence: order encode
Nn

one
needs
adding one Boolean variable rather n1, see Section 4.2.
i=1
Notice also rule (9) literals type Lh, r,i strictly necessary; fact,
SAT problem must consider encode possibly occuring states,
case, e.g., r,i formula occurring disjunction assigned false particular
state label (which, SAT, corresponds assign Lh, r,i false). situation
labeled formulas regarding state label .i useless, particular generated
expansion formulas interacting r,i . 4
assume Km -formulas represented DAGs (Direct Acyclic Graphs),
avoid expansion Def (, ) once. various
Def (, ) expanded breadth-first manner wrt. tree labels, is,
possible expansions (newly introduced) completed starting
expansions different state label 0 , different state label expanded
order introduced (thus expansions given state always handled
deeper state). Moreover, following done Massacci (2000),
assume that, , Def (, )s expanded order: /, , . Thus,
Def (, r ) expanded expansion Def (, r,i )s, Def (, r )
3. say formula CNF represent clauses implications, according notation
described beginning Section 2.
4. Indeed, (9) finite conjunction. fact number -subformulas obviously finite Km
benefits finite-tree-model property (see, e.g., Pan et al., 2002; Pan & Vardi, 2003).

350

fiAutomated Reasoning Modal Description Logics via SAT Encoding

generate one clause ((Lh, r Lh, r,i ) Lh.i, 0r ) one novel definition Def (.i, 0r )
Def (, r,i ) expanded. 5
Intuitively, easy see Km 2SAT () mimics construction SST tableau
expansion (Massacci, 2000; Donini & Massacci, 2000). following fact.
Theorem 1. Km -formula Km -satisfiable corresponding Boolean
formula Km 2SAT () satisfiable.
complete proof Theorem 1 found Appendix A.
Notice that, due (9), number variables clauses Km 2SAT () may grow
exponentially depth(). accordance stated Halpern
Moses (1992).
Example 3.1 (NNF). Let nnf (3A1 3(A2 A3 )) 2A1 2A2 2A3 . 6
easy see nnf K1 -unsatisfiable: 3-atoms impose least one atom Ai
true least one successor root state, whilst 2-atoms impose atoms
Ai false successor states root state. Km 2SAT (nnf ) is: 7
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.













(
(
(
(
(
(
(
(
(
(
(

Ah1, nnf
Ah1, nnf (Ah1, 3A1 3(A2 A3 )i Ah1, 2A1 Ah1,
Ah1, 3A1 3(A2 A3 )i (Ah1, 3A1 Ah1, 3(A2 A3 )i ) )
Ah1, 3A1 Ah1.1, A1 )
Ah1, 3(A2 A3 )i Ah1.2, A2 A3 )
(Ah1, 2A1 Ah1, 3A1 ) Ah1.1, A1 )
(Ah1, 2A2 Ah1, 3A1 ) Ah1.1, A2 )
(Ah1, 2A3 Ah1, 3A1 ) Ah1.1, A3 )
(Ah1, 2A1 Ah1, 3(A2 A3 )i ) Ah1.2, A1 )
(Ah1, 2A2 Ah1, 3(A2 A3 )i ) Ah1.2, A2 )
(Ah1, 2A3 Ah1, 3(A2 A3 )i ) Ah1.2, A3 )
Ah1.2, A2 A3 (Ah1.2, A2 Ah1.2, A3 ) )

(1)
2A2 Ah1,

2A3 ) )

(6)
(7)
(8)
(8)
(9)
(9)
(9)
(9)
(9)
(9)
(7)

run Boolean constraint propagation (BCP), 3. reduces implicate disjunction. first element Ah1, 3A1 assigned true, BCP conflict 4.
6. set false, second element Ah1, 3(A2 A3 )i assigned true,
BCP conflict 12. Thus Km 2SAT (nnf ) unsatisfiable.
3

4. Optimizations
basic encoding Section 3 rather naive, much improved many extents,
order reduce size output propositional formula, make easier solve
DPLL, both. distinguish two main kinds optimizations:
5. practice, even definition Km 2SAT recursive, Def expansions performed grouped
states. precisely, Def (.n, ) expansions, formula every defined n, done
together (in /, , order exposed) necessarily Def (, ) expansions
completed.
6. K1 -formulas omit box diamond indexes, i.e., write 2, 3 21 , 31 .
7. examples report end line, i.e. clause, number Km 2SAT
encoding rule applied generate clause. also drop application rules (2), (3), (4)
(5).

351

fiSebastiani & Vescovi

Preprocessing steps, applied input modal formula encoding.
Among them, Pre-conversion BNF (Section 4.1), Atom Normalization
(Section 4.2), Box Lifting (Section 4.3), Controlled Box Lifting (Section 4.4).
On-the-fly simplification steps, applied Boolean formula construction. Among them, On-the-fly Boolean Simplification Truth Propagation Boolean Operators (Section 4.5) Truth Propagation
Modal Operators (Section 4.6), On-the-fly Pure-Literal Reduction (Section 4.7),
On-the-fly Boolean Constraint Propagation (Section 4.8).
analyze techniques detail.
4.1 Pre-conversion BNF
Many systems use pre-convert input Km -formulas NNF (e.g., Baader et al.,
1994; Massacci, 2000). approach, instead, pre-convert BNF (like, e.g.,
Giunchiglia & Sebastiani, 1996; Pan et al., 2002). approach, advantage
latter representation that, one 2r occurs positively negatively (like, e.g.,
(2r ...) (2r ...) ...), occurrences 2r labeled
Boolean atom Ah, 2r , hence always assigned truth value DPLL.
NNF, instead, negative occurrence 2r rewritten 3r (nnf ()),
two distinct Boolean atoms Ah, 2r (nnf ())i Ah, 3r (nnf ())i generated; DPLL
assign truth value, creating hidden conflict may require extra
Boolean search reveal. 8
Example 4.1 (BNF). consider BNF variant nnf formula Example 3.1,
bnf = (2A1 2(A2 A3 )) 2A1 2A2 2A3 . before, easy
see bnf K1 -unsatisfiable. Km 2SAT (bnf ) is: 9
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.













(
(
(
(
(
(
(
(
(
(
(

Ah1, bnf
Ah1, bnf (Ah1, (2A1 2(A2 A3 ))i Ah1, 2A1 Ah1, 2A2 Ah1,
Ah1, (2A1 2(A2 A3 ))i (Ah1, 2A1 Ah1, 2(A2 A3 )i ) )
Ah1, 2A1 Ah1.1, A1 )
Ah1, 2(A2 A3 )i Ah1.2, (A2 A3 )i )
(Ah1, 2A1 Ah1, 2A1 ) Ah1.1, A1 )
(Ah1, 2A2 Ah1, 2A1 ) Ah1.1, A2 )
(Ah1, 2A3 Ah1, 2A1 ) Ah1.1, A3 )
(Ah1, 2A1 Ah1, 2(A2 A3 )i ) Ah1.2, A1 )
(Ah1, 2A2 Ah1, 2(A2 A3 )i ) Ah1.2, A2 )
(Ah1, 2A3 Ah1, 2(A2 A3 )i ) Ah1.2, A3 )
Ah1.2, (A2 A3 )i (Ah1.2, A2 Ah1.2, A3 ) )

(1)
2A3 )) (6)
(7)
(8)
(8)
(9)
(9)
(9)
(9)
(9)
(9)
(7)

Unlike NNF formula nnf Example 3.1, Km 2SAT (bnf ) found unsatisfiable
directly BCP. fact, unit-propagation Ah1, 2A1 2. causes Ah1, 2A1
8. Notice consideration holds every representation involving boxes diamonds;
refer NNF simply popular representations.
9. Notice valid clause 6. dropped. See explanation Section 4.5.

352

fiAutomated Reasoning Modal Description Logics via SAT Encoding

3. false, one two (unsatisfiable) branches induced disjunction
cut priori. nnf , Km 2SAT recognize 2A1 3A1 one
negation other, two distinct atoms Ah1, 2A1 Ah1, 3A1 generated.
Hence Ah1, 2A1 Ah1, 3A1 cannot recognized DPLL one negation
other, s.t. DPLL may need exploring one Boolean branch more.
3
following assume formulas BNF (although optimizations follow work also representations).
4.2 Normalization Modal Atoms
One potential source inefficiency DPLL-based procedures occurrence
input formula semantically-equivalent though syntactically-different modal atoms 0
00 (e.g., 21 (A1 A2 ) 21 (A2 A1 )), recognized Km 2SAT .
causes introduction duplicated Boolean atoms Ah, 0 Ah, 00 much
worse duplicated subformulas Def (, 0 ) Def (, 00 ). fact
negative consequences, particular 0 00 occur negative polarity,
causes creation distinct versions successor states, duplication
whole parts output formula.
Example 4.2. Consider Km -formula (1 21 (A2 A1 )) (2 21 (A1 A2 )) 3 ,
s.t. 1 , 2 , 3 possibly-big Km -formulas. Km 2SAT creates two distinct atoms
Ah1, 21 (A2 A1 )i Ah1, 21 (A1 A2 )i two distinct formulas Def (1, 21 (A2 A1 ))
Def (1, 21 (A1 A2 )). latter cause creation two distinct states 1.1 1.2.
Thus, recursive expansion 21 -formulas occurring positively 1 , 2 , 3
duplicated two states.
3
order cope problem, done Giunchiglia Sebastiani (1996),
apply normalization steps modal atoms intent rewriting many
possible syntactically-different semantically-equivalent modal atoms syntacticallyidentical ones. achieved recursive application simple validitypreserving rewriting rules.
Sorting: modal atoms internally sorted according criterion, atoms
identical modulo reordering rewritten atom (e.g., 2i (2
1 ) 2i (1 2 ) rewritten 2i (1 2 )).
Flattening: associativity exploited combinations
flattened n-ary respectively (e.g., 2i (1 (2 3 )) 2i ((1
2 ) 3 ) rewritten 2i (1 2 3 )).
Flattening also advantage reducing number novel atoms introduced
encoding, consequence fact noticed Section 3. One possible drawback
technique reduce sharing subformulas (e.g., 2i ((1 2 ) 3 )
2i ((1 2 ) 4 ), common part shared). However, empirically
experienced drawback negligible wrt. advantages flattening.
353

fiSebastiani & Vescovi

4.3 Box Lifting
second preprocessing Km -formula also rewritten recursively applying
Km -validity-preserving box lifting rules:
(2r 1 2r 2 ) = 2r (1 2 ),

(2r 1 2r 2 ) = 2r (1 2 ).

(10)

potential benefit reducing number r formulas, hence number
labels .i take account expansion Def (, r )s (9). call lifting
preprocessing.
Example 4.3 (Box lifting). apply rules (10) formula Example 4.1,
bnf lift = 2(A1 A2 A3 ) 2(A1 A2 A3 ). Consequently,
Km 2SAT (bnf lift ) is:
1.
2.
3.
4.
5.
6.







Ah1, bnf lift
( Ah1, bnf lift (Ah1, 2(A1 A2 A3 )i Ah1, 2(A1 A2 A3 )i ) )
( Ah1, 2(A1 A2 A3 )i Ah1.1, (A1 A2 A3 )i )
(( Ah1, 2(A1 A2 A3 )i Ah1, 2(A1 A2 A3 )i ) Ah1.1, (A1 A2 A3 )i )
( Ah1.1, (A1 A2 A3 )i (Ah1.1, A1 Ah1.1, A2 Ah1.1, A3 ) )
( Ah1.1, (A1 A2 A3 )i (Ah1.1, A1 Ah1.1, A2 Ah1.1, A3 ) ).

(1)
(6)
(8)
(9)
(7)
(6)

Km 2SAT (bnf lift ) found unsatisfiable directly BCP clauses 1. 2.. one
successor state (1.1) considered. Notice 3., 4., 5. 6. redundant, 1.
2. alone unsatisfiable. 10
3
4.4 Controlled Box Lifting
One potential drawback applying lifting rules that, collapsing formula
(2r 1 2r 2 ) 2r (1 2 ) (2r 1 2r 2 ) 2r (1 2 ), possibility
sharing box subformulas DAG representation input Km -formula reduced.
order cope problem provide alternative policy applying box
lifting, is, apply rules (10) neither box subformula occurring
implicant (10) multiple occurrences. call policy controlled box lifting.
Example 4.4 (Controlled Box Lifting). apply Controlled Box Lifting formula
Example 4.1, bnf clift = (2A1 2(A2 A3 )) 2A1 2(A2 A3 )
since rules (10) applied among box subformulas except 2A1 ,
10. actual implementation, trivial cases like bnf lift found unsatisfiable directly
construction DAG representations, encoding never generated.

354

fiAutomated Reasoning Modal Description Logics via SAT Encoding

shared. follows Km 2SAT (bnf clift ) is:
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.













(
(
(
(
(
(
(
(
(
(
(

Ah1, bnf clift
Ah1, bnf clift (Ah1, (2A1 2(A2 A3 ))i Ah1, 2A1 Ah1, 2(A2 A3 )i )
Ah1, (2A1 2(A2 A3 ))i (Ah1, 2A1 Ah1, 2(A2 A3 )i ) )
Ah1, 2A1 Ah1.1, A1 )
Ah1, 2(A2 A3 )i Ah1.2, (A2 A3 )i )
(Ah1, 2A1 Ah1, 2A1 ) Ah1.1, A1 )
(Ah1, 2(A2 A3 )i Ah1, 2A1 ) Ah1.1, (A2 A3 )i )
(Ah1, 2A1 Ah1, 2(A2 A3 )i ) Ah1.2, A1 )
(Ah1, 2(A2 A3 )i Ah1, 2(A2 A3 )i ) Ah1.2, (A2 A3 )i )
Ah1.1, (A2 A3 )i (Ah1.1, A2 Ah1.1, A3 ) )
Ah1.2, (A2 A3 )i (Ah1.2, A2 Ah1.2, A3 ) )
Ah1.2, (A2 A3 )i (Ah1.2, A2 Ah1.2, A3 ) )

(1)
(6)
(7)
(8)
(8)
(9)
(9)
(9)
(9)
(6)
(7)
(6)

Km 2SAT (bnf clift ) found unsatisfiable directly BCP clauses 1., 2. 3.. Notice
unit propagation Ah1, 2A1 Ah1, 2(A2 A3 )i 2. causes implicate
disjunction 3. false.
3
4.5 On-the-fly Boolean Simplification Truth Propagation
first straightforward on-the-fly optimization applying recursively standard
rewriting rules Boolean simplification formula like, e.g.,
h, h,
h, 1 h, (1 2 )i
h, h,
...,

= h, i,
= h, 1 i,
= h, i,

h, h,
h, 1 h, (1 2 )i
h, h,

= h, i,
= h, 1 i,
= h, >i,

propagation truth/falsehood Boolean operators like, e.g.,
h,
h, h, >i
h, h, >i
....

= h, >i,
= h, i,
= h, >i,

h, >i
h, h,
h, h,

= h, i,
= h, i,
= h, i,

Example 4.5. consider Km -formula bnf lift = 2(A1 A2 A3 )
2(A1 A2 A3 ) Example 4.3 apply Boolean simplification rule h,
h, = h, i, h, bnf lift simplified h, i.
3
One important subcase on-the-fly Boolean simplification avoids useless encoding
incompatible r r formulas. BNF, fact, subformula 2r may occur
state positively negatively (like r = 2r r = 2r ).
so, Km 2SAT labels occurrences 2r Boolean atom Ah, 2r ,
produces recursively two distinct subsets clauses encoding, applying (8)
2r (9) 2r respectively. However, latter step (9) generates valid clause
(Ah, 2r Ah, 2r ) Ah.i, , avoid generating it. Consequently,
355

fiSebastiani & Vescovi

Ah.i, occurs formula, Def (.i, ) generated,
need defining h.i, i. 11
Example 4.6. apply observation construction formulas Examples
4.1 4.4, following facts:
formula Km 2SAT (bnf ) Example 4.1, clause 6. valid thus dropped.
formula Km 2SAT (bnf clift ) Example 4.4, valid clauses 6. 9.
dropped, 12. generated.
3
Hereafter assume on-the-fly Boolean simplification applied also combination
techniques described next sections.
4.6 On-the-fly Truth Propagation Modal Operators
Truth falsehood derive application techniques Section 4.5,
Section 4.7 Section 4.8 may propagated on-the-fly also though modal operators.
First, every , positive negative instances h, 2r >i safely simplified
applying rewriting rule h, 2r >i = h, >i.
Second, notice following fact. positive occurrence h, 2r
(we suppose wlog. r -formula ), 12 definition
(8) (9)
Def (, 2r ) = (Lh,

2r

Def (, 2r ) = ((Lh,

2r

Ah.j,
Lh,

>i )

Def (.j, >),

(11)

Lh.j,

(12)

2r )

)

Def (.j, )

new label .j every 2r occurring positively . Def (, 2r ) reduces
> Ah.j, >i Def (.j, >) reduce >. least another distinct formula 2r occurs positively , however, need .j label (11)
(12) new label, re-use instead label .i introduced expansion
Def (, 2r ), follows:
Def (, 2r ) = (Lh,

2r

Lh.i,

)

Def (.i, ).

(13)

Thus (11) dropped and, every h, 2r occurring positively, write:
Def (, 2r ) = ((Lh,

2r

Lh,

2r )

Lh.i,

)

Def (.i, )

(14)

instead (12). (Notice label .i introduced (13) rather label .j (11).)
motivated fact Def (, 2r ) forces existence least one
successor imposes constraints formulas hold there,
use already-defined successor state, any. fact important
benefit eliminating useless successor states encoding.
11. due fact may case Ah.i, generated anyway
expansion subformula, like, e.g., 2r ( ). case, Def (.i, ) must
generated anyway.
12. E.g., 2r may result applying steps Section 4.1 Section 4.5 2r (2r A1 3r A1 ).

356

fiAutomated Reasoning Modal Description Logics via SAT Encoding

Example 4.7. Let BNF K-formula:
(A1 2A2 ) (A1 2) (A1 A3 ) (A1 A3 ) (A1 2A4 ) 2A4 .
K-inconsistent, possible assignment {A1 , 2, 2A4 , 2A4 },
K-inconsistent. Km 2SAT () encoded follows:
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.

Ah1,
(Ah1, (Ah1, (A1 2A2 )i Ah1, (A1 2)i Ah1,
Ah1, (A1 2A4 )i Ah1, 2A4 ))
(Ah1, (A1 2A2 )i (Ah1, A1 Ah1, 2A2 ))
(Ah1, (A1 2)i (Ah1, A1 Ah1, 2i ))
(Ah1, (A1 A3 )i (Ah1, A1 Ah1, A3 ))
(Ah1, (A1 A3 )i (Ah1, A1 Ah1, A3 ))
(Ah1, (A1 2A4 )i (Ah1, A1 Ah1, 2A4 ))
(Ah1, 2A2 Ah1.1, A2 )
((Ah1, 2A4 Ah1, 2A2 ) Ah1.1, A4 )
((Ah1, 2A4 Ah1, 2A2 ) Ah1.1, A4 )
(Ah1, 2i Ah1.1, )

12.
13.

((Ah1,
((Ah1,

Ah1, 2i ) Ah1.1, A4 )
2A4 Ah1, 2i ) Ah1.1, A4 )
2A4

(1)
(A1 A3 )i
(6)
(7)
(7)
(7)
(7)
(7)
(8)
(9)
(9)
(8)
(9)
(9)

Clause 11. simplified >. (In practical implementation even generated.)
Notice clauses 11., 12. 13. used label 1.1 clauses 8., 9. 10. rather
new label 1.2. Thus, one successor label generated.
DPLL run Km 2SAT (), BCP 1. 2. immediately satisfied
implicants removed 3., 4., 5., 6.. Thanks 5. 6., Ah1, A1 assigned
false, causes 3. satisfied forces assignment literals Ah1, 2i ,
Ah1, 2A4 BCP 3. 7. hence Ah1.1, , Ah1.1, A4 Ah1.1, A4 BCP
12. 13., causing contradiction.
3
worth noticing (14) strictly necessary correctness encoding
even another -formula occurs . (E.g., Example 4.7, without 12. 13.
formula Km 2SAT () would become satisfiable Ah1, 2A2 could safely assigned
true DPLL, would satisfy 8., 9. 10..)
Hereafter assume technique applied also combination techniques described Section 4.5 next sections.
4.7 On-the-fly Pure-Literal Reduction
Another technique, evolved proposed Pan Vardi (2003), applies PureLiteral Reduction (PLR) on-the-fly construction Km 2SAT ().
label clauses containing atoms form Ah, generated,
occurs positively [resp. negatively], safely assigned true [resp.
false], hence clauses containing Ah, dropped. 13 consequence,
13. actual implementation reduction performed directly within intermediate data structure,
clauses never generated.

357

fiSebastiani & Vescovi

atom Ah,
reached.

0

become pure, process repeated fixpoint

Example 4.8. Consider formula bnf Example 4.1. construction
Km 2SAT (bnf ), 1.-8. generated, clause containing atoms form
Ah1.1, generated. notice Ah1.1, A2 Ah1.1, A3 occur negatively, safely assigned false. Therefore, 7. 8. safely
dropped. discourse applies lately Ah1.2, A1 9.. resulting formula found
inconsistent BCP. (In fact, notice Example 4.1 atoms Ah1.1, A2 , Ah1.1, A3 ,
Ah1.2, A1 play role unsatisfiability Km 2SAT (bnf ).)
3
remark differences PLR Pure-Literal Reduction technique proposed Pan Vardi (2003). KBDD (Pan et al., 2002; Pan & Vardi, 2003),
Pure-Literal Reduction preprocessing step applied input modal formula,
either global level (i.e. looking pure-polarity primitive propositions whole formula) or, effectively, different modal depths (i.e. looking pure-polarity primitive
propositions subformulas nesting level modal operators).
technique much fine-grained, PLR applied on-the-fly single-state
granularity, obtaining much stronger reduction effect.
Example 4.9. Consider BNF Km -formula bnf discussed Examples 4.1
4.8: bnf = (2A1 2(A2 A3 )) 2A1 2A2 2A3 . immediate see
primitive propositions A1 , A2 , A3 occur every modal depth polarities,
technique Pan Vardi (2003) produces effect formula.
3
4.8 On-the-fly Boolean Constraint Propagation
One major problem basic encoding Section 3 purely-syntactic,
is, consider possible truth values subformulas, effect
propagation Boolean modal connectives. particular, Km 2SAT applies
(8) [resp. (9)] every -subformula [resp. -subformula], regardless fact truth
values deterministically assigned labeled subformulas h1, may
allow dropping labeled -/-subformulas, thus prevent need encoding
them.
One solution problem applying Boolean Constraint Propagation (BCP)
on-the-fly construction Km 2SAT (), starting fact Ah1, must
true. contradiction found, Km 2SAT () unsatisfiable, formula
expanded further, encoder returns formula . 14 BCP allows
dropping one implication (6)-(9) without assigning implicate literals,
namely Lh, , h, needs defined, Def (, ) must
expanded. 15 Importantly, dropping Def (, r,j ) -formula h, r,j prevents
generating label .j (8) successor labels .j. 0 (corresponding subtree
states rooted .j), corresponding labeled subformulas encoded.
14. sake compatibility standard SAT solvers, actual implementation returns formula
A1 A1 .
15. make consideration Footnote 11: Lh.j, generated also expansion
subformula, (e.g., 2r ( )), (another instance of) Def (.i, ) must generated
anyway.

358

fiAutomated Reasoning Modal Description Logics via SAT Encoding

Example 4.10. Consider Example 4.1, suppose apply on-the-fly BCP.
construction 1., 2. 3. Km 2SAT (bnf ), atoms Ah1, bnf , Ah1, (2A1 2(A2 A3 ))i ,
Ah1, 2A1 , Ah1, 2A2 Ah1, 2A3 deterministically assigned true BCP.
causes removal 3. first-implied disjunct Ah1, 2A1 , need
generate Def (1, 2A1 ), hence label 1.1. defined 4. generated.
building 5., Ah1.2, (A2 A3 )i , unit-propagated. label 1.1. defined, 6.,
7. 8. generated. construction 5., 9., 10., 11. 12.,
applying BCP contradiction found, Km 2SAT () .
analogous situation happens bnf lift Example 4.3: building 1. 2.
contradiction found BCP, s.t. Km 2SAT returns without expanding formula
further. discourse holds bnf clift Example 4.4: building 1., 2. 3.
contradiction found BCP, s.t. Km 2SAT returns without expanding formula
further.
3
4.9 Paradigmatic Example: Halpern & Moses Branching Formulas.
Among optimizations described Section 4, on-the-fly BCP far
effective. order better understand fact, consider paradigmatic example
branching formulas K
h Halpern Moses (1992, 1995) (also called k branch n
set benchmark formulas proposed Heuerding Schwendimann, 1996)
unsatisfiable version (called k branch p above-mentioned benchmark suite).
Given single modality 2, integer parameter h, primitive propositions
16
D0 , . . . , Dh+1 , P1 , . . . , Ph , formulas K
h defined follows:
K
h

def

= D0 D1

h
^

2i (depth determined branching),

(15)

i=0
def

depth =

h+1
^

(Di Di1 ),

i=1
h
^


( Pi 2(Di Pi ))
determined =
Di
,
(Pi 2(Di Pi ))
i=1

h1
^
3(Di+1 Di+2 Pi+1 )
def
branching =
(Di Di+1 )
3(Di+1 Di+2 Pi+1 )
def



(16)



(17)

. (18)

i=0

conjunction formulas depth, determined branching repeated every
nesting level modal operators (i.e. every depth): depth captures relation
Di every level; determined states that, Pi true [false] state depth i,
true [false] successor states depth i; branching states that, every
node depth i, possible find two successor states depth + 1 Pi+1
true one false other. value parameter h, K
h K-satisfiable,
every Kripke model satisfies least 2h+1 1 states. fact, K
h build
way force construction binary-tree Kripke model depth h + 1,
16. sake better readability, adopt description given Halpern Moses (1992)
without converting formulas BNF. fact affect discussion.

359

fiSebastiani & Vescovi

whose leaves encodes distinct truth assignment primitive propositions P1 , . . . , Ph ,
whilst Di true states occurring depth tree (and
thus denotes level nesting).
unsatisfiable counterpart formulas proposed Heuerding Schwendimann (1996)
(whose negations valid formulas called k branch p previously-mentioned
benchmark suite, exposed details Section 5.1.1) obtained conjoining (15) formula:
2h Pb h c+1
(19)
3

(where bxc integer part x) forces atom Pb h c+1 true depth-h
3
states candidate Kripke model, incompatible fact remaining
specifications say false half depth-h states. 17
formulas pathological many approaches (Giunchiglia & Sebastiani,
2000; Giunchiglia, Giunchiglia, Sebastiani, & Tacchella, 2000; Horrocks et al., 2000). particular, introducing on-the-fly BCP, used pet hate Km 2SAT approach, caused generation huge Boolean formulas. fact, due branching
(18), K
h contains 2h 3-formulas (i.e., -formulas) every depth. Therefore, Km 2SAT
encoder Section 3 consider 1 + 2h + (2h)2 + ... + (2h)h+1 = ((2h)h+2 1)/(2h 1)
distinct labels, hh+1 times number labeling states
actually needed. (None optimizations Sections 4.1-4.7 help
formulas, neither BNF encoding atom normalization causes sharing
subformulas, formulas already lifted form, literal occurs pure. 18 )
pathological behavior mostly overcome applying on-the-fly-BCP,
truth values deterministically assigned subformulas K
h on-thefly-BCP, prevent encoding even 2/3-subformulas.
fact, consider branching determined formulas occurring K
h generic
depth {0...h}, determine states level tree. states
D0 , ..., Dd forced true Dd+1 , ..., Dh+1 forced false,
d-th conjunct branching (all conjuncts = h) forced true thus
could dropped. Therefore, 2 3-formulas per non-leaf level could considered
instead, causing generation 2h+1 1 labels overall. Similarly, states level
last h conjuncts determined forced true could dropped, reducing
significantly number 2-formulas considered.
easy see exactly happens applying on-the-fly-BCP. fact,
suppose construction Km 2SAT (K
h ) reached depth (that is, point
every state level d, Def (, )s Def (, )s expanded
Def (, ) Def (, ) expanded yet). Then, BCP deterministically assigns true
literals Lh, D0 , ..., Lh, Dd false Lh, Dd+1 , ..., Lh, Dh+1 , removes one
conjuncts branching, two Def (, )s 2h ones actually expanded;
similarly, last h conjuncts determined removed, corresponding
Def (, )s expanded.
17. Heuerding Schwendimann explain choice index b h3 c + 1. understand
also choices would done job.
18. precisely, one literal, Dh+1 , occurs pure branching, assigning plays role
simplifying formula.

360

fiAutomated Reasoning Modal Description Logics via SAT Encoding

1e+08

1e+07

1e+08

BNF-lift-plr
BNF-nolift-plr

1e+07

1000

BNF-lift-plr
BNF-nolift-plr
100

1e+06

1e+06

BNF-lift
BNF-nolift
BNF-lift-bcp
BNF-nolift-bcp
BNF-lift-plr-bcp
BNF-nolift-plr-bcp

100000

10000

BNF-lift
BNF-nolift
BNF-lift-bcp
BNF-nolift-bcp
BNF-lift-plr-bcp
BNF-nolift-plr-bcp

100000

10000

1000

1000

100

100

10

10

10

BNF-lift
BNF-nolift
BNF-lift-plr
BNF-nolift-plr

1

0.1

1

1
5

10

15

20

(a) k branch n, var#

0.01
5

10

15

20

5

(b) k branch n, clause#

1e+08

1e+07

BNF-lift-bcp
BNF-nolift-bcp
BNF-lift-plr-bcp
BNF-nolift-plr-bcp

1e+07

15

20

(c) k branch n, cpu time

1e+08

BNF-lift-plr
BNF-nolift-plr

10

1000

BNF-lift-plr
BNF-nolift-plr
100

1e+06

1e+06

BNF-lift
BNF-nolift

100000

BNF-lift
BNF-nolift

100000
10

10000

10000

1000

1000

100

100

BNF-lift
BNF-nolift
BNF-lift-plr
BNF-nolift-plr

1

0.1
BNF-lift-bcp
BNF-nolift-bcp
BNF-lift-plr-bcp
BNF-nolift-plr-bcp

10

1

BNF-lift-bcp
BNF-nolift-bcp
BNF-lift-plr-bcp
BNF-nolift-plr-bcp

BNF-lift-bcp
BNF-nolift-bcp
BNF-lift-plr-bcp
BNF-nolift-plr-bcp

10

1
5

10

15

(d) k branch p, var#

20

0.01
5

10

15

(e) k branch p, clause#

20

5

10

15

20

(f) k branch p, cpu time

Figure 2: Empirical analysis Km 2SAT Halpern & Moses formulas wrt. depth
parameter h, different options encoder. 1st row: k branch n, corresponding Km 2SAT (K
h ), formulas (satisfiable); 2nd row: k branch p, correh
sponding Km 2SAT (K
h 2 Pb h
c+1 ), formulas (unsatisfiable). Left: number
3
Boolean variables; center: number clauses; right: total CPU time requested
encoding+solving (where solving step performed Rsat).
See Section 5 technical details.

361

fiSebastiani & Vescovi

h
far unsatisfiable version Km 2SAT (K
h 2 Pb h
c+1 ) concerned,
3
expansion reaches depth h, thanks (19), Lh, P h generated deterministically
b 3 c+1

assigned true BCP every depth-h label ; thanks determined branching,
BCP assigns literals Lh, P1 , ..., Lh, Ph deterministically, Lh, P h assigned
b 3 c+1

false 50% depth-h labels . causes contradiction, encoder
stops expansion returns .
Figure 2 shows growth size CPU time required encode solve
K
h
Km 2SAT (K
h ) (1st row) Km 2SAT (h 2 Pb h
c+1 ) (2nd row) wrt. parameter h,
3
eight combinations following options encoder: without box-lifting,
without on-the-fly PLR, without on-the-fly BCP. (Notice log scale
axis.) Figure 2(d) plots four versions -xxx-bcp (with on-the-fly
BCP) coincide line value 1 (i.e, one variable) Figure 2(e) coincide
horizontal line value 2 (i.e, two clauses), corresponding fact
1-variable/2-clause formula A1 A1 returned (see Footnote 14).
notice facts. First, formulas, eight plots always collapse two
groups overlapping plots, representing four variants without on-the-fly BCP
respectively. shows box-lifting on-the-fly PLR almost irrelevant
encoding formulas, causing little variations time required encoder
(Figures 2(c) 2(f)); notice enabling on-the-fly PLR alone permits encode (but
solve) one problem wrt. versions without on-the-fly PLR
BCP. Second, four versions on-the-fly-BCP always outperform several orders
magnitude without option, terms size encoded formulas CPU
time required encode solve them. particular, case unsatisfiable
variant (Figure 2, second row) encoder returns formula, actual work
required SAT solver (the plot Figure 2(f) refers encoding time).

5. Empirical Evaluation
order verify empirically effectiveness approach, performed veryextensive empirical test session 14,000 Km /ALC formulas. implemented
encoder Km 2SAT C++, flags corresponding optimizations exposed previous section: (i) NNF/BNF, performing pre-conversion NNF/BNF
encoding; (ii) lift/ctrl.lift/nolift, performing respectively Box Lifting,
Controlled Box Lifting Box Lifting encoding; (iii) plr on-the-fly Pure
Literal Reduction performed (iv) bcp on-the-fly Boolean Constraint Propagation
performed. techniques introduced Section 4.2, Section 4.5 Section 4.6
hardwired encoder. Moreover, pre-conversion BNF almost always produces
smaller formulas NNF, set BNF flag default.
combination Km 2SAT tried several SAT solvers encoded formulas (including Zchaff 2004.11.15, Siege v4, BerkMin 5.6.1, MiniSat v1.13, SATElite v1.0, SAT-Elite GTI 2005 submission 19 , MiniSat 2.0 061208 Rsat 1.03).
19. preliminary evaluation available SAT solvers also tried SAT-Elite preprocessor
reduce size SAT formula generated Km 2SAT without bcp option solve it.
However, even preprocessing signinificantly reduce size formula, turned

362

fiAutomated Reasoning Modal Description Logics via SAT Encoding

preliminary evaluation intensive experiments selected Rsat 1.03
(Pipatsrisawat & Darwiche, 2006), produced best overall performances
benchmark suites (although performance gaps wrt. SAT tools, e.g. MiniSat
2.0, dramatic).
downloaded available versions state-of-the-art tools Km -satisfiability.
empirical evaluation 20 selected Racer 1-7-24 (Haarslev & Moeller, 2001)
*SAT 1.3 (Tacchella, 1999) best representatives tableaux/DPLL-based
tools, Mspass v 1.0.0t.1.3 (Hustadt & Schmidt, 1999; Hustadt et al., 1999) 21
best representative FOL-encoding approach, KBDD (unique version) (Pan et al.,
2002; Pan & Vardi, 2003) 22 representative automata-theoretic approach.
representative CSP-based inverse method approaches could used. 23
Notice tools Racer experimental tools, far Km 2SAT
prototype, many (e.g. *SAT KBDD) longer maintained.
Finally, representative QBF-encoding approach, selected K-QBF
translator (Pan & Vardi, 2003) combined sKizzo version 0.8.2 QBF solver
(Benedetti, 2005), turned far 24 best QBF solver benchmarks among freely-available QBF solvers QBF2006 competition (Narizzano,
Pulina, & Tacchella, 2006). (In evaluation considered tools : 2clsQ, SQBF,
preQuantori.e. preQuel +Quantor Quantor 2.11, Semprop 010604.)
tests presented section performed two-processor Intel Xeon
3.0GHz computer, 1 MByte Cache processor, 4 GByte RAM, Red Hat
Linux 3.0 Enterprise Server, four processes run parallel. reporting
results one Km 2SAT +Rsat version, CPU times reported sums

20.

21.

22.
23.

24.

preprocessing time-expensive overall time spent preprocessing
solving reduced problem higher solving directly original encoded SAT formula.
selection SAT solver, order select tools used empirical
evaluation, performed preliminary evaluation smaller benchmark suites (i.e. LWB
and, sometimes, TANCS 2000 ones; see later). Importantly, preliminary evaluation Racer
turned definitely efficient FaCT++, able solve problems less time.
Also, order meet reviewers suggestions, repeated preliminary evaluation latest
versions FaCT++ (v1.2.3, March 5th, 2009) version Racer used paper.
evaluation Racer solves ten problems FaCT++ LWB benchmark,
one hundred problems FaCT++ whole TANCS 2000 suite. Also 2m -CNF
random problems Racer outperforms FaCT++. (We include online appendix plots
comparison Racer FaCT++.)
run Mspass options -EMLTranslation=2 -EMLFuncNary=1 -Sorts=0
-CNFOptSkolem=0 -CNFStrSkolem=0 -Select=2 -Split=-1 -DocProof=0 -PProblem=0 -PKept=0
-PGiven=0, suggested Km -formulas Mspass README file. also tried
options, former gave best performances.
KBDD recompiled run increased internal memory bound 1 GB.
moment KK freely available, failed attempt obtaining authors.
KCSP prolog piece software, difficult compare performances wrt. optimized
tools common platform; moreover, KCSP maintained since 2005, competitive wrt. state-of-the-art tools (Brand, 2008). tools like leanK, 2KE, LWB, Kris
competitive ones listed (Horrocks et al., 2000). KSAT (Giunchiglia & Sebastiani, 1996,
2000; Giunchiglia et al., 2000) reimplemented *SAT.
Unlike choice SAT solver, performance gaps best choice others
significant: e.g., LWB benchmark (see later), sKizzo able solve nearly 90 problems
best QBF competitor.

363

fiSebastiani & Vescovi

encoding Rsat solving times. reporting results K-QBF +sKizzo,
CPU times reported due sKizzo time spent K-QBF
converter negligible.
anticipate that, formulas benchmark suites, tools test i.e.
variants Km 2SAT +Rsat state-of-the-art Km -satisfiability solvers
agreed satisfiability/unsatisfiability result terminating within timeout.
Remark 1. Due big number empirical tests performed huge amount
data plotted, due limitations size, order make plots clearly
distinguishable figures, limited number plots included following
part paper, considering meaningful ones regarding
challenging benchmark problems faced. sake readers convenience, however,
full-size versions plots many plots regarding not-exposed results (also
easier problems), available online appendix, together files
data. discussing empirical evaluation may include considerations
also results.
5.1 Test Description
performed empirical evaluation three different well-known benchmarks
suites Km /ALC problems: LWB (Heuerding & Schwendimann, 1996), random 2m -CNF (Horrocks et al., 2000; Patel-Schneider & Sebastiani, 2003) TANCS
2000 (Massacci & Donini, 2000) benchmark suites. aware publiclyavailable benchmark suite Km /ALC-satisfiability literature. three groups
benchmark formulas allow us test effectiveness approach large number
problems various sizes, depths, hardness characteristics, total amount
14,000 formulas.
particular, benchmark formulas allow us fairly evaluate different tools
modal component Boolean component reasoning intrinsic Km -satisfiability problem, discuss later Section 5.4.
following describe three benchmark suites.
5.1.1 LWB Benchmark Suite
first group benchmark formulas used LWB benchmark suite used
comparison Tableaux98 (Heuerding & Schwendimann, 1996). consists 9 classes
parametrized formulas (each two versions, provable p not-provable n 25 ),
total amount 378 formulas. parameter allows creating formulas increasing size
difficulty.
benchmark methodology test formulas class, increasing difficulty,
one formula cannot solved within given timeout, 1000 seconds tests. 26
result class parameters value largest (and hardest) formula
solved within time limit. parameter ranges 1 21 that,
25. Since tools check Km -(un)satisfiability, formulas negated, negations provable
formulas checked unsatisfiable, whilst negation formulas checked
satisfiable.
26. also set 1 GB file-size limit encoding produced Km 2SAT .

364

fiAutomated Reasoning Modal Description Logics via SAT Encoding

system solve 21 instances class, result given 21. discussion
benchmark suite, refer reader work Heuerding Schwendimann (1996)
Horrocks et al. (2000).
5.1.2 Random 2m -CNF Benchmark Suite
second group benchmark formulas, selected random 2m -CNF testbed
described Horrocks et al. (2000), Patel-Schneider Sebastiani (2003).
generalization well-known random k-SAT test methods, final result
long discussion communities modal description logics obtain
significant flawless random benchmarks modal/description logics (Giunchiglia &
Sebastiani, 1996; Hustadt & Schmidt, 1999; Giunchiglia et al., 2000; Horrocks et al., 2000;
Patel-Schneider & Sebastiani, 2003).
2m -CNF test methodology, 2m -CNF formula randomly generated according
following parameters:
(maximum) modal depth d;
number top-level clauses L;
number literal per clause clauses k;
number distinct propositional variables N ;
number distinct box symbols m;
percentage p purely-propositional literals clauses occurring depth < d, s.t.
clause length k contains average p k randomly-picked Boolean literals
k p k randomly-generated modal literals 2r , 2r . 27
(We refer reader works Horrocks et al., 2000, Patel-Schneider & Sebastiani,
2003 detailed description.)
typical problem set characterized fixed values d, k, N , m, p: L
varied way empirically cover 100% satisfiable / 100% unsatisfiable
transition. words, many problems values d, k, N, m, p
increasing number clauses L generated, starting really small, typically satisfiable
problems (i.e. probability generating satisfiable problem near one) huge
problems, increasing interactions among numerous clauses typically leads
unsatisfiable problems (i.e. makes probability generating satisfiable problems
converging zero). Then, tuple five values problem set, certain
number 2m -CNF formulas randomly generated, resulting formulas given
input procedure test, maximum time bound. fraction
formulas solved within given timeout, median/percentile values
CPU times plotted ratio L/N . Also, fraction satisfiable/unsatisfiable
formulas plotted better understanding.
27. precisely, number Boolean literals clause bp kc (resp. dp ke) probability
dp ke p k (resp. 1 (dp ke p k)). Notice typically smaller p, harder
problem (Horrocks et al., 2000; Patel-Schneider & Sebastiani, 2003).

365

fiSebastiani & Vescovi

Following methodology proposed Horrocks et al. (2000), Patel-Schneider
Sebastiani (2003), fixed = 1, k = 3 100 samples per point tests,
selected two groups: easier one, = 1, p = 0.5, N = 6, 7, 8, 9,
L/N = 10..60, harder one, = 2, p = 0.6, 0.5, N = 3, 4, L/N = 30..150
p = 0.6 L/N = 50..140 p = 0.5, varying L/N ratio steps 5, total
amount 13,200 formulas.
test, imposed timeout 500 seconds per sample 28 calculated
number samples solved within timeout, 50%th 90%th percentiles CPU time. 29 order correlate performances (un)satisfiability
sample formulas, background plot also plot satisfiable/unsatisfiable
ratio.

5.1.3 TANCS 2000 Benchmark Suite
Finally, third group benchmark formulas, used MODAL PSPACE division
benchmark suite used comparison TANCS 2000 (Massacci & Donini, 2000).
contains satisfiable unsatisfiable formulas, scalable hardness. benchmark suite, call TANCS 2000, formulas constructed translating QBF
formulas K using three translation schemas, namely Schmidt-Schauss-Smolka translation (240 problems many different depths, 19 112), Ladner translation
(240 problems, depths range 19 112), Halpern translation
(56 problems depth among: 20, 28, 40, 56, 80 112) (Massacci & Donini, 2000).
done Massacci Donini, call classes easy, medium hard respectively.
formulas class tested within timeout 1000 seconds. 30
class, report number solved formulas (X axis) total (cumulative) CPU
time spent solving formulas (Y axes). class results plotted sorting
solved problems easiest one hardest one.

5.2 Empirical Comparison Different Variants Km 2SAT
first evaluated various variants encoding combination Rsat.
order avoid considering many combinations flags, considered BNF
format, grouped plr bcp one parameter plr-bcp, restricting thus
investigation 6 combinations: BNF, lift/ctrl.lift/nolift, plr-bcp on/off.
(We recall techniques introduced Section 4.2, Section 4.5 Section 4.6
hardwired encoder.) expose analyze results wrt. three different
suites benchmark problems.

28. also 512 MB file-size limit encoding produced Km 2SAT .
29. Due lack space sake clarity wont include paper 90%th percentiles
plots. Further, reasons, well skip report plots regarding easiest class
benchmark suite (e.g. = 1 lower values N ). plots, however,
found online appendix.
30. also set 1 GB file-size limit encoding produced Km 2SAT .

366

fiAutomated Reasoning Modal Description Logics via SAT Encoding

5.2.1 Results LWB Benchmark Suite
results LWB benchmark suite summarized Table 1 Figure 3.
Table 1(a) reports left block indexes hardest formulas encoded within
file-size limit and, right block, hardest formulas solved within
timeout Rsat; Table 1(b) reports numbers variables clauses Km 2SAT (),
referring hardest formulas solved within timeout Rsat (i.e., reported
right block Table 1(a)). instance, BNF-ctrl.lift-plr-bcp encoding
k dum n(21) contains 11106 variables 14106 clauses; hardest k dum n problem
solved Rsat BNF-ctrl.lift-plr-bcp first solved
BNF-ctrl.lift.
Looking numbers cases solved Table 1(a), notice introduction
on-the-fly Pure Literal Reduction Boolean Constraint Propagation optimizations
really effective produces consistent performance enhancement (the effect
optimizations eye-catching branching formulas k branch * see Section 4.9
k path * formulas). also notice lift sometimes introduces slight
improvement.
view Tables 1(a) 1(b) hides actual CPU times required encode
solve problems. Small gaps numbers Table 1(a) may correspond big gaps
CPU time. order analyze also aspect, Figure 3 plotted total cumulative
amount CPU time spent variants Km 2SAT +Rsat solve problems
LWB benchmark, sorted hardness. plot, also considered three
options BNF, lift/ctrl.lift/nolift, plr bcp evaluate
also effect plr bcp separately. notice plots clearly clustered
three groups increasing performance: BNF-*, BNF-*-plr, BNF-*-plr-bcp., *
representing three options lift/ctrl.lift/nolift. highlights fact
suite on-the-fly Pure Literal Reduction significantly improves performances,
on-the-fly Boolean Constraint Propagation introduces drastic improvements,
variations due Box Lifting minor wrt. two optimizations.
Overall, configuration BNF-lift-plr-bcp turns best performer
suite, tiny advantage wrt. BNF-ctrl.lift-plr-bcp.
5.2.2 Results Random 2m -CNF Benchmark Suite
results random 2m -CNF benchmark suite reported Figures 4 5.
Figure 4 report 50%-percentile CPU times required encode solve
formulas different Km 2SAT +Rsat variants hardest benchmarks problems.
dont report percentage solved problems since always 100%, i.e. Km 2SAT
+Rsat terminates within timeout every problem benchmark suite.
tests depth = 1 (see results hardest problems class
first row Figure 4) simply easy Km 2SAT +Rsat (but competitors,
see Section 5.3) solved every sample formula less 1 second. Although
tests exposed second third row Figure 4 challenging,
solved within timeout well. noticed also results rather regular,
since big gaps 50%- 90%-percentile values.
367

fiSebastiani & Vescovi

k
k
k
k
k
k
k
k
k
k
k
k
k
k
k
k
k
k

lifting
branch n
branch p
d4 n
d4 p
dum n
dum p
grz n
grz p
lin n
lin p
path n
path p
ph n
ph p
poly n
poly p
t4p n
t4p p


4
4
8
14
20
19
21
21
21
21
7
8
21
21
21
21
6
11

Km 2SAT , encoded
plr-bcp
yes ctrl
yes
4
4
18
18
4
4
18
18
8
8
8
9
14 14
14
14
20
20
21
21
19
19
21
21
21 21
21
21
21 21
21
21
21 21
21
21
21 21
21
21
7
7
14
15
8
8
15
16
21 21
21
21
21 21
21
21
21 21
21
21
21 21
21
21
6
6
6
6
11 11
11
11

ctrl
18
18
8
14
21
21
21
21
21
21
14
15
21
21
21
21
6
11

Km 2SAT + Rsat, solved
plr-bcp
yes ctrl
yes
4
4
4
17
17
4
4
4
18
18
8
8
8
8
8
14
14 14
14
14
20
20
20
21
21
18
18
18
21
21
21
21 21
21
21
21
21 21
21
21
21
21 21
21
21
21
21 21
21
21
7
7
7
13
14
8
8
8
15
16
21
21 21
21
21
10
11
10
10
10
21
21 21
21
21
21
21 21
21
21
5
6
5
6
6
10
10
10
11
11

ctrl
17
18
8
14
21
21
21
21
21
21
13
15
21
11
21
21
6
11

(a) Indexes hardest problems encoded (left)
hardest problems solved (right).

k
k
k
k
k
k
k
k
k
k
k
k
k
k
k
k
k
k

lifting
branch n
branch p
d4 n
d4 p
dum n
dum p
grz n
grz p
lin n
lin p
path n
path p
ph n
ph p
poly n
poly p
t4p n
t4p p


1000
1000
12000
19000
19000
11000
10
8
30
0
11000
11000
50
3
200
200
4000
12000

number variables (103 )
plr-bcp
yes
ctrl

yes
ctrl
1000 1000 20000 20000 20000
1000 1000
0
0
0
6000 12000 10000 26000 10000
18000 19000
0
0
0
19000 19000 11000 11000 11000
11000 11000 20000 19000 20000
10
10
5
5
5
8
8
0.2
0.1
0.2
30
20
20
10
20
0
0
0
0
0
12000 11000 10000 7000 10000
12000 11000 26000 16000 26000
300
50
50
300
50
13
3
3
8
4
20
20
200
20
20
20
20
200
20
20
21000 4000 17000 14000 17000
10000 12000 20000 18000 20000


1000
1000
17000
28000
23000
14000
10
8
50
0
13000
13000
50
3
200
200
4000
12000

number clauses (103 )
plr-bcp
yes
ctrl

yes
ctrl
1000 1000 23000 23000 23000
1000 1000
0
0
0
9000 17000 16000 43000 16000
25000 28000
0
0
0
23000 23000 14000 14000 14000
13000 14000 26000 25000 26000
10
10
6
6
6
8
8
0.3
0.2
0.2
50
20
30
30
30
0
0
0
0
0
14000 13000 14000 9000 13000
14000 13000 35000 20000 35000
300
50
50
600
50
14
3
3
14
5
20
20
200
20
20
20
20
200
20
20
22000 4000 20000 17000 20000
11000 12000 24000 21000 24000

(b) # variables # clauses hardest problems solved.
Note: 0 means formula simplified Km 2SAT .

Table 1: Comparison variants Km 2SAT +Rsat LWB benchmarks.

368

fiAutomated Reasoning Modal Description Logics via SAT Encoding

10000
BNF-lift (Rsat)
BNF-nolift (Rsat)
BNF-ctrl.lift (Rsat)
BNF-lift-plr (Rsat)
BNF-nolift-plr (Rsat)
BNF-ctrl.lift-plr (Rsat)
BNF-lift-plr-bcp (Rsat)
BNF-nolift-plr-bcp (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

1000

100

10

1

0.1
50

100

150

200

250

300

Figure 3: Comparison different variants Km 2SAT +Rsat LWB problems.
X axis: number solved problems; axis: total CPU time spent (sorting
problems easiest hardest).

general, relevant performance gaps various configurations
benchmark suite; seems majority cases ctrl.lift slightly beats
nolift nolift slightly beats lift. gaps even relevant consider
size formulas generated (Figure 5). believe may due fact
random 2m -CNF formulas may contain lots shared subformulas 2r , lifting
may cause reduction sharing (see Section 3). Further, plr-bcp seem
introduce relevant improvements here. believe due fact
random formulas produce pure unit literals low even zero probability.
Overall, configuration BNF-nolift turns best performer suite,
slight advantage wrt. BNF-ctrl.lift-plr-bcp.
Finally, plots Figure 4 notice Km 2SAT +Rsat problems
tend harder within satisfiability/unsatisfiability transition area. (This fact holds
especially Racer *SAT, see Section 5.3.) seems confirm fact
easy-hard-easy pattern random k-SAT extends also 2m -CNF, already observed
literature (Giunchiglia & Sebastiani, 1996, 2000; Giunchiglia et al., 2000; Horrocks et al.,
2000; Patel-Schneider & Sebastiani, 2003).
369

fiSebastiani & Vescovi

1000

100

500

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

100

1000

100

500

(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)

80

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

100

50

(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)

80

50

10

60

10

5

60

5

1

40

1

0.5

40

0.5

0.1

20

0.1

0.05

20

0.05

0.01

0
10

20

30

40

50

0.01

60

1000

100

500

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

100

0
10

20

40

50

60

1000

100

500

(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)

30

80

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

100

50

(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)

80

50

10

60

10

5

60

5

1

40

1

0.5

40

0.5

0.1

20

0.1

0.05

20

0.05

0.01

0
40

60

80

100

120

1000

0.01

140
100

500

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

100

(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)

0
40

60

80

100

120

1000

140
100

500

80

100

50

80

50

10

60

10

5

60

5

1

40

1

0.5

40

0.5

0.1

20

0.1

0.05

0.05

0.01
60

80

100

120

0
140

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)

20

0.01
60

80

100

120

0
140

Figure 4: Comparison among different variants Km 2SAT +Rsat random problems.
X axis: #clauses/N . axis: median (50th percentile) CPU time, 100 samples/point. 1st row: = 1, p = 0.5, N = 8, 9; 2nd row: = 2, p = 0.6, N = 3, 4;
3rd row: = 2, p = 0.5, N = 3, 4. Background: % satisfiable instances.

370

fiAutomated Reasoning Modal Description Logics via SAT Encoding

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

180000
160000

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

180000
160000

140000

140000

120000

120000

100000

100000

80000

80000

60000

60000

40000

40000

20000

20000

0

0
10

20

30

40

50

60

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

1.2e+06

10

20

1e+06

800000

800000

600000

600000

400000

400000

200000

200000

0

40

50

60

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

1.2e+06

1e+06

30

0
40

60

80

100

120

140

40

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

2e+06

1.5e+06

1e+06

1e+06

500000

500000

0

80

100

120

140

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

2e+06

1.5e+06

60

0
60

80

100

120

140

60

80

100

120

140

Figure 5: Comparison among different variants Km 2SAT random problems. X axis:
#clauses/N . axis: 1st column: #variables SAT encoding (90th percentiles), 100 samples/point; 2nd column: #clauses SAT encoding (90th
percentiles), 100 samples/point. 1st row: = 1, p = 0.5, N = 9; 2nd row: = 2,
p = 0.6, N = 4; 3rd row: = 2, p = 0.5, N = 4.

371

fiSebastiani & Vescovi

5.2.3 Results TANCS 2000 Benchmark Suite
comparison among Km 2SAT variants TANCS 2000 benchmark presented
Figures 6 7, different BNF variants Km 2SAT compared enabling
disabling lift/ctrl.lif plr-bcp.
Figure 6, top-left bottom, present cumulative CPU times spent
Km 2SAT +Rsat easy, medium hard categories respectively (the corresponding
plots reporting non-cumulative CPU times included online appendix).
Figure 7 present plots number variables clauses formulas solved
challenging cases medium hard problems. 31 notice
slight differences among different variants Km 2SAT ; BNF lift
best option allows solving problems hard class requiring less CPU
time.
remark that, despite expected exponential growth encoded formulas wrt.
modal depth, benchmark Km 2SAT +Rsat allows encoding solving
problems modal depth greater 100 easy class greater 50
medium hard classes, producing solving SAT-encoded formulas 107
variables 1.4 107 clauses.
5.3 Empirical Comparison wrt. Approaches
proceed comparison approach wrt. current state-of-the-art evaluating Km 2SAT +Rsat Km -satisfiability solvers listed above.
details, chose compare performance solvers best
local Km 2SAT +Rsat variant single benchmark suite best global
Km 2SAT +Rsat variant among benchmark suites, identified
BNF-ctrl.lift-plr-bcp.
5.3.1 Comparison LWB Benchmark Suite
results LWB benchmark suite summarized numerically graphically
Table 2. Table 2(a) notice facts: Racer *SAT best performers
(confirming analysis done Horrocks et al., 2000) significant gap wrt. others;
then, K-QBF +sKizzo solves problems Km 2SAT +Rsat; follows
KBDD outperforms Mspass, worst performer. detail, Km 2SAT
+Rsat (one of) worst performer(s) k d4 * k t4 *, fourth best performer
k path n, third best performer k path p k branch p, (one of)
best performer(s) k branch n, k dum *, k grz *, k lin *, k ph * k poly *;
absolute best performer k branch n k ph p.
Table 2(b) give graphical representation comparison, plotting number
solved problems approach total cumulative amount CPU time
spent. notice that, even Km 2SAT +Rsat solves problems less K-QBF
+sKizzo, Km 2SAT +Rsat mostly faster K-QBF +sKizzo.
31. plots easy problems included online appendix.

372

fiAutomated Reasoning Modal Description Logics via SAT Encoding

2000

2000

1000

1000

100

100

10

10

1

1
BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

0.1
20

40

60

80

(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)

100

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

0.1

120

140

10

20

30

40

50

60

(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
70

80

90 100

1000

100

10

1
BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

0.1
5

10

15

(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
(Rsat)
20

25

30

Figure 6: Comparison among different variants Km 2SAT +Rsat TANCS 2000 problems. X axis: number solved problems. axis: total cumulative CPU time
spent. 1st (top-left) 3th (bottom) plot: easy, medium, hard problems. (Problems sorted easiest hardest).

373

fiSebastiani & Vescovi

1e+08

1e+08

1e+07

1e+07

1e+06

1e+06

100000

100000
BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

10000
10

20

30

40

50

60

70

10000
80

90 100

10

1e+08

1e+08

1e+07

1e+07

1e+06

1e+06

100000

100000
BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

10000
5

10

15

20

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp
20

30

40

50

60

70

80

90 100

BNF-lift
BNF-nolift
BNF-ctrl.lift
BNF-lift-plr-bcp
BNF-nolift-plr-bcp
BNF-ctrl.lift-plr-bcp

10000
25

30

5

10

15

20

25

30

Figure 7: Comparison among different variants Km 2SAT TANCS 2000 problems. X
axis: number harder solved problem. axis: 1st column: #variables
SAT encoding problem; 2nd column: #clauses SAT encoding
problem. 1st 2th row: medium, hard problems.

374

fiAutomated Reasoning Modal Description Logics via SAT Encoding

tools
K-QBF
+ sKizzo KBDD Mspass Racer *SAT
4
8
10
15
14
16
8
10
21
21
8
21
21
21
21
21
21
21
21
21
21
21
21
21
21
21
21
21
21
21
19
21
21
21
21
21
21
21
21
21
20
21
21
21
21
21
21
3
21
21
9
21
4
21
21
13
17
5
21
21
21
4
12
21
13
10
4
8
9
9
21
8
7
21
21
21
8
7
21
21
21
21
12
21
21
21
21
21
21
21

test
k branch n
k branch p
k d4 n
k d4 p
k dum n
k dum p
k grz n
k grz p
k lin n
k lin p
k path n
k path p
k ph n
k ph p
k poly n
k poly p
k t4p n
k t4p p

Km 2SAT + Rsat
BNF-plr-bcp
-ctrl.lift
-lift
17
17
18
18
8
8
14
14
21
21
21
21
21
21
21
21
21
21
21
21
13
14
15
16
21
21
11
10
21
21
21
21
6
6
11
11

(a) Indexes hardest problems solved.

10000
kQBF+sKizzo
*SAT
Racer
kBDD
1000
MSpass
BNF-lift-plr-bcp (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)
100

10

1

0.1
50

100

150

200

250

300

350

(b) X axis: # problems solved; axis: total (cumulative) CPU time spent.

Table 2: Comparison Km 2SAT +Rsat tools LWB benchmarks.

375

fiSebastiani & Vescovi

100

100

90

90

80

80

70

70

60

60

50

50

40

40

30

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

30
kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

20

10

20

10

0

0
10

20

30

40

50

60

10

100

100

90

90

80

80

70

70

60

20

30

40

50

60

60
kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

50

40

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

50

40

30

30

20

20

10

10

0

0
40

60

80

100

120

140

40

100

100

90

90

80

80

70

70

60

60

80

100

120

140

60
kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

50

40

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

50

40

30

30

20

20

10

10

0

0
60

80

100

120

140

60

80

100

120

140

Figure 8: Comparison approaches random problems. X axis: #clauses/N .
axis: % problems solved within timeout, 100 samples/point. 1st row:
= 1, p = 0.5, N = 8, 9; 2nd row: = 2, p = 0.6, N = 3, 4; 3rd row: = 2,
p = 0.5, N = 3, 4.

376

fiAutomated Reasoning Modal Description Logics via SAT Encoding

1000

100

1000

500

100

500

100

80

100

50

80

50

10

60

10

5

60

5

1

40

1

0.5

40

0.5

0.1

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

0.05

20

0.1

0.01

0
10

20

30

40

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

0.05

50

0.01

60

1000

20

0
10

100

20

30

40

50

60

1000

500

100

500

100

80

100

50

80

50

10

60

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

5

1

10

60

5

40

1

0.5

40

0.5

0.1

20

0.1

0.05

0.01

0
40

60

80

100

120

1000

20

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

0.05

0.01

140

0
40

100

60

80

100

120

1000

500

140
100

500

100

80

100

50

80

50

10

60

10

5

60

5

1

40

1

0.5

40

0.5

0.1

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

0.05

0.01
60

80

100

120

20

0.1

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-nolift (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

0.05

0
140

0.01
60

80

100

120

20

0
140

Figure 9: Comparison approaches random problems. X axis: #clauses/N .
axis: median (50th percentile) CPU time, 100 samples/point. 1st row: = 1,
p = 0.5, N = 8, 9; 2nd row: = 2, p = 0.6, N = 3, 4; 3rd row: = 2, p = 0.5,
N = 3, 4. Background: % satisfiable instances.

377

fiSebastiani & Vescovi

5.3.2 Comparison Random 2m -CNF Benchmark Suite
random 2m -CNF benchmark suite results dominated Km 2SAT +Rsat.
hardest categories among three groups problems used evaluation,
report Figure 8 number problems solved tool within timeout,
Figure 9 median CPU time (i.e. 50%th percentile).
Looking Figure 8 notice Km 2SAT +Rsat (in versions) tool
always terminates within timeout, whilst *SAT Racer sometimes
terminate hardest problems, K-QBF +sKizzo often terminate,
Mspass KBDD terminate values.
Figure 9 notice Km 2SAT +Rsat often best performer (in particular hardest problems), followed *SAT Racer. (This even much
evident consider 90%th percentile CPU time, whose plots included
online appendix.) tests, K-QBF +sKizzo, Mspass KBDD drastically
outperformed others.
5.3.3 Comparison TANCS 2000 Benchmark Suite
results TANCS 2000 benchmark summarized Figure 10, easy
category (top-left) hard category (bottom).
plots notice relative performances tools test vary
significantly category: Racer *SAT among best performers
categories; K-QBF +sKizzo behaves well easy medium categories solves
problems hard one; KBDD behaves well easy category, solves
problems medium hard ones. Mspass among worst performers
categories: particular, solve problem hard category; finally,
Km 2SAT +Rsat worst performer easy category, outperforms competitors
*SAT Racer medium category, competes head-to-head
Racer *SAT first position hard category: local-best configuration
BNF-lift beats competitors; global-best configuration BNF-ctrl.lift-prl-bcp
solves many problems Racer, one-order-magnitude CPU-time performance gap,
two problems less *SAT.
Notice classification benchmark problems easy, medium
hard given Massacci Donini (2000) based translation schema used
produce every modal problem refers reasoning component, necessarily related components (like, e.g., modal depth) affect size
encoding and, hence, efficiency approach. may explain fact, e.g.,
easy problems easy approach, viceversa.
5.4 Discussion
highlighted Giunchiglia et al. (2000), Horrocks et al. (2000), satisfiability
problem modal logics like Km characterized alternation two orthogonal
components reasoning: Boolean component, performing Boolean reasoning within
state, modal component, generating successor states state. latter
must cope fact candidate models may exponentially big wrt.
depth(), whilst former must cope fact may exponentially
378

fiAutomated Reasoning Modal Description Logics via SAT Encoding

10000

1e+05

10000
1000

1000
100
100
10
10

1

0.1

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-lift-plr-bcp (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)
50

100

150

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-lift-plr-bcp (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

1

0.1
200

20

40

60

80

100

120

10000

1000

100

10

kQBF+sKizzo
*SAT
Racer
kBDD
MSpass
BNF-lift-plr-bcp (Rsat)
BNF-ctrl.lift-plr-bcp (Rsat)

1

0.1
5

10

15

20

25

30

Figure 10: Comparison approaches TANCS 2000 problems. X axis: number solved problems. axis: total cumulative CPU time spent. 1st (top-left)
3th (bottom) plot: easy, medium, hard problems. (Problems sorted
easiest hardest).

379

fiSebastiani & Vescovi

many candidate (sub)models explore. Km 2SAT +DPLL approach encoder
handle whole modal component (rules (8) (9)), whilst handling
whole Boolean component delegated external SAT solver.
results displayed Section 5.3 notice relative performances
Km 2SAT +DPLL approach wrt. state-of-the-art tools range cases
Km 2SAT +Rsat much less efficient state-of-the-art approaches (e.g., k d4
k t4p formulas) formulas much efficient (e.g., k ph p
2m -CNF formulas = 1). middle stands large majority formulas
approach competes well state-of-the art tools; particular, Km 2SAT
+Rsat competes well even outperforms approaches based translations
different formalisms (the translational approach, automata-theoretic approach
QBF-encoding approach).
simple explanation former fact could Km 2SAT +DPLL approach
loses problems high modal depth, modal component reasoning
dominates (like, e.g., k d4 k t4p formulas), wins problems Boolean
component reasoning dominates (like, e.g., k ph n 2m -CNF formulas
= 1), competitive formulas components relevant.
notice, however, Km 2SAT +Rsat wins hard category TANCS 2000
benchmarks, modal depths greater 50, k branch n, search
dominated modal component. 32 fact, recall reducing Boolean
component reasoning may produce reduction also modal reasoning effort,
may reduce number successor states analyze (e.g. Sebastiani, 2007, 2007). Thus,
e.g., techniques like on-the-fly BCP, although exploiting purely-Boolean properties,
may produce drastic pruning Boolean search, also drastic reduction
size model investigated, cut priori amount successor
states expand.

6. Related Work Research Trends
last fifteen years one main research line description logic focused investigating increasingly expressive logics, goal establishing theoretical boundaries
decidability allowing expressive power languages defined (Baader,
Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). Consequently, expressive
though hard description logics today notable application field Semantic
Web. example, SHOIN (D) logic (which NExpTime complexity) captures
sub-language OWL DL full OWL (Web Ontology Language) language (Bechhofer,
van Harmelen, Hendler, Horrocks, McGuinness, Patel-Schneider, & Stein, 2004),
recommended standard language semantic web proposed W3C consortium.
contrast, recent quest tractable description logic-based languages arising
field bio-medical ontologies (e.g., Spackman, Campbell, & Cote, 1997; Sioutos,
de Coronado, Haber, Hartel, Shaiu, & Wright, 2007; Gene Ontology Consortium, 2000;
32. k branch n formulas hard perspective modal reasoning, require
finding one model 2d+1 1 states (Halpern & Moses, 1992), Boolean reasoning within
state really required (Giunchiglia et al., 2000; Horrocks et al., 2000): e.g., *SAT solves k branch n(d)
2d+1 1 calls embedded DPLL engine, one state M, call solved BCP only.

380

fiAutomated Reasoning Modal Description Logics via SAT Encoding

Rector & Horrocks, 1997) stimulated opening another research line tractable
description logics (also called lightweight description logics), suitable reasoning
big bio-medical ontologies. particular, Baader et al. (2005, 2006, 2007, 2008)
spent considerable effort attempt defining small maximal subset
logical constructors, expressive enough cover needs practical applications,
whose inference problems must remain tractable. example, simple tractable
description logics like EL, EL+ EL++ (Baader et al., 2005) expressive enough
describe several important bio-medical ontologies SNoMed (Spackman et al., 1997),
NCI (Sioutos et al., 2007), Gene Ontology (The Gene Ontology Consortium, 2000)
majority Galen (Rector & Horrocks, 1997).
Reasoning ontologies represents important application lightweight
description logics, also challenge due required efficiency huge dimensions ontologies. perspective, important face efficiently
basic reasoning services (e.g., satisfiability, subsumption, queries) logics like EL, EL+
EL++ , also sophisticated reasoning problems like e.g., axiom pinpointing (Baader
et al., 2007; Baader & Penaloza, 2008) logical difference terminologies (Konev,
Walther, & Wolter, 2008).

7. Conclusions Future Work
paper explored idea encoding Km /ALC-satisfiability SAT,
handled state-of-the-art SAT tools. showed that, despite intrinsic
risk blowup size encoded formulas, performances approach
comparable current state-of-the-art tools rather extensive variety
empirical tests. Furthermore, remark approach allows directly benefitting
free current future enhancements SAT solver technology.
see many possible directions explore order enhance extend approach.
important open research line explore feasibility SAT encodings
expressive modal description logics (e.g., whilst logics like Tm extension
straightforward, logics like S4m , elaborated description logics ALC,
challenging) complex form reasoning (e.g., including TBoxes
ABoxes).
current investigation, however, focuses lightweight logics Baader et al.
(2005). investigated (and currently enhancing) encoding main
reasoning services EL EL+ Horn-SAT, allows reasoning efficiently
(often huge) bio-medical ontologies mentioned Section 6, handling
sophisticated inference problems mentioned (e.g., currently handle axiom
pinpointing), exploiting advanced functionalities implemented
top modern SAT solver (Sebastiani & Vescovi, 2009).

8. Acknowledgments
authors partly supported SRC/GRC Custom Research Project 2009-TJ1880 WOLFLING, MIUR PRIN project 20079E5KM8 002.
381

fiSebastiani & Vescovi

Appendix A. Proof Correctness & Completeness
A.1 Notation
Let Km -formula. denote representation current formalism:
def
def
def
def
def
NNF, 3r = 2r , 2r = 3r , 1 2 = 1 2 , 1 2 = 1 2 , Ai = Ai ,
def
def
def
def
def
Ai = Ai ; BNF, 2r = 2r , 2r = 2r , 1 2 = 1 2 , 1 2 = 1 2 ,
def
def
Ai = Ai , Ai = Ai .
represent truth assignment set literals, intended meaning
positive literal Ai (resp. negative literal Ai ) means Ai assigned true (resp.
false). say assigns literal l either l l . say literal l
occurs Boolean formula iff atom l occurs .
Let denote Kripke model, let label generic state u M.
label (and denote) 1 root state M. h : mean u
M, u |= . Thus, every s.t. u M, either h : h : M.
convenience, instead (9) sometimes use equivalent definition:
^
^
def
(Lh, r,i Lh.i, 0r ))
Def (.i, 0r ). (20)
Def (, r ) = (Lh, r
every
h, r,i

every
h, r,i

Notice Def (, ) (6), (7), (8), (20) written general form
^
(Lh, h,i )
Def ( 0 , 0 ).

(21)

h 0 , 0

call definition implication Def (, ) expressions (Lh,



h,i ).

A.2 Soundness Completeness Km 2SAT
Let Km -formula. prove following theorem, states soundness
completeness Km 2SAT .
Theorem 1. Km -formula Km -satisfiable corresponding Km 2SAT ()
satisfiable.
Proof. direct consequence following Lemmas 2 3.

Lemma 2. Given Km -formula , Km 2SAT () satisfiable, exists Kripke
model s.t. M, 1 |= .
Proof. Let total truth assignment satisfying Km 2SAT (). build Kripke
model = hU, L, R1 , . . . , Rm follows:
U

def

= { : Ah, occurs Km 2SAT () f }

rue Lh, Ai
def
L(, Ai ) =
F alse Lh, Ai
Rr

def

= {h, .ii : Lh,

r,i

382

}.

(22)
(23)
(24)

fiAutomated Reasoning Modal Description Logics via SAT Encoding

show induction structure that, every h, s.t. Lh,
Km 2SAT (),
h : Lh, .



occurs
(25)

Base
= Ai = Ai . (25) follows trivially (23).
Step
= . Let Lh,



.

satisfies (6), Lh,

every {1, 2}.



inductive hypothesis, h : every {1, 2}.
Then, definition, h : M.
Thus, h : Lh,
= . Let Lh,





.

.

satisfies (7), Lh,



{1, 2}.

inductive hypothesis, h : {1, 2}.
Then, definition, h : M.
Thus, h : Lh,
= r,j . Let Lh,

r,j



.

.

satisfies (8), Lh.j,

0r,j

.

inductive hypothesis, h.j : 0r,j M.
Then, definition (24), h : r,j M.
Thus, h : r,j Lh,
= r . Let Lh,

ri

r,j

.

.

satisfies (9), every h, r,i s.t. Lh,

r,i

, Lh.i,

0r

inductive hypothesis, h : r,i h.i : 0r M.
Then, definition (24), h : r M.
Thus, h : r Lh,
|= Km 2SAT (), Ah1,

r



.

. Thus, (25), h1 : M, i.e., M, 1 |= .

383

.

fiSebastiani & Vescovi

Lemma 3. Given Km -formula , exists Kripke model s.t. M, 1 |= ,
Km 2SAT () satisfiable.
Proof. Let Kripke model s.t. M, 1 |= . build truth assignment
Km 2SAT () recursively follows: 33
def

=

(26)



def



def

=

(28)



def

(29)



def

= {Lh,




{Lh,

= {Lh,


{Lh,

Km 2SAT () : h, M}



Km 2SAT () : h, M}

r,i

r

(27)

Km 2SAT () : 6 M}

Km 2SAT () : 6 M}

= {Lh,

Km 2SAT ()

: 6 Lh,



{1, 2}} (30)



Km 2SAT ()

: 6 Lh,



every {1, 2}}.

{Lh,

consistent truth assignment literals Lh,

Ai

s.t. Ai 6 M.

construction, every Lh, Km 2SAT (), assigns Lh, true iff assigns
Lh, false vice versa, consistent truth assignment.
First, show satisfies definition implications Def (, )s
Def (, ) s.t. M. Let M. distinguish four cases.
= . Thus = s.t. 1 = 1 2 = 2 .
h : (and hence h : 6 M), h :
h : 6 M. Thus, (27), {Lh, 1 , Lh, 2 , Lh, } ,
satisfies definition implications Def (, ) Def (, ).
h : 6 (and hence h, M), h : 6
h : M. Thus, (27), {Lh, , Lh, } , satisfies
definition implications Def (, ) Def (, ).
= . Like previous case, inverting .
= r,j . Thus = r s.t. 0r = 0r,j .
h : r,j (and hence h : r 6 M), h.j : 0r,j M. Thus, (27),
{Lh.j, r,j , Lh, r } , satisfies definition implications
0

Def (, r,j ) Def (, r ).
33. assume , generated order, generated recursively starting
. Intuitively, assigns literals Lh, s.t. way mimic M; assigns
literals way mimic fact state outside generated (i.e.,
Lh, assigned false Lh, s, Lh, s, Lh, assigned consequently).

384

fiAutomated Reasoning Modal Description Logics via SAT Encoding

h : r,j 6 (and hence h : r M), (27) Lh, r,j ,
satisfies definition implications Def (, r,j ).
far Def (, r ) concerned, partition clauses (9):
((Lh,

r

Lh,

r,i )

Lh.i,

0r )

(31)

two subsets. first set clauses (31) h : r,i M.
h : r M, h.i : 0r M. Thus, (27), Lh.i, 0r ,
satisfies (31). second set clauses (31) h : r,i 6 M.
(27) Lh, r,i , satisfies (31). Thus,
satisfies definition implications also Def (, r ).
= r . Like previous case, inverting .
Notice that, 6 M, .i 6 every i. Thus, every Def (, ) s.t. 6 M,
atoms implication definition Def (, ) assigned .
Second, show induction recursive structure satisfies
definition implications Def (, )s Def (, )s s.t. 6 M. Let 6 M.
base step, (29), satisfies definition implications Def (, r,i )s
Def (, r )s assigns false Lh, r,i s. Indeed, assigns every literal
type Lh, Ai s.t Ai 6 (notice Def (, Ai )s definitions
trivially satisfied dont contain definition implications).
inductive step, show inductive structure satisfies
definition implications Def (, )s Def (, )s
def
Let = = s.t. = (or vice versa). that:
Lh, (respectively least one Lh, ) assigned true ,
definition implications Def (, ) (respectively Def (, )) already trivially
satisfied;
least one Lh, (respectively Lh, s) assigned false ,
(30) Lh, (respectively Lh, ) assigned false , satisfies definition
implication Def (, ) (respectively Def (, )).
Thus satisfies definition implications Def (, )s Def (, )s s.t.
6 M.
whole, |= Def (, ) every Def (, ). construction, |= Ah1,
since h1 : M. Therefore |= Km 2SAT ().

385



fiSebastiani & Vescovi

References
Areces, C., Gennari, R., Heguiabehere, J., & de Rijke, M. (2000). Tree-based heuristics
modal theorem proving. Proc. ECAI00, pp. 199203. IOS Press.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proc. IJCAI05,
pp. 364369. Morgan-Kaufmann Publishers.
Baader, F., Franconi, E., Hollunder, B., Nebel, B., & Profitlich, H. (1994). Empirical
Analysis Optimization Techniques Terminological Representation Systems or:
Making KRIS get move on. Applied Artificial Intelligence. Special Issue Knowledge Base Management, 4, 109132.
Baader, F., & Hollunder, B. (1991). Terminological Knowledge Representation System
Complete Inference Algorithms. Proc. First International Workshop
Processing Declarative Knowledge, Vol. 572 LNCS, pp. 6785. SpringerVerlag.
Baader, F., Lutz, C., & Suntisrivaraporn, B. (2006). CELa polynomial-time reasoner
life science ontologies. Proc. IJCAR06, Vol. 4130 LNAI, pp. 287291.
SpringerVerlag.
Baader, F., Brandt, S., & Lutz, C. (2008). Pushing EL envelope further. Proc.
OWLED 2008 DC Workshop.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). Description Logic Handbook: Theory, Implementation, Applications.
Cambridge University Press.
Baader, F., & Penaloza, R. (2008). Automata-Based Axiom Pinpointing. Proc.
IJCAR08, Vol. 5195 LNAI, pp. 226241. Springer.
Baader, F., Penaloza, R., & Suntisrivaraporn, B. (2007). Pinpointing description
logic EL+ . Proc. KI 2007, Vol. 4667 LNCS, pp. 5267. Springer.
Balsiger, P., Heuerding, A., & Schwendimann, S. (1998). Logics workbench 1.0. Proc.
Tableaux98, Vol. 1397 LNCS, pp. 3537. Springer.
Bechhofer, S., van Harmelen, F., Hendler, J., Horrocks, I., McGuinness, D. L., PatelSchneider, P. F., & Stein, L. A. (2004). OWL Web Ontology Language reference.
W3C Recommendation. Available http://www.w3.org/TR/owl-ref/.
Benedetti, M. (2005). sKizzo: Suite Evaluate Certify QBFs. Proc. CADE-20,
Vol. 3632 LNCS, pp. 369376. Springer.
Biere, A., Cimatti, A., Clarke, E. M., & Zhu, Y. (1999). Symbolic Model Checking without
BDDs. Proc. TACAS99, Vol. 1579 LNCS, pp. 193207. Springer.
Brand, S. (2008). Personal communication.
Brand, S., Gennari, R., & de Rijke, M. (2003). Constraint Programming Modelling
Solving Modal Satisfability. Proc. CP 2003, Vol. 2833 LNCS, pp. 795800.
Springer.
Davis, M., Longemann, G., & Loveland, D. (1962). machine program theorem-proving.
Journal ACM, 5 (7), 394397.
386

fiAutomated Reasoning Modal Description Logics via SAT Encoding

Davis, M., & Putnam, H. (1960). computing procedure quantification theory. Journal
ACM, 7, 201215.
Donini, F., & Massacci, F. (2000). EXPTIME tableaux ALC. Artificial Intelligence,
124 (1), 87138.
Een, N., & Sorensson, N. (2004). Extensible SAT-solver. Proc. SAT03, Vol. 2919
LNCS, pp. 502518. Springer.
Fitting, M. (1983). Proof Methods Modal Intuitionistic Logics. D. Reidel Publishing.
Franconi, E. (1998). CRACK. Proc. Description Logics 98, Vol. 11 CEUR Workshop
Proceedings. CEUR-WS.org.
Giunchiglia, E., Giunchiglia, F., Sebastiani, R., & Tacchella, A. (2000). SAT vs. Translation
based decision procedures modal logics: comparative evaluation. Journal
Applied Non-Classical Logics, 10 (2), 145172.
Giunchiglia, E., Giunchiglia, F., & Tacchella, A. (2002). SAT-Based Decision Procedures
Classical Modal Logics. Journal Automated Reasoning, 28 (2), 143171.
Giunchiglia, F., & Sebastiani, R. (1996). Building decision procedures modal logics
propositional decision procedures - case study modal K. Proc. CADE-13,
Vol. 1104 LNAI, pp. 583597. Springer.
Giunchiglia, F., & Sebastiani, R. (2000). Building decision procedures modal logics
propositional decision procedures - case study modal K(m). Information
Computation, 162 (1/2), 158178.
Giunchiglia, F., Roveri, M., & Sebastiani, R. (1996). new method testing decision
procedures modal terminological logics. Proc. Description Logics 96,
Vol. WS-96-05 AAAI Technical Reports, pp. 119123. AAAI Press.
Haarslev, V., & Moeller, R. (2001). RACER System Description. Proc. IJCAR01,
Vol. 2083 LNAI, pp. 701706. Springer.
Halpern, J. Y. (1995). effect bounding number primitive propositions
depth nesting complexity modal logic. Artificial Intelligence, 75 (3),
361372.
Halpern, J., & Moses, Y. (1992). guide completeness complexity modal
logics knowledge belief. Artificial Intelligence, 54 (3), 319379.
Heuerding, A., & Schwendimann, S. (1996). benchmark method propositional
modal logics K, KT, S4. Tech. rep. IAM-96-015, University Bern, Switzerland.
Horrocks, I. (1998). Using expressive description logic: FaCT fiction?. Proc.
KR98, pp. 636647. Morgan Kaufmann.
Horrocks, I., & Patel-Schneider, P. F. (1999). Optimizing Description Logic Subsumption.
Journal Logic Computation, 9 (3), 267293.
Horrocks, I., Patel-Schneider, P. F., & Sebastiani, R. (2000). Analysis Empirical
Testing Modal Decision Procedures. Logic Journal IGPL, 8 (3), 293323.
387

fiSebastiani & Vescovi

Hustadt, U., Schmidt, R. A., & Weidenbach, C. (1999). MSPASS: Subsumption Testing
SPASS. Proc. Description Logics 99, Vol. 22 CEUR Workshop Proceedings,
pp. 136137. CEUR-WS.org.
Hustadt, U., & Schmidt, R. (1999). empirical analysis modal theorem provers. Journal
Applied Non-Classical Logics, 9 (4), 479522.
Kautz, H., McAllester, D., & Selman, B. (1996). Encoding Plans Propositional Logic.
Proc. KR96, pp. 374384. AAAI Press.
Konev, B., Walther, D., & Wolter, F. (2008). Logical Difference Problem Description
Logic Terminologies. Proc. IJCAR08, Vol. 5195 LNAI, pp. 259274. Springer.
Ladner, R. (1977). computational complexity provability systems modal propositional logic. SIAM Journal Computing, 6 (3), 467480.
Massacci, F. (1999). Design Results Tableaux-99 Non-Classical (Modal) System Competition. Proc. Tableaux99, Vol. 1617 LNCS, pp. 1418. SpringerVerlang.
Massacci, F. (2000). Single Step Tableaux modal logics: methodology, computations,
algorithms. Journal Automated Reasoning, 24 (3), 319364.
Massacci, F., & Donini, F. (2000). Design results TANCS-2000, Automated Reasoning Analytic Tableaux Related Methods. Proc. Tableaux 2000, Vol.
1847 LNCS, pp. 5256. Springer.
Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering efficient SAT solver. Proc. DAC01, pp. 530535. ACM.
Narizzano, M., Pulina, L., & Tacchella, A. (2006).
QBFEVAL Web Portal. Proc. JELIA06, Vol. 4160 LNCS, pp. 494497. Springer. See:
http://www.qbflib.org/index eval.php.
Pan, G., Sattler, U., & Vardi, M. Y. (2002). BDD-Based Decision Procedures K.
Proc. CADE-18, Vol. 2392 LNCS, pp. 1630. Springer.
Pan, G., & Vardi, M. Y. (2003). Optimizing BDD-based modal solver. Proc.
CADE-19, Vol. 2741 LNAI, pp. 7589. Springer.
Patel-Schneider, P. F. (1998). DLP system description. Proc. Tableaux98, pp. 8789.
Patel-Schneider, P. F., & Sebastiani, R. (2001). new system methodology generating random modal formulae. Proc. IJCAR01, Vol. 2083 LNAI, pp. 464468.
Springer-Verlag.
Patel-Schneider, P. F., & Sebastiani, R. (2003). New General Method Generate Random Modal Formulae Testing Decision Procedures. Journal Artificial Intelligence Research, (JAIR), 18, 351389.
Pipatsrisawat, T., & Darwiche, A. (2006). SAT Solver Description: Rsat. Available at:
http://fmv.jku.at/sat-race-2006/descriptions/9-rsat.pdf.
Rector, A., & Horrocks, I. (1997). Experience building large, re-usable medical ontology
using description logic transitivity concept inclusions. Proc. Workshop
Ontological Engineering, AAAI Spring Symposium (AAAI97). AAAI Press.
388

fiAutomated Reasoning Modal Description Logics via SAT Encoding

Schild, K. D. (1991). correspondence theory terminological logics: preliminary report.
Proc. IJCAI91, pp. 466471.
Sebastiani, R. (2007). Lazy Satisfiability Modulo Theories. Journal Satisfiability, Boolean
Modeling Computation (JSAT), 3, 141224.
Sebastiani, R., & Vescovi, M. (2006). Encoding Satisfiability Modal Description
Logics SAT: Case Study K(m)/ALC. Proc. SAT06, Vol. 4121
LNCS, pp. 130135. Springer.
Sebastiani, R., & Vescovi, M. (2009). Axiom Pinpointing Lightweight Description Logics
via Horn-SAT Encoding Conflict Analysis. Proc. CADE-22, Vol. 5663
LNAI. Springer. print.
Sebastiani, R. (2007). KSAT Delayed Theory Combination: Exploiting DPLL
Outside SAT Domain. Proc. FroCoS07, Vol. 4720 LNCS, pp. 2846.
Springer. Invited talk.
Seshia, S. A., Lahiri, S. K., & Bryant, R. E. (2003). Hybrid SAT-Based Decision Procedure
Separation Logic Uninterpreted Functions. Proc. DAC03, pp. 425430.
ACM.
Silva, J. P. M., & Sakallah, K. A. (1996). GRASP - new Search Algorithm Satisfiability.
Proc. ICCAD96, pp. 220227. IEEE Computer Society.
Sioutos, N., de Coronado, S., Haber, M. W., Hartel, F. W., Shaiu, W., & Wright, L. W.
(2007). NCI Thesaurus: semantic model integrating cancer-related clinical
molecular information. Journal Biomedical Informatics, 40 (1), 3043.
Spackman, K. A., Campbell, K., & Cote, R. (1997). SNOMED RT: reference terminology
healt care. Journal American Medical Informatics Association (Fall Symposium
Supplement), 640644.
Strichman, O. (2002). Solving Presburger Linear Arithmetic SAT. Proc.
FMCAD02, Vol. 2517 LNCS, pp. 160170. SpringerVerlag.
Strichman, O., Seshia, S., & Bryant, R. (2002). Deciding separation formulas SAT.
Proc. CAV02, Vol. 2404 LNCS, pp. 209222. Springer.
Tacchella, A. (1999). *SAT system description. Proc. Description Logics 99, Vol. 22
CEUR Workshop Proceedings, pp. 142144. CEUR-WS.org.
Gene Ontology Consortium (2000). Gene ontology: Tool unification biology.
Nature Genetics, 25, 2529.
Voronkov, A. (1999). KK: theorem prover K. Proc. CADE-16, Vol. 1632
LNAI, pp. 383387. Springer.
Voronkov, A. (2001). optimize proof-search modal logics: new methods proving
redundancy criteria sequent calculi. ACM Transactions Computational Logic,
2 (2), 182215.
Zhang, L., & Malik, S. (2002). quest efficient boolean satisfiability solvers. Proc.
CAV02, Vol. 2404 LNCS, pp. 1736. Springer.

389

fiJournal Artificial Intelligence Research 35 (2009) 161-191

Submitted 05/08; published 06/09

Eliciting Single-Peaked Preferences
Using Comparison Queries
Vincent Conitzer

conitzer@cs.duke.edu

Departments Computer Science
Economics, Duke University
Durham, NC, USA

Abstract
Voting general method aggregating preferences multiple agents.
agent ranks possible alternatives, based this, aggregate ranking
alternatives (or least winning alternative) produced. However, many
alternatives, impractical simply ask agents report complete preferences.
Rather, agents preferences, least relevant parts thereof, need elicited.
done asking agents (hopefully small) number simple queries
preferences, comparison queries, ask agent compare two
alternatives. Prior work preference elicitation voting focused case
unrestricted preferences. shown setting, sometimes necessary
ask agent (almost) many queries would required determine arbitrary
ranking alternatives. contrast, paper, focus single-peaked preferences.
show preferences elicited using linear number comparison
queries, either order respect preferences single-peaked known,
least one agents complete preferences known. show using sublinear
number queries suffice. also consider case cardinally single-peaked
preferences. case, show alternatives cardinal positions known,
agents preferences elicited using logarithmic number queries;
however, also show cardinal positions known, sublinear number
queries suffice. present experimental results elicitation algorithms.
also consider problem eliciting enough information determine aggregate
ranking, show even modest objective, sublinear number queries
per agent suffice known ordinal unknown cardinal positions. Finally,
discuss whether techniques applied preferences almost
single-peaked.

1. Introduction
multiagent systems, group agents often make joint decisions even
agents conflicting preferences alternatives. example, agents may
different preferences possible joint plans group, allocations tasks resources
among members group, potential representatives (e.g., presidential candidates), etc.
settings, important able aggregate agents individual preferences.
result aggregation single alternative, corresponding groups
collective decision, complete aggregate (compromise) ranking alternatives
(which useful, instance, alternatives later turn
feasible). general framework aggregating agents preferences
c
2009
AI Access Foundation. rights reserved.

fiConitzer

agents vote alternatives. is, agent announces complete ranking
alternatives (the agents vote), based votes outcome (i.e., winning
alternative complete aggregate ranking alternatives) chosen according
voting rule.1
One might try create aggregate ranking follows: given alternatives b,
votes prefer b vice versa (i.e., wins pairwise election b),
ranked b aggregate ranking. Unfortunately, preferences
agents unrestricted least three alternatives, Condorcet cycles
may occur. Condorcet cycle sequence alternatives a1 , a2 , . . . , ak
1 < k, agents prefer ai ai+1 vice versa, agents prefer ak
a1 vice versa. presence Condorcet cycle, impossible produce
aggregate ranking consistent outcomes pairwise elections. Closely
related phenomenon numerous impossibility results show every voting
rule significant drawbacks general setting. example, least
three alternatives, Arrows impossibility theorem (Arrow, 1963) shows voting rule
relative order two alternatives aggregate ranking independent
agents rank alternatives two (i.e., rule satisfies independence
irrelevant alternatives) must either dictatorial (i.e., rule simply copies ranking
fixed agent, ignoring agents) conflicting unanimity (i.e.,
alternatives b, rule sometimes ranks b even agents prefer b a).
another example, least three alternatives, Gibbard-Satterthwaite
theorem (Gibbard, 1973; Satterthwaite, 1975) shows voting rule onto (for
every alternative, exist votes would make alternative win) nondictatorial,
instances agent best casting vote correspond
agents true preferences (i.e., rule strategy-proof).
1.1 Single-Peaked Preferences
Fortunately, difficulties disappear agents preferences restricted, i.e.,
display structure. best-known, arguably important restriction
single-peaked preferences (Black, 1948). Suppose alternatives ordered
line, left right, representing alternatives positions. example,
political election, candidates position line may indicate whether left-wing
right-wing candidate (and strongly so). another example, alternatives may
numerical values: example, agents may vote size budget. yet another
example, alternatives may locations along road (for example, agents voting
construct building, meet dinner, etc.). say agents
preferences single-peaked respect alternatives positions if, side
agents preferred alternative (the agents peak), agent prefers alternatives
closer peak. example, set alternatives {a, b, c, d, e, f }, positions
may represented < b < e < f < < c, case vote f e b c
1. One may argue approach fully general allow agents specify
preferences probability distributions alternatives. example, impossible know
agents vote whether agent prefers second-ranked alternative 1/2 - 1/2 probability
distribution first-ranked third-ranked alternatives. principle, addressed
voting probability distributions instead, although practice usually tractable.

162

fiEliciting Single-Peaked Preferences Using Comparison Queries

single-peaked, vote f e c b (b side f
positions, b closer f , ranked higher b f peak).
(Throughout, assume preferences strict, is, agents never indifferent
two alternatives.) Preferences likely single-peaked alternatives
positions primary importance determining agents preferences. example,
political elections, voters preferences determined primarily candidates proximity
stance left-to-right spectrum, preferences likely single-peaked.
factors also important, perceived amicability candidates,
preferences necessarily likely single-peaked.
Formally, agent single-peaked preferences prefers a1 a2 , one following
must true:
a1 agents peak,
a1 a2 opposite sides agents peak,
a1 closer peak a2 .
agents preferences single-peaked (with respect positions
alternatives), known Condorcet cycles. If, addition, assume
number agents odd, pairwise election result tie. Hence,
aggregate ranking simply correspond outcomes pairwise elections.
case, also incentive agent misreport preferences, since
reporting preferences truthfully, will, pairwise election, rank alternative
prefers higher.
1.2 Important Single-Peaked Preferences?
start developing techniques deal specifically single-peaked preferences,
consider whether restricted class preferences interest.
concept single-peaked preferences extremely influential political science:
presumably best-known restriction preferences there, lies basis
much analytical work political science. thorough discussion given book
Analytical Politics Hinich Munger (1997). book almost immediately jumps
single-peaked preferences, argues that, form, models date back
Aristotle.
spite importance single-peaked preferences (analytical) political science
literature, aware empirical studies often voters preferences
actually single-peaked. reflection reveals answer probably highly dependent
particular election. example, election clear left-wing candidate,
clear centrist candidate, clear right-wing candidate, seems likely voters
preferences fact single-peaked: case, single-peakedness equivalent
ranking centrist candidate last. However, easy imagine scenarios
preferences single-peaked. example, candidates may lie
one-dimensional spectrum, two-dimensional spacefor instance, candidate may
left-wing candidate terms social issues right-wing candidate terms
economic issues. Unfortunately, turns multidimensional analogues singlepeaked preferences longer nice properties listed above: Condorcet cycles
occur again, strategic misreporting preferences becomes issue.
163

fiConitzer

seems political context, settings preferences
fact single-peaked, settings close single-peaked (for example,
may voters take physical attractiveness candidates account
result preferences quite single-peaked), settings
really single-peaked (for example, truly multidimensional
setting multiple unrelated issues). discuss generalization techniques
almost single-peaked preferences Section 6; multidimensional settings left
future research.
However, political elections means setting interest, especially
AI perspective; fact, important settings seem fit singlepeaked preference model much better. example, let us consider case
alternatives numerical values, potential sizes budget agents
deciding. Here, seems quite reasonable every agent preferred budget size,
always prefers sizes closer ideal one. fact, single-peaked preferences seem
reasonable political election, left-to-right spectrum merely
approximation complex phenomena; case numerical alternatives, however,
order alternatives absolute, likely reflected preferences.
Even settings preferences necessarily single-peaked, could simply
require agents report single-peaked preferences. merits somewhat
debatable, forcing non-single-peaked agents misreport preferences. hand, benefit avoiding Condorcet cycles (at
least reported preferences) natural aggregate ranking.2 related
approach sometimes suggested frame issue agents voting
(that is, choose set possible alternatives) way make single-peaked
preferences likely; again, merits debatable.
1.3 Preference Elicitation
key difficulty aggregating preferences multiple agents elicitation
agents preferences. many settings, particularly large sets alternatives,
agent communicate preferences impractical. one, take
large amount communication bandwidth. Perhaps importantly, order
agent communicate preferences, must first determine exactly
preferences are. complex task, especially guidance provided
agent key questions needs answer determine preferences.
alternative approach elicitor sequentially ask agents certain natural
queries preferences. example, elictor ask agent two alternatives prefers (a comparison query). Three natural goals elicitor (1) learn
enough agents preferences determine winning alternative, (2) learn enough
determine entire aggregate ranking, (3) learn agents complete preferences.
(1) (2) advantage general, agents preferences need
determined. example, (1), elicitor need elicit agents preferences
2. Analogously, combinatorial auctions (where agents bid bundles items instead
individual items) (Cramton et al., 2006), often bundles agents allowed
bid, variety reasons. Presumably, still leads better results separate
auction item.

164

fiEliciting Single-Peaked Preferences Using Comparison Queries

among alternatives already determined (from agents preferences) chance winning. even (3) significant benefits
elicitation (i.e., agent communicate preferences
own). First, elicitor provides agent systematic way assessing
preferences: agent needs answer simple queries. Second, perhaps
importantly, elicitor elicited preferences agents, elicitor
understanding preferences likely occur (and, perhaps,
understanding so). elicitor use understanding guide
elicitation next agents preferences, learn preferences rapidly.
paper, study elicitation single-peaked preferences using comparison
queries. mostly focus approach (3), i.e., learning agents complete preferences
though show Section 5 that, least worst case, cannot much better
pursue approach (2), i.e., learning enough preferences determine
aggregate ranking. paper, devote much space (1), i.e., learning enough
preferences determine winning alternative. noted (1)
significantly easier objective: well known (Condorcet) winner always
median agents preferred alternatives, suffices find agents
preferred alternative (see note end Section 5). paper,
assume preferences always single-peaked (the exception Section 6
discuss case preferences usually, always, single-peaked).
Section 3, study single-peaked preferences general form described
Subsection 1.1 (also known ordinally single-peaked preferences, contrast cardinally single-peaked preferences studied later paper). study setting
elicitor knows positions alternatives (Subsection 3.1), setting
elicitor (at least initially) (Subsection 3.2). Experimental results section provided Appendix A. Then, Section 4, study restricted setting
preferences cardinally single-peakedthat is, alternative agent
cardinal position R, agents rank alternatives distance cardinal position. (To prevent confusion, emphasize Section 4, alternatives position
refers place order alternatives, real number.) Again, study
setting elicitor knows positions alternatives (Subsection 4.1),
setting elicitor (at least initially) (Subsection 4.2). Experimental results section provided Appendix B. elicitation algorithms
completely elicit one agents preferences moving next agent (as opposed
going back forth agents). gives algorithms nice online property:
agents arrive time, elicit agents preferences arrives,
agent free leave (as opposed forced wait arrival
next agent). Especially case goal find aggregate ranking,
one may wonder efficient go back forth agents. turns
that, least worst case, cannot help (significantly), result presented
Section 5.
following tables summarize results paper (with exception
experimental results Appendices B, illustrate algorithms perform
random instances, results Section 6 preferences almost singlepeaked).
165

fiConitzer

ordinal
cardinal

positions known
(m) (Subsection 3.1)
(log m) (Subsection 4.1)

positions unknown
(m) (Subsections 3.1, 3.2)
(m) (Subsections 3.2, 4.2)

Table 1: Number comparison queries required fully elicit one agents single-peaked
preferences alternatives, worst case. upper bounds positions
unknown column, least one agents preferences must known (otherwise,
restriction current agents preferences hence answer (m log m)).

ordinal
cardinal

positions known
(nm) (Subsection 3.1, Section 5)
O(n log m) (Subsection 4.1)

positions unknown
(nm) (Subsection 3.2, Section 5)
(nm) (Subsection 3.2, Section 5)

Table 2: Number comparison queries required find aggregate ranking
alternatives n agents single-peaked preferences, worst case. upper
bounds positions unknown column, least one agents preferences must
known (otherwise, first agents preferences elicited using O(m log m) queries).

2. Related Research Case Unrestricted Preferences
section, first discuss related research; then, make basic observations
eliciting general (not necessarily single-peaked) preferences, serve
useful baseline comparison.
2.1 Related Research
Voting techniques drawing increasing interest artificial intelligence community,
especially multiagent systems researchers. Voting used applications
collaborative filtering, example, Pennock, Horvitz, Giles (2000); planning
among multiple automated agents, example, Ephrati Rosenschein (1991, 1993).
key research topics include voting rules computationally hard execute,
example, Bartholdi, Tovey, Trick (1989), Hemaspaandra, Hemaspaandra,
Rothe (1997), Cohen, Schapire, Singer (1999), Rothe, Spakowski, Vogel (2003),
Conitzer (2006); voting rules computationally hard manipulate strategic
agents, example, Bartholdi Orlin (1991), Hemaspaandra Hemaspaandra
(2007), Procaccia Rosenschein (2007), Conitzer, Sandholm, Lang (2007);
concisely representing preferences voting (Lang, 2007). Single-peaked preferences
also studied computational-complexity viewpoint (Walsh, 2007).
166

fiEliciting Single-Peaked Preferences Using Comparison Queries

Preference elicitation also important research topic artificial intelligence; prominent examples research include work Vu Haddawy (1997), Chajewska,
Getoor, Norman, Shahar (1998), Vu Haddawy (1998), Chajewska, Koller, Parr
(2000), Boutilier (2002), Braziunas Boutilier (2005), list means
exhaustive. preference elicitation multiagent systems, significant body work focuses combinatorial auctionsfor overview work, see chapter Sandholm
Boutilier (2006). Much work focuses elicitation approach (1), i.e., learning
enough bidders valuations determine optimal allocation. Sometimes, additional information must elicited bidders determine payments
make according Clarke (1971), generally, Groves (1973), mechanism. Example elicitation approaches include ascending combinatorial auctionsfor
overview, see chapter Parkes (2006)as well frameworks auctioneer
ask queries flexible way (Conen & Sandholm, 2001). significant amount
research preference elicitation combinatorial auctions also devoted elicitation
approach (3), i.e. learning agents complete valuation function. research, typically valuation functions assumed lie restricted class, given shown
agents complete valuation function elicited using polynomial number
queries kind. Various results nature obtained Zinkevich, Blum,
Sandholm (2003), Blum, Jackson, Sandholm, Zinkevich (2004), Lahaie Parkes
(2004), Santi, Conitzer, Sandholm (2004). work Lahaie Parkes (2004)
also includes results goal determine outcome.

also already work preference elicitation voting (the setting
paper). earlier work focused elicitation approach (1), eliciting
enough information agents determine winner, without restriction
space possible preferences. Conitzer Sandholm (2002) studied complexity
deciding whether enough information elicited declare winner, well
complexity choosing votes elicit given strong suspicions agents
vote. also studied additional opportunities strategic misreporting
preferences elicitation introduces, well avoid introducing opportunities.
(Strategic misreporting significant concern setting paper:
restriction single-peaked preferences, reporting truthfully dominant strategy
agents simultaneously report complete preferences, hence responding truthfully
elicitors queries ex-post equilibrium. such, paper make
distinction agents vote true preferences.) Conitzer Sandholm (2005)
studied elicitation algorithms determining winner various voting rules (without
suspicion agents vote), gave lower bounds worst-case amount
information agents must communicate. recentlyafter AAMAS-07 version
paper (Conitzer, 2007)the communication complexity determining winner
aggregate ranking domains single-peaked preferences studied (Escoffier
et al., 2008). communication-complexity part work (which main
contribution paper) considers ordinal case known positions,
results build AAMAS-07 version paper, well communication
complexity paper mentioned (Conitzer & Sandholm, 2005).
167

fiConitzer

2.2 Eliciting General Preferences
basis comparison, let us first analyze difficult elicit arbitrary (not
necessarily single-peaked) preferences using comparison queries. recall goal
extract agents complete preferences, i.e., want know agents exact ranking
alternatives. exactly problem sorting set elements,
binary comparisons elements used sorting.
extremely well-studied problem, well-known solved using O(m log m)
comparisons, example using MergeSort algorithm (which splits set elements
two halves, solves half recursively, merges solutions using linear
number comparisons). also well-known (m log m) comparisons required
(in worst case). One way see m! possible orders,
order encodes log(m!) bits informationand log(m!) (m log m). Hence, general,
method communicating order (not methods based comparison queries)
require (m log m) bits (in worst case).
Interestingly, common voting rules (including Borda, Copeland, Ranked
Pairs), shown using techniques communication complexity theory even
determining whether given alternative winner requires communication
(nm log m) bits (in worst case), n number agents (Conitzer & Sandholm,
2005). is, even try elicit agents complete preferences, (in worst
case) impossible constant factor better agent
communicate preferences! lower bounds even hold nondeterministic
communication, assume preferences unrestricted. contrast,
assuming preferences single-peaked, elicit agents complete preferences
using O(m) queries, show paper. course, know agents
complete preferences, execute voting rule. shows useful
elicitation know agents preferences lie restricted class.

3. Eliciting Ordinally Single-Peaked Preferences
section, study elicitation ordinally single-peaked preferences (the general
form single-peaked preferences described Subsection 1.1, contrast cardinally
single-peaked preferences, study Section 4). first study case
alternatives positions known (Subsection 3.1), case
known (Subsection 3.2). Experimental results algorithms section given
Appendix A; results give idea algorithms behave randomly drawn
instances, rather worst case.
3.1 Eliciting Knowledge Alternatives Ordinal Positions
subsection, focus setting elicitor knows positions
alternatives. Let p : {1, . . . , m} denote mapping positions alternatives,
i.e., p(1) leftmost alternative, p(2) alternative immediately right
p(1), . . ., p(m) rightmost alternative. algorithms make calls function
Query(a1 , a2 ), returns true agent whose preferences currently eliciting
168

fiEliciting Single-Peaked Preferences Using Comparison Queries

prefers a1 a2 , false otherwise. (Since one agents preferences elicited time,
need specify agent queried.)
first algorithm serves find agents peak (most preferred alternative).
basic idea algorithm binary search peak. so, need
able assess whether peak left right given alternative a.
discover asking whether alternative immediately right preferred
a: is, peak must right a, otherwise, peak must left
of, equal to, a.

FindPeakGivenPositions(p)
l1
rm
l < r {
m1 (l + r)/2
m2 m1 + 1
Query(p(m1 ), p(m2 ))
r m1
else
l m2
}
return l

found peak, continue construct agents ranking
alternatives follows. know agents second-ranked alternative must either
alternative immediately left peak, one immediately right.
single query settle one preferred. Without loss generality, suppose left
alternative preferred. Then, third-ranked alternative must either alternative
immediately left second-ranked alternative, alternative immediately
right peak. Again, single query sufficeetc. determined
ranking either leftmost rightmost alternative, construct remainder
ranking without asking queries (by simply ranking remaining alternatives
according proximity peak). algorithm formalized below. uses function
Append(a1 , a2 ), makes a1 alternative immediately succeeds a2 current
ranking (i.e., current agents preferences far constructed them).
pseudocode, omit (simple) details maintaining ranking linked list.
algorithm returns highest-ranked alternative; interpreted including
linked-list structure, effectively entire ranking returned. c always
alternative ranked last among currently ranked alternatives.
169

fiConitzer

FindRankingGivenPositions(p)
FindPeakGivenPositions(p)
p(t)
l t1
r t+1
cs
l 1 r {
Query(p(l), p(r)) {
Append(p(l), c)
c p(l)
l l1
} else {
Append(p(r), c)
c p(r)
r r+1
}
}
l 1 {
Append(p(l), c)
c p(l)
l l1
}
r {
Append(p(r), c)
c p(r)
r r+1
}
return

Theorem 1 FindRankingGivenPositions correctly determines agents preferences, using
2 + log comparison queries.
Proof: Correctness follows arguments given above. FindPeakGivenPositions requires log comparison queries. Every query allows us add
additional alternative ranking, last alternative need query,
hence 2 additional queries.
Thus, number queries algorithm requires linear number
alternatives. impossible succeed using sublinear number queries,
agents single-peaked preferences encode linear number bits, follows. Suppose
alternatives positions follows: am1 < am3 < am5 < . . . < a4 < a2 < a1 <
a3 < a5 < . . . < am4 < am2 < . Then, vote form a1 {a2 , a3 }
{a4 , a5 } . . . {am1 , } (where set notation indicates constraint
170

fiEliciting Single-Peaked Preferences Using Comparison Queries

preference alternatives set, is, {ai , ai+1 } replaced either
ai ai+1 ai+1 ai ) single-peaked respect alternatives positions.
agents preference alternatives ai ai+1 (for even i) encodes single bit, hence
agents complete preferences encode (m 1)/2 bits. Since answer comparison
query communicate single bit information, follows linear number
queries fact necessary.
3.2 Eliciting without Knowledge Alternatives Ordinal Positions
subsection, study difficult question: hard elicit agents
preferences alternatives positions known? Certainly, would desirable elicitor software require us enter domain-specific information
(namely, positions alternatives) elicitation begins, two reasons: (1)
information may available entity running election, (2) entering
information may perceived agents unduly influencing process, perhaps
outcome, election. Rather, software learn (relevant) information
domain elicitation process itself.
clear learning take place process eliciting
preferences multiple agents. Specifically, without knowledge positions
alternatives, first agents preferences could ranking alternatives, since
ranking single-peaked respect positions. Hence, eliciting first agents
preferences require (m log m) queries. elicitor knows first agents preferences, though, ways alternatives may positioned eliminated
(but many remain).
elicitor learn exact positions alternatives? answer no, several
reasons. First all, invert positions alternatives, making leftmost
alternative rightmost, etc., without affecting preferences single-peaked
respect positions. fundamental problem elicitor could
choose either one positionings. significantly, agents preferences may simply
give elicitor enough information determine positions. example,
agents turn preferences, elicitor never learn anything
alternatives positions beyond learned first agent. case,
however, elicitor could simply try verify next agent whose preferences
elicited preferences, done using linear number
queries. generally, one might imagine intricate elicitation scheme either
requires queries elicit agents preferences, learns something new useful
preferences shorten elicitation process later agents. Then, one
might imagine complex accounting scheme, spirit amortized analysis, showing
total elicitation cost many agents cannot large.
Fortunately, turns need anything complex. fact, knowing even
one agents (complete) preferences enough elicit agents preferences using
linear number queries! (And sublinear number suffice, since already
showed linear number necessary even know alternatives positions. Hence,
matter many agents preferences already know, always require linearly
many queries next agent.) prove this, give elicitation algorithm
171

fiConitzer

takes input one (the first) agents preferences (not positions alternatives),
elicits another agents preferences using linear number queries. course, algorithm
still applied already know preferences one agent;
case, use preferences one agents input. worst case,
knowing preferences one agent help us, may
agents preferences, case teach us anything
alternatives positions beyond learned first agent. general
(not worst case), may able learn something additional agents
preferences; however, best, learn alternatives positions exactly, even
case still need linear number queries. Hence, paper, investigate
use known preferences multiple agents.
First, need subroutine finding agents peak. cannot use algorithm
FindPeakGivenPositions previous subsection, since know positions.
However, even trivial algorithm examines alternatives one one maintains
preferred alternative far requires linear number queries,
simply use algorithm.

FindPeak()
a1
{a2 , . . . , }
Query(a, s)
sa
return

found agents peak, next find alternatives lie
peak, peak known vote (i.e., peak agent whose preferences
know). following lemma key tool so.
Lemma 1 Consider votes v1 v2 peaks s1 s2 , respectively. Then, alternative

/ {s1 , s2 } lies two peaks v1 s2 v2 s1 .
Proof: lies two peaks, i, lies closer si s3i (the
votes peak) lies si . Hence v1 s2 v2 s1 . Conversely, vi s3i implies
lies side s3i si (otherwise, vi would ranked s3i higher).
since true i, implies must lie peaks.

Thus, find alternatives peak known vote peak
current agent, simply ask current agent, alternative known vote
prefers peak current agent, whether prefers alternative known
votes peak. answer positive, add alternative list alternatives
peaks.
two votes must rank alternatives peaks exact opposite
order. Thus, point, know current agents preferences alternatives
172

fiEliciting Single-Peaked Preferences Using Comparison Queries

lie peak peak known vote (including peaks themselves).
final complex step integrate remaining alternatives ranking.
(Some remaining alternatives may ranked higher alternatives
peaks.) strategy integrate alternatives current
ranking one one, order known vote ranks them, starting
one known vote ranks highest. integrating alternative, first
current agent compare worst-ranked alternative already ranking.
note known vote must prefer latter alternative, latter alternative
either known votes peak, alternative integrated earlier
hence preferred known vote. latter alternative also preferred current
agent, add new alternative bottom current ranking move
next alternative. not, learn something useful positions
alternatives, namely new alternative lies side current agents
peak alternative currently ranked last. following lemma proves this. it, v1
takes role known vote, v2 takes role agent whose preferences
currently eliciting, a1 takes role alternative currently ranked last current
agent, a2 takes role alternative currently integrating.
Lemma 2 Consider votes v1 v2 peaks s1 s2 , respectively. Consider two alternatives a1 , a2 6= s2 lie s1 s2 . Suppose a1 v1 a2 a2 v2 a1 .
Then, a1 a2 must lie opposite sides s2 .
Proof: a1 a2 lie side s2 without loss generality, left side
then, neither lies s1 s2 , must also lie left side s1
(possibly, one equal s1 ). then, v1 v2 cannot disagree a1
a2 ranked higher.

purpose integrating new alternative, knowing side
alternative currently ranked last helpful. fact, reach point
algorithm, simply start comparing new alternative alternatives
already ranking, one one. benefit knowing
side make later alternatives easier integrate: specifically,
integrated new alternative, know alternatives integrate later must
end ranked alternative. following lemma, v1
takes role known vote, v2 takes role agent whose preferences
currently eliciting, a1 takes role alternative currently ranked last current
agent, a2 takes role alternative currently integrating, a3 takes
role alternative integrated later.
Lemma 3 Consider votes v1 v2 peaks s1 s2 , respectively. Consider three
alternatives a1 , a2 , a3 6= s2 lie s1 s2 . Suppose a1 v1 a2 v1 a3 ,
a2 v2 a1 . Then, a2 v2 a3 .
Proof: Lemma 2, know a1 a2 must lie opposite sides s2 . Moreover,
lie s1 s2 , equal s2 , must also lie
opposite sides s1 (with additional possibility a1 = s1 ). a1 v1 a2 v1 a3 ,
173

fiConitzer

follows either a3 lies side peaks a2 , away;
side a1 , away. former case, immediately obtain a2 v2 a3 ;
latter case, a1 v2 a3 , implies a2 v2 a3 a2 v2 a1 .

Based observation, algorithm, let c1 alternative
know later alternatives integrate end ranked it. (In sense,
c1 takes role a2 lemmas.) also keep track c2 , alternative
currently ranked last. (In sense, c2 takes role a1 lemmas.) Then,
integrate new alternative, first compare c2 ; ranked c2 ,
place c2 , make new c2 . ranked c2 , compare
successor c1 ; ranked that, compare successor
successor; on, find place; finally, make new c1 . last
stage may require linear number queries, key observation time
ask query, c1 moves ranking one place. Hence, total number
queries ask (summing alternatives integrate)
1, c1 move 1 times; get linear
bound number queries.
Example 1 Suppose alternatives positions e < c < b < f < < d. Also suppose
known vote f b c e, preferences agent
currently eliciting c e b f d. algorithm proceed follows.
First, FindPeak identify c current agents peak. Now, alternatives
known vote prefers c (the known votes peak), well d, f, b. last
three alternatives, algorithm queries current agent whether preferred a.
answer positive (only) b f , know two alternatives must lie
peaks c line (and hence must ranked oppositely known
vote current agent). point, know current agent must prefer
c b f a, algorithm must integrate e ranking. set c1 = c
c2 = a. algorithm first integrates since ranked higher e known
vote. algorithm queries agent c2 = a, preferred. know
current agent must prefer c b f d, algorithm sets c2 = d.
Finally, algorithm must integrate e. algorithm queries agent e c2 = d,
e preferred. algorithm queries agent e successor c1 ,
b. e preferred, algorithm inserts e c1 = c b, sets c1 = e.
point know entire ranking c e b f d.
present algorithm formally. algorithm uses function
Append(a1 , a2 ), makes a1 alternative immediately succeeds a2 current
ranking. also uses function InsertBetween(a1 , a2 , a3 ), inserts a1 a2
a3 current ranking. algorithm (eventually) set m(a) true lies
peaks current agent known vote v, peak v; otherwise,
m(a) set false. v(i) returns alternative known vote ranks ith (and hence
v 1 (a) returns ranking alternative known vote, v 1 (a1 ) < v 1 (a2 ) means
v prefers a1 a2 ). n(a) returns alternative immediately following current
ranking. Again, peak returned, includes linked-list structure
hence entire ranking.
174

fiEliciting Single-Peaked Preferences Using Comparison Queries

FindRankingGivenOtherVote(v)
FindPeak()

m(a) false
{s, v(1)}
v 1 (a) < v 1 (s)
Query(a, v(1))
m(a) true
c1
c2
m(v(1)) true
= 1 step 1 {
m(v(i)) = true {
Append(v(i), c2 )
c2 v(i)
}
}
= 1
(m(v(i)) v(i) = s)
Query(c2 , v(i)) {
Append(v(i), c2 )
c2 v(i)
} else {
Query(n(c1 ), v(i))
c1 n(c1 )
InsertBetween(v(i), c1 , n(c1 ))
c1 v(i)
}
return

Theorem 2 FindRankingGivenOtherVote correctly determines agents preferences, using
4m 6 comparison queries.
Proof: Correctness follows arguments given above. FindPeak requires 1
comparison queries. next stage, discovering alternatives lie current
agents peak known votes peak, requires 2 queries. Finally,
must count number queries integration step. complex,
integrating one alternative (which may 2 times) require
multiple queries. Certainly, algorithm ask agent compare alternative
currently integrated current c2 . contributes 2 queries
total. However, current alternative preferred c2 , must ask queries,
comparing current alternative alternative currently ranked immediately behind
current c1 (perhaps multiple times). every time ask query, c1
175

fiConitzer

changes another alternative, happen 1 times total. follows
total number queries ask (summed alternatives
integrate) 1. Adding bounds, get total bound
(m 1) + (m 2) + (m 2) + (m 1) = 4m 6.

course, impossible succeed sublinear number queries,
reason Subsection 3.1. practice, algorithm ends requiring average roughly
3m queries, seen Appendix A.

4. Eliciting Cardinally Single-Peaked Preferences
section, restrict space allowed preferences slightly further. assume
alternative cardinal position r(a) R (as opposed merely ordinal
positions considered point paper, provide
distance function alternatives). already mentioned examples
reasonable: agents voting budget, location along
road (in case alternatives cardinal position equals distance beginning
road). Even alternatives political candidates, may reasonable
cardinal positions: example, candidates voting records,
candidates cardinal position percentage times cast right-wing
vote. Additionally, assume every agent also cardinal position r,
ranks alternatives order proximity position. is, agent sorts
alternatives according |r r(a)| (preferring closer ones). preferences known
cardinally single-peaked preferences (again, opposed ordinally single-peaked
preferences considered point) (Brams et al., 2002, 2005).3
Cardinally single-peaked preferences always ordinally single-peaked well (with
respect order < defined < r(a) < r(a )), converse hold:
possible agents preferences ordinally single-peaked, cardinally singlepeaked. is, possible agents preferences consistent single order
alternatives, way assign cardinal positions alternatives
agents agents preferences given ranking alternatives according
proximity cardinal position. following example shows this:
Example 2 Suppose agent 1 prefers b c d, agent 2 prefers b c a, agent
3 prefers c b d. preferences consistent order < b < c < d,
ordinally single-peaked. Nevertheless, cardinal positions
consistent. prove contradiction: let us suppose cardinal
positions. sometimes ranked last, implies must leftmost
rightmost alternatives (without loss generality, suppose leftmost alternative).
Agent 1s preferences imply order must < b < c < d. {1, 2, 3}, let
r(i) agent cardinal position. agents preferences, must |r(2) r(d)| <
|r(2) r(a)| |r(3) r(d)| > |r(3) r(a)|. Therefore, must r(2) > r(3).
3. precise, Brams, Jones, Kilgour (2002; 2005) consider settings agents
alternatives coincide, agent exact position one alternatives;
extension case quite obvious. fact, Brams et al. (2002) explicitly
mention separating agents alternatives extension.

176

fiEliciting Single-Peaked Preferences Using Comparison Queries

hand, must |r(2) r(b)| < |r(2) r(c)| |r(3) r(b)| > |r(3) r(c)|.
Therefore, must r(2) < r(3)but contradiction. Hence
profile preferences cardinally single-peaked.
similar example given Brams et al. (2002)in fact, preferences
example proper subset ones used examplealthough proof
examples preferences cardinally single-peaked, make use fact
agents alternatives coincide model, simplifies argument somewhat.
So, restriction cardinally single-peaked preferences come loss
preferences represent. particular, emphasized even alternatives
correspond numerical values, may well still case preferences
ordinally, cardinally, single-peaked. example, agents voting budget
project, one agents preferences might budget definitely
go 500 (perhaps starts coming cost another project
agent cares about); long constraint met, budget
large possible. Hence, agents preferences would 500 499 498 . . . 1
0 501 502 . . ., would ordinally, cardinally, single-peaked. Thus,
cardinally single-peaked preferences strictly less likely occur practice ordinally
single-peaked preferences; nevertheless, cardinally single-peaked preferences (or something
close it) may well occur practice. example, agents voting
location project along road, likely rank locations simply
distance positions along road. detailed discussion symmetry
assumption inherent cardinally single-peaked preferences given Hinich Munger
(1997).
ordinally single-peaked preferences, first study case
alternatives cardinal positions known (Subsection 4.1), case
known (Subsection 4.2). Experimental results known cardinal positions
given Appendix B; results give idea algorithm behaves
randomly drawn instances, rather worst case. (We experimental results
unknown cardinal positions, negative result there.)
4.1 Eliciting Knowledge Alternatives Cardinal Positions
subsection, show preferences cardinally single-peaked, addition
cardinal positions alternatives known, preference elicitation
algorithm uses logarithmic number comparison queries per agent. high
level, algorithm works follows. pair alternatives a, (r(a) < r(a )),
consider midpoint m(a, ) = (r(a) + r(a ))/2. Now, agent prefers ,
agents position r must smaller m(a, ); otherwise, r > m(a, ). (As before,
assume ties, r 6= m(a, ).) So, using single comparison query,
determine whether agents position left right particular midpoint.
allows us binary search midpoints. (As aside, know
agents position coincides position one alternatives,
binary search alternatives find agents position, using FindPeakGivenPositions.
case general, need binary search midpoints instead.) end, know two adjacent midpoints
177

fiConitzer

agents position lies, sufficient determine preferences. following example
illustrates this.
Example 3 Suppose alternatives cardinal positions (which known algorithm) follows: r(a) = .46, r(b) = .92, r(c) = .42, r(d) = .78, r(e) = .02. Also suppose
agent whose preferences eliciting cardinal position r = .52 (which
known algorithm), preferences c b e. midpoints are,
order: m(c, e) = .22, m(a, e) = .24, m(d, e) = .40, m(a, c) = .44, m(b, e) = .47, m(c, d) =
.60, m(a, d) = .62, m(b, c) = .67, m(a, b) = .69, m(b, d) = .85. Since 5th midpoint (out
10) m(b, e) = .47, algorithm first queries agent whether prefers b e.
agent prefers b, algorithm conclude r > .47 (since r(b) > r(e)). 7th midpoint
m(a, d) = .62, algorithm next queries agent whether prefers d. agent
prefers a, algorithm conclude r < .62. Finally, 6th midpoint m(c, d) = .60,
algorithm next queries agent whether prefers c d. agent prefers c,
algorithm conclude r < .60. algorithm knows .47 < r < .60, since
midpoints range, sufficient deduce agents preferences
c b e.
precise algorithm follows.

FindRankingGivenCardinalPositions(r)

a, r(a) < r(a )
{(a, , (r(a) + r(a ))/2)}
sort 3rd entry obtain = {(ai , ai , mi )}1i(m)
2
l0
u
2 +1
l + 1 < u {
h (l + u)/2
Query(ah , ah )
uh
else
lh
}
r (ml + mu )/2
return alternatives sorted |r(a) r|

Theorem 3 FindRankingGivenCardinalPositions correctly determines agents preferences,
using 2log(m) comparison queries.
Proof: Correctness follows arguments given above. Let us consider expression
u l 1, number midpoints strictly lth uth. expression starts
2 , (at least) halved query. reaches 0,
178

fiEliciting Single-Peaked Preferences Using Comparison Queries


queries
asked. Therefore, log(
2 ) + 1 queries required.

2
log(
2 ) = log(m(m 1)/2) = log(m(m 1)) 1 < log(m ) 1 = 2 log(m) 1. Hence
FindRankingGivenCardinalPositions requires 2log(m) comparison queries.


algorithm require us store manage
2 midpoints,
scale computationally extremely large numbers alternatives, unlike
previous algorithms. important remember elicitation cost (as measured
number comparison queries) different type cost computational cost.
Typically, constraints elicitation much severe computation.
agent human, number queries willing answer likely
low. Even agent software agent, answer query, may
solve hard computational problem. also reasons agents (human
software) may wish keep number queries answer minimum:
example, concerned privacy, may worry answering queries
leaves exposed malevolent party intercepting answers. Therefore,
typically much important elicitation algorithm ask queries
computationally efficient. Nevertheless, rare settings queries
easily answered (presumably software agent), (privacy) issues
come play, number alternatives large, computational constraint
may binding. case, earlier FindRankingGivenPositions may preferred
algorithm.
also (log m) lower bound: example, even agents positions coincide alternatives positions, even without restrictions type
communication (queries), agent needs communicate (log m) bits identify
alternatives preferred alternative.
4.2 Eliciting without Knowledge Alternatives Cardinal Positions
consider case preferences cardinally single-peaked, alternatives cardinal positions known (at beginning). case
ordinally single-peaked preferences, know alternatives positions,
first agents preferences arbitrary hence require (m log m) queries elicit.
hand, already know one agents preferences, use algorithm ordinally single-peaked preferences elicit next agents preferences using
O(m) queries, cardinally single-peaked preferences special case ordinally
single-peaked preferences. However, since considering cardinally single-peaked
preferences, may hope better O(m) bound: all, saw
cardinal positions known, O(log m) queries required per agent. So,
might hope eliciting number agents preferences, learned
enough cardinal positions alternatives elicit next agents
preferences using O(log m) queries, least using sublinear number
queries. Unfortunately, turns possible: following result gives
(nm) lower bound number queries necessary elicit n agents preferences. (The
result phrased also applies ordinally single-peaked preferences, case
even (ordinal) positions known.)
179

fiConitzer

Theorem 4 Suppose n agents alternatives (where even),
agents preferences known cardinally single-peaked, alternatives positions
known. Then, elicit agents preferences exactly, worst case, least
nm/2 comparison queries necessary.
Remarks: remains true even alternatives ordinal positions known beginningmore
specifically, even alternatives cardinal position known beginning lie certain interval,
intervals overlap. also remains true (in addition) agents need queried
orderthat is, possible ask agent 1 queries first, agent 2, return agent 1, etc.

Proof: Suppose alternative ai (with {1, . . . , m}) known (from beginning)
lie interval [ki 1, ki + 1], ki = 10 (1)i (i + 1)/2. is, k1 = 10, k2 =
10, k3 = 20, k4 = 20, k5 = 30, etc. Moreover, suppose agents position
known (from beginning) lie interval [1, 1]. clear agent js
preferences form {a1 , a2 } j {a3 , a4 } j {a5 , a6 } j . . . j {am1 , } (where
set notation indicates clear two preferred). Certainly,
sufficient ask agent, every {1, 3, 5, . . . , 1}, whether ai preferred
ai+1 . total number queries would nm/2 (and ask
order). need show, however, worst case, queries
also necessary. see why, suppose answer every one queries
ask ai j ai+1 (the odd-numbered alternative preferred). (This certainly possible:
example, may every alternative ai position ki , every agent
1.) Then, absolutely sure every agents complete preferences, must ask
every one queries, following reason. Suppose asked queries
one: j odd i, yet asked agent j whether j prefers ai ai+1 .
must show still possible ai+1 j ai . see possible, suppose
every agent j j 6= j position 1; j position 0.1; every alternative
ai 6= position ki ; ai position ki 1. easy see
positions, {1, 3, 5, . . . , 1} \ {i}, agent j (including j),
ai j ai +1 . Moreover, even i, agent j 6= j, ai j ai+1 :
|(ki 1) (1)| = 10(i + 1)/2, |ki+1 (1)| = 10(i + 1)/2 + 1.
means positions consistent answers queries far. However,
positions, ai+1 j ai : |(ki 1) (0.1)| = 10(i + 1)/2 + .9,
|ki+1 (0.1)| = 10(i + 1)/2 + 0.1. Hence, must ask last query.


5. Determining Aggregate Ranking
far, goal determine agents complete preferences.
desirable preference information, always necessary. example,
goal may simply determine aggregate ranking alternatives. (We recall
introduction single-peaked preferences, Condorcet cycles,
natural aggregate ranking determined outcomes pairwise elections
assuming number agents odd.) purpose, certainly sufficient elicit
agents preferences completely, clear necessary. So,
Theorem 4 gives us (nm) lower bound elicting agents preferences completely
180

fiEliciting Single-Peaked Preferences Using Comparison Queries

unknown cardinal positions case (and also known ordinal positions case),
immediately clear lower bound also holds merely trying
determine aggregate ranking. Unfortunately, turns still (nm)
lower bound case (under conditions Theorem 4). Hence,
possible get away o(nm) querieswith exception case known cardinal
positions, case lower bound apply,
fact need O(n log m) queries determine agents preferences completely (by using
algorithm Subsection 4.1 agent).
Theorem 5 Suppose n agents alternatives (where n odd even),
agents preferences known cardinally single-peaked, alternatives
positions known. Then, determine aggregate ranking, worst case,
least (n + 1)m/4 comparison queries necessary.
Remarks: remains true even alternatives ordinal positions known beginningmore
specifically, even alternatives cardinal position known beginning lie certain interval,
intervals overlap. also remains true (in addition) agents need queried
orderthat is, possible ask agent 1 queries first, agent 2, return agent 1, etc.

Proof: proof reuses much structure proof Theorem 4. Again,
suppose alternative ai (with {1, . . . , m}) known (from beginning) lie
interval [ki 1, ki + 1], ki = 10 (1)i (i + 1)/2. is, k1 = 10, k2 = 10, k3 =
20, k4 = 20, k5 = 30, etc. Moreover, again, suppose agents position
known (from beginning) lie interval [1, 1]. Again, clear agent
js preferences form {a1 , a2 } j {a3 , a4 } j {a5 , a6 } j . . . j {am1 , }
(where set notation indicates clear two preferred).
relevantly theorem, aggregate ranking must also form. Hence,
need determine is, every {1, 3, 5, . . . , 1}, whether agents prefer ai
ai+1 , vice versa. show worst case, m/2 pairwise
elections, need query least (n+1)/2 agents, thereby proving theorem.
proof Theorem 4, suppose answer every one queries ask
ai j ai+1 (the odd-numbered alternative preferred). Then, certainly sufficient to,
every {1, 3, 5, . . . , 1}, query (any) (n + 1)/2 agents ai vs. ai+1 ,
result majority ai . also necessary ask many queries
pairwise elections, following reasons. Suppose {1, 3, 5, . . . , 1},
queried (n 1)/2 agents ai vs. ai+1 . Then, even
pairwise elections, asked queries, still possible ai+1 defeats ai
pairwise election. reason similar proof Theorem 4: let J set
agents already asked ai vs. ai+1 . Suppose every agent j
j J position 1; every j
/ J position 0.1; every alternative ai 6=
position ki ; ai position ki 1. easy see positions,
{1, 3, 5, . . . , 1} \ {i}, agent j (including agents J outside J),
ai j ai +1 . Moreover, even i, agent j J, ai j ai+1 :
|(ki 1) (1)| = 10(i + 1)/2, |ki+1 (1)| = 10(i + 1)/2 + 1. means
positions consistent answers queries far. However,
181

fiConitzer

positions, j
/ J, ai+1 j ai : |(ki 1) (0.1)| = 10(i + 1)/2 + .9,
|ki+1 (0.1)| = 10(i + 1)/2 + 0.1. Hence, positions, ai+1 defeats ai
pairwise election. So, rule out, need ask least one query.

contrast, even less ambitious goal determine winner
(the top-ranked alternative aggregate ranking), need know
agents peak: well known winner median peaks (if
alternative peak multiple agents, alternative counted multiple
times calculation median). know least ordinal positions,
use FindPeakGivenPositions using O(log m) queries per agent.

6. Robustness Slight Deviations Single-Peakedness
far, assumed agents preferences always (at least ordinally,
cases cardinally) single-peaked. section, consider possibility
agents preferences close single-peaked, quite. easy imagine
political election: example, one candidates happens close friend
agent, agent might place candidate high ranking even
opposite ends political spectrum.
get detail this, keep mind case agents
preferences (completely) single-peakedthe case studied sectionis still
important. political election, likely deviations
single-peakedness (due to, example, candidates charisma positions issues
unrelated left-right spectrum), settings seems significantly less likely.
example, agents voting numerical alternatives, size
budget, seems likely preferences fact single-peaked.
Another issue many nice properties single-peaked preferences
hold almost single-peaked preferences. example, may get Condorcet cycles
again, longer clear preferences aggregated. context, rule
aggregating preferences also likely manipulable (by declaring false preferences).
know alternatives positions, one simple solution simply require
agent submit single-peaked preferences (thereby forcing agents without singlepeaked preferences manipulate, definition). analogous approach often taken
combinatorial auctions, restricting set valuation functions agents
report, purpose making winner determination problem elicitation
problem easier. reasoning result likely still better running
combinatorial auction all. One may wonder requiring reported preferences
single-peaked could desirable effects. example, one might argue
forces voters political election ignore candidates charisma (which one may argue
could desirable). precise analysis beyond scope paper.
Still, interesting potentially important able elicit almost singlepeaked preferences, section, study extent done.
interpret idea agents preferences close single-peaked multiple
ways.
182

fiEliciting Single-Peaked Preferences Using Comparison Queries

1. may case agents single-peaked
preferences (e.g., agents know candidates personally, ones
rank candidates according positions political spectrum).
2. may case individual agent, agents preferences close
single-peaked (e.g., agent may know one two candidates personally,
many candidates, agent rank remaining candidates
according positions political spectrum).
section, consider mostly first interpretation (an example illustrating
second interpretation seems difficult deal given below). Let us
suppose fraction agents preferences single-peaked. One
straightforward strategy following. Let us suppose, now, know alternatives positions. agent j, elicit preferences agent
single-peaked. result, obtain ranking a1 a2 . . . alternatives,
know must js ranking js preferences fact single-peaked.
verify whether indeed js preferences, asking j compare a1 a2 , a2
a3 , . . ., am1 . 1 queries result expected answer
(ai j ai+1 ), know agent js preferences. Otherwise, start beginning, elicit js preferences without single-peakedness assumption, using standard
O(m log m) sorting algorithm. (Of course, practice, actually start
completely beginningwe still use answers queries
already asked.) leads following propositions.
Proposition 1 fraction agents preferences (ordinally cardinally) single-peaked, (ordinal cardinal) positions alternatives known,
elicit agents preferences using (on average) O(m + log m) queries.
Proof: FindRankingGivenPositions requires O(m) queries; verification step.
time verification step fails, need another O(m log m) queries case.

note approach, longer significant advantage
cardinally single-peaked preferences: whereas O(log m) queries enough sure
preferences cardinally single-peaked (and know alternatives positions),
sure, need 1 queries verification step.
may tempting think agents preferences almost single-peaked
(in sense second interpretation above), verification queries
turn way expect, already know agents preferences.
see case, suppose alternatives positions (from left
right) a1 < a2 < . . . < a8 . consider left-wing voter nevertheless friends
a5 , resulting very-close-to-single-peaked preferences a1 a2 a3 a5 a4
a6 a7 a8 . FindRankingGivenPositions start running FindPeakGivenPositions,
whose first query result answer a5 a4 . result, FindPeakGivenPositions
conclude agent right-wing voter, eventually conclude a5
peak. FindRankingGivenPositions return preferences a5 a4 a3
183

fiConitzer

a2 a1 a6 a7 a8 . Then, roughly half verification queries turn
way expect. (This example easily generalized number alternatives.)
So, let us return first interpretation, let us consider case
alternatives positions known. case, algorithm FindRankingGivenOtherVote, uses O(m) queries requires us know another agents preferences, elicit first. Now, consider case agents
preferences single-peaked, two ways get trouble
algorithm FindRankingGivenOtherVote: current agents preferences may singlepeaked, earlier agents preferences using algorithm may
single-peaked. either case, algorithm may fail identify current agents preferences correctly. Again, address asking verification queries current
agent, necessary reverting standard sorting algorithm.
Assuming independence, probability current earlier agents
preferences single-peaked (1 )2 . eliciting many agents preferences,
little bit careful: always use agents preferences known
preferences, run risk agents preferences actually single-peaked,
hence verification step might fail (almost) every agent. conservative approach
always use previous agents preferences. assume whether one agents
preferences single-peaked independent whether case previous
agent, indeed, (1 )2 time agents preferences single-peaked.
leads following proposition.

Proposition 2 fraction agents preferences (ordinally
cardinally) single-peaked (and independent one agent next),
(ordinal cardinal) positions alternatives unknown, elicit agents
preferences using (on average) O(m + (2 2 )m log m) queries (with exception
first agent).

Proof: FindRankingGivenOtherVote requires O(m) queries; verification step.
verification step fail either previous current agents preferences
single-peaked, probability 1 (1 )2 = 2 2 ; need another
O(m log m) queries case.

(In practice, may want use different known vote every time. Rather,
may want use one see verification step fails often;
so, switch another known vote, otherwise not.)
Propositions 1 2 seem provide good solution first interpretation
almost single-peaked preferences, less clear second interpretation (the
example illustrates difficulty). Still, would desirable design algorithms
efficiently elicit preferences almost single-peaked second interpretation. generally, large amount work preference elicitation
assumes preferences lie particular class, would interesting see
algorithms generalized deal preferences close classes.
184

fiEliciting Single-Peaked Preferences Using Comparison Queries

7. Conclusions
Voting general method aggregating preferences multiple agents. agent
ranks possible alternatives, based this, aggregate ranking alternatives
(or least winning alternative) produced. However, many alternatives,
impractical simply ask agents report complete preferences. Rather,
agents preferences, least relevant parts thereof, need elicited. done
asking agents (hopefully small) number simple queries preferences,
comparison queries, ask agent compare two alternatives. Prior
work preference elicitation voting focused case unrestricted preferences.
shown setting, sometimes necessary ask agent (almost)
many queries would required determine arbitrary ranking alternatives.
contrast, paper, focused single-peaked preferences. agents preferences
said single-peaked fixed order alternatives, alternatives
positions (representing, instance, alternatives left-wing
right-wing), agent prefers alternatives closer agents
preferred alternative ones away. first showed agents
preferences single-peaked, alternatives positions known, agents
(complete) preferences elicited using linear number comparison queries.
alternatives positions known, first agents preferences arbitrary
therefore cannot elicited using linear number queries. However, showed
already know least one agents preferences, elicit (next)
agents preferences using linear number queries (albeit larger number queries
first algorithm). also showed using sublinear number queries
suffice. also considered case cardinally single-peaked preferencesthat is,
alternative agent cardinal position R, agents rank alternatives
distance position. case, showed alternatives cardinal
positions known, agents preferences elicited using logarithmic
number queries; however, also showed cardinal positions known,
sublinear number queries suffice. presented experimental results
elicitation algorithms. also considered problem eliciting enough information
determine aggregate ranking, showed even modest objective,
sublinear number queries per agent suffice known ordinal unknown
cardinal positions. Finally, discussed whether techniques applied
preferences almost single-peaked. showed algorithms presented earlier
paper used agents preferences (completely) single-peaked.
case agents preferences almost single-peaked seems difficult; gave
example illustrating why.
Future research includes studying elicitation voting restricted classes
preferences. class single-peaked preferences (over single-dimensional domains)
natural one study first, due practical relevance useful theoretical
properties (no Condorcet cycles and, result, ability aggregate preferences
strategy-proof manner). Classes practically relevant nice
theoretical properties still interest, though. example, one may consider settings
alternatives take positions two-dimensional rather single-dimensional space.
185

fiConitzer

well-known generalization, Condorcet cycles occur again.
true almost single-peaked settings discussed above. Nevertheless,
imply efficient elicitation algorithms exist settings.
imply elicitation algorithms would useless, since still often necessary
vote alternatives settings. However, use voting rule strategyproof, must carefully evaluate strategic effects elicitation. Specifically,
queries agents asked, may able infer something
agents answered queries them; this, turn, may affect (strategically)
choose answer queries, since rule strategy-proof. phenomenon
studied detail Conitzer Sandholm (2002).

Acknowledgments
thank AAMAS JAIR reviewers valuable feedback (the JAIR
reviewers provided especially detailed helpful feedback). Conitzer supported
NSF award number IIS-0812113 Alfred P. Sloan Research Fellowship.

Appendix A. Experimental Results Ordinally Single-Peaked
Preferences
following experiment compares FindRankingGivenPositions, FindRankingGivenOtherVote,
MergeSort. discussed Subsection 2.2, MergeSort standard sorting algorithm
uses comparison queries, therefore used elicit agents preferences
without knowledge alternatives positions votes. Conversely,
algorithm elicits general preferences using comparison queries used solve
sorting problem. So, effectively, want compare algorithm
use fact preferences single-peaked, cannot compare anything
sorting algorithm. conceivable sorting algorithms perform slightly
better MergeSort problem, require (m log m) comparisons.
run, first random permutation alternatives drawn represent
positions alternatives. Then, two random votes (rankings) single-peaked
respect positions drawn. vote, done randomly
choosing peak, randomly choosing second-highest ranked alternative
two adjacent alternatives, etc. algorithm elicited second vote; FindRankingGivenPositions given (costless) access positions, FindRankingGivenOtherVote
given (costless) access first vote. (For run, verified algorithm
produced correct ranking.) Figure 1 shows results (please note logarithmic scale
x-axis). FindRankingGivenPositions outperforms FindRankingGivenOtherVote,
turn clearly outperforms MergeSort.
One interesting observation FindRankingGivenOtherVote sometimes repeats
query asked before. Thus, simply storing results previous queries,
number queries reduced. However, general, keeping track queries
asked imposes significant computational burden,
2 possible com186

fiEliciting Single-Peaked Preferences Using Comparison Queries

1.6e+06
MergeSort
FindRankingGivenOtherVote
FindRankingGivenPositions
1.4e+06

# comparison queries

1.2e+06

1e+06

800000

600000

400000

200000

0
1000

10000
# alternatives

100000

Figure 1: Experimental comparison MergeSort, FindRankingGivenOtherVote, FindRankingGivenPositions. Please note logarithmic scale x-axis.
data point averaged 5 runs.

parison queries. Hence, experiment, results previous queries stored.
FindRankingGivenPositions MergeSort never repeat query.

Appendix B. Experimental Results Cardinally Single-Peaked
Preferences
Next, experimentally compare FindRankingGivenCardinalPositions FindRankingGivenPositions. Since former requires cardinally single-peaked preferences, must generate
preferences different way Appendix A. generate preferences run
drawing cardinal position uniformly random [0, 1] alternative, well
agent. agent ranks alternatives according proximity cardinal
position. (For run, verified algorithm produced correct ranking.)
note computationally, algorithm scale large numbers alternatives previous algorithms, reasons mentioned earlier (managing
midpoints). Figure 2 shows results, clearly contrasting FindRankingGivenCardinalPositionss logarithmic nature FindRankingGivenPositionss linear (and somewhat less
predictable) nature. (Please note logarithmic scale x-axis.)
187

fiConitzer

800
FindRankingGivenPositions
FindRankingGivenCardinalPositions
700

# comparison queries

600

500

400

300

200

100

0
10

100
# alternatives

1000

Figure 2: Experimental comparison FindRankingGivenPositions FindRankingGivenCardinalPositions. Please note logarithmic scale x-axis. data point
averaged 5 runs.

References
Arrow, K. (1963). Social choice individual values (2nd edition). New Haven: Cowles
Foundation. 1st edition 1951.
Bartholdi, III, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social
Choice Welfare, 8 (4), 341354.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989). Voting schemes difficult
tell election. Social Choice Welfare, 6, 157165.
Black, D. (1948). rationale group decision-making. Journal Political Economy,
56 (1), 2334.
Blum, A., Jackson, J., Sandholm, T., & Zinkevich, M. (2004). Preference elicitation
query learning. Journal Machine Learning Research, 5, 649667.
Boutilier, C. (2002). POMDP formulation preference elicitation problems. Proceedings National Conference Artificial Intelligence (AAAI), pp. 239246, Edmonton, AB, Canada.
Brams, S. J., Jones, M. A., & Kilgour, D. M. (2002). Single-peakedness disconnected
coalitions. Journal Theoretical Politics, 14 (3), 359383.
188

fiEliciting Single-Peaked Preferences Using Comparison Queries

Brams, S. J., Jones, M. A., & Kilgour, D. M. (2005). Forming stable coalitions: process
matters. Public Choice, 125, 6794.
Braziunas, D., & Boutilier, C. (2005). Local utility elicitation GAI models. Proceedings
21st Annual Conference Uncertainty Artificial Intelligence (UAI), pp. 4249,
Edinburgh, UK.
Chajewska, U., Getoor, L., Norman, J., & Shahar, Y. (1998). Utility elicitation
classification problem. Proceedings Conference Uncertainty Artificial
Intelligence (UAI), pp. 7988, Madison, WI, USA.
Chajewska, U., Koller, D., & Parr, R. (2000). Making rational decisions using adaptive
utility elicitation. Proceedings National Conference Artificial Intelligence
(AAAI), pp. 363369, Austin, TX, USA.
Clarke, E. H. (1971). Multipart pricing public goods. Public Choice, 11, 1733.
Cohen, W., Schapire, R., & Singer, Y. (1999). Learning order things. Journal Artificial
Intelligence Research, 10, 213270.
Conen, W., & Sandholm, T. (2001). Preference elicitation combinatorial auctions: Extended abstract. Proceedings ACM Conference Electronic Commerce (EC),
pp. 256259, Tampa, FL, USA.
Conitzer, V. (2006). Computing Slater rankings using similarities among candidates.
Proceedings National Conference Artificial Intelligence (AAAI), pp. 613619,
Boston, MA, USA.
Conitzer, V. (2007). Eliciting single-peaked preferences using comparison queries. Proceedings International Conference Autonomous Agents Multi-Agent Systems
(AAMAS), pp. 408415, Honolulu, HI, USA.
Conitzer, V., & Sandholm, T. (2002). Vote elicitation: Complexity strategy-proofness.
Proceedings National Conference Artificial Intelligence (AAAI), pp. 392397,
Edmonton, AB, Canada.
Conitzer, V., & Sandholm, T. (2005). Communication complexity common voting rules.
Proceedings ACM Conference Electronic Commerce (EC), pp. 7887, Vancouver, BC, Canada.
Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidates
hard manipulate? Journal ACM, 54 (3), Article 14, 133.
Cramton, P., Shoham, Y., & Steinberg, R. (2006). Combinatorial Auctions. MIT Press.
Ephrati, E., & Rosenschein, J. S. (1991). Clarke tax consensus mechanism among
automated agents. Proceedings National Conference Artificial Intelligence
(AAAI), pp. 173178, Anaheim, CA, USA.
189

fiConitzer

Ephrati, E., & Rosenschein, J. S. (1993). Multi-agent planning dynamic search social
consensus. Proceedings Thirteenth International Joint Conference Artificial
Intelligence (IJCAI), pp. 423429, Chambery, France.
Escoffier, B., Lang, J., & Ozturk, M. (2008). Single-peaked consistency complexity.
Proceedings DIMACS-LAMSADE Workshop Algorithmic Decision Theory,
pp. 101114, Paris, France.
Gibbard, A. (1973). Manipulation voting schemes: general result. Econometrica, 41,
587602.
Groves, T. (1973). Incentives teams. Econometrica, 41, 617631.
Hemaspaandra, E., & Hemaspaandra, L. A. (2007). Dichotomy voting systems. Journal
Computer System Sciences, 73 (1), 7383.
Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (1997). Exact analysis Dodgson
elections: Lewis Carrolls 1876 voting system complete parallel access NP. Journal
ACM, 44 (6), 806825.
Hinich, M. J., & Munger, M. C. (1997). Analytical Politics. Cambridge University Press.
Lahaie, S., & Parkes, D. (2004). Applying learning algorithms preference elicitation.
Proceedings ACM Conference Electronic Commerce (EC), pp. 180188, New
York, NY, USA.
Lang, J. (2007). Vote aggregation combinatorial domains structured preferences.
Proceedings Twentieth International Joint Conference Artificial Intelligence
(IJCAI), pp. 13661371, Hyderabad, India.
Parkes, D. (2006). Iterative combinatorial auctions. Cramton, P., Shoham, Y., & Steinberg, R. (Eds.), Combinatorial Auctions, chap. 2, pp. 4177. MIT Press.
Pennock, D. M., Horvitz, E., & Giles, C. L. (2000). Social choice theory recommender
systems: Analysis axiomatic foundations collaborative filtering. Proceedings
National Conference Artificial Intelligence (AAAI), pp. 729734, Austin, TX,
USA.
Procaccia, A. D., & Rosenschein, J. S. (2007). Junta distributions average-case
complexity manipulating elections. Journal Artificial Intelligence Research, 28,
157181.
Rothe, J., Spakowski, H., & Vogel, J. (2003). Exact complexity winner problem
Young elections. Theory Computing Systems, Vol. 36(4), pp. 375386. SpringerVerlag.
Sandholm, T., & Boutilier, C. (2006). Preference elicitation combinatorial auctions.
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.), Combinatorial Auctions, chap. 10, pp.
233263. MIT Press.
190

fiEliciting Single-Peaked Preferences Using Comparison Queries

Santi, P., Conitzer, V., & Sandholm, T. (2004). Towards characterization polynomial
preference elicitation value queries combinatorial auctions. Conference
Learning Theory (COLT), pp. 116, Banff, Alberta, Canada.
Satterthwaite, M. (1975). Strategy-proofness Arrows conditions: Existence correspondence theorems voting procedures social welfare functions. Journal
Economic Theory, 10, 187217.
Vu, H., & Haddawy, P. (1997). Problem-focused incremental elicitation multi-attribute
utility models. Proceedings Conference Uncertainty Artificial Intelligence
(UAI), pp. 215222, San Francisco, CA, USA.
Vu, H., & Haddawy, P. (1998). Towards case-based preference elicitation: Similarity measures preference structures. Proceedings Conference Uncertainty Artificial Intelligence (UAI), pp. 193201.
Walsh, T. (2007). Uncertainty preference elicitation aggregation. Proceedings
National Conference Artificial Intelligence (AAAI), pp. 38, Vancouver, BC,
Canada.
Zinkevich, M., Blum, A., & Sandholm, T. (2003). polynomial-time preference elicitation
value queries. Proceedings ACM Conference Electronic Commerce (EC),
pp. 176185, San Diego, CA, USA.

191

fiJournal Artificial Intelligence Research 35 (2009) 49-117

Submitted 10/08; published 05/09

Message-Based Web Service Composition, Integrity Constraints,
Planning Uncertainty: New Connection
Jorg Hoffmann

JOE . HOFFMANN @ SAP. COM

SAP Research
Karlsruhe, Germany

Piergiorgio Bertoli

BERTOLI @ FBK . EU

Fondazione Bruno Kessler
Trento, Italy

Malte Helmert

HELMERT @ INFORMATIK . UNI - FREIBURG . DE

Albert-Ludwigs-Universitat Freiburg
Freiburg, Germany

Marco Pistore

PISTORE @ FBK . EU

Fondazione Bruno Kessler
Trento, Italy

Abstract
Thanks recent advances, AI Planning become underlying technique several applications. Figuring prominently among automated Web Service Composition (WSC)
capability level, services described terms preconditions effects ontological concepts. key issue addressing WSC planning ontologies formal
vocabularies; also axiomatize possible relationships concepts. axioms correspond termed integrity constraints actions change literature,
applying web service essentially belief update operation. reasoning required belief
update known harder reasoning ontology itself. support belief update
severely limited current planning tools.
first contribution consists identifying interesting special case WSC
significant tractable. special case, term forward effects, characterized
fact every ramification web service application involves least one new constant
generated output web service. show that, setting, reasoning required
belief update simplifies standard reasoning ontology itself. relates to, extends,
current notions message-based WSC, need belief update removed strong
(often implicit informal) assumption locality individual messages. clarify
computational properties forward effects case, point strong relation standard notions planning uncertainty, suggesting effective tools latter successfully
adapted address former.
Furthermore, identify significant sub-case, named strictly forward effects, actual
compilation planning uncertainty exists. enables us exploit off-the-shelf planning tools solve message-based WSC general form involves powerful ontologies,
requires reasoning partial matches concepts. provide empirical evidence
approach may quite effective, using Conformant-FF underlying planner.

c
2009
AI Access Foundation. rights reserved.

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

1. Introduction
Since mid-nineties, AI Planning tools become several orders magnitude scalable,
invention automatically generated heuristic functions search techniques
(see McDermott, 1999; Bonet & Geffner, 2001; Hoffmann & Nebel, 2001; Gerevini, Saetti, &
Serina, 2003; Helmert, 2006; Chen, Wah, & Hsu, 2006). paved way adoption
planning underlying technology several applications. One application area
web service composition (WSC), paper mean automated composition
semantic web services (SWS). SWS pieces software advertised formal description
do. Composing SWS means link together aggregate behavior
satisfies complex user requirement. ability automatically compose web services key
reducing human effort time-to-market constructing integrated enterprise applications.
result, widely recognized economic potential WSC.
wide-spread SWS frameworks OWL-S1 WSMO2 , SWS described two distinct
levels. One addresses overall functionality SWS, details precisely
interact SWS. former level, called service profile OWL-S service
capability WSMO, SWS described akin planning operators, preconditions effects. Therefore, planning prime candidate realizing WSC level. approach
follow paper.
setting, key aspect SWS preconditions effects described relative
ontology defines formal (logical) vocabulary. Indeed, ontologies much
formal vocabularies introducing set logical concepts. also define axioms constrain behavior domain. instance, ontology may define subsumption relationship
two concepts B, stating members necessarily members B.
natural interpretation axiom, context WSC, every state encountered every possible configuration domain entities must satisfy axiom. sense,
ontology axioms correspond integrity constraints discussed actions change literature
(Ginsberg & Smith, 1988; Eiter & Gottlob, 1992; Brewka & Hertzberg, 1993; Lin & Reiter, 1994;
McCain & Turner, 1995; Herzig & Rifi, 1999).3 Hence WSC considered like planning
presence integrity constraints. Since constraints affect outcome action executions,
facing frame ramification problems, execution actions corresponds closely
complex notions belief update (Lutz & Sattler, 2002; Herzig, Lang, Marquis, & Polacsek,
2001). Unsurprisingly, providing support integrity constraints modern scalable planning tools mentioned poses serious challenges. best knowledge, yet
attempted all.
Regarding existing WSC tools, planning tools employed solving WSC problems,
situation isnt much better. tools ignore ontology, i.e., act constraints
domain behavior given (Ponnekanti & Fox, 2002; Srivastava, 2002; Narayanan & McIlraith,
2002; Sheshagiri, desJardins, & Finin, 2003; Pistore, Traverso, & Bertoli, 2005b; Pistore, Marconi, Bertoli, & Traverso, 2005a; Agarwal, Chafle, Dasgupta, Karnik, Kumar, Mittal, & Srivastava,
2005a). approaches tackle full generality belief update using general reasoners,
1. example, see work Ankolekar et al. (2002) Burstein et al. (2004).
2. example, see work Roman et al. (2005) Fensel et al. (2006).
3. Integrity constraints sometimes also called state constraints domain constraints.

50

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

suffer inevitable performance deficiencies (Eiter, Faber, Leone, Pfeifer, & Polleres, 2003;
Giunchiglia, Lee, Lifschitz, McCain, & Turner, 2004).
planningbased
formalization

WSC Formalism

variant

restriction
rich version

Forward Effects

WSC

planningbased
formalization

Messagebased WSC

restriction

Conformant Planning


tackled

Strictly Forward Effects

Figure 1: overview planning WSC frameworks addressed paper. Special cases
identified herein shown red / boldface.
work addresses middle ground two extremes, i.e., trade-off
expressivity scalability WSC. via identification special cases
tackled efficiently. Figure 1 gives overview WSC planning frameworks involved.
brief, forward effects case requires every effect ramification web service
affects least one new constant generated web services output. situation,
frame problem trivializes, making planning problem similar common notions
conformant planning (Smith & Weld, 1998; Bonet & Geffner, 2000; Cimatti, Roveri, & Bertoli,
2004; Hoffmann & Brafman, 2006). discuss existing tools latter, particular
Conformant-FF (Hoffmann & Brafman, 2006), extended deal WSC forward
effects. strictly forward effects, action effects required affect outputs,
devise actual compilation conformant planning. thus obtain scalable tool interesting
WSC problems integrity constraints. particular able exploit (some of) heuristic
techniques mentioned (Hoffmann & Nebel, 2001; Hoffmann & Brafman, 2006).
follows, explain various parts Figure 1 little detail. starting
point WSC formalism, addressing WSC terms planning presence integrity constraints, discussed above. formalism essentially enriched form conformant planning.
distinguishing aspects are:
initial state description conjunction literals (possibly mentioning
logical facts task, hence introducing uncertainty).
Actions conditional effects semantics, meaning executed state,
effect applicable.
Actions may output variables, i.e., may create new constants.
set integrity constraints, universally quantified clause.
semantics action execution defined terms belief update operation.
Section 2 provides details choices, motivates example
results literature. show, planning formalism hard. Particularly,
51

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

even testing whether given action sequence plan p2 -complete. contrast
common notions conformant planning, plan testing coNP-complete.
see, forward effects remove additional complexity. Intuitively, forward effects
case covers situation web service outputs new constants, sets characteristic
properties relative inputs, relies ontology axioms describe ramifications
concerning new constants. case syntactically characterized follows:
(1) Every effect literal contains least one output variable.
(2) Within integrity constraint, every literal set variables arguments.
definition best understood example. Consider following variant widespread virtual travel agency (VTA). Web services book travel accommodation must
linked. web services generate new constants corresponding tickets reservations.
example, integrity constraints stating subsumption, z : trainTicket(z)
ticket(z). web service bookTicket may input variable x, precondition train(x ),
output variable y, effect trainTicket(y) ticketFor (y, x). forward effects task:
every effect literal contains output variable y, integrity constraint single variable
z provides arguments literals constraint. Say one instantiates input
bookTicket constant c output new constant d. applying resulting
ground action state train(c) holds true, constant gets created, characteristic
properties relative inputs trainTicket(d) ticketFor (d, c) set directly action.
integrity constraint takes care ramification, establishing ticket(d) holds. Note
status c apart relation affected way. 4
forward effects case closely related wide-spread notion WSC problems,
refer message-based WSC. approaches, composition semantics based
chaining input output messages web services, one sense. Inferences
ontology axioms made many approaches, restricted way limited
assumption locality individual messages, interferences affect particular
message transfer, implications transfers ignored. locality assumption
usually made informal way, often stated explicitly all. One contribution work
shed light issue, via identification forward effects case lies
message-based WSC full planning framework belief update semantics.
message-based WSC forward effects case share focus output constants.
two important differences. First, forward effects case restricted messagebased WSC terms ontology axioms allowed. Essentially, forward effects correspond
special case WSC locality assumption message-based WSC actually justified,
within full planning framework. Second, full framework comes benefit increased
flexibility combination services, locality enforced (e.g. output one
service may reused several points plan).
computational point view, key property forward effects case
removes need belief update. nutshell, reason actions affect new propositions, i.e., propositions involving least one output constant. (Recall point made
4. latter would case effect bookTicket included literal affecting x (example:
train(x)), integrity constraint capable mixing old new constants (example: x, :
trainTicket(y) train(x)).

52

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

unchanged status c, VTA example above.) output constant (d, example)
exist prior application action, hence previous belief carries knowledge
need revised. Consider characterization forward effects, given above.
Condition (1) ensures immediate effect action affects new propositions. Condition (2) ensures changes new propositions propagate new propositions. Since
literals constraint share variables, output constant question copied
them. see, virtue properties complexity plan testing coNP-complete,
rather p2 -complete, forward effects case.
complexity reduction critical reduced complexity
common notions conformant planning initial state uncertainty. Therefore feasible adapt conformant planning tools address WSC forward effects. Scalable planning
tools conformant planning already developed (Cimatti et al., 2004; Bryce, Kambhampati, & Smith, 2006; Hoffmann & Brafman, 2006; Palacios & Geffner, 2007). Hence
promising line research. example, focus Conformant-FF tool (Hoffmann
& Brafman, 2006) (short CFF) outline main steps need taken adapting CFF
handle WSC forward effects.
identify case actual compilation conformant planning initial
state uncertainty exists. that, one must fix set constants priori. manner
fairly standard (see, e.g., Settlers domain Long & Fox, 2003), simply include set
subset potential constants used instantiate outputs. subtle idea
put forward identify condition actions predict properties
assigned potential constants, case created. enables us design
compilation moves action effects initial state formula, uses actions
modify set constants already exist. way, reasoning initial state formula
compiled task reasoning output constants original task,
reasoning mechanisms included tools CFF naturally used implement latter.
trick predicting output properties require actions compatible sense
either produce different outputs, effects. turns condition
naturally given restriction forward effects, call strictly forward effects,
web service effects concern new constants.
Clearly, able reference inputs limitation. example, longer
say, VTA example, output ticket input x. Still, strictly forward
effects case describes interesting class WSC problems. class corresponds web services
modeled early versions OWL-S, example, logical connection
inputs outputs. Further, class WSC problems allows powerful ontologies
universally quantified clauses makes possible combine services flexibly. Using
compilation, class problems solved off-the-shelf tools planning
uncertainty.
validate compilation approach empirically running number tests using CFF
underlying planner. use two test scenarios, scalable variety parameters, covering range different problem structures. examine CFF reacts various
parameters. Viewed isolation, results demonstrate large complex WSC instances
comfortably solved using modern planning heuristics.
comparison alternative WSC tools problematic due widely disparate nature
kinds problems tools solve, kinds input languages understand,
53

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

purpose respective developers mind. nevertheless provide assessment
comparative benefits approach run tests DLVK tool Eiter et al. (2003)
Eiter, Faber, Leone, Pfeifer, Polleres (2004). DLVK one planning tools
deals ontology axioms called static causal rules directly, without need restrict
forward effects without need compilation. Since, context work,
main characteristic WSC presence ontology axioms, means DLVK one
existing native WSC tools. comparison, forward effects compilation approach
solves similar problem, sacrifices expressivity. question is, principle
gain anything sacrifice? Absolutely, answer yes. DLVK much slower
compilation+CFF, solving small fraction test instances even always provided
correct plan length bound. emphasize wish over-state results,
due above-mentioned differences tools. conclusion draw
trade-off expressivity scalability WSC important, forward effects case
seems constitute interesting point trade-off.
paper organized follows. First, Section 2 provides background necessary
understand context contribution work. Section 3 introduces WSC planning
formalism. Section 4 defines discusses forward effects. Section 5 introduces compilation
planning uncertainty, Section 6 presents empirical results. discuss closely
related work relevant points text, Section 7 provides complete overview.
Finally, Section 8 concludes discusses future work. improve readability, proofs
moved Appendix replaced text proof sketches.

2. Background
context work rather intricate. WSC new topic posing many different
challenges existing techniques, effect field populated disparate works differing considerably underlying purpose scope. words, common ground
fairly thin area. Further, work actually involves three fields research WSC, planning,
reasoning actions change relevant understanding contribution.
reasons, explain background detail. first discuss WSC general,
WSC Planning particular. state relevant facts belief update. finally
consider message-based WSC.
2.1 WSC, WSC Planning
Composition semantic web services received considerable attention last years.
general formulation problem, shared large variety works, focuses capability
level, web service conceived atomic operator transforms concepts.
specifically, service defined via IOPE description: service receives input set
typed objects, and, provided precondition P holds, produces output set
typed objects effect E guaranteed hold. typing objects exchanged
services given terms membership concepts. Concepts classes defined
within ontologies, exploit Description Logics (DL), form logic, formally
define universe concepts admitted discourse. ontology express complex relationships among concepts, like subsumption hierarchy, way objects belonging concept
structured parts referring concepts.
54

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

general setting instantiated various ways depending kind conditions
admitted preconditions/effects services, kind logics underlying ontology
definitions. Independent this, problem semantic web service composition stated
one linking appropriately set existing services aggregate behavior
desired service (the goal). illustrate problem, consider following example,
inspired work Thakkar, Ambite, Knoblock (2005) e-services bioinformatics
(and relies actual structure proteins, see example Petsko & Ringe, 2004; Branden &
Tooze, 1998; Chasman, 2003; Fersht, 1998):
Example 1 Say want compose web service provides information different classes
proteins. ontology states classes proteins exist, structural characteristics
may occur. available information service every structural characteristic,
presentation service combines range information. Given particular protein class,
composed web service run relevant information services, present output.
Concretely, classes proteins distinguished location (cell, membrane, intermembrane, . . . ). modeled predicates protein(x), cellProtein(x), membraneProtein(x),
intermembraneProtein(x), along sub-concept relations x : cellProtein(x)
protein(x). individual protein characterized following four kinds structures:
1. primary structure states proteins sequence amino-acids, e.g., 1kw3(x) (a protein called Glyoxalase) 1n55(x) (a protein called Triosephosphate Isomerase).
2. secondary structure states proteins external shape terms DSSP (Dictionary Secondary Structure Proteins) code, admitting limited set possible values.
example, G indicates 3-turn helix, B -sheet, on. total set values
G,H,I,T,E,B,S.
3. tertiary structure categorizes proteins 3-D shape.
4. subset proteins, quaternary structure categorizes proteins shape
combined complexes proteins (amounting 3000 different shapes, see example
3DComplex.org, 2008).
various axioms constrain domain, apart mentioned subconcept
relations. First, obvious axioms specify protein value four
kinds structures (i.e., protein sequence amino-acids, external shape, etc). However,
also complex axioms. Particular kinds proteins come particular structure
values. modeled axioms as:
x : cellProtein(x) G(x) 1n55(x)
x : cellProtein(x) B(x) 1kw3(x) complexBarrel(x)
DSSP code Z information service, named getInfoDSSPZ , whose precondition
Z(x) whose effect InfoDSSP(y) output service. Similarly, information services amino-acids, 3-D shapes, shapes complexes. presentation service,
named combineInfo, requires information four kinds structures created,
effect combinedPresentation(y) (where output combineInfo).
55

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

input composed web service protein c (a logical constant) class.
goal x : combinedPresentation(x). solution reason characteristics may
occur, apply respective information services, run combineInfo. variant
problem, additional requestInfo service used initiate information request, i.e.,
output requestInfo protein c class.
example shows ontology axioms play crucial role form WSC, formulating
complex dependencies different concepts. Note applying web service may indirect consequences implied ontology axioms. example, output requestInfo
service implications kinds information services required.
Another interesting aspect Example 1 requires SWS community calls partial matches, opposed plug-in matches (Paolucci, Kawamura, Payne, & Sycara, 2002; Li
& Horrocks, 2003; Kumar, Neogi, Pragallapati, & Ram, 2007).5 Consider situation one
wants connect web service w another web service w . is, w executed prior
w , output w used instantiate input w . w w said
partial match if, given ontology axioms, output w sometimes suffices provide
necessary input w . contrast, w w said plug-in match if, given ontology
axioms, output w always suffices provide necessary input w .
Plug-in matches tackled many approaches WSC, whereas partial matches tackled
few. Part reason probably plug-in matches easier handle, many types
WSC algorithms. Indeed existing WSC tools support plug-in matches (see detailed
discussion WSC tools Section 7). Example 1 cannot solved plug-in matches
information services provides necessary input combineInfo service
particular cases.
base work planning formalism allows specify web services (i.e., actions)
outputs, allows specify ontology axioms. axioms interpreted integrity
constraints, resulting semantics corresponds closely common intuitions behind WSC,
well existing formal definitions related WSC (Lutz & Sattler, 2002; Baader, Lutz,
Milicic, Sattler, & Wolter, 2005; Liu, Lutz, Milicic, & Wolter, 2006b, 2006a; de Giacomo, Lenzerini, Poggi, & Rosati, 2006). Since one main aims able exploit existing planning
techniques, consider particular form ontology axioms, correspondence representations used existing tools planning uncertainty. Namely, axioms
universally quantified clauses. example subsumption relation x : trainTicket(x)
ticket(x) mentioned above, usual B abbreviation B. planning task
specifies set clauses, interpreted conjunction clauses. Note provides
significant modeling power. meaning universal quantification clauses
clauses hold planning objects logical constants known exist. sense,
interpretation formulas closed-world customary planning tools. However, contrast
standard planning formalisms including PDDL, assume fixed set constants.
Rather, specification actions outputs enables dynamic creation new constants.
quantifiers ontology axioms range constants exist respective world.
similar fashion, planning goal may contain variables, existentially quantified.
constants used instantiate goal may pre-existed, may generated
5. terminology works slightly different use here, also describe additional kinds
matches. details given Section 7.

56

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

outputs web services applied path world. Consider
illustration goal x : combinedPresentation(x) Example 1, goal variable x
instantiated output created combineInfo service.
Another important aspect planning formalism allow incomplete initial state
descriptions. initial state corresponds input user provides composed web
service. Certainly cannot assume contains complete information every aspect
world. (In Example 1, initial state tells us class proteins interested
in, leaves open consequences regarding possible structural characteristics.)
consider case observability, i.e., conformant planning. outcome
WSC sequence web services satisfies user goal possible situations.6
customary conformant planning, actions conditional effects semantics, i.e., fire
precondition holds true, otherwise nothing. Note that, way, obtain notion
partial matches: solution employs different actions depending situation.
main difference planning formalism formalisms underlying current planning tools presence integrity constraints, effect semantics executing actions. semantics defined belief update operation.
2.2 Belief Update
correspondence web service applications belief update first observed Lutz
Sattler (2002), followed Baader et al. (2005), Liu et al. (2006b, 2006a) de Giacomo
et al. (2006). original statement belief update problem, given belief ,
i.e., logical formula defining worlds considered possible. given formula ,
update. Intuitively, corresponds observation telling us world changed
way that, now, true. want obtain formula defining worlds
possible given update. Certainly, need |= . Ensuring corresponds
well-known ramification problem. time, however, world change
unnecessarily. is, want close possible , among formulas
satisfy . corresponds frame problem.
Say want apply action presence integrity constraints. describes
worlds possible prior application a. resulting set possible worlds.
integrity constraints correspond formula IC holds , require
hold . update formula given conjunction action effect IC , i.e.,
= effa IC . means update previous belief information that,
a, effa new formula required hold, IC still true. example, may
action effect A(c) subsumption relation concepts B, formulated clause
x : A(x) B(x). update formula A(c) x : A(x) B(x) ensures B(c) true
.
Belief update widely considered literature AI databases (see example
Fagin, Kuper, Ullman, & Vardi, 1988; Ginsberg & Smith, 1988; Winslett, 1988, 1990; Katzuno
& Mendelzon, 1991; Herzig, 1996; Herzig & Rifi, 1999; Liu et al., 2006b; de Giacomo et al.,
2006). various approaches differ exactly defined. best consensus
one approach adequate every application context. approaches
6. course, generally, observability partial web service effects also uncertain. consider
generalizations here. Extending notions accordingly straightforward, future work.

57

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

agree hold updated state affairs, |= . Major differences lie
exactly taken mean close possible . Various authors,
example Brewka Hertzberg (1993), McCain Turner (1995), Herzig (1996), Giunchiglia
Lifschitz (1998), argue notion causality needed, addition (or even instead of)
notion integrity constraints, model domain behavior natural way. counter
arguments, neither follow causal approach work. reason ontologies
context WSC, example ontologies formulated web ontology language OWL
(McGuinness & van Harmelen, 2004), incorporate notion causality. given
set axioms, made intention describe behavior domain itself, rather
behavior exhibits changed particular web services. idea work try
leverage (or reasonably close having). Consideration causal
approaches WSC left future work.
Belief update computationally hard problem. Eiter Gottlob (1992) Liberatore
(2000) show that, non-causal approaches defining , reasoning typically
harder reasoning class formulas used formulating . Specifically, deciding
whether particular literal true 2p -hard even complete conjunction
literals (corresponding single world state) propositional CNF formula.
problem coNP-hard even single world state propositional Horn formula.
use results show that, planning formalism, checking plan testing whether
given action sequence plan 2p -complete, deciding polynomially bounded plan
existence 3p -complete.
Given complexity, perhaps unsurprising support integrity constraints current planning tools severely limited. existing planning tools support integrity
constraints, namely Eiter et al. (2003) Giunchiglia et al. (2004), based generic
deduction, like satisfiability testing answer set programming. hence lack planningspecific heuristic search techniques key scalability modern planning tools
developed since mid-nineties. even investigated yet integrity constraints could handled latter tools. existing approach ventures direction implements so-called derived predicates modern planning tools (Thiebaux,
Hoffmann, & Nebel, 2005; Gerevini, Saetti, Serina, & Toninelli, 2005; Chen et al., 2006).
approach postulates strict distinction basic predicates may affected actions,
derived predicates may affected integrity constraints taking form logic programming rules. predicate appears action effect, allowed appear
head rule. desirable restriction context WSC, web services
bound affect properties constrained ontology axioms.
existing work connecting WSC belief update (Lutz & Sattler, 2002; Baader et al.,
2005; Liu et al., 2006b, 2006a; de Giacomo et al., 2006) theoretical nature. actual implemented WSC tools make severe simplifying assumptions. often, assumption ignore
ontology axioms (Ponnekanti & Fox, 2002; Srivastava, 2002; McIlraith & Son, 2002; Sheshagiri
et al., 2003; Sirin, Parsia, Wu, Hendler, & Nau, 2004; Pistore et al., 2005b, 2005a). Sometimes,
ontology constraints restricted subsumption hierarchies, makes update problem
easy (Constantinescu & Faltings, 2003; Constantinescu, Faltings, & Binder, 2004b, 2004a). Sirin
Parsia (2004) Sirin, Parsia, Hendler (2006) discuss problem dealing ontology axioms WSC, make connection belief update, describe alternative
solution. Finally, authors, example Meyer Weske (2006), deal ontology ax58

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

ioms composition, provide formal semantics specify exactly
action applications handled. seems fully formalized WSC approaches implicitly
assume message-based framework. frameworks closely related forward effects
special case identified herein.
2.3 Message-Based WSC
message-based approaches WSC, composition semantics based chaining input
output messages web services. word message standard term context.
authors use individual vocabulary. far aware, first appearance
word message WSC paper title work Liu, Ranganathan, Riabov (2007).
work describes message-based WSC follows. solution directed acyclic graph (DAG)
web services, input needed web service (DAG graph node) w must provided
outputs predecessors w graph. is, plan determines fixed connections
actions. Reasoning, then, takes place within connections. two connections
different output input messages, i.e., two graph edges ending different node,
assumed mutually independent. Consider following example illustration. Say web
service w effect hasAttributeA(c, d) output constant c input (i.e., c
existed already prior application w). Say axiom x, : hasAttributeA(x, y)
conceptB(x) expressing attribute domain restriction. x value attribute A,
x must concept B. Given this, ws effect implies conceptB(c). Now, suppose
belief prior applying w constrain c concept B. applying w leads new
knowledge c. Hence need non-trivial belief update taking account changed
status c, implications may have. Message-based WSC simply acts latter
case. checks whether w correctly supplies inputs web services w w
connected to. is, new fact hasAttributeA(c, d) may taken part proof
effect w implies precondition connected web service w . considered
implications hasAttributeA(c, d) may respect previous state affairs.
sense, message-based WSC ignores need belief update.
intuitions underlying message-based WSC fairly wide-spread. Many papers use
less direct way. many approaches explicitly define WSC solutions
DAGs local input/output connections (Zhan, Arpinar, & Aleman-Meza, 2003; Lecue
& Leger, 2006; Lecue & Delteil, 2007; Kona, Bansal, Gupta, & Hite, 2007; Liu et al., 2007; Ambite
& Kapoor, 2007). various works (Constantinescu & Faltings, 2003; Constantinescu et al.,
2004b, 2004a; Meyer & Weske, 2006), message-based assumptions implicit.
manifest mainly sense ontology axioms used infer properties
output messages, often checking whether inferences imply desired input
message definitely given.
Previous work message-based WSC address message-based WSC relates
various notions, like belief update, considered literature. One contribution work
shed light issue, via identification forward effects case lies
message-based WSC full planning framework belief update semantics.
message-based WSC forward effects case share focus outputs. Indeed,
output constants generated actions viewed messages. output constant
represents information object created one web service, form

59

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

input web service. forward effects case, due restriction axioms,
individual messages interact. much like message-based WSC. main difference
this: message-based WSC ignores possible interactions, forward effects actually
arent interactions, according formal planning-based execution semantics. sense,
forward effects correspond special case WSC assumptions message-based WSC
justified.
Reconsider example above, featuring web service w effect implying
conceptB(c) c pre-existing constant. explained above, message-based WSC
simply ignore need updating knowledge c. contrast, forward effects case
disallows axiom x, : hasAttributeA(x, y) conceptB(x) may lead new
conclusions old belief (note literals axiom refer different sets variables).
forward effects case also differs significantly approaches message-based WSC
terms flexibility allows combine actions plans. messagebased approach using DAGs, solution DAG ensures inputs service w always
provided ws predecessors. is, plug-in match set W ws
predecessors DAG, w itself. Note slightly general usual notion
plug-in matches, |W | may greater 1, hence single service W may
partial match w. notion used, amongst others, Liu et al. (2007).
authors, example Lecue Leger (2006) Lecue Delteil (2007), restrictive
consider every individual input x w turn require exists w W
w plug-in match x (i.e., w guarantees always provide x). Even generous
two definitions, partial matches restricted appear locally, DAG links. Every
action/web service required always executable point applied.
words, services used fixed manner, considering dynamics actual execution.
Example 1, would mean using information services regardless class
protein, hence completely ignoring relevant not.
forward effects case incorporates much general notion partial matches. happens straightforward way, exploiting existing notions planning, form conditional effects semantics. standard notion conformant solution defines partial matches
must work together global level, accomplish goal. best knowledge,
one line work WSC, Constantinescu et al. (Constantinescu & Faltings, 2003;
Constantinescu et al., 2004b, 2004a), incorporates comparable notion partial matches.
work, web services characterized terms input output types. handle partial
matches, so-called switches combine several web services way ascertains relevant
cases covered. switches designed relative subsumption hierarchy types.
Note subsumption hierarchies special case much general integrity constraints
universally quantified clauses consider work.

3. Formalizing WSC
solid basis addressing WSC, define planning formalism featuring integrity constraints,
on-the-fly creation output constants, incomplete initial state descriptions, actions conditional effects semantics. application actions defined belief update operation, following possible models approach Winslett (1988). definition belief update somewhat
canonical widely used discussed. particular underlies recent work

60

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

relating formalizations WSC (Lutz & Sattler, 2002; Baader et al., 2005; Liu et al., 2006b,
2006a; de Giacomo et al., 2006; de Giacomo, Lenzerini, Poggi, & Rosati, 2007). show
(Section 4.3), belief update operations equivalent anyway soon
forward effects case. Recall forward effects case central object
investigation paper.
first give syntax formalism, denote WSC, give semantics. conclude analysis main computational properties.
3.1 Syntax
denote predicates G, H, I, variables x, y, z, constants c, d, e. Literals possibly negated predicates whose arguments variables constants. arguments constants,
literal ground. refer positive ground literals propositions. Given set P predicates
set C constants, denote P C set propositions formed P
C. Given set X variables, denote LX set literals l use variables
X. Note l may use arbitrary predicates constants.7 l literal, write
l[X] indicate l variable arguments X. X = {x1 , . . . , xk } C = (c1 , . . . , ck ),
l[c1 , . . . , ck /x1 , . . . , xk ] denote respective substitution, abbreviated l[C].
way, use substitution notation construct involving variables. Slightly abusing
notation, use vector constants also denote set constants appearing it. Further,
function assigns constants variables X, l[a/X] denote substitution
argument x X replaced a(x). concerned first-order logic, is,
whenever write formula mean first-order formula. denote true 1 false 0.
clause, integrity constraint, disjunction literals universal quantification
outside. variables quantified exactly appear least one literals.
example, x, : G(x, y) H(x) integrity constraint x, y, z : G(x, y) H(x) x :
G(x, y)H(x) not. operator tuple (Xo , preo , Yo , effo ), Xo , Yo sets variables, preo conjunction literals LXo , effo conjunction literals LXo Yo .8
intended meaning Xo inputs Yo outputs, i.e., new constants created
operator. operator o, action given (prea , effa ) (preo , effo )[Ca /Xo , Ea /Yo ]
Ca Ea vectors constants. Ea require constants pairwise different makes sense output new constant twice. Given action a, refer
inputs outputs Ca Ea , respectively. also use notations prea , effa
obvious meaning.
WSC task, planning task, tuple (P, IC , O, C0 , 0 , G ). Here, P set predicates.
IC set integrity constraints. set operators C0 set constants, initial
constants supply. 0 conjunction ground literals, describing possible initial states. G
conjunction literals existential quantification outside, describing goal states,
e.g., x, : G(x) H(y). predicates taken P, constants taken C0 .
constructs (e.g., sets conjunctions) finite. sometimes identify IC
conjunction clauses contains. Note existential quantification goal variables
7. One could course introduce general notations logical constructs using set predicates constants.
However, herein two notations given suffice.
8. stated, address disjunctive non-deterministic effects. topic future work.

61

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

provides option instantiate goal constants created planning obtaining objects
requested goal may possible use outputs.
various formulas occurring (P, IC , O, C0 , 0 , G ) may make use constants C0 .
Specifically, case clauses IC goal formula G . Allowing use
constants effect complexity algorithmic results. conceivable
feature may useful. simple example, VTA domain user may wish select
particular train. Say train company provides table trains itineraries. table
represented 0 , possibly help IC stating constraints hold particular trains.
user select train, say ICE107, pose goal : ticketFor (y, ICE107).
Constraining produced ticket way would possible without use pre-existing
constants (or would least require rather dirty hack, e.g., encoding desired train terms
special predicate).
Operator descriptions, is, preconditions effects, may also use constants C0 .
value benign IC G one always replace constant c
precondition/effect new input/output variable x, instantiate x (during planning)
c. Note, however, would give planner option (uselessly) instantiate x
constant, may hence affect planning performance. example, might
special operator booking ticket ICE107 (e.g., train particular ticketing regulations).
correspondence WSC task web service composition task fairly obvious.
set P predicates formal vocabulary used underlying ontology. set IC
integrity constraints set axioms specified ontology, i.e., domain constraints
subsumption relations. set operators set web services. Note formalization
corresponds closely notion IOPE descriptions: inputs, outputs, preconditions,
effects (Ankolekar et al., 2002; Burstein et al., 2004). action corresponds web service call,
web services parameters instantiated call arguments.
constructs C0 , 0 , G extracted user requirement composition.
assume requirements also take form IOPE descriptions. Then, C0
user requirement inputs, 0 user requirement precondition. words, C0 0
describe input given composition user. Similarly, G user requirement effect
condition user wants accomplished user requirement outputs
(existentially quantified) variables G .
3.2 Semantics
follows, assume given WSC task (P, IC , O, C0 , 0 , G ). able model
creation constants, states (also called world states) formalism enriched set
constants exist them. state pair (Cs , ) Cs set constants,
Cs -interpretation, i.e., truth value assignment : P Cs 7 {0, 1}. Quantifiers taken range
constants exist state. is, C-interpretation formula,
writing |= mean |= C C except quantifiers
restricted range C. avoid clumsy notation, sometimes write |= abbreviate
|= .
core definition specifies application action affects state. defined
form belief update. Let us first define latter. Assume state s, set constants
C Cs , formula . define update(s, C , ) set interpretations result

62

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

creating constants C \ Cs , updating according semantics proposed
Winslett (1988).
Say I1 I2 C -interpretations. define partial order interpretations,
setting I1 <s I2
{p P Cs | I1 (p) 6= (p)} {p P Cs | I2 (p) 6= (p)}.

(1)

words, I1 ordered I2 iff differs proper subset values. Given this,
formally define update(s, C , ). Let arbitrary C -interpretation. define
update(s, C , ) : |= {I | |= , <s I} = .

(2)

Hence, update(s, C , ) defined set C -interpretations satisfy ,
minimal respect partial order <s . Put different terms, update(s, C , ) contains
interpretations differ set-inclusion minimal set values.
Now, assume action a. say applicable s, short appl(s, a), |= prea ,
Ca Cs , Ea Cs = . is, top usual precondition satisfaction require
inputs exist outputs yet exist. result executing is:

{(C , ) | C = Cs Ea , update(s, C , IC effa )} appl(s, a)
(3)
res(s, a) :=
{s}
otherwise
Note executed even applicable. case, outcome singleton
set containing itself, i.e., action affect state. important aspect
formalism, get back below. IC effa unsatisfiable, obviously get
res(s, a) = . say case inconsistent.9
overall semantics WSC tasks easily defined via standard notion beliefs.
model uncertainty true state world. belief b set world states
possible given point time. initial belief
b0 := {s | Cs = C0 , |= IC 0 }.

(4)

action inconsistent belief b inconsistent least one b. latter
case, res(b, a) undefined. Otherwise, defined
[
res(s, a).
(5)
res(b, a) :=
sb

extended action sequences obvious way. plan sequence ha1 , . . . ,
res(b0 , ha1 , . . . , i) : |= G .

(6)

illustration, consider formalization example Section 2.
Example 2 Reconsider Example 1. sake conciseness, formalize part
example, simplified axioms. WSC task defined follows:
9. Unless IC mentions constants, based operator inconsistent, action based
inconsistent. operators can, principle, filtered pre-process planning.

63

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

P = {protein, cellProtein, G, H, I, 1n55, 1kw3, InfoDSSP, Info3D, combinedPresentation},
predicates unary.
IC consists clauses:
x : cellProtein(x) protein(x) [subsumption]
x : protein(x) G(x) H(x) I(x) [at least one DSSP value]
x : protein(x) 1n55(x) 1kw3(x) [at least one 3-D shape]
x : cellProtein(x) G(x) 1n55(x) [dependency]
x : cellProtein(x) H(x) 1n55(x) [dependency]
consists operators:
getInfoDSSPG : ({x}, G(x), {y}, InfoDSSP(y))
getInfoDSSPH : ({x}, H(x), {y}, InfoDSSP(y))
getInfoDSSPI : ({x}, I(x), {y}, InfoDSSP(y))
getInfo3D1n55 : ({x}, 1n55(x), {y}, Info3D(y))
getInfo3D1kw3 : ({x}, 1kw3(x), {y}, Info3D(y))
combineInfo: ({x1 , x2 }, InfoDSSP(x1 ) Info3D(x2 ), {y}, combinedPresentation(y))
C0 = {c}, 0 = cellProtein(c)
G = x : combinedPresentation(x)
illustrate formalism, consider plan example task.
initial belief b0 consists states Cs = {c} |= IC cellProtein(c). Say
apply following sequence actions:
1. Apply getInfoDSSPG (c, d) b0 . get belief b1 b0 except
that, b0 |= G(c), new states generated constant
InfoDSSP(d).
2. Apply getInfoDSSPH (c, d) b1 . get belief b2 new states
InfoDSSP(d) generated b1 |= H(c).
3. Apply getInfo3D1n55 (c, e) b2 , yielding b3 .
4. Apply getInfo3D1kw3 (c, e) b3 . yields b4 , get e Info3D(e) b2
|= 1n55(c) |= 1kw3(c).
5. Apply combineInfo(d, e, f ) b4 . brings us b5 like b4 except
b4 d, e Cs new states generated f combinedPresentation(f ).
dependencies IC (the last two clauses), get b0 satisfies either G(c)
H(c). subsumption clause clause regarding 3-D shapes (first third clauses)
get b0 satisfies either 1n55(c) 1kw3(c). Hence, easy verify, b5 |=
G hgetInfoDSSPG (c, d), getInfoDSSPH (c, d), getInfo3D1n55 (c, e), getInfo3D1kw3 (c, e),
combineInfo(d, e, f )i plan.
64

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

Note plan make use getInfoDSSPI (c, d). obtain plan, domain
one always apply information services. However, plan trivial take
account relevant not. Reasoning IC enables us find better plans.
semantics executing non-applicable actions vital workings Example 2.
pointed above, definition res(s, a) (Equation (3)), executed even
applicable. realizes partial matches: web service called soon might match
one possible situations. planning terms, actions conditional effects semantics.10
contrasting notion would enforce preconditions, i.e., say res(s, a) undefined
applicable s. would correspond plug-in matches.
Example 2, partial match semantics necessary order able apply actions
cover particular cases. example, consider action getInfoDSSPG (c, d), applied
initial belief example plan. precondition action G(c). However,
states initial belief satisfy precondition. initial belief allows
interpretation satisfying IC 0 (cf. Equation (4)), interpretations satisfy H(c)
rather G(c). Due partial match semantics, getInfoDSSPG (c, d) affect
states match initial belief partial.
Clarification also order regarding understanding constants. First, like every PDDLlike planning formalism (we aware of), make unique name assumption, i.e., different
constants refer different objects. Second, understanding web services output
create separate individual, i.e., separate information object.
latter directly raises question allow actions share output constants.
answer allow planner treat two objects same. makes
sense two objects play role plan. Consider Example 2. actions
getInfoDSSPG (c, d) getInfoDSSPH (c, d) share output constant, d. means
one name two separate information objects. two objects properties,
derived InfoDSSP(d). difference created different
cases, namely states satisfy G(c) H(c) respectively. single name
two objects useful take name parameter actions need
distinguish different cases. example, combineInfo(d, e, f ) action.
hinted, cases correspond different classes concrete execution traces.
Importantly, particular execution trace, output constant created once. see
this, consider execution trace s0 , a0 , s1 , a1 , . . . , ak , sk+1 , i.e., alternating sequence states
actions s0 b0 , si+1 res(si , ai ) 0 k. Say ai aj share
output constant, d. Say ai applicable si , hence Csi+1 . Then, quite
obviously, Csl + 1 l k + 1. particular, aj applicable sj :
intersection output constants Csj non-empty (cf. definition appl(s, a)). So, due
definition action applicability, never happen constant created twice.
words, never reachable state single constant name refers
one individual information object. sense, use one name several objects occurs
planning time, actual execution trace actual case occur
known. illustration, consider getInfoDSSPG (c, d) getInfoDSSPH (c, d), shared
10. obvious generalization allow several conditional effects per action, style ADL language (Pednault, 1989). omit sake simplifying discussion. extension direction straightforward.

65

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

output d, Example 2. Even concrete state s0 b0 execution starts satisfies
G(c) H(c), one actions fire namely one comes first.
remark initially experimented definition actions instantiate
inputs, applied state outputs are, virtue execution semantics,
instantiated constants outside Cs . framework, one never choose share output
constants, i.e., use name two different outputs. notion settled
strictly richer: planner always choose instantiate outputs constants outside Cs .
question is, make sense share outputs? Answering question domainindependent planner may turn quite non-trivial. get back discuss
possible adaptation CFF Section 4.5. experiments reported herein (Section 6), use
simple heuristic. Outputs shared iff operator effects identical (giving indication
respective outputs may indeed play role plan).
conclude sub-section final interesting observation regarding modeling
framework. Negative effects essential part WSC formalism: compiled
away. simply replace negative effect G(x1 , . . . , xk ) notG(x1 , . . . , xk ) (introducing
new predicate) state integrity constraints two equivalent. is, introduce two new clauses x1 , . . . , xk : G(x1 , . . . , xk ) notG(x1 , . . . , xk ) x1 , . . . , xk :
G(x1 , . . . , xk ) notG(x1 , . . . , xk ). simple compilation technique, formal
details little intricate, moved Appendix A. action original task,
a+ denotes corresponding action compiled task, vice versa. Similarly, action
original task, s+ denotes corresponding state compiled task. get:
Proposition 1 (Compilation Negative Effects WSC) Assume WSC task (P, IC , O, C0 ,
+
0 , G ). Let (P + , +
IC , , C0 , 0 , G ) task negative effects compiled away.
Assume action sequence ha1 , . . . , i. Let b result executing ha1 , . . . , (P, IC ,
+
+
+
+
O, C0 , 0 , G ), let b+ result executing ha+
1 , . . . , (P , IC , , C0 , 0 , G ).
Then, state s, b iff s+ b+ .
proved straightforward application relevant definitions. important aspect result new clauses introduced allowed forward effects
strictly forward effects special cases identified later. Hence, hardness results transfer directly
tasks without negative effects dropping negative effects cannot make algorithms easier.
3.3 Computational Properties
perform brief complexity analysis WSC formalism general form
introduced above. line many related works kind (Eiter & Gottlob, 1992; Bylander,
1994; Liberatore, 2000; Eiter et al., 2004), consider propositional case. context,
means assume fixed upper bound arity predicates, number input/output
parameters operator, number variables appearing goal, number
variables clause. refer WSC tasks restricted way WSC tasks fixed
arity.
consider problems checking plans testing whether given action sequence
plan deciding plan existence. latter, distinguish polynomially bounded
plan existence, unbounded plan existence. deem particularly relevant decision
problems context plan generation. Certainly, plan checks integral part plan gen66

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

eration. Indeed, planning tool based state space search, tool either performs
checks explicitly (potentially many) plan candidates generated search, complexity
inherent effort underlies computation state transitions. Polynomially bounded
plan existence relevant because, commonly used planning benchmark domains, plans
polynomial length (it also wide-spread intuition SWS community composed
web services contain exceedingly large numbers web services). Finally, unbounded plan
existence general decision problem involved, thus generic interest.
problems turn hard. prove this, reuse adapt various results
literature. start complexity plan checking, hardness follows
long established result (Eiter & Gottlob, 1992) regarding complexity belief update.
results, detailed proofs available Appendix A.
Theorem 1 (Plan Checking WSC) Assume WSC task fixed arity, sequence
ha1 , . . . , actions. p2 -complete decide whether ha1 , . . . , plan.
Proof Sketch: Membership shown guess-and-check argument. Guess proposition
values along ha1 , . . . , i. check whether values comply res, lead
inconsistent action, final state satisfy goal. ha1 , . . . , plan iff
case guess proposition values. Checking goal satisfaction polynomial, checking
compliance res coNP, checking consistency NP.
Hardness follows simple adaptation proof Lemma 6.2 Eiter Gottlob
(1992). proof uses reduction checking validity QBF formula X.Y.[X, ].
lemma considers case propositional belief updated arbitrary (propositional)
formula , decision problem ask whether formula implied
updated belief. proof, complete conjunction literals, i.e., corresponds single
world state. single propositional fact r true . semantics X.Y.[X, ]
encoded complicated construction defining update . nutshell, CNF telling
us every assignment X (which yield world state updated belief), either
find assignment [X, ] holds (completing ), falsify r.
difference setting lies restricted update formulas action effects
fact integrity constraints supposed hold every belief. adapt
proof by, first, taking integrity constraints clauses Eiter Gottlobs CNF formula
. modify constraints need true new fact holds i.e., insert
every clause. initial belief false, otherwise corresponds exactly above.
action plan makes true. goal Eiter Gottlobs fact r.
2
remark membership Theorem 1 remains valid allowing actions multiple
conditional effects, allowing parallel actions, even allowing combination.
hand, virtue proof argument outlined, hardness holds even initial state
literals 0 complete (describe single world state), plan consists single action
single positive effect literal, goal single propositional fact initially true.
next consider polynomially bounded plan existence. this, membership follows directly
Theorem 1. prove hardness, construct planning task extends Eiter Gottlobs
construction actions allow choose valuation third, existentially quantified, set variables, hence reduces validity checking QBF formula X.Y.Z.[X, Y, Z].

67

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

Theorem 2 (Polynomially Bounded Plan Existence WSC) Assume WSC task fixed arity, natural number b unary representation. p3 -complete decide whether exists
plan length b.
Proof: membership, guess sequence b actions. Theorem 1, check
p2 oracle whether sequence plan.
hardness, validity QBF formula X.Y.Z.[X, Y, Z], CNF, reduced
testing plan existence. Say X = {x1 , . . . , xn }. planning task, n actions (operators
empty input/output parameters) oxi oxi former sets xi true latter
sets xi false. Further, action ot corresponds action used hardness
proof Theorem 1. actions equipped preconditions effects ensuring
plan must first apply, 1 n, either oxi oxi , thereafter must apply ot (of course
enforcing latter also requires new goal fact achieved ot ). Hence, choosing
plan candidate task choosing value assignment aX variables X.
construction, oxi oxi actions executed, one ends belief
contains single world state, value assignment aX variables X corresponds
chosen actions. world state basically corresponds belief hardness proof
Theorem 1. difference construction extended cater third
set variables. straightforward. Then, belief results executing ot satisfies
goal iff Eiter Gottlobs fact r holds world states. virtue similar arguments
Eiter Gottlob, latter case iff Y.Z.[aX /X, Y, Z], i.e., substitution
X.Y.Z.[X, Y, Z] aX , valid. this, claim follows.
2
final result regards unbounded plan existence WSC. result relatively easy
obtain generic reduction described Bylander (1994) prove PSPACE-hardness plan
existence STRIPS. Somewhat shockingly, turns plan existence WSC undecidable
even without integrity constraints, complete initial state description. source
undecidability is, course, ability generate new constants on-the-fly.
Theorem 3 (Unbounded Plan Existence WSC) Assume WSC task. decision problem
asking whether plan exists undecidable.
Proof Sketch: modification proof Bylander (1994) plan existence propositional STRIPS planning PSPACE-hard. original proof proceeds generic reduction,
constructing STRIPS task Turing Machine polynomially bounded space. latter restriction necessary model machines tape: tape cells pre-created positions within
bound. Exploiting ability create constants on-the-fly, instead introduce simple
operators allow extend tape, ends.
2
able decide plan existence is, course, significant limitation principle. However, limitation probably marginal importance practice, planning tools
assume plan, try find rather trying prove
plan. sense, planning tools are, nature, semi-decision procedures anyway.
matters decidability setting question whether one find plan

68

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

quickly enough, i.e., exhausting time memory.11 also relevant question
web service composition.

4. Forward Effects
high complexity planning WSC motivates search interesting special cases.
define special case, called forward effects, every change action makes state involves
newly generated constant.
start section defining forward effects case making core observation
semantics. discuss modeling power special case. Next, discuss forward effects general perspective belief update. analyze main computational
properties forward effects, conclude section assessment existing
planning tool could adapted handle forward effects.
4.1 WSC|f wd Semantics
forward effects special case WSC defined follows.
Definition 1 Assume WSC task (P, IC , O, C0 , 0 , G ). task forward effects iff:
1. O, l[X] effo , X Yo 6= .
2. clauses IC , = x1 , . . . , xk : l1 [X1 ] ln [Xn ], X1 =
= Xn .
set WSC tasks forward effects denoted WSC|f wd .
first condition says variables every effect literal contain least one output variable. implies every ground effect literal action contains least one new constant.
second condition says that, within every integrity constraint, literals share arguments.
implies effects involving new constants affect literals involving new constants.
Note that, since x1 , . . . , xk definition exactly variables occurring literals,
Xi Xi = x1 , . . . , xk . Note may k = 0, i.e., literals
clause may ground. intentional. constants mentioned clause must
taken C0 , cf. discussion Section 3.1. Therefore, clauses interaction
statements new constants generated WSC|f wd action.
discuss modeling power WSC|f wd (Section 4.2). First, observe
semantics WSC|f wd much simpler general WSC. One longer needs
notion minimal change respect previous state. state precisely, assume

WSC task predicates P. Say interpretation P C , C set constants.
Say C C . denote |C restriction P C , i.e., interpretation P C
coincides propositions. Given state action a, define:

{(C , ) | C = Cs Ea , |Cs = , |= IC effa } appl(s, a)
(7)
res|f wd (s, a) :=
{s}
otherwise
11. Indeed planning community generally rather unconcerned undecidability, cf. numeric track international planning competitions, Helmerts (2002) results decidability numerical planning problems.

69

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

Compare Equation (3), defined member update(s, C , IC effa ),
returns interpretations satisfy IC effa differ minimally . Equation (7), simply set identical , constants (on propositions constants)
existed beforehand. words, set new states get cross-product old
state satisfying assignments IC effa .
Lemma 1 (Semantics WSC|f wd ) Assume WSC|f wd task, reachable state s, action
a. res(s, a) = res|f wd (s, a).
Proof Sketch: WSC|f wd , differs minimally s, follows agrees totally
s, set propositions P Cs interpreted s. see this, denote P Cs +Ea
set propositions arguments Cs Ea , least one argument Ea , denote
IC [Cs + Ea ] instantiation IC constants Cs Ea , clause
least one variable instantiated Ea . key argument |= IC effa equivalent
|= IC [Cs Ea ] effa , turn equivalent |= IC [Cs ] IC [Cs + Ea ] effa .
last formula, IC [Cs ] uses propositions P Cs , whereas IC [Cs + Ea ] effa
uses propositions P Cs +Ea . Since reachable, |= IC [Cs ]. Therefore, satisfy
IC effa , need change values assigned s.
2
4.2 Modeling Power
Intuitively, WSC|f wd covers situation web service outputs new constants, sets
characteristic properties relative inputs, relies ontology axioms describe
ramifications concerning new constants. detailed Section 2, closely corresponds
various notions message-based WSC explored literature. sense, modeling
power WSC|f wd comparable message-based WSC, one most-widespread
approaches area.
simple concrete way assessing modeling power WSC|f wd consider allowed
disallowed axioms. Examples axioms allowed WSC|f wd are: attribute domain
restrictions, taking form x, : G(x, y) H(x); attribute range restrictions, taking form
x, : G(x, y) H(y); relation transitivity, taking form x, y, z : G(x, y) G(y, z)
G(x, z). Note that, axioms, easy construct case action effect, even
though involves new constant, affects old belief. example, constants c e existed
beforehand, action outputs sets G(c, d) G(d, e), axiom x, : G(x, y)
G(y, z) G(x, z) infers G(c, e) statement involve new constant d.
Typical ontology axioms allowed WSC|f wd are: subsumption relations, taking
form x : G(x) H(y); mutual exclusion, taking form x : G(x) H(y); relation reflexivity, taking form x : G(x, x); relation symmetry, taking form x, :
G(x, y) G(y, x). also express concept G contained union concepts
H1 , . . . , Hn , generally express complex dependencies concepts, taking form clausal constraints allowed combinations concept memberships.
One example complex dependencies important domain proteins illustrated
Example 1. Capturing dependencies important order able select correct web services. Similar situations arise many domains involve complex interdependencies
and/or complex regulations. example latter Virtual Travel Agency discussed before. example, German rail system kinds regulations regarding
70

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

train may booked kind discount conditions. Modeling
regulations would enable WSC algorithm select appropriate booking services. Another interesting case hospital domain described de Jonge, van der Linden, Willems (2007).
There, problem hospital asset tracking handled means set tracking, logging
filter services, transform logs extract various kinds information. setting, would
make sense model complex dependencies web service composer may determine
hospital assets need tracked retrieved. Namely, latter depends type operation
question, kind examinations operation requires. Accordingly,
need model categorization operations, mapping sets required examinations,
examinations associated hospital assets. complications arise since
required examinations/assets may depend particular circumstances. Clearly, express
categorization dependencies terms clauses. course captures fraction
relevant hospital, considerably informed composer always
tracks assets.
main weakness WSC|f wd allow us express changes regarding preexisting objects. best illustrated considering case negative effects.12
planning community, commonly used model previous properties objects
invalidated action. illustration, reconsider Example 1. Say additional operator
dropCoffeeIn3Dmachine, effect Info3D(y). One would normally expect that,
operator applied, fact Info3D(y) deleted must re-established.
WSC|f wd . According restrictions special case imposes, variable Info3D(y)
must output dropCoffeeIn3Dmachine. is, dropping coffee machine creates
new object, whose characteristic property happens Info3D(y) rather Info3D(y). Clearly,
intended semantics operator.
model intended semantics, would need instantiate pre-existing constant.
Say that, belief b3 Example 1, constant e Info3D(e) previously created
getInfo3D1n55 (c, e). WSC|f wd allow us instantiate dropCoffeeIn3Dmachine
e, effect Info3D(e). However, virtue definition action applicability,
action applicable states e yet exist corresponding execution
paths getInfo3D1n55 (c, e) executed. Hence property Info3D(e) get
deleted state, e used dropCoffeeIn3Dmachine still regarded newly
created object whose characteristic property Info3D(y). difference new action
makes that, now, plan uses name (e) refer two different information objects
(output getInfo3D1n55 (c, e) vs. output dropCoffeeIn3Dmachine) play
role plan, cf. discussion Section 3.2.
interesting workaround let operators output time steps, spirit reminiscent
situation calculus (McCarthy & Hayes, 1969; Reiter, 1991). Every operator obtains extra
output variable t, included every effect literal. new time step stated stand
relation previous time steps, e.g., next(tprev, t) tprev input variable
instantiated previous time step. setting, state world changes
time. particular state object property different tprev.
example, action moves file f RAEDME README could state
name(f, RAEDME, tprev) name(f, README, t). problem construction
12. Or, WSC, positive effects triggering negative effects via IC , cf. Proposition 1.

71

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

time steps special interpretation, ordinary objects.13 causes
least two difficulties. (1) want refer object property, know time step
first place is, know whether actual time step tprev. Note
cannot maintain predicate actualTime(x) would require us invalidate
property tprev. (2) solution frame problem. operators must explicitly state
every relevant property previous time step, property changed new time
step.14
conclude sub-section, let us consider WSC|f wd generalized without losing
Lemma 1. importantly, instead requiring every effect literal involves new constant,
one postulate literals may actually affected integrity constraints.
particular, predicate appear clauses, certainly effect literal
predicate harmful even involve output constant. One obtains potentially stronger notion considering ground literals, rather predicates. Note kind
generalization solves difficulty (1) time-step construction, presuming time steps
constrained clauses. (The frame problem, however, persists.)
Another possibility, deviating somewhat way WSC WSC|f wd currently defined, define integrity constraints terms logic programming style rules, along lines
Eiter et al. (2003, 2004). requirement WSC|f wd relaxed postulate
effect literals without new constants appear rule heads.
remark latter observation suggests certain strategic similarity aforementioned derived predicates (Thiebaux et al., 2005) previously used AI Planning manage
complexity integrity constraints. There, integrity constraints take form stratified logic
programming style derivation rules, predicates appearing rule heads allowed
appear operator effects. overly restricted solution, WSC context. effects
web services indeed likely affect concepts relations appearing ontology
axioms. may WSC|f wd , long output constants involved.
4.3 Belief Update
Lemma 1 specific possible models approach (Winslett, 1988) underlies semantics
action applications. interesting consider semantics WSC|f wd general
perspective belief update. Recall update involves formula characterizing current
belief, formula describing update. seek formula characterizes updated belief.
wide variety definitions proposed updated belief defined.
However, common ground exists. Katzuno Mendelzon (1991) suggest eight postulates,
named (U1) . . . (U8), every sensible belief update operation satisfy. Herzig Rifi
(1999) discuss detail degree postulates satisfied wide range alternative
belief update operators. particular call postulate uncontroversial update operators
investigation satisfy them. take results following. examine
extent draw conclusions updated belief, , setting forward effects
case, relying Herzig Rifis uncontroversial postulates.
13. Note similarity situation calculus ends. Whereas time steps assigned specific role
formulas used situation calculus, ordinary objects handled actions, packages
blocks.
14. Despite difficulties, Theorem 6 shows time step construction used simulate Abacus
machine, hence prove undecidability plan existence WSC|f wd .

72

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

assume planning task predicates P given. need following notations:
formulas, denotes formula results updating belief
update , semantics belief update operator .
Given disjoint sets constants C E, P C+E denotes set propositions formed
predicates P, arguments contained C E exists least one
argument contained E. (Recall P C denotes set propositions formed
predicates P arguments C.)
Given set constants C, IC [C] denotes instantiation IC C. is, IC [C]
conjunction clauses result replacing variables clause IC ,
= x1 , . . . , xk : l1 [X1 ] ln [Xn ], tuple (c1 , . . . , ck ) constants C.
Given disjoint sets constants C E, IC [C + E] conjunction clauses
result replacing variables clause IC , = x1 , . . . , xk : l1 [X1 ]
ln [Xn ], tuple (c1 , . . . , ck ) constants C E, least one constant taken
E.15
ground formula P () denote set propositions occurring .
denote current belief update . another convention, given set
constants C, writing C indicate P () P C . Similarly, given disjoint sets constants
C E, writing C+E indicate P () P C+E . state, denote
conjunction literals satisfied s.
first consider case where, similar claim Lemma 1, corresponds single
concrete world state s. want apply action a. wish characterize set states
res(s, a), i.e., wish construct formula . simplicity notation, denote C := Cs
E := Ea . applicable s, nothing do. Otherwise, that:
(I) IC [C] C P ( C ) P C .
example, set C := . Since |= IC , get desired equivalence. Further,
that:
(IIa) IC [C] IC [C + E] effa ;
(IIb) P (IC [C + E]) P C+E P (effa ) P C+E .
(IIa) holds trivially: defined IC effa , equivalent IC [C E] effa
equivalent IC [C] IC [C + E] effa . (IIb), consequence forward effects
case. Every effect literal contains least one output constant, hence effa contains propositions
P C+E . IC [C + E], least one variable clause instantiated
constant e E. Since, definition, literals clause share variables, e appears
every literal therefore IC [C + E] contains propositions P C+E .
illustration, consider simple VTA example. four predicates, train(x),
ticket(x), trainTicket(x), ticketFor (x, y). set integrity constraints IC consists
15. clause IC contains variable, IC [C + E] empty. customary, empty conjunction
taken true, i.e., 1.

73

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

single axiom x : trainTicket(x) ticket(x). current state s, Cs = {c},
sets propositions 0 except train(c). consider application action
= bookTicket(c, d), whose precondition train(c), whose set E output constants {d},
whose effect effa trainTicket(d) ticketFor (d, c). setting, have: IC [C] =
(trainTicket(c) ticket(c)); C = (train(c)ticket(c)trainTicket(c)ticketFor (c, c));
IC [C + E] = (trainTicket(d) ticket(d)).
derive following that:
(III) (IC [C] C ) (IC [C + E] effa ).
is, characterize updated belief simply conjunction previous belief
action effect extended instantiation ontology axioms. corresponds
exactly Lemma 1. illustrate, continue VTA example. left hand side (III)
refers four propositions based c, sets according s. right hand side
refers propositions based trainTicket(d) ticket(d) well proposition
ticketFor (d, c) links c d.
one prerequisite derivation (III), make assumption which, best
knowledge, discussed anywhere belief update literature:
(IV) Let 1 , 1 , 2 , 2 formulas P (1 ) P (1 ) = , P (1 ) P (2 ) = , P (2 )
P (1 ) = , P (2 ) P (2 ) = . (1 1 ) (2 2 ) (1 2 ) (1 2 ).
assumption postulates formulas talking disjoint sets variables updated
separately. Since formulas disjoint variables essentially speak different aspects
world, seems reasonable assumption.
Now, start formula . make replacements according (I) (IIa), leading
equivalent formula (IC [C] C ) (IC [C] IC [C + E] effa ). map
formula onto (IV) taking 1 IC [C] C , 1 1, 2 IC [C], 2
IC [C + E] effa . Hence, separate update two parts follows:
(A) ( )C := (IC [C] C ) IC [C]
(B) ( )C+E := 1 (IC [C + E] effa )
According (IV), obtain desired formula ( )C ( )C+E .
Illustrating VTA example, simply separate parts update talk
c talk combination constants. (A) part
update trainTicket(c) ticket(c) conjoined , updated trainTicket(c)
ticket(c). (B) part update 1 representing (empty) statement previous state
makes updated (trainTicket(d) ticket(d)) trainTicket(d) ticketFor (d, c).
remains examine ( )C ( )C+E . need prove that:
(C) ( )C IC [C] C ,
(D) ( )C+E IC [C + E] effa .
Essentially, means prove that: (C) updating formula something already implies
incur changes; (D) updating 1 formula yields belief equivalent formula.
see this, compare (A) (C) (B) (D).
74

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

two statements may sound quite trivial, fact far trivial prove
wide variety of, partly rather complex, belief update operations literature. build
works Katzuno Mendelzon (1991) Herzig Rifi (1999). need two
postulates made Katzuno Mendelzon (1991), namely:
(U1) 1 2 : (1 2 ) 2 .
(U2) 1 2 : 1 2 (1 2 ) 1 .
Herzig Rifi (1999) prove (U1) uncontroversial, meaning satisfied belief
update operators investigated (cf. above). also prove (U2) equivalent conjunction two weaker statements, one uncontroversial, namely:
(U2a) 1 2 : (1 2 ) (1 2 ).
statement uncontroversial. However, proved satisfied non-causal
update operators investigation, except so-called Winsletts standard semantics (Winslett,
1990). latter semantics useful context anyway. restriction makes
states res(s, a) differ propositions mentioned update
formula. case, include propositions appearing IC [C E], bound
quite lot. So, use Winsletts standard semantics, res(s, a) would likely
retain hardly information s.
Consider formula ( )C specified (A), ( )C = (IC [C] C ) IC [C].
prove (C). indeed quite simple. (IC [C] C ) IC [C],
instantiate 1 (U2) IC [C] C , 2 (U2) IC [C]. obtain
(IC [C] C ) IC IC [C] C , hence ( )C IC [C] C desired.
said above, result uncontroversial, holds non-causal update operators
(except Winsletts standard semantics) investigated Herzig Rifi (1999). terms VTA
example, (U2) allowed us conclude update trainTicket(c) ticket(c) make
change previous belief, already contains property.
Next, consider formula ()C+E specified (B), ()C+E = 1(IC [C +E]effa ).
prove (D). postulate (U1), get ( )C+E IC [C + E] effa ,
IC [C +E]effa update formula 2 . direction, exploit (U2a). instantiate
1 (U2a) 1, get 1 (IC [C + E] effa ) 1 (IC [C + E] effa ),
1 (IC [C + E] effa ) ( )C+E , equivalent IC [C + E] effa
( )C+E . proves claim. Note used postulates uncontroversial
according Herzig Rifi (1999). Reconsidering VTA example, IC [C +E]effa =
(trainTicket(d) ticket(d)) trainTicket(d) ticketFor (d, c). previous state say
anything propositions, thus represented 1. postulates allow us conclude
(for belief update operators investigated Herzig & Rifi, 1999) resulting belief
equivalent (trainTicket(d) ticket(d)) trainTicket(d) ticketFor (d, c).
far, restricted case , belief updated, corresponds single
world state s. Consider general case characterizes belief b, want
characterize set states res(b, a). first glance, seems much changes,
Katzuno Mendelzon (1991) also make following postulate:
(U8) 1 , 2 , : (1 2 ) (1 ) (2 ).

75

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

means that, consists two alternate parts, updating taking union
updated parts. words, compute update state-by-state basis.
statement (I) still true, C disjunction states
b, rather single . rest argumentation stays exactly same. Herzig
Rifi (1999) prove (U8) uncontroversial leave that.
However, matters simple. source complications use partial
matches/conditional effects semantics. update formula different individual states
b. Hence cannot directly apply (U8). Obviously, states s1 b applicable updated differently states s2 b applicable latter updated all.16
somewhat subtle distinction states b constants exist them: different
sets constants, integrity constraints update different. Hence, obtain generic
update , split equivalence classes 1 , . . . , n states within
cannot distinguished based prea based existing constants. Then, (U8)
argumentation used show equivalent (III) . last step,
defining final disjunction individual , appears sensible.
follow immediately Katzuno Mendelzon (1991).
illustration, consider variant VTA example two preceding states, one
state train(c) before, new state ticket(c) instead. ,
bookTicket(c, d) applicable, hence update different . part
above, yielding result (trainTicket(d) ticket(d)) trainTicket(d) ticketFor (d, c).
update trivial, yields result. final outcome disjunction
two beliefs.
point situation much easier consider plug-in matches (i.e., forced preconditions) instead partial matches. There, applicable states, also easy
see every state b constants. Therefore, plug-in matches, (III) follows immediately (U8). VTA example, update would computed since
bookTicket(c, d) would considered applicable preceding belief. satisfies
train(c) disagrees aspect, e.g. (quite nonsensically) also ticket(c) holds,
updated belief equivalent (s ) (trainTicket(d) ticket(d)) trainTicket(d)
ticketFor (d, c).
4.4 Computational Properties
Paralleling analysis general WSC Section 3.3, perform brief complexity
analysis WSC|f wd special case. before, consider propositional case
assumes fixed upper bound arity predicates, number input/output parameters
operator, number variables appearing goal, number variables
clause. Also before, consider decision problems checking plans, deciding
polynomially bounded plan existence, deciding unbounded plan existence, order.
contrast before, cannot reuse results literature much because, course,
particular circumstances WSC|f wd investigated before. include proof sketches
here, refer Appendix detailed proofs.
16. One might speculate common update would prea , case. example,
possible models approach adopt WSC, updating |= prea prea gives rise result
states change violate prea instead changing satisfy .

76

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

Thanks simpler semantics per Lemma 1, plan checking much easier WSC|f wd
WSC.
Theorem 4 (Plan Checking WSC|f wd ) Assume WSC|f wd task fixed arity, sequence ha1 , . . . , actions. coNP-complete decide whether ha1 , . . . , plan.
Proof Sketch: Hardness obvious, considering empty sequence. Membership shown
guess-and-check argument. Say C union C0 output constants appearing
ha1 , . . . , i. guess interpretation propositions P C. Further,
1 n, guess set Ct constants. needs time-stamped because, action
generated outputs, properties respective propositions remain fixed forever. Thanks
Lemma 1, check polynomial time whether (a) Ct correspond execution
ha1 , . . . , i. Also, check polynomial time whether (b) Cn satisfy G . ha1 , . . . ,
plan iff guess answer (a) yes answer (b) no.
2
Membership Theorem 4 remains valid allowing parallel actions multiple conditional effects provided one imposes restrictions ensuring effects/actions applied simultaneously (in one step) never self-contradictory. Otherwise, checking plans also involves
consistency test plan step, NP-complete problem. Note quite reasonable demand simultaneous actions/effects contradict other. Widely used
restrictions imposed ensure mutually exclusive effect conditions, and/or non-conflicting
sets effect literals.
next consider polynomially bounded plan existence. Membership follows directly Theorem 4. prove hardness, reduce validity checking QBF formula X.Y.[X, ].
constructed planning task allows choose values X, thereafter apply actions evaluating arbitrary values . goal accomplished iff setting X exists works
.
Theorem 5 (Polynomially Bounded Plan Existence WSC|f wd ) Assume WSC|f wd task
fixed arity, natural number b unary representation. p2 -complete decide whether
exists plan length b.
Proof Sketch: membership, guess sequence b actions. Theorem 4,
check NP oracle whether sequence plan.
Hardness proved reduction
Wk validity checking QBF formula X.Y.[X, ]
DNF normal form, i.e., = j=1 j . key idea use outputs creation
time steps, hence ensure operators adhere restrictions WSC|f wd . Setting
xi allowed time step i. is, xi operators oxi 1 oxi 0 . take
input set time steps {t0 , . . . , ti1 } required successive, precondition
start(t0 ) next(t0 , t1 ) next(ti2 , ti1 ). output new time step ti attach
successor ti1 , set xi 1 0, respectively, time step i. is,
effect literal form xi (ti ) xi (ti ), respectively. rest planning task consists of:
operators ot allow extending sequence time steps step B, suitable value B (see
below); operators oj allow achieving goal, given j true end time
step sequence length B. integrity constraints (IC empty). values yi
specified, i.e., variables take value initial belief.

77

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

X.Y.[X, ] valid obviously one construct plan task simply setting
xi accordingly, using ot stepping time B, applying oj . necessitates
complicated construction direction proof: namely, plan may cheat
setting xi 1 0. construction ensures costly, plan
forced maintain two parallel sequences time steps, starting faulty xi . choose
sufficiently large value B, together sufficiently small plan length bound b, cheating
possible.
2
final result regards unbounded plan existence. Somewhat surprisingly, turns
still undecidable WSC|f wd . Similar above, key idea let actions output
new time step, thereby ensuring membership constructed task WSC|f wd .
Theorem 6 (Unbounded Plan Existence WSC|f wd ) Assume WSC|f wd task. decision
problem asking whether plan exists undecidable.
Proof Sketch: reduction halting problem Abacus machines, undecidable.
Abacus machine consists tuple integer variables v1 , . . . , vk (ranging positive
integers including 0), tuple instructions I1 , . . . , . state given content
v1 , . . . , vk plus index pc active instruction. machine stops iff reaches state
pc = n. vi initially 0, pc initially 0. instructions either increment variable
jump another instruction, decrement variable jump different instructions
depending whether variable already 0.
difficult encode Abacus machine WSC|f wd task. two key ideas are: (1)
design operator outputs next successor integer; (2) design operators simulating
instructions, stepping successors predecessors integer values. latter kind
operators, membership WSC|f wd ensured letting operators output new time step
new variable values associated. goal asks existence time step
active instruction .
2
argued end Section 3.3 already, dont deem undecidability unbounded plan
existence critical issue practice. planning tools nature semi-decision procedures,
anyway. particular, web service composition typically expected occur real-time setting,
severe time-outs apply.
4.5 Issues Adapting CFF
view, crucial observation WSC|f wd test plans coNP,
rather p2 general WSC. Standard notions planning uncertainty
complexity plan testing, research already resulted sizable number approaches
(comparatively) scalable tools (Cimatti et al., 2004; Bryce et al., 2006; Hoffmann & Brafman,
2006; Palacios & Geffner, 2007). show next section that, certain additional
restrictions WSC|f wd , tools applied off-the-shelf. Regarding general WSC|f wd ,
match complexity plan testing suggests underlying techniques successfully
adapted. following, consider detail CFF tool (Hoffmann & Brafman, 2006).
promising options would extend MBP (Cimatti et al., 2004) POND (Bryce et al.,
2006), look compilation techniques investigated Palacios Geffner (2007).
CFF characterized follows:
78

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

(1) Search performed forward space action sequences.
(2) sequence a, CNF formula (a) generated encodes semantics a,
SAT reasoning (a) checks whether plan.
(3) reasoning results namely literals always true executing cached
speed future tests.
(4) Search guided adaptation FFs (Hoffmann & Nebel, 2001) relaxed plan heuristic.
(5) Relaxed planning makes use strengthened variant CNF formulas (a) used
reasoning action sequences, clauses projected onto 2
literals (i.e., 2 literals removed respective clause).
techniques self-explanatory, except possibly last one. Projecting CNF
formulas ensures relaxed planning remains over-approximation real planning,
projected formulas allow us draw conclusions. time, projected
formulas handled sufficiently runtime-efficiently.17 method 2-projecting
clauses is, nutshell, ignore one condition literals conditional effect
relaxed planning graph.
fairly obvious basic answers given CFF, i.e., techniques (1) (5), also apply
WSC|f wd . Note that, indeed, main enabling factor check plans coNP,
rather p2 general WSC. enables us design desired CNF formulas (a)
straightforward fashion. plan checking p2 -hard, either need replace CNF
formulas QBF formulas, create worst-case exponentially large CNF formulas.
are, least, technically quite challenging.
adaptation CFF WSC|f wd immediate promise, trivial. involves
technical challenges regarding on-the-fly creation constants well computation
heuristic function. latter also brings significant new opportunities WSC context,
pertaining exploitation typical forms ontology axioms. Let us consider issues
little detail.
First, like todays planning tools, CFF pre-instantiates PDDL purely propositional
representation, based core planning algorithms implemented. one allows on-thefly creation constants, pre-instantiation longer possible, hence adaptation
WSC|f wd involves re-implementing entire tool. challenge itself,
difficult obstacles overcome. sloppy formulation key question is: many
constants create? One can, course, create new tuple constants (the outputs of)
every new action application. However, seems likely approach would blow
representation size quickly, would hence infeasible. one instead share
output constants reasonable. one recognize reasonable points? issue
especially urgent inside heuristic function. Namely, easy see that, worst case,
relaxed planning graph grows exponentially number layers. imagine example
web service w1 takes input type generates output type B, whereas w2 takes
input type B generates output type A. Starting one constant type
one type B, get 2 constants type next graph layer. Then, w1 w2
17. Inside heuristic function, formulas come relaxed planning graphs quite big. handling
without approximations seems hopeless. discussed detail Hoffmann Brafman (2006).

79

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

applied two times, get 4 constants type next graph layer, forth.
dilemma probably cannot handled without making approximations relaxed
planning graph.
One positive note, seems possible exploit typical structures ontologies
practice. particular, practical ontologies make extensive use subsumption relations,
structuring domain interest concept hierarchy. Additional ontology axioms often come
form constraints relations (reflexivity, symmetry, transitivity) typing number
relation arguments. may make sense exploit structures optimizing
formulas (a) associated SAT reasoning. Certainly, makes sense exploit structures
inside heuristic function. One include specialized analysis sub-solver techniques
recognize structures solve separately order obtain precise relaxed plans.
One even try take account structures inside relaxed planning, hence
(potentially) obtain fast heuristic function.

5. Compilation Initial State Uncertainty
show that, certain additional restrictions, off-the-shelf scalable tools planning
uncertainty exploited solve WSC|f wd . main limiting factors are: (1)
tools allow generation new constants. (2) tools allow specification
clausal formula initial state, states. approach deal (1) considers
set constants fixed priori, namely initially available constants plus additional potential
constants used instantiate outputs. subtle observation that, within special
case WSC|f wd , dynamics states become predictable priori, one also deal
(2) natural way.
follows, first introduce core observation case state space becomes
predictable, certain sense. observe predictability naturally given special
case forward effects, term strictly forward effects. discuss strengths limitations new special case. finally provide compilation strictly forward effects
planning initial state uncertainty.
5.1 Predictable State Spaces
core observation based notion compatible actions. Assume WSC|f wd task (P, IC ,
O, C0 , 0 , G ). Two actions a, compatible either Ea Ea = , effa = effa . is,
either disjunct outputs hence affect disjunct sets literals since
WSC|f wd effects agree completely. set actions compatible Ea C0 =
A, every pair actions compatible.
Lemma 2 states that, given used actions compatible, every state ever reached
satisfies action effects, modulo existing constants.
Lemma 2 (Predictable State Spaces WSC|f wd ) Assume WSC|f wd task, compatible set
actions A, state reached actions A. |= 0 and, A,
Ea Cs |= effa .
Proof: proof induction. base case, b0 , claim holds definition since
Cs Ea = A. Say reached action A. applicable

80

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

s, induction assumption nothing prove. Otherwise, WSC|f wd ,
Lemma 1 res(s, a) = {(C , ) | C = Cs Ea , |Cs = , |= IC effa }.
V
induction assumption applied s, res(s, a) = {(C , ) | C = Cs Ea , |=
0 A,E Cs effa IC effa }. Now, Ea Cs Ea Ea 6 Cs ,

2
Ea Ea 6= hence effa = effa prerequisite. concludes argument.
virtue lemma, possible configurations constants
generated
V
actions characterized formula IC 0 aA effa . Since parts
formula known prior planning, set possible configurations predictable.
even begin plan, already know constants behave generated.
list possible behaviors potential constants initial belief, let actions
affect constants actually exist. words, compile initial state
uncertainty. detail below. First, need identify setting Lemma 2
actually applied.
5.2 Strictly Forward Effects
Given WSC|f wd task, must settle finite set compatible actions planner
try compose plan from. One option simply require every action
unique output constants. appears undesirable since planning tasks often contain many actions,
set potential constants would huge. Further, enable chaining several actions,
potential constants allowed instantiate input parameters every operator, hence
necessitating creation new action and, that, new potential constants. unclear
break recursion, sensible way.
Herein, focus instead restriction WSC|f wd suffices assign unique output
constants individual operators, rather individual actions.
Definition 2 Assume WSC task (P, IC , O, C0 , 0 , G ). task strictly forward effects
iff:
1. O, l[X] effo , |X| > 0 X Yo .
2. clauses IC , = x1 , . . . , xk : l1 [X1 ] ln [Xn ], X1 =
= Xn .
set WSC tasks strictly forward effects denoted WSC|sf wd .
second condition identical corresponding condition WSC|f wd . first condition strictly stronger. WSC|f wd requires least one effect literal variable taken
outputs, WSC|sf wd requires variables taken outputs. Therefore,
obviously, WSC|sf wd WSC|f wd . Note WSC task formulated Example 2 member
WSC|sf wd .
key property WSC|sf wd that, without input variables effect, actions based
operator effect. So, action set compatible, need
choose set unique output constants every operator. Indeed, every set
operators whose effects pairwise identical. also choose several sets output constants
group operators.
81

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

5.3 Modeling Power
limitations WSC|f wd , discussed Section 4.2, naturally inherited WSC|sf wd . Moreover, unlike WSC|f wd , cannot state properties effect connect inputs
outputs. serious limitation. illustration, consider small VTA example
using. operator bookTicket effect ticketFor (y, x), relating produced ticket
train x given input. Clearly, notion ticket rather weak cannot state
ticket actually valid for. Another interesting case one extend Example 2 considering two proteins rather one. is, set C0 = {c, c }, 0 =
cellProtein(c) cellProtein(c ). wish encode need combined presentation
those, i.e., G = : combinedPresentation(y, c) combinedPresentation(y, c ). WSC|f wd ,
solve including, every information providing operator, input variable x
effect literal. example, set getInfo3D1n55 := ({x}, 1n55(x), {y}, Info3D(y, x)).
possible WSC|sf wd .
extent, difficulties overcome encoding relevant inputs predicate names. handle composition two proteins c c , would essentially mean
making copy entire model renaming part c . goal would G = y, :
combinedPresentation(y) combinedPresentation (y ), operator preconditions would make
sure combinedPresentation(y) generated before, combinedPresentation (y ) generated using new operators. Note rather dirty hack, depends knowing
number copies needed, prior planning. equivalent solution VTA would introduce
separate ticketFor-x predicate every entity x ticket may bought.
least, would result rather oversized unreadable model. yet troublesome case
time-step construction outlined Section 4.2, added new output variable
effect related via effect literal next(prevt, t) previous time step prevt provided
input. WSC|sf wd , longer relate prevt way stating time
step happens one. Trying encode information predicate names,
would include one predicate per possible time step. necessitates assuming bound
number time steps, clear limitation respect natural encoding.
Despite above, WSC|sf wd far pathological irrelevant special case. example
applies domain proteins shown Example 1. Similarly, hospital domain
discussed Section 4.2 naturally modeled WSC|sf wd . generally, fact
wealth WSC formalisms encode connections inputs outputs.
example, category contains formalisms rely exclusively specifying types
input output parameters. information modeled types kind input
service requires, kind output produces example, input train output
ticket. Examples formalisms various notions message-based composition (Zhan
et al., 2003; Constantinescu et al., 2004a; Lecue & Leger, 2006; Lecue & Delteil, 2007; Kona
et al., 2007; Liu et al., 2007). fact, early versions OWL-S regarded inputs outputs
independent semantic entities, using Description Logic formalization types.
Thus, existence compilation WSC|sf wd planning uncertainty quite
interesting. shows composition model similar early versions OWL-S, general
form partial matches powerful background ontologies, attacked off-the-shelf
planning techniques. opens new connection WSC planning.

82

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

5.4 Compilation
compile WSC|sf wd task task conformant planning initial state uncertainty,
takes form (P, A, 0 , G ). P finite set propositions used. finite set
actions, takes form (pre(a), eff(a)) pair sets literals P. 0
CNF formula P, G conjunction literals P. notions given standard
belief state semantics. state truth value assignment P. initial belief set states
satisfying 0 . result executing action state res(s, a) := 6|= pre(a),18
otherwise res(s, a) := (sadd(a))\del(a). use standard notation gives terms
set propositions makes true, uses add(a) denote positive literals eff(a),
del(a) denote negative literals eff(a). Extension res beliefs definition
plan remain unchanged.
Assume WSC|sf wd task (P, IC , O, C0 , 0 , G ). compiled task (P , A, 0 , G ) makes
use new unary predicate Ex expresses constants yet brought existence. compilation obtained follows. operator O, outputs
Yo =

{y1 , . . . , yk }, create set new constants Eo = {e1 , . . . , ek }. Then, C := C0 oO Eo
set constants fixed priori. Initialize := . operator O,
V include

preo ( xXo Ex(x))
V
V set actions resulting using C instantiate precondition
( eEo Ex(e)). Give actions effect, eEo Ex(e). words, instantiate os outputs Eo , enrich os precondition saying inputs exist
outputs yet exist, replace os effect statement simply bringing outputs
existence.
Replacing effects way, original effects go? included
initial state formula. is, initialize 0 conjunction effo [Eo /Yo ] operators
O. Then, instantiate clauses inVIC C andV
conjoin 0 . obtain final
0 conjoining 0 ( cC0 Ex(c)) cC\C0 Ex(c)) Goal. Here, Goal
new proposition. serves model goal. Namely, introduce set artificial
goal achievement actions. goal form G = x1 , . .V
. , xk .[x1 , . . . , xk ]. new actions
obtained instantiating operator ({x1 , . . . , xk }, ki=1 Ex(xi ), , Goal) C.
is, goal achievement actions instantiate existentially quantified variables goal
possible constants. actions added set A. overall compiled task takes
form (P , A, 0 , Goal), P simply set mentioned propositions.
summary, compile WSC|sf wd task (P, IC , O, C0 , 0 , G ) conformant planning
task (P , A, 0 , G ) follows:
operator O, create uniqueSset new constants Eo = {e1 , . . . , ek }
Yo = {y1 , . . . , yk }. denote C := C0 oO Eo .

P contains instantiations, C, P plus two new predicates, Ex Goal. Ex
arity 1 expresses constants yet brought existence. Goal arity 0
forms new goal, i.e., G = Goal.

actions instantiations O, XV
instantiated
VC, Yo inEx(x))

(
stantiated Eo . preconditions

enriched

(
eEo Ex(e)),
xXo
V
effects replaced eEo Ex(e).

18. before, give actions conditional effects semantics, rather usual distinction forced
preconditions, non-forced effect conditions.

83

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

Further, contains goal achievement actions, achieving Goal preconditions instantiating G C.
original action effects, i.e., conjunction effo [Eo /Yo ] operators
O,
V
. Further, contains ,
moved


instantiated

C,

(
0
IC
0
0
cC0 Ex(c))
V
cC\C0 Ex(c)) Goal.

terminology Section 5.1, means choose set actions actions
obtained operator instantiating inputs constants C,
outputs Eo . suggested Lemma 2, initial state formula 0 compiled
task describes possible configurations constants C, effect applying
action bring respective output constants existence. Note that, although effects
compiled actions positive, planning still hard (coNP-complete, precise) due
uncertainty. (If allow WSC operators also delete constants, negative effects
deleting constants compiled task.)
According strategy, create one set output constants per operator,
take account sets operators identical effects. simplify
presentation. results carry immediately complicated strategies create
one set output constants per operator, well strategies share sets output
constants operators identical effects. noted, however, operators
whose effects identical not, general, share outputs. particular, two
effects conflict, e.g., InfoDSSP(d) InfoDSSP(d), initial state formula 0
unsatisfiable. compiled planning task trivially solved empty plan, and,
course, encode solutions original problem.
Example 3 Re-consider planning task defined Example 2. specify compiled task. set
C = {c, d, e, f } c initially available constant, d, e, f potential constants
operator outputs. compiled planning task (P , A, 0 , G ) following:
P = {protein, cellProtein, G, H, I, 1n55, 1kw3, combinedPresentation, InfoDSSP,
Info3D, Ex, Goal}, predicates except Goal unary (have one argument).
consists instantiations of:
getInfoDSSPG [d/y]: ({x}, G(x) Ex(x) Ex(d), Ex(d))
getInfoDSSPH [d/y]: ({x}, H(x) Ex(x) Ex(d), Ex(d))
getInfoDSSPI [d/y]: ({x}, I(x) Ex(x) Ex(d), Ex(d))
getInfo3D1n55 [e/y]: ({x}, 1n55(x) Ex(x) Ex(e), Ex(e))
getInfo3D1kw3 [e/y]: ({x}, 1kw3(x) Ex(x) Ex(e), Ex(e))

combineInfo[f /y]: ({x1 , x2 }, InfoDSSP(x1 ) Info3D(x2 ) Ex(x1 ) Ex(x2 )
Ex(f ), Ex(f ))
GoalOp: ({x}, combinedPresentation(x) Ex(x), Goal)
0 conjunction of:
instantiations IC [consisting five axioms given Example 2]
84

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

cellProtein(c) [0 ]
InfoDSSP(d) Info3D(e) combinedPresentation(f ) [original action effects]
Ex(c) Ex(d) Ex(e) Ex(f ) [constants existence]
Goal [goal yet achieved]
G = Goal
consider plan original task (see Example 2): hgetInfoDSSPG (c, d),
getInfoDSSPH (c, d), getInfo3D1n55 (c, e), getInfo3D1kw3 (c, e), combineInfo(d, e, f )i.
illustrate, verify plan yields plan compiled task. task,
initial belief b0 consists states c existing constant, d, e, f satisfy
respective effects, |= IC cellProtein(c). apply action sequence:
1. Apply getInfoDSSPG (c, d) b0 . get belief b1 b0 except that,
b0 |= G(c), also exists.
2. Apply getInfoDSSPH (c, d) b1 . get belief b2 b1 except that,
b1 |= H(c), exists.
3. Apply getInfo3D1n55 (c, e) b2 , yielding b3 .
4. Apply getInfo3D1kw3 (c, e) b3 . brings us b4 Ex(e) b2
|= 1n55(c) |= 1kw3(c).
5. Apply combineInfo(d, e, f ) b4 . brings us b5 like b4 except b4
e exist also Ex(f ).
6. Apply GoalOp(f ) b5 , yielding b6 .
reasoning IC used Example 2 show b5 satisfies original goal,
used show GoalOp(f ) applicable b5 hence resulting belief b6
satisfies goal. obtain plan compiled task simply attaching goal achievement
action original plan.
prove soundness completeness compilation, need rule inconsistent
operators, i.e., operators whose effects conflict background theory (meaning
IC Xo , Yo : effo unsatisfiable). example, case x : A(x) B(x)
contained IC , effo = A(y) B(y). presence operator, initial belief
compiled task empty, making task meaningless. Note inconsistent operators
never part plan, hence filtered pre-process. Note also that, WSC|sf wd ,
operator inconsistent iff actions based inconsistent.
Non-goal achievement actions correspond actions original task, obvious
way. connection, transform plans compiled task directly plans
original task, vice versa.
Theorem 7 (Soundness Compilation) Consider WSC|sf wd task (P, IC , O, C0 , 0 , G )
without inconsistent operators plan ha1 , . . . , compiled task (P , A, 0 , G ).
sub-sequence non-goal achievement actions ha1 , . . . , plan task
(P, IC , O, C0 , 0 , G ).
85

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

Proof Sketch: arbitrary sequence non-goal achievement actions, denote b belief
execution original task, b belief execution compiled task.
state sSin original task, denote [s] class compiled-task states overVthe constants
C0 oO Eo {c
| s(Ex(c)) = 1} = Cs , s|Cs = , |= IC 0 oO effo [Eo ].
One prove b = sb [s]. claim follows directly that.
2

Theorem 8 (Completeness Compilation) Consider WSC|sf wd task (P, IC , O, C0 , 0 ,
G ) without inconsistent operators plan ha1 , . . . , every operator appears
one instantiation Eo outputs. ha1 , . . . , extended goal achievement
actions form plan compiled task (P , A, 0 , G ) obtained using outputs Eo .

Proof Sketch: Follows immediately b = sb [s] shown proof Theorem 7. Say
one executes ha1 , . . . , compiled task, ending belief b. there, plan
compiled task obtained simply attaching one goal achievement action every tuple
constants satisfying G world state b.
2

reader may noticed number instantiations goal achievement operator
exponential arity goal. worst case, instantiations must included
plan compiled task. particular, may happen plan constructed per
proof Theorem 8. However, practical purposes appears reasonable assume fixed upper
bound number goal variables.
indicated, proofs Theorems 7 8 remain valid allowing one Eo
per operator, and/or operators identical effects share output constants. Note operators identical effects several web services provide alternative ways achieving something.
Example 3 illustrates situation (cf. earlier discussion Section 3.2). experiments
described next section, groups operators identical effects assigned
output constants.

6. Empirical Results
show compilation approach merits, report number empirical experiments using CFF underlying planner. start discussion general experimental
setup discuss results two different test scenarios.
6.1 Experiments Setup
implemented compilation WSC|sf wd planning uncertainty described
above, connected CFF tool. noted that, although compiled planning
tasks delete effects, solved CFFs relaxed-plan-based heuristic function.
function makes relaxation ignoring one conditions effect (see
earlier discussion CFF Section 4.5). Ignoring one condition significantly affects
compiled tasks effects typically involve many conditions, particularly conditions
stating inputs exist outputs yet exist.
One problematic point evaluating planning-based WSC choice test cases. field
still rather immature, due widely disparate nature existing WSC tools,

86

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

common set benchmarks.19 fact, web service composition new topic
posing many challenges existing techniques, different works differ widely terms
underlying purpose, specific aspect WSC address. detailed discussion
existing WSC tools given Section 7. method choose evaluation design
two test scenarios reflect intuitively relevant kinds problem structures potential
applications planning-based WSC, scalable number interesting parameters.
test reaction approach parameters.
test scenarios artificial benchmarks, cannot lead broad conclusions significance practice, allow us draw conclusions planning behavior differently
structured test problems. solution method scales quite well tested cases, efficiently finding solutions involve many web service calls, successfully employ
services really necessary. Viewing results isolation, one conclude
representation techniques heuristic functions planning uncertainty may useful
attack large complex planning-like WSC instances.
comparison alternative WSC tools is, again, problematic, due broad range problems tools solve, different kinds solutions find, different kinds input
syntax/language read. obtain least notion empirical comparison tools,
following consider expressivity (How general input language tool?)
scalability (How quickly tool compose?). existing WSC tools constitutes
separate point trade-off two. question whether compilation
approach, restricting WSC|sf wd using CFF solve compiled tasks, sensible point
trade-off.
terms expressivity, approach located general planning methods (like
Eiter et al., 2003, 2004; Giunchiglia et al., 2004), inspired actions change literature,
restricted methods applied WSC far. question whether gain
scalability comparison expressive methods.
confirm experiments answer is, expected, yes. run DLVK tool
(Eiter et al., 2003, 2004), handles powerful planning language based logic programming.
language particular features static causal rules similar integrity constraints
fully general WSC.20 sense, perspective DLVK native WSC tool
handles ontology axioms directly rather via restricting expressivity compiling
away. particular, encoded WSC test problems directly DLVKs input language, without
compilation use CFF.
DLVK relies answer set programming, instead relaxed plan heuristics, find plans. Further, style many reasoning-based planners, DLVK requires input length bound
plan, hence used find optimal plans running several times different bounds.
cases, ran DLVK once, bound corresponding optimal plan length.
Even so, DLVK much slower CFF, solving small fraction test instances.
wish over-interpret results. conclude WSC|sf wd constitutes interesting
point trade-off expressivity scalability WSC.
19. VTA example could considered one benchmark, essentially every individual approach defines
particular version example.
20. similarity lies static causal rules fully general integrity constraints can, side effect applying
action, yield ramifications affecting properties inherited previous state.

87

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

running first tests compilation approach, noticed encoding
per Section 5.4 unnecessarily generous set initial states. Observe compiled
tasks always easier solve propositions true initial state. is, simply, literals operator preconditions, effects, goal positive. Hence, proposition
p appear positively initial state clause, one set p 0 initially, thereby
reduce number initial states, without introducing new plans.21 Setting proposition
0 may cause unit propagations, setting propositions 1 0. iterate steps
fixpoint occurs. resulting initial state description stricter before, yields better performance CFF DLVK. use optimized encoding experiments reported
below.
also experimented another optimization. optimization makes assumption
constants requested goal generated step-wise fashion, intermediate
constant generated certainty generating next constant. RecallVthat encoding
per Section 5.4, existence inputs operators, i.e., condition xXo exists(x),
part operator precondition thus interpreted conditional effects semantics. However, CFF DLVK offer distinction effect conditions forced preconditions
must hold entire belief
action applicable. exploit distinction
V
postulate condition xXo exists(x) forced. reduces state space, may cut
solutions. reduction quite beneficial CFF DLVK. Since optimization
affects set plans, switch part test cases, point possible speedup. tests optimization switched discussed text, indicated
keyword forced name test case.
use two versions CFF. One CFFs default configuration makes use FFs enforced Hill-climbing search algorithm well helpful actions pruning technique (Hoffmann
& Nebel, 2001). configuration, CFF helpful actions pruning turned off, search
proceeds standard greedy best-first fashion, open queue ordered increasing heuristic
values. henceforth denote former configuration CFF-def latter configuration
CFF-std.
results obtained 2.8GHz Pentium IV PC running Linux. tests run
time-out 600 seconds CPU, limiting memory usage 1 GB.
6.2 Subsumption Hierarchies
first investigate well approach deal scaling subsumption hierarchies,
building chains successively created entities (outputs). purpose, design test scenario
called SH, demands composition web services realizing chain generation steps,
every generation step deal subsumption hierarchy.
scenario depicted Figure 2. n top-level concepts L1 , . . . , Ln , depicted
TL Figure 2. goal input L1 , goal output Ln . Beneath Li ,
tree-shaped hierarchy sub-concepts. precisely, tree perfectly balanced
branching factor b, depth d. inner nodes tree called intermediate-level
(or simply intermediate) concepts, depicted IL Figure 2. leaf nodes tree
called basic-level (or simply basic) concepts, depicted BL Figure 2. every
non-leaf concept C tree, children C1 , . . . , Cb , axioms x : Ci (x) C(x)
21. course, reducing set initial states invalidate old plans, either.

88

fiTL

W EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

IL

IL

BL

BL

BL

BL

BL

BL

SWS

SWS

SWS

SWS

SWS

SWS

TL

IL

BL

IL

BL

BL

BL

BL

BL

Figure 2: Schematic illustration SH scenario.
expressing subsumption, well axiom x : C(x) C1 (x) Cb (x) expressing
parent covered children.
available web services defined follows. top level concept Li ,
leaf BLi,j corresponding tree structure, web service available takes
BLi,j input outputs Li+1 . corresponding WSC operator takes form oi,j =
({x}, BLi,j (x), {y}, Li+1 (y)). Then, applying, 1 < n order, services oi,j ,
possible make sure constant concept Li+1 created possible cases. Hence,
sequencing steps plan, length (n 1) bd . Note that, already stated
Section 5.4, experiments groups operators identical effects assigned
output constants. SH scenario, means 1 < n, oi,j share
output constant. Hence total number output constants generated, i.e., number
potential constants initial state, equal number top-level concepts, n.
Although SH scenario abstract nature, representative variety relevant
situations. Specifically, scenario model situations sets different services must
used address request none handle alone. role single service
handle particular possible case. example, set different services
set services oi,j assembled Li . Given constant c member Li , i.e.,
Li (c) holds, particular possible case handled service oi,j case c happens
member leaf BLi,j one cases must hold due coverage clauses tree.
89

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

Similar situations arise, e.g., geographically located (regional) services composition
request location-specific addresses locations higher (inter-regional) level. similar
pattern also found e-government scenarios clear-cut classification activities
leads establishing several parallel services serve different departmental areas.
Orthogonal horizontal composition, scenario model vertical composition,
one function pursued concatenating existing functions. case
complex procedures diverse areas e-government e-commerce.
scenario instantiated study different aspects scalability approach.
empirical tests measure scalability horizontal vertical direction. Further,
consider two extreme cases possible shapes individual concept trees chain,
giving us instances identical numbers leaves. set test scenario SH-broad,
= 1 b scales 2, 4, 8, 16, 32. set test scenario SH-deep, b = 2
scales 1, 2, 3, 4, 5. scenarios, n scales 2 20.
Further, designed SH-trap variant second chain n concepts linked,
completely irrelevant goal service. variant suitable testing extent
composition techniques affected irrelevant information. Finally, recall encoding
method
comes two versions explained above, default method treats input existence
V
xXo exists(x) conditional effects semantics, whereas non-default method, forced,
compromises completeness efficiency treating input existence forced precondition.
all, following choices: 3 different planners (CFF-def, CFF-std, DLVK);
2 different encoding methods; SH without trap; SH-broad SH-deep. crossproduct choices yields 24 experiments, within 19 possible values
n 5 possible values b d, i.e., 95 test instances. CFF, measured 3 performance
parameters: total runtime, number search states inserted open queue, number
actions plan. DLVK, measured total runtime number actions plan.
course, large amount data interesting. follows, summarize
important observations. Figure 3 shows data selected purpose. Part (a) figure
shows CFF-std SH-broad; (b) shows CFF-std SH-deep; (c) shows CFF-def SH-forcedbroad; (d) shows DLVK SH-broad SH-deep; (e) shows DLVK SH-forced-broad
SH-forced-deep; (f) shows DLVK CFF-std SH-trap. vertical axes show log-scaled
runtime (sec). horizontal axes show n (a), (b) (c). (d), (e) (f), n fixed n = 2
horizontal axes show number leaves concept hierarchy.
Consider first Figure 3 (a) (b). plots point efficiently CFF handle
kind WSC problem, even forced optimization. Comparing two plots points
difference handling broad deep concept hierarchies. plots, CFF-std runtime
shown n, length chain built. (a), show 5 curves 5 different
values b (the number leaves hierarchy depth 1), (b) show 5 curves 5
different values (the depth hierarchy branching factor 2). cases, scaling
behavior fairly good. small concept hierarchies (b = 2 = 1), chains almost arbitrary
length built easily. hierarchies grow, runtime becomes exponentially worse. Note,
however, one curve next size hierarchies doubles, growth
exponential. concept hierarchies 16 leaves, i.e., 16 alternative cases handled
step, still easily build chains 6 steps, solution involves 96 web services.
interesting aspect comparing two plots, (a) (b), underlying search spaces
actually identical: open queues same. difference performance stems
90

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

100

100

10

10

1

1

b=2
b=4
b=8
b=16
b=32

0.1

0.01
2

4

6

8

10

12

14

d=1
d=2
d=3
d=4
d=5

0.1

0.01

16

18

20

2

4

6

(a) CFF-std SH-broad

8

10

12

14

16

18

20

(b) CFF-std SH-deep
10000
SH-broad
SH-deep
1000

100

100

10
10

1
1

0.1
0.1

0.01
2

4

6

8

10

12

14

16

18

20

0.01
2

3

(c) CFF-def SH-forced-broad

4

5

6

(d) DLVK SH-broad SH-deep

10000

10000
SH-forced-broad
SH-forced-deep

DLVK SH-trap-broad
DLVK SH-forced-trap-broad
CFF-std SH-trap-broad

1000

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

2

(e) DLVK SH-forced-broad SH-forced-deep

4

6

8

10

12

14

16

18

20

22

24

26

30

(f) DLVK CFF-std SH-trap

Figure 3: Selected results SH scenario. See detailed explanation text.

91

28

32

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

overhead CFFs reasoning techniques, consume runtime case deep
concept hierarchies. Hence slightly worse behavior (b).
run CFF-def test suites Figure 3 (a) (b), obtain much worse behavior.
example, b = 8 get n = 3. reason seems FFs helpful actions
pruning enforced hill-climbing greedy domain. simple way overcome
use standard heuristic search algorithm instead, done CFF-std shown Figure 3 (a)
(b). hand, forced optimization switched on, helpful actions pruning
enforced hill-climbing work much better, obtain significant performance boost using
CFF-def. latter pointed Figure 3 (c), showing data CFF-def SH-forced-broad.
Like Figure 3 (a) CFF-std SH-broad, plot shows 5 curves, one 5 values
b (legend omitted plot would overlap curves). see that, case,
easily build arbitrarily long chains even b = 16, giving us solution involving 320 web
services n = 20. Even b = 32, still get n = 9.
Figure 3 (d) (e) show one gets trying solve examples, encoding
directly DLVK instead using compilation solving CFF. expected,
performance much worse. Since hardly test instance solved n > 2, fixed n
minimum value 2 plots, unlike (a), (b) (c). (d) (e) shows data
broad deep variants, showing number leaves horizontal axis. order obtain
fine-grained view, broad variant increase number steps 1 rather
multiplicative factor 2 before. see that, without forced optimization Figure 3 (d)
performance poor, largest case solve n = 2, b = 6 solution involves
6 web services. switch forced Figure 3 (e) performance dramatically improved
still different level obtain compilation+CFF.
Figure 3 (f), finally, exemplifies results get trap scenario. show data
broad version, default encoding CFF-std, default forced
encoding DLVK. DLVK quite affected irrelevant chain concepts, solving
single instance n = 2, b = 2 default encoding, getting n = 2, b = 16
forced encoding, instead n = 2, b = 20 without trap. behavior expected since
DLVK make use heuristic techniques would able detect irrelevance
second chain concepts. question whether CFFs techniques better that. Figure 3
(f) shows CFF-std largely unaffected n = 2 one see comparing curve
points vertical axis Figure 3 (a). However, n > 2 performance CFF-std
drastically degrades: instances solved n = 3, b = 2 n = 4, b = 2. reason
seems additional actions yield huge blow-up open queue used global
heuristic search algorithm CFF-std. Indeed, picture different using CFF-def
forced encoding instead: search spaces identical explored trap,
behavior get identical shown Figure 3 (c).
plans found SH scenario optimal, i.e., plans returned contain web
services needed. single exception DLVK trap, solutions include
useless web services trap chain.22
22. Note DLVKs plans parallel. parallel length optimal (because provided correct plan
length bound, cf. Section 6.1. However, parallel step may contain unnecessary actions, top necessary
ones. Thats happens trap.

92

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

6.3 Complex Concept Dependencies
two variants SH scenario feature tightly structured relationships involved
concepts, allow investigation scalability issues varying size structure.
consider advanced scenario, way top-level concepts covered lowerlevel concepts subject complex concept dependencies, similar axioms constraining protein classes characteristics Example 1. Therefore investigate performance
impacted complex concept structures subsumption hierarchies.
TL

TL

IL

IL

IL

IL

BL

BL

BL

BL

BL

BL

BL

BL

BL

BL

BL

BL

SWS

SWS

SWS

SWS

SWS

SWS

SWS

SWS

SWS

SWS

SWS

SWS

TL

TL

IL

BL

BL

IL

IL

BL

BL

BL

BL

BL

BL

IL

BL

BL

BL

BL

Figure 4: Schematic illustration CD scenario vs. SH scenario.
new scenario called CD, concept dependencies. Figure 4 illustrates scenario,
contrasts SH scenario. Similarly SH, top-level concepts,
one associated set basic sub-concepts. b basic concepts
every top-level concept. n top-level concepts L1 , . . . , Ln , goal achieve
Ln starting L1 . before, done combining web services cover
possibilities. Namely, every top-level concept Li every basic concept BLi,j associated
it, operator oi,j = (({x}, BLi,j (x), {y}, Li+1 (y)).23
difference lies connection basic concepts top-level concepts.
SH, rigidly given terms tree structure subsumption coverage axioms
intermediate concepts. Every basic concept i.e., every operator oi,j corresponding
concept included plan order cover possible cases. CD, use instead
complex set axioms connect basic concepts top-level. top-level concept
intermediate concepts ILi,1 , . . . , ILi,m , axioms stating ILi,j
23. Note that, i, operators assigned output constant compilation
technique.

93

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

sub-concept Li , well axiom x : Li (x) ILi,1 (x) ILi,m (x) stating
Li covered ILi,1 , . . . , ILi,m . connection intermediate concepts
basic concepts, complex dependencies used. intermediate subconcept constrained
covered non-empty set combinations basic subconcepts. Precisely, create
random DNF, positive literals, using basic concepts predicates. take
DNF imply ILi,j . Note that, implication, DNF negated hence becomes
CNF, directly encode formalism. every ILi,j .
setting, interesting control many combinations required cover
top-level concept Li . directly corresponds total number random combinations (random DNF disjuncts) generated, intermediate concepts ILi,j taken together.
control via call coverage factor, c, ranging (0, 1]. 2b 1 possible
combinations basic concepts, pick random subset size c (2b 1). combination associated DNF randomly chosen intermediate concept. Note CNF
formulas generated way may enormous. minimize size encoding, use
formula minimization software Espresso (Brayton, Hachtel, McMullen, & Sangiovanni-Vincentelli,
1984; McGeer, Sanghavi, Brayton, & Sangiovanni-Vincentelli, 1993).
hypothetically c set 0 task unsolvable. experiments reported below,
whenever write c = 0% means exactly one combination selected, associated
every intermediate concept.
escaping rigid schema relationships presented SH, CD scenario suitable test whether performance approach tied specific structure SH
problem. Moreover, way CD designed allows us determine degree planners
react intelligently different concept structures. particular, scenario allows analysis of:
1. ability approach, particular selected underlying planner CFF, identify plans contain relevant actions. Especially coverage factor c low,
basic subconcepts may never appear partition intermediate concepts, thus,
plan need include respective operators. Still, due conditional effects/partial matches semantics, plans include operators valid plans. Evaluating
plan length performance varying c therefore interesting.
2. ability approach deal complex axiomatizations. measured
terms impact coverage factor runtime performance. randomization
choice combinations basic factors, different settings c, may induce significant
differences CNF axiomatizations, result, subject underlying reasoning
engine different situations.
summary, CD scenario representative situations complex dependencies must
taken account order select correct services. Examples domains discussed
Sections 4.2 5.3. particular, CD scenario corresponds closely (a scalable version of)
protein domain example. different values DSSP code correspond different basic
concepts, respective getInfoDSSP services operators taking intermediate
concept, InfoDSSP(y). similar amino-acids, 3-D shapes, shapes complexes.
top level concept combinedPresentation(y) achieved constants every intermediate
concept created. So, difference CD lies that, rather single
top-level concept generated intermediates, CD sequence top-level concepts
need generated turn.
94

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

SH scenario, total data experiments extensive, even since
4 scenario parameters rather 2 before, since individual instances contain
random element. Figure 5, report selected results pointing main observations. Part
(a)/(b) figure show CFF-std runtime/plan length n = 4, b = 5; (c)/(d) show CFFstd runtime/search nodes c n = 5, = 3, b = 7; (e) shows DLVK CFF-std runtime
b CD n = 2, c = 100%; (f) show latter data CFF-def CD-forced.
Figure 5 (a) (b) consider scalability solution lengths test varying size
scenario, representing different coverage factors different lines. report data CFFstd. Results similar CD-forced CFF-def, i.e., contrary SH, CD setting
options bring significant performance gain. see Figure 5 (a) CFF scales
pretty well, though well SH, easily able solve tasks 7 top level concepts
4 intermediate concepts 5 basic concepts. Tasks minimum coverage factor,
c = 0%, solved particularly effortlessly. higher c values, one observe somewhat
easy-hard-easy pattern, where, example, curve c = 100% lies significantly
curves c = 40% c = 60%. examine easy-hard-easy pattern detail below.
Figure 5 (b), obvious expected observation plan length grows linearly
n, i.e., number top level concepts. likewise obvious, much important,
observation plan length grows monotonically coverage factor c. reported above,
lower coverage factor opens opportunity employ less basic services, namely
relevant ones. Figure 5 (b) clearly shows CFF-std effective determining
services relevant not.
Let us get back intriguing observation Figure 5 (a), easy-hard-easy pattern
growing c. Figure 5 (c) (d) examine phenomenon detail. plots scale c
horizontal axis, fixed setting n, b. Runtime shown (c), (d) shows
number search states inserted open queue. value c, plots give
average standard deviation results 30 randomized instances. clearly see easyhard-easy pattern (c) runtime, high variance particularly c = 80%. (d),
see pattern number search states, variance much less
pronounced. shows easy-hard-easy pattern due differences actual search
performed CFF, due effort spent search nodes. traced behavior CFF
detail, found reason easy-hard-easy pattern lies runtime CFF spends
SAT reasoning state transitions, i.e., reasoning uses determine facts
definitely true/false belief. high non-100 values c, CNF encodings
concept dependency structures take rather complex form. cases CFF takes lot
runtime, almost runtime spent within single call SAT solver. is, seems
CFFs SAT solver exhibits kind heavy-tailed behavior formulas, phenomenon
well known SAT CP community, see example work Gomes, Selman, Crato,
Kautz (2000). noted that, typical planning benchmarks, CNFs much
simpler structure, motivates use fairly naive SAT solver CFF, using neither clause
learning restarts, order save overhead formulas simple anyway. seems likely
addition advanced SAT techniques solver could ameliorate observed problem.
Finally, Figure 5 (e) (f) compare performances compilation+CFF DLVK (with
compilation). plots fix n = 2, i.e., data shown 2 top level concepts.
instances DLVK solves n > 2 ones forced optimization used n = 3,
= 2, b = 2. Further, plots c fixed c = 100%. reason
95

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

1000

35

c=0%
c=20%
c=40%
c=60%
c=80%
c=100%

100

c=0%
c=20%
c=40%
c=60%
c=80%
c=100%

30

25

10

20

15

1

10
0.1
5

0.01

0
2

3

4

5

6

7

2

3

(a) CFF-std runtime n

4

5

6

7

(b) CFF-std plan length n
250

100

200

10

150

1

100

0.1

50

0.01

0
0

20

40

60

80

100

0

(c) CFF-std runtime c
10000

40

60

1000

DLVK m=2
DLVK m=4
DLVK m=6
CFF-std m=2
CFF-std m=4
CFF-std m=6

1000

20

80

100

(d) CFF-std plan length c
DLVK m=2
DLVK m=4
DLVK m=6
CFF-def m=2
CFF-def m=4
CFF-def m=6

100

100
10
10
1
1

0.1
0.1

0.01

0.01
2

4

6

8

10

12

2

(e) DLVK CFF-std runtime b

4

6

8

10

12

(f) DLVK CFF-def runtime b

Figure 5: Selected results CD scenario. See detailed explanation text.
find significant difference performance DLVK different values c. DLVK
unable exploit lower c lower runtime, neither show easy-hard-easy pattern.
speculate DLVKs answer set programming solver tends perform exhaustive search anyway
accordingly affected different structures heuristic techniques employed
CFF. However, like CFF, DLVK able exploit lower coverage factors c shorter plans.
96

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

Figure 5 (e) shows default setting without forced optimization. see performance DLVK explodes quickly CFF experience much trouble. CFF fails
upper ends curves, Figure 5 (e) (f), problem files, i.e., CNFs
describing complex concept dependencies, become large parse (> 4 MB). notwithstanding, CFFs runtime behavior clearly exponential. Note, however, actual encodings,
i.e., problem instances solved, also grow exponentially c.
observe DLVK exhibits quite variance, particularly across different
settings m: curves cross Figure 5 (e). even pronounced Figure 5 (f),
also observe, SH, forced optimization brings huge advantage
DLVK. = 2 = 6 Figure 5 (f), DLVK fails first unsolved problem instance
due running memory shortly parsing problem.
Concluding section, observe empirical behavior CFF SH CD scenarios promising. results over-interpreted, though. test scenarios
capture problem structure typical variety potential applications WSC technology,
approach yet put test actual practice. same, however, said essentially
current planning-based WSC technology, since field whole still rather immature.

7. Related Work
relation work belief update literature covered detail already Sections 2.2 4.3. relation planning, formalism basically follows commonly
used frameworks. notions operators, actions, conditional effects exactly used
PDDL framework (McDermott et al., 1998; Bacchus, 2000; Fox & Long, 2003), except
extension outputs. Regarding latter, recognized time planning
community, example Golden (2002, 2003) Edelkamp (2003), on-the-fly creation
constants relevant feature certain kinds planning problems. However, attempts actually
address feature planning tools scarce. fact attempt aware work
Golden (2002, 2003) Golden, Pand, Nemani, Votava (2003). Part reason
situation probably almost current state art tools employ pre-processing procedures
compile PDDL task fully grounded description. core algorithms implemented based propositional representation. Lifting algorithms representation
involves variables on-the-fly instantiations requires major (implementation) effort. work
herein, circumvent effort using potential constants feeding resulting problem
CFF, like planners employs said pre-processing. Extending CFF WSC|f wd
involve dealing non-propositional representations sub-problem.
notion initial state uncertainty conformant plans closely follows related literature
planning uncertainty (Smith & Weld, 1998; Cimatti et al., 2004; Hoffmann & Brafman,
2006). formalization terms beliefs adapted work Bonet Geffner (2000).
related works planning allow domain axiomatization, i.e., form
axioms constraining possible world states (Eiter et al., 2003; Giunchiglia et al., 2004).
best knowledge, work planning exists, apart work presented herein,
considers combination domain axioms outputs.
words order regarding notions partial plug-in matches. terminology originates work service discovery SWS community (see example Paolucci
et al., 2002; Li & Horrocks, 2003; Kumar et al., 2007). service discovery, one concerned

97

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

matching service advertisements service requests. discovery result set services
whose advertisement matches request. descriptions services requests similar
functional-level service descriptions, i.e., planning operators use here. However,
terminology works slightly different ours, also describe additional kinds
matches. notions given Li Horrocks (2003) closest relation ours. Service
descriptions defined terms constructed Description Logic concepts. Say concept
describing advertisement, R concept describing request. Li Horrocks
say R have: exact match R; plug-in match R; subsume match
R; intersection match R 6 . compare setting, consider
situation effect action a, R precondition action r. Exact matches
special case plug-in matches distinguish herein. Intersection matches
correspond call partial matches. Concerning plug-in subsume matches, matters
subtle. intuitive meaning plug-in match advertisement fully suffices
fulfill request. planning terms, means effect implies precondition r.
However, service discovery traditionally taken mean every requested entity
provided, i.e., R. latter notion precondition r implies effect
meaningful planning. Hence use one two notions, correspondence Li
Horrockss subsume matches.
contrast work Li Horrocks (2003), work, Paolucci et al. (2002)
Kumar et al. (2007) define matches individual input/output parameters service descriptions,
rather service descriptions global level (precondition/effect us, constructed
concept Li & Horrocks, 2003). level individual parameters, Paolucci et al. (2002)
suggest notions Li Horrocks (2003) except less formal notation,
define intersection matches. true Kumar et al. (2007). latter
authors also define notions contains part-of matches, relating building blocks
constructed concepts. Obviously, notions make sense framework,
arent constructed concepts. Finally, Kumar et al. define ways aggregating matches
individual parameters matches entire service descriptions. Again, applicable
case since work global level first place.
brief survey existing works WSC follows. variety works compile composition less standard deterministic planning formalisms (Ponnekanti & Fox,
2002; Srivastava, 2002; Sheshagiri et al., 2003). works (Agarwal, Dasgupta, Karnik,
Kumar, Kundu, Mittal, & Srivastava, 2005b; Agarwal et al., 2005a) additionally focus end-to-end
integration SWS composition larger context. Akkiraju, Srivastava, Anca-Andreea, Goodwin, Syeda-Mahmood (2006) investigate techniques disambiguate concept names. McIlraith
Fadel (2002) achieve composition particular forms non-atomic services, modeling
latter atomic actions take meaning kind macro-actions. Narayanan McIlraith
(2002) obtain composition ability side-effect verifying SWS properties using Petri Nets.
Kuter, Sirin, Nau, Parsia, Hendler (2005), Au, Kuter, Nau (2005), Au Nau (2006)
focus information gathering composition time, rather plan execution time. McDermott
(2002) treats actual interaction (communication) web service planning problem.
Mediratta Srivastava (2006) design approach WSC based conditional planning, i.e.,
form planning uncertainty. suggests close relation work, focus
Mediratta Srivastavas work actually quite different ours. Mediratta Srivastava
consider output variables, neither consider domain axiomatizations.
98

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

overlap formalism lies allow incomplete initial state descriptions, i.e.,
initial states assign value subset propositions. handle observation
actions allow observing value unspecified proposition. ameliorate need
complete modeling, consider definition user acceptable plans, subset
plan branches, specified user, guaranteed lead goal. latter may
interesting option look extending framework handle partial observability.
Two approaches explore adapt formalisms so-called hand-tailored planning
SWS composition. approaches based Golog (McIlraith & Son, 2002) HTN planning (Sirin et al., 2004), respectively. frameworks enable human user provide control
information. However, non-deterministic action choice allowed. control information
given, planning fully automatic. Hence, sense, frameworks strictly
powerful planning without control information. Further, approaches capable
handling advanced plan constructs loops branches. Golog, possible plans
possible composition solutions described kind logic high-level instructions
given programmer, planner bind instructions concrete actions part
execution. HTN, programmer supplies planning algorithm set so-called
decomposition methods, specifying certain task accomplished terms combination sub-tasks. Recursively, decomposition methods sub-tasks. Thus
overall task decomposed step-wise fashion, atomic actions reached. Neither
McIlraith Son (2002) Sirin et al. (2004) concerned handling ontology axioms,
paper. Hence, combining insights directions synergetic potential,
interesting topic future work.
Another approach capable handling advanced plan constructs (loops, branches) described
Pistore et al. (2005b), Pistore, Traverso, Bertoli, Marconi (2005c), Pistore et al. (2005a),
Bertoli, Pistore, Traverso (2006). work, process level composition implemented,
opposed profile/capability level composition addressed paper. process level,
semantic descriptions detail precisely interact SWS, rather characterizing
terms preconditions effects. Pistore et al. (2005b, 2005c, 2005a) Bertoli
et al. (2006) exploit BDD (Binary Decision Diagram) based search techniques obtain complex
solutions fully automatically. However, ontology axioms handled input/output types
matched based type names.
approaches ontology axioms used requirements
matches relaxed. One described Sirin, Hendler, Parsia (2003), Sirin, Parsia,
Hendler (2004), Sirin Parsia (2004), Sirin et al. (2006). first two papers
series (Sirin et al., 2003, 2004), SWS composition support tool human programmers
proposed: stage composition process, tool provides user list
matching services. matches found examining subconcept relation. output
considered match input B B. corresponds plug-in matches. later work
(Sirin & Parsia, 2004; Sirin et al., 2006), HTN approach (Sirin et al., 2004) mentioned
adapted work standard planning semantics, description logics semantics
OWL-S. difficulties inherent updating belief observed, connection belief
update studied literature made, remains unclear solution adopted.
far aware, methods relaxed matches follow
termed message-based approach WSC. approaches already discussed depth
Section 2.3. Next, give details ones closely related work.
99

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

approach Liu et al. (2007) discussed sufficient detail already Section 2.3,
reconsider here.
Meyer Weske (2006) handle ontology axioms WSC tool, provide
semantics action applications. Reasoning used determine whether particular output
used establish particular input, approach classified message-based,
terms. kind matches handled said plug-in. best knowledge,
tool existing WSC tool employs relaxed plan based heuristic function, like
CFF. However, various design decisions, authors sacrifice scalability. explicitly
enumerate world states every belief, hence suffer exponentially large beliefs.
search forward parallel actions consequently suffer huge branching factor.
take heuristic relaxed planning graph length (rather relaxed plan length) thus
suffer fact that, time, hmax much less informative heuristic h+ (Bonet
& Geffner, 2001; Hoffmann, 2005).
approach rather closely related ours, handle partial matches, described
Constantinescu Faltings (2003) Constantinescu et al. (2004a, 2004b). work
ontology assumed take form tree concepts, edges indicate subconcept
relation. tree compiled intervals, interval represents concept
contents arranged correspond tree. intervals used efficient implementation
indexing service lookup (discovery), well matching composition. latter
searches forward space switches. Starting initial input, current input type
A, service input Ai matches Ai 6= . services collected set
collected Ai covers (that is, union intervals various Ai contains
interval A). collected services form switch, next step search,
outputs becomes new input must treated (i.e., switch node). Composition
interleaved discovery, i.e., every search state discovery called find services
match state. search proceeds depth-first fashion. Major differences work
following. First, formalization different, using intervals vs. using standard notions
planning based logics. Second, approach interleaves discovery composition,
separate steps framework (web service discovery needed determine operators
WSC task). Third, approach considers concept trees vs. clausal integrity constraints. Last,
approach uses depth-first search, whereas one main points making one
exploit heuristic techniques implemented standard planning tools scalable WSC.
Finally, interesting approach related planning described Ambite Kapoor (2007).
capture dependencies different input variables web service, input described terms relation variables. done outputs.
relations formulated terms logical formulas relative ontology. underlying formalism first-order logic, modeling language quite expressive.24 Reasoning performed
order establish links (messages, terms) inputs outputs. algorithmic
framework happens inspired partial-order planning (Penberthy & Weld, 1992),
starting goal relation maintaining set open links. solution DAG web
services links correspond different kinds data exchanges (selection, projection, join,
union). Automatic insertion mediator services, e.g., converting set standard formats,
also supported.
24. cost undecidable reasoning, according authors major issue practice.

100

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

extent, preconditions/effects clausal integrity constraints used model
relations sense Ambite Kapoor (2007). Say r k-ary relation definition
, describing input web service. set corresponding operators precondition
r(x1 , . . . , xk ), transform set universally quantified clauses. long
latter done, long ontology axioms likewise transformed, obtain
model equivalent Ambite Kapoor. sense, main modeling advantage
approach Ambite Kapoor WSC|f wd existential quantification. open question
whether quantification accommodated framework. Insertion mediator services
supported WSC|f wd , limited sense recognizing, via particular preconditions, particular kind mediator required. Modeling actual data flow bound
awkward. summary, work Ambite Kapoor advanced data description transformation point view. hand, Ambite Kapoor neither consider
belief update, place work context fully-fledged planning formalism,
less concerned exploiting heuristic technologies recent planners. Combining
virtues approaches within either framework interesting direction
research.

8. Discussion
suggested natural planning formalism significant notion web service composition
profile / capability level, incorporating on-the-fly creation constants model outputs, incomplete initial states model incomplete user input, conditional effects semantics model partial
matches, and, importantly, clausal integrity constraints model ontology axioms.
identified interesting special case, forward effects, semantics action applications
simpler general case. demonstrated relates belief update
literature, shown results reduced computational complexity. Forward effects
relate closely message-based WSC, results serve put form WSC context, extend towards general notion partial matches. Further, identified
compilation planning (initial state) uncertainty, opening interesting new connection
planning WSC areas.
empirical results encouraging, over-interpreted. test scenarios serve capture structural properties likely appear applications WSC
technology, approach yet put test actual practice. same, however,
said essentially current planning-based WSC technology, since field still rather immature. sense, thorough evaluation approach, planning-based WSC
whole, challenge future.
Apart evaluation, several directions research improving extending
technology introduced herein. line research find particularly interesting adapt
modern planning tools WSC, starting special cases, complications incurred
integrity constraints manageable. already outlined ideas adapting
CFF, pointed new challenges arise. appears particularly promising tailor generic
heuristic functions, originating planning, exploit typical forms ontology axioms occur
practice. Considering wealth heuristic functions available now, topic alone provides
material whole family subsequent work.

101

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

Acknowledgments
thank anonymous reviewers, well managing editor Derek Long, comments,
significant help improving paper.
Jorg Hoffmann performed part work employed University Innsbruck, Austria. work partly funded European Unions 6th Framework Programme SUPER project (IST FP6-026850, http://www.ip-super.org).
Piergiorgio Bertolis Marco Pistores work partly supported project Software
Methodology Technology Peer-to-Peer Systems (STAMPS).
Malte Helmerts work partly supported German Research Council (DFG) part
Transregional Collaborative Research Center Automatic Verification Analysis Complex
Systems (SFB/TR 14 AVACS). See www.avacs.org information.

Appendix A. Proofs
first formally prove Proposition 1, stating negative effects compiled away WSC.
so, first need introduce compilation formally. Assume WSC task (P,
+
IC , O, C0 , 0 , G ). construct second WSC task (P + , +
IC , , C0 , 0 , G ), initially
+
P + , IC O+ P, IC O, respectively. proceed follows. Let G
P predicate arity k, exists O, = (Xo , preo , Yo , effo ) effo
contains negative literal G(x1 , . . . , xk ). introduce new predicate notG P + ,
introduce two new clauses x1 , . . . , xk : G(x1 , . . . , xk ) notG(x1 , . . . , xk ) x1 , . . . , xk :
G(x1 , . . . , xk ) notG(x1 , . . . , xk ). every operator whose effect contains negation
G, replace, effo , G(a1 , . . . , ak ) notG(a1 , . . . , ak ).25 continue
negative effect literals remain O+ .
action (P, IC , O, C0 , 0 , G ) denote a+ corresponding action
+
+
(P + , +
IC , , C0 , 0 , G ). also use notation vice versa, i.e., action
+
+
+
(P , IC , , C0 , 0 , G ) denotes corresponding action (P, IC , O, C0 , 0 , G ).
= (Cs , ) state using predicates P, denote s+ state using predicates P + ,
following properties: Cs+ = Cs ; p P Cs Is+ (p) = (p); notp
p P Cs Is+ (notp) = 1 iff (p) = 0. Since is, obviously, exactly one
s+ , also use correspondence vice versa.
+
Proposition 1 Assume WSC task (P, IC , O, C0 , 0 , G ). Let (P + , +
IC , , C0 , 0 , G )
task negative effects compiled away. Assume action sequence ha1 , . . . , i.
Let b result executing ha1 , . . . , (P, IC , O, C0 , 0 , G ), b+ result
+
+
+
+
executing ha+
1 , . . . , (P , IC , , C0 , 0 , G ). Then, state s, b iff
s+ b+ .

Proof: induction length action sequence question. sequence empty,
consider initial beliefs two tasks, claim follows directly
definition. inductive step, say claim holds b b+ , action. need
show that, state s, res(b, a) iff s+ res(b+ , a+ ).
direction right left, say s+ res(b+ , a+ ). definition s+
+
+
+
res(s+
0 , ) state s0 b . induction hypothesis, s0 b. therefore suffices show
25. arguments ai may either variables constants.

102

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

res(s0 , a). need show (1) |= IC effa (2) differs s0 set-inclusion
minimal set values. (1) obvious definitions. Assume contrary (2)
exists s1 s1 |= IC effa s1 identical except exists least one propo+
sition p s1 (p) = s0 (p) s(p) 6= s0 (p). definition, get s+
1 |= IC effa+ .
+
+
+
+
Further, get s1 (p) = s0 (p) s+ (p) 6= s0 (p), altogether s1 <s+ s+ .
0
contradiction s+ res(s+ , a+ ), hence proves res(s0 , a) desired.
direction left right proceeds fashion. Say res(b, a). definition
+
res(s0 , a) state s0 b. induction hypothesis, s+
0 b . suffices
+
+
+
+
+
show s+ res(s+
0 , ). need show (1) |= IC effa (2) differs s0
set-inclusion minimal set values. (1) obvious definitions. Assume contrary
+
+
+
+
(2) exists s+
1 s1 |= IC effa+ s1 identical except
+
+
+
exists least one proposition p s+
1 (p) = s0 (p) (p) 6= s0 (p). definition, get
C

s1 |= IC effa . Further, p P 0 get s1 (p) = s0 (p) s(p) 6= s0 (p).
p = notq 6 P Cs0 get property q. Altogether, get s1 <s0 s.
+
contradiction res(s, a), hence proves s+ res(s+
2
0 , ) desired.
Theorem 1 Assume WSC task fixed arity, sequence ha1 , . . . , actions.
p2 -complete decide whether ha1 , . . . , plan.
Proof: Membership proved guess-and-check argument. First, observe that, arbitrary s, ,
A, decide within coNP whether res(s, A). Guess state Cs = Cs Ea .
Check whether |= IC effa . Check whether 6s . res(s, a) iff guess
succeeds. Further, action a, deciding whether inconsistent is, obviously, equivalent
satisfiability test, contained NP. instruments hand, design
guess-and-check procedure decide whether ha1 , . . . , plan. guess proposition
values along ha1 , . . . , i. check whether values comply res, lead
inconsistent action, final state satisfy goal. detail, checking proceeds
follows. First, check whether initial proposition values satisfy IC 0 . not, stop without
success. Otherwise, iteratively consider action ai , pre-state post-state . Check
NP oracle whether inconsistent. yes, stop success. not, test NP oracle
whether res(s, a). not, stop without success. Otherwise, < n, go ai+1 .
= n, test whether |= G . Stop success 6|= G , stop without success |= G .
ha1 , . . . , plan iff guess proposition values successful.
Hardness follows following adaptation proof Lemma 6.2 Eiter Gottlob
(1992). Validity QBF formula X.Y.[X, ], CNF, reduced plan testing
single action a. use 0-ary predicates X = {x1 , . . . , xm }, = {y1 , . . . , yn },
new 0-ary predicates {z1 , . . . , zm , r, t}. set operators contains single operator
empty in/out parameters, empty precondition, effect t. initial constants empty; 0
conjunction xi , yi , zi , r, t; G r. theory is:
(


^

i=1

(t xi zi )) (


^

(t xi zi )) (

i=1

^

(t r C)) (

C

103

n
^

i=1

(t yi r))

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

viewed set clauses C. readably, theory equivalent to:
[(


^

xi zi ) (r ) ((

n
_

yi ) r)]

i=1

i=1

refer initial belief b. plan test contains single action based (equal to,
fact) o. refer resulting belief b . Obviously, b contains single state everything
except true. Also, consistent: interpretation sets r yi 0 satisfies IC effa .
theory conjuncts xi zi make sure w b makes exactly one xi , zi true.
particular, different assignments X incomparable respect set inclusion. Hence,
every assignment aX truth values X, exists state b complies
aX : aX satisfiable together IC effa , assignment aX distant
least one variable (e.g., aX (xi ) = 1 aX (xi ) = 0 aX closer aX
regarding interpretation zi ).
prove that, plan, X.Y.[X, ] valid. Let aX truth value assignment X. above, state b complies aX . Since plan,
|= r. Therefore, due theory conjunct r , |= . Obviously, values
assigned satisfy aX .
direction, say X.Y.[X, ] valid. Assume that, contrary theW
claim,
plan. b 6|= r. then, due theory conjunct ( ni=1 yi ) r,
sets yi false. Now, X.Y.[X, ] valid, exists truth value
assignment aY complies setting xi zi s. Obtain modifying
comply aY , setting r 1. |= IC effa . then, closer
, hence 6 b contradiction. concludes argument.
2
Theorem 2. Assume WSC task fixed arity, natural number b unary representation.
p3 -complete decide whether exists plan length b.
Proof: membership, guess sequences actions containing b actions (note
size sequence polynomial size input representation). Theorem 1,
check p2 oracle whether sequence plan.
Hardness follows extension proof Lemma 6.2 Eiter Gottlob (1992). Validity QBF formula X.Y.Z.[X, Y, Z], CNF, reduced testing plan existence.
use 0-ary predicates X = {x1 , . . . , xn }, = {y1 , . . . , ym }, Z = {z1 , . . . , zk }, new
0-ary predicates {q1 , . . . , qm , r, t, f1 , . . . fn , h, g}. set operators composed of:
ot := (, f1 fn h, , g h)
1 n: oxi := (, h, , xi fi )
1 n: oxi := (, h, , xi fi )
initial constants empty. initial literal conjunction 0 composed yi , zi , qi ,
r, t, fi , h, g. is, yi , zi , qi well r h true, fi well
g false. value specified (only) xi . goal G r g. theory is:
(


^

i=1

(t yi qi )) (


^

(t yi qi )) (

i=1

^

(t r C)) (

C

104

n
^

i=1

(t zi r))

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

viewed set clauses C. readably, theory equivalent to:
[(


^

yi qi ) (r ) ((

i=1

n
_

zi ) r)]

i=1

First, note obvious things construction:
ot must included plan.
ot applied, action applied anymore.
ot applied, either oxi oxi must applied, every 1 n.
theory switched off, i.e., made irrelevant false, point ot
applied.
is, plan task must first apply oxi oxi , every 1 n, thereby choosing
value every xi . Then, ot must applied plan must stop. applying ot , changes
made states except values xi set fi made true one
other. Hence, belief b ot applied contains single state corresponds
extension 0 value assignment X, values fi flipped.
denote value assignment X aX . denote b := res(b, ot ). Note
ot consistent: interpretation sets r zi 0, besides setting immediate effects
g h, satisfies IC effot . Obviously, applications oxi oxi consistent
well.
theory conjuncts yi qi make sure w b makes exactly one yi , qi true.
particular, different assignments incomparable respect set inclusion. Hence,
every assignment aY truth values , exists state b complies
aY : aY satisfiable together IC effot , assignment aY distant
least one variable (e.g., aY (yi ) = 1 aY (yi ) = 0 aY closer aY regarding
interpretation qi ).
prove that, exists plan ~a yielding assignment aX , X.Y.Z.[X, Y, Z]
valid. Let aY arbitrary truth value assignment . state b
complies aX aY . aX aY satisfiable together IC effot . above,
assignment aY distant least one variable. And, course, one deviates
aX one distant respective variable. Since ~a plan, |= r.
Therefore, due theory conjunct r , |= . Obviously, values assigned Z
satisfy aX aY . proves claim aY chosen arbitrarily.
direction, say X.Y.Z.[X, Y, Z] valid. Let aX assignment X
Y.Z.[aX /X, Y, Z] valid. Let ~a corresponding plan, i.e., ~a first applies, 1 n,
either oxi oxi according aX . Thereafter, ~a applies ot . Assume
Wn ~a plan.



b 6|= r. then, due theory conjunct ( i=1 zi ) r, sets
zi false. Now, Y.Z.[aX /X, Y, Z] valid, exists truth value assignment
aZ Z complies setting xi , yi , qi s. Obtain modifying comply
aZ , setting r 1. |= IC effot . then, closer ,
hence 6 b contradiction. concludes argument.
2

105

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

Theorem 3. Assume WSC task. decision problem asking whether exists plan
undecidable.
Proof: result holds even empty background theory, complete specification
initial state, predicates arity 2, operators arity 2, goal variables
(arity 0), positive literals preconditions goal. result follows minor
modification Tom Bylanders proof (Bylander, 1994) plan existence propositional STRIPS
planning PSPACE-complete.26 original proof proceeds generic reduction, constructing
STRIPS task Turing Machine (TM) polynomially bounded space. latter restriction
necessary model machines tape: tape cells pre-created positions within bound.
makes difference PSPACE-membership undecidability ability create
constants. introduce simple operators allow us extend tape, ends.
detail, say TM (a finite number of) states q tape alphabet symbols (where b
blank); transition function, q0 initial state, F set accepting states;
input word. planning encoding contains following predicates. State(q) indicates
current TM state q. In(a, c) indicates current content tape cell c a. N eighbors(c, c )
true iff c (immediate) right neighbor c. At(c) indicates current position
TM head c. Rightmost(c) (Lef tmost(c)) true iff c currently right (left) neighbor.
set initial constants contains states q, alphabet symbols a, tape cells c corresponding
. initial literals, propositions constants assigned truth values
obvious. every transition (q, a, q , , R) include operator:
({x, x }, State(q) In(x, a) N eighbors(x, x ) At(x),
, State(q ) State(q) In(x, ) In(x, a) At(x ) At(x)).
Obviously, encodes exactly transition. likewise transitions (q, a, q , , L) .
model final states, introduce 0-ary predicate G, include q F operator:
(, State(q), , G)
finally include operators:
({x}, Rightmost(x), {x }, N eighbors(x, x ) In(b, x ) Rightmost(x ) Rightmost(x))

({x }, Lef tmost(x ), {x}, N eighbors(x, x ) In(b, x) Lef tmost(x) Lef tmost(x ))
definitions, easy verify exists plan iff TM reach accepting
state .
2
Lemma 1. Assume WSC|f wd task, reachable state s, action a. res(s, a) =
res|f wd (s, a).
26. Propositional STRIPS like framework, empty background theory, complete specification
initial state, goal variables, positive literals preconditions goal, output
parameters operators.

106

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

Proof: applicable s, claim holds trivially. Consider case.
Equation 3, res(s, a) defined

{(C , ) | C = Cs Ea , min(s, C , IC effa )} appl(s, a)
res(s, a) :=
{s}
otherwise
min(s, C , IC effa ) set C -interpretations satisfy IC effa
minimal respect partial order defined I1 I2 :iff propositions p Cs ,
I2 (p) = (p) I1 (p) = (p).
obvious res|f wd (s, a) res(s, a) satisfies IC effa identical
propositions Cs , particular minimal according .
direction, let res(s, a). Assume (p) 6= (p) proposition p
Cs . Define equal except (p) := (p). Obviously, 6s I2 .
suffices show |= IC effa : then, get 6 min(s, C , IC effa ) contradiction,
hence agrees propositions p Cs , hence res|f wd (s, a).
before, denote P Cs +Ea set propositions arguments Cs Ea ,
least one argument E, denote IC [Cs + Ea ] instantiation IC
constants Cs Ea , clause least one variable instantiated Ea . see
|= IC effa , consider first equivalent |= IC [Cs Ea ] effa ,
turn equivalent |= IC [Cs ] IC [Cs + Ea ] effa . last formula, task
WSC|f wd , IC [Cs ] speaks propositions P Cs , whereas IC [Cs +Ea ]effa speaks
propositions P Cs +Ea . treat two parts separately. |= IC [Cs ]
|= IC [Cs ] prerequisite since reachable. |= IC [Cs + Ea ] effa
definition. concludes argument.
2
Theorem 4. Assume WSC|f wd task fixed arity, sequence ha1 , . . . , actions.
coNP-complete decide whether ha1 , . . . , plan.
Proof: Hardness obvious, considering empty sequence. Membership shown
following guess-and-check argument. Say C union C0 output constants appearing
hA1 , . . . , i. guess interpretation propositions P C. Further,
1 n, guess set Ct constants. check polynomial time whether
Ct correspond execution hA1 , . . . , i. 1 n , say
applicable |= prea , Ca Ct , Ea Ct = . First, assert |= IC . Second,
, assert that, applicable, |= effa . Third, assert Ct+1 =
Ct {Ea | , applicable}. Using Lemma 1, easy see Ct correspond
execution iff three assertions hold. Note needs time-stamped
action generated outputs properties respective propositions remain fixed
forever. claim follows because, fixed arity, also test polynomial time whether
Cn satisfy G . guess Ct successful corresponds execution
satisfy G . Obviously, hA1 , . . . , plan iff guess Ct .
2
Theorem 5. Assume WSC|f wd task fixed arity, natural number b unary representation. p2 -complete decide whether exists plan length b.
Proof: membership, guess sequence b actions. Theorem 1, check
p2 oracle whether sequence plan.
107

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

prove hardness, assume QBF formula X.Y.[X, ] DNF normal form.
(This formula class complete p2 .) Say X = x1 , . . . , xn , = y1 , . . . , ym , = 1
k . design WSC|f wd task plan iff X.Y.[X, ] true. key construction
use outputs creation time steps, allow setting xi time step i.
yi take arbitrary values. xi set, one operator per k allows achieve goal
given k true. main property need ensure construction xi set
once, i.e., either 1 0. plan task iff one set X that,
, least one true case iff X.Y.[X, ] true.
predicates task P = {x1 (.), . . . , xn (.), y1 (), . . . , ym (), time(.), start(.),
next(..), goal(.)}. indicate predicate arity number points parentheses.
example, predicate next(..) arity 2. theory IC empty. initial constants
C0 = {t0 }. initial literals 0 = time(t0 ). goal y.goal(y). operators
follows:
1 n, have: oxi 1 = ({t0 , . . . , ti1 }, start(t0 ) next(t0 , t1 )
next(ti2 , ti1 ), {ti }, time(ti )next(ti1 , ti )xi (ti )). operator allows generating
time step i, setting xi 1 step.
1 n, have: oxi 0 = ({t0 , . . . , ti1 }, start(t0 ) next(t0 , t1 )
next(ti2 , ti1 ), {ti }, time(ti ) next(ti1 , ti ) xi (ti )). operator allows generating time step i, setting xi 0 step.
define value B below. n j < n + B, have: otj = ({t0 , . . . , tj1 },
start(t0 ) next(t0 , t1 ) next(tj2 , tj1 ), {tj }, time(tj ) next(tj1 , tj )).
operators allow increasing time step n n + B.
1 k, say = xlxj1 xlxjxn ylyj1 ylyjyn xlj {xj , xj }
ylj {yj , yj }. have: oi = ({t0 , . . . , tn+B }, start(t0 ) next(t0 , t1 )
next(tn+B1 , tn+B ) xlxj1 (txj1 ) xlxjxn (txjxn ) ylyj1 () ylyjyn (), {c},
goal(c)). operator allows achieve goal time step n + B, provided
respective true. Note xj precondition literals refer time step tj , i.e.,
value set xj earlier time step, yj precondition literals arguments
refer initial values yj , arbitrary.
Assume choose value B (polynomial input size). X.Y.[X, ] true,
then, obviously, find plan size n + B + k. apply oxi 1 oxi 0 operator xi ,
depending whether xi must set 1 0. apply B operators otj . apply operators
oi . respective input parameter instantiations obvious.
opposite direction proving truth X.Y.[X, ] based plan problematic.
plan might cheat setting xi 1 0. reason construction
complicated able avoid precisely case, based specifying strict enough plan length
bound b. key property that, order cheat xi , plan generate two sequences
time steps ti , . . . , tn+B . Therefore, lower bound length cheating plan n + 2B.
already seen, upper bound length non-cheating plan n + B + k.
determine plan length bound b, simply choose B cheating plan
use many steps: n+2B > n+B +k case iff B > k. set B := k +1, obtain
b := n + 2k + 1. bound b, plan proceed setting xi value (n actions),
108

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

increasing time step n + B = n + k + 1 (k + 1 actions), applying sufficient subset
oi (at k actions). plan cheats, needs apply least n+2B = n+2k +2 actions
able apply oi actions exploiting different value settings xi . concludes
argument.
2
Theorem 6. Assume WSC|f wd task. decision problem asking whether exists plan
undecidable.
Proof: reduce halting problem Abacus machines, undecidable. Abacus machine consists tuple integer variables v1 , . . . , vk (ranging positive integers
including 0), tuple instructions I1 , . . . , . state given content v1 , . . . , vk plus
index pc active instruction. machine stops iff reaches state pc = n. vi
initially 0, pc initially 0. two kinds instructions. Ii : INC j; GOTO Ii increments value vj jumps pc = . Ii : DEC j; BRANCH Ii+ /Ii0 asks whether vj = 0.
so, jumps pc = i0 . Otherwise, decrements value vj jumps pc = i+ .
map arbitrary abacus program WSC|f wd instance follows:
Predicates: number(v), zero(v), succ(v , v), value1 (v, t), . . . , valuek (v, t), instruction1 (t),
. . . , instructionn (t)
Background theory: none (i.e., trivial theory)
Operators:
operator h{v}, {number(v)}, {v }, {number(v ), succ(v , v)}i
instructions form Ii : INC j; GOTO Ii , operator
h{v1 , . . . , vk , t},
{instructioni (t), value1 (v1 , t), . . . , valuek (vk , t), succ(v , vj )},
{t },
{instructioni (t ), value1 (v1 , ), . . . , valuej1 (vj1 , ), valuej (v , ),
valuej+1 (vj+1 , ), . . . , valuek (vk , )}i.
instructions form Ii : DEC j; BRANCH Ii+ /Ii0 , operators
h{v1 , . . . , vk , t},
{instructioni (t), value1 (v1 , t), . . . , valuek (vk , t), succ(vj , v )},
{t },
{instructioni+ (t ), value1 (v1 , ), . . . , valuej1 (vj1 , ), valuej (v , ),
valuej+1 (vj+1 , ), . . . , valuek (vk , )}i.

h{v1 , . . . , vk , t},
{instructioni (t), value1 (v1 , t), . . . , valuek (vk , t), zero(vj )},
{t },
{instructioni0 (t ), value1 (v1 , ), . . . , valuej1 (vj1 , ), valuej (vj , ),
valuej+1 (vj+1 , ), . . . , valuek (vk , )}i.
109

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

Initial constants: v0 , t0
Initial literals: number(v0 )zero(v0 )value1 (v0 , t0 ) valuek (v0 , t0 )instruction1 (t0 )
Goal condition: t.instructionn (t)
describe intuitive meaning constants predicates. two kinds constants: numbers, represent natural numbers (including 0), time points, represent
computation steps Abacus machine. Variables refer time points denoted
above. variables represent numbers.
Three predicates refer numbers exclusively: number(v) true iff v encodes natural number
(and time point); zero(v) true iff v encodes number 0; succ(v , v) true iff v
encodes number one larger number encoded v. reduction
enforce every number uniquely represented (e.g., may several representations
number 3), unique representation necessary. guaranteed number 0
uniquely represented, though.
remaining predicates encode configurations Abacus machine: valuei (v, t) true iff,
time point t, i-th Abacus variable holds number represented v, instructionj (t)
true iff current instruction time point Ij .
Obviously, accepting run Abacus machine extract plan task,
vice versa. proves claim.
2
prove Theorems 7 8, first establish core lemma theorems follow
relatively easily. need notations. denote beliefs (states) (P, IC , O, C0 , 0 , G )
b (s), denote beliefs (states) (P , A, 0 , G ) b (s). Assume sequence ha1 , . . . ,
ai non-goal achievement actions. denote b := res(b0 , ha1 , . . . , ai i) b := res(b0 ,
ha1 , . . . , ai i). Note overload res function also denote state transitions
compiled task formalism. Further, state s, C(s) := {c | s(Ex(c)) = 1} denote
constants exist s. denote C relation states true iff
C(s) = C(s ) s|C(s) = |C(s) . C equivalence relation, equivalent states agree
constants exist howVthey interpreted. Note every state reachable
compiled
V
task satisfies |= IC 0 oO effo [Eo ]. Note IC 0 oO effo [Eo ] actually
satisfiable prerequisite, unless IC 0 unsatisfiable, outputs instantiated
unique constants operators consistent. state s, define [s] :=
^
[
{s | defined C0
effo [Eo ]}
Eo , C(s) = Cs , s|Cs = , |= IC 0
oO

oO

is, [s] equivalence class states reachable compiled task agree
constants exist interpreted.
Lemma 3 Assume WSC|sf wd task without inconsistent operators. Let ha1 , . . . , ai consist
non-goal achievement
actions, let b := res(b0 , ha1 , . . . , ai i) b := res(b0 , ha1 , . . . , ai i).

b = sb [s].

Proof: proof induction i. base case, = 0, i.e., b = b0 b = b0 .
b0 =
{s | Cs = C0 , |= IC 0 }
110

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

hand, b0 =
{s | C(s) = C0 , |= IC 0

^

effo [Eo ]}

oO

Obviously, latter comprised one equivalence class possibility assign propositions C0 way compliant IC 0 . exactly claim.
theS
inductive case, say add another action
ha1 , . . . , ai i. induction assumption,
b = sb [s]. need prove res(b, a) = res(b,a) [s ]. Obviously, suffices prove

that, every b, res([s], a) = res(s,a) [s ]. First, say applicable s.

neither applicable [s], res([s], a) = [s] = res(s,a) [s ]. Second, say
applicable s. Lemma 1 res(s, a) =
{(Cs Ea , ) | |Cs = , |= IC effa }
hand, res([s], a) =
{s | ex. [s], C(s ) = C(s) Ea , |C(s) = s, |= IC 0

^

effo [Eo ]}

oO

re-write latter
{s | C(s ) = Cs Ea , |Cs = , |= IC 0

^

effo [Eo ]}

oO

Obviously, desired, latter set comprised one equivalence class possibility
assign propositions Cs Ea way compliant IC effa . concludes
argument.
2
Theorem 7. Assume WSC|sf wd task (P, IC , O, C0 , 0 , G ) without inconsistent operators,
plan ha1 , . . . , compiled task (P , A, 0 , G ). sub-sequence non-goal
achievement actions ha1 , . . . , plan (P, IC , O, C0 , 0 , G ).
Proof: IC 0 unsatisfiable, nothing prove, start belief original
task empty. non-trivial case, first note that, plan compiled task, goal
achievement actions moved back plan. Hence, without loss generality,
assume ha1 , . . . , ai consist entirely non-goal achievement actions, hai+1 , . . . , ai consist
entirely goal achievement actions.
Denote b := res(b0 , ha1 , . . . , ai i) b := res(b0 , ha1 , . . . ,
ai i). Lemma 3, b = sb [s]. Since ha1 , . .
. , plan compiled task, every
b tuple constants satisfying G . b = sb [s], follows every b satisfies
G .
2
Theorem 8. Assume WSC|sf wd task (P, IC , O, C0 , 0 , G ) without inconsistent operators,
plan ha1 , . . . , every operator appears one instantiation Eo
outputs. ha1 , . . . , extended goal achievement actions form plan
compiled task (P , A, 0 , G ) obtained using outputs Eo .
Proof:
Denote b := res(b0 , ha1 , . . . , i) b := res(b0 , ha1 , . . . , i).
Lemma 3,
b = sb [s]. Since ha1 , . . . , plan, every b satisfies G . b = sb [s], follows
every b tuple constants satisfying G . Attaching respective goal achievement
actions yields plan compiled task.
2
111

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

References
3DComplex.org (2008). web server browse protein complexes known 3d structures.
http://supfam.mrc-lmb.cam.ac.uk/elevy/3dcomplex/data/hierarchy 1/root.html.
Agarwal, V., Chafle, G., Dasgupta, K., Karnik, N., Kumar, A., Mittal, S., & Srivastava, B. (2005a).
Synthy: system end end composition web services. Journal Web Semantics,
3(4).
Agarwal, V., Dasgupta, K., Karnik, N., Kumar, A., Kundu, A., Mittal, S., & Srivastava, B. (2005b).
service creation environment based end end composition web services. 14th
International Conference World Wide Web (WWW05), pp. 128137.
Akkiraju, R., Srivastava, B., Anca-Andreea, I., Goodwin, R., & Syeda-Mahmood, T. (2006). Semaplan: Combining planning semantic matching achieve web service composition.
4th International Conference Web Services (ICWS06).
Ambite, J., & Kapoor, D. (2007). Automatically composing data workflows relational descriptions shim services. 6th International Semantic Web Conference (ISWC07).
Ankolekar, A., Burstein, M., Hobbs, J., Lassila, O., Martin, D., McDermott, D., McIlraith, S.,
Narayanan, S., Paolucci, M., Payne, T., & Sycara, K. (2002). DAML-S: Web service description semantic web. 1st International Semantic Web Conference (ISWC02).
Au, T.-C., Kuter, U., & Nau, D. (2005). Web service composition volatile information. 4th
International Semantic Web Conference (ISWC05).
Au, T.-C., & Nau, D. (2006). incompleteness planning volatile external information.
17th European Conference Artificial Intelligence (ECAI06).
Baader, F., Lutz, C., Milicic, M., Sattler, U., & Wolter, F. (2005). Integrating description logics
action formalisms: First results. 20th National Conference Artificial Intelligence
(AAAI05).
Bacchus, F. (2000). Subset PDDL AIPS2000 Planning Competition. AIPS-00 Planning Competition Committee.
Bertoli, P., Pistore, M., & Traverso, P. (2006). Automated web service composition on-the-fly
belief space search. 16th International Conference Automated Planning Scheduling
(ICAPS06).
Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic search belief
space. 5th International Conference Artificial Intelligence Planning Systems (AIPS00),
pp. 5261.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),
533.
Branden, C., & Tooze, J. (1998). Introduction Protein Structure: Second Edition. Garland Publishing Company, New York. ISBN 0815323050.
Brayton, R., Hachtel, G., McMullen, C., & Sangiovanni-Vincentelli, A. (1984). Logic Minimization
Algorithms VLSI Synthesis. Kluwer Academic Publishers.
Brewka, G., & Hertzberg, J. (1993). things worlds: formalizing actions
plans. J. Logic Computation, 3(5), 517532.
112

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

Bryce, D., Kambhampati, S., & Smith, D. E. (2006). Planning graph heuristics belief space
search. Journal Artificial Intelligence Research, 26, 3599.
Burstein, M., Hobbs, J., Lassila, O., McDermott, D., McIlraith, S., Narayanan, S., Paolucci, M.,
Parsia, B., Payne, T., Sirin, E., Srinivasan, N., Sycara, K., & Martin, D. (2004). OWL-S:
Semantic Markup Web Services. OWL-S 1.1. http://www.daml.org/services/owl-s/1.1/.
Version 1.1.
Bylander, T. (1994). computational complexity propositional STRIPS planning. Artificial
Intelligence, 69(12), 165204.
Chasman, D. (Ed.). (2003). Protein Structure Determination, Analysis Applications Drug
Discovery. Marcel Dekker Ltd. 0-8247-4032-7.
Chen, Y., Wah, B., & Hsu, C. (2006). Temporal planning using subgoal partitioning resolution
SGPlan. Journal Artificial Intelligence Research, 26, 323369.
Cimatti, A., Roveri, M., & Bertoli, P. (2004). Conformant planning via symbolic model checking
heuristic search. Artificial Intelligence, 159(12), 127206.
Constantinescu, I., & Faltings, B. (2003). Efficient matchmaking directory services. 2nd
International Conference Web Intelligence (WI03).
Constantinescu, I., Faltings, B., & Binder, W. (2004a). Large scale, type-compatible service composition. 2nd International Conference Web Services (ICWS04).
Constantinescu, I., Faltings, B., & Binder, W. (2004b). Typed Based Service Composition. 13th
International Conference World Wide Web (WWW04).
de Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2006). update description
logic ontologies instance level. 21st National Conference Artificial Intelligence
(AAAI06).
de Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2007). approximation instance
level update erasure description logics. 22nd National Conference American
Association Artificial Intelligence (AAAI07).
de Jonge, M., van der Linden, W., & Willems, R. (2007). eServices hospital equipment. 6th
International Conference Service-Oriented Computing (ICSOC07), pp. 391397.
Edelkamp, S. (2003). Promela planning. 10th International SPIN Workshop Model Checking
Software (SPIN03).
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2003). logic programming approach
knowledge-state planning, II: DLVK system. Artificial Intelligence, 144(1-2), 157211.
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2004). logic programming approach
knowledge-state planning: Semantics complexity. Transactions Computational Logic,
5(2), 206263.
Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision, updates, counterfactuals. Artificial Intelligence, 57(2-3), 227270.
Fagin, R., Kuper, G., Ullman, J., & Vardi, M. (1988). Updating logical databases. Advances
Computing Research, 3, 118.

113

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

Fensel, D., Lausen, H., Polleres, A., de Bruijn, J., Stollberg, M., Roman, D., & Domingue, J. (2006).
Enabling Semantic Web Services Web Service Modeling Ontology. Springer-Verlag.
Fersht, A. (1998). Structure Mechanism Protein Science: Guide Enzyme Catalysis
Protein Folding. MPS. ISBN-13 9780716732686.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planning
domains. Journal Artificial Intelligence Research, 20, 61124.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local search temporal
action graphs. Journal Artificial Intelligence Research, 20, 239290.
Gerevini, A., Saetti, A., Serina, I., & Toninelli, P. (2005). Fast planning domains derived
predicates: approach based rule-action graphs local search. 20th National Conference American Association Artificial Intelligence (AAAI05).
Ginsberg, M., & Smith, D. (1988). Reasoning action I: possible worlds approach. Artificial
Intelligence, 35(2), 165195.
Giunchiglia, E., Lee, J., Lifschitz, V., McCain, N., & Turner, H. (2004). Nonmonotonic causal
theories. Artificial Intelligence, 153(1-2), 49104.
Giunchiglia, E., & Lifschitz, V. (1998). action language based causal explanation: Preliminary report. 15th National Conference Artificial Intelligence (AAAI98).
Golden, K. (2002). DPADL: action language data processing domains. Proc. 3rd
International NASA Planning Scheduling Workshop.
Golden, K. (2003). domain description language data processing. Proc. Workshop
Future PDDL ICAPS03.
Golden, K., Pand, W., Nemani, R., & Votava, P. (2003). Automating processing earth observation data. Proceedings 7th International Symposium Artificial Intelligence,
Robotics Automation Space.
Gomes, C., Selman, B., Crato, N., & Kautz, H. (2000). Heavy-tailed phenomena satisfiability
constraint satisfaction problems. Journal Automated Reasoning, 24(1/2), 67100.
Helmert, M. (2002). Decidability undecidability results planning numerical state variables. 6th International Conference Artificial Intelligence Planning Systems (AIPS02).
Helmert, M. (2006). Fast Downward planning system. Journal Artificial Intelligence Research, 26, 191246.
Herzig, A. (1996). PMA revisited. 5th International Conference Principles Knowledge
Representation Reasoning (KR96).
Herzig, A., Lang, J., Marquis, P., & Polacsek, T. (2001). Updates, actions, planning. 17th
International Joint Conference Artificial Intelligence (IJCAI01), pp. 119124.
Herzig, A., & Rifi, O. (1999). Propositional belief base update minimal change. Artificial
Intelligence, 115(1), 107138.
Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning benchmarks. Journal Artificial Intelligence Research, 24, 685758.
Hoffmann, J., & Brafman, R. (2006). Conformant planning via heuristic forward search: new
approach. Artificial Intelligence, 170(67), 507541.
114

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Katzuno, H., & Mendelzon, A. (1991). difference updating knowledge base
revising it. 2nd International Conference Principles Knowledge Representation
Reasoning (KR91).
Kona, S., Bansal, A., Gupta, G., & Hite, D. (2007). Automatic composition semantic web services. 5th International Conference Web Services (ICWS07).
Kumar, A., Neogi, A., Pragallapati, S., & Ram, J. (2007). Raising programming abstraction
objects services. 5th International Conference Web Services (ICWS07).
Kuter, U., Sirin, E., Nau, D., Parsia, B., & Hendler, J. (2005). Information gathering planning
web service composition. Journal Web Semantics, 3(2-3), 183205.
Lecue, F., & Delteil, A. (2007). Making difference semantic web service composition.
22nd National Conference American Association Artificial Intelligence (AAAI07).
Lecue, F., & Leger, A. (2006). formal model semantic web service composition. 5th
International Semantic Web Conference (ISWC06).
Li, L., & Horrocks, I. (2003). software framework matchmaking based semantic web
technology. 12th International Conference World Wide Web (WWW03).
Liberatore, P. (2000). complexity belief update. Artificial Intelligence, 119(1-2), 141190.
Lin, F., & Reiter, R. (1994). State constraints revisited. Journal Logic Computation, 4(5),
655678.
Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006a). Reasoning actions using description
logics general TBoxes. 10th European Conference Logics Artificial Intelligence
(JELIA 2006).
Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006b). Updating description logic ABoxes. 10th International Conference Principles Knowledge Representation Reasoning (KR06).
Liu, Z., Ranganathan, A., & Riabov, A. (2007). planning approach message-oriented semantic
web service composition. 22nd National Conference American Association
Artificial Intelligence (AAAI07).
Long, D., & Fox, M. (2003). 3rd international planning competition: Results analysis.
Journal Artificial Intelligence Research, 20, 159.
Lutz, C., & Sattler, U. (2002). proposal describing services DLs. International
Workshop Description Logics 2002 (DL02).
McCain, N., & Turner, H. (1995). causal theory ramifications qualifications. 14th
International Joint Conference Artificial Intelligence (IJCAI-95), pp. 19781984.
McCarthy, J., & Hayes, P. (1969). philosophical problems standpoint artificial
intelligence. Machine Intelligence, 4, 463502.
McDermott, D. (2002). Estimated-regression planning interactions web services. 6th
International Conference Artificial Intelligence Planning Systems (AIPS02).
McDermott, D., et al. (1998). PDDL Planning Domain Definition Language. AIPS-98
Planning Competition Committee.
115

fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE

McDermott, D. V. (1999). Using regression-match graphs control search planning. Artificial
Intelligence, 109(1-2), 111159.
McGeer, P., Sanghavi, J., Brayton, R. K., & Sangiovanni-Vincentelli, A. (1993). ESPRESSOSignature: new exact minimizer logic functions. Proceedings 30th ACM/IEEE
Design Automation Conference (DAC-93).
McGuinness, D. L., & van Harmelen, F. (2004). OWL Web Ontology Language Overview (W3C
Recommendation). online http://www.w3.org/TR/owl-features/.
McIlraith, S., & Fadel, R. (2002). Planning complex actions. 9th International Workshop
Non-Monotonic Reasoning (NMR02), pp. 356364.
McIlraith, S., & Son, T. C. (2002). Adapting Golog composition semantic Web services.
8th International Conference Principles Knowledge Representation Reasoning
(KR02).
Mediratta, A., & Srivastava, B. (2006). Applying planning composition web services
user-driven contingent planner. Tech. rep. RI 06002, IBM Research.
Meyer, H., & Weske, M. (2006). Automated service composition using heuristic search. 4th
International Conference Business Process Management (BPM06).
Narayanan, S., & McIlraith, S. (2002). Simulation, verification automated composition web
services. 11th International Conference World Wide Web (WWW02).
Palacios, H., & Geffner, H. (2007). conformant classical planning: Efficient translations
may complete too. 17th International Conference Automated Planning
Scheduling (ICAPS07).
Paolucci, M., Kawamura, T., Payne, T., & Sycara, K. (2002). Semantic matching web services
capabilities. 1st International Semantic Web Conference (ISWC02).
Pednault, E. P. (1989). ADL: Exploring middle ground STRIPS situation
calculus. 1st International Conference Principles Knowledge Representation
Reasoning (KR89).
Penberthy, J., & Weld, D. (1992). UCPOP: sound, complete, partial order planner ADL.
3rd International Conference Principles Knowledge Representation Reasoning
(KR92), pp. 103114.
Petsko, G. A., & Ringe, D. (2004). Protein Structure Function. New Science Press. ISBN
1405119225, 9781405119221.
Pistore, M., Marconi, A., Bertoli, P., & Traverso, P. (2005a). Automated composition web services planning knowledge level. 19th International Joint Conference Artificial
Intelligence (IJCAI05).
Pistore, M., Traverso, P., & Bertoli, P. (2005b). Automated composition web services planning
asynchronous domains. 15th International Conference Automated Planning
Scheduling (ICAPS05).
Pistore, M., Traverso, P., Bertoli, P., & Marconi, A. (2005c). Automated synthesis composite
BPEL4WS web services. 3rd International Conference Web Services (ICWS05).

116

fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION

Ponnekanti, S., & Fox, A. (2002). SWORD: developer toolkit web services composition.
11th International Conference World Wide Web (WWW02).
Reiter, R. (1991). frame problem situation calculus: simple solution (sometimes)
completeness result goal regression. Artificial intelligence mathematical theory
computation: papers honour John McCarthy, pp. 359380.
Roman, D., Keller, U., Lausen, H., de Bruijn, J., Lara, R., Stollberg, M., Polleres, A., Feier, C.,
Bussler, C., & Fensel, D. (2005). Web Service Modeling Ontology. Applied Ontology, 1(1),
77106.
Sheshagiri, M., desJardins, M., & Finin, T. (2003). planner composing services described
DAML-S. Third Symposium Adaptive Agents Multi-Agent Systems (AAMAS03).
Sirin, E., Parsia, B., Wu, D., Hendler, J., & Nau, D. (2004). HTN planning web service composition using SHOP2. Journal Web Semantics, 1(4).
Sirin, E., Hendler, J., & Parsia, B. (2003). Semi-automatic composition web services using
semantic descriptions. Workshop Web Services ICEIS03.
Sirin, E., & Parsia, B. (2004). Planning semantic web services. Workshop Semantic Web
Services ISWC04.
Sirin, E., Parsia, B., & Hendler, J. (2004). Composition-driven filtering selection semantic
web services. AAAI Fall Symposium Semantic Web Services.
Sirin, E., Parsia, B., & Hendler, J. (2006). Template-based composition semantic web services.
AAAI Fall Symposium Agents Search.
Smith, D. E., & Weld, D. (1998). Conformant Graphplan. 15th National Conference
American Association Artificial Intelligence (AAAI-98).
Srivastava, B. (2002). Automatic web services composition using planning. Knowledge Based
Computer Systems (KBCS02), pp. 467477.
Thakkar, S., Ambite, J. L., & Knoblock, C. (2005). Composing, optimizing, executing plans
bioinformatics web services. VLDB Journal, Special Issue Data Management, Analysis
Mining Life Sciences, 14(3), 330353.
Thiebaux, S., Hoffmann, J., & Nebel, B. (2005). defense PDDL axioms. Artificial Intelligence,
168(12), 3869.
Winslett, M. (1988). Reasoning actions using possible models approach. 7th National
Conference American Association Artificial Intelligence (AAAI88).
Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.
Zhan, R., Arpinar, B., & Aleman-Meza, B. (2003). Automatic composition semantic web services. 1st International Conference Web Services (ICWS03).

117

fi
