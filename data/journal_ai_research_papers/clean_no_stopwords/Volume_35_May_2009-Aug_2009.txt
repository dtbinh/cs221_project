Journal Artificial Intelligence Research 35 (2009) 593621Submitted 02/09; published 07/09Bounds Arc Consistency Weighted CSPsMatthias ZytnickiMatthias.Zytnicki@versailles.inra.frINRA, Unite de Recherche en Genomique et InformatiqueUR 1164, Versailles, FranceChristine GaspinSimon de GivryThomas SchiexChristine.Gaspin@toulouse.inra.frSimon.DeGivry@toulouse.inra.frThomas.Schiex@toulouse.inra.frINRA, Unite de Biometrie et Intelligence ArtificielleUR 875, Toulouse, FranceAbstractWeighted Constraint Satisfaction Problem (WCSP) framework allows representingsolving problems involving hard constraints cost functions. applied various problems, including resource allocation, bioinformatics, scheduling, etc.solve problems, solvers usually rely branch-and-bound algorithms equippedlocal consistency filtering, mostly soft arc consistency. However, techniqueswell suited solve problems large domains. Motivated resolutionRNA gene localization problem inside large genomic sequences, spirit boundsconsistency large domains crisp CSPs, introduce soft bounds arc consistency,new weighted local consistency specifically designed WCSP large domains.Compared soft arc consistency, BAC provides significantly improved time spaceasymptotic complexity. paper, show semantics cost functionsexploited improve time complexity BAC. also comparetheory practice efficiency BAC WCSP bounds consistency enforcedcrisp CSP using cost variables. two different real problems modeled WCSP,including RNA gene localization problem, observe maintaining bounds arc consistency outperforms arc consistency also improves bounds consistency enforcedconstraint model cost variables.1. IntroductionWeighted Constraint Satisfaction Problem (WCSP) extension crisp Constraint Satisfaction Problem (CSP) allows direct representation hard constraintscost functions. WCSP defines simple optimization (minimization) frameworkwide range applications resource allocation, scheduling, bioinformatics (Sanchez,de Givry, & Schiex, 2008; Zytnicki, Gaspin, & Schiex, 2008), electronic markets (Sandholm,1999), etc. also captures fundamental AI statistical problems MaximumProbability Explanation Bayesian nets Markov Random Fields (Chellappa & Jain,1993).crisp CSP, two main approaches solve WCSP inference search.last approach usually embodied branch-and-bound algorithm. algorithmestimates node search tree lower bound cost solutionssub-tree.c2009AI Access Foundation. rights reserved.fiZytnicki, Gaspin, de Givry & SchiexOne successful approaches build lower bounds obtainedextending notion local consistency WCSP (Meseguer, Rossi, & Schiex, 2006).includes soft AC (Schiex, 2000), AC* (Larrosa, 2002), FDAC* (Larrosa & Schiex,2004), EDAC* (Heras, Larrosa, de Givry, & Zytnicki, 2005), OSAC (Cooper, de Givry,& Schiex, 2007) VAC (Cooper, de Givry, Sanchez, Schiex, & Zytnicki, 2008) amongothers. Unfortunately, worst case time complexity bounds associated enforcingalgorithms least cubic domain size use amount spaceleast linear domain size. makes consistencies useless problemslarge domains.motivation designing local consistency enforced efficientlyproblems large domains follows interest RNA gene localization problem. Initially modeled crisp CSP, problem tackled using bounds consistency (Choi, Harvey, Lee, & Stuckey, 2006; Lhomme, 1993) dedicated propagatorsusing efficient pattern matching algorithms (Thebault, de Givry, Schiex, & Gaspin, 2006).domain sizes related size genomic sequences considered reachhundreds millions values. order enhance tool scoring capabilitiesimproved quality localization, shift crisp weighted CSP natural steprequires extension bounds consistency WCSP. Beyond direct motivation,extension also useful domains large domains occur naturally temporal reasoning scheduling.local consistencies define combine principles bounds consistencyprinciples soft local consistencies. definitions general restrictedbinary cost functions. corresponding enforcing algorithms improve timespace complexity AC* factor also nice rare property,WCSP local consistencies, confluent.done AC-5 Van Hentenryck, Deville, Teng (1992) functionalmonotonic constraints, show different forms cost functions (largely capturednotion semi-convex cost functions) processed efficiently. alsoshow powerful bounds arc consistencies strictly strongerapplication bounds consistency reified representation WCSP proposedPetit, Regin, Bessiere (2000).conclude, experimentally compare efficiency algorithms maintaindifferent local consistencies inside branch-and-bound agile satellite scheduling problems (Verfaillie & Lematre, 2001) RNA gene localization problems (Zytnicki et al.,2008) observe clear speedups compared different existing local consistencies.2. Definitions Notationssection introduce main notions used throughout paper.define (Weighted) Constraint Satisfaction Problems, well local consistencyproperty frequently used solving Weighted Constraint Satisfaction Problem: arcconsistency (AC*).594fiBounds Arc Consistency Weighted CSPs2.1 Constraint NetworksClassic weighted constraint networks share finite domain variables one components. paper, domain variable xi denoted D(xi ). denote valueD(xi ), use index vi , vi ,. . . variable xi , assume domain xitotally ordered denote inf(xi ) sup(xi ) minimum (resp. maximum)values domain D(xi ). assignment tS set variables = {xi1 , . . . , xir }function maps variables elements domains: tS = (xi1 vi1 , . . . , xir vir ){i1 , . . . , ir }, tS (xi ) = vi D(xi ). given assignment tS xi S,simply say value vi D(xi ) belongs tS mean tS (xi ) = vi . denote, set possible assignments S.Definition 2.1 constraint network (CN) tuple P = hX , D, Ci, X = {x1 , . . . , xn }set variables = {D(x1 ), . . . , D(xn )} set finite domainsvariable. C set constraints. constraint cS C defines set authorizedcombinations values variables subset . called scope cS .|S| called arity cS . simplicity, unary (arity 1) binary (arity 2) constraintsmay denoted ci cij instead c{xi } c{xi ,xj } respectively. denotemaximum domain size, n, number variables network e, numberconstraints. central problem constraint networks find solution, definedassignment tX variables constraint cS C, restriction tXauthorized cS (all constraints satisfied). Constraint SatisfactionProblem (CSP).Definition 2.2 Two CNs variables equivalent setsolutions.CN said empty one variables empty domain.may happen following local consistency enforcement. CN large domains, usebounds consistency usual approach. Historically, different variants boundsconsistency introduced, generating confusion. Using terminology introduced Choi et al. (2006), bounds consistency considered paperbounds(D) consistency. consider large domains defining intervals,actually equivalent bounds(Z) consistency. simplicity, rest paperdenote bounds consistency.Definition 2.3 (Bounds consistency) variable xi bounds consistent iff every constraint cS C xi contains pair assignments (t, )inf(xi ) sup(xi ) . case, called supports two boundsxi domain.CN bounds consistent iff variables bounds consistent.enforce bounds consistency given CN, domain bound satisfyproperties deleted fixed point reached.595fiZytnicki, Gaspin, de Givry & Schiex2.2 Weighted Constraint NetworksWeighted constraint networks obtained using cost functions (also referred softconstraints) instead constraints.Definition 2.4 weighted constraint network (WCN) tuple P = hX , D, W, ki,X = {x1 , . . . , xn } set variables = {D(x1 ), . . . , D(xn )} set finitedomains variable. W set cost functions. cost function wS W associatesinteger cost wS (tS ) [0, k] every assignment tS variables S. positivenumber k defines maximum (intolerable) cost.cost k, may finite infinite, cost associated forbidden assignments. cost used represent hard constraints. Unary binary cost functionsmay denoted wi wij instead w{xi } w{xi ,xj } respectively. usuallyWCNs, assume existence zero-arity cost function, w [0, k], constant costwhose initial value usually equal 0. cost assignment tX variablesobtained combining costs cost functions wS W applied restrictiontX S. combination done using function defined b = min(k, + b).Definition 2.5 solution WCN assignment tX variables whose costless k. optimal assignment X strictly lower cost.central problem WCN find optimal solution.Definition 2.6 Two WCNs variables equivalent givecost assignments variables.Initially introduced Schiex (2000), extension arc consistency WCSPrefined Larrosa (2002) leading definition AC*. decomposedtwo sub-properties: node arc consistency itself.Definition 2.7 (Larrosa, 2002) variable xi node consistent iff:vi D(xi ), w wi (vi ) < k.vi D(xi ) wi (vi ) = 0. value vi called unary support xi .WCN node consistent iff every variable node consistent.enforce NC WCN, values violate first property simply deleted.Value deletion alone capable enforcing second property. shown CooperSchiex (2004), fundamental mechanism required ability move costsdifferent scopes. cost b subtracted greater cost functiondefined b = (a b) 6= k k otherwise. Using , unary supportvariable xi created subtracting smallest unary cost minvi D(xi ) wi (vi )wi (vi ) adding (using ) w. operation shifts costs variablesw, creating unary support, called projection wi w.cancel out, defining fair valuation structure (Cooper & Schiex, 2004), obtained WCNequivalent original one. equivalence preserving transformation (CooperSchiex) precisely described ProjectUnary() function Algorithm 1.able define arc AC* consistency WCN.596fiBounds Arc Consistency Weighted CSPsAlgorithm 1: Projections unary binary levels12345678910Procedure ProjectUnary(xi )min minvi D(xi ) {wi (vi )} ;(min = 0) return;foreach vi D(xi ) wi (vi ) wi (vi ) min ;w w min ;[ Find unary support xi ]Procedure Project(xi , vi , xj )[ Find support vi w.r.t. wij ]min minvj D(xj ) {wij (vi , vj )} ;(min = 0) return;foreach vj D(xj ) wij (vi , vj ) wij (vi , vj ) min ;wi (vi ) wi (vi ) min ;Definition 2.8 variable xi arc consistent iff every cost function wS Wxi S, every value vi D(xi ), exists assignment viwS (t) = 0. assignment called support vi wS . WCN AC* iffevery variable arc node consistent.enforce arc consistency, support given value vi xi cost function wScreated subtracting (using ) cost mintS ,vi wS (t) costsassignments containing vi adding wi (vi ). cost movements, appliedvalues vi D(xi ), define projection wS wi . Again, transformationpreserves equivalence problems. precisely described (for simplicity,case binary cost functions) Project() function Algorithm 1.Example 2.9 Consider WCN Figure 1(a). contains two variables (x1 x2 ),two possible values (a b, represented vertices). unary cost functionassociated variable, cost value represented inside correspondingvertex. binary cost function two variables represented weighted edgesconnecting pairs values. absence edge two values represents zero cost.Assume k equal 4 w equal 0.Since cost w1 (x1 a) equal k, value deleted domainx1 (by NC, first property). resulting WCN represented Figure 1(b). Then,since x2 unary support (second line definition NC), project cost1 w (cf. Figure 1(c)). instance NC. enforce AC*, project 1binary cost function w12 value x1 since value support w12(cf. Figure 1(d)). Finally, project 1 w1 w, seen Figure 1(e). Ultimately,note value b x2 support. enforce AC*, project binary cost1 value remove since unary cost 2 which, combined wreaches k = 4.597fiZytnicki, Gaspin, de Givry & Schiexw = 0, k = 4x1x2w = 0, k = 4x2x1141102b2b1022bb01bb010x101bb(e) find unary support usingProjectUnary(x1 )(d) find support (x1b) using Project(x1 , b, x2 )(AC*)021b(c) find unary support usingProjectUnary(x2 ) (NC*)w = 2, k = 4x2x1w = 1, k = 4x1x211(b) prune forbidden values(NC*)(a) original instanceb11bw = 1, k = 4x2x1w = 2, k = 4x200b(f) Arc consistency enforcedFigure 1: Enforcing Arc Consistency.3. Bounds Arc Consistency (BAC)crisp CSP, bounds consistency enforcing process deletes boundssupported one constraint. weighted CSP, enforcement complex. similarvalue deletion process exists based first node consistency property violation (wheneverw wi (vi ) reaches k), additional cost movements performed enforce node arcconsistency.shown AC*, projections require ability represent arbitrary unarycost function wi every variable xi . requires space O(d) general since projectionslead arbitrary changes original wi cost function (even efficientinternal representation). prevent this, therefore avoid move cost cost functionsarity greater one unary constraints. Instead projections, keepvalue deletion mechanism applied bounds current domain takesaccount cost functions involving variable considered. given variable xiinvolved cost function wS , choice given value vi least induce costincrease mintS ,vi tS wS (tS ). minimum costs, combined cost functionsinvolving xi , together w, reach intolerable cost k, value deleted.bounds consistency, done two bounds domain. leadsfollowing definition BAC (bounds arc consistency) WCSP:Definition 3.1 WCN P = hX , D, W, ki, variable xi bounds arc consistent iff:XwS (tS ) < kwmintS ,inf(xi )tSwS W,xiwXwS W,ximintS ,sup(xi )tSwS (tS ) < kWCN bounds arc consistent every variable bounds arc consistent.598fiBounds Arc Consistency Weighted CSPsOne note definition proper generalization bounds consistency sincek = 1, actually equivalent definition bounds(D) consistency crispCSP (Choi et al., 2006) (also equivalent bounds(Z) consistency since domains definedintervals).algorithm enforcing BAC described Algorithm 2. enforcing BACuses value deletion, similar structure bounds consistency enforcement.maintain queue Q variables whose domain modified (or untested).better efficiency, use extra data-structures efficiently maintain combined cost associated domain bound inf(xi ), denoted winf (xi ). cost function wS involvingxi , contribution wS combined cost equal mintS ,inf(xi )tS wS (tS ).contribution maintained data-structure inf (xi , wS ) updated whenever minimum cost may change value removals. Notice that, Algorithm 2, line 14concise way denote hidden loops initialize winf , wsup , inf supdata-structures zero.Domain pruning achieved function PruneInf() also resets data-structuresassociated variable line 35 data-structures recomputedvariable extracted queue. Indeed, inside loop line 20, contributionsinf (xi , wS ) cost winf (xi ) cost functions wS involving xj reset.Function pop removes element queue returns it.Proposition 3.2 (Time space complexity) WCN maximum arity rconstraints, enforcing BAC Algorithm 2 time O(er2 dr ) space O(n + er).Proof: Regarding time, every variable pushed Q + 1 times:beginning, one values removed. consequence,foreach loop line 18 iterates O(erd) times, foreach loop line 20 iteratesO(er2 d) times. min computation line 22 takes time O(dr1 ) thus, overalltime spent line takes time O(er2 dr ). PruneInf() called O(er2 d) times.condition line 32 true O(nd) times so, line 35 takes time O(ed)(resetting inf (xi , ) line 35 hides loop cost functions involving xi ). totaltime complexity thus O(er2 dr ).Regarding space, used winf , wsup data-structures. space complexitythus O(n + er).Note exploiting information last supports AC2001 (Bessiere & Regin,2001) reduce worst-case time complexity minimum cost costfunction must recomputed scratch time domain reducedlast support lost (Larrosa, 2002). However, using last supports helps practicereduce mean computation time done implementation.Compared AC*, enforced O(n2 d3 ) time O(ed) space binaryWCN, BAC enforced times faster, space complexity becomes independentrequirement problems large domains.Another interesting difference AC* BAC confluent boundsconsistency is. Considering AC*, known may exist several different AC*closures possibly different associated lower bounds w (Cooper & Schiex, 2004). Notealthough OSAC (Cooper et al., 2007) able find optimal w (at much higher599fiZytnicki, Gaspin, de Givry & SchiexAlgorithm 2: Algorithm enforcing BAC.111214151618202223242526272829303233353637383940414243Procedure BAC(X , D, W, k)QX ;winf () 0 ; wsup () 0 ; inf (, ) 0 ; sup (, ) 0 ;(Q 6= )xj pop(Q) ;foreach wS W, xjforeach ximintS ,inf(xi )tS wS (tS ) ;winf (xi ) winf (xi ) inf (xi , wS ) ;inf (xi , wS ) ;PruneInf(xi ) Q Q {xi } ;mintS ,sup(xi )tS wS (tS ) ;wsup (xi ) wsup (xi ) sup (xi , wS ) ;sup (xi , wS ) ;PruneSup(xi ) Q Q {xi } ;Function PruneInf(xi ) : boolean(w winf (xi ) = k)delete inf(xi ) ;winf (xi ) 0 ; inf (xi , ) 0 ;return true;else return false;Function PruneSup(xi ) : boolean(w wsup (xi ) = k)delete sup(xi ) ;wsup (xi ) 0 ; sup (xi , ) 0 ;return true;else return false;600fiBounds Arc Consistency Weighted CSPscomputational cost), still confluent. following property shows BACconfluent.Proposition 3.3 (Confluence) Enforcing BAC given problem always leadsunique WCN.Proof: prove proposition follows. first define set problemscontains problems reached original WCN BACenforcement. Notice that, step BAC enforcement, general case, severaloperations performed specific order imposed. Therefore, set problemsreached step. show set problems lattice structureultimately show closure BAC lower bound lattice,therefore unique, proves property. proof technique usual provingconvergence chaotic iteration collection suitable functions usedcharacterizing CSP local consistency Apt (1999).enforcement BAC, original problem P = hX , D, W, ki iterativelytransformed set different problems equivalent P, obtaineddeleting values violating BAC. problems obtained value removals,belong set 1 (P ) defined by: {hX , , W, ki : D}.define relation, denoted , set 1 (P ):(P1 , P2 ) 21 (P), P1 P2 [1, n], D1 (xi ) D2 (xi )easy see relation defines partial order. Furthermore, pairelements greatest lower bound glb least upper bound lub 1 (P), defined by:(P1 , P2 ) 21 (P),glb(P1 , P2 ) = hX , {D1 (xi ) D2 (xi ) : [1, n]}, W, ki 1 (P)lub(P1 , P2 ) = hX , {D1 (xi ) D2 (xi ) : [1, n]}, W, ki 1 (P)h1 (P), thus complete lattice.BAC filtering works removing values violating BAC properties, transformingoriginal problem succession equivalent problems. transformationdescribed application dedicated functions 1 (P) 1 (P). precisely,two functions variable, one minimum bound inf(xi )domain xi symmetrical one maximum bound. inf(xi ), associatedfunction keeps instance unchanged inf(xi ) satisfies condition Definition 3.1otherwise returns WCN inf(xi ) alone deleted. collectionfunctions defines set functions 1 (P ) 1 (P ) denote FBAC .Obviously, every function f FBAC order preserving:(P1 , P2 ) 21 (P), P1 P2 f (P1 ) f (P2 )application Tarski-Knaster theorem (Tarski, 1955), known everyfunction f FBAC (applied quiescence BAC enforcement) least onefixpoint, set fixed points forms lattice . Moreover, intersection lattices fixed points functions f FBAC , denoted 1 (P), also601fiZytnicki, Gaspin, de Givry & Schiexlattice. 1 (P) empty since problem hX , {, . . . , }, Wi fixpoint everyfiltering function FBAC . 1 (P) exactly set fixed points FBAC .show that, algorithm reaches fixpoint, reaches greatest element1 (P). prove induction successive application elements FBACP yields problems greater element 1 (P) order . Letus consider fixpoint P 1 (P). Initially, algorithm applies P,greatest element 1 (P), thus P P. base case induction. Letus consider problem P1 obtained execution algorithm. have,induction, P P1 . Since order preserving, know that, function fFBAC , f (P ) = P f (P1 ). therefore proves induction.conclude, algorithm terminates, gives maximum element 1 (P).Since proposition 3.2 showed algorithm actually terminates, concludeconfluent.enforcing BAC may reduce domains, never increases lower bound w.important limitation given increase w may generate value deletionspossibly, failure detection. Note even cost function becomes totally assigned,cost corresponding assignment projected w BAC enforcement.simply done maintaining form backward checking simpleWCSP branch-and-bound algorithm (Freuder & Wallace, 1992). go beyond simpleapproach, consider combination BAC another WCSP local consistency which,similarly AC*, requires cost movements enforced avoids modificationunary cost functions keep reasonable space complexity. achieved directlymoving costs w.4. Enhancing BACmany cases, BAC may weak compared AC* situations seemspossible infer decent w value. Consider example following cost function:D(x1 ) D(x2 )ED(x1 ) = D(x2 ) = [1, 10]w12 :(v1 , v2 )7 v1 + v2AC* increase w 2, projecting cost 2 w12 unary constraint w1every value, projecting costs w1 w enforcing NC. However,w = w1 = w2 = 0 k strictly greater 11, BAC remains idle here.however simply improve BAC directly taking account minimum possible costcost function w12 possible assignments given current domains.Definition 4.1 cost function wS -inverse consistent (-IC) iff:tS , wS (tS ) = 0tuple tS called support wS . WCN -IC iff every cost function (exceptw) -IC.Enforcing -IC always done follows: every cost function wS nonempty scope, minimum cost assignment wS given current variable domains602fiBounds Arc Consistency Weighted CSPscomputed. cost assignment subtracted tuple costs wSadded w. creates least one support wS makes cost function-IC. given cost function wS , done Project() function Algorithm 3.order strengthen BAC, natural idea combine -IC. call BACresulting combination BAC -IC. enforce BAC, previous algorithmmodified first adding call Project() function (see line 53 Algorithm 3).Moreover, maintain BAC whenever w modified projection, every variable testedpossible pruning line 66 put back Q case domain change. Notesubtraction applied constraint tuples line 75 done constant timewithout modifying constraint using additional wS data-structure, similardata-structure introduced Cooper Schiex (2004). data-structure keepstrack cost projected wS w. feature makes possibleleave original costs unchanged enforcement local consistency.example, tS , wS (t) refers wS (t) wS , wS (t) denotes originalcost. Note wS , later used confluence proof, precisely containsamount cost moved wS w. whole algorithm describedAlgorithm 3. highlighted black parts different Algorithm 2whereas unchanged parts gray.Proposition 4.2 (Time space complexity) WCN maximum arity rconstraints, enforcing BAC Algorithm 3 enforced O(n2 r2 dr+1 ) timeusing O(n + er) memory space.Proof: Every variable pushed O(d) times Q, thus foreach line 51(resp. line 55) loops O(erd) (resp. O(er2 d)) times. projection line 53 takesO(dr ) time. operation line 57 carried O(dr1 ) time. overall timespent inside PruneInf() function bounded O(ed). Thus overall timespent loop line 51 (resp. line 55) bounded O(er2 dr+1 ) (resp. O(er2 dr )).flag line 66 true w increases, cannot true k times(assuming integer costs). flag true, spend O(n) time check boundsvariables. Thus, time complexity bounded O(min{k, nd} n).sum up, overall time complexity O(er2 dr+1 + min{k, nd} n), boundedO(n2 r2 dr+1 ).space complexity given , winf , wsup wS data-structures sumsO(n + re) WCN arity bounded r.time complexity algorithm enforcing BAC multiplied comparedBAC without -IC. usual trade-off strength local propertytime spent enforce it. However, space complexity still independent d.Moreover, like BAC, BAC confluent.Proposition 4.3 (Confluence) Enforcing BAC given problem always leadsunique WCN.Proof: proof similar proof Proposition 3.3. However,possible cost movements induced projections, BAC transforms original problem Pcomplex ways, allowing either pruning domains (BAC) moving costs cost603fiZytnicki, Gaspin, de Givry & SchiexAlgorithm 3: Algorithm enforcing BAC44454647484951535557585960616263646667686970717273757677Procedure BAC(X , D, W, k)QX ;winf () 0 ; wsup () 0 ; inf (, ) 0 ; sup (, ) 0 ;(Q 6= )xj pop(Q) ;flag false ;foreach wS W, xjProject(wS ) flag true ;foreach ximintS ,inf(xi )tS wS (tS ) ;winf (xi ) winf (xi ) inf (xi , wS ) ;inf (xi , wS ) ;PruneInf(xi ) Q Q {xi } ;mintS ,sup(xi )tS wS (tS ) ;wsup (xi ) wsup (xi ) sup (xi , wS ) ;sup (xi , wS ) ;PruneSup(xi ) Q Q {xi } ;(flag)foreach xi XPruneInf(xi ) Q Q {xi } ;PruneSup(xi ) Q Q {xi } ;Function Project(wS ) : booleanmintS wS (tS ) ;( > 0)w w ;wS () wS () ;return true;else return false;604fiBounds Arc Consistency Weighted CSPsfunctions w. set problems considered needs therefore takeaccount. Instead defined domains, WCN reached BACalso characterized amount cost moved cost function wSw. quantity already denoted wS Section 4, page 603. thereforeconsider set 2 (P) defined by:(hX , , W, ki, {w : w W}) : [1, n], (xi ) D(xi ), w W, w [0, k]define relation 2 (P):wP1 P2 ((w W, w1 2 ) (xi X , D1 (xi ) D2 (xi )))relation reflexive, transitive antisymmetric. first two propertieseasily verified. Suppose (P1 , P2 ) 22 (P) (P1 P2 ) (P2 P1 ).thus (w W, w = w )(xi X , D(xi ) = (xi )). ensures domains,well amounts cost projected cost function, same. Thus,problems antisymmetric.Besides, h2 (P), complete lattice, since:(P1 , P2 ) 22 (P),wglb(P1 , P2 ) = (hX , {D1 (xi ) D2 (xi ) : [1, n]}, W, ki, {max{w1 , 2 } : w W})wlub(P1 , P2 ) = (hX , {D1 (xi ) D2 (xi ) : [1, n]}, W, ki, {min{w1 , 2 } : w W})2 (P).Every enforcement BAC follows application functions set functions FBAC may remove maximum minimum domain bound (same definitionBAC) may project cost cost functions w. given cost functionw W, function keeps instance unchanged minimum w 0possible tuples. Otherwise, > 0, problem returned derived P projectingamount cost w w. functions easily shown order preserving.proof Proposition 3.3, define lattice 2 (P), intersection sets fixed points functions f FBAC . 2 (P) empty, since(hX , {, . . . , }, W, ki, {k, . . . , k}) it. proof proposition 3.3, since Algorithm 3 terminates, conclude algorithm confluent, resultslub(2 (P)).5. Exploiting Cost Function Semantics BACcrisp AC, several classes binary constraints make possible enforce AC significantlyfaster (in O(ed) instead O(ed2 ), shown Van Hentenryck et al., 1992). Similarly,possible exploit semantics cost functions improve time complexityBAC enforcement. proof Proposition 4.2 shows, dominating factorscomplexity comes complexity computing minimum cost functionsprojection lines 53 57 Algorithm 3. Therefore, cost function property605fiZytnicki, Gaspin, de Givry & Schiexmakes computations less costly may lead improvement overall timecomplexity.Proposition 5.1 binary WCN, cost function wij W subintervals Ei D(xi ), Ej D(xj ), minimum wij Ei Ej found timeO(d), time complexity enforcing BAC O(n2 d2 ).Proof: follows directly proof Proposition 4.2. case, complexityprojection line 53 O(d) instead O(d2 ). Thus overall time spentloop line 51 bounded O(ed2 ) overall complexity O(ed2 + n2 d) O(n2 d2 ).Proposition 5.2 binary WCN, cost function wij W subintervals Ei D(xi ), Ej D(xj ), minimum wij Ei Ej foundconstant time, time complexity enforcing BAC O(n2 d).Proof: follows proof Proposition 4.2. case, complexityprojection line 53 O(1) instead O(d2 ). Moreover, operation line 57carried time O(1) instead O(d). Thus, overall time spent loopline 51 bounded O(ed) overall complexity O(ed + n2 d) = O(n2 d).two properties quite straightforward one may wonder nontrivial usage. actually directly exploited generalize results presentedVan Hentenryck et al. (1992) functional, anti-functional monotonic constraints.following sections, show functional, anti-functional semi-convex cost functions (which include monotonic cost functions) indeed benefit O(d) speedupfactor application Proposition 5.1. monotonic cost functions generallyconvex cost function, stronger speedup factor O(d2 ) obtained Proposition 5.2.5.1 Functional Cost Functionsnotion functional constraint extended cost functions follows:Definition 5.3 cost function wij functional w.r.t. xi iff:(vi , vj ) D(xi ) D(xj ), wij (vi , vj ) {0, } [1, k]vi D(xi ), one value vj D(xj ) wij (vi , vj ) = 0.exists, value called functional support vi .assume rest paper functionalsupport computed constant(0 xi = xj= =time. example, cost function wijfunctional. case,1 otherwisefunctional support vi itself. Note k = 1, functional cost functions representfunctional constraints.Proposition 5.4 minimum functional cost function wij w.r.t. xi alwaysfound O(d).606fiBounds Arc Consistency Weighted CSPsProof: every value vi xi , one check functional support vi belongsdomain xj . requires O(d) checks. never case, minimumcost function known . Otherwise, 0. result follows.5.2 Anti-Functional Semi-Convex Cost FunctionsDefinition 5.5 cost function wij anti-functional w.r.t. variable xi iff:(vi , vj ) D(xi ) D(xj ), wij (vi , vj ) {0, } [1, k]vi D(xi ), one value vj D(xj ) wij (vi , vj ) = .exists, value called anti-support vi .(0 xi 6= xj6=cost function wij =example anti-functional cost function.1 otherwisecase, anti-support vi itself. Note k = 1, anti-functional cost functionsrepresent anti-functional constraints.Anti-functional cost functions actually specific case semi-convex cost functions,class cost functions appear example temporal constraint networkspreferences (Khatib, Morris, Morris, & Rossi, 2001).Definition 5.6 Assume domain D(xj ) contained set Dj totally orderedorder <j .function wij semi-convex w.r.t. xi iff [0, k], vi Di , set {vj Dj :wij (vi , vj ) }, called -support vi , defines interval Dj according <j .Semi-convexity relies definition intervals defined totally ordered discreteset denoted Dj , ordered <j . Even may identical, important avoidconfusion order j D(xj ), used define interval domains boundsarc consistency, order <j Dj used define intervals semi-convexity.order guarantee constant time access minimum maximum elements D(xj )according <j (called <j -bounds domain), assume <j =j <j =j 1 .case, <j -bounds domain bounds identical.One simply check anti-functional cost functions indeed semi-convex:case, -support value either whole domain ( = 0), reduced onepoint (0 < ) empty set (otherwise). Another example cost functionwij = x2i x2j semi-convex w.r.t. xi .Proposition 5.7 minimum cost function wij semi-convex w.r.t. onevariables always found O(d).Proof: first show that, wij semi-convex w.r.t. one variables (letsay xi ), value vi xi , cost function wij must minimum one<j -bounds Dj .1. restriction could removed using example doubly-linked list data-structure valuesD(xj ), keeping domain sorted according <j allowing constant time access deletionwould cost linear space cannot afford context BAC.607fiZytnicki, Gaspin, de Givry & SchiexAssume xi set vi . Let b lowest cost reached either two <j -boundsdomain. Since wij semi-convex, {vj Dj : wij (vi , vj ) b } interval,thus every cost wij (vi , vj ) less b every value Dj . Therefore, leastone two <j -bounds minimum cost.order find global minimum wij , restrict <j -boundsdomain xj every value xi . Therefore, 2d costs need checked.Proposition 5.1, concludeCorollary 5.8 binary WCN, cost functions functional, anti-functionalsemi-convex, time complexity enforcing BAC O(n2 d2 ) only.5.3 Monotonic Convex Cost FunctionsDefinition 5.9 Assume domain D(xi ) (resp. D(xj )) contained set Di (resp.Dj ) totally ordered order <i (resp. <j ).cost function wij monotonic iff:(vi , vi , vj , vj ) Di2 Dj2 , vi vi vj j vj wij (vi , vj ) wij (vi , vj )(0 xi xjexample monotonic cost function.1 otherwiseMonotonic cost functions actually instances larger class functions called convexfunctions.cost functionwij=Definition 5.10 function wij convex iff semi-convex w.r.t. variables.example, wij = xi + xj convex.Proposition 5.11 minimum convex cost function always found constanttime.Proof: Since cost function semi-convex w.r.t. variable, knowproof Proposition 5.7 must reach minimum cost one <j -boundsdomain xj similarly xi . therefore four costs check ordercompute minimum cost.Proposition 5.2, concludeCorollary 5.12 binary WCN, cost functions convex, time complexityenforcing BAC O(n2 d) only.One interesting example convex cost function wij = max{xi xj + cst, 0}.type cost function, efficiently filtered BAC, may occur temporalreasoning problems also used RNA gene localization problem specifyingpreferred distances elements gene.608fiBounds Arc Consistency Weighted CSPs6. Comparison Crisp Bounds ConsistencyPetit et al. (2000) proposed transform WCNs crisp constraint networksextra cost variables. transformation, every cost function reified constraint,applies original cost function scope augmented one extra variable representing assignment cost. reification costs domain variables transforms WCNcrisp CN variables augmented arities. proposed Petit et al.,achieved using meta-constraints, i.e. logical operators applied constraints. Givenrelation WCNs crisp CNs relation BAC boundsconsistency, natural wonder BAC enforcing relates enforcing boundsconsistency reified version WCN.section show BAC precise sense stronger enforcingbounds consistency reified form. natural consequence factdomain filtering BAC based combined cost several cost functions insteadtaking constraint separately bounds consistency. first define reification process precisely. show BAC stronger reified bounds consistencyone example conclude proving never weaker.following example introduces cost reification process.Example 6.1 Consider WCN Figure 2(a). contains two variables x1 x2 , onebinary cost function w12 , two unary cost functions w1 w2 . sake clarity,every variable constraint reified hard model, described Figure 2(b),indexed letter R.First all, model every cost function hard constraint, express assigningb x1 yields cost 1. create new variable x1 CR , cost variable w1 , storescost assignment x1 . Then, replace unary cost function w1 binaryconstraint c1R involves x1 x1 CR , value v1 assigned x1 ,x1 Ctakevaluew(v).unary cost function w2 .11Ridea binary cost function w12 : create new variable x12 CR ,replace w12 ternary constraint c12R , makes sure assignment x1x2 v1 v2 respectively, x12 CR takes value w12 (v1 , v2 ). Finally, global costCconstraint cR states sum cost variables less k added:CCx1 CR + x2 R + x12 R < k. completes description reified cost hard constraintnetwork.define formally reification process WCN.Definition 6.2 Consider WCN P = hX , D, W, ki. Let reify(P) = hXR , DR , WRcrisp CN that:set XR contains one variable xi R every variable xi X , augmentedextra cost variable xS CR per cost function wS W {w}.domains DR are:DR (xiR ) = D(xi ) xiR variables, domain bounds lbiR ubiR ,CC[lbS CR , ubS R ] = [0, k 1] xS R variables.609fiZytnicki, Gaspin, de Givry & Schiexx2 CRx1 CRx2Rx1R00bb1122k=3x1x201b10bx12 CR012cCR(a) small cost functionnetwork(b) reified constraint networkFigure 2: small cost function network reified counterpart.set WR constraints contains:cS R = {(t, wS (t)) : , w wS (t) < k}, scope {xS CR }, every costfunction wS W,PCcCwS W xS R < k), extra constraint makes sureR defined (wsum cost variables strictly less k.simple check problem reify(P) solution iff P solutionsum cost variables solution cost corresponding solution (definedvalues xiR variables) original WCN.Definition 6.3 Let P problem, two local consistency properties. Let (P)problem obtained filtering P . said weaker iff (P)emptiness implies (P) emptiness.said stronger iff weaker , exists problemP (P) empty (P) empty.definition practically significant since emptiness filtered problemevent generates backtracking tree search algorithms used solving CSPWCSP.Example 6.4 Consider WCN defined three variables (x1 , x2 x3 ) two binarycost functions (w12 w13 ). D(x1 ) = {a, b, c, d}, D(x2 ) = D(x3 ) = {a, b, c} (we assumeb c d). costs binary cost functions described Figure 3.Assume k = 2 w = 0.One check associated reified problem already bounds consistentobviously empty. example, support minimum bound domainx1 R w.r.t. c12 R (a, a, 1), support maximum bound (d, a, 1). Supportsmaximum minimum bounds domain x12 CR w.r.t. c12R (b, a, 0) (a, a, 1)respectively. Similarly, one check variable bounds also supportedconstraints involve them.610fiBounds Arc Consistency Weighted CSPs1(x2 ) b 1c 1(x1 )b c0 20 20 21(x3 ) b 1c 1111(x1 )b c2 02 02 0111Figure 3: Two cost matrices.However, original problem BAC since example, value a, minimumbound domain x1 , satisfy BAC property:wXwS W,x1mintS ,atSwS (tS ) < kmeans value deleted BAC filtering. symmetry, appliesmaximum bound x1 ultimately, problem inconsistency provedBAC. shows bounds consistency reified problem cannot strongerBAC original problem.show BAC actually stronger bounds consistency appliedreified WCN. BAC consistency implies non-emptiness (since requiresexistence assignments cost 0 every cost function) start BACconsistent WCN P (therefore empty) prove filtering reified problem reify(P)bounds consistency lead empty problem.Lemma 6.5 Let P BAC consistent binary WCN. filtering reify(P) boundsconsistency produce empty problem.Proof: prove bounds consistency reduce maximum boundsdomains cost variables xS CR non empty set leave domainsunchanged.precisely, final domain xS CR become [0, max{wS (t) : , w wS (t) <k}]. Note interval empty network BAC consistentmeans every cost function assignment cost 0 (by -IC) w < k (or elsebounds domains could supports problem would BAC).prove bounds consistency reduce problem this,simply prove problem defined domain reductions actually boundsconsistent.bounds consistency required properties apply bounds domainsvariables reify(P). Let us consider every type variable reified reduced problem:reified variables xiR . Without loss generality, assume minimum bound lbiRxiR bounds consistent (the symmetrical reasoning applies maximumbound). means would support respect given reified constraint611fiZytnicki, Gaspin, de Givry & SchiexcS R , xi S. However, BAC,wmintS ,lbiRwS (t) < k, lbiR t,wS (t) max{wS (t) : , w wS (t) < k}means lbi R supported w.r.t. cS R .cost variables. minimum bound cost variables always bounds consistentw.r.t. global constraint cCR constraint less inequality.Moreover, since minimum bounds cost variables set 0, alsoconsistent w.r.t. reified constraints, definition -inverse consistency.Consider maximum bound ubS CR cost variable reduced reified problem.Remember defined max{wS (t) : , w wS (t) < k}, w ubS CR < k.minimum bounds cost variables reified problem, 0,CCform support ubS CR w.r.t. global constraint cR . ubS R cannot removedbounds consistency.prove final assertion:Proposition 6.6 BAC stronger bounds consistency.Proof: Lemma 6.5 shows BAC weaker bounds consistency. Then,example 6.4 instance BAC, therefore BAC actually strongerbounds consistency reification.filtering related BAC could achieved reified approach extra shavingprocess variable assigned one domain bounds bound deletedinconsistency found enforcing bounds consistency (Lhomme, 1993).7. Related WorksDefinition 3.1 BAC closely related notion arc consistency counts introduced Freuder Wallace (1992) Max-CSP processing. Max-CSP seensimplified form WCN cost functions generate costs 0 1 (whenassociated constraint violated). definition BAC seen extensionAC counts allowing dealing arbitrary cost functions, including usage w k,applied domain bounds bounds consistency. addition -IC makesBAC powerful.Dealing large domains Max-CSP also considered Range-BasedAlgorithm, designed Max-CSP Petit, Regin, Bessiere (2002). algorithm uses reversible directed arc consistency (DAC) counts exploits factMax-CSP, several successive values domain may DAC counts.algorithm intimately relies fact problem Max-CSP problem, definedset constraints actively uses bounds consistency dedicated propagatorsconstraints Max-CSP. case number different values reachableDAC counters variable bounded degree variable, much612fiBounds Arc Consistency Weighted CSPssmaller domain size. Handling intervals values DAC cost onevalue allows space time savings. arbitrary binary cost functions, translationconstraints could generate d2 constraints single cost function makesscheme totally impractical.Several alternative definition bounds consistency exist crisp CSPs (Choi et al.,2006). extension WCSP based bounds(D) bounds(Z) consistencies (whichequivalent intervals). numerical domains, another possible weaker definitionbounds consistency bounds(R) consistency, obtained relaxation realnumbers. shown Choi et al. bounds(R) consistency checkedpolynomial time constraints whereas bounds(D) bounds(Z) NP-hard (eg.linear equality). use relaxed version WCSP context togetherintentional description cost functions would side effect extending costdomain integer real numbers. extensional algorithmical descriptioninteger cost functions general frequent problems, possibilityconsidered. Since cost comparison fundamental mechanism used pruningWCSP, shift real numbers costs would require safe floating number implementationlocal consistency enforcing algorithms branch bound algorithm.8. Experimental Resultsexperimented bounds arc consistency two benchmarks translated weighted CSPs.first benchmark AI planning scheduling. mission managementbenchmark agile satellites (Verfaillie & Lematre, 2001; de Givry & Jeannin, 2006).maximum domain size temporal variables 201. reasonable size factbinary cost functions allows us compare BAC strong localconsistencies EDAC*. Additionally, benchmark also modeled usingreified version WCN, thus allowing experimental counterpart theoreticalcomparison Section 6.second benchmark comes bioinformatics models problem localization non-coding RNA molecules genomes (Thebault et al., 2006; Zytnicki et al.,2008). aim mostly confirm bounds arc consistency useful practicalreal complex problem huge domains, reach several millions.8.1 Mission Management Benchmark Agile Satellitessolved simplified version described de Givry Jeannin (2006) problemselecting scheduling earth observations agile satellites. complete descriptionproblem given Verfaillie Lematre (2001). satellite pool candidatephotographs take. must select schedule subset passcertain strip territory. satellite take one photograph time (disjunctivescheduling). photograph taken time window dependslocation photographed. Minimal repositioning times required two consecutivephotographs. physical constraints (time windows repositioning times) mustmet, sum revenues selected photographs must maximized.equivalent minimizing rejected revenues non selected photographs.613fiZytnicki, Gaspin, de Givry & SchiexLet N number candidate photographs. define N decision variables representing acquisition starting times candidate photographs. domainvariable defined time window corresponding photograph plus extra domainvalue represents fact photograph selected. proposed de GivryJeannin (2006), create binary hard constraint every pair photographs (resulting complete constraint graph) enforces minimal repositioning timesphotographs selected (represented disjunctive constraint). photograph,unary cost function associates rejected revenue corresponding extra value.order better filtering, moved costs unary cost functions insidebinary hard constraints preprocessing step. allows bounds arc consistencyfiltering exploit revenue information repositioning times jointly, possiblyincreasing w starting times photographs. achieve this, variablexi , unary cost function wi successively combined (using ) binary harddefinedconstraint wij involves xi . yields N 1 new binary cost functions wijwij (t) = wij (t) wi (t[xi ]), hard (+) soft weights. binaryreplace unary cost function w N 1 original binary hardcost functions wijconstraints wij . Notice transformation side effect multiplying softweights N 1. preserve equivalence original problem sincefinite weights multiplied constant (N 1).search procedure exact depth-first branch-and-bound dedicated schedulingproblems, using schedule postpone strategy described de Givry Jeannin (2006)avoids enumeration possible starting time values. initial upper boundprovided (k = +).generated 100 random instances different numbers candidate photographs(N varying 10 30)2 . compared BAC (denoted BAC0 experimentalresults) EDAC* (Heras et al., 2005) (denoted EDAC*). Note FDAC* VAC(applied preprocessing search, addition EDAC*) also testedinstances, improve EDAC* (FDAC* slightly faster EDAC*developed search nodes VAC significantly slower EDAC*, withoutimproving w preprocessing). OSAC practical benchmark (for N = 20,solve linear problem 50, 000 variables 4 million constraints).algorithms using search procedure. implementedtoulbar2 C++ solver3 . Finding minimum cost previously-described binary costfunctions (which convex consider extra domain values rejected photographsseparately), done constant time BAC. done time O(d2 ) EDAC*(d = 201).also report results obtained maintaining bounds consistency reifiedproblem using meta-constraints described de Givry Jeannin (2006), usingclaire/Eclair C++ constraint programming solver (de Givry, Jeannin, Josset, Mattioli,Museux, & Saveant, 2002) developed THALES (denoted B-consistency).results presented Figure 4, using log-scale. results obtained3 GHz Intel Xeon 4 GB RAM. Figure 4 shows mean CPU time secondsmean number backtracks performed search procedure find optimum2. instances available http://www.inra.fr/mia/ftp/T/bep/.3. See http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro.614fiBounds Arc Consistency Weighted CSPsSatellite benchmark10000EDAC*B consistencyBAC0Cpu time seconds10001001010.10.010.0011e-0410152025Number candidate photographs30Satellite benchmark1e+08B consistencyBAC0EDAC*Number backtracks1e+071e+06100000100001000100101015202530Number candidate photographsFigure 4: Comparing various local consistencies satellite benchmark. Cpu-time (top)number backtracks (bottom) given.615fiZytnicki, Gaspin, de Givry & Schiexprove optimality problem size increases. legends, algorithms sortedincreasing efficiency.analysis experimental results shows BAC 35 times fasterEDAC* 25% backtracks EDAC* (for N = 30, backtrack results reported EDAC* solve instance within time limit 6 hours).shows bounds arc consistency prune almost many search nodes strongerlocal consistency much less time temporal reasoning problems semantic cost functions exploited, explained Section 5. second fastestapproach bounds consistency reified representation least 2.3 worseBAC terms speed number backtracks N 25. practical confirmation comparison Section 6. reified approach used boundsconsistency introduces Boolean decision variables representing photograph selectionuses criteria defined linear function variables. Contrarily BAC, boundsconsistency definition unable reason simultaneously combination severalconstraints prune starting times.8.2 Non-coding RNA Gene Localizationnon-coding RNA (ncRNA) gene functional molecule composed smaller molecules,called nucleotides, linked together covalent bonds. four types nucleotides, commonly identified single letter: A, U, G C. Thus, RNArepresented word built four letters. sequence defines calledprimary structure RNA molecule.RNA molecules ability fold back developing interactionsnucleotides, forming pairs. frequently interacting pairs are: G interactsC, U interacts A. sequence interactions forms structurecalled helix. Helices fundamental structural element ncRNA genesbasis complex structures. set interactions often displayed graphvertices represent nucleotides edges represent either covalent bonds linking successive nucleotides (represented plain lines Figure 5) interacting nucleotide pairs(represented dotted lines). representation usually called molecules secondarystructure. See graph helix Figure 5(a).set ncRNAs common biological function called family.signature gene family set conserved elements either sequencesecondary structure. expressed collection properties must satisfiedset regions occurring sequence. Given signature family, probleminterested involves searching new members gene family existing genomes,members fact set regions appearing genome satisfysignature properties. Genomic sequences long texts composed nucleotides.thousand nucleotides long simplest organisms several hundredmillion nucleotides complex ones. problem searching occurrencegene signature genomic sequence NP-complete complex combinations helixstructures (Vialette, 2004).order find ncRNAs, build weighted constraint network scansgenome, detects regions genome signature elements present616fiBounds Arc Consistency Weighted CSPsUCG C G U CGU C G C GhelixCG UG C UA U Uxixjloop(cost: 1)(b)pattern(xi , xj , ACGUA)cost function.(a) helix loop.GxiU CU GGxlcostxjG Ckxk(cost: 2)(c) helix(xi , xj , xk , xl , 6) costfunction.0xj xid1 d2d3d4(d)costprofilespacer(xi , xj , d1 , d2 , d3 , d4 )costtion.func-Figure 5: Examples signature elements cost functions.correctly positioned. variables positions signature elementssequence. size domains size genomic sequence. Cost functionsenforce presence signature elements positions taken variablesinvolved. Examples cost functions given Figure 5.pattern(xi , xj , p) function states fixed word p, given parameter,found positions indicated variables xi xj . cost givenfunction edit distance word found xi :xj word p(see cost function pattern word ACGUA Figure 5(b)).helix(xi , xj , xk , xl , m) function states nucleotides positions xixj able bind nucleotides xk xl . Parameterspecifies minimum helix length. cost given number mismatchesnucleotides left unmatched (see helix function 5 interacting nucleotide pairsFigure 5(c)).Finally, function, spacer(xi , xj , d1 , d2 , d3 , d4 ) specifies favorite range distancespositions xi xj using trapezoidal cost function shown Figure 5(d).See work Zytnicki et al. (2008) complete description cost functions.sheer domain size, given complex pattern matching orientedcost functions specific property could speedup filtering, BAC aloneused filtering cost functions (Zytnicki et al., 2008). exception617fiZytnicki, Gaspin, de Givry & Schiexpiecewise linear spacer cost function: minimum computed constant timeBAC enforcement. resulting C++ solver called DARN!4 .Size# solutionsAC*Time# backtracksBACTime (sec.)# backtracks10k3250k33100k33500k331M414.9M2741hour 25min.9344 hours101----0.016930.0361010.0641020.251370.502232.581159Table 1: Searching solutions tRNA motif Escherichia coli genome.typical benchmark ncRNA localization problem transfer RNA (tRNA)localization. tRNA signature (Gautheret, Major, & Cedergren, 1990) modelled22 variables, 3 nucleotide words, 4 helices, 7 spacers. DARN! searchedsolutions cost strictly lower maximum cost k = 3. illustrateabsolute necessity using bounds arc consistency problem, compared boundsarc consistency enforcement AC* (Larrosa, 2002) sub-sequences genomeEscherichia coli, 4.9 million nucleotides long. identical spacecomplexity defined implemented non-binary costfunctions (helix quaternarycost function), DAC, FDAC EDAC tested(see work Sanchez et al., 2008, however extension FDAC ternary costfunctions).results displayed Table 1. different beginning sub-sequences complete sequence, report size sub-sequence signature searched(10k sequence 10,000 nucleotides), well number solutions found.also show number backtracks time spent 3 GHz Intel Xeon2 GB. - means instance could solved due memory reasons, despitememory optimizations. BAC solved complete sequence less 3 seconds. BACapproximately 300, 000 (resp. 4, 400, 000) times faster AC* 10k (resp. 50k)sub-sequence. results genomes ncRNA signatures foundwork Zytnicki et al. (2008).reason superiority BAC AC* twofold. First, AC* needs storeunary costs every variable projects costs binary cost functions unarycost functions. Thus, space complexity AC* least O(nd). large domains(in experiments, greater 100,000 values), computer cannot allocate sufficientmemory program aborted. kind projection, BAC needsstore costs bounds domains, leading space complexity O(n).Second, BAC care interior values focuses boundsdomains only. hand, AC* projects binary costs interior values,4. DARN!,severalgenomichttp://carlit.toulouse.inra.fr/Darn/.sequences618familysignaturesavailablefiBounds Arc Consistency Weighted CSPstakes lot time, remove values detect inconsistencies earlier.However, Table 1 shows number backtracks performed AC* BACsame. explained follows. Due nature cost functions usedproblems, supports bounds domains variables usuallybounds variables. Thus, removing values inside domains,AC* does, help removing bounds variables. consequence, boundsfounds BAC found AC*. explains enforcing AC*generally lead new domain wipe compared BAC, finding supportinside bounds domains useless.Notice spacer cost functions dramatically reduce size domains.single variable assigned, domain sizes dramatically reduced,instance becomes quickly tractable. Moreover, helix constraint extra knowledgemaximum distance djk variables xj xk (see Fig. 5(c)) boundstime complexity finding minimum cost w.r.t. djk length sequence.9. Conclusions Future Workpresented new local consistencies weighted CSPs dedicated large domains well algorithms enforce properties. first local consistency, BAC,time complexity easily reduced semantics cost functionappropriate. possible enhancement property, -IC, also presented.experiments showed maintaining bounds arc consistency much better AC*problems large domains, ncRNA localization scheduling Earth observation satellites. due fact AC* cannot handle problems largedomains, especially high memory complexity, also BAC behavesparticularly well specific classes cost functions.Similarly bounds consistency, implemented almost state-of-the-artCSP solvers, new local property implemented open source toulbar2WCSP solver.5BAC, BAC -inverse consistency allowed us transfer bounds consistency CSPweighted CSP, including improved propagation specific classes binary cost functions.implementation RNA gene finding also able filter non-binary constraints.would therefore quite natural try define efficient algorithms enforcing BAC,BAC -inverse consistency specific cost functions arbitrary arity softglobal constraints derived All-Diff, GCC regular (Regin, 1994; Van Hoeve, Pesant,& Rousseau, 2006). line research recently explored Lee Leung(2009).Finally, another interesting extension work would better exploit connection BAC bounds consistency exploiting idea Virtual Arc Consistencyintroduced Cooper et al. (2008). connection established Virtual AC crispCNs WCNs much finer grained reification approach considered Petitet al. (2000) could provide strong practical theoretical results.5. Available http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro.619fiZytnicki, Gaspin, de Givry & SchiexReferencesApt, K. (1999). essence constraint propagation. Theoretical computer science, 221 (12), 179210.Bessiere, C., & Regin, J.-C. (2001). Refining basic constraint propagation algorithm.Proc. IJCAI01, pp. 309315.Chellappa, R., & Jain, A. (1993). Markov Random Fields: Theory Applications. Academics Press.Choi, C. W., Harvey, W., Lee, J. H. M., & Stuckey, P. J. (2006). Finite domain boundsconsistency revisited. Proc. Australian Conference Artificial Intelligence, pp.4958.Cooper, M., & Schiex, T. (2004). Arc consistency soft constraints. Artificial Intelligence,154, 199227.Cooper, M. C., de Givry, S., Sanchez, M., Schiex, T., & Zytnicki, M. (2008). Virtual arcconsistency weighted CSP.. Proc. AAAI2008.Cooper, M. C., de Givry, S., & Schiex, T. (2007). Optimal soft arc consistency. Proc.IJCAI07, pp. 6873.de Givry, S., & Jeannin, L. (2006). unified framework partial hybrid searchmethods constraint programming. Computer & Operations Research, 33 (10), 28052833.de Givry, S., Jeannin, L., Josset, F., Mattioli, J., Museux, N., & Saveant, P. (2002).THALES constraint programming framework hard soft real-time applications.PLANET Newsletter, Issue 5 ISSN 1610-0212, pages 5-7.Freuder, E., & Wallace, R. (1992). Partial constraint satisfaction. Artificial Intelligence,58, 2170.Gautheret, D., Major, F., & Cedergren, R. (1990). Pattern searching/alignment RNAprimary secondary structures: effective descriptor tRNA. Comp. Appl.Biosc., 6, 325331.Heras, F., Larrosa, J., de Givry, S., & Zytnicki, M. (2005). Existential arc consistency:Getting closer full arc consistency weighted CSPs. Proc. IJCAI05, pp.8489.Khatib, L., Morris, P., Morris, R., & Rossi, F. (2001). Temporal constraint reasoningpreferences. Proc. IJCAI01, pp. 322327.Larrosa, J. (2002). Node arc consistency weighted CSP. Proc. AAAI02, pp.4853.Larrosa, J., & Schiex, T. (2004). Solving weighted CSP maintaining arc-consistency.Artificial Intelligence, 159 (1-2), 126.Lee, J., & Leung, K. (2009). Towards Efficient Consistency Enforcement Global Constraints Weighted Constraint Satisfaction. Proc. IJCAI09.Lhomme, O. (1993). Consistency techniques numeric CSPs. Proc. IJCAI93, pp.232238.620fiBounds Arc Consistency Weighted CSPsMeseguer, P., Rossi, F., & Schiex, T. (2006). Soft constraints. Rossi, F., van Beek, P.,& Walsh, T. (Eds.), Handbook constraint programming, Foundations ArtificialIntelligence, chap. 9, pp. 281328. Elsevier.Petit, T., Regin, J.-C., & Bessiere, C. (2000). Meta-constraints violations constrained problems. Proc. ICTAI00, pp. 358365.Petit, T., Regin, J. C., & Bessiere, C. (2002). Range-based algorithm Max-CSP.Proc. CP02, pp. 280294.Regin, J.-C. (1994). filtering algorithm constraints difference CSPs. Proc.AAAI94, pp. 362367.Sanchez, M., de Givry, S., & Schiex, T. (2008). Mendelian error detection complexpedigrees using weighted constraint satisfaction techniques. Constraints, 13 (1-2), 130154.Sandholm, T. (1999). Algorithm Optimal Winner Determination CombinatorialAuctions. Proc. IJCAI99, pp. 542547.Schiex, T. (2000). Arc consistency soft constraints. Proc. CP00, pp. 411424.Tarski, A. (1955). lattice-theoretical fixpoint theorem applications. Pacific JournalMathematics, 5 (2), 285309.Thebault, P., de Givry, S., Schiex, T., & Gaspin, C. (2006). Searching RNA motifsintermolecular contacts constraint networks. Bioinformatics, 22 (17), 207480.Van Hentenryck, P., Deville, Y., & Teng, C.-M. (1992). generic arc-consistency algorithmspecializations. Artificial Intelligence, 57 (23), 291321.Van Hoeve, W., Pesant, G., & Rousseau, L. (2006). global warming: Flow-based softglobal constraints. Journal Heuristics, 12 (4), 347373.Verfaillie, G., & Lematre, M. (2001). Selecting scheduling observations agile satellites: lessons constraint reasoning community point view. Proc.CP01, pp. 670684.Vialette, S. (2004). computational complexity 2-interval pattern matching problems. Theoretical Computer Science, 312 (2-3), 223249.Zytnicki, M., Gaspin, C., & Schiex, T. (2008). DARN! soft constraint solver RNAmotif localization. Constraints, 13 (1-2), 91109.621fiJournal Artificial Intelligence Research 35 (2009) 717-773Submitted 12/08; published 08/09Complexity Circumscription Description LogicPiero A. Bonattibonatti@na.infn.itSection Computer Science, Department PhysicsUniversity Naples Federico II, ItalyCarsten Lutzclu@informatik.uni-bremen.deDepartment Mathematics Computer ScienceUniversity Bremen, GermanyFrank Wolterwolter@liverpool.ac.ukDepartment Computer ScienceUniversity Liverpool, UKAbstractfragments first-order logic, Description logics (DLs) provide nonmonotonicfeatures defeasible inheritance default rules. Since many applications wouldbenefit availability features, several families nonmonotonic DLsdeveloped mostly based default logic autoepistemic logic.paper, consider circumscription interesting alternative approach nonmonotonicDLs that, particular, supports defeasible inheritance natural way. study DLsextended circumscription different language restrictions differentconstraints sets minimized, fixed, varying predicates, pinpoint exactcomputational complexity reasoning DLs ranging ALC ALCIO ALCQO.minimized fixed predicates include concept names role names,reasoning complete NExpNP . becomes complete NPNExp numberminimized fixed predicates bounded constant. roles minimizedfixed, complexity ranges NExpNP undecidability.1. IntroductionEarly knowledge representation (KR) formalisms semantic networks framesincluded wealth features order provide much expressive power possible(Quillian, 1968; Minsky, 1975). particular, formalisms usually admitted structured representation classes objects similar modern description logics (DLs),also mechanisms defeasible inheritance, default rules, features nowadays studied area nonmonotonic logics (NMLs). KR theory developedfurther, all-embracing approaches largely given favour specializedones due unfavourable computational properties problems semantics.process caused DLs NMLs develop two independent subfields. Consequently,modern description logics SHIQ lack expressive power represent defeasibleinheritance nonmonotonic features (Horrocks, Sattler, & Tobies, 2000).Despite (or due to) development, continuous interest(re)integration nonmonotonic features description logics. recent years, advent several new applications DLs increased interest even further. brieflydiscuss two them. First, DLs become popular tool formalization biomedc2009AI Access Foundation. rights reserved.fiBonatti, Lutz, & Wolterical ontologies GALEN (Rector & Horrocks, 1997) SNOMED (Cote, Rothwell,Palotay, Beckett, & Brochu, 1993). argued example Rector (2004) Stevenset al. (2005), important ontologies represent exceptions formhumans, heart usually located left-hand side body; humanssitus inversus, heart located right-hand side body. Modellingsituations requires defeasible inheritance, i.e., properties transfer instances classdefault, explicitly overridden special cases (McCarthy, 1986; Horty, 1994;Brewka, 1994; Baader & Hollunder, 1995b). second application use DLs security policy languages (Uszok, Bradshaw, Johnson, Jeffers, Tate, Dalton, & Aitken, 2004;Kagal, Finin, & Joshi, 2003; Tonti, Bradshaw, Jeffers, Montanari, Suri, & Uszok, 2003).formalizing access control policies, one must deal situation given requestneither explicitly allowed explicitly denied. Then, default decision takenopen closed policies, authorizations respectively granted denieddefault (Bonatti & Samarati, 2003). Moreover, policies often formulated incrementally, i.e., start general authorizations large classes subjects, objects, actions,progressively refine introducing exceptions specific subclasses.approach clearly incarnation defeasible inheritance.applications illustrate integrating nonmonotonic features DLsworthwhile, actual engineering computationally well-behaved nonmonotonic DLprovides sufficient expressive power turns non-trivial task. particular,combinations DLs nonmonotonic logics typically involve subtle interactionstwo component logics easily leads undecidability. appearsone optimal way circumnavigate difficulties, thus many different combinationsDLs nonmonotonic logics proposed literature, individualstrengths limitations (we provide survey Section 7). However, strikinggap: almost existing approaches based default logic autoepistemic logic,circumscription received little attention connection DLs, computational properties DLs circumscription almost completely unknown.surprising since circumscription known one weakest formsnonmonotonic reasoningsee work Janhunen (1999) one recent surveys, paper Bonatti Eiter (1996) expressiveness analysis termsqueries. Therefore, natural idea use circumscription defining computationallywell-behaved, yet expressive DL nonmonotonic features.paper, study circumscription (McCarthy, 1980) alternative approachdefining nonmonotonic DLs. particular, define family DLs circumscription enable natural modelling defeasible inheritance. general approachgeneralize standard DL knowledge bases circumscribed knowledge bases (cKBs) which,additionally TBox representing terminological knowledge ABox representing knowledge individuals, equipped circumscription pattern.pattern lists predicates (i.e., concept role names) minimized sense that,admitted models cKB, extension listed predicates minimalw.r.t. set inclusion. Following McCarthy (1986), minimized predicates usedabnormality predicates identify instances typical class. Circumscription patterns require predicates fixed minimization, allowvary freely (McCarthy, 1986). main feature DLs family718fiThe Complexity Circumscription DLscome built-in mechanism defeasible inheritance: default, propertiesclass (humans first example above) transfer subclass (humans situsinversus), exceptions specified based priority mechanism. well-knowndefeasible inheritance priority cannot modularly encoded pure defaultautoepistemic logic (Horty, 1994), workarounds explicit listing exceptionslead serious maintainability problems. Circumscription lends naturally priorities,based circumscription patterns express preferences minimized predicates terms partial ordering. argued Baader Hollunder (1995b),approach well-suited ensure smooth interplay defeasible inheritance DLsubsumption, thus prefer traditional prioritized circumscription.achieve decidability, nonmonotonic DLs usually adopt suitable restrictionsexpressive power DL component, non-monotonic features, interaction. case default logic autoepistemic logic, typical restriction concernsdifferent treatment individuals explicitly denoted constant,not. goes back reasoning first-order default logic (Reiter, 1980)autoepistemic logic (Moore, 1985), also involve tricky technical issues relateddenotation individuals. make reasoning decidable DLs based default logic, default rules applied individuals denoted constants occur explicitlyknowledge base (Baader & Hollunder, 1995a), unnamed individuals.consequence, named unnamed individuals treated uniformly. approachesbased autoepistemic logic (Donini, Lenzerini, Nardi, Nutt, & Schaerf, 1998; Donini,Nardi, & Rosati, 1997, 2002), alternative solution restrict domain fixed, denumerable set constants. approach overcomes different treatment namedunnamed individuals since individuals named. flipside ad-hoc encodingsrequired domain finite unique name assumption enforced, i.e.,different constants allowed denote individual. respect, DLscircumscription pose difficulty all, named individuals treated exactlyway unnamed ones without assumptions domain.time, able base nonmonotonic DLs rather expressive DL componentsALCIO ALCQO without losing decidability. However, cannot withoutrestrictions either: allow fix minimize concept names circumscriptionrequire role names vary.main contribution paper detailed analysis computational propertiesreasoning cKBs. show that, expressive DLs ALCIO ALCQO, instancechecking, satisfiability subsumption decidable concept-circumscribed KBsconcept names (and role names) minimized fixed. precisely,prove reasoning problems NExpNP -complete, lower bound appliesalready concept-circumscribed KBs ALC empty preference relation withoutfixed concept names (1) empty TBox (2) empty ABox acyclic TBox. addition,show constant bound imposed number minimized fixed conceptnames, complexity drops NPNExp .situation completely different role names minimized fixed. First,complexity reasoning cKBs formulated ALC single fixed role name, emptyTBox, empty preference relation, minimized role names turns outsideanalytic hierarchy, thus highly undecidable. result shown reduction719fiBonatti, Lutz, & WolterNameSyntaxrinverse rolenominalnegationconjunctiondisjunctionat-least restrictionat-most restriction{a}CC uDC tD(> n r C)(6 n r C)Semantics(rI )` = {(d, e) | (e, d) rI }{aI }\ CC DIC DI{d | #{e C | (d, e) rI } n}{d | #{e C | (d, e) rI } n}Figure 1: Syntax semantics ALCQIO.satisfiability monadic second-order logic (MSO) binary predicates arbitrary(i.e., necessarily tree-shaped) structures. reduction apply cKBsrole names minimized, fixed. Surprisingly, find casereasoning empty TBoxes becomes decidable (and NExpNP -complete) DLsALC ALCQO, ALCI extensions undecidable.logics, however, adding acyclic TBoxes leads undecidability. reader findtable summarising complexity results Section 7.interesting note results somewhat unusual perspectiveNMLs. First, arity predicates impact decidability: fixing concept names(unary predicates) impair decidability, whereas fixing single role name (binarypredicate) leads strong undecidability result. Second, number predicatesminimized fixed (bounded vs. unbounded) affects computational complexityreasoning. Although (as note passing) similar effect observed propositionallogic circumscription, has, best knowledge, never explicitlynoted.paper organized follows. next section introduce syntax, semantics,reasoning problems circumscribed KBs, provide examples. Section 3provides basic results polynomial simulation fixed concepts meansminimized concepts, polynomial reduction reasoning general TBoxes reasoning acyclic TBoxes, polynomial reduction simultaneous satisfiabilitymultiple cKBs standard satisfiability. Then, Section 4 proves decidability complexity results concept-circumscribed knowledge bases. Fixed minimized rolesconsidered Sections 5 6, respectively. Section 7 discusses related work, Section 8concludes paper summarizing main results pointing interestingdirections research. improve readability, many proof details deferredappendix. paper extended version article Bonatti, Lutz, Wolter(2006).2. Description Logics CircumscriptionDLs, concepts inductively defined help set constructors, startingset NC concept names, set NR role names, (possibly) set NI individual720fiThe Complexity Circumscription DLsnames (all countably infinite). use term predicates refer elements NC NR .concepts expressive DL ALCQIO formed using constructors shownFigure 1.There, inverse role constructor role constructor, whereas remaining sixconstructors concept constructors. Figure 1 throughout paper, use #Sdenote cardinality set S, b denote individual names, r denoteroles (i.e., role names inverses thereof), A, B denote concept names, C,denote (possibly complex) concepts. usual, use > abbreviation arbitrary(but fixed) propositional tautology, >, usual Boolean abbreviations,r.C (existential restriction) (> 1 r C), r.C (universal restriction) (6 0 r C).paper, concerned ALCQIO itself, severalfragments.1 basic fragment allows negation, conjunction, disjunction,universal existential restrictions, called ALC. availability additionalconstructors indicated concatenation corresponding letter: Q stands number restrictions, stands inverse roles, nominals. explains nameALCQIO, also allows us refer fragments ALCIO, ALCQO, ALCQI.semantics ALCQIO-concepts defined terms interpretation =( , ). domain non-empty set individuals interpretation function maps concept name NC subset AI , role name r NRbinary relation rI , individual name NI individual aI .extension inverse roles arbitrary concepts inductively defined shownthird column Figure 1. interpretation called model concept C C 6= .model C, also say C satisfied I.(general) TBox finite set concept implications (CIs) C v C.concepts. usual, use C = abbreviation C v v C. ABoxfinite set concept assertions C(a) role assertions r(a, b), a, b individualnames, r role name, C concept. interpretation satisfies (i) CI C vC DI , (ii) assertion C(a) aI C , (iii) assertion r(a, b) (aI , bI ) rI .Then, model TBox satisfies implications , model ABoxsatisfies assertions A.important class TBoxes acyclic TBoxes: call TBox acyclic set.concept definitions = C, concept name following two conditionshold:concept name occurs left hand side definition ;.relation , defined setting B iff = C B occurs C,well-founded.1. reason consider ALCQIO paper finite modelproperty; i.e., satisfiable concepts satisfiable finite models. proofscomplexity upper bounds assume finite model property and, therefore, work ALCQIO. Investigating circumscription description logics without finite model property remains interestingopen problem.721fiBonatti, Lutz, & Wolter2.1 Circumscription, Varying Predicates, Partial Priority OrderingCircumscription logical approach suitable modelling normally typicallyholds, thus admits modeling defeasible inheritance (McCarthy, 1986; Lifschitz,1993). idea define, standard first-order language, domain knowledgeso-called abnormality predicates identify instances class violate normaltypical properties class. capture intuition abnormality exceptional,inference based set models resulting theory classical logic,rather restricted models extension abnormality predicatesminimal respect set inclusion. Intuitively, means reasoning basedmodels normal possible. Since models classical modelsgiven knowledge base, classical first-order inferences valid circumscription (butadditional inferences may become possible).Since description logics fragments first-order logic, circumscription readilyapplied. Using ALC syntax, assert mammals normally inhabitate land,whales live land:Mammal v habitat.Land AbMammalWhale v Mammal u habitat.Landupper inclusion states mammal inhabitating land abnormal mammal,thus satisfying abnormality predicate AbMammal . applying circumscriptionTBox, thus consider models extension AbMammal minimal.However, one way defining preferred models nonminimized predicate treated two different ways minimization: mayeither fix extension let vary freely.Intuitively, fixed predicates retain classical semantics varying predicates mayaffected minimization. concrete example, consider TBoxassume non-minimized predicates fixed. derive followingsubsumptions:Whale v AbMammal().AbMammal = Mammal u habitat.Land.Here, Whale v AbMammal AbMammal w Mammal u habitat.Land classical consequencesTBox. minimization AbMammal adds inclusion AbMammal v Mammal uhabitat.Land.analyze fixed predicates, suppose explicitly introduce concretemammal whale adding ABox assertionMammal u Whale(flipper)might expect derive habitat.Land(flipper), actuallycase. see this, observe classical model knowledge base falsifieshabitat.Land(flipper); extension fixed predicates habitat Landaffected minimization, habitat.Land(flipper) must still false minimization.argument applied negation habitat.Land(flipper),thus also derivable. seen sentence uses fixed722fiThe Complexity Circumscription DLspredicates, consequence circumscribed knowledge base if, if,classical consequence knowledge base.assume let role habitat concept name Land vary freely, fixMammal Whale. view concept inclusion Mammal original TBox,setup may interpreted expressing unlikely mammallive land: willing modify extension habitat Land order avoidabnormality. obtain additional consequence, namely:.Whale = AbMammal .()see indeed consequence note that, minimization, (i) makeLand non-empty (ii) mammal whale, ensureAbMammal linking via habitat generated instance Land.2 Intuitively,equality () seen reflecting unlikeliness abnormal: mammalabnormal reason, reason captured knowledgebase whale.Let us return assertion Mammal u Whale(flipper). applying classicalreasoning () (), derive Whale w Mammal u habitat.Land (i.e., whalesmammals live land). Thus derive expected conclusion habitat.Land(flipper). summary, turning habitat Land varyingpredicates, obtained natural modelling habitat attributemammals forced default value.Driving example further, might consider whales abnormal degreebelieve exist unless evidence do. should,additionally, let Whale vary freely. result () () still derived,additionally obtain consequence..Whale = AbMammal = .use ABox add evidence whales exist, e.g. assertionWhale(mobydick). expected, result change..Whale = AbMammal = {mobydick}.Evidence existence another, anonymous whale could generated addingABox assertion Male(mobydick) TBox statementWhale v mother.(Whale u Male)mother Male varying freely. knowledge base classically entails existtwo whales, satisfying Male Male, respectively. former denoted mobydick,latter denoted ABox individual (which corresponds first-orderconstant). minimization, Whale contains exactly two individuals.general, appropriate combination fixed varying predicates dependsapplication. Therefore, adhere standard circumscription give users freedomchoose predicates minimized, fixed, varying.2. Indeed, reason let Land vary: ensure made non-emptyminimization.723fiBonatti, Lutz, & Wolteranother example, consider sentences: humans, heart usually locatedleft-hand side body; humans situs inversus, heart locatedright-hand side body. axiomatized follows:Human v heart.has position.{Left} AbHumanSitus Inversus v heart.has position.{Right}heart.has position.{Left} u heart.has position.{Right} v .predicate AbHuman represents abnormal humans minimized. humanssitus inversus restricted individuals explicitly declaredproperty, analogy previous example roles specifying heartposition class exceptional individuals Situs Inversus allowed varyHuman fixed retain classical semantics. result absenceaxioms, AbHuman Situs Inversus empty minimized models.additional axiom friend.Situs Inversus(John) turns AbHuman Situs Inversussingleton set containing anonymous individual (though models, mayJohn himself). example nonclassical consequence, consider:Human u Situs Inversus v heart.has position.{Left} ,is, humans default heart position exceptionexplicitly declared situs inversus.extensively argued (McCarthy, 1986; Horty, 1994; Brewka, 1994; Baader &Hollunder, 1995b) interplay subsumption abnormality predicates addressed nonmonotonic DLs. Consider, example, followingTBox:UserStaffStaffBlacklistedStaffvvvvhasAccessTo.{ConfidentialFile} AbUserUserhasAccessTo.{ConfidentialFile} AbStaffStaff u hasAccessTo.{ConfidentialFile}get models normal possible, first attempt could minimizetwo abnormality predicates AbUser AbStaff parallel. Assume hasAccessTovarying, User, Staff, BlacklistedStaff fixed. Then, result parallelminimization staff members may may access confidential files,equal preference. first case, abnormal users, second case,abnormal staff. However, one may argue first option preferred: sinceStaff v User (but way round), normality information staffspecific normality information users higher priority. effects well-known also propositional/first-order case indeed, circumscriptionsoon introduction extended priorities address issues specificity(McCarthy, 1986).formalism, users specify priorities minimized predicates. Typically,priorities reflect subsumption hierarchy (as computed w.r.t. classmodels). Since subsumption hierarchy general partial order, prioritiesminimized predicates may form partial order, too. approach analogous partially724fiThe Complexity Circumscription DLsordering priorities default rules, proposed Brewka (1994). generalstandard prioritized circumscription, assumes total ordering (McCarthy, 1986;Lifschitz, 1985), special case nested circumscription (Lifschitz, 1995).2.2 Circumscribed Knowledge Basesdefine DLs circumscription, start introducing circumscription patterns.describe individual predicates treated minimization.Definition 1 (Circumscription pattern, <CP ) circumscription pattern tuple CPform (, M, F, V ), strict partial order , , F , Vmutually disjoint subsets NC NR , minimized, fixed, varying predicates,respectively. , denote reflexive closure . Define preference relation <CPinterpretations setting <CP J iff following conditions hold:1. = J and, NI , aI = aJ ,2. p F , pI = pJ ,3. p , pI 6 pJ exists q , q p, q q J ,4. exists p pI pJ q q p, q = q J .F NC (i.e., minimized fixed predicates concepts) call(, M, F, V ) concept circumscription pattern.4use term concept circumscription concept circumscription patterns admitted. Based circumscription patterns, define circumscribed DL knowledgebases models.Definition 2 (Circumscribed KB) circumscribed knowledge base (cKB) expression CircCP (T , A), TBox, ABox, CP = (, M, F, V ) circumscription pattern M, F, V partition predicates used A. interpretationmodel CircCP (T , A) model exists model 00 <CP I.cKB CircCP (T , A) called concept-circumscribed KB CP concept circumscription pattern.4Note partially ordered circumscription becomes standard parallel circumscriptionempty relation used .main reasoning tasks (non-circumscribed) KBs satisfiability conceptsw.r.t. KBs, subsumption w.r.t. KBs, instance checking w.r.t. KBs. reasoningtasks fundamental circumscribed KBs well. provide precise definitionstasks. Throughout following section, DL denotes set DLsintroduced previous section; i.e., ALC, ALCI, ALCO, ALCQ, ALCQI, ALCIO,ALCQO, ALCQIO.Definition 3 (Reasoning tasks)725fiBonatti, Lutz, & Wolterconcept C satisfiable w.r.t. cKB CircCP (T , A) model CircCP (T , A)satisfies C 6= . Let L DL. satisfiability problem w.r.t. cKBs L definedfollows: given concept C L cKB CircCP (T , A) L, decide whether Csatisfiable w.r.t. CircCP (T , A).concept C subsumed concept w.r.t. cKB CircCP (T , A), symbolsCircCP (T , A) |= C v D, C DI models CircCP (T , A). Let L DL.subsumption problem w.r.t. cKBs L defined follows: given concepts CL cKB CircCP (T , A) L, decide whether CircCP (T , A) |= C v D.individual name instance concept C w.r.t. cKB CircCP (T , A),symbols CircCP (T , A) |= C(a), aI C models CircCP (T , A). LetL DL. instance problem w.r.t. cKBs L defined follows: givenconcept C L, individual name a, cKB CircCP (T , A) L, decide whetherCircCP (T , A) |= C(a).4reasoning problems polynomially reduced one another: first, C satisfiablew.r.t. CircCP (T , A) iff CircCP (T , A) 6|= C v , CircCP (T , A) |= C v iff C usatisfiable w.r.t. CircCP (T , A). second, C satisfiable w.r.t. CircCP (T , A) iffCircCP (T , A) 6|= C(a), individual name appearing A; conversely,CircCP (T , A) |= C(a) iff AuC satisfiable w.r.t. CircCP0 (T , A{A(a)}),concept name occurring A, CP0 obtained CP adding(and leaving is). paper, use satisfiability w.r.t. cKBs basicreasoning problem.3. Basic Reductionspresent three basic reductions reasoning problems circumscribed knowledgebases interesting right and, additionally, useful establishingmain results paper later on. precisely, replay well-known reductionfixed predicates minimized predicates context DLs, reduce reasoning w.r.t. cKBsgeneral TBoxes reasoning w.r.t. cKBs acyclic TBoxes, show that,certain conditions, simultaneous satisfiability w.r.t. collection cKBs reduciblesatisfiability w.r.t. single cKB.3.1 Fixed minimized conceptscircumscription, folklore fixed predicates simulated terms minimizedpredicates, see e.g. de Kleer (1989). case DLs, simulation possibleconcept names. see this, let C0 concept CircCP (T , A) circumscribedKB CP = (, M, F, V ) F0 = {A1 , . . . , Ak } = F NC . Define new patternCP0 = (, 0 , F \ F0 , V )0 = {A1 , . . . , Ak , A01 , . . . , A0k }, A01 , . . . , A0k concept namesoccur C0 , , F , V , , A;.0 = {A0i = Ai | 1 k}.726fiThe Complexity Circumscription DLsdifficult see C0 satisfiable w.r.t. CircCP (T , A) iff satisfiable w.r.t.CircCP0 (T 0 , A). Thus, get following result.Lemma 4 Let L DL. satisfiability w.r.t. (concept-)circumscribed KBs Lpolynomially reduced satisfiability w.r.t. (concept-)circumscribed KBs Lfixed concept names.contrast concept names, fixed role names cannot reduced minimized role namessince Boolean operators roles available standard DLs ALCQIO.proof given Section 6, show that, cases, reasoning minimizedrole names decidable, whereas corresponding reasoning task cKBs fixed rolenames undecidable.reduction clearly relies TBoxes. However, paper sometimeswork circumscribed KBs TBox empty. following lemma, provedappendix, shows cKBs ALC without fixed role names empty TBox,one simulate fixed concept names using minimized concept names without introducingTBox. proof, may viewed much careful version proofLemma 4, adapted yield analogous result logics DL.Lemma 5 ALC, satisfiability w.r.t. (concept-)circumscribed KBs empty TBoxwithout fixed roles polynomially reduced satisfiability w.r.t. (concept-)circumscribedKBs empty TBox without fixed predicates.3.2 Acyclic General TBoxesmany DLs, satisfiability w.r.t. (non-circumscribed) KBs general TBoxes hardersatisfiability w.r.t. (non-circumscribed) KBs acyclic TBoxes. case ALC,ALCI, ALCQ, ALCQO, latter problem PSpace-complete (Baader, McGuiness,Nardi, & Patel-Schneider, 2003; Baader, Milicic, Lutz, Sattler, & Wolter, 2005b; Y. Ding& Wu, 2007) former ExpTime-complete (Baader et al., 2003).DLs considered paper satisfiability ExpTime-hard already acyclicTBoxes ALCIO extensions (Areces, Blackburn, & Marx, 2000). show that,circumscribed KBs, difference computational complexity acyclicgeneral TBoxes.Let C0 concept CircCP (T , A) cKB CP = (, M, F, V ). may assume.without loss generality = {> = C} concept C. (To see this, observe.axioms C v equivalent > = C D). Define....acyclic TBox 0 = {A = C, B = u.A, A0 = A, B 0 = B}, A, B, A0 , B 0 , unew concept role names occurring , A, , F , V , C0 .circumscription pattern CP0 = (, 0 , F, V 0 ), 0 = {A0 , B 0 } V 0 =V {A, B, u}.ad B 0 conjunctively C0 thus interested models CircCP0 (T 0 , A)(B 0 )I 6= . models, AI = (and thus C = ) since, otherwise,turn instance B 0 instance B 0 making instance B727fiBonatti, Lutz, & Wolterlinking via role u instance A, thus obtaining preferred model w.r.t.<CP0 . basis proof following lemma, given appendix.Lemma 6 C0 satisfiable w.r.t. CircCP (T , A) iff C0 uB 0 satisfiable w.r.t. CircCP0 (T 0 , A).Thus, obtained following result.Proposition 7 Let L DL. Satisfiability w.r.t. (concept-)circumscribed KBs Lpolynomially reduced satisfiability w.r.t. (concept-)circumscribed KBs L acyclicTBoxes without changing ABox.shows satisfiability w.r.t. cKBs acyclic TBoxes complexitysatisfiability w.r.t. cKBs general TBoxes. many cases considered paper,even true cKBs empty TBoxes, c.f. Section 4. However, also identifycases cKBs non-empty TBoxes higher complexity (see Theorems 24 28),thus general reduction one underlying Proposition 7 cannot exist caseempty TBoxes.3.3 Simultaneous Satisfiabilityapplications, often necessary merge TBoxes, ABoxes, whole knowledge basestaking union. show that, certain conditions, reasoning w.r.t. unionseveral circumscribed KBs reduced reasoning w.r.t. component cKBs.concept C simultaneously satisfiable w.r.t. cKBs CircCP1 (T1 , A1 ), . . . , CircCPk (Tk , Ak )exists interpretation model cKBs satisfies C 6= .following lemma says simultaneous satisfiability polynomially reducedsatisfiability w.r.t. single cKB two cKBs share role name.proof idea case k = 2 follows. Given CircCP1 (T1 , A1 ) CircCP2 (T2 , A2 ),first take union two cKBs, replacing CircCP2 (T2 , A2 ) concept namealso used CircCP1 (T1 , A1 ) fresh concept name A0 . introduceadditional concept name P (for problem) make sure P satisfied ABoxindividual whenever point model interpretation A0disagrees. look model P satisfied ABox. Intuitively,additional concept name P satisfies purpose decoupling A0 , importante.g. case A/A0 minimized CircCP1 (T1 , A1 ) CircCP2 (T2 , A2 ). Detailsgiven appendix.Lemma 8 L DL, simultaneous satisfiability w.r.t. (concept-)circumscribed KBsCircCP1 (T1 , A1 ), . . . CircCPk (Tk , Ak ), CircCPi (Ti , Ai ) CircCPj (Tj , Aj ) sharerole names 1 < j k, reduced polynomial time satisfiability w.r.t. single(concept-)circumscribed KBs.4. Complexity Reasoning Concept-Circumscribed KBsmain contributions paper consist (i) showing that, many cases, reasoningcircumscribed knowledge bases decidable; (ii) performing detailed analysis728fiThe Complexity Circumscription DLscomputational complexity decidable cases. section, showsatisfiability w.r.t. concept-circumscribed KBs NExpNP -complete DL ALCextensions ALCIO ALCQO. also show NPNExp -complete numberfixed minimized concept names bounded constant. first present proofsupper bounds establish matching lower bounds.4.1 Upper Boundsstart general case bound number fixedminimized predicates.4.1.1 General Caseprepare upper bound proof showing concept satisfiable w.r.t.concept-circumscribed KB, satisfiable model bounded size. use |C|denote length concept C, i.e.,P number (occurrences of) symbols neededwrite C. size |T | TBox CvDT |C| + |D|, size |A| ABoxsum sizes assertions A, size role assertion 1size concept assertions C(a) |C|.Lemma 9 Let C0 concept, CircCP (T , A) concept-circumscribed KB, n := |C0 | +|T | + |A|. C0 satisfiable w.r.t. CircCP (T , A), following holds:(i) , C0 formulated ALCIO, C0 satisfied modelCircCP (T , A) #I 22n .(ii) , C0 formulated ALCQO maximal parameter occurring number restriction , A, C0 , C0 satisfied modelCircCP (T , A) #I 22n (m + 1) n.Proof. Let CP, , A, C0 Lemma 9. may assume = everyassertion C(a) expressed implication {a} v C, every assertion r(a, b)expressed {a} v r.{b}. Denote cl(C, ) smallest set concepts containssubconcepts C, subconcepts concepts appearing , closed singlenegations (i.e., cl(C, ) start , cl(C, )).Let common model C0 CircCP (T , A), let d0 C0I . Define equivalence relation setting d0 iff{C cl(C0 , ) | C } = {C cl(C0 , ) | d0 C }.use [d] denote equivalence class w.r.t. relation. Pickequivalence class [d] exactly one member denote resulting subset 0 .first prove Point (i). Thus, assume C0 formulated ALCIO.define new interpretation J follows:J:= 0AJ:= {d 0 | AI }rJ:= {(d1 , d2 ) 0 0 | d01 [d1 ], d02 [d2 ] : (d01 , d02 ) rI }aJ:= 0 aI [d].729fiBonatti, Lutz, & Wolterfollowing claim easily proved using induction structure C.Claim: C cl(C0 , ) , C iff d0 C J elementd0 [d] J .Thus, J model satisfying C0 . show J model CircCP (T , A), thusremains show model J 0 J 0 <CP J . Assume contraryJ 0 . define interpretation 0 follows:0AI0:=[[d]:=dAJ 0rI0[:=(d1 ,d20aI[d1 ] [d2 ])rJ 0:= aI .matter routine show following:00Claim: concepts C cl(C0 , ) , C iff d0 C Jelement d0 [d] J .00follows 0 model . Observe AI fi AI iff AJ fi AJ conceptname fi {, }. Therefore since CP concept circumscription pattern0 <CP follows J 0 <CP J . derived contradiction conclude Jmodel CircCP (T , A). Thus done since size J bounded 22n .Point (ii). Pick, 0 concept (> k r C) cl(C0 , )(> k r C)I , k elements {d0 | d0 C , (d, d0 ) rI }. Also pick,concept (6 k r C) cl(C0 , ) ((6 k r C))I , k + 1 elements{d0 | d0 C , (d, d0 ) rI }. Denote 00 collection elements picked. Take00 \ 0 element ds 0 ds define interpretation JJ:= 0 00AJ:= {d 0 00 | AI }rJ:= {(d1 , d2 ) 0 (0 00 ) | (d1 , d2 ) rI }{(d1 , d2 ) (00 \ 0 ) (0 00 ) | (ds1 , d2 ) rI }aJ:= aI [d].following claim easily proved.Claim: C cl(C0 , ), following:(i) d, d0 J , d0 , C J iff d0 C J ;(ii) , C iff d0 C J element d0 [d] J .Thus, J model satisfying C0 . show J model CircCP (T , A),thus remains show model J 0 J 0 <CP J . Assumecontrary J 0 . define interpretation 0 . end, take730fiThe Complexity Circumscription DLs\ J dp 0 dp . define 0 follows0I0:=00:= AJ {d \ J | dp AJ }rI0:= rJ {(d1 , d2 ) (I \ J ) | (dp1 , d2 ) rJ }aI0:= aI .00Again, matter routine show:0Claim: concepts C cl(C0 , ) , C J iff C J00C (I \ J ) iff dp C J element dp [d] 0 .000follows 0 model . Observe AI fi AI iff AJ fi AJ conceptname fi {, }. Therefore since CP concept circumscription pattern0 <CP follows J 0 <CP J . derived contradiction concludeJ model CircCP (T , A). Thus done since size J clearly bounded22n (m + 1) n.interesting note proof Lemma 9 go role namesminimized fixed. problem cannot overcome, proved undecidabilityresults presented Sections 5 6.Using bounded model property established, prove decidabilityreasoning concept-circumscribed KBs formulated ALCIO ALCQO.precisely, Lemma 9 suggests non-deterministic decision procedure satisfiability w.r.t.concept circumscription patterns: simply guess interpretation bounded sizecheck whether model. turns procedure shows containment satisfiability complexity class NExpNP , contains problems solvednon-deterministic exponentially time-bounded Turing machine accessNP oracle. known NExp NExpNP ExpSpace.Theorem 10 ALCIO ALCQO, NExpNP decide whether conceptsatisfiable w.r.t. concept-circumscribed KB CircCP (T , A).Proof. hard see exists NP algorithm takes inputcKB CircCP (T , A) finite interpretation I, checks whether modelCircCP (T , A): algorithm first verifies polynomial time whether modelA, answering yes case. Otherwise, algorithm guesses interpretationJ domain interprets individual names way,checks whether (i) J model A, (ii) J <CP I. answers yeschecks succeed, otherwise. Clearly, checking whether J <CP donetime polynomial w.r.t. size J I.NP algorithm may used oracle NExp-algorithm decidingsatisfiability concept C0 w.r.t. cKB CircCP (T , A): Lemma 9, suffices guessinterpretation size 24k k = |C0 | + |T | + |A|,3 use NP algorithmcheck whether model CircCP (T , A). proves concept satisfiabilityNExpNP .3. bound 24k clearly dominates two bounds given Parts (i) (ii) Lemma 9.731fiBonatti, Lutz, & Wolterreductions given Section 2, Theorem 10 yields co-NExpNP upper boundssubsumption instance problem. show Section 4.2 upperbounds tight.4.1.2 Bounded Number Minimized Fixed PredicatesSince NExpNP rather large complexity class, natural question whetherimpose restrictions concept circumscription reasoning becomes simpler.following, identify case considering concept-circumscribed KBsnumber minimized fixed concept names bounded constant. case,complexity satisfiability w.r.t. concept-circumscribed KBs drops NPNExp . readersuninitiated oracle complexity classes, recall NExp NPNExp NExpNP ,NPNExp believed much less powerful NExpNP , see example workEiter et al. (2004).prove NPNExp upper bound, first introduce counting formulas commongeneralization TBoxes ABoxes.Definition 11 (Counting Formula) counting formula Boolean combinationconcept implications, ABox assertions C(a), cardinality assertions (C = n) Cconcept n non-negative integer. use , , denote Booleanoperators counting formulas. interpretation satisfies cardinality assertion (C = n)#C = n. satisfaction relation |= models counting formulasdefined obvious way.4following, assume integers occurring cardinality assertions codedbinary. NPNExp algorithm devised use algorithm satisfiability(non-circumscribed) counting formulas oracle. Therefore, first determinecomputational complexity latter. follows results Tobies (2000) that,ALC, satisfiability counting formulas NExp-hard. matching upper boundDLs ALCIO ALCQO obtained facts (i) polynomialtranslation counting formulas formulated languages C2, two-variablefragment first-order logic extended counting quantifiers (Gradel, Otto, & Rosen,1997; Pacholski, Szwast, & Tendera, 2000), (ii) satisfiability C2 NExp evennumbers counting quantifiers coded binary (Pratt-Hartmann, 2005).Theorem 12 (Tobies, Pratt) ALC, ALCIO ALCQO, satisfiability countingformulas NExp-complete even numbers number restrictions coded binary.establish improved upper bound.Theorem 13 Let c constant. ALCIO ALCQO, NPNExp decide satisfiability w.r.t. concept-circumscribed KBs CircCP (T , A), CP = (, M, F, V )#M c #F c.732fiThe Complexity Circumscription DLsProof. Assume want decide satisfiability concept C0 w.r.t. cKBCircCP (T , A), CP = (, M, F, V ) #M c #F c. Lemma 4,may assume F = (we may increase constant c appropriately).may assume without loss generality cardinality exactly c. Thus, let= {A0 , . . . , Ac }. Lemma 9, C0 satisfiable w.r.t. CircCP (T , A) iff existsmodel C0 CircCP (T , A) size 24k , k = |C0 | + |T | + |A|. Consider,, conceptCS :=AuA.uuA{A1 ,...,Ac }\Sc constant, number 2c concepts constant well. Clearly, sets CSI ,, form partition domain model I. Introduce, conceptname B role name r A, fresh concept name B 0 fresh role name r0 ,respectively. concept C, denote C 0 result replacing C concept nameB role name r B 0 r0 , respectively. primed versions A0 0defined analogously. Denote N set individual names {C0 }.NExp-oracle going use algorithm checks whether counting formulasatisfiable not. Now, NPNExp -algorithm follows (we use C @abbreviation counting formula (C v D) (D v C)):1. Guesssequence (nS | ) numbers nS 24k coded binary;individual name N , exactly one set Sa ;subset E N N .2. calling oracle, check whether counting formula 1 satisfiable, 1conjunction{(C0 = 0)};(CS = nS ), ;CSa (a), N ;{({a} v {b}) | (a, b) E} {({a} v {b}) | (a, b) N E}.3. calling oracle, check whether counting formula 2 satisfiable, 2conjunction0 A0 ;(CS = nS ), (note use unprimed versions);CSa (a), individual name N (we use unprimed versions);{({a} v {b}) | (a, b) E} {({a} v {b}) | (a, b) N E};,(A0 v A)_BM,BA733(B 0 @ B);fiBonatti, Lutz, & Wolterand, finally,_^(A0 @ A).(B = B 0 ) .BM,BA4. algorithm states C0 satisfiable model CircCP (T , A) if, if,1 satisfiable 2 satisfiable.Using fact c fixed, hard verify NPNExp -algorithm.remains show correctness completeness.Suppose exists model CircCP (T , A) satisfying C0 .model size bounded 24k . Let algorithm guessnumbers nS = #CSI , ,sets Sa aI CSIa ,set E = {(a, b), (b, a) | aI = bI , a, b N }.Clearly, 1 satisfied I. remains show 2 unsatisfiable. supposeexists model J satisfying 2 . definitions 1 2 , may assume= J ;AI = AJ ;aI = aJ individual names a.Moreover, unprimed role names occur 2 unprimed concept names2 , may assume interpretation unprimed concept rolenames J coincide. Thus, J model CircCP (T , A) satisfying C0 .define model J 0 domain J setting0aJ = aJ , individual names a;0rJ = (r0 )J , role names r;0AJ = (A0 )J , concept names A.Then, conjunct Item 1 definition 2 , J 0 model .Items 5 6 definition 2 , J 0 <CP J , derived contradiction.Conversely, suppose algorithm says exists model CircCP (T , A) satisfying C0 . take model 1 . conjunct Item 1 1 , modelsatisfying C0 . follows unsatisfiability 2 modelCircCP (T , A).734fiThe Complexity Circumscription DLscorollary, obtain co-NPNExp upper bounds subsumption instance problem. similar drop complexity occurs propositional logic, satisfiability w.r.t.circumscribed theories complete NPNP difficult see boundingminimized fixed predicates allows us find PNP algorithm.4.2 Lower Boundsprove lower complexity bounds reasoning concept-circumscribed KBsmatch upper bounds given Section 4.1.4.2.1 General CaseSection 4.1, start general case number fixed minimizedpredicates bounded. aim establish two NExpNP -lower boundsmatch upper bound established Theorem 10. first bound satisfiability w.r.t.concept-circumscribed KBs formulated ALC empty TBox, nonempty ABox. second bound also satisfiability w.r.t. concept-circumscribed KBsformulated ALC, assumes acyclic TBox empty ABox. reductions workalready case empty preference relation, without fixed predicates. Noteconsidering satisfiability concept C w.r.t. concept-circumscribed KB CircCP (T , A)empty interesting: seen C satisfiable w.r.t.CircCP (T , A) iff C satisfiable (without reference KB), C conceptobtained C replacing minimized concept names .proof first result reduction succinct version problem coCERT3COL, NExpNP -complete (Eiter, Gottlob, & Mannila, 1997), satisfiabilityw.r.t. concept-circumscribed KBs empty TBox. Let us first introduce regular (nonsuccinct) version co-CERT3COL:Instance size n: undirected graph G vertices {0, 1, . . . , n 1} everyedge labelled disjunction two literals Boolean variables {Vi,j | i, j < n}.Yes-Instance size n: instance G size n that, truth value assignmentBoolean variables, graph t(G) obtained G including edgeswhose label evaluates true 3-colorable.shown Stewart (1991), co-CERT3COL complete NPNP . obtain problemcomplete NExpNP , Eiter et al. use complexity upgrade technique: encodinginput succinct form using Boolean circuits, complexity raised one exponentialNExpNP (Eiter et al., 1997). precisely, succinct version co-CERT3COLSco-CERT3COL obtained representing input graph G nodes {0, . . . , 2n 1}4n + 3 Boolean circuits 2n inputs (and one output) each. Boolean circuits(1)(2)(i)named cE , cS , cS , cj , {1, 2, 3, 4} j < n. circuits, 2n inputsbits binary representation two nodes graph. purposecircuits follows:circuit cE outputs 1 edge two input nodes, 0 otherwise;(1)edge input nodes, circuit cS outputs 1 first literal(2)disjunction labelling edge positive, 0 otherwise; circuit cS735fiBonatti, Lutz, & Woltersecond literal; edge input nodes, outputarbitrary;(i)edge input nodes, circuits cj compute labellingVk1 ,k2 Vk3 ,k4 edge input nodes generating numbers k1 , . . . , k4 :(i)circuit cj outputs j-th bit ki ; edge input nodes,output arbitrary.reduce co-CERT3COLS satisfiability w.r.t. concept-circumscribed KBs formulated ALC whose TBox preference relation empty. remainsapply Lemma 5 eliminate fixed concept names (we note constructionproof lemma leaves preference relation untouched). Let(1)(2)(i)G = (n, cE , cS , cS , {cj }i{1,..,4},j<n )(succinct representation the) input graph 2n nodes. construct ABoxAG = {C0 u Root(a0 )}, circumscription pattern CPG , concept CG Gyes-instance co-CERT3COLS iff CG satisfiable w.r.t. CircCPG (, AG ).concept C0 used AG conjunction whose presentation split two parts.Intuitively, purpose first group conjuncts fix truth assignmentvariables {Vi,j | i, j < n}, construct (an isomorphic image of) graph t(G)obtained G including edges whose label evaluates true t. Then,purpose second group make sure t(G) 3-colorable.formulating C0 , use several binary counters counting modulo 2n (thenumber nodes input graph). main counters X use concept namesX0 , . . . , Xn1 Y0 , . . . , Yn1 bits, respectively. Additionally, introduce concept(i)(i)names K0 , . . . , Kn1 , {1, 2, 3, 4}, serve four additional counters K (1) , . . . , K (4) .first group conjuncts C0 found Figure 2, following abbreviationsused:ri .C denotes n-fold nesting r. .r.C;r.(K (i) = X) abbreviationuj<n(i)(i)(Xj r.Kj ) u (Xj r.Kj )similarly r.(K (i) = );abbreviations Wc , c Boolean circuit, explained later on.intuition behind Figure 2 follows. Lines (1) (5) build binary tree depth2n whose edges labeled role name r. 22n leaves tree instancesconcept name Leaf, labeled possible values counters X. Since minimize Leaf via circumscription pattern CPG , concept namedenotes precisely leaves tree. Due use counters X , leavesdistinct.leaves tree established satisfy number purposes. start with,leaf counter values X = = j corresponds variable Vi,j co3CERTCOLS determines truth value variable via truth/falsity concept736fiThe Complexity Circumscription DLsri .(r.Xi u r.Xi )jr .((Xi r.Xi ) u (Xi r.Xi ))rrn+jn+i.(r.Yi u r.Yi ).((Yi r.Yi ) u (Yi r.Yi ))< n(1)< n, j < 2n(2)i<n(3)< j < n(4)2nr .Leaf(5)r2n .(WcE u Wc(1) u Wc(2) )(6)r2n .(Wc(1) u u Wc(4) )jj < nr2n .(var1.LeafFix u var1.(K (1) =X) u var1.(K (2) =Y ))2nr .(var2.LeafFix u var2.(K(3)(7)j=X) u var2.(K(4)(8)=Y ))(9)2nP r .var1.Leaf(10)2nP r .var2.Leaf(11)r2n .(S1 (Tr1 var1.Tr))(12)2nr .(S1 (Tr1 var1.Tr))2nr .(S2 (Tr2 var2.Tr))2n(13)(14)r .(S2 (Tr2 var1.Tr))(15)2n(16)r .(Elim (E (Tr1 Tr2 )))Figure 2: first group conjuncts C0 .name Tr. Thus, leaves jointly describe truth assignment instance G co3CERTCOLS . second purpose leaves represent potential edges G:additionally representing variable, leaf X = = j correspondspotential edge nodes j. explain properly, must firstdiscuss abbreviations Wc used Lines (6) (7) Figure 2.concept Wc , c Boolean circuit 2n inputs, result converting cconcept uses constructors , u, following condition satisfied:instance Wc , output c upon input b0 , . . . , b2n1 b, truthvalue concept names X0 , . . . , Xn1 , Y0 , . . . , Yn1 described b0 , . . . , b2n1 ,truth value concept name described b. introducing oneauxiliary concept name every inner gate c, translation donesize Wc linear size c. following concept names used output:WcE uses concept name E output;Wc(i) uses concept name Si output, {1, 2};(i)Wc(i) uses concept name Kjoutput, {1, . . . , 4} j < n.j737fiBonatti, Lutz, & Wolterr2n .(col1.LeafFix u col2.LeafFix)2nr .(col1.(X = X) u col1.(Y = 0))2nr .(col2.(Y = X) u col2.(Y = 0))(17)(18)(19)2n(20)2nP r .col2.Leaf(21)r2n .((Y = 0) (R B G))(22)r .((Y = 0) ((R u B) u (R u G) u (B u G)))(23)r2n .((Elim u col1.R u col2.R) Clash)(24)r2n .((Elim u col1.G u col2.G) Clash)(25)r2n .((Elim u col1.B u col2.B) Clash)(26)P r .col1.Leaf2nFigure 3: second group conjuncts C0 .Lines (6) (7) ensure concepts propagated leaves. next aimensure leaf represents potential edge (i, j) connected via role var1leaf represents variable first disjunct label (i, j), analogouslyrole var2 variable second disjunct edge label. replacedconcept name LeafFix Leaf Lines (8) (9), lines would apparentlyencode properties. However, careful mentioned replacementwould interact minimization Leaf. fix problem, resort trick:use concept name LeafFix instead Leaf. way, may may reachinstance Leaf. not, force concept name P true roottree Lines (10) (11). use CG rule models P true. Finally,fix LeafFix via CPG eliminate interaction minimization Leaf.remaining Lines (12) (16) ensure leaf instance Elim iff potentialedge represents present graph t(G) induced truth assignmentdescribed leaves.second group conjuncts C0 found Figure 3. Here, (Y = 0) standsconcept (Y0 u u Yn1 ). already mentioned, purpose conjunctsensure graph t(G) described leaves 3-coloring.strategy ensuring follows: use 2n leaves = 0 store colorsnodes, i.e., leaf X = = 0 stores color node i. Lines (22)(23), unique coloring. Then, Lines (17) (21) ensure leaf (viewededge) connected via role col1 leaf stores color first nodeedge, analogously role col2 second node edge. LeafFixP role before. Lines (24) (26) guarantee concept name Clashidentifies problems coloring: leaf Clash represents edge existsG, dropped t(G), endpoints color. ideaClash minimized R, G, B vary. additional conceptnames fixed, corresponds universal quantification possible colorings.738fiThe Complexity Circumscription DLsSet CG = Root u P u r2n .Clash, recall AG = {C0 u Root(a0 )}. followinglemma proved appendix.Lemma 14 G yes-instance co-3CERTCOLS iff CG satisfiable respectCircCPG (, AG ), CPG = (, M, F, V ) = , = {Root, Leaf, Clash},F = {LeafFix, Tr, X0 , . . . , Xn1 , Y0 , . . . , Yn1 , },V set remaining predicates AG .Since size AG polynomial n, get following result applying Lemma 5.Theorem 15 ALC, satisfiability w.r.t. concept-circumscribed KBs NExpNP -hard evenTBox preference relation empty fixed predicates.rather straightforward establish announced second NExpNP lower boundreduction satisfiability w.r.t. concept-circumscribed KBs special case formulatedTheorem 15. Details given appendix.Corollary 16 ALC, satisfiability w.r.t. concept-circumscribed KBs NExpNP -hardeven TBox acyclic, ABox preference relations empty,fixed predicates.Corresponding lower bounds subsumption instance problems followreduction given Section 2.4.2.2 Bounded Number Minimized Fixed Predicatesestablish matching lower bound Theorem 13 showing that, ALC, satisfiability w.r.t. concept-circumscribed KBs NPNExp -hard even constant numberpredicates allowed minimized fixed. contrast previous section,ignore case empty TBoxes directly establish lower bound casenon-empty TBoxes empty ABoxes. allows us demonstrate usefulnessLemma 8 separating different parts lower bound proof: main reductionprevious section, two parts reduction shown Figure 2 3 trulyindependent, forced us implement technical trick involves conceptnames LeafFix P . using Lemma 8, contrast, achieve true separationconcerns. general, though, conjecture lower bound proved sectionalso established case empty TBoxes adapting mentioned technical trick.leave problem interested reader.Recall (non-deterministic) k-tape Turing machine described tuple(Q, , q0 , , qacc , qrej ),Q finite set states, finite alphabet, q0 Q starting state,Q k Q k {L, R}ktransition relation, qacc , qrej Q accepting rejecting states. purposes,oracle Turing machine 2-tape Turing machine that, additionally, equipped739fiBonatti, Lutz, & Wolter1-tape Turing machine M0 (the oracle) whose alphabet identical M,query state q? ,two answer states qyes qno .second tape called oracle tape. enters q? , oracle determinesnext state M: content oracle tape contained language acceptedoracle, next state qyes . Otherwise, qno . transition, headmoved symbols written. state q? cannot occur left-most componenttuple Ms transition relation.Let = (Q, , q0 , , qacc , qrej , M0 , q? , qyes , qno ) oracle Turing machinefollowing holds:solves NPNExp -complete problem;time consumption bounded polynomial p (where oracle calls contribute single clock tick);0 , q 0 ) bounded 2q(n) , qtime consumption M0 = (Q0 , , q00 , 0 , qaccrejpolynomial.assume without loss generality M0 never attempt move leftleft-most position tape (neither right right-most position). NPNExp hardness proof uses reduction word problem M. Thus, let w inputlength n, let = p(n) m0 = q(p(n)). construct three TBoxesTw , Tw0 , Tw00 , circumscription patterns CP, CP0 , CP00 , concept Cwaccepts w iff Cw simultaneously satisfiable w.r.t. CircCP (Tw , ), CircCP0 (Tw0 , ),CircCP00 (Tw00 , ). Then, Lemma 8 yields reduction (non-simultaneous) satisfiability w.r.t.concept-circumscribed cKBs. Intuitively, purpose first TBox Tw imposebasic structure domain, Tw0 describes computations M, Tw00 describescomputations M0 . use general TBoxes rather acyclic ones since, Lemma 6,done without loss generality.TBox Tw shown Figure 4. previous reduction, use conceptnames X0 , . . . , Xm0 1 Y0 , . . . , Ym0 1 implement two binary counters counting0modulo 2m . also use abbreviations previous reduction. Additionally,r.(X++) states value counter X0 , . . . , Xm0 1 incremented goingr-successors, i.e.,Xj (Xk r.Xk ) u (Xk r.Xk )0uuk=0..m 1k=0..m0 1uj=0..k1j=0..k1Xj (Xk r.Xk ) u (Xk r.Xk )purpose Lines (27) (30) ensure that, possible value (i, j)counters X , least one instance NExp satisfies (X = i) (Y = j).00minimize NExp, thus enforce NExp exactly 2m 2m elements.00elements interconnected via roles r (for right) u (for up) form 2m 2m grid. Later on, use grid encode computations oracle machine M0 . Observe740fiThe Complexity Circumscription DLs> v aux.NExpm0NExp v ((X = 2(27)m01) r.NExp) u ((Y = 21) u.NExp)(28)NExp v r.(Y =Y ) u r.(X++)(29)NExp v u.(X=X) u u.(Y ++)(30)> vResult vu aux.(Result u R )u (R u R )(31)i<mi<j<m(32)j> v aux.NP(33)Figure 4: TBox Tw .that, since working simultaneous satisfiability, minimization NExpinteract anything going put TBoxes Tw0 Tw00 .also minimize concept name Result, thus Lines (31) (32) guaranteeexactly instances Result, identified concept names R0 , . . . , Rm1 .makes call oracle i-th step, result call stored(unique) instance Result u Ri , i.e., instance satisfy concept name RejM0 rejected input falsify otherwise. Finally, also minimize NP, thusLine (33) guarantees exactly one instance NP. instance usedrepresent computation M. Summing up, circumscription pattern TwCP := (, {NExp, Result, NP}, , V ),V containing remaining predicates used Tw .purpose Tw0 describe computations M. use following conceptnames:, i, j < m, k {1, 2}, introduce concept name Sai,j,k . Intuitively,Sai,j,k expresses symbol j-th cell k-th tape i-th stepMs computation. start numbering tape cells steps 0.q Q < m, Qiq concept name expressing state qi-th step computation.q Q, i, j < m, k {1, 2}, Hji,k concept name expressingk-th head cell j i-th step computation.q Q, , i, j < m, k {1, 2}, {L, R}, concept names Aiq , Ai,j,k,i,kserve markers. precisely, Aq means that, time point i,executed transition switches state q. Similarly, Ai,j,kdescribes symboli,kwritten transition tape k, describes move tape k.details Tw0 given Figure 5. One copy concept inclusions figureneeded every i, j, j 0 < every k {1, 2}. assume w = a0 an1741fiBonatti, Lutz, & Wolter0,n,10,m1,1NP v Q0q0 u H00,1 u Sa0,0,1u Sa0,n1,1u SBu u SB0n1(34)0,0,20,m1,2NP v H00,2 u SBu u SB(35)NP vui,j 0 ,2a,b,qQ\{q? }(Sai,j,1 u SbNP vNP vNP vNP vNP vuAuAqQAi,kLi,kARqi,j,k0 ,2i,2Aiq u Ai,j,1u Ai,ju Ai,1u 0 ) (37)b< 1(38)(q,a,b,q 0 ,a0 ,b0 ,M,M 0 )Sai+1,j,ki+1,kHj1< 1(39)< 1 j > 0(40)i+1,kHj+1< 1 j < 1i+1,j,k< 1 j 6= j 0(Sai,j,k u Hji,k0 ) SauuQi+1qa,b,a6=b(Sai,j,k u Sbi,j,k ) uNP v (Hji,k u Hji,k0 )(36)(NP vu Qiq u Hji,1 u Hji,20 )uq,q 0 Q,q6=q 0(Qiq u Qiq0 )j 6= j 0(41)(42)(43)(44)NP v resi .(Result u Ri ) u resi .(Result u Ri )(45)NP v (Qiq? u resi .Rej) Qi+1qno(46)NP v(Qiq?u resi .Rej)Qi+1qyesNP v (Qiq? u Hji,k ) Hji+1,kNP vu< 1< 1< 1i+1,j,k(Qiq? u Sai,j,k ) Sa(47)(48)(49)Figure 5: TBox Tw0 .use B denote (shared) blank symbol M0 . Lines (34) (43) describebehaviour Turing machine usual way: transitions follow transition table,computation starts initial configuration, etc. Line (45) ensures instanceNP reach (unique) instance Result u Ri via role resi , < m. Lines (46)(47) deal transitions query state looking result oraclecall corresponding instance Result. Finally, Lines (48) (49) ensure headposition tape symbols change querying oracle. circumscriptionpattern Tw0 simply CP0 := (, , , V ), V set predicates used Tw0 .purpose Tw00 describe computations oracle Turing machine M0 . Notedescribe single computation M0 (but polynomially many) since may visit state q? once. computationsrepresented NExp grid, different computations untangled usedifferent concept names computation. use counter X0 , . . . , Xm0 1 count742fiThe Complexity Circumscription DLsconfigurations counter Y0 , . . . , Ym0 1 count tape cells configuration.Moreover, use following concept names:< m, concept name Sai . Sai satisfied instanceNExp X0 , . . . , Xm0 1 value j Y0 , . . . , Ym0 1 value k, i-thcomputation M0 has, j-th step, symbol k-th cell.q Q < m, concept name Qiq . purpose concept nametwo-fold: first, represents current state M0 i-th computation.second, indicates head position i-th computation., q Q, {L, R} < m, concept name Aiq,a,M marker.meaning marker Aiq,a,M that, reach current configuration, M0switched q, written a, moved head direction . Additionally, markerindicates head position previous configuration.additional concept name NH (for nohead) helps us enforce M0single head position.details Tw00 shown Figure 6, require one copy line every< m. purpose Lines (50) (51) regenerate grid structure NExp usingroles r0 und u0 . necessary since roles r u used Tw , and, useLemma 8, TBoxes cannot share role names. Lines (52) (53) ensure everyinstance NExp reaches (only) unique instance NP via role toNP, (only)unique instance Result u Ri via role res0i , < m. Lines (54) (64) describecomputation M0 straightforward way. precisely, Lines (54) (56) setinitial configuration reading contents Ms oracle tape instance NP.Lines (57) (61) implement transitions, Lines (62) (64) enforce unique labeltape, unique state, unique head position. Finally, Line (65) ensures that,i-th computation M0 rejecting, Rej true instance Result u Ri .Note M0 non-deterministic machine may one computation.storing Rej Result u Ri , need know computations rejecting.deal issue, Rej minimized predicates varying: existsaccepting computation M0 i-th input, represent computationNExp make Rej false instance Result u Ri . Hence, Rej holds Result u Riiff exists accepting computation. Note cannot fix concept namesX0 , . . . , Xm0 1 , Y0 , . . . , Ym0 1 minimizing Rej since would get unbounded numberfixed concept names. means elements NExp change positionminimization, roles r0 u0 . harmful since TwLines (50) (51) ensure structure (NExpI , (r0 )I , (u0 )I ) always isomorphicgrid, rest Tw00 ensures elements NExp always encode computationsM0 . thus use circumscription pattern CP00 := (, {Rej}, , V 0 ), V 0 containspredicates used Tw00 except Rej.proof following lemma left reader. formulation, unionQiqacc imposes least one state computation accepting.743fiBonatti, Lutz, & Wolter00NExp v ((X = 2m 1) r0 .NExp) u ((Y = 2m 1) u0 .NExp)0000(50)NExp v r .(Y =Y ) u r .(X++) u u .(X=X) u u .(Y ++)(51)NExp v res0i .(Result u Ri ) u res0i .(Result u Ri )(52)NExp v toNP.NP u toNP.NP(53)NExp vuuj<m(X = 0) u (Y = j) u toNP.Sai,j,2 Sai(54)NExp v ((X = 0) u (Y m)) SB(55)Qiq00(56)NExp v ((X = 0) u (Y = 0))NExp vuu(Sai u Qiq )qQ0(q,a,q 0 ,a0 ,M )0r0 .Aiq0 ,a0 ,M(57)NExp v Aiq,a,R (Sai u u0 .Qiq )(58)Aiq,a,L Saiu0 .Aiq,a,L(59)NExp vNExp vNExp vNExp v(60)Q u (S r .S )u (S u ) u uQ u .NHNH Q u u .NHNExp vNExp vQiqqQ0qa,b,a6=bqQ0rejb(61)(Qiq u Qiq0 )q,q 0 Q0 ,q6=q 00qqQ0NExp v Qiq00q(62)(63)0(64)res0i .Rej(65)Figure 6: TBox Tw00 .Lemma 17 accepts w iff Cw := NP utQi<mqaccsimultaneously satisfiable w.r.t.CircCP (Tw , ), CircCP0 (Tw0 , ), CircCP00 (Tw00 , ).remains apply Lemmas 6, 4, 8 obtain following result.Theorem 18 ALC, satisfiability w.r.t. concept-circumscribed cKBs NPNExp -hardeven TBox acyclic, ABox preference relations empty,fixed predicates, number minimized predicates bounded constant.already mentioned, conjecture result proved empty TBoxes(but non-empty ABoxes). Corresponding lower bounds subsumption instanceproblems follow reduction given Section 2.744fiThe Complexity Circumscription DLs5. Circumscription Fixed Rolespreceeding sections, analyzed computational complexity reasoningw.r.t. concept-circumscribed KBs and, particular, established decidability. currentsection, extend concept-circumscribed KBs call role-fixing cKBs, differformer allowing role names fixed (but minimized). Interestingly,result seemingly harmless modification reasoning becomes highly undecidable.start defining cKBs studied section.Definition 19 cKB CircCP (T , A) CP = (, M, F, V ) called role-fixing contains role names.4pinpoint exact complexity reasoning role-fixing cKBs, present reductionlogical consequence problem monadic second-order logic binary relation symbols(over unrestricted structures, trees) instance problem w.r.t. role-fixing cKBsformulated ALC. follows latter problem harder problem definablesecond-order arithmetic thus outside analytical hierarchy. Analogous resultssatisfiability subsumption follow reductions given Section 2. reductionapplies already case TBox preference relation empty.finite set R binary relation symbols, denote MSO(R) set formulasconstructed countably infinite set P1 , P2 , . . . variables sets, countable infiniteset x1 , x2 , . . . individual variables, binary relation symbols R, using Booleanconnectives, first-order quantification, monadic second-order quantification.hard see reasoning role-fixing cKBs corresponds reasoning tiny fragment MSO(R). specifically, consider standard translation ALC-concepts CFO-formulas (and thus MSO(R)-formulas) C ] (x) one free individual variable x e.g.given Borgida (1996) take cKB CircCP (T , A) CP = (, M, F, V ), = V = ,= {A}, F = {r}. Translate (T , A) MSO(R)-sentence^^^=x(C ] (x) D] (x))C ] (xa )r(xa , xb ),CvDTC(a)Ar(a,b)Axa individual variables.ALC-concept C satisfiable w.r.t. CircCP (T , A) if, if, MSO(R)formulaxC ] (x) P (P A] [P/A] ])satisfiable, P A] stands x (P (x) A] (x))x (A] (x)P (x)) [P/A] ]denotes A] replaced P . translation easily extended casearbitrary number concept names minimized arbitrary number conceptrole names fixed varies.prove logical consequence MSO(R) reducible instance problem w.r.t. role-fixing cKBs, thus establish surprising result reasoningsKBs correspond tiny fragment MSO(R), hardMSO(R). reduction indirect: instead directly reducing logical consequenceMSO(R), reduce semantic consequence problem modal logic exploit Thomasons result logical consequence MSO(R) reduced latter problem, see745fiBonatti, Lutz, & Wolterworks Thomason (1975b, 1975a) survey articles Wolter et al. (2007)Goldblatt (2003) details.first define semantic consequence problem modal logic (in frameworkdescription logic) present Thomasons result, starting notation.Let R finite set role names. R-frame structure F = (F , RF ), Fnon-empty domain rF F F r R. interpretation = (I , )based R-frame F iff F = rI = rF r R. say concept Cvalid F write F |= C C = every interpretation based F. semanticconsequence modal logic defined follows. Let C ALC conceptsroles R. semantic consequence C, symbols C D, everyR-frame F, F |= C follows F |= D. Note since validity R-frame Finvolves quantification possible interpretations symbols contained R,relation C invariant uniform renamings atomic concepts (thisused later on).simplicity, consider MSO(r), monadic second-order logic one binaryrelation symbol r. straighforward extend result arbitrary finite sets Rrelation symbols. Given set role names R, ALC-concept called ALC R -conceptuses role names R. following theorem followsresults Thomason (1975b, 1975a).Theorem 20 exist effective translation : 7 () MSO(r) sentencesALC {r} -concepts ALC {r} -concept C0 MSO(r) sentences ,following conditions equivalent:logical consequence MSO(r);C0 u () ().thus establish reduction MSO(r) instance problem w.r.t. role-fixingcKBs reducing instead semantic consequence problem. fact, reductionimplemented transparent way extend ALC universal role, whereasreduction ALC requires number rather technical intermediate steps.reason, defer ALC case appendix give proof universalrole.Let u new role name, called universal role. every interpretation I, ufixed interpretation uI = . Since interpretation u fixed anyway,allow use circumscription patterns.suppose C ALC {r} -concepts. establish reduction, construct role-fixing cKB CircCP (, {C0 (a)}) concept C1 C if, if,instance C1 w.r.t. CircCP (, {C0 (a)}). noted above, may assume C,share concept names (otherwise, simply replace concept names freshones). Let concept name occur C D, let CP = (, M, {r}, V ),= , consists set concept names C, V consistsconcept names D. Set = {(u.C u.B)(a)}.uBMLemma 21 following conditions equivalent:746fiThe Complexity Circumscription DLsC D;instance (u.C) w.r.t. CircCP (, A).Proof. prove Point 1 implies Point 2, assume Point 2 hold. Letmodel CircCP (, A) aI ((u.C) u D)I . Let based F. provePoint 1 hold, show F |= C F 6|= D. latter easywitnessed interpretation I. show former, let J interpretation basedF. show C J = . First note that, since model aI (u.C)I ,B = , B . distinguish two cases:B J = , B . case, B interpretationJ . Thus, since concept names C J basedframe, obtain C J = C = .B J 6= , least one B . J <CP I. Assume C J 6= .aI (u.C)J J model A. Thus, derived contradictionassumption model CircCP (, A).prove Point 2 implies Point 1, assume Point 1 hold. Considerframe F F |= C, F 6|= D. Let interpretation based FaI (D)I . may also assume B = B (since B occursD). aI ((u.C) u D)I model A. remains showexist 0 <CP 0 model A. straightforward: F |= C,00obtain exist 0 aI (u.C)I . Moreover, clearly000exist 0 B ( B B aI (u.B)I .uBMthus proved logical consequence problem MSO(r) effectively reducibleinstance problem w.r.t. role-fixing cKBs formulated ALC extended universal role. reduction, TBox preference relation empty. appendix,show reduction modified prove result ALCwithout universal role.Theorem 22 logical consequence problem MSO(r) effectively reducibleinstance problem w.r.t. role-fixing cKBs formulated ALC. even holdsTBox preference relation empty.6. Circumscription Minimized RolesUnlike fixed concept names, fixed role names cannot simulated using minimized rolenames. due fact Boolean operators roles available standardDLs. Thus, Theorem 22 imply undecidability reasoning cKBs rolenames allowed minimized, fixed. section, investigate cKBstype. formal definition follows.Definition 23 cKB CircCP (T , A) CP = (, M, F, V ) called role-minimizing Fcontains role names.4747fiBonatti, Lutz, & Woltershow role-minimizing cKBs behave rather differently concept-circumscribedKBs role-fixing cKBs. First, turns reasoning role-minimizingcKBs empty TBox NExpNP -complete ALCQO, undecidable ALCI.Thus, contrast circumscribed KBs considered far, observe differencecomplexity ALCQO ALCI, even ALC ALCI. seconddifference results obtained far NExpNP -lower bound, appliescKBs formulated ALC empty TBox, even holds role-minimizing cKBssingle role minimized predicate fixed minimized. resultinterest shows complexity drop NPNExp numberminimized predicates constant. Finally, show that, non-empty TBoxes, reasoningrole-minimizing cKBs becomes undecidable already ALC.6.1 Role-minimizing cKBs Empty TBox ALCQOfirst prove NExpNP -completeness result discussed DLs without inverseroles. start upper bound. prove it, first establish bounded modelproperty using selective filtration-style argument, see e.g. Blackburn et al. (2001).difference bounded model property proof given concept-circumscribed KBsthat, here, build quotient model given model identifying nodes usingequivalence relation, construct submodel given model selecting relevant nodes.contrast forming quotient models, technique works empty TBoxes sinceTBox force us select infinitely many nodes. Similarly, selection techniquework DLs inverse role because, shall see below, inverse rolesused simulate TBoxes.Recall role depth rd(C) concept C defined nesting depthconstructors (> k r D) (6 k r D) C.Theorem 24 ALCQO, satisfiability w.r.t. role-minimizing cKBs empty TBoxNExpNP .Proof. Let CircCP (, A) role-minimizing cKB CP = (, M, F, V ), let C0concept satisfiable w.r.t. CircCP (, A). Let m0 maximal parameter occurringnumber restrictions C0 . Setn := max({rd(C0 )} {rd(C) | C(a) A}) := ((m0 + 1) (|A| + |C0 |))n+1 ,show exists model J CircCP (, A) satisfying C0 #J m.Let model CircCP (, A) exists d0 C0I . , fixminimal set D(d) that,every concept (> k r C) occurs C0 (> k r C)I existleast k distinct e D(d) (d, e) rI e Cevery concept (6 k r C) occurs C0 6 (6 k r C)I existleast k + 1 distinct e D(d) (d, e) rI e C .Clearly, #D(d) (m0 + 1) (|C0 | + |A|) . Next, define set D0settingD0 := {d0 } {aI | NI occurs C0 }.748fiThe Complexity Circumscription DLsDefine sets Di , 1 n, inductively[Di+1 := (D(d))dDiset n :=0in Di .Define interpretation 0 domain follows:0aI = aI , individual names a;0r V , (d, e) rI n \ Dn , e D(d), (d, e) rI ;0V , AI = AI n ;0F , AI = AI .0straightforward inductive argument shows 0 model d0 C0I .0Note change interpretation F . Moreover, pI pIevery p . Together fact 0 model 0 6<CP I, even get0pI = pI every p . follows 0 model CircCP (, A) J <CP 0would imply J <CP I.0Note rI n n , every role r. define interpretation J domainJ = n putting0AJ = AI n , every concept name A;0rJ = rI , every role name r;aJ = aI , every individual name C0 .still J model satisfying C0 . Moreover, interpretation J 0 <CP Jsatisfying easily extended interpretation 00 <CP 0 satisfying A. Hence,interpretation exists J model CircCP (, A). #n derive#J m.proof NExpNP -upper bound exactly proof Theorem10; suffices replace bound 24k size interpretations bound m.give lower bound matching upper bound Theorem 24.Theorem 25 ALC, satisfiability w.r.t. role-minimizing cKBs empty TBox NExpNP hard. holds even one minimized role name fixed prediatesProof. Theorem 15, ALC NExpNP -hard decide whether concept C0satisfiable w.r.t. CircCP (, A), CP = (, M, , V ) contains concept names only.Clearly, still NExpNP -hard decide whether exists common model C0CircCP (, A) size least #M . Thus, sufficient provide polynomial reductionproblem satisfiability problem w.r.t. cKBs ALC single minimizedrole remaining predicates varying. Suppose C0 CircCP (, A) given. Let= {A1 , . . . , Ak } taketwo fresh role names r0 , r1 ;749fiBonatti, Lutz, & WolterBoolean concepts C1 , . . . , Ck built using fresh concept names B1 , . . . , Bkevery Ci , k, satisfiable every Ci u Cj , 6= j, unsatisfiable. One take,example, Ci = B1 u u Bi1 u Bi u Bi+1 u u Bk , k.Let CP0 = (, {r0 }, , V {B1 , . . . , Bk , r1 }) define A0 C00 result replacing,C0 , every occurrence Ai r0 .Ci , k. Finally, set = A0 {r1 .Ci (a) |k}. show following:() C0 satisfiable w.r.t. CircCP (, A) model size least #M if, if, C00satisfiable w.r.t. CircCP0 (, ).Let model CircCP (, A) C0 size least #M . Define interpretation 0domain extending follows: take mutually distinct d1 , . . . , dkinterpret B1 , . . . , Bk , r0 , r1 , way0CiI = {di }, k,0r0I = {(d, di ) | AIi , k},aI = d1 ,0r1I = {(d1 , d1 ), . . . , (d1 , dk )}.readily checked 0 model C00 CircCP0 (, ).Conversely, let model CircCP0 (, ) C00 . Define interpretation 00extending AIi = (r0 .Ci )I , k. readily checked 0 model C0CircCP (, A).6.2 Role-minimizing cKBs Nonempty TBoxbounded model property proof above, important selection nodesstops n iterations set n , n maximum role depthsconcepts ABox concept C want satisfy. bound selectionnodes exist TBox non-empty, show reasoning w.r.t.role-minimizing cKBs indeed undecidable case. proof reduction-tiling problem (Berger, 1966).N NDefinition 26 tiling problem quadruple triple P = (T, H, V ), finiteset tile types H, V horizontal vertical matching conditions.solution P mapping :N N( (i, j), (i + 1, j)) H i, j 0;( (i, j), (i, j + 1)) V i, j 0.4750fiThe Complexity Circumscription DLsLet P = (T, H, V ) instance tiling problem. define TBox TP follows:> v x.> u y.>> v> vuutTtTu(66)At00(67)t0 T,t6=t(t,t0 )Hx.At0 uutT(t,t0 )Vy.At0> v N (x.y.B u y.x.B)Nv(68)(69)(70)w x.D y.D(71)v x.D u y.D(72)Let CP = (, M, , V ) circumscription pattern V = {B, D} consistsremaining concept role names.Lemma 27 CircCP (TP , ) 6|= D(a) iff P solution.Proof. Assume P solution . Define interpretation follows:xIyIAItNIBIDIaI:=:=:=:=:=:=:=:=NN{((i, j), (i + 1, j)) | i, j 0}{((i, j), (i, j + 1)) | i, j 0}{(i, j) | (i, j) = t}{(i, j) | > 0 j > 0}(0, 0)straightforward verify model TP . Additionally, clearly aI/ DI .thus remains show model J TP J <CP I. AssumeJ . Since concept role names except B minimized, follows1. xI = xJ = J J model (66);2. AIt = AJPoint 1 J model (67);3. N = N J because, matter B J is, Point 1(x.y.B u y.x.B)J = .Thus J model (69), N = N J .Thus, J differ interpretation concept names B,varying. contradiction J <CP I.Conversely, assume CircCP (TP , ) 6|= D(a), let model CircCP (TP , )aI 6 DI . induction + j, define mapping assigns (i, j)element (i, j) i, j 0,N N751fiBonatti, Lutz, & Wolter1. ((i, j), (i + 1, j)) xI ;2. ((i, j), (i, j + 1)) ;start, set (0, 0) = aI . `-th step, following:Select ((0, ` 1), d) put (0, `) := d. existssince model (66).Select ((` 1, 0), d) xI put (`, 0) := d. Again,exists since model (66).let i, j > 0 + j = `. Since model (66), d, d0((i 1, j), d) xI ((i, j 1), d0 ) . show = d0 ,set (i, j) := d.Assume contrary 6= d0 . (70)(72) since aI/ DI ,(i 1, j 1) N . Define new interpretation J obtainedfollowing modifications:(i 1, j 1) removed N ;let d0 B J 6 B J ;let DJ = .Clearly, J <CP I. obtain contradiction model CircCP (TP , ),thus remains show J model TP . suffices consider (69) (72),concept inclusions referring N , B, D. axioms (70) (72) holdDJ = . show (69), let e . show e C J C conceptright hand side (69). Clearly, e C J since e C , e {x, y, x , }reachable aI . e N e {x, y, x , }-reachable aI ,otherwise would aI DI axioms (70) (72). Thus, e N J C Je 6= (i 1, j 1). Finally, (i 1, j 1) C J , definition B J .N Ndefine mapping : setting (i, j) := (i, j) . (67),mapping well-defined. (68), satisfies horizontal vertical matching conditions.Thus, P solution.Thus, shown following result.Theorem 28 ALC, satisfiability w.r.t role-minimizing cKBs undecidable.6.3 Reasoning Role-minimizing cKBs Empty TBox ALCIprove undecidability reasoning role-minimizing cKBs empty TBox ALCI.proof uses spy-point technique (Areces, Blackburn, & Marx, 1999); namely,show ABoxes simulate TBox reasoning employing inverse roles simulationnominals circumscription. Using idea proof rather similar proofTheorem 28.752fiThe Complexity Circumscription DLsLet P instance tiling problem consider cKB CircCP (TP , ) definedproof Lemma 27. simulate TBox axioms C v C 0 ABox assertions((C C 0 ) u r0 .(C C 0 ))(a) enforcing role r0 connect relevant points a.achieved forcing relevant points domain satisfy r0 .{a}. Sincenominals language use concept name instead {a} ensurebehaves like nominal. present details.sake readability, write concept assertions C(a) form : Cset 1 {r}.C = C u r.C. Let A, B 0 , N 0 fresh concept names r0 fresh rolename occurring TP . AP consists assertions: 1 {r0 }.(C C 0 ),(73)C v C 0 TP ,: A,: 1 {r0 }.u s.rs=x,y: 1 {r0 }. N 0 (A u B 0 ) r0 .(A u B 0 ) u: 1 {r0 }.(N 0 D),0 .A,(74)s.rs=x,y0 .(Au B 0 ): r0 .D(75)(76)let CP = (, M, , {D, B, B 0 }), consists concept role names distinctD, B, B 0 .Lemma 29 CircCP (, AP ) 6|= D(a) iff P solution.Proof. Assume P solution . Take interpretation proof Lemma 27expandedAI = {(0, 0)},N 0I = ,B 0I = ,r0I = {(aI , d) | }.show model CircCP (, AP ). Clearly, model AP . Thus remainsshow model J AP J <CP I. Assume exists J <CPmodel AP . minimized (74), AJ = {(0, 0)}. follows axiom(66) encoded (73) ((0, 0), (1, 0)) xJ ((0, 0), (0, 1)) J . one proveinduction ` > 0 using axiom (66) encoded (73) (74) (i, j)+ j = `, ((0, 0), (i, j)) r0J ((i, j), (i + 1, j)) xJ , ((i, j), (i, j + 1)) J .follows xJ = xI J = . Also observe N 0J = because, matterB 0 interpreted,J(A u B 0 ) r0 .(A u B 0 ) us.r0 .(A u B 0 )= .s=x,yone prove similarly proof Lemma 27 J differinterpretation B, B 0 , D, contradiction.Conversely, suppose model CircCP (, AP ) aI 6 DI . first show(aI , d) r0I whenever 6= aI {x, y}-reachable aI finite numbersteps. Assume case. exist d, d0753fiBonatti, Lutz, & Wolter= aI (aI , d) r0I ,(d, d0 ) xI (d, d0 ) ,(a, d0 ) 6 r0I .(74), exists d00 aI 6= d00 , (d00 , d0 ) r0I , d00 AI . ObserveN 0I (76) aI 6 DI . Define new interpretation J modifying follows:removed N 0I ;let aI B 0J d00 6 B 0J ;let DJ = .Clearly J <CP I. obtain contradiction thus sufficient show J modelAP . Clearly, assertion AP containing neither D, B 0 , N 0 satisfied J .remaining assertions except (75), follows DJ = satisfied J .Finally, (75), observe N 0I {aI } {e | (aI , e) r0I } 6 DI assertions(76). Thus, definition N 0J , consider point removed N 0I .Js.r0 .(A u B 0 )definition J .(A u B 0 ) r0 .(A u B 0 ) us=x,yone use assertions (73) construct solution P wayproof Lemma 27.thus proved following result.Theorem 30 ALCI, satisfiability w.r.t. role-minimizing cKBs empty TBox undecidable.7. Related Workalready pointed introduction circumscription one severalpossible approaches nonmonotonic DLs that, order achieve decidability,approaches adopt suitable restriction expressive power DLcomponent, non-monotonic features, interaction DL nonmonotonic features. section, survey existing approaches, discuss adoptedrestrictions, relate DLs circumscription whenever possible. However,point full-fledged formal comparison different approaches serious research endeavor outside scope paper. main approachesnonmonotonic DLs (excluding relying integration DLs logic programming) summarized Table 1, n.a. stands analyzed specificitycolumn states whether formalism equipped priority mechanism basedspecificity (i.e., subsumption) concepts.start two early approaches based circumscription. work Brewka(1987), frame system given nonmonotonic semantics based circumscription.focus showing appropriateness proposed semantics, decidabilitycomplexity reasoning analyzed. Cadoli et al. (1990), apply circumscription DL754fiThe Complexity Circumscription DLsRef(Brewka, 1987)(Cadoli, Donini, &Schaerf, 1990)(Padgham & Zhang,1993)(Straccia, 1993)(Baader & Hollunder, 1995a)(Baader & Hollunder, 1995b)(Lambrix,Shahmehri, & Wahlloef,1998)(Donini et al., 1997)DLframe lang.< ALENM featuresCircCircComplexityn.a.p2SpecificityNAL concrete domainsALCinheritancenetworksprioritizeddefault logicdefault logicn.a.decidabledecidableNprioritizeddefault logicprioritizeddefault logicdecidablen.a.MKNFrestrictionsMKNFrestrictionsmaximizedtypicalitydepends DLN3-ExpTimeNco-NExpNPNALCFALCALQO+featureagreementdecidable DL(Donini et al., 2002)ALC(Giordano, Gliozzi,Olivetti, & Pozzato,2008)ALCTable 1: approaches nonmonotonic DLsway here. authors consider non-prioritized circumscriptionapply fragment description logic ALE. Decidability reasoning shownreduction propositional reasoning Extended Closed World Assumption(ECWA), p2 . best knowledge, first effective reasoningmethod nonmonotonic description logic.another early approach Padgham Zhang (1993), nonmonotonic DL obtained adaptation inheritance networks approach (Horty, 1994)underlying DL essentially AL extended concrete data values. main contribution definition formalism discussion applications, decidabilitycomplexity analyzed.recent approach similar spirit circumscription taken Giordanoet al. (2008). authors extend ALC modal operator representing typicality,maximize extension achieve nonmonotonic inferences. Decidability proved viatableau algorithm also establishes co-NExpNP upper bound subsumption.lower bound given.turn approaches based default logic (Reiter, 1980). nonmonotonicDLs Baader Hollunder (1995a, 1995b), Straccia (1993), Lambrix et al. (1998)share common restriction: default rules applied individualname, is, denoted individual constant occurs explicitlyknowledge base. restriction motivated observation that, defaults alsoapplied implicit (anonymous) individuals, reasoning becomes undecidable (Baader &755fiBonatti, Lutz, & WolterHollunder, 1995a). Since models DL knowledge bases usually include large numberanonymous individuals due existential quantification, restriction introducesstrong asymmetry treatment individuals.Another line nonmonotonic DLs (Donini et al., 1998, 1997, 2002) based first-orderautoepistemic logics whose interpretation domains restricted fixed denumerableset constants. use unique domain resolves several issues related quantificationmodal logics (such whether Barcan formula hold, whether differentworlds Kripke structure allowed different domains). alsoavoids asymmetry approaches based default logic because, definition,individuals name. side coin domains finite varyingcardinality knowledge bases satisfy unique name assumptiondealt using rather technical encodings (such explicit axiomatizationfinite domain represented concept name D).first paper mentioned (Donini et al., 1998), decidability results applymonotonic knowledge bases4 autoepistemic operator used nonmonotonicfashion queries. restriction lifted subsequent publications.make use logic minimal knowledge negation failure (MKNF),equipped two (auto)epistemic operators K (Donini et al., 1997, 2002).former paper (Donini et al., 1997), underlying monotonic fragmentdescription logic decidable instance checking problem. price payedgenerality decidability results apply so-called simple knowledge bases,quantifying-in (that is, quantification across modal operators, R.K C) allowed.Nonetheless, KBs expressive enough model default rules. different restrictionexplored Donini et al. (2002). underlying DL restricted ALC limited formsquantifying-in allowed, so-called subjectively quantified ALCKN F knowledge bases.Decidability results obtained subclass simple subjectively quantified knowledgebases, whose nonmonotonic part restricted inclusions form KC v> v C inferred knowledge base. restriction incompatibletraditional embedding (priority-free) circumscription autoepistemic logic,based prerequisite-free default rules would equivalent inclusionsform K> v C.recent line research integrating DLs logic programming rules introducesnonmonotonic extensions DLs based negation-as-failure. approaches(Eiter et al., 2004) use loosely coupled integration logic programs DLs,interpretations DL component restricted logic program variablesrange named DL individuals. Thus, approaches somewhat similarclassical extensions DLs based defaults nonmonotonic inferenceslimited named individuals. recent approach (Motik & Rosati, 2007) basedMKNF shares MKNF-DLs discussed pros cons adopting fixeddenumerable domain. complexity reasoning underlying DL C 6 NP,data complexity entailment bounded C C . Finally, mention 3-valued variantapproach (Knorr, Alferes, & Hitzler, 2007) based well-founded semantics.4. autoepistemic operator used restricted contexts suffice encode so-calledprocedural rules, monotonic.756fiThe Complexity Circumscription DLsALCConcept circ.Minim. rolesFixed roles#M n, #F n(unrestricted)TBox=TBox6=ALCQOALCI ALCIONExpNPNExpNP even = , either TBox= ABox=NExpNP even #M 1, #F 0UndecidableUndecidableHighly undecidable, even TBox= , =Table 2: Summary complexity results satisfiability w.r.t. cKBscommon limitation nonmonotonic extensions DLs based MKNFprovide support specificity priorities. particular, defeasible inheritancementioned expressiveness analysis autoepistemic approaches (Donini et al.,1997, 2002) appear goal MKNF-based approach. pointedintroduction, well-known that, propositional case, nonmonotonic logicscannot modularly encode specificity-based priorities needed inheritancemechanisms (Horty, 1994).8. Conclusions Perspectivesshown circumscription provides elegant approach defining nonmonotonicDLs, resulting formalisms appealing expressive power decidableappropriate restrictions adopted. main restriction, leads ratherrobust decidability results, concept names minimized fixed whereasrole names vary. empty TBoxes, decidability retained roles allowedminimized, fixed. decidability complexity results obtainedpaper listed detail Table 2. results Section 3, boundsTBox 6= apply general acyclic TBoxes.view paper promising step towards establishing circumscribed DLsmajor family nonmonotonic DLs used practical applications. reach goal,additional research topics addressed, mention two. First,algorithms presented paper based massive non-deterministic guessingthus admit efficient implementation. Ideally, one would like algorithmswell-behaved extensions tableau algorithms underly state-of-the-artDL reasoners (Baader & Sattler, 2000). crucial issue whether sophisticated optimization techniques implemented reasoners (tableaux caching, dependency-directedbacktracking etc.; cf. Baader et al., 2003, Chap. 9) adapted circumscribed DLs.first steps made Grimm Hitzler (2008). Second, seems necessarydevelop design methodology modelling defeasible inheritance. examples givenpaper indicate main challenge find rules thumb determinepredicates fixed, varied, minimized. may appealing hide abnormalitypredicates behind explicit syntax defeasible inclusions, trade generalitysimplicity usability.Also theoretical perspective, initial investigation leave open leastinteresting questions. example, current techniques limited circumscribed DLs757fiBonatti, Lutz, & Wolterfinite model property. would desirable overcome limitationobtain decidability results even expressive DLs SHIQ OWL. alsopossible follow opposite approach consider circumscribed versions inexpressiveDLs EL DL-Lite family (Baader, Brandt, & Lutz, 2005a; Calvanese,Giacomo, Lembo, Lenzerini, & Rosati, 2007), currently popular largenumber applications. First steps made Bonatti, Faella, Sauro (2009),investigated circumscribed versions EL DL-lite.Finally, worth mentioning complexity results circumscriptionused prove complexity bounds other, seemingly unrelated, reasoning problemsdescription logic. example, certain reasoning services conservative extensionsmodularity description logic satisfiability problem w.r.t. concept-circumscribedknowledge bases mutually reducible polynomial time (Konev, Lutz,Walther, & Wolter, 2008). many problems known NExpNP complete, circumscription thus provides new class problems potentiallyemployed give NExpNP lower bound proofs.Acknowledgmentsfirst author partially supported network excellence REWERSE, IST-2004506779. third author partially supported UK EPSRC grant no. GR/S63182/01.Appendix A. Missing Proofs Section 3Lemma 5. ALC, satisfiability w.r.t. (concept-)circumscribed KBs empty TBoxwithout fixed roles polynomially reduced satisfiability w.r.t. (concept-)circumscribed KBs empty TBox without fixed predicates.Proof. proof Lemma 4, used TBox axioms state freshconcept names interpreted complement fixed concept names. general,cannot done using ABox assertions only. Instead, construct ABox assertionsforce case objects relevant truth given ABox.care required using ABox assertions polynomial size. first partproof deals problem. second part straighforward modificationproof Lemma 4.first part proof consists introducing notation proving centraltechnical claim. w = r1 rn NR , interpretation I, d, e , write(d, e) wI iff d0 , . . . , dn = d0 , e = dn , (di , di+1 ) ri+1< n.Let N set individual names Paths mapping N powerset NR .interpretation well-behaved mapping Paths every ,N w Paths(a) (aI , d) wI . ALC-concept C, associateset P(C) pairs (w, D) w NR subconcept C follows:C NC , P(C) = {(, C)};C = D, P(C) = {(, C)} P(D);758fiThe Complexity Circumscription DLsC = D1 u D2 C = D1 D2 , P(C) = {(, C)} P(D1 ) P(D2 );C = r.D C = r.D, P(C) = {(, C)} {(rw, E) | (w, E) P(D)}.set ABox assertions individual name a, use P(S, a) denote setC(a)S P(C). write Paths(S, a) {w | : (w, D) P(S, a)}. formulateannounced claim.Claim 1. Suppose CircCP (, A) 6|= C0 (a0 ), CP contain fixed role namesC0 (a) formulated ALC. Let = {C0 (a0 )} let N set individualnames S. Let 0 restriction that, Nw Paths(S, a), (aI , d) wI . 0 model CircCP (, A)C0 (a0 ) well-behaved mapping Paths(a) = Paths(S, a), N .prove claim. Let model CircCP (, A) aI0 (C0 )I . Note0N , Paths(S, a) thus aI . Clearly, 0 well-behavedPaths. One prove induction C0() N , (w, C) P(S, a), (aI , d) wI , C iff0CI .show case C = r.D, leave cases reader. Let C .e DI (d, e) rI . Since (w, C) P(S, a), (wr, D) P(S, a).0Since (aI , d) wI , (aI , e) (wr)I . Thus, e induction hypothesis000yields e DI . definition 0 semantics, C . let C .00e DI (d, e) rI . definition 0 , (d, e) rI . Since (w, C) P(S, a),(wr, D) P(S, a). Since (aI , d) wI , (aI , e) (wr)I . Thus, inductionhypothesis yields e DI .Thus, () established. Using () facts aI0 (C0 )I model00A, hard verify aI0 (C0 )I 0 model A. show0 also model CircCP (, A). Assume contrary model J 0J 0 <CP 0 . Define interpretation J follows:J = ;AJ = AI F ;0AJ = AJ V ;0rJ = rJ r NC ;bJ = bI b NI .Using assumption CP contain fixed role names, hard verifyJ <CP I. obtain contradiction fact model CircCP (, A),thus remains show J model A. end, prove induction C0() N , (w, C) P(S, a), J (aJ , d) wJ , C J iff0CJ .Since induction step proof (), induction start. Thus, letC = NC . V , done definition J . let F . Since 0759fiBonatti, Lutz, & Wolter0000restriction AJ = AI , definition J yields AJ J = AJ , required.finishes proof claim.prove Lemma 5, consider instance checking instead satisfiability. Sinceprovided polynomial reductions satisfiability (non)-instance checking viceversa Section 2, nevertheless obtain desired result. Let CircCP (, A) cKBCP = (, M, F, V ) F = {A1 , . . . , Ak }. Take concept assertion C0 (a0 ). Let= {C0 (a0 )} N set individual names S. Define0 = {A1 , . . . , Ak , A01 , . . . , A0k }, A0i fresh concept names;A0 = {w.(A0i Ai )(a) | w Paths(S, a), N , k};CP0 = (, 0 , , V ).CircCP (, A) |= C0 (a0 ) iff CircCP0 (, A0 ) |= C0 (a0 ) follows immediately Claim 1fact Paths(S, a) = Paths(S 0 , a), 0 = A0 C0 (a0 ).Lemma 6 C0 satisfiable w.r.t. CircCP (T , A) iff C0 u B 0 satisfiable w.r.t. CircCP0 (T 0 , A).Proof. Suppose model C0 CircCP (T , A). Expand interpretation 0setting00000AI = B 0I = , B = A0I = , uI = .Clearly, 0 model C0 u B 0 (T 0 , A). show 0 model CircCP0 (T 0 , A).Assume contrary model J (T 0 , A) J <CP0 0 . A0J = ,AJ = , B J = , B 0 J = . Since u varying J <CP0 0 , thus easyshow J <CP I. contradicts fact model CircCP (T , A).Conversely, suppose model C0 u B 0 CircCP0 (T 0 , A). show alsomodel C0 CircCP (T , A). First observe AI = . supposecase. Define new interpretation J way except uJ = ,B J = , B 0J = . J <CP0 (since B 0I 6= ) J model (T 0 , A). Thusderived contradiction. follows C = hence model (T , A)C0 . remains show J <CP J model (T , A).Assume J exists. C J = . Define model J 0 expanding J follows:00AJ = B 0J = ,00B J = A0J = ,0uJ = .Note A, B, A0 , B 0 , u interpreted way, J 0 <CP0 I.Moreover, J 0 model (T 0 , A). derived contradiction.Lemma 8 L DL, simultaneous satisfiability w.r.t. (concept) circumscribed KBsCircCP1 (T1 , A1 ), . . . CircCPk (Tk , Ak ), CircCPi (Ti , Ai ) CircCPj (Tj , Aj ) sharerole names 1 < j k, reduced satisfiability w.r.t. single (concept)circumscribed KBs polynomial time.Proof. suffices reduce simultaneous satisfiability without shared role names complement instance checking w.r.t. single cKBs. generalization straightforward,give proof case k = 2. Thus, let L DL CircCP1 (T1 , A1 ), CircCP2 (T2 , A2 )760fiThe Complexity Circumscription DLscKBs formulated L share role names, C0 L-concept. Moreover, letA0 , . . . , An1 concept names shared two cKBs, R role names usedleast one two cKBs together fresh role name r0 , CPi = (i , Mi , Fi , Vi ){1, 2}. obtain new TBox T20 T2 replacing concept name Ai , < n,new concept name A0i . Let A02 obtained A2 CP02 = (02 , M20 , F20 , V20 )CP2 way. Define TBox follows, P fresh concept name:Ai u A0i v P < nAi u A0i v P < nPr.Pv r.P r Rv P r Rset::= T1 T20:= A1 A02 {r0 (b1 , b2 ) | b1 , b2 occur A1 A2 T1 T2 }:= 1 02:= M1 M20F:= F1 F20V:= V1 V20 {P, r0 }CP := (, M, F, V )Let a0 individual name A1 (clearly, may assume A1 6= ). remainsshow following:Claim. C0 simultaneously satisfiable w.r.t. CircCP1 (T1 , A1 ) CircCP2 (T2 , A2 ) iff a0instance (P u r0 .C0 ) w.r.t. CircCP (T , A).If. Assume a0 instance (P u r0 .C0 ) w.r.t. CircCP (T , A).model CircSCP (T , A) aI0 (P u r0 .C0 )I . call connected directedgraph GI = (I , rR rI (r )I ) connected. connected component 000restriction domain (maximal) connected component GI .may assume without loss generality chosen model connected:not, use role r0 ensures connected component 0contains bI individual names b A1 A2 T1 T2 . easy see 000model CircCP (T , A) aI0 (P u r0 .C0 )I , thus may simply replace 0 .show C0 simultaneously satisfiable respect CircCP1 (T1 , A1 )CircCP2 (T1 , A2 ). Clearly, model C0 . construction CircCP (T , A),model T1 , A1 . show also model CircCP1 (T1 , A1 ), assumecontrary case. exists model J T1 A1J <CP1 I. Define model J 0 follows:0J = J ;predicates used T1 A1 interpreted J ;predicates used T20 A02 interpreted I.761fiBonatti, Lutz, & Wolter(PJ0:=0((Ai u A0i ) (Ai u A0i ))J 6= < notherwise00r0J = J J .readily checked J 0 model A, J 0 <CP I. Thus,derived contradiction fact model CircCP (T , A), followsmodel CircCP (T1 , A1 ).remains show model CircCP (T2 , A2 ). Since connected, model, satisfies aI0/ P , AIi = (A0i )I < n. Therefore, alsomodel T2 A2 . Analogously case CircCP1 (T1 , A1 ), showmodel CircCP2 (T2 , A2 ).if. Assume C0 simultaneously satisfiable w.r.t. cKBs CircCP1 (T1 , A1 )CircCP2 (T2 , A2 ). exists model C0 model CircCP1 (T1 , A1 )CircCP2 (T2 , A2 ). modify new model 0 setting0(A0i )I := AIi < n;0P := ;0r0I := .00easy see 0 model A, aI (P u r0 .C0 )I . showa0 instance (P u r0 .C0 ) w.r.t. CircCP (T , A), thus remains prove0 also model CircCP (T , A). this, first show following:(a) 0 model CircCP1 (T1 , A1 ). case since model J T1 A1J <CP1 0 satisfies J <CP1 I. Thus, existence model contradictsfact model CircCP1 (T1 , A1 ).(b) 0 model CircCP02 (T20 , A02 ). Assume contrary model J T20:= (A0i )JA02 J <CP02 0 . Convert J interpretation J setting AJ< n. Then, J model T2 A2 satisfies J <CP2 I.contradiction fact model CircCP2 (T2 , A2 ).Now, assume contrary shown model J 0J 0 <CP 0 . definition CP, J 0 <CP 0 implies either J 0 <CP1 0J 0 <CP02 0 hold. Since J 0 clearly satisfies T1 , A1 , T20 , A02 , obtain contradiction(a) (b).Appendix B. Missing Proofs Section 4Lemma 14 G yes-instance co-3CERTCOLS iff CG satisfiable w.r.t. CircCPG (, AG ),CPG = (, M, F, V ) = , = {Root, Leaf, Clash},F = {LeafFix, Tr, X0 , . . . , Xn1 , Y0 , . . . , Yn1 , },V set remaining predicates AG .762fiThe Complexity Circumscription DLsProof. If. Suppose CG satisfiable w.r.t. CircCPG (, AG ) let model6= . show G yes-instance co-CERT3COL .CircCPG (, AG ) CGstart, showI.(I) aI0 CG6= aI .Assume contrary case. CG0, RootI . Let J interpretation obtained settingSince CGRootJ = {aI0 }. definition AG , aI0 RootI , thus J CPG I. Moreover,easily seen J model AG . thus established contradictionfact model CircCPG (, AG ), proves (I).Lines (1)-(4) (Fig. 2), elements di,w (the nodes tree) 2nw {0, 1}iaI0 = d0, ;di,w XjI iff j + 1-st bit w 1, 2n j min{i, n 1};di,w YjI iff n + j + 1-st bit w 1, i, j n < 2n j n;(di,w , di+1,w0 ), (di,w , di+1,w1 ) rI , < 2n;d2n,w Leaf , w {0, 1}2n .i, j < 2n , use `i,j denote element d2n,w w {0, 1}2n denotesbinary encoding followed j. show(II) Leaf = {`i,j | i, j < 2n }.Assume contrary case, i.e., Leaf6= `i,j i, j < 2n . Let id , jd > 0 integers truth values X0 , . . . , Xn1encode id truth values Y0 , . . . , Yn1 encode jd . Starting I, constructinterpretation J settingLeaf JrJ:= Leaf \ {d}:= (rI \ (I d)) {(e, `id ,jd ) | (e, d) rI }modify J J 0 settingPJ0:= (r2n .var1.Leaf)J(r2n .var2.Leaf)J(r2n .col1.Leaf)J(r2n .col2.Leaf)Jgoing Lines (1) (26), straightforward check J 0 model AG .Moreover, clearly J 0 <CPG I, thus obtain contradiction factmodel CircCPG (, AG ). thus shown (II).following easy consequence (I), fact P conjunct CG ,Lines (8), (9), (20), (21):(III) (`i,j , d) rI implies Leaf i, j < 2n , , r {var1, var2, col1, col2}.763fiBonatti, Lutz, & Woltersuppose contrary aim prove G yes-instance. Then,truth assignments t, subgraph t(G) 3-colorable. particular, holdsassignment defined setting, i, j < 2n ,t(Vij ) := 1 iff `i,j TrI .Let c : {0, . . . , 2n 1} {R, G, B} 3-coloring t(G) construct interpretationJ starting applying following modifications:CJClashJ= {`i,0 | < 2n , c(i) = C} C {R, G, B}= .Clearly, J <CPG I: minimized predicate Clash empty J , non-empty sincenon-empty. obtain contradiction, thus suffices show J modelCGAG .Since J agree predicates R, G, B, Clash, inclusionsmention concepts satisfied J . Lines (1) (21). Lines (22) (23)2n Jsatisfied construction J since, due Line (5) (II), (aJ0 , d) (r )nimplies = `i,j i, j < 2 . thus remains consider Lines (24) (26). firstshow(IV) i, j < 2n , (i, j) edge t(G) iff `i,j/ ElimJ .Let i, j < 2n let potential edge (i, j) labeled Vk1 ,k2 Vk3 ,k4 (since(i)(i)circuits cS cj deliver output input, assume also potential,non-existing edges label). (II) (III) together Lines (8) (9),var1J = var1I = {`k1 ,k2 } var2J = var2I = {`k3 ,k4 }. Thus, definition togetherLines (12) (16) yields (i, j) edge t(G) iff `i,j/ ElimI . prove (IV),remains note J interpret concept name Elim way.2n JNow, prove (24) (26) satisfied J . Let (aJ0 , d) (r ) . Line 5Jn(II), = `i,j i, j < 2 . Let `i,j/ Elim . (IV) since c 3-coloringt(G), get c(i) 6= c(j). Thus, construction J , `i,0/ C J `j,0/ C JC {R, G, B}. (II) (III) together Lines (18) (19), col1J = col1I = {`i,0 }col2J = col2I = {`j,0 }. Therefore, `i,j/ (col1.C u col2.C)J C {R, G, B}.JSince holds `i,j/ Elim , preconditions implications Lines (24)(26) never satisfied. Thus, implications satisfied.if. Suppose G yes-instance let truth assignmentt(G) 3-colorable. Let c : {0, . . . , 2n 1} {R, G, B} color assignmentminimizes (w.r.t. set inclusion) set {(i, j) | (i, j) edge G c(i) = c(j)}. Defineinterpretation follows (here following, distinguishnumber binary encoding string):764fiThe Complexity Circumscription DLsRootI= {di,w | 2n , w {0, 1}i }= {d0, }= {d2n,w | w {0, 1}2n }LeafFixI= {d2n,w | w {0, 1}2n }LeafXjI= {di,w | j + 1-st bit w 1, 2n, j min{i, n 1}};YjI= {di,w | n + j + 1-st bit w 1, n < 2n, j n};Tr= {d2n,ij | t(Vij ) = 1}TrI`= {d2n,ij | t(Vij ) cS (i, j)}(`)(` = 1, 2)Elim= {d2n,ij | (i, j) edge t(G)}C= {d2n,i0 | c(i) = C} (C = R, G, B)= {d2n,ij | (i, j) edge t(G) c(i) = c(j)}r= {(di,w , di+1,w0 ), (di,w , di+1,w1 ) | < 2n}var1= {(d2n,ij , d2n,kl ) | first variable label (i, j) Vkl }var2= {(d2n,ij , d2n,kl ) | second variable label (i, j) Vkl }col1= {(d2n,ij , d2n,i0 ) | < 2n }col2I= {(d2n,ij , d2n,j0 ) | < 2n }ClashPI=aI0= d0, .Boolean circuit c, corresponding output concept name Outc interpretedOutIc := {d2n,ij | i, j < 2n c(i, j) true}.show CG satisfiable w.r.t. CircCPG (, AG ), suffices show aI0 CGmodel CircCPG (, AG ). former easy: recall CG = RootuP ur2n .Clash.definition I, aI0 (Root u P )I . Since c 3-coloring, aI0 (r2n .Clash)I .thus remains show model CircCPG (, AG ). Since easy verifymodel AG , boils showing model J AG J <CPG I.Assume contrary J . Lines (1)(5) since J modelAG , #Leaf J 22n . Since #Leaf = 22n J <CPG I, get Leaf J = Leaf .similar simpler reasons, RootJ = RootI . Thus, J <CPG implies ClashJ ( ClashI .Lines (1) (5) since J models AG Leaf J = Leaf #Leaf =#Leaf J = 22n ,J2n J(I) {d | (aI0 , d) (r2n )I } = {d J | (aJ0 , d) (r ) } = Leaf = LeafThus, Lines (24) (26) fact J model AG ensure(II) C{R,G,B} (Leaf u Elim u col1.C u col2.C)J ClashJ .Define coloring c0 settingc0 (i) = C iff d2n,i0 C JSuppose765(C = R, G, B).fiBonatti, Lutz, & Wolter(III) (a) ElimI Leaf = ElimJ Leaf ,(b) col1I (Leaf Leaf ) = col1J (Leaf J Leaf J ),(c) col2I (Leaf Leaf ) = col2J (Leaf J Leaf J ).(II) guarantees (i, j) edge G c0 (i) = c0 (j), d2n,ij ClashJ .Since ClashJ ( ClashI , get1. c0 (i) = c0 (j), c(i) = c(j);2. i, j c0 (i) 6= c0 (j), c(i) = c(j).contradicts initial minimality assumption coloring c.thus remains prove (III). start (a). Assume(d) var1I (Leaf Leaf ) = var1J (Leaf J Leaf J )(e) var2I (Leaf Leaf ) = var2J (Leaf J Leaf J ).Then, Lines (12) (16) together (I) fact TrI = TrJ implies (a). thusremains prove (b) (e). concentrate (b) cases analogous. Take(d, d0 ) col1I (Leaf Leaf ). = d2n,ij d0 = d2n,i0 j 0 i, j, i0 , j 0 < 2n .Line (18), i0 = j 0 = 0. (I) Lines (17) (18) since Jagree interpretation X0 , . . . , Xn1 , Y0 , . . . , Yn1 , e LeafFixJ(d2n,ij , e) col1J , value encoded X0 , . . . , Xn1 e J i, valueencoded Y0 , . . . , Yn1 e J 0. Since LeafFixI = LeafFixJ , LeafFixJ =Leaf J . However, single element Leaf J X0 , . . . , Xn1 encodesY0 , . . . , Yn1 encodes 0 d2n,i0 = d0 . converse direction analogous.Corollary 16 ALC, satisfiability w.r.t. concept-circumscribed KBs NExpNP -hardeven TBox acyclic, ABox preference relations empty,fixed predicates.Proof. ABox AG reduction given Section 4.2.1 form {C0 (a0 )}circumscription pattern CPG empty preference relation. thus sufficesshow polynomial reduction satisfiability w.r.t. concept-circumscribed KBsform satisfiability w.r.t. concept-circumscribed KBs acyclic TBox, emptyABox preference relation, fixed predicates.Let CircCP (, A) concept-circumscribed KB = {C0 (a0 )} CP = (, M, V, F ) = , let C ALC concept. Define = {A v u.C0 },concept name occur C u role nameoccur C. Also define CP0 = (, M, V {u}, F {A}). C satisfiable w.r.t.CircCP (, A) iff u C satisfiable w.r.t. CircCP0 (T , ):If. u C satisfiable w.r.t. CircCP0 (T , ), model CircCP0 (T , )d0 (A u C)I . Thus, e0 C0I . Modify obtain new interpretationJ setting aJ0 = e0 . Clearly, J model C. show also modelCircCP (, A), assume contrary model J 0 J 0 <CP J . Modify00J 0 interpretation 0 setting AI = AI uI = . readily checked0 model 0 <CP0 I, thus obtained contradiction factmodel CircCP0 (T , ).766fiThe Complexity Circumscription DLsif. C satisfiable w.r.t. CircCP (, A), model CircCP (, A)d0 C . Let J defined I, except AJ = {d0 } uJ = . Clearly,J model u C. show J also model CircCP (T , ), assume0contrary model J 0 J 0 <CP0 J . Since fixed CP0 , d0 AI ,00thus d0 (u.C0 )J e0 C0J . Modify J 0 new interpretation 00setting aI0 = e0 . readily checked 0 model 0 <CP I, thusobtained contradiction fact model CircCP0 (, A). get rid fixedpredicates, suffices apply Lemma 5.Appendix C. Missing Proofs Section 5show semantic consequence problem reduced instance checkingw.r.t. role-fixing cKBs ALC. already proved ALC extendeduniversal role. fact, remaining problem approximate concepts u.C using ALC concepts state extension C contains points within certain,sufficiently large neighbourhood.construct approximation, first introduce local version notionframe validity concepts. pointed R-frame pair (F, d) F FR-frame. concept C valid pointed R-frame (F, d), symbols (F, d) |= C, iffC every interpretation based F. R finite set role names,R.C denotes C = 0,Nm1 R.C uu r.m1rRR.C> 0. follows, use concepts form R.C approximationsu.C. remind reader correspondence results modal logic. Let transA =s.A s.s.A contA = s.A r.A. well known (Blackburn & vanBenthem, 2007) easy prove every {r, s}-frame F, following holds:transA valid F if, if, sF transitive;contA valid F if, if, rF sF .Say d0 F {r, s}-reachable F (d, d0 ) (rF sF ) call Froot F every d0 F {r, s}-reachable F. F {r, s}-frame rootd, following conditions equivalent:1 {r, s}.(transA u contA ) valid (F, d);sF transitive rF sF .observations used proof Lemma 31 below. before, sometimes writeconcept assertions C(a) form : C. Recall role depth rd(C) concept Cdefined nesting depth constructors r.D r.D, r R, C.Lemma 31 Let C ALC {r} -concepts sharing concept names letfresh concept name. Let CP = (, M, {r, s}, V ) circumscription pattern,consists concept names C V consists concept names D. Letindividual name. following conditions equivalent:767fiBonatti, Lutz, & Wolter1. {r, s}-frames F rF sF sF transitive: F |= C, F |= D;2. pointed {r, s}-frames (F, d):(F, d) |= 1 {r, s}.(transA u contA u C)(F, d) |=3. instance 1 {r, s}.(transA u contA u C) w.r.t. CircCP (, A),= {a : (1 {r, s}.(transA u contA u C) 1+max {2,rd(C)} {r, s}.u B)}.BMProof. Point 1 implies Point 2. Suppose Point 2 hold. Let (F, d) pointed{r, s}-frame (F, d) |= 1 {r, s}.(transA u contA u C) (F, d) 6|= D. mayassume root F. (F, d) |= 1 {r, s}.(transA u contA ) obtain rF sFsF transitive. Therefore, (F, d) |= 1 {s}.C obtain F |= C. follows Fframe refuting Point 1.follows use, every , d,I denote set e{r, s}-reachable 1 + max {2, rd(C)} steps.Point 2 implies Point 3. Suppose Point 3 hold. Let model CircCP (, A)aI (1 {r, s}.(transA u contA u C) u D)I .(77)Let based F set := aI . show (F, d) |= 1 {r, s}.(transA u contA u C)(F, d) 6|= D. latter easy witnessed interpretation I. show former,let J interpretation based F. show (1 {r, s}.(transA u contA u C))J .(77) since model CircCP (, A), aI (1+max {2,rd(C)} {r, s}.B)I .uBMfollows immediatelyB = d,I ,(78)B . distinguish two cases:B J d,I , B . Since J based frame conceptnames contA , transA , C , truth 1 {r, s}.(transA u contA u C)depends truth value concept names d,I only. (78)obtain B d,I = B J d,I , B . Hence, (77), (1 {r, s}.(transA ucontA u C))J , required.B J 6 d,I , least one B .0Let J 0 modification J B J = B J d,I , B . (78), J 0 <CP0I. (1 {r, s}.(transA u contA u C))J , J 0 modelcontradiction fact model CircCP (, A). Thus, (1 {r, s}.(transA u0contA u C))J . Since, again, truth 1 {r, s}.(transA u contA u C) dependstruth value B d,I only, (1 {r, s}.(transA u contA u C))J ,required.768fiThe Complexity Circumscription DLsPoint 3 implies Point 1. Suppose Point 1 hold. Consider frame F sFtransitive, rF sF , F |= C, F 6|= D. follows F |= transA u contA . Letinterpretation based F (D)I . may assume root F.may also assume B = d,I B (since B occurs D) aI = d.aI (1 {r, s}.(transA u contA u C) u D))I model A. remains showexist 0 <CP0aI (1 {r, s}.(transA u contA u C) 1+max {2,rd(C)} {r, s}.u B)I0BM.straightforward: (F, d) |= 1 {r, s}.C, obtain exist000 (1 {r, s}.C)I clearly exist B B B0(1+max {2,rd(C)} {r, s}.B)I .position prove reduction ALC.Theorem 22 logical consequence problem MSO(r) effectively reducibleinstance problem w.r.t. role-fixing cKBs formulated ALC. even holdsTBox preference relation empty.Proof. Theorem 20, logical consequence problem MSO(r) effectively reduciblemodal consequence problem ALC {r} -concepts. Hence, suffices reducemodal consequence problem ALC {r} -concepts. Let ALC {r} -concepts C given.may assume C concept names common (if have, replaceevery concept name B new concept name B 0 denote resulting conceptD0 ; noted above, C iff C D0 .) Let CP = (, M, {s, r}, V ) = ,consists concept names C, V consists concept names D. Let= {a : (1 {r, s}.(transA u contA u C) 1+max {2,rd(C)} {r, s}.u B)}BMC0 = 1 {r, s}.(transA u contA u C) D. equivalence Point 1 Point 3Lemma 31, CircCP (, A) |= C0 (a) if, if, frames F rF sF sFtransitive, F |= C follows F |= D. C contain s,CircCP (, A) |= C0 (a)C D.ReferencesAreces, C., Blackburn, P., & Marx, M. (1999). road-map complexity hybrid logics.Proceedings Eighth Annual Conference EACSL (CSL99), No. 1683Lecture Notes Computer Science, pp. 307321. Springer-Verlag.Areces, C., Blackburn, P., & Marx, M. (2000). computational complexity hybridtemporal logics. Logic Journal IGPL, 8 (5), 653679.Baader, F., Brandt, S., & Lutz, C. (2005a). Pushing EL envelope. Kaelbling, L. P.,& Saffiotti, A. (Eds.), Proceedings Nineteenth International Joint ConferenceArtificial Intelligence (IJCAI05), pp. 364369. Professional Book Center.769fiBonatti, Lutz, & WolterBaader, F., Milicic, M., Lutz, C., Sattler, U., & Wolter, F. (2005b). Integrating description logics action formalisms reasoning web services. LTCSreport LTCS-05-02, Chair Automata Theory, Institute Theoretical Computer Science, Dresden University Technology, Germany. See http://lat.inf.tudresden.de/research/reports.html.Baader, F., & Hollunder, B. (1995a). Embedding defaults terminological knowledgerepresentation formalisms.. Journal Automated Reasoning, 14 (1), 149180.Baader, F., & Hollunder, B. (1995b). Priorities defaults prerequisites, application treating specificity terminological default logic.. Journal AutomatedReasoning, 15 (1), 4168.Baader, F., McGuiness, D. L., Nardi, D., & Patel-Schneider, P. (2003). DescriptionLogic Handbook: Theory, implementation applications. Cambridge UniversityPress.Baader, F., & Sattler, U. (2000). Tableau algorithms description logics. Dyckhoff,R. (Ed.), Proceedings International Conference Automated ReasoningTableaux Related Methods (Tableaux2000), Vol. 1847 Lecture Notes ArtificialIntelligence, pp. 118. Springer-Verlag.Berger, R. (1966). undecidability dominoe problem. Memoirs AmericanMathematical Society, 66.Blackburn, P., & van Benthem, J. (2007). Modal logic: semantic perspective. HandbookModal Logic. Elsevier.Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal Logic. Cambridge UniversityPress.Bonatti, P., Faella, M., & Sauro, L. (2009). Defeasible inclusions low-complexity DLs:Preliminary notes. Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI09). AAAI Press.Bonatti, P., Lutz, C., & Wolter, F. (2006). Expressive non-monotonic description logicsbased circumscription. Proceedings Tenth International ConferencePrinciples Knowledge Representation Reasoning (KR06), pp. 400410. AAAIPress.Bonatti, P. A., & Eiter, T. (1996). Querying disjunctive databases nonmonotoniclogics.. Theoretical Computer Science, 160 (1&2), 321363.Bonatti, P. A., & Samarati, P. (2003). Logics authorization security. LogicsEmerging Applications Databases, pp. 277323. Springer-Verlag.Borgida, A. (1996). relative expressiveness description logics predicate logics.Artificial Intelligence, 82 (1 - 2), 353367.Brewka, G. (1987). logic inheritance frame systems. Proceedings 10thInternational Joint Conference Artificial Intelligence (IJCAI87), pp. 483488. Morgan Kaufmann.770fiThe Complexity Circumscription DLsBrewka, G. (1994). Adding priorities specificity default logic.. ProceedingsLogics Artificial Intelligence (JELIA94), Vol. 838 Lecture Notes ComputerScience, pp. 247260. Springer-Verlag.Cadoli, M., Donini, F., & Schaerf, M. (1990). Closed world reasoning hybrid systems.Proceedings 6th International Symposium Methodologies IntelligentSystems (ISMIS90), pp. 474481. Elsevier.Calvanese, D., Giacomo, G. D., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractablereasoning efficient query answering description logics: DL-lite family. Journal Automated Reasoning, 39 (3), 385429.Cote, R., Rothwell, D., Palotay, J., Beckett, R., & Brochu, L. (1993). systematizednomenclature human veterinary medicine. Tech. rep., SNOMED International,Northfield, IL: College American Pathologists.de Kleer, J., & Konolige, K. (1989). Eliminating fixed predicates circumscription.Artificial Intelligence, 39 (3), 391398.Donini, F. M., Lenzerini, M., Nardi, D., Nutt, W., & Schaerf, A. (1998). epistemicoperator description logics.. Artificial Intelligence, 100 (1-2), 225274.Donini, F. M., Nardi, D., & Rosati, R. (1997). Autoepistemic description logics. Proceedings Fifteenth International Joint Conference Artificial Intelligence (IJCAI97), pp. 136141. Morgan Kaufmann.Donini, F. M., Nardi, D., & Rosati, R. (2002). Description logics minimal knowledgenegation failure. ACM Transactions Computational Logic, 3 (2), 177225.Eiter, T., Gottlob, G., & Mannila, H. (1997). Disjunctive Datalog. ACM TransactionsDatabase Systems, 22 (3), 364418.Eiter, T., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2004). Combining answer setprogramming description logics semantic web. ProceedingsNinth International Conference Principles Knowledge RepresentationReasoning (KR 2004), pp. 141151.Giordano, L., Gliozzi, V., Olivetti, N., & Pozzato, G. L. (2008). Reasoning typicalitypreferential description logics. Proceedings Logics Artificial Intelligence(JELIA08), Vol. 5293 Lecture Notes Computer Science, pp. 192205. SpringerVerlag.Goldblatt, R. (2003). Mathematical modal logic: view evolution. Journal AppliedLogic, 1, 309392.Gradel, E., Otto, M., & Rosen, E. (1997). Two-Variable Logic Counting Decidable.Proceedings Twelfth IEEE Symposium Logic Computer Science (LICS97),pp. 306317. IEEE Computer Society Press.Grimm, S., & Hitzler, P. (2008). Defeasible inference circumscriptive OWL ontologies.Proceedings Workshop Advancing Reasoning Web: ScalabilityCommonsense, No. 350 CEUR-WS (http://ceur-ws.org/).Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning expressive description logics. Logic Journal IGPL, 8 (3), 239264.771fiBonatti, Lutz, & WolterHorty, J. F. (1994). direct theories nonmonotonoc inheritance. HandbookLogic Artificial Intelligence Logic Programming-Nonmonotonic ReasoningUncertain Reasoning(Volume 3), pp. 111187. Clarendon Press.Janhunen, T. (1999). intertranslatability non-monotonic logics. Annals Mathematics Artificial Intelligence, 27 (1-4), 79128.Kagal, L., Finin, T., & Joshi, A. (2003). policy language pervasive computingenvironment. Fourth IEEE International Workshop Policies DistributedSystems Networks (POLICY2003).Knorr, M., Alferes, J. J., & Hitzler, P. (2007). well-founded semantics hybrid MKNFknowledge bases. Proceedings 2007 International Workshop DescriptionLogics (DL2007), No. 250 CEUR-WS (http://ceur-ws.org/).Konev, B., Lutz, C., Walther, D., & Wolter, F. (2008). Semantic modularity module extraction description logics. Proceedings 18th European ConferenceArtificial Intelligence (ECAI), pp. 5559.Lambrix, P., Shahmehri, N., & Wahlloef, N. (1998). default extension descriptionlogics use intelligent search engine. : Proceedings Thirty-FirstAnnual Hawaii International Conference System Sciences (HICSS98)-Volume 5,p. 2835. IEEE Computer Society.Lifschitz, V. (1985). Computing circumscription. Proceedings Ninth International Joint Conference Artificial Intelligence (IJCAI85), pp. 121127. MorganKaufmann.Lifschitz, V. (1993). Circumscription. Handbook Logic AI Logic Programming 3, pp. 298352. Oxford University Press.Lifschitz, V. (1995). Nested abnormality theories. Artificial Intelligence, 74 (2), 351365.McCarthy, J. (1980). Circumscription: form nonmonotonic reasoning. Artificial Intelligence, 13, 2739.McCarthy, J. (1986). Applications circumscription formalizing common sense knowledge. Artificial Intelligence, 28, 89116.Minsky, M. (1975). framework representating knowledge. Winston, P. H. (Ed.),Psychology Computer Vision, pp. 211277. McGraw-Hill.Moore, R. C. (1985). Semantical considerations nonmonotonic logics. Artificial Intelligence, 25, 7594.Motik, B., & Rosati, R. (2007). Faithful Integration Description Logics LogicProgramming. Proceedings Twentieth International Joint ConferenceArtificial Intelligence (IJCAI2007), pp. 477482. Morgan Kaufmann.Pacholski, L., Szwast, W., & Tendera, L. (2000). Complexity results first-order twovariable logic counting. SIAM Journal Computing, 29 (4), 10831117.Padgham, L., & Zhang, T. (1993). terminological logic defaults: definitionapplication. Proceedings Thirteenth International Joint ConferenceArtificial Intelligence (IJCAI93), pp. 662668. Morgan Kaufmann.772fiThe Complexity Circumscription DLsPratt-Hartmann, I. (2005). Complexity two-variable fragment counting quantifiers. Journal Logic, Language, Information, 14 (3), 369395.Quillian, M. R. (1968). Semantic memory. Semantic Information Processing, pp. 227270.MIT Press.Rector, A. (2004). Defaults, context, knowledge: Alternatives OWL-indexed knowledge bases. Proceedings Pacific Symposium Biocomputing (PSB04), pp.226237. World Scientific.Rector, A., & Horrocks, I. (1997). Experience building large, re-usable medical ontologyusing description logic transitivity concept inclusions. ProceedingsWorkshop Ontological Engineering, AAAI Spring Symposium. AAAI Press.Reiter, R. (1980). logic default reasoning. Artificial Intelligence, 13, 81132.Stevens, R., Aranguren, M. E., Wolstencroft, K., Sattler, U., Drummond, N., Horridge,M., & Rector, A. (2005). Using OWL model biological knowledge. InternationalJournal Man-Machine Studies, 65 (7), 583594.Stewart, I. A. (1991). Complete problems involving Boolean labelled structures projection transactions. Journal Logic Computation, 1 (6), 861882.Straccia, U. (1993). Default inheritance reasoning hybrid KL-ONE-style logics..Proceedings Thirteenth International Joint Conference Artificial Intelligence(IJCAI93), pp. 676681. Morgan Kaufmann.Thomason, S. (1975a). logical consequence relation propositional tense logic.Zeitschrift fur Mathematische Logik und Grundlagen der Mathematik, 21, 2940.Thomason, S. (1975b). Reduction second-order logic modal logic. Zeitschrift furMathematische Logik und Grundlagen der Mathematik, 21, 107114.Tobies, S. (2000). complexity reasoning cardinality restrictions nominalsexpressive description logics. Journal Artificial Intelligence Research, 12, 199217.Tonti, G., Bradshaw, J. M., Jeffers, R., Montanari, R., Suri, N., & Uszok, A. (2003). Semantic web languages policy representation reasoning: comparison KAoS,Rei, Ponder. Proceedings Second International Semantic Web Conference(ISWC03), Vol. 2870 Lecture Notes Computer Science, pp. 419437. SpringerVerlag.Uszok, A., Bradshaw, J. M., Johnson, M., Jeffers, R., Tate, A., Dalton, J., & Aitken, S.(2004). KAoS policy management semantic web services. IEEE Intelligent Systems,19 (4), 3241.Wolter, F., & Zakharyaschev, M. (2007). Modal decision problems. Handbook ModalLogic. Elsevier.Y. Ding, V. H., & Wu, J. (2007). new mapping ALCI ALC. Proceedings2007 International Workshop Description Logics (DL2007), No. 250 CEURWS (http://ceur-ws.org/).773fiJournal Artificial Intelligence Research 35 (2009) 193-234Submitted 4/08; published 6/09Transductive Rademacher Complexity ApplicationsRan El-YanivDmitry Pechyonyrani@cs.technion.ac.ilpechyony@cs.technion.ac.ilDepartment Computer ScienceTechnion - Israel Institute TechnologyHaifa, 32000, IsraelAbstractdevelop technique deriving data-dependent error bounds transductivelearning algorithms based transductive Rademacher complexity. technique basednovel general error bound transduction terms transductive Rademacher complexity, together novel bounding technique Rademacher averages particularalgorithms, terms unlabeled-labeled representation. technique relevantmany advanced graph-based transductive algorithms demonstrate effectiveness deriving error bounds three well known algorithms. Finally, present newPAC-Bayesian bound mixtures transductive algorithms based Rademacherbounds.1. IntroductionAlternative learning models utilize unlabeled data received considerable attentionpast years. Two prominent models semi-supervised transductive learning. main attraction models theoretical empirical evidence (Chapelle,Scholkopf, & Zien, 2006) indicating often allow efficient significantly faster learning terms sample complexity. paper support theoreticalevidence providing risk bounds number state-of-the-art transductive algorithms.bounds utilize labeled unlabeled examples much tighterbounds relying labeled examples alone.focus distribution-free transductive learning. setting givenlabeled training sample well unlabeled test sample. goal guess labelsgiven test points accurately possible1 . Rather generating general hypothesiscapable predicting label point, inductive learning, advocatedVapnik (1982) aim transduction solve easier problem transferringknowledge directly labeled points unlabeled ones.Transductive learning already proposed briefly studied thirty yearsago Vapnik Chervonenkis (1974), lately empirically recognizedtransduction often facilitate efficient accurate learning traditional supervised learning approach (Chapelle et al., 2006). recognition motivatedflurry recent activity focusing transductive learning, many new algorithms1. Many papers refer model semi-supervised learning. However, setting semi-supervisedlearning different transduction. semi-supervised learning learner given randomly drawntraining set consisting labeled unlabeled examples. goal learner generatehypothesis providing accurate predictions unseen examples.c2009AI Access Foundation. rights reserved.fiEl-Yaniv & Pechyonyheuristics proposed. Nevertheless, issues identification universally effective learning principles transduction remain unresolved. Statistical learningtheory provides principled approach attacking questions studyerror bounds. example, inductive learning bounds proven instrumentalcharacterizing learning principles deriving practical algorithms (Vapnik, 2000).paper consider classification setting transductive learning. far,several general error bounds transductive classification developed Vapnik (1982), Blum Langford (2003), Derbeko, El-Yaniv, Meir (2004), El-YanivPechyony (2006). continue fruitful line research develop new techniquederiving explicit data-dependent error bounds. bounds less tight implicit ones, developed Vapnik Blum Langford. However explicit boundsmay potentially used model selection guide development new learningalgorithms.technique consists two parts. first part develop novel general errorbound transduction terms transductive Rademacher complexity. boundsyntactically similar known inductive Rademacher bounds (see, e.g., Bartlett & Mendelson, 2002), fundamentally different sense transductive Rademacher complexity computed respect hypothesis space chosen observingunlabeled training test examples. opportunity unavailable inductivesetting hypothesis space must fixed example observed.second part bounding technique generic method bounding Rademachercomplexity transductive algorithms based unlabeled-labeled representation (ULR).representation soft-classification vector generated algorithm productU , U matrix depends unlabeled data vector maydepend given information, including labeled training set. transductive algorithm infinite number ULRs, including trivial ULR, U identitymatrix. show many state-of-the-art algorithms non-trivial ULR leadingnon-trivial error bounds. Based ULR representation bound Rademacher complexitytransductive algorithms terms spectrum matrix U ULR.bound justifies spectral transformations, developed Chapelle, Weston, Scholkopf(2003), Joachims (2003), Johnson Zhang (2008), commonly done improveperformance transductive algorithms. instantiate Rademacher complexitybound consistency method Zhou et al. (2004), spectral graph transducer(SGT) algorithm Joachims (2003) Tikhonov regularization algorithm Belkin,Matveeva, Niyogi (2004). bounds obtained algorithms expliciteasily computed.also show simple Monte-Carlo scheme bounding Rademacher complexitytransductive algorithm using ULR. demonstrate efficacy schemeconsistency method Zhou et al. (2004). final contribution PAC-Bayesian boundtransductive mixture algorithms. result, stated Theorem 4, obtainedconsequence Theorem 2 using techniques Meir Zhang (2003). resultmotivates use ensemble methods transduction yet exploredsetting.paper following structure. Section 1.1 survey resultsclosely related work. Section 2 define learning model transductive2fiTransductive Rademacher Complexity ApplicationsRademacher complexity. Section 3 develop novel concentration inequality functions partitions finite set points. inequality transductive Rademachercomplexity used Section 4 derive uniform risk bound, depends transductive Rademacher complexity. Section 5 introduce generic method boundingRademacher complexity transductive algorithm using unlabeled-labeled representation. Section 6 exemplify technique obtain explicit risk bounds severalknown transductive algorithms. Finally, Section 7 instantiate risk boundtransductive mixture algorithms. discuss directions future research Section 8.technical proofs results presented Appendices A-I.Preliminary (and shorter) version paper appeared Proceedings20th Annual Conference Learning Theory, page 157171, 2007.1.1 Related WorkVapnik (1982) presented first general 0/1 loss bounds transductive classification.bounds implicit sense tail probabilities specified boundoutcome computational routine. Vapniks bounds refined include priorbeliefs noted Derbeko et al. (2004). Similar implicit somewhat tighter boundsdeveloped Blum Langford (2003) 0/1 loss case. Explicit PAC-Bayesiantransductive bounds bounded loss function presented Derbeko et al. (2004).Catoni (2004, 2007) Audibert (2004) developed PAC-Bayesian VC dimensionbased risk bounds special case size test set multiplesize training set. Unlike PAC-Bayesian bound, published transductive PACBayesian bounds hold deterministic hypotheses Gibbs classifiers. boundsBalcan Blum (2006) semi-supervised learning also hold transductive setting,making conceptually similar transductive PAC-Bayesian bounds. Generalerror bounds based stability developed El-Yaniv Pechyony (2006).Effective applications general bounds mentioned particular algorithmslearning principles automatic. case PAC-Bayesian bounds severalsuccessful applications presented terms appropriate priors promotevarious structural properties data (see, e.g., Derbeko et al., 2004; El-Yaniv & Gerzon,2005; Hanneke, 2006). Ad-hoc bounds particular algorithms developed Belkinet al. (2004) Johnson Zhang (2007). Unlike bounds (including ours)bound Johnson Zhang depend empirical errorproperties hypothesis space. size training test set increasesbound converges zero2 . Thus bound Johnson Zhang effectively provesconsistency transductive algorithms consider. However bound holdshyperparameters algorithms chosen w.r.t. unknown test labels.Hence bound Johnson Zhang cannot computed explicitly.Error bounds based Rademacher complexity introduced Koltchinskii (2001)well-established topic induction (see Bartlett & Mendelson, 2002, referencestherein). first Rademacher transductive risk bound presented Lanckriet et al.(2004, Theorem 24). bound, straightforward extension inductive2. known explicit bounds increase training test sets decreases slack termempirical error.3fiEl-Yaniv & PechyonyRademacher techniques Bartlett Mendelson (2002), limited special casetraining test sets equal size. bound presented overcomeslimitation.2. DefinitionsSection 2.1 provide formal definition learning model. Section 2.2define transductive Rademacher complexity compare inductive counterpart.2.1 Learning Modelpaper use distribution-free transductive model, defined Vapnik (1982,Section 10.1, Setting 1). Consider fixed set Sm+u = {(xi , yi )}m+ui=1 + u points xispace together labels yi . learner provided (unlabeled)full-sample Xm+u = {xi }m+ui=1 . set consisting points selected Xm+u uniformlyrandom among subsets size m. points together labels givenlearner training set. Re-numbering points denote unlabeled trainingset points Xm = {x1 , . . . , xm } labeled training set Sm = {(xi , yi )}mi=1 .set unlabeled points Xu = {xm+1 , . . . , xm+u } = Xm+u \ Xm called test set.learners goal predict labels test points Xu based Sm Xu .Remark 1 learner model example xi unique label yi . However allow6= j, xi = xj yi 6= yj .choice set points described viewed three equivalentways:1. Drawing points Xm+u uniformly without replacement. Due draw,points training test sets dependent.2. Random permutation full sample Xm+u choosing first pointstraining set.3. Random partitioning + u points two disjoint sets u points.emphasize different aspects transductive learning model, throughout paperuse interchangeably three views generation training test sets.paper focuses binary learning problems labels {1}. learningalgorithms consider generate soft classification vectors h = (h(1), . . . h(m + u))Rm+u , h(i) (or h(xi )) soft, confidence-rated, label example xi givenhypothesis h. actual (binary) classification xi algorithm outputs sgn(h(i)).denote Hout Rm+u set possible soft classification vectors (over possibletranining/test partitions) generated algorithm.Based full-sample Xm+u , algorithm selects hypothesis space H Rm+usoft classification hypotheses. Note Hout H. Then, given labels trainingpoints algorithm outputs one hypothesis h Hout H classification. goaltransductive learner find hypothesis h minimizing test error Lu (h) =4fiTransductive Rademacher Complexity Applications1 Pm+ui=m+1 `(h(i), yi ) w.r.t.u1 Pmi=1 `(h(i), yi )0/1 loss function `. empirical error h Lbm (h) =Pm+u1full sample error h Lm+u (h) = m+ui=1 `(h(i), yi ).work also use margin loss function ` . positive real , ` (y1 , y2 ) = 0y1 y2 ` (y1 , y2 ) = min{1, 1 y1 y2 /} otherwise. empirical (margin) error1 Pmh Lbm (h) =i=1 ` (h(i), yi ). denote Lu (h) margin error test setLm+u (h) margin full sample error.denote Irs , r < s, set natural numbers {r, r + 1, . . . , s}. Throughoutpaper assume vectors column ones. mark vectors boldface.2.2 Transductive Rademacher Complexityadapt inductive Rademacher complexity transductive setting generalizebit also include neutral Rademacher values.Definition 1 (Transductive Rademacher complexity) Let V Rm+u p [0, 1/2].Let = (1 , . . . , m+u )T vector i.i.d. random variables1= 10probabilityprobabilityprobabilityp;p;1 2p.(1)transductive Rademacher complexity parameter p11Rm+u (V, p) =+E sup v .uvVneed novel definition Rademacher complexity technical. Two mainissues lead new definition are:P1. need bound test error Lu (h) = u1 m+ui=m+1 `(h(i), yi ). Notice inductive risk bounds standard definition Rademacher complexity (see Definition 2below), binary values , used bound generalizationerror,1 Pm+uinductive analogue full sample error Lm+u (h) = m+u`(h(i),yi ).i=12. Different sizes (m u respectively) training test set.See Section 4.1 technical details lead definition Rademachercomplexity.sake comparison also state inductive definition Rademacher complexity.Definition 2 (Inductive Rademacher complexity, Koltchinskii, 2001) Letprobability distribution X . Suppose examples Xn = {xi }ni=1 sampled independently X according D. Let F class functions mapping X R. Let= {i }ni=1 independent uniform {1}-valued random variables, = 1 probability 1/2 = 1 probability. empirical Rademacher complex5fiEl-Yaniv & Pechyony(ind)Rn (F)Pn2f (xi )nnE supf Fi=1(ind)bn (F) .= EXn Dn Rbn(ind) (F) =ity is3 RRademacher complexity Fb(ind) (V).case p = 1/2, = u n = + u Rm+u (V) = 2Rm+uWhenever p < 1/2, Rademacher variables attain (neutral) zero values reducecomplexity (see Lemma 1). use property tighten bounds.Notice transductive complexity empirical quantity dependunderlying distribution, including one choices training set. Sincedistribution-free transductive model unlabeled full sample training test pointsfixed, transductive Rademacher complexity dont need outer expectation,appears inductive definition. Also, transductive complexity depends(unlabeled) training test points whereas inductive complexity depends(unlabeled) training points.following lemma, whose proof appears Appendix A, states Rm+u (V, p)monotone increasing p. proof based technique used proofLemma 5 paper Meir Zhang (2003).Lemma 1 V Rm+u 0 p1 < p2 1/2, Rm+u (V, p1 ) < Rm+u (V, p2 ).forthcoming results utilize transductive Rademacher complexitymup0 = (m+u)2 . abbreviate Rm+u (V) = Rm+u (V, p0 ). Lemma 1, bounds alsoapply Rm+u (V, p) p > p0 . Since p0 < 21 , Rademacher complexity involvedresults strictly smaller standard inductive Rademacher complexity definedXm+u . Also, transduction approaches induction, namely fixed u ,b(ind) (V) 2Rm+u (V).Rm+u3. Concentration Inequalities Functions Partitionssection develop novel concentration inequality functions partitionscompare several known ones. concentration inequality utilizedderivation forthcoming risk bound.Let Z = Zm+u= (Z1 , . . . , Zm+u ) random permutation vector variable1Zk , k I1m+u , kth component permutation I1m+u chosen uniformlyrandom. Let Zij perturbed permutation vector obtained exchanging values ZiZj Z. function f permutations I1m+u called (m, u)-permutation symmetMric f (Z) = f (Z1 , . . . , Zm+u ) symmetric Z1 , . . . , Zm well Zm+1 , . . . , Zm+u .section present novel concentration inequality (m, u)-permutation symmetric functions. Note (m, u)-permutation symmetric function essentially function partition + u items sets sizes u. Thus, forthcominginequalities Lemmas 2 3, stated (m, u)-permutation symmetric functions, also hold exactly form functions partitions. Conceptually3. original definition Rademacher complexity,Pgiven Koltchinskii(2001),Pslightly differentnone presented here, contains supf F ni=1 f (xi ) instead supf Fi=1 f (xi ). However, conceptual point view, Definition 2 one given Koltchinskii equivalent.6fiTransductive Rademacher Complexity Applicationsconvenient view results concentration inequalities functions partitions. However, technical point view find convenient consider(m, u)-permutation symmetric functions.following lemma (that utilized proof Theorem 1) presents concentration inequality extension Lemma 2 El-Yaniv Pechyony (2006).proof (appearing Appendix B) relies McDiarmids inequality (McDiarmid, 1989,Corollary 6.10) martingales.m+uLemma 2 Let Z random permutation(m, u) vectorI1 . Let f (Z)m+uijpermutation symmetric function satisfying f (Z) f (Z ) I1m , j Im+1.22 (m + u 1/2)1PZ {f (Z) EZ {f (Z)} } exp1.(2)mu 22 max(m, u)2 11right hand side (2) approximately exp 2+. similar, less tight2uinequality obtained reduction draw random permutation drawmin(m, u) independent random variables application bounded difference inequality McDiarmid (1989):Lemma 3 Suppose conditions Lemma 2 hold.PZ {f (Z) EZ {f (Z)} } exp222 min(m, u).(3)proof Lemma 3 appears Appendix C.Remark 2 inequalities developed Section 5 Talagrand (1995) imply concentration inequality similar (3), worse constants.inequality (2) defined (m, u)-permutation symmetric function f .specializing f obtain following two concentration inequalities:P1 PmRemark 3 g : I1m+u {0, 1} f (Z) = u1 m+ui=m+1 g(Zi )i=1 g(Zi ),m+u1EZ {f (Z)} = 0. Moreover, I1m , j Im+1, |f (Z) f (Zij )|+ u1 . Therefore,specializing (2) f obtain(PZm+u1 X1 Xg(Zi )g(Zi )ui=m+1i=1)2mu(m + u 1/2) 2 max(m, u) 1exp.(m + u)2max(m, u)(4)right hand side (4) approximately exp. inequality (4) explicit (and looser) version Vapniks absolute bound (see El-Yaniv & Gerzon, 2005).note using (2) unable obtain explicit version Vapniks relative bound(inequality 10.14 Vapnik, 1982).72 mu2m+ufiEl-Yaniv & Pechyony1 Pm1 Pm+uRemark 4 g : I1m+u {0, 1}, f (Z) =i=1 g(Zi ), EZ {f (Z)} = m+ui=1 g(Zi ).m+u1ijMoreover, I1 , j Im+1 , |f (Z) f (Z )| . Therefore, specializing (2)f obtain()2m+uX1 X1(m + u 1/2)m 2 max(m, u) 1g(Zi )g(Zi ) expPZ.m+uumax(m, u)i=1i=1(5)22 (m+u)m. bound asymptoticallyright hand side (5) approximately expufollowing bound, developed Serfling (1974):()m+uX1 X122 (m + u)mPZg(Zi )g(Zi ) exp.m+uu+1i=1i=14. Uniform Rademacher Error Boundsection develop transductive risk bound, based transductiveRademacher complexity (Definition 1). derivation follows standard two-step scheme,induction4 :1. Derivation uniform concentration inequality set vectors (or functions).inequality depends Rademacher complexity set. substitutingvectors (or functions) values loss functions, obtain error bounddepending Rademacher complexity values loss function. stepdone Section 4.1.2. order bound Rademacher complexity terms properties hypothesis space, Rademacher complexity translated, using contraction property (Ledoux & Talagrand, 1991, Theorem 4.12), domain loss functionvalues domain soft hypotheses hypothesis space. step doneSection 4.2.show Sections 4.1 4.2, adaptation steps transductivesetting immediate involves several novel ideas. Section 4.3 combineresults two steps obtain transductive Rademacher risk bound. alsoprovide thorough comparison risk bound corresponding inductive bound.4.1 Uniform Concentration Inequality Set Vectorsinduction (Koltchinskii & Panchenko, 2002), derivation uniform concentrationinequality set vectors consists three steps:1. Introduction ghost sample.2. Bounding supremum suphH g(h), g(h) random real-valued function, expectation using concentration inequality functions randomvariables.4. scheme introduced Koltchinskii Panchenko (2002). examples usestechnique found papers Bartlett Mendelson (2002) Meir Zhang (2003).8fiTransductive Rademacher Complexity Applications3. Bounding expectation supremum using Rademacher variables.follow three steps induction, establishment stepsachieved using inductive techniques. Throughout section, performingderivation step transductive context discuss differences inductivecounterpart.introduce several new definitions. Let V set vectors [B1 , B2 ]m+u , B10, B2 0 set B = B2 B1 , Bmax = max(|B1 |, |B2 |). Consider two independentpermutations I1m+u , Z Z0 . v V denotev(Z) = (v(Z1 ), v(Z2 ), . . . , v(Zm+u )) ,vector v permuted according Z. use following abbreviations averages1 Pm+u1 Pkv subsets components: Hk {v(Z)} =i=1 v(Zi ), Tk {v(Z)} = ui=k+1 v(Zi )(note H stands head T, tail). special case k = setH{v(Z)} = Hm {v(Z)}, T{v(Z)} = Tm {v(Z)}. uniform concentration inequalitydevelop shortly states > 0, probability least 1 randompermutation Z I1m+u , v V,!11T{v(Z)} H{v(Z)} + Rm+u (V) +ln.min(m, u)Step 1: Introduction ghost sample.1 Pm+udenote v = m+ui=1 v(i) average component v. v Vm+upermutation Z I1T{v(Z)} = H{v(Z)} + T{v(Z)} H{v(Z)}hH{v(Z)} + sup T{v(Z)} v + v H{v(Z)}vVh= H{v(Z)} + sup T{v(Z)} EZ0 T{v(Z0 )} + EZ0 H{v(Z0 )} H{v(Z)}vVhH{v(Z)} + EZ0 sup T{v(Z)} T{v(Z0 )} + H{v(Z0 )} H{v(Z)} . (6)vV|{z}=(Z)Remark 5 derivation ghost sample permutation Z0 + u elementsdrawn distribution Z. inductive Rademacher-based risk boundsghost sample new training set size m, independently drawn original one.Note transductive setting ghost sample corresponds independent drawtraining/test set partition, equivalent independent draw random permutation Z0 .Remark 6 principle could avoid introduction ghost sample Z0 considerelements H{v(Z)} ghosts u elements T{v(Z)}. approach would lead9fiEl-Yaniv & Pechyonynew definition Rademacher averages (with = 1/m probability m/(m + u)1/u probability u/(m + u)). definition obtain Corollary 1. However,since distribution alternative Rademacher averages symmetric around zero,technically know prove Lemma 5 (the contraction property).Step 2: Bounding supremum expectation.m+uLet = (m+u1/2)(11/(2max(m,u))) . sufficiently large u, value almost 1. function (Z) (m, u)-permutation symmetric Z. verified11|(Z) (Zij )| B+ u1 . Therefore, apply Lemma 2 = B+ u1(Z). obtain, probability least 1 random permutation Z I1m+u ,v V:111T{v(Z)} H{v(Z)} + EZ {(Z)} + B+ln .(7)2 uRemark 7 induction step performed using application McDiarmids boundeddifference inequality (McDiarmid, 1989, Lemma 1.2). cannot apply inequalitysetting since function supremum (i.e. (Z)) function independent variables, rather permutations. Lemma 2 replaces boundeddifference inequality step.Step 3: Bounding expectation supremum using Rademacher random variables.goal bound expectation EZ {(Z)}. done following lemma.Lemma 4 Let Z random permutationI1m+u .EZ {(Z)} Rm+u (V) + c0 BmaxqLet c0 =11+up32 ln(4e)3< 5.05.min(m, u) .Proof: proof based ideas proof Lemma 3 Bartlett Mendelson(2002). technical convenience use following definition pairwise Rademachervariables.Definition 3 (Pairwise Rademacher variables) Let v = (v(1), . . . , v(m + u)) Rm+u .m+uLet V set vectors Rm+u . Let = {i }i=1vector i.i.d. random variablesdefined as:1mu, u1probability (m+u)2 ;m21, 1probability (m+u)2 ;(8)= (i,1 , i,2 ) = 1 m1mu,probability;2u(m+u)11u2,probability.uu(m+u)2obtain Definition 3 Definition 1 (with p =Rademacher variable = 1 split =10mu2)(m+u)1 1u, .following way.Rademacher variablefiTransductive Rademacher Complexity Applications11= 1 splitm,u . Rademacher variable = 0it1 to1 =split randomly , u1 , u1 . first component indicates ithcomponent v first elements v(Z) last u elements v(Z).1former case valuelatter case value u1 . secondcomponent meaning first one, Z replaced Z0 .1valuesu1 exactly coefficients appearing inside T{v(Z)}, T{v(Z0 )},0H{v(Z )} H{v(Z)} (6). coefficients random distributioninduced uniform distribution permutations. course proof1establish precise relation distributionu1 coefficientsdistribution (8) pairwise Rademacher variables.easy verify()m+uXRm+u (V) = E sup(i,1 + i,2 )v(i) .(9)vV i=11Let n1 , n2 n3 number random variables realizing value, u1 ,1 1 1 1, , u , , respectively. Set N1 = n1 + n2 N2 = n2 + n3 . Note niNi random variables. Denote Rad distribution defined (8)Rad(N1 , N2 ), distribution Rad conditioned events n1 +n2 = N1 n2 +n3 = N2 .define()m+uXs(N1 , N2 ) = ERad(N1 ,N2 ) sup(i,1 + i,2 ) v(i) .vV i=1rest proof based following three claims:Claim 1. Rm+u (V) = EN1 ,N2 {s(N1 , N2 )}.Claim 2. EZ {(Z)} = (E N1 , E N2 ).Claim 3. (E N1 , E N2 ) EN1 ,N2 {s(N1 , N2 )} c0 Bmax1u+1m.established three claims immediately obtain11EZ {g(Z)} Rm+u (V) + c0 Bmax+.u(10)entire development symmetric u and, therefore, also obtainresult u instead m. taking minimum (10) symmetricbound (with u) establish theorem.proof three claims appears Appendix D.Remark 8 technique use bound expectation supremum complicated technique Koltchinskii Panchenko (2002) commonly usedinduction. caused structure function supremum (i.e.,g(Z)). conceptual point view, step utilizes novel definition transductive Rademacher complexity.combining (7) Lemma 4 obtain next concentration inequality,main result section.11fiEl-Yaniv & PechyonyTheorem 1 Let B1 0, B2 0 V (possibly infinite) set real-valued vectors1[B1 , B2 ]m+u . Let B = B2 B1 Bmax = max(|B1 |, |B2 |). Let Q = u1 +, =q32 ln(4e)m+u< 5.05. probability least 13(m+u1/2)(11/2(max(m,u))) c0 =m+urandom permutation Z I1 , v V,pT{v(Z)} H{v(Z)} + Rm+u (V) + Bmax c0 Q min(m, u) + Br1Q ln .(11)2qpdefer analysis slack terms Bmax c0 Q min(m, u) B S2 Q ln 1 Section 4.3. instantiate inequality (11) obtain first risk bound. ideaapply Theorem 1 appropriate instantiation set V T{v(Z)}correspond test error H{v(Z)} empirical error. true (unknown)labeling full-sample h Hout define`y (h) = (`(h(1), y1 ), . . . , `(h(m + u), ym+u ))set LH = {v : v = `y (h), h Hout }. Thus `y (h) vector values0/1 loss full sample examples, transductive algorithm operatedtraining/test partition. set LH set possible vectors `y (h), possibletraining/test partitions. apply Theorem 1 V = LH , v = `(h), Bmax = B = 1obtain following corollary:Corollary 1 Let Q, c0 defined Theorem 1. > 0, probabilityleast 1 choice training set Xm+u , h Hout ,rp1Q ln .(12)Lu (h) Lbm (h) + Rm+u (LH ) + Bmax c0 Q min(m, u) +2qpdefer analysis slack terms Bmax c0 Q min(m, u) B S2 Q ln 1 Section 4.3.bound (12) obtained straightforward application concentrationinequality (11), convenient deal with. Thats clear boundRademacher complexity Rm+u (LH ) 0/1 loss values terms propertiestransductive algorithm. next sections eliminate deficiency utilizing marginloss function.4.2 Contraction Rademacher Complexityfollowing lemma version well-known contraction principle theoryRademacher averages (see Theorem 4.12 Ledoux & Talagrand, 1991, Ambroladze,Parrado-Hernandez, & Shawe-Taylor, 2007). lemma adaptation, accommodates transductive Rademacher variables, Lemma 5 Meir Zhang (2003).proof provided Appendix E.Lemma 5 Let V Rm+u set vectors. Let f g real-valued functions. Let= {i }m+ui=1 Rademacher variables, defined (1). 1 + u12fiTransductive Rademacher Complexity Applicationsv, v0 V, |f (vi ) f (vi0 )| |g(vi ) g(vi0 )|,"m+u#"m+u#XXE supf (vi ) E supg(vi ) .vVvVi=1i=1Let = (y1 , . . . , ym+u ) Rm+u true (unknown) labeling full-sample. Similarly done derivation Corollary 1, h Hout define`y (h(i)) = ` (h(i), yi )`y (h) = (`y (h(1)), . . . , `Y (h(m + u)))set LH = {v : v = `y (h), h Hout }. Noting `y satisfies Lipschitz condition|`y (h(i)) `y (h0 (i))| 1 |h(i) h0 (i)|, apply Lemma 5 V = LH , f (vi ) = `y (h(i))g(vi ) = h(i)/, get(Esupm+uXhHout i=1)`y (h(i))follows (13)Rm+u (LH )1E(supm+uXhHout i=11Rm+u (Hout ) .)h(i).(13)(14)4.3 Risk Bound Comparison Related ResultsApplying Theorem 1 V = LH , v = ` (h), Bmax = B = 1, using inequality(14) obtain5 :Theorem 2 Let Hout set full-sample soft labelings algorithm, generatedoperating possible training/test qset partitions. choice Hout de132 ln(4e)1pend full-sample Xm+u . Let c0 =<5.05,Q=+3u=m+u(m+u1/2)(11/(2 max(m,u))) . fixed , probability least 1 choicetraining set Xm+u , h Hout ,rpRm+u (Hout )SQ 1b+ c0 Q min(m, u) +ln .(15)Lu (h) Lu (h) Lm (h) +2large enough valuesqof u value close1. Therefore slackpp1term c0 Q min(m, u) + 2 Q ln order 1/ min(m, u) . convergence ratep1/ min(m, u) slow small u m. Slow rate smallsurprising, latter case u somewhat surprising. However noteu mean u elements, drawn + u elements, large variance.Hence, case high-confidence interval estimation large.confidence interval reflected slack term (15).5. bound holds fixed margin parameter . Using technique proof Theorem 18Bousquet Elisseeff (2002), also obtain bound uniform .13fiEl-Yaniv & Pechyonycompare bound (15) Rademacher-based inductive risk bounds.use following variant Rademacher-based inductive risk bound Meir Zhang(2003):Theorem 3 Let probability distribution X . Suppose set examplesSm = {(xi , yi )}mi=1 sampled i.i.d. X according D. Let F class functions(ind)bmmaps X R R(F) empirical Rademachercomplexity F (Defini1 Pmtion 2). Let L(f ) = E(x,y)D {`(f (x), y)} Lb (f ) =`i=1 (f (xi ), yi ) respectively0/1 generalization error empirical margin error f . > 0> 0, probability least 1 random draw Sm , f F,r(ind)bmR(F)2 ln(2/)L(f ) Lb (f ) ++.(16)slack term bound (16) order O(1/ m). bounds (15) (16)quantitatively comparable. inductive bound holds high probabilityrandom selection examples distribution D. bound average(generalization) error, examples D. transductive bound holds highprobability random selection training/test partition. bound testerror hypothesis particular set u points.kind meaningful comparison obtained follows. Using given full(transductive) sample Xm+u , define corresponding inductive distribution Dtransuniform distribution Xm+u ; is, training set size generatedsampling Xm+u times replacements. Given inductive hypothesis spaceF = {f } function define transductive hypothesis space HF projection Ffull sample Xm+u : HF = {h Rm+u : f F, 1 + u, h(i) = f (xi )}.definition HF , L(f ) = Lm+u (h).final step towards meaningful comparison would translate transductivebound form Lu (h) Lbm (h)+slack bound average error hypothesis6h:bm (h) + u Lbm (h) + slackLbmLm (h) + uLu (h)Lm+u (h) Lm+u (h) =m+um+uuslack= Lbm (h) +(17)m+uinstantiate (17) bound (15) obtainLm+u (h)Lbm (h)"#rpu Rm+u (HF )uSQ 1++c0 Q min(m, u) +ln. (18)m+um+u26. Alternatively, compare (15) (16), could try express bound (16) bounderror f Xu (the randomly drawn subset u examples). bound (16) holds settingrandom draws replacement. setting number unique training examples smallerthus number remaining test examples larger u. Hence drawtraining examples replacement imply draw subset u test examples,transductive setting. Thus cannot express bound (16) bound randomly drawn Xu14fiTransductive Rademacher Complexity Applicationsgiven transductive problem consider corresponding inductive bound obtained(16) distribution Dtrans compare bound (18).Note inductive bound (16) sampling training set donereplacement, transductive bound (18) done without replacement. Thus,inductive case actual number distinct training examples may smaller m.bounds (16) (18) consist three terms: empirical error term (first summand(16) (18)), term depending Rademacher complexity (second summand(16) (18)) slack term (third summand (16) third fourth summands(18)). empirical error terms bounds. hard compareanalytically Rademacher complexity terms. inductive boundderived setting sampling replacement transductive bound derivedsetting sampling without replacement. Thus, transductive Rademachercomplexity example xi Xm+u appears Rm+u (Hout ) multiplied. contrast, due sampling replacement, inductive Rademacherb(ind) (F), multiplied differentterm example xi Xm+u appear several times Rm+uvalues Rademacher variables.Nevertheless, transduction full control Rademacher complexity(since choose Hout observing full sample Xm+u ) choose hypothesis space Hout arbitrarily small Rademacher complexity. induction choose Fb(ind) (F)observing data. Hence, lucky full sample Xm+u Rm+ub(ind) (F) large. Thus,small, unlucky Xm+u Rm+uprovisions argue transductive Rademacher term largerinductive counterpart.Finally, compare slack terms (16) (18). u u slackterm (18) order (1/ m), corresponding term (16).u slack term (18) order (1/(m u)), much smallerO(1/ m) slack term (16).Based comparison corresponding terms (16) (18) conclusionregime u transductive bound significantly tighterinductive one.75. Unlabeled-Labeled Representation (ULR) Transductive AlgorithmsLet r natural number let U (m + u) r matrix depending Xm+u .Let r 1 vector may depend Sm Xu . soft classificationoutput h transductive algorithm representedh=U .(19)refer (19) unlabeled-labeled representation (ULR). section developbounds Rademacher complexity algorithms based ULRs. notetransductive algorithm trivial ULR, example, taking r = + u, setting U7. regime u occurs following class applications. Given large library taggedobjects, goal learner assign tags small quantity newly arrived objects.example application organization daily news.15fiEl-Yaniv & Pechyonyidentity matrix assigning desired (soft) labels. interestednon-trivial ULRs provide useful bounds representations.8vanilla ULR, U (m + u) (m + u) matrix = (1 , . . . , m+u ) simplyspecifies given labels Sm (where = yi labeled points, = 0 otherwise).point view vanilla ULR trivial encodefinal classification algorithm. example, algorithm Zhou et al. (2004)straightforwardly admits vanilla ULR. hand, natural (non-trivial) ULRalgorithms Zhu et al. (2003) Belkin Niyogi (2004) vanillatype. algorithms necessarily obvious find non-trivial ULRs.Sections 6 consider two cases particular, algorithms Joachims (2003)Belkin et al. (2004).rest section organized follows. Section 5.1 present genericbound Rademacher complexity transductive algorithm based ULR.Section 5.2 consider case matrix U kernel matrix. casedevelop another bound transductive Rademacher complexity. Finally, Section 5.3present method computing high-confidence estimate transductive Rademachercomplexity.5.1 Generic Bound Transductive Rademacher Complexitypresent bound transductive Rademacher complexity transductivealgorithm basedULR. Let {i }ri=1 singular values U . use well-knownqPqPr2 , kU k2=fact kU kFro =Froi=1i,j (U (i, j)) Frobenius normU . Suppose kk2 1 1 . Let Hout = Hout (U ) set possibleoutputs algorithm operated possible training/test set partitions1+ u1 . Using abbreviation U (i, ) ith row Ufull-sample Xm+u . Let Q =following proof idea Lemma 22 Bartlett Mendelson (2002),()()m+um+uXXRm+u (Hout ) = Q Esuph(xi ) = Q Esuph, U (i, )i(= Q EhHout i=1sup:kk2 1h,m+uX):kk2 1 i=1U (i, )ii=1)(m+uX= Q1 EU (i, )i=12vum+uu X= Q1 Ej hU (i, ), U (j, )ii,j=1vu m+uuXQ1E {i j hU (i, ), U (j, )i}(20)(21)i,j=18. trivial representation U identity matrix multiplied constant show Lemma 6risk bound (15), combined forthcoming Rademacher complexity bound (22), greater1.16fiTransductive Rademacher Complexity Applicationsvvrum+uuruX 2u 2 X22hU (i, ), U (i, )i = 1kU kFro = 12i .= 1mumumui=1(22)i=1(20) (21) obtained using, respectively, Cauchy-Schwarz Jensen inequalities. Using bound (22) conjunction Theorem 2 immediately getdata-dependent error bound algorithm, computed deriveupper bound maximal length possible values vector, appearingULR. Notice vanilla ULR (and thus consistency method Zhouet al. (2004)), 1 = m. Section 6 derive tight bound 1 non-trivial ULRsSGT Joachims (2003) consistency method Zhou et al. (2004).bound (22) syntactically similar form corresponding inductive Rademacherbound kernel machines (Bartlett & Mendelson, 2002). However, noted above,fundamental difference induction, choice kernel (and therefore Hout )must data-independent sense must selected training examplesobserved. transductive setting, U Hout selected unlabeledfull-sample observed.Rademacher bound (22), well forthcoming Rademacher bound (25), depend spectrum matrix U . see Section 6, non-trivial ULRstransductive algorithms (the algorithms Zhou et al., 2004 Belkin et al.,2004) spectrum U depends spectrum Laplacian graph usedalgorithm. Thus transforming spectrum Laplacian control Rademachercomplexity hypothesis class. exists strong empirical evidence (see Chapelleet al., 2003; Joachims, 2003; Johnson & Zhang, 2008) spectral transformationsimprove performance transductive algorithms.next lemma (proven Appendix F) shows trivial ULRs resultingrisk bound vacuous.Lemma 6 Let Rm+u vector depending Sm Xu . Let c R, U = ctransductive algorithm generating soft-classification vector h = U . Let {i }ri=1singular values U 1 upper bound kk2 . algorithmbound (22) conjunction bound (15) vacuous; namely, (0, 1)h generated holdsvrukp11 u2 X 2bQ ln 1 .Lm (h) ++ c0 Q min(m, u) +mu2i=15.2 Kernel ULRr = + u matrix U kernel matrix (this holds U positive semidefinite),say decomposition kernel-ULR. Let G Rm+u reproducingkernel Hilbert space (RKHS), corresponding U . denote h, iG inner productG. Since U kernel matrix, reproducing property9 G, U (i, j) = hU (i, ), U (j, )iG .9. means h G I1m+u , h(i) = hU (i, ), hiG .17fiEl-Yaniv & PechyonySuppose vector satisfies U 2 2 . Let {i }m+ui=1 eigenvalues U . similar arguments used derive (22) have:m+uXX m+uh(xi ) = Q E supj U (i, j)Q EsuphHout i=1j=1i=1m+uXX m+uj hU (i, ), U (j, )iGQ E supj=1i=1*m+u+m+uXXU (i, ),j U (j, )Q E supi=1j=1Gm+um+uXXQ E supU (i, )j U (j, )(23)i=1j=1GGm+uXQ2 EU (i, )i=1Gv*+um+uu m+uXXU (i, ),j U (j, )Q2 Ei=1j=1Gvvu m+uuXuXu m+uj U (i, j) Q2E {i j U (i, j)}(24)Q2 Ei,j=1i,j=1vvrum+uuXuX 2u 2 m+u2 trace(U)2U (i, i) = 2= 2.(25)mumumu(Rm+u (Hout ) =======)m+uXi=1i=1inequalities (23) (24) obtained using, respectively, Cauchy-Schwarz Jenseninequalities. Finally, first equality (25) follows definition Rademachervariables (see Definition 1).transductive algorithm kernel-ULR use (25) (22) boundRademacher complexity. kernel bound (25) tighter non-kernelcounterpart (22) kernel matrix eigenvalues larger one and/or 2 < 1 .Section 6 derive tight bound 1 non-trivial ULRs consistency methodZhou et al. (2004) Tikhonov regularization method Belkin et al. (2004).5.3 Monte-Carlo Rademacher Boundsshow compute Monte-Carlo Rademacher bounds high confidencetransductive algorithm using ULR. empirical examination bounds(see Section 6.3) shows tighter analytical bounds (22) (25).technique, based simple application Hoeffdings inequality, madeparticularly simple vanilla ULRs.18fiTransductive Rademacher Complexity Applications1Let V Rm+u set vectors, Q =+ u1 , Rm+u Rademacher vector(1), g() = supvV v. Definition 1, Rm+u (V) = Q E {g()}. Let 1 , . . . , ni.i.d. sample Rademacher vectors.P estimate Rm+u (V) high confidenceapplying Hoeffding inequality ni=1 n1 g(i ). apply Hoeffding inequalityneed bound sup |g()|, derived case V = Hout . Namelyassume V set possible outputs algorithm (for fixed Xm+u ).Specifically, suppose v V output algorithm, v = U , assumekk2 1 .Definition 1, , kk2 b = + u. Let 1 . . . k singularvalues U u1 , . . . , uk w1 , . . . , wk corresponding unit-length right leftsingular vectors10 .kXsup |g()| =sup| U | =supui wiT b1 k .kk2 b, kk2 1kk2 b, kk2 1i=1Applying one-sided Hoeffding inequality n samples g() have, given ,probability least 1 random i.i.d. choice vectors 1 , . . . , n ,n1X2 ln111.Rm+u (V)+sup U + 1 k + u(26)unn:kk2 1i=1use bound (26), value sup:kk2 1 U computed randomly drawn . computation algorithm-dependent Section 6.3 showcompute algorithm Zhou et al. (2004).11 cases computesupremum exactly (as vanilla ULRs; see below) also get lower bound usingsymmetric Hoeffding inequality.6. Applications: Explicit Bounds Specific Algorithmssection exemplify use Rademacher bounds (22), (25) (26)particular transductive algorithms. Section 6.1 instantiate generic ULR bound(22) SGT algorithm Joachims (2003). Section 6.2 instantiate kernel-ULRbound (25) algorithm Belkin et al. (2004). Finally, Section 6.3 instantiatethree bounds (22), (25) (26) algorithm Zhou et al. (2004) compareresulting bounds numerically.6.1 Spectral Graph Transduction (SGT) Algorithm Joachims (2003)start description simplified version SGT captures essencealgorithm.12 Let W symmetric (m + u) (m + u) similarity matrix full-sample10. vectors found singular value decomposition U .11. application approach induction seems hard, impossible. example,case RBF kernel machines need optimize (typically) infinite-dimensional vectorsfeature space.12. omit heuristics optional SGT. exclusion affect error boundderive.19fiEl-Yaniv & PechyonyXm+u . (i, j)th entry W represents similarity xi xj . matrix Wconstructed various ways, example, k-nearest neighbors graph.graph vertex represents example full sample Xm+u . edgepair vertices one corresponding examples among k similarexamples other. weights edges proportional similarityadjacent vertices (points). examples commonly used measures similarity cosinesimilarity RBF kernel. Let diagonal matrix, whose (i, i)th entry sumith row W . unnormalized Laplacian W L = W .Let r {1, . . . , + u 1} fixed, {i , vi }m+ueigenvectors eigenvalues LPr+1 2i=1e0 = 1 . . . m+u L = i=2 vv . Let = (1 , . . . , m+u ) vectorspecifies given labels Sm ; is, {1} labeled points, = 0otherwise. Let c fixed constant 1 (m + u) 1 vector whose entries 1let C diagonal matrix C(i, i) = 1/m iff example training set (andzero otherwise). soft classification h produced SGT algorithm solutionfollowing optimization problem:minhRm+ue + c(h ~ )T C(h ~ )hT Lhs.t. hT 1 = 0,hT h = + u.(27)(28)shown Joachims (2003) h = U , U (m + u) r matrix whosecolumns vi s, 2 r + 1, r 1 vector. dependstraining test sets, matrix U depends unlabeled full-sample. Substitutingh = U second constraint (28) using orthonormalitycolumnsU , weget + u = hT h = U U = . Hence, kk2 = + u take1 = + u. Since U (m + u) r matrix orthonormal columns, kU k2Fro = r.conclude (22) following bound transductive Rademacher complexity SGT11+,(29)Rm+u (Hout ) 2rur number non-zero eigenvalues L. Notice bound (29) obliviousmagnitude eigenvalues. small value r bound (29) small,but, shown Joachims (2003) test error SGT bad. r increases bound(29) increases test error improves. Joachims shows empirically smallestvalue r achieving nearly optimal test error 40.6.2 Kernel-ULR Algorithm Belkin et al. (2004)defining RKHS induced graph (unnormalized) Laplacian, doneHerbster, Pontil, Wainer (2005), applying generalized representer theoremScholkopf, Herbrich, Smola (2001), show algorithm Belkin et al. (2004)kernel-ULR. Based kernel-ULR derive explicit risk bound this.also derive explicit risk bound based generic ULR. show former (kernel)bound tighter latter (generic) one. Finally, compare kernel boundrisk bound Belkin et al. (2004). proofs lemmas section appearAppendix G.20fiTransductive Rademacher Complexity Applicationsalgorithm Belkin et al. (2004) similar SGT algorithm, describedSection 6.1. Hence appendix use notation description SGT(see Section 6.1). algorithm Belkin et al. formulated follows.minhRm+uhT Lh + c(h ~ )T C(h ~ )(30)hT 1 = 0(31)s.t.difference (30)-(31) (27)-(28) constraint (28), may changeresulting hard classification. Belkin et al. developed stability-based error boundalgorithm based connected graph. analysis follows also assumeunderlying graph connected, shown end section, argumentalso extended unconnected graphs.represent full-sample labeling vector Reproducing Kernel Hilbert Space(RKHS) associated graph Laplacian (as described Herbster et al., 2005)derive transductive version generalized representer theorem Scholkopf et al.(2001). Considering (30)-(31) set H = {h | hT 1 = 0, h Rm+u }. Let h1 , h2 Htwo soft classification vectors. define inner producthh1 , h2 iL = hT1 Lh2 .(32)denote HL set H along inner product (32). Let 1 , . . . , m+ueigenvalues L increasing order. Since L Laplacian connected graph,1 = 0 2 + u, 6= 0. Let ui eigenvector corresponding .Since L symmetric, vectors {ui }m+ui=1 orthogonal. assume also w.l.o.g.11. Letvectors {ui }m+uorthonormalu1 = m+ui=1U=m+uXi=21ui uTi .(33)Note matrix U depends unlabeled full-sample.Lemma 7 (Herbster et al., 2005) space HL RKHS reproducing kernelmatrix U .consequence Lemma 7 algorithm (30)-(31) performs regularizationRKHS HL regularization term khk2L = hT Lh (this fact also noted Herbsteret al., 2005). following transductive variant generalized representer theoremScholkopf et al. (2001) concludes derivation kernel-ULR algorithmBelkin et al. (2004).Lemma 8 Let h H solution optimization problem (30)-(31), let Udefined above. Then, exists Rm+u h = U .Remark 9 consider case unconnected graph. Let numberconnected components underlying graph. zero eigenvalue LaplacianL multiplicity t. Let u1 , . . . , ut eigenvectors corresponding zero eigenvalue L. Let ut+1 , . . . , um+u eigenvectors corresponding non-zero eigenvalues21fiEl-Yaniv & Pechyonyt+1 , . . . , m+u L. replace constraint (31) constraints hT ui = 0 defineP1kernel matrix U = m+ui=t+1 ui ui . rest analysis caseconnected graph.obtain explicit bounds transductiveRademacher complexity algorithm Belkin et al. remains bound U kk2 . start boundingU .substitute h = U (30)-(31). PSince u2 , . . . , um+u orthogonal u1 =m+u 111, h 1 = U 1 =i=2 ui ui 1 = 0. Moreover,m+u1hT Lh = U LU = m+u1 1T U = U . Thus (30)-(31) equivalentsolvingmin U + c(U ~ )T C(U ~ )(34)Rm+uoutputting h = U , solution (34). Let 0 (m + u) 1vector consisting zeros.Tout U Tout U + c(U ~ )T C(U ~ )0T U 0 + c(U 0 ~ )T C(U 0 ~ ) = c .ThusqTout Uc = 2 .(35)Let 1 , . . . , m+u eigenvalues U , sorted increasing order. follows1, 1 , . . . , m+u(33) 1 = 0 2 + u, = m+ui+2eigenvalues L sorted increasing order.substitute bound (35) (25), obtain kernel boundvvuuXX 1u 2c m+uu 2c m+u.=mumui=2i=2P1Suppose that13 m+ui=2 = O(m + u). substitute kernel bound (15)obtain probability least 1 random training/test partition,!1b.(36)Lu (h) Lm (h) + pmin(m, u)briefly compare bound risk bound algorithm (30)-(31) givenBelkin et al. (2004). Belkin et al. provide following bound algorithm14 .probability least 1 random draw training examples Xm+u ,1bLm+u (h) Lm (h) +.(37)13. assumption restricting since define matrix L spectrum observingunlabeled full-sample. Thus set L way assumption hold.14. original bound Belkin et al. terms squared loss. equivalent bound terms 0/1margin loss obtained derivation paper Belkin et al. (2004).22fiTransductive Rademacher Complexity ApplicationsSimilarly done Section 4.3, bring bounds common denominator,rewrite bound (36)!u1Lu (h) Lbm (h) +p.(38)m+umin(m, u)u u bounds (37) (38) convergence rate. Howeveru convergence rate (38) (which O(1/(m u))) much fasterone (37) (which O(1/ m)).6.3 Consistency Method Zhou et al. (2004)section instantiate bounds (22), (25) (26) consistency methodZhou et al. (2004) provide numerical comparison.start brief description Consistency Method (CM) algorithm Zhouet al. (2004). algorithm natural vanilla ULR (see definition beginningSection 5), matrix U computed follows. Let W matricesSGT (see Section 6.1). Let L = D1/2 W D1/2 parameter (0, 1). Then,U = (1 )(I L)1 output CM h = U , specifies givenlabels. Consequently kk2 = m. following lemma, proven Appendix H, providescharacterization eigenvalues U :Lemma 9 Let max min be, respectively, largest smallest eigenvalues U .max = 1 min > 0.follows Lemma 9 U positive definite matrix hence also kernelmatrix. Therefore, decompositionU kernel-ULR. apply kernelbound (25) compute bound 2 U . Rayleigh-Ritz theorem (Horn& Johnson, 1990), TU max . Since definition vanilla ULR,p== m, obtain U maxmax m.obtained 1 = 2 = max m, max maximal eigenvalueU . Since Lemma 9 max = 1, CM algorithm bound (22) always tighter(25).turns CM, exact value supremum (26) analyticallyderived. Recall vectors , induce CM hypothesis space particularU , exactly components values {1}; rest components zeros.Let set possible s. Let t(i ) = (t1 , . . . , tm+u ) = U R1(m+u)|t(i )| = (|t1 |, . . . , |tm+u |). Then, fixed , sup U sumlargest elements |t(i )|. derivation holds vanilla ULR.demonstrate Rademacher bounds discussed paper present empiricalcomparison bounds two datasets (Voting, Pima) UCI repository15 .dataset took + u size dataset (435 768 respectively)took 1/3 full-sample size. matrix W 10-nearest neighbors graphcomputed cosine similarity metric. applied CM algorithm = 0.5.Monte-Carlo bounds (both upper lower) computed = 0.05 n = 105 .15. also obtained similar results several UCI datasets.23fiEl-Yaniv & PechyonyPima DatasetBound Transductive RademacherBound Transductive RademacherVoting Dataset1.41.210.80.6Kernel ULR bound0.4Generic ULR boundUpper Monte Carlo bound0.200Lower Monte Carlo501001502002503003501.41.210.80.6Kernel ULR bound0.4Upper Monte Carlo bound0.200400Generic ULR boundLower Monte Carlo100200300400500600700Number Eigenvalues/Singular valuesNumber Eigenvalues/Singular valuesFigure 1: comparison transductive Rademacher bounds.compared upper lower Mote-Carlo bounds generic ULR bound (22)kernel-ULR bound (25). graphs Figure 1 compare four boundsdatasets function number non-zero eigenvalues U . Specifically,point x-axis corresponds bounds computed matrix Ut approximates U using smallest eigenvalues U . examples lower upperMonte-Carlo bounds tightly sandwich true Rademacher complexity. strikinggeneric-ULR bound close true Rademacher complexity. principle,simple Monte-Carlo method approximate true Rademacher complexitydesired accuracy (with high confidence) cost drawing sufficiently manyRademacher vectors.7. PAC-Bayesian Bound Transductive Mixturessection adapt part results Meir Zhang (2003) transduction.proofs results presented section appear Appendix I.|B|Let B = {hi }i=1 finite set base-hypotheses. class B formedobserving full-sample Xm+u , obtaining training/test set partitionP|B|labels. Let q = (q1 , . . . , q|B| ) R|B| probability vector, i.e. i=1 qi = 1 qi 01 |B|. vector q computed observing training/test partitiontraining labels. goal find posterior vectorP qmixturePP|B||B|m+u1eq =ehypothesis hj=m+1 `i=1 qi hi (j), yj .i=1 qi hi minimizes Lu (hq ) = usection derive uniform risk bound set qs. bound dependsKL-divergence (see definition below) q prior probability vectorp R|B| , vector p defined based unlabeled full-sample. Thusforthcoming bound (see Theorem 4) belongs family PAC-Bayesian bounds(McAllester, 2003; Derbeko et al., 2004), depend prior posterior information.Notice bound, differentPAC-Bayesian bounds Gibbs classifiers1 Pm+ubound EhB(q) Lu (h) = u j=m+1 EhB(q) `(h(j), yj ), h B(q) random drawbase hypothesis B according distribution q.24fiTransductive Rademacher Complexity Applicationse q ) EhB(q) Lu (h).Remark 10 one reviewers noted, Jensen inequality Lu (hHence risk bound transductive Gibbs classifier holds true also transductive mixture classifier. Currently known risk bound transductive Gibbs classifiers (Theorem 18paper Derbeko et al., 2004) diverges u . forthcoming risk bound(41) deficiency.assume q belongs domain g,A = {q | g(q) A}, g : R|B|R predefined function R constant. domain g,A set Be q . Recalling Q =induce class Beg,A possible mixtures h(1/m + 1/u), =pm+u32 ln(4e)/3 < 5.05, apply Theorem 2 Hout =(m+u0.5)(10.5/ max(m,u)) c0 =Beg,A obtain probability least 1 training/test partitione q Beg,A ,Xm+u , hrpRm+u (Beg,A )1eebLu (hq ) Lm (hq ) ++ c0 Q min(m, u) +Q ln .(39)2qLet Q1 = S2 Q (ln(1/) + 2 ln logs (sg(q)/g0 )). straightforward apply techniqueused proof Theorem 10 Meir Zhang (2003) obtain following bound,eliminates dependence A.Corollary 2 Let g0 > 0, > 1 g(q) = max(g(q), g0 ). fixed g > 0,eq,probability least 1 training/test set partition, all16 he q ) Lb (heLu (hq) +pRm+u (Beg,g(q) )+ c0 Q min(m, u) + Q1 .(40)instantiate Corollary 2 g(q)KL-divergence derive PAC-BayesianP|B|bound. Let g(q) = D(qkp) = i=1 qi ln pqii KL-divergence p q. AdoptingLemma 11 Meir Zhang (2003) transductive Rademacher variables, defined(1), obtain following bound.Theorem 4 Let g0 > 0, > 1, > 0. Let p q prior posterior distributionB, respectively. Set g(q) = D(qkp) g(q) = max(g(q), g0 ). Then, probabilityeq,least 1 training/test set partition, hrpe q ) Lb (he q ) + Q 2g(q) sup khk2 + c0 Q min(m, u) + Q1 .Lu (h(41)2hBTheorem 4 PAC-Bayesian result, prior p depend Xm+u posterior optimized adaptively, basedp also Sm . general bound (15), bound(41) convergence rate O(1/ min(m, u)). bound (41) syntactically similarinductive PAC-Bayesian bound mixture hypothesis (see Theorem 10 Lemma 11paper Meir & Zhang, 2003), similar convergence rate O(1/ m). Howeverconceptual difference inductive transductive bounds transductiondefine prior vector p observing unlabeled full-sample inductiondefine p observing data.eg,eg(q) ) follows:16. bound (40) meaning Rm+u (BRm+u (Beg,g(q) ) = Rm+u (Beg,A ).25q, let = ge(q)fiEl-Yaniv & Pechyony8. Concluding Remarksstudied use Rademacher complexity analysis transductive setting.results include first general Rademacher bound soft classification algorithms,unlabeled-labeled representation (ULR) technique bounding Rademacher complexitytransductive algorithm bound Bayesian mixtures. demonstratedusefulness results and, particular, effectiveness ULR frameworkderiving error bounds several advanced transductive algorithms.would nice improve bounds using, example, local Rademacherapproach Bartlett, Bousquet, Mendelson (2005). However, believe mainadvantage transductive bounds possibility selecting hypothesis spacebased full-sample. clever data-dependent choice space provide sufficientflexibility achieve low training error low Rademacher complexity. opinionopportunity explored exploited much further. particular, wouldinteresting develop efficient procedure choice hypothesis space learnerknows properties underlying distribution (e.g., clustering assumption holds).work opens new avenues future research. example, would interestingoptimize matrix U ULR explicitly (to fit data) constraint lowRademacher complexity. Also, would nice find low-Rademacher approximationsparticular U matrices. PAC-Bayesian bound mixture algorithms motivatesdevelopment use transductive mixtures, area yet investigated.Finally, would interesting utilize bounds model selection process.Acknowledgmentsgrateful anonymous reviewers insightful comments. also thankYair Wiener Nati Srebro fruitful discussions. Dmitry Pechyony supportedpart IST Programme European Community, PASCAL NetworkExcellence, IST-2002-506778.Appendix A. Proof Lemma 1proof based technique used proof Lemma 5 paper MeirZhang (2003). Let = (1 , . . . , m+u )T Rademacher random variablesRm+u (V, p1 ) = (1 , . . . , m+u )T Rademacher random variables Rm+u (V, p2 ).real-valued function g(v), n I1m+u v0 V,)sup [g(v)] = Enn 6= 0 .vV(42)usePabbreviation 1s = 1 , . . . , . apply (42) fixed 1n1 g(v) =f (v) + n1i=1 vi , obtain)(n vn0 + sup [g(v)] n 6= 0 En sup [n vn + g(v)]vVvV(supvV"n1Xi=1#vi + f (v) En(supvV26" nXi=1)#vi + f (v) n 6= 0 .(43)fiTransductive Rademacher Complexity Applicationscomplete proof lemma, prove general claim: real-valuedfunction f (v), 0 n + u,(" n#)(" n#)XXE supvi + f (v)E supvi + f (v).(44)vVvVi=1i=1proof induction n. claim trivially holds n = 0 (in case (44) holdsequality). Suppose claim holds k < n functions f (v). useabbreviation 1s = 1 , . . . , . function f 0 (v)" n#X0vi + f (v)E1n supvV(i=1"n1#"n1#)XX11= 2p1vi + vn + f 0 (v) + En1 supvi vn + f 0 (v)E n1 sup2 1 vV2 1 vVi=1i=1"n1#X+ (1 2p1 ) En1 supvi + f 0 (v)1(2p1vV"n1X1E n1 sup2 1 vV1+ E n1 sup2 1 vV((i=1#vi + vn + f 0 (v)i=1"n1X(45)#)0vi vn + f (v)i=1#vi + f 0 (v)= E n1 2p1 En sup1vV i=1" n#((Xvi + f 0 (v)= E n1 2p1 En sup1" nXvV+ supvV"n1Xi=1i=1+ (1 2p1 ) E n1 sup1)n 6= 0vV+ (1 2p1 ) supvV"n1X#vi + f (v)i=1"n1X#)vi + f 0 (v)i=1)"n1#!X0vi + f (v)n 6= 0 supvVi=1#)vi + f 0 (v)#)"n1#!X02p2 En supvi + f (v) n =6 0 supvi + f (v)vV i=1vV i=1"n1#)X+ supvi + f 0 (v)(E n111= E1n supvVvV(i=1" nX0)#02p2 En supvi + f (v) n 6= 0vV i=1"n1#)X+ (1 2p2 ) supvi + f 0 (v)(= E n1" nX(0" nXvV#i=1vi + f 0 (v) .i=127(46)fiEl-Yaniv & Pechyonyinequality (45) follow inductive hypothesis, applied thrice f (v) = vn +f 0 (v), f (v) = vn + f 0 (v) f (v) = f 0 (v). inequality (46) follows (43)fact p1 < p2 .Appendix B. Proof Lemma 2require following standard definitions facts martingales.17 Let Bn1 =(B1 , . . . , Bn ) sequence random variables bn1 = (b1 , . . . , bn ) respectivew.r.t.values. sequence W0n = (W0 , W1 , . . . , Wn ) called martingaleunderlyingsequence Bn1 I1n , Wi function Bi1 EBi Wi |B1i1 = Wi1 .Let f (Xn1 ) = f (X1 , . . . , Xn ) arbitrary function n (possibly dependent) randomvariables. Let W0 = EXn1 {f (Xn1 )} Wi = EXn1 f (Xn1 )|Xi1 I1n . elementary fact W0n martingale w.r.t. underlying sequence Xn1 . Thus obtainmartingale function (possibly dependent) random variables. routineobtaining martingale arbitrary function called Doobs martingale process.definition Wn Wn = EXn1 {f (Xn1 )|Xn1 } = f (Xn1 ). Consequently, bounddeviation f (Xn1 ) mean sufficient bound difference Wn W0 .fundamental inequality, providing bound, McDiarmids inequality (McDiarmid,1989).Lemma 10 (McDiarmid, 1989, Corollary 6.10) Let W0n martingale w.r.t. Bn1 .Let bn1 = (b1 , . . . , bn ) vector possible values random variables B1 , . . . , Bn .Leti1i1ri (bi1= bi1= b1i1 , Bi = bi .1 ) = sup Wi : B11 , Bi = bi inf Wi : B1bibiLet r2 (bn1 ) =Pni1 2i=1 (ri (b1 ))rb2 = supbn1 r2 (bn1 ). Then,22PBn1 {Wn W0 > } < exp 2.rb(47)inequality (47) improved version Hoeffding-Azuma inequality (Hoeffding,1963; Azuma, 1967).proof Lemma 2 inspired McDiarmids proof bounded differenceinequality permutation graphs (McDiarmid, 1998, Section 3). Let W0m+u martingale)obtained f (Z) Doobs martingale process, namely W0 = EZm+u f (Zm+u11. compute upper bound r2 apply Lemma 10.)|ZbWi = EZm+u f (Zm+u111Fix i, I1m . Let m+u= 1 , . . . , m+u specific permutation I1m+u i01m+umi{i+1 , . . . , m+u }. Let p1 = PjI m+u j Ii+1= m+uip2 = PjI m+u j Im+1=i+1i+117. See, e.g., Chapter 12 Grimmett Stirzaker (1995), Section 9.1 Devroye et al. (1996)details.28fiTransductive Rademacher Complexity Applications1 p1 =um+ui .1i1 ) = sup Wi : Bi1Wi : B1i1 = 1i1 , Bi =ri (= i111 , Bi = infi1i1= sup EZ f (Z) | Z1 = 1 , Zi = EZ f (Z) | Zi1= 1i1 , Zi = i01,i0nsup EjI m+u EZ f (Z) | Zi1= 1i1 , Zi = , Zj = i01===,i0i+1,i0i+1EjI m+u EZ f (Zij ) | Zi1= i1, Zi = , Zj = i011i+1n0sup EjI m+u EZ f (Z) f (Zij ) | Z1i1 = i11 , Zi = , Zj =(48)n0sup p1 EZ,jIi+1f (Z) f (Zij ) | Z1i1 = i11 , Zi = , Zj =(49),i00+ p2 EZ,jI m+u f (Z) f (Zij ) | Z1i1 = i11 , Zi = , Zj =m+1Since f (Z) (m, u)-permutation symmetric function, expectation (49) zero. Therefore,i1ri (1 ) =nsup p2 EZ,jI m+u f (Z) f (Zij ) | Z1i1 = 1i1 , Zi = , Zj = i0m+1,i0u.m+uim+uSince f (Z) (m, u)-permutation symmetric, also follows (48) Im+1,Rj+1/2 1i111 ) = 0. verified j > 1/2, j 2 j1/2 t2 dt, therefore,ri (2rb=supm+uXm+ui=11Zu2 2Xi1 21 )ri (m+u1/2u1/2i=1um+ui22 2=um+u1Xj=u1j2mu2 21dt =.t2(u 1/2)(m + u 1/2)(50)applying Lemma 10 bound (50) obtain22 (u 1/2)(m + u 1/2)PZ {f (Z) EZ {f (Z)} } exp.mu2 2(51)entire derivation symmetric u. Therefore, also22 (m 1/2)(m + u 1/2)PZ {f (Z) EZ {f (Z)} } exp.m2 u 2(52)taking tightest bound (51) (52) obtain statement lemma.29fiEl-Yaniv & PechyonyAppendix C. Proof Lemma 3consider following algorithm18 (named RANDPERM) drawing first elementsm+u{Zi }m:i=1 random permutation Z I1Let Zi = I1m+u .2: = 13:Draw di uniformly Iim+u .4:Swap values Zi Zdi .5: endAlgorithm 1: RANDPERM - draw first elements random permutation + uelements.1:algorithm RANDPERM abridged version procedure drawing randompermutation n elements drawing n1 non-identically distributed independent randomvariables, presented Section 5 paper Talagrand (1995) (which accordingTalagrand due Maurey, 1979).Lemma 11 algorithm RANDPERM performs uniform draw first elementsZ1 , . . . , Zm random permutation Z.Proof: proof induction m. = 1, single random variable d1uniformly drawn among Im+u , therefore, Z1 uniform distribution I1m+u . Letdm1 = d1 , . . . , dm . Suppose claim holds m1 < m. two possible values0m 001 = 1 , . . . , 1 = 1 , . . . , Z1 , . . . , Zm ,m1Pdm{Zm= m1} Pdm {Zm = | Zm1= m1}1 = 1 } = Pdm1 {Z1111111(53)u+10= 0m1} Pdm {Zm =| Zm1= 0m1}111= Pdm1 {Zm1= 0m1}111= Pdm1 {Zm1110m= Pdm{Zm1 = 1 } .1equality (53) follows inductive assumption definition dm .Consider (m, u)-permutation symmetric function f = f (Z) random permutations Z. Using algorithm RANDPERM represent random permutation Zfunction g(d) independent random variables. value function g(d)output algorithm RANDPERM operated values random draws given d.next lemma relates Lipschitz constant function f (g(d)) Lipschitzconstant f (Z):18. Another algorithm generating random permutation independent draws presented Appendix B Lanckriet et al. (2004). algorithm draws random permutation means drawing+ u independent random variables. Since deal (m, u)-permutation symmetric functions,interested first elements random permutation. algorithm Lanckrietet al. needs + u draws independent random variables define elements. algorithm RANDPERM, presented section, needs draws. use algorithm Lanckrietet al. instead RANDPERM, forthcoming bound (55) would term + u instead m.change, turn, would result non-convergent risk bound derived using techniques.30fiTransductive Rademacher Complexity ApplicationsLemma 12 Let f (Z) (m, u)-permutation symmetric function random permutationm+uZ. Suppose I1m , j Im+1, |f (Z) f (Zij )| . Let d0i independentdraw random variable di . I1m ,|f (g(d1 , . . . , di1 , di , di+1 , . . . , dm )) f (g(d1 , . . . , di1 , d0i , di+1 , . . . , dm ))| .(54)Proof: values = (d1 , . . . , di , . . . , dm ) d0 = (d1 , . . . , d0i , . . . , dm ) induce, re0m00spectively, first values19 Zm1 = {Z1 , . . . , Zm } Z1 = {Z1 , . . . , Zm } twom+udependent permutations I1 . Since f (m, u)-permutation symmetric, value0uniquely determined value Zm1 . prove change di di resultsijchange single element Z1 . Combined property |f (Z) f (Z )| ,conclude proof (54).refer d0 as, respectively, old new draws. Consider operationRANDPERM draws d0 . Let , di d0i values of, respectively,Zi , Zdi Zd0i ith iteration RANDPERM. Note di d0i i.old permutation, ith iteration Zi = di , Zdi = Zd0i = d0i .new permutation, ith iteration Zi = d0i , Zdi = di Zd0i = . ithiteration RANDPERM value Zi remains intact. However values Zdi Zd0imay change. particular values di may among Zi+1 , . . . , Zm endrun RANDPERM. four cases:0m/ Z0m/ Z0mCase 1 d0i/ Zm/ Zm1 ,1 Z1 = Z1 \{di } {d0i }.11 di0m0m0mCase 2 d0i Zm1 Z1 di Z1 , Z1 Z1 = Z1 .0m0mCase 3 Zm/ Zm/ Z0m1 d0i1 di Z1 ,1 Z1 = Z1 \{i } {d0i }.0m0m/ Z0mCase 4 d0i Zm/ Zm1 Z1 = Z1 \{di } {i }.11 Z1 , diapply bounded difference inequality McDiarmid (1989) f (g(d)) obtain22Pd {f (g(d)) Ed {f (g(d))} } exp 2.(55)Since f (Z) (m, u)-permutation symmetric, follows (55)22.PZ {f (Z) EZ {f (Z)} } exp 2Since entire derivation symmetric u also22PZ {f (Z) EZ {f (Z)} } exp 2.u(56)(57)proof Lemma 3 completed taking minimum bounds (56) (57).19. notational convenience section, refer Zm1 set values vector values(as done sections).31fiEl-Yaniv & PechyonyAppendix D. Proof Claims Lemma 4Proof Claim 1. Note N1 N2 random variables whose distributioninduced distribution . (9)Rm+u (V) = EN1 ,N2 ERad(N1 ,N2 ) supm+uXvV i=1(i,1 + i,2 ) v(i) = EN1 ,N2 s(N1 , N2 ) .Proof Claim 2. definitions Hk Tk (appearing start Section 4.1),N1 , N2 I1m+uhEZ,Z0 sup TN1 {v(Z)} TN2 {v(Z0 )} + HN2 {v(Z0 )} HN1 {v(Z)} =vV#N2N1m+um+uXX1 X111 Xv(Zi )v(Zi0 ) +v(Zi0 )v(Zi ) . (58)EZ,Z0 supuvV u i=N +1i=1i=1i=N+112|{z}"=r(v,Z,Z0 ,N1 ,N2 )values N1 N2 , distribution Z Z0 , respecttake1 11expectation (58), induce distribution assignments coefficients,, u , u1components v. N1 , N2 realizations Z Z0 , component v(i),I1m+u , assigned exactly two coefficients, one two permutations (ZZ0 ). Let = (a1 , . . . , am+u ), ai = (ai,1 , ai,2 ) pair coefficients.m+uI1 , pair (ai,1 , ai,2 ) takes values coefficients v(i), first1u1 ) secondcomponent induced realization Z (i.e., ai,1 either110component realization Z (i.e., ai,2 either u ).Let A(N1 , N2 ) distribution vectors a, induced distribution Z Z0 ,particular N1 , N2 . Using definition write" m+u#X(58) = EaA(N1 ,N2 ) sup(ai,1 + ai,2 )v(i) .(59)vVi=1Let Par(k) uniform distribution partitions m+u elements two subsets,k+ u k elements, respectively. Clearly, Par(k) uniform distributionm+uelements. distribution random vector (a1,1 , a2,1 , . . . , am+u,1 )kfirst elements pairs equivalent Par(N1 ). is, vector obtained1correspondingtaking first N1 indices realization Z assigning1components. components assigned u . Similarly, distributionrandom vector (a1,2 , a2,2 , . . . , am+u,2 ) equivalent Par(N2 ). Therefore, distributionA(N1 , N2 ) entire vector equivalentto productPar(N1 )distributionm+um+uPar(N2 ), uniform distributionelements,N1N2element pair independent permutations.show distributions Rad(N1 , N2 ) A(N1 , N2 ) identical. Given N1N2 setting = (m+u)2 , probability drawing specific realization (satisfying32fiTransductive Rademacher Complexity Applicationsn1 + n2 = N1 n2 + n3 = N2 )m2n2mu N1 n2 mu N2 n2u2m+uN1 N2 +n2=mN1 +N2 u2(m+u)N1 N2. (60)(m + u)2(m+u)Since (60) independent ni s, distribution Rad(N1 , N2 ) uniformpossible Rademacher assignments satisfying constraints N1 N2 . easy seesupport size Rad(N1 , N2 ) support size A(N1 , N2 ). Moreover,support sets distributions identical; hence distributions identical.Therefore, follows (59)((58) = ERad(N1 ,N2 )" m+u#)Xsup(i,1 + i,2 )v(i)= s(N1 , N2 ) .vVi=1easy see E N1 = E {n1 + n2 } = E N2 = E {n2 + n3 } = m. SinceEZ {(Z)} (58) N1 = N2 = m,(EZ {(Z)} = ERad(m,m)supvV"m+uX#)(i,1 + i,2 ) v(i)= (E N1 , E N2 ) .i=1Proof Claim 3.bound differences |s(N1 , N2 ) (N10 , N2 ) | |s(N1 , N2 ) (N1 , N20 ) |1 N1 , N2 , N10 , N20 + u. Suppose w.l.o.g. N10 N1 . Recalling definitionr() (58)"#s(N1 , N2 ) = EZ,Z0 sup r(v, Z, Z0 , N1 , N2 )vV"s(N10 , N2 ) = EZ,Z0 sup r(v, Z, Z0 , N1 , N2 ) +vV11+uXN1#v(Zi ) .(61)i=N10 +1expressions supremums s(N1 , N2 ) s(N10 , N2 ) differ twoterms (61). Therefore, N1 N10 ,s(N1 , N2 ) s(N10 , N2 ) Bmax N1 N10 1 + 1.(62)uSimilarly N2 N20 ,s(N1 , N2 ) s(N1 , N20 ) Bmax N2 N2011+u.(63)use following Bernstein-type concentration inequality (see Devroye et al., 1996,Problem8.3)binomial random variable X Bin(p, n): PX {|X EX|> t} <13t212 exp 8np . Abbreviate Q = + u . Noting N1 , N2 Bin m+u , + u , use33fiEl-Yaniv & Pechyony(62), (63) Bernstein-type inequality (applied n = + u p =obtainm+u )PN1 ,N2 {|s(N1 , N2 ) s(E {N1 } , E {N2 })| }PN1 ,N2 {|s(N1 , N2 ) s(N1 , E N2 )| + |s(N1 , E N2 ) s(E N1 , E N2 )| }nPN1 ,N2 |s(N1 , N2 ) s(N1 , E N2 )|2n+PN1 ,N2 |s(N1 , E N2 ) s(E N1 , E N2 )|2nnPN2 |N2 E N2 | Bmax Q+ PN1 |N1 E N1 | Bmax Q2 !222334 exp= 4 exp.222 Q232(m + u) m+u Bmax Q32mBmaxNext use following fact (see Devroye et al., 1996, Problem 12.1): nonnegativerandom variable X satisfies P{X > t} c exp(kt2 ) c 1 k > 0,pEX ln(ce)/k. Using fact, along c = 4 k = 3/(32mQ2 ),|EN1 ,N2 {s(N1 , N2 )} s(E N1 , E N2 )| EN1 ,N2 |s(N1 , N2 ) s(E N1 , E N2 )|32 ln(4e)11 22mBmax+.3uAppendix E. Proof Lemma 5proof straightforward extension proof Lemma 5 Meir Zhang(2003) also similar proof Lemma 1 Appendix A. prove strongerclaim: I1m+u v, v0 V, |f (vi )f (vi0 )| |g(vi )g(vi0 )|, functionec : Rm+u R."m+u#"m+u#XXf (vi ) + ec(v) E supg(vi ) + ec(v) .E supvVvVi=1i=1use abbreviation 1n = 1 , . . . , n . proof induction n,0 n + u. lemma trivially holds n = 0. Suppose lemma holds n 1.words, function c(v),"En1 sup c(v) +1Let p =mu.(m+u)2vVn1X#"f (vi ) En1 sup c(v) +1i=1vVn1X#g(vi ).i=1"= E1n sup c(v) +vVnX#"f (vi ) = En En1 sup c(v) +1i=134vVnXi=1#f (vi )(64)fiTransductive Rademacher Complexity Applications(= pEn1"sup c(v) +1vVn1X#"f (vi ) + f (vn ) + sup c(v) +vVi=1n1X#)f (vi ) f (vn )i=1"+(1 2p)En1 sup c(v) +1n1XvV(65)#f (vi ).(66)i=1apply inductive hypothesis three times: first second summands (65)c(v) = c(v)+f (vn ) c(v) = c(v)f (vn ), respectively, (66) c(v) = c(v).obtain("#"#)n1n1XXpEn1 sup c(v) +g(vi ) + f (vn ) + sup c(v) +g(vi ) f (vn )1vV|i=1vV{zi=1}=B"+ (1 2p)En1 sup c(v) +1|vV#n1Xg(vi )i=1{z.}=Cexpression B written follows."#)"#(n1n1XX000g(vi ) f (vn )g(vi ) + f (vn ) + sup c(v ) +B = pEn1 sup c(v) +1vVc(v) + c(v0 ) += pEn1 sup1v,v0 V"0= pEn1 sup1v0 Vi=1"c(v) + c(v ) +v,v0 Vi=1n1Xh(g(vi ) + g(vi0 )) + (f (vn ) f (vn0 ))i=1n1Xh(g(vi ) +g(vi0 ))+ f (vn ) f (vn0 )##.(67)i=1P0equality (67) holds since expression c(v)+c(v0 )+ n1i=1 (g(vi )+g(vi )) symmetric00v v . Thus, f (v) < f (v ) exchange values v v0increase value expression supremum. Since |f (vn ) f (vn0 )||g(vn ) g(vn0 )|"#n1Xh000B pEn1 sup c(v) + c(v ) +(g(vi ) + g(vi )) + |g(vn ) g(vn )|1v,v0 V"c(v) + c(v0 ) += pEn1 sup1v,v0 V(= pEn11n1X#(g(vi ) + g(vi0 )) + (g(vn ) g(vn0 ))i=1"sup c(v) +vVi=1n1Xh#"n1Xg(vi ) + g(vn ) + sup c(v) +vVi=1Therefore, using reverse argument (64)-(66),"C + = E1n sup c(v) +vV35nXi=1i=1#g(vi ).#)g(vi ) g(vn )= D.fiEl-Yaniv & PechyonyAppendix F. Proof Lemma 6Let c R, U = c I. c = 0, soft classification generated constant zero.case, h generated A, Lbm (h) = 1 lemma holds.Suppose c 6= 0.1= h .(68)cSince (m + u) (m + u) matrix U + u singular values, one precisely c,(22) Rademacher complexity trivial ULR boundedr2112.(69)1(m + u)c = c1 2+muuassume w.l.o.g. training points indices 1 m. Let = {iI1m | yi h(i) > 0 |h(i)| > } set indices training examples zero marginloss. Let B = {i I1m | |h(i)| [, ]} C = {i I1m | yi h(i) < 0 |h(i)| > }.(68) definition sets A, C, C, |i | > c . Similarly,B, |i | = |h(i)|c . obtain bound (69) leastr2 X h(i)2 1c (|A| + |C|) 2 +.cc2iBTherefore, risk bound (15) boundedrX222(|A| + |C|) +h(i)iBrPX h(i)22iB (1 |h(i)|/) + |C|+ |A| + |C| +2iBrPX|B| + |C| iB ri2+ |A| + |C| +ri2iBrPX|A| iB ri22+ |A| + |C| +ri1Lbm (h) +=== ,iBri = |h(i)|. prove 1. Equivalently, sufficient proveri1 , . . . , ri|B| [0, 1]|B| holdsf ri1 , . . . , ri|B| =P(|A| + iB ri )2P.|A| + |C| + iB ri2claim stronger statement holds:(|A| + |C| + Pri )2PiB 2 .f ri1 , . . . , ri|B| =|A| + |C| + iB ri36(70)fiTransductive Rademacher Complexity Applicationsprove (70) use Cauchy-Schwarz inequality, stating two vectors a, bRm , ha, bi kak2 kbk2 . set bi = 1 I1m . vector set follows: ai = riB ai = 1 otherwise. definition b, ha, bi 0thus (ha, bi)2 kak22 kbk22 . application inequality defined vectorsb results inequality (70).Appendix G. Proofs Section 6.2Proof Lemma 7: Let ei (m + u) 1 vector whose ith entry equals 1entries zero. According definition RKHS, need show1 + u, h(i) = hU (i, ), hiL .hU (i, ), hiL = U (i, )Lh = ei U Lh! m+u!m+u!m+uXXX 1ui uiui ui h = eiui ui h= eii=2i=1i=21= eTi (I u1 uT1 )h = eTi1 1T h = h(i) .m+uLemma 13 1 + u, U (i, ) HL .Proof: Since L Laplacian matrix, u1 = 1. Since vectors {ui }m+ui=1 orthonormalPm+u 11 = 0. Therefore, 1 + u,u1 = 1, U 1 =i=2 ui uiU (i, ) 1 = 0.pProof Lemma 8: Let khkL = hh, hiL = hT Lh norm GL . optimizationproblem (30)-(31) stated following form:minhHLkhk2L + c(h ~ )T C(h ~ ) .(71)Let U HL vector space spanned vectors {U (i, )}m+ui=1 . Let hk =(i,)iL+ u, = hh,UkU (i,)kL .verified hprojection h onto U. 1part h perpendicular U.1 + u, hh , U (i, )iL = 0. 1 + um+uXh(i) = hh, U (i, )iL = hPm+ui=1U (i, )Let h = h hkHLj U (j, ), U (i, )iL + hh , U (i, )iLj=1=m+uXj hU (j, ), U (i, )iL =j=1m+uXj U (i, j) = hk (i) .(72)j=1second equation (72) holds Lemma 13. consequence (72), empiricalerror (the second term (71)) depends hk . Furthermore,hT Lh = hh, hiL = khk2L = km+uXU (i, )k2L + kh k2L ki=1m+uXi=137U (i, )k2L .fiEl-Yaniv & PechyonyTherefore, h H minimizes (71), h = 0 h = hk =Pm+ui=1U (i, ) = U .Appendix H. Proof Lemma 9Let LN = L = D1/2 W D1/2 normalized Laplacian W . eigenvalues0{0i }m+ui=1 LN non-negative smallest eigenvalue LN , denoted min ,zero (Chung, 1997). eigenvalues matrix L = (1 )I + LN{1 + 0i }m+uL strictly positive.i=1 . Since 0 < < 1, eigenvaluesom+un1. Finally,Hence matrix L invertible eigenvalues 1+0i=1nom+u1eigenvalues matrix U 1+. Since 0min = 0, largest eigenvalue0i=1U 1. Since eigenvalues LN non-negative, min > 0.Appendix I. Proofs Section 7Proof Corollary 2: Let {Ai }i=1 {pi }i=1 set positive numbersPi=1 pi 1. weighted union bound argument (39) probability least 1 training/test set partitions, Ai q g,Ai ,eq)Lu (heq)Lbm (hpRm+u (Beg,Ai )++ c0 Q min(m, u) +1Q ln.2pi(73)P1set Ai = g0 si pi = i(i+1). verifiedi=1 pi 1. q let iqsmallest index Aiq g(q). two cases:Case 1 iq = 1. case iq = logs (g(q)/g0 ) = 1.Case 2 iq 2. case Aiq 1 = g0 siq 1 < g(q) g(q)s1 , therefore, iqlogs (g(q)/g0 ).Thus always iq logs (g(q)/g0 ). follows definition Aiq g(q)Aiq g(q). ln(1/piq ) 2 ln(iq + 1) 2 ln logs (sg(q)/g0 ). Substitutingbounds (73) taking account monotonicity Rm+u (Beg,Ai ) (in Ai ),probability least 1 , q, bound (40) holds.Proof Theorem 4: require several definitions facts convex analysis(Rockafellar, 1970). function f : Rn R conjugate function f : Rn Rdefined f (z) = supxRn (hz, xi f (x)). domain f consists values zvalue supremum finite. consequence definition fso-called Fenchel inequality:hx, zi f (x) + f (z) .(74)P|B|verified conjugate function g(q) = D(qkp) g (z) = ln j=1 pj ezj .e =Let h(i)(h1 (i), . . . , h|B| (i)). derivation follows use following inequality38fiTransductive Rademacher Complexity Applications(Hoeffding, 1963): X random variable X b c constant,EX exp(cX) expc2 (b a)28.(75)> 0 have,*e q = QE supRm+u (Beg,A ) = QE sup h, hqg,A==q,qg,A*m+uXm+uX+eh(i)i=1+QeE supq,h(i)qg,Ai=1!!m+uXQesup g(q) + E gh(i)qg,Ai=1" m+u#|B|XXQ+ E lnpj exphj (i)j=1i=1" m+u#!XQ+ sup E ln exph(i)hBi=1" m+u#!XQ+ sup ln E exph(i)hBi=1" m+u#!2XQ+ sup ln exph(i)22hBi=1Q+ sup khk22 .2 hB(76)(77)(78)(79)(80)Inequality (76) obtained applying (74) f = g f = g . Inequality (77) followsdefinition g g . Inequality (78) obtained application Jenseninequality inequality (79) obtained applying + u times (75). minimizing(80) w.r.t. obtainrRm+u (Beg,A ) Q 2A sup khk2 .hB2Substituting bound (39) get fixed A, probability least 1,q Bg,ApQre q ) Lb (heLu (h2A sup khk22 + c0 Q min(m, u) +q) +hBr1Q ln .2Finally, applying weighted union bound technique, proof Corollary 2,obtain statement theorem.39fiEl-Yaniv & PechyonyReferencesAmbroladze, A., Parrado-Hernandez, E., & Shawe-Taylor, J. (2007). Complexity patternclasses Lipschitz property. Theoretical Computer Science, 382 (3), 232246.Audibert, J.-Y. (2004). better variance control PAC-Bayesian classification. Tech.rep. 905, Laboratoire de Probabilites et Modeles Aleatoires, Universites Paris 6Paris 7.Azuma, K. (1967). Weighted sums certain dependent random variables. Tohoku Mathematical Journal, 19, 357367.Balcan, M., & Blum, A. (2006). augmented PAC model semi-supervised learning.Chapelle, O., Scholkopf, B., & Zien, A. (Eds.), Semi-Supervised Learning, chap. 22,pp. 383404. MIT Press.Bartlett, P., Bousquet, O., & Mendelson, S. (2005). Local Rademacher complexities. AnnalsProbability, 33 (4), 14971537.Bartlett, P., & Mendelson, S. (2002). Rademacher Gaussian complexities: risk boundsstructural results. Journal Machine Learning Research, 3, 463482.Belkin, M., Matveeva, I., & Niyogi, P. (2004). Regularization semi-supervised learninglarge graphs. Shawe-Taylor, J., & Singer, Y. (Eds.), Proceedings 17thAnnual Conference Learning Theory, pp. 624638. Springer-Verlag.Belkin, M., & Niyogi, P. (2004). Semi-supervised learning Riemannian manifolds. Machine Learning, 56, 209239.Blum, A., & Langford, J. (2003). PAC-MDL bounds. Scholkopf, B., & Warmuth, M.(Eds.), Proceedings 16th Annual Conference Learning Theory, pp. 344357.Springer-Verlag.Bousquet, O., & Elisseeff, A. (2002). Stability generalization. Journal MachineLearning Research, 2, 499526.Catoni, O. (2004). Improved Vapnik-Cervonenkis bounds. Tech. rep. 942, Laboratoire deProbabilites et Modeles Aleatoires, Universites Paris 6 Paris 7.Catoni, O. (2007). PAC-Bayesian supervised classification, Vol. 56 IMS Lecture Notes Monograph Series. Institute Mathematical Statistics.Chapelle, O., Scholkopf, B., & Zien, A. (Eds.). (2006). Semi-Supervised Learning. MITPress, Cambridge, MA.Chapelle, O., Weston, J., & Scholkopf, B. (2003). Cluster kernels semi-supervised learning. Becker, S., Thrun, S., & Obermayer, K. (Eds.), Advances Neural InformationProcessing Systems 15, pp. 585592. MIT Press, Cambridge, MA.Chung, F. R. (1997). Spectral graph theory, Vol. 92 CBMS Regional Conference SeriesMathematics. American Mathematical Society.Derbeko, P., El-Yaniv, R., & Meir, R. (2004). Explicit learning curves transductionapplication clustering compression algorithms. Journal Artificial IntelligenceResearch, 22, 117142.40fiTransductive Rademacher Complexity ApplicationsDevroye, L., Gyorfi, L., & Lugosi, G. (1996). Probabilistic Theory Pattern Recognition.Springer-Verlag.El-Yaniv, R., & Gerzon, L. (2005). Effective transductive learning via objective modelselection. Pattern Recognition Letters, 26, 21042115.El-Yaniv, R., & Pechyony, D. (2006). Stable transductive learning. Lugosi, G., & Simon,H. (Eds.), Proceedings 19th Annual Conference Learning Theory, pp. 3549.Springer-Verlag.Grimmett, G., & Stirzaker, D. (1995). Probability Random Processes. Oxford SciencePublications. Second edition.Hanneke, S. (2006). analysis graph cut size transductive learning. Proceedings23rd International Conference Machine Learning, pp. 393399. ACM Press.Herbster, M., Pontil, M., & Wainer, L. (2005). Online learning graphs. Proceedings22nd International Conference Machine Learning, pp. 305312. ACM Press.Hoeffding, W. (1963). Probability inequalities sums bounded random variables. Journal American Statistical Association, 58, 1330.Horn, R., & Johnson, C. (1990). Matrix Analysis. Cambridge University Press.Joachims, T. (2003). Transductive learning via spectral graph partitioning. Proceedings20th International Conference Machine Learning, pp. 290297. ACM Press.Johnson, R., & Zhang, T. (2007). effectiveness Laplacian normalization graphsemi-supervised learning. Journal Machine Learning Research, 8, 14891517.Johnson, R., & Zhang, T. (2008). Graph-based semi-supervised learning spectral kerneldesign. IEEE Transactions Information Theory, 54.Koltchinskii, V. (2001). Rademacher penalties structural risk minimization. IEEETransactions Information Theory, 47 (5), 19021915.Koltchinskii, V., & Panchenko, D. (2002). Empirical margin distributions boundinggeneralization error combined classifiers. Annals Statistics, 30 (1), 150.Lanckriet, G., Cristianini, N., Bartlett, P., Ghaoui, L. E., & Jordan, M. (2004). Learningkernel matrix semidefinite programming. Journal Machine Learning Research,5, 2772.Ledoux, M., & Talagrand, M. (1991). Probability Banach spaces. Springer-Verlag.Maurey, B. (1979). Construction de suites symetriques. Comptes Rendus Acad. Sci. Paris,288, 679681.McAllester, D. (2003). PAC-Bayesian stochastic model selection. Machine Learning, 51 (1),521.McDiarmid, C. (1989). method bounded differences. Siemons, J. (Ed.), SurveysCombinatorics, pp. 148188. London Mathematical Society Lecture Note Series 141,Cambridge University Press.McDiarmid, C. (1998). Concentration. Habib, M., McDiarmid, C., Ramirez, J., & Reed,B. (Eds.), Probabilistic methods algorithmic discrete mathematics, pp. 195248.Springer-Verlag.41fiEl-Yaniv & PechyonyMeir, R., & Zhang, T. (2003). Generalization error bounds Bayesian mixture algorithms.Journal Machine Learning Research, 4, 839860.Rockafellar, R. (1970). Convex Analysis. Princeton University Press, Princeton, N.J.Scholkopf, B., Herbrich, R., & Smola, A. (2001). generalized representer theorem.Helmbold, D., & Williamson, B. (Eds.), 14th Annual Conference ComputationalLearning Theory 5th European Conference Computational Learning Theory,pp. 416426. Springer-Verlag.Serfling, R. (1974). Probability inequalities sum sampling without replacacement.Annals Statistics, 2 (1), 3948.Talagrand, M. (1995). Concentration measure isoperimetric inequalities productspaces. Publications Mathematiques de lI.H.E.S., 81, 73203.Vapnik, V. (1982). Estimation Dependences Based Empirical Data. Springer-Verlag.Vapnik, V. (2000). nature statistical learning theory (2nd edition). Springer-Verlag.Vapnik, V., & Chervonenkis, A. (1974). Theory Pattern Recognition. Moscow:Nauka.Zhou, D., Bousquet, O., Lal, T., Weston, J., & Scholkopf, B. (2004). Learning localglobal consistency. Thrun, S., Saul, L., & Scholkopf, B. (Eds.), AdvancesNeural Information Processing Systems 16. MIT Press, Cambridge, MA.Zhu, X., Ghahramani, Z., & Lafferty, J. (2003). Semi-supervised learning using gaussianfields harmonic functions. Proceedings 20th International ConferenceMachine Learning, pp. 912919. ACM Press.42fiJournal Artificial Intelligence Research 35 (2009) 485-532Submitted 08/08; published 07/09Hard Bribery Elections?Piotr Faliszewskifaliszew@agh.edu.plDepartment Computer ScienceAGH University Science TechnologyKrakow, PolandEdith Hemaspaandraeh@cs.rit.eduDepartment Computer ScienceRochester Institute TechnologyRochester, NY 14623 USALane A. Hemaspaandralane@cs.rochester.eduDepartment Computer ScienceUniversity RochesterRochester, NY 14627 USAAbstractstudy complexity influencing elections bribery: computationally complex external actor determine whether paying certain voterschange preferences specified candidate made elections winner?study problem election systems varied scoring protocols Dodgson voting,variety settings regarding homogeneous-vs.-nonhomogeneous electorate bribability, bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted voters,succinct-vs.-nonsuccinct input specification. obtain polynomial-time bribery algorithms proofs intractability bribery, indeed results showcomplexity bribery extremely sensitive setting. example, find settingsbribery NP-complete manipulation (by voters) P, find settings bribing weighted voters NP-complete bribing voters individualbribe thresholds P. broad class elections (including plurality, Borda, kapproval, veto) known scoring protocols, prove dichotomy result briberyweighted voters: find simple-to-evaluate condition classifies every case eitherNP-complete P.1. Introductionpaper studies complexity bribery elections, is, complexity computing whether possible, modifying preferences given number voters,make preferred candidate winner.Election systems provide framework aggregating voters preferencesideally(though truly ideal voting system, see Duggan & Schwartz, 2000; Gibbard,1973; Satterthwaite, 1975) way satisfying, attractive, natural. Societies useelections select leaders, establish laws, decide policies, practicalapplications elections restricted people politics. Many parallel algorithmsstart electing leaders. Multiagent systems sometimes use voting purposeplanning (Ephrati & Rosenschein, 1997). Web search engines aggregate results usingmethods based elections (Dwork, Kumar, Naor, & Sivakumar, 2001). widec2009AI Access Foundation. rights reserved.fiFaliszewski, Hemaspaandra, & Hemaspaandrarange applications, surprising elections vary tremendously. example,one might think first typical elections many voters candidates.However, fact, may wide range voter-to-candidate proportions:typical presidential elections relatively candidates may millionsvoters. context web, one may consider web pages voting pageslinking them, may consider humans voting pages site timespend each. setting may large number voterslarge number candidates. hand, Dwork et al. (2001) suggest designingmeta search engine treats search engines voters web pages candidates.yields voters many candidates. summarize paragraph, electionsgreat variety sizes terms numbers candidates numbers voters. So,surely, one simply say Elections tend small, always solvebrute-force issues related them.principles democracy mind, also tend think vote equallyimportant. However, scenarios make much sense settingvoter different voting power. example, U.S. presidential electionssense weighted (different states different voting powers Electoral College);shareholders company votes weighted number shares own;search engines example could weighted quality. Weighted votingnatural choice many settings well.importance election systems naturally inspired questions regarding resistance abuse, several potential dangers identified studied. example,elections organizers make attempts control outcome elections procedural tricks adding deleting candidates encouraging/discouraging peoplevoting. Classical social choice theory concerned possibility impossibility procedural control. However, recently realized even controlpossible, may still difficult find actions needed effect control, e.g., computational problem NP-complete. complexity controlling winselection studied first Bartholdi, Tovey, Trick (1992) later manyauthors (Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2007; Hemaspaandra,Hemaspaandra, & Rothe, 2007; Erdelyi, Nowak, & Rothe, 2008a, 2008b; Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2008; Meir, Procaccia, Rosenschein, & Zohar, 2008).Elections endangered organizers also voters (manipulation),might tempted vote strategically (that is, according true preferences)obtain preferred outcome. desirable skew resultelections way arguably best interest society. GibbardSatterthwaite/DugganSchwartz Theorems (Gibbard, 1973; Satterthwaite, 1975; Duggan &Schwartz, 2000) show essentially election systems manipulated,important discover systems manipulation computationally difficult execute.line research started Bartholdi, Tovey, Trick (1989a) continuedmany researchers (as varied examples, mention work Elkind & Lipmaa, 2005b, 2005a; Conitzer, Sandholm, & Lang, 2007; Hemaspaandra & Hemaspaandra,2007; Procaccia & Rosenschein, 2007; Brelsford, Faliszewski, Hemaspaandra, Schnoor, &Schnoor, 2008; Faliszewski, Hemaspaandra, & Schnoor, 2008; Zuckerman, Procaccia, &Rosenschein, 2008; readers interested manipulation able reach broader collec486fiHow Hard Bribery Elections?tion papers standard process recursive bibliography search). Faliszewski,Hemaspaandra, Hemaspaandra, Rothe (2009b) provide relatively nontechnical surveycomplexity issues related altering outcomes elections.Surprisingly, nobody seems addressed issue (the complexity of) bribery,i.e., attacks person interested success particular candidate picksgroup voters convinces vote says. Bribery seems stronglymotivated real-life computational agent-based settings, sharesflavor manipulation (changing voters (reported) preferences) control (decidingvoters influence). paper (in version conference precursor) initiatesstudy complexity bribery elections.many different settings bribery studied. simplest oneinterested least number voters need bribe make favoredcandidate win. natural extension consider prices voter. setting,voter willing change true preferences anything say,meet price. even complicated setting conceivablevoters would different prices depending want affect vote (however,clear succinctly encode voters price scheme). mainly focusprevious two scenarios point reader results approval votingwork Faliszewski (2008) discussion bribery prices representedflexibly.classify election systems respect bribery seeking case either (a)prove complexity low giving polynomial-time algorithm (b) argue intractabilityvia proving NP-completeness discovering whether bribery affect given case.obtain broad range results showing complexity bribery depends closelysetting. example, weighted plurality elections bribery P (Theorem 3.3)jumps NP-complete voters price tags (Theorem 3.2). another example,approval voting manipulation problem easily seen P, contrastprove bribery problem NP-complete (Theorem 4.2). Yet also provebribery cost function made local, complexity approval voting falls backP (Theorem 4.4). constraints added, problem goes NP-complete(Theorem 4.5). scoring protocols obtain complete characterizationscomplexity bribery possible voter types, i.e., without weightswithout price tags. particular, via dichotomy theorems (Theorems 4.8 4.9)algorithmic constructions (Theorems 4.13 4.15) provide voter type simplecondition partitions scoring protocols ones NP-complete bribery problemsones P bribery problems. point reader Tables 1 2 (they appear nearend paper) summary complexity results regarding briberyelections.paper organized follows. Section 2 describe election systemsbribery problems interested cover complexity background preliminaries. Section 3, provide detailed study plurality elections.study connections manipulation bribery, obtain dichotomy resultsbribery scoring protocols Section 4. Section 5, study case succinctlyrepresented elections fixed number candidates.487fiFaliszewski, Hemaspaandra, & Hemaspaandra2. Preliminariessection introduces notions notations use paper.2.1 Election Systemsdescribe elections providing set C = {c1 , . . . , cm } candidates, set V n votersspecified preferences, rule selecting winners. voter vs preferencesrepresented list ci1 > ci2 > . . . > cim , {i1 , i2 , . . . , im } = {1, 2, . . . , m}, ci1preferred candidate cim despised one. assume preferencestransitive, complete (for every two candidates voter knows oneprefers), strict (no ties). Sometimes authors also allow ties preference lists,ties clear interpretation election rules simplicityuniformity consider them.Given list votes (i.e., voters preference lists), election rule determinescandidates winners elections. briefly describe election systemsanalyze paper, standard literature social choice theory.1Winners plurality elections candidate(s) top choice largestnumber voters (of course, different voters different winners). approvalvoting voter selects candidates approves of; candidate(s) approvalswin. Unlike systems discussed paper, approval voting inputpreference order rather bit-vector approvals/disapprovals. scoringprotocol candidates described vector = (1 , . . . , ) nonnegative integers1 2 . . . . (We require 1 > , wish classifybroadest class cases possible, including usually easy boundary caseequal.) time candidate appears ith position voters preference list,candidate gets points; candidate(s) receive points win. Wellknown examples scoring protocols include Borda count, plurality, k-approval,veto voting systems, m-candidate elections Borda uses = (m 1, 2, . . . ,kmkz }| { z }| {0), plurality uses = (1, 0, . . . , 0, 0), k-approval uses = (1, . . . , 1, 0, . . . , 0), veto uses= (1, 1, . . . , 1, 0). Note selecting scoring protocol automatically selectnumber candidates within elections. Though scoring protocols easilynaturally generalized arbitrary candidate sets, formally individual scoringprotocol deals fixed number candidates. Thus results regarding scoringprotocols automatically talk fixed number candidates.Condorcet winner candidate (strictly) beats candidates pairwisecontests, is, Condorcet winner beats everyone else pairwise plurality elections.Clearly, one Condorcet winner, sometimes none (ascase Condorcet Paradox, Condorcet, 1785). many voting systems1. social choice literature, often voting systems assumed least one winner, exactlyone winner, least terms basic definition voting system, requirerestriction, since one imagine wanting study elections whichperhaps due tie effectssymmetry effects perhaps even due zero candidatesthere always exactly one winner.Indeed, practice, elections Hall Fame induction worthinesshired given academic department, quite possible real-world election system might giveanswer one year.488fiHow Hard Bribery Elections?choose Condorcet winner one exists use compatible rule otherwise. Onesystemdeveloped 1800sis Charles Lutwidge Dodgson (a.k.a. LewisCarroll). Dodgsons system winner person(s) become Condorcetwinner smallest number switches voters preference lists. (A switch changesorder two adjacent candidates list.2 ) Thus, Condorcet winner exists,also unique winner Dodgsons election. See work Dodgson (1876)and alsowork Bartholdi, Tovey, Trick (1989b)for details regarding Dodgsons votingrule, known winner testing complete parallel access NP(Hemaspaandra, Hemaspaandra, & Rothe, 1997). different election rule introducedrecently Young (1977). Young elections winner person becomeCondorcet winner removing smallest number voters. way contrast, noteplurality rule property elects candidates who, removingleast number votes, preferred everyone. work Rothe, Spakowski,Vogel (2003), see also expository presentation Rothe (2005), proves winnerproblem Young elections extremely difficultcomplete parallel access NP.Another election rule Kemeny (1959), see also work Kemeny Snell(1960). Kemeny consensus preference order maximizes number agreementsvoters preference lists, voter two candidates bsay preference order agrees voters preference list place bplace b a. Naturally, many different Kemeny consensuses may possible.candidate winner Kemeny election preferred candidateKemeny consensus election. (The original work Kemeny allowed votersnonstrict preference orders, like, e.g., Saari & Merlin, 2000, use Kemenyelections refer case input orderings strict.) Note winnertesting problem Kemeny elections known complete parallel access NP,known hold case input preference orders must strict,case nonstrict input preference orders allowed (Hemaspaandra, Spakowski,& Vogel, 2005, see particular comments footnote page 383paper). Kemeny rule might first sound Dodgson rule,fact different. Dodgsons elections based making minimumnumber local changes, Kemenys elections hinge overall closeness voterspreference orders certain consensus orderingswhich possibly maypreferences voters.2.2 Bribery ProblemsInformally, bribery problem following: Given description election (i.e.,set candidates, preferences voters, etc.), number k, distinguishedcandidate p, make p winner changing preference lists k voters.formally, election rule (i.e., election system) E define E-bribery problemfollowing. assume standard encoding mathematical objects finite2. mention, since source confusion, seminal paper Dodgson explicitlystate switches limited adjacent candidates. However, mathematics examplesconsistent reading, clear intended meaning.489fiFaliszewski, Hemaspaandra, & Hemaspaandrasets lists (e.g., see Garey & Johnson, 1979). Also, numbers nonnegativeintegers and, unless otherwise specified, represented binary.Name: E-bribery.Given: set C candidates, collection V voters specified via preference lists.distinguished candidate p C nonnegative integer k.Question: possible make p winner E election changing preferencelists k voters?speak unweighted case (all voters equal; paper alwaysholds unless weighted problem name) weighted case (voters weighted).Essentially results apply case want make preferredcandidate winner case want make preferred candidateunique winner, explicitly put nonunique/unique settingproblem names. clarity specificity, focus nonunique case discussionsproofs, problem statements theorems default refer nonuniquecase. However, settings differences proofs unique casenonunique case minor amount couple small tweaks, e.g., changingweak inequalities strong ones, adding special voter already prefers p, etc.,often end proof briefly note theorem also holds unique case.E-$bribery family problems assume voter price changingpreference list. case ask whether bribe k people,whether make p winner spending k dollars. example,plurality-weighted-$bribery problem described follows.Name: plurality-weighted-$bribery.Given: set C candidates. collection V voters specified via preferencelists (prefs 1 , . . . , prefs ), (nonnegative, integer) weights (w1 , . . . , wm ),(nonnegative, integer) prices (p1 , . . . , pm ). distinguished candidate p Cnonnegative integer k (which sometimes refer budget).PQuestion: set B {1, . . . , m} iB pi k way bribevoters B way p becomes winner?Regarding fact models voters assumed vote bribes dictate,stress using term bribery intend necessarily imply moralfailure part bribe recipients bribe givers: Bribes simply payments. Although human, political elections, payments typically considered morally wrong(perhaps voter supposed thinking overall social welfare),electronic/web/multiagent systems settings, morality issues often apply.voter may simply electronic entity trying maximize utility, bribe priceentity may crisply reflect fact.Throughout paper use term bribery regular sensenonstandard sense collection bribes. using latter sense oftenspeak bribery, thus mean collection bribes. So, example,490fiHow Hard Bribery Elections?Alice end winner bribery ask anyone vote Alice, meanpattern/scheme bribes (e.g., give two dollars Bob make changeFOO vote give five dollars Carol make change BAR vote) noneasks anyone vote Alice yet make Alice overall winner.dealing variety settings, need common format speakinstances bribery problems. adopt following convention (and viewalready specified problems implicitly recast form): instance briberyproblem 4-tuple E = (C, V, p, k),1. C list candidates,2. V list voters (see below),3. p C candidate want make winner (for problems makingcandidate unique winner, winner replaced unique winner),4. k bribe limit (either amount money spend bribingmaximum number voters bribe, depending flavor briberyproblem).list voters contains tuples describing votes cast. voter 3-tuple(prefs, , ),1. prefs preference list voter (or, case approval voting,preference vector),2. price changing voters preference list,3. weight voter.tuple V describes precisely one voter. drop price and/or weight fieldgiven election voters prices/weights. (However, assumedropped prices weights unit values, refer them.proofs handle two cases, one priced voters one weighted voters,time need able uniformly refer weights prices.) v V voterrefer price weight (v) (v). manner, U VX(U ) =(v)vU(U ) =X(v).vUoften refer (U ) either vote weight U total weight U .Note throughout paper V , though input list, typically functions multiset, summations additive term appropriateoccurrence multisetthe multiplicities carry sums, also set-like491fiFaliszewski, Hemaspaandra, & Hemaspaandraoperations, e.g., {v V | . . .} multiset, multiplicities appropriatelypreserved. dealing V use set/subset mean multiset/submultiset.Section 5 deal succinct representations. dealing succinctrepresentations, V consist 4-tuples (prefs, , , m), multiplicityvote, is, number voters identical preferences, price, weight entryV standing for. m(v) denote value v V . Note single entryV often represents multiple voters.notation help us speak bribery problems uniform fashion. Noteaddition specifying E = (C, V, p, k) always need explicitly state election ruleusing.Positive results regarding demanding bribery problems imply positive resultsweaker ones. example, weighted bribery P election system E clearlyunweighted bribery also easy E. Conversely, hardness results regardingsimpler models imply hardness results involved ones. often mentionimplied results separately interesting (e.g., algorithm simplercase provides insights understanding complicated case), omitenlightening.2.3 Reductions NP-completenessproceed study bribery, let us briefly review notions computational complexity standard NP-complete problems useproofs.usual, kSk denotes cardinality set S. fix alphabet = {0, 1}assume standard encodings mathematical entities involved problems.particular, integers represented binary unless specified otherwise. (For example,see Garey & Johnson, 1979, discussion issues.) NP-completenessstandard mean completeness respect many-one (polynomial-time) reductions.Definition 2.1 pm B (A many-one reduces B) polynomial-time computable function f(x )[x f (x) B].one results relating manipulation bribery also need disjunctive truth-tablereductions.Definition 2.2 say pdtt B (A disjunctively truth-table reduces B)polynomial-time procedure input x outputs list strings y1 , . . . , ymx least one yi , 1 m, B.definitions standard commonly used within field complexity theory. Detailed treatment various reduction types including found,e.g., work Ladner, Lynch, Selman (1975).standard way showing problem NP-complete proving NPreducing known NP-complete problem it. former easy briberyproblems deal with: compute winners elections polynomialtime, nondeterministically guess bribe test whether yields492fiHow Hard Bribery Elections?desired outcome. latter issue use reductions either partition problemexact cover 3-sets problem (e.g., see Garey & Johnson, 1979; Papadimitriou, 1994,general background problems proving NP-completeness).problem Partition asks whether possible split sequence integers twosubsequences equal sums.Name: Partition.PGiven: sequence s1 , . . . , sn nonnegative integers satisfying ni=1 si 0 (mod 2).3PPQuestion: set {1, . . . , n} iA si = i{1,...,n}A si ?prove main dichotomy result Section 4 need restrictive versionPn partition problem. Let s1 , . . . , sn sequence nonnegative integersi=1 si 0 (mod 2). Partition assume i, 1 n, holdsnsi1 Xsi2+n(1)i=1P(reminder: footnote 3 coursePapplies regarding handling ni=1 si 0 (mod 2)n1(i {1, . . . , n})[si 2+ni=1 si ]), ask whether exists {1, . . . , n}PPn1i=1 si . sake completeness include proofiA si = 2Partition remains NP-complete.Lemma 2.3 Partition NP-complete.Proof. Clearly, Partition NP. show, reduction standardpartition problem, Partition also NP-hard.PLet q = s1 , . . . , sn sequence nonnegative integers let 2S = ni=1 si . First,construct sequence q = s1 , o1 , . . . , sn , 2n nonnegative integers followingtwo properties. (1) q partitioned q be. (2) partition qsplits q two sequences cardinality. define si oi , 1 n,follows.si = 3i1 + 3n si .oi = 3i1 .partition s1 , o1 , . . . , sn , splits q two subsequences sum ,definednnX1X3n 1n=.3i1 = 3n +(si + oi ) = 3 +22i=1i=1Pn3. given input holds i=1 si 6 0 (mod 2), consider input syntactically illegalthus consider input member Partition. rest paper assume,reducing Partition problem (Q), syntactic constraint violatedinput, reduction whatever reduction give states, rather instantlymap fixed element Q. (often tacitly) make assumptionthat syntactically (bymean true conditions syntax polynomial-time constraints problem viaGiven assumes apply inputs) illegal inputs handled via reductions operationinputs components, rather mapped fixed element complement setreduced to.493fiFaliszewski, Hemaspaandra, & HemaspaandraClearly, partition s1 , o1 , . . . , sn , splits q two halves si belongsone oi belongs other. also immediate q partitionedq can.satisfy condition (1) add constant si oi . Define qb sequencenumbers sb1 , ob1 , . . . , sbn , obn i, 1 n,sbi = si +obi = oi + .Clearly, partition q still partition qb, since partition q splits qtwo subsequences cardinality. converse holds partition qbsplit subsequences sum Sb = + nS possiblesubsequence contains exactly n elements. (A sum n elements wouldgreater (n + 1)S would subsequence could sumto.) remains show (1) holds qb. case sbi obi2 bS. (Note sequence qb 2n elements.) Since qbgreater = 2+2ncomputed polynomial time, proof completed.exact cover 3-sets problem (X3C) asks way pick, given list,three-element subsets set B cover whole set without ever introducingelement once.Name: X3C.Given: set B = {b1 , . . . , b3t } family three-element subsets B, ={S1 , . . . , Sm }.Question: set {1, . . . , m} kAk = iA Si = B?two problemsPartition X3Chave useful tools proving NPcompleteness control manipulation problems, paper seepowerful used bribery problems. Specifically, Partitionuseful dealing weighted elections X3C particularly usefulunweighted cases.3. Pluralitysection determine complexity bribery plurality-rule elections. Pluralityrule perhaps popular election system practical use; point viewdemocracy natural appealing make decision many people prefer.However, also downsides plurality rule. Plurality rule may slight voicesminorities take account full information voters preferences.particular, candidate voters rank second bestcandidate top choice many rankings, might seem natural elect secondbest person. However, plurality blind this. fact, typically view voteplurality-rule elections vote particular candidate, namely, preferredcandidate according preference order actual vote; purposespaper thing matters voter, though mention494fiHow Hard Bribery Elections?contexts, control problems allowing deletion candidates (Bartholdi et al.,1992; Hemaspaandra et al., 2007; Hemaspaandra, Hemaspaandra, & Rothe, 2009), fullordering might important. simplicity widespread use plurality-rule electionsmake results section particular relevance.previous section somewhat carelessly mentioned plurality scoring rulevector = (1, 0, . . . , 0). course, formally speaking, really holdsnumber candidates k notion plurality-of-k-candidates scoring-ruleelection, vector 1 followed k 1 0s. one also considersystem takes input number candidates applies plurality scoringrule (for inputs number candidates). people thinkspeaking plurality elections, throughout section, results pluralitycourse fixed numbers candidates, case.simplest bribery scenario voters unweighted voterexpensive bribe voter. surprisingly, bribery easy setting.Theorem 3.1 plurality-bribery P.Proof.proof theorem simple, describe detail simpleintroduction proofs regarding bribery. give polynomial-time algorithmgiven instance bribery E = (C, V, p, k) decides whether possible make pwinner bribing k voters.algorithm works following way. Initially bribed zero voters.check whether p currently winner. so, accept. Otherwise,exceed bribe limit, pick current winner, bribe one voters (recall,mentioned earlier section, [i.e., selected winners] votersmean voters particular selected winner preferred candidate)vote p, jump back testing whether p winner. reach bribe limit(i.e., exceed bribe limit break usloop) without making p winner reject.algorithm accepts obviously bribery possible. showpossible ensure p winner via k bribes algorithm accepts.proof follows induction k. base case enough notealgorithm works correctly k = 0. induction step, let us assumeinput E = (C , V , p, k ) k < k, k positive integer, algorithmaccepts exactly possible ensure p winner via k bribes. Now, letE = (C, V, p, k) arbitrary input p become winner via k bribes.show algorithm accepts input. consider two cases.bribery k voters ensures ps victory never involves voterscurrent winners election (C, V ) clear algorithm accepts. (LetVp V set voters vote p. case bribery min(k, ||Vp ||)voters ensures p becomes winner.) Thus, let us assume briberiesmake p winner involve bribing least one voter one current winners. Let c1one winners (C, V ) (note c1 6= p) let E = (C, V , p, k 1)instance plurality-bribery obtained E bribing one c1 voters. Clearly,executing single iteration loop, algorithm transforms input one495fiFaliszewski, Hemaspaandra, & Hemaspaandraisomorphic E . assumptions inductive hypothesis algorithmaccepts transformed input.algorithm works polynomial time kV k bribes suffice make pwinner iterations executed polynomial time. theoremproven. mention approach clearly also works unique case.ease obtaining algorithm might fool us thinking briberywithin plurality system always easy. However, case.Theorem 3.2 plurality-weighted-$bribery NP-complete, even two candidates.Proof. Recall nonunique version problem default case,addressing here.plurality-weighted-$bribery NP: guess voters bribe test whetherbribe makes designated candidate winner exceed budget.remains show problem NP-hard.show NP-hardness, constructPa reduction Partition. Let s1 , . . . , snsequence nonnegative integers let ni=1 si = 2S. goal design electionE = (C, V, p, k) p become Pwinner bribery cost kset {1, . . . , n} iA si = S. define election twocandidates, p c, exactly n voters, v1 , . . . , vn , vi weightprice equal si . voters prefer c p. budget k set S. claim pbecome winner s1 , . . . , sn partitioned twoP equal-sum groups.Let us assume set {1, . . . , n} iA si = S. meansbribe vi vote p get p total vote weight (innatural sense, defined Section 2) S. makes p winner. hand,assume p made winner bribes total cost k = S. weightvoter equal price p obtain vote weight k = S.fact, p must obtain exactly vote weight S, since setup clear p gainsstrictly less vote weight c unique winner. meansway picking voters whose weights sum exactly S, thus sequences1 , . . . , sn partitioned two subsequences sum S.reduction carried polynomial time proof complete.course regards default case, namely nonunique case. unique case also follows,namely, observing enough add one voter weight 1 price 0 votesp. arguments show correct reduction.theorems show bribery easy basic case becomes intractableallow voters prices weights. natural ask additionalfeatures (prices? weights?) responsible making problem difficult. turnsneither sole reason combination yields enough powermake problem NP-complete.4Theorem 3.3 plurality-$bribery plurality-weighted-bribery P.4. However, interesting compare Theorems 4.8, 4.9, 4.13, 4.15, suggest highweights often feature responsible making problem NP-complete.496fiHow Hard Bribery Elections?Theorem 3.3 special case result prove later (namely, Theorem 3.8)thus, instead giving proof, provide informal discussion polynomial-timealgorithms plurality-$bribery plurality-weighted-bribery.direct greedy algorithm, like one underpinning Theorem 3.1, fails proveTheorem 3.3: problem one judge whether better bribe voterscurrently prefer one winners bribe voters highest weights (orlowest prices). (To see former may sometime make sense, consider electiontwo weight-4 voters, b one weight-5 voter, p one weight-2 voter.Bribing one weight-4 voter winning bribery bribing one weight-5 voter not.)approach Theorem 3.3s proof follows. Assume p r votesbribery (or weighted case, vote weight r), r number specifiedlater. make p winner, need make sure everyone else gets rvotes. Thus carefully choose enough cheapest (heaviest) voters candidates defeatp bribing vote p candidate p r votes.simply make sure p gets least r votes bribing cheapest (theheaviest) remaining voters. process p ever becomes winner withoutexceeding budget (the bribe limit) know bribery possible.pick value r? case plurality-$bribery, simply runprocedure possible values r, i.e., 0 r kV k, accept exactlysucceeds least one them. plurality-weighted-bribery slightly trickier approachworks. Namely, enough try values r obtained vote weightcandidate (other p) via bribing number heaviest voters.polynomially many values whole algorithm works polynomialtime. intuition using values r following: (a) bribing voterscandidate one always limit oneself heaviest ones, (b) successfulbribery value r ps vote weight least r , candidatesvote weight r , candidate c 6= p cs vote weightexactly r . algorithm, essence, performs exhaustive search (within heavilylimited search space) value r .Note algorithms assume bribe people vote p.reasonable method bribing one wants p become winner, alsopotential real-world downsides: people bribe, likely maymalicious attempts detected work p. minimize chanceshappening might instead bribe voters vote pcandidate(s). way p get extra votes might able take away enoughpopular candidates become winner.Definition 3.4 plurality-weighted-negative-bribery defined pluralityweighted-bribery, except restriction illegal bribe people votedesignated candidate.problem plurality-negative-$bribery defined analogously. call settingnegative-bribery motivation p get votes him- herself,take away others. Unlike Theorem 3.3, version problem drawssharp line complexity bribing weighted priced voters.497fiFaliszewski, Hemaspaandra, & HemaspaandraTheorem 3.5 plurality-weighted-negative-bribery NP-complete, plurality-negative$bribery P.Proof. first give polynomial-time algorithm plurality-negative-$bribery. LetE = (C, V, p, k) bribery instance want solve. need make p winnertaking votes away popular candidates distributing among less popularones. (The previous sentence said winner since usual addressing nonuniquecase. However, clear similar approach works unique case, i.e., casegoal make p winner.)partition set candidates three sets: candidates defeat p,votes need taken away, candidates defeated p,give extra votes, candidates score p. unweighted case,score E (c) mean number voters within E prefer candidate c.weighted case, score E (c) means total vote weight voters within E prefer c.Cabove = {c | c C, score E (c) > score E (p)}.Cbelow = {c | c C, score E (c) < score E (p)}.Cequal = {c | c C, score E (c) = score E (p)}.Since voters weight (weight 1) current case, plurality-negative$bribery, hard see successful negative briberysuccessful negative bribery bribe voters Cequalalso wont bribe voters move within group, e.g., bribing voter shiftone Cbelow candidate another. (However, weights case, crazy bribessometimes needed; see footnote 5.) make sure p becomes winner,candidate c Cabove need bribe many cs voters neededreducePscore score E (p). Thus, altogether, need bribe cCabove (score E (c)score E (p)) voters. numberP votes candidate c Cbelow accept withoutpreventing p winning cCbelow (score E (p) score E (c)). Thus, hard seenegative bribery possible exactly following inequality holds.XX(score E (c) score E (p))(score E (p) score E (c)).(2)cCabovecCbelowinequality (2) hold immediately reject. Otherwise, remains checkwhether cost negative bribery within budget: every candidate c CaboveletP bc cost bribing cs score E (c) score E (p) cheapest voters. holdscCabove bc k accept, negative bribery possible. Otherwise reject.Clearly, algorithm works polynomial time. correctness follows factneed make candidates Cabove score score E (p)c Cabove bc lowest possible cost achieving that. Equation (2) guaranteesvotes taken candidates Cabove distributed among Cbelow withoutpreventing p winning.let us turn showing NP-hardness plurality-weighted-negative-bribery.must careful here. plurality-negative-$bribery, argued one could withoutloss generality ignore Cequal , i.e., one never needs bribe voters Cequal ,ignore bribing voters one candidate group (Cbelow , Cequal ,498fiHow Hard Bribery Elections?Cabove three groups) another candidate within group.hard see claim false weights case, essentially due fact that,example, members Cequal Cbelow useful making changethat is,splitting large weights small ones.5 However, image reductionconstruct, Cequal contain p, bribing votes change p forbiddennegative setting, bribing votes change away p clearly never requiredsuccess; setting Cequal fact play interesting role.similarly, kCabove k = kCbelow k = 1 image reduction,worry within-a-group bribes.Now, start construction show NP-hardness plurality-weighted-negativebribery. particular, construct reduction Partition. Let s1 , . . . , snsequence nonnegative integers. design instance pluralityweighted-negative-bribery bribery possible s1P, . . . , sn splittwo parts sum value. Let ni=1 si = 2S.elections three candidates: p, c1 , c2 , n + 1 weighted voters:1. v0 weight S, whose preferences p > c1 > c2 ,2. v1 , . . . , vn weights s1 , . . . , sn , preferences c1 > c2 > p.goal briber ensure ps victory via bribing k = n + 1 voters (i.e.,voters). Note reasonable bribes ones transfer votes vi ,1 n, c1 c2 . (Strictly speaking, v0 could legally bribed vote c1 c2 ,safely ignored.) set {1, . . . , n}Xsi = S,(3)iAcould bribe voters vi , A, vote c2 candidates would winners.hand, p end winner bribery ask anyone votep, set satisfies Equation (3): p winner electionc1 c2 vote weight exactly S. However, beginning c1 holds 2Svote weight successful bribery needs transfer exactly vote weight c1c2 . possible (3) holds A.finish proof, observe reduction computed polynomialtime.Theorems 3.2 3.3 state plurality-weighted-$bribery NP-completeattempt make simpler immediately pushes back realm P. fact,5. see this, consider setting candidate Big preferred candidate one weight-10 voterone weight-2 voter, candidate p preferred candidate one weight-10 voter, candidateMakeChange preferred candidate ten weight-1 voters, candidate SmallOnepreferred candidate one weight-9 voter, candidate SmallTwo preferred candidate oneweight-9 voter, limit number bribes 3. Cabove = {Big}, Cequal = {p, MakeChange},Cbelow = {SmallOne, SmallTwo}. Note successful negative bribery leavesMakeChange uninvolved. However, moving Big MakeChange weight-2 voter,moving one weight-1 voter SmallOne SmallTwo MakeChange, successfulnegative bribery. example uses Cequal make change, one construct similar examplesrequire one bribe votes one member Cbelow another member Cbelow .499fiFaliszewski, Hemaspaandra, & Hemaspaandrasituation even dramatic. NP-complete problem plurality-weighted-$briberyassume prices weights encoded binary. However, either pricesweights encoded unary, problem, again, becomes easy.proceed formal proof fact, let us discuss issue informal manner.unary encoding either one weights prices matter? reasonthat, example, weights encoded unary trivially linearlymany (with respect size input problem) different total weights subsetsvoters. Together additional tricks allows us use dynamic programmingobtain solution.Definition 3.6 plurality-weighted-$briberyunary defined exactly pluralityweighted-$bribery, except prices encoded unary. plurality-weightedunary $bribery plurality-weighted-$bribery except weights encoded unary.tempting use exactly proof approach one hinteddiscussion Theorem 3.3, i.e., split bribery two parts: demoting otherspromoting p. However, would correct. Sometimes optimal waygetting scores candidates certain threshold r prevents onegetting optimal bribe complete problem. Consider elections threecandidates, c, d, p, three voters v1 , v2 , v3 v1 price weightequal 10, v2 price weight equal 7, v3 price 1, 000, 000 weight10. v1 v2 prefer c v3 prefers d. Clearly, optimal bribe problemrequires threshold 10. optimal way getting c vote weight 10bribing v2 . However, point making p winner requires bribing v1 well. Yet,bribing v1 cheaper way making p winner getting c below-or-equal-to10 threshold.refer plurality-weighted-$briberyunary unary prices case,plurality-weightedunary -$bribery unary weights case. give overviewalgorithm works unary prices case, input E = (C, V, p, k). unaryweights case handled analogously. main idea that, using factlinearly many possible prices paid, argue exists polynomialtime computable function Heaviest(E, C , , r)where C subset candidates,integer price, r integer thresholdthat gives maximum voteweight obtain bribing voters candidates C1. cost bribery ,2. bribery every candidate C vote weight r.test whether possible make p winner spending k dollars,need find threshold r score E (p) + Heaviest(E, C {p}, k, r) r, i.e.,weight p originally via bribed voters least great post-briberyweight candidates. Unfortunately, case plurality-weighted$briberyunary cannot try thresholds since may exponentially manythem. Instead use strategy similar one hinted discussingTheorem 3.3. every successful bribery (in elections least two candidates)candidate c 6= pnamely, candidate(s) p greatest500fiHow Hard Bribery Elections?post-bribery total weightthat either tied-with-p winner loses p.use after-bribery vote weight candidate threshold briberyvoters candidates. course, neither know candidatevote weight would successful bribery. Nonetheless, trycandidates c 6= p candidate possible sub-budget b kask maximum amount additional weight get p bribing csvoters allowed spend b (this require solving certain instancesknapsack problem). Then, using thus obtained threshold, bribe votersrest candidates. (at most) linearly many candidates (at most)linearly many prices yields (at most) polynomially many combinations.Let us describe plan implemented. longer limitunary prices case, describe cases parallel. Let E = (C, V, p, k)input. candidate c C defineVEc = {v V | c preferred candidate v}.Since additional restrictions makes sense bribe voterssupport p. given candidate c C, describe bribing options eitherfunction gives highest weight cs voters bribe b dollars functiongives lowest price needed gain vote weight least w bribing cs voters.heaviest(E, c, b) = max{(U ) | U VEc (U ) b}.cheapest(E, c, w) = min{(U ) | U VEc (U ) w}.c candidate E, functions undefined. restproof, take max min empty set undefined. Note ccandidate E, heaviest(E, c, b) defined b 0 cheapest(E, c, w) definedw (VEc ). Also note heaviest easily computed polynomial timeunary prices case cheapest easily computed polynomial timeunary weights case. cases simply use dynamic programming solutionsappropriate optimization variant knapsack problem.6 generalizefunctions give us information best bribes regarding sets candidates.U V , define bribed (E, U ) bribery problem exactly like Evoters U bribed vote p. defineficfi (UcC ) ((U ) b),Heaviest(E, C , b, r) = max (U ) fifi(c C )[score bribed(E,U ) (c) r]ficfi (U) ((U ) w)cC.Cheapest(E, C , w, r) = min (U ) fifi(c C )[score bribed(E,U ) (c) r]C subset Es candidate set, functions undefined.6. knapsack problem following. Given set items, price weight ,possible select items total weight least W , without exceeding total price K? wellknown knapsack problem polynomial-time dynamic programming algorithm eitherprices encoded unary weights encoded unary. (See work Martello & Toth,1990, background/reference knapsack problem.)501fiFaliszewski, Hemaspaandra, & HemaspaandraLemma 3.7 consider elections voter priceweight. prices encoded unary algorithm computes Heaviestpolynomial time. weights encoded unary algorithm computesCheapest polynomial time.Proof. Note unary prices case linearly many sub-budgets bneed compute value Heaviest, namely 0 b (V ), unaryweights case linearly many weights w need evaluate Cheapest,namely 0 w (V ). Using fact provide dynamic programming algorithmscomputing functions. base case following: c candidateE, functions undefined. Otherwise,heaviest(E, c, b) score E (c) heaviest(E, c, b) r,Heaviest(E, {c}, b, r) =undefinedotherwise.cheapest(E, c, w)score E (c) w r,Cheapest(E, {c}, w, r) =cheapest(E, c, score E (c) r) otherwise.following observation allows us compute Cheapest Heaviest larger sets.assume C contain c. candidates C {c} candidatesE, functions undefined. Otherwise,Heaviest(E, C {c}, b, r) = max{Heaviest(E, C , b , r) + Heaviest(E, {c}, b b , r) |0 b b Heaviest(E, C , b , r) Heaviest(E, {c}, b b , r) defined}.Cheapest(E, C {c}, w, r) = min{Cheapest(E, C , w , r) + Cheapest(E, {c}, w w , r) |0 w w Cheapest(E, C , w , r) Cheapest(E, {c}, w w , r) defined}.Thus, unary prices case compute Heaviest(E, C , b, r) using dynamic programming polynomial time. applies Cheapest(E, C , w, r) unaryweights case.Theorem 3.8$bribery P.plurality-weighted-$briberyunaryplurality-weightedunary -Proof. Algorithms problems similar describe(nonunique) unary prices case detail. provide pseudocode (nonunique)unary weights case, omit proof correctness, analogous proofunary prices case. mention passing two unique cases easilyobtained well, via natural modifications algorithm.Figure 1 shows procedure unary prices case. idea algorithmfollowing: Suppose set B voters bribe membersB vote p p becomes winner. assume candidate c,cs voters bribed optimally, i.e., cheaper way getting (orgreater) vote weight bribing different subset cs voters. candidatec votes among non-p candidates bribery. Thus, decidebribery possible enough test whether candidate c 6= p sub-budgetb, 0 b k, bribing cs voters optimally, spending b dollars, stillpossible bribe (without, overall, exceeding budget) voters candidatesway502fiHow Hard Bribery Elections?procedure UnaryPricesBribery(E = (C, V, p, k))beginC = C {p};k (V )return(accept);c Cb 0 b kbeginw = heaviest(E, c, b);r = score E (c) w ;w = Heaviest(E, C {c}, k b, r);w defined score E (p) + w + w rreturn(accept);endreturn(reject);endFigure 1: main procedure plurality-weighted-$briberyunary .1. candidate ends vote weight higher c,2. enough voters bribed p becomes winner.algorithm tests exactly case accepts so. (Though if-then linemight first seem focus candidates C {c} beat p, note cspost-bribery score r, line handles c also.) reasoning, briberypossible algorithm accepts. also clear algorithm acceptsbribery indeed possible. Since functions heaviest Heaviest computedpolynomial time, whole algorithm runs polynomial time. Thus,plurality-weighted-$briberyunary P.analogous algorithm works unary weights case, see Figure 2. proofcorrectness analogous unary prices case.Theorem 3.8 particularly interesting says plurality-weighted-$briberydifficult choose weights bribe prices high. However,prices set voters, many cases one could assume would setfairly low, sense rendering bribery problem easy.Another possible attack complexity plurality-weighted-$briberyapproximation algorithms. fact, using Theorem 3.8, Faliszewski (2008) obtained fullypolynomial approximation scheme plurality-weighted-$bribery. Many researchers asktypical-case complexity practically encountered NP-complete problems (seework Conitzer & Sandholm, 2006; Procaccia & Rosenschein, 2007; Erdelyi, Hemaspaandra, Rothe, & Spakowski, 2007, discussions issue context voting problems;see also Erdelyi, Hemaspaandra, Rothe, & Spakowski, appear; Erdelyi, Hemaspaandra,503fiFaliszewski, Hemaspaandra, & Hemaspaandraprocedure UnaryWeightsBribery(E = (C, V, p, k))beginC = C {p};c Cw 0 w (VEc )beginb = cheapest(E, c, w );r = score E (c) w ;b = Cheapest(E, C {c}, r (score E (p) + w ), r);b defined b + b kreturn(accept);endreturn(reject);endFigure 2: main procedure plurality-weightedunary -$bribery.Rothe, & Spakowski, 2009), clearly important direction.7 However, oftendifficult come distribution inputs realistic simple enoughstudy. hand, providing good polynomial-time approximation algorithmwould worst-case result: matter difficult instance would given,could compute decent answer. Recent papers Brelsford et al. (2008), Faliszewski (2008),Zuckerman et al. (2008) take steps interesting direction.7. much excitement recent paper Elections Manipulated Often (Friedgut,Kalai, & Nisan, 2008, see also Dobzinski & Procaccia, 2008; Xia & Conitzer, 2008), indeed refereecommented us paper proves (under certain assumptions) elections manipulabletime. However, one fact bit careful one claims. lowerbound paper (given assumptions) establishes frequency manipulation is, typeproblem related unpriced bribery (but involves choosing single manipulator drawingvotes ones distribution), (1/kV k), typical approach manipulationfocusing single voter, gives lower bound frequency manipulation (1/kV k2 ).isnt most, time, rather goes zero asymptotically. Even lower bound couldtremendously raised (1), still might mean manipulation frequency 0.00001 percenttimenot frequency means election methods frequently open manipulation.exciting, active research direction suspect future work much clarifyupper lower bounds hold frequency manipulation (both single-manipulatorcase coalition manipulation case), assumptions election systems neededobtain results. Changing topics, mention cases time detours aroundNP-hardness election-complexity results already obtained. example, although winnerproblem Dodgson elections known complete parallel access NP, two recent papers,McCabe-Dansted, Pritchard, Slinko (2008) Homan Hemaspaandra (2009), shown(under particular assumptions distributions relationship numberscandidates voters) heuristic algorithms rigorous sense correcttime.504fiHow Hard Bribery Elections?4. Bribery Versus Manipulation, Two Dichotomy Theoremsprevious section provided detailed discussion complexity bribery pluralityvoting. obtain results carefully hand-crafted altered various algorithmsreductions. Designing algorithms reductions specific bribery problems certainlyreasonable approach, even better would find general tools establishingcomplexity bribery elections. general tools would especially interestingallowed one inherit results already existent literature election systems.section implement plan studying relationships briberymanipulation, showing obtain results using relationships find.next section, studying ways integer programming employed solvebribery problems continue emphasis exploring flexible tools establishingcomplexity bribery. There, using theorem Lenstra show many bribery problemsregarding elections fixed-size candidate sets P, even voterssuccinctly represented. (Regarding coming results studying relationshipbribery manipulation, generally commend reader issue findingeven extensive links problems bribery, manipulation, control.find natural important direction.)Manipulation flavor somewhat similar bribery, difference manipulation set voters may change preference lists specified input.Formally, E election rule E-manipulation following problem (e.g., seeBartholdi et al., 1989a; Conitzer et al., 2007).Name: E-manipulation.Given: set C candidates, collection V voters specified via preference lists, setmanipulative voters (without loss generality, including members V ),candidate p C.Question: way set preference lists voters electionrule E voters V together choose p winner?Instances manipulation problems described tuples (C, V, S, p), Clist candidates, V list voters (in format bribery problems),list manipulative voters, p designated candidate voterswant winner (a unique winner, unique case).Manipulation, like bribery, comes many flavors. may asked make punique winner winner, voters may weights (in case specifiedtogether weights voters S), etc. Bribery viewed manipulationset manipulators fixed advance deciding manipulatepart challenge. Note check whether bribery successful given inputsimply try possible manipulations k voters, k number bribeswilling allow. way, fixed k, disjunctively truth-table reducebribery problem analogous manipulation problem. (Note priceslimit bribing k voters effect unit prices budget k.)Theorem 4.1 Let k arbitrary positive integer. Let B bribery problems,following constraints: Voters prices (i.e., consider $bribery505fiFaliszewski, Hemaspaandra, & Hemaspaandraproblems) bribing k voters forbidden (that is, requireyes-instance (C, V, p, k ) B k k). Let analogous manipulationproblem, i.e., manipulation problem election system, weighted votersB allows that, allowing manipulating set contain number voters 0k. holds B pdtt M.Proof. show B pdtt need give polynomial-time procedureinput x outputs list strings y1 , . . . , ym x B least one yi ,1 m, M. describe procedure.Let x input string. first check whether x parsed instanceB (reminder: is, x meets syntactic constraints B).output empty list terminate; otherwise decode V , voter set, k k,maximum number voters bribed, string x. every subset Wk = min(k , kV k) elements V form instance manipulation problemvoter set V W manipulating set equal W . go k -elementsubsets output list manipulation instances formed.procedure clearly works polynomial time kVk k = O(kV kk )sets test form instances manipulation polynomial time.manipulation instances output bribery possible; enough bribeexactly voters selected manipulating group. hand, briberypossible, least one instances output belongs M, namely oneincludes voters would bribe.simple, result still powerful enough allow inheritance resultsprevious papers. Bartholdi et al. (1989a) discuss manipulation single votersTheorem 4.1 translates results bribery case. particular, translation saysbribery k = 1 P plurality, Borda count, many systems.strengthen Theorem 4.1 constant-bounded bribery general bribery?answer no: election systems bribery NP-complete manipulationP. particular, manipulation approval voting (both weightedunweighted case) P size manipulating set: manipulating group simplyapproves favorite candidate nobody else.8 However, followingtheorem, bribery approval voting NP-complete.Theorem 4.2 approval-bribery NP-complete.Proof. Clearly, approval-bribery NP. NP-completeness follows reductionX3C.Let B = {b1 , . . . , b3t } let = {S1 , . . . , Sm } family three-element subsetsB. Without loss generality, assume t; otherwise exact cover impossible.i, 1 3t, let number sets Sj contain bi . input (B, S)construct approval-bribery instance E = (C, V, p, k), k = t, set candidates Cequal B {p}, following voters.1. Si voter vi approves exactly members Si .8. Meir et al. (2008) somewhat different flexible setting previously noted approvalmanipulation P one manipulator.506fiHow Hard Bribery Elections?2. bi + 1 voters approve bi .3. voters approve p.Note p gets approvals bi , 1 3t, gets + 1 approvals.claim p made winner bribing voters B exactcover sets S.First assume set kAk = iA Si = B. make pwinner, bribe vi approve p. result p gets approvalsbi loses exactly one approval. Thus, candidates winners. hand,assume bribery voters makes p winner. bribed votercontributes one additional approval p. Thus, p get approvals.candidate B + 1 approvals, bribery needs take away least oneapproval candidate B. Since bribe voters, happenbribe voters vi correspond cover B.reduction computed polynomial time.course, number bribes bounded fixed constant then,Theorem 4.1, approval-bribery solved polynomial time.mention bribery approval elections actually easy lookslightly different model. bribery problems allow us completely modify approvalvector voter, may demanding. voter might willing changeapproval vectors entries change completely.Definition 4.3 approval-bribery problem takes input descriptionapproval election along designated candidate p nonnegative integer k, askswhether possible make p winner k entry changes (total) approvalvectors. approval-$bribery defined analogously, extra twist changingentry approval vector may different price.9different prices flipping different entries approval-$bribery models possibility voter might willing change approval candidatescandidates. modified problems turn easy. fact,easy even weights prices, provided one encoded unary.Theorem 4.4$bribery P.approval-weighted-$briberyunaryapproval-weightedunary -Proof. polynomial-time algorithm provide based observationapproval-weighted-$briberyunary approval-weightedunary -$bribery getting voteweight favorite candidate (carefully) treated separately demotingcandidates. (This basically approval voting bribery model costslinked entries voters approval vectors candidates point total weightedaddition, voters, candidates 0-or-1 entry voter.)9. referee points out, technical perspective one view voters approval-bribery (andvariants) broken multiple plurality voters turned off. Partially duesimilarity, various flavors approval-bribery computational properties similarcorresponding variants plurality-bribery.507fiFaliszewski, Hemaspaandra, & Hemaspaandradivide bribery two phases: First, bribe voters approve p,favorite candidate, second, bribe enough voters retract approvalscandidates still defeat p. polynomially many relevant vote weightsp may obtain bribery, try all.Let E = (C, V, p, k) bribery instance need solve. candidate c, priceb, subset voters V , define heaviest(V , c, b) highest vote weightvoters V whose approval c switched spending b dollars. Similarly,candidate c, vote weight w, subset voters V , define cheapest(V , c, w)lowest price switch approval-of-c voters V totalweight least w. proof use sets V either voters approve cvoters disapprove c. Note heaviest(V , c, b) defined b 0cheapest(V , c, w) defined w (V ). Section 3, heaviest easilycomputed polynomial time unary prices case cheapest easily computedpolynomial time unary weights case. addition, cheapest computedpolynomial time unary prices case. Notecheapest(V , c, w) = min{b | heaviest(V , c, b) w}.Since polynomially many prices try, done polynomial time.Figure 3 gives pseudocode procedure UnaryPricesApproval, decidesapproval-weighted-$briberyunary . score E (c) denotes number approvals candidate c election E. procedure simply tries relevant weights p could obtainbribery tests whether possible, them, bring candidates vote weight p without exceeding budget. procedurecorrect separation achieved (as discussed above, applied withinproof framework trying thresholds) issue bribing voters approve p issue bribing approve candidate. Also,cheapest heaviest computable polynomial time, procedure works polynomial time. analogous procedure decides unary weights case: Simply change lineb = 0 k w = 0 (V ) line w = heaviest(V , p, b)b = cheapest(V , p, w).prices weights encoded binary, approval-weighted-$bribery becomesNP-complete.Theorem 4.5 approval-weighted-$bribery NP-complete.Proof. immediate approval-weighted-$bribery NP. show NP-hardness,constructPa reduction Partition. Let s1 , . . . , sn sequence nonnegativeintegers let ni=1 si = 2S. construct election E candidates p cn + 1 voters, v0 , . . . , vn , following properties.1. v0 weight S, approves p, changing v0 approvals costs 2S + 1.2. vi , 1 n, weight si , approves c, changing vi approval p costssi , changing vi approval c costs 2S + 1.claim p made winnerbribery costPset {1, . . . , n} iA si = S.508fiHow Hard Bribery Elections?procedure UnaryPricesApproval(E = (C, V, p, k))begink (V )return(accept);V = {v | v V v approve p};b = 0 kbeginw = heaviest(V , p, b);r = score E (p) + w;k = k b;c C {p}beginVc = {v | v V v approves c};score E (c) > rk = k cheapest(Vc , c, score E (c) r);endk 0 return(accept);endreturn(reject);endFigure 3: procedure UnaryPricesApproval.First suppose p made winner bribery cost S.bribe voters v1 , . . . , vn approve p. election E, p approvals c2S approvals, bribery needs give p least extra approvals. Since changingvi approval p costs si , weight vi also si , follows p gains exactlyapprovals, weights bribed voters v1 , . . . , vn add exactly S.implies sequence s1 , . . . , sn partitioned two subsequences sumS.Phand, assume set {1, . . . , n} iA si = S.bribe voters vi , A, approve p. result, p c voteweight 2S winners. reduction computed polynomialtime thus theorem proved.above-discussed bribery models approval appropriate dependssetting. example, bribery seems natural look web treatweb pages voting linking pages. certainly easier ask webmasteradd/remove link completely redesign page. point reader workFaliszewski (2008) discussion bribery scenarios similar bribery .somewhat lengthy discussion approval bribery, let us returncentral goal relating bribery manipulation. Theorem 4.1 managed disjunctively truth-table reduce restricted version bribery manipulation. discussiontheorems follow show working opposite direction, reducing manipulationbribery, first might seem difficult, fact fruitful.509fiFaliszewski, Hemaspaandra, & Hemaspaandrareason reducing manipulation bribery appears difficultbribery allows freedom person interested affecting elections. embedmanipulation within bribery, find way expressing factcertain group voters bribed (or, least, expressing factsuccessful bribery also one bribes manipulators). fairlyeasily implement plan, though cost reducing stronger bribery model,namely bribery prices.Theorem 4.6 Let manipulation problem let B analogous $briberyproblem (for election system). holds pm B.Proof. Let = (C, V, S, p) instance M. design instance B BB = (C, V , p, 0),1. V equal V , except voter price 1,2. equal S, except voter price 0 fixed arbitrary preferencelist.Since bribery budget set zero, voters may possibly bribe. preference lists voters bribery directly correspondmanipulation . reduction carried polynomial time.Clearly, Theorem 4.6 holds even $bribery problems prices representedunary required come set {0, 1}. Theorem 4.6 useful allows usinherit powerful results theory manipulation. HemaspaandraHemaspaandra proved following dichotomy result (see also Procaccia & Rosenschein,2007; Conitzer et al., 2007).Theorem 4.7 (Hemaspaandra & Hemaspaandra, 2007) Let = (1 , . . . , )scoring protocol. case 2 = 3 = = , -weightedmanipulation NP-complete; otherwise, P. result holds uniquenonunique variants.Combining two theorems Theorem 3.2 immediately classifycomplexity weighted-$bribery scoring protocols.Theorem 4.8 scoring protocol = (1 , . . . , ), 1 = -weighted$bribery P; otherwise NP-complete.Proof. consider three cases.1. 1 = = .2. 1 > 2 = = .3. settings.first case, 1 = = , -weighted-$bribery trivially P candidatesalways tied. remaining two cases, note -weighted-$bribery clearly NP.remains show NP-hardness.510fiHow Hard Bribery Elections?second case, 1 > 2 = = , employ proof Theorem 3.2.Theorem 3.2 shows NP-hardness (1, 0)-weighted-$bribery. easy see2 pad reduction 2 candidates never ranked firstm1z }| {obtain NP-hardness (1, 0, . . . , 0)-weighted-$bribery. Note describes electionsequivalent plurality (i.e., candidate winner electionm1z }| {would also winner (1, 0, . . . , 0) election voters candidates;see Observation 2.2 paper Hemaspaandra & Hemaspaandra, 2007). Thus,get NP-completeness -weighted-$bribery case since least twocandidates.third case follows combining Theorem 4.6 Theorem 4.7. Since -weightedmanipulation many-one reduces -weighted-$bribery -weighted-manipulationNP-complete -weighted-$bribery NP-hard. exhausts cases.Theorem 4.8 applies $bribery, course also interesting ask happenscase voters prices. bribery remain NP-complete?express constraints bribery without using direct embedding above?following dichotomy theorem shows answer Yes, fewer cases.Theorem 4.9 scoring protocol = (1 , 2 , . . . , ), 2 = 3 = =-weighted-bribery P; otherwise NP-complete.2 = 3 = = either -weighted-bribery trivially P (if 1 == ) solved using algorithm plurality-weighted-bribery. coreproof show NP-hardness. would nice reducingcorresponding manipulation problems (which share characterizations boundary lineregarding s). seems work, Lemma 4.11 constructreduction right properties whenever inputs satisfy additional condition,namely, weight lightest manipulating voter least doubleheaviest nonmanipulator. would suffice thus-restricted manipulation problemNP-hard. Lemma 4.12 shows thus-restricted manipulation problem NPhard. examining manipulation-dichotomy proof HemaspaandraHemaspaandra (2007) noting apply papers reduction Partition (seeSection 2.3) rather Partition guarantee restriction mentioned above.Definition 4.10 -weighted-manipulation mean manipulation problem weighted-manipulation restriction manipulative voter weightleast twice high weight heaviest nonmanipulative voters. instance restriction violated considered element -weightedmanipulation .Lemma 4.11 Let = (1 , . . . , ) scoring protocol. -weighted-manipulation pm-weighted-bribery.Proof. Without loss generality assume = 0. 6= 0consider scoring protocol = (1 , 2 , . . . , ) instead. Giveninstance = (C, V, S, p) manipulation problem, construct B = (C, V , p, kSk),511fiFaliszewski, Hemaspaandra, & Hemaspaandrabribery instance, successful manipulation withinsuccessful bribery within B. assume fulfills -weighted-manipulationrequirements regarding relative weights voters V S. not, output fixedB successful briberies.reduction works constructing V = V , set votersfixed arbitrary preference list p least preferred candidate. Clearly,manipulation possible within bribery works B. showdirection also holds arguing successful bribery within B exists,successful bribery affects voters . implies viewedmanipulative group.Let us assume way bribing kSk voters V pbecomes winner. bribed voters theorem proven. Otherwise,select bribed voter v V . bribing v, p gains (1 + 1 ) (v) pointscandidate c 6= p. (The first 1 p get 1 additional pointsbribery, second 1 c lose 1 votes.) However,instead bribing v would bribe voter v , p would gain least 1 (v ) pointsc. (We would bribe v put p preferred candidate shiftcandidates back.) Since holds (v ) 2(v), might well bribev instead v, p would still winner. Thus, p made winner, pmade winner bribing voters .reduction easily computed polynomial time.remains show restricted version manipulation NP-completescoring protocols nonrestricted version is.Lemma 4.12 = (1 , . . . , ) scoring protocol case2 = 3 = = , -weighted-manipulation NP-complete.Proof. Let = (1 , . . . , ) scoring protocol 2 6= . use Hemaspaandra Hemaspaandras (2007) proof result called Theorem 4.7show NP-completeness -weighted-manipulation . Clearly, -weighted-manipulationNP need prove NP-hardness.Hemaspaandra Hemaspaandras (2007) proof Theorem 4.7 reduces Partition (restricted positive integers) -weighted-manipulation. close inspection proof10shows exist constants c 2 dependevPery sequence positive integers s1 , . . . , sn ni=1 si = 2S, HemaspaandraHemaspaandra reduction outputs manipulation problem following properties.1. nonmanipulative voter weight cS,2. weights manipulative voters ds1 , ds2 , . . . , dsn .use facts provide reduction Partition -weighted-manipulation .reductionworks follows. Let s1 , . . . , sn input sequence nonnegativeP2integers, ni=1 si = 2S, i, 1 n, holds si 2+nS. (As perfootnote 3, conditions hold return fixed string -weighted10. repeat proof here. Interested readers referred paper HemaspaandraHemaspaandra (2007).512fiHow Hard Bribery Elections?manipulation .) Without loss generality, assume > 0, thus s1 , . . . , snpositive integers. Let f reduction given proof Theorem 4.7 paperHemaspaandra Hemaspaandra (2007). compute f ((s1 , . . . , sn )) = (C, V, T, p).(Reduction f works general Partition so, since already checked special properties required Partition , work correctly input.) is, s1 , . . . , snpartitioned successful manipulation (C, V, T, p). Unfortunately, cannot output (C, V, T, p) necessarily fulfill conditionvoters weights. Recall ensure manipulative voter weightleast twice high weight heaviest nonmanipulative voters. Letsmin = min{sj | 1 j n}. (C, V, T, p), least weight voter exactly dsmin ,highest weight voter V cS. However, split voter vV . weights voters participate manipulation irrelevantlong total weight voters given preference order change. Thus,replace voter high weight several voters preferenceorder lower weights. case, need make sure nonmanipulativevoter weight 21 dsmin . Since heaviest nonmanipulative votersweight cS, need replace voter v V&'cS(4)21 dsminvoters, weight 12 dsmin . Since 2, > 0, smin positive integer,22+n smin , bound (4)&cS12 dsmin'cSsmin&cS2S2+n'=c(n + 2),2clearly polynomially bounded n. Thus, splitting voters easilyperformed polynomial time, since change result manipulation,theorem proven.proof Theorem 4.9 simply combines Lemmas 2.3, 4.11, 4.12.Theorem 4.9 shows bribery within weighted scoring protocols is, cases,difficult. Though weighted bribery light Theorem 4.9 easy trivial elections (1 =), plurality, even pluralitys equivalent clones (all scoring systems 1 > 2 == ), voters weighted also prices (by Theorem 3.2)bribery also becomes difficult case plurality pluralitys equivalent clones.interesting ask whether voters prices weighted also yieldsdichotomy result. Theorem 4.13 shows, behavior scoring protocols respectpriced voters different respect weighted ones.Theorem 4.13 Let = (1 , . . . , ) scoring protocol. -$bribery P.Proof. give polynomial-time algorithm -$bribery. Let E = (C, V, p, k)instance problem. First, observe considering scoring protocol =(1 , . . . , ) we, definition, limit scenario candidates,fixed constant. implies constant number different preference513fiFaliszewski, Hemaspaandra, & Hemaspaandraorders, o1 , . . . , om! , voters might have. partition V sets V1 , V2 , . . . , Vm!Vi contains exactly voters preference order oi . Vi mightempty Vi n elements, n = kV k.bribery within E described giving two sequences integers, b1 , . . . , bm!d1 , . . . , dm! , 0 bi kVi k 0 di n, 1 m!,m!Xi=1bi =m!Xdi .i=1bi says many voters Vi bribing. sufficient givenumbers biPsince want bribe cheapest members Vi . bribeb = m!i=1 bi voters, need decide preferences assign them.described sequence d1 , . . . , dm! : di says many b voters assignedpreferences oi . Since voters indistinguishable, specifying numbersenough.remains observe nm! sequences b1 , . . . , bm!nm! sequences d1 , . . . , dm! b. Thus, n2(m!) sequences tryout. pair sequences easy check whether performing describedbribery p becomes winner whether budget exceeded. Thus, -$briberyP.algorithm given proof Theorem 4.13, almost changes, usedprove following corollary.Corollary 4.14 Let E election system (a) fixed number candidates outcome computable polynomial time (b) outcome dependorder votes. fixed number candidates E-$bribery P.Theorem 4.13 stands sharp contrast Theorem 4.9. natural ask pricesweights exhibit differing behavior. One answer weighted casevoters retain individualitytheir weightsthroughout whole process bribery.hand, priced case voters disassociated pricessoon decide bribe them. decide bribe particular priced votersimply need add price total budget, voterindistinguishable bribed ones. Precisely observation facilitatedproof Theorem 4.13.algorithm given proof Theorem 4.13 rather disappointing runningtime. nO(m!) polynomial setting, one would certainly preferalgorithm whose time complexity depend way. particular, wouldnice algorithm running time polynomial n + m. However,algorithm exists P = NP. follows proof fact approval-briberyNP-complete. proof showed reduce X3C approval-briberyway voter approves 3 candidates. polynomial palgorithm ran time p(kCk + kV k) every scoring protocol , couldsolve X3C reducing approval-bribery embedding approval-briberyproblem -bribery problem = (1, 1, 1, 0, . . . , 0), possibly addingdummy candidates. embedding straightforward describe detail.514fiHow Hard Bribery Elections?Let = (1 , . . . , ) scoring protocol case 2 == . Theorem 4.9 know -weighted-bribery NP-complete. alsoknow, Theorem 4.13, -$bribery P. clearly holds -weighted-$briberyNP-complete, interesting ask whether NP-completeness -weightedbribery -weighted-$bribery holds possibly exponentially large valuesweights, problems remain NP-complete even weights encodedunary? turns out, following theorem, high weight values necessaryNP-completeness.Theorem 4.15 Let = (1 , . . . , ) scoring protocol. -weightedunary -$briberyP.Proof. Let = (1 , . . . , ) scoring protocol. proof theorem cashesobservation made proof Theorem 4.13:finitely many different preference orders, polynomially many substantiallydifferent ways bribing.Let E = (C, V, p, k) bribery problem let o1 , . . . , om! different possiblepreference orders C. partition V m! disjoint sets V1 , . . . , Vm! Vicontains exactly voters preference order oi . bribery within E describedsequence m! vectors bi = (bi,1 , bi,2 , . . . , bi,m! ), 1 m!, i, j,1 i, j m!, bi,j nonnegative integer i, 1 m!,m!Xbi,j = (Vi ).j=1interpretation vector bi voters Vi partitioned m! setsVi,1 , . . . , Vi,m! (Vi,j ) = bi,j , intention bribing voters Vi,j changepreference lists oj (recall Vi multiset, course multisetpartition Vi,j multisets). 6= j bribery price,= j free nothing really needs done. Note vectorsrealizable; every splitting vote weight (Vi ) achieved. rest proofdevoted developing method evaluating whether given split possibleminimal cost is. ((V )m! )m! ways selecting vectors b1 , . . . , bm!test whether given vector realizable (and compute minimal price realization),simply try sequences vectors test whether makesp winner (the winner, unique case) total cost fall within budget.describe algorithm checks particular vector w = (w1 , . . . , wm! ),wi {0, . . . , (V )} {1, . . . , m!}, realizable computes minimalprice ws realization. Vi (w1 , . . . , wm! ) mean following set m!-elementsequences subsets Vi :Vi (w) = {(Vi,1 , . . . , Vi,m! ) | (Vi = m!j=1 Vi,j ) (1 j m!)[(Vi,j ) = wj ]}.w definePmin{ | ((Vi,1 , . . . , Vi,m! ) Vi (w))[ = j6=i (Vi,j )]}gi (w) =515Vi (w) 6= ,otherwise.fiFaliszewski, Hemaspaandra, & Hemaspaandrais, gi (w) gives lowest price bribing voters Vi according weight vector(w1 , . . . , wm! ). compute gi (w) polynomial time using dynamic programmingtechniques. Let us rename candidates Vi = {v1 , . . . , vt } let gi, (w)gi (w) except restricted voters v , . . . , vt . Thus, gi,1 exactly gi . Naturally,following boundary condition holds gi,t+1 .0w1 = w2 = = wm! = 0,gi,t+1 (w1 , . . . , wm! ) =otherwise.compute values gi, (w1 , . . . , wm! ) using dynamic programming observationgi, (w1 , . . . , wm! ) equal minimum following:gi,+1 (w1 (v ), w2 , . . . , wm! ) + (v ),gi,+1 (w1 , w2 (v ), w3 , . . . , wm! ) + (v ),...gi,+1 (w1 , . . . , wm!1 , wm! (v )) + (v ),gi,+1 (w1 , . . . , wi1 , wi (v ), wi+1 , . . . , wm! ).Note last values handles fact bribe v report preferenceorder oi actually need pay her; v already preference orderoi . Otherwise, need decide m! 1 preference orders ask vreport, need pay change. Clearly, using rule boundarycondition compute gi,1 (w), thus gi (w), time polynomial (V ). Since (V )polynomial size input, completes proof.Corollary 4.16 Let E election system (a) fixed number candidates outcome computable polynomial time (b) outcome dependorder votes. fixed number candidates E-weightedunary -$briberyP.Note that, Theorem 4.6, holds scoring protocol ,-weightedunary -manipulation pm -weightedunary -$bribery, latter P,following corollary.Corollary 4.17 scoring protocol , -weightedunary -manipulation P.Certain scoring protocols natural generalizations arbitrary number candidates, e.g., plurality rule, Borda count, veto rule. resultsimply easiness bribery election systems, need single P algorithmwork cases. example, case Borda count, recently Brelsford et al. (2008)shown even Borda-bribery NP-complete. cases, easiness resultseasily obtained hand. example, Theorem 4.9 immediately implies vetoweighted-bribery NP-complete even 3 candidates, yet following result showsdifficulty bribery veto voting comes purely weighted votes.Theorem 4.18 veto-bribery P.516fiHow Hard Bribery Elections?Proof. proof theorem essentially Theorem 3.1.view veto elections elections every voter vetos one candidate, candidateleast number vetoes wins. (In unique case, candidate winnercandidate vetoes has.)Thus, given instance E = (C, V, p, k), keep bribing voters veto p askveto candidate that, time, least number vetoes.k bribes p winner accept; otherwise reject. simple inductive argumentshows correct strategy, algorithm clearly runs polynomial time.Interestingly, Brelsford et al. (2008) showed veto-weightedunary -manipulationNP-complete. immediately gives, Theorem 4.6, veto-weightedunary -$briberyNP-complete. Using techniques similar used proof Theorem 4.9(but much simpler) modify reduction show even veto-weightedunary bribery NP-complete. hand, main result paper Faliszewski (2008)implies veto-$bribery P. whether Theorem 4.18 follows immediatelyTheorem 4.13, not. Why? Recall 4.13 covers veto fixed numbercandidates, Theorem 4.18 covering protocol handles veto numberscandidateswhat people commonly think think veto voting system.spirit obtained Theorem 4.9 reducing (with much work adjustment)manipulation bribery, scoring protocols. work settings?answer no; designed artificial voting system checking manipulabilityeven one voter NP-complete, checking bribability easy.Theorem 4.19 exists voting system E manipulation NP-complete,bribery P.Proof. Let NP-complete set let B P1. = {x | (y )[hx, yi B]},2. (x, )[hx, yi B |x| = |y|].sets easily constructed NP-complete set padding. ideaproof embed verifier within election rule E. way forcesmanipulation solve arbitrary instances, allowing bribery still easy.First, observe preference lists used encode arbitrary binary strings.use following encoding. C set candidates, let c1 , c2 , . . . , cmcandidates lexicographical order. view preference listci1 > ci2 > ci3 > > cimencoding binary string b1 b2 bm/2 ,bj =01i2j1 > i2j ,otherwise.encoding course efficient one, given binary string maymany preference lists encode it. However, encoding easyproperties need construction.517fiFaliszewski, Hemaspaandra, & Hemaspaandrareduction, binary strings starting 1 encode instances, binary stringsstarting 0 encode witnesses. Given setup, describe election systemE. Let (C, V ) election. c C, c winner electionkV k = 3Rule 1: preference lists encode strings starting 1 preference lists encodestrings starting 0,Rule 2: exactly one preference list encodes string starts 1, say 1x, leastone preference list encodes string 0y hx, yi B.Thus, either candidates winners none winners. Note testingwhether candidate c winner E election easily done polynomial time.following polynomial-time algorithm shows perform optimal bribery.implies E-bribery P.1. c winner, nothing.2. Otherwise, kV k =6 3, bribery impossible.3. Otherwise, exactly one voter whose preference list encodes stringstarts 1, bribe voter encode string starts 0.Rule 1, c winner election.4. Otherwise, exactly one voter whose preference list encodes string starts0 bribe voter preference list encodes stringstarts 1. Rule 1, c winner election.hand, ability solve manipulation problem E implies abilitysolve A. construct reduction E-manipulation. Given string x ,first check whether hx, 0|x| B. so, clearly x output fixedmember E-manipulation. Otherwise, output manipulation problem candidates{1, 2, . . . , 2(|x| + 1)} three voters, v0 , v1 , v2 , v0 preference list encodes1x, v1 preference list encodes 00|x| , v2 manipulative voter. claimcandidate 1 made winner x A.Since hx, 0|x| 6 B, way v2 make 1 winner v2 encodesstring 0y hx, yi B case x A. converse, x A,exists string |x| hx, yi B. encode string 0y preference list{1, 2, . . . , 2(|x| + 1)}, let preference list v2 . ensures 1winner election.Since reduction computed polynomial time, E-manipulationsmembership NP clear, E-manipulation NP-complete.result holds case unique winners. case modify Elexicographically smallest candidate win election reductiondefine distinguished candidate lexicographically smallest candidate.election system natural, show unless restrictelection rules somehow prove P = NP, obtaining general reduction manipulationbribery seems precluded.518fiHow Hard Bribery Elections?5. Succinct Electionsfar discussed nonsuccinct electionsones voterspreference lists (and weights, voters weighted) given listing onetime (as given stack ballots). also natural consider casepreference list frequency conveyed via count (in binary), refersuccinct input. succinct representation particularly relevant casenumber candidates bounded constant. many candidates,natural expect lot voters preferences vary insignificantways. hand, candidates then, naturally,large numbers voters preferences, using succinct representationsave lot space.section provide P membership results (and due proofs, facteven FPT membership results11 ) regarding bribery succinctly represented electionsfixed number candidates. main tool Lenstras (1983) extremelypowerful result integer programming feasibility problem P numbervariables fixed.Theorem 5.1 (Lenstra, 1983) Let k fixed nonnegative integer.polynomial-time algorithm given k integer matrix vector b Zmdetermines whether{x Zk | Ax b} =6holds. is, integer linear programming P fixed number variables.mention Lenstras polynomial-time algorithm attractive practicallyspeaking. particular, although algorithm uses linear number arithmeticoperations linear-sized integersand thus theoretically attractive, low-degreepolynomial run-timethe multiplicative constant large. uscritical issue since mostly interested polynomial-time computability resultsgeneral tools obtaining them, rather actual optimized optimal algorithms.Although Lenstras result applies integer linear programming problemnumber variables fixed achieves P-time case, section fact11. Regarding natural issue P results strengthened FPT results, mentionpassing every P membership result section clearly (although implicitly), via proof, evenFPT membership result. (A problem parameter j FPT, class capturing notionfixed-parameter tractable, exists algorithm whose running time instances size nbounded f (j)nO(1) , f function depending j; see Niedermeier, 2006, detailedcoverage parameterized complexity.) Essentially, Lenstras method well knownuse linear number arithmetic operations linear-sized variables (Lenstra, 1983, see also Downey,2003; Niedermeier, 2002). Although fact voting problems FPT implicitseminal work Bartholdi et al., 1989b (e.g., see Christian, Fellows, Rosamond, & Slinko, 2007; Betzler,Guo, & Niedermeier, 2008b), mention work Christian et al. (2007, Section 4),bribery-like flavor results, explicitly addresses issue FPT, particular mentioningwell known (nonsuccinct) winner score problems Kemeny DodgsonFPT, indebted earlier version paper motivated us mentionsections P results (even succinct elections) FPT results. Among papersaddressing FPT results election problems (although regarding bribery problems), mentionexamples work Betzler, Fellows, Guo, Niedermeier, Rosamond (2008a) Kemeny votingFaliszewski et al. (2008) Llull Copeland voting.519fiFaliszewski, Hemaspaandra, & Hemaspaandratypically need special case result number variablesnumber constraints fixed (and parameter changingconstants within constraints).P membership results regarding succinctly represented elections naturally imply analogous results nonsuccinct representation. express fact succinctnessrepresentation optional cases, put phrase succinct curly bracesnames problems. example, say plurality-{succinct}-bribery P,mean plurality-bribery plurality-succinct-bribery P. (By way,Theorem 3.1, similar careful algorithm one proof also holdssuccinct case.)proceed results, let us introduce notation. Throughoutsection assume bribery problems dealing exactlycandidates, arbitrary fixed constant. Thus, E = (C, V, p, k) briberyproblem may assume C = {1, . . . , m}, o1 , o2 , . . . , om!possible preference orders C. Given set voters V , Vi , 1 m!, meanset voters v V preference order oi . given i, define wh(c, i)index candidate c within preferences oi (informally, c oi ). notationassumed proofs section.Using integer programming approach obtain polynomial-time algorithmsbribery scoring protocols succinct nonsuccinct cases.approach yields similar result manipulation. (The nonsuccinct case manipulationalready obtained work Conitzer et al., 2007.)Theorem 5.2 every scoring protocol = (1 , . . . , ), -{succinct}-bribery-{succinct}-manipulation P.Proof. Let = (1 , . . . , ) scoring protocol let E = (C, V, p, k) briberyproblem want solve, C = {1, . . . , m}. bribery described providingnumbers si,j , 1 i, j m!, saying many people switch preference order oipreference order oj . (The values si,i simply say many voters preference orderoi switch anything else. allow superfluous exchanging, e.g.,legal, even 6= j, si,j sj,i strictly greater zero. However,note proof, example, solution happensanother solution holds that, 6= j, least one si,jsj,i zero.) may express integer program fact si,j describesuccessful bribery, follows. Here, variables, si,j s, requiredintegers. (m!)2 variables. constants (they constantsi.e.,coefficientsfrom integer linear programming perspective, constantscomplexity perspective, effect k, ||V1 ||, . . . , ||Vm! || inputs problem)k, 1 , 2 , . . . , , kV1 k, kV2 k, . . . , kVm! k.1. number bribed voters nonnegative. i, j, 1 i, j m!,si,j 0.2. cannot bribe voters given preference are. i,1 m!, constraint (keeping mind si,i pick520fiHow Hard Bribery Elections?leftover, thus state equality)m!Xsi,j = kVi k.j=13. Altogether, bribe k people.m! Xm!Xsi,ji=1 j=1m!Xs, k.=14. score p least high anybody elses. h, 1 h m,constraint says candidate h defeat p:!!m!m!m!m!XXXXwh(h,j)si,j .wh(p,j)si,jj=1j=1i=1i=1constant number variables, (m!)2 , constant number constraints.(Of course, hand size integer linear programs constantsparticular k, kV1 k, . . . , kVm! kmay increase number voters.) Thus usingLenstras algorithm polynomial time test whether set constraintssatisfied legal si,j s. clear constraints satisfiedbribery k voters leads making p winner. Also, notemake program test whether p become unique winner simply makeinequalities final set constraints strict.case manipulation proved similarly, would variablessi would say many manipulators decide report preference order oi .omit detailed description clear given above.power integer programming approach limited case scoringprotocols. fact, seminal paper Bartholdi et al. (1989b) shows applyingmethod computing Dodgson score nonsuccinct elections fixed numbercandidates yields polynomial-time score algorithm (and though paper Bartholdiet al., 1989b, address issue succinct elections, one seemethod works perfectly; is, implicit Lenstra approach paperBartholdi et al., 1989b, Dodgson score elections fixed number candidatesis, even succinct case, FPT, see also footnote 11).12 similar programused compute scores within Young elections. Let us recall definitionDodgson Young scores.Definition 5.3 Given set candidates C set voters V specified via preferences, Dodgson score candidate c C minimum number adjacent switcheswithin preferences voters make c Condorcet winner.Young score candidate c C minimum number voters needremoved elections make c Condorcet winner.12. mention passing recent work responds theoretical complexity Dodgson scoresdifferent direction, namely, studying success rate simple heuristics problem (McCabe-Dansted et al., 2008; Homan & Hemaspaandra, 2009).521fiFaliszewski, Hemaspaandra, & HemaspaandraApplying integer programming attack case bribery within Dodgson-likeelection systems, i.e., Dodgson system Young system, complicated.systems involve intricate interaction bribing voterschanging preferences. Dodgson elections, bribery, still need worryadjacent switches within voters preference lists make particular candidateCondorcet winner. Young elections, need consider voters removedelections. interaction seems complicated capturedinteger linear program, building flavor Bartholdi et al. (1989b) integerprogramming attack achieve following: Instead making p winner,attempt make p given Dodgson Young score.Formally, DodgsonScore-bribery (and succinctly encoded variant,DodgsonScore-succinct-bribery) mean problem takes input Dodgsonbribery instance (C, V, p, k) (with voters encoded succinctly succinct variant)nonnegative integer t, asks possible ensureby bribing k votersthatps Dodgson score t. define YoungScore-bribery YoungScore-succinctbribery analogously.Theorem 5.4 fixed number candidates, DodgsonScore-{succinct}-briberyP restricted number candidates.Proof. Given nonnegative integer bribery problem instance E = (C, V, p, k)Dodgson elections, C = {1, . . . , m}, give integer program testswhether possible bribe k voters way that, bribery, pDodgson score t. program constant number variablesconstant number constraints. Thus Lenstras algorithm solved polynomialtime.process bribery has, case Dodgson elections, two phases: bribery phasedecide bribe voters, swapping phase (in effect) allowadjacent swaps occur. model first phase integer variables bi,jsecond phase integer variables si,j : i, j, 1 i, j m!, interpretationbi,j si,j follows.bi,j number voters preference order oi bribed report preferenceorder oj .si,j number voters who, bribery, change preference order oioj .values bi,i say many voters preferences oi bribed. values si,isay many voters preferences oi change preferences.variables bi,i si,i make equations neater.Recall Dodgson score number adjacent switches within preference listsneeded make given candidate Condorcet winner. However, variables si,jtalk much complicated operations, namely transfers preference order oipreference order oj . i, j, 1 i, j m!, define constant switches i,j givesminimum number adjacent switches lead preference order oi preference522fiHow Hard Bribery Elections?order oj .13 every preference order oi every two candidates r q definewho(r, q, i) 1 r strictly defeats q preference order oi 1 otherwise.who(r, r, i) = 1, never invoke fact. integer linear programfollowing constraints.1. number bribes switches nonnegative. i, j, 1 i, j m!,bi,j0,si,j0.2. cannot bribe voters are. i, 1 m!, requirem!Xbi,j = kVi k.j=13. Altogether, bribe k people.m!m! XXbi,ji=1 j=1m!Xb, k.=14. number voters switch preference order oi swapping phaseneeds equal number voters who, bribery, preference orderoi . i, 1 m!,m!m!XXsi,k .bj,i =j=1k=15. swapping phase, p Condorcet winner. every q C {p},m!m! XXwho(p, q, j ) si,j > 0.i=1 j=16. swapping phase involves adjacent switches within preference lists.m!m! XXswitches i,j si,j t.i=1 j=113. astute reader note seek meet beat given score p given amountbribery, one would never need Dodgson score calculation invoke exchanges anythingexcept move p ahead number slots. true, thus rather (m!)2 variables si,j ,one could define integer linear program replaced si,j (m!)(m1) variables captureshifting. define things current general way since current approach removesdependence get away shifts sort argument type made(which would work fine might hold sharply different settings), current approachalso leads quite simple, uncluttered constraint equations.523fiFaliszewski, Hemaspaandra, & HemaspaandraClearly, program contains constant number variables constant numberconstraints. Thus light Lenstras algorithm theorem proven.Theorem 5.5 fixed number candidates, YoungScore-{succinct}-bribery Prestricted number candidates.Proof. proof similar proof Theorem 5.4. Let nonnegativeinteger let E = (C, V, p, k) bribery instance C = {1, . . . , m}. wantprovide algorithm tests whether possible ensure p Young scorebribing k voters. providing integer linear program.workings integer linear program divided two phases: briberyphase removal phase. bribery described variables bi,j , 1 i, j m!,say many voters preferences oi bribed preferences oj .removal described variables ri , 1 m!, say many voterspreferences oi bribery removed. enforce above, usefollowing constraints:1. number bribes removals nonnegative. i, j, 1 i, j m!,0,bi,jri 0.2. cannot bribe voters are. i, 1 m!,m!Xbi,j = kVi k.j=13. Altogether, bribe k people.m!m! XXbi,ji=1 j=1m!Xb, k.=14. number voters preference order oi removed electionremoval phase bounded number votersbribery preference order oi . i, 1 m!,rim!Xbj,i .j=15. removal phase, p Condorcet winner. every q C {p},!!m!m!XXbi,j rj who(p, q, j ) > 0.j=1i=1524fiHow Hard Bribery Elections?6. removal phase removes voters.m!Xri t.i=1Clearly, constant number variables constraints, integer linearprogram solved using Lenstras algorithm polynomial time.two theorems say test polynomial time whether given bribesuffices obtain beat given Dodgson Young score. Thus using binary searchfact find optimal bribe obtaining particular score.issue actually making candidate p winner (a unique winner, studyingunique winner case) Dodgson elections is, already indicated, much difficultdirect attack using integer linear programming seems fail. Nonetheless, combininginteger programming method brute-force algorithm resolves issuenonsuccinct case.Theorem 5.6 fixed number candidates, Dodgson-bribery, Dodgson-$bribery,Young-bribery, Young-$bribery P.Proof. Theorem 4.13, polynomially many briberies need check.test whether favorite candidate becomes winner, using Bartholdiet al.s (1989b) integer linear program Dodgson score-testing similar one Youngscore-testing.discussions bribery respect Dodgson elections lead observationsmall change voting system allow us resolve natural bribery-relatedwinner problem. Note bribes allow us completely change given voters preferencelistand goes far beyond switches allowed Dodgson score-counting. interesting observe one define Dodgson-like voting system based bribes: Insteadcounting many switches needed make given candidate Condorcet winner,count many bribes (where bribe complete overwrite, unit cost, onevoters preference list) suffice guarantee outcome. call election systemDodgson . comments, fixed number candidates computing winnersDodgson elections done polynomial time.Theorem 5.7 fixed number candidates, winner problem succinctDodgson elections P.Proof. follows immediately Theorem 5.4. candidate c simply needbinary search smallest bribe makes Condorcet winner (i.e.,gives c Dodgson score zero). winners candidates least numberbribes needed.Clearly, like Dodgson, Dodgson elects Condorcet winner whenever one exists.Theorem 5.7 shows fixed number candidates winner problem succinctDodgson elections P, Definition 5.3 noted holdssuccinct Dodgson elections. attractive, dependone feels natural model counting distancecounting adjacent switchcounting voter change. settings latter seems525fiFaliszewski, Hemaspaandra, & Hemaspaandraattractive, course Dodgson seems preferable Dodgson. Nonetheless, jumpingDodgson bandwagon, one probably first carefully study propertiesnew election system. Note even though computing Dodgson winners fixednumber candidates polynomial-time procedure, immediately implybribery problem easy Dodgson , conjecture not.light discussion, might seem Dodgson-like election rules gettingpolynomial-time bribery results (in succinct model) difficult using integer linearprogramming. However, always case. particular, following theoremstates bribery Kemeny system easy fix number candidates. Recallcandidate c winner Kemeny elections exists preference order ohlists c top agrees strongly votes. (See Section 2.1.)Theorem 5.8 fixed number candidates, Kemeny-{succinct}-bribery Prestricted number candidates.Proof. proof employs integer linear programming, time needone program. informally put, integer linear programs seeminglyexpress conjunctions, disjunctions, case Kemeny electionsneed express fact least one preference orders lists favoritecandidate top disagrees least number voters preferences.14Let E = (C, V, p, k) bribery instance Kemeny elections, C = {1, . . . , m}.preference order oh , 1 h m!, p top candidate oh , constructseparate integer linear program feasible solution briberyk candidates oh ordering maximizes (comparedorders) number agreements voters reported preferences. agree i,jmean number agreements preference orders oi oj (see Section 2.1).Let us consider arbitrary h p top candidate preference order oh .describe bribery using variables bi,j , 1 i, j m!, saying many voterspreference order oi bribed preference order oj . employ followingconstraints.1. number bribes nonnegative. i, j, 1 i, j m!,bi,j 0.2. cannot bribe voters are. i, 1 m!,m!Xbi,j = kVi k.j=13. Altogether, bribe k people.m!m! XXbi,ji=1 j=1m!Xb, k.=114. natural way expressing disjunction within single integer program use boolean variablesindicating preference order concentrating on. However, leads integer quadraticprogram.526fiHow Hard Bribery Elections?bribery problemE-briberyE-$briberyE-weightedunary -$briberyE-weighted-briberyE-weighted-$briberyunaryE-weighted-$briberypluralityPPPPPNP-completeelection system EapprovalvetoNP-completePNP-completeP (Faliszewski, 2008)NP-complete NP-complete (Brelsford et al., 2008)NP-completeNP-completeNP-completeNP-completeNP-completeNP-completeTable 1: complexity bribery plurality, approval, veto (in settingnumber candidates bounded). results attributed workBrelsford et al. (2008) Faliszewski (2008) follow via simple argumentsresults papers.4. preference order disagrees voters preferences least many timesoh . , 1 m!,m!m!m!m!XXXXagree i,hbj,iagree i,bj,i .i=1j=1i=1j=1Clearly, integer program constant number constraints constantnumber variables. Thus solved separately, using Lenstras algorithm,polynomial time. since constant numberm!of integer linearprograms regarding given input, m! applications Lenstras algorithm solvethem. one feasible solution bribery possible otherwisenot.interesting consider features Kemeny elections allow us employattack, given approach seem work either DodgsonYoung elections. One reasons universal quantification implicit DodgsonYoung elections exponentially large search space, quantificationKemeny is, case fixed candidate set, fixed number options.6. Conclusionspaper provides study bribery respect plurality rule provides toolsresults regarding many election systems, scoring protocols, approval voting,Condorcet-winner based elections. Bribery seems important issue manipulation control; paper addresses gap knowledge complexityvoting systems. Tables 1 2 collect main results paper regardingcomplexity bribery scoring protocols related election systems. (However,course, paper contains many results cannot easily presented formtable, e.g., Theorem 4.19 results Section 5.)One important contributions paper pointing out, concrete examples,NP-completeness results may guarantee difficulty natural probleminstances. particular, Theorem 3.2 says plurality-weighted-$bribery NP-complete,527fiFaliszewski, Hemaspaandra, & Hemaspaandrabribery problem-bribery-$bribery-weightedunary -$bribery-weighted-bribery-weighted-$briberyunary-weighted-$briberyScoring protocol = (1 , . . . , ).1 > 2true1 = = 2 = = 2 = =PPPPPPPPPPPNP-completePPNP-completePNP-completeNP-completeTable 2: complexity bribery within scoring protocols.Theorem 3.8 observes either weights prices small enough,problem solved efficiently.Another contribution paper relate manipulation bribery, thus making result transfer former latter reasonable line attackand one alreadyexploited spirit proof approach central dichotomy result (Theorem 4.9).suggested future work, believe studying approximation algorithmscontrol (by voter/candidate addition/deletion) bribery problems currently knownNP-complete would attractive next step point reader recent papersregarding approximation manipulation, bribery, control (Brelsford, 2007; Brelsfordet al., 2008; Faliszewski, 2008; Zuckerman et al., 2008). would also interesting studycomplexity bribery settings, incomplete information, multiplecompeting bribers, complicated bribe structures (see work Faliszewski, 2008,preliminary results bribery involved pricing schemes).Acknowledgmentsgrateful Samir Khuller helpful conversations Bartholdi et al.(1989b) integer programming attack fixed-candidate Dodgson elections. alsograteful anonymous referees Preetjot Singh helpful comments. worksupported part grants NSF-CCR-0311021, NSF-CCF-0426761, NSF-IIS-0713061,AGH-UST 11.11.120.777, Friedrich Wilhelm Bessel Research Awards Edith Hemaspaandra Lane A. Hemaspaandra, Alexander von Humboldt FoundationsTransCoop program. work done part Piotr Faliszewski University Rochester. early version paper, titled Complexity BriberyElections, appeared proceedings AAAI-06 (Faliszewski, Hemaspaandra, & Hemaspaandra, 2006) also presented COMSOC-06 NESCAI-07.ReferencesBartholdi, III, J., Tovey, C., & Trick, M. (1989a). computational difficulty manipulating election. Social Choice Welfare, 6 (3), 227241.Bartholdi, III, J., Tovey, C., & Trick, M. (1989b). Voting schemesdifficult tell election. Social Choice Welfare, 6 (2), 157165.528fiHow Hard Bribery Elections?Bartholdi, III, J., Tovey, C., & Trick, M. (1992). hard control election?Mathematical Computer Modeling, 16 (8/9), 2740.Betzler, N., Fellows, M., Guo, J., Niedermeier, R., & Rosamond, F. (2008a). Fixedparameter algorithms Kemeny scores. Proceedings 4th International Conference Algorithmic Aspects Information Management, pp. 6071. SpringerVerlag Lecture Notes Computer Science #5034.Betzler, N., Guo, J., & Niedermeier, R. (2008b). Parameterized computational complexityDodgson Young elections. Proceedings 11th Scandinavian WorkshopAlgorithm Theory, pp. 402413. Springer-Verlag Lecture Notes Computer Science#5124.Black, D. (1958). Theory Committees Elections. Cambridge University Press.Brelsford, E. (2007). Approximation elections. Masters thesis, Rochester InstituteTechnology, Rochester, NY.Brelsford, E., Faliszewski, P., Hemaspaandra, E., Schnoor, H., & Schnoor, I. (2008). Approximability manipulating elections. Proceedings 23rd AAAI ConferenceArtificial Intelligence, pp. 4449. AAAI Press.Christian, R., Fellows, M., Rosamond, F., & Slinko, A. (2007). complexity lobbyingmultiple referenda. Review Economic Design, 11 (3), 217224.Condorcet, J. (1785). Essai sur lApplication de LAnalyse la Probabilite des DecisionsRendues la Pluralite des Voix. Facsimile reprint original published Paris, 1972,Imprimerie Royale.Conitzer, V., & Sandholm, T. (2006). Nonexistence voting rules usually hardmanipulate. Proceedings 21st National Conference Artificial Intelligence,pp. 627634. AAAI Press.Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidateshard manipulate? Journal ACM, 54 (3), Article 14.Dobzinski, S., & Procaccia, A. (2008). Frequent manipulability elections: case twovoters. Proceedings 4th International Workshop Internet NetworkEconomics, pp. 653664. Springer-Verlag Lecture Notes Computer Science #5385.Dodgson, C. (1876). method taking votes two issues. Pamphlet printedClarendon Press, Oxford, headed yet published (see discussions(McLean & Urken, 1995; Black, 1958), reprint paper).Downey, R. (2003). Parameterized complexity skeptic. Proceedings 18th Annual IEEE Conference Computational Complexity, pp. 147168. IEEE ComputerSociety Press.Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness sharedbeliefs: GibbardSatterthwaite generalized. Social Choice Welfare, 17 (1), 8593.Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methodsweb. Proceedings 10th International World Wide Web Conference, pp.613622. ACM Press.529fiFaliszewski, Hemaspaandra, & HemaspaandraElkind, E., & Lipmaa, H. (2005a). Hybrid voting protocols hardness manipulation.Proceedings 16th Annual International Symposium Algorithms Computation, pp. 206215. Springer-Verlag Lecture Notes Computer Science #3872.Elkind, E., & Lipmaa, H. (2005b). Small coalitions cannot manipulate voting. Proceedings9th International Conference Financial Cryptography Data Security, pp.285297. Springer-Verlag Lecture Notes Computer Science #3570.Ephrati, E., & Rosenschein, J. (1997). heuristic technique multi-agent planning.Annals Mathematics Artificial Intelligence, 20 (14), 1367.Erdelyi, G., Hemaspaandra, L., Rothe, J., & Spakowski, H. Generalized juntas NP-hardsets. Theoretical Computer Science. appear.Erdelyi, G., Hemaspaandra, L., Rothe, J., & Spakowski, H. (2007). approximating optimal weighted lobbying, frequency correctness versus average-case polynomialtime. Proceedings 16th International Symposium Fundamentals Computation Theory, pp. 300311. Springer-Verlag Lecture Notes Computer Science#4639.Erdelyi, G., Hemaspaandra, L., Rothe, J., & Spakowski, H. (2009). Frequency correctnessversus average polynomial time. Information Processing Letters, 109 (16), 946949.Erdelyi, G., Nowak, M., & Rothe, J. (2008a). Sincere-strategy preference-based approvalvoting broadly resists control. Proceedings 33rd International SymposiumMathematical Foundations Computer Science, pp. 311322. Springer-Verlag LectureNotes Computer Science #5162.Erdelyi, G., Nowak, M., & Rothe, J. (2008b). Sincere-strategy preference-based approvalvoting fully resists constructive control broadly resists destructive control. Tech.rep. arXiv:0806.0535 [cs.GT], arXiv.org. precursor appears (Erdelyi et al., 2008a).Faliszewski, P. (2008). Nonuniform bribery (short paper). Proceedings 7th International Conference Autonomous Agents Multiagent Systems, pp. 15691572.International Foundation Autonomous Agents Multiagent Systems.Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2006). complexity briberyelections. Proceedings 21st National Conference Artificial Intelligence,pp. 641646. AAAI Press.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). LlullCopeland voting broadly resist bribery control. Proceedings 22nd AAAIConference Artificial Intelligence, pp. 724730. AAAI Press. Journal version available (Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009a).Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2008). Copeland votingfully resists constructive control. Proceedings 4th International ConferenceAlgorithmic Aspects Information Management, pp. 165176. Springer-VerlagLecture Notes Computer Science #5034.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009a). LlullCopeland voting broadly resist bribery control. Journal Artificial IntelligenceResearch, 35, 275341.530fiHow Hard Bribery Elections?Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009b). richer understanding complexity election systems. Ravi, S., & Shukla, S. (Eds.), Fundamental Problems Computing: Essays Honor Professor Daniel J. Rosenkrantz,pp. 375406. Springer.Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: Ties matter.Proceedings 7th International Conference Autonomous Agents Multiagent Systems, pp. 983990. International Foundation Autonomous AgentsMultiagent Systems.Friedgut, E., Kalai, G., & Nisan, N. (2008). Elections manipulated often. Proceedings 49rd IEEE Symposium Foundations Computer Science, pp. 243249.IEEE Computer Society.Garey, M., & Johnson, D. (1979). Computers Intractability: Guide TheoryNP-Completeness. W. H. Freeman Company.Gibbard, A. (1973). Manipulation voting schemes. Econometrica, 41 (4), 587601.Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy voting systems. JournalComputer System Sciences, 73 (1), 7383.Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (1997). Exact analysis Dodgsonelections: Lewis Carrolls 1876 voting system complete parallel access NP.Journal ACM, 44 (6), 806825.Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Anyone him: complexityprecluding alternative. Artificial Intelligence, 171 (5-6), 255285.Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). Hybrid elections broadencomplexity-theoretic resistance control. Mathematical Logic Quarterly, 55 (4), 397424.Hemaspaandra, E., Spakowski, H., & Vogel, J. (2005). complexity Kemeny elections.Theoretical Computer Science, 349 (3), 382391.Homan, C., & Hemaspaandra, L. (2009). Guarantees success frequency algorithm finding Dodgson-election winners. Journal Heuristics, 15 (4), 403423.Kemeny, J. (1959). Mathematics without numbers. Daedalus, 88, 577591.Kemeny, J., & Snell, L. (1960). Mathematical Models Social Sciences. Ginn.Ladner, R., Lynch, N., & Selman, A. (1975). comparison polynomial time reducibilities.Theoretical Computer Science, 1 (2), 103124.Lenstra, Jr., H. (1983). Integer programming fixed number variables. MathematicsOperations Research, 8 (4), 538548.Martello, S., & Toth, P. (1990). Knapsack Problems: Algorithms Computer Implementations. John Wiley Sons.McCabe-Dansted, J., Pritchard, G., & Slinko, A. (2008). Approximability Dodgsonsrule. Social Choice Welfare, 31 (2), 311330.McLean, I., & Urken, A. (1995). Classics Social Choice. University Michigan Press.531fiFaliszewski, Hemaspaandra, & HemaspaandraMeir, R., Procaccia, A., Rosenschein, J., & Zohar, A. (2008). complexity strategicbehavior multi-winner elections. Journal Artificial Intelligence Research, 33,149178.Niedermeier, R. (2002). Invitation fixed-parameter algorithms. Habilitation thesis, University Tubingen.Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford University Press.Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.Procaccia, A., & Rosenschein, J. (2007). Junta distributions average-case complexitymanipulating elections. Journal Artificial Intelligence Research, 28, 157181.Rothe, J. (2005). Complexity Theory Cryptology: Introduction Cryptocomplexity.Springer-Verlag.Rothe, J., Spakowski, H., & Vogel, J. (2003). Exact complexity winner problemYoung elections. Theory Computing Systems, 36 (4), 375386.Saari, D., & Merlin, V. (2000). geometric examination Kemenys rule. Social ChoiceWelfare, 17 (3), 403438.Satterthwaite, M. (1975). Strategy-proofness Arrows conditions: Existence correspondence theorems voting procedures social welfare functions. JournalEconomic Theory, 10 (2), 187217.Xia, L., & Conitzer, V. (2008). sufficient condition voting rules frequentlymanipulable. Proceedings 9th ACM Conference Electronic Commerce,pp. 99108. ACM Press.Young, H. (1977). Extending Condorcets rule. Journal Economic Theory, 16 (2), 335353.Zuckerman, M., Procaccia, A., & Rosenschein, J. (2008). Algorithms coalitionalmanipulation problem. Artificial Intelligence, 173 (2), 392412.532fiJournal Artificial Intelligence Research 35 (2009) 623-675Submitted 10/08; published 08/09Compiling Uncertainty Away Conformant PlanningProblems Bounded WidthHector Palacioshlp@ldc.usb.veUniversitat Pompeu FabraRoc Boronat, 13808018 Barcelona, SPAINHector Geffnerhector.geffner@upf.eduICREA & Universitat Pompeu FabraRoc Boronat, 13808018 Barcelona, SPAINAbstractConformant planning problem finding sequence actions achieving goalpresence uncertainty initial state action effects. problemapproached path-finding problem belief space good belief representationsheuristics critical scaling up. work, different formulation introducedconformant problems deterministic actions automatically convertedclassical ones solved off-the-shelf classical planner. translation mapsliterals L sets assumptions initial situation, new literals KL/trepresent L must true initially true. lay general translation schemesound establish conditions translation also complete.show complexity complete translation exponential parameterproblem called conformant width, benchmarks bounded. plannerbased translation exhibits good performance comparison existing planners,basis T0 , best performing planner Conformant Track 2006International Planning Competition.1. IntroductionConformant planning form planning goal achieved initialsituation fully known actions may non-deterministic effects (Goldman &Boddy, 1996; Smith & Weld, 1998). Conformant planning computationally harderclassical planning, even polynomial restrictions plan length, plan verificationremains hard (Haslum & Jonsson, 1999; Baral, Kreinovich, & Trejo, 2000; Turner, 2002;Rintanen, 2004). practical problems purely conformant, ability findconformant plans needed contingent planning conformant situations specialcase relaxations conformant planning yield useful heuristics (Hoffmann &Brafman, 2005).problem conformant planning formulated path-finding problembelief space sequence actions map given initial belief state targetbelief sought (Bonet & Geffner, 2000). belief state represents set statesdeemed possible, actions, whether deterministic not, map one belief statec2009AI Access Foundation. rights reserved.fiPalacios & Geffneranother. formulation, underlies current conformant planners (Hoffmann &Brafman, 2006; Bryce, Kambhampati, & Smith, 2006; Cimatti, Roveri, & Bertoli, 2004)must address two problems: problem representing beliefs compact way,problem obtaining effective heuristics beliefs. first problem approachedlogical representations make use SAT OBDD technology,intractable worst case, scale better plain state representations. secondproblem, hand, complex, heuristics searching beliefspace successful far heuristics developed classical planning (Bonet& Geffner, 2001; Hoffmann & Nebel, 2001).work, introduce different approach conformant planning problemsautomatically compiled classical problems solved classical planner.translation maps sets literals initial situation literals L new literalsKL/t express true initial situation, L must true. lay firstgeneral translation scheme sound establish conditionstranslation also complete. Also, show complexity complete translationexponential parameter problem call conformant width,benchmark domains bounded, implying complete translationcases polynomial. planner based translation exhibits good performancecomparison existing conformant planners basis T0 , best performingplanner Conformant Track 2006 International Planning Competition.translation-based approach provides solution two problems faced conformant planners search belief space: belief representation heuristicbeliefs. translation-based approach, beliefs represented literals KL/tstand conditionals, representation polynomial complete conformant problems bounded width. addition, since belief states representedplain states, heuristic beliefs classical heuristic. computational pointview, though, explicit search belief-space: conformant problems Pconverted classical problems K(P ) knowledge-level (Petrick & Bacchus, 2002),whose solutions, computed classical planner, encode conformant solutions P .formulation limited conformant problems deterministicuncertainty lies initial situation. address nonetheless issues musthandled order generalize translation-based approach non-deterministic domainsreport empirical results non-deterministic domains well.paper organized follows. define first syntax semantics conformantplanning problems P (Section 2), consider simple sound incomplete translationK0 (Section 3). consider general translation scheme KT,Mtwo parameters, set tags encoding assumptions initial situation,set merges encoding valid disjunctions tags (Section 4), analyze severalinstances scheme follow particular choices sets tags merges:complete exponential translation KS0 tags associated possibleinitial states problem (Section 5), polynomial translation Ki fixed integer0 complete problems conformant width bounded (Section 6).provide alternative explanation compact complete translationshowing problems bounded width, exponential number possible initialstates S0 includes always polynomial number critical initial states S00 plans624fiCompiling Uncertainty Away Conformant Planning Problemsconform S00 conform also S0 (Section 7). finally present conformantplanner T0 (Section 8), empirical evaluation planner (Section 9), extensionnon-deterministic actions (Section 10), discussion related work (Section 11).followed brief summary (Section 12) formal proofs (Appendix).work revision extension formulation presented PalaciosGeffner (2007), turn based ideas first sketched also Palacios Geffner(2006).2. Conformant Problem Pdefine next syntax semantics conformant planning problems considered.2.1 SyntaxConformant planning problems P represented tuples form P = hF, I, O, GiF stands fluent symbols problem, set clauses F defininginitial situation, stands set (ground) operators actions a, G setliterals F defining goal. Every action precondition P re(a) given setfluent literals, set conditional effects C L C set fluent literalsL fluent literal.actions assumed deterministic hence uncertainty lies initialsituation. Thus, language conformant problem P excluding uncertaintyinitial situation, Strips extended conditional effects negation. Moreover,uncertainty initial situation, fluents appear unit clausesI, P equivalent classical planning problem.refer conditional effects C L action rules associateda, sometimes write : C L. convenient, also join several effectsassociated action condition : C L L0 write C Ltrue L C empty. Finally, literal L, L denotes complement L.2.2 Semanticsstate truth assignment fluents F P = hF, I, O, Gi possible initialstate P state satisfies clauses I.state s, write I(s) refer set atoms (positive literals) trues, write P/s refer classical planning problem P/s = hF, I(s), O, Gilike conformant problem P except initial state fixed s.action sequence = {a0 , a1 , . . . , } classical plan P/s action sequenceexecutable state results goal state sG ; i.e., = 0, . . . , n,preconditions action ai true si , si+1 state results actionai state si , goal literals true sn+1 .Finally, action sequence conformant plan P iff classical plan P/severy possible initial state P .Conformant planning computationally harder classical planning, plan verification remains hard even polynomial restrictions plan length (Haslum & Jonsson,1999; Baral et al., 2000; Turner, 2002; Rintanen, 2004). common approach625fiPalacios & Geffnerconformant planning based belief state formulation (Bonet & Geffner, 2000).belief state b non-empty set states deemed possible given situation,every action executable b, maps b new belief state ba . conformantplanning task becomes path-finding problem graph nodes belief statesb, source node b0 belief state corresponding initial situation, targetbelief states bG goals true.assume throughout logically consistent, set possible initialstates empty, P consistent, bodies C C 0 conflictingeffects : C L : C 0 L associated action mutually exclusivemutex. details this; see Part B Appendix.3. Basic Translation K0simple translation conformant problem P classical problem K(P )obtained replacing literals L literals KL KL aimed capturing whetherL known true known false respectively.Definition 1 (Translation K0 ). conformant planning problem P = hF, I, O, Gi,translation K0 (P ) = hF 0 , 0 , O0 , G0 classical planning problemF 0 = {KL, KL | L F }0 = {KL | L unit clause I}G0 = {KL | L G}O0 = precondition L replaced KL, conditionaleffect : C L replaced : KC KL : KC KL,expressions KC KC C = L1 , L2 . . . abbreviations formulasKL1 , KL2 . . . KL1 , KL2 . . . respectively.intuition behind translation simple: first, literal KL true initialstate 0 L known true I; otherwise false. removes uncertaintyK0 (P ), making classical planning problem. addition, soundness,rule : C L P mapped two rules: support rule : KC KL, ensuresL known true condition known true, cancellationrule : KC KL guarantees KL deleted (prevented persist)action applied C known false. use support cancellation rulesencoding original rules knowledge-level subtlety translation.translation K0 (P ) sound every classical plan solves K0 (P ) conformantplan P , incomplete, conformant plans P classical plans K(P ).meaning KL literals follows similar pattern: plan achieves KL K0 (P ),plan achieves L certainty P , yet plan may achieve L certaintyP without making literal KL true K0 (P ).1Proposition 2 (Soundness K0 (P )). classical plan K0 (P ),conformant plan P .1. Formal proofs found appendix.626fiCompiling Uncertainty Away Conformant Planning Problemsillustration, consider conformant problem P = hF, I, O, Gi F = {p, q, r},= {q}, G = {p, r}, actions = {a, b} effects: q r , : p p , b : q p .problem, action sequence = {a, b} conformant plan P actionsequence 0 = {a} not. Indeed, classical plan P/s possible initial states, 0 classical plan possible initial state s0 p true (recallpossible initial state P satisfies neither p r assumedinitially false problem).Definition 1, translation K0 (P ) = hF 0 , 0 , O0 , G0 classical planning problemfluents F 0 = {Kp, Kp, Kq, Kq, Kr, Kr}, initial situation 0 = {Kq}, goals G0 ={Kp, Kr}, actions O0 = {a, b} effects: Kq Kr , : Kp Kp , b : Kq Kp,encode supports, effects: Kq Kr , : Kp Kp , b : Kq Kp,encode cancellations.Proposition 2 implies, example, 0 = {a}, conformant planP , cannot classical plan K(P ) either. easy verify, support: Kq Kr achieves goal Kr Kq true 0 , cancellation : Kp Kpassociated action, preserves Kp false goal p.translation K0 complete, meaning fails capture conformant plans P classical plans, completeness assessed terms weakersemantics. so-called 0-approximation semantics (Baral & Son, 1997), belief states brepresented 3-valued states fluents true, false, unknown. incomplete belief representation, checking whether action applicable belief state b,computing next belief state ba , verifying polynomial length plans polynomialtime operations. particular, literal L true next belief state ba iff a) actioneffect C L literals C true b, b) L true beffects C 0 L action a, complement literal L0 C 0 true b. actionsequence conformant plan P according 0-approximation semanticsbelief sequence generated according 0-approximation semantics makesaction sequence applicable terminates belief state goals true.possible prove that:Proposition 3 (K0 (P ) 0-Approximation). action sequence classical planK0 (P ) iff conformant plan P according 0-approximation semantics.correspondence surprising though 0-approximation semanticsK0 (P ) translation throw away disjunctive information restrict plansmake use uncertain knowledge. Indeed, states s0 , s1 , . . . generatedaction sequence = {a0 , a1 , . . .} classical problem K0 (P ) encode precisely627fiPalacios & Geffnerliterals known true according 0-approximation; namely, L truetime according 0-approximation iff literal KL true state si .Proposition 3 mean basic translation K0 0-approximationsemantics equivalent rather rely equivalent belief representations.translation K0 delivers also way get valid conformant plans using classicalplanner. translation-based approach thus addresses representationalheuristic issues arise conformant planning.illustration Proposition 3, given conformant problem P = {p, r}actions b effects : p q, : r v, b : q v, plan = {a, b} validachieving goal G = {q, v} according K0 (P ) 0-approximation,plan = {b} valid according either. time, initial situationchanged = {p q}, neither approach sanctions plan = {a} G = {q}, evenvalid conformant plan. this, ability reason disjunctions needed.extension basic translation K0 allows limited form disjunctive reasoning presented Palacios Geffner (2006). extension based introductionnew literals L/Xi used encoding conditionals Xi L. Below, basic translationK0 extended different manner ensures tractability completenesslarge class problems.4. General Translation Scheme KT,Mbasic translation K0 extended general translation scheme KT,Mtwo parameters: set tags set merges m. showsuitable choices two parameters, translation KT,M , unlike translation K0 ,sound complete.tag set (conjunction) literals L P whose truth value initialsituation known. tags used introduce new class literals KL/tclassical problem KT,M (P ) represent conditional true initially, Ltrue, assertion could written K(t0 L) temporal modal logic. usenotation KL/t rather L/t used Palacios Geffner (2006),distinction KL/t KL/t: roughly KL/t means conditionalK(t0 L) true, KL/t means conditional K(t0 L) true.Likewise, merge non-emptyW collection tags stands Disjunctive Normal Form (DNF) formula tm t. merge valid one tagsmust true I; i.e.,_|=.tmmerge literal L P translate merge action single effect^KL/t KLtmcaptures simple form reasoning cases.valid merge used reasoning literal L P , computationallyconvenient (although logically necessary) specify certain mergesused literals L others. Thus, formally, collection pairs628fiCompiling Uncertainty Away Conformant Planning Problems(m, L), merge L literal P . pair means mergeL. group merges literal L set ML , thus, understoodcollection sets ML L P . simplicity, however, exceptmay cause confusion, keep referring plain set merges.assume collection tags always includes tag standsempty collection literals, call empty tag denote . emptytag, denote KL/t simply KL.translation KT,M (P ) basic translation K0 (P ) conditioned tagsextended actions capture merges :Definition 4 (Translation KT,M ). Let P = hF, I, O, Gi conformant problem,KT,M (P ) classical planning problem KT,M (P ) = hF 0 , 0 , O0 , G0F 0 = {KL/t, KL/t | L F }0 = {KL/t | I, |= L}G0 = {KL | L G}O0 = {a V: KC/t KL/t, : KC/t KL/t | : C L P }{am,L : [ tm KL/t] KL XL | L P, ML }KL precondition action KT,M (P ) L precondition P , KC/tKC/t stand KL1 /t, KL2 /t,V . . . , KL1 /t, KL2 /t, . . . respectively,C = L1 , L2 , . . ., XL stands L0 KL0 L0 ranging literals L0 mutexL P .translation KT,M (P ) reduces basic translationK0 (P ) emptyVcontains empty tag. extra effects XL = L0 KL0 merge actions am,Lneeded ensure translation KT,M (P ) consistent P consistent,otherwise ignored. Indeed, L L0 mutex consistent P , invariantKL/t KL0 /t holds KT,M (P ) non-empty tags t, hence successful mergeL always followed successful merge L0 . rest paperthus assume P KT,M (P ) consistent, ignore extra merge effects,come back Appendix B proving consistency KT,M (P )consistency P .suitable choices , translation KT,M (P ) sound complete.establishing results, however, let us make notions precise.Definition 5 (Soundness). translation KT,M (P ) sound classical plansolves classical planning problem KT,M (P ), plan 0 results droppingmerge actions conformant plan P .Definition 6 (Completeness). translation KT,M (P ) complete conformantplan 0 solves conformant problem P , classical plan solvesclassical problem KT,M (P ) 0 equal merge actions removed.general translation scheme KT,M sound provided merges validtags consistent (literals tag true possible initial state):629fiPalacios & GeffnerTheorem 7 (Soundness KT,M (P )). translation KT,M (P ) sound providedmerges valid tags consistent.Unless stated otherwise, assume merges valid tags consistent,call translations, valid translations.convention keeping notation simple, singleton tags like = {p}, curlybrackets often dropped. Thus, literals KL/t = {p} written KL/p,merges = {t1 , t2 } singleton tags t1 = {p} t2 = {q}, written = {p, q}.Example. illustration, consider problem moving object origindestination using two actions: pick(l), picks object location handempty object location, drop(l), drops object locationobject held. making problem interesting, let us also assumeaction pick(l) drops object held l hand empty.conditional effects action preconditions. Assuming singleobject, effects written as:pick(l) : hold, at(l) hold at(l)pick(l) : hold hold at(l)drop(l) : hold hold at(l) .Consider instance P domain, hand initially emptyobject, initially either l1 l2 , must moved l3 ; i.e., P = hF, I, O, Gi= {hold , at(l1 ) at(l2 ) , at(l1 ) at(l2 ) , at(l3 )}G = {at(l3 )} .action sequence1 = {pick(l1 ), drop(l3 ), pick(l2 ), drop(l3 )}conformant plan problem, attempt pick object locationl1 followed drop target location l3 , ensuring object ends l3originally l1 . followed attempt pick object l2drop l3 .hand, action sequence 2 results 1 removing firstdrop action2 = {pick(l1 ), pick(l2 ), drop(l3 )}conformant plan, since object originally l1 , would end l2action pick(l2 ). notation introduced above, 1 classical plan classicalproblem P/s two possible initial states s, 2 classical plan problemP/s state object initially l2 .630fiCompiling Uncertainty Away Conformant Planning ProblemsConsider classical problem KT,M (P ) = hF 0 , 0 , O0 , G0 obtained P= {at(l1 ), at(l2 )}2 contains merge = {at(l1 ), at(l2 )} literalshold at(l3 ). definition, fluents F 0 KT,M (P ) form KL/tKL/t L {at(l), hold}, l {l1 , l2 }, , initial situation 00 = {Khold, Khold/at(l), Kat(l3 ), Kat(l3 )/at(l), Kat(l)/at(l), Kat(l0 )/at(l)}l, l0 {l1 , l2 } l0 6= l, goal G0G0 = {Kat(l3 )} .effects associated actions pick(l) drop(l) O0 support rulespick(l) : Khold, Kat(l) Khold Kat(l)pick(l) : Khold Khold Kat(l)drop(l) : Khold Khold Kat(l)one three locations l = li , condition rule empty tag,along support rules:pick(l) : Khold/at(l0 ), Kat(l)/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )pick(l) : Khold/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )drop(l) : Khold/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )condition rule tags at(l0 ) , l0 {l1 , l2 }. correspondingcancellation rules are:pick(l) : Khold, Kat(l) Khold Kat(l)pick(l) : Khold Khold Kat(l)drop(l) : Khold Khold Kat(l)pick(l) : Khold/at(l0 ), Kat(l)/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )pick(l) : Khold/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 )drop(l) : Khold/at(l0 ) Khold/at(l0 ) Kat(l)/at(l0 ) .addition, actions O0 include merge actions am,hold am,at(l3 ) followmerge = {at(l1 ), at(l2 )} literals hold at(l3 ):am,hold : Khold/at(l1 ), Khold/at(l2 ) Kholdam,at(l3 ) : Kat(l3 )/at(l1 ), Kat(l3 )/at(l2 ) Kat(l3 ) .2. empty tag assumed every thus mentioned explicitly.631fiPalacios & Geffnershown plan10 = {pick(l1 ), drop(l3 ), pick(l2 ), drop(l3 ), am,at(l3 ) }solves classical problem KT,M (P ) hence, Theorem 7, plan 1 obtained10 dropping merge action, valid conformant plan P (shown above).see literals KT,M (P ) evolve actions 10 executed:0:1:2:3:4:5:Kat(l1 )/at(l1 ), Kat(l2 )/at(l2 )Khold/at(l1 ), Kat(l2 )/at(l2 )Kat(l3 )/at(l1 ), Kat(l2 )/at(l2 )Kat(l3 )/at(l1 ), Khold/at(l2 )Kat(l3 )/at(l1 ), Kat(l3 )/at(l2 )Kat(l3 )truetruetruetruetruetrue0pick(l1 )drop(l3 )pick(l2 )drop(l3 )merge am,at(l3 ) .also verify manner action sequence 2020 = {pick(l1 ), pick(l2 ), am,hold , drop(l3 )}classical plan KT,M (P ), reason atom Khold/at(l1 ) holdsfirst pick action second. due cancellation rule:pick(l2 ) : Khold/at(l1 ) Khold/at(l1 ) Kat(l2 )/at(l1 )expresses assumption at(l1 ) initial situation, hold at(l2 )known true action pick(l2 ), assumption, holdknown true action.5. Complete Translation: KS0complete instance translation scheme KT,M obtained simple mannersetting tags possible initial states problem P mergeprecondition goal literal L includes tags. call resultingexhaustive translation KS0 :Definition 8 (Translation KS0 ). conformant problem P , translation KS0 (P )instance translation KT,M (P )set union empty tag set S0 possible initial states P(understood maximal sets literals consistent I),set contain single merge = S0 precondition goal literal LP.translation KS0 valid hence sound, complete due correspondencetags possible initial states:Theorem 9 (Completeness KS0 ). conformant plan P ,classical plan 0 KS0 (P ) result dropping merge actions 0 .632fiCompiling Uncertainty Away Conformant Planning Problems#S0Problemadder-01blocks-02blocks-03bomb-10-1bomb-10-5bomb-10-10bomb-20-1coins-08coins-09coins-10coins-11comm-08comm-09comm-10corners-square-16corners-square-24corners-square-28corners-square-116corners-square-120square-center-16square-center-24log-2-10-10log-3-10-10ring-5ring-6safe-50safe-70safe-100sortnet-07sortnet-08sortnet-09sortnet-10uts-k-08uts-k-1018182311k1k1k1M1k1k1k1M5121k2k444442565761k59k1,2k4,3k50701002565121k2k1620KS0timelen> 2h0,22359,2805,91911,31518,310> 2.1GB20,22719,92521,531> 2.1GB18,36177,768> 2.1GB0,21020,72021,2264581,4 3652> 2.1GB13,1102> 2.1GB183,585> 2h12,617> 2.1GB0,5501,47061002,9289,83677,745> 2.1GB0,6461,258PONDtime len0,4260,426126,8 129119315810413939228526528> 2h153159165113167> 2h> 2h> 2h> 2h132261> 2h> 2h> 2h62033279504170> 2.1GB48025> 2h> 2h> 2h2447221967CFFtimelen> 2h> 2h> 2h0190150100390280260,13817805305906513,1140321304> 2h> 2h> 2h> 2h> 2h1,6834,71084,33193,64829,450109,9701252,4 100SNHSNHSNHSNH4,44616,558Table 1: KS0 translation fed FF planner compared POND Conformant FF(CFF) along times reported plan lengths. #S0 stands numberinitial states, SNH means goal syntax handled (by CFF). Times reportedseconds rounded closest decimal.633fiPalacios & Geffnerproblems P whose actions preconditions, argument simple:conformant plan P must classical plan P/s possible initialstate s, achieves (goal) literal Gi P/s s, must achieveliteral KGi /s KS0 (P ) well, followed merge action Gi ,must achieve literal KGi . presence action preconditions, argument mustapplied inductively plan length, idea remains (see proofappendix details): correspondence established evolutionfluents L problem P/s evolution fluents KL/s problemKS0 (P ).significance exhaustive KS0 translation theoretical.plenty conformant problems quite hard current planners even involvehandful possible initial states. example Square-Center-n task (Cimattiet al., 2004), agent reach center empty square grid certainty,knowing initial location. four actions move agent one unitdirection, except border grid, effects. standardversion problem, initial position fully unknown resulting n2 possible initialstates, yet problem remains difficult, actually beyond reach planners,small values n, even uncertainty reduced pair possible initial states.reason agent must locate heading goal. domainCorners-Square-n Table 1 variation Square-Center-n possible initialstates four corners grid.Table 1 shows results conformant planner based KS0 (P ) translationuses FF (Hoffmann & Nebel, 2001) solving resulting classical problem, comparestwo planners entered Conformant track 2006 Int. PlanningCompetition (Bonet & Givan, 2006): POND (Bryce et al., 2006) Conformant FF(Hoffmann & Brafman, 2006) (the two planners competition translationbased: T0 , based formulation developed paper, K(P ), based earlierrestricted formulation due Palacios & Geffner, 2006). Clearly, approachbased KS0 (P ) translation scale problems many possible initialstates, yet number states small, quite well.6. Complete Translations May Compactorder complete translations polynomial, certain assumptionsformulas initial situation need made. Otherwise, checking whethergoal true intractable itself, therefore polynomial complete translationwould impossible (unless P = NP). thus assume prime implicate (PI)form (Marquis, 2000), meaning includes inclusion-minimal clausesentails tautologies. known checking whether clause follows logicallyformula PI form reduces checking whether clause subsumed clausetautology, hence polynomial operation. initial situationsbenchmarks P form easily cast PI form normally specifiedmeans set non-overlapping oneof (X1 , . . . , Xn ) expressions translateclauses X1 Xn binary clauses Xi Xj 6= j resolventtautology.634fiCompiling Uncertainty Away Conformant Planning Problems6.1 Conformant Relevancetranslation KS0 (P ) complete introduces number literals KL/t exponential worst case: one possible initial state s0 . raises question:possible complete translations exhaustive sense? answeryes section provide simple condition ensures translationKT,M (P ) complete. makes use notion relevance:3Definition 10 (Relevance). conformant relevance relation L L0 P , read Lrelevant L0 , defined inductively1. L L2. L L0 : C L0 P L C action P3. L L0 L L00 L00 L04. L L0 L L00 L00 L0 .first clause stands reflexivity, third transitivity, second captures conditions relevant effect, fourth, conditions L preemptsconditional effects may delete L0 . replace 44 L L0 L L0equivalent 4 context 13, resulting definition one SonTu (2006), notion relevance used generate limited set possible partialinitial states 0-approximation complete (see Section 11 discussionrelation tags partial initial states).Notice according definition, precondition p action takenrelevant effect q. reason want relation L L0 captureconditions uncertainty L relevant uncertainty L0 .say relation conformant relevance. Preconditions must knowntrue order action applied, introduce propagate uncertaintyeffects action.let CI stand set clauses representing uncertainty initial situation, namely, non-unit clauses along tautologies L L complementaryliterals L L appearing unit clauses I, notion (conformant) relevanceextended clauses follows:Definition 11 (Relevant Clauses). clause c CI relevant literal L Pliterals L0 c relevant L. set clauses CI relevant L denoted CI (L).representation uncertainty initial situation relevantliteral L, possible analyze completeness translation KT,M termsrelation merges literals L, one hand, sets clauses CI (L)relevant L other.3. follow earlier account (Palacios & Geffner, 2007), many definitions theoremsdiffer number details (for example, notion relevance depends rules Pclauses initial situation). changes aimed making resulting formulation simplercleaner.635fiPalacios & Geffner6.2 Covering Translationsmay appear translation KT,M would completeW merges precondition goal literals L, understood DNF formulas tm t, contain muchinformation, thus equivalent CNF formula CI (L) captures fragmentinitial situation relevant L. intuition partially correct, missesone important point; namely every DNF formula equivalent CI (L) do:DNF representation captured merges must vivid enough. example, CI (L)single clause x x, completeness requires tag x, tag x, merge= {x, x} L containing two tags, even clause x x tautologythus equivalent DNF formula true.defining types tags merges required completeness then,let us first define closure set literals S, relative conformant problemP = hF, I, O, Gi, set literals follow I:= {L | I, |= L} .Let us also say consistent contain pair complementary literals.type merges required precondition goal literals Limply CI (L) satisfy well. notion satisfaction associatesconsistent set literals partial truth assignment implicit closureS, extended account conditions DNF formula (e.g.,merge L) satisfies CNF formula (e.g., CI (L)).Definition 12 (Satisfaction).1. consistent set literals satisfies clause L1 L2Lm contains one literals Li , = 1, . . . , m.2. consistent set literals satisfies collection clauses C satisfies clauseC.3. collection consistent sets literals satisfies collection clauses C setsatisfies C.type merges required completeness simply valid mergessatisfy set clauses CI (L). call covering merges:Definition 13 (Covering Merges). valid merge translation KT,M (P ) coversliteral L satisfies CI (L).example, CI (L) given clauses result oneof (x1 , . . . , xn ) expression, i.e. x1 x2 xn xi xj j, 1 i, j n, 6= j,merge = {x1 , . . . , xn } covers literal L, xi includes xi also xjj 6= i, thus xi satisfies CI (L).Wmerge = {t1 , . . . , tn }, denote DNF formula ti ti ,tag ti replaced closure ti , simple prove covers literal L,entails CI (L). merge covers L thus DNF formula strong enoughimply CNF formula CI (L) (through closure), weak enough entailed I,vivid enough satisfy CI (L).636fiCompiling Uncertainty Away Conformant Planning Problemsillustration, CI (L) given tautologies p p q q,= CI (L), merge m1 = {p, p} implies CI (L) satisfy CI (L). Likewise,merge m2 = {{p, q}, {p, q}} satisfies CI (L) entailed I. Finally, mergem3 = {{p, q}, {p, q}, {p, q}, {p, q}} satisfies CI (L) entailed I, thusvalid merge covers L.valid translation KT,M (P ) contains merge covers L preconditiongoal literal L P , say translation covers P coveringtranslation:Definition 14 (Covering Translation). covering translation valid translationKT,M (P ) includes one merge covers L, precondition goal literal LP .central result paper covering translations complete:Theorem 15 (Completeness). Covering translations KT,M (P ) complete; i.e.,conformant plan P , classical plan 0 KT,M (P ) 0merge actions removed.words, complete translations KT,M (P ) result tags mergescapture information initial situation relevant preconditiongoal literal suitable manner.Theorem 15 used two ways: proving completeness translation,checking covering condition holds, constructing complete translations,enforcing covering condition. addition, interest paper conformant planning optimality guarantees, theorem useful optimal conformantplanning well, whether cost plans defined length (action costs equal1) sum non-uniform action costs. cases, theorem ensuresproblem optimal conformant planning gets mapped problem optimal classicalplanning provided cost merge actions KT,M (P ) made sufficiently small.illustration Theorem 15, consider conformant problem P initial situation = {x1 xm }, goal G = L, actions ai , = 1, . . . , m, effect xi L.number possible initial states problem exponential m, disjunctionamong xi exclusive. So, translation KS0 (P ) complete exponentialsize. hand, consider translation KT,M (P ) = {x1 , . . . , xm }contains single valid merge = {x1 , . . . , xm } L. simple verifymerge covers goal L (satisfies CI (L) = I), hence translation KT,M (P )covering, Theorem 15, complete, polynomial m.Notice testing whether valid translation KT,M (P ) covering translationdone polynomial time, particular, computing set literals every tagtractable operation provided PI form; indeed, I, |= L0 iff |= L0iff L0 tautology subsumed clause I.6.3 Translation Kmodelsstraightforward show exponential translation KS0 considered Section 3,(non-empty) tags stand possible initial states, covering hence complete637fiPalacios & Geffneraccording Theorem 15. possible, however, take advantage Theorem 15devising complete translation usually compact. call Kmodels.Definition 16. translation Kmodels(P ) obtained general scheme KT,M (P )definingcontain one merge precondition goal literal L given modelsCI (L) consistent I,4contain tags merges along empty tag.translation Kmodels equivalent KS0 precondition goalliterals L, CI (L) = I; i.e., clauses relevant L. Yet, cases,first translation exponential number variables appearing one CI (L)set (the one largest number variables), second exponentialnumber unknown variables I. example, n precondition goalliterals Li , = 1, . . . , n P one, CI (Li ) unique oneof (xi1 , . . . , xim )expression, merge literal Li KS0 (P ) contain mn models n one-ofexpressions I, merge Li Kmodels(P ) contain modelssingle oneof (xi1 , . . . , xim ) expression CI (Li ). translation Kmodels thusexponentially compact exhaustive KS0 translation remaining soundcomplete:Theorem 17. translation Kmodels(P ) sound complete.worst case, however, Kmodels also exponential translation. thus considernext polynomial translations conditions complete.6.4 Conformant Widthaddress conditions compact, covering translation constructed polynomial time. this, define structural parameter callconformant width problem P , analogy notion width used graphicalmodels (Dechter, 2003), provide upper bound time space complexityrequired generating covering translation. precisely, complexity construction exponential conformant width problem P cannot exceednumber fluents P much lower.principle, would like define width w(P ) maximum tag size requiredtranslation KT,M (P ) covering translation. definition, however, wouldgive us complexity bounds want, checking validity mergetags bounded size intractable operation, whether initial situationprime implicate form not.5 need define width different way. First, letcover set clauses defined follows:4. models CI (L) understood conjuntions literals.5. problem checking whether entails DNF formula whose terms may 2 literalscoNP-hard even equivalent true. Indeed, 3-CNF formula; contradictory iffnegation (which 3-DNF) valid, turn true iff implied I. Actually,general prime implicate form, problem remains coNP-hard even terms DNF formulacontain 2 literals. thank Pierre Marquis pointing results us.638fiCompiling Uncertainty Away Conformant Planning ProblemsDefinition 18 (Cover). cover c(C) set clauses C, relative conformantproblem P initial situation I, collection minimal sets literals consistentcontains literal clause C.Two important properties cover c(C) set clauses C c(C) standsDNF formula logically equivalent CNF formula C given I, c(C)computed polynomial time size C bounded constant. Moreover, c(C)implies C satisfies C well. Thus particular, C collection clausesCI (L) relevant literal L, cover c(CI (L)) CI (L) valid mergecovers L. completeness covering translations, follows completetranslation KT,M (P ) constructed polynomial time size |CI (L)| setsclauses CI (L) precondition goal literals L P bounded. Unfortunately,condition rarely seems hold, yet weaker sufficient condition does: namely,often possible find subset C clauses either CI (L) tautologiesc(C) satisfies CI (L) thus covers literal L. thus define widthliteral L size smallest set (cardinality-wise). this, denoteCI (L) set clauses CI (L) extended tautologies form p p fluents peither p p appears CI (L) (if appear CI (L) p p CI (L)definition).Definition 19 (Width Literal). conformant width literal L P , written w(L),size smallest (cardinality-wise) set clauses C CI (L) c(C) satisfiesCI (L).consequence definition width literal must lie interval0 w(L) n, n number fluents P whose status initial situationknown. Indeed, CI (L) empty, w(L) = 0, set clauses CI (L),cover c(C) set C tautologies CI (L) must satisfy CI (L), thus w(L) |C| n.Similarly, CI (L) contains single clause x1 xm clauses x1 xmxi xj correspond oneof (x1 , . . . , xm ) expression, simple provew(L) = 1 singleton C = {x1 xm } generating cover c(C) = {{x1 }, . . . , {xn }}satisfies CI (L). Finally, CI (L) contains two tautologies pp qq, w(L) = 2smallest C CI (L) whose cover satisfies CI (L) CI (L) itself.width problem width precondition goal literal maximumwidth:Definition 20 (Width Problem). conformant width problem P , writtenw(P ), w(P ) = maxL w(L), L ranges precondition goal literals P .show problems bounded width, complete translationsconstructed polynomial time, moreover, almost existing conformant benchmarks bounded width, precisely, width equal 1. case,resulting translations use tags never greater size w(P ),problems width 1, tags single literals.Like (tree)width graphical models, computing width problem Pexponential w(P ), recognition problems small width carriedquite efficiently:639fiPalacios & GeffnerProposition 21 (Determining Width). width w(P ) P determined timeexponential w(P ).particular, test w(P ) = 1 considering one one sets Cincludes single clause CI (L), verifying whether c(C) satisfies CI (L) not.w(P ) 6 1, verification must carried setting C setclauses CI (L) increasing values i. fixed value i, polynomialnumber clause sets C verification one done polynomialtime. Moreover, arguments regarding w(L), w(P ) never exceednumber unknown fluents problem:Proposition 22 (Bounds Width). width P 0 w(P ) n, nnumber fluents whose value initial situation known.6.5 Polynomial Translation Kitranslation Ki , parameter non-negative integer, instancegeneral KT,M scheme designed sound, polynomial fixed i, completeproblems width w(P ) i. Thus, example, translation K1 sound, polynomial,complete problems width 1.Definition 23 (Translation Ki ). translation Ki (P ) obtained generalscheme KT,M (P )set contain one merge = c(C) precondition goal literal L Pset C clauses CI (L) covers L. setexists, one merge = c(C) L created set C clauses CI (L),merges created L CI (L) empty;collection tags appearing merges empty tag.translation Ki (P ) applies problems P width, remaining cases exponential polynomial number fluents, actions, clauses P . addition,translation Ki (P ) sound, problems width bounded i, complete.Theorem 24 (Properties Ki ). fixed i, translation Ki (P ) sound, polynomial,w(P ) i, covering complete.Soundness result merges valid construction, covers c(C)C CI (L) entailed C hence I. complexity polynomial fixedi, polynomial number clause sets C size CI (L), constructingcover c(C) one them, polynomial operation. Finally, completeness followsdefinition width: w(P ) i, set clauses C CI (L)size |C| greater whose cover satisfies CI (L), thus Ki (P ) must containmerge = c(C) L covers L.Notice = 0, translation Ki (P ) reduces basic K0 (P ) translationintroduced Section 3 tags (other empty tag) merges. Before,assessed completeness translation terms 0-approximation semantics.Theorem 24 provides alternative interpretation: translation K0 (P ) complete640fiCompiling Uncertainty Away Conformant Planning Problems123456789101112131415Domain-ParameterSafe-n combinationsUTS-n locsRing-n roomsBomb-in-the-toilet-n bombsComm-n signalsSquare-Center-n n gridCube-Center-n n n cubeGrid-n shapes n keysLogistics n pack locsCoins-n coins locsBlock-Tower-n BlocksSortnet-n bitsAdder n pairs bitsLook-and-Grab objs n n locs1-dispose objs n n locs# Unknown Fluentsnn4nnn2n3nnmnmnmn (n 1) + 3n + 1n2nnnmnnmWidth1111111111n (n 1) + 3n + 1n2nTable 2: Width parameterized domainsproblems P zero width. problems set clauses CI (L)relevant precondition goal literal L empty. makes precise intuitionmentioned K0 (P ) translation complete problems uncertaininformation relevant. cases, none clauses initial situationmake sets relevant clauses CI (L) preconditions goal literals L.illustration Theorem 24, consider conformant problem P initialsituation = {x1 xm }, goal G = {L}, actions ai , = 1, . . . , m,effect xi L. problem, singleton set clauses C = CI (L) =c(C) = {{x1 }, . . . , {xm }} covers CI (L). Then, since precondition goalliteral, K1 (P ) includes single merge = c(C) L singleton tags ti = {xi },write simply = {x1 , . . . , xm }. translation K1 (P ) polynomial m,since w(P ) = 1, Theorem 24 complete. Notice example,translations KS0 (P ) Kmodels(P ) identical exponential (the numbermodels CI (L)).6.6 Width Conformant Benchmarkspractical value notion width becomes apparent width existingbenchmarks considered. Table 2 summarizes width many existing benchmarkdomains conformant planning. domains depend certain parameters ncapture size instances (e.g., size grid, number objects, etc).6 domainbounded width width grow size instances,width equal instances width regardless parameter values.seen table, width existing benchmarks 1.cases, means sets CI (L) clauses relevant precondition6. names parameterized domains table coincide names instancescurrently used. E.g. Comm-n IPC5 refers Communication instance necessarilyinstance n signals.641fiPalacios & Geffnergoal literal L contain single clause (often tautology p p disjunction x1 . . . xm )single oneof (x1 , . . . , xm ) expression (that translates disjunction x1 xmclauses xi xk ). shown above, w(L), therefore, w(P ), equal 1 thesescases.extreme domains Blocks, Sortnet, Adder,maximal widths; i.e., widths equivalent number fluents whose status initial situation known. fluents interactaction conditions (not preconditions). numbers Blocks Table 2, thus follownumber fluents involved; namely, fluents on(x, y), clear(x), ontable(x),holding(x).Finally, domains 1-dispose Look-and-Grab (Palacios & Geffner, 2006, 2007)objects unknown locations grid n n must collected robotwhose gripper hold one object time, width equal m, meaningwidth domains grows number objects size grid.case, clauses possible locations objectsrelevant condition hand empty pick actions.Let us point completeness translation Ki (P ) problems Pwidth w(P ) bounded i, establishes correspondence conformant plansP classical plans KT,M (P ). solving P , however, correspondenceneeded; suffices Ki (P ) solvable; plan Ki (P ) encodeconformant plan P , even Ki (P ) capture conformant plans P .perspective, makes sense refer smallest value parameterclassical problem Ki (P ) solvable, effective width P , denoted (P ). turns(P ) cannot larger w(P ), may much smaller. interestingexample comes Sortnet-n domain (Bonet & Geffner, 2000). Sortnet-nconsidered challenging domain conformant planning planners ablescale even small values n (the number entries sorted sorting network).domain width n, compact encoding used IPC5, input vectorrepresented set bits, exploiting fact sorting vectors numbers reducessorting vector bits (0s 1s). domain cannot solved K1 translationFF reports correctly unsolvable brief unsuccessful search. hand,possible reformulate domain, replacing unary high(i) low(i) predicatesbinary predicates less(i, j) compare two vector entries. call reformulationSort-2-n. encoding Sort-n linear n, encoding Sort-2-n quadratic n,cases, problem width maximum, given number fluents whosestatus initial situation unknown. Yet, compact Sort-n encodingsolvable K1 translation, K1 suffices solve problem expanded Sort2-n encoding actually also solved K0 . Thus effective width Sort-2-n0. Interestingly, provided K0 translation Sort-2-n, instances solved20 entries. hand, conformant planners Conformant-FF PONDsolve Sort-2-n instances n greater 3.642fiCompiling Uncertainty Away Conformant Planning Problems7. Tags Initial Statesdeeper understanding results obtained relating tags possibleinitial states. looking closely relation context covering translations,able answer question polynomial number contexts (tags)play role exponential number possible initial states problems boundedwidth.this, let us first recall notation introduced Section 2.2, state s,wrote I(s) refer set atoms encoding (i.e, p I(s) iff p true s) P/srefer classical planning problem P/s = hF, I(s), O, Gi like conformantproblem P = hF, I, O, Gi initial state fixed s.Let us extend notation say action sequence conforms setstates given conformant problem P iff plan classical problem P/sS. Clearly, conformant plan P nothing else action sequenceconforms set S0 possible initial states P , yet notion conforms allowsus abstract away initial situation make precise notion basis:Definition 25 (Basis P ). set states 0 basis conformant problem P =hF, I, O, Gi 0 subset set S0 possible initial states P every planconforms 0 conforms set possible initial states S0 .words, 0 basis P , necessary consider states S0computing conformant plans P ; suffices consider states 0 . aimshow width P bounded, P polynomial basis 0 even S0exponential size. Moreover, states basis close correspondencetags appearing covering translation.illustration, consider problem P actions ai , = 1, . . . , n, effectsai : xi L. Let G = {L} goal = {x1 xn } initial situation.set S0 possible initial states truth valuations xi atoms leastone atoms true. 2n 1 states. hand, one showset S00 n valuations exactly one atoms true provides basisP ; i.e., plans conform n possible initial states, exactly plansconform complete set 2n 1 possible initial states S0 .reduction number possible initial states must considered computing conformant plans results two monotonicity properties formulate usingnotation rel(s, L) refer set literals L0 true staterelevant literal L:rel(s, L) = {L0 | L0 L0 relevant L} .Proposition 26 (Monotonicity 1). Let s0 two states let action sequenceapplicable classical problems P/s P/s0 . achieves literal L P/s0rel(s0 , L) rel(s, L), achieves literal L P/s.Proposition 27 (Monotonicity 2). 0 two collections statesevery state every precondition goal literal L P , state s0 0rel(s0 , L) rel(s, L), plan P conforms 0 , planP conforms S.643fiPalacios & Geffnerproperties, followsProposition 28. 0 basis P every possible initial state P everyprecondition goal literal L P , 0 contains state s0 rel(s0 , L) rel(s, L).proposition allows us verify claim made example set S00 ,contains number states linear n, basis P exponentialnumber possible initial states. Indeed, problem precondition singlegoal literal L, every state makes one atom xi true (theseliterals relevant L), state s0 S00 makes one atoms true,hence relation rel(s0 , L) rel(s, L) holds.question address build basis complies conditionProposition 28 given covering translation KT,M (P ). this, let = {t1 , . . . , tn }merge covers precondition goal literal L, let S[ti , L] denote setpossible initial states P rel(s, L) ti ; i.e., S[ti , L] contains possible initialstates P make literals L0 relevant L false, exceptclosure ti ti . show first prime implicate form, S[ti , L] non-emptyset:7Proposition 29. initial situation prime implicate form = {t1 , . . . , tn }valid merge covers literal L P , set S[ti , L] possible initial statesP rel(s, L) ti non-empty.Let s[ti , L] stand arbitrary state S[ti , L]. obtain following result:Theorem 30. Let KT,M (P ) covering translation problem P initialsituation PI form, let 0 stand collection states s[ti , L] Lprecondition goal literal P ti tag merge covers L. 0 basisP .important result three reasons. First, tells us build basis Pgiven tags ti covering translation KT,M (P ). Second, tells us sizeresulting basis linear number precondition goal literals L tags ti .third, makes role tags ti covering translation KT,M (P ) explicit, providingintuition works: tag ti merge covers literal L represents onepossible initial state; namely, state s[ti , L] makes false literals L0relevant L except ti . plan conforms critical states,conform possible initial states monotonicity (Proposition 27). followsparticular that:Theorem 31. P conformant planning problem bounded width, P admitsbasis polynomial size.Namely, conformant problems P width bounded non-negative integer admitpolynomial translations complete, plans conform possiblyexponential number initial states P correspond plans conform7. Recall assuming throughout initial situation logically consistenttags consistent I.644fiCompiling Uncertainty Away Conformant Planning Problemssubset critical initial states polynomial number (namely,polynomial basis). Thus, one complete polynomial translation problemsKi translation; another one, KS0 translation tags associatedcritical initial states rather initial states.illustration, problem P actions ai effects ai : xi L,goal G = {L}, initial situation = {x1 xn }, K1 (P ) translation tags xi ,= 1, . . . , n, merge = {x1 , . . . , xn } goal literal L, covering translation.Theorem 30 states basis 0 P results collection states simake tag xi true, literals relevant L xi false (i.e.,xk atoms k 6= i). precisely basis P includesstates make single atom xi true = 1, . . . , n: plans conformbasis exactly plans conform whole collection possible initialstates P . basis size polynomial though, numberpossible initial states P exponential m.8. Planner T0current version conformant planner T0 based two instances generaltranslation scheme KT,M (P ) whose outputs fed classical planner FF v2.3.8 Oneinstance polynomial necessarily complete; complete necessarilypolynomial. incomplete translation, T0 uses K1 complete problemswidth greater 1, argued above, result solvable instances problemslarger widths. complete translation, Kmodels translation used insteadsimple optimization: K1 translation produces single merge covers L,merge used L instead potentially complex one determinedKmodels. mere optimization resulting translation remains complete.merges Kmodels, result models set clauses CI (L)consistent I, computed using SAT solver relsat v2.20 (Bayardo Jr. & Schrag,1997). current default mode T0 , one used experiments below,two translations K1 Kmodels used sequence: FF called first uponoutput K1 fails, called upon output Kmodels. experimentsbelow, indicate cases Kmodels invoked.translations used T0 accommodate certain simplifications two additionalactions capture types deductions. simplifications facttranslations considered uniform sense literals L Prules C L conditioned tags . practical pointview, however, needed. simplifications address source inefficiency.particular:literals KL/t created closure contains literal relevant L.case, invariance KL/t KL holds, thus, every occurrenceliteral KL/t KT,M (P ) replaced KL.8. conformant planner T0 along benchmarks considered paper availablehttp://www.ldc.usb.ve/hlp/software.645fiPalacios & Geffnersupport rules : KC/t KL/t non-empty tags created Lrelevant literal L0 merge contains t, case, literalKL/t cannot contribute establish precondition goal. Similarly, cancellationrules : KC/t KL/t non-empty tags created Lrelevant literal L0 merge contains t.support cancellation rules : KC/t KL/t : KC/t KL/tgrouped : KC/t KL/t KL/t every fluent L0 relevant L, eitherL0 L0 entailed t. case, incomplete informationL given initial situation, thus invariant KL/t KL/t holds,KC/t equivalent KC/t.Two types sound deductive rules included translations:rule : KC KL added : C, L L rule P action a,rule P form : C 0 L,rules KL1 , . . . , KLi1 , KLi+1 , . . . , KLn KLi = 1, . . . , n addednew unique action precondition, L1 Ln static clause P (aclause P static true initial situation provably true action).rules versions action compilation static disjunctions rules (Palacios &Geffner, 2006, 2007), appear help certain domains without hurting others.version T0 reported assume initial situation Pprime implicate form rather renders PI form running version Tisonsalgorithm (1967), computation none benchmarks solved took 48seconds.translators T0 written OCaml code parsing PDDL fileswritten C++.9. Experimental Resultsconsidered instances three sources: Conformant-FF distribution, conformant track 2006 International Planning Competition (IPC5), relevant publications (Palacios & Geffner, 2006, 2007; Cimatti et al., 2004). instances runcluster Linux boxes 2.33 GHz 8GB. experiment cutoff 2h 2.1GBmemory. Times T0 include steps, particular, computation prime implicates, translation, search (done FF). also include results ConformantTrack recent 2008 International Planning Competition (IPC6).Goals sets literals sets clauses transformed T0 standardway: goal clause C : L1 Lm modeled new goal atom GC , newaction executed added rules Li GC , = 1, . . . , m.99. alternative way represent CNF goals converting DNF firstaction End map non-mutex terms dummy goal LG . alternative encoding payscases, Adder-01 instance get solved default CNF goalencoding (see below).646fiCompiling Uncertainty Away Conformant Planning ProblemsProblembomb-100-100square-center-96sortnet-09blocks-03dispose-16-1look-and-grab-8-1-1sgripper-30P#Acts #Atoms #Effects101004044020041967604668109323015212171479243435235822204872391456Time235,18,34163,66,921,5K1 (P )PDDL#Acts #Atoms #Effects Size102011595505002,9737248750543,856297071549135,13711370352320,71218 13312234580,335387081184977,88601127127691Table 3: Translation data selected instances: #Acts, #Atoms, #Effects standnumber actions, fluents, conditional effects. Time translationtime seconds rounded closest decimal, PDDL Size sizePDDL file Megabytes.Table 3 shows data concerning translation group selected instances.seen, number conditional effects grows considerably cases, sometimestranslation may take several seconds.Tables 4, 5, 6, 7, 8, show plan times lengths obtained numberbenchmarks T0 , POND 2.2 (Bryce et al., 2006), Conformant FF (Hoffmann & Brafman,2006), MBP (Cimatti et al., 2004) KACMBP (Bertoli & Cimatti, 2002). lasttwo planners accept problems standard syntax (based PDDL),limited number experiments performed them. general picture T0scales well domains, exceptions Square-Center Cube-CenterTable 5, KACMBP scales better, Sortnet Table 6, KACMBP MBPscale better; Adder Table 6, POND planner able solve oneinstance.problems Table 4 encodings Conformant-FF repository: Bomb-x-yrefers Bomb-in-the-toilet problem x packages, toilets, clogging; Logistics-ij-k variation classical version uncertainty initial location packages;Ring-n closing locking windows ring n rooms without knowingcurrent room; Safe-n opening safe n possible combinations.problems width 1. T0 clearly best last two domains, first twodomains, Conformant-FF well too.Table 5 reports experiments four grid domains: Cube-Center-n refers problemreaching center cube size n3 completely unknown location; SquareCenter-n similar involves square n2 possible locations; Corners-Cube-nCorners-Square-n variations problems set possible initial locationsrestricted Cube Square corners respectively. MBP KACMBP appeareffective domains, although KACMBP doesnt scale well corner versions.T0 solves problems, corner versions, quality plans poor.problems also width 1.Table 6 reports experiments problems 2006 International Planning Competition (Bonet & Givan, 2006). domains Coins, Comm UTS width 1.others max width given number unknown fluents initial situation.647fiPalacios & GeffnerProblembomb-20-1bomb-20-5bomb-20-10bomb-20-20bomb-100-1bomb-100-5bomb-100-10bomb-100-60bomb-100-100logistics-4-3-3logistics-2-10-10logistics-3-10-10logistics-4-10-10ring-4ring-5ring-6ring-7ring-8ring-30safe-10safe-30safe-50safe-70safe-100T0time0,10,10,10,10,50,71,14,259,40,111,52,50,10,10,10,10,113,40,10,10,41,122,5len493530201991951901401003584108125131720303912110305070100PONDtime len4139 39> 2h> 2h> 2h5640> 2h> 2h> 2h118620332744433> 2h0102309504170> 2.1GBCFFtimelen03903503002056,719952,919546,81909,414011000371,6834,71084,41210,4184,33193,64883771> 2h0101,43029,450109,9701252,4 100MBPtime len> 2h> 2h> 2h> 2h> 2h> 2h> 2h> 2h0110,1140,6173,8204023> 2h0,110> 2h> 2h> 2h> 2hKACMBPtime len0400,2400,5402401,92004,320016,4 200> 2h> 2h> 2.1GB> 2.1GB> 2.1GB> 2.1GB0260,1580,2990,52042432> 2.1GB0100,2300,7502,4708,6100Table 4: Experiments well known benchmarks. Times reported seconds roundedclosest decimal. means time memory smaller instances.648fiCompiling Uncertainty Away Conformant Planning ProblemsProblemsquare-center-8square-center-12square-center-16square-center-24square-center-92square-center-96square-center-100square-center-120cube-center-5cube-center-7cube-center-9cube-center-11cube-center-15cube-center-19cube-center-63cube-center-67cube-center-87cube-center-91cube-center-119corners-square-12corners-square-16corners-square-20corners-square-24corners-square-28corners-square-36corners-square-40corners-square-72corners-square-76corners-square-80corners-square-120corners-cube-15corners-cube-16corners-cube-19corners-cube-20corners-cube-23corners-cube-24corners-cube-27corners-cube-52corners-cube-55T0timelen0,2210,2330,3440,86945,327350,2285> 2.1GB> 2.1GB0,1180,1270,2330,3450,5630,88128,527941,6297137,5 387> 2.1GB> 2.1GB0,1640,21020,31480,52020,72641,74122,549826,1 147430,5 163238,2 1798223,6 38980,81470,91742,52252,72586,33196,735814,64294481506> 2.1GBPONDtime len24112521322 61> 2h> 2h1222433472987880 109> 2h> 2h11441131 67> 2h> 2h907 1053168 115> 2h> 2hCFFtimelen70,650> 2h> 2h8,245> 2h> 2h1,78213,114073,7214321304MPL134,5 284439,4 214868,4 4563975,6 332MPLMBPtimelen0240360480720,92760,92881,13001,93600280330,1540,2590,2691,611128285> 2.1GB> 2.1GB0360480,3600,6721,1841,51087,8120118,8 216371228649,6 240> 2.1GB3,76912,572549,5 1111061,9 90> 2h> 2hKACMBPtimelen0280420560840,33220,33360,33500,44200250350450550750,1950,53150,73351,24351,24552,15950,21060,615832687,534620,75023308,8 808> 2h> 2h174,1 391270,5 3161503,1 48827596256265,9 899> 2h> 2hTable 5: Experiments grid problems. Times reported seconds roundedclosest decimal. MPL CFF means plan exceeds maximal plan length(500 actions). means time memory smaller instances.649fiPalacios & GeffnerProblemadder-01adder-02blocks-01blocks-02blocks-03coins-10coins-12coins-15coins-16coins-17coins-18coins-19coins-20coins-21comm-07comm-08comm-09comm-10comm-15comm-16comm-20comm-25sortnet-06sortnet-07sortnet-08sortnet-09sortnet-10sortnet-11uts-k-04uts-k-05uts-k-06uts-k-07uts-k-08uts-k-09uts-k-10uts-l-07uts-l-08uts-l-09uts-l-10T0time len> 2h> 2h0,150,32382,6800,1260,1670,1790,31130,2960,2970,21050,2107> 2h0,1540,1610,1680,1750,11100,21380,82782,34530,6212,5289,63676,845> 2.1GB> 2.1GB0,1230,1290,2350,4410,6470,9531,3590,2700,3800,6930,797PONDtime len15915> 2h0,140,426126,8 129528> 2h> 2h047153159165695> 2h> 2.1GB182048025> 2h> 2h222428103413402447> 2h2219672015893767> 2h> 2hCFFtime lenSNHSNH06> 2h> 2h0,1380,87238933,3 1451,4946,211816,5 12820,6 143> 2h0470530590650,2950,41196,423956,1 389SNHSNHSNHSNHSNHSNH0,1220,3280,8341,9404,4468,65216,5580,2410,4470,8531,659MBPtimelenNRNRNRNRNR> 2h> 2h0,2550,2710,2770,3850,91151,615150,9340> 2h0170200280360,1370,1475,4321247,3 381704,8 50> 2h> 2h10,58941,11061176137> 2hKACMBPtimelenNRNRNRNRNR4,21063654,7 674> 2h> 2h63,6531966,8 53> 2h> 2h0210280360450,1550,1661,530195,442> 2h> 2h> 2h> 2hTable 6: Experiments problems IPC5. Times reported seconds roundedclosest decimal. SNH CFF means goal syntax handled,NR MBP KACMBP planners run due lacktranslations PDDL. means time memory smaller instances.650fiCompiling Uncertainty Away Conformant Planning ProblemsProblemdispose-4-1dispose-4-2dispose-4-3dispose-8-1dispose-8-2dispose-8-3dispose-12-1dispose-12-2dispose-12-3dispose-16-1dispose-16-2look-and-grab-4-1-1look-and-grab-4-1-2look-and-grab-4-1-3look-and-grab-4-2-1look-and-grab-4-2-2look-and-grab-4-2-3look-and-grab-4-3-1look-and-grab-4-3-2look-and-grab-4-3-3look-and-grab-8-1-1look-and-grab-8-1-2look-and-grab-8-1-3look-and-grab-8-2-1look-and-grab-8-2-2look-and-grab-8-2-3look-and-grab-8-3-1look-and-grab-8-3-2look-and-grab-8-3-3T0timelen0,1590,11100,31222,742618,4639197,1 7617812742555 1437> 2.1GB3821702> 2.1GB0,3300,540,614351249,41460,024> 2.1GB213,34> 2.1GB58,224275,39055,8958> 2h> 2h> 2h> 2h> 2h> 2hPONDtime len9553670308 102> 2.1GB> 2.1GB3098 16> 2h> 2h> 2.1GB> 2h> 2h> 2.1GBCFFtimelen0,1390,2560,673339,1 2272592,1 338> 2h> 2.1GB> 2hMclMcl> 2hMclMcl> 2h> 2h> 2hMBPtime len> 2h> 2h> 2h0,0250,015> 2h0,0250,025> 2h0,0250,025> 2h> 2h> 2h> 2h> 2h> 2h> 2h> 2h> 2hKACMBPtime len17,181> 2h> 2h0,6540,060,060,63400,0160,0160,98600,0260,016> 2h> 2h> 2h> 2h> 2h1195 178> 2h> 2h17,958Table 7: Problems Palacios Geffner (2006, 2007): Times reported secondsrounded closest decimal. means time memory smaller instances.Mcl mean many edges many clauses respectively.T0 dominates domains except Adder POND planner ablesolve instance, Sortnet, MBP KACMBP well, possibly dueuse cardinality heuristic OBDD representations. T0 fails Adder FFgets lost search. Looking problem closely, found FF could solve(translation the) first instance less minute provided CNF goalproblem encoded DNF explained footnote 9, page 646. domains Adder,Blocks, Sortnet table, along domain Look-and-Grab next table,domains considered FF run K1 translation reports solutionbrief search, triggering use complete Kmodels translation.cases Kmodels used, K1 translation unreachable goal fluentneed try FF it.651fiPalacios & GeffnerProblempush-to-4-1push-to-4-2push-to-4-3push-to-8-1push-to-8-2push-to-8-3push-to-12-1push-to-12-2push-to-12-31-dispose-8-11-dispose-8-21-dispose-8-3T0timelen0,2780,3850,68781,8464457,94231293,1 597> 2h> 2h> 2.1GB82,21316> 2.1GB> 2.1GBPONDtime len55017158> 2h> 2h> 2h> 2.1GB> 2.1GBCFFtime len0,3460,7471,648> 2.1GB> 2.1GB> 2.1GB> 2h> 2hTable 8: problems Palacios Geffner (2006, 2007). MBP KACMBPtried problems use different syntax. Times reportedseconds rounded closest decimal. means time memorysmaller instances.problems reported Table 7 Table 8 variations family grid problems(Palacios & Geffner, 2006, 2007). Dispose retrieving objects whose initial locationunknown placing trash given, known location; Push-to variationobjects picked two designated positions gridobjects pushed to: pushing object cell contiguous cell movesobject cell. 1-Dispose variation Dispose robot handempty condition pick actions work. result, plan 1-Disposescan grid, performing pick ups every cell, followed excursions trash can,on. plans get long (a plan reported 1316 actions). Look-and-Grabaction picks objects sufficiently close any, pickup must dump objects collected trash continuing. problemP-n-m table, n grid size number objects. Look-n-Grab,third parameter radius action: 1 means hand picksobjects 8 surrounding cells, 2 hand picks objects 15surrounding cells, on. domains Tables 7 8 width 1 except 1-DisposeLook-n-Grab. because, hand empty fluent relevantgoal, clauses location objects relevant hand empty.domains T0 appears better planners. Kmodels translationtriggered instances Look-and-Grab-n-m-r > 1 (the widthinstances, mentioned Section 6.6, m, independent grid size).also report additional data Table 9, comparing search resultsuse FF planner classical translations T0 , search carriedConformant-FF original conformant problems. Conformant-FF conformantplanner built top FF searches explicitly belief space. table illustratestwo problems faced belief-space planners mentioned introduction handle652fiCompiling Uncertainty Away Conformant Planning ProblemsProblembomb-100-1bomb-100-100Safe-100logistics-4-10-10square-center-8square-center-12cube-center-5cube-center-7blocks-01blocks-02coins-20comm-25uts-k-10dispose-8-1dispose-8-2dispose-8-3look-and-grab-4-1-1Nodes51491001003564634390002211816004614201235517581107179724944955CFFTime32,90,81747,44,4259,3>5602,58,2>5602,50,0>5602,520,656,116,5339,12592,1>5602,5>5602,5Nodes/sec156,51250,180,578,17269,614,646000,3609,23,53,30,70,40,9Nodes52502011027744672741054786783177762117138703058089679FF T0Time Nodes/sec0,4112804,97,5326,70255000,471646,80,059200,0324000,0174000,052500117500,043000,04195750,434132,60,34182,40,7815016,714,326077,5190,23054,10,1790Table 9: CFF Conformant Problems vs. FF Translations: Nodes stand numbernodes evaluated, Time expressed seconds, Nodes/sec stands averagenumber nodes per second. Numbers shown bold either CFF FFevaluate significantly less nodes (an order-of-magnitude reduction more). Timespreceded > time outs.653fiPalacios & Geffnerresults translation-based approach. belief representationupdate problem appears overhead maintaining evaluating beliefs,shows number nodes evaluated per second: CFF evaluateshundred nodes per second; FF evaluates several thousands. time, heuristicused CFF conformant setting, appears less informed heuristic usedFF classical translations. domains like Square-Center-n, Cube-Center-n,Blocks, Look-and-Grab, FF needs orders-of-magnitude less nodes CFF findplan, oppositive true Dispose-n-m FF evaluates many nodesCFF. Nonetheless, even then, due overhead involved carrying beliefs, FFmanages solve problems CFF cannot solve. example, instance Dispose-8-3solved T0 evaluating half million nodes, times CFFevaluating less three thousand nodes.Tables 10 11 provide details results Conformant Track 2008International Planning Competition (IPC6) (Bryce & Buffet, 2008), held almost timeoriginal version paper submitted, planner binaries submittedorganizers months before. version T0 IPC6 differentversion T0 used IPC5, winning entry, different alsoversion reported paper. relation, former, T0 IPC6 cleanercomplete reimplementation; relation latter, T0 IPC6 handled problems widthgreater 1 different way. explained previous section, current versionT0 , uses K1 basic translation regardless width problem, switchingKmodels search K1 fails. version T0 IPC6, basic translationcombination K0 K1 ; precisely, merges literals L width w(L) = 1,generated according K1 , merges literals L width w(L) 6= 1generated all. result basic translation T0 IPC6 lighterbasic translation current version T0 could fail problems widthhigher 1 latter solve. Retrospectively, good choice,didnt much impact results. however bug programprevented two width-1 domains, Forest Dispose, recognized such,thus resulted use Kmodels translation, complete widths,scale well.two conformant planners entered IPC6 CpA(H) CpA(C);belief-space planners represent beliefs DNF formulas, use simplebelief-state heuristics guiding search (Tran, Nguyen, Pontelli, & Son, 2008, 2009).belief progression planners done quite effectively, progressing termturn, according 0-approximation semantics. potential blow comesnumber terms DNF formula encoding initial belief state. Rather choosingterms initial belief state possible initial states, planners limitterms DNF formula collection partial initial states assigntruth value literals deemed irrelevant. resulting belief representationcomplete may still result exponential number terms (Son & Tu, 2006). orderreduce number terms initial DNF formula, independent one-ofexpressions combined. example, two independent one-of clauses oneof (x1 , x2 )oneof (y1 , y2 ) would give rise 4 possible initial states DNF terms, combinedsingle one-of expression oneof (x1 y1 , x2 y2 ), results 2 possible initial654fiCompiling Uncertainty Away Conformant Planning ProblemsDomainBlocksAdderUTS CycleForestRaos keysDispose# Instances442792990CpA(H)4121276CpA(C)3121259T0 IPC63138120Table 10: Data Conformant Track recent IPC6 Competition: Numberproblems solved conformant planners, time 20 mins.bold, entry planner performed best domain. dataBryce Buffet (2008)states terms. one-of expressions independent showninteract problem. technique appears related notion criticalinitial states considered Section 7, shown plans conformcritical initial states must conform also possible initial states. heuristics usedCpA(H) CpA(C) combinations cardinality heuristic, measuresnumber states belief state, total sum heuristic, adds heuristic distancesgoal possible state, number satisfied goals, countsnumber top goals achieved. heuristics simple, yet work wellbenchmarks.Tables 10 11 show data obtained IPC6 organizers planner logs.first table appears IPC6 report (Bryce & Buffet, 2008), new domainsForest Raos keys explained, shows number problems solvedplanner, displaying bold planner best domain. planner CpA(H),declared winner, declared best three domains (Blocks, Raos keys,Dispose), T0 best two domains (UTS Cycle Forest), CpA(C)best one (Adder).Table 11 shows additional details instances; particular, total timetaken solve instance length plans three planners.terms domain coverage, planners similarly domains, exceptForest, T0 solved instances CPA(H) solved (8/9 vs. 1/9),Dispose, CPA(H) solved instances T0 solved (76/90 vs. 20/90).terms time plan quality, CpA(H) CpA(C) appear slightly fasterT0 Blocks, produce much longer plans. Dispose, T0 scales betterCpA(H) CpA(C) size grids, worse number objects.Indeed, T0 manages solve largest grid single object (Dispose-10-01),CpA(H) CpA(C) solve instances 2 objects largest grids.cases, plan lengths produced T0 shorter; e.g., plan Dispose-04-03contains 125 actions T0 , 314 CpA(H), 320 CpA(C).Dispose actually domain cardinality heuristic well generation plans, even plans tend rather long. discussed above, domain,agent scan grid collecting set objects unknown locations, time655fiPalacios & Geffneraction picking object cell may contain object made (exceptfirst time), cardinality belief state reduced. Indeed, initially objectmay positions p1 , p2 , . . . , pn , pick p1 , object positionsp2 , . . . , pn gripper, pick p2 , object positions p3 , . . . , pngripper, on, pick action decreasing cardinality belief state,becoming singleton belief object must gripper certainty.problem version T0 used IPC6 Dispose domain,FF explores many states search, explained above, usedexpensive Kmodels translation instead lighter K1 translation completedomain width 1. bug fixed, T0 solves 60 rather 20 90Dispose instances, still failing larger grids many objects, producingmuch shorter plans. example, Dispose-06-8 solved plan 470 actions,CpA(H) CpA(C) solve plans 2881 3693 actions respectively.bug surfaced Forest domain, prevented solution one instanceonly. Forest, Dispose, UTS Cycle conformant widths equal 1,domains larger widths (see Table 2 widths Blocks Adder).second domain IPC6 FF got lost search Adder, indeed, T0solve instance. instance shown solved T0 competitionreport, appears mistake. Similarly, fourth instance blocks, reportedsolved CPA(H), may mistake too; indeed, plan instance foundlogs, T0 reports goal unreachable Kmodels translationcomplete. According T0 , instance four Raos key unsolvable too.hand, T0 failed larger UTS Cycle Raos key instances translation.first, resulting PDDLs large cant loaded FF;second, number init clauses turns quite large (above 300), giving risestill larger set prime implicates (above 5000) caused translator runmemory. second instance Raos keys, however, rather small T0 didnt solvedue different bug. bug fixed, T0 solves 0.3 seconds, producing plan53 actions, compares well solutions produced CpA(H) CpA(C)0.7 1.9 seconds, 85 99 steps, respectively.10. Non-Deterministic Actionstranslation schemes considered limited problems deterministic actionsonly. Nonetheless, illustrate below, schemes applied non-deterministicactions well provided suitable transformations included. cover transformations briefly matter illustration only.Consider conformant problem P non-deterministic action effects : C oneof (S1 ,S2 , . . . , Sm ), Si set (conjunction) literals, transformed problemP 0 , effects mapped deterministic rules form : C, hi Si ,expression oneof (h1 , . . . , hm ) added initial situation P 0 . P 0 , hidden hivariables used encoding uncertainty possible outcomes Si action a.easy show non-deterministic conformant problem P deterministic conformant problem P 0 equivalent provided plans P P 0considered non-deterministic action P executed once. Namely,656fiCompiling Uncertainty Away Conformant Planning ProblemsProblemInstanceBlocks1234112312345678124,14,24,34,46,16,26,36,48,18,28,310,110,2AdderUTS CycleForestRaos keysDisposeCpA(H)timelen040,1285,9411143,9 2578,530,8325,36CpA(C)timelen070,1356,31578,30,624,73363,62411,6180,10,70,30,71,324,710,417,727,640,186,786,7288580197314431270643101613897531851185101,90,40,91,82,84,542,297,9172,540,3524,6299988206320434187735122817215181962T0 IPC6time len0,150,12317,8830,10,75,40,21,32,212,114,469,7355,137101645781291152002560160,13,6528,3771101250,9217,72043297,432645683Table 11: Running time plan length IPC6 logs. Time seconds. Blanks standtime memory out. 13 90 Dispose-n-m instances shown,IPC6, size n grid ranged 2 10, number objects, 110. T0 scales best n worst m.657fiPalacios & Geffnercorrespondence exists conformant plans P use actionsconformant plans P 0 use actions too.hand, conformant plan P 0 actions done many timesnecessarily represent conformant plan P . Indeed, non-deterministically movesagent right square grid n n, starting bottom left corner, n actionsrow would leave agent either top left corner bottom right corner P 0 ,anywhere Manhattan distance n origin P. divergence PP 0 , however, arise non-deterministic actions executed once.Building idea, non-deterministic conformant planner obtaineddeterministic conformant planner following way. non-deterministic problemP , let P1 problem P 0 above, additional constraint actionsP1 arising non-deterministic actions P executed once.easily achieved adding precondition enabled(a) true initiallysets false. Let P2 represent deterministic conformant problemnon-deterministic action P mapped 2 deterministic actions, executableonce, hidden fluents h1 , . . . , hm oneof (h1 , . . . , hm )expression initial situation. Similarly, let Pi deterministic problem resultsencoding non-deterministic action P deterministic copies.encoding, simple iterative conformant planner non-deterministic problems P defined terms conformant planner deterministic problemsinvoking latter upon P1 , P2 , P3 , on, solution reported. reportedsolution uses copy non-deterministic action once, thus encodessolution original problem.implemented strategy top T0 additional refinementtakes advantage nature KT,M translation, assumptions initialsituation maintained explicitly tags. Basically, non-deterministic actions Piallowed executed provided literals KL/hi dependparticular outcome actions (Si ) erased. implemented meansadditional reset(a) action Pi whose unconditional effect enabled(a) (i.e.,action done again) whose conditional effects KL KL/hiKL KL/hi = 1, . . . , m. Namely, literals KL/hi truth L dependsparticular non-deterministic outcome (Si ) erased, except L trueassumptions; i.e. KL true. non-deterministic actions executedplan provided occurrence a, except first one, precededreset(a) action.Table 12 compares resulting non-deterministic planner MBP KACMBPnumber non-deterministic problems considered MBP KACMBP papers.added additional domain, Slippery Gripper (sgripper), similarclassical Gripper number balls moved room B, exceptrobot cannot move B directly, non-deterministic move actionmove(A, C, D) moves robot either C D. typical plan movingtwo balls B pick A, move C D, move C B,B, finally dropping balls B.deterministic conformant planner (T0 ) used non-deterministic settingadded following modification: merges introduced precondition goal658fiCompiling Uncertainty Away Conformant Planning ProblemsProblemsgripper-10sgripper-20sgripper-30btuc-100btuc-150btuc-200btuc-250btuc-300bmtuc-10-10bmtuc-20-10bmtuc-20-20bmtuc-50-10bmtuc-50-50bmtuc-100-10bmtuc-100-50bmtuc-100-100nondet-ring-5nondet-ring-10nondet-ring-15nondet-ring-20nondet-ring-50nondet-ring-1key-5nondet-ring-1key-10nondet-ring-1key-15nondet-ring-1key-20nondet-ring-1key-25nondet-ring-1key-30T0time len1,44816,793901382,92009,23002340044,6 500826000,1200,1400,3400,91003,31004,920014,9 20030,2 20018,319> 2h> 2h> 2h> 2.1GBMBPtimelen> 2h> 2h> 2h> 2h65,929> 2h> 2h0182,1381298,9 58> 2h0,13311,21225164,4 87> 2.1GBKACMBPtimelen0,6685,414823,322822007,930016,940033,250062,16000,2200,6402,2403,61002722,4 10025,1200> 2h> 2h0,1320,51122,42427,3422603,1 25520,242419733,7375246,5 11041417,5 2043> 2hTable 12: Non-deterministic problems. problems except sgripper MBPKACMBP. problems modified render simple translationPDDL; particular, complex preconditions moved conditions. Timesreported seconds rounded closest decimal. means time memorysmaller instances.literals literals. reason setting pays remove uncertaintyliterals reset mechanism used. Indeed, provided simple changereset mechanism, none problems move beyond P1 (a single copynon-deterministic action) even domains non-deterministic actionsrequired many times plans (e.g., 2 balls room A).seen table, T0 better MBP collection nondeterministic domains, although well KACMBP, particular, NonDetRing Non-Det-Ring-1Key domains. case, results obtained T0domains quite meaningful. cases T0 failed solved problem, reasonclassical planner (FF) got lost search plans, something mayimprove advances classical planning technology.659fiPalacios & Geffner11. Related Workrecent conformant planners CFF, POND, MBP cast conformant planningheuristic search problem belief space (Bonet & Geffner, 2000). Compact beliefrepresentations informed heuristic functions, however, critical making approach work. effective belief representation, planners use SAT OBDDstechniques intractable worst case often exhibit good behavior average.heuristics, hand, use fixed cardinality heuristics count numberstates possible given belief state (a tractable operation OBDD representations) heuristics obtained relaxed planning graph suitably extended takeuncertain information account. heuristics appear work well domainsothers. perspective, translation-based approach provides handletwo problems: belief states P become plain states translation KT,M (P ),solved using classical heuristics. also established conditionsbelief representation compact complete.sound incomplete approach planning incomplete information advancedPetrick Bacchus (2002) represent belief states formulas. order makebelief updates efficient though, several approximations introduced, particular,existing disjunctions carried one belief next, new disjunctionsadded. imposes limitation type problems handled.two limitations approach domains must crafted hand,control information derived domains search plans blind.approach understood providing solution two problems too:one hand, move knowledge-level done automatically, other,problem lifted knowledge-level solved classical planners able searchcontrol information derived automatically new representation.third thread work related approach arises so-called 0-approximationsemantics (Baral & Son, 1997). 0-approximation semantics, belief states b represented sets states single 3-valued state fluents true, false,unknown. Proposition 3 above, correspondence established plansP conformant according 0-approximation semantics classical planstranslation K0 (P ), turns instance general translationKi (P ) complete problems width = 0. semantics translationK0 thus related 0-approximation semantics, yet K0 translation delivers something more: computational method obtaining conformant plans comply0-approximation semantics using classical planner.0-approximation basic K0 translation weak dealingexisting benchmarks. translations Ki extend K0 problems higher widthreplacing set fluents KL fluents KL/t tags encode assumptionsinitial situation. extensions 0-approximation semantics contextconformant planning taken different form: switching single 3-valued staterepresenting beliefs sets 3-valued states, 3-valued state progressed efficientlyindependently others (Son, Tu, Gelfond, & Morales, 2005). initial set3-valued states obtained forcing states assign boolean truth-value (true false)number fluents. Crucial approach work number fluents;660fiCompiling Uncertainty Away Conformant Planning Problemsbelief representation update exponential it. conditions ensurecompleteness extension 0-approximation semantics expressed termsrelevance analysis similar one underlying analysis width (Son & Tu,2006): fluents must set true false initial 3-valued stateappearing clause CI (L) precondition goal literal L. particular,initial situation n tautologies pi pi , relevant precondition goal literalL, number initial 3-valued states required completeness exponential n,make fluent pi true false. difference approach seentautologies pi pi relevant unique precondition goal literal Li .case, number 3-valued partial states required completeness remainsexponential n, resulting problem width 1 thus solvedK1 translation involves tags single literal. words, tags usedtranslation scheme encode local contexts required different literalsproblem, initial 3-valued states (Son & Tu, 2006) encode possible combinationsform global contexts. global contexts correspond consistent combinationslocal contexts, may thus exponential number even problembounded width. planners CpA(H) CpA(C), discussed contextConformant Track recent 2008 Int. Planning Competition (IPC6), buildapproach, reduce number partial initial states required using techniquereplace many one-of expressions single one (Tran et al., 2008, 2009); simplificationrelated notion critical initial states discussed Section 7.Another difference 3-valued approach (Son et al., 2005; Son & Tu, 2006),translation approach addresses representation beliefs alsocomputation conformant plans: conformant problem P translated problemKT,M (P ), solved classical planner. approaches definedtop 0-approximation semantics, like knowledge-level approach planningincomplete information Petrick Bacchus (2002), need way guide searchplans simplified belief space. search Petrick Bacchus (2002) blind(iterative deepening), search Son et al. (2005), Son Tu (2006) guidedcombination simple heuristics cardinality subgoal counting.12. Summarypractical problems purely conformant, ability find conformant plansneeded contingent settings conformant situations special case. paper,introduced new approach conformant planning conformant problems Pconverted classical planning problems KT,M (P ) solved classicalplanner. also studied conditions general translation soundcomplete. translation depends two parameters: set tags, referring localcontexts initial situation, set merges stand valid disjunctionstags. seen different translations, KS0 Kmodels, obtainedsuitable choices tags merges, introduced measure complexityconformant planning called conformant width, translation scheme Ki polynomial fixed complete problems width bounded i. also shownconformant benchmarks width 1, developed conformant planner T0661fiPalacios & Geffnerbased translations, shown planner exhibits good performancecomparison existing conformant planners. Recently, explored useideas general setting contingent planning (Albore, Palacios, & Geffner,2009).Acknowledgmentsthank Alex Albore help syntax MBP KACMBP, Pierre Marquiskindly answering question complexity deductive task. also thankanonymous reviewers useful comments. H. Geffner partially supported grantTIN2006-15387-C03-03.Appendix A. ProofsP stands conformant planning problem P = hF, I, O, Gi KT,M (P ) =hF 0 , 0 , O0 , G0 translation. Propositions theorems body paperappear appendix numbers; new lemmas propositionsnumbers preceded letters B (for Appendix B). conformant problemP classical problems P/s KT,M (P ) arise P assumedconsistent. Consistency issues important, addressed detailsecond part appendix shown P consistent, KT,M (P ) consistent(Appendix B). consistent classical problem P 0 , standard progression lemmaapplies; namely, literal L achieved applicable action sequence +1 = , a,action sequence action iff A) achieves C rule : C L P 0 ,B) achieves L negation L0 literal L0 body C 0 rule P 0form : C 0 L (see Theorem B.2 below).Lemma A.1. Let action sequence applicable P K0 (P ).achieves KL K0 (P ), achieves L P .Proof. induction length . empty achieves KL K0 (P ),KL must 0 , hence L must I, achieves L P .Likewise, +1 = , achieves KL K0 (P ) A) rule : KC KLK0 (P ), achieves KC K0 (P ); B) achieves KL K0 (P )rule : KC 0 KL K0 (P ), achieves KL0 K0 (P ) L0 C 0 .A) true, P must contain rule : C L, inductive hypothesis,must achieve C P , therefore, +1 = , must achieve L P . B) true,inductive hypothesis, must achieve L P along L0 literal L0 bodyC 0 rule : C 0 L, thus +1 = , must achieve L P too.Lemma A.2. action sequence applicable K0 (P ), applicable P .Proof. empty, trivial. Likewise, +1 = , applicable K0 (P ),applicable K0 (P ), thus inductive hypothesis, applicable P . Also since,, applicable K0 (P ), must achieve literals KL K0 (P ) preconditionL a, Lemma A.1, must achieve literals L preconditionsP , thus, sequence +1 = , applicable P .662fiCompiling Uncertainty Away Conformant Planning ProblemsProposition 2 classical plan K0 (P ), conformant plan P .Proof. Direct Lemma A.2 consider problem P 0 similar P newdummy action aG whose preconditions goals G P . plan K0 (P ),, aG applicable K0 (P 0 ), Lemma A.2, , aG applicable P 0 , impliesapplicable P achieves G, thus, plan P .Proposition 3 action sequence classical plan K0 (P ) iff conformantplan P according 0-approximation semantics.Proof. Let us say action sequence = a0 , . . . , 0-applicable P 0-achievesliteral L P belief sequence b0 , . . . , bn+1 generated according 0-approximationsemantics preconditions actions ai true bi , goalstrue bn+1 respectively. definition 0-approximation semantics (andconsistency P ), applicable action sequence thus 0-achieves literal L P iffempty L I, = 0 , A) : C L effect P 0 0-achievesliteral L0 C, B) 0 0-achieves L effects : C 0 L P , 0 0-achieves L0L0 C 0 . These, however, conditions achieves literal KLK0 (P ) sequence 0-achieving literal L P replaced sequence achievingliteral KL K0 (P ). Thus, action sequence applicable K0 (P )0-applicable P achieves literal KL K0 (P ) iff 0-achieves literal L P ,applicable K0 (P ) iff 0-applicable P , last part following firstusing induction plan length.Definition A.3. action P , define action sequencefollowed merges KT,M (P ) arbitrary order. Similarly, = a0 , . . . , aiaction sequence P , define action sequence = a0 , . . . , KT,M (P ).Lemma A.4. Let action sequence applicable P applicablevalid translation KT,M (P ). achieves KL/t KT,M (P ), achieves LP/s possible initial states satisfy t.Proof. empty , achieves KL/t, definition KT,M (P ) since|= L, L must s, thus must achieve L P/s.Likewise, +1 = , empty tag, +1= , achieves KL/tKT,M (P ) iff A) achieves KC/t KT,M (P ) rule : KC/t KL/t KT,M (P ),B) achieves KL/t, rule : KC 0 /t KL/t, achieves KL0 /tKT,M (P ) L0 C 0 (merge actions delete positive literals KL/t).A, inductive hypothesis, achieves C P/s possible initial statesatisfies t, hence +1 = , achieves L P/s rule : C L mustP . B, inductive hypothesis, achieves L L0 P/s, L0 bodyrule : C 0 L P , thus +1 = , achieves L P/s.Vempty tag = , third case must considered: merge action t0 KL/t0= , achieving KL KKL may cause action sequence +1T,M (P ).case, sequence , a, hence , , must achieve KL/t0 (nonempty) t0 KT,M (P ), hence inductive hypothesis two casesabove, sequence , must achieve L P/s possible initial state satisfies663fiPalacios & Geffnert0 . Yet, since merge valid, possible initial states must satisfy onet0 , thus must achieve L P/s possible initial states s, initialstates satisfy = .Lemma A.5. applicable valid translation KT,M (P ), applicable P .= , applicable KProof. empty, direct. +1 = , a, +1T,M (P ),applicable KT,M (P ), achieving KL precondition L a, henceinductive hypothesis, applicable P , Lemma A.4, must achieveL precondition L a, thus +1 = , applicable P .Theorem 7 translation KT,M (P ) sound provided merges validtags consistent.Proof. Consider problem P 0 similar P new dummy action aGwhose preconditions goals G P . plan KT,M (P ) iff1 , aG applicable KT,M (P 0 ), Lemma A.5 implies , aG applicableP 0 , means plan P .Lemma A.6. Let action sequence applicable P applicableKS0 (P ). achieves L P/s possible initial state s, achieves KL/sKS0 (P ).Proof. empty achieves L P/s, L s, since |= L, KL/s must0 thus achieves KL/s KS0 (P ).Likewise, +1 = , achieves L P/s A) rule : C Lachieves C P/s; B) achieves L rule : C 0 L, achieves L0KS0 (P ) L0 C 0 .A), inductive hypothesis, achieves KC/s KS0 (P ) and, rule : KC/s= , achieves KL/s (mergesKL/s, , must achieve KL/s, thus, +1delete positive literals KL/t).B), inductive hypothesis, achieves KL/s KL0 /s KS0 (P ) L0body rule : C 0 L P , therefore , achieves KL/s,= , .+1Lemma A.7. applicable P , applicable KS0 (P ).Proof. empty, trivial. +1 = , applicable P , mustapplicable P must achieve precondition L P/s every possible initialstate s, S0 . inductive hypothesis, must applicable KS0 (P ),Lemma A.6, Vmust achieve literals KL/s S0 , then, lastmerge action effect sS0 KL/s KL must achieve KL, ,therefore, , applicable KS0 (P ).Theorem 9 conformant plan P , classical plan 0 KS0 (P )result dropping merge actions 0 .664fiCompiling Uncertainty Away Conformant Planning ProblemsProof. Direct Lemma A.7 consider problem P 0 similar P newaction aG whose preconditions goals G P . plan P , sequence , aGapplicable P 0 , Lemma A.7, , aG applicable KS0 (P 0 ), thusplan KS0 (P ).Definition A.8. rel(s, L) stands set literals L0 relevant L P :rel(s, L) = {L0 | L0 L0 relevant L} .Definition A.9. stands deductive closure I:= { L | I, |= L} .Theorem A.10. Let = {t1 , . . . , tn } covering merge literal L valid translation KT,M (P ) problem P whose initial situation prime implicate form.tag ti must possible initial state P rel(s, L) ti .Proof. Assume otherwise state satisfying makes true literal Ls relevantL Ls 6 ti . take c disjunction literals Lsstates satisfy I, obtain entails c, since prime implicate form,means c contains tautology c0 subsumed clause c00 I. But, either case,contradiction, literals c0 c00 relevant L, hence ti , tipart covering merge m, must contain literal either c0 c00 , hence c.Lemma A.11. Let action sequence applicable P applicable covering translation KT,M (P ). Then, achieves L P/s possibleinitial state tag rel(s, L) , achieves KL/tKT,M (P ).Proof. empty achieves L P/s, L thus, rel(s, L). Sincerel(s, L) , L , thus KL/t initial situation 0 KT,M (P ),achieves KL/t KT,M (P ). Likewise, +1 = , achieves L P/s, A)rule : C L P achieves C P/s, B) achieves L P/srule : C 0 L, achieves L0 P/s L0 C 0 . A, inductivehypothesis, achieves KC/t, support rule : KC/t KL/t KT,M (P ),= , , merges cannot, must achieve KL/t KT,M (P ), must +1delete positive literal KL/t. B, inductive hypothesis, achieves KL/t,cancellation rule : KC 0 /t KL/t arising rule : C 0 L P , must= , ,achieve KL0 /t literal L0 C 0 . means , a, therefore, +1must achieve KL/t.Lemma A.12. Let KT,M (P ) covering translation P . applicable P ,applicable KT,M (P ).Proof. empty, direct. Else, +1 = , applicable P , mustapplicable P must achieve literal L P re(a), therefore, inductivehypothesis must applicable KT,M (P ). Then, let = {t1 , . . . , tn } coveringmerge L P re(a) KT,M (P ). Theorem A.10, ti must665fiPalacios & Geffnerpossible initial state rel(s, L) ti , Lemma A.11, achieving LP/s implies achieving KL/ti KT,M (P ). Since true ti achievesL P re(a) P/s possible initial states s, follows achieves KL/titi KT,M (P ), therefore achieves KL KT,MV(P ) endssequence merges include action merge am,L effect ti KL/ti KL.= , applicable Kresult, +1T,M (P ).Theorem 15 Covering translations KT,M (P ) complete; i.e., conformant planP , classical plan 0 KT,M (P ) 0 merge actionsremoved.Proof. theorem follows trivially Lemma A.12 problem P 0 likeP additional, dummy action aG goals G P preconditionsaG . action sequence plan P iff action sequence , aG applicable P 0 ,due Lemma A.12 implies action sequence , aG applicable KT,M (P 0 )turn true iff action sequence plan KT,M (P ). sequence ,turn, sequence merge actions removed.Theorem 17 translation Kmodels(P ) sound complete.Proof. Direct merges generated Kmodels precondition goalliterals L. Clearly merges valid, tags consistent I,cover L (the models CI (L) satisfy CI (L)). Thus result follows Theorems 715.Proposition 21 width w(P ) P determined time exponentialw(P ).Proof. number clauses CI (L), mi sets clauses CCI (L) |C| = i. clause one set must n literals, nnumber fluents P , hence, one literal clause C collected, endni sets literals size greater i, inconsistentconsistent minimal (no consistent set collectionproperly included); tests polynomial given prime implicate form.Thus constructing cover c(C) set clauses C |C| = exponential i,checking whether one cover satisfies CI (L) polynomial operation providedprime implicate form. Indeed, c(C) = {t1 , . . . , tn }, computing closuresti ti c(C), PI, testing whether ti intersects clauseCI (L) polynomial operations (the former reducing checking literal L0whether |= ti L0 ). Thus computing width(L), generate sets C clausesCI (L) |C| = i, starting = 0, increasing one one one set,c(C) satisfies CI (L). computation exponential w(L), computationpreconditions goal literals P exponential w(P ).Proposition 22 width P 0 w(P ) n, n numberfluents whose value initial situation known.666fiCompiling Uncertainty Away Conformant Planning ProblemsProof. inequality 0 w(P ) direct w(L) defined size |C| minimal setclauses C CI (L) c(C) satisfies CI (L), w(P ) = w(L) preconditiongoal literal L. inequality w(P ) n follows noticing set C clausesgiven tautologies L0 L0 CI (L), c(C) must satisfy clause c CI (L),c(C) must assign truth value literal c, inconsistent c,inconsistent thus pruned c(C). Finally, max number tautologiesCI (L) number fluents L0 neither L0 L0 unit clauses I.Theorem 24 fixed i, translation Ki (P ) sound, polynomial, w(P ) i,covering complete.Proof. soundness, need prove merges Ki (P ) validtags Ki (P ) consistent. soundness follows Theorem 7. mergesliteral L Ki (P ) given covers c(C) collections C less clausesCi (L) clearly sincemodel must satisfy CI (L), must satisfyWc(C) |= tm = c(C). time, definitioncover c(C), tags must consistent I.proving Ki polynomial fixed i, follow ideas similar ones usedproof Proposition 21 above, shown width Pdetermined time exponential w(P ) polynomial number clausesfluents P . fixed i, number sets clauses C CI (L) size |C|polynomial, complexity computing covers c(C) sets, hence,merges L Ki (P ) polynomial too. Thus, whole translation Ki (P ) fixedpolynomial number clauses, fluents, rules P .Finally, proving completeness, w(P ) i, w(L) preconditiongoal literal L P . Therefore, literal L, set C clauses CI (L)c(C) satisfies CI (L). translation Ki (P ) generate unique mergeL covers L. Since Ki (P ) valid translation, means Ki (P ) coveringtranslation, complete, virtue Theorem 15.Lemma A.13. L0 relevant L rel(s, L) rel(s0 , L), rel(s, L0 ) rel(s0 , L0 ).Proof. L00 rel(s, L0 ), L00 relevant L0 , since L0 relevant Lrelevance relation transitive, L00 relevant L. Thus, L00 rel(s, L) therefore,since rel(s, L) rel(s0 , L), L00 rel(s0 , L). L00 s0 since relevantL0 , L00 rel(s0 , L0 ).Proposition 26 Let s0 two states let action sequence applicableclassical problems P/s P/s0 . achieves literal L P/s0 rel(s0 , L)rel(s, L), achieves literal L P/s.Proof. induction length . empty, achieves literal L P/s0 , Lmust s0 , since L relevant itself, L rel(s0 , L). rel(s0 , L) rel(s, L),L must s, thus achieves L P/s.667fiPalacios & GeffnerLikewise, +1 = , achieves L P/s0 A) rule : C Lachieves C P/s0 ; B) achieves L P/s0 rule : C 0 L, achievesL0 P/s0 L0 C 0 .A, must achieve literal Li C P/s0 . Since Li relevant L rel(s0 , L)rel(s, L), Lemma A.13, rel(s0 , Li ) rel(s, Li ). Then, inductive hypothesis, planmust achieve Li P/s Li C, thus +1 = , must achieve L P/sB, since L0 relevant L (as L0 relevant L), rel(s0 , L)rel(s, L), Lemma A.13, rel(s0 , L0 ) rel(s, L0 ), thus inductive hypothesis,must achieve L0 P/s also L, +1 = , must achieve L P/s.Lemma A.14. 0 two collection states every stateevery precondition goal literal L P , state s0 0 rel(s0 , L)rel(s, L), applicable P/S 0 , applicable P/S.Proof. induction length . empty, obvious. +1 = , applicableP/S 0 , applicable P/S 0 and, inductive hypothesis, applicable P/S.need prove achieves preconditions action P/S.L P rec(a) S, hypothesis, state s0 0rel(s0 , L) rel(s, L). Proposition 26, since achieves L P/s0 , mustachieve L P/s. Since argument applies S, achieves L P/S, thus+1 = , must applicable P/S.Proposition 27 0 two collections states every stateevery precondition goal literal L P , state s0 0 rel(s0 , L)rel(s, L), plan P conforms 0 , plan P conformsS.Proof. Lemma A.14, consider problem P 0 similar P new actionaG whose preconditions goals G P . plan P conforms 0 ,action sequence , aG applicable P 0 /S 0 , lemma, , aGapplicable P 0 /S, thus must plan P/SProposition 28 0 basis P every possible initial state P everyprecondition goal literal L P , 0 contains state s0 rel(s0 , L) rel(s, L).Proof. Direct Proposition 27, considering set possible initial statesP .Proposition 29 initial situation prime implicate form = {t1 , . . . , tn }merge covers literal L P , set S[ti , L] possible initial states Prel(s, L) ti non-empty.Proof. Direct Theorem A.10.Theorem 30 Let KT,M (P ) covering translation let 0 stand collectionstates s[ti , L] L precondition goal literal P ti tag mergecovers L. 0 basis P .668fiCompiling Uncertainty Away Conformant Planning ProblemsProof. show every possible initial state precondition goal literalL, 0 theorem contains state s0 rel(s0 , L) rel(s, L). resultfollows Proposition 28. Indeed, state must satisfy tag ti coveringmerge = {t1 , . . . , tn } L, merges valid. Theorem A.10, mustpossible initial state s0 rel(s0 , L) ti , therefore, rel(s0 , L) rel(s, L)must satisfy ti possibly literals L0 relevant L.Theorem 31 P conformant planning problem bounded width, P admitsbasis polynomial size.Proof. w(P ) fixed i, Ki (P ) covering translation polynomial numbermerges tags, case, basis 0 P defined Theorem 30 containspolynomial number states, regardless number possible initial states.Appendix B. Consistencyassuming throughout paper conformant planning problems Ptranslations KT,M (P ) consistent. section make notion precise,explain needed, prove KT,M (P ) consistent P is. proof,take account heads KL merge actions am,L KT,M (P ), extendedliterals KL0 literals L0 mutex L P (see Definition 4).start beginning assuming states truth-assignments setsliterals fluents language. state complete every literal L, L Ls, consistent literal L L s. Complete consistentstates represent truth-assignments fluents F consistency Ptranslation KT,M (P ) ensures applicable action sequences map completeconsistent states complete consistent states s0 . guaranteed, completeconsistent states referred simply states donepaper.Given complete state action applicable s, next state sasa = (s \ Del(a, s)) Add(a, s)Add(a, s) = {L | : C L P C s}Del(a, s) = {L | L Add(a, s)} .follows sa complete state complete state, actiondeletes literal L L added s. hand, may consistentsa inconsistent, example, rules : C L : C 0 LC C 0 s. order exclude possibility, ensuring reachablestates complete consistent, thus represent genuine truth assignmentsfluents F , consistency condition P needed:Definition B.1 (Consistency). classical conformant problem P = hF, I, O, Gi consistent initial situation logically consistent every pair complementaryliterals L L mutex P .669fiPalacios & Geffnerconsistent classical problem P , reachable states complete consistent,standard progression lemma used preceding proofs holds:Theorem B.2 (Progression). action sequence +1 = , applicable completeconsistent state achieves literal L consistent classical problem P iff A) achievesbody C rule : C L P , B) achieves L every rule : C 0 L,achieves L0 literal L0 C 0 .see conformant problem P consistent sense,valid translation KT,M (P ). tested benchmarks considered paperconsistency found consistent except two domainsintroduced elsewhere: 1-Dispose Look-and-Grab. cases, since consistencyclassical problem KT,M (P ) cannot inferred consistency P ,checked explicitly using Definition B.1, similarly, plans obtainedKT,M (P ) checked consistency indicated Section 8: soundnessplans ensured provided never trigger conflicting effects KL/t KL/t.10Proof. proof Theorem B.2 rest particular definition mutexes,mutex atoms true reachable state. consistent problem P ,applicable action sequence maps complete consistent state s0 representstruth assignment. Then, action sequence +1 = , achieves L iff C) L Add(a, s0 )D) L s0 L 6 Del(a, s0 ). Condition theorem, however, equivalent C,Condition B theorem, equivalent D. Indeed, L 6 Del(a, s0 ) iff rule: C 0 L literal L0 C 0 L0 6 s0 , which, given s0 completeconsistent, true iff L0 s0 (this precisely consistency needed; else L0 s0would imply L0 6 s0 ).notion mutex used definition consistency expresses guaranteepair literals true reachable state. Sufficient polynomial conditionsmutual exclusivity type invariants defined various papers,follow definition Bonet Geffner (1999).Definition B.3 (Mutex Set). mutex set collection R unordered literals pairs(L, L0 ) classical conformant problem P that:1. pair (L, L0 ) R, L L0 possible initial state s,2. : C L : C 0 L0 two rules action (L, L0 ) pairR, P re(a) C C 0 mutex R,3. : C L rule P literal L pair (L, L0 ) R, either a) L0 = L,b) P re(a) C mutex L0 R, c) P re(a) C implies C 0 R rule: C 0 L0 P ;10. consistency two domains, 1-Dispose Look-and-Grab, established howeverdefinition mutexes slightly stronger one used. actually suffices changeexpression P re(a) C clause 3c) definition mutex sets P re(a) C {L0 }.670fiCompiling Uncertainty Away Conformant Planning Problemsdefinition, pair said mutex R belongs R, set literalssaid mutex R contains pair R, set literals said imply setliterals 0 R mutex R complement L literal L 0 \ S.easy verify R1 R2 mutex sets, union R1 R2 mutex set,thus maximal mutex set P denote R . pairs Rcalled mutexes.simplicity without loss generality, assume preconditions P re(a)empty. Indeed, simple show mutexes problem P remainpreconditions pushed conditions. also assume condition C ruleC L P mutex, rules simply pruned. addition, assumeliteral L mutex pair complementary literals L0 L0 , L cannottrue reachable state, thus, pruned well.definition mutexes sound, meaning pair mutex set truereachable state:Theorem B.4. (L, L0 ) pair mutex set R classical conformant problemP , reachable state P , {L, L0 } s.Proof. proceed inductively. Clearly, L L0 cannot part possible initial state,ruled definition mutex sets. Thus, let us assume inductive hypothesisL L0 part state reachable less steps, let us provetrue states s0 = sa reachable one step. ClearlyL L0 belong s0 , either A) L L0 belong Add(a, s), B) L belongsAdd(a, s) L0 belongs Del(a, s). show possible.A, P must comprise rules : C L : C 0 L0 C C 0 s, yetdefinition mutex sets, C C 0 must mutex, inductive hypothesisC C 0 6 s. B, must rule : C L C s, L0inductive hypothesis, follows L0 mutex C R, thus,mutex set definition, either L0 = L C implies C 0 rule : C 0 L0 .first case, however, due rule : C L C s, L0 Del(a, s),second case, completeness reachable states, must C 0 s, henceL0 Del(a, s), contradicting B cases.Provided initial situation conformant planning problem P primeimplicate form, computing largest mutex set R testing consistency Ppolynomial time operations. former, one starts set literal pairsiteratively drops set pairs comply definitionreaching fixed point (Bonet & Geffner, 1999).move prove conformant problem P consistent, validtranslation KT,M (P ). consistency classical problems P/s possible initialstates direct, set mutexes P subset set mutexes P/sinitial situation constrained.Proposition B.5 (Mutex Set RT ). valid translation KT,M (P ) consistent conformant problem P , define RT set (unordered) literals pairs (KL/t, KL0 /t0 )(KL/t, KL0 /t) (L, L0 ) mutex P , t0 two tags jointly satisfiable(I 6|= (t t0 )). RT mutex set KT,M (P ).671fiPalacios & Geffnerfollows KT,M (P ) consistent P consistent, L0 = Lmutex L P , (KL/t, KL/t) must mutex RT .Theorem B.6 (Consistency KT,M (P )). valid translation KT,M (P ) consistent Pconsistent.consistency translation K0 (P ) follows special case, K0 (P ) KT,M (P )empty set merges set tags containing empty tag.left prove Proposition B.5.Proof Proposition B.5. must show set RT comprised pairs (KL/t, KL0 /t0 )(KL/t, KL0 /t) L0 mutex L P , tags t0 jointly satisfiableI, set complies clauses 1, 2, 3 Definition B.3. go one clausetime.1. pair RT true initially KT,M (P ) = hF 0 , 0 , O0 , G0 jointly satisfiableI, t, t0 . Indeed, KL/t KL0 /t0 0 must possible initialstate satisfying t0 L L0 true contradiction L L0mutex P . Similarly, KL/t 0 KL0 /t not, must case|= L 6|= L0 , must possible initial state Pt, L, L0 hold, contradiction L L0 mutex P too.2. action rules KL/t KL0 /t0 rules must supportrules form : KC/t KL/t : KC 0 /t0 KL0 /t0 arising rules: C L : C 0 L0 P .11 since L L0 mutex P , C C 0must contain literals L1 C L2 C 0 (L1 , L2 ) mutex P ,hence (KL1 /t, KL2 /t0 ) belongs RT , KC/t KC 0 /t0 mutex RTwell.Similarly, action rules KL/t KL0 /t literal L0mutex L P , rules must support cancellation rules form: KC/t KL/t : KC 0 /t KL0 /t, arising rules : C L: C 0 L0 P . Since L L0 mutex P , C C 0 must contain literalsL1 C L2 C 0 mutex P , hence RT must contain pair(KL1 /t, KL2 /t), KC/t KC 0 /t must mutex RT .3. left show set RT given pairs (KL/t, KL0 /t0 ) (KL/t,KL0 /t) complies clause 3 definition mutex sets well. Considerfirst class pairs (KL/t, KL0 /t0 ) rule : KC/t KL/t KL/t arisingrule : C L P . Since L mutex L0 P , one conditions 3a,3b, 3c must hold rule : C L L0 . 3a, L0 = L, KC/tmust imply body KC/t0 cancellation rule : KC/t0 KL/t0 ,literal L1 C, RT must contain pair (KL1 /t, KL1 /t0 ) KL1 /timplies KL1 /t0 , KC/t implies KC/t0 (case 3c). 3b, C L011. action cannot merge literal L00 mutex L L0 , case, L00 impliesL L0 mutex. Similarly, cannot merge L case, L mutexL0 L0 . reason, cannot merge L0 either. Thus, actioncannot merge must action P .672fiCompiling Uncertainty Away Conformant Planning Problemsmutex P , thus C contains literal L1 mutex L0 P . meanspair (KL1 /t, KL0 /t0 ) RT hence KC/t mutex KL0 /t0 RT(case 3b). Last, 3c, C implies C 0 P rule : C 0 L0 , KC/t mustimply body KC 0 /t0 cancellation rule : KC 0 /t0 KL0 /t0 . Indeed,literal L1 C C 0 , KL1 /t implies KL1 /t0 ,L2 literal C 0 C, literal L3 C must mutexL2 P , hence pair (KL3 /t, KL2 /t0 ) must RT KL3 /t impliesKL2 /t0 (case 3c)0 0ConsiderV pair (KL/t, KL /t ) along merge action am,Lrule ti KL/ti KL KL/t = KL (thus empty tag). case, sincemerge valid t0 consistent, must ti tit0 jointly consistent I. follows (KL/ti , KL0 /t0 ) pair RTthus body merge mutex KL0 /t0 RT (case 3b).need consider pair (KL/t, KL0 /t0 ) along rules KL0 /t0 ,literals KL/t KL0 /t0 structure, thus argumentapplies, replacing t0 L L0 .switch second class pairs (KL/t, K/L0 /t) rules :KC/t KL/t KL/t. Since L L0 mutex P , conditions 3a, 3b,3c must hold. a, L0 = L, case, condition 3c holds KT,M (P )KC/t implies body KC/t rule : KC/t KL0 (L0 = L). b, Cmutex L0 , thus literal L1 C L1 L0 mutexP , therefore KC/t KL0 /t mutex RT (case 3b). Finally, c, Cimplies C 0 rule : C 0 L0 P , KC/t must imply KC 0 /t RTrule : KC 0 /t KL0 /t (case 3c).empty tag t, rule KL/t may also merge, due extraeffects KL0 merge action L, merge KL also merge KL0 ,case 3c holds.Last, class pairs, rules KL0 /t cancellation rulesform : KC 00 /t KL0 /t rule : C 00 L0 P . Since L0 mutexL P , conditions 3a, 3b, 3c must hold rule : C 00 L0 L0 P .a, L = L0 , cancellation rule : KC 00 /t KL (case 3c).b, C 00 mutex L, thus literal L2 C 00 (L2 , L) mutexP , therefore KL/t implies KL2 /t RT , hence KL2 /t KC 00 /timply KL/t RT (case 3b). Finally, c, C 00 implies C 0 rule : C 0 L P ,KC 00 /t must imply KC 0 /t rule : KC 0 /t KL/t RT .Indeed, LA implies LB P , LB implies LA P , KLB /t implies KLA /tRT , KLA /t implies KLB /t.ReferencesAlbore, A., Palacios, H., & Geffner, H. (2009). translation-based approach contingentplanning. Proc. 21st Int. Joint Conference AI (IJCAI-09), pp. 16231628.673fiPalacios & GeffnerBaral, C., Kreinovich, V., & Trejo, R. (2000). Computational complexity planningapproximate planning presence incompleteness. Artificial Intelligence, 122 (12), 241267.Baral, C., & Son, T. C. (1997). Approximate reasoning actions presence sensingincomplete information. Proc. ILPS 1997, pp. 387401.Bayardo Jr., R., & Schrag, R. (1997). Using CSP look-back techniques solve real-worldsat instances. Proc. AAAI, pp. 203208.Bertoli, P., & Cimatti, A. (2002). Improving heuristics planning search belief space.Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proc. AIPS-2002, pp. 143152.AAAI Press.Bonet, B., & Geffner, H. (1999). Planning heuristic search: New results. ProceedingsECP-99, pp. 359371. Springer.Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic searchbelief space. Proc. AIPS-2000, pp. 5261. AAAI Press.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (12), 533.Bonet, B., & Givan, B. (2006). Results conformant track 5th int. planningcompetition. http://www.ldc.usb.ve/bonet/ipc5/docs/results-conformant.pdf.Bryce, D., & Buffet, O. (2008). International planning competition uncertainty part: Benchmarks results. http://ippc-2008.loria.fr/wiki/images/0/03/Results.pdf.Bryce, D., Kambhampati, S., & Smith, D. E. (2006). Planning graph heuristics beliefspace search. Journal Artificial Intelligence Research, 26, 3599.Cimatti, A., Roveri, M., & Bertoli, P. (2004). Conformant planning via symbolic modelchecking heuristic search. Artificial Intelligence, 159, 127206.Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.Goldman, R. P., & Boddy, M. S. (1996). Expressive planning explicit knowledge.Proc. AIPS-1996, pp. 110117.Haslum, P., & Jonsson, P. (1999). results complexity planning incomplete information. Proc. ECP-99, Lect. Notes AI Vol 1809, pp. 308318.Springer.Hoffmann, J., & Brafman, R. (2005). Contingent planning via heuristic forward searchimplicit belief states. Proc. 15th Int. Conf. Automated Planning Scheduling(ICAPS 2005), pp. 7180. AAAI.Hoffmann, J., & Brafman, R. (2006). Conformant planning via heuristic forward search:new approach. Artificial Intelligence, 170 (6-7), 507541.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 253302.Marquis, P. (2000). Consequence finding algorithms. Gabbay, D., & Smets, P. (Eds.),Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 5, pp.41145. Kluwer.674fiCompiling Uncertainty Away Conformant Planning ProblemsPalacios, H., & Geffner, H. (2006). Compiling uncertainty away: Solving conformant planning problems using classical planner (sometimes). Proc. AAAI-06, pp. 900905.Palacios, H., & Geffner, H. (2007). conformant classical planning: Efficient translations may complete too. Proc. ICAPS-07, pp. 264271.Petrick, R., & Bacchus, F. (2002). knowledge-based approach planning incompleteinformation sensing. Proc. AIPS-02, pp. 212221.Rintanen, J. (2004). Complexity planning partial observability. Proc. ICAPS2004, pp. 345354.Smith, D., & Weld, D. (1998). Conformant graphplan. Proceedings AAAI-98, pp. 889896. AAAI Press.Son, T. C., Tu, P. H., Gelfond, M., & Morales, A. (2005). Conformant planning domainsconstraints: new approach. Proc. AAAI-05, pp. 12111216.Son, T. C., & Tu, P. H. (2006). completeness approximation based reasoningplanning action theories incomplete information.. Proc. 10th Int. Conf.Principles KR Reasoning (KR-06), pp. 481491.Tison, P. (1967). Generalized consensus theory applications minimizationboolean circuits. IEEE Transactions Computers, EC-16 (4), 446456.Tran, D., Nguyen, H., Pontelli, E., & Son, T. C. (2008). CPA(C)/(H): Two approximationbased conformant planners. http://ippc-2008.loria.fr/wiki/images/5/57/Team2CPA.pdf.Tran, D., Nguyen, H., Pontelli, E., & Son, T. C. (2009). Improving performance conformant planners: Static analysis declarative planning domain specifications. Practical Aspects Declarative Languages, 11th International Symposium, PADL 2009,Proceedings, Vol. 5418 Lecture Notes Computer Science, pp. 239253. Springer.Turner, H. (2002). Polynomial-length planning spans polynomial hierarchy. JELIA02: Proc. European Conference Logics AI, pp. 111124. Springer-Verlag.675fiJournal Artificial Intelligence Research 35 (2009) 449-484Submitted 1/09; published 7/09Efficient Markov Network Structure DiscoveryUsing Independence TestsFacundo Brombergfbromberg@frm.utn.edu.arDepartamento de Sistemas de Informacion,Universidad Tecnologica Nacional,Mendoza, ArgentinaDimitris MargaritisVasant Honavardmarg@cs.iastate.eduhonavar@cs.iastate.eduDept. Computer Science,Iowa State University,Ames, IA 50011Abstractpresent two algorithms learning structure Markov network data:GSMN GSIMN. algorithms use statistical independence tests infer structure successively constraining set structures consistent resultstests. recently, algorithms structure learning based maximum likelihood estimation, proved NP-hard Markov networks duedifficulty estimating parameters network, needed computationdata likelihood. independence-based approach require computationlikelihood, thus GSMN GSIMN compute structure efficiently(as shown experiments). GSMN adaptation Grow-Shrink algorithmMargaritis Thrun learning structure Bayesian networks. GSIMN extends GSMN additionally exploiting Pearls well-known properties conditionalindependence relation infer novel independences known ones, thus avoiding performance statistical tests estimate them. accomplish efficiently GSIMN usesTriangle theorem, also introduced work, simplified version setMarkov axioms. Experimental comparisons artificial real-world data sets showGSIMN yield significant savings respect GSMN , generating Markovnetwork comparable cases improved quality. also compare GSIMNforward-chaining implementation, called GSIMN-FCH, produces possible conditional independences resulting repeatedly applying Pearls theorems knownconditional independence tests. results comparison show GSIMN,sole use Triangle theorem, nearly optimal terms set independencestests infers.1. IntroductionGraphical models (Bayesian Markov networks) important subclass statistical models possess advantages include clear semantics sound widelyaccepted theoretical foundation (probability theory). Graphical models usedrepresent efficiently joint probability distribution domain. usednumerous application domains, ranging discovering gene expression pathwaysbioinformatics (Friedman, Linial, Nachman, & Peer, 2000) computer vision (e.g. Gemanc2009AI Access Foundation. rights reserved.fiBromberg, Margaritis, & HonavarFigure 1: Example Markov network. nodes represent variables domain V ={0, 1, 2, 3, 4, 5, 6, 7}.& Geman, 1984, Besag, York, & Mollie, 1991, Isard, 2003, Anguelov, Taskar, Chatalbashev,Koller, Gupta, Heitz, & Ng, 2005). One problem naturally arises constructionmodels data (Heckerman, Geiger, & Chickering, 1995, Buntine, 1994). solutionproblem, besides theoretically interesting itself, also holds potentialadvancing state-of-the-art application domains models used.paper focus task learning Markov networks (MNs) datadomains variables either discrete continuous distributed accordingmultidimensional Gaussian distribution. MNs graphical models consist twoparts: undirected graph (the model structure), set parameters. exampleMarkov network shown Figure 1. Learning models data consists two interdependent tasks: learning structure network, and, given learned structure,learning parameters. work focus problem learning structureMN domain data.present two algorithms MN structure learning data: GSMN (Grow-ShrinkMarkov Network learning algorithm) GSIMN (Grow-Shrink Inference-based MarkovNetwork learning algorithm). GSMN algorithm adaptation Markov networksGS algorithm Margaritis Thrun (2000), originally developed learningstructure Bayesian networks. GSMN works first learning local neighborhoodvariable domain (also called Markov blanket variable),using information subsequent steps improve efficiency. Although interestinguseful itself, use GSMN point reference performance regardtime complexity accuracy achieved GSIMN, main result work.GSIMN algorithm extends GSMN using Pearls theorems propertiesconditional independence relation (Pearl, 1988) infer additional independencesset independences resulting statistical tests previous inferences, thus avoidingexecution tests data. allows savings execution time and, datadistributed, communication bandwidth.rest paper organized follows: next section present previousresearch related problem. Section 3 introduces notation, definitions presentsintuition behind two algorithms. Section 4 contains main algorithms, GSMNGSIMN, well concepts practical details related operation. evaluateGSMN GSIMN present results Section 5, followed summary450fiEfficient Markov Network Structure Discovery Using Independence Testswork possible directions future research Section 6. Appendices B containproofs correctness GSMN GSIMN.2. Related WorkMarkov networks used physics computer vision communities (Geman &Geman, 1984, Besag et al., 1991, Anguelov et al., 2005) historicallycalled Markov random fields. Recently interest use spatialdata mining, applications geography, transportation, agriculture, climatology,ecology others (Shekhar, Zhang, Huang, & Vatsavai, 2004).One broad popular class algorithms learning structure graphical modelsscore-based approach, exemplified Markov networks Della Pietra, Della Pietra,Lafferty (1997), McCallum (2003). Score-based approaches conduct searchspace legal structures attempt discover model structure maximum score.Due intractable size search space i.e., space legal graphs,super-exponential size, score-based algorithms must usually resort heuristic search.step structure search, probabilistic inference step necessary evaluatescore (e.g., maximum likelihood, minimum description length, Lam & Bacchus, 1994,pseudo-likelihood, Besag, 1974). Bayesian networks inference step tractabletherefore several practical score-based algorithms structure learning developed(Lam & Bacchus, 1994, Heckerman, 1995, Acid & de Campos, 2003). Markov networkshowever, probabilistic inference requires calculation normalizing constant (alsoknown partition function), problem known NP-hard (Jerrum & Sinclair, 1993,Barahona, 1982). number approaches considered restricted class graphicalmodels (e.g. Chow & Liu, 1968, Rebane & Pearl, 1989, Srebro & Karger, 2001). However,Srebro Karger (2001) prove finding maximum likelihood network NP-hardMarkov networks tree-width greater 1.work area structure learning undirected graphical models concentrated learning decomposable (also called chordal) MNs (Srebro & Karger,2001). example learning non-decomposable MNs presented work Hofmann Tresp (1998), approach learning structure continuous domainsnon-linear relationships among domain attributes. algorithm removes edgesgreedily based leave-one-out cross validation log-likelihood score. non-score basedapproach work Abbeel, Koller, Ng (2006), introduces new class efficient algorithms structure parameter learning factor graphs, class graphicalmodels subsumes Markov Bayesian networks. approach based newparameterization Gibbs distribution potential functions forcedprobability distributions, supported generalization Hammersley-Cliffordtheorem factor graphs. promising theoretically sound approach maylead future practical efficient algorithms undirected structure learning.work present algorithms belong independence-based constraintbased approach (Spirtes, Glymour, & Scheines, 2000). Independence-based algorithms exploit fact graphical model implies set independences exist distribution domain, therefore data set provided input algorithm (underassumptions, see next section); work conducting set conditional independence451fiBromberg, Margaritis, & Honavartests data, successively restricting number possible structures consistentresults tests singleton (if possible), inferring structurepossible one. desirable characteristic independence-based approaches factrequire use probabilistic inference discovery structure.Also, algorithms amenable proofs correctness (under assumptions).Bayesian networks, independence-based approach mainly exemplifiedSGS (Spirtes et al., 2000), PC (Spirtes et al., 2000), algorithms learnMarkov blanket step learning Bayesian network structure Grow-Shrink(GS) algorithm (Margaritis & Thrun, 2000), IAMB variants (Tsamardinos, Aliferis,& Statnikov, 2003a), HITON-PC HITON-MB (Aliferis, Tsamardinos, & Statnikov,2003), MMPC MMMB (Tsamardinos, Aliferis, & Statnikov, 2003b), max-min hillclimbing (MMHC) (Tsamardinos, Brown, & Aliferis, 2006), widely usedfield. Algorithms restricted classes trees (Chow & Liu, 1968) polytrees(Rebane & Pearl, 1989) also exist.learning Markov networks previous work mainly focused learning Gaussiangraphical models, assumption continuous multivariate Gaussian distributionmade; results linear dependences among variables Gaussian noise (Whittaker, 1990, Edwards, 2000). recent approaches included works Dobra,Hans, Jones, Nevins, Yao, West (2004), (Castelo & Roverato, 2006), Pena (2008),Schafer Strimmer (2005), focus applications Gaussian graphical modelsBioinformatics. make assumption continuous Gaussian variablespaper, algorithms present applicable domains useappropriate conditional independence test (such partial correlation). GSMNGSIMN algorithms presented apply case arbitrary faithful distribution assumed probabilistic conditional independence test distributionavailable. algorithms first introduced Bromberg, Margaritis, Honavar(2006); contributions present paper include extending results conductingextensive evaluation experimental theoretical properties. specifically,contributions include extensive systematic experimental evaluation proposed algorithms (a) data sets sampled artificially generated networks varyingcomplexity strength dependences, well (b) data sets sampled networksrepresenting real-world domains, (c) formal proofs correctness guaranteeproposed algorithms compute correct Markov network structure domain,stated assumptions.3. Notation Preliminariesdenote random variables capitals (e.g., X, Y, Z) sets variables boldcapitals (e.g., X, Y, Z). particular, denote V = {0, . . . , n 1} set nvariables domain. name variables indices V; instance,refer third variable V simply 3. denote data set size(number data points) |D| N . use notation (XY | Z) denoteproposition X independent conditioned Z, disjoint sets variables X,Y, Z. (X 6Y | Z) denotes conditional dependence. use (XY | Z) shorthand({X}{Y } | Z) improve readability.452fiEfficient Markov Network Structure Discovery Using Independence TestsMarkov network undirected graphical model represents joint probabilitydistribution V. node graph represents one random variablesdomain, absences edges encode conditional independences among them.assume underlying probability distribution graph-isomorph (Pearl, 1988) faithful(Spirtes et al., 2000), means faithful undirected graph. graph Gsaid faithful distribution graph connectivity represents exactlydependencies independences existent distribution. detail, meansdisjoint sets X, Y, Z V, X independent given Z setvertices Z separates set vertices X set vertices graph G (thissometimes called global Markov property, Lauritzen, 1996). words, meansthat, removing vertices Z G (including edges incident them),exists (undirected) path remaining graph variable Xvariable Y. example, Figure 1, set variables {0, 5} separates set {4, 6}set {2}. generally, shown (Pearl, 1988; Theorem 2, page 94 definitiongraph isomorphism, page 93) necessary sufficient condition distributiongraph-isomorph set independence relations satisfy following axiomsdisjoint sets variables X, Y, Z, W individual variable :(Symmetry)(Decomposition)(Intersection)(Strong Union)(Transitivity)(XY | Z)(XY W | Z)(XY | Z W)(XW | Z Y)(XY | Z)(XY | Z)(YX | Z)(XY | Z) (XW | Z)===(XY W | Z)(XY | Z W)(X | Z) (Y | Z)(1)operation algorithms also assume existence oracleanswer statistical independence queries. standard assumptions neededformally proving correctness independence-based structure learning algorithms(Spirtes et al., 2000).3.1 Independence-Based Approach Structure LearningGSMN GSIMN independence-based algorithms learning structureMarkov network domain. approach works evaluating number statisticalindependence statements, reducing set structures consistent resultstests singleton (if possible), inferring structure possible one.mentioned above, theory assume existence independence-query oracleprovide information conditional independences among domain variables.viewed instance statistical query oracle (Kearns & Vazirani, 1994).practice oracle exist; however, implemented approximatelystatistical test evaluated data set D. example, discrete dataPearsons conditional independence chi-square (2 ) test (Agresti, 2002), mutualinformation test etc. continuous Gaussian data statistical test usedmeasure conditional independence partial correlation (Spirtes et al., 2000). determineconditional independence two variables X given set Z data,453fiBromberg, Margaritis, & Honavarstatistical test returns p-value. p-value test equals probability obtainingvalue test statistic least extreme one actually observedgiven null hypothesis true, corresponds conditional independencecase. Assuming p-value test p(X, | Z), statistical test concludesdependence p(X, | Z) less equal threshold i.e.,(X 6Y | Z) p(X, | Z) .quantity 1 sometimes referred tests confidence threshold. usestandard value = 0.05 experiments, corresponds confidencethreshold 95%.faithful domain, shown (Pearl & Paz, 1985) edge existstwo variables X 6= V Markov network domaindependent conditioned remaining variables domain, i.e.,(X, ) edge iff (X 6Y | V {X, }).Thus, learn structure, theoretically suffices perform n(n 1)/2 tests i.e.,one test (X, | V {X, }) pair variables X, V, X 6= . Unfortunately,non-trivial domains usually involves test conditions large numbervariables. Large conditioning sets produce sparse contingency tables (count histograms)result unreliable tests. number possible configurationsvariables grows exponentially size conditioning setfor example,2n cells test involving n binary variables, fill table one data pointper cell would need data set least exponential size i.e., N 2n . Exacerbatingproblem, one data point per cell typically necessary reliable test:recommended Cochran (1954), 20% cells contingency tableless 5 data points test deemed unreliable. Therefore GSMNGSIMN algorithms (presented below) attempt minimize conditioning set size;choosing order examining variables irrelevant variablesexamined last.4. Algorithms Related Conceptssection present main algorithms, GSMN GSIMN, supporting concepts required description. purpose aiding understandingreader, discussing first describe abstract GSMN algorithm nextsection. helps showing intuition behind algorithms laying foundationthem.4.1 Abstract GSMN Algorithmsake clarity exposition, discussing first algorithm GSMN ,describe intuition behind describing general structure using abstract GSMNalgorithm deliberately leaves number details unspecified; filled-inconcrete GSMN algorithm, presented next section. Note choices454fiEfficient Markov Network Structure Discovery Using Independence TestsAlgorithm 1 GSMN algorithm outline: G = GSMN (V, D).1: Initialize G empty graph.2: variables X domain V3:/* Learn Markov Blanket BX X using GS algorithm. */4:BX GS (X, V, D)5:Add undirected edge G X variable BX .6: return GAlgorithm 2 GS algorithm. Returns Markov Blanket BX variable X V: BX =GS (X, V, D).1:2:3:4:5:6:7:8:9:10:11:12:BX/* Grow phase. */variable V {X}(X 6Y | BX ) (estimated using data D)BX BX {Y }goto 3 /* Restart grow loop. *//* Shrink phase. */variable BX(XY | BX {Y }) (estimated using data D)BX BX {Y }goto 8 /* Restart shrink loop. */return BXdetails source optimizations reduce algorithms computational cost.make explicit discuss concrete GSMN GSIMN algorithms.abstract GSMN algorithm shown Algorithm 1. Given input data setset variables V, GSMN computes set nodes (variables) BX adjacentvariable X V; completely determine structure domain MN.algorithm consists main loop learns Markov blanket BX node(variable) X domain using GS algorithm. constructs Markov networkstructure connecting X variable BX .GS algorithm first proposed Margaritis Thrun (2000) shownAlgorithm 2. consists two phases, grow phase shrink phase. grow phaseX proceeds attempting add variable current set hypothesizedneighbors X, contained BX , initially empty. BX grows variableiteration grow loop X found dependent Xgiven current set hypothesized neighbors BX . Due (unspecified) orderingvariables examined (this explicitly specified concrete GSMN algorithm,presented next section), end grow phase variables BXmight true neighbors X underlying MNthese called false positives.justifies shrink phase algorithm, removes false positive BXtesting independence X conditioned BX {Y }. found independentX shrink phase, cannot true neighbor (i.e., cannot edge X ),GSMN removes BX . Assuming faithfulness correctness independencequery results, end shrink phase BX contains exactly neighbors Xunderlying Markov network.455fiBromberg, Margaritis, & Honavarnext section present concrete implementation GSMN, called GSMN .augments GSMN specifying concrete ordering variables X examinedmain loop GSMN (lines 25 Algorithm 1), well concrete ordervariables examined grow shrink phases GS algorithm (lines 36811 Algorithm 2, respectively).4.2 Concrete GSMN Algorithmsection discuss first algorithm, GSMN (Grow-Shrink Markov Networklearning algorithm), learning structure Markov network domain. Notereason introducing GSMN addition main contribution, GSIMNalgorithm (presented later Section 4.5), comparison reasons. particular, GSIMNGSMN identical structure, following order examination variables,difference use inference GSIMN (see details subsequentsections). Introducing GSMN therefore makes possible measure precisely (throughexperimental results Section 5) benefits use inference performance.GSMN algorithm shown Algorithm 3. structure similar abstractGSMN algorithm. One notable difference order variables examinedspecified; done initialization phase so-called examination ordergrow order X variable X V determined. X priorityqueues initially permutation V (X permutation V {X})position variable queue denotes priority e.g., = [2, 0, 1] meansvariable 2 highest priority (will examined first), followed 0 finally 1.Similarly, position variable X determines order examinedgrow phase X.initialization phase algorithm computes strength unconditionaldependence pair variable X , given unconditional p-valuep(X, | ) independence test pair variables X 6= , denotedpXY algorithm. (In practice logarithm p-values computed, allowsgreater precision domains dependencies may strong weak.)particular, algorithm gives higher priority (examines earlier) variableslower average log p-value (line 5), indicating stronger dependence. average definedas:X1avg log(pXY ) =log(pXY ).|V| 16=Xgrow order X variable X, algorithm gives higher priority variableswhose p-value (or equivalently log p-value) variable X small (line 8).ordering due intuition behind folk-theorem (as Koller & Sahami, 1996,puts it) states probabilistic influence association attributes tendsattenuate distance graphical model. suggests pair variables Xhigh unconditional p-value less likely directly linked. Note orderingheuristic guaranteed hold general. example, may holdunderlying domain Bayesian network e.g., two spouses may independentunconditionally dependent conditional common child. Note howeverexample apply faithful domains i.e., graph-isomorph Markov network. Also456fiEfficient Markov Network Structure Discovery Using Independence TestsAlgorithm 3 GSMN , concrete implementation GSMN: G = GSMN (V, D).1:2:3:4:5:Initialize G empty graph./* Initialization. */X, V, X 6=pXY p(X, | )Initialize i, {0, . . . , n 1}, < avg log(pi j ) < avg log(pi j ) .jj6: X V7:BX8:Initialize X j, j {0, . . . , n 1}, j < j pXX < pXX .jj9:Remove X X .10: /* Main loop. */11: empty12:X dequeue()13:/* Propagation phase. */14:{Y : examined X }15:F {Y : examined X/ }16:T, move end X .17:F, move end X .18:/* Grow phase. */19:20:X empty21:dequeue(X )22:pXY23:IGSMN (X, Y, S, F, T)24:{Y }25:/* Change grow order . */26:Move X beginning .27:W = S|S|2 S028:Move W beginning .29:/* Change examination order. */30:W = S|S|1 S031:W32:Move W beginning .33:break line 3434:/* Shrink phase. */35:= S|S|1 S036:IGSMN (X, Y, {Y } , F, T)37:{Y }38:BX39:Add undirected edge G X variable BX .40: return Gnote correctness algorithms present depend holding i.e.,prove Appendices B, GSMN GSIMN guaranteed returncorrect structure assumptions stated Section 3 above. Also notecomputational cost calculation pXY low due empty conditioning set.remaining GSMN algorithm contains main loop (lines 1039)variable V examined according examination order , determined457fiBromberg, Margaritis, & HonavarAlgorithm 4 IGSMN (X, Y, S, F, T): Calculate independence test (X, | S) propagation, possible, otherwise run statistical test data.1:2:3:4:5:6:7:8:9:/* Attempt infer dependence propagation. */return false/* Attempt infer independence propagation. */Freturn true/* Else statistical test data. */1(p(X,Y |Z)>) /* = true iff p-value statistical test (X, | S) > . */returninitialization phase. main loop includes three phases: propagation phase (lines1317), grow phase (lines 1833), shrink phase (lines 3437). propagationphase optimization variables already computed(i.e., variables already examined) collected two sets F T. Set F (T) containsvariables X/ (X ). sets passed independenceprocedure IGSMN , shown Algorithm 4, purpose avoiding executiontests X algorithm. justified fact that, undirectedgraphs, Markov blanket X X Markov blanket .Variables already found contain X blanket (set F) cannot membersBX exists set variables rendered conditionallyindependent X previous step, independence therefore inferred easily.Note experiments section paper (Section 5) evaluate GSMNwithout propagation phase, order measure effect propagationoptimization performance. Turning propagation accomplished simply settingsets F (as computed lines 14 15, respectively) empty set.Another difference GSMN abstract GSMN algorithm use condition pXY (line 22). additional optimization avoids independence testcase X found (unconditionally) independent initializationphase, since case would imply X independent given conditioningset axiom Strong Union.crucial difference GSMN abstract GSMN algorithm GSMNchanges examination order grow order every variable X . (SinceX/ X , excludes grow order X itself.) changes ordering proceedfollows: end grow phase variable X, new examination order (setlines 3033) dictates next variable W examined X lastadded growing phase yet examined (i.e., W still ).grow order variables found dependent X also changed; donemaximize number optimizations GSIMN algorithm (our main contributionpaper) shares algorithm structure GSMN . changes grow ordertherefore explained detail Section 4.5 GSIMN presented.final difference GSMN abstract GSMN algorithm restartactions grow shrink phases GSMN whenever current Markov blanketmodified (lines 6 11 Algorithm 2), present GSMN . restarting458fiEfficient Markov Network Structure Discovery Using Independence TestsFigure 2: Illustration operation GSMN using independence graph. figureshows growing phase variable 5. Variables examined accordinggrow order 5 = [3, 4, 1, 6, 2, 7, 0].loops necessary GS algorithm due original usage learningstructure Bayesian networks. task, possible true memberblanket X found initially independent grow loop conditioningset found dependent later conditioned superset S.could happen unshielded spouse X i.e., one commonchildren X existed direct link X underlying Bayesiannetwork. However, behavior impossible domain distribution faithfulMarkov network (one assumptions): independence X givenmust hold superset axiom Strong Union (see Eqs. (1)).restart grow shrink loops therefore omitted GSMN order saveunnecessary tests. Note that, even though possible behavior impossiblefaithful domains, possible unfaithful ones, also experimentally evaluatedalgorithms real-world domains assumption Markov faithfulness maynecessarily hold (Section 5).proof correctness GSMN presented Appendix A.4.3 Independence Graphsdemonstrate operation GSMN graphically concept independencegraph, introduce. define independence graph undirectedgraph conditional independences dependencies single variablesrepresented one annotated edges them. solid (dotted) edgevariables X annotated Z represents fact X founddependent (independent) given Z. conditioning set Z enclosed parenthesesedge represents independence dependence inferred Eqs. (1) (asopposed computed statistical tests). Shown graphically:459fiBromberg, Margaritis, & HonavarXXXXZ(X 6Y | Z)(XY | Z)(X 6Y | Z) (inferred)(XY | Z) (inferred)Z(Z)(Z)instance, Figure 2, dotted edge 5 1 annotated 3, 4 representsfact (51 | {3, 4}). absence edge two variables indicatesabsence information independence dependence variablesconditioning set.Example 1. Figure 2 illustrates operation GSMN using independence graphdomain whose underlying Markov network shown Figure 1. figure showsindependence graph end grow phase variable 5, first examinationorder . (We discuss example initialization phase GSMN ; instead,assume examination () grow () orders shown figure.) Accordingvertex separation underlying network (Figure 1), variables 3, 4, 6, 7 founddependent 5 growing phase i.e.,I(5, 3 | ),I(5, 4 | {3}),I(5, 6 | {3, 4}),I(5, 7 | {3, 4, 6})therefore connected 5 independence graph solid edges annotated sets, {3}, {3, 4} {3, 4, 6} respectively. Variables 1, 2, 0 found independent i.e.,I(5, 1 | {3, 4}),I(5, 2 | {3, 4, 6}),I(5, 0 | {3, 4, 6, 7})thus connected 5 dotted edges annotated {3, 4}, {3, 4, 6} {3, 4, 6, 7}respectively.4.4 Triangle Theoremsection present prove theorem used subsequent GSIMNalgorithm. seen, main idea behind GSIMN algorithm attempt decrease number tests done exploiting properties conditional independencerelation faithful domains i.e., Eqs. (1). properties seen inference rulesused derive new independences ones know true. carefulstudy axioms suggests two simple inference rules, stated Triangletheorem below, sufficient inferring useful independence informationinferred systematic application inference rules. confirmedexperiments Section 5.460fiEfficient Markov Network Structure Discovery Using Independence TestsFigure 3: Independence graph depicting Triangle theorem. Edges graphlabeled sets represent conditional independences dependencies. solid(dotted) edge X labeled Z means X dependent(independent) given Z. set label enclosed parentheses means edgeinferred theorem.Theorem 1 (Triangle theorem). Given Eqs. (1), every variable X, , W sets Z1Z2 {X, Y, W } Z1 = {X, Y, W } Z2 = ,(X 6W | Z1 ) (W 6Y | Z2 )=(X 6Y | Z1 Z2 )(XW | Z1 ) (W 6Y | Z1 Z2 )=(XY | Z1 ).call first relation D-triangle rule second I-triangle rule.Proof. using Strong Union Transitivity Eqs. (1) shown contrapositive form.(Proof D-triangle rule):Strong Union (X 6W | Z1 ) get (X 6W | Z1 Z2 ).Strong Union (W 6Y | Z1 ) get (W 6Y | Z1 Z2 ).Transitivity, (X 6W | Z1 Z2 ), (W 6Y | Z1 Z2 ), get (X 6Y | Z1 Z2 ).(Proof I-triangle rule):Strong Union (W 6Y | Z1 Z2 ) get (W 6Y | Z1 ).Transitivity, (XW | Z1 ) (W 6Y | Z1 ) get (XY | Z1 ).represent Triangle theorem graphically using independence graph construct Section 4.2. Figure 3 depicts two rules Triangle theorem using twoindependence graphs.Triangle theorem used infer additional conditional independencestests conducted operation GSMN . example shown Figure 4, illustrates application Triangle theorem example presentedFigure 2. independence information inferred Triangle theorem showncurved edges (note conditioning set edge enclosed parentheses).461fiBromberg, Margaritis, & HonavarFigure 4: Illustration use Triangle theorem example Figure 2. setvariables enclosed parentheses correspond tests inferred Triangletheorem using two adjacent edges antecedents. example, result(17 | {3, 4}), inferred I-triangle rule, independence (51 | {3, 4})dependence (5 67 | {3, 4, 6}).example, independence edge (4, 7) inferred D-triangle rule adjacent edges (5, 4) (5, 7), annotated {3} {3, 4, 6} respectively. annotationinferred edge {3}, intersection annotations {3} {3, 4, 6}.example application I-triangle rule edge (1, 7), inferred edges(5, 1) (5, 7) annotations {3, 4} {3, 4, 6} respectively. annotationinferred edge {3, 4}, intersection annotations {3, 4, 6} {3, 4}.4.5 GSIMN Algorithmprevious section saw possibility using two rules Triangletheorem infer result novel tests grow phase. GSIMN algorithm(Grow-Shrink Inference-based Markov Network learning algorithm), introduced section, uses Triangle theorem similar fashion extend GSMN inferring valuenumber tests GSMN executes, making evaluation unnecessary. GSIMNGSMN work exactly way (and thus GSIMN algorithm shares exactlyalgorithmic description i.e., follow Algorithm 3), differencesconcentrated independence procedure use: instead using independenceprocedure IGSMN GSMN , GSIMN uses procedure IGSIMN , shown Algorithm 5. Procedure IGSIMN , addition attempting propagate blanket information obtainedexamination previous variables (as IGSMN does), also attempts infervalue independence test provided input either Strong Unionaxiom (listed Eqs. (1)) Triangle theorem. attempt successful, IGSIMNreturns value inferred (true false), otherwise defaults statistical testdata set (as IGSMN does). purpose assisting inference process, GSIMN462fiEfficient Markov Network Structure Discovery Using Independence TestsAlgorithm 5 IGSIMN (X, Y, S, F, T): Calculate independence test result inference (including propagation), possible. Record test result knowledge base.1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:/* Attempt infer dependence propagation. */return false/* Attempt infer independence propagation. */Freturn true/* Attempt infer dependence Strong Union. */(A, false) KXYreturn false/* Attempt infer dependence D-triangle rule. */W(A, false) KXW (B, false) KW BAdd (A B, false) KXY KY X .return false/* Attempt infer independence Strong Union. */(A, true) KXYreturn true/* Attempt infer independence I-triangle rule. */W(A, true) KXW s.t. (B, false) KW s.t. BAdd (A, true) KXY KY X .return true/* Else statistical test data. */1(p(X,Y |Z)>) /* = true iff p-value statistical test (X, | S) > . */Add (S, t) KXY KY X .returnIGSIMN maintain knowledge base KXY pair variables X , containingoutcomes tests evaluated far X (either data inferred).knowledge bases empty beginning GSIMN algorithm (the initialization step shown algorithm since GSMN use it), maintainedwithin test procedure IGSIMN .explain IGSIMN (Algorithm 5) detail. IGSIMN attempts infer independence value input triplet (X, | S) applying single step backwardchaining using Strong Union Triangle rules i.e., searches knowledge baseK = {KXY : X, V} antecedents instances rules input triplet(X, | S) consequent. Strong Union rule used direct shownEqs. (1) also contrapositive form. direct form used infer independences, therefore refer I-SU rule on. contrapositive form,I-SU rule becomes (X 6Y | W) = (X 6Y | S), referred D-SU rulesince used infer dependencies. According D-Triangle D-SU rules,dependence (X 6Y | S) inferred knowledge base K contains1. test (X 6Y | A) S,2. tests (X 6W | A) (W 6Y | B) variable W , B S,463fiBromberg, Margaritis, & HonavarFigure 5: Illustration operation GSIMN. figure shows grow phase twoconsecutively examined variables 5 7. figure shows variableexamined second 3 7, according change examination orderlines 3033 Algorithm 3. set variables enclosed parenthesescorrespond tests inferred Triangle theorem using two adjacent edgesantecedents. results (7 63 | ), (7 64 | {3}), (7 66 | {3, 4}), (7 65 |{3, 4, 6}) (b), shown highlighted, executed inferred testsdone (a).respectively. According I-Triangle I-SU rules, independence (XY | S)inferred knowledge base contains3. test (XY | A) S,4. tests (XW | A) (W 6Y | B) variable W , B A,respectively.changes grow orders variables occur inside grow phasecurrently examined variable X (lines 2528 GSIMN i.e., Algorithm 3 IGSMN replaced IGSIMN .). particular, if, variable , algorithm reaches line 24,i.e., pXY IGSIMN (X, Y, S) = false, X variables founddependent X (i.e., variables currently S) promoted beginninggrow order . illustrated Figure 5 variable 7, depicts growphase two consecutively examined variables 5 7. figure, curved edgesshow tests inferred IGSIMN grow phase variable 5. groworder 7 changes 7 = [2, 6, 3, 0, 4, 1, 5] 7 = [3, 4, 6, 5, 2, 0, 1] grow phasevariable 5 complete variables 5, 6, 4 3 promoted (in order)beginning queue. rationale observation increasesnumber tests inferred GSIMN next step: change examinationgrow orders described chosen inferred tests learningblanket variable 7 match exactly required algorithm future step.464fiEfficient Markov Network Structure Discovery Using Independence Testsparticular, note example set inferred dependencies variablefound dependent 5 7 exactly required initial part growphase variable 7, shown highlighted Figure 5(b) (the first four dependencies).independence tests inferred (not conducted), resulting computational savings.general, last dependent variable grow phase X maximum numberdependences independences inferred provides rationale changegrow order selection algorithm examined next.shown assumptions GSMN , structure returnedGSIMN correct one i.e., set BX computed GSIMN algorithm equalsexactly neighbors X. proof correctness GSIMN based correctnessGSMN presented Appendix B.4.6 GSIMN Technical Implementation Detailssection discuss number practical issues subtly influence accuracyefficiency implementation GSIMN. One order application I-SU, D-SU,I-Triangle D-Triangle rules within function IGSIMN . Given independence-queryoracle, order application matterassuming one rulesinferring value independence, guaranteed producevalue due soundness axioms Eqs. (1) (Pearl, 1988). practice however,oracle implemented statistical tests conducted data incorrect,previously mentioned. particular importance observation false independenceslikely occur false dependencies. One example casedomain dependencies weakin case pair variables connected (dependent)underlying true network structure may incorrectly deemed independent pathslong enough. hand, false dependencies much rareconfidence threshold 1 = 0.95 statistical test tells us probabilityfalse dependence chance alone 5%. Assuming i.i.d. data test, chancemultiple false dependencies even lower, decreasing exponentially fast. practicalobservation i.e., dependencies typically reliable independences, providerationale way IGSIMN algorithm works. particular, IGSIMN prioritizesapplication rules whose antecedents contain dependencies first i.e., D-TriangleD-SU rules, followed I-Triangle I-SU rules. effect, uses statistical resultstypically known greater confidence ones usually less reliable.second practical issue concerns efficient inference. GSIMN algorithm uses onestep inference procedure (shown Algorithm 5) utilizes knowledge base K = {KXY }containing known independences dependences pair variables X .implement inference efficiently utilize data structure K purposestoring retrieving independence facts constant time. consists two 2D arrays,one dependencies another independencies. array n n size, nnumber variables domain. cell array corresponds pairvariables (X, ), stores known independences (dependences) Xform list conditioning sets. conditioning set Z list, knowledgebase KXY represents known independence (XY | Z) (dependence (X 6Y | Z)).important note length list 2, two465fiBromberg, Margaritis, & Honavartests done variable X execution GSIMN (donegrowing shrinking phases). Thus, always takes constant time retrieve/storeindependence (dependence), therefore inferences using knowledge baseconstant time well. Also note uses Strong Union axion IGSIMNalgorithm constant time well, accomplished testing (attwo) sets stored KXY subset superset inclusion.5. Experimental Resultsevaluated GSMN GSIMN algorithms artificial real-world data sets.experimental results presented show simple applicationPearls inference rules GSIMN algorithm results significant reduction numbertests performed compared GSMN without adversely affecting qualityoutput network. particular report following quantities:Weighted number tests. weighted number tests computedsummation weight test executed, weight test (X, | Z)defined 2+|Z|. quantity reflects time complexity algorithm (GSMNGSIMN) used assess benefit GSIMN using inference insteadexecuting statistical tests data. standard method comparisonindependence-based algorithms justified observation runningtime statistical test triplet (X, | Z) proportional size N dataset number variables involved i.e., O(N (|Z|+2)) (and exponentialnumber variables involved nave implementation might assume).one construct non-zero entries contingency table usedtest examining data point data set exactly once, time proportionalnumber variables involved test i.e., proportional |{X, }Z| = 2+|Z|.Execution time. order assess impact inference running time(in addition impact statistical tests), report execution timealgorithm.Quality resulting network. measure quality two ways.Normalized Hamming distance. Hamming distance outputnetwork structure underlying model another measurequality output network, actual network used generatedata known. Hamming distance defined number reversededges two network structures, i.e., number times actualedge true network missing returned network edge absenttrue network exists algorithms output network. value zeromeans output network correct structure. able comparedomainsdifferent dimensionalities (number variables n) normalizen2 , total number node pairs corresponding domain.Accuracy. real-world data sets underlying network unknown,Hamming distance calculation possible. case impossible knowtrue value independence. therefore approximate statisticaltest entire data set, use limited, randomly chosen subset (1/3data set) learn network. measure accuracy compare result466fiEfficient Markov Network Structure Discovery Using Independence Tests(true false) number conditional independence tests networkoutput (using vertex separation), tests performed full dataset.experiments involving data sets used 2 statistical test estimationconditional independences. mentioned above, rules thumb exist deem certaintests potentially unreliable depending counts contingency table involved;example, one rule Cochran (1954) deems test unreliable 20%cells contingency table less 5 data points test. Due requirementanswer must obtained independence algorithm conducting test, usedoutcomes tests well experiments. effect possibly unreliabletests quality resulting network measured accuracy measures, listedabove.next section present results domains underlying probabilisticmodel known. followed real-world data experiments model structureavailable.5.1 Known-Model Experimentsfirst set experiments underlying model, called true model true network,known Markov network. purpose set experiments conduct controlledevaluation quality output network systematic study algorithmsbehavior varying conditions domain size (number variables) amountdependencies (average node degree network).true network contains n variables generated randomly follows:network initialized n nodes edges. user-specified parameternetwork structure average node degree equals average number neighborsper node. Given , every node set neighbors determined randomlyuniformly selecting first n2 pairs random permutation possible pairs.factor 1/2 necessary edge contributes degree two nodes.conducted two types experiments using known network structure: Exact learningexperiments sample-based experiments.5.1.1 Exact Learning Experimentsset known-model experiments, assume result statistical queriesasked GSMN GSIMN algorithms available, assumes existenceoracle answer independence queries. underlying model known,oracle implemented vertex separation. benefits queryingtrue network independence two: First, ensures faithfulness correctnessindependence query results, allows evaluation algorithmsassumptions correctness. Second, tests performed much faster actualstatistical tests data. allowed us evaluate algorithms large networksweable conduct experiments domains containing 100 variables.first report weighted number tests executed GSMN withoutpropagation GSIMN. results summarized Figure 6, shows ratioweighted number tests GSIMN two versions GSMN . One467fiWC(GSIMN) / WC(GSMN* propagation)Ratio weighted cost GSIMN vs. GSMN* without propagation10.90.80.70.6=1=2=4=80.50.40.30.20.1001020304050607080Domain size (number variables)90100WC(GSIMN) / WC(GSMN* without propagation)Bromberg, Margaritis, & HonavarRatio weighted cost GSIMN vs. GSMN* propagation10.90.80.70.60.50.4=1=2=4=80.30.20.1001020304050607080Domain size (number variables)90100Figure 6: Ratio weighted number tests GSIMN GSMN without propagation (left plot) propagation (right plot) network sizes (numbernodes) n = 100 average degree = 1, 2, 4, 8.Algorithm 6 IFCH (X, Y, S, F, T). Forward-chaining implementation independence testIGSIMN (X, Y, S, F, T).1:2:3:4:5:6:7:/* Query knowledge base. */(S, t) KXYreturnresult test (X, | S) /* = true iff test (X, | S) returns independence. */Add (S, t) KXY KY X .Run forward-chaining inference algorithm K, update K.returnhundred true networks generated randomly pair (n, ), figure showsmean value. see limiting reduction (as n grows large) weightednumber tests depends primarily average degree parameter . reductionGSIMN large n dense networks ( = 8) approximately 40% compared GSMNpropagation 75% compared GSMN without propagation optimization,demonstrating benefit GSIMN vs. GSMN terms number tests executed.One reasonable question performance GSIMN extent inferenceprocedure complete i.e., tests GSIMN needs operation,number tests infers (by applying single step backward chainingStrong Union axiom Triangle theorem, rather executing statistical testdata) compare number tests inferred (for example using completeautomated theorem prover Eqs. (1))? measure this, compared number testsdone GSIMN number done alternative algorithm, call GSIMNFCH (GSIMN Forward Chaining). GSIMN-FCH differs GSIMN functionIFCH , shown Algorithm 6, replaces function IGSIMN GSIMN. IFCH exhaustivelyproduces independence statements inferred properties Eqs. (1)using forward-chaining procedure. process iteratively builds knowledge base Kcontaining truth value conditional independence predicates. Whenever outcometest required, K queried (line 2 IFCH Algorithm 6). value test468fiEfficient Markov Network Structure Discovery Using Independence TestsRatio Number tests GSIMN-FCH GSIMN=1=2=4=81.41.2Ratio10.80.60.40.202345678910 11 12Number variables (n)Figure 7: Ratio number tests GSIMN-FCH GSIMN network sizes (numbervariables) n = 2 n = 13 average degrees = 1, 2, 4, 8.found K, returned (line 3). not, GSIMN-FCH performs test uses resultstandard forward-chaining automatic theorem prover subroutine (line 6) produceindependence statements inferred test result K, adding newfacts K.comparison number tests executed GSIMN vs. GSIMN-FCH presentedFigure 7, shows ratio number tests GSIMN GSIMN-FCH.figure shows mean value four runs, corresponding network generatedrandomly pair (n, ), = 1, 2, 4 8 n 12. Unfortunately, twodays execution GSIMN-FCH unable complete execution domains containing13 variables more. therefore present results domain sizes 12 only.figure shows n 9, every ratio exactly 1 i.e., tests inferableproduced use Triangle theorem GSIMN. smaller domains, ratio0.95 exception single case, (n = 5, = 1).5.1.2 Sample-based Experimentsset experiments evaluate GSMN (with without propagation) GSIMNdata sampled true model. allows realistic assessmentperformance algorithms. data sampled true (known) Markovnetwork using Gibbs sampling.exact learning experiments previous section structure truenetwork required, generated randomly fashion described above. sample dataknown structure however, one also needs specify network parameters.random network, parameters determine strength dependencies among connectedvariables graph. Following Agresti (2002), used log-odds ratio measurestrength probabilistic influence two binary variables X , definedPr(X = 0, = 0) Pr(X = 1, = 1)XY = log.Pr(X = 0, = 1) Pr(X = 1, = 0)469fiBromberg, Margaritis, & HonavarHamming distance sampled datan = 50, = 1, = 1.5GSMN* without propagationGSMN* propagationGSIMN0.60.40.2046810121416180.40.202002Data set size (thousands data points)GSMN* without propagationGSMN* propagationGSIMN0.60.40.20246810121416180.20101214161820.201012141618Data set size (thousands data points)810121416180.20681012141618200.40.204681012141618Data set size (thousands data points)1618200.40.20024681012141618201GSMN* without propagationGSMN* propagationGSIMN0.80.60.40.2002468101214161820Hamming distance sampled datan = 50, = 8, = 2.00.6214Data set size (thousands data points)GSMN* without propagationGSMN* propagationGSIMN0120.62010.810Hamming distance sampled datan = 50, = 4, = 2.00.448Data set size (thousands data points)0.626GSMN* without propagationGSMN* propagationGSIMN0.820GSMN* without propagationGSMN* propagationGSIMN0.80Normalized Hamming distanceNormalized Hamming distance0.48641Hamming distance sampled datan = 50, = 8, = 1.50.662Data set size (thousands data points)GSMN* without propagationGSMN* propagationGSIMN441Hamming distance sampled datan = 50, = 8, = 1.020Hamming distance sampled datan = 50, = 2, = 2.0020100Data set size (thousands data points)0.2Data set size (thousands data points)0.80.2200.40Normalized Hamming distanceNormalized Hamming distance0.48180.4Hamming distance sampled datan = 50, = 4, = 1.50.66160.6Data set size (thousands data points)GSMN* without propagationGSMN* propagationGSIMN4140.6Hamming distance sampled datan = 50, = 4, = 1.0212GSMN* without propagationGSMN* propagationGSIMN0.82010101Data set size (thousands data points)0.88GSMN* without propagationGSMN* propagationGSIMN0.8Hamming distance sampled datan = 50, = 2, = 1.5Normalized Hamming distanceNormalized Hamming distanceHamming distance sampled datan = 50, = 2, = 1.0061Data set size (thousands data points)10.84Normalized Hamming distance20.6Normalized Hamming distance0GSMN* without propagationGSMN* propagationGSIMN0.8Normalized Hamming distance0.8Hamming distance sampled datan = 50, = 1, = 2.01Normalized Hamming distanceNormalized Hamming distanceNormalized Hamming distanceHamming distance sampled datan = 50, = 1, = 1.01201GSMN* without propagationGSMN* propagationGSIMN0.80.60.40.2002468101214161820Data set size (thousands data points)Figure 8: Normalized Hamming distances true network network outputGSMN (with without propagation) GSIMN domain size n = 50average degrees = 1, 2, 4, 8.network parameters generated randomly log-odds ratio everypair variables connected edge graph specified value. setexperiments, used values = 1, = 1.5 = 2 every pair variablesnetwork.Figures 8 9 show plots normalized Hamming distance truenetwork output GSMN (with without propagation) GSIMNdomain sizes n = 50 n = 75 variables, respectively. plots showHamming distance GSIMN comparable ones GSMN algorithms470fiEfficient Markov Network Structure Discovery Using Independence TestsHamming distance sampled datan = 75, = 1, = 1.5GSMN* without propagationGSMN* propagationGSIMN0.60.40.2046810121416180.40.202002Data set size (thousands data points)GSMN* without propagationGSMN* propagationGSIMN0.60.40.20246810121416180.20101214161820.201012141618Data set size (thousands data points)810121416180.20681012141618200.40.204681012141618Data set size (thousands data points)1618200.40.20024681012141618201GSMN* without propagationGSMN* propagationGSIMN0.80.60.40.2002468101214161820Hamming distance sampled datan = 75, = 8, = 2.00.6214Data set size (thousands data points)GSMN* without propagationGSMN* propagationGSIMN0120.62010.810Hamming distance sampled datan = 75, = 4, = 2.00.448Data set size (thousands data points)0.626GSMN* without propagationGSMN* propagationGSIMN0.820GSMN* without propagationGSMN* propagationGSIMN0.80Normalized Hamming distanceNormalized Hamming distance0.48641Hamming distance sampled datan = 75, = 8, = 1.50.662Data set size (thousands data points)GSMN* without propagationGSMN* propagationGSIMN441Hamming distance sampled datan = 75, = 8, = 1.020Hamming distance sampled datan = 75, = 2, = 2.0020100Data set size (thousands data points)0.2Data set size (thousands data points)0.80.2200.40Normalized Hamming distanceNormalized Hamming distance0.48180.4Hamming distance sampled datan = 75, = 4, = 1.50.66160.6Data set size (thousands data points)GSMN* without propagationGSMN* propagationGSIMN4140.6Hamming distance sampled datan = 75, = 4, = 1.0212GSMN* without propagationGSMN* propagationGSIMN0.82010101Data set size (thousands data points)0.88GSMN* without propagationGSMN* propagationGSIMN0.8Hamming distance sampled datan = 75, = 2, = 1.5Normalized Hamming distanceNormalized Hamming distanceHamming distance sampled datan = 75, = 2, = 1.0061Data set size (thousands data points)10.84Normalized Hamming distance20.6Normalized Hamming distance0GSMN* without propagationGSMN* propagationGSIMN0.8Normalized Hamming distance0.8Hamming distance sampled datan = 75, = 1, = 2.01Normalized Hamming distanceNormalized Hamming distanceNormalized Hamming distanceHamming distance sampled datan = 75, = 1, = 1.01201GSMN* without propagationGSMN* propagationGSIMN0.80.60.40.2002468101214161820Data set size (thousands data points)Figure 9: Normalized Hamming distance results Figure 8 domain size n = 75.domain sizes n = 50 n = 75, average degrees = 1, 2, 4, 8 log-odds ratios = 1,= 1.5 = 2. reinforces claim inference done GSIMN smallimpact quality output networks.Figure 10 shows weighted number tests GSIMN vs. GSMN (with withoutpropagation) sampled data set 20,000 points domains n = 50, n = 75,average degree parameters = 1, 2, 4, 8 log-odds ratios = 1, 1.5 2. GSIMNshows reduced weighted number tests respect GSMN without propagationcases compared GSMN propagation cases (with exceptions( = 4, = 2) ( = 8, = 1.5)). sparse networks weak dependences i.e.,= 1, reduction larger 50% domain sizes, reduction much larger471fiBromberg, Margaritis, & HonavarWeighted cost sampled data= 1, = 1.0, 20,000 data pointsWeighted cost sampled data= 1, = 1.5, 20,000 data points200000150000100000500000250000GSMN* without propagationGSMN* propagationGSIMN2000001500001000005000005075Weighted cost sampled data= 2, = 1.0, 20,000 data points100000500000250000GSMN* without propagationGSMN* propagationGSIMN2000001500001000005000075GSMN* without propagationGSMN* propagationGSIMN200000150000100000500007550Weighted cost sampled data= 4, = 1.5, 20,000 data pointsWeighted cost sampled data= 4, = 2.0, 20,000 data points200000150000100000500000250000300000GSMN* without propagationGSMN* propagationGSIMNWeighted number testsGSMN* without propagationGSMN* propagationGSIMN20000015000010000050000075250000GSMN* without propagationGSMN* propagationGSIMN20000015000010000050000050Number variables7550Number variablesWeighted cost sampled data= 8, = 1.0, 20,000 data pointsWeighted cost sampled data= 8, = 2.0, 20,000 data points300000Weighted number testsGSMN* without propagationGSMN* propagationGSIMN200000150000100000500000250000300000GSMN* without propagationGSMN* propagationGSIMN2000001500001000005000007575Number variablesWeighted cost sampled data= 8, = 1.5, 20,000 data points30000075Number variables300000Weighted number testsWeighted number tests250000Number variablesWeighted cost sampled data= 4, = 1.0, 20,000 data pointsWeighted number testsWeighted cost sampled data= 2, = 2.0, 20,000 data points05030000075300000Number variablesNumber variables50000Number variables05010000050Weighted number tests150000250000150000Weighted cost sampled data= 2, = 1.5, 20,000 data pointsWeighted number testsWeighted number testsGSMN* without propagationGSMN* propagationGSIMN5020000075300000200000250000GSMN* without propagationGSMN* propagationGSIMNNumber variables30000050250000050Number variables250000Weighted number testsGSMN* without propagationGSMN* propagationGSIMN300000Weighted number tests250000Weighted cost sampled data= 1, = 2.0, 20,000 data points300000Weighted number testsWeighted number tests300000250000GSMN* without propagationGSMN* propagationGSIMN2000001500001000005000005075Number variables5075Number variablesFigure 10: Weighted number tests executed GSMN (with without propagation)GSIMN |D| = 20, 000, domains sizes n = 50 75, average degreeparameters = 1, 2, 4, 8, log-odds ratios = 1, 1, 5, 2.one observed exact learning experiments. actual execution timesvarious data set sizes network densities shown Figure 11 largest domainn = 75, = 1, verifying reduction cost GSIMN various data set sizes.Note reduction proportional number data points; reasonabletest executed must go entire data set construct contingency table.confirms claim cost inference GSIMN small (constant time pertest, see discussion Section 4.6) compared execution time tests themselves,indicates increasing cost benefits use GSIMN even large data sets.472fiEfficient Markov Network Structure Discovery Using Independence TestsExecution times sampled data setsn = 75 variables, = 1, = 1Execution times sampled data setsn = 75 variables, = 2, = 1300300GSMN* without propagationGSMN* propagationGSIMNGSMN* without propagationGSMN* propagationGSIMN250Execution time (sec)Execution time (sec)25020015010050200150100500002000 4000 6000 8000 10000 12000 14000 16000 18000 200000Execution times sampled data setsn = 75 variables, = 4, = 1Execution times sampled data setsn = 75 variables, = 8, = 1300300GSMN* without propagationGSMN* propagationGSIMNGSMN* without propagationGSMN* propagationGSIMN250Execution time (sec)250Execution time (sec)2000 4000 6000 8000 10000 12000 14000 16000 18000 2000020015010050200150100500002000 4000 6000 8000 10000 12000 14000 16000 18000 2000002000 4000 6000 8000 10000 12000 14000 16000 18000 20000Figure 11: Execution times sampled data experiments = 1, = 1, 2 (top row)= 4, 8 (bottom row) domain n = 75 variables.5.1.3 Real-World Network Sampled Data Experimentsalso conducted sampled data experiments well-known real-world networks.known repository Markov networks drawn real-world domains, insteadutilized well-known Bayesian networks widely used Bayesian network researchavailable number repositories.1 generate Markov networksBayesian network structures used process moralization (Lauritzen, 1996)consists two steps: (a) connect pair nodes Bayesian networkcommon child undirected edge (b) remove directions edges.results Markov network local Markov property valid i.e., nodeconditionally independent nodes domain given direct neighbors.procedure conditional independences may lost. This, however, affectaccuracy results compare independencies output networkmoralized Markov network (as opposed Bayesian network).conducted experiments using 5 real-world domains: Hailfinder, Insurance, Alarm,Mildew, Water. domain sampled varying number data pointscorresponding Bayesian network using logic sampling (Henrion, 1988), used inputGSMN (with without propagation) GSIMN algorithms. comparednetwork output algorithms original moralized network usingnormalized Hamming distance metric previously described. results shown1. used http://compbio.cs.huji.ac.il/Repository/. Accessed December 5, 2008.473fiBromberg, Margaritis, & HonavarHamming distance hailfinder data setHamming distance insurance data setHamming distance alarm data setGSMN* without propagationGSMN* propagationGSIMN0.90.80.70.60.50.40.30.20.100246810 12 14 161GSMN* without propagationGSMN* propagationGSIMN0.90.80.70.60.50.40.30.20.1018 20 220Data set size (thousands data points)246810 12 14 16Normalized Hamming distanceNormalized Hamming distance0.70.60.50.40.30.20.1018 20 220246810 12 14 1618 20 22Data set size (thousands data points)Hamming distance Water data setGSMN* without propagationGSMN* propagationGSIMN0.80.7GSMN* without propagationGSMN* propagationGSIMN0.90.8Data set size (thousands data points)Hamming distance Mildew data set10.9Normalized Hamming distance1Normalized Hamming distanceNormalized Hamming distance10.60.50.40.30.20.1010.9GSMN* without propagationGSMN* propagationGSIMN0.80.70.60.50.40.30.20.100246810 12 14 1618 20 220Data set size (thousands data points)246810 12 14 1618 20 22Data set size (thousands data points)Figure 12: Normalized Hamming distance network output GSMN (withwithout propagation) GSIMN true Markov networks network usingvarying data set sizes sampled Markov networks various real-worlddomains modeled Bayesian networks.Fig. 12 indicate distances produced three algorithms similar.cases (e.g., Water Hailfinder) network resulting use GSIMNactually better (of smaller Hamming distance) ones output GSMNalgorithms.also measured weighted cost three algorithms domains,shown Fig. 13. plots show significant decrease weighted number testsGSIMN respect GSMN algorithms: cost GSIMN 66% costGSMN without propagation average, savings 34%, cost GSIMN 28%cost GSMN without propagation average, savings 72%.5.2 Real-World Data Experimentsartificial data set studies previous section advantage allowingcontrolled systematic study performance algorithms, experimentsreal-world data necessary realistic assessment performance. Real datachallenging may come non-random topologies (e.g., possiblyirregular lattice many cases spatial data) underlying probability distributionmay faithful.conducted experiments number data sets obtained UCI machinelearning data set repository (Newman, Hettich, Blake, & Merz, 1998). Continuous variablesdata sets discretized using method widely recommended introductory statistics texts (Scott, 1992); dictates optimal number equally-spaced discretizationbins continuous variable k = 1 + log2 N , N number points474fiEfficient Markov Network Structure Discovery Using Independence TestsGSMN* without propagationGSMN* propagationGSIMN700006000050000400003000020000GSMN* without propagationGSMN* propagationGSIMN8000Weighted cost testsWeighted cost tests80000Weighted cost tests insurance data set900070005000400030002000140001200010000800060004000200010000005101520005Data set size (thousands data points)1015051015GSMN* without propagationGSMN* propagationGSIMN250008000600040002000200001500010000500000051015200Data set size (thousands data points)5101520Data set size (thousands data points)Figure 13: Weighted cost tests conducted GSMN (with without propagation)GSIMN algorithms various real-world domains modeled Bayesiannetworks.Weighted cost accuracy real-world data sets1acc(GSIMN) - acc(GSMN* without propagation)acc(GSIMN) - acc(GSMN* propagation)wc(GSIMN) / wc(GSMN* without propagation)wc(GSIMN) / wc(GSMN* propagation)0.90.80.70.60.50.40.30.20.10-0.1-0.219 12 14 67 12 83 13 15 4Data set index5 10 11 18 17 9 16Figure 14: Ratio weighted number tests GSIMN versus GSMN differenceaccuracy GSIMN GSMN real data sets. Ratios smaller1 positive bars indicate advantage GSIMN GSMN .numbers x-axis indices data sets shown Table 1.data set. data set algorithm, report weighted number conditional independence tests conducted discover network accuracy, definedbelow.47520Data set size (thousands data points)Weighted cost tests Water data setGSMN* without propagationGSMN* propagationGSIMNWeighted cost testsWeighted cost tests20Data set size (thousands data points)Weighted cost tests Mildew data set10000GSMN* without propagationGSMN* propagationGSIMN16000600010000Weighted cost tests alarm data setWeighted cost testsWeighted cost tests hailfinder data setfiBromberg, Margaritis, & HonavarTable 1: Weighted number tests accuracy several real-world data sets.evaluation measure, best performance GSMN (with withoutpropagation) GSIMN indicated bold. number variablesdomain denoted n number data points data set N .#12345678910111213141516171819Data setNameechocardiogramecolilenseshayes-rothhepatitiscmcbalance-scalebaloonsflagtic-tac-toebridgescarmonks-1habermannurserycrximports-85dermatologyadultnN149562010552910127759162535106133624132801473625201949587017285563061296065319335832561Weighted number testsGSMNGSMNGSIMN(w/o prop.) (w/ prop.)1311105060442530918760402010272301412980392434292154824729604020533527879944352911195204551411941406713593429876424112701231719999305451930641102990266872635870652418GSMN(w/o prop.)0.2440.3530.9660.8520.8730.7460.4980.9320.3000.6570.8140.6220.9360.3080.4440.2790.3290.3480.526AccuracyGSMN(w/ prop.)0.2440.3940.9660.8520.9120.7670.7970.9320.6740.6570.6350.6770.9360.3080.7930.5560.4600.5410.537GSIMN0.2440.4110.9660.8520.9680.7940.6980.9320.9290.7040.9160.7610.9360.3080.7550.8920.8470.8080.551real-world data structure underlying Bayesian network (if any)unknown, impossible measure Hamming distance resulting networkstructure. Instead, measured estimated accuracy network produced GSMNGSIMN comparing result (true false) number conditional independencetests network learned (using vertex separation) result testsperformed data set (using 2 test). approach similar estimating accuracyclassification task unseen instances inputs triplets (X, Y, Z)class attribute value corresponding conditional independence test.used 1/3 real-world data set (randomly sampled) input GSMN GSIMNentire data set 2 test. corresponds hypothetical scenariomuch smaller data set available researcher, approximates true valuetest outcome entire data set. Since number possible testsexponential, estimated independence accuracy sampling 10,000 triplets (X, Y, Z)randomly, evenly distributed among possible conditioning set sizes {0, . . . , n 2}(i.e., 10000/(n 1) tests m). triplets constructed follows:First, two variables X drawn randomly V. Second, conditioning setdetermined picking first variables random permutation V {X, }.Denoting set 10,000 triplets, triplet, Idata (t) result testperformed entire data set Inetwork (t) result test performed476fiEfficient Markov Network Structure Discovery Using Independence Testsnetwork output either GSMN GSIMN, estimated accuracy defined as:fiofifi1 fifin| Inetwork (t) = Idata (t) fifi.accuracy\ =|T | fidata sets, Table 1 shows detailed results accuracy weightednumber tests GSMN GSIMN algorithms. results also plottedFigure 14, horizontal axis indicating data set index appearing first columnTable 1. Figure 14 plots two quantities graph real-world data sets:ratio weighted number tests GSIMN versus two GSMN algorithmsdifference accuracies. data set, improvement GSIMNGSMN corresponds number smaller 1 ratios positive histogram baraccuracy differences. observe GSIMN reduced weighted numbertests every data set, maximum savings 82% GSMN without propagation(for crx data set) 60% GSMN propagation (for crx data setwell). Moreover, 11 19 data sets GSIMN resulted improved accuracy, 6 tie2 somewhat reduced accuracy compared GSMN propagation (fornursery balance-scale data sets).6. Conclusions Future Researchpaper presented two algorithms, GSMN GSIMN, learning efficientlystructure Markov network domain data using independence-basedapproach (as opposed NP-hard algorithms based maximum likelihood estimation)evaluated performance measurement weighted number testsrequire learn structure network quality networks learnedartificial real-world data sets. GSIMN showed decrease vast majorityartificial real-world domains output network quality comparableGSMN , cases showing improvement. addition, GSIMN shownnearly optimal number tests executed compared GSIMN-FCH, usesexhaustive search produce independence information inferred Pearlsaxioms. directions future research include investigation way topologyunderlying Markov network affects number tests required qualityresulting network, especially commonly occurring topologies grids. Anotherresearch topic impact number tests examination grow orderingsvariables.Acknowledgmentsthank Adrian Silvescu insightful comments accuracy measures general advicetheory undirected graphical models.Appendix A. Correctness GSMNvariable X V examined main loop GSMN algorithm (lines1039), set BX variable X V constructed growing shrinking set S,477fiBromberg, Margaritis, & Honavarstarting empty set. X connected member BX producestructure Markov network. prove procedure returns actual Markovnetwork structure domain.proof correctness make following assumptions.axioms Eqs. (1) hold.probability distribution domain strictly positive (required Intersectionaxiom hold).Tests conducted querying oracle, returns true value underlying model.algorithm examines every variable X inclusion (and thus BX )grow phase (lines 18 33) and, added grow phase,considers removal shrinking phase (lines 34 37). Noteone test executed X growing phase X; call growtest X (line 23). Similarly, one tests executed Xshrinking phase; test (if executed) called shrink test X (line36).general idea behind proof show that, learning blanket X,variable end shrinking phase dependence (X 6Y |V {X, }) X holds (which, according Theorem 2 endAppendix, implies edge X ). immediately prove onedirection.Lemma 1./ end shrink phase, (XY | V {X, }).Proof. Let us assume/ end shrink phase. Then, eitheradded set grow phase (i.e., line 24 never reached), removedshrink phase (i.e., line 37 reached). former true(pXY > ) line 22 (indicating X unconditionally independent) foundindependent X line 23. latter true found independent Xline 36. cases V {X, } (XY | A), Strong Union(XY | V {X, }).opposite direction proved Lemma 6 below. However, proof involved,requiring auxiliary lemmas, observations, definitions. two main auxiliaryLemmas 4 5. use lemma presented next (Lemma 2) inductively extendconditioning set dependencies found grow shrink tests X ,remaining variables V{X, }. Lemma shows that, certain independenceholds, conditioning set dependence increased one variable.Lemma 2. Let X, V, Z V {X, }, Z Z. W V,(X 6Y | Z) (XW | Z {Y }) = (X 6Y | Z {W }).478fiEfficient Markov Network Structure Discovery Using Independence TestsProof. prove contradiction, make use axioms Intersection (I), StrongUnion (SU), Decomposition (D). Let us assume (X 6Y | Z) (XW | Z {Y })(XY | Z {W }).(XY | Z {W }) (XW | Z {Y })SU=(XY | Z {W }) (XW | Z {Y })(X{Y, W } | Z)=(XY | Z) (XW | Z)=(XY | Z).=contradicts assumption (X 6Y | Z).introduce notation definitions prove auxiliary lemmas.denote SG value end grow phase (line 34) i.e., setvariables found dependent X grow phase, SS value endshrink phase (line 39). also denote G set variables found independentX grow phase U = [U0 , . . . , Uk ] sequence variables shrunkBX , i.e., found independent X shrink phase. sequence U assumedordered follows: < j variable Ui found independent X Ujshrinking phase. prefix first variables [U0 , . . . , Ui1 ] U denotedUi . test performed algorithm, define k(t) integerUk(t) prefix U containing variables found independent Xloop t. Furthermore, abbreviate Uk(t) Ut .definition U fact grow phase conditioning setincreases dependent variables only, immediately make following observation:Observation 1. variable Ui U, denotes shrink test performedX Ui Ut = Ui1 .relate conditioning set shrink test Ut follows:Lemma 3. SS = (X, | Z) shrink test , Z = SG Ut {Y }.Proof. According line 36 algorithm, Z = {Y }. beginning shrinkphase (line 34) = SG , variables found independent afterward conductedremoved line 37. Thus, time performed, = SG Utconditioning set becomes SG Ut {Y }.Corollary 1. (XUi | SG Ui ).Proof. proof follows immediately Lemma 3, Observation 1, factUi = Ui1 {Ui }.following two lemmas use Lemma 2 inductively extend conditioning setdependence X variable SS . first lemma starts shrinktest X (a dependence), extends conditioning set SS {Y } (orequivalently SG {Y } Ut according Lemma 3) SG {Y }.479fiBromberg, Margaritis, & HonavarLemma 4. SS shrink test , (X 6Y | SG {Y }).Proof. proof proceeds proving(X 6Y | SG {Y } Ui )induction decreasing values i, {0, 1, . . . , k(t)}, starting = k(t).lemma follows = 0 noticing U0 = .Base case (i = k(t)): Lemma 3, = (X, | SG {Y } Ut ), equals(X, | SG {Y } Uk(t) ) definition Ut . Since SS , must casefound dependent, i.e., (X 6Y | SG {Y } Uk(t) ).Inductive step: Let us assume statement true = m, 0 < k(t)1:(X 6Y | SG {Y } Um ).(2)need prove also true = 1:(X 6Y | SG {Y } Um1 ).Corollary 1,(XUm | SG Um )Strong Union,(XUm | (SG Um ) {Y })(XUm | (SG Um {Y }) {Y }).(3)Eqs. (2), (3) Lemma 2 get desired relation:(X 6Y | (SG {Y } Um ) {Um }) = (X 6Y | SG {Y } Um1 ).Observation 2. definition SG , every test = (X, | Z) performedgrow phase, Z SG .following lemma completes extension conditioning set dependenceX SS universe variables V {X, }, starting SG {Y }(where Lemma 4 left off) extending SG G {Y }.Lemma 5. SS , (X 6Y | SG G {Y }).Proof. proof proceeds proving(X 6Y | SG Gi {Y })induction increasing values 0 |G|, Gi denotes first elementsarbitrary ordering set G.480fiEfficient Markov Network Structure Discovery Using Independence TestsBase Case (i = 0): Follows directly Lemma 4 = 0, since G0 = .Inductive Step: Let us assume statement true = m, 0 < |G|:(X 6Y | SG Gm {Y }).(4)need prove also true = + 1:(X 6Y | SG Gm+1 {Y }).(5)Observation 2 grow test Gm results independence:(XGm | Z), Z SG .Strong Union axiom become:(XGm | Z {Y }), Z SG(6)(XGm | (Z {Y }) {Y }), Z SG .(7)equivalentlySince Z SG SG Gm , Z {Y } SG Gm , Eq. (4)Lemma 2 get desired relation:(X 6Y | (SG Gm {Y }) Gm ) = (X 6Y | SG Gm+1 {Y }).Finally, prove X dependent every variable SS given universeV {X, }.Lemma 6. SS , (X 6Y | V {X, }).Proof. Lemma 5,(X 6Y | SG G {Y })suffices prove SG G {Y } = V {X, }. loop 69 GSMN ,queue X populated elements V {X}, then, line 21, removedX . grow phase partitions X variables dependent X (set SG )independent X (set G).Corollary 2. SS (X 6Y | V {X, }).Proof. Follows directly Lemmas 1 6.Corollary immediately show graph returnedconnecting X member BX = SS exactly Markov network domainusing following theorem, first published Pearl Paz (1985).Theorem 2. (Pearl & Paz, 1985) Every dependence model satisfying symmetry, decomposition, intersection (Eqs. (1)) unique Markov network G = (V, E) produceddeleting complete graph every edge (X, ) (XY | V {X, }) holds, i.e.,(X, )/ E (XY | V {X, }) .481fiBromberg, Margaritis, & HonavarAppendix B. Correctness GSIMNGSIMN algorithm differs GSMN use test subroutine IGSIMNinstead IGSMN (Algorithms 5 4, respectively), turn differs numberadditional inferences conducted obtain independencies (lines 8 22).inferences direct applications Strong Union axiom (which holds assumption)Triangle theorem (which proven hold Theorem 1). Using correctnessGSMN (proven Appendix A) therefore conclude GSIMN algorithmcorrect.ReferencesAbbeel, P., Koller, D., & Ng, A. Y. (2006). Learning factor graphs polynomial timesample complexity. Journal Machine Learning Research, 7, 17431788.Acid, S., & de Campos, L. M. (2003). Searching Bayesian network structuresspace restricted acyclic partially directed graphs. Journal Artificial IntelligenceResearch, 18, 445490.Agresti, A. (2002). Categorical Data Analysis (2nd edition). Wiley.Aliferis, C. F., Tsamardinos, I., & Statnikov, A. (2003). HITON, novel Markov blanketalgorithm optimal variable selection. Proceedings American MedicalInformatics Association (AMIA) Fall Symposium.Anguelov, D., Taskar, B., Chatalbashev, V., Koller, D., Gupta, D., Heitz, G., & Ng, A.(2005). Discriminative learning Markov random fields segmentation 3D rangedata. Proceedings Conference Computer Vision Pattern Recognition(CVPR).Barahona, F. (1982). computational complexity Ising spin glass models. JournalPhysics A: Mathematical General, 15 (10), 32413253.Besag, J. (1974). Spacial interaction statistical analysis lattice systems. JournalRoyal Statistical Society, Series B, 36, 192236.Besag, J., York, J., & Mollie, A. (1991). Bayesian image restoration two applicationsspatial statistics.. Annals Institute Statistical Mathematics, 43, 159.Bromberg, F., Margaritis, D., & Honavar, V. (2006). Efficient Markov network structure discovery independence tests. Proceedings SIAM International ConferenceData Mining.Buntine, W. L. (1994). Operations learning graphical models. Journal ArtificialIntelligence Research, 2, 159225.Castelo, R., & Roverato, A. (2006). robust procedure Gaussian graphical model searchmicroarray data p larger n. Journal Machine Learning Research,7, 26212650.Chow, C., & Liu, C. (1968). Approximating discrete probability distributions dependence trees. IEEE Transactions Information Theory, 14 (3), 462 467.482fiEfficient Markov Network Structure Discovery Using Independence TestsCochran, W. G. (1954). methods strengthening common 2 tests. Biometrics,10, 417451.Della Pietra, S., Della Pietra, V., & Lafferty, J. (1997). Inducing features random fields.IEEE Transactions Pattern Analysis Machine Intelligence, 19 (4), 390393.Dobra, A., Hans, C., Jones, B., Nevins, J. R., Yao, G., & West, M. (2004). Sparse graphicalmodels exploring gene expression data. Journal Multivariate Analysis, 90, 196212.Edwards, D. (2000). Introduction Graphical Modelling (2nd edition). Springer, NewYork.Friedman, N., Linial, M., Nachman, I., & Peer, D. (2000). Using Bayesian networksanalyze expression data. Computational Biology, 7, 601620.Geman, S., & Geman, D. (1984). Stochastic relaxation, gibbs distributions, bayesianrelation images.. IEEE Transactions Pattern Analysis Machine Intelligence,6, 721741.Heckerman, D. (1995). tutorial learning bayesian networks. Tech. rep. MSR-TR-95-06,Microsoft Research.Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks:combination knowledge statistical data. Machine Learning, 20, 197243.Henrion, M. (1988). Propagation uncertainty probabilistic logic sampling Bayesnetworks. Lemmer, J. F., & Kanal, L. N. (Eds.), Uncertainty Artificial Intelligence 2. Elsevier Science Publishers B.V. (North Holland).Hofmann, R., & Tresp, V. (1998). Nonlinear Markov networks continuous variables.Neural Information Processing Systems, Vol. 10, pp. 521529.Isard, M. (2003). Pampas: Real-valued graphical models computer vision. IEEEConference Computer Vision Pattern Recognition, Vol. 1, pp. 613620.Jerrum, M., & Sinclair, A. (1993). Polynomial-time approximation algorithms Isingmodel. SIAM Journal Computing, 22, 10871116.Kearns, M. J., & Vazirani, U. V. (1994). Introduction Computational Learning Theory.MIT Press, Cambridge, MA.Koller, D., & Sahami, M. (1996). Toward optimal feature selection. International Conference Machine Learning, pp. 284292.Lam, W., & Bacchus, F. (1994). Learning Bayesian belief networks: approach basedMDL principle. Computational Intelligence, 10, 269293.Lauritzen, S. L. (1996). Graphical Models. Oxford University Press.Margaritis, D., & Thrun, S. (2000). Bayesian network induction via local neighborhoods.Solla, S., Leen, T., & Muller, K.-R. (Eds.), Advances Neural Information ProcessingSystems 12, pp. 505511. MIT Press.McCallum, A. (2003). Efficiently inducing features conditional random fields. Proceedings Uncertainty Artificial Intelligence (UAI).483fiBromberg, Margaritis, & HonavarNewman, D. J., Hettich, S., Blake, C. L., & Merz, C. J. (1998). UCI repository machinelearning databases. Tech. rep., University California, Irvine, Dept. InformationComputer Sciences.Pena, J. M. (2008). Learning Gaussian graphical models gene networks false discovery rate control. Proceedings 6th European Conference EvolutionaryComputation, Machine Learning Data Mining Bioinformatics, pp. 165176.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers, Inc.Pearl, J., & Paz, A. (1985). Graphoids: graph-based logic reasoning releveancerelations. Tech. rep. 850038 (R-53-L), Cognitive Systems Laboratory, UniversityCalifornia.Rebane, G., & Pearl, J. (1989). recovery causal poly-trees statistical data.Kanal, L. N., Levitt, T. S., & Lemmer, J. F. (Eds.), Uncertainty ArtificialIntelligence 3, pp. 175182, Amsterdam. North-Holland.Schafer, J., & Strimmer, K. (2005). empirical bayes approach inferring large-scalegene association networks. Bioinformatics, 21, 754764.Scott, D. W. (1992). Multivariate Density Estimation. Wiley series probabilitymathematical statistics. John Wiley & Sons.Shekhar, S., Zhang, P., Huang, Y., & Vatsavai, R. R. (2004) Kargupta, H., Joshi, A.,Sivakumar, K., & Yesha, Y. (Eds.), Trends Spatial Data Mining, chap. 19, pp.357379. AAAI Press / MIT Press.Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, Prediction, Search (2ndedition). Adaptive Computation Machine Learning Series. MIT Press.Srebro, N., & Karger, D. (2001). Learning Markov networks: Maximum bounded tree-widthgraphs. ACM-SIAM Symposium Discrete Algorithms.Tsamardinos, I., Aliferis, C. F., & Statnikov, A. (2003a). Algorithms large scale Markovblanket discovery. Proceedings 16th International FLAIRS Conference, pp.376381.Tsamardinos, I., Aliferis, C. F., & Statnikov, A. (2003b). Time sample efficient discovery Markov blankets direct causal relations. Proceedings 9th ACMSIGKDD International Conference Knowledge Discovery Data Mining, pp.673678.Tsamardinos, I., Brown, L. E., & Aliferis, C. F. (2006). max-min hill-climbing Bayesiannetwork structure learning algorithm. Machine Learning, 65, 3178.Whittaker, J. (1990). Graphical Models Applied Multivariate Statistics. John Wiley &Sons, New York.484fiJournal Artificial Intelligence Research 35 (2009) 275-341Submitted 09/08; published 06/09Llull Copeland Voting Computationally ResistBribery Constructive ControlPiotr FaliszewskiFALISZEW @ AGH . EDU . PLDepartment Computer Science, AGH University Science TechnologyKrakow, PolandEdith HemaspaandraEH @ CS . RIT. EDUDepartment Computer Science, Rochester Institute TechnologyRochester, NY 14623 USALane A. HemaspaandraLANE @ CS . ROCHESTER . EDUDepartment Computer Science, University RochesterRochester, NY 14627 USAJorg RotheROTHE @ CS . UNI - DUESSELDORF. DEInstitut fur Informatik, Heinrich-Heine-Universitat Dusseldorf40225 Dusseldorf, GermanyAbstractControl bribery settings external agent seeks influence outcomeelection. Constructive control elections refers attempts agent to, via actionsaddition/deletion/partition candidates voters, ensure given candidate wins. Destructivecontrol refers attempts agent to, via actions, preclude given candidates victory.election system agent sometimes affect result determinedpolynomial time inputs agent succeed said vulnerable given typecontrol. election system agent sometimes affect result, yetNP-hard recognize inputs agent succeed, said resistant giventype control.Aside election systems NP-hard winner problem, systems previouslyknown resistant standard control types highly artificial election systems created hybridization. paper studies parameterized version Copeland voting, denotedCopeland , parameter rational number 0 1 specifies tiesvalued pairwise comparisons candidates. every previously studied constructive destructive control scenario, determine resistance vulnerability holds Copelandrational , 0 1. particular, prove Copeland0.5 , system commonlyreferred Copeland voting, provides full resistance constructive control, proveCopeland , rational , 0 < < 1. Among systems polynomial-timewinner problem, Copeland voting first natural election system proven full resistanceconstructive control. addition, prove Copeland0 Copeland1 (interestingly,Copeland1 election system developed thirteenth-century mystic Llull) resistantstandard types constructive control one variant addition candidates. Moreover, show rational , 0 1, Copeland voting fully resistant briberyattacks, establish fixed-parameter tractability bounded-case control Copeland .also study Copeland elections flexible models microbribery extended control, integrate potential irrationality voter preferences many results,prove results unique-winner model nonunique-winner model.vulnerability results microbribery proven via novel technique involving min-cost networkflow.c2009AI Access Foundation. rights reserved.fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE1. Introductionsection gives history outline results.1.1 Historical Remarks: Llulls Copelands Election SystemsElections played important role human societies thousands years. example,elections central importance democracy ancient Athens. citizens typicallycould agree (vote yes) disagree (vote no) speaker, simple majority-ruleeffect. mathematical study elections, give take discussions ancient GreeksRomans, recently thought initiated hundred years ago, namelybreakthrough work Borda Condorcetlater part reinvented Dodgson (see, e.g.,McLean Urken, 1995, reprints classic papers). One interesting resultsearly work Condorcets (1785) observation one conducts elections twoalternatives even voters rational (i.e., transitive) preferences, society aggregateirrational (indeed, cycles strict preference). Nonetheless, Condorcet believedexists candidate c c defeats candidate head-to-head contestsc win election (see, e.g., page 114 McLean Urken, 1995). candidatecalled Condorcet winner. Clearly, one Condorcet winner election,might none.understanding history reconsidered past decades,discovered study elections considered deeply early thirteenth century (seeHagele Pukelsheim, 2001, citations therein regarding Ramon Llull fifteenthcentury figure Cusanus, especially citations Hagele Pukelsheim, 2001, numbered3, 5, 2427). Ramon Llull (b. 1232, d. 1315), Catalan mystic, missionary, philosopherdeveloped election system (a) efficient winner-determination procedure (b) electsCondorcet winner whenever one exists otherwise elects candidates are, certain sense,closest Condorcet winners.Llulls motivation developing election system obtain method choosingabbesses, abbots, bishops, perhaps even pope. election ideas never gained public acceptance medieval Europe long forgotten.interesting note Llull allowed voters so-called irrational preferences. Giventhree candidates, c, d, e, perfectly acceptable voter prefer c d, e, ec. hand, modern studies voting election systems voters preferencestypically modeled linear order candidates. (In paper, commondiscussing elections, linear order implies strictness, i.e., tie ordering; is,linear order mean strict, complete order, i.e., irreflexive, antisymmetric, complete,transitive relation.) Yet allowing irrationality tempting natural. Consider Bob,likes eat often hurry. Bob prefers diners fast food willingwait little longer get better food. Also, given choice fancy restaurant dinerprefers fancy restaurant, willing wait somewhat longer get betterquality. However, given choice fast-food place fancy restaurant Bob mightreason willing wait much longer served fancy restaurantchoose fast food instead. Thus regarding catering options, Bobs preferences irrationalsense, i.e., intransitive. voters make choices based multiple criteriaacommon natural occurrence among humans software agentssuch irrationalities276fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLoccur. Another example irrationality might naturally occur, suggested referee,case voter delegate group (having odd number members),pair alternatives delegate votes whichever alternative majorityconstituents prefers among pair.Llulls election system remarkably similar known Copeland elections (Copeland, 1951), half-century old voting procedure based pairwisecomparisons candidates: winner (by majority votesin paper majority always,standard, means strict majority) head-to-head contest awarded one pointloser awarded zero points; ties, parties (in common interpretationCopelands meaning) awarded half point; whoever collects points contests (including tie-related points) elections winner. fact, point value awarded tieshead-to-head majority-rule contests treated two ways literature speakingCopeland elections: half point (most common) zero points (less common). provideframework capture notions, well capturing Llulls system wholefamily systems created choices value ties, propose introduce parameterized version Copeland elections, denoted Copeland , parameter rationalnumber, 0 1, case tie candidates receive points. system widelyreferred literature Copeland elections Copeland0.5 , tied candidates receivehalf point (see, e.g., Saari Merlin, 1996, Merlin Saari, 1997; definition usedConitzer, Sandholm, & Lang, 2007, scaled equivalent Copeland0.5 ). Copeland0 ,tied candidates come away empty-handed, sometimes also referred Copelandelections (see, e.g., Procaccia, Rosenschein, Kaminka, 2007, Faliszewski, Hemaspaandra,Hemaspaandra, Rothe, 2007, early version paper). above-mentioned electionsystem proposed Ramon Llull thirteenth century notation Copeland1 , tiedcandidates awarded one point each, like winners head-to-head contests.1 group stage1FIFA World Cup finals essence collection Copeland 3 tournaments.first glance, one might tempted think definitional perturbation due parameter Copeland elections negligible. However, fact make dynamics Llullssystem quite different of, instance, Copeland0.5 Copeland0 . Specific exampleswitnessing claim, regarding complexity results regarding proofs, givenend Section 1.3.Finally, mention probabilistic variant Copeland voting (known Jech method)defined already 1929 Zermelo (1929) later reintroduced several researches (see, e.g., Levin Nalebuff, 1995, references description Jechmethod). note passing Jech method applicable even fed incomplete information. present paper, however, consider incomplete-information probabilisticscenarios, although commend settings interesting future work.1. Page 23 Hagele Pukelsheim 2001 indicates way find deeply convincing (namely direct quoteLlulls in-this-case-very-clear words Artifitium Electionis Personarumwhich rediscoveredauthors year 2000) least one Llulls election systems Copeland1 , paper referboth-candidates-score-a-point-on-a-tie variant Llull voting.settings Llull required candidate voter sets identical elaborate two-stage tiehandling rule ending randomization. disregard issues cast system modern idiomelection systems. (However, note passing exist modern papers votercandidate sets taken identical, see example work references Altman Tennenholtz, 2007.)277fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE1.2 Computational Social Choicegeneral impossible design perfect election system. Arrow (1963) famously showedsocial choice system satisfies certain small set arguably reasonable requirements,later Gibbard (1973), Satterthwaite (1975), Duggan Schwartz (2000) showednatural election system sometimes manipulated strategic voting, i.e., voter revealingdifferent preferences true ones order affect elections resultfavor. Also, natural election system polynomial-time winner-determination procedureyet shown resistant types control via procedural changes. Control refersattempts external agent (called chair) to, via actions addition/deletion/partitioncandidates voters, make given candidate win election (in case constructive control,Bartholdi, Tovey, Trick, 1992) preclude given candidates victory (in case destructivecontrol, Hemaspaandra, Hemaspaandra, Rothe, 2007a).obstacles discouraging, field computational social choice theory grewpart realization computational complexity provides potential shield manipulation/control/etc. particular, around 1990, Bartholdi, Tovey, Trick (1989a), BartholdiOrlin (1991), Bartholdi et al. (1992) brilliantly observed perhaps mightable make manipulation (i.e., strategic voting) control elections impossible, couldleast try make manipulation control computationally difficult neither voterselection organizers attempt it. example, way committees chair setelection within committee way favorite option guaranteedwin, chairs computational task would take million years, practical purposesmay feel chair prevented finding set-up.Since seminal work Bartholdi, Orlin, Tovey, Trick, large body researchdedicated study computational properties election systems. topics received much attention complexity manipulating elections (Conitzer & Sandholm, 2003,2006; Conitzer et al., 2007; Elkind & Lipmaa, 2005; Hemaspaandra & Hemaspaandra, 2007; Procaccia & Rosenschein, 2007; Meir, Procaccia, Rosenschein, & Zohar, 2008) controllingelections via procedural changes (Hemaspaandra et al., 2007a; Hemaspaandra, Hemaspaandra, &Rothe, 2007b; Meir et al., 2008; Erdelyi, Nowak, & Rothe, 2008b). Recently, Faliszewski, Hemaspaandra, Hemaspaandra (2006a) introduced study complexity bribery elections.Bribery shares features manipulation features control. particular, briberpicks voters wants affect (as voter control problems) asks votewishes (as manipulation). (For additional citations pointers, see recent survey Faliszewski, Hemaspaandra, Hemaspaandra, Rothe, 2009.)paper study Copeland elections respect computational complexitybribery procedural control; see Faliszewski, Hemaspaandra, Schnoor 2008 studymanipulation within Copeland .study election systems computational properties, complexitymanipulation, control, bribery problems, important topic multiagent systems.Agents/voters may different, often conflicting, individual preferences given alternatives (or candidates) voting rules (or, synonymously, election systems) provide useful methodagents come reasonable decision alternative choose. Thus electionsemployed multiagent settings also contexts solve many practical problems.examples, mention Ephrati Rosenschein (1997) use elections planning, Ghosh,278fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLMundhe, Hernandez, Sen (1999) develop recommender system movies basedvoting, Dwork, Kumar, Naor, Sivakumar (2001) use elections aggregate resultsmultiple web-search engines. multiagent setting may hundreds elections happeningevery minute cannot hope carefully check case whether party organizedelection attempted procedural change skew results. However, computationallyhard find procedural changes hope practically infeasible organizersundertake them.standard technique showing particular election-related problem (for example,problem deciding whether chair make favorite candidate winner influencingk voters cast votes) computationally intractable show NP-hard.approach taken almost papers computational social choice cited above,approach take paper. One justifications using NP-hardnessbarrier manipulation control elections multiagent settings attemptsinfluence elections outcome made computationally bounded software agentsneither human intuition computational ability solve NP-hard problems.Recently, Conitzer Sandholm (2006), Procaccia Rosenschein (2007), HomanHemaspaandra (to appear), McCabe-Dansted, Pritchard, Slinko (2008) studiedfrequency (or sometimes, probability weight) correctness heuristics voting problems. Although fascinating important direction, point remove need studyworst-case hardness. Indeed, view worst-case study natural prerequisite frequency-ofhardness attack: all, point seeking frequency-of-hardness results problemhand P begin with. one cannot even prove worst-case hardness problem,proving average-case hardness even beyond reach. Also, current frequency resultsdebilitating limitations (for example, locked specific distributions; depending unproven assumptions; adopting tractability notions declare undecidable problems tractablerobust even linear-time reductions). models arguably readyprime time and, contrary peoples impression, imply (and goalimplying, since studying frequency hardness) average-case polynomial runtime claims.Erdelyi, Hemaspaandra, Rothe, Spakowski (2007) Homan Hemaspaandra (to appear)provide discussions issues. Regarding recent work Friedgut, Kalai,Nisan (2008) (see also Xia Conitzer, 2008a, 2008b), interesting work control, lower bounds proven show one manipulate time,rather work provides lower bounds unfortunately go zero number votersincreases, case studied. course, limitations current results frequencyhardness surely mean direction interesting; clearly, field bestgo beyond limitations.1.3 Outline Resultsgoal paper study Copeland elections point view computational socialchoice theory, setting voters rational setting voters allowedirrational preferences. (Note: henceforward say irrational voters, meanvoters may irrational preferences, must.) study issues briberycontrol point reader Faliszewski et al. 2008 work manipulation. (Verybriefly summarized, work Faliszewski et al., 2008, manipulation Copeland elections279fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEshows rational , 0 < < 1, 6= 12 , coalitional manipulation problem unweightedCopeland elections, even coalitions two manipulators, NP-complete.constructions present paper adopted adapted paper order proveresults manipulation.)Bribery control problems natural real-life interpretations. example,presidential elections candidate might want encourage many supporterspossible vote (get-out-the-vote efforts): control via addition voters; elections heldinconvenient date group voters (e.g., holiday) hard-to-reach location (e.g.,requiring one car, getting location involves passing dangerous areas):control via deleting voters; one choose voting districts way favorable particular candidate party (gerrymandering): control via partitioning voters; one introduce new candidateelection hope steal votes away opponents ones favoritecandidate without affecting favorite candidates performance: control via adding candidates.control scenarios study also natural interpretations.Similarly, bribery natural important issue context elections. stress, however, bribery problems necessarily need correspond cheating sort illegalaction. One could view bribery problems as, example, problems finding minimum number voters switch result election and, thus, problems finding coalitions,especially one assigns prices voters measure difficulty convincing particular voterjoin coalition (see, e.g., Faliszewski, 2008, example bribery probleminterpretation natural).quite natural study control bribery constructive settings (where wantmake favorite candidate winner) destructive settings (where try prevent candidate winning). context real-life elections, one often hears voters speakingcandidate hope win, one also often hears voters expressing sentiment Anyonehim. constructive destructive settings correspond actions agents belonginggroups might interested in.One main achievements paper classify resistance vulnerability holds Copeland every previously studied control scenario rational value ,0 1. so, provide example control problem complexityCopeland0.5 (which system commonly referred Copeland) differsCopeland0 Copeland1 : latter two problems vulnerable constructive controladding (an unlimited number of) candidates, Copeland0.5 resistant control type (seeSection 2 definitions Theorem 4.10 result).fact, Copeland (i.e., Copeland0.5 ) first natural election system (with polynomial-timewinner problem) proven resistant every type constructive control proposedliterature date. result closes 15-year quest natural election system fully resistantconstructive control.22. referee wondered whether (and speculated that) virtually every common rule (other plurality Condorcet,said referee, although actually plurality displays breathtakingly many resistances itself, albeit constructive resistances) would display broad resistance control Copeland, one obtain resultsrules. course open issue, see reason think case (and approval votingalready provides counterexample, see Hemaspaandra et al., 2007a). even caserules resisted many control types, suspect pattern types resisted differ amongrules, although case four quadrants (of constructive/destructive voter/candidate seemoften stand fall block). pattern seems us something natural importance, since ones choice280fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLalso show Copeland resistant constructive destructive bribery,case rational voters case irrational voters. hardness proofs work caseunweighted voters without price tags (see Faliszewski et al., 2006a) thus, naturally, applywell involved scenarios weighted unpriced voters, unweighted priced voters,weighted priced voters.prove bribery results, introduce method controlling relative performancescertain voters way that, one sets restrictions appropriately, legal possibilitiesbribery actions sharply constrained. call approach UV technique, sincebased dummy candidates u v. proofs Theorems 3.2 3.4 particular applicationsmethod. feel UV technique useful, even beyond scope paper,analysis bribery election systems based head-to-head contests.also study Copeland elections flexible models microbribery (see Section 3.2) extended control (see Section 4.3). show Copeland (with irrational votersallowed) vulnerable destructive microbribery destructive candidate control via providingfairly simple greedy algorithms. contrast, polynomial-time algorithms constructive microbribery proven via technique involving min-cost network flows. best knowledge,first application min-cost flows election problems. believe range applicability flow networks election problems extends well beyond microbribery Copelandelections point reader recent, independent paper Procaccia, Rosenschein,Zohar (2008)3 paper Faliszewski (2008) examples applications.also mention study Copeland control noticed proofimportant result Bartholdi et al. (1992, Theorem 12), namely, Condorcet voting resistantconstructive control deleting voters, invalid. invalidity due proof centrallyusing nonstrict voters, violation Bartholdi et al.s (1992) (and our) model, invalidityseems potentially daunting impossible fix proof approach taken there. note alsoTheorem 14 paper similar flaw. Section 5 validly reprove claimedresults using techniques.mentioned Section 1.1, Copeland elections may behave quite differently dependingvalue tie-rewarding parameter . give concrete examples make case.Specifically, proofs results Copeland occasionally differ considerably distinct values, cases even computational complexity various control manipulationproblems (for manipulation case see Faliszewski et al., 2008) may jump P membershipNP-completeness depending . Regarding control, already noted Theorem 4.10shows control problem (namely, control adding unlimited number candidates)Copeland NP-complete rational 0 < < 1, yet Theorem 4.11 showscontrol problem P {0, 1}. give another example involving differentelection rule probably (along many factors influence rule choice) shapedstrength rule respect resisting types attacks one expects system faced with. example, Copeland exceedingly strongin fact, perfectwith respect constructive control types studied here.contrast, plurality, Condorcet, approval (Bartholdi et al., 1992; Hemaspaandra et al., 2007a), cantspeak issue yet unstudied systems. holds rules, suspect dream-casepath would find broad results one stroke reveal control-resistance patterns whole classes electionsystems. example, see Hemaspaandra Hemaspaandra 2007, essentially manipulationscoring systems.3. Procaccia et al. (2008) independently work Faliszewski et al. 2007 used similar technique workregarding complexity achieving proportional representation.281fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEcontrol problem, namely control partition candidates ties-eliminate tie-handling rule(see Section 2), note proofs Theorem 4.15 (which applies = 1 controlproblem within Copeland ) Theorem 4.16 (which applies rational 0 < 1problem) differ substantially. Regarding constructive microbribery, vulnerabilityconstructions = 0 (see Lemma 3.13) = 1 (see Lemma 3.15) significantly differother, neither works tie-rewarding values 0 1.remarks notwithstanding, results show possible obtain unifiedthough due uniformity sometimes rather involvedconstruction works Copelandevery rational , 0 1.1.4 Organizationpaper organized follows. Section 2, formalize notion elections particular Copeland elections, introduce useful notation, formally define controlbribery problems interested in. Section 3, show rational , 0 1,Copeland elections fully resistant bribery, case rational voters caseirrational voters. hand, one changes bribery model allow microbribesvoters (a fine-grained approach bribery, one changes voters vote,one pay voter), prove vulnerability rational , 0 1, irrationalvoters destructive case specific values irrational-voters constructive case.Sections 4.1 4.2, present results procedural control Copeland electionsrational 0 1. see broad resistance holds constructive-controlcases. Section 4.3 presents results fixed-parameter tractability bounded-case controlCopeland . Section 5 provides valid proofs several control problems Condorcet elections(studied Bartholdi et al., 1992) whose original proofs invalid due oddsmodel elections used Bartholdi et al. 1992. conclude paper brief summarySection 6 stating open problems.every proof included paper, would extremely long difficult read.Nonetheless, course important make proofs available claims. handled follows. made available Faliszewski, Hemaspaandra, Hemaspaandra,Rothe 2008b full technical report version paper, complete detailed proofs essentially every result. current article, proofs would repetitive tedious relativeproofs include here, simply included proofsinstead included place pointer detailed proof result full technical reportversion.2. Preliminariessection defines many notions use paper, various election systems,election problems, hardness notions.2.1 Elections: Systems Llull Copelandelection E = (C,V ) consists finite candidate set C = {c1 , . . . , cm } finite collection Vvoters, voter represented (individually, except later discuss succinct inputs)282fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLvia preferences candidates. election system (or election rule) ruledetermines winner(s) given election, i.e., mapping pairs (C,V ) subsets C.consider two ways voters express preferences. rational case (ourdefault case), voters preferences represented linear order set C,4 i.e.,voter vi preference list ci1 > ci2 > > cim , {i1 , i2 , . . . , im } = {1, 2, . . . , m}. irrational case, voters preferences represented preference table every unorderedpair distinct candidates ci c j C indicates whether voter prefers ci c j (i.e., ci > c j )prefers c j ci (i.e., c j > ci ).well-known election rules case rational voters plurality, Borda count,Condorcet. Plurality elects candidate(s) ranked first largest number voters.Borda count elects candidate(s) receive points, voter vi givescandidate c j many points number candidates c j preferred respect vipreferences. candidate ci Condorcet winner every candidate c j holds cipreferred c j majority voters. Note election instance oneCondorcet winner.paper, introduce parameterized version Copelands (1951) election system,denote Copeland , parameter rational number 0 1 specifiesties rewarded head-to-head majority-rule contests two distinct candidates.Definition 2.1 Let , 0 1, rational number. Copeland election, head-tohead contest two distinct candidates, candidate preferred majority votersobtains one point candidate obtains zero points, tie occurscandidates obtain points. Let E = (C,V ) election. c C, scoreE (c) (bydefinition) sum cs Copeland points E. Every candidate c maximum scoreE (c) (i.e.,every candidate c satisfying (d C)[scoreE (c) scoreE (d)]) wins.Let CopelandIrrational denote election system voters allowed irrational.mentioned earlier, literature term Copeland elections often usedsystem Copeland0.5 (e.g., Saari Merlin, 1996, Merlin Saari, 1997, rescaled versionConitzer et al., 2007), occasionally used Copeland0 (e.g., Procaccia et al., 2007,Faliszewski et al., 2007, early version paper). mentioned earlier, systemCopeland1 proposed Llull thirteenth century (see literature pointers givenintroduction) called Llull voting.define notation help discussion Copeland elections. Informally put,E = (C,V ) election ci c j two candidates C vsE (ci , c j ) meansurplus votes candidate ci c j . Formally, define notion follows.Definition 2.2 Let E = (C,V ) election let ci c j two arbitrary candidates C.Define relative vote-score ci respect c j0ci = c jvsE (ci , c j ) =k{v V | v prefers ci c j }k k{v V | v prefers c j ci }k otherwise.4. paper, take linear order mean strict total order. common convention within voting theory,see, e.g., book Austen-Smith Banks 2000. However, mention field mathematics termlinear order typically taken allow nonstrictness, i.e., allow ties.283fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHESo, ci defeats c j head-to-head contest E vsE (ci , c j ) > 0, tiedvsE (ci , c j ) = 0, c j defeats ci vsE (ci , c j ) < 0. (Throughout paper, defeats excludespossibility tie, i.e., defeats means (strictly) defeats. say ties-or-defeatswish allow tie suffice.) Clearly, vsE (ci , c j ) = vsE (c j , ci ). often speak, plural,relative vote-scores mean group results head-to-head contests particularcandidates.Let , 0 1, rational number. Definition 2.1 introduced scoreE (c), Copelandscore candidate c election E. Note candidate ci C,scoreE (ci ) = k{c j C | ci 6= c j vsE (ci , c j ) > 0}k+ k{c j C | ci 6= c j vsE (ci , c j ) = 0}k.particular, score0E (ci ) = k{c j C | ci 6= c j vsE (ci , c j ) > 0}k, score1E (ci ) = k{c jC | ci 6= c j vsE (ci , c j ) 0}k. Note highest possible Copeland scoreelection E = (C,V ) kCk 1.Recall candidate ci C Copeland winner E = (C,V ) c j C holdsscoreE (ci ) scoreE (c j ). (Clearly, elections one Copeland winner.)candidate ci Condorcet winner E score0E (ci ) = kCk 1, is, ci defeatscandidates head-to-head contests.many constructions presented upcoming proofs, use followingnotation rational voters.Notation 2.3 Within every election fix arbitrary order candidates. occurrencesubset candidates preference list means candidates listed respectfixed order. Occurrences mean except candidates listedreverse order.example, C = {a, b, c, d, e}, alphabetical order used, = {a, c, e}b > > means b > > c > e > d, b > > means b > e > c > > d.2.2 Bribery Control Problemsdescribe computational problems study paper. problems come twoflavors: constructive destructive. constructive version goal determine whether,via bribery control action type study, possible make given candidate winnerelection. destructive case goal determine whether possible preventgiven candidate winner election.Let E election system. case, E either Copeland CopelandIrrational ,, 0 1, rational number. bribery problem E rational voters definedfollows (Faliszewski et al., 2006a).Name: E -bribery E -destructive-bribery.Given: set C candidates, collection V voters specified via preference lists C,distinguished candidate p C, nonnegative integer k.Question (constructive): possible make p winner E election resulting (C,V )modifying preference lists k voters?284fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLQuestion (destructive): possible ensure p winner E election resulting(C,V ) modifying preference lists k voters?version problem elections irrational voters allowed defined exactly likerational one, difference voters represented via preference tablesrather preference lists, briber may completely change voters preference table unitcost. end present section, Section 2.2, describe variants based seekingmake p (or preclude p being) unique winner. Later paper study anothervariant bribery problemsa variant one allowed perform microbribes: bribescost depends preference-table entry change, briber pays separatelychange.Bribery problems seek change outcome elections via modifying reported preferences voters. contrast, control problems seek change outcome electionmodifying elections structure via adding/deleting/partitioning either candidates voters.formally defining control types, use following naming conventions corresponding control problems. name control problem starts election system used(when clear context, may omitted), followed CC constructive control DCdestructive control, followed acronym type control: AC adding (a limitednumber of) candidates, ACu adding (an unlimited number of) candidates, DC deletingcandidates, PC partition candidates, RPC run-off partition candidates, AVadding voters, DV deleting voters, PV partition voters. partitioningcases (PC, RPC, PV) two-stage elections, use tie-handling rules Hemaspaandra et al. (2007a) first-stage subelections two-stage elections. particular,partitioning cases, acronym PC, RPC, PV, respectively, followed acronymtie-handling rule used first-stage subelections, namely TP ties promote (i.e., winnersfirst-stage subelections promoted final round election) TE ties eliminate(i.e., unique winners first-stage subelections promoted final round election,one winner given first-stage subelection winner givenfirst-stage subelection subelection move candidates forward).formally define control problems. definitions due Bartholdi et al.(1992) constructive control Hemaspaandra et al. (2007a) destructive control.Let E election system. Again, E either Copeland CopelandIrrational ,, 0 1, rational number. describe control problems caserational preferences, irrational cases perfectly analogous, except replacing preferencelists preference tables.C ONTROL VIA DDING C ANDIDATESstart two versions control via adding candidates. unlimited version goalelection chair introduce candidates pool spoiler candidates makefavorite candidate winner election (in constructive case) prevent despisedcandidate winner (in destructive case). suggested name problem,unlimited version chair introduce subset spoiler candidates (none, some,legal options) election.Name: E -CCACu E -DCACu (control via adding unlimited number candidates).285fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEGiven: Disjoint sets C candidates, collection V voters specified via preferencelists candidates set C D, distinguished candidate p C.Question (E -CCACu ): subset E p winner E election votersV candidates C E?Question (E -DCACu ): subset E p winner E electionvoters V candidates C E?definition E -CCACu (using different notation) introduced Bartholdi et al. (1992).contrast control problems involving adding deleting candidates voters,adding candidates problem Bartholdi, Tovey, Trick introduce nonnegative integer kbounds number candidates (from set D) chair allowed add. feelasymmetry definitions well justified,5 thus define with-change-parameterversion control-by-adding-candidates problems, denote ACl (where lstands fact part problem instance limit number candidatesadded, contrast model Bartholdi et al., 1992, denote ACu ustanding fact number added candidates unlimited, least senselimited via separately input integer). with-parameter version long-studied caseAV, DV, DC, paper use AC synonymous ACl , thususe notation AC rest paper speaking ACl . suggest naturalregularization definitions hope version become normal versionadding-candidates problem study. However, caution reader earlier papersAC used mean ACu .present paper, obtain results ACl also ACu case, orderallow comparisons results paper earlier works.Turning mean AC (equivalently, ACl ), per definition E -CCAC(i.e., E -CCACl ) ask whether possible make distinguished candidate p winnerE election obtained adding k candidates spoiler candidate set D. (Notek part input.) define destructive version, E -DCAC (i.e., E -DCACl ), analogously.Name: E -CCAC E -DCAC (control via adding limited number candidates).Given: Disjoint sets C candidates, collection V voters specified via preferencelists candidates set C D, distinguished candidate p C, nonnegativeinteger k.Question (E -CCAC): subset E kEk k p winner E electionvoters V candidates C E?Question (E -DCAC): subset E kEk k p winner Eelection voters V candidates C E?5. Bartholdi et al. (1992) aware asymmetry. write: certain extent exact formalizationproblem matter taste. [. . . ] could equally well formalized [the problem control via addingcandidates] whether K fewer candidates added [. . . ] much matter problemsdiscuss, since versions complexity (Bartholdi et al., 1992). contrast, complexityproblems studied crucially hinges formalization used, thus define versions formally.286fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLC ONTROL VIA ELETING C ANDIDATESconstructive control via deleting candidates, chair seeks ensure favoritecandidate p winner election suppressing k candidates. destructive variantproblem, chairs goal block p winning suppressing k candidatesp.6Name: E -CCDC E -DCDC (control via deleting candidates).Given: set C candidates, collection V voters represented via preference lists C,distinguished candidate p C, nonnegative integer k.Question (E -CCDC): possible deleting k candidates ensure p winnerresulting E election?Question (E -DCDC): possible deleting k candidates p ensure pwinner resulting E election?C ONTROL VIA PARTITION RUN -O FF PARTITION C ANDIDATESBartholdi et al. (1992) gave two types control elections via partition candidates.cases candidate set C partitioned two groups, C1 C2 (i.e., C1 C2 = C C1 C2 = 0),/election conducted two stages. control via run-off partition candidates,elections first stage conducted separately group candidates, C1 C2 , groupwinners survive tie-handling rule compete second stage. controlvia partition candidates, first-stage election performed candidate set C1elections winners survive tie-handling rule compete candidates C2second stage.ties-promote (TP) model, first-stage winners within group promoted finalround. ties-eliminate (TE) model, first-stage winner within group promoted finalround unique winner within group.Although loosely correspond real-world settings, let us give rough example regarding case run-off partition candidates. Consider department, powerful director,trying decide among collection alternatives. certainly plausible director might announce divided candidates two groups, entire departmentwould vote separately among candidates group, candidatesmoved forward votes (under whatever tie-handling rule used,ties) would compete final election, entire department would vote. (How6. referee asked whether control adding candidates, redefined require adding certain numbercandidates instead least certain number candidates, cover forthcoming notion (whichstandard notion) control deleting (at certain number of) candidates. answer seemscase. Consider election thirty candidates ask whether certain constructive control goalreached via deleting five candidates. Note reframing twenty-candidate electionone tries reach goal adding least five candidates ten-candidate spoiler set doesnt make sense,one particular twenty-candidate election start; far many possibilities.referee similarly asked representing addition candidates new notion deleting candidates putlower bound number deletions, attempt seems also fail, case different reasondeletion case one might delete originally spoiler candidates one might delete candidatescore election addition case, allowed.287fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEconvincingly director could would course depend directors powerwell director could think justification partition candidates. Clearly partitions may easy justify, e.g., Lets regarding hire academic computer sciencedepartment first vote separately among fresh-Ph.D. candidates among senior hiringcandidates, may harder justify except executive fiat.)Name: E -CCRPC E -DCRPC (control via run-off partition candidates).Given: set C candidates, collection V voters represented via preference lists C,distinguished candidate p C.Question (E -CCRPC): partition C C1 C2 p winner twostage election winners subelection (C1 ,V ) survive tie-handling rulecompete winners subelection (C2 ,V ) survive tie-handling rule?subelection (in stages) conducted using election system E .Question (E -DCRPC): partition C C1 C2 p winnertwo-stage election winners subelection (C1 ,V ) survive tie-handling rulecompete winners subelection (C2 ,V ) survive tie-handling rule?subelection (in stages) conducted using election system E .description defines four computational problems given election system E :E -CCRPC-TE, E -CCRPC-TP, E -DCRPC-TE, E -DCRPC-TP. Note concept possible TE case candidates, due ties, eliminated first round here,case overall election would winner.Name: E -CCPC E -DCPC (control via partition candidates).Given: set C candidates, collection V voters represented via preference lists C,distinguished candidate p C.Question (E -CCPC): partition C C1 C2 p winner two-stageelection winners subelection (C1 ,V ) survive tie-handling rule competecandidates C2 ? subelection (in stages) conducted using electionsystem E .Question (E -DCPC): partition C C1 C2 p winnertwo-stage election winners subelection (C1 ,V ) survive tie-handling rulecompete candidates C2 ? subelection (in stages) conducted usingelection system E .description defines four computational problems given election system E :E -CCPC-TE, E -CCPC-TP, E -DCPC-TE, E -DCPC-TP.C ONTROL VIA DDING VOTERSscenario control via adding voters, chairs goal either ensure p winner (inconstructive case) ensure p winner (in destructive case) via causing k288fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLadditional voters participate election. chair draw voters add electionprespecified collection voters (with given preferences).loosely model real-world situations get-out-the-vote efforts. example,suppose campaign enough money volunteers drive one hundred setthousand car-less elderly people polling place, decide ones choose.Name: E -CCAV E -DCAV (control via adding voters).Given: set C candidates, two disjoint collections voters, V W , represented via preference lists C, distinguished candidate p, nonnegative integer k.Question (E -CCAV): subset Q, kQk k, voters W voters V Qjointly elect p C winner according system E ?Question (E -DCAV): subset Q, kQk k, voters W voters V Qelect p winner according system E ?reason unlimited control notion here, anywhere else exceptACu , ACu historically special case. seminal paper Bartholdi et al. 1992 definedaddition/deletion problems (only) limited version, number k limitingadditions/deletions, except paper, describing matter individual taste, definedaddition candidates (only) unlimited version. consider limited versionsaddition/deletion problems far natural, study those, Bartholdi, Tovey,Trick every case addition candidates. However, allow comparison earlierpapers, keep defined control type case ACu .C ONTROL VIA ELETING VOTERScontrol via deleting voters case chair seeks either ensure p winner (inconstructive case) prevent p winner (in destructive case) via blocking kvoters participating election.loosely models vote suppression. example, consider case given campaign afford send doors k voters smooth-talking operativedemoralize wont bother vote.Name: E -CCDV E -DCDV (control via deleting voters).Given: set C candidates, collection V voters represented via preference lists C,distinguished candidate p C, nonnegative integer k.Question (E -CCDV): possible deleting k voters ensure p winnerresulting E election?Question (E -DCDV): possible deleting k voters ensure p winnerresulting E election?289fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEC ONTROL VIA PARTITION VOTERScase control via partition voters, following two-stage election performed. First,voter set V partitioned two subcommittees, V1 V2 . winners election (C,V1 )survive tie-handling rule compete winners (C,V2 ) survive tie-handlingrule. Again, tie-handling rules TE TP (ties-eliminate ties-promote).control type bit harder others imagine real world, somewhatcontrived example, consider following case. given organization, director splitsworkers two study groups (and let us say choose partition likes, eitherpowerful director, good enough manager make justificationdivision) study problem propose thinks best alternative.entire organization comes together vote among alternatives chosen firstround (that survive tie-handling rule case ties).Name: E -CCPV E -DCPV (control via partition voters).Given: set C candidates, collection V voters represented via preference lists C,distinguished candidate p C.Question (E -CCPV): partition V V1 V2 p winner twostage election winners election (C,V1 ) survive tie-handling rule competewinners (C,V2 ) survive tie-handling rule? subelection (instages) conducted using election system E .Question (E -DCPV): partition V V1 V2 p winner twostage election winners election (C,V1 ) survive tie-handling rule competewinners (C,V2 ) survive tie-handling rule? subelection (instages) conducted using election system E .U NIQUE W INNERSRRATIONALITYbribery control problems defined rational votersnonunique-winner model, i.e., asking whether given candidate made, preventedbeing, winner. Nonetheless, proven control results case nonuniquewinners (to able fairly compare existing control results, uniquewinner model) unique winners (a candidate unique winner winnerwinner). Similarly, bribery results proven unique-winner model (toable fairly compare existing bribery results literature) nonunique-winnermodel. addition rational-voters case, also study problems case votersallowed irrational. mentioned earlier, case irrational voters, votersrepresented via preference tables rather preference lists.2.3 Graphsundirected graph G pair (V (G), E(G)), V (G) set vertices E(G)set edges edge unordered pair distinct vertices.7 directed graph defined7. paper, symbols E V generally reserved elections voters, except just-introduced overloading mean sets edges vertices given graph. intended meaning E V clearcontext, even proofs involve multiple elections graphs.290fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLanalogously, except edges represented ordered pairs. example, u vdistinct vertices undirected graph G G either edge e = {u, v} connects u vdoesnt. hand, G directed graph G either edge e = (u, v)u v, edge e = (v, u) v u, e e , neither e e .directed graph G, indegree vertex u V (G) number Gs edges enter u(i.e., number edges form (v, u) E(G)). Similarly, outdegree u V (G)number edges leave u (i.e., number edges form (u, v) E(G)).2.4 NP-Complete Problems ReductionsWithout loss generality, assume problems consider encoded natural,efficient way alphabet = {0, 1}. use standard notion NP-completeness, definedvia polynomial-time many-one reductions. say computational problem polynomial-timemany-one reduces problem B exists polynomial-time computable function f(x )[x f (x) B].problem NP-hard members NP polynomial-time many-one reduce it. Thus,NP-hard problem polynomial-time many-one reduces problem B, B NP-hardwell. problem NP-complete NP-hard member NP. clear contextuse reduce reduction shorthands polynomial-time many-one reducepolynomial-time many-one reduction.NP-hardness results typically follow via reduction either exact-cover-by-3-setsproblem vertex cover problem (see, e.g., Garey Johnson, 1979). wellknown NP-complete problems, define sake completeness.Name: X3C (exact cover 3-sets).Given: set B = {b1 , . . . , b3k }, k 1, family sets = {S1 , . . . , Sn } i,1 n, holds Si B kSi k = 3.Question: set {1, . . . , n}, kAk = k,iA Si= B?set ask problem called exact cover B. coverevery member B belongs Si A, exactmember B belongs exactly one Si A.Whenever consider instances X3C problem, assume well-formed,is, assume follow syntactic requirements stated Given field (e.g.,cardinality set B indeed multiple three). apply convention consideringsyntactically correct inputs problems well. Let computational problemlet x instance A. consider algorithm A, input x malformed,immediately reject. building reduction problem B,whenever hit malformed input x output fixed B. (In reductions B never, always possible.)Copeland elections often considered terms appropriate graphs. representationparticularly useful face control problems modify structure candidateset, since case operations election directly translate suitable operations291fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEcorresponding graph. candidate control problems, weinstead using reductions X3Cconstruct reductions vertex cover problem. vertex cover undirected graph Gsubset Gs vertices edge G adjacent least one vertex subset.Name: VertexCover.Given: undirected graph G nonnegative integer k.Question: set W W V (G), kW k k, every edge e E(G) holdse W 6= 0?/2.5 Resistance Vulnerabilityelection systems affected control type; not, system said immunetype control. example, candidate c Condorcet winner impossiblemake Condorcet winner adding candidates (see Bartholdi et al., 1992, Hemaspaandra et al., 2007a, immunity results). However, Copeland elections easysee type control defined Section 2.2 scenario outcomeelection indeed changed via conducting corresponding control action. electionsystem immune type control (as witnessed scenario), election systemsaid susceptible control type.Proposition 2.4 rational number , 0 1, Copeland susceptible typecontrol defined Section 2.2.say election system (Copeland CopelandIrrational , case) resistantparticular attack (be type control bribery) appropriate computational problemNP-hard susceptibility holds.8 hand, computational problem Psusceptibility holds, say system vulnerable attack. briberycontrol problems defined, vulnerability definition merely requires existpolynomial-time algorithm determines whether successful bribe control action existsgiven input. However, every single one vulnerability proofs provide somethingfar stronger. provide polynomial-time algorithm actually finds successful bribecontrol action input successful bribe control action exists, inputsuccessful bribe control action exists announce fact.notions resistance vulnerability (and immunity susceptibility) controlproblems election systems introduced Bartholdi et al. (1992), followdefinition alteration Hemaspaandra et al. (2007b) resistance NP-complete NPhard, change compelling (because old definition, NP-completeness, things could8. true unnatural election systems immunity bribery holds, e.g., election system Every candidate winner immune types bribery. However, Copeland -type systems susceptiblebribery types look paper, wont explicitly discuss state susceptibility briberycases.referee asked whether definition resistance could equivalently stated simply requiring appropriate computational problem NP-hard. seems yield notion, P 6= NP yetknown result, one doesnt know NP-hard problems cannot possibly P, subtly susceptibility defined terms changing outcomes corresponding control problems NP-hardness (whichpart determines resistance) related outcome (regardless started as).292fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLactually become nonresistant hard, natural). However, resistanceclaims paper NP-membership clear, NP-completeness fact hold.3. Briberysection present results complexity bribery Copeland election systems, rational number 0 1. main result, presentedSection 3.1, system resistant bribery, regardless voters rationalitymode operation (constructive versus destructive). Section 3.2, provide vulnerability results Llull Copeland0 respect microbribery.3.1 Resistance BriberyTheorem 3.1 rational , 0 1, Copeland CopelandIrrational resistantconstructive destructive bribery, nonunique-winner model unique-winnermodel.prove Theorem 3.1 via Theorems 3.2, 3.4, 3.5 below. proofs employ approachcall UV technique. constructive cases, technique proceeds constructingbribery instances briberies could possibly ensure favorite candidate pwinner involve voters rank group special candidates (often group containexactly two candidates, u v) p. remaining voters, bystanders speak,used create appropriate padding structure within election. destructive cases followvia cute observation regarding dynamics constructive cases.remainder section devoted proving Theorem 3.1. start caserational voters Theorems 3.2 3.4 argue analogous results caseirrational voters follow via, essentially, proof.Theorem 3.2 rational number , 0 1, Copeland resistant constructivebribery unique-winner model destructive bribery nonunique-winner model.Proof. Fix arbitrary rational number 0 1. proof provides reductionsX3C problem to, respectively, unique-winner variant constructive briberynonunique-winner variant destructive bribery. reductions differ regarding specification goal (i.e., regarding candidate attempt make unique winnercandidate prevent winner) thus describe jointly as, essentially, singlereduction.reduction produce instance appropriate bribery problem odd numbervoters, never ties head-to-head contests. Thus proof works regardlessrational number 0 1 chosen.Let (B, ) instance X3C, B = {b1 , b2 , . . . , b3k }, collection {S1 , S2 , . . . , Sn }three-element subsets B nj=1 j = B, k 1. input meet conditions output fixed instance bribery problem negative answer.Construct Copeland election E = (C,V ) follows. candidate set C {u, v, p} B,none u, v, p B. voter set V contains 2n + 4k + 1 voters following types.293fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE1. Si , introduce one voter type (i) one voter type (ii):(i) u > v > Si > p > B Si ,(ii) B Si > p > u > v > Si .2. introduce k voters types (iii)-1, (iii)-2, (iv)-1, (iv)-2:(iii)-1(iii)-2(iv)-1(iv)-2u > v > p > B,v > u > p > B,u > B > p > v,v > B > p > u.3. introduce single type (v) voter:(v) B > p > u > v.following relative vote-scores:1. vsE (u, v) = 2n + 1 2k + 1, inequality follows assumption(which implies n kBk/3 = k),Snj=1 j=B2. vsE (u, p) = vsE (v, p) = 2k 1,3. {1, 2, . . . , 3k}, vsE (u, bi ) = vsE (v, bi ) 2k + 1,4. {1, 2, . . . , 3k}, vsE (bi , p) = 1,5. i, j {1, 2, . . . , 3k} 6= j, |vsE (bi , b j )| = 1.example, see vsE (u, bi ) 2k + 1 {1, 2, . . . , 3k}, note bileast one j nj=1 j = B, voters types (i) (ii) give u advantageleast two votes bi . Furthermore, voters types (iii)-1, (iii)-2, (iv)-1, (iv)-2 give uadvantage 2k additional votes bi , single type (v) voter gives bi one-voteadvantage u. Summing up, obtain vsE (u, bi ) 2 + 2k 1 = 2k + 1. relativevote-scores similarly easy verify.relative vote-scores yield following Copeland scores upper bounds scores:1. scoreE (u) = 3k + 2,2. scoreE (v) = 3k + 1,3. {1, 2, . . . , 3k}, scoreE (bi ) 3k,4. scoreE (p) = 0.prove theorem, need following claim.Claim 3.3 following three statements equivalent:1. (B, ) X3C.294fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL2. Candidate u prevented winning via bribing k voters E.3. Candidate p made unique winner via bribing k voters E.Proof Claim 3.3. (1) implies (2): easy see (B, ) X3C bribeinvolving k fewer voters prevents u winner: enough bribe type (i)voters correspond cover size k report p top choice (while changinganything else preference lists): p > u > v > Si > B Si . Call resulting election E . Efollowing relative vote-scores change: vsE (p, u) = vsE (p, v) = n + k (n k) 2k + 1 = 1vsE (p, bi ) 1 {1, 2, . . . , 3k}, relative vote-scores remain unchanged.Thus scoreE (p) = 3k + 2, scoreE (u) = 3k + 1, scoreE (v) = 3k, scoreE (bi ) < 3k{1, 2, . . . , 3k}, p defeats candidates unique winner. particular, bribe(of k voters E) ensures u winner.(2) implies (3): Suppose bribe involving k fewer voters prevents uwinner. Note u defeats everyone except p 2k votes E. meansvia bribery k voters us score decrease one. Thus, prevent uwinner via bribery, need ensure u receives Copeland score 3k + 1candidate u gets Copeland score 3k + 2, is, candidate defeats everyone.Neither v bi possibly obtain Copeland score 3k + 2 via bribery,since bribery k voters affect head-to-head contests relative vote-scoresparticipants 2k. Thus, via bribery, u prevented winningp made (in fact, unique) winner election.(3) implies (1): Let W set k voters whose bribery ensures p uniquewinner election. Thus know kW k = k W contains voters ranku v p (as otherwise p would defeat u v), casevoters types (i), (iii)-1, (iii)-2. Furthermore, bribery makes p unique winnerensure p defeats members B; note type (iii)-1 (iii)-2 voters E already rankp B. Thus, via simple counting argument, W must contain exactly k type (i) voterscorrespond size-k cover B.Claim 3.3Since reduction computable polynomial time, Claim 3.3 completes proofTheorem 3.2.Theorem 3.4 rational , 0 1, Copeland resistant constructive briberynonunique-winner model destructive bribery unique-winner model.proof Theorem 3.4, follows general structure proofTheorem 3.2,9 reasons space nonrepetitiveness included foundfull TR version (Faliszewski et al., 2008b).9. Since proof Theorem 3.4 slightly involved, let us briefly mention key differences proofTheorem 3.2. Starting X3C instance (B, ) kBk = 3k, case construct election E = (C,V )two candidates (i.e., C = {p, s,t, u, v} B) V having, addition voter types similarproof Theorem 3.2, 20k normalizing voters eight subtypes. unique winner E s,candidate able prevent unique winner via k voters bribed p.construction ensures (B, ) X3C exactly k voters bribed p tie winner,simultaneously handles nonunique-winner constructive case unique-winner destructive case.295fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEproofs theorems interesting feature. discuss bribery,never rely fact voters rational. Thus allow voters irrationalform CopelandIrrational -bribery CopelandIrrational -destructive-bribery instances simply deriving voters preference tables voters preference lists given proofs.easy see proofs remain valid change; proofs assume bribedvoter, bribery, prefers p candidates, make assumptions(and, particular, use linearity preferences). Thus following corollaryproofs Theorems 3.2 3.4.Theorem 3.5 rational number , 0 1, CopelandIrrational resistant constructive bribery destructive bribery, nonunique-winner model uniquewinner model.Theorems 3.2, 3.4, Theorem 3.5 together constitute proof Theorem 3.1 establishrational , 0 1, Copeland CopelandIrrational possess broadessentiallyperfectresistance bribery regardless whether interested constructive destructiveresults. However, next section shows perfect picture is, fact, near-perfectconsider microbribes, dont allow changing complete preferences votersrather change results head-to-head contests candidates voters preferences.show efficient way finding optimal microbriberies case irrationalvoters Copeland elections.3.2 Vulnerability Microbribery Irrational Voterssection explore problems related microbribery irrational voters. standardbribery problems, considered Section 3.1, ask whether possible ensuredesignated candidate p winner (or, destructive case, ensure p winner)via modifying preference tables k voters. is, unit cost completelyredefine preference table voter bribed. model, pay service (namely,modification reported preference table) pay bulk (when buy voter,secured total obedience). However, sometimes may far reasonableadopt local approach pay separately preference-table entryflipto pay alter vote.Throughout remainder section use term microbribe refer flippingentry preference table, use term microbribery refer bribing possiblyirrational voters via microbribes. Recall irrational voters simply meanallowed have, must have, irrational preferences.study microbribery, consider irrational voters clearly natural modelstudy. all, one changing (and measuring overall change terms numberchanges in) pairwise preferences, changes easily move one rational preferenceirrational preference. (We mention passing one could define versions problemcase rational voters various ways, e.g., allowing changes stay rationalprofiles. seems far less natural model use microbribery problem.)rational , 0 1, define following two problems.Name: CopelandIrrational -microbribery CopelandIrrational -destructive-microbribery.296fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLGiven: set C candidates, collection V voters specified via preference tables C,distinguished candidate p C, nonnegative integer k.Question (constructive): possible, flipping k entries preference tablesvoters V , ensure p winner resulting election?Question (destructive): possible, flipping k entries preference tables votersV , guarantee p winner resulting election?flip multiple entries preference table voter, pay separately flip. microbribery problems CopelandIrrational similar flavorso-called bribery problems approval voting studied Faliszewski et al. (2006a),unit cost flipping approvals disapprovals voters paid. However, proofsCopelandIrrational seem much involved counterparts approval voting.reason CopelandIrrational elections allow subtle complicated interactionscandidates scores.proceed results, let us define notation useful throughoutsection. Let E election candidate set C = {c1 , c2 , . . . , cm } voter collection V ={v1 , v2 , . . . , vn }. define two functions, wincostE tiecostE , describe costs ensuringvictory tie given candidate particular head-to-head contest.Definition 3.6 Let E = (C,V ) election let ci c j two distinct candidates C.1. wincostE (ci , c j ) mean minimum number microbribes ensure ci defeatsc j head-to-head contest. ci already wins contest wincostE (ci , c j ) = 0.2. tiecostE (ci , c j ) mean minimum number microbribes ensure ci tiesc j head-to-head contest, E odd number voters thus tiesimpossible.first result regarding microbribery destructive microbribery easyCopelandIrrational . Since papers first vulnerability proof, take opportunityremind reader (recall Section 2.5) although definition vulnerability requirespolynomial-time algorithm determine whether successful action (in present case,destructive microbribery) exists, vulnerability proof provide something far stronger,namely polynomial-time algorithm determines whether successful action existsthat, so, finds successful action (e.g., flow algorithms later on, successful actionimplicit flow computed).Theorem 3.7 rational , 0 1, CopelandIrrational vulnerable destructive microbribery nonunique-winner model unique-winner model.Proof.Fix arbitrary rational number 0 1. give algorithmCopelandIrrational , destructive microbribery nonunique-winner model. (We omit analogous algorithm unique-winner case.)Let E = (C,V ) input election C = {p, c1 , c2 , . . . , cm } V = {v1 , v2 , . . . , vn },let k number microbribes allowed make. define predicate297fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEM(E, p, ci , k) true microbribery cost k ensures ciscore higher p. algorithm computes M(E, p, ci , k) ci C acceptstrue least one them. describe compute M(E, p, ci , k).10applying appropriate minimum-cost microbriberies E, obtain elections E1 , E2 , E3identical E except1. E1 , p defeats ci head-to-head contest,2. E2 , p loses ci head-to-head contest,3. E3 , p ties ci head-to-head contest (we disregard E3 number voters oddthus ties impossible).Let k1 , k2 , k3 minimum costs microbriberies transform E E1 , E E2 , EE3 , respectively. microbriberies involve head-to-head contest p ci .define predicate (E , p, ci , k ), E {E1 , E2 , E3 } k integer, truemicrobribery cost k involve head-to-head contestp ci ensures ci CopelandIrrational score higher ps. easy seeM(E, p, ci , k) (E1 , p, ci , k k1 ) (E2 , p, ci , k k2 ) (E3 , p, ci , k k3 ) .Thus enough focus problem computing (E , p, ci , k ).Let (E , k ) one (E1 , k k1 ), (E2 , k k2 ), (E3 , k k3 ). Define promoteE (ci , w , w ,t),ci C candidate w , w , nonnegative integers, minimum costmicrobribery that, applied E , increases ci CopelandIrrational score w + (1 )w +via ensuring1. ci wins additional w head-to-head contests candidates C {p} useddefeat ci originally,2. ci wins additional w head-to-head contests candidates C {p} ciused tie originally,3. ci ties additional head-to-head contests candidates C {p} used defeat cioriginally.microbribery exist set promoteE (ci , w , w ,t) . easyexercise see promoteE computable polynomial time simple greedy algorithm.define demoteE (ci , , ,t) minimum cost microbribery that, appliedelection E , decreases ps score + + (1 )t via ensuring1. p loses additional head-to-head contests candidates C {ci } p used defeatoriginally,2. p loses additional head-to-head contests candidates C {ci } p usedtie originally,10. stress optimized algorithm simplicity rather performance.298fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL3. p ties additional head-to-head contests candidates C {ci } p used defeatoriginally.microbribery exist set demoteE (ci , , ,t) . Note demoteEcomputed polynomial time using algorithm similar promoteE .Naturally, microbriberies used implicitly within promoteE (ci , w , w ,t ), withindemoteE (ci , , ,t ), within transforming E E disjoint, i.e., never involvepair candidates. Thus (E , p, ci , k ) true exist integersw , w , , ,t ,t {0, 1, . . . , m}scoreE (ci ) + (w + + (1 )(t + w ) + (t + )) scoreE (p) > 0promoteE (ci , w , w ,t ) + demoteE (ci , , ,t ) k.polynomially many combinations w , w , , ,t , , tryall. Thus given polynomial-time algorithm (E , p, ci , k ). Via observations givenbeginning proof implies M(E, p, ci , k) computable polynomial timeproof complete.destructive-case algorithm approach fairly straightforward; destructivecase need worry side effects promoting c demoting p. constructivecase complicated, still able obtain polynomial-time algorithms via fairlyinvolved use flow networks model particular points shift candidates.remainder section restrict values {0, 1} settings numbervoters odd ties never happen. remind reader Copeland1 Copeland1Irrational ,respectively, refer Llull voting.flow network network nodes directed edges want transportamount flow source sink (these two designated nodes). edge ecarry c(e) units flow, transporting unit flow e costs a(e).min-cost-flow problem target flow value F, goal find way transporting Funits flow source sink, minimizing cost. (If way achievingtarget flow F, cost effect infinite.)define notions related flow networks formally. Let N = {0, 1, 2, . . .}Z = {. . . , 2, 1, 0, 1, 2, . . .}.Definition 3.81. flow network quintuple (K, s,t, c, a), K set nodesincludes source sink t, c : K 2 N capacity function, : K 2 Ncost function. assume c(u, u) = a(u, u) = 0 node u K,one c(u, v) c(v, u) nonzero pair distinct nodes u, v K. also assumec(u, v) = 0 a(u, v) = 0 well.2. Given flow network (K, s,t, c, a), flow function f : K 2 Z satisfies followingconditions:(a) u, v K, f (u, v) c(u, v), i.e., capacities limit flow.299fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE(b) u, v K, f (u, v) = f (v, u).11(c) u K {s,t}, vK f (u, v) = 0, i.e., flow conserved nodesexcept source sink.3. value flow f is:flowvalue( f ) =f (s, v).vKparticular flow network mind always clear contextindicate explicitly (we write explicitly subscript function flowvalue).4. cost flow f defined as:flowcost( f ) =a(u, v) f (u, v).u,vKis, pay price a(u, v) unit flow passes node u node v.Given flow network (K, s,t, c, a) often use term edges refer pairs distinctnodes (u, v) K 2 c(u, v) > 0.define min-cost-flow problem, well known literature.definition employ general one suffice needs. (Readersseeking broader discussion problem may wish see, example, monograph Ahuja,Magnanti, Orlin, 1993.)Definition 3.9 define min-cost-flow problem follows: Given flow network (K, s,t, c, a)target flow value F, find flow f value F (if one exists) minimum cost amongflows, otherwise indicate flow f exists.min-cost-flow problem polynomial-time algorithm.12 large body workdevoted flow problems even attempt provide complete list referenceshere. Instead, point reader excellent monograph Ahuja et al. 1993,provides descriptions polynomial-time algorithms, theoretical analysis, numerous referencesprevious work flow-related problems. also mention issue flows prevalentstudy algorithms textbook Cormen, Leiserson, Rivest, Stein 2001, page 787,contains exposition min-cost-flow problem.Coming back study constructive microbribery Llull Copeland0 , irrationalvoters allowed, present following result.Theorem 3.10 {0, 1}, CopelandIrrational vulnerable constructive microbribery,nonunique-winner model unique-winner model.11. Note flow fully defined via nonnegative values. Whenever speak flow (e.g., definingparticular flows) speak nonnegative part.12. min-cost-flow problem often defined terms capacity cost functions necessarily limitednonnegative integer values corresponding flows restricted integer values either. However,crucially us, known capacity cost functions integral values (as assumed)exist optimal solutions min-cost-flow problem use integer-valued flows foundpolynomial time.300fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLEdgee = (s, ci ),ci Ce = (ci , c j ),ci , c j C vsE (ci , c j ) > 0e = (c0 ,t)e = (ci ,t),> 0 ci Cevery edge eParametersc(e) = scoreE (ci )a(e) = 0c(e) = 1a(e) = wincostE (c j , ci )c(e) =a(e) = 0c(e) =a(e) = Bc(e) = 0a(e) = 0Figure 1: Edge capacities costs min-cost-flow instance I(T ), built election E.prove Theorem 3.10 via Lemmas 3.11 3.16 below, cover three cases: (a)odd number voters, CopelandIrrational elections 0 1 identical duelack ties, (b) Copeland1Irrational even number voters, (c) Copeland0Irrationaleven number voters. lemmas discuss nonunique-winner model caseeasy see change algorithms proofs make work unique-winnermodel.Lemma 3.11 rational 0 1, polynomial-time algorithm solvesconstructive microbribery problem CopelandIrrational elections odd number voters(in nonunique-winner model).Proof. input nonnegative integer k (the budget) election E = (C,V ),candidate set C {c0 , c1 , . . . , cm }, number voters odd, p = c0 candidate whosevictory want ensure via k microbribes. Note interchangeably use p c0refer candidate, since sometimes convenient able speak pcandidates uniformly. number voters odd, ties never occur. Thus candidate ciCopelandIrrational score rational value , 0 1. Fix arbitrary .give polynomial-time algorithm constructive microbribery problem. high-leveloverview try find threshold value microbribery costk transforms E E (a) p scoreE exactly , (b) every candidatescoreE .Let B number greater cost possible microbribery within E (e.g.,B = kV k kCk2 + 1). possible threshold , consider min-cost-flow instance I(T )node set K = C {s,t}, source sink, edge capacities costsspecified Figure 1, target flow valueF=scoreE (ci ) =ci C301kCk(kCk 1).2fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEVoter 1 :Voter 2 :Voter 3 :vsE (ci , c j )c0c1c2c3c0 > c1 > c2 > c3c3 > c2 > c1 > c0c2 > c0 > c3 > c1c00111c11011c21101c31110Figure 2: Sample election E Example 3.12 proof Lemma 3.11.(2, 0)(0, 0)c0(T, 0)(1, 1)(1, 1)(1, 1)c1(T, 49)(1, 1)(3, 0)(T, 49)c2(1, 1)(1, 0)(1, 1)c3(T, 49)Figure 3: Flow network I(T ) corresponding instance (E, c0 , k) Example 3.12.Example 3.12 illustration, consider following example. Suppose given election Efour candidates three voters, preference tables voters (who happenrational example) obtained preference orders shown Figure 2,also gives corresponding values vsE (ci , c j ) pair candidates. ThusscoreE (c0 ) = 2, scoreE (c1 ) = 0, scoreE (c2 ) = 3, scoreE (c3 ) = 1. Supposeallowed perform one microbribe, k = 1. Clearly, one microbribe changes preferencethird voter c2 > c0 c0 > c2 flip outcome head-to-head contest c2winning c0 winning, enough reach goal making c0 win election,course cheapest possible successful microbribery. Finally, note exampleB = 49.threshold 0 3, flow network I(T ) corresponding instance(E, c0 , k) constructive microbribery problem shown Figure 3, target flowvalue F = 6. Every edge e flow network labeled pair (c(e), a(e)) numbersgive capacity cost edge e, respectively.continue proof Lemma 3.11, note odd number voters, constructivemicrobribery CopelandIrrational simply requires us choose pairs distinct candidateswant flip outcome head-to-head contest order ensure ps victory. Thussufficient represent microbribery collection pairs (ci , c j ) distinct candidatesneed flip result head-to-head contest ci winning c j winning. Clearly,302fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLgiven collection M, cheapest way implement costswincostE (c j , ci ).(ci ,c j )Mcrucial observation algorithm directly translate flows microbriberiesusing following interpretation. Let f flow (as per Definition 3.8 edge flows integers) value F within instance I(T ). units flow travel network correspondCopelandIrrational points. ci C, interpret amount flow goes directlyci number CopelandIrrational points ci microbribery attempted,13amount flow goes directly ci number CopelandIrrational pointsci microbribery (defined flow). units flow travel distinct ci(i.e., edges form (ci , c j ), 6= j) correspond microbribes exerted: unit flowtraveling node ci c j corresponds changing result head-to-head contestci c j ci winning c j winning. case, CopelandIrrational point moves cic j cost flow increases a(ci , c j ) = wincost(c j , ci ), exactly minimum costmicrobribery flips contests result. Let f microbribery defined, described,flow f . easy seeflowcost( f ) = B (F f (c0 ,t)) +wincostE (c j , ci ).(ci ,c j )M fThus easily extract cost microbribery f cost flow f .algorithm crucially depends correspondence flows microbriberies.(Also, proofs Lemmas 3.14 3.16 cover case even number voterssimply show modify instances I(T ) handle ties, show correspondencesnew networks microbriberies; rest proofs here.)Note small values flow value F exists I(T ). reasonedges coming sink might enough capacity hold flow value F.situation impossible guarantee every candidate gets points;many CopelandIrrational points distribute.Figure 4 gives algorithm constructive microbribery CopelandIrrational . algorithmruns polynomial time since, already mentioned, min-cost-flow problem solvablepolynomial time.Let us prove algorithm correct. presented flow f valueF within flow network I(T ) (with 0 F) defines microbribery. Based this, clearalgorithm accepts microbribery cost k ensures ps victory.hand, suppose exists microbribery cost k ensuresps victory election. show algorithm accepts case.Let minimum-cost bribery (of cost k) ensures ps victory. pointedabove, represented collection pairs (ci , c j ) distinct candidates flipresult head-to-head contest ci winning c j winning. costwincostE (c j , ci ).(ci ,c j )M13. Note ci C flow value F within I(T ) needs send exactly scoreE (ci ) units ci .303fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEprocedure CopelandIrrational -odd-microbribery(E = (C,V ), k, p)beginp winner E accept;;F = ci C scoreE (ci ) = kCk(kCk1)2= 0 kCk 1beginbuild instance I(T ) min-cost-flow;I(T ) flow value Frestart loop next value ;f = minimum-cost flow I(T );f (c0 ,t) < restart loop;= flowcost( f ) B (F );k accept;end;reject;endFigure 4: constructive microbribery algorithm CopelandIrrational elections odd number voters.Since applying microbribery ensures p winner, candidate amongc1 , c2 , . . . , cm many CopelandIrrational points p does. Let E electionresults E applying microbribery E (i.e., flipping results contestsspecified optimal way, given wincostE ). Let scoreE (p), ps CopelandIrrationalscore implementing M. Clearly, 0 kCk 1.Consider instance I(T ) let fM flow corresponds microbribery M.flow edge form (s, ci ) carries flow maximum capacity, scoreE (ci ), edgeform (ci , c j ) carries one unit flow exactly e listed carries zero units flowotherwise, edge form (ci ,t) carries scoreE (ci ) units flow. easy seelegal flow. cost fMflowcost( fM ) = B (F ) +wincostE (c j , ci ).(ci ,c j )Mapplying M, p gets CopelandIrrational points travel sink via edge (c0 ,t) costa(c0 ,t) = 0, remaining F points travel via edges (ci ,t), {1, 2, . . . , m}, costa(ci ,t) = B. remaining part flowcost( fM ) cost units flow travelingedges (ci , c j ) directly correspond cost microbribery M.consider minimum-cost flow fmin I(T ). Since fM exists, minimum-cost flowmust exist well. Clearly,flowcost( fmin ) flowcost( fM ).Let number units flow fmin assigns travel edge (c0 ,t), i.e.,= fmin (c0 ,t). edges nonzero cost sending flow304fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLset {(ci , c j ) | ci , c j C vsE (ci , c j ) > 0} {(ci ,t) | {1, . . . , m}} thus cost fminexpressed (recall vsE (ci , c j ) > 0 implies 6= j)flowcost( fmin ) = B (F ) +fmin (ci , c j ) wincostE (c j , ci ).ci ,c j CvsE (ci ,c j )>0holds (1) B > i, j,i6= j wincostE (ci , c j ), (2) ci , c j C vsE (ci , c j ) > 0fmin (ci , c j ) {0, 1}, (3) flowcost( fmin ) flowcost( fM ). = must hold14 mustholdfmin (ci , c j ) wincostE (c j , ci )wincostE (c j , ci ).(ci ,c j )Mci ,c j CvsE (ci ,c j )>0Thus flow fmin corresponds microbribery guarantees ps victory costhigh M. Since chosen minimum cost among microbriberies, flowfmin corresponds microbribery minimum cost algorithm correctly accepts withinloop Figure 4, latest loop set .turn algorithms showing Llull Copeland0 , irrational voters allowed,vulnerable constructive microbribery number voters even. needtake account sometimes desirable candidates tiehead-to-head contest one win contest. handle cases LlullCopeland0 separately, case proofs follow general structure. casefirst provide lemma restricts set microbriberies model, use slightlymodified version algorithm Theorem 3.11, modified set min-cost-flow instances,solve thus limited microbribery problem. omit proofs remaining four lemmassection somewhat lengthy repetitive. However, proofs foundfull TR version paper (Faliszewski et al., 2008b).Lemma 3.13 Let E = (C,V ) election candidate set C = {c0 , c1 , . . . , cm }even number voters, specified via preference tables C. election conducted using Copeland0Irrational minimum-cost microbribery ensures victory c0 involves either(a) flipping result head-to-head contest two distinct candidates ci , c j C {c0 }ci winning c j winning, (b) changing result head-to-head contest twodistinct candidates C {c0 } tie one winning.14. Let us explain = . I(T ), definition, c(c0 ,t) = , know = fmin (c0 ,t) .show = . sake contradiction, let us assume < .flowcost( fmin )=B (F ) +fmin (ci , c j ) wincostE (c j , ci )ci ,c j CvsE (ci ,c j )>0B (F ) + B +fmin (ci , c j ) wincostE (c j , ci )ci ,c j CvsE (ci ,c j )>0>B (F ) +wincostE (c j , ci )(ci ,c j )M=flowcost( fM ),last inequality follows fact B greater cost microbribery within E.reached contradiction, since fmin minimum-cost flow I(T ). Thus = .305fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHELemma 3.14 polynomial-time algorithm solves constructive microbribery problem Copeland0Irrational elections even number voters (in nonunique-winner model).Lemma 3.15 Let E = (C,V ) election candidate set C = {c0 , c1 , . . . , cm }even number voters, specified via preference tables C. election conducted usingCopeland1Irrational minimum-cost microbribery ensures victory c0 involves obtainingtie head-to-head contest two distinct candidates C {c0 }.Lemma 3.16 polynomial-time algorithm solves constructive microbribery problem Copeland1Irrational elections even number voters (in nonunique-winner model).Together, Theorem 3.7 Lemmas 3.11, 3.14, 3.16 show that, particular,Copeland1Irrational Copeland0Irrational vulnerable microbribery, constructivedestructive settings. interesting note microbribery proofs would workwell considered slight twist definition microbribery problem, namely, insteadsaying flip voters preference table unit cost would allow voterpossibly different price flipping separate entry preference table. changewould affect computing values functions wincost tiecost (or, strictly speaking, analogues priced setting). (Technically, would also modify Lemmas 3.133.15, unit-cost setting say optimal microbribery never involves certain specified pairs candidates, whereas priced setting would need rephrase stateexist optimal microbriberies involve specified pairs candidates.)interesting direction study complexity bribery within Copeland systemsconsider version microbribery problem case rational voters. There, one wouldpay unit cost switch two adjacent candidates given voters preference list.CopelandIrrational , would also like know complexity constructive microbriberyrational number strictly 0 1. network-flow-based approachseem generalize easily values strictly 0 1 (when number voters even)flow network hard split unit flow tie. promising approach wouldseveral units flow model one CopelandIrrational point (e.g., case = 12 couldtry use two units flow model single Copeland0.5 point), seems difficult(if impossible) find edge costs appropriately model microbribery. (It possiblerestricted setting, namely = 12 exactly two votersbribed.) Also, results regarding hardness manipulation Faliszewski et al. (2008) suggestmicrobribery strictly 0 1 might NP-hard. However, again, nontrivialtranslate reduction world microbribery.related note, Kern Paulusma (2001) shown following problem,call SC(0, , 1), NP-complete. Let rational number 0 < < 1 6= 21 .given undirected graph G = (V (G), E(G)), vertex u V (G) assignedrational value cu form + j , nonnegative integers j. question,rephrased state terms (a variant of) notion Copeland , whether possible(possibly partially) orient edges G vertex u V (G) holds usCopeland score cu . Here, Copeland score vertex u mean, natural,number vertices u defeats (i.e., number vertices v directed edgeu v) plus times number vertices u ties (i.e., number verticesundirected edge u v).306fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLProblem SC(0, , 1) closely related microbribery problem. However,see immediate reduction SC(0, , 1) microbribery. natural approach wouldembed graph G election (in sense explored Section 4) waypreferred candidate p become winner, via microbribery, possibleorient edges G way respecting constraints defined values cu (for uV (G)). would, course, set budget microbribery high enough allowmodifying edges G none edges outside G. However, difficult.proof Kern Paulusma uses values cu implemented via using tied headto-head contests. agent performing microbribery could, potentially, affect head-to-headcontests, thus spoiling reduction.4. Controlsection focus complexity control Copeland elections. control problemstrying ensure preferred candidate p winner (or, destructive case,despised candidate winner) given election via affecting elections structure (namely,via adding, deleting, partitioning either candidates voters). contrast bribery problems,control problems never allowed change votes and, consequently, issuesencounter proof techniques use quite different presentedprevious section. reason previously standard type control resistanceresult rational-voters case implies analogous resistance result irrational-voters case,vulnerability result irrational-voters case implies analogous vulnerability resultrational-voters case.literature regarding complexity control problems large. bestknowledge, election systems comprehensive analysis conducted previously plurality, Condorcet, (variants of) approval voting (see Bartholdi et al., 1992; Hemaspaandra et al., 2007a, 2007b; Betzler Uhlmann, 2008; Erdelyi et al., 2008b; see also Meir et al.,2008, results (variants of) approval voting, single nontransferable vote, cumulativevoting respect constructive control via adding voters). Among plurality, Condorcet, (thestandard variant of) approval voting, plurality appears least vulnerable controlnatural compare new results plurality. However, mention passingHemaspaandra et al. (2007b) show construct hybrid election systems resistantstandard types control (including AC ACu ; AC discussed proven Hemaspaandra et al., 2007bthe AC ACu mention techniques clearlyhandle without problem). (It also noted hybrid systemsdesigned natural systems applied real-world elections rather purposeprove certain impossibility theorem impossible.)main result section Theorem 4.1.Theorem 4.1 Let rational number 0 1. Copeland elections resistantvulnerable control types indicated Table 1 nonunique-winner modelunique-winner model, rational irrational voter model.particular, prove section notion widely referred literaturesimply Copeland elections, clarity call Copeland0.5 , possesses tenbasic types (see Table 1) constructive resistance (and addition, even constructive ACu307fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEControl typeACuACDCRPC-TPRPC-TEPC-TPPC-TEPV-TEPV-TPAVDVCopeland=00< <1CC DC CC DCVVRVRVRVRVRVRVRVRVRVRVRVRVRVRRRRRRRRRRRRRRRRPlurality=1CC DCVVRVRVRVRVRVRVRRRRRRRRCCRRRRRRRVRVVDCRRRRRRRVRVVTable 1: Comparison control results Copeland elections, 0 1rational number, plurality-rule elections. R means resistance particular controltype V means vulnerability. results regarding plurality due Bartholdi et al.(1992) Hemaspaandra et al. (2007a). (Note CCAC DCAC resistance resultsplurality, handled explicitly Bartholdi et al., 1992, Hemaspaandra et al., 2007a,follow immediately respective CCACu DCACu results.)resistance). (As consider AC basic ACu , see discussion Control viaAdding Candidates subpart Section 2.2. Nonetheless, obtain ACu results, fansnaturalness ACu know things fare control type.) establishnotion literature occasionally referred Copeland elections, namelyCopeland0 , well Llull elections, denoted Copeland1 , possess tenbasic types constructive resistance. However, show Copeland0 Copeland1vulnerable eleventh type constructive control, incongruous historically resonantnotion constructive control adding unlimited number candidates (i.e., CCACu ).Note Copeland0.5 higher number constructive resistances, three, evenplurality, paper reigning champ among natural election systemspolynomial-time winner-determination procedure. (Although results regarding pluralityTable 1 stated unique-winner version control, tables Copeland cases,0 1, results hold cases unique winners nonunique winners,regardless two winner models one finds natural, one know holdsmodel.) Admittedly, plurality perform better respect destructive candidate controlproblems, still study Copeland makes significant steps forward quest fullycontrol-resistant natural election system easy winner problem.Among systems polynomial-time winner problem, Copeland0.5 indeedCopeland , 0 < < 1have resistances currently known natural election system whose voters vote giving preference lists. mention work, Erdelyi et al.(2008b) shown variant variant approval voting proposed Brams Sanver (2006)a certain rather subtle election system richer voter preference type (each voter308fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLspecifies permutation set) combines approval preference-based votinghasnineteen (out possible twenty-two) control resistances.section organized follows. next two sections devoted proving Theorem 4.1,Section 4.3 considers case control elections bounded number candidatesvoters. particular, Section 4.1 focuses upper part Table 1 studies control problemsaffect candidate structure. Section 4.2 devoted voter control covers lower partTable 1. Finally, Section 4.3 study fixed-parameter complexity control problems.particular, take role someone tries solve in-general-resistant control problemsdevise efficient algorithms case number candidates numbervoters bounded.resistance results regarding candidate control follow via reductions vertex covervulnerability results follow via greedy algorithms. resistance results casecontrol modifying voter structure follow reductions X3C problem.4.1 Candidate Controlstart discussion candidate control Copeland results destructive control.somewhat disappointing rational , 0 1, Copeland vulnerable typedestructive candidate control. positive side, vulnerability proofs follow via naturalgreedy algorithms allow us smoothly get spirit candidate-control problems.4.1.1 ESTRUCTIVE C ANDIDATE C ONTROLresults destructive control adding deleting candidates use following observation.Observation 4.2 Let (C,V ) election, let rational number 0 1.every candidate c C holdsscore(C,V ) (c) =score({c,d},V ) (c).dC{c}Theorem 4.3 rational number 0 1, Copeland vulnerable destructivecontrol via adding candidates (both limited unlimited, i.e., DCAC DCACu ),nonunique-winner model unique-winner model, rational irrationalvoter model.Proof. input set C candidates, set spoiler candidates, collection V voterspreferences (either preference lists preference tables) C D, candidate p C,nonnegative integer k (for unlimited version problem let k = kDk). ask whethersubset kD k k p winner (is unique winner)Copeland election E = (C ,V ). Note k = 0, amounts determining whether pwinner (is unique winner) election E, easily done polynomial time.remainder proof assume k > 0. Let c candidate (C D){p}. define a(c) maximum value expressionscore(CD ,V ) (c) score(CD ,V ) (p)309fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEconditions D, c C , kD k k. Observation 4.2 followsa(c) maximum valuescore(C{c},V ) (c) score(C{c},V ) (p) +score({c,d},V ) (c) score({p,d},V ) (p)dD {c}conditions D, c C , kD k k.Clearly, p prevented winner (a unique winner) existscandidate c (C D) {p} a(c) > 0 (such a(c) 0).Given candidate c (C D) {p}, easy construct polynomial time set D,kD k k, yields value a(c). start = 0./ c D, add c . addcandidates score({c,d},V ) (c) score({p,d},V ) (p) positive, startingvalue highest, kD k = k candidates exist.Theorem 4.4 rational number 0 1, Copeland vulnerable destructive control via deleting candidates (DCDC), nonunique-winner model uniquewinner model, rational irrational voter model.proof Theorem 4.4 similar Theorem 4.3, includeinstead refer full TR version (Faliszewski et al., 2008b).Destructive control via partitioning candidates (with without run-off) also easy. Sincearguments proof involved, present here.Theorem 4.5 rational number 0 1, Copeland vulnerable destructivecontrol via partitioning candidates via partitioning candidates run-off (inTP TE model, i.e., DCPC-TP, DCPC-TE, DCRPC-TP, DCRPC-TE), nonuniquewinner model unique-winner model, rational irrational voter model.Proof. easy see TP model, p prevented winner via partitioning candidates (with without run-off) set C C p Cp winner (C ,V ). follows p prevented winnerp prevented winner deleting kCk 1 candidates,determined polynomial time Theorem 4.4. show handle unique-winnervariants DCPC-TP DCRPC-TP later proof.TE model, easy see set C C p C punique winner (C ,V ) p prevented unique winner via partitioningcandidates (with without run-off). One simply partitions candidates C C Cthus p fails advance final stage. hand, p preventedwinner (a unique winner) via partitioning candidates (with without run-off) TE model,exists set C C p C p unique winner (C ,V ).either p advance final stage (and means p uniquewinner first-stage election) p winner (not unique winner) final stage(note winner implies unique winner).Thus, p prevented winner (a unique winner) via partitioning candidates(with without run-off) TE model set C C p C p310fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLunique winner (C ,V ). Clearly, set exists p preventedunique winner via deleting kCk 1 candidates, Theorem 4.4 testedpolynomial time.remains show Copeland vulnerable destructive control via partitioning candidates (with without run-off), rational irrational voter model, uniquewinner model TP tie-handling rule. argument focus DCRPC-TPcase easy see essentially reasoning works DCPC-TP.First determine whether p precluded winner current controlscenario. done polynomial time explained above. p precludedwinner, p certainly precluded unique winner, done.remainder proof, suppose p cannot precluded winner currentcontrol scenario, i.e., every set C p D, p winner (D,V ). LetD1 = {c C {p} | p defeats c head-to-head contest}let D2 = (D1 {p}). Note c D2 , p ties c head-to-head contest, sinceotherwise p would winner ({c, p},V ). D2 = 0,/ p Condorcet winnerpartition (with without run-off) prevent p unique winner (Hemaspaandraet al., 2007a). remainder proof, assume D2 6= 0./ show pprecluded unique winner current control scenario.< 1, let first subelection (D1 {p},V ). Note p unique winnersubelection. final stage election involves p one candidates D2 . Noteevery pair candidates D2 {p} tied head-to-head election (since c would defeathead-to-head election, c would unique winner ({c, d, p},V ), contradictsassumption p winner every subelection participates in). follows candidatesparticipate final stage election winners, p unique winner.Finally, consider case = 1. score(C,V ) (p) = kCk 1. candidateC {p} score(C,V ) (d) = kCk 1, always (i.e., every subelection containing d) winner, thus p unique winner final stage election,regardless partition C chosen. suppose score(C,V ) (d) < kCk 1C {p}. score(C,V ) (d) kCk 2 C {p}. Let c candidate D2let first subelection (C {c},V ). Let C set winners (C {c},V ). Sincescore(C{c},V ) (p) = kCk 2, holds p C every C {p}, score(C{c},V ) (d) =kCk 2. Since score(C,V ) (d) kCk 2, follows c defeats head-to-head election.final stage election involves candidates C {c}. Note score(C {c},V ) (c) = kC k, thusc winner election, precluded p unique winner.vulnerability results case destructive candidate control contrastedessentially perfect resistance constructive candidate control (with exception constructive control via adding unlimited number candidates Copeland {0, 1})shown Section 4.1.3. first, Section 4.1.2, provide technical prerequisites.311fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE4.1.2 C ONSTRUCTING NSTANCES E LECTIONSMany proofs next section require constructing fairly involved instances Copelandelections. section provide several lemmas observations simplify buildinginstances.first note election E = (C,V ) induces directed graph G(E) whose verticesEs candidates whose edges correspond results head-to-head contests E.is, two distinct vertices G(E) (i.e., two distinct candidates), b,edge b defeats b head-to-head contest (i.e.,vsE (a, b) > 0). Clearly, G(E) depend value . following fundamental resultdue McGarvey. result allows us basically identify elections election graphsproofs resistance candidate control. effect, Copeland candidate-control problemsoften viewed (with care regarding ties) graph-theoretic problems.Lemma 4.6 (McGarvey, 1953) polynomial-time algorithm given input antisymmetric directed graph G outputs election E G = G(E).Proof. sake completeness, give algorithm. Let G antisymmetric directedgraph. algorithm computes election E = (C,V ), C = V (G) edge (a, b)G exactly two voters, one preference list > b > C {a, b} one preferencelist C {a, b} > > b. Since G antisymmetric, easy see G = G(E).basic construction McGarvey improved upon Stearns (1959). McGarveys construction requires twice many voters edges G, constructionStearns needs kV (G)k + 2 voters. Stearns also provides lower bound numbervoters needed represent arbitrary graph via election. (It easy seegraph modeled via two irrational voters lower bound case rational votessomewhat harder.)often construct complicated elections via combining simpler ones (see, particular,rather involved proofs Theorems 4.12 4.16 found full TR version,Faliszewski et al., 2008b). Whenever speak combining two elections, say E1 = (C1 ,V1 )E2 = (C2 ,V2 ), mean building, via algorithm Lemma 4.6, election E = (C,V ) whoseelection graph disjoint union election graphs E1 E2 with, possibly, edgesadded vertices G(E1 ) G(E2 ) (in case explicitly state edges,any, added). particular, often want add padding candidates election,without affecting original election much. order so, typically combinemain election one following padding elections. Note construction,originally developed use study control Copeland voting, also proven usefulstudy manipulation Copeland (Faliszewski et al., 2008).Lemma 4.7 Let rational number 0 1. positive integer n,polynomial-time (in n) computable election Padn = (C,V ) kCk = 2n + 1candidate ci C holds scorePadn (c) = n.Proof. Fix positive integer n. Lemma 4.6 enough construct (in polynomial time n)directed, antisymmetric graph G 2n + 1 vertices, indegree outdegree equal312fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLn. set Gs vertex set {0, 1, . . . , 2n} put edge vertex vertex j (i 6= j)( j i) mod (2n + 1) n. result exactly one directed edge everytwo distinct vertices vertex edges going exactly vertices(i + 1) mod (2n + 1), (i + 2) mod (2n + 1), . . . , (i + n) mod (2n + 1). Thus, indegreeoutdegree vertex equal n proof complete.Lemma 4.6 (McGarvey, 1953) useful building election need directcontrol results head-to-head contests. However, many cases explicitly specifying results head-to-head contests would tedious. Instead would easierspecify results important head-to-head contests require candidatescertain suitable scores. next lemma show construct elections specifiedway via combining small election containing important head-to-head contest largepadding election. mention generalized version lemma since used studymanipulation Copeland (Faliszewski et al., 2008).Lemma 4.8 Let E = (C,V ) election C = {c1 , . . . , cn }, let rational number0 1, let n n integer. candidate ci denote number headto-head ties ci E ti . Let k1 , . . . , kn sequence n nonnegative integerski 0 ki n. algorithm polynomial time n outputs electionE = (C ,V ) that:1. C = C D, = {d1 , . . . , d2n2 },2. E restricted C E,3. ties head-to-head contests E candidates C,4. i, 1 n , scoreE (ci ) = 2n2 ki + ti ,5. i, 1 2n2 , scoreE (di ) n2 + 1.Proof. build E via combining E padding election F (see Lemma 4.7 paragraphit). F = (D,W ), = {d1 , . . . , d2n2 }, essentially election Padn2 onearbitrary candidate removed. partition candidates n groups, D1 , . . . , Dn ,exactly 2n candidates set results head-to-head contests ci Ccandidates according following scheme. j {1, . . . , n } 6= j, cidefeats members j ci defeats exactly many candidates Di (and losesremaining ones) needed ensurescoreE (ci ) = 2n2 ki + ti .easy see possible: ci score (C Di ,V ) 2n2 2n + k + ti k0 k n ti . 2n candidates Di ci reach score form2n2 k + ti , k integer 0 n, via defeating head-to-head contestsappropriate number candidates Di losing remaining ones.Finally, since F Padn2 one candidate removed, di gets n2 pointsdefeating members one point possibly defeating member C.Thus, di D, holds scoreE (di ) n2 + 1. completes proof.313fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEInstead invoking Lemma 4.8 directly, often simply describe election termsresults important head-to-head contests scores important candidatesmention election built, possibly adding extra padding candidatesaffect general structure election, using Lemma 4.8. case clearLemma 4.8 indeed used build election describe.4.1.3 C ONSTRUCTIVE C ANDIDATE C ONTROLLet us turn case constructive candidate control. show resistance holdsCopeland cases (i.e., rational values 0 1 constructive candidate control scenarios), except CCACu {0, 1} vulnerability holds (seeTheorem 4.11).resistance proofs section follow via reductions vertex cover problem.Recall vertex cover problem input (G, k) G undirected graph knonnegative integer accept G vertex cover size k. Withoutloss generality, assume V (G) = {1, . . . , n} E(G) = {e1 , . . . , em }. Noteeither = 0, n = 0, k min(n, m) instance trivial solution proofsalways assume n nonzero k less min(n, m).case, input reduction meet requirements (or otherwise malformed)reduction outputs fixed yes instance fixed instance depending (easily obtained)solution (G, k) malformation input. Also note every input (G, k) meetsrequirements, G vertex cover size less equal k G vertexcover size k.Theorem 4.9 Let rational number 0 1. Copeland resistant constructive control via adding candidates (CCAC), nonunique-winner model uniquewinner model, rational irrational voter model.Proof. give reduction vertex cover problem. Let (G, k) instance vertexcover problem, G undirected graph, k nonnegative integer, V (G) = {1, . . . , n},E(G) = {e1 , . . . , em }, n 6= 0, 6= 0, k < min(n, m). construct instance CCACCopeland designated candidate p become winner adding k candidatesG vertex cover size k.reduction works follows. Via Lemma 4.8, build election E = (C ,V ) that:1. {p, e1 , . . . , em } C ,2. scoreE (p) = 22 1 nonunique-winner case (scoreE (p) = 22 unique-winnercase); sufficiently large (but polynomially bounded) integer takes roleLemma 4.8s n,3. ei C , scoreE (ei ) = 22 ,4. scores candidates C {p, e1 , . . . , em } 22 n 2.form election E = (C,V ) combining E candidates = {1, . . . , n} (correspondingvertices G). results head-to-head contests within set arbitrarily,head-to-head contests members C members set follows:314fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLcandidates C {e1 , . . . , em } defeat members D, e j {e1 , . . . , em },candidate defeats e j e j edge incident loses otherwise. reduction outputsinstance (C, D,V, p, k) CCAC question whether possible choose subset D,kD k k, p winner (the unique winner) Copeland election (C ,V ). clearreduction computable polynomial time. show correct.G vertex cover size k add candidates correspond cover.Adding candidates increases score p k, scores ei increasek 1 each, since edge incident least one member vertex cover. Clearly,candidates C {p, e1 , . . . , em } never become winners adding k candidates D,thus p becomes winner (the unique winner).converse, assume p become winner (the unique winner) via adding kcandidates set D. order p become winner (the unique winner), mustcase via adding candidates ei gets least one point less p. However, possibleadd candidates correspond cover.Interestingly, parameter strictly 0 1 (i.e., 0 < < 1) Copelandresistant constructive control via adding candidates even allow adding unlimited number candidates (the CCACu case). reason rational strictly0 1 construction ensure, via structure, add k candidates.hand, Copeland0 Copeland1 vulnerable constructive control via addingunlimited number candidates (CCACu , see Theorem 4.11).Theorem 4.10 Let rational number 0 < < 1. Copeland resistant constructive control via adding unlimited number candidates (CCACu ), nonunique-winnermodel unique-winner model, rational irrational voter model.Proof. give reduction vertex cover problem.unique-winner case, need specify one candidates scores termsnumber > 0 1 . Let t1 t2 two positive integers = tt12greatest common divisor 1. Clearly, two numbers exist rationalgreater 0. set t12 . elementary number-theoretic arguments, two positiveinteger constants, k1 k2 , k1 = k2 .Let (G, k) instance vertex cover problem, G undirected graph knonnegative integer. Let {e1 , . . . , em } Gs edges let {1, . . . , n} Gs vertices. before,assume n nonzero k < min(n, m). Using Lemma 4.8, buildelection E = (C,V ) following properties:1. {p, r, e1 , . . . , em } C (the remaining candidates C used padding),2. scoreE (p) = 22 1,3. scoreE (r) = 22 1k +k nonunique-winner case (scoreE (r) = 22 1k +kunique-winner case15 ),15. Note via second paragraph proof easy build election r score form.obtain part rs score could, example, r tie k1 padding candidates obtain k2 points.k2 points could accounted part 22 1.315fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE4. ei C, scoreE (ei ) = 22 1 + nonunique-winner case (scoreE (ei ) = 22 1unique-winner case),5. scores candidates C {p, r, e1 , . . . , em } 22 n 2.form election E = (C D,V ) via combining E candidates = {1, . . . , n} appropriate voters results head-to-head contests are:1. p ties candidates D,2. e j , e j incident candidate defeats candidate e j , otherwise tie,3. candidates C defeat candidates D.show G contains vertex cover size k setp winner (the unique winner) Copeland election (C ,V ). easysee corresponds vertex cover size k p winner (the unique winner)Copeland election (C ,V ). reason adding member increases ps scoreincreases rs score one, e j , adding increases e j scoree j incident i. Thus, via simple calculation scores candidates,easy see p winner (the unique winner) election.hand, assume p become winner (the unique winner) Copelandelection (C ,V ) via adding subset candidates D. First, note kD k k,since otherwise r would end points (at least many points as) p p wouldwinner (would unique winner). claim corresponds vertex coverG. sake contradiction, assume edge e j incident vertices u vneither u v . However, case candidate e j wouldpoints (at least many points as) p p would winner (would uniquewinner). Thus, must form vertex cover size k.Note proof crucial neither 0 1. 0 proofwould fall apart would able ensure vertex cover,1 would able limit size . fact, show, Theorem 4.11,Copeland0 Copeland1 vulnerable control via adding unlimited numbercandidates (CCACu ).Theorem 4.11 Let {0, 1}. Copeland vulnerable constructive control via adding unlimited number candidates (CCACu ), nonunique-winner model unique-winnermodel, rational irrational voter model.Proof. input candidate set C, spoiler candidate set D, collection voters preferences(either preference lists preference tables) C D, candidate p C. goal checkwhether subset p winner (the unique winner) (C ,V )within Copeland . show find set , exists, following simplealgorithm.316fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLLet D1 = {d | score({p,d},V ) (p) = 1}. Initialize D1 , delete everyscore(CD ,V ) (p) < score(CD ,V ) (d). unique-winner problem, deleteevery score(CD ,V ) (p) score(CD ,V ) (d).Clearly, algorithm runs polynomial time. show algorithm works, first noteb D, p winner (the unique winner) (C D,Vb ), p winner (the uniquebwinner) (C (D D1 ),V ). because, Observation 4.2,score(CD,Vb ) (p) = score(C(DDb1 ),V )=(p) +score(C(DDb 1 ),V ) (p).score({p,d},V ) (p)bdDD1b D1 , p winner (the unique winner) (C D,Vb ),supposealgorithm computes set p winner (not unique winner) (C ,V ). firstb . Since p winner (not unique winner) (C ,V ), followsconsider caseconstruction exists candidate C {p} score(CD ,V ) (p) <score(CD ,V ) (d) (such score(CD ,V ) (p) score(CD ,V ) (d)). However, nonunique-winnermodelbscore(CD ,V ) (p) = score(CD,Vb ) (p) + kD k kDkbscore(CD,Vb ) (d) + kD k kDk score(CD ,V ) (d),contradiction. unique-winner model, first inequality becomes> reach contradiction well.b 6 . Let first candidateb deletedFinally, consider caseb D1 score (p) < score (d)algorithm. set(CD ,V )(CD ,V )nonunique-winner case (score(CD ,V ) (p) score(CD ,V ) (d) unique-winner case).b D1 ,Sincebb1. score(CD,Vb ) (p) = score(CD ,V ) (p) (kD k kDk) < score(CD ,V ) (d) (kD k kDk)score(CD,Vb ) (d) nonunique-winner case,bb2. score(CD,Vb ) (p) = score(CD ,V ) (p) (kD k kDk) score(CD ,V ) (d) (kD k kDk)score(CD,Vb ) (d) unique-winner case.b ). contradicIt follows p winner (not unique winner) (C D,Vtion.remainder section dedicated showing rational 0 1,Copeland resistant constructive control via deleting candidates constructive control viapartitioning candidates (with without run-off TE TP model). reasonsspace nonrepetitiveness, proofs results included foundfull TR version (Faliszewski et al., 2008b), first handle case constructive controlvia deleting candidates (CCDC) then, using proof CCDC case building block,handle constructive partition-of-candidates cases.317fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHETheorem 4.12 Let rational number 0 1. Copeland resistant constructive control via deleting candidates (CCDC), nonunique-winner modelunique-winner model, rational irrational voter model.proof Theorem 4.13 (which, mentioned above, presented Faliszewski et al.,2008b) employs construction used proving Theorem 4.12 construction combines suitable elections combined election properties useful proving variouspartition-of-candidates cases (with without run-off). particular, constructionapplied proof Theorem 4.13, also designed general enough serve keyingredient proving Theorems 4.14, 4.15, 4.16 below.Theorem 4.13 Let rational number 0 1. Copeland resistant constructive control via run-off partition candidates ties-promote model (CCRPC-TP)ties-eliminate model (CCRPC-TE), nonunique-winner model uniquewinner model, rational irrational voter model.Copeland also resistant constructive control via partition candidates (without run-off)rational value (and including) 0 1. However, proofs TP TEcases (which, again, found full TR version, Faliszewski et al., 2008b) uniformCCRPC scenario soto stay sync structure Faliszewski et al. 2008b,proofs arewe treat cases separately Theorems 4.14, 4.15, 4.16.Theorem 4.14 Let rational number 0 1. Copeland resistant constructive control via partition candidates ties-promote tie-handling rule (CCPC-TP),nonunique-winner model unique-winner model, rational irrationalvoter model.Theorem 4.15 Copeland1 resistant constructive control via partition candidatesties-eliminate tie-handling rule (CCPC-TE), nonunique-winner model uniquewinner model, rational irrational voter model.Theorem 4.16 Let rational number, 0 < 1. Copeland resistant constructivecontrol via partition candidates ties-eliminate tie-handling rule (CCPC-TE),nonunique-winner model unique-winner model, rational irrationalvoter model.4.2 Voter Controlsection, show rational , 0 1, Copeland resistant typesvoter control. Table 2 lists type voter control, rational , 0 1,winner model (i.e., nonunique-winner model unique-winner model) theoremgiven case handled. start control via adding voters.Theorem 4.17 Let rational number 0 1. Copeland resistantconstructive destructive control via adding voters (CCAV DCAV), nonuniquewinner model unique-winner model, rational irrational voter model.318fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLuniqueCCAVDCAVCCDVDCDVCCPV-TPDCPV-TPCCPV-TEDCPV-TE=0nonunique0< <1uniquenonuniqueunique=1nonuniqueThm. 4.17Thm. 4.19Thm. 4.20Thm. 4.20Thm. 4.19Thm. 4.19Thm. 4.20Thm. 4.20Thm. 4.19Thm. 4.19Thm. 4.18Thm. 4.18Thm. 4.19Thm. 4.21Thm. 4.23Thm. 4.26Thm. 4.24Thm. 4.25Table 2: Table theorems covering resistance results voter control Copeland .theorem covers case rational voters case irrational voters.Proof. result follows via reductions X3C problem. first show handlenonunique-winner constructive case later argue construction easilymodified remaining cases.Let (B, ) X3C instance B = {b1 , . . . , b3k } = {S1 , . . . , Sn } finite collection three-element subsets B. Without loss generality, assume k odd (if even,simply add b3k+1 , b3k+2 , b3(k+1) B Sn+1 = {b3k+1 , b3k+2 , b3(k+1) } , add 1 k).question whether one pick k sets Sa1 , . . . , Sak B = kj=1 Sa j .build Copeland election E = (C,V ) follows. candidate set C contains candidates p(the preferred candidate), r (ps rival), s, members B, number padding candidates.select voter collection V head-to-head contests, defeats p, r defeatsbi , following Copeland scores candidates,sufficiently large (but polynomially bounded n) nonnegative integer:1. scoreE (p) = 1,2. scoreE (r) = + 3k,3. candidates Copeland scores 1.easy see E constructed polynomial time Lemma 4.8. addition, ensurefollowing results head-to-head contests candidates C:1. vsE (s, p) = k 1,2. {1, . . . , k}, vsE (r, bi ) = k 3,3. pairs candidates c, d, |vsE (c, d)| k + 1.done since add 2 vsE (c, d) leave relative vote scoresadding two voters, c > > C {c, d} C {c, d} > c > (see Lemma 4.6). Since k oddnumber voters even (see Lemma 4.8), easy see fulfill requirements.319fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEalso specify set W voters chair potentially add. set Sisingle voter wi W preference listp > B Si > r > Si >(all unmentioned candidates follow fixed arbitrary order). claim contains kelement cover B p become winner election via adding kvoters selected W .contains k-element cover B, say Sa1 , . . . , Sak , make p winner via addingvoters U = {wa1 , . . . , wak }. Adding k voters increases ps score one, since pdefeats head-to-head contest. Since voters U correspond cover, score r goes3k points. Why? bi B, adding k 1 voters U correspondsets cover containing bi increases relative performance bi versus r k 1 votes,thus giving bi two votes advantage r. Adding remaining voter U decreasesadvantage 1, still bi wins head-to-head contest r.show make p winner adding k voters contains kelement cover B. Note p candidate possibly become winner addingk voters, p best obtain Copeland score , p obtain scoreadd exactly k voters, r lose 3k points via losing head-to-head contestsbi s. Thus way p become winner adding k votersW add exactly k voters r loses head-to-head contest bi .Assume U W set voters correspond cover B. meanscandidate bi least two voters U prefer r bi . However,case bi cannot defeat r head-to-head contest p winner. U correspondscover. completes proof nonunique-winner constructive case theorem.constructive unique-winner case, modify election E scoreE (p) = .listed properties relative vote scores absolute Copeland scores unchanged.previous case, easy see p become unique winner via adding k voterscorrespond cover B. converse, show still need add exactly k votersp become unique winner.added fewer k 1 voters p would get extra points wouldimpossible p become unique winner. Let us show adding exactly k 1 voterscannot make p unique winner. added exactly k 1 voters p would get points extratie s. consider candidate bi j , j corresponds one addedvoters, w j . Since w j prefers r bi , adding w j election increases relative performance rversus bi k 2. Thus adding remaining k 2 voters result bi either tieing losinghead-to-head contest r. either case p would high enough score becomeunique winner. Thus know exactly k candidates must added want p becomeunique winner and, via argument previous case, knowcorrespond cover.destructive cases suffices note proof constructive nonunique-winnercase works also proof destructive unique-winner case (where preventing runique winner) constructive unique-winner case works also proofdestructive nonunique-winner case (where preventing r winner).Let us turn case control via deleting voters. Unfortunately, proofsuniform need cases handle = 1 separately case320fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL0 < 1. Also, cannot use construction lemma (Lemma 4.8) anymore convenientlybuild elections. case deleting voters (or partitioning voters) needclear understanding voter affects election whole point introducingconstruction lemma abstract away low-level details.Analogously case candidate control, resistance proofs deleting voters reusedwithin resistance proofs partitioning voters. reasons space nonrepetitiveness,include proofs. particular, proofs Theorems 4.18 4.20included found full TR version (Faliszewski et al., 2008b). proofsTheorems 4.19 4.21, however, presented here. mention construction givenproof Theorem 4.19 used later proof Theorem 5.1, constructiongiven proof Theorem 4.21 used later proof Theorem 5.2.Theorem 4.18 Copeland1 resistant constructive control via deleting voters (CCDV)nonunique-winner model destructive control via deleting voters (DCDV) uniquewinner model, rational irrational voter model.Theorem 4.19 Let rational number 0 1. Copeland resistant constructive control via deleting voters (CCDV) unique-winner model destructive controlvia deleting voters (DCDV) nonunique-winner model, rational irrationalvoter model.Proof. Let (B, ) instance X3C, B = {b1 , . . . , b3k } = {S1 , . . . , Sn }finite family three-element subsets B. Without loss generality, assume n kk > 2 (if n < k contain cover B, k 2 solve problembrute force). build election E = (C,V ) that:1. contains k-element cover B, preferred candidate p become uniqueCopeland winner E deleting k voters,2. r become nonwinner deleting k voters, contains k-element coverB.Let candidate set C {p, r, b1 , . . . , b3k } let V following collection 4n k + 1voters:1. n 1 voters preference B > p > r,2. n k + 2 voters preference p > r > B,3. Si two voters, vi vi ,(a) vi preference r > B Si > p > Si ,(b) vi preference r > Si > p > B Si .easy see bi B, vsE (r, bi ) = 2n k + 3, vsE (bi , p) = k 3, vsE (r, p) = k 1.contains k-element cover B, say {Sa1 , . . . , Sak }, delete voters va1 , . . . , vak .resulting election, p defeats every candidate head-to-head contests, thus punique winner.321fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEprove second statement, suppose subset W k votersr winner Eb = (C,V W ). Since vsE (r, bi ) = 2n k + 3 n k, immediateb order r winner E,br defeats every bi B head-to-head contests E.p must certainly defeat r tie-or-defeat every bi B head-to-head contests. pdefeat r head-to-head contest kW k = k every voter W prefers r p. followsW size-k subset {v1 , v1 , . . . , vn , vn }.Let bi B. Recall vsE (bi , p) = k 3 p needs least tie bi head-to-headb Since kW k = k, follows W contain one voter prefers p bi .contest E.Since k > 2, follows W contains voters set {v1 , . . . , vn } voters Wcorrespond k-element cover B.Theorem 4.20 Let rational number 0 < 1. Copeland resistant constructive control via deleting voters (CCDV) nonunique-winner model destructive controlvia deleting voters (DCDV) unique-winner model, rational irrationalvoter model.Theorem 4.21 Let rational number 0 1. Copeland resistantconstructive destructive control via partitioning voters TP model (CCPV-TP DCPVTP), nonunique-winner model unique-winner model, rationalirrational voter model.Proof. Let (B, ) instance X3C, B = {b1 , . . . , b3k } = {S1 , . . . , Sn }finite family three-element subsets B. Without loss generality, assume n kk > 2 (if n < k contain cover B, k 2 solve problembrute force). build election E = (C,V ) that:1. contains k-element cover B, preferred candidate p become uniqueCopeland winner E via partitioning voters TP model,2. r made uniquely win E via partitioning voters TP model,contains k-element cover B.Note implies Copeland resistant constructive destructive control viapartitioning voters TP model, nonunique-winner model unique-winnermodel.construction extension construction Theorem 4.19. let candidateset C {p, r, s, b1 , . . . , b3k } let V following collection voters:1. k + 1 voters preference > r > B > p,2. n 1 voters preference B > p > r > s,3. n k + 2 voters preference p > r > B > s,4. Si two voters, vi vi ,(a) vi preference r > B Si > p > Si > s,322fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL(b) vi preference r > Si > p > B Si > s.Let Vb V collection voters V except k + 1 voters preference > r >B > p. Note Vb exactly voter collection used proof Theorem 4.19 candidateadded least desirable candidate. Since influence differences scorescandidates, following claim follows immediately proof Theorem 4.19.Claim 4.22 r become nonwinner (C, Vb ) deleting k voters, containsk-element cover B.Recall need prove contains k-element cover B, p madeunique Copeland winner E via partitioning voters TP model, r madeuniquely win E via partitioning voters TP model, contains k-element coverB.contains k-element cover B, say {Sa1 , . . . , Sak }, let second subelectionconsist k + 1 voters preference > r > B > p voters va1 , . . . , vak . punique winner first subelection, unique winner second subelection, puniquely wins final run-off p s.prove second statement, suppose partition voters r uniquewinner resulting election model TP. Note least one subelections, withoutloss generality say second subelection, majority voters prefers r candidates{p, b1 , . . . , b3k }. Since r unique winner every run-off participates in, r cannotwinner either subelection. Since r defeats every candidate {p, b1 , . . . , b3k } head-tohead contests second subelection, order r winner second subelection,must certainly case defeats r head-to-head contest second subelection.implies k voters Vb part second subelection.consider first subelection. Note r cannot winner first subelection. Then,clearly, r cannot winner first subelection restricted voters Vb .16 Claim 4.22follows contains k-element cover B.turn TE variant control via partitioning voters. None remaining proofsSection 4.2 (i.e., none proofs Theorems 4.23 4.26) includedfound full TR version (Faliszewski et al., 2008b). particular, proofTheorem 4.23 uses exact construction proof Theorem 4.21 proofsTheorems 4.24 4.25 use modifications thereof. stay sync structure Faliszewskiet al. 2008b, proof-providing full TR (where proof structure, mentioned above, dependsvalue ), state Theorems 4.23 4.26 separately.Theorem 4.23 Let rational number 0 < 1. Copeland resistant constructive control via partitioning voters TE model (CCPV-TE), nonunique-winnermodel unique-winner model, rational irrational voter model.16. r winner first subelection restricted voters Vb r would certainly winner firstsubelection without restrictions: voters V Vb prefer r everyone except s, (by discussionproof) cannot winner first subelection. (Note winner one two subelectionswinner second subelection.)323fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHETheorem 4.24 Copeland1 resistant constructive control via partitioning voters TE model(CCPV-TE), nonunique-winner model unique-winner model, rationalirrational voter model.Theorem 4.25 Copeland1 resistant destructive control via partitioning voters TE model(DCPV-TE), nonunique-winner model unique winner model, rationalirrational voter model.Finally, Theorem 4.26 states resistance Copeland , rational number0 < 1, destructive control partition voters TE model. proof result(see Faliszewski et al., 2008b) extends construction proof Theorem 4.20 (see alsoFaliszewski et al., 2008b) way proof Theorem 4.21 extended constructionproof Theorem 4.19.Theorem 4.26 Let rational number 0 < 1. Copeland resistant destructive control via partitioning voters TE model (DCPV-TE), nonunique-winnermodel unique-winner model, rational irrational voter model.4.3 FPT Algorithm Schemes Bounded-Case ControlResistance control generally viewed desirable property system design. However, suppose one trying solve resistant control problems. hope?Bartholdi, Tovey, Trick (1989b), seminal paper NP-hard winner-determinationproblems, suggested considering hard election problems cases bounded number candidates bounded number voters, obtained efficient-algorithm results cases.Within study elections, approachseeking efficient fixed-parameter algorithmshas, example, also used (although somewhat tacitlysee coming discussion second paragraph Footnote 17) within study bribery (Faliszewski et al., 2006a; Faliszewski,Hemaspaandra, & Hemaspaandra, 2006b). best knowledge, bounded-case approach finding limits resistance results previously used study control problems. section precisely that.particular, obtain resistant-in-general control problems broad range efficient algorithms case number candidates voters bounded. algorithmsmerely polynomial time. Rather, give algorithms prove membership FPT (fixedparameter tractability, i.e., problem merely individually P fixed valueparameter interest (voters candidates), indeed single P algorithm degreebounded independently value fixed number voters candidates)number candidates bounded, also number voters bounded. proveFPT claims hold even succinct input modelin voters input via(preference-list, binary-integer-giving-frequency-of-that-preference-list) pairsand evencase irrational voters. (One imagine succinct-representation case holding initial preprocessing elections ballots compute number people casting preferenceoccurred.)obtain algorithms voter-control cases, bounded candidatesbounded voters, candidate-control cases bounded candidates.hand, show resistant-in-general irrational-voter, candidate-control cases, resistancestill holds even number voters limited two.324fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLstructure section follows. first start briefly stating notions notations.next state, prove, fixed-parameter tractability results. Regarding those, firstaddress FPT results (standard) constructive destructive cases. showmany cases assert FPT results general stillin particular, lookextended control: completely pinpointing whether given type control ensureleast one specified collection Copeland Outcome Tables (to defined later)obtained. Finally, give resistance results.4.3.1 N OTIONSN OTATIONSstudy fixed-parameter complexity (see, e.g., Niedermeier, 2006) expanding explosively since parented field Downey, Fellows, others late 1980s 1990s.Although area built rich variety complexity classes regarding parameterized problems,purpose current paper need focus one important class, namely,class FPT. Briefly put, problem parameterized value j said fixed-parametertractable (equivalently, belong class FPT) algorithm problem whoserunning time f ( j)nO(1) . (Note particular particular constant big-ohholds inputs, regardless j value particular input has.)context, consider two parameterizations: bounding number candidatesbounding number voters. use notations used throughout paperdescribe problems, except postpend -BV j problem name state numbervoters may j, postpend -BC j problem name state numbercandidates may j. case, bound applies full number itemsinvolved problem. example, case control adding voters, j must boundtotal number voters election added together number voters poolvoters available adding.Typically, viewing input votes coming ballot. However, onealso consider case succinct inputs, algorithm given votes (preferencelist, binary-integer-giving-frequency-of-that-preference-list) pairs. (We mention passingadding voter cases, speak succinctness require always-votingvoters specified succinctly also pool voters-available-to-be-added specifiedsuccinctly.) Succinct inputs studied extensively case bribery (Faliszewski et al.,2006a, 2006b), speaking broadly, succinctness-of-input issues often germanecomplexity classification (see, e.g., Wagner, 1986). Note proving FPT result succinctcase problem immediately implies FPT result problem (without requirementsuccinct inputs place), indeed stronger result, since succinctness potentiallyexponentially compress input.Finally, would like able concisely express many results single statement.so, borrow notational approach transformational grammar,h iuse square bracketsrunsindependent choice notation. So, example, claim walks shorthandsix assertions: runs; runs; runs; walks; walks; walks. special casesymbol 0/ which, appearsin bracket, means unwoundviewed text all. example, SuccinctCopeland fun asserts Succinct Copeland0/fun Copeland fun.325fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHE4.3.2 F IXED -PARAMETER RACTABILITY R ESULTSimmediately state main results, show voter-control cases FPT schemeshold bounded-voter bounded-candidate cases, candidate-controlcases FPT schemes hold bounded-candidate cases.Theorem 4.27 rational , 0 1, choice independent choice brackets below, specified parameterized (as j varies N) problem FPT:AVDVCBV jsuccinctCopeland.CPV-TE BC j0/CopelandIrrationalPV-TPTheorem 4.28 rational , 0 1, choice independent choice brackets below, specified parameterized (as j varies N) problem FPT:ACuACDCsuccinctCopelandCC PC-TE-BC j .0/CopelandIrrationalPC-TPRPC-TERPC-TPReaders interested discussion results proofs point safelyskip next labeled section header.proving theorems, let us first make observations them. First,cases particular set choices case known (e.g., due resultsSections 4.1 4.2) P even unbounded case, results uninterestingfollow earlier results (such cases include succinct cases, sincetreated earlier). However, small minority cases. Also, claritycases covered, included items formally needed. example,since FPT succinct case implies FPT no-succinctness-restriction case, since FPTirrationality-allowed case implies FPT rational-only case, first two choice bracketstheorems could, without decreasing results strength, removed eliminating0/ Copeland choices.turn proofs. Since proving every case would uninterestingly repetitive,times (after carefully warning reader) prove cases one two control typesenough make clear omitted cases proofs go.Let us start cases done simply appropriately applied brute force.first prove Theorem 4.28.Proof Theorem 4.28. limited j candidates,cases mentioned, total number ways adding/deleting/partitioning candidates simply(large) constant. example, (at rather exactly since jmerely upper bound number candidates) 2 j possible run-off partitions2 j1 relevant ways deleting candidates (since cant (destructive case) would326fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLnever (constructive case) delete distinguished candidate). brute-force try waysadding/deleting/partitioning candidates, way see whether get desiredoutcome. works polynomial time (with fixed degree independent j ) evensuccinct case, even irrationality allowed.Theorem 4.28brute-force approach similarly works case voter control number votersfixed. particular, prove following subcase Theorem 4.27.Lemma 4.29 rational , 0 1, choice independent choice bracketsbelow, specified parameterized (as j varies N) problem FPT:AVDVCsuccinctCopeland-BV j .CCopelandIrrationalPV-TE0/PV-TPconsidering BV j casesnamely proof resistance section startingpage 334we even discuss succinctness. reason number votersbounded, say j, succinctness doesnt asymptotically change input sizes interestingly,since succinctness best would compress vote description factor jwhichcase fixed constant (relative value parameterization, j).Proof Lemma 4.29. limited j voters, note can,four types control, brute-force check possible approaches type control.example, case control deleting voters, clearly 2 j possible votedeletion choices, case control partitioning voters, 2 jpartitions (into V1 V V1 ) consider. 2 j (large) constant. direct brute-forcecheck yields polynomial-time algorithm, inspection one see run-times degreebounded independently j.Lemma 4.29come interesting cluster FPT cases: voter-control cases numbercandidates bounded. Now, first, one might think handle this, cases,via brute-force approach. almost correct: One get polynomial-time algorithmscases via brute-force approach. However, succinct cases, degreesalgorithms huge, independent bound, j, number candidates.example, even rational case, one would approach obtain run-times termsnkCk! . is, one would obtain family P-time algorithms, one wouldFPT algorithm.overcome obstacle, employ Lenstras (1983) algorithm bounded-variablecardinality integer programming. Although Lenstras algorithm truly amazing power, evenenough accomplish goal. Rather, use scheme involves fixed(though large) number Lenstra-type programs focused different resolutionpath regarding given problem.need prove, complete proof Theorem 4.27, following lemma.Lemma 4.30 rational , 0 1, choice independent choice bracketsbelow, specified parameterized (as j varies N) problem FPT:327fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEAVsuccinctCopelandDVC-BC j .C0/PV-TECopelandIrrationalPV-TPLet us start recalling that, regarding first choice bracket, succinct case implies0/ case, need address succinct case. Recall also that, regarding second choicebracket, rational , 0 1, CopelandIrrational case implies Copeland case,need address CopelandIrrational case.remains handle pair choices third forth choice brackets.prove every case would repetitive. simply prove detail difficult, relativelyrepresentative case, cases either mention type adjustment neededobtain proofs, simply leave simple tedious exercise clear,do, anyone reads section.So, particular, let us prove following result.Lemma 4.31 rational , 0 1, following parameterized (as j varies N)problem FPT: succinct-CopelandIrrational -CCPV-TP-BC j .Proof. Let , 0 1, arbitrary, fixed rational number. particular, supposeexpressed b/d, b N, N+ , b share common integer divisor greater1, b = 0 = 1. wont explicitly invoke b algorithm, timespeak evaluating certain set pairwise outcomes respect , one thinkevaluating respect strict pairwise win giving points, pairwise tie giving b points,strict pairwise loss giving 0 points.need method specifying pairwise outcomes among set candidates. this,use notion Copeland outcome table set candidates. actuallytable, rather function (a symmetric oneit affected ordertwo arguments) that, given pair distinct candidates inputs, saythree possible outcomes allegedly happened: Either tie, one candidate won,candidate won. Note COT simply representation election graph (see Section 4.1.2).jSo, j-candidate election, exactly 3(2) functions. (We care namescandidates, assume tables simply use names 1 j,match names actual candidates integers linking lexicographically, i.e.,lexicographically first candidate associated integer 1 on.) Let us callj-candidates Copeland outcome table j-COT.needbuildalgorithmshowsproblemsuccinct-CopelandIrrational -CCPV-TP-BC j , j N, FPT. So, let j fixed integerbound number candidates.1717. seem specify algorithm merely bound. However, important note enoughestablish exists single algorithm fulfills requirements definition FPT. particular,specification give sufficiently uniform one simply consider single algorithm that,given input, notes value j, number candidates, j algorithmspecify does.take moment mention passing earlier work, Faliszewski et al. 2006a (this expanded, full version that) Faliszewski et al. 2006b, gives P-time algorithms fixed parameter (fixed328fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLj -COT, T1 ,j -COT, T2 ,CopelandIrrational election (involving input voters), respect , candidates win T1 respect ,candidates win T2 respect , preferred candidateinput problem winner,create run integer linear program constraint feasibility problem checkswhether exists partition voters first subelection j COT T1 second subelection j -COT T2 , so, accept.Figure 5: top-level code case succinct-CopelandIrrational -CCPV-TP-BC j .Let us suppose given input instance. Let j j number candidatesinstance (recall j number candidates, rather upper bound numbercandidates).top level algorithm specified pseudocode Figure 5. (Although algorithm seemingly trying tell whether given control possible given case, rathertelling partition achieve control, note iteration doubleloop accepts precise values variables inside integer linear program constraint feasibility problem made iteration satisfied fact tell us precisely partitionmakes preferred candidate win.)Now, note total number j -COTs exist (we need care whetherjjrealized via actual votes) 3( 2 ) . code inside two loops executes 9( 2 ) times,constant-bounded since j j, fixed j.remains give integer linear program constraint feasibility problem mentionedinside inner loop. setting sometimes confusing, e.g., speak constantsgrow without limit. important keep mind integer linear programconstraint feasibility problem, number variables constraints constant (over inputs),integer linear program constraint feasibility problems constants (one may prefer wordcandidate fixed voters) cases fact, claims work, implicitly giving FPT algorithms, even though papers dont explicitly note that. reason generally truepapernamely, Lenstra technique powerful also ideally suited FPT algorithmsused inside algorithms FPT algorithms. interestingly, Lenstra approach tends work evensuccinct inputs, FPT comment made applies even results abovementioned earlier paperssuccinct-inputs case fixed-number-of-candidates fixed-number-of-voters claims. (The fixednumber-of-candidates fixed-number-of-voters Dodgson winner/score work Bartholdi et al., 1989b, knownFPT algorithmsdue proof Bartholdi et al., 1989b, itself, see discussion Faliszewski et al.,2006a, see also Betzler, Guo, Niedermeier, 2008. Although paper Bartholdi et al., 1989b, doesnt addresssuccinct input model, Faliszewski et al., 2006a, notes approach works fine even succinct caseswinner problem. true P-ness algorithms even succinct case, alsoFPT-ness algorithms even succinct case.)329fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEcoefficients, makes things clearer) things change respect input.framework allows us invoke Lenstras powerful algorithm.first specify set constants integer linear program constraint feasibility problem.jparticular, i, 1 2( 2 ) , constant, n ,18 number input votersjwhose vote ith type (among 2( 2 ) possible vote possibilities; keep mind votersjallowed irrational, thus value 2( 2 ) correct). Note number constantsconstant-bounded (for fixed j), though course values constants(of integer linear program constraint feasibility problem) take grow without limit.addition, let us define constants vary input rather simplynotational shorthand use describe integer linear program constraint feasibilityproblem defined (what constraints occur it). particular, 1 j ,1 j , 6= , let val1i, 1 T1 asserts (in head-to-head contest) ties defeats, let 0 T1 asserts (in head-to-head contest) loses . Let val2i, identicallydefined, except respect T2 . Informally put, values used let integer linearprogram constraint feasibility problem seek enforce win/loss/tie pattern respectgiven input vote numbers given type allowed control action.integer linear program constraint feasibility problems variables, coursejjinteger variables, following 2( 2 ) variables. i, 1 2( 2 ) , variable,jmi , represents many ni voters ith among 2( 2 ) possible vote types gofirst subelection.Finally, must specify constraints integer linear program constraint feasibility problem. three groups constraints.first constraint group enforcing plausible numbers put first partition.jparticular, i, 1 2( 2 ) , constraints 0 n .second constraint group enforcing partitioning really firstsubelection situation pairwise contests come exactly specified T1 .particular, 1 j , 1 j , 6= , following. Considerequation() OP (j{a | 1 2( 2 ) votes typeholds preferred }),(4.a)j{a | 1 2( 2 ) votes typeholds preferred i}jsum varies 2( 2 ) possible preferences. val1(i, ) = 1constraint form OP set . val1(, i) = 1 constraintform OP set . Note means val1(i, ) = val1(, i) = 1, i.e.,two voters purported tie, add two constraints.third constraint group function second constraint group, except regardssecond subelection rather first subelection. particular,18. Again, discussed immediately previous paragraph, say that, example, ni constantsinteger linear program constraint feasibility problem, mean constants complexity sense,rather constantsin sense coefficientsof integer linear program constraintfeasibility problem. saying that, mean imply number voters bounded globalvalue cases.330fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL1 j , 1 j , 6= , following. Consider equation (4.a) above,except two occurrences replaced na . val2(i, ) = 1constraint form OP set . val2(, i) = 1 constraintform OP set . above, means val2(i, ) = val2(, i) = 1, add twoconstraints.completes specification integer linear programming constraint feasibility problem.Note top-level code, Figure 5, clearly runs within polynomial time relativeeven succinct-case input original CCPV-TP problem, polynomials degreebounded independently j. Note particular algorithm constructslarge constant (for j fixed) number integer linear programming constraint feasibility problems,polynomial-sized relative even succinct-case input originalCCPV-TP problem, polynomial sizes degree bounded independently j. Further, note integer linear programming constraint feasibility problems clearly testsupposed testmost importantly, test subelections match pairwiseoutcomes specified j -COTs T1 T2 . Finally crucially, Lenstras (1983) algorithm (seealso Downey, 2003, Niedermeier, 2002, clear regarding linears latersentence), since integer linear programming constraint feasibility problem fixed numberconstraints (and case fact also fixed number variables), solvedrelativesize (which includes filled-in constants, ni example, effect inputsinteger programs specification)via linear number arithmetic operations linear-sizedintegers. So, overall, polynomial time even relative succinctly specified input,polynomials degree bounded independently j. Thus established membershipclass FPT.describe briefly proof Lemma 4.31adjustedcasesLemma 4.30, namely, caseshandlepartitionsuccinctCopelandCPV-TEC-BC j . noted before, first two brack0/CopelandIrrationalPV-TPets ignored, chosen demanding choice each. Let us discussvariations. Regarding changing constructive destructive, Figure 5 change winnerwinner. Regarding changing PV-TP PV-TE, block Figure 5change candidates win candidate wins (if unique candidatewins).CAVsuccinctCopelandC-BC j .remaining cases casesCopelandIrrationalDV0/However, cases even straightforward partition cases covered,space reasons write out, rather briefly comment cases. Basically,jones top-level code cases loops j -COTs, (there 3( 2 ) ) checkswhether right outcome happens j -COT (i.e., distinguished candidate either(constructive case) (destructive case) winner), so, runs Lenstras algorithminteger linear programming constraint feasibility problem see whether allowedaction (adding/deleting) get state particular j -COT matches (after additiondeletion voters) election. integer program, variables obvious ones, namely,ji, 1 2( 2 ) , variable, , describes many voters type331fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEadd/delete. key constants (of integer linear program constraint feasibility problem),jhave, i, 1 2( 2 ) , value, n , number type voters input. Also,jproblem addition voters, additional constants, nbi , 1 2( 2 ) ,representing number type voters among pool, W , voters available addition.problem internal k (a limit number additions deletions), enforcenatural constraints, also natural constraints enforce obvious relationships mi , ni , nbi , on. critically, constraints ensuringadditions/deletions specified mi , pairwise outcome specified j -COT realized.Finally, although everything Section 4.3 (both part far part come) writtencase nonunique-winner model, results hold analogously unique-winnermodel, natural, minor proof modifications. (Also, mention passing dueconnection, found Footnote 5 Hemaspaandra et al., 2007a, unique-winner destructivecontrol nonunique-winner constructive control, one could use nonunique-winnerconstructive-case results indirectly prove unique-winner destructive-case results.)4.3.3 FPTE XTENDED C ONTROLsection, look extended control. mean changing ten standardcontrol notions adding/deleting/partitioning candidates/voters. Rather, mean generalizing pastmerely looking constructive (make distinguished candidate winner) destructive(prevent distinguished candidate winner) cases. particular, interestedcontrol goal far flexibly specified, example (though partition caseseven flexible this), allow goal region (reasonabletheretime-related conditions) subcollection Copeland outcome tables (specificationswon/lost/tied head-to-head contest). Since Copeland outcome table, concertcurrent , one read CopelandIrrational scores candidates, allows ustremendous range descriptive flexibility specifying control goals, e.g., specifylinear order desired candidates respect CopelandIrrational scores, specifylinear-order-with-ties desired candidates respect CopelandIrrational scores,specify exact desired CopelandIrrational scores one candidates, specifywant ensure candidate certain subgroup CopelandIrrational score tiesdefeats CopelandIrrational score candidate certain subgroup, etc.19 Latersection give list repeating examples adding new examples.FPT algorithms given previous section regard, surface, standard controlproblem, tests whether given candidate made winner (constructive case)precluded winner (destructive case). note general approaches usedsection fact yield FPT schemes even far flexible notions control mentioned19. mention front initial example list applies additional minor technical caveats.examples speaking final election candidates receiving CopelandIrrational scoresfinal election. fact partition cases (necessarily) so, cases focusCopeland outcome tables natural given case. example, control partition voters,focus subcollections pairs Copeland outcome tables two subelections. Also, though Copelandoutcome tables defined explicitly labeled candidate names, rather use lexicographicalcorrespondence involved candidates, cases wouldthough dont repeat discussionbelowneed allow inclusion goal specification names candidates play giventable tables, particularly, cases addition deletion candidates, partition cases.332fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLabove. fact, one gets, FPT cases covered previous section, FPT algorithmsextended-control problem casesvery loosely put, FPT algorithms test, virtuallynatural collection outcome tables (as long collection recognized waydoesnt take much running time, i.e., checking time polynomial degreebounded independently j), whether given type control one reach oneoutcome tables.Let us discuss bit detail. key concept used inside proof Lemma 4.31Copeland outcome tablea function distinct pair candidates specifieseither tie specifies (not tied) winner pairwise contest. Let us considercontrol algorithm given proof lemma, particular let us consider top-levelcode specified Figure 5. code double-loops size j Copeland outcome tables (a.k.a. j COTs), regarding subpartitions, case outcome tables subelection cases,followed final election imply, correspond desired type constructive (thedistinguished person wins) destructive (the distinguished person win) outcome, checkwhether two j -COTs made hold via current type control (for casediscussed, PV-TP).However, note simply easily varying top-level code obtain natural FPTalgorithm (a single algorithm, see Footnote 17 analogue applies here) questionwhether via allowed type control one reach run-time-quick-to-recognize collectionpairs j -COTs (in subelection), even whether given candidate collection onegiven (run-time-quick-to-recognize) j -COT collection candidate collection ( jsize final-round candidate collection) reached final election. truepartition cases (where, informally put, would by, Figure 5, changingcondition inside instead look membership collection j -COTs20 ) alsocases attacked via Lenstras method (though nonpartition cases typicallysingle-loop Copeland outcome tables may represent outcome control exerted;also, cases, caveat end Footnote 19 apply). even easiernotice cases attacked direct brute force also holds.So, examples (some echoing start section, new), following (with caveats mentioned needed names attached, e.g., cases candidateaddition/deletion/partition, regarding partition cases focusing necessarily directly20. Let us discuss bit formally, using PV-TP example. Consider family boolean functionsFj , j N, Fj computable, even first argument succinctly specified, polynomial timepolynomial degree bounded independently j. Now, consider changing Figure 5s code to:j -COT, T1 ,j -COT, T2 ,(Fj (input, T1 , T2 )).Note change gives FPT control scheme certain extended control problem. particular,extended control problem whose goal ensure realize least one set (T1 , T2 )Fj ( j number candidates particular input), given inputs problems input, T1 , T2evaluates true. is, Fj functions recognizing (viewed bit differently, defining) goal setextended control problem.input, T1 , T2 easily tell scores final election. approach usedchoose extended-control goals natural features final election.333fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEfinal table) FPT extended control algorithms types control boundedness casesFPT results previous section stated.1. Asking whether stated action one obtain final election (simplyelection case partitioning) outcome CopelandIrrational system scores candidates precisely match relations lexicographic namescandidates.2. generally that, asking whether stated action one obtain finalelection (simply election case partitioning) certain linearorder-without-ties regarding CopelandIrrational -system scores candidates.3. generally still, asking whether stated action one obtain final election(simply election case partitioning) certain linear-order-withties regarding CopelandIrrational -system scores candidates.4. Asking whether stated action one obtain final election (simplyelection case partitioning) situation exactly 1492 candidatestie winner regarding CopelandIrrational -system scores.5. Asking whether stated action one obtain final election (simplyelection case partitioning) situation two candidatesCopelandIrrational -system scores other.Again, examples. point previous section flexible enoughaddress constructive/destructive control, also address far general control issues.4.3.4 R ESISTANCE R ESULTSTheorems 4.27 4.28 give FPT schemes voter-control cases bounded voters,voter-control cases bounded candidates, candidate-control cases bounded candidates. might lead one hope cases admit FPT schemes. However, remainingtype case, candidate-control cases bounded voters, follow pattern. fact,note CopelandIrrational candidate-control cases showed earlier paper (i.e., without bounds number voters) resistant remain resistant even casebounded voters. resistance holds even input succinct format,certainly also holds input succinct format.reason that, case irrational voters, two voters (with preferencesj candidates) given j-COT achieved. this, distinct pair candidates, preferred pairwise contest voters prefer , preferredpairwise contest voters prefer i, tie pairwise contestone voter prefer one voter prefer . Since proofs resistance candidatecontrol, identified elections election graphs, i.e., COTs, hard seeresistance proofs carry even case two irrational voters.open cases remaining regard rational-voter, candidate-control, bounded-votercases. note Betzler Uhlmann (2008) recently resolved open issues.334fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROL5. Control Condorcet Electionssection show Condorcet elections resistant constructive control via deletingvoters (CCDV) via partition voters (CCPV). results originally claimedseminal paper Bartholdi et al. (1992), proofs based assumptionvoter indifferent several candidates. model elections allow(and neither ours). show one obtain results case voterspreference lists linear orderswhich model ours.Recall candidate c election E = (C,V ) Condorcet winner E defeatscandidates head-to-head contests. Alternatively, one could say candidate cCondorcet winner election E Copeland0 score kCk 1. Sinceelection one Condorcet winner, doesnt make sense differentiateunique-winner nonunique-winner models. (We pass reader refereescomment different system known weak Condorcet elections, whose winnerscandidates beat tie candidate head-to-head elections, oneone winner.)Theorem 5.1 Condorcet elections resistant constructive control via deleting voters.Proof. follows immediately proof Theorem 4.19. Note Condorcet winneralways unique Copeland winner, rational 0 1, note proofTheorem 4.19, contains k-element cover B, delete k votersresulting election p defeats every candidate head-to-head contest, i.e., p Condorcetwinner resulting election.proceed proof resistance case constructive control via partitionvoters (CCPV), mention slight quirk Bartholdi, Tovey, Tricks model voterpartition. one reads paper carefully, becomes apparent quiet assumptiongiven set voters partitioned subelections elect exactly onewinner, thus severely restricting chairs partitioning possibilities. Hemaspaandraet al. (2007a) replaced Bartholdi, Tovey, Tricks convention natural ties-promoteties-eliminate rules (see discussion Hemaspaandra et al., 2007a), currentsection paper go back Bartholdi, Tovey, Tricks model, since goalreprove results without breaking model.Theorem 5.2 Condorcet elections resistant constructive control via partitioning voters(CCPV) Bartholdi, Tovey, Tricks model (see paragraph above).Proof. proof follows via reduction X3C problem. fact, use exactlyconstruction proof Theorem 4.21. Let E = (C,V ) election constructedproof.Since candidate p defeats head-to-head contest, way pbecome winner via partitioning voters guarantee p wins within subelectionwins within one. (Note since p Condorcet winner, p cannot winsubelections.)contains k-element cover, say, {Sa1 , . . . , Sak }, letting Vp = Vb {va1 , . . . , vak }Vs = V Vp make p Condorcet winner CCPV scenario.335fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEconverse, let (Vp ,Vs ) partition collection voters p globalCondorcet winner CCPV scenario use two subelections, one voters Vpone voters Vs . Via paragraph assume, without loss generality, pCondorcet winner (C,Vp ) Condorcet winner (C,Vs ). Since k + 1 votersV Vb rank first rank p last, assume Vs contains k + 1 voters (i.e., voterspreference > r > B > p). Also, Vs contains k voters Vb , otherwise wouldcertainly Condorcet winner (C,Vs ).result, p made Condorcet winner (C, Vb ) deleting k voters.follows Claim 4.22 contains k-element cover B.6. Conclusionsshown computational point view election systems Llull Copeland(i.e., Copeland0.5 ) broadly resistant bribery constructive procedural control, regardlesswhether voters required rational preferences. rather charming Llulls 700year-old system shows perfect resistance bribery resistances (constructive) controlnatural system (even far modern ones) easy winner-determinationprocedureother Copeland , 0 < < 1is known possess, even remarkable one considers Llulls system defined long control elections evenexplicitly studied. Copeland0.5 voting matches Llulls perfect resistance bribery additionperfect resistance (constructive) control.natural open direction would study complexity control additional electionsystems. would particularly interesting find existing, natural voting systemspolynomial-time winner determination procedures resistant standard typesconstructive destructive control. would also extremely interesting find single resultsclassify, broad families election systems, precisely makes control easy hard,i.e., obtain dichotomy meta-results control (see Hemaspaandra Hemaspaandra, 2007,discussion regarding work flavor manipulation).Acknowledgmentsthank Nadja Betzler, Felix Brandt, Preetjot Singh, Frieder Stolzenburg, Dietrich Stoyan,anonymous AAAI-07, AAIM-08, COMSOC-08, JAIR referees, JAIR handling editor JeffRosenschein helpful comments, suggestions, guidance. work supported partAGH-UST grant 11.11.120.777, DFG grants RO-1202/{9-3, 11-1, 12-1}, NSF grants CCR0311021, CCF-0426761, IIS-0713061, Alexander von Humboldt Foundations TransCoopprogram, European Science Foundations EUROCORES program LogICCC, Friedrich Wilhelm Bessel Research Awards Edith Hemaspaandra Lane A. Hemaspaandra. workdone part visits Piotr Faliszewski, Edith Hemaspaandra, Lane A. HemaspaandraHeinrich-Heine-Universitat Dusseldorf, visits Jorg Rothe University Rochester,Piotr Faliszewski University Rochester. paper combines extendsUniversity Rochester Computer Science Department Technical Reports TR-913 TR-923,papers results presented 22nd AAAI Conference ArtificialIntelligence (AAAI-07) Faliszewski et al. 2007, October 2007 Dagstuhl Seminar Computational Issues Social Choice, 4th International Conference Algorithmic Aspects336fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLInformation Management (AAIM-08) Faliszewski, Hemaspaandra, Hemaspaandra,Rothe 2008a, 2nd International Workshop Computational Social Choice (COMSOC08).ReferencesAhuja, R., Magnanti, T., & Orlin, J. (1993). Network Flows: Theory, Algorithms, Applications.Prentice-Hall.Altman, A., & Tennenholtz, M. (2007). axiomatic approach personalized ranking systems.Proceedings 20th International Joint Conference Artificial Intelligence, pp. 11871192. AAAI Press.Arrow, K. (1951 (revised editon, 1963)). Social Choice Individual Values. John WileySons.Austen-Smith, D., & Banks, J. (2000). Positive Political Theory I: Collective Preference. UniversityMichigan Press.Bartholdi, III, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social ChoiceWelfare, 8(4), 341354.Bartholdi, III, J., Tovey, C., & Trick, M. (1989a). computational difficulty manipulatingelection. Social Choice Welfare, 6(3), 227241.Bartholdi, III, J., Tovey, C., & Trick, M. (1989b). Voting schemes difficult tellelection. Social Choice Welfare, 6(2), 157165.Bartholdi, III, J., Tovey, C., & Trick, M. (1992). hard control election? MathematicalComputer Modeling, 16(8/9), 2740.Betzler, N., Guo, J., & Niedermeier, R. (2008). Parameterized computational complexity Dodgson Young elections. Proceedings 11th Scandinavian Workshop AlgorithmTheory, pp. 402413. Springer-Verlag Lecture Notes Computer Science #5124.Betzler, N., & Uhlmann, J. (2008). Parameterized complexity candidate control electionsrelated digraph problems. Proceedings 2nd Annual International ConferenceCombinatorial Optimization Applications, pp. 4353. Springer-Verlag Lecture NotesComputer Science #5156.Brams, S., & Sanver, R. (2006). Critical strategies approval voting: gets ruledruled out. Electoral Studies, 25(2), 287305.Condorcet, J. (1785). Essai sur lApplication de LAnalyse la Probabilite des Decisions Renduesla Pluralite des Voix. Facsimile reprint original published Paris, 1972, ImprimerieRoyale.Conitzer, V., & Sandholm, T. (2003). Universal voting protocol tweaks make manipulation hard.Proceedings 18th International Joint Conference Artificial Intelligence, pp. 781788. Morgan Kaufmann.Conitzer, V., & Sandholm, T. (2006). Nonexistence voting rules usually hard manipulate. Proceedings 21st National Conference Artificial Intelligence, pp. 627634.AAAI Press.337fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEConitzer, V., Sandholm, T., & Lang, J. (2007). elections candidates hardmanipulate? Journal ACM, 54(3), Article 14.Copeland, A. (1951). reasonable social welfare function. Mimeographed notes SeminarApplications Mathematics Social Sciences, University Michigan.Cormen, T., Leiserson, C., Rivest, R., & Stein, C. (2001). Introduction Algorithms (secondedition). MIT Press/McGraw Hill.Downey, R. (2003). Parameterized complexity skeptic. Proceedings 18th AnnualIEEE Conference Computational Complexity, pp. 147168. IEEE Computer Society Press.Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness shared beliefs:GibbardSatterthwaite generalized. Social Choice Welfare, 17(1), 8593.Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methods web.Proceedings 10th International World Wide Web Conference, pp. 613622. ACMPress.Elkind, E., & Lipmaa, H. (2005). Small coalitions cannot manipulate voting. Proceedings9th International Conference Financial Cryptography Data Security, pp. 285297.Springer-Verlag Lecture Notes Computer Science #3570.Ephrati, E., & Rosenschein, J. (1997). heuristic technique multi-agent planning. AnnalsMathematics Artificial Intelligence, 20(14), 1367.Erdelyi, G., Hemaspaandra, L., Rothe, J., & Spakowski, H. (2007). approximating optimalweighted lobbying, frequency correctness versus average-case polynomial time.Proceedings 16th International Symposium Fundamentals Computation Theory,pp. 300311. Springer-Verlag Lecture Notes Computer Science #4639.Erdelyi, G., Nowak, M., & Rothe, J. (2008a). Sincere-strategy preference-based approval votingbroadly resists control. Proceedings 33rd International Symposium MathematicalFoundations Computer Science, pp. 311322. Springer-Verlag Lecture Notes ComputerScience #5162.Erdelyi, G., Nowak, M., & Rothe, J. (2008b). Sincere-strategy preference-based approval voting fully resists constructive control broadly resists destructive control. Tech. rep.arXiv:0806.0535 [cs.GT], arXiv.org. precursor appears (Erdelyi, Nowak, & Rothe,2008a). Journal version appear Mathematical Logic Quarterly.Faliszewski, P. (2008). Nonuniform bribery (short paper). Proceedings 7th InternationalConference Autonomous Agents Multiagent Systems, pp. 15691572. InternationalFoundation Autonomous Agents Multiagent Systems.Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. complexity bribery elections.Journal Artificial Intelligence Research. appear.Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2006a). complexity bribery elections. Proceedings 21st National Conference Artificial Intelligence, pp. 641646.AAAI Press. Journal version appear (Faliszewski, Hemaspaandra, & Hemaspaandra,appear).338fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLFaliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2006b). hard bribery elections?Tech. rep. TR-895, Department Computer Science, University Rochester, Rochester,NY. Revised, September 2006.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Llull Copelandvoting broadly resist bribery control. Proceedings 22nd AAAI ConferenceArtificial Intelligence, pp. 724730. AAAI Press.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2008a). Copeland voting fullyresists constructive control. Proceedings 4th International Conference Algorithmic Aspects Information Management, pp. 165176. Springer-Verlag Lecture NotesComputer Science #5034.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2008b). Llull Copelandvoting computationally resist bribery control. Tech. rep. arXiv:0809.4484 [cs.GT], Computing Research Repository, http://www.acm.org/repository/.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). richer understandingcomplexity election systems. Ravi, S., & Shukla, S. (Eds.), Fundamental ProblemsComputing: Essays Honor Professor Daniel J. Rosenkrantz, pp. 375406. Springer.Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: Ties matter. Proceedings 7th International Conference Autonomous Agents Multiagent Systems,pp. 983990. International Foundation Autonomous Agents Multiagent Systems.Friedgut, E., Kalai, G., & Nisan, N. (2008). Elections manipulated often. Proceedings49rd IEEE Symposium Foundations Computer Science, pp. 243249. IEEE Computer Society.Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory NPCompleteness. W. H. Freeman Company.Ghosh, S., Mundhe, M., Hernandez, K., & Sen, S. (1999). Voting movies: anatomyrecommender systems. Proceedings 3rd Annual Conference Autonomous Agents,pp. 434435. ACM Press.Gibbard, A. (1973). Manipulation voting schemes. Econometrica, 41(4), 587601.Hagele, G., & Pukelsheim, F. (2001). electoral writings Ramon Llull. Studia Lulliana,41(97), 338.Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy voting systems. Journal Computer System Sciences, 73(1), 7383.Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007a). Anyone him: complexityprecluding alternative. Artificial Intelligence, 171(5-6), 255285.Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007b). Hybrid elections broaden complexitytheoretic resistance control. Proceedings 20th International Joint ConferenceArtificial Intelligence, pp. 13081314. AAAI Press. Journal version appear Mathematical Logic Quarterly.Homan, C., & Hemaspaandra, L. Guarantees success frequency algorithm findingDodgson-election winners. Journal Heuristics. appear. Full version available (Homan& Hemaspaandra, 2005).339fiFALISZEWSKI , H EMASPAANDRA , H EMASPAANDRA , & ROTHEHoman, C., & Hemaspaandra, L. (2005). Guarantees success frequency algorithmfinding Dodgson-election winners. Tech. rep. TR-881, Department Computer Science,University Rochester, Rochester, NY. Revised, June 2007.Kern, W., & Paulusma, D. (2001). new FIFA rules hard: Complexity aspects sportscompetitions. Discrete Applied Mathematics, 108(3), 317323.Lenstra, Jr., H. (1983). Integer programming fixed number variables. MathematicsOperations Research, 8(4), 538548.Levin, J., & Nalebuff, B. (1995). introduction vote-counting schemes. Journal Economic Perspectives, 9(1), 326.McCabe-Dansted, J., Pritchard, G., & Slinko, A. (2008). Approximability Dodgsons rule. SocialChoice Welfare, 31(2), 311330.McGarvey, D. (1953). theorem construction voting paradoxes. Econometrica, 21(4),608610.McLean, I., & Urken, A. (1995). Classics Social Choice. University Michigan Press.Meir, R., Procaccia, A., Rosenschein, J., & Zohar, A. (2008). complexity strategic behaviormulti-winner elections. Journal Artificial Intelligence Research, 33, 149178.Merlin, V., & Saari, D. (1997). Copeland method II: Manipulation, monotonicity, paradoxes.Journal Economic Theory, 72(1), 148172.Niedermeier, R. (2002). Invitation fixed-parameter algorithms. Habilitation thesis, UniversityTubingen.Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford University Press.Procaccia, A., & Rosenschein, J. (2007). Junta distributions average-case complexitymanipulating elections. Journal Artificial Intelligence Research, 28, 157181.Procaccia, A., Rosenschein, J., & Kaminka, G. (2007). robustness preference aggregation noisy environments. Proceedings 6th International Joint ConferenceAutonomous Agents Multiagent Systems, pp. 416422. ACM Press.Procaccia, A., Rosenschein, J., & Zohar, A. (2008). complexity achieving proportionalrepresentation. Social Choice Welfare, 30(3), 353362.Saari, D., & Merlin, V. (1996). Copeland method I: Relationships dictionary. EconomicTheory, 8(1), 5176.Satterthwaite, M. (1975). Strategy-proofness Arrows conditions: Existence correspondence theorems voting procedures social welfare functions. Journal EconomicTheory, 10(2), 187217.Stearns, R. (1959). voting problem. American Mathematical Monthly, 66(9), 761763.Wagner, K. (1986). complexity combinatorial problems succinct input representations.Acta Informatica, 23(3), 325356.Xia, L., & Conitzer, V. (2008a). Generalized scoring rules frequency coalitional manipulability. Proceedings 9th ACM Conference Electronic Commerce, pp. 109118.ACM Press.340fiL LULL C OPELAND VOTING R ESIST B RIBERY C ONSTRUCTIVE C ONTROLXia, L., & Conitzer, V. (2008b). sufficient condition voting rules frequently manipulable.Proceedings 9th ACM Conference Electronic Commerce, pp. 99108. ACM Press.Zermelo, E. (1929). Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der Wahrscheinlichkeitsrechnung. Mathematische Zeitschrift, 29(1), 436460.341fiJournal Artificial Intelligence Research 35 (2009) 557-591Submitted 11/08; published 07/09Optimal Value Information Graphical ModelsAndreas KrauseKRAUSEA @ CALTECH . EDUCalifornia Institute Technology,1200 E California Blvd.,Pasadena, CA 91125 USACarlos GuestrinGUESTRIN @ CS . CMU . EDUCarnegie Mellon University,5000 Forbes Ave.,Pittsburgh, PA 15213 USAAbstractMany real-world decision making tasks require us choose among several expensive observations. sensor network, example, important select subset sensorsexpected provide strongest reduction uncertainty. medical decision making tasks, oneneeds select tests administer deciding effective treatment.general practice use heuristic-guided procedures selecting observations. paper,present first efficient optimal algorithms selecting observations class probabilisticgraphical models. example, algorithms allow optimally label hidden variables HiddenMarkov Models (HMMs). provide results selecting optimal subset observations,obtaining optimal conditional observation plan.Furthermore prove surprising result: graphical models tasks, one designsefficient algorithm chain graphs, HMMs, procedure generalized polytree graphical models. prove optimizing value information NPPP -hard evenpolytrees. also follows results computing decision theoretic value information objective functions, commonly used practice, #P-complete problem evenNaive Bayes models (a simple special case polytrees).addition, consider several extensions, using algorithms scheduling observation selection multiple sensors. demonstrate effectiveness approach severalreal-world datasets, including prototype sensor network deployment energy conservationbuildings.1. Introductionprobabilistic reasoning, one choose among several possible expensive observations,often central issue decide variables observe order effectively increaseexpected utility (Howard, 1966; Howard & Matheson, 1984; Mookerjee & Mannino, 1997;Lindley, 1956). medical expert system, example, multiple tests available, testdifferent cost (Turney, 1995; Heckerman, Horvitz, & Middleton, 1993). systems,thus important decide tests perform order become certainpatients condition, minimum cost. Occasionally, cost testing even exceed valueinformation possible outcome, suggesting discontinue testing.following running example motivates research empirically evaluated Section 6.Consider temperature monitoring task, wireless temperature sensors distributed acrossc2009AI Access Foundation. rights reserved.fiK RAUSE & G UESTRINbuilding. task become certain temperature distribution, whilst minimizingenergy expenditure, critically constrained resource (Deshpande, Guestrin, Madden, Hellerstein, &Hong, 2004). fine-grained building monitoring required obtain significant energy savings(Singhvi, Krause, Guestrin, Garrett, & Matthews, 2005).Many researchers suggested use myopic (greedy) approaches select observations (Scheffer, Decomain, & Wrobel, 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen,1997; Bayer-Zubek, 2004; Kapoor, Horvitz, & Basu, 2007). Unfortunately, general, heuristic provide performance guarantees. paper, present efficient algorithms,guarantee optimal nonmyopic value information chain graphical models. example,algorithms used optimal active labeling hidden states Hidden Markov Models (HMMs, Baum & Petrie, 1966). address two settings: subset selection, optimalsubset observations obtained open-loop fashion, conditional plans, sequential,closed-loop plan observation strategy depends actual value observed variables (c.f., Figure 1). knowledge, first optimal efficient algorithmsobservation selection diagnostic planning based value information class graphical models. settings, address filtering smoothing versions: Filteringimportant online decision making, decisions utilize observations madepast. Smoothing arises example structured classification tasks, temporaldimension data, hence observations taken account. call approachVO IDP algorithms use Dynamic Programming optimize Value Information. evaluate VO IDP algorithms empirically three real-world datasets, also showwell-suited interactive classification sequential data.inference problems graphical models, computing marginal distributionsfinding probable explanation, solved efficiently chain-structured graphs,also solved efficiently polytrees. prove problem selecting best kobservations maximizing decision theoretic value information NPPP -complete evendiscrete polytree graphical models, giving complexity theoretic classification core artificialintelligence problem. NPPP -complete problems believed significantly harder NPcomplete even #P-complete problems commonly arising context graphical models.furthermore prove evaluating decision-theoretic value information objective functions#P-complete even case Naive Bayes models, simple special case polytree graphicalmodels frequently used practice (c.f., Domingos & Pazzani, 1997).Unfortunately, hardness results show that, problem scheduling single sensor optimally solved using algorithms, problem scheduling multiple, correlatedsensors wildly intractable. Nevertheless, show VO IDP algorithms single sensorscheduling used approximately optimize multi-sensor schedule. demonstrateeffectiveness approach real sensor network testbed building management.summary, provide following contributions:present first optimal algorithms nonmyopically computing optimizing valueinformation chain graphical models.show optimizing decision theoretic value information NPPP -hard discretepolytree graphical models. computing decision theoretic value information #Phard even Naive Bayes models.558fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSTmorn =high?Tnoon =high?yesTeve =high?Figure 1: Example conditional plan.present several extensions algorithms, e.g., tree graphical modelsleaves, multiple correlated chains (for multi-sensor scheduling).extensively evaluate algorithms several real-world problems, including sensorscheduling real sensor testbed active labeling bioinformatics Natural Language Processing.2. Problem Statementassume state world described collection random variablesXV = (X1 , . . . , Xn ), V index set. example, V could denote set locations, Ximodels temperature reading sensor placed location V. subset= {i1 , . . . , ik } V, use notation XA refer random vector XA = (Xi1 , . . . , Xik ).algorithms extend continuous distributions, generally assume variables XV discrete. take Bayesian approach, assume prior probability distributionP (XV ) outcomes variables. Suppose select subset variables, XA (forV), observe XA = xA . example, set locations place sensors,set medical tests decide perform. observing realization variablesXA = xA , compute posterior distribution variables P (XV | XA = xA ). Basedposterior probability obtain reward R(P (XV | XA = xA )). example, reward function could depend uncertainty (e.g., measured entropy) distributionP (XV | XA = xA ). describe several examples detail below.general, selecting observation, know ahead time observationsmake. Instead, distribution possible observations. Hence,interested expected reward, take expectation possible observations.optimizing selection variables, consider different settings: subset selection, goal pick subset V variables, maximizingX= argmaxP (XA = xA )R(P (XV | XA = xA )),(1)xAimpose constraints set allowed pick (e.g., numbervariables selected, etc.). subset selection setting, commit selectionvariables get see realization.Instead, also sequentially select one variable other, letting choice dependobservations made past. setting, would like find conditional plan559fiK RAUSE & G UESTRINmaximizes= argmaxXP (xV )R(P (XV | X(xV ) = x(xV ) )).(2)xVHereby, conditional plan select different set variables possible stateworld xV . use notation (xV ) V refer subset variables selectedconditional plan state XV = xV . Figure 1 presents example conditional plantemperature monitoring example. define notion conditional planning formallySection 4.2.general setup selecting observations goes back decision analysis literaturenotion value information Howard (1966) statistical literature notionBayesian Experimental Design Lindley (1956). paper, refer Problems (1)(2) problems optimizing value information.paper, show complexity solving value information problems depend properties probability distribution P . give first algorithms optimallysolving value information interesting challenging class distributions including Hidden Markov Models. also present hardness results showing optimizing value informationwildly intractable (NPPP -complete) even probability distributions efficient inference possible (even Naive Bayes models discrete polytrees).2.1 Optimization Criteriapaper, consider class local reward1 functions Ri , definedmarginal probability distributions variables Xi . class computational advantagelocal rewards evaluated using probabilistic inference techniques. total rewardsum local rewards.Let subset V. P (Xj | XA = xA ) denotes marginal distribution variable Xj conditioned observations XA = xA . example, temperature monitoringapplication, Xj models temperature location j V. conditional marginal distributionP (Xj = xj | XA = xA ) models conditional distribution temperature location jobserving temperature locations V.classification purposes, appropriate consider max-marginalsP max (Xj = xj | XA = xA ) = max P (XV = xV , Xj = xj | XA = xA ),xVis, Xj set value xj , probability probable assignment XV = xVrandom variables (including Xj simplicity notation) conditioned observationsXA = xA .local reward Rj functional probability distribution P P max Xj .is, Rj takes entire distribution variable Xj maps reward value. Typically,reward functions chosen certain peaked distributions obtain higher reward.simplify notation, writeRj (Xj | xA ) , Rj (P (Xj | XA = xA ))1. Local reward functions also widely used additively independent utility models, (c.f., Keeney & Raiffa, 1976).560fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSdenote reward variable Xj upon observing XA = xA ,XRj (Xj | XA ) ,P (XA = xA )Rj (Xj | xA )xArefer expected local rewards, expectation taken assignments xAobservations A. Important local reward functions include:Residual entropy. setRj (Xj | xA ) = H(Xj | xA ) =XP (xj | xA ) log2 P (xj | xA ),xjobjective optimization problem becomes minimize sum residual entropies. Optimizing reward function attempts reduce uncertainty predicting marginals Xi .choose reward function running example measure uncertainty temperature distribution.PJoint entropy. Instead minimizing sum residual entropies H(Xi ), also attempt minimize joint entropy entire distribution,XH(XV ) =P (xV ) log2 P (xV ).xVNote, joint entropy depends full probability distribution P (XV ), rathermarginals P (Xi ), hence local. Nevertheless, exploit chain rule jointentropy H(XB ) set random variables B = {1, . . . , m} (c.f., Cover & Thomas, 1991),H(XB ) = H(X1 ) + H(X2 | X1 ) + H(X3 | X1 , X2 ) + + H(Xm | X1 , . . . , Xm1 ).Hence, choose local reward functions Rj (Xj | XA ) = H(Xj | X1 , . . . , Xj1 , XA ),optimize non-local reward function (the joint entropy) using local reward functions.Decision-theoretic value information. concept local reward functions also includesconcept decision theoretic value information. notion value information widelyused (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), formalized, e.g.,context influence diagrams (Howard & Matheson, 1984) Partially Observable Markov Decision Processes (POMDPs, Smallwood & Sondik, 1973). variable Xj , let Aj finite setactions. Also, let Uj : Aj dom Xj R utility function mapping action Ajoutcome x dom Xj real number. maximum expected utility principle states actionsselected maximize expected utility,XEUj (a | XA = xA ) =P (xj | xA )Uj (a, xj ).xjcertain Xj , economically choose action. ideacaptured notion value information, choose local reward functionRj (Xj | xA ) = max EUj (a | xA ).561fiK RAUSE & G UESTRINMargin structured prediction. also consider margin confidence:Rj (Xj | xA ) = P max (xj | xA ) P max (xj | xA ),xj = argmax P max (xj | xA ) xj = argmax P max (xj | xA ),xj 6=xjxjdescribes margin likely outcome closest runner up. rewardfunction useful structured classification purposes, shown Section 6.Weighted mean-squared error. variables continuous, might want minimizemean squared error prediction. choosingRj (Xj | xA ) = wj Var(Xj | xA ),ZVar(Xj | xA ) =P (xj | xA ) xjZx0j P (x0j|xA )dx0j2dxjconditional variance Xj given XA = xA , wj weight indicating importancevariable Xj .Monitoring critical regions (Hotspot sampling). Suppose want use sensors detecting fire. generally, want detect, j, whether Xj Cj , Cj dom Xjcritical region variable Xj . local reward functionRj (Xj | xA ) = P (Xj Cj | xA )favors observations maximize probability detecting critical regions.Function optimization (Correlated bandits). Consider setting collectionrandom variables XV taking numericalP values interval [m, m], and, selectingvariables, get reward xi . setting arises want optimize unknown(random) function, evaluating function expensive. setting, encouragedevaluate function likely obtain high values. maximize expectedtotal reward choose local reward functionZRj (Xj | xA ) = xj P (xj | xA )dxj ,i.e., expectation variable Xj given observations xA . setting optimizing randomfunction also considered version classical k-armed bandit problem correlatedarms. details relationship bandit problems given Section 8.examples demonstrate generality notion local reward. Note examples apply continuous distributions well discrete distributions.562fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS2.2 Cost Selecting Observationsalso want capture constraint observations expensive. meanobservation Xj associated positive penalty Cj effectively decreases reward.example, might interested trading accuracy sensing energy expenditure. Alternatively, also possible define budget B selecting observations, one associatedinteger cost j . Here, want select observations whose sum cost within budget,costs decrease reward. running example, sensors could poweredsolar power, regain certain amount energy per day, allows certain amountsensing. formulation optimizationpenalties budgets.P problems allows Psimplify notation also write C(A) = jA Cj (A) = jA j extend C sets.Instead fixed penalties costs per observation, also depend stateworld. example, medical domain, applying particular diagnostic test bear differentrisks health patient, depending patients illness. algorithms developadapted accommodate dependencies straight-forward manner.present details conditional planning algorithm Section 4.2.3. Decomposing Rewardssection, present key observation allows us develop efficient algorithmsnonmyopically optimizing value information class chain graphical models.algorithms presented Section 4.set random variables XV = {X1 , . . . , Xn } forms chain graphical model (a chain),Xi conditionally independent XV\{i1,i,i+1} given Xi1 Xi+1 . Without loss generalityassume joint distribution specified prior P (X1 ) variable X1conditional probability distributions P (Xi+1 | Xi ). time series model temperaturemeasured one sensor example formulated chain graphical model. Notetransition probabilities P (Xi+1 | Xi ) allowed depend index (i.e., chain modelsallowed nonstationary). Chain graphical models extensively used machinelearning signal processing.Consider example Hidden Markov Model unrolled n time steps, i.e., V partitioned hidden variables {X1 , . . . , Xn } emission variables {Y1 , . . . , Yn }. HMMs,Yi always observed variables Xi form chain. many applications,discussed Section 6, observe hidden variables Xi well, e.g., askingexpert, addition observing emission variables. cases, problem selectingexpert labels also belongs class chain graphical models addressed paper, sincevariables Xi form chain conditional observed values emission variables Yi . ideageneralized class Dynamic Bayesian Networks separators timeslices size one, separators selected observation. formulationalso includes certain conditional random fields (Lafferty, McCallum, & Pereira, 2001) formchains, conditional emission variables (the features).Chain graphical models originating time series additional, specific properties:system online decision making, observations past present time stepstaken account, observations made future. generally referredfiltering problem. setting, notation P (Xi | XA ) refer distributionXi conditional observations XA prior including time i. structured classification563fiK RAUSE & G UESTRINFigure 2: Illustration decomposing rewards idea. reward chain 1:7 observingvariables X1 , X4 X7 decomposes sum chain 1:4 plus reward chain4:7 plus immediate reward observing XP4 minus cost observing X4 . Herebybrevity use notation Rew(a : b) = bj=a Rj (Xj | X1 , X4 , X7 ).problems discussed Section 6, general observations made anywhere chain musttaken account. situation usually referred smoothing problem. providealgorithms filtering smoothing.describe key insight, allows efficient optimization chains. Considerset observations V. j variable observed, i.e., j A, local rewardsimply R(Xj | XA ) = R(Xj | Xj ). consider j/ A, let Aj subsetcontaining closest ancestor (and smoothing problem also closest descendant) XjXA . conditional independence property graphical model implies that, given XAj , Xjindependent rest observed variables, i.e., P (Xj | XA ) = P (Xj | XAj ). Thus,follows R(Xj | XA ) = R(Xj | XAj ).observations imply expected reward set observations decomposesalong chain. simplicity notation, add two independent dummy variables X0 Xn+1 ,R0 = C0 = 0 = Rn+1 = Cn+1 = n+1 = 0. Let = {i0 , . . . P, im+1 } il < il+1 ,i0 = 0 im+1 = n + 1. Using notation, total reward R(A) = j Rj (Xj | XA )smoothing case given by:iv+1 1XXRiv (Xiv | Xiv ) Civ +Rj (Xj | Xiv , Xiv+1 ) .v=0j=iv +1filtering settings, simply replace Rj (Xj | Xiv , Xiv+1 ) Rj (Xj | Xiv ). Figure 2 illustratesdecomposition.4. Efficient Algorithms Optimizing Value Informationsection, present algorithms efficiently nonmyopically optimizing value information chain graphical models.4.1 Efficient Algorithms Optimal Subset Selection Chain Modelssubset selection problem, want find informative subset variables observeadvance, i.e., observations made. running example, would,deploying sensors, identify k time points expected provide informativesensor readings according model.564fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSFirst, define objective function L subsets VL(A) =nXRj (Xj | XA ) C(A).(3)j=1subset selection problem find optimal subset=argmax L(A)AV,(A)Bmaximizing sum expected local rewards minus penalties, subject constrainttotal cost must exceed budget B.solve optimization problem using dynamic programming algorithm, chainbroken sub-chains using insight Section 3. Consider sub-chain variable XaXb . define Lsma:b (k) represent expected total reward sub-chain Xa , . . . , Xb ,ltsmoothing setting Xa Xb observed, budget level k. Lfa:b(k) representsexpected reward filtering setting Xa observed. formally:lt(k)Lfa:b=b1XmaxA{a+1...b1}j=a+1(A)kRj (Xj | XA , Xa ) C(A),filtering version,Lsma:b (k) =b1XmaxRj (Xj | XA , Xa , Xb ) C(A),A{a+1...b1}j=a+1(A)ksmoothing version. Note cases, L0:n+1 (B) = maxA:(A)B L(A), Equation (3), i.e., computing values La:b (k), compute maximum expected total rewardentire chain.f ltcompute Lsma:b (k) La:b (k) using dynamic programming. base case simply:ltLfa:b(0) =b1XRj (Xj | Xa ),j=a+1filtering,b1XLsma:b (0) =Rj (Xj | Xa , Xb ),j=a+1smoothing. recursion La:b (k) two cases: choose spendbudget, reaching base case, break chain two sub-chains, selecting optimalobservation Xj , < j < b. filtering smoothingLa:b (k) = max La:b (0),max{Rj (Xj | Xj ) Cj + La:j (0) + Lj:b (k j )} .j:a<j<b,j k565fiK RAUSE & G UESTRINInput: Budget B, rewards Rj , costs j penalties CjOutput: Optimal selection observation timesbegin0 < b n + 1 compute La:b (0);k = 1 B0 < b n + 1sel(1) := La:b (0);j = + 1 b 1 sel(j) := Rj (Xj | Xj ) Cj + La:j (0) + Lj:b (k j );La:b (k) = maxj{a+1,...,b1,1} sel(j);a:b (k) = argmaxj{a+1,...,b1,1} sel(j);endend:= 0; b := n + 1; k := B; := ;repeatj := a:b (k);j 0 := {j}; := j; k := k j ;j = 1 ;endAlgorithm 1: VO IDP algorithm optimal subset selection (for filtering smoothing).first, may seem recursion consider optimal split budgettwo sub-chains. However, since subset problem open-loop order observationsirrelevant, need consider split points first sub-chain receives zero budget.pseudo code implementation dynamic programming approach, call VO IDPsubset selection given Algorithm 1. algorithm fills dynamic programming tablestwo loops, inner loop ranging pairs (a, b), < b, outer loop increasing k. Withininner loop, computing best reward sub-chain b, fills table sel,sel(j) reward could obtained making observation j, sel(1)reward observation made.addition computing optimal rewards La:b (k) could achieved sub-chain : bbudget k, algorithm also stores choices a:b (k) realize maximum score. Here,a:b (k) index next variable selected sub-chain : b budgetk, 1 variable selected. order recover optimal subset budget k,Algorithm 1 uses quantities a:b recover optimal subset tracing maximal valuesoccurring dynamic programming equations. Using induction proof, obtain:Theorem 1 (Subset Selection). dynamic programming algorithm described computesoptimal subset budget B ( 61 n3 + O(n2 ))B evaluations expected local rewards.Note consider different costs variable, would simply choose j =1 variables compute La:b (N ). note variables Xi continuous,algorithm still applicable integrations inferences necessary computingexpected rewards performed efficiently. case, example, Gaussian linearmodel (i.e., variables Xi normally distributed) local reward functions residualentropies residual variances variable.566fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS4.2 Efficient Algorithms Optimal Conditional Planning Chain Modelsconditional plan problem, want compute optimal sequential querying policy :observe variable, pay penalty, depending values observed past, select nextquery, proceeding long budget suffices. objective find plan highestexpected reward, where, possible sequence observations, budget B exceeded.filtering, select observations future, whereas smoothing case, nextobservation anywhere chain. running example, filtering algorithm wouldappropriate: sensors would sequentially follow conditional plan, decidinginformative times sense based previous observations. Figure 1 shows exampleconditional plan.4.2.1 F ROM UBSET ELECTION C ONDITIONAL P LANNINGNote contrast subset selection setting considered Section 4.1, conditional planning, set variables depends state world XV = xV . Hence,state, conditional plan could select different set variables, (xV ) V. example, consider Figure 1, set possible observations V = {morn, noon, eve},XV = {Tmorn , Tnoon , Teve }. world state xV = (high, low, high), conditionalplan presented Figure 1 would select (xV ) = {morn, eve}, whereas,xV = (low, low, high), would select (xV ) = {morn, noon}. Since conditional planfunction (random) state world, set-valued random variable. order optimizeProblem (2), define objective function2J() =XP (xV )xVnXRj (Xj | x(xV ) ) C((xV )) ,j=1i.e., expected sum local rewards given observations made plan (xV ) state XV = xVminus penalties selected variables, expectation taken respectdistribution P (XV ). addition defining value policy J(), also define cost ()() = max ((xV )),xVmaximum cost (A) (as defined Section 2.2) set = (xV ) could selectedpolicy , state world XV = xV .Based notation, goal find policy= argmax J() () B,i.e., policy maximum value, guaranteed never cost exceeding budgetB. Hereby class sequential policies (i.e., those, observations chosensequentially, based observations previously made).useful introduce following notation:J(xA ; k) = max J( | XA = xA ) () k,(4)2. Recall that, filtering setting, R(Xj | x(xV ) ) , R(Xj | xA0 ), A0 = {t (xV ) s.t. j}, i.e.,observations past taken account.567fiK RAUSE & G UESTRINJ( | XA = xA ) =XnXP (xV | XA = xA )Rj (Xj | x(xV ) ) C((xV )) .xVj=1Hence, J(xA ; k) best possible reward achieved sequential policy costk, observing XA = xA . Using notation, goal find optimal planreward J(; B).value function J satisfies following recursion. base case considers exhaustedbudget:XJ(xA ; 0) =Rj (Xj | xA ) C(A).jVrecursion, holdsXJ(xA ; k) = max J(xA ; 0), max,P (xj | xA )J(xA , xj ; k j )j/(5)xji.e., best one state XA = xA budget k either stop selecting variables,chose best next variable act optimally thereupon.Note easily allow cost j depend state xj variable Xj . case,would simply replace j j (xj ), define J(XA , r) = whenever r < 0. Equivalently,let penalty C(A) depend state replacing C(A) C(xA ).Relationship finite-horizon Markov Decision Processes (MDPs). Note functionJ(xA ; k) defined (4) analogous concept value function Markov Decision Processes (c.f., Bellman, 1957): finite-horizon MDPs, value function V (s; k) models maximum expected reward obtainable starting state performing k actions. valuefunction holdsXV (s; k) = R(s, k) + maxP (s0 | s, a)V (s0 ; k 1),s0P (s0 | s, a) probability transiting state s0 performing action state s,R(s, k) immediate reward obtained state k steps still left. recursion,similar Eq. (5), exploited value iteration algorithm solving MDPs.conditional planning problem unit observation cost (i.e., (A) = |A|) could modeledfinite-horizon MDP, states correspond observed evidence XA = xA , actions correspondobserving variables (or making observation) transition probabilities givenprobability observing particular instantiation selected variable. immediate rewardR(s, k) = 0 k > 0, R(s, 0) expected reward (in value information problem)observing assignment (i.e., R(P (XV | s)) C(s)). observations unit cost,MDP, holds V (xA ; k) = J(xA ; k). Unfortunately, conditional planning problem, sincestate MDP uniquely determined observed evidence XA = xA , state spaceexponentially large. Hence, existing algorithms solving MDPs exactly (such value iteration)cannot applied solve large value information problems. Section 4.2.2, developefficient dynamic programming algorithm conditional planning chain graphical modelsavoids exponential increase complexity.568fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS4.2.2 DYNAMIC P ROGRAMMING PTIMAL C ONDITIONAL P LANNING C HAINSpropose dynamic programming algorithm obtaining optimal conditional plansimilar subset algorithm presented Section 4.1. Again, utilize decompositionrewards described Section 3. difference observation selection budgetallocation depend actual values observations. order compute valuefunction J(xA ; k) entire chain, compute value functions Ja:b (xA ; k) subchains Xa , . . . , Xb .base case dynamic programming approach deals zero budget setting:f ltJa:b(xa ; 0)=b1XRj (Xj | Xa = xa ),j=a+1filtering,sm(xa , xb ; 0) =Ja:bb1XRj (Xj | Xa = xa , Xb = xb ),j=a+1smoothing. recursion defines Ja:b (xa ; k) (or Ja:b (xa , xb ; k) smoothing), expectedreward problem restricted sub-chain Xa , . . . , Xb conditioned values Xa = xa(and Xb = xb smoothing), budget limited k. compute quantity,iterate possible split points j, < j < b. observe notable differencefiltering smoothing case. smoothing, must consider possiblesplits budget two resulting sub-chains, since observation time j mightrequire us make additional, earlier observation:XnsmsmP (Xj = xj | Xa = xa , Xb = xb )Ja:b (xa ,xb ; k) = max Ja:b (xa , xb ; 0), maxa<j<bRj (Xj | xj ) Cj (xj ) +max0lkj (xj )xjsmJa:j(xa , xj ; l)+sm(xj , xb ; kJj:b.l j (xj ))Looking back time possible filtering case, hence recursion simplifiesXnf ltf ltJa:b (xa ; k) = max Ja:b (xa ; 0),maxP (Xj = xj | Xa = xa )a<j<b:j (xj )kRj (Xj | xj ) Cj (xj ) +xjf ltJa:j(xa ; 0)+f ltJj:b(xj ; kj (xj )).J f lt J sm , optimal reward obtained J0:n+1 (; B) = J(; B) = J( ).Algorithm 2 presents pseudo code implementation smoothing version filtering casestraight-forward modification. call Algorithm 2 VO IDP algorithm conditional planning. algorithm fill dynamic programming tables using three loops, inner loopranging assignments xa , xb , middle loop ranging pairs (a, b) < b,outer loop covers increasing values k B. Within innermost loop, algorithmcomputes table sel sel(j) optimal reward achievable selecting variable j next.569fiK RAUSE & G UESTRINvalue expectation possible observation variable Xj make. Noteevery possible instantiation Xj = xj different allocation remaining budget k j (xj )left right sub-chain (a : j j : b respectively) chosen. quantity (j, xj )tracks optimal budget allocation.Input: Budget B, rewards Rj , costs j penalties CjOutput: Optimal conditional plan (a:b , a:b )beginsm (x , x ; 0);0 < b n + 1, xa dom Xa , xb dom Xb compute Ja:bbk = 1 B0 < b n+1, xa dom Xa , xb dom Xbsm (0);sel(1) := Ja:b< j < bsel(j) := 0;xj dom Xj0 l k j (xj )sm (x , x ; l) + J sm (x , x ; k l (x ));bd(l) := Ja:jjj jjbj:bendsel(j) := sel(j) + P (xj | xa , xb ) [Rj (Xj | xj ) Cj (xj ) + maxl bd(j)];(j, xj ) = argmaxl bd(j);endendsm (k) = maxJa:bj{a+1,...,b1,1} sel(j);a:b (xa , xb ; k) = argmaxj{a+1,...,b1,1} sel(j);xj dom Xa:b (k) a:b (xa , xb , xj ; k) = (a:b (k), xj );endendendAlgorithm 2: VO IDP algorithm computating optimal conditional plan (for smoothingsetting).Input: Budget k, observations Xa = xa , Xb = xb , ,beginj := a:b (xa , xb ; k);j 0Observe Xj = xj ;l := a:b (xa , xb , xj ; k);Recurse k := l, Xa = xa Xj = xj instead Xb = xb ;Recurse k := k l j , Xj = xj instead Xa = xa , Xb = xb ;endendAlgorithm 3: Observation selection using conditional planning.plan compactly encoded quantities a:b a:b . Hereby, a:b (xa , xb ; k)determines next variable query observing Xa = xa Xb = xb , remaining budget k. a:b (xa , xb , xj ; k) determines allocation budget new observationXj = xj made. Considering exponential number possible sequences observations,570fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSremarkable optimal plan represented using polynomial space. Algorithm 3indicates computed plan executed. procedure recursive, requiring parameters := 0, xa := 1, b := n + 1, xb := 1 k := B initial call. temperaturemonitoring example, could first collect temperature timeseries training data,learn chain model data. Offline, would compute conditional plan (forfiltering setting), encode quantities a:b a:b . would deploy computedplan actual sensor node, together implementation Algorithm 3. computation optimal plan (Algorithm 2) fairly computationally expensive, execution plan(Algorithm 3) efficient (selecting next timestep observation requires single lookupa:b a:b tables) hence well-suited deployment small, embedded device.summarize analysis following Theorem:Theorem 2 (Conditional Planning). algorithm smoothing presented computesoptimal conditional plan d3 B 2 ( 61 n3 + O(n2 )) evaluations local rewards,maximum domain size random variables X1 , . . . , Xn . filtering case, optimal plan computed using d3 B ( 61 n3 + O(n2 )) evaluations, or, budget used,d3 ( 16 n4 + O(n3 )) evaluations.faster computation filtering / no-budget case obtained observingrequire third maximum computation, distributes budget sub-chains.Also, note contrary algorithm computing optimal subsets Section 4.1, Algorithm 2 requires evaluations form R(Xj | XA = xA ), general computedd2 times faster expectations R(Xj | XA ). consideration, subset selectionalgorithm general factor B faster, even though conditional planning algorithmnested loops.4.3 Efficient Algorithms Trees LeavesSections 4.1 4.2 presented dynamic programming-based algorithms optimize value information chain graphical models. fact, key observations Section 3local rewards decompose along chains holds chain graphical models, also trees.formally, tree graphical model joint probability distribution P (XV ) collectionrandom variables XV P (XV ) factorsP (XV ) =1i,j (Xi , Xj ),Z(i,j)Ei,j nonnegative potential function, mapping assignments xi xj nonnegativereal numbers, E V V set edges form undirected tree index set V, Znormalization constant enforcing valid probability distribution.dynamic programming algorithms presented previous sections extendedtree models straightforward manner. Instead identifying optimal subsets conditionalplans sub-chains, algorithms would select optimal subsets plans sub-treesincreasing size. Note however number sub-trees grow exponentially numberleaves tree: star n leaves example number subtrees exponentialn. fact, counting number subtrees arbitrary tree n vertices believedintractable (#P-complete, Goldberg & Jerrum, 2000). However, trees contain571fiK RAUSE & G UESTRINsmall (constant) number leaves, number subtrees polynomial, optimal subsetconditional plans computed polynomial time.5. Theoretical LimitsMany problems solved efficiently discrete chain graphical models also efficiently solved discrete polytrees3 . Examples include probabilistic inference probable explanation (MPE).Section 4.3 however seen complexity dynamic programming algorithms chains increases dramatically extended trees: complexity increases exponentially number leafs tree.prove that, perhaps surprisingly, problem optimizing value information,exponential increase complexity cannot avoided, reasonable complexity theoretic assumptions. making statement formal, briefly review complexity classesused results.5.1 Brief Review Relevant Computational Complexity Classesbriefly review complexity classes used following statements presenting completeproblem class. details see, e.g., references Papadimitriou (1995)Littman, Goldsmith, Mundhenk (1998). class NP contains decision problemspolynomial-time verifiable proofs. well-known complete problem 3SATinstances Boolean formulas conjunctive normal form containing three literals perclause (3CNF form). complexity class #P contains counting problems. complete problemclass #P #3SAT counts number satisfying instances 3CNF formula.PP decision version class #P: complete problem AJSAT , decideswhether given 3CNF formula satisfied majority, i.e., halfpossible assignments. B Turing machine based complexity classes, ABcomplexity class derived allowing Turing machines deciding instances oracle callsTuring machines B. intuitively think problems class ABsolved Turing Machine class A, special command solves problem B.PP similar #P PPP = P#P , i.e., allow deterministic polynomial time Turingmachine access counting oracle, cannot solve complex problems giveaccess majority oracle. Combining ideas, class NPPP class problemssolved nondeterministic polynomial time Turing machines access majority(or counting) oracle. complete problem NPPP EM AJSAT which, given 3CNFvariables X1 , . . . , X2n , decides whether exists assignment X1 , . . . , Xnsatisfied majority assignments Xn+1 , . . . , X2n . NPPP introducedfound natural class modeling AI planning problems seminal work Littman et al.(1998). example, MAP assignment problem NPPP -complete general graphicalmodels, shown Park Darwiche (2004).complexity classes satisfy following set inclusions (where inclusions assumed,known strict):P NP PP PPP = P#P NPPP .3. Polytrees Bayesian Networks form trees edge directions dropped.572fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS5.2 Complexity Computing Optimizing Value Informationorder solve optimization problems, likely evaluate objectivefunction, i.e., expected local rewards. first result states that, even specialize decision theoretic value information objective functions defined Section 2.1, problemintractable even Naive Bayes models, special case discrete polytrees. Naive Bayes modelsoften used classification tasks (c.f., Domingos & Pazzani, 1997), class variablepredicted noisy observations (features), assumed conditionally independent givenclass variable. sense, Naive Bayes models next simplest (from perspectiveinference) class Bayesian networks chains. Note Naive Bayes models correspondstars referred Section 4.3, number subtrees exponential numbervariables.Theorem 3 (Hardness computation Naive Bayes models). computation decisiontheoretic value information functions #P-complete even Naive Bayes models. alsohard approximate factor unless P = NP.immediate corollary subset selection problem PP-hard Naive Bayesmodels:Corollary 4 (Hardness subset selection Naive Bayes models). problem determining,given Naive Bayes model, constants c B, cost function set decision-theoretic valueinformation objective functions Ri , whether subset variables VL(A) c (A) B PP-hard.fact, show subset selection arbitrary discrete polytrees (that generalNaive Bayes models, inference still tractable) even NPPP -complete, complexityclass containing problems believed significantly harder NP #P completeproblems. result provides complexity theoretic classification value information, coreAI problem.Theorem 5 (Hardness subset selection computation polytrees). problem determining, given discrete polytree, constants c B, cost function set decision-theoreticvalue information objective functions Ri , whether subset variables VL(A) c (A) B NPPP -complete.running example, implies generalized problem optimally selecting k sensorsnetwork correlated sensors likely computationally intractable without resortingheuristics. corollary extends hardness subset selection hardness conditional plans.Corollary 6 (Hardness conditional planning computation polytrees). Computing conditional plans PP-hard Naive Bayes models NPPP -hard discrete polytrees.proofs results section stated Appendix. rely reductions completeproblems NP, #P NPPP involving boolean formulae problems computing / optimizing value information. reductions inspired works Littman et al. (1998) ParkDarwiche (2004), require development novel techniques, new reductionsBoolean formulae Naive Bayes polytree graphical models associated appropriate rewardfunctions, ensuring observation selections lead feasible assignments Boolean formulae.573fiK RAUSE & G UESTRINPercent improvement101Optimal conditional plan8Mean margin optimal subsetMean margin greedy heuristic10.980.9Mean F1 score0.9660.940.8420.7Optimal subsetGreedy heuristic0148121620Number observations24(a) Sensor scheduling0.60.9Mean accuracygreedy heuristic12345Number observations(b) CpG island detectionMean margin0.92Mean accuracyoptimal subset60.880.86010203040Number observations50(c) Part Speech TaggingFigure 3: Experimental results. (a) Temperature data: Improvement uniform spacingheuristic. (b) CpG island data set: Effect increasing number observationsmargin classification accuracy. (c) Part-of-Speech tagging data set: Effect increasing number observations margin F1 score.6. Experimentssection, evaluate algorithms several real world data sets. special focuscomparison optimal methods greedy heuristic heuristic methods selecting observations, algorithms used interactive structured classification.6.1 Temperature Time Seriesfirst data set consists temperature time series collected sensor network deployedIntel Research Berkeley (Deshpande et al., 2004) described running example. Datacontinuously collected 19 days, linear interpolation used case missing samples.temperature measured every 60 minutes, discretized 10 bins 2 degreesKelvin. avoid overfitting, used pseudo counts = 0.5 learning model. Usingparameter sharing, learned four sets transition probabilities: 12 - 7am, 7 - 12 pm,12 pm - 7 pm 7 pm - 12 am. Combining data three adjacent sensors, got 53 sampletime series.goal task select k 24 time points day, sensorreadings informative. experiment designed compare performanceoptimal algorithms, greedy heuristic, uniform spacing heuristic, distributed kobservations uniformly day. Figure 3(a) shows relative improvement optimal algorithms greedy heuristic uniform spacing heuristic. performance measureddecrease expected entropy, zero observations baseline. seen k lesshalf possible observations, optimal algorithms decreased expected uncertainty several percent heuristics. improvement gained optimal plansubset selection algorithms appears become drastic large number observations(over half possible observations) allowed. Furthermore, large number observations,optimal subset subset selected greedy heuristic almost identical.574fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS6.2 CpG-Island Detectionstudied bioinformatics problem finding CpG islands DNA sequences. CpG islandsregions genome high concentration cytosine-guanine sequence. areasbelieved mainly located around promoters genes, frequently expressedcell. experiment, considered gene loci HS381K22, AF047825 AL133174,GenBank annotation listed three, two one CpG islands each. ran algorithm50 base window beginning end island, using transition emissionprobabilities Durbin, Eddy, Krogh, Mitchison (1999) Hidden Markov Model,used sum margins reward function.goal experiment locate beginning ending CpG islandsprecisely asking experts, whether certain bases belong CpG region not. Figure 3(b) shows mean classification accuracy mean margin scores increasing numberobservations. results indicate that, although expected margin scores similaroptimal algorithm greedy heuristic, mean classification performance optimal algorithm still better performance greedy heuristic. example, making 6observations, mean classification error obtained optimal algorithm 25% lowererror obtained greedy heuristic.6.3 Part-of-Speech Taggingthird experiment, investigated structured classification task part-of-speech (POS)tagging (CoNLL, 2003). Problem instances sequences words (sentences), wordpart entity (e.g., European Union), entity belongs one five categories:Location, Miscellaneous, Organization, Person Other. Imagine application, automaticinformation extraction guided expert: algorithms compute optimal conditional planasking expert, trying optimize classification performance requiring little expertinteraction possible.used conditional random field structured classification task, node corresponds word, joint distribution described node potentials edge potentials.sum margins used reward function. Measure classification performance F1score, geometric mean precision recall. goal experiment analyzeaddition expert labels increases classification performance, indirect, decomposing reward function used algorithms corresponds real world classification performance.Figure 3(c) shows increase mean expected margin F1 score increasing number observations, summarized ten 50 word sequences. seen classificationperformance effectively enhanced optimally incorporating expert labels. Requestingthree 50 labels increased mean F1 score five percent. followingexample illustrates effect: one scenario words entity, sportsman P. Simmons,classified incorrectly P. Simmons Miscellaneous. first requestoptimal conditional plan label Simmons. Upon labeling word correctly, word P.automatically labeled correctly also, resulting F1 score 100 percent.575fiK RAUSE & G UESTRIN7. Applying Chain Algorithms General Graphical ModelsSection 4 seen algorithms used schedule single sensor, assuming timeseries sensor readings (e.g., temperature) form Markov chain. natural assumptionsensor networks (Deshpande et al., 2004). deploying sensor networks however, multiplesensors need scheduled. time series sensors independent, could usealgorithms schedule sensors independently other. However, practice,measurements correlated across different sensors fact, dependence essentialallow generalization measurements locations sensor placed. following, describe approach using single-sensor scheduling algorithm coordinatemultiple sensors.formally, interested monitoring spatiotemporal phenomenon set locations = {1, . . . , m}, time steps = {1, . . . , }. locationtime pair s, t,associate random variable Xs,t describes state phenomenon locationtime. random vector XS,T fully describes relevant state world vector XS,tdescribes state particular time step t. before, make Markov assumption, assumingconditional independence XS,t XS,t0 given XS,t1 t0 < 1.Similarly single-chain case, consider reward functions Rs,t associatedvariable Xs,t . goal select, timestep, set sensors activate,order maximize sum expected rewards. Letting A1:t = A1 , expected totalreward givenXRs,t (Xs,t | XA1:t )s,tfiltering setting (i.e., observations past taken account evaluatingrewards),XRs,t (Xs,t | XA1:T )s,tsmoothing setting (where observations taken account). generalizationconditional planning done described Section 2.Note case single sensor (` = 1), problem optimal sensor schedulingsolved using Algorithm 1. Unfortunately, optimization problem wildly intractable evencase two sensors, ` = 2:Corollary 7 (Hardness sensor selection two chains). Given model two dependentchains, constants c B, cost function set decision theoretic value informationfunctions Rs,t , NPPP -complete determine whether subset A1:T variablesL(A1:T ) c (A1:T ) B.following, develop approximate algorithm uses optimal single-chain algorithms performs well practice.7.1 Approximate Sensor Scheduling Lower Bound Maximizationreason sudden increase complexity case multiple chains decomposition rewards along sub-chains (as described Section 3) extend case multiple576fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSS(1)1S(1)2S(1)3S(1)4S(1)5S(2)1S(2)2S(2)3S(2)4S(2)5Figure 4: Scheduling multiple correlated sensors dynamic processes.sensors, since influence flow across chains. Figure 4 visualizes problem there, distri(1)(1)(2)bution sensor (2) depends three observations S1 S4 sensor (1) S2sensor (2).address complexity issue using (approximate) extension decomposition approach used single chains. focus decision-theoretic value information objective (as described Section 2.1), local reward functions, residual entropy,used well.Considering recent observations. first approximation, allow sensor takeaccount recent observations. Intuitively, appears reasonable approximation,especially potential scheduling times reasonably far apart. Formally, evaluatinglocal rewards time t, replace set observations time t, A1:t subsetA01:t A1:tA01:t = (s, t) A1:t : t0 (s, t0 ) A1:t ,i.e, sensor s, last observation (with largest time index t) kept.approximate Rs,t (Xs,t | A1:t ) Rs,t (Xs,t | A01:t ). Figure 4 example, A1:5 ={(s1 , 1), (s2 , 2), (s1 , 4)}, total expected utility time t5 would computed using observations A01:5 = {(s2 , 2), (s1 , 4)}, i.e., using time t4 sensor one, time t2 sensor two,(1)ignoring influence originating observation S1 flowing chains indicateddashed arrow. following proposition proves approximation lower boundtrue value information:Proposition 8 (Monotonicity value information). decision-theoretic value information Rs,t (A) set sensors monotonic A,Rs,t (A0 ) Rs,t (A)A0 A.Proposition 8 proves conditioning recent observations decreaseobjective function, hence maximizing approximate objective implies maximizing lower boundtrue objective.coordinate ascent approach. propose following heuristic maximizing lowerbound expected utility. Instead jointly optimizing schedules (timesteps selectedsensor), algorithm repeatedly iterate sensors. sensors s,optimize selected observations As1:T , holding schedules sensors fixed.577fiK RAUSE & G UESTRINprocedure resembles coordinate ascent approach, coordinate ranges possibleschedules fixed sensor s.optimizing sensor s, algorithm finds schedule As1:T[XAs1:T = argmaxRs,t Xs,t | XA01:tXA0s0 (As1:T ) B,(6)A1:Ts0 6=ss,t1:ti.e., maximizes, schedules A1:T , sum expected rewards time steps0sensors, given schedules As1:T non-selected sensors s0 .Solving single-chain optimization problem. order solve maximization problem(6) individual sensors, use dynamic programming approach introducedltSection 4. recursive case Lfa:b(k) k > 0 exactly same. However, base casecomputedb1 XX[f ltLa:b (0) =Rs,j Xs,j | XaXA0s0 ,s0 6=sj=a+11:ji.e., takes account recent observation non-selected sensors s0 .lt(0). First all,Several remarks need made computation base case Lfa:bnaive implementation, computation expected utility[Rs,j Xs,j | XaXA0s0s0 6=s1:jrequires time exponential number chains. case since, order computereward Rs,t , chain, possible observations XA0s= xA0scould made need1:t1:ttaken account. computation requires computing expectation joint distributionP (XA01:t ), exponential size. increase complexity avoided using samplingapproximation: Hoeffdings inequality used derive polynomial bounds sample complexity approximating value information arbitrarily small additive error , similarlydone approach Krause Guestrin (2005a)4 . practice, small number samplesappears provide reasonable performance. Secondly, inference becomes intractableincreasing number sensors. Approximate inference algorithms algorithm proposedBoyen Koller (1998) provide viable way around problem.Analysis. Since sensors maximize global objective L(A1:T ), coordinated ascentapproach guaranteed monotonically increase global objective every iteration (ignoringpossible errors due sampling approximate inference). Hence must converge (to localoptimum) finite number steps. procedure formalized Algorithm 4.Although cannot general provide performance guarantees procedure, building algorithm provides optimal schedule sensor isolation,benefit observations provided remaining sensors. Also, note sensorsindependent, Algorithm 4 obtain optimal solution. Even sensors correlated,obtained solution least good solution obtained scheduling sensors independently other. Algorithm 4 always converge, always compute lower bound4. absolute error evaluating reward Rs,t accumulate total error |T ||S|variables hence error optimal schedule.578fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSInput: Budget BOutput: Selection A1 , . . . , A` observation times sensorbeginSelect Ai , 1 ` random;repeat= 1 `Use Algorithm 1 select observations Ai sensor i, conditioning currentsensor scheduling Aj , j 6= i, remaining sensors;endCompute improvement total expected utility;small enough ;endAlgorithm 4: Multi-Sensor scheduling.expected total utility. Considering intractability general problem even two chains(c.f., , Corollary 7), properties reassuring. experiments, coordinated sensorscheduling performed well, discussed Section 7.2.7.2 Proof Concept Study Real Deploymentwork Singhvi et al. (2005), presented approach optimizing light controlbuildings, purpose satisfying building occupants preferences lighting conditions,simultaneously minimizing energy consumption. approach, wireless sensor networkdeployed monitors building environmental conditions (such sunlight intensityetc.). sensors feed measurements building controller actuates lighting system(lamps, blinds, etc.) accordingly. every timestep , building controller chooseaction affects lighting conditions locations building. Utility functionsUt (a, xS,t ) specified map chosen actions current lighting levels utilityvalue. utility chosen capture users preferences light levels, wellenergy consumption lighting system. Details utility functions described detailSinghvi et al..evaluated multi-sensor scheduling approach real building controller testbed,described detail Singhvi et al.. experiments, used Algorithm 4 schedule threesensors, allowing sensor choose subset ten time steps (in one-hour intervalsdaytime). varied number timesteps sensor activated, computedtotal energy consumption total user utility (as defined Singhvi et al.). Figure 5(a) showsmean user utility energy savings achieved, number observations varyingobservations continuous sensing (10 observations discretization)5 . results implyusing predictive model active sensing strategy, even small number observationsachieves results approximately good results achieved continuous sensing.Figure 5(b) presents mean total utility achieved using observations, one observation tenobservations per sensor day. seen even single observation per sensor increasestotal utility close level achieved continuous sensing. Figure 5(c) shows mean energy5. Note Figure 5(a), energy cost utility plotted different units directly compared.579fiK RAUSE & G UESTRIN612Energy cost101 Observ./sensor1510 Observ./sensorEnergy cost8Total utilityUser utility energy cost144observ.28observ.10501 Observ./sensorMeasured user utility6012 3Number observations102(a) Sensing scheduling evaluation101214Hour day16180(b) Total utility1010 Observ./sensor1214Hour day1618(c) Energy costFigure 5: Active sensing results.consumption required experiment. Here, single sensor observation strategy comeseven closer power savings achieved continuous sensing.Since sensor network battery lifetime general inversely proportional amountpower expended sensing communication, conclude sensor scheduling strategypromises lead drastic increases sensor network lifetime, deployment permanence reduced maintenance cost. testbed, network lifetime could increased factor 3without significant reduction user utility increase energy cost.8. Related Worksection, review related work number different areas.8.1 Optimal Experimental DesignOptimal experimental design general methodology selecting informative experiments inferaspects state world (such parameters particular nonlinear function,etc.). large literature different approaches experimental design (c.f., Chaloner &Verdinelli, 1995; Krause, Singh, & Guestrin, 2007).Bayesian experimental design, prior distribution possible states world assumed, experiments chosen, e.g., reduce uncertainty posterior distribution.general form, Bayesian experimental design pioneered Lindley (1956). users encodepreferences utility function U (P (), ? ), first argument, P (), distributionstates world (i.e., parameters) second argument, ? , true stateworld. Observations xA collected, change expected utility prior P ()posterior P ( | XA = xA ) used design criterion. sense, value observation problems considered paper considered instances Bayesian experimental designproblems. Typically, Bayesian Experimental Design employed continuous distributions, oftenmultivariate normal distribution. choosing different utility functions, different notionsoptimality defined, including A- D- optimality developed (Chaloner & Verdinelli,1995). posterior covariance matrix |A , whosemaximumeigenvalue max ,Bayesian A-, D-, E- optimality minimizes tr |A , det |A , max |A , respectively. terminology Section 2.1, D-optimality corresponds choosing total entropy,A-optimality corresponds (weighted) mean-squared error criteria.580fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSEven multivariate normal distributions, optimal Bayesian Experimental design NP-hard(Ko, Lee, & Queyranne, 1995). applications experimental design, number experiments selected often large compared number design choices. cases, onefind fractional design (i.e., non-integral solution defining proportions experimentsperformed), round fractional solutions. fractional formulation, A-, D-,E-optimality criteria solved exactly using semi-definite program (Boyd & Vandenberghe,2004). however known bounds integrality gap, i.e., loss incurredrounding process.algorithms presented Section 4.1 used optimally solve non-fractional BayesianExperimental Design problems chain graphical models, even continuous distributions,long inference distributions tractable (such normal distributions). paper henceprovides new class combinatorial algorithms interesting class Bayesian experimentaldesign problems.8.2 Value Information Graphical ModelsDecision-theoretic value information frequently used principled information gathering (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), popularized decisionanalysis context influence diagrams (Howard & Matheson, 1984). sense, valueinformation problems special cases Bayesian experimental design problems, priordistribution particular structure, typically given graphical model consideredpaper.Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen,1997; Kapoor et al., 2007) suggested myopic, i.e., greedy approaches selectively gatheringevidence graphical models, considered paper, which, unlike algorithms presentedpaper. algorithms applicable much general graphical models,theoretical guarantees. Heckerman et al. (1993) propose method computemaximum expected utility specific sets observations. work considers generalgraphical models paper (Naive Bayes models certain extensions), providelarge sample guarantees evaluation given sequence observations, use heuristicwithout guarantees select sequences. Bilgic Getoor (2007) present branch boundapproach towards exactly optimizing value information complex probabilistic models.contrast algorithms described paper however, approach running timeworst-case exponential. Munie Shoham (2008) present algorithms hardness resultsoptimizing special class value information objective functions motivated optimaleducational testing problems. algorithms apply different class graphical modelschains, apply specific objective functions, rather general local reward functionsconsidered paper. Radovilsky, Shattah, Shimony (2006) extended previous versionpaper (Krause & Guestrin, 2005a) obtain approximation algorithms guaranteescase noisy observations (i.e., selecting subset emission variables observe, ratherselecting among hidden variables considered paper).8.3 Bandit Problems Exploration / Exploitationimportant class sequential value information problems class Bandit problems.classical k-armed bandit problem, formalized Robbins (1952), slot machine given581fiK RAUSE & G UESTRINk arms. draw arm results reward success probability pi fixedarm, different (and independent) across arm. selecting arms pull, importantproblem trade exploration (i.e., estimation success probabilities arms)exploitation (i.e., repeatedly pulling best arm known far). celebrated result GittinsJones (1979) shows fixed number draws, optimal strategy computedpolynomial time, using dynamic programming based algorithm. similar senseoptimal sequential strategy computed polynomial time, Gittins algorithm howeverdifferent structure dynamic programming algorithms presented paper.Note using function optimization objective function described Section 2.1,approach used solve particular instance bandit problems, armsrequired independent, but, contrary classical notion bandit problems,chosen repeatedly.8.4 Probabilistic PlanningOptimized information gathering also extensively studied planning community.Bayer-Zubek (2004) example proposed heuristic method based Markov Decision Process framework. However, approach makes approximations without theoretical guarantees.problem optimizing decision theoretic value information naturally formalized(finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik,1973). Hence, principle, algorithms planning POMDPs, anytime algorithmPineau, Gordon, Thrun (2006), employed optimizing value information. Unfortunately, state space grows exponentially number variables consideredselection problem. addition, complexity planning POMDPs grows exponentiallycardinality state space, hence doubly-exponentially number variables selection. steep increase complexity makes application black-box POMDP solvers infeasible.Recently, Ji, Parr, Carin (2007) demonstrated use POMDP planning multi-sensorscheduling problem. presenting promising empirical results, approach however usesapproximate POMDP planning techniques without theoretical guarantees.robotics literature, Stachniss, Grisetti, Burgard (2005), Sim Roy (2005)Kollar Roy (2008) presented approaches information gathering context Simultaneous Localization Mapping (SLAM). None approaches however provide guaranteesquality obtained solutions. Singh, Krause, Guestrin, Kaiser, Batalin (2007)present approximation algorithm theoretical guarantees problem planning informative path environmental monitoring using Gaussian Process models. contrastalgorithms presented paper, dealing complex probabilistic modelscomplex cost functions arising path planning, approach requires submodular objectivefunctions (a property hold value information show Proposition 9).8.5 Sensor Selection Schedulingcontext wireless sensor networks, sensor nodes limited battery henceenable small number measurements, optimizing value information selectedsensors plays key role. problem deciding selectively turn sensors orderconserve power first discussed Slijepcevic Potkonjak (2001) Zhao, Shin, Reich(2002). Typically, assumed sensors associated fixed sensing region, spatial582fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSdomain needs covered regions associated selected sensors. Abrams, Goel,Plotkin (2004) present efficient approximation algorithm theoretical guaranteesproblem. Deshpande, Khuller, Malekian, Toossi (2008) present approach problembased semidefinite programming (SDP), handling general constraints providing tighterapproximations. approaches described apply problem optimizing sensor schedules complex utility functions as, e.g., increase prediction accuracyobjectives considered paper. address shortcomings, Koushanfary, Taft,Potkonjak (2006) developed approach sensor scheduling guarantees specified prediction accuracy based regression model. However, approach relies solutionMixed Integer Program, intractable general. Zhao et al. (2002) proposed heuristicsselectively querying nodes sensor network order reduce entropy prediction. Unlike algorithms presented paper, approaches performanceguarantees.8.6 Relationship Machine LearningDecision Trees (Quinlan, 1986) popularized value information criterion creatingconditional plans. Unfortunately, guarantees performance greedy method.subset selection problem instance feature selection central issue machinelearning, vast amount literature (see Molina, Belanche, & Nebot, 2002 survey).However, aware work providing similarly strong performance guaranteesalgorithms considered paper.problem choosing observations also strong connection field active learning(c.f., Cohn, Gharamani, & Jordan, 1996; Tong & Koller, 2001) learning system designsexperiments based observations. sample complexity bounds derivedactive learning problems (c.f., Dasgupta, 2005; Balcan, Beygelzimer, & Langford, 2006),aware active learning algorithms perform provably optimal (even restrictedclasses problem instances).8.7 Previous Work Authorsprevious version paper appeared work Krause Guestrin (2005b).contents Section 7 appeared part work Singhvi et al. (2005). present versionmuch extended, new algorithmic hardness results detailed discussions.light negative results presented Section 5, cannot expect able optimize value information complex models chains. However, instead attemptingsolve optimal solution, one might wonder whether possible obtain good approximations. authors showed (Krause & Guestrin, 2005a; Krause et al., 2007; Krause, Leskovec,Guestrin, VanBriesen, & Faloutsos, 2008) large number practical objective functions satisfy intuitive diminishing returns property: Adding new observation helpsobservations far, less already made many observations. intuition formalized using combinatorial concept called submodularity. fundamental result Nemhauseret al. proves optimizing submodular utility function, myopic greedy algorithmfact provides near-optimal solution, within constant factor (11/e) 63% optimal.Unfortunately, decision theoretic value information satisfy submodularity.583fiK RAUSE & G UESTRINProposition 9 (Non-submodularity value information). Decision-theoretic value information submodular, even Naive Bayes models.Intuitively, value information non-submodular, need make several observationsorder convince need change action.9. Conclusionsdescribed novel efficient algorithms optimal subset selection conditional plan computation chain graphical models (and trees leaves), including HMMs. empiricalevaluation indicates algorithms improve upon commonly used heuristics decreasing expected uncertainty. algorithms also effectively enhance performance interactivestructured classification tasks.Unfortunately, optimization problems become wildly intractable even slight generalization chains. presented surprising theoretical limits, indicate even classdecision theoretic value information functions (as widely used, e.g., influence diagramsPOMDPs) cannot efficiently computed even Naive Bayes models. also identified optimization value information new class problems intractable (NPPP -complete)polytrees.hardness results, along recent results polytree graphical models, NPcompleteness maximum posteriori assignment (Park & Darwiche, 2004) NP-hardnessinference conditional linear Gaussian models (Lerner & Parr, 2001), suggest possibilitydeveloping generalized complexity characterization problems hard polytree graphicalmodels.light theoretical limits computing optimal solutions, natural question askwhether approximation algorithms non-trivial performance guarantees found. Recentresults Krause Guestrin (2005a), Radovilsky et al. (2006) Krause et al. (2007) showcase interesting classes value information problems.Acknowledgmentswould like thank Ben Taskar providing part-of-speech tagging model, Reutersmaking news archive available. would also like thank Brigham Anderson Andrew Moore helpful comments discussions. work partially supported NSFGrants No. CNS-0509383, CNS-0625518, ARO MURI W911NF0710287 gift Intel.Carlos Guestrin partly supported Alfred P. Sloan Fellowship, IBM Faculty Fellowship ONR Young Investigator Award N00014-08-1-0752 (2008-2011). Andreas Krausepartially supported Microsoft Research Graduate Fellowship.AppendixProof Theorem 3. Membership #P arbitrary discrete polytrees straightforward sinceinference models P. Let instance #3SAT , countnumber assignments X1 , . . . , Xn satisfying . Let C = {C1 , . . . , Cm } setclauses. create Bayesian network 2n + 1 variables, X1 , . . . , Xn , U1 , . . . , Un Y,Xi conditionally independent given Y. Let uniformly distributed values584fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSU1U2UnX1X2XnFigure 6: Graphical model used proof Theorem 3.{n, (n 1), . . . , 1, 1, . . . , 1, m}, Ui Bernoulli prior p = 0.5. Letobserved variables Xi CPTs defined following way:1, Xi = u satisfies clause Cj ;Xi | [Y = +j, Ui = u]0, otherwise.0, = j;Xi | [Y = j, Ui = u]u, otherwise.model, presented Figure 6, holds X1 = X2 = = Xn = 1 iff U1 , . . . , Unencode satisfying assignment , > 0. Hence, observe X1 = X2 = = Xn = 1,know > 0 certainty. Furthermore, least one Xi = 0, knowP (Y > 0 | X = x) < 1. Let nodes zero reward, except Y, assignedreward function following properties (we show model localreward function using decision-theoretic value information):(n+m)2n, P (Y > 0 | XA = xA ) = 1;R(Y | XA = xA ) =0,otherwise.argument, expected rewardXR(Y | X1 , . . . , Xn ) =P (Y = y)P (U = u)P (x| u)R(Y | X = x)u,y,x=XP (Y > 0)P (u)u satX(n + m)2n=1u satexactly number satisfying assignments . Note model defined yetNaive Bayes model. However, easily turned one marginalizing U.show realize reward function properties maximum expected utility sense. Let = {d1 , d2 } set two decisions. Define utility functionproperty:(n+m)2n,= d1 > 0;(n+m)22n+1u(y, d) =, = d1 < 0;0, notherwise.reward R(Y | XA ) given decision-theoretic value information:XXR(Y | XA ) =P (xA ) maxP (y | xA )u(y, d).xA585fiK RAUSE & G UESTRINFigure 7: Graphical model used proof Theorem 5.utility function u based following consideration. Upon observing particular instantiation variables X1 , . . . , Xn make decision variable Y. goal achievenumber times action d1 chosen exactly corresponds number satisfying assignments . accomplished following way. Xi 1, know Uiencoded satisfying assignment, > 0 probability 1. case, action d1 chosen.need make sure whenever least one Xi = 0 (which indicates either < 0U satisfying assignment) decision d2 chosen. Now, least one Xi = 0, either= j > 0 clause j satisfied, < 0. utilities designed unlessnP (Y > 0 | XA = xA ) 1 n22m , action d2 gives higher expected reward 0. Hereby,n2n2m lower bound probability misclassification P (Y < 0 | XA = xA ).Note construction immediately proves hardness approximation: Supposepolynomial time algorithm computes approximation R withinfactor > 1 (which depend problem instance) R = R(Y | X1 , . . . , Xn ). R > 0implies R > 0, R = 0 implies R = 0. Hence, approximation R useddecide whether satisfiable not, implying P = NP.Proof Corollary 4. Let 3CNF formula. convert Naive Bayes model variables X1 , . . . , Xn construction Theorem 3. function L(V)V = {1, . . . , n} set variables Xi counts number satisfying assignments .Note function L(A) V = {1, . . . , n} monotonic, i.e., L(A) L(V)V, shown Proposition 8. Hence majority assignments satisfiesL(V) > 2n1 .Proof Theorem 5. Membership follows fact inference polytrees P discrete polytrees: nondeterministic Turing machine #P oracle first guess selectionvariables, compute value information using Theorem 3 (since computation#P-complete arbitrary discrete polytrees), compare constant c.show hardness, let instance EM AJSAT , find instantiationX1 , . . . , Xn (X1 , . . . , X2n ) true majority assignments Xn+1 , . . . , X2n .Let C = {C1 , . . . , Cm } set 3CNF clauses. Create Bayesian network shown Figure 7,nodes Ui , uniform Bernoulli prior. Add bivariate variables Yi = (seli , pari ),0 2n, seli takes values {0, . . . , m} pari parity bit. CPTs Yi586fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSdefined as: sel0 uniformly varies {1, . . . , m}, par0 = 0, Y1 , . . . , Y2n :0, j = 0, ui satisfies Cj ;seli | [seli1 = j, Ui = ui ]j, otherwise;pari | [pari1 = bi1 , Ui ] bi1 Ui ,denotes parity (XOR) operator. add variables ZiT ZiF 1 nletUniform({0, 1}), ui = 1;Zi | [Ui = ui ]0,otherwise;Uniform denotes uniform distribution. Similarly, letUniform({0, 1}), ui = 0;ZiF | [Ui = ui ]0,otherwise.Intuitively, ZiT = 1 guarantees us Ui = 1, whereas ZiT = 0 leaves us uncertain Ui .case ZiF symmetric.use subset selection algorithm choose Zi encode solution EM AJSAT .ZiT chosen, indicate Xi set true, similarly ZiF indicates false assignmentXi . parity function going used ensure exactly one {ZiT , ZiF } observedi.first assign penalties nodes except ZiT , ZiF 1 n, Ujn + 1 j 2n, assigned zero penalty. Let nodes zero reward, exceptY2n , assigned following reward:n4 , P (sel2n = 0 | XA = xA ) = 1[P (par2n = 1 | XA = xA ) = 1 P (par2n = 0 | XA = xA ) = 1];R(Y2n | XA = xA ) =0,otherwise.Note sel2n = 0 probability 1 iff U1 , . . . , U2n encode satisfying assignment . Furthermore, get positive reward certain sel2n = 0, i.e., chosen observationset must contain proof satisfied, certain par2n . parity certaintyoccur certain assignment U1 , . . . , U2n . possible infervalue Ui certainty observing one Ui , ZiT ZiF . Since, = 1, . . . , n, costobserving Ui , receive reward must observe least one ZiT ZiF . Assumecompute optimal subset budget 2n, receive positive rewardobserving exactly one ZiT ZiF .interpret selection ZiT ZiF assignment first n variables EM AJSAT .Let R = R(Y2n | O). claim EM AJSAT R > 0.5. First letEM AJSAT , assignment x1 , . . . , xn first n variables. add Un+1 , . . . , U2nadd ZiT iff xi = 1 ZiF iff xi = 0. selection guarantees R > 0.5.assume R > 0.5. call assignment U1 , . . . , U2n consistent 1 n,ZiT O, Ui = 1 ZiF Ui = 0. consistent assignment, chanceobservations Zi prove consistency 2n . Hence R > 0.5 implies majorityprovably consistent assignments satisfy hence EM AJSAT . proves subsetselection NPPP complete.Note realize local reward function R sense maximum expected utilitysimilarly described Proof Theorem 3.587fiK RAUSE & G UESTRINProof Corollary 6. constructions proof Theorem 4 Theorem 5 also provecomputing conditional plans PP-hard NPPP -hard respectively, since, instances,plan positive reward must observe variables corresponding valid instantiations (i.e.,X1 , . . . , Xn Corollary 4, Un+1 , . . . , U2n one Z1 , . . . , Zn satisfyparity condition Theorem 5). cases, order selection irrelevant, and, hence,conditional plan effectively performs subset selection.Proof Corollary 7. proof follows observation polytree constructionproof Theorem 5 arranged two dependent chains. transformation, revertarc ZiT Ui applying Bayes rule. make sure numbernodes sensor timeslice, triple variables Yi , calling copies Yi0 Yi00 .conditional probability tables given equality constraints, Yi0 = Yi Yi00 = Yi0 .transformation, variables associated timesteps 3i 2 (for 1) given sets00 , Z }. timesteps 3i 1 associated sets {U , }, timesteps 3i associated{Yi1{ZiF , Yi0 }.Proof Proposition 8. bound follows fact maximization convex,application Jensens inequality. Using induction argument, simply need showL(A) L().!XXL(A) =P (XA = xA )max EU (a, t, x | XA1:t = xA1:t )xAtV!XtV=XtVmaxXP (XA = xA )EU (a, t, x | XA1:t = xA1:t )xAmax EU (a, t, x) = L()EU (a, t, x | XA1:t = xA1:t ) =XP (xt | XA1:t = xA1:t )Ut (a, xt )xtexpected utility action time observing XA1:t = xA1:t .Proof Proposition 9. Consider following binary classification problem assymetric cost.one Bernoulli random variable (the class label) P (Y = 1) = 0.5P (Y = 1) = 0.5. also two noisy observations X1 , X2 , conditionally independent given Y. Let P (Xi = Y) = 3/4 (i.e., observations agree class labelprobability 3/4, disagree probability 1/4. three actions, a1 (classifying 1),a1 (classifying -1) a0 (not assigning label). define utility functon Ugain utility 1 assign label correctly (U (a1 , 1) = U (a1 , 1) = 1), 3 misassignlabel (U (a1 , 1) = U (a1 , 1) = 3), 0 choose a0 , i.e., assign label. Now,226> 0.verify L() = L({X1 }) = L({X2 }) = 0, L({X1 , X2 }) = 43 3 14 = 16Hence, adding X2 X1 increases utility adding X2 empty set, contradictingsubmodularity.588fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSReferencesAbrams, Z., Goel, A., & Plotkin, S. (2004). Set k-cover algorithms energy efficient monitoringwireless sensor networks.. IPSN.Balcan, N., Beygelzimer, A., & Langford, J. (2006). Agnostic active learning. ICML.Baum, L. E., & Petrie, T. (1966). Statistical inference probabilistic functions finite stateMarkov chains. Ann. Math. Stat, 37, 15541563.Bayer-Zubek, V. (2004). Learning diagnostic policies examples systematic search. UAI.Bellman, R. (1957). Markovian decision process. Journal Mathematics Mechanics, 6.Bilgic, M., & Getoor, L. (2007). Voila: Efficient feature-value acquisition classification.Twenty-Second Conference Artificial Intelligence (AAAI).Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge UP.Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes. Uncertainty Artificial Intelligence (UAI).Chaloner, K., & Verdinelli, I. (1995). Bayesian experimental design: review. Statistical Science,10(3), 273304.Cohn, D. A., Gharamani, Z., & Jordan, M. I. (1996). Active learning statistical models. J AIResearch, 4, 129145.CoNLL (2003).Conference computational natural language learning shared task.http://cnts.uia.ac.be/conll2003/ner/.Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. Wiley Interscience.Dasgupta, S. (2005). Coarse sample complexity bounds active learning. NIPS.Deshpande, A., Guestrin, C., Madden, S., Hellerstein, J., & Hong, W. (2004). Model-driven dataacquisition sensor networks. VLDB.Deshpande, A., Khuller, S., Malekian, A., & Toossi, M. (2008). Energy efficient monitoringsensor networks. LATIN.Dittmer, S., & Jensen, F. (1997). Myopic value information influence diagrams. UAI, pp.142149, San Francisco.Domingos, P., & Pazzani, M. (1997). optimality simple Bayesian classifierzero-one loss. Machine Learning, 29, 103137.Durbin, R., Eddy, S. R., Krogh, A., & Mitchison, G. (1999). Biological Sequence Analysis : Probabilistic Models Proteins Nucleic Acids. Cambridge University Press.Gittins, J. C., & Jones, D. M. (1979). dynamic allocation index discounted multiarmedbandit problem. Biometrika, 66(3), 561565.Goldberg, L. A., & Jerrum, M. (2000). Counting unlabelled subtrees tree #p-complete. LMSJ Comput. Math., 3, 117124.Heckerman, D., Horvitz, E., & Middleton, B. (1993). approximate nonmyopic computationvalue information. IEEE Trans. Pattern Analysis Machine Intelligence, 15, 292298.589fiK RAUSE & G UESTRINHoward, R. A. (1966). Information value theory. IEEE Transactions Systems ScienceCybernetics (SSC-2).Howard, R. A., & Matheson, J. (1984). Readings Principles Applications DecisionAnalysis II, chap. Influence Diagrams, pp. 719762. Strategic Decision Group, Menlo Park.Reprinted 2005 Decision Analysis 2(3) 127-143.Ji, S., Parr, R., & Carin, L. (2007). Non-myopic multi-aspect sensing partially observableMarkov decision processes. IEEE Transactions Signal Processing, 55(6), 27202730.Kapoor, A., Horvitz, E., & Basu, S. (2007). Selective supervision: Guiding supervised learningdecision-theoretic active learning. International Joint Conference Artificial Intelligence(IJCAI).Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences ValueTrade-offs. Wiley.Ko, C., Lee, J., & Queyranne, M. (1995). exact algorithm maximum entropy sampling.Operations Research, 43(4), 684691.Kollar, T., & Roy, N. (2008). Efficient optimization information-theoretic exploration slam.AAAI.Koushanfary, F., Taft, N., & Potkonjak, M. (2006). Sleeping coordination comprehensive sensingusing isotonic regression domatic partitions. Infocom.Krause, A., & Guestrin, C. (2005a). Near-optimal nonmyopic value information graphicalmodels. Proc. Uncertainty Artificial Intelligence (UAI).Krause, A., & Guestrin, C. (2005b). Optimal nonmyopic value information graphical models- efficient algorithms theoretical limits. Proc. IJCAI.Krause, A., Leskovec, J., Guestrin, C., VanBriesen, J., & Faloutsos, C. (2008). Efficient sensorplacement optimization securing large water distribution networks. Journal Water Resources Planning Management, 136(6).Krause, A., Singh, A., & Guestrin, C. (2007). Near-optimal sensor placements Gaussian processes: Theory, efficient algorithms empirical studies. JMLR.Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic modelssegmenting labeling sequence data. ICML.Lerner, U., & Parr, R. (2001). Inference hybrid networks: Theoretical limits practical algorithms. UAI.Lindley, D. V. (1956). measure information provided experiment. AnnalsMathematical Statistics, 27, 9861005.Littman, M., Goldsmith, J., & Mundhenk, M. (1998). computational complexity probabilisticplanning. Journal Artificial Intelligence Research, 9, 136.Molina, L., Belanche, L., & Nebot, A. (2002). Feature selection algorithms: survey experimental evaluation. ICDM.Mookerjee, V. S., & Mannino, M. V. (1997). Sequential decision models expert system optimization. IEEE Trans. Knowl. Data Eng., 9(5), 675687.590fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELSMunie, M., & Shoham, Y. (2008). Optimal testing structured knowledge. Twenty-Third Conference Artificial Intelligence (AAAI).Papadimitriou, C. H. (1995). Computational Complexity. Addison-Wesley.Park, J. D., & Darwiche, A. (2004). Complexity results approximation strategies mapexplanations. Journal Aritificial Intelligence Research, 21, 101133.Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations large pomdps.JAIR, 27, 335380.Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81106.Radovilsky, Y., Shattah, G., & Shimony, S. E. (2006). Efficient deterministic approximation algorithms non-myopic value information graphical models. IEEE InternationalConference Systems, Man Cybernetics (SMC), Vol. 3, pp. 25592564.Robbins, H. (1952). aspects sequential design experiments. Bulletin AmericanMathematical Society, 58, 527535.Scheffer, T., Decomain, C., & Wrobel, S. (2001). Active learning partially hidden Markov modelsinformation extraction. ECML/PKDD Workshop Instance Selection.Sim, R., & Roy, N. (2005). Global a-optimal robot exploration slam. IEEE InternationalConference Robotics Automation (ICRA).Singh, A., Krause, A., Guestrin, C., Kaiser, W. J., & Batalin, M. A. (2007). Efficient planninginformative paths multiple robots. International Joint Conference Artificial Intelligence (IJCAI), pp. 22042211, Hyderabad, India.Singhvi, V., Krause, A., Guestrin, C., Garrett, J., & Matthews, H. (2005). Intelligent light controlusing sensor networks. Proc. 3rd ACM Conference Embedded Networked SensorSystems (SenSys).Slijepcevic, S., & Potkonjak, M. (2001). Power efficient organization wireless sensor networks.ICC.Smallwood, R., & Sondik, E. (1973). optimal control partially observable Markov decisionprocesses finite horizon. Operations Research, 21, 10711088.Stachniss, C., Grisetti, G., & Burgard, W. (2005). Information gain-based exploration using raoblackwellized particle filters. Robotics Science Systems (RSS).Tong, S., & Koller, D. (2001). Active learning parameter estimation Bayesian networks.NIPS.Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation hybrid genetic decisiontree induction algorithm. Journal Artificial Intelligence Research, 2, 369409.van der Gaag, L., & Wessels, M. (1993). Selective evidence gathering diagnostic belief networks.AISB Quart., 86, 2334.Zhao, F., Shin, J., & Reich, J. (2002). Information-driven dynamic sensor collaboration trackingapplications. IEEE Signal Processing, 19(2), 6172.591fiJournal Artificial Intelligence Research 35 (2009) 1-47Submitted 01/09; published 05/09Complex Question Answering: Unsupervised LearningApproaches ExperimentsYllias Chalichali@cs.uleth.caUniversity LethbridgeLethbridge, AB, Canada, T1K 3M4Shafiq R. Jotyrjoty@cs.ubc.caUniversity British ColumbiaVancouver, BC, Canada, V6T 1Z4Sadid A. Hasanhasan@cs.uleth.caUniversity LethbridgeLethbridge, AB, Canada, T1K 3M4AbstractComplex questions require inferencing synthesizing information multipledocuments seen kind topic-oriented, informative multi-document summarization goal produce single text compressed version setdocuments minimum loss relevant information. paper, experimentone empirical method two unsupervised statistical machine learning techniques:K-means Expectation Maximization (EM), computing relative importancesentences. compare results approaches. experiments showempirical approach outperforms two techniques EM performs betterK-means. However, performance approaches depends entirely featureset used weighting features. order measure importancerelevance user query extract different kinds features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic shallow-semantic)document sentences. use local search technique learn weightsfeatures. best knowledge, study used tree kernel functionsencode syntactic/semantic information complex tasks computingrelatedness query sentences document sentences order generatequery-focused summaries (or answers complex questions). methodsgenerating summaries (i.e. empirical, K-means EM) show effects syntacticshallow-semantic features bag-of-words (BOW) features.1. Introductionvast increase amount online text available demand access different types information led renewed interest broad range InformationRetrieval (IR) related areas go beyond simple document retrieval. areasinclude question answering, topic detection tracking, summarization, multimedia retrieval, chemical biological informatics, text structuring, text mining, genomics, etc.Automated Question Answering (QA)the ability machine answer questions, simplecomplex, posed ordinary human languageis perhaps exciting technological development past six seven years (Strzalkowski & Harabagiu, 2008).c2009AI Access Foundation. rights reserved.fiChali, Joty, & Hasanexpectations already tremendous, reaching beyond discipline (a subfield NaturalLanguage Processing (NLP)) itself.tool finding documents web, search engines proven adequate.Although limitation expressiveness user terms query formulation, certain limitations exist search engine query. Complexquestion answering tasks require multi-document summarization aggregatedsearch, faceted search, represents information need cannot answeredsingle document. example, look comparison average numberyears marriage first birth women U.S., Asia, Europe, answerlikely contained multiple documents. Multi-document summarization usefultype query currently tool market designed meetkind information need.QA research attempts deal wide range question types including: fact, list,definition, how, why, hypothetical, semantically-constrained, cross-lingual questions.questions, call simple questions, easier answer. example,question: president Bangladesh? asks persons name. typequestion (i.e. factoid) requires small snippets text answer. Again, question:countries Pope John Paul II visited? sample list question, askinglist small snippets text.made substantial headway factoid list questions, researchersturned attention complex information needs cannot answeredsimply extracting named entities (persons, organizations, locations, dates, etc.) documents. Unlike informationally simple factoid questions, complex questions often seek multiple different types information simultaneously presuppose one singleanswer meet information needs. example, factoid question like:accurate HIV tests? safely assumed submitter question looking number range numbers. However, complex questions like:causes AIDS? wider focus question suggests submittermay single well-defined information need therefore may amenablereceiving additional supporting information relevant (as yet) undefined informational goal (Harabagiu, Lacatusu, & Hickl, 2006). questions require inferencingsynthesizing information multiple documents.well known QA systems Korean Navers Knowledge search1 ,pioneers community QA. tool allows users ask question getanswers users. Navers Knowledge roughly 10 times entriesWikipedia. used millions Korean web users given day. peoplesay Koreans addicted internet Naver. January 2008 Knowledge Search database included 80 million pages user-generated information.Another popular answer service Yahoo! Answers community-driven knowledge market website launched Yahoo!. allows users submit questionsanswered answer questions users. People vote best answer. sitegives members chance earn points way encourage participation basedNaver model. December 2006, Yahoo! Answers 60 million users 651. http://kin.naver.com/2fiComplex Question Answering: Unsupervised Approachesmillion answers. Google QA system2 based paid editors launchedApril 2002 fully closed December 2006.However, computational linguistics point view information synthesisseen kind topic-oriented informative multi-document summarization. goalproduce single text compressed version set documents minimum lossrelevant information. Unlike indicative summaries (which help determine whetherdocument relevant particular topic), informative summaries must attempt findanswers.paper, focus extractive approach summarization subsetsentences original documents chosen. contrasts abstractive summarization information text rephrased. Although summaries producedhumans typically extractive, state art summarization systemsbased extraction achieve better results automated abstraction. Here,experimented one empirical two well-known unsupervised statistical machinelearning techniques: K-means EM evaluated performance generating topicoriented summaries. However, performance approaches depends entirelyfeature set used weighting features. order measure importancerelevance user query extract different kinds features (i.e. lexical, lexicalsemantic, cosine similarity, basic element, tree kernel based syntactic shallow-semantic)document sentences. used gradient descent local search techniquelearn weights features.Traditionally, information extraction techniques based BOW approach augmented language modeling. task requires use complex semantics, approaches based BOW often inadequate perform fine-level textualanalysis. improvements BOW given use dependency trees syntactic parse trees (Hirao, , Suzuki, Isozaki, & Maeda, 2004; Punyakanok, Roth, & Yih, 2004;Zhang & Lee, 2003b), adequate dealing complex questionswhose answers expressed long articulated sentences even paragraphs. Shallowsemantic representations, bearing compact information, could prevent sparsenessdeep structural approaches weakness BOW models (Moschitti, Quarteroni,Basili, & Manandhar, 2007). pinpointing answer question relies deep understanding semantics both, attempting application syntactic semanticinformation complex QA seems natural. best knowledge, study usedtree kernel functions encode syntactic/semantic information complex taskscomputing relatedness query sentences document sentencesorder generate query-focused summaries (or answers complex questions).methods generating summaries (i.e. empirical, K-means EM) show effectssyntactic shallow-semantic features BOW features.past three years, complex questions focus much attentionautomatic question-answering Multi Document Summarization (MDS) communities. Typically, current complex QA evaluations including 2004 AQUAINTRelationship QA Pilot, 2005 Text Retrieval Conference (TREC) Relationship QA Task,TREC definition (and others) require systems return unstructured lists can2. http://answers.google.com/3fiChali, Joty, & Hasandidate answers response complex question. However recently, MDS evaluations (including 2005, 2006 2007 Document Understanding Conference (DUC)) taskedsystems returning paragraph-length answers complex questions responsive,relevant, coherent.experiments based DUC 2007 data show including syntactic semantic features improves performance. Comparison among approaches also shown.Comparing DUC 2007 participants, systems achieve top scoresstatistically significant difference results system results DUC2007 best system.paper organized follows: Section 2 focuses related work, Section 3gives brief description intended final model, Section 4 describes featuresextracted, Section 5 discusses learning issues presents learning approaches,Section 6 discusses remove redundant sentences adding finalsummary, Section 7 describes experimental study. conclude discuss futuredirections Section 8.2. Related WorkResearchers world working query-based summarization trying differentdirections see methods provide best results.number sentence retrieval systems based IR (Information Retrieval)techniques. systems typically dont use lot linguistic information, stilldeserve special attention. Murdock Croft (2005) propose translation model specificallymonolingual data, show significantly improves sentence retrieval querylikelihood. Translation models train parallel corpus used corpus question/answer pairs. Losada (2005) presents comparison multiple-Bernoulli modelsmultinomial models context sentence retrieval task shows multivariate Bernoulli model really outperform popular multinomial models retrievingrelevant sentences. Losada Fernandez (2007) propose novel sentence retrieval methodbased extracting highly frequent terms top retrieved documents. results reinforce idea top retrieved data valuable source enhance retrieval systems.specially true short queries usually query-sentence matching terms. argue method improves significantly precision top rankshandling poorly specified information needs.LexRank method addressed Erkan Radev (2004) successfulgeneric multi-document summarization. topic-sensitive LexRank proposed Otterbacher, Erkan, Radev (2005). LexRank, set sentences document clusterrepresented graph nodes sentences, links nodes induced similarity relation sentences. system ranks sentencesaccording random walk model defined terms inter-sentence similaritiessimilarities sentences topic description question.Concepts coherence cohesion enable us capture theme text. Coherence represents overall structure multi-sentence text terms macro-levelrelations clauses sentences (Halliday & Hasan, 1976). Cohesion, definedHalliday Hasan (1976), property holding text together one single grammat4fiComplex Question Answering: Unsupervised Approachesical unit based relations (i.e. ellipsis, conjunction, substitution, reference, lexicalcohesion) various elements text. Lexical cohesion defined cohesionarises semantic relations (collocation, repetition, synonym, hypernym, hyponym, holonym, meronym, etc.) words text (Morris & Hirst, 1991).Lexical cohesion among words represented lexical chains sequencessemantically related words. summarization methods based lexical chain first extract nouns, compound nouns named entities candidate words (Li, Sun, Kit, &Webster, 2007). using WordNet3 systems find semantic similaritynouns compound nouns. lexical chains built two steps:1. Building single document strong chains disambiguating senses words.2. Building multi-chain merging strongest chains single documentsone chain.systems rank sentences using formula involves a) lexical chain, b) keywords query c) named entities. example, Li et al. (2007) uses followingformula:Score = P (chain) + P (query) + P (namedEntity)P (chain) sum scores chains whose words comecandidate sentence, P (query) sum co-occurrences key words topicsentence, P (namedEntity) number name entities existing topicsentence. three coefficients , set empirically. top rankedsentences selected form summary.Harabagiu et al. (2006) introduce new paradigm processing complex questionsrelies combination (a) question decompositions; (b) factoid QA techniques;(c) Multi-Document Summarization (MDS) techniques. question decompositionprocedure operates Markov chain. is, following random walk mixturemodel bipartite graph relations established concepts related topiccomplex question subquestions derived topic-relevant passages manifestrelations. Decomposed questions submitted state-of-the-art QA systemorder retrieve set passages later merged comprehensive answer MDS system. show question decompositions using methodsignificantly enhance relevance comprehensiveness summary-length answerscomplex questions.approaches based probabilistic models (Pingali, K., & Varma,2007; Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, & Vanderwende, 2007). Pingaliet al. (2007) rank sentences based mixture model componentmodel statistical model:Score(s) = QIScore(s) + (1 ) QF ocus(s, Q)(1)3. WordNet (http://wordnet.princeton.edu/) widely used semantic lexicon English language.groups English words (i.e. nouns, verbs, adjectives adverbs) sets synonyms called synsets,provides short, general definitions (i.e. gloss definition), records various semantic relationssynonym sets.5fiChali, Joty, & HasanScore(s) score sentence s. Query-independent score (QIScore)query-dependent score (QFocus) calculated based probabilistic models. Toutanovaet al. (2007) learns log-linear sentence ranking model maximizing three metricssentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, (c) Model Frequency.scoring function learned fitting weights set feature functions sentencesdocument set trained optimize sentence pair-wise ranking criterion.scoring function adapted apply summaries rather sentences takeaccount redundancy among sentences.Pingali et al. (2007) reduce document-sentences dropping wordscontain important information. Toutanova et al. (2007), Vanderwende, Suzuki,Brockett (2006), Zajic, Lin, Dorr, Schwartz (2006) heuristically decomposedocument-sentences smaller units. apply small set heuristics parsetree create alternatives original sentence (possibly multiple)simplified versions available selection.approaches multi-document summarization try cluster sentencestogether. Guo Stylios (2003) use verb arguments (i.e. subjects, times, locationsactions) clustering. sentence method establishes indices informationbased verb arguments (subject first index, time second, location thirdaction fourth). sentences closest subjects index putcluster sorted according temporal sequence earliestlatest. Sentences spaces/locations index value clustermarked out. clusters ranked based sizes top 10 clusterschosen. Then, applying cluster reduction module system generates compressedextract summaries.approaches Recognizing Textual Entailment, Sentence Alignment,Question Answering use syntactic and/or semantic information order measuresimilarity two textual units. indeed motivated us include syntacticsemantic features get structural similarity document sentence querysentence (discussed Section 4.1). MacCartney, Grenager, de Marneffe, Cer, Manning(2006) use typed dependency graphs (same dependency trees) represent texthypothesis. try find good partial alignment typed dependencygraphs representing hypothesis (contains n nodes) text (graph containsnodes) search space O((m + 1)n). use incremental beam search combinednode ordering heuristic approximate global search space possiblealignments. locally decomposable scoring function chosen scorealignment sum local node edge alignment scores. scoring measuredesigned favor alignments align semantically similar subgraphs, irrespectivepolarity. reason, nodes receive high alignment scores words representsemantically similar. Synonyms antonyms receive highest score unrelatedwords receive lowest. Alignment scores also incorporate local edge scores basedshape paths nodes text graph correspond adjacentnodes hypothesis graph. final step make decision whetherhypothesis entailed text conditioned typed dependency graphswell best alignment them. make decision use supervised6fiComplex Question Answering: Unsupervised Approachesstatistical logistic regression classifier (with feature space 28 features) Gaussianprior parameter regularization.Hirao et al. (2004) represent sentences using Dependency Tree Path (DTP) incorporate syntactic information. apply String Subsequence Kernel (SSK) measuresimilarity DTPs two sentences. also introduce Extended StringSubsequence Kernel (ESK) incorporate semantics DTPs. Kouylekov Magnini(2005) use tree edit distance algorithms dependency trees texthypothesis recognize textual entailment. According approach, text entailshypothesis H exists sequence transformations (i.e. deletion, insertionsubstitution) applied obtain H overall cost certainthreshold. Punyakanok et al. (2004) represent question sentence containinganswer dependency trees. add semantic information (i.e. named entity,synonyms related words) dependency trees. apply approximatetree matching order decide similar given pair trees are. also useedit distance matching criteria approximate tree matching. methodsshow improvement BOW scoring methods.3. Approachaccomplish task answering complex questions extract various important features sentences document collection measure relevancequery. sentences document collection analyzed various levelsdocument sentences represented vector feature-values. feature setincludes lexical, lexical semantic, statistical similarity, syntactic semantic features,graph-based similarity measures (Chali & Joty, 2008b). reimplemented manyfeatures successfully applied many related fields NLP.use simple local search technique fine-tune feature weights. also usestatistical clustering algorithms: EM K-means select relevant sentencessummary generation. Experimental results show systems perform betterinclude tree kernel based syntactic semantic features though summaries basedsyntactic semantic feature achieve good results. Graph-based cosinesimilarity lexical semantic features also important selecting relevant sentences.find local search technique outperforms two EM performsbetter K-means based learning. later sections describe subpartssystems details.4. Feature Extractionsection, describe features used score sentences.provide detailed examples4 show get feature values. first describesyntactic semantic features introducing work. followdetailed description features commonly used question answeringsummarization communities.4. query document sentences used examples taken DUC 2007 collection.7fiChali, Joty, & Hasan4.1 Syntactic Shallow Semantic Featurestask like query-based summarization requires use complex syntacticsemantics, approaches BOW often inadequate perform fine-leveltextual analysis. importance syntactic semantic features contextdescribed Zhang Lee (2003a), Moschitti et al. (2007), Bloehdorn Moschitti(2007a), Moschitti Basili (2006) Bloehdorn Moschitti (2007b).effective way integrate syntactic semantic structures machine learning algorithms use tree kernel functions (Collins & Duffy, 2001; Moschitti & Quarteroni,2008) successfully applied question classification (Zhang & Lee, 2003a;Moschitti & Basili, 2006). Syntactic semantic information used effectively measure similarity two textual units MacCartney et al. (2006). bestknowledge, study used tree kernel functions encode syntactic/semanticinformation complex tasks computing relatedness querysentences document sentences. Another good way encode shallow syntacticinformation use Basic Elements (BE) (Hovy, Lin, Zhou, & Fukumoto, 2006)uses dependency relations. experiments show including syntactic semanticfeatures improves performance sentence selection complex question answeringtask (Chali & Joty, 2008a).4.1.1 Encoding Syntactic StructuresBasic Element (BE) Overlap Measure Shallow syntactic information based dependency relations proved effective finding similarity two textualunits (Hirao et al., 2004). incorporate information using Basic Elementsdefined follows (Hovy et al., 2006):head major syntactic constituent (noun, verb, adjective adverbial phrases),expressed single item.relation head-BE single dependent, expressed triple:(head|modifier|relation).triples encode syntactic information one decide whether two unitsmatch not- easily longer units (Hovy et al., 2006). extracted BEssentences (or query) using package distributed ISI5 .get BEs sentence, computed Likelihood Ratio (LR)following Zhou, Lin, Hovy (2005). Sorting BEs according LR scores producedBE-ranked list. goal generate summary answer users questions.ranked list BEs way contains important BEs top may mayrelevant users questions. filter BEs checking whether containword query word QueryRelatedWords (defined Section 4.3).example, consider following sentence get score 0.77314.Query: Describe steps taken worldwide reaction prior introduction EuroJanuary 1, 1999. Include predictions expectations reported press.5. website:http://www.isi.edu/ cyl/BE8fiComplex Question Answering: Unsupervised ApproachesSentence: Frankfurt-based body said annual report released todaydecided two themes new currency: history European civilizationabstract concrete paintings.Score: 0.77314Here, decided|themes|obj considered contain wordquery words query relevant words report|annual|mod takencontains query word report. way, filter BEs relatedquery. score sentence sum scores divided number BEssentence. limiting number top BEs contribute calculationsentence scores remove BEs little importance sentencesfewer important BEs. set threshold 100 topmost 100 BEs rankedlist contribute normalized sentence score computation. paper,set threshold took BEs counted calculating scoressentences.Tree Kernels Approach order calculate syntactic similarity querysentence first parse sentence well query syntactic tree(Moschitti, 2006) using parser like Charniak (1999). calculate similaritytwo trees using tree kernel. reimplemented tree kernel modelproposed Moschitti et al. (2007).build trees, next task measure similarity trees.this, every tree represented dimensional vector v(T ) = (v1 (T ), v2 (T ), vm (T )),i-th element vi (T ) number occurrences i-th tree fragment tree. tree fragments tree sub-trees include least one productionrestriction production rules broken incomplete parts (Moschittiet al., 2007). Figure 1 shows example tree portion subtrees.Figure 1: (a) example tree (b) sub-trees NP covering press.Implicitly enumerate possible tree fragments 1, 2, , m. fragmentsaxis m-dimensional space. Note could done implicitly sincenumber extremely large. this, Collins Duffy (2001) define treekernel algorithm whose computational complexity depend m.tree kernel two trees T1 T2 actually inner product v(T1 ) v(T2 ):9fiChali, Joty, & HasanK(T1 , T2 ) = v(T1 ).v(T2 )(2)define indicator function Ii (n) 1 sub-tree seen rooted node n0 otherwise. follows:vi (T1 ) =XXIi (n1 ), vi (T2 ) =n1 N1Ii (n2 )(3)n2 N2N1 N2 set nodes T1 T2 respectively. So, derive:K(T1 , T2 ) = v(T1 ).v(T2 ) =Xvi (T1 )vi (T2 )X=X Xn1 N1 n2 N2X=XIi (n1 )Ii (n2 )C(n1 , n2 )(4)n1 N1 n2 N2define C(n1 , n2 ) = Ii (n1 )Ii (n2 ). Next, note C(n1 , n2 )computed polynomial time due following recursive definition:P1. productions n1 n2 different C(n1 , n2 ) = 02. productions n1 n2 same, n1 n2 pre-terminals,C(n1 , n2 ) = 13. Else productions n1 n2 pre-terminals,nc(n1 )C(n1 , n2 ) =(1 + C(ch(n1 , j), ch(n2 , j)))(5)j=1nc(n1 ) number children n1 tree; productions n1n2 nc(n1 ) = nc(n2 ). i-th child-node n1 ch(n1 , i).cases query composed two sentences compute similaritydocument sentence (s) query-sentences (qi ) takeaverage scores syntactic feature value.Syntactic similarity value =Pni=1 K(qi , s)nn number sentences query q sentence consideration. TK similarity value (tree kernel) sentence querysentence q based syntactic structure. example, following sentencequery q get score:10fiComplex Question Answering: Unsupervised ApproachesFigure 2: Example semantic treesQuery (q): Describe steps taken worldwide reaction prior introduction EuroJanuary 1, 1999. Include predictions expectations reported press.Sentence (s): Europes new currency, euro, rival U.S. dollar internationalcurrency long term, Der Spiegel magazine reported Sunday.Scores: 90, 41Average Score: 65.54.1.2 Semantic FeaturesThough introducing syntactic information gives improvement BOW, usesyntactic parses, adequate dealing complex questions whose answers expressed long articulated sentences even paragraphs. Shallow semanticrepresentations, bearing compact information, could prevent sparseness deepstructural approaches weakness BOW models (MacCartney et al., 2006; Moschittiet al., 2007).Initiatives PropBank (PB) (Kingsbury & Palmer, 2002) made designaccurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu, Pradhan, Ward, Martin, & Jurafsky, 2003) possible. Hence, attempting application SRLQA seems natural pinpointing answer question relies deep understandingsemantics both. example, consider PB annotation:[ARG0 all][TARGET use][ARG1 french franc][ARG2 currency]annotation used design shallow semantic representationmatched semantically similar sentences, e.g.[ARG0 Vatican][TARGET use][ARG1 Italian lira][ARG2 currency]order calculate semantic similarity sentences first representannotated sentence (or query) using tree structures like Figure 2 called Semantic Tree(ST) proposed Moschitti et al. (2007). semantic tree arguments replacedimportant wordoften referred semantic head. look nounfirst, verb, adjective, adverb find semantic head argument.none present take first word argument semantic head.11fiChali, Joty, & HasanFigure 3: Two STs composing STNHowever, sentences rarely contain single predicate, rather typically propositions contain one subordinate clauses. instance, let us consider slight modificationsecond sentence: Vatican, located wholly within Italy uses Italian liracurrency. Here, main predicate uses subordinate predicate located.SRL system outputs following two annotations:(1) [ARG0 Vatican located wholly within Italy][TARGET uses][ARG1 Italianlira][ARG2 currency](2) [ARG0 Vatican][TARGET located] [ARGM-LOC wholly][ARGM-LOC withinItaly] uses Italian lira currencygiving STs Figure 3. see Figure 3(A), argument nodecorresponds entire subordinate clause label leaf ST (e.g. leafARG0). ST node actually root subordinate clause Figure 3(B).taken separately, STs express whole meaning sentence. Hence,accurate define single structure encoding dependency twopredicates Figure 3(C). refer kind nested STs STNs.Note tree kernel (TK) function defined Section 4.1.1 computes numbercommon subtrees two trees. subtrees subject constraintnodes taken none children original tree. Thoughdefinition subtrees makes TK function appropriate syntactic trees, wellsuited semantic trees (ST). instance, although two STs Figure 2 sharesubtrees rooted ST node, kernel defined computes match.critical aspect steps (1), (2), (3) TK function productionstwo evaluated nodes identical allow match descendants.means common substructures cannot composed nodechildren effective ST representation would require. Moschitti et al. (2007) solveproblem designing Shallow Semantic Tree Kernel (SSTK) allows portionsST match.Shallow Semantic Tree Kernel (SSTK) reimplemented SSTK accordingmodel given Moschitti et al. (2007). SSTK based two ideas: first, changes12fiComplex Question Answering: Unsupervised ApproachesST, shown Figure 4 adding SLOT nodes. accommodate argument labelsspecific order fixed number slots, possibly filled null argumentsencode possible predicate arguments. Leaf nodes filled wildcard character *may alternatively accommodate additional information. slot nodes usedway adopted TK function generate fragments containing onechildren like example shown frames (b) (c) Figure 4. previouslypointed out, arguments directly attached root node kernel functionwould generate structure children (or structure children, i.e.empty) (Moschitti et al., 2007).Figure 4: Semantic tree fragmentsSecond, original tree kernel would generate many matches slots fillednull label set new step 0 TK calculation:(0) n1 (or n2 ) pre-terminal node child label null, C(n1 , n2 ) = 0;subtract one unit C(n1 , n2 ), step 3:nc(n1 )(3) C(n1 , n2 ) =j=1(1 + C(ch(n1 , j), ch(n2 , j))) 1(6)changes generate new C which, substituted (in place original C )Eq. 4, gives new SSTK.example, following sentence query q get semantic score:Query (q): Describe steps taken worldwide reaction prior introduction EuroJanuary 1, 1999. Include predictions expectations reported press.Sentence (s): Frankfurt-based body said annual report released todaydecided two themes new currency history European civilizationabstract concrete paintings.Scores: 6, 12Average Score: 913fiChali, Joty, & Hasan4.2 Lexical FeaturesHere, discuss lexical features commonly used QAsummarization communities. reimplemented research.4.2.1 N-gram OverlapN-gram overlap measures overlapping word sequences candidate documentsentence query sentence. view measure overlap scores, query poolsentence pool created. order create query (or sentence) pool, tookquery (or document) sentence created set related sentences replacingcontent words6 first-sense synonyms using WordNet. example, given stemmeddocument-sentence: John write poem, sentence pool contains: John composepoem, John write verse form along given sentence.measured recall based n-gram scores sentence P using following formula:N gramScore(P ) = maxi (maxj N gram(si , qj ))Pgram Countmatch (gramn )N gram(S, Q) = P ngramn Count (gramn )(7)(8)n stands length n-gram (n = 1, 2, 3, 4), Countmatch (gramn )number n-grams co-occurring query candidate sentence, qj j-thsentence query pool, si i-th sentence sentence pool sentence P .1-gram Overlap Measure1-gram overlap score measures number words common sentence handquery related words. computed follows:1gram Overlap Score =PCountmatch (w1 )w1 Count (w1 )w1P(9)set content words candidate sentence Countmatchnumber matches sentence content words query related words. Count (gramn )number w1 .Note order measure 1-gram score took query related words insteadexact query words. motivation behind sentence word(s)exactly query words synonyms, hypernyms, hyponym glosswords, get counted.Example:Query Describe steps taken worldwide reaction prior introduction EuroJanuary 1, 1999. Include predictions expectations reported press.Sentence Frankfurt-based body said annual study released todaydecided two themes new currency: history European civilizationabstract concrete paintings.6. hence forth content words nouns, verbs, adverbs adjectives.14fiComplex Question Answering: Unsupervised Approaches1-gram Score 0.06666 (After normalization7 ).Note sentence 1-gram overlap score 0.06666 even thoughexact word common query words. got score sentence wordstudy synonym query word report.N-gram Overlap Measuresabove, calculate n-gram overlap scores. example, consideringfollowing query sentence document sentence (From DUC 2007 collection), 4matching 2-grams: (1 1999,of Euro, January January 1). Hence, employingformula given above, get following 2-gram score normalization. 3-gram scorealso found accordingly.Query Sentence: Describe steps taken worldwide reaction prior introductionEuro January 1, 1999. Include predictions expectations reportedpress.Document Sentence: Despite skepticism actual realization single European currency scheduled January 1, 1999, preparations designEuro note already begun.2-gram: 0.148153-gram: 0.08004.2.2 LCS WLCSsequence W = [w1 , w2 , ..., wn ] subsequence another sequence X = [x1 , x2 , ..., xm ] ,exists strict increasing sequence [i1 , i2 , ..., ] indices Xj = 1, 2, ..., n xij = wj (Cormen, Leiserson, & Rivest, 1989). Given two sequencesS1 S2 , longest common subsequence (LCS) S1 S2 common subsequencemaximum length (Lin, 2004).longer LCS two sentences is, similar two sentences are.used LCS-based F-measure estimate similarity document sentencelength query sentence Q length n follows:LCS(S, Q)LCS(S, Q)Plcs (S, Q) =nFlcs (S, Q) = (1 ) Plcs (S, Q) + Rlcs (S, Q)Rlcs (S, Q) =(10)(11)(12)LCS(S, Q) length longest common subsequence Q,constant determines importance precision recall. computingLCS measure document sentence query sentence viewed sequence words.7. normalize feature values corresponding sentence respect entire contextparticular document.15fiChali, Joty, & Hasanintuition longer LCS two similar are.recall (Rlcs (S, Q)) ratio length longest common subsequenceQ document sentence length measures completeness. Whereas precision(Plcs (S, Q)) ratio length longest common subsequence Qquery sentence length measure exactness. obtain equal importanceprecision recall set value 0.5. Equation 12 called LCS-basedF-measure. Notice Flcs 1 when, S=Q; Flcs 0 nothingcommon Q.One advantage using LCS require consecutive matches insequence matches reflect sentence level word order n-grams. advantageautomatically includes longest in-sequence common n-grams. Therefore, predefined n-gram length necessary. Moreover, property value lessequal minimum unigram (i.e. 1-gram) F-measure Q. Unigram recallreflects proportion words also present Q; unigram precisionproportion words Q also S. Unigram recall precision countco-occurring words regardless orders; LCS counts in-sequence co-occurrences.awarding credit in-sequence unigram matches, LCS measure also capturessentence level structure natural way. Consider following example:S1 John shot thiefS2 John shot thiefS3 thief shot JohnUsing S1 reference sentence, S2 S3 sentences consideration S2S3 would 2-gram score since one bigram (i.e. thief)common S1. However, S2 S3 different meanings. case LCS S2score 3/4=0.75 S3 score 2/4=0.5 = 0.5. Therefore, S2 betterS3 according LCS.However, LCS suffers one disadvantage counts main in-sequencewords; therefore, alternative LCSes shorter sequences reflectedfinal score. example, given following candidate sentence:S4 thief John shotUsing S1 reference, LCS counts either thief John shot both;therefore, S4 LCS score S3 2-gram would prefer S4 S3.order measure LCS score sentence took similar approach previous section using WordNet (i.e. creation sentence pool query pool). calculatedLCS score using following formula:LCS score = maxi (maxj Flcs (si , qj ))(13)qj j-th sentence query pool, si i-th sentencesentence pool.16fiComplex Question Answering: Unsupervised Approachesbasic LCS problem differentiate LCSes different spatialrelations within embedding sequences (Lin, 2004). example, given referencesequence two candidate sequences Y1 Y2 follows:S: B C E F GY1 : B C H KY2 : H B K CY1 Y2 LCS score. However, Y1 better choice Y2Y1 consecutive matches. improve basic LCS method store lengthconsecutive matches encountered far regular two dimensional dynamic program tablecomputing LCS. call weighted LCS (WLCS) use k indicate lengthcurrent consecutive matches ending words xi yj . Given two sentences X Y,WLCS score X computed using similar dynamic programmingprocedure stated Lin (2004). use WLCS advantage measuringsimilarity taking words higher dimension like string kernels indeedreduces time complexity. before, computed WLCS-based F-measureway using query pool sentence pool.W LCS score = maxi (maxj Fwlcs (si , qj ))(14)Example:Query Sentence: Describe steps taken worldwide reaction prior introductionEuro January 1, 1999. Include predictions expectations reportedpress.Document Sentence: Despite skepticism actual realization single European currency scheduled January 1, 1999, preparations designEuro note already begun.find 6 matching strings: (of 1 Euro 1999 January) longest commonsubsequence considering sentence related sentences. WLCS setweight 1.2. normalization, get following LCS WLCS scoressentence applying formula.LCS Score: 0.27586WLCS Score: 0.159614.2.3 Skip-Bigram Measureskip-bigram pair words sentence order allowing arbitrary gaps. Skipbigram measures overlap skip-bigrams candidate sentence querysentence (Lin, 2004). rely query pool sentence pool usingWordNet. Considering following sentences:17fiChali, Joty, & HasanS1 John shot thiefS2 John shoot thiefS3 thief shoot JohnS4 thief John shotget sentence C(4,2)=6 skip-bigrams8 . example, S1 followingskip-bigrams: (John shot, John the, John thief, shot the, shot thiefthief) S2 three skip bi-gram matches S1 (John the, John thief, thief),S3 one skip bi-gram match S1 (the thief), S4 two skip bi-gram matchesS1 (John shot, thief).skip bi-gram score document sentence length querysentence Q length n computed follows:SKIP2 (S, Q)C(m, 2)SKIP2 (S, Q)Pskip2 (S, Q) =C(n, 2)Fskip2 (S, Q) = (1 ) Pskip2 (S, Q) + Rskip2 (S, Q)Rskip2 (S, Q) =(15)(16)(17)SKIP2 (S, Q) number skip bi-gram matches Q,constant determines importance precision recall. set value0.5 associate equal importance precision recall. C combinationfunction. call equation 17 skip bigram-based F-measure. computed skipbigram-based F-measure using formula:SKIP BIGRAM = maxi (maxj Fskip2 (si , qj ))(18)example, given following query sentence, get 8 skip-bigrams: (on 1,January 1, January 1999, Euro, 1 1999, 1999, January on).Applying equations above, get skip bi-gram score 0.05218 normalization.Query Describe steps taken worldwide reaction prior introduction EuroJanuary 1, 1999. Include predictions expectations reported press.Sentence Despite skepticism actual realization single European currencyscheduled January 1, 1999, preparations design Euro notealready begun.Skip bi-gram Score: 0.052188. C(n, r) =n!r!(nr)!18fiComplex Question Answering: Unsupervised ApproachesNote skip bi-gram counts in-order matching word pairs LCS countsone longest common subsequence. put constraint maximum skip distance,dskip , two in-order words form skip bi-gram avoids spurious matcheslike from. example, set dskip 0 equivalent bi-gramoverlap measure (Lin, 2004). set dskip 4 word pairs 4 wordsapart form skip bi-grams. experiment set dskip = 4 order ponder4 words apart get skip bi-grams.Modifying equations: 15, 16, 17 allow maximum skip distance limitstraightforward: following Lin (2004) count skip bi-gram matches, SKIP2 (S, Q),within maximum skip distance replace denominators equationsactual numbers within distance skip bi-grams reference sentencecandidate sentence respectively.4.2.4 Head Head Related-words Overlapnumber head words common two sentences indicate muchrelevant other. order extract heads sentence (or query),sentence (or query) parsed Minipar9 dependency tree extractheads call exact head words. example, head word sentence: Johneats rice eat.take synonyms, hyponyms, hypernyms10 query-head wordssentence-head words form set words call head-related words.measured exact head score head-related score follows:Pw1 HeadSet Countmatch (w1 )(19)w1 HeadRelSet Countmatch (w1 )(20)ExactHeadScore =HeadRelatedScore =PPPw1 HeadSet Count (w1 )w1 HeadRelSet Count (w1 )HeadSet set head words sentence Countmatch numbermatches HeadSet query sentence. HeadRelSet setsynonyms, hyponyms, hypernyms head words sentence Countmatchnumber matches head-related words query sentence.example, list head words query sentence measures:Query: Describe steps taken worldwide reaction prior introduction EuroJanuary 1, 1999. Include predictions expectations reported press.Heads Query: include, reaction, step, take, describe, report, Euro, introduction, press,prediction, 1999, expectationSentence: Frankfurt-based body said annual report released todaydecided two themes new currency: history European civilizationabstract concrete paintings.9. http://www.cs.ualberta.ca/ lindek/minipar.htm10. hypernym hyponym levels restricted 2 3 respectively.19fiChali, Joty, & HasanHeads Sentence: history, release, currency, body, report,painting, say, civilization,theme, decide.Exact Head Score:111= 0.09Head Related Score: 04.3 Lexical Semantic Featuresform set words call QueryRelatedWords taking content wordsquery, first-sense synonyms, nouns hypernyms/hyponyms, nounsgloss definitions using WordNet.4.3.1 Synonym Overlapsynonym overlap measure overlap list synonyms contentwords extracted candidate sentence query related words. computedfollows:Synonym Overlap Score =Pw1 SynSet Countmatch (w1 )w1 SynSet Count (w1 )P(21)SynSet synonym set content words sentence Countmatchnumber matches SynSet query related words.4.3.2 Hypernym/Hyponym Overlaphypernym/hyponym overlap measure overlap list hypernyms (level2) hyponyms (level 3) nouns extracted sentence considerationquery related words. computed follows:Hypernym/hyponym overlap score =Ph1 HypSet Countmatch (h1 )h1 HypSet Count (h1 )P(22)HypSet hyponym/hyponym set nouns sentence Countmatchnumber matches HypSet query related words.4.3.3 Gloss Overlapgloss overlap measure overlap list content words extractedgloss definition nouns sentence consideration query relatedwords. computed follows:Gloss Overlap Score =Pg1 GlossSet Countmatch (g1 )Pg1 GlossSet Count (g1 )(23)GlossSet set content words (i.e. nouns, verbs adjectives) takengloss definition nouns sentence Countmatch number matchesGlossSet query related words.20fiComplex Question Answering: Unsupervised ApproachesExample:example, given query following sentence gets synonym overlap score0.33333, hypernym/hyponym overlap score 0.1860465 gloss overlap score 0.1359223.Query Describe steps taken worldwide reaction prior introduction EuroJanuary 1, 1999. Include predictions expectations reported press.Sentence Frankfurt-based body said annual report released todaydecided two themes new currency: history European civilizationabstract concrete paintings.Synonym Overlap Score: 0.33333Hypernym/Hyponym Overlap Score: 0.1860465Gloss Overlap Score: 0.13592234.4 Statistical Similarity MeasuresStatistical similarity measures based co-occurrence similar words corpus.Two words termed similar belong context. used thesaurusprovided Dr. Dekang Lin11 purpose. used two statistical similaritymeasures:Dependency-based similarity measuremethod uses dependency relations among words order measure similarity (Lin, 1998b). extracts dependency triples uses statistical approachmeasure similarity. Using given corpus one retrieve similar wordsgiven word. similar words grouped clusters.Note word one cluster. cluster representssense word similar words sense. So, selecting right clusterword problem. goals are: i) create bag similar words querywords ii) get bag similar words (dependency based) query wordsmeasure overlap score sentence words.Creating Bag Similar Words:query-word extract clusters thesaurus. orderdetermine right cluster query word measure overlap scorequery related words (i.e. exact words, synonyms, hypernyms/hyponyms gloss)clusters. hypothesis cluster words common queryrelated words right cluster assumption first synonym correctsense. choose cluster word highest overlap score.Overlap scorei =Pw1 QueryRelatedW ords Countmatch (w1 )(24)Cluster = argmaxi (Overlap Scorei )(25)w1 QueryRelatedW ords Count (w1 )P11. http://www.cs.ualberta.ca/ lindek/downloads.htm21fiChali, Joty, & HasanQueryRelatedWords set exact words, synonyms, hyponyms/hypernyms,gloss words words query (i.e query words) Countmatch numbermatches query related words ith cluster similar words.Measuring Overlap Score:get clusters query words measured overlapcluster words sentence words call dependency based similarity measure:DependencyM easure =w1 SenW ords Countmatch (w1 )PPw1 SenW ords Count (w1 )(26)SenWords set words sentence Countmatch numbermatches sentence words cluster similar words.Proximity-based similarity measuresimilarity computed based linear proximity relationship words(Lin, 1998a). uses information theoretic definition similarity measuresimilarity. similar words grouped clusters. took similar approachmeasure feature previous section except used different thesaurus.Example:Considering following query sentence get following measures:Query: Describe steps taken worldwide reaction prior introduction EuroJanuary 1, 1999. Include predictions expectations reported press.Sentence: Frankfurt-based body said annual report released todaydecided two themes new currency: history European civilizationabstract concrete paintings.Dependency-based Similarity Score: 0.0143678Proximity-based Similarity Score: 0.040540544.5 Graph-based Similarity MeasureErkan Radev (2004) used concept graph-based centrality rank set sentencesproducing generic multi-document summaries. similarity graph producedsentences document collection. graph node represents sentence.edges nodes measure cosine similarity respective pair sentences.degree given node indication important sentence is. Figure 5shows example similarity graph 4 sentences.similarity graph constructed, sentences ranked accordingeigenvector centrality. LexRank performed well context generic summarization. apply LexRank query-focused context topic-sensitive version LexRankproposed Otterbacher et al. (2005). followed similar approach order calculatefeature. score sentence determined mixture model relevancesentence query similarity sentence high-scoring sentences.22fiComplex Question Answering: Unsupervised ApproachesFigure 5: LexRank similarityRelevance Questionfirst stem sentences collection compute word IDFs (InverseDocument Frequency) using following formula:N +1idfw = log0.5 + sfw(27)N total number sentences cluster, sfw numbersentences word w appears in.also stem questions remove stop words. relevance sentencequestion q computed by:rel(s|q) =Xwqlog (tfw,s + 1) log (tfw,q + 1) idfw(28)tfw,s tfw,q number times w appears q, respectively.Mixture Modelprevious section measured relevance sentence questionsentence similar high scoring sentences cluster also highscore. instance, sentence gets high score based question relevancemodel likely contain answer question related sentence, maysimilar question itself, also likely contain answer (Otterbacher et al.,2005).capture idea following mixture model:p(s|q) =(Xsim(s, v)rel(s|q)P+ (1 d)PzC rel(z|q)zC sim(z, v)vC)p(v|q)(29)p(s|q), score sentence given question q, determined sumrelevance question similarity sentences collection.C set sentences collection. value parameter call23fiChali, Joty, & Hasanbias trade-off two terms equation set empirically. highervalues prefer relevance question similarity sentences.denominators terms normalization. Although computationallyexpensive, equation 29 calculates sum entire collection since requiredmodel sense global impact voting sentences. measurecosine similarity weighted word IDFs similarity two sentences cluster:sim(x, y) = qPPwx,ytfw,x tfw,y (idfw )22xi x (tfxi ,x idfxi )qP2yi (tfyi ,y idfyi )(30)Equation 29 written matrix notation follows:p = [dA + (1 d)B]T p(31)square matrix given index i, elements i-th columnproportional rel(i|q). B also square matrix entry B(i,j)proportional sim(i,j). matrices normalized row sums add 1.Note result normalization rows resulting square matrix Q =[dA + (1 d)B] also add 1. matrix called stochastic defines Markovchain. view sentence state Markov chain Q(i,j) specifiestransition probability state state j corresponding Markov chain.vector p looking Eq. 31 stationary distribution Markov chain.intuitive interpretation stationary distribution understood conceptrandom walk graph representation Markov chain. probabilitytransition made current node nodes similar query.probability (1-d) transition made nodes lexically similar currentnode. Every transition weighted according similarity distributions. elementvector p gives asymptotic probability ending corresponding statelong run regardless starting state. stationary distribution Markov chaincomputed simple iterative algorithm called power method (Erkan & Radev,2004). starts uniform distribution. iteration eigenvector updatedmultiplying transpose stochastic matrix. Since Markov chainirreducible aperiodic algorithm guaranteed terminate.5. Ranking Sentencesuse several methods order rank sentences generate summaries applyingfeatures described Section 4. section describe systems detail.5.1 Learning Feature-weights: Local Search Strategyorder fine-tune weights features, used local search technique. Initially set feature-weights, w1 , , wn , equal values (i.e. 0.5) (see Algorithm 1).train weights using DUC 2006 data set. Based current weightsscore sentences generate summaries accordingly. evaluate summaries using24fiComplex Question Answering: Unsupervised ApproachesInput: Stepsize l, Weight Initial Value vOutput: vector w~ learned weightsInitialize weight values wi v.1 nrg1 = rg2 = prev = 0(true)scoreSentences(w)~generateSummaries()rg2 = evaluateROUGE()rg1 rg2prev = wiwi + = lrg1 = rg2elsebreakendendendreturn w~Algorithm 1: Tuning weights using Local Search techniqueautomatic evaluation tool ROUGE (Lin, 2004) (described Section 7) ROUGEvalue works feedback learning loop. learning system tries maximizeROUGE score every step changing weights individually specific step size (i.e.0.01). means, learn weight wi change value wi keeping weightvalues (wj j6=i ) stagnant. weight wi algorithm achieves local maximum(i.e. hill climbing) ROUGE value.learned feature-weights compute final scores sentencesusing formula:scorei = x~i .w~(32)x~i feature vector i-th sentence, w~ weight vector, scoreiscore i-th sentence.5.2 Statistical Machine Learning Approachesexperimented two unsupervised statistical learning techniques featuresextracted previous section sentence selection problem:1. K-means learning2. Expectation Maximization (EM) learning5.2.1 K-means LearningK-means hard clustering algorithm defines clusters center massmembers. start set initial cluster centers chosen randomly go25fiChali, Joty, & Hasanseveral iterations assigning object cluster whose center closest.objects assigned recompute center cluster centroid) members. distance function use squared Euclidean distancemean (instead true Euclidean distance.Since square root monotonically growing function squared Euclidean distanceresult true Euclidean distance computation overload smallersquare root dropped.learned means clusters using K-means algorithm nexttask rank sentences according probability model. used Bayesianmodel order so. Bayes law says:x|qk , )P (qk |)p(xx|)p(xx|qk , )P (qk |)p(xPKx|qk , )p(qk |)k=1 p(xx, ) =P (qk |x=(33)qk cluster, x feature vector representing sentence, parameterset class models. set weights clusters equiprobable (i.e. P (qk |) =x|qk , ) using gaussian probability distribution. gaussian1/K). calculated p(xprobability density function (pdf) d-dimensional random variable x given by:x) =p(,) (xe1x)T 1 (xx)(x2dp)2 det((34), mean vector, , covariance matrix, parameters) K-means algorithm calculategaussian distribution. get means (covariance matrix using unbiased covariance estimation procedure:j =N1 Xxi j )(xx j )T(xN 1 i=1(35)5.2.2 EM LearningEM algorithm gaussian mixture models well known method cluster analysis.useful outcome model produces likelihood value clustering modellikelihood values used select best model number differentmodels providing number parameters (i.e. numberclusters).26fiComplex Question Answering: Unsupervised Approachesx) represented feature vector lengthInput: sample n data-points (xLInput: Number Clusters KOutput: array K-means-based ScoresData: Array dnK , K , KData: Array C K , nKRandomly choose K data-points K initial means: k , k = 1, , K.repeat1 nj 1 Kxi j k2 = (xxi j )T (xxi j )ij = kxendik < il , l 6= kassign x C k .endendP1 KCx Cxjj=C i||Cendchange occurs ;/* calculating covariances cluster1 KC i|= |Cj 1C ij ) (CC ij )T+ = (Cend= (1/(m 1))end/* calculating scores sentences1 nj 1 K112 (x j ) j (x j )yij = e2*/*/j )det(endj 1 KP// where, wj = 1/Kzij = (yij wj )/ Kj=1 yij wj ;endk ) k= max(Push zimendreturnAlgorithm 2: Computing K-means based similarity measure27fiChali, Joty, & Hasansignificant problem EM algorithm converges local maximumlikelihood function hence quality result depends initialization.problem along method improving initialization discussed latersection.EM soft version K-means algorithm described above. K-meansstart set random cluster centers c1 ck . iteration soft assignmentdata-points every cluster calculating membership probabilities. EMiterative two step procedure: 1. Expectation-step 2. Maximization-step.expectation step compute expected values hidden variables hi,j clustermembership probabilities. Given current parameters compute likelyobject belongs clusters. maximization step computes likelyparameters model given cluster membership probabilities.data-points considered generated mixture model k-gaussiansform:P (x) =kXP (C = i)P (x|C = i) =i=1kX, )P (C = i)P (x|(36)i=1total likelihood model k components, given observed data pointsX = x 1 , , x n , is:L(|X)=n Xki=1 j=1x |j ) =P (C = j)P (xnXkXi=1logj=1n Xki=1 j=1xi |j , j )wj P (xxi |j , j ) ( taking log likelihood )wj P (x(37)(38)P probability density function (i.e. eq 34). j j meancovariance matrix component j, respectively. component contributes proportion,Pwj , total population that: Kj=1 wj = 1.Log likelihood used instead likelihood turns product sum.describe EM algorithm estimating gaussian mixture.Singularities covariance matrix must non-singular invertible.EM algorithm may converge position covariance matrix becomes singular| = 0) close singular, means invertible anymore. covariance(|matrix becomes singular close singular EM may result wrong clusters.restrict covariance matrices become singular testing cases iterationalgorithm follows:q| > 1e9 ) update( |else update28fiComplex Question Answering: Unsupervised ApproachesDiscussion: Starting values EM algorithmconvergence rate success clustering using EM algorithm degradedpoor choice starting values means, covariances, weights components. experimented one summary (for document number D0703A DUC2007) order test impact initial values EM algorithm. clustermeans initializedp heuristic spreads randomly around ean(DAT A)standard deviation Cov(DAT A) 10. initial covariance set Cov(DAT A)initial values weights wj = 1/K K number clusters.is, d-dimensional data-points parameters j th component follows:~j = rand(1, , d)j = (DAT A)wjq(DAT A) 10 + ~(DAT A)= 1/Khighly variable nature results tests reflected inconsistent values total log likelihood results repeated experiments indicatedusing random starting values initial estimates means frequently gave poorresults. two possible solutions problem. order get good resultsusing random starting values (as specified algorithm) run EM algorithm several times choose initial configuration get maximumlog likelihood among configurations. Choosing best one among several runscomputer intensive process. So, improve outcome EM algorithm gaussianmixture models, necessary find better method estimating initial meanscomponents.best starting position EM algorithm, regard estimates means,would one estimated mean per cluster closer true meancluster.achieve aim explored widely used K-means algorithm cluster(means) finding method. is, means found K-means clusteringutilized initial means EM calculate initial covariance matricesusing unbiased covariance estimation procedure (Equation 35).Ranking Sentencessentences clustered EM algorithm, identify sentencesxi , ) qr denotes clusare question-relevant checking probabilities, P (qr |xx , ) > 0.5 x consideredter question-relevant. sentence x , P (qr |xquestion-relevant. cluster mean values greater oneconsidered question-relevant cluster.next task rank question-relevant sentences order includesummary. done easily multiplying feature vector x~i weightvector w~ learned applying local search technique (Equation 32).29fiChali, Joty, & HasanInput: Sample n data-points ( x ) represented feature vector lengthLInput: Number Clusters KOutput: array EM-based Scoresk , k ) k = 1, , K, equal priors setStart K initial Gaussian models: N (P (qk ) = 1/K.repeat(i)x j , (i) )/* Estimation step: compute probability P (qk |x(i)data point xj , j = 1, , n, belong class qkj 1 nk 1 K(i)x j , (i) ) =P (qk |x(i)(i)xj |qk , (i) )P (qk |(i) )p(xxj |(i) )p(x(i)=endend/* Maximization step:k 1 Kj 1 n// update means:i+1k=(i)(i)x j |k , k )P (qk |(i) )p(x(i)(i)(i)x j |(i)k=1 P (qk | )p(xk , k )PK*/=// update variances:(i+1)k*/(i)xj , (i) )j=1 x j P (qk |xPn(i)xj , (i) )j=1 P (qk |xPn(i)xjxj , (i) )(xxj (i+1))(xj=1 P (qk |xkPN(i)xj , (i) )j=1 P (qk |xPn(i+1))k// update priors:P (qk (i + 1)|(i+1) ) =n1X(i)x j , (i) )P (qk |xn j=1endendtotal likelihood increase falls desired threshold ;returnAlgorithm 3: Computing EM-based similarity measure30fiComplex Question Answering: Unsupervised Approaches6. Redundancy Checking Generating Summarysentences scored easiest way create summaries outputtopmost N sentences required summary length reached. case,ignoring factors: redundancy coherence.know text summarization clearly entails selecting salient information putting together coherent summary. answer summary consistsmultiple separately extracted sentences different documents. Obviously,selected text snippets individually important. However, many competing sentences included summary issue information overlap partsoutput comes mechanism addressing redundancy needed. Therefore,summarization systems employ two levels analysis: first content level everysentence scored according features concepts covers, second textual level,when, added final output, sentences deemed importantcompared similar candidates included final answer summary. Goldstein, Kantrowitz, Mittal, Carbonell (1999)observed authors called Maximum-Marginal-Relevance (MMR). Following Hovy et al. (2006) modeled overlap intermediate summaryto-be-added candidate summary sentence.call overlap ratio R, R 0 1 inclusively. Setting R = 0.7means candidate summary sentence, s, added intermediate summary,S, sentence overlap ratio less equal 0.7.7. Experimental Evaluationsection describes results experiments conducted using DUC12 2007 datasetprovided NIST 13 . questions experiments address include:different features affect behavior summarizer system?one algorithms (K-means, EM Local Search) performs betterparticular problem?used main task DUC 2007 evaluation. task was:Given complex question (topic description) collection relevant documents,task synthesize fluent, well-organized 250-word summary documentsanswers question(s) topic.documents DUC 2007 came AQUAINT corpus comprising newswirearticles Associated Press New York Times (1998-2000) Xinhua NewsAgency (1996-2000). NIST assessors developed topics interest choose set25 documents relevant (document cluster) topic. topic documentcluster given 4 different NIST assessors including developer topic.assessor created 250-word summary document cluster satisfies information12. http://www-nlpir.nist.gov/projects/duc/13. National Institute Standards Technology31fiChali, Joty, & Hasanneed expressed topic statement. multiple reference summaries usedevaluation summary content.purpose experiments study impact different features. accomplish generated summaries 45 topics DUC 2007 sevensystems defined below:LEX system generates summaries based lexical features (Section 4.2):n-gram (n=1,2,3,4), LCS, WLCS, skip bi-gram, head, head synonym overlap.LEXSEM system considers lexical semantic features (Section 4.3): synonym, hypernym/hyponym, gloss, dependency-based proximity-based similarity.SYN system generates summary based syntactic feature (Section 4.1.1).COS system generates summary based graph-based method (Section 4.5).SYS1 system considers features except syntactic semantic features(All features except section 4.1).SYS2 system considers features except semantic feature (All featuresexcept section 4.1.2)system generates summaries taking features (Section 4) account.7.1 Automatic EvaluationROUGE carried automatic evaluation summaries using ROUGE (Lin,2004) toolkit, widely adopted DUC automatic summarization evaluation. ROUGE stands Recall-Oriented Understudy Gisting Evaluation.collection measures determines quality summary comparing reference summaries created humans. measures count number overlapping unitsn-gram, word-sequences, word-pairs system-generated summaryevaluated ideal summaries created humans. available ROUGE measuresare: ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W ROUGE-S. ROUGE-N n-gramrecall candidate summary set reference summaries. ROUGE-L measureslongest common subsequence (LCS) takes account sentence level structuresimilarity naturally identifies longest co-occurring insequence n-grams automatically.ROUGE-W measures weighted longest common subsequence (WLCS) providing improvement basic LCS method computation credit sentencesconsecutive matches words. ROUGE-S overlap skip-bigrams candidate summary set reference summaries skip-bigram pair wordssentence order allowing arbitrary gaps. ROUGE measuresapplied automatic evaluation summarization systems achieved promisingresults (Lin, 2004).systems, report widely accepted important metrics: ROUGE-2ROUGE-SU. also present ROUGE-1 scores since never showncorrelate human judgement. ROUGE measures calculated running32fiComplex Question Answering: Unsupervised ApproachesROUGE-1.5.5 stemming removal stopwords. ROUGE run-time parametersset DUC 2007 evaluation setup. are:ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -aalso show 95% confidence interval important evaluation metrics systemsreport significance meaningful comparison. use ROUGE toolpurpose. ROUGE uses randomized method named bootstrap resampling computeconfidence interval. used 1000 sampling points bootstrap resampling.report evaluation scores one baseline system (The BASE column)tables order show level improvement systems achieve. baselinesystem generates summaries returning leading sentences (up 250 words)hT EXT field recent document(s).presenting results highlight top two F-scores bottom one F-scoreindicate significance glance.7.1.1 Results DiscussionK-means Learning Table 1 shows ROUGE-1 scores different combinationsfeatures K-means learning. noticeable K-means performs bestgraph-based cosine similarity feature. Note including syntactic featureimprove score. Also, including syntactic semantic features increases scoresignificant amount. Summaries based lexical features give us goodROUGE-1 evaluation.ScoresRecallPrecisionF-scoreLEX0.3660.3970.381LEXSEM0.3600.3930.376SYN0.3460.3780.361COS0.3780.4080.393SYS10.3760.4030.389SYS20.3650.4150.3880.3660.4150.389BASE0.3120.3690.334Table 1: ROUGE-1 measures K-means learningTable 2 shows ROUGE-2 scores different combinations features K-meanslearning. like ROUGE-1 graph-based cosine similarity feature performs well here.get significant improvement ROUGE-2 score include syntactic featurefeatures. Semantic features affect score much. Lexical Semantic featuresperform well here.ScoresRecallPrecisionF-scoreLEX0.0740.0800.077LEXSEM0.0760.0840.080SYN0.0630.0690.065COS0.0850.0920.088SYS10.0740.0800.077SYS20.0770.1070.0900.0760.1090.090Table 2: ROUGE-2 measures K-means learning33BASE0.0600.0720.064fiChali, Joty, & HasanTable 3 shows: ROUGE-SU scores best features without syntacticsemantic. Including syntactic/semantic features features degrades scores.Summaries based lexical features achieve good scores.ScoresRecallPrecisionF-scoreLEX0.1310.1540.141LEXSEM0.1270.1520.138SYN0.1160.1390.126COS0.1390.1620.149SYS10.1350.1760.153SYS20.1340.1740.1520.1340.1740.152BASE0.1050.1240.112Table 3: ROUGE-SU measures K-means learningTable 4 shows 95% confidence interval (for F-measures K-means learning)important ROUGE evaluation metrics systems comparison confidenceinterval baseline system. seen systems performed significantlybetter baseline system cases.SystemsBaselineLEXLEXSEMSYNCOSSYS1SYS2ROUGE-10.326680 - 0.3423300.362976 - 0.4004980.357154 - 0.3955940.345512 - 0.3775250.372804 - 0.4134400.367817 - 0.4083900.358237 - 0.4000000.350756 - 0.404275ROUGE-20.060870 - 0.0688400.064983 - 0.0909810.069909 - 0.0913760.056041 - 0.0763370.075127 - 0.1043770.063284 - 0.0951700.065219 - 0.0937330.066281 - 0.095393ROUGE-SU0.108470 - 0.1167200.128390 - 0.1577840.126157 - 0.1518310.116191 - 0.1367990.134971 - 0.1648850.132061 - 0.1625090.123703 - 0.1531650.124157 - 0.159447Table 4: 95% confidence intervals K-means systemEM learning Table 5 Table 7 show different ROUGE measures featurecombinations context EM learning. easily noticedmeasures get significant amount improvement ROUGE scores includesyntactic semantic features along features. get 3-15% improvementSYS1 F-score include syntactic feature 2-24% improvement includesyntactic semantic features. cosine similarity measure perform wellK-means experiments. Summaries considering lexical features achievegood results.Table 8 shows 95% confidence interval (for F-measures EM learning) important ROUGE evaluation metrics systems comparison confidenceinterval baseline system. see systems performed significantlybetter baseline system cases.Local Search Technique ROUGE scores based feature combinationsgiven Table 9 Table 11. Summaries generated including features perform34fiComplex Question Answering: Unsupervised ApproachesScoresRecallPrecisionF-scoreLEX0.3830.4150.398LEXSEM0.3570.3900.373SYN0.3460.3780.361COS0.3750.4060.390SYS10.3790.4110.395SYS20.3990.4110.4050.3980.3990.399BASE0.3120.3690.3340.0900.1380.109BASE0.0600.0720.0640.1430.1850.161BASE0.1050.1240.112Table 5: ROUGE-1 measures EM learningScoresRecallPrecisionF-scoreLEX0.0880.0950.092LEXSEM0.0790.0870.083SYN0.0630.0690.065COS0.0870.0940.090SYS10.0840.0910.088SYS20.0890.1160.100Table 6: ROUGE-2 measures EM learningScoresRecallPrecisionF-scoreLEX0.1450.1710.157LEXSEM0.1280.1530.139SYN0.1160.1390.126COS0.1380.1620.149SYS10.1430.1670.154SYS20.1450.1860.163Table 7: ROUGE-SU measures EM learningSystemsBaselineLEXLEXSEMSYNCOSSYS1SYS2ROUGE-10.326680 - 0.3423300.382874 - 0.4161090.352610 - 0.3950480.345512 - 0.3775250.366364 - 0.4100200.378068 - 0.4136580.360319 - 0.4140680.378177 - 0.412705ROUGE-20.060870 - 0.0688400.075084 - 0.1104540.070816 - 0.0958560.056041 - 0.0763370.076088 - 0.1042430.077480 - 0.0997390.073661 - 0.1121570.077515 - 0.115231ROUGE-SU0.108470 - 0.1167200.144367 - 0.1724490.125276 - 0.1545620.115713 - 0.1365990.133251 - 0.1641100.141550 - 0.1687590.130022 - 0.1713780.141345 - 0.164849Table 8: 95% confidence intervals EM system35fiChali, Joty, & Hasanbest scores measures. get 7-15% improvement SYS1 F-scoreinclude syntactic feature 8-19% improvement SYS1 F-score includesyntactic semantic features. case also lexical features (LEX) perform wellbetter features (ALL).ScoresRecallPrecisionF-scoreLEX0.3790.4110.394LEXSEM0.3580.3900.373SYN0.3460.3780.361COS0.3750.4060.390SYS10.3820.4140.397SYS20.3880.4340.4100.3900.4380.413BASE0.3120.3690.334Table 9: ROUGE-1 measures local search techniqueScoresRecallPrecisionF-scoreLEX0.0850.0920.088LEXSEM0.0790.0870.083SYN0.0630.0690.065COS0.0870.0940.090SYS10.0860.0930.090SYS20.0950.1140.1040.0990.1160.107BASE0.0600.0720.064Table 10: ROUGE-2 measures local search techniqueScoresRecallPrecisionF-scoreLEX0.1430.1680.155LEXSEM0.1280.1530.139SYN0.1160.1390.126COS0.1380.1620.149SYS10.1450.1700.157SYS20.1480.1950.1690.1500.1960.170BASE0.1050.1240.112Table 11: ROUGE-SU measures local search techniqueTable 12 shows 95% confidence interval (for F-measures local search technique)important ROUGE evaluation metrics systems comparison confidence interval baseline system. find systems performed significantlybetter baseline system cases.7.1.2 Comparisonresults reported see three algorithms systems clearly outperform baseline system. Table 13 shows F-scores reported ROUGE measuresTable 14 reports 95% confidence intervals baseline system, best systemDUC 2007, three techniques taking features (ALL) consideration.see method based local search technique outperforms twoEM algorithm performs better K-means algorithm. analyze deeply, findcases ROUGE-SU local search confidence intervals overlapbest DUC 2007 system.36fiComplex Question Answering: Unsupervised ApproachesSystemsBaselineLEXLEXSEMSYNCOSSYS1SYS2ROUGE-10.326680 - 0.3423300.380464 - 0.4090850.353458 - 0.3948530.345512 - 0.3775250.366364 - 0.4100200.381544 - 0.4145340.370310 - 0.4157680.384897 - 0.416301ROUGE-20.060870 - 0.0688400.078002 - 0.1001070.070845 - 0.0962610.056041 - 0.0763370.076088 - 0.1042430.079550 - 0.1012460.078760 - 0.1141750.084181 - 0.114753ROUGE-SU0.108470 - 0.1167200.143851 - 0.1666480.125342 - 0.1547290.115713 - 0.1365990.133251 - 0.1641100.144551 - 0.1700470.141043 - 0.1745750.146302 - 0.171736Table 12: 95% confidence intervals local search systemAlgorithmsBaselineBest SystemK-meansEMLocal SearchROUGE-10.3340.4380.3890.3990.413ROUGE-20.0640.1220.0890.1090.107ROUGE-SU0.1120.1740.1520.1610.170Table 13: ROUGE F-scores different systemsAlgorithmsBaselineBest SystemK-meansEMLocal SearchROUGE-10.326680 - 0.3423300.431680 - 0.4459700.350756 - 0.4042750.378177 - 0.4127050.384897 - 0.416301ROUGE-20.060870 - 0.0688400.118000 - 0.1276800.066281 - 0.0953930.077515 - 0.1152310.084181 - 0.114753ROUGE-SU0.108470 - 0.1167200.169970 - 0.1793900.124157 - 0.1594470.141345 - 0.1648490.146302 - 0.171736Table 14: 95% confidence intervals different systems37fiChali, Joty, & Hasan7.2 Manual Evaluationsample 105 summaries14 drawn different systems generated summariesconduct extensive manual evaluation order analyze effectivenessapproaches. manual evaluation comprised Pyramid-based evaluation contentsuser evaluation get assessment linguistic quality overall responsiveness.7.2.1 Pyramid EvaluationDUC 2007 main task, 23 topics selected optional community-basedpyramid evaluation. Volunteers 16 different sites created pyramids annotatedpeer summaries DUC main task using given guidelines15 . 8 sites amongcreated pyramids. used pyramids annotate peer summariescompute modified pyramid scores16 . used DUCView.jar17 annotation toolpurpose. Table 15 Table 17 show modified pyramid scores systemsthree algorithms. baseline systems score also reported. peer summariesbaseline system generated returning leading sentences (up 250 words)hT EXT field recent document(s). results seesystems perform better baseline system inclusion syntactic semanticfeatures yields better scores. three algorithms also notice lexicalsemantic features best terms modified pyramid scores.7.2.2 User Evaluation10 university graduate students judged summaries linguistic quality overallresponsiveness. given score integer 1 (very poor) 5 (very good)guided consideration following factors: 1. Grammaticality, 2. Non-redundancy,3. Referential clarity, 4. Focus 5. Structure Coherence. also assignedcontent responsiveness score automatic summaries. content scoreinteger 1 (very poor) 5 (very good) based amount informationsummary helps satisfy information need expressed topic narrative.measures used DUC 2007. Table 18 Table 20 present average linguisticquality overall responsive scores systems three algorithms.baseline systems scores given meaningful comparison. closer lookresults, find systems perform worse baseline system termslinguistic quality achieve good scores case overall responsiveness. alsoobvious tables exclusion syntactic semantic features often causeslower scores. hand, lexical lexical semantic features show good overallresponsiveness scores three algorithms.14. 7 systems 3 algorithms, cumulatively 21 systems. Randomly chose5 summaries 21 systems.15. http://www1.cs.columbia.edu/ becky/DUC2006/2006-pyramid-guidelines.html16. equals sum weights Summary Content Units (SCUs) peer summary matches,normalized weight ideally informative summary consisting number contributorspeer.17. http://www1.cs.columbia.edu/ ani/DUC2005/Tool.html38fiComplex Question Answering: Unsupervised ApproachesSystemsBaselineLEXLEXSEMSYNCOSSYS1SYS2Modified Pyramid Scores0.138740.449840.517580.457620.503680.428720.416660.49900Table 15: Modified pyramid scores K-means systemSystemsBaselineLEXLEXSEMSYNCOSSYS1SYS2Modified Pyramid Scores0.138740.518940.532260.450580.484840.477580.447340.49756Table 16: Modified pyramid scores EM systemSystemsBaselineLEXLEXSEMSYNCOSSYS1SYS2Modified Pyramid Scores0.138740.497600.539120.435120.495100.469760.464040.47944Table 17: Modified pyramid scores local search system39fiChali, Joty, & HasanSystemsBaselineLEXLEXSEMSYNCOSSYS1SYS2Linguistic Quality4.243.084.083.244.002.723.123.56Overall Responsiveness1.803.203.803.603.602.202.803.80Table 18: Linguistic quality responsive scores K-means systemSystemsBaselineLEXLEXSEMSYNCOSSYS1SYS2Linguistic Quality4.244.083.564.203.803.684.203.36Overall Responsiveness1.804.403.403.804.003.803.603.40Table 19: Linguistic quality responsive scores EM systemSystemsBaselineLEXLEXSEMSYNCOSSYS1SYS2Linguistic Quality4.243.243.122.643.403.403.123.20Overall Responsiveness1.802.404.202.003.403.603.803.20Table 20: Linguistic quality responsive scores local search system40fiComplex Question Answering: Unsupervised Approaches8. Conclusion Future Workpaper presented works answering complex questions. extracted eighteen important features sentences document collection. Later usedsimple local search technique fine-tune feature weights. weight, wi ,algorithm achieves local maximum ROUGE value. way, learnweights rank sentences multiplying feature-vector weight-vector.also experimented two unsupervised learning techniques: 1) EM 2) K-meansfeatures extracted. assume two clusters sentences: 1. queryrelevant 2. query-irrelevant. learned means clusters using K-meansalgorithm used Bayesian model order rank sentences. learned meansK-means algorithm used initial means EM algorithm. applied EM algorithm cluster sentences two classes : 1) query-relevant 2)query-irrelevant. take query-relevant sentences rank using learnedweights (i.e. local search). methods generating summaries filterredundant sentences using redundancy checking module generate summariestaking top N sentences.also experimented effects different kinds features. evaluatedsystems automatically using ROUGE report significance results95% confidence intervals. conducted two types manual evaluation: 1) Pyramid2) User Evaluation analyze performance systems. experimentalresults mostly show following: (a) approaches achieve promising results, (b)empirical approach based local search technique outperforms two learningtechniques EM performs better K-means algorithm, (c) systems achievebetter results include tree kernel based syntactic semantic features,(d) cases ROUGE-SU local search confidence intervals overlapbest DUC 2007 system.experimenting supervised learning techniques (i.e. SVM, MAXENT, CRF etc) analyzing perform problem. Prior that, produced huge amount labeled data automatically using similarity measures ROUGE(Toutanova et al., 2007).future plan decompose complex questions several simple questionsmeasuring similarity document sentence query sentence.certainly serve create limited trees subsequences might increaseprecision. Thus, expect decomposing complex questions setssubquestions entail systems improve average quality answers returnedachieve better coverage question whole.Acknowledgmentsthank anonymous reviewers useful comments earliest versionpaper. Special thanks go colleagues proofreading paper. also gratefulgraduate students took part user evaluation process. researchreported supported Natural Sciences Engineering Research Council(NSERC) research grant University Lethbridge.41fiChali, Joty, & HasanAppendix A. Stop Word Listreutersmaynovtueaccordinglyaloneanotheranywayappropriateaskawfullybecomesbettercantcertainlycomescontainingcurrentlydidntdontelseetceveryoneexceptfollowedforthgetgoeshhasntapjundecwedacrossaintalongamidanywaysaskingbbecomingbelieveccannotchangesconcerningcontainsdifferentdoneeduelsewhereetc.everythingffollowingfourgetsgoingjanjultechthuableactuallyalreadyamonganybodyanywherearentassociatedbeyondcmoncantclearlyconsequentlycorrespondingdefinitelyegenougheveneverywherefarfollowsgettinggonehadnthavent42febaugnewsfriallowalsoamongstanyhowapartaroundbecamebesidecscausecoconsidercoulddescribeddownwardse.g.entirelyeverexgivengothappensmarsepindexsatafterwardsallowsalthoughanyoneappearavailablebeforehandbesidesbriefcamecausescomconsideringcouldntdespitedoesnteightespeciallyeveryexactlyfifthformerfurthermoregivesgottenhardlyaproctmonaccordingalmostalwaysanythingappreciateasideawaybecomebehindbestcertaincomecontaincourseeeithereteverybodyexamplefiveformerlyggogreetingshesfiComplex Question Answering: Unsupervised Approacheshellohereafterhiimimmediateindicatedinwardkeepllesslikelymeannearlyneverthelessnonnothingoldontooverallperhapsprobablyrregardinghelpherebyhowbeitiveindicateskeepslatelylestlittlemainlymeanwhilemostlynecessarynewnonenovelplacedprovidesratherregardlesshencehereinhoweverieinasmuchinnerisntkeptlaterletlookmanymerelymr.nneednextnooneoftenppleaseqrdregardshereuponi.e.incinsofarjknowlatterletslookingmaymightms.namelyneedsninenowhereohoneothersparticularplusquerelatively43hitheridindeedinsteaditdknowslatterlylikelooksmaybemuchndneithernormallyokonesotherwiseoutsideparticularlypossiblequitereallyrespectivelyhereshopefullyillignoredindicateitllkknownleastlikedltdmoreovermustnearnevernobodyobviouslyokayoughtperpresumablyqvreasonablyrightfiChali, Joty, & Hasansaysseemedsensibleshallsometimespecifiedsuptellthanxtheresthereupontheyvethustowardstwiceunlessususuallyviawerentwheneverwhereinwhitherwhosewithinwouldntyoudsaidsecondseemingsentsometimesspecifysuretendsthereafterthinkthoughtriedtwounlikelyuseuucpvizwasntwevewhereuponwithoutxyoullsecondlyseemsserioussomebodysomewhatspecifyingththatstherebythirdthreetogethertriesuusedvvswaywelcomewhatswhereswhereverwhoswontyourezsawseeseenseriouslyshouldntsomehowsomewherestilltsthatsthereforetheydtrulyununtousefulvaluewwellwhateverwhereafterwhetherwhoeverwillingwonderyesyouvezero44sayseeingselfsevensincesomeonesoonsubtakethankthencethereintheyllthoroughthroughouttooktryusesvariouswantwedwentwhereaswholewishwouldyetsayingseemselvesseveralsixsomethingsorrytakenthankstherestheyrethoroughlythrutowardtryingunfortunatelyuponusingwantswellwhencewherebywouldfiComplex Question Answering: Unsupervised ApproachesReferencesBloehdorn, S., & Moschitti, A. (2007a). Combined syntactic semantic kernels textclassification. 29th European Conference IR Research, ECIR 2007, pp. 307318Rome, Italy.Bloehdorn, S., & Moschitti, A. (2007b). Structure semantics expressive text kernels.CIKM-2007, pp. 861864.Chali, Y., & Joty, S. R. (2008a). Improving performance random walk modelanswering complex questions.. Proceedings 46th Annual MeetingACL-HLT. Short Paper Section, pp. 912 OH, USA.Chali, Y., & Joty, S. R. (2008b). Selecting sentences answering complex questions.Proceedings EMNLP, pp. 304313 Hawaii, USA.Charniak, E. (1999). Maximum-Entropy-Inspired Parser. Technical Report CS-99-12Brown University, Computer Science Department.Collins, M., & Duffy, N. (2001). Convolution Kernels Natural Language. ProceedingsNeural Information Processing Systems, pp. 625632 Vancouver, Canada.Cormen, T. R., Leiserson, C. E., & Rivest, R. L. (1989). Introduction Algorithms.MIT Press.Erkan, G., & Radev, D. R. (2004). LexRank: Graph-based Lexical Centrality SalienceText Summarization. Journal Artificial Intelligence Research, 22, 457479.Goldstein, J., Kantrowitz, M., Mittal, V., & Carbonell, J. (1999). Summarizing Text Documents: Sentence Selection Evaluation Metrics. Proceedings 22nd International ACM Conference Research Development Information Retrieval,SIGIR, pp. 121128 Berkeley, CA.Guo, Y., & Stylios, G. (2003). New Multi-document Summarization System. Proceedings Document Understanding Conference. NIST.Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., & Jurafsky, D. (2003). ShallowSemantic Parsing Using Support Vector Machines. Technical Report TR-CSLR2003-03 University Colorado.Halliday, M., & Hasan, R. (1976). Cohesion English. Longman, London.Harabagiu, S., Lacatusu, F., & Hickl, A. (2006). Answering complex questions randomwalk models. Proceedings 29th annual international ACM SIGIR conferenceResearch development information retrieval, pp. 220 227. ACM.Hirao, T., , Suzuki, J., Isozaki, H., & Maeda, E. (2004). Dependency-based sentencealignment multiple document summarization. Proceedings Coling 2004, pp.446452 Geneva, Switzerland. COLING.45fiChali, Joty, & HasanHovy, E., Lin, C. Y., Zhou, L., & Fukumoto, J. (2006). Automated Summarization Evaluation Basic Elements. Proceedings Fifth Conference LanguageResources Evaluation Genoa, Italy.Kingsbury, P., & Palmer, M. (2002). Treebank PropBank. Proceedingsinternational conference Language Resources Evaluation Las Palmas, Spain.Kouylekov, M., & Magnini, B. (2005). Recognizing textual entailment tree edit distancealgorithms. Proceedings PASCAL Challenges Workshop: Recognising TextualEntailment Challenge.Li, J., Sun, L., Kit, C., & Webster, J. (2007). Query-Focused Multi-Document Summarizer Based Lexical Chains. Proceedings Document UnderstandingConference Rochester. NIST.Lin, C. Y. (2004). ROUGE: Package Automatic Evaluation Summaries. Proceedings Workshop Text Summarization Branches Out, Post-Conference WorkshopAssociation Computational Linguistics, pp. 7481 Barcelona, Spain.Lin, D. (1998a). Information-Theoretic Definition Similarity. ProceedingsInternational Conference Machine Learning, pp. 296304 Madison, Wisconsin.Lin, D. (1998b). Automatic Retrieval Clustering Similar Words. ProceedingsInternational Conference Computational Linguistics AssociationComputational Linguistics, pp. 768774 Montreal, Canada.Losada, D. (2005). Language modeling sentence retrieval: comparisonmultiple-bernoulli models multinomial models. Information Retrieval Theory Workshop Glasgow, UK.Losada, D., & Fernandez, R. T. (2007). Highly frequent terms sentence retrieval.Proc. 14th String Processing Information Retrieval Symposium, SPIRE07, pp.217228 Santiago de Chile.MacCartney, B., Grenager, T., de Marneffe, M., Cer, D., & Manning, C. D. (2006). Learning recognize features valid textual entailments. Proceedings HumanLanguage Technology Conference North American Chapter ACL, p. 4148New York, USA.Morris, J., & Hirst, G. (1991). Lexical cohesion computed thesaural relationsindicator structure text. Computational Linguistics, 17 (1), 2148.Moschitti, A. (2006). Efficient convolution kernels dependency constituent syntactictrees. Proceedings 17th European Conference Machine Learning Berlin,Germany.Moschitti, A., & Basili, R. (2006). Tree Kernel approach Question Answer Classification Question Answering Systems. Proceedings 5th internationalconference Language Resources Evaluation Genoa, Italy.46fiComplex Question Answering: Unsupervised ApproachesMoschitti, A., & Quarteroni, S. (2008). Kernels linguistic structures answer extraction. Proceedings 46th Conference Association ComputationalLinguistics (ACL08). Short Paper Section Columbus, OH, USA.Moschitti, A., Quarteroni, S., Basili, R., & Manandhar, S. (2007). Exploiting SyntacticShallow Semantic Kernels Question/Answer Classificaion. Proceedings45th Annual Meeting Association Computational Linguistics, pp. 776783Prague, Czech Republic. ACL.Murdock, V., & Croft, W. B. (2005). translation model sentence retrieval. HLT 05:Proceedings conference Human Language Technology Empirical MethodsNatural Language Processing, pp. 684691 Morristown, NJ, USA. ACL.Otterbacher, J., Erkan, G., & Radev, D. R. (2005). Using Random Walks Questionfocused Sentence Retrieval. Proceedings Human Language Technology ConferenceConference Empirical Methods Natural Language Processing, pp. 915922Vancouver, Canada.Pingali, P., K., R., & Varma, V. (2007). IIIT Hyderabad DUC 2007. ProceedingsDocument Understanding Conference Rochester. NIST.Punyakanok, V., Roth, D., & Yih, W. (2004). Mapping dependencies trees: applicationquestion answering. Proceedings AI & Math Florida, USA.Strzalkowski, T., & Harabagiu, S. (2008). Advances Open Domain Question Answering.Springer.Toutanova, K., Brockett, C., Gamon, M., Jagarlamudi, J., Suzuki, H., & Vanderwende,L. (2007). pythy summarization system: Microsoft research duc 2007.proceedings Document Understanding Conference Rochester. NIST.Vanderwende, L., Suzuki, H., & Brockett, C. (2006). Microsoft Research DUC2006:Task-Focused Summarization Sentence Simplification Lexical Expansion.Proceedings Document Understanding Conference Rochester. NIST.Zajic, D. M., Lin, J., Dorr, B. J., & Schwartz, R. (2006). Sentence Compression Component Multi-Document Summarization System. Proceedings DocumentUnderstanding Conference Rochester. NIST.Zhang, A., & Lee, W. (2003a). Question Classification using Support Vector Machines.Proceedings Special Interest Group Information Retrieval, pp. 2632 Toronto,Canada. ACM.Zhang, D., & Lee, W. S. (2003b). Language Modeling Approach Passage QuestionAnswering. Proceedings Twelfth Text REtreival Conference, pp. 489495Gaithersburg, Maryland.Zhou, L., Lin, C. Y., & Hovy, E. (2005). BE-based Multi-dccument SummarizerQuery Interpretation. Proceedings Document Understanding Conference Vancouver, B.C., Canada.47fiJournal Artificial Intelligence Research 35 (2009) 813857Submitted 03/09; published 08/09Modularity Aspects Disjunctive Stable ModelsTomi JanhunenEmilia OikarinenTomi.Janhunen@tkk.fiEmilia.Oikarinen@tkk.fiHelsinki University TechnologyDepartment Information Computer ScienceP.O. Box 5400, FI-02015 TKK, FinlandHans TompitsStefan Woltrantompits@kr.tuwien.ac.atwoltran@dbai.tuwien.ac.atTechnische Universitt WienInstitut fr InformationssystemeFavoritenstrae 911, A-1040 Vienna, AustriaAbstractPractically programming languages allow programmer split programseveral modules brings along several advantages software development.paper, interested area answer-set programming fully declarativenonmonotonic languages applied. context, obtaining modular structureprograms means straightforward since output entire program cannotgeneral composed output components. better understand effectsdisjunctive information modularity restrict scope analysis casedisjunctive logic programs (DLPs) subject stable-model semantics. define notionDLP-function, well-defined input/output interface provided, establishnovel module theorem indicates compositionality stable-model semanticsDLP-functions. module theorem extends well-known splitting-set theoremenables decomposition DLP-functions given strongly connected componentsbased positive dependencies induced rules. setting, also possible splitshared disjunctive rules among components using generalized shifting technique.concept modular equivalence introduced mutual comparison DLP-functionsusing generalization translation-based verification method.1. IntroductionPractically programming languages used software development allow programmersplit program several modules interact well-dened input/outputinterfaces. Given this, entire program viewed composition component modules typically linked together respective run-time environment.expected benets modular program development manifold. First, imposesgood programming style followed programmer. complex software systemmuch easier develop set interacting components rather monolithic program. Second, modular architecture allows additional exibility regards delegatingprogramming tasks amongst team programmers. setting, goal programmer implement desired input/output behavior(s) terms concrete module(s)together implement software system developed. Third, modular programc2009AI Access Foundation. rights reserved.fiJanhunen, Oikarinen, Tompits & Woltrandesign also exploited order boost execution programs. Program optimization also facilitated structural information encompassed module interfaces.Answer-set programming (ASP) (Marek & Truszczyski, 1999; Niemel, 1999; Gelfond& Leone, 2002) paradigm declarative problem solving solutions problemsdescribed terms rules subject nonmonotonic semantics based stable models(Gelfond & Lifschitz, 1988). typical problem representations, tight correspondencesolutions stable models sought for, default negation fully exploitedorder obtain concise encodings relations involved problem descriptions.Furthermore, recursive denitions enable, e.g., representation closures relationsnatural way. Due ecient implementations emerging applications,paradigm received increasing attention past two decades.1 meantime,number extensionssuch disjunctions, weight constraints, aggregateshaveproposed basic syntax normal logic programs. paper, concentrateclass disjunctive logic programs (DLPs) appropriate solving search problemsresiding second level polynomial-time hierarchy. semantical accountDLPs based respective generalization stable-model semantics (Gelfond &Lifschitz, 1991).paper, goal investigate modularity context DLPs stablemodel semantics. Since stable models dened complete programs,lend modular programming prima facie. Perhaps reason, conceptmodule yet raised much attention realm answer-set programming.Except dedicated papers (Gaifman & Shapiro, 1989; Eiter, Gottlob, & Veith, 1997b;Baral, Dzifcak, & Takahashi, 2006), modules mostly appeared by-product studiesformal properties like stratication, splitting, or, lately, work equivalencerelations programs (Lifschitz & Turner, 1994; Eiter, Gottlob, & Mannila, 1997a;Eiter, Ianni, Lukasiewicz, Schindlauer, & Tompits, 2008). recent approach OikarinenJanhunen (2008a), modular architecture put forth Gaifman Shapiro (1989)accommodated classes normal smodels programs. main resultmodule theorem links stable models associated individual modulescomposition. result signicant indicates stable modelscompositional much sense classical models propositional logic.major restriction implied module theorem denition setpositively interdependent atoms must given within module.Besides general benets modular program development discussed above,also looking potential computational advantages modularizing reasoning tasks ASP.context, search stable models probably central reasoning task.Results like module theorem discussed provide basis modularizingsearch task. Extra care, however, required computation stable modelsmodules separation necessarily ecient. sophisticated methods,identifying cones influence Boolean circuits (Junttila & Niemel, 2000), devisedidentify modules relevant search stable modelsthe rest usedexpand qualied stable model one entire program. strategy alleviatestreatment extremely large program instances also amenable query evaluation.1. 20th anniversary stable-model semantics celebrated ICLP08 held Udine,Italy, December 2008.814fiModularity Aspects Disjunctive Stable ModelsUnfortunately, contemporary disjunctive answer-set solvers, claspd (Drescheret al., 2008), cmodels (Giunchiglia, Lierler, & Maratea, 2006), dlv (Leone et al., 2006),gnt (Janhunen, Niemel, Seipel, Simons, & You, 2006), exhibit little support modularreasoning although related techniques like strongly connected components exploited internally. also reasoning tasks boosted modular approach.instance, optimization answer-set programs gives rise problem verifyingwhether dierent versions programs answer sets. demonstratedOikarinen Janhunen (2009), verication tasks may benet modularization,and, particular, approximation techniques based modular equivalence introduced.Following idea, rst modular o-line optimizer answer-set programs, called modopt, recently implemented (Janhunen, 2008b).also interesting applications modules sight: Gebser et al. (2008a)propose incremental technique answer-set solving. idea gradually extendprogram instance terms additional modules, e.g., solving AI planning problems.Moreover, theoretical results like splitting-set theorem (Lifschitz & Turner, 1994)module theorem directly exploited correctness proofs. instance, provedOikarinen Janhunen (2008b) models prioritized circumscriptioncaptured disjunctive stable models using particular translation. similar proofstrategy adopted Theorem 8.5 paper.anticipate compositional semantics also prove useful one tries boostsearch stable models via parallelization, e.g., computing stable models modulesparallel. However, order avoid excessive communication costs, extra caution neededstable models computed separation linked together potentially rejected.One possibility identify mutually independent modules basis distribution.Besides aspect, modularization may also lead novel methods (non-parallelized)computation stable models, traditional ones.Structure Preview Results paper, concentrate formal underpinnings modular programming context disjunctive logic programsstable-model semantics. proceed follows. rst goal generalize theorydeveloped normal programs smodels programs (Oikarinen & Janhunen, 2008a)case disjunctive programs. end, rst introduce notion DLPfunction Section 2. term goes back Gelfond Gabaldon (1999) introducedLP-functions (partial) denitions new relations terms old, known ones. enable functional view disjunctive programs, endowed well-denedinput/output interface. idea partition signature program encapsulatedway input atoms, output atoms, hidden (or local ) atoms. distinctionsprovide basis systematic composition larger disjunctive logic programsprogram modules. However, arbitrary combinations program modules meaningful and, rst all, adopt syntactic restrictions introduced Gaifman Shapiro(1989) context negation/disjunction-free logic programs. interplay default negation disjunctions brings along new factors lead relaxationrestrictions sense program modules allowed share rules. Then,basic syntactic issues DLP-functions laid out, concentrate semanticsSection 3. respect, follow strict model-theoretic approach and, particular,815fiJanhunen, Oikarinen, Tompits & Woltranaddress role input atoms comes viewing DLP-functions mathematicalfunctions. proceed step step assign three dierent classes modelsDLP-function, viz. classical models, minimal models, stable models. last providesappropriate generalization disjunctive stable models (Gelfond & Lifschitz, 1991)presence input atoms.second objective establish adequacy concept DLP-functionview compositional semantics. witnessed main result paper,viz. module theorem shows stable models DLP-function, ,alternatively obtained unions compatible stable models modules constituting. proof theorem based notions completion (Clark, 1978) loopformulas (Lin & Zhao, 2004; Lee & Lifschitz, 2003) rst lifted case DLPfunctions Section 4 preparatory step. proof module theorem followsmain topic Section 5. result non-trivial underlying semantics basedstable models inherently nonmonotonic. feature already recognized GaifmanShapiro (1989) much simpler setting denite programsneither involving defaultnegation disjunctions. observed them, too, syntactic restrictions programcomposition necessary order guarantee compositionality properties semanticsbased Herbrand models.2 current paper, strive analogous resultscase programs permitting default negation disjunctions. turnsstrongly connected components positive dependency graphs provide key criterioncomes conning program composition. compositionality properties disjunctiveprograms stable-model semantics also arisen context so-calledsplitting-set theorem (Lifschitz & Turner, 1994; Eiter et al., 1997a, 2008). fact,module theorem established herein proper generalization predecessor (Oikarinen &Janhunen, 2008a). illustrate potential modular architecture evaluationquantied Boolean formulas (QBFs), serve canonical representatives classespolynomial-time hierarchy (PH). Due basic complexity results established EiterGottlob (1995), natural perspective concentrate second levelPH case disjunctive programs.third aim paper look particular applications module theorem disjunctive logic programming. Section 6, take opposite viewmodular construction DLP-functions consider possibilities decompositioneven absence structural information. turns strongly connectedcomponents also exploited respect but, addition, occurrences hiddenatoms must taken account splitting DLP-function components.demonstrated Section 7, results open new prospects regards unwinding disjunctionsusing principle shifting (Gelfond, Przymusinska, Lifschitz, & Truszczyski, 1991; Dix,Gottlob, & Marek, 1996; Eiter, Fink, Tompits, & Woltran, 2004). proper generalizationprinciple partially covers also programs involving head-cycles formulatedproved correct. Moreover, due modular nature DLP-functions, makes perfectsense compare modules. notion modular equivalence introducedpurpose Section 8. Interestingly, modular equivalence supports substitutions equivalentprograms also lends translation-based verification put forth Oikarinen2. main concern Gaifman Shapiro (1989) modularity respect logical consequencesdefinite program hence intersection Herbrand models.816fiModularity Aspects Disjunctive Stable ModelsJanhunen (2004, 2009) related cases ordinary equivalence smodels programs. Section 9 contrasts approach related work. Finally, Section 10 providesbrief summary results concludes paper.2. Class DLP-Functionstopic section syntax DLP-functions well syntactic restrictionsimposed composition DLP-functions. disjunctive rule expression forma1 b1 , . . . , bm , c1 , . . . , ck ,(1)n, m, k 0, a1 , . . . , , b1 , . . . , bm , c1 , . . . , ck propositional atoms. Sinceorder atoms considered insignicant, write B, C shorthand rulesform (1), = {a1 , . . . , }, B = {b1 , . . . , bm }, C = {c1 , . . . , ck } respectivesets atoms. basic intuition behind rule B, C atompositive body B inferred none atoms negative body C,atom head inferred. B C empty, disjunctivefact, written . empty, constraint, written B, C.disjunctive logic program (DLP) conventionally formed nite set disjunctiverules. Additionally, want distinguished input output interface DLP.end, extend denition originally proposed Gaifman Shapiro (1989) casedisjunctive programs.3 natural interface imposes certain restrictionsrules allowed module. Given set R disjunctive rules, write At(R)signature R, i.e., set (ground) atoms eectively appearing rules R.Definition 2.1 DLP-function, , quadruple hR, I, O, Hi, I, O, Hpairwise distinct sets input atoms, output atoms, hidden atoms, respectively, RDLP rule B, C R,1. B C H,2. 6= , (O H) 6= .DLP-function = hR, I, O, Hi occasionally identied R and, slight abusenotation, write B, C denote B, C R. rst conditionDenition 2.1, rules DLP-function must obey interface specication ,i.e., At(R) H. regards sets atoms I, O, H involved moduleinterface, atoms considered visible hence accessibleDLP-functions conjoined ; either produce input utilize output. hand, hidden atoms H used formalize auxiliary conceptsmay make sense context DLP-functions may save spacesubstantially demonstrated, e.g., Janhunen Oikarinen (2007, Example 4.5).second condition Denition 2.1 concerned set atoms H definedrules R. principle non-empty disjunctive head must involve least oneatom H. ensure DLP-function must interfere3. Similar approaches within area ASP previously introduced Gelfond Gabaldon(1999), Janhunen (2006), Oikarinen Janhunen (2008a).817fiJanhunen, Oikarinen, Tompits & Woltrandenitions input atoms terms rules B, C satisfying I.otherwise, rules may conditioned input atoms.4 Given set atoms,distinguish set rules dene atoms R, i.e., set defining rulesDef R (S) = {A B, C R | 6= }.(2)next objective specify conditions composition DLP-functionsmay take place. Roughly speaking, idea larger DLP-functions formedmodular fashion using smaller DLP-functions components. observed alreadyGaifman Shapiro (1989), syntactic restrictions program composition necessaryorder guarantee compositionality properties semantics based Herbrand models, even simple case denite programs. Thus, program union operatorcomposition without restrictions satisfactory respect compositionality.start adapting construction Gaifman Shapiro (1989) casedisjunctive programs.Definition 2.2 Two DLP-functions 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 respect input/output interfaces1. (I1 O1 H1 ) H2 = ,2. (I2 O2 H2 ) H1 = ,3. O1 O2 = ,4. Def R1 (O1 ) = Def R1 R2 (O1 ),5. Def R2 (O2 ) = Def R1 R2 (O2 ).rst three conditions due Gaifman Shapiro (1989)imply sets O1 , H1 , O2 , H1 mutually pairwise distinct. Violationsrespect rst two conditions circumvented renaming strategy.instance, atom H1 appears I2 O2 H2 , hence violating second condition,possible replace occurrences 1 new atom 6 I2 O2 H2appearing 2 . removes conict respect forth.5hand, last two conditions Denition 2.2 concern distributionrules involved definitions (2) sets atoms O1 O2 , i.e., sets rulesDef R1 (O1 ) Def R2 (O2 ), R1 R2 , respectively. regards disjunctive rules,principle sets dening rules must remain intact union R1 R2formed means module supposed copies rules formdenition output atoms. spite this, two modules 1 2 subjectconditions Denition 2.2 may eectively share disjunctive rules B, Cnon-empty head O1 6= O2 6= , demonstrated next.4. particular, input atoms head rule act much like atoms negative body C.5. opposite view program composition considered Section 6, possibilities decomposing disjunctive program smaller DLP-functions studied. counterpart renaming,revealing operator introduced Definition 7.3 used circumventing first two conditionsDefinition 2.2.818fiModularity Aspects Disjunctive Stable ModelsExample 2.3 Consider following two DLP-functions:6{b}b c;a,{a, c}{a}b c;e a, e{b, c}formally, 1 = hR1 , {a, c}, {b}, {d}i 2 = hR2 , {b, c}, {a}, {e}iR1 R2 = {a b c}. show 1 2 respect input/output interfacesother: First, hidden atoms e occur exactly one two programsthus first two conditions Definition 2.2 satisfied. Second, disjoint outputatoms, viz. atom b 1 atom 2 . Finally, Def R1 ({b}) = Def R1 R2 ({b}) =Def R2 ({a}) = Def R1 R2 ({a}) = {a b c}, shows also final two conditionsDefinition 2.2 satisfied, far syntax concerned, makes sense composelarger DLP-function obtained kind union 1 2 ; see (4) below.contrast disjunctive programs, shared rules arise context normallogic programs since one head atom allowed rule. statedsmodels programs (Simons, Niemel, & Soininen, 2002) although programsmay contain, among rule types, choice rules form{a1 , . . . , } B, C(3)heads cardinality greater one. observed Oikarinen Janhunen (2008a),heads choice rules possessing multiple atoms freely split without aectingsemantics. splitting rules n dierent rules {ai } B, C1 n, concern creation n copies rule body B, C couldreserve quadratic space worst case. new atom introduced circumventthis. nature proper disjunctive rules (1), subject study paper,somewhat dierent. Unlike choice rules, disjunctive rules may interact rule heads.Example 2.3, denition depends b vice versa. However, given choice rule{a, b} c instance, choices regarding b independent other: ctrue, atoms truth value. quite dierent interpretationab c makes either b true given c true. grasp interactionb natural b input denition and, conversely, inputb. demonstrated Section 7, shared rules rewritten input atomsremoved rule head drawback rewriting technique, compactnessrepresentation partly lost. Therefore, appreciate extra exibility providedshared rules interpret reect true nature disjunctive rules.general, DLP-functions composed according following principle:6. henceforth make use tabular format represent DLP-functions: output signaturegiven top, input signature bottom, rules listed between. Thus,declaration hidden signature remains implicit.819fiJanhunen, Oikarinen, Tompits & WoltranO2LI1H1H2I2=O1I1 O2O2H2I1I1 I2I2H1O1O1 I2Figure 1: Treatment signatures composition operator .Definition 2.4 (Composition) Let 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2two DLP-functions respect input/output interfaces other. Then, composition 1 2 defined determined1 2 = hR1 R2 , (I1 \ O2 ) (I2 \ O1 ), O1 O2 , H1 H2 i.(4)treatment atom types Denitions 2.2 2.4 summarized Figure 1.two symmetric gures left-hand side illustrate signatures DLP-functions1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 subject composition. Input signaturesoutput signatures emphasized light gray dark gray shadings, respectively.superposition two gures yields diagram given right representsresulting nine categories atoms. three may involve shared atomsoriginate 1 2 . interface conditions introduced intuitivereaders acquainted principles object-oriented programming:1. Although 1 2 must share hidden atoms, may share input atoms, i.e.,I1 I2 6= allowed. Output atoms treated dierently O1 O2 = assumed.2. input atom 1 becomes output atom 1 2 appears outputatom 2 , i.e., 2 provides input 1 setting. input atoms 2treated symmetric fashion.3. hidden atoms 1 2 retain status 1 2 .Example 2.5 Recall Example 2.3 showed DLP-functions 1 2 respectinput/output interfaces other. Thus, composition 1 2 defined,1 2 hR1 R2 , I, O, Hi set input atoms ({a, c} \ {a}) ({b, c} \ {b}) ={c}, set output atoms {a} {b} = {a, b}, set H hidden atoms{d} {e} = {d, e}, i.e., using tabular format represent modules,{b}b c;a,{a, c}{a}b c;e a, e{b, c}={a, b}b c;a, d;e a, e{c}definitions b 1 2 share rule b c. Thanks flexibilityDefinition 2.4, also able split 1 2 components whenever appropriate.820fiModularity Aspects Disjunctive Stable ModelsFollowing previous approaches (Gelfond & Gabaldon, 1999; Oikarinen & Janhunen,2008a), dene signature At() DLP-function = hR, I, O, Hi H.7notational convenience, distinguish visible hidden parts At() settingAtv () = Ath () = H = At() \ Atv (), respectively. Moreover, Ati ()Ato () used refer sets input output atoms , respectively.notations provide us way access module interface left implicit, e.g.,neglect internal structure modules. Lastly, set At() atoms, denote projections Ati (), Ato (), Atv (), Ath () Si , , Sv , Sh ,respectively.formal terms, DLP-function = hR, I, O, Hi designed provide mappingsubsets set subsets H analogy LP-functions formalizedGelfond Gabaldon (1999). However, exact denition mapping deferredSection 3 semantics DLP-functions anchored. sequel,(syntactic) class DLP-functions denoted D. assumed, sake simplicity,spans xed (at denumerable) signature At(D)8 At() At(D)holds DLP-function D. Given DLP-functions 1 , 2 , 3 pairwiserespect input/output interfaces other, holds1 2 (closure),1 = 1 = 1 , empty DLP-function = h, , , (identity),1 2 = 2 1 (commutativity),1 (2 3 ) = (1 2 ) 3 (associativity).theory modules put forth Oikarinen Janhunen (2008a) basedrestrictive operator program composition, viz. join . idea behind operatorforbid positive dependencies programs explicated next.Technically speaking, dene positive dependency graph DG+ () DLP-function= hR, I, O, Hi using positive dependenciesfollowing denition Ben-EliyahuDechter (1994). However, exclude input atoms graph denitionsexternal anyway. Thus, let DG+ () = hO H, 1 b 1 holdspair atoms a, b H rule B, C Rb B. reexive transitive closure 1 gives rise dependency relationAto () Ath (). strongly connected component (SCC) graph DG+ ()maximal set Ato () Ath () b every pair a, b atoms. Given1 2 dened, say 1 2 mutually dependent DG+ (1 2 )SCC Ato (1 ) 6= Ato (2 ) 6= (Oikarinen & Janhunen,2008a), i.e., component shared DLP-functions 1 2 way. 12 mutually dependent, also call mutually independent.Definition 2.6 (Joins) Given two DLP-functions 1 2 , composition 1 2defined 1 2 mutually independent, join, 1 2 , 1 2defined coincides 1 2 .7. Consequently, length symbols, denoted kk, gives upper bound |At()|important one considers computational cost translating programs (Janhunen, 2006).8. practice, set could set identifiers (names propositions similar objects).821fiJanhunen, Oikarinen, Tompits & Woltrancase 1 2 dened, thus 1 2 mutually independent, exactlyone following conditions holds SCC DG+ (1 2 ):Ato (1 ) Ath (1 );(5)Ato (2 ) Ath (2 ).(6)Example 2.7 Recall programs 1 2 Example 2.5 obtainpositive dependency graph DG+ (1 2 ) = h{a, b, d, e}, {ha, di, ha, ei}i. Hence, SCCsgraph simply singletons {a}, {b}, {d}, {e}. Together observationAto (1 ) Ato (2 ) disjoint, derive 1 2 mutually dependent.Thus, join 1 2 = 1 2 defined since composition 1 2 definedbasis analysis performed Example 2.5.Example 2.8 example two DLP-functions composition definedyet ineligible join, consider following situation:{b}b c;b a, c{a, c}{a}b c;b, c{b, c}={a, b}b c;b, c;b a, c{c}Here, result composition involves SCC = {a, b} respective positive dependency graph, non-empty intersection output signatures programssubject composition. Hence, respective join modules question defined.3. Model Theory Stable-Model Semanticssyntax DLP-functions dened, turn semantics. proceedthree steps introduce, correspondingly, three kinds models, viz. classical models,minimal models, and, nally, stable models DLP-function. last provideintended semantics DLP-function whereas rst two serve auxiliary concepts.usual, interpretation DLP-function dened arbitrary subsetAt(). Given particular interpretation At(), atom At() true ,denoted |= a, , otherwise false , denoted 6|= a. negativeliteral a, dene |= 6|= a. set L literals satisfied , denotedWby|= L, |= l, everyWliteral l L. also dene disjunctive interpretation Lset L literals: |= L |= l literal l L.begin with, cover DLP-functions pure classical semantics, treatsdisjunctive rules classical implications. emphasized classical modelsDLP-function specic interpretations dened hence subsets At().Definition 3.1 interpretation At() (classical) model DLP-function= hR, I, O, Hi, denoted |= , iff |= R, i.e., every rule B, C R,W|= B C implies |= A.822fiModularity Aspects Disjunctive Stable Modelsset classical models denoted CM().Classical models provide appropriate level abstraction address role inputatoms DLP-functions. Given DLP-function interpretation At(),projection Mi viewed actual input may (or may not) producerespective output Mo , depending semantics assigned . treatment inputatoms sequel based partial evaluation: idea pre-interpret inputatoms appearing respect Mi .Definition 3.2 DLP-function = hR, I, O, Hi actual input Mi ,instantiation respect Mi , denoted /Mi , quadruple hR , , O, HiR contains reduced rule(A \ I) (B \ I), (C \ I)(7)rule B, C R Mi |= Ai Bi Ci .Example 3.3 Consider following DLP-function :{a, b}b c;c, b;b c,{c}actual input {c} Ati (), reduct /{c} DLP-functionh{a b; b a}, , {a, b}, i.hand, actual input Ati (), obtain reduct/ = h{a b}, , {a, b}, i.rules form (7) free input atoms indicates reduct /MiDLP-function without input. Atoms Ato () Ath () aected /Mi .Proposition 3.4 Let DLP-function At() interpretation definesactual input Mi Ati () . interpretations N At() Ni = Mi ,N |= Nh |= /Mi .Proof. Consider N At() Ni = Mi .(=) Suppose N |= . Assume Nh satisfy (7) ruleB, C . follows Mi |= Ai Bi Ci , therefore Ni |= Ai Bi Ci .Thus, N 6|= B, C, contradiction. follows Nh |= /Mi .(=) Let Nh |=W/Mi hold. Assuming N 6|= B, C rule impliesN |= B C N 6|= A. follows Ni |= Ai Bi Ci correspondingrule (7) included /Mi Ni = Mi . rule satised Nh since823fiJanhunen, Oikarinen, Tompits & WoltranWN 6|= B, C implies Nh |= (B \ I) (C \ I) Nh 6|= (A \ I),contradiction. Hence, N |= .Thus, input reduction, given Denition 3.2, fully compatible classicalsemantics characterize semantic operator CM also terms equation[{Mi N | N CM(/Mi )}.(8)CM() =Mi Ati ()Recall models DLP-function subsets At(). Hence,N CM(/Mi ) subset At(/Mi ) thus Mi N = Mi Ati ()since atom Ati () occurs /Mi denition.Handling input atoms slightly complicated case minimal modelsprimitives parallel circumscription (Lifschitz, 1985; McCarthy, 1986) provide usstraightforward way address them. rough idea keep interpretation inputatoms fixed minimizing (i.e., falsifying) others far possible.Definition 3.5 Let = hR, I, O, Hi DLP-function. model At()I-minimal iff model N Ni = Mi N .sequel, set I-minimal models = hR, I, O, Hi denoted MM()treat input atoms stipulating I-minimality models. Using idea, Proposition 3.4lifts minimal models given fact Ati (/Mi ) = .Proposition 3.6 Let DLP-function At() interpretation definesactual input Mi Ati () . interpretations N At() Ni = Mi ,N MM() Nh MM(/Mi ).Proof. Consider N At() Ni = Mi .(=) Let N MM(). follows Proposition 3.4 Nh |= /Mi . AssumeNh/ MM(/Mi ). Recall Ati (/Mi ) = . Thus, interpretationNh |= /Mi . follows Proposition 3.4 N |=interpretation N = Mi S. Ni = Ni N N jointly contradict N MM().(=) Suppose Nh MM(/Mi ). So, Nh |= /Mi , N |= followsProposition 3.4. Let us assume N 6 MM(), i.e., model N |=Ni = Ni N N . Thus, (No Nh ) (No Nh ), since Ni = Ni = Mifollows Nh |= /Mi Proposition 3.4. Then, however, Nh |= /Micontradiction Nh MM(/Mi ).set MM() Ati ()-minimal models sucient determine semanticspositive DLP-function , i.e., whose rules form B. Recallrules \ Ati () 6= holds whenever 6= . order cover arbitrary DLP-functions,interpret negative body literals way proposed Gelfond Lifschitz (1991).Definition 3.7 Given DLP-function = hR, I, O, Hi interpretation At(),reduct respect positive DLP-function = hRM , I, O, HiRM = {A B | B, C R |= C}.824(9)fiModularity Aspects Disjunctive Stable ModelsDefinition 3.8 interpretation At() stable model DLP-functioninput signature Ati () iff MM(M ), i.e., Ati ()-minimal model .Hidden atoms play special role Denition 3.8. contrast this, aectpossibilities program decomposition, presented Section 6, statusnally explicated notion modular equivalence introduced Section 8.Denition 3.8 covers also case ordinary disjunctive logic program, simplyDLP-function = hR, , O, i: model At() = stableminimal model RM . denition stable models gives rise semantic operatorAt(D)SM : 22DLP-functions:SM() = {M At() | MM(M )}.(10)Proposition 3.6 provides us way dismiss Ati ()-minimality denition stablemodels desirable. Given stable model , projection N = Mo Mh minimalmodel (/Mi )N hence stable model /Mi . words,(/Mi )M = (/Mi )Mo Mh = /Mi .Thus, derive following result:Corollary 3.9 DLP-function ,SM() = {M At() | Mo Mh SM(/Mi )}.Example 3.10 Recall DLP-function Example 3.3, hidden atoms,given follows:{a, b}b c;c, b;b c,{c}four stable models total: M1 = {a}, M2 = {b}, M3 = {a, c}, M4 = {b, c},{c}-minimal models respective reducts :M1M2M3M4= h{a b ; c}, {c}, {a, b}, i,= h{a b ; b c}, {c}, {a, b}, i,= h{a c}, {c}, {a, b}, i,= h{b c}, {c}, {a, b}, i.Now, easy verify Mj {c}-minimal model reduct Mj .illustrating Corollary 3.9, recall reducts/{c} = h{a b; b a}, , {a, b},/ = h{a b}, , {a, b}, i.Then, SM(/{c}) = {{a}, {b}} SM(/) = {{a}, {b}}.825fiJanhunen, Oikarinen, Tompits & Woltranimmediate observation loose general antichain property stablemodels input signatures introduced. instance, M1 M3 M2 M4Example 3.10. However, since interpretation input atoms xed semantics,perceive antichains locally, i.e., set {N SM() | Ni = Mi } stable models formsantichain, input Mi Ati (). Example 3.10, sets stable models associatedactual inputs {c} {M1 , M2 } {M3 , M4 }, respectively.4. Characterizations using Classical Logicwell known set stable models ordinary disjunctive logic program, i.e.,DLP-function form hR, , O, i, characterized via classical propositional logic,using concepts completion (Clark, 1978) loop formulas (Lin & Zhao, 2004; Lee &Lifschitz, 2003). section, generalize concepts arbitrary DLP-functions.end, main concern role input atoms incorporateconcepts. Furthermore, extend tightness property programs (Erdem & Lifschitz,2003) DLP-functions introducing notion I-tightness Section 4.2.4.1 Program Completion Loop FormulasGiven DLP-function , loop non-empty subset strongly connectedcomponent positive dependency graph DG+ (). Recall DG+ ()atoms Ato () Ath () nodes. particular, singleton {a} Ato ()Ath () thus loop.Example 4.1 Consider DLP-functions 1 2 defined follows:1 :{b, c}c b;ba{a}2 :{a, b}c b;ba{c}Here, 1 singleton loops {b} {c}. particular, {a, b} loopcontains input atom a. hand, 2 loops {a}, {b}, {a, b}.follows,use,Wfor set propositionalW formulas (or atoms), denoteVconjunction sS shorthand sS s. Moreover, appearing withinformula, set implicitly understood conjunction elements. DLP-functionatom Ato () Ath (), dene set supporting formulasSuppF(a, ) = {B C (A \ {a}) | B, C A}loop L Ato () Ath () , set externally supporting formulasESuppF(L, ) = {B C (A \ L) | B, C , L 6= , B L = }.Clarks completion procedure (conjunctive) loop formulas generalized DLPfunctions following way:Definition 4.2 DLP-function , completion set formulas826fiModularity Aspects Disjunctive Stable ModelsWComp() = {B CW | B, C }{a SuppF(a, ) | Ato () Ath ()}set loop formulasWLF() = {L ESuppF(L, ) | L Ato () Ath () loop }.9Observe case Ati () = , i.e., Ato () Ath () = At(), completionComp() reduces denition provided Lee Lifschitz (2003) holdsset LF() loop formulas. Generally speaking, propositional theories Comp()LF() characterize set SM() stable models following sense:Theorem 4.3 DLP-function interpretation At(),SM() |= Comp() |= LF().Proof. rst relate sets SuppF(a, ) ESuppF(L, ), introducedDLP-function , respective sets complementary rulesSuppCR(a, ) = {A \ {a} B, C | B, C A}ESuppCR(L, ) = {A \ L B, C | B, C , L 6= , B L = }.First, straightforward that, interpretation At(), |= Comp()jointly |= (Ato () Ath ()), 6|= SuppCR(a, ). Quitesimilarly, holds |= LF() i, loop L (Ato () Ath ()) ,6|= ESuppCR(L, ). hand, viewing SuppCR(a, ) ESuppCR(L, )DLP-functions signatures , apply Proposition 3.4 orderevaluate input atoms. Thus, obtain following relationships DLP-function, interpretation At(), atom Ato () Ath (), loop L Ato () Ath ():1. |= Mo Mh |= /Mi ,2. |= SuppCR(a, ) Mo Mh |= SuppCR(a, /Mi ),3. |= ESuppCR(L, ) Mo Mh |= ESuppCR(L, /Mi ).Finally, recall interpretation At(), Ato () = Ato (/Mi )Ath () = Ath (/Mi ). Inspecting denition Comp() LF() again,conclude interpretation At() |= Comp() LF() Mo Mh |=Comp(/Mi ) LF(/Mi ). turn, know Mo Mh |= Comp(/Mi ) LF(/Mi )Mo Mh stable model program /Mi results Lee Lifschitz (2003);recall /Mi ordinary disjunctive program without input atoms. Finally,SM() = {M At() | Mo Mh SM(/Mi )} Corollary 3.9. equality showsclaim.Example 4.4 Let us demonstrate functioning program completion loop formulas DLP-functions Example 4.1, i.e., 1 = hR, {a}, {b, c}, 2 =hR, {c}, {a, b}, i, R = {a c b; b a}. completions9. Although may seem case singleton loop L = {a} somewhat redundant, so,since tautological rules b make difference.827fiJanhunen, Oikarinen, Tompits & WoltranComp(1 ) = {b c, b} {b a, c b a}Comp(2 ) = {b c, b} {b a, b c}.Furthermore, sets loop formulasWWLF(1 ) = {b ESuppF({b}, 1 ), c ESuppF({c}, 1 )}= {b a,W c b a}WLF(2 ) = {b ESuppF({b},2 ), ESuppF({a}, 2 ),Wb ESuppF({a, b}, 2 )}= {b a, b c, b }.last formula, Woccurrence view ESuppF({a, b}, 2 ) = ,yields empty disjunction ESuppF({a, b}, 2 ) = usual.Computing classical models Comp(1 ) LF(1 ) = Comp(1 ) yields twomodels, M1 = {a, b} M2 = . One check indeed stable models1 recalling Ati (1 ) = {a}. Thus, M1 relates actual input M1 Ati (1 ) = {a}whereas M2 based M2 Ati (1 ) = . hand, classical modelsComp(2 ) LF(2 ) M1 = {c} M2 = , relate two possibleinputs Ati (2 ) = {c}. Finally, note {a, b} also model Comp(2 )ruled LF(2 ).4.2 Tight DLP-functionsextend well-known concept tightness (Erdem & Lifschitz, 2003) DLPfunctions. interest since exploit fact positive dependencygraph DG+ () reduced modulo input atoms. words, since dependency graphDG+ () atoms Ato () Ath () nodes, tightness DLP-functionsdened respect input signature.beginning Section 4, loops dened arbitrary non-empty subsetsstrongly connected components DG+ (). Thus, DG+ () acyclicsingleton loops. However, converse necessarily true, since, programsingleton loops, DG+ () may edges ha, ai, i.e., cycles length one.Definition 4.5 DLP-function Ati ()-tight (or tight, short), positive dependency graph DG+ () acyclic.Example 4.6 Recall DLP-functions 1 = hR, {a}, {b, c}, 2 = hR, {c}, {a, b},based R = {a c b; b a} Example 4.1. Here, 1 {a}-tight since potentialnon-singleton loop {a, b} contains input atom a. hand, 2 {c}-tight.worth mentioning ordinary variant 1 , viz. DLP-function hR, , {a, b, c}, i,-tightin particular, since R tight usual sense.note last observation, viz. DLP-function hR, I, O, Hi may I-tightalthough R tight program, relies use disjunctions program. fact,DLP-functions hR, I, O, Hi, R set normal rules form B, C,DLP-function = hR, I, O, Hi I-tight R tight. verify this, notesecond item Denition 2.1 implies head atom normal rule B, Cmust appear I, thus loop may involve atoms I.828fiModularity Aspects Disjunctive Stable Modelsshow notion tightness introduced Denition 4.5 enables uscharacterize stable models DLP-function classical models completion.Since ordinary program represented DLP-function, thus properlygeneralize well-known completion semantics (Clark, 1978). following lemmaalready sucient result view Denition 4.2 Theorem 4.3.Lemma 4.7 tight DLP-function , LF() Comp().WProof. Recall Ato () Ath (), SuppF(a, ) containedComp(). Moreover,Wsince tight, singleton loops, thus LF() containsformulas ESuppF({a}, ), Ato () Ath (). remainsshow that, atom a, SuppF(a, ) equivalent ESuppF({a}, ) wheneverpositive dependency graph DG+ () acyclic. repeat denition SuppF(a, )give denition ESuppF(L, ), simplied case L = {a}:SuppF(a, ) = {B C (A \ {a}) | B, C A};ESuppF({a}, ) = {B C (A \ {a}) | B, C , A, B {a} = }.easy see acyclic dependency graph DG+ (), implies B{a} =every rule B, C . Thus, conclude SuppF(a, ) = ESuppF({a}, )holds Ato () Ath (). Hence, claim follows.Example 4.8 Recalling DLP-function 1 = hR, {a}, {b, c}, Example 4.4R = {a c b; b a}, obtainComp(1 ) = {bW c, b} {b a, cW b a}LF(1 ) = {b ESuppF({b}, 1 ), c ESuppF({c}, 1 )}= {b a, c b a}.Now, 1 tight observe LF(1 ) Comp(1 ) expected.observations presented far lead us following result:Theorem 4.9 tight DLP-function interpretation At(),SM() |= Comp().particular, result compatible existing characterization stable modelscase Ati () = , i.e., Ato () Ath () = At(). Then, notion Ati ()tightness coincides ordinary tightness, denition completion Comp()reduces one provided Lee Lifschitz (2003).5. Compositional Semanticsfollows, objective establish main result paper, i.e., showstable-model semantics, given Denition 3.8, fully compositional largerDLP-functions formed joins 1 . . . n DLP-functions. precisely,interconnection SM() SM(1 ), . . . , SM(n ) explicated Section 5.1. analogy829fiJanhunen, Oikarinen, Tompits & WoltranSection 3, follow quite rigorous approach consider relationship classicalmodels rst, minimal models, eventually cover case stable modelscomprises module theorem. Then, Section 5.2, use quantied Boolean formulassecond level polynomial hierarchy modular representation termsDLP-functions illustrate module theorem. Finally, devote Section 5.3comparison splitting set theorem proven Lifschitz Turner (1994).5.1 Module Theorembegin with, formalize criteria combining interpretations well models.Definition 5.1 Given two DLP-functions 1 2 , interpretations M1 At(1 )M2 At(2 ) mutually compatible (with respect 1 2 ), compatible,M1 Atv (2 ) = M2 Atv (1 ).(11)According (11), two compatible interpretations M1 M2 1 2 , respectively, agree truth values joint visible atoms Atv (1 ) Atv (2 ).quick inspection Figure 1 reveals three cases may arise join = 1 2dened joint output atoms 1 2 thereafter disallowed: may exist1. joint input atoms Ati () = Ati (1 ) Ati (2 ),2. atoms Ato (1 ) Ati (2 ) output atoms 1 input atoms 2 ,3. symmetry, atoms Ati (1 ) Ato (2 ).Recall according Denition 2.6, atoms last two categories end Ato ()= 1 2 formed. Atoms Atv (1 ) Atv (2 ) provide basis combinecompatible interpretations 1 2 .Definition 5.2 Let 1 2 two DLP-functions = 1 2 defined.Given sets interpretations A1 2At(1 ) A2 2At(2 ) , natural join A1A2 respect Atv (1 ) Atv (2 ), denoted A1A2 , set interpretationsA1A2 = {M1 M2 | M1 A1 , M2 A2 , M1 M2 compatible}.(12)rst modularity result formulated DLP-functions classical semanticsdened Section 3. combination classical models understood (12).Proposition 5.3 positive DLP-functions 1 2 1 2 defined,CM(1 2 ) = CM(1 )CM(2 ).(13)Proof. Consider interpretation At(1 2 ) projections M1 = At(1 )M2 = At(2 ) respect 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 i.follows M1 M2 compatible = M1 M2830fiModularity Aspects Disjunctive Stable ModelsCM(1 2 )|= R1 R2M1 |= R1 M2 |= R2M1 CM(1 ) M2 CM(2 )CM(1 )CM(2 ).Generalizing Proposition 5.3 stable models DLP-functions much elaborate.cover case positive DLP-functions minimal models rst. proofTheorem 5.5 exploits program completion, loop formulas, well characterizationstable minimal models Section 4 follows:Lemma 5.4 DLP-functions 1 2 1 2 defined, followingconditions hold:Comp(1 2 ) = Comp(1 ) Comp(2 );LF(1 2 ) = LF(1 ) LF(2 ).(14)(15)Proof. begin proof analyzing formulas introduced Clarks completionloop formulas related joins DLP-functions. end, establishsets formulas associated 1 2 directly obtained unions setsformulas associatedWwith 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 i: First,implication B C belongs Comp(1 2 ) belongs Comp(1 ),Comp(2 ), case shared rule. Second, let us consider atom H,= O1 O2 H = H1 H2 disjoint 1 2 dened.reason, either O1 H1 O2 H2 , i.e., atom dened either 1 2 . Thus,either Def R1 (a) = Def R1 R2 (a) Def R2 (a) = Def R1 R2 (a) Denition 2.2,implies either SuppF(a, 1 2 )W= SuppF(a, 1 ) SuppF(a, 1 2 ) = SuppF(a, 2 ).follows implication1 2 )W SuppF(a, 1 2 ) member Comp(Weither (i) SuppF(a, 1 ) belongs Comp(1 ) (ii) SuppF(a, 2 )belongs Comp(2 ). Thus, may conclude (14) completions involved.Third, recall loop L At(1 2 ) 1 2 contained SCC1 2 . follows (5), (6), Denition 2.2 either1. L O1 H1 loop 1 Def R1 (L) = Def R1 R2 (L),2. L O2 H2 loop 2 Def R2 (L) = Def R1 R2 (L).cases above, either ESuppF(L, 1 2 ) = ESuppF(L,W1 ) ESuppF(L, 12 ) = ESuppF(L, 2 ). Thus, respective loop formula LESuppF(L, 1 2 )belongs LF(1 2 ) contained LF(1 ) LF(2 ).Theorem 5.5 positive DLP-functions 1 2 1 2 defined,MM(1 2 ) = MM(1 )MM(2 ).(16)Proof. Consider At(1 2 ) respective projections M1 = At(1 )M2 = At(2 ) compatible and, moreover, = M1 M2 . obtainfollowing chain equivalences:831fiJanhunen, Oikarinen, Tompits & WoltranMM(1 2 )|= Comp(1 2 ) |= LF(1 2 )M1 |= Comp(1 ) M1 |= LF(1 )M2 |= Comp(2 ) M2 |= LF(2 )M1 MM(1 ) M2 MM(2 )MM(1 )MM(2 ).[Theorem 4.3][(14) (15)][Theorem 4.3][Denition 5.2]Example 5.6 Let us demonstrate result Theorem 5.5 practical setting using DLPfunctions 1 2 visualized composition = hR, , {a, b, c, d, e}, i.1 :{a, b, c}b ;b;b a;c;c e a, b{d, e}2 :{d, e}c;e d;e;c e a, b{a, b, c}join 1 2 defined SCCs composition S1 = {a, b, c}S2 = {d, e}. Ati (1 )-minimal models 1 {a, b, c}, {a, b, d}, {a, b, e},{a, b, d, e}. Likewise, calculating MM(2 ), getMM(2 ) = {, {a}, {b}, {c, d, e}, {a, b, d, e}, {a, c, d, e}, {b, c, d, e}, {a, b, c, d, e}}.Hence, minimal model = {a, b, d, e} compatibility conditionunderlying (16) correctly excludes N = {a, b, c, d, e} 6 MM(). Note supportc true 1 e true. Accordingly, c e a, b active.prepared present central result:Theorem 5.7 (Module Theorem) DLP-functions 1 2 1 2defined,SM(1 2 ) = SM(1 )SM(2 ).(17)Proof. Again, take interpretation At(1 2 ) respective compatibleprojections M1 = At(1 ) M2 = At(2 ) consideration. proof (17)based (16) number preliminary facts established:M211. compositiondened.1 2Since 1 2 dened, know 1 2 dened. indicates 112 respect input/output interfaces other. construction1M2M1M22 aect property implies 1 2 dened.M21dened.2. join1 2M21preceding item, positive dependency graph DG+ (M1 2 ) dened.M1M2Let us assume 1 2 mutually dependent, i.e., SCCM21graph Ato (M1 ) 6= Ato (2 ) 6= . Sincedependency graph potentially fewer dependencies respective graph832fiModularity Aspects Disjunctive Stable ModelsDG+ (1 2 ) 1 2 , follows contained SCC latter.M21Since Ato (M1 ) = Ato (1 ) Ato (2 ) = Ato (2 ), obtain Ato (1 ) 6=Ato (2 ) 6= . Thus, 1 2 mutually dependent, contradiction.M213. reduct (1 2 )M coincides1 2 .rule B belongs (1 2 )M rule B, C 1 ,2 , C = . Equivalently, rule B, C 1C M1 = , rule B, C 2 C M2 = , i.e.,12BB12 .therefore get following chain equivalences:SM(1 2 )MM((1 2 )M )M21MM(M1 2 )12MM(MMM(M1 )2 )M21M1 MM(M1 ) M2 MM(2 )M1 SM(1 ) M2 SM(2 )SM(1 )SM(2 ).[Denition 3.8][Item 3 above][Theorem 5.5][Denition 5.2][Denition 3.8][Denition 5.2]moral Theorem 5.7 Denition 2.6 stable semantics supports modularization long positively interdependent atoms enforced module.Example 5.8 Let 1 2 DLP-functions defined = 1 2join (which clearly defined):{b}b ;bc{a, c}{c}c ;bc{a, b}={b, c}b ;c ;bc{a}straightforward verify SM(1 ) = {{b}, {a, b}, {a, c}, {b, c}} SM(2 ) ={{c}, {a, b}, {a, c}, {b, c}}. Since Atv (1 ) Atv (2 ) = {a, b, c}, obtainSM(1 )SM(2 ) = SM(1 ) SM(2 ) = {{a, b}, {a, c}, {b, c}}.simple cross-check confirms SM() indeed given set.Example 5.9 Consider DLP-functions 1 2 Example 2.8. Then, SM(1 ) ={, {a, b}, {b, c}} SM(2 ) = {, {a, b}, {a, c}}. shown Example 2.8, join 12 undefined. Thus, Theorem 5.7 applicable. Concerning composition1 2 , note SM(1 2 ) = {, {a, c}, {b, c}} =6 {, {a, b}} = SM(1 )SM(2 ).Theorem 5.7 easily extended DLP-functions consisting twomodules. view this, say nite sequence M1 , . . . , Mn stable modelsmodules 1 , . . . , n , respectively, compatible, Mi Mj pairwise compatible,1i, j n. property guarantees Mi recovered union= ni=1 Mi taking respective projection At(i ) = Mi .833fiJanhunen, Oikarinen, Tompits & WoltranCorollary 5.10 Let 1 , . . . , n sequence DLP-functions join 1n defined. Then,SM(1 n ) = SM(1 )SM(n ).(18)Example 5.11 following example simply extends Example 5.8:{b}b ;bc{a, c}{c}c ;bc{a, b}{a}b ;ac{b, c}={a, b, c}b ;c ;bcSM(1 ) = {{b}, {a, b}, {a, c}, {b, c}}, SM(2 ) = {{c}, {a, b}, {a, c}, {b, c}},SM(3 ) = {{a}, {a, b}, {a, c}, {b, c}}. Thus, learn Corollary 5.10SM(1 2 3 ) = SM(1 )SM(2 )SM(3 ) = {{a, b}, {a, c}, {b, c}}.5.2 Modular Representation Quantified Boolean Formulasnext objective illustrate theory developed far terms extensiveunsat depictedexample. end, consider pair DLP-functions satn nFigure 2. purpose evaluation quantified Boolean formulas (QBFs)formn_XY(Ai Bi Ci Di ),(19)i=1Aj , Bj , Cj , Dj set Boolean variables, parameter n givesnumber disjuncts matrix Boolean formula inSdisjunctive normalform (DNF).10 Without loss generality, may assume X = ni=1 (Ai Bi ), =ni=1 (Ci Di ), X = hold sets X Boolean variables (19).important point general evaluation QBFs form (19) constitutes p2 -complete decision problem perfectly matches complexity checkingexistence stable models disjunctive program. Given completeness property,follows principle decision problem p2 turned QBF form(19), albeit direct representations obtained particular problem domains.respect, let us address three specic domains prior detailing generic approach.1. strategic companies domain identied Leone et al. (2006) one rstpractical domains involving decision problems second level polynomialtime hierarchy solved using ASP techniques. simplied encoding providedKoch, Leone, Pfeifer (2003) based two kinds disjunctive rules:strat(x1 ) strat(x2 ) strat(x3 ) strat(x4 ) prod(y, x1 , x2 , x3 , x4 ),(20)strat(x) ctrl(x, x1 , x2 , x3 , x4 ), strat(x1 ), strat(x2 ), strat(x3 ), strat(x4 ),(21)10. Also, recall shorthands =VsS=V834sSintroduced right Example 4.1.fiModularity Aspects Disjunctive Stable ModelsFunction satn :Function unsat:nX1 n x Ai : x, act(i);1 n x Bi : x act(i);1 n: Ai Bi , act(i){act(1), . . . , act(n)}Ci {u} Di , act(i);u;u u{act(1), . . . , act(n)}1 n::unsatFigure 2: DLP-functions satn n Wfor evaluation quantied Boolean formulaXY matrix = ni=1 (Ai Bi Ci Di ).predicates strat(x), prod(y, x1 , x2 , x3 , x4 ), ctrl(x, x1 , x2 , x3 , x4 ), respectively,denote company x strategic, product produced companies x1 , . . . , x4 ,company x controlled companies x1 , . . . , x4 . Obviously, instancespredicate strat arising rules forms (20) (21) create positive dependencies program . resulting SCCs used split programmodules 1 , . . . , n = 1 . . . n dened. Theorem 5.7, statusspecic company x decided using module denes strat(x)rather entire encoding .2. model-based diagnosis digital circuitry provides another interesting applicationarea. Quite recently, Oikarinen Janhunen (2008b) presented ecient encodingprioritized circumscription disjunctive program (and thus, special case,parallel circumscription well)enabling concise representation minimal diagnoses sense Reiter (1987). resulting disjunctive rules involve head-cycles(see Section 7 details) typically pre-empt polynomial-time translationcomputationally easier normal logic program. observation suggests completeness second level polynomial-time hierarchy although awareexact hardness result. correctness proof encoding exploits two modulesmodule theorem.3. Finally, let us mention Gebser, Schaub, Thiele, Usadel, Veber (2008b) identifyminimal inconsistent cores large biological networks disjunctive programs.decision problem question Dp -complete also indicates appropriatenessdisjunctive logic programs representation domain. Since Dp complete decision problem described independent combination NPcomplete decision problem P1 coNP-complete decision problem P2 , foreseerepresentation form join sat unsat , sat stable model P1succinct certicate, unsat unique stable model P2 succinct835fiJanhunen, Oikarinen, Tompits & Woltran{x1 , x2 }x1 act(1);x1 , act(1);x2 , act(2); x2 act(2);x1 act(3);x1 , act(3);x1 act(4);x2 , act(4);x2 x1 , act(4){act(1), act(2), act(3), act(4)}u y1 , y2 , act(1);u y2 y1 , act(2);u y1 y2 , act(3);u y1 y2 act(4);y1 u; y2 u; u u{act(1), act(2), act(3), act(4)}unsat .Figure 3: Particular instances sat4 4certicates. required DLPs worked via reductions propositional(un)satisability. particular, test unsatisability realized analogyunsatanalyzed below.ngeneral case, use Boolean variables propositional atoms interchangeablyorder describe validity problem (19) captured DLP-functionsunsat based explanatory approach JanhunenFigure 2. design satn net al. (2006), (19) equivalently viewed formula XY matrixconjunctive normal form (CNF). clause 11 Ai Bi Ci Di activewhenever Ai Bi false truth clause becomes dependent Ci Di ;put dually, Ai Bi true truth Ai Bi Ci Di depends Ci Di .validity formula XY captured follows: Given input interpretationMi {act(1), . . . , act(n)}, upper DLP-function satn Figure 2 tries explainactivation statuses clauses checking respective theory {Ai Bi |act(i) Mi } {Ai Bi | act(i) 6 Mi } satisable. lower DLP-function, unsat, playsnrole coNP-oracle: captures test theory {Ci Di | act(i) Mi }unsatisable. correctness representation provided DLP-functionsaddressed soon, enough understand syntax intuitive meaningmoment. concrete QBF instance evaluated follows.unsat Figure 2 case QBFExample 5.12 Consider DLP-functions satn nx1 x2 y1 y2 [(x1 y1 y2 )(x2 y1 y2 )(x1 y1 y2 )(x1 x2 y1 y2 )]. (22)Thus, parameter instance n = 4, input signature {act(1), . . . , act(4)}unsat , illustrated Figure 3. output signature former DLPfor sat4 4function {x1 , x2 } atoms, i.e., y1 , y2 , u, remain hidden latter.joint input signature used specify active part matrix (22). DLPfunction satprovides explanation, i.e., assignment variables x1 x24output, whereas unsatresponsible respective unsatisfiability check. regards4validity QBF given (22), input interpretation {act(1), act(2), act(3), act(4)}yields positive answer. respective explanation, i.e., output interpretation foundsat4 , {x1 }. easy check x1 true x2 false remaindermatrix true whatever values assigned y1 y2 . Hence, QBF (22) valid.11. purposes section, interpret disjunctions B sets B = {b | b B}positive negative literals, respectively, disjunctions elements.836fiModularity Aspects Disjunctive Stable Modelsunsat Figure 2, identicalregards general DLP-functions satn nsatinput signatures, n output atoms, hidden atoms unsatfullynsatunsatrespected. Hence, composition n ndened. Moreover, atoms appearingrules involve positive dependencies belong disjoint sets X {u}.unsat ) cannot SCC X 6=therefore clear DG+ (satn nunsat dened regardless QBF (19)(Y {u}) 6= . implies satn nquestion. Let us exploit fact context specic DLP-functions Example 5.12.Example 5.13 four stable models DLP-function sat4 :{act(1), act(2), act(3), act(4), x1 }, {act(1), act(3), x1 , x2 }, {act(2)}, {x2 },listed decreasing level activation. hand, DLP-function unsat4unique stable model {act(1), act(2), act(3), act(4), y1 , y2 , u}, i.e., interpretation {y1 , y2 , u}/{act(1), act(2), act(3), act(4)} set rulesunique stable model unsat4given{ u y1 , y2 ; u y2 y1 ; u y1 y2 ; u y1 y2 ; y1 u; y2 u; u u },/Mi stable models input interpretation Mi . Moreover,unsat4unsat ) combining compatible pairsmay apply module theorem calculate SM(sat4 4models. one pair:{act(1), act(2), act(3), act(4), x1 } SM(sat4 ){act(1), act(2), act(3), act(4), y1 , y2 , u} SM(unsat).4Thus, {act(1), act(2), act(3), act(4), x1 , y1 , y2 , u} unique stable model join sat4sat unsat ) non-empty, conclude (22) indeed valid.unsat.SinceSM(444natural ask stated stable models general DLPfunctions unsatsatnn associated QBF XY given (19). stablesatmodel n , respective projection MX = X determines , i.e., holds1 n matrix act(i) MX |= Ai Bi . Moreover, modelMX minimal sense strictly smaller interpretation N MXproperty. additional feature brought along minimality stable models.consequence, DLP-function satn capture possible truth assignmentsvariables X relevant truth assignments lost. hand, stableindicates respective theorymodel unsatn{Ci Di | 1 n, act(i) }Winconsistent, alternatively, formula 1in,act(i)M Ci Di valid.Concerning correctness representation given Figure 2, due existingproof Janhunen et al. (2006), present main stepsfully exploiting benetsmodular approach.unsat ) non-empty.Theorem 5.14 QBF XY form (19) valid iff SM(satn nProof sketch. Consider QBF XY form (19). following equivalent:837fiJanhunen, Oikarinen, Tompits & Woltran1. formula XY valid.2. minimal interpretation N X that, set = {1 n |N 6|= Ai Bi } indices determined N N |= {Ai Bi | I}{Ai Bi |6 I}, theory {Ci Di | I} unsatisable.unsat compatible stable models = N {act(i) |3. DLP-functions sat1n nI} M2 = {act(i) | I} {u}, respectively.unsat stable model4. DLP-function satn n= M1 M2 = N {act(i) | I} {u}.second item, minimality N means N N {1 n |N 6|= Ai Bi } = I. assumed without loss generality.Theorem 5.14 module theorem suggest approximation strategy verifyingunsat ) empty, knowvalidity QBFs form (19). either SM(satn ) SM(nunsat ) = .directly formula valid. Otherwise, check whether SM(satn n5.3 Splitting Setssake comparison, formulate splitting-set theorem (Lifschitz & Turner,1994) DLP-function = hR, , O, i, essentially forms ordinary disjunctiveprogram. Splitting sets sets atoms closed following sense:Definition 5.15 Given DLP-function = hR, , O, i, set U atoms splitting set if, every rule B, C R,U 6= implies B C U .Denitions 2.1 5.15, sets always splitting sets . However,one mostly interested non-trivial splitting sets U , setsneed exist. Nevertheless, splitting set U divides respective set rules R twoparts. bottom, bU (R), R respect U contains rules B, C RB C U , whereas top, tU (R), R R \ bU (R). splitting RbU (R) tU (R) becomes proper one, i.e., bU (R) 6= tU (R) 6= ,1. U non-trivial2. every atom least one dening rule B, C R A.According Lifschitz Turner (1994), solution R respect U pairhX, X U , \U , X SM(bU (R)), SM(tU (R)/X). Here, tU (R)/Xdenotes partial evaluation tU (R) sense Denition 3.2 using X Uinput interpretation. Using similar idea, let us introduce DLP-functions correspondingbU (R) tU (R). Given splitting set U , join = B ,B = hbU (R), , U, = htU (R), U, \ U,dened. Then, following result implied Theorem 5.7.838fiModularity Aspects Disjunctive Stable ModelsCorollary 5.16 (Splitting-Set Theorem Lifschitz & Turner, 1994) everyDLP-function = hR, , O, corresponding set R disjunctive rules, every splittingset U , every interpretation At() = O, following conditionsequivalent:1. stable model .2. U SM(B ) SM(T ).3. hM U , \ U solution R respect U .fact, Theorem 5.7 strictly stronger splitting-set theorem. previouslydemonstrated Oikarinen Janhunen (2008a), splitting sets applicable DLPfunctions like = h{a b; b a}, , {a, b}, trivial way, i.e., U1 =U2 = {a, b} splitting sets . contrast, Theorem 5.7 applies precedingDLP-function versatile ways, i.e., 1 2 dened 1 = h{a b}, {b}, {a},2 = h{b a}, {a}, {b}, i. consequence 1 2 dened, possibledetermine sets stable models SM(1 ) = {{a}, {b}} = SM(2 ) separation,appropriate, conclude SM() = SM(1 )SM(2 ) = {{a}, {b}} holdswell. Yet another generality aspect splitting concerns role input atomstheyassumed nonexistent above. Theorem 5.7, however, enables us treat well.6. Decomposing DLP-Functionsobjectives section contrary construction DLP-function joinmodules. idea exploit strongly connected components DG+ (), DLPfunction , order decompose smaller components, e.g., prioriinformation internal structure . simplicity, rst consider DLPfunctions hidden atoms, i.e., Ath () = . eects hidden atomsdecomposition DLP-functions addressed thereafter. dened conjunctionDenition 2.6, SCCs DG+ () induced positive dependency relationreexive transitive, i.e., preorder denition. sequel, setSCCs DG+ () denoted SCC+ (). positive dependency relation liftselements SCC+ () follows: S1 S2 atoms a1 S1 a2 S2a1 a2 . end, matter pair atoms inspected.Lemma 6.1 DLP-function components S1 , S2 SCC+ (), S1 S2a1 a2 every a1 S1 a2 S2 .Proof. (=) S1 S2 , b1 S1 b2 S2 b1 b2 . Considera1 S1 a2 S2 . follows a1 b1 b2 a2 denition SCCs. Thus,a1 a2 transitive.(=) holds trivially SCCs non-empty.Proposition 6.2 relation SCC+ () reflexive, transitive, antisymmetric.839fiJanhunen, Oikarinen, Tompits & WoltranProof. relation SCC+ () reexive transitive denition. antisymmetry, consider S1 , S2 SCC+ () S1 S2 S2 S1 . followsLemma 6.1 that, every a1 S1 a2 S2 , a1 a2 a2 a1 . Thus, S1 = S2maximality components SCC+ ().+Consequently, may conclude hSCC (), partially ordered set. Sincenite denition, hSCC+ (), maxima minima elements needunique. particular, SCC+ () minimum element S1 SCC+ ()S1 S2 S1 implies S2 = S1 , S2 SCC+ (). Thus, mayapply principle well-founded induction using minima hSCC+ (), basis.Given structure hSCC+ (), i, DLP-function = hR, I, O, decomposed following way: set rules associated SCC+ () Def R (S)(2), i.e., set defining rules R. general, head arbitrary ruleB, C R may coincide sense (2) several SCCs, impliesrule included Def R (S) several SCC+ (). However, distributionrules perfect harmony last two conditions Denition 2.2. must alsobear mind integrity constraints B, C included Def R (S)SCC+ (). access integrity constraints set R rules, deneIC(R) = {A B, C R | = }.(23)ready present decomposition based SCC+ ().Definition 6.3 Given DLP-function = hR, I, O, i, decomposition inducedSCC+ () includes DLP-function0 = hIC(R), At(IC(R)) (I \ At(R)), ,(24)and, SCC+ (), DLP-function= hDef R (S), At(Def R (S)) \ S, S, i.(25)purpose extra module 0 keep track integrity constraints wellinput atoms mentioned rules R. modules involveddecomposition induced SCCs. refers modules usingAt(Def R (S)) \ input signature provides dening rules (if any) everyatom S. Recall output atom dening rules falsied default.Proposition 6.4 DLP-function = hR, I, O, decomposition basedSCC+ (), joinF(26)0 ( SSCC+ () )defined equal .Proof. Let us consider 0 SCC+ (). composition 0dened modules involve hidden atoms, Ato (0 ) = ,Def R1 () = = Def R1 R2 () Def R2 (S) = Def R (S) = Def R1 R2 (S) sets rulesR1 = IC(R) R2 = Def R (S). join 0 dened respective compositionintegrity constraints 0 create dependencies DG+ (0 ).840fiModularity Aspects Disjunctive Stable ModelsLet us perform similar analysis S1 S2 based two dierent componentsS1 , S2 SCC+ (). clear S1 S2 dened since modules involvehidden atoms, S1 S2 = , Def R1 (S1 ) = Def R (S1 ) = Def R1 R2 (S1 )Def R2 (S2 ) = Def R (S2 ) = Def R1 R2 (S2 ), R1 = Def R (S1 ) R2 = Def R (S2 ).Since pairwise joins dened, also overall join (26) dened. Denition 2.4denition SCC+ (), outcome equal1. IC(R) SSCC+ () Def R (S) = R,2. SSCC+ () = O,3. (At(IC(R)) \ O) ((I \ At(R)) \ O) SSCC+ () (At(Def R (S)) \ O) = I.Corollary 6.5 DLP-function Ath () = decomposition basedSCC+ (),SM() = SM(0 )( SSCC+ () SM(S )).Example 6.6 Consider following DLP-function :{a, b, c, d}b c ;a, c; b, c;a, d; b, d;b;c d;b a;c.So, Ati () = , Ato () = {a, b, c, d}, Ath () = . two SCCs DG+ (),viz. S1 = {a, b} S2 = {c, d}. resulting decomposition consists0 = h{ a, c; a, d; b, c; b, d}, {a, b, c, d}, , i,S1 = h{a b c ; b; b a}, {c, d}, {a, b}, i,S2 = h{a b c ; c d; c}, {a, b}, {c, d}, i.respective sets stable modelsSM(0 )SM(S1 )SM(S1 )SM()===={{a, b}, {c, d}, {a}, {b}, {c}, {d}, },{{a, b}, {c}, {d}, {c, d}},{{c, d}, {a}, {b}, {a, b}},{{a, b}, {c, d}}.Next, address case DLP-functions involving hidden atoms, i.e.,Ath () 6= holds. Then, components DG+ () subsets Ato () Ath ()revise (25) accordingly. DLP-function = hR, I, O, Hi SCC+ (),= hDef R (S), At(Def R (S)) \ S, O, Hi.(27)Unfortunately, decomposition based modules form (27) likely negrained. certain components S1 , S2 SCC+ () S1 6= S2 , respective841fiJanhunen, Oikarinen, Tompits & Woltranmodules S1 S2 conforming (27) might respect hidden atoms other.similar setting may arise 0 individual module based SCC+ ()integrity constraints refer hidden atoms . problem would disappearhidden atoms revealed hardly appropriatethere good reasonshide certain atoms knowledge representation perspective.way approach problem distinguish components S1 SCC+ ()S2 SCC+ () respective modules S1 S2 would respect hiddenatoms other, i.e., hidden atom dened one would referred othereitherpositively negatively. Similar conicts could also arise due integrity constraints packedmodule 0 distinguished Denition 6.3. rst sight, amalgamate 0module whose hidden atoms occur integrity constraints 0 . But,order avoid fusions kind far possible, worth redistributing integrityconstraints referring hidden atoms. clearly possible integrity constraints referring hidden atoms involved single component only. formalize ideas presentedfar, distinguish precise relation among components SCC+ () follows.Definition 6.7 Given DLP-function , components S1 , S2 SCC+ () respecthidden atoms other, denoted S1 !h S2 , S1 6= S21. hidden atom h Ath (S1 ) h Ati (S2 ),2. hidden atom h Ath (S2 ) h Ati (S1 ),3. hidden atoms h1 Ath (S1 ) h2 Ath (S2 ) occurrence integrity constraint B, C .clear !h irreexive symmetric components SCC+ ()DLP-function . Moreover, transitive closure !h , denoted !+h , gives rise+repartitioning SCC (). maximal block S1 , . . . , Sn componentsSi !+h Sj holds every 6= j induces module determined (27) union= S1 . . . Sn . key observation modules associated dierent blockscomponents respect hidden atoms makes Theorem 5.7 applicablelevel abstraction. summarize treatment DLP-functions involving hiddenatoms rules, revise Denition 6.3 accordingly.Definition 6.8 Given DLP-function = hR, I, O, Hi, decomposition inducedSCC+ () !+h includes DLP-function0 = hIC0 (R), At(IC0 (R)) (I \ At(R)), ,(28)IC0 (R) = { B, C R | (B C) H = } and, maximal block S1 , . . . , Sncomponents SCC+ () Si !+h Sj every 6= j, DLP-function= hDef R (S) ICS (R), At(Def R (S) ICS (R)) \ S, O, Hi(29)= S1 . . . Sn ICS (R) = { B, C R | (B C) (S H) 6= }.regards Example 6.6, Denitions 6.3 6.8 yield identical decompositionsDLP-function question. eects hiding demonstrated following example:842fiModularity Aspects Disjunctive Stable ModelsExample 6.9 Consider DLP-function = hR, , O, Hi,R = { a, c; b ; b c ; c d; c, b}H = {a, b, c, d}, exact partitioning atoms H varies casecase analyzed below. SCCs SCC+ () S1 = {a}, S2 = {b}, S3 = {c, d}.1. take atoms visible , i.e., H = , decomposition yields three modules, S1 = h{a b }, {b}, {a}, i, S2 = h{a b ; b c }, {a, c, d}, {b}, i,S3 = h{b c ; c d; c, b}, {b}, {c, d}, i, addition module0 = h{ a, c}, {a, c}, , encompassing integrity constraints.2. hide H = {a} , obtain S1 !h S2 disjunctive rule b . Therefore, components S1 S2 must placed block also maximalgiving rise module = h{ a, c; b ; b c }, {c, d}, {b}, {a}i= S1 S2 = {a, b}. modules 0 = S3 listed above.3. Finally, set H = {a, c} , obtain S2 !h S3 b c S1 !h S3a, c addition S1 !h S2 stated above. Since 0 = , decompositioneffectively collapses single module = = S1 S2 S3 .note non-trivial modules mentionedSM(S1 )SM(S2 )SM(S3 )SM(0 )SM(S )====={{a}, {b}},{{b}, {a, b}, {b, c}, {a, c}, {b, d}, {a, d}, {b, c, d}, {a, c, d}},{{b}, {c, d}},{, {c}, {a, c}},{{b}, {a, c}, {b, c}, {b, d}, {a, c, d}, {b, c, d}}.But, regardless decomposition obtained, holds respective joinsSM() ====SM(S1 )SM(S2 )SM(S3 )SM(0 )SM(S )SM(S3 )SM()SM(S )SM(){{a, c, d}, {b}}.calculations involvingimportant notice allowed combinationsstable models determined terms joint visible atoms modules involved.instance, Atv (S1 ) Atv (S3 ) = {a, b} {b, c, d} = {b} SM(S1 )SM(S3 ) {{a} {c, d}, {b} {b}} = {{a, c, d}, {b}} Denition 5.2. Thus, interestingly,role remaining two modules S2 0 merely approve upon twomodels. Recalling discussion introduction, suggests strategy givesprecedence1. evaluation modules stable models,2. combination stable models modules visible atoms common.843fiJanhunen, Oikarinen, Tompits & Woltran7. Shifting Disjunctionssection, continue pursuit applications module theorem establishedSection 5. generalize principle shifting disjunctive rules (Gelfond et al., 1991;Dix et al., 1996) applying results paper. Roughly speaking, idea behindshifting translate disjunctive rule B, C several normal (non-disjunctive)rules shifting head atoms h negative literals h body. instance,simple disjunctive rule b c captured normal rulesb, c,b a, c,c a, b.shown Eiter et al. (2004), local shifting transformation preserves ordinaryequivalence, i.e., stable models.12 application technique is, however, pre-emptedpresence head-cycles (Ben-Eliyahu & Dechter, 1994). cycle providedSCC intersects head disjunctive rule B, C|S A| > 1. instance, local shifting longer applicable rule b cpresence b b create strongly connected component = {a, b}.consequence, respective DLP-functions1 = h{a b c ; b; b a}, , {a, b, c}, i,(30)2 = h{a b, c; b a, c; c a, b; b; b a}, , {a, b, c},(31)dierent stable models: SM(1 ) = {{a, b}, {c}} SM(2 ) = {{c}}. discrepancy stable models settled applying decomposition techniqueSection 6. fact, leads proper generalization local shifting transformationformalized DLP-functions strongly connected components.Definition 7.1 Let = hR, I, O, Hi DLP-function SCC+ () respective setSCCs. general shifting DLP-function GSH() = hIC(R) R , I, O, Hi,R set rules{(A S) B, C, (A \ S) | B, C R, SCC+ () 6= }.(32)Hence, idea project head rule respect component S,atoms dierence \ shifted negative body. viewedcontribution disjunctive rule B, C particular component S.Example 7.2 1 (30), SCC+ (1 ) = {{a, b}, {c}},GSH(1 ) = h{a b c; c a, b; b; b a}, , {a, b, c}, i.importantly, SM(GSH(1 )) = {{a, b}, {c}} = SM(1 ), contrast setSM(2 ) = {{c}} stable models 2 (31).12. addition ordinary equivalence, also uniform equivalence (Eiter & Fink, 2003) preserved localshifting strong equivalence (Lifschitz, Pearce, & Valverde, 2001).844fiModularity Aspects Disjunctive Stable Modelsprove correctness general shifting principle Denition 7.1.aim exploit decomposition Denition 6.3 together modularreconstruction Proposition 6.4 compositionality stable semanticsCorollary 6.5. extend coverage Corollary 6.5, introduce explicit operatorsrevealing hiding atoms DLP-functions follows:Definition 7.3 Let = hR, I, O, Hi DLP-function. Then,1. Reveal(, A) = hR, I, A, H \ Ai, set H hidden atoms,2. Hide(, A) = hR, I, \ A, H Ai, set output atoms.Since denition stable models make dierence output atomshidden atoms, following properties easy verify. role hidden atomsbecomes important Section 8 DLP-functions compared other.Proposition 7.4 Let DLP-function.1. Ath (), SM() = SM(Reveal(, A)).2. Ato (), SM() = SM(Hide(, A)).Lemma 7.5 Let DLP-function Ath () = , component SCC+ (),respective module decomposition according Definition 6.3. Then,SM(S ) = SM(GSH(S )).(33)Proof. Recall = hDef R (S), I, S, i, input signature = At(Def R (S)) \S. Notice component SCC+ (S ) hence GSH(S ) set rulesR = {(A S) B, C, (A \ S) | B, C Def R (S)}.Consider interpretation S, input output signatures, respectively. Thus, Mi = Mo = S. Then, following equivalenceshold:B (Def R (S)/Mi )MoB, C Def R (S)/Mi Mo |= CB , C Def R (S) = Ao , B = Bo , C = Co ,Mi |= Ai Bi Ci , Mo |= CoB , C , Ai R = Ao , B = Bo , C = Co ,Mi |= Ai Bi Ci , Mo |= CoB, C R /Mi Mo |= CB (R /Mi )Mo .Thus, conclude (Def R (S)/Mi )Mo coincides (R /Mi )Mo , and, consequently, MoMM((Def R (S)/Mi )Mo ) Mo MM((R /Mi )Mo ). Therefore, SM(S /Mi ) =SM(GSH(S )/Mi ). Since and, particular, Mi arbitrarily chosen beginning,obtain equality stable models stated (33) directly Corollary 3.9.845fiJanhunen, Oikarinen, Tompits & WoltranTheorem 7.6 DLP-function = hR, I, O, Hi, SM() = SM(GSH()).Proof. Since may hidden atoms, Corollary 6.5 applicable decomposition based SCC+ (). Thus, start = Reveal(, H) = hR, I, H,rather itself. Since SCCs independent hiding, SCC+ ( ) = SCC+ ()GSH( ) = Reveal(GSH(),H). Since Ath ( ) = construction, knowFProposition 6.4 0 ( SSCC+ () ) = . Applying GSH() equation yieldsFGSH( ) = 0 ( SSCC+ () GSH(S )).(34)regards respective sets stable models, obtainSM( )=(SM( )=SM(GSH( )).=SM(0 )(0SSCC+ ( )SM(S ))[Corollary 6.5]SSCC+ ( )SM(GSH(S )))[Lemma 7.5][Corollary 6.5 (34)]follows Proposition 7.4 SM(Hide( , H)) = SM( ) = SM(GSH( )) =SM(Hide(GSH( ), H)). Since Hide( , H) = Hide(GSH( ), H) = GSH(),established SM() = SM(GSH()) desired.According Denition 6.3, decompositions DLP-functions create multiple copiesdisjunctive rules whose heads intersect several SCCs. introduction copiescircumvented applying general shifting technique Denition 7.1.Example 7.7 DLP-function Example 6.6, obtain R1 = {ab c, d;b; b a} R2 = {c a, b; c d; c} sets rules associated 1 = hR1 , {c, d}, {a, b}, 2 = hR2 , {a, b}, {c, d}, i, 1 2 =hR1 R2 , , {a, b, c, d}, defined.observations enable us view disjunctive rules shared modulesassociated SCCs syntactic sugar. However, clever implementation save spaceusing shared rules. worst case, unwinding rule a1 B, C coincidesrespective SCCs S1 , . . . , Sn a1 S1 , . . . , Sn may create n copiesbody B C. quadratic blow-up partly alleviated introducing newatom b name body. Thus result shifting a1 S1 , . . . , Sn becomesa1 b, a2 , . . . , ;...ai b, a1 , . . . , ai1 , ai+1 , . . . , ;...b, a1 , . . . , an1together dening rule b B, C b. implementation generalshifting principle called dencode.13 requested so, calculates beforehand whetherpays introduce new atom body disjunctive rule not.13. Available http://www.tcs.hut.fi/Software/asptools/ experimenting.846fiModularity Aspects Disjunctive Stable Models8. Equivalence DLP-Functionsconcept visible equivalence originally introduced order neglect hidden atomslogic programs, theories interest, compared basis models (Janhunen, 2006). Oikarinen Janhunen (2008a) extended idea levellogic program modulesgiving rise notion modular equivalence logic programs.section, generalize concept modular equivalence DLP-functionsintroduce translation-based method checking modular equivalence DLP-functionsfollowing analogous approaches Oikarinen Janhunen (2004, 2009).8.1 Modular EquivalenceModule interfaces must taken properly account DLP-functions compared.reason, consider two DLP-functions 1 2 compatibleAti (1 ) = Ati (2 ) Ato (1 ) = Ato (2 ).Definition 8.1 DLP-functions 1 2 modularly equivalent, denoted 1 2 ,1. 1 2 compatible2. bijection f : SM(1 ) SM(2 ) interpretationsSM(1 ), Atv (1 ) = f (M ) Atv (2 ).proof congruent lifts case normal programs (Oikarinen& Janhunen, 2008a) disjunctive case using Theorem 5.7.Proposition 8.2 Let 1 , 2 , DLP-functions. 1 2 12 defined, 1 2 .Proof. Let 1 = hR1 , I1 , O1 , H1 2 = hR2 , I2 , O2 , H2 DLP-functions1 2 , = hR, I, O, Hi DLP-function acting arbitrary context 12 1 2 dened. Consider SM(1 ). Theorem 5.7implies M1 = At(1 ) SM(1 ) N = At() SM(). Since 1 2 ,I1 = I2 , O1 = O2 , bijection f : SM(1 ) SM(2 )M1 (I1 O1 ) = f (M1 ) (I2 O2 )(35)holds M1 . Dene M2 = f (M1 ). Since M1 N compatible denition (35)holds, models M2 N compatible I1 = I2 O1 = O2 . Thus, M2 NSM(2 ) Theorem 5.7 eectively described mapped modelSM(2 ) function g : SM(1 ) SM(2 ) denedg(M ) = f (M At(1 )) (M At()).Clearly, g maps set visible atoms itself, is,(I1 O1 O) = g(M ) (I2 O2 O).justications g bijection follows:847fiJanhunen, Oikarinen, Tompits & Woltrang injection: 6= N implies g(M ) 6= g(N ) M, N SM(1 ), sincef (M At(1 )) 6= f (N At(1 )) At() 6= N At().g surjection: N SM(2 ), = f 1 (N At(2 )) (N At())SM(1 ) g(M ) = N , since f surjection.inverse function g 1 : SM(2 ) SM(1 ) g dened setting= f 1 (N At(2 )) (N At()). Thus, 1 2 .Note GSH() follows directly Theorem 7.6. Applying Proposition 8.2context Theorem 7.6 indicates shifting localized particular component1 larger DLP-function 1 since 1 GSH(1 ) .g 1 (N )8.2 Verifying Modular EquivalenceOikarinen Janhunen (2004) proposed translation-based method vericationweak equivalence disjunctive logic programs. Two logic programs weakly equivalent exactly set stable models. Thus, weak equivalenceseen special case modular equivalence DLP-functions 1 2Ati (1 ) Ath (1 ) = Ati (2 ) Ath (2 ) = . motivates us adjust translationbased technique verication modular equivalence. observed previous work(Janhunen & Oikarinen, 2007; Oikarinen & Janhunen, 2008a), verication visible/modular equivalence involves counting problem general. reduction computational time complexity achieved programs enough visible atoms,referred EVA property short, (Janhunen & Oikarinen, 2007). DLPfunction = hR, I, O, Hi, dene hidden part restricted DLP-functionh = hDef R (H), O, H, enables evaluation hidden atoms H givenarbitrary truth values atoms O. Recalling Denition 3.2, useinstantiation h respect interpretation Mv Ati (h ), i.e., h /Mv , deneEVA property DLP-function .Definition 8.3 DLP-function = hR, I, O, Hi enough visible atoms iff h /Mvunique stable model Mv Atv () = Ati (h ).idea behind translation-based method Oikarinen Janhunen (2004)ordinary disjunctive programs R1 R2 weakly equivalent translationsTR(R1 , R2 ) TR(R2 , R1 ) stable models. following, propose modiedversion translation function adjusted verication modular equivalence. orderable verify modular equivalence, need take semantics atomsinput signature account well role hidden atoms modular equivalenceprograms consideration. case DLP-functions, transform pair 12 compatible DLP-functions DLP-function EQT(1 , 2 ) stablemodel stable model SM(1 ) stable modelN SM(2 ) Atv (1 ) = N Atv (2 ). form translation compositionDLP-functions order fully exploit compositionality stable model semanticsjustifying correctness method.follows, use new atoms , , appearing At(1 ) At(2 )atom a, use shorthand = {a | A} set atoms,848fiModularity Aspects Disjunctive Stable Modelsanalogously dened shorthands . Moreover, diff, unsat, unsat , ok newatoms appearing At(1 ) At(2 ). translation EQT(1 , 2 ),summarized Denition 8.4 below, consists following three parts:(i) DLP-function 1 naturally captures stable model SM(1 ).(ii) DLP-function hidden(2 ) = hRh , O, H , provides representationhidden part 2 = hR, I, O, Hi evaluated respect visible part .input signature hidden(2 ) consists visible atoms Atv (2 ) = Atv (1 ) =O. set Rh contains rule Ah Bv Bh , (Av Cv Ch ) B, C RAh 6= , i.e., B, C Def R (H). hidden parts rules renamedsystematically using atoms Ath (2 ) . capture unique stable modelN (2 )h /Mv expressed Ath (2 ) rather Ath (2 ). Note existenceuniqueness N guaranteed EVA property.(iii) Finally, DLP-functionTR(2 ) = hRTR , H , H {unsat, unsat , diff, ok}, Hprovides minimality check. set RTR contains1. rule unsat Bv Bh , (Av Ah Cv Ch ) rule B, C R,2. rules a, , unsat a, , unsat O, rules, , unsat , , unsat H,3. rule unsat Bi Bo Bh , (Ai Ao Ah Cv Ch ), unsat ruleB, C R,4. rule diff a, , unsat O, rule diff , , unsatH,5. following rules:ok unsat,ok diff, unsat, unsat ,ok.intuition behind translation TR(2 ) follows. rules rstitem check whether interpretation L At(2 ) corresponding actual inputK = (L (I O)) {a | L H} Atv (2 ) Ath (2 ) TR(2 ) satisesrules 2 . rules 2 satised, rules items 24 activatedliterals unsat bodies. rules second item usedgenerate subset L L L Ati (2 ) = L Ati (2 ). achievedintroducing new atom Ato (2 ) Ath (2 ). rules thirditem check whether representation L Ati (2 ) Ato (2 ) Ath (2 ) , i.e.,K = (L I) {a | L (O H)}, satises rules L2 . rulesfourth item check whether L proper subset L. Finally, rules fthitem summarize reasons L cannot stable model 2 , i.e., eitherrules 2 satised L, L minimal model L2 . net eectconstruction, TR(2 )/K stable model L stable model 2 .849fiJanhunen, Oikarinen, Tompits & WoltranDefinition 8.4 Let 1 2 = hR, I, O, Hi compatible DLP-functions enoughvisible atoms. Then, translation EQT(1 , 2 ) given 1 hidden(2 ) TR(2 ).translation TR(2 ) minimality check essentially contains rulesTR(R1 , R2 ) \ R1 , TR(R1 , R2 ) translation dened Oikarinen Janhunen(2004) sets R1 R2 disjunctive rules. two aspects, however.First, occurrences hidden atoms H additionally represented using counterparts H . Second, need renamed versions atoms Hinterpretation atoms input signature kept xed. Finally, noteDLP-functions 1 2 correspond ordinary disjunctive logic programs, i.e.,1 = hR1 , , O, 2 = hR2 , , O, i, translation EQT(1 , 2 ) coincidesTR(R1 , R2 ).Theorem 8.5 Let 1 2 compatible DLP-functions enough visible atoms.Then, 1 2 iff SM(EQT(1 , 2 )) = SM(EQT(2 , 1 )) = .Proof sketch. Let 1 2 = hR, I, O, Hi compatible DLP-functions enoughvisible atoms. Theorem 5.7, given compatible interpretations M1 At(1 ), M2At(hidden(2 )), M3 At(TR(2 )), = M1 M2 M3 stable model translation EQT(1 , 2 ) M1 SM(1 ), M2 SM(hidden(2 )), M3 SM(TR(2 )). Giveninterpretation M1 At(1 ), unique stable model M2 SM(hidden(2 ))compatible M1 , since 2 EVA property. Hence, hidden(2 ) constrainstable models composition EQT(1 , 2 ). Whenever M3 compatible M1M2 , holds M3 (I OH ) = (M1 M2 )(I OH ) M3 SM(TR(2 ))interpretation M3 (I O){a H | M3 } stable model 2 establishedOikarinen Janhunen (2004, Theorem 1).verifying modular equivalence DLP-functions forms 1 2 ,possible streamline translations involved verication task.Theorem 8.6 Let 1 2 compatible DLP-functions enough visible atoms,DLP-function 1 2 defined. Then, 1 2iff SM(EQT(1 , 2 ) ) = SM(EQT(2 , 1 ) ) = .context arbitrary DLP-function, i.e., necessaryEVA property, long 1 2 dened. prove Theorem 8.6, noticedue structure translation, EQT(1 , 2 ) dened whenever 1dened, Theorems 5.7 8.5 applied.9. Related WorkEiter et al. (1997a) consider use disjunctive datalog programs query programsrelational databases. approach, query programs formalized triples h, R, Siset disjunctive rules R signatures input outputrelations, respectively, whereas auxiliary (hidden) predicates left implicit. Hence,propositional case, notable dierence respect Denition 2.1 inputatoms allowed occur heads disjunctive rules. regards semantics,850fiModularity Aspects Disjunctive Stable Modelsprogram reduced respect complete input database specied termsR, yielding instantiation [D], and, among others, stable-model semantics applied[D] analogy Denition 3.2. However, contrast modular architecture, Eiteret al. (1997a) take positive negative dependencies account recursionmodules tolerated. resulting hierarchy complete components admitsstraightforward generalization splitting sequences (Lifschitz & Turner, 1994).essential dierence partial order rather total order modules assumed.respect, worth pointing partial orders DLP-functions permitted.Modularity gained attention context conventional (monotonic) logicprogramming; see work Bugliesi, Lamma, Mello (1994) survey. Two mainstream approaches identied: rst called programming-in-the-large algebraic operators introduced construction logic programs modules.approach paper falls categorythe join example operators.other, quite dierent programming-in-the-small approach, extend underlying logical language terms abstraction mechanisms. approach Eiter et al.(1997b), instance, logic program modules viewed generalized quantifiersallowed nest hierarchical fashion. give idea approach, considermodule formalizes transitive closure relation denoted predicate rel(, ):tclo(x, y) rel(x, y);tclo(x, y) tclo(x, z), rel(z, y).Here, tclo(, ) acts output predicate module tclo[rel] whereas rel(, )input predicate. module invoked create transitive closure binaryrelation substituted rel(, ) above. Consider, instance, ruleloop(x) tclo[edge](x, y), tclo[edge](y, x)captures nodes involved loops directed graph whose edges supposedrepresented predicate edge(, ). approach, call tclo[edge] would resultone module part respective ground program input output signatures= {edge(x, y) | 1 x, n} = {tclo(x, y) | 1 x, n}case n vertices. However, architecture Eiter et al. (1997b), moduletclo[rel] invoked several times form transitive closures dierent relations.eectively propositional approach, invocation tclo[rel] would map new module.Although modules could obtained straightforward renaming predicates,aspect illustrates power programming-in-the-small approach. Here, tclo[rel] actsnew parameterized connective programmer concisely refer newrelation, viz. transitive closure rel case. But, spite succinctnesspoint, relations may unwound actual implementation. aspectmade explicit modular action description (MAD) language proposed LifschitzRen (2006): modular action description turned single-module descriptionrecursive fashion. outcome determines meaning modular description viaembedding ASP (Lifschitz & Turner, 1999).Faber, Greco, Leone (2007) apply magic-set method evaluation datalogprograms negation. notion module based concept independent851fiJanhunen, Oikarinen, Tompits & Woltranset. non-disjunctive logic program = hR, , O, i, set satises,S, following two conditions:1. rule h B, C R h = a, B C S,2. B C dangerous rule h B, C R, {h} B C S.skip exact denition dangerous rules which, roughly speaking, may interfereexistence stable models. clear independent sets splitting sets senseDenition 5.15, vice versa general. Hence, module theorem providedFaber et al. (2007) viewed special case splitting-set theorem and, therefore,observations presented Section 5.3 apply independent sets well.10. Conclusion Discussionpaper, introduced formal framework modular programming contextdisjunctive logic programs stable-model semantics. framework basednotion DLP-function puts eect appropriate input/output interfacingdisjunctive logic programs. Analogous module concepts already studiedcases normal logic programs smodels programs (Oikarinen & Janhunen, 2008a)even propositional theories (Janhunen, 2008a), special characteristics disjunctiverules properly taken account syntactic semantic denitions DLPfunctions presented herein. respect, would like draw readers attentionDenition 2.1 (item 2), Denition 2.2 (items 45), well Denition 3.2.Undoubtedly, main result paper module theorem, i.e., Theorem 5.7,proved DLP-functions generalthus covering class disjunctive programs. module theorem important provides compositional semanticsdisjunctive programs generalizes existing approaches based splitting sets (Lifschitz & Turner, 1994) magic sets (Faber et al., 2007). Althoughapproach based number design decisions, e.g., regards denition modulecomposition, nevertheless brings limits modular programming contextnonmonotonic declarative language. module theorem exploited number ways ASP based disjunctive logic programs. demonstrated Section 6,provides basis decomposing disjunctive programs components hencelocalization reasoning tasks. Moreover, established Section 7, techniqueshifting disjunctive rules generalized disjunctive programs involving head-cycles.Actually, generalized form enables us remove shared disjunctive rules altogethermight desirable due higher space requirements. Finally, theory modularequivalence fully applicable DLP-functions demonstrated Section 8.addition results discussed above, anticipate applications module theorem future. strongly believe research direction yieldsresults theoretical interest also leads development practicably useful softwareengineering methods ASP. fact, rst tools decomposing linking programsalready implemented context smodels system.14 results Section 6 enable development analogous tools used disjunctive solvers14. See modlist lpcat ASP tools collection http://www.tcs.hut.fi/Software/asptools/.852fiModularity Aspects Disjunctive Stable ModelsclaspD, cmodels, dlv, GnT. also implementation general shiftingprinciple, called dencode, ASP tool collection. results Section 8 pave wayextending translation-based verication tool, dlpeq (Janhunen & Oikarinen, 2004),verication modular equivalence. extension already availablerespective tool, lpeq, smodels programs (Oikarinen & Janhunen, 2009).15Acknowledgments work partially supported Academy Finland projects #211025 (Advanced Constraint Programming Techniques Large Structured Problems) #122399 (Methods Constructing Solving Large ConstraintModels), Austrian Science Foundation (FWF) projects P18019 (Formal Methods Comparing Optimizing Nonmonotonic Logic Programs) P21698(Methods Methodologies Developing Answer-Set Programs). authors wouldlike thank anonymous referees constructive comments well Martin Gebser Torsten Schaub suggestion exploit program completion loop formulasproof module theorem. preliminary version paper appearedproceedings 9th International Conference Logic Programming NonmonotonicReasoning (LPNMR07), Vol. 4483 LNCS, pp. 175187, Tempe, AZ, USA, Springer.ReferencesBaral, C., Dzifcak, J., & Takahashi, H. (2006). Macros, macro calls use ensemblesmodular answer set programming. Etalle, S., & Truszczyski, M. (Eds.), Proceedings22nd International Conference Logic Programming (ICLP06 ), Vol. 4079LNCS, pp. 376390, Seattle, WA, USA. Springer.Ben-Eliyahu, R., & Dechter, R. (1994). Propositional semantics disjunctive logic programs. Annals Mathematics Artificial Intelligence, 12 (12), 5387.Bugliesi, M., Lamma, E., & Mello, P. (1994). Modularity logic programming. JournalLogic Programming, 19/20, 443502.Clark, K. L. (1978). Negation failure. Gallaire, H., & Minker, J. (Eds.), LogicData Bases, pp. 293322. Plenum Press, New York.Dix, J., Gottlob, G., & Marek, V. W. (1996). Reducing disjunctive non-disjunctivesemantics shift-operations. Fundamenta Informaticae, 28 (1-2), 87100.Drescher, C., Gebser, M., Grote, T., Kaufmann, B., Knig, A., Ostrowski, M., & Schaub,T. (2008). Conict-driven disjunctive answer set solving. Brewka, G., & Lang, J.(Eds.), Proceedings 11th International Conference Principles KnowledgeRepresentation Reasoning, pp. 170176, Sydney, Australia. AAAI Press.Eiter, T., & Fink, M. (2003). Uniform equivalence logic programs stable modelsemantics. Palamidessi, C. (Ed.), Proceedings 19th International ConferenceLogic Programming (ICLP03), Vol. 2916 LNCS, pp. 224238, Mumbay, India.Springer.15. Verification tools mentioned available http://www.tcs.hut.fi/Software/lpeq/.853fiJanhunen, Oikarinen, Tompits & WoltranEiter, T., Fink, M., Tompits, H., & Woltran, T. (2004). Simplifying logic programsuniform strong equivalence. Lifschitz, V., & Niemel, I. (Eds.), Proceedings7th International Conference Logic Programming Nonmonotonic Reasoning(LPNMR04 ), Vol. 2923 LNAI, pp. 8799, Fort Lauderdale, FL, USA. Springer.Eiter, T., & Gottlob, G. (1995). computational cost disjunctive logic programming:Propositional case. Annals Mathematics Artificial Intelligence, 15 (3-4), 289323.Eiter, T., Gottlob, G., & Mannila, H. (1997a). Disjunctive datalog. ACM TransactionsDatabase Systems, 22 (3), 364418.Eiter, T., Gottlob, G., & Veith, H. (1997b). Modular logic programming generalizedquantiers. Dix, J., Furbach, U., & Nerode, A. (Eds.), Proceedings 4thInternational Conference Logic Programming Nonmonotonic Reasoning (LPNMR97 ), Vol. 1265 LNCS, pp. 290309, Dagstuhl, Germany. Springer.Eiter, T., Ianni, G., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2008). Combininganswer set programming description logics Semantic Web. ArtificialIntelligence, 172 (1213), 14951539.Erdem, E., & Lifschitz, V. (2003). Tight logic programs. Theory Practice LogicProgramming, 3 (4-5), 499518.Faber, W., Greco, G., & Leone, N. (2007). Magic sets application data integration. Journal Computer System Sciences, 73, 584609.Gaifman, H., & Shapiro, E. (1989). Fully abstract compositional semantics logic programs. Proceedings 16th Annual ACM Symposium Principles Programming Languages, pp. 134142, Austin, TX, USA. ACM Press.Gebser, M., Kaminski, R., Kaufmann, B., Ostrowski, M., Schaub, T., & Thiele, S. (2008a).Engineering incremental ASP solver. de la Banda, M., & Pontelli, E. (Eds.),Proceedings 24th International Conference Logic Programming (ICLP08),Vol. 5366 LNCS, pp. 190205, Udine, Italy. Springer.Gebser, M., Schaub, T., Thiele, S., Usadel, B., & Veber, P. (2008b). Detecting inconsistencies large biological networks answer set programming. de la Banda,M., & Pontelli, E. (Eds.), Proceedings 24th International Conference LogicProgramming (ICLP08), Vol. 5366 LNCS, pp. 130144, Udine, Italy. Springer.Gelfond, M., & Gabaldon, A. (1999). Building knowledge base: example. AnnalsMathematics Artificial Intelligence, 25 (3-4), 165199.Gelfond, M., & Leone, N. (2002). Logic programming knowledge representationA-Prolog perspective. Artificial Intelligence, 138 (1-2), 338.Gelfond, M., & Lifschitz, V. (1988). stable model semantics logic programming.Kowalski, R. A., & Bowen, K. A. (Eds.), Proceedings 5th International Conference Logic Programming (ICLP88), pp. 10701080, Seattle, WA, USA. MITPress.854fiModularity Aspects Disjunctive Stable ModelsGelfond, M., & Lifschitz, V. (1991). Classical negation logic programs disjunctivedatabases. New Generation Computing, 9, 365385.Gelfond, M., Przymusinska, H., Lifschitz, V., & Truszczyski, M. (1991). Disjunctive defaults. Allen, J. F., Fikes, R., & Sandewall, E. (Eds.), Proceedings 2nd International Conference Principles Knowledge Representation Reasoning, pp.230237, Cambridge, MA, USA. Morgan Kaufmann.Giunchiglia, E., Lierler, Y., & Maratea, M. (2006). Answer set programming basedpropositional satisability. Journal Automated Reasoning, 36 (4), 345377.Janhunen, T. (2006). (in)translatability results normal logic programs propositional theories. Journal Applied Non-Classical Logics, 16 (12), 3586.Janhunen, T. (2008a). Modular equivalence general. Ghallab, M., Spyropoulos, C.,Fakotakis, N., & Avouris, N. (Eds.), Proceedings 18th European ConferenceArtificial Intelligence (ECAI08), pp. 7579, Patras, Greece. IOS Press.Janhunen, T. (2008b). Removing redundancy answer set programs. de la Banda,M., & Pontelli, E. (Eds.), Proceedings 24th International Conference LogicProgramming (ICLP08), Vol. 5366 LNCS, pp. 729733, Udine, Italy. Springer.Janhunen, T., Niemel, I., Seipel, D., Simons, P., & You, J.-H. (2006). Unfolding partialitydisjunctions stable model semantics. ACM Transactions ComputationalLogic, 7 (1), 137.Janhunen, T., & Oikarinen, E. (2004). lpeq dlpeq translators automated equivalence testing logic programs. Lifschitz, V., & Niemel, I. (Eds.), Proceedings7th International Conference Logic Programming Nonmonotonic Reasoning(LPNMR04 ), Vol. 2923 LNAI, pp. 336340, Fort Lauderdale, FL, USA. Springer.Janhunen, T., & Oikarinen, T. (2007). Automated verication weak equivalence withinsmodels system. Theory Practice Logic Programming, 7 (6), 697744.Junttila, T., & Niemel, I. (2000). Towards ecient tableau method boolean circuitsatisability checking. Lloyd, J. W., et al. (Eds.), Proceedings First International Conference Computational Logic (CL 2000), Vol. 1861 LNCS, pp. 553567,London, UK. Springer.Koch, C., Leone, N., & Pfeifer, G. (2003). Enhancing disjunctive logic programming systemsSAT checkers. Artificial Intelligence, 151 (1-2), 177212.Lee, J., & Lifschitz, V. (2003). Loop formulas disjunctive logic programs. Palamidessi,C. (Ed.), Proceedings 19th International Conference Logic Programming(ICLP03 ), Vol. 2916 LNCS, pp. 451465, Mumbay, India. Springer.Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., & Scarcello, F. (2006). DLVsystem knowledge representation reasoning. ACM Transactions Computational Logic, 7 (3), 499562.855fiJanhunen, Oikarinen, Tompits & WoltranLifschitz, V. (1985). Computing circumscription. Joshi, A. K. (Ed.), Proceedings9th International Joint Conference Artificial Intelligence (IJCAI85 ), pp. 121127,Los Angeles, CA, USA. Morgan Kaufmann.Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. ACMTransactions Computational Logic, 2 (4), 526541.Lifschitz, V., & Ren, W. (2006). modular action description language. Proceedings21st National Conference Artificial Intelligence (AAAI06), pp. 853859,Boston, MA, USA. AAAI Press.Lifschitz, V., & Turner, H. (1994). Splitting logic program. Hentenryck, P. V. (Ed.),Proceedings 11th International Conference Logic Programming (ICLP94 ),pp. 2337, Santa Margherita Ligure, Italy. MIT Press.Lifschitz, V., & Turner, H. (1999). Representing transition systems logic programs.Gelfond, M., Leone, N., & Pfeifer, G. (Eds.), Proceedings 6th InternationalConference Logic Programming Nonmonotonic Reasoning, (LPNMR99 ), Vol.1730 LNAI, pp. 92106, El Paso, TX, USA. Springer.Lin, F., & Zhao, Y. (2004). ASSAT: computing answer sets logic program SATsolvers. Artificial Intelligence, 157 (1-2), 115137.Marek, V. W., & Truszczyski, M. (1999). Stable models alternative logic programming paradigm. Apt, K. R., Marek, V. W., Truszczyski, M., & Warren,D. S. (Eds.), Logic Programming Paradigm: 25-Year Perspective, pp. 375398.Springer.McCarthy, J. (1986). Applications circumscription formalizing commonsense knowledge. Artificial Intelligence, 28, 89116.Niemel, I. (1999). Logic programs stable model semantics constraint programmingparadigm. Annals Mathematics Artificial Intelligence, 25 (34), 241273.Oikarinen, E., & Janhunen, T. (2004). Verifying equivalence logic programsdisjunctive case. Lifschitz, V., & Niemel, I. (Eds.), Proceedings 7th International Conference Logic Programming Nonmonotonic Reasoning (LPNMR04 ),Vol. 2923 LNAI, pp. 180193, Fort Lauderdale, FL, USA. Springer.Oikarinen, E., & Janhunen, T. (2008a). Achieving compositionality stable modelsemantics smodels programs. Theory Practice Logic Programming, 8 (56),717761.Oikarinen, E., & Janhunen, T. (2008b). Implementing prioritized circumscription computing disjunctive stable models. Dochev, D., Pistore, M., & Traverso, P. (Eds.),Artificial Intelligence: Methodology, Systems, Applications, 13th InternationalConference (AIMSA08), Vol. 5253 LNCS, pp. 167180, Varna, Bulgaria. Springer.Oikarinen, E., & Janhunen, T. (2009). translation-based approach vericationmodular equivalence. Journal Logic Computation, 19 , 591613.856fiModularity Aspects Disjunctive Stable ModelsReiter, R. (1987). theory diagnosis rst principles. Artificial Intelligence, 32 (1),5795.Simons, P., Niemel, I., & Soininen, T. (2002). Extending implementing stablemodel semantics. Artificial Intelligence, 138 (12), 181234.857fiJournal Artificial Intelligence Research 35 (2009) 677-716Submitted 11/08; published 08/09Variable Forgetting Reasoning KnowledgeKaile Susukl@pku.edu.cnSchool Electronics Engineering Computer SciencePeking UniversityBeijing, P.R. ChinaAbdul Sattara.sattar@griffith.edu.auInstitute IISGriffith UniversityBrisbane, Qld 4111, AustraliaGuanfeng Lvlvgf@yahoo.comSchool Computer ScienceBeijing University TechnologyBeijing, P.R. ChinaYan Zhangyan@cit.uws.edu.auSchool Computing Information TechnologyUniversity Western SydneyPenrith South DC NSW 1797, AustraliaAbstractpaper, investigate knowledge reasoning within simple framework calledknowledge structure. use variable forgetting basic operation one agent reasonagents knowledge. framework, two notions namely agentsobservable variables weakest sufficient condition play important roles knowledgereasoning. Given background knowledge base set observable variables Oiagent i, show notion agent knowing formula definedweakest sufficient condition Oi . Moreover, show capturenotion common knowledge using generalized notion weakest sufficient condition.Also, show public announcement operator conveniently dealt vianotion knowledge structure. Further, explore computational complexityproblem whether epistemic formula realized knowledge structure.general case, problem PSPACE-hard; however, interesting subcases,reduced co-NP. Finally, discuss possible applications frameworkinteresting domains automated analysis well-known muddychildren puzzle verification revised Needham-Schroeder protocol. believemany scenarios natural presentation available informationknowledge form knowledge structure. makes valuablecompared corresponding multi-agent S5 Kripke structure muchsuccinct.1. IntroductionEpistemic logics, logics knowledge usually recognized originatedwork Jaakko Hintikka - philosopher showed certain modal logics couldused capture intuitions nature knowledge early 1960s (Hintikka,c2009AI Access Foundation. rights reserved.fiSu, Sattar, Lv, & Zhang1962). mid 1980s, Halpern colleagues discovered S5 epistemic logicscould given natural interpretation terms states processes (commonly calledagents) distributed system. model known interpreted system model(Fagin, Halpern, Moses, & Vardi, 1995). found model plays importantrole theory distributed systems applied successfully reasoningcommunication protocols (Halpern & Zuck, 1992). However, work epistemiclogic mainly focused theoretical issues variants modal logic, completeness,computational complexity, derived notions like distributed knowledge commonknowledge.paper, explore knowledge reasoning within concrete model knowledge. framework reasoning knowledge simple powerful enoughanalyze realistic protocols widely used security protocols.illustrate problem investigated paper, let us consider communicationscenario Alice sends Bob message Bob sends Alice acknowledgementreceiving message. assume Alice Bob commonly following backgroundknowledge base CS :Bob recv msg Alice send msgBob send ack Bob recv msgAlice recv ack Bob send ackBob recv msg Bob send ack observable variables Bob, Alice send msgAlice recv ack observable Alice.problem concerned verify Alice Bob knows statement. Intuitively, able prove statement observable Alice (Bob),Alice (Bob) knows statement statement holds.knowledge non-observable statements, following hold:1. Alice knows Bob recv msg Alice recv ack holds; hand, Alice knowsBob recv msg, Alice recv ack holds, means that, contextexample, way Alice gets know Bob recv msg Alice receivesacknowledgement Bob.2. Bob knows Alice send msg Bob recv msg holds; moreover, Bob knows Alice send msg,Bob recv msg holds. latter indicates way Bob getsknow Alice send msg Bob receives message Alice.3. Finally, Bob know Alice recv ack.idea behind presented knowledge model scenarios demonstratedagents knowledge agents observations logical consequencesagents observations background knowledge base.One key notions introduced paper agents observable variables.notion shares similar spirit local variables work van der HoekWooldridge (2002) local propositions work Engelhardt, van der MeydenMoses (1998) work Engelhardt, van der Meyden Su (2003). Informallyspeaking, local propositions depending upon agents local information;agent always determine whether given local proposition true. Local variables678fiVariable Forgetting Reasoning Knowledgeprimitive propositions local. Nevertheless, notion local propositions(Engelhardt et al., 1998, 2003) semantics property truth assignment functionKripke structure, notion local variables (van der Hoek & Wooldridge, 2002)property syntactical variables. paper, prefer use term observable variable order avoid confusion term local variable used programming,non-local variables global variables may often observable.knowledge model also closely related notion weakest sufficient condition,first formalized Lin (2001). Given background knowledge base setobservable variables Oi agent i, show notion agent knowingformula defined weakest sufficient condition Oi ,computed via operation variable forgetting (Lin & Reiter, 1994) eliminationsmiddle terms (Boole, 1854). Moreover, generalize notion weakest sufficientcondition capture notion common knowledge.briefly discuss role variable forgetting knowledge model. Let usexamine scenario described again. Consider question: Alice figureBobs knowledge receives acknowledgement Bob? Note Alicesknowledge conjunction background knowledge base CS observationsAlice recv ack etc. Moreover, Alice knows Bobs knowledge conjunctionbackground knowledge base CS knows Bobs observations. Thus,Alice gets Bobs knowledge computing knows Bobs observations.setting, Alice gets knowledge Bobs observations simply forgetting Bobs nonobservable variables knowledge.recent trend extending epistemic logics dynamic operatorsevolution knowledge expressed (van Benthem, 2001; van Ditmarsch, van der Hoek,& Kooi, 2005a). basic extension public announcement logic (PAL),obtained adding operator truthful public announcements (Plaza, 1989; Baltag,Moss, & Solecki, 1998; van Ditmarsch, van der Hoek, & Kooi, 2005b). show publicannouncement operator conveniently dealt via notion knowledge structure. makes notion knowledge structure genuinely useful applicationslike automated analysis well-known muddy children puzzle.discussion above, see framework reasoning knowledge appropriate situations every agent specified set observablevariables. show significance framework, investigateinteresting applications automated analysis well-known muddy children puzzleverification revised Needham-Schroeder protocol (Lowe, 1996).believe many scenarios natural presentation availableinformation knowledge form knowledge structure. makesvaluable compared corresponding multi-agent S5 Kripke structuremuch succinct. course, price pay determining whether formulaholds knowledge structure PSPACE-hard general case, PTIMEcorresponding S5 Kripke structure taken input. However, achievedtrade-off time space prove computationally valuable. particular,validity problem knowledge structure addressed instancesgenerating corresponding Kripke structure would unfeasible. muddy childrenpuzzle shows point clearly: generating corresponding Kripke structure impossible679fiSu, Sattar, Lv, & Zhangpractical point view, even least number children consideredexperiments.organization paper follows. next section, briefly introduceconcept forgetting notion weakest sufficient strongest necessary conditions.Section 3, define framework reasoning knowledge via variable forgetting.Section 4, generalize notion weakest sufficient condition strongest necessarycondition capture common knowledge within framework. Section 5, showpublic announcement operator also conveniently dealt via notionknowledge structure. Section 6 discusses computational complexity issueproblem whether epistemic formula realized knowledge structure.general case, problem PSPACE-hard; however, interesting subcases,reduced co-NP. Section 7, consider case study applying frameworkmodel well known muddy children puzzle; security protocolverification Section 8. Finally, discuss related work conclude paperremarks.2. Preliminariessection, provide preliminaries notions variable forgettingweakest sufficient condition, epistemic logic.2.1 ForgettingGiven set propositional variables P , identify truth assignment P subsetP . say formula formula P propositional variable occurringP . convenience, define true abbreviation fixed valid propositionalformula, say p p, p primitive proposition P . abbreviate true false.also use |= denote usual satisfaction relation truth assignmentformula. Moreover, set formulas formula , use |= denoteevery assignment , |= , |= .p)Given propositional formula , propositional variable p, denote ( truepresult replacing every p true. define ( false ) similarly.notion variable forgetting (Lin & Reiter, 1994), eliminations middle terms(Boole, 1854), defined follows:Definition 1 Let formula P , V P . forgetting V , denotedV , quantified formula P , defined inductively follows:1. = ;2. {p} =ptruepfalse ;3. (V {p}) = V ({p}).convenience, use V denote V ().Example 2: Let = (p q) (p r). {p} (q r) {q} (p r).2680fiVariable Forgetting Reasoning KnowledgeMany characterizations variable forgetting, together complexity results, reported work Lang Marquis (1998). particular, notion variableforgetting closely related formula-variable independence (Lang, Liberatore, &Marquis, 2003).Definition 3 Let propositional formula, V set propositional variables.say independent V logically equivalent formulanone variables V appears.following proposition given work Lang, Liberatore Marquis (2003).Proposition 4 Let propositional formula, V set propositional variables.V logically strongest consequence independent V (uplogical equivalence).2.2 Weakest Sufficient Conditionsformal definitions weakest sufficient conditions strongest necessary conditionsfirst formalized via notion variable forgetting Lin (2001), turn playessential role approach.Definition 5 Let V set propositional variables V 0 V . Given set formulasV background knowledge base formula V .formula V 0 called sufficient condition V 0 |= .called weakest sufficient condition V 0 sufficientcondition V 0 , sufficient condition 0 V 0, |= 0 .formula V 0 called necessary condition V 0 |= .called strongest necessary condition V 0 necessarycondition V 0 , necessary condition 0 V 0, |= 0 .notions given closely related theory abduction. Given observation,may one abduction conclusion draw. usefulfind weakest one conclusions, i.e., weakest sufficient conditionobservation (Lin, 2001). notions strongest necessary weakest sufficient conditionsproposition also many potential applications areas reasoningactions. following proposition, due Lin (2001), shows computetwo conditions.Proposition 6 Given background knowledge base {} V , formula V ,subset V 0 V . Let SN C W SC strongest necessary condition weakestsufficient condition V 0 {} respectively.W SC equivalent (V V 0 )( );SN C equivalent (V V 0 )( ).681fiSu, Sattar, Lv, & Zhang2.3 Epistemic Logic Kripke Structurerecall standard concepts notations related modal logics multiagents knowledge.Given set V propositional variables. Let L(V ) set propositional formulasV . language epistemic logic, denoted Ln (V ), L(V ) augmented modaloperator Ki agent i. Ki read agent knows . Let LCn (V )language Ln (V ) augmented modal operator C set agents . formulaC indicates common knowledge among agents holds. omitargument V write Ln LCn , clear context.According paper Halpern Moses (1992), semantics formulasgiven means Kripke structure (Kripke, 1963), formalizes intuition behindpossible worlds. Kripke structure tuple (W, , K1 , , Kn ), W setworlds, associates world truth assignment propositional variables,(w)(p) {true, false} world w propositional variable p, K1 , , Knbinary accessibility relations. convention, W , KiM used referset W possible worlds, Ki relation function Kripke structure ,respectively. omit superscript clear context. Finally, let Ctransitive closure Ki .situation pair (M, w) consisting Kripke structure world w .using situations, inductively give semantics formulas follows: primitivepropositions p,(M, w) |= p iff (w)(p) = true.Conjunctions negations dealt standard way. Finally,(M, w) |= Ki iff w0 W wKiM w0 , (M, w0 ) |= ;w 0 , (M, w 0 ) |= .(M, w) |= C iff w0 W wCsay formula satisfiable Kripke structure (M, w) |= possibleworld w Kripke structure .Kripke structure called S5n Kripke structure if, every i, KiM equivalence relation. Kripke structure called finite Kripke structure set possibleworlds finite. According work Halpern Moses (1992), followinglemma.Lemma 7 formula satisfiable S5n Kripke structure, finite S5nKripke structure.3. Knowledge Weakest Sufficient Conditionsframework, knowledge structure simple model reasoning knowledge.advantage model is, shown later, agents knowledgecomputed via operation variable forgetting.682fiVariable Forgetting Reasoning Knowledge3.1 Knowledge StructureDefinition 8 knowledge structure F n-agents (n + 2)-tuple (V, , O1 , , )(1) V set propositional variables; (2) consistent set propositionalformulas V ; (3) agent i, Oi V .variables Oi called agent observable variables. assignment satisfiescalled state knowledge structure F. Given state F, define agent localstate state Oi . Two knowledge structures said equivalentset propositional variables, set states and, agent i,set agent observable variables.pair (F, s) knowledge structure F state F called scenario.Given knowledge structure (V, , O1 , , ) set V subsets V , use EVdenote relation two assignments s, s0 V satisfying (s, s0 ) EViff exists P V P = s0 P . use EV denote transitive closureEV .Let V = {Oi | }. (s, s0 ) EV iff existsOi = s0 Oi .simple instance knowledge structure F0 = ({p, q}, {p q}, {p}, {q}), p, qpropositional variables. two agents knowledge structure F0 . Variables pq observable agents 1 2, respectively. V{1,2} = {{p}, {q}};two subsets s0 {p, q} satisfy p q, (s, s0 ) EV{1,2} .give semantics language LCn based scenarios.Definition 9 satisfaction relationship |= scenario (F, s) formuladefined induction structure .1. propositional variable p, (F, s) |= p iff |= p.2. formulas , (F, s) |= iff (F, s) |= (F, s) |= ;(F, s) |= iff (F, s) |= .3. (F, s) |= Ki iff s0 F s0 Oi = Oi , (F, s0 ) |= .4. (F, s) |= C iff (F, s0 ) |= s0 F (s, s0 ) EV .say proposition formula L(V ) i-local Oi . Clearly, agentknows i-local formula F iff |= .Let F = (V, , O1 , , ) knowledge structure. say formula realizedknowledge structure F, every state F, (F, s) |= . convenience, F |= ,denote formula realized knowledge structure F.conclude subsection following lemma, used remainspaper.Lemma 10 Let V finite set variables, F = (V, , O1 , , ) knowledge structure, state F. Also suppose {1, , n}, V = {Oi | }.683fiSu, Sattar, Lv, & Zhang1. objective formula (i.e., propositional formula V ), (F, s) |= iff |= ;2. formula , (F, s) |= ;3. i-local formula , (F, s) |= Ki ;4. formula , exists, , i-local formula logically equivalent, (F, s) |= C ;5. formulas 1 2 , (F, s) |= Ki (1 2 ) (Ki 1 Ki 2 );6. formulas 1 2 , (F, s) |= C (1 2 ) (C 1 C 2 );7. formula , (F, s) |= C Ki C .Proof:1. first item proposition proved induction structure .primitive proposition, done first item Definition 9.form negation conjunction, conclusion also follows immediatelyfirst item Definition 9.2. second item proposition proved first item factsatisfies .3. Given i-local formula , suffices show (F, s) |= Ki iff (F, s) |= .first item proposition, (F, s) |= iff |= . Moreover,i-local Oi , assignments s0 s0 Oi = Oi , s0 |=iff |= . Therefore, get following three iffs: (F, s) |= Ki iff, states0 F s0 Oi = Oi , (F, s0 ) |= iff, state s0 Fs0 Oi = Oi , s0 |= iff |= . Thus, (F, s) |= Ki iff (F, s) |= .4. Suppose that, , exists i-local formula logically equivalent. need show (F, s) |= C . First, (s, s) EV EV ,formula , (F, s) |= C implies (F, s) |= . Therefore, sufficesprove (F, s) |= C . Assume (F, s) |= . prove (F, s) |= C ,need show every assignment s0 (s, s0 ) EV , (F, s0 ) |= .definition EV , suffices show every finite sequence assignmentss0 , , sk s0 = (sj , sj+1 ) EV (0 j < k), every j k,(F, sj ) |= . show induction j. j = 0, result clearly true.Assume (F, sj ) |= . prove (F, sj+1 ) |= . (sj , sj+1 ) EV ,Oi sj = Oi sj+1 . hand, sj |= iffsj+1 |= equivalent i-local formula. Hence, (F, sj+1 ) |=desired.5. suffice show (F, s) |= Ki (1 2 ) (F, s) |= Ki 1 , (F, s) |=Ki 2 . Assume (F, s) |= Ki (1 2 ) (F, s) |= Ki 1 , item 3 Definition 9 get that, s0 F s0 Oi = Oi , (F, s0 ) |= (1 2 )(F, s0 ) |= 1 . However, item 2 Definition 9, get (F, s0 ) |= 2(F, s0 ) |= (1 2 ) (F, s0 ) |= 1 . Therefore, get that, s0 Fs0 Oi = Oi , (F, s0 ) |= 2 . follows immediately (F, s) |= Ki 2 .684fiVariable Forgetting Reasoning Knowledge6. item shown way proof item 5.7. suffices prove state s00 state s0 Oi =s0 Oi s0 EV s00 , get sEV s00 , follows immediately factEV transitive closure EV . 23.2 Relationship S5 Kripke StructureGiven knowledge structure F = (V, , O1 , , ), let (F) Kripke structure(W, , K1 , , Kn ),1. W set states F;2. w W , assignment (w) w;3. agent assignments w, w0 W , wKi w0 iff w Oi = w0 Oi .following proposition indicates knowledge structure viewed specific Kripke structure.Proposition 11 Given knowledge structure F, state F, formula LCn (V ),(F, s) |= iff (M (F), s) |= .Proof: Immediately definition satisfaction relationship scenarioformula situation formula. 2Proposition 11, conclude formula LCn satisfiable knowledgestructure, formula also satisfiable Kripke structure. followingproposition Lemma 7, get formula LCn satisfiable Kripkestructure, formula also satisfiable knowledge structure.Proposition 12 finite S5n Kripke structure propositional variable set Vpossible world w , exists knowledge structure FM state sw Fthat, every formula LCn (V ), (FM , sw ) |= iff (M, w) |= .Proof: Let = (W, , R1 , , Rn ), W finite set R1 , , Rn equivalencerelations. Let O1 , , sets new propositional variables1. O1 , , finite pairwise disjoint;2. (0 < n), number subsets Oi lessequivalence classes Ri .latter condition, is, i, function gi : W 7 2Oiw1 , w2 W , gi (w1 ) gi (w2 ) subset Oi iff w1 w2equivalence class Ri .0Let V 0 = V 0<in Oi . define function g : W 7 2V follows. possibleworld w W ,[g(w) = {v V | (w)(v) = true}gi (w).0<infollowing two claims hold:685fiSu, Sattar, Lv, & ZhangC1 w1 , w2 W , (0 < n), g(w1 ) Oi = g(w2 ) Oi iffw1 Ri w2 .C2 w W v V , v g(w) iff (w)(v) = true.Let= { | V 0 , g(w) |= w W }.get knowledge structureFM = (V 0 , , O1 , , ).show following claim:C3 every V 0 , state FM iff = g(w) w W.part claim C3 easy prove. = g(w0 ) w0 W ,definition , g(w0 ) |= hence g(w0 ) state FM . showpart, assume every w W , 6= g(w). Then, every w W ,Vexists w V 0 |= w g(w) |= w . Therefore, |= wW w .WWMoreover, that, every w0 W , g(w0 ) |= wW w , hence wW w. Consequently, 6|= hence state FM .complete proof, suffices show, every LCn (V ), (FM , g(w)) |=iff (M, w) |= . conditions C1, C2 C3, induction .base case, assume propositional variable, say p. Then, condition C2,(FM , g(w)) |= p iff p g(w) iff (w)(p) = true iff (M, w) |= p.Suppose propositional variable claim holds every subformula. three cases:1. form . case dealt definitions satisfactionrelations directly.2. form Ki . case, (FM , g(w)) |= Ki iff (FM , s) |= statesFM g(w) Oi = Oi . condition C3, (FM , g(w)) |= Kiiff (FM , g(w0 )) |= w0 W g(w) Oi = g(w0 ) Oi . condition C1,(FM , g(w)) |= Ki iff (FM , g(w0 )) |= w0 W wRi w0 .Therefore, induction assumption, (FM , g(w)) |= Ki iff (M, w0 ) |=w0 W wRi w0 . right part (M, w) |= Ki .3. form C . Recall that, arbitrary two states s0 FM , (s, s0 ) EViff exists Oi = s0 Oi . condition C1, w1 , w2 W ,(g(w1 ), g(w2 )) EV iff (w1 , w2 )[Ri .EV transitive closure EV , CRi , condition C3get(g(w1 ), g(w2 )) EV iff (w1 , w2 ) Cw1 , w2 W .686fiVariable Forgetting Reasoning Knowledgewant show (FM , g(w)) |= C iff (M, w) |= C . one hand, (FM , g(w)) |=C iff states FM (g(w), s) EV , (FM , s) |= . condition C3,(FM , g(w)) |= C iff w0 W (g(w), g(w0 )) EV .. Therefore,hand, (M, w) |= C iff w0 W (w, w0 ) Cconclude (FM , g(w)) |= C iff (M, w) |= C discussion. 2Propositions 11 12 show satisfiability issue formula languagemulti-agent S5 common knowledge modality whatever satisfiabilitymeant w.r.t. standard Kripke structure w.r.t. knowledge structure.3.3 Knowledge Weakest Sufficient Conditionsfollowing theorem establishes bridge notion knowledge notionweakest sufficient strongest necessary conditions.Theorem 13 Let V finite set variables, F = (V, , O1 , , ) knowledge structure, propositional formula L(V ), agent i, W SCi SN Ci weakestsufficient condition strongest necessary condition Oi respectively.Then, state F,(F, s) |= Ki W SCi(F, s) |= Ki SN Ci .Proof: show (F, s) |= Ki W SCi , part comes straightforward way duality WSCs SNCs. W SCi sufficient condition, |= W SCi . Let conjunction formulas, |= (W SCi ), leads (F, s) |= Ki W SCi Ki (byitem 5 Lemma 10.) W SCi i-local, Lemma 10 (item 3) again,(F, s) |= W SCi Ki W SCi . Hence, (F, s) |= W SCi Ki .show direction (F, s) |= Ki W SCi , consider formula (VOi )( ), above. Proposition 6, |= (V Oi )() W SCi . hand, know (F, s) |= Ki (V Oi )( )definition Ki . proves (F, s) |= Ki W SCi . 2following corollary characterizes subjective formulas Ki (where objective)satisfied given knowledge structure.Corollary 14 Let V finite set variables, F = (V, {}, O1 , , ) knowledgestructure n agents, formula V . Then, every state F,(F, s) |= Ki (V Oi )( ).Proof:Immediately Theorem 13 Proposition 6. 2consider communication scenario Alice BobExample 15 :addressed section 1 again. show system deal knowledgereasoning issue scenario, define knowledge structure F follows:F = (V, {}, OA , OB ),687fiSu, Sattar, Lv, & ZhangOA = {Alice send msg, Alice recv ack},OB = {Bob recv msg, Bob send ack},V = OA OB ,conjunction following three formulas:Bob recv msg Alice send msg,Bob send ack Bob recv msg,Alice recv ack Bob send ack,given state Fs=Alice send msg,Alice recv ack,Bob recv msg,Bob send ack,would like know whether Alice knows Bob received message. Considerformula()Bob recv msg,( Bob recv msg).Bob send ackDefinition 1, formula simplified Alice recv ack, which, obviously,satisfied scenario (F, s), i. e. ,(F, s) |= Alice recv ack.Corollary 14,(F, s) |= KA Bob recv msg.item 3 lemma 10, follows(F, s) |= KA Alice send msg(F, s) |= KA Alice recv ack,indicates Alice knows sent message knows receivedacknowledgement Bob. 2Given set states knowledge structure F formula , (F, S) |= ,mean S, (F, s) |= . following proposition presents alternativeway compute agents knowledge.688fiVariable Forgetting Reasoning KnowledgeProposition 16 Let V finite set variables, F = (V, , O1 , , ) knowledgestructure n agents, formula V , formula LCn . Suppose SN Cistrongest necessary condition Oi , denotes set statesF (F, s) |= , SSN C denotes set states (F, s) |=SN Ci . Then, agent i,(F, ) |= Ki iff (F, SSN C ) |= .Proof: Let S1 set states satisfying (F, s) |= (V Oi )( ).|= SN Ci (V Oi )( ), S1 = SSN C . Also easy see stateF, S1 iff state s0 F s0 |= Oi = s0 Oi . Therefore(F, ) |= Ki iff S1 {s | (F, s) |= }. leads (F, ) |= Ki iff (F, S1 ) |=iff (F, SSN C ) |= . 2intuitive meaning behind Proposition 16 know currentstate , know agent knowledge (or agent observations)strongest necessary condition Oi .following proposition provides method determined whether formulanested depth knowledge operators (like Ki1 Kik , propositional formula)always true states, given proposition formula true.Proposition 17 Let V finite set variables, F = (V, {}, O1 , , ) knowledgestructure n agents, two formulas V , denotes set statesF (F, s) |= . Then, group agents i1 , , ik , (F, ) |=Ki1 Kik holds iff|= kk defined inductively follows:1 = (V Oi1 )( );j < k,j+1 = (V Oij+1 )( j ).Proof: show proposition induction nested depth knowledge operations. base case implied directly Proposition 16. Assume claim holdscases nested depth k, want show also holds nested depthk + 1, i. e. ,(F, ) |= Ki1 Kik+1 iff |= k+1 .Proposition 16,(F, ) |= Ki1 Kik+1 iff (F, S1 ) |= Ki2 Kik+1 .inductive assumption,(F, S1 ) |= Ki2 Kik+1 iff |= k+1 .689fiSu, Sattar, Lv, & ZhangCombining two assertions above, get(F, ) |= Ki1 Kik+1 iff |= k+1 .2consider case nested depth knowledge operators2, get following corollary.Corollary 18 Let V, F, , Proposition 17. Then, agentagent j,1. (F, ) |= Ki holds iff|= ( (V Oi )( )) ;2. (F, ) |= Kj Ki holds iff|= ( (V Oi )( (V Oj )( ))) .Proof:Immediately Proposition 17. 2illustrated analysis security protocols (i.e. Section 6), part 2Corollary 18 useful verifying protocol specifications nested knowledge operators.Given background knowledge base , face task testing whether Kj Ki holdsstates satisfying , part 2 Corollary 18, first get 1 = (V Oj )( ),strongest necessary condition Oj . know agentj observes . compute 2 = (V Oi )( 1 ), i. e. , strongest necessarycondition 1 Oi is, viewpoint agent j, agent observes.way, task checking Kj Ki reduced task checking 2 .following corollary gives two methods check truth Ki (wherepropositional formula) states given formula true. One viastrongest necessary condition via weakest sufficient condition.Corollary 19 Let V finite set propositional variables F = (V, {}, O1 , , )knowledge structure n agents, two formulas V . Suppose denotesset states F (F, s) |= , SN Ci W SCi strongestnecessary condition Oi weakest sufficient condition Oi {}respectively.1. (F, ) |= Ki iff |= ( ) W SCi ;2. (F, ) |= Ki iff |= ( SN Ci ) .Proof: first part corollary follows Theorem 13 Lemma 10,second part follows immediately Proposition 16. 2analysis security protocols, observe often, seems efficientcheck agents knowledge via second part Corollary 19 rather via firstpart. may always true applications (e.g. see examplemuddy children puzzle next section).690fiVariable Forgetting Reasoning Knowledge4. Common KnowledgeCommon knowledge special kind knowledge group agents, playsimportant role reasoning knowledge (Fagin et al., 1995). group agentscommonly know agents know , know know ,know know know , ad infinitum. recallcommon knowledge characterized terms Kripke structures. Given Kripkestructure = (W, , K1 , , Kn ), group agents commonly know ( modallogic language, C true ) world w iff true worlds w0 (w, w0 ) C ,C denotes transitive closure Ki .section, generalize concept weakest sufficient strongest necessaryconditions used compute common knowledge.4.1 Generalized Weakest Sufficient Strongest Necessary Conditionsfollowing gives generalized notion weakest sufficient conditions strongest necessary conditions.Definition 20 Given set formulas V background knowledge base. Letformula V , V nonempty set subsets V .formula called V-definable (or simply called V-definableconfusion context), P V, formula P P|= P .formula called V-sufficient condition V-definable|= . called weakest V-sufficient conditionV-sufficient condition , V-sufficient condition 0, |= 0 .Similarly, formula called V-necessary condition V-definable|= . called strongest V-necessary conditionV-necessary condition , V-necessary condition 0, |= 0 .notice notion V-definability introduced simple elaborationnotion V-definability given work Lang Marquis (1998): V-definableiff V -definable V V. Moreover, easy seeformulas implied inconsistent exactly formulas -definable ,definability exhibits monotonicity property: V -definable ,V 0 -definable superset V 0 V (Lang & Marquis, 1998). Observe alsoV -definable iff V -definable , extends triviallyV-definability.following lemma says notions weakest V-sufficient conditions strongestV-necessary ones dual other.Lemma 21 Given set formulas V background knowledge base, Vset subsets V . Let formulas V . Then, weakest691fiSu, Sattar, Lv, & ZhangV-sufficient condition iff strongest V-necessary condition.Proof:Straightforward duality WSCs SNCs. 2give intuition motivation definition, let us consider following example.Example 22: Imagine two babies, say Marry Peter, playingdog. Suppose propositions dog moderately satisfied (denoted m, short)dog full(f ) understandable Marry, propositions doghungry (h) dog unhappy(u) understandable Peter.Let = {h u, (m f ), (m f ) h}, V1 = {m, f }, V2 = {h, u}, V = {V1 , V2 }.show1. h V-definable ;2. h weakest V-sufficient condition u ;3. h strongest V-necessary condition u .first claim easy check definition. last two claims follow immediatelyprove V-definable propositions f alse, true, h h (uplogical equivalence ). 8 propositions V1 logical equivalence.8 propositions are: true, f alse, m, m, f, f, f, f . Similarly, 8propositions V2 logical equivalence , i.e., true, f alse, h, h, u, u, hu, h u. However, find, two classes propositions, 4 pairsequivalence relations , i.e., |= true true, |= f alse f alse, |= (m f )h, |= (m f ) h. Therefore, V-definable propositions f alse,true, h h (up logical equivalence ). 2Example 23: recall background knowledge CS communicationscenario Alice Bob introduction section. CS set followingthree formulas:Bob recv msg Alice send msgBob send ack Bob recv msgAlice recv ack Bob send ackLetOA = {Alice send msg, Alice recv ack},OB = {Bob recv msg, Bob send ack},VAB = {OA , OB }.Clearly, formula logically implied CS inconsistent CS ,VAB -definable CS . Moreover, Example 22, able checkVAB -definable formulas implied CS inconsistent CS .Therefore, given formula , weakest VAB -sufficient condition CS impliedCS CS |= , inconsistent CS . 2692fiVariable Forgetting Reasoning KnowledgeLet set formulas, V set propositional variables, V set subsetsV . following proposition gives existence weakest V-sufficient strongestV-necessary conditions. given formula V , weakest V-sufficient condition 1strongest V-necessary condition 2 obtained proposition. Indeed,set assignments satisfying 1 assignments satisfying 2 giventerms relation EV .Proposition 24 Given finite set V propositional variables, set formulas Vbackground knowledge base, formula V , set V subsets V . Denote0SWSC set assignments V |= , assignments satisfying00(s, ) EV , |= . Also denote SSN C set assignments V|= , exists s0 s0 |= , s0 |= (s, s0 ) EV . Then, followingtwo points hold.formula satisfied exactly assignments SWSC , formulaweakest V-sufficient condition ;formula satisfied exactly assignments SSNC , formulastrongest V-necessary condition .Proof: first prove former point, show Lemma 21. Let 1propositional formula V that, assignments s, |= 1 iff SWSC .Then, every assignment SW SC , |= (s, s) EV . Thus, 1 |= .remark arbitrarily given formula V assignment V , |=(V P ) iff assignments s0 V P = s0 P , s0 |= .prove 1 V-definable, show that, P V, 1 |= (V P )1 ,implies 1 equivalent formula (V P )1 P . prove 1 |= (V P )1 ,0semantical way, suffices show that, every assignment SWSC |= ,000P = P , SW SC . Let given suppose P = s0 P .Then, (s, s0 ) EV . Given assignment |= , (s0 , t) EV , (s, t) EV(s, s0 ) EV . Thus, s0 SWSC . proves 1 V-definable.show 1 weakest V-sufficient condition . Suppose Vdefinable sufficient condition , want prove |= 1 .semantical argument proof follows. Let assignment |=00, must show SWSC , i.e., every assignment |=000(s, ) EV , |= . |= , suffices show |= . condition(s, s0 ) EV , finite sequence assignments s0 , , sk sj |= s0 =sk = s0 , every j < k, (sj , sj+1 ) EV . V-definability , knowevery j < k, sj |= implies sj+1 |= . Thus, s0 |= induction.prove second point proposition Lemma 21. Let 2 propositional formula V that, assignments s, |= 2 iff SSNC . Letconjunction formulas . Then, |= 2 iff assignments s0 s0 |=sEV s0 , s0 |= . Thus, first point proposition,2 weakest V-sufficient condition . Thus, 2 hence 2 strongestV-necessary condition according Lemma 21. 2proposition thought semantical characterization weakestV-sufficient strongest V-necessary conditions.693fiSu, Sattar, Lv, & Zhang4.2 Characterizations Least Greatest Fixed Pointsinvestigate computation weakest V-sufficient strongest V-necessary conditions using notions least greatest fixed points operator,introduced follows. Let V set propositional variables, operator (ormapping) set propositional formulas V set propositional formulasV . say fixed point , |= () . say 0 greatest fixedpoint , 0 fixed point every fixed point , |= 0 .Clearly, two greatest fixed points logically equivalent other. Thus, denotegreatest fixed point gfpZ(Z). Similarly, say 0 least fixed point ,0 fixed point every fixed point , |= 0 . denoteleast fixed point lfpZ(Z). say monotonic, every two formulas 12 |= 1 2 , |= (1 ) (2 ). finite set V propositionalvariables monotonic, exists least fixed point greatest fixed point(Tarski, 1955).Theorem 25 Let V finite set variables, F = (V, {}, O1 , , ) knowledgestructure, formula V , {1, , n}, V = {Oi | }. Assume 12 two operators1 (Z) =^(x Oi )( Z)2 (Z) =_(x Oi )( Z).Then,weakest V -sufficient condition {} equivalent gfp Z( 1 (Z));strongest V -necessary condition {} equivalent lfp Z( 2 (Z)).weakest V -sufficient condition {}. NoteProof: Let W SCoperator ( 1 (Z)) monotonic thus exists greatest fixed point it. Let1 = gfp Z( 1 (Z)). prove first point theorem, must show.|= W SC1. purpose, need provefirst show |= W SC1( (true));1. |= W SC1, |= W SC ( ()).2. formulas V , |= W SC1first point trivially true 1 (true) equivalent true W SCsufficient condition {}. show second point, suppose |= W SC .. Then, |= ., let formula Oi |= W SCfollows |= ( ) hence |= (V Oi )( )depend variables (V Oi ). So, that, , |= W SC(V Oi )( ). conclusion second point follows immediately., |= ( ) W SC . suffices showshow |= 1 W SC11 V -sufficient condition {}, is,694fiVariable Forgetting Reasoning Knowledge1. 1 V -definable;2. |= ( 1 ) .fact 1 fixed point operator ( 1 (Z)),|= 1 (^(x Oi )( 1 )).follows |= 1 , hence |= ( 1 ) . show point,, need prove 1 equivalent formula Oi . above,1 (V Oi )( 1 ). follows |= ( 1 ) (V Oi )( 1 ),hence|= ( 1 ) (V Oi )( 1 )|= (V Oi )( 1 ) ( 1 ) holds trivially. Thus ( 1 ) equivalent(V Oi )( 1 ), Oi . completes first pointconclusion theorem.show second point theorem using first point Lemma 21.strongest V -necessary condition {}. Lemma 21,Let SN CSN C weakest V -sufficient condition {}. Thus, first pointequivalent gfp Z( (Z)) . Hence, SN Ctheorem, SN C1equivalent gfp Z( 1 (Z)) . However, gfp Z( 1 (Z)) logicallyequivalent lfp Z(( 1 (Z))), turn equivalent lfp Z( 2 (Z)).completes second point theorem. 24.3 Common Knowledge Weakest V-sufficient ConditionsGiven set agents family V observable variable sets agents,investigate relationship common knowledge weakest V -sufficientstrongest V -necessary conditions.Theorem 26 Let V finite set variables, F = (V, , O1 , , ) knowledge struc SN Cture, {1, , n}, V = {Oi | }, formula V , W SCweakest V -sufficient condition strongest V -necessary conditionrespectively. Then, every state F,(F, s) |= C W SC(F, s) |= C SN C.,Proof: show first part theorem, i.e., (F, s) |= C W SCsufficientLemma 21 get part immediately. W SCcondition , |= W SC . Let conjunction formulas), leads (F, s) |= C W SC C (by, |= (W SCV -definable, have, point 4 Lemma 10,point 6 Lemma 10). W SCC W SC . Hence, (F, s) |= W SC C .(F, s) |= W SC695fiSu, Sattar, Lv, & Zhang, consider formulashow direction (F, s) |= C W SC1proof Theorem 25, i.e., greatest fixed point operator(Z) =^(V Oi )( Z).Theorem 25, suffices show (F, s) |=already (F, s) |= 1 W SCC 1 . greatest fixed point 1 operator obtainedfinite iteration operator starting point (true), need prove1. F |= C (true);2. arbitrary propositional formula V , F |= C , F |= C().first point trivially true (true) equivalent . prove second,suppose F |= C . Then, , F |= Ki (C ). Thus,F |= C Ki points 5 7 Lemma 10. Hence, F |= C (V Oi )( )V(by Corollary 14). follows F |= C (V Oi )( ) hence F |=C (). thus get F |= C 1 . completes proof. 2Proposition 27 Given V , F, , V , defined Theorem 26. Let formula. DenoteV . Assume strongest V -necessary condition SN Cset states F (F, s) |= , SSN C set states. Then,(F, s) |= SN C(F, ) |= C iff (F, SSN C ) |= .Proof: Let S1 set states state s0 s0 |=(s0 , s) V . (F, ) |= C iff every S1 , (F, s) |= . leads(F, ) |= C iff (F, S1 ) |= . hand, Proposition 24,S1 = SSN C . conclusion proposition follows immediately. 2Note that, Proposition 27, propositional formula, (F, ) |= Ciff |= SN C. Moreover, Theorem 26, (F, ) |= C iff |=weakest V -sufficient .W SC , W SC5. Adding Public Announcement Operatorrecent trend extending epistemic logic dynamic operators evolution knowledge expressed. basic extension public announcementlogic (PAL), obtained adding operator truthful public announcements.original version PAL proposed Plaza (1989). section, showpublic announcement operator conveniently dealt via notion knowledgestructure.696fiVariable Forgetting Reasoning Knowledge5.1 Public Announcement LogicGiven set agents = {1, . . . , n} set V propositional variables. languagepublic announcement logic (P ALn ) inductively defined::= p|| |Ki |C |[]p V , A.words, P ALn obtained epistemic logic LCn (V ) adding public announcement operator [] formula . Formula [] means public announcement , formula true.give semantics public announcement logic Kripke model. GivenKripke structure = (W, , K1 , . . . , Kn ), semantics new operators definedfollows.M, w |= [] iff M, w |= implies | , w |= , | Kripke structure| = (W 0 , 0 , K10 , . . . , Kn0 )W 0 = {w W |M, w |= },0 (w0 )(p) = (w0 )(p) w0 W 0 p V ,Ki0 = Ki (W 0 W 0 ) A.sentences become false immediately announcementthem. Consider, example, sentence p true commonly knowntrue . announcement sentence agents learn p therefore pcommonly known. modelled public announcement logic valid formula[], = p C p. see validity, let (M, w) arbitrary situation.M, w |= ,then M, w |= p, implies | , w |= C p, therefore | , w |= .5.2 Semantics Knowledge Structuresemantics public announcement logic conveniently characterized notionknowledge structure. define satisfaction relationship |= scenario (F, s)formula P ALn . need consider formulas form []; casesDefinition 9.Let V finite set propositional variables F = (, V, O1 , , ). semanticsdefinition new operators follows. First, let F| knowledge structure({}, V, O1 , , ), propositional formula V (F, s) |= iffsatisfies . V finite set, propositional formula always exists.Then, set (F, s) |= [] iff (F, s) |= implies (F| , s) |= .remark formula equivalent propositional one 0 knowledge structureF, i.e., F |= 0 propositional formula 0 , simply define F|( {0 }, V, O1 , , ).following proposition indicates semantics public announcement logicknowledge structure coincides Kripke model.697fiSu, Sattar, Lv, & ZhangProposition 28 (1) Let V finite set propositional variables F = (, V, O1 , , ).every state F every formula P ALn , (F, s) |= iff situation (M (F), s) |= . (2) finite S5n Kripke structure possible world w, knowledge structure FM state sw F that, every formulaP ALn , (FM , sw ) |= iff (M, w) |= .Proof: (1) Let us proceed induction structure formula . considercase form []; cases straightforward definitions.definition, (F, s) |= [] iff (F, s) |= implies (F| , s) |= .Thus, inductive assumption, (F, s) |= [] iff (M (F), s) |= implies(M (F| ), s) |= . want show (F, s) |= [] iff (M (F), s) |= []. sufficesshow (F| ) equals (F)| (M (F), s) |= [] iff (M (F), s) |= implies(M (F)| , s) |= .First, set possible states (F| ) equals set states s0 F(F, s0 ) |= . inductive assumption, (F, s0 ) |= iff (M (F), s0 ) |= . Thus, setpossible states (F| ) equals set states s0 F (M (F), s0 ) |= ,hence equals set possible states (F)| . Second, s0 F(M (F), s0 ) |= , (F| ) (s0 ) = s0 (F )| (s0 ) = (F ) (s0 ) = s0 . Hence (F | ) =(F )| . Finally, states s1 s2 F (M (F), s1 ) |= (M (F), s2 ) |= ,(F| )(F )(s1 , s2 ) Kiiff (s1 , s2 ) Kiiff s1 Oi = s2 Oi . Moreover,(F )|(F | )(F )|. completes(s1 , s2 ) Kiiff s1 Oi = s2 Oi . Therefore, Ki= Kiproof (F| ) = (F)| .(2) Suppose = (W0 , 0 , R1 , , Rn ), W0 finite set R1 , , Rnequivalence relations. assume also set propositional variables V0 .Let O1 , , sets new propositional variables1. O1 , , finite pairwise disjoint;2. (0 < n), number subsets Oi lessequivalence classes Ri .latter condition, is, i, function gi : W0 7 2Oiw1 , w2 W0 , gi (w1 ) gi (w2 ) subset Oi iff w1 w2equivalence class Ri .Let V = V0 0<in Oi . define function g : W0 7 2V follows. possibleworld w W0 ,[g(w) = {v V | (w)(v) = true}gi (w).0<infollowing two claims hold:C1 w1 , w2 W0 , (0 < n), g(w1 ) Oi = g(w2 ) Oi iffw1 Ri w2 .C2 w W0 v V0 , v g(w) iff (w)(v) = true.W W0 , letW = { | V, g(w) |= w W }.698fiVariable Forgetting Reasoning Knowledgeget knowledge structureFW = (V, W , O1 , , ).show following claim:C3 every V , state FW iff = g(w) w W.part claim C3 easy prove. = g(w0 ) w0 W ,definition W , g(w0 ) |= W hence g(w0 ) state FM . showpart, assume every w W , 6= g(w). Then, every w W , existsVw V |= w g(w) |= w . Therefore, |= wW w . Moreover,WWthat, every w0 W , g(w0 ) |= wW w , hence wW w W . Consequently,6|= W hence state FW .complete proof second part, suffices show, every P ALn ,(FW , g(w)) |= iff (M |W , w) |= , |W Kripke structure | =(W, , R10 , . . . , Rn0 )(w)(p) = 0 (w)(p) w W p V0 ,Ri0 = Ri (W 0 W 0 ) 0 < n.claims C1, C2 C3, induction . Again, considercase form []; cases dealt wayproof Proposition 12.first show knowledge structure FW | equivalent FW 0 ,W 0 = {w0 W | MW , w |= }.two knowledge structures set V propositional variables and,agent i, set Oi observable variables agent i, need proveset states. assignment V state FW | iff stateFW FW , |= . Thus, claim C3, state FW | iff = g(w0 ) w0 WFW , g(w0 ) |= . hand, have, claim C3 again, assignmentstate FW 0 iff = g(w0 ) w0 W 0 , i.e., w0 W MW , w0 |= . However,induction assumption, FW , g(w0 ) |= iff MW , w0 |= . Therefore, knowledge structuresFW | FW 0 set states.show (FW , g(w)) |= [] iff (M |W , w) |= [], have, induction assumption, (FW , g(w)) |= iff (M |W , w) |= . Also, claim provedabove, (FW | , g(w)) |= iff (FW 0 , g(w)) |= . induction assumption again, (FW 0 , g(w)) |= iff MW 0 , w |= . definition W 0 ,MW | , w |= . Hence, (FW | , g(w)) |= iff MW | , w |= . Therefore, semanticsannouncement operators Kripke structure knowledge structure,(FW , g(w)) |= [] iff (M |W , w) |= []. 2proposition generalization Propositions 11 12 PALn ,shows satisfiability issue formula language multi-agent S5announcement operators whatever satisfiability meant w.r.t. standardKripke structure w.r.t. knowledge structure.Notice that, every formula P ALn , get equivalent propositional formula.specifically, following:699fiSu, Sattar, Lv, & ZhangRemark 29 Let V finite set propositional variables F = ({}, V, O1 , , ).Given formula P ALn , define propositional formula inductionstructure :propositional formula, = .b e = .bKi e = (V Oi )( ).Let {1, , n}, V = {Oi | }.bC e = W SCW SCweakest V -sufficient condition .b[]e = bebeThen, every P ALn , F |= .6. Complexity Resultsinterested following problem: given knowledge structure F formulalanguage epistemic logic, whether formula realized structure F. kindproblem called realization problem. section, examine inherent difficultyrealization problem terms computational complexity. general case,problem PSPACE-Complete; however, interesting subset language,reduced co-NP.Let L epistemic logic (or language). realization problem L is, givenknowledge structure F formula L, determine whether F |= holds.realization problem closely related model checking problem: givenepistemic formula Kripke structure , determine whether |= . checkingdefinition Kripke structure semantics epistemic logic, see modelchecking problem solved polynomial time (with respect input size (| |+ | |). determine whether formula realized knowledge structure Ffirst translating knowledge structure F Kripke structure checking |= .However, resulting algorithm exponential space. sizecorresponding Kripke structure exponential respect knowledge structureF.number algorithms model checking epistemic specifications computational complexity related realization problems studied (van der Meyden,1998). However, like Kripke structure, semantics framework adopt listglobal states explicitly. result, size input concerned decision problemlarge.Proposition 30 realization problem Ln PSPACE-complete.700fiVariable Forgetting Reasoning KnowledgeProof: proposition two parts: PSPACE-easiness PSPACE-hardness.PSPACE-easiness part means algorithm determines polynomialspace whether epistemic formula Ln realized knowledge structure F.PSPACE-completeness indicates PSPACE-hard problem, say satisfiabilityproblem quantified propositional formulas (QBF) (Stockmeyer & Meyer, 1973),effectively reduced realization problem consider.difficult see PSPACE-easiness. Given knowledge structure epistemicformula , Corollary 14, replace knowledge modalities propositional quantifiersformula . So, problem whether realized F reduced determinewhether quantified Boolean formulas valid. latter done polynomial space(Stockmeyer & Meyer, 1973).PSPACE-hardness, suffices show every QBF formulap1 q2 p2 q3 pm1 qm A(p1 , q2 , p2 , q3 , pm1 , qm ),construct knowledge structure F` p1 q2 p2 q3 pm1 qm A(p1 , q2 , p2 , pm1 , qm )iffF |= d1 d2 (K1 K2 )m1 (dm A(p1 , q2 , p2 , q3 , pm1 , qm )).Let F = (V, {}, O1 , O2 ),1. V = {c} {d1 , , dm } {d01 , , d0m } {p1 , , pm } {q1 , , qm }2. conjunction following formulas(a)^(dj+1 dj ) (d0j+1 d0j )j<m(b)^^dj dj+1j<m(pi qi )i6=j(c)^c(dj d0j )j<m+1(d)c (dm1 dm ) d0m^j<m13. O1 = {c} {d1 , , dm } {q1 , , qm }4. O2 = {d01 , , d0m } {p1 , , pm }701(dj dj+1 ) (d0j+1 d0j+2 )fiSu, Sattar, Lv, & Zhangpicture, two agents: agents 1 2. assign every stateinteger number, called depth state convenience. every j, dj expressesdepth state least j. Propositions d1 , , dm observable agent1, agent 2. Nevertheless, agent 2 observe d01 , , d0m , closelyrelated d1 , , dm . formula item 2c indicates d01 , , d0md1 , , dm c holds, formula item 2d says that, c hold, depthexpressed d1 , , dm less d01 , , d0m difference 1. formulaitem 2b implies that, condition depth state exactly j,pj unobservable agent 1 qj unobservable agent 2.order show` p1 q2 p2 q3 pm1 qm A(p1 , q2 , p2 , pm1 , qm )impliesF |= d1 d2 (K1 K2 )m1 (dm A(p1 , q2 , p2 , q3 , pm1 , qm )),suffices prove that, every j propositional formula p1 , , pm ,q1 , , q ,F |= dj dj+1 pj qj+1 K1 K2 (dj+1 dj+2 )so, need showF |= dj dj+1 pj K1 (dj dj+1 )F |= dj dj+1 qj+1 K2 (dj+1 dj+2 ).direction, notice that, l < 1,F |= d1 d2 (K1 K2 )l dl+2 .also notice that, 1 < m0 m,F |= K1 K2 dm0 dm0 1F |= dm0 1 dm0 K1 K2 (dm0 ) pm0 1 qm0 .applying three claims repeatedly, obtainF |= d1 d2 (K1 K2 )m1 (dm ) p1 q2 p2 q3 pm1 qm .Therefore,F |= d1 d2 (K1 K2 )m1 (dm )p1 q2 p2 q3 pm1 qm satisfiable F d1 d2 .However, QBF formula p1 q2 p2 q3 pm1 qm contain free variable, immediately conclude QBF formula valid QBF formulasatisfiable F. 2702fiVariable Forgetting Reasoning KnowledgeRemark 29, see that, language formulas P ALn without commonknowledge operators, realization problem reduced problem validnessproblem quantified Boolean formulas, hence PSPACE-complete Proposition 30.conjecture realization problem also PSPACE-complete LCn P ALn .Proposition 30 indicates realization problem general case hardcomputer solve. Thus, interesting give special cases lower computationalcomplexity. Let L+Kfragment positive formulas Ln . consists formulasnnegation applied propositional formulas modalitiesrestricted K1 , , Kn . instance, formula K1 K2 pK1 K2 p (where p propositionalformula) belongs L+Kn , formula K1 K2 p K1 K2 p not.sublanguage L+Kinteresting sufficient represent importantnsecurity properties security protocols. Moreover, shown following proposition,complexity realization problem L+Kco-NP-complete.nProposition 31 realization problem L+Kco-NP-complete.nProof: well-known validity problem propositional formulas co-NPcomplete. easily get co-NP-hardness realization problem L+Kn ,validity problem propositional formulas reduced realization problempropositional formulas (considering case background knowledge basetautology).co-NP, showhand, show realization problem L+Knreduced validity problem propositional formulas. Given knowledge structureF formula L+Kn , translate propositional formula kkF (whichdefine below), realized F iff kkF valid, backgroundknowledge base knowledge structure F.Suppose F = (V, {}, O1 , , ). every subformula Ki , introduce setVi new propositional variables | Vi |=| V Oi |.propositional translation kkF inductively given follows.1. propositional formula, kkF = .2. conjunction form 1 2 ,kkF = k1 kF k2 kF .3. disjunction form 1 2 ,kkF = k1 kF k2 kF .4. form Ki ,kkF = ( kkF )(V Oi),Vi( kkF )( V VO) formula obtained ( kkF ) replacingvariables V Oi new ones Vi .703fiSu, Sattar, Lv, & Zhangidea behind translation first translate formula quantifiedpropositional formula, quantifiers universal ones, eliminateuniversal quantifiers introducing new variables.Let V set new variables kkF . show correctness translation,suffices show F |= V kkF .prove claim induction .trivial, propositional formula.form 1 2 , claim obtained immediately inductionassumption.form 1 2 , V (k1 kF k2 kF ) logically equivalentV1 k1 kF V2 k2 kF , variables V1 appear V2 k2 kFV2 V1 k1 kF . Thus, claim holds induction assumption.Finally, form Ki ,kkF = ( kkF )(V Oi).ViTherefore, V = V Vi V kkF logically equivalent ( Vi V kkF )( V VO).Thus, induction assumption,F |= V kkF ( Vi (V Oi)VihenceF |= V kkF ( (V Oi )).Therefore, F |= V kkF Ki . 2Proposition 31 implies that, arbitrary formula L+Kknowledge structurenF background knowledge base ,F |= iff kkF unsatisfiable.Thus, solve realization problem formulas L+Kusing propositionalnsatisfiability solver.7. Case Study: Muddy Children Puzzlesection, demonstrate framework applied practical problemsusing example muddy children puzzle.704fiVariable Forgetting Reasoning Knowledge7.1 Muddy Children Puzzlemuddy children puzzle well-known variant wise men puzzle. story goesfollows (Fagin et al., 1995): Imagine n children playing together. children,say k them, get mud foreheads. see mud othershis/her forehead. Along comes father, says, least one mudforehead. father asks following question, over:know whether mud forehead?Assuming children perceptive, intelligent, truthful, answer simultaneously, want show first (k 1) times father asks question,say k th time children muddy foreheads answer Yes.7.2 Modeling Muddy Children Puzzlemodel muddy children puzzle, let mi propositional variable, meanschild muddy (i < n). Denote V set {mi | < n}. Suppose assignments0 = {mi | < k} represents actual state: child 0, , child k 1 mudforeheads; children not. captured scenario (F0 , s0 ),F0 = (V, 0 , O0 , , On1 )V = {mi | < n};0 = ;Oi = V {mi } < n.VLet = i<n Ki mi , indicates every child know whethermud forehead. convenience, introduce, natural number l,notations []l []0 = []l+1 = [][]l . properties want showformally expressed P ALn :W[W[ji<n mi ][]every 0 j < k 1,k1 Vi<n mi ][]i<kKi mi .WFormula [ i<n mi ][]j means children say j + 1th timefather asks question. particular, j = 0, condition 0 j < k 1 simplifiedWWk > 1; resulting formula [ i<n mi ] says father announces i<n miWVevery child says No. Formula [ i<n mi ][]k1 i<k Ki mi indicates k th timechildren muddy foreheads answer Yes.Therefore, want prove(F0 , s0 ) |=^[_mi ][]j [0j<k1 i<n_i<nmi ][]k1^K mi .i<kcheck above, basically follow definition P AL semantics knowledgestructure. checking process, series Fj (0 < j k) knowledge structuresconstructed F1 = F0 |W mi and, every j (0 < j < k), Fj+1 = Fj | .i<n705fiSu, Sattar, Lv, & ZhangFigure 1: Performances two algorithms muddy children puzzleSpecifically, that, step j k, getFj = (V, j , O0 , , On1 )Oi = V {mi } < n, j defined follows:Wstep 1: 1 = {i<n mi }.Vstep j + 1: Let b = i<n mi (j mi ). < n, Fj |=nKioibbmi (j mi ), Fj |= . Thus, may set j+1 = j .Therefore, suffices verify, 0 < j < k < n, (Fj , s0 ) |= Ki mi , < k,(Fk , s0 ) |= Ki mi .7.3 Experimental Resultsframework knowledge structure implemented using BDD library(CUDD) developed Fabio Somenzi Colorado University. Notice BDD-basedQBF solvers satisfiability problems among best solvers nowadays. However,experiments need compute represent serial Boolean functions(say j ), decision problems solved general QBF solver.check agents knowledge, implemented two different algorithms terms Part1 2 Corollary 19 Section 3, respectively. Algorithm 1, based part1 Corollary 19, seems much efficient Algorithm 2, based part 2Corollary 19, particular example. reason follows. clearmain task algorithms check whether (Fj , s0 ) |= Ki mi . However, Algorithm 1smethod compute s0 |= mi (j mi ), Algorithm 2 compute |= mi (js0 ) mi . main reason Algorithm 1 much efficient particularproblem clear: mi (j mi ) simply equivalent j ( false ). Assuming halfchildren muddy, Fig. 1 gives performances Pentium IV PC 2.4GHz,512RAM. figure, x-axis number children, y-axis CPUrun time seconds.706fiVariable Forgetting Reasoning Knowledgemuddy children puzzle famous benchmark problem reasoning knowledge resolved proof-theoretic semantical approaches (Baltag et al., 1998;Gerbrandy, 1999; Lomuscio, 1999). Proof-theoretic approaches depend efficient proversmulti-modal logics; semantical ones may suffer state-explosion problem.approach essentially semantic one, give syntactical compact wayrepresent Kripke structures using knowledge structures, hence may avoidstate-explosion problem extent.8. Application Verification Security Protocolssection, apply knowledge model security protocol verification. Securityprotocols set credits parties deal distribution cryptographickeys essential communication vulnerable networks. Authentication plays keyrole security protocols. Subtle bugs lead attack often found protocolsused many years. presents challenge prove correctnesssecurity protocol. Formal methods introduced establish prove whethersecure protocol satisfies certain authentication specification.8.1 Background Authentication ProtocolsAuthentication protocols aim coordinate activity different parties (usually referredprincipals) network. generally consist sequence message exchangeswhose format fixed advance must conformed to. Usually, principal takepart protocol run different ways, initiator responder ; often callprincipal different roles. often principal take part several protocolruns simultaneously different roles.designers authentication protocols must conscious mindmessage may intercepted someone malicious intention impersonatehonest principal. One key issues authentication ensure confidentiality,is, prevent private information disclosed unauthorized entities. Anotherissue avoid intruder impersonating principals. general, principalensure message receives created recently sent principalclaims sent it.Cryptography fundamental element authentication. message transmittedchannel without cryptographic converting called plaintext. intention cryptography transform given message form unrecognizable anyoneexcept intended receiver. procedure called encryption correspondingparameter known encryption key. encoded message referred ciphertext.reverse procedure called decryption uses corresponding decryption key.symmetric-key cryptography, also called secret-key cryptography, uses keyencryption decryption. asymmetric-key cryptography, also calledpublic-key cryptography, uses different keys encryption decryption. oneencryption public key generally available anyone. Correspondingpublic key private key, decryption owned one principal.707fiSu, Sattar, Lv, & Zhang8.2 Dolev-Yao Intruder Modelstandard adversary model analysis security protocols introducedDolev Yao 1983 commonly known Dolev-Yao model (Dolev & Yao, 1983).According model, set conservative assumptions made follows:1. Messages considered indivisible abstract values instead sequences bits.2. messages one principal principals must passadversary adversary acts general router communication.3. adversary read, alter redirect message.4. adversary decrypt message right keys,compose new messages keys messages already possesses.5. adversary perform statistical cryptanalytic attacks.Although model drawback finding implementation dependent attacks,simplifies protocol analysis. proved powerful modelingadversary (Cervesato, 2001) simulate possible attackers.8.3 Revised Needham-Schroeder ProtocolLowe (1996) pointed out, Needham-Schroeder protocol problem lackingidentity responder fixed small modification. However,clear revised version correct. approach provides method automaticallyprove correctness security protocols instead finding bugs usual analysistools security protocols.cryptography literature, revised Needham-Schroeder protocol describedfollows:1. B: {N a, A}Kb2. B A: {B, N a, N b}Ka3. B: {N b}KbB : notation sends B message B receives messageA. notation {M }K means encryption key K. Also, A, Bdenote principal identifiers; Ka, Kb indicate, respectively, Bs public keys.Moreover, N N b nonces newly generated unguessable valuesB, respectively, guarantee freshness messages.Two informal goals specifications protocol knows B knows saidN N fresh, B knows knows B said N b N b fresh .analyze protocol, introduce B local histories protocol:plays role initiator protocol, assumes B responsor,local history1. said {N a, A}KbA708fiVariable Forgetting Reasoning Knowledge2. sees {B , N a, N bA }Ka3. said {N bA }KbAsaid means sent message , message containing ;sees indicates receives got received messages; Bresponsor protocol local view; KbA N bA are, local view,responsors public key nonce, respectively.B plays role responsor protocol, assumes initiator,local history1. B sees {N aB , AB }Kb2. B said {B, N aB , N b}Ka3. B sees {N b}KbAB initiator protocol Bs local observations; KaB N aB are,Bs local view, initiators public key nonce, respectively.main point analysis agent involved protocol,agents real observations compatible so-called local history. example,initiator protocol, sees {B, N aB , N b}Ka , according localhistory protocol assumes B responsor protocol,responsors nonce N b, responsors view, initiators nonce N (see4th formula background knowledge below).Let us see framework reasoning knowledge appliedprotocol.variable set VRN consists following atoms:f resh(N a): Nonce N fresh.f resh(N b): Nonce N b fresh.role(Init, A): plays role initiator protocol.role(Resp, B): B plays role responder protocol.RespA = B: assumes responder protocol B.InitB = A: B assumes initiator protocol A.N aB = N a: B assumes partners nonce execution protocolN a.N bA = N b: assumes partners nonce execution protocolN b.said(B, N a): B said N sending message containing N a.said(A, N b): said N b.709fiSu, Sattar, Lv, & Zhangsees(B, {N a, A}Kb ): B sees {N a, A}Kb (possibly decrypting messages received.)sees(A, {B, N aB , N b}Ka ): sees {B, N aB , N b}Ka .background knowledge RN consists following formulas:sees(B, {N a, A}Kb )1. said(B, N a)role(Resp, B)f resh(N a)sees(A, {B, N aB , N b}Ka )2. said(A, N b)role(Init, A)f resh(N b)3.4.5.6.7.role(Resp, B)sees(B, {N a, A}Kb )said(B, N a)f resh(N a)InitB =N aB = N!role(Init, A)RespA = Bsees(A, {B, N aB , N b}Ka )N aB = Nsaid(A, N b)N bA = N bf resh(N b)!!role(Init, A)RespA = Brole(Resp, B)InitB =sees(B, {N a, A}Kb )said(B, N a)sees(A, {B, N aB , N b}Ka )said(A, N b)(role(Init, A) f resh(N a))(role(Resp, B) f resh(N b))Notice first two formulas required rationality agents B.formulas obtained automatically fixed set meta rules.obtain third fourth formulas comparing local history protocolsconditions appearing formulas. get fifth formula informally, considerlocal history conditions role(Init, A) RespA = B,1. said {N a, A}Kb2. sees {B, N a, N bA }Ka3. said {N bA }Kb .According local history, sees nonce N generated itself. Nsaid message {N a, A}Kb , thus B, inverse key Kb, must seemessage said N a. Similarly, see sixth formula holds. last formulafollows immediately definition protocol.710fiVariable Forgetting Reasoning Knowledgeset OA observable variables{f resh(N a), role(Init, A), RespA = B}.set OB observable variables B{f resh(N b), role(Resp, B), InitB = A}.consider knowledge structureF = (VRN , RN , OA , OB ).Let SpecA formal specification:!f resh(N a)said(A, N a)role(Init, A) KA KBf resh(N a)RespA = BSpecB formal specification:!f resh(N b)said(B, N b).role(Resp, B) KB KAf resh(N b)InitB =easy show that, states F,(F, s) |= SpecA SpecBdesired.mention that, original Needham-Schroeder protocol (Needham &Schroeder, 1978), second message B A: {N a, N b}Ka instead B A: {B, N a, N b}Ka .Therefore, fourth formula would changedrole(Init, A)sees(A, {N aB , N b}Ka )N aB = Nsaid(A, N b)N bA = N bf resh(N b)Thus, RespA = B necessarily hold conditionrole(Init, A) sees(A, {N aB , N b}Ka ) said(A, N b) f resh(N b).specifications SpecA SpecB hold original NeedhamSchroeder protocol.8.4 DiscussionBAN logic (Burrows, Abadi, & Needham, 1990) one successful logical toolsreason security protocols. However, semantics BAN always arguable,clear assumption rules BAN logic sound complete.711fiSu, Sattar, Lv, & Zhangmotivated research seeking adequate frameworks (models). Providing modeltheoretic semantics BAN logic central idea development BAN-likelogics (Abadi & Tuttle, 1991) SVO (Syversion & van Oorschot, 1996).advantage approach use knowledge structures semantic models verifycorrectness epistemic goals security protocols.important problem that, given security protocol, corresponding knowledge structure comes from. get knowledge structure correspondingsecurity protocol, developed semantic model, background knowledge basecorresponding knowledge structure consists formulas valid semanticmodel. Moreover, generate background knowledge systematically. ongoingwork implement approach promising automatic security protocol verifier.9. Related Worknumber approaches dealing concept variable forgetting eliminations middle terms (Boole, 1854) several contexts. notion variable forgettingformally defined propositional first order logics Lin Reiter (1994).recent years, theories forgetting answer set programming semantics proposed(Zhang & Foo, 2006; Eiter & Wang, 2008). Forgetting also generalized descriptionlogics (Kontchakov, Wolter, & Zakharyaschev, 2008; Wang, Wang, Topor, & Pan, 2008;Kontchakov, Walther, & Wolter, 2009).context epistemic logic, notion forgetting studied numberways. Baral Zhang (2006) treated knowledge forgetting special form updateeffect K K: knowledge forgetting , agent would neither know. Ditmarsch, Herzig, Lang Marquis (2008) proposed dynamic epistemiclogic epistemic operator K dynamic modal operator [F g(p)] formula[F g(p)] means agent forgets knowledge p, true. (Zhang &Zhou, 2008) modeled forgetting via bisimulation invariance except forgotten variable.notion variable forgetting closely related quantified modal logics,existential variable quantification modeled via bisimulation invariance exceptquantified variable (Engelhardt et al., 2003).notion variable forgetting various applications knowledge representationreasoning. example, Weber (1986) applied updating propositional knowledgebases. Lang Marquis (2002) used merging set knowledge bases simplytaking union may result inconsistency. notion variable forgetting alsoclosely related formula-variable independence, result forgettingset variables V formula defined strongest consequenceindependent V (Lang et al., 2003). recently, Liu Lakemeyer (2009) appliednotion forgetting situation calculus, obtained interesting resultsfirst-order definability computability progression local-effect actions.10. Conclusionmain contribution paper follows. First, investigated knowledgereasoning within simple framework called knowledge structure, consists global712fiVariable Forgetting Reasoning Knowledgeknowledge base set observable variables agent. notion knowledgestructure used semantic model multi-agent logic knowledge commonknowledge. model, computation knowledge common knowledgereduced operation variable forgetting; moreover, objective formula knownagent state weakest sufficient condition Oi holds state s.Second, capture notion common knowledge framework, generalized notion weakest sufficient conditions obtained, set V sets propositional variables, notion weakest V-sufficient conditions. Given set agentsfamily V observable variable sets agents, shown objectiveformula common knowledge agents iff weakest {Oi | }-sufficientholds. Also, shown public announcement operator conveniently dealtvia notion knowledge structure.Third, relationship S5 Kripke structure knowledge structureexplored. Specifically, satisfiability issue formula language multi-agentS5 public announcement operator satisfiability meant w.r.t.standard Kripke structure w.r.t. knowledge structure.Fourth, examined computational complexity problem whetherformula realized structure F. general case, problem PSPACE-hard;however, interesting subcases reduced co-NP.Finally, shown strength concept knowledge structure practical side empirical results satisfiability problem knowledge structuresbased instances muddy children puzzle, since even smallest instancesconsidered experiments generating corresponding S5 Kripke structure wouldreach. also discussed automated analysis verification correctedNeedham-Schroeder protocol via knowledge structures.work presented paper extended several directions. First,investigate whether knowledge structures extended used basisknowledge based programming (Fagin et al., 1995). Secondly, current frameworkknowledge structures, considered issue knowingextensively studied knowledge reasoning models (Halpern & Lakemeyer, 1996;van der Hock, Jaspars, & Thijsse, 2003; Levesque, 1990). interesting topicknowledge model handles knowing reasoning knowledge. Thirdly,notions methods work extended investigate extensionvariable forgetting operator multi-agent logics beliefs. Finally, recent researchshown knowledge update many important applications reasoning actionsplans dynamic modeling multi-agent systems (Zhang, 2003). first stepdirection (in mono-agent S5) found work Herzig, Lang Marquis(2003). Baral Zhang proposed general model performing knowledge updatebased standard single agent S5 modal logic (Baral & Zhang, 2001). believework extended multi-agent modal logics using knowledge structuredefined paper therefore develop general system knowledge update.Along direction, interesting research issue explore underlying relationshipknowledge forgetting - specific type knowledge update, variable forgettingaddressed paper.713fiSu, Sattar, Lv, & ZhangAcknowledgmentsauthors thank Ron van der Meyden, Fangzheng Lin anonymous reviewersvaluable comments earlier version paper. work partially supported Australian Research Council grant DP0452628, National Basic Research973 Program grants (Nos. 2010CB328103, 2009CB320701 2005CB321902), National Natural Science Foundation China grants (Nos. 60725207 60763004).paper revised extended version paper appeared Proceedings KR2004 (Su, Lv, & Zhang, 2004)ReferencesAbadi, M., & Tuttle, M. (1991). semantics logic authentication. ProceedingsTenth Annual ACM Symposium Principles Distributed Computing, pp.201216.Baltag, A., Moss, L., & Solecki, S. (1998). logic public announcements commonknowledge distributed applications (extended abstract). Proceedings TARKVII, pp. 4356.Baral, C., & Zhang, Y. (2001). semantics knowledge update. Proceedings17th International Joint Conference Artificial Intelligence (IJCAI-01), pp.97102.Baral, C., & Zhang, Y. (2006). Knowledge updates: semantic complexity issues. Artificial Intelligence, 164, 209243.Boole, G. (1854). Investigation Laws Thought. Walton, London.Burrows, M., Abadi, M., & Needham, R. M. (1990). logic authentication. ACMTransactions Computer Systems, 8 (1).Cervesato, I. (2001). Dolev-Yao intruder powerful attacker. Proc. 16thAnnual Int. Symp Logic Computer Science.Dolev, D., & Yao, A. (1983). security public-key protocols. CommunicationsACM, 29 (8), 198208.Eiter, T., & Wang, K. (2008). Semantic forgetting answer set programming. ArtificialIntelligence, 172, 16441672.Engelhardt, K., van der Meyden, R., & Moses, Y. (1998). Knowledge logic localpropositions. Proceedings TARK VII.Engelhardt, K., van der Meyden, R., & Su, K. (2003). Modal logics hierarchy localpropositional quantifiers. Advance Modal Logic, Vol. 4, pp. 930. Kings CollegePublications.Fagin, R., Halpern, J., Moses, Y., & Vardi, M. (1995). Reasoning knowledge. MITPress, Cambridge, MA.Gerbrandy, J. (1999). Bisimulation Plant Kripke. Ph.D thesis, Institute Logic,Language Computation, University Amsterdam.714fiVariable Forgetting Reasoning KnowledgeHalpern, J., & Moses, Y. (1992). guide completeness complexity modal logicsknowledge belief. Artificial Intelligence, 54, 319379.Halpern, J., & Zuck, L. (1992). little knowledge goes long way: Simple knowledge basedderivations correctness proofs family protocols. Journal ACM,39 (3), 449478.Halpern, J. Y., & Lakemeyer, G. (1996). Multi-agent knowing. Proceedings TARKVI, pp. 251265.Herzig, A., Lang, J., & Marquis, P. (2003). Action representation partially observableplanning using epistemic logic. Proceedings IJCAI-03, pp. 10671072.Hintikka, J. (1962). Knowledge Belief. Cornell University Press, Ithaca, NY.Kontchakov, R., Walther, D., & Wolter, F. (2009). Forgetting uniform interpolationlarge-scale description logic terminologies. Proc. IJCAI09.Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2008). tell differencedl-lite ontologies. Proc. KR08.Kripke, S. (1963). semantical analysis modal logic. i: Normal modal propositionalcalculi. Z. Math. Logik Grundl. Math., 9, 6796.Lang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence: Formulavariable independence forgetting. Journal Artificial Intelligence Research, 18,391443.Lang, J., & Marquis, P. (1998). Complexity results independence definability.Proc. 6th International Conference Knowledge Representation Reasoning,pp. 356367.Lang, J., & Marquis, P. (2002). Resolving inconsistencies variable forgetting. Proc.KR2002, pp. 239250.Levesque, H. (1990). know: study autoepistemic logic. Artificial Intelligence, 42,263309.Lin, F. (2001). strongest necessary weakest sufficient conditions. ArtificialIntelligence, 128, 143159.Lin, F., & Reiter, R. (1994). Forget it!. Greiner, R., & Subramanian, D. (Eds.), WorkingNotes AAAI Fall Symposium Relevance, pp. 154159, New Orleans.Liu, Y., & Lakemeyer, G. (2009). first-order definability computability progressionlocal-effect actions beyond. Proc. IJCAI09.Lomuscio, A. (1999). Knowledge Sharing among Ideal Agents. Ph.D thesis, SchoolComputer Science, University Birmingham.Lowe, G. (1996). Breaking fixing Needham-Schroeder public-key protocol usingFDR. Margaria, & Steffen (Eds.), Tools Algorithms ConstructionAnalysis Systems, Vol 1055 Lecture Notes Computer Science, pp. 147166.Springer Verlag.Needham, R. M., & Schroeder, M. D. (1978). Using encryption authentication largenetworks computers. Communication ACM, 21 (12), 993999.715fiSu, Sattar, Lv, & ZhangPlaza, J. (1989). Logics public communications. Proceedings 4th InternationalSymposium Methodologies Intelligent Systems, pp. 201216346.Stockmeyer, L., & Meyer, A. (1973). Word problem requiring exponential time: prelimnaryreport. Proc. 5th ACM Symp. Theory Computing, pp. 19.Su, K., Lv, G., & Zhang, Y. (2004). Reasoing knowledge variable forgetting.Proceedings KR-04, pp. 576586.Syversion, P. F., & van Oorschot, P. (1996). unified cryptographic protocol logic. Tech.rep. NRL Publication 5540-227, Naval Research Lab.Tarski, A. (1955). lattice-theoretical fixpoint theorem ans applications. Pacific J.Math., 5, 285309.van Benthem, J. (2001). Logics information update. Proceedings TARK-VIII, pp.5158.van der Hock, W., Jaspars, J., & Thijsse, E. (2003). Theories knowledge ignorance.S. Rahman, J. Symons, D. G., & van Bendegem, J. (Eds.), Logic, EpistemologyUnity Science. Kluwer.van der Hoek, W., & Wooldridge, M. (2002). Model checking knowledge time. Proc.19th Workshop SPIN (Model Checking Software), pp. 95111, Grenoble.van der Meyden, R. (1998). Common knowledge update finite environments. Information Computation, 140 (2), 115157.van Ditmarsch, H., Herzig, A., Lang, J., & Marquis, P. (2008). Introspective forgetting.Wobcke, W., & Zhang, M. (Eds.), AI 2008: Advances Artificial Intelligence, Vol.5360.van Ditmarsch, H., van der Hoek, W., & Kooi, B. (2005a). Dynamic epistemic logicassignment. Proceedings AAMAS-05, pp. 141148.van Ditmarsch, H., van der Hoek, W., & Kooi, B. (2005b). Public announcements beliefexpansion. Advances Modal Logic, Volume 5, pp. 335346.Wang, Z., R., Wang, K., Topor, R., & Pan, J. (2008). Forgetting dl-lite. Proc.ESWC08.Weber, A. (1986). Updating propositional formulas. Proc. First Conference ExpertDatabase Systems, pp. 487500.Zhang, Y. (2003). Minimal change maximal coherence epistemic logic programupdates. Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI-03), pp. 112117.Zhang, Y., & Foo, N. (2006). Solving logic program conflict strong weakforgettings. Artificial Intelligence, 170, 739778.Zhang, Y., & Zhou, Y. (2008). Properties knowledge forgetting.. Proceedings20th International Workshop Non-monoronic Reasoning ( NMR08), pp. 6875.716fiJournal Artificial Intelligence Research 35 (2009) 775-811Submitted 03/09; published 08/09Enhancing QA Systems Complex TemporalQuestion Processing CapabilitiesEstela SaqueteJose L. VicedoPatricio Martnez-BarcoRafael MunozHector Llorensstela@dlsi.ua.esvicedo@dlsi.ua.espatricio@dlsi.ua.esrafael@dlsi.ua.eshllorens@dlsi.ua.esNatural Language Processing Information System GroupDepartment Software Computing SystemsUniversity AlicanteApartado de Correos 99, E-03080 Alicante, SpainAbstractpaper presents multilayered architecture enhances capabilities currentQA systems allows different types complex questions queries processed.answers questions need gathered factual information scatteredthroughout different documents. Specifically, designed specialized layer processdifferent types temporal questions. Complex temporal questions first decomposedsimple questions, according temporal relations expressed original question.way, answers resulting simple questions recomposed, fulfillingtemporal restrictions original complex question. novel aspect approachresides decomposition uses minimal quantity resources, finalaim obtaining portable platform easily extensible languages.paper also present methodology evaluation decomposition questionswell ability implemented temporal layer perform multilingual level.temporal layer first performed English, evaluated compared with:a) general purpose QA system (F-measure 65.47% QA plus English temporal layervs. 38.01% general QA system), b) well-known QA system. Much betterresults obtained temporal questions multilayered system. systemtherefore extended Spanish good results obtained evaluation(F-measure 40.36% QA plus Spanish temporal layer vs. 22.94% general QAsystem).1. IntroductionNowadays, fact huge amount digital information available (mainlytextual form) also large number users want easiest possible accessinformation. situation continuously fosters research developmentinformation systems make possible analyze, locate, manage, access processinformation automatically. Commonly, systems referred searchengines.search engine especially useful obtain specific piece information withoutneed manually go available documentation related searchtopic. Search engines currently evolving towards new generation engines capablec2009AI Access Foundation. rights reserved.fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorensunderstanding user needs better (the necessity behind every query) offering specificservices interfaces, depending domain context. new generation searchengines able offer list ordered web pages, also discover piecesinformation scattered throughout different information sources even summaries (Barzilay,Elhadad, & McKeown, 2002). is, integrate information text search (webpages, documents), multimedia search (images, video, audio) database search (tourist,biomedicine, etc.) comprehensible answers delivered users. addition,correctly process questions formulated free natural language opposed keywordqueries fixed templates, information extraction scenarios (Michelson & Knoblock,2008). Question answering systems (QA) one best examples new generationsearch engines, allowing users formulate questions free natural language (NL)providing exactly information required, also NL form.However, QA mature technology current systems mainly focusedtreatment questions require specific items data answer dates,names entities quantities. capital Brazil? examplecalled factual questions. case, answer name city.long road towards next generation systems, work presented takesnew step forward. defines layer that, installed top current NL-based searchengines QA systems, enhances capabilities processing different types complextemporal questions.specific case temporal QA trivial task due potential complexitytemporal questions. Current search engines, operational QA systems dealsimple factual temporal questions, is, questions requiring date answer(When Bob Marley die?) questions involve simple temporal expressionsformulation (Who U.S. Open 1999?). Processing kinds questionsusually accomplished identifying explicit temporal expressions questionsrelevant documents contain temporal expressions order answer questions.However, system described paper also processes complex temporal questions.is, questions whose complexity related temporal properties entitiesenquired relative ordering events mentioned question. followingexamples complex temporal questions:spokesman Soviet Embassy Baghdad invasionKuwait?Bill Clinton currently President United States?approach present work tries imitate temporal reasoning humansolving types questions. example, person trying answer question:spokesman Soviet Embassy Baghdad invasion Kuwait?would proceed follows:1. First, complex question would decomposed two simple ones:spokesman Soviet Embassy Baghdad? invasionKuwait occur?.776fiEnhancing QA Systems Complex Temporal Question Processing Capabilities2. He/She would look possible answers first simple question:spokesman Soviet Embassy Baghdad?.3. that, he/she would look answer second question:invasion Kuwait occur?4. Finally, he/she would give final answer one answers first question (ifany) temporal compatibility answer second question.case, answer first question must temporally compatibleperiod dates associated invasion Kuwait (during).Therefore, logical approach treatment complex questions baseddecomposition questions simple ones resolved using conventionalQA systems. Finally, answers simple questions, fulfilling temporal constraints, wouldused construct answer original complex question.study presents development evaluation tool processes complexNL-temporal questions information retrieval purposes. Apart fact toolcapable processing type complex questions, following advantages:incorporated layer top one already existing QA systems.contain integrate answer different data obtained different typesinformation sources (web pages, databases, documents, etc.) retrievedusing different types search engines (QA, NLIDB1 , etc.).layer portable platform since language-dependent features processeasily extended languages.information necessary process complex question obtained directlyit, extra auxiliary questions annotations required.paper, main aim demonstrate temporal layer improvegeneral purpose QA system questions simple factual, higherdegree complexity. Specifically, implemented temporal layer order dealquestions different levels temporal complexity. Furthermore, proposedtreatment questions uses minimum quantity linguistic resources order obtainportable platform, easily extended different languages.paper structured following way: first all, section 2 briefly introduces current situation temporal reasoning QA; section 3 depicts proposalclassifying temporal questions four groups, depending features question; section 4 explains concept Multilayered QA system; section 5 describesdifferent modules temporal layer detail; section 6, decompositionquestion Multilayered QA system evaluated English. portabilitysystem languages described, procedure repeated evaluatedSpanish. Finally, conclusions comments future work made.1. Natural Language Interfaces Databases777fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens2. Backgroundexplained introduction, one aims paper process complex questions. Complex questions general dealt previous studies using differentapproaches decompose them. Harabagiu, Lacatusu Hickl (2006) presented procedure question produces lots queries semantically related originalquestion, main aim obtaining information answers. approach requires significant amount semantic information. question decompositionpresented Katz, Borchardt Felshin (2005) involves three decomposition techniques:a) syntactic decomposition using linguistic knowledge language-based descriptionsresource content, b) semantic decomposition using domain-motivated explanation patterns language-based annotations resource content, c) semantic decompositionquestions resource content lower-level assertions. approach makes useconsiderable amount linguistic knowledge order move new domains, newsets parameterized language-based annotations need composed. additionstudies dealing single focus complex questions, Lin Lui (2008) propose processingcomplex questions multiple foci obtaining one subquestion focusoriginal question. approach determines four possible relations subquestions derived original question. However, temporal relation consideredapproach.Apart complex questions treatment, motivation temporal aspectwork due great importance question answering field relating questionsinformation temporal dimension order find correct answer. Take,example, following two similar questions:president USA?president USA 1975?obvious dependency answers time, order obtain right answertwo questions, temporal information needs extracted processed,first question refers current president USA (the exact point timequestion formulated), whereas second one refers president 1975.temporal information explicit, questions considered complex temporalquestions.importance temporal dimension data information search processescorroborated recent interest shown major evaluation forums QA like TextREtrieval Conference - TREC (2008) Cross Language Evaluation Forum - CLEF (2008),see also works Voorhees (2002) Magnini et al. (2005), including different typestemporal questions part evaluation benchmarks.Furthermore, CLEF explicitly fostered research complex temporal questionsorganizing specific pilot task questions (Herrera, Penas, & Verdejo, 2005)including CLEF (Magnini et al., 2006) temporal dimension questions answerspart main QA task.temporal question appropriately processed by: (1) relating available information temporal dimension (2) adapting search link temporalinformation information search process.778fiEnhancing QA Systems Complex Temporal Question Processing CapabilitiesConcerning first task, analysis time challenging problem, needsapplications based information extraction techniques expand include varying degreestime stamping (identification reasoning) events expressions within narrativequestion. Interest temporal representation reasoning evolving throughout years resulted growing number meetings related topic.present here, descending chronological order, important ones: TIME (2008)annual symposium Temporal Representation Reasoning (Demri & Jensen, 2008),involves different areas including Time Natural Language; TempEval 2007 (Verhagenet al., 2007) workshop held within SemEval-2007 evaluation systems performing Time-Event Temporal Relation Identification; ARTE 2006 new workshop focusedAnnotating Reasoning Time Events (Ahn, 2006; Dalli & Wilks, 2006; Mani &Wellner, 2006) part relevant conference COLING-ACL (2006) (Pan, Mulkar,& Hobbs, 2006a); Dagstuhl 2005 seminar annotating, extracting reasoningtime events (Katz, Pustejovsky, & Schilder, 2005); TERN (2004) internationalcompetition different systems identify normalize temporal expressionsevaluated compared; TANGO 2003 specialized developing appropriate infrastructure annotation (Pustejovsky & Mani, 2008); LREC (2002) dedicatedworkshop Annotation Standards Temporal Information Natural Language (Mani& Wilson, 2002; Setzer & Gaizauskas, 2002; Saquete, Martnez-Barco, & Munoz, 2002);ACL (2001) included Temporal Spatial Information Processing workshop (Setzer &Gaizauskas, 2001; Filatova & Hovy, 2001; Katz & Arosio, 2001; Moia, 2001; Schilder & Habel, 2001; Wilson, Mani, Sundheim, & Ferro, 2001) finally, COLING (2000),papers related temporal expression identification temporal databases.important emphasize meetings led development standardspecification language events temporal expressions ordering (TimeML,2008). Nowadays, also growing number automatic systems extracting temporalexpression information2 , as: ATEL (2008), Chronos (Negri, 2007), TempEx (2008),GUTime (Mani & Wilson, 2000a), DANTE (Mazur & Dale, 2007), TimexTag (Ahn, 2006)TERSEO (Saquete, Munoz, & Martnez-Barco, 2006).Regarding second task, significant progress made temporal analysis applied IE QA tasks presented TERQAS workshop (Pustejovsky, 2002; Radev& Sundheim, 2002). purpose TERQAS workshop address problemenhance natural language question answering systems answer temporally-basedquestions events entities news articles. Besides, temporal question corpusdeveloped. far know, one first systems treated temporal information QA purposes described Breck et al. (2000) used temporal expressionidentification applying temporal tagger developed Mani Wilson (2000b). Another important study related temporal constraints question answering presentedPrager, Chu-Carroll Czuba (2004). presented method improve accuracyQA system asking auxiliary questions related original question whose answers used temporally verify restrict original answer. method calledQA-by-Dossier Constraints suitable TREC-style factoid questions,inconvenience requiring generation set auxiliary questions. Besides,2. http://timexportal.wikidot.com/systems779fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorensrecently, researchers also focused important features temporal reasoningfinal applications, as: a) event detection: Evita (Saur, Knippen, Verhagen, &Pustejovsky, 2005) application recognizing events natural language texts,recognition applied QA, b) event extension: Pan, Mulkar Hobbs (2006b) describe method automatically learn durations event descriptions, c) temporalrelations temporal expressions events, described Lapata Lascarides(2006).However, strategies implied complex temporal processing question,using information extracted original question small amount linguisticresources temporal reasoning beyond scope investigations.proposal focused temporal reasoning complex temporal questionsnecessary add new layer existing systems, thereby allowing complex questionsprocessed (Saquete, Martnez-Barco, Munoz, & Vicedo, 2004). decompositionperformed temporal layer based temporal relation eventsoriginal question, linguistic information required decomposition.addition, system identifies normalizes temporal expressions used partprocessing layer (Negri, Saquete, Martnez-Barco, & Munoz, 2006), taking advantagemultilingual feature system order use cross-lingual tasks.However, temporal questions need treated way sincemay different characteristics, reason, classification different typestemporal questions also proposed.3. Temporal Questions Taxonomyexplaining answer temporal questions, must classified differentcategories since way solve differ depending type question involved.temporality question depends two levels complexity: a) number eventsquestion: Questions formed single event whose answers founddocument (simple questions), questions formed one eventtemporally related whose answers could found multiple documents (complexquestions), b) temporal information appearing question, like implicitexplicit temporal expressions, needs recognized normalized. combinationtwo features results four different types temporal questions.Simple Temporal Questions:Type 1: Single event temporal questions without temporal expression (TE).questions require temporal expression answer contain temporalexpression formulation. questions formed single eventtemporal reasoning required, resolved QA system directly withoutpre postprocessing question. example: Jordan close portAqaba Kuwait?. However, since taxonomy temporal question taxonomy,type basic temporal questions need considered.Type 2: Single event temporal questions temporal expression. questionsrequire temporal reasoning temporal expression contained formulationquestion. single event question, one temporalexpressions need identified, normalized annotated. temporal infor780fiEnhancing QA Systems Complex Temporal Question Processing Capabilitiesmation necessary search correct answer, due fact establishingtemporal constraints candidate answers. example: 1988 NewHampshire Republican primary?. Temporal Expression: 1988Complex Temporal Questions:Type 3: Multiple event temporal questions temporal expression. Questionscontain one event, related temporal signal. signal establishes orderevents question. Moreover, one temporal expressionsquestion. temporal expressions need identified, normalized annotated,establish temporal constraints answers question. example:George Bush U.N. Security Council ordered global embargo tradeIraq August 90? example, temporal signal temporalconstraint 8/1/1990 8/31/1990. question consists two events:Event 1: George Bush somethingEvent 2: U.N. Security Council ordered global embargo trade Iraq(Temporal constraint: August 1990)Type 4: Multiple event temporal questions without temporal expression. Like Type 3,questions consist one event, related temporal signal,case, questions contain temporal expressions. temporal signal establishesorder events question. example: presidentUS AARP founded?. example, temporal signalquestion would decomposed into:Event 1: someone president USEvent 2: AARP foundationprocess type question explained detail following sections.4. Architecture Multilayered QA Systemorder process special types questions beyond scope currently QAsystems, work proposes multilayered architecture increases functionalityQA systems, allowing solve type complex question. work,temporal layer implemented. Moreover, architecture enables different layersadded cope questions need kinds complex processingtemporally oriented, script questions (How assemble bicycle?)template-based questions (What main biographical data Nelson Mandela?).Complex questions common need additional processing questionorder solve adequately. architecture presented paper enables differenttypes complex questions dealt superposing additional processing layers,one type, top existing general purpose QA system, shown Figure1. layers will:decompose question simple events generate simple questions (sub-questions)ordered according original question,781fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorenssend simple questions general purpose QA system,receive answers simple questions general purpose QA system,filter, compare validate sub-answers, according relation detected sub-questions, order construct final complex answer.Complex QuestionComplex AnswerINTERFACETEMPORALQUESTIONLAYERSCRIPTQUESTIONLAYERTEMPLATEQUESTIONLAYERSimple Questions...Simple AnswersSEARCH ENGINETextMultimediaDatabasesFigure 1: Multi-layered Architecture QA systemarchitecture large number advantages, followingmentioned:allows researchers use existing general purpose QA system.Since complex questions processed superior layer, necessary modifycurrent QA system want deal complex questions. layerenhances capabilities existing QA system without changing way.additional processing layer works independently others processes questions accepted layer.possible one type QA system working parallel,specialized searching specific type information (text,multimedia,databases).Next, layer oriented processing temporal questions according taxonomy shownsection 3 presented.782fiEnhancing QA Systems Complex Temporal Question Processing Capabilities5. Temporal Layertemporal layer proposed consists two units, Question Decomposition UnitAnswer Recomposition Unit, superposed general purpose QAsystem, shown Figure 2.ComplexQuestionComplexAnswerINTERFACETEMPORAL LAYERQUESTIONDECOMPOSITION UNITANSWERRECOMPOSITION UNITTEtagsTE IdentificationNormalizationIndividual AnswerFilteringType IdentificationAnswer ComparisonCompositionSignalQuestion SplitterQ-FocusQ-RestrictionQ-FocusAnswersQ-Rest.AnswerSEARCH ENGINETextMultimediaDatabasesFigure 2: Architecture temporal layercomponents work together order obtain final answer follows:Question Decomposition Unit preprocessing unit performs three main tasks.First all, temporal expressions question identified normalized. Secondly, following taxonomy shown section 3, different types questionstype must treated different way. reason, type needsidentified. that, complex questions (Type 3 4) split simpleones using temporal signal reference. first sub-question definedquestion focus (Q-Focus) specifies type information user needsfind. second sub-question called question restriction (Q-Restriction)answer sub-question establishes temporal restrictions listanswers Q-Focus. Q-Focus Q-Restriction input QAsystem. example, question Bill Clinton study going Oxford University?, divided two sub-questions related temporalsignal before: Q-Focus: Bill Clinton study? Q-Restriction:WhenBill Clinton go Oxford University?.783fiSaquete, Vicedo, Martnez-Barco, Munoz, & LlorensGeneral purpose QA system. simple questions generated processed general purpose QA system. QA system could used (QA systems, Multimediasearch engines NLIDB). example above, current QA system returnsfollowing answers:Q-Focus Answers:Georgetown University (1964-68)Oxford University (1968-70)Yale Law School (1970-73)Q-Restriction Answer: 1968Answer Recomposition Unit. unit constructs answer original question answers Q-Focus Q-Restriction using temporalconstraints, temporal signals (which fully explained later) temporalexpressions, available original question. temporal signal establishesappropriate order answers Q-Focus Q-Restrictionquestion. Finally, unit returns appropriate answer analyzing temporal compatibility list possible Q-Focus answers Q-Restrictionanswer.example temporal layer operates shown Figure 3.Bill Clinton study going Oxford University?Q-FocusQ-RestrictionBill Clinton study?Bill Clinton goOxford University?ANSWERS:Georgetown University(1964-1968)Oxford University(1968-1970)Yale Law School(1970-1973)ANSWER:TemporalSignal1968-1970<Temporal CompatibleAnswerGeorgetown UniversityFigure 3: Example performance Temporal Layerimportant emphasize temporal layer language dependent platform(it uses lexical syntactic patterns) English language chosen initiallydevelop layer; however, easily extended languages, seensection 6.3. units integrate temporal layer described detailfollowing sections.784fiEnhancing QA Systems Complex Temporal Question Processing Capabilities5.1 Question Decomposition Unitmain task unit, divided three main modules, temporalreasoning temporal information question decomposition question(only required Type 3 4 questions). temporal expression identificationnormalization module detects resolves temporal expressions question.type identification module classifies question according taxonomy proposedsection 3. Finally, question splitter module splits complex question simpleones.Thus, output Question Decomposition unit consists of:two sub-questions (Q-Focus Q-Restriction), processed QAsystem order obtain answer them,temporal tags, containing concrete dates returned TERSEO system (Saqueteet al., 2006), tags part input Answer Recomposition Unitused unit temporal constraints order filter individualanswers,temporal signal, part input Answer Recomposition Unitwell, information needed order compose final answerdetermine temporal compatibility answers Q-Focusanswer Q-Restriction.modules decomposition unit fully explained following subsections.5.1.1 Temporal Expression Identification Normalizationmodule uses TERSEO system (Saquete et al., 2006) identify, annotatenormalize temporal expressions question.system, implicit explicit temporal expressions annotated. Expressions like 12/06/1975 explicit, like two days implicitneed location another complete temporal expression (TE) understood.specific purposes temporal layer, TERSEO simply returns text temporalexpression string normalization resolution value temporal expressionusing ISO standard format concrete dates periods.work, TERSEO use complete text input, question.temporal tags (TE tag value attribute) obtained questions outputmodule used Answer Recomposition Unit order filterindividual answers obtained QA system. TE tag necessary orderdetermine temporal compatibility answers Q-Focus answerQ-Restriction. example, question like: U.S. ship attackedIsraeli forces Six Day war sixties?, temporal constraint mustfulfilled is: date Q-Focus answers 1960-01-01 1969-12-31(196 ISO format). means answers whose dates within rangedates question temporally compatible.important emphasize that, initially, TERSEO developed Spanish,platform automatically extend system languages developed785fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorenswell. Therefore, system evaluated three different languages: Spanish, EnglishItalian. Spanish results 91% precision 73% recall. systemevaluated English using TERN (2004) corpus results obtained Fmeasure 86% identification 65% normalization. Italianevaluation, I-CAB corpus used. corpus consists 525 news documents takenlocal newspaper LAdige 3 . Ita-TERSEO obtained F-measure around 77%identification. results quite good extension English Italiancompletely automatic therefore, also fast.multilingual capabilities TERSEO interesting various NLP fields,particular application Crosslingual QA systems, therefore Temporal Layerwell.5.1.2 Type IdentificationType Identification module classifies question one four typestaxonomy proposed above. identification necessary type questionproduces different behavior (scenario) system. Type 1 Type 2 questionsclassified simple, answer obtained without splitting original question.hand, Type 3 Type 4 questions need split set simplesub-questions. types sub-questions always Type 1, Type 2 non-temporalquestion, considered simple questions.question type established according rules Figure 4. fourpossibilities: (a) Temporal Expression Temporal Signal, questionclassified Type 1 ; (b) Temporal Expression Temporal Signal,question classified Type 4 ; (c) Temporal Expression TemporalSignal, question classified Type 2 ; (d) Temporal ExpressionTemporal signal, question classified Type 3.5.1.3 Question Splittertask necessary when, according type identification module, questionType 3 Type 4. questions considered complex questions needdivided simple ones (Type 1, Type 2 non-temporal questions). decompositioncomplex question based identification temporal signals, link simpleevents form complex questions (see Table 1).explained before, using temporal signal referent, two events relatedtransformed two simple questions: Question-Focus (Q-Focus) QuestionRestriction (Q-Restriction).Q-Focus question specifies information user searchingfor. question simple obtain, syntactic changes requiredconstruct it, question mark must added. Q-Focus processedQA system, system return list possible answers.Q-Restriction constructed using part complex question followstemporal signal. question always transformed question using set3. http://www.adige.it786fiEnhancing QA Systems Complex Temporal Question Processing CapabilitiesQUESTION(Q)QUESTIONANALYSISYESTEMPORALEXPRESSION?YESTEMPORALSIGNAL?TYPE 1YESTEMPORALSIGNAL?TYPE 4TYPE 2TYPE 3Figure 4: Decision tree Type Identificationlexical syntactic patterns defined layer. Q-Restriction processedQA system, one appropriate answer expected.addition, temporal signals denote ordering events linked. Assuming F1 date associated answers Q-Focus F2 dateassociated answer Q-Restriction4 , signal establish certain orderanswers, called ordering key. example ordering keysshown Table 1.Table 1: Example signals ordering keysSIGNALF2 F3F2 F3/timeSinceORDERING KEYF1 > F2F1 = F2F1 < F2F2i <= F1 <= F2fF2 <= F1 <= F3F2 <= F1 <= F3F1 = F2F2i <= F1 <= F2fF2i <= F1 <= F2fF1 = F2F1 > F2Using list answers Q-Focus, answer Q-Restrictiontemporal signal, Answer Recomposition Unit determines temporal compatibility4. F2:Q-Restriction concrete date / [F2i-F2f]:Q-Restriction period dates787fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorensanswers composes final answer original complex question.process fully explained following subsection.5.2 Answer Recomposition Unitmain task Answer Recomposition Unit obtain final answercomplex question using available inputs Decomposition Unit answersobtained QA system. Recomposition Unit divided two modules.Individual Answer Filtering module filters possible answers Q-Focus, avoidingnon-temporally compatible ones, Answer Comparison Composition module,composes answer original question using ordering key establishedtemporal signal.complex questions split Decomposition Unit Q-FocusQ-Restriction answers questions obtained QAsystem, Recomposition Unit determines list potential answersQ-Focus one compatible temporal constraints obtained process:temporal expressions, temporal signal answer Q-Restriction. answersQ-Focus fulfill temporal constraints considered answer initialcomplex question.5.2.1 Individual Answer Filteringlist possible answers Q-Focus answer Q-Restriction givenQA system input Individual Answer Filtering module. Q-FocusQ-Restriction temporal expression, selects answers satisfytemporal constraints obtained TE Identification Normalization Unit. dateanswer temporally compatible temporal tag, is, dateanswer must lie within date values tag. not, rejected.answers fulfill constraints go Answer Comparison Composition module.5.2.2 Answer Comparison CompositionFinally, answers filtered using signals ordering key,results Q-Focus compared answer Q-Restriction orderdetermine temporally compatible. Temporal signals denote relationshiporder date answer Q-Focus date answerQ-Restriction.answers fulfill compatibility established temporal signalpossible answers original question. answer selected considered moduleanswer complex question. Hence, system able solve complextemporal questions.6. Evaluation Experimentsevaluation experiments performed paper done initially English,porting system Spanish, evaluation procedure carriednew language.788fiEnhancing QA Systems Complex Temporal Question Processing Capabilitiesevaluation dual aim: one hand, determine DecompositionUnit able process type question properly order obtain appropriate simplefactual questions answered kind general purpose QA system,hand, show extent general purpose QA system could improvedtechniques applied.6.1 Test EnvironmentFirst all, corpus English questions contains many simple complex temporal questions possible necessary. first idea use already existing resources,TREC (2008) CLEF (2008) question corpora, due large numberquestions contain. Unfortunately, studying corpora, discardedcontain complex temporal questions. Thus, using initial TERQASquestion corpus proposal (Radev & Sundheim, 2002; Pustejovsky, 2002) model, newquestion corpus manually developed collecting questions group volunteersunacquainted work. instructions given volunteers were: 1) answersquestions proposed must found Internet, 2) questions must constructedaccording temporal question taxonomy described Section 3, 3) questionsmust expect fact answer (factual questions). case complex questions, twofactual questions must related temporal signal5 . last instruction necessaryorder make evaluation procedure straightforward since open-ended questionsusually require long answers, makes difficult judge. orderbalanced corpus, questions discarded finally corpus developed contains balanced number (50) type temporal question (Types 1,2,3 4),resulted 200 temporal questions English6 . Spanish evaluation, Englishquestion corpus manually translated language. Therefore, distributionquestions type English.question corpus English Spanish developed, following stepconstruct testbeds languages order allow rigorous, transparentreplicable evaluation tests. testbed annotation performed using XMLschema developed three independent annotators. case doubtsdisagreement, annotation reviewed referee, made final decision.interannotator agreement calculated every attribute, resulting 100% casesexcept temporal signal (98%) due complexity temporal signals.testbed annotation, every question annotated Q tag, tagid attribute identifies every question. question string annotated usingQUESTION tag. Furthermore, every question, five items must annotated:1. Identification Normalization temporal expressions question.annotation item done using TE tag, content stores string textTE. tag also attribute value, stores resolutionexpression using ISO format,5. important emphasize Type 3 questions contain two events temporal expression,used extra time constraint answering procedure, limiting number potential answers,speeding answering step, refining final answer6. http://gplsi.dlsi.ua.es/corpus/CTQ789fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens2. type question according classification proposed paper. typeannotated using tag TYPE must value 1 4. Sincequestions manually built using temporal question taxonomy reference,3. temporal signal. tag used annotate item called SIGNAL storesexact string text temporal signal question,4. two possible sub-questions case complex questions (Type 3 4):first sub-question annotated using tag Q-FOCUS second using tagQ-REST, and,5. answer complex question: answer annotated using ANSWER tagcontains correct answer complex question. One example annotationformat question is:<Q id="107"><QUESTION>Who best actress Oscar award James Dean died 50s?< /QUESTION><TE value="195">the 50s< /TE><TYPE>3< /TYPE><SIGNAL>when< /SIGNAL><Q-FOCUS>Who best actress Oscar award?< /Q-FOCUS><Q-REST>When James Dean die 1950s?< /Q-REST><ANSWER>Anna Magnani< /ANSWER>< /Q>case Decomposition Unit, following five aspects evaluated:TE Identification Normalization: temporal expressions questioncorrectly detected normalized?Type Identification: type question correctly identified according classification presented previously paper?Signal Detection: temporal signals question correctly detected?Question Splitter: complex questions correctly split simple factual questions, answered general purpose QA system?DECOMPOSITION UNIT whole: system correctly undertakensub-tasks previously defined, since sub-tasks whole compose decomposition unit?However, evaluation aspects explained need consideredquestions. Table 2 determines aspect must evaluated ignored particulartype question. decomposition unit whole taking considerationevaluated sub-tasks every question.determined aspects evaluated decomposition casesmust evaluated, depending type question, next step establishaspects evaluated. purpose, criteria matrix, containing rulesfollowed evaluation order determine elements treated (ACT)correct (CORR), shown Table 3.790fiEnhancing QA Systems Complex Temporal Question Processing CapabilitiesTable 2: Aspects evaluated Decomposition Unit depending Q-typeType1234TE Id.Norm.YesYesTypeYesYesYesYesSignalYesYesQ SplitterYesYesDECOMP.YesYesYesYesTable 3: ACT CORR criteria matrix Decomposition Unit evaluationTEIdent.Norm.ACTTE annotated systemTypeSignalQ Splittertype returned systemtemporal signal annotated systemComplex Q divided two sub-QsDECOMP.previous aspects ACTCORR-Exact agreement TE tag content-Exact agreement value attribute contentwithin TE tagExact agreement TYPE tag contentExact agreement SIGNAL tag contentSub-Qs agreement Q-FOCUS Q-REST tagin:-Interrogative particle-Main verb correctly detected tensed-All keywords appear keywordsoriginal Q, except stopwordsprevious aspect CORRcase QA evaluation, using current CLEF evaluation criteria7starting point, determining correct inexact answers. use evaluationcriteria possible since corpus contains factual questions. Therefore, determiningcorrectness answer straightforward. CLEF judgmentsspecified evaluation measure incorrectness may calculated directlysubtracting number correct answers total number questions.addition, unknown judgement also omitted since two human assessors must evaluateanswers. finally, consider unsupported judgement neither, sincecorpus consisted data obtained Internet, correct answers found.criteria matrix QA, shown Table 4, describes rules followed evaluation order determine treated (ACT), correct (CORR) inexact (INE) answers.Table 4: ACT CORR criteria matrix QA systemQAACTanwer returnedsystemCORR (CLEF R)-Exact agreement oneanswers containedANSWER tag contentINE (CLEF X)answer contains correct answer, incomplete longerminimum amount information requiredevaluations performed work, following measures used:POS:Total number items7. http://www.clef-campaign.org/791fiSaquete, Vicedo, Martnez-Barco, Munoz, & LlorensACT: Number items treated systemCORR: Number items properly treated (Correct)(CLEF R)PREC: Precision ( CORRACT ) percentage items output systemproperly treatedREC: Recall ( CORRP OS ) percentage items treated system (CLEF Accuracy)2)(P R)) Combination precision recall single value. = 1F: ( (1+( 2 P +R)QA evaluation purposes:1MRR: ordered list possible answers question ( CorrectAnswerPosition ).final MRR average every individual MRR obtained.INE: number answers judged inexact human assessors (CLEF X).6.2 Evaluation Resultssection presents results decomposition unit analysis influenceQA systems English.6.2.1 Evaluating Decomposition Unit Englishsection, decomposition unit8 processing simple complex temporalquestions English evaluated, based testbed defined previously. evaluation,addition decomposition unit efficiency, aspects temporal expressionsinfluence complex questions analyzed.evaluation results good F-measure 89.5%. resultsshown Table 5. evaluation, 176 total 200 questions correctlypreprocessed. Since decomposition unit divides complex questions alsodetermines type question performs temporal reasoning necessary,whole set questions considered global measure decomposition unit.obvious case Type 1 questions decomposition unit simply determinestype question, interested evaluating performance unitrespect.Table 5: Evaluation decomposition unit EnglishPOSACTCORRPRECRECFTE Identification Normalization100938086.0%80.0%82.9%Type Identification20020019497.0%97.0%97.0%Signal Detection1001009696.0%96.0%96.0%Question Splitter1001009292.0%92.0%92.0%20019317691.1%88.0%89.5%DECOMPOSITION UNIT8. http://gplsi.dlsi.ua.es/demos/TQA/792fiEnhancing QA Systems Complex Temporal Question Processing CapabilitiesNext, detailed analysis results evaluation aspect shown (see Appendixdetailed error examples):Identification normalization Temporal Expressions: corpus,100 temporal expressions system detected 93, 80 correctlyresolved. said previously, module uses TERSEO system identifynormalize temporal expressions question. three typeserrors: (1) expressions treated TERSEO temporal expressionsfact temporal; (2) expressions identified wrongly because: a)expression outside scope TERSEO system, b) identificationextent exact; (3) expressions that: a) normalized wronglynormalization rule TERSEO appropriate expressions, b)normalized normalization rule exist.Type Identification: 200 temporal questions corpus,processed module, 194 correctly identified according taxonomyproposed section 3. errors module due fact TEdetected TERSEO, shown Appendix A. However, type errorusually affect question splitting cases complex questionsplit correctly.Signal Detection: corpus, 100 questions considered complex (Type 3 Type 4). system able treat recognize correctlytemporal signal 96 questions. main error detected module arosetemporal expression part signal, denoting complex signal, as:EVENT1 year EVENT2. type complex signal outside scopesystem. system also fails preposition, classified signalsystem, part TE, like 18 century therefore wronglyconsidered temporal signal.Question Splitter: set 100 complex questions, system ableprocess split 92 properly. errors unit due to: a)wrong signal identification; b) syntactic problems, obtaining tensed verbsubject Q-Focus construct Q-Restriction properly. instance,question language invented Berliner patented Gramophone?,POSTagger identify patented past tense verb Q-Restrictionwrongly generated as: Berliner patented Gramophone happen?.One possible problem appear complex questions, yet treatedsystem, questions contain anaphoric co-references. Therefore, splittingcomplex question two separate simple questions, question containsanaphoric co-reference treated directly QA system needs processedmodule performs anaphora resolution first. example: studiesMs. Whitman graduate got MBA? . Q-Restriction obtained is:get MBA?. case, referring Ms. Whitman. case,type question outside scope. However, adding module adaptsanaphora resolution techniques dialogs texts (Palomar et al., 2001; Palomar &793fiSaquete, Vicedo, Martnez-Barco, Munoz, & LlorensMartnez-Barco, 2001) questions, situation solved. Moreover, applyingmodule question affect decomposition process case.6.2.2 Evaluating influence temporal processing QA systemsEnglishQA system used evaluation general purpose one uses Internet datacorpus (Moreda, Llorens, Saquete, & Palomar, 2008a). simple opendomain QA system, whose main feature Answer Extraction Unit able lookpossible answers two ways: performing mapping type name entityquestion requires (NE-based), type semantic role question needsanswer (SR-based)9 .Due modularity QA system, evaluation, NE-based answer extraction used. baseline, using subset factual questions, extractedTREC1999 TREC2000 NE oriented, authors evaluated systemfound 87.50% precision, 84% recall, 85.70% F 87.25% MRR (Moreda, Llorens, Saquete,& Palomar, 2008b).evaluation performed work divided two experiments:1. Base QA system evaluation: First QA system evaluated without usingtemporal layer.2. Multilayered QA system evaluation: QA system evaluated performs temporal layer.main aim evaluation compare results two experimentsdetermine temporal layer enhances general purpose QA system like one usedcase. Besides, experiments, 200 temporal question corpus createdpurpose containing simple (Type 1 Type 2) complex (Type 3 Type 4)questions used.results obtained general purpose QA system without temporal layershown Table 6.Table 6: Evaluation QA system English temporal questionsPOSACTCORRINEPRECRECFMRRType 1505035070.00%70.00%70.00%77.60%Type 2504523151.11%46.00%48.42%48.00%Type 35081012.50%2.00%3.45%3.00%Type 450182011.11%4.00%5.88%5.00%GLOBAL20012132150.41%30.50%38.01%33.40%results obtained system enhanced temporal layer shown Table7.9. http://gplsi.dlsi.ua.es/demos/TMQA/794fiEnhancing QA Systems Complex Temporal Question Processing CapabilitiesTable 7: Evaluation QA system plus temporal layer English temporal questionsPOSACTCORRINEPRECRECFMRRType 1505035070.00%70.00%70.00%77.60%Type 2504738180.85%76.00%78.35%78.00%Type 3504829260.42%58.00%59.18%63.66%Type 4504626256.52%52.00%54.17%55.66%GLOBAL200191128567.02%64.00%65.47%68.73%shown tables, QA system enhanced temporal layer offers betterresults measures (72.24% improvement F 33.58% error reduction F).outstanding improvements occur complex temporal questions, due extrareasoning temporal layer applies find candidate answer. Moreover, extraexperiment, manually corrected temporal expression identification normalization,performed. Obviously, questions Type 3 4 affected improved. Resultsshown Table 8.Table 8: Evaluation QA system plus temporal layer English temporal questionsmanually corrected TERNPOSACTCORRINEPRECRECFMRRType 1505035070.00%70.00%70.00%77.60%Type 2504840183.33%80.00%81.63%82.00%Type 3504830262.50%60.00%61.22%65.66%Type 4504626256.52%52.00%54.17%55.66%GLOBAL200192131568.22%65.50%66.83%70.23%graphical comparison results type question shown Figure 5.clear Multilayered QA system enhances performance QA systemtypes questions except Type 1 (simple factual temporal questions), since typequestion processed way systems. types, precision,recall, F-measure MRR improved, especially case Type 3 Type 4questions, base QA system almost incapable answering questionsproperly. system gave inexact answers since questions need short answersconsisting NE TE.interesting examples analyzed shown Figure 6.first example, question Type 2 question, contains implicittemporal expression 16 years ago. question processed systems,important difference Multilayered QA system able process temporalexpression normalize expression concrete date, case 1992.preprocessing temporal expression done, question processed Base QAsystem Olympics held 1992?, allowing system find correctanswer. Without preprocessing temporal layer, Base QA system returns795fiSaquete, Vicedo, Martnez-Barco, Munoz, & LlorensFMRR80%80%70%70%60%60%50%50%40%Q.A.40%Q.A.30%M.Q.A30%M.Q.A.20%20%10%10%0%0%Type 1Type 2Type 3Type 4Type 1(a) F-measure comparisonType 2Type 3Type 4(b) MRR comparisonFigure 5: Comparative graphics Base QA system Multilayered QA systemIndian Prime MinisterBlack Panthers founded?Olympics held 16 years ago?Type 2Base Q.A.systemType 4MultilayeredQ.A. systemBase Q.A.system16 years ago= 1992Beijing(Wrong)MultilayeredQ.A. systemQ-R=1996T.S.=whenBarcelona(OK)H.Rap Brown(Wrong)(a) Example 1Indira Gandhi(OK)(b) Example 2Figure 6: Examples Multilayered QA system performancepopular answer, corresponds last Olympic games Beijing, thereforefails answer question correctly.second example, question Type 4 complex question processedsystems. Since Base QA system able reason second partquestion simply uses keywords question, Multilayered QA systemreturns correct answer, taking restriction date event second partquestions occurred.conclude, study demonstrates including type layer help general purpose QA systems resolve questions complex simple factualquestions, without changing implementation general QA system.6.2.3 Comparison QA systemsorder compare results another QA system, carriedtest widely known START QA system (Katz, 1990, 1997), availableInternet10 . results obtained START system obtained10. http://start.csail.mit.edu/796fiEnhancing QA Systems Complex Temporal Question Processing CapabilitiesQA system enhanced temporal layer shown compared Table 9.general purpose QA systems using Internet corpus.Table 9: QA system plus temporal layer compared START QA systemQA + temp layerSTARTPRECRECFMRRPRECRECFMRRType 170.00%70.00%70.00%77.60%85.71%24.00%37.50%24.00%Type 280.85%76.00%78.35%78.00%75.00%6.00%11.11%7.00%Type 360.42%58.00%59.18%63.66%00.00%00.00%00.00%00.00%Type 456.52%52.00%54.17%55.66%00.00%00.00%00.00%00.00%GLOBAL67.02%64.00%65.47%68.73%83.33%7.50%13.76%7.75%START QA system able answer Types 1 Type 2 questions. Although, precision achieved system types questions high, recalllower, specially Type 2 questions (6.00%). Focusing complex temporal questions(Types 2, 3 4), QA system, uses temporal information, seen obtainbetter results START QA, use temporal layer. conclusion,results show application temporal layer improves QA results complextemporal questions. Concretely, considering overall results, QA system usingtemporal layer exceeds START system 375.79% regards F-measure (48.36%error reduction).6.3 Portability Languages: Spanish Approachsaid before, system initially developed English extended Spanishwell. Since task performed layer processes complex questions languagedependent, adaptation system required: (1) TERSEO Spanishused, (2) temporal signals stored system translated Spanish, (3)question splitter module adapted build grammatically correct Spanish Cuando(When) questions.6.3.1 Decomposition Unit Evaluation Spanishresults evaluation shown Table 10.Table 10: Evaluation system SpanishPOSACTTE Identification Normalization100908291.1%82.0%86.3%Type Identification20020018994.5%94.5%94.5%Signal Detection100999797.9%97.0%97.4%Question Splitter1001009393.0%93.0%93.0%20019017491.5%87.0%89.2%DECOMPOSITION UNIT797CORRPRECRECFfiSaquete, Vicedo, Martnez-Barco, Munoz, & LlorensBriefly, evaluation Spanish, 174 total 200 questions properlyprocessed decomposed aspects (TE identification, type identification, temporalsignal detection splitting, necessary), means F-measure 89.2%whole decomposition process. best results obtained Signal Detection module (F-measure almost 100%), results Question Splitting Type Identification(F-measure around 93-94%) TE Identification Normalization also quite good(F-measure around 86%).main errors similar English ones. However, new problemsappeared Spanish (see Appendix B details), principally produced by:Grammatical errors transformation second question due ambiguitywords produces incorrect POS-tagging. example: expressionel cometa Hale (the Hale comet), POSTAGGER classifies cometa (comet)verb rather noun, would appropriate tag case.Temporal expressions like el ano 99 el 99 (year 99), Spanish refer1999 case, detected resolved. problem appearsexpressions containing non explicit numeric temporal expressions, i.e. el siglo XIX(XIX century), el segundo milenio (second millennium) less commonword-spelled dates mil novecientos noventa ocho (one thousand nine hundredninety eight) successfully processed temporal layer.Finally, questions temporal signal complex, as: un ano despuesde que...(a year after...), signal detection question splitting wrongtype complex signal outside scope system.6.3.2 Evaluating influence temporal processing QA systemsSpanishevaluation, QA system described (Moreda et al., 2008a) adaptedSpanish language. English, used NE-based answer extraction module.divided Spanish evaluation two experiments, English evaluation:1. Base QA system evaluation: First adapted QA system evaluated without usingtemporal layer.2. Multilayered QA system evaluation: adapted QA system evaluatedperforms temporal layer.main aim evaluation analyze whether temporal layer successfully extended languages deal language QA system, likeSpanish-adapted QA system case. 200 temporal question corpus createdEnglish test manually translated Spanish used experiments.results obtained general purpose Spanish QA system without temporallayer shown Table 11.results obtained system enhanced temporal layershown Table 12.798fiEnhancing QA Systems Complex Temporal Question Processing CapabilitiesTable 11: Evaluation QA system Spanish temporal questionsPOSACTCORRINEPRECRECFMRRType 1503520157.14%40.00%47.06%45.34%Type 2503712032.43%24.00%27.59%29.06%Type 3503000.00%0.00%0.00%0.00%Type 4504000.00%0.00%0.00%0.00%GLOBAL2007932140.51%16.00%22.94%18.60%Table 12: Evaluation QA system plus temporal layer Spanish temporal questionsPOSACTCORRINEPRECRECFMRRType 1503520157.14%40.00%47.06%45.34%Type 2504019047.50%38.00%42.22%45.96%Type 3503115148.39%30.00%37.04%37.00%Type 4503114145.16%28.00%34.57%34.00%GLOBAL20013768349.64%34.00%40.36%40.58%results Spanish show, expected already proven English case,that: a) QA system enhanced temporal layer gives better results measures (79.42% improvement F 22.60% error reduction F), b) temporal layereasily extensible languages. final global results worst Englishapproach QA system, due fact baseline resultsSpanish QA system also worst compared English QA system (F-English 38%compared F-Spanish 23%). addition, English experiments, extra experiment manually corrected Spanish temporal expression identification normalizationperformed results shown Table 13.Table 13: Evaluation QA system plus temporal layer Spanish temporal questionsmanually corrected TERNPOSACTCORRINEPRECRECFMRRType 1503520157.14%40.00%47.06%45.34%Type 2504322051.16%44.00%47.31%51.96%Type 3503115148.39%30.00%37.04%37.00%Type 4503114145.16%28.00%34.57%34.00%GLOBAL20014071350.71%35.50%41.76%42.08%Despite fact, Multilayered QA system enhances performance SpanishQA system types questions, even case complex questions,Base Spanish QA system unable find correct answer. inexact799fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorensanswers Spanish well, thus proving system usually obtains exact answerslanguages.conclude second evaluation analysis, extension evaluation Spanishcorroborates conclusions obtained English evaluation also demonstratestemporal layer improves system way regardless language.7. Conclusions Workstudy presents multilayered temporal QA architecture performs multilingual level, case English Spanish. system processes complex temporalquestions splitting simple questions answered different typesgeneral purpose QA systems. addition, system performs temporal reasoningquestions temporal information.proposal consists adding new layer, top current QA system,two main features:Complex question decomposition. Questions decomposed simple eventsgenerate simple questions (sub-questions) using temporal signal relatesevents. first sub-question (Q-Focus) specifies type information userneeds find. answer second sub-question (Q-Restriction) establishestemporal restrictions list answers Q-Focus. Q-FocusQ-Restriction input QA system (any type QA system could usedhere).Question recomposition. Answers Q-Focus Q-Restriction, obtainedQA system, filtered compared order determine temporal compatibility construct final complex answer.Since layer processes complex questions uses lexical syntactic rules (agrammar), task language dependent. Initially, decomposition unit preparedEnglish, general way. Extension system Spanish thereforesimple (only small changes required), applies languages.addition, temporal reasoning system performed TERSEO,multilingual system (now working Spanish, English, Catalan Italian) easilyextensible European language.evaluation purposes, two aims: a) determine decomposition unitprocesses type question properly order obtain appropriate simple factualquestions, b) show techniques enhance general purpose QA system. orderaccomplish aims, test bed English Spanish constructed, annotatingquestion corpus correct results decomposition QA tasks,determining criteria establishing question properly decomposedanswered.decomposition unit evaluation results English Spanish goodcomplex questions (F-measure 89.5% English 89.2% Spanish). evaluatingperformance whole multilayered architecture, results comparedobtained base QA system without temporal layer. Great improvement800fiEnhancing QA Systems Complex Temporal Question Processing Capabilitiesfound, especially case complex questions (Type 3 4), basesystem able answer (4.66% average F-measure English 0.00%Spanish). multilayered QA system obtained overall F-measure approximately65% English 40% Spanish types questions. Besides, temporal layerQA system also compared online general purpose QA system called START,demonstrating difficulty encountered general purpose QA systems answeringquestions complex temporal information temporal relations.work done along three main lines research: 1) resolving problemsdetected temporal layer evaluation process, 2) adding module resolveanaphoric co-reference questions, 3) integrating event link informationTIMEML schema (TimeML, 2008) system order extract deeper understandingcomplex questions, 4) taking consideration techniques determine event durationscase open-ended questions, as: happened world oil prices Iraqiannexation Kuwait?. task, previous work field considered (Panet al., 2006b), 5) applying layer procedure types complex questions,well studying new features need added system enableperform languages like Chinese.Acknowledgmentspaper partially supported Spanish government, project TIN-200615265-C06-01, framework project QALL-ME, 6th FrameworkResearch Programme European Union (EU), contract number: FP6-IST-033860.Appendix A. Question Decomposition Error Analysis Englishappendix gives detailed information decomposition errors detected testEnglish language. shown Table 5 distinguish TE identification normalization, type identification, signal detection question splitter errors. Table 14specify questions English testbed correspond error types.questions bold correspond one type error.Table 14: Question Decomposition Error Analysis EnglishError typeTE Identification Normalizationtestbed question81, 83, 89, 92, 97, 98, 99, 102, 108, 112, 114, 115, 116, 117,126, 129, 133, 135, 142, 148Type Identification97, 98, 108, 129, 135, 148Signal Detection101, 114, 116, 129Question Splitter101, 110, 114, 116, 129, 142, 179, 192questions implied listed below. erroneous elements listed correctvalues indicated brackets.<Q id=81> (ACT: Yes CORR: No)801fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens<QUESTION>Who Nobel Peace Prize 91?</QUESTION><TE value=>91</TE> (CORR value=1991)</Q><Q id=83> (ACT: Yes CORR: No)<QUESTION>What tennis player win Wimbledon women singles second millennium year?</QUESTION><TE value=>year</TE>(CORR <TE value=2000>second millennium year</TE>)</Q><Q id=89> (ACT: Yes CORR: No)<QUESTION>How many planes crashed Twin Towers 01?</QUESTION><TE value=>01</TE> (CORR value=2001)</Q><Q id=92> (ACT: Yes CORR: No)<QUESTION>What organization founded 75 Bill Gates?</QUESTION><TE value=>75 by</TE> (CORR <TE value=1975>75</TE>)</Q><Q id=97> (ACT: CORR: No)<QUESTION>What city capital Nicaragua eighteen fifty-five?</QUESTION><TE value=></TE> (CORR <TE value=1855>eighteen fifty-five</TE>)<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)</Q><Q id=98> (ACT: CORR: No)<QUESTION>What largest city Italy 17th century?</QUESTION><TE value=></TE> (CORR <TE value=16>the 17th century</TE>)<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)</Q><Q id=99> (ACT: Yes CORR: No)<QUESTION>Where Eurovision held 68?</QUESTION><TE value=>68</TE> (CORR value=1968)</Q><Q id=101> (ACT: Yes CORR: No)<QUESTION>Who Prime Minister Spain four years Jose Maria Aznar presided Spain2000 2004?</QUESTION><SIGNAL>after</SIGNAL> (CORR <SIGNAL>four years after</SIGNAL>)<Q-FOCUS>Who Prime Minister Spain four years?</Q-FOCUS>(CORR <Q-FOCUS>Who Prime Minister Spain?</Q-FOCUS>)</Q><Q id=102> (ACT: CORR: No)<QUESTION>Who king Spain Charles III died 1780s?</QUESTION><TE value=></TE> (CORR <TE value=178>the 1780s</TE>)</Q><Q id=108> (ACT: CORR: No)<QUESTION>Who president US AARP founded five decades ago?</QUESTION><TE value=></TE>(CORR <TE value=195>five decades ago</TE>)<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)</Q><Q id=110> (ACT: Yes CORR: No)<QUESTION>Who Prime Minister Spain Columbia first flight 1980s?</QUESTION><Q-FOCUS>Who Prime Minister Spain just?</Q-FOCUS>(CORR <Q-FOCUS>Who Prime Minister Spain?</Q-FOCUS>)</Q><Q id=112> (ACT: Yes CORR: No)<QUESTION>How many members European Union Gladiator released 00?</QUESTION><TE value=>00</TE> (CORR <TE value=2000>00</TE>)802fiEnhancing QA Systems Complex Temporal Question Processing Capabilities</Q><Q id=114> (ACT: Yes CORR: No)<QUESTION>What company introduced onto market seat adjustable shoulder support yearMariah Carey born 1960s?</QUESTION><TE value=>the 1960s</TE> (CORR <TE value=196>the 1960s</TE>)<SIGNAL>before</SIGNAL> (CORR <SIGNAL>a year before</SIGNAL>)<Q-FOCUS>What company introduced onto market seat adjustable shoulder support year?</QFOCUS>(CORR <Q-FOCUS>What company introduced onto market seat adjustable shoulder support?</QFOCUS>)</Q><Q id=115> (ACT: Yes CORR: No)<QUESTION>Which language forbidden Spain Francos Dictatorship period 1939-1975?</QUESTION><TE value=1975>1939-1975</TE>(CORR <TE value=1939-1975>1939-1975</TE>)</Q><Q id=116> (ACT: Yes CORR: No)<QUESTION>When Indurain win Tour year Shawshank Redemption film released1990s?</QUESTION><TE value=>the 1990s</TE> (CORR <TE value=199>the 1990s</TE>)<SIGNAL>after</SIGNAL> (CORR <SIGNAL>a year after</SIGNAL>)<Q-FOCUS>When Indurain win Tour year?</Q-FOCUS>(CORR <Q-FOCUS>When Indurain win Tour?</Q-FOCUS>)</Q><Q id=117> (ACT: Yes CORR: No)<QUESTION>When Vesuvius erupt Sinclair Lewis Literature Nobel Prize 1930s?</QUESTION><TE value=>1930s</TE> (CORR <TE value=193>1930s</TE>)</Q><Q id=126> (ACT: Yes CORR: No)<QUESTION>Who died plane crash Vietnam war started late 1960s?</QUESTION><TE value=>1960s</TE> (CORR <TE value=1965-1969>late 1960s</TE>)</Q><Q id=129> (ACT: CORR: No)<QUESTION>Who king Spain Charles IV reigned Spain eighteenth century?</QUESTION><TE value=></TE> (CORR <TE value=17>eighteenth century</TE>)<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)<SIGNAL>during</SIGNAL> (CORR <SIGNAL>after</SIGNAL>)<Q-FOCUS>Who king Spain Charles IV reigned Spain?</Q-FOCUS>(CORR <Q-FOCUS>Who king Spain?</Q-FOCUS>)<Q-REST>When eighteenth century happen?</Q-REST>(CORR <Q-REST>When Charles IV reign Spain eighteenth century?</Q-REST>)</Q><Q id=133> (ACT: Yes CORR: No)<QUESTION>What person Literature Nobel Prize James Dean born 31?</QUESTION><TE value=>31</TE> (CORR value=1931)</Q><Q id=135> (ACT: CORR: No)<QUESTION>Who prime minister United Kingdom AARP founded five decadesago?</QUESTION><TE value=></TE> (CORR <TE value=195>five decades ago</TE>)<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)</Q><Q id=142> (ACT: Yes CORR: No)<QUESTION>Which language invented Zamenhof Berliner patented Gramophone 1880s?</QUESTION><TE value=>the 1880s</TE> (CORR <TE value=188>the 1880s</TE>)803fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens<Q-REST>When Berliner patented Gramophone 1880s happen?</Q-REST>(CORR <Q-REST>When Berliner patent Gramophone 1880s?</Q-REST>)</Q><Q id=148> (ACT: CORR: No)<QUESTION>Where Woodstock Festival held August 15 Unix developed?</QUESTION><TE value=></TE> (CORR <TE value=XXXX-08-15>August 15</TE>)<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)</Q><Q id=179> (ACT: Yes CORR: No)<QUESTION>Who king Spain Charles IV reigned Spain?</QUESTION><Q-REST>When Charles IV reign Spain happen?</Q-REST>(CORR <Q-REST>When Charles IV reign Spain?</Q-REST>)</Q><Q id=192> (ACT: Yes CORR: No)<QUESTION>Which language invented Zamenhof Berliner patented Gramophone?</QUESTION><Q-REST>When Berliner patented Gramophone happen?</Q-REST>(CORR <Q-REST>When Berliner patent Gramophone?</Q-REST>)</Q>Appendix B. Question Decomposition Error Analysis Spanishappendix gives detailed information decomposition errors detected testSpanish language (see table 10). Table 15 specify questions correspond error types. questionsbold correspond one type error.Table 15: Question decomposition error analysis SpanishError typetestbed questionTE Identification Normalization81, 83, 89, 92, 97, 98, 99, 108, 114, 116, 129, 130, 133, 135,142, 143, 145, 148Type Identification2, 6, 9, 31, 45, 81, 97, 98, 108, 129, 135Signal Detection114, 116, 129Question Splitter105, 110, 114, 116, 129, 133, 155questions implied listed below. erroneous elements listed correctvalues indicated brackets.<Q id=2> (ACT: Yes CORR: No)<QUESTION>Durante que decada fue inventado el test del polgrafo?</QUESTION><TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)</Q><Q id=6> (ACT: Yes CORR: No)<QUESTION>En que ano fue lanzado el submarino Nautilus?</QUESTION><TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)</Q><Q id=9> (ACT: Yes CORR: No)804fiEnhancing QA Systems Complex Temporal Question Processing Capabilities<QUESTION>En que ano entro en vigor la enmienda 18?</QUESTION><TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)</Q><Q id=31> (ACT: Yes CORR: No)<QUESTION>Que ano volaron los Wright Brothers por primera vez?</QUESTION><TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)</Q><Q id=45> (ACT: Yes CORR: No)<QUESTION>Que ano fue el gran Incendio de Londres?</QUESTION><TYPE>2</TYPE> (CORR <TYPE>1</TYPE>)</Q><Q id=81> (ACT: CORR: No)<QUESTION>Quien gano el Nobel de la Paz en el 91?</QUESTION><TE value=></TE> (CORR <TE value=1991>el 91</TE>)<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)</Q><Q id=83> (ACT: Yes CORR: No)<QUESTION>Que jugador de tenis gano Wimbledon mujeres individuales en el ano del segundo milenio?</QUESTION><TE value=>el ano</TE>(CORR <TE value=2000>en el ano del segundo milenio</TE>)</Q><Q id=89> (ACT: Yes CORR: No)<QUESTION>Cuantos aviones chocaron en las Torres Gemelas en el 01?</QUESTION><TE value=>el 01</TE> (CORR value=2001)</Q><Q id=92> (ACT: Yes CORR: No)<QUESTION>Que empresa fue fundada en el 75 por Bill Gates?</QUESTION><TE value=2008>el 75</TE> (CORR value=1975)</Q><Q id=97> (ACT: CORR: No)<QUESTION>Que ciudad fue la capital de Nicaragua en mil ochocientos cincuenta cinco?</QUESTION><TE value=></TE> (CORR <TE value=1855>mil ochocientos cincuenta cinco</TE>)<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)</Q><Q id=98> (ACT: CORR: No)<QUESTION>Cual fue la ciudad mas grande de Italia en el siglo XVII?</QUESTION><TE value=></TE> (CORR <TE value=16>el siglo XVII</TE>)<TYPE>1</TYPE> (CORR <TYPE>2</TYPE>)</Q><Q id=99> (ACT: Yes CORR: No)<QUESTION>Donde se celebro Eurovision en el ano 68?</QUESTION><TE value=2008>el ano 68</TE> (CORR value=1968)</Q><Q id=105> (ACT: Yes CORR: No)<QUESTION>Quien gano el Nobel de Fsica cuando el cometa Hale Bopp fue descubierto hace 13 anos?</QUESTION><Q-REST>Cuando cometio el Hale Bopp fue descubierto hace 13 anos?</Q-REST>(CORR <Q-REST>Cuando fue descubierto el cometa Hale Bopp hace 13 anos?</Q-REST>)</Q><Q id=108> (ACT: CORR: No)<QUESTION>Quien fue el presidente de los Estados Unidos cuando se fundo AARP hace cinco decadas?</QUESTION><TE value=></TE> (CORR <TE value=195>hace cinco decadas </TE>)<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)</Q>805fiSaquete, Vicedo, Martnez-Barco, Munoz, & Llorens<Q id=110> (ACT: Yes CORR: No)<QUESTION>Quien fue el Presidente de Espana justo despues de que se produjera el primer vuelo del Columbiaen los anos 80?</QUESTION><Q-REST>Cuando se produjo se el primer vuelo del Columbia en los anos 80?</Q-REST>(CORR <Q-REST>Cuando se produjo el primer vuelo del Columbia en los anos 80?</Q-REST>)</Q><Q id=114> (ACT: CORR: No)<QUESTION>Que empresa introdujo en el mercado el primer asiento con respaldo regulable un ano antes deque naciera Mariah Carey en los anos 60?</QUESTION><TE value=>un ano antes</TE> (CORR <TE value=196>los anos 60</TE>)<SIGNAL>antes de que</SIGNAL>(CORR <SIGNAL>un ano antes de que</SIGNAL>)<Q-FOCUS>Que empresa introdujo en el mercado el primer asiento con respaldo regulable un ano?</Q-FOCUS>(CORR <Q-FOCUS>Que empresa introdujo en el mercado el primer asiento con respaldo regulable?</QFOCUS>)</Q><Q id=116> (ACT: CORR: No)<QUESTION>Cuando gano Indurain el Tour un ano despues de que se estrenara Cadena Perpetua en los anos90?</QUESTION><TE value=>un ano despues</TE> (CORR <TE value=199>los anos 90</TE>)<SIGNAL>despues de que</SIGNAL>(CORR <SIGNAL>un ano despues de que</SIGNAL>)<Q-FOCUS>Cuando gano Indurain el Tour un ano?</Q-FOCUS>(CORR <Q-FOCUS>Cuando gano Indurain el Tour?</Q-FOCUS>)</Q><Q id=129> (ACT: CORR: No)<QUESTION>Quien fue el Rey de Espana despues de que Carlos IV reinara Espana durante el siglo XVIII?</QUESTION><TE value=></TE> (CORR <TE value=17>el siglo XVIII</TE>)<TYPE>4</TYPE> (CORR <TYPE>3</TYPE>)<SIGNAL>durante</SIGNAL> (CORR <SIGNAL>despues</SIGNAL>)<Q-REST>Cuando fue el siglo XVIII?</Q-REST>(CORR <Q-REST>Cuando reino Carlos IV Espana durante el siglo XVIII?</Q-REST>)</Q><Q id=130> (ACT: Yes CORR: No)<QUESTION>Quien gano Wimbledon femenino individuales antes de que Rafa Nadal ganara Wimbledon esteano?</QUESTION><TE value=>este ano</TE> (CORR <TE value=2008>este ano</TE>)</Q><Q id=133> (ACT: Yes CORR: No)<QUESTION>Que persona gano el premio Nobel de Literatura cuando James Dean nacio en el ano 31?</QUESTION><TE value=>el ano 31</TE> (CORR value=1931)<Q-REST>Cuando jamo Dean nacio en el ano 31?</Q-REST>(CORR <Q-REST>Cuando nacio James Dean en el ano 31?</Q-REST>)</Q><Q id=135> (ACT: CORR: No)<QUESTION>Quien fue el Presidente de Reino Unido cuando AARP fue fundada hace cinco decadas?</QUESTION><TE value=></TE> (CORR <TE value=195>hace cinco decadas</TE>)<TYPE>3</TYPE> (CORR <TYPE>4</TYPE>)</Q><Q id=142> (ACT: Yes CORR: No)<QUESTION>Que lengua fue inventada por Zamenhof cuando Berliner patento el disco de vinilo en la decadade 1880?</QUESTION><TE value=1880>1880</TE>(CORR <TE value=188>la decada de 1880</TE>)</Q>806fiEnhancing QA Systems Complex Temporal Question Processing Capabilities<Q id=143> (ACT: CORR: No)<QUESTION>Donde se celebraran las Olimpiadas cuando Polonia adopte el Euro en la decada de 2010?</QUESTION><TE value=></TE> (CORR <TE value=201>la decada de 2010</TE>)</Q><Q id=145> (ACT: Yes CORR: No)<QUESTION>Cuando gano Gary Becker el premio Nobel de Economa antes de que Zapatero fuera elegidoPresidente de Espana en los ultimos anos?</QUESTION><TE value=>los ultimos anos</TE> (CORR value=[2003-2008])</Q><Q id=148> (ACT: CORR: No)<QUESTION>Donde se celebro el Festival de Woodstock el 15 de agosto cuando el Unix fue desarrollado?</QUESTION><TE value=></TE> (CORR <TE value=XXXX-08-15>el 15 de agosto</TE>)</Q><Q id=155> (ACT: Yes CORR: No)<QUESTION>Quien gano el Nobel de Fsica cuando el cometa Hale Bopp fue descubierto?</QUESTION><Q-REST>Cuando cometio el Hale Bopp fue descubierto?</Q-REST>(CORR <Q-REST>Cuando fue descubierto el cometa Hale Bopp?</Q-REST>)</Q>ReferencesACL (2001). Association computational linguistics. http://www.aclweb.org/.Ahn, D. (2006). stages event extraction. Computational Linguistics, A.(Ed.), ARTE: Workshop 44th Annual Meeting Association ComputationalLinguistics, pp. 18, Sydney, Australia.ATEL (2008). Computational Language Education Research. University Colorado.http://timex2.mitre.org/taggers/timex2 taggers.html.Barzilay, R., Elhadad, N., & McKeown, K. (2002). Inferring strategies sentence orderingmultidocument news summarization. J. Artif. Intell. Res. (JAIR), 17, 3555.Breck, E., Burger, J., Ferro, L., Greiff, W., Light, M., Mani, I., & Rennie, J. (2000). AnotherSys Called Quanda. Ninth Text REtrieval Conference, Vol. 500-249 NIST SpecialPublication, pp. 369378. National Institute Standards Technology.CLEF (2008). Cross Language Evaluation Forum. http://www.clef-campaign.org/.COLING (2000).18th internationalhttp://coling.dfki.de/.conferencecomputationallinguistics.COLING-ACL (2006). 44th annual meeting association computational linguistics.http://www.aclweb.org/mirror/acl2006/.Dalli, A., & Wilks, Y. (2006). Annotating Dating Documents Temporal Text Classification. Computational Linguistics, A. (Ed.), ARTE: Workshop 44th AnnualMeeting Association Computational Linguistics, pp. 1122, Sydney, Australia.Demri, S., & Jensen, C. S. (Eds.). (2008). 15th International Symposium TemporalRepresentation Reasoning, Vol. 15 TIME symposium. IEEE Computer Society.807fiSaquete, Vicedo, Martnez-Barco, Munoz, & LlorensFilatova, E., & Hovy, E. (2001). Assigning Time-Stamps Event-Clauses. Proceedings2001 ACL Workshop Temporal Spatial Information Processing, pp. 8895.Harabagiu, S., Lacatusu, F., & Hickl, A. (2006). Answering complex questions randomwalk models. Proceedings 29th Annual International ACM SIGIR ConferenceResearch Development Information Retrieval, pp. 220227.Herrera, J., Penas, A., & Verdejo, F. (2005). Question answering pilot task CLEF 2004.Peters, C., Clough, P., Gonzalo, J., Jones, G. J., Kluck, M., & Magnini, B. (Eds.),Berlin Heidelberg New York, Vol. 3491 Lecture Notes Computer Science, pp.581590. Springer-Verlag.Katz, B. (1990). Using English indexing retrieving. Artificial intelligence MITexpanding frontiers, 1, 134165.Katz, B. (1997). Annotating World Wide Web using Natural Language. Proceedings5th RIAO Conference Computer Assisted Information SearchingInternet.Katz, B., Borchardt, G., & Felshin, S. (2005). Syntactic semantic decomposition strategies question ansering multiple resources. Proceedings AAAI 2005Workshop Inference Textual Question Answering, pp. 3541.Katz, G., & Arosio, F. (2001). Annotation Temporal Information Natural Language Sentences. Proceedings 2001 ACL Workshop Temporal SpatialInformation Processing, pp. 104111.Katz, G., Pustejovsky, J., & Schilder, F. (2005). Annotating, Extracting ReasoningTimeEvents.http://www.dagstuhl.de/en/programm/kalender/semhp/?semnr=05151.Lapata, M., & Lascarides, A. (2006). Learning Sentence-internal Temporal Relations. Journal Artificial Intelligence Research, 27, 85117.Lin, C.-J., & Liu, R.-R. (2008). Analysis Multi-Focus Questions. ProceedingsSIGIR 2008 Workshop Focused Retrieval, pp. 3036.LREC (2002). Proceedings LREC Workshop Temporal Annotation Standards.http://www.lrec-conf.org/lrec2002/.Magnini, B., Giampiccolo, D., Forner, P., Ayache, C., Jijkoun, V., Osenova, P., Penas,A.,Rocha,P.,Sacaleanu,B.,& Sutcliffe, R. (2006). Overview CLEF 2006 Multilingual Question AnsweringTrack. http://www.clef-campaign.org/2006/working notes/workingnotes2006/magniniOCLEF2006.pdf.Magnini, B., Vallin, A., Ayache, C., Erbach, G., Penas, A., de Rijke, M., Rocha, P., Simov,K., & Sutcliffe, R. (2005). Overview CLEF 2004 Multilingual Question Answering Track. Peters, C., Clough, P., Gonzalo, J., Jones, G. J., Kluck, M., & Magnini,B. (Eds.), Berlin Heidelberg New York, Vol. 3491 Lecture Notes Computer Science, pp. 371391. Springer-Verlag.Mani, I., & Wellner, B. (2006). Pilot Study Acquiring Metric Temporal ConstraintsEvents. Computational Linguistics, A. (Ed.), ARTE: Workshop 44th808fiEnhancing QA Systems Complex Temporal Question Processing CapabilitiesAnnual Meeting Association Computational Linguistics, pp. 2329, Sydney,Australia.Mani, I., & Wilson, G. (2000a). Processing news. Proceedings 38th AnnualMeeting Association Computational Linguistics (ACL2000), pp. 6976.Mani, I., & Wilson, G. (2000b). Robust temporal processing news. ACL (Ed.),Proceedings 38th Meeting Association Computational Linguistics (ACL2000), Hong Kong.Mani, I., & Wilson, G. (2002). Annotation Standards Temporal Information NaturalLanguage. Proceedings LREC Workshop Temporal Annotation Standards.Mazur, P., & Dale, R. (2007). DANTE temporal expression tagger. Proceedings3rd Language Technology Conference.Michelson, M., & Knoblock, C. A. (2008). Creating Relational Data UnstructuredUngrammatical Data Sources. J. Artif. Intell. Res. (JAIR), 31, 543590.Moia, T. (2001). Telling apart temporal locating adverbials time-denoting expressions.Proceedings 2001 ACL Workshop Temporal Spatial Information Processing.Moreda, P., Llorens, H., Saquete, E., & Palomar, M. (2008a). Automatic generalizationQA answer extraction module based semantic roles. 11th editionIbero-American Conference Artificial Intelligence (IBERAMIA 2008), AdvancesArtificial Intelligence. Springer-Verlag LNAI.Moreda, P., Llorens, H., Saquete, E., & Palomar, M. (2008b). influence SemanticRoles QA: comparative analysis. XXIV edicion del Congreso Anual de laSociedad Espanola para el Procesamiento del Lenguaje Natural 2008 (SEPLN 08) .Negri, M. (2007). Dealing italian temporal expressions: ITA-Chronos system.Proceedings EVALITA 2007, Workshop held conjunction AI*IA.Negri, M., Saquete, E., Martnez-Barco, P., & Munoz, R. (2006). Evaluating Knowledgebased Approaches Multilingual Extension Temporal Expression Normalizer.Computational Linguistics, A. (Ed.), ARTE: Workshop 44th Annual MeetingAssociation Computational Linguistics, pp. 3037, Sydney, Australia.Palomar, M., Ferrandez, A., Moreno, L., Martnez-Barco, P., Peral, J., Saiz-Noeda, M., &Munoz, R. (2001). algorithm anaphora resolution Spanish text. Computational Linguistics, 27 (4), 545567.Palomar, M., & Martnez-Barco, P. (2001). Computational approach anaphora resolutionSpanish dialogues. Journal Artificial Intelligence Research, 15 (4), 263287.Pan, F., Mulkar, R., & Hobbs, J. (2006a). Learning Event Durations Event Descriptions. 44th Annual Meeting Association Computational Linguistics, pp.393400.Pan, F., Mulkar-Mehta, R., & Hobbs, J. R. (2006b). Learning Event Durations EventDescriptions. ACL. Association Computer Linguistics.Prager, J. M., Chu-Carroll, J., & Czuba, K. (2004). Question Answering Using ConstraintSatisfaction: QA-By-Dossier-With-Contraints. ACL, pp. 574581.809fiSaquete, Vicedo, Martnez-Barco, Munoz, & LlorensPustejovsky, J. (2002). TERQAS: Time Event Recognition Question AnsweringSystems. www.timeml.org/terqas/.Pustejovsky, J., & Mani, I. (2008). TANGO: TimeML Annotation Graphical Organizer.http://www.timeml.org/site/tango/index.html.Radev,D.,&Sundheim,B. (2002). Using TimeML question answering. http://www.cs.brandeis.edu/~jamesp/arda/time/documentation/ TimeML-use-in-qa-v1.0.pdf.Saquete, E., Martnez-Barco, P., & Munoz, R. (2002). Recognising Tagging TemporalExpressions Spanish. Proceedings LREC Workshop Temporal Annotation Standards, pp. 4451.Saquete, E., Martnez-Barco, P., Munoz, R., & Vicedo, J. (2004). Splitting Complex Temporal Questions Question Answering systems. ACL (Ed.), 42nd Annual MeetingAssociation Computational Linguistics, pp. 566573, Barcelona, Espana.Saquete, E., Munoz, R., & Martnez-Barco, P. (2006). Event Ordering using TERSEOsystem. Data Knowledge Engineering Journal, 58 (1), 7089.Saur, R., Knippen, R., Verhagen, M., & Pustejovsky, J. (2005). EVITA: robust eventrecognizer QA systems. HLT 05: Proceedings conference HumanLanguage Technology Empirical Methods Natural Language Processing, pp.700707, Morristown, NJ, USA. Association Computational Linguistics.Schilder, F., & Habel, C. (2001). Temporal Expressions Temporal Information:Semantic Tagging News Messages. Proceedings 2001 ACL WorkshopTemporal Spatial Information Processing, pp. 6572.Setzer, A., & Gaizauskas, R. (2001). Pilot Study Annotating Temporal RelationsText. Proceedings 2001 ACL Workshop Temporal Spatial InformationProcessing.Setzer, A., & Gaizauskas, R. (2002). Importance Annotating Event-Event Temporal Relations Text. Proceedings LREC Workshop Temporal AnnotationStandards, pp. 5260.TempEx(2008).MITREhttp://timex2.mitre.org/taggers/timex2 taggers.html.TERN(2004).TimeExpressionhttp://timex2.mitre.org/tern.html.RecognitionCorporation.Normalization.TIME (2008). International Symposium Temporal Representation Reasoning.http://time.dico.unimi.it/.TimeML (2008).Markup Language Temporalhttp://www.timeml.org/site/index.html.EventExpressions.TREC (2008). Text REtrieval Conference. http://trec.nist.gov/.Verhagen, M., Gaizauskas, R., Schilder, F., Hepple, M., Katz, G., & Pustejosvky, J. (2007).Temeval-2007 task 15: Tempeval temporal relation identification. Proceedings4th International Workshop SemEval-2007, pp. 7580.810fiEnhancing QA Systems Complex Temporal Question Processing CapabilitiesVoorhees, E. M. (2002). Overview TREC 2002 Question Answering Track. EleventhText REtrieval Conference, Vol. 500-251 NIST Special Publication, pp. 115123.National Institute Standards Technology.Wilson, G., Mani, I., Sundheim, B., & Ferro, L. (2001). Multilingual ApproachAnnotating Extracting Temporal Information. Proceedings 2001 ACLWorkshop Temporal Spatial Information Processing, pp. 8187.811fiJournal Artificial Intelligence Research 35 (2009) 391 447Submitted 09/08; published 06/09Learning Bayesian Network Equivalence ClassesAnt Colony OptimizationRnn DalyRDALY @ DCS . GLA . AC . UKDepartment Computing ScienceUniversity GlasgowSir Alwyn Williams BuildingGlasgow, G12 8QQ, UKQiang ShenQQS @ ABER . AC . UKDepartment Computer ScienceAberystwyth UniversityPenglais CampusAberystwyth, SY23 3DB, UKAbstractBayesian networks useful tool representation uncertain knowledge. paperproposes new algorithm called ACO-E, learn structure Bayesian network.conducting search space equivalence classes Bayesian networks using AntColony Optimization (ACO). end, two novel extensions traditional ACO techniquesproposed implemented. Firstly, multiple types moves allowed. Secondly, movesgiven terms indices based construction graph nodes. results testingshow ACO-E performs better greedy search state-of-the-art metaheuristicalgorithms whilst searching space equivalence classes.1. Introductiontask learning Bayesian networks data has, relatively short amount time, becomemainstream application process knowledge discovery model building (Aitken, JirapechUmpai, & Daly, 2005; Heckerman, Mamdani, & Wellman, 1995). reasons many.one, model built process intuitive feel Bayesian network consistsdirected acyclic graph (DAG), conditional probability tables annotating node.node graph represents variable interest problem domain arcs (withcaveats) seen represent causal relations variables (Heckerman, Meek, & Cooper,1999) nature causal relations governed conditional probability tables associatednode/variable. example Bayesian network shown Figure 1.Another reason popularity Bayesian networks aside visual attractivenessmodel, underlying theory quite well understood solid foundation. Bayesiannetwork seen factorization joint probability distribution, conditionalprobability distributions node making factors graph structure makingmethod combination. equivalence, network answer probabilisticquestion regarding variables modeled.addition, popularity Bayesian networks increased accessibility methods query model learn structure parameters networkc2009AI Access Foundation. rights reserved.fiDALY & HENFigure 1: example Bayesian network(Daly, Shen, & Aitken, 2009). shown inference Bayesian networks NP-complete(Dagum & Luby, 1993; Shimony, 1994), approximate methods found performoperation acceptable amount time. Learning structure Bayesian networks alsoNP-complete (Chickering, 1996a), too, methods found render operationtractable. include greedy search, iterated hill climbing simulated annealing (Chickering,Geiger, & Heckerman, 1996). Recently however, heuristics become popularproblem combinatorial optimization high dimensional spaces. include approachestabu search (Glover, 1989, 1990), genetic algorithms (Mitchell, 1996) approachpaper investigate Ant Colony Optimization (ACO).ACO fairly recent, called metaheuristic, used solution combinatorially hardproblems (Dorigo & Sttzle, 2004). iterated, stochastic technique biased resultsprevious iterations (Birattari, Caro, & Dorigo, 2002). method modeled behaviorreal-life ants foraging food.Many ants secrete pheromone trail recognizable ants positively biasesfollow trail, stronger trail meaning likely biased towards it.time pheromone trail evaporates. hunting food, ants behavior randomly walkabout, perhaps following pheromone trail, finds food. returns directionwhence came. strength trail factor choosing follow it, antfaced two pheromone trails choose from, tend choose trailshighest concentration pheromone.characteristics, situation multiple paths food source, antsgenerally follow shortest path. explained follows. Assuming ants start nestpheromone trails present, randomly wander reach food sourcereturn home, laying pheromone way back. ant chooses shortest pathfood source return home quickest, means pheromone trail highestconcentration, pheromone laid per unit time. stronger trail cause ants392fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONprefer longer trails. ants leave pheromone short trail, therebyproviding reinforcing behavior choose trail others.computing technique, ACO roughly modeled behavior. Artificial ants walk aroundgraph nodes represent pieces solution. continue complete solutionfound. node, choice next edge traverse made, depending pheromone valueassociated edge problem specific heuristic. number ants performedtraversal graph, one best solutions chosen pheromone edgestaken increased, relative edges. biases ants towards choosing edgesfuture iterations. search stops problem specific criterion reached. couldstagnation quality solutions passage fixed amount time.paper seek use ACO technique learning Bayesian networks. Specifically,used learn equivalence class Bayesian network structures. end, restpaper structured following fashion. Firstly, in-depth studyproblem searching optimum Bayesian network, space Bayesian networksequivalence classes Bayesian networks. Then, new method formulatingsearch Bayesian network structure terms ACO metaheuristic introduced.method based part earlier work done topic (Chickering, 2002a; de Campos,Fernndez-Luna, Gmez, & Puerta, 2002). Next, results tests previous techniquesdiscussed finally, conclusions possible future directions stated.2. Searching Bayesian Network Structureare, general, three different methods used learning structure Bayesian networkdata. first finds conditional independencies data uses conditionalindependencies produce structure (Spirtes, Glymour, & Scheines, 2000). Probablywell known algorithms use method PC algorithm Spirtes Glymour (1990)CI FCI algorithms Spirtes, Meek, Richardson (1995) able identify latentvariables selection bias. second uses dynamic programming optionally, clustering,construct DAG (Ott, Imoto, & Miyano, 2004; Ott & Miyano, 2003). third methoddealt defines search space Bayesian networks. method usesscoring function defined implementer, says relatively good network comparedothers.Although classification three different methods noted useful differentiatingapplicability, boundaries often clear may seem. E.g.score search approach dynamic programming approach similaruse scoring functions. Indeed, view Cowell (2001) conditional independenceapproach equivalent minimizing Kullback-Leibler (KL) divergence (Kullback & Leibler,1951) using score search approach. discussing score search methodworks, definitions notation introduced.graph G given pair (V, E), V = {v1 , . . . , vn } set vertices nodesgraph E set edges arcs nodes V . directed graph graphedges associated direction one node another. directed acyclic graph DAG,directed graph without cycles, i.e. possible return node graph followingdirection arcs. illustration, graph Figure 2 DAG. parents node vi ,Pa (vi ), nodes v j arrow v j vi (v j vi ). descendants vi ,393fiDALY & HENFigure 2: directed acyclic graphFigure 3: skeleton DAG Figure 2(vi ), nodes (not including vi ) reachable vi following arrows forwardsdirection repeatedly. non-descendants vi , ND (vi ), nodes (not including vi )descendants vi .Let graph G = (V, E) joint probability distribution P nodes V . LetIP (X,Y |Z) mean variables set X conditionally independentvariables set probability distribution P given variables set Z. Say alsofollowing truev V. IP ({v} , ND (v) |Pa (v)) .is, node conditionally independent non-descendants, given parents.said G satisfies Markov condition P, (G, P) Bayesian network. Noticeconditional independencies implied Markov condition. allow joint distribution Pwritten product conditional distributions; P (v1 |Pa (v1 )) P (v2 |Pa (v2 )) P (vn |Pa (vn )) =P (v1 , v2 , . . . , vn ). However, importantly, reverse also true. Given DAG Geither discrete conditional distributions certain types continuous conditional distributions (e.g.Gaussians), form P (vi |Pa (vi )) exists joint probability distributionP (v1 , v2 , . . . , vn ) = P (v1 |Pa (v1 )) P (v2 |Pa (v2 )) P (vn |Pa (vn )) .means specify DAG known structure conditional probability distributions node given parents, often parameterised, Bayesian network,representation joint probability distribution.learning Bayesian network data, structure G parameters conditionalprobability distributions must learned, normally separately. case complete multinomialdata, problem learning parameters easy given certain reasonable assumptions,simple closed form formula (Heckerman, 1995). However, case learning structure,formula exists methods needed. fact, learning optimal structurediscrete variables NP-hard problem almost circumstances consequently enumerationtest network structures likely succeed (Chickering, 1996a; Chickering, Heckerman,& Meek, 2004). ten variables roughly 1018 possible DAGs. Whilst existdynamic programming methods handle roughly 30 variables discussed above, general,non-exact heuristic methods possibly tractable solution anything this.394fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONorder create space search through, three components needed. Firstlypossible solutions must identified set states space. Secondly representationmechanism state needed. Finally set operators must given, order movestate state space.search space defined, two pieces needed complete searchalgorithm, scoring function evaluates goodness fit structure set datasearch procedure decides operator apply, normally using scoring functionsee good particular operator application might be. example search proceduregreedy search, every stage applies operator produces best change structure,according scoring function. scoring function, various formul foundsee well DAG matches data sample.One functions given computing relative posterior probability structure Ggiven sample data D, i.e.S(G, D) = P(G, D) = P(D|G)P(G).likelihood term take many forms. One popular method called Bayesian Dirichlet(BD) metric. Here,ri (N 0 + N )n qi(Ni0j )jkjkP(D|G) =(1)00(Ni jk )i=1 j=1 (Ni j + Ni j ) k=1formula, n variables graph, first product variable.qi configurations parents node i, second product possible parentconfigurations, i.e. Cartesian product number possible values parent variabletake. variable take one ri possible values. value Ni jk number timesvariable = k parents configuration j data sample D. Ni j givenriNi jk , i.e. sum Ni jk possible values take on. Ni0j = ri=1Ni0jk ,k=1values Ni0jk given parameters give different variants BD metric. E.g. Ni0jk set 1K2 metric results, given Cooper Herskovits (1992). Ni0jk set N 0 /(ri qi ) (whereN 0 , known equivalent sample size measure confidence prior network),BDeu metric results proposed Buntine (1991) generalised Heckerman,Geiger, Chickering (1995).prior value P(G) measure probable particular structure dataseen. values often hard estimate massive numbers graphs,needing probability. Therefore, values often given uniform possiblenetwork structures possibly favouring structures less arcs.forms used scoring function S(G, D) = log P(D|G, ) d2 log N, knownBayesian information criterion (BIC) (Schwarz, 1978) S(G, D) = log P(D|G, ) d, knownAkaike Information Criterion (AIC) (Akaike, 1974). models, parameters givemaximum likelihood estimate likelihood, number free parameters structureN number samples data D.Traditionally, searching Bayesian network structure, set states set possibleBayesian network structures, representation DAG set operators various smalllocal changes DAG, e.g. adding, removing reversing arc, illustrated Table 1. type395fiDALY & HENOperatorInsert_Arc(X,Y)Delete_Arc(X,Y)Reverse_Arc(X,Y)Table 1: Basic modification operatorssearch convenient decomposition properties score functions,nS(G, D) = vi , PaG (vi ) , ,i=1scoring function takes node vi parents node graph G, PaG (vi ).Popular scoring functions BD metric decomposable manner. Successfulapplication operators also dependent changed graph DAG, i.e. cycleformed applying operator.3. Searching Space Equivalence ClassesAccording many scoring criteria, DAGs equivalent one another, senseproduce score other. known time DAGsequivalent one another, entail set independence constraintsother, even though structures different. According theorem Verma Pearl (1991),two DAGs equivalent skeletons set v-structures.skeleton undirected graph results undirecting edges DAG (see Figure3) v-structure (sometimes referred morality), head-to-head meeting two arcs,tails arcs joined. concepts illustrated Figure 4. notionequivalence, class DAGs equivalent defined, notatedClass(G).3.1 Representation Equivalence Classesapparent redundancy space DAGs, attempts made conductsearch Bayesian network structures space equivalence classes DAGs (Acid &de Campos, 2003; Chickering, 1996b, 2002a; Munteanu & Bendou, 2001). search setspace set equivalence classes DAGs referred E-space. representmembers equivalence class, different type structure used, known partially directedacyclic graph (PDAG). PDAG (an example shown Figure 5) graph maycontain undirected directed edges contains directed cycles notated396fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION(a) (X,Y, Z) v-structure(b) (X,Y, Z) v-structureFigure 4: V-StructuresFigure 5: partially directed acyclic graphFigure 6: PDAG exists consistent extensionherein P. equivalence class DAGs corresponding PDAG denoted Class(P),DAG G Class(P) G P skeleton set v-structures.Related idea consistent extension. DAG G skeletonset v-structures PDAG P said G consistent extension P.PDAGs DAG consistent extension itself. consistent extension exists,said PDAG admits consistent extension. PDAGs admit consistent extensionused represent equivalence class DAGs hence Bayesian network. examplePDAG consistent extension shown Figure 6. figure, directingedge x either way create v-structure exist PDAG henceconsistent extension exist.Directed edges PDAG either: compelled, made directed way; reversible,could undirected PDAG would still represent equivalence class.idea, completed PDAG (CPDAG) defined, every undirected edge reversibleequivalence class every directed edge compelled equivalence class. CPDAGdenoted P C . shown one-to-one mapping CPDAGP C Class(P C ). Therefore, supplying CPDAG, one uniquely denote set conditionalindependencies. in-depth look topic, see papers Andersson, Madigan,Perlman (1997) Chickering (1995).397fiDALY & HEN3.2 Techniques Searching Equivalence ClassesNote below, move referred application operator particular statesearch space.able conduct search space equivalence classes, method must ablefind whether particular move valid valid, good move is. tasksrelatively easy whilst searching space DAGs check whether move validequivalent check whether move keeps DAG acyclic. goodness move foundusing scoring function, rather scoring neighboring DAG search space,decomposability scoring criteria taken advantage of, resultnodes whose parent sets changed need scored.However, task checking move validity move score easy spaceequivalence classes. classes often represented PDAGs, discussed previoussection. one, instead checking cycles, checks also made unintendedv-structures created consistent extension PDAG. Scoring move also createsdifficulties, hard know extension hence changes parent sets nodesoccur, without actually performing extension. Also, local change PDAG might makenon-local change corresponding consistent extension force unnecessary applicationsscore function.problems voiced concerns Chickering (1996b). paper, validity checkingmoves performed trying obtain consistent extension resulting PDAG noneexists move valid. Scoring move achieved scoring changed nodesconsistent extension given. methods generic, resulted significant slowdownalgorithm execution, compared search space DAGs.alleviate problem, authors proposed improvements would allow move validitymove score computed without needing obtain consistent extension PDAG (Acid &de Campos, 2003; Chickering, 2002a; Munteanu & Bendou, 2001). done definingexplicit set operators, operator validity test corresponding score changefunction, could calculated PDAG. changes led speedup executiontime algorithm, result search space equivalence classes Bayesiannetworks became competitive search space Bayesian networks. example one setoperators given Table 2. table, variables x refer nodes graph.example, InsertU operator takes two nodes arguments, x y. seenoperators take two arguments, except MakeV, takes three arguments. operator alsoset validity tests must passed order application operator particulararguments valid. Finally, score difference old new PDAGs givenlast column.Note table:x parent set node x, i.e. set nodes directed arcs going node x;Nx neighbor set node x, i.e. set nodes undirected arcs going node x;Nx,y set shared neighbors nodes x y, i.e. Nx Ny ;x,y set parents x neighbors y, i.e. x Ny .398fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONOperatorEffectInsertUxyAddundirected arcxDeleteUxyDeleteundirected arcxValidity TestsChange Score1. Every undirected path xcontains node Nx,y2. x =Nx,y clique+xy, Nx,y(y, Nx,y )(y, Nx,y )+xy, Nx,y1. Every semi-directed pathInsertDxyAdd directedarc xx contains nodex,y2. x,y cliquey, x,y +x(y, x,y )3. x 6=DeleteDxyDelete directedarc xNy clique1. Every semi-directed pathReverseDxyReversedirected arcxxinclude edge xcontains node y,x Ny2. y,x cliqueMakeVxzyDirect undirectedarcs xzEvery undirected pathxcontains node Nx,yy, Ny x(y, Ny )y, x+ x, +yx y,x(y, )(x, x y,x )z+xz, +yz Nx,yz+ y, Nx,yz+xz, z Nx,y(y, Nx,y )Table 2: Validity conditions change score operator399fiDALY & HENAlso, convenience, +x notation {x} x notation \ {x}.notation set operators Table 2 come proposed Chickering (2002a).definitions include: undirected path path one node another followsundirected edges; semi-directed path path one node another follows undirectededges directed edges tail head; set nodes N clique, completelyconnected subgraph graph, (i.e. every node connected every subgraph).3.3 Advantages Searching E-spacerepresentation equivalence classes Bayesian network structures set operatorsmodify CPDAGs represent (e.g. insert undirected arc, insert directed arcetc.), search procedure proceed. However, reasons pursuing typesearch? Chickering (2002a) gives list reasons, discussed here.one, equivalence class represent many different DAGs single structure.DAG representation, time wasted rescoring DAGs equivalence class.search space DAGs, connectivity search space mean abilitymove particular neighboring equivalence class constrained particular representationgiven DAG. also problem given prior probability used scoring function.Whilst searching space DAGs, certain equivalence classes representedprior, many DAGs contained class. example givencase networks two nodes. B-space 3 possible structures, equalpriors give P (G) = 1/3, DAG G. However, two DAGs connected representequivalence class, giving effective prior 2/3. E-space 2 possible structures,equal priors give P (P) = 1/2, PDAG P. necessarily problemperforming model selection, becomes much issue performing model averaging.concerns motivated researchers. particular, recent implementations algorithmssearch space equivalence classes produced results show markedimprovement execution time small improvement learning accuracy, depending typedata set (Chickering, 2002a,b).4. Ant Colony OptimizationAnt colony optimization global optimization technique generally used area combinatorialproblems, i.e. problems set solutions discrete. Since inception present formDorigo (1992), ACO successfully applied many combinatorially hard problemsincluding sequential ordering problem (Gambardella & Dorgio, 2000), vehicle routingproblem (Bullnheimer, Hartl, & Strauss, 1999), bin-packing problem (Levine & Ducatelle, 2004)many (Costa & Hertz, 1997; Gambardella & Dorgio, 2000; Maniezzo & Colorni, 1999;Sttzle, 1998). diverse range applications must ask question naturesystem solve them.particular form ACO metaheuristic field swarm intelligence (Bonabeau,Dorigo, & Theraulaz, 1999), based behavior real-life ants forage food.metaheuristic general purpose heuristic guides other, problem specific heuristics,whilst swarm intelligence may defined as:400fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONalgorithms distributed problem-solving devices inspired collective behaviour social insect colonies animal societies (Bonabeau, Dorigo et al.,1999).conceptual framework ACO defined.4.1 Ant Colony OptimizationAnt colony optimization swarm intelligence technique based foraging behaviorreal-life ants. particular, uses principle stigmergy (the indirect communication agentsenvironment) communication mechanism. Real-life ants leave chemical trail behindexplore environment. trail known pheromone. moving around, antslikely follow path pheromone, path less (or no) pheromone.behavior investigated Deneubourg, Aron, Goss, Pasteels (1990), designedexperiment nest Argentine ants, food source two trails could setdifferent length. Ants would leave nest, find food source return back food.trails length, found ants would eventually settle single trailtravel nest. behavior explained follows.experiment begins, ants initially choose one trails random. Whilst traversingtrail, deposit pheromone. causes following ants choose trail initial ants tookoften, deposit pheromone trail. Again, causes ants chooseinitially chosen trail, greater degree first set ants. Put another way, antchooses certain trail reinforces probability following ants choose trail. trailinitially gets chosen ants pheromone deposited per unit time hencepositive feedback autocatalytic process created, eventually ants converge singletrail.trails start different lengths, found ants converge shorter trailoften longer. explained ants able traverse shortertrail food source return nest amount time would take traverselonger trail. ants traversing trail, pheromone deposited, ants eventuallyconverge path.behavior ants faced trails different lengths ACO modeled upon.Instead real-life ants, artificial ants conceived computing unit. Instead trails, antstraverse construction graph. paths ants take graph solutions problemlooked idea reinforce pheromone better solutions. However, fundamental idealaying pheromone kept, ants depositing arcs traverse node node.Also, ants programmed follow arcs stronger pheromone often arcs weakerpheromone.Artificial ants useful real-life ants given memory.stop ants looping around helps laying pheromone return journey. Alsoprogrammed use problem dependent heuristics, guide search towards bettersolutions. ideas discussed.4.2 ACO MetaheuristicNowadays, ACO algorithms tend defined terms ACO metaheuristic (Dorigo & DiCaro, 1999). metaheuristic general purpose heuristic guides other, problem specific401fiDALY & HENheuristics. Examples metaheuristics include simulated annealing (Kirkpatrick, Gelatt, & Vecchi,1983), tabu search (Glover, 1989, 1990), evolutionary computation etc.ACO metaheuristic, problem represented triple (S, f , ), setcandidate solutions, f : objective scoring function measures solutions qualityparticular time : set constraints time , used solutions construction.range f dependent particular instance metaheuristic. trying mapcombinatorial optimization problem onto representation, following framework used.finite set solution components C = {c1 , c2 , . . . , cNc }. buildingblocks candidate solutions.ffproblem states represented sequences solution components x = ci , c j , . . . .set possible sequences given X .set candidate solutions mentioned subset X , i.e. X .set feasible states X , X X . feasible state x X statepossible add components C x create solution satisfying constraints .candidate solution cost g(s,t). Normally g(s,t) f (s,t), S,= X set feasible candidate solutions. However, might alwayscase; f expensive compute, g might easier compute function broadlysimilar f used generation solutions.set optimal solutions non-empty, S.Sometimes may also possible associate cost J(x,t) state x Xcandidate solution.framework, solutions problem (S, f , ) generated artificial antsperform random walk complete graph G defined components C. graph Gknown construction graph. random walk graph series moves node nodegraph, move random degree. walk Markovian, nextmove always completely random; next move influenced previous moves.Hence, using terminology ACO non-Markovian. walk ant makes generallybiased two things heuristic value (i heuristic associated individual nodesG, j associated edges G) pheromone trail (again, pheromoneassociated individual nodes G, j pheromone associated edgesG). way heuristic pheromone implemented problem dependent, generalheuristic measure goodness taking particular move construction graphdefined local measure. pheromone measure goodness takingparticular move defined aggregate behavior ants selecting move qualitysolutions ants generate.Finally, artificial ant k following properties order fully specify randomwalk proceed:Memory ant k memory Mk stores information path far followed.Start State ant k start state xsk non-empty set termination conditions ek .402fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONTermination Criteria ant state x, checks one termination criteria eksatisfied. not, moves node j N k (x). N k function returns neighborhoodnode x, i.e. nodes construction graph G reached currentstate, given constraints .Decision Rule ant chooses node j according probabilistic decision rule,function pheromone heuristic . specification rules problemdependent, usually random choice biased towards moves higher heuristicpheromone value.Pheromone Update pheromone path modified ant traversing it,return journey, returns start. Again, problem dependent, standardformulation increase pheromone good solutions decrease pheromonebad solutions, good bad given specific formulation.terms algorithmic actions, ACO algorithm normally broken three parts.are:ConstructAntsSolutions part algorithm concerned sending ants aroundconstruction graph according rules given above.UpdatePheromones part concerned changing values pheromones,depositing evaporating. Parts task might performed ants traversalgraph, ants traversal finished iteration ants traversals.DaemonActions part algorithm performs tasks directly related ants. E.g.local search procedure might performed ant finishes traversal.Given framework, multiple artificial ants released perform random walk.procedure repeated number times, pheromone gradually increasing best partssolution.many implementations metaheuristic. first originalACO system designed Dorigo, Maniezzo, Colorni (1996) known Ant System. usedstudy traveling salesman problem, construction graph defined distancescities. Another extension Ant System Ant Colony System (ACS) (Dorigo & Gambardella,1997). Here, search biased towards best-so-far path, pseudo-random proportionaldecision rule takes best solution component time normal randomproportional decision rule rest time. Also, best-so-far ant deposits pheromone.ACS based system known ANT-Q designed Gambardella Dorigo (1995),inspired reinforcement learning technique Q-learning (Sutton & Barto, 1998).ACS particularly interesting context, system new work describedlater sections modeled. work inspired previous approachlearning Bayesian networks using ACO (described Section 5.1) used ACS formACO.5. Using Ant Colony Optimization Learning Equivalence Classdate, many state-based search algorithms create Bayesian network structure reliedsimple techniques greedy-based searches. produce good results, ever403fiDALY & HENprevalent problem getting caught local minima. sophisticated heuristics applied,iterated hill climbing simulated annealing (Chickering, Geiger et al., 1996), far,none applied E-space. related approach, Acid de Campos (2003)applied tabu search space restricted partially directed acyclic graphs (RPDAGs), halfwayhouse spaces given DAGs CPDAGs.paper seeks apply ACO metaheuristic E-space, space equivalence classesDAGs. end, two extensions made basic metaheuristic. first allow multipletypes moves. allow one operator used traversing state space.needed, general, one type operator used whilst searching E-space.second allow pheromone accessed arbitrary values normally accessedsingle index two indices. needed operators used E-spaceMakeV operator takes three nodes arguments.proposed algorithm, ACO-E, based large part work de Campos, FernndezLuna et al. (2002), described next section.5.1 ACO Algorithms Learning Bayesian Network StructuresWhilst ACO applied many problems area combinatorial optimization, datemuch research using technique learn Bayesian network structures. Twoalternate methods defined de Campos, Fernndez-Luna et al. (2002) de Campos,Gmez, Puerta (2002). first conducts search space orderings DAGs, whilstsecond searches space DAGs. Since main topic work problem,description given here, order examine early work done subjectsee inform future studies.5.1.1 ACO-K2SNfirst technique, known ACO-K2SN, searching space orderings DAGs,various problem components, taken Section 4.2 defined follows:Construction Graph one node attribute data, extra dummy nodesearch starts.Constraints constraints tour Hamiltonian path.Pheromone Trails pheromone associated arc graph. arc graphintialised initial small value.Heuristic Information heuristic arc set inverse negative log likelihoodscore explained below.Solution Construction ants work system similar ACS system. Beginningdummy node, ants construct complete path defines ordering nodes.Pheromone Update works exactly ACS, local pheromone updates global updatebest-so-far solution.Local Search version local search orderings known HCSN (de Campos & Puerta, 2001a).used last iteration run.404fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONGiven components, search ordering proceeds follows. Starting dummynode ant decides node go next. first node ordering. choosenode, heuristic information pheromone used. heuristic arc j given1fi,j = fifif (x j, Pa (x j ))fif scoring metric used Pa (x j ), parents x j found K2 algorithm,possible parents nodes already visited. initial pheromone value 0 given0 =1,n | f (SK2SN )|SK2SN structure given K2SN algorithm de Campos Puerta (2001b).update value pheromone givenj =1,| f (S+ )|S+ best-so-far structure.5.1.2 ACO-Bsecond algorithm given de Campos, Fernndez-Luna et al. (2002) ACO-B algorithm.components algorithm are:Construction Graph one node possible directed arc pair attributes(excluding self directed arcs). also dummy node ants start from.Constraints constraints DAG must acyclic step.Pheromone Trails pheromone associated node graph. pheromonenode (i, j) corresponds directed arc j i.Heuristic Information heuristic node (i, j) gain score would occuradding arc j i.Solution Construction ants work system similar ACS system. Beginningdummy node, ants construct path defines arcs added DAG.process ends gain score.Pheromone Update works exactly ACS, local pheromone updates global updatebest far solution.Local Search standard greedy search arc addition, deletion reversal carriedcurrent candidate DAG. done every 10 iterations.opposed ACO-K2SN algorithm given Section 5.1.1, search spaceDAGs, orderings DAGs. Otherwise, similarities definitions partsalgorithm. heuristic givenj = f xi , Pa (xi ) x j f (xi , Pa (xi )) ,405fiDALY & HENis, change score adding arc j candidate DAG. initial pheromonegiven1,0 =n | f (SK2SN )|i.e. heuristic ACO-K2SN. Also, pheromone update valueACO-K2SN, i.e.1j =| f (S+ )|5.1.3 P ERFORMANCE C OMPARISONresults given de Campos, Gmez et al. (2002) de Campos, Fernndez-Luna et al.(2002), ACO-B algorithm performs slightly better terms accuracy ACO-K2SN acrossALARM (Beinlich, Suermondt, Chavez, & Cooper, 1989) INSURANCE (van der Putten &van Someren, 2004) gold-standard networks. also contains order magnitude less statisticaltests always faster. comparisons ACO-B algorithmsde Campos, Fernndez-Luna et al. (2002). Here, compared ILS, iterative localsearch algorithm random perturbations local maximum two estimation distribution(EDA) genetic algorithms, univariate marginal distribution algorithm (UMDA) Mhlenbein(1997) population-based incremental learning algorithm (PBIL) Baluja (1994). Comparedacross ALARM, INSURANCE BOBLO (Rasmussen, 1995) networks, ACO-B performedbetter methods.5.2 Relation ACO-E ACO Metaheuristicproposed algorithm, ACO-E, based large part, work de Campos, Fernndez-Lunaet al. (2002). work, ACO algorithm called ACO-B applied learning Bayesiannetworks. current work differs searches E-space, uses one operator (addarc) constrain using matrices store pheromone. algorithm shownAlgorithm 1.section, relation various parts algorithm ACO frameworkgiven. problem learning Bayesian network structure stated triple (S, f , ),S, set candidate solutions, set CPDAGs nodes Bayesiannetwork. set massive cardinality, super-exponential number nodes.f , objective function function used score candidate DAG. function wouldgenerally one scoring criteria mentioned Section 2., set constraints, makes sure PDAGs consistent extensionsgenerated solutions. explanation idea consistent extension PDAG givenSection 3.1. formulation presented, constraints implicit operatorsused move state state.Given statement problem, ACO-E algorithm described followingproperties. properties relate ACO metaheuristic described Section 4.2.406fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONAlgorithm 1 ACO-EInput: Operators O, tmax , tstep , m, , q0 , , nOutput: PDAG P +(P + , Path+ ) GREEDY- E(P empty , Pathempty )0 1/n |SCORE (P + )|operatorpossible move P empty0endend1 tmaxk 1P k , Pathk ANT- E(O, q0 , , , 0 )(t mod tstep = 0)P k , Pathk GREEDY- E P k , Pathkendendkb arg maxmSCOREPk=1SCORE P b > SCORE (P + )P+ PbPath+ Pathbendmove Path+(1 ) + / |SCORE (P + )|endendreturn P +5.2.1 C ONSTRUCTION G RAPHconstruction graph ACO algorithm describes mechanism solutionsassembled. specified complete graph given solution components. such,components play crucial part viability algorithm.ACO-E algorithm, components C construction graph various movesmay made, i.e. move instantiation supplied operator; experiments presentedpaper, six operators Table 2 used. operators used verifiedwork correctly effectively Chickering (2002a). Designing correct operators difficult,Chickering showed finding counter examples validity operators MunteanuCau (2000). ant constructs solution walking construction graph. correspondsapplying sequence moves CPDAG. order procedure begin, starting state mustspecified. ACO-E given empty graph.usual, states problem sequences moves. However, every statecandidate solution, = X ACO metaheuristic framework. imply statesfeasible candidate solutions, candidate solutions length. alsomeans = X . Another way view state ant consider empty graph P (the407fiDALY & HENffstarting state) current state sequence moves (components) x = ci , . . . , c j . Applyingmove c x order P give CPDAG another representation current state.noted constraints implicitly taken care operators, i.e.validity tests operators satisfy constraint state valid PDAG. alsostated usual definitiong(s,t) f (s,t),applies, function J(x,t), since x candidate solutions adding solutioncomponent decrease cost.5.2.2 P ROBLEM H EURISTICACO algorithm, heuristic used guide search good solutions. oftenimplicitly terms cost associated choosing particular component add currentstate; adding component least cost often useful way proceeding constructingsolution.ACO-E, heuristic used manner, addition cost addingcomponent negative, i.e. adding component current state improve cost functiong. heuristic dynamic depends current state ant. Also, associatedcomponent c C opposed arcs ci c j components.value heuristic given score gain move ci C possible givencurrent state. essence corresponds change score given performing particularmove current CPDAG. operators used article, means valuesTable 2.5.2.3 P ROBLEM P HEROMONEpheromone ACO algorithm guides search based results previous searches.many instances, associated arcs construction graph, ACO-E associatednodes construction graph. gives pheromone values ci C.pheromone initialised value 0 given0 =1n |SCORE (P + )|.(2)formula, n number variables data, SCORE objective functionf , defined Section 5.2 P + best-so-far solution. start algorithm,initialised found greedy search starting empty graph.order pheromone may change reflect tours ants, pheromone update rulesgiven. Similar ACS, local evaporation rule, whereby pheromone removed pathant traverses(1 ) + 0shows effect parameter , pheromone evaporation deposition rate.formula, implicit bounds high low pheromone componentget. Also similar ACS, global pheromone update rule deposits new pheromonebest-so-far pathfifi(1 ) + / fiSCORE P + fi408fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONoccurs end run ants. Again, SCORE P + defined Equation 2. Alsoagain, formula implements implicit limits values pheromone take.5.2.4 P ROBABILISTIC RANSITION RULEchoosing component visit next given particular state, ACO algorithm utilisesprobabilistic transition rule. rule normally uses values given heuristic pheromoneinform choice node pick. actual choice random based distributiongiven heuristic pheromone possible choice. ACO-E, probabilistic choicerule given pseudo random proportional choice rule, similar one used ACS.type rule allows balance exploration exploitation varied. ablechange balance important, shown produce quite different results (Dorigo &Sttzle, 2004). ant chooses component cm , given(arg maxmN (x) [m ] , q q0random proportional,otherwise.formula, N (x) set components ant state x move to, given problemconstraints . rule pseudo-random proportional, sometimes behaves mannerrandom. random number q drawn uniformly range [0, 1]. numberless equal parameter q0 , rule behaves greedily; best move possible takendependent value [m ] component cm . Here, pheromoneheuristic explained previously parameter says much favour heuristicpheromone.number q greater q0 random proportional rule used selectcomponent visit next. probability ant visit component cm given pm ,pm =[m ],N (x)N (x).(3)seen probability ant moves component cm directly given [m ] ,normalised possible moves range [0, 1].5.2.5 P ROPERTIES NTSterms ants used construct solutions, following properties ant k noted:memory Mk equated current state problem given ant k. this,current CPDAG constructed order implement constraints , computeheuristic values , evaluate current solution lay pheromone tour. practice,current CPDAG normally kept order avoid recompute every step.start state xsk given empty sequence hi, i.e. empty CPDAG.single termination condition ek , stop tour improvement scorepossible.neighborhood N k (x) set valid moves given current CPDAG.409fiDALY & HEN5.2.6 L OCAL EARCH P ROCEDUREoften case ACO algorithms, ACO-E use local search procedure intermediatepoints throughout run algorithm end. local search procedure usedquickly bring solution local maximum. current heuristic standard localsearch would used circumstances greedy search operators defined Table2, known GREEDY-E local search would provide additional benefit solutionfound ant. Nevertheless, local search put algorithm case problemheuristic implemented differently. example would static heuristic obtainedscoring operations empty graph. Since invariant algorithm run, wouldneed calculated start run.5.3 Description ACO-Esection focus giving algorithmic description ACO-E. done conjunctionpseudo code given Algorithms 1 2. ACO-E takes input number parametersreturns best PDAG found, according scoring criterion SCORE , defined objectivefunction f . assumed scoring criteria generally give negative values; higher value,better model. case standard criteria discussed Section 2.meaning parameters follows:set operators modify current PDAG state search. Examplesones given Table 2, e.g. InsertU, DeleteU, etc. However, operators couldused, e.g. Munteanu Cau (2000) Munteanu Bendou (2001).tmax number iterations algorithm run. iteration, number antsconstruct solutions. Pheromone deposition happens ants finished tours.tstep gap, iterations, local search procedures run. settstep > tmax , local search happens end algorithm run.number ants run iteration.This, value [0, 1], rate pheromone evaporates deposited. usedpheromone evaporation pheromone deposition rules Section 5.2.3.q0 This, value [0, 1], gives preference exploitation exploration. usedpseudo-random probabilistic transition rule explained Section 5.2.4.exponent gives relative importance heuristic pheromone levels decidingchance particular trail followed. used pseudo-random probabilistictransition rule Section 5.2.4.n number nodes PDAG.also variables algorithm. include:P + best-so-far PDAG;Path+ best-so-far path;410fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONAlgorithm 2 ANT-EInput: Operators O, , q0 ,Output: PDAG P, Path PathEmpty PDAG P, Empty path Pathtruepossible moves P using|M| = 0 maxlM TOTAL - SCORE(l, ) 0return (P, Path)endq random number [0, 1)q q0l arg maxlM TOTAL - SCORE(l)elsel random according Equation 3end(1 ) l + 0P apply l PPath append l PathendP empty empty PDAG;Pathempty empty path, i.e. path entries.starting algorithm, greedy search (called GREEDY-E) performed. searchspace equivalence classes using framework operators given Chickering (2002a)shown Table 2. gives starting best-so-far graph path search proceed.Pheromone levels solution component initialised 0 = 1/n |SCORE (P + )|.main loop algorithm begins tmax iterations. iteration, ants perform search,given algorithm ANT-E, shown Algorithm 2. Also, every tstep iterations, local searchperformed PDAGs returned ANT-E, try improve results. Using local searchpart ACO algorithm common technique (Dorigo & Sttzle, 2004), easy wayobtain good results little effort. ants traversed graph, best graphAlgorithm 3 TOTAL - SCOREInput: Move l,Output: Score(l (l )return =0l > 0otherwisepath selected best-so-far graph path ones found antscurrent iteration. Finally, global pheromone update lays evaporates pheromonebest-so-far path.ANT-E algorithm creates PDAG examining various states may proceededcurrent state, given set operators may act current PDAG. selects411fiDALY & HENFigure 7: Bayesian network used sample tracenew state based random-proportional choice rule. parameters functiondescription ones ACO-E function.Starting out, algorithm constructs empty PDAG. stage move madenew PDAG, reached applying one operators O. Initially, numbergiven move TOTAL - SCORE, shown Algorithm 3. number represents weight givenmove l depending current pheromone associated making move l ,heuristic associated making move l . heuristic given increase score obtainedtaking move, higher overall scores meaning better solutions. increasescore, ant stops returns solution P path followed. Otherwise possiblemove ant decides make it. Firstly random number q obtained. lessspecified value q0 , best move taken. greater q0 , random proportionalchoice made, probability better moves higher. this, local pheromoneupdate applied path taken, path updated new location endcurrent state updated become new state given l. Note applying move CPDAGchange state implies resulting PDAG extended DAG suitable method(e.g., Dor & Tarsi, 1992) DAG changed back CPDAG. Details foundarticle Chickering (2002a).5.4 Trace Algorithm Executionsimple example execution ACO-E algorithm, trace behavioractual execution given section. Consider Bayesian network Figure 7.network fully specified, DAG structure parameters given form conditional412fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONq0tmax0.10.11.012Table 3: Parameters sample traceMove1InsertU(0,1)InsertU(0,2)InsertU(1,2)0.003120.003120.00312-0.2156534.652711.22042InsertU(0,1)InsertU(1,2)DeleteU(0,2)0.003120.003120.00312-0.2156511.2204-34.65273InsertU(0,1)DeleteU(0,2)DeleteU(1,2)MakeV(0,1,2)0.003120.003120.003120.003120.37742-34.6527-11.22040.59307Move4 (Ant 1)InsertU(0,1)DeleteD(0,2)DeleteD(1,2)ReverseD(0,2)ReverseD(1,2)0.003120.003120.003120.003120.00312-0.21565-35.2457-11.8134-0.59306-0.593074 (Ant 2)DeleteU(0,1)DeleteU(0,2)DeleteU(1,2)0.003120.003120.00312-0.47742-35.2457-11.8134Table 4: Values corresponding moves Figure 8probability tables. seen, variable 0 take values b, variable 1take values c variable 2 take values e, f g.purposes demonstration, 90 data sampled Bayesian network.ACO-E algorithm started parameters set Table 3. PDAG foundinitial GREEDY-E run sample Bayesian network structure. P + setPDAG. score PDAG 106.918. 0 set 0.00312. tmax set1, one iteration algorithm. iteration, two ants constructed solutionsusing ANT-E procedure. trace ants proceeded shown Figure 8 Table4. diagram, sequence moves seen along value q step.score final network ant also shown. table, possible moves pointant shown, along pheromone heuristic value . notedpheromone moves, start ACO-E algorithmpheromone deposition occurred. move, pheromone evaporation occurs, more,difference found pheromone values equal 0 .two ants finish run, best solution chosen variable b. caseAnt 1, score -106.918. compared score P + . two structuressame, score difference hence change occurs. Pheromone depositionoccurs moves made P + , i.e. moves Path+ . case, pheromoneInsertU(0,2), InsertU(1,2) MakeV(0,1,2) got updated (1 0.1) 0.00312 + 0.1/ |106.918| =0.00374. Since tmax set 1, iterations algorithm returns P + .413fiDALY & HENFigure 8: Trace progress ANT-E414fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION5.5 Implementation Issuesimplementing algorithms given paper, care must taken avoid long run times.Firstly, caching score node given parents simple technique greatly improveperformance. Secondly, caching results validity tests needed check movesapplicable certain state, increase performance dramatically. However techniqueeasy implement might appear (Daly, Shen, & Aitken, 2006).Care must also taken implementing pheromone moves. Traditionally, matricesvalues used, allow fast access updating. However case MakeV operator,takes three indices, three dimensional matrix would needed. would quickly becomeinfeasible problem size grew, especially entries would used. woulddue algorithm never getting states. Instead structure map storeinformation. map scale linearly number elements actually used. mapimplemented tree, entries accessed logarithmic time hash table used, accessconstant time.6. Experimental Methodologysection concerned testing ACO-E algorithm presented Section 5 evaluationresults produced. order facilitate understanding experimental methodology used,section structured follows.Firstly, account given objects testing performed.objects six gold-standard Bayesian networks well known field. variousproperties networks discussed. networks data sampleddata used input algorithms.Then, experiments using ACO-E algorithm shown. methodology used runningexperiments defined, along description various evaluation criteria.involve criteria well known field. Two different sets experiments presented, onefocused comparison ACO-E similar algorithms, comparison ACO-Estate-of-the-art algorithms. Also, behavior ACO-E algorithm different parametersshown.6.1 Standard Bayesian Networkssection set six gold-standard Bayesian networks presented. networksbasis testing showcased later. Various properties networks given,covering: number nodes structure, number edges structure, averagenumber edges etc.6.1.1 IX G OLD -S TANDARD N ETWORKSexperiments shown next section, six gold-standard networks used.ALARM (Beinlich, Suermondt et al., 1989), Barley (Kristensen & Rasmussen, 2002), Diabetes(Andreassen, Hovorka, Benn, Olesen, & Carson, 1991), HailFinder (Abramson, Brown, Edwards,Murphy, & Winkler, 1996), Mildew (Jensen, 1995) Win95pts networks (Microsoft Research,1995). networks chosen covered wide range domains, easily415fiDALY & HENNodesEdgesMean In-DegreeV-StructuresV-Struct/NodesAlarmBarleyDiabetesHailFinderMildewWin95pts37461.24260.7048841.75661.3836481.33210.5856661.18370.6635461.31371.06761121.471351.78Table 5: Bayesian network propertiesavailable contained discrete attributes. last property important scoringcriterion would used experiments implemented multinomial random variables.Various properties Bayesian networks shown Table 5. table, NodesEdges specify number nodes edges respectively graph. Mean In-Degreeaverage number arcs coming node graph. equal Mean Out-Degreenumber edges divided number nodes. Finally, V-Structures V-Struct/Nodes showamount v-structures graph amount v-structures divided numbernodes.6.2 Methodologysection contains details experiments performed using ACO-E algorithm describedSection 5. Firstly, methodology used running experiments presented. includesanalysis needed outcomes, design five experimental conditions explanationevaluation criteria.6.2.1 E XPERIMENTAL ESIGNdesigning experimental methodology test efficacy ACO-E algorithm, three differentoutcomes desired.first analyze behavior algorithm function parameterstest networks. needed order try understand range valuesparameters might useful show effect ACO behavior outcomes.next desired outcome test ACO-E similar algorithms. end,ACO-E tested another ACO algorithm algorithms searched spaceequivalence classes.Finally last desired outcome test ACO-E state-of-the-art algorithmsliterature. tests would show comparative usefulness ACO-Ewell-known good-performing methods.order obtain outcomes, various experimental conditions designed,explained below.Scoring Function experiments, decided use BDeu criterion inventedBuntine (1991) described Section 2. According study Shaughnessy Livingston416fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION(2005), BDeu best tradeoff precision recall edges (confusingly BDeucalled BAYES study, BDeu study meaning K2 metric). criterion givesfully Bayesian score, assumption Dirichlet parameter priors uniform priorpossible states joint distribution given prior network.To fully specify BDeu criterion,two pieces information needed. First prior structures P (G). could uniformprior, structures P (G). Another method shown Heckerman, Geigeret al. (1995) expert specify structure, method penalises differencesexperts structure candidate structure.second piece information needed equivalent sample size, N 0 , parameterencodes confidence prior parameters prior structure. Selecting valuetroublesome (Silander, Kontkanen, & Myllymaki, 2007; Steck & Jaakkola, 2003), reasonablevalues range [1, 10] often work well.recognition simpler structures often appealing, prior specifiedmethod shown Heckerman, Geiger et al. (1995). formulation, two objects specified;prior structure G prior prior distribution given as:P (G) = c ,c normalisation constant ignored, parameter needs specifiedgiven formulan= ,i=1symmetric difference parent set node G prior G.Condition 1 Experimental condition 1 designed analyze behavior ACO-E acrossdifferent parameters compare similar algorithms. algorithms ACOB (de Campos, Fernndez-Luna et al., 2002), EPQ (Cotta & Muruzbal, 2004; Muruzbal &Cotta, 2004) greedy search space equivalence classes using Chickerings operators(Chickering, 2002a) (called GREEDY-E here). description given.ACO-B ACO-E based part construction algorithm similarities. ACO-B ACO based algorithm provides search space DAGs,moves addition directed arc current DAG. detaileddescription given Section 5.1.2.EPQ method uses evolutionary programming algorithm performs searchspace equivalence classes DAGs. Like Chickering (2002a), explicitly use CPDAGs(defined Section 3.1) represent individuals, i.e. equivalence classes DAGs.generation, population P, members population selected using binarytournament mutated using operators Chickering. best P 2P selectedput forward next round, rounds.GREEDY-E algorithm uses operators Chickering perform greedy search spaceCPDAGs. results tests performed Chickering showed search generallyperformed better search space DAGs.417fiDALY & HENParameterValueN040.22005, 7, 10, 12, 15, 200.0, 0.1, 0.2, 0.3, 0.4, 0.50.7, 0.75, 0.8, 0.85, 0.9, 0.950.0, 0.5, 1.0, 1.5, 2.0, 2.5tmaxq0Table 6: Parameter values testing ACO-Eexperiments section, testing involved six standard networks presented Section6.1.1. BDeu scoring criterion used, suggested Kayaalp Cooper (2002)Heckerman, Geiger et al. (1995), equivalent sample size 4 used parameter priors.Also empty structure prior defined Heckerman, Geiger et al. (1995) used.individual run, 10,000 data sampled network used construct scoringfunction. combination values parameter settings , q0 , m, runexperiment made ACO-E ACO-B algorithms. range valuesparameters taken shown Table 6.total gave 1296 runs algorithm, network. consequence, gavetotal 216 results setting parameter. order match number runs,EPQ GREEDY-E algorithm also run 216 times each. stressed runACO-E using particular combination parameters run EPQ GREEDY-Edone different data set sampled network. technique guards overfittingparameters particular data set. also noted algorithm, limit 5parents allowed node, order speed algorithm execution.Condition 2 Experimental condition 2 designed test ACO-E state-of-the-artBayesian network structure learning algorithms. purposes results found studyconducted Tsamardinos, Brown, Aliferis (2006) used. study produced thoroughcomparison many different algorithms made results available, allows resultsACO-E compared algorithms used study. various parametersused ACO-E (that equivalent parameters algorithms) kept close possibleused Tsamardinos, Brown et al. various algorithms comparedwere: max-min hill-climbing algorithm (MMHC) (Tsamardinos, Brown et al., 2006), optimalreinsertion algorithm (OR) (Moore & Wong, 2003), sparse candidate algorithm (SC) (Friedman,Nachman, & Peer, 1999), greedy search using three standard operators Table 1 (GS),PC algorithm (PC) (Spirtes, Glymour et al., 2000), three phase dependency analysis algorithm(TPDA) (Cheng, Greiner, Kelly, Bell, & Liu, 2002) greedy equivalent search algorithm(GES) (Chickering, 2002b).experiments, testing involved four six standard networks presented Section6.1.1; Alarm, Barley, HailFinder Mildew. networks used experimentsTsamardinos, Brown et al. use two (Diabetes Win95pts). networksshown paper Tsamardinos, Brown et al. used available usable418fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONParameterValueN0100.09tmax200200.4q00.750.75Table 7: Parameter values testing ACO-Eq0AlarmBarleyDiabetesHailFinderMildewWin95pts0.40.80.50.40.81.00.40.71.00.20.81.00.40.70.50.20.952.5Table 8: Tuned parameters ACO-Eformat. run algorithm, 5000 data generated sampling particular networksquestion. chosen opposed 10,000 data Condition 1, amountchosen Tsamardinos, Brown et al.Condition 1, BDeu scoring function used. parameter values functionACO-E parameters shown Table 7. ACO-E parameter values chosenrepresented reasonable values perform well instances. experiment run100 times network.Condition 3 Condition 3 designed number objectives mind. were:examine effect different sample sizes ACO-E output;use separate test sample scoring networks output ACO-E;examine complexity ACO-E noting number statistics computed run.order achieve objectives, new experiments run. experiments, parametersset examining output experiments Condition 1 outputs seenSection 7.1. optimum value parameters chosen finding best combinationCondition 1 (note Table 10 shows average BDeu score parameter setting).experiments performed across six standard networks, five different sample sizes 100,500, 1000, 5000 10000. various parameters set Table 8.combination network sample size run 100 times. various parametersset = 20 tmax = 200. BDeu scoring criterion used, empty structureprior, equivalent sample size N 0 4 value = 0.05. meaning BDeu parametersdescribed above.Condition 4 Tuned Metaheuristics order able compare ACO-E metaheuristics described Condition 1, experiments run tuned parameters. experimentsperformed across six standard networks, sample size 10000. ACO-B,various parameters set Table 9. combination parameters gave best BDeuscore ACO-B Condition 1. GREEDY-E EPQ meaningful parameters tune.experiment run 100 times. ACO-B, various parameters set = 20tmax = 200. Similar Condition 3, BDeu scoring criterion used, empty structure419fiDALY & HENq0AlarmBarleyDiabetesHailFinderMildewWin95pts0.10.852.00.50.72.00.50.82.00.40.92.00.40.72.50.10.952.5Table 9: Tuned parameters ACO-Bprior, equivalent sample size N 0 4 value = 0.05. runs, limit 7 parentsallowed node, opposed 5 Condition 1.Condition 5 Examining Applicability ACO-E Experimental Condition 5 designedtest applicability ACO-E given data sets. achieve this, simple procedure designedindicate level ACO-E algorithm would perform better simple greedy search.procedure based GREEDY-E algorithm mentioned Condition 1. procedurefollows.original data set sampled replacement GREEDY-E algorithm run.purposes experiments, original data set sampled Bayesian network.algorithm terminates, number v-structures returned structure counted dividednumber variables data set. statistic noted procedure starts again,new set resampled data. whole procedure repeated confident predictionnormalized v-structure mean made. mean value obtained used measurecomplexity search space. higher value indicates v-structures hencecomplicated space.purposes paper, BDeu scoring function equivalent sample size N 0 4equal structure priors used. Test performed across six standard networkssample sizes 100, 500, 1000, 5000 10000. 100 resamplings used case.6.2.2 E VALUATION C RITERIArunning experiments, various scoring metrics picked ascertain well certainalgorithms behaved. were: scoring function used running experiments, test scoringfunction based different sample, structural Hamming distance (SHD), numberscoring function evaluations number distinct scoring function evaluations.explained below.Scoring Function experiments, BDeu scoring function used differingparameters, depending experimental condition. parameters uniform givencondition, score value Bayesian network structure could used compare resultsdifferent algorithms. terms BDeu score, means higher average scoreachieved, better results.Test Scoring Function well scoring function used running algorithm,separate BDeu scoring function defined, using independent, same-size samplenetwork used.420fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONStructural Hamming Distance order provide objective measure network structurereconstruction behavior compare results work Tsamardinos, Brown et al.(2006), value structural Hamming distance (SHD) metric given. measuresdifference learned network gold-standard generating network. networkstransformed DAG CPDAG (if already representation) penalties givennumber missing extra edges incorrectly directed arcs.Score Function Evaluations order estimate complexity running ACO-E algorithm,two statistics measured. first statistic number times scoring functionevaluated particular point time.Distinct Score Function Evaluations next statistic number times distinct scoringfunction evaluation occurred, i.e. number times arguments scoring functiondifferent. statistic often wildly different total number scoring function evaluationsoften better measure complexity, caching evaluations standard technique speedalgorithm runs.7. Experimental Resultssection results experiments performed according methodologies given Section6.2 presented. 6.2, five experimental conditions given. first dealt analyzingbehavior ACO-E respect parameters comparison metaheuristicalgorithms shared similar behavior. second condition dealt comparing ACO-Estate-of-the-art Bayesian network structure learning algorithms. third condition focusedeffect sample sizes output quality, behavior scoring function defined separatetest set computational complexity algorithm. fourth looked behaviormetaheuristic algorithms tuned behavior. Finally fifth condition dealt situationsACO-E used. results presented order, followed discussioninterpretation results.7.1 Condition 1results runs using experimental condition 1 shown two sets, reflectanalyzed later. Firstly, detailed results ACO-E shown Tables 10 11.tables, figures given results parameters; e.g. figure = 0.1 givencalculating mean standard deviation results = 0.1. case, sizesamples 216, calculated combinations parameters.noted specific values = 0 = 0 special cases. = 0,pheromone evaporation pheromone deposition graph; i.e. pheromone plays partalgorithm. = 0, heuristic used whilst ants traverse construction graph.comparative results involving ACO-E, ACO-B, GREEDY-E EPQ shown Table12 Figures 9 10. show behavior ACO-E algorithms,function algorithm iteration final value. results, iterations figureACO-E ACO-B. EPQ iteration number three times shown iteration. such,whilst ACO-E ACO-B run 200 iterations, EPQ run 600 results scaled200. done, concept iteration one framework translate wellterms time another framework.421fiDALY & HEN7.2 Condition 2results experiments conducted experimental Condition 2 illustrated here. secondset comparisons involved ACO-E state-of-the-art Bayesian network structure learningalgorithms.results comparison shown Table 13. acronyms specified givenTsamardinos, Brown et al. (2006) discussed Section 6.2.1. resultssupplied Tsamardinos, Brown et al. missing marked N/A Table 13. resultrange others, represented number stating median.7.3 Condition 3results experiments conducted according Condition 3 shown here. setexperiments designed show effects sample size ACO-E output also providemeasure computational complexity algorithm.SHD results runs 200 iterations seen Table 14, whilst score results200 iterations Table 15. Table 16 shows score results different test sample.remaining results experiments shown Figures 11 12. showtotal number score evaluations distinct number score evaluations respectively, runsACO-E algorithm.7.4 Condition 4Experimental Condition 4 used compare ACO-E metaheuristic algorithms usedCondition 1 parameters tuned best combinations Condition 1.algorithms ACO-B, EPQ GREEDY-E. results consolidated Table 17show results runs finished.7.5 Condition 5results experiments Condition 5 shown Table 18. experiments,multiple searches performed using GREEDY-E algorithm, data resampledexperiment. Experiments performed 100 times across combination test networkssample sizes.422fiAlarmBarley (105 )Diabetes (105 )HailFinder (105 )Mildew (105 )Win95pts (104 )0.00.10.20.30.40.51.0383 0.00375.0756 0.01361.9394 0.00324.9207 0.00394.5426 0.00969.4322 0.04481.0385 0.00365.0697 0.00391.9391 0.00344.9206 0.00384.5412 0.00919.4169 0.04331.0387 0.00355.0702 0.00961.9394 0.00294.9204 0.00424.5417 0.01019.4086 0.04521.0385 0.00375.0696 0.00391.9386 0.00344.9210 0.00344.5401 0.00949.4125 0.04541.0388 0.00375.0699 0.00411.9395 0.00344.9202 0.00404.5388 0.00839.4154 0.04571.0380 0.00385.0699 0.00411.9393 0.00354.9207 0.00374.5395 0.00909.4210 0.0468q0(105 )423AlarmBarley (105 )Diabetes (105 )HailFinder (105 )Mildew (105 )Win95pts (104 )0.70.750.80.850.90.951.0385 0.00355.0703 0.00661.9391 0.00354.9207 0.00394.5362 0.00699.4200 0.04531.0388 0.00355.0704 0.00651.9391 0.00334.9208 0.00354.5378 0.00749.4183 0.04411.0383 0.00355.0705 0.00601.9393 0.00324.9208 0.00384.5389 0.00849.4199 0.04621.0382 0.00355.0708 0.00571.9393 0.00354.9205 0.00384.5410 0.00989.4192 0.04811.0386 0.00395.0710 0.00731.9393 0.00354.9204 0.00404.5430 0.00979.4160 0.04681.0383 0.00405.0720 0.01261.9390 0.00304.9204 0.00404.5472 0.00929.4130 0.04390.00.51.01.52.02.51.0387 0.00365.0775 0.01211.9389 0.00334.9212 0.00394.53780.00759.4515 0.05051.0378 0.00385.0694 0.00431.9390 0.00344.9205 0.00374.5371 0.00759.4092 0.04041.0383 0.00365.0689 0.00351.9394 0.00344.9203 0.00404.5392 0.00909.4135 0.04201.0386 0.00355.0696 0.00401.9393 0.00324.9206 0.00384.5404 0.00949.4101 0.03851.0387 0.00405.0697 0.00551.9394 0.00324.9205 0.00384.5440 0.01029.4116 0.04441.0387 0.00345.0698 0.00971.9391 0.00354.9205 0.00404.5456 0.00909.4106 0.0427(105 )AlarmBarley (105 )Diabetes (105 )HailFinder (105 )Mildew (105 )Win95pts (104 )(105 )AlarmBarley (105 )Diabetes (105 )HailFinder (105 )Mildew (105 )Win95pts (104 )57101215201.0383 0.00365.0721 0.01201.9392 0.00314.9202 0.00414.5440 0.00979.4201 0.04521.0386 0.00365.0707 0.000611.9392 0.00364.9209 0.00374.5427 0.00979.4190 0.04521.0381 0.00375.0709 0.00831.9391 0.00324.9201 0.00394.5409 0.00939.4178 0.04571.0389 0.00375.0704 0.00661.9392 0.00324.9210 0.00404.5392 0.00929.4188 0.04581.0384 0.00395.0704 0.00611.9391 0.00334.9204 0.00364.5386 0.00829.4164 0.04621.0384 0.00365.07050.00601.9394 0.00344.9210 0.00374.5385 0.00859.4145 0.0467Table 10: Mean standard deviation BDeu score ACO-E parameter settingL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION(105 )fiDALY & HEN0.00.10.20.30.40.5Alarm6.9 4.95.6 3.16.0 3.25.6 3.35.9 3.05.5 3.0Barley56.4 10.852.8 3.953.0 4.153.2 4.452.6 4.052.9 4.8Diabetes63.5 5.865.5 5.365.0 5.465.1 5.664.2 5.463.7 4.8HailFinder50.6 6.950.5 6.851.0 7.851.8 7.851.8 6.751.1 7.9Mildew25.7 5.822.6 5.022.8 5.122.0 4.921.4 4.721.9 4.8Win95pts94.6 27.783.3 24.381.7 20.381.9 22.981.9 24.283.5 21.90.850.90.95q00.70.750.8Alarm5.2 2.55.6 3.15.4 3.06.1 3.76.4 4.26.7 4.1Barley53.9 5.953.6 5.853.35.453.4 5.953.1 5.853.6 7.268.1 4.7Diabetes62.1 4.862.6 4.963.4 4.964.7 5.466.1 5.3HailFinder52.1 7.351.9 7.551.5 8.250.7 7.150.6 7.750.0 5.9Mildew20.2 3.821.1 4.721.5 4.822.6 5.124.3 5.426.6 4.9Win95pts90.3 23.588.1 24.584.9 22.683.2 22.581.0 22.279.3 27.11.52.02.50.00.51.0Alarm7.4 5.14.3 1.14.9 2.45.4 2.86.2 3.67.2 3.7Barley61.4 9.052.0 3.951.93.051.7 3.351.9 3.352.0 3.7Diabetes64.3 4.964.3 5.564.2 5.564.7 5.164.6 5.964.8 5.5HailFinder52.2 7.052.0 6.551.5 6.750.1 8.050.3 8.050.5 7.5Mildew20.9 4.920.9 4.021.9 5.022.4 5.124.6 5.425.7 5.2Win95pts109.1 28.689.6 23.979.8 17.977.0 17.277.1 19.374.4 15.51215205710Alarm7.1 4.36.6 3.85.9 3.55.2 2.85.7 3.65.1 2.4Barley54.3 7.352.8 5.353.8 6.653.2 5.753.7 5.853.15.1Diabetes66.2 5.165.8 5.964.1 5.464.5 5.663.5 5.462.8 4.7HailFinder50.5 7.250.5 6.751.0 6.551.8 7.051.4 8.551.6 7.9Mildew24.6 5.323.9 5.622.7 5.522.2 4.921.6 4.721.4 4.7Win95pts83.3 23.383.0 21.985.6 26.285.5 22.185.5 25.084.0 25.5Table 11: Mean standard deviation SHD ACO-E parameter setting424fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONACO-EAlarmBarleyHailFinderMildewWin95ptsACO-BEPQSHD5.9 3.521.9 9.011.9 11.926.1 13.4Score (105 )1.0385 0.00371.0389 0.00391.0388 0.00381.0415 0.0045SHD53.5 6.0104.8 9.767.3 21.6101.4 14.45.0708 0.00785.2449 0.01245.0944 0.04235.2354 0.062864.5 5.469.2 3.070.7 8.577.2 7.11.9392 0.00331.9394 0.00331.9406 0.00411.9457 0.0048ScoreDiabetesGREEDY-E(105 )SHDScore(105 )SHD51.1 7.349.1 0.874.1 19.782.8 18.2Score (105 )4.9206 0.00384.9213 0.00364.9248 0.00584.9481 0.0177SHD22.7 5.329.3 0.736.1 14.050.3 13.8Score (105 )4.5407 0.00934.5531 0.00394.5548 0.01704.6148 0.0369SHD84.5 24.1104.9 15.5178.9 58.8220.1 31.6Score (104 )9.4178 0.04579.4649 0.04669.4589 0.07179.9181 0.0970Table 12: Mean standard deviation metaheuristic algorithms Condition 1 resultsACO-EMMHCOR1 k = 5OR1 k = 10OR1 k = 20OR2 k = 5OR2 k = 10OR2 k = 20SC k = 5SC k = 10GSPCTPDAGESAlarmBarleyHailFinderMildew16.4 4.79.6 7.027.8 10.031.2 11.137.8 9.421.2 4.633.2 5.439.4 6.534.2 3.620.4 11.858.8 6.515.2 1.59.6 1.5N/A80.9 5.3102.6 9.2109.6 9.5113.6 15.6136.4 2.9120.0 4.5109.2 16.2116.8 18.4129.6 13.1N/A143.3 7.3610.0 10.6207.2 4.0159.0 0.055.0 5.3208.0 1.6190.8 14.1183.2 14.9184.6 17.2184.6 14.5187.0 15.7200.8 9.2194.2 2.5N/A204.2 9.9385.6 12.5255.4 3.4154.6 54.331.0 3.658.4 7.470.6 4.275.6 6.375.0 4.869.2 3.364.0 4.467.4 3.4N/AN/A62.2 12.2421.2 10.797.8 6.838.8 0.8Table 13: SHD mean standard deviation state-of-the-art algorithms425fiDALY & HEN5x 105.055x 101.0385.11.045.151.0425.21.0441.046ScoreScore5.251.0485.35.351.055.41.0525.45ACOEACOBEPQGREEDYE1.0541.056020406080100120Iterations140160180ACOEACOBEPQGREEDYE5.55.550200204060(a) Alarm80100120Iterations140160180200(b) Barley5x 104.925x 101.9384.931.944.941.9424.95ScoreScore1.9441.9464.964.974.981.9484.99ACOEACOBEPQGREEDYE1.951.952020406080100120Iterations140160180ACOEACOBEPQGREEDYE55.0102002040(c) Diabetes5100120Iterations1401601802005x 100.944.550.964.60.98ScoreScore80(d) HailFinderx 104.54.654.711.02ACOEACOBEPQGREEDYE4.754.806020406080100120Iterations140160180ACOEACOBEPQGREEDYE1.041.060200(e) Mildew20406080100120Iterations(f) Win95ptsFigure 9: Scores metaheuristic algorithm comparison426140160180200fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION70140ACOEACOBEPQGREEDYE60ACOEACOBEPQGREEDYE130Structural Hamming DistanceStructural Hamming Distance1205040302011010090807010600020406080100120Iterations140160180500200204060(a) Alarm80100120Iterations140160180200(b) Barley82110ACOEACOBEPQGREEDYE80ACOEACOBEPQGREEDYE100Structural Hamming DistanceStructural Hamming Distance78767472709080706068506664020406080100120Iterations1401601804002002040(c) Diabetes80100120Iterations140160180200(d) HailFinder80300ACOEACOBEPQGREEDYEACOEACOBEPQGREEDYE280260Structural Hamming Distance70Structural Hamming Distance606050402402202001801601401203010020020406080100120Iterations140160180800200(e) Mildew20406080100120Iterations(f) Win95ptsFigure 10: SHD metaheuristic algorithm comparison427140160180200fiNetworkSample Size1005001000500010000AlarmBarleyDiabetesHailFinderMildewWin95pts49.32 8.3723.30 5.3217.73 4.586.45 2.714.33 1.74145.62 2.62132.25 5.32106.05 3.8566.30 5.1551.49 2.8278.24 5.2870.95 5.9268.13 7.6567.15 4.3061.01 3.1898.54 7.2879.99 11.6069.61 8.0058.50 5.9952.64 6.8584.49 1.3474.84 2.4655.53 2.7836.68 5.2118.96 0.79164.34 17.4991.45 21.7577.10 15.7756.29 14.5350.84 11.19Table 14: Structural Hamming distance different sample sizesBarley 1050.0138 0.00050.0568 0.00100.1092 0.00150.5228 0.00291.0388 0.00370.0774 0.00060.3263 0.00410.5833 0.00282.6028 0.00245.0695 0.0035NetworkDiabetes 105HailFinder 105Mildew 105Win95pts 1040.0316 0.00090.1135 0.00280.2102 0.00130.9810 0.00271.9399 0.00320.0667 0.00060.2946 0.00190.5576 0.00222.4178 0.00374.5338 0.00430.1507 0.00060.5446 0.00121.0198 0.00164.7468 0.00359.3794 0.0043NetworkDiabetes 105HailFinder 105Mildew 105Win95pts 1040.0316 0.00070.1136 0.00280.2104 0.00120.9811 0.00251.9394 0.00370.0668 0.00060.2947 0.00150.5581 0.00212.4181 0.00384.5350 0.00450.1669 0.00070.5564 0.00111.0238 0.00144.7535 0.00319.3883 0.00400.0596 0.00030.2687 0.00120.5189 0.00232.4807 0.00274.9205 0.0037Table 15: Training score different sample sizesSample Size1005001000500010000Alarm 105Barley 1050.0143 0.00050.0571 0.00090.1092 0.00140.5229 0.00271.0387 0.00390.0778 0.00070.3272 0.00420.5835 0.00262.6031 0.00305.0702 0.00300.0604 0.00040.2690 0.00120.5192 0.00192.4811 0.00304.9214 0.0036Table 16: Test score different sample sizesDALY & HEN428Sample Size1005001000500010000Alarm 105fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATION848x 105100 samples500 samples1000 samples5000 samples10000 samples3.5100 samples500 samples1000 samples5000 samples10000 samples4.54Score Function EvaluationsScore Function Evaluations3x 102.521.53.532.521.5110.500.5020406080100120Iterations14016018002000204060(a) Alarm1401601802001401601802001401601802008x 106100 samples500 samples1000 samples5000 samples10000 samples14x 10100 samples500 samples1000 samples5000 samples10000 samples5Score Function Evaluations12Score Function Evaluations100120Iterations(b) Barley7168010864324120020406080100120Iterations14016018002000204060(c) Diabetes9x 102.5100 samples500 samples1000 samples5000 samples10000 samplesx 10100 samples500 samples1000 samples5000 samples10000 samples2Score Function EvaluationsScore Function Evaluations21.510.50100120Iterations(d) HailFinder82.5801.510.5020406080100120Iterations1401601800200(e) Mildew020406080100120Iterations(f) Win95ptsFigure 11: Score function evaluations different sample sizes429fiDALY & HEN44x 108100 samples500 samples1000 samples5000 samples10000 samplesDistinct Score Function Evaluations10x 10100 samples500 samples1000 samples5000 samples10000 samples7Distinct Score Function Evaluations1286465432210020406080100120Iterations14016018002000204060(a) Alarm10100 samples500 samples1000 samples5000 samples10000 samples2.521.512001401601802001401601802007654320.51020406080100120Iterations14016018002000204060(c) Diabetes80100120Iterations(d) HailFinder45x 103.5100 samples500 samples1000 samples5000 samples10000 samplesx 10100 samples500 samples1000 samples5000 samples10000 samples3Distinct Score Function Evaluations2.5Distinct Score Function Evaluations180100 samples500 samples1000 samples5000 samples10000 samples8321.510.50160x 109Distinct Score Function EvaluationsDistinct Score Function Evaluations3.531404x 1040100120Iterations(b) Barley44.5802.521.510.5020406080100120Iterations1401601800200(e) Mildew020406080100120Iterations(f) Win95ptsFigure 12: Distinct score function evaluations different sample sizes430fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONSHDScoreAlarmBarleyDiabetesTest Score (105 )MildewWin95ptsGREEDY-EACO-BEPQ4.33 1.7424.17 9.165.98 4.6316.09 9.921.0388 0.00371.0396 0.00381.0388 0.00401.0389 0.00371.0387 0.00391.0395 0.00441.0391 0.00381.0396 0.0037Score Eval.3.7e8 2.7e76.8e4 7.3e31.7e7 1.9e53.2e7 1.3e6Dist. Score Eval.1.2e5 5.6e32.9e3 2.1e27.2e4 2.3e32.7e4 2.3e3SHD51.49 2.82106.58 8.9552.95 3.7191.18 17.36Score (105 )5.0695 0.00355.2415 0.01145.0698 0.00335.1677 0.0740Test Score (105 )5.0702 0.00305.2413 0.01165.0702 0.00345.1673 0.0734Score Eval.4.7e8 1.5e71.3e5 5.7e33.1e7 1.5e54.5e7 2.1e6Dist. Score Eval.6.2e4 1.3e34.1e3 1.3e25.8e4 1.0e34.3e4 2.7e3SHD61.01 3.1868.71 3.1366.97 4.8877.13 7.10Score (105 )1.9399 0.00321.9397 0.00331.9392 0.00391.9451 0.0047Test Score (105 )1.9394 0.00371.9395 0.00301.9398 0.00321.9449 0.0044Score Eval.1.4e8 7.5e62.7e4 1.6e31.6e7 1.0e52.7e7 2.7e6Dist. Score Eval.4.1e4 1.8e32.2e3 3.1e13.3e4 1.0e31.6e4 1.6e3SHD52.64 6.8549.20 0.8961.59 11.6378.81 16.324.9205 0.00374.9212 0.00394.9213 0.00364.9293 0.00734.9214 0.00364.9209 0.00384.9214 0.00384.9296 0.0080ScoreHailFinder(105 )ACO-E(105 )Test Score (105 )Score Eval.5.3e8 3.3e71.0e5 2.7e34.0e7 2.9e56.1e7 3.7e6Dist. Score Eval.8.9e4 3.2e35.4e3 8.7e16.9e4 2.2e35.5e4 3.9e3SHD18.96 0.7929.22 0.7719.41 3.8343.59 11.79Score (105 )4.5338 0.00434.5527 0.00384.5348 0.00584.5982 0.0292Test Score (105 )4.5350 0.00454.5526 0.00444.5350 0.00524.5989 0.0299Score Eval.2.1e8 9.8e64.2e4 1.1e31.6e7 1.6e52.8e7 2.4e6Dist. Score Eval.2.2e4 5.4e22.2e3 3.7e11.5e4 2.6e21.5e4 1.2e3SHD50.84 11.1985.75 16.4491.08 18.52231.25 42.46Score (104 )9.3794 0.00439.4121 0.00439.3890 0.00369.6061 0.0075Test Score (104 )9.3883 0.00409.4153 0.00459.3897 0.00429.6058 0.0080Score Eval.2.2e9 2.4e85.2e5 8.9e48.5e7 8.6e51.2e8 4.6e6Dist. Score Eval.3.2e5 1.9e41.5e4 7.9e22.7e5 1.2e41.9e5 5.4e3Table 17: Mean standard deviation tuned metaheuristic algorithmsSample Size1005001000500010000AlarmBarleyDiabetesHailFinderMildewWin95pts0.250.400.460.570.580.010.120.320.720.930.060.320.340.360.300.060.180.280.440.450.000.020.120.710.820.531.301.531.962.11Table 18: Mean number v-structures divided number nodes greedy searches431fiDALY & HEN8. Discussionsection discuss results presented previous section. general, discussioninvolve looking score SHD values (as defined Section 6.2.2) obtainedalgorithms. noted better score necessarily mean better SHD valuevice-versa. occur small sample sizes parameters givenscoring function (such equivalent sample size value), shownproduce differences scoring function behavior (Kayaalp & Cooper, 2002). general, differentdata sets different parameter values behave optimally. seemgeneral method find optimum values. problem looked depthSilander, Kontkanen et al. (2007).first figures examined Tables 10 11 Condition 1.presented results experiments varied parameter values ACO-E algorithm.Looking figures, evidence ACO-E algorithm provides useful behaviorreasonable values parameters.Next, results experimental Condition 5 examined, particularly Table 18context Bayesian network properties given Table 5. Along results showbehavior ACO-E function sample size Condition 3 (Tables 14, 15 16), discussionseek characterize ACO-E performance perspective generating networksample. Evidence presented shows ACO-E performs better complicatednetworks, i.e. networks v-structures.previous discussion focuses behavior ACO-E function various parameters.next results looked intended provide comparison Bayesiannetwork structure learning algorithms. include Figures 9 10 Table 12 Condition1 Table 17 Condition 4. present ACO-E metaheuristic algorithmssimilar. results strong evidence ACO-E performing wellalgorithms.Also comparative perspective, results given Table 13 discussed.present series tests comparing ACO-E state-of-the-art Bayesian network structurelearning algorithms. Again, looking figures, strong evidence ACO-E competitiveperformance.Finally, complexity results Condition 3 shown form Figures 11 12.order perform comparison, statistical tests needed. non-normalitydistributions results, tests others ones rely normalitydata used. mentioned below.8.1 ACO-E Behaviorsection behavior ACO-E parameters varied analyzed. shownTables 10 11 evidence difference behavior ACO-E algorithmdepending input parameters. differences analyzed using two-tailed MannWhitney U test Students test. particular test used depends normality data,tested Jarque-Bera test.order perform comparison, best figures Tables 10 11 comparedsituation particular part ACO-E algorithm turned off. E.g. Table 10Alarm row, best figure = 0.5. compared value = 0.0,432fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONvalue pheromone deposition evaporation occurring. values various partsACO-E algorithm turned = 0.0, q0 = 1 = 0.0. valueq0 = 1, algorithm behaves purely greedy fashion. Therefore purposes testing,value GREEDY-E algorithm Table 12 used comparison, results wouldexactly case q0 = 1. results comparisons shown Table 19.table shows p-values comparison.8.1.1 B EHAVIORLooking Table 19 results seem certain Barley, MildewWin95pts. Looking Tables 10 11 networks, values 0.2 0.4 range.Also looking features networks Table 5 correspondence = 0.276 nodes (Win95pts), = 0.3 48 nodes (Barley) = 0.4 35 nodes (Mildew). Whilstconclusive, suggests behaves well region 0.2 0.4 (for data sets worksall). fact much variance range networks means rangequite robust. also suggestion datasets nodes would use smaller values .makes sense, larger networks would probably need spend time following bestsolutions, low value would provide.8.1.2 B EHAVIOR q0parameter q0 appears effect networks, possible exceptionHailFinder. networks (Alarm Barley) parameter large effectwide range, whereas others (Diabetes, Mildew Win95pts), effect depends large extentvalue q0 . largest effects scoring function point view appearBarley, Mildew Win95pts networks.Looking networks, large variations behavior across different values q0 makedifficult predict best value parameter might particular data set. One rulethumb might smaller values q0 create exploration might useful smallerdata sets, whereas larger data sets need exploitation order get reasonable answer.8.1.3 B EHAVIORTable 19, networks parameter plays role appear Alarm,Barley Win95pts. differences best values scoring functionSHD difficult predict best value . case Barley, behavior quite robustvalues range 0.5 2.5. However, Alarm Win95pts, behavior dependsvalue parameter smaller value better Alarm larger value Win95pts.rule thumb appears networks less numbers nodes need smaller valueshelp avoid local minima, whereas networks nodes need larger values order focussearch effectively.8.1.4 B EHAVIORLooking Tables 10 11 seen value sometimes small effecteffectiveness ACO-E. case, effect pronounced Alarm, DiabetesMildew networks, higher values giving smaller SHD. Indeed cases, higher values433fiDALY & HENnever produce statistically worse results, expected. However, important bearmind increased running times larger values m.8.1.5 G ENERAL ISCUSSIONreason strange behavior HailFinder results possibly explained examininggraphs score function SHD time (Figures 9 10). seen scoreimproving iterations, SHD value deteriorating. might lead one conclusionproblem scoring function HailFinder case, perhaps parameters.Another plausible reason HailFinder Win95pts results sync otherslarger networks, might favour aggressive exploitation best-so-farsolution smaller ones. case, would correspond lower values highervalues q0 . Also heuristic information might useful large numbers variables, leadingbetter results large values . Note problems HailFinder networkalso seen de Campos Castellano (2007).8.2 Behavior ACO-E Respect Test Network Sampleprevious section, seen ACO-E useful algorithm learning structureBayesian networks. also seen values parameters produced bestbehavior depended network tested. rules thumb consolidatecharacteristics observed previous section were:data variables, lower values , higher values q0 higher values.data less variables, higher values , lower values q0 lower values .However, also seen ACO-E always successful learning.little difference seen certain parameters turned certain networks. LookingTable 19, seems networks effect felt Barley,Mildew Win95pts networks. this?Looking Table 5 seem discernible pattern networkproperties suitability algorithm. However, values number v-structuresnormalised number nodes graph show definite reason. networksACO-E performed well larger number nodes higher in-degreehence larger number v-structures. result this, data sampled networks bettergoing match similar network scoring function, i.e. one similar standardnetwork. search starts empty graph, likely search would gettrapped local minimum trying add enough arcs get needed number. Due ACO-Estochastic algorithm, able avoid local minima.upshot ACO-E would good candidate algorithm data samplednetworks large number v-structures. However, real world generating networkexist. Therefore, experiments Condition 5 designed try estimate quantity.results experiments shown Table 18. seen associationresults number samples 10000 average number v-structuresper node. Indeed, value correlation coefficient values r = 0.94,434fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONindicates linear relationship p-value 0.006. However, results quitenumber samples decrease. example, 100 samples correlation visible.makes sense, low number samples would able support many v-structures.such, estimate might valid large sample limit.However, procedure employed Condition 5 indicate way seeing effectiveACO-E algorithm would arbitrary set data. value calculated resamplingmethod low (towards 0), ACO-E would probably particularly effective simpleralgorithm would perform well. However, value rises, probable number v-structures alsorises hence ACO-E (and methods designed avoid local maxima) would fare better.ideas seem borne examining Table 14. appears high expected valuesv-structures per node imply good performance ACO-E algorithm, i.e. obvious largeimprovement SHD. contrary, low expected values v-structures per node associatedsmall improvements SHD algorithm progresses.8.3 Metaheuristic Algorithm ComparisonFigures 9 10 Table 12 show results comparing ACO-E metaheuristicalgorithms. seen ACO-E performs better algorithms shown exceptcase HailFinder network, GREEDY-E gives better result SHD. However,case, ACO-E gives better score value. problem discussed Section 8.1,gave better score worse structure.statements backed looking Table 20 gives p-values two-tailedunpaired Mann-Whitney-U test comparing ACO-E results algorithms runsended. statistic, smaller number, significant test. Since resultsruns comes separate sample network, correct tests would unpaired.data used tests metaheuristic algorithm comparison, i.e.combinations parameters ACO-E ACO-B. seems cases, resultshighly significant, supports assumption ACO-E performs well. casessignificance high (ACO-E score compared GREEDY-E score Alarm networkACO-E score compared GREEDY-E score Diabetes network), noted tinychanges score value lead large structural changes algorithm converges towardsoptimum (generating) network. cases, SHD p-values show highly significant difference.Comparisons also made variances results seen Table 21,gives p-values Conovers (1999) Squared Ranks one-tailed test. table seenACO-E generally lower standard deviation results finishing run comparedACO-B EPQ. Whilst standard deviation results compared GREEDY-E significantlylower respect Alarm Barley networks, cases GREEDY-E seemsconsistent regard final results.noted non-parametric tests used, results general non-normaldistributions. also noted results tables might seem incorrect.E.g. Table 20, Win95pts-Score row, test ACO-B significantGREEDY-E, even though mean GREEDY-E ACO-E ACO-BTable 12. larger sample size ACO-B test, 1296 samples,compared 216 samples GREEDY-E.435fiDALY & HENq0AlarmSHDScore8.0 1041.7 1014.3 10912.1 1021.3 10169.1 103BarleySHDScore1.3 1061.3 1094.2 102302.9 10726.9 10412.5 1026DiabetesSHDScore1.0 1009.2 1033.6 10422.8 1018.7 1011.0 100HailFinderSHDScore9.3 1011.9 1013.3 1021.1 1025.5 1032.7 102MildewSHDScore5.1 10161.4 1053.6 101258.0 10631.0 1003.5 101Win95ptsSHDScore4.9 1081.3 1079.0 10412.0 10265.3 10442.2 1018Table 19: Comparisons parameter behaviorGREEDY-EACO-BEPQAlarmSHDScore1.1 101134.3 1021.6 10316.0 1031.9 101181.8 1018BarleySHDScore9.6 101245.0 101231.8 10925.9 10969.3 101237.0 10123DiabetesSHDScore3.1 10342.5 1014.7 101042.3 10181.5 10884.5 1069HailFinderSHDScore3.8 10124.4 1032.7 102372.2 10931.8 10994.6 10113MildewSHDScore2.3 10507.4 10624.7 101605.7 101147.5 101155.3 10118Win95ptsSHDScore1.5 10423.0 103704.1 10534.8 101235.0 10123Table 20: p-values Mann-Whitney U test, 10,000 samples436fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONresults Condition 4 confirm findings. experiments, algorithmsrun parameters tuned. results experiments seen Table 17.findings results similar ones discussed above, differences. ACO-Eoutperforms algorithms SHD test score measures cases exceptHailFinder network, GREEDY-E better, above. differences ACO-EACO-B, main competitor, pronounced, still exist. Alarm, BarleyMildew networks practical difference quite small, whereas Diabetes, HailFinderWin95pts networks still quite large. However, even this, ACO-E said performbetter three areas:ACO-E robust respect parameter values input. Comparing tunedparameter-value results results across parameter values, seen ACO-Esensitive values ACO-B. implies ACO-E could used learningproblem without long parameter optimization stage. Note reasonable parameter valuesstill important, discussed Section 8.1.ACO-E converges faster optimum values ACO-B, terms number iterations.example, Barley, Mildew Win95pts cases, ACO-E reaches best SHD value20 iterations, whereas ACO-B takes 200 iterations.ACO-E generally provides smaller variance output values algorithms.important situations robust output needed.8.4 State-of-the-Art Algorithm Comparisonsection, comparison ACO-E state-of-the-art Bayesian network structurelearning algorithms analyzed. shown Table 13, ACO-E appears good performance algorithms. results statistical comparisons ACO-Ealgorithms shown Table 22.table shown p-values individual comparisons ACO-E algorithms.test used comparisons Mann-Whitney U test. test used,distributions found normal. foot table combined p-value foundindividual p-values it. total p-value comparing ACO-Ealgorithms. method combining valuesnpcombined = 1 1 pi ,i=1pi p-value entry table, n values total. method combiningp-values needed chance causing Type error otherwise. Type errorfalse positive result, i.e. null hypothesis rejected be. occurcase experiment small chance failing repeated enough times,large chance least one fail. noted value foot Alarmcombine p-values it. Instead leaves SC k = 10, PC OR2k = 5. median results figures close ACO-E wouldpushed p-value high. Therefore, overall test valid testsinclude three mentioned.437fiDALY & HENAlarmBarleyDiabetesHailFinderMildewWin95ptsSHDESHDScoreSHDESHDScoreSHDESHDScoreSHDESHDScoreSHDESHDScoreSHDESHDScoreGREEDY-E3.6 10844.1 101038.0 1021.3 10616.2 10441.5 1066114.8 101119.1 101111115.8 101ACO-B2.4 1025201.3 1011.5 102747.6 1027701.5 1031.2 10132.1 1042.0 101532.5 101694.3 10231.5 101111.3 10771.5 10855.7 102098.7 102102.6 1026EPQ5.3 101047.3 101175.0 1052.3 10661.6 10694.6 101461.0 1043.1 1057.3 1082.6 10622.0 10581.3 101439.3 10541.9 10498.3 10795.4 10121.4 10133.2 1038Table 21: p-values Conovers squared ranks test, 10,000 samplesAlarmBarleyHailFinderMildewMMHCOR1 k = 5OR1 k = 10OR1 k = 20OR2 k = 5OR2 k = 10OR2 k = 20SC k = 5SC k = 10GSPCTPDAGES3.2 1025.9 1041.9 1054.1 1083.5 1021.4 1072.1 1082.1 1088.1 1012.1 1084.0 1016.5 104N/A2.1 1082.1 1082.1 1082.1 1082.1 1081.1 1053.8 1062.1 108N/A4.3 1072.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 108N/A2.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 1082.1 108N/AN/A2.1 1082.1 1082.1 1082.1 106Total3.3 1021.6 1052.5 1072.3 106Table 22: p-values comparing ACO-E state-of-the-art algorithms438fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONresults given Table 13 appear indicative results given Section 8.1.discussed there, ACO-E effect learning Alarm network, especiallystraight greedy search. However, effectiveness appeared come randomnesssearch, make much use parameters.networks ACO-E performed well, Barley Mildew, performance reflectedacross current results also performed well here. results HailFinder networkseem odd, ACO-E algorithm better search using GREEDY-E. However,figure, performance GES also seen well. GES works spaceequivalence classes Bayesian networks, postulated ACO-E performs wellstructure search space.ever, comparisons must taken tentatively, especially case, results givenTsamardinos, Brown et al. five samples.8.5 Computational Complexity ACO-Eresults experimental Condition 3 show two figures (11 12) related computationalcomplexity ACO-E. first shows total number score function evaluationsalgorithm run, second shows number distinct score function evaluations.counted, score function evaluations usually cached order improve running times.surmized general, larger sample sizes imply evaluations. makes sense,larger samples support networks arcs. Since algorithm starts empty graph,would take moves hence evaluations get maximum. also seentotal number evaluations general, linear respect number iterations passed.Looking plots Figure 12, seen number distinct function evaluationsmany magnitudes less total number evaluations. also noteddistinct function evaluations take place within first twenty thirty iterations gradually tailslogarithmic fashion. expected, beginning, algorithm exploremany new paths pheromone evenly distributed. scores pathscached computed again. means time, less less new scorefunction applications needed. However, worthwhile noticing many cases plotslevel out. implies new paths taken algorithm stagnating.finish up, worthwhile comparing complexity ACO-E metaheuristic algorithms tested. Looking Table 17, appears ACO-E much higher computationalcomplexity algorithms. However, seen evaluationsdistinct. Since evaluations normally cached cache lookup proceed constant time,total score evaluation results important. Focusing instead distinct score evaluationresults, seen much difference ACO-E algorithmsterms actual score function evaluations. Since often dominant factor algorithmrunning time, complexity algorithms observed quite similar.9. Conclusions Future Directionsmain results paper development ACO-E algorithm implementationACO metaheuristic problem learning Bayesian network structure providesgood fit set data. nutshell, ACO-E performed well reconstructing test networks,439fiDALY & HENdata sampled. detailed look behavior ACO-E depending parameters,type test network compared algorithms given.9.1 ACO-E Behavior Parameters Variedanalyzing behavior ACO-E function parameters, best worst performingfigures compared, across range parameter. best result found parametersetting produced either highest score smallest difference test network. worstresult found parameter switched off, i.e. effect algorithmsbehavior.parameters, difference behavior best worst settings.Whether difference significant depended particular network usedtest; networks responded better algorithm others. networks ACO-Eworked well with, following trends noticed:data features, lower values , higher values q0 higher valuesworked better;data less features, higher values , lower values q0 lower values workedbetter;rate pheromone deposition/evaporation, q0 balance explorationexploitation power heuristic probabilistic transition rule.9.2 Utility ACO-E Function Test Network Samplenoticed ACO-E performed better test networks others. networksfared best Barley, Mildew Win95pts, described Section 6.1.1. closerexamination networks found large average v-structure (as discussedSection 6.1.1) per node value.reason might make difference nodes large number v-structuresimply possible local maxima search space. Greedy methods would run maxima,whereas ACO-E able find way around stochastic nature alwayschoosing best move. Experiments run estimate average v-structure per node valuecorrespondence found large sample case. general, method used experimentcould used estimate usefulness ACO-E particular situations.9.3 ACO-E Performance Compared Similar Algorithmsresults Sections 7.1 7.4 show ACO-E performs well algorithmssimilar nature. algorithms were:GREEDY-E, performs greedy search space equivalence classes Bayesiannetwork structures (Chickering, 2002a);EPQ, performs evolutionary programming search space equivalence classes(Cotta & Muruzbal, 2004; Muruzbal & Cotta, 2004);ACO-B, performs search using ACO space DAGs (de Campos, FernndezLuna et al., 2002).440fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONcases, BDeu score ACO-E better score algorithms, everyiteration. case structural differences, better cases, except HailFindernetwork, odd behavior scoring function meant better BDeu scores implied worsestructural differences. Concurring discussion 9.2, networks ACO-Eperformed best Barley, Mildew Win95pts networks.ACO-E also shown comparable computational complexity metaheuristicalgorithms.9.4 ACO-E Performance Compared Alternative State-of-the-Art AlgorithmsSimilar section above, ACO-E performed well comparison state-of-the-art Bayesiannetwork structure learning algorithms, performing better 3 4 tested: Barley, MildewHailFinder. first two networks performed well self test. HailFindernetwork postulated results good search space; good results alsoshown greedy equivalent search (GES) algorithm, also searches spaceequivalence classes.Whilst ACO-E perform best Alarm network, perform badly either,coming joint third rankings. reasons performance Alarm networkdiscussed Section 8.2.9.5 Extending ACO-E Increase Performance ScalabilitySince validity checking slowest part ACO-E algorithm, currently remains first issuemust dealt with, order improve running times. However, problem solvedfocus turn back parts algorithm, particularly scoring function.Reducing Number Scoring Function Evaluations One easy way cuttingnumber score evaluations would static heuristic defined could say, e.g.would benefit adding arc empty graph. way, scoring functions wouldevaluated per move hence lead speeding algorithm. situationlike this, local search would become important order finish traversals bestpossible positions.Pruning Search Space Recently, hybrid learning algorithms shown good successlearning Bayesian network structures, whilst cutting running time, sometimes dramatically.generally work using conditional independence test discover nodes would likelyconnected given node remove rest consideration. effectrequiring less scoring function evaluations, thus speeding algorithm requiring less memorystore results evaluations. bound number possible parents, numbercached values would grow least quadratically number variables eventually exhaustcomputers memory.Applying ACO-E Different Search Operators Better Avoid Local Maxima AccordingCastelo Kocka (2003), certain operators able avoid local maximasearch space, provided sample size tends infinity. example operatorsgiven Chickering (2002b) used greedy search space equivalence classesstructures (GES).441fiDALY & HENHowever, small sample sizes guarantees strictly true search algorithmsstill get caught maxima. example method tries avoid KES algorithmNielsen, Kocka, Pea (2003), uses operators GES, parameter controlsoften algorithm acts greedily; algorithm act greedily, chooses movenecessarily best. Experiments show KES behaves better GEStime.procedure bears similarities ACO-E. randomness augmented pheromoneheuristics, possibility performance would improve even more.Acknowledgmentsauthors grateful Associate Editor reviewers comments,helpful guiding revision research.ReferencesAbramson, B., Brown, J., Edwards, W., Murphy, A., & Winkler, R. L. (1996). Hailfinder:Bayesian system forecasting severe weather. International Journal Forecasting, 12(1),5771. doi:10.1016/0169-2070(95)00664-8.Acid, S., & de Campos, L. M. (2003). Searching Bayesian network structures spacerestricted acyclic partially directed graphs. Journal Artificial Intelligence Research, 18, 445490.Aitken, S., Jirapech-Umpai, T., & Daly, R. (2005). Inferring gene regulatory networks classifiedmicroarray data: Initial results. BMC Bioinformatics, 6(Suppl 3), S4. doi:10.1186/1471-2105-6S3-S4.Akaike, H. (1974). new look statistical model identification. IEEE Transactions AutomaticControl, 19(6), 716723. doi:10.1109/TAC.1974.1100705.Andersson, S. A., Madigan, D., & Perlman, M. D. (1997). characterization Markovequivalence classes acyclic digraphs.Annals Statistics, 25(2), 505541.doi:10.1214/aos/1031833662.Andreassen, S., Hovorka, R., Benn, J., Olesen, K. G., & Carson, E. (1991). model-based approachinsulin adjustment. Proceedings Third Conference Artificial Intelligence Medicine(AIME 91), volume 44 Lecture Notes Medical Informatics, (pages 239249).Baluja, S. (1994). Population-based incremental learning: method integrating genetic searchbased function optimization competitive learning. Technical Report CMU-CS-94-163, SchoolComputer Science, Carnegie Mellon University.Beinlich, I., Suermondt, H., Chavez, R., & Cooper, G. (1989). ALARM monitoring system:case study two probabilistic inference techniques belief networks. ProceedingsSecond European Conference Artificial Intelligence Medicine (AIME 89), volume 38Lecture Notes Medical Informatics, (pages 247256). Springer.Birattari, M., Caro, G. D., & Dorigo, M. (2002). Toward formal foundation ant programming.Proceedings Third International Workshop Ant Algorithms, volume 2463 Lecture442fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONNotes Computer Science, (pages 188201). Springer-Verlag.Bonabeau, E., Dorigo, M., & Theraulaz, G. (1999). Swarm Intelligence: Natural ArtificialSystems. Studies Sciences Complexity. Oxford University Press.Bullnheimer, B., Hartl, R. F., & Strauss, C. (1999). improved ant system algorithm vehiclerouting problem. Annals Operations Research, 89, 319328. doi:10.1023/A:1018940026670.Buntine, W. (1991). Theory refinement Bayesian networks. B. DAmbrosio, & P. Smets (Eds.),Proceedings Seventh Annual Conference Uncertainty Artificial Intelligence (UAI 91),(pages 5260). Morgan Kaufmann.Castelo, R., & Kocka, T. (2003). inclusion-driven learning Bayesian networks. JournalMachine Learning Research, 4, 527574.Cheng, J., Greiner, R., Kelly, J., Bell, D., & Liu, W. (2002). Learning Bayesian networks data:information-theory based approach. Artificial Intelligence, 137(12), 4390. doi:10.1016/S00043702(02)00191-1.Chickering, D. M. (1995). transformational characterization equivalent Bayesian network structures. P. Besnard, & S. Hanks (Eds.), Proceedings Eleventh Conference UncertaintyArtificial Intelligence (UAI-95), (pages 8798). Morgan Kaufmann.Chickering, D. M. (1996a). Learning Bayesian networks NP-complete. D. Fisher, & H.-J. Lenz(Eds.), Learning Data: Artificial Intelligence Statistics V, volume 112 Lecture NotesStatistics, (pages 121130). Springer.Chickering, D. M. (1996b). Learning equivalence classes Bayesian network structures.E. Horvitz, & F. Jensen (Eds.), Proceedings Twelfth Conference Uncertainty ArtificialIntelligence (UAI-96), (pages 150157). Morgan Kaufmann.Chickering, D. M. (2002a). Learning equivalence classes Bayesian-network structures. JournalMachine Learning Research, 2, 445 498.Chickering, D. M. (2002b). Optimal structure identification greedy search. Journal MachineLearning Research, 3, 507554.Chickering, D. M., Geiger, D., & Heckerman, D. (1996). Learning Bayesian networks: Searchmethods experimental results. D. Fisher, & H.-J. Lenz (Eds.), Learning Data: ArtificialIntelligence Statistics V, volume 112 Lecture Notes Statistics, (pages 112128). Springer.Chickering, D. M., Heckerman, D., & Meek, C. (2004). Large-sample learning Bayesian networksNP-hard. Journal Machine Learning Research, 5, 12871330.Conover, W. J. (1999). Practical Nonparametric Statistics. John Wiley & Sons, Third edition.Cooper, G. F., & Herskovits, E. (1992). Bayesian method induction probabilistic networksdata. Machine Learning, 9(4), 309347. doi:10.1007/BF00994110.Costa, D., & Hertz, A. (1997). Ants colour graphs. Journal Operational Research Society,48(3), 295305. doi:10.1057/palgrave.jors.2600357.443fiDALY & HENCotta, C., & Muruzbal, J. (2004). learning Bayesian network graph structures viaevolutionary programming. P. Lucas (Ed.), Proceedings Second European WorkshopProbabilistic Graphical Models, (pages 6572).Cowell, R. (2001). Conditions conditional independence scoring methods leadidentical selection Bayesian network models. J. Breese, & D. Koller (Eds.), ProceedingsSeventeenth Conference Uncertainty Artificial Intelligence (UAI-01), (pages 9197).Morgan Kaufmann.Dagum, P., & Luby, M. (1993). Approximating probabilistic inference Bayesian belief networksNP-hard. Artificial Intelligence, 60(1), 141154. doi:10.1016/0004-3702(93)90036-B.Daly, R., Shen, Q., & Aitken, S. (2006). Speeding learning equivalence classes Bayesiannetwork structures. A. P. del Pobil (Ed.), Proceedings Tenth IASTED InternationalConference Artificial Intelligence Soft Computing, (pages 3439). ACTA Press.Daly, R., Shen, Q., & Aitken, S. (2009). Learning Bayesian networks: Approaches issues.Knowledge Engineering Review. press.de Campos, L. M., & Castellano, J. G. (2007). Bayesian network learning algorithms using structural restrictions. International Journal Approximate Reasoning, 45(2), 233254.doi:10.1016/j.ijar.2006.06.009.de Campos, L. M., Fernndez-Luna, J. M., Gmez, J. A., & Puerta, J. M. (2002). Ant colonyoptimization learning Bayesian networks. International Journal Approximate Reasoning,31(3), 291311. doi:10.1016/S0888-613X(02)00091-9.de Campos, L. M., Gmez, J. A., & Puerta, J. M. (2002). Learning Bayesian networks ant colonyoptimisation: Searching two different spaces. Mathware & Soft Computing, 9(23).de Campos, L. M., & Puerta, J. M. (2001a). Stochastic local algorithms learning belief networks:Searching space orderings. S. Benferhat, & P. Besnard (Eds.), ProceedingsSixth European Conference Symbolic Quantitative Approaches ReasoningUncertainty (ECSQARU 2001), volume 2143 Lecture Notes Artificial Intelligence, (pages228239). Springer. doi:10.1007/3-540-44652-4_21.de Campos, L. M., & Puerta, J. M. (2001b). Stochastic local distributed search algorithmslearning belief networks. Proceedings Third International Symposium Adaptive Systems: Evolutionary Computation Probabilistic Graphical Models, (pages 109115).ICIMAF.Deneubourg, J. L., Aron, S., Goss, S., & Pasteels, J. M. (1990). self-organizing exploratorypattern argentine ant. Journal Insect Behavior, 3(2), 159168. doi:10.1007/BF01417909.Dor, D., & Tarsi, M. (1992). simple algorithm construct consistent extension partiallyoriented graph. Technical Report R-185, Cognitive Systems Laboratory, Department ComputerScience, UCLA.Dorigo, M. (1992). Ottimizzazione, apprendimento automatico, ed algoritmi basati su metaforanaturale. Ph.D. thesis, Politecnico di Milano, Italy.444fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONDorigo, M., & Di Caro, G. (1999). ant colony optimization meta-heuristic. D. Corne,M. Dorigo, & F. Glover (Eds.), New Ideas Optimization, (pages 1132). McGraw-Hill.Dorigo, M., & Gambardella, L. M. (1997). Ant colony system: cooperative learning approachtraveling salesman problem. IEEE Transactions Evolutionary Computation, 1(1), 5366.doi:10.1109/4235.585892.Dorigo, M., Maniezzo, V., & Colorni, A. (1996). Ant System: Optimization colonycooperating agents. IEEE Transactions Systems, Man, Cybernetics Part B: Cybernetics,26(1), 2941. doi:10.1109/3477.484436.Dorigo, M., & Sttzle, T. (2004). Ant Colony Optimization. MIT Press.Friedman, N., Nachman, I., & Peer, D. (1999). Learning Bayesian network structure massivedatasets: Sparse Candidate algorithm. H. Prade, & K. Laskey (Eds.), ProceedingsFifteenth Conference Uncertainty Artificial Intelligence (UAI-99), (pages 206215). MorganKaufmann.Gambardella, L. M., & Dorgio, M. (2000). ant colony system hybridized new localsearch sequential ordering problem. INFORMS Journal Computing, 12(3), 237255.doi:10.1287/ijoc.12.3.237.12636.Gambardella, L. M., & Dorigo, M. (1995). Ant-Q: reinforcement learning approach travellingsalesman problem. A. Prieditis, & S. J. Russell (Eds.), Proceedings Twelfth InternationalConference Machine Learning (ICML 1995), (pages 252260). Morgan Kaufmann.Glover, F. (1989). Tabu searchPart I. ORSA Journal Computing, 1(3), 190206.Glover, F. (1990). Tabu searchPart II. ORSA Journal Computing, 2(1), 432.Heckerman, D. (1995). tutorial learning Bayesian networks. Technical Report MSR-TR95-06, Microsoft Research.Heckerman, D., Geiger, D., & Chickering, D. M. (1995).Learning Bayesian networks:combination knowledge statistical data. Machine Learning, 20(3), 197243.doi:10.1023/A:1022623210503.Heckerman, D., Mamdani, A., & Wellman, M. P. (1995). Real-world applications Bayesiannetworks. Communications ACM, 38(3), 2426. doi:10.1145/203330.203334.Heckerman, D., Meek, C., & Cooper, G. (1999). Bayesian approach causal discovery.C. Glymour, & G. F. Cooper (Eds.), Computation, Causation, & Discovery, (pages 141165).AAAI Press.Jensen, A. L. (1995). probabilistic model based decision support system mildew managementwinter wheat. Ph.D. thesis, Dina Foulum, Research Center Foulum, Aalborg University.Kayaalp, M., & Cooper, G. F. (2002). Bayesian network scoring metric based globallyuniform parameter priors. A. Darwiche, & N. Friedman (Eds.), Proceedings Eighteenth Conference Uncertainty Artificial Intelligence (UAI-02), (pages 251258). MorganKaufmann.445fiDALY & HENKirkpatrick, S., Gelatt, C. D., Jr., & Vecchi, M. P. (1983). Optimization simulated annealing.Science, 220(4598), 671680. doi:10.1126/science.220.4598.671.Kristensen, K., & Rasmussen, I. A. (2002). use Bayesian network design decisionsupport system growing malting barley without use pesticides. Computers ElectronicsAgriculture, 33(3), 197217. doi:10.1016/S0168-1699(02)00007-8.Kullback, S., & Leibler, R. A. (1951). information sufficiency. Annals MathematicalStatistics, 22(1), 7986. doi:10.1214/aoms/1177729694.Levine, J., & Ducatelle, F. (2004). Ant colony optimisation local search bin packingcutting stock problems. Journal Operational Research Society, 55(7), 705716.doi:10.1057/palgrave.jors.2601771.Maniezzo, V., & Colorni, A. (1999). ant system applied quadratic assignment problem.IEEE Transactions Knowledge Data Engineering, 11(5), 769778. doi:10.1109/69.806935.Microsoft Research (1995). Win95pts. model printer troubleshooting Microsoft Windows95.Mitchell, M. (1996). Introduction Genetic Algorithms. MIT Press.Moore, A., & Wong, W.-K. (2003). Optimal reinsertion: new search operator acceleratedaccurate Bayesian network structure learning. T. Fawcett, & N. Mishra (Eds.), ProceedingsTwentieth International Conference Machine Learning, (pages 552559). AAAI Press.Mhlenbein, H. (1997). equation response selection use prediction. EvolutionaryComputation, 5(3), 303346. doi:10.1162/evco.1997.5.3.303.Munteanu, P., & Bendou, M. (2001). EQ framework learning equivalence classes Bayesiannetworks. Proceedings 2001 IEEE International Conference Data Mining, (pages417424). IEEE Computer Society. doi:10.1109/ICDM.2001.989547.Munteanu, P., & Cau, D. (2000). Efficient score-based learning equivalence classes Bayesiannetworks. D. A. Zighed, H. J. Komorowski, & J. M. Zytkow (Eds.), Proceedings FourthEuropean Conference Principles Data Mining Knowledge Discovery (PKDD 2000),volume 1910 Lecture Notes Computer Science, (pages 96105). Springer. doi:10.1007/3-54045372-5_10.Muruzbal, J., & Cotta, C. (2004). primer evolution equivalence classes Bayesiannetwork structures. X. Yao, E. Burke, J. A. Lozano, J. Smith, J. J. Merelo-Guervs, J. A.Bullinaria, J. Rowe, P. Tino, A. Kabn, & H.-P. Schwefel (Eds.), Proceedings 8th International Conference Parallel Problem Solving Nature - PPSN VIII, volume 3242 LectureNotes Computer Science, (pages 612621). Springer. doi:10.1007/b100601.Nielsen, J. D., Kocka, T., & Pea, J. (2003). local optima learning Bayesian networks.C. Meek, & U. Kjrulff (Eds.), Proceedings Ninteenth Conference UncertaintyArtificial Intelligence, (pages 435444). Morgan Kaufmann.Ott, S., Imoto, S., & Miyano, S. (2004). Finding optimal models small gene networks.Proceedings Ninth Pacific Symposium Biocomputing, (pages 557567). World Scientific.446fiL EARNING BAYESIAN N ETWORK E QUIVALENCE C LASSES NT C OLONY PTIMIZATIONOtt, S., & Miyano, S. (2003). Finding optimal gene networks using biological constraints. GenomeInformatics, 14, 124133.Rasmussen, L. K. (1995). Bayesian Network Blood Typing Parentage Verification Cattle.Ph.D. thesis, Dina Foulum, Research Center Foulum.Schwarz, G. (1978). Estimating dimension model. Annals Statistics, 6(2), 461464.doi:10.1214/aos/1176344136.Shaughnessy, P., & Livingston, G. (2005). Evaluating causal explanatory value Bayesiannetwork structure learning algorithms. Research Paper 2005-013, Department Computer Science,University Massachusetts Lowell.Shimony, S. E. (1994). Finding maps belief networks NP-hard. Artificial Intelligence, 68(2),399410. doi:10.1016/0004-3702(94)90072-8.Silander, T., Kontkanen, P., & Myllymaki, P. (2007). sensitivity MAP Bayesian networkstructure equivalent sample size parameter. Proceedings Twenty-Third ConferenceUncertainty Artificial Intelligence (UAI-07).Spirtes, P., & Glymour, C. (1990). algorithm fast recovery sparse causal graphs. ReportCMU-PHIL-15, Department Philosophy, Carnegie Mellon University.Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, Prediction, Search. AdaptiveComputation Machine Learning. MIT Press, 2nd edition.Spirtes, P., Meek, C., & Richardson, T. (1995). Causal inference presence latent variablesselection bias. P. Besnard, & S. Hanks (Eds.), Proceedings Eleventh ConferenceUncertainty Artificial Intelligence (UAI-95), (pages 499506). Morgan Kaufmann.Steck, H., & Jaakkola, T. S. (2003). Dirichlet prior Bayesian regularization.S. Becker, S. Thrun, & K. Obermayer (Eds.), Advances Neural Information Processing Systems15 (NIPS*2002), (pages 697704). MIT Press.Sttzle, T. (1998). ant approach flow shop problem. Proceedings Sixth EuropeanCongress Intelligent Techniques Soft Computing (EUFIT 98), volume 3, (pages 15601564). Aachen, Germany: ELITE Foundation.Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press.Tsamardinos, I., Brown, L. E., & Aliferis, C. F. (2006). max-min hill-climbing Bayesian networkstructure learning algorithm. Machine Learning, 65(1), 3178. doi:10.1007/s10994-006-6889-7.van der Putten, P., & van Someren, M. (2004). bias-variance analysis real worldlearning problem: CoIL challenge 2000.Machine Learning, 57(1-2), 177195.doi:10.1023/B:MACH.0000035476.95130.99.Verma, T., & Pearl, J. (1991). Equivalence synthesis causal models. P. Bonissone,M. Henrion, L. Kanal, & J. Lemmer (Eds.), Uncertainty Artificial Intelligence 6, (pages 255268). North-Holland.447fiJournal Artificial Intelligence Research 35 (2009) 119-159Submitted 11/08; published 06/09Trust-Based Mechanisms Robust Efficient Task AllocationPresence Execution UncertaintySarvapali D. RamchurnSDR @ ECS . SOTON . AC . UKIntelligence, Agents, MultimediaSchool Electronics Computer ScienceUniversity Southampton, Southampton, UKClaudio MezzettiC . MEZZETTI @ WARWICK . AC . UKDepartment EconomicsUniversity Warwick, Coventry, UKAndrea GiovannucciAGIOVANNUCCI @ IUA . UPF. EDUSPECS LaboratoryPompeu Fabra UniversityBarcelona, SpainJuan A. Rodriguez-AguilarJAR @ IIIA . CSIC . ESArtificial Intelligence Research InstituteSpanish Council Scientific ResearchBarcelona, SpainRajdeep K. DashNicholas R. JenningsRKD @ ECS . SOTON . AC . UKNRJ @ ECS . SOTON . AC . UKIntelligence, Agents, MultimediaSchool Electronics Computer ScienceUniversity Southampton, Southampton, UKAbstractVickrey-Clarke-Groves (VCG) mechanisms often used allocate tasks selfish rationalagents. VCG mechanisms incentive compatible, direct mechanisms efficient (i.e., maximise social utility) individually rational (i.e., agents prefer join rather opt out). However, important assumption mechanisms agents always successfully complete allocated tasks. Clearly, assumption unrealistic many real-world applications,agents can, often do, fail endeavours. Moreover, whether agent deemedfailed may perceived differently different agents. subjective perceptionsagents probability succeeding given task often captured reasoned usingnotion trust. Given background, paper investigate design novel mechanismstake account trust agents allocating tasks.Specifically, develop new class mechanisms, called trust-based mechanisms,take account multiple subjective measures probability agent succeeding giventask produce allocations maximise social utility, whilst ensuring agent obtainsnegative utility. show mechanisms pose challenging new combinatorialoptimisation problem (that NP-complete), devise novel representation solving problem,develop effective integer programming solution (that solve instances 2 105possible allocations 40 seconds).c2009AI Access Foundation. rights reserved.fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS1. IntroductionTask allocation important challenging problem within field multi-agent systems.problem involves deciding assign number tasks set agents accordingallocation protocol. example, number computational jobs may need allocated agentsrun high performance computing data centres (Byde, 2006), number network maintenancetasks may need performed communications companies number business clients(Jennings, Faratin, Norman, OBrien, Odgers, & Alty, 2000), number transportation tasksmay need allocated number delivery companies (Sandholm, 1993). general case,agents performing jobs asking jobs performed trying maximisegains (e.g., companies owning data centres servers trying minimisenumber servers utilised, communications companies try minimise number peopleneeded complete tasks demanded, transportation companies try use minimumnumber vehicles). Given this, Mechanism Design (MD) techniques employed designtask allocation protocols since techniques produce solutions provabledesirable properties faced autonomous utility maximising actors (Dash, Parkes, &Jennings, 2003). particular, Vickrey-Clarke-Groves (VCG) class mechanismsadvocated number problem domains (Walsh & Wellman, 1998; Hershberger & Suri, 2001;Dash et al., 2003) maximise social welfare (i.e., efficient) guarantee nonnegative utility participating agents (i.e., individually rational). mechanisms,agents typically reveal costs performing tasks valuation requested taskscentre centre computes allocation tasks agent paymentsneed make receive. However, important underpinning assumption mechanismsmake agent always successfully completes every task assigned centre.result assumption allocation (i.e., assignment tasks askedrequester agents executed task performer agents) selected centre basedcosts valuations provided agents. ensures centre always choosesperformers cheapest requesters ready pay most. However,agents chosen centre may ultimately successful completing assignment.example, agent providing access data centre cost 10, success rate100%, might preferable one providing service cheaper cost 510% chance successful. Thus, order make efficient allocations circumstances,need design mechanisms consider task performers costs serviceprobability success (POS). Now, probability may perceived differently different agentstypically different standards means evaluating performancecounterparts. example, different customers might evaluate performance data centredifferent ways timeliness, security, quality output. Given this, turn notiontrust capture subjective perceptions (Ramchurn, Huynh, & Jennings, 2004). takeaccount agents trust agents, well costs, allocating tasks requiresdesign new class mechanisms previously termed trust-based (Dash, Ramchurn,& Jennings, 2004).date, however, existing work trust-based mechanisms (TBMs) ignores number important aspects task allocation problem makes less robust uncertainty (see Section2 details). First, Porter, Ronen, Shoham, Tennenholtz (2008) allow POS reportscome task performer, rather agent. means task requester2fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONmisled task performers opinion (even truthfully revealed) since task requestermay believe, times, task performer failed task performer believes succeeded. Second, previous work (Dash et al., 2004), presented trust-based mechanismcould result inefficient allocations agents strong incentives over-report POS.Even importantly, however, existing trust-based mechanisms completely ignore computational cost associated including POS computing optimal allocation payments.Thus, previous work highlights economic benefits, specify new problem effectively represented efficiently solved. ignoring issues, previous workfailed prove mechanisms actually implemented, solved, whetherscale reasonable numbers agents.background, paper provides economically efficient individually rationalmechanisms scenarios exists uncertainty agents successfully completingassigned tasks. execution uncertainty generally modelled follows. First, potential task performers assessed task requester uses individual experienceperformance information gathered environment (such reports agentsperformance) construct estimation POS. Often sources called confidence reputation respectively (Ramchurn et al., 2004; Dasgupta, 1998), combinedgive notion trust agent performing particular task. combined view trustused robust measure POS single estimate (especially oneoriginating task performer). evident fact agent likelypartial view performance task performer derived finite subsetinteractions. example, task requester ten tasks performed agent may benefitexperience acquired another requesters fifty interactions agent. However, incorporating trust decision mechanism requester introduces two major issues.First, agents use reports agents build trust, introduces possibility interdependent valuations. means value generated one agent systemaffected another agents report mechanism (Jehiel & Moldovanu, 2001; Mezzetti, 2004).This, turn, makes much harder standard VCG-based techniques incentivise agentsreveal private information truthfully. Second, using trust find optimal allocation involvessignificant computational cost show solving optimisation problem trust-basedmechanisms NP-complete.tackle issue interdependence, build upon work Mezzetti (2004, 2007)construct novel mechanism incentivises agents reveal private information. Moreover, help combat computational complexity generated trust, go develop novelrepresentation optimisation problem posed trust-based mechanisms provide implementation based Integer Programming (IP). Given this, show main bottleneckmechanism lies searching large set possible allocations, demonstrateIP solution comfortably solve small medium instances within minutes (e.g., 6 tasks50 agents) hours (e.g., 8 tasks 70 agents).1 doing, provide first benchmarkalgorithms aim solve optimisation problems.detail, paper advances state art following ways:1. Though time taken find optimal solution grows exponentially number tasks, mechanism setsbaseline performance solving optimisation problem posed trust-based mechanisms.3fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS1. design novel TBMs allocate tasks uncertainty completion. TBMs non-trivial extensions paper Porter et al. (2008)first consider reputation task performer within system, additionself-report. allows us build greater robustness mechanism since takesaccount subjective perceptions agents (task requesters particular) POStask performers.2. prove TBMs incentive compatible, efficient individually rational.3. develop novel representation optimisation problem posed TBMs and, giventhis, cast problem special matching problem (Berge, 1973). show solvinggeneralised version TBMs NP-complete provide first Integer Programmingsolution it. solution solve instances 50 agents 6 tasks within one minuteeven larger instances within hours.rest paper structured follows. start providing overview relatedwork Section 2. provide contributions listed step-wise manner. First,simple task allocation model detailed Section 3, introduce TBM singlerequester, single task scenario. Section 4 develops generalised TBM multiple requestersmultiple tasks prove economic properties. dealt economic aspects,turn computational problem implementing TBMs Section 5. Specifically,develop new representation optimisation problem posed generalised TBM, studycomputational costs associated solving problem, provide IP-based solution it.Section 6 discusses number broader issues related development future trust-basedmechanisms.2. Related Workassociating uncertainty mechanism design, build upon work areas. regardscapturing uncertainty multi-agent interactions, work focused devising computationalmodels trust reputation (see papers Teacy, Patel, Jennings, & Luck, 2006, Ramchurnet al., 2004, reviews). models mostly use statistical methods estimate reliabilityopponent agents reports direct interactions opponent.models also try identify false inaccurate reports checking closely report matchesagents direct experience opponent (Teacy et al., 2006; Jurca & Faltings, 2006). Now,models help choosing successful agents, shown generateefficient outcomes given mechanism. contrast, paper provide means usemodels order this.case MD, surprisingly little work achieving efficient, incentive compatible individually rational mechanisms take account uncertainty general.approaches adopted separated work reputation mechanisms mechanismstask resource allocation. former mainly aim eliciting honest feedback reputationproviders. Examples mechanisms include papers Dellarocas (2002), Miller, Resnick,Zeckhauser (2005), Jurca Faltings (2003, 2006). particular, Miller et al. (2005) recently developed peer prediction model, incentivises agents report truthfullyexperience. mechanism operates rewarding reporters according well reports4fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONcoincide experience peers. Specifically, assigns scores distancegiven agents report selected reference reporters reports given task performer.similar way, Jurca Faltings (2007) also attempted solve problem placingimportance repeated presence agents system order induce truthful reporting. However, given focus eliciting honest feedback, mechanism silentfeedback actually used for. particular, cannot employed task allocationscenario study paper case objective maximise overall utilitysociety that, therefore, considers value POS agents. example, car repairlower value building bridge. Hence, feedback car repairer less criticalfeeback bridge builder terms impact social welfare. Interestingly, mechanism shown truth-telling (non-unique) Nash equilibrium budget balanced,individually rational (see Section 6 social desiderata interplay).terms MD task allocation, type uncertainty taken account Bayesian mechanisms dAGVA (dAspremont & Gerard-Varet, 1979; Arrow, 1979). considerscase payoffs agents determined via probability distribution typescommon knowledge agents. However, mechanism cannot deal problemuncertainty task completion, agent information POSagents, common knowledge type distributions. Porter et al. (2008)also considered task allocation problem mechanism one closelyrelated ours. However, limit case agents reportPOS. serious drawback assumes agents measure POSaccurately consider case agents may different perceptionsPOS (e.g., performer believes performs better worse requester perceives).Moreover, consider single requester setting, mechanisms developdeal multiple tasks multiple requesters. Thus, mechanisms consideredtwo-way generalisation theirs. First, allow multiple reports uncertainty needfused appropriately give precise POS perceived requester. Second, generalisemechanism case multiple requesters agents provide combinatorial valuationsmultiple tasks. earlier work problem (Dash et al., 2004), proposed preliminary TBM agents could followed risky, potentially profitable strategy,over-reporting costs under-reporting valuations since payments made according whether succeed fail allocated task (which new mechanism).contrast, work, payment scheme ensures strategy viable thusmechanism robust. Moreover, previous work assumed trust functions monotonically increasing POS reports (similar Porter et al.) develop algorithmsneeded actually solve optimisation problem posed TBM. paper, presentmechanism applies general trust functions also develop algorithms solve TBMs.Finally, work case interdependent, multidimensional allocation schemes. interdependent payoffs, Jehiel Moldovanu (2001) shown impossible achieve efficiencyone-stage mechanism. Mezzetti (2004), however, shown possible achieve efficiency elegant two-stage mechanism reasonable assumptions. mechanismachieves efficiency without needing two reporting stages because, setting consider, payments contingent whether tasks successful agents derivedirect payoff allocation task another agent agents assessmentscompletion probabilities. setting, exists specific function captures interde5fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSpendence exists among agents assessments others POS. functionis, case, agents trust model.3. Single Requester, Single Task Allocation Mechanismssection, first present basic VCG mechanism simple task allocation model (asingle task requested single agent) allocated task guaranteed completed(i.e., agents POS equal 1). briefly describe Porter et al.s (2008) extensionconsiders task performers privately known objective probability finishassigned task. Finally, consider case POS task performer functionprivately known variables held task performer system. ensureschoice made task requester better informed (drawing data various sources)POS task performers. show Porter et al.s mechanism would fail produceefficient allocation settings go provide non-trivial extension modelcater this. doing, define new trust-based mechanism single requester,single task scenario (as prelude generalised mechanism develop nextsection). go prove economic properties simple TBM. Throughoutsection, running example task allocation problem employed demonstrate workingsmechanisms discussed.3.1 Allocation Guaranteed Task Completiontask allocation scenario, single agent derives value certain task performed.end, agent needs allocate task one available task performers,charge certain amount execute task. start considering following simple example:Example 1. MoviePictures.com, computer graphics company, image rendering taskwishes complete new movie. Hence, MoviePictures.com publicly announces intentioncompanies owning data centres execute task. Given interest shown manycompanies, MoviePictures.com needs decide mechanism allocate contractmuch pay chosen contractor, given MoviePictures.com knowcontractors costs execute job (i.e., know much actually costs companyprocess images render required quality).example captured following model. set agents (datacentre agents example), = {1, 2, . . . , i, . . . , I}, privately-known costci ( ) R+ {0} performing rendering task . Furthermore, let MoviePictures.comrepresented special agent 0, value v0 ( ) R+ {0} rendering taskcost c0 ( ) > v0 ( ) perform task (c0 ( ) = case agent 0 cannot execute task).Hence, MoviePictures.com get task performed another agent setcost ci ( ) v0 ( ).Now, MoviePictures.com needs decide procedure award contract, hence,acts centre invite offers agents perform task. devisingmechanism task allocation, focus incentive-compatible direct revelation mechanisms(DRMs) invoking revelation principle states mechanism transformedDRM (Krishna, 2002). context, direct revelation means strategy space (i.e.,possible actions) agents restricted reporting type (i.e., private information,6fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONexample cost valuation task) incentive-compatible means equilibrium strategy(i.e., best strategy certain equilibrium concept) truth-telling.Thus, DRM, designer control two parts: 1) allocation rule determines wins contract, 2) payment rule determines transfer money centre (i.e., MoviePictures.com) agents (i.e., data centres). Let K denoteparticular allocation within space possible allocations K i0 represent agentgets allocated task agent 0. Then, setting, space possible allocationsK = {, 10 , 20 , . . . , I0 } denotes case task allocated.Moreover, abuse notation slightly define cost allocation K agent i,ci (K) = ci ( ) K = i0 ci (K) = 0 otherwise. Similarly, centre, valuenon empty allocation simply value task, i.e., v0 (K) = v0 ( ) K 6=v0 (K) = 0 K = . Finally, let ri () R payment centre agent i. case ri ()negative, agent pay |ri ()| centre.Within context task allocation, direct mechanisms take form sealed-bid auctionstask performers report costs centre (or auctioneer). Agents may wish reporttrue costs reporting falsely leads preferable outcome them. thereforedistinguish actual costs reported ones superscripting latter b.task allocation problem consists choosing allocation payment rulescertain desirable system objectives (some detailed below) satisfied. allocationrule mapping reported costs set allocations, K(bci , bci ) allocationchosen agent reports bci agents report vector bci . Similarly, payment rulemapping reported costs payments agent, ri (bci , bci ) paymentagent agent reports bci agents report vector bci .Following task execution payments, agent derives utility given utility function ui : K R R. common domain, assume agent rational (expectedutility maximiser) quasi-linear utility function (MasColell, Whinston, & Green, 1995):Definition 1. quasi-linear utility function one expressed as:ui (K, ri ) = ri ci (K)(1)K K given allocation.modelled problem above, MoviePictures.com would like use protocolpossesses desirable properties efficiency individual rationality. also needs make sureprotocol incentive compatible: agents must find optimal report true costs.desiderata formally defined follows:Definition 2. Efficiency: allocation mechanism said achieve efficiency outcomegenerates maximises total utility agents system (without considering transfers).is, vectors reports bc, calculates K that:"#Xbci (K)(2)K (bc) = arg max v0 (K)KKiIDefinition 3. Individual Rationality: allocation mechanism said achieve individual rationality agents derive higher utility participating mechanism opting it.7fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSAssuming utility agent obtains opting zero, individually rationalallocation K one (Krishna, 2002):ui (K, ri ) 0 ,(3)Definition 4. Incentive compatibility: allocation mechanism said achieve incentive compatibility agents true type optimal report matter agents report.is:ri (ci , bci ) ci (K(ci , bci )) ri (bci , bci ) ci (K(bci , bci )) ci , bci , bci .Note incentive compatibility implies vector reports agents bcipayments agent must depend report chosen allocation. Incentivecompatibility requires telling truth (weakly) dominant strategy. also importantnote incentive compatibility dominant strategies strongest possible form incentivecompatibility. VCG mechanism property.MoviePictures.com decides employ Vickrey auction (also known second-pricesealed bid auction) since protocol possesses desired properties incentive compatibility,efficiency, individual rationality (Krishna, 2002). detail, receivedsealed bids (reports bc) agents, centre calculates allocation K (bc) accordingEquation (2), transfer ri () winner given by:Xv0 (K )ri (bc) = v0 (K (bc)) maxbcj (K )(4)K KijI\iKi set allocations involve task performer.3.2 Allocation Execution Uncertaintymechanism presented previous section, assumed allocation Kdecided, value v0 (K ) obtained centre (either v0 ( ) task allocated0 otherwise). Thus, implicit assumption allocated task, agentalways perform successfully. However, unrealistic, illustrated following example:Example 2. Many previous rendering tasks required MoviePictures.com allocatedPoorRender Ltd competitive prices. Unfortunately, PoorRender Ltd couldcomplete task many cases lack staff technical problems (whichknew even bidding task). result, MoviePictures.com incurred severelosses. Hence, MoviePictures.com decides alter allocation mechanism wayagents POS completing tasks factored selection cheapest agent.MoviePictures.com assumes contractor knows POS cost privately needsmechanism elicit information truthfully order choose best allocation.problem studied Porter et al. (2008) briefly describe, terms,mechanism order extend generalise later (see Sections 3.3 4). first introduceboolean indicator variable denote whether task completed ( = 1)( = 0). Thus, observable task allocated. Moreover, extend8fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONnotation capture centres valuation task execution v0 () = v0 (K )= 1 v0 () = 0 = 0. setting, assume commonly observed (i.e.,agent believes = 1, agents {0} believe same). renderingexample, might denote whether images rendered appropriate resolutionallow usage not. Furthermore, probability = 1 task allocatedagent dependent upon another privately known variable, pi ( ) [0, 1], POSagent executing task . Note variable privately known task performer itself,single observation within system, carried task performer,POS. Also note task performer incurs cost ci ( ) soon attempts taskirrespective whether successful not.seen, value centre (MoviePictures.com) derive, v0 (), knownallocation calculated. Hence, notions efficiency individual rationality introduced section 3.1 need adjusted new setting. Given probability taskexecuted given agent, consider expected value allocation, v 0 (K, p),calculated as:v 0 (K, p) = v0 (K) pi ( )(5)agent chosen perform task allocation K p = hp1 ( ), . . . , pI ( )ivector POS values agents (the list assessments contractorprobability complete rendering task example). need requireb vector reported POS valuesagents report POS, addition cost. denote phbp1 ( ), . . . , pbI ( )i.following modified desiderata need considered now:Definition 5. Efficiency: mechanism said achieve efficiency chooses allocationmaximises sum expected utilities (without considering transfers):#"Xb)b) = arg max v 0 (K, pbci (K)(6)K (bc, pKKiIb reported agents key computing efficientNote bci (K) pallocation.Definition 6. Individual Rationality: mechanism achieves individual rationality participatingagent derives expected utility, ui , always non-negative:ui (c, p) = ri (c, p) ci (K) 0ri (c, p) expected payment agent receives.order achieve desiderata, one could suppose nave extension standardVickrey mechanism presented would sufficient. mechanism, centre wouldask agents report extended types (bci , pbi ( )). allocation chosen would onemaximising expected utility agents payment rule would conditioned accordingEquation (4) v 0 (K , p) replacing v0 (K ). However, mechanism would failsettings, illustrated next section.9fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS3.2.1 NA IVE PPLICATION V ICKREY AUCTIONExample 3. Consider case MoviePictures.com derives value v0 ( ) = 300rendering task completed let three contractors whose costs ci ( ) renderimages given (c1 ( ), c2 ( ), c3 ( )) = (100, 150, 200). Furthermore, assume contractorPOS given (p1 ( ), p2 ( ), p3 ( )) = (0.5, 0.9, 1). information represented Table1.efficient allocation case (shaded line Table 1) involves assigning task agent2 expected social utility 300 0.9 150 = 120. payment agent 2 using(reverse) Vickrey auction expected values 300 0.9 (300 200) = 170 (from Equation(4)). However, mechanism incentive-compatible. example, agent 1 revealspb1 ( ) = 1, centre implement K = 10 pay agent 1, r1 = 300120 = 180.Thus, agents mechanism always better reporting pbi ( ) = 1, matteractual POS is! Hence, centre able implement efficient allocation.Agent123ci ( )100150200pi ( )0.50.91Table 1: Costs performing task agents perceived probability successfully completingtask.type extension (i.e., including POS) non-trivial POS report agentaffects social value expected centre, agents cost allocation.result, reporting higher POS positively affect agents probability winningallocation thus positively affect utility. rectify this, need meansgain utility balanced penalty truthfully reporting type, agentmaximise utility. achieved Porter et al.s (2008) mechanism, briefly detailnext section.3.2.2 P ORTER ET AL . ECHANISMmechanism based around payments applied completion tasks. Specifically,mechanism finds marginal contribution agent made expected welfareagents depending whether completes assigned task not. Intuitively, works sincepayment scheme punishes agent assigned task complete (i.e., = 0).result, agent incentivised reveal higher POS value real POS sinceallocated task, likely reap punishment rather reward obtainssuccessfully completes task (i.e., = 1).detail, allocation determined centre according Equation (6). payment rule agent task allocated similar VCGmarginal contribution agent system extracted comparing efficient allocationb, ) = 0 allowith second best allocation, excluding agent (the agent gets ri (bc, pcated task). difference expected marginal contribution extracted (i.e.,10fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONtaking account agents real probability success). achieved follows:P(b), pbbv(Kc,p))maxbc(K, = 1v(K)00jI\i jK Kib, ) =ri (bc, pPb)bcj (K ), = 0max v 0 (K , p(7)jI\iK KiKi set allocations excluding agent i.mechanism would work example provided Table 1 since if, example, agent1 reports pb1 ( ) = 1, allocated task paid 300 120 = 180probability 0.5 120 probability 0.5. Thus, average, agent 1 paid 30time incur cost 100, thereby making expected utility 70. Clearly, then,rational agent overstate POS. fact, incentive compatibility mechanismarises agent expected utility, given allocated task, is:b) = pi ( ) v0 (K (bb)) ci (K (bb)) maxui (bc, pc, pc, pK Kib)) max+ (1 pi ( )) ci (K (bc, pK Kib), p) ci (K (bb)) maxc, pc, p= v 0 (K (bK Kib)v 0 (K , pb)v 0 (K , pXjI\ib)v 0 (K , pXjI\ibcj (K )bcj (K )XjI\i(8)bcj (K )Note expected utility within mechanism wouldderived agents nave extension VCG truthful reporting p. However,Porter et al.s mechanism, agents incentive lie. because, pbi ( ) > pi ( )(i.e., agent over-reports POS), agent might allocated task even though:h6= arg max v0 (K x )px ( ) cx (K x )xIKx=x0 ,means could that:b), p) ci (K (bb)) < maxv 0 (K (bc, pc, pK KiXb)bcj (K )v 0 (K , pjI\iresults agent deriving negative utility per Equation (8). Hence, agentreport higher POS values. complete treatment proof incentive-compatibilitymechanism given paper Porter et al. (2008). Furthermore, mechanism alsoproven individually rational efficient.3.3 Allocation Multiple Reports Execution Uncertaintyprevious section, considered mechanism agent privately knownestimation uncertainty task completion. mechanism considers centre11fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSreceive single estimate agents POS. turn attention previouslyunconsidered, general, case several agents may estimate. example,number agents may interacted given data centre provisioning company manyoccasions past therefore acquired partial view POS company. Usingestimates, centre obtain accurate picture given agents likely performancecombines different estimates together. combination results better estimatenumber reasons, including:1. Accuracy estimation: accuracy estimation typically affected noise. Thus,combining number observations lead refined estimate obtainingsingle point estimate.2. Personal Preferences: agent within system may different opinionsconstitutes success attempting task. result, centre may willing assignweight agents estimate believes agents perspective similarown.illustrate points considering following example:Example 4. MoviePictures.com still satisfied solution chosen far.PoorRender Ltd still reports high POS, even though MoviePictures.com noticedfailed task number occasions. PoorRender Ltd believesimages rendered high enough quality used feature film MoviePictures.com believed not. MoviePictures.com therefore cannot rely agentsperception POS decide allocation. Rather, MoviePictures.com wants askagents submit perception others POS. doing, MoviePictures.com aimscapture knowledge agents might either previous sub-contractedtasks simple observations. end, MoviePictures.com needs devise mechanismcapture agents perceptions (including own) measures POS agent usefused measures selection process.example modelled introducing new variable, Expected QualityService (EQOS), noted ij ( ), perception agent POS agent jtask . Now, vector agent EQOS agents (including itself) within systemnoted = hi1 ( ), . . . , iI ( )i. Furthermore, shall denote j EQOS agentswithin system (including itself) agent j. Thus, image rendering example, ij ( )might denote probability perceived agent rendering task completed accordingcertain level quality computer graphics (which perceived differently differentagents). Then, MoviePictures.com needs function order combine EQOS agentsgive resultant POS movie rendered graphic requirements.detail, given previous personal interaction j, compute, based frequency good bad interactions, probability, termed confidence, j POS. Second,also take account agents (i) opinions j, known js reputation society, order compute POS j (Ramchurn et al., 2004). combination measuresgenerally captured concept trust, defined aggregate expectation, derivedhistory direct interactions information sources, j complete12fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONtask assigned it. aggregate trust agent j successfully complete task agent 0function tr0j : [0, 1]|I| [0, 1].multiple ways trust function could computed, often capturedfollows:tr0j () =Xwl lj(9)lIPwl [0, 1] wl = 1. function generates trust weighted sum EQOS values.cases, actually considered probability distributions trust functionexpected value joint distribution constructed individually reported distributions(Teacy et al., 2006; Jurca & Faltings, 2007). Much work exists literature dealsdifferent ways combining distributions biases incompatibilities agentsperceptions taken account. Essentially, however, assign weights different reportsagents choose expected value reports trust agent. However,date, none models actually studies get self-interested agents generate reportstruthfully along maximising social welfare.Now, direct mechanism case elicits agent i, cost EQOS vector,{ci ( ), }, centre decides allocation payments agents. computing expected utility mechanism, agent must evaluate trust, probability success,agent allocated task. raises conceptual difficulty. agenttreat agents POS reports assessing probability task completion (as opposedcomputing best response type reports)? approach take paperagent assumes reported POS agents truthful computing trust anotheragent; precisely, agent computes value trust function using true EQOSreported EQOS agents. Thus, trust agent agent j able comb ). already seen, general payment agent dependsplete task tr0j ( ,b)reported types agents whether task succeeds fails. end, let i(bc,b ). Then, defineagent allocated task vector reported types (bc,b )expected payment agent true types (c, ) reported types (bc,follows:hb ,b)b ,b)i(ci(cb ; c, ) = ri (bb , = 1)tr0b ) + ri (bb , = 0) 1 tr0b )Eri (bc,c,( ,c,( ,point type agent (EQOS plus cost) multidimensional and,common multidimensional world, could several type reports generateexpected payment agent. ready define modified notion incentive compatibility use.2Definition 7. Incentive compatibility (in Dominant Strategies): allocation mechanism saidachieve incentive compatibility dominant strategies agents true type optimal reportmatter agents report. is: c, , bci , b, bci , b,b ; c, ) ci (K(ci , bbi,b ; c, ) ci (K(bEri (ci , bci , ,ci )) Eri (bci , bci ,ci , bci ))2. agent uses reported POS agents computing value trust function seems naturalassumption agent rely agents truthfully reporting types. case, example,history interactions POS reporters publicly known (e.g., eBay Amazon).13fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSNow, case agents view EQOS reports agentstruthful, trust agent agent j able complete task may dependtrue reported types agents; case could relax incentive compatibilityrequirement dominant strategy (ex-post) Nash equilibrium (MasColell et al., 1995),means agents report truthfully, optimal agent always reporttrue type, matter true types agents are. replacing new trust functiondefinition expected payment agent i, definition incentive compatibility wouldchange to:Definition 8. Incentive compatibility (in Nash Equilibrium): allocation mechanism saidachieve incentive compatibility (ex-post) Nash equilibrium agents true type optimalreport provided agents report type truthfully. is: ci , bci , , b, ci , ,b , ; c, ) ci (K(bEri (ci , ci , , ; c, ) ci (K(ci , ci )) Eri (bci , ci ,ci , ci ))next demonstrate Porter et al.s mechanism would work setting extendingexample 1.3.3.1 FAILURE P ORTER ET.ALECHANISMExample 5. Two agents costs performing task requested centre formedperceptions set agents given Table 2. Suppose tr0i () = [1i ( ) + 2i ( )]/2,v0 ( ) = 1.Agent12tr0 ()ci ( )00i1 ( )0.60.80.7i2 ( )10.60.8Table 2: Costs EQOS reports agents single task scenario. trust requester calculatedassuming truthful reports.Porter et al. specify procedure deals EQOS reports. However, natural) instead pbi ( ), ignoreextension technique would allocate according tr0i (breports agent computation payment. implement example.Agent 2 winner since generates expected social utility 0.8, agent 1 wouldgenerate utility 0.7. expected utility agent allocated task (accordingEquation (8)):b ) ci (K (bb )) maxb ) = v0 (K (bb )) tr0i ( ,c,ui (bc,c,K Kih) bcj (K )v0 (K ) tr0j (b(10)b excludes reports agent i, Ki set allocations excluding agent i,j agent allocated task allocation K . Unfortunately, extension breaksincentive compatibility following way. Given efficient allocation computed usingb values agents (using tr0 (bb Equation (6)), value bestreported) instead p14fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONallocation obtained removing one agent could arbitrarily lower. example above, agent1 reports 12 = 0, efficient allocation becomes agent 1 expected social utility 0.7agent 1 gets expected utility 0.1 systems utility drops 0.6 reportsremoved allocation recomputed. agent 1 truthful obtain 0 utility since agent 2would winner case. effect, removal agent system breaksmechanism interdependence valuations introduced trust model.elaborate issue show solve next section.thus need develop mechanism incentive-compatible agents reportingperceptions agents POS. order so, however, need additionallyconsider effect reporting EQOS vector agents expected utility. Specifically,need develop trust-based mechanism EQOS reports agent provideway increasing overall expected utility (as per intuition behind VCG). Then,true value EQOS, mechanism result selection optimal allocationtasks.3.3.2 INGLE R EQUESTER INGLE TASK RUST-BASED ECHANISMIntuitively, following mechanism works ascertaining agent derives positive utilitysuccessfully completes task EQOS report change allocationfavour (thus, mechanism develop regarded generalisation paper Porteret al., 2008).detail, let i(K) agent performing task allocation K; centre firstdetermines allocation according to:#Xi(K)b ) = arg max v0 (K) tr0 (bK (bc,)bci (K)KK"(11)iIcomputed efficient allocation above, adopt similar approach Porter etal.s compute payments tasks executed (see section 3.2.2). However,novelty mechanism lies use agents EQOS reports computationefficient allocation (as showed above). Moreover, additional payments losersincentivise agents select efficient allocation.Thus, apply different payments cases agent winning allocation succeeds(i.e., = 1) fails (i.e., = 0). agent allocated task (i.e., K = { i0 })payment is:b , ) =ri (bc,b )) Bi (bb ) , = 1c,ci ,v0 (K (bb )Bi (bci ,(12), = 0Bi () 0 term independent report (a constant point view)reduces payment needs made agent. briefly discuss value Bi ()could set reduce payout made centre later section, provide greaterdetail section 4.4.addition paying winner, also reward losers k \ following way,depending whether succeeds not:15fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSb , ) =rk (bc,b )) bb )) Bk (bb k ) , = 1c,ci (K (bc,ck ,v0 (K (bb )) Bk (bb k )bci (K (bc,ck ,(13), = 0Intuitively, payment scheme aims incentivise agents reveal typeefficient allocation chosen. Let K0i allocation assigning task agent i. Supposeb ) agents report (bb ).agent type (ci , ) reports type (bci ,ci ,agent wins task, derive following expected utility:b = v0 K0i tr0i ,b ci K0i Bi (bb )ui K0i , ,ci ,(14)b reflects true POS agent i. agent k 6= assigned task,Note tr0i ,agent obtains following expected utility participating mechanism:b = v0 K0k tr0k ,b bb )ui K0k , ,ck K0k Bi (bci ,(15)difference Equations (14) (15) identity winner. Hence,falsely reporting, agent influence identity winner. Agent expected utilitymechanism equal expected social utility system minus constant independentreport. Hence, agent rational report true type, efficient agent(outcome) chosen. shows single task trust-based mechanism incentive compatibleefficient.3Proposition 1. mechanism described Equations (11), (12), (13) incentive compatible.Proposition 2. mechanism described Equations (11), (12), (13) efficient.Proof. Since agent ks report k affects expected utility agents (see Equations(14) (15)), interdependence agents payoffs, valuations. However, agentinfluence transfer report, computation agent paymentb (and bindependent reportci ) dependent actual execution tasktherefore true value. feature permits implementation efficientallocation single-stage mechanism.exemplify payments mechanism, consider following extension Example 5.Example 6. Two agents zero cost performing task requested centreEQOS ij ( ) {0.6, 0.7, 0.8} i, j = 1, 2. Suppose tr0i () = [1i ( ) + 2i ( )]/2,v0 ( ) = 1.setting Bi = 0.6 example, payment agenttask completed successfully 0.4, payment task fails 0.6. Hence,centre profits implementing mechanism. Agents incentive report truthfully,agent likely succeed allocated task. Furthermore, agents willingparticipate, probability success least 0.6 (it 0.6 worst case scenario)3. provide detailed proof generalised case Section 4.3.16fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONhence, agents expect obtain least zero participating: mechanism individually rational. Also note total expected payment centre agents0.8 0.4 2 0.2 0.6 2 = 0.4, could low 0.6 0.4 2 0.4 0.6 2 = 0.show, Bi always chosen individual rationality satisfied.Proposition 3. appropriate choice Bi (), mechanism described Equations (11),(12), (13) individually rational.Proof. participating mechanism, agent obtain 0 utility. However,agent decides participate, virtue selection efficient allocation (which returnsallocation social welfare generated less 0), guaranteed, winner, obtainutility ui described Equation (14) or, loser, utility uk Equation (15). Sincecases ui Bi () efficient allocation chosen, Bi set 0, mechanismindividually rational.Obviously, since agents utilities tied winning agent, also losewinning agent fails but, expectation, agents make profit least 0 case Bi set0. Example 6 shows, centre trying minimise payments (and increase profits),could set Bi greater zero still satisfy individual rationality. Section 4.4,show set Bi value maintains individual rationality minimising paymentsgeneral model.note sometimes may preferable centre give individual rationality.Consider, example, modify Example 6 allow additional EQOS value ij ( ) = 0.3i, j = 1, 2. induce type ij ( ) = 0.3 participate, centre could set Bi () = 0.3,payment following success 0.7 payment failure 0.3. worstcase scenario centre (i.e., centres profit lowest), total expected paymentmechanism 0.8 0.7 2 0.2 0.3 2 = 1 (in best case scenario, totalexpected payment zero). shall see Section 4.4, centre could substantially reducepayments making Bi () depend report agents (i.e., i). Still,may preferable centre set Bi () = 0.6, giving participation agentsEQOS values ii ( ) = ij ( ) = 0.3. general, low EQOS types, centre facestrade efficient task allocation payments minimisation. leave studytrade-off future work (see Section 6 initial thoughts).4. Generalised Trust-Based Mechanismmechanisms presented previous section dealt basic task allocation problemone requester, one task, several performers. Here, aim efficiently solvegeneral problem trust-based interactions one agent requestsperforms (or both) one task. end, extend single requester single tasksetting general one multiple requesters multiple tasks Generalised TrustBased Mechanism (GTBM). extension needs consider number complex featurestop dealt previously. First, need consider multiple requestersmake requests sets tasks task performers perform sets tasks well.Thus, centre acts clearing house, determining allocation payments17fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSmultiple bids task requesters multiple asks task performers. significantlycomplicates problem incentivising agents reveal types since makesure agents reveal costs, valuations, EQOS truthfully one task.Second, computation efficient allocation payments consider much largerspace previously. Thus, believe important show problem modelled,implemented, solved demonstrate mechanism scales increasing numbersagents tasks (the computability aspects dealt Section 5).following example illustrates general setting.Example 7. using trust-based mechanism months, MoviePictures.com madesignificant profits expanded several independent business units, performing renderingtasks rendering tasks performed certain clients. Now, MoviePictures.com would likefind ways business units efficiently allocate tasks amongst themselves. However,companies uncertainties others performance rendering tasks.example, business units, HighDefFilms.com, believe PoorRender Ltd (now partMoviePictures.com) inefficient, others, GoodFilms.com, believe bad,recently large set animations rendered well cheap price. caterdifferences opinion maximising overall utility, MoviePictures.com needs extendsingle task trust-based mechanism implement generalised mechanism efficiently.order deal complex setting, extend task allocation model nextsubsection, describing allocation rule payment scheme Section 4.2 provingeconomic properties mechanism Section 4.3.4.1 Extended Task Allocation SettingLet = {1 , 2 , ..., } denote set tasks requested performed (comparedsingle task before). use notation .i specify subset tasksperformed specifically agent i.4 Similarly, adding superscript task, i. Kdenotes subset tasks agent performs. Note nothing model restrictsagent task performer requester.selected allocation K multiple task, multiple requester model generates matching problem involves finding agents perform tasks requestedII }). Let set possible allocationsagents (e.g., K = {111 , 112 , . . . , 1I1I , . . . ,denoted K. Note requested tasks need allocated: is, matching K needperfect.multiple task case, agents may express valuations costs sets tasks wellsubsets sets tasks. example, agent may vi (1 , 2 , 3 ) = 100 vi (1 , 2 ) =10 vi (3 ) = 0. Then, agent gets 1 , 2 3 executed gets value 100,1 2 get executed 3 fails, agent still obtains value 10. Similarly, agent maytask execution costs ci (4 , 5 , 6 ) = 100 ci (4 , 5 ) = 40 ci (6 ) = 10. captureinter-relationships valuations, let Kij set tasks within allocation Kperformed agent j agent (Kij could empty set). Note taskspecific task requester. means agents 1 2 request task , task performer4. paper, consider agents requesting performance multiple units tasks. Although modeleasily extensible case, explanation much intricate.18fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION(putting one bid ) matched agent 1, performs agent 1 agent2. abuse notation slightly define K = {Ki , K }iI Ki = (Ki1 , ..., KiI )K = (K1i , ..., KIi ). agent value (assuming tasks K completed) costallocation K, vi (K) + {0} ci (K) + {0} respectively, whereby:5vi (K) = vi (Ki )ci (K) = ci (K )Kh KMoreover, within model, agent EQOS vector, = {ij (Kh )}j,hIrepresents belief successful agents within system completing tasks Khagent h. Thus, general level, agent type given = {v , ci , }.je j K j EQOSgiven set tasks Kithatfij mustperform i, subset tasks Ke j completede j fifi K j , trust exactly set tasks Kvector , let trj Kj. trust computed shown Section 3.3 simply replacing agent 0agent replacing single task set tasks . single requester case, trustfunction represents aggregate belief agents given task performer hencetask requesters form probability(give agents EQOS reports) givenfi successQ j e j fifi jfietri Ki fi Ki , .task performer. Finally, let tri Ki fi Ki , =jIready present generalised trust-based mechanism.4.2 Allocation Rule Payment Schemegeneralised mechanism (GTBM), task requesters first provide centre listtasks require performed, along valuation vector associated settasks, whereas task performers provide costs performing sets tasks.6 agentsalso submit EQOS vector centre. Thus, agent provides centre reportsbi = {bb = (b1 , ...,bI ) report profile. Given this, centre appliesb },vi, bci ,rules mechanism order find allocation K net payments ri agent i.detail:1. centre computes allocation according following:b =Karg maxXK={Ki ,K }iI K iIXe KiKe ) trivbi (Kfie fifi Ki ,b bci (K)K(16)Thus, centre uses reports agents order find allocation maximisesexpected utility agents within system.b .2. agents carry tasks allocated allocation vector K5. result setup, agent may want sets tasks performed may unable performtasks. cases, assign default value 0 cost sets tasks.6. noted before, task performers also task requesters time (and vice versa).19fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS3. centre computes payments agents, conditional completion tasks allocated. Let (Ki ) indicator function takes value one Ki settasks (requested agent agents) completed, takes value zerootherwise. payment agent follows:riXb () =,jI\iXe j K (b)Kjej bej Kcjvbj Kbbi )KBi ((17)bi ) 0 constant point view (i.e., computed independentlyBi (agent reports, may depend reports agents), usedreduce payout centre make.discussed Section 3.3.2, centre faces trade-off. reducing value Bi ()induces participation larger set types (i.e., types low EQOS), increasescentres payments agents, making mechanism less profitable centre. Thus,scale payments one might expect application GTBM depends whethercentre decides satisfy individual rationality constraint, thus making sure everytype wants participate. shall see Section 4.4, centre decides satisfyindividual rationality constraint, scale payments agent increases lowerbound trust values could derived using EQOS report.also noted computation payments requires solving several optimisation problems (i.e., finding optimal allocation without several reports).number agents increases, difficulty computing payments increaseimportant show payments efficiently computed. elaboratesolution Section 5. so, however, detail prove economicproperties mechanism follows.4.3 Economic PropertiesHere, provide proofs incentive compatibility7 efficiency mechanism.also prove values Bi make mechanism individually rational.Proposition 4. GTBM incentive compatible.Proof. order prove incentive-compatibility, analyse agent best response ( i.e.,bi = {bbi . first calculate expectedb } )) agents reportbest reportvi, bci ,utility agent derive given mechanism.7. Again, place caveat notion incentive compatibility use Section 3.3 (i.e.,Dominant Strategy (ex-post) Nash equilibrium depending whether agent computes trust functionsusing agents POS reports true not).20fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONby:b true types givenexpected utility agent reported typesb =ui ;Xb be KK,bi ,bi , ,e |Kie ) tri Kbvi (Kbi ,bici Kb+ Eri ;(18)Eri expectation ri taken respect likelihood task completion.e j ) equal one (i.e., tasks Ke jprobability attached indicator variable (Kcompleted), givenfi settasks requested j Kj agents different reportfibeb . Hence, use formula payments obtain:, trj Kj fi Kj , ,fibi ,bi , ,e j fifi Kje j trj Kbvbj KXKe j K b ,bbi )bEri ; =jBi (jI\ibi ,bibcj K(19)replace expression formula ui observe agentb key point note agent computesaffect utility report changing K ().b ).value trust function using true value ( rather reported valueNow, Equation (16) implies allocations K:bi ,bi ; ,bi ; ui(20)ui ,Xefficient allocation, computed taking account true type reportedbi better equal allocation.types agentsGiven condition since Equation (20) applies possible realisations ,mechanism incentive compatible.Proposition 5. GTBM efficient.Proof. Given incentive compatibility mechanism, centre receive truthful reportsagents. result, compute allocation according Equation (16), therebyleading efficient outcome.Proposition 6. exist values Bi () GTBM individually rational.Proof. begin making standard assumption agent derives ui = 0,participating mechanism. Then, remains shown agent derives non-negativeutility mechanism. Since efficient allocation chosen (and worst null allocation),expected utility agent always greater equal Bi () according Equation(18). Since Bi () set 0, mechanism individually rational.Note possibly many values Bi ( ), besides Bi = 0, guarantee individualrationality.21fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSSpeaking generally, easily seen GTBM mechanism multiple task,multiple requester scenario generalisation GTBM mechanism single requestersingle task. also generalisation mechanism Porter et al. simply assumeagent EQOS probability success. Moreover, paperPorter et al. example, Bi specified follows:Xbi ) = max v0 (K ) pbi ( )Bi (bc(K)jK KijI\ipbi ( ) reported probability completion agent assigned task allocationK Ki set allocations excluding agent i.4.4 Extracting Minimum Marginal Contributionnow, considered Bi ( ) could set arbitrary values try reducepayments made centre agents. interestingly, possible,standard VCG mechanism, pay agent marginal contribution system.However, case, due interdependence valuations, simple comparingsocial welfare without given agent system commonly done VCG-basedmechanisms (Porter et al., 2008 obvious example this). because, case,agent removed domain used compute efficient allocation, remaining EQOSreports arbitrarily change allocation value. could, turn, exploited agentsimprove utility. example Section 3.3.1 showing failure simple extensionPorter et al.s mechanism illustrates point.Assuming centre wants induce participation agent types, proposenovel approach extracting marginal contribution agent, taking account EQOSreports agents possible reports agent could make. Let Ki set possibleallocations agent excluded society. value Bi () chosenequivalent social utility mechanism agent excluded EQOS reportschosen minimise social utility, is:fiXXbi ) =e j fifi Kj , ,e j trj Kb bcj (K) (21)Bi (minmaxvbj K|I||T|KKi[0,1]jI\i eKj Kjnoted Bi computed using lowest trust values could derived usingEQOS reports.Then, generalised payment scheme is:X Xbb (.) =ej bej KKcKvbri ,jjbjI\i Ke j KjfiXXe j fifi Kj , ,e j trj Kb bcj (K)vbj Kminmax[0,1]|I||T | KKi jI\i eKj Kj(22)22fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONpoint note incentive compatibility (and hence efficiency mechanism)still holds given payment scheme still independent reports. fact, ri rewardsmaximum difference agent could make setting elements different values[0, 1]|I||T | .8procedure reduces payments made centre, keeping individual rationalitysince value efficient allocation (given incentive compatibility proven earlier) alwayshigher equal value Bi , means that:ui (K (), ) =XjIXe j K ( )Kje j trjvj KXfie j fifi Kj () , cj (K ())Kminmax[0,1]|I||T | KKi jI\i eXKj Kje j trjvj Kfifie j fi Kj , , cj (K) 0;Kalso noted equation implies restriction placedfunctional form trust function tr payment scheme work propertiesmechanism hold. improvement previous mechanisms (see Section 2)considered trust functions monotonically increasing i.Now, choice Bi determines whether centre runs mechanism profit not.Hence, understand scale payments may GTBM discussed section,consider following example.Example 8. n agents, = {1, ..., n}, requiring single task performedthem. agents value 1 task performed zero costperforming tasks. EQOS agent h agent probability succeedingtaskPagent j h (Kj ) [x, 1] h, i, j = 1, ..., n. Suppose trj () =hI h (Kj ) /n.example, EQOS agent interval [x, 1], x viewedlower bound expected probability success task. Equation (21)compute value Bi :"P#(K ) + xXjhI\i hBi ( ) =maxnjI\iNote that, depending value , Bi ( ) could value (n 1)x(n 1)(n 1 + x)/n; Bi increases lower bound x agents EQOS. actual paymentagent depend success failure task (e.g., payment Bi tasksfail). Equation (19), calculate value expected payment agent as:"#X (Kj )XhEri () =maxBi ( )nhIjI\i"#X (j) xnjI\i8. minimisation takes place domain trust values could [0, 1] general case.23fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS(j) agent allocated task agent j efficient allocation rule. Let EVtotal expected value tasks:"P#(j)XiIEV () =njINote total expected value tasks greater sum expected paymentsagents, is:" (j)##"P(j)XXXxiI>nnjIiIjI\iThus, centre always profits mechanism. lower bound difference totalP(i)+ (n 1)x]/n. Note also lowerexpected value total expected payments [ iIbound centres profit mechanism increases lower bound EQOS x.pointed discussion Example 6 Section 3.3.2, centre trying minimise payments, could give individual rationality, increasing Bi , cost inducingagent types participate mechanism. may appealing probabilitytask failure high; cases, centre may prefer avoid paying amount almost largetotal value tasks. hand, number practical applications centremay want use mechanism induces participation types, described section.certainly case, example, lower bound EQOS (i.e., lower boundprobability tasks successful) high. Moreover, mechanism participationtypes appropriate centre mainly seeks maximise social welfare. Consider, example, government trying boost economy major public infrastructure projects.order so, may willing invest trust-based mechanism get best infrastructures built cheapest cost. Moreover, government may willing make low profitorder ensure survivability construction companies guaranteeing payoffparticipate mechanism. Another example company might want involvetask performers would company trying acquire much information possibletask performers order maximise returns future decisions. Following running scenario, say MovePictures.com needs contract video editing company add computergraphics movie may become blockbuster graphics well done. case tasksuccessful, MoviePictures.com likely get many contracts future. therefore criticalavailable information collected agents order choose reliable videoediting company. case, MoviePictures.com may accept smaller short-run profit runningmechanism full participation, order guarantee selected agent best onefuture contracts obtained.summarise, section devised mechanism incentive compatible, individually rational efficient task allocation uncertainty multiple distributed reportsused order judge uncertainty. noted need two-stage mechanisms, work Mezzetti (2004), settings condition paymentscompletion tasks (the indicator function () captures dependence payments taskcompletion). far, considered economic properties mechanisms,argued earlier, part picture. next section, report implementation.24fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION5. Implementing Generalised Trust-Based Mechanismshown above, addition trust basic task allocation problem complicatespayment scheme, also requires larger number important optimisation steps normalVCG. detail, trust-based mechanisms require agents specify expected value settasks depending performer tasks which, turn, means space solutionsexplored significantly larger common task allocation problems. Moreover, paymentscheme trust-based mechanisms requires finding efficient allocation multiple timeswithout agents reports. added level complexity, important showmechanisms actually implementable solutions found usefully sized problemsreasonable time.9background, section describe first formulation implementation GTBM. particular, GTBM, tackle main optimisation problem posedEquation (16) (which repeated several times payment scheme). commonlyreferred winner determination problem combinatorial auctions. order solve it,take insight solutions combinatorial exchanges often map problem wellstudied matching problem (Kalagnanam & Parkes, 2004; Engel, Wellman, & Lochner, 2006).doing, develop novel representation optimisation problem using hypergraphsdescribe relationships valuations, trust, bids task performers castproblem special hypergraph matching problem. Given representation, ablesolve problem using Integer Programming techniques concise formulationobjective function constraints.5.1 Representing Search Spaceimportant define search space way relationships valuations, bids,trust, tasks clearly concisely captured. particular, representation aims mapGTBM optimisation problem matching problem well studied literature.this, representation must allow us define whole space feasible task allocations,and, subsequently, define select valid solutions GTBM optimisation problem.Now, allow bidders (task performers) askers (task requesters) express bids valuations consistent implementable way, choose XOR bidding language. biddinglanguage requires auctioneer accept one bid XOR bidXOR bid belong one agent. choose particular bidding languageshown valuation expressed using (Nisan, 2006).10 example XORbid context would {ci (1 , 2 ) XOR ci (1 , 3 ) XOR ci (1 , 2 , 3 )} means agentwould go one three bids tasks 1 , 2 3 (ci could also replacedvi task requesters). terms running example, bid would express PoorRenderLtds cost performing sound editing task (i.e., 1 ), movie production task (i.e., 2 ),combination (i.e., 1 , 2 ).9. already known computing efficient allocation payments VCG mechanisms NP-hard (Sandholm,Suri, Gilpin, & Levine, 2002). Therefore, finding efficient solutions VCG mechanisms already significantchallenge right.10. bidding languages (such describing Atomic bids, Nisan, 2006) could equally well usedmodel would require minor changes constraints need apply.25fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS"VH+fiff$fi(%fi)'&"#"((!H!"C$%()&(*'Figure 1: Graphical representation GTBM search space. Nodes colour represent valuationcost nodes belong agent (here nodes v1 belong agent 1 c4 belongagent 4). Edges colour either originate node end node.build overall representation problem, first focus representing expected valuations costs well relationships. depicted Figure 1. detail,specify three types nodes: (1) valuations (along V column); (2) bids (under C column);(3) task-per-bidder nodes (under column). node vi ( ) V column standsvaluation submitted agent set tasks . node cj ( ) C column standsj.bid issued agent j tasks . element represents allocationsingle task task performer (bidder) j task requester yet determined (represented dot). words, elements represent patterns single-task allocations.term elements task-per-bidder nodes.Note possible different valuations come requester.labelled subscript. Moreover, since opted XOR bidding language,valuations belonging requester mutually exclusive.26fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATION5.1.1 EFINING R ELATIONSHIPSVALUATIONS , TASKS ,B IDSj.node vi (..., , ...) VGiven nodes defined A, V, C, relating nodedefine assignment task j specific valuation vi (..., , ...). Similarly,j.node cj (..., , ...) C define assignment taskrelating nodej.j.specific bid cj (..., , ...) agent j. Therefore, triple (v,, c) v V,A, c Cfully characterises allocation task , namely single-task allocation. Hence, seenFigure 1, define two types relationships: valuations task-per-bidder nodes(noted edges e1 , e2 , ...), bids task-per-bidder nodes (noted edges e1 , e2 , ...).11Using relationships, valuation related set task-per-bidder nodesfully cover performance task(s) valuation. instance, relatev1 (1 , 2 ) nodes 14. (agent 4 performs task 1 ) 22. (agent 2 performs task 2 )guarantee performance tasks 1 2 . Similar valuation relationships, node Crelated set task-per-bidder nodes bid splits. Thus, Figure 1,bid c4 (1 ) related 14. , whereas bid c2 (2 , 3 ) related nodes 22. 32. .Thus, identify task performers task given valuation. critical sinceGTBM, contrary common task allocation mechanisms (such VCG Mth price auctions),requires identify exactly performs task order determine POS task (byvirtue requesters trust performer) hence expected value task.seen, representation allows us capture tasks performers taskssince valuation node V potentially related multiple nodes A; and, likewise,bid C column potentially related multiple nodes A. capture related relationships precisely, define special edges connect several nodes (e.g., ones depictede1 , e2 , ,e1 , e2 ,... Figure 1). edges termed hyperedges combine numbersingleton edges. Hence, Figure 1 best described hypergraph (Berge, 1973). orderprecisely define matching problem GTBM poses, elaborate formalismhypergraphs since help concisely expressing problem later on. specifically,formal notion hypergraphs, introduced paper Berge (1973), is:Definition 9. Hypergraph. Let X = {x1 , x2 , . . . , xn } finite set n elements, let E ={ej |j J} family subsets X J = {1, 2, ...}. family E said hypergraphX if:1. ej 6= (j J)2. jJ ej = X.pair H = (X, E) called hypergraph. elements x1 , x2 , . . . , xn called verticessets e1 , e2 , . . . , ej called hyperedges.say hypergraph weighted associate hyperedge e E real number,w(e), called weight e. used give less importance edges.formal definition hypergraphs, observe Figure 1 results overlapping two separate hypergraphs: (i) valuation hypergraph occurs linking valuations11. Figure 1 depicts sample possible relationships ease illustration.27fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGStask-per-bidder nodes; (ii) bid hypergraph occurs linking bid corresponding task-per-bidder nodes. follows, formally define hypergraphs valuations bids later structurally characterise notions feasible optimalallocations.5.1.2 VALUATION H YPERGRAPHvaluation hypergraph highlights main difference GTBM common combinatorial exchanges (e.g., based traditional VCG Mth -price auctions). particular,GTBM valuations need take account trust task requester task performerwhile, normal combinatorial exchanges, task requesters indifferent task performers.means weight hyperedge valuation hypergraph dependent trust largenumber edges need generated (one per task performer) case normalcombinatorial exchanges.define valuation hypergraph, need define hyperedges emanate nodeV one nodes A. end, let V = {vi ( ) 6= 0| , I} C = {cj ( ) 6=| , j I} sets valuations bids respectively. Let j. = { |: cj ( ) 6= } set tasks agent j submits bids. Hence,= {kj. |k j. , j I, cj ( ) C} set containing tasks bid bidder.12Furthermore, need define auxiliary sets follows. Given valuation settasks , set nodes fulfils if:[{kj. } = | | = |A|kj.instance, set nodes = {14. , 22. } fulfils valuation {1 , 2 }. Hence,subsets fulfil valuation set tasks expressed using defined as:[= {A |{kj. } = | | = |A|}kj.instance, considering example Figure 1,A{1 ,2 } = {{14. , 24. }, {14. , 22. }, {14. , 25. }}A{1 ,3 } = {{14. , 34. }, {14. , 32. }}Given definitions, define set hyperedges connected valuationvi ( ) V as:Eiv ( ) = aA {{vi ( )} a}instance, Figure 1:E1v (1 , 2 ) = {e1 , e2 , e3 } E1v (3 ) = {e4 , e5 },e1 = {v1 (1 , 2 ), 14. , 24. }, e2 = {v1 (1 , 2 ), 14. , 22. }, . . . , on.12. Recall since mechanism proven incentive-compatible use agents true valuationscosts instead reported counterparts.28fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONset hyperedges containing valuations agent defined as:[Eiv =Eiv ( )Then, set hyperedges connecting nodes V nodes defined as:[Ev =EiviIGiven this, define valuation hypergraph pair:Hv = (V A, E v )Thus, hyperedge Hv consists single valuation vertex corresponding element Valong complete task allocation valued tasks task-per-bidder nodes A.valuation hypergraph Hv partly defines space within solution needs found.However, order define quality solution found, important define weightattached hyperedge hypergraph Hv . weight hyperedge actually equalexpected value allocation tasks set task performers (bidders). Consider,instance, valuation v1 (1 , 2 ). possible matchings fulfil representedpairs (1.1 , 2.1 ). example, hyperedge e2 involving pairing (141 , 221 ) denotesagent 4 performs task 1 agent 1 agent 2 performs task 2 agent 1. expected valuationassociated allocation depends POS agents 4 2 performing tasks 1 2respectively.case, expected valuation associated e2 assessed as:v 1 (141 , 221 ) = v1 (1 , 2 ) p4 (141 ) p2 (221 )+v1 (1 ) p4 (141 ) (1 p2 (221 ))+v1 (2 ) (1p4 (141 ))(23)p2 (221 )p function returns POS agent assigned given task (computed usingconfidence, reputation, trust). Notice value (1 pi (kij )) represents probabilityagent failing perform task k agent j. Since requests submitted 1 2 alone,v(1 ) = v(2 ) = 0. Thus, expected valuation associated particular allocation representedarc e2 becomes v 1 (141 , 221 ) = v1 (1 , 2 ) p4 (141 ) p2 (221 ). similar argument,obtain v 1 (141 , 251 ) = v1 (1 , 2 ) p4 (141 ) p5 (251 ) 6= v 1 (141 , 221 ), correspondinghyperedge e3 .Generalising, given hyperedge e E v valuation vi ( ), readily build allocationtasks elements e vi ( ). p function returns POS (beconfidence, reputation, trust) given task performer requesters point view,compute expected valuation allocation defined hyperedge e follows:Xv ( ) =pj (lji )1 pj (wji )(24)vi ( )lj. e,llj. e,w \29fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSwords, given hyperedge e E v , weight assessed using Equation (24)equivalent expected value computed Equation (16) (i.e., sum expected valuesallocations agent i). Now, given edge valuation hypergraph assignedweight, Hv termed weighted hypergraph.5.1.3 B ID H YPERGRAPHdefine bid hypergraph need determine hyperedges connect bids task-perbidder nodes. detail, given bid cj ( ) C, relate task-per-bidder nodesconstructing hyperedge Ejc ( ) = {cj ( )} {kj. |k }. hyperedge assigned weightequal cost cj ( ). set hyperedges containing bids agentdefined as:[Eic =Eci ( )Given this, set hyperedges connecting nodes C nodes defined as:[Ec =EiciIFinally, define bid hypergraph pair:Hc = (A C, E c )words, hyperedge Hc consists single bid vertex corresponding elementC along corresponding task-per-bidder nodes A. Notice definitions valuationbid hypergraphs ensure hyperedge H v contains single valuation Vhyperedge H c contains single bid C.5.1.4 EFINING ATCHING P ROBLEM GTBMdefined valuation bid hypergraphs, structurally characterise notionsfeasible optimal allocations (these needed determine computational complexityproblem define objective function particular). purpose, must firstly recallnotions hypergraph theory. hypergraph, two hyperedges said adjacentintersection empty. Otherwise said disjoint. hypergraph H = (X, E),family E E defined matching hyperedges E pairwise disjoint. respectgiven matching E , vertex xi said matched covered hyperedge Eincident xi . vertex matched, said unmatched exposed. matchingleaves vertices exposed said complete.Based definitions above, characterise feasible allocations GTBM follows.First, must find matching valuation hypergraph necessarily complete (somevaluations may remain exposed). Second, must find another matching bid hypergraphnecessarily complete either. two matchings must related following manner:task-per-bidder nodes matchings same. words, given task-perbidder node, must related valuation node bid node, else excludedmatchings. way, valuations bids linked create single-task allocations.instance, Figure 1, e2 belongs matching valuation hypergraph, e4 must30fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONpart matching bid hypergraph ensure bid 22. eithere1 , e2 , e3 part matching bid hypergraph ensure bid 14. .formally:Definition 10. Feasible allocation. say pair (E v , E c ) defines feasible allocation iff:E v matching Hv .E c matching Hc .A: ( matched E v ) ( matched E c ).Given feasible allocation (E v , E c ) defined above, straightforward assess expected utility agents within system follows:XeE vw(e)Xw(e )e E csince weights hyperedges valuation hypergraph stand expected valuationsweights hyperedges bid hypergraph stand costs. Solving Equation (16)GTBM amounts finding feasible allocation maximises expected utility agentswithin system. Therefore, following definition naturally follows.Definition 11. GTBM Task Allocation Problem problem assessing task allocationmaximises expected utility agents within system amounts solving:arg max(Ev,E c )XeEwv (e)Xwc (e )(25)e E cv(E v , E c ) stands feasible allocation.defined matching problem GTBM, next describe solutionproblem using Integer Programming techniques commonly used solve problems(Cerquides, Endriss, Giovannucci, & Rodrguez-Aguilar, 2007).135.2 Integer Programming Solutionsection show map problem posed Equation (25) integer program(Papadimitriou & Steiglitz, 1982) efficiently implemented solved. Giventranslation, resulting program solved powerful commercial solvers ILOGCPLEX14 LINGO.1513. special purpose algorithms (e.g., using dynamic programming search trees) could also designed solvecombinatorial problem. However, understand magnitude problem compare difficultysolving problem similar problems, believe better first attempt find solution usingstandard techniques IP.14. http://www.ilog.com15. http://www.lindo.com31fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGS5.2.1 BJECTIVE F UNCTIONIDE C ONSTRAINTStranslation Equation (25) IP reasonably straightforward given representation.Thus, solving GTBM task allocation problem amounts maximising following objectivefunction:Xxe wv (e)Xye wc (e )(26)e E ceE vxe {0, 1} binary decision variable representing whether valuation hyperedgee selected not, ye {0, 1} binary decision variable representing whether bidhyperedge e selected not. Thus, xe decision variable selects given valuationgiven task-bidder matching, ye selects given bid.However, side constraints must fulfilled order obtain valid solution. First,semantics bidding language must satisfied. Second, hyperedge containing set taskper-bidder nodes selected, must ensure bids covering nodes selected too.Moreover, employ XOR bidding language, auctioneer centre caseselect one bid per bidder one valuation per asker. Thus, bidders,constraint translates into:Xye 1(27)e Eicinstance, Figure 1 constraint ensures auctioneer selects one hyperedge e1 , e2 ,e3 , since belong agent 4 (they come nodes labelled subscriptc4 (.)).valuations, XOR constraints involving collected following expression:Xxe 1(28)eEivinstance, Figure 1 constraint forces auctioneer select one hyperedge e1 , e2 ,e3 , e4 , e5 since belong agent 1 (they come nodes labelledsubscript v1 (.)).valuation hyperedge e E v selected, set task-per-bidder nodes connectede must performed corresponding bidder agent. instance, Figure 1, hyperedgee5 selected, task-per-bidder nodes 141 341 must covered bid agent 4.case, bid c4 (1 , 3 ) one covering tasks. Thus, select hyperedge e5forced select bid c4 (1 , 3 ) selecting hyperedge e3 . Thus, terms hyperedges, mustensure number valuation hyperarcs containing given task-per-bidder node lessequal number bid hyperarcs containing it. Graphically, means numberincident valuation hyperedges given node must less number incident bidhyperedges a.XeE v ,kj. exeXyekj.(29)e E c ,kj. ecase free-disposal (i.e., allow agents execute tasks without askedfor) simply replace =. summarise, solving GTBM task allocation problem32fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONamounts maximising objective function defined expression (26) subject constraintsexpressions (27), (28), (29). Next, determine complexity results problem.5.2.2 C OMPLEXITY R ESULTSrepresented GTBM task allocation problem defined corresponding IP formulation, analyse computational complexity order show difficulty solving GTBM.also identify main parameters affect computational costs finding optimal allocation. parameters allow us determine settings GTBMpractically used.Proposition 7. GTBM task allocation problem N P-complete cannot approximatedratio n1 polynomial time unless P = ZPP, n total number bidsvaluations.Proof. Notice optimisation model, formalised Equation (26), naturally translatescombinatorial exchange (Kalagnanam, Davenport, & Lee, 2000). translation achievedusing representation taking goods (in combinatorial exchange) dummy tasks, bids elements C, asks weights hyperedges Hv . Thus,bids remain exchange, number valuations may significantly increase.reason introduction trust theoretical model makes initial valuations (asks),elements V, allocation-dependent. Hence, every single valuation V causes several asksoriginated exchange considering bidder task may allocated(see examples Section 5.1.2). shown Sandholm et al. (2002), decision problembinary single-unit combinatorial exchange winner determination problem N P-completeoptimisation problem cannot approximated ratio n1 polynomial time unless P = ZPP,n number bids. Therefore, optimisation problem N P-hard,GTBM.proof, understood search space GTBM task allocationproblem significantly larger traditional combinatorial exchanges dependency valuations bidders performing tasks. follows, provide formulaallows us calculate exactly big search space is. allows us determine whetherinstance solved actually handled solver (which limitsmemory requirements computation time).detail, say Ak subset containing task-per-bidder nodes referringtasks. formally, Ak = {kj. | j I}. example Figure 1,A2 = {24. , 22. , 25. }. Thus, expression assess number feasible allocations is:|E v | =X X|Ak |(30)iI vi ( )6=0 kObserve number possible allocations computed cardinality E v (i.e.,number valuation hyperarcs) since exactly determines number ways valuationssatisfied provided bids. total number decision variables Integer Programthus |E v | + |E c |. Since number expected valuations several times larger numberbids, expect number decision variables associated bid hyperedges much less33fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSnumber valuation hyperedges. Hence, assuming |E c | |E v |, number decisionvariables order |E v |.order understand implications parameters, consider case taskperformers bid tasks requesters submit single valuation tasks. Specifically,consider scenario 15 task performers, 20 requesters, 5 tasks. Given case|Ak = 5|, number allocations |Ev | = 20 155 = 15187500. reality, agents mayable submit bids asks tasks would result significantly lower numberallocations (given possible matchings). Hence, see whether instances practicallysolved, Appendix A, report running times solver, showing instances less2 105 variables comfortably solved within 40 seconds (in worst case). takentogether, empirical results formula compute size input (i.e., Equation 30)allow us affirm that, even computational cost associated GTBM potentialrather high, solution handle small medium sized problems reasonable time (seetable 3). However, seen, time complete grows exponentially numberSet123Tasks5810Task Requesters202020Task Performers151515Worst Case Running Time3440 mins3 daysTable 3: Average running times different numbers tasks agents (taken 300 sample runs set1, 50 sample runs sets 2 3).tasks. experimental analysis, also found impact increasing numbertask performers task requesters significant increasing number tasks.explained fact that, given setup, larger number tasks allows significantlymatchings bids asks larger number bids asks. Hence, many taskrequesters performers accommodated small numbers tasks. also notedexpect worst case results occur fairly rarely average (much less halfinstances generated parameters), shown Figure 2 Appendix A.described complete picture GTBM implementation, next discussimportant issues may arise trying use GTBM task allocation.6. Discussionpaper developed task allocation mechanisms operate effectively agentscannot reliably complete tasks assigned them. Specifically, designed novel GeneralisedTrust-Based Mechanism efficient individually rational. mechanism dealscase task requesters form opinions task performers using reports environment direct interactions performers. addition studying economicproperties allocation mechanisms, provided optimisation model generates solutions guarantee efficiency mechanism. optimisation model first solvertrust-based mechanisms (and mechanisms value allocation dependsperformer allocation) based Integer Programming. result, showninput explodes combinatorially due huge number possible allocations mustenumerated. Nevertheless, computational cost associated GTBM shown34fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONrather high, given implementation, still able manage small medium-sized problemsreasonable time.Speaking generally, work trust-based mechanisms number broader implications. First, GTBM shows explicitly blend work trust models workmechanism design. Since mechanism guarantees certain properties hold task allocationproblems, used new, well-founded testbed within trust models evaluated. now, trust models mainly tested randomly generated scenariosinteractions obey somewhat ad hoc market rules used ART testbed (Fullam,Klos, Muller, Sabater, Topol, Barber, Rosenschein, & Vercouter, 2005). Second, workfirst single-stage interdependent valuations mechanism efficient individually rational (asopposed Mezzettis two-stage mechanism). made achievable settingsconsider capturing interdependence types trust function makingpayments agents contingent actual execution tasks. Another novelty approachable extract (maximum) marginal contribution agent despite valuations interdependent (as shown Section 4.4). Third, implementation GTBMhighlights importance considering computational aspects new mechanism, sincedetermine whether mechanism implementable realistic scenarios indeedbring claimed benefits. work strong statement direction since providecomplete picture problem, starting representation, implementationsample results, complexity analysis.practical terms, GTBM step towards building robust multi-agent systems uncertainenvironments. environments, important aggregate agents preferences,taking account uncertainty order ensure solutions chosen result bestpossible outcome whole system. Prior GTBM, possible comeefficient solution would maximise expected utility. Moreover, fact agentsexpress perception task performers POS new way building expressiveinteractions buyers sellers services (Sandholm, 2007). believeperceptions expressed, better ensuing matching buyers sellersresults proof gain efficiency better matching brings (see sections 3.2.1,3.3.1, 4.3).introducing GTBM new class mechanisms, work lays foundations severalareas inquiry. end, outline main areas below.Budget Balance: important economic property mechanisms contexts budgetbalance.16 However, mentioned Section 3.3.2, designed TBMs withoutconsidering budget balance. fact, GTBM budget balanced similar VCGPorter et al.s mechanism. Now, one possible way overcoming problem sacrificeeither efficiency individual rationality. fact, dAGVA mechanism counterpartVCG indeed sacrifice individual rationality budget balance (see Section2). Moreover, Parkes, Kalagnanam, Eso (2001) develop mechanisms numberbudget balancing schemes proposed near-incentive compatibility attainedmaking payments agents close possible VCG ones.16. mechanism budget balanced, computes transfers allocation overall transfer systemzero (MasColellPet al., 1995). Thus, budget balanced mechanism, allocation K associated transfervector r, ri r ri = 0.35fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSeffective scheme, Threshold rule, results low loss incentive-compatibilityrelatively high efficiency (around 80%). budget balance may usefulsituations centre cannot run risk incurring loss generating efficientoutcome set agents system. example, MoviePictures.com may findworth injecting money system find efficient outcome subunitsnearly equally competitive (both price POS). Instead MoviePictures.com mightprefer mechanism generates near-efficient outcome increasing Bi discussedSection 4.4. this, set agents participate might reducedindividually rational participate mechanism, but, nevertheless,MoviePictures.com may obtain better outcome. future, study trade-offsefficiency achieved system profit made centre.Trust Task Requesters: One potential criticism mechanismstask requesters (and centre) must trusted reveal observed execution task(Mezzetti, 2004). However, setting, task requesters strong incentive revealobservations (in case publicly visible) since would prefer chosentask performer available next time mechanism run. end, mustensure task performer go bankrupt. noted Equations (12) (17),task performer would pay significant amount centre case reportedfail task. Hence, task requester better revealing successful executiontask performer indeed successful.Another issue trust function used weights given agents EQOS reportmay uncertain. Thus, case, agents may learn weights multipleinteractions. Given this, important develop learning search techniquesable deal large number possible weights could used trustfunctions. techniques take account fact agents may losesignificantly exploring search space.Iterative Mechanisms: GTBM one-shot mechanism allocationpayments calculated given type agents {v, c, } using trust models.However, cases participants may engaged repeated interactionsexploited trust models order build accurate trust values counterparts.situations, introduction multiple rounds compromise propertiesmechanism allowing greater range strategies (e.g., cornering market consistently offering low prices initial rounds accepting losses initial rounds providingfalse damaging information competitors). However, explosion strategyspace also implies agents might able compute optimal strategy dueintractability process. Now, one way solving problem constrainstrategies agents myopic (i.e., best response current round) shownParkes Ungar (2000) using proxy bidding. Another allow agents learn trustmodels without participating allocation problem. Then, agents accurate representation trust functions POS values, mechanism implementedone-shot encounter. Note problem arises one-shot mechanismimplemented iterative context solely realm GTBM.36fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONComputational Cost: discussed Section 5, algorithms developed computeefficient allocation run multiple times compute individual paymentsagents TBMs. Hence, time needed compute allocation pay agentsmay impractical agents limited time find solution, put forward largenumber bids, ask large number tasks performed. Hence, importanteither less complex mechanisms described Nisan Ronen (2007)approximate (and computationally less expensive) algorithms developed solveproblems (Archer, Papadimitriou, Talwar, & Tardos, 2003). require workdeveloping local approximation algorithms approximate mechanisms preserveproperties seek. vein, paper provides point departurefuture mechanisms since provides efficient mechanisms approximateones compared.Acknowledgmentsthank anonymous reviewers highly valuable comments; allowed usimprove upon previous version paper, restrictive mechanism,also helped rework proofs. grateful Juuso Valimaki initial commentsmechanism, Ioannis Vetsikas, Enrico Gerding, Archie Chapman checking proofsdiscussing ideas. Juan A. Rodriguez-Aguilar thanks IEA (TIN2006-15662-C02-01), AgreementTechnologies (CONSOLIDER CSD2007-0022, INGENIO 2010) Jose Castillejo programme(JC2008-00337) Spanish Ministry Science Innovation. Andrea Giovannucci fundedJuan De La Cierva Contract (JCI-2008-03006) EU funded Synthetic Forager project(ICT-217148-SF). Claudio Mezzetti thanks Fondazione Cassa di Risparmio di Padova e Rovigosupport. research paper also undertaken part ALADDIN (AutonomousLearning Agents Decentralised Data Information Systems) project jointly fundedBAE Systems EPSRC (Engineering Physical Research Council) strategic partnership(EP/C548051/1).Appendix A. Analysing Performance IP Solutionsection analyse computational performance Integer Programming solutiondetailed Section 5 order gauge sizes problems solved reasonable time.end, important recall (as shown Section 5) number input variablesoptimization problem nearly equal number valuation hyperedges |Ev |, since |Ec | |Ev |.Given this, assume performance solver directly related numberpossible allocations approximated |Ev |.Therefore, test set composed several instances GTBM Task Allocation Problemcharacterised number possible allocations. detail, produce allocations, bidsvaluations generated number bids submitted single bidder numbervaluations submitted single requester follow geometric distribution p parameterset 0.23 (Milton & Arnold, 1998) (in order randomly generate relatively large numbersbids/asks per agent).17 medium-sized problem set follows. number negotiated tasksset 5. number task performers set 15 number task requesters set 20.17. Setting p higher would result fewer bids/asks per agent.37fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSaverage number generated valuations instance 88 average number bids65. Finally, number runs experiments 300. experiments performedXeon dual processor machine 3Ghz CPUs, 2 GB RAM commercial software employedsolve Integer Program ILOG CPLEX 9.1.35Clock time find optimal solution (seconds)30252015105000.20.40.60.811.2No. possible allocations1.41.61.825x 10Figure 2: Performance IP solution.results shown Figure 2. Specifically, x-axis represents number allocationsgiven problem instance y-axis represents time seconds elapsed solvingcorresponding problem instance. Notice dependence difficulty problemnumber allocations quite clear. Moreover, seen, possible solve problemless 2 105 variables within 40 seconds. important note performancesolver used critical case future advancements Mixed Integer Programming (MIP)solvers CPU clock speeds improve results.Given results since provide general formula (see Equation (30)) computepriori number generated allocations, possible estimate feasibility generalproblem performing it. means system designer ask task requestersperformers constrain number tasks ask number bids issue comeinput solved program reasonable time. important,however, design special purpose algorithms deal larger inputs leftfuture work.ReferencesArcher, A., Papadimitriou, C., Talwar, K., & Tardos, E. (2003). approximate truthful mechanismcombinatorial auctions single parameter agent. Internet Mathematics, 1(2), 129150.38fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONArrow, K. J. (1979). property rights doctrine demand revelation incomplete information. Boskin, M. (Ed.), Economics Human Welfare. Academic Press.Berge, C. (1973). Graphs Hypergraphs. North-Holland Publishing Company.Byde, A. (2006). comparison mechanisms sequential compute resource auctions.Proceedings Fifth International Joint Conference Autonomous Agents MultiAgent systems (AAMAS-06), pp. 11991201. ACM Press.Cerquides, J., Endriss, U., Giovannucci, A., & Rodrguez-Aguilar, J. A. (2007). Bidding languageswinner determination mixed multi-unit combinatorial auctions. ProceedingsTwentieth International Joint Conference Artificial Intelligence, pp. 12211226.Dasgupta, P. (1998). Trust commodity. Gambetta, D. (Ed.), Trust: Making BreakingCooperative Relations, pp. 4972. Blackwell.Dash, R. K., Parkes, D. C., & Jennings, N. R. (2003). Computational mechanism design: callarms. IEEE Intelligent Systems, 18(6), 4047.Dash, R. K., Ramchurn, S. D., & Jennings, N. R. (2004). Trust-based mechanism design.Proceedings Third International Joint Conference Autonomous Agents MultiAgent Systems (AAMAS-04), Vol. 2, pp. 726753.dAspremont, C., & Gerard-Varet, L. A. (1979). Incentives incomplete information. JournalPublic Economics, 11(1), 2545.Dellarocas, C. (2002). Goodwill hunting: economically efficient online feedback mechanismenvironments variable product quality. Proceedings (AAMAS-02) WorkshopAgent-Mediated Electronic Commerce, pp. 238252.Engel, Y., Wellman, M. P., & Lochner, K. (2006). Bid expressiveness clearing algorithmsmulti-attribute double auctions. Proceedings Seventh ACM Conference ElectronicCommerce (EC-06), pp. 110119.Fullam, K., Klos, T., Muller, G., Sabater, J., Topol, Z., Barber, K. S., Rosenschein, J., & Vercouter,L. (2005). agent reputation trust (ART) testbed architecture. Proceedings(AAMAS-05) Workshop Trust Agent Societies, pp. 5062.Hershberger, J., & Suri, S. (2001). Vickrey pricing network routing: Fast payment computation.Proceedings Forty-Second IEEE Symposium Foundations Computer Science,pp. 252259.Jehiel, P., & Moldovanu, B. (2001). Efficient design interdependent valuations. Econometrica,69(5), 123759.Jennings, N. R., Faratin, P., Norman, T. J., OBrien, P., Odgers, B., & Alty, J. L. (2000). Implementing business process management system using adept: real-world case study. International Journal Applied Artificial Intelligence, 14(5), 421465.Jurca, R., & Faltings, B. (2003). incentive compatible reputation mechanism. ProceedingsIEEE Conference E-Commerce (CEC-03), pp. 285292.Jurca, R., & Faltings, B. (2006). Minimum payments reward honest reputation feedback.Proceedings Seventh ACM conference Electronic commerce (EC-06), pp. 190199.39fiR AMCHURN , EZZETTI , G IOVANNUCCI , RODRIGUEZ , DASH , J ENNINGSJurca, R., & Faltings, B. (2007). Obtaining reliable feedback sanctioning reputation mechanisms.Journal Artificial Intelligence Research (JAIR), 29, 391419.Kalagnanam, J., Davenport, A. J., & Lee, H. S. (2000). Computational aspects clearing continuous double auctions assignment constraints indivisible demand. Tech. rep., IBMResearch RC21660(97613).Kalagnanam, J., & Parkes, D. C. (2004). Auctions, bidding exchange design. Simchi-Levi,D., Wu, S. D., & Shen, M. (Eds.), Handbook Quantitative Supply Chain Analysis: ModelingE-Business Era, International Series Operations Research Management Science,chap. 5. Kluwer.Krishna, V. (2002). Auction Theory. Academic Press.MasColell, A., Whinston, M., & Green, J. (1995). Microeconomic Theory. Oxford University Press.Mezzetti, C. (2004). Mechanism design interdependent valuations: Efficiency. Econometrica,72(5), 16171626.Mezzetti, C. (2007). Mechanism design interdependent valuations: Surplus extraction. Economic Theory, 31(3), 473488.Miller, N., Resnick, P., & Zeckhauser, R. (2005). Eliciting honest feedback: peer predictionmethod. Management Science, 51(9), 13591373.Milton, J., & Arnold, J. C. (1998). Introduction Probability Statistics. Principles Applications Engineering Computing Sciences. McGraw-Hill Inc.Nisan, N. (2006). Bidding languages combinatorial auctions. Cramton, P., Shoham, Y., &Steinberg, R. (Eds.), Combinatorial Auctions, pp. 215231. MIT Press.Nisan, N., & Ronen, A. (2007). Computationally feasible VCG mechanisms. Journal ArtificialIntelligence Research (JAIR), 29, 1947.Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial optimization: algorithms complexity. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance vickreybased payment schemes exchanges. Proceedings Seventeenth International JointConference Artificial Intelligence (IJCAI-01), pp. 11611168.Parkes, D. C., & Ungar, L. H. (2000). Preventing strategic manipulation iterative auctions: Proxyagents price-adjustment. Proceedings Seventeenth National Conference Artificial Intelligence Twelfth Conference Innovative Applications Artificial Intelligence, pp. 8289.Porter, R., Ronen, A., Shoham, Y., & Tennenholtz, M. (2008). Fault tolerant mechanism design.Artificial Intelligence, 172(15), 17831799.Ramchurn, S. D., Huynh, D., & Jennings, N. R. (2004). Trust multi-agent systems. Knowledge Engineering Review, 19, 125.Sandholm, T. (2007). Expressive commerce application sourcing: conducted 35billion generalized combinatorial auctions. AI Magazine, 28(3), 4558.40fiT RUST-BASED ECHANISMS ROBUST E FFICIENT TASK LLOCATIONSandholm, T., Suri, S., Gilpin, A., & Levine, D. (2002). Winner determination combinatorialauction generalizations. Proceedings First International Joint Conference Autonomous Agents Multi-Agent Systems (AAMAS-02), pp. 6976.Sandholm, T. W. (1993). implementation contract net protocol based marginal costcalculations. Proceedings Twelfth International Workshop Distributed ArtificialIntelligence, pp. 295308.Teacy, W. T. L., Patel, J., Jennings, N. R., & Luck, M. (2006). Travos: Trust reputationcontext inaccurate information sources. Autonomous Agents Multi-Agent Systems,12(2), 183198.Walsh, W., & Wellman, M. (1998). market protocol decentralized task allocation. Proceedings Third International Conference Multi-Agent Systems (ICMAS-98).41fiJournal Artificial Intelligence Research 35 (2009) 235-274Submitted 08/08; published 06/09Bilinear Programming Approach Multiagent PlanningMarek Petrikpetrik@cs.umass.eduShlomo Zilbersteinshlomo@cs.umass.eduDepartment Computer ScienceUniversity Massachusetts, Amherst, 01003, USAAbstractMultiagent planning coordination problems common known computationally hard. show wide range two-agent problems formulatedbilinear programs. present successive approximation algorithm significantlyoutperforms coverage set algorithm, state-of-the-art method classmultiagent problems. algorithm formulated bilinear programs,general simpler implement. new algorithm terminated timeandunlike coverage set algorithmit facilitates derivation useful online performance bound. also much efficient, average reducing computation timeoptimal solution four orders magnitude. Finally, introduce automatic dimensionality reduction method improves effectiveness algorithm,extending applicability new domains providing new way analyze subclassbilinear programs.1. Introductionpresent new approach solving range multiagent planning coordinationproblems using bilinear programming. problems focus represent various extensions Markov decision process (MDP) multiagent settings. success MDPalgorithms planning learning uncertainty motivated researchers extendmodel cooperative multiagent problems. One possibility assumeagents share information underlying state. results multiagentMarkov decision process (Boutilier, 1999), essentially MDP factoredaction set. complex alternative allow partial sharing informationamong agents. settings, several agentseach different partial informationworldmust cooperate order achieve joint objective. problems common practice modeled decentralized partiallyobservable MDPs (DEC-POMDPs) (Bernstein, Zilberstein, & Immerman, 2000). refinements model studied, example making certain independenceassumptions (Becker, Zilberstein, & Lesser, 2003) adding explicit communicationactions (Goldman & Zilberstein, 2008). DEC-POMDPs closely related extensivegames (Rubinstein, 1997). fact, DEC-POMDP represents exponentially largerextensive game common objective. Unfortunately, DEC-POMDPs twoagents intractable general, unlike MDPs solved polynomial time.Despite recent progress solving DEC-POMDPs, even state-of-the-art algorithmsgenerally limited small problems (Seuken & Zilberstein, 2008). motivateddevelopment algorithms either solve restricted class problems (Becker,c2009AI Access Foundation. rights reserved.fiPetrik & ZilbersteinLesser, & Zilberstein, 2004; Kim, Nair, Varakantham, Tambe, & Yokoo, 2006) provideapproximate solutions (Emery-Montemerlo, Gordon, Schneider, & Thrun, 2004; Nair,Roth, Yokoo, & Tambe, 2004; Seuken & Zilberstein, 2007). paper, introduceefficient algorithm several restricted classes, notably decentralized MDPstransition observation independence (Becker et al., 2003). sake simplicity,denote model DEC-MDP, although usually used denote modelwithout independence assumptions. objective problems maximizecumulative reward set cooperative agents finite horizon. agentviewed single decision-maker operating local MDP. complicatesproblem fact MDPs linked common reward functiondepends states.coverage set algorithm (CSA) first optimal algorithm solve efficientlytransition observation independent DEC-MDPs (Becker, Zilberstein, Lesser, & Goldman, 2004). exploiting fact interaction agents limitedcompared individual local problems, CSA solve problems cannot solvedgeneral exact DEC-POMDP algorithms. also exhibits good anytime behavior. However, anytime behavior limited applicability solution qualityknown hindsight, algorithm terminates.develop new approach solve DEC-MDPsas well range multiagentplanning problemsby representing bilinear programs. also present efficientnew algorithm solving kinds separable bilinear problems. algorithmapplied DEC-MDPs, improves efficiency several orders magnitude comparedprevious state-of art algorithms (Becker, 2006; Petrik & Zilberstein, 2007a).addition, algorithm provides useful runtime bounds approximation error,makes useful anytime algorithm. Finally, algorithm formulatedgeneral separable bilinear programs therefore easily applied rangeproblems.rest paper organized follows. First, Section 2, describe basicbilinear program formulation range multiagent planning problemsexpressed within framework. Section 3, describe new successive approximationalgorithm bilinear programs. performance algorithm depends heavilynumber interactions agents. address that, propose Section 4method automatically reduces number interactions provides bounddegradation solution quality. Furthermore, able project computationaleffort required solve given problem instance, develop offline approximation boundsSection 5. Section 6, examine performance approach standardbenchmark problem. conclude summary results discussion futurework could improve performance approach.2. Formulating Multiagent Planning Problems Bilinear Programsbegin formal description bilinear programs different types multiagentplanning problems formulated such. addition multiagent planningproblems, bilinear programs used solve variety problems roboticmanipulation (Pang, Trinkle, & Lo, 1996), bilinear separation (Bennett & Mangasarian,236fiA Bilinear Programming Approach Multiagent Planning1992), even general linear complementarity problems (Mangasarian, 1995). focusmultiagent planning problems formulation turns particularly effective.Definition 1. separable bilinear program normal form defined follows:maximizew,x,y,zf (w, x, y, z) = sT1 w + r1 x + x Cy + r2 + s2 zsubject A1 x + B1 w = b1(1)A2 + B2 z = b2w, x, y, z 0size program total number variables w, x, z. numbervariables determines dimensionality program1 .Unless otherwise specified, vectors column vectors. use boldface 0 1denote vectors zeros ones respectively appropriate dimensions. program specifies two linear programs connected nonlinear objectivefunction term xT Cy. program contains two types variables. first type includesvariables x, appear bilinear term objective function. secondtype includes additional variables w, z appear bilinear term.show later, distinction important complexity algorithm proposedepends mostly dimensionality problem, number variablesinvolved bilinear term.bilinear program Eq. (1) separable constraints x windependent constraints z. is, variables participatebilinear term objective function independently constrained. theory nonseparable bilinear programs much complicated corresponding algorithmsefficient (Horst & Tuy, 1996). Thus, limit discussion paperseparable bilinear programs often omit term separable. discussed laterdetail, separable bilinear program may seen concave minimization problemmultiple local minima. shown solving problem NP-complete,compared polynomial time complexity linear programs.addition formulation bilinear program shown Eq. (1), also usefollowing formulation, stated terms inequalities:maximizex,yxT Cysubject A1 x b1x0A2 b2y0(2)latter formulation easily transformed normal form using standardtransformations linear programs (Vanderbei, 2001). particular, introduce slack1. possible define dimensionality terms x, minimum dimensions x y.issue discussed Appendix B.237fiPetrik & Zilbersteinvariables w, z obtain following identical bilinear program normal form:xT Cymaximizew,x,y,zsubject A1 x w = b1A2 z = b2(3)w, x, y, z 0use following matrix block matrix notation paper. Matricesdenoted square brackets, columns separated commas rows separatedsemicolons. Columns haveprecedencerows. example, notation [A, B; C, D]corresponds matrixCB.show later, presence variables w, z objective function may preventcrucial function convex. Since unfavorable impact propertiesbilinear program, introduce compact form problem.Definition 2. say bilinear program Eq. (1) compact form s1s2 zero vectors. semi-compact form s2 zero vector.compactness requirement limiting bilinear program formshown Eq. (1) expressed semi-compact form follows:maximizew,x,y,z,x,ysT1w+r1T x+xTsubject A1 x + B1 w = b1x = 1 =C 0x+ r2T0 1A2 + B2 z = b2(4)sT2zw, x, y, z 0Clearly, feasible solutions Eq. (1) Eq. (4) objective value setappropriately. Notice dimensionality bilinear term objective functionincreases 1 x y. Hence, transformation increases dimensionalityprogram 1.rest section describes several classes multiagent planning problemsformulated bilinear programs. Starting observation transition independentDEC-MDPs, extend formulation allow different objective function (maximizingaverage reward infinite horizon), handle interdependent observations, findNash equilibria competitive settings.2.1 DEC-MDPsmentioned previously, transition-independent observation-independent DECMDP (Becker et al., 2004) may formulated bilinear program. Intuitively, DECMDP transition independent agent influence agents transitions.DEC-MDP observation independent agent observe states agents.assumptions crucial since ensure lower complexity problem (Becker238fiA Bilinear Programming Approach Multiagent Planninget al., 2004). remainder paper, use simply term DEC-MDP refertransition observation independent DEC-MDP.DEC-MDP model proved useful several multiagent planning domains. Oneexample use Mars rover planning problem (Bresina, Golden, Smith, & Washington, 1999), first formulated DEC-MDP Becker et al. (2003). domaininvolves two autonomous rovers visit several sites given order may decideperform certain scientific experiments site. overall activity must completedwithin given time limit. uncertainty duration experiment modeled given discrete distribution. rovers operate independently receivelocal rewards completed experiment, global reward function also dependsexperiments completed rovers. interaction rovers thuslimited relatively small number overlapping tasks. return problemdescribe detail Section 6.DEC-MDP problem composed two MDPs state-sets S1 , S2 action setsA1 , A2 . functions r1 r2 define local rewards action-state pairs. initialstate distributions 1 2 . MDPs coupled global reward functiondefined matrix R. entry R(i, j) represents joint reward state-actionone agent j other. definition DEC-MDP based workBecker et al. (2004), modifications discuss below.Definition 3. two-agent transition observation independent DEC-MDP extended reward structure defined tuple hS, F, , A, P, Ri:= (S1 , S2 ) factored set world statesF = (F1 S1 , F2 S2 ) factored set terminal states.= (1 , 2 ) : Si 7 [0, 1] initial state distribution functions= (A1 , A2 ) factored set actionsP = (P1 , P2 ), Pi : Si Ai Si 7 [0, 1] transition functions. Let Aiaction, Pia : Si Si 7 [0, 1] stochastic transition matrixPi (s, a, s0 ) = Pia (s, s0 ) probability transition state Si states0 Si agent i, assuming takes action a. transitions final states0 probability; Pi (s, a, s0 ) = 0 Fi , s0 Si , Ai .R = (r1 , r2 , R) ri : Si Ai 7 R local reward functionsR : (S1 A1 ) (S2 A2 ) 7 R global reward function. Local rewards rirepresented vectors, R matrix (s1 , a1 ) rows (s2 , a2 ) columns.Definition 3 differs original definition transition observation independent DEC-MDP (Becker et al. 2004, Definition 1) two ways. modifications allowus explicitly capture assumptions implicit previous work. First, individual MDPs model formulated stochastic shortest-path problems (Bertsekas &Tsitsiklis, 1996). is, explicit time horizon, instead statesterminal. process stops upon reaching terminal state. objective maximizecumulative reward received reaching terminal states.second modification original definition Definition 3 generalizesreward structure DEC-MDP formulation, using extended reward structure.joint rewards original DEC-MDP defined joint states (s1 S1 , s2 S2 )239fiPetrik & Zilbersteins11s21s12s22s13s23s31t1s1s2s3t2s32t3s33Figure 1: MDP stochastic shortest path version time horizon 3. dottedcircles terminal states.visited agents simultaneously. is, agent 1 visits states s11 , s12 agent 2visits states s21 , s22 , reward defined joint states (s11 , s21 ) (s12 , s22 ).However, DEC-MDP formulation extended reward structure also allows rewarddepend (s11 , s22 ) (s12 , s21 ), even visited simultaneously. result,global reward may depend history, current state. Notereward structure general commonly used DEC-POMDPs.prefer general definition already implicitly usedprevious work. particular, extended reward structure arises introducingprimitive compound events work Becker et al. (2004). reward structurenecessary capture characteristics Mars rover benchmark. Interestingly,extension complicate proposed solution methods way. Notestochastic shortest path formulation (right side Figure 1) inherently eliminatesloops time always advances action taken. Therefore, every staterepresentation may visited once. property commonly used MDPformulated linear program (Puterman, 2005).solution DEC-MDP deterministic stationary policy = (1 , 2 ),: Si 7 Ai standard MDP policy (Puterman, 2005) agent i. particular, (si )represents action taken agent state si . define bilinear program, usevariables x(s1 , a1 ) denote probability agent 1 visits state s1 takes action a1y(s2 , a2 ) denote agent 2. standard dual variables MDPformulation. Given solution terms x agent 1, policy calculated S1follows, breaking ties arbitrarily.1 (s) = arg max x(s, a)aA1policy 2 similarly calculated y. correctness policy calculation followsexistence optimal policy deterministic depends localstates agent (Becker et al., 2004).objective DEC-MDPs terms x maximize:XXX Xr1 (s1 , a1 )x(s1 , a1 ) +R(s1 , a1 , s2 , a2 )x(s1 , a1 )y(s2 , a2 ) +r2 (s2 , a2 )y(s2 , a2 ).s1 S1a1 A1s1 S1 s2 S2a1 A1 a2 A2s2 S2a2 A2stochastic shortest path representation general finite-horizon MDPrepresented keeping track time part state, illustrated240fiA Bilinear Programming Approach Multiagent Plannings11s21s12r1s13s22s14s23s24s25Figure 2: sample DEC-MDP.Figure 1. modification allows us apply model directly Mars roverbenchmark problem. Actions Mars rover problem may different durations,actions finite-horizon MDPs take amount time.DEC-MDP problem extended reward structure formulated bilinear mathematical program follows. Vector variables x represent state-actionprobabilities agent, used dual linear formulation MDPs. Given transition observation independence, feasible regions may defined linear equalitiesA1 x = 1 x 0, A2 = 2 0. matrices A1 A2dual formulation total expected reward MDPs (Puterman, 2005), representingfollowing equalities agent i:Xa0 AiX Xx(s0 , a0 )Pi (s, a, s0 )x(s, a) = (s0 ),sSi aAievery s0 Si . described above, variables x(s, a) represent probabilities visitingstate taking action appropriate agent plan execution. Noteagent 2, variables y(s, a) rather x(s, a). Intuitively, equalities ensureprobability entering non-terminal state, either initial stepstates, probability leaving state. bilinear problemformulated follows:maximizex,yr1T x + xT Ry + r2Tsubject A1 x = 1x0A2 = 2y0(5)formulation, treat initial state distributions vectors, based fixedordering states. following simple example illustrates formulation.Example 4. Consider DEC-MDP two agents, depicted Figure 2. transitionsproblem deterministic, thus branches represent actions ai , orderedstate left right. states, one action available. shared rewardr1 , denoted dotted line, received agents visit state. local rewardsdenoted numbers next states. terminal states omitted.241fiPetrik & Zilbersteinagents start states s11 s21 respectively. bilinear formulation problem is:maximizesubjectx(s14 , a1 ) r1 y(s24 , a1 )x(s11 , a1 )11x(s2 , a1 ) + x(s2 , a2 ) x(s11 , a1 )x(s13 , a1 ) x(s12 , a1 )x(s14 , a1 ) x(s12 , a2 )=1=0=0=0y(s21 , a1 ) + y(s21 , a2 )y(s22 , a1 ) y(s21 , a1 )y(s23 , a1 ) y(s22 , a1 )y(s24 , a1 ) y(s22 , a1 )y(s25 , a1 ) y(s23 , a1 )=1=0=0=0=0results paper focus two-agent problems, approach extended DEC-MDPs two agents two ways. first approach requirescomponent global reward depends two agents. DEC-MDPmay viewed graph vertices representing agents edges representingimmediate interactions dependencies. formulate problem bilinear program, graph must bipartite. Interestingly, class problems previouslyformulated (Kim et al., 2006). Let G1 G2 indices agents twopartitions bipartite graph. problem formulated follows:XmaximizeriT xi + xTRij yj + rj yjx,yiG1 ,jG2subject Ai xi = 1x 0 G1Aj yj = 2yj 0 j G2(6)Here, Rij denotes global reward interactions agents j. program bilinear separable constraints variables G1 G2independent.second approach generalize framework represent DEC-MDPmultilinear program. case, restrictions reward structure necessary.algorithm solve, say trilinear program, could almost identical algorithmpropose, except best response would calculated using bilinear, linearprograms. However, scalability approach agents doubtful.2.2 Average-Reward Infinite-Horizon DEC-MDPsprevious formulation deals finite-horizon DEC-MDPs. average-reward problem may also formulated bilinear program (Petrik & Zilberstein, 2007b).particularly useful infinite-horizon DEC-MDPs. example, consider infinitehorizon version Multiple Access Broadcast Channel (MABC) (Rosberg, 1983; Ooi &Wornell, 1996). problem, used widely recent studies decentralized decision making, two communication devices share single channel, needperiodically transmit data. However, channel transmit single messagetime. agents send messages time, leads collision,transmission fails. memory devices limited, thus need sendmessages sooner rather later. adapt model work Rosberg (1983),particularly suitable assumes sharing local information amongdevices.242fiA Bilinear Programming Approach Multiagent Planningdefinition average-reward two-agent transition observation independent DECMDP Definition 3, exception terminal states, policy, objective. terminal states average-reward DEC-MDPs, policy (1 , 2 )may stochastic. is, (s, a) 7 [0, 1] probability agent taking actionstate s. objective find stationary infinite-horizon policy maximizesaverage reward, gain, defined follows.Definition 5. Let = (1 , 2 ) stochastic policy, Xt Yt random variablesrepresent probability distributions state-action pairs time twoagents respectively according . gain G policy defined statess1 S1 s2 S2 as:"N 1#X1G(s , ) = limE(s1 ,1 (s1 )),(s2 ,2 (s2 ))r1 (Xt ) + R(Xt , Yt ) + r2 (Yt ) ,N N12t=0(si ) distribution actions state si . Note expectationrespect initial states action distributions (s1 , 1 (s1 )), (s2 , 2 (s2 )).actual gain policy depends agents initial state distributions 1 , 2may expressed 1T G2 , G represented matrix. Puterman (2005),example, provides detailed discussion definition meaning policy gain.simplify bilinear formulation average-reward DEC-MDP, assumer1 = 0 r2 = 0. bilinear program follows.maximizep1 ,p2 ,q1 ,q2(p1 , p2 , q1 , q2 ) = pT1 Rp2subject pX1 , p2 0s0 S1p1 (s0 , a)s0 S1s0 S2s0 S2aAX1aAX1aAX2aA2p1 (s0 , a) +p2 (s0 , a)p2 (s0 , a) +Xp1 (s, a)P1a s, s0 = 0sSX1 ,aA1q1 (s0 , a)aAX1Xq1 (s, a)P1a s, s0 = 1 (s0 )sS1 ,aA1p2 (s, a)P2a s, s0sSX2 ,aA2q2 (s0 , a)aA2X(7)=0q2 (s, a)P2a s, s0 = 2 (s0 )sS2 ,aA2variables program come dual formulation average-reward MDPlinear program (Puterman, 2005). state sets MDPs divided recurrenttransient states. recurrent states expected visited infinitely many times,transient states expected visited finitely many times. Variables p1p2 represent limiting distributions MDP, non-zero recurrentstates. (possibly stochastic) policy agent defined recurrent statesprobability taking action Ai state Si :pi (s, a).0a0 Ai pi (s, )(s, a) = P243fiPetrik & Zilbersteina11a12b12b11b11b12a21a22a21a22a23a24a23a24r1r2r3r4r5r1r7r8Figure 3: tree form policy DEC-POMDP extensive game. dottedellipses denote information sets.variables pi 0 transient states. policy transient states calculatedvariables qi as:qi (s, a)(s, a) = P.0a0 Ai qi (s, )correctness constraints follows dual formulation optimal averagereward (Puterman 2005, Equation 9.3.4). Petrik Zilberstein (2007b) providedetails formulation.2.3 General DEC-POMDPs Extensive Gamesgeneral DEC-POMDP problem extensive-form games two agents, players,also formulated bilinear programs. However, constraints may separableactions one agent influence agent. approach case maysimilar linear complementarity problem formulation extensive games (Koller, Megiddo,& von Stengel, 1994), integer linear program formulation DEC-POMDPs (Aras& Charpillet, 2007). approach develop closely related event-driven DECPOMDPs (Becker et al., 2004), general efficient. Nevertheless, sizebilinear program exponential size DEC-POMDP. expectedsince solving DEC-POMDPs NEXP-complete (Bernstein et al., 2000), solvingbilinear programs NP-complete (Mangasarian, 1995). general formulationcase somewhat cumbersome, illustrate using following simple example.Aras (2008) provides details similar construction.Example 6. Consider problem depicted Figure 3, assuming agentscooperative. actions agent observable, denoted informationsets. approach generalized problem observable sets longperfect recall condition satisfied. Agents satisfy perfect recall conditionremember set actions taken prior moves (Osborne & Rubinstein, 1994).Rewards collected leaf-nodes case. variables edges representprobability taking action. Here, variables denote actions one agent,244fiA Bilinear Programming Approach Multiagent Planningvariables b other. total common reward received end is:r = a11 b11 a21 r1 + a11 b11 a22 r2 + a11 b12 a21 r3 + a11 b12 a22 r4 +a12 b11 a23 r5 + a12 b11 a24 r6 + a12 b12 a23 r7 + a12 b12 a24 r8 .constraints problem following form: a11 + a12 = 1.DEC-POMDP problem represented using approach used above.also straightforward extend approach problems rewards every node. However, formulation clearly bilinear. apply algorithm classproblems, need reformulate problem bilinear form. easilyaccomplished way similar construction dual linear program MDP.Namely, introduce variables:c11 = a11c12 = a12c21 = a11 a21c22 = a11 a22every set variables path leaf node. Then, objective mayreformulated follows:r = c21 b11 r1 + c22 b11 r2 + c23 b12 r3 + c24 b12 r4 +c25 b11 r5 + c26 b11 r6 + c27 b12 r7 + c28 b12 r8 .Variables bij replaced fashion. objective function clearly bilinear.constraints may reformulated follows. constraint a21 + a22 = 1multiplied a11 replaced c21 + c22 = c11 , on. is, variableslevel sum variable least common parent levelagent.2.4 General Two-Player Gamesaddition cooperative problems, competitive problems 2 players mayformulated bilinear programs. known problem finding equilibriumbi-matrix game may formulated linear complementarity problem (Cottle, Pang,& Stone, 1992). also shown linear complementarity problem mayformulated bilinear problem (Mangasarian, 1995). However, direct applicationtwo reductions results complex problem large dimensionality. Below,demonstrate general game directly formulated bilinear program.many ways formulate game, thus take general approach. simplyassume agent optimizes linear program, follows.maximizexmaximized1 (x) = r1T x + xT C1subject A1 x = b1(8)d2 (y) = r2T + xT C2subject A2 = b2y0x0245(9)fiPetrik & ZilbersteinEq. (8), variable considered constant similarly Eq. (9)variable x considered constant. normal form games, constraint matricesA1 A2 simply rows ones, b1 = b2 = 1. competitive DEC-MDPs,constraint matrices A1 A2 Section 2.1. Extensive games mayformulated similarly DEC-POMDPs, described Section 2.3.game specified linear programs Eq. (8) Eq. (9) may formulatedbilinear program follows. First, define reward vectors agent, given policyagent.q1 (y) = r1 + C1q2 (x) = r2 + C2T x.values unrelated Eq. (7). complementary slackness values (Vanderbei, 2001) linear programs Eq. (8) Eq. (9) are:k1 (x, y, 1 ) = q1 (y)T1 A1 xk2 (x, y, 2 ) = q2 (x)T2 2 y,1 2 dual variables corresponding linear programs.primal feasible x y, dual feasible 1 2 , k1 (x, y, 1 ) 0k2 (x, y, 2 ) 0. equality attained x optimal.used write following optimization problem, implicitly assumex,y,1 ,2 feasible appropriate primal dual linear programs:0====min k1 (x, y, 1 ) + k2 (x, y, 2 )x,y,1 ,2min (q1 (y)T1 A1 )x + (q2 (x) 2 A2 )yx,y,1 ,2min ((r1 + C1 y)T1 A1 )x + ((r2 + C2 x) 2 A2 )yx,y,1 ,2min r1T x + r2T + xT (C1 + C2 )y xT1 1 A2 2x,y,1 ,2min r1T x + r2T + xT (C1 + C2 )y bT1 1 b2 2 .x,y,1 ,2Therefore, feasible x set right hand side 0 solve linear programsEq. (8) Eq. (9) optimally. Adding primal dual feasibility conditionsabove, get following bilinear program:minimizex,y,1 ,2r1T x + r2T + xT (C1 + C2 )y bT1 1 b2 2subject A1 x = b1A2 = b2r1 + C11 1 0r2 + C2T x2 2 0x0y0246(10)fiA Bilinear Programming Approach Multiagent PlanningAlgorithm 1: IterativeBestResponse(B)x0 , w0 rand ;i1;yi1 6= yi xi1 6= xi(yi , zi ) arg maxy,z f (wi1 , xi1 , y, z) ;(xi , wi ) arg maxx,w f (w, x, yi , zi ) ;6ii+1123457return f (wi , xi , yi , zi )optimal solution Eq. (10) 0 corresponds Nash equilibrium.primal variables x, dual variables 1 , 2 feasiblecomplementary slackness condition satisfied. open question exampleinterpretation approximate result formulation would select equilibrium.clear yet whether possible formulate program optimal solutionNash equilibrium maximizes certain criterion. approximate solutionsprogram probably correspond -Nash equilibria, remain open question.algorithm case also relies number shared rewards smallcompared size problem. even case, often possiblenumber shared rewards may automatically reduced described Section 4.fact, easy show zero-sum normal form game automatically reducedtwo uncoupled linear programs. follows dimensionality reduction procedureSection 4.3. Solving Bilinear ProgramsOne simple method often used solving bilinear programs iterative procedure shownAlgorithm 1. parameter B represents bilinear program. algorithmoften performs well practice, tends converge suboptimal solution (Mangasarian,1995). applied DEC-MDPs, algorithm essentially identical JESP (Nair,Tambe, Yokoo, Pynadath, & Marsella, 2003)one early solution methods.following, use f (w, x, y, z) denote objective value Eq. (1).rest section presents new anytime algorithm solving bilinear programs.goal algorithm produce good solution quickly improvesolution remaining time. Along approximate solution, maximal approximation bound respect optimal solution provided. show below,algorithm benefit results produced suboptimal algorithms, Algorithm 1,quickly determine tight approximation bounds.3.1 Successive Approximation Algorithmbegin overview successive approximation algorithm bilinear problemstakes advantage low number interactions agents. particularlysuitable input problem large comparison dimensionality, definedSection 2. address issue dimensionality reduction Section 4.247fiPetrik & Zilbersteinbegin simple intuitive explanation algorithm, showformalized. bilinear program seen optimization game playedtwo agents, first agent sets variables w, x second one setsvariables y, z. general observation applies bilinear program.practical application, feasible sets two sets variables may largeexplore exhaustively. fact, method applied DEC-MDPs, setsinfinite continuous. basic idea algorithm first identify set bestresponses one agents, say agent 1, policy agent.simple variables agent 2 fixed, program becomes linear,relatively easy solve. set best-response policies agent 1 identified,assuming reasonable size, possible calculate best response agent 2.general approach also used coverage set algorithm (Becker et al., 2004).One distinction representation used CSA applies DEC-MDPs,formulation applies bilinear programsa general representation. maindistinction algorithm CSA way variables y, z chosen.CSA, values y, z calculated way simply guarantees terminationfinite time. We, hand, choose values y, z greedily minimizeapproximation bound optimal solution. possible establish boundsoptimality solution throughout calculation. result, algorithmconverges rapidly may terminated time guaranteed performancebound. Unlike earlier version algorithm (Petrik & Zilberstein, 2007a), versiondescribed paper calculates best response using subset values y, z.show, possible identify regions y, z impossible improvecurrent best solution exclude regions consideration.formalize ideas described above. simplify notation, define feasiblesets follows:X = {(x, w) A1 x + B1 w = b1 }= {(y, z) A2 + B2 z = b2 }.use denote exists z (y, z) . addition, assumeproblem semi-compact form. reasonable bilinear programmay converted semi-compact form increase dimensionality one,shown earlier.Assumption 7. sets X bounded, is, contained ballfinite radius.Assumption 7 limiting, coordination problems uncertainty typicallybounded feasible sets variables correspond probabilities bounded [0, 1].Assumption 8. bilinear program semi-compact form.main idea algorithm compute set X X containselements satisfy necessary optimality condition. set X formally definedfollows:X (x , w ) (y, z) f (w , x , y, z) = max f (w, x, y, z) .(x,w)X248fiA Bilinear Programming Approach Multiagent Planningdescribed above, set may seen set best responses one agentvariable settings other. best responses easy calculate since bilinearprogram Eq. (1) reduces linear program fixed w, x fixed y, z. algorithm,assume X potentially proper subset necessary optimality pointsfocus approximation error optimal solution. Given set X, followingsimplified problem solved.maximizew,x,y,zf (w, x, y, z)subject (x, w) X(11)A2 + B2 z = b2y, z 0Unlike original continuous set X, reduced set X discrete small. Thuselements X may enumerated. fixed w x, bilinear program Eq. (11)reduces linear program.help compute approximation bound guide selection elementsX, use best-response function g(y), defined follows:g(y) =max{w,x,z (x,w)X,(y,z)Y }f (w, x, y, z) =max{x,w (x,w)X}f (w, x, y, 0),second equality semi-compact programs feasible . Noteg(y) also defined/ , case choice z arbitrary sinceinfluence objective function. best-response function easy calculate usinglinear program. crucial property function g use calculateapproximation bound convexity. following proposition holds g(y) =max{x,w (x,w)X} f (w, x, y, 0) maximum finite set linear functions.Proposition 9. function g(y) convex program semi-compact form.Proposition 9 relies heavily separability Eq. (1), means constraints variables one side bilinear term independent variablesside. separability ensures w, x valid solutions regardless valuesy, z. semi-compactness program necessary establish convexity, shownExample 23 Appendix C. example constructed using properties describedappendix, show f (w, x, y, z) may expressed sum convexconcave function.ready describe Algorithm 2, computes set X bilinearproblem B approximation error 0 . algorithm iteratively addsbest response (x, w) selected pivot point X. pivot points selectedhierarchically. iteration j, algorithm keeps set polyhedra S1 . . . Sjrepresent triangulation feasible space , possible based Assumption 7.polyhedron Si = (y1 . . . yn+1 ), algorithm keeps bound maximaldifference optimal solution polyhedron best solution foundfar. error bound polyhedron Si defined as:= e(Si ) =max{w,x,y|(x,w)X,ySi }f (w, x, y, 0)max{w,x,y|(x,w)X,ySi }249f (w, x, y, 0),fiPetrik & ZilbersteinAlgorithm 2: BestResponseApprox(B, 0 ) returns (w, x, y, z)1234567891011121314// Create initial polyhedron S1 .S1 (y1 . . . yn+1 ), S1 ;// Add best-responses vertices S1 XX {arg max(x,w)X f (w, x, y1 , 0), ..., arg max(x,w)X f (w, x, yn+1 , 0)} ;// Calculate error pivot point initial polyhedron(1 , 1 ) P olyhedronError(S1 ) ;// Section 3.2,Section 3.3// Initialize number polyhedra 1j1;// Continue reaching predefined precision 0maxi=1,...,j 0// Find polyhedron largest errorarg maxk=1,...,j k ;// Select pivot point polyhedron largest error;// Add best response pivot point set XX X {arg max(x,w)X f (w, x, y, 0)} ;// Calculate errors pivot points refined polyhedrak = 1, . . . , n + 1j j+1 ;// Replace k-th vertex pivot pointSj (y, y1 . . . yk1 , yk+1 , . . . yn+1 ) ;(j , j ) P olyhedronError(Sj ) ;// Section 3.2,Section 3.3// Take smaller errors original refinedpolyhedron. error may increase refinement,although bound mayj min{i , j } ;// Set error refined polyhedron 0, since regioncovered refinements0 ;(w, x, y, z) arg max{w,x,y,z16 return (w, x, y, z) ;15(x,w)X,(y,z)Y }f (w, x, y, 0) ;X represents current, final, set best responses.Next, point y0 selected described n + 1 new polyhedra createdreplacing one vertices y0 get: (y0 , y2 , . . .), (y1 , y0 , y3 , . . .), . . . , (y1 , . . . , yn , y0 ).depicted 2-dimensional set Figure 4. old polyhedron discardedprocedure repeatedly applied polyhedron maximalapproximation error.sake clarity, pseudo-code Algorithm 2 simplified addressefficiency issues. practice, g(yi ) could cached, errors could storedprioritized heap least sorted array. addition, lower bound li upperbound ui calculated stored polyhedron Si = (y1 . . . yn+1 ). function e(Si )calculates maximal difference polyhedron Si point attained.error bound polyhedron Si may tight, describe Remark 13.result, polyhedron Si refined n polyhedra S10 . . . Sn0 online error250fiA Bilinear Programming Approach Multiagent Planningy3y0y1y2Figure 4: Refinement polyhedron two dimensions pivot y0 .bounds 01 . . . 0n , possible k: 0k > . Since S10 . . . Sn0 Si , true errorSk0 less Si therefore 0k may set .Conceptually, algorithm similar CSA, important differences.main difference choice pivot point y0 bounds g. CSAkeep upper bound evaluates g(y) intersection points planes definedcurrent solutions X. guarantees g(y) eventually known precisely(Becker et al., 2004). similar approach also taken POMDPs (Cheng, 1988).|X|upper bound number intersection points CSA dim. principal problembound exponential dimension , experiments showslower growth typical problems. contrast, choose pivot points minimizeapproximation error. selective tends rapidly reduce errorbound. addition, error pivot point may used determine overallerror bound. following proposition states soundness triangulation, provedAppendix A. correctness triangulation establishes iterationapproximation error equivalent maximum approximation errorscurrent polyhedra S1 . . . Sj .Proposition 10. proposed triangulation, sub-polyhedra overlapcover whole feasible set , given pivot point interior S.3.2 Online Error Boundselection pivot point plays key role performance algorithm,calculating error bound speed convergence optimal solution.section show exactly use triangulation algorithm calculate errorbound. compute approximation bound, define approximate best-responsefunction g(y) as:g(y) =maxf (w, x, y, 0).{x,w (x,w)X}Notice z considered expression, since assume bilinear program semi-compact form. value best approximate solutionexecution algorithm is:maxf (w, x, y, 0) = max g(y).yY{w,x,y,z (x,w)X,yY }251fiPetrik & Zilbersteinvalue calculated runtime new element X added.maximal approximation error current solution optimal one maycalculated approximation error best-response function g(), statedfollowing proposition.Proposition 11. Consider bilinear program semi-compact form. let w, x,optimal solution Eq. (11) let w , x , optimal solution Eq. (1).approximation error bounded by:f (w , x , , 0) f (w, x, y, 0) max (g(y) g(y)) .yYProof.f (w , x , , 0) f (w, x, y, 0) = max g(y) max g(y) max g(y) g(y)yYyYyYNow, approximation error maxyY g(y) g(y), bounded differenceupper bound lower bound g(y). Clearly, g(y) lower boundg(y). Given points g(y) best-response function g(y), useJensens inequality obtain upper bound. summarized following lemma.Lemma12.Let yi = 1, . . . , n + 1 g(yi ) = g(yi ).PPn+1Pn+1n+1gi=1 ci yii=1 ci g(yi )i=1 ci = 1 ci 0 i.actual implementation bound relies choice pivot points. Nextdescribe maximal error calculation single polyhedron defined = (y1 . . . yn ).Let matrix yi columns, let L = {x1 . . . xn+1 } set best responsesvertices. matrix used convert absolute coordinates relativerepresentation convex combination vertices. defined formallyfollows:...= = y1 y2 . . ....1 = 1T0yi column vectors.represent lower bound l(y) g(y) upper bound u(y) g(y) as:l(y) = max rT x + xT CyxLu(y) = [g(y1 ), g(y2 ), . . .]T = [g(y1 ), g(y2 ), . . .]T1T1,1upper bound correctness follows Lemma 12. Notice u(y) linear function,enables us use linear program determine maximal-error point.252fiA Bilinear Programming Approach Multiagent PlanningAlgorithm 3: PolyhedronError(B, S)P one Eq. (12), (13), (14), (20) ;optimal solution P ;optimal objective value P ;// Coordinates relative vertices S, convert absolutevalues4 Tt ;5 return (, ) ;123Remark 13. Notice use L instead X calculating l(y). Using X wouldlead tighter bound, easy show three-dimensional examples. However,also would substantially increase computational complexity.Now, error polyhedron may expressed as:e(S) max u(y) l(y) = max u(y) max rT x + xT CyySySxL= max min u(y) r x x Cy.yS xLalso= 0 1T = 1 .result, point maximal error bound may determined using followinglinear program terms variables t, :maximizet,subject u(T t) rT x xT CTx L(12)1T = 1 0x variable. formulation correct feasible solutionsbounded maximal error maximal-error solution feasible.Proposition 14. optimal solution Eq. (12) equivalent maxyS |u(y) l(y)|.thus select next pivot point greedily minimize error. maximal difference actually achieved points planes meet, Becker et al. (2004)suggested. However, checking intersections similar running simplexalgorithm. general, simplex algorithm preferable interior point methodsprogram small size (Vanderbei, 2001).Algorithm 3 shows general way calculate maximal error pivot pointpolyhedron S. algorithm may use basic formulation Eq. (12),advanced formulations Eqs. (13), (14), (20) defined Section 3.3.following section, describe refined pivot point selection methodcases dramatically improve performance.253fiPetrik & Zilberstein2015h1056Yh42024Yh6Figure 5: reduced set Yh needs considered pivot point selection.3.3 Advanced Pivot Point Selectiondescribed above, pivot points chosen greedily determine maximalerror polyhedron minimize approximation error. basic approachdescribed Section 3.1 may refined, goal approximate functiong(y) least error, find optimal solution. Intuitively, ignoreregions guarantee improvement current solution, illustratedFigure 5. show below, search maximal error point could limitedregion well.first define set Yh search maximal error, givenoptimal solution f h.Yh = {y g(y) h, }.next proposition states maximal error needs calculated supersetYh .Proposition 15. Let w, x, y, z approximate optimal solution w , x , , zoptimal solution. Also let f (w , x , , z ) h assume Yh Yh . approximation error bounded by:f (w , x , , z ) f (w, x, y, z) max g(y) g(y).yYhProof. First, f (w , x , , z ) = g(y ) h thus Yh . Then:f (w , x , , z ) f (w, x, y, z) = max g(y) max g(y)yYhyYmax g(y) g(y)yYhmax g(y) g(y)yYhProposition 15 indicates point maximal error needs selectedset Yh . question easily identify Yh . set convexgeneral, tight approximation set needs found. particular, use methods254fiA Bilinear Programming Approach Multiagent Planningapproximate intersection superset Yh polyhedronrefined, using following methods:1. Feasibility [Eq. (13)]: Require pivot points feasible .2. Linear bound [Eq. (14)]: Use linear upper bound u(y) h.3. Cutting plane [Eq. (20)]: Use linear inequalities define YhC ,YhC = R|Y | \ Yh complement Yh .combination methods also possible.Feasibility first method simplest, also least constraining. linearprogram find pivot point maximal error bound follows:maximize,t,y,zsubject u(T t) rT x + xT CTx L1T = 1 0(13)= TtA2 + B2 z = b2y, z 0approach require bilinear program semi-compact form.Linear Bound second method, using linear bound, also simple implement compute, selective requiring feasibility. Let:Yh = {y u(y) h} {y g(y) h} = Yh .set convex thus need approximated. linear program usedfind pivot point maximal error bound follows:maximize,tsubject u(T t) rT x + xT CTx L1T = 1 0(14)u(T t) hdifference Eq. (12) last constraint. approach requires bilinearprogram semi-compact form ensure u(y) bound total return.Cutting Plane third method, using cutting plane elimination, computationally intensive one, also selective one. Using approach requiresadditional assumptions parts algorithm, discuss below.method based principle -extensions concave cuts (Horst & Tuy, 1996).start set YhC convex may expressed as:max sTw+rx+Cx+rh(15)112w,x255A1 x + B1 w = b1(16)w, x 0(17)fiPetrik & Zilbersteiny3f1y1y2f2YhFigure 6: Approximating Yh using cutting plane elimination method.use inequalities selecting pivot point, need make linear.two obstacles: Eq. (15) contains bilinear term maximization.issues addressed using dual formulation Eq. (15). corresponding linear program dual fixed y, ignoring constants h r2T y, are:maximizew,xsT1 w + r1 x + C xsubject A1 x + B1 w = b1minimize(18)bT1subject1 r1 + Cyw, x 0(19)B1T s1Using dual formulation, Eq. (15) becomes:min b1 + r2h1 r1 + CyB1T s1Now, use function value following holds:min (x) (x) (x) .xFinally, leads following set inequalities.r2T h bT1Cy1 r1s1 B1Tinequalities define convex set YhC . complement Yhnecessarily convex, need use convex superset Yh given polyhedron.done projecting YhC , subset, onto edges polyhedron depictedFigure 6 described Algorithm 4. algorithm returns single constraintcuts part set YhC . Notice combination first n points fk256fiA Bilinear Programming Approach Multiagent PlanningAlgorithm 4: PolyhedronCut({y1 , . . . , yn+1 }, h) returns constraint123456789// Find vertices polyhedron {y1 , . . . , yn+1 } inside YhC{yi yi YhC } ;// Find vertices polyhedron outside YhC{yi yi Yh } ;// Find least n points fk edge Yh intersects edgepolyhedronk1;jfk yj + max { (yi yj ) (YhC )} ;k k+1 ;k nbreak ;Find , [f1 , . . . , fn ] = 1T = 1 ;// Determine correct orientation constraint Yhfeasible11 yj O, yj >// Reverse constraint points wrong way12;13;1014returnused. general, may n points, subset points fk size nused define new cutting plane constraints Yh . lead significantimprovements experiments. linear program find pivot pointcutting plane option follows:maximize,t,ysubject u(T t) rT x + xT CT1T = 1 0x L(20)= TtTyHere, , obtained result running Algorithm 4.Note approach requires bilinear program semi-compact formensure g(y) convex. following proposition states correctnessprocedure.Proposition 16. resulting polyhedron produced Algorithm 4 supersetintersection polyhedron complement Yh .Proof. convexity g(y) implies YhC also convex. Therefore, intersectionQ = {y }257fiPetrik & Zilbersteinalso convex. also convex hull points fk YhC . Therefore, convexityYhC , Q YhC , therefore Q Yh .4. Dimensionality Reductionexperiments show efficiency algorithm depends heavily dimensionality matrix C Eq. (1). section, show principles behind automaticallydetermining necessary dimensionality given problem. Using proposed procedure, possible identify weak interactions eliminate them. Finally, procedureworks arbitrary bilinear programs generalization method previouslyintroduced (Petrik & Zilberstein, 2007a).dimensionality inherently part model, problem itself. mayequivalent models given problem different dimensionality. Thus, proceduresreducing dimensionality necessary modeler create modelminimal dimensionality. However, nontrivial many cases. addition,dimensions may little impact overall performance. determine onesdiscarded, need measure contribution computed efficiently.define notions formally later section.assume feasible sets bounded L2 norms, assume general formulation bilinear program, necessarily semi-compact form. Given Assumption 7,achieved scaling constraints feasible region bounded.Assumption 17. x X , norms satisfy kxk2 1 kyk2 1.discuss implications problems assumption presenting Theorem 18. Intuitively, dimensionality reduction removes dimensions g(y)constant, almost constant. Interestingly, dimensions may recovered basedeigenvectors eigenvalues C C. use eigenvectors C C insteadeigenvectors C, analysis based L2 norm x thus C.L2 norm kCk2 bounded largest eigenvalue C C. addition, symmetricmatrix required ensure eigenvectors perpendicular span wholespace.Given problem represented using Eq. (1), let F matrix whose columnseigenvectors C C eigenvalues greater . Let G matrixremaining eigenvectors columns. Notice together, columns matrices spanwhole space real-valued, since C C symmetric matrix. Assume withoutloss generality eigenvectors unitary. compressed version bilinearprogram following:maximizew,x,y1 ,y2 ,zf(w, x, y1 , y2 , z) = r1T x + sT2 w + x CF y1 + r2 Fsubject A1 x + B1 w = by1A2 F G+ B2 z = b2y2w, x, y1 , y2 , z 0258y1G+ sT2zy2(21)fiA Bilinear Programming Approach Multiagent PlanningNotice program missing element xT CGy2 , would make optimalsolutions identical optimal solutions Eq. (1). describe practical approachreducing dimensionality Appendix B. approach based singular valuedecomposition may directly applied bilinear program. following theoremquantifies maximum error using compressed program.Theorem 18. Let f f optimal solutions Eq. (1) Eq. (21) respectively. Then:p= |f f | .Moreover, maximal linear dimensionality reduction possible error withoutconsidering constraint structure.Proof. first show indeed error linearly compressedproblem given error least f dimensions. Using mapping preservesfeasibility programs, error bounded by:fifi fifififi fiy1fi = fix CGy2 fifi .fifif w, x, F G, z f w, x, y1 , 2fiy2zDenote feasible region y2 Y2 . orthogonality [F, G],ky2 k2 1 follows:y1= F Gy2y1F=y2GTGT = y2kGT yk2 = ky2 k2have:fifififimax max fixT CGy2 fi max kCGy2 k2y2 Y2 xXy2 Y2qqpmax y2T GT C CGy2 max y2T Ly2y2 Y2y2 Y2result follows Cauchy-Schwartz inequality, fact C C symmetric,Assumption 17. matrix L denotes diagonal matrix eigenvalues correspondingeigenvectors G.Now, let H arbitrary matrix satisfies preceding error inequality G.Clearly, H F = , otherwise y, kyk2 = 1, kCHyk2 > . Therefore,|H| n |F | |G|, |H| + |F | = |Y |. | | denotes number columnsmatrix.Alternatively, bound proved replacing equality A1 x + B1 w = b1kxk2 = 1. bound obtained Lagrange necessary optimality conditions.bounds use L2 -norm; extension different norm straightforward. Note259fiPetrik & Zilbersteinkyk2 1Figure 7: Approximation feasible set according Assumption 17.also dimensionality reduction technique ignores constraint structure.constraints special structure, might possible obtain even tighterbound. described next section, dimensionality reduction technique generalizesreduction Becker et al. (2004) used implicitly.result Theorem 18 based approximation feasible set kyk2 1,Assumption 17 states. approximation may quite loose problems,may lead significant multiplicative overestimation bound Theorem 18.example, consider feasible set depicted Figure 7. bound may achievedpoint y, far feasible region. specific problems, tighter bound couldobtained either appropriately scaling constraints, using weighted L2 betterprecision. partially address issue considering structure constraints.derive this, consider following linear program corresponding theorem:maximizexcT xsubject Ax = b(22)x0Theorem 19. optimal solution Eq. (22) objective functionmodifiedcT (I (AAT )1 A)x,identity matrix.Proof. objective function is:max{x Ax=b, x0}cT x ==max{x Ax=b, x0}cT (I (AAT )1 A)x + cT (AAT )1 Ax= cT (AAT )1 b +max{x Ax=b, x0}cT (I (AAT )1 A)x.first term may ignored depend solution x.260fiA Bilinear Programming Approach Multiagent Planningfollowing corollary shows theorem used strengthendimensionality reduction bound. example, zero-sum games, stronger dimensionality reduction splits bilinear program two linear programs.Corollary 20. Assume variables w z Eq. (1). Let:1Qi = (I(Ai Ai ) Ai )),{1, 2},Ai defined Eq. (1). Let C be:C = Q1 CQ2 ,C bilinear-term matrix Eq. (1). bilinear programsidentical optimal solutions either C C.Proof. Using Theorem 19, modify original objective function Eq. (1) to:11f (x, y) = r1T x + xT (I1 (A1 A1 ) A1 ))C(I A2 (A2 A2 ) A2 ))y + r2 y.sake simplicity ignore variables w z, influence bilinear1term. (I(Ai Ai ) Ai ) = 1, 2 orthogonal projection matrices,none eigenvalues Theorem 18 increase.dimensionality reduction presented section related idea compound events used CSA. Allen, Petrik, Zilberstein (2008a, 2008b) provide detaileddiscussion issue.5. Offline Boundsection develop approximation bound depends numberpoints g(y) evaluated structure problem. kind bounduseful practice provides performance guarantees without actually solvingproblem. addition, bound reveals parameters problem influencealgorithms performance. bound derived based maximal slope g(y)maximal distance among points.Theorem 21. achieve approximation error , number pointsevaluated regular grid k points every dimension must satisfy:nkCk2 nnk,n number dimensions .theorem follows using basic algebraic manipulations following lemma.Lemma 22. Assume y1 exists y2 ky1 y2 k2g(y2 ) = g(y2 ). maximal approximation error is:= max g(y) g(y) kCk2 .yY261fiPetrik & ZilbersteinProof. Let y1 point maximal error attained. point ,set compact. Now, let y2 closest point y1 L2 norm. Let x1 x2best responses y1 y2 respectively. definition solution optimalityderive:r1T x1 + r2T y2 + xT1 Cy2 r1 x2 + r2 y2 + x2 Cy2r1T (x1 x2 ) (x1 x2 )T Cy2 .error expressed, using fact kx1 x2 k2 1, as:= r1T x1 + r2T y1 + xT1 Cy1 r1 x2 r2 y1 x2 Cy1= r1T (x1 x2 ) + (x1 x2 )T Cy1(x1 x2 )T Cy2 + (x1 x2 )T Cy1(x1 x2 )T C(y1 y2 )(x1 x2 )T(y1 y2 )ky1 y2 k2Ck(x1 x2 )k2 ky1 y2 k2ky1 y2 k2maxmax{x kxk2 1} {y kyk2 1}xT CykCk2derivation follows Assumption 17, bound reduces matrixnorm using Cauchy-Schwartz inequality.surprisingly, bound independent local rewards transition structureagents. Thus fact shows complexity achieving fixed approximationfixed interaction structure linear problem size. However, boundsstill exponential dimensionality space. Notice also bound additive.6. Experimental Resultsturn empirical analysis performance algorithm. purposeuse Mars rover problem described earlier. compared algorithmoriginal CSA mixed integer linear program (MILP), derived Eq. (1) PetrikZilberstein (2007b) describe. Although Eq. (1) also modeled linear complementarity problem (LCP) (Murty, 1988; Cottle et al., 1992), evaluateoption experimentally LCPs closely related MILPs (Rosen, 1986). expecttwo formulations exhibit similar performance. also comparemethods described Horst Tuy (1996) Bennett Mangasarian (1992) duedifferent nature high complexity, algorithmsprovide optimality guarantees.experiments, applied algorithm randomly generated problem instancesparameters Becker et al. (2003, 2004) used. problem instanceincludes 2 rovers 6 sites. site, rovers decide perform experimentskip site. Performing experiments takes time, experiments mustperformed 15 time units. time required perform experiment drawndiscrete normal distribution mean uniformly chosen 4.0-6.0. variance262fiA Bilinear Programming Approach Multiagent PlanningAlgorithm 5: MPBP: Multiagent Planning Bilinear Programming6Formulate DEC-MDP bilinear program B ;// [Section 2.1]B 0 ReduceDimensionality(B) 104 ;// [Section 4, Appendix B]Convert B 0 semi-compact form ;// [Definition 2]h ;// Presolve step: run Algorithm 1 times random initialization{1 . . . }h max{h, IterativeBestResponse(B 0 )} ;// [Algorithm 1]7BestResponseApprox(B 0 , 0 ) ;12345// [Algorithm 2]0.4 mean. local reward performing experiment selected uniformlyinterval [0.1,1.0] site identical rovers. global reward,received rovers perform experiment shared site, super-additive1/2 local reward. experiments performed sites {1, 2, 3, 4, 5} sharedsites. Typically, performance algorithm degrades number shared sites.problem fewer 5 shared sitesas used original CSA paperwereeasy solve, present results problems 5 shared sites. Note CSAused problem implicit dimensionality reduction due usecompound events.experiments, naive dimensionality Eq. (5) 6 15 2 = 180.dimensionality reduced one per shared site using automaticdimensionality reduction procedure. dimension represents probabilityexperiment shared site performed regardless time. Therefore, dimensionrepresents sum individual probabilities. Becker et al. (2004) achievedcompression using compound events, compound event represents factexperiment performed site regardless specific time.complete algorithmMultiagent Planning Bilinear Programming (MPBP)issummarized Algorithm 5. automatic dimensionality reduction reduces 5 dimensions. Then, reformulating problem semi-compact form increases dimensionality6. experimented different configurations algorithm differ wayrefinements pivot point selection performed. different methods, describedSection 3.3, used create six configurations shown Figure 8. configurationC1 corresponds earlier version algorithm (Petrik & Zilberstein, 2007a).executed algorithm 20 times configuration every problem, randomlygenerated according distribution described above. results represent averagerandom instances. maximum number iterations algorithm 200.Due rounding errors, considered error less 104 0. algorithmimplemented MATLAB release 2007a. linear solver used MOSEK version5.0. hardware configuration Intel Core 2 Duo 1.6 GHz Low Voltage 2GBRAM. time perform dimensionality reduction negligible includedresult.direct comparison CSA possible CSA cannot solve problemsdimensionality within reasonable amount time. However, similar263fiPetrik & ZilbersteinConfigurationFeasible[Eq. (13)]C1Linear bound[Eq. (14)]Cutting plane[Eq. (20)]0C2C30C40C60C5Presolve []1010Figure 8: six algorithm configurations evaluated. Feasible, linear bound,cutting plane refer methods used determine optimal solution.problem setup 4 shared sites, CSA solved 76% problems,longest solution took approximately 4 hours (Becker et al., 2004). contrast, MPBPsolved 200 problems 4 shared sites optimally less 1 second average,10000 times faster. addition, MPBP returns solutions guaranteed closeoptimal first iterations. CSA also returns solutions close optimalrapidly, takes long time confirm that.Figure 9 shows average guaranteed ratio optimal solution, achievedfunction number iterations, is, points g(y) evaluated. figure,others, shows result online error bound. value guaranteedbased optimal solution. compares performance various configurationsalgorithm, without using presolve step. optimal solution typicallydiscovered first iterations, takes significantly longer prove optimality.average absolute errors linear log scale shown Figure 10.results indicate methods proposed eliminate dominated region searchingpivot point dramatically improve performance. requiring newpivot points feasible improves performance, much significant1Fraction OptimalC10.95C20.9C3C40.850.80.750.70.650.6050100Iteration150200Figure 9: Guaranteed fraction optimality according online bound.264fiA Bilinear Programming Approach Multiagent Planning1510C1C1C24C2010CC3C432Absolute ErrorAbsolute Error342103100C11010450100Iteration15010200050100Iteration150200Figure 10: Comparison absolute error various region elimination methods.better approximation Yh . expected, cutting plane elimination efficient,also complex.evaluate tradeoffs implementation, also show average time periteration average total time Figure 11. figures show time periteration significantly larger cutting plane elimination used. Overall,algorithm faster simpler linear bound used.0.03120.025100.028Total SecondsSeconds per Iterationtrend likely problem specific. problems higher dimensionality,precise cutting plane algorithm may efficient. Implementation issues playsignificant role problem too, likely implementation Algorithm 4improved.0.0150.01420.00506C1C2C30C4C1C2C3C4Figure 11: Time per iteration total time solve. configurations C1 C2 ,optimal value reached 200 iterations . figure showstime compute 200 iterations.265fiPetrik & Zilberstein110C3C4C5Absolute Error2C61031041001020Iteration3040Figure 12: Influence presolve method.Figure 12 shows influence using presolve method. plots C3 C4identical plots C5 C6 respectively, indicating presolve methodsignificant influence. also indicates solution closeoptimal obtained values initial points calculated.also performed experiments CPLEXa state-of-the-art MILP solver direct MILP formulation DEC-MDP. CPLEX able solve problemswithin 30 minutes, matter many sites shared. main reasontake advantage limited interaction. Nevertheless, possiblespecialized MILP solvers may perform better.7. Conclusion Workpresent algorithm significantly improves state-of-the-art solving two-agentcoordination problems. algorithm takes input bilinear program representingproblem, solves problem using new successive approximation method. providesuseful online performance bound used decide approximationgood enough. algorithm take advantage limited interaction among agents,translated small dimensionality bilinear program. Moreover, usingapproach, possible reduce dimensionality problems automatically,without extensive modeling effort. makes easy apply new method practice.applied DEC-MDPs, algorithm much faster existing CSA method,average reducing computation time four orders magnitude. also showvariety coordination problems treated within framework.Besides multiagent coordination problems, bilinear programs previously usedsolve problems operations research global optimization (Sherali & Shetty, 1980;White, 1992; Gabriel, Garca-Bertrand, Sahakij, & Conejo, 2005). Global optimizationdeals finding optimal solutions problems multi-extremal objective function.Solution techniques often share idea based cutting plane methods.main idea iteratively restrict set feasible solutions, improving incumbent266fiA Bilinear Programming Approach Multiagent Planningsolution. Horst Tuy (1996) provide excellent overview techniques.algorithms different characteristics cannot directly compared algorithmdeveloped. Unlike traditional algorithms, focus providing quickly goodapproximate solutions error bounds. addition, exploit small dimensionalitybest-response space get tight approximation bounds.Future work address several interesting open questions respect bilinearformulation well improvement efficiency algorithm. regardrepresentation, yet determined whether anytime behavior exploitedapplied games. is, necessary verify approximate solutionbilinear program also meaningful approximation Nash equilibrium. alsoimportant identify classes extensive games efficiently formulatedbilinear programs.algorithm present made efficient several ways. particular,significant speedup could achieved reducing size individual linear programs.programs solved many times constraints, different objectivefunction. objective function always small-dimensional space. Therefore,problems solved similar. DEC-MDP domain, one option woulduse procedure similar action elimination. addition, performance couldsignificantly improved starting tight initial triangulation. implementation,simply use single large polyhedron covers whole feasible region. betterapproach would start something approximates feasible regiontightly. tighter approximation feasible region could also improve precisiondimensionality reduction procedure. Instead naive ellipsis used Assumption 7,possible use one approximates feasible region tightly possible.however encouraging see even without improvements, algorithmeffective compared existing solution techniques.Acknowledgmentsthank Chris Amato, Raghav Aras, Alan Carlin, Hala Mostafa, anonymousreviewers useful comments suggestions. work supported partAir Force Office Scientific Research Grants No. FA9550-05-1-0254 FA955008-1-0181, National Science Foundation Grants No. IIS-0535061IIS-0812149.Appendix A. ProofsProof Proposition 10 proposition states proposed triangulation,sub-polyhedra overlap cover whole feasible set , given pivotpoint interior S.Proof. prove theorem induction number polyhedron splitsperformed. base case trivial: single polyhedron, coverswhole feasible region.inductive case, show polyhedron sub-polyhedra inducedpivot point cover overlap. notation use following:267fiPetrik & Zilbersteindenotes original polyhedron = c pivot point, 1T c = 1 c 0.Note matrix c, d, vectors, scalar.show sub-polyhedra cover original polyhedron follows. Take= 1T = 1 0. show exists sub-polyhedroncontains vertex. First, let=1Tmatrix square invertible, since polyhedron non-empty. get representation contains y, show vector i,o(i) = 0:= = +10,> 0. ensure sub-polyhedron vertexreplaced y. value depends follows:1.=1achieved setting:= mind(i)(T 1 y)(i).Since c = 1 non-negative. leaves us equationsub-polyhedron containing point a. Notice resulting polyhedron maysmaller dimension n o(j) = 0 6= j.show polyhedra overlap, assume exists point common interior least two polyhedra. is, assume convexcombination vertices:= T3 c1 + h1 + 1 y1= T3 c2 + h2 + 2 y2 ,T3 represents set points common two polyhedra, y1 y2 representdisjoint points two polyhedra. values h1 , h2 , 1 , 2 scalars,c1 c2 vectors. Notice sub-polyhedra differ one vertex.coefficients satisfy:c1 0c2 0h1 0h2 01 02 01 c1 + h1 + 1 = 11 c2 + h2 + 2 = 1268fiA Bilinear Programming Approach Multiagent PlanningSince interior polyhedron non-empty, convex combination unique.First assume h = h1 = h2 . show following:= T3 c1 + hy + 1 y1 = T3 c2 + hy + 2 y2T3 c1 + 1 y1 = T3 c2 + 2 y21 y1 = 2 y21 = 2 = 0holds since y1 y2 independent T3 polyhedron nonemptyy1 6= y2 . last equality follows fact y1 y2 linearly independent.contradiction, since 1 = 2 = 0 implies point interiortwo polyhedra, intersection.Finally, assume WLOG h1 > h2 . let = T3 c + 1 y1 + 2 y2 , scalars1 0 2 0 represent convex combination. get:= T3 c1 + h1 + 1 y1 = T3 (c1 + h1 c) + (h1 1 + 1 )y1 + h1 2 y2= T3 c2 + h2 + 2 y2 = T3 (c2 + h2 c) + h2 1 y1 + (h2 2 + 2 )y2 .coefficients sum one shown below.1T (c1 + h1 c) + (h1 1 + 1 ) + h1 2 = 1T c1 + 1 + h1 (1T c + 1 + 2 ) = 1T c1 + 1 + h1 = 11T (c2 + h2 c) + 1 + (h2 2 + 2 ) = 1T c2 + 2 + h2 (1T c + 1 + 2 ) = 1T c2 + 2 + h2 = 1Now, convex combination unique, therefore coefficients associatedvertex two representations must identical. particular, equating coefficients y1 y2 results following:h1 1 + 1 = h2 1h1 2 = h2 2 + 21 = h2 1 h1 12 = h1 2 h2 21 = 1 (h2 h1 ) > 02 = 2 (h1 h2 ) < 01 > 0 2 > 0 fact interior polyhedron S.Then, 2 0 contradiction convex combination verticesS.Appendix B. Practical Dimensionality Reductionsection describe approach dimensionality reduction easy implement. Note least two possible approaches take advantage reduceddimensionality. First, possible use dimensionality information limit algorithm work significant dimensions . Second, possible modifybilinear program small dimensionality. changing algorithm maystraightforward, limits use advanced pivot point selection methods describedSection 3.3. Here, show implement second option straightforward wayusing singular value decomposition.269fiPetrik & Zilbersteindimensionality reduction applied following bilinear program:maximizew,x,y,zr1T x + sT1 w + x Cy + r2 + s2 zsubject A1 x + B1 w = b1A2 + B2 z = b2(23)w, x, y, z 0Let C = SV singular value decomposition. Let = [T1 , T2 ],singular value vectors ti T2 less required . Then, bilinear programreduced dimensionality may defined follows:maximizew,x,y,y,zr1T x + sT1 w + x SV T1 + r2 + s2 zsubject T1 =A1 x + B1 w = b1(24)A2 + B2 z = b2w, x, y, z 0Note constrained non-negative. One problematic aspect reducingdimensionality define initial polyhedron needs encompass feasiblesolutions. One option make large enough contain set {y kyk2 = 1},may large. Often practice, may efficient first triangulate roughapproximation feasible region, execute algorithm triangulation.Appendix C. Sum Convex Concave Functionssection show best-response function g(y) may convexprogram semi-compact form. convexity best-response functioncrucial bounding approximation error eliminating dominated regions.show program semi-compact form, best-responsefunction written sum convex function concave function. showconsider following bilinear program.maximizew,x,y,zf = r1T x + sT1 w + x Cy + r2 + s2 zsubject A1 x + B1 w = b1A2 + B2 z = b2w, x, y, z 0problem may reformulated as:f==maxmaxmaxg 0 (y) + sT2 z,{y,z (y,z)Y } {x,w (x,w)X}{y,z (y,z)Y }r1T x + sT1 w + x Cy + r2 + s2 z270(25)fiA Bilinear Programming Approach Multiagent Planningg 0 (y) =max{x,w (x,w)X}r1T x + sT1 w + x Cy + r2 y.Notice function g 0 (y) convex, maximum set linear functions.Since f = max{y (y,z)Y } g(y), best-response function g(y) expressed as:g(y) =max{z (y,z)Y }00g 0 (y) + sT2 z = g (y) +max{z (y,z)Y }sT2z= g (y) + t(y),t(y) =max{z A2 y+B2 z=b2 , y,z0}sT2 z.Function g 0 (y) depend z, therefore could taken maximization.function t(y) corresponds linear program, dual using variable q is:(b2 A2 y)T qminimizeqsubject B2T q s2(26)Therefore:t(y) =(b2 A2 y)T q,min{q B2T qs2 }concave function, minimum set linear functions.best-response function written as:g(y) = g 0 (y) + t(y),sum convex function concave function, also known d.c. function (Horst & Tuy, 1996). Using property, easy construct programg(y) convex one part concave another part , followingexample shows. Note semi-compact bilinear programs t(y) = 0, guaranteesconvexity g(y).Example 23. Consider following bilinear program:maximizex,y,zx + xy 2zsubject 1 x 1yz 2z0plot best response function program shown Figure 13.271(27)fiPetrik & Zilberstein2maxx f(x,y)1.510.500.51101234Figure 13: plot non-convex best-response function g bilinear program,semi-compact form.ReferencesAllen, M., Petrik, M., & Zilberstein, S. (2008a). Interaction structure dimensionalitydecentralized problem solving. Conference Artificial Intelligence (AAAI), pp.14401441.Allen, M., Petrik, M., & Zilberstein, S. (2008b). Interaction structure dimensionalitydecentralized problem solving. Tech. rep. 08-11, Computer Science Department,University Massachussetts.Aras, R. (2008). Mathematical programming methods decentralized POMDPs. Ph.D.thesis, Universite Henri Poincare, Nancy, France.Aras, R., & Charpillet, F. (2007). mixed integer linear programming method finitehorizon Dec-POMDP problem. International Conference Automated PlanningScheduling (ICAPS), pp. 1825.Becker, R. (2006). Exploiting Structure Decentralized Markov Decision Processes. Ph.D.thesis, University Massachusetts Amherst.Becker, R., Lesser, V., & Zilberstein, S. (2004). Decentralized Markov decision processesevent-driven interactions. International Joint Conference AutonomousAgents Multi Agent Systems (AAMAS), pp. 302309.Becker, R., Zilberstein, S., & Lesser, V. (2003). Transition-independent decentralizedMarkov decision processes. International Joint Conference Autonomous AgentsMulti Agent Systems (AAMAS), pp. 4148.Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independent decentralized Markov decision processes. Journal Artificial IntelligenceResearch, 22, 423455.Bennett, K. P., & Mangasarian, O. L. (1992). Bilinear separation two sets n-space.Tech. rep., Computer Science Department, University Wisconsin.Bernstein, D. S., Zilberstein, S., & Immerman, N. (2000). complexity decentralized control Markov decision processes. Conference Uncertainty ArtificialIntelligence (UAI), pp. 3237.272fiA Bilinear Programming Approach Multiagent PlanningBertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic programming. Athena Scientific.Boutilier, C. (1999). Sequential optimality coordination multiagent systems.International Joint Conference Artificial Intelligence, pp. 478485.Bresina, J. L., Golden, K., Smith, D. E., & Washington, R. (1999). Increased fexibilityrobustness Mars rovers. International Symposium AI, Robotics,Automation Space, pp. 167173.Cheng, H. T. (1988). Algorithms Partially Observable Markov Decision Processes. Ph.D.thesis, University British Columbia.Cottle, R. W., Pang, J.-S., & Stone, R. E. (1992). Linear Complementarity Problem.Academic Press.Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions partially observable stochastic games common payoffs. InternationalJoint Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 136143.Gabriel, S. A., Garca-Bertrand, R., Sahakij, P., & Conejo, A. J. (2005). practical approachapproximate bilinear functions mathematical programming problems usingSchurs decomposition SOS type 2 variables. Journal Operational ResearchSociety, 57, 9951004.Goldman, C. V., & Zilberstein, S. (2008). Communication-based decomposition mechanismsdecentralized MDPs. Journal Artificial Intelligence Research, 32, 169202.Horst, R., & Tuy, H. (1996). Global optimization: Deterministic approaches. Springer.Kim, Y., Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2006). Exploiting localityinteraction networked distributed POMDPs. AAAI Spring SymposiumDistributed Planning Scheduling, pp. 4148.Koller, D., Megiddo, N., & von Stengel, B. (1994). Fast algorithms finding randomizedstrategies game trees. ACM Symposium Theory Computing, pp. 750759.Mangasarian, O. L. (1995). linear complementarity problem separable bilinearprogram. Journal Global Optimization, 12, 17.Murty, K. G. (1988).Helderman-Verlag.Linear complementarity, linear nonlinear programming.Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralized POMDPs: Towards efficient policy computation multiagent settings..International Joint Conference Artificial Inteligence, pp. 705711.Nair, R., Roth, M., Yokoo, M., & Tambe, M. (2004). Communication improving policycomputation distributed pomdps. International Joint Conference AgentsMultiagent Systems (AAMAS), pp. 10981105.Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcastchannel: performance bounds. Proceeding IEEE Conference DecisionControl, Vol. 1, pp. 293298.273fiPetrik & ZilbersteinOsborne, M. J., & Rubinstein, A. (1994). course game theory. MIT Press.Pang, J.-S., Trinkle, J. C., & Lo, G. (1996). complementarity approach quasistaticrigid body motion problem. Journal Computational Optimization Applications,5 (2), 139154.Petrik, M., & Zilberstein, S. (2007a). Anytime coordination using separable bilinear programs. Conference Artificial Intelligence, pp. 750755.Petrik, M., & Zilberstein, S. (2007b). Average reward decentralized Markov decision processes. International Joint Conference Artificial Intelligence, pp. 19972002.Puterman, M. L. (2005). Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, Inc.Rosberg, Z. (1983). Optimal decentralized control multiaccess channel partialinformation. IEEE Transactions Automatic Control, 28, 187193.Rosen, J. B. (1986). Solution general LCP 0-1 mixed integer programming. Tech. rep.Computer Science Tech. Report 8623, University Minnesota, Minneapolis.Rubinstein, A. (1997). Modeling bounded rationality. MIT Press.Seuken, S., & Zilberstein, S. (2007). Memory bounded dynamic programming DECPOMDPs. International Joint Conference Artificial Intelligence, pp. 20092016.Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decisionmaking uncertainty. Autonomous Agents Multiagent Systems, 17 (2), 190250.Sherali, H. D., & Shetty, C. M. (1980). finitely convergent algorithm bilinear programming problems using polar cuts disjunctive face cuts. Mathematical Programming,19 (1), 1431.Vanderbei, R. J. (2001). Linear Programming: Foundations Extensions (2nd edition).Springer.White, D. J. (1992). linear programming approach solving bilinear programmes.Mathematical Programming, 56 (1), 4550.274fiJournal Artificial Intelligence Research 35 (2009) 533-555Submitted 12/08; published 07/09Solving Weighted Constraint Satisfaction ProblemsMemetic/Exact Hybrid AlgorithmsJose E. GallardoCarlos CottaAntonio J. Fernandezpepeg@lcc.uma.esccottap@lcc.uma.esafdez@lcc.uma.esDept. Lenguajes Ciencias de la ComputacionUniversidad de Malaga, ETSI InformaticaCampus de Teatinos, 29071 Malaga, SpainAbstractweighted constraint satisfaction problem (WCSP) constraint satisfaction problempreferences among solutions expressed. Bucket elimination completetechnique commonly used solve kind constraint satisfaction problem.memory required apply bucket elimination high, heuristic method based(denominated mini-buckets) used calculate bounds optimal solution.Nevertheless, curse dimensionality makes techniques impractical large scaleproblems. response situation, present memetic algorithm WCSPsbucket elimination used mechanism recombining solutions, providingbest possible child parental set. Subsequently, multi-level modelexact/metaheuristic hybrid hybridized branch-and-bound techniquesmini-buckets studied. case study, applied algorithms resolutionmaximum density still life problem, hard constraint optimization problem basedConways game life. resulting algorithm consistently finds optimal patternsdate solved instances less time current approaches. Moreover, shownproposal provides new best known solutions large instances.1. IntroductionMany real problems formulated constraint satisfaction problems (CSPs)solutions assignments set variables (each variable taking values certaindomain), exists collection constraints restrict assignmentparticular values combination values; solving CSP means finding feasible assignmentvalues variables, i.e., one constraints satisfied. However, wide rangeproblems cannot posed way, either problem over-constrained (andthus solution) problem multiple solutions objectivefind best one according optimality criterion. cases, problem mighthandled optimization point view associating preferences constraints.kind CSP preferences among constraints/solutions expressedcalled weighted constraint satisfaction problems (WCSPs) (Schiex, Fargier, & Verfaillie,1995; Bistarelli, Montanari, & Rossi, 1997). Solving WCSP means optimally satisfyingset weighted constraints. clearly enlarges scope CSPs: many practicalproblems modeled WCSPs, instance, radio frequency assignment,scheduling cellular manufacturing, among others (Cabon, de Givry, Lobjois, Schiex, &Warners, 1999; Khemmoudj & Bennaceur, 2007; Nonobe & Ibaraki, 2001).c2009AI Access Foundation. rights reserved.fiGallardo, Cotta, & FernandezComplete methods, like branch-and-bound (Lawler & Wood, 1966) bucket elimination (Dechter, 1999), technique originated early work BerteleBrioschi (1972) nonserial dynamic programming, two popular waysattack WCSPs. However, although picture CSP general, notedinclusion preferences constraints makes particular WCSP specificconsequence WCSPs tackled using specialized algorithmsspecifically designed (Freuder & Wallace, 1992; Verfaillie, Lematre, & Schiex, 1996;Kask & Detcher, 2001; Lematre, Verfaillie, Bourreau, & Laburthe, 2001; Larrosa & Schiex,2004; Gelain, Pini, Rossi, & Venable, 2007; Khemmoudj & Bennaceur, 2007; Marinescu& Dechter, 2007). Moreover, general techniques require large computational effort(in time, memory both) solve many WCSPs, due size complexity,therefore impractical many cases. alleviated using heuristic methods,e.g., beam search (BS) (Barr & Feigenbaum, 1981) mini-buckets (Dechter, 1997),branch-and-bound bucket elimination respectively. However, large scale problems,high computational cost still evident.context use alternative techniques must considered overcomelimitations general techniques; instance, evolutionary algorithms (Back, 1996; Back,Fogel, & Michalewicz, 1997) powerful heuristics optimization problems basedprinciples natural evolution, flexible enough deployed widerange problems. However, generality reduces competitiveness, unless domainknowledge also incorporated. need exploiting domain knowledge optimizationmethods repeatedly shown (Wolpert & Macready, 1997; Culberson, 1998),memetic algorithms (Moscato & Cotta, 2003, 2007; Krasnogor & Smith, 2005) representone successful responses need (Hart, Krasnogor, & Smith, 2005).paper explores different ways hybridizing branch-and-bound/bucket elimination (andcorresponding heuristic methods) memetic algorithms, combining searchcapabilities synergetic way.hybrid techniques proposed used general problem solvers WCSPs.Note essentially heuristic nature hence cannot provide optimalityproofs solutions obtain. Notice however probably provide optimalnear-optimal solutions wide range WCSPs. Furthermore, hybrid techniquesless time-consuming general methods involved them, thus appliedlarger problem instances. order experimentally evaluate hybrid techniques,tackled Maximum Density Still Life Problem, hard combinatorial optimization problem also prime example weighted constraint optimization problem.polynomial-time algorithm known solve problem, although, bestknowledge, problem yet proven NP-hard. reasons,surprising problem attracted interest constraint-programmingcommunity, central development assessment sophisticated techniques bucket elimination. Indeed, constitutes excellent test bed differentoptimization techniques, included CSPLib1 repository. web page2keeps record up-to-date results.1. http://www.csplib.org2. http://www.ai.sri.com/~nysmith/life534fiSolving WCSPs Memetic/Exact Hybrid Algorithms2. Preliminariessection, briefly introduce concepts techniques used restpaper. end, first define weighted constraint satisfaction problems, welltechniques bucket elimination mini-buckets. Subsequently, describe beamsearch, heuristic tree search algorithm derived branch-and-bound. Finally, memeticalgorithms presented. sake notational simplicity, appropriate sticknotation Larrosa et al. (2003, 2005).2.1 Weighted Constraint Satisfaction Problemsweighted constraint satisfaction problem (WCSP) (Schiex et al., 1995; Bistarelli et al.,1997) constraint satisfaction problem (CSP) preferences among solutionsexpressed. Formally, WCSP defined tuple (X , D, F), = {D1 , , Dn }set finite domains, X = {x1 , , xn } set variables taking values finitedomains (Di domain variable xi ) F set cost functions (also called softconstraints weighted constraints) used declare preferences among possible solutions.Variable correctly assigned receive finite costs express degree preference (thelower value better preference) variables correctly assigned receive cost. Note f F defined subset variables, var(f ) X , calledP scope.objective function F defined sum functions F, i.e., F = f F f .assignment value vi Di variable xi noted xi = vi . partial assignment< n variables tuple = (xi1 = v1 , xi2 = v2 , , xim = vm ) ij {1, . . . , n}different. complete assignment variables values domainssatisfies every soft constraint (i.e., finite valuation F ) represents solutionWCSP. optimization goal find solution minimizes objective function.2.2 Bucket EliminationBucket elimination (BE) (Dechter, 1999) generic technique suitable many automatedreasoning optimization problems and, particular, solving WCSP. functioningbased upon following two operators functions (Larrosa et al., 2005):sum two functions f g, denoted (f + g), new function scopevar(f )var(g) returns tuple sum costs f g, i.e., (f +g)(t) =f (t) + g(t).elimination variable xi f , denoted f xi , new function scopevar(f ) {xi } returns tuple minimum cost extension xi ,(f xi )(t) = minvDi {f (t (xi = v))}, (xi = v) means extensionassignment assignment value v variable xi . Observe funary function (i.e., arity one), constant obtained upon eliminationvariable scope.Without losing generality, let us assume lexicographic ordering variablesX , i.e., = (x1 , x2 , , xn ). Figure 1 shows pseudo-code algorithm solvingWCSP instance, returns optimal cost F one optimal assignment535fiGallardo, Cotta, & FernandezBucket Elimination WCSP (X , D, F )1:2:3:4:5:6:7:8:9:10 :11 :function BE(X , D, F):= n downto 1Bi := {fP F | xi var(f )}gi := (f Bi f ) xiF := (F {gi }) Biend:=:= 1 n Pv := argminaDi {( f Bi f )(t (xi = a))}:= (xi = v)endreturn(F, t)end functionFigure 1: general template, adapted Larrosa Morancho (2003), bucketelimination WCSP (X , D, F ).t. Observe that, initially, eliminates decreasing order one variable xi Xiteration loop comprising lines 1-5. done computing firstly bucket Bivariable xi set cost functions F xi scope. Then, new functiongi defined sum functions Bi variable xi eliminated.Finally, F updated removing functions involving xi (i.e., Bi ) addingnew function contain xi . consequence xi exist Fvalue optimal cost preserved. elimination x1 produces functionempty scope (i.e., constant) optimal cost problem. Then,lines 6-10, generates optimal assignment variables considering orderimposed o: done starting empty assignment assigning xibest value extension t, respect sum functions Bi (argmina {f (a)}represents value producing minimum f (a)).Note exponential space complexity because, general, result summing functions eliminating variables cannot expressed intensionally algebraic expressions and, consequence, intermediate results collected extensionallytables. precise, complexity depends problem structure (as capturedconstraint graph G) ordering o. According Larrosa Morancho (2003),complexity along ordering time (Q n dw (o)+1 ) space (n dw (o) ),largest domain size, Q cost evaluating cost functions (usually assumed (1)), w (o) induced width graph along ordering o, describeslargest clique created graph bucket elimination, correspondslargest scope function recorded algorithm. Although finding optimalordering NP-hard (Arnborg, 1985), heuristics approximation algorithmsdeveloped task check work Dechter (1999) details.536fiSolving WCSPs Memetic/Exact Hybrid Algorithms2.3 Mini-Bucketsmain drawback requires exponential space store functions extensionally. complexity high, solution approximated usingmini-bucket (MB) approach presented Dechter (1997) Detcher Rish (2003). Recall that, order eliminate variable xi ,P corresponding bucket Bi = {fi1 , . . . , fim },calculates new cost function gi = ( f Bi f ) xi , whose time space complexity increases cardinality gi , i.e., size set f Bi var(f ) {xi }.complexity decreased approximating function gi set smallerarity functions. basic idea partition bucket Bi k called mini-bucketsBi1 , . . . , Bik , number variables scope Bij boundedparameter. Afterwards,set k cost functions reduced arity soughtPdefined gij = ( f Bi f ) xi , j = 1 . . . k, required approximation gijPPPcomputed sum gi0 = 16j6k gij = 16j6k (( f Bi f ) xi ).jNote minimization computed gi operator migrated insidesum. Since, general, two non-negative functions f1 (x) f2 (x), minx (f1 (x)+f2 (x)) > minx f1 (x) + minx f2 (x), follows gi0 lower bound gi . Therefore,variable elimination performed using approximated cost functions, provides lowerbound optimal cost requiring less computation BE. Notice describedapproach provides family under-estimating heuristic functions whose complexityaccuracy parameterized maximum number variables allowed mini-bucket.2.4 Beam SearchBranch-and-bound (BB) (Lawler & Wood, 1966) general tree search method solvingcombinatorial optimization problems. Tree search methods constructive, sensework partial solutions. way, tree search methods start emptysolution incrementally extended adding components it. way partialsolutions extended depends constraints imposed problem solved.solution construction mechanism maps search space tree structure,way path root tree leaf node corresponds constructionsolution. order efficiently explore search tree, BB algorithms maintainupper bound estimate lower bounds partially constructed solutions. Assumingminimization problem, upper bound corresponds cost best solution foundfar. search process, lower bound computed partial solutiongenerated, estimating cost best solution constructed extendingit. lower bound greater current upper bound, solutions constructedextending lead improvement, thus nodes descendingpruned search tree. Clearly, capability algorithm pruningsearch tree depends existence accurate lower bound, alsocomputationally inexpensive order practical.Beam search (BS) (Barr & Feigenbaum, 1981) algorithms incomplete derivates BBalgorithms, thus heuristic methods. Essentially, BS works extending every partialsolution set B (called beam) kext possible ways. new partialsolution generated stored set B. solutions B processed,algorithm constructs new beam selecting best kbw (called beam width)537fiGallardo, Cotta, & Fernandezsolutions B. Clearly, way estimating quality partial solutions,lower bound, needed this.interesting peculiarity BS way extends parallel set different partialsolutions several possible ways, making particularly suitable tree search methodused hybrid collaborative framework (it used provide periodically promising partial solutions population-based search method memetic algorithm).Gallardo, Cotta, Fernandez (2007) shown kind hybrid algorithmsprovide excellent results combinatorial optimization problems. subsequently present hybrid tree search/memetic algorithm WCSPs based idea.2.5 Memetic AlgorithmsEvolutionary algorithms (EAs) population-based metaheuristic optimization methodsinspired biological evolution (Back et al., 1997). order explore search space,EA maintains set solutions known population individuals. usuallyrandomly initialized across search space, although heuristics may also used.initialization, three different phases iteratively performed termination conditionreached: selection, reproduction (which encompasses recombination mutation)replacement. context EAs, objective function assigning values solutiontermed fitness function, used guide search.Note EAs black box optimization procedures sense knowledgeproblem (apart fitness function) used. need exploit problemknowledge repeatedly shown (Wolpert & Macready, 1997; Culberson, 1998) however. Different attempts made answer need; Memetic algorithms (Moscato& Cotta, 2003, 2007; Krasnogor & Smith, 2005) (MAs) one successful approaches date (Hart et al., 2005). Like EAs, MAs also population based metaheuristics. main difference components population (sometimes termedagents terminology) passive entities. Rather, active entitiescooperate compete order find improved solutions.many possible ways implement MAs. common implementationconsists combining EA procedure perform local searchsolutions population main generation loop (cf. Krasnogor & Smith, 2005).Figure 2 shows general outline MA; pX , pm arity respectively referrecombination probability, mutation probability recombination arity i.e., numberparents involved recombination. must noted however paradigmsimply reduce particular scheme different places (e.g.,population initialization, genotype phenotype mapping, evolutionary operators, etc.)problem specific knowledge incorporated. work, addition usingtabu search (Glover, 1989, 1990) (TS) local search procedure within MA,designed intelligent recombination operator uses relaxation bucket eliminationorder find best solution constructed set parents withoutintroducing implicit mutation (i.e., exogenous information).538fiSolving WCSPs Memetic/Exact Hybrid AlgorithmsMemetic Algorithm1:2:3:4:5:6:7:8:9:10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :function (pX , pm , arity):= 1 popsizepop[i] := Random solution(n)pop[i] := Local Search(pop[i])Evaluate(pop[i])endtimeout:= 1 offsizerecombination performed (under pX )j := 1 arityparentj := Select(pop)endoffspring[i ] := Recombine(parent1 , parent2 , . . . , parentarity )elseoffspring[i ] := Select(pop)endmutation performed (under pm )offspring[i ] := Mutate(offspring[i ])endoffspring[i ] := Local Search(offspring[i ])Evaluate(offspring[i ])endpop := Replace(pop, offspring)endFigure 2: Pseudo code memetic algorithm (MA). Although different variants possible respect scheme, broadly captures typical algorithmic structureMAs.3. Multi-Level Memetic/Exact Hybrid Algorithm WCSPsWCSPs suitable tackled evolutionary metaheuristics. Obviously,quality results greatly depend well knowledge problemincorporated search mechanism. final goal present algorithmic modelbased hybridization MAs exact techniques two levels: within (asembedded operator), outside (in cooperative model). Firstly, focusnext subsection first level hybridization, incorporates exact technique(namely BE) within embedded recombination operator. Subsequently,proceed second level hybridization, cooperates branchand-bound based beam search algorithm uses technique mini-bucketslower bound (see Figure 3).539fiGallardo, Cotta, & FernandezPromising RegionsBSGATSLocalSearchMBCrossoverLowerBoundUpper BoundFigure 3: Schematic description proposed hybrid algorithm.3.1 Optimal recombinationpreviously mentioned, one phases constitutes typical recombination(i.e., lines 9-14 Figure 2), individuals population combinedaim obtaining improved individuals. purpose, different standard recombination operators proposed literature (see Back et al., 1997). Althoughblind operators feasible computational point view, would perform poorly,problem knowledge used. context WCSPs, resortorder achieve sensible recombination information.Even though performance exact method resolution WCSPsmay better basic search-based approaches, corresponding time space complexity still high, making technique unsuitable large instances.following, explain used implement intelligent recombination operator WCSPs. operator implicitly explore possible children solutionsrecombined, providing best solution constructed without introducingimplicit mutation, i.e., exogenous information (cf. Cotta & Troya, 2003). Noteuse bucket elimination related usually referred Large NeighborhoodSearch (Ahuja, Ergun, Orlin, & Punnen, 2002).sake simplicity, let us assume variables WCSP (X , D, F )domain (i.e., D1 = = Dn ), let x = (x1 , x2 , , xn ) = (y1 , y2 , , yn )two solutions recombined, [zi ] value variable zi . operator calculate best solution obtained combining variablesx without introducing information present parents.achieved restricting domain variables values appearing configurations recombined. recombination operator becomes BE(X , D, F),= {[x1 ], , [xn ], [y1 ], , [yn ]}. Applying approach WCSP variablesmay different domains would require previously separating set variables Xsubsets variables sharing domain.3.2 Beam Search/MA Hybrid Algorithmsubsection, describe hybrid tree search/memetic algorithm WCSPs.algorithm combines, collaborative way (Puchinger & Raidl, 2005), BS algorithmMA. noted previously, BS works extending parallel set different partial solutionsseveral possible ways, thus used provide promising partial solutions540fiSolving WCSPs Memetic/Exact Hybrid AlgorithmsHybrid algorithm WCSP1:2:3:4:5:6:7:8:9:10 :11 :12 :13 :14 :15 :16 :17 :function BS-MA (X , D, kbw , kM )sol :=B := { () }:= 1 nB0 := {}BDiB 0 := B 0 {s (xi = a)}endendB := select best kbw nodes B 0(i > kM )initialize population best popsize nodes B 0runsol := min (sol, solution)endendreturn solend functionFigure 4: Hybrid algorithm WCSP.population based search method MA. goal exploit capability BSidentifying potentially good regions search space, also exploitexplore regions, synergistically combining two different approaches.proposed hybrid algorithm, executes BS interleaved way,depicted Figure 4. pseudo-code, (possibly partial) solution WCSP instancerepresented vector variables = (x1 , x2 , . . . , xi ), 6 n, (xi = a) standsextension partial solution assigning value i-th variable notedpreviously. hybrid algorithm constructs search tree, leaves consistcomplete solutions internal nodes level represent partially specified (up i-thvariable) solutions. tree heuristically traversed breadth first way using BSalgorithm beam width kbw (i.e., maintaining best kbw nodes leveltree). beam selection (line 10), heuristic quality measure definedpartial solutions, whose value must partial solution unfeasible. algorithmstarts (line 2) totally unspecified solution. Initially, BS part algorithmexecuted. iteration BS (lines 3-17), new variable assigned everysolution beam (line 7). interleaved execution starts partialsolutions beam least kM variables (line 11). iteration BS,best popsize solutions beam selected (using quality measure described above)initialize population (line 12). Since partial solutions, mustfirst converted full solutions, e.g., completing remaining variables randomly.541fiGallardo, Cotta, & Fernandezrunning MA, solution used update incumbent solution (sol),process repeated search tree exhausted.3.3 Computing Tight Bounds Mini-Bucketsperformance BS component algorithm described previous sectiondepend quality heuristic function used estimate partial solutions (line10 Figure 4). order compute tight, yet computationally inexpensive, lower boundremanning part solution resort Mini-Buckets (MB). describedKask Detcher (2001), intermediate functions created applying MB schemeused general mechanism compute heuristic functions estimate bestcost yet unassigned variables partial solutions. end, MB must runpreprocessing stage, using reverse order search instantiate variables.set augmented buckets computed process used estimationsbest cost extension partial solutions (check work Kask & Detcher, 2001,details).4. Tackling Maximum Density Still Life ProblemPreviously proposed algorithms general enough used many WCSPsexecuted. section present application case study maximumdensity still life problem (MDSLP). problem defined context game lifeproposed John H. Conway 60s divulged Martin Gardner (Gardner, 1970),let us first describe game. played infinite checkerboardplayer places checkers squares. square board called celleight neighbors; eight cells share one two corners it. cell alivechecker it, dead otherwise. contents board evolve iteratively,way state time determines state time + 1 accordingsimple rules: (1) live cell remains alive two three live neighbors, otherwisedies, (2) dead cell becomes alive exactly three live neighbors.simple rules game life nevertheless generate incredibly complexdynamics. better understand MDSLP, let us define stable pattern (also calledstill life) board configuration change time, let densityregion percentage living cells. MDSLP nn grid consists finding stilllife maximum density. Elkies (1998) shown that, infinite boards, maximumdensity 1/2 (for finite size, exact formula known). paper, concernedMDSLP finite patterns, is, finding maximal n n still lifes.4.1 Related WorkMDSLP tackled literature using different approaches. Bosch Trick(2002) compared different formulations MDSLP using integer programming (IP)constraint programming (CP). best results obtained hybrid algorithmmixing two approaches. able solve cases n = 14 n = 156 8 days CPU time respectively. Smith (2002) used pure constraintprogramming approach address problem. However, instances n = 10542fiSolving WCSPs Memetic/Exact Hybrid AlgorithmsTable 1: Best experimental results reported Bosch Trick (2002) (CP/IP), LarrosaMorancho (2003) (BE) Larrosa et al. (2005) (HYB-BE) solvingMDSLP. Time indicated seconds.optimumCP/IPHYB-BE1268115361638113791205013788214925 1051052151067 105161201713718153191712019058710912029560272 105could solved. best results problem reported Larrosa Morancho(2003) Larrosa et al. (2005), showing usefulness bucket elimination (BE),exact technique based variable elimination commonly used solving constraintsatisfaction problems described detail Section 2.2. basic approach could solveproblem n = 14 105 seconds. improvements increased boundaryn = 20 twice much time. Recently, Cheng Yap (2005, 2006) tackledproblem via use ad-hoc global case constraints, results comparableIP/CP hybrids, thus cannot compared ones obtained previously Larrosaet al.Table 1 summarizes experimental results current approaches used tackle MDSLP, reporting computational times hybrid IP/CP algorithm Bosch Trick(2002), approach Larrosa Morancho (2003) BE/search hybridLarrosa et al. (2005). Although different computational platforms may usedexperiments, trends clear give clear indication potentialdifferent approaches. noted techniques applied MDSLPexact approaches. inherently limited increasing problem sizescapabilities anytime algorithms unclear. tackle problem, recently proposeduse hybrid methods combining exact metaheuristic approaches. consideredhybridization evolutionary algorithms (a stochastic population-based searchmethod) endowed tabu search (a local search method)(Gallardo, Cotta, & Fernandez,2006a). resulting algorithm memetic algorithm (MA; see Section 2.5). usedmechanism recombining solutions, providing best possible childparental set. Experimental tests indicated algorithm provided optimal nearoptimal results acceptable computational cost. Subsequently, studied extendedmulti-level models previous hybrid algorithm hybridizedbranch-and-bound derivative, namely beam search (BS)(Gallardo, Cotta, & Fernandez,2006b). Studies influence variable clustering multi-parent recombinationperformance algorithm also conducted. results indicatedvariable clustering detrimental problem also multi-parent recombination improves performance algorithm. best knowledge,heuristic approaches applied problem date.section, previous research problem included extended.new contributions, redone experiments using improved implementation543fiGallardo, Cotta, & Fernandezbucket elimination crossover operator, described Section 3.1. Additionally,present extensive experimental analysis BS/MA hybrid described (Gallardoet al., 2006b), analyzing sensitivity parameters. also propose new hybridalgorithm uses technique mini-buckets improve lower boundspartial solutions considered BS part hybrid algorithm. new algorithmobtained hybridization, different levels, complete solving techniques (BE),incomplete deterministic methods (BS MB) stochastic algorithms (MAs).experimental analysis shows new proposal consistently finds optimal solutionsMDSLP instances n = 20 considerably less time previous approachesreported literature. Finally, order test scalability approach, novelhybrid algorithm run large instances MDSLP optimalsolution currently unknown. results successful, algorithm performedstate-of-the-art level, providing solutions equal better bestones reported date literature. readability reasons, many particular technicaldetails different algorithms MDSLP omitted, fully describedaccompanying report (Gallardo, Cotta, & Fernandez, 2008). rate, modelMDSLP WCSP presented Appendix A.4.2 Memetic Algorithm MDSLPFirst all, develop MDSLP. MA, n n board representedbinary n n matrix. Based stratified gradient provided penalty based fitnessfunction measures number violated constraints distance feasibility(prioritizing former latter), efficient local search strategy exploresset solutions obtained flipping exactly one cell configuration devised. orderescape local optima, tabu-search scheme used (line 19 Figure 2).uses crossover operation described Section 3.1 (line 12 Figure 2).One interesting property operator described limited recombiningtwo board configurations, instead generalized recombine numberconsidering domains consisting values variable parents.multi-parental capability also explored rest paper.evaluate usefulness described hybrid recombination operator, set experiments problem sizes n = 12 n = 20 realized (recall optimalsolutions MDSLP known n = 20). experiments performedusing steady-state evolutionary algorithm (popsize = 100, pm = 1/n2 , pX = 0.9, binarytournament selection). aim maintaining diversity, duplicated individualsallowed population. Algorithms run optimal solution foundtime limit exceeded. time limit set 3 minutes problem instances size 12gradually increased 60 seconds size increment. algorithminstance size, 20 independent runs made. experiments paperperformed Pentium IV PC (2400MHz 512MB RAM) SuSE Linux.base algorithm using two-dimensional version SPX (single-pointcrossover) recombination, endowed tabu search local improvement.algorithm termed MATS , shown capable finding feasible solutions systematically, solving optimality instances n < 15 (see MATS Figure 5a).544fiSolving WCSPs Memetic/Exact Hybrid Algorithms109.598.587.576.565.554.543.532.521.510.5025MABEMABE1FMABE2FMATSArity=2Arity=4Arity=8Arity=1622.520% distance optimum% distance optimumAlthough performance algorithm degrades larger instances, provides distributions solutions whose average relative distance optimum less 5.29%cases. contrasts case plain EAs, incapable finding evenfeasible solution runs (Gallardo et al., 2006a).17.51512.5107.552.51213141516171819020121314151617181920instance sizeinstance size(a)(b)Figure 5: Relative distances optimum different (a) algorithms (b) arities sizesranging 12 20. subsequent figures, box summarizes 20 runs, boxes comprise second third quartiles distribution(i.e., inner 50%), horizontal line marks median, plus sign indicatesmean, circles indicate results median 1.5 timesinterquartile-distance.MATS firstly compared MAs endowed performing recombination.Since use recombination higher computational cost simple blindrecombination, guarantee recombining two infeasible solutionsresult feasible solution, defined three variants MAs:first one, called MA-BE, always used perform recombination.second, termed MA-BE1F , require least one parents feasibleorder apply BE; otherwise blind recombination used.last variant, identified MA-BE2F , require two parents feasible,thus restrictive application BE.evaluating variants, intend explore computational tradeoffs involvedapplication embedded component MA. algorithms,mutation performed prior recombination order take advantage good solutionsprovided BE. Figure 5a shows empirical performance different algorithms.Results show MA-BE returns significantly better results MATS . MA-BE2Ffind slightly better solutions MA-BE smaller instances (n {13, 15, 16}),545fiGallardo, Cotta, & Fernandezlarger instances winner MA-BE. seems effort saved recombiningunfeasible solutions improve performance algorithm. Note alsothat, larger instances, MA-BE1F better MA-BE2F . correlates wellfact used frequently former latter.mentioned Section 3.1, optimal recombination scheme use readily extended multi-parent recombination (Eiben, Raue, & Ruttkay, 1994): arbitrary numbersolutions contribute constituent rows constructing new solution. Additional experiments done explore effect capability MA-BE. Figure 5bshows results obtained MA-BE different number parents recombined(arities 2, 4, 8 16). arity = 2, algorithm able find optimum solutioninstances except n = 18 n = 20 (the relative distance optimumbest solution found less 1.04% cases). Runs arity = 4 cannot findoptimum solutions remaining instances, note distribution improvescases. Clearly, performance algorithm deteriorates combining4 parents due higher computational cost BE. Variable clustering couldused alleviate higher computational cost, results performance degradationsince coarser granularity pieces information hinders information mixing (Cotta& Troya, 2000; Gallardo et al., 2006b).4.3 BS/MA Hybrid Algorithm MDSLPsection evaluate instantiation BS hybrid algorithm describedSection 3.2 MDSLP, called BS-MA-BE. beam selection (line 10 Figure 4),simple quality measure defined partial solutions, whose value eitherpartial configuration unstable, number dead cells otherwise. methodologySection 4.2 (20 executions performed algorithm instancesize), arities {2, 3, 4}. setting remaining parameterskbw = 2000 (preliminary tests indicated value reasonable), kM{0.3 n, 0.5 n, 0.75 n}, i.e., best 2000 nodes kept level BS algorithm,30%, 50% 75% levels BS tree initially descendedrun. respect termination conditions, execution withinhybrid algorithm consists 1000 generations, time limits imposed hybridalgorithms, run n iterations BS.Figure 6a shows results different values parameter kM . order bettercompare distributions, number optimal solutions obtained algorithm(out 20 executions) shown box plot. kM = 0.3 n, performanceresulting algorithm improves significantly original MA. Note BS-MA-BE,using arity 2 parents, able find optimum cases except n = 18(this instance solved arity = 4). distributions different instance sizessignificantly improved. n < 17 arity {2, 3, 4}, algorithm consistently findsoptimum runs. instances, solution provided algorithmalways within 1.05% optimum, except n = 18, relative distanceoptimum worst solution 1.3%. two charts show that, general,performance algorithm deteriorates increasing values kM parameter.may due low quality bounds used BS part.546fiSolving WCSPs Memetic/Exact Hybrid Algorithms750Arity=2Arity=3Arity=4500k= 0.75 n25001500kMA = 0.50 n1250Arity=2Arity=3Arity=4201kMA = 0.75 n1.50 0 0011 1 03 9 100.5Time best solution(s)2.5100075050025020 20 20 20 20 20 20 20 20 20 20 20 20 20 200% distance optimum2.5022000k0 5kMA = 0.50 n1.50 0 0= 0.30 n1750112 0 1150010 7 100.5125020 20 20 20 20 20 20 20 20 20 20 20 20 20 2002.510002750kMA = 0.30 n1.50 0 34 2 615002 2 111 13 142500.520 20 20 20 20 20 20 20 20 20 20 20 20 20 2001213141516171819020(a)121314151617181920instance sizeinstance size(b)Figure 6: (a) Relative distances optimum (b) time best solution differentarities BS-MA-BE KM {0.3 n, 0.5 n, 0.75 n}, sizes ranging12 20. numbers box indicate many times optimalsolution found.Regarding execution times, Figure 6b shows time distributions (in seconds) reachbest solution needed algorithms. Although BS-MA-BE requires timeMA-BE, time needed remains reasonable instances, always less2000 seconds. Note also execution time increases arity, timeneeded perform crossover operator. hand, executiontime decreases larger values kM number executions decreases,although, already remarked, quality solutions worsens.4.4 Improving Lower Bound using MB MDSLPsimple quality measure beam selection used previous section depends solelypart solution already constructed. section, experimentallystudy use MB technique compute tight, yet computationally inexpensive,547fiGallardo, Cotta, & Fernandezlower bound remanning part configuration aim improvingperformance BS part hybrid algorithm. Basically, idea cluster cellsrow board metavariable. metavariables partitionedcolumns n/M cells each. Finally, resort MB estimate best costextensions partial board configuration considering columns.summing estimations column extensions, bound best board extensionpartial solution obtained. section, experimented = 3 (i.e., threecolumns row), although complexity still high, approachused reduce further, considering columns.2.52.5% distance optimum2kMA = 0.30 n1.516 1811813 196 13 190.520 20 20 20 20 20 20 20 20 20 20 20 20 20 2001213141520 201617201820192kMA = 0.50 n1.51911613 18 194 13 160.52020 20 20 20 20 20 20 20 20 20 20 20 20 20 20instance size20 2020 200(a)(b)2.51.5Arity=2Arity=3Arity=4750Time b. sol(s)2kMA = 0.75 n19116 14 1614 13 183 12 19Arity=2Arity=3Arity=4500k= 0.75 n2500.520 20 20 20 20 20 20 20 20 20 20 20 20 20 2020020(c)121314151617181920instance size0(d)Figure 7: (a)-(c) Relative distances optimum using different aritiesBS-MA-BE-MB KM {0.3 n, 0.5 n, 0.75 n}, sizes ranging12 20. (d) Time (in seconds) best solution different aritiesBS-MA-BE-MB kM = 0.75 n, sizes ranging 12 20.Experiments repeated hybrid algorithm equipped new lower bound,BS-MA-BE-MB. Figure 7a-7c shows results experiments values kM{0.3 n, 0.5 n, 0.75 n}. algorithm finds optimum instances aritiesrelative distance optimum worst solution found less 1.05%cases. best results obtained arity = 4, although requires slightlyexecution time. Note also BS-MA-BE-MB less sensitive setting parameterkM , means execution times reduced considerably using large valueparameter (see Figure 7d). particular combination parameters kM = 0.75 narity = 4 provides excellent results lower computational cost, execution timesalways 570 seconds n 6 20. comparison, recall approachliterature solve instances described Larrosa et al. (2005) requires33 minutes n = 18, 15 hours n = 19 2 days n = 20,approaches unaffordable n > 15. Note however times correspondcomputational platform different ours. order make fairer comparison, executed548fiSolving WCSPs Memetic/Exact Hybrid Algorithmsalgorithm Larrosa et al. 3 platform. case, required 1867 seconds(i.e., 31 minutes) order solve n = 18 instance, 1 day18 hours solve n = 20 instance. values close times reportedLarrosa et al. (2005), hence indicate computational platforms fairlycomparable.1.5% distance1Arity=2Arity=3Arity=40.500.5122242628instance sizeFigure 8: Relative distances best known solutions using different aritiesBS-MA-BE-MB kM = 0.3 n, large instances (i.e., sizes 22,24, 26, 28). Note improvement best known solutions sizes 2426.Figure 9: New best known maximum density still lifes n {24, 26}.Table 2: Optimal solutions SMDLP.nopt12 1368 7914921510616120171371815419172201922223224 26276 326283784.5 Results Large Instancesalready mentioned, currently approach available tackle MDSLPn > 20. Larrosa et al. (2005) tried algorithm n = 21 n = 22, could3. Available http://www.lsi.upc.edu/~larrosa/publications/LIFE-SOURCE-CODE.tar.gz . Timen = 19 could obtained code provided Larrosa et al. used even sizedinstances.549fiGallardo, Cotta, & Fernandezsolve instances within week CPU. large instances,solutions relaxations problem known. One relaxations, knownsymmetrical maximum density still life problem (SMDSLP), proposed BoschTrick (2002), consists considering symmetric boards (either horizontally2vertically) reduces search space 2n 2ndn/2e .alone find vertically symmetric still lifes, considering variable domainssets contain symmetric rows. Larrosa Morancho (2003) Larrosa et al.(2005) used algorithm solve SMDSLP instances considered farpaper (i.e., n {12 . . 20}), well large instances (i.e., n {22, 24, 26, 28}).results summarized Table 2, shows instance size optimalsymmetrical solution (as number dead cells). Clearly, cost optimal symmetricstill lifes upper bounds MDSLP, additionally observedtight n 6 20. Results n > 20 currently best known solutionsinstances.also run algorithm (BS-MA-BE-MB) large instances (i.e., n{22, 24, 26, 28}), compare results symmetrical solutions instances. Results (displayed Figure 8) show algorithm able find two new best knownsolutions MDSLP, namely n = 24 n = 26. 275 324 deadcells respectively new solutions. solutions pictured Figure 9.also worth noting algorithm could also find solution 325 dead cellsn = 26 instance. instances, algorithm could reach best known solutions consistently. computation mini-Buckets large instances doneconsidering four clustered cost functions variables row board,complexity using three cost functions still high.5. ConclusionsMany problems modeled WCSPs. One exact technique usedtackle problems BE. However, high space complexity exact technique,makes approach impractical large instances. case, one resort minibuckets get approximate solution, although complexity large.work, presented several proposals hybridization MBmemetic algorithms beam search order get effective heuristics shownrepresent promising models.experimentally evaluated model MDSLP, excellent exampleWCSP. highly constrained nature typical many optimization scenarios. difficulty solving problem illustrates limitations classical optimization approaches,highlights capabilities proposed approaches. Indeed, experimental results positive, solving large instances MDSLP optimality. Amongdifferent models presented, must distinguish new algorithm resultinghybridization, different levels, complete solving techniques (i.e., bucket elimination),incomplete deterministic methods (i.e., beam search mini-buckets) stochastic algorithms (i.e., memetic algorithms). algorithm empirically produces good-quality results,solving optimality large instances constrained problem relativelyshort time, also providing new best known solutions large instances.550fiSolving WCSPs Memetic/Exact Hybrid Algorithmsfuture work, plan consider complete versions hybrid algorithm.involves use appropriate data structures store yet considered promisingbranch-and-bound nodes. memory requirements course grow enormouslysize problem instance considered, interesting analyze computational tradeoffs algorithm anytime technique.Acknowledgmentswould like thank Javier Larrosa valuable comments, helped us improvesignificantly preliminary version paper. Thanks also due reviewersconstructive comments. work partially supported Spanish MCInngrant TIN2008-05941 (Nemesis).Appendix A. MDSLP WCSPshown Larrosa Morancho (2003) Larrosa et al. (2005), MDSLPwell formulated WCSPs. end, n n board configuration representedn-dimensional vector (r1 , r2 , . . . , rn ). vector component encodes (as binarystring) row, j-th bit row ri (noted rij ) represents state j-th celli-th row (a value 1 represents live cell value 0 dead cell).Two functions rows useful describe constraints must satisfiedvalid configuration. first one,Xzeroes(a) =(1 ai ),(1)1 6i6nreturns number dead cells row (i.e., number zeroes binary string a).second one,Adjs(a) = Adjs 0 (a, 1 , 0 )l,0Adjs (a, , l ) = Adjs 0 (a, + 1 , l + 1 ),max(l, Adjs 0 (a, + 1 , 0 )),(2)i>nai = 1ai = 0,computes maximum number adjacent living cells row a. also introduce ternarypredicate, Stable(ri1 , r , ri+1 ), takes three consecutive rows board configurationsatisfied if, if, cells central row stable (i.e., cells row rremain unchanged next iteration game):Stable(a, b, c) =V16i6n S(a, b, c, i)2 6 (a, b, c, i) 6 3, bi = 1S(a, b, c, i) =(a, b, c, i) 6= 3,bi = 0P(a, b, c, i) = max(1,i1)6j6min(n,i+1) (aj + bj + cj ) bi ,(3)(a, b, c, i) number living neighbors cell bi , assuming c rowsrow b.551fiGallardo, Cotta, & FernandezMDSLP formulated WCSP using n cost functions fi , {1 . . n}.Accordingly, fn binary scope last two rows board (var(fn ) = {rn1 , rn })defined as:,Stable(a, b, 0 ) Adjs(b) > 2fn (a, b) =(4)zeroes(b), otherwise.first line checks cells row rn stable, whereas second one checksnew cells produced nn board. Note pair rows representingunstable configuration assigned cost , whereas stable one assigned numberdead cells (to minimized).{2 . . n 1}, corresponding fi cost functions ternary scope var(fi ) ={ri1 , ri , ri+1 } defined as:,fi (a, b, c) =zeroes(b),Stable(a, b, c) (a1 = b1 = c1 = 1 ) (an = bn = cn = 1 )(5)otherwise.case, boundary conditions checked left right board. regardscost function f1 , binary scope first two rows board (var(f1 ) = {r1 , r2 })specified similarly fn :,Stable(0 , b, c) Adjs(b) > 2(6)f1 (b, c) =zeroes(b), otherwise.ReferencesAhuja, R. K., Ergun, O., Orlin, J. B., & Punnen, A. P. (2002). survey large-scaleneighborhood search techniques. Discrete Appl. Math., 123 (1-3), 75102.Arnborg, S. (1985). Efficient algorithms combinatorial problems graphs boundeddecomposability - survey. BIT, 2, 223.Back, T. (1996). Evolutionary Algorithms Theory Practice. Oxford University Press,New York NY.Back, T., Fogel, D., & Michalewicz, Z. (1997). Handbook Evolutionary Computation.Oxford University Press, New York NY.Barr, A., & Feigenbaum, E. (1981). Handbook Artificial Intelligence. Morhan Kaufmann,New York NY.Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press, NewYork NY.Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint satisfactionoptimization. Journal ACM, 44 (2), 201236.Bosch, R., & Trick, M. (2002). Constraint programming hybrid formulations threelife designs. International Workshop Integration AI TechniquesConstraint Programming Combinatorial Optimization Problems, CP-AI-OR02, pp.7791.552fiSolving WCSPs Memetic/Exact Hybrid AlgorithmsCabon, B., de Givry, S., Lobjois, L., Schiex, T., & Warners, J. P. (1999). Radio linkfrequency assignment. Constraints, 4 (1), 7989.Cheng, K. C. K., & Yap, R. H. C. (2005). Ad-hoc global constraints life. van Beek,P. (Ed.), Principles Practice Constraint Programming CP2005, Vol. 3709Lecture Notes Computer Science, pp. 182195, Berlin Heidelberg. Springer.Cheng, K. C. K., & Yap, R. H. C. (2006). Applying ad-hoc global constraints caseconstraint still-life. Constraints, 11, 91114.Cotta, C., & Troya, J. (2000). influence representation granularity heuristicforma recombination. Carroll, J., Damiani, E., Haddad, H., & Oppenheim, D.(Eds.), ACM Symposium Applied Computing 2000, pp. 433439. ACM Press.Cotta, C., & Troya, J. (2003). Embedding branch bound within evolutionary algorithms.Applied Intelligence, 18(2), 137153.Culberson, J. (1998). futility blind search: algorithmic view free lunch.Evolutionary Computation, 6 (2), 109128.Dechter, R. (1997). Mini-buckets: general scheme generating approximations automated reasoning. 15th International Joint Conference Artificial Intelligence,pp. 12971303, Nagoya, Japan.Dechter, R. (1999). Bucket elimination: unifying framework reasoning. ArtificialIntelligence, 113 (1-2), 4185.Detcher, R., & Rish, I. (2003). Mini-buckets: general scheme bounded inference.Journal ACM, 50 (2), 107153.Eiben, A., Raue, P.-E., & Ruttkay, Z. (1994). Genetic algorithms multi-parent recombination. Davidor, Y., Schwefel, H.-P., & Manner, R. (Eds.), Parallel ProblemSolving Nature III, Vol. 866 Lecture Notes Computer Science, pp. 7887,Berlin Heidelberg. Springer.Elkies, N. D. (1998). still-life problem generalizations. Engel, P., & Syta, H.(Eds.), Voronois Impact Modern Science, Book 1, pp. 228253. Institute Math,Kyiv.Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58 (1-3), 2170.Gallardo, J. E., Cotta, C., & Fernandez, A. J. (2008). Finding still lifes memetic/exacthybrid algorithms. CoRR, Available http://arxiv.org/abs/0812.4170.Gallardo, J., Cotta, C., & Fernandez, A. (2007). hybridization memetic algorithms branch-and-bound techniques. IEEE Transactions Systems, ManCybernetics, part B, 37 (1), 7783.Gallardo, J. E., Cotta, C., & Fernandez, A. J. (2006a). memetic algorithm bucketelimination still life problem. Gottlieb, J., & Raidl, G. (Eds.), EvolutionaryComputation Combinatorial Optimization, Vol. 3906 Lecture Notes ComputerScience, pp. 7385, Berlin Heidelberg. Springer.553fiGallardo, Cotta, & FernandezGallardo, J. E., Cotta, C., & Fernandez, A. J. (2006b). multi-level memetic/exact hybridalgorithm still life problem. Runarsson, T. P., et al. (Eds.), Parallel ProblemSolving Nature IX, Vol. 4193 Lecture Notes Computer Science, pp. 212221,Berlin Heidelberg. Springer.Gardner, M. (1970). fantastic combinations John Conways new solitaire game.Scientific American, 223, 120123.Gelain, M., Pini, M. S., Rossi, F., & Venable, K. B. (2007). Dealing incomplete preferences soft constraint problems. Bessiere, C. (Ed.), Principles PracticeConstraint Programming CP 2007, Vol. 4741 Lecture Notes Computer Science,pp. 286300, Berlin Heidelberg. Springer.Glover, F. (1989). Tabu search part I. ORSA Journal Computing, 1 (3), 190206.Glover, F. (1990). Tabu search part II. ORSA Journal Computing, 2 (1), 432.Hart, W., Krasnogor, N., & Smith, J. (2005). Recent Advances Memetic Algorithms, Vol.166 Studies Fuzziness Soft Computing. Springer, Berlin Heidelberg.Kask, K., & Detcher, R. (2001). general scheme automatic generation searchheuristics specification dependencies. Artificial Intelligence, 129, 91131.Khemmoudj, M. O. I., & Bennaceur, H. (2007). Valid inequality based lower boundsWCSP. Bessiere, C. (Ed.), Principles Practice Constraint ProgrammingCP 2007, Vol. 4741 Lecture Notes Computer Science, pp. 394408, BerlinHeidelberg. Springer.Krasnogor, N., & Smith, J. (2005). tutorial competent memetic algorithms: model,taxonomy, design issues. IEEE Transactions Evolutionary Computation, 9 (5),474488.Larrosa, J., & Morancho, E. (2003). Solving still life soft constraints bucketelimination. Principles Practice Constraint Programming CP2003, Vol.2833 Lecture Notes Computer Science, pp. 466479, Berlin Heidelberg. Springer.Larrosa, J., Morancho, E., & Niso, D. (2005). practical use variable eliminationconstraint optimization problems: still life case study. Journal ArtificialIntelligence Research, 23, 421440.Larrosa, J., & Schiex, T. (2004). Solving weighted CSP maintaining arc consistency.Artificial Intelligence, 159 (1-2), 126.Lawler, E., & Wood, D. (1966). Branch bounds methods: survey. Operations Research,4 (4), 669719.Lematre, M., Verfaillie, G., Bourreau, E., & Laburthe, F. (2001). Integrating algorithmsweighted CSP constraint programming framework. International WorkshopModelling Solving Problems Soft Constraints, Paphos, Cyprus.Marinescu, R., & Dechter, R. (2007). Best-first and/or search graphical models.Twenty-Second AAAI Conference Artificial Intelligence, pp. 11711176, Vancouver, Canada. AAAI Press.Moscato, P., & Cotta, C. (2003). gentle introduction memetic algorithms. HandbookMetaheuristics, pp. 105144. Kluwer Academic Press, Boston, Massachusetts, USA.554fiSolving WCSPs Memetic/Exact Hybrid AlgorithmsMoscato, P., & Cotta, C. (2007). Memetic algorithms. Gonzalez, T. (Ed.), HandbookApproximation Algorithms Metaheuristics, chap. 27. Chapman & Hall/CRCPress.Nonobe, K., & Ibaraki, T. (2001). improved tabu search method weightedconstraint satisfaction problem. INFOR, 39 (2), 131151.Puchinger, J., & Raidl, G. (2005). Combining metaheuristics exact algorithmscombinatorial optimization: survey classification. Mira, J., & Alvarez, J.(Eds.), Artificial Intelligence Knowledge Engineering Applications: BioinspiredApproach, Vol. 3562 Lecture Notes Computer Science, pp. 4153, Berlin Heidelberg. Springer.Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued constraint satisfaction problems: hardeasy problems. 14th International Joint Conference Artificial Intelligence,pp. 631637, Montreal, Canada.Smith, B. M. (2002). dual graph translation problem life. Hentenryck, P. V.(Ed.), Principles Practice Constraint Programming - CP2002, Vol. 2470Lecture Notes Computer Science, pp. 402414, Berlin Heidelberg. Springer.Verfaillie, G., Lematre, M., & Schiex, T. (1996). Russian doll search solving constraintoptimization problems. Thirteenth National Conference Artificial IntelligenceEighth Innovative Applications Artificial Intelligence Conference, AAAI / IAAI96, pp. 181187. AAAI Press / MIT Press.Wolpert, D., & Macready, W. (1997). free lunch theorems optimization. IEEETransactions Evolutionary Computation, 1 (1), 6782.555fiJournal Artificial Intelligence Research 35 (2009) 343-389Submitted 08/08; published 06/09Automated Reasoning Modal Description Logics viaSAT Encoding: Case Study Km /ALC-SatisfiabilityRoberto Sebastianiroberto.sebastiani@disi.unitn.itMichele Vescovimichele.vescovi@disi.unitn.itDISI, Universita di TrentoVia Sommarive 14, I-38123, Povo, Trento, ItalyAbstractlast two decades, modal description logics applied numerousareas computer science, including knowledge representation, formal verification, databasetheory, distributed computing and, recently, semantic web ontologies.reason, problem automated reasoning modal description logicsthoroughly investigated. particular, many approaches proposed efficientlyhandling satisfiability core normal modal logic Km , notational variant,description logic ALC. Although simple structure, Km /ALC computationallyhard reason on, satisfiability PSpace-complete.paper start exploring idea performing automated reasoning tasksmodal description logics encoding SAT, handled stateof-the-art SAT tools; previous approaches, begin investigationsatisfiability Km . propose efficient encoding, test extensiveset benchmarks, comparing approach main state-of-the-art tools available.Although encoding necessarily worst-case exponential, experimentsnotice that, practice, approach handle problemsreach approaches, performances comparable with, evenbetter than, current state-of-the-art tools.1. Motivations Goalslast two decades, modal description logics provided essential frameworkmany applications numerous areas computer science, including artificial intelligence, formal verification, database theory, distributed computing and, recently, semantic web ontologies. reason, problem automated reasoning modaldescription logics thoroughly investigated (e.g., Fitting, 1983; Ladner, 1977;Baader & Hollunder, 1991; Halpern & Moses, 1992; Baader, Franconi, Hollunder, Nebel, &Profitlich, 1994; Massacci, 2000). particular, research modal description logicsfollowed two parallel routes seminal work Schild (1991), provedcore modal logic Km core description logic ALC one notational variantother. Since then, analogous results produced bunch logics,that, nowadays two research lines mostly merged one research flow.Many approaches proposed efficiently reasoning modal descriptionlogics, starting problem checking satisfiability core normal modallogic Km notational variant, description logic ALC (hereafter simply Km ).classify follows.c2009AI Access Foundation. rights reserved.fiSebastiani & Vescoviclassic tableau-based approach (Fitting, 1983; Baader & Hollunder, 1991; Massacci, 2000) based construction propositional tableau branches,recursively expanded demand generating successor nodes candidate Kripkemodel. Kris (Baader & Hollunder, 1991; Baader et al., 1994), Crack (Franconi,1998), LWB (Balsiger, Heuerding, & Schwendimann, 1998) among mainrepresentative tools approach.DPLL-based approach (Giunchiglia & Sebastiani, 1996, 2000) differsprevious one mostly fact Davis-Putnam-Logemann-Loveland (DPLL)procedure, treats modal subformulas propositions, used insteadclassic propositional tableaux procedure nesting level modal operators. KSAT (Giunchiglia & Sebastiani, 1996), ESAT (Giunchiglia, Giunchiglia,& Tacchella, 2002) *SAT (Tacchella, 1999), representative toolsapproach.two approaches merged modern tableaux-based approach,extended work expressive description logics provide sophisticatereasoning functions. Among tools employing approach, recall FaCT/FaCT++DLP (Horrocks & Patel-Schneider, 1999), Racer (Haarslev & Moeller, 2001). 1translational approach (Hustadt & Schmidt, 1999; Areces, Gennari, Heguiabehere,& de Rijke, 2000) modal formula encoded first-order logic (FOL),encoded formula decided efficiently FOL theorem prover (Areces et al.,2000). Mspass (Hustadt, Schmidt, & Weidenbach, 1999) representativetool approach.CSP-based approach (Brand, Gennari, & de Rijke, 2003) differs tableauxbased DPLL-based ones mostly fact CSP (Constraint SatisfactionProblem) engine used instead tableaux/DPLL. KCSP representativetool approach.Inverse-method approach (Voronkov, 1999, 2001), search procedure basedinverted version sequent calculus (which seen modalizedversion propositional resolution). KK (Voronkov, 1999) representativetool approach.Automata-theoretic approach, (a symbolic representation based BDDsBinary Decision Diagrams of) tree automaton accepting tree modelsinput formula implicitly built checked emptiness (Pan, Sattler, & Vardi,2002; Pan & Vardi, 2003). KBDD (Pan & Vardi, 2003) representative toolapproach.1. Notice universal agreement terminology tableaux-based DPLL-based.E.g., tools like FaCT, DLP, Racer often called tableau-based, although useDPLL-like algorithm instead propositional tableaux handling propositional componentreasoning (Horrocks, 1998; Patel-Schneider, 1998; Horrocks & Patel-Schneider, 1999; Haarslev & Moeller,2001).344fiAutomated Reasoning Modal Description Logics via SAT EncodingPan Vardi (2003) presented also encoding K-satisfiability QBF-satisfiability(which PSpace-complete too), combined use state-of-the-art QBF(Quantified Boolean Formula) solver. call approach QBF-encoding approach.best knowledge, last four approaches far restricted satisfiabilityKm only, whilst translational approach applied numerous modaldescription logics (e.g. traditional modal logics like Tm S4m , dynamic modallogics) relational calculus.significant amount benchmarks formulas produced testing effectiveness different techniques (Halpern & Moses, 1992; Giunchiglia, Roveri, & Sebastiani, 1996; Heuerding & Schwendimann, 1996; Horrocks, Patel-Schneider, & Sebastiani,2000; Massacci, 1999; Patel-Schneider & Sebastiani, 2001, 2003).last two decades also witnessed impressive advance efficiencypropositional satisfiability techniques (SAT), brought large previouslyintractable problems reach state-of-the-art SAT solvers. success SATtechnologies motivated impressive efficiency reached current implementationsDPLL procedure, (Davis & Putnam, 1960; Davis, Longemann, & Loveland, 1962),most-modern variants (Silva & Sakallah, 1996; Moskewicz, Madigan, Zhao, Zhang, &Malik, 2001; Een & Sorensson, 2004). Current implementations handle formulasorder 107 variables clauses.consequence, many hard real-world problems successfully solvedencoding SAT (including, e.g., circuit verification synthesis, scheduling, planning,model checking, automatic test pattern generation , cryptanalysis, gene mapping). Effectiveencodings SAT proposed also satisfiability problems quantifier-freeFOL theories interest formal verification (Strichman, Seshia, & Bryant,2002; Seshia, Lahiri, & Bryant, 2003; Strichman, 2002). Notably, successful SAT encodingsinclude also PSpace-complete problems, like planning (Kautz, McAllester, & Selman, 1996)model checking (Biere, Cimatti, Clarke, & Zhu, 1999).paper start exploring idea performing automated reasoning tasksmodal description logics encoding SAT, handled state-ofthe-art SAT tools; previous approaches, begin investigationsatisfiability Km .theory, task may look hopeless worst-case complexity issues: fact,exceptions, satisfiability problem modal description logicsNP, typically PSpace-complete even harder PSpace-complete Km (Ladner,1977; Halpern & Moses, 1992) encoding worst-case non polynomial. 2practice, however, considerations allow discarding approachmay competitive state-of-the-art approaches. First, non-polynomial boundsworst-case bounds, formulas may different behaviorspathological formulas found textbooks. (E.g., notice exponentialitybased hypothesis unboundedness parameter like modal depth;Halpern & Moses, 1992; Halpern, 1995.) Second, tricks encoding may allowreducing size encoded formula significantly. Third, amount RAM2. implicitly make assumption NP 6= PSpace.345fiSebastiani & Vescovimemory current computers order GBytes current SAT solverssuccessfully handle huge formulas, encoding many modal formulas (at leasthard solve also competitors) may reach SAT solver.Finally, even PSpace-complete logics like Km , also state-of-the-art approachesguaranteed use polynomial memory.paper show that, least satisfiability Km , exploiting smartoptimizations encoding SAT-encoding approach becomes competitive practiceprevious approaches. extent, contributions paper manyfold.propose basic encoding Km formulas purely-propositional ones, proveencoding satisfiability-preserving.describe optimizations encoding, form preprocessingon-the-fly simplification. techniques allow significant (and casesdramatic) reductions size resulting Boolean formulas, performancesSAT solver thereafter.perform extensive empirical comparison main state-of-the-arttools available. show that, despite NP-vs.-PSpace issue, approachhandle problems reach approaches,performances comparable with, sometimes even better than,current state-of-the-art tools. perspective, surprisingcontribution paper.byproduct work, obtain empirical evaluation current tools Km satisfiability available, extensive terms amount varietybenchmarks number representativeness tools evaluated.aware evaluation recent literature.also stress fact approach encoder interfaced everySAT solver plug-and-play manner, benefit free every improvementtechnology SAT solvers made available.Content. paper structured follows. Section 2 provide necessarybackground notions modal logics SAT. Section 3 describe basic encodingKm SAT. Section 4 describe discuss main optimizations, providemany examples. Section 5 present empirical evaluation, discuss results.Section 6 present related work current research trends. Section 7conclude, describe possible future evolutions.six-page preliminary version paper, containing basic ideas presentedhere, presented SAT06 conference (Sebastiani & Vescovi, 2006). readersconvenience, online appendix provided, containing plots Section 5 full size.Moreover, order make results reproducible, encoder, benchmarksrandom generators seeds used also available online appendix.346fiAutomated Reasoning Modal Description Logics via SAT Encoding2. Backgroundsection provide necessary background modal logic Km (Section 2.1)SAT DPLL procedure (Section 2.2).2.1 Modal Logic Kmrecall basic definitions properties Km . Given non-empty set primitivepropositions = {A1 , A2 , . . .}, set modal operators B = {21 , . . . , 2m }, constants True False (that denote respectively > ) languageKm least set formulas containing A, closed set propositional connectives {, , , , } set modal operators B {31 , . . . , 3m }. Notationally,use Greek letters , , , , , denote formulas language Km (Km -formulashereafter). Notice consider {, } together B group primitive connectives/operators, defining remaining standard way, is: 3r2r , 1 2 (1 2 ), 1 2 (1 2 ), 1 2(1 2 ) (2 1 ). (Hereafter formulas like implicitly assumedsimplified V,.) Notationally,VW mean VWW that, ,often write ( li ) jVlj Wfor clause j li j lj , ( li ) ( j lj )conjunction clauses j ( li lj ). Further, often write 2r 3r meaning onespecific/generic modal operator, assumed r = 1, . . . , m; denote2ir nested application 2r operator times: 20r := 2i+1r := 2r 2r .call depth , written depth(), maximum number nested modal operators .call propositional atom every primitive proposition A, propositional literalevery propositional atom (positive literal) negation (negative literal). call modalatom every formula either form 2r form 3r .order make presentation uniform, avoid considering polaritysubformulas, adopt traditional representation Km -formulas (introduced, farknow, Fitting, 1983 widely used literature, e.g. Fitting, 1983; Massacci,2000; Donini & Massacci, 2000) following table:(1 2 )(1 2 )(1 2 )11112222(1 2 )(1 2 )(1 2 )11112222r3r 12r 10r11r2r 13r 10r11non-literal Km -formulas grouped four categories: (conjunctive),(disjunctive), (existential), (universal). Importantly, formulas occurmain formula positive polarity only. allows disregarding issue polaritysubformulas.semantic modal logics given means Kripke structures. Kripke structureKm tuple = hU, L, R1 , . . . , Rm i, U set states, L functionL : U 7 {T rue, F alse}, Rr binary relation states U.abuse notation write u instead u U. call situation pair M, u,Kripke structure u M. binary relation |= modal formula347fiSebastiani & Vescovisituation M, u defined follows:M, u |= >;M, u 6|= ;M, u |= Ai , AiM, u |= Ai , AiM, u |=M, u |=M, u |= rM, u |= rL(Ai , u) = rue;L(Ai , u) = F alse;M, u |= 1 M, u |= 2 ;M, u |= 1 M, u |= 2 ;M, w |= 0r w U s.t. Rr (u, w) holds M;M, w |= 0r every w U s.t. Rr (u, w) holds M.M, u |= read M, u satisfy Km (alternatively, M, u Km -satisfies). say Km -formula satisfiable Km (Km -satisfiable henceforth)exist u s.t. M, u |= . (When causes ambiguity, sometimesdrop prefix Km -.) say w successor u Rr iff Rr (u, w) holdsM.problem determining Km -satisfiability Km -formula decidablePSPACE-complete (Ladner, 1977; Halpern & Moses, 1992), even restricting languagesingle Boolean atom (i.e., = {A1 }; Halpern, 1995); impose bound modaldepth Km -formulas, problem reduces NP-complete (Halpern, 1995).detailed description Km including, e.g., axiomatic characterization, decidabilitycomplexity results refer reader works Halpern Moses (1992),Halpern (1995).Km -formula said Negative Normal Form (NNF) written termssymbols 2r , 3r , , propositional literals Ai , Ai (i.e., negations occurpropositional atoms A). Every Km -formula converted equivalentone NNF () recursively applying rewriting rules: 2r =3r , 3r =2r ,(1 2 )=(1 2 ), (1 2 )=(1 2 ), =.Km -formula said Box Normal Form (BNF) (Pan et al., 2002; Pan & Vardi,2003) written terms symbols 2r , 2r , , , propositional literals Ai ,Ai (i.e., diamonds there, negations occur boxespropositional atoms A). Every Km -formula converted equivalent oneBNF () recursively applying rewriting rules: 3r =2r , (1 2 )=(12 ), (1 2 )=(1 2 ), =.2.2 Propositional Satisfiability DPLL Algorithmstate-of-the-art SAT procedures evolutions DPLL procedure (Davis &Putnam, 1960; Davis et al., 1962). high-level schema modern DPLL engine, adaptedone presented Zhang Malik (2002), reported Figure 1. Booleanformula CNF (Conjunctive Normal Form); assignment initially empty,updated stack-based manner.main loop, decide next branch(, ) chooses unassigned literal laccording heuristic criterion, adds . (This operation called decision,l called decision literal number decision literals operationcalled decision level l.) inner loop, deduce(, ) iteratively deduces literals l348fiAutomated Reasoning Modal Description Logics via SAT Encoding1.2.3.4.5.6.7.8.9.10.11.12.13.14.SatValue DPLL (formula , assignment ) {(1) {decide next branch(, );(1) {status = deduce(, );(status == sat)return sat;else (status == conflict) {blevel = analyze conflict(, );(blevel == 0) return unsat;else backtrack(blevel,, );}else break;}}}Figure 1: Schema modern SAT solver engine based DPLL.deriving current assignment updates accordingly; step repeatedeither satisfies , falsifies , literals deduced, returning sat,conflict unknown respectively. (The iterative application Boolean deduction stepsdeduce also called Boolean Constraint Propagation, BCP.) first case, DPLL returnssat. second case, analyze conflict(, ) detects subset causedconflict (conflict set) decision level blevel backtrack. blevel == 0,conflict exists even without branching, DPLL returns unsat. Otherwise,backtrack(blevel, , ) adds clause (learning) backtracks blevel(backjumping), updating accordingly. third case, DPLL exits inner loop,looking next decision.Notably, modern DPLL implementations implement techniques, like two-watchedliteral scheme, allow extremely efficient handling BCP (Moskewicz et al., 2001;Zhang & Malik, 2002). Old versions DPLL used implement also Pure-Literal Rule(PLR) (Davis et al., 1962): one proposition occurs positively (resp. negatively)formula, safely assigned true (resp. false). Modern DPLL implementations,however, often implement anymore due computational cost. muchdeeper description modern DPLL-based SAT solvers, refer reader literature(e.g., Zhang & Malik, 2002).3. Basic Encodingborrow notation Single Step Tableau (SST) framework (Massacci, 2000;Donini & Massacci, 2000). represent uniquely states labels , representednon empty sequences integers 1.nr11 .nr22 . ... .nrkk , s.t. label 1 represents root state,.nr represents n-th Rr -successor (where r {1, . . . , m}). little abusenotation, hereafter may say state meaning state labeled . calllabeled formula pair h, i, state label Km -formula,349fiSebastiani & Vescovicall labeled subformulas labeled formula h, labeled formulas h,subformula .Let Ah , injective function maps labeled formula h, i, s.t.form , Boolean variable Ah, . conventionally assume Ah, >i> Ah, . Let Lh, denote Ah, form , Ah, otherwise.Given Km -formula , encoder Km 2SAT builds Boolean CNF formula follows: 3defKm 2SAT () = Ah1,(1)Def (1, )def(2)def(3)def(4)def(5)Def (, >) = >Def (, ) = >Def (, Ai ) = >Def (, Ai ) = >defDef (, ) = (Lh,(Lh,1Lh,2 ))Def (, 1 ) Def (, 2 )(6)(Lh,1Lh,2 ))Def (, 1 ) Def (, 2 )(7)defDef (, ) = (Lh,r,j0r,j )defDef (, ) = (Lh, r,j Lh.j, r,j ) Def (.j,0^defrDef (, ) =((Lh, r Lh, r,i ) Lh.i,rr)Def(.i,).00(8)(9)everyh, r,ir,j mean r,j j-th distinct r formula labeled . NoticeNn(6) (7) generalize case n-ary obvious way:i=1VNNdefs.t.{, }, Def (, ) = (Lh, ni=1 Lh, ) ni=1 Def (, ). Althoughconceptuallytrivial, fact important practical consequence: order encodeNnoneneedsadding one Boolean variable rather n1, see Section 4.2.i=1Notice also rule (9) literals type Lh, r,i strictly necessary; fact,SAT problem must consider encode possibly occuring states,case, e.g., r,i formula occurring disjunction assigned false particularstate label (which, SAT, corresponds assign Lh, r,i false). situationlabeled formulas regarding state label .i useless, particular generatedexpansion formulas interacting r,i . 4assume Km -formulas represented DAGs (Direct Acyclic Graphs),avoid expansion Def (, ) once. variousDef (, ) expanded breadth-first manner wrt. tree labels, is,possible expansions (newly introduced) completed startingexpansions different state label 0 , different state label expandedorder introduced (thus expansions given state always handleddeeper state). Moreover, following done Massacci (2000),assume that, , Def (, )s expanded order: /, , . Thus,Def (, r ) expanded expansion Def (, r,i )s, Def (, r )3. say formula CNF represent clauses implications, according notationdescribed beginning Section 2.4. Indeed, (9) finite conjunction. fact number -subformulas obviously finite Kmbenefits finite-tree-model property (see, e.g., Pan et al., 2002; Pan & Vardi, 2003).350fiAutomated Reasoning Modal Description Logics via SAT Encodinggenerate one clause ((Lh, r Lh, r,i ) Lh.i, 0r ) one novel definition Def (.i, 0r )Def (, r,i ) expanded. 5Intuitively, easy see Km 2SAT () mimics construction SST tableauexpansion (Massacci, 2000; Donini & Massacci, 2000). following fact.Theorem 1. Km -formula Km -satisfiable corresponding Booleanformula Km 2SAT () satisfiable.complete proof Theorem 1 found Appendix A.Notice that, due (9), number variables clauses Km 2SAT () may growexponentially depth(). accordance stated HalpernMoses (1992).Example 3.1 (NNF). Let nnf (3A1 3(A2 A3 )) 2A1 2A2 2A3 . 6easy see nnf K1 -unsatisfiable: 3-atoms impose least one atom Aitrue least one successor root state, whilst 2-atoms impose atomsAi false successor states root state. Km 2SAT (nnf ) is: 71.2.3.4.5.6.7.8.9.10.11.12.(((((((((((Ah1, nnfAh1, nnf (Ah1, 3A1 3(A2 A3 )i Ah1, 2A1 Ah1,Ah1, 3A1 3(A2 A3 )i (Ah1, 3A1 Ah1, 3(A2 A3 )i ) )Ah1, 3A1 Ah1.1, A1 )Ah1, 3(A2 A3 )i Ah1.2, A2 A3 )(Ah1, 2A1 Ah1, 3A1 ) Ah1.1, A1 )(Ah1, 2A2 Ah1, 3A1 ) Ah1.1, A2 )(Ah1, 2A3 Ah1, 3A1 ) Ah1.1, A3 )(Ah1, 2A1 Ah1, 3(A2 A3 )i ) Ah1.2, A1 )(Ah1, 2A2 Ah1, 3(A2 A3 )i ) Ah1.2, A2 )(Ah1, 2A3 Ah1, 3(A2 A3 )i ) Ah1.2, A3 )Ah1.2, A2 A3 (Ah1.2, A2 Ah1.2, A3 ) )(1)2A2 Ah1,2A3 ) )(6)(7)(8)(8)(9)(9)(9)(9)(9)(9)(7)run Boolean constraint propagation (BCP), 3. reduces implicate disjunction. first element Ah1, 3A1 assigned true, BCP conflict 4.6. set false, second element Ah1, 3(A2 A3 )i assigned true,BCP conflict 12. Thus Km 2SAT (nnf ) unsatisfiable.34. Optimizationsbasic encoding Section 3 rather naive, much improved many extents,order reduce size output propositional formula, make easier solveDPLL, both. distinguish two main kinds optimizations:5. practice, even definition Km 2SAT recursive, Def expansions performed groupedstates. precisely, Def (.n, ) expansions, formula every defined n, donetogether (in /, , order exposed) necessarily Def (, ) expansionscompleted.6. K1 -formulas omit box diamond indexes, i.e., write 2, 3 21 , 31 .7. examples report end line, i.e. clause, number Km 2SATencoding rule applied generate clause. also drop application rules (2), (3), (4)(5).351fiSebastiani & VescoviPreprocessing steps, applied input modal formula encoding.Among them, Pre-conversion BNF (Section 4.1), Atom Normalization(Section 4.2), Box Lifting (Section 4.3), Controlled Box Lifting (Section 4.4).On-the-fly simplification steps, applied Boolean formula construction. Among them, On-the-fly Boolean Simplification Truth Propagation Boolean Operators (Section 4.5) Truth PropagationModal Operators (Section 4.6), On-the-fly Pure-Literal Reduction (Section 4.7),On-the-fly Boolean Constraint Propagation (Section 4.8).analyze techniques detail.4.1 Pre-conversion BNFMany systems use pre-convert input Km -formulas NNF (e.g., Baader et al.,1994; Massacci, 2000). approach, instead, pre-convert BNF (like, e.g.,Giunchiglia & Sebastiani, 1996; Pan et al., 2002). approach, advantagelatter representation that, one 2r occurs positively negatively (like, e.g.,(2r ...) (2r ...) ...), occurrences 2r labeledBoolean atom Ah, 2r , hence always assigned truth value DPLL.NNF, instead, negative occurrence 2r rewritten 3r (nnf ()),two distinct Boolean atoms Ah, 2r (nnf ())i Ah, 3r (nnf ())i generated; DPLLassign truth value, creating hidden conflict may require extraBoolean search reveal. 8Example 4.1 (BNF). consider BNF variant nnf formula Example 3.1,bnf = (2A1 2(A2 A3 )) 2A1 2A2 2A3 . before, easysee bnf K1 -unsatisfiable. Km 2SAT (bnf ) is: 91.2.3.4.5.6.7.8.9.10.11.12.(((((((((((Ah1, bnfAh1, bnf (Ah1, (2A1 2(A2 A3 ))i Ah1, 2A1 Ah1, 2A2 Ah1,Ah1, (2A1 2(A2 A3 ))i (Ah1, 2A1 Ah1, 2(A2 A3 )i ) )Ah1, 2A1 Ah1.1, A1 )Ah1, 2(A2 A3 )i Ah1.2, (A2 A3 )i )(Ah1, 2A1 Ah1, 2A1 ) Ah1.1, A1 )(Ah1, 2A2 Ah1, 2A1 ) Ah1.1, A2 )(Ah1, 2A3 Ah1, 2A1 ) Ah1.1, A3 )(Ah1, 2A1 Ah1, 2(A2 A3 )i ) Ah1.2, A1 )(Ah1, 2A2 Ah1, 2(A2 A3 )i ) Ah1.2, A2 )(Ah1, 2A3 Ah1, 2(A2 A3 )i ) Ah1.2, A3 )Ah1.2, (A2 A3 )i (Ah1.2, A2 Ah1.2, A3 ) )(1)2A3 )) (6)(7)(8)(8)(9)(9)(9)(9)(9)(9)(7)Unlike NNF formula nnf Example 3.1, Km 2SAT (bnf ) found unsatisfiabledirectly BCP. fact, unit-propagation Ah1, 2A1 2. causes Ah1, 2A18. Notice consideration holds every representation involving boxes diamonds;refer NNF simply popular representations.9. Notice valid clause 6. dropped. See explanation Section 4.5.352fiAutomated Reasoning Modal Description Logics via SAT Encoding3. false, one two (unsatisfiable) branches induced disjunctioncut priori. nnf , Km 2SAT recognize 2A1 3A1 onenegation other, two distinct atoms Ah1, 2A1 Ah1, 3A1 generated.Hence Ah1, 2A1 Ah1, 3A1 cannot recognized DPLL one negationother, s.t. DPLL may need exploring one Boolean branch more.3following assume formulas BNF (although optimizations follow work also representations).4.2 Normalization Modal AtomsOne potential source inefficiency DPLL-based procedures occurrenceinput formula semantically-equivalent though syntactically-different modal atoms 000 (e.g., 21 (A1 A2 ) 21 (A2 A1 )), recognized Km 2SAT .causes introduction duplicated Boolean atoms Ah, 0 Ah, 00 muchworse duplicated subformulas Def (, 0 ) Def (, 00 ). factnegative consequences, particular 0 00 occur negative polarity,causes creation distinct versions successor states, duplicationwhole parts output formula.Example 4.2. Consider Km -formula (1 21 (A2 A1 )) (2 21 (A1 A2 )) 3 ,s.t. 1 , 2 , 3 possibly-big Km -formulas. Km 2SAT creates two distinct atomsAh1, 21 (A2 A1 )i Ah1, 21 (A1 A2 )i two distinct formulas Def (1, 21 (A2 A1 ))Def (1, 21 (A1 A2 )). latter cause creation two distinct states 1.1 1.2.Thus, recursive expansion 21 -formulas occurring positively 1 , 2 , 3duplicated two states.3order cope problem, done Giunchiglia Sebastiani (1996),apply normalization steps modal atoms intent rewriting manypossible syntactically-different semantically-equivalent modal atoms syntacticallyidentical ones. achieved recursive application simple validitypreserving rewriting rules.Sorting: modal atoms internally sorted according criterion, atomsidentical modulo reordering rewritten atom (e.g., 2i (21 ) 2i (1 2 ) rewritten 2i (1 2 )).Flattening: associativity exploited combinationsflattened n-ary respectively (e.g., 2i (1 (2 3 )) 2i ((12 ) 3 ) rewritten 2i (1 2 3 )).Flattening also advantage reducing number novel atoms introducedencoding, consequence fact noticed Section 3. One possible drawbacktechnique reduce sharing subformulas (e.g., 2i ((1 2 ) 3 )2i ((1 2 ) 4 ), common part shared). However, empiricallyexperienced drawback negligible wrt. advantages flattening.353fiSebastiani & Vescovi4.3 Box Liftingsecond preprocessing Km -formula also rewritten recursively applyingKm -validity-preserving box lifting rules:(2r 1 2r 2 ) = 2r (1 2 ),(2r 1 2r 2 ) = 2r (1 2 ).(10)potential benefit reducing number r formulas, hence numberlabels .i take account expansion Def (, r )s (9). call liftingpreprocessing.Example 4.3 (Box lifting). apply rules (10) formula Example 4.1,bnf lift = 2(A1 A2 A3 ) 2(A1 A2 A3 ). Consequently,Km 2SAT (bnf lift ) is:1.2.3.4.5.6.Ah1, bnf lift( Ah1, bnf lift (Ah1, 2(A1 A2 A3 )i Ah1, 2(A1 A2 A3 )i ) )( Ah1, 2(A1 A2 A3 )i Ah1.1, (A1 A2 A3 )i )(( Ah1, 2(A1 A2 A3 )i Ah1, 2(A1 A2 A3 )i ) Ah1.1, (A1 A2 A3 )i )( Ah1.1, (A1 A2 A3 )i (Ah1.1, A1 Ah1.1, A2 Ah1.1, A3 ) )( Ah1.1, (A1 A2 A3 )i (Ah1.1, A1 Ah1.1, A2 Ah1.1, A3 ) ).(1)(6)(8)(9)(7)(6)Km 2SAT (bnf lift ) found unsatisfiable directly BCP clauses 1. 2.. onesuccessor state (1.1) considered. Notice 3., 4., 5. 6. redundant, 1.2. alone unsatisfiable. 1034.4 Controlled Box LiftingOne potential drawback applying lifting rules that, collapsing formula(2r 1 2r 2 ) 2r (1 2 ) (2r 1 2r 2 ) 2r (1 2 ), possibilitysharing box subformulas DAG representation input Km -formula reduced.order cope problem provide alternative policy applying boxlifting, is, apply rules (10) neither box subformula occurringimplicant (10) multiple occurrences. call policy controlled box lifting.Example 4.4 (Controlled Box Lifting). apply Controlled Box Lifting formulaExample 4.1, bnf clift = (2A1 2(A2 A3 )) 2A1 2(A2 A3 )since rules (10) applied among box subformulas except 2A1 ,10. actual implementation, trivial cases like bnf lift found unsatisfiable directlyconstruction DAG representations, encoding never generated.354fiAutomated Reasoning Modal Description Logics via SAT Encodingshared. follows Km 2SAT (bnf clift ) is:1.2.3.4.5.6.7.8.9.10.11.12.(((((((((((Ah1, bnf cliftAh1, bnf clift (Ah1, (2A1 2(A2 A3 ))i Ah1, 2A1 Ah1, 2(A2 A3 )i )Ah1, (2A1 2(A2 A3 ))i (Ah1, 2A1 Ah1, 2(A2 A3 )i ) )Ah1, 2A1 Ah1.1, A1 )Ah1, 2(A2 A3 )i Ah1.2, (A2 A3 )i )(Ah1, 2A1 Ah1, 2A1 ) Ah1.1, A1 )(Ah1, 2(A2 A3 )i Ah1, 2A1 ) Ah1.1, (A2 A3 )i )(Ah1, 2A1 Ah1, 2(A2 A3 )i ) Ah1.2, A1 )(Ah1, 2(A2 A3 )i Ah1, 2(A2 A3 )i ) Ah1.2, (A2 A3 )i )Ah1.1, (A2 A3 )i (Ah1.1, A2 Ah1.1, A3 ) )Ah1.2, (A2 A3 )i (Ah1.2, A2 Ah1.2, A3 ) )Ah1.2, (A2 A3 )i (Ah1.2, A2 Ah1.2, A3 ) )(1)(6)(7)(8)(8)(9)(9)(9)(9)(6)(7)(6)Km 2SAT (bnf clift ) found unsatisfiable directly BCP clauses 1., 2. 3.. Noticeunit propagation Ah1, 2A1 Ah1, 2(A2 A3 )i 2. causes implicatedisjunction 3. false.34.5 On-the-fly Boolean Simplification Truth Propagationfirst straightforward on-the-fly optimization applying recursively standardrewriting rules Boolean simplification formula like, e.g.,h, h,h, 1 h, (1 2 )ih, h,...,= h, i,= h, 1 i,= h, i,h, h,h, 1 h, (1 2 )ih, h,= h, i,= h, 1 i,= h, >i,propagation truth/falsehood Boolean operators like, e.g.,h,h, h, >ih, h, >i....= h, >i,= h, i,= h, >i,h, >ih, h,h, h,= h, i,= h, i,= h, i,Example 4.5. consider Km -formula bnf lift = 2(A1 A2 A3 )2(A1 A2 A3 ) Example 4.3 apply Boolean simplification rule h,h, = h, i, h, bnf lift simplified h, i.3One important subcase on-the-fly Boolean simplification avoids useless encodingincompatible r r formulas. BNF, fact, subformula 2r may occurstate positively negatively (like r = 2r r = 2r ).so, Km 2SAT labels occurrences 2r Boolean atom Ah, 2r ,produces recursively two distinct subsets clauses encoding, applying (8)2r (9) 2r respectively. However, latter step (9) generates valid clause(Ah, 2r Ah, 2r ) Ah.i, , avoid generating it. Consequently,355fiSebastiani & VescoviAh.i, occurs formula, Def (.i, ) generated,need defining h.i, i. 11Example 4.6. apply observation construction formulas Examples4.1 4.4, following facts:formula Km 2SAT (bnf ) Example 4.1, clause 6. valid thus dropped.formula Km 2SAT (bnf clift ) Example 4.4, valid clauses 6. 9.dropped, 12. generated.3Hereafter assume on-the-fly Boolean simplification applied also combinationtechniques described next sections.4.6 On-the-fly Truth Propagation Modal OperatorsTruth falsehood derive application techniques Section 4.5,Section 4.7 Section 4.8 may propagated on-the-fly also though modal operators.First, every , positive negative instances h, 2r >i safely simplifiedapplying rewriting rule h, 2r >i = h, >i.Second, notice following fact. positive occurrence h, 2r(we suppose wlog. r -formula ), 12 definition(8) (9)Def (, 2r ) = (Lh,2rDef (, 2r ) = ((Lh,2rAh.j,Lh,>i )Def (.j, >),(11)Lh.j,(12)2r ))Def (.j, )new label .j every 2r occurring positively . Def (, 2r ) reduces> Ah.j, >i Def (.j, >) reduce >. least another distinct formula 2r occurs positively , however, need .j label (11)(12) new label, re-use instead label .i introduced expansionDef (, 2r ), follows:Def (, 2r ) = (Lh,2rLh.i,)Def (.i, ).(13)Thus (11) dropped and, every h, 2r occurring positively, write:Def (, 2r ) = ((Lh,2rLh,2r )Lh.i,)Def (.i, )(14)instead (12). (Notice label .i introduced (13) rather label .j (11).)motivated fact Def (, 2r ) forces existence least onesuccessor imposes constraints formulas hold there,use already-defined successor state, any. fact importantbenefit eliminating useless successor states encoding.11. due fact may case Ah.i, generated anywayexpansion subformula, like, e.g., 2r ( ). case, Def (.i, ) mustgenerated anyway.12. E.g., 2r may result applying steps Section 4.1 Section 4.5 2r (2r A1 3r A1 ).356fiAutomated Reasoning Modal Description Logics via SAT EncodingExample 4.7. Let BNF K-formula:(A1 2A2 ) (A1 2) (A1 A3 ) (A1 A3 ) (A1 2A4 ) 2A4 .K-inconsistent, possible assignment {A1 , 2, 2A4 , 2A4 },K-inconsistent. Km 2SAT () encoded follows:1.2.3.4.5.6.7.8.9.10.11.Ah1,(Ah1, (Ah1, (A1 2A2 )i Ah1, (A1 2)i Ah1,Ah1, (A1 2A4 )i Ah1, 2A4 ))(Ah1, (A1 2A2 )i (Ah1, A1 Ah1, 2A2 ))(Ah1, (A1 2)i (Ah1, A1 Ah1, 2i ))(Ah1, (A1 A3 )i (Ah1, A1 Ah1, A3 ))(Ah1, (A1 A3 )i (Ah1, A1 Ah1, A3 ))(Ah1, (A1 2A4 )i (Ah1, A1 Ah1, 2A4 ))(Ah1, 2A2 Ah1.1, A2 )((Ah1, 2A4 Ah1, 2A2 ) Ah1.1, A4 )((Ah1, 2A4 Ah1, 2A2 ) Ah1.1, A4 )(Ah1, 2i Ah1.1, )12.13.((Ah1,((Ah1,Ah1, 2i ) Ah1.1, A4 )2A4 Ah1, 2i ) Ah1.1, A4 )2A4(1)(A1 A3 )i(6)(7)(7)(7)(7)(7)(8)(9)(9)(8)(9)(9)Clause 11. simplified >. (In practical implementation even generated.)Notice clauses 11., 12. 13. used label 1.1 clauses 8., 9. 10. rathernew label 1.2. Thus, one successor label generated.DPLL run Km 2SAT (), BCP 1. 2. immediately satisfiedimplicants removed 3., 4., 5., 6.. Thanks 5. 6., Ah1, A1 assignedfalse, causes 3. satisfied forces assignment literals Ah1, 2i ,Ah1, 2A4 BCP 3. 7. hence Ah1.1, , Ah1.1, A4 Ah1.1, A4 BCP12. 13., causing contradiction.3worth noticing (14) strictly necessary correctness encodingeven another -formula occurs . (E.g., Example 4.7, without 12. 13.formula Km 2SAT () would become satisfiable Ah1, 2A2 could safely assignedtrue DPLL, would satisfy 8., 9. 10..)Hereafter assume technique applied also combination techniques described Section 4.5 next sections.4.7 On-the-fly Pure-Literal ReductionAnother technique, evolved proposed Pan Vardi (2003), applies PureLiteral Reduction (PLR) on-the-fly construction Km 2SAT ().label clauses containing atoms form Ah, generated,occurs positively [resp. negatively], safely assigned true [resp.false], hence clauses containing Ah, dropped. 13 consequence,13. actual implementation reduction performed directly within intermediate data structure,clauses never generated.357fiSebastiani & Vescoviatom Ah,reached.0become pure, process repeated fixpointExample 4.8. Consider formula bnf Example 4.1. constructionKm 2SAT (bnf ), 1.-8. generated, clause containing atoms formAh1.1, generated. notice Ah1.1, A2 Ah1.1, A3 occur negatively, safely assigned false. Therefore, 7. 8. safelydropped. discourse applies lately Ah1.2, A1 9.. resulting formula foundinconsistent BCP. (In fact, notice Example 4.1 atoms Ah1.1, A2 , Ah1.1, A3 ,Ah1.2, A1 play role unsatisfiability Km 2SAT (bnf ).)3remark differences PLR Pure-Literal Reduction technique proposed Pan Vardi (2003). KBDD (Pan et al., 2002; Pan & Vardi, 2003),Pure-Literal Reduction preprocessing step applied input modal formula,either global level (i.e. looking pure-polarity primitive propositions whole formula) or, effectively, different modal depths (i.e. looking pure-polarity primitivepropositions subformulas nesting level modal operators).technique much fine-grained, PLR applied on-the-fly single-stategranularity, obtaining much stronger reduction effect.Example 4.9. Consider BNF Km -formula bnf discussed Examples 4.14.8: bnf = (2A1 2(A2 A3 )) 2A1 2A2 2A3 . immediate seeprimitive propositions A1 , A2 , A3 occur every modal depth polarities,technique Pan Vardi (2003) produces effect formula.34.8 On-the-fly Boolean Constraint PropagationOne major problem basic encoding Section 3 purely-syntactic,is, consider possible truth values subformulas, effectpropagation Boolean modal connectives. particular, Km 2SAT applies(8) [resp. (9)] every -subformula [resp. -subformula], regardless fact truthvalues deterministically assigned labeled subformulas h1, mayallow dropping labeled -/-subformulas, thus prevent need encodingthem.One solution problem applying Boolean Constraint Propagation (BCP)on-the-fly construction Km 2SAT (), starting fact Ah1, musttrue. contradiction found, Km 2SAT () unsatisfiable, formulaexpanded further, encoder returns formula . 14 BCP allowsdropping one implication (6)-(9) without assigning implicate literals,namely Lh, , h, needs defined, Def (, ) mustexpanded. 15 Importantly, dropping Def (, r,j ) -formula h, r,j preventsgenerating label .j (8) successor labels .j. 0 (corresponding subtreestates rooted .j), corresponding labeled subformulas encoded.14. sake compatibility standard SAT solvers, actual implementation returns formulaA1 A1 .15. make consideration Footnote 11: Lh.j, generated also expansionsubformula, (e.g., 2r ( )), (another instance of) Def (.i, ) must generatedanyway.358fiAutomated Reasoning Modal Description Logics via SAT EncodingExample 4.10. Consider Example 4.1, suppose apply on-the-fly BCP.construction 1., 2. 3. Km 2SAT (bnf ), atoms Ah1, bnf , Ah1, (2A1 2(A2 A3 ))i ,Ah1, 2A1 , Ah1, 2A2 Ah1, 2A3 deterministically assigned true BCP.causes removal 3. first-implied disjunct Ah1, 2A1 , needgenerate Def (1, 2A1 ), hence label 1.1. defined 4. generated.building 5., Ah1.2, (A2 A3 )i , unit-propagated. label 1.1. defined, 6.,7. 8. generated. construction 5., 9., 10., 11. 12.,applying BCP contradiction found, Km 2SAT () .analogous situation happens bnf lift Example 4.3: building 1. 2.contradiction found BCP, s.t. Km 2SAT returns without expanding formulafurther. discourse holds bnf clift Example 4.4: building 1., 2. 3.contradiction found BCP, s.t. Km 2SAT returns without expanding formulafurther.34.9 Paradigmatic Example: Halpern & Moses Branching Formulas.Among optimizations described Section 4, on-the-fly BCP fareffective. order better understand fact, consider paradigmatic examplebranching formulas Kh Halpern Moses (1992, 1995) (also called k branch nset benchmark formulas proposed Heuerding Schwendimann, 1996)unsatisfiable version (called k branch p above-mentioned benchmark suite).Given single modality 2, integer parameter h, primitive propositions16D0 , . . . , Dh+1 , P1 , . . . , Ph , formulas Kh defined follows:Khdef= D0 D1h^2i (depth determined branching),(15)i=0defdepth =h+1^(Di Di1 ),i=1h^( Pi 2(Di Pi ))determined =Di,(Pi 2(Di Pi ))i=1h1^3(Di+1 Di+2 Pi+1 )defbranching =(Di Di+1 )3(Di+1 Di+2 Pi+1 )def(16)(17). (18)i=0conjunction formulas depth, determined branching repeated everynesting level modal operators (i.e. every depth): depth captures relationDi every level; determined states that, Pi true [false] state depth i,true [false] successor states depth i; branching states that, everynode depth i, possible find two successor states depth + 1 Pi+1true one false other. value parameter h, Kh K-satisfiable,every Kripke model satisfies least 2h+1 1 states. fact, Kh buildway force construction binary-tree Kripke model depth h + 1,16. sake better readability, adopt description given Halpern Moses (1992)without converting formulas BNF. fact affect discussion.359fiSebastiani & Vescoviwhose leaves encodes distinct truth assignment primitive propositions P1 , . . . , Ph ,whilst Di true states occurring depth tree (andthus denotes level nesting).unsatisfiable counterpart formulas proposed Heuerding Schwendimann (1996)(whose negations valid formulas called k branch p previously-mentionedbenchmark suite, exposed details Section 5.1.1) obtained conjoining (15) formula:2h Pb h c+1(19)3(where bxc integer part x) forces atom Pb h c+1 true depth-h3states candidate Kripke model, incompatible fact remainingspecifications say false half depth-h states. 17formulas pathological many approaches (Giunchiglia & Sebastiani,2000; Giunchiglia, Giunchiglia, Sebastiani, & Tacchella, 2000; Horrocks et al., 2000). particular, introducing on-the-fly BCP, used pet hate Km 2SAT approach, caused generation huge Boolean formulas. fact, due branching(18), Kh contains 2h 3-formulas (i.e., -formulas) every depth. Therefore, Km 2SATencoder Section 3 consider 1 + 2h + (2h)2 + ... + (2h)h+1 = ((2h)h+2 1)/(2h 1)distinct labels, hh+1 times number labeling statesactually needed. (None optimizations Sections 4.1-4.7 helpformulas, neither BNF encoding atom normalization causes sharingsubformulas, formulas already lifted form, literal occurs pure. 18 )pathological behavior mostly overcome applying on-the-fly-BCP,truth values deterministically assigned subformulas Kh on-thefly-BCP, prevent encoding even 2/3-subformulas.fact, consider branching determined formulas occurring Kh genericdepth {0...h}, determine states level tree. statesD0 , ..., Dd forced true Dd+1 , ..., Dh+1 forced false,d-th conjunct branching (all conjuncts = h) forced true thuscould dropped. Therefore, 2 3-formulas per non-leaf level could consideredinstead, causing generation 2h+1 1 labels overall. Similarly, states levellast h conjuncts determined forced true could dropped, reducingsignificantly number 2-formulas considered.easy see exactly happens applying on-the-fly-BCP. fact,suppose construction Km 2SAT (Kh ) reached depth (that is, pointevery state level d, Def (, )s Def (, )s expandedDef (, ) Def (, ) expanded yet). Then, BCP deterministically assigns trueliterals Lh, D0 , ..., Lh, Dd false Lh, Dd+1 , ..., Lh, Dh+1 , removes oneconjuncts branching, two Def (, )s 2h ones actually expanded;similarly, last h conjuncts determined removed, correspondingDef (, )s expanded.17. Heuerding Schwendimann explain choice index b h3 c + 1. understandalso choices would done job.18. precisely, one literal, Dh+1 , occurs pure branching, assigning plays rolesimplifying formula.360fiAutomated Reasoning Modal Description Logics via SAT Encoding1e+081e+071e+08BNF-lift-plrBNF-nolift-plr1e+071000BNF-lift-plrBNF-nolift-plr1001e+061e+06BNF-liftBNF-noliftBNF-lift-bcpBNF-nolift-bcpBNF-lift-plr-bcpBNF-nolift-plr-bcp10000010000BNF-liftBNF-noliftBNF-lift-bcpBNF-nolift-bcpBNF-lift-plr-bcpBNF-nolift-plr-bcp1000001000010001000100100101010BNF-liftBNF-noliftBNF-lift-plrBNF-nolift-plr10.1115101520(a) k branch n, var#0.0151015205(b) k branch n, clause#1e+081e+07BNF-lift-bcpBNF-nolift-bcpBNF-lift-plr-bcpBNF-nolift-plr-bcp1e+071520(c) k branch n, cpu time1e+08BNF-lift-plrBNF-nolift-plr101000BNF-lift-plrBNF-nolift-plr1001e+061e+06BNF-liftBNF-nolift100000BNF-liftBNF-nolift10000010100001000010001000100100BNF-liftBNF-noliftBNF-lift-plrBNF-nolift-plr10.1BNF-lift-bcpBNF-nolift-bcpBNF-lift-plr-bcpBNF-nolift-plr-bcp101BNF-lift-bcpBNF-nolift-bcpBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-lift-bcpBNF-nolift-bcpBNF-lift-plr-bcpBNF-nolift-plr-bcp10151015(d) k branch p, var#200.0151015(e) k branch p, clause#205101520(f) k branch p, cpu timeFigure 2: Empirical analysis Km 2SAT Halpern & Moses formulas wrt. depthparameter h, different options encoder. 1st row: k branch n, corresponding Km 2SAT (Kh ), formulas (satisfiable); 2nd row: k branch p, correhsponding Km 2SAT (Kh 2 Pb hc+1 ), formulas (unsatisfiable). Left: number3Boolean variables; center: number clauses; right: total CPU time requestedencoding+solving (where solving step performed Rsat).See Section 5 technical details.361fiSebastiani & Vescovihfar unsatisfiable version Km 2SAT (Kh 2 Pb hc+1 ) concerned,3expansion reaches depth h, thanks (19), Lh, P h generated deterministicallyb 3 c+1assigned true BCP every depth-h label ; thanks determined branching,BCP assigns literals Lh, P1 , ..., Lh, Ph deterministically, Lh, P h assignedb 3 c+1false 50% depth-h labels . causes contradiction, encoderstops expansion returns .Figure 2 shows growth size CPU time required encode solveKhKm 2SAT (Kh ) (1st row) Km 2SAT (h 2 Pb hc+1 ) (2nd row) wrt. parameter h,3eight combinations following options encoder: without box-lifting,without on-the-fly PLR, without on-the-fly BCP. (Notice log scaleaxis.) Figure 2(d) plots four versions -xxx-bcp (with on-the-flyBCP) coincide line value 1 (i.e, one variable) Figure 2(e) coincidehorizontal line value 2 (i.e, two clauses), corresponding fact1-variable/2-clause formula A1 A1 returned (see Footnote 14).notice facts. First, formulas, eight plots always collapse twogroups overlapping plots, representing four variants without on-the-fly BCPrespectively. shows box-lifting on-the-fly PLR almost irrelevantencoding formulas, causing little variations time required encoder(Figures 2(c) 2(f)); notice enabling on-the-fly PLR alone permits encode (butsolve) one problem wrt. versions without on-the-fly PLRBCP. Second, four versions on-the-fly-BCP always outperform several ordersmagnitude without option, terms size encoded formulas CPUtime required encode solve them. particular, case unsatisfiablevariant (Figure 2, second row) encoder returns formula, actual workrequired SAT solver (the plot Figure 2(f) refers encoding time).5. Empirical Evaluationorder verify empirically effectiveness approach, performed veryextensive empirical test session 14,000 Km /ALC formulas. implementedencoder Km 2SAT C++, flags corresponding optimizations exposed previous section: (i) NNF/BNF, performing pre-conversion NNF/BNFencoding; (ii) lift/ctrl.lift/nolift, performing respectively Box Lifting,Controlled Box Lifting Box Lifting encoding; (iii) plr on-the-fly PureLiteral Reduction performed (iv) bcp on-the-fly Boolean Constraint Propagationperformed. techniques introduced Section 4.2, Section 4.5 Section 4.6hardwired encoder. Moreover, pre-conversion BNF almost always producessmaller formulas NNF, set BNF flag default.combination Km 2SAT tried several SAT solvers encoded formulas (including Zchaff 2004.11.15, Siege v4, BerkMin 5.6.1, MiniSat v1.13, SATElite v1.0, SAT-Elite GTI 2005 submission 19 , MiniSat 2.0 061208 Rsat 1.03).19. preliminary evaluation available SAT solvers also tried SAT-Elite preprocessorreduce size SAT formula generated Km 2SAT without bcp option solve it.However, even preprocessing signinificantly reduce size formula, turned362fiAutomated Reasoning Modal Description Logics via SAT Encodingpreliminary evaluation intensive experiments selected Rsat 1.03(Pipatsrisawat & Darwiche, 2006), produced best overall performancesbenchmark suites (although performance gaps wrt. SAT tools, e.g. MiniSat2.0, dramatic).downloaded available versions state-of-the-art tools Km -satisfiability.empirical evaluation 20 selected Racer 1-7-24 (Haarslev & Moeller, 2001)*SAT 1.3 (Tacchella, 1999) best representatives tableaux/DPLL-basedtools, Mspass v 1.0.0t.1.3 (Hustadt & Schmidt, 1999; Hustadt et al., 1999) 21best representative FOL-encoding approach, KBDD (unique version) (Pan et al.,2002; Pan & Vardi, 2003) 22 representative automata-theoretic approach.representative CSP-based inverse method approaches could used. 23Notice tools Racer experimental tools, far Km 2SATprototype, many (e.g. *SAT KBDD) longer maintained.Finally, representative QBF-encoding approach, selected K-QBFtranslator (Pan & Vardi, 2003) combined sKizzo version 0.8.2 QBF solver(Benedetti, 2005), turned far 24 best QBF solver benchmarks among freely-available QBF solvers QBF2006 competition (Narizzano,Pulina, & Tacchella, 2006). (In evaluation considered tools : 2clsQ, SQBF,preQuantori.e. preQuel +Quantor Quantor 2.11, Semprop 010604.)tests presented section performed two-processor Intel Xeon3.0GHz computer, 1 MByte Cache processor, 4 GByte RAM, Red HatLinux 3.0 Enterprise Server, four processes run parallel. reportingresults one Km 2SAT +Rsat version, CPU times reported sums20.21.22.23.24.preprocessing time-expensive overall time spent preprocessingsolving reduced problem higher solving directly original encoded SAT formula.selection SAT solver, order select tools used empiricalevaluation, performed preliminary evaluation smaller benchmark suites (i.e. LWBand, sometimes, TANCS 2000 ones; see later). Importantly, preliminary evaluation Racerturned definitely efficient FaCT++, able solve problems less time.Also, order meet reviewers suggestions, repeated preliminary evaluation latestversions FaCT++ (v1.2.3, March 5th, 2009) version Racer used paper.evaluation Racer solves ten problems FaCT++ LWB benchmark,one hundred problems FaCT++ whole TANCS 2000 suite. Also 2m -CNFrandom problems Racer outperforms FaCT++. (We include online appendix plotscomparison Racer FaCT++.)run Mspass options -EMLTranslation=2 -EMLFuncNary=1 -Sorts=0-CNFOptSkolem=0 -CNFStrSkolem=0 -Select=2 -Split=-1 -DocProof=0 -PProblem=0 -PKept=0-PGiven=0, suggested Km -formulas Mspass README file. also triedoptions, former gave best performances.KBDD recompiled run increased internal memory bound 1 GB.moment KK freely available, failed attempt obtaining authors.KCSP prolog piece software, difficult compare performances wrt. optimizedtools common platform; moreover, KCSP maintained since 2005, competitive wrt. state-of-the-art tools (Brand, 2008). tools like leanK, 2KE, LWB, Kriscompetitive ones listed (Horrocks et al., 2000). KSAT (Giunchiglia & Sebastiani, 1996,2000; Giunchiglia et al., 2000) reimplemented *SAT.Unlike choice SAT solver, performance gaps best choice otherssignificant: e.g., LWB benchmark (see later), sKizzo able solve nearly 90 problemsbest QBF competitor.363fiSebastiani & Vescoviencoding Rsat solving times. reporting results K-QBF +sKizzo,CPU times reported due sKizzo time spent K-QBFconverter negligible.anticipate that, formulas benchmark suites, tools test i.e.variants Km 2SAT +Rsat state-of-the-art Km -satisfiability solversagreed satisfiability/unsatisfiability result terminating within timeout.Remark 1. Due big number empirical tests performed huge amountdata plotted, due limitations size, order make plots clearlydistinguishable figures, limited number plots included followingpart paper, considering meaningful ones regardingchallenging benchmark problems faced. sake readers convenience, however,full-size versions plots many plots regarding not-exposed results (alsoeasier problems), available online appendix, together filesdata. discussing empirical evaluation may include considerationsalso results.5.1 Test Descriptionperformed empirical evaluation three different well-known benchmarkssuites Km /ALC problems: LWB (Heuerding & Schwendimann, 1996), random 2m -CNF (Horrocks et al., 2000; Patel-Schneider & Sebastiani, 2003) TANCS2000 (Massacci & Donini, 2000) benchmark suites. aware publiclyavailable benchmark suite Km /ALC-satisfiability literature. three groupsbenchmark formulas allow us test effectiveness approach large numberproblems various sizes, depths, hardness characteristics, total amount14,000 formulas.particular, benchmark formulas allow us fairly evaluate different toolsmodal component Boolean component reasoning intrinsic Km -satisfiability problem, discuss later Section 5.4.following describe three benchmark suites.5.1.1 LWB Benchmark Suitefirst group benchmark formulas used LWB benchmark suite usedcomparison Tableaux98 (Heuerding & Schwendimann, 1996). consists 9 classesparametrized formulas (each two versions, provable p not-provable n 25 ),total amount 378 formulas. parameter allows creating formulas increasing sizedifficulty.benchmark methodology test formulas class, increasing difficulty,one formula cannot solved within given timeout, 1000 seconds tests. 26result class parameters value largest (and hardest) formulasolved within time limit. parameter ranges 1 21 that,25. Since tools check Km -(un)satisfiability, formulas negated, negations provableformulas checked unsatisfiable, whilst negation formulas checkedsatisfiable.26. also set 1 GB file-size limit encoding produced Km 2SAT .364fiAutomated Reasoning Modal Description Logics via SAT Encodingsystem solve 21 instances class, result given 21. discussionbenchmark suite, refer reader work Heuerding Schwendimann (1996)Horrocks et al. (2000).5.1.2 Random 2m -CNF Benchmark Suitesecond group benchmark formulas, selected random 2m -CNF testbeddescribed Horrocks et al. (2000), Patel-Schneider Sebastiani (2003).generalization well-known random k-SAT test methods, final resultlong discussion communities modal description logics obtainsignificant flawless random benchmarks modal/description logics (Giunchiglia &Sebastiani, 1996; Hustadt & Schmidt, 1999; Giunchiglia et al., 2000; Horrocks et al., 2000;Patel-Schneider & Sebastiani, 2003).2m -CNF test methodology, 2m -CNF formula randomly generated accordingfollowing parameters:(maximum) modal depth d;number top-level clauses L;number literal per clause clauses k;number distinct propositional variables N ;number distinct box symbols m;percentage p purely-propositional literals clauses occurring depth < d, s.t.clause length k contains average p k randomly-picked Boolean literalsk p k randomly-generated modal literals 2r , 2r . 27(We refer reader works Horrocks et al., 2000, Patel-Schneider & Sebastiani,2003 detailed description.)typical problem set characterized fixed values d, k, N , m, p: Lvaried way empirically cover 100% satisfiable / 100% unsatisfiabletransition. words, many problems values d, k, N, m, pincreasing number clauses L generated, starting really small, typically satisfiableproblems (i.e. probability generating satisfiable problem near one) hugeproblems, increasing interactions among numerous clauses typically leadsunsatisfiable problems (i.e. makes probability generating satisfiable problemsconverging zero). Then, tuple five values problem set, certainnumber 2m -CNF formulas randomly generated, resulting formulas giveninput procedure test, maximum time bound. fractionformulas solved within given timeout, median/percentile valuesCPU times plotted ratio L/N . Also, fraction satisfiable/unsatisfiableformulas plotted better understanding.27. precisely, number Boolean literals clause bp kc (resp. dp ke) probabilitydp ke p k (resp. 1 (dp ke p k)). Notice typically smaller p, harderproblem (Horrocks et al., 2000; Patel-Schneider & Sebastiani, 2003).365fiSebastiani & VescoviFollowing methodology proposed Horrocks et al. (2000), Patel-SchneiderSebastiani (2003), fixed = 1, k = 3 100 samples per point tests,selected two groups: easier one, = 1, p = 0.5, N = 6, 7, 8, 9,L/N = 10..60, harder one, = 2, p = 0.6, 0.5, N = 3, 4, L/N = 30..150p = 0.6 L/N = 50..140 p = 0.5, varying L/N ratio steps 5, totalamount 13,200 formulas.test, imposed timeout 500 seconds per sample 28 calculatednumber samples solved within timeout, 50%th 90%th percentiles CPU time. 29 order correlate performances (un)satisfiabilitysample formulas, background plot also plot satisfiable/unsatisfiableratio.5.1.3 TANCS 2000 Benchmark SuiteFinally, third group benchmark formulas, used MODAL PSPACE divisionbenchmark suite used comparison TANCS 2000 (Massacci & Donini, 2000).contains satisfiable unsatisfiable formulas, scalable hardness. benchmark suite, call TANCS 2000, formulas constructed translating QBFformulas K using three translation schemas, namely Schmidt-Schauss-Smolka translation (240 problems many different depths, 19 112), Ladner translation(240 problems, depths range 19 112), Halpern translation(56 problems depth among: 20, 28, 40, 56, 80 112) (Massacci & Donini, 2000).done Massacci Donini, call classes easy, medium hard respectively.formulas class tested within timeout 1000 seconds. 30class, report number solved formulas (X axis) total (cumulative) CPUtime spent solving formulas (Y axes). class results plotted sortingsolved problems easiest one hardest one.5.2 Empirical Comparison Different Variants Km 2SATfirst evaluated various variants encoding combination Rsat.order avoid considering many combinations flags, considered BNFformat, grouped plr bcp one parameter plr-bcp, restricting thusinvestigation 6 combinations: BNF, lift/ctrl.lift/nolift, plr-bcp on/off.(We recall techniques introduced Section 4.2, Section 4.5 Section 4.6hardwired encoder.) expose analyze results wrt. three differentsuites benchmark problems.28. also 512 MB file-size limit encoding produced Km 2SAT .29. Due lack space sake clarity wont include paper 90%th percentilesplots. Further, reasons, well skip report plots regarding easiest classbenchmark suite (e.g. = 1 lower values N ). plots, however,found online appendix.30. also set 1 GB file-size limit encoding produced Km 2SAT .366fiAutomated Reasoning Modal Description Logics via SAT Encoding5.2.1 Results LWB Benchmark Suiteresults LWB benchmark suite summarized Table 1 Figure 3.Table 1(a) reports left block indexes hardest formulas encoded withinfile-size limit and, right block, hardest formulas solved withintimeout Rsat; Table 1(b) reports numbers variables clauses Km 2SAT (),referring hardest formulas solved within timeout Rsat (i.e., reportedright block Table 1(a)). instance, BNF-ctrl.lift-plr-bcp encodingk dum n(21) contains 11106 variables 14106 clauses; hardest k dum n problemsolved Rsat BNF-ctrl.lift-plr-bcp first solvedBNF-ctrl.lift.Looking numbers cases solved Table 1(a), notice introductionon-the-fly Pure Literal Reduction Boolean Constraint Propagation optimizationsreally effective produces consistent performance enhancement (the effectoptimizations eye-catching branching formulas k branch * see Section 4.9k path * formulas). also notice lift sometimes introduces slightimprovement.view Tables 1(a) 1(b) hides actual CPU times required encodesolve problems. Small gaps numbers Table 1(a) may correspond big gapsCPU time. order analyze also aspect, Figure 3 plotted total cumulativeamount CPU time spent variants Km 2SAT +Rsat solve problemsLWB benchmark, sorted hardness. plot, also considered threeoptions BNF, lift/ctrl.lift/nolift, plr bcp evaluatealso effect plr bcp separately. notice plots clearly clusteredthree groups increasing performance: BNF-*, BNF-*-plr, BNF-*-plr-bcp., *representing three options lift/ctrl.lift/nolift. highlights factsuite on-the-fly Pure Literal Reduction significantly improves performances,on-the-fly Boolean Constraint Propagation introduces drastic improvements,variations due Box Lifting minor wrt. two optimizations.Overall, configuration BNF-lift-plr-bcp turns best performersuite, tiny advantage wrt. BNF-ctrl.lift-plr-bcp.5.2.2 Results Random 2m -CNF Benchmark Suiteresults random 2m -CNF benchmark suite reported Figures 4 5.Figure 4 report 50%-percentile CPU times required encode solveformulas different Km 2SAT +Rsat variants hardest benchmarks problems.dont report percentage solved problems since always 100%, i.e. Km 2SAT+Rsat terminates within timeout every problem benchmark suite.tests depth = 1 (see results hardest problems classfirst row Figure 4) simply easy Km 2SAT +Rsat (but competitors,see Section 5.3) solved every sample formula less 1 second. Althoughtests exposed second third row Figure 4 challenging,solved within timeout well. noticed also results rather regular,since big gaps 50%- 90%-percentile values.367fiSebastiani & Vescovikkkkkkkkkkkkkkkkkkliftingbranch nbranch pd4 nd4 pdum ndum pgrz ngrz plin nlin ppath npath pph nph ppoly npoly pt4p nt4p p448142019212121217821212121611Km 2SAT , encodedplr-bcpyes ctrlyes441818441818888914 141414202021211919212121 21212121 21212121 21212121 21212177141588151621 21212121 21212121 21212121 212121666611 111111ctrl1818814212121212121141521212121611Km 2SAT + Rsat, solvedplr-bcpyes ctrlyes44417174441818888881414 141414202020212118181821212121 2121212121 2121212121 2121212121 212121777131488815162121 21212110111010102121 2121212121 212121565661010101111ctrl1718814212121212121131521112121611(a) Indexes hardest problems encoded (left)hardest problems solved (right).kkkkkkkkkkkkkkkkkkliftingbranch nbranch pd4 nd4 pdum ndum pgrz ngrz plin nlin ppath npath pph nph ppoly npoly pt4p nt4p p10001000120001900019000110001083001100011000503200200400012000number variables (103 )plr-bcpyesctrlyesctrl1000 1000 20000 20000 200001000 10000006000 12000 10000 26000 1000018000 1900000019000 19000 11000 11000 1100011000 11000 20000 19000 200001010555880.20.10.230202010200000012000 11000 10000 7000 1000012000 11000 26000 16000 26000300505030050133384202020020202020200202021000 4000 17000 14000 1700010000 12000 20000 18000 2000010001000170002800023000140001085001300013000503200200400012000number clauses (103 )plr-bcpyesctrlyesctrl1000 1000 23000 23000 230001000 10000009000 17000 16000 43000 1600025000 2800000023000 23000 14000 14000 1400013000 14000 26000 25000 260001010666880.30.20.250203030300000014000 13000 14000 9000 1300014000 13000 35000 20000 350003005050600501433145202020020202020200202022000 4000 20000 17000 2000011000 12000 24000 21000 24000(b) # variables # clauses hardest problems solved.Note: 0 means formula simplified Km 2SAT .Table 1: Comparison variants Km 2SAT +Rsat LWB benchmarks.368fiAutomated Reasoning Modal Description Logics via SAT Encoding10000BNF-lift (Rsat)BNF-nolift (Rsat)BNF-ctrl.lift (Rsat)BNF-lift-plr (Rsat)BNF-nolift-plr (Rsat)BNF-ctrl.lift-plr (Rsat)BNF-lift-plr-bcp (Rsat)BNF-nolift-plr-bcp (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)10001001010.150100150200250300Figure 3: Comparison different variants Km 2SAT +Rsat LWB problems.X axis: number solved problems; axis: total CPU time spent (sortingproblems easiest hardest).general, relevant performance gaps various configurationsbenchmark suite; seems majority cases ctrl.lift slightly beatsnolift nolift slightly beats lift. gaps even relevant considersize formulas generated (Figure 5). believe may due factrandom 2m -CNF formulas may contain lots shared subformulas 2r , liftingmay cause reduction sharing (see Section 3). Further, plr-bcp seemintroduce relevant improvements here. believe due factrandom formulas produce pure unit literals low even zero probability.Overall, configuration BNF-nolift turns best performer suite,slight advantage wrt. BNF-ctrl.lift-plr-bcp.Finally, plots Figure 4 notice Km 2SAT +Rsat problemstend harder within satisfiability/unsatisfiability transition area. (This fact holdsespecially Racer *SAT, see Section 5.3.) seems confirm facteasy-hard-easy pattern random k-SAT extends also 2m -CNF, already observedliterature (Giunchiglia & Sebastiani, 1996, 2000; Giunchiglia et al., 2000; Horrocks et al.,2000; Patel-Schneider & Sebastiani, 2003).369fiSebastiani & Vescovi1000100500BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp1001000100500(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)80BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp10050(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)8050106010560514010.5400.50.1200.10.05200.050.01010203040500.01601000100500BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp100010204050601000100500(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)3080BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp10050(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)8050106010560514010.5400.50.1200.10.05200.050.01040608010012010000.01140100500BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp100(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)0406080100120100014010050080100508050106010560514010.5400.50.1200.10.050.050.0160801001200140BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)200.0160801001200140Figure 4: Comparison among different variants Km 2SAT +Rsat random problems.X axis: #clauses/N . axis: median (50th percentile) CPU time, 100 samples/point. 1st row: = 1, p = 0.5, N = 8, 9; 2nd row: = 2, p = 0.6, N = 3, 4;3rd row: = 2, p = 0.5, N = 3, 4. Background: % satisfiable instances.370fiAutomated Reasoning Modal Description Logics via SAT EncodingBNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp180000160000BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp180000160000140000140000120000120000100000100000800008000060000600004000040000200002000000102030405060BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp1.2e+0610201e+068000008000006000006000004000004000002000002000000405060BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp1.2e+061e+0630040608010012014040BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp2e+061.5e+061e+061e+06500000500000080100120140BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp2e+061.5e+0660060801001201406080100120140Figure 5: Comparison among different variants Km 2SAT random problems. X axis:#clauses/N . axis: 1st column: #variables SAT encoding (90th percentiles), 100 samples/point; 2nd column: #clauses SAT encoding (90thpercentiles), 100 samples/point. 1st row: = 1, p = 0.5, N = 9; 2nd row: = 2,p = 0.6, N = 4; 3rd row: = 2, p = 0.5, N = 4.371fiSebastiani & Vescovi5.2.3 Results TANCS 2000 Benchmark Suitecomparison among Km 2SAT variants TANCS 2000 benchmark presentedFigures 6 7, different BNF variants Km 2SAT compared enablingdisabling lift/ctrl.lif plr-bcp.Figure 6, top-left bottom, present cumulative CPU times spentKm 2SAT +Rsat easy, medium hard categories respectively (the correspondingplots reporting non-cumulative CPU times included online appendix).Figure 7 present plots number variables clauses formulas solvedchallenging cases medium hard problems. 31 noticeslight differences among different variants Km 2SAT ; BNF liftbest option allows solving problems hard class requiring less CPUtime.remark that, despite expected exponential growth encoded formulas wrt.modal depth, benchmark Km 2SAT +Rsat allows encoding solvingproblems modal depth greater 100 easy class greater 50medium hard classes, producing solving SAT-encoded formulas 107variables 1.4 107 clauses.5.3 Empirical Comparison wrt. Approachesproceed comparison approach wrt. current state-of-the-art evaluating Km 2SAT +Rsat Km -satisfiability solvers listed above.details, chose compare performance solvers bestlocal Km 2SAT +Rsat variant single benchmark suite best globalKm 2SAT +Rsat variant among benchmark suites, identifiedBNF-ctrl.lift-plr-bcp.5.3.1 Comparison LWB Benchmark Suiteresults LWB benchmark suite summarized numerically graphicallyTable 2. Table 2(a) notice facts: Racer *SAT best performers(confirming analysis done Horrocks et al., 2000) significant gap wrt. others;then, K-QBF +sKizzo solves problems Km 2SAT +Rsat; followsKBDD outperforms Mspass, worst performer. detail, Km 2SAT+Rsat (one of) worst performer(s) k d4 * k t4 *, fourth best performerk path n, third best performer k path p k branch p, (one of)best performer(s) k branch n, k dum *, k grz *, k lin *, k ph * k poly *;absolute best performer k branch n k ph p.Table 2(b) give graphical representation comparison, plotting numbersolved problems approach total cumulative amount CPU timespent. notice that, even Km 2SAT +Rsat solves problems less K-QBF+sKizzo, Km 2SAT +Rsat mostly faster K-QBF +sKizzo.31. plots easy problems included online appendix.372fiAutomated Reasoning Modal Description Logics via SAT Encoding2000200010001000100100101011BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp0.120406080(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)100BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp0.1120140102030405060(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)708090 1001000100101BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp0.151015(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)(Rsat)202530Figure 6: Comparison among different variants Km 2SAT +Rsat TANCS 2000 problems. X axis: number solved problems. axis: total cumulative CPU timespent. 1st (top-left) 3th (bottom) plot: easy, medium, hard problems. (Problems sorted easiest hardest).373fiSebastiani & Vescovi1e+081e+081e+071e+071e+061e+06100000100000BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp1000010203040506070100008090 100101e+081e+081e+071e+071e+061e+06100000100000BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp100005101520BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp2030405060708090 100BNF-liftBNF-noliftBNF-ctrl.liftBNF-lift-plr-bcpBNF-nolift-plr-bcpBNF-ctrl.lift-plr-bcp10000253051015202530Figure 7: Comparison among different variants Km 2SAT TANCS 2000 problems. Xaxis: number harder solved problem. axis: 1st column: #variablesSAT encoding problem; 2nd column: #clauses SAT encodingproblem. 1st 2th row: medium, hard problems.374fiAutomated Reasoning Modal Description Logics via SAT EncodingtoolsK-QBF+ sKizzo KBDD Mspass Racer *SAT4810151416810212182121212121212121212121212121212121212119212121212121212121202121212121213212192142121131752121214122113104899218721212187212121211221212121212121testk branch nk branch pk d4 nk d4 pk dum nk dum pk grz nk grz pk lin nk lin pk path nk path pk ph nk ph pk poly nk poly pk t4p nk t4p pKm 2SAT + RsatBNF-plr-bcp-ctrl.lift-lift17171818881414212121212121212121212121131415162121111021212121661111(a) Indexes hardest problems solved.10000kQBF+sKizzo*SATRacerkBDD1000MSpassBNF-lift-plr-bcp (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)1001010.150100150200250300350(b) X axis: # problems solved; axis: total (cumulative) CPU time spent.Table 2: Comparison Km 2SAT +Rsat tools LWB benchmarks.375fiSebastiani & Vescovi10010090908080707060605050404030kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)30kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)20102010001020304050601010010090908080707060203040506060kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)5040kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)5040303020201010004060801001201404010010090908080707060608010012014060kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)5040kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)50403030202010100060801001201406080100120140Figure 8: Comparison approaches random problems. X axis: #clauses/N .axis: % problems solved within timeout, 100 samples/point. 1st row:= 1, p = 0.5, N = 8, 9; 2nd row: = 2, p = 0.6, N = 3, 4; 3rd row: = 2,p = 0.5, N = 3, 4.376fiAutomated Reasoning Modal Description Logics via SAT Encoding1000100100050010050010080100508050106010560514010.5400.50.1kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)0.05200.10.01010203040kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)0.05500.016010002001010020304050601000500100500100801005080501060kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)51106054010.5400.50.1200.10.050.010406080100120100020kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)0.050.011400401006080100120100050014010050010080100508050106010560514010.5400.50.1kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)0.050.016080100120200.1kQBF+sKizzo*SATRacerkBDDMSpassBNF-nolift (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)0.0501400.016080100120200140Figure 9: Comparison approaches random problems. X axis: #clauses/N .axis: median (50th percentile) CPU time, 100 samples/point. 1st row: = 1,p = 0.5, N = 8, 9; 2nd row: = 2, p = 0.6, N = 3, 4; 3rd row: = 2, p = 0.5,N = 3, 4. Background: % satisfiable instances.377fiSebastiani & Vescovi5.3.2 Comparison Random 2m -CNF Benchmark Suiterandom 2m -CNF benchmark suite results dominated Km 2SAT +Rsat.hardest categories among three groups problems used evaluation,report Figure 8 number problems solved tool within timeout,Figure 9 median CPU time (i.e. 50%th percentile).Looking Figure 8 notice Km 2SAT +Rsat (in versions) toolalways terminates within timeout, whilst *SAT Racer sometimesterminate hardest problems, K-QBF +sKizzo often terminate,Mspass KBDD terminate values.Figure 9 notice Km 2SAT +Rsat often best performer (in particular hardest problems), followed *SAT Racer. (This even muchevident consider 90%th percentile CPU time, whose plots includedonline appendix.) tests, K-QBF +sKizzo, Mspass KBDD drasticallyoutperformed others.5.3.3 Comparison TANCS 2000 Benchmark Suiteresults TANCS 2000 benchmark summarized Figure 10, easycategory (top-left) hard category (bottom).plots notice relative performances tools test varysignificantly category: Racer *SAT among best performerscategories; K-QBF +sKizzo behaves well easy medium categories solvesproblems hard one; KBDD behaves well easy category, solvesproblems medium hard ones. Mspass among worst performerscategories: particular, solve problem hard category; finally,Km 2SAT +Rsat worst performer easy category, outperforms competitors*SAT Racer medium category, competes head-to-headRacer *SAT first position hard category: local-best configurationBNF-lift beats competitors; global-best configuration BNF-ctrl.lift-prl-bcpsolves many problems Racer, one-order-magnitude CPU-time performance gap,two problems less *SAT.Notice classification benchmark problems easy, mediumhard given Massacci Donini (2000) based translation schema usedproduce every modal problem refers reasoning component, necessarily related components (like, e.g., modal depth) affect sizeencoding and, hence, efficiency approach. may explain fact, e.g.,easy problems easy approach, viceversa.5.4 Discussionhighlighted Giunchiglia et al. (2000), Horrocks et al. (2000), satisfiabilityproblem modal logics like Km characterized alternation two orthogonalcomponents reasoning: Boolean component, performing Boolean reasoning withinstate, modal component, generating successor states state. lattermust cope fact candidate models may exponentially big wrt.depth(), whilst former must cope fact may exponentially378fiAutomated Reasoning Modal Description Logics via SAT Encoding100001e+051000010001000100100101010.1kQBF+sKizzo*SATRacerkBDDMSpassBNF-lift-plr-bcp (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)50100150kQBF+sKizzo*SATRacerkBDDMSpassBNF-lift-plr-bcp (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)10.12002040608010012010000100010010kQBF+sKizzo*SATRacerkBDDMSpassBNF-lift-plr-bcp (Rsat)BNF-ctrl.lift-plr-bcp (Rsat)10.151015202530Figure 10: Comparison approaches TANCS 2000 problems. X axis: number solved problems. axis: total cumulative CPU time spent. 1st (top-left)3th (bottom) plot: easy, medium, hard problems. (Problems sortedeasiest hardest).379fiSebastiani & Vescovimany candidate (sub)models explore. Km 2SAT +DPLL approach encoderhandle whole modal component (rules (8) (9)), whilst handlingwhole Boolean component delegated external SAT solver.results displayed Section 5.3 notice relative performancesKm 2SAT +DPLL approach wrt. state-of-the-art tools range casesKm 2SAT +Rsat much less efficient state-of-the-art approaches (e.g., k d4k t4p formulas) formulas much efficient (e.g., k ph p2m -CNF formulas = 1). middle stands large majority formulasapproach competes well state-of-the art tools; particular, Km 2SAT+Rsat competes well even outperforms approaches based translationsdifferent formalisms (the translational approach, automata-theoretic approachQBF-encoding approach).simple explanation former fact could Km 2SAT +DPLL approachloses problems high modal depth, modal component reasoningdominates (like, e.g., k d4 k t4p formulas), wins problems Booleancomponent reasoning dominates (like, e.g., k ph n 2m -CNF formulas= 1), competitive formulas components relevant.notice, however, Km 2SAT +Rsat wins hard category TANCS 2000benchmarks, modal depths greater 50, k branch n, searchdominated modal component. 32 fact, recall reducing Booleancomponent reasoning may produce reduction also modal reasoning effort,may reduce number successor states analyze (e.g. Sebastiani, 2007, 2007). Thus,e.g., techniques like on-the-fly BCP, although exploiting purely-Boolean properties,may produce drastic pruning Boolean search, also drastic reductionsize model investigated, cut priori amount successorstates expand.6. Related Work Research Trendslast fifteen years one main research line description logic focused investigating increasingly expressive logics, goal establishing theoretical boundariesdecidability allowing expressive power languages defined (Baader,Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). Consequently, expressivethough hard description logics today notable application field SemanticWeb. example, SHOIN (D) logic (which NExpTime complexity) capturessub-language OWL DL full OWL (Web Ontology Language) language (Bechhofer,van Harmelen, Hendler, Horrocks, McGuinness, Patel-Schneider, & Stein, 2004),recommended standard language semantic web proposed W3C consortium.contrast, recent quest tractable description logic-based languages arisingfield bio-medical ontologies (e.g., Spackman, Campbell, & Cote, 1997; Sioutos,de Coronado, Haber, Hartel, Shaiu, & Wright, 2007; Gene Ontology Consortium, 2000;32. k branch n formulas hard perspective modal reasoning, requirefinding one model 2d+1 1 states (Halpern & Moses, 1992), Boolean reasoning withinstate really required (Giunchiglia et al., 2000; Horrocks et al., 2000): e.g., *SAT solves k branch n(d)2d+1 1 calls embedded DPLL engine, one state M, call solved BCP only.380fiAutomated Reasoning Modal Description Logics via SAT EncodingRector & Horrocks, 1997) stimulated opening another research line tractabledescription logics (also called lightweight description logics), suitable reasoningbig bio-medical ontologies. particular, Baader et al. (2005, 2006, 2007, 2008)spent considerable effort attempt defining small maximal subsetlogical constructors, expressive enough cover needs practical applications,whose inference problems must remain tractable. example, simple tractabledescription logics like EL, EL+ EL++ (Baader et al., 2005) expressive enoughdescribe several important bio-medical ontologies SNoMed (Spackman et al., 1997),NCI (Sioutos et al., 2007), Gene Ontology (The Gene Ontology Consortium, 2000)majority Galen (Rector & Horrocks, 1997).Reasoning ontologies represents important application lightweightdescription logics, also challenge due required efficiency huge dimensions ontologies. perspective, important face efficientlybasic reasoning services (e.g., satisfiability, subsumption, queries) logics like EL, EL+EL++ , also sophisticated reasoning problems like e.g., axiom pinpointing (Baaderet al., 2007; Baader & Penaloza, 2008) logical difference terminologies (Konev,Walther, & Wolter, 2008).7. Conclusions Future Workpaper explored idea encoding Km /ALC-satisfiability SAT,handled state-of-the-art SAT tools. showed that, despite intrinsicrisk blowup size encoded formulas, performances approachcomparable current state-of-the-art tools rather extensive varietyempirical tests. Furthermore, remark approach allows directly benefittingfree current future enhancements SAT solver technology.see many possible directions explore order enhance extend approach.important open research line explore feasibility SAT encodingsexpressive modal description logics (e.g., whilst logics like Tm extensionstraightforward, logics like S4m , elaborated description logics ALC,challenging) complex form reasoning (e.g., including TBoxesABoxes).current investigation, however, focuses lightweight logics Baader et al.(2005). investigated (and currently enhancing) encoding mainreasoning services EL EL+ Horn-SAT, allows reasoning efficiently(often huge) bio-medical ontologies mentioned Section 6, handlingsophisticated inference problems mentioned (e.g., currently handle axiompinpointing), exploiting advanced functionalities implementedtop modern SAT solver (Sebastiani & Vescovi, 2009).8. Acknowledgmentsauthors partly supported SRC/GRC Custom Research Project 2009-TJ1880 WOLFLING, MIUR PRIN project 20079E5KM8 002.381fiSebastiani & VescoviAppendix A. Proof Correctness & CompletenessA.1 NotationLet Km -formula. denote representation current formalism:defdefdefdefdefNNF, 3r = 2r , 2r = 3r , 1 2 = 1 2 , 1 2 = 1 2 , Ai = Ai ,defdefdefdefdefAi = Ai ; BNF, 2r = 2r , 2r = 2r , 1 2 = 1 2 , 1 2 = 1 2 ,defdefAi = Ai , Ai = Ai .represent truth assignment set literals, intended meaningpositive literal Ai (resp. negative literal Ai ) means Ai assigned true (resp.false). say assigns literal l either l l . say literal loccurs Boolean formula iff atom l occurs .Let denote Kripke model, let label generic state u M.label (and denote) 1 root state M. h : mean uM, u |= . Thus, every s.t. u M, either h : h : M.convenience, instead (9) sometimes use equivalent definition:^^def(Lh, r,i Lh.i, 0r ))Def (.i, 0r ). (20)Def (, r ) = (Lh, reveryh, r,ieveryh, r,iNotice Def (, ) (6), (7), (8), (20) written general form^(Lh, h,i )Def ( 0 , 0 ).(21)h 0 , 0call definition implication Def (, ) expressions (Lh,h,i ).A.2 Soundness Completeness Km 2SATLet Km -formula. prove following theorem, states soundnesscompleteness Km 2SAT .Theorem 1. Km -formula Km -satisfiable corresponding Km 2SAT ()satisfiable.Proof. direct consequence following Lemmas 2 3.Lemma 2. Given Km -formula , Km 2SAT () satisfiable, exists Kripkemodel s.t. M, 1 |= .Proof. Let total truth assignment satisfying Km 2SAT (). build Kripkemodel = hU, L, R1 , . . . , Rm follows:Udef= { : Ah, occurs Km 2SAT () f }rue Lh, AidefL(, Ai ) =F alse Lh, AiRrdef= {h, .ii : Lh,r,i382}.(22)(23)(24)fiAutomated Reasoning Modal Description Logics via SAT Encodingshow induction structure that, every h, s.t. Lh,Km 2SAT (),h : Lh, .occurs(25)Base= Ai = Ai . (25) follows trivially (23).Step= . Let Lh,.satisfies (6), Lh,every {1, 2}.inductive hypothesis, h : every {1, 2}.Then, definition, h : M.Thus, h : Lh,= . Let Lh,..satisfies (7), Lh,{1, 2}.inductive hypothesis, h : {1, 2}.Then, definition, h : M.Thus, h : Lh,= r,j . Let Lh,r,j..satisfies (8), Lh.j,0r,j.inductive hypothesis, h.j : 0r,j M.Then, definition (24), h : r,j M.Thus, h : r,j Lh,= r . Let Lh,rir,j..satisfies (9), every h, r,i s.t. Lh,r,i, Lh.i,0rinductive hypothesis, h : r,i h.i : 0r M.Then, definition (24), h : r M.Thus, h : r Lh,|= Km 2SAT (), Ah1,r.. Thus, (25), h1 : M, i.e., M, 1 |= .383.fiSebastiani & VescoviLemma 3. Given Km -formula , exists Kripke model s.t. M, 1 |= ,Km 2SAT () satisfiable.Proof. Let Kripke model s.t. M, 1 |= . build truth assignmentKm 2SAT () recursively follows: 33def=(26)defdef=(28)def(29)def= {Lh,{Lh,= {Lh,{Lh,Km 2SAT () : h, M}Km 2SAT () : h, M}r,ir(27)Km 2SAT () : 6 M}Km 2SAT () : 6 M}= {Lh,Km 2SAT (): 6 Lh,{1, 2}} (30)Km 2SAT (): 6 Lh,every {1, 2}}.{Lh,consistent truth assignment literals Lh,Ais.t. Ai 6 M.construction, every Lh, Km 2SAT (), assigns Lh, true iff assignsLh, false vice versa, consistent truth assignment.First, show satisfies definition implications Def (, )sDef (, ) s.t. M. Let M. distinguish four cases.= . Thus = s.t. 1 = 1 2 = 2 .h : (and hence h : 6 M), h :h : 6 M. Thus, (27), {Lh, 1 , Lh, 2 , Lh, } ,satisfies definition implications Def (, ) Def (, ).h : 6 (and hence h, M), h : 6h : M. Thus, (27), {Lh, , Lh, } , satisfiesdefinition implications Def (, ) Def (, ).= . Like previous case, inverting .= r,j . Thus = r s.t. 0r = 0r,j .h : r,j (and hence h : r 6 M), h.j : 0r,j M. Thus, (27),{Lh.j, r,j , Lh, r } , satisfies definition implications0Def (, r,j ) Def (, r ).33. assume , generated order, generated recursively starting. Intuitively, assigns literals Lh, s.t. way mimic M; assignsliterals way mimic fact state outside generated (i.e.,Lh, assigned false Lh, s, Lh, s, Lh, assigned consequently).384fiAutomated Reasoning Modal Description Logics via SAT Encodingh : r,j 6 (and hence h : r M), (27) Lh, r,j ,satisfies definition implications Def (, r,j ).far Def (, r ) concerned, partition clauses (9):((Lh,rLh,r,i )Lh.i,0r )(31)two subsets. first set clauses (31) h : r,i M.h : r M, h.i : 0r M. Thus, (27), Lh.i, 0r ,satisfies (31). second set clauses (31) h : r,i 6 M.(27) Lh, r,i , satisfies (31). Thus,satisfies definition implications also Def (, r ).= r . Like previous case, inverting .Notice that, 6 M, .i 6 every i. Thus, every Def (, ) s.t. 6 M,atoms implication definition Def (, ) assigned .Second, show induction recursive structure satisfiesdefinition implications Def (, )s Def (, )s s.t. 6 M. Let 6 M.base step, (29), satisfies definition implications Def (, r,i )sDef (, r )s assigns false Lh, r,i s. Indeed, assigns every literaltype Lh, Ai s.t Ai 6 (notice Def (, Ai )s definitionstrivially satisfied dont contain definition implications).inductive step, show inductive structure satisfiesdefinition implications Def (, )s Def (, )sdefLet = = s.t. = (or vice versa). that:Lh, (respectively least one Lh, ) assigned true ,definition implications Def (, ) (respectively Def (, )) already triviallysatisfied;least one Lh, (respectively Lh, s) assigned false ,(30) Lh, (respectively Lh, ) assigned false , satisfies definitionimplication Def (, ) (respectively Def (, )).Thus satisfies definition implications Def (, )s Def (, )s s.t.6 M.whole, |= Def (, ) every Def (, ). construction, |= Ah1,since h1 : M. Therefore |= Km 2SAT ().385fiSebastiani & VescoviReferencesAreces, C., Gennari, R., Heguiabehere, J., & de Rijke, M. (2000). Tree-based heuristicsmodal theorem proving. Proc. ECAI00, pp. 199203. IOS Press.Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proc. IJCAI05,pp. 364369. Morgan-Kaufmann Publishers.Baader, F., Franconi, E., Hollunder, B., Nebel, B., & Profitlich, H. (1994). EmpiricalAnalysis Optimization Techniques Terminological Representation Systems or:Making KRIS get move on. Applied Artificial Intelligence. Special Issue Knowledge Base Management, 4, 109132.Baader, F., & Hollunder, B. (1991). Terminological Knowledge Representation SystemComplete Inference Algorithms. Proc. First International WorkshopProcessing Declarative Knowledge, Vol. 572 LNCS, pp. 6785. SpringerVerlag.Baader, F., Lutz, C., & Suntisrivaraporn, B. (2006). CELa polynomial-time reasonerlife science ontologies. Proc. IJCAR06, Vol. 4130 LNAI, pp. 287291.SpringerVerlag.Baader, F., Brandt, S., & Lutz, C. (2008). Pushing EL envelope further. Proc.OWLED 2008 DC Workshop.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2003). Description Logic Handbook: Theory, Implementation, Applications.Cambridge University Press.Baader, F., & Penaloza, R. (2008). Automata-Based Axiom Pinpointing. Proc.IJCAR08, Vol. 5195 LNAI, pp. 226241. Springer.Baader, F., Penaloza, R., & Suntisrivaraporn, B. (2007). Pinpointing descriptionlogic EL+ . Proc. KI 2007, Vol. 4667 LNCS, pp. 5267. Springer.Balsiger, P., Heuerding, A., & Schwendimann, S. (1998). Logics workbench 1.0. Proc.Tableaux98, Vol. 1397 LNCS, pp. 3537. Springer.Bechhofer, S., van Harmelen, F., Hendler, J., Horrocks, I., McGuinness, D. L., PatelSchneider, P. F., & Stein, L. A. (2004). OWL Web Ontology Language reference.W3C Recommendation. Available http://www.w3.org/TR/owl-ref/.Benedetti, M. (2005). sKizzo: Suite Evaluate Certify QBFs. Proc. CADE-20,Vol. 3632 LNCS, pp. 369376. Springer.Biere, A., Cimatti, A., Clarke, E. M., & Zhu, Y. (1999). Symbolic Model Checking withoutBDDs. Proc. TACAS99, Vol. 1579 LNCS, pp. 193207. Springer.Brand, S. (2008). Personal communication.Brand, S., Gennari, R., & de Rijke, M. (2003). Constraint Programming ModellingSolving Modal Satisfability. Proc. CP 2003, Vol. 2833 LNCS, pp. 795800.Springer.Davis, M., Longemann, G., & Loveland, D. (1962). machine program theorem-proving.Journal ACM, 5 (7), 394397.386fiAutomated Reasoning Modal Description Logics via SAT EncodingDavis, M., & Putnam, H. (1960). computing procedure quantification theory. JournalACM, 7, 201215.Donini, F., & Massacci, F. (2000). EXPTIME tableaux ALC. Artificial Intelligence,124 (1), 87138.Een, N., & Sorensson, N. (2004). Extensible SAT-solver. Proc. SAT03, Vol. 2919LNCS, pp. 502518. Springer.Fitting, M. (1983). Proof Methods Modal Intuitionistic Logics. D. Reidel Publishing.Franconi, E. (1998). CRACK. Proc. Description Logics 98, Vol. 11 CEUR WorkshopProceedings. CEUR-WS.org.Giunchiglia, E., Giunchiglia, F., Sebastiani, R., & Tacchella, A. (2000). SAT vs. Translationbased decision procedures modal logics: comparative evaluation. JournalApplied Non-Classical Logics, 10 (2), 145172.Giunchiglia, E., Giunchiglia, F., & Tacchella, A. (2002). SAT-Based Decision ProceduresClassical Modal Logics. Journal Automated Reasoning, 28 (2), 143171.Giunchiglia, F., & Sebastiani, R. (1996). Building decision procedures modal logicspropositional decision procedures - case study modal K. Proc. CADE-13,Vol. 1104 LNAI, pp. 583597. Springer.Giunchiglia, F., & Sebastiani, R. (2000). Building decision procedures modal logicspropositional decision procedures - case study modal K(m). InformationComputation, 162 (1/2), 158178.Giunchiglia, F., Roveri, M., & Sebastiani, R. (1996). new method testing decisionprocedures modal terminological logics. Proc. Description Logics 96,Vol. WS-96-05 AAAI Technical Reports, pp. 119123. AAAI Press.Haarslev, V., & Moeller, R. (2001). RACER System Description. Proc. IJCAR01,Vol. 2083 LNAI, pp. 701706. Springer.Halpern, J. Y. (1995). effect bounding number primitive propositionsdepth nesting complexity modal logic. Artificial Intelligence, 75 (3),361372.Halpern, J., & Moses, Y. (1992). guide completeness complexity modallogics knowledge belief. Artificial Intelligence, 54 (3), 319379.Heuerding, A., & Schwendimann, S. (1996). benchmark method propositionalmodal logics K, KT, S4. Tech. rep. IAM-96-015, University Bern, Switzerland.Horrocks, I. (1998). Using expressive description logic: FaCT fiction?. Proc.KR98, pp. 636647. Morgan Kaufmann.Horrocks, I., & Patel-Schneider, P. F. (1999). Optimizing Description Logic Subsumption.Journal Logic Computation, 9 (3), 267293.Horrocks, I., Patel-Schneider, P. F., & Sebastiani, R. (2000). Analysis EmpiricalTesting Modal Decision Procedures. Logic Journal IGPL, 8 (3), 293323.387fiSebastiani & VescoviHustadt, U., Schmidt, R. A., & Weidenbach, C. (1999). MSPASS: Subsumption TestingSPASS. Proc. Description Logics 99, Vol. 22 CEUR Workshop Proceedings,pp. 136137. CEUR-WS.org.Hustadt, U., & Schmidt, R. (1999). empirical analysis modal theorem provers. JournalApplied Non-Classical Logics, 9 (4), 479522.Kautz, H., McAllester, D., & Selman, B. (1996). Encoding Plans Propositional Logic.Proc. KR96, pp. 374384. AAAI Press.Konev, B., Walther, D., & Wolter, F. (2008). Logical Difference Problem DescriptionLogic Terminologies. Proc. IJCAR08, Vol. 5195 LNAI, pp. 259274. Springer.Ladner, R. (1977). computational complexity provability systems modal propositional logic. SIAM Journal Computing, 6 (3), 467480.Massacci, F. (1999). Design Results Tableaux-99 Non-Classical (Modal) System Competition. Proc. Tableaux99, Vol. 1617 LNCS, pp. 1418. SpringerVerlang.Massacci, F. (2000). Single Step Tableaux modal logics: methodology, computations,algorithms. Journal Automated Reasoning, 24 (3), 319364.Massacci, F., & Donini, F. (2000). Design results TANCS-2000, Automated Reasoning Analytic Tableaux Related Methods. Proc. Tableaux 2000, Vol.1847 LNCS, pp. 5256. Springer.Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering efficient SAT solver. Proc. DAC01, pp. 530535. ACM.Narizzano, M., Pulina, L., & Tacchella, A. (2006).QBFEVAL Web Portal. Proc. JELIA06, Vol. 4160 LNCS, pp. 494497. Springer. See:http://www.qbflib.org/index eval.php.Pan, G., Sattler, U., & Vardi, M. Y. (2002). BDD-Based Decision Procedures K.Proc. CADE-18, Vol. 2392 LNCS, pp. 1630. Springer.Pan, G., & Vardi, M. Y. (2003). Optimizing BDD-based modal solver. Proc.CADE-19, Vol. 2741 LNAI, pp. 7589. Springer.Patel-Schneider, P. F. (1998). DLP system description. Proc. Tableaux98, pp. 8789.Patel-Schneider, P. F., & Sebastiani, R. (2001). new system methodology generating random modal formulae. Proc. IJCAR01, Vol. 2083 LNAI, pp. 464468.Springer-Verlag.Patel-Schneider, P. F., & Sebastiani, R. (2003). New General Method Generate Random Modal Formulae Testing Decision Procedures. Journal Artificial Intelligence Research, (JAIR), 18, 351389.Pipatsrisawat, T., & Darwiche, A. (2006). SAT Solver Description: Rsat. Available at:http://fmv.jku.at/sat-race-2006/descriptions/9-rsat.pdf.Rector, A., & Horrocks, I. (1997). Experience building large, re-usable medical ontologyusing description logic transitivity concept inclusions. Proc. WorkshopOntological Engineering, AAAI Spring Symposium (AAAI97). AAAI Press.388fiAutomated Reasoning Modal Description Logics via SAT EncodingSchild, K. D. (1991). correspondence theory terminological logics: preliminary report.Proc. IJCAI91, pp. 466471.Sebastiani, R. (2007). Lazy Satisfiability Modulo Theories. Journal Satisfiability, BooleanModeling Computation (JSAT), 3, 141224.Sebastiani, R., & Vescovi, M. (2006). Encoding Satisfiability Modal DescriptionLogics SAT: Case Study K(m)/ALC. Proc. SAT06, Vol. 4121LNCS, pp. 130135. Springer.Sebastiani, R., & Vescovi, M. (2009). Axiom Pinpointing Lightweight Description Logicsvia Horn-SAT Encoding Conflict Analysis. Proc. CADE-22, Vol. 5663LNAI. Springer. print.Sebastiani, R. (2007). KSAT Delayed Theory Combination: Exploiting DPLLOutside SAT Domain. Proc. FroCoS07, Vol. 4720 LNCS, pp. 2846.Springer. Invited talk.Seshia, S. A., Lahiri, S. K., & Bryant, R. E. (2003). Hybrid SAT-Based Decision ProcedureSeparation Logic Uninterpreted Functions. Proc. DAC03, pp. 425430.ACM.Silva, J. P. M., & Sakallah, K. A. (1996). GRASP - new Search Algorithm Satisfiability.Proc. ICCAD96, pp. 220227. IEEE Computer Society.Sioutos, N., de Coronado, S., Haber, M. W., Hartel, F. W., Shaiu, W., & Wright, L. W.(2007). NCI Thesaurus: semantic model integrating cancer-related clinicalmolecular information. Journal Biomedical Informatics, 40 (1), 3043.Spackman, K. A., Campbell, K., & Cote, R. (1997). SNOMED RT: reference terminologyhealt care. Journal American Medical Informatics Association (Fall SymposiumSupplement), 640644.Strichman, O. (2002). Solving Presburger Linear Arithmetic SAT. Proc.FMCAD02, Vol. 2517 LNCS, pp. 160170. SpringerVerlag.Strichman, O., Seshia, S., & Bryant, R. (2002). Deciding separation formulas SAT.Proc. CAV02, Vol. 2404 LNCS, pp. 209222. Springer.Tacchella, A. (1999). *SAT system description. Proc. Description Logics 99, Vol. 22CEUR Workshop Proceedings, pp. 142144. CEUR-WS.org.Gene Ontology Consortium (2000). Gene ontology: Tool unification biology.Nature Genetics, 25, 2529.Voronkov, A. (1999). KK: theorem prover K. Proc. CADE-16, Vol. 1632LNAI, pp. 383387. Springer.Voronkov, A. (2001). optimize proof-search modal logics: new methods provingredundancy criteria sequent calculi. ACM Transactions Computational Logic,2 (2), 182215.Zhang, L., & Malik, S. (2002). quest efficient boolean satisfiability solvers. Proc.CAV02, Vol. 2404 LNCS, pp. 1736. Springer.389fiJournal Artificial Intelligence Research 35 (2009) 161-191Submitted 05/08; published 06/09Eliciting Single-Peaked PreferencesUsing Comparison QueriesVincent Conitzerconitzer@cs.duke.eduDepartments Computer ScienceEconomics, Duke UniversityDurham, NC, USAAbstractVoting general method aggregating preferences multiple agents.agent ranks possible alternatives, based this, aggregate rankingalternatives (or least winning alternative) produced. However, manyalternatives, impractical simply ask agents report complete preferences.Rather, agents preferences, least relevant parts thereof, need elicited.done asking agents (hopefully small) number simple queriespreferences, comparison queries, ask agent compare twoalternatives. Prior work preference elicitation voting focused caseunrestricted preferences. shown setting, sometimes necessaryask agent (almost) many queries would required determine arbitraryranking alternatives. contrast, paper, focus single-peaked preferences.show preferences elicited using linear number comparisonqueries, either order respect preferences single-peaked known,least one agents complete preferences known. show using sublinearnumber queries suffice. also consider case cardinally single-peakedpreferences. case, show alternatives cardinal positions known,agents preferences elicited using logarithmic number queries;however, also show cardinal positions known, sublinear numberqueries suffice. present experimental results elicitation algorithms.also consider problem eliciting enough information determine aggregateranking, show even modest objective, sublinear number queriesper agent suffice known ordinal unknown cardinal positions. Finally,discuss whether techniques applied preferences almostsingle-peaked.1. Introductionmultiagent systems, group agents often make joint decisions evenagents conflicting preferences alternatives. example, agents maydifferent preferences possible joint plans group, allocations tasks resourcesamong members group, potential representatives (e.g., presidential candidates), etc.settings, important able aggregate agents individual preferences.result aggregation single alternative, corresponding groupscollective decision, complete aggregate (compromise) ranking alternatives(which useful, instance, alternatives later turnfeasible). general framework aggregating agents preferencesc2009AI Access Foundation. rights reserved.fiConitzeragents vote alternatives. is, agent announces complete rankingalternatives (the agents vote), based votes outcome (i.e., winningalternative complete aggregate ranking alternatives) chosen accordingvoting rule.1One might try create aggregate ranking follows: given alternatives b,votes prefer b vice versa (i.e., wins pairwise election b),ranked b aggregate ranking. Unfortunately, preferencesagents unrestricted least three alternatives, Condorcet cyclesmay occur. Condorcet cycle sequence alternatives a1 , a2 , . . . , ak1 < k, agents prefer ai ai+1 vice versa, agents prefer aka1 vice versa. presence Condorcet cycle, impossible produceaggregate ranking consistent outcomes pairwise elections. Closelyrelated phenomenon numerous impossibility results show every votingrule significant drawbacks general setting. example, leastthree alternatives, Arrows impossibility theorem (Arrow, 1963) shows voting rulerelative order two alternatives aggregate ranking independentagents rank alternatives two (i.e., rule satisfies independenceirrelevant alternatives) must either dictatorial (i.e., rule simply copies rankingfixed agent, ignoring agents) conflicting unanimity (i.e.,alternatives b, rule sometimes ranks b even agents prefer b a).another example, least three alternatives, Gibbard-Satterthwaitetheorem (Gibbard, 1973; Satterthwaite, 1975) shows voting rule onto (forevery alternative, exist votes would make alternative win) nondictatorial,instances agent best casting vote correspondagents true preferences (i.e., rule strategy-proof).1.1 Single-Peaked PreferencesFortunately, difficulties disappear agents preferences restricted, i.e.,display structure. best-known, arguably important restrictionsingle-peaked preferences (Black, 1948). Suppose alternatives orderedline, left right, representing alternatives positions. example,political election, candidates position line may indicate whether left-wingright-wing candidate (and strongly so). another example, alternatives maynumerical values: example, agents may vote size budget. yet anotherexample, alternatives may locations along road (for example, agents votingconstruct building, meet dinner, etc.). say agentspreferences single-peaked respect alternatives positions if, sideagents preferred alternative (the agents peak), agent prefers alternativescloser peak. example, set alternatives {a, b, c, d, e, f }, positionsmay represented < b < e < f < < c, case vote f e b c1. One may argue approach fully general allow agents specifypreferences probability distributions alternatives. example, impossible knowagents vote whether agent prefers second-ranked alternative 1/2 - 1/2 probabilitydistribution first-ranked third-ranked alternatives. principle, addressedvoting probability distributions instead, although practice usually tractable.162fiEliciting Single-Peaked Preferences Using Comparison Queriessingle-peaked, vote f e c b (b side fpositions, b closer f , ranked higher b f peak).(Throughout, assume preferences strict, is, agents never indifferenttwo alternatives.) Preferences likely single-peaked alternativespositions primary importance determining agents preferences. example,political elections, voters preferences determined primarily candidates proximitystance left-to-right spectrum, preferences likely single-peaked.factors also important, perceived amicability candidates,preferences necessarily likely single-peaked.Formally, agent single-peaked preferences prefers a1 a2 , one followingmust true:a1 agents peak,a1 a2 opposite sides agents peak,a1 closer peak a2 .agents preferences single-peaked (with respect positionsalternatives), known Condorcet cycles. If, addition, assumenumber agents odd, pairwise election result tie. Hence,aggregate ranking simply correspond outcomes pairwise elections.case, also incentive agent misreport preferences, sincereporting preferences truthfully, will, pairwise election, rank alternativeprefers higher.1.2 Important Single-Peaked Preferences?start developing techniques deal specifically single-peaked preferences,consider whether restricted class preferences interest.concept single-peaked preferences extremely influential political science:presumably best-known restriction preferences there, lies basismuch analytical work political science. thorough discussion given bookAnalytical Politics Hinich Munger (1997). book almost immediately jumpssingle-peaked preferences, argues that, form, models date backAristotle.spite importance single-peaked preferences (analytical) political scienceliterature, aware empirical studies often voters preferencesactually single-peaked. reflection reveals answer probably highly dependentparticular election. example, election clear left-wing candidate,clear centrist candidate, clear right-wing candidate, seems likely voterspreferences fact single-peaked: case, single-peakedness equivalentranking centrist candidate last. However, easy imagine scenariospreferences single-peaked. example, candidates may lieone-dimensional spectrum, two-dimensional spacefor instance, candidate mayleft-wing candidate terms social issues right-wing candidate termseconomic issues. Unfortunately, turns multidimensional analogues singlepeaked preferences longer nice properties listed above: Condorcet cyclesoccur again, strategic misreporting preferences becomes issue.163fiConitzerseems political context, settings preferencesfact single-peaked, settings close single-peaked (for example,may voters take physical attractiveness candidates accountresult preferences quite single-peaked), settingsreally single-peaked (for example, truly multidimensionalsetting multiple unrelated issues). discuss generalization techniquesalmost single-peaked preferences Section 6; multidimensional settings leftfuture research.However, political elections means setting interest, especiallyAI perspective; fact, important settings seem fit singlepeaked preference model much better. example, let us consider casealternatives numerical values, potential sizes budget agentsdeciding. Here, seems quite reasonable every agent preferred budget size,always prefers sizes closer ideal one. fact, single-peaked preferences seemreasonable political election, left-to-right spectrum merelyapproximation complex phenomena; case numerical alternatives, however,order alternatives absolute, likely reflected preferences.Even settings preferences necessarily single-peaked, could simplyrequire agents report single-peaked preferences. merits somewhatdebatable, forcing non-single-peaked agents misreport preferences. hand, benefit avoiding Condorcet cycles (atleast reported preferences) natural aggregate ranking.2 relatedapproach sometimes suggested frame issue agents voting(that is, choose set possible alternatives) way make single-peakedpreferences likely; again, merits debatable.1.3 Preference Elicitationkey difficulty aggregating preferences multiple agents elicitationagents preferences. many settings, particularly large sets alternatives,agent communicate preferences impractical. one, takelarge amount communication bandwidth. Perhaps importantly, orderagent communicate preferences, must first determine exactlypreferences are. complex task, especially guidance providedagent key questions needs answer determine preferences.alternative approach elicitor sequentially ask agents certain naturalqueries preferences. example, elictor ask agent two alternatives prefers (a comparison query). Three natural goals elicitor (1) learnenough agents preferences determine winning alternative, (2) learn enoughdetermine entire aggregate ranking, (3) learn agents complete preferences.(1) (2) advantage general, agents preferences needdetermined. example, (1), elicitor need elicit agents preferences2. Analogously, combinatorial auctions (where agents bid bundles items insteadindividual items) (Cramton et al., 2006), often bundles agents allowedbid, variety reasons. Presumably, still leads better results separateauction item.164fiEliciting Single-Peaked Preferences Using Comparison Queriesamong alternatives already determined (from agents preferences) chance winning. even (3) significant benefitselicitation (i.e., agent communicate preferencesown). First, elicitor provides agent systematic way assessingpreferences: agent needs answer simple queries. Second, perhapsimportantly, elicitor elicited preferences agents, elicitorunderstanding preferences likely occur (and, perhaps,understanding so). elicitor use understanding guideelicitation next agents preferences, learn preferences rapidly.paper, study elicitation single-peaked preferences using comparisonqueries. mostly focus approach (3), i.e., learning agents complete preferencesthough show Section 5 that, least worst case, cannot much betterpursue approach (2), i.e., learning enough preferences determineaggregate ranking. paper, devote much space (1), i.e., learning enoughpreferences determine winning alternative. noted (1)significantly easier objective: well known (Condorcet) winner alwaysmedian agents preferred alternatives, suffices find agentspreferred alternative (see note end Section 5). paper,assume preferences always single-peaked (the exception Section 6discuss case preferences usually, always, single-peaked).Section 3, study single-peaked preferences general form describedSubsection 1.1 (also known ordinally single-peaked preferences, contrast cardinally single-peaked preferences studied later paper). study settingelicitor knows positions alternatives (Subsection 3.1), settingelicitor (at least initially) (Subsection 3.2). Experimental results section provided Appendix A. Then, Section 4, study restricted settingpreferences cardinally single-peakedthat is, alternative agentcardinal position R, agents rank alternatives distance cardinal position. (To prevent confusion, emphasize Section 4, alternatives positionrefers place order alternatives, real number.) Again, studysetting elicitor knows positions alternatives (Subsection 4.1),setting elicitor (at least initially) (Subsection 4.2). Experimental results section provided Appendix B. elicitation algorithmscompletely elicit one agents preferences moving next agent (as opposedgoing back forth agents). gives algorithms nice online property:agents arrive time, elicit agents preferences arrives,agent free leave (as opposed forced wait arrivalnext agent). Especially case goal find aggregate ranking,one may wonder efficient go back forth agents. turnsthat, least worst case, cannot help (significantly), result presentedSection 5.following tables summarize results paper (with exceptionexperimental results Appendices B, illustrate algorithms performrandom instances, results Section 6 preferences almost singlepeaked).165fiConitzerordinalcardinalpositions known(m) (Subsection 3.1)(log m) (Subsection 4.1)positions unknown(m) (Subsections 3.1, 3.2)(m) (Subsections 3.2, 4.2)Table 1: Number comparison queries required fully elicit one agents single-peakedpreferences alternatives, worst case. upper bounds positionsunknown column, least one agents preferences must known (otherwise,restriction current agents preferences hence answer (m log m)).ordinalcardinalpositions known(nm) (Subsection 3.1, Section 5)O(n log m) (Subsection 4.1)positions unknown(nm) (Subsection 3.2, Section 5)(nm) (Subsection 3.2, Section 5)Table 2: Number comparison queries required find aggregate rankingalternatives n agents single-peaked preferences, worst case. upperbounds positions unknown column, least one agents preferences mustknown (otherwise, first agents preferences elicited using O(m log m) queries).2. Related Research Case Unrestricted Preferencessection, first discuss related research; then, make basic observationseliciting general (not necessarily single-peaked) preferences, serveuseful baseline comparison.2.1 Related ResearchVoting techniques drawing increasing interest artificial intelligence community,especially multiagent systems researchers. Voting used applicationscollaborative filtering, example, Pennock, Horvitz, Giles (2000); planningamong multiple automated agents, example, Ephrati Rosenschein (1991, 1993).key research topics include voting rules computationally hard execute,example, Bartholdi, Tovey, Trick (1989), Hemaspaandra, Hemaspaandra,Rothe (1997), Cohen, Schapire, Singer (1999), Rothe, Spakowski, Vogel (2003),Conitzer (2006); voting rules computationally hard manipulate strategicagents, example, Bartholdi Orlin (1991), Hemaspaandra Hemaspaandra(2007), Procaccia Rosenschein (2007), Conitzer, Sandholm, Lang (2007);concisely representing preferences voting (Lang, 2007). Single-peaked preferencesalso studied computational-complexity viewpoint (Walsh, 2007).166fiEliciting Single-Peaked Preferences Using Comparison QueriesPreference elicitation also important research topic artificial intelligence; prominent examples research include work Vu Haddawy (1997), Chajewska,Getoor, Norman, Shahar (1998), Vu Haddawy (1998), Chajewska, Koller, Parr(2000), Boutilier (2002), Braziunas Boutilier (2005), list meansexhaustive. preference elicitation multiagent systems, significant body work focuses combinatorial auctionsfor overview work, see chapter SandholmBoutilier (2006). Much work focuses elicitation approach (1), i.e., learningenough bidders valuations determine optimal allocation. Sometimes, additional information must elicited bidders determine paymentsmake according Clarke (1971), generally, Groves (1973), mechanism. Example elicitation approaches include ascending combinatorial auctionsforoverview, see chapter Parkes (2006)as well frameworks auctioneerask queries flexible way (Conen & Sandholm, 2001). significant amountresearch preference elicitation combinatorial auctions also devoted elicitationapproach (3), i.e. learning agents complete valuation function. research, typically valuation functions assumed lie restricted class, given shownagents complete valuation function elicited using polynomial numberqueries kind. Various results nature obtained Zinkevich, Blum,Sandholm (2003), Blum, Jackson, Sandholm, Zinkevich (2004), Lahaie Parkes(2004), Santi, Conitzer, Sandholm (2004). work Lahaie Parkes (2004)also includes results goal determine outcome.also already work preference elicitation voting (the settingpaper). earlier work focused elicitation approach (1), elicitingenough information agents determine winner, without restrictionspace possible preferences. Conitzer Sandholm (2002) studied complexitydeciding whether enough information elicited declare winner, wellcomplexity choosing votes elicit given strong suspicions agentsvote. also studied additional opportunities strategic misreportingpreferences elicitation introduces, well avoid introducing opportunities.(Strategic misreporting significant concern setting paper:restriction single-peaked preferences, reporting truthfully dominant strategyagents simultaneously report complete preferences, hence responding truthfullyelicitors queries ex-post equilibrium. such, paper makedistinction agents vote true preferences.) Conitzer Sandholm (2005)studied elicitation algorithms determining winner various voting rules (withoutsuspicion agents vote), gave lower bounds worst-case amountinformation agents must communicate. recentlyafter AAMAS-07 versionpaper (Conitzer, 2007)the communication complexity determining winneraggregate ranking domains single-peaked preferences studied (Escoffieret al., 2008). communication-complexity part work (which maincontribution paper) considers ordinal case known positions,results build AAMAS-07 version paper, well communicationcomplexity paper mentioned (Conitzer & Sandholm, 2005).167fiConitzer2.2 Eliciting General Preferencesbasis comparison, let us first analyze difficult elicit arbitrary (notnecessarily single-peaked) preferences using comparison queries. recall goalextract agents complete preferences, i.e., want know agents exact rankingalternatives. exactly problem sorting set elements,binary comparisons elements used sorting.extremely well-studied problem, well-known solved using O(m log m)comparisons, example using MergeSort algorithm (which splits set elementstwo halves, solves half recursively, merges solutions using linearnumber comparisons). also well-known (m log m) comparisons required(in worst case). One way see m! possible orders,order encodes log(m!) bits informationand log(m!) (m log m). Hence, general,method communicating order (not methods based comparison queries)require (m log m) bits (in worst case).Interestingly, common voting rules (including Borda, Copeland, RankedPairs), shown using techniques communication complexity theory evendetermining whether given alternative winner requires communication(nm log m) bits (in worst case), n number agents (Conitzer & Sandholm,2005). is, even try elicit agents complete preferences, (in worstcase) impossible constant factor better agentcommunicate preferences! lower bounds even hold nondeterministiccommunication, assume preferences unrestricted. contrast,assuming preferences single-peaked, elicit agents complete preferencesusing O(m) queries, show paper. course, know agentscomplete preferences, execute voting rule. shows usefulelicitation know agents preferences lie restricted class.3. Eliciting Ordinally Single-Peaked Preferencessection, study elicitation ordinally single-peaked preferences (the generalform single-peaked preferences described Subsection 1.1, contrast cardinallysingle-peaked preferences, study Section 4). first study casealternatives positions known (Subsection 3.1), caseknown (Subsection 3.2). Experimental results algorithms section givenAppendix A; results give idea algorithms behave randomly drawninstances, rather worst case.3.1 Eliciting Knowledge Alternatives Ordinal Positionssubsection, focus setting elicitor knows positionsalternatives. Let p : {1, . . . , m} denote mapping positions alternatives,i.e., p(1) leftmost alternative, p(2) alternative immediately rightp(1), . . ., p(m) rightmost alternative. algorithms make calls functionQuery(a1 , a2 ), returns true agent whose preferences currently eliciting168fiEliciting Single-Peaked Preferences Using Comparison Queriesprefers a1 a2 , false otherwise. (Since one agents preferences elicited time,need specify agent queried.)first algorithm serves find agents peak (most preferred alternative).basic idea algorithm binary search peak. so, needable assess whether peak left right given alternative a.discover asking whether alternative immediately right preferreda: is, peak must right a, otherwise, peak must leftof, equal to, a.FindPeakGivenPositions(p)l1rml < r {m1 (l + r)/2m2 m1 + 1Query(p(m1 ), p(m2 ))r m1elsel m2}return lfound peak, continue construct agents rankingalternatives follows. know agents second-ranked alternative must eitheralternative immediately left peak, one immediately right.single query settle one preferred. Without loss generality, suppose leftalternative preferred. Then, third-ranked alternative must either alternativeimmediately left second-ranked alternative, alternative immediatelyright peak. Again, single query sufficeetc. determinedranking either leftmost rightmost alternative, construct remainderranking without asking queries (by simply ranking remaining alternativesaccording proximity peak). algorithm formalized below. uses functionAppend(a1 , a2 ), makes a1 alternative immediately succeeds a2 currentranking (i.e., current agents preferences far constructed them).pseudocode, omit (simple) details maintaining ranking linked list.algorithm returns highest-ranked alternative; interpreted includinglinked-list structure, effectively entire ranking returned. c alwaysalternative ranked last among currently ranked alternatives.169fiConitzerFindRankingGivenPositions(p)FindPeakGivenPositions(p)p(t)l t1r t+1csl 1 r {Query(p(l), p(r)) {Append(p(l), c)c p(l)l l1} else {Append(p(r), c)c p(r)r r+1}}l 1 {Append(p(l), c)c p(l)l l1}r {Append(p(r), c)c p(r)r r+1}returnTheorem 1 FindRankingGivenPositions correctly determines agents preferences, using2 + log comparison queries.Proof: Correctness follows arguments given above. FindPeakGivenPositions requires log comparison queries. Every query allows us addadditional alternative ranking, last alternative need query,hence 2 additional queries.Thus, number queries algorithm requires linear numberalternatives. impossible succeed using sublinear number queries,agents single-peaked preferences encode linear number bits, follows. Supposealternatives positions follows: am1 < am3 < am5 < . . . < a4 < a2 < a1 <a3 < a5 < . . . < am4 < am2 < . Then, vote form a1 {a2 , a3 }{a4 , a5 } . . . {am1 , } (where set notation indicates constraint170fiEliciting Single-Peaked Preferences Using Comparison Queriespreference alternatives set, is, {ai , ai+1 } replaced eitherai ai+1 ai+1 ai ) single-peaked respect alternatives positions.agents preference alternatives ai ai+1 (for even i) encodes single bit, henceagents complete preferences encode (m 1)/2 bits. Since answer comparisonquery communicate single bit information, follows linear numberqueries fact necessary.3.2 Eliciting without Knowledge Alternatives Ordinal Positionssubsection, study difficult question: hard elicit agentspreferences alternatives positions known? Certainly, would desirable elicitor software require us enter domain-specific information(namely, positions alternatives) elicitation begins, two reasons: (1)information may available entity running election, (2) enteringinformation may perceived agents unduly influencing process, perhapsoutcome, election. Rather, software learn (relevant) informationdomain elicitation process itself.clear learning take place process elicitingpreferences multiple agents. Specifically, without knowledge positionsalternatives, first agents preferences could ranking alternatives, sinceranking single-peaked respect positions. Hence, eliciting first agentspreferences require (m log m) queries. elicitor knows first agents preferences, though, ways alternatives may positioned eliminated(but many remain).elicitor learn exact positions alternatives? answer no, severalreasons. First all, invert positions alternatives, making leftmostalternative rightmost, etc., without affecting preferences single-peakedrespect positions. fundamental problem elicitor couldchoose either one positionings. significantly, agents preferences may simplygive elicitor enough information determine positions. example,agents turn preferences, elicitor never learn anythingalternatives positions beyond learned first agent. case,however, elicitor could simply try verify next agent whose preferenceselicited preferences, done using linear numberqueries. generally, one might imagine intricate elicitation scheme eitherrequires queries elicit agents preferences, learns something new usefulpreferences shorten elicitation process later agents. Then, onemight imagine complex accounting scheme, spirit amortized analysis, showingtotal elicitation cost many agents cannot large.Fortunately, turns need anything complex. fact, knowing evenone agents (complete) preferences enough elicit agents preferences usinglinear number queries! (And sublinear number suffice, since alreadyshowed linear number necessary even know alternatives positions. Hence,matter many agents preferences already know, always require linearlymany queries next agent.) prove this, give elicitation algorithm171fiConitzertakes input one (the first) agents preferences (not positions alternatives),elicits another agents preferences using linear number queries. course, algorithmstill applied already know preferences one agent;case, use preferences one agents input. worst case,knowing preferences one agent help us, mayagents preferences, case teach us anythingalternatives positions beyond learned first agent. general(not worst case), may able learn something additional agentspreferences; however, best, learn alternatives positions exactly, evencase still need linear number queries. Hence, paper, investigateuse known preferences multiple agents.First, need subroutine finding agents peak. cannot use algorithmFindPeakGivenPositions previous subsection, since know positions.However, even trivial algorithm examines alternatives one one maintainspreferred alternative far requires linear number queries,simply use algorithm.FindPeak()a1{a2 , . . . , }Query(a, s)sareturnfound agents peak, next find alternatives liepeak, peak known vote (i.e., peak agent whose preferencesknow). following lemma key tool so.Lemma 1 Consider votes v1 v2 peaks s1 s2 , respectively. Then, alternative/ {s1 , s2 } lies two peaks v1 s2 v2 s1 .Proof: lies two peaks, i, lies closer si s3i (thevotes peak) lies si . Hence v1 s2 v2 s1 . Conversely, vi s3i implieslies side s3i si (otherwise, vi would ranked s3i higher).since true i, implies must lie peaks.Thus, find alternatives peak known vote peakcurrent agent, simply ask current agent, alternative known voteprefers peak current agent, whether prefers alternative knownvotes peak. answer positive, add alternative list alternativespeaks.two votes must rank alternatives peaks exact oppositeorder. Thus, point, know current agents preferences alternatives172fiEliciting Single-Peaked Preferences Using Comparison Querieslie peak peak known vote (including peaks themselves).final complex step integrate remaining alternatives ranking.(Some remaining alternatives may ranked higher alternativespeaks.) strategy integrate alternatives currentranking one one, order known vote ranks them, startingone known vote ranks highest. integrating alternative, firstcurrent agent compare worst-ranked alternative already ranking.note known vote must prefer latter alternative, latter alternativeeither known votes peak, alternative integrated earlierhence preferred known vote. latter alternative also preferred currentagent, add new alternative bottom current ranking movenext alternative. not, learn something useful positionsalternatives, namely new alternative lies side current agentspeak alternative currently ranked last. following lemma proves this. it, v1takes role known vote, v2 takes role agent whose preferencescurrently eliciting, a1 takes role alternative currently ranked last currentagent, a2 takes role alternative currently integrating.Lemma 2 Consider votes v1 v2 peaks s1 s2 , respectively. Consider two alternatives a1 , a2 6= s2 lie s1 s2 . Suppose a1 v1 a2 a2 v2 a1 .Then, a1 a2 must lie opposite sides s2 .Proof: a1 a2 lie side s2 without loss generality, left sidethen, neither lies s1 s2 , must also lie left side s1(possibly, one equal s1 ). then, v1 v2 cannot disagree a1a2 ranked higher.purpose integrating new alternative, knowing sidealternative currently ranked last helpful. fact, reach pointalgorithm, simply start comparing new alternative alternativesalready ranking, one one. benefit knowingside make later alternatives easier integrate: specifically,integrated new alternative, know alternatives integrate later mustend ranked alternative. following lemma, v1takes role known vote, v2 takes role agent whose preferencescurrently eliciting, a1 takes role alternative currently ranked last currentagent, a2 takes role alternative currently integrating, a3 takesrole alternative integrated later.Lemma 3 Consider votes v1 v2 peaks s1 s2 , respectively. Consider threealternatives a1 , a2 , a3 6= s2 lie s1 s2 . Suppose a1 v1 a2 v1 a3 ,a2 v2 a1 . Then, a2 v2 a3 .Proof: Lemma 2, know a1 a2 must lie opposite sides s2 . Moreover,lie s1 s2 , equal s2 , must also lieopposite sides s1 (with additional possibility a1 = s1 ). a1 v1 a2 v1 a3 ,173fiConitzerfollows either a3 lies side peaks a2 , away;side a1 , away. former case, immediately obtain a2 v2 a3 ;latter case, a1 v2 a3 , implies a2 v2 a3 a2 v2 a1 .Based observation, algorithm, let c1 alternativeknow later alternatives integrate end ranked it. (In sense,c1 takes role a2 lemmas.) also keep track c2 , alternativecurrently ranked last. (In sense, c2 takes role a1 lemmas.) Then,integrate new alternative, first compare c2 ; ranked c2 ,place c2 , make new c2 . ranked c2 , comparesuccessor c1 ; ranked that, compare successorsuccessor; on, find place; finally, make new c1 . laststage may require linear number queries, key observation timeask query, c1 moves ranking one place. Hence, total numberqueries ask (summing alternatives integrate)1, c1 move 1 times; get linearbound number queries.Example 1 Suppose alternatives positions e < c < b < f < < d. Also supposeknown vote f b c e, preferences agentcurrently eliciting c e b f d. algorithm proceed follows.First, FindPeak identify c current agents peak. Now, alternativesknown vote prefers c (the known votes peak), well d, f, b. lastthree alternatives, algorithm queries current agent whether preferred a.answer positive (only) b f , know two alternatives must liepeaks c line (and hence must ranked oppositely knownvote current agent). point, know current agent must preferc b f a, algorithm must integrate e ranking. set c1 = cc2 = a. algorithm first integrates since ranked higher e knownvote. algorithm queries agent c2 = a, preferred. knowcurrent agent must prefer c b f d, algorithm sets c2 = d.Finally, algorithm must integrate e. algorithm queries agent e c2 = d,e preferred. algorithm queries agent e successor c1 ,b. e preferred, algorithm inserts e c1 = c b, sets c1 = e.point know entire ranking c e b f d.present algorithm formally. algorithm uses functionAppend(a1 , a2 ), makes a1 alternative immediately succeeds a2 currentranking. also uses function InsertBetween(a1 , a2 , a3 ), inserts a1 a2a3 current ranking. algorithm (eventually) set m(a) true liespeaks current agent known vote v, peak v; otherwise,m(a) set false. v(i) returns alternative known vote ranks ith (and hencev 1 (a) returns ranking alternative known vote, v 1 (a1 ) < v 1 (a2 ) meansv prefers a1 a2 ). n(a) returns alternative immediately following currentranking. Again, peak returned, includes linked-list structurehence entire ranking.174fiEliciting Single-Peaked Preferences Using Comparison QueriesFindRankingGivenOtherVote(v)FindPeak()m(a) false{s, v(1)}v 1 (a) < v 1 (s)Query(a, v(1))m(a) truec1c2m(v(1)) true= 1 step 1 {m(v(i)) = true {Append(v(i), c2 )c2 v(i)}}= 1(m(v(i)) v(i) = s)Query(c2 , v(i)) {Append(v(i), c2 )c2 v(i)} else {Query(n(c1 ), v(i))c1 n(c1 )InsertBetween(v(i), c1 , n(c1 ))c1 v(i)}returnTheorem 2 FindRankingGivenOtherVote correctly determines agents preferences, using4m 6 comparison queries.Proof: Correctness follows arguments given above. FindPeak requires 1comparison queries. next stage, discovering alternatives lie currentagents peak known votes peak, requires 2 queries. Finally,must count number queries integration step. complex,integrating one alternative (which may 2 times) requiremultiple queries. Certainly, algorithm ask agent compare alternativecurrently integrated current c2 . contributes 2 queriestotal. However, current alternative preferred c2 , must ask queries,comparing current alternative alternative currently ranked immediately behindcurrent c1 (perhaps multiple times). every time ask query, c1175fiConitzerchanges another alternative, happen 1 times total. followstotal number queries ask (summed alternativesintegrate) 1. Adding bounds, get total bound(m 1) + (m 2) + (m 2) + (m 1) = 4m 6.course, impossible succeed sublinear number queries,reason Subsection 3.1. practice, algorithm ends requiring average roughly3m queries, seen Appendix A.4. Eliciting Cardinally Single-Peaked Preferencessection, restrict space allowed preferences slightly further. assumealternative cardinal position r(a) R (as opposed merely ordinalpositions considered point paper, providedistance function alternatives). already mentioned examplesreasonable: agents voting budget, location alongroad (in case alternatives cardinal position equals distance beginningroad). Even alternatives political candidates, may reasonablecardinal positions: example, candidates voting records,candidates cardinal position percentage times cast right-wingvote. Additionally, assume every agent also cardinal position r,ranks alternatives order proximity position. is, agent sortsalternatives according |r r(a)| (preferring closer ones). preferences knowncardinally single-peaked preferences (again, opposed ordinally single-peakedpreferences considered point) (Brams et al., 2002, 2005).3Cardinally single-peaked preferences always ordinally single-peaked well (withrespect order < defined < r(a) < r(a )), converse hold:possible agents preferences ordinally single-peaked, cardinally singlepeaked. is, possible agents preferences consistent single orderalternatives, way assign cardinal positions alternativesagents agents preferences given ranking alternatives accordingproximity cardinal position. following example shows this:Example 2 Suppose agent 1 prefers b c d, agent 2 prefers b c a, agent3 prefers c b d. preferences consistent order < b < c < d,ordinally single-peaked. Nevertheless, cardinal positionsconsistent. prove contradiction: let us suppose cardinalpositions. sometimes ranked last, implies must leftmostrightmost alternatives (without loss generality, suppose leftmost alternative).Agent 1s preferences imply order must < b < c < d. {1, 2, 3}, letr(i) agent cardinal position. agents preferences, must |r(2) r(d)| <|r(2) r(a)| |r(3) r(d)| > |r(3) r(a)|. Therefore, must r(2) > r(3).3. precise, Brams, Jones, Kilgour (2002; 2005) consider settings agentsalternatives coincide, agent exact position one alternatives;extension case quite obvious. fact, Brams et al. (2002) explicitlymention separating agents alternatives extension.176fiEliciting Single-Peaked Preferences Using Comparison Querieshand, must |r(2) r(b)| < |r(2) r(c)| |r(3) r(b)| > |r(3) r(c)|.Therefore, must r(2) < r(3)but contradiction. Henceprofile preferences cardinally single-peaked.similar example given Brams et al. (2002)in fact, preferencesexample proper subset ones used examplealthough proofexamples preferences cardinally single-peaked, make use factagents alternatives coincide model, simplifies argument somewhat.So, restriction cardinally single-peaked preferences come losspreferences represent. particular, emphasized even alternativescorrespond numerical values, may well still case preferencesordinally, cardinally, single-peaked. example, agents voting budgetproject, one agents preferences might budget definitelygo 500 (perhaps starts coming cost another projectagent cares about); long constraint met, budgetlarge possible. Hence, agents preferences would 500 499 498 . . . 10 501 502 . . ., would ordinally, cardinally, single-peaked. Thus,cardinally single-peaked preferences strictly less likely occur practice ordinallysingle-peaked preferences; nevertheless, cardinally single-peaked preferences (or somethingclose it) may well occur practice. example, agents votinglocation project along road, likely rank locations simplydistance positions along road. detailed discussion symmetryassumption inherent cardinally single-peaked preferences given Hinich Munger(1997).ordinally single-peaked preferences, first study casealternatives cardinal positions known (Subsection 4.1), caseknown (Subsection 4.2). Experimental results known cardinal positionsgiven Appendix B; results give idea algorithm behavesrandomly drawn instances, rather worst case. (We experimental resultsunknown cardinal positions, negative result there.)4.1 Eliciting Knowledge Alternatives Cardinal Positionssubsection, show preferences cardinally single-peaked, additioncardinal positions alternatives known, preference elicitationalgorithm uses logarithmic number comparison queries per agent. highlevel, algorithm works follows. pair alternatives a, (r(a) < r(a )),consider midpoint m(a, ) = (r(a) + r(a ))/2. Now, agent prefers ,agents position r must smaller m(a, ); otherwise, r > m(a, ). (As before,assume ties, r 6= m(a, ).) So, using single comparison query,determine whether agents position left right particular midpoint.allows us binary search midpoints. (As aside, knowagents position coincides position one alternatives,binary search alternatives find agents position, using FindPeakGivenPositions.case general, need binary search midpoints instead.) end, know two adjacent midpoints177fiConitzeragents position lies, sufficient determine preferences. following exampleillustrates this.Example 3 Suppose alternatives cardinal positions (which known algorithm) follows: r(a) = .46, r(b) = .92, r(c) = .42, r(d) = .78, r(e) = .02. Also supposeagent whose preferences eliciting cardinal position r = .52 (whichknown algorithm), preferences c b e. midpoints are,order: m(c, e) = .22, m(a, e) = .24, m(d, e) = .40, m(a, c) = .44, m(b, e) = .47, m(c, d) =.60, m(a, d) = .62, m(b, c) = .67, m(a, b) = .69, m(b, d) = .85. Since 5th midpoint (out10) m(b, e) = .47, algorithm first queries agent whether prefers b e.agent prefers b, algorithm conclude r > .47 (since r(b) > r(e)). 7th midpointm(a, d) = .62, algorithm next queries agent whether prefers d. agentprefers a, algorithm conclude r < .62. Finally, 6th midpoint m(c, d) = .60,algorithm next queries agent whether prefers c d. agent prefers c,algorithm conclude r < .60. algorithm knows .47 < r < .60, sincemidpoints range, sufficient deduce agents preferencesc b e.precise algorithm follows.FindRankingGivenCardinalPositions(r)a, r(a) < r(a ){(a, , (r(a) + r(a ))/2)}sort 3rd entry obtain = {(ai , ai , mi )}1i(m)2l0u2 +1l + 1 < u {h (l + u)/2Query(ah , ah )uhelselh}r (ml + mu )/2return alternatives sorted |r(a) r|Theorem 3 FindRankingGivenCardinalPositions correctly determines agents preferences,using 2log(m) comparison queries.Proof: Correctness follows arguments given above. Let us consider expressionu l 1, number midpoints strictly lth uth. expression starts2 , (at least) halved query. reaches 0,178fiEliciting Single-Peaked Preferences Using Comparison Queriesqueriesasked. Therefore, log(2 ) + 1 queries required.2log(2 ) = log(m(m 1)/2) = log(m(m 1)) 1 < log(m ) 1 = 2 log(m) 1. HenceFindRankingGivenCardinalPositions requires 2log(m) comparison queries.algorithm require us store manage2 midpoints,scale computationally extremely large numbers alternatives, unlikeprevious algorithms. important remember elicitation cost (as measurednumber comparison queries) different type cost computational cost.Typically, constraints elicitation much severe computation.agent human, number queries willing answer likelylow. Even agent software agent, answer query, maysolve hard computational problem. also reasons agents (humansoftware) may wish keep number queries answer minimum:example, concerned privacy, may worry answering queriesleaves exposed malevolent party intercepting answers. Therefore,typically much important elicitation algorithm ask queriescomputationally efficient. Nevertheless, rare settings querieseasily answered (presumably software agent), (privacy) issuescome play, number alternatives large, computational constraintmay binding. case, earlier FindRankingGivenPositions may preferredalgorithm.also (log m) lower bound: example, even agents positions coincide alternatives positions, even without restrictions typecommunication (queries), agent needs communicate (log m) bits identifyalternatives preferred alternative.4.2 Eliciting without Knowledge Alternatives Cardinal Positionsconsider case preferences cardinally single-peaked, alternatives cardinal positions known (at beginning). caseordinally single-peaked preferences, know alternatives positions,first agents preferences arbitrary hence require (m log m) queries elicit.hand, already know one agents preferences, use algorithm ordinally single-peaked preferences elicit next agents preferences usingO(m) queries, cardinally single-peaked preferences special case ordinallysingle-peaked preferences. However, since considering cardinally single-peakedpreferences, may hope better O(m) bound: all, sawcardinal positions known, O(log m) queries required per agent. So,might hope eliciting number agents preferences, learnedenough cardinal positions alternatives elicit next agentspreferences using O(log m) queries, least using sublinear numberqueries. Unfortunately, turns possible: following result gives(nm) lower bound number queries necessary elicit n agents preferences. (Theresult phrased also applies ordinally single-peaked preferences, caseeven (ordinal) positions known.)179fiConitzerTheorem 4 Suppose n agents alternatives (where even),agents preferences known cardinally single-peaked, alternatives positionsknown. Then, elicit agents preferences exactly, worst case, leastnm/2 comparison queries necessary.Remarks: remains true even alternatives ordinal positions known beginningmorespecifically, even alternatives cardinal position known beginning lie certain interval,intervals overlap. also remains true (in addition) agents need queriedorderthat is, possible ask agent 1 queries first, agent 2, return agent 1, etc.Proof: Suppose alternative ai (with {1, . . . , m}) known (from beginning)lie interval [ki 1, ki + 1], ki = 10 (1)i (i + 1)/2. is, k1 = 10, k2 =10, k3 = 20, k4 = 20, k5 = 30, etc. Moreover, suppose agents positionknown (from beginning) lie interval [1, 1]. clear agent jspreferences form {a1 , a2 } j {a3 , a4 } j {a5 , a6 } j . . . j {am1 , } (whereset notation indicates clear two preferred). Certainly,sufficient ask agent, every {1, 3, 5, . . . , 1}, whether ai preferredai+1 . total number queries would nm/2 (and askorder). need show, however, worst case, queriesalso necessary. see why, suppose answer every one queriesask ai j ai+1 (the odd-numbered alternative preferred). (This certainly possible:example, may every alternative ai position ki , every agent1.) Then, absolutely sure every agents complete preferences, must askevery one queries, following reason. Suppose asked queriesone: j odd i, yet asked agent j whether j prefers ai ai+1 .must show still possible ai+1 j ai . see possible, supposeevery agent j j 6= j position 1; j position 0.1; every alternativeai 6= position ki ; ai position ki 1. easy seepositions, {1, 3, 5, . . . , 1} \ {i}, agent j (including j),ai j ai +1 . Moreover, even i, agent j 6= j, ai j ai+1 :|(ki 1) (1)| = 10(i + 1)/2, |ki+1 (1)| = 10(i + 1)/2 + 1.means positions consistent answers queries far. However,positions, ai+1 j ai : |(ki 1) (0.1)| = 10(i + 1)/2 + .9,|ki+1 (0.1)| = 10(i + 1)/2 + 0.1. Hence, must ask last query.5. Determining Aggregate Rankingfar, goal determine agents complete preferences.desirable preference information, always necessary. example,goal may simply determine aggregate ranking alternatives. (We recallintroduction single-peaked preferences, Condorcet cycles,natural aggregate ranking determined outcomes pairwise electionsassuming number agents odd.) purpose, certainly sufficient elicitagents preferences completely, clear necessary. So,Theorem 4 gives us (nm) lower bound elicting agents preferences completely180fiEliciting Single-Peaked Preferences Using Comparison Queriesunknown cardinal positions case (and also known ordinal positions case),immediately clear lower bound also holds merely tryingdetermine aggregate ranking. Unfortunately, turns still (nm)lower bound case (under conditions Theorem 4). Hence,possible get away o(nm) querieswith exception case known cardinalpositions, case lower bound apply,fact need O(n log m) queries determine agents preferences completely (by usingalgorithm Subsection 4.1 agent).Theorem 5 Suppose n agents alternatives (where n odd even),agents preferences known cardinally single-peaked, alternativespositions known. Then, determine aggregate ranking, worst case,least (n + 1)m/4 comparison queries necessary.Remarks: remains true even alternatives ordinal positions known beginningmorespecifically, even alternatives cardinal position known beginning lie certain interval,intervals overlap. also remains true (in addition) agents need queriedorderthat is, possible ask agent 1 queries first, agent 2, return agent 1, etc.Proof: proof reuses much structure proof Theorem 4. Again,suppose alternative ai (with {1, . . . , m}) known (from beginning) lieinterval [ki 1, ki + 1], ki = 10 (1)i (i + 1)/2. is, k1 = 10, k2 = 10, k3 =20, k4 = 20, k5 = 30, etc. Moreover, again, suppose agents positionknown (from beginning) lie interval [1, 1]. Again, clear agentjs preferences form {a1 , a2 } j {a3 , a4 } j {a5 , a6 } j . . . j {am1 , }(where set notation indicates clear two preferred).relevantly theorem, aggregate ranking must also form. Hence,need determine is, every {1, 3, 5, . . . , 1}, whether agents prefer aiai+1 , vice versa. show worst case, m/2 pairwiseelections, need query least (n+1)/2 agents, thereby proving theorem.proof Theorem 4, suppose answer every one queries askai j ai+1 (the odd-numbered alternative preferred). Then, certainly sufficient to,every {1, 3, 5, . . . , 1}, query (any) (n + 1)/2 agents ai vs. ai+1 ,result majority ai . also necessary ask many queriespairwise elections, following reasons. Suppose {1, 3, 5, . . . , 1},queried (n 1)/2 agents ai vs. ai+1 . Then, evenpairwise elections, asked queries, still possible ai+1 defeats aipairwise election. reason similar proof Theorem 4: let J setagents already asked ai vs. ai+1 . Suppose every agent jj J position 1; every j/ J position 0.1; every alternative ai 6=position ki ; ai position ki 1. easy see positions,{1, 3, 5, . . . , 1} \ {i}, agent j (including agents J outside J),ai j ai +1 . Moreover, even i, agent j J, ai j ai+1 :|(ki 1) (1)| = 10(i + 1)/2, |ki+1 (1)| = 10(i + 1)/2 + 1. meanspositions consistent answers queries far. However,181fiConitzerpositions, j/ J, ai+1 j ai : |(ki 1) (0.1)| = 10(i + 1)/2 + .9,|ki+1 (0.1)| = 10(i + 1)/2 + 0.1. Hence, positions, ai+1 defeats aipairwise election. So, rule out, need ask least one query.contrast, even less ambitious goal determine winner(the top-ranked alternative aggregate ranking), need knowagents peak: well known winner median peaks (ifalternative peak multiple agents, alternative counted multipletimes calculation median). know least ordinal positions,use FindPeakGivenPositions using O(log m) queries per agent.6. Robustness Slight Deviations Single-Peakednessfar, assumed agents preferences always (at least ordinally,cases cardinally) single-peaked. section, consider possibilityagents preferences close single-peaked, quite. easy imaginepolitical election: example, one candidates happens close friendagent, agent might place candidate high ranking evenopposite ends political spectrum.get detail this, keep mind case agentspreferences (completely) single-peakedthe case studied sectionis stillimportant. political election, likely deviationssingle-peakedness (due to, example, candidates charisma positions issuesunrelated left-right spectrum), settings seems significantly less likely.example, agents voting numerical alternatives, sizebudget, seems likely preferences fact single-peaked.Another issue many nice properties single-peaked preferenceshold almost single-peaked preferences. example, may get Condorcet cyclesagain, longer clear preferences aggregated. context, ruleaggregating preferences also likely manipulable (by declaring false preferences).know alternatives positions, one simple solution simply requireagent submit single-peaked preferences (thereby forcing agents without singlepeaked preferences manipulate, definition). analogous approach often takencombinatorial auctions, restricting set valuation functions agentsreport, purpose making winner determination problem elicitationproblem easier. reasoning result likely still better runningcombinatorial auction all. One may wonder requiring reported preferencessingle-peaked could desirable effects. example, one might argueforces voters political election ignore candidates charisma (which one may arguecould desirable). precise analysis beyond scope paper.Still, interesting potentially important able elicit almost singlepeaked preferences, section, study extent done.interpret idea agents preferences close single-peaked multipleways.182fiEliciting Single-Peaked Preferences Using Comparison Queries1. may case agents single-peakedpreferences (e.g., agents know candidates personally, onesrank candidates according positions political spectrum).2. may case individual agent, agents preferences closesingle-peaked (e.g., agent may know one two candidates personally,many candidates, agent rank remaining candidatesaccording positions political spectrum).section, consider mostly first interpretation (an example illustratingsecond interpretation seems difficult deal given below). Let ussuppose fraction agents preferences single-peaked. Onestraightforward strategy following. Let us suppose, now, know alternatives positions. agent j, elicit preferences agentsingle-peaked. result, obtain ranking a1 a2 . . . alternatives,know must js ranking js preferences fact single-peaked.verify whether indeed js preferences, asking j compare a1 a2 , a2a3 , . . ., am1 . 1 queries result expected answer(ai j ai+1 ), know agent js preferences. Otherwise, start beginning, elicit js preferences without single-peakedness assumption, using standardO(m log m) sorting algorithm. (Of course, practice, actually startcompletely beginningwe still use answers queriesalready asked.) leads following propositions.Proposition 1 fraction agents preferences (ordinally cardinally) single-peaked, (ordinal cardinal) positions alternatives known,elicit agents preferences using (on average) O(m + log m) queries.Proof: FindRankingGivenPositions requires O(m) queries; verification step.time verification step fails, need another O(m log m) queries case.note approach, longer significant advantagecardinally single-peaked preferences: whereas O(log m) queries enough surepreferences cardinally single-peaked (and know alternatives positions),sure, need 1 queries verification step.may tempting think agents preferences almost single-peaked(in sense second interpretation above), verification queriesturn way expect, already know agents preferences.see case, suppose alternatives positions (from leftright) a1 < a2 < . . . < a8 . consider left-wing voter nevertheless friendsa5 , resulting very-close-to-single-peaked preferences a1 a2 a3 a5 a4a6 a7 a8 . FindRankingGivenPositions start running FindPeakGivenPositions,whose first query result answer a5 a4 . result, FindPeakGivenPositionsconclude agent right-wing voter, eventually conclude a5peak. FindRankingGivenPositions return preferences a5 a4 a3183fiConitzera2 a1 a6 a7 a8 . Then, roughly half verification queries turnway expect. (This example easily generalized number alternatives.)So, let us return first interpretation, let us consider casealternatives positions known. case, algorithm FindRankingGivenOtherVote, uses O(m) queries requires us know another agents preferences, elicit first. Now, consider case agentspreferences single-peaked, two ways get troublealgorithm FindRankingGivenOtherVote: current agents preferences may singlepeaked, earlier agents preferences using algorithm maysingle-peaked. either case, algorithm may fail identify current agents preferences correctly. Again, address asking verification queries currentagent, necessary reverting standard sorting algorithm.Assuming independence, probability current earlier agentspreferences single-peaked (1 )2 . eliciting many agents preferences,little bit careful: always use agents preferences knownpreferences, run risk agents preferences actually single-peaked,hence verification step might fail (almost) every agent. conservative approachalways use previous agents preferences. assume whether one agentspreferences single-peaked independent whether case previousagent, indeed, (1 )2 time agents preferences single-peaked.leads following proposition.Proposition 2 fraction agents preferences (ordinallycardinally) single-peaked (and independent one agent next),(ordinal cardinal) positions alternatives unknown, elicit agentspreferences using (on average) O(m + (2 2 )m log m) queries (with exceptionfirst agent).Proof: FindRankingGivenOtherVote requires O(m) queries; verification step.verification step fail either previous current agents preferencessingle-peaked, probability 1 (1 )2 = 2 2 ; need anotherO(m log m) queries case.(In practice, may want use different known vote every time. Rather,may want use one see verification step fails often;so, switch another known vote, otherwise not.)Propositions 1 2 seem provide good solution first interpretationalmost single-peaked preferences, less clear second interpretation (theexample illustrates difficulty). Still, would desirable design algorithmsefficiently elicit preferences almost single-peaked second interpretation. generally, large amount work preference elicitationassumes preferences lie particular class, would interesting seealgorithms generalized deal preferences close classes.184fiEliciting Single-Peaked Preferences Using Comparison Queries7. ConclusionsVoting general method aggregating preferences multiple agents. agentranks possible alternatives, based this, aggregate ranking alternatives(or least winning alternative) produced. However, many alternatives,impractical simply ask agents report complete preferences. Rather,agents preferences, least relevant parts thereof, need elicited. doneasking agents (hopefully small) number simple queries preferences,comparison queries, ask agent compare two alternatives. Priorwork preference elicitation voting focused case unrestricted preferences.shown setting, sometimes necessary ask agent (almost)many queries would required determine arbitrary ranking alternatives.contrast, paper, focused single-peaked preferences. agents preferencessaid single-peaked fixed order alternatives, alternativespositions (representing, instance, alternatives left-wingright-wing), agent prefers alternatives closer agentspreferred alternative ones away. first showed agentspreferences single-peaked, alternatives positions known, agents(complete) preferences elicited using linear number comparison queries.alternatives positions known, first agents preferences arbitrarytherefore cannot elicited using linear number queries. However, showedalready know least one agents preferences, elicit (next)agents preferences using linear number queries (albeit larger number queriesfirst algorithm). also showed using sublinear number queriessuffice. also considered case cardinally single-peaked preferencesthat is,alternative agent cardinal position R, agents rank alternativesdistance position. case, showed alternatives cardinalpositions known, agents preferences elicited using logarithmicnumber queries; however, also showed cardinal positions known,sublinear number queries suffice. presented experimental resultselicitation algorithms. also considered problem eliciting enough informationdetermine aggregate ranking, showed even modest objective,sublinear number queries per agent suffice known ordinal unknowncardinal positions. Finally, discussed whether techniques appliedpreferences almost single-peaked. showed algorithms presented earlierpaper used agents preferences (completely) single-peaked.case agents preferences almost single-peaked seems difficult; gaveexample illustrating why.Future research includes studying elicitation voting restricted classespreferences. class single-peaked preferences (over single-dimensional domains)natural one study first, due practical relevance useful theoreticalproperties (no Condorcet cycles and, result, ability aggregate preferencesstrategy-proof manner). Classes practically relevant nicetheoretical properties still interest, though. example, one may consider settingsalternatives take positions two-dimensional rather single-dimensional space.185fiConitzerwell-known generalization, Condorcet cycles occur again.true almost single-peaked settings discussed above. Nevertheless,imply efficient elicitation algorithms exist settings.imply elicitation algorithms would useless, since still often necessaryvote alternatives settings. However, use voting rule strategyproof, must carefully evaluate strategic effects elicitation. Specifically,queries agents asked, may able infer somethingagents answered queries them; this, turn, may affect (strategically)choose answer queries, since rule strategy-proof. phenomenonstudied detail Conitzer Sandholm (2002).Acknowledgmentsthank AAMAS JAIR reviewers valuable feedback (the JAIRreviewers provided especially detailed helpful feedback). Conitzer supportedNSF award number IIS-0812113 Alfred P. Sloan Research Fellowship.Appendix A. Experimental Results Ordinally Single-PeakedPreferencesfollowing experiment compares FindRankingGivenPositions, FindRankingGivenOtherVote,MergeSort. discussed Subsection 2.2, MergeSort standard sorting algorithmuses comparison queries, therefore used elicit agents preferenceswithout knowledge alternatives positions votes. Conversely,algorithm elicits general preferences using comparison queries used solvesorting problem. So, effectively, want compare algorithmuse fact preferences single-peaked, cannot compare anythingsorting algorithm. conceivable sorting algorithms perform slightlybetter MergeSort problem, require (m log m) comparisons.run, first random permutation alternatives drawn representpositions alternatives. Then, two random votes (rankings) single-peakedrespect positions drawn. vote, done randomlychoosing peak, randomly choosing second-highest ranked alternativetwo adjacent alternatives, etc. algorithm elicited second vote; FindRankingGivenPositions given (costless) access positions, FindRankingGivenOtherVotegiven (costless) access first vote. (For run, verified algorithmproduced correct ranking.) Figure 1 shows results (please note logarithmic scalex-axis). FindRankingGivenPositions outperforms FindRankingGivenOtherVote,turn clearly outperforms MergeSort.One interesting observation FindRankingGivenOtherVote sometimes repeatsquery asked before. Thus, simply storing results previous queries,number queries reduced. However, general, keeping track queriesasked imposes significant computational burden,2 possible com186fiEliciting Single-Peaked Preferences Using Comparison Queries1.6e+06MergeSortFindRankingGivenOtherVoteFindRankingGivenPositions1.4e+06# comparison queries1.2e+061e+068000006000004000002000000100010000# alternatives100000Figure 1: Experimental comparison MergeSort, FindRankingGivenOtherVote, FindRankingGivenPositions. Please note logarithmic scale x-axis.data point averaged 5 runs.parison queries. Hence, experiment, results previous queries stored.FindRankingGivenPositions MergeSort never repeat query.Appendix B. Experimental Results Cardinally Single-PeakedPreferencesNext, experimentally compare FindRankingGivenCardinalPositions FindRankingGivenPositions. Since former requires cardinally single-peaked preferences, must generatepreferences different way Appendix A. generate preferences rundrawing cardinal position uniformly random [0, 1] alternative, wellagent. agent ranks alternatives according proximity cardinalposition. (For run, verified algorithm produced correct ranking.)note computationally, algorithm scale large numbers alternatives previous algorithms, reasons mentioned earlier (managingmidpoints). Figure 2 shows results, clearly contrasting FindRankingGivenCardinalPositionss logarithmic nature FindRankingGivenPositionss linear (and somewhat lesspredictable) nature. (Please note logarithmic scale x-axis.)187fiConitzer800FindRankingGivenPositionsFindRankingGivenCardinalPositions700# comparison queries600500400300200100010100# alternatives1000Figure 2: Experimental comparison FindRankingGivenPositions FindRankingGivenCardinalPositions. Please note logarithmic scale x-axis. data pointaveraged 5 runs.ReferencesArrow, K. (1963). Social choice individual values (2nd edition). New Haven: CowlesFoundation. 1st edition 1951.Bartholdi, III, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. SocialChoice Welfare, 8 (4), 341354.Bartholdi, III, J., Tovey, C., & Trick, M. (1989). Voting schemes difficulttell election. Social Choice Welfare, 6, 157165.Black, D. (1948). rationale group decision-making. Journal Political Economy,56 (1), 2334.Blum, A., Jackson, J., Sandholm, T., & Zinkevich, M. (2004). Preference elicitationquery learning. Journal Machine Learning Research, 5, 649667.Boutilier, C. (2002). POMDP formulation preference elicitation problems. Proceedings National Conference Artificial Intelligence (AAAI), pp. 239246, Edmonton, AB, Canada.Brams, S. J., Jones, M. A., & Kilgour, D. M. (2002). Single-peakedness disconnectedcoalitions. Journal Theoretical Politics, 14 (3), 359383.188fiEliciting Single-Peaked Preferences Using Comparison QueriesBrams, S. J., Jones, M. A., & Kilgour, D. M. (2005). Forming stable coalitions: processmatters. Public Choice, 125, 6794.Braziunas, D., & Boutilier, C. (2005). Local utility elicitation GAI models. Proceedings21st Annual Conference Uncertainty Artificial Intelligence (UAI), pp. 4249,Edinburgh, UK.Chajewska, U., Getoor, L., Norman, J., & Shahar, Y. (1998). Utility elicitationclassification problem. Proceedings Conference Uncertainty ArtificialIntelligence (UAI), pp. 7988, Madison, WI, USA.Chajewska, U., Koller, D., & Parr, R. (2000). Making rational decisions using adaptiveutility elicitation. Proceedings National Conference Artificial Intelligence(AAAI), pp. 363369, Austin, TX, USA.Clarke, E. H. (1971). Multipart pricing public goods. Public Choice, 11, 1733.Cohen, W., Schapire, R., & Singer, Y. (1999). Learning order things. Journal ArtificialIntelligence Research, 10, 213270.Conen, W., & Sandholm, T. (2001). Preference elicitation combinatorial auctions: Extended abstract. Proceedings ACM Conference Electronic Commerce (EC),pp. 256259, Tampa, FL, USA.Conitzer, V. (2006). Computing Slater rankings using similarities among candidates.Proceedings National Conference Artificial Intelligence (AAAI), pp. 613619,Boston, MA, USA.Conitzer, V. (2007). Eliciting single-peaked preferences using comparison queries. Proceedings International Conference Autonomous Agents Multi-Agent Systems(AAMAS), pp. 408415, Honolulu, HI, USA.Conitzer, V., & Sandholm, T. (2002). Vote elicitation: Complexity strategy-proofness.Proceedings National Conference Artificial Intelligence (AAAI), pp. 392397,Edmonton, AB, Canada.Conitzer, V., & Sandholm, T. (2005). Communication complexity common voting rules.Proceedings ACM Conference Electronic Commerce (EC), pp. 7887, Vancouver, BC, Canada.Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidateshard manipulate? Journal ACM, 54 (3), Article 14, 133.Cramton, P., Shoham, Y., & Steinberg, R. (2006). Combinatorial Auctions. MIT Press.Ephrati, E., & Rosenschein, J. S. (1991). Clarke tax consensus mechanism amongautomated agents. Proceedings National Conference Artificial Intelligence(AAAI), pp. 173178, Anaheim, CA, USA.189fiConitzerEphrati, E., & Rosenschein, J. S. (1993). Multi-agent planning dynamic search socialconsensus. Proceedings Thirteenth International Joint Conference ArtificialIntelligence (IJCAI), pp. 423429, Chambery, France.Escoffier, B., Lang, J., & Ozturk, M. (2008). Single-peaked consistency complexity.Proceedings DIMACS-LAMSADE Workshop Algorithmic Decision Theory,pp. 101114, Paris, France.Gibbard, A. (1973). Manipulation voting schemes: general result. Econometrica, 41,587602.Groves, T. (1973). Incentives teams. Econometrica, 41, 617631.Hemaspaandra, E., & Hemaspaandra, L. A. (2007). Dichotomy voting systems. JournalComputer System Sciences, 73 (1), 7383.Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (1997). Exact analysis Dodgsonelections: Lewis Carrolls 1876 voting system complete parallel access NP. JournalACM, 44 (6), 806825.Hinich, M. J., & Munger, M. C. (1997). Analytical Politics. Cambridge University Press.Lahaie, S., & Parkes, D. (2004). Applying learning algorithms preference elicitation.Proceedings ACM Conference Electronic Commerce (EC), pp. 180188, NewYork, NY, USA.Lang, J. (2007). Vote aggregation combinatorial domains structured preferences.Proceedings Twentieth International Joint Conference Artificial Intelligence(IJCAI), pp. 13661371, Hyderabad, India.Parkes, D. (2006). Iterative combinatorial auctions. Cramton, P., Shoham, Y., & Steinberg, R. (Eds.), Combinatorial Auctions, chap. 2, pp. 4177. MIT Press.Pennock, D. M., Horvitz, E., & Giles, C. L. (2000). Social choice theory recommendersystems: Analysis axiomatic foundations collaborative filtering. ProceedingsNational Conference Artificial Intelligence (AAAI), pp. 729734, Austin, TX,USA.Procaccia, A. D., & Rosenschein, J. S. (2007). Junta distributions average-casecomplexity manipulating elections. Journal Artificial Intelligence Research, 28,157181.Rothe, J., Spakowski, H., & Vogel, J. (2003). Exact complexity winner problemYoung elections. Theory Computing Systems, Vol. 36(4), pp. 375386. SpringerVerlag.Sandholm, T., & Boutilier, C. (2006). Preference elicitation combinatorial auctions.Cramton, P., Shoham, Y., & Steinberg, R. (Eds.), Combinatorial Auctions, chap. 10, pp.233263. MIT Press.190fiEliciting Single-Peaked Preferences Using Comparison QueriesSanti, P., Conitzer, V., & Sandholm, T. (2004). Towards characterization polynomialpreference elicitation value queries combinatorial auctions. ConferenceLearning Theory (COLT), pp. 116, Banff, Alberta, Canada.Satterthwaite, M. (1975). Strategy-proofness Arrows conditions: Existence correspondence theorems voting procedures social welfare functions. JournalEconomic Theory, 10, 187217.Vu, H., & Haddawy, P. (1997). Problem-focused incremental elicitation multi-attributeutility models. Proceedings Conference Uncertainty Artificial Intelligence(UAI), pp. 215222, San Francisco, CA, USA.Vu, H., & Haddawy, P. (1998). Towards case-based preference elicitation: Similarity measures preference structures. Proceedings Conference Uncertainty Artificial Intelligence (UAI), pp. 193201.Walsh, T. (2007). Uncertainty preference elicitation aggregation. ProceedingsNational Conference Artificial Intelligence (AAAI), pp. 38, Vancouver, BC,Canada.Zinkevich, M., Blum, A., & Sandholm, T. (2003). polynomial-time preference elicitationvalue queries. Proceedings ACM Conference Electronic Commerce (EC),pp. 176185, San Diego, CA, USA.191fiJournal Artificial Intelligence Research 35 (2009) 49-117Submitted 10/08; published 05/09Message-Based Web Service Composition, Integrity Constraints,Planning Uncertainty: New ConnectionJorg HoffmannJOE . HOFFMANN @ SAP. COMSAP ResearchKarlsruhe, GermanyPiergiorgio BertoliBERTOLI @ FBK . EUFondazione Bruno KesslerTrento, ItalyMalte HelmertHELMERT @ INFORMATIK . UNI - FREIBURG . DEAlbert-Ludwigs-Universitat FreiburgFreiburg, GermanyMarco PistorePISTORE @ FBK . EUFondazione Bruno KesslerTrento, ItalyAbstractThanks recent advances, AI Planning become underlying technique several applications. Figuring prominently among automated Web Service Composition (WSC)capability level, services described terms preconditions effects ontological concepts. key issue addressing WSC planning ontologies formalvocabularies; also axiomatize possible relationships concepts. axioms correspond termed integrity constraints actions change literature,applying web service essentially belief update operation. reasoning required beliefupdate known harder reasoning ontology itself. support belief updateseverely limited current planning tools.first contribution consists identifying interesting special case WSCsignificant tractable. special case, term forward effects, characterizedfact every ramification web service application involves least one new constantgenerated output web service. show that, setting, reasoning requiredbelief update simplifies standard reasoning ontology itself. relates to, extends,current notions message-based WSC, need belief update removed strong(often implicit informal) assumption locality individual messages. clarifycomputational properties forward effects case, point strong relation standard notions planning uncertainty, suggesting effective tools latter successfullyadapted address former.Furthermore, identify significant sub-case, named strictly forward effects, actualcompilation planning uncertainty exists. enables us exploit off-the-shelf planning tools solve message-based WSC general form involves powerful ontologies,requires reasoning partial matches concepts. provide empirical evidenceapproach may quite effective, using Conformant-FF underlying planner.c2009AI Access Foundation. rights reserved.fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE1. IntroductionSince mid-nineties, AI Planning tools become several orders magnitude scalable,invention automatically generated heuristic functions search techniques(see McDermott, 1999; Bonet & Geffner, 2001; Hoffmann & Nebel, 2001; Gerevini, Saetti, &Serina, 2003; Helmert, 2006; Chen, Wah, & Hsu, 2006). paved way adoptionplanning underlying technology several applications. One application areaweb service composition (WSC), paper mean automated compositionsemantic web services (SWS). SWS pieces software advertised formal descriptiondo. Composing SWS means link together aggregate behaviorsatisfies complex user requirement. ability automatically compose web services keyreducing human effort time-to-market constructing integrated enterprise applications.result, widely recognized economic potential WSC.wide-spread SWS frameworks OWL-S1 WSMO2 , SWS described two distinctlevels. One addresses overall functionality SWS, details preciselyinteract SWS. former level, called service profile OWL-S servicecapability WSMO, SWS described akin planning operators, preconditions effects. Therefore, planning prime candidate realizing WSC level. approachfollow paper.setting, key aspect SWS preconditions effects described relativeontology defines formal (logical) vocabulary. Indeed, ontologies muchformal vocabularies introducing set logical concepts. also define axioms constrain behavior domain. instance, ontology may define subsumption relationshiptwo concepts B, stating members necessarily members B.natural interpretation axiom, context WSC, every state encountered every possible configuration domain entities must satisfy axiom. sense,ontology axioms correspond integrity constraints discussed actions change literature(Ginsberg & Smith, 1988; Eiter & Gottlob, 1992; Brewka & Hertzberg, 1993; Lin & Reiter, 1994;McCain & Turner, 1995; Herzig & Rifi, 1999).3 Hence WSC considered like planningpresence integrity constraints. Since constraints affect outcome action executions,facing frame ramification problems, execution actions corresponds closelycomplex notions belief update (Lutz & Sattler, 2002; Herzig, Lang, Marquis, & Polacsek,2001). Unsurprisingly, providing support integrity constraints modern scalable planning tools mentioned poses serious challenges. best knowledge, yetattempted all.Regarding existing WSC tools, planning tools employed solving WSC problems,situation isnt much better. tools ignore ontology, i.e., act constraintsdomain behavior given (Ponnekanti & Fox, 2002; Srivastava, 2002; Narayanan & McIlraith,2002; Sheshagiri, desJardins, & Finin, 2003; Pistore, Traverso, & Bertoli, 2005b; Pistore, Marconi, Bertoli, & Traverso, 2005a; Agarwal, Chafle, Dasgupta, Karnik, Kumar, Mittal, & Srivastava,2005a). approaches tackle full generality belief update using general reasoners,1. example, see work Ankolekar et al. (2002) Burstein et al. (2004).2. example, see work Roman et al. (2005) Fensel et al. (2006).3. Integrity constraints sometimes also called state constraints domain constraints.50fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONsuffer inevitable performance deficiencies (Eiter, Faber, Leone, Pfeifer, & Polleres, 2003;Giunchiglia, Lee, Lifschitz, McCain, & Turner, 2004).planningbasedformalizationWSC Formalismvariantrestrictionrich versionForward EffectsWSCplanningbasedformalizationMessagebased WSCrestrictionConformant PlanningtackledStrictly Forward EffectsFigure 1: overview planning WSC frameworks addressed paper. Special casesidentified herein shown red / boldface.work addresses middle ground two extremes, i.e., trade-offexpressivity scalability WSC. via identification special casestackled efficiently. Figure 1 gives overview WSC planning frameworks involved.brief, forward effects case requires every effect ramification web serviceaffects least one new constant generated web services output. situation,frame problem trivializes, making planning problem similar common notionsconformant planning (Smith & Weld, 1998; Bonet & Geffner, 2000; Cimatti, Roveri, & Bertoli,2004; Hoffmann & Brafman, 2006). discuss existing tools latter, particularConformant-FF (Hoffmann & Brafman, 2006), extended deal WSC forwardeffects. strictly forward effects, action effects required affect outputs,devise actual compilation conformant planning. thus obtain scalable tool interestingWSC problems integrity constraints. particular able exploit (some of) heuristictechniques mentioned (Hoffmann & Nebel, 2001; Hoffmann & Brafman, 2006).follows, explain various parts Figure 1 little detail. startingpoint WSC formalism, addressing WSC terms planning presence integrity constraints, discussed above. formalism essentially enriched form conformant planning.distinguishing aspects are:initial state description conjunction literals (possibly mentioninglogical facts task, hence introducing uncertainty).Actions conditional effects semantics, meaning executed state,effect applicable.Actions may output variables, i.e., may create new constants.set integrity constraints, universally quantified clause.semantics action execution defined terms belief update operation.Section 2 provides details choices, motivates exampleresults literature. show, planning formalism hard. Particularly,51fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREeven testing whether given action sequence plan p2 -complete. contrastcommon notions conformant planning, plan testing coNP-complete.see, forward effects remove additional complexity. Intuitively, forward effectscase covers situation web service outputs new constants, sets characteristicproperties relative inputs, relies ontology axioms describe ramificationsconcerning new constants. case syntactically characterized follows:(1) Every effect literal contains least one output variable.(2) Within integrity constraint, every literal set variables arguments.definition best understood example. Consider following variant widespread virtual travel agency (VTA). Web services book travel accommodation mustlinked. web services generate new constants corresponding tickets reservations.example, integrity constraints stating subsumption, z : trainTicket(z)ticket(z). web service bookTicket may input variable x, precondition train(x ),output variable y, effect trainTicket(y) ticketFor (y, x). forward effects task:every effect literal contains output variable y, integrity constraint single variablez provides arguments literals constraint. Say one instantiates inputbookTicket constant c output new constant d. applying resultingground action state train(c) holds true, constant gets created, characteristicproperties relative inputs trainTicket(d) ticketFor (d, c) set directly action.integrity constraint takes care ramification, establishing ticket(d) holds. Notestatus c apart relation affected way. 4forward effects case closely related wide-spread notion WSC problems,refer message-based WSC. approaches, composition semantics basedchaining input output messages web services, one sense. Inferencesontology axioms made many approaches, restricted way limitedassumption locality individual messages, interferences affect particularmessage transfer, implications transfers ignored. locality assumptionusually made informal way, often stated explicitly all. One contribution workshed light issue, via identification forward effects case liesmessage-based WSC full planning framework belief update semantics.message-based WSC forward effects case share focus output constants.two important differences. First, forward effects case restricted messagebased WSC terms ontology axioms allowed. Essentially, forward effects correspondspecial case WSC locality assumption message-based WSC actually justified,within full planning framework. Second, full framework comes benefit increasedflexibility combination services, locality enforced (e.g. output oneservice may reused several points plan).computational point view, key property forward effects caseremoves need belief update. nutshell, reason actions affect new propositions, i.e., propositions involving least one output constant. (Recall point made4. latter would case effect bookTicket included literal affecting x (example:train(x)), integrity constraint capable mixing old new constants (example: x, :trainTicket(y) train(x)).52fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONunchanged status c, VTA example above.) output constant (d, example)exist prior application action, hence previous belief carries knowledgeneed revised. Consider characterization forward effects, given above.Condition (1) ensures immediate effect action affects new propositions. Condition (2) ensures changes new propositions propagate new propositions. Sinceliterals constraint share variables, output constant question copiedthem. see, virtue properties complexity plan testing coNP-complete,rather p2 -complete, forward effects case.complexity reduction critical reduced complexitycommon notions conformant planning initial state uncertainty. Therefore feasible adapt conformant planning tools address WSC forward effects. Scalable planningtools conformant planning already developed (Cimatti et al., 2004; Bryce, Kambhampati, & Smith, 2006; Hoffmann & Brafman, 2006; Palacios & Geffner, 2007). Hencepromising line research. example, focus Conformant-FF tool (Hoffmann& Brafman, 2006) (short CFF) outline main steps need taken adapting CFFhandle WSC forward effects.identify case actual compilation conformant planning initialstate uncertainty exists. that, one must fix set constants priori. mannerfairly standard (see, e.g., Settlers domain Long & Fox, 2003), simply include setsubset potential constants used instantiate outputs. subtle ideaput forward identify condition actions predict propertiesassigned potential constants, case created. enables us designcompilation moves action effects initial state formula, uses actionsmodify set constants already exist. way, reasoning initial state formulacompiled task reasoning output constants original task,reasoning mechanisms included tools CFF naturally used implement latter.trick predicting output properties require actions compatible senseeither produce different outputs, effects. turns conditionnaturally given restriction forward effects, call strictly forward effects,web service effects concern new constants.Clearly, able reference inputs limitation. example, longersay, VTA example, output ticket input x. Still, strictly forwardeffects case describes interesting class WSC problems. class corresponds web servicesmodeled early versions OWL-S, example, logical connectioninputs outputs. Further, class WSC problems allows powerful ontologiesuniversally quantified clauses makes possible combine services flexibly. Usingcompilation, class problems solved off-the-shelf tools planninguncertainty.validate compilation approach empirically running number tests using CFFunderlying planner. use two test scenarios, scalable variety parameters, covering range different problem structures. examine CFF reacts variousparameters. Viewed isolation, results demonstrate large complex WSC instancescomfortably solved using modern planning heuristics.comparison alternative WSC tools problematic due widely disparate naturekinds problems tools solve, kinds input languages understand,53fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREpurpose respective developers mind. nevertheless provide assessmentcomparative benefits approach run tests DLVK tool Eiter et al. (2003)Eiter, Faber, Leone, Pfeifer, Polleres (2004). DLVK one planning toolsdeals ontology axioms called static causal rules directly, without need restrictforward effects without need compilation. Since, context work,main characteristic WSC presence ontology axioms, means DLVK oneexisting native WSC tools. comparison, forward effects compilation approachsolves similar problem, sacrifices expressivity. question is, principlegain anything sacrifice? Absolutely, answer yes. DLVK much slowercompilation+CFF, solving small fraction test instances even always providedcorrect plan length bound. emphasize wish over-state results,due above-mentioned differences tools. conclusion drawtrade-off expressivity scalability WSC important, forward effects caseseems constitute interesting point trade-off.paper organized follows. First, Section 2 provides background necessaryunderstand context contribution work. Section 3 introduces WSC planningformalism. Section 4 defines discusses forward effects. Section 5 introduces compilationplanning uncertainty, Section 6 presents empirical results. discuss closelyrelated work relevant points text, Section 7 provides complete overview.Finally, Section 8 concludes discusses future work. improve readability, proofsmoved Appendix replaced text proof sketches.2. Backgroundcontext work rather intricate. WSC new topic posing many differentchallenges existing techniques, effect field populated disparate works differing considerably underlying purpose scope. words, common groundfairly thin area. Further, work actually involves three fields research WSC, planning,reasoning actions change relevant understanding contribution.reasons, explain background detail. first discuss WSC general,WSC Planning particular. state relevant facts belief update. finallyconsider message-based WSC.2.1 WSC, WSC PlanningComposition semantic web services received considerable attention last years.general formulation problem, shared large variety works, focuses capabilitylevel, web service conceived atomic operator transforms concepts.specifically, service defined via IOPE description: service receives input settyped objects, and, provided precondition P holds, produces output settyped objects effect E guaranteed hold. typing objects exchangedservices given terms membership concepts. Concepts classes definedwithin ontologies, exploit Description Logics (DL), form logic, formallydefine universe concepts admitted discourse. ontology express complex relationships among concepts, like subsumption hierarchy, way objects belonging conceptstructured parts referring concepts.54fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONgeneral setting instantiated various ways depending kind conditionsadmitted preconditions/effects services, kind logics underlying ontologydefinitions. Independent this, problem semantic web service composition statedone linking appropriately set existing services aggregate behaviordesired service (the goal). illustrate problem, consider following example,inspired work Thakkar, Ambite, Knoblock (2005) e-services bioinformatics(and relies actual structure proteins, see example Petsko & Ringe, 2004; Branden &Tooze, 1998; Chasman, 2003; Fersht, 1998):Example 1 Say want compose web service provides information different classesproteins. ontology states classes proteins exist, structural characteristicsmay occur. available information service every structural characteristic,presentation service combines range information. Given particular protein class,composed web service run relevant information services, present output.Concretely, classes proteins distinguished location (cell, membrane, intermembrane, . . . ). modeled predicates protein(x), cellProtein(x), membraneProtein(x),intermembraneProtein(x), along sub-concept relations x : cellProtein(x)protein(x). individual protein characterized following four kinds structures:1. primary structure states proteins sequence amino-acids, e.g., 1kw3(x) (a protein called Glyoxalase) 1n55(x) (a protein called Triosephosphate Isomerase).2. secondary structure states proteins external shape terms DSSP (Dictionary Secondary Structure Proteins) code, admitting limited set possible values.example, G indicates 3-turn helix, B -sheet, on. total set valuesG,H,I,T,E,B,S.3. tertiary structure categorizes proteins 3-D shape.4. subset proteins, quaternary structure categorizes proteins shapecombined complexes proteins (amounting 3000 different shapes, see example3DComplex.org, 2008).various axioms constrain domain, apart mentioned subconceptrelations. First, obvious axioms specify protein value fourkinds structures (i.e., protein sequence amino-acids, external shape, etc). However,also complex axioms. Particular kinds proteins come particular structurevalues. modeled axioms as:x : cellProtein(x) G(x) 1n55(x)x : cellProtein(x) B(x) 1kw3(x) complexBarrel(x)DSSP code Z information service, named getInfoDSSPZ , whose preconditionZ(x) whose effect InfoDSSP(y) output service. Similarly, information services amino-acids, 3-D shapes, shapes complexes. presentation service,named combineInfo, requires information four kinds structures created,effect combinedPresentation(y) (where output combineInfo).55fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREinput composed web service protein c (a logical constant) class.goal x : combinedPresentation(x). solution reason characteristics mayoccur, apply respective information services, run combineInfo. variantproblem, additional requestInfo service used initiate information request, i.e.,output requestInfo protein c class.example shows ontology axioms play crucial role form WSC, formulatingcomplex dependencies different concepts. Note applying web service may indirect consequences implied ontology axioms. example, output requestInfoservice implications kinds information services required.Another interesting aspect Example 1 requires SWS community calls partial matches, opposed plug-in matches (Paolucci, Kawamura, Payne, & Sycara, 2002; Li& Horrocks, 2003; Kumar, Neogi, Pragallapati, & Ram, 2007).5 Consider situation onewants connect web service w another web service w . is, w executed priorw , output w used instantiate input w . w w saidpartial match if, given ontology axioms, output w sometimes suffices providenecessary input w . contrast, w w said plug-in match if, given ontologyaxioms, output w always suffices provide necessary input w .Plug-in matches tackled many approaches WSC, whereas partial matches tackledfew. Part reason probably plug-in matches easier handle, many typesWSC algorithms. Indeed existing WSC tools support plug-in matches (see detaileddiscussion WSC tools Section 7). Example 1 cannot solved plug-in matchesinformation services provides necessary input combineInfo serviceparticular cases.base work planning formalism allows specify web services (i.e., actions)outputs, allows specify ontology axioms. axioms interpreted integrityconstraints, resulting semantics corresponds closely common intuitions behind WSC,well existing formal definitions related WSC (Lutz & Sattler, 2002; Baader, Lutz,Milicic, Sattler, & Wolter, 2005; Liu, Lutz, Milicic, & Wolter, 2006b, 2006a; de Giacomo, Lenzerini, Poggi, & Rosati, 2006). Since one main aims able exploit existing planningtechniques, consider particular form ontology axioms, correspondence representations used existing tools planning uncertainty. Namely, axiomsuniversally quantified clauses. example subsumption relation x : trainTicket(x)ticket(x) mentioned above, usual B abbreviation B. planning taskspecifies set clauses, interpreted conjunction clauses. Note providessignificant modeling power. meaning universal quantification clausesclauses hold planning objects logical constants known exist. sense,interpretation formulas closed-world customary planning tools. However, contraststandard planning formalisms including PDDL, assume fixed set constants.Rather, specification actions outputs enables dynamic creation new constants.quantifiers ontology axioms range constants exist respective world.similar fashion, planning goal may contain variables, existentially quantified.constants used instantiate goal may pre-existed, may generated5. terminology works slightly different use here, also describe additional kindsmatches. details given Section 7.56fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONoutputs web services applied path world. Considerillustration goal x : combinedPresentation(x) Example 1, goal variable xinstantiated output created combineInfo service.Another important aspect planning formalism allow incomplete initial statedescriptions. initial state corresponds input user provides composed webservice. Certainly cannot assume contains complete information every aspectworld. (In Example 1, initial state tells us class proteins interestedin, leaves open consequences regarding possible structural characteristics.)consider case observability, i.e., conformant planning. outcomeWSC sequence web services satisfies user goal possible situations.6customary conformant planning, actions conditional effects semantics, i.e., fireprecondition holds true, otherwise nothing. Note that, way, obtain notionpartial matches: solution employs different actions depending situation.main difference planning formalism formalisms underlying current planning tools presence integrity constraints, effect semantics executing actions. semantics defined belief update operation.2.2 Belief Updatecorrespondence web service applications belief update first observed LutzSattler (2002), followed Baader et al. (2005), Liu et al. (2006b, 2006a) de Giacomoet al. (2006). original statement belief update problem, given belief ,i.e., logical formula defining worlds considered possible. given formula ,update. Intuitively, corresponds observation telling us world changedway that, now, true. want obtain formula defining worldspossible given update. Certainly, need |= . Ensuring correspondswell-known ramification problem. time, however, world changeunnecessarily. is, want close possible , among formulassatisfy . corresponds frame problem.Say want apply action presence integrity constraints. describesworlds possible prior application a. resulting set possible worlds.integrity constraints correspond formula IC holds , requirehold . update formula given conjunction action effect IC , i.e.,= effa IC . means update previous belief information that,a, effa new formula required hold, IC still true. example, mayaction effect A(c) subsumption relation concepts B, formulated clausex : A(x) B(x). update formula A(c) x : A(x) B(x) ensures B(c) true.Belief update widely considered literature AI databases (see exampleFagin, Kuper, Ullman, & Vardi, 1988; Ginsberg & Smith, 1988; Winslett, 1988, 1990; Katzuno& Mendelzon, 1991; Herzig, 1996; Herzig & Rifi, 1999; Liu et al., 2006b; de Giacomo et al.,2006). various approaches differ exactly defined. best consensusone approach adequate every application context. approaches6. course, generally, observability partial web service effects also uncertain. considergeneralizations here. Extending notions accordingly straightforward, future work.57fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREagree hold updated state affairs, |= . Major differences lieexactly taken mean close possible . Various authors,example Brewka Hertzberg (1993), McCain Turner (1995), Herzig (1996), GiunchigliaLifschitz (1998), argue notion causality needed, addition (or even instead of)notion integrity constraints, model domain behavior natural way. counterarguments, neither follow causal approach work. reason ontologiescontext WSC, example ontologies formulated web ontology language OWL(McGuinness & van Harmelen, 2004), incorporate notion causality. givenset axioms, made intention describe behavior domain itself, ratherbehavior exhibits changed particular web services. idea work tryleverage (or reasonably close having). Consideration causalapproaches WSC left future work.Belief update computationally hard problem. Eiter Gottlob (1992) Liberatore(2000) show that, non-causal approaches defining , reasoning typicallyharder reasoning class formulas used formulating . Specifically, decidingwhether particular literal true 2p -hard even complete conjunctionliterals (corresponding single world state) propositional CNF formula.problem coNP-hard even single world state propositional Horn formula.use results show that, planning formalism, checking plan testing whethergiven action sequence plan 2p -complete, deciding polynomially bounded planexistence 3p -complete.Given complexity, perhaps unsurprising support integrity constraints current planning tools severely limited. existing planning tools support integrityconstraints, namely Eiter et al. (2003) Giunchiglia et al. (2004), based genericdeduction, like satisfiability testing answer set programming. hence lack planningspecific heuristic search techniques key scalability modern planning toolsdeveloped since mid-nineties. even investigated yet integrity constraints could handled latter tools. existing approach ventures direction implements so-called derived predicates modern planning tools (Thiebaux,Hoffmann, & Nebel, 2005; Gerevini, Saetti, Serina, & Toninelli, 2005; Chen et al., 2006).approach postulates strict distinction basic predicates may affected actions,derived predicates may affected integrity constraints taking form logic programming rules. predicate appears action effect, allowed appearhead rule. desirable restriction context WSC, web servicesbound affect properties constrained ontology axioms.existing work connecting WSC belief update (Lutz & Sattler, 2002; Baader et al.,2005; Liu et al., 2006b, 2006a; de Giacomo et al., 2006) theoretical nature. actual implemented WSC tools make severe simplifying assumptions. often, assumption ignoreontology axioms (Ponnekanti & Fox, 2002; Srivastava, 2002; McIlraith & Son, 2002; Sheshagiriet al., 2003; Sirin, Parsia, Wu, Hendler, & Nau, 2004; Pistore et al., 2005b, 2005a). Sometimes,ontology constraints restricted subsumption hierarchies, makes update problemeasy (Constantinescu & Faltings, 2003; Constantinescu, Faltings, & Binder, 2004b, 2004a). SirinParsia (2004) Sirin, Parsia, Hendler (2006) discuss problem dealing ontology axioms WSC, make connection belief update, describe alternativesolution. Finally, authors, example Meyer Weske (2006), deal ontology ax58fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONioms composition, provide formal semantics specify exactlyaction applications handled. seems fully formalized WSC approaches implicitlyassume message-based framework. frameworks closely related forward effectsspecial case identified herein.2.3 Message-Based WSCmessage-based approaches WSC, composition semantics based chaining inputoutput messages web services. word message standard term context.authors use individual vocabulary. far aware, first appearanceword message WSC paper title work Liu, Ranganathan, Riabov (2007).work describes message-based WSC follows. solution directed acyclic graph (DAG)web services, input needed web service (DAG graph node) w must providedoutputs predecessors w graph. is, plan determines fixed connectionsactions. Reasoning, then, takes place within connections. two connectionsdifferent output input messages, i.e., two graph edges ending different node,assumed mutually independent. Consider following example illustration. Say webservice w effect hasAttributeA(c, d) output constant c input (i.e., cexisted already prior application w). Say axiom x, : hasAttributeA(x, y)conceptB(x) expressing attribute domain restriction. x value attribute A,x must concept B. Given this, ws effect implies conceptB(c). Now, supposebelief prior applying w constrain c concept B. applying w leads newknowledge c. Hence need non-trivial belief update taking account changedstatus c, implications may have. Message-based WSC simply acts lattercase. checks whether w correctly supplies inputs web services w wconnected to. is, new fact hasAttributeA(c, d) may taken part proofeffect w implies precondition connected web service w . consideredimplications hasAttributeA(c, d) may respect previous state affairs.sense, message-based WSC ignores need belief update.intuitions underlying message-based WSC fairly wide-spread. Many papers useless direct way. many approaches explicitly define WSC solutionsDAGs local input/output connections (Zhan, Arpinar, & Aleman-Meza, 2003; Lecue& Leger, 2006; Lecue & Delteil, 2007; Kona, Bansal, Gupta, & Hite, 2007; Liu et al., 2007; Ambite& Kapoor, 2007). various works (Constantinescu & Faltings, 2003; Constantinescu et al.,2004b, 2004a; Meyer & Weske, 2006), message-based assumptions implicit.manifest mainly sense ontology axioms used infer propertiesoutput messages, often checking whether inferences imply desired inputmessage definitely given.Previous work message-based WSC address message-based WSC relatesvarious notions, like belief update, considered literature. One contribution workshed light issue, via identification forward effects case liesmessage-based WSC full planning framework belief update semantics.message-based WSC forward effects case share focus outputs. Indeed,output constants generated actions viewed messages. output constantrepresents information object created one web service, form59fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREinput web service. forward effects case, due restriction axioms,individual messages interact. much like message-based WSC. main differencethis: message-based WSC ignores possible interactions, forward effects actuallyarent interactions, according formal planning-based execution semantics. sense,forward effects correspond special case WSC assumptions message-based WSCjustified.Reconsider example above, featuring web service w effect implyingconceptB(c) c pre-existing constant. explained above, message-based WSCsimply ignore need updating knowledge c. contrast, forward effects casedisallows axiom x, : hasAttributeA(x, y) conceptB(x) may lead newconclusions old belief (note literals axiom refer different sets variables).forward effects case also differs significantly approaches message-based WSCterms flexibility allows combine actions plans. messagebased approach using DAGs, solution DAG ensures inputs service w alwaysprovided ws predecessors. is, plug-in match set W wspredecessors DAG, w itself. Note slightly general usual notionplug-in matches, |W | may greater 1, hence single service W maypartial match w. notion used, amongst others, Liu et al. (2007).authors, example Lecue Leger (2006) Lecue Delteil (2007), restrictiveconsider every individual input x w turn require exists w Ww plug-in match x (i.e., w guarantees always provide x). Even generoustwo definitions, partial matches restricted appear locally, DAG links. Everyaction/web service required always executable point applied.words, services used fixed manner, considering dynamics actual execution.Example 1, would mean using information services regardless classprotein, hence completely ignoring relevant not.forward effects case incorporates much general notion partial matches. happens straightforward way, exploiting existing notions planning, form conditional effects semantics. standard notion conformant solution defines partial matchesmust work together global level, accomplish goal. best knowledge,one line work WSC, Constantinescu et al. (Constantinescu & Faltings, 2003;Constantinescu et al., 2004b, 2004a), incorporates comparable notion partial matches.work, web services characterized terms input output types. handle partialmatches, so-called switches combine several web services way ascertains relevantcases covered. switches designed relative subsumption hierarchy types.Note subsumption hierarchies special case much general integrity constraintsuniversally quantified clauses consider work.3. Formalizing WSCsolid basis addressing WSC, define planning formalism featuring integrity constraints,on-the-fly creation output constants, incomplete initial state descriptions, actions conditional effects semantics. application actions defined belief update operation, following possible models approach Winslett (1988). definition belief update somewhatcanonical widely used discussed. particular underlies recent work60fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONrelating formalizations WSC (Lutz & Sattler, 2002; Baader et al., 2005; Liu et al., 2006b,2006a; de Giacomo et al., 2006; de Giacomo, Lenzerini, Poggi, & Rosati, 2007). show(Section 4.3), belief update operations equivalent anyway soonforward effects case. Recall forward effects case central objectinvestigation paper.first give syntax formalism, denote WSC, give semantics. conclude analysis main computational properties.3.1 Syntaxdenote predicates G, H, I, variables x, y, z, constants c, d, e. Literals possibly negated predicates whose arguments variables constants. arguments constants,literal ground. refer positive ground literals propositions. Given set P predicatesset C constants, denote P C set propositions formed PC. Given set X variables, denote LX set literals l use variablesX. Note l may use arbitrary predicates constants.7 l literal, writel[X] indicate l variable arguments X. X = {x1 , . . . , xk } C = (c1 , . . . , ck ),l[c1 , . . . , ck /x1 , . . . , xk ] denote respective substitution, abbreviated l[C].way, use substitution notation construct involving variables. Slightly abusingnotation, use vector constants also denote set constants appearing it. Further,function assigns constants variables X, l[a/X] denote substitutionargument x X replaced a(x). concerned first-order logic, is,whenever write formula mean first-order formula. denote true 1 false 0.clause, integrity constraint, disjunction literals universal quantificationoutside. variables quantified exactly appear least one literals.example, x, : G(x, y) H(x) integrity constraint x, y, z : G(x, y) H(x) x :G(x, y)H(x) not. operator tuple (Xo , preo , Yo , effo ), Xo , Yo sets variables, preo conjunction literals LXo , effo conjunction literals LXo Yo .8intended meaning Xo inputs Yo outputs, i.e., new constants createdoperator. operator o, action given (prea , effa ) (preo , effo )[Ca /Xo , Ea /Yo ]Ca Ea vectors constants. Ea require constants pairwise different makes sense output new constant twice. Given action a, referinputs outputs Ca Ea , respectively. also use notations prea , effaobvious meaning.WSC task, planning task, tuple (P, IC , O, C0 , 0 , G ). Here, P set predicates.IC set integrity constraints. set operators C0 set constants, initialconstants supply. 0 conjunction ground literals, describing possible initial states. Gconjunction literals existential quantification outside, describing goal states,e.g., x, : G(x) H(y). predicates taken P, constants taken C0 .constructs (e.g., sets conjunctions) finite. sometimes identify ICconjunction clauses contains. Note existential quantification goal variables7. One could course introduce general notations logical constructs using set predicates constants.However, herein two notations given suffice.8. stated, address disjunctive non-deterministic effects. topic future work.61fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREprovides option instantiate goal constants created planning obtaining objectsrequested goal may possible use outputs.various formulas occurring (P, IC , O, C0 , 0 , G ) may make use constants C0 .Specifically, case clauses IC goal formula G . Allowing useconstants effect complexity algorithmic results. conceivablefeature may useful. simple example, VTA domain user may wish selectparticular train. Say train company provides table trains itineraries. tablerepresented 0 , possibly help IC stating constraints hold particular trains.user select train, say ICE107, pose goal : ticketFor (y, ICE107).Constraining produced ticket way would possible without use pre-existingconstants (or would least require rather dirty hack, e.g., encoding desired train termsspecial predicate).Operator descriptions, is, preconditions effects, may also use constants C0 .value benign IC G one always replace constant cprecondition/effect new input/output variable x, instantiate x (during planning)c. Note, however, would give planner option (uselessly) instantiate xconstant, may hence affect planning performance. example, mightspecial operator booking ticket ICE107 (e.g., train particular ticketing regulations).correspondence WSC task web service composition task fairly obvious.set P predicates formal vocabulary used underlying ontology. set ICintegrity constraints set axioms specified ontology, i.e., domain constraintssubsumption relations. set operators set web services. Note formalizationcorresponds closely notion IOPE descriptions: inputs, outputs, preconditions,effects (Ankolekar et al., 2002; Burstein et al., 2004). action corresponds web service call,web services parameters instantiated call arguments.constructs C0 , 0 , G extracted user requirement composition.assume requirements also take form IOPE descriptions. Then, C0user requirement inputs, 0 user requirement precondition. words, C0 0describe input given composition user. Similarly, G user requirement effectcondition user wants accomplished user requirement outputs(existentially quantified) variables G .3.2 Semanticsfollows, assume given WSC task (P, IC , O, C0 , 0 , G ). able modelcreation constants, states (also called world states) formalism enriched setconstants exist them. state pair (Cs , ) Cs set constants,Cs -interpretation, i.e., truth value assignment : P Cs 7 {0, 1}. Quantifiers taken rangeconstants exist state. is, C-interpretation formula,writing |= mean |= C C except quantifiersrestricted range C. avoid clumsy notation, sometimes write |= abbreviate|= .core definition specifies application action affects state. definedform belief update. Let us first define latter. Assume state s, set constantsC Cs , formula . define update(s, C , ) set interpretations result62fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONcreating constants C \ Cs , updating according semantics proposedWinslett (1988).Say I1 I2 C -interpretations. define partial order interpretations,setting I1 <s I2{p P Cs | I1 (p) 6= (p)} {p P Cs | I2 (p) 6= (p)}.(1)words, I1 ordered I2 iff differs proper subset values. Given this,formally define update(s, C , ). Let arbitrary C -interpretation. defineupdate(s, C , ) : |= {I | |= , <s I} = .(2)Hence, update(s, C , ) defined set C -interpretations satisfy ,minimal respect partial order <s . Put different terms, update(s, C , ) containsinterpretations differ set-inclusion minimal set values.Now, assume action a. say applicable s, short appl(s, a), |= prea ,Ca Cs , Ea Cs = . is, top usual precondition satisfaction requireinputs exist outputs yet exist. result executing is:{(C , ) | C = Cs Ea , update(s, C , IC effa )} appl(s, a)(3)res(s, a) :={s}otherwiseNote executed even applicable. case, outcome singletonset containing itself, i.e., action affect state. important aspectformalism, get back below. IC effa unsatisfiable, obviously getres(s, a) = . say case inconsistent.9overall semantics WSC tasks easily defined via standard notion beliefs.model uncertainty true state world. belief b set world statespossible given point time. initial beliefb0 := {s | Cs = C0 , |= IC 0 }.(4)action inconsistent belief b inconsistent least one b. lattercase, res(b, a) undefined. Otherwise, defined[res(s, a).(5)res(b, a) :=sbextended action sequences obvious way. plan sequence ha1 , . . . ,res(b0 , ha1 , . . . , i) : |= G .(6)illustration, consider formalization example Section 2.Example 2 Reconsider Example 1. sake conciseness, formalize partexample, simplified axioms. WSC task defined follows:9. Unless IC mentions constants, based operator inconsistent, action basedinconsistent. operators can, principle, filtered pre-process planning.63fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREP = {protein, cellProtein, G, H, I, 1n55, 1kw3, InfoDSSP, Info3D, combinedPresentation},predicates unary.IC consists clauses:x : cellProtein(x) protein(x) [subsumption]x : protein(x) G(x) H(x) I(x) [at least one DSSP value]x : protein(x) 1n55(x) 1kw3(x) [at least one 3-D shape]x : cellProtein(x) G(x) 1n55(x) [dependency]x : cellProtein(x) H(x) 1n55(x) [dependency]consists operators:getInfoDSSPG : ({x}, G(x), {y}, InfoDSSP(y))getInfoDSSPH : ({x}, H(x), {y}, InfoDSSP(y))getInfoDSSPI : ({x}, I(x), {y}, InfoDSSP(y))getInfo3D1n55 : ({x}, 1n55(x), {y}, Info3D(y))getInfo3D1kw3 : ({x}, 1kw3(x), {y}, Info3D(y))combineInfo: ({x1 , x2 }, InfoDSSP(x1 ) Info3D(x2 ), {y}, combinedPresentation(y))C0 = {c}, 0 = cellProtein(c)G = x : combinedPresentation(x)illustrate formalism, consider plan example task.initial belief b0 consists states Cs = {c} |= IC cellProtein(c). Sayapply following sequence actions:1. Apply getInfoDSSPG (c, d) b0 . get belief b1 b0 exceptthat, b0 |= G(c), new states generated constantInfoDSSP(d).2. Apply getInfoDSSPH (c, d) b1 . get belief b2 new statesInfoDSSP(d) generated b1 |= H(c).3. Apply getInfo3D1n55 (c, e) b2 , yielding b3 .4. Apply getInfo3D1kw3 (c, e) b3 . yields b4 , get e Info3D(e) b2|= 1n55(c) |= 1kw3(c).5. Apply combineInfo(d, e, f ) b4 . brings us b5 like b4 exceptb4 d, e Cs new states generated f combinedPresentation(f ).dependencies IC (the last two clauses), get b0 satisfies either G(c)H(c). subsumption clause clause regarding 3-D shapes (first third clauses)get b0 satisfies either 1n55(c) 1kw3(c). Hence, easy verify, b5 |=G hgetInfoDSSPG (c, d), getInfoDSSPH (c, d), getInfo3D1n55 (c, e), getInfo3D1kw3 (c, e),combineInfo(d, e, f )i plan.64fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONNote plan make use getInfoDSSPI (c, d). obtain plan, domainone always apply information services. However, plan trivial takeaccount relevant not. Reasoning IC enables us find better plans.semantics executing non-applicable actions vital workings Example 2.pointed above, definition res(s, a) (Equation (3)), executed evenapplicable. realizes partial matches: web service called soon might matchone possible situations. planning terms, actions conditional effects semantics.10contrasting notion would enforce preconditions, i.e., say res(s, a) undefinedapplicable s. would correspond plug-in matches.Example 2, partial match semantics necessary order able apply actionscover particular cases. example, consider action getInfoDSSPG (c, d), appliedinitial belief example plan. precondition action G(c). However,states initial belief satisfy precondition. initial belief allowsinterpretation satisfying IC 0 (cf. Equation (4)), interpretations satisfy H(c)rather G(c). Due partial match semantics, getInfoDSSPG (c, d) affectstates match initial belief partial.Clarification also order regarding understanding constants. First, like every PDDLlike planning formalism (we aware of), make unique name assumption, i.e., differentconstants refer different objects. Second, understanding web services outputcreate separate individual, i.e., separate information object.latter directly raises question allow actions share output constants.answer allow planner treat two objects same. makessense two objects play role plan. Consider Example 2. actionsgetInfoDSSPG (c, d) getInfoDSSPH (c, d) share output constant, d. meansone name two separate information objects. two objects properties,derived InfoDSSP(d). difference created differentcases, namely states satisfy G(c) H(c) respectively. single nametwo objects useful take name parameter actions needdistinguish different cases. example, combineInfo(d, e, f ) action.hinted, cases correspond different classes concrete execution traces.Importantly, particular execution trace, output constant created once. seethis, consider execution trace s0 , a0 , s1 , a1 , . . . , ak , sk+1 , i.e., alternating sequence statesactions s0 b0 , si+1 res(si , ai ) 0 k. Say ai aj shareoutput constant, d. Say ai applicable si , hence Csi+1 . Then, quiteobviously, Csl + 1 l k + 1. particular, aj applicable sj :intersection output constants Csj non-empty (cf. definition appl(s, a)). So, duedefinition action applicability, never happen constant created twice.words, never reachable state single constant name refersone individual information object. sense, use one name several objects occursplanning time, actual execution trace actual case occurknown. illustration, consider getInfoDSSPG (c, d) getInfoDSSPH (c, d), shared10. obvious generalization allow several conditional effects per action, style ADL language (Pednault, 1989). omit sake simplifying discussion. extension direction straightforward.65fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREoutput d, Example 2. Even concrete state s0 b0 execution starts satisfiesG(c) H(c), one actions fire namely one comes first.remark initially experimented definition actions instantiateinputs, applied state outputs are, virtue execution semantics,instantiated constants outside Cs . framework, one never choose share outputconstants, i.e., use name two different outputs. notion settledstrictly richer: planner always choose instantiate outputs constants outside Cs .question is, make sense share outputs? Answering question domainindependent planner may turn quite non-trivial. get back discusspossible adaptation CFF Section 4.5. experiments reported herein (Section 6), usesimple heuristic. Outputs shared iff operator effects identical (giving indicationrespective outputs may indeed play role plan).conclude sub-section final interesting observation regarding modelingframework. Negative effects essential part WSC formalism: compiledaway. simply replace negative effect G(x1 , . . . , xk ) notG(x1 , . . . , xk ) (introducingnew predicate) state integrity constraints two equivalent. is, introduce two new clauses x1 , . . . , xk : G(x1 , . . . , xk ) notG(x1 , . . . , xk ) x1 , . . . , xk :G(x1 , . . . , xk ) notG(x1 , . . . , xk ). simple compilation technique, formaldetails little intricate, moved Appendix A. action original task,a+ denotes corresponding action compiled task, vice versa. Similarly, actionoriginal task, s+ denotes corresponding state compiled task. get:Proposition 1 (Compilation Negative Effects WSC) Assume WSC task (P, IC , O, C0 ,+0 , G ). Let (P + , +IC , , C0 , 0 , G ) task negative effects compiled away.Assume action sequence ha1 , . . . , i. Let b result executing ha1 , . . . , (P, IC ,++++O, C0 , 0 , G ), let b+ result executing ha+1 , . . . , (P , IC , , C0 , 0 , G ).Then, state s, b iff s+ b+ .proved straightforward application relevant definitions. important aspect result new clauses introduced allowed forward effectsstrictly forward effects special cases identified later. Hence, hardness results transfer directlytasks without negative effects dropping negative effects cannot make algorithms easier.3.3 Computational Propertiesperform brief complexity analysis WSC formalism general formintroduced above. line many related works kind (Eiter & Gottlob, 1992; Bylander,1994; Liberatore, 2000; Eiter et al., 2004), consider propositional case. context,means assume fixed upper bound arity predicates, number input/outputparameters operator, number variables appearing goal, numbervariables clause. refer WSC tasks restricted way WSC tasks fixedarity.consider problems checking plans testing whether given action sequenceplan deciding plan existence. latter, distinguish polynomially boundedplan existence, unbounded plan existence. deem particularly relevant decisionproblems context plan generation. Certainly, plan checks integral part plan gen66fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONeration. Indeed, planning tool based state space search, tool either performschecks explicitly (potentially many) plan candidates generated search, complexityinherent effort underlies computation state transitions. Polynomially boundedplan existence relevant because, commonly used planning benchmark domains, planspolynomial length (it also wide-spread intuition SWS community composedweb services contain exceedingly large numbers web services). Finally, unbounded planexistence general decision problem involved, thus generic interest.problems turn hard. prove this, reuse adapt various resultsliterature. start complexity plan checking, hardness followslong established result (Eiter & Gottlob, 1992) regarding complexity belief update.results, detailed proofs available Appendix A.Theorem 1 (Plan Checking WSC) Assume WSC task fixed arity, sequenceha1 , . . . , actions. p2 -complete decide whether ha1 , . . . , plan.Proof Sketch: Membership shown guess-and-check argument. Guess propositionvalues along ha1 , . . . , i. check whether values comply res, leadinconsistent action, final state satisfy goal. ha1 , . . . , plan iffcase guess proposition values. Checking goal satisfaction polynomial, checkingcompliance res coNP, checking consistency NP.Hardness follows simple adaptation proof Lemma 6.2 Eiter Gottlob(1992). proof uses reduction checking validity QBF formula X.Y.[X, ].lemma considers case propositional belief updated arbitrary (propositional)formula , decision problem ask whether formula impliedupdated belief. proof, complete conjunction literals, i.e., corresponds singleworld state. single propositional fact r true . semantics X.Y.[X, ]encoded complicated construction defining update . nutshell, CNF tellingus every assignment X (which yield world state updated belief), eitherfind assignment [X, ] holds (completing ), falsify r.difference setting lies restricted update formulas action effectsfact integrity constraints supposed hold every belief. adaptproof by, first, taking integrity constraints clauses Eiter Gottlobs CNF formula. modify constraints need true new fact holds i.e., insertevery clause. initial belief false, otherwise corresponds exactly above.action plan makes true. goal Eiter Gottlobs fact r.2remark membership Theorem 1 remains valid allowing actions multipleconditional effects, allowing parallel actions, even allowing combination.hand, virtue proof argument outlined, hardness holds even initial stateliterals 0 complete (describe single world state), plan consists single actionsingle positive effect literal, goal single propositional fact initially true.next consider polynomially bounded plan existence. this, membership follows directlyTheorem 1. prove hardness, construct planning task extends Eiter Gottlobsconstruction actions allow choose valuation third, existentially quantified, set variables, hence reduces validity checking QBF formula X.Y.Z.[X, Y, Z].67fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORETheorem 2 (Polynomially Bounded Plan Existence WSC) Assume WSC task fixed arity, natural number b unary representation. p3 -complete decide whether existsplan length b.Proof: membership, guess sequence b actions. Theorem 1, checkp2 oracle whether sequence plan.hardness, validity QBF formula X.Y.Z.[X, Y, Z], CNF, reducedtesting plan existence. Say X = {x1 , . . . , xn }. planning task, n actions (operatorsempty input/output parameters) oxi oxi former sets xi true lattersets xi false. Further, action ot corresponds action used hardnessproof Theorem 1. actions equipped preconditions effects ensuringplan must first apply, 1 n, either oxi oxi , thereafter must apply ot (of courseenforcing latter also requires new goal fact achieved ot ). Hence, choosingplan candidate task choosing value assignment aX variables X.construction, oxi oxi actions executed, one ends beliefcontains single world state, value assignment aX variables X correspondschosen actions. world state basically corresponds belief hardness proofTheorem 1. difference construction extended cater thirdset variables. straightforward. Then, belief results executing ot satisfiesgoal iff Eiter Gottlobs fact r holds world states. virtue similar argumentsEiter Gottlob, latter case iff Y.Z.[aX /X, Y, Z], i.e., substitutionX.Y.Z.[X, Y, Z] aX , valid. this, claim follows.2final result regards unbounded plan existence WSC. result relatively easyobtain generic reduction described Bylander (1994) prove PSPACE-hardness planexistence STRIPS. Somewhat shockingly, turns plan existence WSC undecidableeven without integrity constraints, complete initial state description. sourceundecidability is, course, ability generate new constants on-the-fly.Theorem 3 (Unbounded Plan Existence WSC) Assume WSC task. decision problemasking whether plan exists undecidable.Proof Sketch: modification proof Bylander (1994) plan existence propositional STRIPS planning PSPACE-hard. original proof proceeds generic reduction,constructing STRIPS task Turing Machine polynomially bounded space. latter restriction necessary model machines tape: tape cells pre-created positions withinbound. Exploiting ability create constants on-the-fly, instead introduce simpleoperators allow extend tape, ends.2able decide plan existence is, course, significant limitation principle. However, limitation probably marginal importance practice, planning toolsassume plan, try find rather trying proveplan. sense, planning tools are, nature, semi-decision procedures anyway.matters decidability setting question whether one find plan68fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONquickly enough, i.e., exhausting time memory.11 also relevant questionweb service composition.4. Forward Effectshigh complexity planning WSC motivates search interesting special cases.define special case, called forward effects, every change action makes state involvesnewly generated constant.start section defining forward effects case making core observationsemantics. discuss modeling power special case. Next, discuss forward effects general perspective belief update. analyze main computationalproperties forward effects, conclude section assessment existingplanning tool could adapted handle forward effects.4.1 WSC|f wd Semanticsforward effects special case WSC defined follows.Definition 1 Assume WSC task (P, IC , O, C0 , 0 , G ). task forward effects iff:1. O, l[X] effo , X Yo 6= .2. clauses IC , = x1 , . . . , xk : l1 [X1 ] ln [Xn ], X1 == Xn .set WSC tasks forward effects denoted WSC|f wd .first condition says variables every effect literal contain least one output variable. implies every ground effect literal action contains least one new constant.second condition says that, within every integrity constraint, literals share arguments.implies effects involving new constants affect literals involving new constants.Note that, since x1 , . . . , xk definition exactly variables occurring literals,Xi Xi = x1 , . . . , xk . Note may k = 0, i.e., literalsclause may ground. intentional. constants mentioned clause musttaken C0 , cf. discussion Section 3.1. Therefore, clauses interactionstatements new constants generated WSC|f wd action.discuss modeling power WSC|f wd (Section 4.2). First, observesemantics WSC|f wd much simpler general WSC. One longer needsnotion minimal change respect previous state. state precisely, assumeWSC task predicates P. Say interpretation P C , C set constants.Say C C . denote |C restriction P C , i.e., interpretation P Ccoincides propositions. Given state action a, define:{(C , ) | C = Cs Ea , |Cs = , |= IC effa } appl(s, a)(7)res|f wd (s, a) :={s}otherwise11. Indeed planning community generally rather unconcerned undecidability, cf. numeric track international planning competitions, Helmerts (2002) results decidability numerical planning problems.69fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORECompare Equation (3), defined member update(s, C , IC effa ),returns interpretations satisfy IC effa differ minimally . Equation (7), simply set identical , constants (on propositions constants)existed beforehand. words, set new states get cross-product oldstate satisfying assignments IC effa .Lemma 1 (Semantics WSC|f wd ) Assume WSC|f wd task, reachable state s, actiona. res(s, a) = res|f wd (s, a).Proof Sketch: WSC|f wd , differs minimally s, follows agrees totallys, set propositions P Cs interpreted s. see this, denote P Cs +Easet propositions arguments Cs Ea , least one argument Ea , denoteIC [Cs + Ea ] instantiation IC constants Cs Ea , clauseleast one variable instantiated Ea . key argument |= IC effa equivalent|= IC [Cs Ea ] effa , turn equivalent |= IC [Cs ] IC [Cs + Ea ] effa .last formula, IC [Cs ] uses propositions P Cs , whereas IC [Cs + Ea ] effauses propositions P Cs +Ea . Since reachable, |= IC [Cs ]. Therefore, satisfyIC effa , need change values assigned s.24.2 Modeling PowerIntuitively, WSC|f wd covers situation web service outputs new constants, setscharacteristic properties relative inputs, relies ontology axioms describeramifications concerning new constants. detailed Section 2, closely correspondsvarious notions message-based WSC explored literature. sense, modelingpower WSC|f wd comparable message-based WSC, one most-widespreadapproaches area.simple concrete way assessing modeling power WSC|f wd consider alloweddisallowed axioms. Examples axioms allowed WSC|f wd are: attribute domainrestrictions, taking form x, : G(x, y) H(x); attribute range restrictions, taking formx, : G(x, y) H(y); relation transitivity, taking form x, y, z : G(x, y) G(y, z)G(x, z). Note that, axioms, easy construct case action effect, eventhough involves new constant, affects old belief. example, constants c e existedbeforehand, action outputs sets G(c, d) G(d, e), axiom x, : G(x, y)G(y, z) G(x, z) infers G(c, e) statement involve new constant d.Typical ontology axioms allowed WSC|f wd are: subsumption relations, takingform x : G(x) H(y); mutual exclusion, taking form x : G(x) H(y); relation reflexivity, taking form x : G(x, x); relation symmetry, taking form x, :G(x, y) G(y, x). also express concept G contained union conceptsH1 , . . . , Hn , generally express complex dependencies concepts, taking form clausal constraints allowed combinations concept memberships.One example complex dependencies important domain proteins illustratedExample 1. Capturing dependencies important order able select correct web services. Similar situations arise many domains involve complex interdependenciesand/or complex regulations. example latter Virtual Travel Agency discussed before. example, German rail system kinds regulations regarding70fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONtrain may booked kind discount conditions. Modelingregulations would enable WSC algorithm select appropriate booking services. Another interesting case hospital domain described de Jonge, van der Linden, Willems (2007).There, problem hospital asset tracking handled means set tracking, loggingfilter services, transform logs extract various kinds information. setting, wouldmake sense model complex dependencies web service composer may determinehospital assets need tracked retrieved. Namely, latter depends type operationquestion, kind examinations operation requires. Accordingly,need model categorization operations, mapping sets required examinations,examinations associated hospital assets. complications arise sincerequired examinations/assets may depend particular circumstances. Clearly, expresscategorization dependencies terms clauses. course captures fractionrelevant hospital, considerably informed composer alwaystracks assets.main weakness WSC|f wd allow us express changes regarding preexisting objects. best illustrated considering case negative effects.12planning community, commonly used model previous properties objectsinvalidated action. illustration, reconsider Example 1. Say additional operatordropCoffeeIn3Dmachine, effect Info3D(y). One would normally expect that,operator applied, fact Info3D(y) deleted must re-established.WSC|f wd . According restrictions special case imposes, variable Info3D(y)must output dropCoffeeIn3Dmachine. is, dropping coffee machine createsnew object, whose characteristic property happens Info3D(y) rather Info3D(y). Clearly,intended semantics operator.model intended semantics, would need instantiate pre-existing constant.Say that, belief b3 Example 1, constant e Info3D(e) previously createdgetInfo3D1n55 (c, e). WSC|f wd allow us instantiate dropCoffeeIn3Dmachinee, effect Info3D(e). However, virtue definition action applicability,action applicable states e yet exist corresponding executionpaths getInfo3D1n55 (c, e) executed. Hence property Info3D(e) getdeleted state, e used dropCoffeeIn3Dmachine still regarded newlycreated object whose characteristic property Info3D(y). difference new actionmakes that, now, plan uses name (e) refer two different information objects(output getInfo3D1n55 (c, e) vs. output dropCoffeeIn3Dmachine) playrole plan, cf. discussion Section 3.2.interesting workaround let operators output time steps, spirit reminiscentsituation calculus (McCarthy & Hayes, 1969; Reiter, 1991). Every operator obtains extraoutput variable t, included every effect literal. new time step stated standrelation previous time steps, e.g., next(tprev, t) tprev input variableinstantiated previous time step. setting, state world changestime. particular state object property different tprev.example, action moves file f RAEDME README could statename(f, RAEDME, tprev) name(f, README, t). problem construction12. Or, WSC, positive effects triggering negative effects via IC , cf. Proposition 1.71fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREtime steps special interpretation, ordinary objects.13 causesleast two difficulties. (1) want refer object property, know time stepfirst place is, know whether actual time step tprev. Notecannot maintain predicate actualTime(x) would require us invalidateproperty tprev. (2) solution frame problem. operators must explicitly stateevery relevant property previous time step, property changed new timestep.14conclude sub-section, let us consider WSC|f wd generalized without losingLemma 1. importantly, instead requiring every effect literal involves new constant,one postulate literals may actually affected integrity constraints.particular, predicate appear clauses, certainly effect literalpredicate harmful even involve output constant. One obtains potentially stronger notion considering ground literals, rather predicates. Note kindgeneralization solves difficulty (1) time-step construction, presuming time stepsconstrained clauses. (The frame problem, however, persists.)Another possibility, deviating somewhat way WSC WSC|f wd currently defined, define integrity constraints terms logic programming style rules, along linesEiter et al. (2003, 2004). requirement WSC|f wd relaxed postulateeffect literals without new constants appear rule heads.remark latter observation suggests certain strategic similarity aforementioned derived predicates (Thiebaux et al., 2005) previously used AI Planning managecomplexity integrity constraints. There, integrity constraints take form stratified logicprogramming style derivation rules, predicates appearing rule heads allowedappear operator effects. overly restricted solution, WSC context. effectsweb services indeed likely affect concepts relations appearing ontologyaxioms. may WSC|f wd , long output constants involved.4.3 Belief UpdateLemma 1 specific possible models approach (Winslett, 1988) underlies semanticsaction applications. interesting consider semantics WSC|f wd generalperspective belief update. Recall update involves formula characterizing currentbelief, formula describing update. seek formula characterizes updated belief.wide variety definitions proposed updated belief defined.However, common ground exists. Katzuno Mendelzon (1991) suggest eight postulates,named (U1) . . . (U8), every sensible belief update operation satisfy. Herzig Rifi(1999) discuss detail degree postulates satisfied wide range alternativebelief update operators. particular call postulate uncontroversial update operatorsinvestigation satisfy them. take results following. examineextent draw conclusions updated belief, , setting forward effectscase, relying Herzig Rifis uncontroversial postulates.13. Note similarity situation calculus ends. Whereas time steps assigned specific roleformulas used situation calculus, ordinary objects handled actions, packagesblocks.14. Despite difficulties, Theorem 6 shows time step construction used simulate Abacusmachine, hence prove undecidability plan existence WSC|f wd .72fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONassume planning task predicates P given. need following notations:formulas, denotes formula results updating beliefupdate , semantics belief update operator .Given disjoint sets constants C E, P C+E denotes set propositions formedpredicates P, arguments contained C E exists least oneargument contained E. (Recall P C denotes set propositions formedpredicates P arguments C.)Given set constants C, IC [C] denotes instantiation IC C. is, IC [C]conjunction clauses result replacing variables clause IC ,= x1 , . . . , xk : l1 [X1 ] ln [Xn ], tuple (c1 , . . . , ck ) constants C.Given disjoint sets constants C E, IC [C + E] conjunction clausesresult replacing variables clause IC , = x1 , . . . , xk : l1 [X1 ]ln [Xn ], tuple (c1 , . . . , ck ) constants C E, least one constant takenE.15ground formula P () denote set propositions occurring .denote current belief update . another convention, given setconstants C, writing C indicate P () P C . Similarly, given disjoint sets constantsC E, writing C+E indicate P () P C+E . state, denoteconjunction literals satisfied s.first consider case where, similar claim Lemma 1, corresponds singleconcrete world state s. want apply action a. wish characterize set statesres(s, a), i.e., wish construct formula . simplicity notation, denote C := CsE := Ea . applicable s, nothing do. Otherwise, that:(I) IC [C] C P ( C ) P C .example, set C := . Since |= IC , get desired equivalence. Further,that:(IIa) IC [C] IC [C + E] effa ;(IIb) P (IC [C + E]) P C+E P (effa ) P C+E .(IIa) holds trivially: defined IC effa , equivalent IC [C E] effaequivalent IC [C] IC [C + E] effa . (IIb), consequence forward effectscase. Every effect literal contains least one output constant, hence effa contains propositionsP C+E . IC [C + E], least one variable clause instantiatedconstant e E. Since, definition, literals clause share variables, e appearsevery literal therefore IC [C + E] contains propositions P C+E .illustration, consider simple VTA example. four predicates, train(x),ticket(x), trainTicket(x), ticketFor (x, y). set integrity constraints IC consists15. clause IC contains variable, IC [C + E] empty. customary, empty conjunctiontaken true, i.e., 1.73fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREsingle axiom x : trainTicket(x) ticket(x). current state s, Cs = {c},sets propositions 0 except train(c). consider application action= bookTicket(c, d), whose precondition train(c), whose set E output constants {d},whose effect effa trainTicket(d) ticketFor (d, c). setting, have: IC [C] =(trainTicket(c) ticket(c)); C = (train(c)ticket(c)trainTicket(c)ticketFor (c, c));IC [C + E] = (trainTicket(d) ticket(d)).derive following that:(III) (IC [C] C ) (IC [C + E] effa ).is, characterize updated belief simply conjunction previous beliefaction effect extended instantiation ontology axioms. correspondsexactly Lemma 1. illustrate, continue VTA example. left hand side (III)refers four propositions based c, sets according s. right hand siderefers propositions based trainTicket(d) ticket(d) well propositionticketFor (d, c) links c d.one prerequisite derivation (III), make assumption which, bestknowledge, discussed anywhere belief update literature:(IV) Let 1 , 1 , 2 , 2 formulas P (1 ) P (1 ) = , P (1 ) P (2 ) = , P (2 )P (1 ) = , P (2 ) P (2 ) = . (1 1 ) (2 2 ) (1 2 ) (1 2 ).assumption postulates formulas talking disjoint sets variables updatedseparately. Since formulas disjoint variables essentially speak different aspectsworld, seems reasonable assumption.Now, start formula . make replacements according (I) (IIa), leadingequivalent formula (IC [C] C ) (IC [C] IC [C + E] effa ). mapformula onto (IV) taking 1 IC [C] C , 1 1, 2 IC [C], 2IC [C + E] effa . Hence, separate update two parts follows:(A) ( )C := (IC [C] C ) IC [C](B) ( )C+E := 1 (IC [C + E] effa )According (IV), obtain desired formula ( )C ( )C+E .Illustrating VTA example, simply separate parts update talkc talk combination constants. (A) partupdate trainTicket(c) ticket(c) conjoined , updated trainTicket(c)ticket(c). (B) part update 1 representing (empty) statement previous statemakes updated (trainTicket(d) ticket(d)) trainTicket(d) ticketFor (d, c).remains examine ( )C ( )C+E . need prove that:(C) ( )C IC [C] C ,(D) ( )C+E IC [C + E] effa .Essentially, means prove that: (C) updating formula something already impliesincur changes; (D) updating 1 formula yields belief equivalent formula.see this, compare (A) (C) (B) (D).74fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONtwo statements may sound quite trivial, fact far trivial provewide variety of, partly rather complex, belief update operations literature. buildworks Katzuno Mendelzon (1991) Herzig Rifi (1999). need twopostulates made Katzuno Mendelzon (1991), namely:(U1) 1 2 : (1 2 ) 2 .(U2) 1 2 : 1 2 (1 2 ) 1 .Herzig Rifi (1999) prove (U1) uncontroversial, meaning satisfied beliefupdate operators investigated (cf. above). also prove (U2) equivalent conjunction two weaker statements, one uncontroversial, namely:(U2a) 1 2 : (1 2 ) (1 2 ).statement uncontroversial. However, proved satisfied non-causalupdate operators investigation, except so-called Winsletts standard semantics (Winslett,1990). latter semantics useful context anyway. restriction makesstates res(s, a) differ propositions mentioned updateformula. case, include propositions appearing IC [C E], boundquite lot. So, use Winsletts standard semantics, res(s, a) would likelyretain hardly information s.Consider formula ( )C specified (A), ( )C = (IC [C] C ) IC [C].prove (C). indeed quite simple. (IC [C] C ) IC [C],instantiate 1 (U2) IC [C] C , 2 (U2) IC [C]. obtain(IC [C] C ) IC IC [C] C , hence ( )C IC [C] C desired.said above, result uncontroversial, holds non-causal update operators(except Winsletts standard semantics) investigated Herzig Rifi (1999). terms VTAexample, (U2) allowed us conclude update trainTicket(c) ticket(c) makechange previous belief, already contains property.Next, consider formula ()C+E specified (B), ()C+E = 1(IC [C +E]effa ).prove (D). postulate (U1), get ( )C+E IC [C + E] effa ,IC [C +E]effa update formula 2 . direction, exploit (U2a). instantiate1 (U2a) 1, get 1 (IC [C + E] effa ) 1 (IC [C + E] effa ),1 (IC [C + E] effa ) ( )C+E , equivalent IC [C + E] effa( )C+E . proves claim. Note used postulates uncontroversialaccording Herzig Rifi (1999). Reconsidering VTA example, IC [C +E]effa =(trainTicket(d) ticket(d)) trainTicket(d) ticketFor (d, c). previous state sayanything propositions, thus represented 1. postulates allow us conclude(for belief update operators investigated Herzig & Rifi, 1999) resulting beliefequivalent (trainTicket(d) ticket(d)) trainTicket(d) ticketFor (d, c).far, restricted case , belief updated, corresponds singleworld state s. Consider general case characterizes belief b, wantcharacterize set states res(b, a). first glance, seems much changes,Katzuno Mendelzon (1991) also make following postulate:(U8) 1 , 2 , : (1 2 ) (1 ) (2 ).75fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREmeans that, consists two alternate parts, updating taking unionupdated parts. words, compute update state-by-state basis.statement (I) still true, C disjunction statesb, rather single . rest argumentation stays exactly same. HerzigRifi (1999) prove (U8) uncontroversial leave that.However, matters simple. source complications use partialmatches/conditional effects semantics. update formula different individual statesb. Hence cannot directly apply (U8). Obviously, states s1 b applicable updated differently states s2 b applicable latter updated all.16somewhat subtle distinction states b constants exist them: differentsets constants, integrity constraints update different. Hence, obtain genericupdate , split equivalence classes 1 , . . . , n states withincannot distinguished based prea based existing constants. Then, (U8)argumentation used show equivalent (III) . last step,defining final disjunction individual , appears sensible.follow immediately Katzuno Mendelzon (1991).illustration, consider variant VTA example two preceding states, onestate train(c) before, new state ticket(c) instead. ,bookTicket(c, d) applicable, hence update different . partabove, yielding result (trainTicket(d) ticket(d)) trainTicket(d) ticketFor (d, c).update trivial, yields result. final outcome disjunctiontwo beliefs.point situation much easier consider plug-in matches (i.e., forced preconditions) instead partial matches. There, applicable states, also easysee every state b constants. Therefore, plug-in matches, (III) follows immediately (U8). VTA example, update would computed sincebookTicket(c, d) would considered applicable preceding belief. satisfiestrain(c) disagrees aspect, e.g. (quite nonsensically) also ticket(c) holds,updated belief equivalent (s ) (trainTicket(d) ticket(d)) trainTicket(d)ticketFor (d, c).4.4 Computational PropertiesParalleling analysis general WSC Section 3.3, perform brief complexityanalysis WSC|f wd special case. before, consider propositional caseassumes fixed upper bound arity predicates, number input/output parametersoperator, number variables appearing goal, number variablesclause. Also before, consider decision problems checking plans, decidingpolynomially bounded plan existence, deciding unbounded plan existence, order.contrast before, cannot reuse results literature much because, course,particular circumstances WSC|f wd investigated before. include proof sketcheshere, refer Appendix detailed proofs.16. One might speculate common update would prea , case. example,possible models approach adopt WSC, updating |= prea prea gives rise resultstates change violate prea instead changing satisfy .76fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONThanks simpler semantics per Lemma 1, plan checking much easier WSC|f wdWSC.Theorem 4 (Plan Checking WSC|f wd ) Assume WSC|f wd task fixed arity, sequence ha1 , . . . , actions. coNP-complete decide whether ha1 , . . . , plan.Proof Sketch: Hardness obvious, considering empty sequence. Membership shownguess-and-check argument. Say C union C0 output constants appearingha1 , . . . , i. guess interpretation propositions P C. Further,1 n, guess set Ct constants. needs time-stamped because, actiongenerated outputs, properties respective propositions remain fixed forever. ThanksLemma 1, check polynomial time whether (a) Ct correspond executionha1 , . . . , i. Also, check polynomial time whether (b) Cn satisfy G . ha1 , . . . ,plan iff guess answer (a) yes answer (b) no.2Membership Theorem 4 remains valid allowing parallel actions multiple conditional effects provided one imposes restrictions ensuring effects/actions applied simultaneously (in one step) never self-contradictory. Otherwise, checking plans also involvesconsistency test plan step, NP-complete problem. Note quite reasonable demand simultaneous actions/effects contradict other. Widely usedrestrictions imposed ensure mutually exclusive effect conditions, and/or non-conflictingsets effect literals.next consider polynomially bounded plan existence. Membership follows directly Theorem 4. prove hardness, reduce validity checking QBF formula X.Y.[X, ].constructed planning task allows choose values X, thereafter apply actions evaluating arbitrary values . goal accomplished iff setting X exists works.Theorem 5 (Polynomially Bounded Plan Existence WSC|f wd ) Assume WSC|f wd taskfixed arity, natural number b unary representation. p2 -complete decide whetherexists plan length b.Proof Sketch: membership, guess sequence b actions. Theorem 4,check NP oracle whether sequence plan.Hardness proved reductionWk validity checking QBF formula X.Y.[X, ]DNF normal form, i.e., = j=1 j . key idea use outputs creationtime steps, hence ensure operators adhere restrictions WSC|f wd . Settingxi allowed time step i. is, xi operators oxi 1 oxi 0 . takeinput set time steps {t0 , . . . , ti1 } required successive, preconditionstart(t0 ) next(t0 , t1 ) next(ti2 , ti1 ). output new time step ti attachsuccessor ti1 , set xi 1 0, respectively, time step i. is,effect literal form xi (ti ) xi (ti ), respectively. rest planning task consists of:operators ot allow extending sequence time steps step B, suitable value B (seebelow); operators oj allow achieving goal, given j true end timestep sequence length B. integrity constraints (IC empty). values yispecified, i.e., variables take value initial belief.77fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREX.Y.[X, ] valid obviously one construct plan task simply settingxi accordingly, using ot stepping time B, applying oj . necessitatescomplicated construction direction proof: namely, plan may cheatsetting xi 1 0. construction ensures costly, planforced maintain two parallel sequences time steps, starting faulty xi . choosesufficiently large value B, together sufficiently small plan length bound b, cheatingpossible.2final result regards unbounded plan existence. Somewhat surprisingly, turnsstill undecidable WSC|f wd . Similar above, key idea let actions outputnew time step, thereby ensuring membership constructed task WSC|f wd .Theorem 6 (Unbounded Plan Existence WSC|f wd ) Assume WSC|f wd task. decisionproblem asking whether plan exists undecidable.Proof Sketch: reduction halting problem Abacus machines, undecidable.Abacus machine consists tuple integer variables v1 , . . . , vk (ranging positiveintegers including 0), tuple instructions I1 , . . . , . state given contentv1 , . . . , vk plus index pc active instruction. machine stops iff reaches statepc = n. vi initially 0, pc initially 0. instructions either increment variablejump another instruction, decrement variable jump different instructionsdepending whether variable already 0.difficult encode Abacus machine WSC|f wd task. two key ideas are: (1)design operator outputs next successor integer; (2) design operators simulatinginstructions, stepping successors predecessors integer values. latter kindoperators, membership WSC|f wd ensured letting operators output new time stepnew variable values associated. goal asks existence time stepactive instruction .2argued end Section 3.3 already, dont deem undecidability unbounded planexistence critical issue practice. planning tools nature semi-decision procedures,anyway. particular, web service composition typically expected occur real-time setting,severe time-outs apply.4.5 Issues Adapting CFFview, crucial observation WSC|f wd test plans coNP,rather p2 general WSC. Standard notions planning uncertaintycomplexity plan testing, research already resulted sizable number approaches(comparatively) scalable tools (Cimatti et al., 2004; Bryce et al., 2006; Hoffmann & Brafman,2006; Palacios & Geffner, 2007). show next section that, certain additionalrestrictions WSC|f wd , tools applied off-the-shelf. Regarding general WSC|f wd ,match complexity plan testing suggests underlying techniques successfullyadapted. following, consider detail CFF tool (Hoffmann & Brafman, 2006).promising options would extend MBP (Cimatti et al., 2004) POND (Bryce et al.,2006), look compilation techniques investigated Palacios Geffner (2007).CFF characterized follows:78fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION(1) Search performed forward space action sequences.(2) sequence a, CNF formula (a) generated encodes semantics a,SAT reasoning (a) checks whether plan.(3) reasoning results namely literals always true executing cachedspeed future tests.(4) Search guided adaptation FFs (Hoffmann & Nebel, 2001) relaxed plan heuristic.(5) Relaxed planning makes use strengthened variant CNF formulas (a) usedreasoning action sequences, clauses projected onto 2literals (i.e., 2 literals removed respective clause).techniques self-explanatory, except possibly last one. Projecting CNFformulas ensures relaxed planning remains over-approximation real planning,projected formulas allow us draw conclusions. time, projectedformulas handled sufficiently runtime-efficiently.17 method 2-projectingclauses is, nutshell, ignore one condition literals conditional effectrelaxed planning graph.fairly obvious basic answers given CFF, i.e., techniques (1) (5), also applyWSC|f wd . Note that, indeed, main enabling factor check plans coNP,rather p2 general WSC. enables us design desired CNF formulas (a)straightforward fashion. plan checking p2 -hard, either need replace CNFformulas QBF formulas, create worst-case exponentially large CNF formulas.are, least, technically quite challenging.adaptation CFF WSC|f wd immediate promise, trivial. involvestechnical challenges regarding on-the-fly creation constants well computationheuristic function. latter also brings significant new opportunities WSC context,pertaining exploitation typical forms ontology axioms. Let us consider issueslittle detail.First, like todays planning tools, CFF pre-instantiates PDDL purely propositionalrepresentation, based core planning algorithms implemented. one allows on-thefly creation constants, pre-instantiation longer possible, hence adaptationWSC|f wd involves re-implementing entire tool. challenge itself,difficult obstacles overcome. sloppy formulation key question is: manyconstants create? One can, course, create new tuple constants (the outputs of)every new action application. However, seems likely approach would blowrepresentation size quickly, would hence infeasible. one instead shareoutput constants reasonable. one recognize reasonable points? issueespecially urgent inside heuristic function. Namely, easy see that, worst case,relaxed planning graph grows exponentially number layers. imagine exampleweb service w1 takes input type generates output type B, whereas w2 takesinput type B generates output type A. Starting one constant typeone type B, get 2 constants type next graph layer. Then, w1 w217. Inside heuristic function, formulas come relaxed planning graphs quite big. handlingwithout approximations seems hopeless. discussed detail Hoffmann Brafman (2006).79fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREapplied two times, get 4 constants type next graph layer, forth.dilemma probably cannot handled without making approximations relaxedplanning graph.One positive note, seems possible exploit typical structures ontologiespractice. particular, practical ontologies make extensive use subsumption relations,structuring domain interest concept hierarchy. Additional ontology axioms often comeform constraints relations (reflexivity, symmetry, transitivity) typing numberrelation arguments. may make sense exploit structures optimizingformulas (a) associated SAT reasoning. Certainly, makes sense exploit structuresinside heuristic function. One include specialized analysis sub-solver techniquesrecognize structures solve separately order obtain precise relaxed plans.One even try take account structures inside relaxed planning, hence(potentially) obtain fast heuristic function.5. Compilation Initial State Uncertaintyshow that, certain additional restrictions, off-the-shelf scalable tools planninguncertainty exploited solve WSC|f wd . main limiting factors are: (1)tools allow generation new constants. (2) tools allow specificationclausal formula initial state, states. approach deal (1) considersset constants fixed priori, namely initially available constants plus additional potentialconstants used instantiate outputs. subtle observation that, within specialcase WSC|f wd , dynamics states become predictable priori, one also deal(2) natural way.follows, first introduce core observation case state space becomespredictable, certain sense. observe predictability naturally given specialcase forward effects, term strictly forward effects. discuss strengths limitations new special case. finally provide compilation strictly forward effectsplanning initial state uncertainty.5.1 Predictable State Spacescore observation based notion compatible actions. Assume WSC|f wd task (P, IC ,O, C0 , 0 , G ). Two actions a, compatible either Ea Ea = , effa = effa . is,either disjunct outputs hence affect disjunct sets literals sinceWSC|f wd effects agree completely. set actions compatible Ea C0 =A, every pair actions compatible.Lemma 2 states that, given used actions compatible, every state ever reachedsatisfies action effects, modulo existing constants.Lemma 2 (Predictable State Spaces WSC|f wd ) Assume WSC|f wd task, compatible setactions A, state reached actions A. |= 0 and, A,Ea Cs |= effa .Proof: proof induction. base case, b0 , claim holds definition sinceCs Ea = A. Say reached action A. applicable80fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONs, induction assumption nothing prove. Otherwise, WSC|f wd ,Lemma 1 res(s, a) = {(C , ) | C = Cs Ea , |Cs = , |= IC effa }.Vinduction assumption applied s, res(s, a) = {(C , ) | C = Cs Ea , |=0 A,E Cs effa IC effa }. Now, Ea Cs Ea Ea 6 Cs ,2Ea Ea 6= hence effa = effa prerequisite. concludes argument.virtue lemma, possible configurations constantsgeneratedVactions characterized formula IC 0 aA effa . Since partsformula known prior planning, set possible configurations predictable.even begin plan, already know constants behave generated.list possible behaviors potential constants initial belief, let actionsaffect constants actually exist. words, compile initial stateuncertainty. detail below. First, need identify setting Lemma 2actually applied.5.2 Strictly Forward EffectsGiven WSC|f wd task, must settle finite set compatible actions plannertry compose plan from. One option simply require every actionunique output constants. appears undesirable since planning tasks often contain many actions,set potential constants would huge. Further, enable chaining several actions,potential constants allowed instantiate input parameters every operator, hencenecessitating creation new action and, that, new potential constants. unclearbreak recursion, sensible way.Herein, focus instead restriction WSC|f wd suffices assign unique outputconstants individual operators, rather individual actions.Definition 2 Assume WSC task (P, IC , O, C0 , 0 , G ). task strictly forward effectsiff:1. O, l[X] effo , |X| > 0 X Yo .2. clauses IC , = x1 , . . . , xk : l1 [X1 ] ln [Xn ], X1 == Xn .set WSC tasks strictly forward effects denoted WSC|sf wd .second condition identical corresponding condition WSC|f wd . first condition strictly stronger. WSC|f wd requires least one effect literal variable takenoutputs, WSC|sf wd requires variables taken outputs. Therefore,obviously, WSC|sf wd WSC|f wd . Note WSC task formulated Example 2 memberWSC|sf wd .key property WSC|sf wd that, without input variables effect, actions basedoperator effect. So, action set compatible, needchoose set unique output constants every operator. Indeed, every setoperators whose effects pairwise identical. also choose several sets output constantsgroup operators.81fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE5.3 Modeling Powerlimitations WSC|f wd , discussed Section 4.2, naturally inherited WSC|sf wd . Moreover, unlike WSC|f wd , cannot state properties effect connect inputsoutputs. serious limitation. illustration, consider small VTA exampleusing. operator bookTicket effect ticketFor (y, x), relating produced tickettrain x given input. Clearly, notion ticket rather weak cannot stateticket actually valid for. Another interesting case one extend Example 2 considering two proteins rather one. is, set C0 = {c, c }, 0 =cellProtein(c) cellProtein(c ). wish encode need combined presentationthose, i.e., G = : combinedPresentation(y, c) combinedPresentation(y, c ). WSC|f wd ,solve including, every information providing operator, input variable xeffect literal. example, set getInfo3D1n55 := ({x}, 1n55(x), {y}, Info3D(y, x)).possible WSC|sf wd .extent, difficulties overcome encoding relevant inputs predicate names. handle composition two proteins c c , would essentially meanmaking copy entire model renaming part c . goal would G = y, :combinedPresentation(y) combinedPresentation (y ), operator preconditions would makesure combinedPresentation(y) generated before, combinedPresentation (y ) generated using new operators. Note rather dirty hack, depends knowingnumber copies needed, prior planning. equivalent solution VTA would introduceseparate ticketFor-x predicate every entity x ticket may bought.least, would result rather oversized unreadable model. yet troublesome casetime-step construction outlined Section 4.2, added new output variableeffect related via effect literal next(prevt, t) previous time step prevt providedinput. WSC|sf wd , longer relate prevt way stating timestep happens one. Trying encode information predicate names,would include one predicate per possible time step. necessitates assuming boundnumber time steps, clear limitation respect natural encoding.Despite above, WSC|sf wd far pathological irrelevant special case. exampleapplies domain proteins shown Example 1. Similarly, hospital domaindiscussed Section 4.2 naturally modeled WSC|sf wd . generally, factwealth WSC formalisms encode connections inputs outputs.example, category contains formalisms rely exclusively specifying typesinput output parameters. information modeled types kind inputservice requires, kind output produces example, input train outputticket. Examples formalisms various notions message-based composition (Zhanet al., 2003; Constantinescu et al., 2004a; Lecue & Leger, 2006; Lecue & Delteil, 2007; Konaet al., 2007; Liu et al., 2007). fact, early versions OWL-S regarded inputs outputsindependent semantic entities, using Description Logic formalization types.Thus, existence compilation WSC|sf wd planning uncertainty quiteinteresting. shows composition model similar early versions OWL-S, generalform partial matches powerful background ontologies, attacked off-the-shelfplanning techniques. opens new connection WSC planning.82fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION5.4 Compilationcompile WSC|sf wd task task conformant planning initial state uncertainty,takes form (P, A, 0 , G ). P finite set propositions used. finite setactions, takes form (pre(a), eff(a)) pair sets literals P. 0CNF formula P, G conjunction literals P. notions given standardbelief state semantics. state truth value assignment P. initial belief set statessatisfying 0 . result executing action state res(s, a) := 6|= pre(a),18otherwise res(s, a) := (sadd(a))\del(a). use standard notation gives termsset propositions makes true, uses add(a) denote positive literals eff(a),del(a) denote negative literals eff(a). Extension res beliefs definitionplan remain unchanged.Assume WSC|sf wd task (P, IC , O, C0 , 0 , G ). compiled task (P , A, 0 , G ) makesuse new unary predicate Ex expresses constants yet brought existence. compilation obtained follows. operator O, outputsYo ={y1 , . . . , yk }, create set new constants Eo = {e1 , . . . , ek }. Then, C := C0 oO Eoset constants fixed priori. Initialize := . operator O,V includepreo ( xXo Ex(x))VV set actions resulting using C instantiate precondition( eEo Ex(e)). Give actions effect, eEo Ex(e). words, instantiate os outputs Eo , enrich os precondition saying inputs existoutputs yet exist, replace os effect statement simply bringing outputsexistence.Replacing effects way, original effects go? includedinitial state formula. is, initialize 0 conjunction effo [Eo /Yo ] operatorsO. Then, instantiate clauses inVIC C andVconjoin 0 . obtain final0 conjoining 0 ( cC0 Ex(c)) cC\C0 Ex(c)) Goal. Here, Goalnew proposition. serves model goal. Namely, introduce set artificialgoal achievement actions. goal form G = x1 , . .V. , xk .[x1 , . . . , xk ]. new actionsobtained instantiating operator ({x1 , . . . , xk }, ki=1 Ex(xi ), , Goal) C.is, goal achievement actions instantiate existentially quantified variables goalpossible constants. actions added set A. overall compiled task takesform (P , A, 0 , Goal), P simply set mentioned propositions.summary, compile WSC|sf wd task (P, IC , O, C0 , 0 , G ) conformant planningtask (P , A, 0 , G ) follows:operator O, create uniqueSset new constants Eo = {e1 , . . . , ek }Yo = {y1 , . . . , yk }. denote C := C0 oO Eo .P contains instantiations, C, P plus two new predicates, Ex Goal. Exarity 1 expresses constants yet brought existence. Goal arity 0forms new goal, i.e., G = Goal.actions instantiations O, XVinstantiatedVC, Yo inEx(x))(stantiated Eo . preconditionsenriched(eEo Ex(e)),xXoVeffects replaced eEo Ex(e).18. before, give actions conditional effects semantics, rather usual distinction forcedpreconditions, non-forced effect conditions.83fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREFurther, contains goal achievement actions, achieving Goal preconditions instantiating G C.original action effects, i.e., conjunction effo [Eo /Yo ] operatorsO,V. Further, contains ,movedinstantiatedC,(0IC00cC0 Ex(c))VcC\C0 Ex(c)) Goal.terminology Section 5.1, means choose set actions actionsobtained operator instantiating inputs constants C,outputs Eo . suggested Lemma 2, initial state formula 0 compiledtask describes possible configurations constants C, effect applyingaction bring respective output constants existence. Note that, although effectscompiled actions positive, planning still hard (coNP-complete, precise) dueuncertainty. (If allow WSC operators also delete constants, negative effectsdeleting constants compiled task.)According strategy, create one set output constants per operator,take account sets operators identical effects. simplifypresentation. results carry immediately complicated strategies createone set output constants per operator, well strategies share sets outputconstants operators identical effects. noted, however, operatorswhose effects identical not, general, share outputs. particular, twoeffects conflict, e.g., InfoDSSP(d) InfoDSSP(d), initial state formula 0unsatisfiable. compiled planning task trivially solved empty plan, and,course, encode solutions original problem.Example 3 Re-consider planning task defined Example 2. specify compiled task. setC = {c, d, e, f } c initially available constant, d, e, f potential constantsoperator outputs. compiled planning task (P , A, 0 , G ) following:P = {protein, cellProtein, G, H, I, 1n55, 1kw3, combinedPresentation, InfoDSSP,Info3D, Ex, Goal}, predicates except Goal unary (have one argument).consists instantiations of:getInfoDSSPG [d/y]: ({x}, G(x) Ex(x) Ex(d), Ex(d))getInfoDSSPH [d/y]: ({x}, H(x) Ex(x) Ex(d), Ex(d))getInfoDSSPI [d/y]: ({x}, I(x) Ex(x) Ex(d), Ex(d))getInfo3D1n55 [e/y]: ({x}, 1n55(x) Ex(x) Ex(e), Ex(e))getInfo3D1kw3 [e/y]: ({x}, 1kw3(x) Ex(x) Ex(e), Ex(e))combineInfo[f /y]: ({x1 , x2 }, InfoDSSP(x1 ) Info3D(x2 ) Ex(x1 ) Ex(x2 )Ex(f ), Ex(f ))GoalOp: ({x}, combinedPresentation(x) Ex(x), Goal)0 conjunction of:instantiations IC [consisting five axioms given Example 2]84fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONcellProtein(c) [0 ]InfoDSSP(d) Info3D(e) combinedPresentation(f ) [original action effects]Ex(c) Ex(d) Ex(e) Ex(f ) [constants existence]Goal [goal yet achieved]G = Goalconsider plan original task (see Example 2): hgetInfoDSSPG (c, d),getInfoDSSPH (c, d), getInfo3D1n55 (c, e), getInfo3D1kw3 (c, e), combineInfo(d, e, f )i.illustrate, verify plan yields plan compiled task. task,initial belief b0 consists states c existing constant, d, e, f satisfyrespective effects, |= IC cellProtein(c). apply action sequence:1. Apply getInfoDSSPG (c, d) b0 . get belief b1 b0 except that,b0 |= G(c), also exists.2. Apply getInfoDSSPH (c, d) b1 . get belief b2 b1 except that,b1 |= H(c), exists.3. Apply getInfo3D1n55 (c, e) b2 , yielding b3 .4. Apply getInfo3D1kw3 (c, e) b3 . brings us b4 Ex(e) b2|= 1n55(c) |= 1kw3(c).5. Apply combineInfo(d, e, f ) b4 . brings us b5 like b4 except b4e exist also Ex(f ).6. Apply GoalOp(f ) b5 , yielding b6 .reasoning IC used Example 2 show b5 satisfies original goal,used show GoalOp(f ) applicable b5 hence resulting belief b6satisfies goal. obtain plan compiled task simply attaching goal achievementaction original plan.prove soundness completeness compilation, need rule inconsistentoperators, i.e., operators whose effects conflict background theory (meaningIC Xo , Yo : effo unsatisfiable). example, case x : A(x) B(x)contained IC , effo = A(y) B(y). presence operator, initial beliefcompiled task empty, making task meaningless. Note inconsistent operatorsnever part plan, hence filtered pre-process. Note also that, WSC|sf wd ,operator inconsistent iff actions based inconsistent.Non-goal achievement actions correspond actions original task, obviousway. connection, transform plans compiled task directly plansoriginal task, vice versa.Theorem 7 (Soundness Compilation) Consider WSC|sf wd task (P, IC , O, C0 , 0 , G )without inconsistent operators plan ha1 , . . . , compiled task (P , A, 0 , G ).sub-sequence non-goal achievement actions ha1 , . . . , plan task(P, IC , O, C0 , 0 , G ).85fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREProof Sketch: arbitrary sequence non-goal achievement actions, denote b beliefexecution original task, b belief execution compiled task.state sSin original task, denote [s] class compiled-task states overVthe constantsC0 oO Eo {c| s(Ex(c)) = 1} = Cs , s|Cs = , |= IC 0 oO effo [Eo ].One prove b = sb [s]. claim follows directly that.2Theorem 8 (Completeness Compilation) Consider WSC|sf wd task (P, IC , O, C0 , 0 ,G ) without inconsistent operators plan ha1 , . . . , every operator appearsone instantiation Eo outputs. ha1 , . . . , extended goal achievementactions form plan compiled task (P , A, 0 , G ) obtained using outputs Eo .Proof Sketch: Follows immediately b = sb [s] shown proof Theorem 7. Sayone executes ha1 , . . . , compiled task, ending belief b. there, plancompiled task obtained simply attaching one goal achievement action every tupleconstants satisfying G world state b.2reader may noticed number instantiations goal achievement operatorexponential arity goal. worst case, instantiations must includedplan compiled task. particular, may happen plan constructed perproof Theorem 8. However, practical purposes appears reasonable assume fixed upperbound number goal variables.indicated, proofs Theorems 7 8 remain valid allowing one Eoper operator, and/or operators identical effects share output constants. Note operators identical effects several web services provide alternative ways achieving something.Example 3 illustrates situation (cf. earlier discussion Section 3.2). experimentsdescribed next section, groups operators identical effects assignedoutput constants.6. Empirical Resultsshow compilation approach merits, report number empirical experiments using CFF underlying planner. start discussion general experimentalsetup discuss results two different test scenarios.6.1 Experiments Setupimplemented compilation WSC|sf wd planning uncertainty describedabove, connected CFF tool. noted that, although compiled planningtasks delete effects, solved CFFs relaxed-plan-based heuristic function.function makes relaxation ignoring one conditions effect (seeearlier discussion CFF Section 4.5). Ignoring one condition significantly affectscompiled tasks effects typically involve many conditions, particularly conditionsstating inputs exist outputs yet exist.One problematic point evaluating planning-based WSC choice test cases. fieldstill rather immature, due widely disparate nature existing WSC tools,86fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONcommon set benchmarks.19 fact, web service composition new topicposing many challenges existing techniques, different works differ widely termsunderlying purpose, specific aspect WSC address. detailed discussionexisting WSC tools given Section 7. method choose evaluation designtwo test scenarios reflect intuitively relevant kinds problem structures potentialapplications planning-based WSC, scalable number interesting parameters.test reaction approach parameters.test scenarios artificial benchmarks, cannot lead broad conclusions significance practice, allow us draw conclusions planning behavior differentlystructured test problems. solution method scales quite well tested cases, efficiently finding solutions involve many web service calls, successfully employservices really necessary. Viewing results isolation, one concluderepresentation techniques heuristic functions planning uncertainty may usefulattack large complex planning-like WSC instances.comparison alternative WSC tools is, again, problematic, due broad range problems tools solve, different kinds solutions find, different kinds inputsyntax/language read. obtain least notion empirical comparison tools,following consider expressivity (How general input language tool?)scalability (How quickly tool compose?). existing WSC tools constitutesseparate point trade-off two. question whether compilationapproach, restricting WSC|sf wd using CFF solve compiled tasks, sensible pointtrade-off.terms expressivity, approach located general planning methods (likeEiter et al., 2003, 2004; Giunchiglia et al., 2004), inspired actions change literature,restricted methods applied WSC far. question whether gainscalability comparison expressive methods.confirm experiments answer is, expected, yes. run DLVK tool(Eiter et al., 2003, 2004), handles powerful planning language based logic programming.language particular features static causal rules similar integrity constraintsfully general WSC.20 sense, perspective DLVK native WSC toolhandles ontology axioms directly rather via restricting expressivity compilingaway. particular, encoded WSC test problems directly DLVKs input language, withoutcompilation use CFF.DLVK relies answer set programming, instead relaxed plan heuristics, find plans. Further, style many reasoning-based planners, DLVK requires input length boundplan, hence used find optimal plans running several times different bounds.cases, ran DLVK once, bound corresponding optimal plan length.Even so, DLVK much slower CFF, solving small fraction test instances.wish over-interpret results. conclude WSC|sf wd constitutes interestingpoint trade-off expressivity scalability WSC.19. VTA example could considered one benchmark, essentially every individual approach definesparticular version example.20. similarity lies static causal rules fully general integrity constraints can, side effect applyingaction, yield ramifications affecting properties inherited previous state.87fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORErunning first tests compilation approach, noticed encodingper Section 5.4 unnecessarily generous set initial states. Observe compiledtasks always easier solve propositions true initial state. is, simply, literals operator preconditions, effects, goal positive. Hence, propositionp appear positively initial state clause, one set p 0 initially, therebyreduce number initial states, without introducing new plans.21 Setting proposition0 may cause unit propagations, setting propositions 1 0. iterate stepsfixpoint occurs. resulting initial state description stricter before, yields better performance CFF DLVK. use optimized encoding experiments reportedbelow.also experimented another optimization. optimization makes assumptionconstants requested goal generated step-wise fashion, intermediateconstant generated certainty generating next constant. RecallVthat encodingper Section 5.4, existence inputs operators, i.e., condition xXo exists(x),part operator precondition thus interpreted conditional effects semantics. However, CFF DLVK offer distinction effect conditions forced preconditionsmust hold entire beliefaction applicable. exploit distinctionVpostulate condition xXo exists(x) forced. reduces state space, may cutsolutions. reduction quite beneficial CFF DLVK. Since optimizationaffects set plans, switch part test cases, point possible speedup. tests optimization switched discussed text, indicatedkeyword forced name test case.use two versions CFF. One CFFs default configuration makes use FFs enforced Hill-climbing search algorithm well helpful actions pruning technique (Hoffmann& Nebel, 2001). configuration, CFF helpful actions pruning turned off, searchproceeds standard greedy best-first fashion, open queue ordered increasing heuristicvalues. henceforth denote former configuration CFF-def latter configurationCFF-std.results obtained 2.8GHz Pentium IV PC running Linux. tests runtime-out 600 seconds CPU, limiting memory usage 1 GB.6.2 Subsumption Hierarchiesfirst investigate well approach deal scaling subsumption hierarchies,building chains successively created entities (outputs). purpose, design test scenariocalled SH, demands composition web services realizing chain generation steps,every generation step deal subsumption hierarchy.scenario depicted Figure 2. n top-level concepts L1 , . . . , Ln , depictedTL Figure 2. goal input L1 , goal output Ln . Beneath Li ,tree-shaped hierarchy sub-concepts. precisely, tree perfectly balancedbranching factor b, depth d. inner nodes tree called intermediate-level(or simply intermediate) concepts, depicted IL Figure 2. leaf nodes treecalled basic-level (or simply basic) concepts, depicted BL Figure 2. everynon-leaf concept C tree, children C1 , . . . , Cb , axioms x : Ci (x) C(x)21. course, reducing set initial states invalidate old plans, either.88fiTLW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONILILBLBLBLBLBLBLSWSSWSSWSSWSSWSSWSTLILBLILBLBLBLBLBLFigure 2: Schematic illustration SH scenario.expressing subsumption, well axiom x : C(x) C1 (x) Cb (x) expressingparent covered children.available web services defined follows. top level concept Li ,leaf BLi,j corresponding tree structure, web service available takesBLi,j input outputs Li+1 . corresponding WSC operator takes form oi,j =({x}, BLi,j (x), {y}, Li+1 (y)). Then, applying, 1 < n order, services oi,j ,possible make sure constant concept Li+1 created possible cases. Hence,sequencing steps plan, length (n 1) bd . Note that, already statedSection 5.4, experiments groups operators identical effects assignedoutput constants. SH scenario, means 1 < n, oi,j shareoutput constant. Hence total number output constants generated, i.e., numberpotential constants initial state, equal number top-level concepts, n.Although SH scenario abstract nature, representative variety relevantsituations. Specifically, scenario model situations sets different services mustused address request none handle alone. role single servicehandle particular possible case. example, set different servicesset services oi,j assembled Li . Given constant c member Li , i.e.,Li (c) holds, particular possible case handled service oi,j case c happensmember leaf BLi,j one cases must hold due coverage clauses tree.89fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORESimilar situations arise, e.g., geographically located (regional) services compositionrequest location-specific addresses locations higher (inter-regional) level. similarpattern also found e-government scenarios clear-cut classification activitiesleads establishing several parallel services serve different departmental areas.Orthogonal horizontal composition, scenario model vertical composition,one function pursued concatenating existing functions. casecomplex procedures diverse areas e-government e-commerce.scenario instantiated study different aspects scalability approach.empirical tests measure scalability horizontal vertical direction. Further,consider two extreme cases possible shapes individual concept trees chain,giving us instances identical numbers leaves. set test scenario SH-broad,= 1 b scales 2, 4, 8, 16, 32. set test scenario SH-deep, b = 2scales 1, 2, 3, 4, 5. scenarios, n scales 2 20.Further, designed SH-trap variant second chain n concepts linked,completely irrelevant goal service. variant suitable testing extentcomposition techniques affected irrelevant information. Finally, recall encodingmethodcomes two versions explained above, default method treats input existenceVxXo exists(x) conditional effects semantics, whereas non-default method, forced,compromises completeness efficiency treating input existence forced precondition.all, following choices: 3 different planners (CFF-def, CFF-std, DLVK);2 different encoding methods; SH without trap; SH-broad SH-deep. crossproduct choices yields 24 experiments, within 19 possible valuesn 5 possible values b d, i.e., 95 test instances. CFF, measured 3 performanceparameters: total runtime, number search states inserted open queue, numberactions plan. DLVK, measured total runtime number actions plan.course, large amount data interesting. follows, summarizeimportant observations. Figure 3 shows data selected purpose. Part (a) figureshows CFF-std SH-broad; (b) shows CFF-std SH-deep; (c) shows CFF-def SH-forcedbroad; (d) shows DLVK SH-broad SH-deep; (e) shows DLVK SH-forced-broadSH-forced-deep; (f) shows DLVK CFF-std SH-trap. vertical axes show log-scaledruntime (sec). horizontal axes show n (a), (b) (c). (d), (e) (f), n fixed n = 2horizontal axes show number leaves concept hierarchy.Consider first Figure 3 (a) (b). plots point efficiently CFF handlekind WSC problem, even forced optimization. Comparing two plots pointsdifference handling broad deep concept hierarchies. plots, CFF-std runtimeshown n, length chain built. (a), show 5 curves 5 differentvalues b (the number leaves hierarchy depth 1), (b) show 5 curves 5different values (the depth hierarchy branching factor 2). cases, scalingbehavior fairly good. small concept hierarchies (b = 2 = 1), chains almost arbitrarylength built easily. hierarchies grow, runtime becomes exponentially worse. Note,however, one curve next size hierarchies doubles, growthexponential. concept hierarchies 16 leaves, i.e., 16 alternative cases handledstep, still easily build chains 6 steps, solution involves 96 web services.interesting aspect comparing two plots, (a) (b), underlying search spacesactually identical: open queues same. difference performance stems90fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION100100101011b=2b=4b=8b=16b=320.10.012468101214d=1d=2d=3d=4d=50.10.01161820246(a) CFF-std SH-broad8101214161820(b) CFF-std SH-deep10000SH-broadSH-deep10001001001010110.10.10.0124681012141618200.0123(c) CFF-def SH-forced-broad456(d) DLVK SH-broad SH-deep1000010000SH-forced-broadSH-forced-deepDLVK SH-trap-broadDLVK SH-forced-trap-broadCFF-std SH-trap-broad100010001001001010110.10.10.010.012345678910111213141516171819202(e) DLVK SH-forced-broad SH-forced-deep46810121416182022242630(f) DLVK CFF-std SH-trapFigure 3: Selected results SH scenario. See detailed explanation text.912832fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREoverhead CFFs reasoning techniques, consume runtime case deepconcept hierarchies. Hence slightly worse behavior (b).run CFF-def test suites Figure 3 (a) (b), obtain much worse behavior.example, b = 8 get n = 3. reason seems FFs helpful actionspruning enforced hill-climbing greedy domain. simple way overcomeuse standard heuristic search algorithm instead, done CFF-std shown Figure 3 (a)(b). hand, forced optimization switched on, helpful actions pruningenforced hill-climbing work much better, obtain significant performance boost usingCFF-def. latter pointed Figure 3 (c), showing data CFF-def SH-forced-broad.Like Figure 3 (a) CFF-std SH-broad, plot shows 5 curves, one 5 valuesb (legend omitted plot would overlap curves). see that, case,easily build arbitrarily long chains even b = 16, giving us solution involving 320 webservices n = 20. Even b = 32, still get n = 9.Figure 3 (d) (e) show one gets trying solve examples, encodingdirectly DLVK instead using compilation solving CFF. expected,performance much worse. Since hardly test instance solved n > 2, fixed nminimum value 2 plots, unlike (a), (b) (c). (d) (e) shows databroad deep variants, showing number leaves horizontal axis. order obtainfine-grained view, broad variant increase number steps 1 rathermultiplicative factor 2 before. see that, without forced optimization Figure 3 (d)performance poor, largest case solve n = 2, b = 6 solution involves6 web services. switch forced Figure 3 (e) performance dramatically improvedstill different level obtain compilation+CFF.Figure 3 (f), finally, exemplifies results get trap scenario. show databroad version, default encoding CFF-std, default forcedencoding DLVK. DLVK quite affected irrelevant chain concepts, solvingsingle instance n = 2, b = 2 default encoding, getting n = 2, b = 16forced encoding, instead n = 2, b = 20 without trap. behavior expected sinceDLVK make use heuristic techniques would able detect irrelevancesecond chain concepts. question whether CFFs techniques better that. Figure 3(f) shows CFF-std largely unaffected n = 2 one see comparing curvepoints vertical axis Figure 3 (a). However, n > 2 performance CFF-stddrastically degrades: instances solved n = 3, b = 2 n = 4, b = 2. reasonseems additional actions yield huge blow-up open queue used globalheuristic search algorithm CFF-std. Indeed, picture different using CFF-defforced encoding instead: search spaces identical explored trap,behavior get identical shown Figure 3 (c).plans found SH scenario optimal, i.e., plans returned contain webservices needed. single exception DLVK trap, solutions includeuseless web services trap chain.2222. Note DLVKs plans parallel. parallel length optimal (because provided correct planlength bound, cf. Section 6.1. However, parallel step may contain unnecessary actions, top necessaryones. Thats happens trap.92fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTION6.3 Complex Concept Dependenciestwo variants SH scenario feature tightly structured relationships involvedconcepts, allow investigation scalability issues varying size structure.consider advanced scenario, way top-level concepts covered lowerlevel concepts subject complex concept dependencies, similar axioms constraining protein classes characteristics Example 1. Therefore investigate performanceimpacted complex concept structures subsumption hierarchies.TLTLILILILILBLBLBLBLBLBLBLBLBLBLBLBLSWSSWSSWSSWSSWSSWSSWSSWSSWSSWSSWSSWSTLTLILBLBLILILBLBLBLBLBLBLILBLBLBLBLFigure 4: Schematic illustration CD scenario vs. SH scenario.new scenario called CD, concept dependencies. Figure 4 illustrates scenario,contrasts SH scenario. Similarly SH, top-level concepts,one associated set basic sub-concepts. b basic conceptsevery top-level concept. n top-level concepts L1 , . . . , Ln , goal achieveLn starting L1 . before, done combining web services coverpossibilities. Namely, every top-level concept Li every basic concept BLi,j associatedit, operator oi,j = (({x}, BLi,j (x), {y}, Li+1 (y)).23difference lies connection basic concepts top-level concepts.SH, rigidly given terms tree structure subsumption coverage axiomsintermediate concepts. Every basic concept i.e., every operator oi,j correspondingconcept included plan order cover possible cases. CD, use insteadcomplex set axioms connect basic concepts top-level. top-level conceptintermediate concepts ILi,1 , . . . , ILi,m , axioms stating ILi,j23. Note that, i, operators assigned output constant compilationtechnique.93fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREsub-concept Li , well axiom x : Li (x) ILi,1 (x) ILi,m (x) statingLi covered ILi,1 , . . . , ILi,m . connection intermediate conceptsbasic concepts, complex dependencies used. intermediate subconcept constrainedcovered non-empty set combinations basic subconcepts. Precisely, createrandom DNF, positive literals, using basic concepts predicates. takeDNF imply ILi,j . Note that, implication, DNF negated hence becomesCNF, directly encode formalism. every ILi,j .setting, interesting control many combinations required covertop-level concept Li . directly corresponds total number random combinations (random DNF disjuncts) generated, intermediate concepts ILi,j taken together.control via call coverage factor, c, ranging (0, 1]. 2b 1 possiblecombinations basic concepts, pick random subset size c (2b 1). combination associated DNF randomly chosen intermediate concept. Note CNFformulas generated way may enormous. minimize size encoding, useformula minimization software Espresso (Brayton, Hachtel, McMullen, & Sangiovanni-Vincentelli,1984; McGeer, Sanghavi, Brayton, & Sangiovanni-Vincentelli, 1993).hypothetically c set 0 task unsolvable. experiments reported below,whenever write c = 0% means exactly one combination selected, associatedevery intermediate concept.escaping rigid schema relationships presented SH, CD scenario suitable test whether performance approach tied specific structure SHproblem. Moreover, way CD designed allows us determine degree plannersreact intelligently different concept structures. particular, scenario allows analysis of:1. ability approach, particular selected underlying planner CFF, identify plans contain relevant actions. Especially coverage factor c low,basic subconcepts may never appear partition intermediate concepts, thus,plan need include respective operators. Still, due conditional effects/partial matches semantics, plans include operators valid plans. Evaluatingplan length performance varying c therefore interesting.2. ability approach deal complex axiomatizations. measuredterms impact coverage factor runtime performance. randomizationchoice combinations basic factors, different settings c, may induce significantdifferences CNF axiomatizations, result, subject underlying reasoningengine different situations.summary, CD scenario representative situations complex dependencies musttaken account order select correct services. Examples domains discussedSections 4.2 5.3. particular, CD scenario corresponds closely (a scalable version of)protein domain example. different values DSSP code correspond different basicconcepts, respective getInfoDSSP services operators taking intermediateconcept, InfoDSSP(y). similar amino-acids, 3-D shapes, shapes complexes.top level concept combinedPresentation(y) achieved constants every intermediateconcept created. So, difference CD lies that, rather singletop-level concept generated intermediates, CD sequence top-level conceptsneed generated turn.94fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONSH scenario, total data experiments extensive, even since4 scenario parameters rather 2 before, since individual instances containrandom element. Figure 5, report selected results pointing main observations. Part(a)/(b) figure show CFF-std runtime/plan length n = 4, b = 5; (c)/(d) show CFFstd runtime/search nodes c n = 5, = 3, b = 7; (e) shows DLVK CFF-std runtimeb CD n = 2, c = 100%; (f) show latter data CFF-def CD-forced.Figure 5 (a) (b) consider scalability solution lengths test varying sizescenario, representing different coverage factors different lines. report data CFFstd. Results similar CD-forced CFF-def, i.e., contrary SH, CD settingoptions bring significant performance gain. see Figure 5 (a) CFF scalespretty well, though well SH, easily able solve tasks 7 top level concepts4 intermediate concepts 5 basic concepts. Tasks minimum coverage factor,c = 0%, solved particularly effortlessly. higher c values, one observe somewhateasy-hard-easy pattern, where, example, curve c = 100% lies significantlycurves c = 40% c = 60%. examine easy-hard-easy pattern detail below.Figure 5 (b), obvious expected observation plan length grows linearlyn, i.e., number top level concepts. likewise obvious, much important,observation plan length grows monotonically coverage factor c. reported above,lower coverage factor opens opportunity employ less basic services, namelyrelevant ones. Figure 5 (b) clearly shows CFF-std effective determiningservices relevant not.Let us get back intriguing observation Figure 5 (a), easy-hard-easy patterngrowing c. Figure 5 (c) (d) examine phenomenon detail. plots scale chorizontal axis, fixed setting n, b. Runtime shown (c), (d) showsnumber search states inserted open queue. value c, plots giveaverage standard deviation results 30 randomized instances. clearly see easyhard-easy pattern (c) runtime, high variance particularly c = 80%. (d),see pattern number search states, variance much lesspronounced. shows easy-hard-easy pattern due differences actual searchperformed CFF, due effort spent search nodes. traced behavior CFFdetail, found reason easy-hard-easy pattern lies runtime CFF spendsSAT reasoning state transitions, i.e., reasoning uses determine factsdefinitely true/false belief. high non-100 values c, CNF encodingsconcept dependency structures take rather complex form. cases CFF takes lotruntime, almost runtime spent within single call SAT solver. is, seemsCFFs SAT solver exhibits kind heavy-tailed behavior formulas, phenomenonwell known SAT CP community, see example work Gomes, Selman, Crato,Kautz (2000). noted that, typical planning benchmarks, CNFs muchsimpler structure, motivates use fairly naive SAT solver CFF, using neither clauselearning restarts, order save overhead formulas simple anyway. seems likelyaddition advanced SAT techniques solver could ameliorate observed problem.Finally, Figure 5 (e) (f) compare performances compilation+CFF DLVK (withcompilation). plots fix n = 2, i.e., data shown 2 top level concepts.instances DLVK solves n > 2 ones forced optimization used n = 3,= 2, b = 2. Further, plots c fixed c = 100%. reason95fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORE100035c=0%c=20%c=40%c=60%c=80%c=100%100c=0%c=20%c=40%c=60%c=80%c=100%30251020151100.150.01023456723(a) CFF-std runtime n4567(b) CFF-std plan length n2501002001015011000.1500.0100204060801000(c) CFF-std runtime c1000040601000DLVK m=2DLVK m=4DLVK m=6CFF-std m=2CFF-std m=4CFF-std m=610002080100(d) CFF-std plan length cDLVK m=2DLVK m=4DLVK m=6CFF-def m=2CFF-def m=4CFF-def m=61001001010110.10.10.010.01246810122(e) DLVK CFF-std runtime b4681012(f) DLVK CFF-def runtime bFigure 5: Selected results CD scenario. See detailed explanation text.find significant difference performance DLVK different values c. DLVKunable exploit lower c lower runtime, neither show easy-hard-easy pattern.speculate DLVKs answer set programming solver tends perform exhaustive search anywayaccordingly affected different structures heuristic techniques employedCFF. However, like CFF, DLVK able exploit lower coverage factors c shorter plans.96fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONFigure 5 (e) shows default setting without forced optimization. see performance DLVK explodes quickly CFF experience much trouble. CFF failsupper ends curves, Figure 5 (e) (f), problem files, i.e., CNFsdescribing complex concept dependencies, become large parse (> 4 MB). notwithstanding, CFFs runtime behavior clearly exponential. Note, however, actual encodings,i.e., problem instances solved, also grow exponentially c.observe DLVK exhibits quite variance, particularly across differentsettings m: curves cross Figure 5 (e). even pronounced Figure 5 (f),also observe, SH, forced optimization brings huge advantageDLVK. = 2 = 6 Figure 5 (f), DLVK fails first unsolved problem instancedue running memory shortly parsing problem.Concluding section, observe empirical behavior CFF SH CD scenarios promising. results over-interpreted, though. test scenarioscapture problem structure typical variety potential applications WSC technology,approach yet put test actual practice. same, however, said essentiallycurrent planning-based WSC technology, since field whole still rather immature.7. Related Workrelation work belief update literature covered detail already Sections 2.2 4.3. relation planning, formalism basically follows commonlyused frameworks. notions operators, actions, conditional effects exactly usedPDDL framework (McDermott et al., 1998; Bacchus, 2000; Fox & Long, 2003), exceptextension outputs. Regarding latter, recognized time planningcommunity, example Golden (2002, 2003) Edelkamp (2003), on-the-fly creationconstants relevant feature certain kinds planning problems. However, attempts actuallyaddress feature planning tools scarce. fact attempt aware workGolden (2002, 2003) Golden, Pand, Nemani, Votava (2003). Part reasonsituation probably almost current state art tools employ pre-processing procedurescompile PDDL task fully grounded description. core algorithms implemented based propositional representation. Lifting algorithms representationinvolves variables on-the-fly instantiations requires major (implementation) effort. workherein, circumvent effort using potential constants feeding resulting problemCFF, like planners employs said pre-processing. Extending CFF WSC|f wdinvolve dealing non-propositional representations sub-problem.notion initial state uncertainty conformant plans closely follows related literatureplanning uncertainty (Smith & Weld, 1998; Cimatti et al., 2004; Hoffmann & Brafman,2006). formalization terms beliefs adapted work Bonet Geffner (2000).related works planning allow domain axiomatization, i.e., formaxioms constraining possible world states (Eiter et al., 2003; Giunchiglia et al., 2004).best knowledge, work planning exists, apart work presented herein,considers combination domain axioms outputs.words order regarding notions partial plug-in matches. terminology originates work service discovery SWS community (see example Paolucciet al., 2002; Li & Horrocks, 2003; Kumar et al., 2007). service discovery, one concerned97fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREmatching service advertisements service requests. discovery result set serviceswhose advertisement matches request. descriptions services requests similarfunctional-level service descriptions, i.e., planning operators use here. However,terminology works slightly different ours, also describe additional kindsmatches. notions given Li Horrocks (2003) closest relation ours. Servicedescriptions defined terms constructed Description Logic concepts. Say conceptdescribing advertisement, R concept describing request. Li Horrockssay R have: exact match R; plug-in match R; subsume matchR; intersection match R 6 . compare setting, considersituation effect action a, R precondition action r. Exact matchesspecial case plug-in matches distinguish herein. Intersection matchescorrespond call partial matches. Concerning plug-in subsume matches, matterssubtle. intuitive meaning plug-in match advertisement fully sufficesfulfill request. planning terms, means effect implies precondition r.However, service discovery traditionally taken mean every requested entityprovided, i.e., R. latter notion precondition r implies effectmeaningful planning. Hence use one two notions, correspondence LiHorrockss subsume matches.contrast work Li Horrocks (2003), work, Paolucci et al. (2002)Kumar et al. (2007) define matches individual input/output parameters service descriptions,rather service descriptions global level (precondition/effect us, constructedconcept Li & Horrocks, 2003). level individual parameters, Paolucci et al. (2002)suggest notions Li Horrocks (2003) except less formal notation,define intersection matches. true Kumar et al. (2007). latterauthors also define notions contains part-of matches, relating building blocksconstructed concepts. Obviously, notions make sense framework,arent constructed concepts. Finally, Kumar et al. define ways aggregating matchesindividual parameters matches entire service descriptions. Again, applicablecase since work global level first place.brief survey existing works WSC follows. variety works compile composition less standard deterministic planning formalisms (Ponnekanti & Fox,2002; Srivastava, 2002; Sheshagiri et al., 2003). works (Agarwal, Dasgupta, Karnik,Kumar, Kundu, Mittal, & Srivastava, 2005b; Agarwal et al., 2005a) additionally focus end-to-endintegration SWS composition larger context. Akkiraju, Srivastava, Anca-Andreea, Goodwin, Syeda-Mahmood (2006) investigate techniques disambiguate concept names. McIlraithFadel (2002) achieve composition particular forms non-atomic services, modelinglatter atomic actions take meaning kind macro-actions. Narayanan McIlraith(2002) obtain composition ability side-effect verifying SWS properties using Petri Nets.Kuter, Sirin, Nau, Parsia, Hendler (2005), Au, Kuter, Nau (2005), Au Nau (2006)focus information gathering composition time, rather plan execution time. McDermott(2002) treats actual interaction (communication) web service planning problem.Mediratta Srivastava (2006) design approach WSC based conditional planning, i.e.,form planning uncertainty. suggests close relation work, focusMediratta Srivastavas work actually quite different ours. Mediratta Srivastavaconsider output variables, neither consider domain axiomatizations.98fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONoverlap formalism lies allow incomplete initial state descriptions, i.e.,initial states assign value subset propositions. handle observationactions allow observing value unspecified proposition. ameliorate needcomplete modeling, consider definition user acceptable plans, subsetplan branches, specified user, guaranteed lead goal. latter mayinteresting option look extending framework handle partial observability.Two approaches explore adapt formalisms so-called hand-tailored planningSWS composition. approaches based Golog (McIlraith & Son, 2002) HTN planning (Sirin et al., 2004), respectively. frameworks enable human user provide controlinformation. However, non-deterministic action choice allowed. control informationgiven, planning fully automatic. Hence, sense, frameworks strictlypowerful planning without control information. Further, approaches capablehandling advanced plan constructs loops branches. Golog, possible planspossible composition solutions described kind logic high-level instructionsgiven programmer, planner bind instructions concrete actions partexecution. HTN, programmer supplies planning algorithm set so-calleddecomposition methods, specifying certain task accomplished terms combination sub-tasks. Recursively, decomposition methods sub-tasks. Thusoverall task decomposed step-wise fashion, atomic actions reached. NeitherMcIlraith Son (2002) Sirin et al. (2004) concerned handling ontology axioms,paper. Hence, combining insights directions synergetic potential,interesting topic future work.Another approach capable handling advanced plan constructs (loops, branches) describedPistore et al. (2005b), Pistore, Traverso, Bertoli, Marconi (2005c), Pistore et al. (2005a),Bertoli, Pistore, Traverso (2006). work, process level composition implemented,opposed profile/capability level composition addressed paper. process level,semantic descriptions detail precisely interact SWS, rather characterizingterms preconditions effects. Pistore et al. (2005b, 2005c, 2005a) Bertoliet al. (2006) exploit BDD (Binary Decision Diagram) based search techniques obtain complexsolutions fully automatically. However, ontology axioms handled input/output typesmatched based type names.approaches ontology axioms used requirementsmatches relaxed. One described Sirin, Hendler, Parsia (2003), Sirin, Parsia,Hendler (2004), Sirin Parsia (2004), Sirin et al. (2006). first two papersseries (Sirin et al., 2003, 2004), SWS composition support tool human programmersproposed: stage composition process, tool provides user listmatching services. matches found examining subconcept relation. outputconsidered match input B B. corresponds plug-in matches. later work(Sirin & Parsia, 2004; Sirin et al., 2006), HTN approach (Sirin et al., 2004) mentionedadapted work standard planning semantics, description logics semanticsOWL-S. difficulties inherent updating belief observed, connection beliefupdate studied literature made, remains unclear solution adopted.far aware, methods relaxed matches followtermed message-based approach WSC. approaches already discussed depthSection 2.3. Next, give details ones closely related work.99fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREapproach Liu et al. (2007) discussed sufficient detail already Section 2.3,reconsider here.Meyer Weske (2006) handle ontology axioms WSC tool, providesemantics action applications. Reasoning used determine whether particular outputused establish particular input, approach classified message-based,terms. kind matches handled said plug-in. best knowledge,tool existing WSC tool employs relaxed plan based heuristic function, likeCFF. However, various design decisions, authors sacrifice scalability. explicitlyenumerate world states every belief, hence suffer exponentially large beliefs.search forward parallel actions consequently suffer huge branching factor.take heuristic relaxed planning graph length (rather relaxed plan length) thussuffer fact that, time, hmax much less informative heuristic h+ (Bonet& Geffner, 2001; Hoffmann, 2005).approach rather closely related ours, handle partial matches, describedConstantinescu Faltings (2003) Constantinescu et al. (2004a, 2004b). workontology assumed take form tree concepts, edges indicate subconceptrelation. tree compiled intervals, interval represents conceptcontents arranged correspond tree. intervals used efficient implementationindexing service lookup (discovery), well matching composition. lattersearches forward space switches. Starting initial input, current input typeA, service input Ai matches Ai 6= . services collected setcollected Ai covers (that is, union intervals various Ai containsinterval A). collected services form switch, next step search,outputs becomes new input must treated (i.e., switch node). Compositioninterleaved discovery, i.e., every search state discovery called find servicesmatch state. search proceeds depth-first fashion. Major differences workfollowing. First, formalization different, using intervals vs. using standard notionsplanning based logics. Second, approach interleaves discovery composition,separate steps framework (web service discovery needed determine operatorsWSC task). Third, approach considers concept trees vs. clausal integrity constraints. Last,approach uses depth-first search, whereas one main points making oneexploit heuristic techniques implemented standard planning tools scalable WSC.Finally, interesting approach related planning described Ambite Kapoor (2007).capture dependencies different input variables web service, input described terms relation variables. done outputs.relations formulated terms logical formulas relative ontology. underlying formalism first-order logic, modeling language quite expressive.24 Reasoning performedorder establish links (messages, terms) inputs outputs. algorithmicframework happens inspired partial-order planning (Penberthy & Weld, 1992),starting goal relation maintaining set open links. solution DAG webservices links correspond different kinds data exchanges (selection, projection, join,union). Automatic insertion mediator services, e.g., converting set standard formats,also supported.24. cost undecidable reasoning, according authors major issue practice.100fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONextent, preconditions/effects clausal integrity constraints used modelrelations sense Ambite Kapoor (2007). Say r k-ary relation definition, describing input web service. set corresponding operators preconditionr(x1 , . . . , xk ), transform set universally quantified clauses. longlatter done, long ontology axioms likewise transformed, obtainmodel equivalent Ambite Kapoor. sense, main modeling advantageapproach Ambite Kapoor WSC|f wd existential quantification. open questionwhether quantification accommodated framework. Insertion mediator servicessupported WSC|f wd , limited sense recognizing, via particular preconditions, particular kind mediator required. Modeling actual data flow boundawkward. summary, work Ambite Kapoor advanced data description transformation point view. hand, Ambite Kapoor neither considerbelief update, place work context fully-fledged planning formalism,less concerned exploiting heuristic technologies recent planners. Combiningvirtues approaches within either framework interesting directionresearch.8. Discussionsuggested natural planning formalism significant notion web service compositionprofile / capability level, incorporating on-the-fly creation constants model outputs, incomplete initial states model incomplete user input, conditional effects semantics model partialmatches, and, importantly, clausal integrity constraints model ontology axioms.identified interesting special case, forward effects, semantics action applicationssimpler general case. demonstrated relates belief updateliterature, shown results reduced computational complexity. Forward effectsrelate closely message-based WSC, results serve put form WSC context, extend towards general notion partial matches. Further, identifiedcompilation planning (initial state) uncertainty, opening interesting new connectionplanning WSC areas.empirical results encouraging, over-interpreted. test scenarios serve capture structural properties likely appear applications WSCtechnology, approach yet put test actual practice. same, however,said essentially current planning-based WSC technology, since field still rather immature. sense, thorough evaluation approach, planning-based WSCwhole, challenge future.Apart evaluation, several directions research improving extendingtechnology introduced herein. line research find particularly interesting adaptmodern planning tools WSC, starting special cases, complications incurredintegrity constraints manageable. already outlined ideas adaptingCFF, pointed new challenges arise. appears particularly promising tailor genericheuristic functions, originating planning, exploit typical forms ontology axioms occurpractice. Considering wealth heuristic functions available now, topic alone providesmaterial whole family subsequent work.101fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREAcknowledgmentsthank anonymous reviewers, well managing editor Derek Long, comments,significant help improving paper.Jorg Hoffmann performed part work employed University Innsbruck, Austria. work partly funded European Unions 6th Framework Programme SUPER project (IST FP6-026850, http://www.ip-super.org).Piergiorgio Bertolis Marco Pistores work partly supported project SoftwareMethodology Technology Peer-to-Peer Systems (STAMPS).Malte Helmerts work partly supported German Research Council (DFG) partTransregional Collaborative Research Center Automatic Verification Analysis ComplexSystems (SFB/TR 14 AVACS). See www.avacs.org information.Appendix A. Proofsfirst formally prove Proposition 1, stating negative effects compiled away WSC.so, first need introduce compilation formally. Assume WSC task (P,+IC , O, C0 , 0 , G ). construct second WSC task (P + , +IC , , C0 , 0 , G ), initially+P + , IC O+ P, IC O, respectively. proceed follows. Let GP predicate arity k, exists O, = (Xo , preo , Yo , effo ) effocontains negative literal G(x1 , . . . , xk ). introduce new predicate notG P + ,introduce two new clauses x1 , . . . , xk : G(x1 , . . . , xk ) notG(x1 , . . . , xk ) x1 , . . . , xk :G(x1 , . . . , xk ) notG(x1 , . . . , xk ). every operator whose effect contains negationG, replace, effo , G(a1 , . . . , ak ) notG(a1 , . . . , ak ).25 continuenegative effect literals remain O+ .action (P, IC , O, C0 , 0 , G ) denote a+ corresponding action++(P + , +IC , , C0 , 0 , G ). also use notation vice versa, i.e., action+++(P , IC , , C0 , 0 , G ) denotes corresponding action (P, IC , O, C0 , 0 , G ).= (Cs , ) state using predicates P, denote s+ state using predicates P + ,following properties: Cs+ = Cs ; p P Cs Is+ (p) = (p); notpp P Cs Is+ (notp) = 1 iff (p) = 0. Since is, obviously, exactly ones+ , also use correspondence vice versa.+Proposition 1 Assume WSC task (P, IC , O, C0 , 0 , G ). Let (P + , +IC , , C0 , 0 , G )task negative effects compiled away. Assume action sequence ha1 , . . . , i.Let b result executing ha1 , . . . , (P, IC , O, C0 , 0 , G ), b+ result++++executing ha+1 , . . . , (P , IC , , C0 , 0 , G ). Then, state s, b iffs+ b+ .Proof: induction length action sequence question. sequence empty,consider initial beliefs two tasks, claim follows directlydefinition. inductive step, say claim holds b b+ , action. needshow that, state s, res(b, a) iff s+ res(b+ , a+ ).direction right left, say s+ res(b+ , a+ ). definition s++++res(s+0 , ) state s0 b . induction hypothesis, s0 b. therefore suffices show25. arguments ai may either variables constants.102fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONres(s0 , a). need show (1) |= IC effa (2) differs s0 set-inclusionminimal set values. (1) obvious definitions. Assume contrary (2)exists s1 s1 |= IC effa s1 identical except exists least one propo+sition p s1 (p) = s0 (p) s(p) 6= s0 (p). definition, get s+1 |= IC effa+ .++++Further, get s1 (p) = s0 (p) s+ (p) 6= s0 (p), altogether s1 <s+ s+ .0contradiction s+ res(s+ , a+ ), hence proves res(s0 , a) desired.direction left right proceeds fashion. Say res(b, a). definition+res(s0 , a) state s0 b. induction hypothesis, s+0 b . suffices+++++show s+ res(s+0 , ). need show (1) |= IC effa (2) differs s0set-inclusion minimal set values. (1) obvious definitions. Assume contrary++++(2) exists s+1 s1 |= IC effa+ s1 identical except+++exists least one proposition p s+1 (p) = s0 (p) (p) 6= s0 (p). definition, getCs1 |= IC effa . Further, p P 0 get s1 (p) = s0 (p) s(p) 6= s0 (p).p = notq 6 P Cs0 get property q. Altogether, get s1 <s0 s.+contradiction res(s, a), hence proves s+ res(s+20 , ) desired.Theorem 1 Assume WSC task fixed arity, sequence ha1 , . . . , actions.p2 -complete decide whether ha1 , . . . , plan.Proof: Membership proved guess-and-check argument. First, observe that, arbitrary s, ,A, decide within coNP whether res(s, A). Guess state Cs = Cs Ea .Check whether |= IC effa . Check whether 6s . res(s, a) iff guesssucceeds. Further, action a, deciding whether inconsistent is, obviously, equivalentsatisfiability test, contained NP. instruments hand, designguess-and-check procedure decide whether ha1 , . . . , plan. guess propositionvalues along ha1 , . . . , i. check whether values comply res, leadinconsistent action, final state satisfy goal. detail, checking proceedsfollows. First, check whether initial proposition values satisfy IC 0 . not, stop withoutsuccess. Otherwise, iteratively consider action ai , pre-state post-state . CheckNP oracle whether inconsistent. yes, stop success. not, test NP oraclewhether res(s, a). not, stop without success. Otherwise, < n, go ai+1 .= n, test whether |= G . Stop success 6|= G , stop without success |= G .ha1 , . . . , plan iff guess proposition values successful.Hardness follows following adaptation proof Lemma 6.2 Eiter Gottlob(1992). Validity QBF formula X.Y.[X, ], CNF, reduced plan testingsingle action a. use 0-ary predicates X = {x1 , . . . , xm }, = {y1 , . . . , yn },new 0-ary predicates {z1 , . . . , zm , r, t}. set operators contains single operatorempty in/out parameters, empty precondition, effect t. initial constants empty; 0conjunction xi , yi , zi , r, t; G r. theory is:(^i=1(t xi zi )) (^(t xi zi )) (i=1^(t r C)) (C103n^i=1(t yi r))fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREviewed set clauses C. readably, theory equivalent to:[(^xi zi ) (r ) ((n_yi ) r)]i=1i=1refer initial belief b. plan test contains single action based (equal to,fact) o. refer resulting belief b . Obviously, b contains single state everythingexcept true. Also, consistent: interpretation sets r yi 0 satisfies IC effa .theory conjuncts xi zi make sure w b makes exactly one xi , zi true.particular, different assignments X incomparable respect set inclusion. Hence,every assignment aX truth values X, exists state b compliesaX : aX satisfiable together IC effa , assignment aX distantleast one variable (e.g., aX (xi ) = 1 aX (xi ) = 0 aX closer aXregarding interpretation zi ).prove that, plan, X.Y.[X, ] valid. Let aX truth value assignment X. above, state b complies aX . Since plan,|= r. Therefore, due theory conjunct r , |= . Obviously, valuesassigned satisfy aX .direction, say X.Y.[X, ] valid. Assume that, contrary theWclaim,plan. b 6|= r. then, due theory conjunct ( ni=1 yi ) r,sets yi false. Now, X.Y.[X, ] valid, exists truth valueassignment aY complies setting xi zi s. Obtain modifyingcomply aY , setting r 1. |= IC effa . then, closer, hence 6 b contradiction. concludes argument.2Theorem 2. Assume WSC task fixed arity, natural number b unary representation.p3 -complete decide whether exists plan length b.Proof: membership, guess sequences actions containing b actions (notesize sequence polynomial size input representation). Theorem 1,check p2 oracle whether sequence plan.Hardness follows extension proof Lemma 6.2 Eiter Gottlob (1992). Validity QBF formula X.Y.Z.[X, Y, Z], CNF, reduced testing plan existence.use 0-ary predicates X = {x1 , . . . , xn }, = {y1 , . . . , ym }, Z = {z1 , . . . , zk }, new0-ary predicates {q1 , . . . , qm , r, t, f1 , . . . fn , h, g}. set operators composed of:ot := (, f1 fn h, , g h)1 n: oxi := (, h, , xi fi )1 n: oxi := (, h, , xi fi )initial constants empty. initial literal conjunction 0 composed yi , zi , qi ,r, t, fi , h, g. is, yi , zi , qi well r h true, fi wellg false. value specified (only) xi . goal G r g. theory is:(^i=1(t yi qi )) (^(t yi qi )) (i=1^(t r C)) (C104n^i=1(t zi r))fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONviewed set clauses C. readably, theory equivalent to:[(^yi qi ) (r ) ((i=1n_zi ) r)]i=1First, note obvious things construction:ot must included plan.ot applied, action applied anymore.ot applied, either oxi oxi must applied, every 1 n.theory switched off, i.e., made irrelevant false, point otapplied.is, plan task must first apply oxi oxi , every 1 n, thereby choosingvalue every xi . Then, ot must applied plan must stop. applying ot , changesmade states except values xi set fi made true oneother. Hence, belief b ot applied contains single state correspondsextension 0 value assignment X, values fi flipped.denote value assignment X aX . denote b := res(b, ot ). Noteot consistent: interpretation sets r zi 0, besides setting immediate effectsg h, satisfies IC effot . Obviously, applications oxi oxi consistentwell.theory conjuncts yi qi make sure w b makes exactly one yi , qi true.particular, different assignments incomparable respect set inclusion. Hence,every assignment aY truth values , exists state b compliesaY : aY satisfiable together IC effot , assignment aY distantleast one variable (e.g., aY (yi ) = 1 aY (yi ) = 0 aY closer aY regardinginterpretation qi ).prove that, exists plan ~a yielding assignment aX , X.Y.Z.[X, Y, Z]valid. Let aY arbitrary truth value assignment . state bcomplies aX aY . aX aY satisfiable together IC effot . above,assignment aY distant least one variable. And, course, one deviatesaX one distant respective variable. Since ~a plan, |= r.Therefore, due theory conjunct r , |= . Obviously, values assigned Zsatisfy aX aY . proves claim aY chosen arbitrarily.direction, say X.Y.Z.[X, Y, Z] valid. Let aX assignment XY.Z.[aX /X, Y, Z] valid. Let ~a corresponding plan, i.e., ~a first applies, 1 n,either oxi oxi according aX . Thereafter, ~a applies ot . AssumeWn ~a plan.b 6|= r. then, due theory conjunct ( i=1 zi ) r, setszi false. Now, Y.Z.[aX /X, Y, Z] valid, exists truth value assignmentaZ Z complies setting xi , yi , qi s. Obtain modifying complyaZ , setting r 1. |= IC effot . then, closer ,hence 6 b contradiction. concludes argument.2105fiH OFFMANN , B ERTOLI , H ELMERT & P ISTORETheorem 3. Assume WSC task. decision problem asking whether exists planundecidable.Proof: result holds even empty background theory, complete specificationinitial state, predicates arity 2, operators arity 2, goal variables(arity 0), positive literals preconditions goal. result follows minormodification Tom Bylanders proof (Bylander, 1994) plan existence propositional STRIPSplanning PSPACE-complete.26 original proof proceeds generic reduction, constructingSTRIPS task Turing Machine (TM) polynomially bounded space. latter restrictionnecessary model machines tape: tape cells pre-created positions within bound.makes difference PSPACE-membership undecidability ability createconstants. introduce simple operators allow us extend tape, ends.detail, say TM (a finite number of) states q tape alphabet symbols (where bblank); transition function, q0 initial state, F set accepting states;input word. planning encoding contains following predicates. State(q) indicatescurrent TM state q. In(a, c) indicates current content tape cell c a. N eighbors(c, c )true iff c (immediate) right neighbor c. At(c) indicates current positionTM head c. Rightmost(c) (Lef tmost(c)) true iff c currently right (left) neighbor.set initial constants contains states q, alphabet symbols a, tape cells c corresponding. initial literals, propositions constants assigned truth valuesobvious. every transition (q, a, q , , R) include operator:({x, x }, State(q) In(x, a) N eighbors(x, x ) At(x),, State(q ) State(q) In(x, ) In(x, a) At(x ) At(x)).Obviously, encodes exactly transition. likewise transitions (q, a, q , , L) .model final states, introduce 0-ary predicate G, include q F operator:(, State(q), , G)finally include operators:({x}, Rightmost(x), {x }, N eighbors(x, x ) In(b, x ) Rightmost(x ) Rightmost(x))({x }, Lef tmost(x ), {x}, N eighbors(x, x ) In(b, x) Lef tmost(x) Lef tmost(x ))definitions, easy verify exists plan iff TM reach acceptingstate .2Lemma 1. Assume WSC|f wd task, reachable state s, action a. res(s, a) =res|f wd (s, a).26. Propositional STRIPS like framework, empty background theory, complete specificationinitial state, goal variables, positive literals preconditions goal, outputparameters operators.106fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONProof: applicable s, claim holds trivially. Consider case.Equation 3, res(s, a) defined{(C , ) | C = Cs Ea , min(s, C , IC effa )} appl(s, a)res(s, a) :={s}otherwisemin(s, C , IC effa ) set C -interpretations satisfy IC effaminimal respect partial order defined I1 I2 :iff propositions p Cs ,I2 (p) = (p) I1 (p) = (p).obvious res|f wd (s, a) res(s, a) satisfies IC effa identicalpropositions Cs , particular minimal according .direction, let res(s, a). Assume (p) 6= (p) proposition pCs . Define equal except (p) := (p). Obviously, 6s I2 .suffices show |= IC effa : then, get 6 min(s, C , IC effa ) contradiction,hence agrees propositions p Cs , hence res|f wd (s, a).before, denote P Cs +Ea set propositions arguments Cs Ea ,least one argument E, denote IC [Cs + Ea ] instantiation ICconstants Cs Ea , clause least one variable instantiated Ea . see|= IC effa , consider first equivalent |= IC [Cs Ea ] effa ,turn equivalent |= IC [Cs ] IC [Cs + Ea ] effa . last formula, taskWSC|f wd , IC [Cs ] speaks propositions P Cs , whereas IC [Cs +Ea ]effa speakspropositions P Cs +Ea . treat two parts separately. |= IC [Cs ]|= IC [Cs ] prerequisite since reachable. |= IC [Cs + Ea ] effadefinition. concludes argument.2Theorem 4. Assume WSC|f wd task fixed arity, sequence ha1 , . . . , actions.coNP-complete decide whether ha1 , . . . , plan.Proof: Hardness obvious, considering empty sequence. Membership shownfollowing guess-and-check argument. Say C union C0 output constants appearinghA1 , . . . , i. guess interpretation propositions P C. Further,1 n, guess set Ct constants. check polynomial time whetherCt correspond execution hA1 , . . . , i. 1 n , sayapplicable |= prea , Ca Ct , Ea Ct = . First, assert |= IC . Second,, assert that, applicable, |= effa . Third, assert Ct+1 =Ct {Ea | , applicable}. Using Lemma 1, easy see Ct correspondexecution iff three assertions hold. Note needs time-stampedaction generated outputs properties respective propositions remain fixedforever. claim follows because, fixed arity, also test polynomial time whetherCn satisfy G . guess Ct successful corresponds executionsatisfy G . Obviously, hA1 , . . . , plan iff guess Ct .2Theorem 5. Assume WSC|f wd task fixed arity, natural number b unary representation. p2 -complete decide whether exists plan length b.Proof: membership, guess sequence b actions. Theorem 1, checkp2 oracle whether sequence plan.107fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREprove hardness, assume QBF formula X.Y.[X, ] DNF normal form.(This formula class complete p2 .) Say X = x1 , . . . , xn , = y1 , . . . , ym , = 1k . design WSC|f wd task plan iff X.Y.[X, ] true. key constructionuse outputs creation time steps, allow setting xi time step i.yi take arbitrary values. xi set, one operator per k allows achieve goalgiven k true. main property need ensure construction xi setonce, i.e., either 1 0. plan task iff one set X that,, least one true case iff X.Y.[X, ] true.predicates task P = {x1 (.), . . . , xn (.), y1 (), . . . , ym (), time(.), start(.),next(..), goal(.)}. indicate predicate arity number points parentheses.example, predicate next(..) arity 2. theory IC empty. initial constantsC0 = {t0 }. initial literals 0 = time(t0 ). goal y.goal(y). operatorsfollows:1 n, have: oxi 1 = ({t0 , . . . , ti1 }, start(t0 ) next(t0 , t1 )next(ti2 , ti1 ), {ti }, time(ti )next(ti1 , ti )xi (ti )). operator allows generatingtime step i, setting xi 1 step.1 n, have: oxi 0 = ({t0 , . . . , ti1 }, start(t0 ) next(t0 , t1 )next(ti2 , ti1 ), {ti }, time(ti ) next(ti1 , ti ) xi (ti )). operator allows generating time step i, setting xi 0 step.define value B below. n j < n + B, have: otj = ({t0 , . . . , tj1 },start(t0 ) next(t0 , t1 ) next(tj2 , tj1 ), {tj }, time(tj ) next(tj1 , tj )).operators allow increasing time step n n + B.1 k, say = xlxj1 xlxjxn ylyj1 ylyjyn xlj {xj , xj }ylj {yj , yj }. have: oi = ({t0 , . . . , tn+B }, start(t0 ) next(t0 , t1 )next(tn+B1 , tn+B ) xlxj1 (txj1 ) xlxjxn (txjxn ) ylyj1 () ylyjyn (), {c},goal(c)). operator allows achieve goal time step n + B, providedrespective true. Note xj precondition literals refer time step tj , i.e.,value set xj earlier time step, yj precondition literals argumentsrefer initial values yj , arbitrary.Assume choose value B (polynomial input size). X.Y.[X, ] true,then, obviously, find plan size n + B + k. apply oxi 1 oxi 0 operator xi ,depending whether xi must set 1 0. apply B operators otj . apply operatorsoi . respective input parameter instantiations obvious.opposite direction proving truth X.Y.[X, ] based plan problematic.plan might cheat setting xi 1 0. reason constructioncomplicated able avoid precisely case, based specifying strict enough plan lengthbound b. key property that, order cheat xi , plan generate two sequencestime steps ti , . . . , tn+B . Therefore, lower bound length cheating plan n + 2B.already seen, upper bound length non-cheating plan n + B + k.determine plan length bound b, simply choose B cheating planuse many steps: n+2B > n+B +k case iff B > k. set B := k +1, obtainb := n + 2k + 1. bound b, plan proceed setting xi value (n actions),108fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONincreasing time step n + B = n + k + 1 (k + 1 actions), applying sufficient subsetoi (at k actions). plan cheats, needs apply least n+2B = n+2k +2 actionsable apply oi actions exploiting different value settings xi . concludesargument.2Theorem 6. Assume WSC|f wd task. decision problem asking whether exists planundecidable.Proof: reduce halting problem Abacus machines, undecidable. Abacus machine consists tuple integer variables v1 , . . . , vk (ranging positive integersincluding 0), tuple instructions I1 , . . . , . state given content v1 , . . . , vk plusindex pc active instruction. machine stops iff reaches state pc = n. viinitially 0, pc initially 0. two kinds instructions. Ii : INC j; GOTO Ii increments value vj jumps pc = . Ii : DEC j; BRANCH Ii+ /Ii0 asks whether vj = 0.so, jumps pc = i0 . Otherwise, decrements value vj jumps pc = i+ .map arbitrary abacus program WSC|f wd instance follows:Predicates: number(v), zero(v), succ(v , v), value1 (v, t), . . . , valuek (v, t), instruction1 (t),. . . , instructionn (t)Background theory: none (i.e., trivial theory)Operators:operator h{v}, {number(v)}, {v }, {number(v ), succ(v , v)}iinstructions form Ii : INC j; GOTO Ii , operatorh{v1 , . . . , vk , t},{instructioni (t), value1 (v1 , t), . . . , valuek (vk , t), succ(v , vj )},{t },{instructioni (t ), value1 (v1 , ), . . . , valuej1 (vj1 , ), valuej (v , ),valuej+1 (vj+1 , ), . . . , valuek (vk , )}i.instructions form Ii : DEC j; BRANCH Ii+ /Ii0 , operatorsh{v1 , . . . , vk , t},{instructioni (t), value1 (v1 , t), . . . , valuek (vk , t), succ(vj , v )},{t },{instructioni+ (t ), value1 (v1 , ), . . . , valuej1 (vj1 , ), valuej (v , ),valuej+1 (vj+1 , ), . . . , valuek (vk , )}i.h{v1 , . . . , vk , t},{instructioni (t), value1 (v1 , t), . . . , valuek (vk , t), zero(vj )},{t },{instructioni0 (t ), value1 (v1 , ), . . . , valuej1 (vj1 , ), valuej (vj , ),valuej+1 (vj+1 , ), . . . , valuek (vk , )}i.109fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREInitial constants: v0 , t0Initial literals: number(v0 )zero(v0 )value1 (v0 , t0 ) valuek (v0 , t0 )instruction1 (t0 )Goal condition: t.instructionn (t)describe intuitive meaning constants predicates. two kinds constants: numbers, represent natural numbers (including 0), time points, representcomputation steps Abacus machine. Variables refer time points denotedabove. variables represent numbers.Three predicates refer numbers exclusively: number(v) true iff v encodes natural number(and time point); zero(v) true iff v encodes number 0; succ(v , v) true iff vencodes number one larger number encoded v. reductionenforce every number uniquely represented (e.g., may several representationsnumber 3), unique representation necessary. guaranteed number 0uniquely represented, though.remaining predicates encode configurations Abacus machine: valuei (v, t) true iff,time point t, i-th Abacus variable holds number represented v, instructionj (t)true iff current instruction time point Ij .Obviously, accepting run Abacus machine extract plan task,vice versa. proves claim.2prove Theorems 7 8, first establish core lemma theorems followrelatively easily. need notations. denote beliefs (states) (P, IC , O, C0 , 0 , G )b (s), denote beliefs (states) (P , A, 0 , G ) b (s). Assume sequence ha1 , . . . ,ai non-goal achievement actions. denote b := res(b0 , ha1 , . . . , ai i) b := res(b0 ,ha1 , . . . , ai i). Note overload res function also denote state transitionscompiled task formalism. Further, state s, C(s) := {c | s(Ex(c)) = 1} denoteconstants exist s. denote C relation states true iffC(s) = C(s ) s|C(s) = |C(s) . C equivalence relation, equivalent states agreeconstants exist howVthey interpreted. Note every state reachablecompiledVtask satisfies |= IC 0 oO effo [Eo ]. Note IC 0 oO effo [Eo ] actuallysatisfiable prerequisite, unless IC 0 unsatisfiable, outputs instantiatedunique constants operators consistent. state s, define [s] :=^[{s | defined C0effo [Eo ]}Eo , C(s) = Cs , s|Cs = , |= IC 0oOoOis, [s] equivalence class states reachable compiled task agreeconstants exist interpreted.Lemma 3 Assume WSC|sf wd task without inconsistent operators. Let ha1 , . . . , ai consistnon-goal achievementactions, let b := res(b0 , ha1 , . . . , ai i) b := res(b0 , ha1 , . . . , ai i).b = sb [s].Proof: proof induction i. base case, = 0, i.e., b = b0 b = b0 .b0 ={s | Cs = C0 , |= IC 0 }110fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONhand, b0 ={s | C(s) = C0 , |= IC 0^effo [Eo ]}oOObviously, latter comprised one equivalence class possibility assign propositions C0 way compliant IC 0 . exactly claim.theSinductive case, say add another actionha1 , . . . , ai i. induction assumption,b = sb [s]. need prove res(b, a) = res(b,a) [s ]. Obviously, suffices provethat, every b, res([s], a) = res(s,a) [s ]. First, say applicable s.neither applicable [s], res([s], a) = [s] = res(s,a) [s ]. Second, sayapplicable s. Lemma 1 res(s, a) ={(Cs Ea , ) | |Cs = , |= IC effa }hand, res([s], a) ={s | ex. [s], C(s ) = C(s) Ea , |C(s) = s, |= IC 0^effo [Eo ]}oOre-write latter{s | C(s ) = Cs Ea , |Cs = , |= IC 0^effo [Eo ]}oOObviously, desired, latter set comprised one equivalence class possibilityassign propositions Cs Ea way compliant IC effa . concludesargument.2Theorem 7. Assume WSC|sf wd task (P, IC , O, C0 , 0 , G ) without inconsistent operators,plan ha1 , . . . , compiled task (P , A, 0 , G ). sub-sequence non-goalachievement actions ha1 , . . . , plan (P, IC , O, C0 , 0 , G ).Proof: IC 0 unsatisfiable, nothing prove, start belief originaltask empty. non-trivial case, first note that, plan compiled task, goalachievement actions moved back plan. Hence, without loss generality,assume ha1 , . . . , ai consist entirely non-goal achievement actions, hai+1 , . . . , ai consistentirely goal achievement actions.Denote b := res(b0 , ha1 , . . . , ai i) b := res(b0 , ha1 , . . . ,ai i). Lemma 3, b = sb [s]. Since ha1 , . .. , plan compiled task, everyb tuple constants satisfying G . b = sb [s], follows every b satisfiesG .2Theorem 8. Assume WSC|sf wd task (P, IC , O, C0 , 0 , G ) without inconsistent operators,plan ha1 , . . . , every operator appears one instantiation Eooutputs. ha1 , . . . , extended goal achievement actions form plancompiled task (P , A, 0 , G ) obtained using outputs Eo .Proof:Denote b := res(b0 , ha1 , . . . , i) b := res(b0 , ha1 , . . . , i).Lemma 3,b = sb [s]. Since ha1 , . . . , plan, every b satisfies G . b = sb [s], followsevery b tuple constants satisfying G . Attaching respective goal achievementactions yields plan compiled task.2111fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREReferences3DComplex.org (2008). web server browse protein complexes known 3d structures.http://supfam.mrc-lmb.cam.ac.uk/elevy/3dcomplex/data/hierarchy 1/root.html.Agarwal, V., Chafle, G., Dasgupta, K., Karnik, N., Kumar, A., Mittal, S., & Srivastava, B. (2005a).Synthy: system end end composition web services. Journal Web Semantics,3(4).Agarwal, V., Dasgupta, K., Karnik, N., Kumar, A., Kundu, A., Mittal, S., & Srivastava, B. (2005b).service creation environment based end end composition web services. 14thInternational Conference World Wide Web (WWW05), pp. 128137.Akkiraju, R., Srivastava, B., Anca-Andreea, I., Goodwin, R., & Syeda-Mahmood, T. (2006). Semaplan: Combining planning semantic matching achieve web service composition.4th International Conference Web Services (ICWS06).Ambite, J., & Kapoor, D. (2007). Automatically composing data workflows relational descriptions shim services. 6th International Semantic Web Conference (ISWC07).Ankolekar, A., Burstein, M., Hobbs, J., Lassila, O., Martin, D., McDermott, D., McIlraith, S.,Narayanan, S., Paolucci, M., Payne, T., & Sycara, K. (2002). DAML-S: Web service description semantic web. 1st International Semantic Web Conference (ISWC02).Au, T.-C., Kuter, U., & Nau, D. (2005). Web service composition volatile information. 4thInternational Semantic Web Conference (ISWC05).Au, T.-C., & Nau, D. (2006). incompleteness planning volatile external information.17th European Conference Artificial Intelligence (ECAI06).Baader, F., Lutz, C., Milicic, M., Sattler, U., & Wolter, F. (2005). Integrating description logicsaction formalisms: First results. 20th National Conference Artificial Intelligence(AAAI05).Bacchus, F. (2000). Subset PDDL AIPS2000 Planning Competition. AIPS-00 Planning Competition Committee.Bertoli, P., Pistore, M., & Traverso, P. (2006). Automated web service composition on-the-flybelief space search. 16th International Conference Automated Planning Scheduling(ICAPS06).Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic search beliefspace. 5th International Conference Artificial Intelligence Planning Systems (AIPS00),pp. 5261.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),533.Branden, C., & Tooze, J. (1998). Introduction Protein Structure: Second Edition. Garland Publishing Company, New York. ISBN 0815323050.Brayton, R., Hachtel, G., McMullen, C., & Sangiovanni-Vincentelli, A. (1984). Logic MinimizationAlgorithms VLSI Synthesis. Kluwer Academic Publishers.Brewka, G., & Hertzberg, J. (1993). things worlds: formalizing actionsplans. J. Logic Computation, 3(5), 517532.112fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONBryce, D., Kambhampati, S., & Smith, D. E. (2006). Planning graph heuristics belief spacesearch. Journal Artificial Intelligence Research, 26, 3599.Burstein, M., Hobbs, J., Lassila, O., McDermott, D., McIlraith, S., Narayanan, S., Paolucci, M.,Parsia, B., Payne, T., Sirin, E., Srinivasan, N., Sycara, K., & Martin, D. (2004). OWL-S:Semantic Markup Web Services. OWL-S 1.1. http://www.daml.org/services/owl-s/1.1/.Version 1.1.Bylander, T. (1994). computational complexity propositional STRIPS planning. ArtificialIntelligence, 69(12), 165204.Chasman, D. (Ed.). (2003). Protein Structure Determination, Analysis Applications DrugDiscovery. Marcel Dekker Ltd. 0-8247-4032-7.Chen, Y., Wah, B., & Hsu, C. (2006). Temporal planning using subgoal partitioning resolutionSGPlan. Journal Artificial Intelligence Research, 26, 323369.Cimatti, A., Roveri, M., & Bertoli, P. (2004). Conformant planning via symbolic model checkingheuristic search. Artificial Intelligence, 159(12), 127206.Constantinescu, I., & Faltings, B. (2003). Efficient matchmaking directory services. 2ndInternational Conference Web Intelligence (WI03).Constantinescu, I., Faltings, B., & Binder, W. (2004a). Large scale, type-compatible service composition. 2nd International Conference Web Services (ICWS04).Constantinescu, I., Faltings, B., & Binder, W. (2004b). Typed Based Service Composition. 13thInternational Conference World Wide Web (WWW04).de Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2006). update descriptionlogic ontologies instance level. 21st National Conference Artificial Intelligence(AAAI06).de Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2007). approximation instancelevel update erasure description logics. 22nd National Conference AmericanAssociation Artificial Intelligence (AAAI07).de Jonge, M., van der Linden, W., & Willems, R. (2007). eServices hospital equipment. 6thInternational Conference Service-Oriented Computing (ICSOC07), pp. 391397.Edelkamp, S. (2003). Promela planning. 10th International SPIN Workshop Model CheckingSoftware (SPIN03).Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2003). logic programming approachknowledge-state planning, II: DLVK system. Artificial Intelligence, 144(1-2), 157211.Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2004). logic programming approachknowledge-state planning: Semantics complexity. Transactions Computational Logic,5(2), 206263.Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision, updates, counterfactuals. Artificial Intelligence, 57(2-3), 227270.Fagin, R., Kuper, G., Ullman, J., & Vardi, M. (1988). Updating logical databases. AdvancesComputing Research, 3, 118.113fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREFensel, D., Lausen, H., Polleres, A., de Bruijn, J., Stollberg, M., Roman, D., & Domingue, J. (2006).Enabling Semantic Web Services Web Service Modeling Ontology. Springer-Verlag.Fersht, A. (1998). Structure Mechanism Protein Science: Guide Enzyme CatalysisProtein Folding. MPS. ISBN-13 9780716732686.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planningdomains. Journal Artificial Intelligence Research, 20, 61124.Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local search temporalaction graphs. Journal Artificial Intelligence Research, 20, 239290.Gerevini, A., Saetti, A., Serina, I., & Toninelli, P. (2005). Fast planning domains derivedpredicates: approach based rule-action graphs local search. 20th National Conference American Association Artificial Intelligence (AAAI05).Ginsberg, M., & Smith, D. (1988). Reasoning action I: possible worlds approach. ArtificialIntelligence, 35(2), 165195.Giunchiglia, E., Lee, J., Lifschitz, V., McCain, N., & Turner, H. (2004). Nonmonotonic causaltheories. Artificial Intelligence, 153(1-2), 49104.Giunchiglia, E., & Lifschitz, V. (1998). action language based causal explanation: Preliminary report. 15th National Conference Artificial Intelligence (AAAI98).Golden, K. (2002). DPADL: action language data processing domains. Proc. 3rdInternational NASA Planning Scheduling Workshop.Golden, K. (2003). domain description language data processing. Proc. WorkshopFuture PDDL ICAPS03.Golden, K., Pand, W., Nemani, R., & Votava, P. (2003). Automating processing earth observation data. Proceedings 7th International Symposium Artificial Intelligence,Robotics Automation Space.Gomes, C., Selman, B., Crato, N., & Kautz, H. (2000). Heavy-tailed phenomena satisfiabilityconstraint satisfaction problems. Journal Automated Reasoning, 24(1/2), 67100.Helmert, M. (2002). Decidability undecidability results planning numerical state variables. 6th International Conference Artificial Intelligence Planning Systems (AIPS02).Helmert, M. (2006). Fast Downward planning system. Journal Artificial Intelligence Research, 26, 191246.Herzig, A. (1996). PMA revisited. 5th International Conference Principles KnowledgeRepresentation Reasoning (KR96).Herzig, A., Lang, J., Marquis, P., & Polacsek, T. (2001). Updates, actions, planning. 17thInternational Joint Conference Artificial Intelligence (IJCAI01), pp. 119124.Herzig, A., & Rifi, O. (1999). Propositional belief base update minimal change. ArtificialIntelligence, 115(1), 107138.Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning benchmarks. Journal Artificial Intelligence Research, 24, 685758.Hoffmann, J., & Brafman, R. (2006). Conformant planning via heuristic forward search: newapproach. Artificial Intelligence, 170(67), 507541.114fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONHoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristicsearch. Journal Artificial Intelligence Research, 14, 253302.Katzuno, H., & Mendelzon, A. (1991). difference updating knowledge baserevising it. 2nd International Conference Principles Knowledge RepresentationReasoning (KR91).Kona, S., Bansal, A., Gupta, G., & Hite, D. (2007). Automatic composition semantic web services. 5th International Conference Web Services (ICWS07).Kumar, A., Neogi, A., Pragallapati, S., & Ram, J. (2007). Raising programming abstractionobjects services. 5th International Conference Web Services (ICWS07).Kuter, U., Sirin, E., Nau, D., Parsia, B., & Hendler, J. (2005). Information gathering planningweb service composition. Journal Web Semantics, 3(2-3), 183205.Lecue, F., & Delteil, A. (2007). Making difference semantic web service composition.22nd National Conference American Association Artificial Intelligence (AAAI07).Lecue, F., & Leger, A. (2006). formal model semantic web service composition. 5thInternational Semantic Web Conference (ISWC06).Li, L., & Horrocks, I. (2003). software framework matchmaking based semantic webtechnology. 12th International Conference World Wide Web (WWW03).Liberatore, P. (2000). complexity belief update. Artificial Intelligence, 119(1-2), 141190.Lin, F., & Reiter, R. (1994). State constraints revisited. Journal Logic Computation, 4(5),655678.Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006a). Reasoning actions using descriptionlogics general TBoxes. 10th European Conference Logics Artificial Intelligence(JELIA 2006).Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006b). Updating description logic ABoxes. 10th International Conference Principles Knowledge Representation Reasoning (KR06).Liu, Z., Ranganathan, A., & Riabov, A. (2007). planning approach message-oriented semanticweb service composition. 22nd National Conference American AssociationArtificial Intelligence (AAAI07).Long, D., & Fox, M. (2003). 3rd international planning competition: Results analysis.Journal Artificial Intelligence Research, 20, 159.Lutz, C., & Sattler, U. (2002). proposal describing services DLs. InternationalWorkshop Description Logics 2002 (DL02).McCain, N., & Turner, H. (1995). causal theory ramifications qualifications. 14thInternational Joint Conference Artificial Intelligence (IJCAI-95), pp. 19781984.McCarthy, J., & Hayes, P. (1969). philosophical problems standpoint artificialintelligence. Machine Intelligence, 4, 463502.McDermott, D. (2002). Estimated-regression planning interactions web services. 6thInternational Conference Artificial Intelligence Planning Systems (AIPS02).McDermott, D., et al. (1998). PDDL Planning Domain Definition Language. AIPS-98Planning Competition Committee.115fiH OFFMANN , B ERTOLI , H ELMERT & P ISTOREMcDermott, D. V. (1999). Using regression-match graphs control search planning. ArtificialIntelligence, 109(1-2), 111159.McGeer, P., Sanghavi, J., Brayton, R. K., & Sangiovanni-Vincentelli, A. (1993). ESPRESSOSignature: new exact minimizer logic functions. Proceedings 30th ACM/IEEEDesign Automation Conference (DAC-93).McGuinness, D. L., & van Harmelen, F. (2004). OWL Web Ontology Language Overview (W3CRecommendation). online http://www.w3.org/TR/owl-features/.McIlraith, S., & Fadel, R. (2002). Planning complex actions. 9th International WorkshopNon-Monotonic Reasoning (NMR02), pp. 356364.McIlraith, S., & Son, T. C. (2002). Adapting Golog composition semantic Web services.8th International Conference Principles Knowledge Representation Reasoning(KR02).Mediratta, A., & Srivastava, B. (2006). Applying planning composition web servicesuser-driven contingent planner. Tech. rep. RI 06002, IBM Research.Meyer, H., & Weske, M. (2006). Automated service composition using heuristic search. 4thInternational Conference Business Process Management (BPM06).Narayanan, S., & McIlraith, S. (2002). Simulation, verification automated composition webservices. 11th International Conference World Wide Web (WWW02).Palacios, H., & Geffner, H. (2007). conformant classical planning: Efficient translationsmay complete too. 17th International Conference Automated PlanningScheduling (ICAPS07).Paolucci, M., Kawamura, T., Payne, T., & Sycara, K. (2002). Semantic matching web servicescapabilities. 1st International Semantic Web Conference (ISWC02).Pednault, E. P. (1989). ADL: Exploring middle ground STRIPS situationcalculus. 1st International Conference Principles Knowledge RepresentationReasoning (KR89).Penberthy, J., & Weld, D. (1992). UCPOP: sound, complete, partial order planner ADL.3rd International Conference Principles Knowledge Representation Reasoning(KR92), pp. 103114.Petsko, G. A., & Ringe, D. (2004). Protein Structure Function. New Science Press. ISBN1405119225, 9781405119221.Pistore, M., Marconi, A., Bertoli, P., & Traverso, P. (2005a). Automated composition web services planning knowledge level. 19th International Joint Conference ArtificialIntelligence (IJCAI05).Pistore, M., Traverso, P., & Bertoli, P. (2005b). Automated composition web services planningasynchronous domains. 15th International Conference Automated PlanningScheduling (ICAPS05).Pistore, M., Traverso, P., Bertoli, P., & Marconi, A. (2005c). Automated synthesis compositeBPEL4WS web services. 3rd International Conference Web Services (ICWS05).116fiW EB ERVICE C OMPOSITION P LANNING U NCERTAINTY: N EW C ONNECTIONPonnekanti, S., & Fox, A. (2002). SWORD: developer toolkit web services composition.11th International Conference World Wide Web (WWW02).Reiter, R. (1991). frame problem situation calculus: simple solution (sometimes)completeness result goal regression. Artificial intelligence mathematical theorycomputation: papers honour John McCarthy, pp. 359380.Roman, D., Keller, U., Lausen, H., de Bruijn, J., Lara, R., Stollberg, M., Polleres, A., Feier, C.,Bussler, C., & Fensel, D. (2005). Web Service Modeling Ontology. Applied Ontology, 1(1),77106.Sheshagiri, M., desJardins, M., & Finin, T. (2003). planner composing services describedDAML-S. Third Symposium Adaptive Agents Multi-Agent Systems (AAMAS03).Sirin, E., Parsia, B., Wu, D., Hendler, J., & Nau, D. (2004). HTN planning web service composition using SHOP2. Journal Web Semantics, 1(4).Sirin, E., Hendler, J., & Parsia, B. (2003). Semi-automatic composition web services usingsemantic descriptions. Workshop Web Services ICEIS03.Sirin, E., & Parsia, B. (2004). Planning semantic web services. Workshop Semantic WebServices ISWC04.Sirin, E., Parsia, B., & Hendler, J. (2004). Composition-driven filtering selection semanticweb services. AAAI Fall Symposium Semantic Web Services.Sirin, E., Parsia, B., & Hendler, J. (2006). Template-based composition semantic web services.AAAI Fall Symposium Agents Search.Smith, D. E., & Weld, D. (1998). Conformant Graphplan. 15th National ConferenceAmerican Association Artificial Intelligence (AAAI-98).Srivastava, B. (2002). Automatic web services composition using planning. Knowledge BasedComputer Systems (KBCS02), pp. 467477.Thakkar, S., Ambite, J. L., & Knoblock, C. (2005). Composing, optimizing, executing plansbioinformatics web services. VLDB Journal, Special Issue Data Management, AnalysisMining Life Sciences, 14(3), 330353.Thiebaux, S., Hoffmann, J., & Nebel, B. (2005). defense PDDL axioms. Artificial Intelligence,168(12), 3869.Winslett, M. (1988). Reasoning actions using possible models approach. 7th NationalConference American Association Artificial Intelligence (AAAI88).Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.Zhan, R., Arpinar, B., & Aleman-Meza, B. (2003). Automatic composition semantic web services. 1st International Conference Web Services (ICWS03).117fi