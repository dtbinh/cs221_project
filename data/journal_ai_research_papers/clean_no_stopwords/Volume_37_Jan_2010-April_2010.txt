Journal Artificial Intelligence Research 37 (2010) 141-188Submitted 10/09; published 02/10Frequency Meaning:Vector Space Models SemanticsPeter D. Turneypeter.turney@nrc-cnrc.gc.caNational Research Council CanadaOttawa, Ontario, Canada, K1A 0R6Patrick Pantelme@patrickpantel.comYahoo! LabsSunnyvale, CA, 94089, USAAbstractComputers understand little meaning human language. profoundlylimits ability give instructions computers, ability computers explainactions us, ability computers analyse process text. Vector spacemodels (VSMs) semantics beginning address limits. paper surveysuse VSMs semantic processing text. organize literature VSMs accordingstructure matrix VSM. currently three broad classes VSMs,based termdocument, wordcontext, pairpattern matrices, yielding three classesapplications. survey broad range applications three categoriestake detailed look specific open source project category. goalsurvey show breadth applications VSMs semantics, provide newperspective VSMs already familiar area, providepointers literature less familiar field.1. IntroductionOne biggest obstacles making full use power computerscurrently understand little meaning human language. Recent progresssearch engine technology scratching surface human language, yetimpact society economy already immense. hints transformativeimpact deeper semantic technologies have. Vector space models (VSMs), surveyedpaper, likely part new semantic technologies.paper, use term semantics general sense, meaning word,phrase, sentence, text human language, study meaning.concerned narrower senses semantics, semantic web approachessemantics based formal logic. present survey VSMs relationdistributional hypothesis approach representing aspects natural languagesemantics.VSM developed SMART information retrieval system (Salton, 1971)Gerard Salton colleagues (Salton, Wong, & Yang, 1975). SMART pioneeredmany concepts used modern search engines (Manning, Raghavan, &Schutze, 2008). idea VSM represent document collectionpoint space (a vector vector space). Points close together spacesemantically similar points far apart semantically distant. usersc2010AI Access Foundation National Research Council Canada. Reprinted permission.fiTurney & Pantelquery represented point space documents (the query pseudodocument). documents sorted order increasing distance (decreasing semanticsimilarity) query presented user.success VSM information retrieval inspired researchers extendVSM semantic tasks natural language processing, impressive results.instance, Rapp (2003) used vector-based representation word meaning achievescore 92.5% multiple-choice synonym questions Test English ForeignLanguage (TOEFL), whereas average human score 64.5%.1 Turney (2006) usedvector-based representation semantic relations attain score 56% multiple-choiceanalogy questions SAT college entrance test, compared average human score57%.2survey, organized past work VSMs according type matrixinvolved: termdocument, wordcontext, pairpattern. believe choiceparticular matrix type fundamental choices, particularlinguistic processing mathematical processing. Although three matrix types coverwork, reason believe three types exhaust possibilities.expect future work introduce new types matrices higher-order tensors.31.1 Motivation Vector Space Models SemanticsVSMs several attractive properties. VSMs extract knowledge automaticallygiven corpus, thus require much less labour approaches semantics,hand-coded knowledge bases ontologies. example, main resource usedRapps (2003) VSM system measuring word similarity British National Corpus(BNC),4 whereas main resource used non-VSM systems measuring word similarity(Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003)lexicon, WordNet5 Rogets Thesaurus. Gathering corpus new languagegenerally much easier building lexicon, building lexicon often involves alsogathering corpus, SemCor WordNet (Miller, Leacock, Tengi, & Bunker, 1993).VSMs perform well tasks involve measuring similarity meaningwords, phrases, documents. search engines use VSMs measure similarityquery document (Manning et al., 2008). leading algorithms measuring semantic relatedness use VSMs (Pantel & Lin, 2002a; Rapp, 2003; Turney, Littman,Bigham, & Shnayder, 2003). leading algorithms measuring similarity semantic relations also use VSMs (Lin & Pantel, 2001; Turney, 2006; Nakov & Hearst, 2008).(Section 2.4 discusses differences types similarity.)find VSMs especially interesting due relation distributional hypothesis related hypotheses (see Section 2.7). distributional hypothesis1. Regarding average score 64.5% TOEFL questions, Landauer Dumais (1997) notethat, Although know performance would compare, example, U.S. schoolchildren particular age, told average score adequate admission manyuniversities.2. average score highschool students senior year, applying US universities.discussion score, see Section 6.3 Turneys (2006) paper.3. vector first-order tensor matrix second-order tensor. See Section 2.5.4. See http://www.natcorp.ox.ac.uk/.5. See http://wordnet.princeton.edu/.142fiFrom Frequency Meaningwords occur similar contexts tend similar meanings (Wittgenstein, 1953;Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990). Efforts apply abstract hypothesis concrete algorithms measuringsimilarity meaning often lead vectors, matrices, higher-order tensors.intimate connection distributional hypothesis VSMs strong motivationtaking close look VSMs.uses vectors matrices count vector space models. purposessurvey, take defining property VSMs values elementsVSM must derived event frequencies, number times given wordappears given context (see Section 2.6). example, often lexicon knowledgebase may viewed graph, graph may represented using adjacency matrix,imply lexicon VSM, because, general, valueselements adjacency matrix derived event frequencies. emphasisevent frequencies brings unity variety VSMs explicitly connectsdistributional hypothesis; furthermore, avoids triviality excluding many possiblematrix representations.1.2 Vectors AI Cognitive ScienceVectors common AI cognitive science; common VSMintroduced Salton et al. (1975). novelty VSM use frequenciescorpus text clue discovering semantic information.machine learning, typical problem learn classify cluster set items(i.e., examples, cases, individuals, entities) represented feature vectors (Mitchell, 1997;Witten & Frank, 2005). general, features derived event frequencies,although possible (see Section 4.6). example, machine learning algorithmapplied classifying clustering documents (Sebastiani, 2002).Collaborative filtering recommender systems also use vectors (Resnick, Iacovou,Suchak, Bergstrom, & Riedl, 1994; Breese, Heckerman, & Kadie, 1998; Linden, Smith, &York, 2003). typical recommender system, person-item matrix,rows correspond people (customers, consumers), columns correspond items(products, purchases), value element rating (poor, fair, excellent)person given item. Many mathematical techniques work welltermdocument matrices (see Section 4) also work well person-item matrices,ratings derived event frequencies.cognitive science, prototype theory often makes use vectors. basic ideaprototype theory members category central others (Rosch& Lloyd, 1978; Lakoff, 1987). example, robin central (prototypical) membercategory bird, whereas penguin peripheral. Concepts varying degreesmembership categories (graded categorization). natural way formalizerepresent concepts vectors categories sets vectors (Nosofsky, 1986; Smith, Osherson, Rips, & Keane, 1988). However, vectors usually based numerical scoreselicited questioning human subjects; based event frequencies.Another area psychology makes extensive use vectors psychometrics,studies measurement psychological abilities traits. usual instrument143fiTurney & Pantelmeasurement test questionnaire, personality test. results testtypically represented subject-item matrix, rows represent subjects(people) experiment columns represent items (questions) test(questionnaire). value element matrix answer correspondingsubject gave corresponding item. Many techniques vector analysis, factoranalysis (Spearman, 1904), pioneered psychometrics.cognitive science, Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer & Dumais, 1997), Hyperspace Analogue Language (HAL) (Lund, Burgess, &Atchley, 1995; Lund & Burgess, 1996), related research (Landauer, McNamara, Dennis, & Kintsch, 2007) entirely within scope VSMs, defined above, sinceresearch uses vector space models values elements derivedevent frequencies, number times given word appears given context. Cognitive scientists argued empirical theoretical reasonsbelieving VSMs, LSA HAL, plausible models aspects human cognition (Landauer et al., 2007). AI, computational linguistics, informationretrieval, plausibility essential, may seen sign VSMspromising area research.1.3 Motivation Surveypaper survey vector space models semantics. currently comprehensive, up-to-date survey field. show survey, vector space modelshighly successful approach semantics, wide range potential actualapplications. much recent growth research area.paper interest AI researchers work natural language,especially researchers interested semantics. survey serve generalintroduction area provide framework unified perspectiveorganizing diverse literature topic. encourage new research area,pointing open problems areas exploration.survey makes following contributions:New framework: provide new framework organizing literature: termdocument, wordcontext, pairpattern matrices (see Section 2). framework showsimportance structure matrix (the choice rows columns) determining potential applications may inspire researchers explore new structures(different kinds rows columns, higher-order tensors instead matrices).New developments: draw attention pairpattern matrices. use pairpattern matrices relatively new deserves study. matrices addresscriticisms directed wordcontext matrices, regarding lack sensitivityword order.Breadth approaches applications: existing survey showsbreadth potential actual applications VSMs semantics. Existing summariesomit pairpattern matrices (Landauer et al., 2007).Focus NLP CL: focus survey systems perform practicaltasks natural language processing computational linguistics. Existing overviews focuscognitive psychology (Landauer et al., 2007).144fiFrom Frequency MeaningSuccess stories: draw attention fact VSMs arguablysuccessful approach semantics, far.1.4 Intended Readershipgoal writing paper survey state art vector space modelssemantics, introduce topic new area, give newperspective already familiar area.assume reader basic understanding vectors, matrices, linear algebra,one might acquire introductory undergraduate course linear algebra,text book (Golub & Van Loan, 1996). basic concepts vectors matricesimportant mathematical details. Widdows (2004) gives gentleintroduction vectors perspective semantics.also assume reader familiarity computational linguistics information retrieval. Manning et al. (2008) provide good introduction information retrieval.computational linguistics, recommend Manning Schutzes (1999) text.reader familiar linear algebra computational linguistics, surveypresent barriers understanding. Beyond background, necessaryfamiliar VSMs used information retrieval, natural language processing, computational linguistics. However, reader would likebackground reading, recommend Landauer et al.s (2007) collection.1.5 Highlights Outlinearticle structured follows. Section 2 explains framework organizingliterature VSMs according type matrix involved: termdocument, wordcontext,pairpattern. section, present overview VSMs, without gettingdetails matrix generated corpus raw text.high-level framework place, Sections 3 4 examine steps involvedgenerating matrix. Section 3 discusses linguistic processing Section 4 reviewsmathematical processing. order corpus would processedVSM systems (first linguistic processing, mathematical processing).VSMs used semantics, input model usually plain text.VSMs work directly raw text, first apply linguistic processingtext, stemming, part-of-speech tagging, word sense tagging, parsing. Section 3looks linguistic tools semantic VSMs.simple VSM, simple termdocument VSM, value elementdocument vector number times corresponding word occurs givendocument, VSMs apply mathematical processing raw frequency values.Section 4 presents main mathematical operations: weighting elements, smoothingmatrix, comparing vectors. section also describes optimization strategiescomparing vectors, distributed sparse matrix multiplication randomizedtechniques.end Section 4, reader general view concepts involvedvector space models semantics. take detailed look three VSM systemsSection 5. representative termdocument VSMs, present Lucene information145fiTurney & Pantelretrieval library.6 wordcontext VSMs, explore Semantic Vectors package,builds Lucene.7 representative pairpattern VSMs, review LatentRelational Analysis module S-Space package, also builds Lucene.8source code three systems available open source licensing.turn broad survey applications semantic VSMs Section 6. section also serves short historical view research semantic VSMs, beginninginformation retrieval Section 6.1. purpose give reader ideabreadth applications VSMs also provide pointers literature,reader wishes examine applications detail.termdocument matrix, rows correspond terms columns correspond documents (Section 6.1). document provides context understanding term.generalize idea documents chunks text arbitrary size (phrases, sentences,paragraphs, chapters, books, collections), result wordcontext matrix, includes termdocument matrix special case. Section 6.2 discusses applicationswordcontext matrices. Section 6.3 considers pairpattern matrices, rows correspond pairs terms columns correspond patterns pairsoccur.Section 7, discuss alternatives VSMs semantics. Section 8 considersfuture VSMs, raising questions power limitations. concludeSection 9.2. Vector Space Models Semanticstheme unites various forms VSMs discuss paperstated statistical semantics hypothesis: statistical patterns human word usageused figure people mean.9 general hypothesis underlies severalspecific hypotheses, bag words hypothesis, distributional hypothesis,extended distributional hypothesis, latent relation hypothesis, discussed below.2.1 Similarity Documents: TermDocument Matrixpaper, use following notational conventions: Matrices denoted boldcapital letters, A. Vectors denoted bold lowercase letters, b. Scalars representedlowercase italic letters, c.large collection documents, hence large number documentvectors, convenient organize vectors matrix. row vectors matrixcorrespond terms (usually terms words, discuss possibilities)6.7.8.9.See http://lucene.apache.org/java/docs/.See http://code.google.com/p/semanticvectors/.See http://code.google.com/p/airhead-research/wiki/LatentRelationalAnalysis.phrase taken Faculty Profile George Furnas University Michigan,http://www.si.umich.edu/people/faculty-detail.htm?sid=41. full quote is, Statistical SemanticsStudies statistical patterns human word usage used figure peoplemean, least level sufficient information access. term statistical semantics appearedwork Furnas, Landauer, Gomez, Dumais (1983), defined there.146fiFrom Frequency Meaningcolumn vectors correspond documents (web pages, example). kindmatrix called termdocument matrix.mathematics, bag (also called multiset) like set, except duplicatesallowed. example, {a, a, b, c, c, c} bag containing a, b, c. Order matterbags sets; bags {a, a, b, c, c, c} {c, a, c, b, a, c} equivalent. representbag {a, a, b, c, c, c} vector x = h2, 1, 3i, stipulating first elementx frequency bag, second element frequency b bag,third element frequency c. set bags represented matrix X,column x:j corresponds bag, row xi: corresponds unique member,element xij frequency i-th member j-th bag.termdocument matrix, document vector represents corresponding documentbag words. information retrieval, bag words hypothesisestimate relevance documents query representing documentsquery bags words. is, frequencies words document tend indicaterelevance document query. bag words hypothesis basisapplying VSM information retrieval (Salton et al., 1975). hypothesis expressesbelief column vector termdocument matrix captures (to degree)aspect meaning corresponding document; document about.Let X termdocument matrix. Suppose document collection contains n documents unique terms. matrix X rows (one row uniqueterm vocabulary) n columns (one column document). Let wi i-thterm vocabulary let dj j-th document collection. i-th rowX row vector xi: j-th column X column vector x:j . row vectorxi: contains n elements, one element document, column vector x:j containselements, one element term. Suppose X simple matrix frequencies.element xij X frequency i-th term wi j-th document dj .general, value elements X zero (the matrix sparse),since documents use small fraction whole vocabulary. randomlychoose term wi document dj , likely wi occur anywhere dj ,therefore xij equals 0.pattern numbers xi: kind signature i-th term wi ; likewise,pattern numbers x:j signature j-th document dj . is, patternnumbers tells us, degree, term document about.vector x:j may seem rather crude representation document dj . tellsus frequently words appear document, sequential order wordslost. vector attempt capture structure phrases, sentences,paragraphs, chapters document. However, spite crudeness, searchengines work surprisingly well; vectors seem capture important aspect semantics.VSM Salton et al. (1975) arguably first practical, useful algorithmextracting semantic information word usage. intuitive justification termdocument matrix topic document probabilistically influence authorschoice words writing document.10 two documents similar topics,two corresponding column vectors tend similar patterns numbers.10. Newer generative models, Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003), directlymodel intuition. See Sections 4.3 7.147fiTurney & Pantel2.2 Similarity Words: WordContext MatrixSalton et al. (1975) focused measuring document similarity, treating query searchengine pseudo-document. relevance document query givensimilarity vectors. Deerwester et al. (1990) observed shift focusmeasuring word similarity, instead document similarity, looking row vectorstermdocument matrix, instead column vectors.Deerwester et al. (1990) inspired termdocument matrix Salton et al. (1975),document necessarily optimal length text measuring word similarity.general, may wordcontext matrix, context given words,phrases, sentences, paragraphs, chapters, documents, exotic possibilities,sequences characters patterns.distributional hypothesis linguistics words occur similar contextstend similar meanings (Harris, 1954). hypothesis justification applying VSM measuring word similarity. word may represented vectorelements derived occurrences word various contexts,windows words (Lund & Burgess, 1996), grammatical dependencies (Lin, 1998;Pado & Lapata, 2007), richer contexts consisting dependency links selectionalpreferences argument positions (Erk & Pado, 2008); see Sahlgrens (2006) thesiscomprehensive study various contexts. Similar row vectors wordcontext matrixindicate similar word meanings.idea word usage reveal semantics implicit thingsWittgenstein (1953) said language-games family resemblance. Wittgensteinprimarily interested physical activities form context word usage (e.g.,word brick, spoken context physical activity building house), maincontext word often words.11Weaver (1955) argued word sense disambiguation machine translationbased co-occurrence frequency context words near given target word (theword want disambiguate). Firth (1957, p. 11) said, shall know wordcompany keeps. Deerwester et al. (1990) showed intuitions Wittgenstein (1953), Harris (1954), Weaver, Firth could used practical algorithm.2.3 Similarity Relations: PairPattern Matrixpairpattern matrix, row vectors correspond pairs words, mason : stonecarpenter : wood, column vectors correspond patterns pairs cooccur, X cuts X works . Lin Pantel (2001) introducedpairpattern matrix purpose measuring semantic similarity patterns;is, similarity column vectors. Given pattern X solves , algorithmable find similar patterns, solved X, resolved X,X resolves .Lin Pantel (2001) proposed extended distributional hypothesis, patternsco-occur similar pairs tend similar meanings. patterns X solves11. Wittgensteins intuition might better captured matrix combines words modalities,images (Monay & Gatica-Perez, 2003). values elements derived eventfrequencies, would include VSM approach semantics.148fiFrom Frequency Meaningsolved X tend co-occur similar X : pairs, suggestspatterns similar meanings. Pattern similarity used infer one sentenceparaphrase another (Lin & Pantel, 2001).Turney et al. (2003) introduced use pairpattern matrix measuringsemantic similarity relations word pairs; is, similarity row vectors.example, pairs mason : stone, carpenter : wood, potter : clay, glassblower : glassshare semantic relation artisan : material. case, first member pairartisan makes artifacts material second member pair.pairs tend co-occur similar patterns, X used Xshaped into.latent relation hypothesis pairs words co-occur similar patternstend similar semantic relations (Turney, 2008a). Word pairs similar rowvectors pairpattern matrix tend similar semantic relations. inverseextended distributional hypothesis, patterns similar column vectorspairpattern matrix tend similar meanings.2.4 SimilaritiesPairpattern matrices suited measuring similarity semantic relationspairs words; is, relational similarity. contrast, wordcontext matrices suitedmeasuring attributional similarity. distinction attributional relationalsimilarity explored depth Gentner (1983).attributional similarity two words b, sima (a, b) <, dependsdegree correspondence properties b. correspondenceis, greater attributional similarity. relational similarity two pairswords : b c : d, simr (a : b, c : d) <, depends degree correspondencerelations : b c : d. correspondence is, greater relationalsimilarity. example, dog wolf relatively high degree attributional similarity, whereas dog : bark cat : meow relatively high degree relational similarity(Turney, 2006).tempting suppose relational similarity reduced attributionalsimilarity. example, mason carpenter similar words stone woodsimilar words; therefore, perhaps follows mason : stone carpenter : woodsimilar relations. Perhaps simr (a : b, c : d) reduced sima (a, c) + sima (b, d). However,mason, carpenter, potter, glassblower similar words (they artisans),wood, clay, stone, glass (they materials used artisans), cannot infermason : glass carpenter : clay similar relations. Turney (2006, 2008a)presented experimental evidence relational similarity reduce attributionalsimilarity.term semantic relatedness computational linguistics (Budanitsky & Hirst, 2001)corresponds attributional similarity cognitive science (Gentner, 1983). Two wordssemantically related kind semantic relation (Budanitsky & Hirst,2001); semantically related degree share attributes (Turney, 2006).Examples synonyms (bank trust company), meronyms (car wheel), antonyms(hot cold), words functionally related frequently associated (pencil149fiTurney & Pantelpaper). might usually think antonyms similar, antonyms highdegree attributional similarity (hot cold kinds temperature, black whitekinds colour, loud quiet kinds sound). prefer term attributionalsimilarity term semantic relatedness, attributional similarity emphasizescontrast relational similarity, whereas semantic relatedness could confusedrelational similarity.computational linguistics, term semantic similarity applied words sharehypernym (car bicycle semantically similar, share hypernymvehicle) (Resnik, 1995). Semantic similarity specific type attributional similarity.prefer term taxonomical similarity term semantic similarity, termsemantic similarity misleading. Intuitively, attributional relational similarityinvolve meaning, deserve called semantic similarity.Words semantically associated tend co-occur frequently (e.g., beehoney) (Chiarello, Burgess, Richards, & Pollock, 1990). Words may taxonomically similar semantically associated (doctor nurse), taxonomically similar semantically associated (horse platypus), semantically associated taxonomically similar(cradle baby), neither semantically associated taxonomically similar (calculuscandy).Schutze Pedersen (1993) defined two ways words distributed corpus text: two words tend neighbours other, syntagmaticassociates. two words similar neighbours, paradigmatic parallels. Syntagmatic associates often different parts speech, whereas paradigmatic parallelsusually part speech. Syntagmatic associates tend semantically associated (bee honey often neighbours); paradigmatic parallels tend taxonomicallysimilar (doctor nurse similar neighbours).2.5 Semantic VSMspossibilities exhausted termdocument, wordcontext, pairpatternmatrices. might want consider triplepattern matrices, measuring semanticsimilarity word triples. Whereas pairpattern matrix might row mason :stone column X works , triplepattern matrix could row mason :stone : masonry column X uses build Z. However, n-tuples words growincreasingly rare n increases. example, phrases contain mason, stone,masonry together less frequent phrases contain mason stone together.triplepattern matrix much sparse pairpattern matrix (ceteris paribus).quantity text need, order enough numbers make matricesuseful, grows rapidly n increases. may better break n-tuples pairs.example, : b : c could decomposed : b, : c, b : c (Turney, 2008a). similaritytwo triples, : b : c : e : f , could estimated similarity correspondingpairs. relatively dense pairpattern matrix could serve surrogate relativelysparse triplepattern matrix.may also go beyond matrices. generalization matrix tensor (Kolda& Bader, 2009; Acar & Yener, 2009). scalar (a single number) zeroth-order tensor,vector first-order tensor, matrix second-order tensor. tensor order three150fiFrom Frequency Meaninghigher called higher-order tensor. Chew, Bader, Kolda, Abdelali (2007) use termdocumentlanguage third-order tensor multilingual information retrieval. Turney (2007)uses wordwordpattern tensor measure similarity words. Van de Cruys (2009) usesverbsubjectobject tensor learn selectional preferences verbs.Turneys (2007) tensor, example, rows correspond words TOEFLmultiple-choice synonym questions, columns correspond words Basic English (Ogden, 1930),12 tubes correspond patterns join rows columns (hencewordwordpattern third-order tensor). given word TOEFL questions represented corresponding wordpattern matrix slice tensor. elementsslice correspond patterns relate given TOEFL word wordBasic English. similarity two TOEFL words calculated comparing twocorresponding matrix slices. algorithm achieves 83.8% TOEFL questions.2.6 Types Tokenstoken single instance symbol, whereas type general class tokens (Manninget al., 2008). Consider following example (from Samuel Beckett):Ever tried. Ever failed.matter. Try again.Fail again. Fail better.two tokens type Ever, two tokens type again, two tokenstype Fail. Lets say line example document, threedocuments two sentences each. represent example tokendocumentmatrix typedocument matrix. tokendocument matrix twelve rows, onetoken, three columns, one line (Figure 1). typedocument matrixnine rows, one type, three columns (Figure 2).row vector token binary values: element 1 given token appearsgiven document 0 otherwise. row vector type integer values: elementfrequency given type given document. vectors related,type vector sum corresponding token vectors. example, row vectortype Ever sum two token vectors two tokens Ever.applications dealing polysemy, one approach uses vectors represent wordtokens (Schutze, 1998; Agirre & Edmonds, 2006) another uses vectors representword types (Pantel & Lin, 2002a). Typical word sense disambiguation (WSD) algorithmsdeal word tokens (instances words specific contexts) rather word types.mention approaches polysemy Section 6, due similarity closerelationship, although defining characteristic VSM concernedfrequencies (see Section 1.1), frequency property types, tokens.12. Basic English highly reduced subset English, designed easy people learn. wordsBasic English listed http://ogden.basic-english.org/.151fiTurney & PantelEver tried.Ever failed.EvertriedEverfailedmatterTryFailFailbettermatter.Try again.111100000000000011110000Fail again.Fail better.000000001111Figure 1: tokendocument matrix. Rows tokens columns documents.Ever tried.Ever failed.EvertriedfailedmatterTryFailbettermatter.Try again.211000000000111100Fail again.Fail better.000000121Figure 2: typedocument matrix. Rows types columns documents.152fiFrom Frequency Meaning2.7 Hypothesesmentioned five hypotheses section. repeat hypothesesinterpret terms vectors. hypothesis, cite work explicitlystates something like hypothesis implicitly assumes something like hypothesis.Statistical semantics hypothesis: Statistical patterns human word usageused figure people mean (Weaver, 1955; Furnas et al., 1983). units textsimilar vectors text frequency matrix,13 tend similar meanings.(We take general hypothesis subsumes four specific hypothesesfollow.)Bag words hypothesis: frequencies words document tend indicaterelevance document query (Salton et al., 1975). documents pseudodocuments (queries) similar column vectors termdocument matrix,tend similar meanings.Distributional hypothesis: Words occur similar contexts tend similarmeanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990). words similar rowvectors wordcontext matrix, tend similar meanings.Extended distributional hypothesis: Patterns co-occur similar pairs tendsimilar meanings (Lin & Pantel, 2001). patterns similar column vectorspairpattern matrix, tend express similar semantic relations.Latent relation hypothesis: Pairs words co-occur similar patterns tendsimilar semantic relations (Turney et al., 2003). word pairs similar rowvectors pairpattern matrix, tend similar semantic relations.yet explained means say vectors similar. discussSection 4.4.3. Linguistic Processing Vector Space Modelsassume raw data large corpus natural language text.generate termdocument, wordcontext, pairpattern matrix, useful applylinguistic processing raw text. types processing usedgrouped three classes. First, need tokenize raw text; is, need decideconstitutes term extract terms raw text. Second, may wantnormalize raw text, convert superficially different strings charactersform (e.g., car, Car, cars, Cars could normalized car). Third, may wantannotate raw text, mark identical strings characters different (e.g., flyverb could annotated fly/VB fly noun could annotated fly/NN).Grefenstette (1994) presents good study linguistic processing wordcontextVSMs. uses similar three-step decomposition linguistic processing: tokenization,surface syntactic analysis, syntactic attribute extraction.13. text frequency matrix, mean matrix higher-order tensor values elementsderived frequencies pieces text context pieces text collectiontext. text frequency matrix intended general structure, includes termdocument,wordcontext, pairpattern matrices special cases.153fiTurney & Pantel3.1 TokenizationTokenization English seems simple first glance: words separated spaces.assumption approximately true English, may work sufficiently well basicVSM, advanced VSM requires sophisticated approach tokenization.accurate English tokenizer must know handle punctuation (e.g., dont, Janes,and/or), hyphenation (e.g., state-of-the-art versus state art), recognize multi-wordterms (e.g., Barack Obama ice hockey) (Manning et al., 2008). may also wishignore stop words, high-frequency words relatively low information content,function words (e.g., of, the, and) pronouns (e.g., them, who, that). popular liststop words set 571 common words included source code SMARTsystem (Salton, 1971).14languages (e.g., Chinese), words separated spaces. basic VSMbreak text character unigrams bigrams. sophisticated approachmatch input text entries lexicon, matching oftendetermine unique tokenization (Sproat & Emerson, 2003). Furthermore, native speakersoften disagree correct segmentation. Highly accurate tokenization challengingtask human languages.3.2 Normalizationmotivation normalization observation many different strings characters often convey essentially identical meanings. Given want get meaningunderlies words, seems reasonable normalize superficial variations converting form. common types normalization case folding(converting words lower case) stemming (reducing inflected words stemroot form).Case folding easy English, problematic languages. French,accents optional uppercase, may difficult restore missing accentsconverting words lowercase. words cannot distinguished without accents;example, PECHE could either peche (meaning fishing peach) peche (meaning sin).Even English, case folding cause problems, case sometimes semanticsignificance. example, SMART information retrieval system, whereas smartcommon adjective; Bush may surname, whereas bush kind plant.Morphology study internal structure words. Often word composedstem (root) added affixes (inflections), plural forms past tenses (e.g.,trapped composed stem trap affix -ed). Stemming, kind morphologicalanalysis, process reducing inflected words stems. English, affixessimpler regular many languages, stemming algorithms basedheuristics (rules thumb) work relatively well (Lovins, 1968; Porter, 1980; Minnen,Carroll, & Pearce, 2001). agglutinative language (e.g., Inuktitut), many conceptscombined single word, using various prefixes, infixes, suffixes, morphologicalanalysis complicated. single word agglutinative language may correspondsentence half dozen words English (Johnson & Martin, 2003).14. source code available ftp://ftp.cs.cornell.edu/pub/smart/.154fiFrom Frequency Meaningperformance information retrieval system often measured precisionrecall (Manning et al., 2008). precision system estimate conditionalprobability document truly relevant query, system says relevant.recall system estimate conditional probability system saydocument relevant query, truly relevant.general, normalization increases recall reduces precision (Kraaij & Pohlmann,1996). natural, given nature normalization. remove superficialvariations believe irrelevant meaning, make easier recognize similarities; find similar things, recall increases. sometimes superficialvariations semantic significance; ignoring variations causes errors, precisiondecreases. Normalization also positive effect precision cases varianttokens infrequent smoothing variations gives reliable statistics.small corpus, may able afford overly selective, maybest aggressively normalize text, increase recall. large corpus,precision may important, might want normalization. Hull (1996)gives good analysis normalization information retrieval.3.3 AnnotationAnnotation inverse normalization. different strings characters maymeaning, also happens identical strings characters may differentmeanings, depending context. Common forms annotation include part-of-speechtagging (marking words according parts speech), word sense tagging (markingambiguous words according intended meanings), parsing (analyzing grammatical structure sentences marking words sentences accordinggrammatical roles) (Manning & Schutze, 1999).Since annotation inverse normalization, expect decrease recallincrease precision. example, tagging program noun verb, mayable selectively search documents act computer programming(verb) instead documents discuss particular computer programs (noun); henceincrease precision. However, document computer programs (noun) maysomething useful say act computer programming (verb), even documentnever uses verb form program; hence may decrease recall.Large gains IR performance recently reported result query annotation syntactic semantic information. Syntactic annotation includes querysegmentation (Tan & Peng, 2008) part speech tagging (Barr, Jones, & Regelson,2008). Examples semantic annotation disambiguating abbreviations queries (Wei,Peng, & Dumoulin, 2008) finding query keyword associations (Lavrenko & Croft, 2001;Cao, Nie, & Bai, 2005).Annotation also useful measuring semantic similarity words concepts(wordcontext matrices). example, Pantel Lin (2002a) presented algorithmdiscover word senses clustering row vectors wordcontext matrix, usingcontextual information derived parsing.155fiTurney & Pantel4. Mathematical Processing Vector Space Modelstext tokenized (optionally) normalized annotated, first stepgenerate matrix frequencies. Second, may want adjust weightselements matrix, common words high frequencies, yet lessinformative rare words. Third, may want smooth matrix, reduceamount random noise fill zero elements sparse matrix. Fourth,many different ways measure similarity two vectors.Lowe (2001) gives good summary mathematical processing wordcontext VSMs.decomposes VSM construction similar four-step process: calculate frequencies,transform raw frequency counts, smooth space (dimensionality reduction),calculate similarities.4.1 Building Frequency Matrixelement frequency matrix corresponds event: certain item (term, word,word pair) occurred certain situation (document, context, pattern) certain numbertimes (frequency). abstract level, building frequency matrix simple mattercounting events. practice, complicated corpus large.typical approach building frequency matrix involves two steps. First, scan sequentially corpus, recording events frequencies hash table,database, search engine index. Second, use resulting data structure generatefrequency matrix, sparse matrix representation (Gilbert, Moler, & Schreiber, 1992).4.2 Weighting Elementsidea weighting give weight surprising events less weight expectedevents. hypothesis surprising events, shared two vectors, discriminative similarity vectors less surprising events. example,measuring semantic similarity words mouse rat, contexts dissectexterminate discriminative similarity contexts like.information theory, surprising event higher information content expectedevent (Shannon, 1948). popular way formalize idea termdocumentmatrices tf-idf (term frequency inverse document frequency) family weightingfunctions (Sparck Jones, 1972). element gets high weight corresponding termfrequent corresponding document (i.e., tf high), term raredocuments corpus (i.e., df low, thus idf high). Salton Buckley (1988)defined large family tf-idf weighting functions evaluated information retrieval tasks, demonstrating tf-idf weighting yield significant improvementsraw frequency.Another kind weighting, often combined tf-idf weighting, length normalization(Singhal, Salton, Mitra, & Buckley, 1996). information retrieval, document lengthignored, search engines tend bias favour longer documents. Lengthnormalization corrects bias.Term weighting may also used correct correlated terms. example,terms hostage hostages tend correlated, yet may want normalize156fiFrom Frequency Meaningterm (as Section 3.2), slightly different meanings.alternative normalizing them, may reduce weights co-occurdocument (Church, 1995).Feature selection may viewed form weighting, terms getweight zero hence removed matrix. Forman (2003) provides goodstudy feature selection methods text classification.alternative tf-idf Pointwise Mutual Information (PMI) (Church & Hanks, 1989;Turney, 2001), works well wordcontext matrices (Pantel & Lin, 2002a)termdocument matrices (Pantel & Lin, 2002b). variation PMI Positive PMI(PPMI), PMI values less zero replaced zero (Niwa &Nitta, 1994). Bullinaria Levy (2007) demonstrated PPMI performs betterwide variety weighting approaches measuring semantic similarity wordcontext matrices. Turney (2008a) applied PPMI pairpattern matrices. giveformal definition PPMI here, example effective weighting function.Let F wordcontext frequency matrix nr rows nc columns. i-th rowF row vector fi: j-th column F column vector f:j . row fi:corresponds word wi column f:j corresponds context cj . valueelement fij number times wi occurs context cj . Let X matrixresults PPMI applied F. new matrix X number rowscolumns raw frequency matrix F. value element xij X definedfollows:fijpij = Pnr Pncj=1 fiji=1(1)Pncj=1 fijpi = Pnr Pnc(2)PnrfPncij= Pnr i=1(3)i=1pji=1j=1 fijj=1 fijpijpmiij = logpi pjpmiij pmiij > 0xij =0 otherwise(4)(5)definition, pij estimated probability word wi occurs contextcj , pi estimated probability word wi , pj estimated probabilitycontext cj . wi cj statistically independent, pi pj = pij (by definitionindependence), thus pmiij zero (since log(1) = 0). product pi pjwould expect pij wi occurs cj pure random chance. hand,interesting semantic relation wi cj , expect pij largerwould wi cj indepedent; hence find pij > pi pj ,thus pmiij positive. follows distributional hypothesis (see Section 2).word wi unrelated context cj , may find pmiij negative. PPMIdesigned give high value xij interesting semantic relation157fiTurney & Pantelwi cj ; otherwise, xij value zero, indicating occurrence wicj uninformative.well-known problem PMI biased towards infrequent events. Considercase wi cj statistically dependent (i.e., maximum association).pij = pi = pj . Hence (4) becomes log (1/pi ) PMI increases probabilityword wi decreases. Several discounting factors proposed alleviate problem.example follows (Pantel & Lin, 2002a):P cP rfik )fkj , nk=1min ( nk=1fijPncPnrij =fij + 1 min ( k=1 fkj , k=1 fik ) + 1newpmiij = ij pmiij(6)(7)Another way deal infrequent events Laplace smoothing probabilityestimates, pij , pi , pj (Turney & Littman, 2003). constant positive value addedraw frequencies calculating probabilities; fij replaced fij + k,k > 0. larger constant, greater smoothing effect. Laplace smoothingpushes pmiij values towards zero. magnitude push (the differencepmiij without Laplace smoothing) depends raw frequency fij .frequency large, push small; frequency small, push large. ThusLaplace smoothing reduces bias PMI towards infrequent events.4.3 Smoothing Matrixsimplest way improve information retrieval performance limit numbervector components. Keeping components representing frequently occurringcontent words way; however, common words, have, carry littlesemantic discrimination power. Simple component smoothing heuristics, based properties weighting schemes presented Section 4.2, shown maintainsemantic discrimination power improve performance similarity computations.Computing similarity pairs vectors, described Section 4.4,computationally intensive task. However, vectors share non-zero coordinatemust compared (i.e., two vectors share coordinate dissimilar).frequent context words, word the, unfortunately result vectors matchingnon-zero coordinate. words precisely contexts little semanticdiscrimination power. Consider pointwise mutual information weighting describedSection 4.2. Highly weighted dimensions co-occur frequently wordsdefinition highly discriminating contexts (i.e., high associationwords co-occur). keeping context-word dimensionsPMI conservative threshold setting others zero, Lin (1998) showednumber comparisons needed compare vectors greatly decreases losinglittle precision similarity score top-200 similar words every word.smoothing matrix, one computes reverse index non-zero coordinates.Then, compare similarity words context vector words contextvectors, vectors found match non-zero component reverse index mustcompared. Section 4.5 proposes optimizations along lines.158fiFrom Frequency MeaningDeerwester et al. (1990) found elegant way improve similarity measurementsmathematical operation termdocument matrix, X, based linear algebra. operation truncated Singular Value Decomposition (SVD), also called thin SVD. Deerwesteret al. briefly mentioned truncated SVD applied document similarityword similarity, focus document similarity. Landauer Dumais (1997)applied truncated SVD word similarity, achieving human-level scores multiple-choicesynonym questions Test English Foreign Language (TOEFL). TruncatedSVD applied document similarity called Latent Semantic Indexing (LSI),called Latent Semantic Analysis (LSA) applied word similarity.several ways thinking truncated SVD works. firstpresent math behind truncated SVD describe four ways looking it:latent meaning, noise reduction, high-order co-occurrence, sparsity reduction.SVD decomposes X product three matrices UVT , U Vcolumn orthonormal form (i.e., columns orthogonal unit length, UT U =VT V = I) diagonal matrix singular values (Golub & Van Loan, 1996). Xrank r, also rank r. Let k , k < r, diagonal matrix formedtop k singular values, let Uk Vk matrices produced selectingcorresponding columns U V. matrix Uk k VkT matrix rank kbest approximates original matrix X, sense minimizes approximationerrors. is, X = Uk k VkT minimizes kX XkF matrices X rank k,k . . . kF denotes Frobenius norm (Golub & Van Loan, 1996).Latent meaning: Deerwester et al. (1990) Landauer Dumais (1997) describetruncated SVD method discovering latent meaning. Suppose wordcontext matrix X. truncated SVD, X = Uk k VkT , creates low-dimensional linearmapping row space (words) column space (contexts). low-dimensionalmapping captures latent (hidden) meaning words contexts. Limitingnumber latent dimensions (k < r) forces greater correspondence wordscontexts. forced correspondence words contexts improves similaritymeasurement.Noise reduction: Rapp (2003) describes truncated SVD noise reduction technique.may think matrix X = Uk k VkT smoothed version original matrix X.matrix Uk maps row space (the space spanned rows X) smallerk-dimensional space matrix Vk maps column space (the space spannedcolumns X) k-dimensional space. diagonal matrix k specifiesweights reduced k-dimensional space. singular values rankeddescending order amount variation X fit. think matrixX composed mixture signal noise, signal noise,Uk k VkT mostly captures variation X due signal, whereas remainingvectors UVT mostly fitting variation X due noise.High-order co-occurrence: Landauer Dumais (1997) also describe truncatedSVD method discovering high-order co-occurrence. Direct co-occurrence (firstorder co-occurrence) two words appear identical contexts. Indirect co-occurrence(high-order co-occurrence) two words appear similar contexts. Similaritycontexts may defined recursively terms lower-order co-occurrence. LemaireDenhiere (2006) demonstrate truncated SVD discover high-order co-occurrence.159fiTurney & PantelSparsity reduction: general, matrix X sparse (mostly zeroes),truncated SVD, X = Uk k VkT , dense. Sparsity may viewed problem insufficientdata: text, matrix X would fewer zeroes, VSM would performbetter chosen task. perspective, truncated SVD way simulatingmissing text, compensating lack data (Vozalis & Margaritis, 2003).different ways viewing truncated SVD compatible other;possible perspectives correct. Future work likely provideviews SVD perhaps unifying view.good C implementation SVD large sparse matrices Rohdes SVDLIBC.15Another approach Brands (2006) incremental truncated SVD algorithm.16 Yet anotherapproach Gorrells (2006) Hebbian algorithm incremental truncated SVD. BrandsGorrells algorithms introduce interesting new ways handling missing values,instead treating zero values.higher-order tensors, operations analogous truncated SVD,parallel factor analysis (PARAFAC) (Harshman, 1970), canonical decomposition(CANDECOMP) (Carroll & Chang, 1970) (equivalent PARAFAC discovered independently), Tucker decomposition (Tucker, 1966). overview tensor decompositions, see surveys Kolda Bader (2009) Acar Yener (2009). Turney (2007)gives empirical evaluation well four different Tucker decomposition algorithmsscale large sparse third-order tensors. low-RAM algorithm, Multislice Projection,large sparse tensors presented evaluated.17Since work Deerwester et al. (1990), subsequent research discovered manyalternative matrix smoothing processes, Nonnegative Matrix Factorization (NMF)(Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf,Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), DiscreteComponent Analysis (DCA) (Buntine & Jakulin, 2006).four perspectives truncated SVD, presented above, apply equally wellrecent matrix smoothing algorithms. newer smoothing algorithms tendcomputationally intensive truncated SVD, attempt modelword frequencies better SVD. Truncated SVD implicitly assumes elementsX Gaussian distribution. Minimizing Frobenius norm kX XkFminimize noise, noise Gaussian distribution. However, knownword frequencies Gaussian distributions. recent algorithms basedrealistic models distribution word frequencies.184.4 Comparing Vectorspopular way measure similarity two frequency vectors (raw weighted)take cosine. Let x two vectors, n elements.15.16.17.18.SVDLIBC available http://tedlab.mit.edu/dr/svdlibc/.MATLAB source code available http://web.mit.edu/wingated/www/resources.html.MATLAB source code available http://www.apperceptual.com/multislice/.experience, pmiij appears approximately Gaussian, may explain PMI works welltruncated SVD, PPMI puzzling, less Gaussian PMI, yet apparentlyyields better semantic models PMI.160fiFrom Frequency Meaningx = hx1 , x2 , . . . , xn(8)= hy1 , y2 , . . . , yn(9)cosine angle x calculated follows:Pncos(x, y) = qPnyiPni=1 xi22i=1 xii=1 yixy=xx yyx=kxk kyk(10)(11)(12)words, cosine angle two vectors inner productvectors, normalized unit length. x frequency vectorswords, frequent word long vector rare word short vector,yet words might synonyms. Cosine captures idea length vectorsirrelevant; important thing angle vectors.cosine ranges 1 vectors point opposite directions ( 180degrees) +1 point direction ( 0 degrees). vectorsorthogonal ( 90 degrees), cosine zero. raw frequency vectors,necessarily cannot negative elements, cosine cannot negative, weightingsmoothing often introduce negative elements. PPMI weighting yield negativeelements, truncated SVD generate negative elements, even input matrixnegative values.measure distance vectors easily converted measure similarityinversion (13) subtraction (14).sim(x, y) = 1/dist(x, y)(13)sim(x, y) = 1 dist(x, y)(14)Many similarity measures proposed IR (Jones & Furnas, 1987)lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir,& McCarthy, 2004). commonly said IR that, properly normalized, differenceretrieval performance using different measures insignificant (van Rijsbergen, 1979).Often vectors normalized way (e.g., unit length unit probability)applying similarity measure.Popular geometric measures vector distance include Euclidean distance Manhattan distance. Distance measures information theory include Hellinger, Bhattacharya,Kullback-Leibler. Bullinaria Levy (2007) compared five distance measurescosine similarity measure four different tasks involving word similarity. Overall,best measure cosine. popular measures Dice Jaccard coefficients(Manning et al., 2008).161fiTurney & PantelLee (1999) proposed that, finding word similarities, measures focusedoverlapping coordinates less importance negative features (i.e., coordinatesone word nonzero value zero value) appear performbetter. Lees experiments, Jaccard, Jensen-Shannon, L1 measures seemedperform best. Weeds et al. (2004) studied linguistic statistical propertiessimilar words returned various similarity measures found measuresgrouped three classes:1. high-frequency sensitive measures (cosine, Jensen-Shannon, -skew, recall),2. low-frequency sensitive measures (precision),3. similar-frequency sensitive methods (Jaccard, Jaccard+MI, Lin, harmonic mean).Given word w0 , use high-frequency sensitive measure score words wiaccording similarity w0 , higher frequency words tend get higher scoreslower frequency words. use low-frequency sensitive measure,bias towards lower frequency words. Similar-frequency sensitive methods prefer word wiapproximately frequency w0 . one experiment determiningcompositionality collocations, high-frequency sensitive measures outperformedclasses (Weeds et al., 2004). believe determining appropriate similaritymeasure inherently dependent similarity task, sparsity statistics,frequency distribution elements compared, smoothing method appliedmatrix.4.5 Efficient ComparisonsComputing similarity rows (or columns) large matrix non-trivialproblem, worst case cubic running time O(n2r nc ), nr number rowsnc number columns (i.e., dimensionality feature space). Optimizationsparallelization often necessary.4.5.1 Sparse-Matrix MultiplicationOne optimization strategy generalized sparse-matrix multiplication approach (Sarawagi& Kirpal, 2004), based observation scalar product two vectorsdepends coordinates vectors nonzero values. Further,observe commonly used similarity measures vectors x y, cosine,overlap, Dice, decomposed three values: one depending nonzerovalues x, another depending nonzero values y, third dependingnonzero coordinates shared x y. formally, commonly used similarityscores, sim(x, y), expressed follows:sim(x, y) = f0 (Pni=1 f1 (xi , yi ), f2 (x), f3 (y))(15)example, cosine measure, cos(x, y), defined (10), expressed modelfollows:162fiFrom Frequency MeaningPcos(x, y) = f0 ( ni=1 f1 (xi , yi ), f2 (x), f3 (y))f0 (a, b, c) =bcf1 (a, b) = bqPn2f2 (a) = f3 (a) =i=1 ai(16)(17)(18)(19)Let X matrix want compute pairwise similarity, sim(x, y),rows columns x y. Efficient computation similarity matrixachieved leveraging fact sim(x, y) determined solely nonzerocoordinates shared x (i.e., f1 (0, xi ) = f1 (xi , 0) = 0 xi )vectors sparse. case, calculating f1 (xi , yi ) requiredvectors shared nonzero coordinate, significantly reducing cost computation.Determining vectors share nonzero coodinate easily achieved first buildinginverted index coordinates. indexing, also precompute f2 (x)f3 (y) without changing algorithm complexity. Then, vector x retrieveconstant time, index, vector shares nonzero coordinate xwePapply f1 (xi , yi ) shared coordinates i. computational cost algorithmNi2 Ni number vectors nonzero i-th coordinate. worstcase time complexity O(ncv) n number vectors compared, cmaximum number nonzero coordinates vector, v number vectorsnonzero i-th coordinate coordinate nonzerovectors. words, algorithm efficient density coordinateslow. experiments computing semantic similarity pairswords large web crawl, observed near linear average running time complexity n.computational cost reduced leveraging element weightingtechniques described Section 4.2. setting zero coordinates lowPPMI, PMI tf-idf score, coordinate density dramatically reduced costlosing little discriminative power. vein, Bayardo, Ma, Srikant (2007) describedstrategy omits coordinates highest number nonzero values.algorithm gives significant advantage interested finding solelysimilarity highly similar vectors.4.5.2 Distributed Implementation using MapReducealgorithm described Section 4.5.1 assumes matrix X fit memory,large X may impossible. Also, element X processed independently, running parallel processes non-intersecting subsets X makes processingfaster. Elsayed, Lin, Oard (2008) proposed MapReduce implementation deployed using Hadoop, open-source software package implementing MapReduce frameworkdistributed file system.19 Hadoop shown scale several thousands machines,allowing users write simple code, seamlessly manage sophisticated parallel execution code. Dean Ghemawat (2008) provide good primer MapReduceprogramming.19. Hadoop available download http://lucene.apache.org/hadoop/.163fiTurney & PantelMapReduce models Map step used start n Map tasks parallel,caching one m-th part X inverted index streaming one n-th part Xit. actual inputs read tasks directly HDFS (Hadoop DistributedFile System). value determined amount memory dedicatedinverted index, n determined trading fact that, n increases,parallelism obtained increased cost building inverted indexn times.similarity algorithm Section 4.5.1 runs task Map stepMapReduce job. Reduce step groups output rows (or columns) X.4.5.3 Randomized Algorithmsoptimization strategies use randomized techniques approximate various similarity measures. aim randomized algorithms improve computational efficiency(memory time) projecting high-dimensional vectors low-dimensional subspace.Truncated SVD performs projection, SVD computationally intensive.20insight randomized techniques high-dimensional vectors randomly projected low-dimensional subspace relatively little impact final similarityscores. Significant reductions computational cost reported little average error computing true similarity scores, especially applications wordsimilarity interested top-k similar vectors vector(Ravichandran, Pantel, & Hovy, 2005; Gorman & Curran, 2006).Random Indexing, approximation technique based Sparse Distributed Memory(Kanerva, 1993), computes pairwise similarity rows (or vectors) matrixcomplexity O(nr nc 1 ), 1 fixed constant representing length indexvectors assigned column. value 1 controls tradeoff accuracy versusefficiency. elements index vector mostly zeros small numberrandomly assigned +1s 1s. cosine measure two rows r1 r2approximated computing cosine two fingerprint vectors, fingerprint(r1 )fingerprint(r2 ), fingerprint(r) computed summing index vectorsnon-unique coordinate r. Random Indexing shown perform well LSAword synonym selection task (Karlgren & Sahlgren, 2001).Locality sensitive hashing (LSH) (Broder, 1997) another technique approximatessimilarity matrix complexity O(n2r 2 ), 2 constant number randomprojections, controls accuracy versus efficiency tradeoff.21 LSH general classtechniques defining functions map vectors (rows columns) short signaturesfingerprints, two similar vectors likely similar fingerprints. DefinitionsLSH functions include Min-wise independent function, preserves Jaccardsimilarity vectors (Broder, 1997), functions preserve cosine similarityvectors (Charikar, 2002). word similarity task, Ravichandran et al. (2005)showed that, average, 80% top-10 similar words random words foundtop-10 results using Charikars functions, average cosine error 0.01620. However, efficient forms SVD (Brand, 2006; Gorrell, 2006).21. LSH stems work Rabin (1981), proposed use hash functions random irreduciblepolynomials create short fingerprints collections documents. techniques useful manytasks, removing duplicate documents (deduping) web crawl.164fiFrom Frequency Meaning(using 2 = 10,000 random projections). Gorman Curran (2006) provide detailedcomparison Random Indexing LSH distributional similarity task. BNCcorpus, LSH outperformed Random Indexing; however, larger corpora combining BNC,Reuters Corpus, English news holdings LDC 2003, RandomIndexing outperformed LSH efficiency accuracy.4.6 Machine Learningintended application VSM clustering classification, similarity measurecosine (Section 4.4) may used. classification, nearest-neighbour algorithmuse cosine measure nearness (Dasarathy, 1991). clustering, similaritybased clustering algorithm use cosine measure similarity (Jain, Murty, & Flynn,1999). However, many machine learning algorithms work directlyvectors VSM, without requiring external similarity measure, cosine.effect, machine learning algorithms implicitly use internal approachesmeasuring similarity.machine learning algorithm works real-valued vectors use vectorsVSM (Witten & Frank, 2005). Linguistic processing (Section 3) mathematicalprocessing (Section 4) may still necessary, machine learning algorithm handlevector comparison (Sections 4.4 4.5).addition unsupervised (clustering) supervised (classification) machine learning, vectors VSM may also used semi-supervised learning (Ando & Zhang,2005; Collobert & Weston, 2008). general, nothing unique VSMs wouldcompel choice one machine learning algorithm another, aside algorithmsperformance given task. Therefore refer readers machine learningliterature (Witten & Frank, 2005), since advice specific VSMs.5. Three Open Source VSM Systemsillustrate three types VSMs discussed Section 2, section presents three opensource systems, one VSM type. chosen present open source systemsinterested readers obtain source code find systemsapply systems projects. three systems written Javadesigned portability ease use.5.1 TermDocument Matrix: LuceneLucene22 open source full-featured text search engine library supported ApacheSoftware Foundation. arguably ubiquitous implementation termdocumentmatrix, powering many search engines CNET, SourceForge, Wikipedia, Disney,AOL Comcast. Lucene offers efficient storage, indexing, well retrieval rankingfunctionalities. Although primarily used termdocument matrix, generalizesVSMs.22. Apache Lucene available download http://lucene.apache.org/.165fiTurney & PantelContent, webpages, PDF documents, images, video, programmaticallydecomposed fields stored database. database implements termdocument matrix, content corresponds documents fields correspond terms.Fields stored database indices computed field values. Luceneuses fields generalization content terms, allowing string literal indexdocuments. example, webpage could indexed terms contains, alsoanchor texts pointing it, host name, semantic classesclassified (e.g., spam, product review, news, etc.). webpage retrievedsearch terms matching fields.Columns termdocument matrix consist fields particular instancecontent (e.g., webpage). rows consist instances content index.Various statistics frequency tf-idf stored matrix. developerdefines fields schema identifies indexed Lucene. developeralso optionally defines content ranking function indexed field.index built, Lucene offers functionalities retrieving content. Usersissue many query types phrase queries, wildcard queries, proximity queries, rangequeries (e.g., date range queries), field-restricted queries. Results sortedfield index updates occur simultaneously searching. Lucenes indexdirectly loaded Tomcat webserver offers APIs common programming languages. Solr,23 separate Apache Software Foundation project, open source enterprisewebserver searching Lucene index presenting search results. full-featuredwebserver providing functionalities XML/HTTP JSON APIs, hit highlighting,faceted search, caching, replication.simple recipe creating web search service, using Nutch, Lucene Solr, consistscrawling set URLs (using Nutch), creating termdocument matrix index (usingLucene), serving search results (using Solr). Nutch,24 Apache Software Foundationopen source web search software, offers functionality crawling web seed setURLs, building link-graph web crawl, parsing web documentsHTML pages. good set seed URLs Nutch downloaded freelyOpen Directory Project.25 Crawled pages HTML-parsed, indexedLucene. resulting indexed collection queried served Solrinstallation Tomcat.information Lucene, recommend Gospodnetic Hatchers (2004)book. Konchady (2008) explains integrate Lucene LingPipe GATEsophisticated semantic processing.2623.24.25.26.Apache Solr available download http://lucene.apache.org/solr/.Apache Nutch available download http://lucene.apache.org/nutch/.See http://www.dmoz.org/.Information LingPipe available http://alias-i.com/lingpipe/. GATE (General Architecture Text Engineering) home page http://gate.ac.uk/.166fiFrom Frequency Meaning5.2 WordContext Matrix: Semantic VectorsSemantic Vectors27 open source project implementing random projection approachmeasuring word similarity (see Section 4.5.3). package uses Lucene create termdocument matrix, creates vectors Lucenes termdocument matrix, usingrandom projection dimensionality reduction. random projection vectorsused, example, measure semantic similarity two words find wordssimilar given word.idea random projection take high-dimensional vectors randomly projectrelatively low-dimensional space (Sahlgren, 2005). viewedkind smoothing operation (Section 4.3), developers Semantic Vectorspackage emphasize simplicity efficiency random projection (Section 4.5), rathersmoothing ability. argue matrix smoothing algorithms mightsmooth better, none perform well random indexing, termscomputational complexity building smooth matrix incrementally updatingmatrix new data arrives (Widdows & Ferraro, 2008). aim encourageresearch development semantic vectors creating simple efficient opensource package.Semantic Vectors package designed convenient use, portable, easyextend modify. design software incorporates lessons learnedearlier Stanford Infomap project.28 Although default generate random projectionvectors, system modular design allows kinds wordcontext matricesused instead random projection matrices.package supports two basic functions: building wordcontext matrix searchingvectors matrix. addition generating word vectors, buildingoperation generate document vectors calculating weighted sums word vectorswords document. searching operation used search similarwords search documents similar query. query single wordseveral words combined, using various mathematical operations correspondingvectors. mathematical operations include vector negation disjunction, basedquantum logic (Widdows, 2004). Widdows Ferraro (2008) provide good summarySemantic Vectors software.5.3 PairPattern Matrix: Latent Relational Analysis S-SpaceLatent Relational Analysis29 (LRA) open source project implementing pairpatternmatrix. component S-Space package, library tools buildingcomparing different semantic spaces.LRA takes input textual corpus set word pairs. pairpattern matrixbuilt deriving lexical patterns link together word pairs corpus. example, consider word pair hKorea, Japani following retrieved matching sentences:27. Semantic Vectors software package measuring word similarity, available Simplified BSDLicense http://code.google.com/p/semanticvectors/.28. See http://infomap-nlp.sourceforge.net/.29. Latent Relational Analysis part S-Space package distributed GNU GeneralPublic License version 2. available http://code.google.com/p/airhead-research/. timewriting, LRA module development.167fiTurney & PantelKorea looks new Japan prime ministers effect Korea-Japan relations.channel Korea vs. Japan football game?two sentences, LRA extracts two patterns: X looks new X vs. .patterns become two columns pairpattern matrix, word pair hKorea,Japani becomes row. Pattern frequencies counted smoothed using SVD (seeSection 4.3).order mitigate sparseness occurrences word pairs, thesaurusWordNet used expand seed word pairs alternatives. example pairhKorea, Japani may expanded include hSouth Korea, Japani, hRepublic Korea,Japani, hKorea, Nipponi, hSouth Korea, Nipponi, hRepublic Korea, Nipponi.LRA uses Lucene (see Section 5.1) backend store matrix, index it, servecontents. detailed description LRA algorithm, suggest Turneys (2006)paper.6. Applicationssection, survey semantic applications VSMs. aimbreadth, rather depth; readers want depth consult references.goal give reader impression scope flexibility VSMs semantics.following applications grouped according type matrix involved: termdocument, wordcontext, pairpattern. Note section exhaustive;many references applications space list here.6.1 TermDocument MatricesTermdocument matrices suited measuring semantic similarity documentsqueries (see Section 2.1). usual measure similarity cosine column vectorsweighted termdocument matrix. variety applications measuresdocument similarity.Document retrieval: termdocument matrix first developed documentretrieval (Salton et al., 1975), large body literature VSMdocument retrieval (Manning et al., 2008), including several journals conferencesdevoted topic. core idea is, given query, rank documents orderdecreasing cosine angles query vector document vectors (Saltonet al., 1975). One variation theme cross-lingual document retrieval,query one language used retrieve document another language (Landauer &Littman, 1990). important technical advance discovery smoothingtermdocument matrix truncated SVD improve precision recall (Deerwesteret al., 1990), although commercial systems use smoothing, due computationalexpense document collection large dynamic. Random indexing (Sahlgren,2005) incremental SVD (Brand, 2006) may help address scaling issues. Anotherimportant development document retrieval addition collaborative filtering,form PageRank (Brin & Page, 1998).Document clustering: Given measure document similarity, clusterdocuments groups, similarity tends high within group, low across168fiFrom Frequency Meaninggroups (Manning et al., 2008). clusters may partitional (flat) (Cutting, Karger,Pedersen, & Tukey, 1992; Pantel & Lin, 2002b) may hierarchical structure(groups groups) (Zhao & Karypis, 2002); may non-overlapping (hard) (Croft,1977) overlapping (soft) (Zamir & Etzioni, 1999). Clustering algorithms also differclusters compared abstracted. single-link clustering, similaritytwo clusters maximum similarities members. Complete-linkclustering uses minimum similarities average-link clustering uses averagesimilarities (Manning et al., 2008).Document classification: Given training set documents class labelstesting set unlabeled documents, task document classification learntraining set assign labels testing set (Manning et al., 2008). labels maytopics documents (Sebastiani, 2002), sentiment documents (e.g.,positive versus negative product reviews) (Pang, Lee, & Vaithyanathan, 2002; Kim, Pantel,Chklovski, & Pennacchiotti, 2006), spam versus non-spam (Sahami, Dumais, Heckerman, &Horvitz, 1998; Pantel & Lin, 1998), labels might inferred wordsdocuments. classify documents, implying documentsclass similar way; thus document classification implies notion documentsimilarity, machine learning approaches document classification involve termdocument matrix (Sebastiani, 2002). measure document similarity, cosine,directly applied document classification using nearest-neighbour algorithm (Yang,1999).Essay grading: Student essays may automatically graded comparingone high-quality reference essays given essay topic (Wolfe, Schreiner, Rehder,Laham, Foltz, Kintsch, & Landauer, 1998; Foltz, Laham, & Landauer, 1999). studentessays reference essays compared cosines termdocument matrix.grade assigned student essay proportional similarity onereference essays; student essay highly similar reference essay gets high grade.Document segmentation: task document segmentation partition document sections, section focuses different subtopic document(Hearst, 1997; Choi, 2000). may treat document series blocks, blocksentence paragraph. problem detect topic shift one blocknext. Hearst (1997) Choi (2000) use cosine columns wordblockfrequency matrix measure semantic similarity blocks. topic shift signaleddrop cosine consecutive blocks. wordblock matrix viewedsmall termdocument matrix, corpus single document documentsblocks.Question answering: Given simple question, task Question Answering (QA)find short answer question searching large corpus. typical question is, many calories Big Mac? algorithms QA fourcomponents, question analysis, document retrieval, passage retrieval, answer extraction(Tellex, Katz, Lin, Fern, & Marton, 2003; Dang, Lin, & Kelly, 2006). Vector-based similarity measurements often used document retrieval passage retrieval (Tellexet al., 2003).Call routing: Chu-carroll Carpenter (1999) present vector-based systemautomatically routing telephone calls, based callers spoken answer question,169fiTurney & Pantelmay direct call? callers answer ambiguous, system automaticallygenerates question caller, derived VSM, prompts callerinformation.6.2 WordContext MatricesWordcontext matrices suited measuring semantic similarity words (seeSection 2.2). example, measure similarity two words cosineangle corresponding row vectors wordcontext matrix. manyapplications measures word similarity.Word similarity: Deerwester et al. (1990) discovered measure word similarity comparing row vectors termdocument matrix. Landauer Dumais (1997)evaluated approach 80 multiple-choice synonym questions Test English Foreign Language (TOEFL), achieving human-level performance (64.4% correctwordcontext matrix 64.5% average non-English US college applicant).documents used Landauer Dumais average length 151 words,seems short document, long context word. researchers soonswitched much shorter lengths, prefer call wordcontext matrices, instead termdocument matrices. Lund Burgess (1996) used context windowten words. Schutze (1998) used fifty-word window (25 words, centered targetword). Rapp (2003) achieved 92.5% correct 80 TOEFL questions, using four-wordcontext window (2 words, centered target word, removing stop words).TOEFL results suggest performance improves context window shrinks. seemsimmediate context word much important distant contextdetermining meaning word.Word clustering: Pereira, Tishby, Lee (1993) applied soft hierarchical clusteringrow-vectors wordcontext matrix. one experiment, words nounscontexts verbs given nouns direct objects. another experiment,words verbs contexts nouns direct objects givenverbs. Schutzes (1998) seminal word sense discrimination model used hard flat clusteringrow-vectors wordcontext matrix, context given window 25words, centered target word. Pantel Lin (2002a) applied soft flat clusteringwordcontext matrix, context based parsed text. algorithmsable discover different senses polysemous words, generating different clusterssense. effect, different clusters correspond different concepts underliewords.Word classification: Turney Littman (2003) used wordcontext matrix classify words positive (honest, intrepid) negative (disturbing, superfluous). usedGeneral Inquirer (GI) lexicon (Stone, Dunphy, Smith, & Ogilvie, 1966) evaluatealgorithms. GI lexicon includes 11,788 words, labeled 182 categories relatedopinion, affect, attitude.30 Turney Littman hypothesize 182 categoriesdiscriminated wordcontext matrix.Automatic thesaurus generation: WordNet popular tool research naturallanguage processing (Fellbaum, 1998), creating maintaing lexical resources30. GI lexicon available http://www.wjh.harvard.edu/inquirer/spreadsheet guide.htm.170fiFrom Frequency Meaninglabour intensive, natural wonder whether process automateddegree.31 task seen instance word clustering (when thesaurusgenerated scratch) classification (when existing thesaurus automaticallyextended), worthwhile consider task automatic thesaurus generationseparately clustering classification, due specific requirements thesaurus,particular kind similarity appropriate thesaurus (see Section 2.4).Several researchers used wordcontext matrices specifically task assistingautomating thesaurus generation (Crouch, 1988; Grefenstette, 1994; Ruge, 1997; Pantel &Lin, 2002a; Curran & Moens, 2002).Word sense disambiguation: typical Word Sense Disambiguation (WSD) system(Agirre & Edmonds, 2006; Pedersen, 2006) uses feature vector representationvector corresponds token word, type (see Section 2.6). However, Leacock,Towell, Voorhees (1993) used wordcontext frequency matrix WSD,vector corresponds type annotated sense tag. Yuret Yatbaz (2009) appliedwordcontext frequency matrix unsupervised WSD, achieving results comparableperformance supervised WSD systems.Context-sensitive spelling correction: People frequently confuse certain setswords, there, theyre, their. confusions cannot detected simple dictionary-based spelling checker; require context-sensitive spelling correction.wordcontext frequency matrix may used correct kinds spelling errors (Jones& Martin, 1997).Semantic role labeling: task semantic role labeling label parts sentence according roles play sentence, usually terms connectionmain verb sentence. Erk (2007) presented system wordcontextfrequency matrix used improve performance semantic role labeling. Pennacchiotti, Cao, Basili, Croce, Roth (2008) show wordcontext matrices reliablypredict semantic frame unknown lexical unit refers, good levelsaccuracy. lexical unit induction important semantic role labeling, narrowcandidate set roles observed lexical unit.Query expansion: Queries submitted search engines Google Yahoo!often directly match terms relevant documents. alleviateproblem, process query expansion used generating new search termsconsistent intent original query. VSMs form basis query semanticsmodels (Cao, Jiang, Pei, He, Liao, Chen, & Li, 2008). methods represent queriesusing session contexts, query cooccurrences user sessions (Huang, Chien, &Oyang, 2003; Jones, Rey, Madani, & Greiner, 2006), others use click contexts,urls clicked result query (Wen, Nie, & Zhang, 2001).Textual advertising: pay-per-click advertising models, prevalent search enginesGoogle Yahoo!, users pay keywords, called bidterms, useddisplay ads relevant queries issued users. scarcity data makes admatching difficult and, response, several techniques bidterm expansion using VSMsproposed. wordcontext matrix consists rows bidterms columns31. WordNet available http://wordnet.princeton.edu/.171fiTurney & Pantel(contexts) consist advertiser identifiers (Gleich & Zhukov, 2004) co-bidded bidterms(second order co-occurrences) (Chang, Pantel, Popescu, & Gabrilovich, 2009).Information extraction: field information extraction (IE) includes namedentity recognition (NER: recognizing chunk text name entity,person place), relation extraction, event extraction, fact extraction. Pasca etal. (2006) demonstrate wordcontext frequency matrix facilitate fact extraction.Vyas Pantel (2009) propose semi-supervised model using wordcontext matrixbuilding iteratively refining arbitrary classes named entities.6.3 PairPattern MatricesPairpattern matrices suited measuring semantic similarity word pairspatterns (see Section 2.3). example, measure similarity two wordpairs cosine angle corresponding row vectors pairpatternmatrix. many applications measures relational similarity.Relational similarity: measure attributional similarity cosineangle row vectors wordcontext matrix, measure relationalsimilarity cosine angle rows pairpattern matrix. approachmeasuring relational similarity introduced Turney et al. (2003) examineddetail Turney Littman (2005). Turney (2006) evaluated approachrelational similarity 374 multiple-choice analogy questions SAT collegeentrance test, achieving human-level performance (56% correct pairpattern matrix57% correct average US college applicant). highest performancefar algorithm. best algorithm based attributional similarity accuracy35% (Turney, 2006). best non-VSM algorithm achieves 43% (Veale, 2004).Pattern similarity: Instead measuring similarity row vectors pairpattern matrix, measure similarity columns; is, measurepattern similarity. Lin Pantel (2001) constructed pairpattern matrixpatterns derived parsed text. Pattern similarity used infer onephrase paraphrase another phrase, useful natural language generation,text summarization, information retrieval, question answering.Relational clustering: Bicici Yuret (2006) clustered word pairs representingrow vectors pairpattern matrix. Davidov Rappoport (2008) first clusteredcontexts (patterns) identified representative pairs context cluster.used representative pairs automatically generate multiple-choice analogy questions,style SAT analogy questions.Relational classification: Chklovski Pantel (2004) used pairpattern matrixclassify pairs verbs semantic classes. example, taint : poison classifiedstrength (poisoning stronger tainting) assess : review classified enablement(assessing enabled reviewing). Turney (2005) used pairpattern matrix classifynoun compounds semantic classes. example, flu virus classified cause (thevirus causes flu), home town classified location (the home located town),weather report classified topic (the topic report weather).Relational search: Cafarella, Banko, Etzioni (2006) described relational searchtask searching entities satisfy given semantic relations. example172fiFrom Frequency Meaningquery relational search engine list X X causes cancer.example, relation, cause, one terms relation, cancer, givenuser, task search engine find terms satisfy users query.organizers Task 4 SemEval 2007 (Girju, Nakov, Nastase, Szpakowicz, Turney, & Yuret,2007) envisioned two-step approach relational search: first conventional search enginewould look candidate answers, relational classification system would filterincorrect answers. first step manually simulated Task 4 organizersgoal Task 4 design systems second step. task attracted 14 teamssubmitted 15 systems. Nakov Hearst (2007) achieved good results using pairpatternmatrix.Automatic thesaurus generation: discussed automatic thesaurus generationSection 6.2, wordcontext matrices, arguably relational similarity relevantattributional similarity thesaurus generation. example, information WordNet relations words rather words individually.Snow, Jurafsky, Ng (2006) used pairpattern matrix build hypernym-hyponymtaxonomy, whereas Pennacchiotti Pantel (2006) built meronymy causation taxonomy. Turney (2008b) showed pairpattern matrix distinguish synonymsantonyms, synonyms non-synonyms, taxonomically similar words (hair fur)words merely semantically associated (cradle baby).Analogical mapping: Proportional analogies form : b :: c : d, meansb c d. example, mason : stone :: carpenter : wood means mason stonecarpenter wood. 374 multiple-choice analogy questions SAT collegeentrance test (mentioned above) involve proportional analogies. pairpatternmatrix, solve proportional analogies selecting choice maximizes relationalsimilarity (e.g., simr (mason : stone, carpenter : wood) high value). However, oftenencounter analogies involve four terms. well-known analogysolar system Rutherford-Bohr model atom contains least fourteenterms. solar system, planet, attracts, revolves, sun, gravity, solar system,mass. atom, revolves, atom, attracts, electromagnetism, nucleus,charge, electron. Turney (2008a) demonstrated handle complex,systematic analogies decomposing sets proportional analogies.7. Alternative Approaches Semanticsapplications list Section 6 necessarily require VSM approach.application, many possible approaches. section, brieflyconsider main alternatives.Underlying applications termdocument matrices (Section 6.1) taskmeasuring semantic similarity documents queries. main alternativesVSMs task probabilistic models, traditional probabilistic retrievalmodels information retrieval (van Rijsbergen, 1979; Baeza-Yates & Ribeiro-Neto, 1999)recent statistical language models inspired information theory (Liu &Croft, 2005). idea statistical language models information retrieval measuresimilarity query document creating probabilistic language model173fiTurney & Pantelgiven document measuring probability given query accordinglanguage model.progress information retrieval, distinction VSM approachprobabilistic approach becoming blurred, approach borrows ideasother. Language models typically involve multiplying probabilities, viewadding logs probabilities, makes language models look similar VSMs.applications wordcontext matrices (Section 6.2) share task measuringsemantic similarity words. main alternatives VSMs measuring word similarityapproaches use lexicons, WordNet (Resnik, 1995; Jiang & Conrath, 1997;Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Budanitsky & Hirst, 2001). ideaview lexicon graph, nodes correspond word senses edges representrelations words, hypernymy hyponymy. similarity twowords proportional length path graph joins two words.Several approaches measuring semantic similarity words combine VSMlexicon (Turney et al., 2003; Pantel, 2005; Patwardhan & Pedersen, 2006; Mohammad &Hirst, 2006). Humans use dictionary definitions observations word usage,natural expect best performance algorithms use distributionallexical information.Pairpattern matrices (Section 6.3) common task measuring semanticsimilarity relations. wordcontext matrices, main alternatives approachesuse lexicons (Rosario & Hearst, 2001; Rosario, Hearst, & Fillmore, 2002; Nastase& Szpakowicz, 2003; Veale, 2003, 2004). idea reduce relational similarityattributional similarity, simr (a : b, c : d) sima (a, c) + sima (b, d), use lexiconmeasure attributional similarity. discuss Section 2.4, reductionwork general. However, reduction often good approximation,evidence hybrid approach, combining VSM lexicon, beneficial (Turneyet al., 2003; Nastase, Sayyad-Shirabad, Sokolova, & Szpakowicz, 2006).8. Future Vector Space Models SemanticsSeveral authors criticized VSMs (French & Labiouse, 2002; Pado & Lapata, 2003;Morris & Hirst, 2004; Budanitsky & Hirst, 2006). criticism stemsfact termdocument wordcontext matrices typically ignore word order. LSA,instance, phrase commonly represented sum vectors individualwords phrase; hence phrases house boat boat house representedvector, although different meanings. English, word order expressesrelational information. house boat boat house Tool-Purpose relation,house boat means Tool-Purpose(boat, house) (a boat serves house), whereas boathouse means Tool-Purpose(house, boat) (a house sheltering storing boats).Landauer (2002) estimates 80% meaning English text comes wordchoice remaining 20% comes word order. However, VSMs inherentlylimited 80% meaning text. Mitchell Lapata (2008) propose compositionmodels sensitive word order. example, make simple additive model becomesyntax-aware, allow different weightings contributions vector components. Constituents important composition therefore participate174fiFrom Frequency Meaningactively. Clark Pulman (2007) assigned distributional meaning sentences using Hilbert space tensor product. Widdows Ferraro (2008), inspired quantummechanics, explores several operators modeling composition meaning. Pairpatternmatrices sensitive order words pair (Turney, 2006). Thusseveral ways handle word order VSMs.raises question, limits VSMs semantics? semanticsrepresented VSMs? much yet know representVSMs. example, Widdows (2004) van Rijsbergen (2004) show disjunction,conjunction, negation represented vectors, yet knowrepresent arbitrary statements first-order predicate calculus. However, seems possiblefuture work may discover answers limitations.survey, assumed VSMs composed elements valuesderived event frequencies. ties VSMs form distributional hypothesis(see Sections 1.1 2.7); therefore limits VSMs depend limits familydistributional hypotheses. statistical patterns word usage sufficient figurepeople mean? arguably major open question VSMs, answerdetermine future VSMs. strong argument one way other,believe continuing progress VSMs suggests far reachinglimits.9. Conclusionswant information help person, use words make requestdescribe problem, person replies words. Unfortunately, computersunderstand human language, forced use artificial languages unnatural userinterfaces. science fiction, dream computers understand human language,listen us talk us. achieve full potential computers, must enableunderstand semantics natural language. VSMs likely partsolution problem computing semantics.Many researchers struggled problem semantics comeconclusion meaning words closely connected statistics word usage(Section 2.7). try make intuition precise, soon find workingvectors values derived event frequencies; is, dealing VSMs.survey, organized past work VSMs according structurematrix (termdocument, wordcontext, pairpattern). believe structurematrix important factor determining types applicationspossible. linguistic processing (Section 3) mathematical processing (Section 4)play smaller (but important) roles.goal survey show breadth power VSMs, introduceVSMs less familiar them, provide new perspective VSMsalready familiar them. hope emphasis structurematrix inspire new research. reason believe three matrixtypes present exhaust possibilities. expect new matrix types new tensorsopen applications VSMs. seems possible us semanticshuman language might one day captured kind VSM.175fiTurney & PantelAcknowledgmentsThanks Annie Zaenen prompting paper. Thanks Saif Mohammad MarianaSoffer comments. Thanks Arkady Borkovsky Eric Crestan developingdistributed sparse-matrix multiplication algorithm, Marco Pennacchiottiinvaluable comments. Thanks anonymous reviewers JAIR helpfulcomments suggestions.ReferencesAcar, E., & Yener, B. (2009). Unsupervised multiway data analysis: literature survey.IEEE Transactions Knowledge Data Engineering, 21 (1), 620.Agirre, E., & Edmonds, P. G. (2006). Word Sense Disambiguation: Algorithms Applications. Springer.Ando, R. K. (2000). Latent semantic space: Iterative scaling improves precision interdocument similarity measurement. Proceedings 23rd Annual ACM SIGIRConference Research Development Information Retrieval (SIGIR-2000), pp.216223.Ando, R. K., & Zhang, T. (2005). framework learning predictive structuresmultiple tasks unlabeled data. Journal Machine Learning Research, 6, 18171853.Baeza-Yates, R., & Ribeiro-Neto, B. (1999). Modern Information Retrieval. Addison Wesley.Barr, C., Jones, R., & Regelson, M. (2008). linguistic structure English websearch queries. Conference Empirical Methods Natural Language Processing(EMNLP).Bayardo, R. J., Ma, Y., & Srikant, R. (2007). Scaling pairs similarity search.Proceedings 16th international conference World Wide Web (WWW 07),pp. 131140, New York, NY. ACM.Bicici, E., & Yuret, D. (2006). Clustering word pairs answer analogy questions.Proceedings Fifteenth Turkish Symposium Artificial Intelligence NeuralNetworks (TAINN 2006), Akyaka, Mugla, Turkey.Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. JournalMachine Learning Research, 3, 9931022.Brand, M. (2006). Fast low-rank modifications thin singular value decomposition.Linear Algebra Applications, 415 (1), 2030.Breese, J., Heckerman, D., & Kadie, C. (1998). Empirical analysis predictive algorithmscollaborative filtering. Proceedings 14th Conference UncertaintyArtificial Intelligence, pp. 4352. Morgan Kaufmann.Brin, S., & Page, L. (1998). anatomy large-scale hypertextual Web search engine.Proceedings Seventh World Wide Web Conference (WWW7), pp. 107117.Broder, A. (1997). resemblance containment documents. CompressionComplexity Sequences (SEQUENCES97), pp. 2129. IEEE Computer Society.176fiFrom Frequency MeaningBudanitsky, A., & Hirst, G. (2001). Semantic distance WordNet: experimental,application-oriented evaluation five measures. Proceedings WorkshopWordNet Lexical Resources, Second Meeting North AmericanChapter Association Computational Linguistics (NAACL-2001), pp. 2924,Pittsburgh, PA.Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures semantic distance. Computational Linguistics, 32 (1), 1347.Bullinaria, J., & Levy, J. (2007). Extracting semantic representations word cooccurrence statistics: computational study. Behavior Research Methods, 39 (3),510526.Buntine, W., & Jakulin, A. (2006). Discrete component analysis. Subspace, LatentStructure Feature Selection: Statistical Optimization Perspectives WorkshopSLSFS 2005, pp. 133, Bohinj, Slovenia. Springer.Cafarella, M. J., Banko, M., & Etzioni, O. (2006). Relational web search. Tech. rep., University Washington, Department Computer Science Engineering. TechnicalReport 2006-04-02.Cao, G., Nie, J.-Y., & Bai, J. (2005). Integrating word relationships language models.Proceedings 28th Annual International ACM SIGIR Conference ResearchDevelopment Information Retrieval (SIGIR 05), pp. 298305, New York, NY.ACM.Cao, H., Jiang, D., Pei, J., He, Q., Liao, Z., Chen, E., & Li, H. (2008). Context-aware querysuggestion mining click-through session data. Proceeding 14th ACMSIGKDD International Conference Knowledge Discovery Data Mining (KDD08), pp. 875883. ACM.Carroll, J. D., & Chang, J.-J. (1970). Analysis individual differences multidimensionalscaling via n-way generalization Eckart-Young decomposition. Psychometrika,35 (3), 283319.Chang, W., Pantel, P., Popescu, A.-M., & Gabrilovich, E. (2009). Towards intent-drivenbidterm suggestion. Proceedings WWW-09 (Short Paper), Madrid, Spain.Charikar, M. S. (2002). Similarity estimation techniques rounding algorithms. Proceedings thiry-fourth annual ACM symposium Theory computing (STOC02), pp. 380388. ACM.Chew, P., Bader, B., Kolda, T., & Abdelali, A. (2007). Cross-language information retrieval using PARAFAC2. Proceedings 13th ACM SIGKDD InternationalConference Knowledge Discovery Data Mining (KDD07), pp. 143152. ACMPress.Chiarello, C., Burgess, C., Richards, L., & Pollock, A. (1990). Semantic associativepriming cerebral hemispheres: words do, words dont . . . sometimes,places. Brain Language, 38, 75104.Chklovski, T., & Pantel, P. (2004). VerbOcean: Mining web fine-grained semanticverb relations. Proceedings Experimental Methods Natural Language Processing 2004 (EMNLP-04), pp. 3340, Barcelona, Spain.177fiTurney & PantelChoi, F. Y. Y. (2000). Advances domain independent linear text segmentation.Proceedings 1st Meeting North American Chapter AssociationComputational Linguistics, pp. 2633.Chu-carroll, J., & Carpenter, B. (1999). Vector-based natural language call routing. Computational Linguistics, 25 (3), 361388.Church, K. (1995). One term two?. Proceedings 18th Annual InternationalACM SIGIR Conference Research Development Information Retrieval, pp.310318.Church, K., & Hanks, P. (1989). Word association norms, mutual information, lexicography. Proceedings 27th Annual Conference Association Computational Linguistics, pp. 7683, Vancouver, British Columbia.Clark, S., & Pulman, S. (2007). Combining symbolic distributional models meaning.Proceedings AAAI Spring Symposium Quantum Interaction, pp. 5255.Collobert, R., & Weston, J. (2008). unified architecture natural language processing:Deep neural networks multitask learning. Proceedings 25th InternationalConference Machine Learning (ICML-08), pp. 160167.Croft, W. B. (1977). Clustering large files documents using single-link method.Journal American Society Information Science, 28 (6), 341344.Crouch, C. J. (1988). cluster-based approach thesaurus construction. Proceedings11th Annual International ACM SIGIR Conference, pp. 309320, Grenoble,France.Curran, J. R., & Moens, M. (2002). Improvements automatic thesaurus extraction.Unsupervised Lexical Acquisition: Proceedings Workshop ACL SpecialInterest Group Lexicon (SIGLEX), pp. 5966, Philadelphia, PA.Cutting, D. R., Karger, D. R., Pedersen, J. O., & Tukey, J. W. (1992). Scatter/gather:cluster-based approach browsing large document collections. Proceedings15th Annual International ACM SIGIR Conference, pp. 318329.Dagan, I., Lee, L., & Pereira, F. C. N. (1999). Similarity-based models word cooccurrenceprobabilities. Machine Learning, 34 (13), 4369.Dang, H. T., Lin, J., & Kelly, D. (2006). Overview TREC 2006 question answeringtrack. Proceedings Fifteenth Text REtrieval Conference (TREC 2006).Dasarathy, B. (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques.IEEE Computer Society Press.Davidov, D., & Rappoport, A. (2008). Unsupervised discovery generic relationships usingpattern clusters evaluation automatically generated SAT analogy questions.Proceedings 46th Annual Meeting ACL HLT (ACL-HLT-08), pp.692700, Columbus, Ohio.Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified data processing large clusters.Communications ACM, 51 (1), 107113.178fiFrom Frequency MeaningDeerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.(1990). Indexing latent semantic analysis. Journal American SocietyInformation Science (JASIS), 41 (6), 391407.Elsayed, T., Lin, J., & Oard, D. (2008). Pairwise document similarity large collectionsmapreduce. Proceedings Association Computational LinguisticsHuman Language Technology Conference 2008 (ACL-08: HLT), Short Papers, pp.265268, Columbus, Ohio. Association Computational Linguistics.Erk, K. (2007). simple, similarity-based model selectional preferences. Proceedings45th Annual Meeting Association Computational Linguistics, pp. 216223,, Prague, Czech Republic.Erk, K., & Pado, S. (2008). structured vector space model word meaning context.Proceedings 2008 Conference Empirical Methods Natural LanguageProcessing (EMNLP-08), pp. 897906, Honolulu, HI.Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies LinguisticAnalysis, pp. 132. Blackwell, Oxford.Foltz, P. W., Laham, D., & Landauer, T. K. (1999). intelligent essay assessor: Applications educational technology. Interactive Multimedia Electronic JournalComputer-Enhanced Learning, 1 (2).Forman, G. (2003). extensive empirical study feature selection metrics text classification. Journal Machine Learning Research, 3, 12891305.French, R. M., & Labiouse, C. (2002). Four problems extracting human semanticslarge text corpora. Proceedings 24th Annual Conference CognitiveScience Society.Furnas, G. W., Landauer, T. K., Gomez, L. M., & Dumais, S. T. (1983). Statistical semantics: Analysis potential performance keyword information systems. BellSystem Technical Journal, 62 (6), 17531806.Gentner, D. (1983). Structure-mapping: theoretical framework analogy. CognitiveScience, 7 (2), 155170.Gilbert, J. R., Moler, C., & Schreiber, R. (1992). Sparse matrices MATLAB: Designimplementation. SIAM Journal Matrix Analysis Applications, 13 (1), 333356.Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). Semeval2007 task 04: Classification semantic relations nominals. ProceedingsFourth International Workshop Semantic Evaluations (SemEval 2007), pp.1318, Prague, Czech Republic.Gleich, D., & Zhukov, L. (2004). SVD based term suggestion ranking system.Proceedings Fourth IEEE International Conference Data Mining (ICDM04), pp. 391394. IEEE Computer Society.Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). JohnsHopkins University Press, Baltimore, MD.179fiTurney & PantelGorman, J., & Curran, J. R. (2006). Scaling distributional similarity large corpora.Proceedings 21st International Conference Computational Linguistics44th annual meeting Association Computational Linguistics (ACL 2006),pp. 361368. Association Computational Linguistics.Gorrell, G. (2006). Generalized Hebbian algorithm incremental singular value decomposition natural language processing. Proceedings 11th ConferenceEuropean Chapter Association Computational Linguistics (EACL-06), pp.97104.Gospodnetic, O., & Hatcher, E. (2004). Lucene Action. Manning Publications.Grefenstette, G. (1994). Explorations Automatic Thesaurus Discovery. Kluwer.Harris, Z. (1954). Distributional structure. Word, 10 (23), 146162.Harshman, R. (1970). Foundations parafac procedure: Models conditionsexplanatory multi-modal factor analysis. UCLA Working Papers Phonetics, 16.Hearst, M. (1997). Texttiling: Segmenting text multi-paragraph subtopic passages.Computational Linguistics, 23 (1), 3364.Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detectioncorrection malapropisms. Fellbaum, C. (Ed.), WordNet: ElectronicLexical Database, pp. 305332. MIT Press.Hofmann, T. (1999). Probabilistic Latent Semantic Indexing. Proceedings 22ndAnnual ACM Conference Research Development Information Retrieval (SIGIR 99), pp. 5057, Berkeley, California.Huang, C.-K., Chien, L.-F., & Oyang, Y.-J. (2003). Relevant term suggestion interactiveweb search based contextual information query session logs. JournalAmerican Society Information Science Technology, 54 (7), 638649.Hull, D. (1996). Stemming algorithms: case study detailed evaluation. JournalAmerican Society Information Science, 47 (1), 7084.Jain, A., Murty, N., & Flynn, P. (1999). Data clustering: review. ACM ComputingSurveys, 31 (3), 264323.Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus semantic similarity.Proceedings International Conference Recent Advances Natural LanguageProcessing (RANLP-03), pp. 212219, Borovets, Bulgaria.Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statisticslexical taxonomy. Proceedings International Conference ResearchComputational Linguistics (ROCLING X), pp. 1933, Tapei, Taiwan.Johnson, H., & Martin, J. (2003). Unsupervised learning morphology EnglishInuktitut. Proceedings HLT-NAACL 2003, pp. 4345.Jones, M. P., & Martin, J. H. (1997). Contextual spelling correction using latent semantic analysis. Proceedings Fifth Conference Applied Natural LanguageProcessing, pp. 166173, Washington, DC.180fiFrom Frequency MeaningJones, R., Rey, B., Madani, O., & Greiner, W. (2006). Generating query substitutions.Proceedings 15th international conference World Wide Web (WWW 06),pp. 387396, New York, NY. ACM.Jones, W. P., & Furnas, G. W. (1987). Pictures relevance: geometric analysissimilarity measures. Journal American Society Information Science, 38 (6),420442.Kanerva, P. (1993). Sparse distributed memory related models. Hassoun, M. H.(Ed.), Associative neural memories, pp. 5076. Oxford University Press, New York,NY.Karlgren, J., & Sahlgren, M. (2001). words understanding. Uesaka, Y., Kanerva,P., & Asoh, H. (Eds.), Foundations Real-World Intelligence, pp. 294308. CSLIPublications.Kim, S.-M., Pantel, P., Chklovski, T., & Pennacchiotti, M. (2006). Automatically assessingreview helpfulness. Proceedings 2006 Conference Empirical MethodsNatural Language Processing, pp. 423430.Kolda, T., & Bader, B. (2009). Tensor decompositions applications. SIAM Review,51 (3), 455500.Konchady, M. (2008). Building Search Applications: Lucene, LingPipe, Gate. MustruPublishing.Kraaij, W., & Pohlmann, R. (1996). Viewing stemming recall enhancement. Proceedings 19th Annual International ACM SIGIR Conference, pp. 4048.Lakoff, G. (1987). Women, Fire, Dangerous Things. University Chicago Press,Chicago, IL.Landauer, T. K. (2002). computational basis learning cognition: ArgumentsLSA. Ross, B. H. (Ed.), Psychology Learning Motivation: AdvancesResearch Theory, Vol. 41, pp. 4384. Academic Press.Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latent semantic analysis theory acquisition, induction, representation knowledge.Psychological Review, 104 (2), 211240.Landauer, T. K., & Littman, M. L. (1990). Fully automatic cross-language documentretrieval using latent semantic indexing. Proceedings Sixth Annual ConferenceUW Centre New Oxford English Dictionary Text Research, pp. 3138, Waterloo, Ontario.Landauer, T. K., McNamara, D. S., Dennis, S., & Kintsch, W. (2007). Handbook LatentSemantic Analysis. Lawrence Erlbaum, Mahwah, NJ.Lavrenko, V., & Croft, W. B. (2001). Relevance based language models. Proceedings24th Annual International ACM SIGIR Conference Research DevelopmentInformation Retrieval (SIGIR 01), pp. 120127, New York, NY. ACM.Leacock, C., & Chodrow, M. (1998). Combining local context WordNet similarityword sense identification. Fellbaum, C. (Ed.), WordNet: Electronic LexicalDatabase. MIT Press.181fiTurney & PantelLeacock, C., Towell, G., & Voorhees, E. (1993). Corpus-based statistical sense resolution.Proceedings ARPA Workshop Human Language Technology, pp. 260265.Lee, D. D., & Seung, H. S. (1999). Learning parts objects nonnegative matrixfactorization. Nature, 401, 788791.Lee, L. (1999). Measures distributional similarity. Proceedings 37th AnnualMeeting Association Computational Linguistics, pp. 2532.Lemaire, B., & Denhiere, G. (2006). Effects high-order co-occurrences word semanticsimilarity. Current Psychology Letters: Behaviour, Brain & Cognition, 18 (1).Lin, D. (1998). Automatic retrieval clustering similar words. roceedings17th international conference Computational linguistics, pp. 768774. AssociationComputational Linguistics.Lin, D., & Pantel, P. (2001). DIRT discovery inference rules text. ProceedingsACM SIGKDD Conference Knowledge Discovery Data Mining 2001, pp.323328.Linden, G., Smith, B., & York, J. (2003). Amazon.com recommendations: Item-to-itemcollaborative filtering. IEEE Internet Computing, 7680.Liu, X., & Croft, W. B. (2005). Statistical language modeling information retrieval.Annual Review Information Science Technology, 39, 328.Lovins, J. B. (1968). Development stemming algorithm. Mechanical TranslationComputational Linguistics, 11, 2231.Lowe, W. (2001). Towards theory semantic space. Proceedings Twenty-firstAnnual Conference Cognitive Science Society, pp. 576581.Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces lexicalco-occurrence. Behavior Research Methods, Instruments, Computers, 28 (2), 203208.Lund, K., Burgess, C., & Atchley, R. A. (1995). Semantic associative priming highdimensional semantic space. Proceedings 17th Annual ConferenceCognitive Science Society, pp. 660665.Manning, C., & Schutze, H. (1999). Foundations Statistical Natural Language Processing.MIT Press, Cambridge, MA.Manning, C. D., Raghavan, P., & Schutze, H. (2008). Introduction Information Retrieval.Cambridge University Press, Cambridge, UK.Miller, G., Leacock, C., Tengi, R., & Bunker, R. (1993). semantic concordance.Proceedings 3rd DARPA Workshop Human Language Technology, pp. 303308.Minnen, G., Carroll, J., & Pearce, D. (2001). Applied morphological processing English.Natural Language Engineering, 7 (3), 207223.Mitchell, J., & Lapata, M. (2008). Vector-based models semantic composition. Proceedings ACL-08: HLT, pp. 236244, Columbus, Ohio. Association ComputationalLinguistics.182fiFrom Frequency MeaningMitchell, T. (1997). Machine Learning. McGraw-Hill, Columbus, OH.Mohammad, S., & Hirst, G. (2006). Distributional measures concept-distance: taskoriented evaluation. Proceedings Conference Empirical Methods NaturalLanguage Processing (EMNLP-2006), pp. 3543.Monay, F., & Gatica-Perez, D. (2003). image auto-annotation latent space models.Proceedings Eleventh ACM International Conference Multimedia, pp.275278.Morris, J., & Hirst, G. (2004). Non-classical lexical semantic relations. WorkshopComputational Lexical Semantics, HLT-NAACL-04, Boston, MA.Nakov, P., & Hearst, M. (2007). UCB: System description SemEval Task 4. Proceedings Fourth International Workshop Semantic Evaluations (SemEval 2007),pp. 366369, Prague, Czech Republic.Nakov, P., & Hearst, M. (2008). Solving relational similarity problems using thewebcorpus. Proceedings ACL-08: HLT, pp. 452460, Columbus, Ohio.Nastase, V., Sayyad-Shirabad, J., Sokolova, M., & Szpakowicz, S. (2006). Learning nounmodifier semantic relations corpus-based WordNet-based features. Proceedings 21st National Conference Artificial Intelligence (AAAI-06), pp.781786.Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations.Fifth International Workshop Computational Semantics (IWCS-5), pp. 285301,Tilburg, Netherlands.Niwa, Y., & Nitta, Y. (1994). Co-occurrence vectors corpora vs. distance vectorsdictionaries. Proceedings 15th International Conference ComputationalLinguistics, pp. 304309, Kyoto, Japan.Nosofsky, R. (1986). Attention, similarity, identification-categorization relationship.Journal Experimental Psychology: General, 115 (1), 3957.Ogden, C. K. (1930). Basic English: General Introduction Rules Grammar.Kegan Paul, Trench, Trubner Co.Pado, S., & Lapata, M. (2003). Constructing semantic space models parsed corpora.Proceedings 41st Annual Meeting Association Computational Linguistics, pp. 128135, Sapporo, Japan.Pado, S., & Lapata, M. (2007). Dependency-based construction semantic space models.Computational Linguistics, 33 (2), 161199.Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? sentiment classification usingmachine learning techniques. Proceedings Conference Empirical MethodsNatural Language Processing (EMNLP), pp. 7986, Philadelphia, PA.Pantel, P. (2005). Inducing ontological co-occurrence vectors. Proceedings AssociationComputational Linguistics (ACL-05), pp. 125132.Pantel, P., & Lin, D. (1998). Spamcop: spam classification organization program.Learning Text Categorization: Papers AAAI 1998 Workshop, pp. 9598.183fiTurney & PantelPantel, P., & Lin, D. (2002a). Discovering word senses text. ProceedingsEighth ACM SIGKDD International Conference Knowledge Discovery DataMining, pp. 613619, Edmonton, Canada.Pantel, P., & Lin, D. (2002b). Document clustering committees. Proceedings25th Annual International ACM SIGIR Conference, pp. 199206.Pasca, M., Lin, D., Bigham, J., Lifchits, A., & Jain, A. (2006). Names similaritiesWeb: Fact extraction fast lane. Proceedings 21st InternationalConference Computational Linguistics 44th Annual Meeting ACL, pp.809816, Sydney, Australia.Patwardhan, S., & Pedersen, T. (2006). Using wordnet-based context vectors estimatesemantic relatedness concepts. Proceedings Workshop MakingSense Sense 11th Conference European Chapter AssociationComputational Linguistics (EACL-2006), pp. 18.Pedersen, T. (2006). Unsupervised corpus-based methods WSD. Word Sense Disambiguation: Algorithms Applications, pp. 133166. Springer.Pennacchiotti, M., Cao, D. D., Basili, R., Croce, D., & Roth, M. (2008). Automatic inductionFrameNet lexical units. Proceedings 2008 Conference Empirical MethodsNatural Language Processing (EMNLP-08), pp. 457465, Honolulu, Hawaii.Pennacchiotti, M., & Pantel, P. (2006). Ontologizing semantic relations. Proceedings21st International Conference Computational Linguistics 44th annualmeeting Association Computational Linguistics, pp. 793800. AssociationComputational Linguistics.Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering English words.Proceedings 31st Annual Meeting Association Computational Linguistics,pp. 183190.Porter, M. (1980). algorithm suffix stripping. Program, 14 (3), 130137.Rabin, M. O. (1981). Fingerprinting random polynomials. Tech. rep., Center researchComputing technology, Harvard University. Technical Report TR-15-81.Rapp, R. (2003). Word sense discovery based sense descriptor dissimilarity. Proceedings Ninth Machine Translation Summit, pp. 315322.Ravichandran, D., Pantel, P., & Hovy, E. (2005). Randomized algorithms nlp: usinglocality sensitive hash function high speed noun clustering. Proceedings43rd Annual Meeting Association Computational Linguistics (ACL 05), pp.622629, Morristown, NJ. Association Computational Linguistics.Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., & Riedl, J. (1994). Grouplens: openarchitecture collaborative filtering netnews. Proceedings ACM 1994Conference Computer Supported Cooperative Work, pp. 175186. ACM Press.Resnik, P. (1995). Using information content evaluate semantic similarity taxonomy.Proceedings 14th International Joint Conference Artificial Intelligence(IJCAI-95), pp. 448453, San Mateo, CA. Morgan Kaufmann.184fiFrom Frequency MeaningRosario, B., & Hearst, M. (2001). Classifying semantic relations noun-compoundsvia domain-specific lexical hierarchy. Proceedings 2001 ConferenceEmpirical Methods Natural Language Processing (EMNLP-01), pp. 8290.Rosario, B., Hearst, M., & Fillmore, C. (2002). descent hierarchy, selectionrelational semantics. Proceedings 40th Annual Meeting AssociationComputational Linguistics (ACL-02), pp. 247254.Rosch, E., & Lloyd, B. (1978). Cognition Categorization. Lawrence Erlbaum, Hillsdale,NJ.Ruge, G. (1997). Automatic detection thesaurus relations information retrieval applications. Freksa, C., Jantzen, M., & Valk, R. (Eds.), Foundations ComputerScience, pp. 499506. Springer.Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998). Bayesian approachfiltering junk e-mail. Proceedings AAAI-98 Workshop Learning TextCategorization.Sahlgren, M. (2005). introduction random indexing. Proceedings MethodsApplications Semantic Indexing Workshop 7th International ConferenceTerminology Knowledge Engineering (TKE), Copenhagen, Denmark.Sahlgren, M. (2006). Word-Space Model: Using distributional analysis represent syntagmatic paradigmatic relations words high-dimensional vector spaces.Ph.D. thesis, Department Linguistics, Stockholm University.Salton, G. (1971). SMART retrieval system: Experiments automatic document processing. Prentice-Hall, Upper Saddle River, NJ.Salton, G., & Buckley, C. (1988). Term-weighting approaches automatic text retrieval.Information Processing Management, 24 (5), 513523.Salton, G., Wong, A., & Yang, C.-S. (1975). vector space model automatic indexing.Communications ACM, 18 (11), 613620.Sarawagi, S., & Kirpal, A. (2004). Efficient set joins similarity predicates. Proceedings 2004 ACM SIGMOD International Conference Management Data(SIGMOD 04), pp. 743754, New York, NY. ACM.Scholkopf, B., Smola, A. J., & Muller, K.-R. (1997). Kernel principal component analysis.Proceedings International Conference Artificial Neural Networks (ICANN1997), pp. 583588, Berlin.Schutze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24 (1),97124.Schutze, H., & Pedersen, J. (1993). vector model syntagmatic paradigmaticrelatedness. Making Sense Words: Proceedings Conference, pp. 104113,Oxford, England.Sebastiani, F. (2002). Machine learning automated text categorization. ACM ComputingSurveys (CSUR), 34 (1), 147.Shannon, C. (1948). mathematical theory communication. Bell System TechnicalJournal, 27, 379423, 623656.185fiTurney & PantelSinghal, A., Salton, G., Mitra, M., & Buckley, C. (1996). Document length normalization.Information Processing Management, 32 (5), 619633.Smith, E., Osherson, D., Rips, L., & Keane, M. (1988). Combining prototypes: selectivemodification model. Cognitive Science, 12 (4), 485527.Snow, R., Jurafsky, D., & Ng, A. Y. (2006). Semantic taxonomy induction heterogenous evidence. Proceedings 21st International Conference ComputationalLinguistics 44th annual meeting ACL, pp. 801808.Sparck Jones, K. (1972). statistical interpretation term specificity applicationretrieval. Journal Documentation, 28 (1), 1121.Spearman, C. (1904). General intelligence, objectively determined measured. American Journal Psychology, 15, 201293.Sproat, R., & Emerson, T. (2003). first international Chinese word segmentation bakeoff. Proceedings Second SIGHAN Workshop Chinese Language Processing,pp. 133143, Sapporo, Japan.Stone, P. J., Dunphy, D. C., Smith, M. S., & Ogilvie, D. M. (1966). General Inquirer:Computer Approach Content Analysis. MIT Press, Cambridge, MA.Tan, B., & Peng, F. (2008). Unsupervised query segmentation using generative languagemodels Wikipedia. Proceeding 17th international conference WorldWide Web (WWW 08), pp. 347356, New York, NY. ACM.Tellex, S., Katz, B., Lin, J., Fern, A., & Marton, G. (2003). Quantitative evaluationpassage retrieval algorithms question answering. Proceedings 26th AnnualInternational ACM SIGIR Conference Research Development InformationRetrieval (SIGIR), pp. 4147.Tucker, L. R. (1966). mathematical notes three-mode factor analysis. Psychometrika, 31 (3), 279311.Turney, P. D. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL.Proceedings Twelfth European Conference Machine Learning (ECML-01),pp. 491502, Freiburg, Germany.Turney, P. D. (2005). Measuring semantic similarity latent relational analysis. Proceedings Nineteenth International Joint Conference Artificial Intelligence(IJCAI-05), pp. 11361141, Edinburgh, Scotland.Turney, P. D. (2006). Similarity semantic relations. Computational Linguistics, 32 (3),379416.Turney, P. D. (2007). Empirical evaluation four tensor decomposition algorithms. Tech.rep., Institute Information Technology, National Research Council Canada.Technical Report ERB-1152.Turney, P. D. (2008a). latent relation mapping engine: Algorithm experiments.Journal Artificial Intelligence Research, 33, 615655.Turney, P. D. (2008b). uniform approach analogies, synonyms, antonyms, associations. Proceedings 22nd International Conference ComputationalLinguistics (Coling 2008), pp. 905912, Manchester, UK.186fiFrom Frequency MeaningTurney, P. D., & Littman, M. L. (2003). Measuring praise criticism: Inferencesemantic orientation association. ACM Transactions Information Systems,21 (4), 315346.Turney, P. D., & Littman, M. L. (2005). Corpus-based learning analogies semanticrelations. Machine Learning, 60 (13), 251278.Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independentmodules solve multiple-choice synonym analogy problems. ProceedingsInternational Conference Recent Advances Natural Language Processing(RANLP-03), pp. 482489, Borovets, Bulgaria.Van de Cruys, T. (2009). non-negative tensor factorization model selectional preferenceinduction. Proceedings Workshop Geometric Models Natural LanguageSemantics (GEMS-09), pp. 8390, Athens, Greece.van Rijsbergen, C. J. (2004). Geometry Information Retrieval. Cambridge UniversityPress, Cambridge, UK.van Rijsbergen, C. J. (1979). Information Retrieval. Butterworths.Veale, T. (2003). analogical thesaurus. Proceedings 15th Innovative Applications Artificial Intelligence Conference (IAAI 2003), pp. 137142, Acapulco,Mexico.Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy.Proceedings 16th European Conference Artificial Intelligence (ECAI 2004),pp. 606612, Valencia, Spain.Vozalis, E., & Margaritis, K. (2003). Analysis recommender systems algorithms.Proceedings 6th Hellenic European Conference Computer MathematicsApplications (HERCMA-2003), Athens, Greece.Vyas, V., & Pantel, P. (2009). Semi-automatic entity set refinement. ProceedingsNAACL-09, Boulder, CO.Weaver, W. (1955). Translation. Locke, W., & Booth, D. (Eds.), Machine TranslationLanguages: Fourteen Essays. MIT Press, Cambridge, MA.Weeds, J., Weir, D., & McCarthy, D. (2004). Characterising measures lexical distributional similarity. Proceedings 20th International Conference Computational Linguistics (COLING 04), pp. 10151021. Association ComputationalLinguistics.Wei, X., Peng, F., & Dumoulin, B. (2008). Analyzing web text association disambiguateabbreviation queries. Proceedings 31st Annual International ACM SIGIRConference Research Development Information Retrieval (SIGIR 08), pp.751752, New York, NY. ACM.Wen, J.-R., Nie, J.-Y., & Zhang, H.-J. (2001). Clustering user queries search engine.Proceedings 10th International Conference World Wide Web (WWW 01),pp. 162168, New York, NY. ACM.Widdows, D. (2004). Geometry Meaning. Center Study LanguageInformation, Stanford, CA.187fiTurney & PantelWiddows, D., & Ferraro, K. (2008). Semantic vectors: scalable open source packageonline technology management application. Proceedings Sixth InternationalConference Language Resources Evaluation (LREC 2008), pp. 11831190.Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning ToolsTechniques Java Implementations. Morgan Kaufmann, San Francisco.Wittgenstein, L. (1953). Philosophical Investigations. Blackwell. Translated G.E.M.Anscombe.Wolfe, M. B. W., Schreiner, M. E., Rehder, B., Laham, D., Foltz, P. W., Kintsch, W., &Landauer, T. K. (1998). Learning text: Matching readers texts latentsemantic analysis. Discourse Processes, 25, 309336.Yang, Y. (1999). evaluation statistical approaches text categorization. InformationRetrieval, 1 (1), 6990.Yuret, D., & Yatbaz, M. A. (2009). noisy channel model unsupervised word sensedisambiguation. Computational Linguistics. review.Zamir, O., & Etzioni, O. (1999). Grouper: dynamic clustering interface Web searchresults. Computer Networks: International Journal Computer Telecommunications Networking, 31 (11), 13611374.Zhao, Y., & Karypis, G. (2002). Evaluation hierarchical clustering algorithms document datasets. Proceedings Eleventh International Conference Information Knowledge Management, pp. 515524, McLean, Virginia.188fiJournal Artificial Intelligence Research 37 (2010) 437477Submitted 8/09; published 3/10Reasoning Transfer ControlWiebe van der HoekDirk WaltherMichael WooldridgeW IEBE .VAN -D ER -H OEK @ LIV. AC . UKDWALTHER @ LIV. AC . UKMJW @ LIV. AC . UKDepartment Computer ScienceUniversity Liverpool, UKAbstractpresent DCL - PC: logic reasoning abilities agents coalitionsagents altered transferring control one agent another. logical foundationDCL - PC CL - PC, logic reasoning cooperation abilities agentscoalitions agents stem distribution atomic Boolean variables individual agentschoices available coalition correspond assignments variables coalition controls.basic modal constructs CL - PC form coalition C cooperate bring .DCL - PC extends CL - PC dynamic logic modalities atomic programs formagent gives control variable p agent j; usual dynamic logic, atomic programsmay combined using sequence, iteration, choice, test operators form complex programs.combining dynamic transfer programs cooperation modalities, becomes possiblereason power agents coalitions affected transfer control. givetwo alternative semantics logic: direct semantics, capture distributionsBoolean variables agents; conventional Kripke semantics. provesemantics equivalent, present axiomatization logic. investigatecomputational complexity model checking satisfiability DCL - PC, showproblems PSPACE-complete (and hence worse underlying logic CL - PC). Finally,investigate characterisation control DCL - PC. distinguish first-order controlability agent coalition control state affairs assignment valuesvariables control agent coalition second-order control abilityagent exert control control agents transferring variablesagents. give logical characterisation second-order control.1. Introductionrecent years, much activity development logics reasoningstrategic cooperative abilities agents game-like multi-agent systems. Coalition Logic (Pauly,2001) Alternating-time Temporal Logic (ATL) Alur, Henzinger, Kupferman (2002)perhaps best-known examples work. logics widely used baseinvestigate reasoning cooperation multi-agent systems (van der Hoek &Wooldridge, 2003; Jamroga & van der Hoek, 2004; Goranko & Jamroga, 2004).Although differ details, basic construct Coalition Logic ATLcooperation modality, construct written ATL hhCii. intended meaningexpression coalition C cooperate way ensure that, matteragents outside C do, property becomes true. Another way think hhCiimeaning coalition C collective power ensure . often assumed powersadditive, sense powers coalition derive powers coalition members,c2010AI Access Foundation. rights reserved.fiVAN DERH OEK , WALTHER , & W OOLDRIDGEadding agent coalition reduce powers coalition. However,origin individual agents powers is, powers derive rarely discussedcooperation logic literature.One natural interpretation powers abilities computational systems arisesconsidering system components ability assign values variables makingoverall system state. Power, sense, equates ability choose valueparticular variable. Motivated observation, van der Hoek Wooldridge developed CL - PC,cooperation logic powers specified allocating every agent set Booleanvariables: choices (and hence powers) available coalition correspond possibleassignments truth falsity may made variables control (van der Hoek& Wooldridge, 2005b). CL - PC expression C means coalition C assign valuesvariables control way make true. Van der Hoek Wooldridgegave complete axiomatization CL - PC, showed model checking satisfiabilityproblems logic PSPACE-complete; also investigated CL - PC could usedcharacterise closely related notion control. However, one drawback CL - PCpower structure underpinning logic distribution variables agents assumedfixed, hence coalitional powers static CL - PC.Ultimately, course, assumption powers static realistic. example,explicit transfer power control fundamental component human organisations,enabling avoid bottlenecks respect centralised power control. Moreover,open environments, agents join leave system run-time, may possibleknow advance agents fulfill roles, static power allocation schemessimply appropriate environments. software agents deployed environmentspower structures dynamic, important consider issues representingreasoning them, issue address present paper.study variant CL - PC allows us explicitly reason dynamic power structures.logic DCL - PC extends CL - PC dynamic logic operators (Harel, Kozen, & Tiuryn, 2000),atomic programs form ;p j, read agent gives control variablep agent j. pre-condition program variable p agent allocationvariables, executing program effect transferring variable p agent agentj. Thus dynamic component DCL - PC concerned transferring power systems,using logic, reason abilities agents coalitions affectedtransfers. Note that, conventional dynamic logic, atomic programs may combinedDCL - PC usual sequential composition (;), non-deterministic choice (), test (?),iteration ( ) operations, form complex programs. features, DCL - PC providesrich framework represent reason systems power/controldynamically allocated.remainder paper, following introduction logic, make four main contributions respect DCL - PC:First, Section 2, give two alternative semantics logic: direct semantics,models directly represent allocation propositional variables agentscontrol them, conventional Kripke semantics. prove two semanticsequivalent.438fiR EASONING BOUT RANSFER C ONTROLSecond, give axiomatization DCL - PC Section 3, show axiomatizationsound complete (with respect semantics).Third, show Section 4 that, despite apparently additional expressive power provideddynamic component DCL - PC, satisfiability model checking problemsDCL - PC complex corresponding problems CL - PC (van der Hoek &Wooldridge, 2005b): PSPACE-complete.Fourth, distinguish first-order control second-order control Section 5.first-order control, introduced studied van der Hoek Wooldridge (2005b),ability control state affairs assigning values variables, second-ordercontrol ability agent exert control ability agents controlstates affairs. Agents coalitions exercise second-order control transferring variables control agents. informally discussing introducing secondorder control, develop logical characterisation it, sense characteriseformulae agent second-order control.conclude brief comments related work conclusions. Note omitdetailed introduction cooperation logics particular motivation behind CL - PC,done van der Hoek Wooldridge (2005b).2. Logic DCL - PCsection, define logic DCL - PC.2.1 Informal Introductionbegin informal introduction; readers familiar CL - PC dynamiclogic may wish skim skip completely introductory section.noted earlier, DCL - PC extends logic CL - PC,"#####$"#####$begin briefly reviewing logic. CL - PC'*"!intended allow us reason domains containing+(collection agents, collection propositionalvariables; let = {1, . . . , n} denote set agents,,P denote variables. assumed CL - PC"#####$"#####$agent system controls subset vari&%)ables P. keep things simple, assumed agentsexercise unique control: every variable controlled.exactly one agent, variables P partitionedamong agents A. agent, denote variables control Pi , Pi P.Figure 1: typical scenario.abilities powers agent scenario correspond assignments truth falsity make variables control.Figure 1 illustrates typical scenario: four agents, = {1, 2, 3, 4}, eight variables,P = {p, q, r, s, t, u, v, w}. Agent 1 controls variables p q, (P 1 = {p, q}), agent 2 controlsvariable r, (P2 = {r}), on. scenario illustrated, variables p, q, s, w value439fiVAN DERH OEK , WALTHER , & W OOLDRIDGE1 (i.e., true), variables value 0 (false). language CL - PC intended allow us represent reason scenarios. represent values variables,use propositional logic, following formula completely characterises valuesvariables scenario:p q w r u vAgents able change value variablescontrol, represent abilities CL PC use contingent ability operator (van der Hoek& Wooldridge, 2005b): expression C means that,assumption world remains otherwiseunchanged, set agents C modify valuevariables make true. respectscenario Figure 1, example,"-----.""-----.%(&)!*$'+#,1,2 (p r q)."agent 1 leave variable p set truemaking variable q false, agent 2 makes variable r true: result formula p r qtrue.fact matter coalition C do,remain true expressed 2C . scenario Figure 1, matter agent 1 does, r remain false(assuming agent acts). Thus have:%$&!)*$21 r.("%'+#,shown elsewhere (van der Hoek & Wooldridge, 2005b),defined below, types ability operators mayalso defined.Figure 2: effect executingThus far, operators introducedatomic transfer program.part CL - PC language. Let us start introduce dynamic aspects language, specificDCL - PC . First, idea atomic transfer program, written ; p j, meaning agenttransfers power choose truth value variable p agent j. Now, possibleexecute program ;p j iff variable p actually control agent i. example,respect Figure 1, programs 1 ;p 2 2 ;r 1 executable, program 1 ;r 2(since r control 1). (fairly obvious) effect executing program1 ;p 2 illustrated Figure 2; note actual value variable transferredunchanged transfer.DCL - PC, allow atomic programs combined together make complex programsusing constructs dynamic logic: ; (for sequential composition), (iteration), ? (thetest operator). simplest sequential composition: example, program1 ;p 2; 2 ;r 1440fiR EASONING BOUT RANSFER C ONTROLmeans that, first, agent 1 gives variable p 2, then, agent 2 gives r 1. operatornon-deterministic choice operator. 1 2 transfer programs, 1 2 means eitherprogram 1 2 . operator used define iteration: expression means executeprogram zero times (it defined exactly many times executed).Finally, ? used perform tests. program ? executed particular scenarioformula true scenario. illustrate operator work, considerfollowing example programs.p?; 1 ;p 2first program says p true, agent 1 gives p agent 2. Now, since p truescenario Figure 1, program executed scenario Figure 1, netresult final scenario Figure 2.following program uses non-deterministic choice, essentially says agent 1 gives eitherp q 2.(1 ;p 2) (1 ;q 2)usual dynamic logic, define iteration selection constructs used conventional imperative programming languages basic program constructs. example,conventional programming constructdefined using following transfer program construct (see, e.g., Harel et al., 2000):(?; ) ; ?next step see transfer programs incorporated ability constructs CL - PC.able refer transfer programs properties within language DCL - PC,use dynamic operators h [ ]. operators play role DCL - PCplay conventional dynamic logic (Harel et al., 2000). Thus formula h assertsexists computation program , starting current situation,terminated holds. Note h assert guaranteed terminate, merelyleast one terminating computation. moreover, state satisfiedevery terminating computation ; merely terminating computationend situation satisfying . Thus h acts existential quantifier computations. operator [ ] universal quantifier computations . asserts everyterminating computation , property holds. Note assert factterminating computations.example use constructs, following formula asserts agent 1 giveseither p q 2, 2 able achieve (p q) r.[(1 ;p 2) (1 ;q 2)]2 (p q) reasy see formula expresses true property scenario Figure 1: program(1 ;p 2) (1 ;q 2) executable scenario, executed, agent 2 controlvariable r either variable p variable q. Agent 2 thus able make (p q) r true.conclude introductory section, consider following complex example.following DCL - PC formula asserts possible agent give away variables agent j,441fiVAN DERH OEK , WALTHER , & W OOLDRIDGEnon-deterministically choosing one variable time, agent j ability achieve .[hwhile j;p ji>pPi2.2 SyntaxFormally, language DCL - PC formed respect (fixed, finite, non-empty) setagents, (fixed, finite, non-empty) set P propositional variables. Figure 3 defines syntaxDCL - PC. use > logical constant truth, negation, disjunction.usual, define remaining connectives classical propositional logic abbreviations:====>( )( ) ( ).Additionally, set DCL - PC formulas, write 5 mean exactly onemember true:_^(1 2 ).5 =1 6=2= {1 , 2 . . . n }, also write 1 5 2 5 5 n 5 .respect transfer programs, constructs conventional imperative programs maydefined follows (Harel et al., 2000):1 else 2repeatskipfail=====((?; 1 ) (?; 2 ))((?; ) ; ?); (?; ) ; ?>??possibility confusion, omit set brackets cooperation modalities,example writing 1,2 rather {1,2} . DCL - PC formula containing modalities saidobjective formula.Let P() denote set propositional variables occurring DCL - PC formula , let A()denote set agents named (i.e., A() union coalitions occurringcooperation modalities agents occurring transfer programs ).Although operator useful define programs succinctly, fact see Theorem 2superfluous, essentially uses fact set atoms agents finite.2.3 Direct Semanticsintroduce first two semantics DCL - PC. call semantics directsemantics directly based intuitive model introduced earlier: every agentunique control set propositional variables, every variable controlledagent.Given fixed, finite non-empty set agents, fixed, finite non-empty set Ppropositional variables, say allocation P indexed tuple = hP 1 , . . . , Pn i,442fiR EASONING BOUT RANSFER C ONTROLDCL - PCDCLformulas:::= >| p| DCL| DCL DCL| C DCL| h iDCLTransfer programs:::= ;p j| ;||| DCL?/* truth constant *//* propositional variables *//* negation *//* disjunction *//* contingent cooperative ability *//* existential dynamic operator *//* gives p j *//* sequential composition *//* non-deterministic choice *//* iteration *//* test */Figure 3: Syntax DCL - PC: p P propositional variable, C set agents, i, jagents.indexed element Pi A, P1 , . . . , Pn forms partition P(i.e., P = iA Pi Pi Pj = 6= j A). intended interpretation allocation= hP1 , . . . , Pn Pi P set propositional variables agent control.is, agent freedom allocate whatever Boolean values sees fit members P .course, could defined allocation function : P A, (p) denotesagent controlling propositional variable p; seems particular reason preferring onerepresentation rather other, consistency historical record, adoptpartition representation, used van der Hoek Wooldridge (2005b).Now, say model DCL - PC structure:= hA, P, 0 ,where:= {1, . . . , n} finite, non-empty set agents;P = {p, q, . . .} finite, non-empty set propositional variables;0 = hP1 , . . . , Pn initial allocation P A, intended interpretation Psubset P representing variables control agent A; finally,: P {tt, ff} propositional valuation function, determines initial truth valueevery propositional variable.additional notation convenient follows. coalition C subset A, i.e., C A.C A, denote complement C, (i.e., \ C) C. write P C443fiVAN DERH OEK , WALTHER , & W OOLDRIDGEPi . two valuations 0 , set propositional variables P, write = 0(mod ) 0 differ propositional variables , say0 modulo . sometimes understand model consist frameF = hA, P, 0 together propositional valuation function . Given model = hA, P, 0 ,coalition C M, C-valuation function:iCC : PC {tt, ff}.Thus C-valuation propositional valuation function assigns truth values propositional variables controlled members coalition C. = hA, P, 0 , 0 =hP1 . . . , Pn model, C coalition M, C C-valuation, C meanmodel hA, P, 0 , 0 i, 0 valuation function defined follows(C (p) p PC0(p) =(p)otherwiseelements model M. Thus C denotes model identicalexcept values assigned valuation function propositional variables controlledmembers C determined C .define size model = hA, P, 0 , |A| + |P|; denote sizesize(M).2.4 Transfer Program Relationsgive modal semantics dynamic logic constructs DCL - PC, must define, everytransfer program binary relation R models (M1 , M2 ) R iff M2 modelmay result one possible execution program 1 . start defining relationRi;p j , atomic transfer programs form ;p j, i.e., agent gives control propositionalvariable p agent j. Let = hA, P, 0 , M0 = hA0 , P0 , 00 , 0 two models 0 =hP1 , . . . , Pn 00 = hP01 , . . . , P0n i.(M, M0 ) Ri;p jiff1. p Pi (agent controls p begin with)2. case = j:(a) = M0 (agent gives p herself, change model)3. case 6= j:(a) P0i = Pi \ {p} (agent longer controls p afterwards);(b) P0j = Pj {p} (agent j controls p afterwards);(c) components M0 M.444fiR EASONING BOUT RANSFER C ONTROLorder define |=d , means true direct semantics, needable determine interpretation arbitrary program is, M; define below.Notice executing atomic transfer program effect valuation function model.Transfer programs affect distribution propositional variables agents.remaining constructs transfer programs, define program relations inductively,terms relations atomic transfer programs, defined above. Let compositionrelations R1 R2 denoted R1 R2 , reflexive transitive closure (ancestral) relationR R . accessibility relations complex programs defined follows (Harel et al.,2000):R1 ;2 =R 1 R 2R1 2 =R 1 R 2(R )R =R? ={(M, M) | |=d }.Notice last definitions refers relation |=d , course yetdefined. aim next section define relation. emphasise that, althoughrelations R? |=d mutually refer one-another, relations fact well-defined (asconventional dynamic logic).2.5 Truth Conditionsinterpret formulas DCL - PC respect models, introduced above. Given model= hA, P, 0 , formula , write |=d mean satisfied (or, equivalently,true) M, direct semantics. rules defining satisfaction relation |=follows:|=d >|=d p iff (p) = tt(where p P)|=d iff 6|=d|=d iff |=d |=d|=dC iff exists C-valuation C C |=d|=d h iff exists model M0 (M, M0 ) R M0 |=d .say formula objective contains modal constructs (i.e., operators C h i). Thusobjective formulae formulae classical propositional logic.assume conventional definitions satisfiability validity: DCL - PC formula-satisfiable iff exists DCL - PC model |= , -valid iff everyDCL - PC model |=d . write |=d indicate -valid. valid formulaalso called tautology. say feasible satisfiable valid. Finally, setformulas formula , define |=d M( |=d |=d .Let us define box 2 dual cooperative ability modality as:2C =C445fiVAN DERH OEK , WALTHER , & W OOLDRIDGE[] dual transfer modality hi as:[ ] =h i.C coalition formulachoose either true false:DCL - PC ,write controls(C, ) mean Ccontrols(C, ) =C C(1)using controls(, ) construct, capture distribution propositional variablesamong agents model.Lemma 1 Let = hA, P, 0 , modelp P propositional variable M.DCL - PC ,agent, C set agents,1. (van der Hoek & Wooldridge, 2005b) |=d controls(i, p) iff p Pi ;2. |=d controls(C, p) iff p PC .Remark 1 characterize formulas control coalition C? have:feasible objective, |=d^controls(C, p) controls(C, )(2)pP()Observe property (2) true arbitrary DCL - PC formulas. see this, take exampleformula hi ;p ji>, matter whether define P(hi ;p ji>) {p} . |=dcontrols(i, hi ;p ji>): independent owning p, exactly one two formulas hi ; p ji>hi ;p ji> true. is, p Pi iff |=d hi ;p ji>.Also note -direction right hand side (2) valid objective : suppose= hA, P, 0 , (q) = tt, p Pi , q/ Pi . Then, |=d controls(i, pq)(controls(i, p) controls(i, q)): q happens true M, controls conjunctionp q, conjuncts.2.6 Kripke SemanticsAlthough direct semantics naturally captures notions propositional control transfercontrol, purposes establishing completeness particular, relating mainstream modal logic convenient formulate semantics DCL - PC using conventional Kripke structures (Chellas, 1980; Blackburn, de Rijke, & Venema, 2001). idea that,given (fixed, finite, non-empty) set agents (fixed, finite, non-empty) set P propositional variables, possible world every possible allocation variablesP agents every possible propositional valuation function P.worlds, basically two orthogonal accessibility relations (cf. Figure 4): horizontalvertical one. First all, horizontal relation Ri agent two worlds uv agent able, given valuation u u, turn valuation v described v,choosing appropriate values variables. Formally, (u, v) R iff u = v (mod Pi ).is, Ri equivalence relation. follows, drop symbolic distinctionworlds valuations, i.e., use denoting world valuation interchangeably. Notice446fiR EASONING BOUT RANSFER C ONTROL= hP1 , . . . , Pi , Pj , Pk , . . . , Pnp, q, rRip, q, rvuRjp, q, rw;p jM00 = hP1 , . . . , Pi \ {p}, Pj {p}, Pk , . . . , Pnp, q, rRjp, q, rvuRjp, q, rwj ;q kM0000 = hP1 , . . . , P0i , P0j \ {q}, Pk {q}, . . . , Pnp, q, rRjp, q, rvuRkp, q, rwFigure 4: Kripke models DCL - PC.horizontal relation affect allocation : remains unchanged. Let us thereforedefine Kripke models = h, RiA , i, set valuations P.important realize sets agents P variables fixed, allocationsvariables agents may vary. denote set Kripke models K(A, P). callpair (M, ) pointed Kripke model, sometimes omit brackets pair.Secondly, vertical accessibility relation pointed models (M, ) (M 0 , 0 ),= h, RiA , i, M0 = h, RiA , 0 K(A, P), indicate change allocation0 . Since change allocation affect current world, pairs= 0 . Slightly abusing notation, define (M, )(i ;p j)(M0 , 0 ) exactly = 0p Pi , either = j = M0 , else P0i = Pi \ {p} P0j = Pj {p},sets Ph remain same.447fiVAN DERH OEK , WALTHER , & W OOLDRIDGEtruth relation |=K interpreting formulas Kripke structures holds pairsform (M, ) formulas . definition follows (we omit Boolean cases casescomplex transfer programs):M, |=K C iff exists valuation 0 (, 0 ) Ri C,M, 0 |=KM, |=K hi ;p ji iff exists Kripke model M0 (M, )(i ;p j)(M0 , )M0 , |=Kset formulas formula , define |=K (M, )( (M, ) |=KM, |=K ). Figure 4 illustrates Kripke semantics. Note sets P 0i P0j Kripkemodel M0 P0i = Pi \{p} P0j = Pj {p}. Note clause C , two pointedmodels M, M, 0 except atoms PC . special casetwo models similar upto set atoms (French, 2006; Ghilardi & Zawadowski, 2000).Remark 2 Note fact, Kripke semantics, formulas interpreted modelvaluation only, context models (which reached atomic program ; p j).finitely many them, one . Call collection models . fact,structure respect formulas interpreted. sense, one Kripkemodel language (w.r.t. A, P): . prove completeness respect uniquetwo-dimensional model, Section 3.following lemma easily established induction :Lemma 2 fixed sets agents propositional variables P, direct semanticsKripke semantics equivalent, i.e., , K(A, P) = h, R iA , i,model = hA, P, , i:|=d iff M, |=K .usual, define |=K : M, |=K , |=K : |=K .3. Complete Axiomatizationsound complete axiomatization DCL - PC presented Figure 5. ease exposition, divide axiomatization five categories, follows. PropositionalComponent Rules Inference straightforward, Dynamic Component immediate adaptation Propositional Dynamic Logic (Harel et al., 2000). Control Axiomsinherited CL - PC (van der Hoek & Wooldridge, 2005b). (The occurrence `(p) refersliteral atomic proposition p: either p p, obvious meaning `(p).) Noteallocation specifies every propositional variable assigned exactly one agent (i.e.,allocation), contrast, fixed allocation assumed CL - PC, onecould explicitly state controls(i, p), every p Pi (van der Hoek & Wooldridge, 2005b).Transfer & Control Axioms, atomic permanence states program changesvaluation. this, one easily extends arbitrary objective formulas (obtaining objectivepermanence, see Theorem 1 below). axiom persistence 1 (control) says controlp affected move another valuation, axiom persistence 2 (control) specifies448fiR EASONING BOUT RANSFER C ONTROLPropositional ComponentPropDynamic ComponentK( )union( )comp( )test( )mix( )ind( )Control AxiomsK(i)T(i)B(i)emptycontrol(i)allocationeffect(i)Comp-objective tautology[ ]( ) [ ] [][ 0 ] [ ] [ 0 ][ ; 0 ] [ ][ 0 ][?] ()[ ][ ] [ ][ ]( [ ]) [ ]2i ( ) 2i 2i2i22controls(i, p) p pVpP controls(1, p)5 5controls(n, p)= {1, . . . , n}p 6 P(),`(p) controls(i, p) `(p)objective2C1 2C2 2C1 C2Transfer & Control Axiomsatomic permanence(;) hi ;p ji> [i ;p j]q qpersistence1 (control)controls(i, p) 2j controls(i, p)persistence2 (control)controls(i, p) [j ;q h]controls(i, p)precondition(transfer)hi ;p ji> controls(i, p)transfercontrols(i, p) hi ;p jicontrols(j, p)funccontrols(i, p) hi ;p ji [i ;p j]Rules InferenceModus PonensNecessitation` , ` ( ) `` ` 26= j p 6= q2 = [ ], 2iFigure 5: Axiomatic System DCL - PC.remains control p, even transfer program executed: either variablepassed program p, delegating agent i. axiom precondition(transfer)expresses agents give variables away possess, and, finally func saystransition relation associated atomic transfer program functional: one resultingworld emerges.following theorem lists properties DCL - PC, controls(C, p) defined equation (1) above.Theorem 11. axioms K(i), T(i), B(i), effect(i) coalitional counterparts K(C), T(C), B(C),effect(C) derivable coalition C.449fiVAN DERH OEK , WALTHER , & W OOLDRIDGEat-least(control) :`(p) controls(i, p) `(p)at-most(control) :`(p) `(p) 2j `(p)(i 6= j)non-effect(i) :`(p) controls(i, p) 2i `(p)persistence(non-control) :controls(i, p) 2j controls(i, p)objective permanence(;) :hi ;p ji> [i ;p j] objectiveobjective permanence:h i> [ ]objectiveinverse :controls(i, p) [i ;p j; j ;p i]reverse :[i ;p j][k ;q h] [k ;q h][i ;p j](j 6= k h 6= i) p 6= qFigure 6: Theorems DCL - PC.2. Moreover, know (van der Hoek & Wooldridge, 2005b) axioms K(i), T(i), B(i),effect(i) coalitional counterparts K(C), T(C), B(C), effect(C)derivable coalition C.3. ` controls(C, p)WiCcontrols(i, p).4. ` controls(C, p) 2j controls(C, p), i.e., property persistence1 (control) also derivable replace agent arbitrary coalition C.Proof: See Appendix A.QEDConsider language without dynamic transfer operators, propositionallogic cooperation modalities C . Models M, M0 K(A, P). program-freelanguage, every formula equivalent one without occurrences coalition operators van derHoek Wooldridge (2005b). instance, suppose P = {p, q}. formula (p r)equivalent (p r) (p r) (we read current value variable r outside control).establish similar result language including programs. world (M, )completely characterized know variables true it, allocationvariables agents is. case, truth objective formulas, formulas involving abilitiestransfer programs completely determined.Lemma 3 Let arbitrary DCL - PC formula conjunction assertions formcontrols(j, p) controls(j, p). Then, DCL - PC, derive` C ( ) ( C ).450fiR EASONING BOUT RANSFER C ONTROLProof: Since Comp-, C = {a1 , a2 , . . . , aC }, C a1 a2 aC ,sufficient prove claim individual agent i. Moreover, move conjunctsone one, know` ( controls(j, p)) (controls(j, p) ),controls(j, p) either controls(j, p) controls(j, p). reasoning nonnegated case (the one similar): ( controls(j, p)) equivalentcontrols(j, p) ( controls(j, p))controls(j, p) ( controls(j, p)) .However, using theorem persistence(non-control) Figure 6 (whichderive below),second disjunct controls(j, p) ( controls(j, p)) . concludesproof.persistence(non-control), right-to-left direction follows immediately T(j).direction, assume controls(i, p). allocation derivecontrols(1, p)5 5controls(i 1, p)5controls(i + 1, p)5 5controls(n, p),Wthis, persistence1 (control), get k6=i 2j controls(k, p). every k 6= i,controls(k, p) controls(i, p), follows allocation. Hence, using Necessitation,2j (controls(k, p) controls(i, p)).WFrom Axiom K(j), follows 2 j controls(k, p)2j controls(i, p). Combining k6=i 2j controls(k, p), obtain desired conclusion2j controls(i, p).QEDSoundness axiom schemes Figure 5 readily checked. proceed proveaxiomatic system DCL - PC Figure 5 complete. First, introduce notation.Definition 1 Given set propositional variables P, valuation description conjunctionliterals (p p) every propositional variable P occurs one literal.Notice that, propositional variable p P, holds either p, p.denote set valuation descriptions P . Notice that, valuation ,V= V{p | p P (p) = tt}{p | p P (p) = ff}.Definition 2 Given set propositional variables P set agents A, allocation description conjunction formulas form controls(i, p) every p P,exactly one controls(i, p) appears .denote set allocation descriptions . Notice allocations conjunctionscorrespond other: allocation = hP1 , . . . , Pn variables P agentsA,^controls(i, p).=iA,pPi451fiVAN DERH OEK , WALTHER , & W OOLDRIDGETherefore, refer formulas allocation descriptions. Given two allocation descriptions, 0 , say (i ;p j) 0 following three conditions satisfied: controls(i, p),0 controls(j, p), 0 agree control expressions.Definition 3 Let, allocation description , set valuation descriptions. Then,formula form_ _(3)called proposition description.later, Theorem 2, see every formula equivalent proposition description.intuition truth requires, every allocation description , possibletruth values atoms fixed. give example, suppose two agents j,three atoms p, q r. Consider formula = hi ;p ji(q j (p r)). order findequivalent proposition description, must, every make proper choices .implies controls(i, p) allocation would make false (since cannotW transfer control p),, choose empty set, ensuring ( ) equivalent. implies controls(i, p),W basically two cases: either also implies controls(j, r),constraint equivalent q, else implies controls(j, r), caseWequivalent q r.Let us, two valuation descriptions 0 , coalition C, allocation description, write 0 (mod C, ) two conjunctions literals 0 differ variables control C, determined . instance, C = {1, 2} =controls(1, p1 )controls(2, p2 )controls(3, p3 ), (p1 p2 p3 ) (p1 p2 p3 ) (mod C, ).first collect facts valuation descriptions, allocation descriptions, propositiondescriptions.Recall that, set DCL - PC formulas, 5 used shorthandWV1 6=2 (1 2 ).Lemma 4 Given set valuation descriptions , set allocation descriptions ,following six items satisfied:1. ` 52. ` 5W3. ` ( )4. : ` (CW0 (mod C,)0 ).5. ` ( hi ;p ji 0 ) (( ) hi ;p ji( 0 )).6. Let n number agents, k number propositional variables.nkN(n, k) = 22 provably non-equivalent proposition descriptions.Proof:1. follows Prop definition : mutually exclusive cannotfalse.452fiR EASONING BOUT RANSFER C ONTROL2. Item (2) easily seen equivalent allocation axiom: allocation impliesand, every allocation description , implies allocation.W,3. Item (3) immediate Item (2) axiom Prop. particular, using Prop derive` B C (C A) (C B).4. Assume . right-to-left direction, also assume 0 , valuation description 00 (mod C, ). means 0 differ variables p1 , . . . , pmcontrols(C, pj ) implied (for j = 1 . . . m). Note 0 objectiveformula. write 0 `(pi ) (for = 1 . . . m), 0 literal`(pi ) left out. Apparently, 1 `(p1 ) controls(C, p1 ). Since 1 objective,apply effect(C) conclude C (1 `(p1 )). Using Lemma 3, derive C (1 `(p1 )).rewrite 1 2 `(p2 ), obtain C (2 `(p2 ) `(p1 ) ). useeffect(C) Lemma 3 get C C (2 `(p2 ) `(p1 ) ). Comp-,C (2 `(p2 ) `(p1 ) ). repeat process getC (j `(pj ) `(p2 ) `(p1 ) ). But, definition 0 , implies C .WeWshow direction left right contrapositive: fix assume0 (mod C,) 0 . show C . Let Q(, ) = {`(p) |` `(p) 0controls(C, p)} set literals variables controlagents C allocation . Notice valuations 0 (mod C, )W agree on0literalsQ(,).usepropositionalreasoningderive0 (mod C,)WV`(p)Q(,) `(p). Using T(C) K(C), conclude `(p)Q(,) C `(p)., follows, literal `(p) Q(, ), controls(C,`(p)), which, equaWtion (1), equals C `(p)`(p).then,deriveC`(p)Q(,) C `(p). UsingVK(C), obtain C `(p)Q(,) `(p). Hence, C .5. First all, ( hi ;p ji 0 , follows controls(i, p). Hence, given ,formulas hi ;p ji [i ;p j] equivalent func. particular, hi ;p ji>.Let us first show literals `(q) `(q) hi ;p ji`(q). negative literals q, qhi ;p jiq equals q [i ;p j]q, follows atomic permanence(;). positiveliterals q, use func obtain q [i ;p j]q, holds atomic permanence(;).Now, given , q hi ;p jiq q hi ;p jiq. Then, valuationdescription , also hi ;p ji. remains show hi ;p ji 0 .left right, note controls(i, p) hi ;p jicontrols(j, p) transfer.controls expression implied , follows persistence 2 (control)func. Finally, consider direction right left: hi ;p ji 0 . showthat, first, hi ;p jicontrols(j, p) controls(i, p), and, second, hi ;p jicontrols(h, q)controls(h, q), controls(h, q) (with q 6= p) implied 0 . first part follows immediately precondition(transfer). second part, let h agent q 6= p variable 0 impliescontrols(h, q). Suppose hi ;p jicontrols(h, q). allocation,Vcontrols(k,q). contrapositive persistence 2 (control)hi;jipk6=hVyields k6=h controls(k, q). follows allocation controls(h, q).6. Item 6 follows straightforward counting argument proposition description formulas.number proposition descriptions depends cardinalities sets .Given number n = |A| agents, number k = |P| propositional variables,453fiVAN DERH OEK , WALTHER , & W OOLDRIDGEeasy see 2k valuation descriptions , nk allocation descriptions(i.e., number ways distribute k variables n agents). Observeproposition description formula obtained assigning set valuation descriptionskkallocation description . Hence, 22 n proposition descriptions. Sincekknknk22 n 22 , obtain N(n, k) = 22 upper bound number differentproposition description formulas.QEDpresent main result section. first formulate it, reflect briefly it,give proof.Theorem 2 every DCL - PC formula , sets () valuation descriptions, one,_ _`() .According Theorem 2, get rid hi ;p ji. show derivation systemgood enough establish that, main task proof. let us first convincesemantically normal form makes sense. Remember every model comesallocation . formula like hi ;p ji true M, , true model lookslike M, control p transferred j. means formula 0must already true M, , takes role j far p concerned. instance,hi ;p ji(q j (p r)) true M, , q j r controls(i, p) true current allocation. formula reference layers anymore. precisely, hi ; p ji(q j (p r))equivalentq ((p r) (p r)) controls(i, p) controls(j, r)q ((p r) (p r) (p r) (p r)) controls(i, p) controls(j, r) .Proof: proof induction norm |||| defined DCL - PC formulas follows:||>||||||||1 2 ||||C ||||[i ;p j]||||[?]||||[1 2 ]||||[1 ; 2 ]||||[ ]||=========||p|| = 0, p P1 + ||||1 + ||1 || + ||2 ||1 + ||||, C1 + ||||1 + || ||1 + ||[1 ] [2 ]||1 + ||[V1 ][2 ]||1 + || i=0..N [ ]i ||N = N(n, k) number defined Lemma 4, Item (6).induction base proof theorem two cases:W= >. take (>) = ,every.Item(1)Lemma4,WWW(>) equivalent , turnobjective tautology. Hence,equivalent > (2) Lemma 4.454fiR EASONING BOUT RANSFER C ONTROL=(p) = { |` p}, every . Clearly, pW p, p P. Take WWis equivalent(p), i.e., ` p (p). Now, using (3) Lemma 4, get ` p (p ).Finally,WreplacingW second occurrence p derived equivalent formula, get(p) . direction follows simple propositional reasoning.` pConsider induction step.= . set () = \ (), every . works,following:_^_(_(_()___(4)())())()(5)(6)(7)steps purely propositional,(5) (6)V except equivalence Wexplain now. Abbreviate (5) W (A ), (6) (A ). NoteLemmaV 4, Item (2), derive . words, oneW must true, say . note(A )W implies (A ) hence also (A ), abbreviation(6). Conversely, (A ) holds, know Lemma 4, item (2) 4 ,i.e., exactly one must hold, say . formula (A ) true,allocation description formula true. , also(A ). Moreover, anyV 6= , , hence (A ). (A )holds , hence (A ), shorthand (5).= 1 2 . set (1 2 ) = (1 ) (2 ), every . followingequivalences, need propositional reasoning:1 2____(_(1 )__(1 )__(2 )(2 ))(1 2 )= C . every , set(C ) = { | 0 (mod C, ) 0 ()}455(8)(9)(10)fiVAN DERH OEK , WALTHER , & W OOLDRIDGEderive following equivalences:CC__C_C____()()_(12)(14)()(C )(11)(13)equivalence (11) holds induction hypothesis. Using K(i), equivalent(12) (for diamond C ( ) (C C )). equivalence latter(13) Lemma 3.WWremains show equivalence (13) (14).()=CC() .WK(C) Comp-, formula equivalent () C . Using Item (4)WW0Lemma4,seeequivalent()0 | 0 (mod C,) . equalsW(C ) definition (C ).= [i ;p j]. define ([i ;p j]) follows: every ,0 () controls(i, p)([i ;p j]) =(i ;p j) 0otherwisesee yields formula right form equivalent [i ; p j], let us firstpartition + (i, p) = { | ` controls(i, p)} (i, p) = { | `controls(i, p)}. consider following derivable equivalences:_ _[i ;p j] [i ;p j]( 0 () 0 )(15)0controls(i, p)_ _( 0 () 0 ))(controls(i, p) hi ;p ji(16)0_((i,p)__)hi ;p ji__(17)0+ (i,p)_ _( 0 () 0 )([i ;p j])(18)equation (15) holds induction hypothesis. equivalent (16) propositional reasoning, changing [i ;p j] hi ;p ji allowed func. equivalence (16) (17) follows definition + (i, p) (i, p) fact456fiR EASONING BOUT RANSFER C ONTROLW>. order prove equivalence (17) W(18), Wsufficient+ (i, p), formula hi ; jishow that, fixed(0 ()p0W00 ) equivalent 0 (), (i ;p j) .W Lemma 4,W followsItemW(5), follows.W First all, write hi ;p ji 0 ( 0 () 0 )0 hi ;p ji( 0 () 0 ). mentioned lemma,W know exactly 00 giving hi ; ji( 0 () 0 ). rewriteneed: 0(i;j)ppWhi ;p ji {W 0 | 0 ()}, push diamond hi ;p ji inside00disjunctionW get {hi ;p ji( ) | ()}.W Lemma 4, Item (5)yields 0 (). direction isWsimilar: 0 () (i ;p j) 0 , then,Lemma 4, Item (5), get hi ;p ji( 0 () 0 ), result follows.= [ 0 ?]. axiom test( ), [ 0 ?] equivalent 0 , equivalentformula right form induction hypothesis.= [1 ; 2 ]. axiom comp( ), [1 ; 2 ] equivalent [1 ][2 ], equivalentformula right form induction hypothesis.= [1 2 ]. axiom union( ), [1 2 ] equivalent [1 ] [2 ],equivalent formula right form induction hypothesis.= [ ]. Recall N = N(n, k) given Lemma 4, Item (6). Using axiom mix( )K( ), know [ ] equivalent [ ][ ]. N times, obtain[ ] [ ]2 [ ]N [ ]N [ ].induction hypothesis, know except last conjunct equivalentnormal form. since N different forms, Vconjunct [ ] N mustequivalent one earlier conjuncts [ ]i (i < N). Define = i=0..N [ ]i . claim[ ].(19)induction hypothesis, equivalent formula right form. direction (19) obvious since first part unraveling [ ][ ][ ] . . .[ ] using axioms mix( ) K( ). show direction sufficient derive[ ], because, fact one conjuncts , immediatelygives [ ]. show derivability [ ], use ind( ). First all,show [ ]. see this, note [ ] [ ][ ]0 [ ][ ]1 [ ][ ]N .induction hypothesis, conjunct [ ]i (i N) normal form, say, Bi .obtain sequence B0 , B1 , . . . , BN N + 1 formulas normal form. SinceN provably non-equivalent formulas Lemma 4, Item (6), know B jequals previous Ba < j N. Let Bj first repetition sequence.Notice [ ]j Bj = Ba [ ]a , thus [ ]k [ ]j [ ]k [ ]a ,k 0. then, follows last conjunct [ ][ ]N [ ] equivalent [ ]k [ ]a(with k = N j), already appears . derived [ ], applyNecessitation [ ], obtain ` [ ]( [ ]). applying ind( ), get [ ].QEDknow Lemma 4 finitely many different normal forms: since everyformula normal form, finitely many non-equivalent formulas. also457fiVAN DERH OEK , WALTHER , & W OOLDRIDGEknow proof Theorem 2 that, [ ], consider finite numberconjuncts [ ]i unraveling.Corollary 1 finitely many pairwise non-equivalent formulasgiven number n agents, k propositional variables, have:W1. 6= j 6` (i j ) ` iM ,V2. ` [ ] [ ]i ,DCL - PC .fact,nk= 2nk N = 22 (as defined Lemma 4, Item (6)).Completeness derivation system inference relation ` respect semantics meansevery semantically valid formula also provable: |= ` . order prove completeness,often contrapositive shown: 6` 6|= . is, every consistent formulamodel. popular technique modal logic construct dedicated model canonicalmodel, (cf. e.g., Blackburn et al., 2001) consistent formula . canonical modelbridge syntax semantics: consists maximal consistent sets (as worlds),constructed way membership world truth corresponding worldcoincide.DCL - PC works straightforwardly follows. Fix finite sets P, takeconsistent formula . Build WmaximalW consistent set around it. Let normal form ,guaranteedTheorem2,(). Since maximal consistent, ,W( ) . Again, maximal consistency , must contain, exactly one, formula . uniquely determines valuation = , whereas determinesallocation = . words, uniquely determines pointed Kripke model (M, )= h, RiA , i. worlds determined P, RiA (the horizontal layerM, terms Figure 4) determined Control Axioms. availability rightmodels M0 = h, RiA , 0 (the vertical layer Figure 4) determined DelegationControl Axioms. result, directly interpret subformulas form hi ; p jiC proper way, (M, ).argument easily extends strong completeness, states sets formulasformulas , |=K ` . see this, suppose 6` , i.e., {}consistent. Since finitely many pairwise non-equivalent formulas, mustformula equivalent {}. previous argument, find pointed model(M, ) (M, ) |=K . model also (M, ) |=K {}. Hence,every model one , i.e., 6|=K . Strong completeness also follows alternativeway resaoning: note language compact: i.e., |= K , finite set0 0 |=K . seen follows: know Corollary 1different formulas provably equivalent, depends numberagents number atoms. then, soundness, also semanticallydifferent formulas . Putting formulas 0 gives desired result. Strong completenessfollows (weak) completeness compactness.all, obtain following (also using Lemma 2).Theorem 3 language DCL - PC compact. Moreover, axiomatic system DCL - PC soundcomplete respect Kripke direct semantics. also strongly complete.458fiR EASONING BOUT RANSFER C ONTROL1. function program-eval(, = hA, P, , i, d) returns fail model A, P2.= ?3.return DCL - PC-eval(, M)4.fail otherwise5.elsif = (i ;p j)6.return hA, P, 0 , p Pi7.0 = = j8.otherwise = hP1 , . . . , Pn 0 = hP01 , . . . , P0n9.P0i = Pi \ {p},10.P0j = Pj {p},11.P0m = Pm n, 6= i, j12.fail otherwise13. elsif = (1 ; 2 )14.return program-eval(2 , program-eval(1 , M, d), d)15. elsif = (1 2 ) non-deterministically choose either16.return program-eval(1 , M, d)17.program-eval(2 , M, d)18. elsif = 019.return fail = 020.otherwise (if > 0) non-deterministically choose either21.return22.program-eval(( 0 ; 0 ), M, 1)23. end-functionFigure 7: algorithm deciding (M, M0 ) R .is, setsDCL - PCformulas every DCL - PC formula ,` iff |=K iff |=d4. Complexitymodel checking satisfiability problems CL - PC PSPACE-complete (van der Hoek &Wooldridge, 2005b), since DCL - PC subsumes CL - PC, implies PSPACE-hardness lowerbound corresponding problems DCL - PC. obvious question whether additional dynamic constructs DCL - PC lead complex decision problem particular,whether DCL - PC satisfiability matches EXPTIME-completeness PDL satisfiability (Harel et al.,2000). section, show model checking satisfiability problems factworse CL - PC: PSPACE-complete. Notice EXPTIME-completeness usuallyregarded characteristic complexity logics modal operator anotheroperator representing transitive closure operator (Blackburn et al., 2001).Note consider model checking problem section, consider problem respect direct models, Kripke models. course, respect satisfiability,makes difference: formula satisfiable respect direct models iff satisfiable w.r.t.Kripke models.proving PSPACE-completeness DCL - PC model checking, consider auxiliary notions first. program sequence transfer program composed atomic transfer programs,tests, sequential composition only. program admits program sequence un459fiVAN DERH OEK , WALTHER , & W OOLDRIDGEfolded recursively applying following rules: atomic transfer program (i ; p j),test ?, transfer programs (m 0):(i ;p j)?1 ; 21 2(i ;p j)?1 ; 21 21 ; 2 ; . . . ; n , n 0,= , nfollowing two lemmas establish membership accessibility relation R transfer program decided polynomial space.Lemma 5 transfer programs 0 (direct) models M0 , (M, M0 ) R 0 implies0 admits program sequence length exponential length 00 3(M, M0 ) R . fact, length limited 2| | .Proof: Let 0 , M, M0 lemma. proof induction structure0 . interesting case 0 = ; cases straightforward. Suppose(M, M0 ) R . Since R = (R ) , sequence M0 , . . . , Mn , n > 0, modelsM0 = M, Mn = M0 , (Mi1 , Mi ) R , 1 n. transitivityR , assume sequence M0 , . . . , Mn Mi 6= Mj , i, j1 < j n, i.e., sequence models contains loops. induction hypothesis yields3admits program sequences 1 , . . . , n length 2| | (Mi1 , Mi ) Ri1 n. = 1 ; 2 ; . . . ; n program sequence admitted(M, M0 ) R . following, shown required length. Notemodels reachable = hA, P, , via R differ allocation propositionalvariables P agents A. precisely, differ allocation propositionalvariables agents occur . Thus `m reachable models, `number propositional variables occurring number agents occurring . Noticen exceed `m ; otherwise sequence M0 , . . . , Mn contains loops contradicting2assumption. Together fact `m | || | 2| | , upper bound lengthgiven follows:|| = |1 ; 2 ; . . . ; n |=n sup{|i | : 1 n} + n2322| | 2| | + 2| |2322| | +| | + 2| |32(| |+1)32| | .QEDLemma 6 programs (direct) models M0 , membership problem (M, M0 )R decided PSPACE.Proof: Let program let M, M0 two (direct) models. Consider following algorithmdecides (M, M0 ) R using function program-eval( ) Figure 7:460fiR EASONING BOUT RANSFER C ONTROL31. Set = 2| | .2. program-eval(, M, d) = M0 , return (M, M0 ) R , otherwise.see algorithm correct, shown program-eval(, M, d) = 0 iff (M, M0 )R . direction left right, readily checked program-eval(, M, d) = 0implies existence program sequence admitted length | |(M, M0 ) R 0 . Clearly, R R thus (M, M0 ) R . Consider direction rightleft. (M, M0 ) R , follows Lemma 5 program sequence admitted3length 2| | (M, M0 ) R . Step 1 ensures value32| | | | d. obvious construction algorithm non-deterministicchoices lines 15 20 Figure 7 yield program-eval(, M, d) = 0 . Noticealgorithm terminates since recursive calls lines 14, 16, 17 applied strictsubprograms recursive call Line 22 followed one Line 14parameter limits recursion depth.algorithm run polynomial space. see this, notice functionDCL - PC-eval( ), called Line 3, computed polynomial spaceparameter encoded binary. Moreover, stack algorithm computing functionprogram-eval( ) limited size polynomial length . Note stackneeds store currently evaluated program programs backtracking points,introduced nested function call Line 14. since nested function callapplied strict subprograms, linearly many backtracking points needed time.Although algorithm non-deterministic, follows well-known fact NPSPACE equalsPSPACE (Savitch, 1970) runs PSPACE .QEDUsing previous two lemmas, prove following.Theorem 4 model checking problem DCL - PC (w.r.t. direct models) PSPACE-complete.Proof: Given DCL - PC subsumes PSPACE-hard logic CL - PC, need prove upperbound. Consider function DCL - PC-eval( ) Figure 8. Soundness obvious construction.First note algorithm strictly analytic: recursion always sub-formula input.algorithm PSPACE follows fact loops lines 1012 15-18involve, first case simply binary counting variables P C , second simplylooping direct models P: need store modelschecked, done polynomial space. Finally, Lemma 6 yields check(M, M0 ) R Line 16 done polynomial space.QEDNow, make use following result, proof identical equivalent resultproved van der Hoek Wooldridge (2005b).Lemma 7 DCL - PC formula satisfiable, satisfied (direct) modelsize(M) = |P()| + |Ag()| + 1.prove following.Theorem 5 satisfiability checking problem DCL - PC PSPACE-complete.461fiVAN DERH OEK , WALTHER , & W OOLDRIDGE1. function DCL - PC-eval(, = hA, P, 0 , i) returns tt ff2.P3.return ()4.elsif =5.return DCL - PC-eval(, M)6.elsif = 1 27.return DCL - PC-eval(1 , hA, P, 0 , i)8.DCL - PC-eval(2 , hA, P, 0 , i)9.elsif = C10.C-valuation C11.DCL - PC-eval(, hA, P, 0 , C ) return tt12.end-for13.return ff14. elsif = h15.model M0 A, P16.(M, M0 ) R17.DCL - PC-eval(, M0 ) return tt18.end-for19.return ff20. end-functionFigure 8: model checking algorithm DCL - PC.Proof: Given formula , loop model containing P() Ag()size(M) = |P()| + |Ag()| + 1, |=d return Yes. consideredmodels, return No. Theorem 4, check whether |= polynomial space.QEDNotice PSPACE complexity checking satisfiability depends upon fact modelsDCL - PC concise, hence loop polynomial space (weneed remember model considered).5. Characterizing ControlOne main concerns original study CL - PC (van der Hoek & Wooldridge, 2005b)investigate logical characterization control: extent could characterize,logic, states affairs agents could reliably control. Control distinguished abilitysense that, example, agent could said control tautology, even one mightprepared concede agent would ability bring tautology. startingpoint study control (van der Hoek & Wooldridge, 2005b) controls(i, p) construct:already seen, expression true iff variable p controlagent i. led analysis characterization types formulas agent couldsaid control. type control studied van der Hoek Wooldridge derivesability agents choose values propositional variables control. Let us refertype control, agent directly able exert influence state affairsassigning values variables, first-order control. section, undertake similarstudy control richer setting DCL - PC. Here, however, second type control,462fiR EASONING BOUT RANSFER C ONTROLderives ability transfer control variables agents. Thus, example,controls p, also power ensure instance controls(j, p), j agent differenti. control expressed transfer modality: hi ; p jicontrols(j, p). refertype control second-order control. see types control indeedrather orthogonal. instance, hi ;p jij (i give p j, achieve ) i,j (ij cooperate, achieve ) logically incomparable. example, taking = hj ; p ii>gives|=d controls(i, p) (hi ;p ji i,j )= hi ;p ji> assuming 6= j,|=d controls(i, p) (hi ;p ji i,j ).However, goal objective formula, relate atomic control transfer,shortly see.begin study, consider transfer program[[;p j .(20)controls(i, p)?;givei =jApPhgivei would express way give one propositional variables oneagents (possibly herself) way consequently holds. Thus, hgive meansdistribute variables among agents way afterwards holds. Hence,reasoning power, strongest achieve hgivei i,expressing achieve either choosing appropriate value variables,distributing variables appropriate way. Note hgivei implyhgivei ii , hence hgivei ii holds seen achieveown. come back program givei below.program give generalized incorporate coalitions give away variables,receive: let[[[controls(i, p)?;hi ;p ji .(21)giveC;D =iC pPjD{i}program giveC;D lets arbitrary agent coalition C either givevariables p arbitrary member coalition D, nothing (i.e., give herself).Now, objective formulas , following, dedicated agent C:C hgiveC;{i} ii .words: agents coalition C choose values variables ,way give variables dedicated agent i, achieve . Notegeneral able eliminate occurrences s, since wayexpress first-order control, i.e., reason different valuation.examples language without transfer, refer paper van der HoekWooldridge (2005b), especially example Bach Stravinsky, (i.e., Example 2.4, van derHoek & Wooldridge, 2005b). looking two examples control dynamic setting, noteallows following inference, objective formula :consistent |=d(22)inference says grand coalition achieve satisfiable objective formula.463fiVAN DERH OEK , WALTHER , & W OOLDRIDGEExample 1 Suppose n agents: 1, . . . , n. controls flag r (i = 1 . . . n) indicatedesire control particular resource, modeled variable p. want ptrue false every then, (which could taken care central agent executingprogram making p false true alternatively), rather, want control p eventually. Let+n denote addition modulo n, and, similarly, n subtraction modulo n. Let skip denote >?, i.e.,test tautology. Consider following program:grant-req(i) =...controls(i, p) skip elseri+n 1(i ;p +n 1) else...ri+n (n1)(i ;p +n (n 1)) else skipprogram grant-req(i) makes agent pass resource p whenever somebody else needs it, need checked order starting agent next + nindex. Note use variable p, i.e., making true false, encodedprogram constructs. consider programpass-on(i, j) =grant-req(i); . . . ; grant-req(j n 1).program pass-on(i, j) pass control variable p agent j, provided initially r jset one agents sequence i, +n 1, . . . , j n 1 owns it. expressedfollows:rj controls({i, +n 1, . . . , j n 1}, p) [pass-on(i, j)]controls(j, p).have:rihpass-on(i +n 1, i)icontrols(i, p)[pass-on(i +n 1, i)]controls(i, p) .is: agent flags request ri resource p, then, program pass-on(i +n 1, i)executed, control p.Notice previous example freely passes variable along chains agents, therebytaking granted control variable fly, making true false will.following example, control variable important, also truth sideconditions involving them.Example 2 scenario three agents: two clients c 1 c2 , server s.server always control one propositional variables p 1 p2 , particular wantsguarantee variables never true simultaneously. time, c 1 c2 wantensure least one variables pi (i = 1, 2) true, variable pi belongs clientci . describe invariant system formula Inv:Inv =_controls(s, pi )i=1,2_controls(ci , pi )i=1,2Consider following transfer program :=(controls(s, p1 )? ; ;p1 c1 ; c2 ;p2 s)(controls(s, p2 )? ; ;p2 c2 ; c1 ;p1 s) .464fiR EASONING BOUT RANSFER C ONTROLsays arbitrary number times one variable pi passed server clientci , another variable pj (i 6= j) client cj server.Using Inv , describe whole scenario follows:Inv []InvInv [] (p1 p2 ) {c1 ,c2 } (p1 p2 )general characterization types formulas agents coalitions could controlgiven van der Hoek Wooldridge (2005a), aim undertake studyDCL - PC . appear done local global level, also seenotion control inherited CL - PC, natural generalization context.next corollary establishes result concerning characterization control. first itemsays strict sub-coalition C 6= A, valid C controls something.words, control coalition always feature specific model, particular, specificallocation. According second item, grand coalition derivably,W models, controlsWexactly formulas property theirWequivalent form ( () )every allocation description , formula () contingency,W i.e., tautologyneither contradiction. propositional formula, () easysee contingency sufficient necessary |= K controls(A, ).Whand, instance controls(i, p), () = controls(i, p) () equals>. indeed, 6|=K controls(A, ). Contrast = p controls(i, p). follows easilytruth definition Kripke semantics defined Section 2.6, Theorem 2Theorem 3.Corollary 2 Let Kripke model, C coalition C C 6= A, let rangingDCL - PC formulas. follows that:1. , |=K controls(C, ),WW2. |=K controls(A, ) iff formula ( () ), equivalent accordingTheorem 2, allocation description , () = () = .Proof:1. order controls(C, ) valid Kripke semantics, true worldsKripke models. Take Kripke model = h, RiA , allocation= hP1 , P2 , . . . , Pn Pj = , j C. M, |=K Ciff world 0 0 = (mod PC ), holds M, 0 |=K . But, sincePC = , 0 itself, cannot M, |=K C C . Hence,M, 6|=K controls(C, ).2. FirstW left-to-right direction contraposition. Let equivalent forW provemula ( () ), Theorem 2. Suppose, allocationdescription , () = . means, every Kripke model = h, RiA ,every valuation , M, |=K . Consequently, M, |=K . However,agents change , current allocation . Since () = , cannot choose valuation falsifies , i.e., M, |=K . Similarly, () = ,465fiVAN DERH OEK , WALTHER , & W OOLDRIDGEM, |=K C , . But, given , cannot choose valuation satisfyingcurrent allocation described , i.e., M, |=K . Hence, either case6|=K controls(A, ).WWConsider direction right left. Suppose equivalent ( ())while, , set () either . Let = h, RiA , Kripke model.Remember allocation description corresponding allocation . fact6= , valuation description corresponding valuationsatisfies (M, ). then, choose order satisfy , thusM, |=K , . Similarly, M, |=K follows fact6= . Hence, |=K controls(A, ).QEDOne may ask local characterization coalition controls: Kripkemodels valuations M, |=K controls(C, )? notion control,answer immediately read Theorem 6, given shortly. theoremgeneral notion: recover characterization result current notion controls(i, ),would need items (1b) (2b) Theorem 6.notion control discussed far taken CL - PC: lifted characterization results richer language. However, clear discussion earlier section,appropriate notion control individual language might obtained usingprogram givei , givei defined (20). Note hgivei ii valid, hencehgivei seems general way reason control: achievetoggling propositional variables delegating them. One easily discusscoalitional level, lifting Definition (20) case coalitions C suggested(21) giveC;D . However, stick individual case simplicity. Let us thereforedefineCONTROLS(i, )=hgivei ii hgivei ii(23)definition says agent controls formula iff way distribute propositional variables agents makes appropriate choices remaining variables, holds, also way distributing variables enables enforce .validity hgivei ii , infer controls(i, ) implies CONTROLS(i, ).Notice implication way around valid since controls(i, ) never truecontrol agents variables. example, CONTROLS(i, controls(j, p)) holds iffp Pi . this, know controls(i, p) CONTROLS(i, controls(j, p)) theorem,basically says control variable, freely choose keeppass on. However, controls(i, p) controls(i, controls(j, p)) valid, evencontrols(i, p) controls(i, controls(j, p)): agent owns p, cannot choose keep ppass toggling propositional variables.state characterization result, introduce notation. twoKripke models = h, RiA , M0 = h, RiA , 0 K(A, P) agent i, sayM0 allocations = hP1 , . . . , Pi , . . . , Pn 0 = hP01 , . . . , P0i , . . . , P0nP0i Pi and, j 6= i, Pj P0j . is, M0 obtained executing givei .case, also say 0 .466fiR EASONING BOUT RANSFER C ONTROL11 ()giveifffi22 ()2 () 6=33 () 6=3 ()Figure 9: IllustrationWW() .valuation description , let valuation described , and,allocation description , let allocation described , let Pi setpropositional variables controlled agent .WWTheorem 6 Let DCL - PC formula() , given Theorem 2.Let = h, RiA , Kripke model K(A, P), world M. Then, agentA,M, |=K CONTROLS(i, )iff following two conditions satisfied:1. ()(a) ,(b) = (mod Pi ).467fiVAN DERH OEK , WALTHER , & W OOLDRIDGE2. \ ()(a) ,(b) = (mod Pi ).first demonstrate requirements (1) (2) Theorem (6). Suppose satisfies (p qr), Agent = 1 owns p M, Agent 2 owns q r, Agent 3 propositional variablesM. First all, see Item (1b) needed, guarantee 0 , 0 , holdsM0 , 0 |=K 1 . means that, even 1 given away atoms (resultingallocation 0 ), still able make true. possible = (p q r):Agent 1 could simply stay within current allocation make p false. However,possible = (p q r controls(3, p)) since, 1 delegated control p3, agent 1 cannot make p false anymore. Moreover, agent give atoms away,model allocation makes possible satisfy one ,explains Item (1a). Item (2a) exactly motivation, requirement (2b) easilyunderstood similar (1b), one realizes normal form expressedterms normal form follows:_ _( \ ()) .simple illustrating example, suppose two allocations 1 2 , equivalent((p q) (p q)) 1((p q) (p q)) 2 .Note normal form describes valuations 1 2 satisfied. normalform complementary one sense describes valuations 12 falsified:((p q) (p q)) 1((p q) (p q)) 2 .Proof: illustrate proof pictorial story shows requirementsW 1 2Wtheorem sufficient necessary. Given equivalent ( () )Theorem 2, semantically means corresponds collection shaded areas,depicted Figure 9. Now, CONTROLS(i, ) true world Kripke model= h, RiA , i, Agent able move inside shaded area, move outsidewell. moving inside shaded area, means able first go model allocation, world within model valuation description . Noticemove allocation delegating control variables agents (hencerequirement ), move valuation toggling remaining variablesPi (hence condition = (mod Pi )). shows Condition 1 equivalentM, |=K hgivei ii . Accordingly, Condition 2 corresponds able move outsideshaded area Figure 9. Semantically, means able first go model allocation0 , world 0 within model valuation description 0 0 .Consequently, Condition 2 equivalent M, |=K hgivei ii , finishes proof. QED468fiR EASONING BOUT RANSFER C ONTROL6. Possible Extensions Refinementssection, consider possible extensions refinements frameworkpresented paper. claim substantial results relating extensionsaim simply indicate possible directions future research.6.1 Separating First- Second-Order ControlDCL - PC presented here, agent assign value variable (exercising first-order control)iff give variable away (exercising second-order control). is, pair agentsi, j propositional variable p, following.|=d controls(i, p) hi ;p ji>(24)moments reflection confirm always things work human societies. might empower individual make choice behalf, might happyidea individual could turn transfer power somebody else. Sometimes,might acceptable; certainly cases.straightforwardly distinguish situations extending modelsmodifying semantics language follows. model defined structure:= hA, P, , 0 ,components A, P, 0 , originally defined, = h1 , . . . , n tuplesubsets P, elements indexed agents A, 1 , . . . , n forms partition P.Now, intended interpretation models follows:partition 0 defines (initially) ability assign values variables (i.e.,first-order control variables);partition defines transfer control variables (i.e., second-ordercontrol variables).Syntactically, logic define reason structures identical DCL - PC,however semantics different. fact, element semantics needchange relates definition accessibility relation atomic transfer programs.Let = hA, P, , 0 , M0 = hA, P, , 00 , two models 0 = hP1 , . . . , Pn00 = hP01 , . . . , P0n i. Then:(M, M0 ) Ri;p jiff1. p (agent second-order control p begin with)2. k A, p Pk then:(a) k = j l A, Pl = P0l .(b) k 6= j then:469fiVAN DERH OEK , WALTHER , & W OOLDRIDGEP0j = Pj {p},P0k = Pk \ {p},l \ {j, k}, Pl = P0l .setup, first-order control dynamic, changed transfer programs,second-order control defined static. Moreover, fact agent first-order controlvariable mean second-order control: longer equivalence (24).6.2 Hierarchies Networks Controlcourse, reason one stop second-order control. One could extendsetup finite hierarchy control levels, level u > 1 defining transfercontrol variables level u 1, level u = 1 defining exercise first-order control.need extend atomic programs indicate level control transferred. Atomicprograms take form:;up jmean agent transfers level u control agent j. semantics language become yetinvolved, straightforward define. Somewhat related ideas studied Boellavan der Torre (2008).Another direction consider multiple agents write access propositional variables.example, might consider authority relation P A, intended interpretation(i, j) P means everything empowered everything j empowered do.Propositional variables allocated sink nodes P (i.e., agents outgoing edgesP). One might ask, example, whether structural properties graph P characteriseformulae object language.7. Related WorkAlthough researchers begun develop formal systems reasoning delegationtransfer control (e.g., Li, Grosof, & Feigenbaum, 2003), best knowledgeDCL - PC first system rigorous semantics, complete axiomatization. Also,emphasis Li et al. (2003) decentralized trust management, roles likerequester, credentials authorizer distinguished. work presented here, emphasiscoalitions achieve, allowed hand control propositionalvariables.Norman Reed (2002) consider logic delegation, particularly focussing group delegation. logic underpinning work STIT (sees that) logic, main operatorform Si A, meaning agent sees A. extends delegation consideringexpressions form Si Sj (i sees j sees . . . ). example, axiomresulting system is:Si Sj Si A.work Norman Reed represents serious attempt develop philosophically robust logicdelegation, appropriate use computational systems. However, notion delegationdifferent ours, (crudely, agents delegate responsibility, rather transfer control),470fiR EASONING BOUT RANSFER C ONTROLdynamic logic flavour DCL - PC absent. Finally, relatively technical results relatinglogic presented.Jones Sergot (1996) consider problem reasoning power individual obtains virtue organisational role. There, notion actions carried orderempower agents certain capabilities central, Jones Sergot also consider interplay actions ability. However, logical formalisation rather differentSTIT -like language used, rather dynamic logic framework, relatively technicalresults relating framework presented. However, setting Jones Sergot (1996)much general ours: focus propositional control. somewhat related work,Boella van der Torre (2006) present formalisation power delegation setting normative multi-agent systems. consider, example, issue delegated goals interactgoals. framework provides rich compelling setting investigating questionsrelating delegation. However, overarching object language developed representingframework, relatively technical results presented relating framework. wouldinteresting consider whether dynamic logic approach developed within present papermight adapted framework Boella van der Torre.respect logics reasoning controlled variables, Boutilier (1994) presents logicintended capture notions achieve plan using actions relate variablescontrol. spirit, logic close kind situation aiming model,although technical details Boutiliers logic (the way control captured logic)different. Moreover, Boutiliers logic consider multi-agent aspects, dynamicscontrol present paper.refer reader work van der Hoek Wooldridge (2005b) extensive discussion many references logics ability. Gerbrandy (2006) generalises results vander Hoek Wooldridge, considering situations agent partial controlvariable, shares control others. Gerbrandy also shows logics propositional control related cylindrical modal logic (Venema, 1995). Specifically, generalisationCL - PC considered Gerbrandy understood cylindrical modal logic, immediatelyyielding complete axiomatization decidability/undecidability results various fragmentssystem. somewhat related formalism discussed van Benthem, Girard, Roy (2009).formalism intended enable reasoning ceteris paribus preferences (in sensethings equal). Van Benthem et al. develop logic modality hi,set propositional formulae; intended interpretation hi state ustate v agreeing u valuation formulae true. seems quite closeconnection DCL - PC formalism van Benthem et al., although leave detailsfuture work.framework explains control terms agents change whcih atomschoose true false. Sauro (2006) addresses question agents change world,control coalitions defined terms actions agents repertoire. Finally, notediscussed van der Hoek Wooldridge (2005b), logic CL - PC closely relatedwell-known formalism quantified Boolean formulae, hard see alsoclose relationship DCL - PC quantified Boolean formulae. However, mayultimately gain formal expressive power using DCL - PC rather quantifiedBoolean formulae, benefit respect naturalness expression DCL - PC. QuantifiedBoolean formulae explicit notion agency dynamics control, representing471fiVAN DERH OEK , WALTHER , & W OOLDRIDGEaspects within quantified Boolean formulae leads formulae unintuitive hardunderstand.8. Conclusionspaper, built upon logic CL - PC strategic cooperative ability, controlagents environment represented assigning specific propositionalvariables, agents determine truth value. added dynamiccomponent logic, thus obtaining language DCL - PC one reasonagents (and coalitions agents) achieve setting assigned variables, givingcontrol others. gave two different equivalent semantics languagedirect conventional Kripke semantics provided complete axiomatizationthem. key property establishes proof completeness DCL - PCs axiomatic systemfact every formula language provably equivalent normal form: disjunctionconjunctions literals propositional variables p assertions form controls(i, p).also investigated complexity model checking satisfiability problems DCL - PC,showed problems worse program-free fragment CL - PC:PSPACE -complete. demonstrated that, special case ability ATL interpreted( ) CL - PC, implies simpler satisfiability problem ATL.several avenues development work. First all, interestingadd assignments agents perform transfer actions perform,two dimensions agents achieve become projected one dimension. Although parallelexecution program construct language, hence one could still model situationsagent chooses values atoms, time transfers controlatoms, one could least reason effect programs combination truthassignments transfer control sequence, choice. Secondly, many realistic systems, Property (22) may general: often, want specify overall system satisfiesconstraints. this, seems appropriate, however, reason agentsachieve, also guarantee. framework Social Laws (Moses &Tennenholtz, 1995; van der Hoek, Roberts, & Wooldridge, 2005) could set work orderexpress certain conditions, agent set certain propositional variable true,pass control certain variable specific agent, overall system behaves way every agent gets fair chance trigger specific variable (i.e., usespecific resource) infinitely often. Another interesting direction would consider allowfact agents outside transfer program might change variables programexecuting. might require consideration semantics parallel action. Relatedly,would interesting make possible capture temporal properties system, outsidetransfer programs. Here, combination temporal dynamic logic might appropriate.Similarly, could weaken allocation axiom allow propositionalcontrol agents, capturing idea facts modifiable (by agentsconsideration). Another extension would assign control atoms coalitions, ratherindividual agents. could cater power social contexts, typical examplecoalition bigger threshold n lift piano. Finally, implementation theoremprover logic would course interesting. Finally, implementation theorem proverlogic would course interesting.472fiR EASONING BOUT RANSFER C ONTROLAcknowledgments authors wish thank JAIR reviewers editors useful comments. Michael Wooldridge Dirk Walther supported EPSRC grantGR/S62727/01.Appendix A. ProofsTheorem 1.1. schemes Figure 6 derivable DCL - PC.2. axioms K(i), T(i), B(i), effect(i) coalitional counterparts K(C), T(C), B(C),effect(C) derivable coalition C.W3. ` controls(C, p) iC controls(i, p).4. property persistence1 (control) also derivable replace agent arbitrarycoalition C.Proof:1. describe eight schemes Figure 6 derived axiomatic systemDCL - PC .at-least(control), follows directly axiom effect(i), taking = >.at-most(control), `(p) get, using axiom T(i) contraposition, `(p).Assuming moreover `(p), axiom control, gives controls(i, p). allocationobtain controls(j, p) agent j 6= i, and, using control(j), get j pj p, i.e., j `(p) j `(p). Since T(j) gives us j `(p), obtain j `(p), i.e.,2j `(p).prove non-effect(i), assume `(p) controls(i, p). axiom control(i) yields`(p), equivalent 2i `(p).persistence(non-control), right-to-left direction follows immediately T(j).left-to-right direction, assume controls(i, p). allocation derivecontrols(1, p)5 5controls(i 1, p)5controls(i + 1, p)5 5controls(n, p),Wthis, persistence1 (control) get k6=i 2j controls(k, p). every k 6= i,controls(k, p) controls(i, p), follows allocation. Hence, usingNecessitation, 2j (controls(k, p) controls(i, p)). WAxiom K(j),follows 2j controls(k, p) 2j controls(i, p). Combining k6=i 2j controls(k, p),obtain desired conclusion 2j controls(i, p).Notice atomic permanence(;) place requirement p, j. Also,program ;p j, condition hi ;p ji>, func preconditions(transfer),hi ;p ji [i ;p j] equivalent. Formally:hi ;p ji> (hi ;p ji [i ;p j])473(25)fiVAN DERH OEK , WALTHER , & W OOLDRIDGENow, prove objective permanence(;) induction . induction base,propositional variable, follows atomic permanence(;). Consider induction step. Suppose theorem proven take = . Assume hi ; p ji>.show [i ;p j] equivalent showing hi ;p ji.follows (25) induction hypothesis. final step induction,suppose objective permanence(;) proven 1 2 . means assumehi ;p ji> (1 [i ;p j]1 ) (2 [i ;p j]2 )(26)take = 1 2 . Obviously, 1 2 , [i ;p j](1 2 ),proves hi ;p ji> ( [i ;p j]). direction, suppose, givenhi ;p ji>, [i ;p j](1 2 ). use (25) conclude hi ;p ji(1 2 ),classical modal reasoning obtain hi ;p ji1 hi ;p ji2 . (25)induction hypothesis get (1 2 ), concludes proof.Using K( ) Necessitation, derive fact h i> [ ]. Using fact,possible show propositional reasoning objective permanence equivalent[ ] (h i> )(27)proof induction structure transfer program . first caseinduction base, atomic program, holds atomic permanence(;).second case, suppose test ?. Notice that, axiom test( ), h?iequivalent , thus h?i> equivalent . then, (27) equivalenttest( ).Consider induction step = 1 ; 2 . induction hypothesis tells us hi i>([i ] ), objective = 1, 2. Assume h1 ; 2 i>; implies h1 ih2 i>h1 i> comp( ), and, induction hypothesis 1 , ([1 ] ).diamond operator, hence also h1 i, implication 0derivable, also derive h1 h1 0 using Necessitation K( ). Applying induction hypothesis 2 , i.e., h2 i> ([2 ] ), obtainh1 ih2 i> h1 i([2 ] ), and, Modus Ponens, arrive h1 i([2 ]). like demonstrate [1 ; 2 ] . comp( ), equivalent[1 ][2 ] . direction left right, assume [1 ][2 ]. modalprinciple conclude h1 0 h1 i( 0 ) [1 ]. Taking = [2 ]0 = , obtain h1 i. show holds, suppose . still objective formula, apply induction hypothesis conclude [ 1 ]. This,course, contradicts h1 i, indeed conclude . Conversely, suppose . Then,induction hypothesis 1 , also [1 ]. induction hypothesis 2 ,h2 ([2 ] ) implies [2 ], apply necessitation K(1 )derive [1 ] [1 ][2 ].Now, consider = 1 2 , objective permanence proven 1 2 . axiomunion( ), h1 2 i> (h1 i> h2 i>). then, given h1 2 i>,have: [1 2 ] ([1 ] [2 ]), induction hypothesis explainsright-hand side equivalence equivalent .474fiR EASONING BOUT RANSFER C ONTROLFinally, consider = 1 . axiom mix( ), immediately h1 i>([1 ] ). direction, recall induction hypothesisderive, validity, [1 ]. Using Necessitation [1 ] gives [1 ]([1 ]). then, using assumption axiom ind( ) gives us [ 1 ], hencealso h1 i> ( [1 ]).axiom inverse, rely normal form obtained Theorem 2 (the proofinvolve inverse). know every equivalent disjunctionformulas form 1 2 , 1 objective formula, 2 conjunction formulas form controls(h, q). show 1 2 satisfyinverse, result follows arbitrary . assume controls(i, p).precondition(transfer) entails hi ;p ji>, hence apply objectivepermanence(;) func twice conclude 1 [i ;p j; j ;p i]1 .consider formulas 2 , starting base case controls(h, q). Assumecontrols(i, p): first show left right direction. p 6= q, getpersistence2 (control) [i ;p j; j ;p i]q. p = q consider three subcases: (1)h = i. derive, controls(i, p), using transfer, hi ; p jicontrols(j, p)controls(j, p) hj ;p iicontrols(i, p): func comp( ), gives controls(i, p)[i ;p j; j ;p i]controls(i, p). (2) h 6= i, h = j. Given controls(i, p), [i ; pj] , done. (3) h 6= i, h 6= j. use persistence 2 (control)twice get controls(h, q) [i ;p j; j ;p i]controls(h, q). Finally, given controls(i, p),derive right left direction, is, derive [i ;p j; j ;p i]controls(h, q)controls(h, q). First assume p 6= q suppose would controls(h, q). Then,allocation, agent k 6= h, controls(k, q),persistence2 (control) get [i ;p j; j ;p i]controls(k, q), clearly contradicts[i ;p j; j ;p i]controls(h, q). suppose p = q. Again, three subcases. (1)h = i, conclusion follows overall assumption controls(i, p). (2) Supposeh 6= i, h 6= j. reasoning applies case (1). Finally, (3) suppose h 6= i, h = j.Since controls(i, p) given, hi ;p jicontrols(j, p) (by transfer), hencehi ;p j; j ;p iicontrols(i, p). Now, would [i ;p j; j ;p i]controls(j, p),6= j, leads contradiction (use allocation fact h 6= i, h = j),indeed derive [i ;p j; j ;p i]controls(j, p) controls(j, p).reverse, similar inverse.2. proved van der Hoek Wooldridge (2005b).3. definition controls(C,p) C p WC p. Let C = {a1 , a2 , . . . , aC }. axiomWcontrol(i), iC controls(i, p) iC (i p p). contrapositiveT(i), . apply repeatedly agents C, givinga1 a2 aC . is, according Comp-,WW C . (Noteproven contrapositive T(C).) gives us iC controls(i, p) iC (C pC p). Using Comp- again, see consequent implication equivalentC p C p. direction, first show most(control) Figure 6.`(p) get, using axiom T(i) contraposition, `(p). Assuming moreover `(p),axiom control(i), gives controls(i, p). allocation obtain controls(j, p),475fiVAN DERH OEK , WALTHER , & W OOLDRIDGEagent j 6= i. Using control(j), get j p j p, i.e., j `(p) j `(p). Since T(j)gives us j `(p), obtain j `(p), i.e., 2j `(p).WWsuppose iC controls(i, p). allocation, xA\C controls(x, p).means one x, x p x p. case distinction basedp p. first case, assume p, derive, C, 2 p, thus 2C p.Hence C p, get controls(C, p). case p, similarly have,C, 2i p, gives C p, controls(C, p). all, matterwhether p p, get controls(C, p).W4. easy: previous item showed Wcontrols(C, p) means iC controls(i, p).Applying persistence1 (control), get iC 2j controls(i, p). since controls(i, p)controls(C, p) C, also have, C, 2j controls(i, p) 2j controls(C, p)(use Necessitation K(j)). proves 2j controls(C, p).QEDReferencesAlur, R., Henzinger, T. A., & Kupferman, O. (2002). Alternating-time temporal logic. JournalACM, 49(5), 672713.Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal Logic. Cambridge University Press:Cambridge, England.Boella, G., & van der Torre, L. (2006). Delegation power normative multiagent systems.Deontic Logic Artificial Normative Systems, 8th International Workshop DeonticLogic Computer Science, DEON 2006, Utrecht, Netherlands.Boella, G., & van der Torre, L. (2008). Institutions hierarchy authorities distributeddynamic environments. Artificial Intelligence Law, 16(1), 5371.Boutilier, C. (1994). Toward logic qualitative decision theory. Proceedings KnowledgeRepresentation Reasoning (KR&R-94), pp. 7586.Chellas, B. (1980). Modal Logic: Introduction. Cambridge University Press: Cambridge, England.French, T. (2006). Bisimulation Quantifiers Modal Logic. Ph.D. thesis, UniversityWestern Australia, Perth, Australia.Gerbrandy, J. (2006). Logics propositional control. Proceedings Fifth InternationalJoint Conference Autonomous Agents Multiagent Systems (AAMAS-2006), pp. 193200, Hakodate, Japan.Ghilardi, S., & Zawadowski, M. (2000). bisimulation quantifiers classifying toposes.Wolter, F., Wansing, H., de Rijke, M., & Zakharyaschev, M. (Eds.), Advances Modal Logic,pp. 193220.Goranko, V., & Jamroga, W. (2004). Comparing semantics logics multi-agent systems. Synthese, 139(2), 241280. section Knowledge, Rationality Action.Harel, D., Kozen, D., & Tiuryn, J. (2000). Dynamic Logic. MIT Press: Cambridge, MA.476fiR EASONING BOUT RANSFER C ONTROLJamroga, W., & van der Hoek, W. (2004). Agents know play. Fundamenta Informaticae,63(2-3), 185219.Jones, A. J. I., & Sergot, M. (1996). formal characterisation institutionalised power. LogicJournal IGPL, 3, 427443.Li, N., Grosof, B. N., & Feigenbaum, J. (2003). Delegation logic: logic-based approach distributed authorization. ACM Transactions Information System Security, 6(1), 128171.Moses, Y., & Tennenholtz, M. (1995). Artificial social systems. Computers AI, 14(6), 533562.Norman, T. J., & Reed, C. (2002). Group delegation responsibility. Proceedings FirstInternational Joint Conference Autonomous Agents Multiagent Systems (AAMAS2002), pp. 491498, Bologna, Italy.Pauly, M. (2001). Logic Social Software. Ph.D. thesis, University Amsterdam. ILLC Dissertation Series 2001-10.Sauro, L. (2006). Formalizing Admissibility Criteria Coalition Formation among Goal DirectedAgents. Ph.D. thesis, University Turin, Turin, Italy.Savitch, W. J. (1970). Relationships nondeterministic deterministic tape complexities.Journal Computer Systems Sciences, 4(2), 177192.van Benthem, J., Girard, P., & Roy, O. (2009). Everything else equal: modal logicceteris paribus preferences. Journal Philosophical Logic, 38, 83125.van der Hoek, W., Roberts, M., & Wooldridge, M. (2005). Knowledge social laws. Dignum,F., Dignum, V., Koenig, S., Kraus, S., Singh, M., & Wooldridge, M. (Eds.), ProceedingsFourth International Joint Conference Autonomous Agents Multi-Agent Systems(AAMAS 05), pp. 674681, New York, USA. ACM Inc.van der Hoek, W., & Wooldridge, M. (2003). Time, knowledge, cooperation: Alternating-timetemporal epistemic logic applications. Studia Logica, 75(1), 125157.van der Hoek, W., & Wooldridge, M. (2005a). dynamics delegation, cooperation,control: logical account. Dignum, F., Dignum, V., Koenig, S., Kraus, S., Singh, M.,& Wooldridge, M. (Eds.), Proceedings Fourth International Joint Conference Autonomous Agents Multi-Agent Systems (AAMAS 05), pp. 701708, New York, USA. ACMInc.van der Hoek, W., & Wooldridge, M. (2005b). logic cooperation propositional control.Artificial Intelligence, 64, 81119.Venema, Y. (1995). Cylindric modal logic. Journal Symbolic Logic, 60, 591623.477fiJournal Artificial Intelligence Research 37 (2010) 397-435Submitted 11/09; published 03/10Training Multilingual Sportscaster:Using Perceptual Context Learn LanguageDavid L. ChenJoohyun KimRaymond J. MooneyDLCC @ CS . UTEXAS . EDUSCIMITAR @ CS . UTEXAS . EDUMOONEY @ CS . UTEXAS . EDUDepartment Computer ScienceUniversity Texas Austin1 University Station C0500, Austin TX 78712, USAAbstractpresent novel framework learning interpret generate language using perceptual context supervision. demonstrate capabilities developing system learnssportscast simulated robot soccer games English Korean without language-specificprior knowledge. Training employs ambiguous supervision consisting stream descriptive textual comments sequence events extracted simulation trace. systemsimultaneously establishes correspondences individual comments eventsdescribe building translation model supports parsing generation. alsopresent novel algorithm learning events worth describing. Human evaluationsgenerated commentaries indicate reasonable quality cases even parproduced humans limited domain.1. Introductioncurrent natural language processing (NLP) systems built using statistical learning algorithms trained large annotated corpora. However, annotating sentences requisite parsetrees (Marcus, Santorini, & Marcinkiewicz, 1993), word senses (Ide & Jeronis, 1998) semanticroles (Kingsbury, Palmer, & Marcus, 2002) difficult expensive undertaking. contrast,children acquire language exposure linguistic input context rich, relevant,perceptual environment. Also, connecting words phrases objects events world,semantics language grounded perceptual experience (Harnad, 1990). Ideally, machinelearning system would able acquire language similar manner without explicit human supervision. step direction, present system describe events simulatedsoccer game learning sample language commentaries paired traces simulatedactivity without language-specific prior knowledge. screenshot system generatedcommentary shown Figure 1.fair amount research grounded language learning (Roy, 2002;Bailey, Feldman, Narayanan, & Lakoff, 1997; Barnard, Duygulu, Forsyth, de Freitas, Blei, & Jordan, 2003; Yu & Ballard, 2004; Gold & Scassellati, 2007), focus dealingraw perceptual data rather language issues. Many systems aimed learn meanings words phrases rather interpreting entire sentences. recent work dealtfairly complex language data (Liang, Jordan, & Klein, 2009; Branavan, Chen, Zettlemoyer, &c2010AI Access Foundation. rights reserved.fiC HEN , K IM , & OONEYFigure 1: Screenshot commentator systemBarzilay, 2009) address three problems alignment, semantic parsing, naturallanguage generation. contrast, work investigates build complete language learningsystem using parallel data perceptual context. study problem simulated environment retains many important properties dynamic world multiple agentsactions avoiding many complexities robotics computer vision. Specifically,use RoboCup simulator (Chen, Foroughi, Heintz, Kapetanakis, Kostiadis, Kummeneje, Noda,Obst, Riley, Steffens, Wang, & Yin, 2003) provides fairly detailed physical simulationrobot soccer. several groups constructed RoboCup commentator systems (Andre,Binsted, Tanaka-Ishii, Luke, Herzog, & Rist, 2000) provide textual natural-language (NL)transcript simulated game, systems use manually-developed templates basedlearning.commentator system learns semantically interpret generate language RoboCupsoccer domain observing on-going commentary game paired evolving simulator state. exploiting existing techniques abstracting symbolic description activityfield detailed states physical simulator (Andre et al., 2000), obtain pairingnatural language symbolic description perceptual context uttered.However, training data highly ambiguous comment usually co-occurs several events game. integrate enhance existing methods learning semantic parsersNL generators (Kate & Mooney, 2007; Wong & Mooney, 2007) order learn understandgenerate language ambiguous training data. also develop system that,ambiguous training data, learns events worth describing, also performstrategic generation, is, deciding say well say (tactical generation). 11. conciseness, use terminology early work generation (e.g., McKeown, 1985). Strategic tacticalgeneration also commonly referred content selection surface realization, respectively398fiT RAINING ULTILINGUAL PORTSCASTERevaluate system demonstrate language-independence training generatecommentaries English Korean. Experiments test data (annotated evaluation purposes only) demonstrate system learns accurately semantically parse sentences, generatesentences, decide events describe. Finally, subjective human evaluation commentated game clips demonstrate limited domain, system generates sportscastscases similar quality produced humans.three main contributions make paper. First, explore possibilitylearning grounded language models perceptual context form ambiguous paralleldata. Second, investigate several different methods disambiguating data determinedusing combined score includes tactical strategic generation scores performedbest overall. Finally, built complete system learns sportscast multiple languages.carefully verified automatic human evaluations system able performseveral tasks including disambiguating training data, semantic parsing, tactical strategicgeneration. language involved work restricted compared handcrafted commercial sportscasting systems, goal demonstrate feasibility learning groundedlanguage system language-specific prior knowledge.remainder paper structured follows. Section 2 provides background previous work utilize extend build system. Section 3 describes sportscastingdata collected train test approach. Section 4 Section 5 present detailsbasic methods learning tactical strategic generation, respectively, initial experimental results. Section 6 discusses extensions basic system incorporate informationstrategic generation process disambiguating training data. Section 7 presents experimental results initializing system data disambiguated recent method aligninglanguage facts may refer. Section 8 discusses additions try detect superfluous sentences refer extracted event. Section 9 presents human evaluationautomatically generated sportscasts. Section 10 reviews related work, Section 11 discusses futurework, Section 12 presents conclusions.2. BackgroundSystems learning semantic parsers induce function maps natural-language (NL) sentencesmeaning representations (MRs) formal logical language. Existing work focusedlearning supervised corpus sentence manually annotated correct MR(Mooney, 2007; Zettlemoyer & Collins, 2007; Lu, Ng, Lee, & Zettlemoyer, 2008; Jurcicek, Gasic,Keizer, Mairesse, Thomson, & Young, 2009). human annotated corpora expensivedifficult produce, limiting utility approach. Kate Mooney (2007) introducedextension one system, K RISP (Kate & Mooney, 2006), learn ambiguoustraining data requires little human annotation effort. However, system unablegenerate language required sportscasting task. Thus, enhanced another systemcalled WASP (Wong & Mooney, 2006) capable language generation well semanticparsing similar manner allow learn ambiguous supervision. briefly describeprevious systems below. systems assume access formal deterministiccontext-free grammar (CFG) defines formal meaning representation language (MRL). SinceMRLs formal computer-interpretable languages, grammar usually easily available.399fiC HEN , K IM , & OONEY2.1 KRISP KRISPERK RISP (Kernel-based Robust Interpretation Semantic Parsing) (Kate & Mooney, 2006) usessupport vector machines (SVMs) string kernels build semantic parsers. SVMs state-ofthe-art machine learning methods learn maximum-margin separators prevent over-fittinghigh-dimensional data natural language text (Joachims, 1998). extendednon-linear separators non-vector data exploiting kernels implicitly create even higherdimensional space complex data (nearly) linearly separable (Shawe-Taylor & Cristianini,2004). Recently, kernels strings trees effectively applied variety problemstext learning NLP (Lodhi, Saunders, Shawe-Taylor, Cristianini, & Watkins, 2002; Zelenko,Aone, & Richardella, 2003; Collins, 2002; Bunescu & Mooney, 2005). particular, K RISP usesstring kernel introduced Lodhi et al. (2002) classify substrings NL sentence.First, K RISP learns classifiers recognize word phrase NL sentence indicatesparticular concept MRL introduced MR. uses production rulesMRL grammar represent semantic concepts, learns classifiers productionclassify NL substrings indicative production not. semantically parsingsentence, classifier estimates probability production covering different substringssentence. information used compositionally build complete MRsentence. Given partial matching provided string kernels over-fitting preventionprovided SVMs, K RISP experimentally shown particularly robust noisy trainingdata (Kate & Mooney, 2006).K RISPER (Kate & Mooney, 2007) extension K RISP handles ambiguous trainingdata, sentence annotated set potential MRs, one correct.Psuedocode method shown Algorithm 1. employs iterative approach analogousexpectation maximization (EM) (Dempster, Laird, & Rubin, 1977) improves upon selectioncorrect NLMR pairs iteration. first iteration (lines 3-9), assumesMRs paired sentence correct trains K RISP resulting noisy supervision.subsequent iterations (lines 11-27), K RISPER uses currently trained parser scorepotential NLMR pair, selects likely MR sentence, retrains parserresulting disambiguated supervised data. manner, K RISPER able learn typeweak supervision expected grounded language learner exposed sentences ambiguouscontexts. However, system previously tested artificially corrupted generateddata.2.2 WASP WASP1WASP (Word-Alignment-based Semantic Parsing) (Wong & Mooney, 2006) uses state-of-the-artstatistical machine translation (SMT) techniques (Brown, Cocke, Della Pietra, Della Pietra, Jelinek,Lafferty, Mercer, & Roossin, 1990; Yamada & Knight, 2001; Chiang, 2005) learn semanticparsers. SMT methods learn effective machine translators training parallel corpora consistinghuman translations documents one alternative natural languages. resultingtranslators typically significantly effective manually developed systems SMTbecome dominant approach machine translation. Wong Mooney (2006) adaptedmethods learn translate NL MRL rather one NL another.First, SMT word alignment system, GIZA++ (Och & Ney, 2003; Brown, Della Pietra, DellaPietra, & Mercer, 1993), used acquire bilingual lexicon consisting NL substrings coupled400fiT RAINING ULTILINGUAL PORTSCASTERAlgorithm 1 K RISPERinput sentences associated sets meaning representations R(s)output BestExamplesSet, set NL-MR pairs,SemanticModel , K RISP semantic parser1:2:3:4:5:6:7:8:9:main//Initial training loopsentence simeaning representation mj R(si )add (si , mj ) InitialTrainingSetendendSemanticModel = Train(InitialTrainingSet)10:11:12:13:14:15:16:17:18:19:20:21://Iterative retrainingrepeatsentence simeaning representation mj R(si )mj .score = Evaluate(si , mj , SemanticModel )endendBestExampleSetset consistent examples = {(s, m)|s S, MR(s)}Pm.score maximizedSemanticModel = rain(BestExamplesSet)Convergence MAX ITER reachedend main22:function Train(TrainingExamples)Train K RISP unambiguous TrainingExamples25:return trained K RISP semantic parser26: end function23:24:27:function Evaluate(s, m, SemanticModel )Use K RISP semantic parser SemanticModel find derivation meaning representation sentence30:return parsing score31: end function28:29:401fiC HEN , K IM , & OONEYtranslations target MRL. formal languages, MRLs frequently contain manypurely syntactic tokens parentheses brackets, difficult align wordsNL. Consequently, found much effective align words NL productionsMRL grammar used parse corresponding MR. Therefore, GIZA++ usedproduce N 1 alignment words NL sentence sequence MRLproductions corresponding top-down left-most derivation corresponding MR.Complete MRs formed combining NL substrings translations usinggrammatical framework called synchronous CFG (SCFG) (Aho & Ullman, 1972), formsbasis existing syntax-based SMT (Yamada & Knight, 2001; Chiang, 2005). SCFG,right hand side production rule contains two strings, case one NLMRL. Derivations SCFG simultaneously produce NL sentences corresponding MRs.bilingual lexicon acquired word alignments training data used construct setSCFG production rules. probabilistic parser produced training maximum-entropymodel using EM learn parameters SCFG productions, similar methodsused Riezler, Prescher, Kuhn, Johnson (2000), Zettlemoyer Collins (2005).translate novel NL sentence MR, probabilistic chart parser (Stolcke, 1995) used findprobable synchronous derivation generates given NL, corresponding MRgenerated derivation returned.Since SCFGs symmetric, used generate NL MR well parse NLMR (Wong & Mooney, 2007). allows learned grammar used parsinggeneration, elegant property important advantages (Shieber, 1988). generationsystem, WASP1 , uses noisy-channel model (Brown et al., 1990):arg max Pr(e|f ) = arg max Pr(e) Pr(f |e)e(1)ee refers NL string generated given input MR, f . Pr(e) language model,Pr(f |e) parsing model provided WASPs learned SCFG. generation task findsentence e (1) e good sentence priori, (2) meaning inputMR. language model, use standard n-gram model, useful ranking candidategenerated sentences (Knight & Hatzivassiloglou, 1995).3. Sportscasting Datatrain test system, assembled human-commentated soccer games RoboCupsimulation league (www.robocup.org). Since focus language learning computer vision, chose use simulated games instead real game video simplify extractionperceptual information. Based ROCCO RoboCup commentators incremental event recognition module (Andre et al., 2000) manually developed symbolic representations game eventsrule-based system automatically extract simulator traces. extractedevents mainly involve actions ball, kicking passing, also includegame information whether current playmode kickoff, offside, corner kick.events represented atomic formulas predicate logic timestamps. logical factsconstitute requisite MRs, manually developed simple CFG formal semanticlanguage. Details events detected complete grammar found Appendix A.NL portion data, humans commentate games watchingsimulator. collected commentaries English Korean. English commentaries402fiT RAINING ULTILINGUAL PORTSCASTERTotal # commentsTotal # wordsVocabulary sizeAvg. words per commentEnglish dataset2036117424545.77Korean dataset199979413443.97Table 1: Word statistics English Korean datasetsNumber eventsTotal2001 final2002 final2003 final2004 final40032223211323187225144103902001 final2002 final2003 final2004 final4003222321132318673454412460Number commentsMRs Correct MREnglish dataset671520458376397320342323Korean dataset650600444419396369423375Events per commentMax Average Std. Dev.9101292.2352.4032.8492.7291.6411.6532.0511.69710121092.1382.4892.5512.6012.0763.0833.6722.593Table 2: Alignment statistics English Korean datasets. comments correct meaning representations associated essentially noise trainingdata (18% English dataset 8% Korean dataset). Moreover, average2 possible events linked comment half linksincorrect.produced two different people Korean commentaries produced single person.commentators typed comments text box, recorded timestamp.construct final ambiguous training data, paired comment eventsoccurred five seconds less comment made. Examples ambiguous trainingdata shown Figure 2. edges connect sentences events might refer. Englishtranslations Korean commentaries included figure readers benefitpart actual data. Note use English words predicates constantsMRs human readability only, system treats arbitrary conceptual tokens mustlearn connection English Korean words.annotated total four games, namely, finals RoboCup simulation leagueyear 2001 2004. Word statistics data shown Table 1.sentences fairly short due nature sportscasts, data provides challenges formsynonyms (e.g. Pink1, PinkG pink goalie refer player) polysemes(e.g. kick kicks toward goal refers kick event whereas kicks Pink3 referspass event.) Alignment statistics datasets shown Table 2. 2001 final almosttwice number events games went double overtime.403fiC HEN , K IM , & OONEYNatural Language CommentaryMeaning RepresentationbadPass ( PurplePlayer1 ,PinkPlayer8 )turnover ( PurplePlayer1 ,PinkPlayer8 )kick ( PinkPlayer8 )pass ( PinkPlayer8 , PinkPlayer11 )kick ( PinkPlayer11 )Purple goalie turns ballPink8Purple team sloppy todayPink8 passes Pink11Pink11 looks around teammatekick ( PinkPlayer11 )ballstoppedkick ( PinkPlayer11 )pass ( PinkPlayer11 , PinkPlayer8 )kick ( PinkPlayer8 )pass ( PinkPlayer8 , PinkPlayer11 )Pink11 makes long pass Pink8Pink8 passes back Pink11(a) Sample trace ambiguous English training dataNatural Language CommentaryMeaning Representation10 11 .(purple10 passes purple 11)kick ( PurplePlayer10 )11 10 .(purple11 passes purple 10)kick ( PurplePlayer11 )10 3 .(pink3 steals ball purple 10)steal ( PinkPlayer3 )3 .(pink3 passes pink goalie)kick ( PinkPlayer3 )pass ( PurplePlayer10 , PurplePlayer11 )pass ( PurplePlayer11 , PurplePlayer10 )turnover ( PurplePlayer10 , PinkPlayer3 )playmode ( free_kick_r )(b) Sample trace ambiguous Korean training dataFigure 2: Examples training data. outgoing edges comments indicatepossibly associated meaning representations considered system. bold linksindicate correct matches comments meaning representations.404fiT RAINING ULTILINGUAL PORTSCASTERevaluation purposes only, gold-standard matching produced examining comment manually selecting correct MR exists. matching approximatesometimes comments contain information present MRs. example, comment might describe location length pass MR captures participantspass. bold lines Figure 2 indicate annotated correct matches sample data. Notice sentences correct matches (about one fifth English data one tenthKorean data). example, sentence Purple team sloppy today Figure 2(a)cannot represented MRL consequently corresponding correct MR.another example, Korean sentence translation pink3 passes pink goalie Figure 2(b) represented MRL, correct match due incompleteevent detection. free kick called pink3 passing pink goalie pass eventretrieved. Finally, case sentence Pink11 makes long pass Pink8 Figure 2(a), correct MR falls outside 5-second window. game, Table 2 showstotal number NL sentences, number least one recent extracted eventcould refer, number actually refer one recent extractedevents. maximum, average, standard deviation number recent events pairedcomment also given.4. Learning Tactical Generation Ambiguous Supervisionexisting systems capable solving parts sportscasting problem, noneable perform whole task. need system deal ambiguous supervisionlike K RISPER generate language like WASP. introduce three systemsboth. overview differences existing systems new systems presentshown Table 3.three systems introduced based extensions WASP, underlying languagelearner. main problem need solve disambiguate training datatrain WASP create language generator. new system uses differentdisambiguation criteria determine best matching NL sentences MRs.4.1 WASPERfirst system extension WASP manner similar K RISP extended createK RISPER. uses EM-like retraining handle ambiguously annotated data, resulting systemcall WASPER. general, system learns semantic parsers extended handleambiguous data long produce confidence levels given NLMR pairs. Given setsentences set MRs associated sentence R(s), disambiguatedata finding pairs (s, m), R(s) = arg maxm P r(m|s). Althoughprobability used here, ranking relative potential parses would suffice. pseudocodeWASPER shown Algorithm 2. difference compared K RISPER pseudocodeuse WASP semantic parser instead K RISP parser. Also, produce WASPlanguage generator well desired final output task.405fiC HEN , K IM , & OONEYAlgorithmUnderlying learnerK RISPK RISPERWASPSVMK RISPGIZA align words,MR tokens,learn probalistic SCFGWASPFirst disambiguateK RISPER,train WASPWASPWASPERK RISPER -WASPWASPER -G ENGenerate?Disambiguation criteriaYesAmbiguousdata?YesYesYesYesYesWASPs parsing scoreK RISPs parsing scoreYesYesNIST scorebest NL given MRn/aK RISPs parsing scoren/aTable 3: Overview various learning systems presented. first three algorithms existingsystems. introduce last three systems able learn ambiguoustraining data acquire language generator. differ disambiguatetraining data.Algorithm 2 WASPERinput sentences associated sets meaning representations R(s)output BestExamplesSet, set NL-MR pairs,SemanticModel , WASP semantic parser/language generator1: main2:Algorithm 13: end main4:function Train(TrainingExamples)Train WASP unambiguous TrainingExamples7:return trained WASP semantic parser/language generator8: end function5:6:9:function Evaluate(s, m, SemanticModel )11:Use WASP semantic parser SemanticModel find derivation meaning representation sentence12:return parsing score13: end function10:406fiT RAINING ULTILINGUAL PORTSCASTER4.2 KRISPER-WASPK RISP shown quite robust handling noisy training data (Kate & Mooney, 2006).important training noisy training data used initialize parserK RISPERs first iteration. However, K RISPER cannot learn language generator, necessary sportscasting task. result, create new system called K RISPER-WASPgood disambiguating training data capable generation. first use K RISPERtrain ambiguous data produce disambiguated training set using predictionlikely MR sentence. unambiguous training set used train WASPproduce parser generator.4.3 WASPER-GENK RISPER WASPER, criterion selecting best NLMR pairs retraining based maximizing probability parsing sentence particular MR. However,since WASPER capable parsing generation, could alternatively select bestNLMR pairs evaluating likely generate sentence particular MR. Thus,built another version WASPER called WASPER-G EN disambiguates training dataorder maximize performance generation rather parsing. pseudocode shownAlgorithm 3. algorithm WASPER except evaluation function. usesgeneration-based score rather parsing-based score select best NLMR pairs.Specifically, NLMR pair (s, m) scored computing NIST score, machine translation (MT) metric, sentence best generated sentence (lines 9-12).2Formally, given set sentences set MRs associated sentenceR(s), disambiguate data finding pairs (s, m), R(s) =arg maxm N IST (s, argmaxs P r(s |m)).NIST measures precision translation terms proportion n-grams shareshuman translation (Doddington, 2002). also used evaluate NL generation. Anotherpopular MT metric BLEU score (Papineni, Roukos, Ward, & Zhu, 2002), inadequatepurpose since comparing one short sentence another instead comparing wholedocuments. BLEU score computes geometric mean n-gram precision value n,means score 0 matching n-gram found every value n. commonsetting maximum n 4, two sentences matching 4-gram wouldreceive BLEU score 0. Consequently, BLEU score unable distinguish qualitygenerated sentences since fairly short. contrast, NIST uses additive scoreavoids problem.4.4 Experimental Evaluationsection presents experimental results RoboCup data four systems: K RISPER, WASPER,K RISPER-WASP, WASPER-G EN. Since aware existing systems couldlearn semantically parse generate language using ambiguous supervision based perceptual context, constructed lower upper baselines using unmodified WASP. Since2. natural way use generation-based score would use probability NL given MR (P r(s|m)).However, initial experiments using metric produce good results. also tried changing WASPmaximize joint probability instead parsing probability. However, also improve results.407fiC HEN , K IM , & OONEYAlgorithm 3 WASPER -G ENinput sentences associated sets meaning representations R(s)output BestExamplesSet, set NL-MR pairs,SemanticModel , WASP semantic parser/language generator1: main2:Algorithm 13: end main4:function Train(TrainingExamples)6:Algorithm 27: end function5:8:function Evaluate(s, m, SemanticModel )GeneratedSentence Use WASP language generator SemanticModel producesentence meaning representation11:return NIST score GeneratedSentence12: end function9:10:WASP requires unambiguous training data, randomly pick meaning sentenceset potential MRs serve lower baseline. use WASP trained gold matchingconsists correct NLMR pairs annotated human upper baseline. represents upper-bound systems could achieve disambiguated training dataperfectly.evaluate system three tasks: matching, parsing, generation. matching taskmeasures well systems disambiguate training data. parsing generation tasksmeasure well systems translate NL MR, MR NL, respectively.Since four games total, trained using possible combinations one threegames. matching, measured performance training data since goal disambiguate data. parsing generation, tested games used training.Results averaged train/test combinations. evaluated matching parsing usingF-measure, harmonic mean recall precision. Precision fraction systemsannotations correct. Recall fraction annotations gold-standardsystem correctly produces. Generation evaluated using BLEU scores roughly estimates well produced sentences match target sentences. treat gamewhole document avoid problem using BLEU score sentence-level comparisons mentioned earlier. Also, increase number reference sentences MR usingsentences test data corresponding equivalent MRs, e.g. pass(PinkPLayer7,PinkPlayer8) occurs multiple times test data, sentences matched MRgold matchings used reference sentences MR.4.4.1 ATCHING NLMRSince handling ambiguous training data important aspect grounded language learning,first evaluate well various systems pick correct NLMR pairs. Figure 3 shows Fmeasure identifying correct set pairs various systems. learning systems408fi0.80.80.750.750.70.70.650.65F-measureF-measureRAINING ULTILINGUAL PORTSCASTER0.60.550.60.550.50.50.450.45WASPERWASPER-GENKRISPERrandom matching0.4WASPERWASPER-GENKRISPERrandom matching0.40.350.3512Number Training Games31(a) English2Number Training Games3(b) Korean0.90.90.80.80.70.7F-measureF-measureFigure 3: Matching results basic systems. WASPER -G EN performs best, outperformingexisting system K RISPER datasets.0.60.5WASP gold matchingKRISPERKRISPER-WASPWASPERWASPER-GENWASP random matching0.40.30.60.5WASP gold matchingKRISPERKRISPER-WASPWASPERWASPER-GENWASP random matching0.40.30.20.212Number Training Games31(a) English2Number Training Games3(b) KoreanFigure 4: Semantic parsing results basic systems. results largely mirrorsmatching results WASPER -G EN performing best overall.perform significantly better random F-measure 0.5. EnglishKorean data, WASPER -G EN best system. WASPER also equals outperforms previoussystem K RISPER well.4.4.2 EMANTIC PARSINGNext, present results accuracy learned semantic parsers. trained systemused parse produce MR sentence test set correct MRgold-standard matching. parse considered correct matches gold standardexactly. Parsing fairly difficult task usually one way describeevent. example, Player1 passes player2 refer event Player1 kicksball player2. Thus, accurate parsing requires learning different ways people describe409fi0.50.60.450.550.40.50.350.45BLEUBLEUC HEN , K IM , & OONEY0.30.250.40.35WASP gold matchingKRISPER-WASPWASPERWASPER-GENWASP random matching0.20.15WASP gold matchingKRISPER-WASPWASPERWASPER-GENWASP random matching0.30.250.10.212Number Training Games31(a) English2Number Training Games3(b) KoreanFigure 5: Tactical generation results basic systems. relative performancesvarious systems change, WASPER -G EN still best system.event. Synonymy limited verbs. data, Pink1, PinkG pink goalierefer player1 pink team. Since providing systems prior knowledge,learn different ways referring entity.parsing results shown Figure 4 generally correlate well matching results. Systems better disambiguating training data also better parsingsupervised training data less noisy. WASPER-G EN best overall English Korean data. interesting note K RISPER relatively well English datacompared matching performance. K RISP robust noise WASP(Kate & Mooney, 2006) even though trained noisier set data WASPER -G ENstill produced comparable parser.4.4.3 G ENERATIONthird evaluation task generation. WASP-based systems given MR testset gold-standard matching NL sentence asked generate NL description.quality generated sentence measured comparing gold-standard using BLEUscoring.task tolerant noise training data parsing systemneeds learn one way accurately describe event. property reflected results,shown Figure 5, even baseline system, WASP random matching, fairly well,outperforming K RISPER-WASP datasets WASPER Korean data. numberevent types fairly small, relatively small number correct matchings requiredperform task well long event type associated correct sentence patternoften sentence pattern.two tasks, WASPER -G EN best system task. One possible explanation WASPER-G ENs superior performance stems disambiguation objective function.Systems like WASPER K RISPER-WASP use parsing scores attempt learn good translation model sentence pattern. hand, WASPER-G EN tries learn good410fiT RAINING ULTILINGUAL PORTSCASTERtranslation model MR pattern. Thus, WASPER-G EN likely converge goodmodel fewer MR patterns sentence patterns. However, argued learninggood translation models sentence pattern help producing varied commentaries,quality captured BLEU score. Another possible advantage WASPER -G ENuses softer scoring function. probabilities parsing particular sentenceMR sensitive noise training data, WASPER -G EN looks top generatedsentences MR. Even noise data, top generated sentence remains relativelyconstant. Moreover, minor variations sentence change results dramatically sinceNIST score allows partial matching.5. Learning Strategic Generationlanguage generator alone enough produce sportscast. addition tactical generationdeciding say something, sportscaster must also preform strategic generationchoosing say (McKeown, 1985).developed novel method learning events describe. event type (i.e.predicate like pass, goal), system uses training data estimate probabilitymentioned sportscaster. Given gold-standard NLMR matches, probabilityeasy estimate; however, learner know correct matching. Instead, systemmust estimate probabilities ambiguous training data. compare two basic methodsestimating probabilities.first method uses inferred NLMR matching produced language-learning system.probability commenting event type, ei , estimated percentage eventstype ei matched NL sentence.second method, call Iterative Generation Strategy Learning (IGSL), uses variant EM, treating matching assignments hidden variables, initializing matchprior probability, iterating improve probability estimates commenting eventtype. Unlike first method, IGSL uses information MRs explicitly associatedsentence training. Algorithm 4 shows pseudocode. main loop alternates twosteps:1. Calculating expected probability NLMR matching given current modellikely event commented (line 6)2. Update prior probability event type mentioned human commentator basedmatchings (line 9).first iteration, NLMR match assignedprobability inversely proportionalPamount ambiguity associated sentence ( eEvent(s) Pr (e) = |Event(s)|). example,sentence associated five possible MRs assign match probability 51 . priorprobability mentioning event type estimated average probability assignedinstances event type. Notice process always guarantee proper probability sinceMR associated multiple sentences. Thus, limit probability one.subsequent iterations, probabilities NLMR matchings updated accordingnew priors. assign match prior probability event type normalized acrossassociated MRs NL sentence. update priors event type using411fiC HEN , K IM , & OONEYAlgorithm 4 Iterative Generation Strategy Learninginput event types E = {e1 , ..., en }, number occurrences event type otalCount(ei )entire game trace, sentences event types associated meaning representations Event(s)output probabilities commenting event type P r(ei )1: Initialize Pr (ei ) = 12: repeat3:event type ei E4:MatchCount = 05:sentencePPr (e)P6:ProbOfMatch = eEvent(s)e=ePr (e)eEvent(s)7:8:9:10:11:MatchCount = MatchCount + ProbOfMatchendMatchCountPr (ei ) = min( TotalCount(e,1) {Ensure proper probabilities}i)endConvergence MAX ITER reached!"#$%&&'()*+*,-,%.&)/&*#,$0& 4)(2+-,5#3&6()*+*,-,%.&1)22#$%#3&)$&*+--7%)66#3&89:;&<&8:=>?&?9@:&<&8:=>A&*+3'+77B6C(6-#DE6,$F?G&:9;D:&:9?8A&%C($)"#(B6C(6-#DE6,$F?G&:9;:;&:9H@H&%C($)"#(B6C(6-#DE6,$F?G&:9;:;&:9:;8&IJ#&7#-#1%#3&#"#$%&,7&"#(*+-,5#3&&K$&#"#$%&,7&7#-#1%#3&*+7#3&&)$&%J#&$)(2+-,5#3&6()*+*,-,%.& (+$3)2-.&+11)(3,$0&%)&,%7&6()*+*,-,%.&&)/&*#,$0&1)22#$%#3&)$&Figure 6: example strategic generation component works. every timestep,stochastically select event events occurring moment.decide whether verbalize selected event based IGSLs estimated probabilitycommented upon.new estimated probabilities matchings. process repeated probabilitiesconverge pre-specified number iterations occurred.generate sportscast, use learned probabilities determine events describe.time step, first determine events occurring time. select onerandomly based normalized probabilities. avoid overly verbose, wantmake comment every time something happening, especially event rarely commented on.Thus, stochastically decide whether comment selected event based probability.example process shown Figure 6.412fi0.80.80.70.70.60.6F-measureF-measureRAINING ULTILINGUAL PORTSCASTER0.5inferred gold matchingIGSLinferred KRISPERinferred WASPERinferred WASPER-GENinferred random matching0.40.5inferred gold matchingIGSLinferred KRISPERinferred WASPERinferred WASPER-GENinferred random matching0.40.30.312Number Training Games31(a) English2Number Training Games3(b) KoreanFigure 7: Strategic generation results various systems. novel algorithm IGSL performsbest, almost par upper bound uses gold-annotated matchings.eventballstoppedkickpassturnoverbadPass# occurrences581721221069566371% commented1.72 1040.0 330.9990.2140.429IGSL1.09 1050.0180.9830.9090.970inferred WASPER -G EN0.0160.1170.8000.3530.493Table 4: Top 5 frequent events, % times commented on, probabilitieslearned top algorithms English data5.1 Experimental Evaluationdifferent methods learning strategic generation evaluated based often eventsdescribe test data coincide human decided describe. firstapproach, results using inferred matchings produced K RISPER, WASPER, WASPER-G ENwell gold random matching establishing baselines presented Figure 7.graph, clear IGSL outperforms learning inferred matchings actuallyperforms level close using gold matching. However, important notelimiting potential learning gold matching using predicates decidewhether talk event.English data, probabilities learned IGSL inferred matchings WASPER G EN five frequently occurring events shown Table 4. WASPER -G EN learnsfairly good probabilities general, well IGSL frequent events.IGSL uses occurrences events associated possible commentstraining iterations. Rarely commented events ballstopped kick often occur withoutcomments uttered. Consequently, IGSL assigns low prior probabilitieslowers chances matched sentences. hand, WASPER -G ENuse priors sometimes incorrectly matches comments them. Thus, using inferred413fiC HEN , K IM , & OONEYmatches WASPER -G EN results learning higher probabilities commenting rarelycommented events.methods use predicates MRs decide whether comment not,perform quite well data collected. particular, IGSL performs best, usestrategic generation rest paper.6. Using Strategic Generation Improve Matchingsection, explore knowledge learned strategic generation used improveaccuracy matching sentences MRs. previous section, described several wayslearn strategic generation, including IGSL learns directly ambiguous training data.Knowing events people tend talk also help resolve ambiguities trainingdata. Events likely discussed also likely matchedNL sentence disambiguating training data. Therefore, section describes methodsintegrate strategic generation scores (such Table 4) scoring NLMR pairs usedmatching process.6.1 WASPER-GEN-IGSLWASPER -G EN -IGSL extension WASPER -G EN also uses strategic generation scoresIGSL. WASPER -G EN uses NIST score pick best MR sentence finding MRgenerates sentence closet actual NL sentence. WASPER -G EN -IGSL combines tactical(NIST) strategic (IGSL) generation scores pick best NLMR pairs. simply multipliesNIST score IGSL score together form composite score. new score biasesselection matching pairs include events IGSL determines are, priori, likelydiscussed. helpful, especially beginning WASP produceparticularly good language generator. many instances, generated sentencespossible MRs equally bad overlap target sentence. Even generationproduces perfectly good sentence, generation score unreliable comparingsingle sentence single reference often short well. Consequently, oftendifficult WASPER-G EN distinguish among several MRs equal scores. hand,event types different strategic generation scores, default choosingMR higher prior probability mentioned. Algorithm 5 shows pseudocodeWASPER -G EN -IGSL.6.2 Variant WASPER-GEN SystemsAlthough WASPER -G EN uses NIST score estimate goodness NLMR pairs, could easilyuse MT evaluation metric. already discussed unsuitability BLEU comparing short individual sentences since assigns zero many pairs. However, NIST score alsolimitations. example, normalized, may affect performance WASPER -G EN IGSL combined IGSL score. Another limitation comes using higher-orderN-grams. Commentaries domain often short, frequently higher-orderN-gram matches generated sentences target NL sentences.METEOR metric (Banerjee & Lavie, 2005) designed resolve various weaknessesBLEU NIST metrics, focused word-to-word matches reference414fiT RAINING ULTILINGUAL PORTSCASTERAlgorithm 5 WASPER -G EN -IGSLinput sentences associated sets meaning representations R(s)output BestExamplesSet, set NL-MR pairs,SemanticModel , WASP semantic parser/language generator1: main2:Algorithm 13: end main4:function Train(TrainingExamples)6:Algorithm 27: end function5:8:9:10:11:12:13:14:15:function Evaluate(s, m, SemanticModel )Call Algorithm 4 collect IGSL scoresGeneratedSentence Use WASP language generator SemanticModel producesentence meaning representationTacticalGenerationScore NIST score GeneratedSentenceStrategicGenerationScore Pr (event type m) result Algorithm 4return TacticalGenerationScore StrategicGenerationScoreend functionsentence test sentence. METEOR first evaluates uni-gram matches referencetest sentence also determines well words ordered. METEOR seemsappropriate domain good generated sentences missing adjectives adverbs critical meaning sentence prevent higher-order N-gram matches.addition, METEOR normalized always 0 1, may combine effectively IGSL scores (which also range 01).6.3 Experimental Evaluationevaluated new systems, WASPER-G EN-I GSL NIST METEOR scoring usingmethodology Section 4.4. matching results shown Figure 8, including resultsWASPER -G EN, best system previous section. WASPER-G EN-IGSLeither NIST METEOR scoring clearly outperforms WASPER-G EN. indicates strategicgeneration information help disambiguate data. Using different MT metrics producesless noticeable effect. clear winner English data; however, METEOR seemsimprove performance Korean data.Parsing results shown Figure 9. previously noted, parsing results generally mirrormatching results. new systems outperform WASPER -G EN, previously best system.again, English data show clear advantage using either NIST METEOR,Korean data gives slight edge using METEOR metric.Results tactical generation shown Figure 10. English Koreandata, new systems come close performance WASPER-G EN beat it. However,new systems outperform K RISPER -WASP WASPER shown figure.415fi0.90.90.850.850.80.80.750.75F-measureF-measureC HEN , K IM , & OONEY0.70.650.60.70.650.60.550.55WASPER-GENWASPER-GEN-IGSLWASPER-GEN-IGSL using METEOR0.5WASPER-GENWASPER-GEN-IGSLWASPER-GEN-IGSL using METEOR0.50.450.4512Number Training Games31(a) English2Number Training Games3(b) Korean0.90.90.850.850.80.80.750.75F-measureF-measureFigure 8: Matching results. Integrating strategic information improves results previously best system WASPER -G EN. choice MT metric used, however, makesless impact.0.70.650.70.650.60.6WASP gold matchingWASPER-GENWASPER-GEN-IGSLWASPER-GEN-IGSL using METEOR0.55WASP gold matchingWASPER-GENWASPER-GEN-IGSLWASPER-GEN-IGSL using METEOR0.550.50.512Number Training Games31(a) English2Number Training Games3(b) KoreanFigure 9: Semantic parsing results. results similar matching results integratingstrategic generation information improves performance.416fi0.50.60.450.550.40.50.350.45BLEUBLEURAINING ULTILINGUAL PORTSCASTER0.30.250.40.350.20.3WASP gold matchingWASPER-GENWASPER-GEN-IGSLWASPER-GEN-IGSL using METEOR0.15WASP gold matchingWASPER-GENWASPER-GEN-IGSLWASPER-GEN-IGSL using METEOR0.250.10.212Number Training Games31(a) English2Number Training Games3(b) KoreanFigure 10: Tactical generation results. two new systems come close performanceWASPER -G EN, beat it. However, outperform systemspresented earlier shown figure.Overall, expected, using strategic information improves performance matchingsemantic parsing tasks. English Korean datasets, WASPER-G EN-IGSLvariant using METEOR metric clearly outperform WASPER-G EN utilizestrategic information. However, strategic information improve tactical generation.could due ceiling effect WASPER -G EN already performs level near upperbaseline. matching performance improved, generation performance little roomgrow.7. Using Generative Alignment ModelRecently, Liang et al. (2009) developed generative model used match naturallanguage sentences facts corresponding database may refer. oneevaluation domains, used English RoboCup sportscasting data. method solvesmatching (alignment) problem data, address tasks semantic parsinglanguage generation. However, generative model elegantly integrates simple strategictactical language generation models order find overall probable alignment sentencesevents. demonstrated improved matching performance English data, generatingaccurate NLMR pairs best system. Thus, curious results couldused improve systems, also perform semantic parsing generation. alsoran code new Korean data resulted much worse matching results comparedbest system seen Table 5.simplest way utilizing results use NLMR pairs produced methodsupervised data WASP. expected, improved NLMR pairs English data resultedimproved semantic parsers seen results Table 6. Even Korean dataset,training matchings produced system ended fairly well even though matching performance poor. tactical generation, using matching produced marginalimprovement English dataset surprisingly large improvement Korean data417fiC HEN , K IM , & OONEYAlgorithmLiang et al. (2009)WASPERWASPER-G ENWASPER-G EN-I GSLWASPER-G EN-I GSL -M ETEOREnglish datasetinitialization Initialized75.759.779.368.175.873.573.973.175.1Korean datasetinitialization Initialized69.472.876.675.380.081.981.683.884.1Table 5: Matching results (F1 scores) 4-fold cross-validation English Koreandatasets. Systems run initialization initialized matchings producedLiang et al.s (2009) system.AlgorithmWASPWASPERWASPER-G ENWASPER-G EN-I GSLWASPER-G EN-I GSL -M ETEOREnglish datasetinitialization Initializedn/a80.361.8479.3270.1577.5973.1973.0472.7574.62Korean datasetinitialization Initializedn/a74.0169.1275.6972.0277.4978.7575.2780.6581.21Table 6: Semantic parsing results (F1 scores) 4-fold cross-validation EnglishKorean datasets. Systems run initialization initialized matchingsproduced Liang et al.s (2009) system.AlgorithmWASPWASPERWASPER-G ENWASPER-G EN-I GSLWASPER-G EN-I GSL -M ETEOREnglish datasetinitialization Initializedn/a0.45800.34710.45990.45600.44140.42230.45850.40620.4353Korean datasetinitialization Initializedn/a0.58280.45240.61180.55750.67960.53710.67100.51800.6591Table 7: Tactical generation results (BLEU score) 4-fold cross-validation EnglishKorean datasets. Systems run initialization initialized matchingsproduced Liang et al.s (2009) system.shown Table 7. Overall, using alignments produced Liang et al.s system resulted goodsemantic parsers tactical generators.addition training WASP alignment, also utilize output betterstarting point systems. Instead initializing iterative alignment methodsmodel trained ambiguous NLMR pairs, initialized disambiguatedNLMR pairs produced Liang et al.s system.418fiT RAINING ULTILINGUAL PORTSCASTERInitializing systems manner almost always improved performance threetasks (Tables 5, 6, 7). Moreover, results best systems exceed simply training WASP alignment cases except semantic parsing English data. Thus,combining Liang et al.s alignment disambiguation techniques seems produce bestoverall results. English data, WASPER initialization performs best matching generation. slightly worse semantic parsing task compared WASP trainedLiang et al.s alignment. Korean data, systems better training WASPalignment. WASPER -G EN -I GSL -M ETEOR initialization performs best matchingsemantic parsing WASPER -G EN initialization performs best generation.Overall, initializing systems alignment output Liang et al.s generative modelimproved performance expected. Starting cleaner set data led better initial semanticparsers language generators led better end results. Furthermore, incorporatingsemantic parser tactical generator, able improve Liang et al.s alignmentsachieve even better results cases.8. Removing Superfluous Commentsfar, discussed handle ambiguity multiple possible MRsNL sentence. training, methods assume NL sentence matchesexactly one potential MRs. However, comments superfluous, senserefer currently extracted event represented set potential MRs. previouslyshown Tables 2, one fifth English sentences one tenth Korean sentencessuperfluous sense.many reasons superfluous sentences. occur naturally languagepeople always talk current environment. domain, sportscasters often mentionpast events general information particular teams players. Moreover, dependingapplication, chosen MRL may represent things people talk about. example,RoboCup MRL cannot represent information players actively engagedball. Finally, even sentence represented chosen MRL, errors perceptualsystem incorrect estimation event occurred also lead superfluous sentences.perceptual errors alleviated degree increasing size window usedcapture potential MRs (the previous 5 seconds experiments). However, comescost increased ambiguity associates MRs sentence.deal problem superfluous sentences, eliminate lowest-scoring NLMRpairs (e.g. lowest parsing scores WASPER lowest NIST scores WASPER-G EN). However,order set pruning threshold, need automatically estimate amount superfluouscommentary absence supervised data. Notice problem looks similarstrategic generation problem (estimating likely MR participates correct matchingopposed likely NL sentence participates correct matching), approaches usedcannot applied. First, cannot use matches inferred existing systems estimatefraction superfluous comments since current systems match every sentence MR.also difficult develop algorithm similar IGSL due imbalance NL sentencesMRs. Since many MRs, examples events occurring withoutcommentaries vice versa.419fiC HEN , K IM , & OONEY8.1 Estimating Superfluous Rate Using Internal Cross Validationpropose using form internal (i.e. within training set) cross validation estimate ratesuperfluous comments. algorithm used conjunction systems,chose implement K RISPER trains much faster systems. makestractable train many different semantic parsers choose best one. basic ideause part ambiguous training data estimate accuracy semantic parser even thoughknow correct matchings. Assuming reasonable superfluous sentence rate, knowtime correct MR contained set MRs associated NL sentence.Thus, assume semantic parser parses NL sentence one MRs associatedbetter one parses MR set. approach estimatingaccuracy, evaluate semantic parsers learned using various pruning thresholds pickbest one. algorithm briefly summarized following steps:1. Split training set internal training set internal validation set.2. Train K RISPER N times internal training set using N different threshold values (eliminating lowest scoring NLMR pairs threshold retraining iterationAlgorithm 1).3. Test N semantic parsers internal validation set determine parser ableparse largest number sentences one potential MRs.4. Use threshold value produced best parser previous step train final parsercomplete original training set.8.2 Experimentsevaluated effect removing superfluous sentences three tasks: matching, parsing,generation. present results K RISPER K RISPER -WASP. matching,show results K RISPER responsible disambiguating training datasystems (so K RISPER -WASPs results same). generation, show resultsK RISPER-WASP, since K RISPER cannot perform generation.matching results shown Figure 11 demonstrate removing superfluous sentencesimprove performance English Korean, although difference small absoluteterms. parsing results shown Figure 12 indicate removing superfluous sentences usuallyimproves accuracy K RISPER K RISPER -WASP marginally. observedmany times, parsing results consistent matching results. Finally, tactical generation results shown Figure 13 suggest removing superfluous comments actually decreasesperformance somewhat. again, potential explanation generation less sensitivenoisy training data. removing superfluous comments improves purity training data,also removes potentially useful examples. Consequently, system learn generate sentences removed data. Overall, generation, advantagecleaner disambiguated training data apparently outweighed loss data.420fi0.80.80.750.750.70.70.650.65F-measureF-measureRAINING ULTILINGUAL PORTSCASTER0.60.550.60.550.50.50.450.450.40.4KRISPERKRISPER superfluous comment removal0.35KRISPERKRISPER superfluous comment removal0.3512Number Training Games31(a) English2Number Training Games3(b) Korean0.80.80.750.750.70.70.650.65F-measureF-measureFigure 11: Matching results comparing effects removing superfluous comments.0.60.550.60.550.50.5KRISPERKRISPER superfluous comment removalKRISPER-WASPKRISPER-WASP superfluous comment removal0.45KRISPERKRISPER superfluous comment removalKRISPER-WASPKRISPER-WASP superfluous comment removal0.450.40.412Number Training Games31(a) English2Number Training Games3(b) Korean0.50.60.450.550.40.50.350.45BLEUBLEUFigure 12: Semantic parsing results improved marginally superfluous comment removal.0.30.40.250.350.20.30.150.25KRISPER-WASPKRISPER-WASP superfluous comment removal0.1KRISPER-WASPKRISPER-WASP superfluous comment removal0.212Number Training Games31(a) English2Number Training Games3(b) KoreanFigure 13: Tactical generation performance decreases removing superfluous comments.421fiC HEN , K IM , & OONEY9. Human Subjective Evaluationbest, automatic evaluation generation imperfect approximation human assessment.Moreover, automatically evaluating quality entire generated sportscast even difficult. Consequently, used Amazons Mechanical Turk collect human judgementsproduced sportscasts. human judge shown three clips simulated game video one sitting. 8 video clips total. 8 clips use 4 game segments 4 minutes each, onefour games (2001-2004 RoboCup finals). 4 game segments commentatedhuman system. use IGSL determine events commentWASPER -G EN (our best performing system tactical generation) produce commentaries.make commentaries varied, took top 5 outputs WASPER -G EN choseone stochastically weighted scores. system always trained three games, leaving game test segment extracted. video clips accompaniedcommentaries appear subtitles screen well audio produced automated text speech system 3 videos shown random counter-balanced order ensureconsistent bias toward segments shown earlier later. asked judges scorecommentaries using following metrics:Score54321FluencyFlawlessGoodNon-nativeDisfluentGibberishSemanticCorrectnessAlwaysUsuallySometimesRarelyNeverSportscastingAbilityExcellentGoodAverageBadTerribleFluency semantic correctness, adequacy, standard metrics human evaluations NLtranslations generations. Fluency measures well commentaries structured, includingsyntax grammar. Semantic correctness indicates whether commentaries accurately describehappening game. Finally, sportscasting ability measures overall qualitysportscast. includes whether sportscasts interesting flow well. additionmetrics, also asked whether thought sportscast composed humancomputer (Human?).Since Mechanical Turk recruits judges Internet, make sure judgesassigning ratings randomly. Thus, addition asking rate video, alsoasked count number goals video. Incorrect responses question causedratings discarded. ensure judges faithfully watched entire clipassigning ratings. pruning, average 36 ratings (from 40 original ratings)8 videos English data. Since difficult recruit Korean judgesInternet, recruited person collected 7 ratings average videoKorean data. Table 8 9 show results English Korean data, respectively.Statistically significant results shown boldface.Results surprisingly good English data across categories machine actuallyscoring higher human average. However, differences statistically significant3. Sample video clips sound available web http://www.cs.utexas.edu/users/ml/clamp/sportscasting/.422fiT RAINING ULTILINGUAL PORTSCASTER2001 final2002 final2003 final2004 finalAverageCommentatorHumanMachineHumanMachineHumanMachineHumanMachineHumanMachineFluency3.743.894.133.973.543.894.034.133.863.94SemanticCorrectness3.593.814.583.743.734.264.174.384.034.03SportscastingAbility3.153.614.033.292.613.373.544.003.343.48Human?20.59%40.00%42.11%11.76%13.51%19.30%20.00%56.25%24.31%26.76%Table 8: Human evaluation overall sportscasts English data. Bold numbers indicate statisticalsignificance.2001 final2002 final2003 final2004 finalAverageCommentatorHumanMachineHumanMachineHumanMachineHumanMachineHumanMachineFluency3.753.504.173.253.862.383.002.713.662.93SemanticCorrectness4.133.674.333.384.293.253.753.434.103.41SportscastingAbility4.002.833.833.134.002.883.253.003.762.97Human?50.00%33.33%83.33%50.00%85.71%25.00%37.50%14.29%62.07%31.03%Table 9: Human evaluation overall sportscasts Korean data. Bold numbers indicate statisticalsignificance.423fiC HEN , K IM , & OONEYbased unpaired t-test (p > 0.05). Nevertheless, encouraging see machinerated highly. variance humans performance since two differentcommentators. notably, compared machine, humans performance 2002 finalquite good commentary included many details position players,types passes, comments overall flow game. hand,humans performance 2003 final quite bad human commentatormechanical used sentence pattern repeatedly. machine performanceeven throughout although sometimes gets lucky. example, machine serendipitously saidbeginning exciting match. near start 2004 final clip simplystatement incorrectly learned correspond extracted MR actually unrelated.results Korean impressive. human beats machine averagecategories. However, largest difference scores category 0.8.Moreover, absolute scores indicate generated Korean sportscast least acceptablequality. judges even mistakenly thought produced humans one third time.Part reason worse performance compared English data Korean commentaries fairly detailed included events extracted limited perceptualsystem. Thus, machine simply way competing limited expressinginformation present extracted MRs.also elicited comments human judges get qualitative evaluation. Overall,judges thought generated commentaries good accurately described actionsfield. Picking top 5 generated sentences also added variability machine-generatedsportscasts improved results compared earlier experiments presented ChenMooney (2008). However, machine still sometimes misses significant plays scoringcorner kicks. plays happen much less frequently often coincidemany events (e.g. shooting ball kickoffs co-occur scoring). Thus, machineharder time learning infrequent events. Another issue concerns representation.Many people complain long gaps sportscasts lack details. event detectorconcentrates ball possession positions elapsed time. Thus, player holding ontoball dribbling long time produce events detected simulated perceptualsystem. Also, short pass backfield treated exactly long pass acrossfield near goal. Finally, people desired colorful commentary (background information,statistics, analysis game) fill voids. somewhat orthogonal issue sincegoal build play-by-play commentator described events currently happening.10. Related Worksection review related work semantic parsing, natural language generationwell grounded language learning.10.1 Semantic Parsingmentioned Section 2, existing work semantic parser learners focused supervisedlearning sentence annotated semantic meaning. semantic-parser learners additionally require either syntactic annotations (Ge & Mooney, 2005) prior syntactic knowledge target language (Ge & Mooney, 2009; Zettlemoyer & Collins, 2005, 2007). Sinceworld never provides direct feedback syntactic structure, language-learning methods424fiT RAINING ULTILINGUAL PORTSCASTERrequire syntactic annotation directly applicable grounded language learning. Therefore,methods learn semantic annotation critical learning language perceptualcontext.use logic formulas MRs, particular MRL use contains atomicformulas equivalently represented frames slots. systems usetransformation-based learning (Jurcicek et al., 2009), Markov logic (Meza-Ruiz, Riedel, &Lemon, 2008) learn semantic parsers using frames slots. principle, frameworkused semantic parser learner long provides confidence scores parse results.10.2 Natural Language Generationseveral existing systems sportscast RoboCup games (Andre et al., 2000). Givengame states provided RoboCup simulator, extract game events generate real-timecommentaries. consider many practical issues timeliness, coherence, variability,emotion needed produce good sportscasts. However, systems hand-builtgenerate language using pre-determined templates rules. contrast, concentratelearning problem induce generation components ambiguous training data. Nevertheless, augmenting system components systems could improvefinal sportscasts produced.also prior work learning lexicon elementary semantic expressions corresponding natural language realizations (Barzilay & Lee, 2002). work uses multiple-sequencealignment datasets supply several verbalizations corresponding semantics extractdictionary.Duboue McKeown (2003) first propose algorithm learning strategic generation automatically data. Using semantics associated texts, system learns classifierdetermines whether particular piece information included presentation not.recent work learning strategic generation using reinforcement learning(Zaragoza & Li, 2005). work involves game setting speaker must aid listenerreaching given destination avoiding obstacles. game played repeatedly findoptimal strategy conveys pertinent information minimizing numbermessages. consider different problem setting reinforcements availablestrategic generation learner.addition, also work performing strategic generation collective task(Barzilay & Lapata, 2005). considering strategic generation decisions jointly, capturesdependencies utterances. creates consistent overall output consistenthumans perform task. approach could potentially help system producebetter overall sportscasts.10.3 Grounded Language LearningOne ambitious end-to-end visually-grounded scene-description system VITRA (Herzog & Wazinski, 1994) comments traffic scenes soccer matches. system firsttransforms raw visual data geometrical representations. Next, set rules extract spatial relations interesting motion events representations. Presumed intentions, plans, plan425fiC HEN , K IM , & OONEYinteractions agents also extracted based domain-specific knowledge. However,since system hand-coded cannot adapted easily new domains.Srihari Burhans (1994) used captions accompanying photos help identify peopleobjects. introduced idea visual semantics, theory extracting visual informationconstraints accompanying text. example, using caption information, systemdetermine spatial relationship entities mentioned, likely size shapeobject interest, whether entity natural artificial. However, system also basedhand-coded knowledge.Siskind (1996) performed earliest work learning grounded word meanings.learning algorithm addresses problem ambiguous training referential uncertaintysemantic lexical acquisition, address larger problems learning complete semanticparsers language generators.Several robotics computer vision researchers worked inferring grounded meaningsindividual words short referring expressions visual perceptual context (e.g., Roy, 2002;Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004). However, complexitynatural language used existing work restrictive, many systems use pre-codedknowledge language, almost use static images learn language describing objectsrelations, cannot learn language describing actions. sophisticated grammaticalformalism used learn syntax work finite-state hidden-Markov model. contrast,work exploits latest techniques statistical context-free grammars syntax-based statisticalmachine translation handle complexities natural language.recently, Gold Scassellati (2007) built system called TWIG uses existing language knowledge help learn meaning new words. robot uses partial parses focusattention possible meanings new words. playing game catch, robot ablelearn meaning well identity relations.also variety work learning captions accompany picturesvideos (Satoh, Nakamura, & Kanade, 1997; Berg, Berg, Edwards, & Forsyth, 2004). areaparticular interest given large amount captioned images video available webtelevision. Satoh et al. (1997) built system detect faces newscasts. However, use fairlysimple manually-written rules determine entity picture language refers.Berg et al. (2004) used elaborate learning method cluster faces names. Usingdata, estimate likelihood entity appearing picture given context.recent work video retrieval focused learning recognize events sports videosconnecting English words appearing accompanying closed captions (Fleischman& Roy, 2007; Gupta & Mooney, 2009). However, work learns connectionindividual words video events learn describe events using full grammaticalsentences. avoid difficult problems computer vision, work uses simulated worldperception complex events participants much simpler.addition observing events passively, also work grounded language learning interactive environments computer video games (Gorniak & Roy, 2005).work, players cooperate communicate order accomplish certain task.system learns map spoken instructions specific actions; however, relies existing statisticalparsers learn syntax semantics language perceptual environmentalone. Kerr, Cohen, Chang (2008) developed system learns grounded word-meaningsnouns, adjectives, spatial prepositions human instructing perform tasks vir426fiT RAINING ULTILINGUAL PORTSCASTERtual world; however, system assumes existing syntactic parser prior knowledge verbsemantics unable learn experience.Recently, interest learning interpret English instructions describing use particular website perform computer tasks (Branavan et al., 2009; Lau,Drews, & Nichols, 2009). systems learn predict correct computer action (pressingbutton, choosing menu item, typing text field, etc.) corresponding step instructions. Instead using parallel training data perceptual context, systems utilizedirect matches words natural language instructions English words explicitly occurring menu items computer instructions order establish connectionlanguage environment.One core subproblems work addresses matching sentences facts worldrefer. recent projects attempt align text English summaries Americanfootball games database records contain statistics events game (Snyder& Barzilay, 2007; Liang et al., 2009). However, Snyder Barzilay (2007) use supervisedapproach requires annotating correct correspondences text semanticrepresentations. hand, Liang et al. (2009) developed unsupervised approachusing generative model solve alignment problem. also demonstrated improved resultsmatching sentences events RoboCup English sportscasting data. However, workaddress semantic parsing language generation. Section 7 presents results showingmethods improve NLMR matches produced approach well uselearn parsers generators.11. Future Workpreviously discussed, limitations current system due inadequaciesperception events extracted RoboCup simulator. language commentary,particularly Korean data, refers information events currently representedextracted MRs. example, player dribbling ball captured perceptual system.event extractor could extended include information output representations.Commentaries always immediate actions happening field. alsorefer statistics game, background information, analysis game.difficult obtain, would simple augment potential MRs include eventscurrent score number turnovers, etc. may difficult learn correctly,potentially would make commentaries much natural engaging.statements commentaries specifically refer pattern activity across severalrecent events rather single event. example, one English commentaries,statement Purple team sloppy today. appears series turn-overs team.simulated perception could extended extract patterns activity sloppiness;however assumes concepts predefined, extracting many higher-levelpredicates would greatly increase ambiguity training data. current system assumesalready concepts words needs learn perceive concepts representMRs. However, would interesting include Whorfian style languagelearning (Whorf, 1964) unknown word sloppiness could actually causecreation new concept. content words seem consistently correlateperceived event, system could collect examples recent activity word used try427fiC HEN , K IM , & OONEYlearn new higher-level concept captures regularity situations. example, givenexamples situations referred sloppy, inductive logic programming system (Lavrac &Dzeroski, 1994) able detect pattern several recent turnovers.Another shortcoming current system MR treated independently. failsexploit fact many MRs related other. example, pass preceded kick,bad pass followed turnover. natural way use graphical representationrepresent entities events also relationships them.Currently tactical strategic generation system loosely coupled. However,conceptually much closely related, solving one problem help solveother. Initializing system output Liang et al. (2009), uses generative modelincludes strategic tactical components, produced somewhat better results. However,interaction components loose tighter integration differentpieces could yield stronger results tasks.obvious extension current work apply real RoboCup games rathersimulated ones. Recent work Rozinat, Zickler, Veloso, van der Aalst, McMillen (2008)analyzes games RoboCup Small Size League using video overhead camera.using symbolic event trace extracted real perceptual system, methods couldapplied real-world games. Using speech recognition accept spoken language input anotherobvious extension.currently exploring extending approach learn interpret generate NL instructions navigating virtual environment. system observe one person giving Englishnavigation instructions (e.g. Go hall turn left pass chair.) another person follows directions get chosen destination. collecting examples sentencespaired actions executed together information local environment,system construct ambiguous supervised dataset language learning. approachcould eventually lead virtual agents games educational simulations automaticallylearn interpret generate natural language instructions.12. Conclusionpresented end-to-end system learns generate natural-language sportscastssimulated RoboCup soccer games training sample human commentaries paired automatically extracted game events. learning semantically interpret generate language withoutexplicitly annotated training data, demonstrated system learn language simplyobserving linguistic descriptions ongoing events. also demonstrated systems languageindependence successfully training produce sportscasts English Korean.Dealing ambiguous supervision inherent training environment critical issuelearning language perceptual context. evaluated various methods disambiguatingtraining data order learn semantic parsers language generators. Using generationevaluation metric criterion selecting best NLMR pairs produced better resultsusing semantic parsing scores initial training data noisy. system also learnsmodel strategic generation ambiguous training data estimating probabilityevent type evokes human commentary. Moreover, using strategic generation informationhelp disambiguate training data shown improve results. also demonstratedsystem initialized alignments produced different system achieve better428fiT RAINING ULTILINGUAL PORTSCASTERresults either system alone. Finally, experimental evaluation verified overall systemlearns accurately parse generate comments generate sportscasts competitiveproduced humans.Acknowledgmentsthank Adam Bossy work simulating perception RoboCup games. alsothank Percy Liang sharing software experimental results us. Finally, thankanonymous reviewers JAIR editor, Lillian Lee, insightful commentshelped improve final presentation paper. work funded NSF grant IIS0712907X. experiments run Mastodon Cluster, provided NSF GrantEIA-0303609.Appendix A. Details meaning representation languageTable 10 shows brief explanations different events detect simulated perception.EventPlaymodeBallstoppedTurnoverKickPassBadPassDefenseStealBlockDescriptionSignifies current play mode defined gameball speed minimum thresholdcurrent possessor ball last possessor different teamsplayer possession ball one time interval nextplayer gains possession ball different player teampass player gaining possession ball different teamtransfer one player opposing player penalty areaplayer possession ball one time interval another playerdifferent team next time intervalTransfer one player opposing goalie.Table 10: Description different events detectedinclude context-free grammar developed meaning representation language. derivations start root symbol *S.*S*S*S*S*S*S*S*S*S->->->->->->->->->playmode ( *PLAYMODE )ballstoppedturnover ( *PLAYER , *PLAYER )kick ( *PLAYER )pass ( *PLAYER , *PLAYER )badPass ( *PLAYER , *PLAYER )defense ( *PLAYER , *PLAYER )steal ( *PLAYER )block ( *PLAYER )429fiC HEN , K IM , & OONEY*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYMODE*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER*PLAYER->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->kick_off_lkick_off_rkick_in_lkick_in_rplay_onoffside_loffside_rfree_kick_lfree_kick_rcorner_kick_lcorner_kick_rgoal_kick_lgoal_kick_rgoal_lgoal_rpink1pink2pink3pink4pink5pink6pink7pink8pink9pink10pink11purple1purple2purple3purple4purple5purple6purple7purple8purple9purple10purple11430fiT RAINING ULTILINGUAL PORTSCASTERReferencesAho, A. V., & Ullman, J. D. (1972). Theory Parsing, Translation, Compiling. PrenticeHall, Englewood Cliffs, NJ.Andre, E., Binsted, K., Tanaka-Ishii, K., Luke, S., Herzog, G., & Rist, T. (2000). Three RoboCupsimulation league commentator systems. AI Magazine, 21(1), 5766.Bailey, D., Feldman, J., Narayanan, S., & Lakoff, G. (1997). Modeling embodied lexical development. Proceedings Nineteenth Annual Conference Cognitive Science Society.Banerjee, S., & Lavie, A. (2005). METEOR: automatic metric MT evaluation improvedcorrelation human judgments. Proceedings ACL Workshop IntrinsicExtrinsic Evaluation Measures Machine Translation and/or Summarization, pp. 6572Ann Arbor, Michigan. Association Computational Linguistics.Barnard, K., Duygulu, P., Forsyth, D., de Freitas, N., Blei, D. M., & Jordan, M. I. (2003). Matchingwords pictures. Journal Machine Learning Research, 3, 11071135.Barzilay, R., & Lapata, M. (2005). Collective content selection concept-to-text generation.Proceedings Human Language Technology Conference Conference EmpiricalMethods Natural Language Processing (HLT/EMNLP-05).Barzilay, R., & Lee, L. (2002). Bootstrapping lexical choice via multiple-sequence alignment.Proceedings 2002 Conference Empirical Methods Natural Language Processing(EMNLP-02).Berg, T. L., Berg, A. C., Edwards, J., & Forsyth, D. A. (2004). Whos picture. AdvancesNeural Information Processing Systems 17 (NIPS 2004).Branavan, S., Chen, H., Zettlemoyer, L. S., & Barzilay, R. (2009). Reinforcement learningmapping instructions actions. Proceedings Joint conference 47th AnnualMeeting Association Computational Linguistics 4th International JointConference Natural Language Processing Asian Federation Natural LanguageProcessin (ACL-IJCNLP 2009).Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J. D., Mercer,R. L., & Roossin, P. S. (1990). statistical approach machine translation. ComputationalLinguistics, 16(2), 7985.Brown, P. F., Della Pietra, V. J., Della Pietra, S. A., & Mercer, R. L. (1993). mathematicsstatistical machine translation: Parameter estimation. Computational Linguistics, 19(2),263312.Bunescu, R. C., & Mooney, R. J. (2005). Subsequence kernels relation extraction. Weiss,Y., Scholkopf, B., & Platt, J. (Eds.), Advances Neural Information Processing Systems 19(NIPS 2006) Vancouver, BC.Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded language acquisition. Proceedings 25th International Conference Machine Learning (ICML-2008)Helsinki, Finland.431fiC HEN , K IM , & OONEYChen, M., Foroughi, E., Heintz, F., Kapetanakis, S., Kostiadis, K., Kummeneje, J., Noda, I., Obst,O., Riley, P., Steffens, T., Wang, Y., & Yin, X. (2003). Users manual: RoboCup soccer servermanual soccer server version 7.07 later.. Available http://sourceforge.net/projects/sserver/.Chiang, D. (2005). hierarchical phrase-based model statistical machine translation. Proceedings 43nd Annual Meeting Association Computational Linguistics (ACL05), pp. 263270 Ann Arbor, MI.Collins, M. (2002). New ranking algorithms parsing tagging: Kernels discrete structures, voted perceptron. Proceedings 40th Annual Meeting AssociationComputational Linguistics (ACL-2002), pp. 263270 Philadelphia, PA.Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete datavia EM algorithm. Journal Royal Statistical Society B, 39, 138.Doddington, G. (2002). Automatic evaluation machine translation quality using n-gram cooccurrence statistics. Proceedings ARPA Workshop Human Language Technology,pp. 128132 San Diego, CA.Duboue, P. A., & McKeown, K. R. (2003). Statistical acquisition content selection rulesnatural language generation. Proceedings 2003 Conference Empirical MethodsNatural Language Processing (EMNLP-03), pp. 121128.Fleischman, M., & Roy, D. (2007). Situated models meaning sports video retrieval. Proceedings Human Language Technologies: Conference North American ChapterAssociation Computational Linguistics (NAACL-HLT-07) Rochester, NY.Ge, R., & Mooney, R. J. (2005). statistical semantic parser integrates syntax semantics.Proceedings Ninth Conference Computational Natural Language Learning (CoNLL2005), pp. 916 Ann Arbor, MI.Ge, R., & Mooney, R. J. (2009). Learning compositional semantic parser using existing syntactic parser. Proceedings Joint conference 47th Annual Meeting Association Computational Linguistics 4th International Joint Conference NaturalLanguage Processing Asian Federation Natural Language Processin (ACL-IJCNLP2009).Gold, K., & Scassellati, B. (2007). robot uses existing vocabulary infer non-visual wordmeanings observation. Proceedings Twenty-Second Conference ArtificialIntelligence (AAAI-07).Gorniak, P., & Roy, D. (2005). Speaking sidekick: Understanding situated speechcomputer role playing games. Proceedings 4th Conference Artificial IntelligenceInteractive Digital Entertainment Stanford, CA.Gupta, S., & Mooney, R. (2009). Using closed captions train activity recognizers improvevideo retrieval. Proceedings CVPR-09 Workshop Visual Contextual LearningAnnotated Images Videos (VCL) Miami, FL.432fiT RAINING ULTILINGUAL PORTSCASTERHarnad, S. (1990). symbol grounding problem. Physica D, 42, 335346.Herzog, G., & Wazinski, P. (1994). VIsual TRAnslator: Linking perceptions natural languagedescriptions. Artificial Intelligence Review, 8(2/3), 175187.Ide, N. A., & Jeronis, J. (1998). Introduction special issue word sense disambiguation:state art. Computational Linguistics, 24(1), 140.Joachims, T. (1998). Text categorization support vector machines: Learning many relevantfeatures. Proceedings Tenth European Conference Machine Learning (ECML98), pp. 137142 Berlin. Springer-Verlag.Jurcicek, J., Gasic, M., Keizer, S., Mairesse, F., Thomson, B., & Young, S. (2009). Transformationbased learning semantic parsing. Interspeech Brighton, UK.Kate, R. J., & Mooney, R. J. (2006). Using string-kernels learning semantic parsers. Proceedings 21st International Conference Computational Linguistics 44th AnnualMeeting Association Computational Linguistics (COLING/ACL-06), pp. 913920Sydney, Australia.Kate, R. J., & Mooney, R. J. (2007). Learning language semantics ambiguous supervision.Proceedings Twenty-Second Conference Artificial Intelligence (AAAI-07), pp.895900 Vancouver, Canada.Kerr, W., Cohen, P. R., & Chang, Y.-H. (2008). Learning playing wubble world. Proceedings Fourth Artificial Intelligence Interactive Digital Entertainment Conference(AIIDE) Palo Alto, CA.Kingsbury, P., Palmer, M., & Marcus, M. (2002). Adding semantic annotation Penn treebank.Proceedings Human Language Technology Conference San Diego, CA.Knight, K., & Hatzivassiloglou, V. (1995). Two-level, many-paths generation. Proceedings33rd Annual Meeting Association Computational Linguistics (ACL-95), pp.252260 Cambridge, MA.Lau, T., Drews, C., & Nichols, J. (2009). Interpreting written how-to instructions. ProceedingsTwenty-first International Joint Conference Artificial Intelligence (IJCAI-2009).Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming: Techniques Applications.Ellis Horwood.Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondences less supervision. Proceedings Joint conference 47th Annual Meeting AssociationComputational Linguistics 4th International Joint Conference Natural LanguageProcessing Asian Federation Natural Language Processin (ACL-IJCNLP 2009).Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N., & Watkins, C. (2002). Text classificationusing string kernels. Journal Machine Learning Research, 2, 419444.433fiC HEN , K IM , & OONEYLu, W., Ng, H. T., Lee, W. S., & Zettlemoyer, L. S. (2008). generative model parsing naturallanguage meaning representations. Proceedings 2008 Conference EmpiricalMethods Natural Language Processing (EMNLP-08) Honolulu, HI.Marcus, M., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotated corpusEnglish: Penn treebank. Computational Linguistics, 19(2), 313330.McKeown, K. R. (1985). Discourse strategies generating natural-language text. Artificial Intelligence, 27(1), 141.Meza-Ruiz, I. V., Riedel, S., & Lemon, O. (2008). Spoken language understanding dialoguesystems, using 2-layer Markov logic network: improving semantic accuracy. ProceedingsLondial.Mooney, R. J. (2007). Learning semantic parsing. Gelbukh, A. (Ed.), Computational Linguistics Intelligent Text Processing: Proceedings 8th International Conference,CICLing 2007, Mexico City, pp. 311324. Springer Verlag, Berlin.Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment models.Computational Linguistics, 29(1), 1951.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automatic evaluationmachine translation. Proceedings 40th Annual Meeting AssociationComputational Linguistics (ACL-2002), pp. 311318 Philadelphia, PA.Riezler, S., Prescher, D., Kuhn, J., & Johnson, M. (2000). Lexicalized stochastic modelingconstraint-based grammars using log-linear measures EM training. Proceedings38th Annual Meeting Association Computational Linguistics (ACL-2000), pp.480487 Hong Kong.Roy, D. (2002). Learning visually grounded words syntax scene description task. Computer Speech Language, 16(3), 353385.Rozinat, A., Zickler, S., Veloso, M., van der Aalst, W., & McMillen, C. (2008). Analyzing multiagent activity logs using process mining techniques. Proceedings 9th InternationalSymposium Distributed Autonomous Robotic Systems (DARS-08) Tsukuba, Japan.Satoh, S., Nakamura, Y., & Kanade, T. (1997). Name-it: Naming detecting faces videointegration image natural language processing. Proceedings FifteenthInternational Joint Conference Artificial Intelligence (IJCAI-97).Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods Pattern Analysis. Cambridge University Press.Shieber, S. M. (1988). uniform architecture parsing generation. Proceedings12th International Conference Computational Linguistics (COLING-88), pp. 614619 Budapest, Hungary.Siskind, J. M. (1996). computational study cross-situational techniques learning word-tomeaning mappings. Cognition, 61(1), 3991.434fiT RAINING ULTILINGUAL PORTSCASTERSnyder, B., & Barzilay, R. (2007). Database-text alignment via structured multilabel classification. Proceedings Twentieth International Joint Conference Artificial Intelligence(IJCAI-2007).Srihari, R. K., & Burhans, D. T. (1994). Visual semantics: Extracting visual informationtext accompanying pictures. Proceedings Twelfth National Conference ArtificialIntelligence (AAAI-94).Stolcke, A. (1995). efficient probabilistic context-free parsing algorithm computes prefixprobabilities. Computational Linguistics, 21(2), 165201.Whorf, B. L. (1964). Language, Thought, Reality: Selected Writings. MIT Press.Wong, Y., & Mooney, R. J. (2006). Learning semantic parsing statistical machine translation. Proceedings Human Language Technology Conference / North American ChapterAssociation Computational Linguistics Annual Meeting (HLT-NAACL-06), pp. 439446 New York City, NY.Wong, Y., & Mooney, R. J. (2007). Generation inverting semantic parser uses statisticalmachine translation. Proceedings Human Language Technologies: ConferenceNorth American Chapter Association Computational Linguistics (NAACL-HLT07), pp. 172179 Rochester, NY.Yamada, K., & Knight, K. (2001). syntax-based statistical translation model. Proceedings39th Annual Meeting Association Computational Linguistics (ACL-2001), pp.523530 Toulouse, France.Yu, C., & Ballard, D. H. (2004). integration grounding language learning objects.Proceedings Nineteenth National Conference Artificial Intelligence (AAAI-04), pp.488493.Zaragoza, H., & Li, C.-H. (2005). Learning talk descriptive games. ProceedingsHuman Language Technology Conference Conference Empirical MethodsNatural Language Processing (HLT/EMNLP-05), pp. 291298 Vancouver, Canada.Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods relation extraction. JournalMachine Learning Research, 3, 10831106.Zettlemoyer, L. S., & Collins, M. (2005). Learning map sentences logical form: Structuredclassification probabilistic categorial grammars. Proceedings 21st ConferenceUncertainty Artificial Intelligence (UAI-2005) Edinburgh, Scotland.Zettlemoyer, L. S., & Collins, M. (2007). Online learning relaxed CCG grammars parsinglogical form. Proceedings 2007 Joint Conference Empirical Methods NaturalLanguage Processing Computational Natural Language Learning (EMNLP-CoNLL-07),pp. 678687 Prague, Czech Republic.435fiJournal Artificial Intelligence Research 37 (2010) 247-277Submitted 08/09; published 03/10Context-Based Word Acquisition Situated DialogueVirtual WorldShaolin QuJoyce Y. Chaiqushaoli@cse.msu.edujchai@cse.msu.eduDepartment Computer Science EngineeringMichigan State UniversityEast Lansing, MI 48824 USAAbstracttackle vocabulary problem conversational systems, previous work appliedunsupervised learning approaches co-occurring speech eye gaze interactionautomatically acquire new words. Although approaches shown promise, severalissues related human language behavior human-machine conversationaddressed. First, psycholinguistic studies shown certain temporal regularitieshuman eye movement language production. regularities potentiallyguide acquisition process, incorporated previous unsupervised approaches. Second, conversational systems generally existing knowledgebase domain vocabulary. existing knowledge potentiallyhelp bootstrap constrain acquired new words, incorporatedprevious models. Third, eye gaze could serve different functions human-machine conversation. gaze streams may closely coupled speech stream, thuspotentially detrimental word acquisition. Automated recognition closely-coupledspeech-gaze streams based conversation context important. address issues,developed new approaches incorporate user language behavior, domain knowledge,conversation context word acquisition. evaluated approaches context situated dialogue virtual world. experimental results shownincorporating three types contextual information significantly improves wordacquisition performance.1. IntroductionOne major bottleneck human machine conversation robust language interpretation.encountered vocabulary outside systems knowledge, system tendsfail. conversational interfaces become increasingly important many applications remote interaction robots (Lemon, Gruenstein, & Peters, 2002; Fong& Nourbakhsh, 2005) automated training education (Traum & Rickel, 2002),ability automatically acquire new words online conversation becomes essential.Different traditional telephony-based spoken dialogue systems, conversational interfaces users look graphic display virtual world interacting artificialagents using natural speech. unique setting provides opportunity automatedvocabulary acquisition. interaction, users visual perception (e.g., indicatedeye gaze) provides potential channel system automatically learn new words.c2010AI Access Foundation. rights reserved.fiQu & Chaiidea, shown previous work (Yu & Ballard, 2004; Liu, Chai, & Jin, 2007),parallel data visual perception spoken utterances used unsupervisedapproaches automatically identify mappings words visual entitiesthus acquire new words. previous approaches provide promising direction,mainly rely co-occurrence words visual entities completely unsupervised manner. However, human machine conversation, different typesextra information related human language behaviors characteristics conversationsystems. Although extra information potentially provide supervision guideword acquisition process improve performance, systematically exploredprevious work.First, large body psycholinguistic studies shown eye gaze tightly linkedhuman language processing. evident language comprehension (Tanenhaus, Spivey-Knowiton, Eberhard, & Sedivy, 1995; Eberhard, Spivey-Knowiton, Sedivy, &Tanenhaus, 1995; Allopenna, Magnuson, & Tanenhaus, 1998; Dahan & Tanenhaus, 2005;Spivey, Tanenhaus, Eberhard, & Sedivy, 2002) language production (Meyer, Sleiderink,& Levelt, 1998; Rayner, 1998; Griffin & Bock, 2000; Bock, Irwin, Davidson, & Leveltb, 2003;Brown-Schmidt & Tanenhaus, 2006; Griffin, 2001). Specifically human language production, directly relevant automated computer interpretation human language,studies found significant temporal regularities mentioned objectscorresponding words (Meyer et al., 1998; Rayner, 1998; Griffin & Bock, 2000). objectnaming tasks, onset word begins approximately one second speakerlooked corresponding visual referent (Griffin, 2004), gazes longerdifficult name referent retrieve (Meyer et al., 1998; Griffin, 2001).100-300 ms articulation object name begins, eyes move next object relevant task (Meyer et al., 1998). findings suggest eyes movementioned objects corresponding words uttered. Although languagebehavior used constrain mapping words visual objects,incorporated previous approaches.Second, practical conversational systems always existing knowledge application domains vocabularies. knowledge base acquired developmenttime: either authored domain experts automatically learned available data.Although existing knowledge rather limited desired enhanced automatically online (e.g., automated vocabulary acquisition), provides importantinformation structure domain existing vocabularies,bootstrap constrain new word acquisition. type domain knowledgeutilized previous approaches.Third, although psycholinguistic studies provide us sound empirical basisassuming eye movements predictive speech, gaze behavior interactivesetting much complex. different types eye movements (Kahneman,1973). naturally occurring eye gaze speech production may serve differentfunctions, example, engage conversation manage turn taking (Nakano,Reinstein, Stocky, & Cassell, 2003). Furthermore, interacting graphic display,user could talking objects previously seen display somethingcompletely unrelated object user looking at. Therefore using speechgaze pairs word acquisition detrimental. type gaze mostly useful248fiContext-Based Word Acquisition Situated Dialogue Virtual Worldword acquisition kind reflects underlying attention tightly linkscontent co-occurring speech. Thus, automatically recognizing closely coupled speechgaze streams online conversation word acquisition important. However,examined previous work.address three issues, developed new approaches automatic wordacquisition (1) incorporate findings user language behavior psycholinguisticstudies, particular temporal alignment spoken words eye gaze; (2) utilizeexisting domain knowledge, (3) automatically identify closely-coupled speechgaze streams based conversation context. evaluated approaches contextsituated dialogue virtual world. experimental results shown incorporatingthree types contextual information significantly improves word acquisitionperformance. simulation studies demonstrate effect automatic onlineword acquisition improving language understanding human-machine conversation.following sections, first introduce domain data collection investigation. describe enhanced models word acquisition incorporateadditional contextual information (e.g., language behavior spoken words eyegaze, domain knowledge, conversation context). Finally, present empirical evaluationenhanced models demonstrate effect online word acquisition spokenlanguage understanding human-machine conversation.2. Related Workwork motivated previous work grounded language acquisition eye gazemultimodal human-computer interaction.Grounded language acquisition learn meaning language connectinglanguage perception world. Language acquisition grounding wordsvisual perceptions objects studied various language learning systems.example, given speech paired video images single objects, mutual information audio visual signals used learn words associating acoustic phonemesequences visual prototypes (e.g., color, size, shape) objects (Roy & Pentland,2002; Roy, 2002). Generative models developed learn words associating wordsimage regions given parallel data pictures description text (Barnard, Duygulu,de Freitas, Forsyth, Blei, & Jordan, 2003). Given pairs spoken instructions containingobject names corresponding objects, utterance-object joint probability modelused learn object names identifying object name phonemes associatingobjects (Taguchi, Iwahashi, Nose, Funakoshi, & Nakano, 2009). Given sequencesutterances paired scene representations, incremental translation model developed learn word meaning associating words semantic representationsreferents scene (Fazly, Alishahi, & Stevenson, 2008). addition groundingindividual words, previous work also investigated grounding phrases (referring expressions) visual objects semantic decomposition, example using context freegrammar connects linguistic structures underlying visual properties (Gorniak &Roy, 2004).Besides visual objects, approaches also developed ground words meaning representations events. example, event logic applied ground verbs249fiQu & Chaimotion events represented force dynamics encoding support, contact,attachment relations objects video images (Siskind, 2001). video gamedomain, translation model used ground words semantic roles user actions (Fleischman & Roy, 2005). simulated Robocup soccer domain, given textual gamecommentaries paired symbolic descriptions game events, approaches basedstatistical parsing learning developed ground commentary text gameevents (Chen & Mooney, 2008). less restricted data setting, generative modelsdeveloped simultaneously segment text utterances map utterancesmeaning representations event states (Liang, Jordan, & Klein, 2009). Differentprevious work, work, visual perception indicated eye gaze. Eyegaze, one hand, indicative human attention, provides opportunities linklanguage perception; hand, implicit subconscious input,could bring additional challenge word acquisition.Eye gaze long explored human-computer interaction direct manipulation interfaces pointing device (Jacob, 1991; Wang, 1995; Zhai, Morimoto, & Ihde,1999). Eye gaze modality multimodal interaction goes beyond functionpointing. different speech eye gaze systems, eye gaze exploredpurpose mutual disambiguation (Tanaka, 1999; Zhang, Imamiya, Go, & Mao, 2004),complement speech channel reference resolution (Campana, Baldridge, Dowding,Hockey, Remington, & Stone, 2001; Kaur, Termaine, Huang, Wilder, Gacovski, Flippo,& Mantravadi, 2003; Prasov & Chai, 2008; Byron, Mampilly, Sharma, & Xu, 2005)speech recognition (Cooke, 2006; Qu & Chai, 2007), managing human-computerdialogue (Qvarfordt & Zhai, 2005).Eye gaze explored recently word acquisition. example, Yu Ballard(2004) proposed embodied multimodal learning interface word acquisition, especiallyeye movement. work, given speech paired eye gaze informationvideo images, translation model applied acquire words associating acousticphone sequences visual representations objects actions. work inspiredresearch mostly related effort here. difference workwork Yu Ballard lies two aspects. First, learning environmentdifferent. Yu Ballard focuses narrative descriptions actions (e.g.,making sandwich, pouring drinks, etc.) human subjects, focusinteractive conversation. conversation, human participant take speakerrole addressee role. represents new scenario word acquisition basedeye movement may new implications. Second, work Yu Ballard,IBM Translation Model 1 applied word acquisition. work,incorporate types information user language behavior, domain knowledge,conversation context translation models.previous work, experimented application IBM Translation Model1 vocabulary acquisition gaze modeling conversation setting (Liu et al.,2007). reported initial investigation incorporating temporal informationdomain knowledge translation models (Qu & Chai, 2008) well automaticallyidentifying closely coupled speech gaze streams (Qu & Chai, 2009). paper extendsprevious work provides comprehensive evaluation incorporating knowledgeinteractivity word acquisition much richer application domain. examine250fiContext-Based Word Acquisition Situated Dialogue Virtual Worldword acquisition affected automated speech recognition effectonline word acquisition language understanding human-machine conversation.3. Domain Datafacilitate work word acquisition, collected data based situated dialoguevirtual world. data set different data set used previous investigation (Qu & Chai, 2008). difference lies two aspects: 1) datasetcollected mixed initiative human-machine conversation whereas data previous investigation based question answering; 2) user studies workconducted context situated dialogue, human users immersedcomplex virtual world move around virtual environment.3.1 DomainFigure 1: Treasure hunting domainapplication virtual world treasure hunting. human user immersedvirtual world (i.e., castle) 1 able move around look hidden treasures.castle contains 115 3D objects. user needs consult remote expert (i.e.,artificial agent) advice finding treasures. expert knowledgeenvironment treasures cannot see user castle.user needs describe expert sees order get advicego look for.Figure 1 shows snapshot virtual world. Figure 2 shows excerpt dialoguser expert finding one treasures. focus investigationword acquisition, conducted wizard-of-oz studies collect data 2 . specifically, setresponse templates corresponding different types responses predefined.1. virtual world developed based Irrlicht game engine http://irrlicht.sourceforge.net/.2. WOZ studies allow observation user interacting fully functional system,system functions supplemented hidden human wizard. Users led believeinteracting real system (Klemmer, Sinha, Chen, Landay, Aboobaker, & Wang, 2000).technology, permits experimentally controlled study human computer interaction behaviorwithout exposing users unpredicted responses, used widely earlier research speechinterfaces multimodal systems.251fiQu & ChaiS:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:U:S:Tell see.see golden tomb sarcophagus.Describe it.appears made gold Egyptian decorations it.else see?see carving hanging wall appears African.Go back.Im back sarcophagus. appears lava surrounding fire.couldnt understand you.room sarcophagus surrounded lava fire.see statue?try open sarcophagus?No.see statue.Describe.appears stone statue Egyptian.else see?see small vase pottery vase decorated flowers.treasure near.try move vase?Yes.cant move vase.see another one?Yes, theres wooden table pottery jug purple vase it. try movepurple vase?see statue?Yes, see stone statue.see another one?Yes, see gold black statue Egyptian sitting.Compare objects.stone statue bad shape, golden statue appears good shape.golden crown. sitting wooden chair.Try moving it.move head statue see scroll. take scroll?Describe.white scroll red seal.Take it.took scroll.Good.Figure 2: dialog user system finding one treasuressystem turn, system automatically determines kind responses (i.e., template)provided; example, whether ask clarification, provide confirmation,ask information particular object. decision made baseddistribution simulate different types system acceptance rejection. distributionmodified according different needs experiments. Based chosentemplate, human wizard serves language understanding component fillstemplate specific information related user input. filled template252fiContext-Based Word Acquisition Situated Dialogue Virtual Worldused automatically generate natural language processed MicrosoftText-to-Speech engine generate speech responses. experiments, usersspeech recorded, users eye gaze captured Tobii eye tracker.3.2 Data Preprocessing20 users experiments, collected 3709 utterances accompanying gaze fixations.transcribed collected speech. vocabulary size speech transcript 1082,among 757 nouns/adjectives. users speech also automatically recognizedonline Microsoft speech recognizer word error rate (WER) 48.1%1-best recognition. vocabulary size 1-best speech recognition 3041, among1643 nouns/adjectives. nouns adjectives transcriptionsrecognized 1-best hypotheses automatically identified Stanford Part-of-SpeechTagger (Toutanova & Manning, 2000; Toutanova, Klein, Manning, & Singer, 2003).Therespurplevaseorangefacespeech str eamgaze fixationts[table_vase] [vase_purple]gaze str eamte[vase_greek3] [vase_greek3][vase_greek3][vase_greek3][fixated entity]Figure 3: Accompanying gaze fixations 1-best recognition users utteranceTheres purple vase orange vase. (There two incorrectly recognizedwords face 1-best recognition)collected speech gaze streams automatically paired together system.time system detected sentence boundary users speech, pairedrecognized speech gaze fixations system accumulating sincepreviously detected sentence boundary. Figure 3 shows stream pair users speechaccompanying gaze fixations. speech stream, spoken word timestampedspeech recognizer. gaze stream, gaze fixation starting timestampts ending timestamp te provided eye tracker. gaze fixation resultsfixated entity (3D object). multiple entities fixated one gaze fixation dueoverlapping entities, foremost one chosen. gaze stream, neighboringgaze fixations fixate entity merged.Given paired speech gaze streams, build set parallel word sequencesgaze fixated entity sequences {(w, e)} task word acquisition. word sequencew consists nouns adjectives 1-best recognition spoken utterance.entity sequence e contains entities fixated gaze fixations. parallelspeech gaze streams shown Figure 3, resulting word sequence w = [purple vaseorange face] resulting entity sequence e = [table vase vase purple vase greek3 ].253fiQu & Chai4. Translation Models Word AcquisitionSince working conversational systems users interact visual scene,consider task word acquisition associating words visual entitiesdomain. Given parallel speech gaze fixated entities {(w, e)}, formulate wordacquisition translation problem use translation models estimate word-entityassociation probabilities p(w|e). words highest association probabilitieschosen acquired words entity e.4.1 Base ModelUsing translation model (Brown, Pietra, Pietra, & Mercer, 1993), wordequally likely aligned entity,Xl1p(w|e) =p(wj |ei )(l + 1)m(1)j=1 i=0l lengths entity word sequences respectively. refermodel Model-1.4.2 Base Model IIUsing translation model II (Brown et al., 1993), alignments dependentword/entity positions word/entity sequence lengths,p(w|e) =Xlp(aj = i|j, m, l)p(wj |ei )(2)j=1 i=0aj = means wj aligned ei . aj = 0, wj alignedentity (e0 represents null entity). refer model Model-2.Compared Model-1, Model-2 considers ordering words entities wordacquisition. EM algorithms used estimate probabilities p(w|e) translationmodels.5. Incorporating User Language Behavior Word AcquisitionModel-2, word-entity alignments estimated co-occurring word entity sequences unsupervised way. estimated alignments dependentwords/entities appear word/entity sequences, words gazefixated entities actually occur. Motivated findings users move eyesmentioned object directly speaking word (Griffin & Bock, 2000), makeword-entity alignments dependent temporal relation new model (referredModel-2t):Xlp(w|e) =pt (aj = i|j, e, w)p(wj |ei )(3)j=1 i=0254fiContext-Based Word Acquisition Situated Dialogue Virtual Worldpt (aj = i|j, e, w) temporal alignment probability computed based temporal distance entity ei word wj .define temporal distance ei wjts (ei ) ts (wj ) te (ei )0te (ei ) ts (wj ) ts (wj ) > te (ei )d(ei , wj ) =(4)ts (ei ) ts (wj ) ts (wj ) < ts (ei )ts (wj ) starting timestamp (ms) word wj , ts (ei ) te (ei ) startingending timestamps (ms) gaze fixation entity ei .alignment word wj entity ei decided temporal distance d(ei , wj ).Based psycholinguistic finding eye gaze happens spoken word, wjallowed aligned ei wj happens earlier ei (i.e., d(ei , wj ) > 0). wjhappens earlier ei (i.e., d(ei , wj ) 0), closer are, likelyaligned. Specifically, temporal alignment probability wj ei co-occurringinstance (w, e) computed0d(ei , wj ) > 0exp[ d(ei , wj )]d(ei , wj ) 0lpt (aj = i|j, e, w) =(5)Xexp[ d(ei , wj )]i=0constant scaling d(ei , wj ).EM algorithm used estimate probabilities p(w|e) Model-2t.worthwhile mention that, findings psycholinguistic studies provided specificoffsets terms eye gaze corresponds speech production. example, showsspeakers look object second say it, 100-300 msarticulation object name begins, eyes move next object relevanttask (Meyer et al., 1998). Since conversation setting study muchcomplex simple settings psycholinguistic research, found larger variationsoffset (Liu et al., 2007) data. Therefore chose use offsetalignment model here.6. Incorporating Domain Knowledge Word AcquisitionSpeech-gaze temporal alignment occurrence statistics sometimes sufficientassociate words entities correctly. example, suppose user says lampdresser looking lamp object table object. Due co-occurrencelamp object, words dresser lamp likely associated lampobject translation models. result, word dresser likely incorrectlyacquired lamp object. reason, word lamp could acquiredincorrectly table object. solve type association problem, semanticknowledge domain words helpful. example, knowledgeword lamp semantically related object lamp help system avoidassociating word dresser lamp object. Specifically, solve type wordentity association utilizing domain knowledge present system externallexical semantic resources.255fiQu & Chaione hand, conversational system domain model, knowledgerepresentation domain types objects properties relations, task structures, etc. domain model usually acquired developmentstage deployment system. domain model provides important resource enable domain reasoning language interpretation (DeVault & Stone, 2003).hand, available resources domain independent lexical knowledge (e.g., WordNet, see Fellbaum, 1998). idea domain modellinked external lexical resources either manually automatically developmentstate, external knowledge source used help constrain acquired words.following sections, first describe domain modeling, define semanticrelatedness word entity based domain modeling WordNet semantic lexicon,finally describe different ways using semantic relatedness word entityhelp word acquisition.6.1 Domain Modelingmodel treasure hunting domain shown Figure 4. domain model containsdomain related semantic concepts. practical conversational systems, domainmodeling typically acquired development stage either manual authoringdomain experts automated learning based annotated data. current work,properties domain entities represented domain concepts. entity properties include: semantic type, color, size, shape, material. use WordNet synsetsrepresent domain concepts (i.e., synsets format word#part-of-speech#senseid). sense-id represents specific WordNet sense associated wordrepresenting concept. example, domain concepts SEM PLATE COLORentity plate represented synsets plate#n#4 color#n#1 WordNet.Note link domain concepts WordNet synsets automatically acquired given existing vocabularies. Here, illustrate idea, simplify problemdirectly connect domain concepts synsets.Note domain model, domain concepts specific certain entity,general concepts certain type entity. Multiple entities typeproperties share set domain concepts. Therefore, propertiescolor size entity general concepts color#n#1 size#n#1instead specific concepts like yellow#a#1 big#a#1, conceptsshared entities type, different colors sizes.6.2 Semantic Relatedness Word Entitycompute semantic relatedness word w entity e based semanticsimilarity w properties e using domain model bridge. Specifically,semantic relatedness SR(e, w) definedSR(e, w) = max sim(s(cie ), sj (w))i,j(6)cie i-th property entity e, s(cie ) synset property cie domain model,sj (w) j-th synset word w defined WordNet, sim(, ) similarityscore two synsets.256fiContext-Based Word Acquisition Situated Dialogue Virtual Worldn e lEntities:Domainconcepts:pharaohplateSEM_PLATECOLORSEM_PHARAOHCOLORSIZEcolor#n#1WordNetconcepts:pharaoh#n#1plate#n#4picture#n#2size#n#1Figure 4: Domain model domain concepts represented WordNet synsetscomputed similarity score two synsets based path length them.similarity score inversely proportional number nodes along shortest pathsynsets defined WordNet. two synsets same,maximal similarity score 1. WordNet-Similarity tool (Pedersen, Patwardhan, &Michelizzi, 2004) used synset similarity computation.6.3 Word Acquisition Word-Entity Semantic Relatednessuse semantic relatedness word entity help system acquire semantically compatible words entity, therefore improve word acquisition performance.semantic relatedness applied word acquisition two ways: post-processlearned word-entity association probabilities rescoring semantic relatedness,directly affect learning word-entity associations constraining alignmentword entity translation models.6.3.1 Rescoring Semantic Relatednessacquired word list entity ei , word wj association probability p(wj |ei )learned translation model. use semantic relatedness SR(ei , wj )redistribute probability mass wj . new association probability given by:p(wj |ei )SR(ei , wj )p0 (wj |ei ) = Xp(wj |ei )SR(ei , wj )j257(7)fiQu & Chai6.3.2 Semantic Alignment Constraint Translation Modelused constrain word-entity alignment translation model, semantic relatedness used alone used together speech-gaze temporal information decidealignment probability word entity (Qu & Chai, 2008).Using semantic relatedness constrain word-entity alignments Model-2s,Xlp(w|e) =ps (aj = i|j, e, w)p(wj |ei )(8)j=1 i=0ps (aj = i|j, e, w) alignment probability based semantic relatedness,SR(ei , wj )ps (aj = i|j, e, w) = XSR(ei , wj )(9)Using semantic relatedness temporal information constrain word-entity alignments Model-2ts,p(w|e) =Xlpts (aj = i|j, e, w)p(wj |ei )(10)j=1 i=0pts (aj = i|j, e, w) alignment probability decided temporalrelation semantic relatedness ei wj ,ps (aj = i|j, e, w)pt (aj = i|j, e, w)pts (aj = i|j, e, w) = Xps (aj = i|j, e, w)pt (aj = i|j, e, w)(11)ps (aj = i|j, e, w) semantic alignment probability Equation (9),pt (aj = i|j, e, w) temporal alignment probability given Equation (5).EM algorithms used estimate p(w|e) Model-2s Model-2ts.7. Incorporating Conversation Context Word Acquisitionmentioned earlier, speech-gaze pairs useful word acquisition. speechgaze pair, speech word relates gaze fixatedentities, instance adds noise word acquisition. Therefore, identifyclosely coupled speech-gaze pairs use word acquisition.section, first describe feature extraction based conversation interactivity, describe use logistic regression classifier predict whether speech-gazepair closely coupled speech-gaze instance instance least one nounadjective speech stream referring gaze fixated entity gaze stream.training classifier speech-gaze prediction, manually labeled instance whether closely coupled speech-gaze instance based speech transcriptgaze fixations.258fiContext-Based Word Acquisition Situated Dialogue Virtual World7.1 Features Extractionparallel speech-gaze instance, following sets features automatically extracted.7.1.1 Speech Features (S-Feat)Let cw count nouns adjectives utterance, ls temporal lengthspeech. following features extracted speech:cw count nouns adjectives.nouns adjectives expected users utterance describing entities.cw /ls normalized noun/adjective count.effect speech length ls cw considered.7.1.2 Gaze Features (G-Feat)fixated entity ei , let lei fixation temporal length. Note several gazefixations may fixated entity, lei total length gaze fixationsfixate entity ei . extract following features gaze stream:ce count different gaze fixated entities.Less fixated entities expected user describing entities lookingthem.ce /ls normalized entity count.effect speech temporal length ls ce considered.maxi (lei ) maximal fixation length.least one fixated entitys fixation expected long enough userdescribing entities looking them.mean(lei ) average fixation length.average gaze fixation length expected longer user describingentities looking them.var(lei ) variance fixation lengths.variance fixation lengths expected smaller user describing entities looking them.number gaze fixated entities decided users eye gaze, alsoaffected visual scene. Let cse count entities visiblelength gaze stream. also extract following scene related feature:ce /cse scene normalized fixated entity count.effect visual scene ce considered.259fiQu & Chai7.1.3 User Activity Features (UA-Feat)interacting system, users activity also helpful determiningwhether users eye gaze tightly linked content speech. followingfeatures extracted users activities:maximal distance users movements maximal change user position (3Dcoordinates) speech length.user expected move within smaller range looking entitiesdescribing them.variance users positionsuser expected move less frequently looking entities describingthem.7.1.4 Conversation Context Features (CC-Feat)talking system (i.e., expert), users language gaze behaviorinfluenced state conversation. speech-gaze instance, useprevious system response type nominal feature predict whether closelycoupled speech-gaze instance.treasure hunting domain, eight types system responses two categories:System-Initiative Responses:specific-see system asks whether user sees certain entity, e.g., seeanother couch?.nonspecific-see system asks whether user sees anything, e.g., seeanything else?, Tell see.previous-see system asks whether user previously sees something, e.g.,previously seen similar object?.describe system asks user describe detail user sees, e.g.,Describe it, Tell it.compare system asks user compare user sees, e.g., Compareobjects.clarify system asks user make clarification, e.g., understandthat, Please repeat that.action-request system asks user take action, e.g., Go back, Try movingit.User-Initiative Responses:misc system hands initiative back user without specifyingrequirements, e.g., dont know, Yes.260fiContext-Based Word Acquisition Situated Dialogue Virtual World7.2 Logistic Regression ModelGiven extracted feature x closely coupled label instancetraining set, train ridge logistic regression model (Cessie & Houwelingen, 1992)predict whether instance closely coupled instance (y = 1) (y = 0).logistic regression model, probability yi = 1, given feature xi =(xi1 , xi2 , . . . , xim ), modeledPexp(j=1 j xj )Pp(yi |xi ) =1 + exp(j=1 j xj )j features weights learned.log-likelihood l data (X, y)Xl() =[yi log p(yi |xi ) + (1 yi ) log(1 p(yi |xi ))]ridge logistic regression, parameters j estimated maximizing regularized loglikelihoodl () = l() ||||2ridge parameter introduced achieve stable parameter estimation.7.3 Evaluation Speech-gaze IdentificationSince goal identifying closely coupled speech-gaze instances improve word acquisition interested acquiring nouns adjectives, instancesrecognized nouns/adjectives used training logistic regression classifier. Among2969 instances recognized nouns/adjectives gaze fixations, 2002 (67.4%) instances labeled closely coupled. speech-gaze prediction evaluated 10-foldcross validation.Table 1 shows prediction precision recall different sets features used.seen table, features used, prediction precision goesrecall goes down. important note prediction precision criticalrecall word acquisition sufficient amount data available. Noisy instancesgaze link speech content hurt word acquisition sinceguide translation models ground words wrong entities. Althoughhigher recall helpful, effect expected become less co-occurrencesalready established.results show speech features (S-Feat) conversation context features (CCFeat), used alone, improve prediction precision much compared baselinepredicting instances closely coupled precision 67.4%. used alone,gaze features (G-Feat) user activity features (UA-Feat) two useful featuresets increasing prediction precision. used together, prediction precision increased. Adding either speech features conversation context featuresgaze user activity features (G-Feat + UA-Feat + S-Feat/CC-Feat) increasesprediction precision more. Using four sets features (G-Feat + UA-Feat + S-Feat +261fiQu & ChaiFeature setsNull (baseline)S-FeatG-FeatUA-FeatCC-FeatG-Feat + UA-FeatG-Feat + UA-Feat + S-FeatG-Feat + UA-Feat + CC-FeatG-Feat + UA-Feat + S-Feat + CC-FeatPrecision0.6740.6860.7070.7040.6880.7190.7410.7310.748Recall10.9950.9580.9420.9360.9480.9080.9180.899Table 1: Speech-gaze prediction performances different feature setsCC-Feat) achieves highest prediction precision. McNemar tests shownsignificant change compared using G-Feat + UA-Feat + S-Feat (2 = 8.3, p < 0.004)feature configurations (2 = 45.4 442.7, p < 0.0001). Therefore, chooseuse feature sets identify closely coupled speech-gaze instances word acquisition.8. Evaluation Word Acquisitionpractical conversational system starts initial knowledge base (vocabulary).assume system already one default word entity default vocabulary.default word entity indicates semantic type entity. example,word barrel default word entity barrel. Among acquired words,evaluate new words systems vocabulary. example, wordbarrel would excluded candidate words acquired entity barrel.8.1 Grounding Words Domain ConceptsBased translation models word acquisition (Sections 5 & 6), obtainword-entity association probability p(w|e). probability provides means groundwords entities. conversational systems, one important goal word acquisitionmake system understand semantic meaning new words. Word acquisitiongrounding words objects always sufficient identifying semantic meanings.Suppose word green grounded green chair object, word chair. Althoughsystem aware green word describing green chair, knowword green refers chairs color word chair refers chairssemantic type. Thus, learning word-entity associations p(w|e) translationmodels, need ground words domain concepts entity properties.Based domain model discussed earlier (Section 6.1), apply WordNet groundwords domain concepts. entity e, based association probabilities p(w|e),choose n-best words acquired words e. n-best words n highestassociation probabilities. word w acquired e, grounded concept ce w262fiContext-Based Word Acquisition Situated Dialogue Virtual Worldchosen one highest semantic relatedness w:ce = arg max[max sim(s(cie ), sj (w))]j(12)sim(s(cie ), sj (w)) semantic similarity score defined Equation (6).evaluate acquired words domain concepts, manually compile setgold standard words users speech transcripts gaze fixations. goldstandard words words users used refer entitiesproperties (e.g., color, size, shape) interaction system. automatically acquired words evaluated gold standard words.8.2 Evaluation MetricsFollowing standard evaluation information retrieval, following metrics usedevaluate words acquired domain concepts (i.e., entity properties) {ce }.PrecisionPceRecall# words correctly acquired cePce # words acquired cePce # words correctly acquired cePce # gold standard words ceMean Average Precision (MAP)MAP =X PNw P (r) rel(r)r=1Nee#eNe number gold standard words properties ce entitye, Nw vocabulary size, P (r) acquisition precision given cut-off rank r,rel(r) binary function indicating whether word rank r gold-standardword property ce entity e.8.3 Evaluation Resultsinvestigate effects speech-gaze temporal information domain semantic knowledge word acquisition, compare word acquisition performances followingmodels:Model-1 base model without word-entity alignment (Equation (1)).Model-1-r Model-1 semantic relatedness rescoring word-entity association.Model-2 base model II positional alignment (Equation (2)).Model-2s enhanced model semantic alignment (Equation (8)).Model-2t enhanced model temporal alignment (Equation (3)).263fiQu & ChaiModel-2ts enhanced model temporal semantic alignment (Equation (10)).Model-2t-r Model-2t semantic relatedness rescoring word-entity association.8.3.1 Results Using Speech-Gaze Temporal Information10.4Model-1Model-2Model-2t0.90.38Mean Average Precision0.8Precision0.70.60.50.40.30.20.340.320.30.280.260.100.3600.10.20.30.40.50.60.70.80.910.24M-1M-2M-2tRecall(a) Precision vs Recall(b) Mean Average PrecisionFigure 5: Word acquisition performance speech-gaze temporal information usedFigure 5 shows interpolated precision-recall curves Mean Average Precisions(MAPs) Model-2t baseline models Model-1 Model-2. shown figure,Model-2 improve word acquisition compared Model-1. result showshelpful consider index-based positional alignment word entityword acquisition. incorporating speech-gaze temporal alignment, Model-2t consistentlyachieves higher precisions Model-1 different recalls. terms MAP, Model-2tsignificantly increases MAP (t = 3.08, p < 0.002) compared Model-1. meansuse speech-gaze temporal alignment improves word acquisition.8.3.2 Results Using Domain Semantic RelatednessFigure 6 shows results using domain semantic relatedness word acquisition.shown figure, compared baseline using extra knowledge (Model-1),using domain semantic relatedness improves word acquisition matter usedrescore word-entity association (Model-1-r) constrain word-entity alignment (Model2s). Compared Model-1, MAP significantly improved Model-1-r (t = 6.32, p <0.001) Model-2s (t = 5.36, p < 0.001).Domain semantic relatedness also used together speech-gaze temporal information improve word acquisition. Compared Model-1, MAP significantly increased Model-2ts (t = 5.59, p < 0.001) uses semantic relatedness together temporal information constrain word-entity alignments Model-2t-r (t = 6.01, p < 0.001),semantic relatedness used rescore word-entity associations learned Model2t.264fiContext-Based Word Acquisition Situated Dialogue Virtual World10.4Model-1Model-1-rModel-2sModel-2tsModel-2t-r0.8Precision0.70.38Mean Average Precision0.90.60.50.40.30.20.340.320.30.280.260.100.3600.10.20.30.40.50.60.70.80.910.24M-1M-1-rM-2sM-2tsM-2t-rRecall(a) Precision vs Recall(b) Mean Average PrecisionFigure 6: Word acquisition performance domain semantic relatedness usedComparing two ways using semantic relatedness word acquisition, foundrescoring word-entity association semantic relatedness works better. Model-2t-rachieve higher MAP (t = 2.22, p < 0.015) Model-2ts.also verified using speech-gaze temporal alignment domain semanticrelatedness rescoring works better using either one alone. temporal alignmentsemantic relatedness rescoring, Model-2t-r significantly increases MAP comparedModel-1-r (t = 2.75, p < 0.004) semantic relatedness rescoring usedModel-2t (t = 5.38, p < 0.001) temporal alignment used.8.3.3 Results Based Identified Closely Coupled Speech-Gaze Streamsshown Model-2t-r, speech-gaze temporal alignment domainsemantic relatedness rescoring incorporated, achieves best word acquisition performance. Therefore, Model-2t-r used evaluate word acquisition basedidentified closely coupled speech-gaze data. Since Model-2t-r requires linking domain models external knowledge source (e.g., WordNet) may availableapplications, also evaluate effect identification closely coupled speech-gazestreams word acquisition Model-2t, speech-gaze temporal alignmentincorporated.evaluate effect automatic identification closely coupled speech-gaze instancesword acquisition 10-fold cross validation. fold, 10% data setused train logistic regression classifier predicting closely coupled speechgaze instances, instances, predicted closely coupled instances, true (manuallylabeled) closely coupled instances 90% data set used wordacquisition respectively. Figures 7 & 8 show averaged interpolated precision-recall curvesMAPs achieved Model-2t Model-2t-r using instances (all ), predictedclosely coupled instances (predicted ), true closely coupled instances (true).words acquired Model-2t, shown Figure 7, using predicted closelycoupled instances achieves better performance using instances. MAP significantly increased (t = 2.69, p < 0.005) acquiring words predicted closely coupled265fiQu & Chai10.42predictedtrue0.90.4Mean Average Precision0.8Precision0.70.60.50.40.30.20.380.360.340.1000.10.20.30.40.50.60.70.80.90.321predictedtrueRecall(a) Precision vs Recall(b) Mean Average PrecisionFigure 7: Word acquisition performance Model-2t different data set10.42predictedtrue0.90.4Mean Average Precision0.8Precision0.70.60.50.40.30.20.380.360.340.1000.10.20.30.40.50.60.70.80.910.32predictedtrueRecall(a) Precision vs Recall(b) Mean Average PrecisionFigure 8: Word acquisition performance Model-2t-r different data setinstances. result shows identification closely coupled speech-gaze instanceshelps word acquisition. true closely coupled speech-gaze instances usedword acquisition, acquisition performance improved. means betteridentification closely coupled speech-gaze instances lead better word acquisitionperformance.words acquired Model-2t-r, shown Figure 8, using predicted closelycoupled instances improves acquisition performance compared using instances.acquiring words predicted closely coupled speech-gaze instances, MAP increased(t = 1.81, p < 0.037) although improvement less significant one Model2t.266fiContext-Based Word Acquisition Situated Dialogue Virtual World10.561-besttranscriptpredicted 1-bestpredicted transcript0.80.52Mean Average Precision0.9Precision0.70.60.50.40.30.21-besttranscript0.480.440.40.360.1000.10.20.30.40.50.60.70.80.90.321predictedRecall(a) Precision vs Recall(b) Mean Average PrecisionFigure 9: Word acquisition performance Model-2t speech recognition transcript10.561-besttranscriptpredicted 1-bestpredicted transcript0.80.52Mean Average Precision0.9Precision0.70.60.50.40.30.21-besttranscript0.480.440.40.360.1000.10.20.30.40.50.60.70.80.910.32predictedRecall(a) Precision vs Recall(b) Mean Average PrecisionFigure 10: Word acquisition performance Model-2t-r speech recognition transcript8.3.4 Comparison Results Based Speech Recognition Transcriptshow effect speech recognition quality word acquisition, also compareacquisition performances based speech transcript 1-best recognition. wordacquisition based speech transcript, word sequence parallel speech-gaze dataset contains nouns adjectives speech transcript. Accordingly, speech featureused coupled speech-gaze identification extracted speech transcript.Figures 9 & 10 show word acquisition performances Model-2t Model-2t-r usinginstances using predicted coupled instances based speech transcript1-best recognition respectively. shown figures, quality speech recognitioncritical word acquisition performance. expected, word acquisition performance basedspeech transcript much better recognized speech.267fiQu & Chai9. ExamplesTable 2 shows 10-best candidate words acquired entity couch Model-1, Model2t, Model-2t-r based speech-gaze instances Model-2t-r based predictedclosely coupled instances. probabilities candidate words also giventable. Across models, although four words (shown bold font)acquired model, ranking acquired words achieves best Model-2t-rbased predicted closely coupled instances.Table 3 shows another example 10-best candidate words acquired entitystool four different models. Model-1 acquires four correct words 10-best list.Although Model-2t also acquires four correct words 10-best list, rankingswords higher. speech-gaze temporal alignment domain semanticrelatedness rescoring, Model-2t-r acquires seven correct words 10-best list.rankings correct words also improved. Compared using instancesModel-2t-r, although using predicted coupled instances Model-2t-r resultsseven correct words ranks 10-best list, probabilitiescorrectly acquired words higher. means results based predictedcoupled instances confident.ModelRank 1Rank 2Rank 3Rank 4Rank 5Rank 6Rank 7Rank 8Rank 9Rank 10Model-1couch(0.1105)bedroom(0.1047)chair(0.1004)bad(0.0936)room(0.0539)wooden(0.0354)bench(0.0319)small(0.0289)yellow(0.0274)couple(0.0270)Model-2tcouch(0.1224)chair(0.0798)bed(0.0593)small(0.0536)room(0.0528)bad(0.0489)yellow(0.0333)bench(0.0332)lot(0.0331)wooden(0.0226)Model-2t-rcouch(0.4743)chair(0.1668)bench(0.0949)bed(0.0311)small(0.0235)bad(0.0226)room(0.0174)lot(0.0151)yellow(0.0107)couple(0.0085)Model-2t-r(predicted)couch(0.4667)chair(0.1557)bench(0.11129)bed(0.0368)small(0.0278)bad(0.0265)room(0.0137)yellow(0.0127)couple(0.0101)lot(0.0090)Table 2: N-best candidate words acquired entity couch different modelsModelRank 1Rank 2Rank 3Rank 4Rank 5Rank 6Rank 7Rank 8Rank 9Rank 10Model-1plant(0.0793)room(0.0508)little(0.0471)flower(0.0424)stairs(0.0320)call(0.0319)square(0.0302)footstool(0.0301)brown(0.0300)short(0.0294)Model-2tplant(0.0592)room(0.0440)little(0.0410)flower(0.0409)square(0.0408)small(0.0403)next(0.0308)stool(0.0307)brown(0.0300)stairs(0.0226)Model-2t-rstool(0.1457)little(0.1435)small(0.1412)footstool(0.0573)ottoman(0.0572)ground(0.0275)media(0.0263)chair(0.0257)plant(0.0253)square(0.0234)Model-2t-r(predicted)stool(0.1532)little(0.1509)small(0.1490)footstool(0.0602)ottoman(0.0601)ground(0.0289)media(0.0276)chair(0.0272)plant(0.0270)square(0.0247)Table 3: N-best candidate words acquired entity stool different models268fiContext-Based Word Acquisition Situated Dialogue Virtual World10. Effect Online Word Acquisition Language UnderstandingOne important goal word acquisition use acquired new words help languageunderstanding subsequent conversation. demonstrate effect online word acquisition language understanding, conduct simulation studies based collecteddata. simulations, system starts initial knowledge base vocabularywords associated domain concepts. system continuously enhances knowledgebase acquiring words users Model-2t-r incorporates speech-gazetemporal information domain semantic relatedness. enhanced knowledge baseused understand language new users.evaluate language understanding performance concept identification rate (CIR):CIR =#correctly identified concepts 1-best speech recognition#concepts speech transcriptsimulate process online word acquisition evaluate effect languageunderstanding two situations: 1) system starts training datasmall initial vocabulary, 2) system starts training data.10.1 Simulation 1: System Starts Training Databuild conversational systems, one approach domain experts provide domain vocabulary system design time. first simulation follows practice.system provided default vocabulary start without training data. defaultvocabulary contains one seed word domain concept.Using collected data 20 users, simulation process goes followingsteps:user index = 1, 2, . . . , 20:Evaluate CIR i-th users utterances (1-best speech recognition)current system vocabulary.Acquire words instances (with 1-best speech recognition) users1 i.Among 10-best acquired words, add verified new words system vocabulary.process, language understanding performance individual userdepends users language well users position user sequence.reduce effect user ordering language understanding performance,simulation process repeated 1000 times randomly ordered users. averageCIRs simulations shown Figure 11.Figure 11 also shows CIRs system static knowledge base (vocabulary). curve drawn way curve dynamic knowledgebase, except without word acquisition random simulation processes. seefigure, system doest word acquisition capability, language understanding performance change users communicated system.capability automatic word acquisition, systems language understandingperformance becomes better users talked system.269fiQu & Chai0.7static knowledge basedynamic knowledge base0.650.6CIR0.550.50.450.40.350.30.25123456789 10 11 12 13 14 15 16 17 18 19 20User IndexFigure 11: CIR user language achieved system starting training data10.2 Simulation 2: System Starts Training DataMany conversational systems use real user data derive domain vocabulary. followpractice, second simulation provides system training data. trainingdata serves two purposes: 1) build initial vocabulary system; 2) train classifierpredict closely coupled speech-gaze instances new users data.Using collected data 20 users, simulation process goes followingsteps:Using first users data training data, acquire words training instances(with speech transcript); add verified 10-best words systems vocabularyseed words; build classifier training data prediction closely coupledspeech-gaze instances.Evaluate effect incremental word acquisition CIR remaining (20-m)users data. user index = 1, 2, . . . , (20-m):Evaluate CIR i-th users utterances (1-best speech recognition).Predict closely coupled speech-gaze instances i-th users data.Acquire words training users true coupled instances (with speechtranscript) predicted coupled instances (with 1-best speech recognition)users 1 i.Among 10-best acquired words, add verified new words system vocabulary.simulation process repeated 1000 times randomly ordered usersreduce effect user ordering language understanding performance. Figure 12shows averaged language understanding performance random simulations.language understanding performance system static knowledge basealso shown Figure 12. curve drawn random simulations withoutsteps word acquisition. observe general trend figure that, wordacquisition, systems language understanding becomes better users270fiContext-Based Word Acquisition Situated Dialogue Virtual World0.6static knowledge basedynamic knowledge base0.59CIR0.580.570.560.5512345678910User IndexFigure 12: CIR user language achieved system starting training data 10userscommunicated system. Without word acquisition capability, systems languageunderstanding performance increase users conversedsystem.simulations show automatic vocabulary acquisition beneficial systemslanguage understanding performance training data available. training dataavailable, vocabulary acquisition could important beneficial robustlanguage understanding.10.3 Effect Speech Recognition Online Word AcquisitionLanguage Understandingsimulation results Figures 11 & 12 based 1-best recognized speech hypotheses relatively high WER (48.1%). better speech recognition, systembetter concept identification performance. show effect speech recognitionquality online word acquisition language understanding, also perform Simulation 1 Simulation 2 based speech transcript. simulation processesones based 1-best speech recognition except word acquisition basedspeech transcript CIR evaluated also speech transcript new simulations.Figure 13 shows CIR curves based speech transcript online conversation.word acquisition, systems language understanding becomes betterusers communicated system. consistent CIR curves based1-best speech recognition. However, CIRs based speech transcript much higherCIRs based 1-best speech recognition, verifies speech recognitionquality critical language understanding performance.11. Discussion Future Workexperimental results shown incorporating extra information improves wordacquisition compared completely unsupervised approaches. However, current ap271fiQu & Chai10.95static knowledge basedynamic knowledge basestatic knowledge basedynamic knowledge base0.80.93CIR0.94CIR0.90.70.920.60.910.5123456780.99 10 11 12 13 14 15 16 17 18 19 20User Index(a) Simulation 1: training data12345678910User Index(b) Simulation 2: training data 10 usersFigure 13: CIR user language (transcript) achieved system online conversationproaches several limitations. First, incorporation domain knowledgesemantic relatedness based WordNet restrict acquired words appearWordNet. certainly desirable. limitation readily addressedchanging way word probability distribution tailored semantic relatedness(in Section 6.3.1 Section 6.3.2). example, one simple way keep probabilitymass words WordNet tailor distribution wordsoccur WordNet based semantic relatedness object.Second, current approach, acquired words limited wordsrecognized speech recognizer. shown Section 8.3.4, speech recognitionperformance rather poor experiments. partly due lack languagemodels specifically trained domain. Approaches improve speech recognition,example, based referential semantic language model described (Schuler, Wu, &Schwartz, 2009) potentially improve acquisition performance. Furthermore, setacquired words bounded vocabulary speech recognizer. new wordsdictionary acquired. break barrier, inspired previouswork (Yu & Ballard, 2004; Taguchi et al., 2009), currently extending approachincorporate grounding acoustic phoneme sequences domain concepts.Another limitation current approaches incapable acquiringmultiword expressions. map single words domain concepts. However,observe multiword expressions (e.g., Rubiks cube) data. examineissue future work incorporating linguistic knowledge modelingfertility entities, example, IBM Model 3 4.simplicity current models also limits word acquisition performance.example, alignment model based temporal information directly incorporates findingspsycholinguistic studies. studies generally conducted much simplersettings without interaction. recent work Fang, Chai, Ferreira (2009) showncorrelations intensity gaze fixations objects denoted linguistic centers272fiContext-Based Word Acquisition Situated Dialogue Virtual World(e.g., forward-looking centers based centering theory, Grosz, Joshi, & Weinstein, 1995).plan incorporate results improve alignment modeling future.improve performance, another interesting direction take consideration interactive nature conversation, example, combining dialog managementsolicit user feedback acquired words. However, important identifystrategies balance trade explicit feedback solicitation (and thus lengthening interaction) quality acquired words. Reinforcement learningpotential approach address problem.12. ConclusionsMotivated psycholinguistic findings, investigate use eye gaze automatic word acquisition multimodal conversational systems. paper presents severalenhanced models incorporate user language behavior, domain knowledge, conversation context word acquisition. experiments shown enhancedmodels significantly improve word acquisition performance.Recent advancement eye tracking technology made available non-intrusive eyetracking devices tolerate head motion provide high tracking quality. Integrating eye tracking conversational interfaces longer beyond reach. believeincorporating eye gaze automatic word acquisition provides another potential approachimprove robustness human-machine conversation.Acknowledgmentswork supported IIS-0347548 IIS-0535112 National Science Foundation. would like thank anonymous reviewers valuable commentssuggestions.ReferencesAllopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K. (1998). Tracking time coursespoken word recognition using eye movements: Evidence continuous mappingmodels. Journal Memory & Language, 38, 419439.Barnard, K., Duygulu, P., de Freitas, N., Forsyth, D., Blei, D., & Jordan, M. (2003). Matching words pictures. Journal Machine Learning Research, 3, 11071135.Bock, K., Irwin, D. E., Davidson, D. J., & Leveltb, W. (2003). Minding clock. JournalMemory Language, 48, 653685.Brown, P. F., Pietra, S. D., Pietra, V. J. D., & Mercer, R. L. (1993). mathematicstatistical machine translation: Parameter estimation. Computational Linguistics,19 (2), 263311.Brown-Schmidt, S., & Tanenhaus, M. K. (2006). Watching eyes talking size:investigation message formulation utterance planning. Journal MemoryLanguage, 54, 592609.273fiQu & ChaiByron, D., Mampilly, T., Sharma, V., & Xu, T. (2005). Utilizing visual attentioncross-modal coreference interpretation. Proceedings Fifth InternationalInterdisciplinary Conference Modeling Using Context (CONTEXT-05), pp.8396.Campana, E., Baldridge, J., Dowding, J., Hockey, B., Remington, R., & Stone, L. (2001).Using eye movements determine referents spoken dialogue system. Proceedings Workshop Perceptive User Interface.Cessie, S. L., & Houwelingen, J. V. (1992). Ridge estimators logistic regression. AppliedStatistics, 41 (1), 191201.Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded languageacquisition. Proceedings 25th International Conference Machine Learning(ICML).Cooke, N. J. (2006). Gaze-Contigent Automatic Speech Recognition. Ph.D. thesis, UniversityBirminham.Dahan, D., & Tanenhaus, M. K. (2005). Looking rope looking snake:Conceptually mediated eye movements spoken-word recognition. PsychonomicBulletin & Review, 12 (3), 453459.DeVault, D., & Stone, M. (2003). Domain inference incremental interpretation.Proceedings ICoS.Eberhard, K., Spivey-Knowiton, M., Sedivy, J., & Tanenhaus, M. (1995). Eye movementswindow real-time spoken language comprehension natural contexts. JournalPsycholinguistic Research, 24, 409436.Fang, R., Chai, J. Y., & Ferreira, F. (2009). linguistic attention gaze fixationsinmultimodal conversational interfaces. Proceedings International ConferenceMultimodal Interfaces (ICMI), pp. 143150.Fazly, A., Alishahi, A., & Stevenson, S. (2008). probabilistic incremental model wordlearning presence referential uncertainty. Proceedings 30th AnnualConference Cognitive Science Society.Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.Fleischman, M., & Roy, D. (2005). Intentional context situated language learning.Proceedings 9th Conference Computational Natural Language Learning(CoNLL).Fong, T. W., & Nourbakhsh, I. (2005). Interaction challenges human-robot space exploration. Interactions, 12 (2), 4245.Gorniak, P., & Roy, D. (2004). Grounded semantic composition visual scenes. JournalArtificial Intelligence Research, 21, 429470.Griffin, Z., & Bock, K. (2000). eyes say speaking. Psychological Science,11, 274279.Griffin, Z. M. (2001). Gaze durations speech reflect word selection phonologicalencoding. Cognition, 82, B1B14.274fiContext-Based Word Acquisition Situated Dialogue Virtual WorldGriffin, Z. M. (2004). look? Reasons eye movements related language production.Henderson, J., & Ferreira, F. (Eds.), Interface Language, Vision, Action:Eye Movements Visual World, pp. 213248. Taylor Francis.Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: framework modelinglocal coherence discourse. Computational Linguistics, 21 (2), 203226.Jacob, R. J. K. (1991). use eye movements human-computer interaction techniques:look get. ACM Transactions Information Systems, 9 (3),152169.Kahneman, D. (1973). Attention Effort. Prentice-Hall, Inc., Englewood Cliffs.Kaur, M., Termaine, M., Huang, N., Wilder, J., Gacovski, Z., Flippo, F., & Mantravadi,C. S. (2003). it? event synchronization gaze-speech input systems.Proceedings International Conference Multimodal Interfaces (ICMI).Klemmer, S., Sinha, A., Chen, J., Landay, J., Aboobaker, N., & Wang, A. (2000). SUEDE:wizard oz prototyping tool speech user interfaces. Proceedings ACMSymposium User Interface Software Technology, pp. 110.Lemon, O., Gruenstein, A., & Peters, S. (2002). Collaborative activities multitaskingdialogue systems. Traitement Automatique des Langues, 43 (2), 131154.Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondencesless supervision. Proceedings 47th Annual Meeting AssociationComputational Linguistics (ACL).Liu, Y., Chai, J., & Jin, R. (2007). Automated vocabulary acquisition interpretationmultimodal conversational systems. Proceedings 45th Annual MeetingAssociation Computational Linguistics (ACL).Meyer, A., Sleiderink, A., & Levelt, W. (1998). Viewing naming objects: eye movementsnoun phrase production. Cognition, 66 (22), 2533.Nakano, Y., Reinstein, G., Stocky, T., & Cassell, J. (2003). Towards model face-to-facegrounding. Proceedings Annual Meeting Association ComputationalLinguistics (ACL).Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet::Similarity - measuringrelatedness concepts. Proceedings Nineteenth National ConferenceArtificial Intelligence (AAAI).Prasov, Z., & Chai, J. Y. (2008). Whats gaze? role eye-gaze reference resolutionmultimodal conversational interfaces. Proceedings ACM 12th InternationalConference Intelligent User interfaces (IUI).Qu, S., & Chai, J. Y. (2007). exploration eye gaze spoken language processingmultimodal conversational interfaces. Proceedings Human Language Technology Conference North American Chapter Association ComputationalLinguistics (HLT-NAACL), pp. 284291.Qu, S., & Chai, J. Y. (2008). Incorporating temporal semantic information eye gazeautomatic word acquisition multimodal conversational systems. Proceedings275fiQu & ChaiConference Empirical Methods Natural Language Processing (EMNLP),pp. 244253.Qu, S., & Chai, J. Y. (2009). role interactivity human-machine conversationautomatic word acquisition. Proceedings 10th Annual Meeting SpecialInterest Group Discourse Dialogue (SIGDIAL), pp. 188195.Qvarfordt, P., & Zhai, S. (2005). Conversing user based eye-gaze patterns.Proceedings Conference Human Factors Computing Systems (CHI).Rayner, K. (1998). Eye movements reading information processing - 20 yearsresearch. Psychological Bulletin, 124 (3), 372422.Roy, D. (2002). Learning visually-grounded words syntax scene description task.Computer Speech Language, 16 (3), 353385.Roy, D., & Pentland, A. (2002). Learning words sights sounds, computationalmodel. Cognitive Science, 26 (1), 113146.Schuler, W., Wu, S., & Schwartz, L. (2009). framework fast incremental interpretationspeech decoding. Computational Linguistics, 35 (3), 313343.Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception usingforce dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.Spivey, M. J., Tanenhaus, M. K., Eberhard, K. M., & Sedivy, J. C. (2002). Eye movementsspoken language comprehension: Effects visual context syntactic ambiguityresolution. Cognitive Psychology, 45, 447481.Taguchi, R., Iwahashi, N., Nose, T., Funakoshi, K., & Nakano, M. (2009). Learning lexicons spoken utterances based statistical model selection. ProceedingsInterspeech.Tanaka, K. (1999). robust selection system using real-time multi-modal user-agent interactions. Proceedings International Conference Intelligent User Interfaces(IUI).Tanenhaus, M., Spivey-Knowiton, M., Eberhard, K., & Sedivy, J. (1995). Integrationvisual linguistic information spoken language comprehension. Science, 268,16321634.Toutanova, K., Klein, D., Manning, C., & Singer, Y. (2003). Feature-Rich Part-of-Speechtagging cyclic dependency network. Proceedings Human LanguageTechnology Conference North American Chapter Association Computational Linguistics (HLT-NAACL), pp. 252259.Toutanova, K., & Manning, C. D. (2000). Enriching knowledge sources used maximum entropy part-of-speech tagger. Proceedings Joint SIGDAT Conference Empirical Methods Natural Language Processing Large Corpora(EMNLP/VLC), pp. 6370.Traum, D., & Rickel, J. (2002). Embodied agents multiparty dialogue immersivevirtual worlds. Proceedings 1st international joint conference AutonomousAgents Multi-Agent Systems.276fiContext-Based Word Acquisition Situated Dialogue Virtual WorldWang, J. (1995). Integration eye-gaze, voice manual response multimodal userinterfaces. Proceedings IEEE International Conference Systems, ManCybernetics, pp. 39383942.Yu, C., & Ballard, D. (2004). multimodal learning interface grounding spoken languagesensory perceptions. ACM Transactions Applied Perceptions, 1 (1), 5780.Zhai, S., Morimoto, C., & Ihde, S. (1999). Manual gaze input cascaded (MAGIC)pointing. Proceedings Conference Human Factors Computing Systems(CHI), pp. 246253.Zhang, Q., Imamiya, A., Go, K., & Mao, X. (2004). Overriding errors speech gazemultimodal architecture. Proceedings International Conference IntelligentUser Interfaces (IUI).277fiJournal Artificial Intelligence Research 37 (2010) 1-39Submitted 07/09; published 01/10Text Relatedness Based Word ThesaurusGeorge TsatsaronisGBT @ IDI . NTNU .Department Computer Information ScienceNorwegian University Science Technology, NorwayIraklis VarlamisVARLAMIS @ HUA . GRDepartment Informatics TelematicsHarokopio University, GreeceMichalis VazirgiannisMVAZIRG @ AUEB . GRDepartment InformaticsAthens University Economics Business, GreeceAbstractcomputation relatedness two fragments text automated manner requirestaking account wide range factors pertaining meaning two fragments convey,pairwise relations words. Without doubt, measure relatednesstext segments must take account lexical semantic relatedness words.measure captures well aspects text relatedness may help many tasks,text retrieval, classification clustering. paper present new approach measuringsemantic relatedness words based implicit semantic links. approach exploits word thesaurus order devise implicit semantic links words. Basedapproach, introduce Omiotis, new measure semantic relatedness textscapitalizes word-to-word semantic relatedness measure (SR) extends measurerelatedness texts. gradually validate method: first evaluate performancesemantic relatedness measure individual words, covering word-to-word similarity relatedness, synonym identification word analogy; then, proceed evaluatingperformance method measuring text-to-text semantic relatedness two tasks, namelysentence-to-sentence similarity paraphrase recognition. Experimental evaluation showsproposed method outperforms every lexicon-based method semantic relatedness selectedtasks used data sets, competes well corpus-based hybrid approaches.1. IntroductionRelatedness texts perceived several different ways. Primarily, one thinklexical relatedness similarity texts, easily captured vectorial representation texts (van Rijsbergen, 1979) standard similarity measure, like Cosine, Dice (Salton& McGill, 1983), Jaccard (1901). models high impact information retrievalpast decades. Several improvements proposed techniques towards inventing sophisticated weighting schemes text words, like example TF-IDFvariations (Aizawa, 2003). directions explore need capture latent semantic relations dimensions (words) created vector space model, using techniques latentsemantic analysis (Landauer, Foltz, & Laham, 1998). Another aspect text relatedness, probablyequal importance, semantic relatedness two text segments. example, sentences shares company dropped 14 cents, business institutions stock slumped14 cents obvious semantic relatedness, traditional measures text similarity failc2010AI Access Foundation. rights reserved.fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISrecognize. motivation work show measure relatedness texts,takes account lexical semantic relatedness word elements, performsbetter traditional lexical matching models, handle cases like one above.paper propose Omiotis1 , new measure semantic relatedness texts,extends SR, measure semantic relatedness words. word-to-word relatedness measure, turn, based construction semantic links individual words, accordingword thesaurus, case WordNet (Fellbaum, 1998). pair words potentially connected via one semantic paths, one comprising one semanticrelations (edges) connect intermediate thesaurus concepts (nodes). weighting semanticpath consider three key factors: (a) semantic path length, (b) intermediate nodes specificity denoted node depth thesaurus hierarchy, (c) types semantic edgescompose path. triptych allows measure perform well complex linguistic tasks,require simple similarity, SAT Analogy Test2 demonstratedexperiments. best knowledge, Omiotis first measure semantic relatednesstexts considers tandem three factors measuring pairwise word-to-wordsemantic relatedness scores. Omiotis integrates semantic relatedness word level wordsstatistical information text level provide final semantic relatedness score texts.contributions work summarized following: 1) new measure computing semantic relatedness words, namely SR, exploits semantic information thesaurus offer, including semantic relations crossing parts speech (POS), takingaccount relation weights depth thesaurus nodes; 2) new measure computing semantic relatedness texts, namely Omiotis, require use externalcorpora learning methods, supervised unsupervised, 3) thorough experimental evaluationbenchmark data sets measuring performance word-to-word similarity relatednesstasks, well word analogy; addition, experimental evaluation two text related tasks(sentence-to-sentence similarity paraphrase recognition) measuring performancetext-to-text relatedness measure. Additional contributions work are: a) use semantic relations offered WordNet, increases chances finding semantic pathtwo words, b) availability pre-computed semantic relatedness scores every pairWordNet senses, accelerates computation semantic relatedness texts facilitates incorporation semantic relatedness several applications (Tsatsaronis, Varlamis,Nrvag, & Vazirgiannis, 2009; Tsatsaronis & Panagiotopoulou, 2009).rest paper organized follows: Section 2 discusses preliminary concepts regardingword thesauri, semantic network construction, semantic relatedness similarity measures,summarizes related work fields. Section 3 presents key contributions work.Section 4 provides experimental evaluation analysis results. Finally, Section 5presents conclusions next steps work.2. Preliminaries Related Workapproach capitalizes word thesaurus order define measure semantic relatednesswords, expands measure compute text relatedness using semantic1. Omiotis Greek word relatedness similarity.2. http://www.aclweb.org/aclwiki/index.php?title=SAT_Analogy_Questions2fiT EXT R ELATEDNESS BASED W ORD HESAURUSlexical information. order facilitate understanding methodology elaboratepreliminary concepts section present related research approaches.2.1 Word Thesauri use Text ApplicationsWord thesauri, like WordNet (Fellbaum, 1998) Rogets International Thesaurus (Morris & Hirst,1991), constitute knowledge base several text-related research tasks. WordNetused successfully knowledge base construction Generalized Vector Space Models(GVSM) semantic kernels document similarity application text classification,works Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald Weikum (2005), Basili,Cammisa Moschitti (2005), text retrieval, works Voorhees (1993), Stokoe,Oakes Tait (2003), previous work regarding definition new GVSM usesword-to-word semantic relatedness (Tsatsaronis & Panagiotopoulou, 2009). Furthermore, ideausing thesaurus knowledge base text retrieval also proven successful casecross language information retrieval, like example case CLIR system introducedClough Stevenson (2004). Finally, exploitation word thesauri linguistic tasks,Word Sense Disambiguation (WSD) (Ide & Veronis, 1998) yielded interesting results(Mihalcea & Moldovan, 1999; Tsatsaronis, Vazirgiannis, & Androutsopoulos, 2007; Tsatsaronis,Varlamis, & Vazirgiannis, 2008).application text relatedness measure text classification retrieval tasksfirst consider impact lexical ambiguity WSD overall performance tasks.Sanderson (1994, 2008) concludes ambiguity words take many forms, new test collections needed realize true importance resolving ambiguity embedding semanticrelatedness sense disambiguation text retrieval task. analysis Barzilay Elhadad (1997), Barzilay, Elhadad McKeown (2002) impact WSD performancetext summarization tasks addressed considering possible interpretations lexicalchains created text. Similar methodology, tackle word ambiguity taking account every possible type semantic information thesaurus offer, given sensetext word.aforementioned approaches, clear use word thesaurus offer muchpotential design models capture semantic relatedness texts, consequently, may improve performance existing retrieval classification models certaincircumstances discussed respective research works (Mavroeidis et al., 2005; Basiliet al., 2005; Stokoe et al., 2003; Clough & Stevenson, 2004). word thesaurus employeddevelopment Omiotis WordNet. lexical database contains English nouns, verbs, adjectivesadverbs, organized sets synonym senses (synsets). Hereafter, terms senses, synsetsconcepts used interchangeably. Synsets connected various links represent semanticrelations (i.e., hypernymy / hyponymy, meronymy / holonymy, synonymy / antonymy,entailment / causality, troponymy, domain / domain terms, derivationally related forms, coordinateterms, attributes, stem adjectives, etc.). Several relations cross parts speech, like domainterms relation, connects senses pertaining domain (e.g., light, noun meaning electromagnetic radiation producing visual sensation, belongs domain physics).best knowledge, proposed approach first utilizes aforementionedsemantic relations exist WordNet construction semantic relatedness measure.3fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS2.2 Creating Semantic Networks Word ThesauriOmiotis based creation semantic paths words text using thesaurusconcepts relations. Early approaches field, used gloss words respective worddefinitions order build semantic networks text (Veronis & Ide, 1990). idea representing text semantic network initially introduced Quilian (1969). expansionWordNet semantic relations cross parts speech added possibilities semantic network construction text. recent approaches semantic network constructionword thesauri, Mihalcea, Tarau Figa (2004) Navigli (2008), utilize wide rangeWordNet semantic relations instead gloss words. methods outperformed previousmethods used semantic networks words WSD tasks Senseval 2 3 English language (Palmer, Fellbaum, & Cotton, 2001; Snyder & Palmer, 2004). work adoptsemantic network construction method introduced past (Tsatsaronis et al., 2007).method utilizes available semantic relations WordNet. WSD task, respective method outperformed matched previous methods used semantic networkswords WSD tasks Senseval 2 3 English language, largely due richrepresentation semantic networks offered. Section 3.1 introduces semantic relatednessmeasure.2.3 Measures Semantic RelatednessSemantic relatedness words concepts exploited, past, text summarization (Barzilay et al., 2002), text retrieval (Stokoe et al., 2003; Smeaton, Kelledy, & ODonnell,1995; Richardson & Smeaton, 1995) WSD (Patwardhan, Banerjee, & Pedersen, 2003) tasks.Semantic relatedness measures widely classified dictionary-based3 , corpus-based hybrid.Among dictionary-based measures, measure Agirre Rigau (1995) one firstmeasures developed compute semantic relatedness two concepts (i.e., setconcepts). measure based density depth concepts setlength shortest path connects them. However, assume edges pathequally important.measure proposed Leacock, Miller Chodorow (1998) computing semanticsimilarity pair concepts takes account length shortest path connectingthem, measured number nodes participating path, maximum depthtaxonomy. measure two concepts s1 s2 computed follows:Sim(s1 , s2 ) = loglength2D(1)length length shortest path connecting s1 s2 maximum depthtaxonomy used.Regarding hybrid measures, Resniks (1995, 1999) measure pairs concepts basedInformation Content (IC) deepest concept subsume (least common subsumer),considered hybrid measure, since combines hierarchy used thesaurus, statistical information concepts measured large corpora. specifically,3. Also found bibliography knowledge-based, thesaurus-based, lexicon-based.4fiT EXT R ELATEDNESS BASED W ORD HESAURUSsemantic similarity given pair concepts s1 s2 , s0 least commonsubsumer (i.e., least common ancestor), defined following equation:Sim(s1 , s2 ) = IC(s0 )(2)Information Content (IC) concept (i.e., s0 ) defined as:IC(s0 ) = logP (s0 )(3)P (s0 ) probability occurrence concept s0 large corpus.measure proposed Jiang Conrath (1997), also based concept IC. Giventwo concepts s1 s2 , least common subsumer s0 , semantic similarity definedfollows:1Sim(s1 , s2 ) =(4)IC(s1 ) + IC(s2 ) 2 IC(s0 )measure Lin (1998) also based IC. Given, again, s1 , s2 , s0 , before,similarity s1 s2 defined follows:Sim(s1 , s2 ) =2 IC(s0 )IC(s1 ) + IC(s2 )(5)Hirst St-Onge (1998) reexamine idea constructing lexical chains words,based synsets respective semantic edges connect WordNet. initialidea lexical chains first introduced Morris Hirst (1991), defined lexicalcohesion passage, based cohesion lexical chains passages elements,acted indicator continuity passages lexical meaning.encourage reader consult analysis Budanitsky Hirst (2006) detaileddiscussion aforementioned measures, well measures proposed prioraforementioned. measures use noun hierarchy (except measureHirst St-Onge), implementation several measures provided Patwardhan,Banerjee Pedersen (2003) publicly available WordNet::Similarity package also utilizeverb hierarchy. Still, relations cross parts speech considered, wellfactors discussed detail Section 3. contrast, measure defines semantic relatednesstwo concepts, independently Part Speech (POS), utilizing availablesemantic links offered WordNet.recent works interest semantic relatedness, include: measures JarmaszSzpakowicz (2003), use Rogets thesaurus compute semantic similarity, replicatingnumber WordNet-based approaches, LSA-based measure Finkelstein et al. (2002),perform Latent Semantic Analysis (Landauer et al., 1998) capture text relatednessconsidered corpus-based method, measure Patwardhan Pedersen (2006), utilize gloss words found words definitions create WordNet-based context vectors,methods Strube Ponzetto (2006, 2007a), Gabrilovich Markovitch (2007), MilneWitten (2008) use Wikipedia compute semantic relatedness also consideredcorpus-based approaches, method Mihalcea, Corley Strappavara (2006),hybrid method combines knowledge-based corpus-based measures text relatedness.recent hybrid measures semantic similarity are: measure proposed Li et al. (2006),use information WordNet corpus statistics collected Brown Corpus (Kucera,5fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISFrancis, & Caroll, 1967) compute similarity short texts, measure text distance proposed Tsang (2008), uses distributional similarity ontological/knowledgeinformation compute distance text fragments. Distributional similarity also usedsupervised combination WordNet-based approaches (Agirre, Alfonseca, Hall, Kravalova,Pasca, & Soroa, 2009), produce supervised measure semantic relatedness. Li et al. (2006)created new data set experimental evaluation, also use Section 4evaluate Omiotis measure compare approach.following section formally define Omiotis provide details, creationsemantic links computation relatedness words texts. give evidencemeasures complexity justify design choices. Finally, discuss potential applicationsmeasure text related tasks.3. Measuring Word-to-Word Text-to-Text Semantic Relatednesssection presents details Omiotis, measure text semantic relatedness. measurecapitalizes idea semantic relatedness WordNet senses, extends computerelatedness words finally texts. Since definition semantic relatednessranges pairs keyword senses pairs texts, Omiotis defined way capturesrelatedness every granularity. result, applied wide range linguistictext related tasks WSD, word similarity word analogy, text similarity, keywordranking. key points proposed measure are: (a) constructs semantic linksword senses WordNet pre-computes relatedness score every pair WordNetsenses, (b) computes semantic relatedness pair words taking accountrelatedness corresponding WordNet senses, (c) computes semantic relatedness scoretwo given text segments extending word-to-word relatedness. Depending task,computation semantic relatedness modified take account sensesword, words text, apply additional weights dependingword importance sense importance context. allows Omiotis adapted varioustext related tasks, without modifying main process computing relatedness. Section 3.1follows, formally define semantic relatedness measure Section 3.2 providedetailed justification design decisions.3.1 Construct Semantic Links Wordsfirst step measuring semantic relatedness two text fragments, findimplicit semantic links words two fragments. Thus, present definitionsemantic relatedness pair thesaurus concepts, takes account semantic pathconnecting concepts, expands measure relatedness words. order solveproblem constructing semantic paths words, base approach previousmethod construct semantic networks words (Tsatsaronis et al., 2007).3.1.1 EMANTIC N ETWORK C ONSTRUCTION W ORD HESAURIFigure 1 gives example construction semantic network two words ti tj .simplicity reasons, assume construction semantic path senses S.i.2 S.j.1(Initial Phase), though could every possible combination two words6fiT EXT R ELATEDNESS BASED W ORD HESAURUStiS.i.1S.j.1S.i.2S.j.2......S.i.7S.j.5Synonym...HolonymMeronymS.i.2tj...S.j.1HypernymAntonym...HyponymInitial PhaseIndex:= Word NodeNetwork Expansion= Sense Node= Semantic LinkFigure 1: Constructing semantic networks word thesauri.senses. Initially, two sense nodes expanded using semantic links offered WordNet.semantic links senses, found thesaurus, become edges pointed sensesnodes network (Network Expansion). expansion process repeated recursivelyshortest 4 path S.i.2 S.j.1 found. path found S.i.2 S.j.1senses consequently words semantically related.3.1.2 EMANTIC R ELATEDNESSPAIR C ONCEPTSsemantic relatedness pair concepts measured constructed semantic network.considers path length, captured compactness, path depth, captured semanticpath elaboration, defined following. measure WSD based ideacompactness initially proposed Mavroeidis et al. (2005). original measure usednouns hypernym relation, extended current work support WordNetsrelations noun, verb adjective parts speech. define new compactnessmeasure (Definition 1) core Omiotis measure.Definition 1 Given word thesaurus O, weighting scheme edges assigns weightw (0, 1) edge, pair senses = (s1 , s2 ), path P length l connectingQthe twosenses, semantic compactness (SCM (S, O, P )) defined as: SCM (S, O, P ) = li=1 wi ,w1 , w2 , ..., wl paths edges weights. s1 = s2 SCM (S, O, P ) = 1.path s1 s2 SCM (S, O, P ) = 0.Note compactness takes path length account bound [0, 1]. Higher compactnesssenses implies higher semantic relatedness. intuition behind edge types weightingcertain types provide stronger semantic connections others. Considering lexicographers WordNet tend use relation types often others (we assumeused relation types stronger types less used), straightforward solution define edgetypes weights proportion frequency occurrence WordNet 2.0. weights assignedtype using solution shown Table 1 accordance found Songet al. (2004). table shows probability occurrence WordNet 2.0 every possible edgetype thesaurus, descending order probability values. detailed analysis choicesmade Definition 1 definitions follow performed Section 3.2.depth nodes belong path also affects term relatedness. standard meansmeasuring depth word thesaurus hypernym/hyponym hierarchical relation nounadjective POS hypernym/troponym verb POS. adverb POS related stem4. details presented Algorithm 1.7fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISWordNet 2.0 Edge TypeProbability Occurrencehypernym/hyponymnominalizationcategory domainpart meronym/holonymregion domainsimilarusage domainmember meronym/holonymantonymverb groupalso seeattributeentailmentcausesubstance meronym/holonymderivedparticiple0.610.1470.0940.03670.02380.020.0160.0140.01050.010.00910.004140.001950.001580.000890.00033.4E 06Table 1: Probability occurrence every edge type WordNet 2.0.adjective sense used measure depth. path shallow sense nodes generalcompared path deep nodes. parameter semantic relatedness termscaptured measure semantic path elaboration introduced following definition.Definition 2 Given word thesaurus , pair senses = (s1 , s2 ), s1 ,s2s1 6= s2, path P =< p1 , p2 , ..., pl > length l, either s1 = p1 s2 = pls1 = pl s2 = p1 , semantic path elaboration path (SP E(S, O, P )) defined as:Q1di+1SP E(S, O, P ) = li=1 d2di +ddmax, di depth sense pi according O, dmaxi+1maximum depth O. s1 = s2 , d1 = d2 = SP E(S, O, P ) =path s1 s2 SP E(S, O, P ) = 0.dmax .obvious Definition 2 path length l comprises l+1 nodes, thus = l, di+1last node path. Essentially, SPE harmonic mean two depths normalizedmaximum thesaurus depth. harmonic mean preferred average depths, since offers lower upper bound gives realistic estimation paths depth. CompactnessSemantic Path Elaboration measures capture two important parameters measuring semantic relatedness terms (Budanitsky & Hirst, 2006), namely path length senses depthused thesaurus. combine two measures following definition SemanticRelatedness two terms:Definition 3 Given word thesaurus O, pair senses = (s1 , s2 ) semantic relatedness(SR(S, O)) defined maxP {SCM (S, O, P ) SP E(S, O, P )}.8fiT EXT R ELATEDNESS BASED W ORD HESAURUSAlgorithm 1 Maximum-Semantic-Relatedness(G, u, v, w)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:INPUT: directed weighted graph G, two nodes u, v weighting scheme w : E (0..1).OUTPUT: path u v maximum product edges weights.Initialize-Single-Source(G, u)vertices v VGd[v] =[v] = N Uendd[u] = 1Relax(u, v, w)d[v] < d[u] w(u, v)d[v] = d[u] w(u, v)[v] = uendMaximum-Relatedness(G, u, v, w)Initialize-Single-Source(G, u)S=Q = VGv Q= Extract Q vertex maximum=Ssvertices k Adjacency ListRelax(s, k, w)endendreturn path following ancestors v back uGiven word thesaurus, one semantic path connecting two senses.senses compactness take different values different paths. cases, usepath maximizes semantic relatedness. computation introduce Algorithm 1,modification Dijkstras algorithm (Cormen, Leiserson, & Rivest, 1990) finding shortest path two nodes weighted directed graph. algorithm, G representationdirected weighted graph given input (e.g., using adjacency lists), VG setvertices G. Also, two sets used; S, contains verticesmaximum semantic relatedness computed source vertex (i.e., u), Q,contains vertices algorithm computed yet maximum relatedness source vertex. Furthermore, three tables used; d, which, vertex v storesmaximum semantic relatedness found given time algorithm execution sourcevertex, i.e., u d[v]; , vertex v stores predecessor [v]; w, storesedge weights graph (e.g., w[k, m] stores edge weight edge starts kgoes m).9fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISalgorithm comprises three functions: (a) Initialize-Single-Source(G, u), initializestables , every vertex v graph. precisely, sets d[v] = , since semantic relatedness source unknown beginning, algorithm seeksmaximum semantic relatedness initially set minimum value (i.e., ).also sets [v] = N U LL, since beginning algorithm execution awareyet predecessor vertex v following path source vertex u v results maximum semantic relatedness; (b) Relax(u, v, w), given two vertices, u vdirectly connected edge weight w[u, v], updates value d[v], casefollow edge (u, v) results higher semantic relatedness vertex vsource, compared value computed time algorithm execution;(c) Maximum-Relatedness(G, u, v, w), uses aforementioned functions executesDijkstras algorithm. proof algorithms correctness follows next theorem.Theorem 1 Given word thesaurus O, edges weighting function w : E (0, 1),higher value declares stronger edge, pair senses S(ss , sf ) declaring source (ss ) destination (sf ) vertices, SCM (S, O, P ) SP E(S, O, P ) maximized path returned2di dj=wAlgorithm 1, using weighting scheme wijij dmax (di +dj ) , wij new weightedge connecting senses si sj .Proof 1 show vertex sf VG , d[sf ] maximum product edges weightselected path, starting ss , time sf inserted S. on,notation (ss , sf ) represent product. Path p connects vertex S, namely ss ,vertex VG S, namely sf . Consider first vertex sy along p sy VG letsx ys predecessor. Now, path p decomposed ss sx sy sf . claimd[sy ] = (ss , sy ) sf inserted S. Observe sx S. Then, sf chosenfirst vertex d[sf ] 6= (ss , sf ) inserted S, d[sx ] = (ss , sx )sx inserted S.sy occurs sf path ss sf edge weights nonnegative(0, 1) (ss , sy ) (ss , sf ), thus d[sy ] = (ss , sy ) (ss , sf ) d[sf ].sy sf V sf chosen, d[sf ] d[sy ]. Thus, d[sy ] =(ss , sy ) = (ss , sf ) = d[sf ]. Consequently, d[sf ] = (ss , sf ) contradicts choice sf .conclude time vertex sf inserted S, d[sf ] = (ss , sf ).Next, prove returned maximum product SCM (S, O, P ) SP E(S, O, P ), letpath ss sf maximum edge weight product k edges. Then, Algorithm 1Q2dk df2d2 d32ds d2returns maximum ki=1 wi(i+1)= ws2 dmax(ds +d2 ) w23 dmax (d2 +d3 ) ...wkf dmax (dk +df ) =Qk 2di di+1Qk1i=1 di +di+1 dmax = SCM (S, O, P ) SP E(S, O, P ).i=1 wi(i+1)3.1.3 EMANTIC R ELATEDNESSPAIR ERMSBased Definition 3, measures semantic relatedness pair senses S,define semantic relatedness pair terms (t1 , t2 ) follows.Definition 4 Let word thesaurus O, let = (t1 , t2 ) pair terms entriesO, let X1 set senses t1 X2 set senses t2 O. Let S1 , S2 , ..., S|X1 ||X2 |set pairs senses, Sk = (si , sj ), si X1 sj X2 . semantic relatedness(SR(T, S, O)) defined as:10fiT EXT R ELATEDNESS BASED W ORD HESAURUSmaxSk {maxP {SCM (Sk , O, P ) SP E(Sk , O, P )}} = maxSk {SR(Sk , O)}k = 1..|X1 | |X2 |. Semantic relatedness two terms t1 , t2 t1 t2/ defined 1. Semantic relatedness t1 , t2 t1 t2/ O, vice versa,considered 0.remaining paper, SR(T, S, O) pair terms denoted SR(T ),ease readability.3.2 Analysis SR Measuresection present rationale behind Definitions 1, 2, 3, providing theoreticaland/or experimental evidence decisions made design measure. illustrateadvantages disadvantages different alternatives using simple examples arguedecisions. Finally, discuss advantages SR previous measures semanticrelatedness.list decisions made design semantic relatedness measure comprises: a)use senses POS, instead noun senses only, b) use semantic edge types foundWordNet, instead IS-A relation only, c) use edge weights, d) use senses depthscaling factor. important mention measures semantic relatedness differmeasures semantic similarity, traditionally use hierarchical relations ignoretype semantic relations. addition, concepts differentiate semantic distance,sense latter metric.3.2.1 U SE POS NFORMATIONFirstly, shall argue fact use POS designing semantic relatedness measure important, increase coverage measure. rationale supportingdecision fairly simple. Current data sets evaluating semantic relatedness even semantic similarity measures restricted nouns, like example Rubenstein Goodenough 65 wordpairs (1965), Miller Charles 30 word pairs (1991), Word-Similarity-353 collection(Finkelstein et al., 2002). Thus, experimental evaluation data sets cannot pinpointcaveat omitting remaining parts speech. However, text similarity tasks benchmarkdata sets comprise nouns. Throughout following analysis, reader must considerresulting measure semantic relatedness among words destined embeddedtext-to-text semantic relatedness, shown next section.following two sentences paraphrase example taken Microsoft ParaphraseCorpus (Dolan, Quirk, & Brockett, 2004) show importance using POS well,verbs:charges espionage aiding enemy carry death penalty.convicted spying charges could face death penalty.Words appear WordNet written bold stopwords omitted simplicity5 .two sentences many nouns common (charges, death, penalty), also pairswords two sentences contribute evidence two sentences5. stopwords list used available http://www.db-net.aueb.gr/gbt/resources/stopwords.txt11fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISparaphrase. example espionage spying obvious semantic relatedness, wellenemy spying. Also, convicted charges, well convicted penalty. typeevidence would disregarded measure semantic relatedness similarityuses noun POS hierarchy WordNet. Examples measures are: measureSussna (1993), Wu Palmer (1994), Jiang Conrath (1997), Resnik (1995, 1999),WordNet-based component measure proposed Finkelstein et al. (2002). pointview, decision use POS information expands potential matches found measure allows use measure complicated tasks, like paraphrase recognition, textretrieval, text classification.3.2.2 U SE E YPE EMANTIC R ELATIONSdecision use parts speech construction semantic graphs, introduced previous work (Tsatsaronis et al., 2007), imposes involvement semanticrelations instead merely taxonomic (IS-A) ones. Moreover, decision based evidencerelated literature. work Smeaton et al. (1995) provides experimental evidence measuring semantic similarity incorporating non-hierarchical link types (i.e. part meronym/holonym,member meronym/holonym, substance meronym/holonym) improves much performancemeasure. experimental evaluation conducted adopting small variation Resniksmeasure (1995).Hirst St-Onge (1998) reported discovered several limitations missingconnections set WordNet relations construction lexical chains sentencesdetection correction malapropisms. provided following example usingpair words bold report caveat:School administrators say taxpayers expect schools provide child careschool lunches, integrate immigrants community, offer special classes adultstudents,.intrinsic connection nouns child care school, exist WordNet,cannot discovered considering hierarchical edge types. connection depictedFigure 2, shows path WordNet. rich semantic representation able detectconnections address problems aforementioned type.3.2.3 U SE W EIGHTSE DGESwork Resnik (1999) reports simple edge counting, implicitly assumes linkstaxonomy represent uniform distances, problematic best semantic distancemeasure WordNet. similar direction lie findings Sussna (1993), performedthorough experimental evaluation varying edge weights order measure semantic distanceconcepts. Sussnas findings, revealed weights semantic edges non-negligiblefactor application measure WSD, best results reportededge weighting scheme used, instead assigning edge weight.reasons, decided assign weight every edge type, chose simple probabilityoccurrence edge type WordNet, edge weighting scheme (see Table 1).important factor absent several similarity measures proposed past,measures Leacock et al. (1998), Jarmasz Szpakowicz (2003) Banerjee Pedersen(2003), outperformed experimental evaluation measure.12fiT EXT R ELATEDNESS BASED W ORD HESAURUSactivity(Noun)HyponymHypernymeducation(Noun)aid(Noun)Nominalizationeducate(Verb)HyponymHypernymservice(Noun)school(Verb)Nominalizationschool(Noun)Hypernymchild care(Noun)Figure 2: Semantic path child care school following WordNet edges.instrumentality(Noun)conveyance(Noun)HyponymHypernymHypernymimplement(Noun)container(Noun)vehicle(Noun)Hyponympublictransport(Noun)HyponymHypernymbar(Noun)HypernymHyponymwheeledvehicle(Noun)autobus(Noun)Hyponymwheeledvehicle(Noun)Hypernymlever(Noun)self-propelledvehicle(Noun)Category DomainHyponymHypernymcar(Noun)Hypernympedal(Noun)motor vehicle(Noun)passenger(Noun)Part MeronymHyponymHypernymaccelerator(Noun)car(Noun)Category DomainNWPL PathPR PathFigure 3: Product Relatedness (PR) Normalized Weighted Path Length (NWPL) paths pairs:car accelerator (left), car autobus (right).3.2.4 U SE EPTH CALING FACTORdecision incorporate depth scaling factor (SPE Definition 2) edge weightingmechanism inspired thorough experimental evaluation conducted Sussna (1993),13fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISprovided evidence importance edge weighting factor semantic networkbased measures. According experiments Miller Charles data set Spearmancorrelation human judgements much lower (7 percentage points) omitting depthscaling factor adopting SPE factor (see Definition 3).3.2.5 J USTIFICATION SR EFINITIONSAccording Definition 1, semantic compactness pair concepts product depthscaled weights edges connecting two concepts. use product instead sumnormalized sum edges weights explained following.Since might several paths connecting two concepts, Definition 3 clearly selectspath maximizes product semantic compactness (SC) semantic path elaboration (SPE). simplicity, ignore effect depth scalingfactor (SPE Definition 2)Qconsequently, aim find path maximizes li=1 ei , e1 , e2 , ..., el(non depth-scaled) weights edges path connecting two given concepts. Let us nameless elaborate version semantic relatedness measure product relatedness (PR),P R(S, O) = maxP {SCM (S, O, P )}. alternative would beenPto define semantic comlepactness normalized sum weights path, is: i=1. case,lsemantic relatedness would measured path maximizes latter formula, sincenature, semantic relatedness always seeks find path maximizes connectivitytwo concepts. Let us name alternative normalized weighted path length (NWPL).example Figure 3, show PR NWPL compute semantic relatednessterm pair car accelerator (left) car autobus (right). path maximizesrespective formulas PR NWPL using Algorithm 1 edge weights Table 1, illustratedFigure 3 using black white arrows respectively. pair car accelerator sum-basedformula, normalized path length, selects large path example, finalcomputed relatedness 0.61, weight hypernym/hyponym edges. PR findspath maximizing product immediate part meronym relation car accelerator,computed relatedness 0.0367, weight part meronym edges. mainproblem arising NWPL fact cannot distinguish among relatednesspair concepts hypernym/hyponym hierarchy WordNet. example, NWPLcomputes relatedness (0.61) every possible concept pair shown top figure.contrast, PR able distinguish pairs terms relatedness. precisely,behavior PR due fact embeds notion path length, since computedrelatedness decays factor range (0, 1) every hop made following type semanticrelation. Another example, also shows importance considering WordNet relations,one shown right part Figure 3, NWPL PR paths computedterm pair car autobus. Again, NWPL selects large path, inclinehypernym/hyponym tree.Clearly, NWPL would rather traverse huge path hypernym/hyponym edges,following less important edge type, would decrease average path importance.behavior creates serious drawbacks: (a) lack ability distinguish relatedness amongpair concepts hierarchy, (b) large increase actual computational costAlgorithm 1, due fact tend incline hypernym/hyponym hierarchy,even direct semantic edge (other hypernym/hyponym) connecting two concepts,14fiT EXT R ELATEDNESS BASED W ORD HESAURUSlike shown Figure 3. Furthermore, conducting experiments NWPL 30 word pairsMiller Charles, discovered almost 40% cases, NWPL producesvalue semantic relatedness, equal 0.61, unable distinguish creating manyties. Thus, PR better option use measure, semantic compactness factor.Last, least, regarding overall design SR, mention proposed measure solely based use WordNet, contrast measures semantic relatedness uselarge corpora, Wikipedia. Although, measures, like ones proposed GabrilovichMarkovitch (2007), Ponzetto Strube (2007a), provide larger coverage regarding concepts reside WordNet, require processing large corpora (Wikipedia),also changes fast frequently. Experimental evaluation Section 4 showsmeasure competes well aforementioned word-to-word relatedness measuresused data sets. following section, introduce Omiotis, extension SR measuringtext-to-text relatedness.3.3 Omiotisquantify degree two text segments semantically relate other, build uponSR measure, significantly extend order account terms semanticrelatedness also lexical similarity. texts may contain overly-specializedterms (e.g., algorithms name) represented WordNet. Therefore, relying entirelyterm semantics identifying degree texts relate would hamperperformance approach. hand, semantics serve complement relevanceestimations given different text terms might refer (nearly-) identical concepts.quantify lexical similarity two texts, e.g., text B, begin estimation terms importance weights determined standard TF-IDF weightingscheme (Salton, Buckley, & Yu, 1982).Thereafter, estimate lexical relevance, denoted a,b terms b Bbased harmonic mean respective terms TF-IDF values, given by:a,b =2 F IDF (a, A) F IDF (b, B)F IDF (a, A) + F IDF (b, B)(6)Harmonic mean preferred instead average, since provides tight upper bound (Li,2008). decision based fact F IDF (a, A) F IDF (b, B) two differentquantities measuring qualitative strength b respective texts.computed lexical relevance text terms b, estimate semanticrelatedness, i.e. SR(a, b) described previously. Based estimated lexical relevancesemantic relatedness pairs text terms, next step find every word textcorresponding word b text B maximizes product semantic relatedness lexicalsimilarity values given Equation 7.b = arg max(a,b SR(a, b))(7)bBb corresponds term text B, entails maximum lexical similaritysemantic relatedness term text A.6 similar manner, define , corresponds6. function argmax selects case examined ones, maximizes input formula function.15fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISterm text A, entails maximum lexical similarity semantic relatednessterm b text B.= arg max(a,b SR(a, b))(8)aAConsequently, aggregate lexical semantic relevance scores terms text A,reference best match text B denoted shown Equation 9.!X1(A, B) =(9)a,b SR(a, b )|A|aAopposite direction (i.e. words B words A) covercases two texts equal number terms.Finally, derive degree relevance texts B combining valuesestimated terms entail maximum lexical semantic relevance one another,given by:[(A, B) + (B, A)](10)2Algorithm 2 summarizes computation Omiotis. computation entails series steps,complexity discussed Section 3.5.Omiotis(A, B) =3.4 Applications Semantic Relatednesssection describe methodology incorporating semantic relatedness pairswords pairs text segments, several applications.3.4.1 W ORD - -W ORD IMILARITYRubenstein Goodenough (1965) obtained synonymy judgements 51 human subjects 65pairs words, effort investigate relationship similarity context similarity meaning (synonymy). Since then, idea evaluating computational measures semanticrelatedness comparing human judgments given set word pairs, widelyused, even data sets developed. proposed measure semantic relatednesswords (SR), introduced Definition 4, used directly task, orderevaluate basis Omiotis measure, measurement word-to-word semantic relatedness. application straightforward: Let n pairs words used word similarity dataset. Then, semantic relatedness every pair computed, using SR(T, S, O) defined 4.computed values sorted descending order, produced ranking similaritiescompared gold standard ranking humans, using Spearman correlation. scoresused compute Pearsons product moment correlation. Additional measures semanticrelatedness compared examining respective correlation valueshuman judgements.3.4.2 SAT NALOGY ESTSproblem identifying similarities word analogies among pairs words difficult problemstandardized test assessing human ability language understanding,16fiT EXT R ELATEDNESS BASED W ORD HESAURUSAlgorithm 2 Omiotis(A,B, Sem, Lex )1: INPUT: Two texts B, comprising n terms (a b terms Brespectively),semantic relatedness measure Sem : SR(a, b) (0..1),weighting scheme term importance text Lex : F IDF (a, A) (0..1)2: OUTPUT: Find pair terms maximizes product Sem Lex values.3:4:5:6:7:8:9:10:11:12:13:14:15:16:Compute-Zeta(A,B)sum(A) := 0termsb := N UempZeta := 0terms b Ba,b = 2Lex(a,A)Lex(b,B)Lex(a,A)+Lex(b,B)empZeta < a,b Sem(a, b)empZeta = i,j Sem(a, b)b = bendendsum(A) := sum(A) + empZetaendZeta(A, B) := sum(A)/|A|Compute-Omiotis(A,B)17:Omiotis(A, B) :=Zeta(A,B)+Zeta(B,A)2scope well known SAT analogy tests (Scholastic Aptitude Tests). SAT testsused admission tests universities colleges United States. participants aimlocate five given word pairs one presents similar analogy targetpair.Although difficult machines model human cognition word analogy, severalapproaches exist bibliography attempt tackle problem. Previous approacheswidely categorized into: corpus-based, lexicon-based hybrid. examples corpus-basedapproaches Turney (2008b) Bicici Yuret (2006). Examples lexicon-basedapproaches, Veale (2004) application lexicon-based measure HirstSt-Onge (1998) SAT, found work Turney (2006). Hybrid approachesapplied SAT, application measures Resnik (1995) Lin (1998)also found work Turney (2006).order reader understand difficulty answering SAT questions, must pointaverage US college applicant scores 57% (Turney & Littman, 2005), topcorpus-based approach scores 56.1% (Turney, 2006), top lexicon-based scores 42% (Veale,2004) top hybrid scores 33.2% (Resnik, 1995).17fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISAnother way categorizing approaches measure semantic similarity analogy tasksdistinguish among attributional relational similarity measures (Gentner, 1983).7 Representative approaches first category lexicon-based approaches, paradigms relationalsimilarity measures found approaches based Latent Relational Analysis (LRA) (Turney,2006). great interest point LRA-based approaches, like LRME algorithm proposed recently Turney (2008a), superior attributional similarity approaches discoveringword analogies. fact also supported experimental findings Turney (2006). Withoutdoubt, relational similarity approaches may perform better SAT analogy task, still,shown later experiments conducted applications, like paraphrase recognition,lexicon-based measures outperform LRA-based approaches tasks.Semantic relatedness (SR) words, applied Omiotis, exploited solveword analogy task. aim word analogy is, given pair words w1 w2 , identifyseries semantic relations lead w1 w2 (semantic path). SAT test, target pair(w1 ,w2 ) candidate word pairs (w1k ,w2k ), k usually 1 5, processed orderfind pairs analogy. aim locate pair k, exposes maximum similarity w1w2 . straightforward method choose among 5 candidate pairs employ two criteria:first, k analogies analogy target pair compared, candidateshows far similar analogy selected. However, similar analogyobvious, 6 pairs may examined together order slightest differences leadcorrect answer discovered. attempt model human cognition task using SRtwo fold manner: (a) measure SR capture horizontal analogy given pairpossible candidate pairs, (b) measure SR capture vertical analogygiven pair possible candidate pairs. two aspects covered followingEquations 11 13. capture horizontal analogy pair words candidate pair,measure difference SR score two words respectively shown:s1 (w1k , w2k ) = 1 |SR(w1 , w2 ) SR(w1k , w2k )|(11)Essentially, s1 expresses horizontal analogy candidate pair (w1k , w2k ) givenpair (w1 , w2 ). Similarly, capture notion vertical analogy two pairscomputing difference SR scores among two pairs words, follows:s2 (w1k , w2k ) = 1 |SR(w1 , w1k ) SR(w2 , w2k )|(12)Finally, rank candidates depending combined vertical horizontal analogygiven pair, according following equation:s(w1k , w2k ) =s1 (w1k , w2k ) + s2 (w1k , w2k )2(13)Eventually, select candidate pair maximum combined score, taking accountaspects (horizontal vertical) analogy given candidate pairs.intuition behind selection two scores handling SAT test,following. order words pairs (both target candidates) random. Usually,given pair (w1 , w2 ), candidate pairs (w1k , w2k ) test solved one successfully7. Two objects, X Y, attributionally similar attributes X similar attributes Y. Two pairs, A:B C:D,relationally similar relations B similar relations C D.18fiT EXT R ELATEDNESS BASED W ORD HESAURUSStem: wallet : moneyChoices:(a)safe : lock(b)suitcase : clothingS1: 0.2605S2: 6.75E-04(c)camera : filmS1: 0.4795S2: 0.015(d)setting : jewelS1: 0.1805S2: 7.87E-05(e)car : engineS1: 0.3764S2: 8.99E-05Winner based S1 (Horizontal Analogy): bWinner based S2 (Vertical Analogy): bWinner based combined S: bCorrect Answer: bS1: 0.1506S2: 0.0029Figure 4: Example computing Semantic Relatedness measure (SR) given Scholastic Aptitude Test (SAT) question.find analogy: w1k w2k w1 w2 . perspective, s1 s2 try findcandidate pair best aligns target pair. Figure 4 illustrates two types analogies(horizontal vertical) example SAT question.order motivate selection s1 s2 answering SAT questions,discuss detail two quantities pertain concepts strength typerelations pair SAT words. Turney (2006) describes method comparingrelations candidate word pairs stem word pair, utilizes typerelation connecting words pair finally selects pair best matchestype relation connecting words stem pair. Though explicitly examinelabel edges connecting words pair, implicitly computing SRthem. Since weighting WordNet edges fine grained, distinguishes everytype semantic relation WordNet, instead labels, using edge weights. SR definitionprovide fine grained distinguishment two pairs words, depending typesedges connecting words respectively, expressed weights, also takingaccount factors, like depth nodes comprising connecting path insidethesaurus. Besides s1 , attempts capture aforementioned properties word pairs,s2 attempts words order among two word pairs (i.e., first wordfirst pair, second word second pair). forms attempt capturealigned two word pairs, according SR values words.3.4.3 PARAPHRASE R ECOGNITIONENTENCE - -S ENTENCE IMILARITYPerformance applications relying natural language processing may suffer factprocessed documents might contain lexically different, yet semantically related, text segments.task recognizing synonym text segments, better known paraphrase recognition,detection, challenging difficult solve, shown work Pasca (2005). taskimportant many text related applications, like summarization (Hirao, Fukusima, Oku19fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISmura, Nobata, & Nanba, 2005), information extraction (Shinyama & Sekine, 2003) questionanswering (Pasca, 2003). experimentally evaluate application Omiotis paraphrasingdetection task (Section 4.2), using Microsoft Research Paraphrase Corpus (Dolan et al., 2004).application Omiotis paraphrase detection straightforward: given pair text segments,compute Omiotis score them, using Equation 10 Algorithm 2. Higher valuesOmiotis given pair denote stronger semantic relation examined text segments.task reduced define threshold, Omiotis value considereddetermining sign paraphrasing pair. experimental evaluation Omiotis, explaindetail selected threshold paraphrase recognition task.similar manner, using Equation 10 Algorithm 2, semantic relatedness scorespairs sentences computed. task, using data set Li et al. (2006)evaluate Omiotis, comprising 30 sentence pairs, human scores provided. Section 4describe detail experimental set up.3.5 Complexity Implementation Issuescomputation Omiotis entails series steps, complexity strongly relatedbase measure Semantic Relatedness (SR). Primarily, given two words, w1 w2 constructiontime semantic network used compute SR according Algorithm 1, provenO(2 k l+1 ) (Tsatsaronis et al., 2007), k maximum branching factor usedthesaurus nodes l maximum semantic path length thesaurus. semanticnetwork constructed, complexity Algorithm 1 reduced standard time complexitycost Dijkstras algorithm. Using Fibonacci heaps, possible alleviate computationalburden Dijkstra improve time complexity. semantic network, Dijkstra takesO(nL + mD + nE), n number nodes network, number edges, Ltime insert, time decrease-key E time extract-min. Fibonacci heapsused L = = O(1) cost extract-min O(logn), thus significantly reducingcost execution. whole procedure repeated 2 n1 n2 times computationOmiotis two documents d1 d2 total n1 n2 distinct words respectively.aforementioned, obvious computation Omiotis cheap general.purpose, order improve systems scalability, pre-computed storedSR values every possible pair synsets RDBMS. one-time computationcost, dramatically decreases computational complexity Omiotis. database schemathree entities, namely Node, Edge Paths. Node contains WordNet synsets. Edge indexesedges WordNet graph adding weight information edge computed using SRmeasure. Finally, Paths contains pairs WordNet synsets directly indirectly connected WordNet graph computed relatedness. pairs found runningBreadth First Search (BFS) starting WordNet roots POS. Table 2 provides statisticalinformation RDBMS exceeds 220 Gbytes size. Column 1 indicates numberdistinct synsets examined, column 2 shows total number edges, column 3 depictsnumber connected synsets (by least one path following offered WordNet edges).current implementation takes advantage database structures (indices, stored procedures etc)order decrease computational complexity Omiotis. following example indicativecomplexity SR computation. average number senses per term 5 72(depending POS). pair terms known POS, perform n2 (n 6) combinations20fiT EXT R ELATEDNESS BASED W ORD HESAURUSDistinct Synsets115,424Total Edges324,268Connected Synset Pairs5,591,162,361Table 2: Statistics WordNet graph implemented database.pair synsets compute similarity presented Definition 3.similarities pre-computed, time required processing 100 pairs terms 1 sec,makes computation Omiotis feasible scalable. proof concept, developedon-line version SR Omiotis measures8 , user test term-to-termsentence-to-sentence semantic relatedness measures (Tsatsaronis et al., 2009).4. Experimental Evaluationexperimental evaluation Omiotis two-fold. First, test performance wordto-word semantic relatedness measure (SR), Omiotis based, three types tasks: (a)word-to-word similarity relatedness, (b) synonym identification, (c) Scholastic AptitudeTest (SAT). Second, evaluate performance Omiotis two tasks: (a) sentence-to-sentencesimilarity, (b) paraphrase recognition task.4.1 Evaluation Semantic Relatedness (SR) Measureevaluation proposed semantic relatedness measure two terms experimented three different categories tests. first category comprises data sets containword pairs, human subjects provided similarity scores relatedness scores.provided scores create ranking word pairs, similar irrelevant.evaluate performance measures, computing correlation list humanrankings list produced measures. task, evaluate performance SRthree benchmark data sets, namely Rubenstein Goodenough 65 word pairs (1965) (R&G),Miller Charles 30 word pairs (1991) (M&C), humans provided similarity scores, and, also, Word-Similarity-353 collection (Finkelstein et al., 2002) (353-C),comprises 353 word pairs, humans provided relatedness scores.second category experiments comprises synonym identification tests. tests, giveninitial word, appropriate synonym word must identified among given options.task evaluate performance SR TOEFL data set, comprising 80 multiplechoice synonym questions, ESL data set, comprising 50 multiple choice synonym questionsquestions.9third category experiments based Scholastic Aptitude Test (SAT) questions.SAT, given pair words, relevant pair among five given pairs must selected.task based word analogy identification. evaluation data set comprises 374 test questions.8. Publicly available http://omiotis.hua.gr9. http://www.aclweb.org/aclwiki/index.php?title=TOEFL_Synonym_Questionshttp://www.aclweb.org/aclwiki/index.php?title=ESL_Synonym_Questions_(State_of_the_art)21fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISCategoryLexicon-basedCorpus-basedHybridMethodHSLCJSGMWLMSPIS-A SPJCLRHRSRR&GSpearmans Pearsons r0.7450.7860.7850.838N/A0.8180.816N/A0.64N/AN/A0.52N/A0.700.7090.7810.770.8180.74850.7780.817N/A0.86140.876M&CSpearmans Pearsons r0.6530.7440.7480.816N/A0.8780.723N/A0.70N/AN/A0.47N/A0.690.8050.850.7670.8290.7370.7740.904N/A0.8560.864Table 3: Spearmans Pearsons correlations Rubenstein Goodenough (R&G)Miller Charles (M&C) data sets. Confidence levels: =0.90, =0.95, =0.994.1.1 E VALUATIONEMANTIC IMILARITYR ELATEDNESSfirst category experiments, compared measure ten known measuressemantic relatedness: Hirst St-Onge (1998) (HS), Jiang Conrath (1997) (JC), Leacocket al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz Szpakowicz (2003) (JS),Gabrilovich Markovitch (2007, 2009) (GM), Milne Witten (2008) (WLM), Finkelstein et al.(2002) (LSA), Hughes Ramage (2007) (HR), Strube Ponzetto (2006, 2007a) (SP).measure Strube Ponzetto also included results version measurebased IS-A relations (Ponzetto & Strube, 2007b) (IS-A SP). measure, includingmeasure (SR), computed Spearman rank order correlation coefficient() Pearson product-moment correlation coefficient (r), derived r, sincecomputation relatedness scores transformed rankings. correlationcoefficients computed based relatedness scores rankings provided humansthree data sets (the relatedness scores create ranking pairs words, basedsimilarity). measures HS, JC, LC, L R, rankings relatedness scoresword pairs R&G M&C data sets, given work Budanitsky Hirst(2006). JS measure, r value given work Jarmasz Szpakowicz (2003)R&G M&C data sets, value given work GabrilovichMarkovitch (2007). GM measure values given work GabrilovichMarkovitch (2007). WLM measure values given work Milne Witten(2008). LSA method value given work Gabrilovich Markovitch (2007),353-C data set. HR measure values given work HughesRamage (2007). Finally, SP measure r values given work PonzettoStrube (2007a), IS-A SP given work Ponzetto Strube (2007b).22fiT EXT R ELATEDNESS BASED W ORD HESAURUSTable 3 show values r R&G M&C data sets SRcompared measures. human scores pairs words two data setsfound analysis Budanitsky Hirst (2006). Note M&C data set subsetR&G data set. cases, computation r feasible, due missinginformation regarding detailed rankings relatedness scores respective measures.cases table entry N/A. Also LSA measure omitted tabler reported literature two data sets. also conducted statisticalsignificance test difference SR correlations respective correlationscompared measures, using Fishers z-transformation (Fisher, 1915). reported number,symbol indicates difference correlation produced SR respectivemeasure statistically significant 0.99 confidence level (p < 0.01). symbol indicates0.95 confidence level (p < 0.05) and, finally, symbol indicates statisticalsignificance correlations difference 0.90 confidence level (p < 0.10). casesdifference statistically significant confidence levels, indicatingsymbol.Table 4 show values r 353-C data set. reason present resultsexperiments 353-C data set another table respective results R&BM&C data sets collection focuses concept semantic relatedness, ratherconcept semantic similarity (Gabrilovich & Markovitch, 2007). Relatedness generalconcept similarity, argued analysis Budanitsky Hirst (2006). Thus,argued humans 353-C thought differently scoring, compared caseR&B M&C data sets. detailed human scores 353-C data set made availablecollection10 . measures L, JC HS omitted, information availablecomputing r values. remark regarding 353-C collection, need add factcases inter-judge correlations may fall 65%, R&B M&Cdata sets inter-judge correlations 0.88 0.95. Again, statistical significance testsconducted using Fishers z-transformation, regarding difference SR correlationscorrelations compared measures. used symbols indicate levelstatistical significance previously. regards reported correlationsR&G M&C data sets, shown SR performs well, since majority casesSR higher correlation compared measures semantic relatedness similaritycategory (knowledge-based, corpus-based hybrid). R&G data set SR reportshighest r correlations. M&C data set SR second highest correlation.HR measure highest correlation, R&G 353-C SR outperforms HR.differences SR HR statistically significant two examined data sets.Also, M&C data set SR second r correlation JS reporting highest,JS outperformed SR R&G 353-C data sets. case M&C data set,difference SR JS statistically significant, SR outperforms JS R&G353-C data sets, statistically significant difference reported correlations. Anotherimportant conclusion results, fact IS-A SP measure performs betterSP measure. mainly due fact computation similarity valuesdata sets, inclusion IS-A relations much reasonable (Ponzetto & Strube, 2007b).differences results (SP IS-A SP) motivate even SR measure, since10. http://www.cs.technion.ac.il/gabr/resources/data/wordsim353/23fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISCategoryLexicon-basedCorpus-basedHybridMethodLCJSGMWLMLSASPRHRSR353-CSpearmans Pearsons rN/A0.340.55N/A0.75N/A0.69N/A0.56N/AN/A0.49N/A0.340.552N/A0.610.628Table 4: Spearmans Pearsons correlations 353 word pairs (353-C) data set. Confidencelevels: =0.90, =0.95, =0.99take best worlds, i.e., weigh IS-A relations high, fall back relationsnecessary.Regarding 353-C data set, results Table 4 show SR performs well,top performers Wikipedia-based approaches (Gabrilovich & Markovitch, 2009; Milne& Witten, 2008). difference statistically significant, noteSR outperforms GM WLM R&G M&C data sets, statistically significantdifference well. Partly, difference performance SR compared GM WLMexplained follows: GM measure considers words context (Gabrilovich & Markovitch,2009), thus inherently performs word sense disambiguation; contrast, SR takes input pairwords, lacks context, based information existing WordNet, which, especiallyseveral cases 353-C data set, creates disadvantage (e.g., word pair ArafatJackson, 11 different entries second word WordNet). holdsWLM measure. Another reason difference performance coverage WordNet.several cases, one two words 353-C data set comprising pair, existWordNet (e.g., football player Maradona). However, expected, also shownexperimental analysis Omiotis follows, context considered, proposed semanticrelatedness measure performs better (the reader may wish consult Table 9, subsetR&G data set contains full definitions words, correlations Omiotishuman judgements top found among compared approaches).visualize performance measure comprehensible manner, also presentFigure 5 relatedness values given humans pairs R&G M&C data sets,increasing order value (left side) respective values pairs produced using SR(right side). Note x-axis charts begins least related pair terms, accordinghumans, continues related pair terms. y-axis left chartrespective humans rating pair terms. right figure shows SR pair. closerlook Figure 5 reveals values produced SR (right figure) follow pattern similarhuman ratings (left figure).24fiT EXT R ELATEDNESS BASED W ORD HESAURUSHUMAN RATINGS HUMAN RANKINGS - R&G Data SetSEMANTIC RELATEDNESS HUMAN RANKINGS - R&G Data Set40.9Semantic RelatednessHuman Rating3.532.521.510.5203040500.70.60.50.40.30.2correlation human pairs ranking human ratingscorrelation human pairs ranking semantic relatedness0.10100.86065102030405060Pair NumberPair NumberHUMAN RATINGS HUMAN RANKINGS - M&C Data SetSEMANTIC RELATEDNESS HUMAN RANKINGS - M&C Data Set654Semantic RelatednessHuman Rating3.532.521.510.5correlation human pairs ranking human ratings0.90.80.70.60.50.40.30.20.1correlation human pairs ranking semantic relatedness0510152025305Pair Number1015202530Pair NumberFigure 5: Correlation human ratings Semantic Relatedness measure (SR) Rubenstein Goodenough (R&G) Miller Charles (M&C) data sets.4.1.2 E VALUATIONYNONYM DENTIFICATIONsynonym identification task using TOEFL 80 questions data set ESL 50questions data set. TOEFL data set compare several methods. specifically, examine: lexicon-based measures Leacock et al. (1998) (LC), Hirst St-Onge(1998) (HS), Jarmasz Szpakowicz (2003) (JS); corpus-based measures LandauerDumais (1997) (LD), Pado Lapata (2007) (PL), Turney (2008b) (T), Terra Clarke (2003)(TC), Matveeva et al. (2005) (M); hybrid measures Resnik (1995) (R), Lin (1998) (L),Jiang Conrath (1997) (JC), Turney et al. (2003) (PR); Web-based method RuizCasado et al. (2005) (RC). also report results random guessing (RG) performanceaverage college applicant (H). Table 5 shows results 80 TOEFL questions.table reports number correct respective percentage given measures. ordertest statistical significance differences measures performance, conductedFishers Exact Test (Agresti, 1990). previous tables, symbol indicates statisticallysignificant difference 0.99 confidence level, 0.95 confidence level, 0.90confidence level. results Table 5 show SR ranks second among reported methods,best method hybrid PR (Turney et al., 2003). regards comparisonlexicon-based methods, SR reports better results, statistically significant confidence levelsindicated.similar manner, conducted experiments ESL 50 questions data set,compare results with: lexicon-based measures Leacock et al. (1998) (LC), Hirst StOnge (1998) (HS), Jarmasz Szpakowicz (2003) (JS); corpus-based measures Turney(2001) (PMI-IR), Terra Clarke (2003) (TC); hybrid measures Resnik (1995) (R),25fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISCategoryLexicon-BasedCorpus-BasedHybridWeb-BasedMethodLCHSJSLDPLTCRLJCPRRCRGHSR#Correct Answers17626352586165691619207866205270Percentage Correct Answers0.2120.7750.7870.650.7250.7620.8120.8620.20.2370.250.9750.8250.250.650.875Table 5: Number percentage correct answers TOEFL 80 questions test. Confidencelevels: =0.90, =0.95, =0.99CategoryLexicon-BasedCorpus-BasedHybridMethodLCHSJSPMI-IRTCRLJCRGSR#Correct Answers18314137401618182041Percentage Correct Answers0.360.620.820.740.80.320.360.360.250.82Table 6: Number percentage correct answers ESL 50 questions test. Confidence levels:=0.95, =0.99Lin (1998) (L), Jiang Conrath (1997) (JC). report results, together randomguessing, Table 6. results Table 6 show SR ranks first, performanceJS data set, outperforming compared corpus-based methods.26fiT EXT R ELATEDNESS BASED W ORD HESAURUSCategoryLexicon-BasedCorpus-BasedHybridWeb-BasedMethodLCHSVLRAPMI-IRRLJCBRGS1S2NBUB#Correct Answers11712016121013112410210215075106114128142196Percentage Correct Answers0.3130.3210.430.5610.350.3320.2730.2730.40.20.2830.3040.3420.3810.524Table 7: Number percentage correct answers 374 Scholastic Aptitude Test (SAT)questions. Confidence levels: =0.90, =0.95, =0.99results interesting, since indicate lexicon-based methods promisingsynonym identification tasks.4.1.3 E VALUATIONSAT NALOGY Q UESTIONSapproach choose evaluate SR analogy task use typical benchmark testset employed related bibliography, namely Scholastic Aptitude Test (SAT).11 comprises374 words pairs target pair 5 supplementary pairs words. average US collegeapplicant answered correctly 57 percent questions, machine-based approachyet surpassed performance average college applicant.Table 7, present number correct answers respective percentage (recall)374 SAT questions, following methods: random guessing (RG), Jiang Conrath (1997)(JC), Lin (1998) (L), Leacock et al. (1998) (LC), Hirst St-Onge (1998) (HS), Resnik (1995)(R), Bollegala et al. (2008) (B), Veale (2004) (V), PMI-IR (Turney, 2001) LRA (Turney, 2006).Furthermore, present results s1 (Equation 11), s2 (Equation 12) (Equation 13).also present, before, statistical significance differences performance, conductingFishers exact test.Towards direction combining answers s1 s2 different mannernaive average, also report upper bound performance attempt. computedsimply finding union correct answers s1 s2 may provide. reportedtable (UB). effort design learning mechanism would learn select11. Many thanks Peter Turney, providing us standard set experimentation, comprising 374 SAT questions.27fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISs1 s2 answers SAT question, goal reach upper-bound, designedimplemented simple representation SAT questions training instances.SAT question, created training instance 6 features: minimum s1 value foundquestion (among five computed values possible pairs), maximum s1 value,difference. also added features regarding s2 . trained testedNaive Bayes classifier using ten-fold cross validation 374 SAT questions. resultsexperiment shown table NB (Naive Bayes). Finally, also present top results everreported literature specific data set, LRA method Turney (2006).reported table (LRA).results presented Table 7 show ranks second among compared lexicon-basedmeasures first measure Veale (2004) (V). method Bollegala et al. (2008)(B) achieves higher score SR, needs training SAT questions. pointnote LRA method needs almost 8 days process 374 SAT questions (Turney, 2006),(B) needs around 6 hours (Bollegala et al., 2008), needs less 3 minutes.Furthermore, fact combining s1 s2 reach 52.4% shows producepromising results, classifier learns successfully combine them. N B results,simple attempt construct learner features, shows important boostperformance 4.1%. proper feature engineering task, training SAT questionspotentially yield promising results, gap 38.1% upper bound52.4% still large. all, results prove lexicon-based relatedness measurecomparable performance state art measures SAT task, smallerexecution time majority methods outperform recall.4.2 Evaluation Omiotis Measureorder evaluate performance Omiotis measure, performed two experimentstest ability measure capture similarity sentences. first experimentbased data set produced Li et al. (2006). second experiment based paraphraserecognition task, using Microsoft Research Paraphrase Corpus (Dolan et al., 2004).4.2.1 E VALUATIONENTENCE IMILARITYdata set produced Li et al. (2006) comprises 65 sentence pairs (each pair consists twosentences respective dictionary word definitions R&G 65 word pairs data set).used dictionary Collins Cobuild dictionary (Sinclair, 2001). sentence pair,similarity scores provided 32 human participants, ranging 0.0 (the sentencesunrelated meaning), 4.0 (the sentences identical meaning).12 .65 sentence pairs, Li et al. (2006) decided keep subset 30 sentence pairs,similarly process applied Miller Charles (1991), order retain sentences whosehuman ratings create even distribution across similarity range. Thus, apply Omiotissubset 65 sentence pairs, described Li et al. (2006). data set,compare Omiotis STASIS measure semantic similarity, proposed Li et al. (2006),LSA-based approach described OShea et al. (2008), STS measure proposed IslamInkpen (2008). best knowledge, data set used three12. data set publicly available http://www.docm.mmu.ac.uk/STAFF/J.Oshea/28fiT EXT R ELATEDNESS BASED W ORD HESAURUSprevious works. Table 8 present sentence pairs used, respective scores humans,STASIS, LSA, STS, Omiotis.Sentence Pair1.cord:smile5.autograph:shore9.asylum:fruit13.boy:rooster17.coast:forest21.boy:sage25.forest:graveyard29.bird:woodland33.hill:woodland37.magician:oracle41.oracle:sage47.furnace:stove48.magician:wizard49.hill:mound50.cord:string51.glass:tumbler52.grin:smile53.serf:slave54.journey:voyage55.autograph:signature56.coast:shore57.forest:woodland58.implement:tool59.cock:rooster60.boy:lad61.cushion:pillow62.cemetery:graveyard63.automobile:car64.midday:noon65.gem: jewelHuman0.010.0050.0050.1080.0630.0430.0650.0130.1450.130.2830.3480.3550.2930.470.1380.4850.4830.360.4050.5880.6280.590.8630.580.5230.7730.5580.9550.653STASIS0.3290.2870.2090.530.3560.5120.5460.3350.590.4380.4280.7210.6410.7390.6850.6490.4930.3940.5170.550.7590.70.75310.6630.6620.7290.6390.9980.831LSA0.510.530.5050.5350.5750.530.5950.5050.810.580.5750.7150.6150.540.6750.7250.6950.830.610.70.780.750.830.9850.830.630.740.8710.86STS0.060.110.070.160.260.160.330.120.290.200.090.300.340.150.490.280.320.440.410.190.470.260.510.940.600.290.510.520.930.65Omiotis0.10620.10480.10460.30280.29880.2430.29950.10740.49460.10850.10820.21640.52950.57010.55020.52060.59870.49650.42550.42870.93080.6120.73920.99820.93090.34660.73430.78890.92910.8194Table 8: Human, STASIS, LSA, STS Omiotis scores 30 sentence pairs.Table 9 present results comparison, comprising reporting Spearmansrank order correlation coefficient Pearsons product moment correlation coefficient rSTASIS, LSA, STS, Omiotis. also included results, version Omiotistake account inter-POS relations (i.e., relations cross parts speech).version Omiotis indicated table SimpleOmiotis. objective experimentmeasure contribution relations cross parts speech computation text-to29fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISSTASISLSASTSSimple OmiotisOmiotisAverage ParticipantWorst ParticipantBest ParticipantSpearmans0.81260.87140.8380.68890.8905N/AN/AN/APearsons r0.81620.83840.8530.72770.8560.8250.5940.921Table 9: Spearmans Pearsons correlations human similarity ratings. Confidence levels:=0.95, =0.99text semantic relatedness values, though types relations reported previousbibliography advantageous (Jarmasz, 2003; Jarmasz & Szpakowicz, 2003), individualcontribution never measured.also show r correlation average participant (mean individuals group;n = 32, leave-one-out resampling standard deviation 0.072), worst participant (worst participant group; n = 32, leave-one-out resampling) best participant (best participantgroup; n = 32, leave-one-out resampling), taken work OShea et al. (2008).addition, also conducted z-test regarding difference Omiotis correlationscompared measures correlations. symbols used previous tables indicate confidence level statistical significance. Note, also, reported correlations (STASIS, LSA,STS, Omiotis) individually constitute statistically significant positive correlations human scores (r) rankings (). results indicate, Omiotis best correlation, accordingr values, compared STASIS, LSA, STS. Furthermore, contribution semantic relations cross parts speech obvious, since difference simple versionOmiotis omits defined Omiotis measure large statistically significant0.99 confidence level. Overall, results indicate Omiotis applied successfullycomputation similarities small text segments, like sentences.4.2.2 E VALUATIONPARAPHRASE R ECOGNITIONorder evaluate performance Omiotis measuring semantic relatedness small text segments, conducted additional experiments paraphrase recognition taskusing test pairs Microsoft Research Paraphrase Corpus (Dolan et al., 2004).original data set, containing training test pairs, run experiments 1725 testpairs text segments, collected news sources Web period18 months. pair, human subjects determined whether two texts pairconsists paraphrase (direction issue). reported inter-judge agreementannotators 83%. paraphrase recognition task widely studied past,since important many natural language applications, like question answering (Harabagiu30fiT EXT R ELATEDNESS BASED W ORD HESAURUSCategoryBaselinesCorpus-basedLexicon-basedMachine learning-basedMethodRandomVSM CosinePMI-IRLSASTSJCLCLeskLWPRComb.Wan et al.Zhang PatrickQiu et al.Finch et al.OmiotisAccuracy51.365.469.968.472.669.369.569.369.3696970.37571.97274.9669.97Precision68.371.670.269.774.772.272.472.471.670.26969.67774.372.576.5870.78Recall5079.595.295.289.187.18786.688.792.196.497.79088.293.489.893.4F-Measure57.875.38180.581.3797978.979.28080.481.38380.781.682.6680.52Table 10: Omiotis competitive methods performance Microsoft Research ParaphraseCorpus (MSR).& Hickl, 2006), text summarization (Madnani, Zajic, Dorr, Fazil Ayan, & Lin, 2007).task computed Omiotis sentences every pair marked paraphrasespairs Omiotis value greater threshold. threshold set 0.2, tuningtraining set. used simple approach tuning, namely forward hill-climbing beamsearch (Guyon, Gunn, Nikravesh, & Zadeh, 2006).compare performance Omiotis several methods various categories;precisely, against: (a) two baseline methods, random selection method marks randomlypair paraphrase (Random), vector-based similarity measure, usingcosine similarity measure TF-IDF weighting features (VSM Cosine) 13 , (b) corpusbased methods; PMI-IR proposed Turney (2001), LSA-based approach introducedMihalcea et al. (2006), STS measure proposed Islam Inkpen (2008), (c) lexiconbased methods; Jiang Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik(1995, 1999) (R), Lesk (1986) (Lesk), Wu Palmer (1994) (WP), metric combinesmeasures category, proposed Mihalcea et al. (2006) (Comb.), (d) machine-learningbased techniques, also constitute state art paraphrase recognition, like methodWan et al. (2006), trains classifier lexical dependency similarity measures,method Zhang Patrick (2005), also build feature vector lexical similaritiessentence pairs (e.g., edit distance, number common words), method Qiu et al.13. features words used data set.31fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS(2006), use SVM classifier (Vapnik, 1995) decide whether set featuressentence created parsing semantic role labelling matches respectiveset second sentence pair, importance, and, finally, method Finchet al. (2005), also train SVM classifier based machine translation evaluation metrics.results evaluation shown Table 10. results indicate Omiotis surpasseslexicon-based methods, matches combined method Mihalcea et al. (2006).point must mention also tuned Omiotis goal maximize F-Measuretest set, cost dropping precision favor recall. type tuning reported FMeasure 81.7, larger F-Measures lexicon-based, corpus-basedtwo machine learning-based approaches. Even though reported results used differentsimpler tuning explained previously, still results indicate Omiotis manages wellparaphrase recognition task produces comparable results state art.believe used part machine learning-based method, since one bestchoices lexicon-based methods paraphrase recognition, also constitutes partplan future work application.5. Conclusions Future Workpaper presented new measure text semantic relatedness. major strengthmeasure lies formulation semantic relatedness words. Experimental evaluation, proved measure approximates human understanding semantic relatednesswords better previous related measures. combination path length, nodes depthedges type single formula allowed us apply semantic relatedness measure differenttext-based tasks good performance. specifically, SR measure outperformed overall used data sets state art measures word-to-word tasks Omiotis measureperformed well sentence similarity paraphrase recognition tasks. Although,results word analogy task satisfactory, since special tuning performed,confident still place improvement. extensive evaluation SR Omiotisseveral applications shows capabilities measures proves appliedseveral text related tasks. next plans apply relatedness measures applications, text classification clustering, keyword sentence extraction, queryexpansion, compare state art techniques field. Finally, improvingsupporting infrastructure order facilitate large scale tasks document clustering textretrieval.AcknowledgmentsPart work done George Tsatsaronis Department Informatics AthensUniversity Economics Business. would like thank Kjetil Nrvag constructivecomments, Ion Androutsopoulos feedback early stage work. wouldalso like thank anonymous reviewers detailed feedback.32fiT EXT R ELATEDNESS BASED W ORD HESAURUSReferencesAgirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasca, M., & Soroa, A. (2009). studysimilarity relatedness using distributional wordnet-based approaches.. Proceedings Human Language Technologies: 2009 Annual Conference North AmericanChapter Association Computational Linguistics (NAACL), pp. 1927.Agirre, E., & Rigau, G. (1995). proposal word sense disambiguation using conceptual distance. Proceedings International Conference Recent Advances Natural Language Processing (RANLP).Agresti, A. (1990). Categorical Data Analysis. Wiley, Hoboken, NJ.Aizawa, A. (2003). information-theoretic perspective TF-IDF measures. Information Processing Management, 39(1), 4565.Banerjee, S., & Pedersen, T. (2003). Extended gloss overlaps measure semantic relatedness.Proceedings Eighteenth International Joint Conference Artificial Intelligence(IJCAI), pp. 805810.Barzilay, R., & Elhadad, M. (1997). Using lexical chains text summarization. ProceedingsACL 97/EACL 97 Workshop Intelligent Scalable Text Summarization, pp. 1017.Barzilay, R., Elhadad, M., & McKeown, K. (2002). Inferring strategies sentence orderingmultidocument news summarization. Journal Artificial Intelligence Research, 17, 3555.Basili, R., Cammisa, M., & Moschitti, A. (2005). semantic kernel exploit linguistic knowledge. Proceedings Advances Artificial Intelligence, Ninth Congress ItalianAssociation Artificial Intelligence (AI*IA), pp. 290302.Bicici, E., & Yuret, D. (2006). Clustering word pairs answer analogy questions. ProceedingsFifteenth Turkish Symposium Artificial Intelligence Neural Networks.Bollegala, D., Matsuo, Y., & Ishizuka, M. (2008). WWW sits sat: Measuring relational similarity web. Proceedings Eighteenth European Conference ArtificialIntelligence (ECAI), pp. 333337.Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures lexical semantic relatedness. Computational Linguistics, 32(1), 1347.Clough, P., & Stevenson, M. (2004). Cross-language information retrieval using EuroWordNetword sense disambiguation. Proceedings Twenty Sixth European ConferenceInformation Retrieval (ECIR), pp. 327337.Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction Algorithms. MIT Press.Dolan, W., Quirk, C., & Brockett, C. (2004). Unsupervised construction large paraphrase corpora:Exploiting massively parallel news sources. Proceedings Twentieth InternationalConference Computational Linguistics (COLING).Fellbaum, C. (1998). WordNet electronic lexical database. MIT Press.33fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISFinch, A., Hwang, Y., & Sumita, E. (2005). Using machine translation evaluation techniques determine sentence-level semantic equivalence. Proceedings 3rd International Workshop Paraphrasing,.Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin, E. (2002).Placing search context: concept revisited. ACM Transactions Information Systems,20(1), 116131.Fisher, R. (1915). Frequency distribution values correlation coefficient samplesindefinitely large population. Biometrika, 10, 507521.Gabrilovich, E., & Markovitch, R. (2007). Computing semantic relatedness using Wikipedia-basedexplicit semantic analysis. Proceedings Twentieth International Joint ConferenceArtificial Intelligence (IJCAI), pp. 16061611.Gabrilovich, E., & Markovitch, R. (2009). Wikipedia-based semantic interpretation naturallanguage processing. Journal Artificial Intelligence Research, 34, 443498.Gentner, D. (1983). Structure-mapping: theoretical framework analogy. Cognitive Science,7(2), 155170.Guyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. (2006). Feature Extraction, FoundationsApplications. Springer.Harabagiu, S., & Hickl, A. (2006). Methods using textual entailment open-domain questionanswering.. Proceedings Joint Conference International Committee Computational Linguistics Association Computational Linguistics (COLING-ACL),pp. 905912.Hirao, T., Fukusima, T., Okumura, M., Nobata, C., & Nanba, H. (2005). Corpus evaluationmeasures multiple document summarization multiple sources. ProceedingsTwentieth International Conference Computational Linguistics (COLING), pp. 535541.Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detectioncorrection malapropisms. WordNet: Electronic Lexical Database, chapter 13, pp.305332 Cambridge. MIT Press.Hughes, T., & Ramage, D. (2007). Lexical semantic relatedness random graph walks. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP),pp. 581589.Ide, N., & Veronis, J. (1998). Word Sense Disambiguation: State Art. ComputationalLinguistics, 24(1), 140.Islam, A., & Inkpen, D. (2008). Semantic text similarity using corpus-based word similaritystring similarity. ACM Transactions Knowledge Discovery Data, 2(2), 125.Jaccard, P. (1901). Etude comparative de la distribution florale dans une portion des alpes et desjura.. Bulletin del la Societe Vaudoise des Sciences Naturelles, 37, 547579.34fiT EXT R ELATEDNESS BASED W ORD HESAURUSJarmasz, M. (2003). Rogets thesaurus semantic similarity. Masters Thesis, UniversityOttawa.Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus semantic similarity. ProceedingsInternational Conference Recent Advances Natural Language Processing (RANLP),pp. 212219.Jiang, J., & Conrath, D. (1997). Semantic similarity based corpus statistics lexical taxonomy. Proceedings International Conference Research Computational Linguistics(ROCLING X), pp. 1933.Kucera, H., Francis, W., & Caroll, J. (1967). Computational Analysis Present Day AmericanEnglish. Brown University Press.Landauer, T., & Dumais, S. (1997). solution Platos problem: latent semantic analysistheory acquisition, induction, representation knowledge. Psychological Review,104(2), 211240.Landauer, T., Foltz, P., & Laham, D. (1998). Introduction latent semantc analysis. DiscourseProcesses, 25, 259284.Leacock, C., Miller, G., & Chodorow, M. (1998). Using corpus statistics WordNet relationssense identification. Computational Linguistics, 24(1), 147165.Lesk, M. (1986). Automated sense disambiguation using machine-readable dictionaries:tell pine cone ice cream cone. Proceedings Fifth Annual InternationalConference Systems Documentation (SIGDOC), pp. 2426.Li, P. (2008). Estimators tail bounds dimension reduction l (0 < 2) using stablerandom projections. Proceedings Nineteenth Annual ACM-SIAM SymposiumDiscrete Algorithms (SODA), pp. 1019.Li, Y., McLean, D., Bandar, Z., OShea, J., & Crockett, K. (2006). Sentence similarity basedsemantic nets corpus statistics. IEEE Transactions Knowledge Data Engineering,18(8), 11381150.Lin, D. (1998). information-theoretic definition similarity. Proceedings FifteenthInternational Conference Machine Learning (ICML), pp. 296304.Madnani, N., Zajic, D., Dorr, B., Fazil Ayan, N., & Lin, J. (2007). Multiple alternative sentencecompressions automatic text summarization. Proceedings HLT/NAACL Document Understanding Conference (DUC).Matveeva, I., Levow, G., Farahat, A., & Royer, C. (2005). Generalized latent semantic analysisterm representation. Proceedings International Conference Recent AdvancesNatural Language Processing (RANLP).Mavroeidis, D., Tsatsaronis, G., Vazirgiannis, M., Theobald, M., & Weikum, G. (2005). Word sensedisambiguation exploiting hierarchical thesauri text classification. ProceedingsNinth European Conference Principles Practice Knowledge Discovery Databases(PKDD), pp. 181192.35fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISMihalcea, R., Corley, C., & Strapparava, C. (2006). Corpus-based knowledge-based measurestext semantic similarity. Proceedings Twenty First Conference Artificial Intelligence (AAAI), pp. 775780.Mihalcea, R., & Moldovan, D. (1999). method word sense disambiguation unrestricted text.Proceedings 37th annual meeting Association Computational Linguistics(ACL), pp. 152158.Mihalcea, R., Tarau, P., & Figa, E. (2004). PageRank semantic networks applicationword sense disambiguation. Proceedings Twentieth International ConferenceComputational Linguistics (COLING).Miller, G., & Charles, W. (1991). Contextual correlates semantic similarity. LanguageCognitive Processes, 6(1), 128.Milne, D., & Witten, I. (2008). effective, low-cost measure semantic relatedness obtainedWikipedia links. Proceedings first AAAI Workshop Wikipedia ArtificialIntelligence (WIKIAI).Morris, J., & Hirst, G. (1991). Lexical cohesion computed thesaural relations indicatorstructure text. Computational Linguistics, 17, 2148.Navigli, R. (2008). structural approach automatic adjudication word sense disagreements. Natural Language Engineering, 14(4), 547573.OShea, J., Bandar, Z., Crocket, K., & McLean, D. (2008). comparative study two shorttext semantic similarity measures. Proceedings Agent Multi-Agent Systems:Technologies Applications, Second KES International Symposium (KES-AMSTA), pp.172181.Pado, S., & Lapata, M. (2007). Dependency-based construction semantic space models. Computational Linguistics, 33(2), 161199.Palmer, M., Fellbaum, C., & Cotton, S. (2001). English tasks: All-words verb lexical sample.Proceedings Senseval-2, pp. 2124.Pasca, M. (2003). Open-domain question answering large text collections. CSLI StudiesComputational Linguistics. CSLI Publications, Distributed University ChicagoPress.Pasca, M. (2005). Mining paraphrases self-anchored web sentence fragments. ProceedingsNinth European Conference Principles Practice Knowledge DiscoveryDatabases (PKDD), pp. 193204.Patwardhan, S., Banerjee, S., & Pedersen, T. (2003). Using measures semantic relatednessword sense disambiguation. Proceedings Fourth International Conference Intelligent Text Processing Computational Linguistics (CICLing), pp. 241257.Patwardhan, S., & Pedersen, T. (2006). Using WordNet based context vectors estimate semantic relatedness concepts. Proceedings EACL 2006 Workshop Making SenseSense - Bringing Computational Linguistics Psycholinguistics Together, pp. 18.36fiT EXT R ELATEDNESS BASED W ORD HESAURUSPonzetto, S., & Strube, M. (2007a). Knowledge derived Wikipedia computing semanticrelatedness. Journal Artificial Intelligence Research, 30, 181212.Ponzetto, S., & Strube, M. (2007b). Deriving large-scale taxonomy Wikipedia. Proceedings Twenty Second Conference Artificial Intelligence (AAAI), pp. 14401445.Qiu, L., Kan, M., & Chua, T. (2006). Paraphrase recognition via dissimilarity significance classification. Proceedings Conference Empirical Methods Natural LanguageProcessing (EMNLP), pp. 1826.Quilian, R. (1969). teachable language comprehender: simulation program theorylanguage. Communications ACM, 12(8), 459476.Resnik, P. (1995). Using information content evaluate semantic similarity. ProceedingsFourteenth International Joint Conference Artificial Intelligence (IJCAI), pp. 448453.Resnik, P. (1999). Semantic similarity taxonomy: information-based measure application problems ambiguity natural language. Journal Artificial IntelligenceResearch, 11, 95130.Richardson, R., & Smeaton, A. (1995). Using WordNet knowledge-based approach information retrieval. Proceedings BCS-IRSG Colloquium.Rubenstein, H., & Goodenough, J. (1965). Contextual correlates synonymy. CommunicationsACM, 8(10), 627633.Ruiz-Casado, M., Alfonseca, E., & Castells, P. (2005). Using context-window overlapping synonym discovery ontology extension. Proceedings International ConferenceRecent Advances Natural Language Processing (RANLP).Salton, G., Buckley, C., & Yu, C. (1982). evaluation term dependence models information retrieval. Proceedings Fifth Annual International ACM SIGIR ConferenceResearch Development Information Retrieval, pp. 151173.Salton, G., & McGill, M. (1983). Introduction Modern Information Retrieval. McGraw-Hill.Sanderson, M. (1994). Word sense disambiguation information retrieval. ProceedingsSeventeenth Annual International ACM SIGIR Conference Research DevelopmentInformation Retrieval, pp. 142151.Sanderson, M. (2008). Ambiguous queries: Test collections need sense. ProceedingsThirty First Annual International ACM SIGIR Conference Research DevelopmentInformation Retrieval, pp. 499506.Shinyama, Y., & Sekine, S. (2003). Paraphrase acquisition information extraction. Proceedings ACL 2nd Workshop Paraphrasing: Paraphrase Acquisition Applications,pp. 6571.Sinclair, J. (2001). Collins Cobuild English Dictionary Advanced Learners, 3rd edn. HarperCollins, New York.37fiT SATSARONIS , VARLAMIS , & VAZIRGIANNISSmeaton, A., Kelledy, F., & ODonnell, R. (1995). TREC-4 experiments Dublin City University:Thresholding posting lists, query expansion WordNet POS tagging Spanish.Proceedings Fourth Text REtrieval Conference (TREC).Snyder, B., & Palmer, M. (2004). English All-words task. Proceedings Senseval-3, pp.4143.Song, Y., Han, K., & Rim, H. (2004). term weighting method based lexical chain automatic summarization. Proceedings Fifth International Conference Intelligent TextProcessing Computational Linguistics (CICLing), pp. 636639.Stokoe, C., Oakes, M., & Tait, J. (2003). Word sense disambiguation information retrieval revisited. Proceedings Twenty Sixth Annual International ACM SIGIR ConferenceResearch Development Information Retrieval, pp. 159166.Strube, M., & Ponzetto, S. (2006). WikiRelate! Computing semantic relatedness using Wikipedia.Proceedings Twenty First Conference Artificial Intelligence (AAAI), pp. 14191424.Sussna, M. (1993). Word sense disambiguation free-text indexing using massive semantic network. Proceedings Second International Conference Information KnowledgeManagement (CIKM), pp. 6774.Terra, E., & Clarke, C. (2003). Frequency estimates statistical word similarity measures.Proceedings North American Chapter Association Computational Linguistics- Human Language Technologies Conference (HLT/NAACL)., pp. 244251.Tsang, V. (2008). Graph Approach Measuring Text Distance. PhD Thesis, UniversityToronto.Tsatsaronis, G., & Panagiotopoulou, V. (2009). generalized vector space model text retrievalbased semantic relatedness. Proceedings 12th Conference EuropeanChapter Association Computational Linguistics (EACL - Student Research Workshop), pp. 7078.Tsatsaronis, G., Varlamis, I., Nrvag, K., & Vazirgiannis, M. (2009). Omiotis: thesaurus-basedmeasure text relatedness. Proceedings European Conference Machine LearningPrinciples Practice Knowledge Discovery Databases (ECML-PKDD), pp. 742745.Tsatsaronis, G., Varlamis, I., & Vazirgiannis, M. (2008). Word sense disambiguation semanticnetworks. Proceedings 11th International Conference Text, Speech Dialogue(TSD), pp. 219226.Tsatsaronis, G., Vazirgiannis, M., & Androutsopoulos, I. (2007). Word sense disambiguationspreading activation networks generated thesauri. Proceedings Twentieth International Joint Conference Artificial Intelligence (IJCAI), pp. 17251730.Turney, P. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL. ProceedingsTwelfth European Conference Machine Learning (ECML), pp. 491502.38fiT EXT R ELATEDNESS BASED W ORD HESAURUSTurney, P. (2006). Similarity semantic relations. Computational Linguistics, 32(3), 379416.Turney, P. (2008a). latent relation mapping engine: Algorithm experiments. JournalArtificial Intelligence Research, 33, 615655.Turney, P. (2008b). uniform approach analogies, synonyms, antonyms, associations.Proceedings Twenty Second International Conference Computational Linguistics(COLING), pp. 905912.Turney, P., & Littman, M. (2005). Corpus-based learning analogies semantic relations.Machine Learning, 60(1-3), 251278.Turney, P., Littman, M., Bigham, J., & Shnayder, V. (2003). Combining independent modulessolve multiple-choice synonym analogy problems. Proceedings InternationalConference Recent Advances Natural Language Processing (RANLP), pp. 482489.van Rijsbergen, C. (1979). Information Retrieval. Butterworth.Vapnik, V. (1995). nature statistical learning theory. Springer.Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy. Proceedings Sixteenth European Conference Artificial Intelligence (ECAI), pp. 606612.Veronis, J., & Ide, N. (1990). Word sense disambiguation large neural networks extractedmachine readable dictionaries. Proceedings Thirteenth International Conference Computational Linguistics (COLING), pp. 389394.Voorhees, E. (1993). Using WordNet disambiguate word sense text retrieval. ProceedingsSixteenth Annual International ACM SIGIR Conference Research DevelopmentInformation Retrieval, pp. 171180.Wan, S., Dras, M., Dale, R., & Paris, C. (2006). Using dependency-based features take parafarce paraphrase. Proceedings Australasian Language Technology Workshop,pp. 131138.Wu, Z., & Palmer, M. (1994). Verb semantics lexical selection. Proceedings ThirtySecond Annual Meeting Association Computational Linguistics (ACL), pp. 133138.Zhang, Y., & Patrick, J. (2005). Paraphrase identification text canonicalization. ProceedingsAustralasian Language Technology Workshop, pp. 160166.39fiJournal Artificial Intelligence Research 37 (2010) 329-396Submitted 08/09; published 3/10Investigation Mathematical ProgrammingFinite Horizon Decentralized POMDPsRaghav Arasraghav.aras@gmail.comIMS, Suplec Metz2 rue Edouard Belin, Metz Technopole57070 Metz - FranceAlain Dutechalain.dutech@loria.frMAIA - LORIA/INRIACampus Scientifique - BP 23954506 Vandoeuvre les Nancy - FranceAbstractDecentralized planning uncertain environments complex task generally dealtusing decision-theoretic approach, mainly framework Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). Although DEC-POMDPSgeneral powerful modeling tool, solving task overwhelmingcomplexity doubly exponential. paper, study alternate formulation DEC-POMDPs relying sequence-form representation policies.formulation, show derive Mixed Integer Linear Programming (MILP) problemsthat, solved, give exact optimal solutions DEC-POMDPs. showMILPs derived either using combinatorial characteristics optimalsolutions DEC-POMDPs using concepts borrowed game theory.experimental validation classical test problems DEC-POMDP literature,compare approach existing algorithms. Results show mathematical programming outperforms dynamic programming less efficient forward search, exceptparticular problems.main contributions work use mathematical programming DECPOMDPs better understanding DEC-POMDPs solutions. Besides,argue alternate representation DEC-POMDPs could helpful designingnovel algorithms looking approximate solutions DEC-POMDPs.1. Introductionframework Decentralized Partially Observable Markov Decision Processes (DECPOMDPs) used model problem designing system made autonomousagents need coordinate order achieve joint goal. Solving DEC-POMDPsuntractable task belong class NEXP-complete problems (see Section 1.1).paper, DEC-POMDPs reformulated sequence-form DEC-POMDPsderive Mixed Integer Linear Programs solved using efficient solversorder design exact optimal solutions finite-horizon DEC-POMDPs. mainmotivation investigate benefits limits novel approach getbetter understanding DEC-POMDPs (see Section 1.2). practical level, providenew algorithms heuristics solving DEC-POMDPs evaluate classicalproblems (see Section 1.3).c2010AI Access Foundation. rights reserved.fiAras & Dutech1.1 ContextOne main goals Artificial Intelligence build artificial agents exhibitintelligent behavior. agent entity situated environment perceivesensors act upon using actuators. concept planning, i.e., selectsequence actions order reach goal, central field ArtificialIntelligence years. notion intelligent behavior difficult assessmeasure, prefer refer concept rational behavior formulated RussellNorvig (1995). consequence, work presented uses decision-theoreticapproach order build agents take optimal actions uncertain partiallyunknown environment.particularly interested cooperative multi-agent systems multipleindependent agents limited perception environment must interact coordinate order achieve joint task. central process full knowledge statesystem control agents. contrary, agent autonomousentity must execute actions itself. setting blessing, agentideally deal small part problem, curse, coordinationcooperation harder develop enforce.decision-theoretic approach rational behavior relies mostly frameworkMarkov Decision Processes (MDP) (Puterman, 1994). system seen sequencediscrete states stochastic dynamics, particular states giving positive negativereward. process divided discrete decision periods; number periodscalled horizon MDP. periods, action choseninfluence transition process next state. using right actionsinfluence transition probabilities states, objective controllersystem maximize long term return, often additive function rewardearned given horizon. controller knows dynamics system,made transition function reward function, algorithms derived fieldDynamic Programming (see Bellman, 1957) allow controller compute optimaldeterministic policy, i.e., decision function associates optimal action everystate expected long term return optimal. process called planningMDP community.fact, using MDP framework, quite straightforward model problemone agent full complete knowledge state system. agents,especially multi-agent setting, generally able determine completeexact state system noisy, faulty limited sensorsnature problem itself. consequence, different states system observedsimilar agent problem different optimal actions takenstates; one speaks perceptual aliasing. extension MDPs called PartiallyObservable Markov Decisions Processes (POMDPs) deals explicitly phenomenonallows single agent compute plans setting provided knows conditionalprobabilities observations given state environment (Cassandra, Kaelbling, &Littman, 1994).pointed Boutilier (1996), multi-agent problems could solved MDPsconsidered centralized point view planning control. Here, although330fiMathematical Programming DEC-POMDPsplanning centralized process, interested decentralized settings everyagent executes policy. Even agents could instantly communicate observation, consider problems joint observation resulting communications would still enough identify state system. frameworkDecentralized Partially Observable Markov Decision Processes (DEC-POMDP) proposedBernstein, Givan, Immerman, Zilberstein (2002) takes account decentralizationcontrol partial observability. DEC-POMDP, looking optimal jointpolicies composed one policy agent, individual policiescomputed centralized way independently executed agents.main limitation DEC-POMDPs provably untractablebelong class NEXP-complete problems (Bernstein et al., 2002). Concretely,complexity result implies that, worst case, finding optimal joint policy finitehorizon DEC-POMDP requires time exponential horizon one always makegood choices. complexity, algorithms finding exactoptimal solutions DEC-POMDPs (they doubly exponential complexity)look approximate solutions. discussed detailedwork Oliehoek, Spaan, Vlassis (2008), algorithms follow either dynamicprogramming approach forward search approach adapting concepts algorithmsdesigned POMDPs.Yet, concept decentralized planning focus quite large bodyprevious work fields research. example, Team Decision Problem (Radner,1959), later formulated Markov system field control theory AndersonMoore (1980), led Markov Team Decision Problem (Pynadath & Tambe, 2002).field mathematics, abundant literature Game Theory brings new waylooking multi-agent planning. particular, DEC-POMDP finite horizonthought game extensive form imperfect information identical interests(Osborne & Rubinstein, 1994).Taking inspiration field game theory mathematical programming design exact algorithms solving DEC-POMDPs precisely subject contributionfield decentralized multi-agent planning.1.2 Motivationsmain objective work investigate use mathematical programming,especially mixed-integer linear programs (MILP) (Diwekar, 2008), solving DECPOMDPs. motivation relies fact field linear programming quitemature great interest industry. consequence, exist many efficientsolvers mixed-integer linear programs want see efficient solversperform framework DEC-POMDPs.Therefore, reformulate DEC-POMDP solve mixed-integer linearprogram. shown article, two paths lead mathematical programs, onegrounded work Koller, Megiddo, von Stengel (1994), Koller Megiddo(1996) von Stengel (2002), another one grounded combinatorial considerations.methods rely special reformulation DEC-POMDPs called331fiAras & Dutechsequence-form DEC-POMDPs policy defined histories (i.e., sequencesobservations actions) generate applied DEC-POMDP.basic idea work select, among histories DEC-POMDP,histories part optimal policy. end, optimal solutionMILP presented article assign positive weight history DECPOMDP every history non-negative weight part optimal policyDEC-POMDP. number possible histories exponential horizonproblem, complexity naive search optimal set histories doublyexponential. Therefore, idea appears untractable useless.Nevertheless, show combining efficiency MILP solvers quitesimple heuristics leads exact algorithms compare quite well existing exactalgorithms. fact, sequence-form DEC-POMDPs need memory space exponentialsize problem. Even solving MILPs also exponential sizeMILP thus leads doubly exponential complexity sequence-form based algorithms,argue sequence-form MILPs compare quite well dynamic programming thanksoptimized industrial MILP solvers like Cplex.Still, investigations experiments Mathematical Programming DECPOMDPs solely aim finding exact solutions DEC-POMDPs. main motivation better understanding DEC-POMDPs limits benefitsmathematical programming approach. hope knowledge help decidingextent mathematical programming sequence-form DEC-POMDPs useddesign novel algorithms look approximate solutions DEC-POMDPs.1.3 Contributionspaper develop new algorithms order find exact optimal joint policiesDEC-POMDPs. main inspiration comes work Koller, von StegelMegiddo shows solve games extensive form imperfect informationidentical interests, find Nash equilibrium kind game (Koller et al.,1994; Koller & Megiddo, 1996; von Stengel, 2002). algorithms caused breakthroughmemory space requirement approach linear size game whereascanonical algorithms required space exponential size game.breakthrough mostly due use new formulation policy callsequence-form.main contribution, detailed Section 3.3, adapt sequence-formintroduced Koller, von Stegel Megiddo framework DEC-POMDPs (Kolleret al., 1994; Koller & Megiddo, 1996; von Stengel, 2002). result, possibleformulate resolution DEC-POMDP special kind mathematical programstill solved quite efficiently: mixed linear program variablesrequired either 0 1. adaptation resulting mixed-integer linearprogram straightforward. fact, Koller, von Stegel Megiddo could findone Nash equilibrium 2-agent game. needed DEC-POMDPs findset policies, called joint policy, corresponds Nash equilibriumhighest value, finding one Nash equilibrium already complex taskenough. Besides, whereas Koller, von Stegel Megiddo algorithms could applied332fiMathematical Programming DEC-POMDPs2-agent games, extend approach solve DEC-POMDPs arbitrarynumber agents, constitutes important contribution.order formulate DEC-POMDPs MILPs, analyze detail structureoptimal joint policy DEC-POMDP. joint policy sequence-form expressedset individual policies described set possible trajectoriesagents DEC-POMDP. Combinatorial considerations individualhistories, well constraints ensure histories define valid joint policyheart formulation DEC-POMDP mixed linear program, developpedSections 4 5. Thus, another contribution work better understandingproperties optimal solutions DEC-POMDPs, knowledge might leadformulation new approximate algorithms DEC-POMDPs.Another important contribution work introduce heuristics boosting performance mathematical programs propose (see Section 6).heuristics take advantage succinctness DEC-POMDP model knowledge acquired regarding structure optimal policies. Consequently, ablereduce size mathematical programs (resulting also reducing time takensolve them). heuristics constitute important pre-processing step solvingprograms. present two types heuristics: elimination extraneous historiesreduces size mixed integer linear programs introduction cutsmixed integer linear programs reduces time taken solve program.practical level, article presents three different mixed integer linearprograms, two directly derived work Koller, von Stegel Megiddo(see Table 4 5) third one based solely combinatorial considerationsindividual policies histories (see Table 3). theoretical validity formulations backed several theorems. also conducted experimental evaluationsalgorithms heuristics several classical DEC-POMDP problems. thusable confirm algorithms quite comparable dynamic programming exactalgorithms outperformed forward search algorithms like GMAA* (Oliehoek et al.,2008). problems, though, MILPs indeed faster one order magnitudetwo GMAA*.1.4 Overview Articleremainder article organized follows. Section 2 introduces formalismDEC-POMDP background classical algorithms, usually based dynamicprograming. expose reformulation DEC-POMDP sequence-formSection 3 also define various notions needed sequence-form. Section 4,show use combinatorial properties sequence-form policies derive firstmixed integer linear program (MILP, Table 3) solving DEC-POMDP. using gametheoretic concepts like Nash equilibrium, take inspiration previous work gamesextensive form design two MILPs solving DEC-POMDP (Tables 4, 5).MILPs smaller size detailed derivation presented Section 5.contributed heuristics speed practical resolutions various MILPs makecore Section 6. Section 7 presents experimental validations MILP-basedalgorithms classical benchmarks DEC-POMDP literature well randomly333fiAras & Dutechbuilt problems. Finally, Section 8 analyzes discusses work concludepaper Section 9.2. Dec-POMDPsection gives formal definition Decentralized Partially Observed Markov DecisionProcesses introduced Bernstein et al. (2002). described, solution DECPOMDP policy defined space information sets optimal value.sections ends quick overview classical methods developedsolve DEC-POMDPs.2.1 Formal DefinitionDEC-POMDP defined tuple = h I, S, {Ai }, P, {Oi }, G, R, , where:= {1, 2, , n} set agents.finite set states. set probability distributions shall denoted(S). Members (S) shall called belief states.agent I, Ai set actions. = iI Ai denotes set jointactions.P : [0, 1] state transition function. s,A, P(s, a, ) probability state problem period if,period 1, state agents performed joint action a. Thus,time period 2, pair states s, joint action A,holds:P(s, a, ) = Pr(st = |st1 = s, = a).Thus, (S, A, P) defines discrete-state, discrete-time controlled Markov process.agent I, Oi set observations. = iI Oi denotes set jointobservations.G : [0, 1] joint observation function. A,S, G(a, s, o) probability agents receivejoint observation (that is, agent receives observation oi ) stateproblem period previous period agents took jointaction a. Thus, time period 2, joint action A, statejoint observation O, holds:G(a, s, o) = Pr(ot = o|st = s, at1 = a).R : R reward function. A, R(s, a) Rreward obtained agents take joint action stateprocess s.334fiMathematical Programming DEC-POMDPshorizon problem. agents allowed joint-actionsprocess halts.(S) initial state DEC-POMDP. S, (s) denotesprobability state problem first period s.said, S, P define controlled Markov Process next state dependsprevious state joint action chosen agents. agentsaccess state process rely observations, generallypartial noisy, state, specified observation function G. timetime, agents receive non-zero reward according reward function R.n0n11011s0s1ntn11t11s2nt1tstFigure 1: DEC-POMDP. every period process, environment statest , every agent receives observations oti decides action ati . jointaction hat1 , at2 , , atn alters state process.specifically, illustrated Figure 1, control DEC-POMDP nagents unfolds discrete time periods, = 1, 2, ,T follows. period t,process state denoted st S. first period = 1, state s1 chosenaccording agents take actions a1i . period > 1 afterward, agenttakes action denoted ati Ai according agents policy.agents take joint action = hat1 , at2 , , atn i, following events occur:1. agents obtain reward R(st , ).2. state st+1 determined according function P arguments st .3. agent receives observation ot+1Oi . joint observation ot+1 =t+1t+1t+1 .hot+11 , o2 , , determined function G arguments4. period changes + 1.paper, DEC-POMDP interested following properties:335fiAras & Dutechhorizon finite known agents;agents cannot infer exact state system joint observations (thisgeneral setting DEC-POMDPs);agents observe actions observations agents.aware observations reward;agents perfect memory past; base choice actionsequence past actions observations. speak perfect recall setting;transition observation functions stationary, meaning dependperiod t.Solving DEC-POMDP means finding agents policies (i.e., decision functions)optimize given criterion based rewards received. criterion workcalled cumulative reward defined by:"#XER(s , ha1 , a2 , . . . , i)(1)t=1E mathematical expectation.2.2 Example DEC-POMDPproblem known Decentralized Tiger Problem (hereby denoted MA-Tiger),introduced Nair, Tambe, Yokoo, Pynadath, Marsella (2003), widely usedtest DEC-POMDPs algorithms. variation problem previously introducedPOMDPs (i.e., DEC-POMDPs one agent) Kaelbling, Littman, Cassandra(1998).problem, given two agents confronted two closed doors. Behind onedoor tiger, behind escape route. agents know doorleads what. agent, independently other, open one two doorslisten carefully order detect tiger. either opens wrong door,lives imperiled. open escape door, free.agents limited time decide door open. use timegather information precise location tiger listening carefully detectlocation tiger. problem formalized DEC-POMDP with:two states tiger either behind left door (sl ) right door (sr );two agents, must decide act;three actions agent: open left door (al ), open right door (ar )listen (ao );two observations, thing agent observe hear tigerleft (ol ) right (or ).336fiMathematical Programming DEC-POMDPsinitial state chosen according uniform distribution S. long doorremains closed, state change but, one door opened, state reseteither sl sr equal probability. observations noisy, reflecting difficultydetecting tiger. example, tiger left, action ao producesobservation ol 85% time. agents perform ao , joint observation(ol ,ol ) occurs probability 0.85 0.85 = 0.72. reward function encouragesagents coordinate actions as, example, reward open escapedoor (+20) bigger one listens opens good door (+9).full state transition function, joint observation function reward function describedwork Nair et al. (2003).2.3 Information Sets Historiesinformation set agent sequence (a1 .o2 .a2 .o3 .ot ) even lengthelements odd positions actions agent (members Ai ) evenpositions observations agent (members Oi ). information set length 0shall called null information set, denoted . information set length 1shall called terminal information set. set information sets lengths lessequal 1 shall denoted .define history agent sequence (a1 .o2 . a2 . o3 .ot .at ) oddlength elements odd positions actions agent (members Ai )even positions observations agent (members Oi ). define lengthhistory number actions history (t example). historylength shall called terminal history. Histories lengths less shall callednon-terminal histories. history null length shall denoted . informationset associated history h, denoted (h), information set composed removingh last action. h history observation, h.o information set.shall denote Hit set possible histories length agent i. Thus, Hi1set actions Ai . shall denote Hi set histories agent lengthsless equal . size ni Hi thus:ni = |Hi | =PTt1t=1 |Ai | |Oi |= |Ai |(|Ai ||Oi |)T 1.|Ai ||Oi | 1(2)set HiT terminal histories agent shall denoted Ei . set Hi \HiTnon-terminal histories agent shall denoted Ni .tuple hh1 , h2 , . . . , hn made one history agent called joint history.tuple obtained removing history hi joint history h noted hi calledi-reduced joint history.Example Coming back MA-Tiger example, set valid histories could be: , (ao ),(ao .ol .ao ), (ao .or .ao ), (ao .ol .ao .ol .ao ), (ao .ol .ao .or .ar ), (ao .or .ao .ol .ao ) (ao .or .ao .or .ar ).Incidently, set histories corresponds support policy (i.e., historiesgenerated using policy) Figure 2, explained next section.337fiAras & Dutech2.4 Policiesperiod time, policy must tell agent action choose. choicebased whatever past present knowledge agent processtime t. One possibility define individual policy agent mappinginformation sets actions. formally:: (Ai )(3)Among set policies, three families usually distinguished:Pure policies. pure deterministic policy maps given information set oneunique action. set pure policies agent denoted . Pure policiescould also defined using trajectories past observations since actions,chosen deterministically, reconstructed observations.Mixed policies. mixed policy probability distribution set purepolicies. Thus, agent using mixed policy control DEC-POMDP usingpure policy randomly chosen set pure policies.Stochastic policies. stochastic policy general formulation associatesprobability distribution actions history.come back MA-Tiger problem (Section 2.2), Figure 2 gives possible policyhorizon 2. shown, policy classically represented action-observation tree.kind tree, branch labelled observation. given sequence pastobservations, one starts root node follows branches actionnode. node contains action executed agent seensequence observations.Observation sequenceChosen actionolaoaoaool .olalol .orao.olao.oraraoolaoaoololalaoaoarFigure 2: Pure policy MA-Tiger. pure policy maps sequences observationsactions. represented action-observation tree.joint policy = h1 , 2 , , n n-tuple policy agent i.individual policies must horizon. agent i, also definenotion i-reduced joint policy = h1 , , i1 , i+1 , , n composedpolicies agents. thus = hi , i.338fiMathematical Programming DEC-POMDPs2.5 Value Functionexecuted agents, every -horizon joint policy generates probability distribution possible sequences reward one compute valuepolicy according Equation 1. Thus value joint policy formally defined as:V (, ) = E"XR(st , )|,t=1#(4)given state first period chosen according actions chosenaccording .recursive definition value function policy also waycompute horizon finite. definition requires conceptsshall introduce.Given belief state (S), joint action joint observation O,let (o|, a) denote probability agents receive joint observation takejoint action period state chosen according . probabilitydefinedXX(o|, a) =(s)P(s, a, )G(a, , o)(5)sSGiven belief state (S), joint action joint observation ,updated belief state ao (S) respect defined (forS),ao (s ) =PG(a,s ,o)[ sS (s)P(s,a,s )](o|,a)(o|, a) > 0(6)ao (s ) =0(o|, a) = 0(7)PGiven belief state (S) joint action A, R(, a) denotes sS (s)R(s, a).Using definitions notations, value V (, ) defined follows:V (, ) = V (, , )(8)V (, , ) defined recursion using equations (9), (10) (11), given below.equations straight reformulation classical Bellman equations finitehorizon problems.histories null lengthV (, , ) = R(, ()) +X(o|, ())V (()o , , o)(9)oO() denotes joint action h1 (), 2 (), , n ()i ()o denotesupdated state given () joint observation o.339fiAras & Dutechnon-terminal histories. (S), {1, . . . , 2},1:T1:T1:Ttuple sequences observations o1:T = ho1:T1 , o2 , , oisequence observations agent I:V ( , , o1:T ) = R( , (o1:T )) +X1:T )o(o| , (o1:T ))V ((o, , o1:T .o) (10)oO1:T(o )o updated state given joint action (o1:T ) joint observation= ho1 , o2 , , o1:T .o tuple sequences (t + 1) observations ho1:T1 .o1 ,1:T .o i.o1:T.o,,n2n2terminal histories. (S), tuple sequences (T - 1)111 i:observations o1:T 1 = ho1:T, o1:T, , o1:Tn12V ( , , o1:T 1 ) = R(, (o1:T 1 )) =X(s)R(s, (o1:T 1 ))(11)sSoptimal policy policy best possible value, verifying:V (, ) V (, ).(12)important fact DEC-POMDPs, based following theorem,restrict set pure policies looking solution DEC-POMDP.Theorem 2.1. DEC-POMDP least one optimal pure joint policy.Proof: See proof work Nair et al. (2003).2.6 Overview DEC-POMDPs Solutions Limitationsdetailed work Oliehoek et al. (2008), existing methods solving DECPOMDPs finite-horizon belong several broad families: brute force, alternatingmaximization, search algorithms dynamic programming.Brute Force simplest approach solving DEC-POMDP enumeratepossible joint policies evaluate order find optimal one. However,method becomes quickly untractable number joint policies doubly exponentialhorizon problem.Alternating Maximization Following Chades, Scherrer, Charpillet (2002) Nairet al. (2003), one possible way solve DEC-POMDPs agent (or small groupagents) alternatively search better policy agents freezepolicy. Called alternating maximization Oliehoek alternated co-evolutionChades method guarantees find Nash equilibria, locally optimal jointpolicy.340fiMathematical Programming DEC-POMDPsHeuristic Search Algorithms concept introduced Szer, Charpillet,Zilberstein (2005) relies heuristic search looking optimal joint policy,using admissible approximation value optimal joint policy. searchprogresses, joint policies provably worse current admissible solutionpruned. Szer et al. used underlying MDPs POMDPs compute admissible heuristic,Oliehoek et al. (2008) introduced better heuristic based resolution BayesianGame carefully crafted cost function. Currently, Oliehoeks method called GMAA*(for Generic Multi-Agent A*) quickest exact method large set benchmarks.But, every exact method, limited quite simple problems.Dynamic Programming work Hansen, Bernstein, Zilberstein (2004)adapts solutions designed POMDPs domain DEC-POMDPs. generalidea start policies 1 time step used build 2 time step policieson. process clearly less efficient heuristic search approachexponential number policies must constructed evaluated iterationalgorithm. policies pruned but, again, pruning less efficient.exposed details paper Oliehoek et al. (2008), several others approaches developed subclasses DEC-POMDPs. example, special settings agents allowed communicate exchange informations settingstransition function split independant transition functions agentstudied found easier solve generic DEC-POMDPs.3. Sequence-Form DEC-POMDPssection introduces fundamental concept policies sequence-form. newformulation DEC-POMDP thus possible leads Non-Linear Program(NLP) solution defines optimal solution DEC-POMDP.3.1 Policies Sequence-Formhistory function p agent mapping set histories interval[0, 1]. value p(h) weight history h history function p. policydefines probability function set histories agent saying that,history hi Hi , p(hi ) conditional probability hi given observation sequence(o0i .o1i . .oti ) .every policy defines policy function, every policy function associatedvalid policy. constraints must met. fact, history function p sequenceform policy agent following constraints met:Xp(a) = 1,(13)aAip(h) +Xp(h.o.a) = 0,h Ni , Oi ,(14)aAih.o.a denotes history obtained concatenating h. definitionappears slightly different form Lemma 5.1 work Koller et al. (1994).341fiAras & DutechVariables: x(h), h Hi ,Xx(a) = 1(15)aAix(h) +Xx(h.o.a) = 0,h Ni , Oi(16)h Hi(17)aAix(h) 0,Table 1: Policy Constraints. set linear inequalities, solved, provide validsequence-form policy agent i. is, weights x(h), possibledefine policy agent i.sequence-form policy stochastic probability choosing actioninformation set h.o p(h.o.a)/p(h). support S(p) sequence-form policy madeset histories non-negative weight, i.e. S(p) = {h Hi | p(h) > 0}.sequence-form policy p defines unique policy agent, sequence-form policycalled policy rest paper ambiguity present.set policies sequence-form agent shall denoted Xi . setpure policies sequence-form shall denoted Xi Xi .way similar definitions Section 2.4, define sequence-form jointpolicy tuple sequence-form policies, one agent. weight Qjointhistory h = hhi sequence-form joint policy hp1 , p2 , , pn product iI pi (hi ).set joint policies sequence-form iI Xi shall denoted X seti-reduced sequence-form joint policy called Xi .3.2 Policy Constraintspolicy agent sequence-form found solving set linear inequalities(LI) found Table 1. LI merely implement definition policy sequenceform. LI contains one variable x(h) history h Hi represent weighth policy. solution x LI constitutes policy sequence-form.Example Section E.1 Appendices, policy constraints decentralizedTiger problem given 2 agents horizon 2.Notice policy constraints agent, variable constrainednon-negative whereas definition policy sequence-form, weight historymust interval [0, 1]. mean variable solution policyconstraints assume value higher 1? Actually, policy constraintsprevent variable assuming value higher 1 following lemmashows.Lemma 3.1. every solution x (15)-(17), h Hi , x (h) belongs [0, 1]interval.342fiMathematical Programming DEC-POMDPsProof: shown forward induction.Every x(h) non-negative (see Eq. (17)), also case every actionAi . Then, x(a) greater 1 otherwise constraint (15) would violated. So,h Hi1 , (i.e. Ai ), x(h) belong [0, 1].every h Hit x(h) [0, 1], previous reasoning applied using constraint(16) leads evidently fact x(h) [0, 1] every h Hit+1 .Thereby, induction holds t.Later article, order simplify task looking joint policies, policyconstraints LI used find pure policies. Looking pure policies limitationfinite-horizon DEC-POMDPs admit deterministic policies policies definedinformation set. fact, pure policies needed two three MILPs buildorder solve DEC-POMDPs, otherwise derivation would possible (seeSections 4 5.4).Looking pure policies, obvious solution would impose every variablex(h) belongs set {0, 1}. But, solving mixed integer linear program,generally good idea limit number integer variables integer variablepossible node branch bound method used assign integer valuesvariables. efficient implementation mixed integer linear program takeadvantage following lemma impose weights terminal historiestake 0 1 possible values.Lemma 3.2. (15)-(17), (17) replaced by,x(h) 0,h Nix(h) {0, 1},h Ei(18)(19)every solution x resulting LI, h Hi , x (h) = 0 1.speak 0-1 LI.Proof: prove backward induction. Let h history length - 1.Due (16), Oi , holds,Xx (h) =x (h.o.a).(20)aAiSince h history length - 1, history h.o.a terminal history. Due Lemma3.1, x (h) [0, 1]. Therefore, sum right hand side equation also[0, 1]. due (19), x (h.o.a) {0, 1}. Hence sum right hand sideeither 0 1, value between. Ergo, x (h) {0, 1} valuebetween. reasoning, show x (h) {0, 1} every non-terminalhistory h length - 2, - 3, , 1.formulate linear inequalities Table 1 memory, requireP spaceexponential horizon. agent I, size Hi Tt=1 |Ai |t |Oi |t1 .exponential number variablesLP also exponential .PT 1innumber constraints LI Table 1 t=0 |Ai | |Oi |t , meaning numberconstraints LI also exponential .343fiAras & Dutech3.3 Sequence-Form DEC-POMDPable give formulation DEC-POMDP based use sequence-formpolicies. want stress re-formulation, provide usnew ways solving DEC-POMDPs mathematical programming.Given classical formulation DEC-POMDP (see Section 2.1), equivalentsequence-form DEC-POMDP tuple hI, {Hi }, , Ri where:= {1, 2, , n} set agents.agent I, Hi set histories length less equalagent i, defined previous section. set Hi derived using setsAi Oi .joint history conditional probability function. joint history j H,(, j) probability j occurring conditional agents taking joint actionsaccording given initial state DEC-POMDP . functionderived using set states S, state transition function P jointobservation function G.R joint history value. joint history j H, R(, j) valueexpected reward agents obtain joint history j occurs. functionderived using set states S, state transition function P, joint observationfunction G reward function R. Alternatively, R described functionR.formulation folds S, P G R relying set histories.give details computation R.(, j) conditional probability sequence joint observations receivedagents till period (o1 (j).o2 (j). . ot1 (j)) sequence joint actionstaken till period - 1 (a1 (j). a2 (j). . at1 (j)) initial stateDEC-POMDP . is,(, j) = Prob.(o1 (j).o2 (j). .ot1 (j)|, a1 (j).a2 (j). .at1 (j))(21)probability product probabilities seeing observation ok (j) givenappropriate belief state action chosen time k, is:(, j) =t1(ok (j)|jk1 , ak (j))(22)k=1jk probability distribution given agents followed jointhistory j time k, is:jk (s) = Prob.(s|o1 (j).a1 (j). .ok (j)).344(23)fiMathematical Programming DEC-POMDPsVariables: xi (h), I, h HiMaximizeXR(, j)jExi (ji )(27)iIsubjectXxi (a) = 1,(28)I, h Ni , Oi(29)I, h Hi(30)aAixi (h) +Xxi (h.o.a) = 0,aAixi (h) 0,Table 2: NLP. non-linear program expresses constraints finding sequenceform joint policy optimal solution DEC-POMDP.Regarding value joint history, defined by:R(, j) = R(, j)(, j)(24)R(, j) =XXjk1 (s)R(s, ak (j)).(25)k=1 sSThus, V(, p), value sequence-form joint policy p, weighted sumvalue histories support:XV(, p) =p(j)R(, j)(26)jHp(j) =QiIpi (ji ).3.4 Non-Linear Program Solving DEC-POMDPs.using sequence-form formulation DEC-POMDP, able express jointpolicies sets linear constraints assess value every policy. Solving DECPOMDP amounts finding policy maximal value, donenon-linear program (NLP) Table 2 where, again, xi variables weightshistories agent i.Example example formulation NLP found Appendices,Section E.2. given decentralized Tiger problem 2 agents horizon2.constraints program form convex set, objective functionconcave (as explained appendix A). general case, solving non-linear program345fiAras & Dutechdifficult generalized method guarantee finding global maximumpoint. However, particular NLP fact Multilinear Mathematical Program (seeDrenick, 1992) kind programs still difficult solve. twoagents considered, one speaks bilinear programs, solved easily(Petrik & Zilberstein, 2009; Horst & Tuy, 2003).evident, inefficient, method find global maximum point evaluateextreme points set feasible solutions program since known everyglobal well local maximum point non-concave function lies extreme pointset (Fletcher, 1987). inefficient method test tellsextreme point local maximum point global maximum point. Hence, unlessextreme points evaluated, cannot sure obtained global maximumpoint. set feasible solutions NLP X, set -step joint policies.set extreme points set X, set pure -step joint policies, whose numberdoubly exponential exponential n. enumerating extreme pointsNLP untractable.approach, developed next sections, linearize objective functionNLP order deal linear programs. describe two waysthis: one based combinatorial consideration (Section 4) basedgame theory concepts (Section 5). cases, shall mean adding variablesconstraints NLP, upon so, shall derive mixed integer linear programspossible find global maximum point hence optimal joint policyDEC-POMDP.4. Combinatorial Considerations Mathematical Programmingsection explains possible use combinatorial properties DEC-POMDPstransform previous NLP mixed integer linear program. shown, mathematical program belongs family 0-1 Mixed Integer Linear Programs, meaningvariables linear program must take integer values set {0, 1}.4.1 Linearization Objective FunctionBorrowing ideas field Quadratic Assignment Problems (Papadimitriou & Steiglitz, 1982), turn non-linear objective function previous NLP linearobjective function linear constraints involving new variables z must take integervalues. variable z(j) represents product xi (ji ) variables.Thus, objective function was:XmaximizeR(, j)xi (ji )(31)jEiIrewrittenmaximizeXR(, j)z(j)jEj = hj1 , j2 , , jn i.346(32)fiMathematical Programming DEC-POMDPsmust ensure two way mapping value new variablesz x variables solution mathematical program, is:z (j) =xi (ji ).(33)iIthis, restrict ourself pure policies x variables 0 1.case, previous constraint (33) becomes:z (j) = 1 xi (ji ) = 1,(34)There, take advantage fact support pure policy agentcomposed |Oi |T 1 terminal histories express new constraints. one hand,guarantee z(j) equal 1 enough x variables also equal 1,write:nXxi (ji ) nz(j) 0,j E.(35)i=1hand, limit number z(j) variables take value 1,enumerate number joint terminal histories end with:X|Oi |T 1 .(36)z(j) =iIjEconstraints (35) would weight heavily mathematical program wouldone constraint terminal joint history, number exponential n. idea reduce number constraints reason joint historiesindividual histories. history h agent part support solutionofPthe problem (i.e., xQ(h) = 1) number joint historiesQ belongs( j Ei z(hh, j i)) kI\{i} |Ok |T 1 . Then, suggest replace |Ei | constraints(35)nXxi (ji ) nz(j) 0,j E.(35)i=1P|Ei | constraintsX|Ok |T 1xi (h)|Oi |T 1= xi (h)|Ok |T 1 ,z(hh, j i) =j EiQkII, h Ei .(37)kI\{i}4.2 Fewer Integer Variableslinearization objective function rely fact dealing purepolicies, meaning every x z variable supposed value either 0 1. solvinglinear programs integer variables usually based branch bound technique347fiAras & DutechVariables:xi (h), I, h Hiz(j), j EXMaximizeR(, j)z(j)(38)jEsubject to:Xxi (a) = 1,(39)I, h Ni , Oi(40)aAixi (h) +Xxi (h.o.a) = 0,aAiXz(hh, j i) = xi (h)j Hi|Ok |T 1 ,I, h Ei(41)kI\{i}Xz(j) =jE|Oi |T 1(42)iIxi (h) 0,I, h Nixi (h) {0, 1},(43)I, h Eiz(j) [0, 1],(44)j E(45)Table 3: MILP. 0-1 mixed integer linear program finds sequence-form joint policyoptimal solution DEC-POMDP.(Fletcher, 1987), efficiency reasons, important reduce number integervariables mathematical programs.done Section 3.2, relax x variables allow take non-negativevalues provided x values terminal histories constrained integer values.Furthermore, proved following lemma, constraints x also guaranteez variables take value {0, 1}.eventually end following linear program real integer variables,thus called 0-1 mixed integer linear program (MILP). MILP shown Table 3.Example Section E.3 Appendices, example MILP givenproblem decentralized Tiger 2 agents horizon 2.Lemma 4.1. every solution (x , z ) MILP Table 3, j E, z (j)either 0 1.Proof: Let (x , z ) solution MILP. Let,S(z) = {j E|z (j) > 0}Si (xi ) = {hEi |xi (h)= 1},(46)Si (z, j ) = {j E|ji = j , z (j) > 0},348(47)I,jEi(48)fiMathematical Programming DEC-POMDPsQQ1 . showing |S(z)|1 ,Now, due (42) (45), |S(z)|i|iI |Oi |Q iIT|O1shall establish |S(z)| = iI |Oi |. due upper bound 1 zvariable, implication z (j) 0 1 terminal joint history j thusproving statement lemma.Note Lemma (3.2), agent i, xi pure policy. Therefore,|Si (x)| = |Oi |T 1 . means set constraints (41), i-reduced terminaljoint history j Ei appear right hand side |Oi |T 1 timesleft hand side, xi (h) = 1. Thus, j Ei ,|Si (z, j )| |Oi |T 1 .(49)(h) either 0 1 sinceNow, know agent history h Hi , xi Qxi pure policy. So, given i-reduced terminal joint history j , kI\{i} xk (jk ) either0 1. Secondly, due (41), following implication clearly holds terminal jointhistory j,z (j) > 0 xi (ji ) = 1,I.(50)Therefore, obtain|Si (z, j )| |Oi |T 1(51)1= |Oi |xk (jk ).(52)kI\{i}consequence,XX|Si (z, j )|j Ei|Oi |T 1j Eixk (jk )(53)xk (jk )(54)xk (h )(55)kI\{i}= |Oi |T 1Xj Ei kI\{i}= |Oi |T 1XkI\{i} h Ek= |Oi |T 1|Ok |T 1(56)kI\{i}=1|Oj |.(57)jISincej EiSi (z, j ) = S(z), holdsP|S(z)|j Ei|Si (z, j )| = |S(z)|. Hence,|Oj |T 1 .(58)jIThus statement lemma proved.349fiAras & Dutech4.3 Summaryusing combinatorial considerations, possible design 0-1 MILP solving givenDEC-POMDP. proved theorem 4.1, solution MILP defines optimal jointpolicyNevertheless, MILP quite large, O(kT ) constraintsPfor DEC-POMDP.QnT|Hi | + |Ei | = O(k ) variables, O(kT ) variables must take integer values.next section details another method linearization NLP leadssmaller mathematical program 2-agent case.Theorem 4.1. Given solution (x , z ) MILP, x = hx1 , x2 , , xn pure-period optimal joint policy sequence-form.Proof: Due policy constraints domain constraints agent, xipure sequence-form policyQ agent i. Due constraints (41)-(42), z values 1product iI xi (ji ) values 1. Then, maximizing objective functioneffectively maximizing value sequence-form policy hx1 , x2 , , xn i. Thus,hx1 , x2 , , xn optimal joint policy original DEC-POMDP.5. Game-Theoretical Considerations MathematicalProgrammingsection borrows concepts like Nash equilibrium regret game theoryorder design yet another 0-1 Mixed Integer Linear Program solving DEC-POMDPs.fact, two MILPs designed, one applied 2 agentsone number agents. main objective part derive smallermathematical program 2 agent case. Indeed, MILP-2 agents (see Table 4)slightly less variables constraints MILP (see Table 3) thus might prove easiersolve. hand, 2 agents considered, new derivationleads MILP given completeness bigger MILP.Links fields multiagent systems game theory numerousliterature (see, example, Sandholm, 1999; Parsons & Wooldridge, 2002). elaborate fact optimal policy DEC-POMDP Nash Equilibrium.fact Nash Equilibrium highest utility agents share reward.2-agent case, derivation make order build MILP similarfirst derivation Sandholm, Gilpin, Conitzer (2005). give detailsderivation adapt DEC-POMDP adding objective function it.2 agents, derivation still use find Nash equilibriae pure strategies.rest article, make distinction policy, sequence-formpolicy strategy agent as, context, concepts equivalent. Borrowinggame theory, joint policy denoted p q, individual policy pi qii-reduced policy pi qi .5.1 Nash EquilibriumNash Equilibrium joint policy policy best response reducedjoint policy formed policies joint policy. context sequence-form350fiMathematical Programming DEC-POMDPsDEC-POMDP, policy pi Xi agent said best response i-reducedjoint policy qi Xi holdsV(, hpi , qi i) V(, hpi , qi i) 0,pi Xi .(59)joint policy p X Nash Equilibrium holdsV(, p) V(, hpi , pi i) 0,is,X XhEi j EiR(, hh, j i)kI\{i}I, pi Xi .0,pk (jk ) pi (h) pi (h)I, pi Xi .(60)(61)derivation necessary conditions Nash equilibrium consists derivingnecessary conditions policy best response reduced joint policy.following program finds policy agent best response i-reduced jointpolicy qi Xi . Constraints (63)-(64) ensure policy defines valid joint policy(see Section 3.2) objective function traduction concept best response.Variables: xi (h), I, h HiX XMaximizeR(, hh, j i)qk (jk ) xi (h)hEij Ei(62)kI\{i}subject to:Xxi (a) = 1(63)aAixi (h) +Xxi (h.o.a) = 0,h Ni , Oi(64)h Hi .(65)aAixi (h) 0,linear program (LP) must still refined solution bestresponse agent global best response, i.e., policy agent bestresponse agents. mean introducing new variables (a set variableagent). main point adapt objective function currentobjective function, applied find global best response, would lead non-linearobjective function product weights policies would appear. this,make use dual program (LP).linear program (LP) one variable xi (h) history h Hi representingweight h. one constraint per information set agent i. words,constraint linear program (LP) uniquely labeled information set. instance,constraint (63) labeled null information set , nonterminalhistory h observation o, corresponding constraint (64) labeledinformation set h.o. Thus, (LP) ni variables mi constraints.described appendix (see appendix B), dual (LP) expressed as:351fiAras & DutechVariables: yi (),Minimizeyi ()(66)subject to:yi ((h))Xyi (h.o) 0,h Ni(67)qk (jk ) 0,h Ei(68)oOiyi ((h))XR(, hh, j i)j EikI\{i}yi () (, +),(69)(h) denotes information set h belongs. dual one free variableyi () every information set agent i. function (h) (defined Section 2.3) appears mapping histories information sets1 . dual programone constraint per history agent. Thus, dual mi variables ni constraints.Note objective dual minimize yi () primal (LP),right hand side constraints, except first one, 0.theorem duality (see appendix B), applied primal (LP) (62)-(65)transformed dual (66)-(69), says solutions value. Mathematically,means that:X XR(, hh, j i)qk (jk ) xi (h) = yi ().(70)hEij EikI\{i}Thus, value joint policy hxi , qi expressed eitherX XV(, hxi , qi i) =R(, hh, j i)qk (jk ) xi (h)hEij Ei(71)kI\{i}V(, hxi , qi i) = yi ().(72)Due constraints (63) (64) primal LP, holdsXX X Xxi (a) +yi (h.o) xi (h) +xi (h.o.a)yi () = yi ()aAihNi oOi(73)aAiconstraint (63) guarantees first term braces 1 constraints (65)guarantee remaining terms inside braces 0. right hand side(73) rewrittenXXXPxi (a) yi ()oOi yi (a.o)+xi (h) yi ((h))yi (h.o)aAioOihNi \Ai+Xxi (h)yi ((h))hEi=PhNi xi (h)yi ((h))XoOiXyi (h.o) +xi (h)yi ((h))hEi1. h.o information set, yi (h.o) shortcut writing yi ((h.o)).352(74)fiMathematical Programming DEC-POMDPsSo, combining equations (70) (74), getXXxi (h)yi ((h))yi (h.o)oOihNi+Xxi (h)hEiyi ((h))R(, hh, j i)Xj EikI\{i}qk (jk ) = 0(75)time introduce supplementary variables w information set. variables, usually called slack variables, defined as:Xyi ((h))yi (h.o) = wi (h), h Ni(76)oOiyi ((h))XR(, hh, j i)j Eiqk (jk ) = wi (h),h Ei .(77)kI\{i}shown Section C appendix, slack variables correspond conceptregret defined game theory. regret history expresses loss accumulatedreward agent incurs acts according history rather accordinghistory would belong optimal joint policy.Thanks slack variables, furthermore rewrite (75) simplyXXxi (h)wi (h) +xi (h)wi (h) = 0(78)hNihEiXxi (h)wi (h) = 0.(79)hHiNow, (79) sum ni products, ni size Hi . product sumnecessarily 0 xi (h) wi (h) constrained nonnegative primaldual respectively. property strongly linked complementary slacknessoptimality criterion linear programs (see, example, Vanderbei, 2008). Hence, (79)equivalentxi (h)wi (h) = 0,h Hi .(80)Back framework DEC-POMDPs, constraints written:pi (h)i (hh, qi i) = 0,h Hi .(81)sum up, solving following mathematical program would give optimal jointpolicy DEC-POMDP. constraints (87) non-linear thus prevent ussolving program directly. linearization constraints, called complementarityconstraints, subject next section.Variables:xi (h), wi (h) h Hiyi ()Maximizey1 ()353(82)fiAras & Dutechsubject to:Xxi (a) = 1(83)aAixi (h) +XI, h Ni , Oi(84)yi (h.o) = wi (h),I, h Ni(85)xk (jk ) = wi (h),I, h Ei(86)xi (h.o.a) = 0,aAiyi ((h))XoOiyi ((h))Xj EiR(, hh, j i)kI\{i}xi (h)wi (h) = 0,I, h Hi(87)xi (h) 0,I, h Hi(88)wi (h) 0,I, h Hi(89)yi () (, +),I,(90)5.2 Dealing Complementarity Constraintssection explains non-linear constraints xi (h)wi (h) = 0 previous mathematical program turned sets linear constraints thus lead mixedinteger linear programming formulation solution DEC-POMDP.Consider complementarity constraint ab = 0 variables b. Assumelower bound values b 0. Let upper bounds values brespectively ua ub . let c 0-1 variable. Then, complementarity constraintab = 0 separated following equivalent pair linear constraints,ua c(91)b ub (1 c).(92)words, pair constraints satisfied, surely case ab = 0.easily verified. c either 0 1. c = 0, set 0constrained ua c (and less 0); c = 1, b set 0since b constrained ub (1 c) (and less 0). either case,ab = 0.consider complementarity constraint xi (h)wi (h) = 0 non-linear program (82)-(90) above. wish separate constraint pair linear constraints.recall xi (h) represents weight h wi (h) represents regret h.first requirement convert constraint pair linear constraints lowerbound values two terms 0. indeed case since xi (h) wi (h)constrained non-negative NLP. Next, require upper boundsweights histories regrets histories. shown Lemma 3.1 upperbound value xi (h) h 1. upper bounds regrets histories,require calculus.354fiMathematical Programming DEC-POMDPspolicy pi agent holdsXpi (h) = |Oi |T 1 .(93)hEiTherefore, every i-reduced joint policy hq1 , q2 , , qn Xi , holdsX|Ok |T 1qk (jk ) =j Ei kI\{i}(94)kI\{i}Since regret terminal history h agent given hq1 , q2 , , qn definedX(95)(h, q) = maxqk (jk ) R(, hh , j i) R(, hh, j i) ,h (h)j Ei kI\{i}conclude upper bound Ui (h) regret terminal history h Eiagent is,1Ui (h) =|Ok |max maxR(, hh , j i) min R(, hh, j i) . (96)kI\{i}h (h) j Eij Eilet us consider upper bounds regrets non-terminal histories. Letinformation set length agent i. Let Ei () Ei denote set terminal historiesagent first 2t elements history set identical . Let hhistory length agent i. Let Ei (h) Ei denote set terminal historiesfirst 2t - 1 elements history set identical h. Since policypi agent i, holdsXpi (h ) |Oi |T(97)h Ei (h)conclude upper bound Ui (h) regret nonterminal historyh Ni length agentmaxmaxR(,hh,ji)minminR(,hg,ji)(98)Ui (h) = Lih Ei ((h)) j EigEi (h) j EiLi = |Oi |T|Ok |T 1 .(99)kI\{i}Notice = (that is, h terminal) (98) reduces (96).So, complementarity constraint xi (h)wi (h) = 0 separated pair linearconstraints using 0-1 variable bi (h) follows,xi (h) 1 bi (h)wi Ui (h)bi (h)bi (h) {0, 1}355(100)(101)(102)fiAras & DutechVariables:xi (h), wi (h) bi (h) {1, 2} h Hiyi () {1, 2}Maximizey1 ()(103)subject to:Xxi (a) = 1(104)aAixi (h) +X= 1, 2, h Ni , Oi(105)= 1, 2, h Ni(106)R(, hh, h i)x2 (h ) = w1 (h),h E1(107)R(, hh , hi)x1 (h ) = w2 (h),h E2(108)xi (h.o.a) = 0,aAiyi ((h))Xyi (h.o) = wi (h),oOiy1 ((h))Xh E2y2 ((h))Xh E1xi (h) 1 bi (h),wi (h) Ui (h)bi (h),= 1, 2, h Hi= 1, 2, h Hi(109)(110)xi (h) 0,= 1, 2, h Hi(111)wi (h) 0,= 1, 2, h Hi(112)bi (h) {0, 1},= 1, 2, h Hiyi () (, +),(113)= 1, 2, (114)Table 4: MILP-2 agents. 0-1 mixed integer linear program, derived gametheoretic considerations, finds optimal stochastic joint policies DEC-POMDPs2 agents.5.3 Program 2 Agentscombine policy constraints (Section 3.2), constraints seenpolicy best response (Sections 5.1, 5.2) maximization valuejoint policy, derive 0-1 mixed integer linear program solutionoptimal joint policy DEC-POMDP 2 agents. Table 4 details programcall MILP-2 agents.Example formulation decentralized Tiger problem 2 agentshorizon 2 found appendices, Section E.4variables program vectors xi , wi , bi yi agent i. Noteagent history h agent i, Ui (h) denotes upper boundregret history h.356fiMathematical Programming DEC-POMDPssolution (x , , w , b ) MILP-2 agents consists following quantities: (i)optimal joint policy x = hx1 , x2 may stochastic; (ii) agent = 1,2, history h Hi , wi (h), regret h given policy xi agent;(iii) agent = 1, 2, information set , yi (), value givenpolicy xi agent; (iv) agent = 1, 2, vector bi simply tells ushistories support xi ; history h agent bi (h) = 1support xi . Note replace y1 () y2 () objective functionwithout affecting program. following result.Theorem 5.1. Given solution (x , w , , b ) MILP-2 agents, x = hx1 , x2optimal joint policy sequence-form.Proof: Due policy constraints agent, xi sequence-form policyagent i. Due constraints (106)-(108), yi contains values information setsagent given xi . Due complementarity constraints (109)-(110), xi bestresponse xi . Thus hx1 , x2 Nash equilibrium. Finally, maximizing valuenull information set agent 1, effectively maximizing value hx1 , x2 i.Thus hx1 , x2 optimal joint policy.comparison MILP presented Table 3, MILP-2 agentsconstitutes particularly effective program term computation time finding 2agent optimal -period joint policy much smaller program. numbervariables required MILP exponential n, number variables requiredMILP-2 agents exponential . represents major reduction sizelead improvement term computation time.5.4 Program 3 Agentsnumber agents 2, constraint (86) non-linear program(82)-(90) longer complementarity constraint2 variables could linQearized before. particular, term kI\{i} xk (jk ) constraint (86) involvesmany variables different agents. linearize term, restrict pure joint policies exploit combinatorial facts numberhistories involved. leads 0-1 mixed linear program called MILP-n agentsdepicted Table 5.variables program MILP-n agents vectors xi , wi , bi yiagent vector z. following result.Theorem 5.2. Given solution (x , w , , b , z ) MILP-n agents, x = hx1 , x2 ,, xn pure -period optimal joint policy sequence-form.Proof: Due policy constraints domain constraints agent,pure sequence-form policy agent i. Due constraints (118)-(119), yicontains values information sets agent given xi . Due complementarityconstraints (122)-(123), xi best response xi . Thus x Nash equilibrium.Finally, maximizing value null information set agent 1, effectivelymaximizing value x . Thus x optimal joint policy.xi357fiAras & DutechVariables:xi (h), wi (h) bi (h) h Hiyi () I,z(j) j EMaximizey1 ()(115)xi (a) = 1(116)xi (h.o.a) = 0,I, h Ni , Oi (117)subject to:XaAixi (h) +XaAiyi ((h))Xyi (h.o) = wi (h),I, h Ni(118)oOiyi ((h))X1R(, hh, ji i)z(j) = wi (h), I, h Ei1|Oi |jEXz(hh, j i) = xi (h)|Ok |T 1 ,j Ei(119)kI\{i}I, h EiXz(j) =|Oi |T 1jE(120)(121)iIxi (h) 1 bi (h),I, h Hi (122)wi (h) Ui (h)bi (h),I, h Hi (123)xi (h) 0,I, h Nixi (h) {0, 1}wi (h) 0,I, h EiI, h Hibi (h) {0, 1},h Hiyi () (, +),z(j) [0, 1],j E(124)(125)(126)(127)I, i(128)(129)Table 5: MILP-n agents. 0-1 mixed integer linear program, derived gametheoretic considerations, finds pure optimal joint policies DEC-POMDPs3 agents.358fiMathematical Programming DEC-POMDPsCompared MILP Table 3, MILP-n agents roughly sizereal valued variables 0-1 variables. precise, PMILP 0-1variable every terminal history every agent (that approximatively iI |Ai |T |Oi |T 1integer variables) MILP-n agents two 0-1 variablesevery terminal wellPnonterminal history agent (approximatively 2 iI (|Ai ||Oi |)T integer variables).5.5 Summaryformulation solution DEC-POMDP application DualityTheorem Linear Programs allow us formulate solution DEC-POMDPsolution new kind 0-1 MILP. 2 agents, MILP O(kT ) variablesconstraints thus smaller MILP previous section. Still,MILPS quite large next section investigates heuristic ways speedresolution.6. Heuristics Speeding Mathematical Programssection focusses ways speed resolution various MILPs presentedfar. Two ideas exploited. First, show prune set sequence-form policiesremoving histories provably part optimal joint policy.histories called locally extraneous. Then, give lower uppers boundsobjective function MILPs, bounds sometimes used branchbound method often used MILP solvers finalize values integer variables.6.1 Locally Extraneous Historieslocally extraneous history history required find optimal joint policyinitial state DEC-POMDP could replaced co-historywithout affecting value joint policy. co-history history h agentdefined history agent identical h aspects except lastaction. Ai = {b, c}, co-history c.u.b.v.b history c.u.b.v.c. setco-histories history h shall denoted C(h).Formally, history h Hit length agent said locally extraneous if,i-reduced joint histories length t,every probability distribution set Hiexists history h C(h)X(j ) R(, hh , j i) R(, hh, j i)0(130)j Hti(j ) denotes probability j .alternative definition follows. history h Hit length agent saidlocally extraneous exists probability distribution set co-historiesh i-reduced joint history j length t, holdsX(h )R(, hh , j i) R(, hh, j i)(131)h C(h)359fiAras & Dutech(h ) denotes probability co-history h .following theorem justifies incremental pruning locally extraneous historiessearch optimal joint policies faster performed smallerset possible support histories.Theorem 6.1. every optimal -period joint policy p agentterminal history h agent locally extraneous , pi (h) > 0, existsanother -period joint policy p optimal identical p respectsexcept pi (h) = 0.Proof: Let p -period joint policy optimal . Assumeagent terminal history h agent locally extraneous , pi (h) > 0.(130), exists least one co-history h h that,Xpi (j ) R(, hh , j i) R(, hh, j i)0.(132)j HTLet q -period policy agent identical pi respects except q(h )= pi (h) + pi (h ) q(h) = 0. shall show q also optimal . holds,Xj HTV(, hq, pi i) V(, hpi , pi i) =pi (j ) R(, hh , j i)q(h ) R(, hh , j i)pi (h ) R(, hh, j i)pi (h)=Xj HTpi (j ) R(, hh , j i)(q(h ) pi (h )) R(, hh, j i)pi (h)=Xj HTpi (j ) R(, hh , j i)pi (h) R(, hh, j i)pi (h)since q(h ) = pi (h) + pi (h ). Therefore,Xj HTV(, hq, pi i) V(, hpi , pi i) =pi (j ) R(, hh , j i) R(, hh, j i)0 (due (132)).Hence, p = hq, pi also optimal -period joint policy .One could also wonder order extraneous histories pruned importantnot. answer question, following theorem shows many co-historiesextraneous, pruned order as:either value, one pruned ;pruning one change fact others still extraneous.Theorem 6.2. two co-histories h1 h2 locally extraneous, either valuesequal h also locally extraneousR(, hh1 , j i) R(, hh2 , j i)for j Hi1relatively C(h) \ {h2 }.360fiMathematical Programming DEC-POMDPsProof: Let C + denotes union C(h1 ) C(h2 ). immediately C(h1 ) =C + \ {h1 } C(h2 ) = C + \ {h2 }. h1 (resp. h2 ) locally extraneous meansexists probability distribution 1 C(h1 ) (resp. 2 C(h2 )) that, j:HiX1 (h )R(, hh , j i) R(, hh1 , j i)(133)h C + \{h1 }X2 (h )R(, hh , j i) R(, hh2 , j i)(134)h C + \{h2 }(135)Eq. (133) expanded in:X1 (h2 )R(, hh2 , j i) +h C + \{h1 (h )R(, hh , j i) R(, hh1 , j i).1 ,h2 }Using (134) (136) givesX1 (h2 )2 (h )R(, hh , j i) +h C + \{h2 }leadingX(136)X1 (h )R(, hh , j i) R(, hh1 , j i)h C + \{h1 ,h2 }(137)(1 (h2 )2 (h ) + 1 (h ))R(, hh , j i) (1 1 (h2 )2 (h1 ))R(, hh1 , j i) (138)h C + \{h1 ,h2 }So, two cases possible:1 (h2 ) = 2 (h1 ) = 1. case, R(, hh2 , j i) R(, hh1 , j i) R(, hh1 , j i).R(, hh2 , j i), R(, hh1 , j i) = R(, hh2 , j i) j Hi1 (h2 )2 (h1 ) < 1. case have:Xh C + \{h1 ,h2 }1 (h2 )2 (h ) + 1 (h )R(, hh , j i) R(, hh1 , j i)1 1 (h2 )2 (h1 )(139)meaning even without using h2 , h1 still locally extraneous1 (h2 )2 (h )+1 (h )probability distribution C + \ {h1 , h2 }11 (h2 )2 (h1 )Xh C + \{h1 ,h2 }1 (h2 )(1 2 (h1 )) + (1 1 (h2 ))1 (h2 )2 (h ) + 1 (h )=1 1 (h2 )2 (h1 )1 1 (h2 )2 (h1 )1 1 (h2 )2 (h1 )1 1 (h2 )2 (h1 )= 1.=(140)(141)(142)361fiAras & Dutechorder prune locally extraneous histories, one must able identify histories.indeed two complementary ways this.first method relies definition value history (see Section 3.3),R(, hh, j i) = (, hh, j i)R(, hh, j i).(143)Therefore,(, hh, j i) = 0,j Hi(144)true history h, means every joint history length occurringgiven history part priori probability 0. thus, h clearlyextraneous. Besides, every co-history h also locally extraneous shareprobabilities.second test needed locally extraneous histories verify (144).again, turn linear programing particular following linear programVariables: y(j), j HiMinimize(145)subject to:Xj Htiy(j ) R(, hh , j i) R(, hh, j i),Xh C(h)y(j ) = 1(146)(147)j Htifollowing Lemma.y(j ) 0,j Hi(148)Lemma 6.1. If, exists solution ( , ) linear program (145)-(148) 0,h locally extraneous.Proof : Let ( , ) solution LP (145)-(148). probability distributiondue constraints (147)-(148). 0, since minimizing , dueHi), every co-history h hconstraints (146), every (HiXy(j ) R(, hh , j i) R(, hh, j i).(149)j HtiTherefore, definition, h locally extraneous.following procedure identifies locally extraneous terminal histories agentsproceed iterative pruning. mainly motivated Theorems 6.1 6.2effectively removing extraneous histories. procedure similar procedureiterated elimination dominated strategies game (Osborne & Rubinstein, 1994).concept also quite similar process policy elimination backward stepdynamic programming partially observable stochastic games (Hansen et al., 2004).362fiMathematical Programming DEC-POMDPsStep 1: agent I, set HiT Ei . Let H denote set iI HiT .joint history j H , compute store value R(, j) j jointobservation sequence probability (, j) j.Step 2: agent I, history h HiT , i-reduced joint, (, hh, j i) = 0, remove h H .history j HiStep 3: agent I, history h HiT follows: C(h) HiTnon-empty, check whether h locally extraneous setting solvingset H set C(h)LP (145)-(148). setting LP, replace Hiset C(h) HiT . upon solving LP, h found locally extraneous, remove h HiT .Step 4: Step 3 history (of agent) found locally extraneous, goStep 3. Otherwise, terminate procedure.procedure builds set HiT agent i. set contains every terminalhistory agent required finding optimal joint policy , everyterminal history locally extraneous . agent i, every historyHiT HiT locally extraneous. reason reiterating Step 3history h agent found locally extraneous consequently removedHiT , possible history agent previously locallyextraneous becomes so, due removal h HiT . Hence, order verifycase history not, reiterate Step 3.Besides, Step 2 procedure also prunes histories impossible givenmodel DEC-POMDP observation sequence observed.last pruning step taken order remove non-terminal historieslead extraneous terminal histories. last step recursive, starting historieshorizon 1, remove histories hi non-extraneous terminal histories,is, histories hi h.o.a extraneous Ai Oi .Complexity algorithm pruning locally extraneous histories exponentialcomplexity. joint history must examined compute value occurenceprobability. Then, worst case, Linear Program run every local historyorder check extraneous not. Experimentations needed see prunningreally interesting.6.2 Cutting PlanesPrevious heuristics aimed reducing search space linear programs,incidentally good impact time needed solve programs. Another optiondirectly aims reducing computation time use cutting planes (Cornuejols,2008). cut (Dantzig, 1960) special constraint identifies portion setfeasible solutions optimal solution provably lie. Cuts usedconjunction various branch bounds mechanism reduce number possiblescombination integer variables examined solver.present two kinds cuts.363fiAras & DutechVariables: y(j), j HMaximizeXR(, j)y(j)(153)jEsubject to,Xy(a) = 1(154)aAy(j) +Xy(j.o.a) = 0,j N ,(155)j H(156)aAy(j) 0,Table 6: POMDP. linear program finds optimal policy POMDP.6.2.1 Upper Bound Objective Functionfirst cut propose upper bound POMDP cut. value optimal-period joint policy given DEC-POMDP bounded valueVP optimal -period policy POMDP derived DEC-POMDP.derived POMDP DEC-POMDP assuming centralized controller (i.e.one agent using joint-actions).sequence-form representation POMDP quite straightforward. Calling Hset Tt=1 Ht joint histories lengths less equal N set H\E nonterminal joint histories, policy POMDP horizon sequence-form functionq H [0, 1] that:Xq(a) = 1(150)aAq(j) +Xq(j.o.a) = 0,j N ,(151)aAvalue VP (, q) sequence-form policy q given by:XVP (, q) =R(, j)q(j)(152)jEThereby, solution linear program Table 6 Poptimal policyPOMDP horizon optimal value POMDP jE R(, j)y (j). So,value V(, p ) optimal joint policy p = hp1 , p2 , , pn DEC-POMDPbounded value VP (, q ) associated POMDP.Complexity complexity finding upper bound linked complexitysolving POMDP which, showed Papadimitriou Tsitsiklis (1987), PSPACE(i.e. require memory polynomial size problem, leading possibleexponential complexity time). again, experimentation help us decidecases upper bound cut efficient.364fiMathematical Programming DEC-POMDPs6.2.2 Lower Bound Objective Functioncase DEC-POMDPs non-negative reward, trivial show value-period optimal policy bounded value 1 horizon optimalvalue. So, general case, take account lowest reward possiblecompute lower bound say that:XR(, j)z(j) V 1 () + min min R(s, a)(157)aA sSjEV 1 value optimal policy horizon 1. reasoning leadsiterated computation DEC-POMDPs longer longer horizon, reminiscentMAA* algorithm (Szer et al., 2005). Experiments tell worthwhile solve biggerbigger DEC-POMDPs take advantage lower bound better directlytackle horizon problem without using lower bound.Complexity compute lower bound, one required solve DEC-POMDP whithhorizon one step shorter current horizon. complexity clearlyleast exponential. experiments, value DEC-POMDP usedDEC-POMDP bigger horizon. case, computation timeaugmented best time solve smaller DEC-POMDP.6.3 SummaryPruning locally extraneous histories using bounds objective functionpractical use software solving MILPs presented paper. Pruning historiesmeans space policies used MILP reduced and, formulationMILP depends combinatorial characteristics DEC-POMDP, MILPmust altered show Appendix D.Validity far cuts concerned, alter solution found MILPs,solution MILPs still optimal solution DEC-POMDP. extraneous histories pruned, least one valid policy left solution because, step3 algorithm, history pruned co-histories left. Besides,reduced set histories still used build optimal policy Theroem 6.1.consequence, MILP build reduced set histories admit solutionsolution one optimal joint policy.next section, experimental results allow us understand casesheuristics introduced useful.7. Experimentsmathematical programs heuristics designed paper tested fourclassical problems found literature. problems, involving two agents,mainly compared computation time required solve DEC-POMDP using MixedInteger Linear Programming methods computation time reported methods foundliterature. tested programs three-agent problems randomlydesigned.365fiAras & DutechProblemMABCMA-TigerFire FightingGrid MeetingRandom Pbs|Ai |23352|Oi |22222|S|42271650n22223Table 7: Complexity various problems used test beds.MILP MILP-2 solved using iLog Cplex 10 solver commercial setJava packages relies combination Simplex Branch Boundsmethods (Fletcher, 1987). software run Intel P4 3.4 GHz 2GbRAM using default configuration parameters. mathematical programs, differentcombination heuristics evaluated: pruning locally extraneous histories, usinglower bound cut using upper bound cut, respectively denoted LOC, Lowresult tables come.Non-Linear Program (NLP) Section 3.4 evaluated using various solvers NEOS website (http://www-neos.mcs.anl.gov ), even thoughtmethod guarantee optimal solution DEC-POMDP. Three solversused: LANCELOT (abbreviated LANC.), LOQO SNOPT.result tables also report results found literature following algorithms:DP stands Dynamic Programming Hansen et al. (2004); DP-LPC improvedversion Dynamic Programming policies compressed order fitmemory speed evaluation proposed Boularias Chaib-draa (2008);PBDP extension Dynamic Programming pruning guided knowledgereachable belief-states detailed work Szer Charpillet (2006); MAA*heuristically guided forward search proposed Szer et al. (2005) generalizedimproved version algorithm called GMAA* developed Oliehoek et al. (2008).problems selected evaluate algorithms detailed coming subsections.widely used evaluate DEC-POMDPs algorithms literaturecomplexity, term space size, summarized Table 7.7.1 Multi-Access Broadcast Channel ProblemSeveral versions Multi-Access Broadcast Channel (MABC) problem foundliterature. use description given Hansen et al. (2004) allowsproblem formalized DEC-POMDP.MABC, given two nodes (computers) required send messagescommon channel given duration time. Time imaginedsplit discrete periods. node buffer capacity one message.buffer empty period refilled certain probability next period.period, one node send message. nodes send messageperiod, collision messages occurs neither message transmitted. casecollision, node intimated collision signal. collision366fiMathematical Programming DEC-POMDPssignaling mechanism faulty. case collision, certain probability,send signal either one nodes.interested pre-allocating channel amongst two nodes given numberperiods. pre-allocation consists giving channel one nodes periodfunction nodes information period. nodes information periodconsists sequence collision signals received till period.modeling problem DEC-POMDP, obtain 2-agent, 4-state, 2-actionsper-agent, 2-observations-per-agent DEC-POMDP whose components follows.node agent.state problem described states buffers two nodes.state buffer either Empty Full. Hence, problem four states:(Empty, Empty), (Empty, Full), (Full, Empty) (Full, Full).node two possible actions, Use Channel Dont Use Channel.period, node may either receive collision signal may not. nodetwo possible observations, Collision Collision.initial state problem (Full, Full). state transition function P,joint observation function G reward function R taken Hansen et al.(2004). agents full buffers period, use channel period,state problem unchanged next period; agents full buffersnext period. agent full buffer period uses channelperiod, buffer refilled certain probability next period.agent 1, probability 0.9 agent 2, probability 0.1. agentsempty buffers period, irrespective actions take period, buffersget refilled probabilities 0.9 (for agent 1) 0.1 (for agent 2).observation function G follows. state period (Full, Full)joint action taken agents previous period (Use Channel, Use Channel),probability receive collision signal 0.81, probability onereceives collision signal 0.09 probability neither receivescollision signal 0.01. state problem may periodjoint action agents may taken previous period, agents receivecollision signal.reward function R quite simple. state period (Full, Empty)joint action taken (Use Channel, Dont Use Channel) state period(Empty, Full) joint action taken (Dont Use Channel, Use Channel), reward1; combination state joint action, reward 0.evaluated various algorithms problem three different horizons (3,4 5) respective optimal policies value 2.99, 3.89 4.79. Resultsdetailed Table 8 where, horizon algorithm, value computationtime best policy found given.results show MILP compares favorably classical algorithms exceptGMAA* always far better horizon 4 and, horizon 5, roughly within367fiAras & DutechResolution methodProgramSolverHeuristicsMILPCplexMILPCplexLowMILPCplexMILPCplexLOCMILPCplexLOC, LowMILPCplexLOC,MILP-2CplexNLPSNOPTNLPLANC.NLPLOQOAlgorithmFamilyDPDyn. Prog.DP-LPCDyn. Prog.PBDPDyn. Prog.MAA*Fw. SearchGMAA*Fw. SearchHorizon 3ValueTime2.990.862.99 0.10 / 0.932.99 0.28 / 1.032.99 0.34 / 0.842.99 0.44 / 0.842.99 0.62 / 0.932.990.392.900.012.990.022.900.01ValueTime2.9952.990.362.99< 1s2.99< 1s??Horizon 4ValueTime3.899003.890.39 / 9003.890.56 / 9073.891.05 / 803.891.44 / 1203.89 1.61 / 10.23.893.533.170.013.790.953.790.05ValueTime3.8917.593.894.593.8923.8954003.890.03Horizon 5ValueTime-m3.5 / -m4.73 / -m2.27 / -t5.77 / -t4.797.00 / 25-m4.700.214.69204.690.18ValueTime-m-m4.79105-t4.795.68Table 8: MABC Problem. Value computation time (in seconds) solutionproblem computed several methods, best results highlighted.appropriate, time shows first time used run heuristics globaltime, format heuristic/total time. -t means timeout 10,000s;-m indicates problem fit memory ? indicatesalgorithm tested problem.order magnitude MILP pertinent heuristics. expected, apartsimplest setting (horizon 3), NLP based resolution find optimal policyDEC-POMDP, computation time lower methods. AmongMILP methods, MILP-2 better MILP even best heuristics horizon 34. size problem increases, heuristics way MILPsable cope size problem. table also shows that, MABCproblem, pruning extraneous histories using LOC heuristic always good methodinvestigation revealed 62% heuristics proved locally extraneous.far cutting bounds concerned, dont seem useful first (for horizon3 4) necessary MILP find solution horizon 5. problem,one must also mind one optimal policy horizon.7.2 Multi-Agent Tiger Problemexplained section 2.2, Multi-Agent Tiger problem (MA-Tiger) introducedpaper Nair et al. (2003). general description problem, ob368fiMathematical Programming DEC-POMDPsJoint Action(Listen, Listen)(Listen, Listen)(Listen, Listen)(Listen, Listen)(Listen, Listen)(Listen, Listen)(Listen, Listen)(Listen, Listen)(*, *)StateLeftLeftLeftLeftRightRightRightRight*Joint Observation(Noise Left, Noise Left)(Noise Left, Noise Right)(Noise Right, Noise Left)(Noise Right, Noise Right)(Noise Left, Noise Left)(Noise Left, Noise Right)(Noise Right, Noise Left)(Noise Right, Noise Right)(*, *)Probability0.72250.12750.12750.02250.02250.12750.12750.72250.25Table 9: Joint Observation Function G MA-Tiger Problem.tain 2-agent, 2-state, 3-actions-per-agent, 2-observations-per agent DEC-POMDP whoseelements follows.person agent. So, 2-agent DEC-POMDP.state problem described location tiger. Thus, consiststwo states Left (tiger behind left door) Right (tiger behind rightdoor).agents set actions consists three actions: Open Left (open left door),Open Right (open right door) Listen (listen).agents set observations consists two observations: Noise Left (noise comingleft door) Noise Right (noise coming right door).initial state equi-probability distribution S. state transition function P,joint observation function G reward function R taken paper Nairet al. (2003). P quite simple. one agents opens door period, stateproblem next period set back . agents listen period, stateprocess unchanged next period. G, given Table (9), also quite simple.Nair et al. (2003) describes two reward functions called B problem,report results reward function A, given Table 10, behavioralgorithm similar reward functions. optimal value problemhorizons 3 4 respectively 5.19 4.80.horizon 3, dynamic programming forward search methods generally bettermathematical programs. contrary horizon 4 computation time MILP Low heuristic significatively better other, evenGMAA*. Unlike MABC, pruning extraneous histories improve methodsbased MILP, quite understandable deeper investigations showedextraneous histories. Using lower cutting bounds proves efficientseen kind heuristic search best policy ; directly set policies (like369fiAras & DutechJoint Action(Open Right, Open Right)(Open Left, Open Left)(Open Right, Open Left)(Open Left, Open Right)(Listen, Listen)(Listen, Open Right)(Open Right, Listen)(Listen, Open Left)(Open Left, Listen)Left20-50-100-100-299-101-101Right-5020-100-100-2-101-10199Table 10: Reward Function MA-Tiger Problem.GMAA*) set combination histories, may explain good behaviorMILP+Low.must also noted problem, approximate methods like NLP alsoalgorithms depicted like Memory Bound Dynamic ProgrammingSeuken Zilberstein (2007) able find optimal solution. And, again,methods based NLP quite fast sometimes accurate.7.3 Fire Fighters Problemproblem Fire Fighters (FF) introduced new benchmark Oliehoeket al. (2008). models team n fire fighters extinguish fires row nhhouses.state house given integer parameter, called fire level f ,takes discrete value 0 (no fire) nf (fire maximum severity). every timestep, agent move one house. two agents house,extinguish existing fire house. agent alone, fire level lowered0.6 probability neighbor house also burning 1 probability otherwise.burning house fireman present increase fire level f one point 0.8probability neighbor house also burning probability 0.4 otherwise.unattended non-burning house catch fire probability 0.8 neighbor houseburning. action, agents receive reward f house stillburning. agent observe flames location probabilitydepends fire level: 0.2 f = 0, 0.5 f = 1 0.8 otherwise. start,agents outside houses fire level houses sampleduniform distribution.model following characteristics:na agents, nh actions nf possible informations.h 1states nnf h possible states burning housesnnf h . na +nnh 1different ways distribute na fire fighters houses.na +nnaexample, 2 agents 3 houses 3 levels fire lead 9 6 = 54 states. But,370fiMathematical Programming DEC-POMDPsResolution methodProgramSolverHeuristicsMILPCplexMILPCplexLowMILPCplexMILPCplexLOCMILPCplexLOC, LowMILPCplexLOC,MILP-2CplexNLPSNOPTNLPLANC.NLPLOQOAlgorithmFamilyDPDyn. Prog.DP-LPCDyn. Prog.PBDPDyn. Prog.MAA*Fw. SearchGMAA*Fw. SearchHorizon 3ValueTime5.193.175.19 0.46 / 4.95.19 0.42 / 3.55.19 1.41 / 6.45.19 1.88 / 7.65.19 1.83 / 6.25.1911.16-450.035.190.475.190.01ValueTime5.192.295.191.79??5.190.025.190.04Horizon 4ValueTime-t4.803.5 / 720.75 / -t16.0 / -t4.80 19.5 / 17516.75 / -t-t-9.804.624.805144.7891ValueTime-m4.80534??4.8059614.803208Table 11: MA-Tiger Problem. Value computation time (in seconds) solutionproblem computed several methods, best results highlighted.appropriate, time shows first time used run heuristicsglobal time, format heuristic/total time.-t means timeout10.000s; -m indicates problem fit memory ? indicates algorithm tested problem.371fiAras & DutechResolution methodProgramSolverHeuristicsMILPCplexMILP-2CplexNLPSNOPTNLPLANC.NLPLOQOAlgorithmFamilyMAA*Fw. SearchGMAA*Fw. SearchHorizon 3ValueTime-t-5.9838-5.980.05-5.982.49-6.080.24ValueTime(-5.73) 0.29(-5.73)0.41Horizon 4ValueTime-t-t-7.084.61-7.131637-7.1483ValueTime(-6.57) 5737(-6.57) 5510Table 12: Fire Fighting Problem. Value computation time (in seconds) solution problem computed several methods, best results highlighted.-t means timeout 10.000s. MAA* GMAA*, value parenthesistaken work Oliehoek et al. (2008) optimaldifferent optimal values.possible use information joint action reduce number stateneeded transition function simply nnf h , meaning 27 states 2 agents3 houses 3 levels fire.Transition, observation reward functions easily derived description.problem, dynamic programming based methods tested problemformulation quite new. horizon 3, value optimal policy given Oliehoeket al. (2008) (5.73) differs value found MILP algorithms (5.98) whereasmethods supposed exact. might come slight differencesrespective formulation problems. horizon 4, Oliehoek et al. (2008) reportoptimal value (6.57).problem, MILP methods clearly outperformed MAA* GMAA*.NLP methods, give optimal solution horizon 3, better termcomputation time. might NLP also able find optimal policies horizon 4setting differs work Oliehoek et al. (2008), able checkpolicy found really optimal. main reason superiority forwardsearch method lies fact problem admits many many optimal policiesvalue. fact, horizon 4, MILP-based methods find optimal policy quitequickly (around 82s MILP-2) then, using branch-and-bound, must evaluatepotential policies knowing indeed found optimal policy. Forwardsearch methods stop nearly soon hit one optimal solution.Heuristics reported as, improve performance MILPtake away computation time thus results worse.372fiMathematical Programming DEC-POMDPs7.4 Meeting Gridproblem called Meeting grid deals two agents want meet staytogether grid world. introduced work Bernstein, Hansen,Zilberstein (2005).problem, two robots navigating two-by-two grid worldobstacles. robot sense whether walls left right,goal robots spend much time possible square. actionsmove up, down, left right, stay square. robot attemptsmove open square, goes intended direction probability 0.6,otherwise randomly either goes another direction stays square.move wall results staying square. robots interferecannot sense other. reward 1 agents share square,0 otherwise. initial state distribution deterministic, placing robotsupper left corner grid.problem modelled DEC-POMDP where:2 agents, one 5 actions observations (wall left, wallright).16 states, since robot 4 squares time.Transition, observation reward functions easily derived description.problem, dynamic programming based methods tested problemformulation quite new. problem intrinsically complex FFsolved horizon 2 3. Again, optimal value found method differvalue reported Oliehoek et al. (2008). Whereas found optimal values1.12 1.87 horizon 2 3, report optimal values 0.91 1.55.Results problem roughly pattern results FFproblem. MAA* GMAA* quicker MILP, time MILP able findoptimal solution horizon 3. NLP methods give quite good results slowerGMAA*. FF, numerous optimal policies MILP methodsable detect policy found quickly indeed optimal.Again, heuristics reported as, improve performanceMILP take away computation time thus results worse.7.5 Random 3-Agent Problemstest approach problems 3 agents, used randomly generated DECPOMDPs state transition function, joint observation function rewardfunctions randomly generated. DEC-POMDPs 2 actions 2 observationsper agent 50 states. Rewards randomly generated integers range 1 5.complexity family problem quite similar complexity MABCproblem (see Section 7.1).373fiAras & DutechResolution methodProgramSolverHeuristicsMILPCplexMILP-2CplexNLPSNOPTNLPLANC.NLPLOQOAlgorithmFamilyMAA*Fw. SearchGMAA*Fw. SearchHorizon 2Value Time1.120.651.120.610.910.011.120.061.120.07Value Time(0.91)0s(0.91)0sHorizon 3Value Time1.871624-t1.261.051.872570.4881Value Time(1.55)10.8(1.55) 5.81Table 13: Meeting Grid Problem. Value computation time (in seconds)solution problem computed several methods, best resultshighlighted. -t means timeout 10.000s. MAA* GMAA*, valueparenthesis taken work Oliehoek et al. (2008)optimal different optimal values...ProgramMILPMILP-2Least Time (secs)2.456.85Time (secs)455356Average120.686.88Std. Deviation183.48111.56Table 14: Times taken MILP MILP-2 2-agent Random Problem horizon 4.order assess real complexity Random problem, first testedtwo-agent version problem horizon 4. Results averaged 10 runsprograms given Table 14. compared MABC problem seemedcomparable complexity, Random problem proves easier solve (120s vs 900s).problem, number 0-1 variable relatively small, weight muchresolution time MILP-2 thus faster.Results three-agent problem horizon 3 given Table 15,averaged 10 runs. Even though size search space smaller case(for 3 agents horizon 3, 9 1021 policies whereas problem 2agents horizon 4, 1.5 1051 possible policies), 3 agent problems seemsdifficult solve, demonstrating one big issue policy coordination. Here,heuristics bring significative improvement resolution time MILP. predicted,MILP-n efficient given completeness.374fiMathematical Programming DEC-POMDPsProgramMILPMILP-LowMILP-nLeast Time (secs)2126754Time (secs)173902013Average70.653.21173Std. Deviation64.0224.2715Table 15: Times taken MILP MILP-n 3-agent Random problem horizon 3.8. Discussionorganized discussion two parts. first part, analyze resultsoffer explanations behavior algorithms usefulness heuristics. Then,second part, explicitely address important questions.8.1 Analysis Resultsresults, appears MILP methods better alternative Dynamic Programming methods solving DEC-POMDPs globally generally clearly outperformed forward search methods. structure thus characteristicsproblem big influence efficiency MILP methods. Whereas seemsbehavior GMAA* terms computation time quite correlatedcomplexity problem (size action observation spaces), MILP methods seemsometimes less correlated complexity. case MABC problem (manyextraneous histories pruned) MA-Tiger problem (special structure)outperform GMAA*. contrary, many optimal policies exists, forwardsearch methods like GMAA* clearly better choice. Finally, Non-Linear Programs,even though guarantee optimal solution, generally good alternativesometimes able find good solution computation time oftenbetter GMAA*. might prove useful approximate heuristic-driven forwardsearches.computational record two 2-agent programs shows MILP-2 agentsslower MILP horizon grows. two reasons sluggishnessMILP-2 agents may attributed. time taken branch bound (BB)method solve 0-1 MILP inversely proportional number 0-1 variablesMILP. MILP-2 agents many 0-1 variables MILP event houghtotal number variables exponentially less MILP. first reason.Secondly, MILP-2 agents complicated program MILP; manyconstraints MILP. MILP simple program, concerned finding subsetgiven set. addition finding weights histories, MILP also finds weightsterminal joint histories. extra superfluous quantity forced find.hand, MILP-2 agents takes much circuitous route, finding manysuperfluous quantities MILP. addition weights histories, MILP-2 agentsalso finds supports policies, regrets histories values information sets. Thus,375fiAras & DutechProblemHeuristicMABCLOCLowLOCLowLOCMA-TigerMeetingHorizon 2Time #pruned0.410/181.3615/50*Horizon 3Time #pruned0.3414/320.100.281.410/1080.460.4274.721 191/500*Horizon 4Time #pruned1.04774/1280.393.8916.00/6483.50.75Horizon 5Time #pruned2.27350/5123.54.73Table 16: Computation time heuristics. LOC heuristics, give computation time seconds number locally extraneous histories prunedtotal number histories (for agent). * denotes cases oneadditional history prunned second agent. Low heuristic,computation time given.relaxation MILP-2 agents takes longer solve relaxation MILP.second reason slowness BB method solves MILP-2 agents.bigger problems, namely Fire-Fighters Meeting Grid, horizonstays small, MILP-2 agents compete MILP slightly lower size.complexity grows like O((|Ai ||Oi |)T ) whereas grows like O((|Ai ||Oi |)2T ) MILP.small difference hold long number integer variables quickly lessensefficiency MILP-2 agents.far heuristic concerned, proved invaluable problems (MABCMA-Tiger) useless others. case MABC, heuristics helpfulprune large number extraneous heuristics ultimately, combinationupper bound cut efficient horizon grows. caseMA-Tiger, although extraneous histories found, using lower bound cut heuristicMILP leads quickest algorithm solving problem horizon 4.problems, heuristics burden greedy computationtime speed resolution. example, Grid Meeting problem, timetaken prune extraneous histories bigger time saved solving problem.result, added value using heuristics depends nature problem(as depicted Table 16) but, right now, able predict usefulness withouttrying them.also emphasize results given lie limit possible solveexact manner given memory computer used resolution, especiallyterms horizon. Furthermore, number agent increases, lengthhorizon must decreased problems still solvable.376fiMathematical Programming DEC-POMDPs8.2 Questionsmathematical programing approach presented paper raises different questions.explicitly addressed questions appears important us.Q1: sequence-form approach entirely doomed exponentialcomplexity?number sequence-form joint policies grows doubly exponentially horizon number agents, sequence-form approach seems doomed, even compareddynamic programming doubly exponential worst cases only. But, indeed,arguments must taken consideration.exponential number individual histories need evaluated. jointpart sequence-form left MILP solver. every computation doneparticular history, like computing value checking extraneous, greaterreusability computations done entire policies. history shared manyjoint policies individual policy. way, sequence-form allows us workreusable part policies without work directly world distributionsset joint-policies.Then, MILPs derived sequence-form DEC-POMDPs need memory sizegrows exponentially horizon number agents. Obviously,complexity quickly overwhelming also case every exact methodfar. shown experiments, MILP approach derived sequence-formcompares quite well dynamic programming, even outperformed forward methodslike GMAA*.Q2: MILP sometimes take little time find optimal jointpolicy compared existing algorithms?Despite complexity MILP approach, three factors contribute relativeefficiency MILP.1. First, efficiency linear programming tools themselves. solving MILP,BB method solves sequence linear programs using simplex algorithm.LPs relaxation MILP. theory, simplex algorithm requiresworst case exponential number steps (in size LP) solving LP2 ,well known that, practice, usually solves LP polynomial numbersteps (in size LP). Since size relaxation MILP exponentialhorizon, means that, roughly speaking, time taken solve relaxationMILP exponential horizon whereas doubly exponentialmethods.2. second factor sparsity matrix coefficients constraintsMILP. sparsity matrix formed coefficients constraints2. statement must qualified: worst case time requirement demonstratedvariants simplex algorithm. demonstrated basic version simplexalgorithm.377fiAras & DutechLP determines practice rate pivoting algorithmsimplex solves LP (this also applies Lemkes algorithm contextLCP). sparser matrix, lesser time required perform elementarypivoting (row) operations involved simplex algorithm lesser spacerequired model LP.3. third factor fact supplement MILP cuts; computationalexperience clearly shows speeds computations. first twofactors related solving relaxation MILP (i.e., LP), third factorimpact BB method itself. upper bound cut identifies additionalterminating condition BB method, thus enabling terminate earlierabsence condition. lower bound cut attempts shorten listactive subproblems (LPs) BB method solves sequentially. Duecut, BB method potentially lesser number LPs solve. Noteinserting lower bound cut, emulating forward search propertiesA* algorithm.Q3: know MILP-solver (iLogs Cplex experiments)reason speedup?Clearly, approach would slower, even sometime slower classical dynamicprogramming approach used another program solving MILPs experimented also MILPs solvers NEOS website indeedslow. true Cplex, solver used experiments, quite optimized.Nevertheless, exactly one points wanted experiment paper:one advantages formulating DEC-POMDP MILP possibility usefact that, mixed integer linear programs important industrial world,optimized solvers exist.Then, formulate DEC-POMDP MILP mostly paperabout.Q4: main contribution paper?stated earlier paper, current algorithms DEC-POMDPs largely inspired POMDPs algorithms. main contribution pursue entirely differentapproach, i.e., mixed integer linear programming. such, learned lotDEC-POMDPs pro & con mathematical programming approach.lead formulation new algorithms.designing algorithms, have, first all, drawn attention new representation policy, namely sequence form policy, introduced Koller, Megiddovon Stengel. sequence form policy compact representationpolicy agent, afford compact representation set policiesagent.algorithms proposed finite horizon DEC-POMDPs mathematicalprogramming algorithms. precise, 0-1 MILPs. MDP domain,378fiMathematical Programming DEC-POMDPsmathematical programming long used solving infinite horizon case.instance, infinite horizon MDP solved linear program (dEpenoux, 1963).recently, mathematical programming directed infinite horizon POMDPsDEC-POMDPs. Thus, infinite horizon DEC-MDP (with state transition independence) solved 0-1 MILP (Petrik & Zilberstein, 2007) infinite horizonPOMDP DEC-POMDP solved (for local optima) nonlinear program (Amato, Bernstein, & Zilberstein, 2007b, 2007a). finite horizon case much differentcharacter infinite horizon case dealt using dynamic programming.stated earlier, whereas dynamic programming quite successful finite horizonMDPs POMDPs, less finite horizon DEC-POMDPs.contrast, game theory, mathematical programming successfully directedgames finite horizon. Lemkes algorithm (1965) two-player normal form games,Govindan-Wilson algorithm (2001) n-player normal form games Koller, Megiddovon Stengel approach (which internally uses Lemkes algorithm) two-player extensiveform games finite-horizon games.remained find way appropriate mathematical programmingsolving finite horizon case POMDP/DEC-POMDP domain. work doneprecisely (incidently, algorithm solving kind n-player normal form games). Throughout paper, shown mathematical programming(in particular, 0-1 integer programming) applied solving finite horizon DECPOMDPs (it easy see approach presented yields linear programsolving finite horizon POMDP). Additionally, computational experienceapproach indicates finite horizon DEC-POMDPs, mathematical programming maybetter (faster) dynamic programming. also shown well-entrencheddynamic programming heuristic pruning redundant extraneous objects (incase, histories) integrated mathematical programming approach.Hence, main contribution paper presents, first time, alternative approach solving finite horizon POMDPs/DEC-POMDPs based MILPs.Q5: mathematical programming approach presented paper something dead end?question bit controversial short answer question couldsmall yes. true every approach looks exact optimal solutionsDEC-POMDPs, whether grounded dynamic programming forward searchmathematical programming. complexity problem, exact solutionalways untractable algorithms still improved.longer answer mitigated, especially light recent advances madedynamic programming forward search algorithms. One crucial point sequenceform DEC-POMDPs pruning extraneous histories. recent work Oliehoek,Whiteson, Spaan (2009) shown clusters histories equivalentway could also reduce nomber constraints MILPs. approach Amato,Dibangoye, Zilberstein (2009) improves speed dynamic programmingoperator could help finding extraneous histories. So, least, work379fiAras & Dutechstill required stating every aspect sequence-form DEC-POMDPsstudied.turn even longer answer. Consider long horizon case. Given exactalgorithms (including ones presented paper) tackle horizons less 6,long horizon, mean anything upwards 6 time periods. long horizon case,required conceive possibly sub-optimal joint policy given horizondetermine upper bound loss value incurred using joint policy insteadusing optimal joint policy.current trend long horizon case memory-bounded approach. memorybounded dynamic programming (MBDP) algorithm (Seuken & Zilberstein, 2007)main exponent approach. algorithm based backward induction DPalgorithm (Hansen et al., 2004). algorithm attempts run limited amountspace. order so, unlike DP algorithm, prunes even non-extraneous (i.e., nondominated) policy trees iteration. Thus, iteration, algorithm retainspre-determined number trees. algorithm variants used findjoint policy MABC, MA-tiger Box pushing problems longhorizons (of order thousands time periods).MBDP provide upper bound loss value. bounded DP (BDP)algorithm presented paper Amato, Carlin, Zilberstein (2007c) giveupper bound. However, interesting DEC-POMDP problems (such MA-tiger),MBDP finds much better joint policy BDP.meaningful way introduce notion memory boundedness approachfix priori upper bound size concerned mathematical program.presents sorts difficulties main difficulty seems need representpolicy long horizon limited space. MBDP algorithm solves problemusing may termed recursive representation. recursive representationcauses MBDP algorithm take long time evaluate joint policy, allowalgorithm represent long horizon joint policy limited space. contextmathematical programming approach, would change policy constraintsway long horizon policy represented system consisting limitednumber linear equations linear inequalities. Besides policy constraints,constraints presented programs would also accordingly transfigured.evident (to us) transfiguration constraints possible.hand, infinite horizon case seems promising candidate adaptapproach to. Mathematical programming already applied, success,solving infinite horizon DEC-POMDPs (Amato et al., 2007a). computational experience mathematical programming approach shows better (finds higherquality solutions lesser time) dynamic programming approach (Bernstein et al.,2005; Szer & Charpillet, 2006).Nevertheless, approach two inter-related shortcomings. First, approachfinds joint controller (i.e., infinite horizon joint policy) fixed sizeoptimal size. Second, much graver first, fixed size, finds locally optimaljoint controller. approach guarantee finding optimal joint controller.program presented work Amato et al. (2007a) (non-convex)380fiMathematical Programming DEC-POMDPsnonlinear program (NLP). NLP finds fixed size joint controller canonical form(i.e., form finite state machine). believe shortcomingsremoved conceiving mathematical program (specifically, 0-1 mixed integer linearprogram) finds joint controller sequence-form. stated earlier, mainchallenge regard therefore identification sequence-form infinitehorizon policy. fact, may sequence-form characterization infinitehorizon policy obtained, could used conceiving program long horizon(undiscounted reward) case well.Q6: help achieve designing artificial autonomous agents ?first sight, work direct immediate applied benefitspurpose building artificial intelligent agents understanding intelligence works.Even limited field multi-agent planning, contributions theoreticallevel practical one.Real artificial multi-agent systems indeed modeled DEC-POMDPs, evenmake use communication, common knowledge, common social law. Then, realsystems would likely made large number states, actions observations requiresolutions large horizon. mathematical programming approach practicallyuseless setting limited DEC-POMDPs small size. modelssimpler far trivial solve explicitly take accountcharacteristics real systems exist. works take advantage communications(Xuan, Lesser, & Zilberstein, 2000; Ghavamzadeh & Mahadevan, 2004), existingindependencies system (Wu & Durfee, 2006; Becker, Zilberstein, Lesser, & Goldman,2004), focus interaction agents (Thomas, Bourjot, & Chevrier, 2004),some, said answering previous questions, rely approximate solutions, etc...intention facilitate re-use adaptation modelsconcepts used work knowledge structure optimal solutionDEC-POMDP. end, decided describe MILP programs also,importantly, derived programs making use propertiesoptimal DEC-POMDP solutions.Truly autonomous agents also require adapt new unforeseen situations.work dedicated planning, seems easy argue contributemuch end either. hand, learning DEC-POMDPs neverreally addressed except fringe work particular settings (Scherrer & Charpillet, 2002; Ghavamzadeh & Mahadevan, 2004; Buffet, Dutech, & Charpillet, 2007). fact,even simple POMDPs, learning difficult task (Singh, Jaakkola, & Jordan,1994). Currently, promising research deals learning Predictive StateRepresentation (PSR) POMDP (Singh, Littman, Jong, Pardoe, & Stone, 2003; James& Singh, 2004; McCracken & Bowling, 2005). Making due allowance fundamentaldifferences functional role PSR histories, notice PSR histories quite similar structure. early say, might tryinglearn useful histories DEC-POMDP could take inspiration wayright PSRs learned POMDPs.381fiAras & Dutech9. Conclusiondesigned investigated new exact algorithms solving Decentralized Partially Observable Markov Decision Processes finite horizon (DEC-POMDPs). main contribution paper use sequence-form policies, based sets histories,order reformulate DEC-POMDP non-linear programming problem (NLP).presented two different approaches linearize NLP order find globaloptimal solutions DEC-POMDPs. first approach based combinatorialproperties optimal policies DEC-POMDPs second one relies conceptsborrowed field game theory. lead formulating DEC-POMDPs 0-1Mixed Integer Linear Programming problems (MILPs). Several heuristics speedingresolution MILPs make another important contribution work.Experimental validation mathematical programming problems designedwork conducted classical DEC-POMDP problems found literature.experiments show that, expected, MILP methods outperform classical DynamicProgramming algorithms. But, general, less efficient costlyforward search methods like GMAA*, especially case DEC-POMDP admitsmany optimal policies. Nevertheless, according nature problem, MILP methodssometimes greatly outperform GMAA* (as MA-Tiger problem).clear exact resolution DEC-POMDPs scale sizeproblems length horizon, designing exact methods useful orderdevelop improve approximate methods. see least three research directionswork contribute. One direction could take advantage largeliterature algorithms finding approximate solutions MILPs adaptMILPs formulated DEC-POMDPs. Another direction would use knowledgegained work derive improved heuristics guiding existing approximate existingmethods DEC-POMDPs. example, work Seuken Zilberstein (2007),order limit memory resources used resolution algorithm, prune spacepolicies consider them; work could help using better estimationpolicies important kept search space. Then, one directioncurrently investigating adapt approach DEC-POMDPs infinite lengthlooking yet another representation would allow problems seen MILPs.importantly, work participates better understanding DEC-POMDPs.analyzed understood key characteristics nature optimal policies orderdesign MILPs presented paper. knowledge useful workdealing DEC-POMDPs even POMDPs. experimentations also giveninteresting insights nature various problems tested, term existenceextraneous histories number optimal policies. insights might firststep toward taxonomy DEC-POMDPs.Appendix A. Non-Convex Non-Linear ProgramUsing simplest example, section aims showing Non-Linear Program(NLP) expressed Table 2 non-convex.382fiMathematical Programming DEC-POMDPsLet us consider example two agents, one 2 possible actions (a b)want solve horizon-1 decision problem. set possible joint-histories then:ha, ai, ha, bi, hb, ai hb, bi. NLP solve is:Variables: x1 (a), x1 (b), x2 (a), x2 (a)MaximizeR(, ha, ai)x1 (a)x2 (a) + R(, ha, bi)x1 (a)x2 (b)(158)+R(, hb, ai)x1 (b)x2 (a) + R(, hb, bi)x1 (b)x2 (b)subjectx1 (a) + x1 (b) = 1x2 (a) + x2 (b) = 1x1 (a) 0,x1 (b) 0x2 (a) 0,x2 (b) 0matrix formulation objectivex following kind:0 0 c0 0 eC=c e 0f 0function eq. (158) would xT .C.x Cf00x1 (a)x1 (b)x=x2 (a) .x2 (b)(159)eigen value vector v = [v1 v2 v3 v4 ]T straightforward showalso eigen value: [v1 v2 v3 v4 ]T = C.[v1 v2 v3 v4 ]T . result,matrix C, hessian objective function, positive-definite thus objectivefunction convex.Appendix B. Linear Program DualityEvery linear program (LP) converse linear program called dual. first LPcalled primal distinguish dual. primal maximizes quantity,dual minimizes quantity. n variables constraints primal,variables n constraints dual. Consider following (primal) LP.Variables: x(i), {1, 2, , n}MaximizenXc(i)x(i)i=1subject to:nXa(i, j)x(i) = b(j),j = 1, 2, ,i=1x(i) 0,= 1, 2, , n383fiAras & Dutechprimal LP one variable x(i) = 1 n. data LP consistsnumbers c(i) = 1 n, numbers b(j) j = 1 numbersa(i, j) = 1 n j = 1 m. LP thus n variablesconstraints. dual LP following LP.Variables: y(j), j {1, 2, , }MinimizeXb(j)y(j)j=1subject To:Xa(i, j)y(j) c(i),= 1, 2, , nj=1y(j) (, +),j = 1, 2, ,dual LP one variable y(j) j = 1 m. y(j) variable freevariable. is, allowed take value R. dual LP variables nconstraints.theorem linear programming duality follows.Theorem B.1. (Luenberger, 1984) either primal LP dual LP finite optimalsolution, other, corresponding values objective functionsequal.Applying theorem primal-dual pair given above, holds,nXc(i)x (i) =Xb(j)y (j)j=1i=1x denotes optimal solution primal denotes optimal solutiondual.theorem complementary slackness follows.Theorem B.2. (Vanderbei, 2008) Suppose x feasible primal linear programfeasible dual. Let (w1 , ,wm ) denote corresponding primal slack variables,let (z1 , ,zn ) denote corresponding dual slack variables. x optimalrespective problemsxj zj = 0j = 1, , n,wi yi = 0= 1, , m.384fiMathematical Programming DEC-POMDPsAppendix C. Regret DEC-POMDPsvalue information set Ii agent i-reduced joint policy q,denoted (, q), defined by:X(, q) = maxR(, hh, j i)q(j )(160)hj Eiterminal information set and, non-terminal, by:X(h.o, q)(, q) = maxh(161)oOiThen, regret history h agent i-reduced joint policy q,denoted (h, q), defined by:X(h, q) = ((h), q)R(, hh, j i)q(j )(162)j Hih terminal and, h non-terminal, by:(h, q) = ((h), q)X(h.o, q)(163)oOiconcept regret agent i, independant policy agent i,useful looking optimal policy optimal value known: 0.thus easier manipulate optimal value policy.Appendix D. Program Changes Due OptimizationsPruning locally globally extraneous histories reduces size search spacemathematical programs. Now, constraints programs depend sizesearch space, must alter constraints.Let denote superscript sets actually used program. example, Eiactual set terminal histories agent i, pruned extraneous historiesnot.Programs MILP (Table 3) MILP-n agents (Table 5) rely factnumber histories given length support pure policy agent fixedequal |Oi |t1 . may case pruned sets, following changesmade:constraint (42) MILP (121) MILP-n agents,Xz(j) =|Oi |T 1jEiImust replacedXz(j)iIjE385|Oi |T 1 .(164)fiAras & Dutechset constraints (41) MILP (120) MILP-n agents,Xz(hh, j i) =j Ei|Ok |T 1 xi (h),I, h Ei|Ok |T 1 xi (h),I, h Ei .kI\{i}must replacedXz(hh, j i)(165)kI\{i}j Eiset constraints (119) MILP-n agents,yi ((h))X1R(, hh, ji i)z(j) = wi (h),|Oi |T 1h EijEmust replacedyi ((h))X1R(, hh, ji i)z(j) = wi (h),|Oi |T 1h Ei .(166)jEAppendix E. Example using MA-Tigerexample derived using Decentralized Tiger Problem (MA-Tiger) describedSection 2.2. two agents, 3 actions (al , ar , ao ) 2 observations (ol , ).consider problem horizon 2.18 (32 2) terminal histories agent: ao .ol .ao , ao .ol .al , ao .ol .ar , ao .or .ao ,ao .or .al , ao .or .ar , al .ol .ao , al .ol .al , al .ol .ar , al .or .ao , al .or .al , al .or .ar , ar .ol .ao , ar .ol .al ,ar .ol .ar , ar .or .ao , ar .or .al , ar .or .ar .thus 324 (182 = 322 22 ) joint histories agents: hao .ol .ao ,ao .ol .ao i,hao .ol .ao ,ao .ol .al i,hao .ol .ao ,ao .ol .ar i, , har .or .ar ,ar .or .ar i.E.1 Policy Constraintspolicy constraints horizon 2 one agent MA-Tiger problem would be:Variables: x every historyx(ao ) + x(al ) + x(ar ) = 0x(ao ) + x(ao .ol .ao ) + x(ao .ol .al ) + x(ao .ol .ar ) = 0x(ao ) + x(ao .or .ao ) + x(ao .or .al ) + x(ao .or .ar ) = 0x(al ) + x(al .ol .ao ) + x(al .ol .al ) + x(al .ol .ar ) = 0x(al ) + x(al .or .ao ) + x(al .or .al ) + x(al .or .ar ) = 0x(ar ) + x(ar .ol .ao ) + x(ar .ol .al ) + x(ar .ol .ar ) = 0x(ar ) + x(ar .or .ao ) + x(ar .or .al ) + x(ar .or .ar ) = 0386fiMathematical Programming DEC-POMDPsx(ao ) 0x(al ) 0x(ar ) 0x(ao .ol .ao ) 0 x(ao .ol .al ) 0 x(ao .ol .ar ) 0x(ao .or .ao ) 0 x(ao .or .al ) 0 x(ao .or .ar ) 0x(al .ol .ao ) 0x(al .ol .al ) 0x(al .ol .ar ) 0x(al .or .ao ) 0 x(al .or .al ) 0 x(al .or .ar ) 0x(ar .ol .ao ) 0 x(ar .ol .al ) 0 x(ar .ol .ar ) 0x(ar .or .ao ) 0 x(ar .or .al ) 0 x(ar .or .ar ) 0E.2 Non-Linear Program MA-TigerNon-Linear Program finding optimal sequence-form policy MA-Tigerhorizon 2 would be:Variables: xi every history agentMaximizeR(, hao .ol .ao , ao .ol .ao i)x1 (ao .ol .ao )x2 (ao .ol .ao )+ R(, hao .ol .ao , ao .ol .al i)x1 (ao .ol .ao )x2 (ao .ol .al )+ R(, hao .ol .ao , ao .ol .ar i)x1 (ao .ol .ao )x2 (ao .ol .ar )+subject to:x1 (ao ) + x1 (al ) + x1 (ar ) = 0x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0x1 (al ) + x1 (al .ol .ao ) + x1 (al .ol .al ) + x1 (al .ol .ar ) = 0x1 (al ) + x1 (al .or .ao ) + x1 (al .or .al ) + x1 (al .or .ar ) = 0x1 (ar ) + x1 (ar .ol .ao ) + x1 (ar .ol .al ) + x1 (ar .ol .ar ) = 0x1 (ar ) + x1 (ar .or .ao ) + x1 (ar .or .al ) + x1 (ar .or .ar ) = 0x2 (ao ) + x2 (al ) + x2 (ar ) = 0x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0x2 (al ) + x2 (al .ol .ao ) + x2 (al .ol .al ) + x2 (al .ol .ar ) = 0x2 (al ) + x2 (al .or .ao ) + x2 (al .or .al ) + x2 (al .or .ar ) = 0x2 (ar ) + x2 (ar .ol .ao ) + x2 (ar .ol .al ) + x2 (ar .ol .ar ) = 0x2 (ar ) + x2 (ar .or .ao ) + x2 (ar .or .al ) + x2 (ar .or .ar ) = 0387fiAras & Dutechx1 (ao ) 0x1 (al ) 0x1 (ar ) 0x1 (ao .ol .ao ) 0 x1 (ao .ol .al ) 0 x1 (ao .ol .ar ) 0x1 (ao .or .ao ) 0 x1 (ao .or .al ) 0 x1 (ao .or .ar ) 0x1 (al .ol .ao ) 0x1 (al .ol .al ) 0x1 (al .ol .ar ) 0x1 (al .or .ao ) 0 x1 (al .or .al ) 0 x1 (al .or .ar ) 0x1 (ar .ol .ao ) 0 x1 (ar .ol .al ) 0 x1 (ar .ol .ar ) 0x1 (ar .or .ao ) 0 x1 (ar .or .al ) 0 x1 (ar .or .ar ) 0x2 (ao ) 0x2 (al ) 0x2 (ar ) 0x2 (ao .ol .ao ) 0 x2 (ao .ol .al ) 0 x2 (ao .ol .ar ) 0x2 (ao .or .ao ) 0 x2 (ao .or .al ) 0 x2 (ao .or .ar ) 0x2 (al .ol .ao ) 0x2 (al .ol .al ) 0x2 (al .ol .ar ) 0x2 (al .or .ao ) 0 x2 (al .or .al ) 0 x2 (al .or .ar ) 0x2 (ar .ol .ao ) 0 x2 (ar .ol .al ) 0 x2 (ar .ol .ar ) 0x2 (ar .or .ao ) 0 x2 (ar .or .al ) 0 x2 (ar .or .ar ) 0E.3 MILP MA-TigerMILP horizon 2 agents MA-Tiger problem would be:Variables:xi (h) every history agentz(j) every terminal joint historyMaximizeR(, hao .ol .ao , ao .ol .ao i)z(hao .ol .ao , ao .ol .ao i)+ R(, hao .ol .ao , ao .ol .al i)z(hao .ol .ao , ao .ol .al i)+ R(, hao .ol .ao , ao .ol .ar i)z(hao .ol .ao , ao .ol .ar i)+388fiMathematical Programming DEC-POMDPssubject to:x1 (ao ) + x1 (al ) + x1 (ar ) = 0x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0x2 (ao ) + x2 (al ) + x2 (ar ) = 0x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0z(hao .ol .ao , ao .ol .ao i) + z(hao .ol .ao , ao .ol .al i) + z(hao .ol .ao , ao .ol .ar i) = 2 x1 (ao .ol .ao )z(hao .ol .ao , ao .ol .ao i) + z(hao .ol .al , ao .ol .ao i) + z(hao .ol .ar , ao .ol .ao i) = 2 x2 (ao .ol .ao )z(hao .ol .al , ao .ol .ao i) + z(hao .ol .al , ao .ol .al i) + z(hao .ol .al , ao .ol .ar i) = 2 x1 (ao .ol .al )z(hao .ol .ao , ao .ol .al i) + z(hao .ol .al , ao .ol .al i) + z(hao .ol .ar , ao .ol .al i) = 2 x2 (ao .ol .al )x1 (ao ) 0x1 (al ) 0x1 (ar ) 0x1 (ao .ol .ao ) {0, 1}x1 (ao .ol .al ) {0, 1}x1 (ao .ol .ar ) {0, 1}x1 (ao .or .ao ) {0, 1}x1 (ao .or .al ) {0, 1}x1 (ao .or .ar ) {0, 1}x2 (ao ) 0x2 (al ) 0x2 (ar ) 0x2 (ao .ol .ao ) {0, 1}x2 (ao .ol .al ) {0, 1}x2 (ao .ol .ar ) {0, 1}x2 (ao .or .ao ) {0, 1}x2 (ao .or .al ) {0, 1}x2 (ao .or .ar ) {0, 1}z(hao .ol .ao , ao .ol .ao i) {0, 1} z(hao .ol .ao , ao .ol .al i) {0, 1} z(hao .ol .ao , ao .ol .ar i) {0, 1}z(hao .ol .al , ao .ol .ao i) {0, 1} z(hao .ol .al , ao .ol .al i) {0, 1} z(hao .ol .al , ao .ol .ar i) {0, 1}E.4 MILP-2 Agents MA-TigerMILP-2 agents horizon 2 agents MA-Tiger problem would be:Variables:xi (h), wi (h) bi (h) every history agentyi ()) agent every information setMaximizey1 ()389fiAras & Dutechsubject to:x1 (ao ) + x1 (al ) + x1 (ar ) = 0x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0x2 (ao ) + x2 (al ) + x2 (ar ) = 0x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0y1 () y1 (ao .ol ) y1 (ao .or ) = w1 (ao )y1 () y1 (al .ol ) y1 (al .or ) = w1 (al )y1 () y1 (ar .ol ) y1 (ar .or ) = w1 (ar )y2 () y2 (ao .ol ) y2 (ao .or ) = w2 (ao )y2 () y2 (al .ol ) y2 (al .or ) = w2 (al )y2 () y2 (ar .ol ) y2 (ar .or ) = w2 (ar )y1 (ao .ol ) R(, hao .ol .ao , ao .ol .ao i)x2 (ao .ol .ao )R(, hao .ol .ao , ao .ol .al i)x2 (ao .ol .al )R(, hao .ol .ao , ao .ol .ar i)x2 (ao .ol .ar )R(, hao .ol .ao , al .ol .ao i)x2 (al .ol .ao )R(, hao .ol .ao , al .ol .al i)x2 (al .ol .al )R(, hao .ol .ao , al .ol .ar i)x2 (al .ol .ar )= w1 (ao .ol .ao )y1 (ao .ol ) R(, hao .ol .al , ao .ol .ao i)x2 (ao .ol .ao )R(, hao .ol .al , ao .ol .al i)x2 (ao .ol .al )R(, hao .ol .al , ao .ol .ar i)x2 (ao .ol .ar )R(, hao .ol .al , al .ol .ao i)x2 (al .ol .ao )R(, hao .ol .al , al .ol .al i)x2 (al .ol .al )R(, hao .ol .al , al .ol .ar i)x2 (al .ol .ar )= w1 (ao .ol .al )390fiMathematical Programming DEC-POMDPsy1 (ar .or ) R(, har .or .ar , ao .ol .ao i)x2 (ao .ol .ao )R(, har .or .ar , ao .ol .al i)x2 (ao .ol .al )R(, har .or .ar , ao .ol .ar i)x2 (ao .ol .ar )R(, har .or .ar , al .ol .ao i)x2 (al .ol .ao )R(, har .or .ar , al .ol .al i)x2 (al .ol .al )R(, har .or .ar , al .ol .ar i)x2 (al .ol .ar )= w1 (ar .or .ar )y2 (ao .ol ) R(, hao .ol .ao , ao .ol .ao i)x1 (ao .ol .ao )R(, hao .ol .al , ao .ol .ao i)x1 (ao .ol .al )R(, hao .ol .ar , ao .ol .ao i)x1 (ao .ol .ar )R(, hal .ol .ao , ao .ol .ao i)x1 (al .ol .ao )R(, hal .ol .al , ao .ol .ao i)x1 (al .ol .al )R(, hal .ol .ar , ao .ol .ao i)x1 (al .ol .ar )= w2 (ao .ol .ao )y2 (ao .ol ) R(, hao .ol .ao , ao .ol .al i)x1 (ao .ol .ao )R(, hao .ol .al , ao .ol .al i)x1 (ao .ol .al )R(, hao .ol .ar , ao .ol .al i)x1 (ao .ol .ar )R(, hal .ol .ao , ao .ol .al i)x1 (al .ol .ao )R(, hal .ol .al , ao .ol .al i)x1 (al .ol .al )R(, hal .ol .ar , ao .ol .al i)x1 (al .ol .ar )= w2 (ao .ol .al )x1 (ao ) 1 b1 (ao )x1 (al ) 1 b1 (al )x1 (ar ) 1 b1 (ar )x1 (ao .ol .ao ) 1 b1 (ao .ol .ao )x1 (ao .ol .al ) 1 b1 (ao .ol .al )x1 (ao .ol .ar ) 1 b1 (ao .ol .ar )w1 (ao ) U1 (ao )b1 (ao )w1 (al ) U1 (al )b1 (al )w1 (ar ) U1 (ar )b1 (ar )w1 (ao .ol .ao ) U1 (ao .ol .ao )b1 (ao .ol .ao )w1 (ao .ol .al ) U1 (ao .ol .al )b1 (ao .ol .al )w1 (ao .ol .ar ) U1 (ao .ol .ar )b1 (ao .ol .ar )391fiAras & Dutechx1 (ao ) 0x1 (ao .ol .ao ) 0x1 (al ) 0x1 (ao .ol .al ) 0x1 (ar ) 0x1 (ao .ol .ar ) 0w1 (ao ) 0w1 (ao .ol .ao ) 0w1 (al ) 0w1 (ao .ol .al ) 0w1 (ar ) 0w1 (ao .ol .ar ) 0b1 (ao ) {0, 1}b1 (ao .ol .ao ) {0, 1}b1 (al ) {0, 1}b1 (ao .ol .al ) {0, 1}y1 () (, +)y1 (ao .ol ) (, +) y1 (ao .or ) (, +)... agent 2392b1 (ar ) {0, 1}b1 (ao .ol .ar ) {0, 1}fiMathematical Programming DEC-POMDPsReferencesAmato, C., Bernstein, D. S., & Zilberstein, S. (2007a). Optimizing memory-bounded controllers decentralized POMDPs. Proc. Twenty-Third Conf. UncertaintyArtificial Intelligence (UAI-07).Amato, C., Bernstein, D. S., & Zilberstein, S. (2007b). Solving POMDPs using quadraticallyconstrained linear programs. Proc. Twentieth Int. Joint Conf. ArtificialIntelligence (IJCAI07).Amato, C., Carlin, A., & Zilberstein, S. (2007c). Bounded dynamic programmingdecentralized POMDPs. Proc. Workshop Multi-Agent Sequential DecisionMaking Uncertain Domains (MSDM) AAMAS07.Amato, C., Dibangoye, J., & Zilberstein, S. (2009). Incremental policy generation finitehorizon DEC-POMDPs. Proc. Nineteenth Int. Conf. Automated PlanningScheduling (ICAPS-09).Anderson, B., & Moore, J. (1980). Time-varying feedback laws decentralized control.Nineteenth IEEE Conference Decision Control including SymposiumAdaptive Processes, 19(1), 519524.Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. (2004). Solving transition independentdecentralized Markov decision processes. Journal Artificial Intelligence Research,22, 423455.Bellman, R. (1957). Dynamic programming. Princeton University Press, Princeton, NewJersey.Bernstein, D., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control Markov decision processes. Mathematics Operations Research,27 (4), 819840.Bernstein, D. S., Hansen, E. A., & Zilberstein, S. (2005). Bounded policy iterationdecentralized POMDPs. Proc. Nineteenth Int. Joint Conf. ArtificialIntelligence (IJCAI), pp. 12871292.Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming decentralizedpomdps lossless policy compression. Proc. Int. Conf. AutomatedPlanning Scheduling (ICAPS08).Boutilier, C. (1996). Planning, learning coordination multiagent decision processes.Proceedings 6th Conference Theoretical Aspects Rationality Knowledge (TARK 96), De Zeeuwse Stromen, Nederlands.Buffet, O., Dutech, A., & Charpillet, F. (2007). Shaping multi-agent systems gradient reinforcement learning. Autonomous Agent Multi-Agent System Journal(AAMASJ), 15 (2), 197220.393fiAras & DutechCassandra, A., Kaelbling, L., & Littman, M. (1994). Acting optimally partially observablestochastic domains. Proc. 12th Nat. Conf. Artificial Intelligence (AAAI).Chades, I., Scherrer, B., & Charpillet, F. (2002). heuristic approach solvingdecentralized-POMDP: assessment pursuit problem. Proc. 2002 ACMSymposium Applied Computing, pp. 5762.Cornuejols, G. (2008). Valid inequalities mixed integer linear programs. MathematicalProgramming B, 112, 344.Dantzig, G. B. (1960). significance solving linear programming problemsinteger variables. Econometrica, 28(1), 3044.dEpenoux, F. (1963). probabilistic production inventory problem. ManagementScience, 10(1), 98108.Diwekar, U. (2008). Introduction Applied Optimization (2 edition). Springer.Drenick, R. (1992). Multilinear programming: Duality theories. Journal OptimizationTheory Applications, 72(3), 459486.Fletcher, R. (1987). Practical Methods Optimization. John Wiley & Sons, New York.Ghavamzadeh, M., & Mahadevan, S. (2004). Learning communicate act cooperative multiagent systems using hierarchical reinforcement learning. Proc. 3rdInt. Joint Conf. Autonomous Agents Multi-Agent Systems (AAMAS04).Govindan, S., & Wilson, R. (2001). global newton method compute Nash equilibria.Journal Economic Theory, 110, 6586.Hansen, E., Bernstein, D., & Zilberstein, S. (2004). Dynamic programming partiallyobservable stochastic games. Proc. Nineteenth National Conference Artificial Intelligence (AAAI-04).Horst, R., & Tuy, H. (2003). Global Optimization: Deterministic Approaches (3rd edition).Springer.James, M., & Singh, S. (2004). Learning discovery predictive state representationsdynamical systems reset. Proc. Twenty-first Int. Conf. MachineLearning (ICML04).Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partiallyobservable stochastic domains. Artificial Intelligence, 101, 99134.Koller, D., Megiddo, N., & von Stengel, B. (1994). Fast algorithms finding randomizedstrategies game trees. Proceedings 26th ACM Symposium TheoryComputing (STOC 94), pp. 750759.Koller, D., & Megiddo, N. (1996). Finding mixed strategies small supports extensiveform games. International Journal Game Theory, 25(1), 7392.394fiMathematical Programming DEC-POMDPsLemke, C. (1965). Bimatrix Equilibrium Points Mathematical Programming. Management Science, 11(7), 681689.Luenberger, D. (1984). Linear Nonlinear Programming. Addison-Wesley PublishingCompany, Reading, Massachussetts.McCracken, P., & Bowling, M. H. (2005). Online discovery learning predictive staterepresentations. Advances Neural Information Processing Systems 18 (NIPS05).Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralizedPOMDPs: towards efficient policy computation multiagent setting. Proc.Int. Joint Conference Artificial Intelligence, IJCAI03.Oliehoek, F., Spaan, M., & Vlassis, N. (2008). Optimal approximate Q-value functionsdecentralized POMDPs. Journal Artificial Intelligence Research (JAIR), 32,289353.Oliehoek, F., Whiteson, S., & Spaan, M. (2009). Lossless clustering histories decentralized POMDPs. Proc. International Joint Conference AutonomousAgents Multi Agent Systems, pp. 577584.Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory. MIT Press,Cambridge, Mass.Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial Optimization: AlgorithmsComplexity. Dover Publications.Papadimitriou, C. H., & Tsitsiklis, J. (1987). Complexity Markov Decision Processes. Mathematics Operations Research, 12 (3), 441 450.Parsons, S., & Wooldridge, M. (2002). Game theory decision theory multi-agentsystems. Autonomous Agents Multi-Agent Systems (JAAMAS), 5(3), 243254.Petrik, M., & Zilberstein, S. (2007). Average-reward decentralized Markov decision processes. Proc. Twentieth Int. Joint Conf. Artificial Intelligence (IJCAI2007).Petrik, M., & Zilberstein, S. (2009). bilinear programming approach multiagentplanning. Journal Artificial Intelligence Research (JAIR), 35, 235274.Puterman, M. (1994). Markov Decision Processes: discrete stochastic dynamic programming. John Wiley & Sons, Inc. New York, NY.Pynadath, D., & Tambe, M. (2002). Communicative Multiagent Team Decision Problem: Analyzing Teamwork Theories Models. Journal Artificial IntelligenceResearch, 16, 389423.Radner, R. (1959). application linear programming team decision problems.Management Science, 5, 143150.Russell, S., & Norvig, P. (1995). Artificial Intelligence: modern approach. Prentice Hall.395fiAras & DutechSandholm, T. (1999). Multiagent systems, chap. Distributed rational decision making, pp.201258. MIT Press. Ed. G. Weiss.Sandholm, T., Gilpin, A., & Conitzer, V. (2005). Mixed-integer programming methodsfinding nash equilibria. Proc. National Conference Artificial Intelligence(AAAI).Scherrer, B., & Charpillet, F. (2002). Cooperative co-learning: model based approachsolving multi agent reinforcement problems. Proc. IEEE Int. Conf.Tools Artificial Intelligence (ICTAI02).Seuken, S., & Zilberstein, S. (2007). Memory-bounded dynamic programming DECPOMDPs. Proc. Twentieth Int. Joint Conf. Artificial Intelligence (IJCAI07).Singh, S., Jaakkola, T., & Jordan, M. (1994). Learning without state estimation partiallyobservable markovian decision processes.. Proceedings Eleventh InternationalConference Machine Learning.Singh, S., Littman, M., Jong, N., Pardoe, D., & Stone, P. (2003). Learning predictive staterepresentations. Proc. Twentieth Int. Conf. Machine Learning (ICML03).Szer, D., & Charpillet, F. (2006). Point-based Dynamic Programming DEC-POMDPs.Proc. Twenty-First National Conf. Artificial Intelligence (AAAI 2006).Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithmsolving decentralized POMDPs. Proc. Twenty-First Conf. UncertaintyArtificial Intelligence (UAI05), pp. 576 583.Thomas, V., Bourjot, C., & Chevrier, V. (2004). Interac-DEC-MDP : Towards useinteractions DEC-MDP. Proc. Third Int. Joint Conf. AutonomousAgents Multi-Agent Systems (AAMAS04), New York, USA, pp. 14501451.Vanderbei, R. J. (2008). Linear Programming: Foundations Extensions (3rd edition).Springer.von Stengel, B. (2002). Handbook Game Theory, Vol. 3, chap. 45-Computing equilibriatwo-person games, pp. 17231759. North-Holland, Amsterdam.Wu, J., & Durfee, E. H. (2006). Mixed-integer linear programming transitionindependent decentralized MDPs. Proc. fifth Int. Joint Conf. AutonomousAgents Multiagent Systems (AAMAS06), pp. 10581060 New York, NY, USA.ACM.Xuan, P., Lesser, V., & Zilberstein, S. (2000). Communication multi-agent Markovdecision processes. Proc. ICMAS Workshop Game Theoretic DecisionTheoretics Agents Boston, MA.396fiJournal Artificial Intelligence Research 37 (2010) 85-98Submitted 10/09; published 2/10Mechanisms Multi-Unit AuctionsShahar Dobzinskishahar@cs.cornell.eduComputer Science Department, Cornell UniversityIthaca, NY 14853Noam Nisannoam@cs.huji.ac.ilSchool Computer Science EngineeringHebrew University JerusalemJerusalem, IsraelAbstractpresent incentive-compatible polynomial-time approximation scheme multiunit auctions general k-minded player valuations. mechanism fully optimizesappropriately chosen sub-range possible allocations uses VCG paymentssub-range. show obtaining fully polynomial-time incentive-compatibleapproximation scheme, least using VCG payments, NP-hard. case valuationsgiven black boxes, give polynomial-time incentive-compatible 2-approximationmechanism show better possible, least using VCG payments.1. IntroductionAlgorithmic Mechanism Design goal construct efficient mechanismshandle selfish behavior players. particular, interested designingtruthful mechanisms, is, mechanisms dominant strategy playersimply reveal true valuation.Typical problems field involve allocating goods players. One key problemproblem multi-unit auctions. given identical items n bidders.setting view number items large desire mechanisms whosecomputational complexity polynomial number bits needed represent m. Everybidder valuation function vi : {1, ..., m} <, vi (q) denotes valueobtaining q items. assume vi weakly monotone increasing (free disposal),normalized (vi (0) = 0). goal usual one maximizing social welfare vi (qi )qi m.general case, vi represented real numbers, abstract settingsmay accessed black box. concrete setting, assume vi representedk-minded bid, i.e. given list: (q1 , p1 ), ..., (qk , pk ), vi (q) = max{j|qj q} pj .corresponds XOR bidding language (Nisan, 2006). Observe k-mindedvaluation corresponds step function k steps (step located qiheight pi ). general, k large m, case k = 1 single-mindedcase.problem received much attention, starting Vickreys seminal paper (1961)described truthful mechanism case downward sloping valuationsitems optimally allocated greedily. general case, however, NP-hard,single-minded case re-formulation knapsack problem. Luckily, knapsackc2010AI Access Foundation. rights reserved.fiDobzinski & Nisanproblem fully-polynomial time approximation scheme (i.e. approximatedwithin factor 1 + time polynomial n, log m, 1 ), hard seealgorithm directly extends general case multi-unit auctions.1.1 VCG-Based Mechanismskey positive technique mechanism design VCG payment scheme.payment scheme bidder pays hi (vi ) j6=i vj (a), algorithmic output,hi arbitrary function depend vi . Unfortunately, VCGworks perfectly well game-theoretic perspective, useful computationalsettings, since multi-unit auctions, interesting combinatorial optimizationproblems, calculating exact optimum intractable.One naive idea use approximation algorithm find approximate solutiona, let bidder pay hi (vi ) j6=i vj (a). Applying idea casemulti-unit auction tempting particular light known (1 + )-approximationalgorithm problem. Unfortunately, turns general using approximation algorithm together VCG payments result truthful mechanism.phenomenon studied Nisan Ronen (2007). observedfollowing family allocation algorithms yield truthful VCG-based mechanisms:Definition: allocation algorithm (that produces output inputv1 , ..., vn , set possible alternatives) called maximal-in-range (henceforth MIR) completely optimizes social welfare subrange R A. I.e.,R A, v1 , ..., vn , arg maxaR vi (a).I.e., MIR algorithms use following natural simple strategy find approximatelyoptimal solution: optimally search within pre-specified sub-range feasiblesolutions subrange optimal search algorithmically feasible.main result Nisan Ronen (2007) states essentiallyVCG-based mechanisms incentive compatible.Theorem (Nisan & Ronen, 2007): allocation algorithm incentive-compatibleVCG-based mechanism combinatorial auctions equivalent maximal-in-range algorithm.Equivalent means social utilities identical inputs, i.e. boutputs two allocation algorithms input v1 , . . . , vn vi (a) = vi (b).particular, outputs must coincide generically except perhaps case ties. NisanRonen (2007) prove two specific types mechanism design problems,result general (Dobzinski & Nisan, 2007).Following Nisan Ronen (2007), Dobzinski Nisan (2007) view resultnegative one. Namely, show setting combinatorial auctionssubmodular bidders, MIR algorithms much power. might implysetting truthful deterministic mechanisms much power, since Laviet al. (2003) give partial evidence truthful mechanisms provide goodapproximation ratio must MIR.86fiMechanisms Multi-Unit Auctionscontrast, paper views result positive result. observe MIRalgorithms provide us constructive way obtaining truthful mechanisms1 . Indeed,paper suggests settings power MIR algorithmstrivial all. note several previous papers already obtained upper bounds usingMIR techniques (Holzman, Kfir-Dahav, Monderer, & Tennenholtz, 2004; Dobzinski, Nisan,& Schapira, 2005; Blumrosen & Dobzinski, 2007). Yet, paper initiates systematicstudy MIR algorithms. particular, techniques used sophisticatedobtained previous work.1.2 Previous Work Resultscase multi-unit auctions single-minded bidders, paper Briest, Krysta,Vocking (2005) presents truthful fully polynomial time approximation scheme (FPTAS), improving upon previous result Mualem Nisan (2002). resultknown k-minded bidders randomized 12 -approximation mechanism truthful expectation (Lavi & Swamy, 2005). current paper presents polynomial timeapproximation scheme (PTAS) general case.Theorem: every fixed > 0, exists truthful (1 )-approximation mechanismmulti-unit auctions k-minded bidders whose running time polynomial n, log m,k. dependence running time exponential.However, prove two ways mechanism improved upon. First,show dependence cannot made polynomial without destroyingtruthfulness, long MIR techniques used. contrast, pure algorithmic pointview possible obtain fully polynomial time approximation scheme2 . Furthermore,exists truthful FPTAS bidders known single minded (Briest et al.,2005).Theorem: fully polynomial time truthful approximation mechanism uses VCGpayments exists, unless P=NP.show dependence k necessary, approximation schemepossible general black-box model. shown general communication model,even two bidders.Theorem: Every approximation mechanism among two bidders general valuationsuses VCG payments requires exponentially many queries obtain approximationfactor better 12 .present truthful approximation mechanism general black box modelobtain factor 12 . improves upon randomized one Lavi Swamy(2005) truthful expectation.1. Note payments efficient MIR mechanism computed efficiently: take outputallocation pay bidder sum values bidders output allocation.2. course, ignoring computational issues, standard VCG mechanism MIR provides approximation ratio 1.87fiDobzinski & NisanTheorem: exists truthful 12 -approximation mechanism multi-unit auctionsamong general valuations whose running time polynomial n log m. accessbidders valuations value queries: vi (q)?3 . mechanism uses VCGpayments.Section 5 present fairly general construction takes -approximation MIR1algorithm bidders, 1, converts ( t+1)-approximationalgorithm n bidders, time polynomial n, log m, running time A.construction works long model allows us answer value queries. presentfour applications construction: first two applications provide another proofupper bounds discussed (the PTAS k-minded bidders, 12 -approximationblack-box model). present two new applications: PTAS stronger biddinglanguages, one used Kothari et al. (2005), ( 43 + )-approximationmechanism multi-unit auctions subadditive valuations. Prior paper, nothingknown latter setting.construction provides us interesting example truthful reduction amongproblems: MIR approximation algorithm fixed number bidders automatically translated truthful approximation algorithm number bidders,losing small factor approximation ratio.1.3 Discussion Open Questionsmain open problem remains determine best approximation ratio obtained truthful way. general method known constructing mechanismsVCG, lower bounds state VCG cannot take us further. Furthermore,Lavi et al. (2003) show 2 bidders black-box model, items allocated,MIR algorithms truthful way obtain reasonable approximation ratio.Combined lower bounds, get better-than- 12 truthful approximationpossible polynomial time, 2 bidders items allocated. intriguingopen question understand whether condition items allocated indeednecessary.Another issue paper highlights inherent difference obtainingapproximation algorithms single-parameter environments multi-parameter environments. single parameter environment private information bidder consistsone number, multi-parameter environments private information consistsone number. Recall single parameter variant multi-unit auctions, assume bidders single-minded, exists truthful FPTAS(Briest et al., 2005). However, variants discuss paper multi-parameter,indeed lower bounds suggest obtaining FPTAS impossible (if one provemechanisms give good approximation ratio must MIR).Paper OrganizationSection 3 present PTAS k-minded bidders, 21 -approximationblack-box model. Section 4 considers lower bounds MIR algorithms models.3. yet another improvement upon mechanism Lavi swamy (2005) requiresstronger demand queries.88fiMechanisms Multi-Unit AuctionsSection 5 describe general construction, algorithmic applications: truthfulmechanisms models powerful bidding languages.2. Preliminariessection provide basic definitions notations used paper.2.1 Settingmulti-unit auction set identical items, set N = {1, 2, . . . , n}bidders. bidder valuation function vi : [m] R+ , normalized(vi (0) = 0) non-decreasing. Denote V set possible valuations. allocationitems ~s = (s1 , . . . , sn ) N vector non-negative integers si m.Denote set allocations S. goal find allocation maximizeswelfare: vi (si ).consider two settings differ valuations given us. concretesetting (a bidding language model) assume vi represented k-mindedbid, i.e. given list: (q1 , p1 ), ..., (qk , pk ), vi (q) = max{j|qj q} pj .Otherwise, valuations given us black boxes. algorithms, blackbox v answer weak value queries: given s, value v(s).impossibility result, assume black box v answer query basedv (the communication model). algorithms run time poly(n, log m),impossibility result gives lower bound number bits transferred, holds evenmechanism computationally unbounded.2.2 Truthfulnessdeterministic n-bidder mechanism multi-unit auctions pair (f, p) f : V np = (p1 , , pn ), pi : V n R.Definition 2.1 Let (f, p) deterministic mechanism. (f, p) truthful i,vi , vi0 vi vi (f (vi , vi )i ) pi (vi , vi ) vi (f (vi0 , vi )i ) p(vi0 , vi ).Definition 2.2 f affine maximizer exist set allocations R, constant0 N , constant ~s < ~s S, f (v1 , ..., vn )arg max~s=(s1 ,...,sn )R (i (i vi (si )) + ). f called maximal-in-range (MIR) = 1N , = 0 ~s R.following proposition standard:Proposition 2.3 Let f affine maximizer (in particular, maximal range).payments p (f, p) truthful mechanism.3. Basic Mechanismssection provides basic mechanisms multi-unit auctions: PTAS k-mindedbidders, 12 approximation black-box model.89fiDobzinski & Nisan3.1 Truthful PTAS k-Minded Biddersdesign MIR algorithm problem, directly yields incentive-compatibleVCG-based mechanism. define range R allocations, prove biddersk-minded algorithm outputs polynomial time best allocation R.start defining subrange R.Definition 3.1 say allocation (s1 , ..., sn ) t-round exists setbidders, |T | t, following two conditions hold:Let l = jT sj .jkmlbidder/ , si multiple max( (nt), 1).2kjml, 1) (n t)2 items assigned bidders outside :max( (nt)2/ sijkmlmax( (nt), 1) (n t)22let R set t-round allocations fixed (that dependapproximation guarantee). Next prove value best allocation Rclose optimum:Lemma 3.2 Let (a1 , ..., ) optimal t-round allocation, (o1 , ..., ) optimal1unrestricted allocation, vi (ai ) (1 t+1)i vi (oi ).Proof:Let us start optimal unrestricted allocation (o1 , ..., ), useconstruct t-round allocation high value. Assume items allocatedoptimal allocation (without loss generality due monotonicity valuations),v1 (o1 ) ... vn (on ). Let = {1, ..., t} set bidders required Definition3.1, assign bidder bundle size oi . Definition 3.1, let l = jT oj .Let j/ bidder got largest number items oj ml./ T,jkntml6= j, round oi nearest multiple b = max( (nt)2 , 1) assignmany items bidder i. Assign bidderjkno items. valid t-round allocation sincejmlb 6= 1 added (n t) (nt)2 mlnt items rounding up, deleted leastmlntitems removing oj . Notice second condition also holds. b = 1, observeeven optimal allocation t-round. value solution, observevi (oi )bidder 6= j gets bundle smaller oi . addition, vj (oj ) it+1,gives required approximation.MIR approximation algorithm try subset biddersset bidders. possible selection , algorithm consider possibleallocations bidders according k bids bidder submitted. is,consider allocation assigns bidder exactly si items,si m, si bid (si , pi ) k bids bidder (for pi > 0).selection allocation bidders according bids,algorithm splits remaining l items (n t)2 equi-sized bundles size90fiMechanisms Multi-Unit Auctionskjml, 1), l total number items bidders get. maximalmax( (nt)2in-range algorithm optimally allocate equi-size bundles among bidders. Finally, algorithm outputs best allocation among allocationsconsidered. left show following two lemmas:Lemma 3.3 every fixed algorithm runs time polynomial n log m.Proof:nt possible selections sets . selectionk allocations bidders considered. Finding optimalallocation bidders dynamic programming. Let b size equi-sizebundles. Without loss generality, assume = {n + 1, ..., n}. calculatefollowing information every 1 n 1 q (n t)2 : (i, q)maximum value obtained allocating q equi-size bundles amongbidders 1...i. entry filled polynomial time using relations: (i, q) =maxq0 q (vi (q 0 b) + (i 1, q q 0 )). particular notice b = 1 numberequi-size bundles polynomial number bidders, thus number entriestable polynomial also case. Overall get algorithm runs timepolynomial n log m, every fixed t.Lemma 3.4 algorithm finds optimal t-round allocation.Proof: First, notice algorithm outputs t-round allocation. Let us proveoutputs optimal one. Let = (o1 , ..., ) optimal t-round allocation, letset bidders Definition 3.1, let l = oi . bidder removemaximal number qi (possibly zero) items oi vi (oi ) = vi (oi qi ).Observe exists pair (qj0 , p0j ) XOR bids qj0 = oi qi .handle bidders/ holds bundle multiplek . bidderjml,1)O,orderallocation constructb = max( (nt)2jkml0t-round need bidders receive multiples b0 = max( (nt), 1),2l0 = (oi qi ). However, notice b0 b, number equi-size bundlesleast same. Hence, assigning bidder/ number equi-sizebundles O, bidder holds least value O. lemma follows sincealgorithm considers newly constructed allocation.therefore following theorem:1Theorem 3.5 exists truthful VCG-based mechanism provides (1 t+1)approximation multi-unit auctions k-minded bidders time polynomial n, log m,k, every constant t.3.2 21 -Approximation Multi-Unit Auctions Black-Box AccessLet us consider multi-unit auction problem general valuations given black boxes.assume algorithm oracle access may queried vi (q),q given bundle size4 .4. analogous weakest value query combinatorial auction setting. lower boundspresented later apply query types well.91fiDobzinski & Nisandesign 12 -approximation MIR algorithm problem, yieldsincentive-compatible VCG-based mechanism. MIR approximationalgorithm2first split items n equi-sized bundles size b = n2 well single extrabundle size r holds remaining elements (thus n2 b + r = m). maximumrange algorithm optimally allocate whole bundles among n bidders.need show following two simple facts:Lemma 3.6 optimal allocation bundles found time polynomial nlog m.Lemma 3.7 Let (a1 , ..., ) optimal allocation bundles foundalgorithm, (o1 , ..., ) optimal unrestricted allocation, vi (oi ) 2i vi (ai ).proofs simple:Proof: (of Lemma 3.6): algorithm dynamic programming. calculatefollowing information every 1 n 1 q n2 : (i, q) maximumvalue obtained allocating q regular bundles among bidders 1...i,+ (i, q) maximum value obtained allocating q regular bundlesremainder bundle among bidders 1...i. entry filled polynomialtime using relations: (i, q) = maxq0 q vi (q 0 b) + (i 1, q q 0 ) + (i, q) =max(maxq0 q (vi (q 0 b) + + (i 1, q q 0 )), maxq0 q (vi (q 0 b + r) + (i 1, q q 0 ))).Proof: (of Lemma 3.7): Let us start optimal unrestricted allocation o1 ...onitems allocated (without loss generality since valuations monotone),look bidder j got largest number items oj m/n. twopossibilities: vj (oj ) i6=j vi (oi ) allocating items j (i.e. regular-sizedbundles well remainder bundle) get required 2-approximation. Otherwise,round oi nearest multiple b (i.e. full bundles), except bidder jgets nothing. valid allocation since added nb m/n itemsrounding up, deleted least m/n items removing oj , value certainlyleast i6=j vi (oi ) gives required approximation.thus proved:Theorem 3.8 exists truthful polynomial time VCG-based mechanism gives12 -approximation multi-unit auctions general valuations.4. Lower Bounds VCG-Based Mechanismsmove show mechanisms essentially achieve best approximationratios possible. say allocation (s1 , ..., sn ) complete items allocated:si = m. Consider MIR algorithm n bidders full rangecomplete allocations. I.e., 0 s1 , ..., sn1 m, i<n si never outputsallocation (s1 , ..., sn1 , i<n si ). consider set valuations everybidder vi (q) = 1 q si (and 0 otherwise). allocation valuen (s1 , ..., sn1 , i<n si ) range, allocationsvalue n 1.92fiMechanisms Multi-Unit Auctionseasily get lower bound computationally efficient MIR algorithm models considered paper. start lower bound black-boxmodel. lower bound number queries bidders must queried,holds type query i.e., general communication setting.Proposition 4.1 Let MIR algorithm multi-unit auctions achieves approximation ratio better 12 . Then, communication complexity (m).Proof: case two bidders, optimal algorithm known communicationcomplexity (m):Lemma 4.2 (Nisan & Segal, 2006) Finding optimal allocation multi-unit auctions requires (m) bits communication, even two bidders evenfinding value allocation. lower bound also applies nondeterministicsettings.Thus, MIR algorithm 2 bidders uses o(m) bits communicationnon-optimal thus, argued above, gives better 12 -approximation. case2 bidders follows setting valuations first two 0,considering allocations items allocated first two bidders.second result rules existence FPTAS k-minded bidders.words, dependence running time 1 cannot made polynomial. resultessentially applies models allow single-minded bidders, e.g., XOR bids,bidding language used Kothari et al. (2005) (see next section description).Proposition 4.3 Let MIR algorithm achieves approximation ratio better(1 n1 ). Then, run polynomial time, unless P = N P .Proof: Similarly previous proposition, standard reduction knapsackmulti-unit auctions, follows every polynomial-time MIR algorithm existlarge enough n range complete allocations full, unless P = N P .lemma follows discussion above.concludes proof lower bounds MIR mechanisms. drawconclusion VCG-based mechanisms, one technical detail explicitly mentioned. lower bounds MIR algorithms, VCG-based mechanismsproved give algorithms equivalent MIR algorithms. See Dobzinski Nisan(2007). However, proofs hold even finding value optimal allocationthus directly apply also algorithms equivalent MIR algorithms.5. General Construction Applicationspresent general construction takes maximal-in-range algorithm constantnumber bidders bidding language model5 , extends truthful mechanism unbounded number bidders. Yet, extension loses arbitrarilysmall constant approximation ratio.5. model mean, e.g., restriction valuations accessed.93fiDobzinski & Nisandescribe four applications construction. First, reprove PTAS kminded bidders 12 -approximation algorithm general bidders Section 3. Then,study bidding language considered Kothari et al. (2005). Kothari et al. describeapproximately truthful FPTAS bidding language, present truthfulVCG-based PTAS (this best possible since Section 4 essentially rules possibility VCG-based FPTAS). Finally, present truthful ( 34 +)-approximation mechanismcase valuations sub-additive (a.k.a. complement free) accessed viablack box.5.1 SettingFix bidding language model multi-unit auction biddersanswer value queries. Let maximal-in-range algorithm biddersitems model. Denote complexity A(t, m), range RA,t,m ,approximation guarantee 1.Construction1. Build set Q allocations follows:(a) Let u = (1 +12n ).Let L = {0, 1, buc, bu2 c, . . . , ublogu mc , m}.(b) every set bidders, |T | t, l L:i. Run l items set bidders. Denote si numberitems allocates bidder .ii. Split remaining l items 2n2 bundles, consistingmax( 2nl 2 , 1) items.iii. Find optimal allocation equal-size bundles among bidders. Denote si allocation bidder/ T.iv. Add (s1 , . . . , sn ) Q.2. Output allocation highest welfare Q.Theorem 5.1 exists range allocations R construction maximal R. construction runs time poly(log m, n, A(t, m)) every constant t.1outputs allocation value ( t+1) optimal allocation.Proof:make use following definition:Definition 5.2 allocation (R, t, l)-round if:R set allocations, R R bidders allocated non-emptybundles. bidders allocated together l items.exists set bidders, |T | t, bidders allocatedaccording allocation R.94fiMechanisms Multi-Unit Auctionsbidder/ receives exact multiple max( 2nl 2 , 1) units,/ sil2max( 2n2 , 1) n .hard verify construction always outputs allocationsfollowing range:R = {S|S (RA,k,ml , k, l)-round allocation l L k t}Lemma 5.3 Let (o1 , . . . , ) optimal unrestricted allocation. exists alloca1tion (s1 , . . . , sn ) R vi (si ) ( t+1) vi (oi ).Proof:Denote value optimal solution OPT. Without loss generality,assume items allocated v1 (o1 ) . . . vn (on ). Let l L largestl ti=1 oi . Let (s1 , . . . , st ) allocation outputs runbidders 1, . . . , l items, assign bidder 1 si items. Observeti=1 vi (si ) ti=1 vi (oi ). finish proof show allocationrange recovers value bidders + 1, . . . , n one. lemmafollow since bidder i, vi (oi ) OPt+1 .Claim 5.4 Step 1(b)iii returns allocation (st+1 , . . . , st ) l items bidders +1, . . . , n bidder set, one, vi (si ) vi (oi ).Proof: Let r = ni=t+1 oi . Let j + 1 bidder oj nr (observeexistence bidder guaranteed). Let l L largest l ti=1 oir(observe l r). also l r 2n: chose largest possible valuerrl, therefore l (1+ 1 ) r 2n .2nNow, 6= j round oi nearest multiple max( 2nl 2 , 1),allocate items bidder j. Observe bidders bidder j bundle sizeget increases. Also observe number additional items allocate bidderslrll{t + 1, . . . n} 2nl 2 n = 2n. Thus, l r 2n+ 2nr oj + 2n.left show construction runs polynomial time:Lemma 5.5 optimal allocation R found time poly(log m, n, A(t, m)),every constant t.Proof: Step 1(b)iii construction implemented using dynamic programmingsimilarly Lemma 3.3; optimality allocation R clear. running timepoly(log1+ 1 nk A(t, m)), polynomial relevant parameters every2n+1constant t.5.2 Applications Constructionprovide several applications construction.95fiDobzinski & Nisan5.2.1 PTAS k-Minded Valuationsreprove PTAS k-minded bidders Section 3: multi-unit auction problemitems k-minded bidders optimally solved exhaustive searchtime poly((tk)t , log m), polynomial log k every constant t.construction (and since optimal algorithm particular maximal range), get1PTAS k-minded bidders: every constant t, get (1 t+1)-approximation timepolynomial n log m.5.2.2 12 -Approximation General Valuationsobserve multi-unit auction one bidder optimally solvedallocating items single bidder. let = 1 statement Theorem 5.1,get 12 -approximation algorithm general valuations.5.2.3 PTAS Marginal Piecewise Bidding Languagefollowing marginal piecewise bidding language used Kothari et al. (2005):valuation v determined list k tuples denoted (u1 , m1 ),. . . ,(uk , mk ).assume mi non-negative uk > . . . > u1 = 1. tuples determinemarginal utility jth item. words, determine value setitems, sum marginal utilities. I.e., item j, ul j < ul+1 , letmarginal utility rj = ml , every m, let v(s) = sj=1 rj . (In fact,bidding language powerful one described Kothari et al. (2005),allows marginal-decreasing piecewise valuations.)show optimally solve multi-unit auction problem settingconstant number bidders. PTAS follows, k-minded case.say bidder precisely assigned allocated si items, ui > 0exists tuple (si , ui ) k bids. main observationoptimal solution (o1 , . . . , ) one bidder precisely assigned: supposetwo bidders i0 precisely assigned. Then, move items bidderhigher (or equal) marginal utility. value allocation cannot decrease.Continue process bidders one precisely assigned.optimally solving multi-unit auction problem constant number biddersobvious: select n bidders turn bidder preciselyassigned. iteration, let bidder precisely assigned, goallocations bidders precisely assigned. Then, assign bidderremaining items. Since k possible sets make bidder preciselyassigned, algorithm runs time poly(n (n k)n1 , log m), polynomial logk every constant n.5.2.4 ( 43 + )-Approximation Subadditive Valuationsmodel assume valuations given black boxes (as Section 3.2),valuation v bundles 0 s, v(s) + v(t) v(s + t).valuations called subadditive valuations.96fiMechanisms Multi-Unit AuctionsLet us describe algorithm provides approximation ratio 34setting number bidders constant. construction, get ( 34 + )approximation VCG-based mechanism unbounded number bidders, every constant. algorithm quite simple: Fix small enough constant > 1 ( = 43 suffices).bidders, one, receive bundle power (includingempty bundle). bidder get bundle size power receivesremaining items. use exhaustive search find optimal allocation range.see algorithm indeed provides approximation ratio 34 , consideroptimal solution (o1 , . . . , ). Without loss generality, assume o1 . . . (noticeunlike bidders ordered bundle size). Let setbidders odd indices, E set bidders even indices.analysis divided two cases. First suppose iO vi (oi ) iE vi (oi ).Consider following allocation: bundles bidders E rounded powernear o2i , bundles bidders \ {1} rounded nearest power, bidder 1 gets remaining items. Notice allocation valid sincesmall enough choice assign bidders items removedbidders E. Also notice allocation range. approximationratio, observe bidders hold least value optimal solution,since bidder allocated least number items optimalsolution. addition, bidder E holds least half items allocatedoptimal solution. Thus, subadditivity, bidders E hold together least halfvalue hold optimal solution. total, value allocation obtainedalgorithm least 34 value optimal solution.Let us handle case iO vi (oi ) < iE vi (oi ). Consider allocationbidders rounded power near o2i , biddersE rounded nearest power (except one arbitrary bidder Egets remaining items). allocation range, analysis similarprevious case, leaving us approximation ratio 34 also current case.running time algorithm poly(n (log m)n1 ), polynomial logconstant n .Notice approximation ratio achieved almost best possible, every MIRapproximation algorithm guarantees factor better 43 requires (m) communication: Lemma 4.2 finding optimum solution multi-unit auction two biddersrequires (m) bits communication. make valuations sub-additive definingv new valuation: v 0 (s) = v(s) + v(m), 6= 0. Thus, Section 4,range every polynomial-time MIR mechanism two bidders subadditive valuationscannot contain complete allocations. Fix MIR algorithm, let (s1 , s1 )complete allocation range. Consider following instance: bidder 1values least s1 items value 2, smaller bundles value 1, bidder2 values least s2 items value 2, smaller bundles value 1 (and0 value empty bundle). Notice valuations bidders indeedsubadditive. Also observe optimal welfare 4, mechanism achievewelfare 3.97fiDobzinski & NisanAcknowledgmentspreliminary version paper appeared EC07. grateful Liad BlumrosenAhuva Mualem helpful discussions. second author supported grantIsraeli Science Foundation.ReferencesBlumrosen, L., & Dobzinski, S. (2007). Welfare maximization congestion games. IEEEJournal Selected Areas Communications, 25 (6), 12241236.Briest, P., Krysta, P., & Vocking, B. (2005). Approximation techniques utilitarianmechanism design. STOC, pp. 3948.Dobzinski, S., & Nisan, N. (2007). Limitations vcg-based mechanisms. STOC, pp.338344.Dobzinski, S., Nisan, N., & Schapira, M. (2005). Approximation algorithms combinatorial auctions complement-free bidders. STOC, pp. 610618.Holzman, R., Kfir-Dahav, N., Monderer, D., & Tennenholtz, M. (2004). Bundling equilibrium combinatrial auctions. Games Economic Behavior, 47, 104123.Kothari, A., Parkes, D. C., & Suri, S. (2005). Approximately-strategyproof tractablemulti-unit auctions. Decision Support Systems, 39, 105121. Special issue dedicatedFourth ACMConference Electronic Commerce.Lavi, R., Mualem, A., & Nisan, N. (2003). Towards characterization truthful combinatorial auctions. 44th Annual IEEE Symposium Foundations ComputerScience (FOCS).Lavi, R., & Swamy, C. (2005). Truthful near-optimal mechanism design via linearprogramming. FOCS, pp. 595604.Mualem, A., & Nisan, N. (2002). Truthful approximation mechanisms restricted combinatorial auctions. AAAI-02.Nisan, N. (2006). P. Cramton Y. Shoham R. Steinberg (Editors), CombinatorialAuctions. Chapter 1. Bidding Languages. MIT Press.Nisan, N., & Ronen, A. (2007). Computationally feasible vcg mechanisms. J. Artif. Intell.Res. (JAIR), 29, 1947.Nisan, N., & Segal, I. (2006). communication requirements efficient allocationssupporting prices.. Journal Economic Theory.Vickrey, W. (1961). Counterspeculation, auctions competitive sealed tenders. JournalFinance, 837.98fiJournal Artificial Intelligence Research 37 (2010) 189-246Submitted 10/2009; published 02/2010Action Theory ChangeIvan Jose Varzinczakivan.varzinczak@meraka.org.zaMeraka Institute, CSIRPretoria, South AfricaAbstracthistorically acknowledged Reasoning Actions Change community,intuitiveness logical domain description cannot fully automated. Moreover, likelogical theory, action theories may also evolve, thus knowledge engineers needrevision methods help accommodating new incoming information behavioractions adequate manner. present work changing action domaindescriptions multimodal logic. contribution threefold: first revisit semanticsaction theory contraction proposed previous work, giving robust operatorsexpress minimal change based notion distance Kripke-models. Secondgive algorithms syntactical action theory contraction establish correctnessrespect semantics action theories satisfy principle modularityinvestigated previous work. Since modularity ensured every action theoryand, show here, needs computed evolution domaindescription, represent limitation method studied. Finallystate AGM-like postulates action theory contraction assess behavioroperators respect them. Moreover, also address revision counterpartaction theory change, showing benefits semantics contraction.1. IntroductionConsider intelligent agent designed perform rationally dynamic world, supposereason dynamics automatic coffee machine (Figure 1).NiceCaf$Figure 1: coffee deliverer agent.Suppose, example, agent believes coffee always hot beverage.Suppose day gets coffee machine observes cold.case, agent must change beliefs relationship twopropositions hold coffee hold hot drink. example instanceproblem changing propositional belief bases largely addressedc2010AI Access Foundation. rights reserved.fiVarzinczakliterature belief revision (Alchourron, Gardenfors, & Makinson, 1985; Gardenfors,1988; Hansson, 1999) belief update (Katsuno & Mendelzon, 1992).Next, let agent believe whenever buys coffee machine, getshot drink. means every state world follows execution buyingcoffee, agent ends hot drink. Now, situation machinerunning cups, buying, coffee runs shelf agent, contraryexpecting, hold hot drink hands.Imagine agent never considered relation buying coffeemachine service availability, sense always believed (quite reasonably)buying prevent users using machine. Nevertheless, somedayagent queuing buy coffee observes agentbought, machine went order (maybe due lack coffee powder).Completing agents struggle discovering intricacies operating coffee machine, let us suppose always believed token, possiblebuy coffee, provided preconditions like close enough button,free hand, etc, satisfied. Eventually, due blackout, agent realizesmanage buy coffee, even token.last three examples illustrate cases changing beliefs behavioraction buying coffee mandatory. first one, buying coffee, believeddeterministic outcome, namely always hot drink, seen nondeterministic or, alternatively, different effect specific context (e.g.cup machine). second example, buying coffee knownside-effects (ramifications) one aware of. Finally, last example,feasibility action concern questioned light new information showingcontext known preclude execution.cases theory change important one deals logical descriptionsdynamic domains: may always happen one discovers action actuallybehavior different one always believed had.now, theory change studied mainly knowledge bases classical logics,terms revision update. Since work Fuhrmann (1989), recent studies considered realm modal logics, viz. epistemic logic (Hansson, 1999) dynamic logics (Herzig, Perrussel, & Varzinczak, 2006). Recentlystudies investigated revision beliefs facts world (Shapiro, Pagnucco,Lesperance, & Levesque, 2000; Jin & Thielscher, 2005) agents goals (Shapiro,Lesperance, & Levesque, 2005). scenario, would concern instance truthtoken given state: agent believes token, actually wrongthat. might subsequently forced revise beliefs currentstate affairs change goals according perform state.belief revision operations modify agents beliefs action laws.hand, interested exactly modifications. Starting BaralLobos work (1997), recent studies done issue (Eiter, Erdem, Fink,& Senko, 2005) domain descriptions action languages (Gelfond & Lifschitz, 1993).take step direction propose method robustintegrating notion minimal change complying postulates theory change.190fiOn Action Theory Changepresent text structured follows: Section 2 establish formal background used throughout article. Sections 36 core work:Section 3 present central definitions semantics action theory change, providing justifications design choices made (Section 4). Section 5 devotedsyntactical counterpart operators Section 6 proof correspondence semantics certain acceptable conditions. Section 7 discusspostulates contraction/erasure present semantics action theory revision(Section 8). discussion comparison existing work field (Section 9),conclude overview future directions research.2. Logical PreliminariesFollowing tradition Reasoning Actions Change (RAC) community,consider action theories finite collections statements particularform (Shanahan, 1997):context, effect every execution action (effect laws);precondition, action executable (executability laws).Statements mentioning action represent laws underlying structureworld, i.e., possible states (static laws).Several logical frameworks proposed formalize statements (Shanahan,1997). Among prominent ones first-order based Situation Calculus (McCarthy & Hayes, 1969; Reiter, 2001), family Action Languages (Gelfond & Lifschitz,1993; Giunchiglia, Kartha, & Lifschitz, 1997), Fluent Calculus (Thielscher, 1997),Propositional Dynamic Logic (PDL) (Harel, Tiuryn, & Kozen, 2000) different specific extensions thereof (De Giacomo & Lenzerini, 1995; Castilho, Gasquet, & Herzig, 1999;Zhang & Foo, 2001; Castilho, Herzig, & Varzinczak, 2002).opt formalize action theories using multimodal logic Kn (Popkorn, 1994).Among main reasons choice are:benefit well defined semantics multimodal logics which,going see sequel, provides simple intuitive foundations buildmeaning changing action domain descriptions.Kn syntax allows us express afore mentioned types laws without requiringfull expressiveness PDL machinery first-order language.Since Kn core mentioned PDL-based action formalisms, shallsay sequel smoothly transfer them.Contrary first-order based approaches, Kn decidable several implementedtheorem provers available literature.191fiVarzinczak2.1 Action Theories Multimodal LogicLet Act = {a1 , a2 , . . . , } set atomic action constants given dynamicdomain. example atomic action buy. atomic action associatedmodal operator [a]. suppose multimodal logic independently axiomatized (Kracht & Wolter, 1991), i.e., logic fusion interactiondifferent modal operators.1Prop = {p1 , p2 , . . . , pn } denotes finite set propositional constants, also called fluentselementary atoms. Examples token (the agent token) coffee(the agent holds coffee). Lit = {p, p : p Prop} set literals. use `denote literal. ` = p, identify ` p. |`| denote atom `.use small Greek letters , , . . . denote Boolean (propositional) formulas.recursively defined usual way:::= p | > | | | | | |( denotes ( ) ( ).) Fml set Boolean formulas. exampleBoolean formula coffee hot. propositional valuation v maximal consistent setliterals. denote v fact v satisfies propositional formula . val()denote set valuations satisfying . CPL denote Classical PropositionalLogic |=CPL respective consequence relation. Cn() denotes logical consequencesCPL, i.e., Cn() = { : |=}.CPLpropositional formula, atm() denotes set elementary atoms actuallyoccurring . example, atm(p1 (p1 p2 )) = {p1 , p2 }.Boolean formula, IP() denotes set prime implicants (Quine, 1952),i.e., weakest terms (conjunctions literals) imply . example, IP(p1 p2 ) ={p1 p2 , p1 p2 }. prime implicants, properties computethem, see chapter Marquis (2000). denote prime implicant, given `, ` abbreviates ` literal . given set X, X denotes complement.Hence atm() denotes Prop \ atm().denote complex formulas (possibly modal operators) , , . . .recursively defined following way:::= | [a] | | | | |hai dual operator [a], defined hai =def [a]. instance complexformula scenario example coffee [buy]coffee.Given complex formula , act() denote action names occurring ,i.e., modalities . example, act([a2 ]p1 ([a1 ]p2 [a2 ]p3 )) = {a1 , a2 }.semantics standard semantics multimodal logic Kn (Popkorn, 1994).Definition 2.1 (Kn -Model) Kn -model tuple = hW, Ri W set valuations (also called possible worlds), R maps action constants accessibility relationsRa W W.1. Later see requirement ensure action theory modular.192fiOn Action Theory Changeexample, Act = {a1 , a2 } Prop = {p1 , p2 }, Kn -model =hW, Ri,W = {{p1 , p2 }, {p1 , p2 }, {p1 , p2 }},({p1 , p2 }, {p1 , p2 }), ({p1 , p2 }, {p1 , p2 }),R(a1 ) =({p1 , p2 }, {p1 , p2 }), ({p1 , p2 }, {p1 , p2 })R(a2 ) = {({p1 , p2 }, {p1 , p2 }), ({p1 , p2 }, {p1 , p2 })}Figure 2 gives graphical representation model .a1w1p1 , p2w2a1p1 , p2a2:a1a1w3p1 , p2a2Figure 2: Example Kn -model Act = {a1 , a2 }, Prop = {p1 , p2 }.Notice definition Kn -model follow traditional notion modallogics: two worlds satisfy valuation. pragmatic choice,see Section 5. Nevertheless, shall say sequel straightforwardlyformulated standard Kn models well.Definition 2.2 (Truth Conditions) Given Kn -model = hW, Ri,|=p (p true world w ) iff w p (valuation w satisfies p, i.e., p w);w|=[a] iff |= 0 every w0 (w, w0 ) Ra ;ww|=iff |=|=;www|=iff 6|=w , i.e., |=w ;wtruth conditions connectives usual.denote (possibly empty) set Kn -models.Kn -model model (denoted |= ) w W, |=.wmodel depicted Figure 2, |= p1 [a2 ]p2 |= p1 p2 . model setformulas (noted |= ) |= every . set formulasstart (our non-logical theory), called global axiom.193fiVarzinczakDefinition 2.3 (Global Consequence) formula global consequence setglobal axioms class Kn -models (noted |=) everyKnKn -model , |= , |= .Kn state laws describing behavior actions. One waystating formulas global axioms.2 usually done RAC community (Shanahan, 1997), distinguish three types laws. first kind statements staticlaws, constraints allowed states dynamic domain.Definition 2.4 (Static Law) static law global axiom Fml.example static law coffee hot, saying agent holds coffee,holds hot drink. Situation Calculus formalism (Reiter, 2001) one would writefirst-order formula s.[coffee(s) hot(s)]. set static laws scenario denotedFml. example = {coffee hot}.second kind action law consider given effect laws. formulasrelating action effects, conditional.Definition 2.5 (Effect Law) Let , Fml. effect law action global axiomform [a].consequent effect always obtains accessible states (which needexist general) action executed state antecedent holds.Kripke semantics, means every possible world holds, every transitiona-labeled arrow (if any) leads possible world holds. nondeterministic action, consequent typically disjunction. example effectlaw coffee [buy]coffee, saying situation agent coffee,buying, agent coffee. inconsistent, special kind effectlaw call inexecutability law. example, could also token [buy],expressing buy cannot executed agent token. Situation Calculus examples effect inexecutability laws would expressed respectivelys.[coffee(s) coffee(do(buy, s))] s.[token(s) Poss(buy, s)].set effect laws given scenario denoted E . coffee machine scenario,could example:coffee [buy]coffee,token [buy]token,E =token [buy]Finally, also define executability laws, stipulate context actionguaranteed executable. Kn , operator hai used express executability. hai>thus reads execution possible. Formally, hai> true world w meansleast one world w0 accessible w via Ra (cf. Definition 2.2).2. alternative given Castilho et al. (1999, 2002), laws stated aidextra universal modality local consequence thus considered.194fiOn Action Theory ChangeDefinition 2.6 (Executability Law) Let Fml. executability law actionglobal axiom form hai>.instance, token hbuyi> says buying executed whenever agenttoken. set executability laws given domain denoted X . scenarioexample X = {token hbuyi>}.Note principle one needs know nothing accessible world w0 . However,common (albeit tacit) assumption RAC community state executabilitylaws actions know effects, words act(X ) act(E ).Situation Calculus example would stated s.[token(s) Poss(buy, s)].However, point that, traditionally, Reiter basic action theories (Reiter, 2001)executability laws inexecutability laws mixed together form bi-conditionalslike s.[token(s) Poss(buy, s)], called precondition axioms. critiquepractice implications formalizing dynamic domains, see work HerzigVarzinczak (2007).three basic types laws, able define action theories:Definition 2.7 (Action Theory) Given (possibly empty) sets laws , E , X ,= E X action theory.Given action theory action a, Ea (resp. Xa ) denote seteffect (resp. executability) laws E (resp. X ). Ta = Ea Xa actiontheory a.worth noting a1 , a2 Act, a1 6= a2 , intuition indeed Ta1 Ta2overlap , i.e., laws common Ta1 Ta2 lawsstructure world. requirement somehow related underlyingmodal logic independently axiomatized (see note above).2.2 Frame, Ramification Qualification Problemslast 40 years, effort reasoning actions communitydevoted searching satisfactory solutions frame problem, ramificationproblem qualification problem.Roughly speaking, frame problem (McCarthy & Hayes, 1969) relates needinferring persistence facts world execution action knownaffect them, without state explicitly form frame axioms.(Frame axioms special type effect law, form ` [a]`, ` Lit.)example, buying coffee context agent already got onemake lose coffee: coffee [buy]coffee consequence theory.ramification problem (Finger, 1987) comes observation action mayseveral possibly interdependent effects stating explicitly hugetask. scenario, want able infer [buy]hot without saying theory,way intrinsic causal connection coffee hot takenaccount. Finally, qualification problem (McCarthy, 1977) amounts addressingissue ensuring action executable given context. Specifying sufficient195fiVarzinczakconditions action executable incredibly hard task. example, onemay state token hbuyi>, may well case buying fails duecondition unforeseen design time, like agents arm rusty stuck.core problems RAC community, reader referredbook Shanahan (1997).sake clarity, abstract frame ramification problems,suppose agents theory already entails relevant frame axioms. pointhowever shall say could defined within formalism solutionframe ramification problems. instance, could used suitable solutionframe problem, like e.g. dependence relation (Castilho et al., 1999), usedwork Herzig et al. (2006), kind successor state axioms slightly modifiedsetting (Demolombe, Herzig, & Varzinczak, 2003). make presentation clearreader, bother particular solution frame problemassume frame axioms inferred action theory. Actuallysuppose intended frame axioms automatically recovered statedtheory, specifically, set effect laws.Given largely acknowledged difficulty qualification problem literature (Shanahan, 1997), assume priori solution it. Instead, tacitlyassumed many approaches reasoning actions (Castilho et al., 1999; Zhang &Foo, 2001; Reiter, 2001), suppose knowledge engineer may want state(not necessarily fully specified) executability laws actions. may incorrectstarting point (and probability be), revising wrong executabilitylaws approach towards solution one aims work.information knowledge engineer chance change eventuallycorrespond intuition (cf. Sections 3 8).agreed points, action theory example be:coffeehot,tokenhbuyi>,coffee [buy]coffee,T=token [buy]token, token [buy],coffee [buy]coffee, hot [buy]hot(We stated frame axiom token [buy]token triviallydeduced inexecutability law token [buy].)Figure 3 shows Kn -model action theory above.going see sequel finite base formalizing action theoryplays role contraction laws. particular, base representing static lawsturns quite important. given action theory T, useful considermodels whose possible worlds possible valuations allowed :Definition 2.8 (Canonical Frame) Let = E X action theory.tuple Mcan = hWcan , Rcan canonical frame if:Wcan = val(S );Rcan = aAct Ra s.t. Ra = {(w, w0 ) : [a] Ea , |=w , |= 0 }.w196fiOn Action Theory Changew1t, c, hbbw3w2:t, c, ht, c, hbw4t, c, hFigure 3: model coffee machine scenario: b, t, c, h stand for, respectively,buy, token, coffee, hot.canonical frame action theory need one models. witness why,let Prop = {p}, Act = {a}, consider simple action theory {p [a], p hai>}.associated canonical frame Wcan = {{p}, {p}}. Clearly world {p}satisfy theory.Definition 2.9 (Canonical Model) canonical modelcanonical frame |= T.Figure 4 shows canonical model action theory example T.w1t, c, hb:bw3w2t, c, hw5bt, c, hw6w4t, c, ht, c, ht, c, hFigure 4: canonical model coffee machine scenario.2.3 Prime Valuationssay atom p essential formula p atm(0 ) every 0|=0 . instance, p1 essential p1 (p1 p2 ). Given , atm!()CPLdenotes set essential atoms . (If contingent, i.e., tautologycontradiction, atm!() = .)Given Boolean formula, set formulas 0 |=0CPL0atm( ) atm!(). ForV instance, p1 Vp2/ p1 , p1 |=p p2 atm(p1 p2 ) 6CPL 1atm!(p1 ). Clearly, atm( ) = atm!( ), moreover whenever |=CPL 0 case,atm!() = atm!(0 ) also = 0 .197fiVarzinczakTheorem 2.1 (LeastVAtom-Set Theorem, Parikh, 1999) Let propositional formula. |=CPL , every 0 |=CPL 0 , atm() atm(0 ).proof theorem given Makinson (2007) state here.Essentially, theorem establishes every Boolean formula , uniqueleast set elementary atoms may equivalently expressed using atomsset. Hence, Cn() = Cn().Given valuation v, v0 v subvaluation. Given set valuations W, subvaluationv0 satisfies propositional formula modulo W (noted v0 W ) vv W v0 v. say subvaluation v essentially satisfies (modulo W),!!noted v W , v W {|`| : ` v} atm!(). v W , call vessential subvaluation (modulo W).Definition 2.10 (Prime Subvaluation) Let Boolean formula W set val!uations. subvaluation v prime subvaluation (modulo W) vW!v0 v v0 W .prime subvaluation formula thus one weakest states truthtrue. Hence, prime subvaluations another way seeing prime implicants (Quine,1952) . base(, W) denote set prime subvaluations modulo W.Proposition 2.1 WLet FmlVand W set valuations. w W, ww vbase(,W) `v `.Proof: Right left direction straightforward. left right direction, w ,0w . Let w0 w least subset w stillVsatisfying . Clearly, w primesubvaluation modulo W, w `w0 `, result follows.22.4 Closeness Modelscontracting formula model, perform change structure.several different ways modifying model (not minimal), neednotion distance models identify closest original one.going see depth next section, changing model amountsmodifying possible worlds accessibility relation. Hence, distancetwo Kn -models depend upon distance sets worlds accessibilityrelations. based symmetric difference sets, definedX = (X \ ) (Y \ X).Definition 2.11 (Closeness Kn -Models) Let = hW, Ri model.0 = hW0 , R0 least close 00 = hW00 , R00 i, noted 0 00 ,either WW0 WW00 ;WW0 = WW00 RR0 RR00 .198fiOn Action Theory Changeextension Burger Heidemas relation (Burger & Heidema, 2002)modal case. defines lexicographic order set Kn -models. Although simple,notion closeness turns sufficient purposes here, shall seesequel. Notice notions distance models could consideredwell, namely cardinality symmetric differences Hamming distance. (See Section 4discussion this.)3. Semantics Action Theory Changeadmitting possibility law failing, one must ensure becomes invalid,i.e., true least one model dynamic domain formalized.lots models, may set models (potentially) valid.Thus contracting amounts making longer valid set models.operations must carried achieve that? Throwing modelswork, since keep valid models remaining set. Thus oneadd new models M. models? Well, models true.models: taking models falsifying different originalmodels certainly violate principle minimal change.Hence, shall take model basis manipulate get new model0true. modal semantics, removal law model= hW, Ri means modifying possible worlds accessibility relationbecomes false. operation gives result set modelslonger model . several candidates, ones choose?shall take models minimal modifications original , i.e.,minimal respect distance models. course,one 0 minimal respect . case, addingone new models enough invalidate , take possible combinations{M 0 } expanding original set models one minimal models.(Observe approach relates orderly maxichoice contraction Hansson, 1999.)result set sets models. set models precisely one model0 falsifying .might claimed that, such, contraction method describedrespect so-called principle categorical matching: input output differentsorts objects, namely set models set sets models. easy see,however, reasoning stated way output setmodels corresponds precisely result one contraction operator, satisfyingreferred principle. choice defining result operation set possibleoutputs become clear Section 5, going present algorithmscorrespond exactly semantic constructions.3.1 Model Contraction Executability Lawscontract executability law hai> one model, intuitively removetransitions leaving -worlds. order succeed operation, guaranteeresulting model least one -world departing a-arrow.199fiVarzinczakDefinition 3.1 Let = hW, Ri. 0 = hW0 , R0 Mhai>W0 = W;R0 R;;(w, w0 ) R \ R0 , |=wM0w W0 6|=w hai>.Observe Mhai>6= satisfiable W. Moreover, Mhai>6|= hai>.provide reader insight operation would carriedSituation Calculus, one look given situation holdsmodify interpretation predicate Poss(a) becomes false s.Like case, may many situations must takenaccount. essential difference Kripke structures always finite,whereas space situations possibly infinite (Reiter, 2001).get minimal change, want operation removing transitions minimalrespect original model: one remove minimum set transitionssufficient get desired result.Definition 3.2 contract(M , hai>) =min{Mhai>, }define sets possible models resulting contractionexecutability law set models:Definition 3.3 Let set models, hai> executability law.0000hai> = {M : = {M }, contract(M , hai>), M}running example, consider = {M }, model Figure 4.agent discovers even token manage buy coffee more,change models order admit (new) models states tokencase buy-transition all. oneworld new model enough, taking resulting models whose accessibilityrelations maximal guarantees minimal change. Hencetokenhbuyi> =0000{M {M1 }, {M2 }, {M3 }}, Mi depicted Figure 5.Clearly, satisfied M, i.e., |= M, contractionhai> succeed. line expectations relatesSuccess Postulate (cf. Section 7.2).200fiOn Action Theory Changew1w1t, c, ht, c, hbM10 :bw3w2t, c, hw5w6w4t, c, ht, c, hbt, c, hw6w4w5t, c, ht, c, hw3w2M20 :t, c, hbt, c, ht, c, ht, c, hw1t, c, hbM30 :bw3w2t, c, ht, c, hw5w6w4t, c, ht, c, ht, c, hFigure 5: Models resulting contracting token hbuyi> model Figure 4.3.2 Model Contraction Effect Lawsagent discovers may cases buying getshot drink, must e.g. give belief effect law token [buy]hot setmodels. means token hbuyihot shall admitted least one worldnew models set beliefs. Therefore, contract effect law [a]given model, intuitively add new transitions -worlds worldssatisfying . shall see, great challenge operation preciselyguarantee minimal change.example, contracting token [buy]hot model Figure 4 shalladd transitions token-worlds hot-worlds. coffee hot static lawhot coffee, also give us hbuyicoffee token-world (coffeecausally relevant hot, i.e., hot must also coffee). meansallow hbuyihot token-world, also allow hbuyicoffeeworld. argument necessarily hold token: allowing hbuyihotnecessarily oblige us allow hbuyitoken respective world.token relevant hot (as coffee is). means freedom eitherallow not.Hence, running example add transitions token-worlds hotcoffee token-worlds, well hot coffee token. situation depictedFigure 6. instance, add new buy-arrow world {token, coffee, hot}one candidates (Figure 7).Situation Calculus, modification would slightly different,intuition behind: one look given situation holdsmodify interpretation fluents (atoms) do(a, s), situation resultingperforming s. Alternatively, new -situations lead least one -situation.201fiVarzinczakw1t, c, hb:bw3w2t, c, hw5t, c, hbw6w4t, c, ht, c, ht, c, hFigure 6: Candidate worlds receive transitions coming token-worlds.w1t, c, hb:bw3w2t, c, hw5bt, c, hw6w4t, c, hbt, c, ht, c, hbFigure 7: Two candidate new buy-arrows falsify token [buy]hot .Notice however would require addition new whole branches tree-likefirst-order model induced Reiter basic action theories (Reiter, 2001).Back example, observe adding new transition {token, coffee, hot}would make us lose effect token, true every execution buy originalmodel (|= token [buy]token). preserve law allowing newtransition hot-world? is, get rid effect hot without losing effectsrelevant that? develop approach issue.adding new transition leaving world w intuitively want preserve manyeffects so. achieve this, enough preserve old effectsw (because remaining structure model remains unchanged addingnew transition). course, cannot preserve effects inconsistent (thoselost). Hence suffices preserve effects consistent .achieve must observe true w target world w0 :proper effects action world w0 , i.e., changes w w0 (w0 \ w)new execution must obliged so: eitherliterals change w w0 necessary w0 (like coffeeexample) necessary another effect (independent, like token) world w0 .202fiOn Action Theory Changenon-effects action world w0 , i.e., change w w0(w w0 ) new execution allowed so: certainliterals never preserved (like token example), pointing newtransition towards world change respect leaving world(hot coffee token example), may lose effects held wadding transition.means things allowed change candidate target world mustforced change, either non-related lawmodulo set states W. words, want literals (now) changew w0 sufficient get modulo W, preservingmaximum effects. Every change beyond intended one. Similarly,want literals w (now) preserved target world w0usually preserved given set models. Every preservation beyondmay make us lose law. looks like prime implicants, primesubvaluations play role: worlds new transition pointwhose difference respect departing world literals relevant whosesimilarity respect literals know change.Definition 3.4 (Relevant Target Worlds) Let = hW, Ri model, w, w0 W,set models M, [a] effect law. w0 relevant targetworld w respect [a]|=6|= 0 ;ww` w0 \ w:either v base(, W) v w0 ` v;0 Fml v0 base( 0 , W) v0 w0 , ` v0 ,every Mi M, |=w [a] 0` w w0 :either v base(, W) v w0 ` v;Mi 6|=[a]`;wRelTarget(w, [a], , M) denote set relevant target worlds wrespect [a] M.Note need set models (and suppose contains modelstheory want change) preserving effects depends effectshold models interest us. need take accountlocal operation changing one model. (The reason need definitionlocal, one model contraction executability laws Mhai>removingtransitions way losing effects, every effect law held worldtransition removed remains true world resulting model.)203fiVarzinczakDefinition 3.5 Let = hW, Ri, M. 0 = hW0 , R0M[a]W0 = W;R R0 ;(w, w0 ) R0 \ R, w0 RelTarget(w, [a], , M);M0w W0 6|=w [a].Observe M[a]6= satisfiable W. Moreover,M[a]6|= [a].one world law longer true model enough,taking resulting models whose accessibility relations minimal respectoriginal one guarantees minimal change.Definition 3.6 contract(M , [a]) =min{M[a], }define possible sets models resulting contracting effect lawset models:Definition 3.7 Let set models, [a] effect law.0000[a] = {M : = {M }, contract(M , [a]), M}Taking = {M }, model Figure 4, contracting token000[buy]hot gettoken[buy]hot = {M {M1 }, {M2 }, {M3 }},Mi0 depicted Figure 8.cases satisfiable valid , course operatorsucceed falsifying [a] (cf. end Section 3.1). Again, worksexpected Success Postulate (see also Section 7.2).3.3 Model Contraction Static Lawscontracting static law model, want admit existence leastone (new) possible state falsifying it. means intuitively add new worldsoriginal model. (In Situation Calculus setting would correspond allowingsituations satisfying domain constraints.) quite easy.delicate issue however accessibility relation: new transitionsleave/arrive new world? transition leaves new added world, may loseexecutability law. transition leaves it, may lose effect law,holding add transition pointing new world. hand,transition arrives new world, intuition? intuitiveunreachable state? (Similar issues would also arise Situation Calculus interpretations,means independent underlying formalism.)204fiOn Action Theory Changew1w1t, c, hbM10 :t, c, hbbw3w2t, c, hw3w2M20 :t, c, hbbt, c, hbt, c, hbw5w6w4t, c, hw5t, c, ht, c, hbt, c, hw6w4t, c, ht, c, hw1t, c, hbM30 :bw3w2t, c, hbt, c, hbw5w6w4t, c, ht, c, ht, c, hFigure 8: Models resulting contracting token [buy]hot model Figure 4.discussion shows drastic change static laws might be: changeunderlying structure (possible states) world! Changing mayindirect, unexpected (and probability unwanted) consequence loss effectlaw(s) executability law(s). choose type(s) lawsmay accept lose process postpone change (by operators).Following tradition RAC community, states executability lawsgeneral difficult formalize effect laws, therefore likelyincorrect (Shanahan, 1997), prefer change accessibility relation,means preserve effect laws postpone correction executability laws,required. (Remember approach towards solution qualificationproblem cf. Section 2.2 above.)One may argue things way makes three operators incoherentsense effect executability laws adopt minimal change approach, givingstronger theories, whereas static laws adopt cautious approach, giving weakertheories (see next section). worth noting however largely recognizedRAC community, different laws domain description status:minimal change approach static law contraction preserves many executabilitylaws possible, even coherent, would definitely fail cope qualification problem.Moreover, propagating wrong executability laws, coherent method would definitelyless elaboration tolerant (McCarthy, 1998) one defining regardsmodifications theory.reasons, contention static law contraction cautious.(For detailed discussion this, see Section 4.2 end Section 5.3.)205fiVarzinczakDefinition 3.8 Let = hW, Ri. 0 = hW0 , R0W W0 ;R = R0 ;M0w W0 6|=w .Note = |= . Moreover, 6|= .minimal modifications one model defined usual:Definition 3.9 contract(M , ) = min{M , }define sets models resulting contracting static lawgiven set models:Definition 3.10 Let set models, static law.0000= {M : = {M }, contract(M , ), M}scenario example, initial set models = {M }, modelFigure 4, contracting static law coffee hot would give us resulting000new set modelscoffeehot = {M {M1 }, {M2 }}, Mi depictedFigure 9 below.w1w7t, c, hbM10 :w5t, c, hbw3bM20 :t, c, hw6w4t, c, ht, c, hbw2t, c, hw1w7t, c, ht, c, ht, c, hbw3w2t, c, hbt, c, hw6w4w5t, c, ht, c, ht, c, hFigure 9: Models resulting contracting coffee hot model Figure 4.Notice modifying accessibility relation effect laws trueoriginal model preserved resulting models. ensured [buy]true new world w7 .executability laws potentially lost, due cautiousnessapproach. instance, M10 above, longer case token hbuyi>true, since world, namely w7 , satisfy anymore. (In M20executability law still true every possible world.)worth point out, however, approach indeed line intuition:learning new state possible, necessarily know behavioractions new added state. may expect action laws hold new world(see end Section 5.3), but, information dispose, touching accessibilityrelation safest way contracting static laws (cf. Section 4.2 below).206fiOn Action Theory Change4. Interludepresenting algorithmic counterpart action theory change operators,section discuss alternatives technical constructions. pointissues alternatives would raise. also provide justificationsdesign choices made previous sections.4.1 Distance Notionsdefined used model distance based symmetric difference sets (Definition 2.11). distance extension Kripke structuresWinsletts (1988) notion closeness propositional interpretations PossibleModels Approach (PMA). Instead it, however, could considered distancenotions well, like Dalals (1988) distance, Hamming distance (1950), weighted distance. Due space limitations, develop comparison amongdistances here. (For details, reader may want refer Schlechtas 2004 book.)nevertheless show cardinality-based distance, example, mayalways get intended result.Let card(X) denote number elements set X. suppose closenessKn -models defined follows:Definition 4.1 (Cardinality-based Closeness Kn -Models) Let = hW, Rimodel. 0 = hW0 , R0 least close 00 = hW00 , R00 i, noted0 00 ,either card(WW0 ) card(WW00 );card(WW0 ) = card(WW00 ) card(RR0 ) card(RR00 ).notion distance closely related Dalals (1988) closeness.contracting static law model usually add one newpossible world, easy see cardinality-based distance getresult contract(M , ) distance Definition 2.11.comes contraction action laws, changing accessibilityrelations, however, cardinality-based distance seem fit intuitions.witness, consider model Figure 10, law p1 hai> true.w1w2p1 , p2p1 , p2:w3p1 , p2Figure 10: model executability law p1 hai>.207fiVarzinczakThen, models resulting contraction p1 hai> modelMphai> = {M 0 , 00 }, 0 00 depicted Figure 11.1w2w1w1p1 , p2p1 , p2w2p1 , p2p1 , p2M0 :00 :w3w3p1 , p2p1 , p2Figure 11: Models resulting contracting p1 hai> model Figure 10.00Note 00 intended contracted model: 6|= p1 hai>. However,0cardinality-based distance get {M }p1 hai> = {{M , }}.000000{M , } result since : one transition removed,00 two.4.2 Minimal Change v. Cautiousnessusually done literature classical belief revision, defining (traditional)theory change operator one must always make fundamental decision two opposing principles guiding one: minimizing change, leadsstrong modified theories, versus cautious change, leads weak theories.regard, one might argue action theory change operators incoherent.adopt first principle contraction effect executability laws,latter principle contraction static laws.3turns out, however, view debatable. different perspective onethink three operators coherent following sense: performversion maxichoice, namely addition precisely single model original modelstheory.4case, sequel give justification behavior operatorsshow operator contraction static laws cautiouscoherent operators contraction effect executability laws. (Wesay operator static law contraction coherent respect operatorscontraction effect executability laws also performs minimal change respecttypes laws, i.e., preserves effect executability laws.)claimed incoherence come from? contentioninherent problem action theory change itself, flaw definitions.justification follows. Remembering intuitions semantic constructions,easy see contraction executability laws knowledge actionsfeasibility (the transitions) removed that. contraction effect laws,3. thank anonymous referee pointed out.4. thank another anonymous referee pointed out.208fiOn Action Theory Changepiece knowledge also added (the new transition), notice one guidedgiven concrete extra information, namely effect want allow.Now, contraction static laws, notice extra information whatsoevergiven new possible state could guide addition knowledgefeasibility action. thing know new world exist.Nothing said whether transition leaving arrivingall. property problem per se: problem removing static lawmention executabilities, reflected operator.Therefore, incoherence already problem, surprisingfind proposed operators. designed allowedgiven constraints problem. information handsregarding new added state, coherent version corresponding operator woulddefined. (See discussion Section 9 comparison Eiter et al.s 2005constraint-based method update action theories.)Proposition 4.1 minimal change operator static law contractioncoherent operators contraction effect executability laws.Proof: Suppose minimal change based (non-cautious) contraction operatorstatic laws coherent operators. operator mustcontracting Fml formulas type removed (otherwisecoherent operators). means effect executability lawspreserved. particular, operator coherent respect contractingformula p1 p2 model Figure 12 below.w1w1p1 , p2:p1 , p2w3M0 :w2w2p1 , p2p1 , p2p1 , p2Figure 12: Adding transition new added world alternative semanticsstatic law contraction. denotes original model, 0 shows newadded world candidate transition add Ra .Following intuition contraction Boolean formulas, new world, viz.valuation {p1 , p2 }, added W . operator question non-cautious,transition also added new added world {p1 , p2 } , orderpreserve executability law p1 hai>. Also operator non-cautious,effect law p1 [a]p1 preserved. Hence, new transition pointneither world {p1 , p2 } {p1 , p2 } itself. Now, direct new transition{p1 , p2 } (the world left), get model 0 Figure 12.209fiVarzinczakM0Observe |= (p1 p2 ) [a]p1 . However, 6|= (p1 p2 ) [a]p1 : operatormakes us lose effect law! means coherent. order us keepeffect law, option direct new transition {p1 , p2 }. then,transition added all: operator cautious! Hence operatorstatic law contraction based minimal change coherent operatorslaws.2result supports contention cannot coherent set minimalchange operators action theory contraction. general result holdsmodal-based approaches like ours, applies framework reasoningactions based transition systems also allows three typeslaws consider here.Furthermore, result also illustrates well difference action theory changeclassical belief change. witness, even though contraction static laws amountspropositional contraction Boolean formulas, remains special case latter.reason contracting static laws one always asks happens lawstypes?, question asked classical propositional contractionobvious reason simply types formulas.5. Syntactic Operators Contraction Lawsgiven semantic construction action theory change, turnattention definition syntactic operators changing sets formulas describingdynamic domain.Nebel (1989) says, [. . . ] finite bases usually represent [. . . ] laws,forced change theory would like stay close possible original[. . . ] base. Hence, besides definition syntactical operators, also guaranteeperform minimal change theory level. mean resultingtheory course entail law want contract theory with,also preserve much previous knowledge possible performing syntacticalmanipulations laws original theory. Ideally, knowledge engineersperspective, modified theory also keep certain degree resemblanceoriginal one: resulting laws slight modifications relevant onesoriginal action theory.denote sequel result contracting law set laws T.5.1 Contracting Executability Lawscase contracting executability law hai> action theory, firstensure action keeps executability state contextsantecedent holds, case. achieve strengthening antecedentsrelevant executability laws. Second, order get minimality, must makeexecutable contexts true, viz. -worlds one. Sincepossibly many different alternatives that, means several actiontheories outcome. Algorithm 1 gives syntactical operator achieve this.210fiOn Action Theory Changeeasily checked Algorithm 1 always terminates: input action theoryalways finite; finiteness Prop follows atm(), IP(S ).Moreover, entailment problem multimodal K decidable (Harel et al., 2000),classical propositional logic. Therefore contracting executability laws decidable.Algorithm 1: Contraction Executability LawInput: T, hai>Output:hai> /* set theories output knowledge engineer */1begin2:=hai>3|=hai> 6|=KCPLnforeach IP(S )4forall atm()VV:= pi atm() pi pi atm() pi /* extend valuation */56pi6|=( )CPL79else:= {T}hai>111213/* allowed state *//* construct theory weaker state */0 := (T \ Xa ) {(i ( )) hai> : hai> Xa }0:=hai>hai> {T }810pi/returnhai>endstraightforward see Algorithm 1 adapted Situation Calculusaction theories well. crucial point however would termination, since entailmentSituation Calculus general undecidable.running example, contracting executability law token hbuyi>000action theory would give ustokenhbuyi> = {T1 , T2 , T3 }, where:coffee hot, coffee [buy]coffee,token [buy]token, token [buy],0T1 =coffee [buy]coffee, hot [buy]hot,(token (coffee hot)) hbuyi>coffeehot,coffee[buy]coffee,token [buy]token, token [buy],0T2 =coffee [buy]coffee, hot [buy]hot,(token (coffee hot)) hbuyi>211fiVarzinczakcoffee hot, coffee [buy]coffee,token[buy]token, token [buy],T30 =coffee[buy]coffee, hot [buy]hot,(token (coffee hot)) hbuyi>knowledge engineer choose theory lineintuitions implement required changes (cf. Figure 5).5.2 Contracting Effect Lawscontracting effect law [a] action theory T, intuitively contract effect laws preclude target worlds. order cope minimality,must change laws relevant unwanted [a].Let (Ea, )1 , . . . , (Ea, )n denote minimal subsets (with respect set inclusion) Ea, (Ea, )i |=[a], 1 n. words, (Ea, )i support setKneffect law [a] T. make parallel terminology usually adoptedbelief change community, shall see (Ea, )i special type kernel (Hansson,1994) formula [a].According Herzig Varzinczak (2007), given action theory one alwaysensure least one support set [a] exists. let[Ea =(Ea, )i1inlaws Ea serve guidelines get rid [a] -world allowedtheory T: effect laws weakened allow hai -contexts.resembles classical kernel contraction (Hansson, 1994): finding minimal sets implyingformula changing them. crucial difference, however, instead completelyremoving formula kernel, weaken laws.modifying support sets, first thing must ensure actionstill effect contexts hold, case.means shall weaken laws Ea, specializing . Now, need preserveold effects -worlds one. achieve specialize lawspossible valuation (maximal consistent conjunction literals) satisfying one.Then, left -valuation, must ensure action either old effectsoutcome. achieve weakening consequent laws Ea . Finally,order get minimal change, must ensure literals -valuationforced change -worlds preserved. stating effect lawform (k `) [a]( `), k -valuation. reasonneeded clear: several -valuations, far want onereachable k -world, force one whose differencek -valuation minimal.Situation Calculus terms, syntactical operations would correspond strengthening right-hand side relevant successor state axioms and/or weakening212fiOn Action Theory Changeleft-hand side. Alternatively, done original effect axioms,recompiling new successor state axioms afterwards.output operations described set action theoriesoutput knowledge engineer. Algorithm 2 gives operator.Algorithm 2: Contraction Effect LawInput: T, [a]Output:[a] /* set theories output knowledge engineer */1begin2:=[a]3|=K [a] 6|=CPLnforeach IP(S )4forall atm()VV:= pi atm() pi pi atm() pi /* extend valuation */56pipi/6|=( )CPL7/* allowed state */0foreach IP(S )0 := \ Ea /* support sets weakened */0 := 0 {(i ( )) [a]i : [a]i Ea }891011/* allow state */0 := 0 {(i ) [a](i 0 ) : [a]i Ea }12forall L Lit|=( )CPL13V`L `)n0 := 0 {( `) [a]( `)}160:=[a][a] {T }17else:= {T}[a]19216|=( 0CPL6|=( `) [a]` ` 0K1520`L `foreach ` L1418Vreturn[a]endAgain, finiteness action theory atm(),IP(S ), decidability multimodal K (Harel et al., 2000) wellclassical propositional logic, easily verified Algorithm 2 always terminates.213fiVarzinczakTherefore, contracting effect laws decidable. course, complexity computingsupport sets well prime implicants quite high (see Section 5.4 laterdiscussion matter).example execution Algorithm 2, let us suppose want contracteffect law token [buy]hot action theory running example. Firstcompute support sets token [buy]hot (i.e., minimal subsetsEbuy together entail token [buy]hot). following:token,hot(Ebuy)1token,hot(Ebuy)2coffee [buy]coffee,coffee [buy]coffeehot [buy]hot,coffee [buy]coffee==possible context antecedent token case,token,hottoken,hot)2 . Since = {coffee hot},)1 (Ebuy= (Ebuyweaken effect laws Ebuycontexts token coffee hot, token coffee hot token coffee hot.token coffee hot: Algorithm 2 replaces laws Ebuy(coffee (token coffee hot)) [buy]coffee,(hot (token coffee hot)) [buy]hot,(coffee (token coffee hot)) [buy]coffeepreserve effects possible contexts token coffee hot. Now,order preserve effects token coffee hot-contexts allowing reachablehot-worlds, algorithm adds laws:(token coffee hot) [buy](coffee hot),(token coffee hot) [buy](hot coffee)Now, search possible combinations laws Ebuy apply token coffee hotcontexts find token [buy]token. token must true every executionaction buy, state law (token coffee hot) [buy](hot token), endfollowing theory:coffee hot, token hbuyi>,token[buy]token,token[buy],(coffee (token coffee hot)) [buy]coffee,(hot (token coffee hot)) [buy]hot,T10 =(coffee (token coffee hot)) [buy]coffee,(tokencoffeehot)[buy](coffeehot),(token coffee hot) [buy](hot coffee)hand, language also atom p theory T,added law (token coffee hot p) [buy](hot p) meet minimalchange preserving effects relevant (cf. Definition 3.4).214fiOn Action Theory Changeexecution contexts token coffee hot token coffee hot analogous000algorithm endstoken[buy]hot = {T1 , T2 , T3 }, where:T20 =coffee hot, token hbuyi>,token [buy]token, token [buy],(coffee (token coffee hot)) [buy]coffee,(hot (token coffee hot)) [buy]hot,(coffee (token coffee hot)) [buy]coffee,(token coffee hot) [buy](coffee hot)coffee hot, token hbuyi>,token [buy]token, token [buy],(coffee (token coffee hot)) [buy]coffee,(hot (token coffee hot)) [buy]hot,T30 =(coffee(token coffee hot)) [buy]coffee,(tokencoffee hot) [buy](hot coffee),(token coffee hot) [buy](coffee hot)Looking Figure 8, see correspondence theoriesrespective models. knowledge engineer look action theoriespick one corresponding expectations.5.3 Contracting Static LawsFinally, order contract static law theory, use contraction/erasureoperator classical logic available literature. contracting staticlaws means admitting new possible states (cf. semantics), modifying setstatic laws may enough multimodal logic case. However, since generalnecessarily know behavior actions new discovered stateworld, careful approach change theory action laws remaincontexts contracted law case. (The reader invited seeSituation Calculus allowing new situation exist one may need changeprecondition axioms well, means problem described independentlogical formalism chosen.)scenario example, contracting static law coffee hot knowledgeengineer really sure whether action buy still executable not,weaken set executability laws specializing context coffee hot,make buy priori inexecutable (coffee hot)-contexts. worth notingline assumption commonly made RAC community accordingexecutability laws large much likely incorrect rightbeginning (Shanahan, 1997). Therefore extrapolating previously unknown statesmight (and probability will) result propagation errors and, even worse,loss effect laws (remember discussion Sections 3.3 4.2). operator givenAlgorithm 3 formalizes this.215fiVarzinczakAlgorithm 3: Contraction Static LawInput: T,Output:/* set theories output knowledge engineer */1begin2:=3|=CPL/* call classical contraction */foreach4/* build theory preserving executability old states */0 := ((T \ ) ) \ Xa50 := 0 {(i ) hai> : hai> Xa } { [a]}0:= {T }678else:= {T}91011returnendrunning coffee example, contracting static law coffee hot action00theory producescoffeehot = {T1 , T2 },T10 =T20 =(token coffee hot),(token coffee hot) hbuyi>,coffee [buy]coffee, token [buy]token,token [buy], coffee [buy]coffee,hot [buy]hot, (coffee hot) [buy](token coffee hot),(token coffee hot) hbuyi>,coffee [buy]coffee, token [buy]token,token [buy], coffee [buy]coffee,hot [buy]hot, (coffee hot) [buy]Observe effect laws affected change: farpronounce executability action new added world,effect laws remain true it.knowledge engineer happy (coffee hot) [buy], contractformula theory using Algorithm 2. Ideally, besides stating buy executable context coffee hot, want specify outcome contextwell. example, could want (coffee hot) hbuyihot true result.requires theory revision. See Section 8 semantics operation.216fiOn Action Theory Change5.4 Complexity Issuesterminating, algorithms come considerable computational cost: Kn entailment tests global axioms beginning algorithms inside loopsknown exptime-complete (HarelV et al., 2000).V computation possiblecontexts allowed theory, namely pi atm() pi pi atm() pi , atm()pipi/IP(S ), clearly exponential. Moreover, computation prime implicantsIP(.) might result exponential growth (Marquis, 2000).Given theory change carried offline, perspective knowledge engineer important complexity size computed contracted theories: number formulas well length modified ones.plays important role deciding among several output theories one correspondsknowledge engineers expectations. matter, whereas length new addedformulas may increase exponentially, respect number laws resultspositive: size computed contracted theories linear size originalaction theory. (Remember card(X) denotes number elements set X.)Proposition 5.1 Let action theory, hai> executability law, 00hai> . card(T ) = card(T).0Proof: 6|=hai>,hai> = {T}, = T, resultKn0follows. Suppose |=hai> case. 0 = (T \ Xa ) Xa 0 ,KnXa 0 obtained Xa way (i 0 ) hai> Xa 0hai> Xa , fixed 0 . follows card(Xa 0 ) = card(Xa ). Now,card((T \ Xa ) Xa 0 ) = card(T \ Xa )+card(Xa 0 )card((T \ Xa ) Xa 0 ) = card(T)card(Xa )+card(Xa 0 ) card() = card(T) card(Xa ) + card(Xa ) 0 = card(T).2Proposition 5.2 Let action theory, [a] effect law, 0[a] .card(T 0 ) card(T) + card(Ea ) + card(Lit).0Proof: 6|=[a],[a] = {T}, = T, getKncard(T 0 ) = card(T). Since card(T) card(T) + card(Ea ) + card(Lit), result follows.Suppose |=K [a] case. 0 = (T \ Ea ) Ea 0 Ea 00 Fa , where:n000Ea Ea obtained Ea way (i 0 ) [a]i Ea 0(i 0 ) [a](i 0 ) Ea 00 [a]i Ea , fixed 0 , 0 ;Fa {(0 `) [a]( `) : ` Lit}, fixed 0 ;T, Ea 0 , Ea 00 , Fa pairwise disjoint.Hence card(Ea 0 ) = card(Ea 00 ) = card(Ea ), card(Fa ) card(Lit). card(T 0 ) =card(T \ Ea ) + card(Ea 0 ) + card(Ea 00 ) + card(Fa ) = card(T \ Ea ) + card(Ea ) + card(Ea ) +card(Fa ) = card(T) card(Ea ) + card(Ea ) + card(Ea ) + card(Fa ) = card(T) + card(Ea ) +card(Fa ) card(T) + card(Ea ) + card(Lit).2217fiVarzinczakGiven arbitrary choice contraction operator static laws, without lossgenerality resort slightly modified version it, viz. one always gives usresult set static laws cardinality original . (This possiblesince, contrary E X , conjunction static laws still static law,rewriting.) agreeing that, following proposition straightforward:0Proposition 5.3 Let action theory, static law, 0. card(T ) =card(T) + 1.Propositions 5.15.3 positive results: knowledge engineer dealoriginal action theory, able deal output algorithms.(Observe given 0 conditional frame axioms added Fa contractioneffect law factored single law, resulting theorycardinality card(T) + card(Ea ) + 1.)finish section observing size, set resulting contractedtheories, depends solely set static laws plus law contract with:Proposition 5.4 Let action theory, let law |=.Kncard(T) = card(S ),card(T) = card(val(S {})), either hai> [a].Proof: proof follows straightforwardly outermost loops Algorithms 13. 26. Correctness Operatorsaddress correctness algorithms respect semantics contraction. Correctness understood completeness adequacy. Adequacy meansalgorithms output theories whose models result semantic modificationsmodels original theory. Conversely, completeness says every model resultingsemantic modifications models original theory indeed modeltheory output algorithm.6.1 Challenges Completeness AdequacyLet theory = {p1 hai>, (p1 p2 ) [a], [a]p2 } consider modeldepicted Figure 13. (Notice |=(p1 p2 ).) contracting p1 [a]p2 ,Kn0get Figure 13.0contracting p1 [a]p2 using Algorithm 2 givesp [a]p = {T },12p1 hai>, (p1 p2 ) [a],(p1 p2 ) [a](p2 p2 ),T0=(p1 p2 ) [a](p2 p1 )Notice formula (p1 p2 ) [a](p2 p1 ) put 0 Algorithm 2{p1 } Lit 6|=(p p2 ) 6|=K (p1 p2 ) [a]p1 .CPL 1n218fiOn Action Theory Changew1p1 , p2w2w1p1 , p2:M0 :w3w2p1 , p2p1 , p2w3p1 , p2p1 , p2Figure 13: model result 0 contracting p1 [a]p2 it.M00clearly case 6|= 0 theoryp1 [a]p2 model. meanstheories contraction operators complete.issue arises Algorithm 2 tries allow transition p1 p2 -worldp2 -world closest it, viz. {p1 , p2 }, way knowingV0world exist. remedy replacing test 6`(`L `)KnV06`CPL ( `L `) , would increase even complexity algorithm.better option would complete enough allow algorithm determineworlds new transition could exist.way round, hold general models 0resultsemantic contraction models . see supposeone atom p one action a, consider action theory = {p [a], hai>}.model = h{{p}}, {({p}, {p})}i Figure 14.w2w1w1:pM0 :ppFigure 14: Inadequacy contraction: model model 0 theoryresulting contracting p hai> T.definitions, contract(M , p hai>) = {M }. (There p-world0remove arrow.) hand,phai> singleton {T }0 = {p [a], p hai>}. 0 = h{{p}, {p}}, ({p}, {p})i Figure 14model resulting contracted theory. Clearly, 0 result semanticcontraction p hai> : p valid contraction models T,valid models 0 . means theories operatorsadequate.problem occurs because, example, worlds forbidden T, e.g.{p}, preserved 0 . contracting executability effect law,supposed change possible worlds theory (cf. Section 3).Fortunately correctness algorithms respect semantics established action theories whose maximal, i.e., set static laws alone219fiVarzinczakcharacterize worlds possible models theory. principlemodularity (Herzig & Varzinczak, 2005b) briefly review next section.6.2 Modular Action Theoriesquite useful, albeit simple, property domain descriptions reasoning actionsaction theory modularity (Herzig & Varzinczak, 2005b).Definition 6.1 (Modularity) action theory modular everyBoolean formula Fml, |=, |=CPL .Knexample non-modular theory, let us suppose action theorycoffee machine scenario statedcoffeehot,hbuyi>,coffee [buy]coffee,T=token [buy]token, token [buy],coffee [buy]coffee, hot [buy]hotmodified law underlined: (in case wrongly) stated agentalways buy machine. |=K token, 6|=token.CPLnSince underlying multimodal logic independently axiomatized (see Section 2.1),use algorithms given Herzig Varzinczak (2005b) check whether actiontheory satisfies principle modularity. Whenever case, algorithmsreturn Boolean formulas entailed theory consequences alone.theory above, would return {token}: stated hbuyi>,inexecutability law token [buy] |=token. 6|=token,KnCPLtoken called implicit static law (Herzig & Varzinczak, 2004) actiontheory T.5Modular action theories several interesting computational properties. example, consistency checked checking consistency static laws :modular, |=|=. Deduction effect lawsKnCPLneed executability ones vice versa. Deduction effect sequence actions a1 ; . . . ; (prediction) need take account effect laws actionsa1 , . . . , . applies particular plan validation deciding whetherha1 ; . . . ; case.Modularity exclusive property action theories formalized Kn : similarnotions also investigated different contexts formalisms, like regulation consistency deontic logic (Cholvy, 1999), Situation Calculus (Herzig & Varzinczak,2005a), DL ontologies (Herzig & Varzinczak, 2006), dynamic logic (Zhang, Chopra, & Foo,2002) also Fluent Calculus (Thielscher, 2010). details modularityKn action theories, well role presence solution frameramification problems, see work Varzinczak (2006).5. Implicit static laws closely related veridical paradoxes (Quine, 1962). turns sometimesintuitive, sometimes not. deep discussion implicit static laws, seework Varzinczak (2006).220fiOn Action Theory ChangeAnother interesting property modular action theories following:Theorem 6.1 modular canonical model.Proof: Let Mcan = hWcan , Rcan canonical frame T.McanMcan(): definition, Mcan |= E . remains show |= X . LetMcanhai> Xa , let w Wcan |=w . Therefore j FmlMcan|=K j [a], must 6|=wnMcan|=(i j ), hence |=CPLMcanw0 Wcan |= 0Mcan|=wj , |=(i j ), modular,Kn(i j ). construction Mcan ,Mcan[a] Ea |=w. Thus Ra (w) 6=hai>.(): Suppose modular. must Fml |=Kn6|=. means v val(S ) v 6 . v Wcan (because WcanCPLcontains possible valuations ), Mcan model T.26.3 Correctness Modularityshown Herzig Varzinczak (2007), given action theory formalizedframework available literature allowing expression three basic typeslaws, always possible ensure modularity. Moreover, going seesequel (cf. Section 7.2), computed evolutionaction theory. Hence, relying modular theories limitation approach.following theorem establishes assumption action theorymodular, semantic contraction formula set models producesmodels contracted theory.Theorem 6.2 Let modular, law. M0|=M0000every M, 0|= every .2Proof: See Appendix A.next theorem establishes way round: modularity models theoriesmodels semantic contraction models T.M000Theorem 6.3 Let modular, law, 0. |= ,00M0|= every M.2Proof: See Appendix B.two theorems get correctness operators:M00Corollary 6.1 Let modular, law, 0|=. |=Knevery 0 M0 M0|= M.221fiVarzinczakProof:M000(): Let 0 |= 0 . Theorem 6.3, M0M0|= M. 0 |=, |= .Kn(): Suppose 0 6|=K . (We show model 0 M0 M0nM0|= M, 6|= .)Given modular, Lemma B.1 0 modular, too. Then, Lemma B.3,M0M00 = hval(S 0 ), R0 6|= . Clearly |= 0 , Lemma B.4result follows.27. Assessment Postulates Changeaction theory change operators satisfy classical postulates change?answering question, one ask: operators behave like revision updateoperators? address issue show postulates theory changesatisfied definitions.7.1 Contraction Erasure?distinction revision/contraction update/erasure classical theorieshistorically controversial literature. true case modal theoriesdescribing actions effects. rephrase Katsuno Mendelzons definitions (1992) terms see one method closer.Katsuno Mendelzons view, contracting law action theory intuitively means description possible behavior dynamic world mustadjusted possibility false. amounts selecting modelsclosest models allow models result.contrast, update methods select, model T, set modelsclosest . Erasing means adding models T; model , addmodels closest false. Hence, constructions farseems operators closer update revision.Moreover, according Katsuno Mendelzons view (1992), change operatorswould also classified update make modifications model independently, i.e., without changing models.6 Besides that, setting differentordering resulting models induced model theory (see Definitions 3.3, 3.7 3.10), according Katsuno Mendelzon typical propertyupdate/erasure method.Nevertheless, things get quite different comes postulates theory change.7.2 Postulatessection analyze behavior action theory change operators respectAGM-like postulates. follow Katsuno Mendelzons presentation6. Even contracting effect law one particular model need check modelstheory, modified.222fiOn Action Theory Changepostulates assess contraction erasure. Let = E X denote actiontheory denote law.0 , 0Monotonicity Postulate: |=.Knpostulate version Katsuno Mendelzons (C1) (E1) postulatescontraction erasure, respectively, satisfied change operators.proof Lemma A.1. postulate satisfied operators proposedHerzig et al. (2006): removing e.g. executability law hai> one maymake [a] valid models resulting theory.Preservation Postulate: 6|=, |=0 , 0.KKnnKatsuno Mendelzons (C2) postulate. operators satisfy farwhenever 6|=, models resulting theory exactly models T,Knminimal models falsifying .corresponding version Katsuno Mendelzons (E2) postulate erasure,i.e., |=, |=K 0 , 0, clearly also satisfied operatorsKnnspecial case postulate above. Satisfaction (C2) indicates operatorscloser contraction erasure.6|=K 6|=K , 0 6|=K , 0.Success Postulate:nnnpostulate version Katsuno Mendelzons (C3) (E3) postulates.propositional Fml, operators satisfy it, long classical propositionalchange operator satisfies well. general case, however, stated postulatealways satisfied. shown following example: let = {p, hai>, p [a]}.Note modular consistent. Now, contracting (contingent) formula p hai>gives us 0 = T. Clearly 0 |=K p hai>. happens because, despitentautology, p hai> trivial formula respect T: since p valid T-models,p hai> trivially true models (cf. end Section 3.1).Fortunately, formulas non-trivial consequences theory,operators guarantee success contraction:Theorem 7.1 Let consistent, executability effect law6|=. modular, 0 6|=K every 0.Knn0Proof: Let us suppose 0. Since modular,|=KnM0Corollary 6.1 tells us |= every 0 M0 M0, = {M :|= = hval(S ), Ri}.M000|= every 0 M0 , even 00 M0 \M |= . 0000M, definition 6|= . Hence = , truthdepend accessibility relation Ra . Hence, whether form hai>[a], , Fml, holds |=CPL (see Definitions 3.1 3.5),therefore get |=K .2n223fiVarzinczakEquivalences Postulate: |=K T1 T2 |=1 2 , |=K T10 T20 ,K0T10 (T1 )2 T2 (T2 )1 .nnnpostulate corresponds Katsuno Mendelzons (C4) (E4) postulates.worth noting equivalence considered always modulo action laws, i.e.,formulas assumed either static laws, effect laws executability laws, wellequivalents. Moreover remember theories must action theories,i.e., sets action laws three basic types. modularity assumptionpropositional change operator satisfies (C4)/(E4), operations satisfy postulate:T1 T2 |=K 1 2 ,Theorem 7.2 Let T1 T2 modular. |=Knn00 T20 , vice-versa.T10 (T1 )2 T2 (T2 )1 |=K 1nProof: proof follows straight results: since |=T2 |=K 1 2 ,K 1nnpairwise models. Hence, given |= T1 |= T2 , semanticcontraction 1 2 operations . T1 T2modular, Corollary 6.1 guarantees get syntactical results. Moreover,classical operator satisfies (C4)/(E4), follows |=K T10 T20 .2nRecovery Postulate: 0 {} |=T, 0.Knaction theory counterpart Katsuno Mendelzons (C5) (E5) postulates. rely modularity order satisfy it.Theorem 7.3 Let modular. 0 {} |=T, 0.KnProof: 6|=K , operators satisfy preservation postulate, 0 = T,nresult follows monotonicity.Let |=, let M0 denote set models 0 . modular, CorolKnM0M0lary 6.1 every 0 M0 either |= (and |= ) 0 contract(M , )(and 0 ) |= T.Let M00 denote set models 0 {}. Clearly M00 M0 , monotonicity.00Moreover, every 00 M00 |= , hence 00/ every|= T, 00/ contract(M , ), model T. Thus 00 model0{} |=K T.2nW0Letdenote disjunction .WWDisjunctive Rule: (T1 T2 )(T1 )(T2 )equivalent.version (E8) erasure postulate Katsuno Mendelzon. Clearlysyntactical operators manage contract law disjunction theories: T1 T2action theory cannot general rewritten one. Nevertheless, provingholds semantics, correctness operators, get equivalentoperation. fact theories concern modular gives us result.224fiOn Action Theory ChangeTheorem 7.4 Let T1 T2 modular, law.___(T2 )(T1 )|=K(T1 T2 ))(nProof:W0W0W0W(T2 )(T1 )(): Let 0 |=(T1 ). Suppose|=(T2 ) . |=0M0W|=(T1 ) (the case analogous). (T1 )0 (T1 ) |= (T1 )0 .00Corollary 6.1, M0, set models0T1 . model resulting contracting models T1 , 0 alsoresults contracting models T1 T2 , viz. models T1 . CorolM00W0 , |=|=(T)(T1 T2 )lary 6.1, (T1 T2 )0 (T1 T2 )12.0W(): Let 0 |=M00(T1 T2 ). (T1 T2 ) (T1 T2 )00|= (T1 T2 )0 . Corollary 6.1, M0, set0models T1 T2 . model resulting contracting models T1 T2 .Hence 0 results contracting models T1 models T2 . Supposeformer case (the second analogous). Corollary 6.1 (T1 )0 (T1 )M00W|= (T1 )0 , |=(T1 ).2thus shown constructions satisfy (E8) postulate. Nevertheless,far see, immediate whether really expected here. supportsposition operators behavior closer contraction erasure.seen results above, modularity sufficient conditionsatisfaction AGM-like postulates action theory contraction. finish statenew postulate:Preservation Modularity: modular, every 0modular.Changing modular theory make non-modular. standardpostulate, think since good property modularity preservedacross changing action theory. so, means whether theory modularchecked one need care futureevolution action theory, i.e., changes made it. operatorssatisfy postulate proof given Appendix B.one may naturally asks whether get characterization result traditional AGM sense, i.e., whether contraction operator satisfying versionspostulates one three contraction operations. Unfortunately, good sense pointstowards negative answer: might well operator satisfying postulates that, complying assumptions RAC community (Shanahan,1997), necessarily one operators defined Section 3 (cf. discussiongeneral formula contraction Section 10). witness, consider example operatoralso modifies worlds contracting effect laws. supports one contentionspresent work, viz. classical belief change cannot fully transposed actiontheories expected give exactly kind outcome. Similar negative resultsalso found revision DL ontologies (Flouris, Plexousakis, & Antoniou, 2004)contraction Horn theories (Booth, Meyer, & Varzinczak, 2009).225fiVarzinczak8. Semantics Action Theory Revisionfar analyzed case contraction: knowledge engineer realizestheory strong therefore weakened. Let us take lookway round, i.e., theory (possibly) liberal agent discovers new lawsworld added beliefs, amounts strengthening them.Suppose action theory scenario example initially stated follows:coffee hot, token hbuyi>,coffee [buy]coffee, token [buy],T=coffee [buy]coffee, hot [buy]hotcanonical model theory shown Figure 15.w1t, c, hbbw2:bw3bt, c, ht, c, hbbw5t, c, hw6w4t, c, ht, c, hFigure 15: Canonical model new initial action domain description.Looking model Figure 15 see that, example, agent knowloses token every time buys coffee machine. new lawincorporate knowledge base stage action theory evolution.Contrary contraction, want negation law become satisfiable,revision want make new law valid. means one eliminate casessatisfying negation. depicts duality revision contraction: whereaslatter one invalidates formula making negation satisfiable, former onemakes formula valid forcing negation unsatisfiable prior adding newlaw theory.idea behind semantics revision follows: initially set modelsgiven formula (potentially) valid, i.e., (possibly) trueevery model M. result want models . Adding -modelshelp. Moreover, adding models makes us lose laws: corresponding resultingtheory would liberal.One solution amounts deleting models -models.course removing solve problem, must delete everymodel. that, resulting models models . (This corresponds theoryexpansion, resulting theory satisfiable.) However, contains model, end . Consequence: resulting theory inconsistent. (Thismain revision problem.) case solution substitute model226fiOn Action Theory Changenearest modification M? makes true. lets us keep close possibleoriginal models had. But, one model several minimal(incomparable) modifications validating ? case shall consider them.result also list models M? , models .defining revision sets models, present modifications (individual) models are.8.1 Revising Model Static LawSuppose coffee deliverer agent discovers hot drink servedmachine coffee. case, might want revise beliefs new staticlaw coffee hot: cannot hold hot drink coffee.Considering model depicted Figure 15, one see Boolean formulacoffee hot satisfiable (there world model holds). Sincewant case, first step remove worlds coffee hottrue. second step guarantee remaining worlds (if any) satisfy newstatic law. issue largely addressed literature propositional beliefbase revision update (Gardenfors, 1988; Winslett, 1988; Katsuno & Mendelzon, 1992;Herzig & Rifi, 1999). achieve semantics similar classicalrevision operators: basically one change set possible valuations, removingadding worlds.example, removing possible worlds {t, c, h} {t, c, h} would job(there need add new valuations since new incoming law already satisfiedleast one world original model, therefore resulting set worlds non-empty).delicate point removing worlds may consequence lossexecutability laws: example, transition world wsay {t, c, h}, removing latter model would make actionconcern longer executable w, transition labeled action leavingit. semantic point view, intuitive: state worldcould move longer possible, transition state anymore.Therefore, transition one had, natural lose it.Similarly, one could ask accessibility relation new worldsadded, i.e., expansion possible. Following discussion Section 3.3,prefer add new transitions systematically accessibility relation. Hence shallpostpone correction executability laws, needed. approach may debatable,information hand, safest way changing static laws. (Seealso discussion Sections 3.3 4.2.)semantics revision one model static law follows:Definition 8.1 Let = hW, Ri. 0 = hW0 , R0 M? if:W0 = (W \ val()) W , W val();R0 R.227fiVarzinczakM0Clearly unless |=, |= 0 M? . minimal modelsCPLresulting revising model closest respect :Definition 8.2 Let model static law. revise(M , ) =min{M? , }.example model Figure 15, revise(M , coffee hot) singleton{M 0 }, 0 shown Figure 16.w1t, c, hbw2M0 :bt, c, hbbw5t, c, hw4t, c, hFigure 16: Model resulting revising model Figure 15 coffee hot.8.2 Revising Model Effect LawLet us suppose agent eventually discovers buying coffeekeep token anymore. (That design mistake agent still possessestoken even ordering coffee machine). means theoryrevised way new effect law token [buy]token holds. Lookingmodel Figure 15, amounts guaranteeing formula token hbuyitokensatisfiable none worlds. that, look worlds satisfyingformula (if any) either (i) make token false worlds; (ii) makehbuyitoken false them. chose first option, essentially flip truthvalue literal token respective worlds, changes set valuationsmodel. chose latter, basically remove buy-arrows leading token-worlds.case, change accessibility relation made.example, possible worlds {token, coffee, hot}, {token, coffee, hot}{token, coffee, hot} satisfy token hbuyitoken change.Flipping token worlds token would job, would alsoconsequence introduction new static law: token would valid, i.e.,agent never token! Another issue approach making token trueeverywhere, new incoming law token [buy]token trivially true resultingmodel, mean execution action buy token-worldtoken one. defeats purpose changing action theory basisobserved every execution action consideration leadtoken-contexts.One contentions present work changing action laws neverside effect change static laws (cf. Sections 3 4). Given special status (Shanahan, 1997), change explicitly required. case, world228fiOn Action Theory Changesatisfying token hbuyitoken changed hbuyitoken longer true it.example, remove transitions ({token, coffee, hot}, {token, coffee, hot}),({token, coffee, hot}, {token, coffee, hot}) ({token, coffee, hot}, {token, coffee, hot}).semantics one model revision case new effect law is:?Definition 8.3 Let = hW, Ri. 0 = hW0 , R0 M[a]if:W0 = W;R0 R;(w, w0 ) R \ R0 , |=w ;M0|= [a].minimal models resulting revision model new effect lawclosest respect order models :Definition 8.4 Let model [a] effect law. revise(M , [a]) =?min{M[a], }.Taking shown Figure 15, revise(M , token [buy]token)singleton {M 0 } (Figure 17).w1t, c, hbbw3w2M0 :t, c, ht, c, hbw5w6w4t, c, ht, c, ht, c, hFigure 17: Model resulting revising model Figure 15 new effect lawtoken [buy]token.8.3 Revising Model Executability LawLet us suppose stage decided grant free coffee everybody.Faced information, agent revise laws reflect fact buyalso executed token-contexts: token hbuyi> new executability law (andtherefore hbuyi> new models agents beliefs).Considering model Figure 15, observe (token hbuyi>)satisfiable . means must throw token [buy] away ensurenew formula becomes true new model, i.e., satisfied worlds.229fiVarzinczakremove token [buy] look worlds satisfying modifylonger satisfy formula. Given worlds {token, coffee, hot}{token, coffee, hot}, two options: change interpretation token add newtransitions leaving worlds. question arises choice drastic:change world transition ? Again, think changing worlds content(the valuation) drastic, existence world foreseen staticlaw hence assumed is, unless enough information supportingcontrary, case explicitly change static laws (see above). Moreover,changing truth value token worlds would trivialize new incoming lawtoken hbuyi> new model, defeating purpose guaranteeing existencebuy-transition token-context. Therefore shall add new buy-arrow{token, coffee, hot} {token, coffee, hot}.agreed that, issue is: worlds new transitionsdirected to? Recalling reasoning developed Section 3.2, order complyminimal change, new transitions shall directed worlds relevant targetstoken-worlds question. example, {token, coffee, hot}relevant target world here: two token-worlds violate effect coffee buy,three token-worlds would make us violate frame axiom token [buy]token.semantics one model revision new executability law follows:?Definition 8.5 Let = hW, Ri. 0 = hW0 , R0 Mhai>if:W0 = W;R R0 ;(w, w0 ) R0 \ R, w0 RelTarget(w, [a], , M);M0|= hai>.minimal models resulting revising model new executability lawclosest respect :Definitionmodel hai> executability law. revise(M ,8.6 Let?hai>) = min{Mhai>, }.running example, revise(M , token hbuyi>) singleton {M 0 }, 0depicted Figure 18.example, observe single relevant target world getsingle model result revision.8.4 Revising Sets Modelsseen revision single models means. neededexpansion new law possible due inconsistency. give unifieddefinition revision set models new law :230fiOn Action Theory Changebw1t, c, hbbbw2M0 :bt, c, hbt, c, ht, c, hbbw5bw3w6w4t, c, ht, c, hFigure 18: Result revising Figure 15 new executability law token hbuyi>.Definition 8.7 Let set models law.(\ {M : 6|= }, |= ;?=revise(M , ), otherwise.Observe Definition 8.7 comprises expansion revision: first one, simpleaddition new law gives satisfiable theory; latter deeper change neededget rid inconsistency.9. Related Workbest knowledge, first work updating action domain descriptionLi Pereira (1996) narrative-based action description language (Gelfond &Lifschitz, 1993). Contrary us, however, mainly investigate problem updatingnarrative new observed facts (possibly) occurrences actions explainfacts. amounts updating given state/configuration world (in terms,true possible world) focusing models narrativeactions took place (in terms, models action theory particular sequenceaction executions). Clearly models action laws remain same.Baral Lobo (1997) introduce extensions action languages allowcausal laws stated defeasible. work similar also allowweakening laws: setting, effect propositions replaced calldefeasible (weakened versions of) effect propositions. approach differentway executability laws dealt with. executability laws explicitalso able contract them. feature important qualification problemconsidered: may always discover contexts preclude execution given action(cf. Introduction).Liberatore (2000) proposes framework reasoning actions possible express given semantics belief update, like Winsletts (1988) KatsunoMendelzons (1992). means formalism, essentially action description lan231fiVarzinczakguage, used describe updates (the change propositions one stateworld another) expressing laws action theory.main difference Liberatores work (2000) Li Pereiras (1996)that, despite concerned, least priori, changing action laws, Liberatoresframework allows abductively introducing action theory new effect propositions(effect laws, terms) consistently explain occurrence event.work Eiter et al. (2005) similar also propose frameworkoriented updating action laws. mainly investigate case e.g.new effect law added description (and true modelsmodified theory). problem dual contraction closer definitionrevision (cf. Section 8).Eiter et al.s framework (2005), action theories described variant narrativebased action description language. Like present work, semantics also termstransition systems, transitions (action occurrences) linking states (configurationsworld). Contrary us, however, minimality condition outcomeupdate terms inclusion sets laws, means approach syntaxoriented extent.setting, update action theory seen composed two pieces,Tu Tm , Tu stands part supposed change Tm containslaws may modified. terms, contracting static law wouldTm = Xa ; contracting executability law Tm = Xa ; contractingeffects laws Tm = Ea . difference approach always clearlaws change given type contraction, therefore Tu Tm needexplicitly specified prior update.approach described constraint-based update,theory change carried relative constraints (a set laws want holdresult). framework, example, changes action laws relativeset static laws (and concentrate models val(S )worlds). changing law, want keep set states. differencerespect Eiter et al.s (2005) approach also possible update theoryrelatively e.g. executability laws: expanding new effect law, one may wantconstrain change action concern guaranteed executableresult.7 shown referred work, may require withdrawal staticlaw. Hence, Eiter et al.s framework, static laws status ours.Herzig et al. (2006) define method action theory contraction that, despitesimilarity current work common underlying motivations, limitedpresent constructions.First, referred approach get minimal change. example,referred work operator contracting executability laws resultingtheory modified set executability laws givenXa = {(i ) hai> : hai> Xa }7. could simulate approach two successive modifications T: first adding effectlaw executability law (cf. Section 8).232fiOn Action Theory Changewhich, according semantics, gives theories among whose models resultingremoving transitions -worlds. similar comment made respectcontraction effect laws.Second, Herzig et al.s (2006) contraction method satisfy postulatesaction theory change addressed Section 7. Besides satisfyingmonotonicity postulate, satisfy preservation one. witness, supposelanguage one atom p, model depicted Figure 19.w1:w2ppw1:0w2ppFigure 19: Counter-example preservation method Herzig et al. (2006).|= p [a]p 6|= [a]p. contraction operator definedremoving [a]p yields model 0 Figure 19 R0a = WW.M06|= p [a]p, i.e., effect law p [a]p preserved.Finally, another work related Zhang Ding (2008). Likeours, approach also giving semantic characterization basic operationschanging Kripke models. Contrary us however, focus model checking,entailment. Despite definition use operations essence similar(modifications set possible worlds accessibility relation), workconcerned mainly modifications single model, sets modelsdo, hence provide operations changing action laws.that, approach directly comparable ours, since interestedentailment-based revision.10. Concluding Remarkswork addressed problem changing action domain descriptionreasoning actions, problem sufficiently investigated literature far.seen intuitions behind kind theory modification givensemantics action theory change terms distances models capturesnotion minimal change. given algorithms contract formula theoryterminate correct respect semantics (Corollary 6.1).shown importance modularity notion result others.also extended Varzinczaks investigations (2008) defining semanticsaction theory revision based minimal modifications models. correspondingrevision algorithms, reader referred work Varzinczak (2009). Oneongoing research topics assessing revision operators behavior respectappropriate versions AGM postulates revision (Alchourron et al., 1985)links contraction counterpart.algorithms provide set tools used knowledge engineerinteractive possibly iterative way modify action theory. tools guaranteed233fiVarzinczakperform minimal change assisting knowledge engineer implementingdesired modifications. give set options knowledge engineerdecide one line intuitions.Given action theory change single step operation, knowledge engineerexpected make use contraction/revision operators make series modificationseventually give fine-grained theory entailing contracted laws entailingnew learned laws domain.sake presentation, abstracted frame ramificationproblems. However definitions could stated formalism suitable solution them, like Castilho et al.s approaches (1999, 2002). regardsqualification problem, ignored here: contracting wrong executability lawsapproach towards solution. Indeed, given difficulty stating sufficient conditionsexecutability action, knowledge engineer writes letstheory evolve via subsequent revisions.possible criticism approach developed concerns cautiousnessoperator contracting static laws: prefer lose executability laws ratherinduce lose effect laws. behavior could make operators interpretedincoherent. pointed nevertheless line largely acceptedassumptions RAC community, moreover shown impossibilitynon-cautious static law contraction operator complies coherentoperators.Indeed one purposes present work shed light fundamentaldifferences belief change action domain descriptions logical theoriesgeneral. Classical belief change cannot fully transplanted action theories,shown (cf. Sections 3.2, 4.2, 5.3 8.3).particular, looking postulates classical belief change (or versions thereof)one sees enough fully characterize operators action theory change.achieved fundamental assumptions reasoning actionsextensively used throughout work somehow compiled postulatessupplementing classical ones. immediately clear new postulateswould look like, interesting thread investigation worth pursuing.might also argued semantic operations respect principlecategorical matching, given input output different sorts objects, viz.set models set sets models (cf. Definitions 3.3, 3.7 3.10). easy see,however, semantic constructions could defined wayM0corresponds result one contraction operator. choice definingresult operation set possible outputs driven definitionalgorithms, theory (corresponding set models) given inputoutput set theories (hence corresponding set set models).Although semantic operators redefined satisfy principle categorical matching, immediate algorithms (they would nondeterministic). Therefore preferred keep balance semanticsyntactic definitions see clearly direct correspondence.234fiOn Action Theory ChangeOne contentions sticking modular theories (and hence canonical models) big deal: use existent algorithms literature (Herzig &Varzinczak, 2007) ensure action theory characterized canonical models.seen modularity, operators satisfy postulates contraction: Modularity one sufficient conditions Success Theorem 7.1.also sufficient condition Theorem 7.2, and, shown Theorem 7.3, sufficientcondition Recovery. Finally also sufficient condition Disjunctive Rulehold, shown preserved contraction operators (cf. last paragraphSection 7.2, proof Appendix B). Preservation modularity important result sincemeans checked/ensured lifetime actiontheory. results support thesis modularity notion fruitful.forcing formulas explicitly stated respective modules (and thus possiblymaking inferable independently different ways), modularity intuitively couldseen diminish elaboration tolerance (McCarthy, 1998). instance, contractingBoolean formula non-modular theory, seems reasonable expect changeset static laws , theory modular surely forces changing module.difficult, however, conceive non-modular theories contraction formulamay demand change well. example, suppose = {1 2 } actiontheory whose dynamic part (implicitly) infer 2 . case, contracting 1keeping 2 would necessarily ask change .point nevertheless cases (modular non-modular) extra workchanging modules stays mechanical level, i.e., algorithms carrymodification, augment significant way amount workknowledge engineer expected do.Contrary trend belief change community, focus either beliefbases belief sets (Hansson, 1999), method proposed hybrid one (Delgrande,2009). one hand, semantics plays crucial role notion minimal changestudied. hand, deal domain descriptions reasoningactions, sets laws specific types. top that, modularity property (asyntactical one) fundamental main results.Following lines, another issue drives future research subjectcontract laws Kn -formula. defined, order applicationoperators matter final result: contract [a] theory T,result may contracting [a] first removing .problem would appear general framework formula couldcontracted: removing ( [a]) give result ( [a]) .principle syntax independence (Dalal, 1988).Related question revision definitions relate contractionoperators. known Levi identity (1977), ? ={}, generalhold action laws (effect executability ones). reasoncontraction operator effect executability law. Indeedgeneral contraction problem non-classical logics: contraction generalformula (like above) still open problem belief change area. insightsdirection given revision definitions, make false everypossible world Kripke model.235fiVarzinczakDefinitions 3.1, 3.5 3.8 appear important better understanding problemcontracting general formulas: basically set modifications perform givenmodel order force falsify general formula comprise removal/additiontransitions/worlds. definition general revision/contraction method benefitconstructions.Furthermore, given well-known connection multimodal logics Description Logics (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003), believedefinitions may also contribute ontology evolution debugging specificfamilies DLs.AcknowledgmentsParts work done authors stay Institut de Recherche enInformatique de Toulouse (IRIT), France, visit National ICT Australia(NICTA), Sydney.author grateful anonymous referees constructive useful remarks, helped improving quality work. paper also benefiteddiscussions Andreas Herzig Laurent Perrussel.Special thanks colleagues Meraka Institute Arina Britz, Ken Halland,Johannes Heidema Tommie Meyer invaluable comments suggestionsearlier versions article.Appendix A. Proof Theorem 6.2Let modular, law. M0|= every M,M00000|= every .give proof theorem, need following lemma (cf.Monotonicity Postulate Section 7.2):Lemma A.1 |=0.KnProof: Let action theory, let 0, given law . goinganalyze case.Let form hai>, Fml. 00 = (T \ Xa ) {(i ( )) hai> : hai> Xa }VVIP(S ) = pi atm() pi pi atm() pi , atm().pipi/Let = hW, Ri |= T. enough show model newlaws. every (i ( )) hai>, every w W, |=w ( ), |=.w|=K hai>, |= hai>, Ra (w) 6= .nTherefore |= 0 .236fiOn Action Theory ChangeLet form [a], , Fml. 0(T \ Ea ){(i ( )) [a]i : [a]i Ea }00 = {(i ) [a](i ) : [a]i Ea }` L, forVsome L Lit s.t.( `) [a]( `) : 6` ( 0 `L `) , ` 06`Kn ( `) [a]`Ea =,1in (Ea )i ,IP(S ), =Vpi atm()pipiVpi atm()pi/pi ,atm(), 0 IP(S ).Let = hW, Ri |= T. enough show modeladded laws. Given (i ( )) [a]i , every w W, |=w ( ),|=w . |=K [a]i , |= [a]i , |= 0 every w0 Wwn(w, w0 ) Ra .(i ) [a](i 0 ), every w W, |=w , |= 0wevery w0 W (w, w0 ) Ra .Now, given ( `) [a]( `), every w W, |=w `, |=,w|=. Since |=[a], |= [a], |= 0 every w0 WKnww0(w, w ) Ra .Therefore |= 0 .Let propositional . 0((T \ ) ) \ Xa0 = {(i ) hai> : hai> Xa }{ [a]}.Let = hW, Ri |= T. suffices show satisfies added laws.Since assume behaves like classical contraction operator, like e.g. KatsunoMendelzons (1992), |=, then, |= , |= .CPLgiven (i ) hai>, every w W, |=w , |=,w|= hai>, Ra (w) 6= .Finally, [a], |= , trivially satisfies [a].Therefore |= 0 .2Proof Theorem 6.2M000Let = {M : |= T}, M0. show |=00every .237fiVarzinczakM0M0definition, 0 M0 either |= 6|= .6= ,M0M00must 0. |= T, Lemma A.1 |= done. Let us supposeM06|= . analyze case.Let form hai> Fml. 0 = hW0 , R0 i, W0 = W,0(w, w0 ) Ra }, M.R0 = R \ R, Ra = {(w, w ) : |=wM0M0Let u W0 6|=hai>, i.e., |=R0a (u) = .uuV0u , must v base(, WV) v u. Let = `v `. Clearlyprime implicant . Let also = `u\v `, consider0 = (T \ Xa ) {(i ( )) hai> : hai> Xa }(Clearly, 0 theory produced Algorithm 1.)enough show 0 model new added laws. Given (i (A ))M0M0, follows |=w .( ), |=hai> 0 , every w W0 , |=ww|= hai>, w0 W w0 Ra (w). need show00(w, w0 ) R0a . 6|=, R, either w = u,= , (w, w ) Ra . |=wwM0M0|=u conclude |=u (i ( )) hai>, w 6= u must(w, w0 ) R0a , otherwiseRa R(R \ Sa ) R(R \ Ra ),0000000 = hW0 , R \6|= hai> , contradictionM0M00 minimal respect . Thus (w, w0 ) R0a , |=hai>. Hence |= 0 .wlet form [a], , Boolean. 0 = hW0 , R0 i,W0 = W, R0 = R R,,R,= {(w, w0 ) : w0 RelTarget(w, [a], , M)}= hW, Ri M.M0M0[a]. u0 W0 (u, u0 ) R0aLet u W0 6|=u6|=0 . u , v base(, W0 ) v u, u0 , mustuVVVv 0 base(, W0 ) v 0 u0 . Let = `v `, = `u\v `, 0 = `v0 `.Clearly (resp. 0 ) prime implicant (resp. ).let Ea = 1in (Ea, )i let theory(T \ Ea ){(i ( )) [a]i : [a]i Ea }00 = {(i ) [a](i ) : [a]i Ea }` L, forVsome L Lit s.t.( `) [a]( `) : 6` ( 0 `L `) , ` 06`Kn ( `) [a]`(Clearly, 0 theory produced Algorithm 2.)238fiOn Action Theory Changeorder show 0 model 0 , enough show modelM0added laws. Given (i (A )) [a]i 0 , every w W0 , |=w (A ),M0|=, |=w . |= [a]i , |= 0 w0 W (w, w0 ) Ra .ww= , R0a (w) = Ra (w).need show R0a (w) = Ra (w). 6|=w , R,M0M0|=, either w = u, |=u conclude |=(i ( )) [a]i ,wu,w 6= u, must Ra= , otherwise would S,R,,,0,00R(R Sa ) R(R Ra ), = hW , R Sa would006|= [a] 00 0 , contradiction since 0 minimal respect .M0Hence R0a (w) = Ra (w), |= 0 w0 (w, w0 ) R0a .wM0M0Now, given (i ) [a](i 0 ), every w W0 , |=w ,|=w , |=. Because, |= [a]i , |= 0 w0 WwwM0(w, w0 ) Ra , |= 0 every w0 W0 (w, w0 ) R0a \ Ra, . Now, givenwM0, |= 0 0 , result follows.(w, w0 ) R,wM0M0Now, ( `) [a]( `), every w W0 , |=`,w|=w , |=. |= [a], |= 0 every w0 Ww(w, w0 )wM0Ra , |= 0wM0w00W (w, w0 ) R0a \ R,. remainsshow |= 0 ` every w0 W0 (w, w0 ) R,. Since 0 minimal,wM0M0enough show |=0 ` every ` Lit |=`. ` 0 , resultuuM0follows. Otherwise, suppose 6|=0 `.ueither ` 0 , 0 ` unsatisfiable, case Algorithm 2put law ( `) [a]( `) 0 , contradiction;` u0 \ v 0 . case, valuation u00 = (u0 \ {`}) {`}0000u00 6 .: `i u00 }iVV must u W , otherwise L = {`00|=K ( `i L0 `i ) , and, modular, |=CPL ( `i L0 `i ) ,nAlgorithm 2 put law ( `) [a]( `) 0 , contradiction.u00 W0 , moreover u00/ R,(u), otherwise 0 minimal.,u00 \ u u0 \ u, reason u00/ Ra (u) `0 u u00Mi V0|= `j u `j [a]` every Mi `0/ v0 v0 base(, W 0 )Vv0 u00 . Clearly `0 = `, `/ 0 , |= `j u `j [a]`every Mi M. |=( `) [a]`, Algorithm 2Knput law ( `) [a]( `) 0 , contradiction.M0Hence |= 0 ` every w0 W0 (w, w0 ) R0a .wM0Putting results together, get |= 0 .Let propositional . 0 = hW0 , R0 i, W W0 , R0 = R,minimal respect , i.e., W0 minimal superset W u W0239fiVarzinczaku 6 . assumed syntactical classical contraction operatorsound complete respect semantics moreover minimal, mustM0W0 = val(S ). Therefore |= .R0 = R, every effect law remains true 0 .Now, let((T \ ) ) \ Xa0 = {(i ) hai> : hai> Xa }{ [a]}(Clearly, 0 theory produced Algorithm 3.)M0every (i ) hai> 0 every w W0 , |=, Ra (w) 6= ,wM0hai>. Given [a], every w W0 , |=, w = u,|=wwRa (w) = .M02Putting results together, |= 0 .Appendix B. Proof Theorem 6.3M0000Let modular, law, 0. |= ,0 M0 |= every M.order prove result, first need show four important lemmas.Lemma B.1 Let law. modular, every 0modular.0Proof: Let nonclassical, suppose 0modular.000000Fml |=K 6|=, set staticCPLn00laws . Lemma A.1, |=, |=0 . nonclassical,KnKn0= . Thus 6|=CPL 0 , therefore modular.Let Fml.((T \ ) ) \ Xa0 = {(i ) hai> : hai> Xa }{ [a]}.Assume modular, let 0 Fml 0 |=0 6|=0 .KnCPL6|=0 , v val(S ) v 6 0 . v val(S ), 6|=CPL 0 ,CPLmodular, 6|=K 0 . Lemma A.1, |=K 0 , 0 6|=K 0 , contradiction.nnnHence v/ val(S ). Moreover, must v 6 , otherwise worked expected.Let = hW, Ri |= 0 . (We extend another model 0 .) Let0 = hW0 , R0 W0 = W {v} R0 = R. show 0 modelM00 , suffices show v satisfies every law 0 . v val(S ), |=. Givenv240fiOn Action Theory ChangeM0[a] 0 , v 6 R0a (v) = , |=v [a]. Now, every [a]i 0 ,M0M0|=v , trivially |=0 every v0 (v, v0 ) R0a . Finally, givenvM0(i ) hai> 0 , v 6 , formula trivially holds v. Hence |= 0 ,M0v W0 6|=0 , 0 6|=K 0 , contradiction. Hence 0 Fmlvn0 , 0 modular.0 |=K 0 , |=2CPLnLemma B.2 Mcan = hWcan , Rcan model T, every = hW, Ri|= minimal (with respect set inclusion) extension R0 Rcan \ R0 = hval(S ), R R0 model T.Proof: Let Mcan = hWcan , Rcan model T, let = hW, Ri |= T.M0Consider 0 = hval(S ), Ri. |= T, R0 = Rcan \ R minimal. SupposeM0M06|= T. extend 0 model minimal extension . 6|= T,M0M0v val(S ) \ W 6|=T. 6|=v .vFml, v Wcan , Mcan model T. form [a],, Fml, v0 val(S ) (v, v0 ) Ra v0 6 , contradiction sinceM0Ra (v) = . Let form hai> Fml. |=. v Wcan ,vMcanMcan6|=v hai>, 6|= T. Hence, Rcana (v) 6= . Thus taking (v, v0 ) Rcana givesus minimal R0 = {(v, v0 )} 00 = hval(S ), R R0 model T.2Lemma B.3 Let modular, law. |=every 0 =K0hW,Rihval(S ), R |=n0R R model .Proof:(): Straightforward, since |=implies |= every |= T, particularKnextensions model T.(): Suppose 6|=. = hW, Ri |= 6|= . modular,Kncanonical frame Mcan = hWcan , Rcan model T. Lemma B.2minimal extension R0 R respect Rcan 0 = hval(S ), R R0 modelT. 6|= , w W 6|=w . propositional FmlM0effect law, extension 0 6|=w . form hai>,|=Ra (w) = . extension (u, v) R0wu val(S ) \ W, worlds W get new departing transition. ThusM02(R R0 )a (w) = , 6|=w .000Lemma B.4 Let modular, law, 0. = hval(S ), R model0 , = {M : = hval(S ), Ri |= T} 0 M0M0.241fiVarzinczakM0M0Proof: Let 0 = hval(S 0 ), R0 |= 0 . |= T, result follows. Let usM0suppose 6|= T. analyze case.Let form hai>, Fml. Let = {M : = hval(S ), Ri}.Since hypothesis modular, Lemmas B.2 B.3 follows non-emptycontains models T.Suppose 0 minimal model 0 , i.e., 00 00 0M. 0 00 differ executabilityinVa given -world,Vviz. -context, IP(S ) = pi atm() pi pi atm() pipiM000pi/atm(). 6|= ( ) hai>, must |= ( ) hai>00|= T. Hence 0 minimal respect .contracting executability laws, 0 = . Hence taking right R minimal00R(w, w0 )= hval(S ), Ri R = R \ Ra , Ra {(w, w ) :|=wRa }, construct M0 = {M 0 } Mhai> .Let form [a], , Fml. Let = {M : = hval(S ), Ri}.Since hypothesis modular, Lemmas B.2 B.3 follows non-emptycontains models T.claim 0 oneVtransition linkingV -world, viz. contextIP(S ) = pi atm() pi pi atm() pi , atm(),pipi/0 -world, 0 IP(S ). proof follows: given ` Lit ` holds-world( `) [a]( `)/ 0 , `/ 0 |=( `) [a]`.Knworld `-successors.( `) [a]( `) 0 , every 0 -successor `-world.successively applying reasoning ` holds -world,end one 0 -successor.00Suppose 0 minimal model 0 , i.e., 00 |= 000 0 M. 0 00 differ effects-world: 00 transition linking 0 -world.0000|= (i ) [a]i , |= T. Therefore 0 minimal model 0respect .contracting effect laws, 0 = . Thus taking right R minimal R,0,,0= hval(S ), Ri R = R Ra , Ra {(w, w ) :|=w0wRelTarget(w, [a], , M)}, construct M0 = {M 0 } M[a] .Let Fml. Since modular, Lemmas B.2 B.3= hval(S ), Ri |= T. know val(S ) val(S ). [a] 0 ,R0a (v) = every -world v added 0 . Hence, minimal, taking ={M } gives us result.2242fiOn Action Theory ChangeProof Theorem 6.3hypothesis modular Lemma B.1, follows 0 modular,too. 0 = hval(S 0 ), Ri model 0 , Lemma B.3. Lemma B.4result follows.2ReferencesAlchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet contraction revision functions. Journal Symbolic Logic, 50, 510530.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2003).Description Logic Handbook. Cambridge University Press.Baral, C., & Lobo, J. (1997). Defeasible specifications action theories. Pollack, M.(Ed.), Proceedings 15th International Joint Conference Artificial Intelligence(IJCAI), pp. 14411446. Morgan Kaufmann Publishers.Booth, R., Meyer, T., & Varzinczak, I. (2009). Next steps propositional Horn contraction. Boutilier, C. (Ed.), Proceedings 21st International Joint ConferenceArtificial Intelligence (IJCAI), pp. 702707. AAAI Press.Burger, I., & Heidema, J. (2002). Merging inference conjecture information. Synthese, 131 (2), 223258.Castilho, M., Gasquet, O., & Herzig, A. (1999). Formalizing action change modallogic I: frame problem. Journal Logic Computation, 9 (5), 701735.Castilho, M., Herzig, A., & Varzinczak, I. (2002). depends context! decidablelogic actions plans based ternary dependence relation. 9th InternationalWorkshop Nonmonotonic Reasoning (NMR).Cholvy, L. (1999). Checking regulation consistency using SOL-resolution. Proceedings7th International Conference AI Law, pp. 7379.Dalal, M. (1988). Investigations theory knowledge base revision: preliminary report.Smith, R., & Mitchell, T. (Eds.), Proceedings 7th National ConferenceArtificial Intelligence (AAAI), pp. 475479. Morgan Kaufmann Publishers.De Giacomo, G., & Lenzerini, M. (1995). PDL-based framework reasoning actions. Gori, M., & Soda, G. (Eds.), Proceedings 4th Congress Italian Association Artificial Intelligence (IA*AI), No. 992 LNAI, pp. 103114.Springer-Verlag.Delgrande, J. (2009). Personal communication. Commonsense09, Toronto.Demolombe, R., Herzig, A., & Varzinczak, I. (2003). Regression modal logic. JournalApplied Non-Classical Logic, 13 (2), 165185.Eiter, T., Erdem, E., Fink, M., & Senko, J. (2005). Updating action domain descriptions.Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings 19th International JointConference Artificial Intelligence (IJCAI), pp. 418423. Morgan Kaufmann Publishers.243fiVarzinczakFinger, J. (1987). Exploiting constraints design synthesis. Ph.D. thesis, Stanford University.Flouris, G., Plexousakis, D., & Antoniou, G. (2004). Generalizing AGM postulates.10th International Workshop Nonmonotonic Reasoning (NMR).Fuhrmann, A. (1989). modal logic theory change. Logic Theory Change,pp. 259281.Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.MIT Press.Gelfond, M., & Lifschitz, V. (1993). Representing action change logic programs.Journal Logic Programming, 17 (2/3&4), 301321.Giunchiglia, E., Kartha, G., & Lifschitz, V. (1997). Representing action: indeterminacyramifications. Artificial Intelligence, 95 (2), 409438.Hamming, R. (1950). Error detecting error correcting codes. Bell System TechnicalJournal, 26 (2), 147160.Hansson, S. (1994). Kernel contraction. Journal Symbolic Logic, 59 (3), 845859.Hansson, S. (1999). Textbook Belief Dynamics: Theory Change Database Updating.Kluwer Academic Publishers.Harel, D., Tiuryn, J., & Kozen, D. (2000). Dynamic Logic. MIT Press.Herzig, A., Perrussel, L., & Varzinczak, I. (2006). Elaborating domain descriptions.Brewka, G., Coradeschi, S., Perini, A., & Traverso, P. (Eds.), Proceedings 17thEuropean Conference Artificial Intelligence (ECAI), pp. 397401. IOS Press.Herzig, A., & Rifi, O. (1999). Propositional belief base update minimal change. ArtificialIntelligence, 115 (1), 107138.Herzig, A., & Varzinczak, I. (2004). Domain descriptions modular. Lopez deMantaras, R., & Saitta, L. (Eds.), Proceedings 16th European ConferenceArtificial Intelligence (ECAI), pp. 348352. IOS Press.Herzig, A., & Varzinczak, I. (2005a). Cohesion, coupling meta-theory actions.Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings 19th International JointConference Artificial Intelligence (IJCAI), pp. 442447. Morgan Kaufmann Publishers.Herzig, A., & Varzinczak, I. (2005b). modularity theories. Schmidt, R., PrattHartmann, I., Reynolds, M., & Wansing, H. (Eds.), Advances Modal Logic, Vol. 5,pp. 93109. Kings College Publications.Herzig, A., & Varzinczak, I. (2006). modularity approach fragment ALC.Fisher, M., van der Hoek, W., Konev, B., & Lisitsa, A. (Eds.), Proceedings 10thEuropean Conference Logics Artificial Intelligence (JELIA), No. 4160 LNAI,pp. 216228. Springer-Verlag.Herzig, A., & Varzinczak, I. (2007). Metatheory actions: beyond consistency. ArtificialIntelligence, 171, 951984.244fiOn Action Theory ChangeJin, Y., & Thielscher, M. (2005). Iterated belief revision, revised. Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings 19th International Joint Conference ArtificialIntelligence (IJCAI), pp. 478483. Morgan Kaufmann Publishers.Katsuno, H., & Mendelzon, A. (1992). difference updating knowledgebase revising it. Gardenfors, P. (Ed.), Belief revision, pp. 183203. CambridgeUniversity Press.Kracht, M., & Wolter, F. (1991). Properties independently axiomatizable bimodal logics.Journal Symbolic Logic, 56 (4), 14691485.Levi, I. (1977). Subjunctives, dispositions chances. Synthese, 34, 423455.Li, R., & Pereira, L. (1996). believed explained. Shrobe, H., & Senator, T. (Eds.), Proceedings 13th National Conference Artificial Intelligence(AAAI), pp. 550555. AAAI Press/MIT Press.Liberatore, P. (2000). framework belief update. Proceedings 7th EuropeanConference Logics Artificial Intelligence (JELIA), pp. 361375.Makinson, D. (2007). Friendliness sympathy logic. Beziau, J.-Y. (Ed.), LogicaUniversalis, Vol. 2, pp. 195224. Springer-Verlag.Marquis, P. (2000). Consequence finding algorithms. Gabbay, D., & Smets, P. (Eds.),Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 5:Algorithms Uncertainty Defeasible Reasoning, edited J. Kohlas S.Moral, chap. 2, pp. 41145. Kluwer Academic Publishers.McCarthy, J. (1977). Epistemological problems artificial intelligence. Sridharan, N.(Ed.), Proceedings 5th International Joint Conference Artificial Intelligence(IJCAI), pp. 10381044. Morgan Kaufmann Publishers.McCarthy, J. (1998). Elaboration tolerance. Proceedings 4th International Symposium Logical Formalizations Commonsense Reasoning.McCarthy, J., & Hayes, P. (1969). philosophical problems standpointartificial intelligence. Meltzer, B., & Mitchie, D. (Eds.), Machine Intelligence,Vol. 4, pp. 463502. Edinburgh University Press.Nebel, B. (1989). knowledge level analysis belief revision. Brachman, R., Levesque,H., & Reiter, R. (Eds.), Proceedings 1st International Conference PrinciplesKnowledge Representation Reasoning (KR), pp. 301311. Morgan KaufmannPublishers.Parikh, R. (1999). Beliefs, belief revision, splitting languages. Moss, L. (Ed.),Logic, Language Computation, No. 96 CSLI Lecture Notes, pp. 266278. CSLIPublications.Popkorn, S. (1994). First Steps Modal Logic. Cambridge University Press.Quine, W. (1952). problem simplifying truth functions. American MathematicalMonthly, 59, 521531.Quine, W. (1962). Paradox. Scientific American, 1, 8496.245fiVarzinczakReiter, R. (2001). Knowledge Action: Logical Foundations Specifying Implementing Dynamical Systems. MIT Press.Schlechta, K. (2004). Coherent Systems. Studies Logic Practical Reasoning 2.Elsevier.Shanahan, M. (1997). Solving frame problem: mathematical investigation common sense law inertia. MIT Press.Shapiro, S., Lesperance, Y., & Levesque, H. (2005). Goal change. Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings 19th International Joint Conference ArtificialIntelligence (IJCAI), pp. 582588. Morgan Kaufmann Publishers.Shapiro, S., Pagnucco, M., Lesperance, Y., & Levesque, H. (2000). Iterated belief changesituation calculus. Cohn, T., Giunchiglia, F., & Selman, B. (Eds.), Proceedings7th International Conference Principles Knowledge RepresentationReasoning (KR), pp. 527538. Morgan Kaufmann Publishers.Thielscher, M. (1997). Ramification causality. Artificial Intelligence, 89 (12), 317364.Thielscher, M. (2010). unifying action calculus. appear Artificial Intelligence.Varzinczak, I. (2006). good domain description? Evaluating revising actiontheories dynamic logic. Ph.D. thesis, Universite Paul Sabatier, Toulouse.Varzinczak, I. (2008). Action theory contraction minimal change. Lang, J., &Brewka, G. (Eds.), Proceedings 11th International Conference PrinciplesKnowledge Representation Reasoning (KR), pp. 651661. AAAI Press.Varzinczak, I. (2009). revision action laws: algorithmic approach. IJCAIWorkshop Nonmonotonic Reasoning, Action Change (NRAC).Winslett, M.-A. (1988). Reasoning action using possible models approach. Smith,R., & Mitchell, T. (Eds.), Proceedings 7th National Conference ArtificialIntelligence (AAAI), pp. 8993. Morgan Kaufmann Publishers.Zhang, D., Chopra, S., & Foo, N. (2002). Consistency action descriptions. Ishizuka,M., & Sattar, A. (Eds.), Proceedings 7th Pacific Rim International ConferenceArtificial Intelligence, No. 2417 LNCS, pp. 7079. Springer-Verlag.Zhang, D., & Foo, N. (2001). EPDL: logic causal reasoning. Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI),pp. 131138. Morgan Kaufmann Publishers.Zhang, Y., & Ding, Y. (2008). CTL model update system modifications. JournalArtificial Intelligence Research, 31, 113155.246fiJournal Artificial Intelligence Research 37 (2010) 479-525Submitted 12/09; published 03/10Multiattribute Auctions Based Generalized AdditiveIndependenceYagil Engelyagile@ie.technion.ac.ilTechnion - Israel Institute TechnologyFaculty Industrial Engineering & ManagementTechnion City, Haifa 32000, IsraelMichael P. Wellmanwellman@umich.eduUniversity MichiganDivision Computer Science & Engineering2260 Hayward St, Ann Arbor, MI 48109-2121, USAAbstractdevelop multiattribute auctions accommodate generalized additive independent(GAI) preferences. propose iterative auction mechanism maintains pricespotentially overlapping GAI clusters attributes, thus decreases elicitation computational burden, creates open competition among suppliers multidimensionaldomain. significantly, auction guaranteed achieve surplus approximates optimal welfare small additive factor, reasonable equilibrium strategiestraders. main departure GAI auctions previous literature accommodate non-additive trader preferences, hence allowing traders condition evaluationspecific attributes value attributes. time, GAI structuresupports compact representation prices, enabling tractable auction process. perform simulation study, demonstrating quantifying significant efficiency advantageexpressive preference modeling. draw random GAI-structured utility functionsvarious internal structures, generate additive functions approximate GAIutility, compare performance auctions using two representations.find allowing traders express existing dependencies among attributes improveseconomic efficiency multiattribute auctions.1. IntroductionMultiattribute trading mechanisms extend traditional, price-only mechanisms facilitating negotiation set predefined attributes representing various non-price aspectsdeal. Rather negotiate fully specified good service, multiattributemechanism delays commitment particular configurations extracts sufficient information traders preferences. example, companys procurement department mayrun multiattribute auction select supplier hard drives. Supplier offers mayevaluated price offer, also features volume, RPM,access time, latency, transfer rate, on. addition, suppliers may offer contractsdiffering terms warranty, delivery time, service.order account traders preferences, auction mechanism must extract evaluative information complex domain multidimensional configurations. Constructingcommunicating complete preference specification pose severe burden evenc!2010AI Access Foundation. rights reserved.fiEngel & Wellmanmoderate number attributes, hence practical multiattribute auctions must either accommodate partial specifications, support compact expression preferences assumingsimplified form. far popular multiattribute form adopt simplest:additive representation overall value linear combination values associatedattribute. example, several recent proposals iterative multiattribute auctions(Beil & Wein, 2003; Bichler, 2001; David, Azoulay-Schwartz, & Kraus, 2002; Parkes &Kalagnanam, 2005) require additive preference representations.additivity reduces complexity preference specification exponentially (compared general discrete case), precludes expression interdependencies amongattributes. practice, however, interdependencies among natural attributes quitecommon. example, hard-drive buyer may exhibit complementary preferencesvolume access time (since performance effect salient much data involved), may view strong warranty good substitute high reliability ratings.Similarly, sellers production characteristics easily violate additivity, exampledecreasing access time technically difficult higher-capacity drives. casesadditive value function may able provide adequate approximation realpreferences.hand, fully general models intractable, multiattribute preferencestypically exhibit structure. goal, therefore, identify subtler yetwidely applicable structured representations, exploit properties preferencestrading mechanisms.propose iterative auction mechanism based flexible preferencestructure. approach inspired design iterative multiattribute procurementauction additive preferences, due Parkes Kalagnanam (2005) (PK). PK presenttwo auction designs: first (NLD) makes assumptions traders preferences,lets sellers bid full multidimensional attribute space. NLD maintainsexponential price structure, suitable small domains. auction (AD)assumes additive buyer valuation seller cost functions. collects sell bids per attributelevel single discount term. price configuration sum priceschosen attribute levels minus discount.auction propose also supports compact price spaces, albeit levels clusters attributes rather singletons. employ preference decomposition basedgeneralized additive independence (GAI), model flexible enough accommodate interdependencies exact degree accuracy desired, yet providing compact functionalform extent interdependence limited.First, build direct, formally justified link preference statements pricedoutcomes generalized additive decomposition willingness-to-pay (wtp) function.laying infrastructure, employ representation tool developmentmultiattribute iterative auction mechanism allows traders express complexpreferences GAI format. study auctions allocational, computational,practical properties. Next, present simulation study proposed auction mechanism, order practically evaluate economic computational properties GAIauctions. simulate auctions using random GAI utility functions, includingbased constrained preference structures often exhibited applications. simulationslet us quantify benefits modeling preferences accurately using GAI, comparison480fiGAI Auctionsusing additive approximation. show circumstances, GAI auctionachieves significantly higher surplus auction uses additive approximationpreferences.providing background multiattribute preferences multiattribute auctions(Section 2), develop new multiattribute structures wtp functions, supporting generalized additive decompositions (Section 3). describe auction mechanism Section 4,followed detailed example Section 5, study mechanisms allocational, computational, practical properties Section 6. present simulation frameworkSection 7, discuss experimental results Section 8.2. Backgroundsection provide essential background multiattribute preferences (Sections 2.12.2) multiattribute auctions (Section 2.3).2.1 Multiattribute Preferences UtilityLet denote space possible outcomes, ! preference relation (weak total order). Let = {a0 , . . . , }!be set attributes describing . attributedomain D(a), ni=1 D(ai ). Capital letters denote subsets attributes, smalllatin letters (with without numeric subscripts) denote specific attributes, X = A\X.(and variations " ) indicate specific outcome . instantiation subsetattributes denoted using prime signs (as " ) numerical superscript (as 1 ).particular, " projection instantiations . representinstantiation subsets X, time use sequence instantiation symbols,X 1 2 .preference relation ! outcomes usually represented numerically valuefunction v() (Keeney & Raiffa, 1976).Definition 1 (Value Function). v : % value function representing !, " , v() v( " ) iff ! " .Clearly, monotonic transformation v() also value function !.many cases useful represent, beyond simple preference order outcomes,notion strength preferences. value function expresses strength preferencescalled cardinal value function. measurable value function well-established cardinalpairs outcomes.value framework posits existence preference order !""""! ! , statement (, ) ! (, ) means strengthpreference " greater equal " . Krantz, Luce, Suppes,Tversky (1971) establish set axioms ensuring existence utility functionrepresenting !.Definition 2 (Measurable Value Function). measurable value function (MVF)value function u : %, , " , , " , ! " ! " ,following holds:(, " ).u( " ) u() u( " ) u() (, " ) !(1)481fiEngel & WellmanHence order differences values u() correspond exactly orderpreference differences. Note MVF also used value function representing( "" , ) iff " ! "" , .!, ( " , ) !auction theory mechanism design, traders preferences usually representedusing quasi-linear value function, v(, p) = u()+p, p represents monetaryoutcome.1 cardinal value function u() expresses strength preference,difference u( " ) u( "" ) corresponds additional amount trader willing pay" relative "" . example, " represents red Mercedes sunroof, ""denotes blue Subaru sunroof, u( " ) u( "" ) strength preferenceMercedes configuration Subaru. Mercedes costs p" Subaru p"" ,according v(, p) trader prefers Mercedes deal iff u( " ) u( "" ) p" p"" .fact, u() easily shown MVF, preference differences correspond differences willingness-to-pay (Engel & Wellman, 2007). reason,use MVF basis utility work, assume traders willingness-to-pay(wtp) functions constitute MVF.Reasoning full outcomes hard several ways. notably, difficulthumans compare outcomes many dimensions, complex machines storeanalyze preferences number outcomes exponential numberattributes. therefore useful consider preferences joint productA, considering rest attributes fixed predefined values.order also often referred ceteris paribus preference orderone partial outcomepreferred another else equal.Definition 3 (Conditional Preference). Partial outcome 2 conditionally preferred"""partial outcome 1 given , 1 ! 2 . conditional preference order"""given denoted !Y ! , hence 1 ! 2 abbreviated 1 !Y ! 2 .general, conditional preferences may depend particular assignment chosenrest attributes. precisely, 1 ! 2 , could still find 2 !! 1""",= . case, one needs maintain conditional preferenceorders !Y ! !Y !! , hence general scheme might yield computationalbenefits. Fortunately, many cases one identify subsets preferencereversal occur, preference order invariant instantiation.Definition 4 (Preferential Independence). preferential independent (PI) ,"""written PI(Y, ), 1 2 , , , 1 !Y ! 2 iff 1 !Y !!2.First-order preferential independence (FOPI), independence single attributerest, natural assumption many domains. example, typical purchase decisions greater quantity higher quality desirable regardless assignmentsattributes. Preferential independence higher order, however, requires invariancetradeoffs among attributes respect variation others, stringent independence condition. MPI condition, defined below, global setattributes A, requires possible subsets PI.1. use term trader referring either buyers sellers.482fiGAI AuctionsDefinition 5 (Mutual Preferential Independence). Attributes mutually preferential independent (MPI) iff A, P I(Y, ).Preferential independence greatly simplify form v.Theorem 1 (Debreu, 1959). preference order set attributes representedadditive value functionv(a1 , . . . , ) =n"vi (ai ),i=1iff mutually preferential independent.Dyer Sarin (1979) extend additivity theory MVF, specify conditionsu() well additive structure above. Effectively, additive formsused trading mechanisms assume MPI full set attributes, including moneyattribute. Intuitively means willingness-to-pay levels attribute attributescannot affected instantiation attributes. sweeping condition rarelyholds practice (Von Winterfeldt & Edwards, 1986). Therefore, recent AI literature oftenrelaxes MPI assumption imposing additivity respect subsets attributesmay overlap.Definition 6 (Generalized AdditiveIndependence). Let I1 , . . . , Ig (not necessarily#disjoint) subsets A, gi=1 Ii = A. elements I1 , . . . , Ig called generalizedadditive independent (GAI) exist functions f1 , . . . , fg that,u(a1 , . . . , ) =g"fr (Ir ).(2)r=12.2 Related Work Generalized Independencedefinition GAI somewhat nonstandard. literature defines GAI conditionexpected utility function (von Neumann & Morgenstern, 1944). well-knownmodel, particular choice results lottery, probability distribution outcomes. expected utility function represents complete preference order lotteries.Informally, GAI definition requires preferences lotteries dependmargins subsets I1 , . . . , Ig . form Eq. (2) result definition,obtained Fishburn (1967). Fishburn introduces functional decomposition,also provides well-defined form functional constituents f1 , . . . , fg . Graphicalmodels elicitation procedures GAI decomposable utility developed withinexpected utility framework (Bacchus & Grove, 1995; Boutilier, Bacchus, & Brafman, 2001;Gonzales & Perny, 2004; Braziunas & Boutilier, 2005). addition, generalized additiveutility models employed Hyafil Boutilier (2006) aid direct revelation mechanisms, Robu, Somefun, La Poutre (2005) opponent modelingbilateral multi-item negotiation.Bacchus Grove (1995), fact coined term GAI, show decomposition also obtained result collection local, easier detect, binary483fiEngel & Wellmanindependence conditions. specifically, rely form called conditional additiveindependence, which, informally, corresponds GAI decomposition limited two (overlapping) subsets X A. prove condition expressedseparation criterion graph whose nodes correspond A, means perfectmap (Pearl, 1988). Crucially, utility function decomposes GAI form lower dimensional functions, defined maximal clique graph. combinedFishburns work, result provides well-defined functional form obtainedcollection conditional additive independence conditions. result reliesform lotteries basis utility function independence conditions.expression willingness-to-pay requires cardinal measure preferences, yetwithout uncertainty, need expected utility representation. thereforeinvoke MVF framework, Section 3, build additive decompositions MVFdeveloped Dyer Sarin (1979) develop multiattribute preference structures wtp.development enables us follow footsteps Fishburn (1967) BacchusGrove (1995) show well-defined GAI form MVF also obtained usingcollection easy-to-detect binary independence conditions.2.3 Multiattribute Auctionsdistinguishing feature multiattribute auction goods definedvectors attributes. above, use denote set attributes describing domain. configuration particular attribute vector, . Multiattribute auctions usedprimarily procurement, part strategic sourcing processes (Sandholm, 2007).procurement model single buyer, utility function (representingwillingness-to-pay) ub () purchasing . sellers s1 , . . . , sm utilityfunctions ci : %, representing cost si supply configurations buyer.Definition 7 (Multiattribute Allocation Problem). multiattribute allocation problem (Parkes & Kalagnanam, 2005) is:MAP =maxi{1,...,m},ub () ci ().(3)allocation (si , ) solving MAP said maximize surplus procurementproblem.MAP decomposed two subproblems: first find efficient configurationtrader, find trader whose efficient configuration yields highestsurplus. call first part multiattribute matching problem (Engel, Wellman, &Lochner, 2006).Definition 8 (Multiattribute Matching Problem). multiattribute matching problem (MMP) buyer b seller si is:MMP(b, si ) = arg max ub () ci ().also call configuration selected MMP(b, si ) bilaterally efficient configurationsi .484fiGAI Auctionstheoretical work surplus-maximizing multiattribute auctions relatesway foundational work Che (1993). Ches model, good servicecharacterized single quality attribute, seller independent privatecost function quality. buyer announces scoring rule sellers,price-quality offers evaluated. Che suggests several types auctions, includingsecond-score auction, seller bidding highest score wins, must providecombination price quality achieves second-best score. second-scoremechanism, bidding truthfully equilibrium dominant strategies. particular, Cheshows sellers bid quality maximizes difference buyersscoring rule cost function; words, respective MMP solution.Branco (1997) generalizes Ches model results correlated costs.basic model later generalized several authors account explicitly multiple quality attributes, usually restricting scoring rule additiveattributes (Bichler, 2001; David et al., 2002). Vulkan Jennings (2000) suggest modified version English auctions (iterative auctions require new bids incrementcurrent bid price) bidders required improve current score, ratherprice. Sandholm Suri (2006) consider incorporation non-price attributesmulti-item (combinatorial) auctions.literature surveyed emphasizes auctions require buyer revealscoring function prior bidding. order achieve economic efficiency, scoringfunction must convey buyers full utility function ub (). major obstaclepractical adaption mechanisms. Procurement auctions rarely isolated event,buyer-supplier relationships usually evolve change time,suppliers may retain market power, take advantage information revealedbuyer. Events sometimes conducted recurrent basis, several events mayconducted related goods correlated valuations. addition, buyer may wishkeep secret way utility may discriminating particular suppliers(Koppius, 2002).noted Section 1, Parkes Kalagnanam (2005) suggest alternative approach,employs prices space configurations drive traders efficient configurations. Auction NLD maintains price , sellers bid fullconfiguration round. Auction AD maintains price level a"i D(ai ). Pricesinitially set high. round, sellers bid particular value attribute,auction selects set levels (again, per attribute) myopically buyer preferred round, is, approximately maximize buyers utility respectcurrent prices. addition, auction maintains discount factor applied ensuresingle seller eventually selected. price configuration defined sumprices chosen attribute levels minus discount. round, pricesparticular levels particular attributes decremented constant ", according setprice change rules, ensuring auction ultimately converges efficient solution.auctions shown obtain optimal surplus (up "-proportional error),sellers bid myopically rather strategically (we define concept formally Section 6.1). myopic behavior shown ex-post Nash equilibrium. Auction NLDfully expressive tractable number attributes large. Auction ADcomputationally efficient, expressiveness limited additive preferences (see485fiEngel & Wellmandiscussion following Theorem 1). traders preferences additive, welfareachieved auction necessarily optimal; is, solve MAP optimally,respect inaccurate utility functions. Moreover, clear lackexpressiveness may affect incentives traders act strategically.Theoretically, one could also use well-known Vickrey-Clake-Grove (VCG) mechanism. Parkes Kalagnanam define sell-side VCG mechanism: traders submitfull utility cost functions, MAP solved auction engine, winningseller pays according VCG price (definition pricing provided Section 6.1).auction, traders allowed use compact preference structure, including GAI. However, scheme suffers disadvantages proposalsrequire full revelation utility.summarize, previously suggested surplus-maximizing multiattribute procurementauction time expressive (accommodates interdependencies attributes),tractable (its computations depend fully exponential domain), preservingbuyers private information, meaning (minimally) require buyerreveal full utility function extracting bids sellers. proposed mechanism, show theoretically using simulations, possesses attractive propertiescriteria.3. Detection GAI Structure Measurable Value Functionssection provide basis application GAI decomposition procurementproblems. Section 3.1 show GAI obtained collection local, weakerconditions based invariance willingness-to-pay. Section 3.2 useexample demonstrate process used procurement problems.3.1 Difference Independence GAIDyer Sarin (1979) introduce measurable value analog additive independence,called difference independence. first step introduce conditional generalizationdefinition.Definition 9 (Conditional Difference Independence). Let X, X = ,define Z = \ X . X conditionally difference independent , denotedCDI(X, ), Z " D(Z), X 1 , X 2 D(X), 1 , 2 D(Y ),(X 1 1 Z " , X 2 1 Z " )(X 1 2 Z " , X 2 2 Z " ),(4)1 hold.symbolindicates !definition MVFs (1), CDI condition (4) expressed equivalentlyterms measurable value:u(X 1 1 Z " ) u(X 2 1 Z " ) = u(X 1 2 Z " ) u(X 2 2 Z " )condition states value, willingness-to-pay, change assignmentX depend current assignment , fixed value Z.CDI condition leads convenient decomposition MVF.486fiGAI AuctionsLemma 2. Let u(A) MVF representing preference differences, X, Y, Z specified Definition 9. CDI(X, ) iffu(A) = u(X 0 , Y, Z) + u(X, 0 , Z) u(X 0 , 0 , Z),arbitrary instantiations X 0 , 0 .single CDI condition, therefore replace n-ary function u(X, Y, Z)two lower-dimensional functions u(X 0 , Y, Z) u(X, 0 , Z). reasonable assumeone apply CDI conditions decompose resulting functions.order take full advantage existing CDI conditions, introduce notiondependency graph, simplification concept perfect map mentionedSection 2.2.Definition 10 (Dependency Graph). Let denote set, R binary relation2S . graph G = (S, E) dependency graph R S1 , S2 S, holds(S1 , S2 ) R iff a1 S1 a2 S2 , (a1 , a2 )/ E.Hence dependency graph expresses R separation criterion; two subsetsdirect connection iff R. dependency graph CDI constructedsimply removing edge (a1 , a2 ) CDI({a1 }, {a2 }); CDI(S1 , S2 )holds iff CDI({a1 }, {a2 }) holds a1 S1 a2 S2 . use term CDI mapdependency graph induced CDI relation.next theorem links CDI condition, CDI map, GAI decompositionA. fact, establishes functional constituents GAI decompositionMVF functional constituents GAI decomposition expectedutility model, defined Fishburn (1967). adopt following conventional notation.Let (a01 , . . . , a0n ) predefined vector called reference outcome. A,function u([I]) stands projection u(A) rest attributesfixed reference levels.Theorem 3 (CDI-GAI Theorem). Let G = (A, E) CDI map A, {I1 , . . . , Ig }set (overlapping) maximal cliques G.u(A) =g"fr (Ir ),(5)r=1f1 = u([I1 ]),r = 2, . . . , g,fr = u([Ir ]) +r1"(6)(1)jj=1"1i1 <<ij <ru([j$Iis Ir ]).s=1small example, Table 1 exhibits utility function u(x1 , x2 , x3 ). threeattributes boolean domain, D(xi ) = {0, 1}. Let x0i x1i denote assignments 0 1 (respectively) xi . first observe CDI({x1 }, {x3 }) holds because:22. Note x02 x12 correspond Z ! Definition 9.487fiEngel & Wellmanx101010101x200110011x300001111u(x1 , x2 , x3 )052638711u(x1 , x2 , x03 )05260526u(x01 , x2 , x3 )00223377u(x01 , x2 , x03 )00220022u1 (I1 )05260526u2 (I2 )00003355Table 1: utility function three attributes, decomposable via GAI sum twofunctions two attributes each. u1 () depends {x1 , x2 } u2 () depends{x2 , x3 }.1. utility difference values x1 given x02 5, x03 x13 . explicitly,u(x11 , x02 , x03 ) u(x01 , x02 , x03 ) = 5 0 = 5, u(x11 , x02 , x13 ) u(x01 , x02 , x13 ) = 8 3 = 5.2. Similarly, difference x1 given x12 4, x03 x13 .Though x1 x3 CDI other, see depend x2 . example,differences mentioned x1 5 4 given x02 x12 (respectively), hencedifference x1 given fixed x3 depends value x2 . CDI map exampletherefore includes edge (x1 , x2 ) edge (x2 , x3 ). maximal cliques graphI1 = {x1 , x2 } I2 = {x2 , x3 }.obtain numeric decomposition, first define (x01 , x02 , x03 ) reference values.Next, (6), get u1 (I1 ) = u([I1 ]) = u(x1 , x2 , x03 ) u2 (I2 ) = u([I2 ]) u([I1 I2 ]) =u(x01 , x2 , x3 ) u(x01 , x2 , x03 ). functions involved given Table 1. Notefifth sixth columns obtained appropriate values fourth column;example, u(x01 , x2 , x3 ) line x1 = 1, x2 = 1, x3 = 0 value u(x1 , x2 , x3 ) linex1 = 0, x2 = 1, x3 = 0. easy verify indeed u(x, y, z) = u1 (I1 ) + u2 (I2 ).CDI-GAI Theorem provides operational form GAI, establishing GAIdecomposition obtained collection simple CDI conditions. assumption detection CDI conditions performed incrementally, MVFdecomposed reasonable dimension. CDI conditions, turn, basedinvariance preference differences, relatively intuitive detect. particularlytrue differences carry direct interpretation, case willingness-to-pay:check invariance monetary amount buyer willing pay get one outcomeother.GAI decomposition depicted graphically using clique graph CDImap, is, graph whose nodes correspond maximal cliques CDI map.purposes convenient use particular clique graph called tree decomposition (orjunction tree). introduce well-known concept, discuss implications GAIrepresentation.Definition 11 (Tree Decomposition). tree decomposition graph G = (N, E)pair (T, I), = (, E) acyclic graph, = {Ii | } collection488fiGAI AuctionstermMAPMMPMVFPIFOPIMPIGAICDICDI mapGAI treeMeaningMultiattribute Allocation ProblemMultiattribute Matching ProblemMeasurable Value FunctionPreferential IndependenceFirst-Order Preferential IndependenceMutual Preferential IndependenceGeneralized Additive IndependenceConditional Difference Independencegraph whose separation criterion CDItree decomposition CDI mapReference(Parkes & Kalagnanam, 2005)(Engel et al., 2006)(Dyer & Sarin, 1979)(Keeney & Raiffa, 1976)(Keeney & Raiffa, 1976)(Bacchus & Grove, 1995)cf. (Bacchus & Grove, 1995)cf. (Gonzales & Perny, 2004)Table 2: Acronym terms, references related literature. Empty references indicateterms introduced work. terms arranged according topics: (i) multiattribute economic problems, (ii) independence relations, (iii) graphical concepts.#subsets N , corresponding node , (i) iI Ii = N , (ii) edge(n1 , n2 ) E, exists Ii n1 , n2 Ii , (iii) (running intersection)i, j, k , j path k Ii Ik Ij .graph tree-decomposed, typically one way. example,single node I. width tree decomposition maxiI |Ii | 1,treewidth graph minimum width among possible tree decompositions.easy show maximal clique G contained within I. Therefore, Theorem 3, utility function decomposes additively subsets = {Ii |}, = (, E) tree decomposition CDI map. notion GAI treeadapted work Gonzales Perny (2004), introduce GAI graphical modelsexpected utility framework.Definition 12 (GAI Tree). GAI tree u() tree decomposition CDI mapu().therefore refer elements I1 , . . . , Ig GAI decomposition settree decomposition. next subsection provides qualitative example CDI concept,dependency graph, corresponding GAI tree.results section lay foundations using GAI decompositionmultiattribute trading mechanisms. results generalize additive MVF theory, justifyapplication methods developed expected utility framework (Bacchus &Grove, 1995; Boutilier et al., 2001; Gonzales & Perny, 2004; Braziunas & Boutilier, 2005)representation monetary value certainty. Table 2 summarizes acronymterminology introduced point.3.2 Employing GAI Procurementsection demonstrate process obtaining GAI decompositioncollection CDI conditions. addition, example used motivate approach489fiEngel & Wellmancomparison work Parkes Kalagnanam (2005). Consider procurementdepartment wishes purchase new hard drives (HD) desktops largenumber employees. buyer cares several characteristics (attributes) harddrives particular terms procurement contract. attribute listeddesignated attribute name (the first letter), domain. cases (e.g., attributeI) use arbitrary symbols represent domain elements, abstracting meaningfulinterpretation assumed context.RPM (R) 3600, 4200, 5400 RPMTransfer rate (T) 3.4, 4.3, 5.7 MBSVolume (V) 60, 80, 120, 160 GBSupplier ranking (S) 1, 2, 3, 4, 5Quality rating (Q) (of HD brand) 1, 2, 3, 4, 5Delivery time (D) 10, 15, 20, 25, 30, 35 daysWarranty (W) 1, 2, 3 yearsInsurance (I) (for case deal signed implemented) 1 , 2 , 3Payment timeline (P) 10, 30, 90 daysCompatibility (C) (with existing hardware software) 1 , 2 , 3Consider, example, pair attributes Quality Warranty. value warranty different different values quality; higher quality knownlow, lower quality known high. two attributes therefore depend other. Furthermore, might expect Volume complements QualityWarranty. Larger hard drives prone failures, making quality warranty valuable. Similarly, interdependence Supplier rankingcontract insurance buy, Supplier ranking warranty supplier provides. reasonable dependencies among Delivery, Insurance, Payment timeline(e.g., later delivery requires better insurance, later payment reduces need insurance),Volume RPM Transfer rate. Preferences compatibility maydepend attribute. corresponding CDI map depicted Figure 1a.described Section 2.1, utility function decomposes elements treedecomposition CDI map. tree decomposition depicted Figure 1b.example set elements tree decomposition correspond exactly maximalcliques CDI map. general tree decomposition might include supersetsmaximal cliques, decomposition obviously maintained supersetswell.Non-additive traders, required deal additive price space auctionAD (Parkes & Kalagnanam, 2005), face exposure problem, somewhat analogous traderscombinatorial preferences participate simultaneous auctions (Wellman, Osepayshvili, MacKie-Mason, & Reeves, 2008). Essentially, problem manifest490fiGAI Auctions(a)(b)Figure 1: HD procurement problem: (a) CDI map, (b) GAI tree.two ways. One type exposure occurs one auction round another,following two-attribute example. sellers conditional preference order attributemay optimized assignment a1 given attribute b b1 ,assignment b changes, a1 may become arbitrarily suboptimal. Therefore bidding a1b1 may result poor allocation seller outbid b1 (and thus must resortanother assignment) left winning a1 . second exposure occurs single roundauction, trader bids multiple configurations. example, suppose configurations (a1 , b1 ) (a2 , b2 ) optimal current prices. bids collectedindependently attribute, trader bidding may end configuration(a1 , b2 ), again, may arbitrarily suboptimal.prevent exposure sellers part taking simple measures auctiondesign. First, bids collected anew round, independently previous rounds, hencefirst problem avoided. Sellers likewise avoid second problem limitingbid one configuration per round.buyers side, solution work require buyer bidfull set optimal configurations round, order ensure auctions convergence (this becomes clearer Section 6.1). prevent buyer exposure, auction designstructures prices according buyers preferences, traders bid clustersinterdependent attributes. terms example above, b interdependent(meaning CDI({a}, {b}) hold), able bid cluster ab. bturn depends c, need another cluster bc. still simpler general pricingstructure solicits bids cluster abc. generally, find reasonable CDIconditions correct buyer, obtain corresponding GAI tree decomposition, solicit bids clusters attributes corresponding GAI elements.Section 4, describe auction design detail, along example Section 5.Section 6.1, prove auction terminates (approximately) optimal solutionMAP.4. GAI Auctionsintroducing auction design, reiterate model notation, providedefinition facilitates auction presentation.491fiEngel & Wellman4.1 Notations Definitionsprocurement setting, single buyer wishes procure single good, configuration one candidate sellers s1 , . . . , sm . buyer privatevaluation function ub : %+ , similarly seller si private cost function, ci .ub () ci () MVFs, utility differences express differences willingnessto-pay, explained Section 2.1. Assume buyers preferences reflectedGAI structure I1 , . . . , Ig . call assignment GAI element Ir sub-configuration.use r denote sub-configuration formed projecting configuration element Ir .Definition 13 (Consistent Cover). collection sub-configurations {1 , . . . , g },r {1, . . . , g}, r instantiation Ir , consistent coverr, r " {1, . . . , g}, attribute aj Ir Ir! , r r! agree assignmentaj .words, consistent cover collection sub-configurationscompose valid configuration. collection {1 , . . . , g } consistent coverequivalently considered configuration, denote (1 , . . . , g ). example,consider good three attributes: a, b, c. attributes domain two possibleassignments (e.g., {a1 , a2 } domain a). Let GAI structure I1 = {a, b}, I2 ={b, c}. Here, sub-configurations assignments form a1 b1 , a1 b2 , b1 c1 , on.set sub-configurations {a1 b1 , b1 c1 } consistent cover, corresponding configurationa1 b1 c1 . contrast, set {a1 b1 , b2 c1 } inconsistent.4.2 GAI Auctiondefine iterative, descending-price multiattribute auction maintains GAI pricing structure: is, round t, price pt (), corresponding subconfiguration GAI element. price pt () configuration rounddefined terms sub-configuration prices global discount term ,p () =g"pt (r ) .(7)r=1Importantly, elements r may refer overlapping attributes. Bidders submit subbids sub-configurations global discount .3 Sub-bids submittedround expire next round. sub-bid round configuration rautomatically assigned price pt (r ). set full bids seller contains consistentcovers generated sellers current set sub-bids. existencefull bid configuration represents sellers willingness accept price pt ()supplying .start auction, buyer reports (to auction, sellers) completevaluation function ub (). GAI, expresseddecomposed form (6)%local functions (fb,1 , . . . , fb,g ), ub () =r fb,r (r ). initial prices subconfigurations set level buyers valuations, is, p1 (r ) > fb,r (r )r . discount initialized zero. auction dynamics descending3. discount term could replaced uniform price reduction across sub-configurations.492fiGAI Auctionsclock auction: round t, bids collected current prices pricesreduced according price rules. seller considered active round set subbids submitted contains least one full bid. round > 1, sellersactive round 1 allowed participate, auction terminatessingle seller active. denote set sub-bids submitted si Bit ,corresponding set full bidsBit = { = (1 , . . . , g ) | {1 , . . . , g } Bit }.example Section 4.1, seller could submit sub-bids set sub-configurations{a1 b1 , b1 c1 }, combines full bid a1 b1 c1 .auction proceeds two phases. first phase (A), round auctioncomputes set buyer-preferred sub-configurations Mt : sub-configurationspart configuration within " profit-maximizing buyercurrent prices. Formally, first define buyer profit configuration as4bt () = ub () pt ().buyer-preferred set sub-configurations defined by:Mt = {r | bt () maxbt ( " ) ", r = 1, . . . , g}.!Section 6.2 show Mt computed efficiently. stress though Mtset sub-configurations, criterion selecting based profitfull configurations. Profits individual sub-configurations meaningless outsidecontext configurations.Phase A, auction adjusts prices round, reducing price everysub-configuration received bid buyers preferred set. Let "prespecified priceparameter. Specifically, Phase price change rule#mdecrementapplied r i=1 Bi \ :"pt+1 (r ) pt (r ) .g[A]Let denote set configurations consistent covers Mt := { = (1 , . . . , g ) | {1 , . . . , g } Mt }.auction switches Phase B active sellers least one full bidbuyers preferred set:i. Bit = Bit ,= .[SWITCH]Let round [SWITCH] becomes true. point, auction selectsbuyer-optimal full bid seller si .= arg max (bT ()).BiT(8)4. drop superscript generic statements involving price profit functions, understandingusage respect (currently) applicable prices.493fiEngel & WellmanPhase B, si may bid . Sub-configuration prices fixed pT ()phase. adjustment , increased every round ". (7),increase decreases current price configurations . auctionterminates one seller (if exactly one, designate si ) active. allocationdetermined according four distinct cases:1. sellers drop Phase (i.e., rule [SWITCH] holds). auctionterminates allocation.2. active sellers drop round Phase B. sellersdropped last round, auction selects seller si ub (i ) pT (i )maximal, designates seller winner si . single winner,appropriate case 3 4 applied.3. auction terminates Phase B final price buyers valuation,pT (i ) > ub (i ). still possible exactly one seller (the winningseller) whose cost buyers valuation, case trade positivesurplus possible. Therefore, auction offers winner si opportunitysupply price ub (i ).4. auction terminates Phase B final price pT (i ) ub (i ).ideal situation, auction allocates chosen configuration sellerresulting price.Collect reported valuation, ub () buyer;Set high initial prices, p1 (r ) sub-configuration r , set = 0;[SWITCH], least one active sellerCollect sub-bids sellers;Compute Mt ;Apply price change [A];endCompute ;one active sellerIncrease ";Collect bids (i , ) sellers;endImplement allocation payment winning seller;Algorithm 1: GAI-based multiattribute auction.overall auction described high-level pseudocode Algorithm 1. rolePhase guide traders efficient configurations (MMP solutions),reducing prices configurations chosen least one seller preferredbuyer. price reduction makes configurations slightly less attractive sellerslightly attractive buyer. Phase B one-dimensional competitionprofit remaining seller candidates provide buyer. next sectionformalize notions, prove Phase indeed converges Phase B494fiGAI AuctionsI1fbf1f2a1 b1653535a2 b1502020I2a1 b2553025a2 b2707025b1 c1506555b2 c18565110b1 c2607070b2 c2756195Table 3: GAI utility functions example domain. fb represents buyers valuation,f1 f2 costs sellers s1 s2 .selects seller whose efficient configuration yields (approximately) highest surplus.Section 6.2 discuss computational tasks associated auction.5. GAI Auction Exampleillustrate auction simple three-attribute scenario, employing two-elementGAI structure I1 = {a, b}, I2 = {b, c}. Table 3 shows GAI utilities buyertwo sellers s1 , s2 . efficient allocation (s1 , a1 b2 c1 ): buyers valuation55+ 85 = 140 cost s1 configuration (boldface table) 30+ 65 = 95,hence surplus 45. maximal surplus second-best seller, s2 , 25, achieveda1 b1 c1 , a2 b1 c1 , a2 b2 c2 . set initial prices I1 75, initial pricesI2 90, " = 8, meaning price reduction sub-configurations ("/g) 4.sake example assume seller bids round configuration maximizes profit (price minus cost), respect prices currentround. next section provide formal definitions prove incentive propertiesstrategy.Table 4 shows progress Phase A. Initially configuration price(165), sellers bid lowest-cost configurationa2 b1 c1 (with profit 80 s190 s2 )realized sub-bids a2 b1 b1 c1 . M1 contains sub-configurationsa2 b2 b2 c1 highest value configuration a2 b2 c1 , yields buyer profit 10.show next section (Lemma 7), maximum change throughoutPhase A. Price therefore decreased a2 b1 b1 c1 . price change, profits1 a2 b1 c1 72, higher profit (74) a1 b2 c2 bids a1 b2b2 c2 . (round 2) prices go down, reducing profit a1 b2 c2 66 thereforeround 3 s1 prefers a2 b1 c2 (profit 67). Note point configuration a2 b2 c2yields profit 16 buyer, within " maximal buyers profit (-10),hence b2 c2 marked M3 .next price change, configurations a1 b2 c1 a1 b2 c2 become optimals1 (profit 66), sub-bids a1 b2 , b2 c1 b2 c2 capture two. configurationsstay optimal another round (5), profit 62. round 5 profit configurationa1 b2 c1 140 157 = 17, within " maximizing buyers profit, thereforesub-configuration a1 b2 added M5 . point s1 full bid (in fact two fullbids: a1 b2 c2 a1 b2 c1 ) 5 , longer changes bids priceoptimal configurations decrease. Seller s2 however sticks a2 b1 c1495fiEngel & Wellman1a1 b 175275375475575s2716789I1a2 b 1751 , s271s2671 , s263s25971s26767, s259s25555s251a1 b 27575s17171s167, s167, s167, s167, s167, s1a2 b 2757575757575757575b 1 c1901 , s286s282s278s274s27070s26666, s2I2b 2 c190909090, s190, s190, s190, s190, s190, s1b 1 c290b 2 c2909090s18686, s186, s186, s186, s186, s186, s190s1868686s28282s278Table 4: Auction progression Phase A. Sell bids designation Mt (using )shown price sub-configuration.first four rounds, switching a1 b1 c1 round 5. takes four rounds s2 Mtconverge (M9 B29 = {a1 b1 c1 }).round 9, auction sets 1 = a1 b2 c1 (which yields buyer profit a1 b2 c2 )2 = a1 b1 c1 . second phase, starts point, sellers competeamount surplus transfer buyer, whose profit consequently becomes positive.next round (10) = 8, increased 8 subsequent round. Notep9 (a1 b1 c1 ) = 133, c2 (a1 b1 c1 ) = 90, therefore profit s2 point 43.round 15, = 48 meaning p15 (a1 b1 c1 ) = 85 causes s2 dropprofit becomes negative. ends auction, sets final allocation (s1 , a1 b2 c1 )pT (a1 b2 c1 ) = 157 48 = 109. leaves buyer profit 31 s1profit 14.6. Analysisanalyze economic properties auction Section 6.1, address practicalcomputational issues Section 6.2.6.1 Economic Propertiesadopt following assumptions discussion:A1 optimal (seller, configuration) pair provides non-negative surplus.A2 ub () real utility function buyer.496fiGAI Auctionsoptimal solution MAP (3) provides negative surplus sellers bidcost, auction terminates Phase A, trade occurs, auctiontrivially efficient. Therefore Assumption A1 cause loss generality. A2interpreted follows: given non-truthful buyer report, efficiency results applyface value buyers report rather true utility.6.1.1 Properties buyers profit function{1, . . . , g}, define (partial) profit set sub-configurationscorresponding"b ( ) =(fb,r (r ) p(r )).rfunctions f come GAI breakdown ub (6).Lemma 4. complement ,b () = b ( ) + b ( )Proof. (6) definition b ( ) get""b () =(fb,r (r ) p(r )) +(fb,r (r ) p(r )) = b ( ) + b ( ).rrround 5 example previous section, sub-configuration a1 b2 placedconfiguration a1 b2 c1 within " maximal buyer profit 10. Actually,point a1 b2 c1 added , also a1 b2 c2 whose buyer profit (23)within " maximum. a1 b2 c2 later selected si could leadadditional efficiency loss, beyond ". following lemma bounds potential loss.Lemma 5. Let set configurations, within " maximizing profit trader(buyer seller) given prices. Let = {r | , r {1, . . . , g}}. consistentcover within g" maximizing profit prices.particular, includes exactly optimal configurations, consistent coverexactly optimal well. proof (in Appendix B.1) relies definition GAIdecomposition tree decomposition, uses partial profit function definedalong Lemma 4.bound tight, GAI tree nontrivial domainconstruct example set exists consistent cover whose utilityexactly g" maximal.result get following corollary.Corollary 6.. bt () maxbt ( " ) g"!497fiEngel & WellmanProof. Apply Lemma 5 bt : define set configurations within " max! bt ( " ).Mt , definition, serves lemma. exactly set consistentcovers , hence must within g" optimum max! bt ( " ).show that, noted example, maximal profit buyerchange Phase A.Lemma 7. max bt () = max b1 () round Phase A.Proof. Assume exists " bt+1 ( " ) > bt ( " ). necessarily pt+1 ( " ) =pt ( " ) > 0. price change Rule [A], meaning w gsub-configurations " Mt , = w"g . case, definition ,bt ( " ) < max bt () ".Therefore,bt+1 ( " ) = ( " ) + = ( " ) +w"g"( " ) +< max bt ().ggtrue " whose profit improves, therefore max bt () changePhase A, hence equals value round 1.6.1.2 Straightforward bidding sellersturn attention sellers behavior. first define profit functionseller si () = pt () ci ().Definition 14 (Straightforward Bidder). seller called straightforward bidder (SB)round bids Bit follows: max () < 0, Bit = . Otherwiseselect bti arg max (), setBit = {r | bti , r {1, . . . , g}}.Intuitively, SB sellers follow myopic best response strategy, optimizing profitrespect current prices. approach termed straightforward Milgrom (2000)sense agents bid myopically, rather strategically anticipating subsequentprice responses.SB sellers choose optimal configuration bid on; none results provedaffected choice. also important note SB sellers find optimalfull configuration bti , rather optimize GAI element separately. configuration btitranslated set sub-configurations Bit . order calculate bti , seller si needs findoptimum current profit function. Section 6.2 show optimizationproblem tractable assumption ui (), too, compact GAI structure.following immediate corollary definition SB.Corollary 8. SB seller si ,t, Bit . () = max( " ).!498fiGAI Auctionsgeneral, sellers preference structure may coincide auctions price structure. Nevertheless, Corollary 8 holds definition SB, Bit (defined Section 4.2)contains single configuration submitted bid bti . Alternatively, definitionSB modified, sellers GAI preferences consistent auctions pricestructure bid multiple optimal configurations (if exist). sellers bid multipleconfigurations, speed convergence. case bti denotes set submittedconfigurations, Bit denotes respective collection sub-configurations, Bit setconsistent covers Bit . Lemma 5 (with " = 0) entails Corollary 8 still holds.However, simplicity analysis retain Definition 14.6.1.3 Efficiency given SBLemma 9 states price system price change rules, Phase leadsbuyer sellers mutually efficient configuration. Formally,interested maximizing function : %, represents surplus ub () ci ().prices pt ,() = bt () + ().Lemma 9. SB seller si , g"-efficient:(i ) max () g".Proof. Configuration chosen maximize buyers profit Bit endPhase A. Bit ,= , configuration available Bit , hence onemust chosen maximize buyers utility. , Bit , getCorollary 8,(i ) (),Corollary 6, get ,bT (i ) bT () g".Bit add two inequalities get (i ) () g",desired result.Based Phase Bs simple role single-dimensional bidding competitiondiscount, next assert overall result efficient SB, turn (Section 6.1.4) proves approximately ex-post equilibrium strategy two phases.Theorem 10. Given truthful buyer SB sellers, surplus final allocationwithin (g + 1)" maximal surplus.Proof Sketch: first establish auction must reach Phase B. that,show round Phase A, price least one sub-configuration reduced,whereas Lemma 7, max bt () change. latter enforces lower boundfar prices reduced within Phase A, hence Phase must terminate.initial prices buyers valuation, seller whose surplus (MMP solution)positive cannot drop phase, using Assumption A1 show way499fiEngel & WellmanPhase terminate reaching condition [SWITCH]. Next, showtwo sellers, surplus first drop auction cannot significantly higherone stayed longer. ensures winning seller efficientone, one whose MMP surplus almost maximal, Lemma 9 auction mustobtain (almost) surplus. full proof given Appendix B.2.bound guaranteed Theorem 10 worst-case bound, shown experimentally following sections auction typically achieves efficiency closer optimum.example Section 5, difference efficiencies two sellers lowerpotential efficiency loss (as (g + 1)" = 24). However, instance still guaranteed s1 wins, either efficient allocation, a1 b2 c2 providessurplus 39. reason two configurations s1 surpluswithin g" = 16 solution MMP(b, s1 ), hence Lemma 9 one must chosen 1 . configurations provide " surplus s2 efficientconfiguration, sufficient order win Phase B.bound Theorem 10 improved CDI map contains disconnectedcomponents. example, fully additive decomposition (as assumed previousliterature) exist, CDI map contains disconnected component attribute.take advantage disconnectedness create separate tree decompositiondisconnected components. definition adapted apportion "proportionally across disconnected trees. Formally, redefine Mt follows.Definition 15 (Buyers Preferred Set). Let G comprised trees G1 , . . . , Gh . Letj denote projection configuration tree Gj , gj number GAIelements Gj . Similarly, j denotes projection Gj . Define"", r Gj }.Mtj = {r | bt (j ) max()gjjbgj! jbuyers preferred set given Mt =#hj=1 Mj .Let ej = gj 1 denote number edges Gj . define connectivity parameter,e = maxj=1,...,h ej . turns e + 1 replace g approximation results.first step replace Corollary 6 tighter bound optimality configurations.Corollary 11.. bt () maxbt ( " ) (e + 1)"!Proof. apply Lemma 5 Gj , gj g" instead ", hence consistent coverMtj within gj g" gj maxj! j bt (j" ). Lemma 4, get consistent%cover Mt (meaning configuration ) within hr=1 gj g" gj max! bt ( " ).%e + 1 = maxj=1,...,h gj , bounded g" hr=1 gj (e + 1) = "(e + 1).obtain tighter efficiency result.Theorem 12. Given truthful buyer SB sellers, surplus final allocationwithin (e + 2)" maximal surplus.500fiGAI Auctionsfully additive case loss efficiency reduces 2". extreme,CDI map connected e + 1 = g, reducing Theorem 12 Theorem 10.assume preference structure buyer, meaning CDI map fullyconnected, e = 0 efficiency loss proportional ".6.1.4 Sellers incentives use SBFollowing Parkes Kalagnanam (2005), relate auction Vickrey-ClarkeGroves (VCG) mechanism establish incentive properties sellers. one-sidedmultiattribute VCG auction, buyer reports valuation ub , sellers report cost functionsci , buyer pays sell-side VCG payment winning seller.Definition 16 (Sell-Side VCG Payment). Let ( , ) optimal solution MAP.Let (, i) best solution MAP participate. sell-side VCGpaymentVCG(ub , ci ) = ub ( ) max(0, ub () ci ()).well known truthful bidding dominant strategy sellers one-sidedVCG auction. Parkes Kalagnanam (2005) showed maximal regret buyersbidding truthfully mechanism ub ( ) ci ( ) (ub () ci ()), is,marginal product efficient seller.typical iterative auctions, VCG outcome exactly achieved,deviation bounded minimal price change.Definition 17 (-VCG Payment). sell-side -VCG payment MAP payment pVCG(ub , ci ) p VCG(ub , ci ) + .payment guaranteed -VCG, sellers affect payment withinrange, hence gain falsely reporting cost bounded 2.Lemma 13. sellers SB, GAI auction payment sell-side (e + 2)"-VCG.example Section 5, profit winner (14) less " VCGprofit 20. proof (in Appendix B.4) also covers Case 3 allocation optionsSection 4.2, force payment equal ub (i ).ready final result section, showing approximatelyefficient outcome guaranteed Theorem 12 achieved (approximate) ex-post Nashequilibrium.Theorem 14. SB (3e + 5)" ex-post Nash equilibrium sellers GAI auction.is, sellers cannot gain (3e + 5)" deviating SB, givensellers follow SB.order exploit even bounded potential gain, sellers need know, givenconfiguration , whether explicitly selected approximately optimalbuyer, combination sub-configurations approximately optimal configurations. seems highly unlikely sellers information. likelylose bid myopically optimal configurations.501fiEngel & Wellman6.2 Computation Complexityadvantage GAI auctions additive auction AD (Parkes & Kalagnanam,2005) economic efficiency: accommodating expressive bidding, efficiency resultsrespect accurate utility function. contrast, key advantage respect auction employ preference structures, auction NLD (Parkes& Kalagnanam, 2005), computational efficiency. property show sectioncomputations exponential size largest GAI element, rather|A|. particular, size price space auction maintains equaltotal number sub-configurations. number exponential treewidth (plus one)original CDI map.5 ensure computational tractability, one define prioriconstant C, force treewidth CDI map bounded C ignoringinterdependencies. still much better using additive representationignores interdependencies. constant represents tradeoff economiccomputational efficiency; larger C supports accurate preference representation,GAI elements may larger.!#purpose computational analysis, let = gr=1 aj Ir D(aj ), collection sub-configurations. Since grows monotonically t, nave generationbest outcomes sequentially might end enumerating significant portionsdomain. Fortunately, enumeration avoided, complexity computation (as well optimization performed seller) grows |I|, is,computation depends size exponential domain.Theorem 15. computation Mt performed time O(g|I|2 ). Moreover,total time spent task throughout auction O(g|I|(|I| + )).obtain bound , number rounds Phase A, comparing sumprices sub-configurations rounds 1 .Theorem 16. number rounds required auction bounded"gp1 (r ) ."r%Proof. Let =r p (r ) (the sum prices sub-configurations round i).Assume < 0 1 . ub () 0, must existbi () > 0. chose initial prices , b1 () < 0,contradicts Lemma0, hence sum prices cannot reduced% 7. Therefore,11= r p (r ) throughout auction. Also, round least oneprice reduced g" . leads required result.bound rather looseits purpose ensure number roundsdepend size non-factored domain. depends number subconfigurations, result dividing initial price minimum price decrement. Usually Phase converges much faster. Let initial negative profit chosenauctioneer = max b1 (). worst case, Phase needs run5. use term treewidth subject using optimal tree decomposition.502fiGAI Auctions. b () = m. happens example r I. pt (r ) = fb,r (r ) +g .implies closer initial prices reflect buyers valuation, faster Phaseconverges. One extreme choice set p1 (r ) = fb,r (r ) +g . would make Phaseredundant, cost full initial revelation buyers valuation (Section 2.3). option extreme, , I. p1 () = p1 (), auctioneerrange choices determine right tradeoff convergence time information revelation. example Section 5, choice lower initial pricedomain I1 provides speedup revealing harmless amount information.simulations below, also set constant initial prices within GAI element.Furthermore, many domains natural dependencies mutual traders,case price structure used auction may also accommodate sellers preferencestructures. so, sellers bid multiple equally profitable configurations round,thus speeding convergence, discussed Section 6.1.also consider computational complexity SB strategy sellers.Theorem 17. Let b denote treewidth CDI map ub (), let denotetreewidth CDI map ui (). optimization ui () p() takes time exponentialb + worst case.Proof. Consider graph G includes union edges two CDI maps.treewidth G b + worst case. definition, price function p()decomposed according ub (), hence ui () p() decomposes according additive GAIfactors ui () ub (). Therefore, pair attributes x mutualfactor ui () p(), edge x, G. well known complexitycombinatorial optimization exponential treewidth graphfor example,using cost networks (Dechter, 1997).potential concern may communication cost associated descendingauction style. sellers need send bids round.simple change made avoid much redundant communication: auctionretain sub-bids previous rounds sub-configurations whose price change.combinations sub-bids different rounds yield suboptimal configurations,sub-bid tagged number latest round submitted,consistent combinations round considered full bids.implementation sellers need resubmit bid price least onesub-configurations changed.summarize, GAI auctions shown perform well criteria mentioned Section 2.3: achieve approximate efficiency given reasonable incentive properties,expressive enough accommodate preferences interdependencies among attributes,tractable maximal size GAI clusters reasonably bounded,require full revelation utility. Performance last criterion quantifiedexperimental part paper.7. Experimental Designmain idea behind GAI auctions improve efficiency auctions assumeadditivity, preferences additive. However, (given fixed ") theoretical503fiEngel & Wellmanefficiency guarantee GAI auctions depends e, connectivity parameter GAItree. suggests tradeoff: complex models accurately represent true utility,increase approximation error due higher connectivity. obvious question whetheraccurate preference modeling indeed efficient, particular, whetherGAI auctions efficient additive auctions, given preferencesadditive. address question experimentally, assume buyers preferencesGAI structure, compare performance GAI auctions modelstructure performance auctions restricted additive representation.latter, use instance GAI auction pricing structure additive,name additive approximating auction (AP). auction similar principleauction AD (Parkes & Kalagnanam, 2005).6 best knowledge, ADproposed instance surplus-maximizing multiattribute auction based additivepreferences, besides require full revelation buyers utility.experiments, sellers employ SB strategies.Section 7.1 describe random GAI utilities drawn, Section 7.2extend scheme generate GAI utility functions exhibit additional structure.Section 7.3 show obtain additive approximation random functions,allowing us simulate auction AD. results simulations presented Section 8.7.1 GAI Random Utilityperformed simulations using randomly generated utility functions representingbuyers value function sellers cost functions. random utility generation procedure follows utility elicitation procedure suggested Braziunas Boutilier (2005),uses two-step process: first create local utility functions GAI element,normalized range [0, 1]. Next, draw scaling constants represent relativeweight local function overall utility.formally, let ur (Ir ) = u([Ir ]) denote local utility function Ir , normalized[0, 1]. Next, let fr (Ir ) defined according GAI functional form Eq. (6),u([Ir ]) replaced ur (Ir ), hencef1 = u1 (I1 ),r = 2, . . . , g,fr = ur (Ir ) +r1"(1)jj=1"1i1 <<ij <rur ([j$Iis Ir ]).(9)s=1Braziunas Boutilier (2005) show GAI-structured utility, exist scalingconstants r [0, 1]g"r fr (Ir ).(10)u(A) =r=16. auctions employ additive price space drives bidders efficient configurations. ADefficient ! buyer sellers additive preferences. GAI auctions !-efficientgiven additive buyers preferences, make assumption regarding sellers preference.structural differences: (i) AD employs complicated price change rules, order allowsellers ignore attributes, (ii) discounts used stage AD, auctionselects provisional winner iteration.504fiGAI Auctionsrefer functions ur (Ir ) subutility functions. Note values formur ([Iir Iir! ]) drawn used ur (Ir ) ur (Iir! ). representationlets us draw random GAI functions, given GAI tree structure, using followingsteps:1. Draw random subutility functions ur (Ir ), r = 1, . . . , g range [0,1].2. Compute fr (), r = 1, . . . , g using (9).3. Draw random scaling constants r ,(10).%gr=1 r= 1, compute u(A)scaling constants represent importance decision maker accords correspondingGAI elements overall decision. procedure results utilities normalized[0, 1]. Finally, particular trader draw mean variance , scale u()range [ , + ], resulting utility functions ub () ui () = 1, . . . , m.7.2 Structured Subutilitysubutility function model may represent valuation subspace.practice may often find additional structure within GAI element. introducetwo structures consider typical generally applicable, usesimulations, along completely random local functions.argue Section 2.1, typical purchase sale decisions exhibit FOPI (firstorder preferential independence), meaning single attributes naturalordering quality. example, hard-drive buyers always prefer memory, higherRPM, longer warranty, on. implement FOPI, let integer valuesattribute represent quality. example, belongs GAI element Ir = {a, b},make sure ur (ai , b" ) ur (aj , b" ) ai > aj , ai , aj D(a), b" D(b).must course hold attribute FOPI, GAI element Irincludes a. enforce condition values GAI elementdrawn, special-purpose sorting procedure, applied steps 1 2 above.FOPI condition makes random utility function realistic, particularappropriate target application. attributes exhibit FOPI, dependenciesamong different attributes likely framed complements substitutes.concepts known primarily context combinatorial preferences, is, preferencescombinations distinct items. multiattribute framework, two attributescomplements improvement value worth sumimprovement separately. Two attributes substitutes wayaround. concepts meaningful respect attributes FOPI,otherwise notion improvement conditional value attributes.Definition 18 (Complements Substitutes). Let u() measurable value function " . Let a, b " , Z = " \ {a, b}, assume b FOPIrest attributes. Attributes b called complements ai > ai(ai , ai D(a)) bj > bj (bj , bj D(b)), Z " D(Z),u(ai , bj , Z " ) u(ai , bj , Z " ) > u(ai , bj , Z " ) u(ai , bj , Z " ) + u(ai , bj , Z " ) u(ai , bj , Z " ).505fiEngel & WellmanAttributes b substitutes inequality sign (always) reversed.relationship attributes ruled additive utility function,admitted weaker independence condition, called mutual utility independence (MUI)(Keeney & Raiffa, 1976), implies utility function either multiplicativeadditive. multiplicative, utility function represented n singledimensional functions, n scaling constants, single parameter k (the MUI-factor)controls strength complementarity (k > 0) substitutivity (k < 0)pairs attributes within GAI element (for k = 0 set attributes additive).7experimental purposes, assume attribute cluster (GAI element) exhibits MUI,value k all.elicitation procedure, one would normally extract MUI scaling constantsuser, compute k (Keeney & Raiffa, 1976). purposes, first determinek according relationship wish impose attributes, draw MUIscaling constants consistent value. explicitly, draw randomscaling constants, iteratively modify constants, set constantsfound consistent k. next step compute ur (Ir ) according MUIformula (Keeney & Raiffa, 1976). ur (Ir ) (for r) range [0, 1], hencepoint proceed steps 2 3 above. Note procedure severaldistinct sets scaling constants used: g constants used step 3 scale differentGAI elements, whereas MUI constants, per GAI element, scale attributes withinelement.7.3 Additive ApproximationAnother issue experiment design additive auction (AP) behaves facenon-additive buyer preferences, specifically would select approximately buyerpreferred sets configurations. approach took come additivefunction approximates buyers true utility function, use throughoutauction. aware better strategy, rule possibility oneexists.%natural approach generate linear approximation fi () arbitrary functionub () use linear regression. define indicator variable xij every aij D(ai ),consider value assignment data point. example, assignmenta1j(1) , . . . , amj(m) creates following data point:""cij xij = u(a1j(1) , . . . , amj(m) ),i=1 aij D(ai )value variable xij 1 j = j(i) 0 otherwise. coefficients cijresult regression represent values used fi (aij ).problem includes many attributes, possible consider points. assumption compact GAI representation exists, sensibleexpect could use fewer data points regression. indeed found small7. formalize notion Appendix D.506fiGAI Auctionsrandom sample joint utility yields approximation effective one basedpoints. precisely, largest domain tested (25 attributes, domainsize 4) found efficiency AP improve increasing numbersampled points beyond 200. show chart supporting claim Appendix E.experiments use 300 points instances.method comparison probably overestimates quality additive approximation. general, would true utility function explicitly generateapproximation. Extraction elicitation utility function usuallyserious bottleneck multiattribute mechanism. Therefore, major reason useadditive approximation reduce burden elicitation. Hence practice wouldtry obtain additive function directly, rather obtain full utility approximate it. result process somewhat unpredictable, elicitationqueries may coherent: willingness pay a1 depends value b,willingness pay a1 know b? therefore considerexperimental generation method biased favor additive approximation.8. Simulation Resultsprovide detailed results simulation study. Section 8.1 provides analyses economic efficiency results. Section 8.2 covers computational study, results regardingrevelation private information provided Section 8.3.8.1 Efficiency GAI Structuremeasure efficiency terms percentage MAP solution, surplusachieved optimal seller-configuration pair. evaluate effect preference modeling efficiency, vary structural parameters buyers GAI preferences: connectivity factor e, size largest GAI element. Performance dependsmany additional factors, size attribute domains, number sellers, amountprice decrement ("), distribution utility functions drawn. isolateprimary structural parameters, first tested efficiency varies accordingchoices side factors, several fixed GAI structures fully random subutilityfunctions. result tests, picked following parameter values restsimulations: valuations drawn uniform distribution, buyer meanset 500. mean seller drawn uniformly [500, 700]. variance set200 traders. use domain size 3 4 attributes, numbersellers = 5. explanation process leading choices providedfull report (Engel, 2008).following experiment used roughly fixed GAI structure, g = 6 elementse = 5, (that is, GAI structure tree, forest), " = 24 (meaning reduction= 4 per sub-configuration). vary number attributes varying sizeelement. Figure 2a shows efficiency obtained respect , size largestGAI element. expected, size GAI elements negligible, effectefficiency GAI auctions. dramatic effect efficiency AP. = 1,decomposition fact additive hence AP performs optimally. performancedeteriorates increases.507fiEngel & Wellman(a)(b)Figure 2: Efficiency function of: (a) size largest GAI element (), given e = 5,(b) number GAI elements (e + 1), given = 5.performed test using utility attributes FOPI.FOPI restriction, additive approximation much efficient relative unconstrained random utility. FOPI applies strict subset attributes, wouldexpect efficiency AP fall somewhere efficiency FOPIunrestricted case. Somewhat surprisingly, imposing FOPI renders GAI auctions slightlyless efficient. Nevertheless, additive approximation achieves lower efficiency comparedaccurate preference modeling, differences pass statistical significancetest (P < 0.01), 3. Further, note performance GAI auctions alwaysimproved using smaller value " = g" , whereas hardly improves performanceAP. = 2, statistically significant difference (with confidence level)already detected 2. used = 2 hereafter.next experiment (Figure 2b) measures efficiency function e, given fixed. assume connected GAI trees, e number GAI elements minus one.tested structures e varying 1 10, elements size 3 5, = 5structures.8 single element, GAI auction similar NLD (Parkes &Kalagnanam, 2005), auction assigns price every point jointdomain. e = 0, hence efficiency GAI close perfect. structureextreme compared additive representation, indeed performance APparticularly inferior (only 70% efficient).GAI elements, efficiency GAI auctions declines slow pace.theoretical potential error (e + 2)", mostly result efficiency loss winningseller, based Lemma 5. efficiency loss may occur sub-configurationbelongs configuration yields lowest profit allowed buyer-preferredseta particularly rare case. practice, loss closer e, much smallererror.performance AP improves number elements grows maximalaverage sizes fixed. Intuitively, changing structure way takes closer8. find particular tree structure influential results; final structure usedreported results maximum three children per node.508fiGAI Auctions(a)(b)Figure 3: (a) Efficiency function k 0 (complements). (b) Efficiency functionk 0 (substitutes).additive representation. FOPI, see similar phenomenon before. However,difference GAI FOPI AP FOPI, even ten elements, substantialstatistically significant.Figures 3a 3b present efficiency function MUI-factor k, complementssubstitutes, respectively. used fixed GAI structure four elements, largestfour attributes, imposed k elements. expected,stronger complementarity among attributes, lower efficiency AP, whereasrelationship affect efficiency GAI auctions. case substitutes,contrast, additive approximation performs well, efficiency starts deteriorateextreme values k. roughly, say relationship among attributes(within GAI element) limited (mild) substitutions, could good ideause additive approximation. Unfortunately, interpretation parameter k lacksquantitative scaling: clear intuition actual numbers mean, beyondqualitative classification mentioned above.summarize part, experimental results show GAI auctions yield significant efficiency improvement comparison additive auction, almost classesevaluations. Though efficiency additive auction may come across relativelyhigh perhaps sufficient, observation misleading several respects. (i)large procurement events, 510% efficiency differences translate large amounts money.(ii) wider efficiency loss additive auction (with theoretical bound) mayimpact incentives; SB may longer approximate ex-post Nash equilibrium.(iii) Efficiency expected deteriorate larger problems larger GAI elements,particular FOPI hold many attributes. (iv) argued Section 7.3, expect practical additive auctions perform worse AP tailoredapproximation.8.2 Computational Analysiscomputational tasks required auction simulations performed using algorithms described Appendix C. algorithms suggested applied509fiEngel & Wellman(a)(b)Figure 4: Number rounds function of: (a) size largest GAI element (), givene = 5, (b) number GAI elements (e + 1), given = 5 = 2.combinatorial optimization problems (Dechter, 1997; Nilsson, 1998), thereforecomputational runtime process round particular interest work. Instead,focus number rounds auction requires. tested number roundsrequired auctions GAI AP, fully random FOPI preferences, varying threeparameters: (size largest GAI element), e (connectivity), .complexity terms number rounds shown Figure 4a (with respect )Figure 4b (with respect number elements). observe FOPIGAI auction takes much longer converge, compared case random preferences.reason FOPI, sellers buyers preferences general seenopposites: price, specific attribute, buyer prefers higher quality,whereas sellers prefer lower quality (given fixed values rest attributes),everyone agrees relative quality attribute values. apparent differencegrowth rate (the FOPI case seems steeper curve) somewhat misleading:= 8 (not shown) GAI random preferences already caught curvesee FOPI case. number rounds, expected, grows exponentiallysize largest element. However, observed Figure 4b, numbergrow quickly function number elements, supporting theoretical argumentsSection 6.2. Note also variance chosen traders preferences fixed, thussmall number elements variance wider, resulting large numberrounds required GAI FOPI case.AP, implication increasing respective increase numberattributes. result, complexity AP (not shown) grows slowlyincrease . FOPI case, = 2, AP takes average 481 rounds = 1(6 attributes) 546 rounds = 6 (19 attributes). numbers slightly higherrandom preferences (523 628).high-dimensional multiattribute auctions, expect participation would typically automated software bidding agents (Wellman, Greenwald, & Stone, 2007).circumstances, auction taking thousands rounds causeconcern. However, reason rounds expensive, might reconsider adopt-510fiGAI AuctionsFigure 5: Efficiency function number rounds.ing additive auctions, sacrifice efficiency order decrease number rounds.Alternatively, could keep using GAI auctions increase " (and ). finalexperiment compares two alternatives. vary level ", order view efficiency function number rounds (Figure 5). GAI structure usedexperiment e = 5 = 5.evident chart, cases GAI achieves better efficiency even fixednumber rounds. exception budget rounds small (under200), FOPI holds. case need pay rounds order gethigher efficiency.total computation time, carried GAI auction 10 elements, = 5,= 3, = 2, rest parameters fixed above, around 11 secondsaverage, using Intel Dual Core (2.00 Ghz) CPU, 2048 MB RAM.8.3 Information Revelationkey difference mechanism proposed previous literatureextent buyer required reveal preference information. GAI auctions,buyer need reveal private preference information front.course, price changes reveal buyers information. Another experimentalquestion therefore whether mechanism significantly reduces overall amountinformation revealed buyer.PK study information revelation buyer seller, additivityassumption. utility function additive amount information revealedmeasured terms constraints linear weights. Sellers infer boundsbuyers set weights, amount information hidden representedfraction simplex satisfies constraints. simplex analysispossible GAI utilities. suggest alternative geared towards kind informationrevealed GAI auctions.GAI auctions, buyers private information partially revealed selection buyers preferred set Mt . auction need announce directly;general sellers infer sub-configuration Mt received bid(usually sellers observe bids), yet price change511fiEngel & Wellmannext round. therefore measure exactly thatfor many sub-configurations rleast one round r Mt Bit i. specifically, definesub-configuration revealed, within GAI element measure fractionsub-configurations revealed end auction. measurement overestimates information actually revealed, sellers infer bounds relativepreferences precise values functions fb (). Moreover, assumesseller observes bids (meaning sellers share bid information other),unrealistic event practice.Based criterion, GAI auctions reveal average 15%25% buyers preferences preferences exhibit FOPI, 10%15% subutilities completelyrandom. seem systematically depend parameter tested.validates claim advantage GAI auctions promise second-score typesauctions.9. Conclusionspropose novel exploitation preference structure multiattribute auctions. Ratherassuming full additivity, structure all, model preferences using generalized additive independence (GAI) decomposition. show GAI representationconstructed relatively simple statements willingness-to-pay, developiterative auction mechanism directly relying decomposition. auction mechanismgeneralizes preference modeling employed Parkes Kalagnanam (2005),essence retaining information revelation properties. allows range tradeoffsaccuracy preference representation computational complexityauction, well tradeoff buyer information revelation numberrounds required convergence.performed simulation study proposed multiattribute auctions, comparedmechanism assumes additive preferences. study validated usefulnessGAI auctions preferences non-additive GAI, allowed us quantifyadvantages specific classes preferences. general, found designyields significantly higher economic efficiency comparison additive auctions.GAI subutilities exhibit internal structures, FOPI, efficiency loss additiveapproximation less severe, cases benefit accurate GAI model stillsignificant. Using additive approximation may reasonable approach GAIstructure fairly similar additive one, auction must terminate withinsmall number rounds.tradeoff expressive compactness preference representation ubiquitous applications involving preferences. one hand, would like ask userslittle possible information; other, users preference statements may accurate even meaningful cannot express important dependencies. problemscould useful experimentally compare accuracy GAI additive representations. experimental methodologies used study, particular generationrandom structured utility functions, finding additive approximation GAI functions, may therefore prove applicable broader class preference research problemstradeoff exists.512fiGAI AuctionsAcknowledgmentswork supported part NSF grants IIS-0205435 IIS-0414710,STIET program NSF IGERT grant 0114368. Yagil Engel supported partAly Kaufman fellowship Technion. thank anonymous reviewers manyuseful comments suggestions.Appendix A. Proofs Section 3.1Lemma 2.Let u(A) MVF representing preference differences, let X, Y, Zdefine partition A. CDI(X, | Z) iffu(A) = u(X 0 , Y, Z) + u(X, 0 , Z) u(X 0 , 0 , Z),arbitrary instantiations X 0 , 0 .Proof. Let X 0 , 0 arbitrary instantiations.u(X, Y, Z) = u(X, Y, Z)u(X 0 , Y, Z)+u(X 0 , Y, Z) = u(X, 0 , Z)u(X 0 , 0 , Z)+u(X 0 , Y, Z)second equality holds iff X 0 , 0 , CDI(X, | Z).Theorem 3 (CDI-GAI Theorem).Let G = (A, E) CDI map A,{I1 , . . . , Ig } set overlapping maximal cliques.u(A) =g"fr (Ir ),(A.1)r=1f1 = u([I1 ]),r = 2, . . . , g,fr = u([Ir ]) +(A.2)r1""(1)jj=1u([1i1 <<ij <rj$Iis Ir ]).s=1Proof. actually prove somewhat stronger result.Claim. Let G CDI map utility function u(). Let Q = {C1 , . . . , Cw } denote setmaximal cliques G. Then,u(A) =w"(1)k+1k=1"1i1 <<ik wu([k$Cis ]).(A.3)s=1Let G0 = (A, E 0 ) complete graph nodes G. definition CDImap, edge (x, y) E 0 \ E implies CDI(x, y). use induction series edgeremovals. starting graph G0 , step remove edge E 0 \ E get0graph Gi . last step = |E 0 | |E| G|E ||E| = G. prove claim513fiEngel & Wellmanholds graph Gi . Since clique G0 , step 0, Q0 = {A}claim trivially hold. Following process step 1 provides intuition finaldecomposition obtained. pick pair nodes (x, y) CDI(x, y). usenotation = \ {a} A. Lemma 2 ,u(A) = u(x, y, Ax,y )(A.4)= u(x0 , y, Ax,y ) + u(x, 0 , Ax,y ) u(x0 , 0 , Ax,y )= u([Ax ]) + u([Ay ]) u([Ax Ay ]).set maximal cliques G1 Q1 = {Ax , Ay }. Equation (A.4) shows (A.3)holds Q1 .proving induction step, assume (A.3) holds step i, show carrystep + 1. Let (x, y) denote edge removed step + 1. Let C1 , . . . , Cd (WLOG)indicate sets Qi include x y. Similar (A.4), observeu([C1 ]) = u([C1x ]) + u([C1y ]) u([C1x C1y ]).(A.5)Similarly k = 1, . . . , wi 1, 1 < i1 < < ik wi ,u([k$s=1Cis C1 ]) = u([k$s=1Cis C1x ]) + u([k$Cis C1y ]) u([s=1k$Cis C1x C1y ]). (A.6)s=1(A.3) (assumed hold step) term includes C1 substitutedaccording (A.5) (A.6). result (A.3) holding set (Qi \ {C1 }){C1x , C1y }.repeat operation C2 , . . . , Cd , define resulting collectionQi+1 = (Qi \ {C1 , . . . , Cd }) {C1x , C1y , . . . , Cdx , Cdy }.elements Qi+1 subsets elements Qi , maximal cliques Gi .verify induction property:element Qi+1 clique Gi+1 , difference GiGi+1 removed edge (x, y), set Qi+1 includes x y.clique C Qi+1 maximal, subset maximal cliqueC Gi , either: (i) C C = C \ {x} (ii) x C C = C \ {y},(iii)C = C. x longer connected C remains maximal cases.maximal clique Gi+1 , C C Qi . either = C,= C \ {x}, = C \ {y}, three cases element Qi+1 .proves induction step.result, last step decomposition (A.3) holds set Q = Q|E0 ||E| ,set maximal cliques G, hence claim proved.define f1 , . . . , fg according (A.2). claim, get (A.1) holds.514fiGAI AuctionsAppendix B. Proofs Section 6.1B.1 Proving Lemma 5Lemma 5. Let set configurations, within " maximizing profittrader given price vector. Let = {r | , r {1, . . . , g}}.consistent cover within g" maximizing profit prices.show given suboptimal consistent cover , find suboptimalmember , contradicting premise lemma. traversing GAItree depth-first manner, step flip sub-configurations correspondingelements subtree set sub-configurations source configurationparent subtree (thus trimming subtree). This, show, resultsanother consistent cover also sub-optimal. Eventually obtain configurationstill suboptimal.purpose introduce following notions:operator turns set sub-configurations, consistent cover,configuration:{1 , . . . , g } = (1 , . . . , g ).Let consistent cover . -source element r configurationoriginated (meaning, r = r ).operation trim replaces sub-configurations given configurationcorresponding set sub-configurations different configuration , accordingfollowing rules. Let denote indices GAI elements, correspondingsubtree GAI-tree, whose root GAI element Ii . Let denote consistentcover . operation -trim defined elementscorresponding -source. Formally, exists ,r , r = . Now, Let parent Ii , arbitrary elementoutside disconnected rest graph. Let source.-trim(i , ) = {r | r/ } {r |r }replace r r corresponding sub-configuration ,resulting configuration elements corresponding-source parent Ii .Lemma B.1. " = -trim(, ) consistent cover.Proof. need show pair sub-configurations set {r | r/ } {r |r} consistent, assign value attribute appearcorresponding GAI elements.sub-configurations {r |r } internally consistent mutual-source . sub-configurations {r | r/ } internally consistentsub-configurations . Let r1 r2/ denote indices GAI elements,Ir1 Ir2 ,= . Now, Ir1 subtree whose root Ii , whereas Ir2 outsidesubtree, path must go Ii parent . Due515fiEngel & Wellmanrunning intersection property GAI tree, Ir1 Ir2 Ii . corresponding subconfigurations must consistent also -source , hence r1r2 must also consistent.Lemma B.2. Let defined Lemma 5, let denote consistent cover. " = -trim(i , ) (for i), ( " ) () + ".Proof. Let denote single -source {r | r }. Let = ={1, . . . , g} \ . ( " ) > () + ", (using Lemma 4)( " ) = (" ) + (" ) > ( ) + ( ) + ",= " ,(" ) > ( ) + ".Define following cover:= {r" | r } {r | r }consistent coveragain (as Lemma B.1) possible intersectionelement " element (the root = ) parent. corresponding sub-configurations i" must consistent followingargument: consistent appear together . i" consistent-source definition -trim. Hence i" assignvalues attributes Ii . consistent , musti" . get() = (" ) + ( ) > ( ) + ( ) + " = () + ".last equation follows fact sub-configurations .contradicts "-optimality .Proof Lemma 5. Let 1 consistent cover contradicting lemma, meaning( 1 ) max () g". first reorder GAI elements 1, . . . , g, accordingorder corresponding backtracking Depth-First-Search: is, startingleftmost leaf, next move siblings, next parent, general childrennode Ii visited, next element visited Ii . perform series g 1-trim operations, resulting series 1 , . . . , g . that, must showstep operation -trim(i , ) valid, sub-configurations correspondingmutual -source. Ii leaf, |i | = 1 hence elementssingle source. Otherwise, result trimming subtrees children Ii , hencedefinition -trim -source ii .Now, consider resulting g . assumed ( 1 ) < max () g", henceapplications Lemma B.2 g 1 -trim operations, get ( g ) <max () ". last element g elements mutual -source,meaning g . Therefore, got contradiction "-optimality .516fiGAI AuctionsB.2 Proving Theorem 10order prove Theorem 10 need several additional claims.Lemma B.3. price least one sub-configuration must reduced every roundphase A.Proof. round < phase exists active seller Bit = .However active round t, Bit ,= . Let Bit . r.r Mt ,definition . Therefore must r , Mt .Lemma B.4. auction must reach phase B.Proof. Lemma B.3 prices must go every round phase A. Lemma 7ensures lower bound much prices reduced phase A, thereforeauction either terminates phase must reach condition [SWITCH].set initial prices high max b1 () < 0, max bt () < 0phase Lemma 7. Assumption A2 efficient allocation ( , ) providespositive welfare, ( ) = bt ( ) + ( ) > 0. si SB therefore leaveauction ( ) < 0. happen bt ( ) > 0, therefore sidrop phase A. Phase continues long least one seller active,auction cannot terminate reaching condition [SWITCH].Finally, following lemma states two sellers, potential surplusfirst one drop auction cannot significantly higher potential surplusone stayed longer.Lemma B.5. sellers si sj SB, si active least long sj activephase B,(i ) max j () (g + 1)".Proof. SB definition phase B, sj drops > jT (j ). sidrop point (i ) " > jT (j ) ". , getCorollary 6 that,bT (i ) + (i ) max bT () + jT (j ) (g + 1)".Corollary 8, jT (j ) = max jT (). Therefore(i ) = bT (i ) + (i ) max bT () + max jT () (g + 1)" max j () (g + 1)".Theorem 10. Given truthful buyer SB sellers, surplus final allocationwithin (g + 1)" maximal surplus.Proof. Lemma B.4 auction terminates allocation (si , ). Lemma9, theorem immediate case winning seller si efficient seller. Otherwiseefficient seller sj dropped si . result immediateLemma B.5.517fiEngel & WellmanB.3 Proving Theorem 12first adapt Lemma 7, Lemma 9, Lemma B.5 use e + 1 instead g.Lemma B.6. max bt () change round phase A.Proof. Let G comprised trees G1 , . . . , Gh , let j" denote projection configuration" tree Gj , let gj denote number GAI elements Gj .Assume exists j" bt+1 (j" ) > bt (j" ). necessarily pt+1 (j" ) =p (j" ) . happen must case w gj sub-configurationsj" Mtj , = w"g . case, definition Mj ,"bt (j" ) < max bt (j ) gj .j jgTherefore,bt+1 (j" ) = (j" ) + = (j" ) +w"gj "(j" ) +< max bt (j ).j jggtrue j" whose profit improves, therefore maxj j bt (j ) changephase A.max bt () = maxh"bt (j ) =j=1h"j=1max bt (j ).j jlast equality holds optimal values disconnected components GAItree independent other. result, max bt () well changephase A.Lemma B.7. SB seller si , (e + 1)"-efficient:(i ) max () (e + 1)".proof identical proof Lemma 9, replacing g e+1 using Corollary 11instead Corollary 6.Lemma B.8. sellers si sj SB, si active least long sj activephase B,(i ) max j () (e + 2)".proof identical proof Lemma B.5, using Corollary 11 insteadCorollary 6.Theorem 12. Given truthful buyer SB sellers, surplus final allocationwithin (e + 2)" maximal surplus.Proof. proof identical proof Theorem 10, replacing Lemmas 9 B.5lemmas B.7 B.8, respectively.518fiGAI AuctionsB.4 Lemma 13 Theorem 14Lemma 13. sellers SB, GAI auction payment sell-side (e + 2)"-VCG.Proof. Trivially, consider winning seller si . case final pricebuyers valuation payment ub (i ) exactly VCG payment. thereforeassume final price buyers valuation, paymentwinning seller pT (i ) . Let sj second best seller. sj drops si ,discount ", hence,+ " > jT (j ) = max jT ().(B.1)Corollary 6,ub (i ) pT (i ) max bT () (e + 1)".Therefore (using (B.1) second inequality)pT (i ) ub (i ) max b () + (e + 1)" <ub (i ) max bT () + (e + 2)" max jT () ub (i ) max j () + (e + 2)". (B.2)Bow sj survived auction discount ",jT (j ) + ".Meaning:pT (j ) cj (j ) ".(B.3)Corollary 6ub (i ) pT (i ) ub (j ) pT (j ) + (e + 1)".Therefore (using (B.3) second inequality)pT (i ) ub (i ) ub (j ) + pT (j ) (e + 1)"ub (i ) (ub (j ) cj (j )) " (e + 1)" ub (i ) max j () (e + 2)". (B.4)Equations (B.2) (B.4) place payment pT (i ) within (e + 2)" si VCGpayment.Theorem 14. SB (3e + 5)" ex-post Nash equilibrium sellers GAI auction.is, sellers cannot gain (3e+5)" deviating SB, given sellersfollow SB.Let s1 play arbitrary strategy 1 SB sellers s2 , . . . , sn . s1 winwould clearly done worse using SB, therefore assume s1 wins 1 final pricep gains least (3e + 5)" trade. Let 2, . . . n. calculation(B.2) assumed nothing winning traders strategy, therefore applies well:p = pT (1 ) ub (1 ) max () + (e + 2)".(B.5)Next, define following cost function: c1 (1 ) = p (2e + 3)" c1 ( " ) = , " ,= 1 .Assume s1 plays SB c1 .519fiEngel & WellmanClaim. playing SB assuming cost c1 , s1 still winner, profit (wrt c1 ())within (2e + 3)" profit playing 1 .Proof. Clearly, s1 bids 1 . Let p() denote prices end phase newfinal discount.instance auction, let b () denote buyers profit, let= 1 (1 ) = p(1 ) c1 (1 ) =assume moment prices reach s1 limit,p(1 ) (p (2e + 3)").(for inequality, use pT (1 ) = ub (1 ) bT (1 ) also (B.5)),= pT (1 ) p + (2e + 3)" > ub (1 ) (1 ) (ub (1 ) max () + (e + 2)") + (2e + 3)"b= max ()bT (1 )+ (e + 1)".(B.6)Let denote configuration chosen seller si end phasenew instance. Since instance, get bT (i ) bT (1 ) (e + 1)".Therefore modify (B.6) state,> (i ) (i ) = pT (i ) ci (i ),b(B.7)meaning prices reached limit s1 , sellers dropped off. showss1 wins new instance well. Furthermore, lowest possible price paid s1= p(1 ) (p (2e + 3)"), hence price least p (2e + 3)".determinedProof Theorem 14. Lemma 13:p V CG(c1 , c2 , . . . , cn ) + (e + 2)".Truthful reporting dominant strategy sellers one-sided VCG auctions. ThereforeV CG(c1 , c2 , . . . , cn ) V CG(c1 , c2 , . . . , cn ).result claim getp p + (2e + 3)" V CG(c1 , c2 , . . . , cn ) + (3e + 5)".Therefore playing 1 , s1 could gained (3e + 5)" worstcase payoff playing SB respect true cost c1 .Appendix C. Proofs Section 6.2Theorem 15. computation Mt performed time O(g|I|2 ). Moreover,total time spent task throughout auction O(g|I|(|I| + )).Proof. simplicity notations assume single (connected) GAI-tree.extension multiple connected components immediate Mjt computedseparately.functions ub pt GAI form, hence function bt = ub ptGAI form. noted (Boutilier et al., 2001), functions520fiGAI AuctionsGAI form optimized using variable elimination schemes cost networks (Dechter,1997). fact, GAI structure already tree, case optimization linearsize domain |I|. However, Mt includes sub-configurationsconfigurations within " max b (). find it, must find maximum bt , addsub-configurations Mt , find best configuration already (thatis, maximal \ ) on. done following procedure, adaptedwork Nilsson (1998):1. i=1,. . . , g:Define = { | 1 , . . . , i1 Mt/ Mt }.Find = arg maxi bt ().2. best configuration \ = arg maxi=1,...,g bt ( ) (which means, configuration least one sub-configuration Mt ).bt ( ) max bt () ", sub-configuration already Mtadded Mt . Otherwise, Mt ready.procedure performs g optimizations, takes linear size domain. amounts O(g|I|). time procedure done, either least onesub-configuration added Mt , Mt ready. Therefore number timesprocedure done per round bounded number sub-configurations |I| plus one,giving O(g|I|2 ) bound. Moreover, Mt monotonically increasing auction.round, start Mt computed previous round. Throughout auction,application procedure either yields new sub-configuration Mt , terminatesround, total number times procedure performed throughout auctionbounded |I| + , leading overall bound O(g|I|(|I| + )).Appendix D. Relating MUI condition ComplementsSubstitutesdefinitions utility independence (UI) condition MUI found elsewhere(Keeney & Raiffa, 1976).Definition 19. MUI-factor set MUI attributes solution1+k =n&(1 + kki ).i=1Keeney Raiffa (1976) (KR) show one MUI-factor additionzero (Appendix 6B text). ensures soundness following adaptationMUI representation theorem:9Theorem D.1. Let set MUI attributes.1. MUI-factor zero, u(A) =%ni=1 ki ui (ai ).9. theorem adapted book Keeney Raiffa (1976), Theorem 6.1, page 289.521fiEngel & Wellman2. Otherwise, let k ,= 0 MUI-factor.!n[kki ui (ai ) + 1] 1u(A) = i=1.k(D.1)KR go point k > 0 define u" (A) = 1 + ku(A), strategicallyequivalent function u(), turn (D.1) multiplicative representation.done similar fashion k < 0. Further, show MUI known exist,one elicitation query sufficient order determine whether form functionadditive multiplicative.following relationship allow us interpret MUI factor respect complements substitutes. result generalizes formalizes intuition given KRcase MUI two attributes.Theorem D.2. Let set MUI attributes, MUI-factor k ,= 0.k > 0 iff pairs attributes complements, k < 0 iff pairsattributes substitutes.Proof. proof based work Keeney Raiffa (1976), Theorem 6.1, explained below.Assume u() normalized u(A0 ) = 0. attribute A, let= {a}, know U I(a, a). Utility independence form leads followingfunctional form: exist functions f g that,u(A) = f (a) + g(a)u(a, a0 )instantiate form assignment a0 getu(a0 , a) = f (a) + g(a)u(a0 , a0 ) = f (a)Hence f (a) = u(a0 , a), g(a) =u(A0 ) = 0, getu(A)u(a0 ,a)u(a,a0 )g(a) =(this development done KR).u(A) u(a0 , a).u(a, a0 ) u(a0 , a0 )(D.2)proof Theorem 6.1, KR define MUI-factor follows:k=g(a) 1u(a0 , a)denominator always positive. Furthermore, shown (D.2), g(a) > 1,u(A) u(a0 , a) > u(a, a0 ) u(a0 , a0 ). particular means b a, bcomplements, inequality holds holding fixed attributes b.Similarly, g(a) < 1, b substitutes. Putting pieces together,get desired result.522fiGAI AuctionsFigure 6: Efficiency AP function number sampling points used deviseadditive approximation.Appendix E. Optimal Regression Using Small Sampleshow experiment supporting claim Section 7.3: larger set samplingpoints one used linear regression utility function cannot improveefficiency AP. Figure 6 shows efficiency AP function numbersampling points used, largest domain used experiments: 25 attributes= 4 (e = 9 = 5). Similar results shown distributions FOPIpreferences. chart result 150 experiments 10 points x-axis,largest number tests used.ReferencesBacchus, F., & Grove, A. (1995). Graphical models preference utility. 11thConference Uncertainty Artificial Intelligence, pp. 310, Montreal.Beil, D. R., & Wein, L. M. (2003). inverse-optimization-based auction multiattributeRFQs. Management Science, 49, 15291545.Bichler, M. (2001). Future e-Markets: Multi-Dimensional Market Mechanisms. Cambridge University Press.Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: directed graphicalrepresentation conditional utilities. 17th Conference Uncertainty ArtificialIntelligence, pp. 5664, Seattle.Branco, F. (1997). design multidimensional auctions. RAND Journal Economics,28 (1), 6381.Braziunas, D., & Boutilier, C. (2005). Local utility elicitation GAI models. 21stConference Uncertainty Artificial Intelligence, pp. 4249, Edinburgh.Che, Y.-K. (1993). Design competition multidimensional auctions. RAND JournalEconomics, 24 (4), 668680.David, E., Azoulay-Schwartz, R., & Kraus, S. (2002). English auction protocol multiattribute items. Agent Mediated Electronic Commerce IV: Designing MechanismsSystems, Vol. 2531 Lecture Notes Artificial Intelligence, pp. 5268. Springer.523fiEngel & WellmanDebreu, G. (1959). Topological methods cardinal utility theory. Arrow, K. J., Karlin,S., & Suppes, P. (Eds.), Mathematical Methods Social Sciences, pp. 1626.Stanford University Press.Dechter, R. (1997). Mini-buckets: general scheme generating approximations automated reasoning. 15th International Joint Conference Artificial Intelligence,pp. 12971303, Nagoya.Dyer, J. S., & Sarin, R. K. (1979). Measurable multiattribute value functions. OperationsResearch, 27, 810822.Engel, Y. (2008). Structured Preference Representation Multiattribute Auctions. Ph.D.thesis, University Michigan, Ann Arbor, MI.Engel, Y., & Wellman, M. P. (2007). Generalized value decomposition structuredmultiattribute auctions. 8th ACM Conference Electronic Commerce, pp. 227236, San Diego.Engel, Y., Wellman, M. P., & Lochner, K. M. (2006). Bid expressiveness clearingalgorithms multiattribute double auctions. 7th ACM Conference ElectronicCommerce, pp. 110119, Ann Arbor.Fishburn, P. C. (1967). Interdependence additivity multivariate, unidimensionalexpected utility theory. International Economic Review, 8, 335342.Gonzales, C., & Perny, P. (2004). GAI networks utility elicitation. 9th InternationalConference Principles Knowledge Representation Reasoning, pp. 224234,Whistler, BC.Hyafil, N., & Boutilier, C. (2006). Regret-based incremental partial revelation mechanisms.21st National Conference Artificial Intelligence, pp. 672678, Boston, MA.Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: PreferencesValue Tradeoffs. Wiley.Koppius, O. (2002). Information Architecture Electronic Market Performance. Ph.D.thesis, Erasmus University, Rotterdam, Netherlands.Krantz, D. H., Luce, R. D., Suppes, P., & Tversky, A. (1971). Foundations Measurement,Vol. 1. Academic Press, New York.Milgrom, P. (2000). Putting auction theory work: simultaneous ascending auction.Journal Political Economy, 108, 245272.Nilsson, D. (1998). efficient algorithm finding probable configurationsprobabilistic expert systems. Statistics Computing, 8 (2), 159173.Parkes, D. C., & Kalagnanam, J. (2005). Models iterative multiattribute procurementauctions. Management Science, 51, 435451.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann.Robu, V., Somefun, D. J. A., & La Poutre, J. A. (2005). Modeling complex multi-issue negotiations using utility graphs. 4th International Joint Conference AutonomousAgents Multi-Agent Systems, pp. 280287, Utrecht.524fiGAI AuctionsSandholm, T. (2007). Expressive commerce application sourcing: conducted $35 billion generalized combinatorial auctions. AI Magazine, 28 (3), 4558.Sandholm, T., & Suri, S. (2006). Side constraints non-price attributes markets.Games Economic Behavior, 55, 321330.von Neumann, J., & Morgenstern, O. (1944). Theory Games Economic Behavior.Princeton University Press.Von Winterfeldt, D., & Edwards, W. (1986). Decision Analysis Behavioral Research.Cambridge University Press.Vulkan, N., & Jennings, N. R. (2000). Efficient mechanisms supply servicesmulti-agent environments. Decision Support Systems, 28, 519.Wellman, M. P., Greenwald, A., & Stone, P. (2007). Autonomous Bidding Agents: StrategiesLessons Trading Agent Competition. MIT Press.Wellman, M. P., Osepayshvili, A., MacKie-Mason, J. K., & Reeves, D. M. (2008). Bidding strategies simultaneous ascending auctions. B. E. Journal TheoreticalEconomics (Topics), 8 (1).525fiJournal Artificial Intelligence Research 37 (2010) 279-328Submitted 05/09; published 03/10Join-Graph Propagation AlgorithmsRobert MateescuROMATEES @ MICROSOFT. COMMicrosoft Research7 J J Thomson AvenueCambridge CB3 0FB, UKKalev KaskKKASK @ ICS . UCI . EDUDonald Bren School Information Computer ScienceUniversity California IrvineIrvine, CA 92697, USAVibhav GogateVGOGATE @ CS . WASHINGTON . EDUComputer Science & EngineeringUniversity Washington, SeattleSeattle, WA 98195, USARina DechterDECHTER @ ICS . UCI . EDUDonald Bren School Information Computer ScienceUniversity California IrvineIrvine, CA 92697, USAAbstractpaper investigates parameterized approximate message-passing schemes basedbounded inference inspired Pearls belief propagation algorithm (BP). startbounded inference mini-clustering algorithm move iterative scheme called IterativeJoin-Graph Propagation (IJGP), combines iteration bounded inference. AlgorithmIJGP belongs class Generalized Belief Propagation algorithms, framework allowedconnections approximate algorithms statistical physics shown empirically surpass performance mini-clustering belief propagation, well number stateof-the-art algorithms several classes networks. also provide insight accuracyiterative BP IJGP relating algorithms well known classes constraint propagationschemes.1. IntroductionProbabilistic inference principal task Bayesian networks known NP-hardproblem (Cooper, 1990; Roth, 1996). commonly used exact algorithms jointree clustering (Lauritzen & Spiegelhalter, 1988; Jensen, Lauritzen, & Olesen, 1990) variableelimination (Dechter, 1996, 1999; Zhang, Qi, & Poole, 1994), recently search schemes(Darwiche, 2001; Bacchus, Dalmao, & Pitassi, 2003; Dechter & Mateescu, 2007) exploit network structure. significant advances made last decade exact algorithms, manyreal-life problems big hard, especially structure dense, sincetime space exponential treewidth graph. Approximate algorithms thereforenecessary many practical problems, although approximation within given error bounds alsoNP-hard (Dagum & Luby, 1993; Roth, 1996).c2010AI Access Foundation. rights reserved.fiM ATEESCU , K ASK , G OGATE & ECHTERpaper focuses two classes approximation algorithms task belief updating.inspired Pearls belief propagation algorithm (Pearl, 1988), known exacttrees. distributed algorithm, Pearls belief propagation also applied iterativelynetworks contain cycles, yielding Iterative Belief Propagation (IBP), also known loopy beliefpropagation. networks contain cycles, IBP longer guaranteed exact,many cases provides good approximations upon convergence. notable success casesIBP coding networks (McEliece, MacKay, & Cheng, 1998; McEliece & Yildirim,2002), version IBP called survey propagation classes satisfiability problems(Mezard, Parisi, & Zecchina, 2002; Braunstein, Mezard, & Zecchina, 2005).Although performance belief propagation far well understood general,one promising avenues towards characterizing behavior came analogiesstatistical physics. shown Yedidia, Freeman, Weiss (2000, 2001) belief propagation converge stationary point approximate free energy system, calledBethe free energy. Moreover, Bethe approximation computed pairs variables terms,therefore simplest version general Kikuchi (1951) cluster variational method,computed clusters variables. observation inspired class GeneralizedBelief Propagation (GBP) algorithms, work passing messages clusters variables.mentioned Yedidia et al. (2000), many GBP algorithms correspondKikuchi approximation. version based region graphs, called canonical authors,presented Yedidia et al. (2000, 2001, 2005). algorithm Iterative Join-Graph Propagationmember GBP class, although described language region graphs.approach similar independently developed McEliece Yildirim(2002). information BP state art research see recent survey Koller (2010).first present mini-clustering scheme anytime bounded inference schemegeneralizes mini-bucket idea. viewed belief propagation algorithm treeobtained relaxation networks structure (using technique variable duplication).subsequently present Iterative Join-Graph Propagation (IJGP) sends messagesclusters allowed form cyclic structure.two schemes investigate: (1) quality bounded inference anytimescheme (using mini-clustering); (2) virtues iterating messages belief propagation typealgorithms, result combining bounded inference iterative message-passing (in IJGP).background section 2, overview Tree-Decomposition scheme forms basisrest paper. relaxing two requirements tree-decomposition, connectedness (via mini-clustering) tree structure (by allowing cycles underlying graph),combine bounded inference iterative message-passing basic tree-decompositionscheme, elaborated subsequent sections.Section 3 present partitioning-based anytime algorithm called Mini-Clustering (MC),generalization Mini-Buckets algorithm (Dechter & Rish, 2003). messagepassing algorithm guided user adjustable parameter called i-bound, offering flexible tradeoffaccuracy efficiency anytime style (in general higher i-bound, betteraccuracy). MC algorithm operates tree-decomposition, similar Pearls belief propagation algorithm (Pearl, 1988) converges two passes, tree. contributionbeyond works area (Dechter & Rish, 1997; Dechter, Kask, & Larrosa, 2001) in: (1)Extending partition-based approximation belief updating mini-buckets general treedecompositions, thus allowing computation updated beliefs variables once.280fiJ OIN -G RAPH P ROPAGATION LGORITHMSextension similar one proposed Dechter et al. (2001), replaces optimizationprobabilistic inference. (2) Providing empirical evaluation demonstrates effectivenessidea tree-decomposition combined partition-based approximation belief updating.Section 4 introduces Iterative Join-Graph Propagation (IJGP) algorithm. operatesgeneral join-graph decomposition may contain cycles. also provides user adjustable i-boundparameter defines maximum cluster size graph (and hence bounds complexity),therefore anytime iterative. algorithm IBP typically presented generalization Pearls Belief Propagation algorithm, show IBP viewed IJGPsmallest i-bound.also provide insight IJGPs behavior Section 4. Zero-beliefs variable-value pairszero conditional probability given evidence. show that: (1) value variableassessed zero-belief iteration IJGP, remains zero-belief subsequentiterations; (2) IJGP converges finite number iterations relative set zero-beliefs; and,importantly (3) set zero-beliefs decided iterative belief propagationmethods sound. Namely zero-belief determined IJGP corresponds true zero conditional probability relative given probability distribution expressed Bayesian network.Empirical results various classes problems included Section 5, shedding lightperformance IJGP(i). see often superior, otherwise comparable,state-of-the-art algorithms.paper based part earlier conference papers Dechter, Kask, Mateescu (2002),Mateescu, Dechter, Kask (2002) Dechter Mateescu (2003).2. Backgroundsection provide background exact approximate probabilistic inference algorithmsform basis work. present algorithms context directed probabilistic networks, applicable graphical model, including Markov networks.2.1 PreliminariesNotations: reasoning problem defined terms set variables taking values finitedomains set functions defined variables. denote variables subsetsvariables uppercase letters (e.g., X, Y, Z, S, R . . .) values variables lower case letters(e.g., x, y, z, s). assignment (X1 = x1 , . . . , Xn = xn ) abbreviated x = (x1 , . . . , xn ).subset variables S, DS denotes Cartesian product domains variables S. xSprojection x = (x1 , . . . , xn ) subset S. denote functions letters f , g, h, etc.,scope (set arguments) function f scope(f ).EFINITION 1 (graphical model) (Kask, Dechter, Larrosa, & Dechter, 2005) graphical model3-tuple, = hX, D, Fi, where: X = {X1 , . . . , Xn } finite set variables; ={D1 , . . . , Dn } set respective finite domains values; F = {f1 , . . . , fr } setpositive real-valued discrete functions, defined subset variables Si X, calledscope, denoted scope(fP ). graphical model typically associated combination1operator , (e.g., {, } - product, sum). graphical model represents combination1. combination operator also defined axiomatically (Shenoy, 1992).281fiM ATEESCU , K ASK , G OGATE & ECHTERfunctions: ri=1 fi . graphical model associated primal graph capturesstructural information model:EFINITION 2 (primal graph, dual graph) primal graph graphical model undirected graph variables vertices edge connects two vertices whose corresponding variables appear scope function. dual graph graphical modelone-to-one mapping vertices functions graphical model. Two verticesdual graph connected corresponding functions graphical model share variable.denote primal graph G = (X, E), X set variables E setedges.EFINITION 3 (belief networks) belief (or Bayesian) network graphical model B =hX, D, G, P i, G = (X, E) directed acyclic graph variables X P = {pi },pi = {p(Xi | pa (Xi ) ) } conditional probability tables (CPTs) associated variable Xi pa(Xi ) = scope(pi ){Xi } set parents Xi G. Given subset variablesS, write P (s) probability P (S = s), DS . belief network representsprobability distribution X, P (x1 , . . . ., xn ) = ni=1 P (xi |xpa(Xi ) ). evidence set einstantiated subset variables. primal graph belief network called moral graph.obtained connecting parents vertex G removing directionalityedges. Equivalently, connects two variables appearing family (a variableparents CPT).Two common queries Bayesian networks Belief Updating (BU) Probable Explanation (MPE).EFINITION 4 (belief network queries) Belief Updating (BU) task find posteriorprobability single variable given evidence e, compute P (Xi |e).Probable Explanation (MPE) task find complete assignment variables maximum probability given evidence, compute argmaxX pi .2.2 Tree-Decomposition SchemesTree-decomposition heart general schemes solving wide range automatedreasoning problems, constraint satisfaction probabilistic inference. basismany well-known algorithms, join-tree clustering bucket elimination. presentation follow terminology Gottlob, Leone, Scarcello (2000) Kask et al. (2005).EFINITION 5 (tree-decomposition, cluster-tree) Let B = hX, D, G, P belief network.tree-decomposition B triple hT, , i, = (V, E) tree, labelingfunctions associate vertex v V two sets, (v) X (v) P satisfying:1. function pi P , exactly one vertex v V pi (v),scope(pi ) (v).2. variable Xi X, set {v V |Xi (v)} induces connected subtree .also called running intersection (or connectedness) property.often refer node functions cluster use term tree-decompositioncluster-tree interchangeably.282fiJ OIN -G RAPH P ROPAGATION LGORITHMSEFINITION 6 (treewidth, separator, eliminator) Let = hT, , tree-decompositionbelief network B. treewidth (Arnborg, 1985) maxvV |(v)| 1. treewidthB minimum treewidth tree-decompositions. Given two adjacent vertices uv tree-decomposition, separator u v defined sep(u, v) = (u) (v),eliminator u respect v elim(u, v) = (u) (v). separator-widthmax(u,v) |sep(u, v)|. minimum treewidth graph G shown identical relatedparameter called induced-width (Dechter & Pearl, 1987).Join-tree cluster-tree elimination (CTE) Bayesian network constraint satisfaction communities, used tree-decomposition method join-tree decomposition (Lauritzen& Spiegelhalter, 1988; Dechter & Pearl, 1989), introduced based relational database concepts(Maier, 1983). decompositions generated embedding networks moral graph Gchordal graph, often using triangulation algorithm using maximal cliques nodesjoin-tree. triangulation algorithm assembles join-tree connecting maximal cliqueschordal graph tree. Subsequently, every CPT pi placed one clique containing scope.Using previous terminology, join-tree decomposition belief network B = hX, D, G, P0tree = (V, E), V set cliques chordal graph G contains G, E setedges form tree cliques, satisfying running intersection property (Maier, 1983).join-tree satisfies properties tree-decomposition therefore cluster-tree (Kasket al., 2005). paper, use terms tree-decomposition join-tree decompositioninterchangeably.variants processing join-trees belief updating (e.g., Jensen et al., 1990;Shafer & Shenoy, 1990). adopt version Kask et al. (2005), called cluster-treeelimination (CTE), applicable tree-decompositions general geared towards spacesavings. message-passing algorithm; task belief updating, messages computedsummation eliminator two clusters product functions originating cluster. algorithm, denoted CTE-BU (see Figure 1), pays special attentionprocessing observed variables since presence evidence central component beliefupdating. cluster sends message neighbor, algorithm operates functionscluster except message particular neighbor. message contains single combined function individual functions share variables relevant eliminator.non-individual functions combined product summed eliminator.Example 1 Figure 2a describes belief network Figure 2b join-tree decomposition it.Figure 2c shows trace running CTE-BU evidence G = ge , h(u,v) messagecluster u sends cluster v.HEOREM 1 (complexity CTE-BU) (Dechter et al., 2001; Kask et al., 2005) Given Bayesiannetwork B = hX, D, G, P tree-decomposition hT, , B, time complexity CTEBU O(deg (n + N ) dw +1 ) space complexity O(N dsep ), deg maximumdegree node tree-decomposition, n number variables, N number nodestree-decomposition, maximum domain size variable, w treewidth sepmaximum separator size.283fiM ATEESCU , K ASK , G OGATE & ECHTERAlgorithm CTE Belief-Updating (CTE-BU)Input: tree-decomposition hT, , i, = (V, E) B = hX, D, G, P i. Evidence variablesvar(e).Output: augmented tree whose nodes clusters containing original CPTsmessages received neighbors. P (Xi , e), Xi X.Denote H(u,v) message vertex u v, nev (u) neighbors u excluding v,cluster(u) = (u) {H(v,u) |(v, u) E},clusterv (u) = cluster(u) excluding message v u.Compute messages:every node u , u received messages nev (u), compute message nodev:1. Process observed variables:Assign relevant evidence pi (u)2. Compute combined function:Xh(u,v) =felim(u,v) fset functions clusterv (u) whose scope intersects elim(u, v).Add h(u,v) H(u,v) add individual functions clusterv (u)Send H(u,v) node v.Compute P (Xi , e):every Xi X let u vertex Xi (u). Compute P (Xi , e) =PQ(u){Xi } ( f cluster(u) f )Figure 1: Algorithm Cluster-Tree-Elimination Belief Updating (CTE-BU).1(1) = { A, B, C}(1) = { p(a ), p(b | ), p(c | a, b)}1ABCBCB2C( 2) = { B , C , , F }(2) = { p(d | b), p( f | c, }2 BCDFEBF3(3) = {B, E , F }(3) = { p (e | b, f )}4(4) = {E , F , G}(4) = { p( g | e, f )}3FG(a)BEFEF4(b)EFGh (1 , 2 ) ( b , c ) =(b , c ) =h ( 2 ,1 )p ( ) p (b | ) p (c | , b )p ( | b ) p ( f | c , ) h( 3, 2 ) (b , f )d,fh( 2 ,3 ) (b , f ) =h( 3 , 2 ) ( b , f ) =ep (d | b)h( 3 , 4 ) ( e , f ) =p ( e | b , f ) h( 2 ,3 ) (b , f )c ,dp ( f | c, )h (1 , 2 ) ( b , c )p ( e | b , f ) h( 4 ,3 ) ( e , f )bh( 4 ,3 ) ( e , f ) = p ( G = g e | e , f )(c)Figure 2: (a) belief network; (b) join-tree decomposition; (c) Execution CTE-BU.3. Partition-Based Mini-Clusteringtime, especially space complexity, CTE-BU renders algorithm infeasible problems large treewidth. introduce Mini-Clustering, partition-based anytime algorithmcomputes bounds approximate values P (Xi , e) every variable Xi .284fiJ OIN -G RAPH P ROPAGATION LGORITHMSProcedure MC Belief Updating (MC-BU(i))2. Compute combined mini-functions:Make (i)-size mini-cluster partitioning clusterv (u), {mc(1), . . . , mc(p)};PQh1(u,v) = elim(u,v) f mc(1) fQhi(u,v) = maxelim(u,v) f mc(i) f = 2, . . . , padd {hi(u,v) |i = 1, . . . , p} H(u,v) . Send H(u,v) v.Compute upper bounds P (Xi , e) P (Xi , e):every Xi X let u V cluster Xi (u). Make (i) mini-clusterscluster(u), {mc(1), . . . , mc(p)}; Compute P (Xi , e) =PQQpQ( (u)Xi f mc(1) f ) ( k=2 max(u)Xi f mc(k) f ).Figure 3: Procedure Mini-Clustering Belief Updating (MC-BU).3.1 Mini-Clustering AlgorithmCombining functions cluster product complexity exponential numbervariables, upper bounded induced width. Similar mini-bucket scheme(Dechter, 1999), rather performing expensive exact computation, partition cluster p mini-clusters mc(1), . . . , mc(p),Pi variables,Q accuracy parameter. Instead computing CTE-BU h(u,v) =elim(u,v)f (u) f , divide functions(u)mc(k), k {1, . . . , p}, rewrite h(u,v) =P p mini-clustersQp QPQf=mc(k) f . migrating summation operatorelim(u,v)elim(u,v)f (u)QP k=1 fQpmini-cluster, yielding k=1 elim(u,v) f mc(k) f , get upper bound h(u,v) .resulting algorithm called MC-BU(i).Consequently, combined functions approximated via mini-clusters, follows. Supposeu V received messages neighbors v (the message v ignored evenreceived). functions clusterv (u) combined partitioned mini-clusters{mc(1), . . . , mc(p)}, one containing variables. mini-cluster processedsummation eliminator, resulting combined functions well individualfunctions sent v. shown Dechter Rish (2003) upper boundimproved using maximization operator max rather summation operator summini-buckets. Similarly, lower bounds generated replacing sum min (minimization)mini-buckets. Alternatively, replace sum mean operator (taking sumdividing number elements sum), case deriving approximation jointbelief instead strict upper bound.Algorithm MC-BU upper bounds obtained CTE-BU replacing step 2main loop final part computing upper bounds joint belief procedure givenFigure 3. implementation used experiments reported here, partitioningdone greedy brute-force manner. ordered functions according sizes (numbervariables), breaking ties arbitrarily. largest function placed mini-cluster itself. Then,picked largest remaining function probed mini-clusters order creation,285fiM ATEESCU , K ASK , G OGATE & ECHTER1ABCBCH (1, 2)h(11, 2 ) (b, c) := p (a ) p (b | ) p(c | a, b)1( 2 ,1)hH ( 2,1)(b) := p (d | b) h(13, 2 ) (b, f )d, fh(22,1) (c) := max p ( f | c, )d, f2BCDF1( 2 , 3)hH ( 2 , 3)BF3BEFEFc ,dh(22,3) ( f ) := max p( f | c, )c,d1( 3, 2 )(b, f ) := p(e | b, f ) h(14,3) (e, f )H ( 3, 2 )hH ( 3, 4 )h(13, 4 ) (e, f ) := p (e | b, f ) h(12,3) (b) h(22,3) ( f )H ( 4 , 3)4(b) := p (d | b) h(11, 2 ) (b, c)eb1( 4 , 3)h(e, f ) := p(G = g e | e, f )EFGFigure 4: Execution MC-BU = 3.trying find one together new function would variables. newmini-cluster created whenever existing ones could accommodate new function.Example 2 Figure 4 shows trace running MC-BU(3) problem Figure 2. First, evidence G = ge assigned CPTs. individual functions sent cluster 1cluster 2. Cluster 1 contains 3 variables,(1) = {A, B, C}, therefore partitioned.Pp(a)p(b|a) p(c|a, b) computed messagecombined function h1(1,2) (b, c) =1H(1,2) = {h(1,2) (b, c)} sent node 2. Now, node 2 send message node 3. Again,individual functions. Cluster 2 contains 4 variables, (2) = {B, C, D, F }, partitioning necessary: MC-BU(3) chooseP mc(1) = {p(d|b), h(1,2) (b,2c)} mc(2) = {p(f |c, d)}.1combined functions h(2,3) (b) = c,d p(d|b) h(1,2) (b, c) h(2,3) (f ) = maxc,d p(f |c, d)computed message H(2,3) = {h1(2,3) (b), h2(2,3) (f )} sent node 3. algorithm continues every node received messages neighbors. upper bound p(a, G = ge )computed choosing cluster1, contains variable A. doesnt need partitionPing, algorithm computes b,c p(a) p(b|a) p(c|a, b) h1(2,1) (b) h2(2,1) (c). Noticeunlike CTE-BU processes 4 variables cluster 2, MC-BU(3) never processes 3variables time.already shown that:HEOREM 2 (Dechter & Rish, 2003) Given Bayesian network B = hX, D, G, P evidence e, algorithm MC-BU(i) computes upper bound joint probability P (Xi , e)variable Xi (and values) evidence e.HEOREM 3 (complexity MC-BU(i)) (Dechter et al., 2001) Given Bayesian network B =hX, D, G, P tree-decomposition hT, , B, time space complexity MC-BU(i)O(n hw di ), n number variables, maximum domain size variablehw = maxuT |{f P |scope(f ) (u) 6= }|, bounds number mini-clusters.286fiJ OIN -G RAPH P ROPAGATION LGORITHMSff ffffff fffffi fffiff" ff"#$%& 'ff " &! ! !ff"" ff""()* $ && '%&&!! !! !!ff ff!fffi fffi!+Figure 5: Node duplication semantics MC: (a) trace MC-BU(3); (b) trace CTE-BU.Semantics Mini-Clustering mini-bucket scheme shown semantics relaxation via node duplication (Kask & Dechter, 2001; Choi, Chavira, & Darwiche, 2007).extend mini-clustering showing apply messages flow one direction(inward, leaves root), follows. Given tree-decomposition D, CTE-BU computesfunction h(u,v) (the message cluster u sends cluster v), MC-BU(i) partitions cluster u pmini-clusters u1 , . . . , , processed independently resulting functions h(ui ,v)sent v. Instead consider different decomposition D0 , like D, exception (a) instead u, clusters u1 , . . . , , children v, variableappearing single mini-cluster becomes new variable, (b) child w u (in D)child uk (in D0 ), h(w,u) (in D) assigned uk (in D0 ) partitioning. NoteD0 legal tree-decomposition relative original variables since violates connectedness property: mini-clusters u1 , . . . , contain variables elim(u, v) pathnodes u1 , . . . , (this path goes v) not. However, legal tree-decompositionrelative new variables. straightforward see H(u,v) computed MC-BU(i){h(ui ,v) |i = 1, . . . , p} computed CTE-BU D0 direction leavesroot.want capture semantics outward messages root leaves, need generate different relaxed decomposition (D00 ) MC, defined, allows different partitioningstreams cluster. could course stick decompositionD0 use CTE directions would lead another variant mini-clustering.Example 3 Figure 5(a) shows trace bottom-up phase MC-BU(3) network Figure4. Figure 5(b) shows trace bottom-up phase CTE-BU algorithm problem obtainedproblem Figure 4 splitting nodes (into D0 D00 ) F (into F 0 F 00 ).MC-BU algorithm computes upper bound P (Xi , e) joint probability P (Xi , e).However, deriving bound conditional probability P (Xi |e) easy exact287fiM ATEESCU , K ASK , G OGATE & ECHTERRandom Bayesian N=50 K=2 P=2 C=480.200.180.16Avg abs error0.140.120.100.08#ev=0#ev=10#ev=20#ev=300.060.040.020.000123456789101112131415Number iterationsFigure 6: Convergence IBP (50 variables, evidence 0-30 variables).value P (e) available. try divide (multiply) P (Xi , e) constant,resultP necessarily upper bound P (Xi |e). easy show normalization,P (xi , e)/ xi Di P (xi , e), mean operator identical normalization MC-BU outputapplying summation operator mini-clusters.MC-BU(i) improvement Mini-Bucket algorithm MB(i), allows computation P (Xi , e) variables single run, whereas MB(i) computes P (Xi , e)one variable, single run. computing P (Xi , e) variable, MB(i) runn times, variable, algorithm call nMB(i). demonstrated Mateescuet al. (2002) MC-BU(i) linear speed-up nMB(i). given i, accuracyMC-BU(i) shown worse nMB(i).3.2 Experimental Evaluation Mini-Clusteringwork Mateescu et al. (2002) Kask (2001) provides empirical evaluation MC-BUreveals impact accuracy parameter quality approximation comparesIterative Belief Propagation Gibbs sampling scheme. include subsetexperiments provide essence results. Additional empirical evaluationMC-BU given comparing IJGP later paper.tested performance MC-BU(i) random Noisy-OR networks, random coding networks, general random networks, grid networks, three benchmark CPCS files 54, 360422 variables respectively (these belief networks medicine, derived Computer basedPatient Case Simulation system, known hard belief updating). type networkran Iterative Belief Propagation (IBP) - set run 30 iterations, Gibbs Sampling (GS)MC-BU(i), 2 treewidth w capture anytime behavior MC-BU(i).random networks generated using parameters (N,K,C,P), N numbervariables, K domain size (we used K=2), C number conditional probabilitytables P number parents conditional probability table. parents tablepicked randomly given topological ordering, conditional probability tables filled288fiJ OIN -G RAPH P ROPAGATION LGORITHMS0|e| 1020NHDmaxIBPMC-BU(2)MC-BU(5)MC-BU(8)000000000mean000000000000N=50, P=2, 50 instancesAbs. Errormax1.6E-031.1E-035.7E-041.1E-037.7E-042.8E-043.6E-041.7E-043.5E-05mean9.0E-093.4E-049.6E-041.1E-038.4E-044.8E-049.4E-046.9E-042.7E-043.2E-041.5E-043.5E-05Rel. Errormax1.9E+001.4E+007.1E-011.4E+009.3E-013.5E-014.4E-012.0E-014.3E-02mean1.1E-054.2E-011.2E+001.3E+001.0E+005.9E-011.2E+008.4E-013.3E-013.9E-011.9E-014.3E-02Timemax0.0560.0480.0390.0700.0630.0580.2140.1840.123mean0.1020.0810.0620.0570.0490.0390.0720.0660.0570.2210.1900.127Table 1: Performance Noisy-OR networks, w = 10: Normalized Hamming Distance, absoluteerror, relative error time.randomly. grid networks structure square, edges directed form diagonalflow (all parallel edges direction). generated specifying N (a squareinteger) K (we used K=2). also varied number evidence nodes, denoted |e|tables. parameter values reported table. problems, Gibbs samplingperformed consistently poorly include part results here.experiments focused approximation power MC-BU(i). compared twoversions algorithm. first version, every cluster, used max operatormini-clusters, except one processed summation. second version,used operator mean mini-clusters. investigated second version algorithmtwo reasons: (1) compare MC-BU(i) IBP Gibbs sampling, alsoapproximation algorithms, would possible compare bounding scheme; (2)observed experiments that, although bounds improve i-bound increases, qualitybounds computed MC-BU(i) still poor, upper bounds greater 1 manycases.2 Notice need maintain sum operator least one mini-clusters.mean operator simply performs summation divides number elements sum.example, A, B, C binary variables (taking values 0 1), f (A, B, C) aggregatedfunction one mini-cluster,elim = {A, B}, computing message h(C) meanPoperator gives: 1/4 A,B{0,1} f (A, B, C).computed exact solution used three different measures accuracy: 1) NormalizedHamming Distance (NHD) - picked likely value variable approximateexact, took ratio number disagreements total number variables, averaged number problems ran class; 2) Absolute Error (Abs.Error) - absolute value difference approximate exact, averagedvalues (for variable), variables problems; 3) Relative Error (Rel. Error) -absolute value difference approximate exact, divided exact,averaged values (for variable), variables problems. coding networks,2. Wexler Meek (2008) compared upper/lower bounding properties mini-bucket computing probabilityevidence. Rollon Dechter (2010) investigated heuristic schemes mini-bucket partitioning.289fiM ATEESCU , K ASK , G OGATE & ECHTER10|e| 2030N=50, P=3, 25 instancesAbs. ErrorNHDmaxmean000000000000000000IBP000000000000000MC-BU(2)MC-BU(5)MC-BU(8)MC-BU(11)MC-BU(14)max1.3E-035.3E-042.3E-041.0E-034.6E-042.0E-046.6E-041.8E-043.4E-052.6E-043.8E-056.4E-074.2E-0500mean1.3E-043.6E-046.8E-049.6E-044.0E-041.9E-048.3E-044.1E-041.9E-045.7E-041.8E-043.4E-052.4E-043.8E-056.4E-074.1E-0500Rel. ErrormaxTimemean7.9E-012.2E+004.2E+005.8E+002.4E+001.2E+005.1E+002.4E+001.2E+003.5E+001.0E+002.1E-011.5E+002.3E-014.0E-032.4E-01008.2E+003.1E+001.4E+006.4E+002.7E+001.2E+004.0E+001.1E+002.1E-011.6E+002.3E-014.0E-032.5E-0100maxmean0.2420.1840.1210.1080.0770.0640.1330.1050.0950.5090.4060.3082.3781.4390.6247.8752.0930.6380.1070.0770.0640.1330.1040.0980.4980.3940.3002.3391.4210.6137.8052.0750.630Table 2: Performance Noisy-OR networks, w = 16: Normalized Hamming Distance, absoluteerror, relative error time.Noisy-OR networks, N=50, P=3, evid=20, w*=16, 25 instancesNoisy-OR networks, N=50, P=3, evid=10, w*=16, 25 instances1e+01e+0MCIBPGibbs SamplingMCIBPGibbs Sampling1e-1Absolute errorAbsolute error1e-11e-21e-31e-21e-31e-41e-41e-51e-502468101214160246810121416i-boundi-boundFigure 7: Absolute error Noisy-OR networks.report one measure, Bit Error Rate (BER). terms measures defined above, BERnormalized Hamming distance approximate (computed algorithm)actual input (which case coding networks may different solution givenexact algorithms), denote differently make semantic distinction. also reporttime taken algorithm. reported metrics (time, error, etc.) provided Tables,give mean max values.Figure 6 show IBP converges 5 iterations. So, experimentsreport time 30 iterations, time even better sophisticated termination used.results typical runs.290fiJ OIN -G RAPH P ROPAGATION LGORITHMSRandom networks, N=50, P=2, k=2, evid=0, w*=10, 50 instancesRandom networks, N=50, P=2, k=2, evid=10, w*=10, 50 instances0.160.16MCGibbs SamplingIBP0.140.14MCGibbs SamplingIBP0.120.10Absolute errorAbsolute error0.120.080.060.040.100.080.060.040.020.020.000.000246810024i-bound6810i-boundFigure 8: Absolute error random networks.BER= .22maxmeanIBPGSMC-BU(2)MC-BU(4)MC-BU(6)MC-BU(8)0.0000.4830.0020.0010.0000.0000.0000.4830.0020.0010.0000.000IBPGSMC-BU(2)MC-BU(4)MC-BU(6)MC-BU(8)MC-BU(10)0.0000.5060.0060.0060.0050.0020.0010.0000.5060.0060.0060.0050.0020.001= .26= .32= .40maxmeanmaxmeanmaxmeanN=100, P=3, 50 instances, w*=70.000 0.000 0.002 0.002 0.022 0.0220.483 0.483 0.483 0.483 0.483 0.4830.004 0.004 0.024 0.024 0.068 0.0680.002 0.002 0.018 0.018 0.046 0.0450.000 0.000 0.004 0.004 0.038 0.0380.000 0.000 0.002 0.002 0.023 0.023N=100, P=4, 50 instances, w*=110.000 0.000 0.002 0.002 0.013 0.0130.506 0.506 0.506 0.506 0.506 0.5060.015 0.015 0.043 0.043 0.093 0.0940.017 0.017 0.049 0.049 0.104 0.1020.011 0.011 0.035 0.034 0.071 0.0740.004 0.004 0.022 0.022 0.059 0.0590.001 0.001 0.008 0.008 0.033 0.032= .51maxmeanTime0.0880.4830.1320.1100.1060.0910.0880.4830.1310.1100.1060.0910.0031.360.080.080.120.190.0750.5060.1570.1580.1510.1210.1010.0750.5060.1570.1580.1500.1220.1020.0039.850.190.190.290.711.87Table 3: Bit Error Rate (BER) coding networks.Random Noisy-OR networks results summarized Tables 1 2, Figure 7. NHD,IBP MC-BU gave perfect results. measures, noticed IBPaccurate evidence order magnitude. However, evidence added,IBPs accuracy decreases, MC-BUs increases give similar results. seeMC-BU gets better accuracy parameter increases, shows anytime behavior.General random networks results summarized Figure 8. similarrandom Noisy-OR networks. Again, IBP best result number evidencevariables small. remarkable quickly MC-BU surpasses performance IBPevidence added (for more, see results Mateescu et al., 2002).Random coding networks results given Table 3 Figure 9. instances fall withinclass linear block codes, ( channel noise level). known IBP accurateclass. Indeed, problems experimented IBP outperformedMC-BU throughout. anytime behavior MC-BU seen variation numberscolumn vividly Figure 9.291fiM ATEESCU , K ASK , G OGATE & ECHTERCoding networks, N=100, P=4, sigma=.51, w*=12, 50 instancesCoding networks, N=100, P=4, sigma=.22, w*=12, 50 instances0.180.007MCIBP0.006MCIBP0.160.005Bit Error RateBit Error Rate0.140.0040.0030.0020.120.100.0010.080.0000.06024681001224681012i-boundi-boundFigure 9: Bit Error Rate (BER) coding networks.Grid 15x15, evid=10, w*=22, 10 instancesGrid 15x15, evid=10, w*=22, 10 instances0.0612MCIBP0.05MCIBP10Time (seconds)Absolute error80.040.030.026420.0100.000246810121416180246i-bound81012141618i-boundFigure 10: Grid 15x15: absolute error time.Grid networks results given Figure 10. notice IBP accurate evidenceMC-BU better evidence added. behavior consistently manifestedsmaller grid networks experimented (from 7x7 14x14).CPCS networks results also tested three CPCS benchmark files. results givenFigure 11. interesting notice MC-BU scheme scales fairly large networks,like real life example CPCS422 (induced width 23). IBP accurateevidence, surpassed MC-BU evidence added. However, whereas MC-BUcompetitive IBP time-wise i-bound small, runtime grows rapidly i-boundincreases. details benchmarks see results Mateescu et al. (2002).Summary results show that, expected, IBP superior approximationscoding networks. However, random Noisy-OR, general random, grid networks CPCSnetworks, presence evidence, mini-clustering scheme often superior even weakest form. empirical results particularly encouraging use un-optimized schemeexploits universal principle applicable many reasoning tasks.292fiJ OIN -G RAPH P ROPAGATION LGORITHMSCPCS 422, evid=0, w*=23, 1 instanceCPCS 422, evid=10, w*=23, 1 instance0.050.05MCIBPMCIBP0.04Absolute errorAbsolute error0.040.030.020.010.030.020.010.000.0024681012141618246i-bound81012141618i-boundFigure 11: Absolute error CPCS422.4. Join-Graph Decomposition Propagationsection introduce algorithm Iterative Join-Graph Propagation (IJGP) which, like miniclustering, designed benefit bounded inference, also exploit iterative message-passingused IBP. Algorithm IJGP viewed iterative version mini-clustering, improvingquality approximation, especially low i-bounds. Given cluster decomposition,mini-clustering potentially create different partitioning every message sent neighbor.dynamic partitioning happen incoming message neighborexcluded realizing partitioning, different set functions split mini-clustersevery message neighbor. define version mini-clustering every clustercreate unique static partition mini-clusters every incoming message includedone mini-clusters. version MC extended IJGP introducinglinks mini-clusters cluster, carefully limiting interactionresulting nodes order eliminate over-counting.Algorithm IJGP works general join-graph may contain cycles. cluster sizegraph user adjustable via i-bound (providing anytime nature), cycles graphallow iterative application message-passing. Subsection 4.1 introduce join-graphsdiscuss properties. Subsection 4.2 describe IJGP algorithm itself.4.1 Join-GraphsEFINITION 7 (join-graph decomposition) join-graph decomposition belief network B =hX, D, G, P triple = hJG, , i, JG = (V, E) graph, labelingfunctions associate vertex v V two sets, (v) X (v) P that:1. pi P , exactly one vertex v V pi (v), scope(pi )(v).2. (connectedness) variable Xi X, set {v V |Xi (v)} induces connectedsubgraph JG. connectedness requirement also called running intersection property.293fiM ATEESCU , K ASK , G OGATE & ECHTER2,41,2,42,3,4C1,42,3,43,4B2,41,2,4C1,41,3,4Ba)31,3,4b)Figure 12: edge-labeled decomposition.often refer node V CPT functions cluster3 use term joingraph decomposition cluster-graph interchangeably. Clearly, join-tree decompositioncluster-tree special case join-graph tree.clear one problems message propagation cyclic join-graphs overcounting. reduce problem devise scheme, avoids cyclicity respectsingle variable. algorithm works edge-labeled join-graphs.EFINITION 8 (minimal edge-labeled join-graph decompositions) edge-labeled join-graphdecomposition B = hX, D, G, P four-tuple = hJG, , , i, JG = (V, E)graph, associate vertex v V sets (v) X (v) Passociates edge (v, u) E set ((v, u)) X that:1. function pi P , exactly one vertex v V pi (v),scope(pi ) (v).2. (edge-connectedness) edge (u, v), ((u, v)) (u) (v), Xi X,two clusters containing Xi connected path whose every edge label includesXi .Finally, edge-labeled join-graph minimal variable deleted labelstill satisfying edge-connectedness property.EFINITION 9 (separator, eliminator edge-labeled join-graphs) Given two adjacent verticesu v JG, separator u v defined sep(u, v) = ((u, v)), eliminator urespect v elim(u, v) = (u) ((u, v)). separator width max(u,v) |sep(u, v)|.Edge-labeled join-graphs made label minimal deleting variables labelsmaintaining connectedness (if edge label becomes empty, edge deleted). easysee that,Proposition 1 minimal edge-labeled join-graph contain cycle relative singlevariable. is, two clusters containing variable connected exactly one pathlabeled variable.Notice every minimal edge-labeled join-graph edge-minimal (no edge deleted),vice-versa.3. Note node may associated empty set CPTs.294fiJ OIN -G RAPH P ROPAGATION LGORITHMSExample 4 example Figure 12a shows edge minimal join-graph contains cyclerelative variable 4, edges labeled separators. Notice however remove variable 4 label one edge cycles (relative single variables)connectedness property still maintained.mini-clustering approximation presented previous section works relaxing jointree requirement exact inference collection join-trees smaller cluster size.introduces independencies original problem via node duplication applies exact inference relaxed model requiring two message passings. class IJGP algorithmstake different route. choose relax tree-structure requirement use join-graphsintroduce new independencies, apply iterative message-passing resulting cyclic structure.Indeed, shown join-graph belief network I-map (independency map,Pearl, 1988) underlying probability distribution relative node-separation. Since planuse minimally edge-labeled join-graphs address over-counting problems, questionkind independencies captured graphs.EFINITION 10 (edge-separation edge-labeled join-graphs) Let = hJG, , , i, JG =(V, E) edge-labeled decomposition Bayesian network B = hX, D, G, P i. Let NW , NYV two sets nodes, EZ E set edges JG. Let W, Y, Z correspondingsets variables (W = vNW (v), Z = eEZ (e)). say EZ edge-separates NWNY path NW NY JG graph whose edges EZ removed.case also say W separated given Z D, write hW |Z|Y iD . Edgeseparation regular join-graph defined relative separators.HEOREM 4 edge-labeled join-graph decomposition = hJG, , , belief networkB = hX, D, G, P I-map P relative edge-separation. Namely, edge separationcorresponds conditional independence P .Proof: Let G moral graph BN . Since G I-map P , enough proveJG I-map G. Let NW NY disjoint sets nodes NZ set edges JG,W, Z, corresponding sets variables G. prove:hNW |NZ |NY iJG = hW |Z|Y iM Gcontradiction. Since sets W, Z, may disjoint, actually prove hWZ|Z|Y ZiM G holds, equivalent hW |Z|Y iM G .Supposing hW Z|Z|Y ZiM G false, exists path = 1 , 2 , . . . , n1 , = nG goes variable = 1 W Z variable = n Z withoutintersecting variables Z. Let Nv set nodes JG contain variable v X,let us consider set nodes:= ni=1 Ni NZargue forms connected sub-graph JG. First, running intersection propertyensures every Ni , = 1, . . . , n, remains connected JG removing nodes NZ(otherwise, must path two disconnected parts original JG,implies part Z, contradiction). Second, fact (i , i+1 ), =295fiM ATEESCU , K ASK , G OGATE & ECHTER1, . . . , n 1, edge moral graph G implies conditional probability table(CPT) i+1 , = 1, . . . , n 1 (and perhaps variables). property 1definition join-graph, follows = 1, . . . , n 1 exists node JGcontains i+1 . proves existence path mutilated join-graph (JGNZ pulled out) node NW containing = 1 node containing 1 2 (N1connected), node one containing 2 3 (N2 connected),reach node NY containing = n . shows hNW |NZ |NY iJG false,concluding proof contradiction. 2Interestingly however, deleting variables edge labels removing edges edge-labeledjoin-graphs whose clusters fixed increase independencies captured edge-labeledjoin-graphs. is,Proposition 2 two (edge-labeled) join-graphs defined set clusters, sharing (V ,, ), express exactly set independencies relative edge-separation, setindependencies identical one expressed node separation primal graphjoin-graph.Proof: follows looking primal graph join-graph (obtained connectingtwo nodes cluster arc original variables nodes) observing edgeseparation join-graph corresponds node separation primal graph vice-versa. 2Hence, issue minimizing computational over-counting due cycles appears unrelated problem maximizing independencies via minimal I-mapness. Nevertheless, avoidover-counting much possible, still prefer join-graphs minimize cycles relativevariable. is, prefer minimal edge-labeled join-graphs.Relationship region graphs strong relationship join-graphsregion graphs Yedidia et al. (2000, 2001, 2005). approach inspired advancesstatistical physics, realized computing partition function essentiallycombinatorial problem expresses probabilistic reasoning. result, variational methodsphysics could counterparts reasoning algorithms. proved Yedidia et al. (2000,2001) belief propagation loopy networks converge (when so) stationarypoints Bethe free energy. Bethe approximation simplest casegeneral Kikuchi (1951) cluster variational method. idea group variables togetherclusters perform exact computation cluster. One key question aggregateresults, account variables shared clusters. Again, ideaeverything counted exactly important. led proposal regiongraphs (Yedidia et al., 2001, 2005) associated counting numbers regions.given possible canonical version graphs support Generalized Belief Propagation(GBP) algorithms. join-graphs accomplish thing. edge-labeled join-graphsdescribed region graphs regions clusters labels edges.tree-ness condition respect every variable ensures over-counting.similar approach ours, also based join-graphs appeared independentlyMcEliece Yildirim (2002), based information theoretic perspective.296fiJ OIN -G RAPH P ROPAGATION LGORITHMSAlgorithm Iterative Join-Graph Propagation (IJGP)Input arc-labeled join-graph decomposition hJG, , , i, JG = (V, E) B = hX, D, G, P i. Evidence variables var(e).Output augmented graph whose nodes clusters containing original CPTs messages receivedneighbors. Approximations P (Xi |e), Xi X.Denote h(u,v) message vertex u v, nev (u) neighbors u JG excluding v.cluster(u) = (u) {h(v,u) |(v, u) E}.clusterv (u) = cluster(u) excluding message v u.One iteration IJGP:every node u JG topological order back,1. Process observed variables:Assign relevant evidence pi (u) (u) := (u) var(e), u V2. Compute individual functions:Include H(u,v) function clusterv (u) whose scope contain variables elim(u, v).Denote remaining functions.PQ3. Compute send v combined function: h(u,v) = elim(u,v) f f .Send h(u,v) individual functions H(u,v) node v.EndforCompute approximation P (Xi |e):every Xi X let uP vertex JGQ Xi (u).Compute P (Xi , e) = (u){Xi } ( f cluster(u) f )Figure 13: Algorithm Iterative Join-Graph Propagation (IJGP).4.2 Algorithm IJGPApplying CTE iteratively minimal edge-labeled join-graphs yields algorithm Iterative JoinGraph Propagation (IJGP) described Figure 13. One iteration algorithm applies messagepassing topological order join-graph, forward back. node u sends message(or messages) neighbor node v operates CPTs cluster messagessent neighbors excluding ones received v. First, individual functions sharevariables eliminator collected sent v. rest functions combinedproduct summed eliminator u v.Based results Lauritzen Spiegelhalter (1988) Larrosa, Kask, Dechter(2001) shown that:HEOREM 51. IJGP applied join-tree decomposition reduces join-tree clustering, therefore guaranteed compute exact beliefs one iteration.2. time complexity one iteration IJGP O(deg (n + N ) dw +1 ) spacecomplexity O(N ), deg maximum degree node join-graph, nnumber variables, N number nodes graph decomposition,maximum domain size, w maximum cluster size maximum label size.proof, see properties CTE presented Kask et al. (2005).297fiM ATEESCU , K ASK , G OGATE & ECHTER32BCa)AB3Bb)1ABC2AB1ABABCc)Figure 14: a) belief network; b) dual join-graph singleton labels; c) dual join-graphjoin-tree.special case Iterative Belief Propagation Iterative belief propagation (IBP) iterative application Pearls algorithm defined poly-trees (Pearl, 1988), Bayesiannetwork. describe IBP instance join-graph propagation dual join-graph.EFINITION 11 (dual graphs, dual join-graphs) Given set functions F = {f1 , . . . , fl }scopes S1 , . . . , Sl , dual graph F graph DG = (V, E, L) associates nodefunction, namely V = F edge connects two nodes whose functions scope sharevariable, E = {(fi , fj )|Si Sj 6= }. L set labels edges, edge labeledshared variables nodes, L = {lij = Si Sj |(i, j) E}. dual join-graph edgelabeled edge subgraph DG satisfies connectedness property. minimal dual join-graphdual join-graph none edge labels reduced maintainingconnectedness property.Interestingly, may many minimal dual join-graphs dual graph.define Iterative Belief Propagation dual join-graph. node sends message edgewhose scope identical label edge. Since Pearls algorithm sends messages whosescopes singleton variables only, highlight minimal singleton-label dual join-graphs.Proposition 3 Bayesian network minimal dual join-graph edge labeledsingle variable.Proof: Consider topological ordering nodes acyclic directed graph Bayesiannetwork = X1 , . . . , Xn . define following dual join-graph. Every node dual graphD, associated pi connected node pj , j < Xj pa(Xi ). label edge pjpi variable Xj , namely lij = {Xj }. easy see resulting edge-labeled subgraphdual graph satisfies connectedness. (Take original acyclic graph G add nodeCPT family, namely parents precede ordering. Since G already satisfiesconnectedness minimal graph generated.) resulting labeled graph dual graphsingleton labels. 2Example 5 Consider belief network 3 variables A, B, C CPTs 1.P (C|A, B),2.P (B|A) 3.P (A), given Figure 14a. Figure 14b shows dual graph singleton labels edges. Figure 14c shows dual graph join-tree, belief propagationsolve problem exactly one iteration (two passes tree).298fiJ OIN -G RAPH P ROPAGATION LGORITHMSAlgorithm Iterative Belief Propagation (IBP)Input: edge-labeled dual join-graph DG = (V, E, L) Bayesian network B = hX, D, G, P i. Evidence e.Output: augmented graph whose nodes include original CPTs messages received neighbors. Approximations P (Xi |e), Xi X. Approximations P (Fi |e), Fi B.Denote by: hvu message u v; ne(u) neighbors u V ; nev (u) = ne(u) {v}; luvlabel (u, v) E; elim(u, v) = scope(u) scope(v).One iteration IBPevery node u DJ topological order back, do:1. Process observed variablesAssign evidence variables pi remove labeled edges.2. Compute send v function:Xhvu =(puhui )elim(u,v){hu,inev (u)}EndforCompute approximations P (Fi |e), P (Xi |e):every Xi QX let u vertex family Fi DJ,P (Fi , e) = ( hu ,une(i) hui ) pu ;PP (Xi , e) = scope(u){Xi } P (Fi , e).Figure 15: Algorithm Iterative Belief Propagation (IBP).completeness, present algorithm IBP, special case IJGP, Figure 15.easy see one iteration IBP time space linear size belief network.shown IBP applied minimal singleton-labeled dual graph coincidesPearls belief propagation applied directly acyclic graph representation. Also, dualjoin-graph tree IBP converges one iteration (two passes, tree) exactbeliefs.4.3 Bounded Join-Graph DecompositionsSince want control complexity join-graph algorithms, define decompositions bounded cluster size. number variables cluster bounded i,time space complexity processing one cluster exponential i. Given join-graph decomposition = hJG, , , i, accuracy complexity (iterative) join-graph propagationalgorithm depends two different width parameters, defined next.EFINITION 12 (external internal widths) Given edge-labeled join-graph decomposition= hJG, , , network B = hX, D, G, P i, internal width maxvV |(v)|,external width treewidth JG graph.Using terminology state target decomposition clearly. Given graphG, bounding parameter wish find join-graph decomposition G whose internalwidth bounded whose external width minimized. bound controls complexityjoin-graph processing external width provides measure accuracy speedconvergence, measures close join-graph join-tree.299fiM ATEESCU , K ASK , G OGATE & ECHTERAlgorithm Join-Graph Structuring(i)1. Apply procedure schematic mini-bucket(i).2. Associate resulting mini-bucket node join-graph, variablesnodes appearing mini-bucket, original functions minibucket.3. Keep edges created procedure (called out-edges) label regularseparator.4. Connect mini-bucket clusters belonging bucket chain in-edgeslabeled single variable bucket.Figure 16: Algorithm Join-Graph Structuring(i).Procedure Schematic Mini-Bucket(i)1. Order variables X1 Xn minimizing (heuristically) induced-width, associate bucket variable.2. Place CPT bucket highest index variable scope.3. j = n 1 do:Partition functions bucket(Xj ) mini-buckets variables.mini-bucket mb create new scope-function (message) f scope(f ) ={X|X mb} {Xi } place scope(f) bucket highest variable. Maintainedge mb mini-bucket (created later) f .Figure 17: Procedure Schematic Mini-Bucket(i).consider two classes algorithms. One class partition-based. starts giventree-decomposition partitions clusters decomposition clusters boundedi. alternative approach grouping-based. starts minimal dual-graph-based join-graphdecomposition (where cluster contains single CPT) groups clusters larger clusterslong resulting clusters exceed given bound. methods one attemptreduce external width generated graph-decomposition. partition-based approachinspired mini-bucket idea (Dechter & Rish, 1997) follows.Given bound i, algorithm Join-Graph Structuring(i) applies procedure Schematic MiniBucket(i), described Figure 17. procedure traces scopes functions wouldgenerated full mini-bucket procedure, avoiding actual computation. procedure endscollection mini-bucket trees, rooted mini-bucket first variable.trees minimally edge-labeled. Then, in-edges labeled one variable introduced,added obtain running intersection property branches trees.Proposition 4 Algorithm Join-Graph Structuring(i) generates minimal edge-labeled join-graphdecomposition bound i.Proof: construction join-graph specifies vertices edges join-graph, wellvariable function labels vertex. need demonstrate 1) connectednessproperty holds, 2) edge-labels minimal.300fiJ OIN -G RAPH P ROPAGATION LGORITHMSG: (GFE)GFEP(G|F,E)EFE: (EBF)(EF)EBFP(E|B,F)P(F|C,D)F: (FCD)(BF)BFFFCDBFCDD: (DB)(CD)P(D|B)CDBCBC: (CAB) (CB)P(C|A,B)BCABBAB: (BA)(AB)A: (A)(A)(B)P(B|A)BAP(A)(b)(a)Figure 18: Join-graph decompositions.Connectedness property specifies 2 vertices u v, vertices u v containvariable X, must path u, w1 , . . . , wm , v u v every vertexpath contains variable X. two cases here. 1) u v correspond 2 mini-bucketsbucket, 2) u v correspond mini-buckets different buckets. case 12 cases, 1a) variable X eliminated bucket, 1b) variable Xeliminated bucket. case 1a, mini-bucket must contain X mini-bucketsbucket connected chain, connectedness property holds. case 1b, vertexes u vconnect (respectively) parents, turn connect parents, etc. bucketscheme variable X eliminated. nodes along chain connect variable X,connectedness property holds. Case 2 resolves like case 1b.show edge labels minimal, need prove cycles respectedge labels. cycle respect variable X, must involve least one in-edge(edge connecting two mini-buckets bucket). means variable X must variableeliminated bucket in-edge. means variable X containedparents mini-buckets bucket. Therefore, order cycle exist, another in-edgebucket-tree bucket must contain X. However, impossible wouldimply variable X eliminated twice. 2Example 6 Figure 18a shows trace procedure schematic mini-bucket(3) applied problem described Figure 2a. decomposition Figure 18b created algorithm graphstructuring. cluster partitioned F two scopes (FCD) (BF), connectedin-edge labeled F.range edge-labeled join-graphs shown Figure 19. left side graphsmaller clusters, cycles. type graph IBP works on. right sidetree decomposition, cycles expense bigger clusters. between,could number join-graphs maximum cluster size traded numbercycles. Intuitively, graphs left present less complexity join-graph algorithmscluster size smaller, also likely less accurate. graphs right side301fiM ATEESCU , K ASK , G OGATE & ECHTERCABCABABDEBCCABCABCBCECBCBCABDEABCDEDECECDECDECECECDEFCDEFCDEFCDEFFFGHHFGHHFGIGHGIFHGHIJHFGHHFFFFGABCDEBCECDEBCEFGIGIFFGHGHIJFGIGHGIGHIGHIJFGHIGHIJaccuracyless complexityFigure 19: Join-graphs.computationally complex, larger cluster size, likelyaccurate.4.4 Inference Power IJGPquestion address subsection propagating messages iteratively help.IJGP upon convergence superior IJGP one iteration superior MC? One clueprovided considering deterministic constraint networks viewed extreme probabilistic networks. known constraint propagation algorithms, analogous messages sent belief propagation, guaranteed converge guaranteedimprove iteration. propagation scheme IJGP works similar constraint propagationrelative flat network abstraction probability distribution (where non-zero entriesnormalized positive constant), propagation guaranteed accurateabstraction least.following shed light IJGPs behavior making connectionswell-known concept arc-consistency constraint networks (Dechter, 2003). showthat: (a) variable-value pair assessed zero-belief, remains zero-beliefsubsequent iterations; (b) variable-value zero-beliefs computed IJGP correct; (c)terms zero/non-zero beliefs, IJGP converges finite time. also empirically investigatedhypothesis variable-value pair assessed IBP IJGP positiveclose zero belief, likely correct. Although experimental results shownpaper contradict hypothesis, examples recent experiments Dechter,Bidyuk, Mateescu, Rollon (2010) invalidate it.302fiJ OIN -G RAPH P ROPAGATION LGORITHMS4.4.1 IJGPRC -C ONSISTENCYbelief network define constraint network captures assignmentsstrictly positive probability. show correspondence IJGP applied beliefnetwork arc-consistency algorithm applied constraint network. Since arc-consistencyalgorithms well understood, correspondence proves target claims, mayprovide additional insight behavior IJGP. justifies iterative application beliefpropagation, also illuminates distance complete.EFINITION 13 (constraint satisfaction problem) Constraint Satisfaction Problem (CSP)triple hX, D, Ci, X = {X1 , . . . , Xn } set variables associated set discretevalued domains = {D1 , . . . , Dn } set constraints C = {C1 , . . . , Cm }. constraintCi pair hSi , Ri Ri relation Ri DSi defined subset variables Si XDSi Cartesian product domains variables Si . relation Ri denotes compatibletuples DSi allowed constraint. projection operator creates new relation, Sj (Ri ) ={x|x DSj y, DSi \Sj xy Ri }, Sj Si . Constraints combinedjoin operator 1, resulting new relation, Ri 1 Rj = {x|Si (x) Ri Sj (x) Rj }.solution assignment values variables x = (x1 , . . . , xn ), xi Di ,Ci C, xSi Ri . constraint network represents set solutions, 1i Ci .Given belief network B, define flattening Bayesian network constraintnetwork called f lat(B), zero entries probability table removed corresponding relation. network f lat(B) defined set variablesset domain values B.EFINITION 14 (flat network) Given Bayesian network B = hX, D, G, P i, flat networkf lat(B) constraint network, set variables X, every Xi XCPT P (Xi |pa(Xi )) B define constraint RFi family Xi , Fi = {Xi } pa(Xi )follows: every assignment x = (xi , xpa(Xi ) ) Fi , (xi , xpa(Xi ) ) RFi iff P (xi |xpa(Xi ) ) > 0.HEOREM 6 Given belief network B = hX, D, G, P i, X = {X1 , . . . , Xn }, tuplex = (x1 , . . . , xn ): PB (x) > 0 x sol(f lat(B)), sol(f lat(B)) set solutionsflat constraint network.Proof: PB (x) > 0 ni=1 P (xi |xpa(Xi ) ) > 0 {1, . . . , n}, P (xi |xpa(Xi ) ) > 0{1, . . . , n}, (xi , xpa(Xi ) ) RFi x sol(f lat(B)). 2Constraint propagation class polynomial time algorithms center constraint processing techniques. investigated extensively past three decadeswell known versions arc-, path-, i-consistency (Dechter, 1992, 2003).EFINITION 15 (arc-consistency) (Mackworth, 1977) Given binary constraint network(X, D, C), network arc-consistent iff every binary constraint Rij C, every value v Divalue u Dj s.t. (v, u) Rij .303fiM ATEESCU , K ASK , G OGATE & ECHTERNote arc-consistency defined binary networks, namely relations involvetwo variables. binary constraint network arc-consistent, algorithmsprocess enforce arc-consistency. algorithms remove values domainsvariables violate arc-consistency arc-consistent network generated.several versions improved performance arc-consistency algorithms, however considernon-optimal distributed version, call distributed arc-consistency.EFINITION 16 (distributed arc-consistency algorithm)algorithmdistributedarcconsistency message-passing algorithm constraint network. node variable,maintains current set viable values Di . Let ne(i) set neighbors Xiconstraint graph. Every node Xi sends message node Xj ne(i), consistsvalues Xj domain consistent current Di , relative constraint Rjishare. Namely, message Xi sends Xj , denoted Dij , is:Dij j (Rji 1 Di )(1)Di Di (1kne(i) Dki )(2)addition node computes:Clearly algorithm synchronized iterations, iteration every nodecomputes current domain based messages received far neighbors (Eq. 2),sends new message neighbor (Eq. 1). Alternatively, Equations 1 2 combined.message Xi sends Xj is:Dij j (Rji 1 Di 1kne(i) Dki )(3)Next define join-graph decomposition flat constraint networkestablish correspondence join-graph decomposition Bayesian network Bjoin-graph decomposition flat network f lat(B). Note constraint networks, edgelabeling ignored.EFINITION 17 (join-graph decomposition flat network) Given join-graph decomposition = hJG, , , Bayesian network B, join-graph decomposition Df lat =hJG, , f lat flat constraint network f lat(B) underlying graph structureJG = (V, E) D, variable-labeling clusters , mapping f lat mapscluster relations corresponding CPTs, namely Ri f lat (v) iff CPT pi (v).distributed arc-consistency algorithm Definition 16 applied join-graph decomposition flat network. case, nodes exchange messages clusters(namely elements set V JG). domain cluster V set tuplesjoin original relations cluster (namely domain cluster u ./Rf lat (u) R).constraints binary, involve clusters V neighbors. two clusters u v,corresponding values tu tv (which tuples representing full assignments variablescluster) belong relation Ruv (i.e., (tu , tv ) Ru,v ) projections separator (orlabeling ) u v identical, namely ((u,v)) tu = ((u,v)) tv .304fiJ OIN -G RAPH P ROPAGATION LGORITHMSdefine algorithm relational distributed arc-consistency (RDAC), applies distributed arc-consistency join-graph decomposition constraint network. call relational emphasize nodes exchanging messages fact relations originalproblem variables, rather simple variables case arc-consistency algorithms.EFINITION 18 (relational distributed arc-consistency algorithm: RDAC join-graph)Given join-graph decomposition constraint network, let Ri Rj relations twoclusters (Ri Rj joins respective constraints cluster), scopes SiSj , Si Sj 6= . message Ri sends Rj denoted h(i,j) defined by:h(i,j) Si Sj (Ri )(4)ne(i) = {j|Si Sj 6= } set relations (clusters) share variable Ri .cluster updates current relation according to:Ri Ri 1 (1kne(i) h(k,i) )(5)Algorithm RDAC iterates change.Equations 4 5 combined, like Equation 3. message Ri sends Rjbecomes:h(i,j) Si Sj (Ri 1 (1kne(i) h(k,i) ))(6)establish correspondence IJGP, define algorithm IJGP-RDAC appliesRDAC order computation (schedule processing) IJGP.EFINITION 19 (IJGP-RDAC algorithm) Given Bayesian network B = hX, D, G, P i, letDf lat = hJG, , f lat , join-graph decomposition flat network f lat(B). algorithm IJGP-RDAC applied decomposition Df lat f lat(B), describedIJGP applied D, following modifications:Q1. Instead , use 1.P2. Instead , use .3. end end, update domains variables by:Di Di Xi ((1vne(u) h(v,u) ) 1 (1R(u) R))(7)u cluster containing Xi .Note algorithm IJGP-RDAC, could first merge constraints cluster usingle constraint Ru =1R(u) R. construction, IJGP-RDAC enforces arc-consistencyjoin-graph decomposition flat network. join-graph Df lat join-tree,IJGP-RDAC solves problem namely finds solutions constraint network.305fiM ATEESCU , K ASK , G OGATE & ECHTERProposition 5 Given join-graph decomposition Df lat = hJG, , f lat , i, JG = (V, E),flat constraint network f lat(B), corresponding given join-graph decompositionBayesian network B = hX, D, G, P i, algorithm IJGP-RDAC applied Df lat enforces arcconsistency join-graph Df lat .Proof: IJGP-RDAC applied join-graph decomposition Df lat = hJG, , f lat , i, JG =(V, E), equivalent applying RDAC Definition 18 constraint network vertices Vvariables {1R(u) R|u V } relations. 2Following properties convergence arc-consistency, show that:Proposition 6 Algorithm IJGP-RDAC converges O(m r) iterations, numberedges join-graph r maximum size separator Dsep(u,v) two clusters.Proof: follows fact messages (which relations) clusters IJGP-RDACchange monotonically, tuples successively removed relations separators. Sincesize relation separator bounded r edges, O(mr)iterations needed. 2following establish equivalence IJGP IJGP-RDAC termszero probabilities.Proposition 7 IJGP IJGP-RDAC applied order computation,messages computed IJGP identical computed IJGP-RDAC terms zero / nonzero probabilities. is, h(u,v) (x) 6= 0 IJGP iff x h(u,v) IJGP-RDAC.Proof: proof induction. base case trivially true since messages h IJGP initialized uniform distribution messages h IJGP-RDAC initialized complete relations.induction step. Suppose hIJGP(u,v) message sent u v IJGP. showIJGP RDACIJGP RDAChIJGPh(u,v)message sent IJGP(u,v) (x) 6= 0, x h(u,v)RDAC u v. Assume claim holds messages received u neighbors.Let f clusterv (u) IJGP Rf corresponding relation IJGP-RDAC,PQ asIJGPsignment values variables elim(u, v). h(u,v) (x) 6= 0 elim(u,v) f f (x) 6= 0Qt, f f (x, t) 6= 0 t, f, f (x, t) 6= 0 t, f, scope(Rf ) (x, t) Rf t, elim(u,v) (1RfIJGP RDACIJGP RDAC. 2x h(u,v)scope(Rf ) (x, t)) h(u,v)Next show IJGP computing marginal probability P (Xi = xi ) = 0 equivalentIJGP-RDAC removing xi domain variable Xi .Proposition 8 IJGP computes P (Xi = xi ) = 0 iff IJGP-RDAC decides xi 6 Di .Proof: According Proposition 7 messages computed IJGP IJGP-RDAC identicalterms zero probabilities. Let f cluster(u) IJGP Rf corresponding relationIJGP-RDAC, assignment values variables (u)\Xi . showIJGP computes P (Xi = xi ) = 0 (upon convergence), IJGP-RDAC computes xi 6 Di .306fiJ OIN -G RAPH P ROPAGATION LGORITHMSQQPP (Xi = xi ) =f f (xi , t) = 0 t, f, f (xi , t) = 0f f (xi ) = 0 t,X\Xit, Rf , scope(Rf ) (xi , t) 6 Rf t, (xi , t) 6 (1Rf Rf (xi , t)) xi 6 Di Xi (1Rf Rf (xi , t))xi 6 Di . Since arc-consistency sound, decision zero probabilities. 2Next show P (Xi = xi ) = 0 computed IJGP sound.HEOREM 7 Whenever IJGP finds P (Xi = xi ) = 0, probability P (Xi ) expressedBayesian network conditioned evidence 0 well.Proof: According Proposition 8, whenever IJGP finds P (Xi = xi ) = 0, value xi removeddomain Di IJGP-RDAC, therefore value xi Di no-good network f lat(B),Theorem 6 follows PB (Xi = xi ) = 0. 2following show time takes IJGP find P (Xi = xi ) = 0 bounded.Proposition 9 IJGP finds P (Xi = xi ) = 0 finite time, is, exists number k,P (Xi = xi ) = 0 found k iterations.Proof: follows fact number iterations takes IJGP compute P (Xi =xi ) = 0 exactly number iterations IJGP-RDAC takes remove xi domainDi (Proposition 7 Proposition 8), fact IJGP-RDAC runtime bounded (Proposition6). 2Previous results also imply IJGP monotonic respect zeros.Proposition 10 Whenever IJGP finds P (Xi = xi ) = 0, stays 0 subsequent iterations.Proof: Since know relations IJGP-RDAC monotonically decreasing algorithmprogresses, follows equivalence IJGP-RDAC IJGP (Proposition 7) IJGPmonotonic respect zeros. 24.4.2 F INITE P RECISION P ROBLEMfinite precision machines danger underflow interpreted zerovalue. provide warning implementation belief propagation allowcreation zero values underflow. show example Figure 20 IBPs messagesconverge limit (i.e., infinite number iterations), stabilize finitenumber iterations. nodes Hk set value 1, belief Xi variablesfunction iteration given table Figure 20. 300 iterations, finite precisioncomputer able represent value Bel(Xi = 3), appears zero,yielding final updated belief (.5, .5, 0), fact true updated belief (0, 0, 1).Notice (.5, .5, 0) cannot regarded legitimate fixed point IBP. Namely, wouldinitialize IBP values (.5, .5, 0), algorithm would maintain them, appearingfixed point, initializing IBP zero values cannot expected correct.307fiM ATEESCU , K ASK , G OGATE & ECHTERX1Prior XiH3H1X3X2XiP ( Xi )1.452.453.1H2Hk XiCPT HkXjP ( Hk | Xi , Xj )11211211133110#iterBel(Xi = 1) Bel(Xi = 2) Bel(Xi = 3)1.45.45.12.49721.49721.005453.49986.49986.000271001e-1292001e-260300.5.50Truebelief001Figure 20: Example finite precision problem.initialize zeros forcibly introduce determinism model, IBP always maintainafterwards.However, example contradict theory because, mathematically, Bel(Xi = 3)never becomes true zero, IBP never reaches quiescent state. example shows closezero belief network arbitrarily inaccurate. case inaccuracy seems dueinitial prior belief different posterior ones.4.4.3 ACCURACY IBP ACROSS B ELIEF ISTRIBUTIONpresent empirical evaluation accuracy IBPs prediction range belief distribution 0 1. results also extend IJGP. previous section, proved zerovalues inferred IBP correct, wanted test hypothesis property extendssmall beliefs (namely, close zero). is, IBP infers posterior belief closezero, likely correct. results presented paper seem support hypothesis, however new experiments Dechter et al. (2010) show true general.yet good characterization cases hypothesis confirmed.test hypothesis, computed absolute error IBP per intervals [0, 1]. giveninterval [a, b], 0 < b 1, use measures inspired information retrieval: RecallAbsolute Error Precision Absolute Error.Recall absolute error averaged exact posterior beliefs fall interval[a, b]. Precision, average taken approximate posterior belief values computedIBP interval [a, b]. Intuitively, Recall([a,b]) indicates far belief computedIBP exact, exact [a, b]; Precision([a,b]) indicates far exactIBPs prediction, value computed IBP [a, b].experiments show two measures strongly correlated. also show histograms distribution belief interval, exact IBP, also stronglycorrelated. results given Figures 21 22. left axis corresponds histograms(the bars), right axis corresponds absolute error (the lines).present results two classes problems: coding networks grid network. problemsbinary variables, graphs symmetric 0.5 show interval [0, 0.5].number variables, number iterations induced width w* reported graph.308fiJ OIN -G RAPH P ROPAGATION LGORITHMSRecall Abs. Errornoise = 0.20noise = 0.400.4Absolute Error0.450.300.350.350.010.20%0.020.250%0.030.10%0.040.155%Precision Abs. Error0.05010%5%0.410%5%0.4510%0.315%0.220%15%0.2520%15%0.120%0.1525%030%25%0.0530%25%0.430%0.4535%0.340%35%0.3540%35%0.240%0.2545%0.145%0.1545%050%0.05IBP Histogram50%0.05PercentageExact Histogram50%noise = 0.60Figure 21: Coding, N=200, 1000 instances, w*=15.Recall Abs. Errorevidence = 0evidence = 100.0040.0030.0020.001Absolute Error0.0050.40.450.30.350.20.250.10.15000.40.450.30.350%0.20%0.255%0.110%5%0.1510%015%0.0520%15%0.420%0.4525%0.330%25%0.3530%0.235%0.2540%35%0.140%0.1545%045%Precision Abs. Error0.05IBP Histogram50%0.05PercentageExact Histogram50%evidence = 20Figure 22: 10x10 grids, 100 instances, w*=15.Coding networks IBP famously known impressive performance coding networks.tested linear block codes, 50 nodes per layer 3 parent nodes. Figure 21 showsresults three different values channel noise: 0.2, 0.4 0.6. noise 0.2, beliefscomputed IBP extreme. Recall Precision small, order 1011 . So,case, beliefs small ( small) IBP able infer correctly, resultingalmost perfect accuracy (IBP indeed perfect case bit error rate). noiseincreased, Recall Precision tend get closer bell shape, indicating higher errorvalues close 0.5 smaller error extreme values. histograms also show less beliefvalues extreme noise increased, factors account overall decreaseaccuracy channel noise increases. networks examples large number-small probabilities IBP able infer correctly (absolute error small).Grid networks present results grid networks Figure 22. Contrary case codingnetworks, histograms show higher concentration beliefs around 0.5. However, accuracystill good beliefs close zero. absolute error peaks close 0 maintains plateau,evidence increased, indicating less accuracy IBP.5. Experimental Evaluationanticipated summary Section 3, clearly seen structuringbounded join-graph, close relationship mini-clustering algorithm MC(i)309fiM ATEESCU , K ASK , G OGATE & ECHTERIBP#it1510MC#evid05100510051005100.029880.061780.087620.008290.051820.080390.008280.051820.08040Absolute errorIJGPi=2i=50.03055 0.026230.04434 0.042010.05777 0.054090.00636 0.005920.00886 0.008860.01155 0.010730.00584 0.005140.00774 0.007320.00892 0.00808IBPi=80.029400.045540.059100.006690.011230.013990.004950.007080.008550.04044 0.04287 0.037480.05303 0.05171 0.042500.06033 0.05489 0.042660.063880.150050.237770.017260.125890.217810.017250.125900.21782Relative errorIJGPi=50.056770.120560.142780.012390.019650.025530.010690.016280.01907i=20.156940.123400.180710.013260.019670.030140.012160.017270.02101IBPi=80.071530.111540.156860.013980.024940.032790.010300.015750.020050.08811 0.09342 0.081170.12375 0.11775 0.095960.14702 0.13219 0.100740.002130.008120.015470.000210.006580.013820.000210.006580.01382KL distanceIJGPi=50.002080.004780.007680.000150.000260.000420.000100.000170.00024i=20.003910.005820.009150.000140.000240.000550.000120.000180.00028IBPi=80.002770.005580.008990.000180.000440.000730.000100.000160.000290.00403 0.00435 0.003690.00659 0.00636 0.004770.00841 0.00729 0.005030.00170.00130.00130.00660.00600.00480.01300.01210.0109TimeIJGPi=50.00580.00520.00360.02260.01850.01380.04360.03550.0271i=20.00360.00400.00400.01450.01200.01000.02540.02230.0191i=80.02950.02000.01210.12190.08400.05360.23830.16390.10620.0159 0.0173 0.05520.0146 0.0158 0.05320.0119 0.0143 0.0470Table 4: Random networks: N=50, K=2, C=45, P=3, 100 instances, w*=16.IJGP(i). particular, one iteration IJGP(i) similar MC(i). MC sends messagesalong clusters form set trees. IJGP additional connections allowinteraction mini-clusters cluster. Since cyclic structure, iteratingfacilitated, virtues drawbacks.sevaluation IJGP(i), focus two different aspects: (a) sensitivity parametricIJGP(i) i-bound number iterations; (b) comparison IJGP(i) publiclyavailable state-of-the-art approximation schemes.5.1 Effect i-bound Number Iterationstested performance IJGP(i) random networks, M-by-M grids, two benchmark CPCS files 54 360 variables, respectively coding networks. typenetworks, ran IBP, MC(i) IJGP(i), giving IBP IJGP(i) numberiterations.use partitioning method described Section 4.3 construct join-graph. determineorder message computation, recursively pick edge (u,v), node u fewestincoming messages missing.network except coding, compute exact solution compare accuracyusing absolute relative error, before, well KL (Kullback-Leibler) distance Pexact (X = a) log(Pexact (X = a)/Papproximation (X = a)) averaged values, variablesproblems. coding networks report Bit Error Rate (BER) computed describedSection 3.2. also report time taken algorithm.random networks generated using parameters (N,K,C,P), N numbervariables, K domain size, C number conditional probability tables (CPTs) Pnumber parents CPT. Parents CPT picked randomly CPTfilled randomly. grid networks, N square number CPT filled randomly.problem class, also tested different numbers evidence variables. before, codingnetworks class linear block codes, channel noise level. Notelimited relatively small sparse problem instances evaluation measuresbased comparing exact figures.Random networks results networks N=50, K=2, C=45 P=3 given Table 4Figures 23 24. IJGP(i) MC(i) report 3 different values i-bound: 2, 5, 8.IBP IJGP(i) report results 3 different numbers iterations: 1, 5, 10. report results310fiJ OIN -G RAPH P ROPAGATION LGORITHMSRandom networks, N=50, K=2, P=3, evid=5, w*=160.010IJGP 1IJGP 2IJGP 3IJGP 5IJGP 10IJGP 15IJGP 20MCIBP 1IBP 2IBP 3IBP 5IBP 100.008KL distance0.0060.0040.0020.00001234567891011i-bound(a) Performance vs. i-bound.Random networks, N=50, K=2, P=3, evid=5, w*=160.010IBPIJGP(2)IJGP(10)0.008KL distance0.0060.0040.0020.00005101520253035Number iterations(b) Convergence iterations.Figure 23: Random networks: KL distance.3 different numbers evidence: 0, 5, 10. Table 4 Figure 23a see IJGP(i)always better IBP (except i=2 number iterations 1), sometimes ordermagnitude, terms absolute error, relative error KL distance. IBP rarely changes 5iterations, whereas IJGP(i)s solution improved iterations (up 15-20). theorypredicted, accuracy IJGP(i) one iteration MC(i). IJGP(i)improves number iterations increases, eventually better MC(i) muchorder magnitude, although clearly takes time, especially i-bound large.311fiM ATEESCU , K ASK , G OGATE & ECHTERRandom networks, N=50, K=2, P=3, evid=5, w*=161.0IJPG 1IJGP 2IJGP 3IJGP 5IJGP 10IJGP 15IJGP 20MCIBP 1IBP 20Time (seconds)0.80.60.40.20.001234567891011i-boundFigure 24: Random networks: Time.IBP#it1510MC#evid05100510051005100.035240.053750.070940.003580.032240.055030.003520.032220.05503Absolute errorIJGPi=2i=50.05550 0.042920.05284 0.040120.05453 0.043040.00393 0.003250.00379 0.003190.00364 0.003160.00352 0.002320.00357 0.002480.00347 0.00239IBPi=80.033180.036610.039660.002840.002960.003140.001360.001490.001410.05827 0.04036 0.015790.05973 0.03692 0.013550.05866 0.03416 0.010750.080750.163800.236240.007750.112990.194030.007600.112950.19401Relative errorIJGPi=50.102520.098890.124920.007020.007100.007560.005020.005490.00556i=20.135330.132250.145880.008490.008440.008410.007600.007960.00804IBPi=80.079040.091160.122020.006340.006690.013130.002930.003300.003280.13204 0.08833 0.034400.13831 0.08213 0.030010.14120 0.07791 0.024880.002890.007250.012320.000050.004830.009940.000050.004830.00994KL distanceIJGPi=50.006020.005700.006810.000070.000070.000090.000030.000030.00003i=20.008590.008020.009050.000060.000060.000060.000050.000050.00005IBPi=80.004540.005490.006530.000100.000100.000190.000010.000020.000010.00650 0.00387 0.001050.00696 0.00348 0.000990.00694 0.00326 0.000750.00100.00160.00130.00490.00530.00360.00900.00960.0090TimeIJGPi=50.01060.00920.00720.03470.03090.02710.06710.05580.0495i=20.00530.00410.00380.01520.01310.01270.02770.02460.0223i=80.04260.03150.02560.14620.11270.09130.27760.21490.17160.0106 0.0142 0.03820.0102 0.0130 0.03420.0099 0.0116 0.0321Table 5: 9x9 grid, K=2, 100 instances, w*=12.Figure 23a shows comparison algorithms different numbers iterations, usingKL distance. network structure changes different i-bounds, necessarilysee monotonic improvement IJGP i-bound given number iterations (as caseMC). Figure 23b shows IJGP converges iterations smaller KL distanceIBP. expected, time taken IJGP (and MC) varies exponentially i-bound (seeFigure 24).Grid networks results networks N=81, K=2, 100 instances similarrandom networks. reported Table 5 Figure 25, see impactevidence (0 5 evidence variables) algorithms. IJGP convergence givesbest performance cases, IBPs performance deteriorates evidencesurpassed MC i-bound 5 larger.CPCS networks results CPCS54 CPCS360 given Table 6 Figure 26,even pronounced random grid networks. evidence added, IJGP(i)accurate MC(i), accurate IBP, seen Figure 26a.Coding networks results given Table 7. tested large networks 400 variables,treewidth w*=43, IJGP IBP set run 30 iterations (this enough ensure312fiJ OIN -G RAPH P ROPAGATION LGORITHMSGrid network, N=81, K=2, evid=5, w*=120.010IJGP 1IJGP 2IJGP 3IJGP 5IJGP 10MCIBP 1IBP 2IBP 3IBP 5IBP 100.008KL distance0.0060.0040.0020.00001234567891011i-bound(a) Performance vs. i-bound.Grid network, N=81, K=2, evid=5, w*=127e-5IJGP 20 iterations(at convergence)6e-5KL distance5e-54e-53e-52e-51e-501234567891011i-bound(b) Fine granularity KL.Figure 25: Grid 9x9: KL distance.convergence). IBP known accurate class problems indeed betterMC. However notice IJGP converges slightly smaller BER IBP even smallvalues i-bound. coding network CPCS360 show scalability IJGP largesize problems. Notice anytime behavior IJGP clear.summary, see IJGP almost always superior IBP MC(i) sometimesaccurate several orders magnitude. One note IBP cannot improvedtime, MC(i) requires large i-bound many hard large networks achievereasonable accuracy. question iterative application IJGP instrumentalsuccess. fact, IJGP(2) isolation appears cost-effective variant.313fiM ATEESCU , K ASK , G OGATE & ECHTERIBP#it1510MC11020MC#evidAbsolute errorIJGPi=2i=5Relative errorIJGPi=5IBPi=8i=20.027160.057360.084750.000640.040670.073020.000640.040670.073020.089660.090070.091560.000330.001240.002150.000180.000780.001230.056480.056870.06002KL distanceIJGPi=5IBPi=8CPCS540.07761 0.056160.07676 0.058560.08246 0.066870.00255 0.002250.00194 0.002030.00298 0.003020.00029 0.000310.00071 0.000800.00109 0.001220.05128 0.030470.05314 0.037130.05318 0.03409TimeIBPi=20.000410.001990.003577.75e-70.001610.003217.75e-70.001610.003210.005830.005730.005670.000000.000000.000010.00000.000004.0e-60.002180.002010.002160.005120.004930.005060.000020.000010.000030.000000.000003.0e-60.001710.001860.00177i=80.003780.003660.003900.000010.000010.000020.000000.000004.0e-60.000760.000980.000910.00970.00720.0050.03710.03370.02900.07360.06330.0575i=2IJGPi=5i=80.01370.00940.00470.03340.02150.01440.05870.03890.02510.01440.01030.00940.01460.00870.00520.03840.02600.01780.06670.04710.02970.01250.01260.00900.02750.01690.01150.09120.06310.03780.17200.11780.07230.03330.03460.029505100510051005100.013240.026840.039150.000310.018740.033480.000310.018740.033480.037470.037390.038430.000160.000580.001010.000090.000370.000580.027210.027020.028250.031830.031240.034260.001230.000920.001390.000140.000340.000510.024870.025220.025040.022330.023370.027470.001100.000980.001440.000150.000380.000570.014860.017600.0160010201020102010200.264210.263260.017720.024130.017720.024130.142220.128670.006940.004660.000030.000010.033890.027150.139070.129370.001210.001153.0e-69.0e-60.019840.01543CPCS3600.14334 7.78167 2119.20 2132.78 2133.84 0.17974 0.09297 0.09151 0.09255 0.7172 0.5486 0.5282 0.45930.13665 370.444 28720.38 30704.93 31689.59 0.17845 0.08212 0.08269 0.08568 0.6794 0.5547 0.5250 0.45780.00258 1.06933 6.07399 0.01005 0.04330 0.017718 0.00203 0.00019 0.00116 7.2205 4.7781 4.5191 3.79060.00138 62.99310 26.04308 0.00886 0.01353 0.02027 0.00118 0.00015 0.00036 7.0830 4.8705 4.6468 3.83923.0e-61.06933 0.000448.0e-67.0e-60.017715.0e-60.00.014.4379 9.5783 9.0770 7.60179.0e-662.9931 0.00014 0.00013 0.00004 0.020270.00.00.013.6064 9.4582 9.0423 7.44530.014020.65600 0.20023 0.119900.01299 0.00590 0.003902.8077 2.7112 2.51880.009570.81401 0.17345 0.091130.01007 0.00444 0.002342.8532 2.7032 2.5297Table 6: CPCS54 50 instances, w*=15; CPCS360 10 instances, w*=20.0.22 IJGPMC0.28 IJGPMC0.32 IJGPMC0.40 IJGPMC0.51 IJGPMC0.65 IJGPMC20.000050.005010.000620.021700.002380.040180.012020.087260.076640.153960.190700.21890IJGP 0.36262MC 0.25281Bit Error Ratei-bound4680.00005 0.00005 0.000050.00800 0.00586 0.004620.00062 0.00062 0.000620.02968 0.02492 0.020480.00238 0.00238 0.002380.05004 0.04480 0.038780.01188 0.01194 0.012100.09762 0.09272 0.087660.07498 0.07524 0.075780.16048 0.15710 0.154520.19056 0.19016 0.190300.22056 0.21928 0.21904Time0.41695 0.86213 2.623070.21816 0.31094 0.74851100.000050.003920.000620.018400.002380.035580.011920.083340.075540.151800.190560.21830IBP0.000050.000640.002420.012200.078160.191429.23610 0.0197522.33257Table 7: Coding networks: N=400, P=4, 500 instances, 30 iterations, w*=43.5.2 Comparing IJGP Algorithmssection provide comparison IJGP state-of-the-art publicly available schemes.comparison based recent evaluation algorithms performed Uncertainty AI2008 conference4 . present results solving belief updating task (also called taskcomputing posterior node marginals - MAR). first give brief overview schemesexperimented compared with.1. EDBP - Edge Deletion Belief Propagation4. Complete results available http://graphmod.ics.uci.edu/uai08/Evaluation/Report.314fiJ OIN -G RAPH P ROPAGATION LGORITHMSCPCS360, evid=10, w*=200.20IJGP 1IJGP 10IJGP 20MCIBP 1IBP 10IBP 200.180.16KL distance0.140.120.100.080.060.040.020.0001234567891011i-bound(a) Performance vs. i-bound.CPCS360, evid=10, w*=206e-6IJGP 20 iterations(at convergence)5e-6KL distance4e-63e-62e-61e-601234567891011i-bound(b) Fine granularity KL.Figure 26: CPCS360: KL distance.EDBP (Choi & Darwiche, 2006a, 2006b) approximation algorithm Belief Updating.solves exactly simplified version original problem, obtained deletingedges problem graph. Edges deleted selected based two criteria:quality approximation complexity computation (tree-width reduction). Informationloss lost dependencies compensated introducing auxiliary network parameters.method corresponds Iterative Belief Propagation (IBP) enough edges deletedyield poly-tree, corresponds generalized BP otherwise.2. TLSBP - truncated Loop series Belief propagation algorithm315fiM ATEESCU , K ASK , G OGATE & ECHTERTLSBP based loop series expansion formula Chertkov Chernyak (2006)specifies series terms need added solution output BP exactsolution recovered. series basically sum so-called generalized loopsgraph. Unfortunately, number generalized loops prohibitivelylarge, series little value. idea TLSBP truncate series decomposinggeneralized loops simple smaller loops, thus limiting number loopssummed. evaluation, used implementation TLSBP available workGomez, Mooji, Kappen (2007). implementation handle binary networks only.3. EPIS - Evidence Pre-propagation Importance SamplingEPIS (Yuan & Druzdzel, 2003) importance sampling algorithm Belief Updating.well known sampling algorithms perform poorly presented unlikely evidence.However, samples weighted importance function, good approximationobtained. algorithm computes approximate importance function using loopy beliefpropagation -cutoff heuristic. used implementation EPIS availableauthors. implementation works Bayesian networks only.4. IJGP - Iterative Join-Graph Propagationevaluation, IJGP(i) first run i=2, convergence, i=3, convergence, etc. i= treewidth (when i-bound=treewidth, join-graph becomes join-treeIJGP becomes exact). preprocessing, algorithm performed SAT-based variable domain pruning converting zero probabilities problem SAT problem performing singleton-consistency enforcement. problem size may reduce substantially,cases, preprocessing step may significant impact time-complexityIJGP, amortized increasing i-bound. However, given i-bound, step improves accuracy IJGP marginally.5. SampleSearchSampleSearch (Gogate & Dechter, 2007) specialized importance sampling schemegraphical models contain zero probabilities CPTs. graphical models,importance sampling suffers rejection problem generates large numbersamples zero weight. SampleSearch circumvents rejection problemsampling backtrack-free search space every assignment (sample) guaranteed non-zero weight. backtrack-free search space constructed flyinterleaving sampling backtracking style search. Namely, sample supposedrejected weight zero, algorithm continues instead systematicbacktracking search, non zero weight sample found. evaluation version,importance distribution SampleSearch constructed output IJGPi-bound 3. information importance distribution constructedoutput IJGP, see work Gogate (2009).evaluation conducted following benchmarks (see footnote 4 details):1. UAI06-MPE - UAI-06, 57 instances, Bayesian networks (40 instances used).2. UAI06-PE - UAI-06, 78 instances, Bayesian networks (58 instances used).316fiJ OIN -G RAPH P ROPAGATION LGORITHMSIJGPEDBPTLSBPEPISSampleSearchWCSPs BN2O Grids Linkage Promedas UAI06-MPE UAI06-PE RelationalTable 8: Scope experimental study.Score vs KL distance1Score vs KL distance0.90.8Score0.70.60.50.40.30.20.100.20.40.60.81KL distanceFigure 27: Score function KL distance.3. Relational Bayesian networks - constructed Primula tool, 251 instances, binary variables, large networks large tree-width, high levels determinism (30 instancesused).4. Linkage networks - 22 instances, tree-width 20-35, Markov networks (5 instances used).5. Grids - 12x12 50x50, 320 instances, treewidth 12-50.6. BN2O networks - Two-layer Noisy-OR Bayesian networks, 18 instances, binary variables,55 variables, treewidth 24-27.7. WCSPs - Weighted CSPs, 97 instances, Markov networks (18 instances used).8. Promedas - real-world medical diagnosis, 238 instances, tree-width 1-60, Markov networks(46 instances used).Table 8 shows scope experimental study. indicates solver ablehandle benchmark type therefore evaluated lack indicates otherwise.measure performance algorithms terms KL-distance based score. Formally,score solver problem instance equal 10avgkld avgkld average KLdistance exact marginal (which computed using UCLA Ace solver, see Chavira& Darwiche, 2008) approximate marginal output solver. solver outputsolution, consider KL-distance . score lies 0 1, 1 indicatingsolver outputs exact solution 0 indicating solver either output solutioninfinite average KL distance. Figure 27 shows score function KL distance.317fiM ATEESCU , K ASK , G OGATE & ECHTERFigures 28-35 report results experiments problem sets.solver timeout 20 minutes problem instance; solving problem, solverperiodically outputs best solution found far. Using this, compute, solver,point time, total sum scores problem instances particular set, calledSumScore(t). horizontal axis, time vertical axis, SumScore(t).higher curve solver is, better (the higher score).summary, see IJGP shows best performance first four classes networks(UAI-MPE, UAI-PE, Relational Linkage), tied algorithms two classes (GridBN2O), surpassed EDBP last two classes (WCSPs Promedas). EPISSampleSearch, importance sampling schemes, often inferior IJGP EDBP.theory, accuracy importance sampling schemes improve time. However,rate improvement often unknown practice. hard benchmarks evaluatedon, found rate quite small therefore improvement cannot discernedFigures. discuss results detail below.mentioned earlier, TLSBP works binary networks (i.e., two variables per function)therefore evaluated WCSPs, Linkage, UAI06-MPE UAI06-PE benchmarks.UAI-MPE UAI-PE instances used UAI 2006 evaluation exact solvers (fordetails see report Bilmes & Dechter, 2006). Exact marginals available 40 UAI-MPEinstances 58 UAI-PE instances. results UAI-MPE UAI-PE instances shownFigures 28 29 respectively. IJGP best performing scheme benchmark setsreaching SumScore close maximum possible value cases 2 minutesCPU time. EDBP SampleSearch second best cases.Relational network instances generated grounding relational Bayesian networks usingPrimula tool (Chavira, Darwiche, & Jaeger, 2006). Exact marginals available 30submitted 251 instances. Figure 30, observe IJGPs SumScore steadilyincreases time reaches value close maximum possible score 3016 minutes CPU time. SampleSearch second best performing scheme. EDBP, TLSBPEPIS perform quite poorly instances reaching SumScore 10, 13 13 respectively20 minutes CPU time.Linkage instances generated converting linkage analysis data Markov networkusing Superlink tool (Fishelson & Geiger, 2003). Exact marginals available 522 instances. results shown Figure 31. one minute CPU time, IJGPsSumScore close 5 remains steady thereafter EDBP reaches SumScore 220 minutes. SampleSearch second best performing scheme EDBP third best.results Grid networks shown Figure 32. sink node grid evidencenode. deterministic ratio p parameter specifying fraction nodes deterministic,is, whose values determined given values parents. evaluation benchmarkset consists 30 instances p = 50%,75% 90% exact marginals available 27instances only. EPIS, IJGP, SampleSearch EDBP close tie network,TLSBP lowest performance. hard see, EPIS slightly best performingscheme, IJGP second best followed SampleSearch EDBP. instances IJGPsSumScore increases steadily time.results BN2O instances appear Figure 33. close tie, casefive algorithms. IJGP minuscule decrease SumScore time 17.85 17.7.Although general improvement accuracy expected IJGP higher i-bound,318fiJ OIN -G RAPH P ROPAGATION LGORITHMSApproximate Mar Problem Set uai06-mpe4035Sum Score30252015105002468101214161820Time minutesSampleSearchIJGPEDBPEPISFigure 28: Results UAI-MPE networks. TLSBP plotted cannot handle UAIMPE benchmarks.Approximate Mar Problem Set uai06-pe50Sum Score40302010002468101214161820Time minutesSampleSearchIJGPEDBPEPISFigure 29: Results UAI-PE networks. TLSBP plotted cannot handle UAI-PEbenchmarks.319fiM ATEESCU , K ASK , G OGATE & ECHTERApproximate Mar Problem Set Relational3530Sum Score2520151050024681012141618201820Time minutesSampleSearchIJGPEDBPTLSBPEPISFigure 30: Results relational networks.Approximate Mar Problem Set Linkage65Sum Score432100246810121416Time minutesSampleSearchIJGPEDBPFigure 31: Results Linkage networks. EPIS TLSBP plotted cannothandle Linkage networks.320fiJ OIN -G RAPH P ROPAGATION LGORITHMSApproximate Mar Problem Set Grids25Sum Score2015105002468101214161820161820Time minutesSampleSearchIJGPEDBPTLSBPEPISFigure 32: Results Grid networks.Approximate Mar Problem Set bn2o1816Sum Score1412108642002468101214Time minutesSampleSearchIJGPEDBPTLSBPEPISFigure 33: Results BN2O networks. solvers except IJGP quickly converge maximumpossible score 18 therefore indistinguishable Figure.321fiM ATEESCU , K ASK , G OGATE & ECHTERApproximate Mar Problem Set WCSPs181614Sum Score12108642002468101214161820Time minutesSampleSearchIJGPEDBPFigure 34: Results WCSPs networks. EPIS TLSBP plotted cannothandle WCSPs.Approximate Mar Problem Set Promedas454035Sum Score30252015105002468101214161820Time minutesSampleSearchIJGPEDBPTLSBPFigure 35: Results Promedas networks. EPIS plotted cannot handle Promedasbenchmarks, Markov networks.322fiJ OIN -G RAPH P ROPAGATION LGORITHMSguaranteed, example happen. solvers reach maximumpossible SumScore 18 (or close it) 6 minutes CPU time.WCSP benchmark set 97 instances. However used 18 instancesexact marginals available. Therefore maximum SumScore algorithm reach18. results shown Figure 34. EDBP reaches SumScore 17 almost 3 minutesCPU time IJGP reaches SumScore 13 3 minutes. SumScoresIJGP EDBP remain unchanged interval 3 20 minutes. lookingraw results, found IJGPs score zero 5 instances 18.singleton consistency component implemented via SAT solver finish 20 minutesinstances. Although singleton consistency step generally helps reduce practical timecomplexity IJGP instances, adversely affects WCSP instances.Promedas instances Noisy-OR binary Bayesian networks (Pearl, 1988). instancescharacterized extreme marginals. Namely, given variable, marginals form(1 , ) small positive constant. Exact marginals available 46submitted 238 instances. structured problems (see Figure 35), see EDBPbest performing scheme reaching SumScore close 46 7 minutes CPU timeTLSBP IJGP able reach SumScore 40 20 minutes.6. Related Worknumerous lines research devoted study belief propagation algorithms,message-passing schemes general. Throughout paper mentioned comparedrelated work, especially experimental evaluation section. give short summarydevelopments belief propagation present related schemes mentionedbefore. additional information see also recent review Koller (2010).decade ago, Iterative Belief Propagation (Pearl, 1988) received lot interestinformation theory coding community. realized two best error-correctingdecoding algorithms actually performing belief propagation networks cycles.LDPC code (low-density parity-check) introduced long time ago Gallager (1963), considered one powerful promising schemes often performs impressively closeShannons limit. Turbo codes (Berrou, Glavieux, & Thitimajshima, 1993) also efficientpractice understood instance belief propagation (McEliece et al., 1998).considerable progress towards understanding behavior performance BP madeconcepts statistical physics. Yedidia et al. (2001) showed IBP strongly relatedBethe-Peierls approximation variational (Gibbs) free energy factor graphs. Betheapproximation particular case general Kikuchi (1951) approximation. GeneralizedBelief Propagation (Yedidia et al., 2005) application Kikuchi approximation worksclusters variables, structures called region graphs. Another algorithm employsregion-based approach Cluster Variation Method (CVM) (Pelizzola, 2005). algorithmsfocus selecting good region-graph structure account over-counting (and over-overcounting, etc.) evidence. view generalized belief propagation broadly beliefpropagation nodes clusters functions. Within view IJGP, GBP definedYedidia et al. (2001), well CVM, special realizations generalized belief propagation.Belief Propagation Partially Ordered Sets (PBP) (McEliece & Yildirim, 2002) also generalized form Belief Propagation minimizes Bethe-Kikuchi variational free energy,323fiM ATEESCU , K ASK , G OGATE & ECHTERworks message-passing algorithm data structures called partially ordered sets,junction graphs factor graphs examples. one-to-one correspondencefixed points PBP stationary points free energy. PBP includes special cases manyvariants belief propagation. noted before, IJGP basically PBP.Expectation Propagation (EP) (Minka, 2001) iterative approximation algorithm computing posterior belief Bayesian networks. combines assumed-density filtering (ADF),extension Kalman filter (used approximate belief states using expectations, meanvariance), IBP, iterates expectations consistent throughout network.TreeEP (Minka & Qi, 2004) deals cyclic problem reducing problem graph tree subgraph approximating remaining edges. relationship EP GBP discussedWelling, Minka, Teh (2005).Survey Propagation (SP) (Braunstein et al., 2005) solves hard satisfiable (SAT) problems usingmessage-passing algorithm factor graph consisting variable clause nodes. SP inspiredalgorithm called Warning Propagation (WP) BP. WP determine tree-problemSAT, provide solution. BP compute number satisfying assignmentstree-problem, well fraction assignments variable true. twoalgorithms used heuristics define SP algorithm, shown efficienteither arbitrary networks. SP still heuristic algorithm guaranteeconvergence. SP inspired new concept cavity method statistical physics,interpreted BP variables take values true false, also extradont care value. detailed treatment see book Mezard Montanari (2009).7. Conclusionpaper investigated family approximation algorithms Bayesian networks,could also extended general graphical models. started bounded inference algorithmsproposed Mini-Clustering (MC) scheme generalization Mini-Buckets arbitrary treedecompositions. power lies anytime algorithm governed user adjustable i-boundparameter. MC start small i-bound keep increasing long given time,accuracy usually improves time. enough time given it, guaranteedbecome exact. One virtues also produce upper lower bounds, routeexplored paper.Inspired success iterative belief propagation (IBP), extended MC iterativemessage-passing algorithm called Iterative Join-Graph Propagation (IJGP). IJGP operates general join-graphs contain cycles, sill governed i-bound parameter. Unlike IBP,IJGP guaranteed become exact given enough time.also make connections well understood consistency enforcing algorithms constraintsatisfaction, giving strong support iterating messages, giving insight performanceIJGP (IBP). show that: (1) value variable assessed zero-beliefiteration IJGP, remains zero-belief subsequent iterations; (2) IJGP convergesfinite number iterations relative set zero-beliefs; and, importantly (3) setzero-beliefs decided iterative belief propagation methods sound. Namelyzero-belief determined IJGP corresponds true zero conditional probability relativegiven probability distribution expressed Bayesian network.324fiJ OIN -G RAPH P ROPAGATION LGORITHMSexperimental evaluation IJGP, IBP MC provided, IJGP emerges onepowerful approximate algorithms belief updating Bayesian networks.ReferencesArnborg, S. A. (1985). Efficient algorithms combinatorial problems graphs boundeddecomposability - survey. BIT, 25, 223.Bacchus, F., Dalmao, S., & Pitassi, T. (2003). Value elimination: Bayesian inference via backtracking search. Proceedings Nineteenth Conference Uncertainty ArtificialIntelligence (UAI03), pp. 2028.Berrou, C., Glavieux, A., & Thitimajshima, P. (1993). Near Shannon limit error-correcting coding:Turbo codes. Proceedings 1993 International Conference Communications, pp.10641070.Bilmes, J., & Dechter, R. (2006). Evaluation probabilistic inference systems UAI06.http://ssli.ee.washington.edu/ bilmes/uai06InferenceEvaluation/.Braunstein, A., Mezard, M., & Zecchina, R. (2005). Survey propagation: algorithm satisfiability. Random Struct. Algorithms, 27(2), 201226.Chavira, M., & Darwiche, A. (2008). probabilistic inference weighted model counting.Artificial Intelligence, 172(67), 772799.Chavira, M. D., Darwiche, A., & Jaeger, M. (2006). Compiling relational bayesian networksexact inference. International Journal Approximate Reasoning, 42(1-2), 420.Chertkov, M., & Chernyak, V. Y. (2006). Loop series discrete statistical models graphs.Journal Statistical Mechanics: Theory Experiment, P6009.Choi, A., Chavira, M., & Darwiche, A. (2007). Node splitting: scheme generating upperbounds bayesian networks. Proceedings Twenty Third Conference UncertaintyArtificial Intelligence (UAI07), pp. 5766.Choi, A., & Darwiche, A. (2006a). edge deletion semantics belief propagationpractical impact approximation quality. Proceedings Twenty-First NationalConference Artificial Intelligence (AAAI06), pp. 11071114.Choi, A., & Darwiche, A. (2006b). variational approach approximating bayesian networksedge deletion. Proceedings Twenty Second Conference Uncertainty ArtificialIntelligence (UAI06), pp. 8089.Cooper, G. F. (1990). computational complexity probabistic inferences. Artificial Intelligence, 42, 393405.Dagum, P., & Luby, M. (1993). Approximating probabilistic inference bayesian belief networksNP-hard. Artificial Intelligence, 60(1), 141153.Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 125(1-2), 541.325fiM ATEESCU , K ASK , G OGATE & ECHTERDechter, R. (1992). Constraint networks. Encyclopedia Artificial Intelligence, 276285.Dechter, R. (1996). Bucket elimination: unifying framework probabilistic inference algorithms. Proceedings Twelfth Conference Uncertainty Artificial Intelligence(UAI96), pp. 211219.Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial Intelligence,113, 4185.Dechter, R. (2003). Constraint Processing. Morgan Kaufmann Publishers.Dechter, R., Bidyuk, B., Mateescu, R., & Rollon, E. (2010). power belief propagation:constraint propagation perspective. Dechter, R., Geffner, H., & Halpern, J. (Eds.), Heuristics, Probabilities Causality: Tribute Judea Pearl.Dechter, R., Kask, K., & Larrosa, J. (2001). general scheme multiple lower bound computation constraint optimization. Proceedings Seventh International ConferencePrinciples Practice Constraint Programming (CP01), pp. 346360.Dechter, R., Kask, K., & Mateescu, R. (2002). Iterative join-graph propagation. ProceedingsEighteenth Conference Uncertainty Artificial Intelligence (UAI02), pp. 128136.Dechter, R., & Mateescu, R. (2003). simple insight iterative belief propagations success.Proceedings Nineteenth Conference Uncertainty Artificial Intelligence (UAI03),pp. 175183.Dechter, R., & Mateescu, R. (2007). AND/OR search spaces graphical models. Artificial Intelligence, 171(2-3), 73106.Dechter, R., & Pearl, J. (1987). Network-based heuristics constraint satisfaction problems.Artificial Intelligence, 34, 138.Dechter, R., & Pearl, J. (1989). Tree clustering constraint networks. Artificial Intelligence, 38,353366.Dechter, R., & Rish, I. (1997). scheme approximating probabilistic inference. ProceedingsThirteenth Conference Uncertainty Artificial Intelligence (UAI97), pp. 132141.Dechter, R., & Rish, I. (2003). Mini-buckets: general scheme approximating inference.Journal ACM, 50(2), 107153.Fishelson, M., & Geiger, D. (2003). Optimizing exact genetic linkage computations. ProceedingsSeventh Annual International Conference Computational Biology (RECOMB03),pp. 114121.Gallager, R. G. (1963). Low-Density Parity-Check Codes. MIT Press, Cambridge, MA.Gogate, V., & Dechter, R. (2007). SampleSearch: scheme searches consistent samples.Proceedings Eleventh International Conference Artificial Intelligence Statistics(AISTATS07), pp. 147154.326fiJ OIN -G RAPH P ROPAGATION LGORITHMSGogate, V. (2009). Sampling Algorithms Probabilistic Graphical models Determinism.Ph.D. thesis, School Information Computer Sciences, University California, Irvine.Gomez, V., Mooji, J. M., & Kappen, H. J. (2007). Truncating loop series expansion beliefpropagation. Journal Machine Learning, 8, 19872016.Gottlob, G., Leone, N., & Scarcello, F. (2000). comparison structural CSP decompositionmethods. Artificial Intelligence, 243282.Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. (1990). Bayesian updating causal probabilisticnetworks local computation. Computational Statistics Quarterly, 4, 269282.Kask, K. (2001). Approximation algorithms graphical models. Ph.D. thesis, InformationComputer Science, University California, Irvine.Kask, K., & Dechter, R. (2001). general scheme automatic search heuristics specificationdependencies. Artificial Intelligence, 129(1-2), 91131.Kask, K., Dechter, R., Larrosa, J., & Dechter, A. (2005). Unifying cluster-tree decompositionsreasoning graphical models. Artificial Intelligence, 166 (1-2), 165193.Kikuchi, R. (1951). theory cooperative phenomena. Phys. Rev., 81(6), 9881003.Koller, D. (2010). Belief propagation loopy graphs. Dechter, R., Geffner, H., & Halpern, J.(Eds.), Heuristics, Probabilities Causality: Tribute Judea Pearl.Larrosa, J., Kask, K., & Dechter, R. (2001). mini-bucket: scheme approximatingcombinatorial optimization tasks. Tech. rep., University California Irvine.Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computation probabilities graphicalstructures application expert systems. Journal Royal Statistical Society,Series B, 50(2), 157224.Mackworth, A. K. (1977). Consistency networks relations. Artificial Intelligence, 8(1), 99118.Mezard, M., & Montanari, A. (2009). Information, Physics Computation. Oxford UniversityPress.Mezard, M., Parisi, G., & Zecchina, R. (2002). Analytic algorithmic solution random satisfiability problems. Science, 297, 812815.Maier, D. (1983). Theory Relational Databases. Computer Science Press, Rockville, MD.Mateescu, R., Dechter, R., & Kask, K. (2002). Tree approximation belief updating. Proceedings Eighteenth National Conference Artificial Intelligence (AAAI02), pp. 553559.McEliece, R. J., MacKay, D. J. C., & Cheng, J. F. (1998). Turbo decoding instance Pearlsbelief propagation algorithm. IEEE J. Selected Areas Communication, 16(2), 140152.McEliece, R. J., & Yildirim, M. (2002). Belief propagation partially ordered sets. Mathematical Systems Theory Biology, Communications, Computation, Finance, pp. 275300.327fiM ATEESCU , K ASK , G OGATE & ECHTERMinka, T. (2001). Expectation propagation approximate bayesian inference. ProceedingsSeventeenth Annual Conference Uncertainty Artificial Intelligence (UAI01), pp.362369.Minka, T., & Qi, Y. (2004). Tree-structured approximations expectation propagation. Advances Neural Information Processing Systems 16 (NIPS03).Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.Pelizzola, A. (2005). Cluster variation method statistical physics probabilistic graphicalmodels. Journal Physics A: Mathematical General, 38(33), R309R339.Rollon, E., & Dechter, R. (2010). Evaluating partition strategies mini-bucket elimination.Eleventh International Symposium Artificial Intelligence Mathematics (ISAIM10).Roth, D. (1996). hardness approximate reasoning. Artificial Intelligence, 82(1-2), 273302.Shafer, G. R., & Shenoy, P. P. (1990). Probability propagation. Annals Mathematics ArtificialIntelligence, 2, 327352.Shenoy, P. P. (1992). Valuation-based systems bayesian decision analysis. Operations Research,40, 463484.Welling, M., Minka, T. P., & Teh, Y. W. (2005). Structured region graphs: Morphing ep gbp.Proceedings Twenty First Conference Uncertainty Artificial Intelligence (UAI05),pp. 609614.Wexler, Y., & Meek, C. (2008). MAS: multiplicative approximation scheme probabilistic inference. Proceedings Advances Neural Information Processing Systems 21 (NIPS08),pp. 17611768.Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2000). Generalized belief propagation. Tech. rep.TR2000-26, Mitsubishi Electric Research Laboratories.Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2001). Generalized belief propagation. AdvancesNeural Information Processing Systems 13 (NIPS00), pp. 689695.Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2005). Constructing free energy approximationsgeneralized belief propagation algorithms. IEEE Transactions Information Theory, 51,22822312.Yuan, C., & Druzdzel, M. J. (2003). importance sampling algorithm based evidence prepropagation. Proceedings Nineteenth Conference Uncertainty Artificial Intelligence (UAI03), pp. 624631.Zhang, N. L., Qi, R., & Poole, D. (1994). computational theory decision networks. International Journal Approximate Reasoning, 11, 83158.328fiJournal Artificial Intelligence Research 37 (2010) 99-139Submitted 08/09; published 02/10Interactive Cost Configuration Decision DiagramsHenrik Reif Andersenhra@configit.comConfigit A/SDK-2100 Copenhagen, DenmarkTarik Hadzict.hadzic@4c.ucc.ieCork Constraint Computation CentreUniversity College CorkCork, IrelandDavid Pisingerpisinger@man.dtu.dkDTU ManagementTechnical University DenmarkDK-2800 Kgs. Lyngby, DenmarkAbstractmany AI domains product configuration, user interactively specifysolution must satisfy set constraints. scenarios, offline compilationfeasible solutions tractable representation important approach delivering efficient backtrack-free user interaction online. particular, binary decision diagrams(BDDs) successfully used compilation target product service configuration. paper discuss extend BDD-based configuration scenariosinvolving cost functions express user preferences.first show efficient, robust easy implement extension possiblecost function additive, feasible solutions represented using multi-valued decision diagrams (MDDs). also discuss effect MDD size cost functionnon-additive encoded explicitly MDD. discuss interactive configuration presence multiple cost functions. prove even simplest form,multiple-cost configuration NP-hard input MDD. However, solving two-costconfiguration develop pseudo-polynomial scheme fully polynomial approximation scheme. applicability approach demonstrated experimentsreal-world configuration models product-catalogue datasets. Response times generally within fraction second even large instances.1. IntroductionInteractively specifying solution must satisfy number combinatorial restrictionsimportant problem many AI domains related decision making: buyingproduct online, selling insurance policy setting piece equipment. Solutionsoften modeled assignments variables constraints imposed.assigning variables without sufficient guidance, user might forced backtrack, sincechoices made cannot extended way would satisfysucceeding constraints. improve usability interaction therefore importantindicate user values participate least one remaining solution.c2010AI Access Foundation. rights reserved.fiAndersen, Hadzic, & Pisingeruser assigning values guaranteed able reach feasible solutionnever forced backtrack. refer task computing valuescalculating valid domains (CVD). Since computationally challenging (NP-hard)problem, short execution times important interactive setting,suggested compile offline (prior user interaction) set feasible solutionsrepresentation form supports efficient execution CVD online interaction.Mller, Andersen, Hulgaard (2002) Hadzic, Subbarayan, Jensen, Andersen,Mller, Hulgaard (2004) investigated approach using binary decision diagrams (BDDs) compilation target. BDDs one data-structures investigatedknowledge compilation community preprocess original problem formulationstractable representations enhance solving subsequent tasks. CVD onetasks occurring configuration domain. Knowledge compilation successfully applied number areas planning, diagnosis, model checking etc.Beside BDDs, number structures, various sublanguages negation normal forms (NNFs) (Darwiche & Marquis, 2002), AND/OR diagrams (Mateescu, Dechter, &Marinescu, 2008), finite state automata (Vempaty, 1992; Amilhastre, Fargier, & Marquis,2002) various extensions decision diagrams (Drechsler, 2001; Wegener, 2000; Meinel& Theobald, 1998) used compilation targets. suitable interactive configuration well. particular, Vempaty (1992) suggested compiling constraintsautomaton. However, BDDs investigated data structures toolsupport unrivaled emerging representations. many highly optimizedopen-source BDD packages (e.g., Somenzi, 1996; Lind-Nielsen, 2001) allow easyefficient manipulation BDDs. contrast, publicly available, open-source compilersstill developed many newer representations. particular, application BDDsconfiguration resulted patent approval (Lichtenberg, Andersen, Hulgaard, Mller, &Rasmussen, 2001) establishment spinoff company Configit A/S1 .work paper motivated decision making scenarios solutionsassociated cost function, expressing implicitly properties price, quality, failure probability etc. user might prefer one solution another given valueproperties. natural way user expresses cost preferences configurationsetting bound minimal maximal cost solution willing accept.therefore study problem calculating weighted valid domains (wCVD),eliminate values every valid solution expensive user-providedmaximal cost. present configurator supports efficient cost bounding wideclass additive cost functions. approach easily implementable scales wellinstances previously compiled BDDs standard interactive configuration. cornerstone approach reuse robust compilation constraintsBDD, extract corresponding multi-valued decision diagram (MDD).resulting MDD allows us label edges weights utilize efficient shortest path algorithms label nodes filter expensive values MDD edges. MDD extractiontechnique novel, labeling edges decision diagram suggested works well.generic interpretation (Wilson, 2005), edges decision diagram labeledelements semiring support algebraic computations relevant probabilistic rea1. http://www.configit.com100fiInteractive Cost Configuration Decision Diagramssoning, optimization etc. Amilhastre et al. (2002) suggest labeling edges automatonreason abut optimal restorations explanations. general, many knowledge compilationstructures weighted counterparts, many captured frameworkvalued negation normal forms (VNNFs) (Fargier & Marquis, 2007). structuresutilized probabilistic reasoning, diagnosis, tasks involving reasoningreal-valued rather Boolean functions. principle used wCVDqueries, public tool support weighted variants less available tailoredtasks outside configuration domain.extend approach support valid domains computation presencemultiple cost functions. user often multiple conflicting objectives,satisfied simultaneously. Traditional approaches multi-criteria optimization (Figueira,Greco, & Ehrgott, 2005; Ehrgott & Gandibleux, 2000) typically interact userway unsuitable configuration setting cost functions combined singleobjective interaction step non-dominated solutions sampled displayed user. Based user selections adequate aggregation costs performednext interaction step. suggest configuration-oriented interaction approach domains bounded respect multiple costs. proveparticularly challenging problem. Computing valid domains MDD presencetwo cost functions (2-wCVD) NP-hard, even simplest extension linear inequalitiespositive coefficients Boolean variables. Despite negative result, provideimplementation 2-wCVD queries pseudo-polynomial time space developfully polynomial time approximation scheme (FPTAS). prove pseudo-polynomialalgorithm hence fully polynomial approximation scheme exists computing domains presence arbitrarily many cost functions since NP-hard problemstrong sense. Finally, demonstrate experimental evaluation applicability wCVD 2-wCVD query large real-world configuration modelsproduct-catalogue datasets. best knowledge, present first interactive configurator supporting configuration wrt. cost restrictions backtrack-freecomplete manner. constitutes novel addition existing product-configurationapproaches well approaches within multi-criteria decision making (Figueira et al.,2005).remainder paper organized follows. Section 2 describe backgroundwork notation. Section 3 describe approach implementing wCVD queryMDD Section 4 show compile MDD. Section 5 discussconfiguring presence multiple costs. Section 6 present empirical evaluationapproach. Section 7 describe related work finally conclude Section 8.2. Preliminariesbriefly review important concepts background.2.1 Constraint Satisfaction ProblemsConstraint satisfaction problems (CSPs) form framework modeling solving combinatorial problems, solution problem formulated assignment101fiAndersen, Hadzic, & Pisingervariables satisfy certain constraints. standard form, CSP involves finitenumber variables, defined finite domains.Definition 1 (CSP) constraint satisfaction problem (CSP) triple (X, D, F )X set variables {x1 , . . . , xn }, = D1 . . .Dn Cartesian product finitedomains D1 , . . . , Dn F = {f1 , ..., fm } set constraints defined variables X.constraint f function defined subset variables Xf X called scopef . maps assignment Xf variables {0, 1} 1 indicates fsatisfied 0 indicates f violated assignment. solution assignmentvariables X satisfies constraints simultaneously.Formally, assignment values a1 , . . . , variables x1 , . . . , xn denoted setpairs = {(x1 , a1 ), . . . , (xn , )}. domain assignment dom() setvariables assigned: dom() = {xi | Di .(xi , a) } variablesassigned, i.e. dom() = X, refer total assignment. say totalassignment valid satisfies rules, denoted |= F . partialassignment , dom() X valid extended total assignment 0valid 0 |= F . define solution space Sol set valid total assignments,i.e. Sol = { | |= F, dom() = X}.2.2 Interactive ConfigurationInteractive configuration important application domain user assistedspecifying valid configuration (of product, service something else) interactivelyproviding feedback valid options unspecified attributes. problem arisesnumber domains. example, buying product, user specify numberproduct attributes. attribute combinations might feasible guidanceprovided, user might reach dead-end interacting system.forced backtrack, might seriously decrease user satisfaction.many cases, valid configurations implicitly described specifying restrictionscombining product attributes. use CSP model represent restrictions,CSP solution corresponds valid configuration. configurable attributerepresented variable, attribute option corresponds valuevariable domain. Example 1 illustrate simple configuration problem CSPmodel.Example 1 specify T-shirt choose color (black, white, red, blue),size (small, medium, large) print (Men Black - MIB SaveWhales - STW). choose MIB print color black chosen well,choose small size STW print (including large picture whale)cannot selected picture whale fit small shirt. configurationproblem (X, D, F ) T-shirt example consists variables X = {x1 , x2 , x3 } representingcolor, size print. Variable domains D1 = {0, 1, 2, 3} (black , white, red , blue), D2 ={0, 1, 2} (small , medium, large), D3 = {0, 1} (MIB , STW ). two rules translateF = {f1 , f2 }, f1 x3 = 0 x1 = 0 (MIB black ) f2 (x2 = 0 x3 6= 1)(small STW ). |D1 ||D2 ||D3 | = 24 possible assignments. Elevenassignments valid configurations form solution space shown Fig. 1.102fiInteractive Cost Configuration Decision Diagrams(black , small , MIB )(black , medium, MIB )(black , medium, STW )(black , large, MIB )(black , large, STW )(white, medium, STW )(white, large, STW )(red , medium, STW )(red , large, STW )(blue, medium, STW )(blue, large, STW )Figure 1: Solution space T-shirt example.fundamental task concerned paper calculating validdomains (CVD) query. partial assignment representing previously made user assignments, configurator calculates displays valid domain VD [] Diunassigned variable xi X \ dom(). domain valid containsvalues extended total valid assignment 0 . example,user selects small T-shirt (x2 = 0), valid domains restricted MIB printV D3 = {0} black color V D1 = {0}.Definition 2 (CVD) Given CSP model (X, D, F ), given partial assignment compute valid domains:VDi [] = {a Di | 0 .(0 |= F {(xi , a)} 0 )}task main interest since delivers important interaction requirements: backtrackfreeness (user never forced backtrack) completeness (all valid configurationsreachable) (Hadzic et al., 2004). queries relevant supportinguser interaction explanations restorations failure, recommendationsrelevant products, etc., CVD essential operation mode interactionprimary importance paper.2.3 Decision DiagramsDecision diagrams form family rooted directed acyclic graphs (DAGs) nodeu labeled variable xi outgoing edges e labeled value Di .node may one outgoing edge label. decision diagramcontains one terminal nodes, labeled constant outgoingedges. well known member family binary decision diagrams (BDDs)(Bryant, 1986) used manipulating Boolean functions many areas,verification, model checking, VLSI design (Meinel & Theobald, 1998; Wegener, 2000;Drechsler, 2001) etc. paper primarily operate following variantmulti-valued decision diagrams:Definition 3 (MDD) MDD denoted rooted directed acyclic graph (V, E),V set vertices containing special terminal vertex 1 root r V . Further,var : V {1, . . . , n + 1} labeling nodes variable index var(1) =n + 1. edge e E denoted triple (u, u0 , a) start node u, end node u0associated value a.work ordered MDDs. total ordering < variables assumededges (u, u0 , a), var(u) < var(u0 ). convenience assume variables103fiAndersen, Hadzic, & PisingerX ordered according indices. Ordered MDDs consideredarranged n layers vertices, layer labeled variable index.denote Vi set nodes labeled xi , Vi = {u V | var(u) = i}. Similarly,denote Ei set edges originating Vi , i.e. Ei = {e(u, u0 , a) E |var(u) = i}. Unless otherwise specified, assume path rootterminal, every variable labels exactly one node.MDD encodes CSP solution set Sol D1 . . . Dn , defined variables{x1 , . . . , xn }. check whether assignment = (a1 , . . . , ) D1 . . . Dn Soltraverse root, every node u labeled variable xi , follow edgelabeled ai . edge solution, i.e., 6 Sol. Otherwise,traversal eventually ends terminal 1 Sol. denote p : u1u2path MDD u1 u2 . Also, edges u u0 sometimes denotede : u u0 . value edge e(u, u0 , a) sometimes denoted v(e).make distinction paths assignments. Hence, set solutions representedMDD Sol = {p | p : r1}. fact, every node u Vi associatedsubset solutions Sol(u) = {p | p : u1} Di . . . Dn .x101x20x31x3021x30x31 0111x1230 1x2x221x3x3x3x31111x2211x22x20 2 1x3x32 31 2x30 0 1x311(a) MDD merging.(b) merged MDD.Figure 2: uncompressed merged MDD T-Shirt example.Decision diagrams exponentially smaller size solution setencode merging isomorphic subgraphs. Two nodes u1 , u2 isomorphic encodesolution set Sol(u1 ) = Sol(u2 ). Figure 2 show fully expanded MDD 2(a)equivalent merged MDD 2(b) T-shirt solution space. addition mergingisomorphic subgraphs, another compression rule usually utilized: removing redundantnodes. node u Vi redundant Di outgoing edges, pointingnode u0 . nodes eliminated redirecting incoming edges u u0 deleting uV . introduces long edges skip layers. edge e(u, u0 , a) long var(u)+1 <var(u0 ). case, e encodes set solutions: {a} Dvar(u)+1 . . . Dvar(u0 )1 .refer MDD merging isomorphic nodes removal redundantnodes taken place reduced MDD, constitutes multi-valued generalizationBDDs typically reduced ordered. reduced MDD T-shirt CSPshown Figure 3. paper, unless emphasized otherwise, MDD alwaysassume ordered merged reduced MDD, since exposition simpler, removalredundant nodes linear effect size. Given variable ordering104fiInteractive Cost Configuration Decision Diagramsunique merged MDD given CSP (X, D, F ) solution set Sol.size MDD depends critically ordering, could vary exponentially. growexponentially number variables, practice, many interesting problemssize surprisingly small.x10 1x22 3x20x3122 10x311Figure 3: reduced MDD T-shirt example.Interactive Configuration Decision Diagrams. particularly attractive property decision diagrams support efficient execution number importantqueries, checking consistency, validity, equivalence, counting, optimization etc.utilized number application domains problem descriptionknown offline (diagnosis, verification,etc.). particular, calculating valid domains linearsize MDD. Since calculating valid domains NP-hard problem sizeinput CSP model, possible guarantee interactive response real-time.fact, unacceptably long worst-case response times empirically observedpurely search-based approach computing valid domains (Subbarayan et al., 2004).Therefore, compiling CSP solutions off-line (prior user interaction) decisiondiagram, efficiently (in size MDD) compute valid domains onlineinteraction user. important note order user decidesvariables completely unconstrained, i.e. depend ordering MDD variables. previous work utilized Binary Decision Diagrams (BDDs) representvalid configurations CVD queries executed efficiently (Hadzic et al., 2004).course, BDDs might exponentially large input CSP, many classesconstraints surprisingly compact.3. Interactive Cost Processing MDDsmain motivation work extending interactive configuration approachMller et al. (2002), Hadzic et al. (2004), Subbarayan et al. (2004) situationsaddition CSP model (X, D, F ) involving hard constraints, also costfunction:c : D1 . . . Dn R.product configuration setting, could product price. uncertainty setting,cost function might indicate probability occurrence event represented105fiAndersen, Hadzic, & Pisingersolution (failure hardware component, withdrawal bid auction etc.).decision support context, cost function might indicate user preferences.number cost-related queries user might interested, e.g. finding optimalsolution, computing probable explanation. We, however, assume userinterested tight control variable values well cost selected solutions.example, user might desire specific option xi = a, would also carewould assignment affect cost remaining optimal solutions.communicate information user, allow strike right balancecost variable values allowing interactively limit maximal costproduct addition assigning variable values. Therefore, paper primarilyconcerned implementing weighted CVD (wCVD) query: user-specified maximumcost K, indicate values unassigned variable domains extendedtotal assignment valid costs less K. on, assumeuser interested bounding maximal cost (limiting minimal cost symmetric).Definition 4 (wCVD) Given CSP model (X, D, F ), cost function c : Rmaximal cost K, given partial assignment weighted CVD (wCVD) query requirescomputation valid domains:VDi [, K] = {a Di | 0 .(0 |= F {(xi , a)} 0 c(0 ) K)}section assume MDD representation CSP solutions alreadygenerated offline compilation step. postpone discussion MDD compilationSection 4 discuss delivering efficient online interaction top MDD.first discuss practicability implementing wCVD queries explicit encodingcosts MDD. provide practical efficient approach implementingwCVD MDD cost function additive. Finally, discussextensions handling expressive cost functions.3.1 Handling Costs Explicitlyimmediate approach interactively handling cost function treat costsolution attribute, i.e. add variable variables X add constraint= c(x1 , . . . , xn )(1)formulas F enforce equal total cost. resulting configuration modelcompiled MDD 0 user able bound cost restricting domainy.Assuming variable ordering x1 < . . . < xn original CSP model (X, D, F ),assuming inserted cost variable i-th position, new variable set X 0variable ordering x01 < . . . < x0n+1 s.t. x01 = x1 , . . . , x0i1 = xi1 , x0i = x0i+1 =xi , . . . , x0n+1 = xn . domain Di0 variable x0i set feasible costs C(Sol) ={c(s) | Sol}. demonstrate MDD 0 may exponentially larger.Lemma 1 |Ei0 | |C(Sol)|.106fiInteractive Cost Configuration Decision DiagramsProof 1 i-th layer MDD 0 corresponding variable y, cost c C(Sol)must least one path p : r1 c(p) = c, path, edge e Ei0i-th layer must labeled v(e) = c. Hence, cost must leastone edge Ei0 . proves lemma.0Furthermore, least one layers nodes Vi0 , Vi+1number nodes greaterp 0|Ei |. follows following lemma:0 | |E 0 |.Lemma 2 i-th layer MDD 0 , |Vi0 | |Vi+10 | pairs nodes (u , u ) V 0 V 0 , statementProof 2 Since |Vi0 ||Vi+11 2i+1follows fact pair (u1 , u2 ) one edge e : u1 u2 .Namely, every solution p3 formed concatenating paths p1 : ru1 p2 : u21unique cost c(p3 ). However, two edges e1 , e2 : u1 u2 , woulddifferent values v(e1 ) 6= v(e2 ). then, solution c(p3 ) would correspondtwo different costs v(e1 ), v(e2 ).considerations see whenever range possible costs C(Sol)exponential, resulting MDD 0 would exponentially large well. wouldresult significantly increased size |V 0 |/|V |, particularly large numberisomorphic nodes would become non-isomorphic variable introduced(since root paths different costs). extreme instance behaviorpresented Example 2. Furthermore, even C(Sol) large, could ordersmagnitude increase size 0 due breaking isomorphic nodes MDDempirically demonstrated Section 6, Table 3, number configurationinstances. major disadvantage otherwise efficient CVD algorithms becomeunusable since operate significantly larger structure.Example 2 Consider model C(X, D, F ) constraints F = {}, Boolean variables Dj = {0, 1}, j = 1 . . . , n. solution space includes assignments Sol = D1. . . Dn corresponding MDD (V, E) one vertex twolayer,Pnedgesj1xj ,|V | = n + 1, |E| = 2 n. use cost function: c(x1 , . . . , xn ) = j=1 2exponential number feasible costs C(Sol) = {0, . . . , 2n 1}. Hence, |Ei0 | 2n0 | greateri-th layer corresponding variable y, least one layers |Vi0 |, |Vi+1n/22n = 2 .However, significant node isomorphism , adding variablenecessarily lead significant increase size. extreme instance MDDisomorphic nodes, example every edge labeled unique value.MDD, number non-terminal nodes n |Sol|. adding cost variabley, resulting MDD would add one node per path, leading MDD(n + 1) |Sol| nodes. translates minor increase size: |V 0 |/|V | = (n + 1)/n.property empirically demonstrated Section 6, Table 3, product-cataloguedatasets. remainder paper develop techniques tailored instanceslarge increase size occurs. avoid explicit cost encoding aim exploitstructure cost function implement wCVD.107fiAndersen, Hadzic, & Pisinger3.2 Processing Additive Cost FunctionsOne main contributions paper practical efficient approach deliverwCVD queries cost function additive. additive cost function formc(x1 , . . . , xn ) =nXci (xi )i=1cost ci (ai ) R assigned every variable xi every value domainai Di .Additive functions one important frequently used modeling constructs. number important combinatorial problems modeled integer linear programs often constraints objective function linear, i.e. representspecial cases additive cost functions. multi-attribute utility theory user preferencescertain assumptions aggregated single additive function weightedsummation utilities individual attributes. product configuration context, manyproperties additive memory capacity computer total weight.particular, based experience commercially applying configuration technology,price product often modeled (weighted) sum prices individual parts.3.2.1 Labeling ApproachAssuming given MDD representation solution space Sol costfunction c, approach answering wCVD queries based three steps: 1) restrictingMDD wrt. latest user assignment, 2) labeling remaining nodes executing shortestpath algorithms 3) filtering expensive values using node labels.Restricting MDD. given user assignment xi = ai , xiunassigned variables, regardless position MDD variable ordering. initializeMDD pruning removing edges e(u, u0 , a), agreement latestassignment, i.e. var(u) = 6= ai . might cause number edgesnodes become unreachable terminal root removed last edgeset children edges Ch(u) parent edges P (u0 ). unreachable edge mustremoved well. pruning repeated fixpoint reached, i.e.nodes edges removed. Algorithm 1 implements scheme O(|V | + |E|) timespace using queue Q maintain set edges yet removed.Note unassigning user assignment xi = ai easily implemented lineartime well. suffices restore copy initial MDD , perform restriction wrt.partial assignment \ {(xi , ai )} current assignment. Algorithm 1 easilyextended purpose initializing edge removal list Q edges incompatiblewrt. assignments .Computing Node Labels. Remaining edges e(u, u0 , a) layer Ei implicitlylabeled c(e) = ci (a). second step compute MDD node u Vupstream cost shortest path root r u, denoted U [u], downstreamcost shortest path u terminal 1, denoted D[u]:(())XXU [u] = minc(e)(2)c(e) , D[u] = minp:rup:uep1081epfiInteractive Cost Configuration Decision DiagramsAlgorithm 1: Restrict MDD.Data: MDD (V, E), variable xi , value aiforeach e Ei , v(e) 6= aiQ.push(e);Q 6=e(u, u0 , a) Q.pop();delete e ;Ch(u) =foreach e : u00 uQ.push(e);P (u0 ) =foreach e : u0 u00Q.push(e);Algorithm 2 computes U [u] D[u] labels (|V | + |E|) time space.Algorithm 2: Update U, labels.Data: MDD (V, E), Cost function cD[] = , D[1] = 0;foreach = n, . . . , 1foreach u Viforeach e : u u0D[u] = min{D[u], c(e) + D[u0 ]}U [] = , U [r] = 0;foreach = 1, . . . , nforeach u Viforeach e : u u0U [u0 ] = min{U [u0 ], c(e) + U [u]}Computing Valid Domains. upstream downstream costs U, computed, efficiently compute valid domains VDi wrt. maximal cost bound Ksince:VDi [K] = {v(e) | U [u] + c(e) + D[u0 ] K, e : u u0 , u Vi }(3)achieved linear-time traversal (|V | + |E|) shown Algorithm 3.Algorithm 3: Compute valid domains.Data: MDD (V, E), Cost function c, Maximal cost Kforeach = 1, . . . , nV Di = ;foreach u Viforeach e : u u0U [u] + c[e] + D[u0 ] KV Di V Di {v(e)};Hence overall interaction follows. Given current partial assignment , MDDrestricted wrt. Algorithm 1. Labels U, computed Algorithm 2valid domains computed using Algorithm 3. execution algorithms109fiAndersen, Hadzic, & Pisingerrequires (|V | + |E|) time space. Hence, MDD representation solutionspace available, interactively enforce additive cost restrictions linear timespace.3.3 Processing Additive Costs Long Edgesscheme extended MDDs containing long edges. multivalued CSPmodels large domains space savings due long edges might significant,binary models binary decision diagrams (BDDs) significant savings possible.Furthermore, similar fashion, scheme might adopted versionsdecision diagrams contain long edges (with different semantics) zero-suppressedBDDs long edge implies skipped variables assigned 0.Recall reduced MDDs, redundant nodes u Vi Di outgoing edges,pointing node u0 , eliminated. edge e(u, u0 , a) var(u) =k var(u0 ) = l long k + 1 < l, case, e encodes set solutions:{a} Dk+1 . . . Dl1 . labeling edges generalized accommodateedges well. Let domains Dj0 , j = 1, . . . , n represent variable domains updated wrt.current assignment, i.e. Dj0 = Dj xj unassigned, Dj0 = {[xj ]} otherwise. edgee(u, u0 , a), (var(u) = k, var(u0 ) = l) removed 6 Dk0 analogous way MDDpruning previous subsection. Otherwise, labeledc(e) = ck (a) +l1Xj=k+1min cj (a0 )a0 Dj0(4)cost cheapest assignment xk , . . . , xl1 consistent edgepartial assignment . edges labeled, upstream downstream costsU, computed (|V | + |E|) time, manner previous subsection.However, computing valid domains extended. before, sufficient conditionVD existence edge e : u u0 , originating i-th layer u Viv(e) =U [u] + c[e] + D[u0 ] K.(5)However, longer necessary condition, even edge satisfying (5),could exist long edge skipping i-th layer still allows VD . therefore,layer i, compute cost cheapest path skipping layer:P [i] = min{U [u] + c(e) + D[u0 ] | e : u u0 E, var(u) < < var(u0 )}(6)edge skipping i-th layer, set P [i] = . Let cmin [i] denote cheapestvalue Di0 , i.e. cmin [i] = minaDi0 ci (a). determine long edge allowingVD , unassigned variable xi , following must hold:P [i] + ci (a) cmin [i] K(7)Finally, sufficient necessary condition VD one conditions (5)(7) holds. variable xi assigned value drawn valid domain previousstep, guaranteed V Di = {[xi ]} calculations necessary. Labels P [i]110fiInteractive Cost Configuration Decision DiagramsAlgorithm 4: Update P labels.Data: MDD (V, E), Cost function cP [] = ;foreach = 1, . . . , nforeach u Viforeach e : u u0foreach j {var(u) + 1, . . . , var(u0 ) 1}P [j] = min{P [j], U [u] + c(e) + D[u0 ]};computed Algorithm 4 worst-case O(|E| n) time. Note boundover-pessimistic assumes every edge |E| skipping every variable X.auxiliary structures U, D, P computed, valid domains efficientlyextracted using Algorithm 5. unassigned variable xi , value Di validdomain VDi [K] iff following holds: condition (7) satisfied edge e(u, u0 , a) Econdition (5) satisfied. non-assigned variable i, algorithm first checksvalue Di whether supported skipping edge P [i]. Afterwards, scansi-th layer extractsP values supported edges Ei . achieved (|D| + |V | + |E|)time, |D| = ni=1 |Di |.Algorithm 5: Computing valid domains V Di .Data: MDD (V, E), cost function C, maximal cost Kforeach = 1, . . . , nV Di = ;xi assigned aiV Di {ai };continue;foreach DiP [i] + ci (a) cmin [i] KV Di V Di {a};foreach u Viforeach e : u u0U [u] + c[e] + D[u0 ] KV Di V Di {v(e)};Again, overall interaction remains same. Labels P incrementally updatedworst case O(|E| n) time. Valid domains extracted (|D| + |V | + |E|) time.response changing cost restriction K, auxiliary labels need updated. Validdomains extracted directly using Algorithm 5 (|D| + |V | + |E|) time.3.4 Handling Non-Additive Cost Functionscertain interaction settings, cost function additive. example, user preferences might depend entire package features rather selection individualfeature. Similarly, price product need simple sum costs individualparts, might depend combinations parts selected. general, cost111fiAndersen, Hadzic, & Pisingerfunction c(x1 , . . . , xn ) might sum non-unary cost functions ci , = 1, . . . , k,c(x1 , . . . , xn ) =kXci (Xi )i=1cost function ci expresses unique contribution combination features withinsubset variables Xi X,Dj R.ci :jXi3.4.1 Non-Unary Labelingapproach extended handle non-unary costs adopting labeling techniquesused graphical representations (e.g., Wilson,2005; Mateescu et al.,Pk2008). Assume given cost function c(x1 , . . . , xn ) = i=1 ci (Xi ). Let A(i) denoteset cost functions cj xi last variable scope cj :A(i) = {cj | xi Xj xi0 6 Xj , i0 > i}.Given assignment a(a1 , . . . , ai ) variables x1 , . . . , xi , evaluate every function cjAi . scope cj strict subset {x1 , . . . , xi }, set cj (a) valueu, ucj (Xj (a)) Xj (a) projection onto Xj . Now, every path p : rVi+1 , last edge (in i-th layer) e Ei , label e sum cost functionsbecome completely instantiated assigning xi = ai :Xcj (p).(8)c(e, p) =cj A(i)respect labeling, aPcost solution represented path p would indeedsum costs edges: ep c(e, p). order apply approach developedadditive cost functions Section 3.2, edge labeled costincoming path. However, possible general. thereforeexpand original MDD, creating multiple copies e splitting incoming pathsensure two paths p1 , p2 sharing copy e0 edge e induce edgecost c(e0 , p1 ) = c(e0 , p2 ). MDD, denoted Mc , generated using examplesearch caching isomorphic nodes suggested Wilson (2005), extendingstandard apply operator handle weights suggested Mateescu et al. (2008).3.4.2 Impact Sizeincrease size Mc relatively cost-oblivious version depends additivity cost function c. example, fully additive cost functions (each scopeXi contains single variable) Mc = , since label c(e) regardlessincoming path. However, entire cost function c single non-additive componentc1 (X1 ) global scope (X1 = X), edges last MDD layer labeled,case explicit cost encoding MDD Section 3.1. must leastC(Sol) edges last layer, one feasible cost. Hence, range costs C(Sol)112fiInteractive Cost Configuration Decision Diagramsexponential, size Mc . Furthermore, even C(Sol) limited size, increase Mc might significant due breakup node isomorphisms previous layers.case explicit cost encoding (Section 3.1) effect demonstrated empiricallySection 6. similar effect size would occur graphical-representations.example, representations exploiting global CSP structure - weighted cluster trees(Pargamin, 2003) - adding non-additive cost functions increases size clusters,required non-additive component ci (Xi ) least one cluster containsentire scope Xi . Furthermore, criteria node merging Wilson (2005) Mateescuet al. (2008) refined, since nodes longer isomorphic rootset feasible paths, paths must cost well.3.4.3 Semiring Costs Probabilistic QueriesNote approach generalized accommodate general aggregation costs discussed Wilson (2005). Cost functions ci need map assignmentsXi variables set real numbers R set equipped operators, = (A, 0, 1, , ) semiring. MDD property computedp:r 1 ep c(e). Operator aggregates edge costs operator aggregates path costs.semiring distributes , global computation done efficiently local node-based aggregations, much shortest path computed. framework basedreasoning paths minimal cost corresponds using = (R+ , 0, 1, min, +)different semirings could used. particular, taking = (R+ , 0, 1, +, )handle probabilistic reasoning. cost function ci corresponds conditional probability table, cost edge c(e), e : u u0 Ei corresponds probabilityQP (xi = v(e)) given assignments p : ru. cost path c(p) = ep c(e)probability event represented path,Pfor given value Di 0canget marginal probability P (xi = a) computing e(u,u0 ,a)Ei (U [u] c(e) D[u ]).4. Compiling MDDsprevious section showed implement cost queries solution spacerepresented MDD. section, discuss generate MDDsCSP model description (X, D, F ). goal develop efficient easy implementapproach handle instances handled previously BDD-based configuration(Hadzic et al., 2004).Variable Ordering. first step choose ordering CSP variables X.critical since different variable orders could lead exponential differences MDD size.well investigated problem, especially binary decision diagrams. fixedformula, deciding ordering resulting BDD wouldnodes (for threshold ) NP-hard problem (Bollig & Wegener, 1996). However,well developed heuristics, either exploit structure input modeluse variable swapping existing BDD improve ordering local-search manner(Meinel & Theobald, 1998). example, fan-in weight heuristics popularinput form combinational circuits. input CSP, reasonable heuristicchoose ordering minimizes path-width corresponding constraint graph,113fiAndersen, Hadzic, & PisingerMDD worst case exponential path-width (Bodlaender, 1993; Wilson, 2005;Mateescu et al., 2008). Investigating heuristics variable ordering scopework, remainder paper assume ordering already given.experiments use default orderings provided instances.Compilation Technique. approach first compile CSP model binarydecision diagrams (BDD) exploiting highly optimized stable BDD packages (e.g.,Somenzi, 1996) afterwards extract corresponding MDD. Dedicated MDD packagesrare, provide limited functionality implementations optimizedBDD packages offer competitive performance (Miller & Drechsler, 2002). interestingrecent alternative generate BDDs search caching isomorphic nodes.approach suggested Huang Darwiche (2004) compile BDDs CNFformulas, proved valuable addition standard compilation based pairwiseBDD conjunctions. However, compilation technology still early stagesdevelopment open-source implementation publicly available.4.1 BDD EncodingRegardless BDD compilation method, finite domain CSP variables X firstencoded Boolean variables. Choosing proper encoding important sinceintermediate BDD might large inadequate subsequent extraction. general,CSP variable xi would encoded ki Boolean variables {xi1 , . . . , xiki }. Dimapped bit vector enci (a) = (a1 , . . . , aki ) {0, 1}ki differentvalues 6= a0 get different vectors enci (a) 6= enci (a0 ). several standard Booleanencodings multi-valued variables (Walsh, 2000). log encoding scheme xiencoded ki = dlog|Di |e Boolean variables, representing digit binary notation.multivalued assignment xi = translated set assignments xij = ajP j1Pki j12 xj < |Di | addedaj . Additionally, domain constraint kj=1=j=1 2forbid bit assignments (a1 , . . . , aki ) encode values outside domain Di .direct encoding (or 1-hot encoding) also common, especially well suited efficientpropagation searching single solution. scheme, multi-valued variablexi encoded |Di | Boolean variables {xi1 , . . . , xiki }, variable xij indicateswhether j-th value domain aj Di assigned. variable xi , exactly onevalue Di assigned. Therefore, enforce domain constraint xi1 +. . .+xiki = 1= 1, . . . , n. Hadzic, Hansen, OSullivan (2008) empirically demonstratedusing log encoding rather direct encoding yields smaller BDDs.Sn Thei set Boolean variables fixed union encoding variables, Xb =i=1 {x1 , . . . , xki } still specify ordering. common orderingwell suited efficiently answering configuration queries clustered ordering. Here,Boolean variables {xi1 , . . . , xiki } grouped blocks respect ordering amongfinite-domain variables x1 < . . . < xn . is,xij11 < xij22 i1 < i2 (i1 = i2 j1 < j2 ).might orderings yield smaller BDDs specific classes constraints.Bartzis Bultan (2003) shown linear arithmetic constraints represented114fiInteractive Cost Configuration Decision Diagramscompactly Boolean variables xij grouped wrt. bit-position j ratherfinite-domain variable xi , i.e. xij11 < xij22 j1 < j2 (j1 = j2 i1 < i2 ). However,configuration constraints involve linear arithmetic constraints, space savingsreported Bartzis Bultan (2003) significant variable domainssize power two. Furthermore, clustered orderings yield BDDspreserve essentially combinatorial structure allows us extract MDDsefficiently seen Section 4.2.Example 3 Recall T-shirt example D1 = {0, 1, 2, 3}, D2 = {0, 1, 2}, D3 ={0, 1}. log encoding variables x11 < x12 < x21 < x22 < x31 , inducing variable setXb = {1, 2, 3, 4, 5}. log-BDD clustered variable ordering shown Figure 4(a).x1x1x10 1 2 3x2x2x2x2x2x20x2x3x3x32 10x3111(a) log-BDD.12x2(b) extracted MDD.Figure 4: log-BDD clustered ordering, extracted MDD T-shirt example. BDD, draw terminal node 1 terminal node 0incoming edges omitted clarity. node corresponding Booleanencoding variable xij labeled corresponding CSP variable xi . Edgeslabeled 0 1 drawn dashed full lines, respectively.4.2 MDD ExtractionBDD generated using clustered variable ordering extract correspondingMDD using method originally suggested Hadzic Andersen (2006)subsequently expanded Hadzic et al. (2008). following considerations,use mapping cvar(xij ) = denote CSP variable xi encoding variablexij and, slight abuse notation, apply cvar also BDD nodes u labeledxij . terminal nodes, define cvar(0) = cvar(1) = n + 1 (recall BDD twoterminal nodes 0 1 indicating false true respectively). Analogously, usemapping pos(xij ) = j denote position bit variable encoding.method based recognizing subset BDD nodes captures coreMDD structure, used directly construct corresponding MDD.115fiAndersen, Hadzic, & Pisingerblock BDD layers corresponding CSP variable xi , Li = Vxi . . . Vxi ,1kisuffices consider nodes reachable edge previous blocklayers:Ini = {u Li | (u0 ,u)E cvar(u0 ) < cvar(u)}.00first layerSn+1we take In1 = {r}. resulting MDD (V , E ) contains nodes0Ini , V = i=1 Ini constructed using extraction Algorithm 6. edge e(u, u0 , a)added E 0 whenever traversing BDD B u wrt. encoding ends u0 6= 0.Traversals executed using Algorithm 7. Starting u, step algorithmtraverses BDD taking low branch corresponding bit ai = 0 high branchai = 1. Traversal takes ki steps, terminating soon reaches nodelabeled different CSP variable. MDD extracted log-BDD Figure 4(a)shown Figure 4(b).Algorithm 6: Extract MDD.Data: BDD B(V, E)E 0 {},V 0 {r};foreach = 1, . . . , nforeach u Iniforeach Di1u0 Traverse(u, a);u0 6= 0E 0 E 0 {(u, u0 , a)};V 0 V 0 {u0 }return (V 0 , E 0 );Algorithm 7: Traverse BDD.Data: BDD B(V, E), u,cvar(u);(a1 , . . . , aki ) enci (v);repeatpos(u);= 0u low(u);elseu high(u);cvar(u) 6= ;return u;Since traversal (in lineP1 Algorithm 6) takes O(dlog|Di |e) steps, running timeMDD extractionO( ni=1 |Ini | |Di | dlog|Di |e). resulting MDD (V 0 , E 0 )PnO( i=1 |Ini | |Di |) edgesPnadd |Di | edges every node0u Ini . Since keep nodes Ini , |V | = i=1 |Ini | |V |.4.3 Input Model Implementation Detailsimportant factor usability approach easiness specifying inputCSP model. BDD packages callable libraries default support CSP-like inputlanguage. best knowledge, open-source BDD-compilation tool116fiInteractive Cost Configuration Decision Diagramsaccepts input CSP-like model CLab (Jensen, 2007). configuration interfacetop BDD package BuDDy (Lind-Nielsen, 2001). CLab constructs BDDinput constraint conjoins get final BDD. Furthermore CLab generatesBDD using log-encoding clustered ordering suits well extraction approach.Therefore, compilation approach based using CLab specify input modelgenerate BDD used extraction Algorithm 6.Note extracting MDD, preprocess efficient online querying.expand long edges merge isomorphic nodes get merged MDD.translate efficient form online processing. rename BDD node namesindexes 0, . . . , |V |, root index 0 terminal 1 index |V |.allows subsequent efficient implementation U labels, well efficientaccess children parent edges node. initial experiments got ordermagnitude speed-up wCVD queries switched BDD node names (whichrequired using less efficient mapping U , D, Ch P structures).5. Interactive Configuration Multiple Costsnumber domains, user configure presence multiple cost functionsexpress often conflicting objectives user wants achieve simultaneously.example, configuring product, user wants minimize price, maximizing quality, reducing ecological impact, shortening delivery time etc. assumetherefore addition CSP model (X, D, F ) whose solution space representedmerged MDD , given k additive cost functionsci (x1 , . . . , xn ) =nXcij (xi ), = 1 . . . , kj=1expressing multiple objectives. Multi-cost scenarios often considered within multicriteria optimization framework (Figueira et al., 2005; Ehrgott & Gandibleux, 2000).usually assumed optimal (but unknown) way aggregate multipleobjectives single objective function would lead solution achievesbest balance satisfying various objectives. algorithms sample efficient solutions(nondominated wrt. objective criteria) display user. user input,algorithms learn aggregate objectives adequately usednext sampling efficient solutions etc. approaches user asked explicitlyassignPkweights wi objectives ci aggregated weighted summationc = i=1 wi ci .adopting techniques run compiled representation solution spacewould immediately improve complexity guarantees would useful many scenarios multi-criteria techniques traditionally used, believe configurationsetting, explicit control variable values needed. user easily exploreeffect assigning various variable values variables well cost functions.therefore suggest directly extend wCVD query user could exploreeffect cost restrictions way explores interactions regular variables. key query want deliver computing valid domains wrt. multiple costrestrictions:117fiAndersen, Hadzic, & PisingerDefinition 5 (k-wCVD) Given CSP model (X, D, F ), additive cost functions cj : R,maximal costs Kj , j = 1, . . . , k, given partial assignment , compute:VD [, {Kj }kj=1 ] = {a Di | 0 .(0 |= F {(xi , a)} 0k^j=1cj (0 ) Kj )}particularly interested two-cost configuration likely occurpractice strong connections existing research solving Knapsack problemsmulti-criteria optimization. reminder section first discusscomplexity 2-wCVD queries develop practical implementation approach.discuss general k-wCVD query.5.1 Complexity 2-wCVD queryassume input problem merged MDD , additive costfunctions c1 , c2 cost bounds K1 , K2 . first question whether possiblerestricted forms additive cost functions c1 , c2 implement 2-wCVD polynomialtime. purpose formulate decision-version 2-wCVD problem:Problem 1 (2-wCVD-SAT) Given CSP (X, D, F ) MDDPnM representation solution space, given two additive cost functions ci (x) = j=1 cij (xj ), = 1, 2 costrestrictions K1 , K2 , decide whether F c1 (x) K1 c2 (x) K2 satisfiable.Unfortunately, answer even constraints involve positive coefficients,binary domains. show reduce well-known Two-PartitionProblem (TPP) NP-hard (Garey & Johnson, 1979). given set positiveintegers = {s1 , . . . , sn }, TPP asks decide whether possible split setindexes=P{1, . . . , n} two sets \ sum set same:PiI\A si .iA =Proposition 3 2-wCVD-SAT problem defined Boolean variables involvinglinear cost functions positive coefficients NP-hard.Proof 3 show stated reduction TPP. order reduce TPP two-costconfiguration introduce 2n binary variables x1 , . . . , x2nx2i1 = 1 \ x2i = 1. construct MDD F ={x1 6= x2P, . . . , x2n1 6= x2n } introduceP two linear cost functions positive coefficients,c1 (x) = ni=1 si Px2i1 c2 (x) = ni=1 si x2i . overall capacity constraints setK1 = K2 =iI si /2. setting = {i | x2i1 = 1} easily seenF c1 (x) K1 c2 (x) K2 satisfiable TPP feasible solution.Hence, able solve 2-wCVD-SAT Boolean variables positive linear costfunctions polynomial time, would also able solve TPP problem polynomially.5.2 Pseudo-Polynomial Scheme 2-wCVDprevious subsection demonstrated answering 2-wCVD queries NP-hard evensimplest class positive linear cost functions Boolean domains. Hence,118fiInteractive Cost Configuration Decision Diagramshope solving 2-wCVD guaranteed polynomial execution time unless P = N P .However, still want provide practical solution 2-wCVD problem. hopeavoid worst-case performance exploiting specific nature cost-functionsprocessing. subsection therefore show 2-wCVD solved pseudopolynomial time extending labeling approach Section 3.2. Furthermore,show adopt advanced techniques used Knapsack problem (Kellerer, Pferschy,& Pisinger, 2004).5.2.1 Overall Approachalgorithm runs analogous single-cost approach developed Section 3.2.restricting MDD wrt. current assignment, calculate upstream downstreamcosts U, (which longer constants lists tuples), use checkedge e, whether v(e) valid domain.given edge e : u u0 , labeled costs c1 (e), c2 (e), follows v(e) V Di iffpaths p : ru, p0 : u01 c1 (p) + c1 (e) + c1 (p0 ) K10c2 (p) + c2 (e) + c2 (p ) K2 . node u suffices store two sets labels:U [u] = {(c1 (p), c2 (p)) | p : ru}D[u] = {(c1 (p), c2 (p)) | p : u1}Then, given cost restrictions K1 , K2 , edge e : u u0 , u Vi , domain V Di [K1 , K2 ]contains v(e) (a1 , a2 ) U [u] (b1 , b2 ) D[u] holdsa1 + c1 (e) + b1 K1 a2 + c2 (e) + b2 K2(9)5.2.2 Exploiting Pareto Optimalitysingle-cost case sufficient store U [u], D[u] minimal value(the cost shortest path root/terminal), multi-cost case need store multipletuples. immediate extension would require storing K1 K2 tuples node.However, need store non-dominated tuples U lists. twotuples (a1 , a2 ) (a01 , a02 ) lista1 a01 a2 a02may delete (a01 , a02 ) test (9) succeeds (a01 , a02 ) also succeed (a1 , a2 ).remaining entries costs pareto-optimal solutions. solution pareto-optimalwrt. solution set cost functions c1 , c2 possible find cheaper solutionrespect one cost without increasing other. Path p : r1 representspareto-optimal solution Sol iff node u path, sub-paths p1 : rup2 : u1 pareto-optimal wrt. sets paths {p : ru} {p : u1}respectively. Hence, node u suffices store:U [u] = {(c1 (p), c2 (p)) | p : ru, p0 :ru (c1 (p)c1 (p0 ) (c2 (p) c2 (p0 ))}D[u] = {(c1 (p), c2 (p)) | p : u1, p0 :u1 (c1 (p)c1 (p0 ) (c2 (p) c2 (p0 ))}119fiAndersen, Hadzic, & PisingerNote due pareto-optimality, a1 {0, . . . , K1 } a2 {0, . . . , K2 }one tuple U first coordinate a1 secondcoordinate a2 . Therefore, node u, U [u] D[u] min{K1 , K2 }entries. Hence, space requirements algorithmic scheme worst case O(|V |K)K = min{K1 , K2 }.5.2.3 Computing U Setsdiscuss compute U sets efficiently utilizing advancedtechniques solving Knapsack problems (Kellerer et al., 2004). recursively update Usets layer layer manner shown Algorithm 8. critical componentrecursion step algorithm merging lists lines 2 4. operationnew list formed dominated tuples detected eliminated. orderefficiently, critical keep U lists sorted wrt. first coordinate,i.e.(a1 , a2 ) (a01 , a02 ) a1 < a2 .U sorted, merged O(K) time using list-merging algorithmKnapsack optimization (Kellerer et al., 2004, Section 3.4).Algorithm 8: Update U, labels.1234Data: MDD , Cost functions c1 , c2 , Bounds K1 , K2U [] = {(, )}, U [r] = {(0, 0)};foreach = 1, . . . , nforeach u Viforeach e : u u0;foreach (a1 , a2 ) U [u]a1 + c1 (e) K1 a2 + c2 (e) K2(a1 + c1 (e), a2 + c2 (e));U [u0 ] ergeLists(S, U [u0 ]);D[] = {(, )}, D[1] = {(0, 0)};foreach = n, . . . , 1foreach u Viforeach e : u u0;foreach (a1 , a2 ) D[u0 ]a1 + c1 (e) K1 a2 + c2 (e) K2(a1 + c1 (e), a2 + c2 (e));D[u] ergeLists(S, D[u]);time complexity determined populating list (in lines 1 3) merging(in lines 2 4). updates takes O(K) worst case. Since performupdates edge e E, total time complexity Algorithm 8 O(|E| K)worst case.120fiInteractive Cost Configuration Decision Diagrams5.2.4 Valid Domains ComputationU, sets updated extract valid domains straightforward mannerusing Algorithm 9. edge e : u u0 algorithm evaluates whether v(e) V Diworst case O(|U [u]| |D[u0 ]|) = O(K 2 ) steps. Hence, valid domain extraction takes worstcase O(|E| K 2 ) steps.Algorithm 9: Compute valid domains.Data: MDD , Cost functions c1 , c2 , Cost bounds K1 , K2 , Labels U ,Dforeach = 1, . . . , nVDi ;foreach u Viforeach e : u u0foreach (a1 , a2 ) U [u], (b1 , b2 ) D[u0 ]a1 + c1 (e) + b1 K1 a2 + c2 (e) + b2 K2VDi VDi {v(e)};break;However, improve running time valid domains computation exploiting(1) pareto-optimality (2) fact sets U, sorted. critical observegiven edge e : u u0 , (a1 , a2 ) U [u] suffices perform validitytest (9) tuple (b1 , b2 ) D[u0 ], b1 maximal first coordinate satisfyinga1 + c1 (e) + b1 K1 , i.e.b1 = max{b1 | (b1 , b2 ) D[u0 ], a1 + c1 (e) + b1 K1 }.Namely, test succeeds (b01 , b02 ) b01 < b1 , also succeed (b1 , b2 )since due pareto-optimality, b01 < b1 b2 < b02 hence a2 +c2 (e)+b2 < a2 +c2 (e)+b02K2 . Since lists sorted, comparing relevant tuples performed efficientlytraversing U [u] increasing order, traversing D[u0 ] decreasing order. Algorithm10 implements procedure.Algorithm 10: Extract edge value.12Data: MDD , Cost constraints c1 , c2 , Bounds K1 , K2 , Edge e : u u0 Eia(a1 , a2 ) = U [u].begin();b(b1 , b2 ) = D[u0 ].end();6= > b 6=a1 + c1 (e) + b1 > K1b(b1 , b2 ) D[u0 ].previous();continue;else a1 + c1 (e) + b1 K1 a2 + c2 (e) + b2 K2VDi VDi {v(e)};return;a(a1 , a2 ) U [u].next();algorithm relies several list operations. Given list L sorted tuples, operationsL.begin() L.end() return first last tuple respectively wrt. list ordering.121fiAndersen, Hadzic, & PisingerOperations L.next() L.previous() return next previous elementlist wrt. ordering. Elements > indicate two special elements appearlast first element list respectively. indicatepassed beyond boundary list. algorithm terminates (line 2) soontest succeeds. Otherwise, keeps iterating tuples processed eitherlast tuple U [u] first tuple D[u0 ]. case algorithm terminatesguaranteed v(e) 6 V Di . step, traverse least one elementU [u] D[u0 ]. Hence, total execute U [u] + D[u0 ] 2K operations.Therefore, time complexity single edge traversal O(K) complexity validdomains computation Algorithm 9 (after replacing quadratic loop Algorithm10) O(|E| K) K = min{K1 , K2 }.conclusion, developed pseudo-polynomial scheme computing valid domains wrt. two cost functions (2-wCVD). space complexity dominated storing Usets node. worst case store O(|V | K) entries. timecomplexity compute U labels extract valid domains takes O(|E| K) steps.overall interaction similar single-cost approach. assigning variable,recompute labels well extract domains. tighten cost restrictionsK1 , K2 K10 K1 , K20 K2 need extract domains. However, relax eithercost restrictions, K10 > K1 need recompute labels well.precisely, labels U, need recomputed K1 > K1max K1max initialcost restriction last assignment.5.2.5 ExtensionsNote approach can, principle, extended handle general k-wCVD queryfixed k. Lists U would contain set non-dominated k-tuples, ordered that:(a1 , . . . , ak ) (a01 , . . . , a0k ) iff smallest coordinate j aj 6= a0j holds aj < a0j .list merging well valid domains extraction would directly generalizedoperate ordered sets, although time complexity testing dominansincrease. worst-case complexity would depend size efficient frontier,k cost functions cost bounds K bounded O(K k1 ). practice however,could expect number non-dominated tuples much smaller, especially costfunctions smaller scopes smaller coefficients. Note approachalso extended accommodate non-additive cost functions expanding MDDaccommodate non-unary labels fashion discussed Section 3.4.5.3 Approximation Scheme 2-wCVDsubsection analyze complexity answering 2-wCVD queries approximativemanner, i.e. improve running time guarantees settling approximatesolution. Assume one constraints K2 fixed second constraint mayexceeded small tolerance (1+)K1 . example, user might willing toleratesmall increase price long strict quality restrictions met. section presentfully polynomial time approximation scheme (FPTAS) calculating valid domainstime O(En 1 ) problem. FPTAS satisfy feasible solutionrespect original costs fathomed, feasible configuration found122fiInteractive Cost Configuration Decision Diagramsuse FPTAS domain restriction satisfy cost constraint within(1 + )K1 . Finally, FPTAS running time polynomial 1/ inputsize.order develop FPTAS use standard scaling technique (Schuurman &Woeginger, 2005) originally presented Ibarra Kim (1975). Given , let nnumber decision variables. Set = K1 /(n + 1) determine new costs c1 (e) =bc1 (e)/T c new bounds K1 = dK1 /T e. perform valid domains computation(label updating domain extraction) described Section 5.2, using scaled weights.following propositions prove obtained FPTAS scheme.Proposition 4 running time valid domains computation O( 1 En)Proof 4 may assume K1 < K2 otherwise may interchange two costs.running time becomes1n+1) = O(En )O(E K1 ) = O(EK1 /T ) = O(EK1K1since n V polynomial input size O(V + E) precision 1 .Proposition 5 solution feasible respect original costs, alsofeasible respect scaled costs.PProof 5 Assume ep c1 (e) K1 .XX111Xc1 (e) K1 K1 e = K1c1 (e) =bc1 (e)/T cepepepProposition 6 solution feasible respect scaled costs c1 (e) satisfiesoriginal constraints within (1 + )K1 .PProof 6 Assume ep c1 (e) C1 .PPPPep c1 (e) + nep (bc1 (e)/T c + 1)ep c1 (e)/Tep c1 (e) =K1 + n = dK1 /T e + n (K1 /T + 1) + n= K1 + (n + 1)Since = K1 /(n + 1) getXc1 (e) K1 + (n + 1)K1 /(n + 1) = (1 + )K1epshows stated.time complexity improved using techniques Kellerer et al. (2004)Knapsack Problem, interested showing existenceFPTAS.considerations previous subsections fully analyzed complexityanswering 2-wCVD queries. first showed NP-hard problem.developed pseudo-polynomial scheme solving it, finally devised fully polynomial time approximation scheme. Even though cannot provide polynomial running-timeguarantees, based considerations, hope provide reasonable performancepractical instances, demonstrated Section 6.123fiAndersen, Hadzic, & Pisinger5.4 Complexity k-wCVD Queryconclude section discussing complexity general k-wCVD queries.practical implementation efforts focused implementing 2-wCVD queries, wCVDqueries number cost constraints known advance, completenessconsider generic problem delivering k-wCVD arbitrary k, i.e. k partinput problem.prove problem pseudo-polynomial scheme unlessNP=P. show decision version problem k-wCVD-SAT NP-hardstrong sense (Garey & Johnson, 1979) reduction bin-packing problem (BPP)strongly NP-hard (Garey & Johnson, 1979). decision form BPP askswhether given set numbers s1 , . . . , sn placed k bins size K each. Notice,cannot use reduction showing NP-hardness 2-wCVD-SAT, since kpart input BPP.Theorem 7 k-wCVD-SAT problem variable k, strongly NP-hard.Proof 7 given instance BPP reduce k-wCVD-SAT instance follows:construct MDD CSP (X, D, F ) n variables X = {x1 , . . . , xn }domain size k, Di = {1, . . . , k}, = 1, . . . , n. set F = , resulting MDDallows assignments. n nonterminal nodes u1 , . . . , un corresponding numberss1 , . . . , sn . two nodes ui , ui+1 k edges costs (c1 (e), c2 (e), . . . , ck (e))set(si , 0, . . . , 0), (0, si , 0, . . . , 0), (0, 0, si , 0, . . . , 0), . . . , (0, . . . , si ),first node u1 root u1 = r last node un connected terminalun+1 = 1. overall capacity constraints (K1 , . . . , Kk ) = (K, . . . , K).easily seen may find path r 1 BPPfeasible solution. Since BPP strongly NP-hard shown k-wCVD-SAT alsostrongly NP-hard.6. Experimental Evaluationimplemented compilation scheme algorithms wCVD 2-wCVD queries.performed number experiments evaluate applicability approachwell confirm various hypotheses made throughout paper. used two setsinstances whose properties presented Table 1. first set corresponds real-worldconfiguration problems available configuration benchmarks library CLib2 . CSPmodels configuration constraints. correspond highly structured configurationproblems huge number similar solutions. second set instances representsproduct-catalogue datasets used Nicholson, Bridge, Wilson (2006). cataloguesdefined explicitly, tables solutions. represent much smaller sparser setsolutions.2. http://www.itu.dk/research/cla/externals/clib/124fiInteractive Cost Configuration Decision DiagramsInstanceESVSFSBike2PC2PCBig-PCRenaultTravelLaptopsCamerasLettingsSol2312242262202202832411461683210751X26233441451249971496dmin22242224252dmax61513834336642839438165174davg55698124134424045Table 1: First seven instances real-world configuration problems available configuration benchmarks library CLib. Remaining four instances product cataloguesused Nicholson et al. (2006). instance provide number solutions Sol, number variables X, minimal, maximal average domainsize.6.1 MDD Sizefirst set experiments, instance generated log-encoded BDD B usingCLab (Jensen, 2007). extracted corresponding MDD B. Finally,expanded long edges merged isomorphic nodes generate merged MDD 0 .compare sizes B, 0 Table 2. structure provide numbernodes V edges E. also provide size BDD B. concludetable BDDs MDDs exponentially smaller size solutionspace configuration instances significantly smaller diverse productconfiguration catalogues. Furthermore, see number edges mergedMDDs 0 significantly larger comparison extracted MDDs . Hence, duesimpler online algorithms, using merged MDDs seems well suited online reasoning.also see multi-valued encoding many cases reduces number nodesedges comparison BDDs. Even though compilation times less important sincegeneration MDD performed offline, worth noting largest instance,Renault, took around 2min 30sec compile instance BDD extractMDD.6.1.1 Encoding Cost Explicitlyalso investigated impact encoding cost information explicitly MDD.instance compared size MDD without cost variables (Mc respectively). configurationbenchmarks introduce additional variableP[0, 10000] = ni=1 ai xi coefficients ai randomly drawninterval [0, 50]. put variable last ordering since positionsget MDDs similar size, putting end allows easier theoretical analysis. Since125fiAndersen, Hadzic, & PisingerInstanceESVSFSBike2PC2PCBig-PCRenaultTravelLaptopsCamerasLettingsVB3063,0443,12913,33216,494356,696455,7968479952842742122EB6126,0886,25826,66432,988713,392911,59216,95819,0568,5484,244KB541562372987,9459,8911541727136VM877538533,9074875100,193283,03314692033791351EM2021,9891,7266,1367989132,595334,008292827139991099VM09676793339074875100,272329,13514692033791351EM02202017188661367989132,889426,212292827139991099Table 2: Comparison BDDs MDDs instances Table 1. second,third fourth column give number non-terminal BDD nodes VB ,number edges EB size disk BDD kilobytes KB. fifthsixth column give number vertices VM edges EM MDDextracted BDD using Algorithm 6 page 116. final two columnsprovide number nodes edges merged MDD (M 0 ) longedges extracted MDD expanded.product catalogues already contain cost variable (price), produce cost-obliviousversion existentially quantifying y, = c .Table 3 compare MDDs c . structures provide numberedges well representation size kilobytes. also show size cost rangeC(Sol). observe configuration instances high level sharingcompression, introducing cost information explicitly induces order magnitude increasesize even cost range C(Sol) limited (400 times increase Bike2 instance).MDDs two largest instances could generated. However, product cataloguesmuch less sharing, removing cost information dramatic effect.worst case, number edges c two times larger . Hence,experimental results confirm introducing cost explicitly could dramatic effectMDD representations highly compressed solution spaces, usually implicitly definedconjunction combinatorial constraints. However, effect adding explicit costinformation might modest solution space defined explicitly, (sparse) listdatabase entries, case product catalogues. Furthermore, sizecost range C(Sol) needs significant large increase size take place.6.2 Response Times wCVD Queriessecond set experiments, evaluated performance wCVD queries mergedMDD representations configuration instances. report running timescomputing U labels (Algorithm 2) well computing valid domains (Algorithm 3).Table 4 report average worst-case running times initial merged MDDs126fiInteractive Cost Configuration Decision DiagramsInstanceESVSFSBike2PC2PCBig-PCRenaultTravelLaptopsCamerasLettingsE2021,9891,7266,1367,989132,595334,00816401592725496KB541562372987,9459,8914580449Ec129,514407,662693,8241,099,8421,479,306292827139991099KB4,40812,76731,46757,90970,9001541727136C(Sol)1,9661,4973,0082,0002,072839438165174Table 3: Effect explicitly encoding cost information. second third column indicate number edges representation size kilobytes cost-obliviousMDD, fourth fifth column show MDD containingcost information. Column C(Sol) indicates range available costssolutions.Table 2. also report time necessary restrict MDD wrt. assignment(Algorithm 1). randomly create additive cost function c assigning variablexi value Di cost ci (a) [0, 50]. Valid domains computed wrt.maximal cost restriction K set value larger length longestMDD path wrt. cost function c. ensures longest execution time Algorithm 3.data-point table average maximum 1000 executions Fedora9 operating system, using dual Quad core Intel Xeon processor running 2.66 GHz.one core used instance. Empirical evaluation demonstrates response timeseasily within acceptable interaction bounds even largest instances, worstcase MDD nodes labeled within 0.13 seconds, valid domains computed within0.07 seconds MDD restricted wrt. assignment within 0.28 seconds.6.3 Response Times 2-wCVD Querygenerated analogous statistics 2-wCVD Table 5. tested performancealgorithms computationally demanding circumstances: operateoriginal (fully-sized) MDD, even though interaction would reduced dueuser assignments. Furthermore, cost functions c1 , c2 global scope,use cost restrictions computing U labels (i.e. ignore conditionline 1 Algorithm 10 hence, U [1] D[r] correspond entire efficient frontier).Normally, cost functions would involve subset variables fractionlabels efficient frontier (within restrictions K1 , K2 ) would relevant user.generate cost functions c1 , c2 drawing costs ci (a) randomly [0, 50]. computingvalid domains, use restrictions K1 , K2 larger lengths corresponding longest127fiAndersen, Hadzic, & PisingerInstanceESVSFSBike2PC2PCBig-PCRenaultLabeling U,avgmax0.00010.010.00010.010.00020.010.00020.010.00030.010.02100.040.05900.13Valid domainavgmax0.00010.010.00010.010.00010.010.00020.010.00030.010.01100.030.03100.07Restrictavgmax0.0001 0.010.0002 0.010.0010 0.010.0010 0.020.0010 0.010.0400 0.080.1600 0.28Table 4: Interaction time seconds wCVD queries. report time required computing U labels, valid domain computation restriction wrt. singleassignment.paths, possible solutions efficient frontier allowed. would leadlongest execution time Algorithm 9.algorithms easily handle first five instances. largest two instances,U labels known, calculating valid domains done within fractionsecond. Hence, user efficiently explore effect various cost restrictions K1 , K2 wrt.fixed partial assignment. user assigns variable, recomputing U labelstakes total average less 0.85 seconds, worst case less 1.4 seconds.already within acceptable interaction times, usability systemenhanced, e.g. using layered display information: always reactinginformation fastest compute (such CVD wCVD), user analyzingit, execute time consuming operations. particular, entire efficient frontierknown soon U labels generated worst case within 0.64 seconds.stage, user explore cost-space labels computed (on average withinnext 0.79 seconds). Note running times reduced numberadditional schemes, e.g. computing U labels parallel, two processorspresent.InstanceESVSFSBike2PC2PCBig-PCRenaultLabeling Uavgmax0.0001 0.010.0010 0.010.0010 0.020.0030 0.020.0050 0.020.2070 0.450.3470 0.64Labelingavgmax0.0002 0.010.0020 0.020.0020 0.010.0030 0.020.0040 0.020.3160 0.600.4700 0.79Valid domainavgmax0.00010.010.00010.010.00010.010.00050.010.00080.010.03000.040.07000.08Table 5: Interaction time seconds 2-wCVD query.128fiInteractive Cost Configuration Decision Diagramsempirical evaluation demonstrates practical value approach. EvenNP-hard 2-wCVD query implemented response times suitable interactive use,applied huge configuration instances. Note, however, order achieveperformance critical optimize MDD implementation well utilize advancedlist operation techniques. initial implementation efforts failed so, ledresponse times measured tens seconds largest instances.7. Related Workseveral directions related work. large variety representationsinvestigated area knowledge compilation might suitable supportinginteractive decision making cost restrictions. also number approacheshandle multiple cost functions multi-criteria optimization.7.1 Compiled Knowledge Representation Formspaper used binary decision diagrams (BDDs) multi-valued decision diagrams(MDDs) compiled representations CSP model. However, might compiled representations might suitable supporting interactive configuration.compiled representation supports efficient consistency checking conditioningwould theory support polytime interactive configuration. calculate valid domainssuffices value restrict representation check consistent. representation supports efficient optimization conditioning would support polytimecost restrictions. would suffice restrict representation value checkminimum smaller threshold value. therefore briefly surveyrelated compiled representations evaluate suitability framework.Knowledge-Compilation Structures. Probably well known frameworkcomparing various compiled forms propositional theories based viewingspecial classes negation normal form (NNF) languages (Darwiche & Marquis, 2002).NNFs directed acyclic graphs internal nodes associated conjunctions ()disjunctions (), leaf nodes labeled literals (x,x) constants truefalse. imposing various restrictions get subclasses NNF languages supportefficient execution various queries transformations. restrictive representationsless succinct i.e. exponentially larger instances, supportlarger number queries transformations polytime. comprehensive overviewrepresentations presented Darwiche Marquis (2002).critical restriction makes NNF languages tractable decomposability.exploits variable independencies enforcing children -node nonoverlappingvariable scopes. Hence, propositional formula F = F1 F2var(F1 ) var(F2 ) = , evaluate satisfiability F suffices independently evaluateF1 F2 . resulting language decomposable negation normal form (DNNF)already supports polytime two operations critical calculating valid domains: consistency checking conditioning. However, general DNNF compiler exists. Currentcompilation approach based exhaustive DPLL search caching isomorphic nodes(Huang & Darwiche, 2005) constructs subsets DNNF satisfy additional property129fiAndersen, Hadzic, & Pisingerdeterminism. two children -node mutually exclusive. resulting structure called deterministic decomposable negation normal form (d-DNNF). structurewould interesting target cost-configuration. Boolean CSP models, additivecost functions could efficiently optimized d-DNNFs. multi-valued models however, research necessary encode finite-domain values way allowsefficient cost processing. tool support compiling d-DNNFs far takes inputCNF formulas, unaware extensions allowing direct compilation generalCSP models.known knowledge representation forms retrieved enforcing additionalproperties. example, enforcing nodes decision nodesvariable encountered path (read-once property) get freeBDDs (FBDDs). enforcing decision nodes appear wrt. fixed ordering getordered BDDs (OBDDs). fact, d-DNNF compiler Huang Darwiche (2005)specialized compile OBDDs, proved valuable alternative way BDDcompilation.Weighted Multi-Valued Knowledge Compilation Structures. compiled representations propositional theories valued counterparts. Manyseen special cases valued NNFs (VNNF) (Fargier & Marquis, 2007). Roughly, everyvalued counterpart obtained changing semantics nodes, logical operators(such , ) general operators (that could arithmetic, + ). Valuesfunctions represented structures longer {0, 1} R. Furthermore,functions need defined Boolean domains, could take finite-domain values.general, subsets VNNF satisfy decomposability operator distributivity supportefficient optimization (Fargier & Marquis, 2007) could, principle, used supportcost configuration.Construction MDDs based encoding BDDs discussed Srinivasan,Kam, Malik, Brayton (1990). Amilhastre et al. (2002) augmented automata Vempaty (1992) edge weights reason optimal restorations explanations.weighted extensions correspond closely weighted MDDs since variant automataused Vempaty (1992) equivalent merged MDDs (Hadzic et al., 2008). However,weights used compute different queries generate MDDs based widelyavailable BDD-packages, Vempaty (1992) report compilation tools used. Semiringlabeled decision diagrams (SLDDs) (Wilson, 2005) label edges (unordered) MDDvalues semiring allow computation number queries relevant reasoninguncertainty. Due relaxed ordering, SLDDs succinct weightedMDDs therefore attractive target cost-based configuration. However,proposal seems theoretic, seem implemented. Arithmeticcircuits directed acyclic graphs internal nodes labeled summationmultiplication operators leaf nodes labeled constants variables (Darwiche,2002). could seen valued extension d-DNNFs hence succinctSLDDs. Furthermore, support efficient optimization coefficientspositive (in Bayesian context - support efficient computing probable explanations). Compilation technology ACs directly applicable general CSP models,used primarily representing Bayesian networks. based compiling d-DNNFstree clustering approaches (Darwiche, 2002, 2003). context, ACs might use130fiInteractive Cost Configuration Decision Diagramsful optimizing non-additive objective functions multiplicative coefficientsmulti-linear functions induced Bayesian networks. However, purely propositionalconstraints additive cost function optimized, purely propositionalrepresentation form (such d-DNNF) would adequate. Furthermore, efficient optimization queries based ACs implicitly assume constants (at leaf nodes)positive, case modeling Bayesian networks, hold generalcost functions.Global Structure Approaches. number techniques based tree-clustering (Dechter& Pearl, 1989) variable-elimination (Dechter, 1999) exploit variable independenciespresent globally CSP model. time space complexity techniquesturn bounded exponentially size important graph-connectivity notiontree-width (Bodlaender, 1993). techniques geared towards enhancing search single (optimal) solution (adaptive consistency, bucket elimination etc),concepts utilized compiling representations solutions. AND/ORMDDs (Mateescu et al., 2008) restricted Boolean variables subset d-DNNFformulas, variable labeling respects pseudo-tree obtained variable eliminationorder. Due utilization variable independencies -nodes, succinct MDDs therefore attractive compilation target cost-configuration.Furthermore, already extended handle weighted graphical models supportBayesian reasoning. However, publicly available tool support limited allowprocessing weighted CVD queries. Tree-driven-automata (Fargier & Vilarem, 2004) utilizetree clustering (Dechter & Pearl, 1989), generate partial variable ordering usedgenerate automaton. Tree-driven-automata equivalent AND/OR MDDsrestricted Boolean case represent subset d-DNNF languages calledstrongly ordered decomposable decision graphs (SO-DDG) (Fargier & Marquis, 2006). LikeAND/OR MDDs succinct MDDs therefore interesting targetcost-configuration. However, tools compiling tree-driven-automata yet become publicly available, far extended handle costs. Weightedcluster trees Pargamin (2003) weighted extension cluster trees used supportinteractive configuration preferences. However, publicly available compilation tool (an internal company-based implementation presented), clustersrepresented explicitly without utilizing compressions based local structure decision diagrams compiled representations. Tree-of-BDDs (ToB) (Subbarayan, 2008)directly exploit tree clustering representing cluster BDD. However,support conditioning polytime fundamental transformation supporting userinteraction (assigning variables). However, compiled instancesd-DNNF compilation fails, empirical evaluation shows average conditioningtimes short.BDD Extensions. large variety weighted extensions binary decision diagrams, represent real-valued functions f : {0, 1}n R rather Boolean functionsf : {0, 1}n {0, 1}. extensions limited Boolean variables adoptionfuture would consider encoding techniques multi-valued variables avoid explosion size support cost processing. Comprehensive overviews extensionspresented Drechsler (2001), Wegener (2000), Meinel Theobald (1998). immediate extension form algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona,131fiAndersen, Hadzic, & PisingerHachtel, Macii, Pardo, & Somenzi, 1993), also known multi-terminal BDDs (MTBDDs),essentially BDDs multiple terminal nodes - one cost value.structure-oblivious approach encoding cost, much approach explicitly encodingcost variable. size grows quickly increase number terminals. Therefore number BDD extensions introduced based labeling edges weights.differ mostly cost operators decomposition types associated nodes. Edge-valuedBDDs (EVBDDs) (Lai & Sastry, 1992) label every edge additive cost value c(e)edge e : u u0 , value val(u) = c(e) + val(u0 ) v(e) = 1 (otherwiseval(u) = val(u0 )). Factored EVBDDs (FEVBDDs) (Tafertshofer & Pedram, 1997) introduce multiplicative weights, v(e) = 1, value val(u) = c(e) + w(e) val(u0 )(otherwise val(u) = val(u0 )). Affine ADDs (AADDs) Sanner McAllester (2005)introduce additive multiplicative edge weights edge (regardless v(e)).val(u) = c(e) + w(e) val(u0 ) every edge. shown AADDsspecial case valued NNFs (Fargier & Marquis, 2007).orthogonal extension BDDs change decomposition type nodes. OBDDsbased Shannon decomposition fu = xi fu0 xi fu1 . change decompositiontype positive Davio (pD) decomposition fu = f0 xi f1 negative Davio(nD) decomposition fu = f0 xi f1 . using pD decomposition get ordered functional decisiondiagrams (OFDDs) (Kebschull & Rosenstiel, 1993). structures incomparableOBDDs, i.e. might exponentially larger smaller OBDDs dependinginstance. However, ordered Kronecker functional decision diagrams (OKFDDs)(Drechsler,Sarabi, Theobald, Becker, & Perkowski, 1994) allow three decomposition types, thus generalizing OBDDs OFDDs. Extending OFDDs additive edge weights leadsbinary moment diagrams (BMDs) (Bryant & Chen, 1995), adding also multiplicativeedge weights leads multiplicative binary moment diagrams ( BM Ds). Analogously,extending OKFDDs additive multiplicative edge weights get Kronecker binarymoment diagrams (KBMDs) K BM Ds respectively (Drechsler, Becker, & Ruppertz,1996).unclear whether Boolean structures advanced cost labeling schemesused directly represent multi-valued CSP models cost functions. However, couldcompare generalizations labeling schemes multi-valued structures. multivalued generalization EVBDDs would correspond roughly weighted MDDs. However, introducing additive multiplicative weights AADDs would correspondgeneralization labeling scheme could prove useful labeling multilinear cost functions. Namely, introduction multiplicative weights wouldsubgraph sharing, many nodes would refined accommodate non-additive costs. However, due multiplicative factors, obviouscashing technique based computing U, directly extended, especiallycoefficients negative. case additive cost functions though, schemeswould correspond labeling scheme. structures pay price lessefficient operators (such apply operator) larger memory requirements maintain information. Therefore, compiling Boolean functions, using structureswould pose unnecessary overhead comparison OBDDs. Hence, modelslarge number propositional (configuration) constraints, additive cost function,would gain compiling using structures even Boolean case.132fiInteractive Cost Configuration Decision Diagramscost function non-additive, introducing elaborate cost representations mightprove beneficial reducing memory requirements, might make label computingtechnique unapplicable. practical point view, implementationssupporting Boolean versions structures, aware tool supportingmulti-valued generalizations structures input language format usedspecifying general propositional constraints.7.2 Multi-Objective Cost Processingmultiple-cost configuration close approaches within framework multi-criteriaoptimization decision maker find solution subject multiple (often conflicting) objectives (Figueira et al., 2005; Ehrgott & Gandibleux, 2000). particular,MDD-based algorithms close approaches solving multiobjective shortestpath problem, given graph (V, E) arc labeled multiple costs,goal typically compute set Pareto-optimal (efficient, non-dominated) solutions(Ehrgott & Gandibleux, 2000; Muller-Hannemann & Weihe, 2001; Tarapata, 2007; Reinhardt & Pisinger, 2009). shown multi-objective shortest path problemintractable. particular, number Pareto-optimal solutions grow exponentially number vertices |V |, FPTAS (fully polynomial time approximationscheme) developed approximating set Pareto-optimal solutions. However, way solution space multi-criteria optimization problems exploredsignificantly different approach. Typically, interaction step subsetPareto-optimal solutions computed afterwards decision maker interactively navigates set order reach satisfying compromising solution. Interactivemethods multi-criteria optimization usually compute subset solutions efficientfrontier, suggest user evaluation, based input compute new setsolutions (Figueira et al., 2005, Chapter 16). techniques would use user inputbetter estimate way aggregate multiple objectives, would requireuser explicitly assign weights importance objectives. contrast, instead primarily driven costs solutions, interactive approach supports reasoningvariable assignments solutions valid domains computation. inherently different way exploring solution space adequateusers want explicit control variable assignments indicatingimportance cost functions.approaches CSP community model preferences soft constraints(Meseguer, Rossi, & Shiex, 2006) partially satisfied violated, goalfind satisfying least violating solution. usually presupposes preferences aggregated via algebraic operators, related single-costoptimization problems. However, approach Rollon Larrosa (2006) dealsmultiple costs explicitly. utilizes global structure (i.e. variable independencies)weighted CSP model compute efficient frontier bucket-based variable elimination. highly related approach utilizes global structure generalized additiveindependence (GAI) network presented Dubus, Gonzales, Perny (2009). ordercompute efficient frontier, authors use message passing computation mechanism analogous computing buckets. addition, authors develop fully133fiAndersen, Hadzic, & Pisingerpolynomial approximation scheme approximate efficient frontier demonstratesignificant improvement performance. However, neither methods exploitfact solution space hard constraints available compiled representation.Instead, methods operate unprocessed model specification (whetherweighted CSP GAI network) treating hard soft constraints uniformlyhence allowing scope hard constraints decrease variable independenciesmodel (and thus decrease performance algorithms). Furthermore, resultcomputation methods allow full exploration efficient solutions.value frontier single supporting efficient solution maintainedmaintain efficient value set supporting efficient solutions. Hence,possible efficiently retrieve valid domains even algorithms terminate. wouldinteresting see however, whether methods could adopted work MDDrepresentations solution space.Knapsack constraints special case two-cost configuration problems universally true MDD. Trick (2001) used dynamic programming propagate Knapsack constraints CSP search. Fahle Sellmann (2002) presented approximated filteringalgorithm, based various integer programming bounds Knapsack problem. Sellmann (2004) presented fully polynomial time approximation algorithm approximatedfiltering. However, techniques considered constraint propagation contextnone considered processing existing MDD structure.8. Conclusions Future Workpaper presented extension BDD-based interactive configuration configuring presence cost restrictions. guarantee polynomial-time cost configurationcost function additive feasible solutions represented using multi-valueddecision diagram. process cost restrictions MDD extractedunderlying BDD. therefore strictly extend BDD-based configuration Hadzic et al.(2004) support cost-bounding additive cost functions without incurring exponentialincrease complexity. implementation delivers running times easily satisfy interactive response-time requirements. Furthermore, approach extended supportbounding presence non-additive semiring-based costs.extended approach considering cost bounding wrt. multiple costs.proved NP-hard problem input MDD size even processingtwo linear inequalities positive coefficients Boolean variables. However,provided pseudo-polynomial scheme fully polynomial approximation scheme twocost configuration (which, principle, extended k-cost configurationfixed k). empirical evaluation demonstrated despite inherent hardnessproblem still provide satisfying performance interactive setting. interactionbased computing valid domains wrt. multiple cost restrictions novel additioninteraction modes within multiple-criteria decision making (Figueira et al., 2005).provide explicit control variable assignments well cost functions.future plan investigate compiled representations deliveringcost configuration might efficient investigate practical approaches processingnon-unary cost functions. particular, plan examine whether existing methods134fiInteractive Cost Configuration Decision Diagramsmultiobjective non-unary optimization (e.g., Rollon & Larrosa, 2006; Dubus et al., 2009)adopted operate MDD representation solution space.Acknowledgmentswould like thank anonymous reviewers extensive comments helpedus improve paper. would also like thank Erik van der Meer providingT-shirt example. first version paper created Tarik HadzicUniversity Copenhagen, updated version made Cork ConstraintComputation Centre support IRCSET/Embark Initiative Postdoctoral Fellowship Scheme.ReferencesAmilhastre, J., Fargier, H., & Marquis, P. (2002). Consistency Restoration ExplanationsDynamic CSPs-Application Configuration. Artificial Intelligence, 135 (1-2), 199234.Bahar, R., Frohm, E., Gaona, C., Hachtel, E., Macii, A., Pardo, A., & Somenzi, F. (1993).Algebraic decision diagrams applications. IEEE/ACM International Conference CAD, pp. 188191.Bartzis, C., & Bultan, T. (2003). Construction efficient BDDs bounded arithmeticconstraints. Garavel, H., & Hatcliff, J. (Eds.), TACAS, Vol. 2619 Lecture NotesComputer Science, pp. 394408. Springer.Bodlaender, H. L. (1993). tourist guide treewidth. Acta Cybernetica, 11, 123.Bollig, B., & Wegener, I. (1996). Improving variable ordering OBDDs NP-complete.Computers, IEEE Transactions on, 45 (9), 9931002.Bryant, R. E. (1986). Graph-Based Algorithms Boolean Function Manipulation. IEEETransactions Computers, 35, 677691.Bryant, R. E., & Chen, Y.-A. (1995). Verification Arithmetic Circuits BinaryMoment Diagrams. Proceedings 32nd ACM/IEEE Design AutomationConference, pp. 535541.Darwiche, A., & Marquis, P. (2002). Knowledge Compilation Map. Journal ArtificialIntelligence Research, 17, 229264.Darwiche, A. (2002). Logical Approach Factoring Belief Networks. Fensel, D.,Giunchiglia, F., McGuinness, D., & Williams, M.-A. (Eds.), KR2002: PrinciplesKnowledge Representation Reasoning, pp. 409420 San Francisco, California.Morgan Kaufmann.Darwiche, A. (2003). differential approach inference Bayesian networks. JournalACM, 50 (3), 280305.135fiAndersen, Hadzic, & PisingerDechter, R. (1999). Bucket Elimination: Unifying Framework Reasoning. ArtificialIntelligence, 113 (1-2), 4185.Dechter, R., & Pearl, J. (1989). Tree-Clustering Constraint Networks. Artificial Intelligence, 38 (3), 353366.Drechsler, R., Sarabi, A., Theobald, M., Becker, B., & Perkowski, M. A. (1994). Efficientrepresentation manipulation switching functions based ordered Kroneckerfunctional decision diagrams. DAC 94: Proceedings 31st annual conferenceDesign automation, pp. 415419 New York, NY, USA. ACM.Drechsler, R. (2001). Binary decision diagrams theory practice. International JournalSoftware Tools Technology Transfer (STTT), 3 (2), 112136.Drechsler, R., Becker, B., & Ruppertz, S. (1996). K*BMDs: New Data StructureVerification. EDTC 96: Proceedings 1996 European conference DesignTest, p. 2 Washington, DC, USA. IEEE Computer Society.Dubus, J.-P., Gonzales, C., & Perny, P. (2009). Multiobjective Optimization using GAIModels. Boutilier, C. (Ed.), IJCAI, pp. 19021907.Ehrgott, M., & Gandibleux, X. (2000). Survey Annotated Bibliography Multiobjective Combinatorial Optimization. Spektrum, 22, 425460.Fahle, T., & Sellmann, M. (2002). Cost Based Filtering Constrained KnapsackProblem. Annals Operations Research, 115, 7393.Fargier, H., & Marquis, P. (2006). Use Partially Ordered Decision GraphsKnowledge Compilation Quantified Boolean Formulae. Proceedings AAAI2006, pp. 4247.Fargier, H., & Marquis, P. (2007). Valued Negation Normal Form Formulas. Proceedings IJCAI 2007, pp. 360365.Fargier, H., & Vilarem, M.-C. (2004). Compiling CSPs Tree-Driven AutomataInteractive Solving. Constraints, 9 (4), 263287.Figueira, J. R., Greco, S., & Ehrgott, M. (2005). Multiple Criteria Decision Analysis: StateArt Surveys. Springer Verlag, Boston, Dordrecht, London.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability-A Guide TheoryNP-Completeness. W H Freeman & Co.Hadzic, T., Subbarayan, S., Jensen, R. M., Andersen, H. R., Mller, J., & Hulgaard, H.(2004). Fast Backtrack-Free Product Configuration using Precompiled SolutionSpace Representation. Proceedings PETO Conference, pp. 131138. DTUtryk.Hadzic, T., & Andersen, H. R. (2006). BDD-based Polytime Algorithm Cost-BoundedInteractive Configuration. Proceedings AAAI 2006, pp. 6267.136fiInteractive Cost Configuration Decision DiagramsHadzic, T., Hansen, E. R., & OSullivan, B. (2008). Automata, MDDs BDDsConstraint Satisfaction. Proceedings ECAI 2008 Workshop InferenceMethods based Graphical Structures Knowledge.Huang, J., & Darwiche, A. (2004). Using DPLL efficient OBDD construction.Proceedings SAT 2004, pp. 127136.Huang, J., & Darwiche, A. (2005). DPLL trace: SAT knowledge compilation.Kaelbling, L. P., & Saffiotti, A. (Eds.), IJCAI, pp. 156162. Professional BookCenter.Ibarra, O., & Kim, C. (1975). Fast approximation algorithms knapsack sumsubset problem. Journal ACM, 22, 463468.Jensen, R. M. (2007). CLab: C++ library fast backtrack-free interactive productconfiguration. http://www.itu.dk/people/rmj/clab/.Kebschull, U., & Rosenstiel, W. (1993). Efficient graph-based computation manipulation functional decision diagrams. Design Automation, 1993, EuropeanEvent ASIC Design. Proceedings. [4th] European Conference on, 278282.Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer, Berlin,Germany.Lai, Y.-T., & Sastry, S. (1992). Edge-valued binary decision diagrams multi-level hierarchical verification. DAC 92: Proceedings 29th ACM/IEEE conferenceDesign automation, pp. 608613 Los Alamitos, CA, USA. IEEE Computer SocietyPress.Lichtenberg, J., Andersen, H. R., Hulgaard, H., Mller, J., & Rasmussen, A. S. (2001).Method configuring product. US Patent No: 7,584,079.Lind-Nielsen, J. (2001).BuDDy - Binaryhttp://sourceforge.net/projects/buddy.DecisionDiagramPackage.Mateescu, R., Dechter, R., & Marinescu, R. (2008). AND/OR Multi-Valued Decision Diagrams (AOMDDs) Graphical Models. Journal Artificial Intelligence Research,33, 465519.Meinel, C., & Theobald, T. (1998). Algorithms Data Structures VLSI Design.Springer.Meseguer, P., Rossi, F., & Shiex, T. (2006). Soft constraints. Rossi, F., van Beek,P., & Walsh, T. (Eds.), Handbook Constraint Programming, Foundations Artificial Intelligence, chap. 9, pp. 281328. Elsevier Science Publishers, Amsterdam,Netherlands.Miller, D. M., & Drechsler, R. (2002). Construction Multiple-Valued DecisionDiagrams. Proceedings 32nd International Symposium Multiple-ValuedLogic (ISMVL02), p. 245 Washington, DC, USA. IEEE Computer Society.137fiAndersen, Hadzic, & PisingerMller, J., Andersen, H. R., & Hulgaard, H. (2002). Product configuration internet.INFORMS Conference Information Systems Technology.Muller-Hannemann, M., & Weihe, K. (2001). Pareto Shortest Paths Often FeasiblePractice. WAE 01: Proceedings 5th International Workshop AlgorithmEngineering, pp. 185198 London, UK. Springer-Verlag.Nicholson, R., Bridge, D. G., & Wilson, N. (2006). Decision Diagrams: Fast FlexibleSupport Case Retrieval Recommendation. Proceedings ECCBR 2006,pp. 136150.Pargamin, B. (2003). Extending Cluster Tree Compilation non-Boolean variablesProduct Configuration: Tractable Approach Preference-based Configuration.IJCAI03 Workshop Configuration.Reinhardt, L. B., & Pisinger, D. (2009). Multi-Objective Multi-Constrained NonAdditive Shortest Path Problems. Computers Operations Research. Submitted.Technical report version available at: http://man.dtu.dk/upload/institutter/ipl/publ/publikationer%202009/rapport%2016.pdf.Rollon, E., & Larrosa, J. (2006). Bucket elimination multiobjective optimization problems. Journal Heuristics, 12 (4-5), 307328.Sanner, S., & McAllester, D. A. (2005). Affine Algebraic Decision Diagrams (AADDs)Application Structured Probabilistic Inference. Proceedings IJCAI 2005,pp. 13841390.Schuurman, P., & Woeginger, G. J. (2005). Approximation schemes tutorial.Moehring, R., Potts, C., Schulz, A., Woeginger, G., & Wolsey, L. (Eds.), LecturesScheduling. Forthcoming.Sellmann, M. (2004). Practice Approximated Consistency Knapsack Constraints.McGuinness, D. L., & Ferguson, G. (Eds.), AAAI, pp. 179184. AAAI Press /MIT Press.Somenzi, F. (1996). CUDD: Colorado university decision diagram package. ftp://vlsi.colorado.edu/pub/.Srinivasan, A., Kam, T., Malik, S., & Brayton, R. K. (1990). Algorithms discretefunction manipulation. International Conference CAD, pp. 9295.Subbarayan, S., Jensen, R. M., Hadzic, T., Andersen, H. R., Hulgaard, H., & Mller, J.(2004). Comparing two implementations complete backtrack-free interactiveconfigurator. Proceedings CP04 CSPIA Workshop, pp. 97111.Subbarayan, S. M. (2008). Exploiting Structures Constraint Solving. Ph.D. thesis,University Copenhagen, Copenhagen.Tafertshofer, P., & Pedram, M. (1997). Factored edge-valued binary decision diagrams.Formal Methods System Design, Vol. 10, pp. 243270. Kluwer.138fiInteractive Cost Configuration Decision DiagramsTarapata, Z. (2007). Selected multicriteria shortest path problems: analysis complexity, models adaptation standard algorithms. International Journal AppliedMathematics Computer Science, 17 (2), 269287.Trick, M. (2001). dynamic programming approach consistency propagationknapsack constraints. 3rd international workshop integration AItechniques constraint programming combinatorial optimization problems CPAI-OR, pp. 113124.Vempaty, N. R. (1992). Solving constraint satisfaction problems using finite state automata.Proceedings Tenth National Conference Artificial Intelligence, pp. 453458.Walsh, T. (2000). SAT v CSP. Dechter, R. (Ed.), Proceedings CP 2000, Lecture NotesComputer Science, pp. 441456.Wegener, I. (2000). Branching Programs Binary Decision Diagrams. Society Industrial Applied Mathematics (SIAM).Wilson, N. (2005). Decision diagrams computation semiring valuations.Proceedings Nineteenth International Joint Conference Artificial Intelligence(IJCAI-05), pp. 331336.139fiJournal Artificial Intelligence Research 37 (2010) 41-83Submitted 07/09; published 02/10Predicting Performance IDA* usingConditional DistributionsUzi Zahavizahaviu@cs.biu.ac.ilComputer Science DepartmentBar-Ilan University, IsraelAriel Felnerfelner@bgu.ac.ilDepartment Information Systems EngineeringDeutsche Telekom LabsBen-Gurion University, IsraelNeil Burchburch@cs.ualberta.caComputing Science DepartmentUniversity Alberta, CanadaRobert C. Holteholte@cs.ualberta.caComputing Science DepartmentUniversity Alberta, CanadaAbstractKorf, Reid, Edelkamp introduced formula predict number nodes IDA*expand single iteration given consistent heuristic, experimentally demonstrated could make accurate predictions. paper show that, addition requiring heuristic consistent, formulas predictions accuratelevels brute-force search tree heuristic values obey unconditional distribution defined used formula. proposenew formula works well without requirements, i.e., make accurate predictions IDA*s performance inconsistent heuristics heuristic valueslevel obey unconditional distribution. order achieve introduceconditional distribution heuristic values generalization unconditionalheuristic distribution. also provide extensions formula handle individualstart states augmentation IDA* bidirectional pathmax (BPMX), technique propagating heuristic values inconsistent heuristics used. Experimentalresults demonstrate accuracy new method variations.1. Introduction OverviewHeuristic search algorithms A* (Hart, Nilsson, & Raphael, 1968) IDA* (Korf,1985) guided cost function f (n) = g(n) + h(n), g(n) actual distancestart state state n h(n) heuristic function estimating cost nnearest goal state. heuristic h admissible h(n) dist(n, goal) every state ngoal state goal, dist(n, m) cost least-cost path n m. h(n)admissible, i.e. always returns lower bound estimate optimal cost, algorithmsguaranteed find optimal path start state goal state one exists.c2010AI Access Foundation. rights reserved.fiZahavi, Felner, Burch, & Holteimportant question ask many nodes expanded algorithmssolve given problem. major advance answering question work doneKorf, Reid, Edelkamp introduced formula predict number nodes IDA*expand (Korf & Reid, 1998; Korf, Reid, & Edelkamp, 2001). papers, formulapresent, predictions makes, referred KRE paper. PriorKRE, standard method comparing two heuristic functions compareaverage values, preference given heuristic larger average (Korf,1997; Korf & Felner, 2002; Felner, Korf, Meshulam, & Holte, 2007). KRE made substantialimprovement characterizing quality heuristic function distributionvalues. developed KRE formula based heuristic distributionpredict number nodes expanded IDA* searching specific heuristiccost threshold. Finally, compared predictions formula actualnumber nodes expanded IDA* different thresholds several benchmark searchspaces showed gave virtually perfect predictions. major advanceanalysis search algorithms heuristics.Despite impressive results, KRE formula two main shortcomings. firstKRE assumes addition admissible given heuristic also consistent.heuristic h consistent every pair states, n, h(m) h(n) dist(m, n).1heuristic consistent, heuristic values nodes children thus constrained similar heuristic value node. heuristic inconsistentconsistent, i.e. pair nodes n, h(m) h(n) > dist(m, n). Inconsistencyallows nodes children heuristic values arbitrarily larger smallernodes heuristic value. term inconsistency negative connotationsomething avoided, recent studies shown inconsistent heuristics easydefine many search applications produce substantial performance improvements(Felner, Zahavi, Schaeffer, & Holte, 2005; Zahavi, Felner, Schaeffer, & Sturtevant, 2007;Zahavi, Felner, Holte, & Schaeffer, 2008). reason, important extendKRE formula accurately predict IDA*s performance inconsistent heuristics,heuristics likely become increasingly important future applications.second shortcoming KRE formula works well levelssearch tree heuristic distribution follows equilibrium distribution (definedSection 3.1.2). always holds sufficiently deep levels search tree,heuristic values converge equilibrium distribution. addition, holdlevels heuristic values set start states distributed accordingequilibrium distribution. However, shown (in Section 3.2.2) KREformula inaccurate depths practical interest single start stateslarge sets start states whose values distributed according equilibriumdistribution. cases, heuristic values levels search treeactually examined IDA* obey equilibrium distribution applying KREcases result inaccurate predictions.main objective paper develop formula accurately predict numbernodes IDA* expand, given cost threshold, given heuristic set startstates, including currently covered KRE. first extend KREs idea1. general definition graph. case undirected graphs write consistencydefinition |h(m) h(n)| dist(m, n).42fiPredicting Performance IDA* using Conditional Distributionsheuristic distribution, unconditional, conditional distribution,probability specific heuristic value constant, KRE, conditionedcertain local properties search space. conditional distribution providesinsights behavior heuristic values search informed(in context search tree) specific heuristic value produced.allows better study heuristic behavior.Based conditional distribution develop new formula, CDP (Conditional Distribution Prediction), predicts IDA*s performance set start states (regardlessheuristic values distributed) desired depth (not necessarily large)whether heuristic consistent not. CDP recursive structure informationnumber nodes propagated root leaves search tree.experiments CDPs predictions least accurate KREs, CDP muchaccurate inconsistent heuristics sets start states non-equilibriumheuristic distributions. basic form, CDP particularly accurate single startstates. describe simple extension improves accuracy setting. Finally,adapt CDP make predictions IDA* augmented bidirectional pathmaxmethod (BPMX) (Felner et al., 2005). inconsistent heuristics used, BPMXuseful addition IDA*. prunes many subtrees would otherwise explored,thereby substantially reducing number nodes IDA* expands.Throughout paper provide experimental results demonstrating accuracyCDP scenarios using two benchmark domains used KREsliding-tile puzzle Rubiks Cube.simplicity discussion, assume paper edges cost 1.true many problem domains. generalization ideas case variable edgecosts straightforward, although practical implementation introduces additionalchallenges (briefly described Section 11.2).paper organized follows. Section 2 presents background material. Section 3derives KRE formula first principles discusses limitations. Section 4,notion conditional distribution heuristic values presented. new formula, CDP,presented Section 4.2. Section 5 discusses subtle important wayexperiments differ KREs. Experimental results presented Sections 6 7.extension CDP formula better handle single start states presented Section 8.Section 9 proposes technique, based CDP, estimating upper lower boundsnumber nodes IDA* expand given unconditional distribution. Section 10presents extension CDP predicting performance IDA* BPMX applied.Related work discussed Section 11, conclusions suggestions future workgiven Section 12. preliminary version paper appeared (Zahavi, Felner, Burch,& Holte, 2008).2. BackgroundTwo application domains used KRE demonstrate accuracy formula.experiments use exactly domains. section describewell search algorithm different heuristic functions usedexperiments.43fiZahavi, Felner, Burch, & Holte2.1 Problem DomainsTwo classic examples AI literature single-agent pathfinding problemsRubiks Cube sliding-tile puzzle.2.1.1 Rubiks CubeFigure 1: 3 3 3 Rubiks CubeRubiks Cube invented 1974 Erno Rubik Hungary. standard versionconsists 3 3 3 cube (Figure 1), different colored stickers exposedsquares sub-cubes, cubies. 20 movable cubies 6 stable cubiescenter face. movable cubies divided eight corner cubies,three faces each, twelve edge cubies, two faces each. Corner cubies moveamong corner positions, edge cubies move among edge positions.one 6 faces cube rotated 90, 180, 270 degrees relativerest cube. results 18 possible moves state. Since twistingface twice row redundant, branching factor first move reduced15. addition, movements opposite faces independent. example, twistingleft face right face leads state performing movesopposite order. Pruning redundant moves results search tree asymptoticbranching factor 13.34847 (Korf, 1997).goal state, squares side cube color. puzzlescrambled making number random moves, task restore cubeoriginal unscrambled state. 4 1019 different reachable states.2.1.2 Sliding-tile Puzzlessliding-tile puzzle consists square frame containing set numbered square tiles,empty position called blank. legal operators slide tilehorizontally vertically adjacent blank blank position. problemrearrange tiles random initial configuration particular desired goalconfiguration. state space grows exponentially size number tiles increases,shown finding optimal solutions sliding-tile problem NPcomplete (Ratner & Warmuth, 1986). two common versions sliding-tilepuzzle 3 3 8-puzzle, 4 4 15-puzzle. 8-puzzle contains 9!/2 (181,440)44fiPredicting Performance IDA* using Conditional Distributions1237124563458910 1167812 13 14 15Figure 2: 8-puzzle 15-puzzle goal statesreachable states, 15-puzzle contains 1013 reachable states. goal statespuzzles shown Figure 2.classic heuristic function sliding-tile puzzles called Manhattan Distance. computed counting number grid units tile displacedgoal position, summing values tiles, excluding blank. Sincetile must move least Manhattan Distance goal position, move changeslocation one tile one grid unit, Manhattan Distance lower boundminimum number moves needed solve problem instance.2.2 Iterative Deepening A*Iterative deepening A* (IDA*) (Korf, 1985) performs series depth-first searches, increasing cost threshold time. depth-first search, nodes n f (n)expanded. Threshold initially set h(s), start node. goalfound using current threshold, search ends successfully. Otherwise, IDA* proceedsnext iteration increasing minimum f value exceeded previousiteration.2.3 Pattern Databases (PDBs)powerful approach obtaining admissible heuristics create simplified version,abstraction, given state space use exact distances abstract spaceestimates distances original state space. type abstractions usepaper sliding-tile puzzles illustrated Figure 3. left sidefigure shows 15-puzzle state goal state. right side shows correspondingabstract states, defined erasing numbers tiles except 2, 3, 67. estimate distance goal state 15-puzzle, calculateexact distance abstract state corresponding abstract goal state.pattern database (PDB) lookup table stores distance abstract goalevery abstract state (or pattern) (Culberson & Schaeffer, 1994, 1998). PDB builtrunning breadth-first search2 backwards abstract goal whole abstractspace spanned. compute h(s) state original space, mappedcorresponding abstract state p distance-to-goal p looked PDB.2. description assumes operators cost. technique easily extendedcases operators different costs.45fiZahavi, Felner, Burch, & HoltePDB lookupState119510115 122414 13236783123234567678910 11136712 13 14 15Goal StateGoal Pattern(a)(b)Figure 3: Example regular lookupsexample, PDB 15-puzzle based tiles 2, 3, 6, 7 would contain entryevery possible way placing four tiles blank 16 puzzle positions.PDB could implemented 5-dimensional array, P DB, array indexeslocations blank tiles 2, 3, 6, 7 respectively. lookup stateshown Figure 3 would P DB[0][8][12][13][14] (the blank position 0, tile 2position 8, tile 3 position 12, etc.). paper, accessing PDB statedescribed referred regular lookup, heuristic value returnedregular lookup referred regular heuristic value.Pattern databases proven useful finding lower bounds combinatorialpuzzles (Korf, 1997; Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner, Korf, &Hanan, 2004; Felner et al., 2007). Furthermore, also proven usefulsearch problems, e.g., multiple sequence alignment (McNaughton, Lu, Schaeffer, & Szafron,2002; Zhou & Hansen, 2004) planning (Edelkamp, 2001a).2.4 Geometric Symmetriescommon practice exploit special properties state space enable additionalheuristic evaluations. particular, additional PDB lookups performed givensingle PDB. example, consider Rubiks Cube suppose PDB basedpositions cubies yellow face (the positions cubies dontmatter). Reflecting rotating puzzle enable similar lookups cubiesdifferent color (e.g., green, red, etc.) since puzzle perfectly symmetric respectcolor. Thus, 24 symmetric lookups PDB different heuristicvalues obtained lookups PDB. heuristic valuesadmissible given state puzzle.46fiPredicting Performance IDA* using Conditional Distributionsanother example, consider sliding-tile puzzle. line symmetry maindiagonal (assuming goal location blank upper left corner). configuration tiles reflected main diagonal reflected configurationshares attributes original one. reflections usually used usingPDBs sliding-tile puzzle (Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felneret al., 2004, 2007) looked PDB.2.5 Methods Creating Inconsistent Heuristicsconsistent heuristics, difference heuristic value neighboring nodesconstrained less equal cost connecting edge. inconsistentheuristics, constraint difference heuristic values neighboringnodes much larger cost edge connecting them.KRE formula designed work consistent heuristics therefore KREpapers report experiments done consistent heuristics only. contrast, newformula, CDP, works types heuristics including inconsistent heuristics. Therefore,paper, addition usual consistent heuristics regular PDB lookupsManhattan Distance also experiment inconsistent heuristics. previouslydescribed several methods producing inconsistent heuristics (Zahavi et al., 2007). Twoinconsistent heuristics used experiments Random selectionheuristics Dual evaluations.Random selection heuristics: well-known method overcoming pitfallsgiven heuristic employ several heuristics use maximum value (Holte,Felner, Newton, Meshulam, & Furcy, 2006). example, multiple heuristicsbased domain-specific geometric symmetries ones described above.using geometric symmetries additional storage costs associatedextra evaluations, even evaluations based PDBs.Although using multiple heuristics results improved heuristic value, thereforelikely reduce number nodes expanded finding solution, also increasestime required calculate heuristic values nodes, might increaseoverall running time search. Instead using available heuristicsevery heuristic calculation, one could instead choose consult one them,selection made either randomly systematically. oneheuristic consulted node, time-per-node virtuallyone heuristic available. Even individual heuristics consistent,heuristic values actually used inconsistent different heuristicsconsulted different nodes. showed (Zahavi et al., 2007) inconsistencygenerally reduces number expanded nodes compared using heuristicnodes almost low maximum heuristicscomputed every node. Rubiks Cube, randomly chose one 24different lookups PDB arise 24 lines symmetrycube.Dual evaluation: permutation state spaces Rubiks Cube, stateexists dual state sd located distance goal (Felner47fiZahavi, Felner, Burch, & Holteet al., 2005; Zahavi, Felner, Holte, & Schaeffer, 2006; Zahavi et al., 2008). Therefore,admissible heuristic applied sd also admissible s. puzzles studiedpaper permutation state spaces, dual state puzzlescalculated reversing role locations objects: regular state usesset objects indexed current location, dual state setlocations indexed objects contain. using PDBs, dual lookuplook sd PDB. Performing regular PDB lookups statesgenerated search produces consistent values. However, values producedperforming dual lookup inconsistent identity objectsqueried change dramatically two consecutive lookups. Duediversity, dual heuristic shown preferable regular heuristic (Zahaviet al., 2007). exact definition explanations dual lookup providedoriginal papers (Felner et al., 2005; Zahavi et al., 2006, 2008).important note three PDB lookups (regular, dual, random) consultPDB. Thus, need amount memory share overalldistribution heuristic values (Zahavi et al., 2007).3. KRE Formula Limitationssection begins short derivation KRE formula state spacesstate transitions cost 1. KRE describe generalized accountvariable edge costs (Korf et al., 2001).3.1 KRE formulagiven state IDA* threshold d, KRE aims predict N (s, d), number nodesIDA* expand uses start state complete searchIDA* threshold (i.e., searches depth terminate search goalencountered). writtenN (s, d) =XNi (s, d)(1)i=0Ni (s, d) number nodes expanded IDA* level threshold d.One way decompose Ni (s, d) product two termsNi (s, d) = Ni (s) Pex (s, d, i)(2)Ni (s) number nodes level BF Ssd , brute-force search tree (i.e.,tree created breadth first search without heuristic pruning) depth rootedstart state s, Pex (s, d, i) percentage nodes level BF Ssdexpanded IDA* threshold d.KRE, Ni (s) written Ni , i.e., without dependence start state s.perfectly correct state spaces uniform branching factor b, Ni (s)cases simply bi . state spaces non-uniform regular branching structure,48fiPredicting Performance IDA* using Conditional DistributionsKRE showed Ni could computed exactly using recurrence equations independent s. However, base cases recurrences KRE dependusing Ni instead Ni (s) reasonable strictly correct.3.1.1 Conditions Node Expansion IDA*understand Pex (s, d, i) treated KRE, necessary reflect conditionsrequired node expansion. node n level BF Ssd expanded IDA*satisfies two conditions:1. f (n) = g(n) + h(n) must less equal d. edges unit cost,g(n) = condition equivalent h(n) i. call nodes satisfycondition potential nodes potential expanded.2. n must generated IDA*, i.e., parent (at level 1) must expandedIDA*.KRE restricted analysis heuristics consistent proved casesecond condition implied first condition. words, givenheuristic consistent, nodes expanded IDA* level BF Ssd thresholdexactly set potential nodes level i.3 observation allows Equation 2rewrittenNi (s, d) = Ni (s) PP OT EN IAL (s, i, i)(3)PP OT EN IAL (s, i, v) defined percentage nodes level BF Ssd whoseheuristic value less equal v.Note although PP OT EN IAL(s, i, di) = Pex (s, d, i) given heuristic consistent, PP OT EN IAL (s, i, i) overestimates Pex (s, d, i) heuristic inconsistent,sometimes large amount (see Section 3.2.1).3.1.2 Approximating PP OT EN IAL (s, i, v)KRE use three different approximations PP OT EN IAL(s, i, v). KREs theoretical analysisPP OT EN IAL(s, i, v) approximated equilibrium distribution, denotePEQ (v). defined probability node chosen randomly uniformlyamong nodes given depth brute-force search tree heuristic value lessequal v, limit large depth (Korf et al. 2001, p. 208). KRE proved that,limit large d,XNi (s) PEQ (d i)i=0would converge N (s, d) given heuristic consistent. final formula (the KREformula) therefore:3. See section 3.2.1 discussion KRE formula consistent heuristics.49fiZahavi, Felner, Burch, & HolteN (s, d) =XNi (s) PEQ (d i)(4)i=0KRE contrasted equilibrium distribution overall distribution,defined probability state chosen randomly uniformly statesproblem heuristic value less equal v (p. 207). Unlike equilibriumdistribution, defined search tree, overall distribution propertystate space. overall distribution directly computed pattern database,one pattern database used entries corresponds numberstates original state space, approximated, complex settings,computing heuristic values large random sample states. KRE arguedRubiks Cube overall distribution heuristic defined single pattern databaseequilibrium distribution, sliding-tile puzzles, twodistributions different.heuristic used KREs experiments Rubiks Cube defined maximumthree pattern databases. individual pattern database, overall distributioncomputed exactly. KREs experiments distributions combined approximate PP OT EN IAL(s, i, v) assuming values three pattern databasesindependent.experiments sliding-tile puzzles, KRE defined three types states basedwhether blank located corner position, edge position, interiorposition, approximated PP OT EN IAL(s, i, v) weighted combination overall distributions states type. weights used level exactpercentages states different types level.experiments followed KRE precisely use overall distribution individual Rubiks Cube pattern databases weighted overall distribution describedsliding-tile puzzles. simplicity, reminder paper use phraseunconditional heuristic distribution4 notation P (v) refer probabilitynode heuristic less equal v. let exact context determinedistribution P (v) actually denotes, whether equilibrium distribution, overalldistribution, approximation PP OT EN IAL Pex . Likewise usep(v) (lower case p) denote P (v) P (v 1) (with p(0) = P (0)). p(v) probabilitystate heuristic value exactly v according distribution P .3.2 Limitations KRE FormulaKRE formula (Equation 4) two main shortcomings: (1) predictionsaccurate given heuristic inconsistent, (2) even consistent heuristicspredictions inaccurate individual start states sets start states whose heuristicvalues distributed according unconditional heuristic distribution, P (v).turn examine detail.50fiPredicting Performance IDA* using Conditional DistributionsConsistent4333 3expandedgeneratedgenerated5Inconsistent4 R334 63564 3 nFigure 4: Consistent versus inconsistent heuristics3.2.1 Inconsistent Heuristicsspecifically mentioned KRE papers one property required KRE analysisheuristic consistent. necessary KRE formula aimscount number potential nodes level BF Ssd . consistent heuristics,heuristic value neighboring states never changes change g-value,illustrated left side Figure 4 (where number inside node heuristicvalue). implies f -value nodes ancestor always less equalf -value node (i.e., f monotone non-decreasing along path search tree).Therefore, easy prove consistent heuristics ancestors potentialnode also potential nodes (Korf et al., 2001). Consequently IDA* expandpotential nodes BF Ssd . Hence, formula KRE aims countnumber potential nodes BF Ssd used predict number nodes IDA*expand given consistent heuristic.inconsistent heuristics reasoning apply. heuristic values neighboring states differ much cost edge connects them,thus f -values along path search tree guaranteed monotonicallynon-decreasing. Therefore, ancestors potential node guaranteedpotential nodes themselves, consequence potential node might nevergenerated. example, consider search tree right side Figure 4. numbersinside node show nodes heuristic value. Assume start node RIDA* threshold 5 (a node potential node f -value less equal5). 3 potential nodes depth 2 (all heuristic value 3). Considerpotential node n. path node node potential node(f (m) = 1 + 5 = 6 > 5), generated expanded. Therefore, node nnever generated, preventing IDA* expanding it. Since KRE formula countsnumber potential nodes, count node n thus overestimate numberexpanded nodes inconsistent heuristic used.amount KRE overestimates number nodes expanded IDA*inconsistent heuristic large. illustrate this, consider state spaceRubiks Cube PDB heuristic defined locations 6 (out 12) edgecubies. regular method looking heuristic value PDB produces consistentheuristic. discussed Section 2.5 two alternative PDB lookups produce inconsistent4. unconditional distinguish conditional distribution introduce Section 4.151fiZahavi, Felner, Burch, & Holte8910111213KRE2573,43145,801611,3858,161,064108,937,712Regular2773,62447,546626,7928,298,262110,087,215Dual365186,80992,0941,225,53816,333,931Random Symmetry263464,60861,617823,00310,907,276Table 1: Rubiks Cube - Number nodes expanded IDA* using regular, dual,random-symmetry PDB lookups different IDA* threshold corresponding KRE predictions.heuristics dual evaluation random selection multiple heuristics.Rubiks Cube 24 symmetries applied state create newway perform PDB lookup it. Thus, 24 heuristics Rubiks Cube basedPDB random-symmetry lookup chooses one randomly.three lookups (regular, dual, random-symmetry) consult PDBdistribution heuristic values, P (v), therefore KRE predictIDA* expand number nodes regardless whether regular, dual,random-symmetry lookup done. experimental results Table 1 showsubstantially different number nodes actually expanded practicemethods.row Table 1 presents results specific IDA* threshold (d). resultaverage 1, 000 random initial states, generated making 180 randommoves goal state. KRE column shows KRE prediction basedunconditional heuristic distribution. last three columns Table 1 show numbernodes IDA* expands performs either regular, dual, random-symmetry lookupPDB. KRE prediction within 8% actual number nodes expandedIDA* uses regular (consistent) PDB lookup (third column) substantiallyoverestimates number nodes expanded IDA* uses dual random-symmetryinconsistent lookups PDB (fourth fifth columns).3.2.2 Sets Start States Whose Heuristics Values Obeyunconditional heuristic distributionexplained above, KRE used unconditional heuristic distribution P (v) and,theoretical analysis, proved use KRE formula would give accurate predictions limit large depth. fact, accurate predictions occur soonheuristic distribution depth interest closely approximates P (v). happenslarge depths definition happen even shallow levels certaincircumstances. reason KRE able produce extremely accurate predictionsexperiments using unconditional heuristic distribution P (v) depthsstart states experiments report average predictions performances52fiPredicting Performance IDA* using Conditional Distributionslarge number randomly drawn start states. spaces used KREs experiments,heuristic distribution large random set start states closely approximatedP (v) distribution used. caused heuristic distributions levels closelyapproximate P (v).However, set start states heuristic values distributed accordingP (v), case non-random sets start states single start state,KRE expected make good predictions small depths. words,cases unconditional heuristic distribution P (v) expected goodapproximation Pex (s, d, i).Consider case single start state consistent heuristic. distributionheuristic values search tree close start state highly correlatedheuristic value start state, therefore search treesstart states different heuristic values. example, great deal pruning likelyoccur near top search tree start state large heuristic value, resultingfewer nodes expanded start state small heuristic value. Applying KREtwo states produce prediction, therefore inaccurateleast one them, uses unconditional heuristic distribution P (v)cases.h56789IDA*30,363,82918,533,50310,065,8386,002,0253,538,964KRE8,161,0648,161,0648,161,0648,161,0648,161,064Table 2: Results set 1,000 start states h-value shown first column(regular PDB lookup, IDA* threshold = 12)Table 2 demonstrates phenomenon Rubiks Cube one regular 6-edge PDBlookup IDA* threshold = 12. IDA* column shows average number nodesexpanded 1, 000 start states, heuristic value h given row. KREignores heuristic values start states predicts 8,161,064 nodesexpanded IDA* every start state. row = 12 Table 1 showsaccurate prediction performance averaged large random sample startstates, Table 2 see low start states small heuristic valueshigh ones large heuristic values.3.2.3 Convergence Heuristic Distributions Large Depthsdescribed above, KRE make accurate predictions level nodes levelactually obey unconditional heuristic distribution P (v). increases, distributionheuristic values start converge P (v). rate convergence depends uponstate space. believed fairly slow sliding-tile puzzles, faster53fiZahavi, Felner, Burch, & HolteRubiks Cube. convergence occurs IDA* threshold reached KREprovide accurate predictions set start states (including single start states).order experimentally test repeated KRE Rubiks Cube experiment but,addition using large set random start states, also looked individualperformance two start states, s6 , low heuristic value (6), s11 ,maximum value heuristic used experiment (11). KRE used8-6-6 heuristic takes maximum 3 different PDBs (one based 8 cornercubies two based 6 edge cubies each). heuristic admissible consistent.billion random states sampled estimate P (v) maximum value 11average value 8.898.KRE10111213141516171,51020,169269,2293,593,80047,971,732640,349,1938,547,681,506114,098,463,567Multiple start statesIDA*Ratio1,5010.9920,1511.00270,3961.003,564,4950.9947,961,6991.00642,403,1551.008,599,849,2551.01114,773,120,9961.01s653,262422,2563,413,54729,114,115259,577,9132,451,954,24024,484,797,237258,031,139,364Single start stateRatios110.030.058,5260.08162,6270.122,602,0290.1838,169,3810.26542,241,3150.357,551,612,9570.44 103,934,322,960Ratio2.371.661.381.261.181.131.10Table 3: Rubiks Cube - Max (8,6,6) PDBsTable 3 presents results. KRE column presents KRE predictionMultiple start states columns presents actual number states generated (averagedset random start states) IDA* threshold. columns copied KREjournal paper (Korf et al., 2001). Ratio columns Table 3 shows value predictedKRE formula divided actual number nodes generated. ratio foundclose 1.0 multiple start states, indicating KREs predictionsaccurate.results two individual start states tested shown Single startstate part table. Note states optimally solved depth 17, but,KRE, search depth run completion. cases KRE formulaaccurate small thresholds accuracy prediction increased thresholdincreased. threshold = 17 KRE prediction roughly factor 2 small s610% large s11 . large improvement smaller thresholds.predictions become even accurate depth continues increase.reason predictions improve larger values deeper depthsheuristic distribution within single level converges unconditional heuristic distribution. Using dashed dotted lines various types, Figure 5(a) shows distributionheuristic values seen states 0, 1, 2 4 moves away s6 . solid lineFigure 5(a) unconditional heuristic distribution. x-axis corresponds differentheuristic values y-axis shows percentage states specified depthheuristic values less equal x value. example depth 0 (which includes54fiPredicting Performance IDA* using Conditional Distributions120120Unconditional heuristic distributionDepth = 4Depth = 2Depth = 1Depth = 080100cumulative percentagecumulative percentage100Unconditional heuristic distributionDepth = 4Depth = 2Depth = 1Depth = 0604020806040200002468101202Heuristic value46810Heuristic value(a) Heuristic Distributions s6(b) Heuristic Distributions s11Figure 5: Convergence heuristic distributionsstart state only) heuristic value 6 seen (leftmost curve). depth 1,heuristic values 5, 6 7 seen (second curve left), on. figureshows heuristic distribution successive depths converges unconditionalheuristic distribution (rightmost curve Figure 5(a)). depth 17 (not shown), heuristic distribution probably quite close unconditional heuristic distribution, makingKRE prediction quite accurate even single start state.Figure 5(b) shows heuristic distributions nodes 0, 1, 2, 4 movesaway s11 . case unconditional heuristic distribution leftheuristic distributions shallow depths, heuristic distribution depth 0rightmost curve figure. Comparing parts (a) (b) Figure 5 seeconvergence unconditional heuristic distribution faster s11 s6 ,explains KRE prediction Table 3 accurate s11 .4. Conditional Distribution CDP Formulapresent new formula CDP (Conditional Distribution Prediction), overcomes two shortcomings KRE described previous section. important featureCDP extends unconditional heuristic distribution heuristic values P (v)used KRE conditional distribution.4.1 Conditional Distribution Heuristic Valuesconditional distribution heuristic values denoted P (v|context), contextrepresents local properties search tree neighborhood node influencedistribution heuristic values nodes children. Specifically, Pn (v) percentagenode ns children heuristic value less equal v, defineP (v|context) average Pn (v) nodes n satisfy conditions defined5512fiZahavi, Felner, Burch, & Holtecontext. P (v|context) interpreted probability node heuristicvalue less equal v produced node satisfying conditions specifiedcontext expanded. context empty denoted P (v) Section 3.use p(v|context) (lower case p) denote probability node heuristic valueequal v produced nodeP satisfying conditions specified contextexpanded. Obviously, P (v|context) = vi=0 p(i|context).4.1.1 Basic 1-Step Modelconditioning context combination local properties search tree,including properties node (e.g. heuristic value), operator appliedgenerate node, properties nodes ancestors search tree, etc. simplestconditional distribution p(v|vp ), probability node heuristic value equal vproduced node value vp expanded. call 1-step modelvalue conditioned nodes one step away only. special circumstances,p(v|vp ) determined exactly analysis state space heuristic,general must approximated empirically sampling state space.sampling method p(v|vp ) represented entry [v][vp ] two-dimensionalmatrix [0..hmax ][0..hmax ], hmax maximum possible heuristic value. buildmatrix first set values matrix 0. randomly generate statecalculate heuristic value vp . that, generate child state onetime, calculate childs heuristic value (v), increment [v][vp ]. repeatprocess large number times order generate large sample. Finally, dividevalue cell matrix sum column cell belongs to, entry[v][vp ] represents percentage children generated value v statevalue vp expanded.vpvp6v7891067891060.17 0.11 0.06 0.03 0.0270.36 0.38 0.33 0.25 0.190.00 0.44 0.70 0.67 0.0080.37 0.44 0.53 0.60 0.6290.00 0.00 0.09 0.32 0.8990.02 0.03 0.05 0.08 0.14100.00 0.00 0.00 0.01 0.11100.00 0.00 0.00 0.01 0.0160.30 0.11 0.00 0.00 0.0070.60 0.45 0.21 0.00 0.008v(a) Consistent heuristic(b) Inconsistent heuristicFigure 6: portion Conditional Distribution matrix Rubiks Cube consistentinconsistent heuristics56fiPredicting Performance IDA* using Conditional DistributionsFigure 6 shows bottom right corner two matrices 6-edge PDBRubiks Cube. left matrix (a) shows p(v|vp ) regular (consistent) lookupPDB right matrix (b) shows p(v|vp ) inconsistent heuristic createddual lookup PDB. matrix (a) tridiagonal neighboring valuescannot differ 1. example, states heuristic value 8children heuristics 7, 8 9; occur probabilities 0.21, 0.70 0.09respectively (see column 8). contrast, matrix (b) tridiagonal. column 8,example, see 6% time states heuristic value 8 childrenheuristic values 6.4.1.2 Richer ModelsIDA* expands node, eliminates children operator pruning.example, state spaces undirected operators, using studies,parent node would generated among nodes children IDA* would immediatelyprune away. Distribution p(v|vp ) take account. order takeconsideration necessary extend context conditional probability includeheuristic value parent node expanded (we refer parent nodegp). denote p(v|vp ,vgp ) call 2-step model conditionsinformation ancestors two steps away. p(v|vp ,vgp ) gives probabilitynode heuristic value equal v generated node expandedheuristic value vp parent node expanded heuristic valuevgp . estimated sampling way done estimate p(v|vp ), exceptsample generates random state, gp, neighbors,neighbors except eliminated operator pruning. Naturally, resultssampling 2-step model stored three-dimensional array.context conditional distribution extended ways well.sliding-tile puzzles, KRE conditions overall distribution type stateexpanded, type indicates blank corner, edge, interiorlocation. experiments sliding-tile puzzle below, extend p(v|vp ,vgp )type information: p(v, t|vp , tp ,vgp , tgp ) gives probability node typeheuristic value equal v generated node expanded heuristicvalue vp type tp expanded nodes parent heuristic value vgp type tgp .4.2 New Prediction Formula, CDP (Conditional Distribution Prediction)section use conditional distributions described develop CDP, alternative KRE formula predicting number nodes IDA* expandgiven heuristic, IDA* threshold, set start states. shown experimentally, new formula CDP overcomes limitations KRE works well inconsistentheuristics set start states arbitrary IDA* threshold.overall approach follows. Define Ni (s, d, v) number nodesIDA* generate level heuristic value equal v start stateIDA* threshold.Pdi Given Ni (s, d, v), number nodes IDA* expand levelthreshold v=0 Ni (s, d, v), and, N (s, d), total number nodes expandedcomplete iteration IDA* threshold levels, quantity ultimately57fiZahavi, Felner, Burch, & HolteP Pinterested in, di=0 div=0 Ni (s, d, v). summations v runsnodes heuristic values range [0 . . . i] expanded level i.Ni (s, d, v) could calculated exactly, formula would calculate N (s, d) exactlywhether given heuristic consistent not. However, general method efficiently calculating Ni (s, d, v) exactly. Instead, Ni (s, d, v) estimated recursivelyNi1 (s, d, v) conditional distribution; exact details depend conditionalmodel used given subsections follow. use Ni (s, d, v)denote approximation Ni (s, d, v). Section 4.5.1 describe conditionscalculation is, fact, exact, therefore produces perfect predictions N (s, d).general case predictions may perfect estimates.present time analytical tools estimating accuracy showexperimentally, estimates often accurate.4.3 Prediction Using Basic 1-Step Modelbasic 1-step conditional distribution p(v|vp ) used, Ni (s, d, v) estimatedrecursively follows:d(i1)Ni (s, d, v) Ni (s, d, v) =XNi1 (s, d, vp ) bvp p(v|vp )(5)vp =0bvp average branching factor nodes heuristic value vp , estimatedsampling process estimates conditional distribution.5 reasoningbehind equation Ni1 (s, d, vp )bvp total number children IDA* generatesvia nodes expands level 1 heuristic value equal vp . multipliedp(v|vp ) get expected number children heuristic value v. Nodeslevel i1 expanded heuristic value less equal (i 1),hence summation includes vp values range [0 . . . (i 1)]. restrictingvp less equal (i 1) every recursive application formula,ensure (even inconsistent heuristics) node counted levelancestors expanded IDA*. base case recursion, N0 (s, d, v), 1v = h(s) 0 values v.Based this, number nodes expanded IDA* given start state s, threshold d,particular heuristic predicted follows:CDP1 (s, d) =XdiXNi (s, d, v)(6)i=0 v=0set, S, start states given instead one start state, calculationidentical except base case recursion defined using start statesS. is, define N0 (S, d, v) equal k k states heuristicvalue v. rest formula remains (with substituted everywhere).5. general case equation branching factor depends context defines conditional distribution. Since 1-step model, context heuristic value v, formallyallow branching factor depend it. practice, branching factor usuallyheuristic values.58fiPredicting Performance IDA* using Conditional Distributions4.4 Prediction Using Richer Models2-step conditional distribution p(v|vp , vgp ) used, define Ni (s, d, v, vp )number nodes IDA* generate level heuristic value equal vnodes level 1 heuristic value vp start state IDA*threshold. Ni (s, d, v, vp ) estimated recursively follows:d(i2)Ni (s, d, v, vp ) Ni (s, d, v, vp ) =XNi1 (s, d, vp , vgp ) bvp ,vgp p(v|vp , vgp )(7)vgp =0bvp ,vgp average branching factor nodes heuristic value vp parentheuristic value vgp . base case 2-step model level 1, level 0.N1 (s, d, v, vp ) 0 vp 6= h(s), number children start stateheuristic value v vp = h(s). Based 2-step model number nodes expandedIDA* given start state s, threshold d, particular heuristic predictedfollows:CDP2 (s, d) =Xdi d(i1)XXi=0 v=0Ni (s, d, v, vp )(8)vp =0set start states instead one, base case N1 (S, d, v, vp ),number children heuristic value v states heuristic value vp .Analogous definitions Ni CDP used definition context.example, using 1-step model set state types, one would define Ni (s, d, v, t)number nodes type IDA* generate level heuristic valueequal v, estimate recursively follows:d(i1)Ni (s, d, v, t) Ni (s, d, v, t) =XXvp =0tpNi1 (s, d, vp , tp ) bvp ,tp p(v, t|vp , tp )(9)Based model number nodes expanded IDA* given start state s, thresholdd, particular heuristic predicted follows:CDP(s, d) =Xdi XXNi (s, d, v, t)(10)i=0 v=0 tT4.5 Prediction Accuracyaccuracy predictions arbitrarily good arbitrarily bad dependingaccuracy conditional model used. following subsections examineextreme cases.principle, extending context never decrease accuracy predictionsadditional information taken account. However, conditional modelestimated sampling, extended context result poorer predictionsfewer samples context. explanation 1-step modelaccurate 2-step model rows h = 6 h = 9 Table 7 Section 6.2below.59fiZahavi, Felner, Burch, & Holte4.5.1 Perfect PredictionsConsider definition context includes heuristic value node expanded (vp contexts defined above) contains sufficient information allowoperator pruning correctly accounted for. use notation (v, x) referspecific instance context, v heuristic value node expandedx instantiation information context (e.g., state typeinformation last model above). general form predictive modelcontextCDP(s, d) =XdiXi=0 v=0XNi (s, d, v, x)(11)x(v, x)instancecontextd(i1)Ni (s, d, v, x) =XXvp =0xp(vp , xp )instancecontextNi1 (s, d, vp , xp ) bvp ,xp p(v, x|vp , xp )(12)bvp ,xp average branching factor, operator pruning, nodes satisfyingconditions context (vp , xp ), p(v, x|vp , xp ) average nodes n satisfyingconditions context (vp , xp ) pn (v, x), percentage ns children, operatorpruning, satisfy conditions context (v, x).If, every context (vp , xp ), nodes n satisfying conditions defined (vp , xp )exactly branching factor bvp ,xp exactly value pn (v, x)contexts (v, x), simple proof induction starting correctness base cases,N1 (s, d, v, x), shows Ni (s, d, v, x) = Ni (s, d, v, x) i, i.e., predictionmethod correctly calculates exactly many nodes satisfy conditionscontext every level search tree. follows CDP(s, d) exactlynumber nodes IDA* expand given start state IDA* threshold d.practical setting predictions 2-step model guaranteedperfect reasoning following conditions hold:1. heuristic defined exact distance goal abstract state space,case single pattern database used.2. two states, s1 , s2 , map abstract state x setoperators {op1 , ..., opk } apply them,3. states s1 s2 map abstract state x, operators op {op1 , ..., opk }apply s1 s2 , s1 child op(s1 ) s2 child op(s2 ) map abstractstate, op(x).60fiPredicting Performance IDA* using Conditional DistributionsDefine context node heuristic value abstract state maps.Condition (2) guarantees every context (v, x), nodes satisfying conditions(v, x) exactly branching factor bv,x . true nodes n1n2 satisfy conditions context (v, x), map abstract statex, condition (2) requires exactly set operators applyboth. Conditions (2) (3) together guarantee every context (vp , xp ), nodessatisfying conditions (vp , xp ) exactly value pn (v, x) v x.true nodes n1 n2 satisfy conditions context (vp , xp ),map abstract state xp , set operators applies both, operatorop creates child that, cases, maps specific abstract state, op(xp ). Thereforepercentage children map particular abstract staten1 n2 .straightforward implementation prediction method setting associatescounter abstract state, initialized number start states mapabstract state. counter abstract state x updated value(1 d) adding it, operator op, current value counterabstract state op(y) = x. algorithm computational complexityO(d |A| 2 ) |A| number abstract states effective branchingfactor abstract space. complexity depends linearly d, contrasttypically exponential dependency number nodes IDA* expand,sufficiently large prediction arbitrarily faster compute searchitself. example, PDB 15-puzzle based positions 8 tilesblank (roughly 4 billion abstract states), prediction 1000 start states = 52takes 6% time required execute search.exact prediction setting two potential uses. first determinesearching single PDB feasible not. example, calculation might showeven first iteration IDA* (with threshold h(start)) takeyear complete. second use prediction compare actual performancealternative method executed set start states (e.g. taking maximumset PDBs) performance using single PDB without actually executeIDA* search single PDB.4.5.2 Poor Predictionspredictions made conditional model extremely inaccurate distributionheuristic values independent information supplied context. illustrateexample based 4x3 sliding-tile puzzle two heuristics, PDB basedlocations tiles 1-7 blank, heuristic returns 0 every state.given state blank goal position, position even numbermoves goal position, heuristic value state taken PDB.states heuristic value 0. search tree, heuristic used leveltherefore opposite one used level 1.1-step model situation clearly hopeless predicting heuristicdistribution levels PDB used sufficiently largedistribution level converges unconditional distribution.61fiZahavi, Felner, Burch, & Holtehope 2-step model could make reasonably accurate predictionsPDB, considered itself, defines consistent heuristic therefore distribution heuristic values nodes children somewhat correlated heuristicvalue nodes parent.tested using 4x3 sliding-tile puzzle, small enough couldbuild 2-step model using states state space error introducedsampling process. test prediction accuracy model generated50,000 solvable states random and, explained detail next section, usedstate start state combination IDA* threshold IDA* would actuallyexecuted iteration threshold given state start state. meansdifferent number start states might used value d. Num columnTable 4 indicates many start states used value (first column)included table results 5,000 start states used.IDA* column shows average number nodes expanded IDA* startstates used Prediction column shows number predicted2-step model. Ratio column Prediction divided IDA*. One clearly seeimprovement predictions increases. even deepest depthsample provided 5,000 start states, prediction factor 6 smallertrue value. course, using constant heuristic value 0 alternate levelssomething one would practice, obtained similar results, essentiallyreason, 15-puzzle switching, one level next, patterndatabase based tiles 1-7 pattern database based tiles 9-15 (see Section 7.1).27282930313233343536373839IDA*1,2121,5292,3403,0724,8186,60710,74815,18424,61336,72660,77996,077152,079CDP2Prediction Ratio480.04630.04900.041310.042080.043380.055850.051,0270.071,8960.083,5130.106,7370.1112,9410.1325,1190.17Num5,7547,7809,08611,56112,39714,10914,10914,54513,49212,26110,4058,3556,505Table 4: 4x3 sliding-tile puzzle, alternating good heuristic 0.5. Experimental Setupnext two sections describe experimental results obtained running IDA*comparing number nodes expanded number predicted KRE62fiPredicting Performance IDA* using Conditional DistributionsCDP. experimented two application domains used KRE, namely, RubiksCube (Section 6) sliding-tile puzzle (Section 7). domain evaluatedaccuracy two formulas, consistent inconsistent heuristics, setsolvable start states generated random.experiments reported here, start states used given IDA* thresholdsubject special condition. State used start state combinationthreshold IDA* actually performs iteration threshold startstate. example, would use start state = 17 distance 11goal h(s) > 17. addition, sliding-tile puzzle, start state wouldused IDA* threshold h(s) different parity. contrast,experiments KRE paper restrict choice start states way,start states used every IDA* threshold .difference start states chosen large impact numbernodes IDA* expands. Table 5 illustrates 15-Puzzle using ManhattanDistance heuristic IDA* threshold (first column) 43 50. nodescolumn Unrestricted shows number nodes IDA* expanded average50,000 randomly generated solvable start states. values column closeagreement corresponding results Table 5 KRE paper (Korf et al., 2001).number column shows many start states satisfy additional condition.remove start states violate condition, IDA* expands substantially fewernodes average, shown nodes column Restricted, differenceincreases increases. = 50 almost order magnitude differencenumber nodes expanded two settings. difference needs keptmind making comparisons experimental results reported KREpapers.4344454647484950Unrestrictednodes439,9421,014,9411,985,5654,542,2498,963,74720,355,11040,479,72591,329,281Restrictednumbernodes22,525219,00122,484393,40622,937688,11922,266 1,182,52222,243 2,108,76621,028 3,508,48220,389 6,037,06418,758 9,904,973Table 5: 15-Puzzle Manhattan Distance. effect nodes expanded start statesrandomly chosen subject condition.63fiZahavi, Felner, Burch, & Holte6. Experimental Results Rubiks Cubebegin Rubiks Cube experiments. heuristic used 6-edge PDBheuristic described (Section 3.2.1). experimented (consistent) regularlookup (inconsistent) random-symmetry dual lookups PDB.CDP formula, two models used, CDP1 CDP2 , denote 1-step 2-stepmodels, respectively.outlined Section 4.1.1, conditional distribution tables built generating one billion states (each generated applying 180 random moves goal state),computing neighbors, incorporating heuristic information matrix representing one-step model. two-step model also generatedgrandchildren used heuristic information.addition, order get reliable samples added following two procedures:generating children grandchildren sampling used pruningtechniques based operator ordering used main search (seedescription Section 2.1.1). is, use sequence operatorswould generated main search. done looking randomwalk led initial state using last operator random walkbasis operator pruning.order get reliable sample need entry table sufficientlysampled. entries table low frequency. example, statesheuristic value 0 rare even sample billion states causingtable 0 row generated small sample. Therefore, enrichedentries artificially creating random states heuristic value 0.under-sampled entries sampled similar way. One technique, example,creating (with high probability) random state heuristic value x,perform random walk length x random state heuristic value 0.6.1 Rubiks Cube Consistent HeuristicsTable 6 compares KRE CDP1 CDP2 . accuracy three prediction methodscompared using regular lookups 6-edge PDB. Results rowaverages set 1000 random states. row presents results IDA* iteration8910111213IDA*2773,62447,546626,7928,298,262110,087,215KREPrediction2573,43145,801611,3858,161,064108,937,712Ratio0.930.950.960.980.980.99CDP1Prediction Ratio2350.853,1510.8741,5990.87546,8080.877,188,8630.8794,711,2340.86CDP2Prediction Ratio2570.933,4460.9545,9850.97613,3320.988,180,6760.99109,133,0210.99Table 6: Rubiks Cube consistent heuristic.64fiPredicting Performance IDA* using Conditional Distributionsdifferent threshold (d), given first column. second column (IDA*) presentsactual number nodes expanded IDA* threshold. next columns reportpredictions accuracy (Ratio) prediction defined ratiopredicted number actual number expanded nodes. reportedKRE paper, KRE formula found accurate consistent heuristicaveraged large set random start states. table shows CDP1 reasonablyaccurate systematically underestimates one-step model considernodes parent included among children. elaborate below.CDP2 predictions accurate, slightly accurate KREs.6.2 Rubiks Cube Start States Specific Heuristic ValuesTable 2, presented (Section 3.2.2), related discussion, show KRE mightmake accurate predictions start states restricted specific heuristicvalue h. particular example shown (IDA* threshold 12) KRE always predictvalue 8, 161, 064, exact value depends specific set start states usedIDA* threshold 12 sufficiently large number nodesindependent start states. Table 7 extends Table 2 include predictions CDP.shows versions CDP substantially outperform KRE particular setstart states.h56789IDA*30,363,82918,533,50310,065,8386,002,0253,538,964KREPrediction Ratio8,161,0640.278,161,0640.448,161,0640.818,161,0641.368,161,0642.31CDP1Prediction Ratio48,972,6191.6117,300,4760.937,918,8210.795,094,0180.853,946,1461.12CDP2Prediction Ratio20,771,8950.6813,525,4250.739,131,3030.916,743,6861.125,240,4251.48Table 7: Results different start state heuristic values (h) regular PDBIDA* threshold = 12.6.3 Rubiks Cube Inconsistent Heuristicsexperiments repeated inconsistent heuristics. dual randomsymmetry lookups performed 6-edge PDB instead regular lookup, therebycreating inconsistent heuristic. discussed Section 3.2.1, KRE producesprediction heuristics (consistent inconsistent) derived single PDBoverestimates inconsistent heuristics. Table 8 shows CDP2 extremely accurate.prediction always within 2% actual number nodes expanded.1-step model used CDP1 systematically underestimates actual numbernodes expanded regular dual lookups (see regular lookup Table 6dual lookup Table 8). understand why, consider happens noderight side Figure 7 expanded. generates two children, node n (assuming65fiZahavi, Felner, Burch, & HolteKREPredictionIDA*8910111213365186,80992,0941,225,53816,333,9318910111213263464,60861,617823,00310,907,276CDP1Ratio Prediction RatioDual2577.14310.863,4316.624180.8145,8016.735,5560.82611,3856.6474,0370.808,161,0646.66987,6660.81108,937,7126.67 13,180,9600.81Random Symmetry2579.88261.003,4319.923531.0245,8019.944,7181.02611,3859.9262,9901.028,161,0649.92840,8491.02108,937,7129.99 11,224,1081.03CDP2Prediction Ratio365086,79290,6641,210,22516,154,6401.000.981.000.980.990.99263464,60161,174815,44410,878,2271.001.001.000.990.991.00Table 8: Rubiks Cube dual, random-symmetry (inconsistent) heuristicsoperators inverses case Rubiks Cube) copy parent R (shown msleft child Figure 7). child 2 levels deeper R therefore f -value2 greater Rs. IDA* threshold 5, child potential node1-step model conclude generate potential child probability0.5, whereas fact children remain operator pruning potentialnodes.4 R343 nFigure 7: 1-step model may underestimatereason 1-step model underestimate number nodes expandedrandom-symmetry lookups done child copy R constrainedheuristic value R different symmetries could chosendifferent occurrences R. childs f -value correlation f -value Rexplanation CDP1 underestimates apply.fact, different copies state uncorrelated h-values effect operatorpruning needs taken account reduces number children,done well within 1-step model calculating branching factor.may advantages using wider context 2-step model resultsrandom-symmetry heuristic show minor case.66fiPredicting Performance IDA* using Conditional Distributions7. Experimental Results - Sliding-Tile PuzzleKRE experiments sliding-tile puzzle, three state types used, basedwhether blank corner, edge, interior location. used state typesexperiments used exact recurrence equations N (s, v, d, t) type-dependentversion KRE formula. heuristic used Manhattan Distance (MD). experimented 2-step CDP includes type system recurrence equations.Results 1-step CDP included performed poorly early versions experiments.8-puzzle conditional distribution P (v, t|vp , tp ,vgp , tgp ) needed CDP2typed unconditional distribution P (v, t) needed type-dependent KRE formulacomputed enumerating states 8-puzzle reachable goal.15-puzzle, possible exhaustive enumeration entire statespace conditional distributions estimated generating ten billion reachablestates random. uniform random sample used estimate P (v, t) KRE,state sample used gp sampling method described Section 4.1.2P (v, t|vp , tp ,vgp , tgp ). latter, however, basic sampling method extendedeven processing ten billion gp states entries 6-dimensionalmatrix missing sampled sufficiently. correct this, generategp, children, grandchildren update matrix accordingly, checkmatrix already contains data gps great-grandchildren. generategps great-grandchildren update corresponding entries matrix. continueslong encounter contexts never seen before. introduces smallstatistical bias sample, guarantees sample contains requireddata.h#States121416182011,45419,42618,52810,0992,7193436384042441,3312,3302,9993,0282,4541,507KREIDA*PredictionRatio8-puzzle depth 221,4991,3910.931,0421,4041.356601,4192.153771,4473.841681,5038.9515-puzzle depth 5277,028,888 420,858,2505.4638,206,986 424,113,56111.1016,226,330 428,883,70026.436,310,724 433,096,51468.632,137,488 438,475,079 205.14620,322 444,543,678 716.63CDP2Prediction Ratio1,8091,051544246911.211.010.820.650.54172,845,55964,247,27521,505,4266,477,9031,749,231409,3412.241.681.331.030.820.66Table 9: sliding-tile puzzles consistent heuristic (MD).Prediction results KRE CDP2 8- 15-puzzles shown Table 9format above. 8-puzzle predictions made IDA* threshold67fiZahavi, Felner, Burch, & Holte22 row corresponds group 8-puzzle states heuristicvalue h (shown first column) IDA* would actually used threshold22. second column gives number states group. Clearly, shownIDA* column, states higher initial heuristic values IDA* expanded smallernumber nodes. trend reflected KRE predictions since KRE takeh account. KRE difference attributes different rowsdifferent type distribution given group. Thus, predicted number expandednodes KRE similar rows (around 1,400). CDP formula takes heuristicvalue start state account able predict number expanded nodesmuch better KRE. bottom part Table 9 show results 15-puzzleIDA* threshold 52. Similar tendencies observed.7.1 Inconsistent Heuristics Sliding-tile Puzzlenext experiment inconsistent heuristic 8-puzzle. defined two PDBs,one based location blank tiles 14, based locationblank tiles 58. create inconsistent heuristic, one PDBs consultedregular lookup. choice PDB made systematically, randomly, basedposition blank. Different occurrences state guaranteedlookup neighboring states guaranteed consult different PDBscauses inconsistency. results presented Table 10 variety IDA* thresholds.threshold Num column indicates many start states used.results show CDPs predictions reasonably accurate, much accurateKREs overestimate factor 26.181920212223242526272829Num44,24340,77360,94448,88860,34540,89442,03122,49418,6687,0364,131762IDA*14.522.227.443.358.595.4135.7226.7327.8562.0818.41,431.7KREPrediction80.4151.5244.2459.0734.41,383.62,200.64,155.36,569.912,475.019,515.737,424.6Ratio5.566.828.9110.5912.5514.5016.2118.3320.0422.2023.8526.14CDP2Prediction Ratio10.40.7216.10.7320.20.7432.10.7444.00.7572.50.76103.40.76174.20.77251.00.77432.20.77618.80.761,074.80.75Table 10: Inconsistent heuristic 8-puzzle.Similar experiments conducted 15-puzzle. Here, first PDB basedlocation blank tiles 17, based locationblank tiles 915. Table 11 shows results IDA* thresholds 48 55 (recallmedian solution length puzzle 52). numbers shown averages68fiPredicting Performance IDA* using Conditional Distributions50,000 start states. CDP predictions 15-puzzle considerably worse8-puzzle, KRE predictions degraded much more. reasoninaccuracy predictions discussed Section 4.5.2. Much accuratepredictions produced context extended include heuristic valuepattern databases, one search algorithm actually consults.4849505152535455IDA*231,939.6388,201.1644,350.11,062,597.51,746,025.12,773,611.64,539,767.07,546,286.9KREPrediction311,462,527.1664,920,142.21,413,202,357.93,014,405,997.56,404,191,951.413,639,455,787.329,035,096,650.961,899,533,064.7Ratio1,342.91,712.82,193.22,836.83,667.94,917.66,395.78,202.6CDP2Prediction71,550.2149,257.5313,132.4663,004.41,402,898.22,985,321.16,361,011.513,627,941.8Ratio0.3080.3840.4860.6240.8031.0761.4011.806Table 11: Inconsistent heuristic 15-puzzle.8. Accurate Predictions Single Start Statesseen CDP works well base cases recursive calculationNi (s, d, v) seeded large set start states, matter heuristic valuesdistributed. However, actual number expanded nodes specific single startstate deviate number predicted CDP. conditional distribution reflectsexpected values nodes share context, single start stateinterest might behave differently average state context.Consider Rubiks Cube state heuristic value 8. CDP2 predicts IDA*expand 6, 743, 686 state IDA* threshold 12. Table 2 showsaverage (over 1, 000 start states heuristic value 8) 6, 002, 025 states expanded.Examining results individual start states showed actual numberexpanded nodes ranged 2, 398, 072 15, 290, 697 nodes.order predict number expanded nodes single start state proposefollowing enhancement CDP. Suppose want predict number expandednodes IDA* threshold start state s. First, perform small initial searchdepth r. use states depth r seed base cases CDP formulacompute formula IDA* threshold r. cause larger set nodesused calculating Ni (s, d, v), thereby improving accuracy CDPs predictions.8.1 Rubiks Cube, 6-edge PDB HeuristicTable 12 shows results four specific Rubiks Cube states heuristic value 8 (ofregular 6-edge PDB lookup) IDA* threshold set 12. chosestates least greatest number expanded nodes two states aroundmedian. first column shows actual number nodes IDA* expands state.69fiZahavi, Felner, Burch, & Holtenext columns show number expanded nodes predicted enhanced CDP2formula initial search performed depths (r) 0, 2, 5 6. Clearly,initial searches give much better predictions original CDP2 (with r = 0),predicts 6, 743, 686 states. initial search depth 6, predictionsaccurate.h8888IDA*2,398,0724,826,1549,892,37615,290,697CDP2 (r=0)6,743,6866,743,6866,743,6866,743,686CDP2 (r=2)4,854,4857,072,9528,555,1709,432,008CDP2 (r=5)3,047,8365,495,4759,611,32513,384,290CDP2 (r=6)2,696,5325,184,4539,763,45514,482,001Table 12: Single state (d = 12).8.2 Rubiks Cube, 8-6-6 HeuristicSection 3.2.3 presented KRE predictions two start states, s6 , heuristic value6, s11 , heuristic value 11, Rubiks Cube 8-6-6 heuristic.repeat experiments CDP1 . Tables 13 14 show results initialsearch depth (r) 0 4. tables show CDP1 able achieve substantiallybetter predictions KRE cases, initial search depth 4 usuallyimproved CDP1 predictions.1011121314151617IDA*53,262422,2563,413,54729,114,115259,577,9132,451,954,24024,484,797,237258,031,139,364KRE1,51020,169269,2293,593,80047,971,732640,349,1938,547,681,506114,098,463,567Ratio0.030.050.080.120.180.260.350.44CDP1 (r=0)32,207246,1581,979,41716,690,055149,319,0611,435,177,44514,925,206,678167,181,670,892Ratio0.600.580.580.570.580.590.610.65CDP1 (r=4)69,770690,5565,422,00142,650,077345,370,1482,934,134,12526,380,507,927254,622,231,216Ratio1.311.641.591.461.331.201.080.99Table 13: 8-6-6 PDB, single start state s68.3 Experiments 8-Puzzle - Single Start Statesperformed experiments enhanced CDP2 formula states 8-puzzle(consistent) MD heuristic. use term trial refer pair singlestart state given IDA* threshold d. trials included possible valuesstart states IDA* would actually perform search IDA* thresholdd. Predictions made trial separately, relative error, predicted/actual,trial calculated. results shown Figure 8. four curvesfigure, KRE, CDP, enhanced CDP initial search depths (r) 570fiPredicting Performance IDA* using Conditional Distributions11121314151617IDA*8,526162,6272,602,02938,169,381542,241,3157,551,612,957103,934,322,960KRE20,169269,2293,593,80047,971,732640,349,1938,547,681,506114,098,463,567Ratio2.371.661.381.261.181.131.10CDP1 (r=0)8,246191,0773,188,47047,281,091665,292,8649,125,863,883123,571,401,411Ratio0.971.171.231.241.231.211.19CDP1 (r=4)8,904139,4222,834,54245,690,554614,042,8658,544,807,943120,978,148,822Ratio1.040.861.091.201.131.131.16Table 14: 8-6-6 PDB, single start state s11cumulative percentage100806040KRECDP2CDP2 (radius=5)CDP2 (radius=10)200012345678910predicted / actualFigure 8: Relative error 8-puzzle10. x-axis relative error. y-axis percentage trialsprediction relative error x less. example, y-value 20% KREcurve x = 0.5 means KRE underestimated factor 2 20%trials. rightmost point KRE plot (x = 10, = 94%) indicates 6%trials KREs prediction 10 times actual number nodes expanded.contrast CDP much larger percentage highly accurate predictions, 99%predictions within factor two actual number nodes expanded. figureclearly shows advantage using enhanced CDP. initial search depth10, 90% trials predictions within 10% correct number.9. Performance Range Given Unconditional Distributionexperiments paper used 6-edge PDB Rubiks Cube illustrated fact number nodes IDA* expands given PDB vary tremendouslydepending PDB used (Zahavi et al., 2007). see clearly, middlethree columns Table 15 show data already seen Tables 6 8, namely,number nodes IDA* expands 6-edge PDB used regular manner,71fiZahavi, Felner, Burch, & Holtedual lookups, random-symmetry lookups. IDA* expands ten times fewernodes 6-edge PDB consulted random-symmetry lookupsconsulted normal way.raises intriguing question range performance achievedvarying conditional distribution unconditional distribution fixed.8910111213CorrelationCDP2573,43145,801611,3858,161,064108,937,712Regular2773,62447,546626,7928,298,262110,087,2150.591Dual365186,80992,0941,225,53816,333,9310.359Random Symmetry263464,60861,617823,00310,907,2760.187CDP162102,81337,553501,2736,691,215Table 15: Range IDA* Performace 6-edge Rubiks Cube PDB9.1 Upper Limitupper extreme, results nodes expanded, occurs consistentheuristic used. IDA* expands potential nodes, maximumnumber nodes expanded conditional distribution parentevery potential node level potential node level 1. exact calculationnumber potential nodes brute-force tree therefore theoretical upper boundnumber nodes IDA* expand given unconditional distribution.already discussed, one way estimate number potential nodes useKRE formula. estimate upper bound number nodes IDA* couldexpand denoted CDP Table 15.Alternatively, number potential nodes approximated CDP formulagiven conditional distribution. Consider Equation 6. summation considerpossible vp values [0, d(i1)] nodes potential nodes level i1. Thusnodes expanded IDA* level 1 nodes generatechildren level i.6 Now, lets substitute vp [0, hmax ]. considernodes level 1, even ones potential nodes. Usingsummation calculate number nodes heuristic v level even onesactually generated IDA* (because parents potential nodes, i.e.vp > (i 1). shown Equation 13.Ni (s, v) =hXmaxNi1 (s, vp ) bvp p(v|vp )(13)vp =06. Note heuristic consistent vp values {v 1, v,v + 1} need consideredsummation nodes values vp (smaller v 1 larger v + 1) cannotgenerate children heuristic value v.72fiPredicting Performance IDA* using Conditional DistributionsUsing general prediction equation get:CDP =XdiXNi (s, v)(14)i=0 v=0gives alternative method approximate number potential nodes.methods approximate upper bound. practice, however, possiblenumber expanded nodes slightly exceed approximate bound due noisesmall errors sampling calculations.9.2 Lower Limitconsistent heuristics values neighboring states highly correlated.extreme cases correlation heuristic values neighboringnodes. is, heuristic value child node statistically independent heuristicvalue parent. means regardless parents heuristic value vp , heuristicvalues children distributed according unconditional heuristic distribution,i.e., p(v|vp ) = p(v).motivation using estimated lower bound number nodes IDA*could expand given unconditional distribution empirical observationnumber nodes IDA* expands decreases correlation parents heuristicvalue childrens heuristic values decreases.illustrated last row three middle columns Table 15, showscorrelation heuristic values neighboring states different typeslookups done 6-edge PDB. calculated using Pearsons correlation coefficient,defined n pairs x, values according following equationCorrelationxyPnPnxi yi i=1 xi i=1 yip Pn= p PnPnPnn i=1 x2i ( i=1 xi )2 n i=1 yi2 ( i=1 yi )2nPni=1(15)order calculate correlation, 60, 000 random pairs (xi ,yi ) neighboring statesgenerated. heuristic values computed used Equation 15. bottomrow Table 15 shows number nodes expanded decreases correlationneighboring heuristic values decreases. leads us suggest numbernodes expanded reach minimum correlation zero.7estimated lower bound calculated using CDP formula p(v|vp ) = p(v).denote CDP. 1-step model would calculated using followingequations:7. theory, possible heuristic negative correlation parents heuristic valuechildrens heuristic values, i.e., parents low heuristic values could tend childrenlarge heuristic values vice versa. believe unlikely occur practice.73fiZahavi, Felner, Burch, & Holted(i1)Ni (s, d, v) =XNi1 (s, d, vp ) bvp p(v)(16)vp =0CDP =diXXNi (s, d, v)(17)i=0 v=0seen comparing rightmost two columns Table 15, randomsymmetry use 6-edge PDB within factor two estimated minimumpossible number nodes expanded PDB, suggests substantiallyimprove upon performance one would use different PDB.Table 16 shows estimated upper lower bounds IDA*s performance, rangeIDA* thresholds, three different PDBs Rubiks Cube. bounds calculatedusing 1, 000 random start states. table shows that, according estimates, inconsistent heuristics based 5-edge PDB outperform consistent heuristics based6-edge PDB probably cannot outperform consistent heuristics based 7-edgePDB since estimated lower bound 5-edge PDB larger estimated upperbound 7-edge PDB.89101112135-edge PDBCDPCDP2,86913438,3552,278511,98230,6236,834,185408,77591,225,9205,456,5121,217,726,395 72,836,0796-edge PDBCDPCDP257163,43121045,8012,813611,38537,5538,161,064501,273108,937,712 6,691,2157-edge PDBCDPCDP4210348424,53529160,5353,829808,05151,11610,786,252 682,311Table 16: Estimated Bounds Performance three Rubiks Cube PDBs.10. Predicting Performance IDA* BPMXinconsistent heuristic, heuristic value child much largerparent. happens state space undirected edges, childs heuristicvalue propagated back parent. causes parents f -value exceedIDA* threshold entire search subtree rooted parent pruned withoutgenerating remaining children. propagation technique called bidirectionalpathmax (BPMX) (Felner et al., 2005; Zahavi et al., 2007). shown effectivereducing search effort pruning subtrees would otherwise explored.show modify CDP handle BPMX propagation. Since BPMX appliesstate spaces undirected edges, discussion section limited spaces.74fiPredicting Performance IDA* using Conditional Distributions10.1 Bidirectional Pathmax (BPMX)Traditional pathmax (Mero, 1984) propagates heuristic values parent children,applied state space. Admissibility preserved subtracting costconnecting edge heuristic value. basic insight bidirectional pathmax(BPMX) edges undirected heuristic values propagate neighbors,includes child node parent. process continue distancedirection. BPMX illustrated Figure 9. left side figure showsinconsistent heuristic values node two children. Consider left childheuristic value 5. Since value admissible edges example costone, immediate neighbors least 4 moves away goal, neighborsleast 3 moves away, on. left child generated, heuristic value(h = 5) propagate parent right child. preserveadmissibility, propagation along path reduces h cost traversing path.results h = 4 root h = 3 right child. using IDA*,bidirectional propagation may cause many nodes pruned would otherwiseexpanded. example, suppose current IDA* threshold 2. Without propagationh left child, root node (f = g + h = 0 + 2 = 2) right child(f = g + h = 1 + 1 = 2) would expanded. Using propagation, left childincrease parents h value 4, resulting search node abandonedwithout even generating right child.425513Figure 9: Propagation values inconsistent heuristics10.2 CDP Overestimates BPMX Appliedinconsistent heuristic used BPMX applied, CDP overestimatenumber expanded nodes count nodes subtrees BPMXprunes. Section 4.2, defined Ni (s, d, v) number nodes IDA*generate level heuristic value exactly equal v start stateIDA* threshold. formula given estimating Ni (s, d, v) (Equation 5) was:d(i1)Ni (s, d, v) =XNi1 (s, d, vp ) bvp p(v|vp )vp =0calculating Ni (s, d, v) Ni1 (s, d, vp ) formula assumes nodeexpanded children generated. Ni1 (s, d, vp ) multipliedbranching factor bvp . BPMX applied, child may prune parentrest children generated. happens, assumption childrenexpanded nodes generated would wrong. example, without BPMX,75fiZahavi, Felner, Burch, & Holteexpanding root left tree Figure 9 children generated childright also expanded. Indeed CDP count two nodes case. BPMXapplied root expanded child right generated (and thereforeexpanded). Thus, CDP, counts two nodes, overestimating numbernodes expanded. following section modify equation correct this.10.3 New Formula Estimating Ni (s, d, v)Let n node currently expanded. Assume n b childrenconsider order generated. call order generation order.Note BPMX applied, probability child generated decreasesmove generation order. Children appear late orderlarger chance generated since previous children mightcause BPMX cutoff. Let pbx (l) probability child location l ordergenerated even BPMX applied. definition extend Equation 5follows:d(i1) bvpNi (s, d, v) =X X{Ni1 (s, d, vp ) pbx (l) p(v|vp )}vp =0(18)l=1Ni (s, d, v) calculated similar way Equation 5, except way counttotal number children IDA* generates via nodes expands level 1heuristic value equal vp . idea iterate possible locationsgeneration order calculate probability node location l generated.practice, however, actual context pbx variables besides location l.also includes IDA* threshold (d), depth parent (i 1) heuristicvalue parent (vp ), thus get final formula:d(i1) bvpNi (s, d, v) =X X{Ni1 (s, d, vp ) pbx (l, d, 1, vp ) p(v|vp )}vp =0(19)l=1exactly equal Equation 5 special case pbx (l) = 1 l,happens BPMX used used consistent heuristic.10.4 Calculating pbxsimplicity, model assumes heuristic value propagated BPMXone level tree. means state pruned immediatechildren descendants deeper levels. make assumption anotherreason besides simplicity description. experiments Rubiks Cubedomains showed indeed almost pruning BPMX caused 1-level BPMXpropagation. generalized formula deeper BPMX propagations similarlydeveloped include complicated recursive terms low practical value,least state spaces heuristics studied.Assume c child n location l generation order. Child cgenerated n pruned l 1 children appear c76fiPredicting Performance IDA* using Conditional Distributionsgeneration order. Assume n level threshold d. Since nexpanded, h(n) i. BPMX h(n) increased (and cause BPMX pruning)child k h(k) > + 1. case, h(k) 1 larger i,used instead h(n) IDA* decide expand n additional childrengenerated. Therefore, order child c location l generation ordergenerated, l 1 predecessors generation order must heuristics lessequal + 1. Assuming heuristic value parent v probabilitypbx (l, d, i, v) = {di+1Xp(h|v)}l1(20)h=0sum probability relevant heuristic value raise sumpower l 1 since l 1 children appear c.10.5 Experiments Rubiks Cube BPMXrepeated experiments Rubiks Cube 6-edge PDB BPMXactivated. Since BPMX affects inconsistent heuristics, Dual RandomSymmetry heuristics tested. heuristic tested IDA* thresholds 813. results, averaged set 1, 000 random states, presentedTable 17. BPMX columns repeated Table 8. additional columnsshow results BPMX. column IDA* + BPMX presents actual numberexpanded nodes using BPMX. BPMX reduces number nodes expanded30% Dual 25% reduction Random Symmetry, makingunmodified CDP2 predictions high amount. CDPbx2 columnshows modifications introduced section greatly improve accuracy.IDA*8910111213365186,80992,0941,225,53816,333,9318910111213263464,60861,617823,00310,907,276BPMXCDP2BPMXRatio IDA* + BPMXCDPbx2Dual361.0026245080.983533286,7921.004,7004,38790,6640.9862,40558,5621,210,2250.99831,362781,70416,154,6400.9911,091,676 10,434,547Random Symmetry261.0019183461.002562404,6011.003,4323,20761,1740.9945,88142,818815,4440.99608,816571,55610,878,2271.008,125,9627,629,396Ratio0.920.930.930.940.940.940.950.940.930.930.940.94Table 17: BPMX Rubiks Cube - Dual & Random Symmetry77fiZahavi, Felner, Burch, & Holte11. Related WorkPrevious work predicting A* IDA*s performance properties heuristic fallstwo main camps. first bases analysis accuracy heuristic,second bases analysis, done, distribution heuristic values.next two subsections survey approaches.11.1 Analysis Based Heuristics AccuracyOne common approach characterize heuristic focusing error heuristicvalue (deviation optimal cost). first analysis line, focusing effecterrors performance search algorithms, done Pohl (1970). Manypapers line appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, &Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid &Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & Roger,2008).works usually assume abstract model space tree every nodeexactly b children aim provide asymptotic estimation number expandednodes. mainly differ model assumptions (e.g. binary non-binary trees)case results derived (worst case average case). Worst case analysisshowed correlationheuristic errors search complexity.|h(n)h (n)|found relative error,, constant, search complexityh (n)exponential (in length solution path) absolute error, |h(n) h (n)|,bounded constant search complexity linear (Pohl, 1977; Gaschnig, 1979). Threemain assumptions used Pohl (1977) branching factor assumedconstant across inputs, single goal state transpositionssearch space. assumptions hold, case many standardbenchmark domains planning, general search algorithms A* explore exponentialnumber states even assumption almost perfect heuristic (i.e., heuristicwhose error bounded small additive constant) (Helmert & Roger, 2008).Since difficult guarantee precise bounds magnitude errors producedgiven heuristic, probabilistic characterization magnitudes suggested (Huynet al., 1980; Pearl, 1984). Heuristics modeled random variables (RVs), relativeerrors assumed independent identically distributed (IID model). model,attaining average polynomial A* complexity proved essentially equivalentrequiring values h(n) clustered near h (n) allowed deviationlogarithmic function h (n) itself.Additional research line conducted Chenoweth Davis (1991). Insteadusing IID model, suggested using NC model, places constraintserrors h. model heuristic defined according heuristic values grow respect distance goal, according error.predicted A* complexity polynomial whenever values h(n)logarithmicaly clustered near h (n) + (h (n)), arbitrary, non-negative,non-decreasing function. Heuristics whose values grow slower distancegoal cause exponential complexity. Studies NC model showed replacing78fiPredicting Performance IDA* using Conditional Distributionsheuristic h wh w 0 often change A* complexity exponentialpolynomial.works focused tree searches. contrast, Sen et al. (2004) presentedgeneral technique extending analysis average case performance A*search spaces trees search spaces directed acyclic graphs. analytical results show expected complexity change exponential polynomialheuristic estimates nodes become accurate restrictions placedcost matrix. Recent research line, analyzing complexity A* algorithmpresented Dinh et al. (2007). research presented worst average case analysis performance A* approximately accurate heuristics8 search problemsmultiple solutions. Bounds presented paper proved dependentheuristic accuracy distribution solutions.11.2 Analysis Based Heuristic Distributiondiscussed outset paper, KRE suggested alternative approach calculating time complexity IDA* multiple-goal spaces (Korf & Reid, 1998; Korfet al., 2001). Arguing heuristic accuracy difficult obtain, suggestedderiving analysis unconditional distribution heuristic values, easydetermine least approximately. also came method derivingclosed-form formula Ni , number nodes level brute-force search tree.method later formalized (Edelkamp, 2001b). Unlike work describedprevious subsection, provides big-O complexity analysis, KREs aim (and ours)exactly predict number nodes IDA* expand.KRE correctly point that, operators cost, Ni mustdefined number nodes reached path cost i, opposednumber nodes edges start state. calculation Nigeneral setting studied detail Ruml, slightly different context (Ruml,2002). solution involves using conditional distribution edge costs bearsstrong resemblance conditional distribution heuristic values.Based work KRE insight PDB heuristics correlationsize PDB heuristic value distribution, new analysis limitedPDB heuristics done (Korf, 2007; Breyer & Korf, 2008). prediction achievedbased branching factor problem size PDB without knowingactual heuristic distribution. order derive heuristic distributionsize PDB assumed forward backward branching factorsabstract space equal abstract space negligible number cycles.Since second assumption usually realistic model underestimates numberexpanded nodes.KRE formula developed predict performance IDA* algorithm.general approach also applied A* long appropriate modifications madecomputations Ni P (v) (Korf et al., 2001; Holte & Hernadvolgyi, 2004; Breyer& Korf, 2008). challenge accounting effect A*s pruning searchtree generates state previously reached path smaller equal8. heuristic -approximation (1 )h (s) h(s) (1 + )h (s) states search space.79fiZahavi, Felner, Burch, & Holtecost. particularly challenging heuristic inconsistent, casefirst time A* generates state guaranteed reached via least-costpath, state occur A*s search tree. Indeed, worst case,every state A* enumerate paths state decreasing order cost,thereby generating exactly search tree IDA* (Martelli, 1977). general,A*s pruning reduce Ni , especially large i, ways may hard capturesmall set recurrence equations. heuristic distribution A*s entire searchtree, taken maximum depth, is, consistent heuristics, overall distribution (Korfet al., 2001) since state occurs exactly A*s search tree (as observed,true inconsistent heuristics). imply overall distributionused good effect level-by-level basis, use KRE formula resultaccurate predictions A*s performance 15-puzzle two different consistentheuristics used together exact calculation Ni A*s search tree (Breyer& Korf, 2008).12. Conclusions Future WorkHistorically, heuristics characterized average. KRE introduced ideacharacterizing heuristics unconditional heuristic distribution presentedformula predict number nodes expanded one iteration IDA* basedunconditional heuristic distribution. work presented paper takes anotherstep along line. conditional distribution introduced, predictionformula CDP based it, advance understanding properties heuristic affectperformance IDA*.CDP method advances KRE improving predictions shallow depths,wider range sets start states, inconsistent heuristics. also shownuse make accurate prediction single start state IDA* searchuses BPMX heuristic value propagation.course, sophisticated methods, preprocessing neededspecial care must taken gathering data order get reliable sample.much easier calculate average heuristic calculate 3-dimensionalmatrix. hand, latter approach better characterizes heuristicenables generating accurate predictions larger variety circumstances.Future work address number issues. yet clear attributes makebest context prediction, influenced choice heuristicattributes specific domain. Larger contexts (more parameters) probably provide better prediction cost pre-processing. tradeoff needsstudied. Another direction aim extend analysis approach predictperformance search algorithms A*.13. Acknowledgmentsresearch supported grant number 728/06 305/09 Israeli ScienceFoundation (ISF) Ariel Felner. Robert Holte Neil Burch gratefully acknowledgeongoing support work Canadas Natural Sciences Engineering Research80fiPredicting Performance IDA* using Conditional DistributionsCouncil (NSERC) Albertas Informatics Circle Research Excellence (iCORE).code Rubiks Cube paper based implementation Richard E. Korfused seminal work domain(Korf, 1997). thank anonymous reviewerencouraged us widen experimental results better explain resultsKRE relation results. His/her comments clearly improved strengthpaper. Thanks also Sandra Zilles careful checking details Section 4.ReferencesBreyer, T., & Korf, R. (2008). Recent results analyzing performance heuristicsearch. Proceedings First International Workshop Search ArtificialIntelligence Robotics (held conjunction AAAI), pp. 2431.Chenoweth, S. V., & Davis, H. W. (1991). High-performance A* search using rapidlygrowing heuristics. Proceedings Twelfth International Joint ConferenceArtificial Intelligence (IJCAI-91), pp. 198203.Culberson, J. C., & Schaeffer, J. (1994). Efficiently searching 15-puzzle. Tech. rep.94-08, Department Computer Science, University Alberta.Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,14 (3), 318334.Dinh, H. T., Russell, A., & Su, Y. (2007). value good advice: complexityA* search accurate heuristics. Proceedings Twenty-Second ConferenceArtificial Intelligence (AAAI-07), pp. 11401145.Edelkamp, S. (2001a). Planning pattern databases. Proceedings 6th EuropeanConference Planning (ECP-01), pp. 1334.Edelkamp, S. (2001b). Prediction regular search tree growth spectral analysis.Advances Artificial Intelligence, Joint German/Austrian Conference AI,(KI/OGAI-2001), pp. 154168.Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. JournalArtificial Intelligence Research, 22, 279318.Felner, A., Korf, R. E., Meshulam, R., & Holte, R. C. (2007). Compressed pattern databases.Journal Artificial Intelligence Research, 30, 213247.Felner, A., Zahavi, U., Schaeffer, J., & Holte, R. C. (2005). Dual lookups patterndatabases. Proceedings Nineteenth International Joint Conference Artificial Intelligence (IJCAI-05), pp. 103108.Gaschnig, J. (1979). Performance Measurement Analysis Certain Search Algorithms.Ph.D. thesis, Carnegie-Mellon University.Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination minimum cost paths. IEEE Transactions Systems Science Cybernetics,SCC-4(2), 100107.Helmert, M., & Roger, G. (2008). good almost perfect?. ProceedingsTwenty-Third Conference Artificial Intelligence (AAAI-08), pp. 944949.81fiZahavi, Felner, Burch, & HolteHolte, R. C., Felner, A., Newton, J., Meshulam, R., & Furcy, D. (2006). Maximizingmultiple pattern databases speeds heuristic search. Artificial Intelligence, 170 (1617), 11231136.Holte, R. C., & Hernadvolgyi, I. T. (2004). Steps towards automatic creation searchheuristics. Tech. rep. TR04-02, Computing Science Department, University Alberta.Huyn, N., Dechter, R., & Pearl, J. (1980). Probabilistic analysis complexity A*.Artificial Intelligence, 15 (3), 241254.Karp, R. M., & Pearl, J. (1983). Searching optimal path tree random costs.Artificial Intelligence, 21 (1-2), 99116.Korf, R. E. (1985). Depth-first iterative-deepening: optimal admissible tree search.Artificial Intelligence, 27 (1), 97109.Korf, R. E. (1997). Finding optimal solutions Rubiks Cube using pattern databases.Proceedings Fourteenth Conference Artificial Intelligence (AAAI-97), pp.700705.Korf, R. E. (2007). Analyzing performance pattern database heuristics. ProceedingsTwenty-Second Conference Artificial Intelligence (AAAI-07), pp. 11641170.Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,134 (1-2), 922.Korf, R. E., & Reid, M. (1998). Complexity analysis admissible heuristic search.Proceedings Fifteenth Conference Artificial Intelligence (AAAI-98), pp. 305310.Korf, R. E., Reid, M., & Edelkamp, S. (2001). Time complexity Iterative-Deepening-A* .Artificial Intelligence, 129 (1-2), 199218.Martelli, A. (1977). complexity admissible search algorithms. Artificial Intelligence, 8, 113.McDiarmid, C. J. H., & Provan, G. M. (1991). expected-cost analysis backtrackingnon-backtracking algorithms. Proceedings Twelfth International JointConference Artificial Intelligence (IJCAI-91), pp. 172177.McNaughton, M., Lu, P., Schaeffer, J., & Szafron, D. (2002). Memory-efficient A* heuristicsmultiple sequence alignment. Proceedings Eighteenth ConferenceArtificial Intelligence (AAAI-02), pp. 737743.Mero, L. (1984). heuristic search algorithm modifiable estimate. Artificial Intelligence, 23 (1), 1327.Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving.Addison & Wesley.Pohl, I. (1970). Heuristic search viewed path finding graph. Artificial Intelligence,1 (3), 193204.Pohl, I. (1977). Practical theoretical considerations heuristic search algorithms.Machine Intelligence, 8, 5572.82fiPredicting Performance IDA* using Conditional DistributionsRatner, D., & Warmuth, M. K. (1986). Finding shortest solution n n extension15-puzzle intractable. Proceedings Fifth Conference ArtificialIntelligence (AAAI-86), pp. 168172.Ruml, W. (2002). Adaptive Tree Search. Ph.D. thesis, Harvard University.Sen, A. K., Bagchi, A., & Zhang, W. (2004). Average-case analysis best-first searchtwo representative directed acyclic graphs. Artificial Intelligence, 155 (1-2), 183206.Zahavi, U., Felner, A., Burch, N., & Holte, R. C. (2008). Predicting performanceIDA* conditional distributions. Proceedings Twenty-Third ConferenceArtificial Intelligence (AAAI-08), pp. 381386.Zahavi, U., Felner, A., Holte, R., & Schaeffer, J. (2006). Dual search permutationstate spaces. Proceedings Twenty-First Conference Artificial Intelligence(AAAI-06), pp. 10761081.Zahavi, U., Felner, A., Holte, R. C., & Schaeffer, J. (2008). Duality permutation statespaces dual search algorithm. Artificial Intelligence, 172 (4-5), 514540.Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. R. (2007). Inconsistent heuristics.Proceedings Twenty-Second Conference Artificial Intelligence (AAAI-07),pp. 12111216.Zhou, R., & Hansen, E. A. (2004). Space-efficient memory-based heuristics. ProceedingsNineteenth Conference Artificial Intelligence (AAAI-04), pp. 677682.83fi