Journal Artificial Intelligence Research 37 (2010) 141-188

Submitted 10/09; published 02/10

Frequency Meaning:
Vector Space Models Semantics
Peter D. Turney

peter.turney@nrc-cnrc.gc.ca

National Research Council Canada
Ottawa, Ontario, Canada, K1A 0R6

Patrick Pantel

me@patrickpantel.com

Yahoo! Labs
Sunnyvale, CA, 94089, USA

Abstract
Computers understand little meaning human language. profoundly
limits ability give instructions computers, ability computers explain
actions us, ability computers analyse process text. Vector space
models (VSMs) semantics beginning address limits. paper surveys
use VSMs semantic processing text. organize literature VSMs according
structure matrix VSM. currently three broad classes VSMs,
based termdocument, wordcontext, pairpattern matrices, yielding three classes
applications. survey broad range applications three categories
take detailed look specific open source project category. goal
survey show breadth applications VSMs semantics, provide new
perspective VSMs already familiar area, provide
pointers literature less familiar field.

1. Introduction
One biggest obstacles making full use power computers
currently understand little meaning human language. Recent progress
search engine technology scratching surface human language, yet
impact society economy already immense. hints transformative
impact deeper semantic technologies have. Vector space models (VSMs), surveyed
paper, likely part new semantic technologies.
paper, use term semantics general sense, meaning word,
phrase, sentence, text human language, study meaning.
concerned narrower senses semantics, semantic web approaches
semantics based formal logic. present survey VSMs relation
distributional hypothesis approach representing aspects natural language
semantics.
VSM developed SMART information retrieval system (Salton, 1971)
Gerard Salton colleagues (Salton, Wong, & Yang, 1975). SMART pioneered
many concepts used modern search engines (Manning, Raghavan, &
Schutze, 2008). idea VSM represent document collection
point space (a vector vector space). Points close together space
semantically similar points far apart semantically distant. users
c
2010
AI Access Foundation National Research Council Canada. Reprinted permission.

fiTurney & Pantel

query represented point space documents (the query pseudodocument). documents sorted order increasing distance (decreasing semantic
similarity) query presented user.
success VSM information retrieval inspired researchers extend
VSM semantic tasks natural language processing, impressive results.
instance, Rapp (2003) used vector-based representation word meaning achieve
score 92.5% multiple-choice synonym questions Test English Foreign
Language (TOEFL), whereas average human score 64.5%.1 Turney (2006) used
vector-based representation semantic relations attain score 56% multiple-choice
analogy questions SAT college entrance test, compared average human score
57%.2
survey, organized past work VSMs according type matrix
involved: termdocument, wordcontext, pairpattern. believe choice
particular matrix type fundamental choices, particular
linguistic processing mathematical processing. Although three matrix types cover
work, reason believe three types exhaust possibilities.
expect future work introduce new types matrices higher-order tensors.3
1.1 Motivation Vector Space Models Semantics
VSMs several attractive properties. VSMs extract knowledge automatically
given corpus, thus require much less labour approaches semantics,
hand-coded knowledge bases ontologies. example, main resource used
Rapps (2003) VSM system measuring word similarity British National Corpus
(BNC),4 whereas main resource used non-VSM systems measuring word similarity
(Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003)
lexicon, WordNet5 Rogets Thesaurus. Gathering corpus new language
generally much easier building lexicon, building lexicon often involves also
gathering corpus, SemCor WordNet (Miller, Leacock, Tengi, & Bunker, 1993).
VSMs perform well tasks involve measuring similarity meaning
words, phrases, documents. search engines use VSMs measure similarity
query document (Manning et al., 2008). leading algorithms measuring semantic relatedness use VSMs (Pantel & Lin, 2002a; Rapp, 2003; Turney, Littman,
Bigham, & Shnayder, 2003). leading algorithms measuring similarity semantic relations also use VSMs (Lin & Pantel, 2001; Turney, 2006; Nakov & Hearst, 2008).
(Section 2.4 discusses differences types similarity.)
find VSMs especially interesting due relation distributional hypothesis related hypotheses (see Section 2.7). distributional hypothesis
1. Regarding average score 64.5% TOEFL questions, Landauer Dumais (1997) note
that, Although know performance would compare, example, U.S. school
children particular age, told average score adequate admission many
universities.
2. average score highschool students senior year, applying US universities.
discussion score, see Section 6.3 Turneys (2006) paper.
3. vector first-order tensor matrix second-order tensor. See Section 2.5.
4. See http://www.natcorp.ox.ac.uk/.
5. See http://wordnet.princeton.edu/.

142

fiFrom Frequency Meaning

words occur similar contexts tend similar meanings (Wittgenstein, 1953;
Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990). Efforts apply abstract hypothesis concrete algorithms measuring
similarity meaning often lead vectors, matrices, higher-order tensors.
intimate connection distributional hypothesis VSMs strong motivation
taking close look VSMs.
uses vectors matrices count vector space models. purposes
survey, take defining property VSMs values elements
VSM must derived event frequencies, number times given word
appears given context (see Section 2.6). example, often lexicon knowledge
base may viewed graph, graph may represented using adjacency matrix,
imply lexicon VSM, because, general, values
elements adjacency matrix derived event frequencies. emphasis
event frequencies brings unity variety VSMs explicitly connects
distributional hypothesis; furthermore, avoids triviality excluding many possible
matrix representations.
1.2 Vectors AI Cognitive Science
Vectors common AI cognitive science; common VSM
introduced Salton et al. (1975). novelty VSM use frequencies
corpus text clue discovering semantic information.
machine learning, typical problem learn classify cluster set items
(i.e., examples, cases, individuals, entities) represented feature vectors (Mitchell, 1997;
Witten & Frank, 2005). general, features derived event frequencies,
although possible (see Section 4.6). example, machine learning algorithm
applied classifying clustering documents (Sebastiani, 2002).
Collaborative filtering recommender systems also use vectors (Resnick, Iacovou,
Suchak, Bergstrom, & Riedl, 1994; Breese, Heckerman, & Kadie, 1998; Linden, Smith, &
York, 2003). typical recommender system, person-item matrix,
rows correspond people (customers, consumers), columns correspond items
(products, purchases), value element rating (poor, fair, excellent)
person given item. Many mathematical techniques work well
termdocument matrices (see Section 4) also work well person-item matrices,
ratings derived event frequencies.
cognitive science, prototype theory often makes use vectors. basic idea
prototype theory members category central others (Rosch
& Lloyd, 1978; Lakoff, 1987). example, robin central (prototypical) member
category bird, whereas penguin peripheral. Concepts varying degrees
membership categories (graded categorization). natural way formalize
represent concepts vectors categories sets vectors (Nosofsky, 1986; Smith, Osherson, Rips, & Keane, 1988). However, vectors usually based numerical scores
elicited questioning human subjects; based event frequencies.
Another area psychology makes extensive use vectors psychometrics,
studies measurement psychological abilities traits. usual instrument
143

fiTurney & Pantel

measurement test questionnaire, personality test. results test
typically represented subject-item matrix, rows represent subjects
(people) experiment columns represent items (questions) test
(questionnaire). value element matrix answer corresponding
subject gave corresponding item. Many techniques vector analysis, factor
analysis (Spearman, 1904), pioneered psychometrics.
cognitive science, Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer & Dumais, 1997), Hyperspace Analogue Language (HAL) (Lund, Burgess, &
Atchley, 1995; Lund & Burgess, 1996), related research (Landauer, McNamara, Dennis, & Kintsch, 2007) entirely within scope VSMs, defined above, since
research uses vector space models values elements derived
event frequencies, number times given word appears given context. Cognitive scientists argued empirical theoretical reasons
believing VSMs, LSA HAL, plausible models aspects human cognition (Landauer et al., 2007). AI, computational linguistics, information
retrieval, plausibility essential, may seen sign VSMs
promising area research.
1.3 Motivation Survey
paper survey vector space models semantics. currently comprehensive, up-to-date survey field. show survey, vector space models
highly successful approach semantics, wide range potential actual
applications. much recent growth research area.
paper interest AI researchers work natural language,
especially researchers interested semantics. survey serve general
introduction area provide framework unified perspective
organizing diverse literature topic. encourage new research area,
pointing open problems areas exploration.
survey makes following contributions:
New framework: provide new framework organizing literature: term
document, wordcontext, pairpattern matrices (see Section 2). framework shows
importance structure matrix (the choice rows columns) determining potential applications may inspire researchers explore new structures
(different kinds rows columns, higher-order tensors instead matrices).
New developments: draw attention pairpattern matrices. use pair
pattern matrices relatively new deserves study. matrices address
criticisms directed wordcontext matrices, regarding lack sensitivity
word order.
Breadth approaches applications: existing survey shows
breadth potential actual applications VSMs semantics. Existing summaries
omit pairpattern matrices (Landauer et al., 2007).
Focus NLP CL: focus survey systems perform practical
tasks natural language processing computational linguistics. Existing overviews focus
cognitive psychology (Landauer et al., 2007).
144

fiFrom Frequency Meaning

Success stories: draw attention fact VSMs arguably
successful approach semantics, far.
1.4 Intended Readership
goal writing paper survey state art vector space models
semantics, introduce topic new area, give new
perspective already familiar area.
assume reader basic understanding vectors, matrices, linear algebra,
one might acquire introductory undergraduate course linear algebra,
text book (Golub & Van Loan, 1996). basic concepts vectors matrices
important mathematical details. Widdows (2004) gives gentle
introduction vectors perspective semantics.
also assume reader familiarity computational linguistics information retrieval. Manning et al. (2008) provide good introduction information retrieval.
computational linguistics, recommend Manning Schutzes (1999) text.
reader familiar linear algebra computational linguistics, survey
present barriers understanding. Beyond background, necessary
familiar VSMs used information retrieval, natural language processing, computational linguistics. However, reader would like
background reading, recommend Landauer et al.s (2007) collection.
1.5 Highlights Outline
article structured follows. Section 2 explains framework organizing
literature VSMs according type matrix involved: termdocument, wordcontext,
pairpattern. section, present overview VSMs, without getting
details matrix generated corpus raw text.
high-level framework place, Sections 3 4 examine steps involved
generating matrix. Section 3 discusses linguistic processing Section 4 reviews
mathematical processing. order corpus would processed
VSM systems (first linguistic processing, mathematical processing).
VSMs used semantics, input model usually plain text.
VSMs work directly raw text, first apply linguistic processing
text, stemming, part-of-speech tagging, word sense tagging, parsing. Section 3
looks linguistic tools semantic VSMs.
simple VSM, simple termdocument VSM, value element
document vector number times corresponding word occurs given
document, VSMs apply mathematical processing raw frequency values.
Section 4 presents main mathematical operations: weighting elements, smoothing
matrix, comparing vectors. section also describes optimization strategies
comparing vectors, distributed sparse matrix multiplication randomized
techniques.
end Section 4, reader general view concepts involved
vector space models semantics. take detailed look three VSM systems
Section 5. representative termdocument VSMs, present Lucene information
145

fiTurney & Pantel

retrieval library.6 wordcontext VSMs, explore Semantic Vectors package,
builds Lucene.7 representative pairpattern VSMs, review Latent
Relational Analysis module S-Space package, also builds Lucene.8
source code three systems available open source licensing.
turn broad survey applications semantic VSMs Section 6. section also serves short historical view research semantic VSMs, beginning
information retrieval Section 6.1. purpose give reader idea
breadth applications VSMs also provide pointers literature,
reader wishes examine applications detail.
termdocument matrix, rows correspond terms columns correspond documents (Section 6.1). document provides context understanding term.
generalize idea documents chunks text arbitrary size (phrases, sentences,
paragraphs, chapters, books, collections), result wordcontext matrix, includes termdocument matrix special case. Section 6.2 discusses applications
wordcontext matrices. Section 6.3 considers pairpattern matrices, rows correspond pairs terms columns correspond patterns pairs
occur.
Section 7, discuss alternatives VSMs semantics. Section 8 considers
future VSMs, raising questions power limitations. conclude
Section 9.

2. Vector Space Models Semantics
theme unites various forms VSMs discuss paper
stated statistical semantics hypothesis: statistical patterns human word usage
used figure people mean.9 general hypothesis underlies several
specific hypotheses, bag words hypothesis, distributional hypothesis,
extended distributional hypothesis, latent relation hypothesis, discussed below.
2.1 Similarity Documents: TermDocument Matrix
paper, use following notational conventions: Matrices denoted bold
capital letters, A. Vectors denoted bold lowercase letters, b. Scalars represented
lowercase italic letters, c.
large collection documents, hence large number document
vectors, convenient organize vectors matrix. row vectors matrix
correspond terms (usually terms words, discuss possibilities)
6.
7.
8.
9.

See http://lucene.apache.org/java/docs/.
See http://code.google.com/p/semanticvectors/.
See http://code.google.com/p/airhead-research/wiki/LatentRelationalAnalysis.
phrase taken Faculty Profile George Furnas University Michigan,
http://www.si.umich.edu/people/faculty-detail.htm?sid=41. full quote is, Statistical Semantics
Studies statistical patterns human word usage used figure people
mean, least level sufficient information access. term statistical semantics appeared
work Furnas, Landauer, Gomez, Dumais (1983), defined there.

146

fiFrom Frequency Meaning

column vectors correspond documents (web pages, example). kind
matrix called termdocument matrix.
mathematics, bag (also called multiset) like set, except duplicates
allowed. example, {a, a, b, c, c, c} bag containing a, b, c. Order matter
bags sets; bags {a, a, b, c, c, c} {c, a, c, b, a, c} equivalent. represent
bag {a, a, b, c, c, c} vector x = h2, 1, 3i, stipulating first element
x frequency bag, second element frequency b bag,
third element frequency c. set bags represented matrix X,
column x:j corresponds bag, row xi: corresponds unique member,
element xij frequency i-th member j-th bag.
termdocument matrix, document vector represents corresponding document
bag words. information retrieval, bag words hypothesis
estimate relevance documents query representing documents
query bags words. is, frequencies words document tend indicate
relevance document query. bag words hypothesis basis
applying VSM information retrieval (Salton et al., 1975). hypothesis expresses
belief column vector termdocument matrix captures (to degree)
aspect meaning corresponding document; document about.
Let X termdocument matrix. Suppose document collection contains n documents unique terms. matrix X rows (one row unique
term vocabulary) n columns (one column document). Let wi i-th
term vocabulary let dj j-th document collection. i-th row
X row vector xi: j-th column X column vector x:j . row vector
xi: contains n elements, one element document, column vector x:j contains
elements, one element term. Suppose X simple matrix frequencies.
element xij X frequency i-th term wi j-th document dj .
general, value elements X zero (the matrix sparse),
since documents use small fraction whole vocabulary. randomly
choose term wi document dj , likely wi occur anywhere dj ,
therefore xij equals 0.
pattern numbers xi: kind signature i-th term wi ; likewise,
pattern numbers x:j signature j-th document dj . is, pattern
numbers tells us, degree, term document about.
vector x:j may seem rather crude representation document dj . tells
us frequently words appear document, sequential order words
lost. vector attempt capture structure phrases, sentences,
paragraphs, chapters document. However, spite crudeness, search
engines work surprisingly well; vectors seem capture important aspect semantics.
VSM Salton et al. (1975) arguably first practical, useful algorithm
extracting semantic information word usage. intuitive justification term
document matrix topic document probabilistically influence authors
choice words writing document.10 two documents similar topics,
two corresponding column vectors tend similar patterns numbers.
10. Newer generative models, Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003), directly
model intuition. See Sections 4.3 7.

147

fiTurney & Pantel

2.2 Similarity Words: WordContext Matrix
Salton et al. (1975) focused measuring document similarity, treating query search
engine pseudo-document. relevance document query given
similarity vectors. Deerwester et al. (1990) observed shift focus
measuring word similarity, instead document similarity, looking row vectors
termdocument matrix, instead column vectors.
Deerwester et al. (1990) inspired termdocument matrix Salton et al. (1975),
document necessarily optimal length text measuring word similarity.
general, may wordcontext matrix, context given words,
phrases, sentences, paragraphs, chapters, documents, exotic possibilities,
sequences characters patterns.
distributional hypothesis linguistics words occur similar contexts
tend similar meanings (Harris, 1954). hypothesis justification applying VSM measuring word similarity. word may represented vector
elements derived occurrences word various contexts,
windows words (Lund & Burgess, 1996), grammatical dependencies (Lin, 1998;
Pado & Lapata, 2007), richer contexts consisting dependency links selectional
preferences argument positions (Erk & Pado, 2008); see Sahlgrens (2006) thesis
comprehensive study various contexts. Similar row vectors wordcontext matrix
indicate similar word meanings.
idea word usage reveal semantics implicit things
Wittgenstein (1953) said language-games family resemblance. Wittgenstein
primarily interested physical activities form context word usage (e.g.,
word brick, spoken context physical activity building house), main
context word often words.11
Weaver (1955) argued word sense disambiguation machine translation
based co-occurrence frequency context words near given target word (the
word want disambiguate). Firth (1957, p. 11) said, shall know word
company keeps. Deerwester et al. (1990) showed intuitions Wittgenstein (1953), Harris (1954), Weaver, Firth could used practical algorithm.
2.3 Similarity Relations: PairPattern Matrix
pairpattern matrix, row vectors correspond pairs words, mason : stone
carpenter : wood, column vectors correspond patterns pairs cooccur, X cuts X works . Lin Pantel (2001) introduced
pairpattern matrix purpose measuring semantic similarity patterns;
is, similarity column vectors. Given pattern X solves , algorithm
able find similar patterns, solved X, resolved X,
X resolves .
Lin Pantel (2001) proposed extended distributional hypothesis, patterns
co-occur similar pairs tend similar meanings. patterns X solves
11. Wittgensteins intuition might better captured matrix combines words modalities,
images (Monay & Gatica-Perez, 2003). values elements derived event
frequencies, would include VSM approach semantics.

148

fiFrom Frequency Meaning

solved X tend co-occur similar X : pairs, suggests
patterns similar meanings. Pattern similarity used infer one sentence
paraphrase another (Lin & Pantel, 2001).
Turney et al. (2003) introduced use pairpattern matrix measuring
semantic similarity relations word pairs; is, similarity row vectors.
example, pairs mason : stone, carpenter : wood, potter : clay, glassblower : glass
share semantic relation artisan : material. case, first member pair
artisan makes artifacts material second member pair.
pairs tend co-occur similar patterns, X used X
shaped into.
latent relation hypothesis pairs words co-occur similar patterns
tend similar semantic relations (Turney, 2008a). Word pairs similar row
vectors pairpattern matrix tend similar semantic relations. inverse
extended distributional hypothesis, patterns similar column vectors
pairpattern matrix tend similar meanings.
2.4 Similarities
Pairpattern matrices suited measuring similarity semantic relations
pairs words; is, relational similarity. contrast, wordcontext matrices suited
measuring attributional similarity. distinction attributional relational
similarity explored depth Gentner (1983).
attributional similarity two words b, sima (a, b) <, depends
degree correspondence properties b. correspondence
is, greater attributional similarity. relational similarity two pairs
words : b c : d, simr (a : b, c : d) <, depends degree correspondence
relations : b c : d. correspondence is, greater relational
similarity. example, dog wolf relatively high degree attributional similarity, whereas dog : bark cat : meow relatively high degree relational similarity
(Turney, 2006).
tempting suppose relational similarity reduced attributional
similarity. example, mason carpenter similar words stone wood
similar words; therefore, perhaps follows mason : stone carpenter : wood
similar relations. Perhaps simr (a : b, c : d) reduced sima (a, c) + sima (b, d). However,
mason, carpenter, potter, glassblower similar words (they artisans),
wood, clay, stone, glass (they materials used artisans), cannot infer
mason : glass carpenter : clay similar relations. Turney (2006, 2008a)
presented experimental evidence relational similarity reduce attributional
similarity.
term semantic relatedness computational linguistics (Budanitsky & Hirst, 2001)
corresponds attributional similarity cognitive science (Gentner, 1983). Two words
semantically related kind semantic relation (Budanitsky & Hirst,
2001); semantically related degree share attributes (Turney, 2006).
Examples synonyms (bank trust company), meronyms (car wheel), antonyms
(hot cold), words functionally related frequently associated (pencil
149

fiTurney & Pantel

paper). might usually think antonyms similar, antonyms high
degree attributional similarity (hot cold kinds temperature, black white
kinds colour, loud quiet kinds sound). prefer term attributional
similarity term semantic relatedness, attributional similarity emphasizes
contrast relational similarity, whereas semantic relatedness could confused
relational similarity.
computational linguistics, term semantic similarity applied words share
hypernym (car bicycle semantically similar, share hypernym
vehicle) (Resnik, 1995). Semantic similarity specific type attributional similarity.
prefer term taxonomical similarity term semantic similarity, term
semantic similarity misleading. Intuitively, attributional relational similarity
involve meaning, deserve called semantic similarity.
Words semantically associated tend co-occur frequently (e.g., bee
honey) (Chiarello, Burgess, Richards, & Pollock, 1990). Words may taxonomically similar semantically associated (doctor nurse), taxonomically similar semantically associated (horse platypus), semantically associated taxonomically similar
(cradle baby), neither semantically associated taxonomically similar (calculus
candy).
Schutze Pedersen (1993) defined two ways words distributed corpus text: two words tend neighbours other, syntagmatic
associates. two words similar neighbours, paradigmatic parallels. Syntagmatic associates often different parts speech, whereas paradigmatic parallels
usually part speech. Syntagmatic associates tend semantically associated (bee honey often neighbours); paradigmatic parallels tend taxonomically
similar (doctor nurse similar neighbours).
2.5 Semantic VSMs
possibilities exhausted termdocument, wordcontext, pairpattern
matrices. might want consider triplepattern matrices, measuring semantic
similarity word triples. Whereas pairpattern matrix might row mason :
stone column X works , triplepattern matrix could row mason :
stone : masonry column X uses build Z. However, n-tuples words grow
increasingly rare n increases. example, phrases contain mason, stone,
masonry together less frequent phrases contain mason stone together.
triplepattern matrix much sparse pairpattern matrix (ceteris paribus).
quantity text need, order enough numbers make matrices
useful, grows rapidly n increases. may better break n-tuples pairs.
example, : b : c could decomposed : b, : c, b : c (Turney, 2008a). similarity
two triples, : b : c : e : f , could estimated similarity corresponding
pairs. relatively dense pairpattern matrix could serve surrogate relatively
sparse triplepattern matrix.
may also go beyond matrices. generalization matrix tensor (Kolda
& Bader, 2009; Acar & Yener, 2009). scalar (a single number) zeroth-order tensor,
vector first-order tensor, matrix second-order tensor. tensor order three
150

fiFrom Frequency Meaning

higher called higher-order tensor. Chew, Bader, Kolda, Abdelali (2007) use term
documentlanguage third-order tensor multilingual information retrieval. Turney (2007)
uses wordwordpattern tensor measure similarity words. Van de Cruys (2009) uses
verbsubjectobject tensor learn selectional preferences verbs.
Turneys (2007) tensor, example, rows correspond words TOEFL
multiple-choice synonym questions, columns correspond words Basic English (Ogden, 1930),12 tubes correspond patterns join rows columns (hence
wordwordpattern third-order tensor). given word TOEFL questions represented corresponding wordpattern matrix slice tensor. elements
slice correspond patterns relate given TOEFL word word
Basic English. similarity two TOEFL words calculated comparing two
corresponding matrix slices. algorithm achieves 83.8% TOEFL questions.
2.6 Types Tokens
token single instance symbol, whereas type general class tokens (Manning
et al., 2008). Consider following example (from Samuel Beckett):

Ever tried. Ever failed.
matter. Try again.
Fail again. Fail better.

two tokens type Ever, two tokens type again, two tokens
type Fail. Lets say line example document, three
documents two sentences each. represent example tokendocument
matrix typedocument matrix. tokendocument matrix twelve rows, one
token, three columns, one line (Figure 1). typedocument matrix
nine rows, one type, three columns (Figure 2).
row vector token binary values: element 1 given token appears
given document 0 otherwise. row vector type integer values: element
frequency given type given document. vectors related,
type vector sum corresponding token vectors. example, row vector
type Ever sum two token vectors two tokens Ever.
applications dealing polysemy, one approach uses vectors represent word
tokens (Schutze, 1998; Agirre & Edmonds, 2006) another uses vectors represent
word types (Pantel & Lin, 2002a). Typical word sense disambiguation (WSD) algorithms
deal word tokens (instances words specific contexts) rather word types.
mention approaches polysemy Section 6, due similarity close
relationship, although defining characteristic VSM concerned
frequencies (see Section 1.1), frequency property types, tokens.
12. Basic English highly reduced subset English, designed easy people learn. words
Basic English listed http://ogden.basic-english.org/.

151

fiTurney & Pantel

Ever tried.
Ever failed.
Ever
tried
Ever
failed

matter
Try

Fail

Fail
better

matter.
Try again.

1
1
1
1
0
0
0
0
0
0
0
0

0
0
0
0
1
1
1
1
0
0
0
0

Fail again.
Fail better.
0
0
0
0
0
0
0
0
1
1
1
1

Figure 1: tokendocument matrix. Rows tokens columns documents.

Ever tried.
Ever failed.
Ever
tried
failed

matter
Try

Fail
better

matter.
Try again.

2
1
1
0
0
0
0
0
0

0
0
0
1
1
1
1
0
0

Fail again.
Fail better.
0
0
0
0
0
0
1
2
1

Figure 2: typedocument matrix. Rows types columns documents.

152

fiFrom Frequency Meaning

2.7 Hypotheses
mentioned five hypotheses section. repeat hypotheses
interpret terms vectors. hypothesis, cite work explicitly
states something like hypothesis implicitly assumes something like hypothesis.
Statistical semantics hypothesis: Statistical patterns human word usage
used figure people mean (Weaver, 1955; Furnas et al., 1983). units text
similar vectors text frequency matrix,13 tend similar meanings.
(We take general hypothesis subsumes four specific hypotheses
follow.)
Bag words hypothesis: frequencies words document tend indicate
relevance document query (Salton et al., 1975). documents pseudodocuments (queries) similar column vectors termdocument matrix,
tend similar meanings.
Distributional hypothesis: Words occur similar contexts tend similar
meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990). words similar row
vectors wordcontext matrix, tend similar meanings.
Extended distributional hypothesis: Patterns co-occur similar pairs tend
similar meanings (Lin & Pantel, 2001). patterns similar column vectors
pairpattern matrix, tend express similar semantic relations.
Latent relation hypothesis: Pairs words co-occur similar patterns tend
similar semantic relations (Turney et al., 2003). word pairs similar row
vectors pairpattern matrix, tend similar semantic relations.
yet explained means say vectors similar. discuss
Section 4.4.

3. Linguistic Processing Vector Space Models
assume raw data large corpus natural language text.
generate termdocument, wordcontext, pairpattern matrix, useful apply
linguistic processing raw text. types processing used
grouped three classes. First, need tokenize raw text; is, need decide
constitutes term extract terms raw text. Second, may want
normalize raw text, convert superficially different strings characters
form (e.g., car, Car, cars, Cars could normalized car). Third, may want
annotate raw text, mark identical strings characters different (e.g., fly
verb could annotated fly/VB fly noun could annotated fly/NN).
Grefenstette (1994) presents good study linguistic processing wordcontext
VSMs. uses similar three-step decomposition linguistic processing: tokenization,
surface syntactic analysis, syntactic attribute extraction.
13. text frequency matrix, mean matrix higher-order tensor values elements
derived frequencies pieces text context pieces text collection
text. text frequency matrix intended general structure, includes termdocument,
wordcontext, pairpattern matrices special cases.

153

fiTurney & Pantel

3.1 Tokenization
Tokenization English seems simple first glance: words separated spaces.
assumption approximately true English, may work sufficiently well basic
VSM, advanced VSM requires sophisticated approach tokenization.
accurate English tokenizer must know handle punctuation (e.g., dont, Janes,
and/or), hyphenation (e.g., state-of-the-art versus state art), recognize multi-word
terms (e.g., Barack Obama ice hockey) (Manning et al., 2008). may also wish
ignore stop words, high-frequency words relatively low information content,
function words (e.g., of, the, and) pronouns (e.g., them, who, that). popular list
stop words set 571 common words included source code SMART
system (Salton, 1971).14
languages (e.g., Chinese), words separated spaces. basic VSM
break text character unigrams bigrams. sophisticated approach
match input text entries lexicon, matching often
determine unique tokenization (Sproat & Emerson, 2003). Furthermore, native speakers
often disagree correct segmentation. Highly accurate tokenization challenging
task human languages.
3.2 Normalization
motivation normalization observation many different strings characters often convey essentially identical meanings. Given want get meaning
underlies words, seems reasonable normalize superficial variations converting form. common types normalization case folding
(converting words lower case) stemming (reducing inflected words stem
root form).
Case folding easy English, problematic languages. French,
accents optional uppercase, may difficult restore missing accents
converting words lowercase. words cannot distinguished without accents;
example, PECHE could either peche (meaning fishing peach) peche (meaning sin).
Even English, case folding cause problems, case sometimes semantic
significance. example, SMART information retrieval system, whereas smart
common adjective; Bush may surname, whereas bush kind plant.
Morphology study internal structure words. Often word composed
stem (root) added affixes (inflections), plural forms past tenses (e.g.,
trapped composed stem trap affix -ed). Stemming, kind morphological
analysis, process reducing inflected words stems. English, affixes
simpler regular many languages, stemming algorithms based
heuristics (rules thumb) work relatively well (Lovins, 1968; Porter, 1980; Minnen,
Carroll, & Pearce, 2001). agglutinative language (e.g., Inuktitut), many concepts
combined single word, using various prefixes, infixes, suffixes, morphological
analysis complicated. single word agglutinative language may correspond
sentence half dozen words English (Johnson & Martin, 2003).
14. source code available ftp://ftp.cs.cornell.edu/pub/smart/.

154

fiFrom Frequency Meaning

performance information retrieval system often measured precision
recall (Manning et al., 2008). precision system estimate conditional
probability document truly relevant query, system says relevant.
recall system estimate conditional probability system say
document relevant query, truly relevant.
general, normalization increases recall reduces precision (Kraaij & Pohlmann,
1996). natural, given nature normalization. remove superficial
variations believe irrelevant meaning, make easier recognize similarities; find similar things, recall increases. sometimes superficial
variations semantic significance; ignoring variations causes errors, precision
decreases. Normalization also positive effect precision cases variant
tokens infrequent smoothing variations gives reliable statistics.
small corpus, may able afford overly selective, may
best aggressively normalize text, increase recall. large corpus,
precision may important, might want normalization. Hull (1996)
gives good analysis normalization information retrieval.
3.3 Annotation
Annotation inverse normalization. different strings characters may
meaning, also happens identical strings characters may different
meanings, depending context. Common forms annotation include part-of-speech
tagging (marking words according parts speech), word sense tagging (marking
ambiguous words according intended meanings), parsing (analyzing grammatical structure sentences marking words sentences according
grammatical roles) (Manning & Schutze, 1999).
Since annotation inverse normalization, expect decrease recall
increase precision. example, tagging program noun verb, may
able selectively search documents act computer programming
(verb) instead documents discuss particular computer programs (noun); hence
increase precision. However, document computer programs (noun) may
something useful say act computer programming (verb), even document
never uses verb form program; hence may decrease recall.
Large gains IR performance recently reported result query annotation syntactic semantic information. Syntactic annotation includes query
segmentation (Tan & Peng, 2008) part speech tagging (Barr, Jones, & Regelson,
2008). Examples semantic annotation disambiguating abbreviations queries (Wei,
Peng, & Dumoulin, 2008) finding query keyword associations (Lavrenko & Croft, 2001;
Cao, Nie, & Bai, 2005).
Annotation also useful measuring semantic similarity words concepts
(wordcontext matrices). example, Pantel Lin (2002a) presented algorithm
discover word senses clustering row vectors wordcontext matrix, using
contextual information derived parsing.
155

fiTurney & Pantel

4. Mathematical Processing Vector Space Models
text tokenized (optionally) normalized annotated, first step
generate matrix frequencies. Second, may want adjust weights
elements matrix, common words high frequencies, yet less
informative rare words. Third, may want smooth matrix, reduce
amount random noise fill zero elements sparse matrix. Fourth,
many different ways measure similarity two vectors.
Lowe (2001) gives good summary mathematical processing wordcontext VSMs.
decomposes VSM construction similar four-step process: calculate frequencies,
transform raw frequency counts, smooth space (dimensionality reduction),
calculate similarities.
4.1 Building Frequency Matrix
element frequency matrix corresponds event: certain item (term, word,
word pair) occurred certain situation (document, context, pattern) certain number
times (frequency). abstract level, building frequency matrix simple matter
counting events. practice, complicated corpus large.
typical approach building frequency matrix involves two steps. First, scan sequentially corpus, recording events frequencies hash table,
database, search engine index. Second, use resulting data structure generate
frequency matrix, sparse matrix representation (Gilbert, Moler, & Schreiber, 1992).
4.2 Weighting Elements
idea weighting give weight surprising events less weight expected
events. hypothesis surprising events, shared two vectors, discriminative similarity vectors less surprising events. example,
measuring semantic similarity words mouse rat, contexts dissect
exterminate discriminative similarity contexts like.
information theory, surprising event higher information content expected
event (Shannon, 1948). popular way formalize idea termdocument
matrices tf-idf (term frequency inverse document frequency) family weighting
functions (Sparck Jones, 1972). element gets high weight corresponding term
frequent corresponding document (i.e., tf high), term rare
documents corpus (i.e., df low, thus idf high). Salton Buckley (1988)
defined large family tf-idf weighting functions evaluated information retrieval tasks, demonstrating tf-idf weighting yield significant improvements
raw frequency.
Another kind weighting, often combined tf-idf weighting, length normalization
(Singhal, Salton, Mitra, & Buckley, 1996). information retrieval, document length
ignored, search engines tend bias favour longer documents. Length
normalization corrects bias.
Term weighting may also used correct correlated terms. example,
terms hostage hostages tend correlated, yet may want normalize
156

fiFrom Frequency Meaning

term (as Section 3.2), slightly different meanings.
alternative normalizing them, may reduce weights co-occur
document (Church, 1995).
Feature selection may viewed form weighting, terms get
weight zero hence removed matrix. Forman (2003) provides good
study feature selection methods text classification.
alternative tf-idf Pointwise Mutual Information (PMI) (Church & Hanks, 1989;
Turney, 2001), works well wordcontext matrices (Pantel & Lin, 2002a)
termdocument matrices (Pantel & Lin, 2002b). variation PMI Positive PMI
(PPMI), PMI values less zero replaced zero (Niwa &
Nitta, 1994). Bullinaria Levy (2007) demonstrated PPMI performs better
wide variety weighting approaches measuring semantic similarity word
context matrices. Turney (2008a) applied PPMI pairpattern matrices. give
formal definition PPMI here, example effective weighting function.
Let F wordcontext frequency matrix nr rows nc columns. i-th row
F row vector fi: j-th column F column vector f:j . row fi:
corresponds word wi column f:j corresponds context cj . value
element fij number times wi occurs context cj . Let X matrix
results PPMI applied F. new matrix X number rows
columns raw frequency matrix F. value element xij X defined
follows:
fij
pij = Pnr Pnc

j=1 fij

i=1

(1)

Pnc

j=1 fij
pi = Pnr Pnc

(2)

Pnr
f
Pncij
= Pnr i=1

(3)

i=1

pj

i=1

j=1 fij
j=1 fij



pij
pmiij = log
pi pj

pmiij pmiij > 0
xij =
0 otherwise

(4)
(5)

definition, pij estimated probability word wi occurs context
cj , pi estimated probability word wi , pj estimated probability
context cj . wi cj statistically independent, pi pj = pij (by definition
independence), thus pmiij zero (since log(1) = 0). product pi pj
would expect pij wi occurs cj pure random chance. hand,
interesting semantic relation wi cj , expect pij larger
would wi cj indepedent; hence find pij > pi pj ,
thus pmiij positive. follows distributional hypothesis (see Section 2).
word wi unrelated context cj , may find pmiij negative. PPMI
designed give high value xij interesting semantic relation
157

fiTurney & Pantel

wi cj ; otherwise, xij value zero, indicating occurrence wi
cj uninformative.
well-known problem PMI biased towards infrequent events. Consider
case wi cj statistically dependent (i.e., maximum association).
pij = pi = pj . Hence (4) becomes log (1/pi ) PMI increases probability
word wi decreases. Several discounting factors proposed alleviate problem.
example follows (Pantel & Lin, 2002a):
P c
P r
fik )
fkj , nk=1
min ( nk=1
fij
Pnc
Pnr
ij =

fij + 1 min ( k=1 fkj , k=1 fik ) + 1
newpmiij = ij pmiij

(6)
(7)

Another way deal infrequent events Laplace smoothing probability
estimates, pij , pi , pj (Turney & Littman, 2003). constant positive value added
raw frequencies calculating probabilities; fij replaced fij + k,
k > 0. larger constant, greater smoothing effect. Laplace smoothing
pushes pmiij values towards zero. magnitude push (the difference
pmiij without Laplace smoothing) depends raw frequency fij .
frequency large, push small; frequency small, push large. Thus
Laplace smoothing reduces bias PMI towards infrequent events.
4.3 Smoothing Matrix
simplest way improve information retrieval performance limit number
vector components. Keeping components representing frequently occurring
content words way; however, common words, have, carry little
semantic discrimination power. Simple component smoothing heuristics, based properties weighting schemes presented Section 4.2, shown maintain
semantic discrimination power improve performance similarity computations.
Computing similarity pairs vectors, described Section 4.4,
computationally intensive task. However, vectors share non-zero coordinate
must compared (i.e., two vectors share coordinate dissimilar).
frequent context words, word the, unfortunately result vectors matching
non-zero coordinate. words precisely contexts little semantic
discrimination power. Consider pointwise mutual information weighting described
Section 4.2. Highly weighted dimensions co-occur frequently words
definition highly discriminating contexts (i.e., high association
words co-occur). keeping context-word dimensions
PMI conservative threshold setting others zero, Lin (1998) showed
number comparisons needed compare vectors greatly decreases losing
little precision similarity score top-200 similar words every word.
smoothing matrix, one computes reverse index non-zero coordinates.
Then, compare similarity words context vector words context
vectors, vectors found match non-zero component reverse index must
compared. Section 4.5 proposes optimizations along lines.
158

fiFrom Frequency Meaning

Deerwester et al. (1990) found elegant way improve similarity measurements
mathematical operation termdocument matrix, X, based linear algebra. operation truncated Singular Value Decomposition (SVD), also called thin SVD. Deerwester
et al. briefly mentioned truncated SVD applied document similarity
word similarity, focus document similarity. Landauer Dumais (1997)
applied truncated SVD word similarity, achieving human-level scores multiple-choice
synonym questions Test English Foreign Language (TOEFL). Truncated
SVD applied document similarity called Latent Semantic Indexing (LSI),
called Latent Semantic Analysis (LSA) applied word similarity.
several ways thinking truncated SVD works. first
present math behind truncated SVD describe four ways looking it:
latent meaning, noise reduction, high-order co-occurrence, sparsity reduction.
SVD decomposes X product three matrices UVT , U V
column orthonormal form (i.e., columns orthogonal unit length, UT U =
VT V = I) diagonal matrix singular values (Golub & Van Loan, 1996). X
rank r, also rank r. Let k , k < r, diagonal matrix formed
top k singular values, let Uk Vk matrices produced selecting
corresponding columns U V. matrix Uk k VkT matrix rank k
best approximates original matrix X, sense minimizes approximation
errors. is, X = Uk k VkT minimizes kX XkF matrices X rank k,
k . . . kF denotes Frobenius norm (Golub & Van Loan, 1996).
Latent meaning: Deerwester et al. (1990) Landauer Dumais (1997) describe
truncated SVD method discovering latent meaning. Suppose word
context matrix X. truncated SVD, X = Uk k VkT , creates low-dimensional linear
mapping row space (words) column space (contexts). low-dimensional
mapping captures latent (hidden) meaning words contexts. Limiting
number latent dimensions (k < r) forces greater correspondence words
contexts. forced correspondence words contexts improves similarity
measurement.
Noise reduction: Rapp (2003) describes truncated SVD noise reduction technique.
may think matrix X = Uk k VkT smoothed version original matrix X.
matrix Uk maps row space (the space spanned rows X) smaller
k-dimensional space matrix Vk maps column space (the space spanned
columns X) k-dimensional space. diagonal matrix k specifies
weights reduced k-dimensional space. singular values ranked
descending order amount variation X fit. think matrix
X composed mixture signal noise, signal noise,
Uk k VkT mostly captures variation X due signal, whereas remaining
vectors UVT mostly fitting variation X due noise.
High-order co-occurrence: Landauer Dumais (1997) also describe truncated
SVD method discovering high-order co-occurrence. Direct co-occurrence (firstorder co-occurrence) two words appear identical contexts. Indirect co-occurrence
(high-order co-occurrence) two words appear similar contexts. Similarity
contexts may defined recursively terms lower-order co-occurrence. Lemaire
Denhiere (2006) demonstrate truncated SVD discover high-order co-occurrence.
159

fiTurney & Pantel

Sparsity reduction: general, matrix X sparse (mostly zeroes),
truncated SVD, X = Uk k VkT , dense. Sparsity may viewed problem insufficient
data: text, matrix X would fewer zeroes, VSM would perform
better chosen task. perspective, truncated SVD way simulating
missing text, compensating lack data (Vozalis & Margaritis, 2003).
different ways viewing truncated SVD compatible other;
possible perspectives correct. Future work likely provide
views SVD perhaps unifying view.
good C implementation SVD large sparse matrices Rohdes SVDLIBC.15
Another approach Brands (2006) incremental truncated SVD algorithm.16 Yet another
approach Gorrells (2006) Hebbian algorithm incremental truncated SVD. Brands
Gorrells algorithms introduce interesting new ways handling missing values,
instead treating zero values.
higher-order tensors, operations analogous truncated SVD,
parallel factor analysis (PARAFAC) (Harshman, 1970), canonical decomposition
(CANDECOMP) (Carroll & Chang, 1970) (equivalent PARAFAC discovered independently), Tucker decomposition (Tucker, 1966). overview tensor decompositions, see surveys Kolda Bader (2009) Acar Yener (2009). Turney (2007)
gives empirical evaluation well four different Tucker decomposition algorithms
scale large sparse third-order tensors. low-RAM algorithm, Multislice Projection,
large sparse tensors presented evaluated.17
Since work Deerwester et al. (1990), subsequent research discovered many
alternative matrix smoothing processes, Nonnegative Matrix Factorization (NMF)
(Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf,
Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Discrete
Component Analysis (DCA) (Buntine & Jakulin, 2006).
four perspectives truncated SVD, presented above, apply equally well
recent matrix smoothing algorithms. newer smoothing algorithms tend
computationally intensive truncated SVD, attempt model
word frequencies better SVD. Truncated SVD implicitly assumes elements
X Gaussian distribution. Minimizing Frobenius norm kX XkF
minimize noise, noise Gaussian distribution. However, known
word frequencies Gaussian distributions. recent algorithms based
realistic models distribution word frequencies.18
4.4 Comparing Vectors
popular way measure similarity two frequency vectors (raw weighted)
take cosine. Let x two vectors, n elements.
15.
16.
17.
18.

SVDLIBC available http://tedlab.mit.edu/dr/svdlibc/.
MATLAB source code available http://web.mit.edu/wingated/www/resources.html.
MATLAB source code available http://www.apperceptual.com/multislice/.
experience, pmiij appears approximately Gaussian, may explain PMI works well
truncated SVD, PPMI puzzling, less Gaussian PMI, yet apparently
yields better semantic models PMI.

160

fiFrom Frequency Meaning

x = hx1 , x2 , . . . , xn

(8)

= hy1 , y2 , . . . , yn

(9)

cosine angle x calculated follows:
Pn
cos(x, y) = qP
n

yi
Pn

i=1 xi

2
2
i=1 xi
i=1 yi
xy
=

xx yy
x

=

kxk kyk

(10)
(11)
(12)

words, cosine angle two vectors inner product
vectors, normalized unit length. x frequency vectors
words, frequent word long vector rare word short vector,
yet words might synonyms. Cosine captures idea length vectors
irrelevant; important thing angle vectors.
cosine ranges 1 vectors point opposite directions ( 180
degrees) +1 point direction ( 0 degrees). vectors
orthogonal ( 90 degrees), cosine zero. raw frequency vectors,
necessarily cannot negative elements, cosine cannot negative, weighting
smoothing often introduce negative elements. PPMI weighting yield negative
elements, truncated SVD generate negative elements, even input matrix
negative values.
measure distance vectors easily converted measure similarity
inversion (13) subtraction (14).

sim(x, y) = 1/dist(x, y)

(13)

sim(x, y) = 1 dist(x, y)

(14)

Many similarity measures proposed IR (Jones & Furnas, 1987)
lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir,
& McCarthy, 2004). commonly said IR that, properly normalized, difference
retrieval performance using different measures insignificant (van Rijsbergen, 1979).
Often vectors normalized way (e.g., unit length unit probability)
applying similarity measure.
Popular geometric measures vector distance include Euclidean distance Manhattan distance. Distance measures information theory include Hellinger, Bhattacharya,
Kullback-Leibler. Bullinaria Levy (2007) compared five distance measures
cosine similarity measure four different tasks involving word similarity. Overall,
best measure cosine. popular measures Dice Jaccard coefficients
(Manning et al., 2008).
161

fiTurney & Pantel

Lee (1999) proposed that, finding word similarities, measures focused
overlapping coordinates less importance negative features (i.e., coordinates
one word nonzero value zero value) appear perform
better. Lees experiments, Jaccard, Jensen-Shannon, L1 measures seemed
perform best. Weeds et al. (2004) studied linguistic statistical properties
similar words returned various similarity measures found measures
grouped three classes:
1. high-frequency sensitive measures (cosine, Jensen-Shannon, -skew, recall),
2. low-frequency sensitive measures (precision),
3. similar-frequency sensitive methods (Jaccard, Jaccard+MI, Lin, harmonic mean).
Given word w0 , use high-frequency sensitive measure score words wi
according similarity w0 , higher frequency words tend get higher scores
lower frequency words. use low-frequency sensitive measure,
bias towards lower frequency words. Similar-frequency sensitive methods prefer word wi
approximately frequency w0 . one experiment determining
compositionality collocations, high-frequency sensitive measures outperformed
classes (Weeds et al., 2004). believe determining appropriate similarity
measure inherently dependent similarity task, sparsity statistics,
frequency distribution elements compared, smoothing method applied
matrix.
4.5 Efficient Comparisons
Computing similarity rows (or columns) large matrix non-trivial
problem, worst case cubic running time O(n2r nc ), nr number rows
nc number columns (i.e., dimensionality feature space). Optimizations
parallelization often necessary.
4.5.1 Sparse-Matrix Multiplication
One optimization strategy generalized sparse-matrix multiplication approach (Sarawagi
& Kirpal, 2004), based observation scalar product two vectors
depends coordinates vectors nonzero values. Further,
observe commonly used similarity measures vectors x y, cosine,
overlap, Dice, decomposed three values: one depending nonzero
values x, another depending nonzero values y, third depending
nonzero coordinates shared x y. formally, commonly used similarity
scores, sim(x, y), expressed follows:
sim(x, y) = f0 (

Pn

i=1 f1 (xi , yi ), f2 (x), f3 (y))

(15)

example, cosine measure, cos(x, y), defined (10), expressed model
follows:
162

fiFrom Frequency Meaning

P
cos(x, y) = f0 ( ni=1 f1 (xi , yi ), f2 (x), f3 (y))

f0 (a, b, c) =
bc
f1 (a, b) = b
qP
n
2
f2 (a) = f3 (a) =
i=1 ai

(16)
(17)
(18)
(19)

Let X matrix want compute pairwise similarity, sim(x, y),
rows columns x y. Efficient computation similarity matrix
achieved leveraging fact sim(x, y) determined solely nonzero
coordinates shared x (i.e., f1 (0, xi ) = f1 (xi , 0) = 0 xi )
vectors sparse. case, calculating f1 (xi , yi ) required
vectors shared nonzero coordinate, significantly reducing cost computation.
Determining vectors share nonzero coodinate easily achieved first building
inverted index coordinates. indexing, also precompute f2 (x)
f3 (y) without changing algorithm complexity. Then, vector x retrieve
constant time, index, vector shares nonzero coordinate x
weP
apply f1 (xi , yi ) shared coordinates i. computational cost algorithm
Ni2 Ni number vectors nonzero i-th coordinate. worst
case time complexity O(ncv) n number vectors compared, c
maximum number nonzero coordinates vector, v number vectors
nonzero i-th coordinate coordinate nonzero
vectors. words, algorithm efficient density coordinates
low. experiments computing semantic similarity pairs
words large web crawl, observed near linear average running time complexity n.
computational cost reduced leveraging element weighting
techniques described Section 4.2. setting zero coordinates low
PPMI, PMI tf-idf score, coordinate density dramatically reduced cost
losing little discriminative power. vein, Bayardo, Ma, Srikant (2007) described
strategy omits coordinates highest number nonzero values.
algorithm gives significant advantage interested finding solely
similarity highly similar vectors.
4.5.2 Distributed Implementation using MapReduce
algorithm described Section 4.5.1 assumes matrix X fit memory,
large X may impossible. Also, element X processed independently, running parallel processes non-intersecting subsets X makes processing
faster. Elsayed, Lin, Oard (2008) proposed MapReduce implementation deployed using Hadoop, open-source software package implementing MapReduce framework
distributed file system.19 Hadoop shown scale several thousands machines,
allowing users write simple code, seamlessly manage sophisticated parallel execution code. Dean Ghemawat (2008) provide good primer MapReduce
programming.
19. Hadoop available download http://lucene.apache.org/hadoop/.

163

fiTurney & Pantel

MapReduce models Map step used start n Map tasks parallel,
caching one m-th part X inverted index streaming one n-th part X
it. actual inputs read tasks directly HDFS (Hadoop Distributed
File System). value determined amount memory dedicated
inverted index, n determined trading fact that, n increases,
parallelism obtained increased cost building inverted index
n times.
similarity algorithm Section 4.5.1 runs task Map step
MapReduce job. Reduce step groups output rows (or columns) X.
4.5.3 Randomized Algorithms
optimization strategies use randomized techniques approximate various similarity measures. aim randomized algorithms improve computational efficiency
(memory time) projecting high-dimensional vectors low-dimensional subspace.
Truncated SVD performs projection, SVD computationally intensive.20
insight randomized techniques high-dimensional vectors randomly projected low-dimensional subspace relatively little impact final similarity
scores. Significant reductions computational cost reported little average error computing true similarity scores, especially applications word
similarity interested top-k similar vectors vector
(Ravichandran, Pantel, & Hovy, 2005; Gorman & Curran, 2006).
Random Indexing, approximation technique based Sparse Distributed Memory
(Kanerva, 1993), computes pairwise similarity rows (or vectors) matrix
complexity O(nr nc 1 ), 1 fixed constant representing length index
vectors assigned column. value 1 controls tradeoff accuracy versus
efficiency. elements index vector mostly zeros small number
randomly assigned +1s 1s. cosine measure two rows r1 r2
approximated computing cosine two fingerprint vectors, fingerprint(r1 )
fingerprint(r2 ), fingerprint(r) computed summing index vectors
non-unique coordinate r. Random Indexing shown perform well LSA
word synonym selection task (Karlgren & Sahlgren, 2001).
Locality sensitive hashing (LSH) (Broder, 1997) another technique approximates
similarity matrix complexity O(n2r 2 ), 2 constant number random
projections, controls accuracy versus efficiency tradeoff.21 LSH general class
techniques defining functions map vectors (rows columns) short signatures
fingerprints, two similar vectors likely similar fingerprints. Definitions
LSH functions include Min-wise independent function, preserves Jaccard
similarity vectors (Broder, 1997), functions preserve cosine similarity
vectors (Charikar, 2002). word similarity task, Ravichandran et al. (2005)
showed that, average, 80% top-10 similar words random words found
top-10 results using Charikars functions, average cosine error 0.016
20. However, efficient forms SVD (Brand, 2006; Gorrell, 2006).
21. LSH stems work Rabin (1981), proposed use hash functions random irreducible
polynomials create short fingerprints collections documents. techniques useful many
tasks, removing duplicate documents (deduping) web crawl.

164

fiFrom Frequency Meaning

(using 2 = 10,000 random projections). Gorman Curran (2006) provide detailed
comparison Random Indexing LSH distributional similarity task. BNC
corpus, LSH outperformed Random Indexing; however, larger corpora combining BNC,
Reuters Corpus, English news holdings LDC 2003, Random
Indexing outperformed LSH efficiency accuracy.
4.6 Machine Learning
intended application VSM clustering classification, similarity measure
cosine (Section 4.4) may used. classification, nearest-neighbour algorithm
use cosine measure nearness (Dasarathy, 1991). clustering, similaritybased clustering algorithm use cosine measure similarity (Jain, Murty, & Flynn,
1999). However, many machine learning algorithms work directly
vectors VSM, without requiring external similarity measure, cosine.
effect, machine learning algorithms implicitly use internal approaches
measuring similarity.
machine learning algorithm works real-valued vectors use vectors
VSM (Witten & Frank, 2005). Linguistic processing (Section 3) mathematical
processing (Section 4) may still necessary, machine learning algorithm handle
vector comparison (Sections 4.4 4.5).
addition unsupervised (clustering) supervised (classification) machine learning, vectors VSM may also used semi-supervised learning (Ando & Zhang,
2005; Collobert & Weston, 2008). general, nothing unique VSMs would
compel choice one machine learning algorithm another, aside algorithms
performance given task. Therefore refer readers machine learning
literature (Witten & Frank, 2005), since advice specific VSMs.

5. Three Open Source VSM Systems
illustrate three types VSMs discussed Section 2, section presents three open
source systems, one VSM type. chosen present open source systems
interested readers obtain source code find systems
apply systems projects. three systems written Java
designed portability ease use.
5.1 TermDocument Matrix: Lucene
Lucene22 open source full-featured text search engine library supported Apache
Software Foundation. arguably ubiquitous implementation termdocument
matrix, powering many search engines CNET, SourceForge, Wikipedia, Disney,
AOL Comcast. Lucene offers efficient storage, indexing, well retrieval ranking
functionalities. Although primarily used termdocument matrix, generalizes
VSMs.
22. Apache Lucene available download http://lucene.apache.org/.

165

fiTurney & Pantel

Content, webpages, PDF documents, images, video, programmatically
decomposed fields stored database. database implements term
document matrix, content corresponds documents fields correspond terms.
Fields stored database indices computed field values. Lucene
uses fields generalization content terms, allowing string literal index
documents. example, webpage could indexed terms contains, also
anchor texts pointing it, host name, semantic classes
classified (e.g., spam, product review, news, etc.). webpage retrieved
search terms matching fields.
Columns termdocument matrix consist fields particular instance
content (e.g., webpage). rows consist instances content index.
Various statistics frequency tf-idf stored matrix. developer
defines fields schema identifies indexed Lucene. developer
also optionally defines content ranking function indexed field.
index built, Lucene offers functionalities retrieving content. Users
issue many query types phrase queries, wildcard queries, proximity queries, range
queries (e.g., date range queries), field-restricted queries. Results sorted
field index updates occur simultaneously searching. Lucenes index
directly loaded Tomcat webserver offers APIs common programming languages. Solr,23 separate Apache Software Foundation project, open source enterprise
webserver searching Lucene index presenting search results. full-featured
webserver providing functionalities XML/HTTP JSON APIs, hit highlighting,
faceted search, caching, replication.
simple recipe creating web search service, using Nutch, Lucene Solr, consists
crawling set URLs (using Nutch), creating termdocument matrix index (using
Lucene), serving search results (using Solr). Nutch,24 Apache Software Foundation
open source web search software, offers functionality crawling web seed set
URLs, building link-graph web crawl, parsing web documents
HTML pages. good set seed URLs Nutch downloaded freely
Open Directory Project.25 Crawled pages HTML-parsed, indexed
Lucene. resulting indexed collection queried served Solr
installation Tomcat.
information Lucene, recommend Gospodnetic Hatchers (2004)
book. Konchady (2008) explains integrate Lucene LingPipe GATE
sophisticated semantic processing.26

23.
24.
25.
26.

Apache Solr available download http://lucene.apache.org/solr/.
Apache Nutch available download http://lucene.apache.org/nutch/.
See http://www.dmoz.org/.
Information LingPipe available http://alias-i.com/lingpipe/. GATE (General Architecture Text Engineering) home page http://gate.ac.uk/.

166

fiFrom Frequency Meaning

5.2 WordContext Matrix: Semantic Vectors
Semantic Vectors27 open source project implementing random projection approach
measuring word similarity (see Section 4.5.3). package uses Lucene create term
document matrix, creates vectors Lucenes termdocument matrix, using
random projection dimensionality reduction. random projection vectors
used, example, measure semantic similarity two words find words
similar given word.
idea random projection take high-dimensional vectors randomly project
relatively low-dimensional space (Sahlgren, 2005). viewed
kind smoothing operation (Section 4.3), developers Semantic Vectors
package emphasize simplicity efficiency random projection (Section 4.5), rather
smoothing ability. argue matrix smoothing algorithms might
smooth better, none perform well random indexing, terms
computational complexity building smooth matrix incrementally updating
matrix new data arrives (Widdows & Ferraro, 2008). aim encourage
research development semantic vectors creating simple efficient open
source package.
Semantic Vectors package designed convenient use, portable, easy
extend modify. design software incorporates lessons learned
earlier Stanford Infomap project.28 Although default generate random projection
vectors, system modular design allows kinds wordcontext matrices
used instead random projection matrices.
package supports two basic functions: building wordcontext matrix searching
vectors matrix. addition generating word vectors, building
operation generate document vectors calculating weighted sums word vectors
words document. searching operation used search similar
words search documents similar query. query single word
several words combined, using various mathematical operations corresponding
vectors. mathematical operations include vector negation disjunction, based
quantum logic (Widdows, 2004). Widdows Ferraro (2008) provide good summary
Semantic Vectors software.
5.3 PairPattern Matrix: Latent Relational Analysis S-Space
Latent Relational Analysis29 (LRA) open source project implementing pairpattern
matrix. component S-Space package, library tools building
comparing different semantic spaces.
LRA takes input textual corpus set word pairs. pairpattern matrix
built deriving lexical patterns link together word pairs corpus. example, consider word pair hKorea, Japani following retrieved matching sentences:
27. Semantic Vectors software package measuring word similarity, available Simplified BSD
License http://code.google.com/p/semanticvectors/.
28. See http://infomap-nlp.sourceforge.net/.
29. Latent Relational Analysis part S-Space package distributed GNU General
Public License version 2. available http://code.google.com/p/airhead-research/. time
writing, LRA module development.

167

fiTurney & Pantel

Korea looks new Japan prime ministers effect Korea-Japan relations.
channel Korea vs. Japan football game?
two sentences, LRA extracts two patterns: X looks new X vs. .
patterns become two columns pairpattern matrix, word pair hKorea,
Japani becomes row. Pattern frequencies counted smoothed using SVD (see
Section 4.3).
order mitigate sparseness occurrences word pairs, thesaurus
WordNet used expand seed word pairs alternatives. example pair
hKorea, Japani may expanded include hSouth Korea, Japani, hRepublic Korea,
Japani, hKorea, Nipponi, hSouth Korea, Nipponi, hRepublic Korea, Nipponi.
LRA uses Lucene (see Section 5.1) backend store matrix, index it, serve
contents. detailed description LRA algorithm, suggest Turneys (2006)
paper.

6. Applications
section, survey semantic applications VSMs. aim
breadth, rather depth; readers want depth consult references.
goal give reader impression scope flexibility VSMs semantics.
following applications grouped according type matrix involved: term
document, wordcontext, pairpattern. Note section exhaustive;
many references applications space list here.
6.1 TermDocument Matrices
Termdocument matrices suited measuring semantic similarity documents
queries (see Section 2.1). usual measure similarity cosine column vectors
weighted termdocument matrix. variety applications measures
document similarity.
Document retrieval: termdocument matrix first developed document
retrieval (Salton et al., 1975), large body literature VSM
document retrieval (Manning et al., 2008), including several journals conferences
devoted topic. core idea is, given query, rank documents order
decreasing cosine angles query vector document vectors (Salton
et al., 1975). One variation theme cross-lingual document retrieval,
query one language used retrieve document another language (Landauer &
Littman, 1990). important technical advance discovery smoothing
termdocument matrix truncated SVD improve precision recall (Deerwester
et al., 1990), although commercial systems use smoothing, due computational
expense document collection large dynamic. Random indexing (Sahlgren,
2005) incremental SVD (Brand, 2006) may help address scaling issues. Another
important development document retrieval addition collaborative filtering,
form PageRank (Brin & Page, 1998).
Document clustering: Given measure document similarity, cluster
documents groups, similarity tends high within group, low across
168

fiFrom Frequency Meaning

groups (Manning et al., 2008). clusters may partitional (flat) (Cutting, Karger,
Pedersen, & Tukey, 1992; Pantel & Lin, 2002b) may hierarchical structure
(groups groups) (Zhao & Karypis, 2002); may non-overlapping (hard) (Croft,
1977) overlapping (soft) (Zamir & Etzioni, 1999). Clustering algorithms also differ
clusters compared abstracted. single-link clustering, similarity
two clusters maximum similarities members. Complete-link
clustering uses minimum similarities average-link clustering uses average
similarities (Manning et al., 2008).
Document classification: Given training set documents class labels
testing set unlabeled documents, task document classification learn
training set assign labels testing set (Manning et al., 2008). labels may
topics documents (Sebastiani, 2002), sentiment documents (e.g.,
positive versus negative product reviews) (Pang, Lee, & Vaithyanathan, 2002; Kim, Pantel,
Chklovski, & Pennacchiotti, 2006), spam versus non-spam (Sahami, Dumais, Heckerman, &
Horvitz, 1998; Pantel & Lin, 1998), labels might inferred words
documents. classify documents, implying documents
class similar way; thus document classification implies notion document
similarity, machine learning approaches document classification involve term
document matrix (Sebastiani, 2002). measure document similarity, cosine,
directly applied document classification using nearest-neighbour algorithm (Yang,
1999).
Essay grading: Student essays may automatically graded comparing
one high-quality reference essays given essay topic (Wolfe, Schreiner, Rehder,
Laham, Foltz, Kintsch, & Landauer, 1998; Foltz, Laham, & Landauer, 1999). student
essays reference essays compared cosines termdocument matrix.
grade assigned student essay proportional similarity one
reference essays; student essay highly similar reference essay gets high grade.
Document segmentation: task document segmentation partition document sections, section focuses different subtopic document
(Hearst, 1997; Choi, 2000). may treat document series blocks, block
sentence paragraph. problem detect topic shift one block
next. Hearst (1997) Choi (2000) use cosine columns wordblock
frequency matrix measure semantic similarity blocks. topic shift signaled
drop cosine consecutive blocks. wordblock matrix viewed
small termdocument matrix, corpus single document documents
blocks.
Question answering: Given simple question, task Question Answering (QA)
find short answer question searching large corpus. typical question is, many calories Big Mac? algorithms QA four
components, question analysis, document retrieval, passage retrieval, answer extraction
(Tellex, Katz, Lin, Fern, & Marton, 2003; Dang, Lin, & Kelly, 2006). Vector-based similarity measurements often used document retrieval passage retrieval (Tellex
et al., 2003).
Call routing: Chu-carroll Carpenter (1999) present vector-based system
automatically routing telephone calls, based callers spoken answer question,
169

fiTurney & Pantel

may direct call? callers answer ambiguous, system automatically
generates question caller, derived VSM, prompts caller
information.
6.2 WordContext Matrices
Wordcontext matrices suited measuring semantic similarity words (see
Section 2.2). example, measure similarity two words cosine
angle corresponding row vectors wordcontext matrix. many
applications measures word similarity.
Word similarity: Deerwester et al. (1990) discovered measure word similarity comparing row vectors termdocument matrix. Landauer Dumais (1997)
evaluated approach 80 multiple-choice synonym questions Test English Foreign Language (TOEFL), achieving human-level performance (64.4% correct
wordcontext matrix 64.5% average non-English US college applicant).
documents used Landauer Dumais average length 151 words,
seems short document, long context word. researchers soon
switched much shorter lengths, prefer call wordcontext matrices, instead termdocument matrices. Lund Burgess (1996) used context window
ten words. Schutze (1998) used fifty-word window (25 words, centered target
word). Rapp (2003) achieved 92.5% correct 80 TOEFL questions, using four-word
context window (2 words, centered target word, removing stop words).
TOEFL results suggest performance improves context window shrinks. seems
immediate context word much important distant context
determining meaning word.
Word clustering: Pereira, Tishby, Lee (1993) applied soft hierarchical clustering
row-vectors wordcontext matrix. one experiment, words nouns
contexts verbs given nouns direct objects. another experiment,
words verbs contexts nouns direct objects given
verbs. Schutzes (1998) seminal word sense discrimination model used hard flat clustering
row-vectors wordcontext matrix, context given window 25
words, centered target word. Pantel Lin (2002a) applied soft flat clustering
wordcontext matrix, context based parsed text. algorithms
able discover different senses polysemous words, generating different clusters
sense. effect, different clusters correspond different concepts underlie
words.
Word classification: Turney Littman (2003) used wordcontext matrix classify words positive (honest, intrepid) negative (disturbing, superfluous). used
General Inquirer (GI) lexicon (Stone, Dunphy, Smith, & Ogilvie, 1966) evaluate
algorithms. GI lexicon includes 11,788 words, labeled 182 categories related
opinion, affect, attitude.30 Turney Littman hypothesize 182 categories
discriminated wordcontext matrix.
Automatic thesaurus generation: WordNet popular tool research natural
language processing (Fellbaum, 1998), creating maintaing lexical resources
30. GI lexicon available http://www.wjh.harvard.edu/inquirer/spreadsheet guide.htm.

170

fiFrom Frequency Meaning

labour intensive, natural wonder whether process automated
degree.31 task seen instance word clustering (when thesaurus
generated scratch) classification (when existing thesaurus automatically
extended), worthwhile consider task automatic thesaurus generation
separately clustering classification, due specific requirements thesaurus,
particular kind similarity appropriate thesaurus (see Section 2.4).
Several researchers used wordcontext matrices specifically task assisting
automating thesaurus generation (Crouch, 1988; Grefenstette, 1994; Ruge, 1997; Pantel &
Lin, 2002a; Curran & Moens, 2002).
Word sense disambiguation: typical Word Sense Disambiguation (WSD) system
(Agirre & Edmonds, 2006; Pedersen, 2006) uses feature vector representation
vector corresponds token word, type (see Section 2.6). However, Leacock,
Towell, Voorhees (1993) used wordcontext frequency matrix WSD,
vector corresponds type annotated sense tag. Yuret Yatbaz (2009) applied
wordcontext frequency matrix unsupervised WSD, achieving results comparable
performance supervised WSD systems.
Context-sensitive spelling correction: People frequently confuse certain sets
words, there, theyre, their. confusions cannot detected simple dictionary-based spelling checker; require context-sensitive spelling correction.
wordcontext frequency matrix may used correct kinds spelling errors (Jones
& Martin, 1997).
Semantic role labeling: task semantic role labeling label parts sentence according roles play sentence, usually terms connection
main verb sentence. Erk (2007) presented system wordcontext
frequency matrix used improve performance semantic role labeling. Pennacchiotti, Cao, Basili, Croce, Roth (2008) show wordcontext matrices reliably
predict semantic frame unknown lexical unit refers, good levels
accuracy. lexical unit induction important semantic role labeling, narrow
candidate set roles observed lexical unit.
Query expansion: Queries submitted search engines Google Yahoo!
often directly match terms relevant documents. alleviate
problem, process query expansion used generating new search terms
consistent intent original query. VSMs form basis query semantics
models (Cao, Jiang, Pei, He, Liao, Chen, & Li, 2008). methods represent queries
using session contexts, query cooccurrences user sessions (Huang, Chien, &
Oyang, 2003; Jones, Rey, Madani, & Greiner, 2006), others use click contexts,
urls clicked result query (Wen, Nie, & Zhang, 2001).
Textual advertising: pay-per-click advertising models, prevalent search engines
Google Yahoo!, users pay keywords, called bidterms, used
display ads relevant queries issued users. scarcity data makes ad
matching difficult and, response, several techniques bidterm expansion using VSMs
proposed. wordcontext matrix consists rows bidterms columns
31. WordNet available http://wordnet.princeton.edu/.

171

fiTurney & Pantel

(contexts) consist advertiser identifiers (Gleich & Zhukov, 2004) co-bidded bidterms
(second order co-occurrences) (Chang, Pantel, Popescu, & Gabrilovich, 2009).
Information extraction: field information extraction (IE) includes named
entity recognition (NER: recognizing chunk text name entity,
person place), relation extraction, event extraction, fact extraction. Pasca et
al. (2006) demonstrate wordcontext frequency matrix facilitate fact extraction.
Vyas Pantel (2009) propose semi-supervised model using wordcontext matrix
building iteratively refining arbitrary classes named entities.
6.3 PairPattern Matrices
Pairpattern matrices suited measuring semantic similarity word pairs
patterns (see Section 2.3). example, measure similarity two word
pairs cosine angle corresponding row vectors pairpattern
matrix. many applications measures relational similarity.
Relational similarity: measure attributional similarity cosine
angle row vectors wordcontext matrix, measure relational
similarity cosine angle rows pairpattern matrix. approach
measuring relational similarity introduced Turney et al. (2003) examined
detail Turney Littman (2005). Turney (2006) evaluated approach
relational similarity 374 multiple-choice analogy questions SAT college
entrance test, achieving human-level performance (56% correct pairpattern matrix
57% correct average US college applicant). highest performance
far algorithm. best algorithm based attributional similarity accuracy
35% (Turney, 2006). best non-VSM algorithm achieves 43% (Veale, 2004).
Pattern similarity: Instead measuring similarity row vectors pair
pattern matrix, measure similarity columns; is, measure
pattern similarity. Lin Pantel (2001) constructed pairpattern matrix
patterns derived parsed text. Pattern similarity used infer one
phrase paraphrase another phrase, useful natural language generation,
text summarization, information retrieval, question answering.
Relational clustering: Bicici Yuret (2006) clustered word pairs representing
row vectors pairpattern matrix. Davidov Rappoport (2008) first clustered
contexts (patterns) identified representative pairs context cluster.
used representative pairs automatically generate multiple-choice analogy questions,
style SAT analogy questions.
Relational classification: Chklovski Pantel (2004) used pairpattern matrix
classify pairs verbs semantic classes. example, taint : poison classified
strength (poisoning stronger tainting) assess : review classified enablement
(assessing enabled reviewing). Turney (2005) used pairpattern matrix classify
noun compounds semantic classes. example, flu virus classified cause (the
virus causes flu), home town classified location (the home located town),
weather report classified topic (the topic report weather).
Relational search: Cafarella, Banko, Etzioni (2006) described relational search
task searching entities satisfy given semantic relations. example
172

fiFrom Frequency Meaning

query relational search engine list X X causes cancer.
example, relation, cause, one terms relation, cancer, given
user, task search engine find terms satisfy users query.
organizers Task 4 SemEval 2007 (Girju, Nakov, Nastase, Szpakowicz, Turney, & Yuret,
2007) envisioned two-step approach relational search: first conventional search engine
would look candidate answers, relational classification system would filter
incorrect answers. first step manually simulated Task 4 organizers
goal Task 4 design systems second step. task attracted 14 teams
submitted 15 systems. Nakov Hearst (2007) achieved good results using pairpattern
matrix.
Automatic thesaurus generation: discussed automatic thesaurus generation
Section 6.2, wordcontext matrices, arguably relational similarity relevant
attributional similarity thesaurus generation. example, information WordNet relations words rather words individually.
Snow, Jurafsky, Ng (2006) used pairpattern matrix build hypernym-hyponym
taxonomy, whereas Pennacchiotti Pantel (2006) built meronymy causation taxonomy. Turney (2008b) showed pairpattern matrix distinguish synonyms
antonyms, synonyms non-synonyms, taxonomically similar words (hair fur)
words merely semantically associated (cradle baby).
Analogical mapping: Proportional analogies form : b :: c : d, means
b c d. example, mason : stone :: carpenter : wood means mason stone
carpenter wood. 374 multiple-choice analogy questions SAT college
entrance test (mentioned above) involve proportional analogies. pairpattern
matrix, solve proportional analogies selecting choice maximizes relational
similarity (e.g., simr (mason : stone, carpenter : wood) high value). However, often
encounter analogies involve four terms. well-known analogy
solar system Rutherford-Bohr model atom contains least fourteen
terms. solar system, planet, attracts, revolves, sun, gravity, solar system,
mass. atom, revolves, atom, attracts, electromagnetism, nucleus,
charge, electron. Turney (2008a) demonstrated handle complex,
systematic analogies decomposing sets proportional analogies.

7. Alternative Approaches Semantics
applications list Section 6 necessarily require VSM approach.
application, many possible approaches. section, briefly
consider main alternatives.
Underlying applications termdocument matrices (Section 6.1) task
measuring semantic similarity documents queries. main alternatives
VSMs task probabilistic models, traditional probabilistic retrieval
models information retrieval (van Rijsbergen, 1979; Baeza-Yates & Ribeiro-Neto, 1999)
recent statistical language models inspired information theory (Liu &
Croft, 2005). idea statistical language models information retrieval measure
similarity query document creating probabilistic language model
173

fiTurney & Pantel

given document measuring probability given query according
language model.
progress information retrieval, distinction VSM approach
probabilistic approach becoming blurred, approach borrows ideas
other. Language models typically involve multiplying probabilities, view
adding logs probabilities, makes language models look similar VSMs.
applications wordcontext matrices (Section 6.2) share task measuring
semantic similarity words. main alternatives VSMs measuring word similarity
approaches use lexicons, WordNet (Resnik, 1995; Jiang & Conrath, 1997;
Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Budanitsky & Hirst, 2001). idea
view lexicon graph, nodes correspond word senses edges represent
relations words, hypernymy hyponymy. similarity two
words proportional length path graph joins two words.
Several approaches measuring semantic similarity words combine VSM
lexicon (Turney et al., 2003; Pantel, 2005; Patwardhan & Pedersen, 2006; Mohammad &
Hirst, 2006). Humans use dictionary definitions observations word usage,
natural expect best performance algorithms use distributional
lexical information.
Pairpattern matrices (Section 6.3) common task measuring semantic
similarity relations. wordcontext matrices, main alternatives approaches
use lexicons (Rosario & Hearst, 2001; Rosario, Hearst, & Fillmore, 2002; Nastase
& Szpakowicz, 2003; Veale, 2003, 2004). idea reduce relational similarity
attributional similarity, simr (a : b, c : d) sima (a, c) + sima (b, d), use lexicon
measure attributional similarity. discuss Section 2.4, reduction
work general. However, reduction often good approximation,
evidence hybrid approach, combining VSM lexicon, beneficial (Turney
et al., 2003; Nastase, Sayyad-Shirabad, Sokolova, & Szpakowicz, 2006).

8. Future Vector Space Models Semantics
Several authors criticized VSMs (French & Labiouse, 2002; Pado & Lapata, 2003;
Morris & Hirst, 2004; Budanitsky & Hirst, 2006). criticism stems
fact termdocument wordcontext matrices typically ignore word order. LSA,
instance, phrase commonly represented sum vectors individual
words phrase; hence phrases house boat boat house represented
vector, although different meanings. English, word order expresses
relational information. house boat boat house Tool-Purpose relation,
house boat means Tool-Purpose(boat, house) (a boat serves house), whereas boat
house means Tool-Purpose(house, boat) (a house sheltering storing boats).
Landauer (2002) estimates 80% meaning English text comes word
choice remaining 20% comes word order. However, VSMs inherently
limited 80% meaning text. Mitchell Lapata (2008) propose composition
models sensitive word order. example, make simple additive model become
syntax-aware, allow different weightings contributions vector components. Constituents important composition therefore participate
174

fiFrom Frequency Meaning

actively. Clark Pulman (2007) assigned distributional meaning sentences using Hilbert space tensor product. Widdows Ferraro (2008), inspired quantum
mechanics, explores several operators modeling composition meaning. Pairpattern
matrices sensitive order words pair (Turney, 2006). Thus
several ways handle word order VSMs.
raises question, limits VSMs semantics? semantics
represented VSMs? much yet know represent
VSMs. example, Widdows (2004) van Rijsbergen (2004) show disjunction,
conjunction, negation represented vectors, yet know
represent arbitrary statements first-order predicate calculus. However, seems possible
future work may discover answers limitations.
survey, assumed VSMs composed elements values
derived event frequencies. ties VSMs form distributional hypothesis
(see Sections 1.1 2.7); therefore limits VSMs depend limits family
distributional hypotheses. statistical patterns word usage sufficient figure
people mean? arguably major open question VSMs, answer
determine future VSMs. strong argument one way other,
believe continuing progress VSMs suggests far reaching
limits.

9. Conclusions
want information help person, use words make request
describe problem, person replies words. Unfortunately, computers
understand human language, forced use artificial languages unnatural user
interfaces. science fiction, dream computers understand human language,
listen us talk us. achieve full potential computers, must enable
understand semantics natural language. VSMs likely part
solution problem computing semantics.
Many researchers struggled problem semantics come
conclusion meaning words closely connected statistics word usage
(Section 2.7). try make intuition precise, soon find working
vectors values derived event frequencies; is, dealing VSMs.
survey, organized past work VSMs according structure
matrix (termdocument, wordcontext, pairpattern). believe structure
matrix important factor determining types applications
possible. linguistic processing (Section 3) mathematical processing (Section 4)
play smaller (but important) roles.
goal survey show breadth power VSMs, introduce
VSMs less familiar them, provide new perspective VSMs
already familiar them. hope emphasis structure
matrix inspire new research. reason believe three matrix
types present exhaust possibilities. expect new matrix types new tensors
open applications VSMs. seems possible us semantics
human language might one day captured kind VSM.
175

fiTurney & Pantel

Acknowledgments
Thanks Annie Zaenen prompting paper. Thanks Saif Mohammad Mariana
Soffer comments. Thanks Arkady Borkovsky Eric Crestan developing
distributed sparse-matrix multiplication algorithm, Marco Pennacchiotti
invaluable comments. Thanks anonymous reviewers JAIR helpful
comments suggestions.

References
Acar, E., & Yener, B. (2009). Unsupervised multiway data analysis: literature survey.
IEEE Transactions Knowledge Data Engineering, 21 (1), 620.
Agirre, E., & Edmonds, P. G. (2006). Word Sense Disambiguation: Algorithms Applications. Springer.
Ando, R. K. (2000). Latent semantic space: Iterative scaling improves precision interdocument similarity measurement. Proceedings 23rd Annual ACM SIGIR
Conference Research Development Information Retrieval (SIGIR-2000), pp.
216223.
Ando, R. K., & Zhang, T. (2005). framework learning predictive structures
multiple tasks unlabeled data. Journal Machine Learning Research, 6, 1817
1853.
Baeza-Yates, R., & Ribeiro-Neto, B. (1999). Modern Information Retrieval. Addison Wesley.
Barr, C., Jones, R., & Regelson, M. (2008). linguistic structure English websearch queries. Conference Empirical Methods Natural Language Processing
(EMNLP).
Bayardo, R. J., Ma, Y., & Srikant, R. (2007). Scaling pairs similarity search.
Proceedings 16th international conference World Wide Web (WWW 07),
pp. 131140, New York, NY. ACM.
Bicici, E., & Yuret, D. (2006). Clustering word pairs answer analogy questions.
Proceedings Fifteenth Turkish Symposium Artificial Intelligence Neural
Networks (TAINN 2006), Akyaka, Mugla, Turkey.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal
Machine Learning Research, 3, 9931022.
Brand, M. (2006). Fast low-rank modifications thin singular value decomposition.
Linear Algebra Applications, 415 (1), 2030.
Breese, J., Heckerman, D., & Kadie, C. (1998). Empirical analysis predictive algorithms
collaborative filtering. Proceedings 14th Conference Uncertainty
Artificial Intelligence, pp. 4352. Morgan Kaufmann.
Brin, S., & Page, L. (1998). anatomy large-scale hypertextual Web search engine.
Proceedings Seventh World Wide Web Conference (WWW7), pp. 107117.
Broder, A. (1997). resemblance containment documents. Compression
Complexity Sequences (SEQUENCES97), pp. 2129. IEEE Computer Society.
176

fiFrom Frequency Meaning

Budanitsky, A., & Hirst, G. (2001). Semantic distance WordNet: experimental,
application-oriented evaluation five measures. Proceedings Workshop
WordNet Lexical Resources, Second Meeting North American
Chapter Association Computational Linguistics (NAACL-2001), pp. 2924,
Pittsburgh, PA.
Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures semantic distance. Computational Linguistics, 32 (1), 1347.
Bullinaria, J., & Levy, J. (2007). Extracting semantic representations word cooccurrence statistics: computational study. Behavior Research Methods, 39 (3),
510526.
Buntine, W., & Jakulin, A. (2006). Discrete component analysis. Subspace, Latent
Structure Feature Selection: Statistical Optimization Perspectives Workshop
SLSFS 2005, pp. 133, Bohinj, Slovenia. Springer.
Cafarella, M. J., Banko, M., & Etzioni, O. (2006). Relational web search. Tech. rep., University Washington, Department Computer Science Engineering. Technical
Report 2006-04-02.
Cao, G., Nie, J.-Y., & Bai, J. (2005). Integrating word relationships language models.
Proceedings 28th Annual International ACM SIGIR Conference Research
Development Information Retrieval (SIGIR 05), pp. 298305, New York, NY.
ACM.
Cao, H., Jiang, D., Pei, J., He, Q., Liao, Z., Chen, E., & Li, H. (2008). Context-aware query
suggestion mining click-through session data. Proceeding 14th ACM
SIGKDD International Conference Knowledge Discovery Data Mining (KDD
08), pp. 875883. ACM.
Carroll, J. D., & Chang, J.-J. (1970). Analysis individual differences multidimensional
scaling via n-way generalization Eckart-Young decomposition. Psychometrika,
35 (3), 283319.
Chang, W., Pantel, P., Popescu, A.-M., & Gabrilovich, E. (2009). Towards intent-driven
bidterm suggestion. Proceedings WWW-09 (Short Paper), Madrid, Spain.
Charikar, M. S. (2002). Similarity estimation techniques rounding algorithms. Proceedings thiry-fourth annual ACM symposium Theory computing (STOC
02), pp. 380388. ACM.
Chew, P., Bader, B., Kolda, T., & Abdelali, A. (2007). Cross-language information retrieval using PARAFAC2. Proceedings 13th ACM SIGKDD International
Conference Knowledge Discovery Data Mining (KDD07), pp. 143152. ACM
Press.
Chiarello, C., Burgess, C., Richards, L., & Pollock, A. (1990). Semantic associative
priming cerebral hemispheres: words do, words dont . . . sometimes,
places. Brain Language, 38, 75104.
Chklovski, T., & Pantel, P. (2004). VerbOcean: Mining web fine-grained semantic
verb relations. Proceedings Experimental Methods Natural Language Processing 2004 (EMNLP-04), pp. 3340, Barcelona, Spain.
177

fiTurney & Pantel

Choi, F. Y. Y. (2000). Advances domain independent linear text segmentation.
Proceedings 1st Meeting North American Chapter Association
Computational Linguistics, pp. 2633.
Chu-carroll, J., & Carpenter, B. (1999). Vector-based natural language call routing. Computational Linguistics, 25 (3), 361388.
Church, K. (1995). One term two?. Proceedings 18th Annual International
ACM SIGIR Conference Research Development Information Retrieval, pp.
310318.
Church, K., & Hanks, P. (1989). Word association norms, mutual information, lexicography. Proceedings 27th Annual Conference Association Computational Linguistics, pp. 7683, Vancouver, British Columbia.
Clark, S., & Pulman, S. (2007). Combining symbolic distributional models meaning.
Proceedings AAAI Spring Symposium Quantum Interaction, pp. 5255.
Collobert, R., & Weston, J. (2008). unified architecture natural language processing:
Deep neural networks multitask learning. Proceedings 25th International
Conference Machine Learning (ICML-08), pp. 160167.
Croft, W. B. (1977). Clustering large files documents using single-link method.
Journal American Society Information Science, 28 (6), 341344.
Crouch, C. J. (1988). cluster-based approach thesaurus construction. Proceedings
11th Annual International ACM SIGIR Conference, pp. 309320, Grenoble,
France.
Curran, J. R., & Moens, M. (2002). Improvements automatic thesaurus extraction.
Unsupervised Lexical Acquisition: Proceedings Workshop ACL Special
Interest Group Lexicon (SIGLEX), pp. 5966, Philadelphia, PA.
Cutting, D. R., Karger, D. R., Pedersen, J. O., & Tukey, J. W. (1992). Scatter/gather:
cluster-based approach browsing large document collections. Proceedings
15th Annual International ACM SIGIR Conference, pp. 318329.
Dagan, I., Lee, L., & Pereira, F. C. N. (1999). Similarity-based models word cooccurrence
probabilities. Machine Learning, 34 (13), 4369.
Dang, H. T., Lin, J., & Kelly, D. (2006). Overview TREC 2006 question answering
track. Proceedings Fifteenth Text REtrieval Conference (TREC 2006).
Dasarathy, B. (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques.
IEEE Computer Society Press.
Davidov, D., & Rappoport, A. (2008). Unsupervised discovery generic relationships using
pattern clusters evaluation automatically generated SAT analogy questions.
Proceedings 46th Annual Meeting ACL HLT (ACL-HLT-08), pp.
692700, Columbus, Ohio.
Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified data processing large clusters.
Communications ACM, 51 (1), 107113.
178

fiFrom Frequency Meaning

Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.
(1990). Indexing latent semantic analysis. Journal American Society
Information Science (JASIS), 41 (6), 391407.
Elsayed, T., Lin, J., & Oard, D. (2008). Pairwise document similarity large collections
mapreduce. Proceedings Association Computational Linguistics
Human Language Technology Conference 2008 (ACL-08: HLT), Short Papers, pp.
265268, Columbus, Ohio. Association Computational Linguistics.
Erk, K. (2007). simple, similarity-based model selectional preferences. Proceedings
45th Annual Meeting Association Computational Linguistics, pp. 216
223,, Prague, Czech Republic.
Erk, K., & Pado, S. (2008). structured vector space model word meaning context.
Proceedings 2008 Conference Empirical Methods Natural Language
Processing (EMNLP-08), pp. 897906, Honolulu, HI.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.
Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies Linguistic
Analysis, pp. 132. Blackwell, Oxford.
Foltz, P. W., Laham, D., & Landauer, T. K. (1999). intelligent essay assessor: Applications educational technology. Interactive Multimedia Electronic Journal
Computer-Enhanced Learning, 1 (2).
Forman, G. (2003). extensive empirical study feature selection metrics text classification. Journal Machine Learning Research, 3, 12891305.
French, R. M., & Labiouse, C. (2002). Four problems extracting human semantics
large text corpora. Proceedings 24th Annual Conference Cognitive
Science Society.
Furnas, G. W., Landauer, T. K., Gomez, L. M., & Dumais, S. T. (1983). Statistical semantics: Analysis potential performance keyword information systems. Bell
System Technical Journal, 62 (6), 17531806.
Gentner, D. (1983). Structure-mapping: theoretical framework analogy. Cognitive
Science, 7 (2), 155170.
Gilbert, J. R., Moler, C., & Schreiber, R. (1992). Sparse matrices MATLAB: Design
implementation. SIAM Journal Matrix Analysis Applications, 13 (1), 333356.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). Semeval2007 task 04: Classification semantic relations nominals. Proceedings
Fourth International Workshop Semantic Evaluations (SemEval 2007), pp.
1318, Prague, Czech Republic.
Gleich, D., & Zhukov, L. (2004). SVD based term suggestion ranking system.
Proceedings Fourth IEEE International Conference Data Mining (ICDM
04), pp. 391394. IEEE Computer Society.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). Johns
Hopkins University Press, Baltimore, MD.
179

fiTurney & Pantel

Gorman, J., & Curran, J. R. (2006). Scaling distributional similarity large corpora.
Proceedings 21st International Conference Computational Linguistics
44th annual meeting Association Computational Linguistics (ACL 2006),
pp. 361368. Association Computational Linguistics.
Gorrell, G. (2006). Generalized Hebbian algorithm incremental singular value decomposition natural language processing. Proceedings 11th Conference
European Chapter Association Computational Linguistics (EACL-06), pp.
97104.
Gospodnetic, O., & Hatcher, E. (2004). Lucene Action. Manning Publications.
Grefenstette, G. (1994). Explorations Automatic Thesaurus Discovery. Kluwer.
Harris, Z. (1954). Distributional structure. Word, 10 (23), 146162.
Harshman, R. (1970). Foundations parafac procedure: Models conditions
explanatory multi-modal factor analysis. UCLA Working Papers Phonetics, 16.
Hearst, M. (1997). Texttiling: Segmenting text multi-paragraph subtopic passages.
Computational Linguistics, 23 (1), 3364.
Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detection
correction malapropisms. Fellbaum, C. (Ed.), WordNet: Electronic
Lexical Database, pp. 305332. MIT Press.
Hofmann, T. (1999). Probabilistic Latent Semantic Indexing. Proceedings 22nd
Annual ACM Conference Research Development Information Retrieval (SIGIR 99), pp. 5057, Berkeley, California.
Huang, C.-K., Chien, L.-F., & Oyang, Y.-J. (2003). Relevant term suggestion interactive
web search based contextual information query session logs. Journal
American Society Information Science Technology, 54 (7), 638649.
Hull, D. (1996). Stemming algorithms: case study detailed evaluation. Journal
American Society Information Science, 47 (1), 7084.
Jain, A., Murty, N., & Flynn, P. (1999). Data clustering: review. ACM Computing
Surveys, 31 (3), 264323.
Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus semantic similarity.
Proceedings International Conference Recent Advances Natural Language
Processing (RANLP-03), pp. 212219, Borovets, Bulgaria.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statistics
lexical taxonomy. Proceedings International Conference Research
Computational Linguistics (ROCLING X), pp. 1933, Tapei, Taiwan.
Johnson, H., & Martin, J. (2003). Unsupervised learning morphology English
Inuktitut. Proceedings HLT-NAACL 2003, pp. 4345.
Jones, M. P., & Martin, J. H. (1997). Contextual spelling correction using latent semantic analysis. Proceedings Fifth Conference Applied Natural Language
Processing, pp. 166173, Washington, DC.
180

fiFrom Frequency Meaning

Jones, R., Rey, B., Madani, O., & Greiner, W. (2006). Generating query substitutions.
Proceedings 15th international conference World Wide Web (WWW 06),
pp. 387396, New York, NY. ACM.
Jones, W. P., & Furnas, G. W. (1987). Pictures relevance: geometric analysis
similarity measures. Journal American Society Information Science, 38 (6),
420442.
Kanerva, P. (1993). Sparse distributed memory related models. Hassoun, M. H.
(Ed.), Associative neural memories, pp. 5076. Oxford University Press, New York,
NY.
Karlgren, J., & Sahlgren, M. (2001). words understanding. Uesaka, Y., Kanerva,
P., & Asoh, H. (Eds.), Foundations Real-World Intelligence, pp. 294308. CSLI
Publications.
Kim, S.-M., Pantel, P., Chklovski, T., & Pennacchiotti, M. (2006). Automatically assessing
review helpfulness. Proceedings 2006 Conference Empirical Methods
Natural Language Processing, pp. 423430.
Kolda, T., & Bader, B. (2009). Tensor decompositions applications. SIAM Review,
51 (3), 455500.
Konchady, M. (2008). Building Search Applications: Lucene, LingPipe, Gate. Mustru
Publishing.
Kraaij, W., & Pohlmann, R. (1996). Viewing stemming recall enhancement. Proceedings 19th Annual International ACM SIGIR Conference, pp. 4048.
Lakoff, G. (1987). Women, Fire, Dangerous Things. University Chicago Press,
Chicago, IL.
Landauer, T. K. (2002). computational basis learning cognition: Arguments
LSA. Ross, B. H. (Ed.), Psychology Learning Motivation: Advances
Research Theory, Vol. 41, pp. 4384. Academic Press.
Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latent semantic analysis theory acquisition, induction, representation knowledge.
Psychological Review, 104 (2), 211240.
Landauer, T. K., & Littman, M. L. (1990). Fully automatic cross-language document
retrieval using latent semantic indexing. Proceedings Sixth Annual Conference
UW Centre New Oxford English Dictionary Text Research, pp. 31
38, Waterloo, Ontario.
Landauer, T. K., McNamara, D. S., Dennis, S., & Kintsch, W. (2007). Handbook Latent
Semantic Analysis. Lawrence Erlbaum, Mahwah, NJ.
Lavrenko, V., & Croft, W. B. (2001). Relevance based language models. Proceedings
24th Annual International ACM SIGIR Conference Research Development
Information Retrieval (SIGIR 01), pp. 120127, New York, NY. ACM.
Leacock, C., & Chodrow, M. (1998). Combining local context WordNet similarity
word sense identification. Fellbaum, C. (Ed.), WordNet: Electronic Lexical
Database. MIT Press.
181

fiTurney & Pantel

Leacock, C., Towell, G., & Voorhees, E. (1993). Corpus-based statistical sense resolution.
Proceedings ARPA Workshop Human Language Technology, pp. 260265.
Lee, D. D., & Seung, H. S. (1999). Learning parts objects nonnegative matrix
factorization. Nature, 401, 788791.
Lee, L. (1999). Measures distributional similarity. Proceedings 37th Annual
Meeting Association Computational Linguistics, pp. 2532.
Lemaire, B., & Denhiere, G. (2006). Effects high-order co-occurrences word semantic
similarity. Current Psychology Letters: Behaviour, Brain & Cognition, 18 (1).
Lin, D. (1998). Automatic retrieval clustering similar words. roceedings
17th international conference Computational linguistics, pp. 768774. Association
Computational Linguistics.
Lin, D., & Pantel, P. (2001). DIRT discovery inference rules text. Proceedings
ACM SIGKDD Conference Knowledge Discovery Data Mining 2001, pp.
323328.
Linden, G., Smith, B., & York, J. (2003). Amazon.com recommendations: Item-to-item
collaborative filtering. IEEE Internet Computing, 7680.
Liu, X., & Croft, W. B. (2005). Statistical language modeling information retrieval.
Annual Review Information Science Technology, 39, 328.
Lovins, J. B. (1968). Development stemming algorithm. Mechanical Translation
Computational Linguistics, 11, 2231.
Lowe, W. (2001). Towards theory semantic space. Proceedings Twenty-first
Annual Conference Cognitive Science Society, pp. 576581.
Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces lexical
co-occurrence. Behavior Research Methods, Instruments, Computers, 28 (2), 203
208.
Lund, K., Burgess, C., & Atchley, R. A. (1995). Semantic associative priming highdimensional semantic space. Proceedings 17th Annual Conference
Cognitive Science Society, pp. 660665.
Manning, C., & Schutze, H. (1999). Foundations Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
Manning, C. D., Raghavan, P., & Schutze, H. (2008). Introduction Information Retrieval.
Cambridge University Press, Cambridge, UK.
Miller, G., Leacock, C., Tengi, R., & Bunker, R. (1993). semantic concordance.
Proceedings 3rd DARPA Workshop Human Language Technology, pp. 303
308.
Minnen, G., Carroll, J., & Pearce, D. (2001). Applied morphological processing English.
Natural Language Engineering, 7 (3), 207223.
Mitchell, J., & Lapata, M. (2008). Vector-based models semantic composition. Proceedings ACL-08: HLT, pp. 236244, Columbus, Ohio. Association Computational
Linguistics.
182

fiFrom Frequency Meaning

Mitchell, T. (1997). Machine Learning. McGraw-Hill, Columbus, OH.
Mohammad, S., & Hirst, G. (2006). Distributional measures concept-distance: taskoriented evaluation. Proceedings Conference Empirical Methods Natural
Language Processing (EMNLP-2006), pp. 3543.
Monay, F., & Gatica-Perez, D. (2003). image auto-annotation latent space models.
Proceedings Eleventh ACM International Conference Multimedia, pp.
275278.
Morris, J., & Hirst, G. (2004). Non-classical lexical semantic relations. Workshop
Computational Lexical Semantics, HLT-NAACL-04, Boston, MA.
Nakov, P., & Hearst, M. (2007). UCB: System description SemEval Task 4. Proceedings Fourth International Workshop Semantic Evaluations (SemEval 2007),
pp. 366369, Prague, Czech Republic.
Nakov, P., & Hearst, M. (2008). Solving relational similarity problems using theweb
corpus. Proceedings ACL-08: HLT, pp. 452460, Columbus, Ohio.
Nastase, V., Sayyad-Shirabad, J., Sokolova, M., & Szpakowicz, S. (2006). Learning nounmodifier semantic relations corpus-based WordNet-based features. Proceedings 21st National Conference Artificial Intelligence (AAAI-06), pp.
781786.
Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations.
Fifth International Workshop Computational Semantics (IWCS-5), pp. 285301,
Tilburg, Netherlands.
Niwa, Y., & Nitta, Y. (1994). Co-occurrence vectors corpora vs. distance vectors
dictionaries. Proceedings 15th International Conference Computational
Linguistics, pp. 304309, Kyoto, Japan.
Nosofsky, R. (1986). Attention, similarity, identification-categorization relationship.
Journal Experimental Psychology: General, 115 (1), 3957.
Ogden, C. K. (1930). Basic English: General Introduction Rules Grammar.
Kegan Paul, Trench, Trubner Co.
Pado, S., & Lapata, M. (2003). Constructing semantic space models parsed corpora.
Proceedings 41st Annual Meeting Association Computational Linguistics, pp. 128135, Sapporo, Japan.
Pado, S., & Lapata, M. (2007). Dependency-based construction semantic space models.
Computational Linguistics, 33 (2), 161199.
Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? sentiment classification using
machine learning techniques. Proceedings Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 7986, Philadelphia, PA.
Pantel, P. (2005). Inducing ontological co-occurrence vectors. Proceedings Association
Computational Linguistics (ACL-05), pp. 125132.
Pantel, P., & Lin, D. (1998). Spamcop: spam classification organization program.
Learning Text Categorization: Papers AAAI 1998 Workshop, pp. 9598.
183

fiTurney & Pantel

Pantel, P., & Lin, D. (2002a). Discovering word senses text. Proceedings
Eighth ACM SIGKDD International Conference Knowledge Discovery Data
Mining, pp. 613619, Edmonton, Canada.
Pantel, P., & Lin, D. (2002b). Document clustering committees. Proceedings
25th Annual International ACM SIGIR Conference, pp. 199206.
Pasca, M., Lin, D., Bigham, J., Lifchits, A., & Jain, A. (2006). Names similarities
Web: Fact extraction fast lane. Proceedings 21st International
Conference Computational Linguistics 44th Annual Meeting ACL, pp.
809816, Sydney, Australia.
Patwardhan, S., & Pedersen, T. (2006). Using wordnet-based context vectors estimate
semantic relatedness concepts. Proceedings Workshop Making
Sense Sense 11th Conference European Chapter Association
Computational Linguistics (EACL-2006), pp. 18.
Pedersen, T. (2006). Unsupervised corpus-based methods WSD. Word Sense Disambiguation: Algorithms Applications, pp. 133166. Springer.
Pennacchiotti, M., Cao, D. D., Basili, R., Croce, D., & Roth, M. (2008). Automatic induction
FrameNet lexical units. Proceedings 2008 Conference Empirical Methods
Natural Language Processing (EMNLP-08), pp. 457465, Honolulu, Hawaii.
Pennacchiotti, M., & Pantel, P. (2006). Ontologizing semantic relations. Proceedings
21st International Conference Computational Linguistics 44th annual
meeting Association Computational Linguistics, pp. 793800. Association
Computational Linguistics.
Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering English words.
Proceedings 31st Annual Meeting Association Computational Linguistics,
pp. 183190.
Porter, M. (1980). algorithm suffix stripping. Program, 14 (3), 130137.
Rabin, M. O. (1981). Fingerprinting random polynomials. Tech. rep., Center research
Computing technology, Harvard University. Technical Report TR-15-81.
Rapp, R. (2003). Word sense discovery based sense descriptor dissimilarity. Proceedings Ninth Machine Translation Summit, pp. 315322.
Ravichandran, D., Pantel, P., & Hovy, E. (2005). Randomized algorithms nlp: using
locality sensitive hash function high speed noun clustering. Proceedings
43rd Annual Meeting Association Computational Linguistics (ACL 05), pp.
622629, Morristown, NJ. Association Computational Linguistics.
Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., & Riedl, J. (1994). Grouplens: open
architecture collaborative filtering netnews. Proceedings ACM 1994
Conference Computer Supported Cooperative Work, pp. 175186. ACM Press.
Resnik, P. (1995). Using information content evaluate semantic similarity taxonomy.
Proceedings 14th International Joint Conference Artificial Intelligence
(IJCAI-95), pp. 448453, San Mateo, CA. Morgan Kaufmann.
184

fiFrom Frequency Meaning

Rosario, B., & Hearst, M. (2001). Classifying semantic relations noun-compounds
via domain-specific lexical hierarchy. Proceedings 2001 Conference
Empirical Methods Natural Language Processing (EMNLP-01), pp. 8290.
Rosario, B., Hearst, M., & Fillmore, C. (2002). descent hierarchy, selection
relational semantics. Proceedings 40th Annual Meeting Association
Computational Linguistics (ACL-02), pp. 247254.
Rosch, E., & Lloyd, B. (1978). Cognition Categorization. Lawrence Erlbaum, Hillsdale,
NJ.
Ruge, G. (1997). Automatic detection thesaurus relations information retrieval applications. Freksa, C., Jantzen, M., & Valk, R. (Eds.), Foundations Computer
Science, pp. 499506. Springer.
Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998). Bayesian approach
filtering junk e-mail. Proceedings AAAI-98 Workshop Learning Text
Categorization.
Sahlgren, M. (2005). introduction random indexing. Proceedings Methods
Applications Semantic Indexing Workshop 7th International Conference
Terminology Knowledge Engineering (TKE), Copenhagen, Denmark.
Sahlgren, M. (2006). Word-Space Model: Using distributional analysis represent syntagmatic paradigmatic relations words high-dimensional vector spaces.
Ph.D. thesis, Department Linguistics, Stockholm University.
Salton, G. (1971). SMART retrieval system: Experiments automatic document processing. Prentice-Hall, Upper Saddle River, NJ.
Salton, G., & Buckley, C. (1988). Term-weighting approaches automatic text retrieval.
Information Processing Management, 24 (5), 513523.
Salton, G., Wong, A., & Yang, C.-S. (1975). vector space model automatic indexing.
Communications ACM, 18 (11), 613620.
Sarawagi, S., & Kirpal, A. (2004). Efficient set joins similarity predicates. Proceedings 2004 ACM SIGMOD International Conference Management Data
(SIGMOD 04), pp. 743754, New York, NY. ACM.
Scholkopf, B., Smola, A. J., & Muller, K.-R. (1997). Kernel principal component analysis.
Proceedings International Conference Artificial Neural Networks (ICANN1997), pp. 583588, Berlin.
Schutze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24 (1),
97124.
Schutze, H., & Pedersen, J. (1993). vector model syntagmatic paradigmatic
relatedness. Making Sense Words: Proceedings Conference, pp. 104113,
Oxford, England.
Sebastiani, F. (2002). Machine learning automated text categorization. ACM Computing
Surveys (CSUR), 34 (1), 147.
Shannon, C. (1948). mathematical theory communication. Bell System Technical
Journal, 27, 379423, 623656.
185

fiTurney & Pantel

Singhal, A., Salton, G., Mitra, M., & Buckley, C. (1996). Document length normalization.
Information Processing Management, 32 (5), 619633.
Smith, E., Osherson, D., Rips, L., & Keane, M. (1988). Combining prototypes: selective
modification model. Cognitive Science, 12 (4), 485527.
Snow, R., Jurafsky, D., & Ng, A. Y. (2006). Semantic taxonomy induction heterogenous evidence. Proceedings 21st International Conference Computational
Linguistics 44th annual meeting ACL, pp. 801808.
Sparck Jones, K. (1972). statistical interpretation term specificity application
retrieval. Journal Documentation, 28 (1), 1121.
Spearman, C. (1904). General intelligence, objectively determined measured. American Journal Psychology, 15, 201293.
Sproat, R., & Emerson, T. (2003). first international Chinese word segmentation bakeoff. Proceedings Second SIGHAN Workshop Chinese Language Processing,
pp. 133143, Sapporo, Japan.
Stone, P. J., Dunphy, D. C., Smith, M. S., & Ogilvie, D. M. (1966). General Inquirer:
Computer Approach Content Analysis. MIT Press, Cambridge, MA.
Tan, B., & Peng, F. (2008). Unsupervised query segmentation using generative language
models Wikipedia. Proceeding 17th international conference World
Wide Web (WWW 08), pp. 347356, New York, NY. ACM.
Tellex, S., Katz, B., Lin, J., Fern, A., & Marton, G. (2003). Quantitative evaluation
passage retrieval algorithms question answering. Proceedings 26th Annual
International ACM SIGIR Conference Research Development Information
Retrieval (SIGIR), pp. 4147.
Tucker, L. R. (1966). mathematical notes three-mode factor analysis. Psychometrika, 31 (3), 279311.
Turney, P. D. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL.
Proceedings Twelfth European Conference Machine Learning (ECML-01),
pp. 491502, Freiburg, Germany.
Turney, P. D. (2005). Measuring semantic similarity latent relational analysis. Proceedings Nineteenth International Joint Conference Artificial Intelligence
(IJCAI-05), pp. 11361141, Edinburgh, Scotland.
Turney, P. D. (2006). Similarity semantic relations. Computational Linguistics, 32 (3),
379416.
Turney, P. D. (2007). Empirical evaluation four tensor decomposition algorithms. Tech.
rep., Institute Information Technology, National Research Council Canada.
Technical Report ERB-1152.
Turney, P. D. (2008a). latent relation mapping engine: Algorithm experiments.
Journal Artificial Intelligence Research, 33, 615655.
Turney, P. D. (2008b). uniform approach analogies, synonyms, antonyms, associations. Proceedings 22nd International Conference Computational
Linguistics (Coling 2008), pp. 905912, Manchester, UK.
186

fiFrom Frequency Meaning

Turney, P. D., & Littman, M. L. (2003). Measuring praise criticism: Inference
semantic orientation association. ACM Transactions Information Systems,
21 (4), 315346.
Turney, P. D., & Littman, M. L. (2005). Corpus-based learning analogies semantic
relations. Machine Learning, 60 (13), 251278.
Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independent
modules solve multiple-choice synonym analogy problems. Proceedings
International Conference Recent Advances Natural Language Processing
(RANLP-03), pp. 482489, Borovets, Bulgaria.
Van de Cruys, T. (2009). non-negative tensor factorization model selectional preference
induction. Proceedings Workshop Geometric Models Natural Language
Semantics (GEMS-09), pp. 8390, Athens, Greece.
van Rijsbergen, C. J. (2004). Geometry Information Retrieval. Cambridge University
Press, Cambridge, UK.
van Rijsbergen, C. J. (1979). Information Retrieval. Butterworths.
Veale, T. (2003). analogical thesaurus. Proceedings 15th Innovative Applications Artificial Intelligence Conference (IAAI 2003), pp. 137142, Acapulco,
Mexico.
Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy.
Proceedings 16th European Conference Artificial Intelligence (ECAI 2004),
pp. 606612, Valencia, Spain.
Vozalis, E., & Margaritis, K. (2003). Analysis recommender systems algorithms.
Proceedings 6th Hellenic European Conference Computer Mathematics
Applications (HERCMA-2003), Athens, Greece.
Vyas, V., & Pantel, P. (2009). Semi-automatic entity set refinement. Proceedings
NAACL-09, Boulder, CO.
Weaver, W. (1955). Translation. Locke, W., & Booth, D. (Eds.), Machine Translation
Languages: Fourteen Essays. MIT Press, Cambridge, MA.
Weeds, J., Weir, D., & McCarthy, D. (2004). Characterising measures lexical distributional similarity. Proceedings 20th International Conference Computational Linguistics (COLING 04), pp. 10151021. Association Computational
Linguistics.
Wei, X., Peng, F., & Dumoulin, B. (2008). Analyzing web text association disambiguate
abbreviation queries. Proceedings 31st Annual International ACM SIGIR
Conference Research Development Information Retrieval (SIGIR 08), pp.
751752, New York, NY. ACM.
Wen, J.-R., Nie, J.-Y., & Zhang, H.-J. (2001). Clustering user queries search engine.
Proceedings 10th International Conference World Wide Web (WWW 01),
pp. 162168, New York, NY. ACM.
Widdows, D. (2004). Geometry Meaning. Center Study Language
Information, Stanford, CA.
187

fiTurney & Pantel

Widdows, D., & Ferraro, K. (2008). Semantic vectors: scalable open source package
online technology management application. Proceedings Sixth International
Conference Language Resources Evaluation (LREC 2008), pp. 11831190.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools
Techniques Java Implementations. Morgan Kaufmann, San Francisco.
Wittgenstein, L. (1953). Philosophical Investigations. Blackwell. Translated G.E.M.
Anscombe.
Wolfe, M. B. W., Schreiner, M. E., Rehder, B., Laham, D., Foltz, P. W., Kintsch, W., &
Landauer, T. K. (1998). Learning text: Matching readers texts latent
semantic analysis. Discourse Processes, 25, 309336.
Yang, Y. (1999). evaluation statistical approaches text categorization. Information
Retrieval, 1 (1), 6990.
Yuret, D., & Yatbaz, M. A. (2009). noisy channel model unsupervised word sense
disambiguation. Computational Linguistics. review.
Zamir, O., & Etzioni, O. (1999). Grouper: dynamic clustering interface Web search
results. Computer Networks: International Journal Computer Telecommunications Networking, 31 (11), 13611374.
Zhao, Y., & Karypis, G. (2002). Evaluation hierarchical clustering algorithms document datasets. Proceedings Eleventh International Conference Information Knowledge Management, pp. 515524, McLean, Virginia.

188

fiJournal Artificial Intelligence Research 37 (2010) 437477

Submitted 8/09; published 3/10

Reasoning Transfer Control
Wiebe van der Hoek
Dirk Walther
Michael Wooldridge

W IEBE .VAN -D ER -H OEK @ LIV. AC . UK
DWALTHER @ LIV. AC . UK
MJW @ LIV. AC . UK

Department Computer Science
University Liverpool, UK

Abstract
present DCL - PC: logic reasoning abilities agents coalitions
agents altered transferring control one agent another. logical foundation
DCL - PC CL - PC, logic reasoning cooperation abilities agents
coalitions agents stem distribution atomic Boolean variables individual agents
choices available coalition correspond assignments variables coalition controls.
basic modal constructs CL - PC form coalition C cooperate bring .
DCL - PC extends CL - PC dynamic logic modalities atomic programs form
agent gives control variable p agent j; usual dynamic logic, atomic programs
may combined using sequence, iteration, choice, test operators form complex programs.
combining dynamic transfer programs cooperation modalities, becomes possible
reason power agents coalitions affected transfer control. give
two alternative semantics logic: direct semantics, capture distributions
Boolean variables agents; conventional Kripke semantics. prove
semantics equivalent, present axiomatization logic. investigate
computational complexity model checking satisfiability DCL - PC, show
problems PSPACE-complete (and hence worse underlying logic CL - PC). Finally,
investigate characterisation control DCL - PC. distinguish first-order control
ability agent coalition control state affairs assignment values
variables control agent coalition second-order control ability
agent exert control control agents transferring variables
agents. give logical characterisation second-order control.

1. Introduction
recent years, much activity development logics reasoning
strategic cooperative abilities agents game-like multi-agent systems. Coalition Logic (Pauly,
2001) Alternating-time Temporal Logic (ATL) Alur, Henzinger, Kupferman (2002)
perhaps best-known examples work. logics widely used base
investigate reasoning cooperation multi-agent systems (van der Hoek &
Wooldridge, 2003; Jamroga & van der Hoek, 2004; Goranko & Jamroga, 2004).
Although differ details, basic construct Coalition Logic ATL
cooperation modality, construct written ATL hhCii. intended meaning
expression coalition C cooperate way ensure that, matter
agents outside C do, property becomes true. Another way think hhCii
meaning coalition C collective power ensure . often assumed powers
additive, sense powers coalition derive powers coalition members,
c
2010
AI Access Foundation. rights reserved.

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

adding agent coalition reduce powers coalition. However,
origin individual agents powers is, powers derive rarely discussed
cooperation logic literature.
One natural interpretation powers abilities computational systems arises
considering system components ability assign values variables making
overall system state. Power, sense, equates ability choose value
particular variable. Motivated observation, van der Hoek Wooldridge developed CL - PC,
cooperation logic powers specified allocating every agent set Boolean
variables: choices (and hence powers) available coalition correspond possible
assignments truth falsity may made variables control (van der Hoek
& Wooldridge, 2005b). CL - PC expression C means coalition C assign values
variables control way make true. Van der Hoek Wooldridge
gave complete axiomatization CL - PC, showed model checking satisfiability
problems logic PSPACE-complete; also investigated CL - PC could used
characterise closely related notion control. However, one drawback CL - PC
power structure underpinning logic distribution variables agents assumed
fixed, hence coalitional powers static CL - PC.
Ultimately, course, assumption powers static realistic. example,
explicit transfer power control fundamental component human organisations,
enabling avoid bottlenecks respect centralised power control. Moreover,
open environments, agents join leave system run-time, may possible
know advance agents fulfill roles, static power allocation schemes
simply appropriate environments. software agents deployed environments
power structures dynamic, important consider issues representing
reasoning them, issue address present paper.
study variant CL - PC allows us explicitly reason dynamic power structures.
logic DCL - PC extends CL - PC dynamic logic operators (Harel, Kozen, & Tiuryn, 2000),
atomic programs form ;p j, read agent gives control variable
p agent j. pre-condition program variable p agent allocation
variables, executing program effect transferring variable p agent agent
j. Thus dynamic component DCL - PC concerned transferring power systems,
using logic, reason abilities agents coalitions affected
transfers. Note that, conventional dynamic logic, atomic programs may combined
DCL - PC usual sequential composition (;), non-deterministic choice (), test (?),
iteration ( ) operations, form complex programs. features, DCL - PC provides
rich framework represent reason systems power/control
dynamically allocated.
remainder paper, following introduction logic, make four main contributions respect DCL - PC:
First, Section 2, give two alternative semantics logic: direct semantics,
models directly represent allocation propositional variables agents
control them, conventional Kripke semantics. prove two semantics
equivalent.
438

fiR EASONING BOUT RANSFER C ONTROL

Second, give axiomatization DCL - PC Section 3, show axiomatization
sound complete (with respect semantics).
Third, show Section 4 that, despite apparently additional expressive power provided
dynamic component DCL - PC, satisfiability model checking problems
DCL - PC complex corresponding problems CL - PC (van der Hoek &
Wooldridge, 2005b): PSPACE-complete.
Fourth, distinguish first-order control second-order control Section 5.
first-order control, introduced studied van der Hoek Wooldridge (2005b),
ability control state affairs assigning values variables, second-order
control ability agent exert control ability agents control
states affairs. Agents coalitions exercise second-order control transferring variables control agents. informally discussing introducing secondorder control, develop logical characterisation it, sense characterise
formulae agent second-order control.
conclude brief comments related work conclusions. Note omit
detailed introduction cooperation logics particular motivation behind CL - PC,
done van der Hoek Wooldridge (2005b).

2. Logic DCL - PC
section, define logic DCL - PC.
2.1 Informal Introduction
begin informal introduction; readers familiar CL - PC dynamic
logic may wish skim skip completely introductory section.
noted earlier, DCL - PC extends logic CL - PC,
"#####$
"#####$
begin briefly reviewing logic. CL - PC
'
*
"
!
intended allow us reason domains containing
+
(
collection agents, collection propositional
variables; let = {1, . . . , n} denote set agents,
,
P denote variables. assumed CL - PC
"#####$
"#####$
agent system controls subset vari&
%
)
ables P. keep things simple, assumed agents
exercise unique control: every variable controlled
.
exactly one agent, variables P partitioned
among agents A. agent, denote variables control Pi , Pi P.
Figure 1: typical scenario.
abilities powers agent scenario correspond assignments truth falsity make variables control.
Figure 1 illustrates typical scenario: four agents, = {1, 2, 3, 4}, eight variables,
P = {p, q, r, s, t, u, v, w}. Agent 1 controls variables p q, (P 1 = {p, q}), agent 2 controls
variable r, (P2 = {r}), on. scenario illustrated, variables p, q, s, w value
439

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

1 (i.e., true), variables value 0 (false). language CL - PC intended allow us represent reason scenarios. represent values variables,
use propositional logic, following formula completely characterises values
variables scenario:
p q w r u v
Agents able change value variables
control, represent abilities CL PC use contingent ability operator (van der Hoek
& Wooldridge, 2005b): expression C means that,
assumption world remains otherwise
unchanged, set agents C modify value
variables make true. respect
scenario Figure 1, example,

"-----.
"

"-----.

%

(

&

)

!

*
$

'

+

#

,

1,2 (p r q).
"

agent 1 leave variable p set true
making variable q false, agent 2 makes variable r true: result formula p r q
true.
fact matter coalition C do,
remain true expressed 2C . scenario Figure 1, matter agent 1 does, r remain false
(assuming agent acts). Thus have:

%
$

&

!

)
*

$

21 r.

(

"

%
'

+

#

,

shown elsewhere (van der Hoek & Wooldridge, 2005b),
defined below, types ability operators may
also defined.
Figure 2: effect executing
Thus far, operators introduced
atomic transfer program.
part CL - PC language. Let us start introduce dynamic aspects language, specific
DCL - PC . First, idea atomic transfer program, written ; p j, meaning agent
transfers power choose truth value variable p agent j. Now, possible
execute program ;p j iff variable p actually control agent i. example,
respect Figure 1, programs 1 ;p 2 2 ;r 1 executable, program 1 ;r 2
(since r control 1). (fairly obvious) effect executing program
1 ;p 2 illustrated Figure 2; note actual value variable transferred
unchanged transfer.
DCL - PC, allow atomic programs combined together make complex programs
using constructs dynamic logic: ; (for sequential composition), (iteration), ? (the
test operator). simplest sequential composition: example, program
1 ;p 2; 2 ;r 1
440

fiR EASONING BOUT RANSFER C ONTROL

means that, first, agent 1 gives variable p 2, then, agent 2 gives r 1. operator
non-deterministic choice operator. 1 2 transfer programs, 1 2 means either
program 1 2 . operator used define iteration: expression means execute
program zero times (it defined exactly many times executed).
Finally, ? used perform tests. program ? executed particular scenario
formula true scenario. illustrate operator work, consider
following example programs.
p?; 1 ;p 2
first program says p true, agent 1 gives p agent 2. Now, since p true
scenario Figure 1, program executed scenario Figure 1, net
result final scenario Figure 2.
following program uses non-deterministic choice, essentially says agent 1 gives either
p q 2.
(1 ;p 2) (1 ;q 2)
usual dynamic logic, define iteration selection constructs used conventional imperative programming languages basic program constructs. example,
conventional programming construct

defined using following transfer program construct (see, e.g., Harel et al., 2000):
(?; ) ; ?
next step see transfer programs incorporated ability constructs CL - PC.
able refer transfer programs properties within language DCL - PC,
use dynamic operators h [ ]. operators play role DCL - PC
play conventional dynamic logic (Harel et al., 2000). Thus formula h asserts
exists computation program , starting current situation,
terminated holds. Note h assert guaranteed terminate, merely
least one terminating computation. moreover, state satisfied
every terminating computation ; merely terminating computation
end situation satisfying . Thus h acts existential quantifier computations
. operator [ ] universal quantifier computations . asserts every
terminating computation , property holds. Note assert fact
terminating computations.
example use constructs, following formula asserts agent 1 gives
either p q 2, 2 able achieve (p q) r.
[(1 ;p 2) (1 ;q 2)]2 (p q) r
easy see formula expresses true property scenario Figure 1: program
(1 ;p 2) (1 ;q 2) executable scenario, executed, agent 2 control
variable r either variable p variable q. Agent 2 thus able make (p q) r true.
conclude introductory section, consider following complex example.
following DCL - PC formula asserts possible agent give away variables agent j,
441

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

non-deterministically choosing one variable time, agent j ability achieve .
[
hwhile j
;p ji>
pPi

2.2 Syntax
Formally, language DCL - PC formed respect (fixed, finite, non-empty) set
agents, (fixed, finite, non-empty) set P propositional variables. Figure 3 defines syntax
DCL - PC. use > logical constant truth, negation, disjunction.
usual, define remaining connectives classical propositional logic abbreviations:





=

=

=

=


>
( )

( ) ( ).

Additionally, set DCL - PC formulas, write 5 mean exactly one
member true:
_
^
(1 2 ).
5 =




1 6=2

= {1 , 2 . . . n }, also write 1 5 2 5 5 n 5 .
respect transfer programs, constructs conventional imperative programs may
defined follows (Harel et al., 2000):
1 else 2

repeat
skip
fail

=

=

=

=

=


((?; 1 ) (?; 2 ))
((?; ) ; ?)
; (?; ) ; ?
>?
?

possibility confusion, omit set brackets cooperation modalities,
example writing 1,2 rather {1,2} . DCL - PC formula containing modalities said
objective formula.
Let P() denote set propositional variables occurring DCL - PC formula , let A()
denote set agents named (i.e., A() union coalitions occurring
cooperation modalities agents occurring transfer programs ).
Although operator useful define programs succinctly, fact see Theorem 2
superfluous, essentially uses fact set atoms agents finite.
2.3 Direct Semantics
introduce first two semantics DCL - PC. call semantics direct
semantics directly based intuitive model introduced earlier: every agent
unique control set propositional variables, every variable controlled
agent.
Given fixed, finite non-empty set agents, fixed, finite non-empty set P
propositional variables, say allocation P indexed tuple = hP 1 , . . . , Pn i,
442

fiR EASONING BOUT RANSFER C ONTROL

DCL - PC
DCL

formulas:
::= >
| p
| DCL
| DCL DCL
| C DCL
| h iDCL

Transfer programs:
::= ;p j
| ;
|
|
| DCL?

/* truth constant */
/* propositional variables */
/* negation */
/* disjunction */
/* contingent cooperative ability */
/* existential dynamic operator */

/* gives p j */
/* sequential composition */
/* non-deterministic choice */
/* iteration */
/* test */

Figure 3: Syntax DCL - PC: p P propositional variable, C set agents, i, j
agents.


indexed element Pi A, P1 , . . . , Pn forms partition P
(i.e., P = iA Pi Pi Pj = 6= j A). intended interpretation allocation
= hP1 , . . . , Pn Pi P set propositional variables agent control.
is, agent freedom allocate whatever Boolean values sees fit members P .
course, could defined allocation function : P A, (p) denotes
agent controlling propositional variable p; seems particular reason preferring one
representation rather other, consistency historical record, adopt
partition representation, used van der Hoek Wooldridge (2005b).
Now, say model DCL - PC structure:
= hA, P, 0 ,
where:
= {1, . . . , n} finite, non-empty set agents;
P = {p, q, . . .} finite, non-empty set propositional variables;
0 = hP1 , . . . , Pn initial allocation P A, intended interpretation P
subset P representing variables control agent A; finally,
: P {tt, ff} propositional valuation function, determines initial truth value
every propositional variable.
additional notation convenient follows. coalition C subset A, i.e., C A.
C A, denote complement C, (i.e., \ C) C. write P C
443

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

Pi . two valuations 0 , set propositional variables P, write = 0
(mod ) 0 differ propositional variables , say
0 modulo . sometimes understand model consist frame
F = hA, P, 0 together propositional valuation function . Given model = hA, P, 0 ,
coalition C M, C-valuation function:


iC

C : PC {tt, ff}.
Thus C-valuation propositional valuation function assigns truth values propositional variables controlled members coalition C. = hA, P, 0 , 0 =
hP1 . . . , Pn model, C coalition M, C C-valuation, C mean
model hA, P, 0 , 0 i, 0 valuation function defined follows
(
C (p) p PC
0
(p) =

(p)
otherwise
elements model M. Thus C denotes model identical
except values assigned valuation function propositional variables controlled
members C determined C .
define size model = hA, P, 0 , |A| + |P|; denote size
size(M).
2.4 Transfer Program Relations
give modal semantics dynamic logic constructs DCL - PC, must define, every
transfer program binary relation R models (M1 , M2 ) R iff M2 model
may result one possible execution program 1 . start defining relation
Ri;p j , atomic transfer programs form ;p j, i.e., agent gives control propositional
variable p agent j. Let = hA, P, 0 , M0 = hA0 , P0 , 00 , 0 two models 0 =
hP1 , . . . , Pn 00 = hP01 , . . . , P0n i.
(M, M0 ) Ri;p j
iff
1. p Pi (agent controls p begin with)
2. case = j:
(a) = M0 (agent gives p herself, change model)
3. case 6= j:
(a) P0i = Pi \ {p} (agent longer controls p afterwards);
(b) P0j = Pj {p} (agent j controls p afterwards);
(c) components M0 M.
444

fiR EASONING BOUT RANSFER C ONTROL

order define |=d , means true direct semantics, need
able determine interpretation arbitrary program is, M; define below.
Notice executing atomic transfer program effect valuation function model.
Transfer programs affect distribution propositional variables agents.
remaining constructs transfer programs, define program relations inductively,
terms relations atomic transfer programs, defined above. Let composition
relations R1 R2 denoted R1 R2 , reflexive transitive closure (ancestral) relation
R R . accessibility relations complex programs defined follows (Harel et al.,
2000):
R1 ;2 =
R 1 R 2
R1 2 =
R 1 R 2
(R )
R =
R? =
{(M, M) | |=d }.
Notice last definitions refers relation |=d , course yet
defined. aim next section define relation. emphasise that, although
relations R? |=d mutually refer one-another, relations fact well-defined (as
conventional dynamic logic).
2.5 Truth Conditions
interpret formulas DCL - PC respect models, introduced above. Given model
= hA, P, 0 , formula , write |=d mean satisfied (or, equivalently,
true) M, direct semantics. rules defining satisfaction relation |=
follows:
|=d >
|=d p iff (p) = tt

(where p P)

|=d iff 6|=d
|=d iff |=d |=d
|=d

C iff exists C-valuation C C |=d

|=d h iff exists model M0 (M, M0 ) R M0 |=d .
say formula objective contains modal constructs (i.e., operators C h i). Thus
objective formulae formulae classical propositional logic.
assume conventional definitions satisfiability validity: DCL - PC formula
-satisfiable iff exists DCL - PC model |= , -valid iff every
DCL - PC model |=d . write |=d indicate -valid. valid formula
also called tautology. say feasible satisfiable valid. Finally, set
formulas formula , define |=d M( |=d |=d .
Let us define box 2 dual cooperative ability modality as:

2C =
C
445

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

[] dual transfer modality hi as:
[ ] =
h i.
C coalition formula
choose either true false:

DCL - PC ,

write controls(C, ) mean C

controls(C, ) =
C C

(1)

using controls(, ) construct, capture distribution propositional variables
among agents model.
Lemma 1 Let = hA, P, 0 , model
p P propositional variable M.

DCL - PC ,

agent, C set agents,

1. (van der Hoek & Wooldridge, 2005b) |=d controls(i, p) iff p Pi ;
2. |=d controls(C, p) iff p PC .
Remark 1 characterize formulas control coalition C? have:
feasible objective, |=d

^


controls(C, p) controls(C, )

(2)

pP()

Observe property (2) true arbitrary DCL - PC formulas. see this, take example
formula hi ;p ji>, matter whether define P(hi ;p ji>) {p} . |=d
controls(i, hi ;p ji>): independent owning p, exactly one two formulas hi ; p ji>
hi ;p ji> true. is, p Pi iff |=d hi ;p ji>.
Also note -direction right hand side (2) valid objective : suppose
= hA, P, 0 , (q) = tt, p Pi , q
/ Pi . Then, |=d controls(i, pq)
(controls(i, p) controls(i, q)): q happens true M, controls conjunction
p q, conjuncts.
2.6 Kripke Semantics
Although direct semantics naturally captures notions propositional control transfer
control, purposes establishing completeness particular, relating main
stream modal logic convenient formulate semantics DCL - PC using conventional Kripke structures (Chellas, 1980; Blackburn, de Rijke, & Venema, 2001). idea that,
given (fixed, finite, non-empty) set agents (fixed, finite, non-empty) set P propositional variables, possible world every possible allocation variables
P agents every possible propositional valuation function P.
worlds, basically two orthogonal accessibility relations (cf. Figure 4): horizontal
vertical one. First all, horizontal relation Ri agent two worlds u
v agent able, given valuation u u, turn valuation v described v,
choosing appropriate values variables. Formally, (u, v) R iff u = v (mod Pi ).
is, Ri equivalence relation. follows, drop symbolic distinction
worlds valuations, i.e., use denoting world valuation interchangeably. Notice
446

fiR EASONING BOUT RANSFER C ONTROL

= hP1 , . . . , Pi , Pj , Pk , . . . , Pn



p, q, r
Ri
p, q, r

v
u

Rj

p, q, r
w

;p j
M0

0 = hP1 , . . . , Pi \ {p}, Pj {p}, Pk , . . . , Pn
p, q, r
Rj

p, q, r

v
u

Rj

p, q, r
w

j ;q k
M00

00 = hP1 , . . . , P0i , P0j \ {q}, Pk {q}, . . . , Pn
p, q, r
Rj

p, q, r

v
u

Rk

p, q, r
w

Figure 4: Kripke models DCL - PC.

horizontal relation affect allocation : remains unchanged. Let us therefore
define Kripke models = h, RiA , i, set valuations P.
important realize sets agents P variables fixed, allocations
variables agents may vary. denote set Kripke models K(A, P). call
pair (M, ) pointed Kripke model, sometimes omit brackets pair.
Secondly, vertical accessibility relation pointed models (M, ) (M 0 , 0 ),
= h, RiA , i, M0 = h, RiA , 0 K(A, P), indicate change allocation
0 . Since change allocation affect current world, pairs
= 0 . Slightly abusing notation, define (M, )(i ;p j)(M0 , 0 ) exactly = 0
p Pi , either = j = M0 , else P0i = Pi \ {p} P0j = Pj {p},
sets Ph remain same.
447

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

truth relation |=K interpreting formulas Kripke structures holds pairs
form (M, ) formulas . definition follows (we omit Boolean cases cases
complex transfer programs):
M, |=K C iff exists valuation 0 (, 0 ) Ri C,
M, 0 |=K
M, |=K hi ;p ji iff exists Kripke model M0 (M, )(i ;p j)(M0 , )
M0 , |=K
set formulas formula , define |=K (M, )( (M, ) |=K
M, |=K ). Figure 4 illustrates Kripke semantics. Note sets P 0i P0j Kripke
model M0 P0i = Pi \{p} P0j = Pj {p}. Note clause C , two pointed
models M, M, 0 except atoms PC . special case
two models similar upto set atoms (French, 2006; Ghilardi & Zawadowski, 2000).
Remark 2 Note fact, Kripke semantics, formulas interpreted model
valuation only, context models (which reached atomic program ; p j).
finitely many them, one . Call collection models . fact,
structure respect formulas interpreted. sense, one Kripke
model language (w.r.t. A, P): . prove completeness respect unique
two-dimensional model, Section 3.
following lemma easily established induction :
Lemma 2 fixed sets agents propositional variables P, direct semantics
Kripke semantics equivalent, i.e., , K(A, P) = h, R iA , i,
model = hA, P, , i:
|=d iff M, |=K .
usual, define |=K : M, |=K , |=K : |=K .

3. Complete Axiomatization
sound complete axiomatization DCL - PC presented Figure 5. ease exposition, divide axiomatization five categories, follows. Propositional
Component Rules Inference straightforward, Dynamic Component immediate adaptation Propositional Dynamic Logic (Harel et al., 2000). Control Axioms
inherited CL - PC (van der Hoek & Wooldridge, 2005b). (The occurrence `(p) refers
literal atomic proposition p: either p p, obvious meaning `(p).) Note
allocation specifies every propositional variable assigned exactly one agent (i.e.,
allocation), contrast, fixed allocation assumed CL - PC, one
could explicitly state controls(i, p), every p Pi (van der Hoek & Wooldridge, 2005b).
Transfer & Control Axioms, atomic permanence states program changes
valuation. this, one easily extends arbitrary objective formulas (obtaining objective
permanence, see Theorem 1 below). axiom persistence 1 (control) says control
p affected move another valuation, axiom persistence 2 (control) specifies
448

fiR EASONING BOUT RANSFER C ONTROL

Propositional Component
Prop

Dynamic Component
K( )
union( )
comp( )
test( )
mix( )
ind( )
Control Axioms
K(i)
T(i)
B(i)
empty
control(i)
allocation
effect(i)
Comp-

objective tautology


[ ]( ) [ ] []
[ 0 ] [ ] [ 0 ]
[ ; 0 ] [ ][ 0 ]
[?] (
)
[ ][ ] [ ]
[ ]( [ ]) [ ]


2i ( ) 2i 2i
2i
2
2


controls(i, p) p p

V

pP controls(1, p)5 5controls(n, p)
= {1, . . . , n}


p 6 P(),
`(p) controls(i, p) `(p)
objective
2C1 2C2 2C1 C2

Transfer & Control Axioms

atomic permanence(;) hi ;p ji> [i ;p j]q q
persistence1 (control)
controls(i, p) 2j controls(i, p)
persistence2 (control)
controls(i, p) [j ;q h]controls(i, p)
precondition(transfer)
hi ;p ji> controls(i, p)
transfer
controls(i, p) hi ;p jicontrols(j, p)
func
controls(i, p) hi ;p ji [i ;p j]

Rules Inference
Modus Ponens
Necessitation

` , ` ( ) `
` ` 2

6= j p 6= q

2 = [ ], 2i

Figure 5: Axiomatic System DCL - PC.
remains control p, even transfer program executed: either variable
passed program p, delegating agent i. axiom precondition(transfer)
expresses agents give variables away possess, and, finally func says
transition relation associated atomic transfer program functional: one resulting
world emerges.
following theorem lists properties DCL - PC, controls(C, p) defined equation (1) above.
Theorem 1
1. axioms K(i), T(i), B(i), effect(i) coalitional counterparts K(C), T(C), B(C),
effect(C) derivable coalition C.
449

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

at-least(control) :
`(p) controls(i, p) `(p)
at-most(control) :

`(p) `(p) 2j `(p)
(i 6= j)
non-effect(i) :

`(p) controls(i, p) 2i `(p)
persistence(non-control) :
controls(i, p) 2j controls(i, p)
objective permanence(;) :
hi ;p ji> [i ;p j] objective
objective permanence
:
h i> [ ]
objective
inverse :

controls(i, p) [i ;p j; j ;p i]
reverse :
[i ;p j][k ;q h] [k ;q h][i ;p j]
(j 6= k h 6= i) p 6= q

Figure 6: Theorems DCL - PC.
2. Moreover, know (van der Hoek & Wooldridge, 2005b) axioms K(i), T(i), B(i),
effect(i) coalitional counterparts K(C), T(C), B(C), effect(C)
derivable coalition C.
3. ` controls(C, p)

W

iC

controls(i, p).

4. ` controls(C, p) 2j controls(C, p), i.e., property persistence1 (control) also derivable replace agent arbitrary coalition C.

Proof: See Appendix A.

QED

Consider language without dynamic transfer operators, propositional
logic cooperation modalities C . Models M, M0 K(A, P). program-free
language, every formula equivalent one without occurrences coalition operators van der
Hoek Wooldridge (2005b). instance, suppose P = {p, q}. formula (p r)
equivalent (p r) (p r) (we read current value variable r outside control).
establish similar result language including programs. world (M, )
completely characterized know variables true it, allocation
variables agents is. case, truth objective formulas, formulas involving abilities
transfer programs completely determined.
Lemma 3 Let arbitrary DCL - PC formula conjunction assertions form
controls(j, p) controls(j, p). Then, DCL - PC, derive
` C ( ) ( C ).
450

fiR EASONING BOUT RANSFER C ONTROL

Proof: Since Comp-, C = {a1 , a2 , . . . , aC }, C a1 a2 aC ,
sufficient prove claim individual agent i. Moreover, move conjuncts
one one, know
` ( controls(j, p)) (controls(j, p) ),
controls(j, p) either controls(j, p) controls(j, p). reasoning nonnegated case (the one similar): ( controls(j, p)) equivalent

controls(j, p) ( controls(j, p))
controls(j, p) ( controls(j, p)) .

However, using theorem persistence(non-control) Figure 6 (which
derive below),

second disjunct controls(j, p) ( controls(j, p)) . concludes
proof.
persistence(non-control), right-to-left direction follows immediately T(j).
direction, assume controls(i, p). allocation derive
controls(1, p)5 5controls(i 1, p)5controls(i + 1, p)5 5controls(n, p),
W
this, persistence1 (control), get k6=i 2j controls(k, p). every k 6= i,
controls(k, p) controls(i, p), follows allocation. Hence, using Necessitation,
2j (controls(k, p) controls(i, p)).WFrom Axiom K(j), follows 2 j controls(k, p)
2j controls(i, p). Combining k6=i 2j controls(k, p), obtain desired conclusion
2j controls(i, p).
QED
Soundness axiom schemes Figure 5 readily checked. proceed prove
axiomatic system DCL - PC Figure 5 complete. First, introduce notation.
Definition 1 Given set propositional variables P, valuation description conjunction
literals (p p) every propositional variable P occurs one literal.
Notice that, propositional variable p P, holds either p, p.
denote set valuation descriptions P . Notice that, valuation ,

V
= V{p | p P (p) = tt}
{p | p P (p) = ff}.
Definition 2 Given set propositional variables P set agents A, allocation description conjunction formulas form controls(i, p) every p P,
exactly one controls(i, p) appears .

denote set allocation descriptions . Notice allocations conjunctions
correspond other: allocation = hP1 , . . . , Pn variables P agents
A,
^
controls(i, p).
=
iA,pPi

451

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

Therefore, refer formulas allocation descriptions. Given two allocation descriptions
, 0 , say (i ;p j) 0 following three conditions satisfied: controls(i, p),
0 controls(j, p), 0 agree control expressions.
Definition 3 Let, allocation description , set valuation descriptions. Then,
formula form
_ _


(3)


called proposition description.

later, Theorem 2, see every formula equivalent proposition description.
intuition truth requires, every allocation description , possible
truth values atoms fixed. give example, suppose two agents j,
three atoms p, q r. Consider formula = hi ;p ji(q j (p r)). order find
equivalent proposition description, must, every make proper choices .
implies controls(i, p) allocation would make false (since cannot
W transfer control p),
, choose empty set, ensuring ( ) equivalent
. implies controls(i, p),
W basically two cases: either also implies controls(j, r),
constraint equivalent q, else implies controls(j, r), case
W
equivalent q r.
Let us, two valuation descriptions 0 , coalition C, allocation description
, write 0 (mod C, ) two conjunctions literals 0 differ variables control C, determined . instance, C = {1, 2} =
controls(1, p1 )controls(2, p2 )controls(3, p3 ), (p1 p2 p3 ) (p1 p2 p3 ) (mod C, ).
first collect facts valuation descriptions, allocation descriptions, proposition
descriptions.
Recall that, set DCL - PC formulas, 5 used shorthand
W
V
1 6=2 (1 2 ).

Lemma 4 Given set valuation descriptions , set allocation descriptions ,
following six items satisfied:
1. ` 5
2. ` 5
W
3. ` ( )

4. : ` (C

W

0 (mod C,)

0 ).

5. ` ( hi ;p ji 0 ) (( ) hi ;p ji( 0 )).
6. Let n number agents, k number propositional variables.
nk
N(n, k) = 22 provably non-equivalent proposition descriptions.
Proof:
1. follows Prop definition : mutually exclusive cannot
false.
452

fiR EASONING BOUT RANSFER C ONTROL

2. Item (2) easily seen equivalent allocation axiom: allocation implies
and, every allocation description , implies allocation.

W

,

3. Item (3) immediate Item (2) axiom Prop. particular, using Prop derive
` B C (C A) (C B).
4. Assume . right-to-left direction, also assume 0 , valuation description 0
0 (mod C, ). means 0 differ variables p1 , . . . , pm
controls(C, pj ) implied (for j = 1 . . . m). Note 0 objective
formula. write 0 `(pi ) (for = 1 . . . m), 0 literal
`(pi ) left out. Apparently, 1 `(p1 ) controls(C, p1 ). Since 1 objective,
apply effect(C) conclude C (1 `(p1 )). Using Lemma 3, derive C (1 `(p1 )).
rewrite 1 2 `(p2 ), obtain C (2 `(p2 ) `(p1 ) ). use
effect(C) Lemma 3 get C C (2 `(p2 ) `(p1 ) ). Comp-,
C (2 `(p2 ) `(p1 ) ). repeat process get
C (j `(pj ) `(p2 ) `(p1 ) ). But, definition 0 , implies C .
WeWshow direction left right contrapositive: fix assume
0 (mod C,) 0 . show C . Let Q(, ) = {`(p) |` `(p) 0
controls(C, p)} set literals variables control
agents C allocation . Notice valuations 0 (mod C, )
W agree on0
literals

Q(,
).


use
propositional
reasoning

derive


0 (mod C,)
W
V
`(p)Q(,) `(p). Using T(C) K(C), conclude `(p)Q(,) C `(p).
, follows, literal `(p) Q(, ), controls(C,
`(p)), which, equaW
tion (1), equals C `(p)



`(p).

then,


derive
C
`(p)Q(,) C `(p). Using
V
K(C), obtain C `(p)Q(,) `(p). Hence, C .

5. First all, ( hi ;p ji 0 , follows controls(i, p). Hence, given ,
formulas hi ;p ji [i ;p j] equivalent func. particular, hi ;p ji>.
Let us first show literals `(q) `(q) hi ;p ji`(q). negative literals q, q
hi ;p jiq equals q [i ;p j]q, follows atomic permanence(;). positive
literals q, use func obtain q [i ;p j]q, holds atomic permanence(;).
Now, given , q hi ;p jiq q hi ;p jiq. Then, valuation
description , also hi ;p ji. remains show hi ;p ji 0 .
left right, note controls(i, p) hi ;p jicontrols(j, p) transfer.
controls expression implied , follows persistence 2 (control)
func. Finally, consider direction right left: hi ;p ji 0 . show
that, first, hi ;p jicontrols(j, p) controls(i, p), and, second, hi ;p jicontrols(h, q)
controls(h, q), controls(h, q) (with q 6= p) implied 0 . first part follows immediately precondition(transfer). second part, let h agent q 6= p variable 0 implies
controls(h, q). Suppose hi ;p jicontrols(h, q). allocation,
V
controls(k,
q). contrapositive persistence 2 (control)

hi
;
ji
p
k6=h
V
yields k6=h controls(k, q). follows allocation controls(h, q).
6. Item 6 follows straightforward counting argument proposition description formulas.
number proposition descriptions depends cardinalities sets .
Given number n = |A| agents, number k = |P| propositional variables,
453

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

easy see 2k valuation descriptions , nk allocation descriptions
(i.e., number ways distribute k variables n agents). Observe
proposition description formula obtained assigning set valuation descriptions
k
k
allocation description . Hence, 22 n proposition descriptions. Since
k
k
nk
nk
22 n 22 , obtain N(n, k) = 22 upper bound number different
proposition description formulas.
QED

present main result section. first formulate it, reflect briefly it,
give proof.
Theorem 2 every DCL - PC formula , sets () valuation descriptions, one
,
_ _

`
() .


According Theorem 2, get rid hi ;p ji. show derivation system
good enough establish that, main task proof. let us first convince
semantically normal form makes sense. Remember every model comes
allocation . formula like hi ;p ji true M, , true model looks
like M, control p transferred j. means formula 0
must already true M, , takes role j far p concerned. instance,
hi ;p ji(q j (p r)) true M, , q j r controls(i, p) true current allocation
. formula reference layers anymore. precisely, hi ; p ji(q j (p r))
equivalent

q ((p r) (p r)) controls(i, p) controls(j, r)


q ((p r) (p r) (p r) (p r)) controls(i, p) controls(j, r) .
Proof: proof induction norm |||| defined DCL - PC formulas follows:
||>||
||||
||1 2 ||
||C ||
||[i ;p j]||
||[?]||
||[1 2 ]||
||[1 ; 2 ]||
||[ ]||

=
=
=
=
=
=
=
=
=

||p|| = 0, p P
1 + ||||
1 + ||1 || + ||2 ||
1 + ||||, C
1 + ||||
1 + || ||
1 + ||[1 ] [2 ]||
1 + ||[
V1 ][2 ]||
1 + || i=0..N [ ]i ||

N = N(n, k) number defined Lemma 4, Item (6).
induction base proof theorem two cases:

W
= >. take (>) = ,
every


.

Item
(1)

Lemma
4,





W
W
W
(>) equivalent , turn
objective tautology. Hence,
equivalent > (2) Lemma 4.
454

fiR EASONING BOUT RANSFER C ONTROL

=
(p) = { |` p}, every . Clearly, p
W p, p P. Take W
Wis equivalent
(p), i.e., ` p (p). Now, using (3) Lemma 4, get ` p (p ).
Finally,W
replacing
W second occurrence p derived equivalent formula, get
(p) . direction follows simple propositional reasoning.
` p

Consider induction step.

= . set () = \ (), every . works,
following:


_



^



_

(



_



(



_





()

_

_

_



(4)

())
())

()







(5)
(6)
(7)

steps purely propositional,
(5) (6)
V except equivalence W
explain now. Abbreviate (5) W (A ), (6) (A ). Note
Lemma
V 4, Item (2), derive . words, oneW must true, say . note
(A )W implies (A ) hence also (A ), abbreviation
(6). Conversely, (A ) holds, know Lemma 4, item (2) 4 ,
i.e., exactly one must hold, say . formula (A ) true,
allocation description formula true. , also
(A ). Moreover, anyV 6= , , hence (A ). (A )
holds , hence (A ), shorthand (5).

= 1 2 . set (1 2 ) = (1 ) (2 ), every . following
equivalences, need propositional reasoning:
1 2

_



_



_



_
(

_
(1 )



_

_

(1 )

_

_

(2 )

(2 ))

(1 2 )







= C . every , set
(C ) = { | 0 (mod C, ) 0 ()}
455

(8)
(9)
(10)

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

derive following equivalences:

C

C



_



_



C



_



C



_





_

_

_

()
()

_





(12)



(14)



()

(C )

(11)

(13)

equivalence (11) holds induction hypothesis. Using K(i), equivalent
(12) (for diamond C ( ) (C C )). equivalence latter
(13) Lemma 3.
W
W
remains show equivalence (13) (14).


()
=

C

C
() .
W
K(C) Comp-, formula equivalent () C . Using Item (4)
W
W
0
Lemma
4,

see



equivalent


()
0 | 0 (mod C,) . equals

W
(C ) definition (C ).

= [i ;p j]. define ([i ;p j]) follows: every ,


0 () controls(i, p)
([i ;p j]) =
(i ;p j) 0



otherwise

see yields formula right form equivalent [i ; p j], let us first
partition + (i, p) = { | ` controls(i, p)} (i, p) = { | `
controls(i, p)}. consider following derivable equivalences:
_ _
[i ;p j] [i ;p j]
( 0 () 0 )
(15)
0



controls(i, p)
_ _
( 0 () 0 ))

(controls(i, p) hi ;p ji

(16)

0



_

(

(i,p)

_

_

)

hi ;p ji

_



_

(17)

0

+ (i,p)



_ _

( 0 () 0 )

([i ;p j])



(18)

equation (15) holds induction hypothesis. equivalent (16) propositional reasoning, changing [i ;p j] hi ;p ji allowed func. equivalence (16) (17) follows definition + (i, p) (i, p) fact
456

fiR EASONING BOUT RANSFER C ONTROL

W
>. order prove equivalence (17) W
(18), W
sufficient
+ (i, p), formula hi ; ji
show that, fixed



(
0 ()
p
0
W
0
0 ) equivalent 0 (), (i ;p j) .
W Lemma 4,
W follows
ItemW(5), follows.W First all, write hi ;p ji 0 ( 0 () 0 )
0 hi ;p ji( 0 () 0 ). mentioned lemma,
W know exactly 0
0 giving hi ; ji( 0 () 0 ). rewrite
need: 0
(i
;
j)
p
p

W
hi ;p ji {W 0 | 0 ()}, push diamond hi ;p ji inside
0
0
disjunction
W get {hi ;p ji( ) | ()}.
W Lemma 4, Item (5)
yields 0 (). direction isW
similar: 0 () (i ;p j) 0 , then,
Lemma 4, Item (5), get hi ;p ji( 0 () 0 ), result follows.

= [ 0 ?]. axiom test( ), [ 0 ?] equivalent 0 , equivalent
formula right form induction hypothesis.

= [1 ; 2 ]. axiom comp( ), [1 ; 2 ] equivalent [1 ][2 ], equivalent
formula right form induction hypothesis.
= [1 2 ]. axiom union( ), [1 2 ] equivalent [1 ] [2 ],
equivalent formula right form induction hypothesis.
= [ ]. Recall N = N(n, k) given Lemma 4, Item (6). Using axiom mix( )
K( ), know [ ] equivalent [ ][ ]. N times, obtain
[ ] [ ]2 [ ]N [ ]N [ ].
induction hypothesis, know except last conjunct equivalent
normal form. since N different forms, V
conjunct [ ] N must
equivalent one earlier conjuncts [ ]i (i < N). Define = i=0..N [ ]i . claim
[ ].

(19)

induction hypothesis, equivalent formula right form. direction (19) obvious since first part unraveling [ ][ ][ ] . . .
[ ] using axioms mix( ) K( ). show direction sufficient derive
[ ], because, fact one conjuncts , immediately
gives [ ]. show derivability [ ], use ind( ). First all,
show [ ]. see this, note [ ] [ ][ ]0 [ ][ ]1 [ ][ ]N .
induction hypothesis, conjunct [ ]i (i N) normal form, say, Bi .
obtain sequence B0 , B1 , . . . , BN N + 1 formulas normal form. Since
N provably non-equivalent formulas Lemma 4, Item (6), know B j
equals previous Ba < j N. Let Bj first repetition sequence.
Notice [ ]j Bj = Ba [ ]a , thus [ ]k [ ]j [ ]k [ ]a ,
k 0. then, follows last conjunct [ ][ ]N [ ] equivalent [ ]k [ ]a
(with k = N j), already appears . derived [ ], apply
Necessitation [ ], obtain ` [ ]( [ ]). applying ind( ), get [ ].
QED

know Lemma 4 finitely many different normal forms: since every
formula normal form, finitely many non-equivalent formulas. also
457

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

know proof Theorem 2 that, [ ], consider finite number
conjuncts [ ]i unraveling.
Corollary 1 finitely many pairwise non-equivalent formulas
given number n agents, k propositional variables, have:
W
1. 6= j 6` (i j ) ` iM ,
V
2. ` [ ] [ ]i ,

DCL - PC .

fact,

nk

= 2nk N = 22 (as defined Lemma 4, Item (6)).

Completeness derivation system inference relation ` respect semantics means
every semantically valid formula also provable: |= ` . order prove completeness,
often contrapositive shown: 6` 6|= . is, every consistent formula
model. popular technique modal logic construct dedicated model canonical
model, (cf. e.g., Blackburn et al., 2001) consistent formula . canonical model
bridge syntax semantics: consists maximal consistent sets (as worlds),
constructed way membership world truth corresponding world
coincide.
DCL - PC works straightforwardly follows. Fix finite sets P, take
consistent formula . Build W
maximal
W consistent set around it. Let normal form ,
guaranteed

Theorem
2,

(
). Since maximal consistent, ,

W
( ) . Again, maximal consistency , must contain, exactly one
, formula . uniquely determines valuation = , whereas determines
allocation = . words, uniquely determines pointed Kripke model (M, )
= h, RiA , i. worlds determined P, RiA (the horizontal layer
M, terms Figure 4) determined Control Axioms. availability right
models M0 = h, RiA , 0 (the vertical layer Figure 4) determined Delegation
Control Axioms. result, directly interpret subformulas form hi ; p ji
C proper way, (M, ).
argument easily extends strong completeness, states sets formulas
formulas , |=K ` . see this, suppose 6` , i.e., {}
consistent. Since finitely many pairwise non-equivalent formulas, must
formula equivalent {}. previous argument, find pointed model
(M, ) (M, ) |=K . model also (M, ) |=K {}. Hence,
every model one , i.e., 6|=K . Strong completeness also follows alternative
way resaoning: note language compact: i.e., |= K , finite set
0 0 |=K . seen follows: know Corollary 1
different formulas provably equivalent, depends number
agents number atoms. then, soundness, also semantically
different formulas . Putting formulas 0 gives desired result. Strong completeness
follows (weak) completeness compactness.
all, obtain following (also using Lemma 2).
Theorem 3 language DCL - PC compact. Moreover, axiomatic system DCL - PC sound
complete respect Kripke direct semantics. also strongly complete.
458

fiR EASONING BOUT RANSFER C ONTROL

1. function program-eval(, = hA, P, , i, d) returns fail model A, P
2.
= ?
3.
return DCL - PC-eval(, M)
4.
fail otherwise
5.
elsif = (i ;p j)
6.
return hA, P, 0 , p Pi
7.
0 = = j
8.
otherwise = hP1 , . . . , Pn 0 = hP01 , . . . , P0n
9.
P0i = Pi \ {p},
10.
P0j = Pj {p},
11.
P0m = Pm n, 6= i, j
12.
fail otherwise
13. elsif = (1 ; 2 )
14.
return program-eval(2 , program-eval(1 , M, d), d)
15. elsif = (1 2 ) non-deterministically choose either
16.
return program-eval(1 , M, d)
17.
program-eval(2 , M, d)
18. elsif = 0
19.
return fail = 0
20.
otherwise (if > 0) non-deterministically choose either
21.
return
22.
program-eval(( 0 ; 0 ), M, 1)
23. end-function

Figure 7: algorithm deciding (M, M0 ) R .
is, sets

DCL - PC

formulas every DCL - PC formula ,
` iff |=K iff |=d

4. Complexity
model checking satisfiability problems CL - PC PSPACE-complete (van der Hoek &
Wooldridge, 2005b), since DCL - PC subsumes CL - PC, implies PSPACE-hardness lower
bound corresponding problems DCL - PC. obvious question whether additional dynamic constructs DCL - PC lead complex decision problem particular,
whether DCL - PC satisfiability matches EXPTIME-completeness PDL satisfiability (Harel et al.,
2000). section, show model checking satisfiability problems fact
worse CL - PC: PSPACE-complete. Notice EXPTIME-completeness usually
regarded characteristic complexity logics modal operator another
operator representing transitive closure operator (Blackburn et al., 2001).
Note consider model checking problem section, consider problem respect direct models, Kripke models. course, respect satisfiability,
makes difference: formula satisfiable respect direct models iff satisfiable w.r.t.
Kripke models.
proving PSPACE-completeness DCL - PC model checking, consider auxiliary notions first. program sequence transfer program composed atomic transfer programs,
tests, sequential composition only. program admits program sequence un459

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

folded recursively applying following rules: atomic transfer program (i ; p j),
test ?, transfer programs (m 0):
(i ;p j)
?
1 ; 2
1 2


(i ;p j)
?
1 ; 2
1 2
1 ; 2 ; . . . ; n , n 0,
= , n

following two lemmas establish membership accessibility relation R transfer program decided polynomial space.
Lemma 5 transfer programs 0 (direct) models M0 , (M, M0 ) R 0 implies
0 admits program sequence length exponential length 0
0 3
(M, M0 ) R . fact, length limited 2| | .
Proof: Let 0 , M, M0 lemma. proof induction structure
0 . interesting case 0 = ; cases straightforward. Suppose
(M, M0 ) R . Since R = (R ) , sequence M0 , . . . , Mn , n > 0, models
M0 = M, Mn = M0 , (Mi1 , Mi ) R , 1 n. transitivity
R , assume sequence M0 , . . . , Mn Mi 6= Mj , i, j
1 < j n, i.e., sequence models contains loops. induction hypothesis yields
3
admits program sequences 1 , . . . , n length 2| | (Mi1 , Mi ) Ri
1 n. = 1 ; 2 ; . . . ; n program sequence admitted
(M, M0 ) R . following, shown required length. Note
models reachable = hA, P, , via R differ allocation propositional
variables P agents A. precisely, differ allocation propositional
variables agents occur . Thus `m reachable models, `
number propositional variables occurring number agents occurring . Notice
n exceed `m ; otherwise sequence M0 , . . . , Mn contains loops contradicting
2
assumption. Together fact `m | || | 2| | , upper bound length
given follows:
|| = |1 ; 2 ; . . . ; n |

=



n sup{|i | : 1 n} + n
2
3
2
2| | 2| | + 2| |
2
3
2
2| | +| | + 2| |
3
2(| |+1)
3
2| | .
QED

Lemma 6 programs (direct) models M0 , membership problem (M, M0 )
R decided PSPACE.
Proof: Let program let M, M0 two (direct) models. Consider following algorithm
decides (M, M0 ) R using function program-eval( ) Figure 7:
460

fiR EASONING BOUT RANSFER C ONTROL

3

1. Set = 2| | .
2. program-eval(, M, d) = M0 , return (M, M0 ) R , otherwise.
see algorithm correct, shown program-eval(, M, d) = 0 iff (M, M0 )
R . direction left right, readily checked program-eval(, M, d) = 0
implies existence program sequence admitted length | |
(M, M0 ) R 0 . Clearly, R R thus (M, M0 ) R . Consider direction right
left. (M, M0 ) R , follows Lemma 5 program sequence admitted
3
length 2| | (M, M0 ) R . Step 1 ensures value
3
2| | | | d. obvious construction algorithm non-deterministic
choices lines 15 20 Figure 7 yield program-eval(, M, d) = 0 . Notice
algorithm terminates since recursive calls lines 14, 16, 17 applied strict
subprograms recursive call Line 22 followed one Line 14
parameter limits recursion depth.
algorithm run polynomial space. see this, notice function
DCL - PC-eval( ), called Line 3, computed polynomial space
parameter encoded binary. Moreover, stack algorithm computing function
program-eval( ) limited size polynomial length . Note stack
needs store currently evaluated program programs backtracking points,
introduced nested function call Line 14. since nested function call
applied strict subprograms, linearly many backtracking points needed time.
Although algorithm non-deterministic, follows well-known fact NPSPACE equals
PSPACE (Savitch, 1970) runs PSPACE .
QED
Using previous two lemmas, prove following.
Theorem 4 model checking problem DCL - PC (w.r.t. direct models) PSPACE-complete.
Proof: Given DCL - PC subsumes PSPACE-hard logic CL - PC, need prove upper
bound. Consider function DCL - PC-eval( ) Figure 8. Soundness obvious construction.
First note algorithm strictly analytic: recursion always sub-formula input.
algorithm PSPACE follows fact loops lines 1012 15-18
involve, first case simply binary counting variables P C , second simply
looping direct models P: need store models
checked, done polynomial space. Finally, Lemma 6 yields check
(M, M0 ) R Line 16 done polynomial space.
QED
Now, make use following result, proof identical equivalent result
proved van der Hoek Wooldridge (2005b).
Lemma 7 DCL - PC formula satisfiable, satisfied (direct) model
size(M) = |P()| + |Ag()| + 1.
prove following.
Theorem 5 satisfiability checking problem DCL - PC PSPACE-complete.
461

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

1. function DCL - PC-eval(, = hA, P, 0 , i) returns tt ff
2.
P
3.
return ()
4.
elsif =
5.
return DCL - PC-eval(, M)
6.
elsif = 1 2
7.
return DCL - PC-eval(1 , hA, P, 0 , i)
8.
DCL - PC-eval(2 , hA, P, 0 , i)
9.
elsif = C
10.
C-valuation C
11.
DCL - PC-eval(, hA, P, 0 , C ) return tt
12.
end-for
13.
return ff
14. elsif = h
15.
model M0 A, P
16.
(M, M0 ) R
17.
DCL - PC-eval(, M0 ) return tt
18.
end-for
19.
return ff
20. end-function

Figure 8: model checking algorithm DCL - PC.

Proof: Given formula , loop model containing P() Ag()
size(M) = |P()| + |Ag()| + 1, |=d return Yes. considered
models, return No. Theorem 4, check whether |= polynomial space.
QED
Notice PSPACE complexity checking satisfiability depends upon fact models
DCL - PC concise, hence loop polynomial space (we
need remember model considered).

5. Characterizing Control
One main concerns original study CL - PC (van der Hoek & Wooldridge, 2005b)
investigate logical characterization control: extent could characterize,
logic, states affairs agents could reliably control. Control distinguished ability
sense that, example, agent could said control tautology, even one might
prepared concede agent would ability bring tautology. starting
point study control (van der Hoek & Wooldridge, 2005b) controls(i, p) construct:
already seen, expression true iff variable p control
agent i. led analysis characterization types formulas agent could
said control. type control studied van der Hoek Wooldridge derives
ability agents choose values propositional variables control. Let us refer
type control, agent directly able exert influence state affairs
assigning values variables, first-order control. section, undertake similar
study control richer setting DCL - PC. Here, however, second type control,
462

fiR EASONING BOUT RANSFER C ONTROL

derives ability transfer control variables agents. Thus, example,
controls p, also power ensure instance controls(j, p), j agent different
i. control expressed transfer modality: hi ; p jicontrols(j, p). refer
type control second-order control. see types control indeed
rather orthogonal. instance, hi ;p jij (i give p j, achieve ) i,j (i
j cooperate, achieve ) logically incomparable. example, taking = hj ; p ii>
gives
|=d controls(i, p) (hi ;p ji i,j )
= hi ;p ji> assuming 6= j,
|=d controls(i, p) (hi ;p ji i,j ).
However, goal objective formula, relate atomic control transfer,
shortly see.
begin study, consider transfer program
[
[

;p j .
(20)
controls(i, p)?;
givei =

jA

pP

hgivei would express way give one propositional variables one
agents (possibly herself) way consequently holds. Thus, hgive means
distribute variables among agents way afterwards holds. Hence,
reasoning power, strongest achieve hgivei i,
expressing achieve either choosing appropriate value variables,
distributing variables appropriate way. Note hgivei imply
hgivei ii , hence hgivei ii holds seen achieve
own. come back program givei below.
program give generalized incorporate coalitions give away variables,
receive: let
[
[[

controls(i, p)?;
hi ;p ji .
(21)
giveC;D =

iC pP

jD{i}

program giveC;D lets arbitrary agent coalition C either give
variables p arbitrary member coalition D, nothing (i.e., give herself).
Now, objective formulas , following, dedicated agent C:

C hgiveC;{i} ii .
words: agents coalition C choose values variables ,
way give variables dedicated agent i, achieve . Note
general able eliminate occurrences s, since way
express first-order control, i.e., reason different valuation.
examples language without transfer, refer paper van der Hoek
Wooldridge (2005b), especially example Bach Stravinsky, (i.e., Example 2.4, van der
Hoek & Wooldridge, 2005b). looking two examples control dynamic setting, note
allows following inference, objective formula :
consistent |=d



(22)

inference says grand coalition achieve satisfiable objective formula.
463

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

Example 1 Suppose n agents: 1, . . . , n. controls flag r (i = 1 . . . n) indicate
desire control particular resource, modeled variable p. want p
true false every then, (which could taken care central agent executing
program making p false true alternatively), rather, want control p eventually. Let
+n denote addition modulo n, and, similarly, n subtraction modulo n. Let skip denote >?, i.e.,
test tautology. Consider following program:
grant-req(i) =




...


controls(i, p) skip else
ri+n 1
(i ;p +n 1) else
...
ri+n (n1)
(i ;p +n (n 1)) else skip

program grant-req(i) makes agent pass resource p whenever somebody else needs it, need checked order starting agent next + n
index. Note use variable p, i.e., making true false, encoded
program constructs. consider program
pass-on(i, j) =
grant-req(i); . . . ; grant-req(j n 1).
program pass-on(i, j) pass control variable p agent j, provided initially r j
set one agents sequence i, +n 1, . . . , j n 1 owns it. expressed
follows:
rj controls({i, +n 1, . . . , j n 1}, p) [pass-on(i, j)]controls(j, p).
have:
ri

hpass-on(i +n 1, i)icontrols(i, p)
[pass-on(i +n 1, i)]controls(i, p) .

is: agent flags request ri resource p, then, program pass-on(i +n 1, i)
executed, control p.
Notice previous example freely passes variable along chains agents, thereby
taking granted control variable fly, making true false will.
following example, control variable important, also truth side
conditions involving them.
Example 2 scenario three agents: two clients c 1 c2 , server s.
server always control one propositional variables p 1 p2 , particular wants
guarantee variables never true simultaneously. time, c 1 c2 want
ensure least one variables pi (i = 1, 2) true, variable pi belongs client
ci . describe invariant system formula Inv:
Inv =


_

controls(s, pi )

i=1,2

_

controls(ci , pi )

i=1,2

Consider following transfer program :
=


(controls(s, p1 )? ; ;p1 c1 ; c2 ;p2 s)

(controls(s, p2 )? ; ;p2 c2 ; c1 ;p1 s) .
464

fiR EASONING BOUT RANSFER C ONTROL

says arbitrary number times one variable pi passed server client
ci , another variable pj (i 6= j) client cj server.
Using Inv , describe whole scenario follows:
Inv []Inv

Inv [] (p1 p2 ) {c1 ,c2 } (p1 p2 )

general characterization types formulas agents coalitions could control
given van der Hoek Wooldridge (2005a), aim undertake study
DCL - PC . appear done local global level, also see
notion control inherited CL - PC, natural generalization context.
next corollary establishes result concerning characterization control. first item
says strict sub-coalition C 6= A, valid C controls something.
words, control coalition always feature specific model, particular, specific
allocation. According second item, grand coalition derivably,
W models, controls
W
exactly formulas property theirWequivalent form ( () )
every allocation description , formula () contingency,
W i.e., tautology
neither contradiction. propositional formula, () easy
see contingency sufficient necessary |= K controls(A, ).
W
hand, instance controls(i, p), () = controls(i, p) () equals
>. indeed, 6|=K controls(A, ). Contrast = p controls(i, p). follows easily
truth definition Kripke semantics defined Section 2.6, Theorem 2
Theorem 3.
Corollary 2 Let Kripke model, C coalition C C 6= A, let ranging
DCL - PC formulas. follows that:
1. , |=K controls(C, ),
W
W
2. |=K controls(A, ) iff formula ( () ), equivalent according
Theorem 2, allocation description , () = () = .
Proof:
1. order controls(C, ) valid Kripke semantics, true worlds
Kripke models. Take Kripke model = h, RiA , allocation
= hP1 , P2 , . . . , Pn Pj = , j C. M, |=K C
iff world 0 0 = (mod PC ), holds M, 0 |=K . But, since
PC = , 0 itself, cannot M, |=K C C . Hence,
M, 6|=K controls(C, ).
2. First
W left-to-right direction contraposition. Let equivalent forW prove
mula ( () ), Theorem 2. Suppose, allocation
description , () = . means, every Kripke model = h, RiA ,
every valuation , M, |=K . Consequently, M, |=K . However,
agents change , current allocation . Since () = , cannot choose valuation falsifies , i.e., M, |=K . Similarly, () = ,
465

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

M, |=K C , . But, given , cannot choose valuation satisfying
current allocation described , i.e., M, |=K . Hence, either case
6|=K controls(A, ).
W
W
Consider direction right left. Suppose equivalent ( ())
while, , set () either . Let = h, RiA , Kripke model.
Remember allocation description corresponding allocation . fact
6= , valuation description corresponding valuation
satisfies (M, ). then, choose order satisfy , thus
M, |=K , . Similarly, M, |=K follows fact
6= . Hence, |=K controls(A, ).
QED

One may ask local characterization coalition controls: Kripke
models valuations M, |=K controls(C, )? notion control,
answer immediately read Theorem 6, given shortly. theorem
general notion: recover characterization result current notion controls(i, ),
would need items (1b) (2b) Theorem 6.
notion control discussed far taken CL - PC: lifted characterization results richer language. However, clear discussion earlier section,
appropriate notion control individual language might obtained using
program givei , givei defined (20). Note hgivei ii valid, hence
hgivei seems general way reason control: achieve
toggling propositional variables delegating them. One easily discuss
coalitional level, lifting Definition (20) case coalitions C suggested
(21) giveC;D . However, stick individual case simplicity. Let us therefore
define
CONTROLS(i, )=
hgivei ii hgivei ii



(23)

definition says agent controls formula iff way distribute propositional variables agents makes appropriate choices remaining variables, holds, also way distributing variables enables enforce .
validity hgivei ii , infer controls(i, ) implies CONTROLS(i, ).
Notice implication way around valid since controls(i, ) never true
control agents variables. example, CONTROLS(i, controls(j, p)) holds iff
p Pi . this, know controls(i, p) CONTROLS(i, controls(j, p)) theorem,
basically says control variable, freely choose keep
pass on. However, controls(i, p) controls(i, controls(j, p)) valid, even
controls(i, p) controls(i, controls(j, p)): agent owns p, cannot choose keep p
pass toggling propositional variables.
state characterization result, introduce notation. two
Kripke models = h, RiA , M0 = h, RiA , 0 K(A, P) agent i, say
M0 allocations = hP1 , . . . , Pi , . . . , Pn 0 = hP01 , . . . , P0i , . . . , P0n
P0i Pi and, j 6= i, Pj P0j . is, M0 obtained executing givei .
case, also say 0 .
466

fiR EASONING BOUT RANSFER C ONTROL

1



1 ()


givei




















fffi


























2





2 ()


2 () 6=

3

















































3 () 6=

3 ()



Figure 9: Illustration

W



W


() .

valuation description , let valuation described , and,
allocation description , let allocation described , let Pi set
propositional variables controlled agent .

W
W
Theorem 6 Let DCL - PC formula
() , given Theorem 2.
Let = h, RiA , Kripke model K(A, P), world M. Then, agent
A,
M, |=K CONTROLS(i, )
iff following two conditions satisfied:
1. ()
(a) ,
(b) = (mod Pi ).
467

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

2. \ ()
(a) ,
(b) = (mod Pi ).
first demonstrate requirements (1) (2) Theorem (6). Suppose satisfies (p q
r), Agent = 1 owns p M, Agent 2 owns q r, Agent 3 propositional variables
M. First all, see Item (1b) needed, guarantee 0 , 0 , holds
M0 , 0 |=K 1 . means that, even 1 given away atoms (resulting
allocation 0 ), still able make true. possible = (p q r):
Agent 1 could simply stay within current allocation make p false. However,
possible = (p q r controls(3, p)) since, 1 delegated control p
3, agent 1 cannot make p false anymore. Moreover, agent give atoms away,
model allocation makes possible satisfy one ,
explains Item (1a). Item (2a) exactly motivation, requirement (2b) easily
understood similar (1b), one realizes normal form expressed
terms normal form follows:
_ _

( \ ()) .



simple illustrating example, suppose two allocations 1 2 , equivalent
((p q) (p q)) 1
((p q) (p q)) 2 .
Note normal form describes valuations 1 2 satisfied. normal
form complementary one sense describes valuations 1
2 falsified:
((p q) (p q)) 1
((p q) (p q)) 2 .
Proof: illustrate proof pictorial story shows requirements
W 1 2
W
theorem sufficient necessary. Given equivalent ( () )
Theorem 2, semantically means corresponds collection shaded areas,
depicted Figure 9. Now, CONTROLS(i, ) true world Kripke model
= h, RiA , i, Agent able move inside shaded area, move outside
well. moving inside shaded area, means able first go model allocation
, world within model valuation description . Notice
move allocation delegating control variables agents (hence
requirement ), move valuation toggling remaining variables
Pi (hence condition = (mod Pi )). shows Condition 1 equivalent
M, |=K hgivei ii . Accordingly, Condition 2 corresponds able move outside
shaded area Figure 9. Semantically, means able first go model allocation
0 , world 0 within model valuation description 0 0 .
Consequently, Condition 2 equivalent M, |=K hgivei ii , finishes proof. QED
468

fiR EASONING BOUT RANSFER C ONTROL

6. Possible Extensions Refinements
section, consider possible extensions refinements framework
presented paper. claim substantial results relating extensions
aim simply indicate possible directions future research.
6.1 Separating First- Second-Order Control
DCL - PC presented here, agent assign value variable (exercising first-order control)
iff give variable away (exercising second-order control). is, pair agents
i, j propositional variable p, following.
|=d controls(i, p) hi ;p ji>

(24)

moments reflection confirm always things work human societies. might empower individual make choice behalf, might happy
idea individual could turn transfer power somebody else. Sometimes,
might acceptable; certainly cases.
straightforwardly distinguish situations extending models
modifying semantics language follows. model defined structure:
= hA, P, , 0 ,
components A, P, 0 , originally defined, = h1 , . . . , n tuple
subsets P, elements indexed agents A, 1 , . . . , n forms partition P.
Now, intended interpretation models follows:
partition 0 defines (initially) ability assign values variables (i.e.,
first-order control variables);
partition defines transfer control variables (i.e., second-order
control variables).
Syntactically, logic define reason structures identical DCL - PC,
however semantics different. fact, element semantics need
change relates definition accessibility relation atomic transfer programs.
Let = hA, P, , 0 , M0 = hA, P, , 00 , two models 0 = hP1 , . . . , Pn
0
0 = hP01 , . . . , P0n i. Then:
(M, M0 ) Ri;p j
iff
1. p (agent second-order control p begin with)
2. k A, p Pk then:
(a) k = j l A, Pl = P0l .
(b) k 6= j then:
469

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

P0j = Pj {p},
P0k = Pk \ {p},
l \ {j, k}, Pl = P0l .
setup, first-order control dynamic, changed transfer programs,
second-order control defined static. Moreover, fact agent first-order control
variable mean second-order control: longer equivalence (24).
6.2 Hierarchies Networks Control
course, reason one stop second-order control. One could extend
setup finite hierarchy control levels, level u > 1 defining transfer
control variables level u 1, level u = 1 defining exercise first-order control.
need extend atomic programs indicate level control transferred. Atomic
programs take form:
;up j
mean agent transfers level u control agent j. semantics language become yet
involved, straightforward define. Somewhat related ideas studied Boella
van der Torre (2008).
Another direction consider multiple agents write access propositional variables.
example, might consider authority relation P A, intended interpretation
(i, j) P means everything empowered everything j empowered do.
Propositional variables allocated sink nodes P (i.e., agents outgoing edges
P). One might ask, example, whether structural properties graph P characterise
formulae object language.

7. Related Work
Although researchers begun develop formal systems reasoning delegation
transfer control (e.g., Li, Grosof, & Feigenbaum, 2003), best knowledge
DCL - PC first system rigorous semantics, complete axiomatization. Also,
emphasis Li et al. (2003) decentralized trust management, roles like
requester, credentials authorizer distinguished. work presented here, emphasis
coalitions achieve, allowed hand control propositional
variables.
Norman Reed (2002) consider logic delegation, particularly focussing group delegation. logic underpinning work STIT (sees that) logic, main operator
form Si A, meaning agent sees A. extends delegation considering
expressions form Si Sj (i sees j sees . . . ). example, axiom
resulting system is:
Si Sj Si A.
work Norman Reed represents serious attempt develop philosophically robust logic
delegation, appropriate use computational systems. However, notion delegation
different ours, (crudely, agents delegate responsibility, rather transfer control),
470

fiR EASONING BOUT RANSFER C ONTROL

dynamic logic flavour DCL - PC absent. Finally, relatively technical results relating
logic presented.
Jones Sergot (1996) consider problem reasoning power individual obtains virtue organisational role. There, notion actions carried order
empower agents certain capabilities central, Jones Sergot also consider interplay actions ability. However, logical formalisation rather different
STIT -like language used, rather dynamic logic framework, relatively technical
results relating framework presented. However, setting Jones Sergot (1996)
much general ours: focus propositional control. somewhat related work,
Boella van der Torre (2006) present formalisation power delegation setting normative multi-agent systems. consider, example, issue delegated goals interact
goals. framework provides rich compelling setting investigating questions
relating delegation. However, overarching object language developed representing
framework, relatively technical results presented relating framework. would
interesting consider whether dynamic logic approach developed within present paper
might adapted framework Boella van der Torre.
respect logics reasoning controlled variables, Boutilier (1994) presents logic
intended capture notions achieve plan using actions relate variables
control. spirit, logic close kind situation aiming model,
although technical details Boutiliers logic (the way control captured logic)
different. Moreover, Boutiliers logic consider multi-agent aspects, dynamics
control present paper.
refer reader work van der Hoek Wooldridge (2005b) extensive discussion many references logics ability. Gerbrandy (2006) generalises results van
der Hoek Wooldridge, considering situations agent partial control
variable, shares control others. Gerbrandy also shows logics propositional control related cylindrical modal logic (Venema, 1995). Specifically, generalisation
CL - PC considered Gerbrandy understood cylindrical modal logic, immediately
yielding complete axiomatization decidability/undecidability results various fragments
system. somewhat related formalism discussed van Benthem, Girard, Roy (2009).
formalism intended enable reasoning ceteris paribus preferences (in sense
things equal). Van Benthem et al. develop logic modality hi,
set propositional formulae; intended interpretation hi state u
state v agreeing u valuation formulae true. seems quite close
connection DCL - PC formalism van Benthem et al., although leave details
future work.
framework explains control terms agents change whcih atoms
choose true false. Sauro (2006) addresses question agents change world,
control coalitions defined terms actions agents repertoire. Finally, note
discussed van der Hoek Wooldridge (2005b), logic CL - PC closely related
well-known formalism quantified Boolean formulae, hard see also
close relationship DCL - PC quantified Boolean formulae. However, may
ultimately gain formal expressive power using DCL - PC rather quantified
Boolean formulae, benefit respect naturalness expression DCL - PC. Quantified
Boolean formulae explicit notion agency dynamics control, representing
471

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

aspects within quantified Boolean formulae leads formulae unintuitive hard
understand.

8. Conclusions
paper, built upon logic CL - PC strategic cooperative ability, control
agents environment represented assigning specific propositional
variables, agents determine truth value. added dynamic
component logic, thus obtaining language DCL - PC one reason
agents (and coalitions agents) achieve setting assigned variables, giving
control others. gave two different equivalent semantics language
direct conventional Kripke semantics provided complete axiomatization
them. key property establishes proof completeness DCL - PCs axiomatic system
fact every formula language provably equivalent normal form: disjunction
conjunctions literals propositional variables p assertions form controls(i, p).
also investigated complexity model checking satisfiability problems DCL - PC,
showed problems worse program-free fragment CL - PC:
PSPACE -complete. demonstrated that, special case ability ATL interpreted
( ) CL - PC, implies simpler satisfiability problem ATL.
several avenues development work. First all, interesting
add assignments agents perform transfer actions perform,
two dimensions agents achieve become projected one dimension. Although parallel
execution program construct language, hence one could still model situations
agent chooses values atoms, time transfers control
atoms, one could least reason effect programs combination truth
assignments transfer control sequence, choice. Secondly, many realistic systems, Property (22) may general: often, want specify overall system satisfies
constraints. this, seems appropriate, however, reason agents
achieve, also guarantee. framework Social Laws (Moses &
Tennenholtz, 1995; van der Hoek, Roberts, & Wooldridge, 2005) could set work order
express certain conditions, agent set certain propositional variable true,
pass control certain variable specific agent, overall system behaves way every agent gets fair chance trigger specific variable (i.e., use
specific resource) infinitely often. Another interesting direction would consider allow
fact agents outside transfer program might change variables program
executing. might require consideration semantics parallel action. Relatedly,
would interesting make possible capture temporal properties system, outside
transfer programs. Here, combination temporal dynamic logic might appropriate.
Similarly, could weaken allocation axiom allow propositional
control agents, capturing idea facts modifiable (by agents
consideration). Another extension would assign control atoms coalitions, rather
individual agents. could cater power social contexts, typical example
coalition bigger threshold n lift piano. Finally, implementation theorem
prover logic would course interesting. Finally, implementation theorem prover
logic would course interesting.
472

fiR EASONING BOUT RANSFER C ONTROL

Acknowledgments authors wish thank JAIR reviewers editors useful comments. Michael Wooldridge Dirk Walther supported EPSRC grant
GR/S62727/01.

Appendix A. Proofs
Theorem 1.
1. schemes Figure 6 derivable DCL - PC.
2. axioms K(i), T(i), B(i), effect(i) coalitional counterparts K(C), T(C), B(C),
effect(C) derivable coalition C.
W
3. ` controls(C, p) iC controls(i, p).

4. property persistence1 (control) also derivable replace agent arbitrary
coalition C.
Proof:
1. describe eight schemes Figure 6 derived axiomatic system
DCL - PC .
at-least(control), follows directly axiom effect(i), taking = >.
at-most(control), `(p) get, using axiom T(i) contraposition, `(p).
Assuming moreover `(p), axiom control, gives controls(i, p). allocation
obtain controls(j, p) agent j 6= i, and, using control(j), get j p
j p, i.e., j `(p) j `(p). Since T(j) gives us j `(p), obtain j `(p), i.e.,
2j `(p).
prove non-effect(i), assume `(p) controls(i, p). axiom control(i) yields
`(p), equivalent 2i `(p).
persistence(non-control), right-to-left direction follows immediately T(j).
left-to-right direction, assume controls(i, p). allocation derive

controls(1, p)5 5controls(i 1, p)5controls(i + 1, p)5 5controls(n, p),
W
this, persistence1 (control) get k6=i 2j controls(k, p). every k 6= i,
controls(k, p) controls(i, p), follows allocation. Hence, using
Necessitation, 2j (controls(k, p) controls(i, p)). W
Axiom K(j),
follows 2j controls(k, p) 2j controls(i, p). Combining k6=i 2j controls(k, p),
obtain desired conclusion 2j controls(i, p).

Notice atomic permanence(;) place requirement p, j. Also,
program ;p j, condition hi ;p ji>, func preconditions(transfer),
hi ;p ji [i ;p j] equivalent. Formally:
hi ;p ji> (hi ;p ji [i ;p j])
473

(25)

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

Now, prove objective permanence(;) induction . induction base,
propositional variable, follows atomic permanence(;). Consider induction step. Suppose theorem proven take = . Assume hi ; p ji>.
show [i ;p j] equivalent showing hi ;p ji.
follows (25) induction hypothesis. final step induction,
suppose objective permanence(;) proven 1 2 . means assume

hi ;p ji> (1 [i ;p j]1 ) (2 [i ;p j]2 )



(26)

take = 1 2 . Obviously, 1 2 , [i ;p j](1 2 ),
proves hi ;p ji> ( [i ;p j]). direction, suppose, given
hi ;p ji>, [i ;p j](1 2 ). use (25) conclude hi ;p ji(1 2 ),
classical modal reasoning obtain hi ;p ji1 hi ;p ji2 . (25)
induction hypothesis get (1 2 ), concludes proof.
Using K( ) Necessitation, derive fact h i> [ ]. Using fact,
possible show propositional reasoning objective permanence equivalent

[ ] (h i> )

(27)

proof induction structure transfer program . first case
induction base, atomic program, holds atomic permanence(;).
second case, suppose test ?. Notice that, axiom test( ), h?i
equivalent , thus h?i> equivalent . then, (27) equivalent
test( ).
Consider induction step = 1 ; 2 . induction hypothesis tells us hi i>
([i ] ), objective = 1, 2. Assume h1 ; 2 i>; implies h1 ih2 i>
h1 i> comp( ), and, induction hypothesis 1 , ([1 ] ).
diamond operator, hence also h1 i, implication 0
derivable, also derive h1 h1 0 using Necessitation K( ). Applying induction hypothesis 2 , i.e., h2 i> ([2 ] ), obtain
h1 ih2 i> h1 i([2 ] ), and, Modus Ponens, arrive h1 i([2 ]
). like demonstrate [1 ; 2 ] . comp( ), equivalent
[1 ][2 ] . direction left right, assume [1 ][2 ]. modal
principle conclude h1 0 h1 i( 0 ) [1 ]. Taking = [2 ]
0 = , obtain h1 i. show holds, suppose . still objective formula, apply induction hypothesis conclude [ 1 ]. This,
course, contradicts h1 i, indeed conclude . Conversely, suppose . Then,
induction hypothesis 1 , also [1 ]. induction hypothesis 2 ,
h2 ([2 ] ) implies [2 ], apply necessitation K(1 )
derive [1 ] [1 ][2 ].
Now, consider = 1 2 , objective permanence proven 1 2 . axiom
union( ), h1 2 i> (h1 i> h2 i>). then, given h1 2 i>,
have: [1 2 ] ([1 ] [2 ]), induction hypothesis explains
right-hand side equivalence equivalent .
474

fiR EASONING BOUT RANSFER C ONTROL

Finally, consider = 1 . axiom mix( ), immediately h1 i>
([1 ] ). direction, recall induction hypothesis
derive, validity, [1 ]. Using Necessitation [1 ] gives [1 ](
[1 ]). then, using assumption axiom ind( ) gives us [ 1 ], hence
also h1 i> ( [1 ]).
axiom inverse, rely normal form obtained Theorem 2 (the proof
involve inverse). know every equivalent disjunction
formulas form 1 2 , 1 objective formula, 2 conjunction formulas form controls(h, q). show 1 2 satisfy
inverse, result follows arbitrary . assume controls(i, p).
precondition(transfer) entails hi ;p ji>, hence apply objectivepermanence(;
) func twice conclude 1 [i ;p j; j ;p i]1 .
consider formulas 2 , starting base case controls(h, q). Assume
controls(i, p): first show left right direction. p 6= q, get
persistence2 (control) [i ;p j; j ;p i]q. p = q consider three subcases: (1)
h = i. derive, controls(i, p), using transfer, hi ; p jicontrols(j, p)
controls(j, p) hj ;p iicontrols(i, p): func comp( ), gives controls(i, p)
[i ;p j; j ;p i]controls(i, p). (2) h 6= i, h = j. Given controls(i, p), [i ; p
j] , done. (3) h 6= i, h 6= j. use persistence 2 (control)
twice get controls(h, q) [i ;p j; j ;p i]controls(h, q). Finally, given controls(i, p),
derive right left direction, is, derive [i ;p j; j ;p i]controls(h, q)
controls(h, q). First assume p 6= q suppose would controls(h, q). Then,
allocation, agent k 6= h, controls(k, q),
persistence2 (control) get [i ;p j; j ;p i]controls(k, q), clearly contradicts
[i ;p j; j ;p i]controls(h, q). suppose p = q. Again, three subcases. (1)
h = i, conclusion follows overall assumption controls(i, p). (2) Suppose
h 6= i, h 6= j. reasoning applies case (1). Finally, (3) suppose h 6= i, h = j.
Since controls(i, p) given, hi ;p jicontrols(j, p) (by transfer), hence
hi ;p j; j ;p iicontrols(i, p). Now, would [i ;p j; j ;p i]controls(j, p),
6= j, leads contradiction (use allocation fact h 6= i, h = j),
indeed derive [i ;p j; j ;p i]controls(j, p) controls(j, p).
reverse, similar inverse.
2. proved van der Hoek Wooldridge (2005b).
3. definition controls(C,
p) C p W
C p. Let C = {a1 , a2 , . . . , aC }. axiom
W
control(i), iC controls(i, p) iC (i p p). contrapositive
T(i), . apply repeatedly agents C, giving
a1 a2 aC . is, according Comp-,
W
W C . (Note
proven contrapositive T(C).) gives us iC controls(i, p) iC (C p
C p). Using Comp- again, see consequent implication equivalent
C p C p. direction, first show most(control) Figure 6.
`(p) get, using axiom T(i) contraposition, `(p). Assuming moreover `(p),
axiom control(i), gives controls(i, p). allocation obtain controls(j, p),
475

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

agent j 6= i. Using control(j), get j p j p, i.e., j `(p) j `(p). Since T(j)
gives us j `(p), obtain j `(p), i.e., 2j `(p).
W
W
suppose iC controls(i, p). allocation, xA\C controls(x, p).
means one x, x p x p. case distinction based
p p. first case, assume p, derive, C, 2 p, thus 2C p.
Hence C p, get controls(C, p). case p, similarly have,
C, 2i p, gives C p, controls(C, p). all, matter
whether p p, get controls(C, p).
W
4. easy: previous item showed W
controls(C, p) means iC controls(i, p).
Applying persistence1 (control), get iC 2j controls(i, p). since controls(i, p)
controls(C, p) C, also have, C, 2j controls(i, p) 2j controls(C, p)
(use Necessitation K(j)). proves 2j controls(C, p).
QED

References
Alur, R., Henzinger, T. A., & Kupferman, O. (2002). Alternating-time temporal logic. Journal
ACM, 49(5), 672713.
Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal Logic. Cambridge University Press:
Cambridge, England.
Boella, G., & van der Torre, L. (2006). Delegation power normative multiagent systems.
Deontic Logic Artificial Normative Systems, 8th International Workshop Deontic
Logic Computer Science, DEON 2006, Utrecht, Netherlands.
Boella, G., & van der Torre, L. (2008). Institutions hierarchy authorities distributed
dynamic environments. Artificial Intelligence Law, 16(1), 5371.
Boutilier, C. (1994). Toward logic qualitative decision theory. Proceedings Knowledge
Representation Reasoning (KR&R-94), pp. 7586.
Chellas, B. (1980). Modal Logic: Introduction. Cambridge University Press: Cambridge, England.
French, T. (2006). Bisimulation Quantifiers Modal Logic. Ph.D. thesis, University
Western Australia, Perth, Australia.
Gerbrandy, J. (2006). Logics propositional control. Proceedings Fifth International
Joint Conference Autonomous Agents Multiagent Systems (AAMAS-2006), pp. 193
200, Hakodate, Japan.
Ghilardi, S., & Zawadowski, M. (2000). bisimulation quantifiers classifying toposes.
Wolter, F., Wansing, H., de Rijke, M., & Zakharyaschev, M. (Eds.), Advances Modal Logic,
pp. 193220.
Goranko, V., & Jamroga, W. (2004). Comparing semantics logics multi-agent systems. Synthese, 139(2), 241280. section Knowledge, Rationality Action.
Harel, D., Kozen, D., & Tiuryn, J. (2000). Dynamic Logic. MIT Press: Cambridge, MA.
476

fiR EASONING BOUT RANSFER C ONTROL

Jamroga, W., & van der Hoek, W. (2004). Agents know play. Fundamenta Informaticae,
63(2-3), 185219.
Jones, A. J. I., & Sergot, M. (1996). formal characterisation institutionalised power. Logic
Journal IGPL, 3, 427443.
Li, N., Grosof, B. N., & Feigenbaum, J. (2003). Delegation logic: logic-based approach distributed authorization. ACM Transactions Information System Security, 6(1), 128
171.
Moses, Y., & Tennenholtz, M. (1995). Artificial social systems. Computers AI, 14(6), 533562.
Norman, T. J., & Reed, C. (2002). Group delegation responsibility. Proceedings First
International Joint Conference Autonomous Agents Multiagent Systems (AAMAS2002), pp. 491498, Bologna, Italy.
Pauly, M. (2001). Logic Social Software. Ph.D. thesis, University Amsterdam. ILLC Dissertation Series 2001-10.
Sauro, L. (2006). Formalizing Admissibility Criteria Coalition Formation among Goal Directed
Agents. Ph.D. thesis, University Turin, Turin, Italy.
Savitch, W. J. (1970). Relationships nondeterministic deterministic tape complexities.
Journal Computer Systems Sciences, 4(2), 177192.
van Benthem, J., Girard, P., & Roy, O. (2009). Everything else equal: modal logic
ceteris paribus preferences. Journal Philosophical Logic, 38, 83125.
van der Hoek, W., Roberts, M., & Wooldridge, M. (2005). Knowledge social laws. Dignum,
F., Dignum, V., Koenig, S., Kraus, S., Singh, M., & Wooldridge, M. (Eds.), Proceedings
Fourth International Joint Conference Autonomous Agents Multi-Agent Systems
(AAMAS 05), pp. 674681, New York, USA. ACM Inc.
van der Hoek, W., & Wooldridge, M. (2003). Time, knowledge, cooperation: Alternating-time
temporal epistemic logic applications. Studia Logica, 75(1), 125157.
van der Hoek, W., & Wooldridge, M. (2005a). dynamics delegation, cooperation,
control: logical account. Dignum, F., Dignum, V., Koenig, S., Kraus, S., Singh, M.,
& Wooldridge, M. (Eds.), Proceedings Fourth International Joint Conference Autonomous Agents Multi-Agent Systems (AAMAS 05), pp. 701708, New York, USA. ACM
Inc.
van der Hoek, W., & Wooldridge, M. (2005b). logic cooperation propositional control.
Artificial Intelligence, 64, 81119.
Venema, Y. (1995). Cylindric modal logic. Journal Symbolic Logic, 60, 591623.

477

fiJournal Artificial Intelligence Research 37 (2010) 397-435

Submitted 11/09; published 03/10

Training Multilingual Sportscaster:
Using Perceptual Context Learn Language
David L. Chen
Joohyun Kim
Raymond J. Mooney

DLCC @ CS . UTEXAS . EDU
SCIMITAR @ CS . UTEXAS . EDU
MOONEY @ CS . UTEXAS . EDU

Department Computer Science
University Texas Austin
1 University Station C0500, Austin TX 78712, USA

Abstract
present novel framework learning interpret generate language using perceptual context supervision. demonstrate capabilities developing system learns
sportscast simulated robot soccer games English Korean without language-specific
prior knowledge. Training employs ambiguous supervision consisting stream descriptive textual comments sequence events extracted simulation trace. system
simultaneously establishes correspondences individual comments events
describe building translation model supports parsing generation. also
present novel algorithm learning events worth describing. Human evaluations
generated commentaries indicate reasonable quality cases even par
produced humans limited domain.

1. Introduction
current natural language processing (NLP) systems built using statistical learning algorithms trained large annotated corpora. However, annotating sentences requisite parse
trees (Marcus, Santorini, & Marcinkiewicz, 1993), word senses (Ide & Jeronis, 1998) semantic
roles (Kingsbury, Palmer, & Marcus, 2002) difficult expensive undertaking. contrast,
children acquire language exposure linguistic input context rich, relevant,
perceptual environment. Also, connecting words phrases objects events world,
semantics language grounded perceptual experience (Harnad, 1990). Ideally, machine
learning system would able acquire language similar manner without explicit human supervision. step direction, present system describe events simulated
soccer game learning sample language commentaries paired traces simulated
activity without language-specific prior knowledge. screenshot system generated
commentary shown Figure 1.
fair amount research grounded language learning (Roy, 2002;
Bailey, Feldman, Narayanan, & Lakoff, 1997; Barnard, Duygulu, Forsyth, de Freitas, Blei, & Jordan, 2003; Yu & Ballard, 2004; Gold & Scassellati, 2007), focus dealing
raw perceptual data rather language issues. Many systems aimed learn meanings words phrases rather interpreting entire sentences. recent work dealt
fairly complex language data (Liang, Jordan, & Klein, 2009; Branavan, Chen, Zettlemoyer, &
c
2010
AI Access Foundation. rights reserved.

fiC HEN , K IM , & OONEY

Figure 1: Screenshot commentator system
Barzilay, 2009) address three problems alignment, semantic parsing, natural
language generation. contrast, work investigates build complete language learning
system using parallel data perceptual context. study problem simulated environment retains many important properties dynamic world multiple agents
actions avoiding many complexities robotics computer vision. Specifically,
use RoboCup simulator (Chen, Foroughi, Heintz, Kapetanakis, Kostiadis, Kummeneje, Noda,
Obst, Riley, Steffens, Wang, & Yin, 2003) provides fairly detailed physical simulation
robot soccer. several groups constructed RoboCup commentator systems (Andre,
Binsted, Tanaka-Ishii, Luke, Herzog, & Rist, 2000) provide textual natural-language (NL)
transcript simulated game, systems use manually-developed templates based
learning.
commentator system learns semantically interpret generate language RoboCup
soccer domain observing on-going commentary game paired evolving simulator state. exploiting existing techniques abstracting symbolic description activity
field detailed states physical simulator (Andre et al., 2000), obtain pairing
natural language symbolic description perceptual context uttered.
However, training data highly ambiguous comment usually co-occurs several events game. integrate enhance existing methods learning semantic parsers
NL generators (Kate & Mooney, 2007; Wong & Mooney, 2007) order learn understand
generate language ambiguous training data. also develop system that,
ambiguous training data, learns events worth describing, also perform
strategic generation, is, deciding say well say (tactical generation). 1
1. conciseness, use terminology early work generation (e.g., McKeown, 1985). Strategic tactical
generation also commonly referred content selection surface realization, respectively

398

fiT RAINING ULTILINGUAL PORTSCASTER

evaluate system demonstrate language-independence training generate
commentaries English Korean. Experiments test data (annotated evaluation purposes only) demonstrate system learns accurately semantically parse sentences, generate
sentences, decide events describe. Finally, subjective human evaluation commentated game clips demonstrate limited domain, system generates sportscasts
cases similar quality produced humans.
three main contributions make paper. First, explore possibility
learning grounded language models perceptual context form ambiguous parallel
data. Second, investigate several different methods disambiguating data determined
using combined score includes tactical strategic generation scores performed
best overall. Finally, built complete system learns sportscast multiple languages.
carefully verified automatic human evaluations system able perform
several tasks including disambiguating training data, semantic parsing, tactical strategic
generation. language involved work restricted compared handcrafted commercial sportscasting systems, goal demonstrate feasibility learning grounded
language system language-specific prior knowledge.
remainder paper structured follows. Section 2 provides background previous work utilize extend build system. Section 3 describes sportscasting
data collected train test approach. Section 4 Section 5 present details
basic methods learning tactical strategic generation, respectively, initial experimental results. Section 6 discusses extensions basic system incorporate information
strategic generation process disambiguating training data. Section 7 presents experimental results initializing system data disambiguated recent method aligning
language facts may refer. Section 8 discusses additions try detect superfluous sentences refer extracted event. Section 9 presents human evaluation
automatically generated sportscasts. Section 10 reviews related work, Section 11 discusses future
work, Section 12 presents conclusions.

2. Background
Systems learning semantic parsers induce function maps natural-language (NL) sentences
meaning representations (MRs) formal logical language. Existing work focused
learning supervised corpus sentence manually annotated correct MR
(Mooney, 2007; Zettlemoyer & Collins, 2007; Lu, Ng, Lee, & Zettlemoyer, 2008; Jurcicek, Gasic,
Keizer, Mairesse, Thomson, & Young, 2009). human annotated corpora expensive
difficult produce, limiting utility approach. Kate Mooney (2007) introduced
extension one system, K RISP (Kate & Mooney, 2006), learn ambiguous
training data requires little human annotation effort. However, system unable
generate language required sportscasting task. Thus, enhanced another system
called WASP (Wong & Mooney, 2006) capable language generation well semantic
parsing similar manner allow learn ambiguous supervision. briefly describe
previous systems below. systems assume access formal deterministic
context-free grammar (CFG) defines formal meaning representation language (MRL). Since
MRLs formal computer-interpretable languages, grammar usually easily available.
399

fiC HEN , K IM , & OONEY

2.1 KRISP KRISPER
K RISP (Kernel-based Robust Interpretation Semantic Parsing) (Kate & Mooney, 2006) uses
support vector machines (SVMs) string kernels build semantic parsers. SVMs state-ofthe-art machine learning methods learn maximum-margin separators prevent over-fitting
high-dimensional data natural language text (Joachims, 1998). extended
non-linear separators non-vector data exploiting kernels implicitly create even higher
dimensional space complex data (nearly) linearly separable (Shawe-Taylor & Cristianini,
2004). Recently, kernels strings trees effectively applied variety problems
text learning NLP (Lodhi, Saunders, Shawe-Taylor, Cristianini, & Watkins, 2002; Zelenko,
Aone, & Richardella, 2003; Collins, 2002; Bunescu & Mooney, 2005). particular, K RISP uses
string kernel introduced Lodhi et al. (2002) classify substrings NL sentence.
First, K RISP learns classifiers recognize word phrase NL sentence indicates
particular concept MRL introduced MR. uses production rules
MRL grammar represent semantic concepts, learns classifiers production
classify NL substrings indicative production not. semantically parsing
sentence, classifier estimates probability production covering different substrings
sentence. information used compositionally build complete MR
sentence. Given partial matching provided string kernels over-fitting prevention
provided SVMs, K RISP experimentally shown particularly robust noisy training
data (Kate & Mooney, 2006).
K RISPER (Kate & Mooney, 2007) extension K RISP handles ambiguous training
data, sentence annotated set potential MRs, one correct.
Psuedocode method shown Algorithm 1. employs iterative approach analogous
expectation maximization (EM) (Dempster, Laird, & Rubin, 1977) improves upon selection
correct NLMR pairs iteration. first iteration (lines 3-9), assumes
MRs paired sentence correct trains K RISP resulting noisy supervision.
subsequent iterations (lines 11-27), K RISPER uses currently trained parser score
potential NLMR pair, selects likely MR sentence, retrains parser
resulting disambiguated supervised data. manner, K RISPER able learn type
weak supervision expected grounded language learner exposed sentences ambiguous
contexts. However, system previously tested artificially corrupted generated
data.
2.2 WASP WASP1
WASP (Word-Alignment-based Semantic Parsing) (Wong & Mooney, 2006) uses state-of-the-art
statistical machine translation (SMT) techniques (Brown, Cocke, Della Pietra, Della Pietra, Jelinek,
Lafferty, Mercer, & Roossin, 1990; Yamada & Knight, 2001; Chiang, 2005) learn semantic
parsers. SMT methods learn effective machine translators training parallel corpora consisting
human translations documents one alternative natural languages. resulting
translators typically significantly effective manually developed systems SMT
become dominant approach machine translation. Wong Mooney (2006) adapted
methods learn translate NL MRL rather one NL another.
First, SMT word alignment system, GIZA++ (Och & Ney, 2003; Brown, Della Pietra, Della
Pietra, & Mercer, 1993), used acquire bilingual lexicon consisting NL substrings coupled
400

fiT RAINING ULTILINGUAL PORTSCASTER

Algorithm 1 K RISPER
input sentences associated sets meaning representations R(s)
output BestExamplesSet, set NL-MR pairs,
SemanticModel , K RISP semantic parser
1:
2:
3:
4:
5:
6:
7:
8:
9:

main
//Initial training loop
sentence si
meaning representation mj R(si )
add (si , mj ) InitialTrainingSet
end
end
SemanticModel = Train(InitialTrainingSet)

10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

//Iterative retraining
repeat
sentence si
meaning representation mj R(si )
mj .score = Evaluate(si , mj , SemanticModel )
end
end
BestExampleSet
set consistent examples = {(s, m)|s S, MR(s)}
P
m.score maximized
SemanticModel = rain(BestExamplesSet)
Convergence MAX ITER reached
end main

22:

function Train(TrainingExamples)
Train K RISP unambiguous TrainingExamples
25:
return trained K RISP semantic parser
26: end function
23:

24:

27:

function Evaluate(s, m, SemanticModel )
Use K RISP semantic parser SemanticModel find derivation meaning representation sentence
30:
return parsing score
31: end function

28:

29:

401

fiC HEN , K IM , & OONEY

translations target MRL. formal languages, MRLs frequently contain many
purely syntactic tokens parentheses brackets, difficult align words
NL. Consequently, found much effective align words NL productions
MRL grammar used parse corresponding MR. Therefore, GIZA++ used
produce N 1 alignment words NL sentence sequence MRL
productions corresponding top-down left-most derivation corresponding MR.
Complete MRs formed combining NL substrings translations using
grammatical framework called synchronous CFG (SCFG) (Aho & Ullman, 1972), forms
basis existing syntax-based SMT (Yamada & Knight, 2001; Chiang, 2005). SCFG,
right hand side production rule contains two strings, case one NL
MRL. Derivations SCFG simultaneously produce NL sentences corresponding MRs.
bilingual lexicon acquired word alignments training data used construct set
SCFG production rules. probabilistic parser produced training maximum-entropy
model using EM learn parameters SCFG productions, similar methods
used Riezler, Prescher, Kuhn, Johnson (2000), Zettlemoyer Collins (2005).
translate novel NL sentence MR, probabilistic chart parser (Stolcke, 1995) used find
probable synchronous derivation generates given NL, corresponding MR
generated derivation returned.
Since SCFGs symmetric, used generate NL MR well parse NL
MR (Wong & Mooney, 2007). allows learned grammar used parsing
generation, elegant property important advantages (Shieber, 1988). generation
system, WASP1 , uses noisy-channel model (Brown et al., 1990):
arg max Pr(e|f ) = arg max Pr(e) Pr(f |e)
e

(1)

e

e refers NL string generated given input MR, f . Pr(e) language model,
Pr(f |e) parsing model provided WASPs learned SCFG. generation task find
sentence e (1) e good sentence priori, (2) meaning input
MR. language model, use standard n-gram model, useful ranking candidate
generated sentences (Knight & Hatzivassiloglou, 1995).

3. Sportscasting Data
train test system, assembled human-commentated soccer games RoboCup
simulation league (www.robocup.org). Since focus language learning computer vision, chose use simulated games instead real game video simplify extraction
perceptual information. Based ROCCO RoboCup commentators incremental event recognition module (Andre et al., 2000) manually developed symbolic representations game events
rule-based system automatically extract simulator traces. extracted
events mainly involve actions ball, kicking passing, also include
game information whether current playmode kickoff, offside, corner kick.
events represented atomic formulas predicate logic timestamps. logical facts
constitute requisite MRs, manually developed simple CFG formal semantic
language. Details events detected complete grammar found Appendix A.
NL portion data, humans commentate games watching
simulator. collected commentaries English Korean. English commentaries
402

fiT RAINING ULTILINGUAL PORTSCASTER

Total # comments
Total # words
Vocabulary size
Avg. words per comment

English dataset
2036
11742
454
5.77

Korean dataset
1999
7941
344
3.97

Table 1: Word statistics English Korean datasets

Number events

Total

2001 final
2002 final
2003 final
2004 final

4003
2223
2113
2318

722
514
410
390

2001 final
2002 final
2003 final
2004 final

4003
2223
2113
2318

673
454
412
460

Number comments
MRs Correct MR
English dataset
671
520
458
376
397
320
342
323
Korean dataset
650
600
444
419
396
369
423
375

Events per comment
Max Average Std. Dev.
9
10
12
9

2.235
2.403
2.849
2.729

1.641
1.653
2.051
1.697

10
12
10
9

2.138
2.489
2.551
2.601

2.076
3.083
3.672
2.593

Table 2: Alignment statistics English Korean datasets. comments correct meaning representations associated essentially noise training
data (18% English dataset 8% Korean dataset). Moreover, average
2 possible events linked comment half links
incorrect.

produced two different people Korean commentaries produced single person.
commentators typed comments text box, recorded timestamp.
construct final ambiguous training data, paired comment events
occurred five seconds less comment made. Examples ambiguous training
data shown Figure 2. edges connect sentences events might refer. English
translations Korean commentaries included figure readers benefit
part actual data. Note use English words predicates constants
MRs human readability only, system treats arbitrary conceptual tokens must
learn connection English Korean words.
annotated total four games, namely, finals RoboCup simulation league
year 2001 2004. Word statistics data shown Table 1.
sentences fairly short due nature sportscasts, data provides challenges form
synonyms (e.g. Pink1, PinkG pink goalie refer player) polysemes
(e.g. kick kicks toward goal refers kick event whereas kicks Pink3 refers
pass event.) Alignment statistics datasets shown Table 2. 2001 final almost
twice number events games went double overtime.
403

fiC HEN , K IM , & OONEY

Natural Language Commentary

Meaning Representation
badPass ( PurplePlayer1 ,
PinkPlayer8 )
turnover ( PurplePlayer1 ,
PinkPlayer8 )
kick ( PinkPlayer8 )
pass ( PinkPlayer8 , PinkPlayer11 )
kick ( PinkPlayer11 )

Purple goalie turns ball
Pink8
Purple team sloppy today
Pink8 passes Pink11
Pink11 looks around teammate

kick ( PinkPlayer11 )
ballstopped
kick ( PinkPlayer11 )
pass ( PinkPlayer11 , PinkPlayer8 )
kick ( PinkPlayer8 )
pass ( PinkPlayer8 , PinkPlayer11 )

Pink11 makes long pass Pink8

Pink8 passes back Pink11

(a) Sample trace ambiguous English training data

Natural Language Commentary

Meaning Representation

10 11 .
(purple10 passes purple 11)

kick ( PurplePlayer10 )

11 10 .
(purple11 passes purple 10)

kick ( PurplePlayer11 )

10 3 .
(pink3 steals ball purple 10)

steal ( PinkPlayer3 )

3 .
(pink3 passes pink goalie)

kick ( PinkPlayer3 )

pass ( PurplePlayer10 , PurplePlayer11 )

pass ( PurplePlayer11 , PurplePlayer10 )

turnover ( PurplePlayer10 , PinkPlayer3 )

playmode ( free_kick_r )

(b) Sample trace ambiguous Korean training data

Figure 2: Examples training data. outgoing edges comments indicate
possibly associated meaning representations considered system. bold links
indicate correct matches comments meaning representations.

404

fiT RAINING ULTILINGUAL PORTSCASTER

evaluation purposes only, gold-standard matching produced examining comment manually selecting correct MR exists. matching approximate
sometimes comments contain information present MRs. example, comment might describe location length pass MR captures participants
pass. bold lines Figure 2 indicate annotated correct matches sample data. Notice sentences correct matches (about one fifth English data one tenth
Korean data). example, sentence Purple team sloppy today Figure 2(a)
cannot represented MRL consequently corresponding correct MR.
another example, Korean sentence translation pink3 passes pink goalie Figure 2(b) represented MRL, correct match due incomplete
event detection. free kick called pink3 passing pink goalie pass event
retrieved. Finally, case sentence Pink11 makes long pass Pink8 Figure 2(a), correct MR falls outside 5-second window. game, Table 2 shows
total number NL sentences, number least one recent extracted event
could refer, number actually refer one recent extracted
events. maximum, average, standard deviation number recent events paired
comment also given.

4. Learning Tactical Generation Ambiguous Supervision
existing systems capable solving parts sportscasting problem, none
able perform whole task. need system deal ambiguous supervision
like K RISPER generate language like WASP. introduce three systems
both. overview differences existing systems new systems present
shown Table 3.
three systems introduced based extensions WASP, underlying language
learner. main problem need solve disambiguate training data
train WASP create language generator. new system uses different
disambiguation criteria determine best matching NL sentences MRs.

4.1 WASPER
first system extension WASP manner similar K RISP extended create
K RISPER. uses EM-like retraining handle ambiguously annotated data, resulting system
call WASPER. general, system learns semantic parsers extended handle
ambiguous data long produce confidence levels given NLMR pairs. Given set
sentences set MRs associated sentence R(s), disambiguate
data finding pairs (s, m), R(s) = arg maxm P r(m|s). Although
probability used here, ranking relative potential parses would suffice. pseudocode
WASPER shown Algorithm 2. difference compared K RISPER pseudocode
use WASP semantic parser instead K RISP parser. Also, produce WASP
language generator well desired final output task.
405

fiC HEN , K IM , & OONEY

Algorithm

Underlying learner

K RISP
K RISPER
WASP

SVM
K RISP
GIZA align words,
MR tokens,
learn probalistic SCFG
WASP
First disambiguate
K RISPER,
train WASP
WASP

WASPER
K RISPER -WASP

WASPER -G EN

Generate?

Disambiguation criteria



Yes

Ambiguous
data?

Yes


Yes
Yes

Yes
Yes

WASPs parsing score
K RISPs parsing score

Yes

Yes

NIST score
best NL given MR

n/a
K RISPs parsing score
n/a

Table 3: Overview various learning systems presented. first three algorithms existing
systems. introduce last three systems able learn ambiguous
training data acquire language generator. differ disambiguate
training data.

Algorithm 2 WASPER
input sentences associated sets meaning representations R(s)
output BestExamplesSet, set NL-MR pairs,
SemanticModel , WASP semantic parser/language generator
1: main
2:
Algorithm 1
3: end main
4:

function Train(TrainingExamples)
Train WASP unambiguous TrainingExamples
7:
return trained WASP semantic parser/language generator
8: end function

5:

6:

9:

function Evaluate(s, m, SemanticModel )
11:
Use WASP semantic parser SemanticModel find derivation meaning representation sentence
12:
return parsing score
13: end function
10:

406

fiT RAINING ULTILINGUAL PORTSCASTER

4.2 KRISPER-WASP
K RISP shown quite robust handling noisy training data (Kate & Mooney, 2006).
important training noisy training data used initialize parser
K RISPERs first iteration. However, K RISPER cannot learn language generator, necessary sportscasting task. result, create new system called K RISPER-WASP
good disambiguating training data capable generation. first use K RISPER
train ambiguous data produce disambiguated training set using prediction
likely MR sentence. unambiguous training set used train WASP
produce parser generator.
4.3 WASPER-GEN
K RISPER WASPER, criterion selecting best NLMR pairs retraining based maximizing probability parsing sentence particular MR. However,
since WASPER capable parsing generation, could alternatively select best
NLMR pairs evaluating likely generate sentence particular MR. Thus,
built another version WASPER called WASPER-G EN disambiguates training data
order maximize performance generation rather parsing. pseudocode shown
Algorithm 3. algorithm WASPER except evaluation function. uses
generation-based score rather parsing-based score select best NLMR pairs.
Specifically, NLMR pair (s, m) scored computing NIST score, machine translation (MT) metric, sentence best generated sentence (lines 9-12).2
Formally, given set sentences set MRs associated sentence
R(s), disambiguate data finding pairs (s, m), R(s) =
arg maxm N IST (s, argmaxs P r(s |m)).
NIST measures precision translation terms proportion n-grams shares
human translation (Doddington, 2002). also used evaluate NL generation. Another
popular MT metric BLEU score (Papineni, Roukos, Ward, & Zhu, 2002), inadequate
purpose since comparing one short sentence another instead comparing whole
documents. BLEU score computes geometric mean n-gram precision value n,
means score 0 matching n-gram found every value n. common
setting maximum n 4, two sentences matching 4-gram would
receive BLEU score 0. Consequently, BLEU score unable distinguish quality
generated sentences since fairly short. contrast, NIST uses additive score
avoids problem.
4.4 Experimental Evaluation
section presents experimental results RoboCup data four systems: K RISPER, WASPER,
K RISPER-WASP, WASPER-G EN. Since aware existing systems could
learn semantically parse generate language using ambiguous supervision based perceptual context, constructed lower upper baselines using unmodified WASP. Since
2. natural way use generation-based score would use probability NL given MR (P r(s|m)).
However, initial experiments using metric produce good results. also tried changing WASP
maximize joint probability instead parsing probability. However, also improve results.

407

fiC HEN , K IM , & OONEY

Algorithm 3 WASPER -G EN
input sentences associated sets meaning representations R(s)
output BestExamplesSet, set NL-MR pairs,
SemanticModel , WASP semantic parser/language generator
1: main
2:
Algorithm 1
3: end main
4:

function Train(TrainingExamples)
6:
Algorithm 2
7: end function

5:

8:

function Evaluate(s, m, SemanticModel )
GeneratedSentence Use WASP language generator SemanticModel produce
sentence meaning representation
11:
return NIST score GeneratedSentence
12: end function
9:

10:

WASP requires unambiguous training data, randomly pick meaning sentence
set potential MRs serve lower baseline. use WASP trained gold matching
consists correct NLMR pairs annotated human upper baseline. represents upper-bound systems could achieve disambiguated training data
perfectly.
evaluate system three tasks: matching, parsing, generation. matching task
measures well systems disambiguate training data. parsing generation tasks
measure well systems translate NL MR, MR NL, respectively.
Since four games total, trained using possible combinations one three
games. matching, measured performance training data since goal disambiguate data. parsing generation, tested games used training.
Results averaged train/test combinations. evaluated matching parsing using
F-measure, harmonic mean recall precision. Precision fraction systems
annotations correct. Recall fraction annotations gold-standard
system correctly produces. Generation evaluated using BLEU scores roughly estimates well produced sentences match target sentences. treat game
whole document avoid problem using BLEU score sentence-level comparisons mentioned earlier. Also, increase number reference sentences MR using
sentences test data corresponding equivalent MRs, e.g. pass(PinkPLayer7,
PinkPlayer8) occurs multiple times test data, sentences matched MR
gold matchings used reference sentences MR.
4.4.1 ATCHING NL



MR

Since handling ambiguous training data important aspect grounded language learning,
first evaluate well various systems pick correct NLMR pairs. Figure 3 shows Fmeasure identifying correct set pairs various systems. learning systems
408

fi0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65
F-measure

F-measure

RAINING ULTILINGUAL PORTSCASTER

0.6
0.55

0.6
0.55

0.5

0.5

0.45

0.45

WASPER
WASPER-GEN
KRISPER
random matching

0.4

WASPER
WASPER-GEN
KRISPER
random matching

0.4

0.35

0.35
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

0.9

0.9

0.8

0.8

0.7

0.7

F-measure

F-measure

Figure 3: Matching results basic systems. WASPER -G EN performs best, outperforming
existing system K RISPER datasets.

0.6

0.5
WASP gold matching
KRISPER
KRISPER-WASP
WASPER
WASPER-GEN
WASP random matching

0.4

0.3

0.6

0.5
WASP gold matching
KRISPER
KRISPER-WASP
WASPER
WASPER-GEN
WASP random matching

0.4

0.3

0.2

0.2
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 4: Semantic parsing results basic systems. results largely mirrors
matching results WASPER -G EN performing best overall.

perform significantly better random F-measure 0.5. English
Korean data, WASPER -G EN best system. WASPER also equals outperforms previous
system K RISPER well.
4.4.2 EMANTIC PARSING
Next, present results accuracy learned semantic parsers. trained system
used parse produce MR sentence test set correct MR
gold-standard matching. parse considered correct matches gold standard
exactly. Parsing fairly difficult task usually one way describe
event. example, Player1 passes player2 refer event Player1 kicks
ball player2. Thus, accurate parsing requires learning different ways people describe
409

fi0.5

0.6

0.45

0.55

0.4

0.5

0.35

0.45
BLEU

BLEU

C HEN , K IM , & OONEY

0.3
0.25

0.4
0.35

WASP gold matching
KRISPER-WASP
WASPER
WASPER-GEN
WASP random matching

0.2
0.15

WASP gold matching
KRISPER-WASP
WASPER
WASPER-GEN
WASP random matching

0.3
0.25

0.1

0.2
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 5: Tactical generation results basic systems. relative performances
various systems change, WASPER -G EN still best system.

event. Synonymy limited verbs. data, Pink1, PinkG pink goalie
refer player1 pink team. Since providing systems prior knowledge,
learn different ways referring entity.
parsing results shown Figure 4 generally correlate well matching results. Systems better disambiguating training data also better parsing
supervised training data less noisy. WASPER-G EN best overall English Korean data. interesting note K RISPER relatively well English data
compared matching performance. K RISP robust noise WASP
(Kate & Mooney, 2006) even though trained noisier set data WASPER -G EN
still produced comparable parser.
4.4.3 G ENERATION
third evaluation task generation. WASP-based systems given MR test
set gold-standard matching NL sentence asked generate NL description.
quality generated sentence measured comparing gold-standard using BLEU
scoring.
task tolerant noise training data parsing system
needs learn one way accurately describe event. property reflected results,
shown Figure 5, even baseline system, WASP random matching, fairly well,
outperforming K RISPER-WASP datasets WASPER Korean data. number
event types fairly small, relatively small number correct matchings required
perform task well long event type associated correct sentence pattern
often sentence pattern.
two tasks, WASPER -G EN best system task. One possible explanation WASPER-G ENs superior performance stems disambiguation objective function.
Systems like WASPER K RISPER-WASP use parsing scores attempt learn good translation model sentence pattern. hand, WASPER-G EN tries learn good
410

fiT RAINING ULTILINGUAL PORTSCASTER

translation model MR pattern. Thus, WASPER-G EN likely converge good
model fewer MR patterns sentence patterns. However, argued learning
good translation models sentence pattern help producing varied commentaries,
quality captured BLEU score. Another possible advantage WASPER -G EN
uses softer scoring function. probabilities parsing particular sentence
MR sensitive noise training data, WASPER -G EN looks top generated
sentences MR. Even noise data, top generated sentence remains relatively
constant. Moreover, minor variations sentence change results dramatically since
NIST score allows partial matching.

5. Learning Strategic Generation
language generator alone enough produce sportscast. addition tactical generation
deciding say something, sportscaster must also preform strategic generation
choosing say (McKeown, 1985).
developed novel method learning events describe. event type (i.e.
predicate like pass, goal), system uses training data estimate probability
mentioned sportscaster. Given gold-standard NLMR matches, probability
easy estimate; however, learner know correct matching. Instead, system
must estimate probabilities ambiguous training data. compare two basic methods
estimating probabilities.
first method uses inferred NLMR matching produced language-learning system.
probability commenting event type, ei , estimated percentage events
type ei matched NL sentence.
second method, call Iterative Generation Strategy Learning (IGSL), uses variant EM, treating matching assignments hidden variables, initializing match
prior probability, iterating improve probability estimates commenting event
type. Unlike first method, IGSL uses information MRs explicitly associated
sentence training. Algorithm 4 shows pseudocode. main loop alternates two
steps:
1. Calculating expected probability NLMR matching given current model
likely event commented (line 6)
2. Update prior probability event type mentioned human commentator based
matchings (line 9).
first iteration, NLMR match assigned
probability inversely proportional
P
amount ambiguity associated sentence ( eEvent(s) Pr (e) = |Event(s)|). example,
sentence associated five possible MRs assign match probability 51 . prior
probability mentioning event type estimated average probability assigned
instances event type. Notice process always guarantee proper probability since
MR associated multiple sentences. Thus, limit probability one.
subsequent iterations, probabilities NLMR matchings updated according
new priors. assign match prior probability event type normalized across
associated MRs NL sentence. update priors event type using
411

fiC HEN , K IM , & OONEY

Algorithm 4 Iterative Generation Strategy Learning
input event types E = {e1 , ..., en }, number occurrences event type otalCount(ei )
entire game trace, sentences event types associated meaning representations Event(s)
output probabilities commenting event type P r(ei )
1: Initialize Pr (ei ) = 1
2: repeat
3:
event type ei E
4:
MatchCount = 0
5:
sentence
P
Pr (e)

P
6:
ProbOfMatch = eEvent(s)e=e
Pr (e)
eEvent(s)

7:
8:
9:
10:
11:

MatchCount = MatchCount + ProbOfMatch
end
MatchCount
Pr (ei ) = min( TotalCount(e
,1) {Ensure proper probabilities}
i)
end
Convergence MAX ITER reached

!"#$%&&

'()*+*,-,%.&)/&*#,$0& 4)(2+-,5#3&6()*+*,-,%.&
1)22#$%#3&)$&

*+--7%)66#3&

89:;&<&8:=>?&

?9@:&<&8:=>A&

*+3'+77B6C(6-#DE6,$F?G&

:9;D:&

:9?8A&

%C($)"#(B6C(6-#DE6,$F?G&

:9;:;&

:9H@H&

%C($)"#(B6C(6-#DE6,$F?G&

:9;:;&
:9:;8&

IJ#&7#-#1%#3&#"#$%&,7&"#(*+-,5#3&&
K$&#"#$%&,7&7#-#1%#3&*+7#3&&
)$&%J#&$)(2+-,5#3&6()*+*,-,%.& (+$3)2-.&+11)(3,$0&%)&,%7&6()*+*,-,%.&&
)/&*#,$0&1)22#$%#3&)$&

Figure 6: example strategic generation component works. every timestep,
stochastically select event events occurring moment.
decide whether verbalize selected event based IGSLs estimated probability
commented upon.

new estimated probabilities matchings. process repeated probabilities
converge pre-specified number iterations occurred.
generate sportscast, use learned probabilities determine events describe.
time step, first determine events occurring time. select one
randomly based normalized probabilities. avoid overly verbose, want
make comment every time something happening, especially event rarely commented on.
Thus, stochastically decide whether comment selected event based probability.
example process shown Figure 6.
412

fi0.8

0.8

0.7

0.7

0.6

0.6

F-measure

F-measure

RAINING ULTILINGUAL PORTSCASTER

0.5
inferred gold matching
IGSL
inferred KRISPER
inferred WASPER
inferred WASPER-GEN
inferred random matching

0.4

0.5
inferred gold matching
IGSL
inferred KRISPER
inferred WASPER
inferred WASPER-GEN
inferred random matching

0.4

0.3

0.3
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 7: Strategic generation results various systems. novel algorithm IGSL performs
best, almost par upper bound uses gold-annotated matchings.

event
ballstopped
kick
pass
turnover
badPass

# occurrences
5817
2122
1069
566
371

% commented
1.72 104
0.0 33
0.999
0.214
0.429

IGSL
1.09 105
0.018
0.983
0.909
0.970

inferred WASPER -G EN
0.016
0.117
0.800
0.353
0.493

Table 4: Top 5 frequent events, % times commented on, probabilities
learned top algorithms English data

5.1 Experimental Evaluation
different methods learning strategic generation evaluated based often events
describe test data coincide human decided describe. first
approach, results using inferred matchings produced K RISPER, WASPER, WASPER-G EN
well gold random matching establishing baselines presented Figure 7.
graph, clear IGSL outperforms learning inferred matchings actually
performs level close using gold matching. However, important note
limiting potential learning gold matching using predicates decide
whether talk event.
English data, probabilities learned IGSL inferred matchings WASPER G EN five frequently occurring events shown Table 4. WASPER -G EN learns
fairly good probabilities general, well IGSL frequent events.
IGSL uses occurrences events associated possible comments
training iterations. Rarely commented events ballstopped kick often occur without
comments uttered. Consequently, IGSL assigns low prior probabilities
lowers chances matched sentences. hand, WASPER -G EN
use priors sometimes incorrectly matches comments them. Thus, using inferred
413

fiC HEN , K IM , & OONEY

matches WASPER -G EN results learning higher probabilities commenting rarely
commented events.
methods use predicates MRs decide whether comment not,
perform quite well data collected. particular, IGSL performs best, use
strategic generation rest paper.

6. Using Strategic Generation Improve Matching
section, explore knowledge learned strategic generation used improve
accuracy matching sentences MRs. previous section, described several ways
learn strategic generation, including IGSL learns directly ambiguous training data.
Knowing events people tend talk also help resolve ambiguities training
data. Events likely discussed also likely matched
NL sentence disambiguating training data. Therefore, section describes methods
integrate strategic generation scores (such Table 4) scoring NLMR pairs used
matching process.
6.1 WASPER-GEN-IGSL
WASPER -G EN -IGSL extension WASPER -G EN also uses strategic generation scores
IGSL. WASPER -G EN uses NIST score pick best MR sentence finding MR
generates sentence closet actual NL sentence. WASPER -G EN -IGSL combines tactical
(NIST) strategic (IGSL) generation scores pick best NLMR pairs. simply multiplies
NIST score IGSL score together form composite score. new score biases
selection matching pairs include events IGSL determines are, priori, likely
discussed. helpful, especially beginning WASP produce
particularly good language generator. many instances, generated sentences
possible MRs equally bad overlap target sentence. Even generation
produces perfectly good sentence, generation score unreliable comparing
single sentence single reference often short well. Consequently, often
difficult WASPER-G EN distinguish among several MRs equal scores. hand,
event types different strategic generation scores, default choosing
MR higher prior probability mentioned. Algorithm 5 shows pseudocode
WASPER -G EN -IGSL.
6.2 Variant WASPER-GEN Systems
Although WASPER -G EN uses NIST score estimate goodness NLMR pairs, could easily
use MT evaluation metric. already discussed unsuitability BLEU comparing short individual sentences since assigns zero many pairs. However, NIST score also
limitations. example, normalized, may affect performance WASPER -G EN IGSL combined IGSL score. Another limitation comes using higher-order
N-grams. Commentaries domain often short, frequently higher-order
N-gram matches generated sentences target NL sentences.
METEOR metric (Banerjee & Lavie, 2005) designed resolve various weaknesses
BLEU NIST metrics, focused word-to-word matches reference
414

fiT RAINING ULTILINGUAL PORTSCASTER

Algorithm 5 WASPER -G EN -IGSL
input sentences associated sets meaning representations R(s)
output BestExamplesSet, set NL-MR pairs,
SemanticModel , WASP semantic parser/language generator
1: main
2:
Algorithm 1
3: end main
4:

function Train(TrainingExamples)
6:
Algorithm 2
7: end function

5:

8:
9:
10:
11:
12:
13:
14:
15:

function Evaluate(s, m, SemanticModel )
Call Algorithm 4 collect IGSL scores
GeneratedSentence Use WASP language generator SemanticModel produce
sentence meaning representation
TacticalGenerationScore NIST score GeneratedSentence
StrategicGenerationScore Pr (event type m) result Algorithm 4
return TacticalGenerationScore StrategicGenerationScore
end function

sentence test sentence. METEOR first evaluates uni-gram matches reference
test sentence also determines well words ordered. METEOR seems
appropriate domain good generated sentences missing adjectives adverbs critical meaning sentence prevent higher-order N-gram matches.
addition, METEOR normalized always 0 1, may combine effectively IGSL scores (which also range 01).
6.3 Experimental Evaluation
evaluated new systems, WASPER-G EN-I GSL NIST METEOR scoring using
methodology Section 4.4. matching results shown Figure 8, including results
WASPER -G EN, best system previous section. WASPER-G EN-IGSL
either NIST METEOR scoring clearly outperforms WASPER-G EN. indicates strategicgeneration information help disambiguate data. Using different MT metrics produces
less noticeable effect. clear winner English data; however, METEOR seems
improve performance Korean data.
Parsing results shown Figure 9. previously noted, parsing results generally mirror
matching results. new systems outperform WASPER -G EN, previously best system.
again, English data show clear advantage using either NIST METEOR,
Korean data gives slight edge using METEOR metric.
Results tactical generation shown Figure 10. English Korean
data, new systems come close performance WASPER-G EN beat it. However,
new systems outperform K RISPER -WASP WASPER shown figure.
415

fi0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75
F-measure

F-measure

C HEN , K IM , & OONEY

0.7
0.65
0.6

0.7
0.65
0.6

0.55

0.55
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.5

WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.5

0.45

0.45
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75
F-measure

F-measure

Figure 8: Matching results. Integrating strategic information improves results previously best system WASPER -G EN. choice MT metric used, however, makes
less impact.

0.7
0.65

0.7
0.65

0.6

0.6
WASP gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.55

WASP gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.55

0.5

0.5
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 9: Semantic parsing results. results similar matching results integrating
strategic generation information improves performance.

416

fi0.5

0.6

0.45

0.55

0.4

0.5

0.35

0.45
BLEU

BLEU

RAINING ULTILINGUAL PORTSCASTER

0.3
0.25

0.4
0.35

0.2

0.3
WASP gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.15

WASP gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.25

0.1

0.2
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 10: Tactical generation results. two new systems come close performance
WASPER -G EN, beat it. However, outperform systems
presented earlier shown figure.

Overall, expected, using strategic information improves performance matching
semantic parsing tasks. English Korean datasets, WASPER-G EN-IGSL
variant using METEOR metric clearly outperform WASPER-G EN utilize
strategic information. However, strategic information improve tactical generation.
could due ceiling effect WASPER -G EN already performs level near upper
baseline. matching performance improved, generation performance little room
grow.

7. Using Generative Alignment Model
Recently, Liang et al. (2009) developed generative model used match naturallanguage sentences facts corresponding database may refer. one
evaluation domains, used English RoboCup sportscasting data. method solves
matching (alignment) problem data, address tasks semantic parsing
language generation. However, generative model elegantly integrates simple strategic
tactical language generation models order find overall probable alignment sentences
events. demonstrated improved matching performance English data, generating
accurate NLMR pairs best system. Thus, curious results could
used improve systems, also perform semantic parsing generation. also
ran code new Korean data resulted much worse matching results compared
best system seen Table 5.
simplest way utilizing results use NLMR pairs produced method
supervised data WASP. expected, improved NLMR pairs English data resulted
improved semantic parsers seen results Table 6. Even Korean dataset,
training matchings produced system ended fairly well even though matching performance poor. tactical generation, using matching produced marginal
improvement English dataset surprisingly large improvement Korean data
417

fiC HEN , K IM , & OONEY

Algorithm
Liang et al. (2009)
WASPER
WASPER-G EN
WASPER-G EN-I GSL
WASPER-G EN-I GSL -M ETEOR

English dataset
initialization Initialized
75.7
59.7
79.3
68.1
75.8
73.5
73.9
73.1
75.1

Korean dataset
initialization Initialized
69.4
72.8
76.6
75.3
80.0
81.9
81.6
83.8
84.1

Table 5: Matching results (F1 scores) 4-fold cross-validation English Korean
datasets. Systems run initialization initialized matchings produced
Liang et al.s (2009) system.

Algorithm
WASP
WASPER
WASPER-G EN
WASPER-G EN-I GSL
WASPER-G EN-I GSL -M ETEOR

English dataset
initialization Initialized
n/a
80.3
61.84
79.32
70.15
77.59
73.19
73.04
72.75
74.62

Korean dataset
initialization Initialized
n/a
74.01
69.12
75.69
72.02
77.49
78.75
75.27
80.65
81.21

Table 6: Semantic parsing results (F1 scores) 4-fold cross-validation English
Korean datasets. Systems run initialization initialized matchings
produced Liang et al.s (2009) system.

Algorithm
WASP
WASPER
WASPER-G EN
WASPER-G EN-I GSL
WASPER-G EN-I GSL -M ETEOR

English dataset
initialization Initialized
n/a
0.4580
0.3471
0.4599
0.4560
0.4414
0.4223
0.4585
0.4062
0.4353

Korean dataset
initialization Initialized
n/a
0.5828
0.4524
0.6118
0.5575
0.6796
0.5371
0.6710
0.5180
0.6591

Table 7: Tactical generation results (BLEU score) 4-fold cross-validation English
Korean datasets. Systems run initialization initialized matchings
produced Liang et al.s (2009) system.

shown Table 7. Overall, using alignments produced Liang et al.s system resulted good
semantic parsers tactical generators.
addition training WASP alignment, also utilize output better
starting point systems. Instead initializing iterative alignment methods
model trained ambiguous NLMR pairs, initialized disambiguated
NLMR pairs produced Liang et al.s system.
418

fiT RAINING ULTILINGUAL PORTSCASTER

Initializing systems manner almost always improved performance three
tasks (Tables 5, 6, 7). Moreover, results best systems exceed simply training WASP alignment cases except semantic parsing English data. Thus,
combining Liang et al.s alignment disambiguation techniques seems produce best
overall results. English data, WASPER initialization performs best matching generation. slightly worse semantic parsing task compared WASP trained
Liang et al.s alignment. Korean data, systems better training WASP
alignment. WASPER -G EN -I GSL -M ETEOR initialization performs best matching
semantic parsing WASPER -G EN initialization performs best generation.
Overall, initializing systems alignment output Liang et al.s generative model
improved performance expected. Starting cleaner set data led better initial semantic
parsers language generators led better end results. Furthermore, incorporating
semantic parser tactical generator, able improve Liang et al.s alignments
achieve even better results cases.

8. Removing Superfluous Comments
far, discussed handle ambiguity multiple possible MRs
NL sentence. training, methods assume NL sentence matches
exactly one potential MRs. However, comments superfluous, sense
refer currently extracted event represented set potential MRs. previously
shown Tables 2, one fifth English sentences one tenth Korean sentences
superfluous sense.
many reasons superfluous sentences. occur naturally language
people always talk current environment. domain, sportscasters often mention
past events general information particular teams players. Moreover, depending
application, chosen MRL may represent things people talk about. example,
RoboCup MRL cannot represent information players actively engaged
ball. Finally, even sentence represented chosen MRL, errors perceptual
system incorrect estimation event occurred also lead superfluous sentences.
perceptual errors alleviated degree increasing size window used
capture potential MRs (the previous 5 seconds experiments). However, comes
cost increased ambiguity associates MRs sentence.
deal problem superfluous sentences, eliminate lowest-scoring NLMR
pairs (e.g. lowest parsing scores WASPER lowest NIST scores WASPER-G EN). However,
order set pruning threshold, need automatically estimate amount superfluous
commentary absence supervised data. Notice problem looks similar
strategic generation problem (estimating likely MR participates correct matching
opposed likely NL sentence participates correct matching), approaches used
cannot applied. First, cannot use matches inferred existing systems estimate
fraction superfluous comments since current systems match every sentence MR.
also difficult develop algorithm similar IGSL due imbalance NL sentences
MRs. Since many MRs, examples events occurring without
commentaries vice versa.
419

fiC HEN , K IM , & OONEY

8.1 Estimating Superfluous Rate Using Internal Cross Validation
propose using form internal (i.e. within training set) cross validation estimate rate
superfluous comments. algorithm used conjunction systems,
chose implement K RISPER trains much faster systems. makes
tractable train many different semantic parsers choose best one. basic idea
use part ambiguous training data estimate accuracy semantic parser even though
know correct matchings. Assuming reasonable superfluous sentence rate, know
time correct MR contained set MRs associated NL sentence.
Thus, assume semantic parser parses NL sentence one MRs associated
better one parses MR set. approach estimating
accuracy, evaluate semantic parsers learned using various pruning thresholds pick
best one. algorithm briefly summarized following steps:
1. Split training set internal training set internal validation set.
2. Train K RISPER N times internal training set using N different threshold values (eliminating lowest scoring NLMR pairs threshold retraining iteration
Algorithm 1).
3. Test N semantic parsers internal validation set determine parser able
parse largest number sentences one potential MRs.
4. Use threshold value produced best parser previous step train final parser
complete original training set.
8.2 Experiments
evaluated effect removing superfluous sentences three tasks: matching, parsing,
generation. present results K RISPER K RISPER -WASP. matching,
show results K RISPER responsible disambiguating training data
systems (so K RISPER -WASPs results same). generation, show results
K RISPER-WASP, since K RISPER cannot perform generation.
matching results shown Figure 11 demonstrate removing superfluous sentences
improve performance English Korean, although difference small absolute
terms. parsing results shown Figure 12 indicate removing superfluous sentences usually
improves accuracy K RISPER K RISPER -WASP marginally. observed
many times, parsing results consistent matching results. Finally, tactical generation results shown Figure 13 suggest removing superfluous comments actually decreases
performance somewhat. again, potential explanation generation less sensitive
noisy training data. removing superfluous comments improves purity training data,
also removes potentially useful examples. Consequently, system learn generate sentences removed data. Overall, generation, advantage
cleaner disambiguated training data apparently outweighed loss data.
420

fi0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65
F-measure

F-measure

RAINING ULTILINGUAL PORTSCASTER

0.6
0.55

0.6
0.55

0.5

0.5

0.45

0.45

0.4

0.4

KRISPER
KRISPER superfluous comment removal

0.35

KRISPER
KRISPER superfluous comment removal

0.35
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65
F-measure

F-measure

Figure 11: Matching results comparing effects removing superfluous comments.

0.6
0.55

0.6
0.55

0.5

0.5
KRISPER
KRISPER superfluous comment removal
KRISPER-WASP
KRISPER-WASP superfluous comment removal

0.45

KRISPER
KRISPER superfluous comment removal
KRISPER-WASP
KRISPER-WASP superfluous comment removal

0.45

0.4

0.4
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

0.5

0.6

0.45

0.55

0.4

0.5

0.35

0.45
BLEU

BLEU

Figure 12: Semantic parsing results improved marginally superfluous comment removal.

0.3

0.4

0.25

0.35

0.2

0.3

0.15

0.25

KRISPER-WASP
KRISPER-WASP superfluous comment removal

0.1

KRISPER-WASP
KRISPER-WASP superfluous comment removal

0.2
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 13: Tactical generation performance decreases removing superfluous comments.

421

fiC HEN , K IM , & OONEY

9. Human Subjective Evaluation
best, automatic evaluation generation imperfect approximation human assessment.
Moreover, automatically evaluating quality entire generated sportscast even difficult. Consequently, used Amazons Mechanical Turk collect human judgements
produced sportscasts. human judge shown three clips simulated game video one sitting. 8 video clips total. 8 clips use 4 game segments 4 minutes each, one
four games (2001-2004 RoboCup finals). 4 game segments commentated
human system. use IGSL determine events comment
WASPER -G EN (our best performing system tactical generation) produce commentaries.
make commentaries varied, took top 5 outputs WASPER -G EN chose
one stochastically weighted scores. system always trained three games, leaving game test segment extracted. video clips accompanied
commentaries appear subtitles screen well audio produced automated text speech system 3 videos shown random counter-balanced order ensure
consistent bias toward segments shown earlier later. asked judges score
commentaries using following metrics:

Score
5
4
3
2
1

Fluency
Flawless
Good
Non-native
Disfluent
Gibberish

Semantic
Correctness
Always
Usually
Sometimes
Rarely
Never

Sportscasting
Ability
Excellent
Good
Average
Bad
Terrible

Fluency semantic correctness, adequacy, standard metrics human evaluations NL
translations generations. Fluency measures well commentaries structured, including
syntax grammar. Semantic correctness indicates whether commentaries accurately describe
happening game. Finally, sportscasting ability measures overall quality
sportscast. includes whether sportscasts interesting flow well. addition
metrics, also asked whether thought sportscast composed human
computer (Human?).
Since Mechanical Turk recruits judges Internet, make sure judges
assigning ratings randomly. Thus, addition asking rate video, also
asked count number goals video. Incorrect responses question caused
ratings discarded. ensure judges faithfully watched entire clip
assigning ratings. pruning, average 36 ratings (from 40 original ratings)
8 videos English data. Since difficult recruit Korean judges
Internet, recruited person collected 7 ratings average video
Korean data. Table 8 9 show results English Korean data, respectively.
Statistically significant results shown boldface.
Results surprisingly good English data across categories machine actually
scoring higher human average. However, differences statistically significant
3. Sample video clips sound available web http://www.cs.utexas.edu/users/ml/
clamp/sportscasting/.

422

fiT RAINING ULTILINGUAL PORTSCASTER

2001 final
2002 final
2003 final
2004 final
Average

Commentator
Human
Machine
Human
Machine
Human
Machine
Human
Machine
Human
Machine

Fluency
3.74
3.89
4.13
3.97
3.54
3.89
4.03
4.13
3.86
3.94

Semantic
Correctness
3.59
3.81
4.58
3.74
3.73
4.26
4.17
4.38
4.03
4.03

Sportscasting
Ability
3.15
3.61
4.03
3.29
2.61
3.37
3.54
4.00
3.34
3.48

Human?
20.59%
40.00%
42.11%
11.76%
13.51%
19.30%
20.00%
56.25%
24.31%
26.76%

Table 8: Human evaluation overall sportscasts English data. Bold numbers indicate statistical
significance.

2001 final
2002 final
2003 final
2004 final
Average

Commentator
Human
Machine
Human
Machine
Human
Machine
Human
Machine
Human
Machine

Fluency
3.75
3.50
4.17
3.25
3.86
2.38
3.00
2.71
3.66
2.93

Semantic
Correctness
4.13
3.67
4.33
3.38
4.29
3.25
3.75
3.43
4.10
3.41

Sportscasting
Ability
4.00
2.83
3.83
3.13
4.00
2.88
3.25
3.00
3.76
2.97

Human?
50.00%
33.33%
83.33%
50.00%
85.71%
25.00%
37.50%
14.29%
62.07%
31.03%

Table 9: Human evaluation overall sportscasts Korean data. Bold numbers indicate statistical
significance.

423

fiC HEN , K IM , & OONEY

based unpaired t-test (p > 0.05). Nevertheless, encouraging see machine
rated highly. variance humans performance since two different
commentators. notably, compared machine, humans performance 2002 final
quite good commentary included many details position players,
types passes, comments overall flow game. hand,
humans performance 2003 final quite bad human commentator
mechanical used sentence pattern repeatedly. machine performance
even throughout although sometimes gets lucky. example, machine serendipitously said
beginning exciting match. near start 2004 final clip simply
statement incorrectly learned correspond extracted MR actually unrelated.
results Korean impressive. human beats machine average
categories. However, largest difference scores category 0.8.
Moreover, absolute scores indicate generated Korean sportscast least acceptable
quality. judges even mistakenly thought produced humans one third time.
Part reason worse performance compared English data Korean commentaries fairly detailed included events extracted limited perceptual
system. Thus, machine simply way competing limited expressing
information present extracted MRs.
also elicited comments human judges get qualitative evaluation. Overall,
judges thought generated commentaries good accurately described actions
field. Picking top 5 generated sentences also added variability machine-generated
sportscasts improved results compared earlier experiments presented Chen
Mooney (2008). However, machine still sometimes misses significant plays scoring
corner kicks. plays happen much less frequently often coincide
many events (e.g. shooting ball kickoffs co-occur scoring). Thus, machine
harder time learning infrequent events. Another issue concerns representation.
Many people complain long gaps sportscasts lack details. event detector
concentrates ball possession positions elapsed time. Thus, player holding onto
ball dribbling long time produce events detected simulated perceptual
system. Also, short pass backfield treated exactly long pass across
field near goal. Finally, people desired colorful commentary (background information,
statistics, analysis game) fill voids. somewhat orthogonal issue since
goal build play-by-play commentator described events currently happening.

10. Related Work
section review related work semantic parsing, natural language generation
well grounded language learning.
10.1 Semantic Parsing
mentioned Section 2, existing work semantic parser learners focused supervised
learning sentence annotated semantic meaning. semantic-parser learners additionally require either syntactic annotations (Ge & Mooney, 2005) prior syntactic knowledge target language (Ge & Mooney, 2009; Zettlemoyer & Collins, 2005, 2007). Since
world never provides direct feedback syntactic structure, language-learning methods
424

fiT RAINING ULTILINGUAL PORTSCASTER

require syntactic annotation directly applicable grounded language learning. Therefore,
methods learn semantic annotation critical learning language perceptual
context.
use logic formulas MRs, particular MRL use contains atomic
formulas equivalently represented frames slots. systems use
transformation-based learning (Jurcicek et al., 2009), Markov logic (Meza-Ruiz, Riedel, &
Lemon, 2008) learn semantic parsers using frames slots. principle, framework
used semantic parser learner long provides confidence scores parse results.
10.2 Natural Language Generation
several existing systems sportscast RoboCup games (Andre et al., 2000). Given
game states provided RoboCup simulator, extract game events generate real-time
commentaries. consider many practical issues timeliness, coherence, variability,
emotion needed produce good sportscasts. However, systems hand-built
generate language using pre-determined templates rules. contrast, concentrate
learning problem induce generation components ambiguous training data. Nevertheless, augmenting system components systems could improve
final sportscasts produced.
also prior work learning lexicon elementary semantic expressions corresponding natural language realizations (Barzilay & Lee, 2002). work uses multiple-sequence
alignment datasets supply several verbalizations corresponding semantics extract
dictionary.
Duboue McKeown (2003) first propose algorithm learning strategic generation automatically data. Using semantics associated texts, system learns classifier
determines whether particular piece information included presentation not.
recent work learning strategic generation using reinforcement learning
(Zaragoza & Li, 2005). work involves game setting speaker must aid listener
reaching given destination avoiding obstacles. game played repeatedly find
optimal strategy conveys pertinent information minimizing number
messages. consider different problem setting reinforcements available
strategic generation learner.
addition, also work performing strategic generation collective task
(Barzilay & Lapata, 2005). considering strategic generation decisions jointly, captures
dependencies utterances. creates consistent overall output consistent
humans perform task. approach could potentially help system produce
better overall sportscasts.
10.3 Grounded Language Learning
One ambitious end-to-end visually-grounded scene-description system VITRA (Herzog & Wazinski, 1994) comments traffic scenes soccer matches. system first
transforms raw visual data geometrical representations. Next, set rules extract spatial relations interesting motion events representations. Presumed intentions, plans, plan
425

fiC HEN , K IM , & OONEY

interactions agents also extracted based domain-specific knowledge. However,
since system hand-coded cannot adapted easily new domains.
Srihari Burhans (1994) used captions accompanying photos help identify people
objects. introduced idea visual semantics, theory extracting visual information
constraints accompanying text. example, using caption information, system
determine spatial relationship entities mentioned, likely size shape
object interest, whether entity natural artificial. However, system also based
hand-coded knowledge.
Siskind (1996) performed earliest work learning grounded word meanings.
learning algorithm addresses problem ambiguous training referential uncertainty
semantic lexical acquisition, address larger problems learning complete semantic
parsers language generators.
Several robotics computer vision researchers worked inferring grounded meanings
individual words short referring expressions visual perceptual context (e.g., Roy, 2002;
Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004). However, complexity
natural language used existing work restrictive, many systems use pre-coded
knowledge language, almost use static images learn language describing objects
relations, cannot learn language describing actions. sophisticated grammatical
formalism used learn syntax work finite-state hidden-Markov model. contrast,
work exploits latest techniques statistical context-free grammars syntax-based statistical
machine translation handle complexities natural language.
recently, Gold Scassellati (2007) built system called TWIG uses existing language knowledge help learn meaning new words. robot uses partial parses focus
attention possible meanings new words. playing game catch, robot able
learn meaning well identity relations.
also variety work learning captions accompany pictures
videos (Satoh, Nakamura, & Kanade, 1997; Berg, Berg, Edwards, & Forsyth, 2004). area
particular interest given large amount captioned images video available web
television. Satoh et al. (1997) built system detect faces newscasts. However, use fairly
simple manually-written rules determine entity picture language refers.
Berg et al. (2004) used elaborate learning method cluster faces names. Using
data, estimate likelihood entity appearing picture given context.
recent work video retrieval focused learning recognize events sports videos
connecting English words appearing accompanying closed captions (Fleischman
& Roy, 2007; Gupta & Mooney, 2009). However, work learns connection
individual words video events learn describe events using full grammatical
sentences. avoid difficult problems computer vision, work uses simulated world
perception complex events participants much simpler.
addition observing events passively, also work grounded language learning interactive environments computer video games (Gorniak & Roy, 2005).
work, players cooperate communicate order accomplish certain task.
system learns map spoken instructions specific actions; however, relies existing statistical
parsers learn syntax semantics language perceptual environment
alone. Kerr, Cohen, Chang (2008) developed system learns grounded word-meanings
nouns, adjectives, spatial prepositions human instructing perform tasks vir426

fiT RAINING ULTILINGUAL PORTSCASTER

tual world; however, system assumes existing syntactic parser prior knowledge verb
semantics unable learn experience.
Recently, interest learning interpret English instructions describing use particular website perform computer tasks (Branavan et al., 2009; Lau,
Drews, & Nichols, 2009). systems learn predict correct computer action (pressing
button, choosing menu item, typing text field, etc.) corresponding step instructions. Instead using parallel training data perceptual context, systems utilize
direct matches words natural language instructions English words explicitly occurring menu items computer instructions order establish connection
language environment.
One core subproblems work addresses matching sentences facts world
refer. recent projects attempt align text English summaries American
football games database records contain statistics events game (Snyder
& Barzilay, 2007; Liang et al., 2009). However, Snyder Barzilay (2007) use supervised
approach requires annotating correct correspondences text semantic
representations. hand, Liang et al. (2009) developed unsupervised approach
using generative model solve alignment problem. also demonstrated improved results
matching sentences events RoboCup English sportscasting data. However, work
address semantic parsing language generation. Section 7 presents results showing
methods improve NLMR matches produced approach well use
learn parsers generators.

11. Future Work
previously discussed, limitations current system due inadequacies
perception events extracted RoboCup simulator. language commentary,
particularly Korean data, refers information events currently represented
extracted MRs. example, player dribbling ball captured perceptual system.
event extractor could extended include information output representations.
Commentaries always immediate actions happening field. also
refer statistics game, background information, analysis game.
difficult obtain, would simple augment potential MRs include events
current score number turnovers, etc. may difficult learn correctly,
potentially would make commentaries much natural engaging.
statements commentaries specifically refer pattern activity across several
recent events rather single event. example, one English commentaries,
statement Purple team sloppy today. appears series turn-overs team.
simulated perception could extended extract patterns activity sloppiness;
however assumes concepts predefined, extracting many higher-level
predicates would greatly increase ambiguity training data. current system assumes
already concepts words needs learn perceive concepts represent
MRs. However, would interesting include Whorfian style language
learning (Whorf, 1964) unknown word sloppiness could actually cause
creation new concept. content words seem consistently correlate
perceived event, system could collect examples recent activity word used try
427

fiC HEN , K IM , & OONEY

learn new higher-level concept captures regularity situations. example, given
examples situations referred sloppy, inductive logic programming system (Lavrac &
Dzeroski, 1994) able detect pattern several recent turnovers.
Another shortcoming current system MR treated independently. fails
exploit fact many MRs related other. example, pass preceded kick,
bad pass followed turnover. natural way use graphical representation
represent entities events also relationships them.
Currently tactical strategic generation system loosely coupled. However,
conceptually much closely related, solving one problem help solve
other. Initializing system output Liang et al. (2009), uses generative model
includes strategic tactical components, produced somewhat better results. However,
interaction components loose tighter integration different
pieces could yield stronger results tasks.
obvious extension current work apply real RoboCup games rather
simulated ones. Recent work Rozinat, Zickler, Veloso, van der Aalst, McMillen (2008)
analyzes games RoboCup Small Size League using video overhead camera.
using symbolic event trace extracted real perceptual system, methods could
applied real-world games. Using speech recognition accept spoken language input another
obvious extension.
currently exploring extending approach learn interpret generate NL instructions navigating virtual environment. system observe one person giving English
navigation instructions (e.g. Go hall turn left pass chair.) another person follows directions get chosen destination. collecting examples sentences
paired actions executed together information local environment,
system construct ambiguous supervised dataset language learning. approach
could eventually lead virtual agents games educational simulations automatically
learn interpret generate natural language instructions.

12. Conclusion
presented end-to-end system learns generate natural-language sportscasts
simulated RoboCup soccer games training sample human commentaries paired automatically extracted game events. learning semantically interpret generate language without
explicitly annotated training data, demonstrated system learn language simply
observing linguistic descriptions ongoing events. also demonstrated systems language
independence successfully training produce sportscasts English Korean.
Dealing ambiguous supervision inherent training environment critical issue
learning language perceptual context. evaluated various methods disambiguating
training data order learn semantic parsers language generators. Using generation
evaluation metric criterion selecting best NLMR pairs produced better results
using semantic parsing scores initial training data noisy. system also learns
model strategic generation ambiguous training data estimating probability
event type evokes human commentary. Moreover, using strategic generation information
help disambiguate training data shown improve results. also demonstrated
system initialized alignments produced different system achieve better
428

fiT RAINING ULTILINGUAL PORTSCASTER

results either system alone. Finally, experimental evaluation verified overall system
learns accurately parse generate comments generate sportscasts competitive
produced humans.

Acknowledgments
thank Adam Bossy work simulating perception RoboCup games. also
thank Percy Liang sharing software experimental results us. Finally, thank
anonymous reviewers JAIR editor, Lillian Lee, insightful comments
helped improve final presentation paper. work funded NSF grant IIS
0712907X. experiments run Mastodon Cluster, provided NSF Grant
EIA-0303609.

Appendix A. Details meaning representation language
Table 10 shows brief explanations different events detect simulated perception.
Event
Playmode
Ballstopped
Turnover
Kick
Pass
BadPass
Defense
Steal
Block

Description
Signifies current play mode defined game
ball speed minimum threshold
current possessor ball last possessor different teams
player possession ball one time interval next
player gains possession ball different player team
pass player gaining possession ball different team
transfer one player opposing player penalty area
player possession ball one time interval another player
different team next time interval
Transfer one player opposing goalie.
Table 10: Description different events detected

include context-free grammar developed meaning representation language. derivations start root symbol *S.

*S
*S
*S
*S
*S
*S
*S
*S
*S

->
->
->
->
->
->
->
->
->

playmode ( *PLAYMODE )
ballstopped
turnover ( *PLAYER , *PLAYER )
kick ( *PLAYER )
pass ( *PLAYER , *PLAYER )
badPass ( *PLAYER , *PLAYER )
defense ( *PLAYER , *PLAYER )
steal ( *PLAYER )
block ( *PLAYER )

429

fiC HEN , K IM , & OONEY

*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER

->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->

->
->
->
->
->
->
->
->
->
->
->
->
->
->
->

kick_off_l
kick_off_r
kick_in_l
kick_in_r
play_on
offside_l
offside_r
free_kick_l
free_kick_r
corner_kick_l
corner_kick_r
goal_kick_l
goal_kick_r
goal_l
goal_r

pink1
pink2
pink3
pink4
pink5
pink6
pink7
pink8
pink9
pink10
pink11
purple1
purple2
purple3
purple4
purple5
purple6
purple7
purple8
purple9
purple10
purple11

430

fiT RAINING ULTILINGUAL PORTSCASTER

References
Aho, A. V., & Ullman, J. D. (1972). Theory Parsing, Translation, Compiling. Prentice
Hall, Englewood Cliffs, NJ.
Andre, E., Binsted, K., Tanaka-Ishii, K., Luke, S., Herzog, G., & Rist, T. (2000). Three RoboCup
simulation league commentator systems. AI Magazine, 21(1), 5766.
Bailey, D., Feldman, J., Narayanan, S., & Lakoff, G. (1997). Modeling embodied lexical development. Proceedings Nineteenth Annual Conference Cognitive Science Society.
Banerjee, S., & Lavie, A. (2005). METEOR: automatic metric MT evaluation improved
correlation human judgments. Proceedings ACL Workshop Intrinsic
Extrinsic Evaluation Measures Machine Translation and/or Summarization, pp. 6572
Ann Arbor, Michigan. Association Computational Linguistics.
Barnard, K., Duygulu, P., Forsyth, D., de Freitas, N., Blei, D. M., & Jordan, M. I. (2003). Matching
words pictures. Journal Machine Learning Research, 3, 11071135.
Barzilay, R., & Lapata, M. (2005). Collective content selection concept-to-text generation.
Proceedings Human Language Technology Conference Conference Empirical
Methods Natural Language Processing (HLT/EMNLP-05).
Barzilay, R., & Lee, L. (2002). Bootstrapping lexical choice via multiple-sequence alignment.
Proceedings 2002 Conference Empirical Methods Natural Language Processing
(EMNLP-02).
Berg, T. L., Berg, A. C., Edwards, J., & Forsyth, D. A. (2004). Whos picture. Advances
Neural Information Processing Systems 17 (NIPS 2004).
Branavan, S., Chen, H., Zettlemoyer, L. S., & Barzilay, R. (2009). Reinforcement learning
mapping instructions actions. Proceedings Joint conference 47th Annual
Meeting Association Computational Linguistics 4th International Joint
Conference Natural Language Processing Asian Federation Natural Language
Processin (ACL-IJCNLP 2009).
Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J. D., Mercer,
R. L., & Roossin, P. S. (1990). statistical approach machine translation. Computational
Linguistics, 16(2), 7985.
Brown, P. F., Della Pietra, V. J., Della Pietra, S. A., & Mercer, R. L. (1993). mathematics
statistical machine translation: Parameter estimation. Computational Linguistics, 19(2),
263312.
Bunescu, R. C., & Mooney, R. J. (2005). Subsequence kernels relation extraction. Weiss,
Y., Scholkopf, B., & Platt, J. (Eds.), Advances Neural Information Processing Systems 19
(NIPS 2006) Vancouver, BC.
Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded language acquisition. Proceedings 25th International Conference Machine Learning (ICML-2008)
Helsinki, Finland.
431

fiC HEN , K IM , & OONEY

Chen, M., Foroughi, E., Heintz, F., Kapetanakis, S., Kostiadis, K., Kummeneje, J., Noda, I., Obst,
O., Riley, P., Steffens, T., Wang, Y., & Yin, X. (2003). Users manual: RoboCup soccer server
manual soccer server version 7.07 later.. Available http://sourceforge.
net/projects/sserver/.
Chiang, D. (2005). hierarchical phrase-based model statistical machine translation. Proceedings 43nd Annual Meeting Association Computational Linguistics (ACL05), pp. 263270 Ann Arbor, MI.
Collins, M. (2002). New ranking algorithms parsing tagging: Kernels discrete structures, voted perceptron. Proceedings 40th Annual Meeting Association
Computational Linguistics (ACL-2002), pp. 263270 Philadelphia, PA.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete data
via EM algorithm. Journal Royal Statistical Society B, 39, 138.
Doddington, G. (2002). Automatic evaluation machine translation quality using n-gram cooccurrence statistics. Proceedings ARPA Workshop Human Language Technology,
pp. 128132 San Diego, CA.
Duboue, P. A., & McKeown, K. R. (2003). Statistical acquisition content selection rules
natural language generation. Proceedings 2003 Conference Empirical Methods
Natural Language Processing (EMNLP-03), pp. 121128.
Fleischman, M., & Roy, D. (2007). Situated models meaning sports video retrieval. Proceedings Human Language Technologies: Conference North American Chapter
Association Computational Linguistics (NAACL-HLT-07) Rochester, NY.
Ge, R., & Mooney, R. J. (2005). statistical semantic parser integrates syntax semantics.
Proceedings Ninth Conference Computational Natural Language Learning (CoNLL2005), pp. 916 Ann Arbor, MI.
Ge, R., & Mooney, R. J. (2009). Learning compositional semantic parser using existing syntactic parser. Proceedings Joint conference 47th Annual Meeting Association Computational Linguistics 4th International Joint Conference Natural
Language Processing Asian Federation Natural Language Processin (ACL-IJCNLP
2009).
Gold, K., & Scassellati, B. (2007). robot uses existing vocabulary infer non-visual word
meanings observation. Proceedings Twenty-Second Conference Artificial
Intelligence (AAAI-07).
Gorniak, P., & Roy, D. (2005). Speaking sidekick: Understanding situated speech
computer role playing games. Proceedings 4th Conference Artificial Intelligence
Interactive Digital Entertainment Stanford, CA.
Gupta, S., & Mooney, R. (2009). Using closed captions train activity recognizers improve
video retrieval. Proceedings CVPR-09 Workshop Visual Contextual Learning
Annotated Images Videos (VCL) Miami, FL.
432

fiT RAINING ULTILINGUAL PORTSCASTER

Harnad, S. (1990). symbol grounding problem. Physica D, 42, 335346.
Herzog, G., & Wazinski, P. (1994). VIsual TRAnslator: Linking perceptions natural language
descriptions. Artificial Intelligence Review, 8(2/3), 175187.
Ide, N. A., & Jeronis, J. (1998). Introduction special issue word sense disambiguation:
state art. Computational Linguistics, 24(1), 140.
Joachims, T. (1998). Text categorization support vector machines: Learning many relevant
features. Proceedings Tenth European Conference Machine Learning (ECML98), pp. 137142 Berlin. Springer-Verlag.
Jurcicek, J., Gasic, M., Keizer, S., Mairesse, F., Thomson, B., & Young, S. (2009). Transformationbased learning semantic parsing. Interspeech Brighton, UK.
Kate, R. J., & Mooney, R. J. (2006). Using string-kernels learning semantic parsers. Proceedings 21st International Conference Computational Linguistics 44th Annual
Meeting Association Computational Linguistics (COLING/ACL-06), pp. 913920
Sydney, Australia.
Kate, R. J., & Mooney, R. J. (2007). Learning language semantics ambiguous supervision.
Proceedings Twenty-Second Conference Artificial Intelligence (AAAI-07), pp.
895900 Vancouver, Canada.
Kerr, W., Cohen, P. R., & Chang, Y.-H. (2008). Learning playing wubble world. Proceedings Fourth Artificial Intelligence Interactive Digital Entertainment Conference
(AIIDE) Palo Alto, CA.
Kingsbury, P., Palmer, M., & Marcus, M. (2002). Adding semantic annotation Penn treebank.
Proceedings Human Language Technology Conference San Diego, CA.
Knight, K., & Hatzivassiloglou, V. (1995). Two-level, many-paths generation. Proceedings
33rd Annual Meeting Association Computational Linguistics (ACL-95), pp.
252260 Cambridge, MA.
Lau, T., Drews, C., & Nichols, J. (2009). Interpreting written how-to instructions. Proceedings
Twenty-first International Joint Conference Artificial Intelligence (IJCAI-2009).
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming: Techniques Applications.
Ellis Horwood.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondences less supervision. Proceedings Joint conference 47th Annual Meeting Association
Computational Linguistics 4th International Joint Conference Natural Language
Processing Asian Federation Natural Language Processin (ACL-IJCNLP 2009).
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N., & Watkins, C. (2002). Text classification
using string kernels. Journal Machine Learning Research, 2, 419444.
433

fiC HEN , K IM , & OONEY

Lu, W., Ng, H. T., Lee, W. S., & Zettlemoyer, L. S. (2008). generative model parsing natural
language meaning representations. Proceedings 2008 Conference Empirical
Methods Natural Language Processing (EMNLP-08) Honolulu, HI.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotated corpus
English: Penn treebank. Computational Linguistics, 19(2), 313330.
McKeown, K. R. (1985). Discourse strategies generating natural-language text. Artificial Intelligence, 27(1), 141.
Meza-Ruiz, I. V., Riedel, S., & Lemon, O. (2008). Spoken language understanding dialogue
systems, using 2-layer Markov logic network: improving semantic accuracy. Proceedings
Londial.
Mooney, R. J. (2007). Learning semantic parsing. Gelbukh, A. (Ed.), Computational Linguistics Intelligent Text Processing: Proceedings 8th International Conference,
CICLing 2007, Mexico City, pp. 311324. Springer Verlag, Berlin.
Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment models.
Computational Linguistics, 29(1), 1951.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automatic evaluation
machine translation. Proceedings 40th Annual Meeting Association
Computational Linguistics (ACL-2002), pp. 311318 Philadelphia, PA.
Riezler, S., Prescher, D., Kuhn, J., & Johnson, M. (2000). Lexicalized stochastic modeling
constraint-based grammars using log-linear measures EM training. Proceedings
38th Annual Meeting Association Computational Linguistics (ACL-2000), pp.
480487 Hong Kong.
Roy, D. (2002). Learning visually grounded words syntax scene description task. Computer Speech Language, 16(3), 353385.
Rozinat, A., Zickler, S., Veloso, M., van der Aalst, W., & McMillen, C. (2008). Analyzing multiagent activity logs using process mining techniques. Proceedings 9th International
Symposium Distributed Autonomous Robotic Systems (DARS-08) Tsukuba, Japan.
Satoh, S., Nakamura, Y., & Kanade, T. (1997). Name-it: Naming detecting faces video
integration image natural language processing. Proceedings Fifteenth
International Joint Conference Artificial Intelligence (IJCAI-97).
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods Pattern Analysis. Cambridge University Press.
Shieber, S. M. (1988). uniform architecture parsing generation. Proceedings
12th International Conference Computational Linguistics (COLING-88), pp. 614619 Budapest, Hungary.
Siskind, J. M. (1996). computational study cross-situational techniques learning word-tomeaning mappings. Cognition, 61(1), 3991.
434

fiT RAINING ULTILINGUAL PORTSCASTER

Snyder, B., & Barzilay, R. (2007). Database-text alignment via structured multilabel classification. Proceedings Twentieth International Joint Conference Artificial Intelligence
(IJCAI-2007).
Srihari, R. K., & Burhans, D. T. (1994). Visual semantics: Extracting visual information
text accompanying pictures. Proceedings Twelfth National Conference Artificial
Intelligence (AAAI-94).
Stolcke, A. (1995). efficient probabilistic context-free parsing algorithm computes prefix
probabilities. Computational Linguistics, 21(2), 165201.
Whorf, B. L. (1964). Language, Thought, Reality: Selected Writings. MIT Press.
Wong, Y., & Mooney, R. J. (2006). Learning semantic parsing statistical machine translation. Proceedings Human Language Technology Conference / North American Chapter
Association Computational Linguistics Annual Meeting (HLT-NAACL-06), pp. 439
446 New York City, NY.
Wong, Y., & Mooney, R. J. (2007). Generation inverting semantic parser uses statistical
machine translation. Proceedings Human Language Technologies: Conference
North American Chapter Association Computational Linguistics (NAACL-HLT07), pp. 172179 Rochester, NY.
Yamada, K., & Knight, K. (2001). syntax-based statistical translation model. Proceedings
39th Annual Meeting Association Computational Linguistics (ACL-2001), pp.
523530 Toulouse, France.
Yu, C., & Ballard, D. H. (2004). integration grounding language learning objects.
Proceedings Nineteenth National Conference Artificial Intelligence (AAAI-04), pp.
488493.
Zaragoza, H., & Li, C.-H. (2005). Learning talk descriptive games. Proceedings
Human Language Technology Conference Conference Empirical Methods
Natural Language Processing (HLT/EMNLP-05), pp. 291298 Vancouver, Canada.
Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods relation extraction. Journal
Machine Learning Research, 3, 10831106.
Zettlemoyer, L. S., & Collins, M. (2005). Learning map sentences logical form: Structured
classification probabilistic categorial grammars. Proceedings 21st Conference
Uncertainty Artificial Intelligence (UAI-2005) Edinburgh, Scotland.
Zettlemoyer, L. S., & Collins, M. (2007). Online learning relaxed CCG grammars parsing
logical form. Proceedings 2007 Joint Conference Empirical Methods Natural
Language Processing Computational Natural Language Learning (EMNLP-CoNLL-07),
pp. 678687 Prague, Czech Republic.

435

fiJournal Artificial Intelligence Research 37 (2010) 247-277

Submitted 08/09; published 03/10

Context-Based Word Acquisition Situated Dialogue
Virtual World

Shaolin Qu
Joyce Y. Chai

qushaoli@cse.msu.edu
jchai@cse.msu.edu

Department Computer Science Engineering
Michigan State University
East Lansing, MI 48824 USA

Abstract
tackle vocabulary problem conversational systems, previous work applied
unsupervised learning approaches co-occurring speech eye gaze interaction
automatically acquire new words. Although approaches shown promise, several
issues related human language behavior human-machine conversation
addressed. First, psycholinguistic studies shown certain temporal regularities
human eye movement language production. regularities potentially
guide acquisition process, incorporated previous unsupervised approaches. Second, conversational systems generally existing knowledge
base domain vocabulary. existing knowledge potentially
help bootstrap constrain acquired new words, incorporated
previous models. Third, eye gaze could serve different functions human-machine conversation. gaze streams may closely coupled speech stream, thus
potentially detrimental word acquisition. Automated recognition closely-coupled
speech-gaze streams based conversation context important. address issues,
developed new approaches incorporate user language behavior, domain knowledge,
conversation context word acquisition. evaluated approaches context situated dialogue virtual world. experimental results shown
incorporating three types contextual information significantly improves word
acquisition performance.

1. Introduction
One major bottleneck human machine conversation robust language interpretation.
encountered vocabulary outside systems knowledge, system tends
fail. conversational interfaces become increasingly important many applications remote interaction robots (Lemon, Gruenstein, & Peters, 2002; Fong
& Nourbakhsh, 2005) automated training education (Traum & Rickel, 2002),
ability automatically acquire new words online conversation becomes essential.
Different traditional telephony-based spoken dialogue systems, conversational interfaces users look graphic display virtual world interacting artificial
agents using natural speech. unique setting provides opportunity automated
vocabulary acquisition. interaction, users visual perception (e.g., indicated
eye gaze) provides potential channel system automatically learn new words.
c
2010
AI Access Foundation. rights reserved.

fiQu & Chai

idea, shown previous work (Yu & Ballard, 2004; Liu, Chai, & Jin, 2007),
parallel data visual perception spoken utterances used unsupervised
approaches automatically identify mappings words visual entities
thus acquire new words. previous approaches provide promising direction,
mainly rely co-occurrence words visual entities completely unsupervised manner. However, human machine conversation, different types
extra information related human language behaviors characteristics conversation
systems. Although extra information potentially provide supervision guide
word acquisition process improve performance, systematically explored
previous work.
First, large body psycholinguistic studies shown eye gaze tightly linked
human language processing. evident language comprehension (Tanenhaus, Spivey-Knowiton, Eberhard, & Sedivy, 1995; Eberhard, Spivey-Knowiton, Sedivy, &
Tanenhaus, 1995; Allopenna, Magnuson, & Tanenhaus, 1998; Dahan & Tanenhaus, 2005;
Spivey, Tanenhaus, Eberhard, & Sedivy, 2002) language production (Meyer, Sleiderink,
& Levelt, 1998; Rayner, 1998; Griffin & Bock, 2000; Bock, Irwin, Davidson, & Leveltb, 2003;
Brown-Schmidt & Tanenhaus, 2006; Griffin, 2001). Specifically human language production, directly relevant automated computer interpretation human language,
studies found significant temporal regularities mentioned objects
corresponding words (Meyer et al., 1998; Rayner, 1998; Griffin & Bock, 2000). object
naming tasks, onset word begins approximately one second speaker
looked corresponding visual referent (Griffin, 2004), gazes longer
difficult name referent retrieve (Meyer et al., 1998; Griffin, 2001).
100-300 ms articulation object name begins, eyes move next object relevant task (Meyer et al., 1998). findings suggest eyes move
mentioned objects corresponding words uttered. Although language
behavior used constrain mapping words visual objects,
incorporated previous approaches.
Second, practical conversational systems always existing knowledge application domains vocabularies. knowledge base acquired development
time: either authored domain experts automatically learned available data.
Although existing knowledge rather limited desired enhanced automatically online (e.g., automated vocabulary acquisition), provides important
information structure domain existing vocabularies,
bootstrap constrain new word acquisition. type domain knowledge
utilized previous approaches.
Third, although psycholinguistic studies provide us sound empirical basis
assuming eye movements predictive speech, gaze behavior interactive
setting much complex. different types eye movements (Kahneman,
1973). naturally occurring eye gaze speech production may serve different
functions, example, engage conversation manage turn taking (Nakano,
Reinstein, Stocky, & Cassell, 2003). Furthermore, interacting graphic display,
user could talking objects previously seen display something
completely unrelated object user looking at. Therefore using speechgaze pairs word acquisition detrimental. type gaze mostly useful
248

fiContext-Based Word Acquisition Situated Dialogue Virtual World

word acquisition kind reflects underlying attention tightly links
content co-occurring speech. Thus, automatically recognizing closely coupled speech
gaze streams online conversation word acquisition important. However,
examined previous work.
address three issues, developed new approaches automatic word
acquisition (1) incorporate findings user language behavior psycholinguistic
studies, particular temporal alignment spoken words eye gaze; (2) utilize
existing domain knowledge, (3) automatically identify closely-coupled speech
gaze streams based conversation context. evaluated approaches context
situated dialogue virtual world. experimental results shown incorporating
three types contextual information significantly improves word acquisition
performance. simulation studies demonstrate effect automatic online
word acquisition improving language understanding human-machine conversation.
following sections, first introduce domain data collection investigation. describe enhanced models word acquisition incorporate
additional contextual information (e.g., language behavior spoken words eye
gaze, domain knowledge, conversation context). Finally, present empirical evaluation
enhanced models demonstrate effect online word acquisition spoken
language understanding human-machine conversation.

2. Related Work
work motivated previous work grounded language acquisition eye gaze
multimodal human-computer interaction.
Grounded language acquisition learn meaning language connecting
language perception world. Language acquisition grounding words
visual perceptions objects studied various language learning systems.
example, given speech paired video images single objects, mutual information audio visual signals used learn words associating acoustic phoneme
sequences visual prototypes (e.g., color, size, shape) objects (Roy & Pentland,
2002; Roy, 2002). Generative models developed learn words associating words
image regions given parallel data pictures description text (Barnard, Duygulu,
de Freitas, Forsyth, Blei, & Jordan, 2003). Given pairs spoken instructions containing
object names corresponding objects, utterance-object joint probability model
used learn object names identifying object name phonemes associating
objects (Taguchi, Iwahashi, Nose, Funakoshi, & Nakano, 2009). Given sequences
utterances paired scene representations, incremental translation model developed learn word meaning associating words semantic representations
referents scene (Fazly, Alishahi, & Stevenson, 2008). addition grounding
individual words, previous work also investigated grounding phrases (referring expressions) visual objects semantic decomposition, example using context free
grammar connects linguistic structures underlying visual properties (Gorniak &
Roy, 2004).
Besides visual objects, approaches also developed ground words meaning representations events. example, event logic applied ground verbs
249

fiQu & Chai

motion events represented force dynamics encoding support, contact,
attachment relations objects video images (Siskind, 2001). video game
domain, translation model used ground words semantic roles user actions (Fleischman & Roy, 2005). simulated Robocup soccer domain, given textual game
commentaries paired symbolic descriptions game events, approaches based
statistical parsing learning developed ground commentary text game
events (Chen & Mooney, 2008). less restricted data setting, generative models
developed simultaneously segment text utterances map utterances
meaning representations event states (Liang, Jordan, & Klein, 2009). Different
previous work, work, visual perception indicated eye gaze. Eye
gaze, one hand, indicative human attention, provides opportunities link
language perception; hand, implicit subconscious input,
could bring additional challenge word acquisition.
Eye gaze long explored human-computer interaction direct manipulation interfaces pointing device (Jacob, 1991; Wang, 1995; Zhai, Morimoto, & Ihde,
1999). Eye gaze modality multimodal interaction goes beyond function
pointing. different speech eye gaze systems, eye gaze explored
purpose mutual disambiguation (Tanaka, 1999; Zhang, Imamiya, Go, & Mao, 2004),
complement speech channel reference resolution (Campana, Baldridge, Dowding,
Hockey, Remington, & Stone, 2001; Kaur, Termaine, Huang, Wilder, Gacovski, Flippo,
& Mantravadi, 2003; Prasov & Chai, 2008; Byron, Mampilly, Sharma, & Xu, 2005)
speech recognition (Cooke, 2006; Qu & Chai, 2007), managing human-computer
dialogue (Qvarfordt & Zhai, 2005).
Eye gaze explored recently word acquisition. example, Yu Ballard
(2004) proposed embodied multimodal learning interface word acquisition, especially
eye movement. work, given speech paired eye gaze information
video images, translation model applied acquire words associating acoustic
phone sequences visual representations objects actions. work inspired
research mostly related effort here. difference work
work Yu Ballard lies two aspects. First, learning environment
different. Yu Ballard focuses narrative descriptions actions (e.g.,
making sandwich, pouring drinks, etc.) human subjects, focus
interactive conversation. conversation, human participant take speaker
role addressee role. represents new scenario word acquisition based
eye movement may new implications. Second, work Yu Ballard,
IBM Translation Model 1 applied word acquisition. work,
incorporate types information user language behavior, domain knowledge,
conversation context translation models.
previous work, experimented application IBM Translation Model
1 vocabulary acquisition gaze modeling conversation setting (Liu et al.,
2007). reported initial investigation incorporating temporal information
domain knowledge translation models (Qu & Chai, 2008) well automatically
identifying closely coupled speech gaze streams (Qu & Chai, 2009). paper extends
previous work provides comprehensive evaluation incorporating knowledge
interactivity word acquisition much richer application domain. examine
250

fiContext-Based Word Acquisition Situated Dialogue Virtual World

word acquisition affected automated speech recognition effect
online word acquisition language understanding human-machine conversation.

3. Domain Data
facilitate work word acquisition, collected data based situated dialogue
virtual world. data set different data set used previous investigation (Qu & Chai, 2008). difference lies two aspects: 1) dataset
collected mixed initiative human-machine conversation whereas data previous investigation based question answering; 2) user studies work
conducted context situated dialogue, human users immersed
complex virtual world move around virtual environment.
3.1 Domain

Figure 1: Treasure hunting domain
application virtual world treasure hunting. human user immersed
virtual world (i.e., castle) 1 able move around look hidden treasures.
castle contains 115 3D objects. user needs consult remote expert (i.e.,
artificial agent) advice finding treasures. expert knowledge
environment treasures cannot see user castle.
user needs describe expert sees order get advice
go look for.
Figure 1 shows snapshot virtual world. Figure 2 shows excerpt dialog
user expert finding one treasures. focus investigation
word acquisition, conducted wizard-of-oz studies collect data 2 . specifically, set
response templates corresponding different types responses predefined.
1. virtual world developed based Irrlicht game engine http://irrlicht.sourceforge.net/.
2. WOZ studies allow observation user interacting fully functional system,
system functions supplemented hidden human wizard. Users led believe
interacting real system (Klemmer, Sinha, Chen, Landay, Aboobaker, & Wang, 2000).
technology, permits experimentally controlled study human computer interaction behavior
without exposing users unpredicted responses, used widely earlier research speech
interfaces multimodal systems.

251

fiQu & Chai

S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:

Tell see.
see golden tomb sarcophagus.
Describe it.
appears made gold Egyptian decorations it.
else see?
see carving hanging wall appears African.
Go back.
Im back sarcophagus. appears lava surrounding fire.
couldnt understand you.
room sarcophagus surrounded lava fire.
see statue?
try open sarcophagus?
No.
see statue.
Describe.
appears stone statue Egyptian.
else see?
see small vase pottery vase decorated flowers.
treasure near.
try move vase?
Yes.
cant move vase.
see another one?
Yes, theres wooden table pottery jug purple vase it. try move
purple vase?
see statue?
Yes, see stone statue.
see another one?
Yes, see gold black statue Egyptian sitting.
Compare objects.
stone statue bad shape, golden statue appears good shape.
golden crown. sitting wooden chair.
Try moving it.
move head statue see scroll. take scroll?
Describe.
white scroll red seal.
Take it.
took scroll.
Good.

Figure 2: dialog user system finding one treasures

system turn, system automatically determines kind responses (i.e., template)
provided; example, whether ask clarification, provide confirmation,
ask information particular object. decision made based
distribution simulate different types system acceptance rejection. distribution
modified according different needs experiments. Based chosen
template, human wizard serves language understanding component fills
template specific information related user input. filled template
252

fiContext-Based Word Acquisition Situated Dialogue Virtual World

used automatically generate natural language processed Microsoft
Text-to-Speech engine generate speech responses. experiments, users
speech recorded, users eye gaze captured Tobii eye tracker.
3.2 Data Preprocessing
20 users experiments, collected 3709 utterances accompanying gaze fixations.
transcribed collected speech. vocabulary size speech transcript 1082,
among 757 nouns/adjectives. users speech also automatically recognized
online Microsoft speech recognizer word error rate (WER) 48.1%
1-best recognition. vocabulary size 1-best speech recognition 3041, among
1643 nouns/adjectives. nouns adjectives transcriptions
recognized 1-best hypotheses automatically identified Stanford Part-of-Speech
Tagger (Toutanova & Manning, 2000; Toutanova, Klein, Manning, & Singer, 2003).
Theres



purple

vase





orange

face
speech str eam

gaze fixation

ts
[table_vase] [vase_purple]

gaze str eam

te

[vase_greek3] [vase_greek3]

[vase_greek3]

[vase_greek3]

[fixated entity]

Figure 3: Accompanying gaze fixations 1-best recognition users utterance
Theres purple vase orange vase. (There two incorrectly recognized
words face 1-best recognition)

collected speech gaze streams automatically paired together system.
time system detected sentence boundary users speech, paired
recognized speech gaze fixations system accumulating since
previously detected sentence boundary. Figure 3 shows stream pair users speech
accompanying gaze fixations. speech stream, spoken word timestamped
speech recognizer. gaze stream, gaze fixation starting timestamp
ts ending timestamp te provided eye tracker. gaze fixation results
fixated entity (3D object). multiple entities fixated one gaze fixation due
overlapping entities, foremost one chosen. gaze stream, neighboring
gaze fixations fixate entity merged.
Given paired speech gaze streams, build set parallel word sequences
gaze fixated entity sequences {(w, e)} task word acquisition. word sequence
w consists nouns adjectives 1-best recognition spoken utterance.
entity sequence e contains entities fixated gaze fixations. parallel
speech gaze streams shown Figure 3, resulting word sequence w = [purple vase
orange face] resulting entity sequence e = [table vase vase purple vase greek3 ].
253

fiQu & Chai

4. Translation Models Word Acquisition
Since working conversational systems users interact visual scene,
consider task word acquisition associating words visual entities
domain. Given parallel speech gaze fixated entities {(w, e)}, formulate word
acquisition translation problem use translation models estimate word-entity
association probabilities p(w|e). words highest association probabilities
chosen acquired words entity e.
4.1 Base Model
Using translation model (Brown, Pietra, Pietra, & Mercer, 1993), word
equally likely aligned entity,
X
l

1
p(w|e) =
p(wj |ei )
(l + 1)m

(1)

j=1 i=0

l lengths entity word sequences respectively. refer
model Model-1.
4.2 Base Model II
Using translation model II (Brown et al., 1993), alignments dependent
word/entity positions word/entity sequence lengths,
p(w|e) =

X
l


p(aj = i|j, m, l)p(wj |ei )

(2)

j=1 i=0

aj = means wj aligned ei . aj = 0, wj aligned
entity (e0 represents null entity). refer model Model-2.
Compared Model-1, Model-2 considers ordering words entities word
acquisition. EM algorithms used estimate probabilities p(w|e) translation
models.

5. Incorporating User Language Behavior Word Acquisition
Model-2, word-entity alignments estimated co-occurring word entity sequences unsupervised way. estimated alignments dependent
words/entities appear word/entity sequences, words gaze
fixated entities actually occur. Motivated findings users move eyes
mentioned object directly speaking word (Griffin & Bock, 2000), make
word-entity alignments dependent temporal relation new model (referred
Model-2t):
X
l

p(w|e) =
pt (aj = i|j, e, w)p(wj |ei )
(3)
j=1 i=0

254

fiContext-Based Word Acquisition Situated Dialogue Virtual World

pt (aj = i|j, e, w) temporal alignment probability computed based temporal distance entity ei word wj .
define temporal distance ei wj

ts (ei ) ts (wj ) te (ei )
0
te (ei ) ts (wj ) ts (wj ) > te (ei )
d(ei , wj ) =
(4)

ts (ei ) ts (wj ) ts (wj ) < ts (ei )
ts (wj ) starting timestamp (ms) word wj , ts (ei ) te (ei ) starting
ending timestamps (ms) gaze fixation entity ei .
alignment word wj entity ei decided temporal distance d(ei , wj ).
Based psycholinguistic finding eye gaze happens spoken word, wj
allowed aligned ei wj happens earlier ei (i.e., d(ei , wj ) > 0). wj
happens earlier ei (i.e., d(ei , wj ) 0), closer are, likely
aligned. Specifically, temporal alignment probability wj ei co-occurring
instance (w, e) computed

0
d(ei , wj ) > 0



exp[ d(ei , wj )]
d(ei , wj ) 0
l
pt (aj = i|j, e, w) =
(5)
X


exp[ d(ei , wj )]


i=0

constant scaling d(ei , wj ).
EM algorithm used estimate probabilities p(w|e) Model-2t.
worthwhile mention that, findings psycholinguistic studies provided specific
offsets terms eye gaze corresponds speech production. example, shows
speakers look object second say it, 100-300 ms
articulation object name begins, eyes move next object relevant
task (Meyer et al., 1998). Since conversation setting study much
complex simple settings psycholinguistic research, found larger variations
offset (Liu et al., 2007) data. Therefore chose use offset
alignment model here.

6. Incorporating Domain Knowledge Word Acquisition
Speech-gaze temporal alignment occurrence statistics sometimes sufficient
associate words entities correctly. example, suppose user says lamp
dresser looking lamp object table object. Due co-occurrence
lamp object, words dresser lamp likely associated lamp
object translation models. result, word dresser likely incorrectly
acquired lamp object. reason, word lamp could acquired
incorrectly table object. solve type association problem, semantic
knowledge domain words helpful. example, knowledge
word lamp semantically related object lamp help system avoid
associating word dresser lamp object. Specifically, solve type wordentity association utilizing domain knowledge present system external
lexical semantic resources.
255

fiQu & Chai

one hand, conversational system domain model, knowledge
representation domain types objects properties relations, task structures, etc. domain model usually acquired development
stage deployment system. domain model provides important resource enable domain reasoning language interpretation (DeVault & Stone, 2003).
hand, available resources domain independent lexical knowledge (e.g., WordNet, see Fellbaum, 1998). idea domain model
linked external lexical resources either manually automatically development
state, external knowledge source used help constrain acquired words.
following sections, first describe domain modeling, define semantic
relatedness word entity based domain modeling WordNet semantic lexicon,
finally describe different ways using semantic relatedness word entity
help word acquisition.
6.1 Domain Modeling
model treasure hunting domain shown Figure 4. domain model contains
domain related semantic concepts. practical conversational systems, domain
modeling typically acquired development stage either manual authoring
domain experts automated learning based annotated data. current work,
properties domain entities represented domain concepts. entity properties include: semantic type, color, size, shape, material. use WordNet synsets
represent domain concepts (i.e., synsets format word#part-of-speech#senseid). sense-id represents specific WordNet sense associated word
representing concept. example, domain concepts SEM PLATE COLOR
entity plate represented synsets plate#n#4 color#n#1 WordNet.
Note link domain concepts WordNet synsets automatically acquired given existing vocabularies. Here, illustrate idea, simplify problem
directly connect domain concepts synsets.
Note domain model, domain concepts specific certain entity,
general concepts certain type entity. Multiple entities type
properties share set domain concepts. Therefore, properties
color size entity general concepts color#n#1 size#n#1
instead specific concepts like yellow#a#1 big#a#1, concepts
shared entities type, different colors sizes.
6.2 Semantic Relatedness Word Entity
compute semantic relatedness word w entity e based semantic
similarity w properties e using domain model bridge. Specifically,
semantic relatedness SR(e, w) defined
SR(e, w) = max sim(s(cie ), sj (w))
i,j

(6)

cie i-th property entity e, s(cie ) synset property cie domain model,
sj (w) j-th synset word w defined WordNet, sim(, ) similarity
score two synsets.
256

fiContext-Based Word Acquisition Situated Dialogue Virtual World

n e l

Entities:

Domain
concepts:

pharaoh

plate

SEM_PLATE

COLOR

SEM_PHARAOH

COLOR

SIZE

color#n#1

WordNet
concepts:

pharaoh#n#1

plate#n#4

picture#n#2

size#n#1

Figure 4: Domain model domain concepts represented WordNet synsets

computed similarity score two synsets based path length them.
similarity score inversely proportional number nodes along shortest path
synsets defined WordNet. two synsets same,
maximal similarity score 1. WordNet-Similarity tool (Pedersen, Patwardhan, &
Michelizzi, 2004) used synset similarity computation.
6.3 Word Acquisition Word-Entity Semantic Relatedness
use semantic relatedness word entity help system acquire semantically compatible words entity, therefore improve word acquisition performance.
semantic relatedness applied word acquisition two ways: post-process
learned word-entity association probabilities rescoring semantic relatedness,
directly affect learning word-entity associations constraining alignment
word entity translation models.
6.3.1 Rescoring Semantic Relatedness
acquired word list entity ei , word wj association probability p(wj |ei )
learned translation model. use semantic relatedness SR(ei , wj )
redistribute probability mass wj . new association probability given by:

p(wj |ei )SR(ei , wj )
p0 (wj |ei ) = X
p(wj |ei )SR(ei , wj )
j

257

(7)

fiQu & Chai

6.3.2 Semantic Alignment Constraint Translation Model
used constrain word-entity alignment translation model, semantic relatedness used alone used together speech-gaze temporal information decide
alignment probability word entity (Qu & Chai, 2008).
Using semantic relatedness constrain word-entity alignments Model-2s,

X
l

p(w|e) =
ps (aj = i|j, e, w)p(wj |ei )
(8)
j=1 i=0

ps (aj = i|j, e, w) alignment probability based semantic relatedness,
SR(ei , wj )
ps (aj = i|j, e, w) = X
SR(ei , wj )

(9)



Using semantic relatedness temporal information constrain word-entity alignments Model-2ts,
p(w|e) =

X
l


pts (aj = i|j, e, w)p(wj |ei )

(10)

j=1 i=0

pts (aj = i|j, e, w) alignment probability decided temporal
relation semantic relatedness ei wj ,
ps (aj = i|j, e, w)pt (aj = i|j, e, w)
pts (aj = i|j, e, w) = X
ps (aj = i|j, e, w)pt (aj = i|j, e, w)

(11)



ps (aj = i|j, e, w) semantic alignment probability Equation (9),
pt (aj = i|j, e, w) temporal alignment probability given Equation (5).
EM algorithms used estimate p(w|e) Model-2s Model-2ts.

7. Incorporating Conversation Context Word Acquisition
mentioned earlier, speech-gaze pairs useful word acquisition. speechgaze pair, speech word relates gaze fixated
entities, instance adds noise word acquisition. Therefore, identify
closely coupled speech-gaze pairs use word acquisition.
section, first describe feature extraction based conversation interactivity, describe use logistic regression classifier predict whether speech-gaze
pair closely coupled speech-gaze instance instance least one noun
adjective speech stream referring gaze fixated entity gaze stream.
training classifier speech-gaze prediction, manually labeled instance whether closely coupled speech-gaze instance based speech transcript
gaze fixations.
258

fiContext-Based Word Acquisition Situated Dialogue Virtual World

7.1 Features Extraction
parallel speech-gaze instance, following sets features automatically extracted.
7.1.1 Speech Features (S-Feat)
Let cw count nouns adjectives utterance, ls temporal length
speech. following features extracted speech:
cw count nouns adjectives.
nouns adjectives expected users utterance describing entities.
cw /ls normalized noun/adjective count.
effect speech length ls cw considered.
7.1.2 Gaze Features (G-Feat)
fixated entity ei , let lei fixation temporal length. Note several gaze
fixations may fixated entity, lei total length gaze fixations
fixate entity ei . extract following features gaze stream:
ce count different gaze fixated entities.
Less fixated entities expected user describing entities looking
them.
ce /ls normalized entity count.
effect speech temporal length ls ce considered.
maxi (lei ) maximal fixation length.
least one fixated entitys fixation expected long enough user
describing entities looking them.
mean(lei ) average fixation length.
average gaze fixation length expected longer user describing
entities looking them.
var(lei ) variance fixation lengths.
variance fixation lengths expected smaller user describing entities looking them.
number gaze fixated entities decided users eye gaze, also
affected visual scene. Let cse count entities visible
length gaze stream. also extract following scene related feature:
ce /cse scene normalized fixated entity count.
effect visual scene ce considered.
259

fiQu & Chai

7.1.3 User Activity Features (UA-Feat)
interacting system, users activity also helpful determining
whether users eye gaze tightly linked content speech. following
features extracted users activities:
maximal distance users movements maximal change user position (3D
coordinates) speech length.
user expected move within smaller range looking entities
describing them.
variance users positions
user expected move less frequently looking entities describing
them.
7.1.4 Conversation Context Features (CC-Feat)
talking system (i.e., expert), users language gaze behavior
influenced state conversation. speech-gaze instance, use
previous system response type nominal feature predict whether closely
coupled speech-gaze instance.
treasure hunting domain, eight types system responses two categories:
System-Initiative Responses:
specific-see system asks whether user sees certain entity, e.g., see
another couch?.
nonspecific-see system asks whether user sees anything, e.g., see
anything else?, Tell see.
previous-see system asks whether user previously sees something, e.g.,
previously seen similar object?.
describe system asks user describe detail user sees, e.g.,
Describe it, Tell it.
compare system asks user compare user sees, e.g., Compare
objects.
clarify system asks user make clarification, e.g., understand
that, Please repeat that.
action-request system asks user take action, e.g., Go back, Try moving
it.
User-Initiative Responses:
misc system hands initiative back user without specifying
requirements, e.g., dont know, Yes.
260

fiContext-Based Word Acquisition Situated Dialogue Virtual World

7.2 Logistic Regression Model
Given extracted feature x closely coupled label instance
training set, train ridge logistic regression model (Cessie & Houwelingen, 1992)
predict whether instance closely coupled instance (y = 1) (y = 0).
logistic regression model, probability yi = 1, given feature xi =
(xi1 , xi2 , . . . , xim ), modeled
P

exp(
j=1 j xj )
P
p(yi |xi ) =

1 + exp(
j=1 j xj )
j features weights learned.
log-likelihood l data (X, y)
X
l() =
[yi log p(yi |xi ) + (1 yi ) log(1 p(yi |xi ))]


ridge logistic regression, parameters j estimated maximizing regularized loglikelihood
l () = l() ||||2
ridge parameter introduced achieve stable parameter estimation.
7.3 Evaluation Speech-gaze Identification
Since goal identifying closely coupled speech-gaze instances improve word acquisition interested acquiring nouns adjectives, instances
recognized nouns/adjectives used training logistic regression classifier. Among
2969 instances recognized nouns/adjectives gaze fixations, 2002 (67.4%) instances labeled closely coupled. speech-gaze prediction evaluated 10-fold
cross validation.
Table 1 shows prediction precision recall different sets features used.
seen table, features used, prediction precision goes
recall goes down. important note prediction precision critical
recall word acquisition sufficient amount data available. Noisy instances
gaze link speech content hurt word acquisition since
guide translation models ground words wrong entities. Although
higher recall helpful, effect expected become less co-occurrences
already established.
results show speech features (S-Feat) conversation context features (CCFeat), used alone, improve prediction precision much compared baseline
predicting instances closely coupled precision 67.4%. used alone,
gaze features (G-Feat) user activity features (UA-Feat) two useful feature
sets increasing prediction precision. used together, prediction precision increased. Adding either speech features conversation context features
gaze user activity features (G-Feat + UA-Feat + S-Feat/CC-Feat) increases
prediction precision more. Using four sets features (G-Feat + UA-Feat + S-Feat +
261

fiQu & Chai

Feature sets
Null (baseline)
S-Feat
G-Feat
UA-Feat
CC-Feat
G-Feat + UA-Feat
G-Feat + UA-Feat + S-Feat
G-Feat + UA-Feat + CC-Feat
G-Feat + UA-Feat + S-Feat + CC-Feat

Precision
0.674
0.686
0.707
0.704
0.688
0.719
0.741
0.731
0.748

Recall
1
0.995
0.958
0.942
0.936
0.948
0.908
0.918
0.899

Table 1: Speech-gaze prediction performances different feature sets

CC-Feat) achieves highest prediction precision. McNemar tests shown
significant change compared using G-Feat + UA-Feat + S-Feat (2 = 8.3, p < 0.004)
feature configurations (2 = 45.4 442.7, p < 0.0001). Therefore, choose
use feature sets identify closely coupled speech-gaze instances word acquisition.

8. Evaluation Word Acquisition
practical conversational system starts initial knowledge base (vocabulary).
assume system already one default word entity default vocabulary.
default word entity indicates semantic type entity. example,
word barrel default word entity barrel. Among acquired words,
evaluate new words systems vocabulary. example, word
barrel would excluded candidate words acquired entity barrel.
8.1 Grounding Words Domain Concepts
Based translation models word acquisition (Sections 5 & 6), obtain
word-entity association probability p(w|e). probability provides means ground
words entities. conversational systems, one important goal word acquisition
make system understand semantic meaning new words. Word acquisition
grounding words objects always sufficient identifying semantic meanings.
Suppose word green grounded green chair object, word chair. Although
system aware green word describing green chair, know
word green refers chairs color word chair refers chairs
semantic type. Thus, learning word-entity associations p(w|e) translation
models, need ground words domain concepts entity properties.
Based domain model discussed earlier (Section 6.1), apply WordNet ground
words domain concepts. entity e, based association probabilities p(w|e),
choose n-best words acquired words e. n-best words n highest
association probabilities. word w acquired e, grounded concept ce w
262

fiContext-Based Word Acquisition Situated Dialogue Virtual World

chosen one highest semantic relatedness w:
ce = arg max[max sim(s(cie ), sj (w))]


j

(12)

sim(s(cie ), sj (w)) semantic similarity score defined Equation (6).
evaluate acquired words domain concepts, manually compile set
gold standard words users speech transcripts gaze fixations. gold
standard words words users used refer entities
properties (e.g., color, size, shape) interaction system. automatically acquired words evaluated gold standard words.
8.2 Evaluation Metrics
Following standard evaluation information retrieval, following metrics used
evaluate words acquired domain concepts (i.e., entity properties) {ce }.
Precision

P

ce

Recall

# words correctly acquired ce
P
ce # words acquired ce

P
ce # words correctly acquired ce
P
ce # gold standard words ce

Mean Average Precision (MAP)

MAP =

X PNw P (r) rel(r)
r=1
Ne
e
#e

Ne number gold standard words properties ce entity
e, Nw vocabulary size, P (r) acquisition precision given cut-off rank r,
rel(r) binary function indicating whether word rank r gold-standard
word property ce entity e.
8.3 Evaluation Results
investigate effects speech-gaze temporal information domain semantic knowledge word acquisition, compare word acquisition performances following
models:
Model-1 base model without word-entity alignment (Equation (1)).
Model-1-r Model-1 semantic relatedness rescoring word-entity association.
Model-2 base model II positional alignment (Equation (2)).
Model-2s enhanced model semantic alignment (Equation (8)).
Model-2t enhanced model temporal alignment (Equation (3)).
263

fiQu & Chai

Model-2ts enhanced model temporal semantic alignment (Equation (10)).
Model-2t-r Model-2t semantic relatedness rescoring word-entity association.
8.3.1 Results Using Speech-Gaze Temporal Information
1

0.4

Model-1
Model-2
Model-2t

0.9

0.38
Mean Average Precision

0.8

Precision

0.7
0.6
0.5
0.4
0.3
0.2

0.34
0.32
0.3
0.28
0.26

0.1
0

0.36

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.24
M-1

M-2

M-2t

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 5: Word acquisition performance speech-gaze temporal information used
Figure 5 shows interpolated precision-recall curves Mean Average Precisions
(MAPs) Model-2t baseline models Model-1 Model-2. shown figure,
Model-2 improve word acquisition compared Model-1. result shows
helpful consider index-based positional alignment word entity
word acquisition. incorporating speech-gaze temporal alignment, Model-2t consistently
achieves higher precisions Model-1 different recalls. terms MAP, Model-2t
significantly increases MAP (t = 3.08, p < 0.002) compared Model-1. means
use speech-gaze temporal alignment improves word acquisition.
8.3.2 Results Using Domain Semantic Relatedness
Figure 6 shows results using domain semantic relatedness word acquisition.
shown figure, compared baseline using extra knowledge (Model-1),
using domain semantic relatedness improves word acquisition matter used
rescore word-entity association (Model-1-r) constrain word-entity alignment (Model2s). Compared Model-1, MAP significantly improved Model-1-r (t = 6.32, p <
0.001) Model-2s (t = 5.36, p < 0.001).
Domain semantic relatedness also used together speech-gaze temporal information improve word acquisition. Compared Model-1, MAP significantly increased Model-2ts (t = 5.59, p < 0.001) uses semantic relatedness together temporal information constrain word-entity alignments Model-2t-r (t = 6.01, p < 0.001),
semantic relatedness used rescore word-entity associations learned Model2t.
264

fiContext-Based Word Acquisition Situated Dialogue Virtual World

1

0.4

Model-1
Model-1-r
Model-2s
Model-2ts
Model-2t-r

0.8

Precision

0.7

0.38
Mean Average Precision

0.9

0.6
0.5
0.4
0.3
0.2

0.34
0.32
0.3
0.28
0.26

0.1
0

0.36

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.24
M-1

M-1-r

M-2s

M-2ts

M-2t-r

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 6: Word acquisition performance domain semantic relatedness used
Comparing two ways using semantic relatedness word acquisition, found
rescoring word-entity association semantic relatedness works better. Model-2t-r
achieve higher MAP (t = 2.22, p < 0.015) Model-2ts.
also verified using speech-gaze temporal alignment domain semantic
relatedness rescoring works better using either one alone. temporal alignment
semantic relatedness rescoring, Model-2t-r significantly increases MAP compared
Model-1-r (t = 2.75, p < 0.004) semantic relatedness rescoring used
Model-2t (t = 5.38, p < 0.001) temporal alignment used.
8.3.3 Results Based Identified Closely Coupled Speech-Gaze Streams
shown Model-2t-r, speech-gaze temporal alignment domain
semantic relatedness rescoring incorporated, achieves best word acquisition performance. Therefore, Model-2t-r used evaluate word acquisition based
identified closely coupled speech-gaze data. Since Model-2t-r requires linking domain models external knowledge source (e.g., WordNet) may available
applications, also evaluate effect identification closely coupled speech-gaze
streams word acquisition Model-2t, speech-gaze temporal alignment
incorporated.
evaluate effect automatic identification closely coupled speech-gaze instances
word acquisition 10-fold cross validation. fold, 10% data set
used train logistic regression classifier predicting closely coupled speechgaze instances, instances, predicted closely coupled instances, true (manually
labeled) closely coupled instances 90% data set used word
acquisition respectively. Figures 7 & 8 show averaged interpolated precision-recall curves
MAPs achieved Model-2t Model-2t-r using instances (all ), predicted
closely coupled instances (predicted ), true closely coupled instances (true).
words acquired Model-2t, shown Figure 7, using predicted closely
coupled instances achieves better performance using instances. MAP significantly increased (t = 2.69, p < 0.005) acquiring words predicted closely coupled
265

fiQu & Chai

1

0.42


predicted
true

0.9

0.4
Mean Average Precision

0.8

Precision

0.7
0.6
0.5
0.4
0.3
0.2

0.38

0.36

0.34

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.32

1



predicted

true

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 7: Word acquisition performance Model-2t different data set

1

0.42


predicted
true

0.9

0.4
Mean Average Precision

0.8

Precision

0.7
0.6
0.5
0.4
0.3
0.2

0.38

0.36

0.34

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.32


predicted

true

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 8: Word acquisition performance Model-2t-r different data set

instances. result shows identification closely coupled speech-gaze instances
helps word acquisition. true closely coupled speech-gaze instances used
word acquisition, acquisition performance improved. means better
identification closely coupled speech-gaze instances lead better word acquisition
performance.
words acquired Model-2t-r, shown Figure 8, using predicted closely
coupled instances improves acquisition performance compared using instances.
acquiring words predicted closely coupled speech-gaze instances, MAP increased
(t = 1.81, p < 0.037) although improvement less significant one Model2t.
266

fiContext-Based Word Acquisition Situated Dialogue Virtual World

1

0.56

1-best
transcript
predicted 1-best
predicted transcript

0.8

0.52
Mean Average Precision

0.9

Precision

0.7
0.6
0.5
0.4
0.3
0.2

1-best
transcript

0.48

0.44

0.4

0.36

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.32

1



predicted

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 9: Word acquisition performance Model-2t speech recognition transcript
1

0.56

1-best
transcript
predicted 1-best
predicted transcript

0.8

0.52
Mean Average Precision

0.9

Precision

0.7
0.6
0.5
0.4
0.3
0.2

1-best
transcript

0.48

0.44

0.4

0.36

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.32


predicted

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 10: Word acquisition performance Model-2t-r speech recognition transcript

8.3.4 Comparison Results Based Speech Recognition Transcript
show effect speech recognition quality word acquisition, also compare
acquisition performances based speech transcript 1-best recognition. word
acquisition based speech transcript, word sequence parallel speech-gaze data
set contains nouns adjectives speech transcript. Accordingly, speech feature
used coupled speech-gaze identification extracted speech transcript.
Figures 9 & 10 show word acquisition performances Model-2t Model-2t-r using
instances using predicted coupled instances based speech transcript
1-best recognition respectively. shown figures, quality speech recognition
critical word acquisition performance. expected, word acquisition performance based
speech transcript much better recognized speech.
267

fiQu & Chai

9. Examples
Table 2 shows 10-best candidate words acquired entity couch Model-1, Model2t, Model-2t-r based speech-gaze instances Model-2t-r based predicted
closely coupled instances. probabilities candidate words also given
table. Across models, although four words (shown bold font)
acquired model, ranking acquired words achieves best Model-2t-r
based predicted closely coupled instances.
Table 3 shows another example 10-best candidate words acquired entity
stool four different models. Model-1 acquires four correct words 10-best list.
Although Model-2t also acquires four correct words 10-best list, rankings
words higher. speech-gaze temporal alignment domain semantic
relatedness rescoring, Model-2t-r acquires seven correct words 10-best list.
rankings correct words also improved. Compared using instances
Model-2t-r, although using predicted coupled instances Model-2t-r results
seven correct words ranks 10-best list, probabilities
correctly acquired words higher. means results based predicted
coupled instances confident.
Model
Rank 1
Rank 2
Rank 3
Rank 4
Rank 5
Rank 6
Rank 7
Rank 8
Rank 9
Rank 10

Model-1
couch(0.1105)
bedroom(0.1047)
chair(0.1004)
bad(0.0936)
room(0.0539)
wooden(0.0354)
bench(0.0319)
small(0.0289)
yellow(0.0274)
couple(0.0270)

Model-2t
couch(0.1224)
chair(0.0798)
bed(0.0593)
small(0.0536)
room(0.0528)
bad(0.0489)
yellow(0.0333)
bench(0.0332)
lot(0.0331)
wooden(0.0226)

Model-2t-r
couch(0.4743)
chair(0.1668)
bench(0.0949)
bed(0.0311)
small(0.0235)
bad(0.0226)
room(0.0174)
lot(0.0151)
yellow(0.0107)
couple(0.0085)

Model-2t-r(predicted)
couch(0.4667)
chair(0.1557)
bench(0.11129)
bed(0.0368)
small(0.0278)
bad(0.0265)
room(0.0137)
yellow(0.0127)
couple(0.0101)
lot(0.0090)

Table 2: N-best candidate words acquired entity couch different models

Model
Rank 1
Rank 2
Rank 3
Rank 4
Rank 5
Rank 6
Rank 7
Rank 8
Rank 9
Rank 10

Model-1
plant(0.0793)
room(0.0508)
little(0.0471)
flower(0.0424)
stairs(0.0320)
call(0.0319)
square(0.0302)
footstool(0.0301)
brown(0.0300)
short(0.0294)

Model-2t
plant(0.0592)
room(0.0440)
little(0.0410)
flower(0.0409)
square(0.0408)
small(0.0403)
next(0.0308)
stool(0.0307)
brown(0.0300)
stairs(0.0226)

Model-2t-r
stool(0.1457)
little(0.1435)
small(0.1412)
footstool(0.0573)
ottoman(0.0572)
ground(0.0275)
media(0.0263)
chair(0.0257)
plant(0.0253)
square(0.0234)

Model-2t-r(predicted)
stool(0.1532)
little(0.1509)
small(0.1490)
footstool(0.0602)
ottoman(0.0601)
ground(0.0289)
media(0.0276)
chair(0.0272)
plant(0.0270)
square(0.0247)

Table 3: N-best candidate words acquired entity stool different models

268

fiContext-Based Word Acquisition Situated Dialogue Virtual World

10. Effect Online Word Acquisition Language Understanding
One important goal word acquisition use acquired new words help language
understanding subsequent conversation. demonstrate effect online word acquisition language understanding, conduct simulation studies based collected
data. simulations, system starts initial knowledge base vocabulary
words associated domain concepts. system continuously enhances knowledge
base acquiring words users Model-2t-r incorporates speech-gaze
temporal information domain semantic relatedness. enhanced knowledge base
used understand language new users.
evaluate language understanding performance concept identification rate (CIR):
CIR =

#correctly identified concepts 1-best speech recognition
#concepts speech transcript

simulate process online word acquisition evaluate effect language
understanding two situations: 1) system starts training data
small initial vocabulary, 2) system starts training data.
10.1 Simulation 1: System Starts Training Data
build conversational systems, one approach domain experts provide domain vocabulary system design time. first simulation follows practice.
system provided default vocabulary start without training data. default
vocabulary contains one seed word domain concept.
Using collected data 20 users, simulation process goes following
steps:
user index = 1, 2, . . . , 20:
Evaluate CIR i-th users utterances (1-best speech recognition)
current system vocabulary.
Acquire words instances (with 1-best speech recognition) users
1 i.
Among 10-best acquired words, add verified new words system vocabulary.
process, language understanding performance individual user
depends users language well users position user sequence.
reduce effect user ordering language understanding performance,
simulation process repeated 1000 times randomly ordered users. average
CIRs simulations shown Figure 11.
Figure 11 also shows CIRs system static knowledge base (vocabulary). curve drawn way curve dynamic knowledge
base, except without word acquisition random simulation processes. see
figure, system doest word acquisition capability, language understanding performance change users communicated system.
capability automatic word acquisition, systems language understanding
performance becomes better users talked system.
269

fiQu & Chai

0.7
static knowledge base
dynamic knowledge base

0.65
0.6

CIR

0.55
0.5
0.45
0.4
0.35
0.3
0.25

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20
User Index

Figure 11: CIR user language achieved system starting training data
10.2 Simulation 2: System Starts Training Data
Many conversational systems use real user data derive domain vocabulary. follow
practice, second simulation provides system training data. training
data serves two purposes: 1) build initial vocabulary system; 2) train classifier
predict closely coupled speech-gaze instances new users data.
Using collected data 20 users, simulation process goes following
steps:
Using first users data training data, acquire words training instances
(with speech transcript); add verified 10-best words systems vocabulary
seed words; build classifier training data prediction closely coupled
speech-gaze instances.
Evaluate effect incremental word acquisition CIR remaining (20-m)
users data. user index = 1, 2, . . . , (20-m):
Evaluate CIR i-th users utterances (1-best speech recognition).
Predict closely coupled speech-gaze instances i-th users data.
Acquire words training users true coupled instances (with speech
transcript) predicted coupled instances (with 1-best speech recognition)
users 1 i.
Among 10-best acquired words, add verified new words system vocabulary.
simulation process repeated 1000 times randomly ordered users
reduce effect user ordering language understanding performance. Figure 12
shows averaged language understanding performance random simulations.
language understanding performance system static knowledge base
also shown Figure 12. curve drawn random simulations without
steps word acquisition. observe general trend figure that, word
acquisition, systems language understanding becomes better users
270

fiContext-Based Word Acquisition Situated Dialogue Virtual World

0.6
static knowledge base
dynamic knowledge base
0.59

CIR

0.58

0.57

0.56

0.55

1

2

3

4

5

6

7

8

9

10

User Index

Figure 12: CIR user language achieved system starting training data 10
users

communicated system. Without word acquisition capability, systems language
understanding performance increase users conversed
system.
simulations show automatic vocabulary acquisition beneficial systems
language understanding performance training data available. training data
available, vocabulary acquisition could important beneficial robust
language understanding.
10.3 Effect Speech Recognition Online Word Acquisition
Language Understanding
simulation results Figures 11 & 12 based 1-best recognized speech hypotheses relatively high WER (48.1%). better speech recognition, system
better concept identification performance. show effect speech recognition
quality online word acquisition language understanding, also perform Simulation 1 Simulation 2 based speech transcript. simulation processes
ones based 1-best speech recognition except word acquisition based
speech transcript CIR evaluated also speech transcript new simulations.
Figure 13 shows CIR curves based speech transcript online conversation.
word acquisition, systems language understanding becomes better
users communicated system. consistent CIR curves based
1-best speech recognition. However, CIRs based speech transcript much higher
CIRs based 1-best speech recognition, verifies speech recognition
quality critical language understanding performance.

11. Discussion Future Work
experimental results shown incorporating extra information improves word
acquisition compared completely unsupervised approaches. However, current ap271

fiQu & Chai

1

0.95
static knowledge base
dynamic knowledge base

static knowledge base
dynamic knowledge base

0.8

0.93
CIR

0.94

CIR

0.9

0.7

0.92

0.6

0.91

0.5

1

2

3

4

5

6

7

8

0.9

9 10 11 12 13 14 15 16 17 18 19 20
User Index

(a) Simulation 1: training data

1

2

3

4

5

6

7

8

9

10

User Index

(b) Simulation 2: training data 10 users

Figure 13: CIR user language (transcript) achieved system online conversation

proaches several limitations. First, incorporation domain knowledge
semantic relatedness based WordNet restrict acquired words appear
WordNet. certainly desirable. limitation readily addressed
changing way word probability distribution tailored semantic relatedness
(in Section 6.3.1 Section 6.3.2). example, one simple way keep probability
mass words WordNet tailor distribution words
occur WordNet based semantic relatedness object.
Second, current approach, acquired words limited words
recognized speech recognizer. shown Section 8.3.4, speech recognition
performance rather poor experiments. partly due lack language
models specifically trained domain. Approaches improve speech recognition,
example, based referential semantic language model described (Schuler, Wu, &
Schwartz, 2009) potentially improve acquisition performance. Furthermore, set
acquired words bounded vocabulary speech recognizer. new words
dictionary acquired. break barrier, inspired previous
work (Yu & Ballard, 2004; Taguchi et al., 2009), currently extending approach
incorporate grounding acoustic phoneme sequences domain concepts.
Another limitation current approaches incapable acquiring
multiword expressions. map single words domain concepts. However,
observe multiword expressions (e.g., Rubiks cube) data. examine
issue future work incorporating linguistic knowledge modeling
fertility entities, example, IBM Model 3 4.
simplicity current models also limits word acquisition performance.
example, alignment model based temporal information directly incorporates findings
psycholinguistic studies. studies generally conducted much simpler
settings without interaction. recent work Fang, Chai, Ferreira (2009) shown
correlations intensity gaze fixations objects denoted linguistic centers
272

fiContext-Based Word Acquisition Situated Dialogue Virtual World

(e.g., forward-looking centers based centering theory, Grosz, Joshi, & Weinstein, 1995).
plan incorporate results improve alignment modeling future.
improve performance, another interesting direction take consideration interactive nature conversation, example, combining dialog management
solicit user feedback acquired words. However, important identify
strategies balance trade explicit feedback solicitation (and thus lengthening interaction) quality acquired words. Reinforcement learning
potential approach address problem.

12. Conclusions
Motivated psycholinguistic findings, investigate use eye gaze automatic word acquisition multimodal conversational systems. paper presents several
enhanced models incorporate user language behavior, domain knowledge, conversation context word acquisition. experiments shown enhanced
models significantly improve word acquisition performance.
Recent advancement eye tracking technology made available non-intrusive eye
tracking devices tolerate head motion provide high tracking quality. Integrating eye tracking conversational interfaces longer beyond reach. believe
incorporating eye gaze automatic word acquisition provides another potential approach
improve robustness human-machine conversation.

Acknowledgments
work supported IIS-0347548 IIS-0535112 National Science Foundation. would like thank anonymous reviewers valuable comments
suggestions.

References
Allopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K. (1998). Tracking time course
spoken word recognition using eye movements: Evidence continuous mapping
models. Journal Memory & Language, 38, 419439.
Barnard, K., Duygulu, P., de Freitas, N., Forsyth, D., Blei, D., & Jordan, M. (2003). Matching words pictures. Journal Machine Learning Research, 3, 11071135.
Bock, K., Irwin, D. E., Davidson, D. J., & Leveltb, W. (2003). Minding clock. Journal
Memory Language, 48, 653685.
Brown, P. F., Pietra, S. D., Pietra, V. J. D., & Mercer, R. L. (1993). mathematic
statistical machine translation: Parameter estimation. Computational Linguistics,
19 (2), 263311.
Brown-Schmidt, S., & Tanenhaus, M. K. (2006). Watching eyes talking size:
investigation message formulation utterance planning. Journal Memory
Language, 54, 592609.
273

fiQu & Chai

Byron, D., Mampilly, T., Sharma, V., & Xu, T. (2005). Utilizing visual attention
cross-modal coreference interpretation. Proceedings Fifth International
Interdisciplinary Conference Modeling Using Context (CONTEXT-05), pp.
8396.
Campana, E., Baldridge, J., Dowding, J., Hockey, B., Remington, R., & Stone, L. (2001).
Using eye movements determine referents spoken dialogue system. Proceedings Workshop Perceptive User Interface.
Cessie, S. L., & Houwelingen, J. V. (1992). Ridge estimators logistic regression. Applied
Statistics, 41 (1), 191201.
Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded language
acquisition. Proceedings 25th International Conference Machine Learning
(ICML).
Cooke, N. J. (2006). Gaze-Contigent Automatic Speech Recognition. Ph.D. thesis, University
Birminham.
Dahan, D., & Tanenhaus, M. K. (2005). Looking rope looking snake:
Conceptually mediated eye movements spoken-word recognition. Psychonomic
Bulletin & Review, 12 (3), 453459.
DeVault, D., & Stone, M. (2003). Domain inference incremental interpretation.
Proceedings ICoS.
Eberhard, K., Spivey-Knowiton, M., Sedivy, J., & Tanenhaus, M. (1995). Eye movements
window real-time spoken language comprehension natural contexts. Journal
Psycholinguistic Research, 24, 409436.
Fang, R., Chai, J. Y., & Ferreira, F. (2009). linguistic attention gaze fixations
inmultimodal conversational interfaces. Proceedings International Conference
Multimodal Interfaces (ICMI), pp. 143150.
Fazly, A., Alishahi, A., & Stevenson, S. (2008). probabilistic incremental model word
learning presence referential uncertainty. Proceedings 30th Annual
Conference Cognitive Science Society.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.
Fleischman, M., & Roy, D. (2005). Intentional context situated language learning.
Proceedings 9th Conference Computational Natural Language Learning
(CoNLL).
Fong, T. W., & Nourbakhsh, I. (2005). Interaction challenges human-robot space exploration. Interactions, 12 (2), 4245.
Gorniak, P., & Roy, D. (2004). Grounded semantic composition visual scenes. Journal
Artificial Intelligence Research, 21, 429470.
Griffin, Z., & Bock, K. (2000). eyes say speaking. Psychological Science,
11, 274279.
Griffin, Z. M. (2001). Gaze durations speech reflect word selection phonological
encoding. Cognition, 82, B1B14.
274

fiContext-Based Word Acquisition Situated Dialogue Virtual World

Griffin, Z. M. (2004). look? Reasons eye movements related language production.
Henderson, J., & Ferreira, F. (Eds.), Interface Language, Vision, Action:
Eye Movements Visual World, pp. 213248. Taylor Francis.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: framework modeling
local coherence discourse. Computational Linguistics, 21 (2), 203226.
Jacob, R. J. K. (1991). use eye movements human-computer interaction techniques:
look get. ACM Transactions Information Systems, 9 (3),
152169.
Kahneman, D. (1973). Attention Effort. Prentice-Hall, Inc., Englewood Cliffs.
Kaur, M., Termaine, M., Huang, N., Wilder, J., Gacovski, Z., Flippo, F., & Mantravadi,
C. S. (2003). it? event synchronization gaze-speech input systems.
Proceedings International Conference Multimodal Interfaces (ICMI).
Klemmer, S., Sinha, A., Chen, J., Landay, J., Aboobaker, N., & Wang, A. (2000). SUEDE:
wizard oz prototyping tool speech user interfaces. Proceedings ACM
Symposium User Interface Software Technology, pp. 110.
Lemon, O., Gruenstein, A., & Peters, S. (2002). Collaborative activities multitasking
dialogue systems. Traitement Automatique des Langues, 43 (2), 131154.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondences
less supervision. Proceedings 47th Annual Meeting Association
Computational Linguistics (ACL).
Liu, Y., Chai, J., & Jin, R. (2007). Automated vocabulary acquisition interpretation
multimodal conversational systems. Proceedings 45th Annual Meeting
Association Computational Linguistics (ACL).
Meyer, A., Sleiderink, A., & Levelt, W. (1998). Viewing naming objects: eye movements
noun phrase production. Cognition, 66 (22), 2533.
Nakano, Y., Reinstein, G., Stocky, T., & Cassell, J. (2003). Towards model face-to-face
grounding. Proceedings Annual Meeting Association Computational
Linguistics (ACL).
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet::Similarity - measuring
relatedness concepts. Proceedings Nineteenth National Conference
Artificial Intelligence (AAAI).
Prasov, Z., & Chai, J. Y. (2008). Whats gaze? role eye-gaze reference resolution
multimodal conversational interfaces. Proceedings ACM 12th International
Conference Intelligent User interfaces (IUI).
Qu, S., & Chai, J. Y. (2007). exploration eye gaze spoken language processing
multimodal conversational interfaces. Proceedings Human Language Technology Conference North American Chapter Association Computational
Linguistics (HLT-NAACL), pp. 284291.
Qu, S., & Chai, J. Y. (2008). Incorporating temporal semantic information eye gaze
automatic word acquisition multimodal conversational systems. Proceedings
275

fiQu & Chai

Conference Empirical Methods Natural Language Processing (EMNLP),
pp. 244253.
Qu, S., & Chai, J. Y. (2009). role interactivity human-machine conversation
automatic word acquisition. Proceedings 10th Annual Meeting Special
Interest Group Discourse Dialogue (SIGDIAL), pp. 188195.
Qvarfordt, P., & Zhai, S. (2005). Conversing user based eye-gaze patterns.
Proceedings Conference Human Factors Computing Systems (CHI).
Rayner, K. (1998). Eye movements reading information processing - 20 years
research. Psychological Bulletin, 124 (3), 372422.
Roy, D. (2002). Learning visually-grounded words syntax scene description task.
Computer Speech Language, 16 (3), 353385.
Roy, D., & Pentland, A. (2002). Learning words sights sounds, computational
model. Cognitive Science, 26 (1), 113146.
Schuler, W., Wu, S., & Schwartz, L. (2009). framework fast incremental interpretation
speech decoding. Computational Linguistics, 35 (3), 313343.
Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception using
force dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.
Spivey, M. J., Tanenhaus, M. K., Eberhard, K. M., & Sedivy, J. C. (2002). Eye movements
spoken language comprehension: Effects visual context syntactic ambiguity
resolution. Cognitive Psychology, 45, 447481.
Taguchi, R., Iwahashi, N., Nose, T., Funakoshi, K., & Nakano, M. (2009). Learning lexicons spoken utterances based statistical model selection. Proceedings
Interspeech.
Tanaka, K. (1999). robust selection system using real-time multi-modal user-agent interactions. Proceedings International Conference Intelligent User Interfaces
(IUI).
Tanenhaus, M., Spivey-Knowiton, M., Eberhard, K., & Sedivy, J. (1995). Integration
visual linguistic information spoken language comprehension. Science, 268,
16321634.
Toutanova, K., Klein, D., Manning, C., & Singer, Y. (2003). Feature-Rich Part-of-Speech
tagging cyclic dependency network. Proceedings Human Language
Technology Conference North American Chapter Association Computational Linguistics (HLT-NAACL), pp. 252259.
Toutanova, K., & Manning, C. D. (2000). Enriching knowledge sources used maximum entropy part-of-speech tagger. Proceedings Joint SIGDAT Conference Empirical Methods Natural Language Processing Large Corpora
(EMNLP/VLC), pp. 6370.
Traum, D., & Rickel, J. (2002). Embodied agents multiparty dialogue immersive
virtual worlds. Proceedings 1st international joint conference Autonomous
Agents Multi-Agent Systems.
276

fiContext-Based Word Acquisition Situated Dialogue Virtual World

Wang, J. (1995). Integration eye-gaze, voice manual response multimodal user
interfaces. Proceedings IEEE International Conference Systems, Man
Cybernetics, pp. 39383942.
Yu, C., & Ballard, D. (2004). multimodal learning interface grounding spoken language
sensory perceptions. ACM Transactions Applied Perceptions, 1 (1), 5780.
Zhai, S., Morimoto, C., & Ihde, S. (1999). Manual gaze input cascaded (MAGIC)
pointing. Proceedings Conference Human Factors Computing Systems
(CHI), pp. 246253.
Zhang, Q., Imamiya, A., Go, K., & Mao, X. (2004). Overriding errors speech gaze
multimodal architecture. Proceedings International Conference Intelligent
User Interfaces (IUI).

277

fiJournal Artificial Intelligence Research 37 (2010) 1-39

Submitted 07/09; published 01/10

Text Relatedness Based Word Thesaurus
George Tsatsaronis

GBT @ IDI . NTNU .

Department Computer Information Science
Norwegian University Science Technology, Norway

Iraklis Varlamis

VARLAMIS @ HUA . GR

Department Informatics Telematics
Harokopio University, Greece

Michalis Vazirgiannis

MVAZIRG @ AUEB . GR

Department Informatics
Athens University Economics Business, Greece

Abstract
computation relatedness two fragments text automated manner requires
taking account wide range factors pertaining meaning two fragments convey,
pairwise relations words. Without doubt, measure relatedness
text segments must take account lexical semantic relatedness words.
measure captures well aspects text relatedness may help many tasks,
text retrieval, classification clustering. paper present new approach measuring
semantic relatedness words based implicit semantic links. approach exploits word thesaurus order devise implicit semantic links words. Based
approach, introduce Omiotis, new measure semantic relatedness texts
capitalizes word-to-word semantic relatedness measure (SR) extends measure
relatedness texts. gradually validate method: first evaluate performance
semantic relatedness measure individual words, covering word-to-word similarity relatedness, synonym identification word analogy; then, proceed evaluating
performance method measuring text-to-text semantic relatedness two tasks, namely
sentence-to-sentence similarity paraphrase recognition. Experimental evaluation shows
proposed method outperforms every lexicon-based method semantic relatedness selected
tasks used data sets, competes well corpus-based hybrid approaches.

1. Introduction
Relatedness texts perceived several different ways. Primarily, one think
lexical relatedness similarity texts, easily captured vectorial representation texts (van Rijsbergen, 1979) standard similarity measure, like Cosine, Dice (Salton
& McGill, 1983), Jaccard (1901). models high impact information retrieval
past decades. Several improvements proposed techniques towards inventing sophisticated weighting schemes text words, like example TF-IDF
variations (Aizawa, 2003). directions explore need capture latent semantic relations dimensions (words) created vector space model, using techniques latent
semantic analysis (Landauer, Foltz, & Laham, 1998). Another aspect text relatedness, probably
equal importance, semantic relatedness two text segments. example, sentences shares company dropped 14 cents, business institutions stock slumped
14 cents obvious semantic relatedness, traditional measures text similarity fail
c
2010
AI Access Foundation. rights reserved.

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

recognize. motivation work show measure relatedness texts,
takes account lexical semantic relatedness word elements, performs
better traditional lexical matching models, handle cases like one above.
paper propose Omiotis1 , new measure semantic relatedness texts,
extends SR, measure semantic relatedness words. word-to-word relatedness measure, turn, based construction semantic links individual words, according
word thesaurus, case WordNet (Fellbaum, 1998). pair words potentially connected via one semantic paths, one comprising one semantic
relations (edges) connect intermediate thesaurus concepts (nodes). weighting semantic
path consider three key factors: (a) semantic path length, (b) intermediate nodes specificity denoted node depth thesaurus hierarchy, (c) types semantic edges
compose path. triptych allows measure perform well complex linguistic tasks,
require simple similarity, SAT Analogy Test2 demonstrated
experiments. best knowledge, Omiotis first measure semantic relatedness
texts considers tandem three factors measuring pairwise word-to-word
semantic relatedness scores. Omiotis integrates semantic relatedness word level words
statistical information text level provide final semantic relatedness score texts.
contributions work summarized following: 1) new measure computing semantic relatedness words, namely SR, exploits semantic information thesaurus offer, including semantic relations crossing parts speech (POS), taking
account relation weights depth thesaurus nodes; 2) new measure computing semantic relatedness texts, namely Omiotis, require use external
corpora learning methods, supervised unsupervised, 3) thorough experimental evaluation
benchmark data sets measuring performance word-to-word similarity relatedness
tasks, well word analogy; addition, experimental evaluation two text related tasks
(sentence-to-sentence similarity paraphrase recognition) measuring performance
text-to-text relatedness measure. Additional contributions work are: a) use semantic relations offered WordNet, increases chances finding semantic path
two words, b) availability pre-computed semantic relatedness scores every pair
WordNet senses, accelerates computation semantic relatedness texts facilitates incorporation semantic relatedness several applications (Tsatsaronis, Varlamis,
Nrvag, & Vazirgiannis, 2009; Tsatsaronis & Panagiotopoulou, 2009).
rest paper organized follows: Section 2 discusses preliminary concepts regarding
word thesauri, semantic network construction, semantic relatedness similarity measures,
summarizes related work fields. Section 3 presents key contributions work.
Section 4 provides experimental evaluation analysis results. Finally, Section 5
presents conclusions next steps work.

2. Preliminaries Related Work
approach capitalizes word thesaurus order define measure semantic relatedness
words, expands measure compute text relatedness using semantic
1. Omiotis Greek word relatedness similarity.
2. http://www.aclweb.org/aclwiki/index.php?title=SAT_Analogy_Questions

2

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

lexical information. order facilitate understanding methodology elaborate
preliminary concepts section present related research approaches.
2.1 Word Thesauri use Text Applications
Word thesauri, like WordNet (Fellbaum, 1998) Rogets International Thesaurus (Morris & Hirst,
1991), constitute knowledge base several text-related research tasks. WordNet
used successfully knowledge base construction Generalized Vector Space Models
(GVSM) semantic kernels document similarity application text classification,
works Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald Weikum (2005), Basili,
Cammisa Moschitti (2005), text retrieval, works Voorhees (1993), Stokoe,
Oakes Tait (2003), previous work regarding definition new GVSM uses
word-to-word semantic relatedness (Tsatsaronis & Panagiotopoulou, 2009). Furthermore, idea
using thesaurus knowledge base text retrieval also proven successful case
cross language information retrieval, like example case CLIR system introduced
Clough Stevenson (2004). Finally, exploitation word thesauri linguistic tasks,
Word Sense Disambiguation (WSD) (Ide & Veronis, 1998) yielded interesting results
(Mihalcea & Moldovan, 1999; Tsatsaronis, Vazirgiannis, & Androutsopoulos, 2007; Tsatsaronis,
Varlamis, & Vazirgiannis, 2008).
application text relatedness measure text classification retrieval tasks
first consider impact lexical ambiguity WSD overall performance tasks.
Sanderson (1994, 2008) concludes ambiguity words take many forms, new test collections needed realize true importance resolving ambiguity embedding semantic
relatedness sense disambiguation text retrieval task. analysis Barzilay Elhadad (1997), Barzilay, Elhadad McKeown (2002) impact WSD performance
text summarization tasks addressed considering possible interpretations lexical
chains created text. Similar methodology, tackle word ambiguity taking account every possible type semantic information thesaurus offer, given sense
text word.
aforementioned approaches, clear use word thesaurus offer much
potential design models capture semantic relatedness texts, consequently, may improve performance existing retrieval classification models certain
circumstances discussed respective research works (Mavroeidis et al., 2005; Basili
et al., 2005; Stokoe et al., 2003; Clough & Stevenson, 2004). word thesaurus employed
development Omiotis WordNet. lexical database contains English nouns, verbs, adjectives
adverbs, organized sets synonym senses (synsets). Hereafter, terms senses, synsets
concepts used interchangeably. Synsets connected various links represent semantic
relations (i.e., hypernymy / hyponymy, meronymy / holonymy, synonymy / antonymy,
entailment / causality, troponymy, domain / domain terms, derivationally related forms, coordinate
terms, attributes, stem adjectives, etc.). Several relations cross parts speech, like domain
terms relation, connects senses pertaining domain (e.g., light, noun meaning electromagnetic radiation producing visual sensation, belongs domain physics).
best knowledge, proposed approach first utilizes aforementioned
semantic relations exist WordNet construction semantic relatedness measure.
3

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

2.2 Creating Semantic Networks Word Thesauri
Omiotis based creation semantic paths words text using thesaurus
concepts relations. Early approaches field, used gloss words respective word
definitions order build semantic networks text (Veronis & Ide, 1990). idea representing text semantic network initially introduced Quilian (1969). expansion
WordNet semantic relations cross parts speech added possibilities semantic network construction text. recent approaches semantic network construction
word thesauri, Mihalcea, Tarau Figa (2004) Navigli (2008), utilize wide range
WordNet semantic relations instead gloss words. methods outperformed previous
methods used semantic networks words WSD tasks Senseval 2 3 English language (Palmer, Fellbaum, & Cotton, 2001; Snyder & Palmer, 2004). work adopt
semantic network construction method introduced past (Tsatsaronis et al., 2007).
method utilizes available semantic relations WordNet. WSD task, respective method outperformed matched previous methods used semantic networks
words WSD tasks Senseval 2 3 English language, largely due rich
representation semantic networks offered. Section 3.1 introduces semantic relatedness
measure.
2.3 Measures Semantic Relatedness
Semantic relatedness words concepts exploited, past, text summarization (Barzilay et al., 2002), text retrieval (Stokoe et al., 2003; Smeaton, Kelledy, & ODonnell,
1995; Richardson & Smeaton, 1995) WSD (Patwardhan, Banerjee, & Pedersen, 2003) tasks.
Semantic relatedness measures widely classified dictionary-based3 , corpus-based hybrid.
Among dictionary-based measures, measure Agirre Rigau (1995) one first
measures developed compute semantic relatedness two concepts (i.e., set
concepts). measure based density depth concepts set
length shortest path connects them. However, assume edges path
equally important.
measure proposed Leacock, Miller Chodorow (1998) computing semantic
similarity pair concepts takes account length shortest path connecting
them, measured number nodes participating path, maximum depth
taxonomy. measure two concepts s1 s2 computed follows:
Sim(s1 , s2 ) = log

length
2D

(1)

length length shortest path connecting s1 s2 maximum depth
taxonomy used.
Regarding hybrid measures, Resniks (1995, 1999) measure pairs concepts based
Information Content (IC) deepest concept subsume (least common subsumer),
considered hybrid measure, since combines hierarchy used thesaurus, statistical information concepts measured large corpora. specifically,
3. Also found bibliography knowledge-based, thesaurus-based, lexicon-based.

4

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

semantic similarity given pair concepts s1 s2 , s0 least common
subsumer (i.e., least common ancestor), defined following equation:
Sim(s1 , s2 ) = IC(s0 )

(2)

Information Content (IC) concept (i.e., s0 ) defined as:
IC(s0 ) = logP (s0 )

(3)

P (s0 ) probability occurrence concept s0 large corpus.
measure proposed Jiang Conrath (1997), also based concept IC. Given
two concepts s1 s2 , least common subsumer s0 , semantic similarity defined
follows:
1
Sim(s1 , s2 ) =
(4)
IC(s1 ) + IC(s2 ) 2 IC(s0 )
measure Lin (1998) also based IC. Given, again, s1 , s2 , s0 , before,
similarity s1 s2 defined follows:
Sim(s1 , s2 ) =

2 IC(s0 )
IC(s1 ) + IC(s2 )

(5)

Hirst St-Onge (1998) reexamine idea constructing lexical chains words,
based synsets respective semantic edges connect WordNet. initial
idea lexical chains first introduced Morris Hirst (1991), defined lexical
cohesion passage, based cohesion lexical chains passages elements,
acted indicator continuity passages lexical meaning.
encourage reader consult analysis Budanitsky Hirst (2006) detailed
discussion aforementioned measures, well measures proposed prior
aforementioned. measures use noun hierarchy (except measure
Hirst St-Onge), implementation several measures provided Patwardhan,
Banerjee Pedersen (2003) publicly available WordNet::Similarity package also utilize
verb hierarchy. Still, relations cross parts speech considered, well
factors discussed detail Section 3. contrast, measure defines semantic relatedness
two concepts, independently Part Speech (POS), utilizing available
semantic links offered WordNet.
recent works interest semantic relatedness, include: measures Jarmasz
Szpakowicz (2003), use Rogets thesaurus compute semantic similarity, replicating
number WordNet-based approaches, LSA-based measure Finkelstein et al. (2002),
perform Latent Semantic Analysis (Landauer et al., 1998) capture text relatedness
considered corpus-based method, measure Patwardhan Pedersen (2006), utilize gloss words found words definitions create WordNet-based context vectors,
methods Strube Ponzetto (2006, 2007a), Gabrilovich Markovitch (2007), Milne
Witten (2008) use Wikipedia compute semantic relatedness also considered
corpus-based approaches, method Mihalcea, Corley Strappavara (2006),
hybrid method combines knowledge-based corpus-based measures text relatedness.
recent hybrid measures semantic similarity are: measure proposed Li et al. (2006),
use information WordNet corpus statistics collected Brown Corpus (Kucera,
5

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Francis, & Caroll, 1967) compute similarity short texts, measure text distance proposed Tsang (2008), uses distributional similarity ontological/knowledge
information compute distance text fragments. Distributional similarity also used
supervised combination WordNet-based approaches (Agirre, Alfonseca, Hall, Kravalova,
Pasca, & Soroa, 2009), produce supervised measure semantic relatedness. Li et al. (2006)
created new data set experimental evaluation, also use Section 4
evaluate Omiotis measure compare approach.
following section formally define Omiotis provide details, creation
semantic links computation relatedness words texts. give evidence
measures complexity justify design choices. Finally, discuss potential applications
measure text related tasks.

3. Measuring Word-to-Word Text-to-Text Semantic Relatedness
section presents details Omiotis, measure text semantic relatedness. measure
capitalizes idea semantic relatedness WordNet senses, extends compute
relatedness words finally texts. Since definition semantic relatedness
ranges pairs keyword senses pairs texts, Omiotis defined way captures
relatedness every granularity. result, applied wide range linguistic
text related tasks WSD, word similarity word analogy, text similarity, keyword
ranking. key points proposed measure are: (a) constructs semantic links
word senses WordNet pre-computes relatedness score every pair WordNet
senses, (b) computes semantic relatedness pair words taking account
relatedness corresponding WordNet senses, (c) computes semantic relatedness score
two given text segments extending word-to-word relatedness. Depending task,
computation semantic relatedness modified take account senses
word, words text, apply additional weights depending
word importance sense importance context. allows Omiotis adapted various
text related tasks, without modifying main process computing relatedness. Section 3.1
follows, formally define semantic relatedness measure Section 3.2 provide
detailed justification design decisions.
3.1 Construct Semantic Links Words
first step measuring semantic relatedness two text fragments, find
implicit semantic links words two fragments. Thus, present definition
semantic relatedness pair thesaurus concepts, takes account semantic path
connecting concepts, expands measure relatedness words. order solve
problem constructing semantic paths words, base approach previous
method construct semantic networks words (Tsatsaronis et al., 2007).
3.1.1 EMANTIC N ETWORK C ONSTRUCTION W ORD HESAURI
Figure 1 gives example construction semantic network two words ti tj .
simplicity reasons, assume construction semantic path senses S.i.2 S.j.1
(Initial Phase), though could every possible combination two words
6

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

ti

S.i.1

S.j.1

S.i.2

S.j.2

...

...

S.i.7

S.j.5

Synonym

...

Holonym

Meronym
S.i.2

tj

...

S.j.1

Hypernym
Antonym

...
Hyponym

Initial Phase
Index:

= Word Node

Network Expansion
= Sense Node

= Semantic Link

Figure 1: Constructing semantic networks word thesauri.
senses. Initially, two sense nodes expanded using semantic links offered WordNet.
semantic links senses, found thesaurus, become edges pointed senses
nodes network (Network Expansion). expansion process repeated recursively
shortest 4 path S.i.2 S.j.1 found. path found S.i.2 S.j.1
senses consequently words semantically related.
3.1.2 EMANTIC R ELATEDNESS



PAIR C ONCEPTS

semantic relatedness pair concepts measured constructed semantic network.
considers path length, captured compactness, path depth, captured semantic
path elaboration, defined following. measure WSD based idea
compactness initially proposed Mavroeidis et al. (2005). original measure used
nouns hypernym relation, extended current work support WordNets
relations noun, verb adjective parts speech. define new compactness
measure (Definition 1) core Omiotis measure.
Definition 1 Given word thesaurus O, weighting scheme edges assigns weight
w (0, 1) edge, pair senses = (s1 , s2 ), path P length l connecting
Qthe two
senses, semantic compactness (SCM (S, O, P )) defined as: SCM (S, O, P ) = li=1 wi ,
w1 , w2 , ..., wl paths edges weights. s1 = s2 SCM (S, O, P ) = 1.
path s1 s2 SCM (S, O, P ) = 0.
Note compactness takes path length account bound [0, 1]. Higher compactness
senses implies higher semantic relatedness. intuition behind edge types weighting
certain types provide stronger semantic connections others. Considering lexicographers WordNet tend use relation types often others (we assume
used relation types stronger types less used), straightforward solution define edge
types weights proportion frequency occurrence WordNet 2.0. weights assigned
type using solution shown Table 1 accordance found Song
et al. (2004). table shows probability occurrence WordNet 2.0 every possible edge
type thesaurus, descending order probability values. detailed analysis choices
made Definition 1 definitions follow performed Section 3.2.
depth nodes belong path also affects term relatedness. standard means
measuring depth word thesaurus hypernym/hyponym hierarchical relation noun
adjective POS hypernym/troponym verb POS. adverb POS related stem
4. details presented Algorithm 1.

7

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

WordNet 2.0 Edge Type

Probability Occurrence

hypernym/hyponym
nominalization
category domain
part meronym/holonym
region domain
similar
usage domain
member meronym/holonym
antonym
verb group
also see
attribute
entailment
cause
substance meronym/holonym
derived
participle

0.61
0.147
0.094
0.0367
0.0238
0.02
0.016
0.014
0.0105
0.01
0.0091
0.00414
0.00195
0.00158
0.00089
0.0003
3.4E 06

Table 1: Probability occurrence every edge type WordNet 2.0.
adjective sense used measure depth. path shallow sense nodes general
compared path deep nodes. parameter semantic relatedness terms
captured measure semantic path elaboration introduced following definition.
Definition 2 Given word thesaurus , pair senses = (s1 , s2 ), s1 ,s2
s1 6= s2, path P =< p1 , p2 , ..., pl > length l, either s1 = p1 s2 = pl
s1 = pl s2 = p1 , semantic path elaboration path (SP E(S, O, P )) defined as:
Q
1
di+1
SP E(S, O, P ) = li=1 d2di +d
dmax
, di depth sense pi according O, dmax
i+1
maximum depth O. s1 = s2 , d1 = d2 = SP E(S, O, P ) =
path s1 s2 SP E(S, O, P ) = 0.


dmax .



obvious Definition 2 path length l comprises l+1 nodes, thus = l, di+1
last node path. Essentially, SPE harmonic mean two depths normalized
maximum thesaurus depth. harmonic mean preferred average depths, since offers lower upper bound gives realistic estimation paths depth. Compactness
Semantic Path Elaboration measures capture two important parameters measuring semantic relatedness terms (Budanitsky & Hirst, 2006), namely path length senses depth
used thesaurus. combine two measures following definition Semantic
Relatedness two terms:
Definition 3 Given word thesaurus O, pair senses = (s1 , s2 ) semantic relatedness
(SR(S, O)) defined maxP {SCM (S, O, P ) SP E(S, O, P )}.
8

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Algorithm 1 Maximum-Semantic-Relatedness(G, u, v, w)
1:
2:

3:
4:
5:
6:
7:
8:
9:
10:
11:

12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

INPUT: directed weighted graph G, two nodes u, v weighting scheme w : E (0..1).
OUTPUT: path u v maximum product edges weights.
Initialize-Single-Source(G, u)
vertices v VG
d[v] =
[v] = N U
end
d[u] = 1
Relax(u, v, w)
d[v] < d[u] w(u, v)
d[v] = d[u] w(u, v)
[v] = u
end
Maximum-Relatedness(G, u, v, w)
Initialize-Single-Source(G, u)
S=
Q = VG
v Q
= Extract Q vertex maximum
=Ss
vertices k Adjacency List
Relax(s, k, w)
end
end
return path following ancestors v back u

Given word thesaurus, one semantic path connecting two senses.
senses compactness take different values different paths. cases, use
path maximizes semantic relatedness. computation introduce Algorithm 1,
modification Dijkstras algorithm (Cormen, Leiserson, & Rivest, 1990) finding shortest path two nodes weighted directed graph. algorithm, G representation
directed weighted graph given input (e.g., using adjacency lists), VG set
vertices G. Also, two sets used; S, contains vertices
maximum semantic relatedness computed source vertex (i.e., u), Q,
contains vertices algorithm computed yet maximum relatedness source vertex. Furthermore, three tables used; d, which, vertex v stores
maximum semantic relatedness found given time algorithm execution source
vertex, i.e., u d[v]; , vertex v stores predecessor [v]; w, stores
edge weights graph (e.g., w[k, m] stores edge weight edge starts k
goes m).
9

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

algorithm comprises three functions: (a) Initialize-Single-Source(G, u), initializes
tables , every vertex v graph. precisely, sets d[v] = , since semantic relatedness source unknown beginning, algorithm seeks
maximum semantic relatedness initially set minimum value (i.e., ).
also sets [v] = N U LL, since beginning algorithm execution aware
yet predecessor vertex v following path source vertex u v results maximum semantic relatedness; (b) Relax(u, v, w), given two vertices, u v
directly connected edge weight w[u, v], updates value d[v], case
follow edge (u, v) results higher semantic relatedness vertex v
source, compared value computed time algorithm execution;
(c) Maximum-Relatedness(G, u, v, w), uses aforementioned functions executes
Dijkstras algorithm. proof algorithms correctness follows next theorem.
Theorem 1 Given word thesaurus O, edges weighting function w : E (0, 1),
higher value declares stronger edge, pair senses S(ss , sf ) declaring source (ss ) destination (sf ) vertices, SCM (S, O, P ) SP E(S, O, P ) maximized path returned
2di dj
=w

Algorithm 1, using weighting scheme wij
ij dmax (di +dj ) , wij new weight
edge connecting senses si sj .
Proof 1 show vertex sf VG , d[sf ] maximum product edges weight
selected path, starting ss , time sf inserted S. on,
notation (ss , sf ) represent product. Path p connects vertex S, namely ss ,
vertex VG S, namely sf . Consider first vertex sy along p sy VG let
sx ys predecessor. Now, path p decomposed ss sx sy sf . claim
d[sy ] = (ss , sy ) sf inserted S. Observe sx S. Then, sf chosen
first vertex d[sf ] 6= (ss , sf ) inserted S, d[sx ] = (ss , sx )
sx inserted S.
sy occurs sf path ss sf edge weights nonnegative
(0, 1) (ss , sy ) (ss , sf ), thus d[sy ] = (ss , sy ) (ss , sf ) d[sf ].
sy sf V sf chosen, d[sf ] d[sy ]. Thus, d[sy ] =
(ss , sy ) = (ss , sf ) = d[sf ]. Consequently, d[sf ] = (ss , sf ) contradicts choice sf .
conclude time vertex sf inserted S, d[sf ] = (ss , sf ).
Next, prove returned maximum product SCM (S, O, P ) SP E(S, O, P ), let
path ss sf maximum edge weight product k edges. Then, Algorithm 1
Q
2dk df
2d2 d3
2ds d2

returns maximum ki=1 wi(i+1)
= ws2 dmax
(ds +d2 ) w23 dmax (d2 +d3 ) ...wkf dmax (dk +df ) =
Qk 2di di+1
Qk
1
i=1 di +di+1 dmax = SCM (S, O, P ) SP E(S, O, P ).
i=1 wi(i+1)
3.1.3 EMANTIC R ELATEDNESS



PAIR ERMS

Based Definition 3, measures semantic relatedness pair senses S,
define semantic relatedness pair terms (t1 , t2 ) follows.
Definition 4 Let word thesaurus O, let = (t1 , t2 ) pair terms entries
O, let X1 set senses t1 X2 set senses t2 O. Let S1 , S2 , ..., S|X1 ||X2 |
set pairs senses, Sk = (si , sj ), si X1 sj X2 . semantic relatedness
(SR(T, S, O)) defined as:
10

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

maxSk {maxP {SCM (Sk , O, P ) SP E(Sk , O, P )}} = maxSk {SR(Sk , O)}
k = 1..|X1 | |X2 |. Semantic relatedness two terms t1 , t2 t1 t2

/ defined 1. Semantic relatedness t1 , t2 t1 t2
/ O, vice versa,
considered 0.
remaining paper, SR(T, S, O) pair terms denoted SR(T ),
ease readability.
3.2 Analysis SR Measure
section present rationale behind Definitions 1, 2, 3, providing theoretical
and/or experimental evidence decisions made design measure. illustrate
advantages disadvantages different alternatives using simple examples argue
decisions. Finally, discuss advantages SR previous measures semantic
relatedness.
list decisions made design semantic relatedness measure comprises: a)
use senses POS, instead noun senses only, b) use semantic edge types found
WordNet, instead IS-A relation only, c) use edge weights, d) use senses depth
scaling factor. important mention measures semantic relatedness differ
measures semantic similarity, traditionally use hierarchical relations ignore
type semantic relations. addition, concepts differentiate semantic distance,
sense latter metric.
3.2.1 U SE POS NFORMATION
Firstly, shall argue fact use POS designing semantic relatedness measure important, increase coverage measure. rationale supporting
decision fairly simple. Current data sets evaluating semantic relatedness even semantic similarity measures restricted nouns, like example Rubenstein Goodenough 65 word
pairs (1965), Miller Charles 30 word pairs (1991), Word-Similarity-353 collection
(Finkelstein et al., 2002). Thus, experimental evaluation data sets cannot pinpoint
caveat omitting remaining parts speech. However, text similarity tasks benchmark
data sets comprise nouns. Throughout following analysis, reader must consider
resulting measure semantic relatedness among words destined embedded
text-to-text semantic relatedness, shown next section.
following two sentences paraphrase example taken Microsoft Paraphrase
Corpus (Dolan, Quirk, & Brockett, 2004) show importance using POS well,
verbs:
charges espionage aiding enemy carry death penalty.
convicted spying charges could face death penalty.

Words appear WordNet written bold stopwords omitted simplicity5 .
two sentences many nouns common (charges, death, penalty), also pairs
words two sentences contribute evidence two sentences
5. stopwords list used available http://www.db-net.aueb.gr/gbt/resources/stopwords.txt

11

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

paraphrase. example espionage spying obvious semantic relatedness, well
enemy spying. Also, convicted charges, well convicted penalty. type
evidence would disregarded measure semantic relatedness similarity
uses noun POS hierarchy WordNet. Examples measures are: measure
Sussna (1993), Wu Palmer (1994), Jiang Conrath (1997), Resnik (1995, 1999),
WordNet-based component measure proposed Finkelstein et al. (2002). point
view, decision use POS information expands potential matches found measure allows use measure complicated tasks, like paraphrase recognition, text
retrieval, text classification.
3.2.2 U SE E YPE EMANTIC R ELATIONS
decision use parts speech construction semantic graphs, introduced previous work (Tsatsaronis et al., 2007), imposes involvement semantic
relations instead merely taxonomic (IS-A) ones. Moreover, decision based evidence
related literature. work Smeaton et al. (1995) provides experimental evidence measuring semantic similarity incorporating non-hierarchical link types (i.e. part meronym/holonym,
member meronym/holonym, substance meronym/holonym) improves much performance
measure. experimental evaluation conducted adopting small variation Resniks
measure (1995).
Hirst St-Onge (1998) reported discovered several limitations missing
connections set WordNet relations construction lexical chains sentences
detection correction malapropisms. provided following example using
pair words bold report caveat:
School administrators say taxpayers expect schools provide child care
school lunches, integrate immigrants community, offer special classes adult
students,.

intrinsic connection nouns child care school, exist WordNet,
cannot discovered considering hierarchical edge types. connection depicted
Figure 2, shows path WordNet. rich semantic representation able detect
connections address problems aforementioned type.
3.2.3 U SE W EIGHTS



E DGES

work Resnik (1999) reports simple edge counting, implicitly assumes links
taxonomy represent uniform distances, problematic best semantic distance
measure WordNet. similar direction lie findings Sussna (1993), performed
thorough experimental evaluation varying edge weights order measure semantic distance
concepts. Sussnas findings, revealed weights semantic edges non-negligible
factor application measure WSD, best results reported
edge weighting scheme used, instead assigning edge weight.
reasons, decided assign weight every edge type, chose simple probability
occurrence edge type WordNet, edge weighting scheme (see Table 1).
important factor absent several similarity measures proposed past,
measures Leacock et al. (1998), Jarmasz Szpakowicz (2003) Banerjee Pedersen
(2003), outperformed experimental evaluation measure.
12

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

activity
(Noun)
Hyponym
Hypernym

education
(Noun)

aid
(Noun)

Nominalization

educate
(Verb)

Hyponym
Hypernym

service
(Noun)

school
(Verb)

Nominalization

school
(Noun)

Hypernym

child care
(Noun)

Figure 2: Semantic path child care school following WordNet edges.
instrumentality
(Noun)

conveyance
(Noun)

Hyponym
Hypernym

Hypernym
implement
(Noun)
container
(Noun)

vehicle
(Noun)

Hyponym

public
transport
(Noun)

Hyponym
Hypernym
bar
(Noun)

Hypernym

Hyponym

wheeled
vehicle
(Noun)

autobus
(Noun)

Hyponym
wheeled
vehicle
(Noun)

Hypernym
lever
(Noun)

self-propelled
vehicle
(Noun)
Category Domain

Hyponym
Hypernym

car
(Noun)

Hypernym

pedal
(Noun)

motor vehicle
(Noun)
passenger
(Noun)

Part Meronym
Hyponym
Hypernym

accelerator
(Noun)

car
(Noun)

Category Domain

NWPL Path
PR Path

Figure 3: Product Relatedness (PR) Normalized Weighted Path Length (NWPL) paths pairs:
car accelerator (left), car autobus (right).

3.2.4 U SE EPTH CALING FACTOR
decision incorporate depth scaling factor (SPE Definition 2) edge weighting
mechanism inspired thorough experimental evaluation conducted Sussna (1993),
13

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

provided evidence importance edge weighting factor semantic network
based measures. According experiments Miller Charles data set Spearman
correlation human judgements much lower (7 percentage points) omitting depth
scaling factor adopting SPE factor (see Definition 3).
3.2.5 J USTIFICATION SR EFINITIONS
According Definition 1, semantic compactness pair concepts product depthscaled weights edges connecting two concepts. use product instead sum
normalized sum edges weights explained following.
Since might several paths connecting two concepts, Definition 3 clearly selects
path maximizes product semantic compactness (SC) semantic path elaboration (SPE). simplicity, ignore effect depth scaling
factor (SPE Definition 2)
Q
consequently, aim find path maximizes li=1 ei , e1 , e2 , ..., el
(non depth-scaled) weights edges path connecting two given concepts. Let us name
less elaborate version semantic relatedness measure product relatedness (PR),
P R(S, O) = maxP {SCM (S, O, P )}. alternative would beenPto define semantic coml

e


pactness normalized sum weights path, is: i=1
. case,
l
semantic relatedness would measured path maximizes latter formula, since
nature, semantic relatedness always seeks find path maximizes connectivity
two concepts. Let us name alternative normalized weighted path length (NWPL).
example Figure 3, show PR NWPL compute semantic relatedness
term pair car accelerator (left) car autobus (right). path maximizes
respective formulas PR NWPL using Algorithm 1 edge weights Table 1, illustrated
Figure 3 using black white arrows respectively. pair car accelerator sum-based
formula, normalized path length, selects large path example, final
computed relatedness 0.61, weight hypernym/hyponym edges. PR finds
path maximizing product immediate part meronym relation car accelerator,
computed relatedness 0.0367, weight part meronym edges. main
problem arising NWPL fact cannot distinguish among relatedness
pair concepts hypernym/hyponym hierarchy WordNet. example, NWPL
computes relatedness (0.61) every possible concept pair shown top figure.
contrast, PR able distinguish pairs terms relatedness. precisely,
behavior PR due fact embeds notion path length, since computed
relatedness decays factor range (0, 1) every hop made following type semantic
relation. Another example, also shows importance considering WordNet relations,
one shown right part Figure 3, NWPL PR paths computed
term pair car autobus. Again, NWPL selects large path, incline
hypernym/hyponym tree.
Clearly, NWPL would rather traverse huge path hypernym/hyponym edges,
following less important edge type, would decrease average path importance.
behavior creates serious drawbacks: (a) lack ability distinguish relatedness among
pair concepts hierarchy, (b) large increase actual computational cost
Algorithm 1, due fact tend incline hypernym/hyponym hierarchy,
even direct semantic edge (other hypernym/hyponym) connecting two concepts,

14

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

like shown Figure 3. Furthermore, conducting experiments NWPL 30 word pairs
Miller Charles, discovered almost 40% cases, NWPL produces
value semantic relatedness, equal 0.61, unable distinguish creating many
ties. Thus, PR better option use measure, semantic compactness factor.
Last, least, regarding overall design SR, mention proposed measure solely based use WordNet, contrast measures semantic relatedness use
large corpora, Wikipedia. Although, measures, like ones proposed Gabrilovich
Markovitch (2007), Ponzetto Strube (2007a), provide larger coverage regarding concepts reside WordNet, require processing large corpora (Wikipedia),
also changes fast frequently. Experimental evaluation Section 4 shows
measure competes well aforementioned word-to-word relatedness measures
used data sets. following section, introduce Omiotis, extension SR measuring
text-to-text relatedness.
3.3 Omiotis
quantify degree two text segments semantically relate other, build upon
SR measure, significantly extend order account terms semantic
relatedness also lexical similarity. texts may contain overly-specialized
terms (e.g., algorithms name) represented WordNet. Therefore, relying entirely
term semantics identifying degree texts relate would hamper
performance approach. hand, semantics serve complement relevance
estimations given different text terms might refer (nearly-) identical concepts.
quantify lexical similarity two texts, e.g., text B, begin estimation terms importance weights determined standard TF-IDF weighting
scheme (Salton, Buckley, & Yu, 1982).
Thereafter, estimate lexical relevance, denoted a,b terms b B
based harmonic mean respective terms TF-IDF values, given by:
a,b =

2 F IDF (a, A) F IDF (b, B)
F IDF (a, A) + F IDF (b, B)

(6)

Harmonic mean preferred instead average, since provides tight upper bound (Li,
2008). decision based fact F IDF (a, A) F IDF (b, B) two different
quantities measuring qualitative strength b respective texts.
computed lexical relevance text terms b, estimate semantic
relatedness, i.e. SR(a, b) described previously. Based estimated lexical relevance
semantic relatedness pairs text terms, next step find every word text
corresponding word b text B maximizes product semantic relatedness lexical
similarity values given Equation 7.
b = arg max(a,b SR(a, b))

(7)

bB

b corresponds term text B, entails maximum lexical similarity
semantic relatedness term text A.6 similar manner, define , corresponds
6. function argmax selects case examined ones, maximizes input formula function.

15

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

term text A, entails maximum lexical similarity semantic relatedness
term b text B.
= arg max(a,b SR(a, b))
(8)
aA

Consequently, aggregate lexical semantic relevance scores terms text A,
reference best match text B denoted shown Equation 9.

!
X
1
(A, B) =
(9)
a,b SR(a, b )
|A|
aA

opposite direction (i.e. words B words A) cover
cases two texts equal number terms.
Finally, derive degree relevance texts B combining values
estimated terms entail maximum lexical semantic relevance one another,
given by:
[(A, B) + (B, A)]
(10)
2
Algorithm 2 summarizes computation Omiotis. computation entails series steps,
complexity discussed Section 3.5.
Omiotis(A, B) =

3.4 Applications Semantic Relatedness
section describe methodology incorporating semantic relatedness pairs
words pairs text segments, several applications.
3.4.1 W ORD - -W ORD IMILARITY
Rubenstein Goodenough (1965) obtained synonymy judgements 51 human subjects 65
pairs words, effort investigate relationship similarity context similarity meaning (synonymy). Since then, idea evaluating computational measures semantic
relatedness comparing human judgments given set word pairs, widely
used, even data sets developed. proposed measure semantic relatedness
words (SR), introduced Definition 4, used directly task, order
evaluate basis Omiotis measure, measurement word-to-word semantic relatedness. application straightforward: Let n pairs words used word similarity data
set. Then, semantic relatedness every pair computed, using SR(T, S, O) defined 4.
computed values sorted descending order, produced ranking similarities
compared gold standard ranking humans, using Spearman correlation. scores
used compute Pearsons product moment correlation. Additional measures semantic
relatedness compared examining respective correlation values
human judgements.
3.4.2 SAT NALOGY ESTS
problem identifying similarities word analogies among pairs words difficult problem
standardized test assessing human ability language understanding,
16

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Algorithm 2 Omiotis(A,B, Sem, Lex )
1: INPUT: Two texts B, comprising n terms (a b terms B
respectively),
semantic relatedness measure Sem : SR(a, b) (0..1),
weighting scheme term importance text Lex : F IDF (a, A) (0..1)
2: OUTPUT: Find pair terms maximizes product Sem Lex values.

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

Compute-Zeta(A,B)
sum(A) := 0
terms
b := N U
empZeta := 0
terms b B
a,b = 2Lex(a,A)Lex(b,B)
Lex(a,A)+Lex(b,B)
empZeta < a,b Sem(a, b)
empZeta = i,j Sem(a, b)
b = b
end
end
sum(A) := sum(A) + empZeta
end
Zeta(A, B) := sum(A)/|A|
Compute-Omiotis(A,B)

17:

Omiotis(A, B) :=

Zeta(A,B)+Zeta(B,A)
2

scope well known SAT analogy tests (Scholastic Aptitude Tests). SAT tests
used admission tests universities colleges United States. participants aim
locate five given word pairs one presents similar analogy target
pair.
Although difficult machines model human cognition word analogy, several
approaches exist bibliography attempt tackle problem. Previous approaches
widely categorized into: corpus-based, lexicon-based hybrid. examples corpus-based
approaches Turney (2008b) Bicici Yuret (2006). Examples lexicon-based
approaches, Veale (2004) application lexicon-based measure Hirst
St-Onge (1998) SAT, found work Turney (2006). Hybrid approaches
applied SAT, application measures Resnik (1995) Lin (1998)
also found work Turney (2006).
order reader understand difficulty answering SAT questions, must point
average US college applicant scores 57% (Turney & Littman, 2005), top
corpus-based approach scores 56.1% (Turney, 2006), top lexicon-based scores 42% (Veale,
2004) top hybrid scores 33.2% (Resnik, 1995).
17

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Another way categorizing approaches measure semantic similarity analogy tasks
distinguish among attributional relational similarity measures (Gentner, 1983).7 Representative approaches first category lexicon-based approaches, paradigms relational
similarity measures found approaches based Latent Relational Analysis (LRA) (Turney,
2006). great interest point LRA-based approaches, like LRME algorithm proposed recently Turney (2008a), superior attributional similarity approaches discovering
word analogies. fact also supported experimental findings Turney (2006). Without
doubt, relational similarity approaches may perform better SAT analogy task, still,
shown later experiments conducted applications, like paraphrase recognition,
lexicon-based measures outperform LRA-based approaches tasks.
Semantic relatedness (SR) words, applied Omiotis, exploited solve
word analogy task. aim word analogy is, given pair words w1 w2 , identify
series semantic relations lead w1 w2 (semantic path). SAT test, target pair
(w1 ,w2 ) candidate word pairs (w1k ,w2k ), k usually 1 5, processed order
find pairs analogy. aim locate pair k, exposes maximum similarity w1
w2 . straightforward method choose among 5 candidate pairs employ two criteria:
first, k analogies analogy target pair compared, candidate
shows far similar analogy selected. However, similar analogy
obvious, 6 pairs may examined together order slightest differences lead
correct answer discovered. attempt model human cognition task using SR
two fold manner: (a) measure SR capture horizontal analogy given pair
possible candidate pairs, (b) measure SR capture vertical analogy
given pair possible candidate pairs. two aspects covered following
Equations 11 13. capture horizontal analogy pair words candidate pair,
measure difference SR score two words respectively shown:
s1 (w1k , w2k ) = 1 |SR(w1 , w2 ) SR(w1k , w2k )|

(11)

Essentially, s1 expresses horizontal analogy candidate pair (w1k , w2k ) given
pair (w1 , w2 ). Similarly, capture notion vertical analogy two pairs
computing difference SR scores among two pairs words, follows:
s2 (w1k , w2k ) = 1 |SR(w1 , w1k ) SR(w2 , w2k )|

(12)

Finally, rank candidates depending combined vertical horizontal analogy
given pair, according following equation:
s(w1k , w2k ) =

s1 (w1k , w2k ) + s2 (w1k , w2k )
2

(13)

Eventually, select candidate pair maximum combined score, taking account
aspects (horizontal vertical) analogy given candidate pairs.
intuition behind selection two scores handling SAT test,
following. order words pairs (both target candidates) random. Usually,
given pair (w1 , w2 ), candidate pairs (w1k , w2k ) test solved one successfully
7. Two objects, X Y, attributionally similar attributes X similar attributes Y. Two pairs, A:B C:D,
relationally similar relations B similar relations C D.

18

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Stem: wallet : money

Choices:

(a)

safe : lock

(b)

suitcase : clothing

S1: 0.2605
S2: 6.75E-04
(c)

camera : film

S1: 0.4795
S2: 0.015
(d)

setting : jewel

S1: 0.1805
S2: 7.87E-05
(e)

car : engine

S1: 0.3764
S2: 8.99E-05

Winner based S1 (Horizontal Analogy): b
Winner based S2 (Vertical Analogy): b
Winner based combined S: b
Correct Answer: b

S1: 0.1506
S2: 0.0029

Figure 4: Example computing Semantic Relatedness measure (SR) given Scholastic Aptitude Test (SAT) question.

find analogy: w1k w2k w1 w2 . perspective, s1 s2 try find
candidate pair best aligns target pair. Figure 4 illustrates two types analogies
(horizontal vertical) example SAT question.
order motivate selection s1 s2 answering SAT questions,
discuss detail two quantities pertain concepts strength type
relations pair SAT words. Turney (2006) describes method comparing
relations candidate word pairs stem word pair, utilizes type
relation connecting words pair finally selects pair best matches
type relation connecting words stem pair. Though explicitly examine
label edges connecting words pair, implicitly computing SR
them. Since weighting WordNet edges fine grained, distinguishes every
type semantic relation WordNet, instead labels, using edge weights. SR definition
provide fine grained distinguishment two pairs words, depending types
edges connecting words respectively, expressed weights, also taking
account factors, like depth nodes comprising connecting path inside
thesaurus. Besides s1 , attempts capture aforementioned properties word pairs,
s2 attempts words order among two word pairs (i.e., first word
first pair, second word second pair). forms attempt capture
aligned two word pairs, according SR values words.
3.4.3 PARAPHRASE R ECOGNITION



ENTENCE - -S ENTENCE IMILARITY

Performance applications relying natural language processing may suffer fact
processed documents might contain lexically different, yet semantically related, text segments.
task recognizing synonym text segments, better known paraphrase recognition,
detection, challenging difficult solve, shown work Pasca (2005). task
important many text related applications, like summarization (Hirao, Fukusima, Oku19

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

mura, Nobata, & Nanba, 2005), information extraction (Shinyama & Sekine, 2003) question
answering (Pasca, 2003). experimentally evaluate application Omiotis paraphrasing
detection task (Section 4.2), using Microsoft Research Paraphrase Corpus (Dolan et al., 2004).
application Omiotis paraphrase detection straightforward: given pair text segments,
compute Omiotis score them, using Equation 10 Algorithm 2. Higher values
Omiotis given pair denote stronger semantic relation examined text segments.
task reduced define threshold, Omiotis value considered
determining sign paraphrasing pair. experimental evaluation Omiotis, explain
detail selected threshold paraphrase recognition task.
similar manner, using Equation 10 Algorithm 2, semantic relatedness scores
pairs sentences computed. task, using data set Li et al. (2006)
evaluate Omiotis, comprising 30 sentence pairs, human scores provided. Section 4
describe detail experimental set up.
3.5 Complexity Implementation Issues
computation Omiotis entails series steps, complexity strongly related
base measure Semantic Relatedness (SR). Primarily, given two words, w1 w2 construction
time semantic network used compute SR according Algorithm 1, proven
O(2 k l+1 ) (Tsatsaronis et al., 2007), k maximum branching factor used
thesaurus nodes l maximum semantic path length thesaurus. semantic
network constructed, complexity Algorithm 1 reduced standard time complexity
cost Dijkstras algorithm. Using Fibonacci heaps, possible alleviate computational
burden Dijkstra improve time complexity. semantic network, Dijkstra takes
O(nL + mD + nE), n number nodes network, number edges, L
time insert, time decrease-key E time extract-min. Fibonacci heaps
used L = = O(1) cost extract-min O(logn), thus significantly reducing
cost execution. whole procedure repeated 2 n1 n2 times computation
Omiotis two documents d1 d2 total n1 n2 distinct words respectively.
aforementioned, obvious computation Omiotis cheap general.
purpose, order improve systems scalability, pre-computed stored
SR values every possible pair synsets RDBMS. one-time computation
cost, dramatically decreases computational complexity Omiotis. database schema
three entities, namely Node, Edge Paths. Node contains WordNet synsets. Edge indexes
edges WordNet graph adding weight information edge computed using SR
measure. Finally, Paths contains pairs WordNet synsets directly indirectly connected WordNet graph computed relatedness. pairs found running
Breadth First Search (BFS) starting WordNet roots POS. Table 2 provides statistical
information RDBMS exceeds 220 Gbytes size. Column 1 indicates number
distinct synsets examined, column 2 shows total number edges, column 3 depicts
number connected synsets (by least one path following offered WordNet edges).
current implementation takes advantage database structures (indices, stored procedures etc)
order decrease computational complexity Omiotis. following example indicative
complexity SR computation. average number senses per term 5 7
2
(depending POS). pair terms known POS, perform n2 (n 6) combinations
20

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Distinct Synsets
115,424

Total Edges
324,268

Connected Synset Pairs
5,591,162,361

Table 2: Statistics WordNet graph implemented database.

pair synsets compute similarity presented Definition 3.
similarities pre-computed, time required processing 100 pairs terms 1 sec,
makes computation Omiotis feasible scalable. proof concept, developed
on-line version SR Omiotis measures8 , user test term-to-term
sentence-to-sentence semantic relatedness measures (Tsatsaronis et al., 2009).

4. Experimental Evaluation
experimental evaluation Omiotis two-fold. First, test performance wordto-word semantic relatedness measure (SR), Omiotis based, three types tasks: (a)
word-to-word similarity relatedness, (b) synonym identification, (c) Scholastic Aptitude
Test (SAT). Second, evaluate performance Omiotis two tasks: (a) sentence-to-sentence
similarity, (b) paraphrase recognition task.
4.1 Evaluation Semantic Relatedness (SR) Measure
evaluation proposed semantic relatedness measure two terms experimented three different categories tests. first category comprises data sets contain
word pairs, human subjects provided similarity scores relatedness scores.
provided scores create ranking word pairs, similar irrelevant.
evaluate performance measures, computing correlation list human
rankings list produced measures. task, evaluate performance SR
three benchmark data sets, namely Rubenstein Goodenough 65 word pairs (1965) (R&G),
Miller Charles 30 word pairs (1991) (M&C), humans provided similarity scores, and, also, Word-Similarity-353 collection (Finkelstein et al., 2002) (353-C),
comprises 353 word pairs, humans provided relatedness scores.
second category experiments comprises synonym identification tests. tests, given
initial word, appropriate synonym word must identified among given options.
task evaluate performance SR TOEFL data set, comprising 80 multiple
choice synonym questions, ESL data set, comprising 50 multiple choice synonym questions
questions.9
third category experiments based Scholastic Aptitude Test (SAT) questions.
SAT, given pair words, relevant pair among five given pairs must selected.
task based word analogy identification. evaluation data set comprises 374 test questions.
8. Publicly available http://omiotis.hua.gr
9. http://www.aclweb.org/aclwiki/index.php?title=TOEFL_Synonym_Questions
http://www.aclweb.org/aclwiki/index.php?title=ESL_Synonym_Questions_(State_of_the_art)

21

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Category
Lexicon-based

Corpus-based

Hybrid

Method
HS
LC
JS
GM
WLM
SP
IS-A SP
JC
L
R
HR
SR

R&G
Spearmans Pearsons r
0.745
0.786

0.785
0.838
N/A
0.818

0.816
N/A

0.64
N/A
N/A
0.52
N/A
0.70

0.709
0.781

0.77
0.818
0.7485
0.778
0.817
N/A
0.8614
0.876

M&C
Spearmans Pearsons r
0.653
0.744

0.748
0.816
N/A
0.878
0.723
N/A

0.70
N/A
N/A
0.47
N/A
0.69
0.805
0.85
0.767
0.829
0.737
0.774
0.904
N/A
0.856
0.864

Table 3: Spearmans Pearsons correlations Rubenstein Goodenough (R&G)
Miller Charles (M&C) data sets. Confidence levels: =0.90, =0.95, =0.99

4.1.1 E VALUATION



EMANTIC IMILARITY



R ELATEDNESS

first category experiments, compared measure ten known measures
semantic relatedness: Hirst St-Onge (1998) (HS), Jiang Conrath (1997) (JC), Leacock
et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz Szpakowicz (2003) (JS),
Gabrilovich Markovitch (2007, 2009) (GM), Milne Witten (2008) (WLM), Finkelstein et al.
(2002) (LSA), Hughes Ramage (2007) (HR), Strube Ponzetto (2006, 2007a) (SP).
measure Strube Ponzetto also included results version measure
based IS-A relations (Ponzetto & Strube, 2007b) (IS-A SP). measure, including
measure (SR), computed Spearman rank order correlation coefficient
() Pearson product-moment correlation coefficient (r), derived r, since
computation relatedness scores transformed rankings. correlation
coefficients computed based relatedness scores rankings provided humans
three data sets (the relatedness scores create ranking pairs words, based
similarity). measures HS, JC, LC, L R, rankings relatedness scores
word pairs R&G M&C data sets, given work Budanitsky Hirst
(2006). JS measure, r value given work Jarmasz Szpakowicz (2003)
R&G M&C data sets, value given work Gabrilovich
Markovitch (2007). GM measure values given work Gabrilovich
Markovitch (2007). WLM measure values given work Milne Witten
(2008). LSA method value given work Gabrilovich Markovitch (2007),
353-C data set. HR measure values given work Hughes
Ramage (2007). Finally, SP measure r values given work Ponzetto
Strube (2007a), IS-A SP given work Ponzetto Strube (2007b).
22

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Table 3 show values r R&G M&C data sets SR
compared measures. human scores pairs words two data sets
found analysis Budanitsky Hirst (2006). Note M&C data set subset
R&G data set. cases, computation r feasible, due missing
information regarding detailed rankings relatedness scores respective measures.
cases table entry N/A. Also LSA measure omitted table
r reported literature two data sets. also conducted statistical
significance test difference SR correlations respective correlations
compared measures, using Fishers z-transformation (Fisher, 1915). reported number,
symbol indicates difference correlation produced SR respective
measure statistically significant 0.99 confidence level (p < 0.01). symbol indicates
0.95 confidence level (p < 0.05) and, finally, symbol indicates statistical
significance correlations difference 0.90 confidence level (p < 0.10). cases
difference statistically significant confidence levels, indicating
symbol.
Table 4 show values r 353-C data set. reason present results
experiments 353-C data set another table respective results R&B
M&C data sets collection focuses concept semantic relatedness, rather
concept semantic similarity (Gabrilovich & Markovitch, 2007). Relatedness general
concept similarity, argued analysis Budanitsky Hirst (2006). Thus,
argued humans 353-C thought differently scoring, compared case
R&B M&C data sets. detailed human scores 353-C data set made available
collection10 . measures L, JC HS omitted, information available
computing r values. remark regarding 353-C collection, need add fact
cases inter-judge correlations may fall 65%, R&B M&C
data sets inter-judge correlations 0.88 0.95. Again, statistical significance tests
conducted using Fishers z-transformation, regarding difference SR correlations
correlations compared measures. used symbols indicate level
statistical significance previously. regards reported correlations
R&G M&C data sets, shown SR performs well, since majority cases
SR higher correlation compared measures semantic relatedness similarity
category (knowledge-based, corpus-based hybrid). R&G data set SR reports
highest r correlations. M&C data set SR second highest correlation.
HR measure highest correlation, R&G 353-C SR outperforms HR.
differences SR HR statistically significant two examined data sets.
Also, M&C data set SR second r correlation JS reporting highest,
JS outperformed SR R&G 353-C data sets. case M&C data set,
difference SR JS statistically significant, SR outperforms JS R&G
353-C data sets, statistically significant difference reported correlations. Another
important conclusion results, fact IS-A SP measure performs better
SP measure. mainly due fact computation similarity values
data sets, inclusion IS-A relations much reasonable (Ponzetto & Strube, 2007b).
differences results (SP IS-A SP) motivate even SR measure, since
10. http://www.cs.technion.ac.il/gabr/resources/data/wordsim353/

23

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Category
Lexicon-based

Corpus-based

Hybrid

Method
LC
JS
GM
WLM
LSA
SP
R
HR
SR

353-C
Spearmans Pearsons r
N/A
0.34

0.55
N/A

0.75
N/A
0.69
N/A

0.56
N/A
N/A
0.49
N/A
0.34

0.552
N/A
0.61
0.628

Table 4: Spearmans Pearsons correlations 353 word pairs (353-C) data set. Confidence
levels: =0.90, =0.95, =0.99

take best worlds, i.e., weigh IS-A relations high, fall back relations
necessary.
Regarding 353-C data set, results Table 4 show SR performs well,
top performers Wikipedia-based approaches (Gabrilovich & Markovitch, 2009; Milne
& Witten, 2008). difference statistically significant, note
SR outperforms GM WLM R&G M&C data sets, statistically significant
difference well. Partly, difference performance SR compared GM WLM
explained follows: GM measure considers words context (Gabrilovich & Markovitch,
2009), thus inherently performs word sense disambiguation; contrast, SR takes input pair
words, lacks context, based information existing WordNet, which, especially
several cases 353-C data set, creates disadvantage (e.g., word pair Arafat
Jackson, 11 different entries second word WordNet). holds
WLM measure. Another reason difference performance coverage WordNet.
several cases, one two words 353-C data set comprising pair, exist
WordNet (e.g., football player Maradona). However, expected, also shown
experimental analysis Omiotis follows, context considered, proposed semantic
relatedness measure performs better (the reader may wish consult Table 9, subset
R&G data set contains full definitions words, correlations Omiotis
human judgements top found among compared approaches).
visualize performance measure comprehensible manner, also present
Figure 5 relatedness values given humans pairs R&G M&C data sets,
increasing order value (left side) respective values pairs produced using SR
(right side). Note x-axis charts begins least related pair terms, according
humans, continues related pair terms. y-axis left chart
respective humans rating pair terms. right figure shows SR pair. closer
look Figure 5 reveals values produced SR (right figure) follow pattern similar
human ratings (left figure).
24

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

HUMAN RATINGS HUMAN RANKINGS - R&G Data Set

SEMANTIC RELATEDNESS HUMAN RANKINGS - R&G Data Set

4
0.9
Semantic Relatedness

Human Rating

3.5
3
2.5
2
1.5
1
0.5

20

30

40

50

0.7
0.6
0.5
0.4
0.3
0.2

correlation human pairs ranking human ratings

correlation human pairs ranking semantic relatedness

0.1

0
10

0.8

60

65

10

20

30

40

50

60

Pair Number

Pair Number

HUMAN RATINGS HUMAN RANKINGS - M&C Data Set

SEMANTIC RELATEDNESS HUMAN RANKINGS - M&C Data Set

65

4
Semantic Relatedness

Human Rating

3.5
3
2.5
2
1.5
1
0.5

correlation human pairs ranking human ratings

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

correlation human pairs ranking semantic relatedness

0
5

10

15

20

25

30

5

Pair Number

10

15

20

25

30

Pair Number

Figure 5: Correlation human ratings Semantic Relatedness measure (SR) Rubenstein Goodenough (R&G) Miller Charles (M&C) data sets.

4.1.2 E VALUATION



YNONYM DENTIFICATION

synonym identification task using TOEFL 80 questions data set ESL 50
questions data set. TOEFL data set compare several methods. specifically, examine: lexicon-based measures Leacock et al. (1998) (LC), Hirst St-Onge
(1998) (HS), Jarmasz Szpakowicz (2003) (JS); corpus-based measures Landauer
Dumais (1997) (LD), Pado Lapata (2007) (PL), Turney (2008b) (T), Terra Clarke (2003)
(TC), Matveeva et al. (2005) (M); hybrid measures Resnik (1995) (R), Lin (1998) (L),
Jiang Conrath (1997) (JC), Turney et al. (2003) (PR); Web-based method RuizCasado et al. (2005) (RC). also report results random guessing (RG) performance
average college applicant (H). Table 5 shows results 80 TOEFL questions.
table reports number correct respective percentage given measures. order
test statistical significance differences measures performance, conducted
Fishers Exact Test (Agresti, 1990). previous tables, symbol indicates statistically
significant difference 0.99 confidence level, 0.95 confidence level, 0.90
confidence level. results Table 5 show SR ranks second among reported methods,
best method hybrid PR (Turney et al., 2003). regards comparison
lexicon-based methods, SR reports better results, statistically significant confidence levels
indicated.
similar manner, conducted experiments ESL 50 questions data set,
compare results with: lexicon-based measures Leacock et al. (1998) (LC), Hirst StOnge (1998) (HS), Jarmasz Szpakowicz (2003) (JS); corpus-based measures Turney
(2001) (PMI-IR), Terra Clarke (2003) (TC); hybrid measures Resnik (1995) (R),
25

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Category
Lexicon-Based

Corpus-Based

Hybrid
Web-Based


Method
LC
HS
JS
LD
PL

TC

R
L
JC
PR
RC
RG
H
SR

#Correct Answers
17
62
63
52
58
61
65
69
16
19
20
78
66
20
52
70

Percentage Correct Answers
0.212
0.775
0.787
0.65
0.725
0.762
0.812
0.862
0.2
0.237
0.25
0.975
0.825
0.25
0.65
0.875

Table 5: Number percentage correct answers TOEFL 80 questions test. Confidence
levels: =0.90, =0.95, =0.99

Category
Lexicon-Based
Corpus-Based
Hybrid


Method
LC
HS
JS
PMI-IR
TC
R
L
JC
RG
SR

#Correct Answers
18
31
41
37
40
16
18
18
20
41

Percentage Correct Answers
0.36
0.62
0.82
0.74
0.8
0.32
0.36
0.36
0.25
0.82

Table 6: Number percentage correct answers ESL 50 questions test. Confidence levels:
=0.95, =0.99

Lin (1998) (L), Jiang Conrath (1997) (JC). report results, together random
guessing, Table 6. results Table 6 show SR ranks first, performance
JS data set, outperforming compared corpus-based methods.
26

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Category
Lexicon-Based
Corpus-Based
Hybrid
Web-Based


Method
LC
HS
V
LRA
PMI-IR
R
L
JC
B
RG
S1
S2

NB
UB

#Correct Answers
117
120
161
210
131
124
102
102
150
75
106
114
128
142
196

Percentage Correct Answers
0.313
0.321
0.43
0.561
0.35
0.332
0.273
0.273
0.4
0.2
0.283
0.304
0.342
0.381
0.524

Table 7: Number percentage correct answers 374 Scholastic Aptitude Test (SAT)
questions. Confidence levels: =0.90, =0.95, =0.99

results interesting, since indicate lexicon-based methods promising
synonym identification tasks.
4.1.3 E VALUATION



SAT NALOGY Q UESTIONS

approach choose evaluate SR analogy task use typical benchmark test
set employed related bibliography, namely Scholastic Aptitude Test (SAT).11 comprises
374 words pairs target pair 5 supplementary pairs words. average US college
applicant answered correctly 57 percent questions, machine-based approach
yet surpassed performance average college applicant.
Table 7, present number correct answers respective percentage (recall)
374 SAT questions, following methods: random guessing (RG), Jiang Conrath (1997)
(JC), Lin (1998) (L), Leacock et al. (1998) (LC), Hirst St-Onge (1998) (HS), Resnik (1995)
(R), Bollegala et al. (2008) (B), Veale (2004) (V), PMI-IR (Turney, 2001) LRA (Turney, 2006).
Furthermore, present results s1 (Equation 11), s2 (Equation 12) (Equation 13).
also present, before, statistical significance differences performance, conducting
Fishers exact test.
Towards direction combining answers s1 s2 different manner
naive average, also report upper bound performance attempt. computed
simply finding union correct answers s1 s2 may provide. reported
table (UB). effort design learning mechanism would learn select
11. Many thanks Peter Turney, providing us standard set experimentation, comprising 374 SAT questions.

27

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

s1 s2 answers SAT question, goal reach upper-bound, designed
implemented simple representation SAT questions training instances.
SAT question, created training instance 6 features: minimum s1 value found
question (among five computed values possible pairs), maximum s1 value,
difference. also added features regarding s2 . trained tested
Naive Bayes classifier using ten-fold cross validation 374 SAT questions. results
experiment shown table NB (Naive Bayes). Finally, also present top results ever
reported literature specific data set, LRA method Turney (2006).
reported table (LRA).
results presented Table 7 show ranks second among compared lexicon-based
measures first measure Veale (2004) (V). method Bollegala et al. (2008)
(B) achieves higher score SR, needs training SAT questions. point
note LRA method needs almost 8 days process 374 SAT questions (Turney, 2006),
(B) needs around 6 hours (Bollegala et al., 2008), needs less 3 minutes.
Furthermore, fact combining s1 s2 reach 52.4% shows produce
promising results, classifier learns successfully combine them. N B results,
simple attempt construct learner features, shows important boost
performance 4.1%. proper feature engineering task, training SAT questions
potentially yield promising results, gap 38.1% upper bound
52.4% still large. all, results prove lexicon-based relatedness measure
comparable performance state art measures SAT task, smaller
execution time majority methods outperform recall.
4.2 Evaluation Omiotis Measure
order evaluate performance Omiotis measure, performed two experiments
test ability measure capture similarity sentences. first experiment
based data set produced Li et al. (2006). second experiment based paraphrase
recognition task, using Microsoft Research Paraphrase Corpus (Dolan et al., 2004).
4.2.1 E VALUATION



ENTENCE IMILARITY

data set produced Li et al. (2006) comprises 65 sentence pairs (each pair consists two
sentences respective dictionary word definitions R&G 65 word pairs data set).
used dictionary Collins Cobuild dictionary (Sinclair, 2001). sentence pair,
similarity scores provided 32 human participants, ranging 0.0 (the sentences
unrelated meaning), 4.0 (the sentences identical meaning).12 .
65 sentence pairs, Li et al. (2006) decided keep subset 30 sentence pairs,
similarly process applied Miller Charles (1991), order retain sentences whose
human ratings create even distribution across similarity range. Thus, apply Omiotis
subset 65 sentence pairs, described Li et al. (2006). data set,
compare Omiotis STASIS measure semantic similarity, proposed Li et al. (2006),
LSA-based approach described OShea et al. (2008), STS measure proposed Islam
Inkpen (2008). best knowledge, data set used three
12. data set publicly available http://www.docm.mmu.ac.uk/STAFF/J.Oshea/

28

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

previous works. Table 8 present sentence pairs used, respective scores humans,
STASIS, LSA, STS, Omiotis.
Sentence Pair
1.cord:smile
5.autograph:shore
9.asylum:fruit
13.boy:rooster
17.coast:forest
21.boy:sage
25.forest:graveyard
29.bird:woodland
33.hill:woodland
37.magician:oracle
41.oracle:sage
47.furnace:stove
48.magician:wizard
49.hill:mound
50.cord:string
51.glass:tumbler
52.grin:smile
53.serf:slave
54.journey:voyage
55.autograph:signature
56.coast:shore
57.forest:woodland
58.implement:tool
59.cock:rooster
60.boy:lad
61.cushion:pillow
62.cemetery:graveyard
63.automobile:car
64.midday:noon
65.gem: jewel

Human
0.01
0.005
0.005
0.108
0.063
0.043
0.065
0.013
0.145
0.13
0.283
0.348
0.355
0.293
0.47
0.138
0.485
0.483
0.36
0.405
0.588
0.628
0.59
0.863
0.58
0.523
0.773
0.558
0.955
0.653

STASIS
0.329
0.287
0.209
0.53
0.356
0.512
0.546
0.335
0.59
0.438
0.428
0.721
0.641
0.739
0.685
0.649
0.493
0.394
0.517
0.55
0.759
0.7
0.753
1
0.663
0.662
0.729
0.639
0.998
0.831

LSA
0.51
0.53
0.505
0.535
0.575
0.53
0.595
0.505
0.81
0.58
0.575
0.715
0.615
0.54
0.675
0.725
0.695
0.83
0.61
0.7
0.78
0.75
0.83
0.985
0.83
0.63
0.74
0.87
1
0.86

STS
0.06
0.11
0.07
0.16
0.26
0.16
0.33
0.12
0.29
0.20
0.09
0.30
0.34
0.15
0.49
0.28
0.32
0.44
0.41
0.19
0.47
0.26
0.51
0.94
0.60
0.29
0.51
0.52
0.93
0.65

Omiotis
0.1062
0.1048
0.1046
0.3028
0.2988
0.243
0.2995
0.1074
0.4946
0.1085
0.1082
0.2164
0.5295
0.5701
0.5502
0.5206
0.5987
0.4965
0.4255
0.4287
0.9308
0.612
0.7392
0.9982
0.9309
0.3466
0.7343
0.7889
0.9291
0.8194

Table 8: Human, STASIS, LSA, STS Omiotis scores 30 sentence pairs.
Table 9 present results comparison, comprising reporting Spearmans
rank order correlation coefficient Pearsons product moment correlation coefficient r
STASIS, LSA, STS, Omiotis. also included results, version Omiotis
take account inter-POS relations (i.e., relations cross parts speech).
version Omiotis indicated table SimpleOmiotis. objective experiment
measure contribution relations cross parts speech computation text-to29

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

STASIS
LSA
STS
Simple Omiotis
Omiotis
Average Participant
Worst Participant
Best Participant

Spearmans
0.8126
0.8714
0.838
0.6889
0.8905
N/A
N/A
N/A

Pearsons r
0.8162
0.8384
0.853
0.7277
0.856
0.825
0.594
0.921

Table 9: Spearmans Pearsons correlations human similarity ratings. Confidence levels:
=0.95, =0.99

text semantic relatedness values, though types relations reported previous
bibliography advantageous (Jarmasz, 2003; Jarmasz & Szpakowicz, 2003), individual
contribution never measured.
also show r correlation average participant (mean individuals group;
n = 32, leave-one-out resampling standard deviation 0.072), worst participant (worst participant group; n = 32, leave-one-out resampling) best participant (best participant
group; n = 32, leave-one-out resampling), taken work OShea et al. (2008).
addition, also conducted z-test regarding difference Omiotis correlations
compared measures correlations. symbols used previous tables indicate confidence level statistical significance. Note, also, reported correlations (STASIS, LSA,
STS, Omiotis) individually constitute statistically significant positive correlations human scores (r) rankings (). results indicate, Omiotis best correlation, according
r values, compared STASIS, LSA, STS. Furthermore, contribution semantic relations cross parts speech obvious, since difference simple version
Omiotis omits defined Omiotis measure large statistically significant
0.99 confidence level. Overall, results indicate Omiotis applied successfully
computation similarities small text segments, like sentences.
4.2.2 E VALUATION



PARAPHRASE R ECOGNITION

order evaluate performance Omiotis measuring semantic relatedness small text segments, conducted additional experiments paraphrase recognition task
using test pairs Microsoft Research Paraphrase Corpus (Dolan et al., 2004).
original data set, containing training test pairs, run experiments 1725 test
pairs text segments, collected news sources Web period
18 months. pair, human subjects determined whether two texts pair
consists paraphrase (direction issue). reported inter-judge agreement
annotators 83%. paraphrase recognition task widely studied past,
since important many natural language applications, like question answering (Harabagiu
30

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Category
Baselines
Corpus-based

Lexicon-based

Machine learning-based

Method
Random
VSM Cosine
PMI-IR
LSA
STS
JC
LC
Lesk
L
WP
R
Comb.
Wan et al.
Zhang Patrick
Qiu et al.
Finch et al.
Omiotis

Accuracy
51.3
65.4
69.9
68.4
72.6
69.3
69.5
69.3
69.3
69
69
70.3
75
71.9
72
74.96
69.97

Precision
68.3
71.6
70.2
69.7
74.7
72.2
72.4
72.4
71.6
70.2
69
69.6
77
74.3
72.5
76.58
70.78

Recall
50
79.5
95.2
95.2
89.1
87.1
87
86.6
88.7
92.1
96.4
97.7
90
88.2
93.4
89.8
93.4

F-Measure
57.8
75.3
81
80.5
81.3
79
79
78.9
79.2
80
80.4
81.3
83
80.7
81.6
82.66
80.52

Table 10: Omiotis competitive methods performance Microsoft Research Paraphrase
Corpus (MSR).

& Hickl, 2006), text summarization (Madnani, Zajic, Dorr, Fazil Ayan, & Lin, 2007).
task computed Omiotis sentences every pair marked paraphrases
pairs Omiotis value greater threshold. threshold set 0.2, tuning
training set. used simple approach tuning, namely forward hill-climbing beam
search (Guyon, Gunn, Nikravesh, & Zadeh, 2006).
compare performance Omiotis several methods various categories;
precisely, against: (a) two baseline methods, random selection method marks randomly
pair paraphrase (Random), vector-based similarity measure, using
cosine similarity measure TF-IDF weighting features (VSM Cosine) 13 , (b) corpusbased methods; PMI-IR proposed Turney (2001), LSA-based approach introduced
Mihalcea et al. (2006), STS measure proposed Islam Inkpen (2008), (c) lexiconbased methods; Jiang Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik
(1995, 1999) (R), Lesk (1986) (Lesk), Wu Palmer (1994) (WP), metric combines
measures category, proposed Mihalcea et al. (2006) (Comb.), (d) machine-learning
based techniques, also constitute state art paraphrase recognition, like method
Wan et al. (2006), trains classifier lexical dependency similarity measures,
method Zhang Patrick (2005), also build feature vector lexical similarities
sentence pairs (e.g., edit distance, number common words), method Qiu et al.
13. features words used data set.

31

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

(2006), use SVM classifier (Vapnik, 1995) decide whether set features
sentence created parsing semantic role labelling matches respective
set second sentence pair, importance, and, finally, method Finch
et al. (2005), also train SVM classifier based machine translation evaluation metrics.
results evaluation shown Table 10. results indicate Omiotis surpasses
lexicon-based methods, matches combined method Mihalcea et al. (2006).
point must mention also tuned Omiotis goal maximize F-Measure
test set, cost dropping precision favor recall. type tuning reported FMeasure 81.7, larger F-Measures lexicon-based, corpus-based
two machine learning-based approaches. Even though reported results used different
simpler tuning explained previously, still results indicate Omiotis manages well
paraphrase recognition task produces comparable results state art.
believe used part machine learning-based method, since one best
choices lexicon-based methods paraphrase recognition, also constitutes part
plan future work application.

5. Conclusions Future Work
paper presented new measure text semantic relatedness. major strength
measure lies formulation semantic relatedness words. Experimental evaluation, proved measure approximates human understanding semantic relatedness
words better previous related measures. combination path length, nodes depth
edges type single formula allowed us apply semantic relatedness measure different
text-based tasks good performance. specifically, SR measure outperformed overall used data sets state art measures word-to-word tasks Omiotis measure
performed well sentence similarity paraphrase recognition tasks. Although,
results word analogy task satisfactory, since special tuning performed,
confident still place improvement. extensive evaluation SR Omiotis
several applications shows capabilities measures proves applied
several text related tasks. next plans apply relatedness measures applications, text classification clustering, keyword sentence extraction, query
expansion, compare state art techniques field. Finally, improving
supporting infrastructure order facilitate large scale tasks document clustering text
retrieval.

Acknowledgments
Part work done George Tsatsaronis Department Informatics Athens
University Economics Business. would like thank Kjetil Nrvag constructive
comments, Ion Androutsopoulos feedback early stage work. would
also like thank anonymous reviewers detailed feedback.
32

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

References
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasca, M., & Soroa, A. (2009). study
similarity relatedness using distributional wordnet-based approaches.. Proceedings Human Language Technologies: 2009 Annual Conference North American
Chapter Association Computational Linguistics (NAACL), pp. 1927.
Agirre, E., & Rigau, G. (1995). proposal word sense disambiguation using conceptual distance. Proceedings International Conference Recent Advances Natural Language Processing (RANLP).
Agresti, A. (1990). Categorical Data Analysis. Wiley, Hoboken, NJ.
Aizawa, A. (2003). information-theoretic perspective TF-IDF measures. Information Processing Management, 39(1), 4565.
Banerjee, S., & Pedersen, T. (2003). Extended gloss overlaps measure semantic relatedness.
Proceedings Eighteenth International Joint Conference Artificial Intelligence
(IJCAI), pp. 805810.
Barzilay, R., & Elhadad, M. (1997). Using lexical chains text summarization. Proceedings
ACL 97/EACL 97 Workshop Intelligent Scalable Text Summarization, pp. 1017.
Barzilay, R., Elhadad, M., & McKeown, K. (2002). Inferring strategies sentence ordering
multidocument news summarization. Journal Artificial Intelligence Research, 17, 3555.
Basili, R., Cammisa, M., & Moschitti, A. (2005). semantic kernel exploit linguistic knowledge. Proceedings Advances Artificial Intelligence, Ninth Congress Italian
Association Artificial Intelligence (AI*IA), pp. 290302.
Bicici, E., & Yuret, D. (2006). Clustering word pairs answer analogy questions. Proceedings
Fifteenth Turkish Symposium Artificial Intelligence Neural Networks.
Bollegala, D., Matsuo, Y., & Ishizuka, M. (2008). WWW sits sat: Measuring relational similarity web. Proceedings Eighteenth European Conference Artificial
Intelligence (ECAI), pp. 333337.
Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures lexical semantic relatedness. Computational Linguistics, 32(1), 1347.
Clough, P., & Stevenson, M. (2004). Cross-language information retrieval using EuroWordNet
word sense disambiguation. Proceedings Twenty Sixth European Conference
Information Retrieval (ECIR), pp. 327337.
Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction Algorithms. MIT Press.
Dolan, W., Quirk, C., & Brockett, C. (2004). Unsupervised construction large paraphrase corpora:
Exploiting massively parallel news sources. Proceedings Twentieth International
Conference Computational Linguistics (COLING).
Fellbaum, C. (1998). WordNet electronic lexical database. MIT Press.
33

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Finch, A., Hwang, Y., & Sumita, E. (2005). Using machine translation evaluation techniques determine sentence-level semantic equivalence. Proceedings 3rd International Workshop Paraphrasing,.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin, E. (2002).
Placing search context: concept revisited. ACM Transactions Information Systems,
20(1), 116131.
Fisher, R. (1915). Frequency distribution values correlation coefficient samples
indefinitely large population. Biometrika, 10, 507521.
Gabrilovich, E., & Markovitch, R. (2007). Computing semantic relatedness using Wikipedia-based
explicit semantic analysis. Proceedings Twentieth International Joint Conference
Artificial Intelligence (IJCAI), pp. 16061611.
Gabrilovich, E., & Markovitch, R. (2009). Wikipedia-based semantic interpretation natural
language processing. Journal Artificial Intelligence Research, 34, 443498.
Gentner, D. (1983). Structure-mapping: theoretical framework analogy. Cognitive Science,
7(2), 155170.
Guyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. (2006). Feature Extraction, Foundations
Applications. Springer.
Harabagiu, S., & Hickl, A. (2006). Methods using textual entailment open-domain question
answering.. Proceedings Joint Conference International Committee Computational Linguistics Association Computational Linguistics (COLING-ACL),
pp. 905912.
Hirao, T., Fukusima, T., Okumura, M., Nobata, C., & Nanba, H. (2005). Corpus evaluation
measures multiple document summarization multiple sources. Proceedings
Twentieth International Conference Computational Linguistics (COLING), pp. 535541.
Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detection
correction malapropisms. WordNet: Electronic Lexical Database, chapter 13, pp.
305332 Cambridge. MIT Press.
Hughes, T., & Ramage, D. (2007). Lexical semantic relatedness random graph walks. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP),
pp. 581589.
Ide, N., & Veronis, J. (1998). Word Sense Disambiguation: State Art. Computational
Linguistics, 24(1), 140.
Islam, A., & Inkpen, D. (2008). Semantic text similarity using corpus-based word similarity
string similarity. ACM Transactions Knowledge Discovery Data, 2(2), 125.
Jaccard, P. (1901). Etude comparative de la distribution florale dans une portion des alpes et des
jura.. Bulletin del la Societe Vaudoise des Sciences Naturelles, 37, 547579.
34

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Jarmasz, M. (2003). Rogets thesaurus semantic similarity. Masters Thesis, University
Ottawa.
Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus semantic similarity. Proceedings
International Conference Recent Advances Natural Language Processing (RANLP),
pp. 212219.
Jiang, J., & Conrath, D. (1997). Semantic similarity based corpus statistics lexical taxonomy. Proceedings International Conference Research Computational Linguistics
(ROCLING X), pp. 1933.
Kucera, H., Francis, W., & Caroll, J. (1967). Computational Analysis Present Day American
English. Brown University Press.
Landauer, T., & Dumais, S. (1997). solution Platos problem: latent semantic analysis
theory acquisition, induction, representation knowledge. Psychological Review,
104(2), 211240.
Landauer, T., Foltz, P., & Laham, D. (1998). Introduction latent semantc analysis. Discourse
Processes, 25, 259284.
Leacock, C., Miller, G., & Chodorow, M. (1998). Using corpus statistics WordNet relations
sense identification. Computational Linguistics, 24(1), 147165.
Lesk, M. (1986). Automated sense disambiguation using machine-readable dictionaries:
tell pine cone ice cream cone. Proceedings Fifth Annual International
Conference Systems Documentation (SIGDOC), pp. 2426.
Li, P. (2008). Estimators tail bounds dimension reduction l (0 < 2) using stable
random projections. Proceedings Nineteenth Annual ACM-SIAM Symposium
Discrete Algorithms (SODA), pp. 1019.
Li, Y., McLean, D., Bandar, Z., OShea, J., & Crockett, K. (2006). Sentence similarity based
semantic nets corpus statistics. IEEE Transactions Knowledge Data Engineering,
18(8), 11381150.
Lin, D. (1998). information-theoretic definition similarity. Proceedings Fifteenth
International Conference Machine Learning (ICML), pp. 296304.
Madnani, N., Zajic, D., Dorr, B., Fazil Ayan, N., & Lin, J. (2007). Multiple alternative sentence
compressions automatic text summarization. Proceedings HLT/NAACL Document Understanding Conference (DUC).
Matveeva, I., Levow, G., Farahat, A., & Royer, C. (2005). Generalized latent semantic analysis
term representation. Proceedings International Conference Recent Advances
Natural Language Processing (RANLP).
Mavroeidis, D., Tsatsaronis, G., Vazirgiannis, M., Theobald, M., & Weikum, G. (2005). Word sense
disambiguation exploiting hierarchical thesauri text classification. Proceedings
Ninth European Conference Principles Practice Knowledge Discovery Databases
(PKDD), pp. 181192.
35

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Mihalcea, R., Corley, C., & Strapparava, C. (2006). Corpus-based knowledge-based measures
text semantic similarity. Proceedings Twenty First Conference Artificial Intelligence (AAAI), pp. 775780.
Mihalcea, R., & Moldovan, D. (1999). method word sense disambiguation unrestricted text.
Proceedings 37th annual meeting Association Computational Linguistics
(ACL), pp. 152158.
Mihalcea, R., Tarau, P., & Figa, E. (2004). PageRank semantic networks application
word sense disambiguation. Proceedings Twentieth International Conference
Computational Linguistics (COLING).
Miller, G., & Charles, W. (1991). Contextual correlates semantic similarity. Language
Cognitive Processes, 6(1), 128.
Milne, D., & Witten, I. (2008). effective, low-cost measure semantic relatedness obtained
Wikipedia links. Proceedings first AAAI Workshop Wikipedia Artificial
Intelligence (WIKIAI).
Morris, J., & Hirst, G. (1991). Lexical cohesion computed thesaural relations indicator
structure text. Computational Linguistics, 17, 2148.
Navigli, R. (2008). structural approach automatic adjudication word sense disagreements. Natural Language Engineering, 14(4), 547573.
OShea, J., Bandar, Z., Crocket, K., & McLean, D. (2008). comparative study two short
text semantic similarity measures. Proceedings Agent Multi-Agent Systems:
Technologies Applications, Second KES International Symposium (KES-AMSTA), pp.
172181.
Pado, S., & Lapata, M. (2007). Dependency-based construction semantic space models. Computational Linguistics, 33(2), 161199.
Palmer, M., Fellbaum, C., & Cotton, S. (2001). English tasks: All-words verb lexical sample.
Proceedings Senseval-2, pp. 2124.
Pasca, M. (2003). Open-domain question answering large text collections. CSLI Studies
Computational Linguistics. CSLI Publications, Distributed University Chicago
Press.
Pasca, M. (2005). Mining paraphrases self-anchored web sentence fragments. Proceedings
Ninth European Conference Principles Practice Knowledge Discovery
Databases (PKDD), pp. 193204.
Patwardhan, S., Banerjee, S., & Pedersen, T. (2003). Using measures semantic relatedness
word sense disambiguation. Proceedings Fourth International Conference Intelligent Text Processing Computational Linguistics (CICLing), pp. 241257.
Patwardhan, S., & Pedersen, T. (2006). Using WordNet based context vectors estimate semantic relatedness concepts. Proceedings EACL 2006 Workshop Making Sense
Sense - Bringing Computational Linguistics Psycholinguistics Together, pp. 18.
36

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Ponzetto, S., & Strube, M. (2007a). Knowledge derived Wikipedia computing semantic
relatedness. Journal Artificial Intelligence Research, 30, 181212.
Ponzetto, S., & Strube, M. (2007b). Deriving large-scale taxonomy Wikipedia. Proceedings Twenty Second Conference Artificial Intelligence (AAAI), pp. 14401445.
Qiu, L., Kan, M., & Chua, T. (2006). Paraphrase recognition via dissimilarity significance classification. Proceedings Conference Empirical Methods Natural Language
Processing (EMNLP), pp. 1826.
Quilian, R. (1969). teachable language comprehender: simulation program theory
language. Communications ACM, 12(8), 459476.
Resnik, P. (1995). Using information content evaluate semantic similarity. Proceedings
Fourteenth International Joint Conference Artificial Intelligence (IJCAI), pp. 448453.
Resnik, P. (1999). Semantic similarity taxonomy: information-based measure application problems ambiguity natural language. Journal Artificial Intelligence
Research, 11, 95130.
Richardson, R., & Smeaton, A. (1995). Using WordNet knowledge-based approach information retrieval. Proceedings BCS-IRSG Colloquium.
Rubenstein, H., & Goodenough, J. (1965). Contextual correlates synonymy. Communications
ACM, 8(10), 627633.
Ruiz-Casado, M., Alfonseca, E., & Castells, P. (2005). Using context-window overlapping synonym discovery ontology extension. Proceedings International Conference
Recent Advances Natural Language Processing (RANLP).
Salton, G., Buckley, C., & Yu, C. (1982). evaluation term dependence models information retrieval. Proceedings Fifth Annual International ACM SIGIR Conference
Research Development Information Retrieval, pp. 151173.
Salton, G., & McGill, M. (1983). Introduction Modern Information Retrieval. McGraw-Hill.
Sanderson, M. (1994). Word sense disambiguation information retrieval. Proceedings
Seventeenth Annual International ACM SIGIR Conference Research Development
Information Retrieval, pp. 142151.
Sanderson, M. (2008). Ambiguous queries: Test collections need sense. Proceedings
Thirty First Annual International ACM SIGIR Conference Research Development
Information Retrieval, pp. 499506.
Shinyama, Y., & Sekine, S. (2003). Paraphrase acquisition information extraction. Proceedings ACL 2nd Workshop Paraphrasing: Paraphrase Acquisition Applications,
pp. 6571.
Sinclair, J. (2001). Collins Cobuild English Dictionary Advanced Learners, 3rd edn. Harper
Collins, New York.
37

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Smeaton, A., Kelledy, F., & ODonnell, R. (1995). TREC-4 experiments Dublin City University:
Thresholding posting lists, query expansion WordNet POS tagging Spanish.
Proceedings Fourth Text REtrieval Conference (TREC).
Snyder, B., & Palmer, M. (2004). English All-words task. Proceedings Senseval-3, pp.
4143.
Song, Y., Han, K., & Rim, H. (2004). term weighting method based lexical chain automatic summarization. Proceedings Fifth International Conference Intelligent Text
Processing Computational Linguistics (CICLing), pp. 636639.
Stokoe, C., Oakes, M., & Tait, J. (2003). Word sense disambiguation information retrieval revisited. Proceedings Twenty Sixth Annual International ACM SIGIR Conference
Research Development Information Retrieval, pp. 159166.
Strube, M., & Ponzetto, S. (2006). WikiRelate! Computing semantic relatedness using Wikipedia.
Proceedings Twenty First Conference Artificial Intelligence (AAAI), pp. 1419
1424.
Sussna, M. (1993). Word sense disambiguation free-text indexing using massive semantic network. Proceedings Second International Conference Information Knowledge
Management (CIKM), pp. 6774.
Terra, E., & Clarke, C. (2003). Frequency estimates statistical word similarity measures.
Proceedings North American Chapter Association Computational Linguistics
- Human Language Technologies Conference (HLT/NAACL)., pp. 244251.
Tsang, V. (2008). Graph Approach Measuring Text Distance. PhD Thesis, University
Toronto.
Tsatsaronis, G., & Panagiotopoulou, V. (2009). generalized vector space model text retrieval
based semantic relatedness. Proceedings 12th Conference European
Chapter Association Computational Linguistics (EACL - Student Research Workshop), pp. 7078.
Tsatsaronis, G., Varlamis, I., Nrvag, K., & Vazirgiannis, M. (2009). Omiotis: thesaurus-based
measure text relatedness. Proceedings European Conference Machine Learning
Principles Practice Knowledge Discovery Databases (ECML-PKDD), pp. 742
745.
Tsatsaronis, G., Varlamis, I., & Vazirgiannis, M. (2008). Word sense disambiguation semantic
networks. Proceedings 11th International Conference Text, Speech Dialogue
(TSD), pp. 219226.
Tsatsaronis, G., Vazirgiannis, M., & Androutsopoulos, I. (2007). Word sense disambiguation
spreading activation networks generated thesauri. Proceedings Twentieth International Joint Conference Artificial Intelligence (IJCAI), pp. 17251730.
Turney, P. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL. Proceedings
Twelfth European Conference Machine Learning (ECML), pp. 491502.
38

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Turney, P. (2006). Similarity semantic relations. Computational Linguistics, 32(3), 379416.
Turney, P. (2008a). latent relation mapping engine: Algorithm experiments. Journal
Artificial Intelligence Research, 33, 615655.
Turney, P. (2008b). uniform approach analogies, synonyms, antonyms, associations.
Proceedings Twenty Second International Conference Computational Linguistics
(COLING), pp. 905912.
Turney, P., & Littman, M. (2005). Corpus-based learning analogies semantic relations.
Machine Learning, 60(1-3), 251278.
Turney, P., Littman, M., Bigham, J., & Shnayder, V. (2003). Combining independent modules
solve multiple-choice synonym analogy problems. Proceedings International
Conference Recent Advances Natural Language Processing (RANLP), pp. 482489.
van Rijsbergen, C. (1979). Information Retrieval. Butterworth.
Vapnik, V. (1995). nature statistical learning theory. Springer.
Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy. Proceedings Sixteenth European Conference Artificial Intelligence (ECAI), pp. 606
612.
Veronis, J., & Ide, N. (1990). Word sense disambiguation large neural networks extracted
machine readable dictionaries. Proceedings Thirteenth International Conference Computational Linguistics (COLING), pp. 389394.
Voorhees, E. (1993). Using WordNet disambiguate word sense text retrieval. Proceedings
Sixteenth Annual International ACM SIGIR Conference Research Development
Information Retrieval, pp. 171180.
Wan, S., Dras, M., Dale, R., & Paris, C. (2006). Using dependency-based features take parafarce paraphrase. Proceedings Australasian Language Technology Workshop,
pp. 131138.
Wu, Z., & Palmer, M. (1994). Verb semantics lexical selection. Proceedings Thirty
Second Annual Meeting Association Computational Linguistics (ACL), pp. 133
138.
Zhang, Y., & Patrick, J. (2005). Paraphrase identification text canonicalization. Proceedings
Australasian Language Technology Workshop, pp. 160166.

39

fiJournal Artificial Intelligence Research 37 (2010) 329-396

Submitted 08/09; published 3/10

Investigation Mathematical Programming
Finite Horizon Decentralized POMDPs
Raghav Aras

raghav.aras@gmail.com

IMS, Suplec Metz
2 rue Edouard Belin, Metz Technopole
57070 Metz - France

Alain Dutech

alain.dutech@loria.fr

MAIA - LORIA/INRIA
Campus Scientifique - BP 239
54506 Vandoeuvre les Nancy - France

Abstract
Decentralized planning uncertain environments complex task generally dealt
using decision-theoretic approach, mainly framework Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). Although DEC-POMDPS
general powerful modeling tool, solving task overwhelming
complexity doubly exponential. paper, study alternate formulation DEC-POMDPs relying sequence-form representation policies.
formulation, show derive Mixed Integer Linear Programming (MILP) problems
that, solved, give exact optimal solutions DEC-POMDPs. show
MILPs derived either using combinatorial characteristics optimal
solutions DEC-POMDPs using concepts borrowed game theory.
experimental validation classical test problems DEC-POMDP literature,
compare approach existing algorithms. Results show mathematical programming outperforms dynamic programming less efficient forward search, except
particular problems.
main contributions work use mathematical programming DECPOMDPs better understanding DEC-POMDPs solutions. Besides,
argue alternate representation DEC-POMDPs could helpful designing
novel algorithms looking approximate solutions DEC-POMDPs.

1. Introduction
framework Decentralized Partially Observable Markov Decision Processes (DECPOMDPs) used model problem designing system made autonomous
agents need coordinate order achieve joint goal. Solving DEC-POMDPs
untractable task belong class NEXP-complete problems (see Section 1.1).
paper, DEC-POMDPs reformulated sequence-form DEC-POMDPs
derive Mixed Integer Linear Programs solved using efficient solvers
order design exact optimal solutions finite-horizon DEC-POMDPs. main
motivation investigate benefits limits novel approach get
better understanding DEC-POMDPs (see Section 1.2). practical level, provide
new algorithms heuristics solving DEC-POMDPs evaluate classical
problems (see Section 1.3).
c
2010
AI Access Foundation. rights reserved.

fiAras & Dutech

1.1 Context
One main goals Artificial Intelligence build artificial agents exhibit
intelligent behavior. agent entity situated environment perceive
sensors act upon using actuators. concept planning, i.e., select
sequence actions order reach goal, central field Artificial
Intelligence years. notion intelligent behavior difficult assess
measure, prefer refer concept rational behavior formulated Russell
Norvig (1995). consequence, work presented uses decision-theoretic
approach order build agents take optimal actions uncertain partially
unknown environment.
particularly interested cooperative multi-agent systems multiple
independent agents limited perception environment must interact coordinate order achieve joint task. central process full knowledge state
system control agents. contrary, agent autonomous
entity must execute actions itself. setting blessing, agent
ideally deal small part problem, curse, coordination
cooperation harder develop enforce.
decision-theoretic approach rational behavior relies mostly framework
Markov Decision Processes (MDP) (Puterman, 1994). system seen sequence
discrete states stochastic dynamics, particular states giving positive negative
reward. process divided discrete decision periods; number periods
called horizon MDP. periods, action chosen
influence transition process next state. using right actions
influence transition probabilities states, objective controller
system maximize long term return, often additive function reward
earned given horizon. controller knows dynamics system,
made transition function reward function, algorithms derived field
Dynamic Programming (see Bellman, 1957) allow controller compute optimal
deterministic policy, i.e., decision function associates optimal action every
state expected long term return optimal. process called planning
MDP community.
fact, using MDP framework, quite straightforward model problem
one agent full complete knowledge state system. agents,
especially multi-agent setting, generally able determine complete
exact state system noisy, faulty limited sensors
nature problem itself. consequence, different states system observed
similar agent problem different optimal actions taken
states; one speaks perceptual aliasing. extension MDPs called Partially
Observable Markov Decisions Processes (POMDPs) deals explicitly phenomenon
allows single agent compute plans setting provided knows conditional
probabilities observations given state environment (Cassandra, Kaelbling, &
Littman, 1994).
pointed Boutilier (1996), multi-agent problems could solved MDPs
considered centralized point view planning control. Here, although
330

fiMathematical Programming DEC-POMDPs

planning centralized process, interested decentralized settings every
agent executes policy. Even agents could instantly communicate observation, consider problems joint observation resulting communications would still enough identify state system. framework
Decentralized Partially Observable Markov Decision Processes (DEC-POMDP) proposed
Bernstein, Givan, Immerman, Zilberstein (2002) takes account decentralization
control partial observability. DEC-POMDP, looking optimal joint
policies composed one policy agent, individual policies
computed centralized way independently executed agents.
main limitation DEC-POMDPs provably untractable
belong class NEXP-complete problems (Bernstein et al., 2002). Concretely,
complexity result implies that, worst case, finding optimal joint policy finite
horizon DEC-POMDP requires time exponential horizon one always make
good choices. complexity, algorithms finding exact
optimal solutions DEC-POMDPs (they doubly exponential complexity)
look approximate solutions. discussed detailed
work Oliehoek, Spaan, Vlassis (2008), algorithms follow either dynamic
programming approach forward search approach adapting concepts algorithms
designed POMDPs.
Yet, concept decentralized planning focus quite large body
previous work fields research. example, Team Decision Problem (Radner,
1959), later formulated Markov system field control theory Anderson
Moore (1980), led Markov Team Decision Problem (Pynadath & Tambe, 2002).
field mathematics, abundant literature Game Theory brings new way
looking multi-agent planning. particular, DEC-POMDP finite horizon
thought game extensive form imperfect information identical interests
(Osborne & Rubinstein, 1994).
Taking inspiration field game theory mathematical programming design exact algorithms solving DEC-POMDPs precisely subject contribution
field decentralized multi-agent planning.
1.2 Motivations
main objective work investigate use mathematical programming,
especially mixed-integer linear programs (MILP) (Diwekar, 2008), solving DECPOMDPs. motivation relies fact field linear programming quite
mature great interest industry. consequence, exist many efficient
solvers mixed-integer linear programs want see efficient solvers
perform framework DEC-POMDPs.
Therefore, reformulate DEC-POMDP solve mixed-integer linear
program. shown article, two paths lead mathematical programs, one
grounded work Koller, Megiddo, von Stengel (1994), Koller Megiddo
(1996) von Stengel (2002), another one grounded combinatorial considerations.
methods rely special reformulation DEC-POMDPs called
331

fiAras & Dutech

sequence-form DEC-POMDPs policy defined histories (i.e., sequences
observations actions) generate applied DEC-POMDP.
basic idea work select, among histories DEC-POMDP,
histories part optimal policy. end, optimal solution
MILP presented article assign positive weight history DECPOMDP every history non-negative weight part optimal policy
DEC-POMDP. number possible histories exponential horizon
problem, complexity naive search optimal set histories doubly
exponential. Therefore, idea appears untractable useless.
Nevertheless, show combining efficiency MILP solvers quite
simple heuristics leads exact algorithms compare quite well existing exact
algorithms. fact, sequence-form DEC-POMDPs need memory space exponential
size problem. Even solving MILPs also exponential size
MILP thus leads doubly exponential complexity sequence-form based algorithms,
argue sequence-form MILPs compare quite well dynamic programming thanks
optimized industrial MILP solvers like Cplex.
Still, investigations experiments Mathematical Programming DECPOMDPs solely aim finding exact solutions DEC-POMDPs. main motivation better understanding DEC-POMDPs limits benefits
mathematical programming approach. hope knowledge help deciding
extent mathematical programming sequence-form DEC-POMDPs used
design novel algorithms look approximate solutions DEC-POMDPs.
1.3 Contributions
paper develop new algorithms order find exact optimal joint policies
DEC-POMDPs. main inspiration comes work Koller, von Stegel
Megiddo shows solve games extensive form imperfect information
identical interests, find Nash equilibrium kind game (Koller et al.,
1994; Koller & Megiddo, 1996; von Stengel, 2002). algorithms caused breakthrough
memory space requirement approach linear size game whereas
canonical algorithms required space exponential size game.
breakthrough mostly due use new formulation policy call
sequence-form.
main contribution, detailed Section 3.3, adapt sequence-form
introduced Koller, von Stegel Megiddo framework DEC-POMDPs (Koller
et al., 1994; Koller & Megiddo, 1996; von Stengel, 2002). result, possible
formulate resolution DEC-POMDP special kind mathematical program
still solved quite efficiently: mixed linear program variables
required either 0 1. adaptation resulting mixed-integer linear
program straightforward. fact, Koller, von Stegel Megiddo could find
one Nash equilibrium 2-agent game. needed DEC-POMDPs find
set policies, called joint policy, corresponds Nash equilibrium
highest value, finding one Nash equilibrium already complex task
enough. Besides, whereas Koller, von Stegel Megiddo algorithms could applied
332

fiMathematical Programming DEC-POMDPs

2-agent games, extend approach solve DEC-POMDPs arbitrary
number agents, constitutes important contribution.
order formulate DEC-POMDPs MILPs, analyze detail structure
optimal joint policy DEC-POMDP. joint policy sequence-form expressed
set individual policies described set possible trajectories
agents DEC-POMDP. Combinatorial considerations individual
histories, well constraints ensure histories define valid joint policy
heart formulation DEC-POMDP mixed linear program, developped
Sections 4 5. Thus, another contribution work better understanding
properties optimal solutions DEC-POMDPs, knowledge might lead
formulation new approximate algorithms DEC-POMDPs.
Another important contribution work introduce heuristics boosting performance mathematical programs propose (see Section 6).
heuristics take advantage succinctness DEC-POMDP model knowledge acquired regarding structure optimal policies. Consequently, able
reduce size mathematical programs (resulting also reducing time taken
solve them). heuristics constitute important pre-processing step solving
programs. present two types heuristics: elimination extraneous histories
reduces size mixed integer linear programs introduction cuts
mixed integer linear programs reduces time taken solve program.
practical level, article presents three different mixed integer linear
programs, two directly derived work Koller, von Stegel Megiddo
(see Table 4 5) third one based solely combinatorial considerations
individual policies histories (see Table 3). theoretical validity formulations backed several theorems. also conducted experimental evaluations
algorithms heuristics several classical DEC-POMDP problems. thus
able confirm algorithms quite comparable dynamic programming exact
algorithms outperformed forward search algorithms like GMAA* (Oliehoek et al.,
2008). problems, though, MILPs indeed faster one order magnitude
two GMAA*.
1.4 Overview Article
remainder article organized follows. Section 2 introduces formalism
DEC-POMDP background classical algorithms, usually based dynamic
programing. expose reformulation DEC-POMDP sequence-form
Section 3 also define various notions needed sequence-form. Section 4,
show use combinatorial properties sequence-form policies derive first
mixed integer linear program (MILP, Table 3) solving DEC-POMDP. using game
theoretic concepts like Nash equilibrium, take inspiration previous work games
extensive form design two MILPs solving DEC-POMDP (Tables 4, 5).
MILPs smaller size detailed derivation presented Section 5.
contributed heuristics speed practical resolutions various MILPs make
core Section 6. Section 7 presents experimental validations MILP-based
algorithms classical benchmarks DEC-POMDP literature well randomly
333

fiAras & Dutech

built problems. Finally, Section 8 analyzes discusses work conclude
paper Section 9.

2. Dec-POMDP
section gives formal definition Decentralized Partially Observed Markov Decision
Processes introduced Bernstein et al. (2002). described, solution DECPOMDP policy defined space information sets optimal value.
sections ends quick overview classical methods developed
solve DEC-POMDPs.
2.1 Formal Definition
DEC-POMDP defined tuple = h I, S, {Ai }, P, {Oi }, G, R, , where:
= {1, 2, , n} set agents.
finite set states. set probability distributions shall denoted
(S). Members (S) shall called belief states.
agent I, Ai set actions. = iI Ai denotes set joint
actions.
P : [0, 1] state transition function. s,
A, P(s, a, ) probability state problem period if,
period 1, state agents performed joint action a. Thus,
time period 2, pair states s, joint action A,
holds:
P(s, a, ) = Pr(st = |st1 = s, = a).
Thus, (S, A, P) defines discrete-state, discrete-time controlled Markov process.
agent I, Oi set observations. = iI Oi denotes set joint
observations.
G : [0, 1] joint observation function. A,
S, G(a, s, o) probability agents receive
joint observation (that is, agent receives observation oi ) state
problem period previous period agents took joint
action a. Thus, time period 2, joint action A, state
joint observation O, holds:
G(a, s, o) = Pr(ot = o|st = s, at1 = a).
R : R reward function. A, R(s, a) R
reward obtained agents take joint action state
process s.
334

fiMathematical Programming DEC-POMDPs

horizon problem. agents allowed joint-actions
process halts.
(S) initial state DEC-POMDP. S, (s) denotes
probability state problem first period s.
said, S, P define controlled Markov Process next state depends
previous state joint action chosen agents. agents
access state process rely observations, generally
partial noisy, state, specified observation function G. time
time, agents receive non-zero reward according reward function R.

n0

n1

10

11

s0

s1

nt

n1

1t

11

s2

nt

1t

st

Figure 1: DEC-POMDP. every period process, environment state
st , every agent receives observations oti decides action ati . joint
action hat1 , at2 , , atn alters state process.
specifically, illustrated Figure 1, control DEC-POMDP n
agents unfolds discrete time periods, = 1, 2, ,T follows. period t,
process state denoted st S. first period = 1, state s1 chosen
according agents take actions a1i . period > 1 afterward, agent
takes action denoted ati Ai according agents policy.
agents take joint action = hat1 , at2 , , atn i, following events occur:
1. agents obtain reward R(st , ).
2. state st+1 determined according function P arguments st .
3. agent receives observation ot+1
Oi . joint observation ot+1 =

t+1
t+1
t+1 .
hot+1
1 , o2 , , determined function G arguments
4. period changes + 1.
paper, DEC-POMDP interested following properties:
335

fiAras & Dutech

horizon finite known agents;
agents cannot infer exact state system joint observations (this
general setting DEC-POMDPs);
agents observe actions observations agents.
aware observations reward;
agents perfect memory past; base choice action
sequence past actions observations. speak perfect recall setting;
transition observation functions stationary, meaning depend
period t.
Solving DEC-POMDP means finding agents policies (i.e., decision functions)
optimize given criterion based rewards received. criterion work
called cumulative reward defined by:
"
#
X




E
R(s , ha1 , a2 , . . . , i)
(1)
t=1

E mathematical expectation.
2.2 Example DEC-POMDP
problem known Decentralized Tiger Problem (hereby denoted MA-Tiger),
introduced Nair, Tambe, Yokoo, Pynadath, Marsella (2003), widely used
test DEC-POMDPs algorithms. variation problem previously introduced
POMDPs (i.e., DEC-POMDPs one agent) Kaelbling, Littman, Cassandra
(1998).
problem, given two agents confronted two closed doors. Behind one
door tiger, behind escape route. agents know door
leads what. agent, independently other, open one two doors
listen carefully order detect tiger. either opens wrong door,
lives imperiled. open escape door, free.
agents limited time decide door open. use time
gather information precise location tiger listening carefully detect
location tiger. problem formalized DEC-POMDP with:
two states tiger either behind left door (sl ) right door (sr );
two agents, must decide act;
three actions agent: open left door (al ), open right door (ar )
listen (ao );
two observations, thing agent observe hear tiger
left (ol ) right (or ).
336

fiMathematical Programming DEC-POMDPs

initial state chosen according uniform distribution S. long door
remains closed, state change but, one door opened, state reset
either sl sr equal probability. observations noisy, reflecting difficulty
detecting tiger. example, tiger left, action ao produces
observation ol 85% time. agents perform ao , joint observation
(ol ,ol ) occurs probability 0.85 0.85 = 0.72. reward function encourages
agents coordinate actions as, example, reward open escape
door (+20) bigger one listens opens good door (+9).
full state transition function, joint observation function reward function described
work Nair et al. (2003).
2.3 Information Sets Histories
information set agent sequence (a1 .o2 .a2 .o3 .ot ) even length
elements odd positions actions agent (members Ai ) even
positions observations agent (members Oi ). information set length 0
shall called null information set, denoted . information set length 1
shall called terminal information set. set information sets lengths less
equal 1 shall denoted .
define history agent sequence (a1 .o2 . a2 . o3 .ot .at ) odd
length elements odd positions actions agent (members Ai )
even positions observations agent (members Oi ). define length
history number actions history (t example). history
length shall called terminal history. Histories lengths less shall called
non-terminal histories. history null length shall denoted . information
set associated history h, denoted (h), information set composed removing
h last action. h history observation, h.o information set.
shall denote Hit set possible histories length agent i. Thus, Hi1
set actions Ai . shall denote Hi set histories agent lengths
less equal . size ni Hi thus:
ni = |Hi | =

PT


t1
t=1 |Ai | |Oi |

= |Ai |

(|Ai ||Oi |)T 1
.
|Ai ||Oi | 1

(2)

set HiT terminal histories agent shall denoted Ei . set Hi \HiT
non-terminal histories agent shall denoted Ni .
tuple hh1 , h2 , . . . , hn made one history agent called joint history.
tuple obtained removing history hi joint history h noted hi called
i-reduced joint history.
Example Coming back MA-Tiger example, set valid histories could be: , (ao ),
(ao .ol .ao ), (ao .or .ao ), (ao .ol .ao .ol .ao ), (ao .ol .ao .or .ar ), (ao .or .ao .ol .ao ) (ao .or .ao .or .ar ).
Incidently, set histories corresponds support policy (i.e., histories
generated using policy) Figure 2, explained next section.
337

fiAras & Dutech

2.4 Policies
period time, policy must tell agent action choose. choice
based whatever past present knowledge agent process
time t. One possibility define individual policy agent mapping
information sets actions. formally:
: (Ai )

(3)

Among set policies, three families usually distinguished:
Pure policies. pure deterministic policy maps given information set one
unique action. set pure policies agent denoted . Pure policies
could also defined using trajectories past observations since actions,
chosen deterministically, reconstructed observations.
Mixed policies. mixed policy probability distribution set pure
policies. Thus, agent using mixed policy control DEC-POMDP using
pure policy randomly chosen set pure policies.
Stochastic policies. stochastic policy general formulation associates
probability distribution actions history.
come back MA-Tiger problem (Section 2.2), Figure 2 gives possible policy
horizon 2. shown, policy classically represented action-observation tree.
kind tree, branch labelled observation. given sequence past
observations, one starts root node follows branches action
node. node contains action executed agent seen
sequence observations.
Observation sequence
Chosen action

ol
ao


ao


ao

ol .ol
al

ol .or
ao

.ol
ao

.or
ar

ao
ol



ao

ao

ol



ol



al

ao

ao

ar

Figure 2: Pure policy MA-Tiger. pure policy maps sequences observations
actions. represented action-observation tree.

joint policy = h1 , 2 , , n n-tuple policy agent i.
individual policies must horizon. agent i, also define
notion i-reduced joint policy = h1 , , i1 , i+1 , , n composed
policies agents. thus = hi , i.
338

fiMathematical Programming DEC-POMDPs

2.5 Value Function
executed agents, every -horizon joint policy generates probability distribution possible sequences reward one compute value
policy according Equation 1. Thus value joint policy formally defined as:
V (, ) = E

"
X

R(st , )|,

t=1

#

(4)

given state first period chosen according actions chosen
according .
recursive definition value function policy also way
compute horizon finite. definition requires concepts
shall introduce.
Given belief state (S), joint action joint observation O,
let (o|, a) denote probability agents receive joint observation take
joint action period state chosen according . probability
defined
X
X
(o|, a) =
(s)
P(s, a, )G(a, , o)
(5)


sS

Given belief state (S), joint action joint observation ,
updated belief state ao (S) respect defined (for
S),
ao (s ) =

P
G(a,s ,o)[ sS (s)P(s,a,s )]
(o|,a)

(o|, a) > 0

(6)

ao (s ) =

0

(o|, a) = 0

(7)

P
Given belief state (S) joint action A, R(, a) denotes sS (s)R(s, a).
Using definitions notations, value V (, ) defined follows:
V (, ) = V (, , )

(8)

V (, , ) defined recursion using equations (9), (10) (11), given below.
equations straight reformulation classical Bellman equations finite
horizon problems.
histories null length
V (, , ) = R(, ()) +

X

(o|, ())V (()o , , o)

(9)

oO

() denotes joint action h1 (), 2 (), , n ()i ()o denotes
updated state given () joint observation o.
339

fiAras & Dutech

non-terminal histories. (S), {1, . . . , 2},
1:T
1:T
1:T
tuple sequences observations o1:T = ho1:T

1 , o2 , , oi
sequence observations agent I:
V ( , , o1:T ) = R( , (o1:T )) +

X

1:T )o

(o| , (o1:T ))V ((o

, , o1:T .o) (10)

oO
1:T

(o )o updated state given joint action (o1:T ) joint observation
= ho1 , o2 , , o1:T .o tuple sequences (t + 1) observations ho1:T
1 .o1 ,
1:T .o i.
o1:T
.o
,



,

n
2
n
2
terminal histories. (S), tuple sequences (T - 1)
1
1
1 i:
observations o1:T 1 = ho1:T
, o1:T
, , o1:T
n
1
2
V ( , , o1:T 1 ) = R(, (o1:T 1 )) =

X

(s)R(s, (o1:T 1 ))

(11)

sS

optimal policy policy best possible value, verifying:
V (, ) V (, )

.

(12)

important fact DEC-POMDPs, based following theorem,
restrict set pure policies looking solution DEC-POMDP.
Theorem 2.1. DEC-POMDP least one optimal pure joint policy.
Proof: See proof work Nair et al. (2003).



2.6 Overview DEC-POMDPs Solutions Limitations
detailed work Oliehoek et al. (2008), existing methods solving DECPOMDPs finite-horizon belong several broad families: brute force, alternating
maximization, search algorithms dynamic programming.
Brute Force simplest approach solving DEC-POMDP enumerate
possible joint policies evaluate order find optimal one. However,
method becomes quickly untractable number joint policies doubly exponential
horizon problem.
Alternating Maximization Following Chades, Scherrer, Charpillet (2002) Nair
et al. (2003), one possible way solve DEC-POMDPs agent (or small group
agents) alternatively search better policy agents freeze
policy. Called alternating maximization Oliehoek alternated co-evolution
Chades method guarantees find Nash equilibria, locally optimal joint
policy.
340

fiMathematical Programming DEC-POMDPs

Heuristic Search Algorithms concept introduced Szer, Charpillet,
Zilberstein (2005) relies heuristic search looking optimal joint policy,
using admissible approximation value optimal joint policy. search
progresses, joint policies provably worse current admissible solution
pruned. Szer et al. used underlying MDPs POMDPs compute admissible heuristic,
Oliehoek et al. (2008) introduced better heuristic based resolution Bayesian
Game carefully crafted cost function. Currently, Oliehoeks method called GMAA*
(for Generic Multi-Agent A*) quickest exact method large set benchmarks.
But, every exact method, limited quite simple problems.
Dynamic Programming work Hansen, Bernstein, Zilberstein (2004)
adapts solutions designed POMDPs domain DEC-POMDPs. general
idea start policies 1 time step used build 2 time step policies
on. process clearly less efficient heuristic search approach
exponential number policies must constructed evaluated iteration
algorithm. policies pruned but, again, pruning less efficient.
exposed details paper Oliehoek et al. (2008), several others approaches developed subclasses DEC-POMDPs. example, special settings agents allowed communicate exchange informations settings
transition function split independant transition functions agent
studied found easier solve generic DEC-POMDPs.

3. Sequence-Form DEC-POMDPs
section introduces fundamental concept policies sequence-form. new
formulation DEC-POMDP thus possible leads Non-Linear Program
(NLP) solution defines optimal solution DEC-POMDP.
3.1 Policies Sequence-Form
history function p agent mapping set histories interval
[0, 1]. value p(h) weight history h history function p. policy
defines probability function set histories agent saying that,
history hi Hi , p(hi ) conditional probability hi given observation sequence
(o0i .o1i . .oti ) .
every policy defines policy function, every policy function associated
valid policy. constraints must met. fact, history function p sequenceform policy agent following constraints met:
X

p(a) = 1,

(13)

aAi

p(h) +

X

p(h.o.a) = 0,

h Ni , Oi ,

(14)

aAi

h.o.a denotes history obtained concatenating h. definition
appears slightly different form Lemma 5.1 work Koller et al. (1994).
341

fiAras & Dutech

Variables: x(h), h Hi ,
X

x(a) = 1

(15)

aAi

x(h) +

X

x(h.o.a) = 0,

h Ni , Oi

(16)

h Hi

(17)

aAi

x(h) 0,

Table 1: Policy Constraints. set linear inequalities, solved, provide valid
sequence-form policy agent i. is, weights x(h), possible
define policy agent i.

sequence-form policy stochastic probability choosing action
information set h.o p(h.o.a)/p(h). support S(p) sequence-form policy made
set histories non-negative weight, i.e. S(p) = {h Hi | p(h) > 0}.
sequence-form policy p defines unique policy agent, sequence-form policy
called policy rest paper ambiguity present.
set policies sequence-form agent shall denoted Xi . set
pure policies sequence-form shall denoted Xi Xi .
way similar definitions Section 2.4, define sequence-form joint
policy tuple sequence-form policies, one agent. weight Q
joint
history h = hhi sequence-form joint policy hp1 , p2 , , pn product iI pi (hi ).
set joint policies sequence-form iI Xi shall denoted X set
i-reduced sequence-form joint policy called Xi .
3.2 Policy Constraints
policy agent sequence-form found solving set linear inequalities
(LI) found Table 1. LI merely implement definition policy sequenceform. LI contains one variable x(h) history h Hi represent weight
h policy. solution x LI constitutes policy sequence-form.
Example Section E.1 Appendices, policy constraints decentralized
Tiger problem given 2 agents horizon 2.
Notice policy constraints agent, variable constrained
non-negative whereas definition policy sequence-form, weight history
must interval [0, 1]. mean variable solution policy
constraints assume value higher 1? Actually, policy constraints
prevent variable assuming value higher 1 following lemma
shows.
Lemma 3.1. every solution x (15)-(17), h Hi , x (h) belongs [0, 1]
interval.
342

fiMathematical Programming DEC-POMDPs

Proof: shown forward induction.
Every x(h) non-negative (see Eq. (17)), also case every action
Ai . Then, x(a) greater 1 otherwise constraint (15) would violated. So,
h Hi1 , (i.e. Ai ), x(h) belong [0, 1].
every h Hit x(h) [0, 1], previous reasoning applied using constraint
(16) leads evidently fact x(h) [0, 1] every h Hit+1 .
Thereby, induction holds t.

Later article, order simplify task looking joint policies, policy
constraints LI used find pure policies. Looking pure policies limitation
finite-horizon DEC-POMDPs admit deterministic policies policies defined
information set. fact, pure policies needed two three MILPs build
order solve DEC-POMDPs, otherwise derivation would possible (see
Sections 4 5.4).
Looking pure policies, obvious solution would impose every variable
x(h) belongs set {0, 1}. But, solving mixed integer linear program,
generally good idea limit number integer variables integer variable
possible node branch bound method used assign integer values
variables. efficient implementation mixed integer linear program take
advantage following lemma impose weights terminal histories
take 0 1 possible values.
Lemma 3.2. (15)-(17), (17) replaced by,
x(h) 0,

h Ni

x(h) {0, 1},

h Ei

(18)
(19)

every solution x resulting LI, h Hi , x (h) = 0 1.
speak 0-1 LI.
Proof: prove backward induction. Let h history length - 1.
Due (16), Oi , holds,
X
x (h) =
x (h.o.a).
(20)
aAi

Since h history length - 1, history h.o.a terminal history. Due Lemma
3.1, x (h) [0, 1]. Therefore, sum right hand side equation also
[0, 1]. due (19), x (h.o.a) {0, 1}. Hence sum right hand side
either 0 1, value between. Ergo, x (h) {0, 1} value
between. reasoning, show x (h) {0, 1} every non-terminal
history h length - 2, - 3, , 1.

formulate linear inequalities Table 1 memory, require
P space
exponential horizon. agent I, size Hi Tt=1 |Ai |t |Oi |t1 .
exponential number variables
LP also exponential .
PT 1in

number constraints LI Table 1 t=0 |Ai | |Oi |t , meaning number
constraints LI also exponential .
343

fiAras & Dutech

3.3 Sequence-Form DEC-POMDP
able give formulation DEC-POMDP based use sequence-form
policies. want stress re-formulation, provide us
new ways solving DEC-POMDPs mathematical programming.
Given classical formulation DEC-POMDP (see Section 2.1), equivalent
sequence-form DEC-POMDP tuple hI, {Hi }, , Ri where:
= {1, 2, , n} set agents.
agent I, Hi set histories length less equal
agent i, defined previous section. set Hi derived using sets
Ai Oi .
joint history conditional probability function. joint history j H,
(, j) probability j occurring conditional agents taking joint actions
according given initial state DEC-POMDP . function
derived using set states S, state transition function P joint
observation function G.
R joint history value. joint history j H, R(, j) value
expected reward agents obtain joint history j occurs. function
derived using set states S, state transition function P, joint observation
function G reward function R. Alternatively, R described function
R.
formulation folds S, P G R relying set histories.
give details computation R.
(, j) conditional probability sequence joint observations received
agents till period (o1 (j).o2 (j). . ot1 (j)) sequence joint actions
taken till period - 1 (a1 (j). a2 (j). . at1 (j)) initial state
DEC-POMDP . is,
(, j) = Prob.(o1 (j).o2 (j). .ot1 (j)|, a1 (j).a2 (j). .at1 (j))

(21)

probability product probabilities seeing observation ok (j) given
appropriate belief state action chosen time k, is:
(, j) =

t1


(ok (j)|jk1 , ak (j))

(22)

k=1

jk probability distribution given agents followed joint
history j time k, is:
jk (s) = Prob.(s|o1 (j).a1 (j). .ok (j)).
344

(23)

fiMathematical Programming DEC-POMDPs

Variables: xi (h), I, h Hi
Maximize

X

R(, j)

jE



xi (ji )

(27)

iI

subject
X

xi (a) = 1,



(28)

I, h Ni , Oi

(29)

I, h Hi

(30)

aAi

xi (h) +

X

xi (h.o.a) = 0,

aAi

xi (h) 0,

Table 2: NLP. non-linear program expresses constraints finding sequenceform joint policy optimal solution DEC-POMDP.

Regarding value joint history, defined by:
R(, j) = R(, j)(, j)

(24)


R(, j) =

X
X

jk1 (s)R(s, ak (j)).

(25)

k=1 sS

Thus, V(, p), value sequence-form joint policy p, weighted sum
value histories support:
X
V(, p) =
p(j)R(, j)
(26)
jH

p(j) =

Q

iI

pi (ji ).

3.4 Non-Linear Program Solving DEC-POMDPs.
using sequence-form formulation DEC-POMDP, able express joint
policies sets linear constraints assess value every policy. Solving DECPOMDP amounts finding policy maximal value, done
non-linear program (NLP) Table 2 where, again, xi variables weights
histories agent i.
Example example formulation NLP found Appendices,
Section E.2. given decentralized Tiger problem 2 agents horizon
2.
constraints program form convex set, objective function
concave (as explained appendix A). general case, solving non-linear program
345

fiAras & Dutech

difficult generalized method guarantee finding global maximum
point. However, particular NLP fact Multilinear Mathematical Program (see
Drenick, 1992) kind programs still difficult solve. two
agents considered, one speaks bilinear programs, solved easily
(Petrik & Zilberstein, 2009; Horst & Tuy, 2003).
evident, inefficient, method find global maximum point evaluate
extreme points set feasible solutions program since known every
global well local maximum point non-concave function lies extreme point
set (Fletcher, 1987). inefficient method test tells
extreme point local maximum point global maximum point. Hence, unless
extreme points evaluated, cannot sure obtained global maximum
point. set feasible solutions NLP X, set -step joint policies.
set extreme points set X, set pure -step joint policies, whose number
doubly exponential exponential n. enumerating extreme points
NLP untractable.
approach, developed next sections, linearize objective function
NLP order deal linear programs. describe two ways
this: one based combinatorial consideration (Section 4) based
game theory concepts (Section 5). cases, shall mean adding variables
constraints NLP, upon so, shall derive mixed integer linear programs
possible find global maximum point hence optimal joint policy
DEC-POMDP.

4. Combinatorial Considerations Mathematical Programming
section explains possible use combinatorial properties DEC-POMDPs
transform previous NLP mixed integer linear program. shown, mathematical program belongs family 0-1 Mixed Integer Linear Programs, meaning
variables linear program must take integer values set {0, 1}.
4.1 Linearization Objective Function
Borrowing ideas field Quadratic Assignment Problems (Papadimitriou & Steiglitz, 1982), turn non-linear objective function previous NLP linear
objective function linear constraints involving new variables z must take integer
values. variable z(j) represents product xi (ji ) variables.
Thus, objective function was:
X

maximize
R(, j)
xi (ji )
(31)
jE

iI

rewritten
maximize

X

R(, j)z(j)

jE

j = hj1 , j2 , , jn i.
346

(32)

fiMathematical Programming DEC-POMDPs

must ensure two way mapping value new variables
z x variables solution mathematical program, is:

z (j) =
xi (ji ).
(33)
iI

this, restrict ourself pure policies x variables 0 1.
case, previous constraint (33) becomes:
z (j) = 1 xi (ji ) = 1,



(34)

There, take advantage fact support pure policy agent
composed |Oi |T 1 terminal histories express new constraints. one hand,
guarantee z(j) equal 1 enough x variables also equal 1,
write:
n
X

xi (ji ) nz(j) 0,

j E.

(35)

i=1

hand, limit number z(j) variables take value 1,
enumerate number joint terminal histories end with:

X
|Oi |T 1 .
(36)
z(j) =
iI

jE

constraints (35) would weight heavily mathematical program would
one constraint terminal joint history, number exponential n
. idea reduce number constraints reason joint histories
individual histories. history h agent part support solution
ofPthe problem (i.e., xQ
(h) = 1) number joint histories
Q belongs
( j Ei z(hh, j i)) kI\{i} |Ok |T 1 . Then, suggest replace |Ei | constraints
(35)
n
X

xi (ji ) nz(j) 0,

j E.

(35)

i=1



P

|Ei | constraints
X



|Ok |T 1
xi (h)
|Oi |T 1

= xi (h)
|Ok |T 1 ,

z(hh, j i) =

j Ei

Q

kI

I, h Ei .

(37)

kI\{i}

4.2 Fewer Integer Variables
linearization objective function rely fact dealing pure
policies, meaning every x z variable supposed value either 0 1. solving
linear programs integer variables usually based branch bound technique
347

fiAras & Dutech

Variables:
xi (h), I, h Hi
z(j), j E
X

Maximize

R(, j)z(j)

(38)

jE

subject to:
X

xi (a) = 1,



(39)

I, h Ni , Oi

(40)

aAi

xi (h) +

X

xi (h.o.a) = 0,

aAi

X



z(hh, j i) = xi (h)


j Hi

|Ok |T 1 ,

I, h Ei

(41)

kI\{i}

X

z(j) =

jE



|Oi |T 1

(42)

iI

xi (h) 0,

I, h Ni

xi (h) {0, 1},

(43)

I, h Ei

z(j) [0, 1],

(44)

j E

(45)

Table 3: MILP. 0-1 mixed integer linear program finds sequence-form joint policy
optimal solution DEC-POMDP.

(Fletcher, 1987), efficiency reasons, important reduce number integer
variables mathematical programs.
done Section 3.2, relax x variables allow take non-negative
values provided x values terminal histories constrained integer values.
Furthermore, proved following lemma, constraints x also guarantee
z variables take value {0, 1}.
eventually end following linear program real integer variables,
thus called 0-1 mixed integer linear program (MILP). MILP shown Table 3.
Example Section E.3 Appendices, example MILP given
problem decentralized Tiger 2 agents horizon 2.
Lemma 4.1. every solution (x , z ) MILP Table 3, j E, z (j)
either 0 1.
Proof: Let (x , z ) solution MILP. Let,
S(z) = {j E|z (j) > 0}
Si (xi ) = {h


Ei |xi (h)

= 1},




(46)


Si (z, j ) = {j E|ji = j , z (j) > 0},
348

(47)
I,

j

Ei

(48)

fiMathematical Programming DEC-POMDPs

Q
Q
1 . showing |S(z)|
1 ,
Now, due (42) (45), |S(z)|
i|
iI |Oi |
Q iIT|O
1
shall establish |S(z)| = iI |Oi |
. due upper bound 1 z

variable, implication z (j) 0 1 terminal joint history j thus
proving statement lemma.
Note Lemma (3.2), agent i, xi pure policy. Therefore,
|Si (x)| = |Oi |T 1 . means set constraints (41), i-reduced terminal
joint history j Ei appear right hand side |Oi |T 1 times
left hand side, xi (h) = 1. Thus, j Ei ,
|Si (z, j )| |Oi |T 1 .

(49)

(h) either 0 1 since
Now, know agent history h Hi , xi Q
xi pure policy. So, given i-reduced terminal joint history j , kI\{i} xk (jk ) either
0 1. Secondly, due (41), following implication clearly holds terminal joint
history j,
z (j) > 0 xi (ji ) = 1,

I.

(50)

Therefore, obtain
|Si (z, j )| |Oi |T 1

(51)


1

= |Oi |

xk (jk ).

(52)

kI\{i}

consequence,
X

X

|Si (z, j )|

j Ei



|Oi |T 1

j Ei

xk (jk )

(53)

xk (jk )

(54)

xk (h )

(55)

kI\{i}

= |Oi |T 1

X



j Ei kI\{i}

= |Oi |T 1



X

kI\{i} h Ek

= |Oi |T 1



|Ok |T 1

(56)

kI\{i}

=



1

|Oj |

.

(57)

jI

Since



j Ei

Si (z, j ) = S(z), holds

P



|S(z)|

j Ei

|Si (z, j )| = |S(z)|. Hence,

|Oj |T 1 .

(58)

jI

Thus statement lemma proved.
349



fiAras & Dutech

4.3 Summary
using combinatorial considerations, possible design 0-1 MILP solving given
DEC-POMDP. proved theorem 4.1, solution MILP defines optimal joint
policy
Nevertheless, MILP quite large, O(kT ) constraints
Pfor DEC-POMDP.
Q
nT
|Hi | + |Ei | = O(k ) variables, O(kT ) variables must take integer values.
next section details another method linearization NLP leads
smaller mathematical program 2-agent case.
Theorem 4.1. Given solution (x , z ) MILP, x = hx1 , x2 , , xn pure
-period optimal joint policy sequence-form.
Proof: Due policy constraints domain constraints agent, xi

pure sequence-form policy
Q agent i. Due constraints (41)-(42), z values 1
product iI xi (ji ) values 1. Then, maximizing objective function
effectively maximizing value sequence-form policy hx1 , x2 , , xn i. Thus,

hx1 , x2 , , xn optimal joint policy original DEC-POMDP.

5. Game-Theoretical Considerations Mathematical
Programming
section borrows concepts like Nash equilibrium regret game theory
order design yet another 0-1 Mixed Integer Linear Program solving DEC-POMDPs.
fact, two MILPs designed, one applied 2 agents
one number agents. main objective part derive smaller
mathematical program 2 agent case. Indeed, MILP-2 agents (see Table 4)
slightly less variables constraints MILP (see Table 3) thus might prove easier
solve. hand, 2 agents considered, new derivation
leads MILP given completeness bigger MILP.
Links fields multiagent systems game theory numerous
literature (see, example, Sandholm, 1999; Parsons & Wooldridge, 2002). elaborate fact optimal policy DEC-POMDP Nash Equilibrium.
fact Nash Equilibrium highest utility agents share reward.
2-agent case, derivation make order build MILP similar
first derivation Sandholm, Gilpin, Conitzer (2005). give details
derivation adapt DEC-POMDP adding objective function it.
2 agents, derivation still use find Nash equilibriae pure strategies.
rest article, make distinction policy, sequence-form
policy strategy agent as, context, concepts equivalent. Borrowing
game theory, joint policy denoted p q, individual policy pi qi
i-reduced policy pi qi .
5.1 Nash Equilibrium
Nash Equilibrium joint policy policy best response reduced
joint policy formed policies joint policy. context sequence-form
350

fiMathematical Programming DEC-POMDPs

DEC-POMDP, policy pi Xi agent said best response i-reduced
joint policy qi Xi holds
V(, hpi , qi i) V(, hpi , qi i) 0,

pi Xi .

(59)

joint policy p X Nash Equilibrium holds
V(, p) V(, hpi , pi i) 0,
is,
X X

hEi j Ei

R(, hh, j i)



kI\{i}

I, pi Xi .



0,
pk (jk ) pi (h) pi (h)

I, pi Xi .

(60)

(61)

derivation necessary conditions Nash equilibrium consists deriving
necessary conditions policy best response reduced joint policy.
following program finds policy agent best response i-reduced joint
policy qi Xi . Constraints (63)-(64) ensure policy defines valid joint policy
(see Section 3.2) objective function traduction concept best response.
Variables: xi (h), I, h Hi



X X

Maximize
R(, hh, j i)
qk (jk ) xi (h)


hEi

j Ei

(62)

kI\{i}

subject to:

X

xi (a) = 1

(63)

aAi

xi (h) +

X

xi (h.o.a) = 0,

h Ni , Oi

(64)

h Hi .

(65)

aAi

xi (h) 0,

linear program (LP) must still refined solution best
response agent global best response, i.e., policy agent best
response agents. mean introducing new variables (a set variable
agent). main point adapt objective function current
objective function, applied find global best response, would lead non-linear
objective function product weights policies would appear. this,
make use dual program (LP).
linear program (LP) one variable xi (h) history h Hi representing
weight h. one constraint per information set agent i. words,
constraint linear program (LP) uniquely labeled information set. instance,
constraint (63) labeled null information set , nonterminal
history h observation o, corresponding constraint (64) labeled
information set h.o. Thus, (LP) ni variables mi constraints.
described appendix (see appendix B), dual (LP) expressed as:
351

fiAras & Dutech

Variables: yi (),
Minimize

yi ()

(66)

subject to:
yi ((h))

X

yi (h.o) 0,

h Ni

(67)

qk (jk ) 0,

h Ei

(68)

oOi

yi ((h))

X

R(, hh, j i)

j Ei



kI\{i}

yi () (, +),



(69)

(h) denotes information set h belongs. dual one free variable
yi () every information set agent i. function (h) (defined Section 2.3) appears mapping histories information sets1 . dual program
one constraint per history agent. Thus, dual mi variables ni constraints.
Note objective dual minimize yi () primal (LP),
right hand side constraints, except first one, 0.
theorem duality (see appendix B), applied primal (LP) (62)-(65)
transformed dual (66)-(69), says solutions value. Mathematically,
means that:



X X

R(, hh, j i)
qk (jk ) xi (h) = yi ().
(70)


hEi

j Ei

kI\{i}

Thus, value joint policy hxi , qi expressed either
X X


V(, hxi , qi i) =
R(, hh, j i)
qk (jk ) xi (h)
hEi

j Ei

(71)

kI\{i}



V(, hxi , qi i) = yi ().

(72)

Due constraints (63) (64) primal LP, holds
X


X X X
xi (a) +
yi (h.o) xi (h) +
xi (h.o.a)
yi () = yi ()
aAi

hNi oOi

(73)

aAi

constraint (63) guarantees first term braces 1 constraints (65)
guarantee remaining terms inside braces 0. right hand side
(73) rewritten
X
X
X




P
xi (a) yi ()
oOi yi (a.o)
+
xi (h) yi ((h))
yi (h.o)
aAi

oOi

hNi \Ai

+

X

xi (h)yi ((h))

hEi

=

P


hNi xi (h)



yi ((h))

X

oOi

X
yi (h.o) +
xi (h)yi ((h))
hEi

1. h.o information set, yi (h.o) shortcut writing yi ((h.o)).

352

(74)

fiMathematical Programming DEC-POMDPs

So, combining equations (70) (74), get
X
X


xi (h)
yi ((h))
yi (h.o)
oOi

hNi

+

X

xi (h)



hEi

yi ((h))

R(, hh, j i)

X



j Ei



kI\{i}


qk (jk ) = 0

(75)

time introduce supplementary variables w information set. variables, usually called slack variables, defined as:
X
yi ((h))
yi (h.o) = wi (h), h Ni
(76)
oOi

yi ((h))

X



R(, hh, j i)

j Ei



qk (jk ) = wi (h),

h Ei .

(77)

kI\{i}

shown Section C appendix, slack variables correspond concept
regret defined game theory. regret history expresses loss accumulated
reward agent incurs acts according history rather according
history would belong optimal joint policy.
Thanks slack variables, furthermore rewrite (75) simply
X
X
xi (h)wi (h) +
xi (h)wi (h) = 0
(78)
hNi

hEi

X

xi (h)wi (h) = 0.

(79)

hHi

Now, (79) sum ni products, ni size Hi . product sum
necessarily 0 xi (h) wi (h) constrained nonnegative primal
dual respectively. property strongly linked complementary slackness
optimality criterion linear programs (see, example, Vanderbei, 2008). Hence, (79)
equivalent
xi (h)wi (h) = 0,

h Hi .

(80)

Back framework DEC-POMDPs, constraints written:
pi (h)i (hh, qi i) = 0,

h Hi .

(81)

sum up, solving following mathematical program would give optimal joint
policy DEC-POMDP. constraints (87) non-linear thus prevent us
solving program directly. linearization constraints, called complementarity
constraints, subject next section.
Variables:
xi (h), wi (h) h Hi
yi ()
Maximize

y1 ()

353

(82)

fiAras & Dutech

subject to:
X

xi (a) = 1

(83)

aAi

xi (h) +

X

I, h Ni , Oi

(84)

yi (h.o) = wi (h),

I, h Ni

(85)

xk (jk ) = wi (h),

I, h Ei

(86)

xi (h.o.a) = 0,

aAi

yi ((h))

X

oOi

yi ((h))

X

j Ei

R(, hh, j i)



kI\{i}

xi (h)wi (h) = 0,

I, h Hi

(87)

xi (h) 0,

I, h Hi

(88)

wi (h) 0,

I, h Hi

(89)

yi () (, +),

I,

(90)

5.2 Dealing Complementarity Constraints
section explains non-linear constraints xi (h)wi (h) = 0 previous mathematical program turned sets linear constraints thus lead mixed
integer linear programming formulation solution DEC-POMDP.
Consider complementarity constraint ab = 0 variables b. Assume
lower bound values b 0. Let upper bounds values b
respectively ua ub . let c 0-1 variable. Then, complementarity constraint
ab = 0 separated following equivalent pair linear constraints,
ua c

(91)

b ub (1 c).

(92)

words, pair constraints satisfied, surely case ab = 0.
easily verified. c either 0 1. c = 0, set 0
constrained ua c (and less 0); c = 1, b set 0
since b constrained ub (1 c) (and less 0). either case,
ab = 0.
consider complementarity constraint xi (h)wi (h) = 0 non-linear program (82)-(90) above. wish separate constraint pair linear constraints.
recall xi (h) represents weight h wi (h) represents regret h.
first requirement convert constraint pair linear constraints lower
bound values two terms 0. indeed case since xi (h) wi (h)
constrained non-negative NLP. Next, require upper bounds
weights histories regrets histories. shown Lemma 3.1 upper
bound value xi (h) h 1. upper bounds regrets histories,
require calculus.
354

fiMathematical Programming DEC-POMDPs

policy pi agent holds
X
pi (h) = |Oi |T 1 .

(93)

hEi

Therefore, every i-reduced joint policy hq1 , q2 , , qn Xi , holds

X
|Ok |T 1
qk (jk ) =
j Ei kI\{i}

(94)

kI\{i}

Since regret terminal history h agent given hq1 , q2 , , qn defined
X


(95)
(h, q) = max
qk (jk ) R(, hh , j i) R(, hh, j i) ,
h (h)

j Ei kI\{i}

conclude upper bound Ui (h) regret terminal history h Ei
agent is,



1


Ui (h) =
|Ok |
max max
R(, hh , j i) min R(, hh, j i) . (96)

kI\{i}

h (h) j Ei

j Ei

let us consider upper bounds regrets non-terminal histories. Let
information set length agent i. Let Ei () Ei denote set terminal histories
agent first 2t elements history set identical . Let h
history length agent i. Let Ei (h) Ei denote set terminal histories
first 2t - 1 elements history set identical h. Since policy
pi agent i, holds
X
pi (h ) |Oi |T
(97)
h Ei (h)

conclude upper bound Ui (h) regret nonterminal history
h Ni length agent




max
max
R(,
hh
,
j
i)

min
min
R(,
hg,
j
i)
(98)
Ui (h) = Li


h Ei ((h)) j Ei

gEi (h) j Ei


Li = |Oi |T



|Ok |T 1 .

(99)

kI\{i}

Notice = (that is, h terminal) (98) reduces (96).
So, complementarity constraint xi (h)wi (h) = 0 separated pair linear
constraints using 0-1 variable bi (h) follows,
xi (h) 1 bi (h)
wi Ui (h)bi (h)
bi (h) {0, 1}
355

(100)
(101)
(102)

fiAras & Dutech

Variables:
xi (h), wi (h) bi (h) {1, 2} h Hi
yi () {1, 2}
Maximize

y1 ()

(103)

subject to:
X

xi (a) = 1

(104)

aAi

xi (h) +

X

= 1, 2, h Ni , Oi

(105)

= 1, 2, h Ni

(106)

R(, hh, h i)x2 (h ) = w1 (h),

h E1

(107)

R(, hh , hi)x1 (h ) = w2 (h),

h E2

(108)

xi (h.o.a) = 0,

aAi

yi ((h))

X

yi (h.o) = wi (h),

oOi

y1 ((h))

X

h E2

y2 ((h))

X

h E1

xi (h) 1 bi (h),
wi (h) Ui (h)bi (h),

= 1, 2, h Hi
= 1, 2, h Hi

(109)
(110)

xi (h) 0,

= 1, 2, h Hi

(111)

wi (h) 0,

= 1, 2, h Hi

(112)

bi (h) {0, 1},

= 1, 2, h Hi

yi () (, +),

(113)

= 1, 2, (114)

Table 4: MILP-2 agents. 0-1 mixed integer linear program, derived game
theoretic considerations, finds optimal stochastic joint policies DEC-POMDPs
2 agents.

5.3 Program 2 Agents
combine policy constraints (Section 3.2), constraints seen
policy best response (Sections 5.1, 5.2) maximization value
joint policy, derive 0-1 mixed integer linear program solution
optimal joint policy DEC-POMDP 2 agents. Table 4 details program
call MILP-2 agents.
Example formulation decentralized Tiger problem 2 agents
horizon 2 found appendices, Section E.4
variables program vectors xi , wi , bi yi agent i. Note
agent history h agent i, Ui (h) denotes upper bound
regret history h.
356

fiMathematical Programming DEC-POMDPs

solution (x , , w , b ) MILP-2 agents consists following quantities: (i)
optimal joint policy x = hx1 , x2 may stochastic; (ii) agent = 1,
2, history h Hi , wi (h), regret h given policy xi agent;
(iii) agent = 1, 2, information set , yi (), value given
policy xi agent; (iv) agent = 1, 2, vector bi simply tells us
histories support xi ; history h agent bi (h) = 1
support xi . Note replace y1 () y2 () objective function
without affecting program. following result.
Theorem 5.1. Given solution (x , w , , b ) MILP-2 agents, x = hx1 , x2
optimal joint policy sequence-form.
Proof: Due policy constraints agent, xi sequence-form policy
agent i. Due constraints (106)-(108), yi contains values information sets
agent given xi . Due complementarity constraints (109)-(110), xi best
response xi . Thus hx1 , x2 Nash equilibrium. Finally, maximizing value
null information set agent 1, effectively maximizing value hx1 , x2 i.
Thus hx1 , x2 optimal joint policy.

comparison MILP presented Table 3, MILP-2 agents
constitutes particularly effective program term computation time finding 2agent optimal -period joint policy much smaller program. number
variables required MILP exponential n, number variables required
MILP-2 agents exponential . represents major reduction size
lead improvement term computation time.
5.4 Program 3 Agents
number agents 2, constraint (86) non-linear program
(82)-(90) longer complementarity constraint
2 variables could linQ
earized before. particular, term kI\{i} xk (jk ) constraint (86) involves
many variables different agents. linearize term, restrict pure joint policies exploit combinatorial facts number
histories involved. leads 0-1 mixed linear program called MILP-n agents
depicted Table 5.
variables program MILP-n agents vectors xi , wi , bi yi
agent vector z. following result.
Theorem 5.2. Given solution (x , w , , b , z ) MILP-n agents, x = hx1 , x2 ,
, xn pure -period optimal joint policy sequence-form.
Proof: Due policy constraints domain constraints agent,
pure sequence-form policy agent i. Due constraints (118)-(119), yi
contains values information sets agent given xi . Due complementarity
constraints (122)-(123), xi best response xi . Thus x Nash equilibrium.
Finally, maximizing value null information set agent 1, effectively
maximizing value x . Thus x optimal joint policy.

xi

357

fiAras & Dutech

Variables:
xi (h), wi (h) bi (h) h Hi
yi () I,
z(j) j E
Maximize

y1 ()

(115)

xi (a) = 1

(116)

xi (h.o.a) = 0,

I, h Ni , Oi (117)

subject to:
X

aAi

xi (h) +

X

aAi

yi ((h))

X

yi (h.o) = wi (h),

I, h Ni

(118)

oOi

yi ((h))

X
1
R(, hh, ji i)z(j) = wi (h), I, h Ei

1
|Oi |
jE
X

z(hh, j i) = xi (h)
|Ok |T 1 ,
j Ei

(119)

kI\{i}

I, h Ei
X

z(j) =
|Oi |T 1
jE

(120)
(121)

iI

xi (h) 1 bi (h),

I, h Hi (122)

wi (h) Ui (h)bi (h),

I, h Hi (123)

xi (h) 0,

I, h Ni

xi (h) {0, 1}
wi (h) 0,

I, h Ei

I, h Hi

bi (h) {0, 1},

h Hi

yi () (, +),
z(j) [0, 1],

j E

(124)
(125)
(126)
(127)

I, i(128)
(129)

Table 5: MILP-n agents. 0-1 mixed integer linear program, derived game
theoretic considerations, finds pure optimal joint policies DEC-POMDPs
3 agents.

358

fiMathematical Programming DEC-POMDPs

Compared MILP Table 3, MILP-n agents roughly size
real valued variables 0-1 variables. precise, P
MILP 0-1
variable every terminal history every agent (that approximatively iI |Ai |T |Oi |T 1
integer variables) MILP-n agents two 0-1 variables
every terminal well
P
nonterminal history agent (approximatively 2 iI (|Ai ||Oi |)T integer variables).

5.5 Summary

formulation solution DEC-POMDP application Duality
Theorem Linear Programs allow us formulate solution DEC-POMDP
solution new kind 0-1 MILP. 2 agents, MILP O(kT ) variables
constraints thus smaller MILP previous section. Still,
MILPS quite large next section investigates heuristic ways speed
resolution.

6. Heuristics Speeding Mathematical Programs
section focusses ways speed resolution various MILPs presented
far. Two ideas exploited. First, show prune set sequence-form policies
removing histories provably part optimal joint policy.
histories called locally extraneous. Then, give lower uppers bounds
objective function MILPs, bounds sometimes used branch
bound method often used MILP solvers finalize values integer variables.
6.1 Locally Extraneous Histories
locally extraneous history history required find optimal joint policy
initial state DEC-POMDP could replaced co-history
without affecting value joint policy. co-history history h agent
defined history agent identical h aspects except last
action. Ai = {b, c}, co-history c.u.b.v.b history c.u.b.v.c. set
co-histories history h shall denoted C(h).
Formally, history h Hit length agent said locally extraneous if,
i-reduced joint histories length t,
every probability distribution set Hi
exists history h C(h)
X


(j ) R(, hh , j i) R(, hh, j i)
0
(130)
j Hti

(j ) denotes probability j .
alternative definition follows. history h Hit length agent said
locally extraneous exists probability distribution set co-histories
h i-reduced joint history j length t, holds
X
(h )R(, hh , j i) R(, hh, j i)
(131)
h C(h)

359

fiAras & Dutech

(h ) denotes probability co-history h .
following theorem justifies incremental pruning locally extraneous histories
search optimal joint policies faster performed smaller
set possible support histories.
Theorem 6.1. every optimal -period joint policy p agent
terminal history h agent locally extraneous , pi (h) > 0, exists
another -period joint policy p optimal identical p respects
except pi (h) = 0.
Proof: Let p -period joint policy optimal . Assume
agent terminal history h agent locally extraneous , pi (h) > 0.
(130), exists least one co-history h h that,
X


pi (j ) R(, hh , j i) R(, hh, j i)
0.
(132)
j HT


Let q -period policy agent identical pi respects except q(h )
= pi (h) + pi (h ) q(h) = 0. shall show q also optimal . holds,
X

j HT


V(, hq, pi i) V(, hpi , pi i) =


pi (j ) R(, hh , j i)q(h ) R(, hh , j i)pi (h ) R(, hh, j i)pi (h)
=
X

j HT




pi (j ) R(, hh , j i)(q(h ) pi (h )) R(, hh, j i)pi (h)
=
X

j HT




pi (j ) R(, hh , j i)pi (h) R(, hh, j i)pi (h)

since q(h ) = pi (h) + pi (h ). Therefore,
X

j HT


V(, hq, pi i) V(, hpi , pi i) =


pi (j ) R(, hh , j i) R(, hh, j i)
0 (due (132)).

Hence, p = hq, pi also optimal -period joint policy .



One could also wonder order extraneous histories pruned important
not. answer question, following theorem shows many co-histories
extraneous, pruned order as:
either value, one pruned ;
pruning one change fact others still extraneous.
Theorem 6.2. two co-histories h1 h2 locally extraneous, either values
equal h also locally extraneous
R(, hh1 , j i) R(, hh2 , j i)for j Hi
1
relatively C(h) \ {h2 }.
360

fiMathematical Programming DEC-POMDPs

Proof: Let C + denotes union C(h1 ) C(h2 ). immediately C(h1 ) =
C + \ {h1 } C(h2 ) = C + \ {h2 }. h1 (resp. h2 ) locally extraneous means
exists probability distribution 1 C(h1 ) (resp. 2 C(h2 )) that, j
:
Hi
X
1 (h )R(, hh , j i) R(, hh1 , j i)
(133)
h C + \{h1 }

X

2 (h )R(, hh , j i) R(, hh2 , j i)

(134)

h C + \{h2 }

(135)
Eq. (133) expanded in:
X

1 (h2 )R(, hh2 , j i) +

h C + \{h

1 (h )R(, hh , j i) R(, hh1 , j i).
1 ,h2 }

Using (134) (136) gives
X
1 (h2 )
2 (h )R(, hh , j i) +
h C + \{h2 }

leading
X

(136)

X

1 (h )R(, hh , j i) R(, hh1 , j i)

h C + \{h1 ,h2 }

(137)

(1 (h2 )2 (h ) + 1 (h ))R(, hh , j i) (1 1 (h2 )2 (h1 ))R(, hh1 , j i) (138)

h C + \{h1 ,h2 }

So, two cases possible:
1 (h2 ) = 2 (h1 ) = 1. case, R(, hh2 , j i) R(, hh1 , j i) R(, hh1 , j i)
.
R(, hh2 , j i), R(, hh1 , j i) = R(, hh2 , j i) j Hi
1 (h2 )2 (h1 ) < 1. case have:
X

h C + \{h1 ,h2 }

1 (h2 )2 (h ) + 1 (h )
R(, hh , j i) R(, hh1 , j i)
1 1 (h2 )2 (h1 )

(139)

meaning even without using h2 , h1 still locally extraneous
1 (h2 )2 (h )+1 (h )
probability distribution C + \ {h1 , h2 }
11 (h2 )2 (h1 )
X

h C + \{h

1 ,h2 }

1 (h2 )(1 2 (h1 )) + (1 1 (h2 ))
1 (h2 )2 (h ) + 1 (h )
=
1 1 (h2 )2 (h1 )
1 1 (h2 )2 (h1 )
1 1 (h2 )2 (h1 )
1 1 (h2 )2 (h1 )
= 1.
=

(140)
(141)
(142)


361

fiAras & Dutech

order prune locally extraneous histories, one must able identify histories.
indeed two complementary ways this.
first method relies definition value history (see Section 3.3),

R(, hh, j i) = (, hh, j i)R(, hh, j i).

(143)

Therefore,
(, hh, j i) = 0,


j Hi

(144)

true history h, means every joint history length occurring
given history part priori probability 0. thus, h clearly
extraneous. Besides, every co-history h also locally extraneous share
probabilities.
second test needed locally extraneous histories verify (144).
again, turn linear programing particular following linear program

Variables: y(j), j Hi
Minimize



(145)

subject to:
X

j Hti



y(j ) R(, hh , j i) R(, hh, j i)
,
X

h C(h)

y(j ) = 1

(146)
(147)

j Hti

following Lemma.

y(j ) 0,


j Hi

(148)

Lemma 6.1. If, exists solution ( , ) linear program (145)-(148) 0,
h locally extraneous.
Proof : Let ( , ) solution LP (145)-(148). probability distribution
due constraints (147)-(148). 0, since minimizing , due
Hi
), every co-history h h
constraints (146), every (Hi
X


y(j ) R(, hh , j i) R(, hh, j i)
.
(149)
j Hti

Therefore, definition, h locally extraneous.



following procedure identifies locally extraneous terminal histories agents
proceed iterative pruning. mainly motivated Theorems 6.1 6.2
effectively removing extraneous histories. procedure similar procedure
iterated elimination dominated strategies game (Osborne & Rubinstein, 1994).
concept also quite similar process policy elimination backward step
dynamic programming partially observable stochastic games (Hansen et al., 2004).
362

fiMathematical Programming DEC-POMDPs

Step 1: agent I, set HiT Ei . Let H denote set iI HiT .
joint history j H , compute store value R(, j) j joint
observation sequence probability (, j) j.
Step 2: agent I, history h HiT , i-reduced joint
, (, hh, j i) = 0, remove h H .
history j Hi

Step 3: agent I, history h HiT follows: C(h) HiT
non-empty, check whether h locally extraneous setting solving
set H set C(h)
LP (145)-(148). setting LP, replace Hi

set C(h) HiT . upon solving LP, h found locally extraneous
, remove h HiT .
Step 4: Step 3 history (of agent) found locally extraneous, go
Step 3. Otherwise, terminate procedure.
procedure builds set HiT agent i. set contains every terminal
history agent required finding optimal joint policy , every
terminal history locally extraneous . agent i, every history
HiT HiT locally extraneous. reason reiterating Step 3
history h agent found locally extraneous consequently removed
HiT , possible history agent previously locally
extraneous becomes so, due removal h HiT . Hence, order verify
case history not, reiterate Step 3.
Besides, Step 2 procedure also prunes histories impossible given
model DEC-POMDP observation sequence observed.
last pruning step taken order remove non-terminal histories
lead extraneous terminal histories. last step recursive, starting histories
horizon 1, remove histories hi non-extraneous terminal histories,
is, histories hi h.o.a extraneous Ai Oi .
Complexity algorithm pruning locally extraneous histories exponential
complexity. joint history must examined compute value occurence
probability. Then, worst case, Linear Program run every local history
order check extraneous not. Experimentations needed see prunning
really interesting.
6.2 Cutting Planes
Previous heuristics aimed reducing search space linear programs,
incidentally good impact time needed solve programs. Another option
directly aims reducing computation time use cutting planes (Cornuejols,
2008). cut (Dantzig, 1960) special constraint identifies portion set
feasible solutions optimal solution provably lie. Cuts used
conjunction various branch bounds mechanism reduce number possibles
combination integer variables examined solver.
present two kinds cuts.
363

fiAras & Dutech

Variables: y(j), j H
Maximize

X

R(, j)y(j)

(153)

jE

subject to,
X

y(a) = 1

(154)

aA

y(j) +

X

y(j.o.a) = 0,

j N ,

(155)

j H

(156)

aA

y(j) 0,

Table 6: POMDP. linear program finds optimal policy POMDP.
6.2.1 Upper Bound Objective Function
first cut propose upper bound POMDP cut. value optimal
-period joint policy given DEC-POMDP bounded value
VP optimal -period policy POMDP derived DEC-POMDP.
derived POMDP DEC-POMDP assuming centralized controller (i.e.
one agent using joint-actions).
sequence-form representation POMDP quite straightforward. Calling H
set Tt=1 Ht joint histories lengths less equal N set H\E nonterminal joint histories, policy POMDP horizon sequence-form function
q H [0, 1] that:
X
q(a) = 1
(150)
aA

q(j) +

X

q(j.o.a) = 0,

j N ,

(151)

aA

value VP (, q) sequence-form policy q given by:
X
VP (, q) =
R(, j)q(j)

(152)

jE

Thereby, solution linear program Table 6 P
optimal policy
POMDP horizon optimal value POMDP jE R(, j)y (j). So,
value V(, p ) optimal joint policy p = hp1 , p2 , , pn DEC-POMDP
bounded value VP (, q ) associated POMDP.
Complexity complexity finding upper bound linked complexity
solving POMDP which, showed Papadimitriou Tsitsiklis (1987), PSPACE
(i.e. require memory polynomial size problem, leading possible
exponential complexity time). again, experimentation help us decide
cases upper bound cut efficient.
364

fiMathematical Programming DEC-POMDPs

6.2.2 Lower Bound Objective Function
case DEC-POMDPs non-negative reward, trivial show value
-period optimal policy bounded value 1 horizon optimal
value. So, general case, take account lowest reward possible
compute lower bound say that:
X
R(, j)z(j) V 1 () + min min R(s, a)
(157)
aA sS

jE

V 1 value optimal policy horizon 1. reasoning leads
iterated computation DEC-POMDPs longer longer horizon, reminiscent
MAA* algorithm (Szer et al., 2005). Experiments tell worthwhile solve bigger
bigger DEC-POMDPs take advantage lower bound better directly
tackle horizon problem without using lower bound.
Complexity compute lower bound, one required solve DEC-POMDP whith
horizon one step shorter current horizon. complexity clearly
least exponential. experiments, value DEC-POMDP used
DEC-POMDP bigger horizon. case, computation time
augmented best time solve smaller DEC-POMDP.
6.3 Summary
Pruning locally extraneous histories using bounds objective function
practical use software solving MILPs presented paper. Pruning histories
means space policies used MILP reduced and, formulation
MILP depends combinatorial characteristics DEC-POMDP, MILP
must altered show Appendix D.
Validity far cuts concerned, alter solution found MILPs,
solution MILPs still optimal solution DEC-POMDP. extraneous histories pruned, least one valid policy left solution because, step
3 algorithm, history pruned co-histories left. Besides,
reduced set histories still used build optimal policy Theroem 6.1.
consequence, MILP build reduced set histories admit solution
solution one optimal joint policy.
next section, experimental results allow us understand cases
heuristics introduced useful.

7. Experiments
mathematical programs heuristics designed paper tested four
classical problems found literature. problems, involving two agents,
mainly compared computation time required solve DEC-POMDP using Mixed
Integer Linear Programming methods computation time reported methods found
literature. tested programs three-agent problems randomly
designed.
365

fiAras & Dutech

Problem
MABC
MA-Tiger
Fire Fighting
Grid Meeting
Random Pbs

|Ai |
2
3
3
5
2

|Oi |
2
2
2
2
2

|S|
4
2
27
16
50

n
2
2
2
2
3

Table 7: Complexity various problems used test beds.

MILP MILP-2 solved using iLog Cplex 10 solver commercial set
Java packages relies combination Simplex Branch Bounds
methods (Fletcher, 1987). software run Intel P4 3.4 GHz 2Gb
RAM using default configuration parameters. mathematical programs, different
combination heuristics evaluated: pruning locally extraneous histories, using
lower bound cut using upper bound cut, respectively denoted LOC, Low
result tables come.
Non-Linear Program (NLP) Section 3.4 evaluated using various solvers NEOS website (http://www-neos.mcs.anl.gov ), even thought
method guarantee optimal solution DEC-POMDP. Three solvers
used: LANCELOT (abbreviated LANC.), LOQO SNOPT.
result tables also report results found literature following algorithms:
DP stands Dynamic Programming Hansen et al. (2004); DP-LPC improved
version Dynamic Programming policies compressed order fit
memory speed evaluation proposed Boularias Chaib-draa (2008);
PBDP extension Dynamic Programming pruning guided knowledge
reachable belief-states detailed work Szer Charpillet (2006); MAA*
heuristically guided forward search proposed Szer et al. (2005) generalized
improved version algorithm called GMAA* developed Oliehoek et al. (2008).
problems selected evaluate algorithms detailed coming subsections.
widely used evaluate DEC-POMDPs algorithms literature
complexity, term space size, summarized Table 7.
7.1 Multi-Access Broadcast Channel Problem
Several versions Multi-Access Broadcast Channel (MABC) problem found
literature. use description given Hansen et al. (2004) allows
problem formalized DEC-POMDP.
MABC, given two nodes (computers) required send messages
common channel given duration time. Time imagined
split discrete periods. node buffer capacity one message.
buffer empty period refilled certain probability next period.
period, one node send message. nodes send message
period, collision messages occurs neither message transmitted. case
collision, node intimated collision signal. collision
366

fiMathematical Programming DEC-POMDPs

signaling mechanism faulty. case collision, certain probability,
send signal either one nodes.
interested pre-allocating channel amongst two nodes given number
periods. pre-allocation consists giving channel one nodes period
function nodes information period. nodes information period
consists sequence collision signals received till period.
modeling problem DEC-POMDP, obtain 2-agent, 4-state, 2-actionsper-agent, 2-observations-per-agent DEC-POMDP whose components follows.
node agent.
state problem described states buffers two nodes.
state buffer either Empty Full. Hence, problem four states:
(Empty, Empty), (Empty, Full), (Full, Empty) (Full, Full).
node two possible actions, Use Channel Dont Use Channel.
period, node may either receive collision signal may not. node
two possible observations, Collision Collision.
initial state problem (Full, Full). state transition function P,
joint observation function G reward function R taken Hansen et al.
(2004). agents full buffers period, use channel period,
state problem unchanged next period; agents full buffers
next period. agent full buffer period uses channel
period, buffer refilled certain probability next period.
agent 1, probability 0.9 agent 2, probability 0.1. agents
empty buffers period, irrespective actions take period, buffers
get refilled probabilities 0.9 (for agent 1) 0.1 (for agent 2).
observation function G follows. state period (Full, Full)
joint action taken agents previous period (Use Channel, Use Channel),
probability receive collision signal 0.81, probability one
receives collision signal 0.09 probability neither receives
collision signal 0.01. state problem may period
joint action agents may taken previous period, agents receive
collision signal.
reward function R quite simple. state period (Full, Empty)
joint action taken (Use Channel, Dont Use Channel) state period
(Empty, Full) joint action taken (Dont Use Channel, Use Channel), reward
1; combination state joint action, reward 0.
evaluated various algorithms problem three different horizons (3,
4 5) respective optimal policies value 2.99, 3.89 4.79. Results
detailed Table 8 where, horizon algorithm, value computation
time best policy found given.
results show MILP compares favorably classical algorithms except
GMAA* always far better horizon 4 and, horizon 5, roughly within
367

fiAras & Dutech

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP
Cplex
Low
MILP
Cplex

MILP
Cplex
LOC
MILP
Cplex
LOC, Low
MILP
Cplex
LOC,
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
DP
Dyn. Prog.
DP-LPC
Dyn. Prog.
PBDP
Dyn. Prog.
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 3
Value
Time
2.99
0.86
2.99 0.10 / 0.93
2.99 0.28 / 1.03
2.99 0.34 / 0.84
2.99 0.44 / 0.84
2.99 0.62 / 0.93
2.99
0.39
2.90
0.01
2.99
0.02
2.90
0.01
Value
Time
2.99
5
2.99
0.36
2.99
< 1s
2.99
< 1s
?
?

Horizon 4
Value
Time
3.89
900
3.89
0.39 / 900
3.89
0.56 / 907
3.89
1.05 / 80
3.89
1.44 / 120
3.89 1.61 / 10.2
3.89
3.53
3.17
0.01
3.79
0.95
3.79
0.05
Value
Time
3.89
17.59
3.89
4.59
3.89
2
3.89
5400
3.89
0.03

Horizon 5
Value
Time
-m
3.5 / -m
4.73 / -m
2.27 / -t
5.77 / -t
4.79
7.00 / 25
-m
4.70
0.21
4.69
20
4.69
0.18
Value
Time
-m
-m
4.79
105
-t
4.79
5.68

Table 8: MABC Problem. Value computation time (in seconds) solution
problem computed several methods, best results highlighted.
appropriate, time shows first time used run heuristics global
time, format heuristic/total time. -t means timeout 10,000s;
-m indicates problem fit memory ? indicates
algorithm tested problem.

order magnitude MILP pertinent heuristics. expected, apart
simplest setting (horizon 3), NLP based resolution find optimal policy
DEC-POMDP, computation time lower methods. Among
MILP methods, MILP-2 better MILP even best heuristics horizon 3
4. size problem increases, heuristics way MILPs
able cope size problem. table also shows that, MABC
problem, pruning extraneous histories using LOC heuristic always good method
investigation revealed 62% heuristics proved locally extraneous.
far cutting bounds concerned, dont seem useful first (for horizon
3 4) necessary MILP find solution horizon 5. problem,
one must also mind one optimal policy horizon.
7.2 Multi-Agent Tiger Problem
explained section 2.2, Multi-Agent Tiger problem (MA-Tiger) introduced
paper Nair et al. (2003). general description problem, ob368

fiMathematical Programming DEC-POMDPs

Joint Action
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(*, *)

State
Left
Left
Left
Left
Right
Right
Right
Right
*

Joint Observation
(Noise Left, Noise Left)
(Noise Left, Noise Right)
(Noise Right, Noise Left)
(Noise Right, Noise Right)
(Noise Left, Noise Left)
(Noise Left, Noise Right)
(Noise Right, Noise Left)
(Noise Right, Noise Right)
(*, *)

Probability
0.7225
0.1275
0.1275
0.0225
0.0225
0.1275
0.1275
0.7225
0.25

Table 9: Joint Observation Function G MA-Tiger Problem.
tain 2-agent, 2-state, 3-actions-per-agent, 2-observations-per agent DEC-POMDP whose
elements follows.
person agent. So, 2-agent DEC-POMDP.
state problem described location tiger. Thus, consists
two states Left (tiger behind left door) Right (tiger behind right
door).
agents set actions consists three actions: Open Left (open left door),
Open Right (open right door) Listen (listen).
agents set observations consists two observations: Noise Left (noise coming
left door) Noise Right (noise coming right door).
initial state equi-probability distribution S. state transition function P,
joint observation function G reward function R taken paper Nair
et al. (2003). P quite simple. one agents opens door period, state
problem next period set back . agents listen period, state
process unchanged next period. G, given Table (9), also quite simple.
Nair et al. (2003) describes two reward functions called B problem,
report results reward function A, given Table 10, behavior
algorithm similar reward functions. optimal value problem
horizons 3 4 respectively 5.19 4.80.
horizon 3, dynamic programming forward search methods generally better
mathematical programs. contrary horizon 4 computation time MILP Low heuristic significatively better other, even
GMAA*. Unlike MABC, pruning extraneous histories improve methods
based MILP, quite understandable deeper investigations showed
extraneous histories. Using lower cutting bounds proves efficient
seen kind heuristic search best policy ; directly set policies (like
369

fiAras & Dutech

Joint Action
(Open Right, Open Right)
(Open Left, Open Left)
(Open Right, Open Left)
(Open Left, Open Right)
(Listen, Listen)
(Listen, Open Right)
(Open Right, Listen)
(Listen, Open Left)
(Open Left, Listen)

Left
20
-50
-100
-100
-2
9
9
-101
-101

Right
-50
20
-100
-100
-2
-101
-101
9
9

Table 10: Reward Function MA-Tiger Problem.

GMAA*) set combination histories, may explain good behavior
MILP+Low.
must also noted problem, approximate methods like NLP also
algorithms depicted like Memory Bound Dynamic Programming
Seuken Zilberstein (2007) able find optimal solution. And, again,
methods based NLP quite fast sometimes accurate.
7.3 Fire Fighters Problem
problem Fire Fighters (FF) introduced new benchmark Oliehoek
et al. (2008). models team n fire fighters extinguish fires row nh
houses.
state house given integer parameter, called fire level f ,
takes discrete value 0 (no fire) nf (fire maximum severity). every time
step, agent move one house. two agents house,
extinguish existing fire house. agent alone, fire level lowered
0.6 probability neighbor house also burning 1 probability otherwise.
burning house fireman present increase fire level f one point 0.8
probability neighbor house also burning probability 0.4 otherwise.
unattended non-burning house catch fire probability 0.8 neighbor house
burning. action, agents receive reward f house still
burning. agent observe flames location probability
depends fire level: 0.2 f = 0, 0.5 f = 1 0.8 otherwise. start,
agents outside houses fire level houses sampled
uniform distribution.
model following characteristics:
na agents, nh actions nf possible informations.

h 1
states nnf h possible states burning houses
nnf h . na +n
n


h 1
different ways distribute na fire fighters houses.
na +n
na
example, 2 agents 3 houses 3 levels fire lead 9 6 = 54 states. But,
370

fiMathematical Programming DEC-POMDPs

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP
Cplex
Low
MILP
Cplex

MILP
Cplex
LOC
MILP
Cplex
LOC, Low
MILP
Cplex
LOC,
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
DP
Dyn. Prog.
DP-LPC
Dyn. Prog.
PBDP
Dyn. Prog.
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 3
Value
Time
5.19
3.17
5.19 0.46 / 4.9
5.19 0.42 / 3.5
5.19 1.41 / 6.4
5.19 1.88 / 7.6
5.19 1.83 / 6.2
5.19
11.16
-45
0.03
5.19
0.47
5.19
0.01
Value
Time
5.19
2.29
5.19
1.79
?
?
5.19
0.02
5.19
0.04

Horizon 4
Value
Time
-t
4.80
3.5 / 72
0.75 / -t
16.0 / -t
4.80 19.5 / 175
16.75 / -t
-t
-9.80
4.62
4.80
514
4.78
91
Value
Time
-m
4.80
534
?
?
4.80
5961
4.80
3208

Table 11: MA-Tiger Problem. Value computation time (in seconds) solution
problem computed several methods, best results highlighted.
appropriate, time shows first time used run heuristics
global time, format heuristic/total time.-t means timeout
10.000s; -m indicates problem fit memory ? indicates algorithm tested problem.

371

fiAras & Dutech

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 3
Value
Time
-t
-5.98
38
-5.98
0.05
-5.98
2.49
-6.08
0.24
Value
Time
(-5.73) 0.29
(-5.73)
0.41

Horizon 4
Value
Time
-t
-t
-7.08
4.61
-7.13
1637
-7.14
83
Value
Time
(-6.57) 5737
(-6.57) 5510

Table 12: Fire Fighting Problem. Value computation time (in seconds) solution problem computed several methods, best results highlighted.
-t means timeout 10.000s. MAA* GMAA*, value parenthesis
taken work Oliehoek et al. (2008) optimal
different optimal values.

possible use information joint action reduce number state
needed transition function simply nnf h , meaning 27 states 2 agents
3 houses 3 levels fire.
Transition, observation reward functions easily derived description.
problem, dynamic programming based methods tested problem
formulation quite new. horizon 3, value optimal policy given Oliehoek
et al. (2008) (5.73) differs value found MILP algorithms (5.98) whereas
methods supposed exact. might come slight differences
respective formulation problems. horizon 4, Oliehoek et al. (2008) report
optimal value (6.57).
problem, MILP methods clearly outperformed MAA* GMAA*.
NLP methods, give optimal solution horizon 3, better term
computation time. might NLP also able find optimal policies horizon 4
setting differs work Oliehoek et al. (2008), able check
policy found really optimal. main reason superiority forward
search method lies fact problem admits many many optimal policies
value. fact, horizon 4, MILP-based methods find optimal policy quite
quickly (around 82s MILP-2) then, using branch-and-bound, must evaluate
potential policies knowing indeed found optimal policy. Forward
search methods stop nearly soon hit one optimal solution.
Heuristics reported as, improve performance MILP
take away computation time thus results worse.
372

fiMathematical Programming DEC-POMDPs

7.4 Meeting Grid
problem called Meeting grid deals two agents want meet stay
together grid world. introduced work Bernstein, Hansen,
Zilberstein (2005).
problem, two robots navigating two-by-two grid world
obstacles. robot sense whether walls left right,
goal robots spend much time possible square. actions
move up, down, left right, stay square. robot attempts
move open square, goes intended direction probability 0.6,
otherwise randomly either goes another direction stays square.
move wall results staying square. robots interfere
cannot sense other. reward 1 agents share square,
0 otherwise. initial state distribution deterministic, placing robots
upper left corner grid.
problem modelled DEC-POMDP where:
2 agents, one 5 actions observations (wall left, wall
right).
16 states, since robot 4 squares time.
Transition, observation reward functions easily derived description.
problem, dynamic programming based methods tested problem
formulation quite new. problem intrinsically complex FF
solved horizon 2 3. Again, optimal value found method differ
value reported Oliehoek et al. (2008). Whereas found optimal values
1.12 1.87 horizon 2 3, report optimal values 0.91 1.55.
Results problem roughly pattern results FF
problem. MAA* GMAA* quicker MILP, time MILP able find
optimal solution horizon 3. NLP methods give quite good results slower
GMAA*. FF, numerous optimal policies MILP methods
able detect policy found quickly indeed optimal.
Again, heuristics reported as, improve performance
MILP take away computation time thus results worse.
7.5 Random 3-Agent Problems
test approach problems 3 agents, used randomly generated DECPOMDPs state transition function, joint observation function reward
functions randomly generated. DEC-POMDPs 2 actions 2 observations
per agent 50 states. Rewards randomly generated integers range 1 5.
complexity family problem quite similar complexity MABC
problem (see Section 7.1).
373

fiAras & Dutech

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 2
Value Time
1.12
0.65
1.12
0.61
0.91
0.01
1.12
0.06
1.12
0.07
Value Time
(0.91)
0s
(0.91)
0s

Horizon 3
Value Time
1.87
1624
-t
1.26
1.05
1.87
257
0.48
81
Value Time
(1.55)
10.8
(1.55) 5.81

Table 13: Meeting Grid Problem. Value computation time (in seconds)
solution problem computed several methods, best results
highlighted. -t means timeout 10.000s. MAA* GMAA*, value
parenthesis taken work Oliehoek et al. (2008)
optimal different optimal values...

Program
MILP
MILP-2

Least Time (secs)
2.45
6.85

Time (secs)
455
356

Average
120.6
86.88

Std. Deviation
183.48
111.56

Table 14: Times taken MILP MILP-2 2-agent Random Problem horizon 4.

order assess real complexity Random problem, first tested
two-agent version problem horizon 4. Results averaged 10 runs
programs given Table 14. compared MABC problem seemed
comparable complexity, Random problem proves easier solve (120s vs 900s).
problem, number 0-1 variable relatively small, weight much
resolution time MILP-2 thus faster.
Results three-agent problem horizon 3 given Table 15,
averaged 10 runs. Even though size search space smaller case
(for 3 agents horizon 3, 9 1021 policies whereas problem 2
agents horizon 4, 1.5 1051 possible policies), 3 agent problems seems
difficult solve, demonstrating one big issue policy coordination. Here,
heuristics bring significative improvement resolution time MILP. predicted,
MILP-n efficient given completeness.
374

fiMathematical Programming DEC-POMDPs

Program
MILP
MILP-Low
MILP-n

Least Time (secs)
21
26
754

Time (secs)
173
90
2013

Average
70.6
53.2
1173

Std. Deviation
64.02
24.2
715

Table 15: Times taken MILP MILP-n 3-agent Random problem horizon 3.

8. Discussion
organized discussion two parts. first part, analyze results
offer explanations behavior algorithms usefulness heuristics. Then,
second part, explicitely address important questions.
8.1 Analysis Results
results, appears MILP methods better alternative Dynamic Programming methods solving DEC-POMDPs globally generally clearly outperformed forward search methods. structure thus characteristics
problem big influence efficiency MILP methods. Whereas seems
behavior GMAA* terms computation time quite correlated
complexity problem (size action observation spaces), MILP methods seem
sometimes less correlated complexity. case MABC problem (many
extraneous histories pruned) MA-Tiger problem (special structure)
outperform GMAA*. contrary, many optimal policies exists, forward
search methods like GMAA* clearly better choice. Finally, Non-Linear Programs,
even though guarantee optimal solution, generally good alternative
sometimes able find good solution computation time often
better GMAA*. might prove useful approximate heuristic-driven forward
searches.
computational record two 2-agent programs shows MILP-2 agents
slower MILP horizon grows. two reasons sluggishness
MILP-2 agents may attributed. time taken branch bound (BB)
method solve 0-1 MILP inversely proportional number 0-1 variables
MILP. MILP-2 agents many 0-1 variables MILP event hough
total number variables exponentially less MILP. first reason.
Secondly, MILP-2 agents complicated program MILP; many
constraints MILP. MILP simple program, concerned finding subset
given set. addition finding weights histories, MILP also finds weights
terminal joint histories. extra superfluous quantity forced find.
hand, MILP-2 agents takes much circuitous route, finding many
superfluous quantities MILP. addition weights histories, MILP-2 agents
also finds supports policies, regrets histories values information sets. Thus,
375

fiAras & Dutech

Problem

Heuristic

MABC

LOC
Low

LOC
Low

LOC

MA-Tiger

Meeting

Horizon 2
Time #pruned

0.41

0/18

1.36

15/50*

Horizon 3
Time #pruned
0.34
14/32
0.10
0.28
1.41
0/108
0.46
0.42
74.721 191/500*

Horizon 4
Time #pruned
1.047
74/128
0.39
3.89
16.0
0/648
3.5
0.75

Horizon 5
Time #pruned
2.27
350/512
3.5
4.73

Table 16: Computation time heuristics. LOC heuristics, give computation time seconds number locally extraneous histories pruned
total number histories (for agent). * denotes cases one
additional history prunned second agent. Low heuristic,
computation time given.

relaxation MILP-2 agents takes longer solve relaxation MILP.
second reason slowness BB method solves MILP-2 agents.
bigger problems, namely Fire-Fighters Meeting Grid, horizon
stays small, MILP-2 agents compete MILP slightly lower size.
complexity grows like O((|Ai ||Oi |)T ) whereas grows like O((|Ai ||Oi |)2T ) MILP.
small difference hold long number integer variables quickly lessens
efficiency MILP-2 agents.
far heuristic concerned, proved invaluable problems (MABC
MA-Tiger) useless others. case MABC, heuristics helpful
prune large number extraneous heuristics ultimately, combination
upper bound cut efficient horizon grows. case
MA-Tiger, although extraneous histories found, using lower bound cut heuristic
MILP leads quickest algorithm solving problem horizon 4.
problems, heuristics burden greedy computation
time speed resolution. example, Grid Meeting problem, time
taken prune extraneous histories bigger time saved solving problem.
result, added value using heuristics depends nature problem
(as depicted Table 16) but, right now, able predict usefulness without
trying them.
also emphasize results given lie limit possible solve
exact manner given memory computer used resolution, especially
terms horizon. Furthermore, number agent increases, length
horizon must decreased problems still solvable.
376

fiMathematical Programming DEC-POMDPs

8.2 Questions
mathematical programing approach presented paper raises different questions.
explicitly addressed questions appears important us.
Q1: sequence-form approach entirely doomed exponential
complexity?
number sequence-form joint policies grows doubly exponentially horizon number agents, sequence-form approach seems doomed, even compared
dynamic programming doubly exponential worst cases only. But, indeed,
arguments must taken consideration.
exponential number individual histories need evaluated. joint
part sequence-form left MILP solver. every computation done
particular history, like computing value checking extraneous, greater
reusability computations done entire policies. history shared many
joint policies individual policy. way, sequence-form allows us work
reusable part policies without work directly world distributions
set joint-policies.
Then, MILPs derived sequence-form DEC-POMDPs need memory size
grows exponentially horizon number agents. Obviously,
complexity quickly overwhelming also case every exact method
far. shown experiments, MILP approach derived sequence-form
compares quite well dynamic programming, even outperformed forward methods
like GMAA*.
Q2: MILP sometimes take little time find optimal joint
policy compared existing algorithms?
Despite complexity MILP approach, three factors contribute relative
efficiency MILP.
1. First, efficiency linear programming tools themselves. solving MILP,
BB method solves sequence linear programs using simplex algorithm.
LPs relaxation MILP. theory, simplex algorithm requires
worst case exponential number steps (in size LP) solving LP2 ,
well known that, practice, usually solves LP polynomial number
steps (in size LP). Since size relaxation MILP exponential
horizon, means that, roughly speaking, time taken solve relaxation
MILP exponential horizon whereas doubly exponential
methods.
2. second factor sparsity matrix coefficients constraints
MILP. sparsity matrix formed coefficients constraints
2. statement must qualified: worst case time requirement demonstrated
variants simplex algorithm. demonstrated basic version simplex
algorithm.

377

fiAras & Dutech

LP determines practice rate pivoting algorithm
simplex solves LP (this also applies Lemkes algorithm context
LCP). sparser matrix, lesser time required perform elementary
pivoting (row) operations involved simplex algorithm lesser space
required model LP.
3. third factor fact supplement MILP cuts; computational
experience clearly shows speeds computations. first two
factors related solving relaxation MILP (i.e., LP), third factor
impact BB method itself. upper bound cut identifies additional
terminating condition BB method, thus enabling terminate earlier
absence condition. lower bound cut attempts shorten list
active subproblems (LPs) BB method solves sequentially. Due
cut, BB method potentially lesser number LPs solve. Note
inserting lower bound cut, emulating forward search properties
A* algorithm.
Q3: know MILP-solver (iLogs Cplex experiments)
reason speedup?
Clearly, approach would slower, even sometime slower classical dynamic
programming approach used another program solving MILPs experimented also MILPs solvers NEOS website indeed
slow. true Cplex, solver used experiments, quite optimized.
Nevertheless, exactly one points wanted experiment paper:
one advantages formulating DEC-POMDP MILP possibility use
fact that, mixed integer linear programs important industrial world,
optimized solvers exist.
Then, formulate DEC-POMDP MILP mostly paper
about.
Q4: main contribution paper?
stated earlier paper, current algorithms DEC-POMDPs largely inspired POMDPs algorithms. main contribution pursue entirely different
approach, i.e., mixed integer linear programming. such, learned lot
DEC-POMDPs pro & con mathematical programming approach.
lead formulation new algorithms.
designing algorithms, have, first all, drawn attention new representation policy, namely sequence form policy, introduced Koller, Megiddo
von Stengel. sequence form policy compact representation
policy agent, afford compact representation set policies
agent.
algorithms proposed finite horizon DEC-POMDPs mathematical
programming algorithms. precise, 0-1 MILPs. MDP domain,
378

fiMathematical Programming DEC-POMDPs

mathematical programming long used solving infinite horizon case.
instance, infinite horizon MDP solved linear program (dEpenoux, 1963).
recently, mathematical programming directed infinite horizon POMDPs
DEC-POMDPs. Thus, infinite horizon DEC-MDP (with state transition independence) solved 0-1 MILP (Petrik & Zilberstein, 2007) infinite horizon
POMDP DEC-POMDP solved (for local optima) nonlinear program (Amato, Bernstein, & Zilberstein, 2007b, 2007a). finite horizon case much different
character infinite horizon case dealt using dynamic programming.
stated earlier, whereas dynamic programming quite successful finite horizon
MDPs POMDPs, less finite horizon DEC-POMDPs.
contrast, game theory, mathematical programming successfully directed
games finite horizon. Lemkes algorithm (1965) two-player normal form games,
Govindan-Wilson algorithm (2001) n-player normal form games Koller, Megiddo
von Stengel approach (which internally uses Lemkes algorithm) two-player extensive
form games finite-horizon games.
remained find way appropriate mathematical programming
solving finite horizon case POMDP/DEC-POMDP domain. work done
precisely (incidently, algorithm solving kind n-player normal form games). Throughout paper, shown mathematical programming
(in particular, 0-1 integer programming) applied solving finite horizon DECPOMDPs (it easy see approach presented yields linear program
solving finite horizon POMDP). Additionally, computational experience
approach indicates finite horizon DEC-POMDPs, mathematical programming may
better (faster) dynamic programming. also shown well-entrenched
dynamic programming heuristic pruning redundant extraneous objects (in
case, histories) integrated mathematical programming approach.
Hence, main contribution paper presents, first time, alternative approach solving finite horizon POMDPs/DEC-POMDPs based MILPs.
Q5: mathematical programming approach presented paper something dead end?
question bit controversial short answer question could
small yes. true every approach looks exact optimal solutions
DEC-POMDPs, whether grounded dynamic programming forward search
mathematical programming. complexity problem, exact solution
always untractable algorithms still improved.
longer answer mitigated, especially light recent advances made
dynamic programming forward search algorithms. One crucial point sequenceform DEC-POMDPs pruning extraneous histories. recent work Oliehoek,
Whiteson, Spaan (2009) shown clusters histories equivalent
way could also reduce nomber constraints MILPs. approach Amato,
Dibangoye, Zilberstein (2009) improves speed dynamic programming
operator could help finding extraneous histories. So, least, work
379

fiAras & Dutech

still required stating every aspect sequence-form DEC-POMDPs
studied.
turn even longer answer. Consider long horizon case. Given exact
algorithms (including ones presented paper) tackle horizons less 6,
long horizon, mean anything upwards 6 time periods. long horizon case,
required conceive possibly sub-optimal joint policy given horizon
determine upper bound loss value incurred using joint policy instead
using optimal joint policy.
current trend long horizon case memory-bounded approach. memory
bounded dynamic programming (MBDP) algorithm (Seuken & Zilberstein, 2007)
main exponent approach. algorithm based backward induction DP
algorithm (Hansen et al., 2004). algorithm attempts run limited amount
space. order so, unlike DP algorithm, prunes even non-extraneous (i.e., nondominated) policy trees iteration. Thus, iteration, algorithm retains
pre-determined number trees. algorithm variants used find
joint policy MABC, MA-tiger Box pushing problems long
horizons (of order thousands time periods).
MBDP provide upper bound loss value. bounded DP (BDP)
algorithm presented paper Amato, Carlin, Zilberstein (2007c) give
upper bound. However, interesting DEC-POMDP problems (such MA-tiger),
MBDP finds much better joint policy BDP.
meaningful way introduce notion memory boundedness approach
fix priori upper bound size concerned mathematical program.
presents sorts difficulties main difficulty seems need represent
policy long horizon limited space. MBDP algorithm solves problem
using may termed recursive representation. recursive representation
causes MBDP algorithm take long time evaluate joint policy, allow
algorithm represent long horizon joint policy limited space. context
mathematical programming approach, would change policy constraints
way long horizon policy represented system consisting limited
number linear equations linear inequalities. Besides policy constraints,
constraints presented programs would also accordingly transfigured.
evident (to us) transfiguration constraints possible.
hand, infinite horizon case seems promising candidate adapt
approach to. Mathematical programming already applied, success,
solving infinite horizon DEC-POMDPs (Amato et al., 2007a). computational experience mathematical programming approach shows better (finds higher
quality solutions lesser time) dynamic programming approach (Bernstein et al.,
2005; Szer & Charpillet, 2006).
Nevertheless, approach two inter-related shortcomings. First, approach
finds joint controller (i.e., infinite horizon joint policy) fixed size
optimal size. Second, much graver first, fixed size, finds locally optimal
joint controller. approach guarantee finding optimal joint controller.
program presented work Amato et al. (2007a) (non-convex)
380

fiMathematical Programming DEC-POMDPs

nonlinear program (NLP). NLP finds fixed size joint controller canonical form
(i.e., form finite state machine). believe shortcomings
removed conceiving mathematical program (specifically, 0-1 mixed integer linear
program) finds joint controller sequence-form. stated earlier, main
challenge regard therefore identification sequence-form infinite
horizon policy. fact, may sequence-form characterization infinite
horizon policy obtained, could used conceiving program long horizon
(undiscounted reward) case well.
Q6: help achieve designing artificial autonomous agents ?
first sight, work direct immediate applied benefits
purpose building artificial intelligent agents understanding intelligence works.
Even limited field multi-agent planning, contributions theoretical
level practical one.
Real artificial multi-agent systems indeed modeled DEC-POMDPs, even
make use communication, common knowledge, common social law. Then, real
systems would likely made large number states, actions observations require
solutions large horizon. mathematical programming approach practically
useless setting limited DEC-POMDPs small size. models
simpler far trivial solve explicitly take account
characteristics real systems exist. works take advantage communications
(Xuan, Lesser, & Zilberstein, 2000; Ghavamzadeh & Mahadevan, 2004), existing
independencies system (Wu & Durfee, 2006; Becker, Zilberstein, Lesser, & Goldman,
2004), focus interaction agents (Thomas, Bourjot, & Chevrier, 2004),
some, said answering previous questions, rely approximate solutions, etc...
intention facilitate re-use adaptation models
concepts used work knowledge structure optimal solution
DEC-POMDP. end, decided describe MILP programs also,
importantly, derived programs making use properties
optimal DEC-POMDP solutions.
Truly autonomous agents also require adapt new unforeseen situations.
work dedicated planning, seems easy argue contribute
much end either. hand, learning DEC-POMDPs never
really addressed except fringe work particular settings (Scherrer & Charpillet, 2002; Ghavamzadeh & Mahadevan, 2004; Buffet, Dutech, & Charpillet, 2007). fact,
even simple POMDPs, learning difficult task (Singh, Jaakkola, & Jordan,
1994). Currently, promising research deals learning Predictive State
Representation (PSR) POMDP (Singh, Littman, Jong, Pardoe, & Stone, 2003; James
& Singh, 2004; McCracken & Bowling, 2005). Making due allowance fundamental
differences functional role PSR histories, notice PSR histories quite similar structure. early say, might trying
learn useful histories DEC-POMDP could take inspiration way
right PSRs learned POMDPs.

381

fiAras & Dutech

9. Conclusion
designed investigated new exact algorithms solving Decentralized Partially Observable Markov Decision Processes finite horizon (DEC-POMDPs). main contribution paper use sequence-form policies, based sets histories,
order reformulate DEC-POMDP non-linear programming problem (NLP).
presented two different approaches linearize NLP order find global
optimal solutions DEC-POMDPs. first approach based combinatorial
properties optimal policies DEC-POMDPs second one relies concepts
borrowed field game theory. lead formulating DEC-POMDPs 0-1
Mixed Integer Linear Programming problems (MILPs). Several heuristics speeding
resolution MILPs make another important contribution work.
Experimental validation mathematical programming problems designed
work conducted classical DEC-POMDP problems found literature.
experiments show that, expected, MILP methods outperform classical Dynamic
Programming algorithms. But, general, less efficient costly
forward search methods like GMAA*, especially case DEC-POMDP admits
many optimal policies. Nevertheless, according nature problem, MILP methods
sometimes greatly outperform GMAA* (as MA-Tiger problem).
clear exact resolution DEC-POMDPs scale size
problems length horizon, designing exact methods useful order
develop improve approximate methods. see least three research directions
work contribute. One direction could take advantage large
literature algorithms finding approximate solutions MILPs adapt
MILPs formulated DEC-POMDPs. Another direction would use knowledge
gained work derive improved heuristics guiding existing approximate existing
methods DEC-POMDPs. example, work Seuken Zilberstein (2007),
order limit memory resources used resolution algorithm, prune space
policies consider them; work could help using better estimation
policies important kept search space. Then, one direction
currently investigating adapt approach DEC-POMDPs infinite length
looking yet another representation would allow problems seen MILPs.
importantly, work participates better understanding DEC-POMDPs.
analyzed understood key characteristics nature optimal policies order
design MILPs presented paper. knowledge useful work
dealing DEC-POMDPs even POMDPs. experimentations also given
interesting insights nature various problems tested, term existence
extraneous histories number optimal policies. insights might first
step toward taxonomy DEC-POMDPs.

Appendix A. Non-Convex Non-Linear Program
Using simplest example, section aims showing Non-Linear Program
(NLP) expressed Table 2 non-convex.
382

fiMathematical Programming DEC-POMDPs

Let us consider example two agents, one 2 possible actions (a b)
want solve horizon-1 decision problem. set possible joint-histories then:
ha, ai, ha, bi, hb, ai hb, bi. NLP solve is:
Variables: x1 (a), x1 (b), x2 (a), x2 (a)
Maximize

R(, ha, ai)x1 (a)x2 (a) + R(, ha, bi)x1 (a)x2 (b)

(158)

+R(, hb, ai)x1 (b)x2 (a) + R(, hb, bi)x1 (b)x2 (b)
subject
x1 (a) + x1 (b) = 1
x2 (a) + x2 (b) = 1
x1 (a) 0,

x1 (b) 0

x2 (a) 0,

x2 (b) 0

matrix formulation objective
x following kind:

0 0 c
0 0 e
C=
c e 0
f 0

function eq. (158) would xT .C.x C


f

0
0




x1 (a)
x1 (b)

x=
x2 (a) .
x2 (b)

(159)

eigen value vector v = [v1 v2 v3 v4 ]T straightforward show
also eigen value: [v1 v2 v3 v4 ]T = C.[v1 v2 v3 v4 ]T . result,
matrix C, hessian objective function, positive-definite thus objective
function convex.

Appendix B. Linear Program Duality
Every linear program (LP) converse linear program called dual. first LP
called primal distinguish dual. primal maximizes quantity,
dual minimizes quantity. n variables constraints primal,
variables n constraints dual. Consider following (primal) LP.
Variables: x(i), {1, 2, , n}
Maximize

n
X

c(i)x(i)

i=1

subject to:
n
X

a(i, j)x(i) = b(j),

j = 1, 2, ,

i=1

x(i) 0,

= 1, 2, , n

383

fiAras & Dutech

primal LP one variable x(i) = 1 n. data LP consists
numbers c(i) = 1 n, numbers b(j) j = 1 numbers
a(i, j) = 1 n j = 1 m. LP thus n variables
constraints. dual LP following LP.
Variables: y(j), j {1, 2, , }


Minimize


X

b(j)y(j)

j=1

subject To:



X

a(i, j)y(j) c(i),

= 1, 2, , n

j=1

y(j) (, +),

j = 1, 2, ,

dual LP one variable y(j) j = 1 m. y(j) variable free
variable. is, allowed take value R. dual LP variables n
constraints.
theorem linear programming duality follows.
Theorem B.1. (Luenberger, 1984) either primal LP dual LP finite optimal
solution, other, corresponding values objective functions
equal.
Applying theorem primal-dual pair given above, holds,
n
X

c(i)x (i) =


X

b(j)y (j)

j=1

i=1

x denotes optimal solution primal denotes optimal solution
dual.
theorem complementary slackness follows.
Theorem B.2. (Vanderbei, 2008) Suppose x feasible primal linear program
feasible dual. Let (w1 , ,wm ) denote corresponding primal slack variables,
let (z1 , ,zn ) denote corresponding dual slack variables. x optimal
respective problems
xj zj = 0

j = 1, , n,

wi yi = 0

= 1, , m.
384

fiMathematical Programming DEC-POMDPs

Appendix C. Regret DEC-POMDPs
value information set Ii agent i-reduced joint policy q,
denoted (, q), defined by:
X
(, q) = max
R(, hh, j i)q(j )
(160)
h

j Ei

terminal information set and, non-terminal, by:
X
(h.o, q)
(, q) = max
h

(161)

oOi

Then, regret history h agent i-reduced joint policy q,
denoted (h, q), defined by:
X
(h, q) = ((h), q)
R(, hh, j i)q(j )
(162)

j Hi

h terminal and, h non-terminal, by:
(h, q) = ((h), q)

X

(h.o, q)

(163)

oOi

concept regret agent i, independant policy agent i,
useful looking optimal policy optimal value known: 0.
thus easier manipulate optimal value policy.

Appendix D. Program Changes Due Optimizations
Pruning locally globally extraneous histories reduces size search space
mathematical programs. Now, constraints programs depend size
search space, must alter constraints.
Let denote superscript sets actually used program. example, Ei
actual set terminal histories agent i, pruned extraneous histories
not.
Programs MILP (Table 3) MILP-n agents (Table 5) rely fact
number histories given length support pure policy agent fixed
equal |Oi |t1 . may case pruned sets, following changes
made:
constraint (42) MILP (121) MILP-n agents,
X

z(j) =
|Oi |T 1
jE

iI

must replaced
X

z(j)


iI

jE

385

|Oi |T 1 .

(164)

fiAras & Dutech

set constraints (41) MILP (120) MILP-n agents,
X

z(hh, j i) =

j Ei



|Ok |T 1 xi (h),

I, h Ei



|Ok |T 1 xi (h),

I, h Ei .

kI\{i}

must replaced
X

z(hh, j i)

(165)

kI\{i}

j Ei

set constraints (119) MILP-n agents,
yi ((h))

X
1
R(, hh, ji i)z(j) = wi (h),
|Oi |T 1

h Ei

jE

must replaced
yi ((h))

X
1
R(, hh, ji i)z(j) = wi (h),
|Oi |T 1

h Ei .

(166)

jE

Appendix E. Example using MA-Tiger
example derived using Decentralized Tiger Problem (MA-Tiger) described
Section 2.2. two agents, 3 actions (al , ar , ao ) 2 observations (ol , ).
consider problem horizon 2.
18 (32 2) terminal histories agent: ao .ol .ao , ao .ol .al , ao .ol .ar , ao .or .ao ,
ao .or .al , ao .or .ar , al .ol .ao , al .ol .al , al .ol .ar , al .or .ao , al .or .al , al .or .ar , ar .ol .ao , ar .ol .al ,
ar .ol .ar , ar .or .ao , ar .or .al , ar .or .ar .
thus 324 (182 = 322 22 ) joint histories agents: hao .ol .ao ,ao .ol .ao i,hao .ol .ao ,ao .ol .al i,
hao .ol .ao ,ao .ol .ar i, , har .or .ar ,ar .or .ar i.
E.1 Policy Constraints
policy constraints horizon 2 one agent MA-Tiger problem would be:
Variables: x every history
x(ao ) + x(al ) + x(ar ) = 0
x(ao ) + x(ao .ol .ao ) + x(ao .ol .al ) + x(ao .ol .ar ) = 0
x(ao ) + x(ao .or .ao ) + x(ao .or .al ) + x(ao .or .ar ) = 0
x(al ) + x(al .ol .ao ) + x(al .ol .al ) + x(al .ol .ar ) = 0
x(al ) + x(al .or .ao ) + x(al .or .al ) + x(al .or .ar ) = 0
x(ar ) + x(ar .ol .ao ) + x(ar .ol .al ) + x(ar .ol .ar ) = 0
x(ar ) + x(ar .or .ao ) + x(ar .or .al ) + x(ar .or .ar ) = 0
386

fiMathematical Programming DEC-POMDPs

x(ao ) 0

x(al ) 0

x(ar ) 0

x(ao .ol .ao ) 0 x(ao .ol .al ) 0 x(ao .ol .ar ) 0
x(ao .or .ao ) 0 x(ao .or .al ) 0 x(ao .or .ar ) 0
x(al .ol .ao ) 0

x(al .ol .al ) 0

x(al .ol .ar ) 0

x(al .or .ao ) 0 x(al .or .al ) 0 x(al .or .ar ) 0
x(ar .ol .ao ) 0 x(ar .ol .al ) 0 x(ar .ol .ar ) 0
x(ar .or .ao ) 0 x(ar .or .al ) 0 x(ar .or .ar ) 0
E.2 Non-Linear Program MA-Tiger
Non-Linear Program finding optimal sequence-form policy MA-Tiger
horizon 2 would be:
Variables: xi every history agent

Maximize

R(, hao .ol .ao , ao .ol .ao i)x1 (ao .ol .ao )x2 (ao .ol .ao )
+ R(, hao .ol .ao , ao .ol .al i)x1 (ao .ol .ao )x2 (ao .ol .al )
+ R(, hao .ol .ao , ao .ol .ar i)x1 (ao .ol .ao )x2 (ao .ol .ar )
+

subject to:
x1 (ao ) + x1 (al ) + x1 (ar ) = 0
x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0
x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0
x1 (al ) + x1 (al .ol .ao ) + x1 (al .ol .al ) + x1 (al .ol .ar ) = 0
x1 (al ) + x1 (al .or .ao ) + x1 (al .or .al ) + x1 (al .or .ar ) = 0
x1 (ar ) + x1 (ar .ol .ao ) + x1 (ar .ol .al ) + x1 (ar .ol .ar ) = 0
x1 (ar ) + x1 (ar .or .ao ) + x1 (ar .or .al ) + x1 (ar .or .ar ) = 0

x2 (ao ) + x2 (al ) + x2 (ar ) = 0
x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0
x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0
x2 (al ) + x2 (al .ol .ao ) + x2 (al .ol .al ) + x2 (al .ol .ar ) = 0
x2 (al ) + x2 (al .or .ao ) + x2 (al .or .al ) + x2 (al .or .ar ) = 0
x2 (ar ) + x2 (ar .ol .ao ) + x2 (ar .ol .al ) + x2 (ar .ol .ar ) = 0
x2 (ar ) + x2 (ar .or .ao ) + x2 (ar .or .al ) + x2 (ar .or .ar ) = 0
387

fiAras & Dutech

x1 (ao ) 0

x1 (al ) 0

x1 (ar ) 0

x1 (ao .ol .ao ) 0 x1 (ao .ol .al ) 0 x1 (ao .ol .ar ) 0
x1 (ao .or .ao ) 0 x1 (ao .or .al ) 0 x1 (ao .or .ar ) 0
x1 (al .ol .ao ) 0

x1 (al .ol .al ) 0

x1 (al .ol .ar ) 0

x1 (al .or .ao ) 0 x1 (al .or .al ) 0 x1 (al .or .ar ) 0
x1 (ar .ol .ao ) 0 x1 (ar .ol .al ) 0 x1 (ar .ol .ar ) 0
x1 (ar .or .ao ) 0 x1 (ar .or .al ) 0 x1 (ar .or .ar ) 0

x2 (ao ) 0

x2 (al ) 0

x2 (ar ) 0

x2 (ao .ol .ao ) 0 x2 (ao .ol .al ) 0 x2 (ao .ol .ar ) 0
x2 (ao .or .ao ) 0 x2 (ao .or .al ) 0 x2 (ao .or .ar ) 0
x2 (al .ol .ao ) 0

x2 (al .ol .al ) 0

x2 (al .ol .ar ) 0

x2 (al .or .ao ) 0 x2 (al .or .al ) 0 x2 (al .or .ar ) 0
x2 (ar .ol .ao ) 0 x2 (ar .ol .al ) 0 x2 (ar .ol .ar ) 0
x2 (ar .or .ao ) 0 x2 (ar .or .al ) 0 x2 (ar .or .ar ) 0

E.3 MILP MA-Tiger
MILP horizon 2 agents MA-Tiger problem would be:
Variables:
xi (h) every history agent
z(j) every terminal joint history

Maximize

R(, hao .ol .ao , ao .ol .ao i)z(hao .ol .ao , ao .ol .ao i)
+ R(, hao .ol .ao , ao .ol .al i)z(hao .ol .ao , ao .ol .al i)
+ R(, hao .ol .ao , ao .ol .ar i)z(hao .ol .ao , ao .ol .ar i)
+
388

fiMathematical Programming DEC-POMDPs

subject to:
x1 (ao ) + x1 (al ) + x1 (ar ) = 0
x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0
x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0

x2 (ao ) + x2 (al ) + x2 (ar ) = 0
x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0
x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0

z(hao .ol .ao , ao .ol .ao i) + z(hao .ol .ao , ao .ol .al i) + z(hao .ol .ao , ao .ol .ar i) = 2 x1 (ao .ol .ao )
z(hao .ol .ao , ao .ol .ao i) + z(hao .ol .al , ao .ol .ao i) + z(hao .ol .ar , ao .ol .ao i) = 2 x2 (ao .ol .ao )
z(hao .ol .al , ao .ol .ao i) + z(hao .ol .al , ao .ol .al i) + z(hao .ol .al , ao .ol .ar i) = 2 x1 (ao .ol .al )
z(hao .ol .ao , ao .ol .al i) + z(hao .ol .al , ao .ol .al i) + z(hao .ol .ar , ao .ol .al i) = 2 x2 (ao .ol .al )


x1 (ao ) 0

x1 (al ) 0

x1 (ar ) 0

x1 (ao .ol .ao ) {0, 1}

x1 (ao .ol .al ) {0, 1}

x1 (ao .ol .ar ) {0, 1}

x1 (ao .or .ao ) {0, 1}

x1 (ao .or .al ) {0, 1}

x1 (ao .or .ar ) {0, 1}


x2 (ao ) 0

x2 (al ) 0

x2 (ar ) 0

x2 (ao .ol .ao ) {0, 1}

x2 (ao .ol .al ) {0, 1}

x2 (ao .ol .ar ) {0, 1}

x2 (ao .or .ao ) {0, 1}

x2 (ao .or .al ) {0, 1}

x2 (ao .or .ar ) {0, 1}


z(hao .ol .ao , ao .ol .ao i) {0, 1} z(hao .ol .ao , ao .ol .al i) {0, 1} z(hao .ol .ao , ao .ol .ar i) {0, 1}
z(hao .ol .al , ao .ol .ao i) {0, 1} z(hao .ol .al , ao .ol .al i) {0, 1} z(hao .ol .al , ao .ol .ar i) {0, 1}

E.4 MILP-2 Agents MA-Tiger
MILP-2 agents horizon 2 agents MA-Tiger problem would be:
Variables:
xi (h), wi (h) bi (h) every history agent
yi ()) agent every information set
Maximize

y1 ()

389

fiAras & Dutech

subject to:

x1 (ao ) + x1 (al ) + x1 (ar ) = 0
x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0
x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0

x2 (ao ) + x2 (al ) + x2 (ar ) = 0
x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0
x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0

y1 () y1 (ao .ol ) y1 (ao .or ) = w1 (ao )
y1 () y1 (al .ol ) y1 (al .or ) = w1 (al )
y1 () y1 (ar .ol ) y1 (ar .or ) = w1 (ar )
y2 () y2 (ao .ol ) y2 (ao .or ) = w2 (ao )
y2 () y2 (al .ol ) y2 (al .or ) = w2 (al )
y2 () y2 (ar .ol ) y2 (ar .or ) = w2 (ar )

y1 (ao .ol ) R(, hao .ol .ao , ao .ol .ao i)x2 (ao .ol .ao )
R(, hao .ol .ao , ao .ol .al i)x2 (ao .ol .al )
R(, hao .ol .ao , ao .ol .ar i)x2 (ao .ol .ar )
R(, hao .ol .ao , al .ol .ao i)x2 (al .ol .ao )
R(, hao .ol .ao , al .ol .al i)x2 (al .ol .al )
R(, hao .ol .ao , al .ol .ar i)x2 (al .ol .ar )
= w1 (ao .ol .ao )

y1 (ao .ol ) R(, hao .ol .al , ao .ol .ao i)x2 (ao .ol .ao )
R(, hao .ol .al , ao .ol .al i)x2 (ao .ol .al )
R(, hao .ol .al , ao .ol .ar i)x2 (ao .ol .ar )
R(, hao .ol .al , al .ol .ao i)x2 (al .ol .ao )
R(, hao .ol .al , al .ol .al i)x2 (al .ol .al )
R(, hao .ol .al , al .ol .ar i)x2 (al .ol .ar )
= w1 (ao .ol .al )
390

fiMathematical Programming DEC-POMDPs


y1 (ar .or ) R(, har .or .ar , ao .ol .ao i)x2 (ao .ol .ao )
R(, har .or .ar , ao .ol .al i)x2 (ao .ol .al )
R(, har .or .ar , ao .ol .ar i)x2 (ao .ol .ar )
R(, har .or .ar , al .ol .ao i)x2 (al .ol .ao )
R(, har .or .ar , al .ol .al i)x2 (al .ol .al )
R(, har .or .ar , al .ol .ar i)x2 (al .ol .ar )
= w1 (ar .or .ar )
y2 (ao .ol ) R(, hao .ol .ao , ao .ol .ao i)x1 (ao .ol .ao )
R(, hao .ol .al , ao .ol .ao i)x1 (ao .ol .al )
R(, hao .ol .ar , ao .ol .ao i)x1 (ao .ol .ar )
R(, hal .ol .ao , ao .ol .ao i)x1 (al .ol .ao )
R(, hal .ol .al , ao .ol .ao i)x1 (al .ol .al )
R(, hal .ol .ar , ao .ol .ao i)x1 (al .ol .ar )
= w2 (ao .ol .ao )
y2 (ao .ol ) R(, hao .ol .ao , ao .ol .al i)x1 (ao .ol .ao )
R(, hao .ol .al , ao .ol .al i)x1 (ao .ol .al )
R(, hao .ol .ar , ao .ol .al i)x1 (ao .ol .ar )
R(, hal .ol .ao , ao .ol .al i)x1 (al .ol .ao )
R(, hal .ol .al , ao .ol .al i)x1 (al .ol .al )
R(, hal .ol .ar , ao .ol .al i)x1 (al .ol .ar )
= w2 (ao .ol .al )


x1 (ao ) 1 b1 (ao )

x1 (al ) 1 b1 (al )

x1 (ar ) 1 b1 (ar )

x1 (ao .ol .ao ) 1 b1 (ao .ol .ao )

x1 (ao .ol .al ) 1 b1 (ao .ol .al )

x1 (ao .ol .ar ) 1 b1 (ao .ol .ar )



w1 (ao ) U1 (ao )b1 (ao )

w1 (al ) U1 (al )b1 (al )

w1 (ar ) U1 (ar )b1 (ar )

w1 (ao .ol .ao ) U1 (ao .ol .ao )b1 (ao .ol .ao )

w1 (ao .ol .al ) U1 (ao .ol .al )b1 (ao .ol .al )

w1 (ao .ol .ar ) U1 (ao .ol .ar )b1 (ao .ol .ar )


391

fiAras & Dutech

x1 (ao ) 0
x1 (ao .ol .ao ) 0

x1 (al ) 0
x1 (ao .ol .al ) 0

x1 (ar ) 0
x1 (ao .ol .ar ) 0


w1 (ao ) 0
w1 (ao .ol .ao ) 0

w1 (al ) 0
w1 (ao .ol .al ) 0

w1 (ar ) 0
w1 (ao .ol .ar ) 0


b1 (ao ) {0, 1}
b1 (ao .ol .ao ) {0, 1}

b1 (al ) {0, 1}
b1 (ao .ol .al ) {0, 1}


y1 () (, +)
y1 (ao .ol ) (, +) y1 (ao .or ) (, +)

... agent 2

392

b1 (ar ) {0, 1}
b1 (ao .ol .ar ) {0, 1}

fiMathematical Programming DEC-POMDPs

References
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007a). Optimizing memory-bounded controllers decentralized POMDPs. Proc. Twenty-Third Conf. Uncertainty
Artificial Intelligence (UAI-07).
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007b). Solving POMDPs using quadratically
constrained linear programs. Proc. Twentieth Int. Joint Conf. Artificial
Intelligence (IJCAI07).
Amato, C., Carlin, A., & Zilberstein, S. (2007c). Bounded dynamic programming
decentralized POMDPs. Proc. Workshop Multi-Agent Sequential Decision
Making Uncertain Domains (MSDM) AAMAS07.
Amato, C., Dibangoye, J., & Zilberstein, S. (2009). Incremental policy generation finitehorizon DEC-POMDPs. Proc. Nineteenth Int. Conf. Automated Planning
Scheduling (ICAPS-09).
Anderson, B., & Moore, J. (1980). Time-varying feedback laws decentralized control.
Nineteenth IEEE Conference Decision Control including Symposium
Adaptive Processes, 19(1), 519524.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. (2004). Solving transition independent
decentralized Markov decision processes. Journal Artificial Intelligence Research,
22, 423455.
Bellman, R. (1957). Dynamic programming. Princeton University Press, Princeton, NewJersey.
Bernstein, D., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control Markov decision processes. Mathematics Operations Research,
27 (4), 819840.
Bernstein, D. S., Hansen, E. A., & Zilberstein, S. (2005). Bounded policy iteration
decentralized POMDPs. Proc. Nineteenth Int. Joint Conf. Artificial
Intelligence (IJCAI), pp. 12871292.
Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming decentralized
pomdps lossless policy compression. Proc. Int. Conf. Automated
Planning Scheduling (ICAPS08).
Boutilier, C. (1996). Planning, learning coordination multiagent decision processes.
Proceedings 6th Conference Theoretical Aspects Rationality Knowledge (TARK 96), De Zeeuwse Stromen, Nederlands.
Buffet, O., Dutech, A., & Charpillet, F. (2007). Shaping multi-agent systems gradient reinforcement learning. Autonomous Agent Multi-Agent System Journal
(AAMASJ), 15 (2), 197220.
393

fiAras & Dutech

Cassandra, A., Kaelbling, L., & Littman, M. (1994). Acting optimally partially observable
stochastic domains. Proc. 12th Nat. Conf. Artificial Intelligence (AAAI).
Chades, I., Scherrer, B., & Charpillet, F. (2002). heuristic approach solving
decentralized-POMDP: assessment pursuit problem. Proc. 2002 ACM
Symposium Applied Computing, pp. 5762.
Cornuejols, G. (2008). Valid inequalities mixed integer linear programs. Mathematical
Programming B, 112, 344.
Dantzig, G. B. (1960). significance solving linear programming problems
integer variables. Econometrica, 28(1), 3044.
dEpenoux, F. (1963). probabilistic production inventory problem. Management
Science, 10(1), 98108.
Diwekar, U. (2008). Introduction Applied Optimization (2 edition). Springer.
Drenick, R. (1992). Multilinear programming: Duality theories. Journal Optimization
Theory Applications, 72(3), 459486.
Fletcher, R. (1987). Practical Methods Optimization. John Wiley & Sons, New York.
Ghavamzadeh, M., & Mahadevan, S. (2004). Learning communicate act cooperative multiagent systems using hierarchical reinforcement learning. Proc. 3rd
Int. Joint Conf. Autonomous Agents Multi-Agent Systems (AAMAS04).
Govindan, S., & Wilson, R. (2001). global newton method compute Nash equilibria.
Journal Economic Theory, 110, 6586.
Hansen, E., Bernstein, D., & Zilberstein, S. (2004). Dynamic programming partially
observable stochastic games. Proc. Nineteenth National Conference Artificial Intelligence (AAAI-04).
Horst, R., & Tuy, H. (2003). Global Optimization: Deterministic Approaches (3rd edition).
Springer.
James, M., & Singh, S. (2004). Learning discovery predictive state representations
dynamical systems reset. Proc. Twenty-first Int. Conf. Machine
Learning (ICML04).
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101, 99134.
Koller, D., Megiddo, N., & von Stengel, B. (1994). Fast algorithms finding randomized
strategies game trees. Proceedings 26th ACM Symposium Theory
Computing (STOC 94), pp. 750759.
Koller, D., & Megiddo, N. (1996). Finding mixed strategies small supports extensive
form games. International Journal Game Theory, 25(1), 7392.
394

fiMathematical Programming DEC-POMDPs

Lemke, C. (1965). Bimatrix Equilibrium Points Mathematical Programming. Management Science, 11(7), 681689.
Luenberger, D. (1984). Linear Nonlinear Programming. Addison-Wesley Publishing
Company, Reading, Massachussetts.
McCracken, P., & Bowling, M. H. (2005). Online discovery learning predictive state
representations. Advances Neural Information Processing Systems 18 (NIPS05).
Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralized
POMDPs: towards efficient policy computation multiagent setting. Proc.
Int. Joint Conference Artificial Intelligence, IJCAI03.
Oliehoek, F., Spaan, M., & Vlassis, N. (2008). Optimal approximate Q-value functions
decentralized POMDPs. Journal Artificial Intelligence Research (JAIR), 32,
289353.
Oliehoek, F., Whiteson, S., & Spaan, M. (2009). Lossless clustering histories decentralized POMDPs. Proc. International Joint Conference Autonomous
Agents Multi Agent Systems, pp. 577584.
Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory. MIT Press,
Cambridge, Mass.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial Optimization: Algorithms
Complexity. Dover Publications.
Papadimitriou, C. H., & Tsitsiklis, J. (1987). Complexity Markov Decision Processes. Mathematics Operations Research, 12 (3), 441 450.
Parsons, S., & Wooldridge, M. (2002). Game theory decision theory multi-agent
systems. Autonomous Agents Multi-Agent Systems (JAAMAS), 5(3), 243254.
Petrik, M., & Zilberstein, S. (2007). Average-reward decentralized Markov decision processes. Proc. Twentieth Int. Joint Conf. Artificial Intelligence (IJCAI
2007).
Petrik, M., & Zilberstein, S. (2009). bilinear programming approach multiagent
planning. Journal Artificial Intelligence Research (JAIR), 35, 235274.
Puterman, M. (1994). Markov Decision Processes: discrete stochastic dynamic programming. John Wiley & Sons, Inc. New York, NY.
Pynadath, D., & Tambe, M. (2002). Communicative Multiagent Team Decision Problem: Analyzing Teamwork Theories Models. Journal Artificial Intelligence
Research, 16, 389423.
Radner, R. (1959). application linear programming team decision problems.
Management Science, 5, 143150.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: modern approach. Prentice Hall.
395

fiAras & Dutech

Sandholm, T. (1999). Multiagent systems, chap. Distributed rational decision making, pp.
201258. MIT Press. Ed. G. Weiss.
Sandholm, T., Gilpin, A., & Conitzer, V. (2005). Mixed-integer programming methods
finding nash equilibria. Proc. National Conference Artificial Intelligence
(AAAI).
Scherrer, B., & Charpillet, F. (2002). Cooperative co-learning: model based approach
solving multi agent reinforcement problems. Proc. IEEE Int. Conf.
Tools Artificial Intelligence (ICTAI02).
Seuken, S., & Zilberstein, S. (2007). Memory-bounded dynamic programming DECPOMDPs. Proc. Twentieth Int. Joint Conf. Artificial Intelligence (IJCAI07).
Singh, S., Jaakkola, T., & Jordan, M. (1994). Learning without state estimation partially
observable markovian decision processes.. Proceedings Eleventh International
Conference Machine Learning.
Singh, S., Littman, M., Jong, N., Pardoe, D., & Stone, P. (2003). Learning predictive state
representations. Proc. Twentieth Int. Conf. Machine Learning (ICML03).
Szer, D., & Charpillet, F. (2006). Point-based Dynamic Programming DEC-POMDPs.
Proc. Twenty-First National Conf. Artificial Intelligence (AAAI 2006).
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithm
solving decentralized POMDPs. Proc. Twenty-First Conf. Uncertainty
Artificial Intelligence (UAI05), pp. 576 583.
Thomas, V., Bourjot, C., & Chevrier, V. (2004). Interac-DEC-MDP : Towards use
interactions DEC-MDP. Proc. Third Int. Joint Conf. Autonomous
Agents Multi-Agent Systems (AAMAS04), New York, USA, pp. 14501451.
Vanderbei, R. J. (2008). Linear Programming: Foundations Extensions (3rd edition).
Springer.
von Stengel, B. (2002). Handbook Game Theory, Vol. 3, chap. 45-Computing equilibria
two-person games, pp. 17231759. North-Holland, Amsterdam.
Wu, J., & Durfee, E. H. (2006). Mixed-integer linear programming transitionindependent decentralized MDPs. Proc. fifth Int. Joint Conf. Autonomous
Agents Multiagent Systems (AAMAS06), pp. 10581060 New York, NY, USA.
ACM.
Xuan, P., Lesser, V., & Zilberstein, S. (2000). Communication multi-agent Markov
decision processes. Proc. ICMAS Workshop Game Theoretic Decision
Theoretics Agents Boston, MA.

396

fiJournal Artificial Intelligence Research 37 (2010) 85-98

Submitted 10/09; published 2/10

Mechanisms Multi-Unit Auctions
Shahar Dobzinski

shahar@cs.cornell.edu

Computer Science Department, Cornell University
Ithaca, NY 14853

Noam Nisan

noam@cs.huji.ac.il

School Computer Science Engineering
Hebrew University Jerusalem
Jerusalem, Israel

Abstract
present incentive-compatible polynomial-time approximation scheme multiunit auctions general k-minded player valuations. mechanism fully optimizes
appropriately chosen sub-range possible allocations uses VCG payments
sub-range. show obtaining fully polynomial-time incentive-compatible
approximation scheme, least using VCG payments, NP-hard. case valuations
given black boxes, give polynomial-time incentive-compatible 2-approximation
mechanism show better possible, least using VCG payments.

1. Introduction
Algorithmic Mechanism Design goal construct efficient mechanisms
handle selfish behavior players. particular, interested designing
truthful mechanisms, is, mechanisms dominant strategy player
simply reveal true valuation.
Typical problems field involve allocating goods players. One key problem
problem multi-unit auctions. given identical items n bidders.
setting view number items large desire mechanisms whose
computational complexity polynomial number bits needed represent m. Every
bidder valuation function vi : {1, ..., m} <, vi (q) denotes value
obtaining q items. assume vi weakly monotone increasing (free disposal),
normalized (vi (0) = 0). goal usual one maximizing social welfare vi (qi )
qi m.
general case, vi represented real numbers, abstract settings
may accessed black box. concrete setting, assume vi represented
k-minded bid, i.e. given list: (q1 , p1 ), ..., (qk , pk ), vi (q) = max{j|qj q} pj .
corresponds XOR bidding language (Nisan, 2006). Observe k-minded
valuation corresponds step function k steps (step located qi
height pi ). general, k large m, case k = 1 single-minded
case.
problem received much attention, starting Vickreys seminal paper (1961)
described truthful mechanism case downward sloping valuations
items optimally allocated greedily. general case, however, NP-hard,
single-minded case re-formulation knapsack problem. Luckily, knapsack
c
2010
AI Access Foundation. rights reserved.

fiDobzinski & Nisan

problem fully-polynomial time approximation scheme (i.e. approximated
within factor 1 + time polynomial n, log m, 1 ), hard see
algorithm directly extends general case multi-unit auctions.
1.1 VCG-Based Mechanisms
key positive technique mechanism design VCG payment scheme.
payment scheme bidder pays hi (vi ) j6=i vj (a), algorithmic output,
hi arbitrary function depend vi . Unfortunately, VCG
works perfectly well game-theoretic perspective, useful computational
settings, since multi-unit auctions, interesting combinatorial optimization
problems, calculating exact optimum intractable.
One naive idea use approximation algorithm find approximate solution
a, let bidder pay hi (vi ) j6=i vj (a). Applying idea case
multi-unit auction tempting particular light known (1 + )-approximation
algorithm problem. Unfortunately, turns general using approximation algorithm together VCG payments result truthful mechanism.
phenomenon studied Nisan Ronen (2007). observed
following family allocation algorithms yield truthful VCG-based mechanisms:
Definition: allocation algorithm (that produces output input
v1 , ..., vn , set possible alternatives) called maximal-in-range (henceforth MIR) completely optimizes social welfare subrange R A. I.e.,
R A, v1 , ..., vn , arg maxaR vi (a).
I.e., MIR algorithms use following natural simple strategy find approximately
optimal solution: optimally search within pre-specified sub-range feasible
solutions subrange optimal search algorithmically feasible.
main result Nisan Ronen (2007) states essentially
VCG-based mechanisms incentive compatible.
Theorem (Nisan & Ronen, 2007): allocation algorithm incentive-compatible
VCG-based mechanism combinatorial auctions equivalent maximal-in-range algorithm.
Equivalent means social utilities identical inputs, i.e. b
outputs two allocation algorithms input v1 , . . . , vn vi (a) = vi (b).
particular, outputs must coincide generically except perhaps case ties. Nisan
Ronen (2007) prove two specific types mechanism design problems,
result general (Dobzinski & Nisan, 2007).
Following Nisan Ronen (2007), Dobzinski Nisan (2007) view result
negative one. Namely, show setting combinatorial auctions
submodular bidders, MIR algorithms much power. might imply
setting truthful deterministic mechanisms much power, since Lavi
et al. (2003) give partial evidence truthful mechanisms provide good
approximation ratio must MIR.
86

fiMechanisms Multi-Unit Auctions

contrast, paper views result positive result. observe MIR
algorithms provide us constructive way obtaining truthful mechanisms1 . Indeed,
paper suggests settings power MIR algorithms
trivial all. note several previous papers already obtained upper bounds using
MIR techniques (Holzman, Kfir-Dahav, Monderer, & Tennenholtz, 2004; Dobzinski, Nisan,
& Schapira, 2005; Blumrosen & Dobzinski, 2007). Yet, paper initiates systematic
study MIR algorithms. particular, techniques used sophisticated
obtained previous work.
1.2 Previous Work Results
case multi-unit auctions single-minded bidders, paper Briest, Krysta,
Vocking (2005) presents truthful fully polynomial time approximation scheme (FPTAS), improving upon previous result Mualem Nisan (2002). result
known k-minded bidders randomized 12 -approximation mechanism truthful expectation (Lavi & Swamy, 2005). current paper presents polynomial time
approximation scheme (PTAS) general case.
Theorem: every fixed > 0, exists truthful (1 )-approximation mechanism
multi-unit auctions k-minded bidders whose running time polynomial n, log m,
k. dependence running time exponential.
However, prove two ways mechanism improved upon. First,
show dependence cannot made polynomial without destroying
truthfulness, long MIR techniques used. contrast, pure algorithmic point
view possible obtain fully polynomial time approximation scheme2 . Furthermore,
exists truthful FPTAS bidders known single minded (Briest et al.,
2005).
Theorem: fully polynomial time truthful approximation mechanism uses VCG
payments exists, unless P=NP.
show dependence k necessary, approximation scheme
possible general black-box model. shown general communication model,
even two bidders.
Theorem: Every approximation mechanism among two bidders general valuations
uses VCG payments requires exponentially many queries obtain approximation
factor better 12 .
present truthful approximation mechanism general black box model
obtain factor 12 . improves upon randomized one Lavi Swamy
(2005) truthful expectation.
1. Note payments efficient MIR mechanism computed efficiently: take output
allocation pay bidder sum values bidders output allocation.
2. course, ignoring computational issues, standard VCG mechanism MIR provides approximation ratio 1.

87

fiDobzinski & Nisan

Theorem: exists truthful 12 -approximation mechanism multi-unit auctions
among general valuations whose running time polynomial n log m. access
bidders valuations value queries: vi (q)?3 . mechanism uses VCG
payments.
Section 5 present fairly general construction takes -approximation MIR
1
algorithm bidders, 1, converts ( t+1
)-approximation
algorithm n bidders, time polynomial n, log m, running time A.
construction works long model allows us answer value queries. present
four applications construction: first two applications provide another proof
upper bounds discussed (the PTAS k-minded bidders, 12 -approximation
black-box model). present two new applications: PTAS stronger bidding
languages, one used Kothari et al. (2005), ( 43 + )-approximation
mechanism multi-unit auctions subadditive valuations. Prior paper, nothing
known latter setting.
construction provides us interesting example truthful reduction among
problems: MIR approximation algorithm fixed number bidders automatically translated truthful approximation algorithm number bidders,
losing small factor approximation ratio.
1.3 Discussion Open Questions
main open problem remains determine best approximation ratio obtained truthful way. general method known constructing mechanisms
VCG, lower bounds state VCG cannot take us further. Furthermore,
Lavi et al. (2003) show 2 bidders black-box model, items allocated,
MIR algorithms truthful way obtain reasonable approximation ratio.
Combined lower bounds, get better-than- 12 truthful approximation
possible polynomial time, 2 bidders items allocated. intriguing
open question understand whether condition items allocated indeed
necessary.
Another issue paper highlights inherent difference obtaining
approximation algorithms single-parameter environments multi-parameter environments. single parameter environment private information bidder consists
one number, multi-parameter environments private information consists
one number. Recall single parameter variant multi-unit auctions, assume bidders single-minded, exists truthful FPTAS
(Briest et al., 2005). However, variants discuss paper multi-parameter,
indeed lower bounds suggest obtaining FPTAS impossible (if one prove
mechanisms give good approximation ratio must MIR).
Paper Organization
Section 3 present PTAS k-minded bidders, 21 -approximation
black-box model. Section 4 considers lower bounds MIR algorithms models.
3. yet another improvement upon mechanism Lavi swamy (2005) requires
stronger demand queries.

88

fiMechanisms Multi-Unit Auctions

Section 5 describe general construction, algorithmic applications: truthful
mechanisms models powerful bidding languages.

2. Preliminaries
section provide basic definitions notations used paper.
2.1 Setting
multi-unit auction set identical items, set N = {1, 2, . . . , n}
bidders. bidder valuation function vi : [m] R+ , normalized
(vi (0) = 0) non-decreasing. Denote V set possible valuations. allocation
items ~s = (s1 , . . . , sn ) N vector non-negative integers si m.
Denote set allocations S. goal find allocation maximizes
welfare: vi (si ).
consider two settings differ valuations given us. concrete
setting (a bidding language model) assume vi represented k-minded
bid, i.e. given list: (q1 , p1 ), ..., (qk , pk ), vi (q) = max{j|qj q} pj .
Otherwise, valuations given us black boxes. algorithms, black
box v answer weak value queries: given s, value v(s).
impossibility result, assume black box v answer query based
v (the communication model). algorithms run time poly(n, log m),
impossibility result gives lower bound number bits transferred, holds even
mechanism computationally unbounded.
2.2 Truthfulness
deterministic n-bidder mechanism multi-unit auctions pair (f, p) f : V n
p = (p1 , , pn ), pi : V n R.
Definition 2.1 Let (f, p) deterministic mechanism. (f, p) truthful i,
vi , vi0 vi vi (f (vi , vi )i ) pi (vi , vi ) vi (f (vi0 , vi )i ) p(vi0 , vi ).
Definition 2.2 f affine maximizer exist set allocations R, constant
0 N , constant ~s < ~s S, f (v1 , ..., vn )
arg max~s=(s1 ,...,sn )R (i (i vi (si )) + ). f called maximal-in-range (MIR) = 1
N , = 0 ~s R.
following proposition standard:
Proposition 2.3 Let f affine maximizer (in particular, maximal range).
payments p (f, p) truthful mechanism.

3. Basic Mechanisms
section provides basic mechanisms multi-unit auctions: PTAS k-minded
bidders, 12 approximation black-box model.
89

fiDobzinski & Nisan

3.1 Truthful PTAS k-Minded Bidders
design MIR algorithm problem, directly yields incentive-compatible
VCG-based mechanism. define range R allocations, prove bidders
k-minded algorithm outputs polynomial time best allocation R.
start defining subrange R.
Definition 3.1 say allocation (s1 , ..., sn ) t-round exists set
bidders, |T | t, following two conditions hold:
Let l = jT sj .
j
k
ml
bidder
/ , si multiple max( (nt)
, 1).
2
k
j
ml
, 1) (n t)2 items assigned bidders outside :
max( (nt)
2
/ si
j
k
ml
max( (nt)
, 1) (n t)2
2
let R set t-round allocations fixed (that depend
approximation guarantee). Next prove value best allocation R
close optimum:
Lemma 3.2 Let (a1 , ..., ) optimal t-round allocation, (o1 , ..., ) optimal
1
unrestricted allocation, vi (ai ) (1 t+1
)i vi (oi ).
Proof:
Let us start optimal unrestricted allocation (o1 , ..., ), use
construct t-round allocation high value. Assume items allocated
optimal allocation (without loss generality due monotonicity valuations),
v1 (o1 ) ... vn (on ). Let = {1, ..., t} set bidders required Definition
3.1, assign bidder bundle size oi . Definition 3.1, let l = jT oj .
Let j
/ bidder got largest number items oj ml
.
/ T,
j
knt
ml
6= j, round oi nearest multiple b = max( (nt)2 , 1) assign
many items bidder i. Assign bidder
jkno items. valid t-round allocation since
j
ml
b 6= 1 added (n t) (nt)2 ml
nt items rounding up, deleted least
ml
nt

items removing oj . Notice second condition also holds. b = 1, observe
even optimal allocation t-round. value solution, observe
vi (oi )
bidder 6= j gets bundle smaller oi . addition, vj (oj ) it+1
,
gives required approximation.
MIR approximation algorithm try subset bidders
set bidders. possible selection , algorithm consider possible
allocations bidders according k bids bidder submitted. is,
consider allocation assigns bidder exactly si items,
si m, si bid (si , pi ) k bids bidder (for pi > 0).
selection allocation bidders according bids,
algorithm splits remaining l items (n t)2 equi-sized bundles size
90

fiMechanisms Multi-Unit Auctions

k
j
ml
, 1), l total number items bidders get. maximalmax( (nt)
2
in-range algorithm optimally allocate equi-size bundles among bidders
. Finally, algorithm outputs best allocation among allocations
considered. left show following two lemmas:
Lemma 3.3 every fixed algorithm runs time polynomial n log m.
Proof:
nt possible selections sets . selection
k allocations bidders considered. Finding optimal
allocation bidders dynamic programming. Let b size equi-size
bundles. Without loss generality, assume = {n + 1, ..., n}. calculate
following information every 1 n 1 q (n t)2 : (i, q)
maximum value obtained allocating q equi-size bundles among
bidders 1...i. entry filled polynomial time using relations: (i, q) =
maxq0 q (vi (q 0 b) + (i 1, q q 0 )). particular notice b = 1 number
equi-size bundles polynomial number bidders, thus number entries
table polynomial also case. Overall get algorithm runs time
polynomial n log m, every fixed t.
Lemma 3.4 algorithm finds optimal t-round allocation.
Proof: First, notice algorithm outputs t-round allocation. Let us prove
outputs optimal one. Let = (o1 , ..., ) optimal t-round allocation, let
set bidders Definition 3.1, let l = oi . bidder remove
maximal number qi (possibly zero) items oi vi (oi ) = vi (oi qi ).
Observe exists pair (qj0 , p0j ) XOR bids qj0 = oi qi .
handle bidders

/ holds bundle multiple
k . bidder
j
ml
,
1)

O,


order


allocation construct
b = max( (nt)
2
j
k
ml0
t-round need bidders receive multiples b0 = max( (nt)
, 1),
2
l0 = (oi qi ). However, notice b0 b, number equi-size bundles
least same. Hence, assigning bidder
/ number equi-size
bundles O, bidder holds least value O. lemma follows since
algorithm considers newly constructed allocation.
therefore following theorem:
1
Theorem 3.5 exists truthful VCG-based mechanism provides (1 t+1
)approximation multi-unit auctions k-minded bidders time polynomial n, log m,
k, every constant t.

3.2 21 -Approximation Multi-Unit Auctions Black-Box Access
Let us consider multi-unit auction problem general valuations given black boxes.
assume algorithm oracle access may queried vi (q),
q given bundle size4 .
4. analogous weakest value query combinatorial auction setting. lower bounds
presented later apply query types well.

91

fiDobzinski & Nisan

design 12 -approximation MIR algorithm problem, yields
incentive-compatible VCG-based mechanism. MIR approximation
algorithm


2
first split items n equi-sized bundles size b = n2 well single extra
bundle size r holds remaining elements (thus n2 b + r = m). maximum
range algorithm optimally allocate whole bundles among n bidders.
need show following two simple facts:
Lemma 3.6 optimal allocation bundles found time polynomial n
log m.
Lemma 3.7 Let (a1 , ..., ) optimal allocation bundles found
algorithm, (o1 , ..., ) optimal unrestricted allocation, vi (oi ) 2i vi (ai ).
proofs simple:
Proof: (of Lemma 3.6): algorithm dynamic programming. calculate
following information every 1 n 1 q n2 : (i, q) maximum
value obtained allocating q regular bundles among bidders 1...i,
+ (i, q) maximum value obtained allocating q regular bundles
remainder bundle among bidders 1...i. entry filled polynomial
time using relations: (i, q) = maxq0 q vi (q 0 b) + (i 1, q q 0 ) + (i, q) =
max(maxq0 q (vi (q 0 b) + + (i 1, q q 0 )), maxq0 q (vi (q 0 b + r) + (i 1, q q 0 ))).
Proof: (of Lemma 3.7): Let us start optimal unrestricted allocation o1 ...on
items allocated (without loss generality since valuations monotone),
look bidder j got largest number items oj m/n. two
possibilities: vj (oj ) i6=j vi (oi ) allocating items j (i.e. regular-sized
bundles well remainder bundle) get required 2-approximation. Otherwise,
round oi nearest multiple b (i.e. full bundles), except bidder j
gets nothing. valid allocation since added nb m/n items
rounding up, deleted least m/n items removing oj , value certainly
least i6=j vi (oi ) gives required approximation.
thus proved:
Theorem 3.8 exists truthful polynomial time VCG-based mechanism gives
1
2 -approximation multi-unit auctions general valuations.

4. Lower Bounds VCG-Based Mechanisms
move show mechanisms essentially achieve best approximation
ratios possible. say allocation (s1 , ..., sn ) complete items allocated:
si = m. Consider MIR algorithm n bidders full range
complete allocations. I.e., 0 s1 , ..., sn1 m, i<n si never outputs
allocation (s1 , ..., sn1 , i<n si ). consider set valuations every
bidder vi (q) = 1 q si (and 0 otherwise). allocation value
n (s1 , ..., sn1 , i<n si ) range, allocations
value n 1.
92

fiMechanisms Multi-Unit Auctions

easily get lower bound computationally efficient MIR algorithm models considered paper. start lower bound black-box
model. lower bound number queries bidders must queried,
holds type query i.e., general communication setting.
Proposition 4.1 Let MIR algorithm multi-unit auctions achieves approximation ratio better 12 . Then, communication complexity (m).
Proof: case two bidders, optimal algorithm known communication
complexity (m):
Lemma 4.2 (Nisan & Segal, 2006) Finding optimal allocation multi-unit auctions requires (m) bits communication, even two bidders even
finding value allocation. lower bound also applies nondeterministic
settings.
Thus, MIR algorithm 2 bidders uses o(m) bits communication
non-optimal thus, argued above, gives better 12 -approximation. case
2 bidders follows setting valuations first two 0,
considering allocations items allocated first two bidders.
second result rules existence FPTAS k-minded bidders.
words, dependence running time 1 cannot made polynomial. result
essentially applies models allow single-minded bidders, e.g., XOR bids,
bidding language used Kothari et al. (2005) (see next section description).
Proposition 4.3 Let MIR algorithm achieves approximation ratio better
(1 n1 ). Then, run polynomial time, unless P = N P .
Proof: Similarly previous proposition, standard reduction knapsack
multi-unit auctions, follows every polynomial-time MIR algorithm exist
large enough n range complete allocations full, unless P = N P .
lemma follows discussion above.
concludes proof lower bounds MIR mechanisms. draw
conclusion VCG-based mechanisms, one technical detail explicitly mentioned. lower bounds MIR algorithms, VCG-based mechanisms
proved give algorithms equivalent MIR algorithms. See Dobzinski Nisan
(2007). However, proofs hold even finding value optimal allocation
thus directly apply also algorithms equivalent MIR algorithms.

5. General Construction Applications
present general construction takes maximal-in-range algorithm constant
number bidders bidding language model5 , extends truthful mechanism unbounded number bidders. Yet, extension loses arbitrarily
small constant approximation ratio.
5. model mean, e.g., restriction valuations accessed.

93

fiDobzinski & Nisan

describe four applications construction. First, reprove PTAS kminded bidders 12 -approximation algorithm general bidders Section 3. Then,
study bidding language considered Kothari et al. (2005). Kothari et al. describe
approximately truthful FPTAS bidding language, present truthful
VCG-based PTAS (this best possible since Section 4 essentially rules possibility VCG-based FPTAS). Finally, present truthful ( 34 +)-approximation mechanism
case valuations sub-additive (a.k.a. complement free) accessed via
black box.
5.1 Setting
Fix bidding language model multi-unit auction bidders
answer value queries. Let maximal-in-range algorithm bidders
items model. Denote complexity A(t, m), range RA,t,m ,
approximation guarantee 1.
Construction
1. Build set Q allocations follows:
(a) Let u = (1 +

1
2n ).

Let L = {0, 1, buc, bu2 c, . . . , ublogu mc , m}.
(b) every set bidders, |T | t, l L:
i. Run l items set bidders. Denote si number
items allocates bidder .
ii. Split remaining l items 2n2 bundles, consisting
max( 2nl 2 , 1) items.
iii. Find optimal allocation equal-size bundles among bidders
. Denote si allocation bidder
/ T.
iv. Add (s1 , . . . , sn ) Q.
2. Output allocation highest welfare Q.
Theorem 5.1 exists range allocations R construction maximal R. construction runs time poly(log m, n, A(t, m)) every constant t.
1
outputs allocation value ( t+1
) optimal allocation.
Proof:

make use following definition:

Definition 5.2 allocation (R, t, l)-round if:
R set allocations, R R bidders allocated non-empty
bundles. bidders allocated together l items.
exists set bidders, |T | t, bidders allocated
according allocation R.
94

fiMechanisms Multi-Unit Auctions


bidder
/ receives exact multiple max( 2nl 2 , 1) units,
/ si
l
2
max( 2n2 , 1) n .
hard verify construction always outputs allocations
following range:

R = {S|S (RA,k,ml , k, l)-round allocation l L k t}
Lemma 5.3 Let (o1 , . . . , ) optimal unrestricted allocation. exists alloca1
tion (s1 , . . . , sn ) R vi (si ) ( t+1
) vi (oi ).
Proof:
Denote value optimal solution OPT. Without loss generality,
assume items allocated v1 (o1 ) . . . vn (on ). Let l L largest
l ti=1 oi . Let (s1 , . . . , st ) allocation outputs run
bidders 1, . . . , l items, assign bidder 1 si items. Observe
ti=1 vi (si ) ti=1 vi (oi ). finish proof show allocation
range recovers value bidders + 1, . . . , n one. lemma

follow since bidder i, vi (oi ) OP
t+1 .
Claim 5.4 Step 1(b)iii returns allocation (st+1 , . . . , st ) l items bidders +
1, . . . , n bidder set, one, vi (si ) vi (oi ).
Proof: Let r = ni=t+1 oi . Let j + 1 bidder oj nr (observe
existence bidder guaranteed). Let l L largest l ti=1 oi
r
(observe l r). also l r 2n
: chose largest possible value
r
r
l, therefore l (1+ 1 ) r 2n .
2n

Now, 6= j round oi nearest multiple max( 2nl 2 , 1),
allocate items bidder j. Observe bidders bidder j bundle size
get increases. Also observe number additional items allocate bidders
l
r
l
l
{t + 1, . . . n} 2nl 2 n = 2n
. Thus, l r 2n
+ 2n
r oj + 2n
.
left show construction runs polynomial time:
Lemma 5.5 optimal allocation R found time poly(log m, n, A(t, m)),
every constant t.
Proof: Step 1(b)iii construction implemented using dynamic programming
similarly Lemma 3.3; optimality allocation R clear. running time
poly(log1+ 1 nk A(t, m)), polynomial relevant parameters every
2n+1

constant t.

5.2 Applications Construction
provide several applications construction.
95

fiDobzinski & Nisan

5.2.1 PTAS k-Minded Valuations
reprove PTAS k-minded bidders Section 3: multi-unit auction problem
items k-minded bidders optimally solved exhaustive search
time poly((tk)t , log m), polynomial log k every constant t.
construction (and since optimal algorithm particular maximal range), get
1
PTAS k-minded bidders: every constant t, get (1 t+1
)-approximation time
polynomial n log m.
5.2.2 12 -Approximation General Valuations
observe multi-unit auction one bidder optimally solved
allocating items single bidder. let = 1 statement Theorem 5.1,
get 12 -approximation algorithm general valuations.
5.2.3 PTAS Marginal Piecewise Bidding Language
following marginal piecewise bidding language used Kothari et al. (2005):
valuation v determined list k tuples denoted (u1 , m1 ),. . . ,(uk , mk ).
assume mi non-negative uk > . . . > u1 = 1. tuples determine
marginal utility jth item. words, determine value set
items, sum marginal utilities. I.e., item j, ul j < ul+1 , let
marginal utility rj = ml , every m, let v(s) = sj=1 rj . (In fact,
bidding language powerful one described Kothari et al. (2005),
allows marginal-decreasing piecewise valuations.)
show optimally solve multi-unit auction problem setting
constant number bidders. PTAS follows, k-minded case.
say bidder precisely assigned allocated si items, ui > 0
exists tuple (si , ui ) k bids. main observation
optimal solution (o1 , . . . , ) one bidder precisely assigned: suppose
two bidders i0 precisely assigned. Then, move items bidder
higher (or equal) marginal utility. value allocation cannot decrease.
Continue process bidders one precisely assigned.
optimally solving multi-unit auction problem constant number bidders
obvious: select n bidders turn bidder precisely
assigned. iteration, let bidder precisely assigned, go
allocations bidders precisely assigned. Then, assign bidder
remaining items. Since k possible sets make bidder precisely
assigned, algorithm runs time poly(n (n k)n1 , log m), polynomial log
k every constant n.
5.2.4 ( 43 + )-Approximation Subadditive Valuations
model assume valuations given black boxes (as Section 3.2),
valuation v bundles 0 s, v(s) + v(t) v(s + t).
valuations called subadditive valuations.
96

fiMechanisms Multi-Unit Auctions

Let us describe algorithm provides approximation ratio 34
setting number bidders constant. construction, get ( 34 + )approximation VCG-based mechanism unbounded number bidders, every constant
. algorithm quite simple: Fix small enough constant > 1 ( = 43 suffices).
bidders, one, receive bundle power (including
empty bundle). bidder get bundle size power receives
remaining items. use exhaustive search find optimal allocation range.
see algorithm indeed provides approximation ratio 34 , consider
optimal solution (o1 , . . . , ). Without loss generality, assume o1 . . . (notice
unlike bidders ordered bundle size). Let set
bidders odd indices, E set bidders even indices.
analysis divided two cases. First suppose iO vi (oi ) iE vi (oi ).
Consider following allocation: bundles bidders E rounded power
near o2i , bundles bidders \ {1} rounded nearest power
, bidder 1 gets remaining items. Notice allocation valid since
small enough choice assign bidders items removed
bidders E. Also notice allocation range. approximation
ratio, observe bidders hold least value optimal solution,
since bidder allocated least number items optimal
solution. addition, bidder E holds least half items allocated
optimal solution. Thus, subadditivity, bidders E hold together least half
value hold optimal solution. total, value allocation obtained
algorithm least 34 value optimal solution.
Let us handle case iO vi (oi ) < iE vi (oi ). Consider allocation
bidders rounded power near o2i , bidders
E rounded nearest power (except one arbitrary bidder E
gets remaining items). allocation range, analysis similar
previous case, leaving us approximation ratio 34 also current case.
running time algorithm poly(n (log m)n1 ), polynomial log
constant n .
Notice approximation ratio achieved almost best possible, every MIR
approximation algorithm guarantees factor better 43 requires (m) communication: Lemma 4.2 finding optimum solution multi-unit auction two bidders
requires (m) bits communication. make valuations sub-additive defining
v new valuation: v 0 (s) = v(s) + v(m), 6= 0. Thus, Section 4,
range every polynomial-time MIR mechanism two bidders subadditive valuations
cannot contain complete allocations. Fix MIR algorithm, let (s1 , s1 )
complete allocation range. Consider following instance: bidder 1
values least s1 items value 2, smaller bundles value 1, bidder
2 values least s2 items value 2, smaller bundles value 1 (and
0 value empty bundle). Notice valuations bidders indeed
subadditive. Also observe optimal welfare 4, mechanism achieve
welfare 3.
97

fiDobzinski & Nisan

Acknowledgments
preliminary version paper appeared EC07. grateful Liad Blumrosen
Ahuva Mualem helpful discussions. second author supported grant
Israeli Science Foundation.

References
Blumrosen, L., & Dobzinski, S. (2007). Welfare maximization congestion games. IEEE
Journal Selected Areas Communications, 25 (6), 12241236.
Briest, P., Krysta, P., & Vocking, B. (2005). Approximation techniques utilitarian
mechanism design. STOC, pp. 3948.
Dobzinski, S., & Nisan, N. (2007). Limitations vcg-based mechanisms. STOC, pp.
338344.
Dobzinski, S., Nisan, N., & Schapira, M. (2005). Approximation algorithms combinatorial auctions complement-free bidders. STOC, pp. 610618.
Holzman, R., Kfir-Dahav, N., Monderer, D., & Tennenholtz, M. (2004). Bundling equilibrium combinatrial auctions. Games Economic Behavior, 47, 104123.
Kothari, A., Parkes, D. C., & Suri, S. (2005). Approximately-strategyproof tractable
multi-unit auctions. Decision Support Systems, 39, 105121. Special issue dedicated
Fourth ACMConference Electronic Commerce.
Lavi, R., Mualem, A., & Nisan, N. (2003). Towards characterization truthful combinatorial auctions. 44th Annual IEEE Symposium Foundations Computer
Science (FOCS).
Lavi, R., & Swamy, C. (2005). Truthful near-optimal mechanism design via linear
programming. FOCS, pp. 595604.
Mualem, A., & Nisan, N. (2002). Truthful approximation mechanisms restricted combinatorial auctions. AAAI-02.
Nisan, N. (2006). P. Cramton Y. Shoham R. Steinberg (Editors), Combinatorial
Auctions. Chapter 1. Bidding Languages. MIT Press.
Nisan, N., & Ronen, A. (2007). Computationally feasible vcg mechanisms. J. Artif. Intell.
Res. (JAIR), 29, 1947.
Nisan, N., & Segal, I. (2006). communication requirements efficient allocations
supporting prices.. Journal Economic Theory.
Vickrey, W. (1961). Counterspeculation, auctions competitive sealed tenders. Journal
Finance, 837.

98

fiJournal Artificial Intelligence Research 37 (2010) 189-246

Submitted 10/2009; published 02/2010

Action Theory Change
Ivan Jose Varzinczak

ivan.varzinczak@meraka.org.za

Meraka Institute, CSIR
Pretoria, South Africa

Abstract
historically acknowledged Reasoning Actions Change community,
intuitiveness logical domain description cannot fully automated. Moreover, like
logical theory, action theories may also evolve, thus knowledge engineers need
revision methods help accommodating new incoming information behavior
actions adequate manner. present work changing action domain
descriptions multimodal logic. contribution threefold: first revisit semantics
action theory contraction proposed previous work, giving robust operators
express minimal change based notion distance Kripke-models. Second
give algorithms syntactical action theory contraction establish correctness
respect semantics action theories satisfy principle modularity
investigated previous work. Since modularity ensured every action theory
and, show here, needs computed evolution domain
description, represent limitation method studied. Finally
state AGM-like postulates action theory contraction assess behavior
operators respect them. Moreover, also address revision counterpart
action theory change, showing benefits semantics contraction.

1. Introduction
Consider intelligent agent designed perform rationally dynamic world, suppose
reason dynamics automatic coffee machine (Figure 1).

NiceCaf
$

Figure 1: coffee deliverer agent.
Suppose, example, agent believes coffee always hot beverage.
Suppose day gets coffee machine observes cold.
case, agent must change beliefs relationship two
propositions hold coffee hold hot drink. example instance
problem changing propositional belief bases largely addressed
c
2010
AI Access Foundation. rights reserved.

fiVarzinczak

literature belief revision (Alchourron, Gardenfors, & Makinson, 1985; Gardenfors,
1988; Hansson, 1999) belief update (Katsuno & Mendelzon, 1992).
Next, let agent believe whenever buys coffee machine, gets
hot drink. means every state world follows execution buying
coffee, agent ends hot drink. Now, situation machine
running cups, buying, coffee runs shelf agent, contrary
expecting, hold hot drink hands.
Imagine agent never considered relation buying coffee
machine service availability, sense always believed (quite reasonably)
buying prevent users using machine. Nevertheless, someday
agent queuing buy coffee observes agent
bought, machine went order (maybe due lack coffee powder).
Completing agents struggle discovering intricacies operating coffee machine, let us suppose always believed token, possible
buy coffee, provided preconditions like close enough button,
free hand, etc, satisfied. Eventually, due blackout, agent realizes
manage buy coffee, even token.
last three examples illustrate cases changing beliefs behavior
action buying coffee mandatory. first one, buying coffee, believed
deterministic outcome, namely always hot drink, seen nondeterministic or, alternatively, different effect specific context (e.g.
cup machine). second example, buying coffee known
side-effects (ramifications) one aware of. Finally, last example,
feasibility action concern questioned light new information showing
context known preclude execution.
cases theory change important one deals logical descriptions
dynamic domains: may always happen one discovers action actually
behavior different one always believed had.
now, theory change studied mainly knowledge bases classical logics,
terms revision update. Since work Fuhrmann (1989), recent studies considered realm modal logics, viz. epistemic logic (Hansson, 1999) dynamic logics (Herzig, Perrussel, & Varzinczak, 2006). Recently
studies investigated revision beliefs facts world (Shapiro, Pagnucco,
Lesperance, & Levesque, 2000; Jin & Thielscher, 2005) agents goals (Shapiro,
Lesperance, & Levesque, 2005). scenario, would concern instance truth
token given state: agent believes token, actually wrong
that. might subsequently forced revise beliefs current
state affairs change goals according perform state.
belief revision operations modify agents beliefs action laws.
hand, interested exactly modifications. Starting Baral
Lobos work (1997), recent studies done issue (Eiter, Erdem, Fink,
& Senko, 2005) domain descriptions action languages (Gelfond & Lifschitz, 1993).
take step direction propose method robust
integrating notion minimal change complying postulates theory change.
190

fiOn Action Theory Change

present text structured follows: Section 2 establish formal background used throughout article. Sections 36 core work:
Section 3 present central definitions semantics action theory change, providing justifications design choices made (Section 4). Section 5 devoted
syntactical counterpart operators Section 6 proof correspondence semantics certain acceptable conditions. Section 7 discuss
postulates contraction/erasure present semantics action theory revision
(Section 8). discussion comparison existing work field (Section 9),
conclude overview future directions research.

2. Logical Preliminaries
Following tradition Reasoning Actions Change (RAC) community,
consider action theories finite collections statements particular
form (Shanahan, 1997):
context, effect every execution action (effect laws);
precondition, action executable (executability laws).
Statements mentioning action represent laws underlying structure
world, i.e., possible states (static laws).
Several logical frameworks proposed formalize statements (Shanahan,
1997). Among prominent ones first-order based Situation Calculus (McCarthy & Hayes, 1969; Reiter, 2001), family Action Languages (Gelfond & Lifschitz,
1993; Giunchiglia, Kartha, & Lifschitz, 1997), Fluent Calculus (Thielscher, 1997),
Propositional Dynamic Logic (PDL) (Harel, Tiuryn, & Kozen, 2000) different specific extensions thereof (De Giacomo & Lenzerini, 1995; Castilho, Gasquet, & Herzig, 1999;
Zhang & Foo, 2001; Castilho, Herzig, & Varzinczak, 2002).
opt formalize action theories using multimodal logic Kn (Popkorn, 1994).
Among main reasons choice are:
benefit well defined semantics multimodal logics which,
going see sequel, provides simple intuitive foundations build
meaning changing action domain descriptions.
Kn syntax allows us express afore mentioned types laws without requiring
full expressiveness PDL machinery first-order language.
Since Kn core mentioned PDL-based action formalisms, shall
say sequel smoothly transfer them.
Contrary first-order based approaches, Kn decidable several implemented
theorem provers available literature.
191

fiVarzinczak

2.1 Action Theories Multimodal Logic
Let Act = {a1 , a2 , . . . , } set atomic action constants given dynamic
domain. example atomic action buy. atomic action associated
modal operator [a]. suppose multimodal logic independently axiomatized (Kracht & Wolter, 1991), i.e., logic fusion interaction
different modal operators.1
Prop = {p1 , p2 , . . . , pn } denotes finite set propositional constants, also called fluents
elementary atoms. Examples token (the agent token) coffee
(the agent holds coffee). Lit = {p, p : p Prop} set literals. use `
denote literal. ` = p, identify ` p. |`| denote atom `.
use small Greek letters , , . . . denote Boolean (propositional) formulas.
recursively defined usual way:
::= p | > | | | | | |
( denotes ( ) ( ).) Fml set Boolean formulas. example
Boolean formula coffee hot. propositional valuation v maximal consistent set
literals. denote v fact v satisfies propositional formula . val()
denote set valuations satisfying . CPL denote Classical Propositional
Logic |=CPL respective consequence relation. Cn() denotes logical consequences
CPL, i.e., Cn() = { : |=
}.
CPL
propositional formula, atm() denotes set elementary atoms actually
occurring . example, atm(p1 (p1 p2 )) = {p1 , p2 }.
Boolean formula, IP() denotes set prime implicants (Quine, 1952),
i.e., weakest terms (conjunctions literals) imply . example, IP(p1 p2 ) =
{p1 p2 , p1 p2 }. prime implicants, properties compute
them, see chapter Marquis (2000). denote prime implicant, given `
, ` abbreviates ` literal . given set X, X denotes complement.
Hence atm() denotes Prop \ atm().
denote complex formulas (possibly modal operators) , , . . .
recursively defined following way:
::= | [a] | | | | |
hai dual operator [a], defined hai =def [a]. instance complex
formula scenario example coffee [buy]coffee.
Given complex formula , act() denote action names occurring ,
i.e., modalities . example, act([a2 ]p1 ([a1 ]p2 [a2 ]p3 )) = {a1 , a2 }.
semantics standard semantics multimodal logic Kn (Popkorn, 1994).
Definition 2.1 (Kn -Model) Kn -model tuple = hW, Ri W set valuations (also called possible worlds), R maps action constants accessibility relations
Ra W W.
1. Later see requirement ensure action theory modular.

192

fiOn Action Theory Change

example, Act = {a1 , a2 } Prop = {p1 , p2 }, Kn -model =
hW, Ri,
W = {{p1 , p2 }, {p1 , p2 }, {p1 , p2 }},


({p1 , p2 }, {p1 , p2 }), ({p1 , p2 }, {p1 , p2 }),
R(a1 ) =
({p1 , p2 }, {p1 , p2 }), ({p1 , p2 }, {p1 , p2 })
R(a2 ) = {({p1 , p2 }, {p1 , p2 }), ({p1 , p2 }, {p1 , p2 })}
Figure 2 gives graphical representation model .
a1
w1
p1 , p2

w2

a1

p1 , p2

a2

:

a1

a1

w3
p1 , p2
a2

Figure 2: Example Kn -model Act = {a1 , a2 }, Prop = {p1 , p2 }.
Notice definition Kn -model follow traditional notion modal
logics: two worlds satisfy valuation. pragmatic choice,
see Section 5. Nevertheless, shall say sequel straightforwardly
formulated standard Kn models well.
Definition 2.2 (Truth Conditions) Given Kn -model = hW, Ri,


|=
p (p true world w ) iff w p (valuation w satisfies p, i.e., p w);
w




|=
[a] iff |= 0 every w0 (w, w0 ) Ra ;
w
w







|=
iff |=
|=
;
w
w
w






|=
iff 6|=w , i.e., |=w ;
w
truth conditions connectives usual.
denote (possibly empty) set Kn -models.




Kn -model model (denoted |= ) w W, |=
.
w




model depicted Figure 2, |= p1 [a2 ]p2 |= p1 p2 . model set


formulas (noted |= ) |= every . set formulas
start (our non-logical theory), called global axiom.
193

fiVarzinczak

Definition 2.3 (Global Consequence) formula global consequence set
global axioms class Kn -models (noted |=
) every
K


n



Kn -model , |= , |= .
Kn state laws describing behavior actions. One way
stating formulas global axioms.2 usually done RAC community (Shanahan, 1997), distinguish three types laws. first kind statements static
laws, constraints allowed states dynamic domain.
Definition 2.4 (Static Law) static law global axiom Fml.
example static law coffee hot, saying agent holds coffee,
holds hot drink. Situation Calculus formalism (Reiter, 2001) one would write
first-order formula s.[coffee(s) hot(s)]. set static laws scenario denoted
Fml. example = {coffee hot}.
second kind action law consider given effect laws. formulas
relating action effects, conditional.
Definition 2.5 (Effect Law) Let , Fml. effect law action global axiom
form [a].
consequent effect always obtains accessible states (which need
exist general) action executed state antecedent holds.
Kripke semantics, means every possible world holds, every transition
a-labeled arrow (if any) leads possible world holds. nondeterministic action, consequent typically disjunction. example effect
law coffee [buy]coffee, saying situation agent coffee,
buying, agent coffee. inconsistent, special kind effect
law call inexecutability law. example, could also token [buy],
expressing buy cannot executed agent token. Situation Calculus examples effect inexecutability laws would expressed respectively
s.[coffee(s) coffee(do(buy, s))] s.[token(s) Poss(buy, s)].
set effect laws given scenario denoted E . coffee machine scenario,
could example:


coffee [buy]coffee,
token [buy]token,
E =


token [buy]
Finally, also define executability laws, stipulate context action
guaranteed executable. Kn , operator hai used express executability. hai>
thus reads execution possible. Formally, hai> true world w means
least one world w0 accessible w via Ra (cf. Definition 2.2).
2. alternative given Castilho et al. (1999, 2002), laws stated aid
extra universal modality local consequence thus considered.

194

fiOn Action Theory Change

Definition 2.6 (Executability Law) Let Fml. executability law action
global axiom form hai>.
instance, token hbuyi> says buying executed whenever agent
token. set executability laws given domain denoted X . scenario
example X = {token hbuyi>}.
Note principle one needs know nothing accessible world w0 . However,
common (albeit tacit) assumption RAC community state executability
laws actions know effects, words act(X ) act(E ).
Situation Calculus example would stated s.[token(s) Poss(buy, s)].
However, point that, traditionally, Reiter basic action theories (Reiter, 2001)
executability laws inexecutability laws mixed together form bi-conditionals
like s.[token(s) Poss(buy, s)], called precondition axioms. critique
practice implications formalizing dynamic domains, see work Herzig
Varzinczak (2007).
three basic types laws, able define action theories:
Definition 2.7 (Action Theory) Given (possibly empty) sets laws , E , X ,
= E X action theory.
Given action theory action a, Ea (resp. Xa ) denote set
effect (resp. executability) laws E (resp. X ). Ta = Ea Xa action
theory a.
worth noting a1 , a2 Act, a1 6= a2 , intuition indeed Ta1 Ta2
overlap , i.e., laws common Ta1 Ta2 laws
structure world. requirement somehow related underlying
modal logic independently axiomatized (see note above).
2.2 Frame, Ramification Qualification Problems
last 40 years, effort reasoning actions community
devoted searching satisfactory solutions frame problem, ramification
problem qualification problem.
Roughly speaking, frame problem (McCarthy & Hayes, 1969) relates need
inferring persistence facts world execution action known
affect them, without state explicitly form frame axioms.
(Frame axioms special type effect law, form ` [a]`, ` Lit.)
example, buying coffee context agent already got one
make lose coffee: coffee [buy]coffee consequence theory.
ramification problem (Finger, 1987) comes observation action may
several possibly interdependent effects stating explicitly huge
task. scenario, want able infer [buy]hot without saying theory,
way intrinsic causal connection coffee hot taken
account. Finally, qualification problem (McCarthy, 1977) amounts addressing
issue ensuring action executable given context. Specifying sufficient
195

fiVarzinczak

conditions action executable incredibly hard task. example, one
may state token hbuyi>, may well case buying fails due
condition unforeseen design time, like agents arm rusty stuck.
core problems RAC community, reader referred
book Shanahan (1997).
sake clarity, abstract frame ramification problems,
suppose agents theory already entails relevant frame axioms. point
however shall say could defined within formalism solution
frame ramification problems. instance, could used suitable solution
frame problem, like e.g. dependence relation (Castilho et al., 1999), used
work Herzig et al. (2006), kind successor state axioms slightly modified
setting (Demolombe, Herzig, & Varzinczak, 2003). make presentation clear
reader, bother particular solution frame problem
assume frame axioms inferred action theory. Actually
suppose intended frame axioms automatically recovered stated
theory, specifically, set effect laws.
Given largely acknowledged difficulty qualification problem literature (Shanahan, 1997), assume priori solution it. Instead, tacitly
assumed many approaches reasoning actions (Castilho et al., 1999; Zhang &
Foo, 2001; Reiter, 2001), suppose knowledge engineer may want state
(not necessarily fully specified) executability laws actions. may incorrect
starting point (and probability be), revising wrong executability
laws approach towards solution one aims work.
information knowledge engineer chance change eventually
correspond intuition (cf. Sections 3 8).
agreed points, action theory example be:


coffee

hot,
token

hbuyi>,






coffee [buy]coffee,
T=
token [buy]token, token [buy],





coffee [buy]coffee, hot [buy]hot
(We stated frame axiom token [buy]token trivially
deduced inexecutability law token [buy].)
Figure 3 shows Kn -model action theory above.
going see sequel finite base formalizing action theory
plays role contraction laws. particular, base representing static laws
turns quite important. given action theory T, useful consider
models whose possible worlds possible valuations allowed :
Definition 2.8 (Canonical Frame) Let = E X action theory.
tuple Mcan = hWcan , Rcan canonical frame if:
Wcan = val(S );



Rcan = aAct Ra s.t. Ra = {(w, w0 ) : [a] Ea , |=w , |= 0 }.
w

196

fiOn Action Theory Change

w1
t, c, h
b

b
w3

w2

:

t, c, h

t, c, h

b

w4
t, c, h

Figure 3: model coffee machine scenario: b, t, c, h stand for, respectively,
buy, token, coffee, hot.

canonical frame action theory need one models. witness why,
let Prop = {p}, Act = {a}, consider simple action theory {p [a], p hai>}.
associated canonical frame Wcan = {{p}, {p}}. Clearly world {p}
satisfy theory.
Definition 2.9 (Canonical Model) canonical model

canonical frame |= T.
Figure 4 shows canonical model action theory example T.
w1
t, c, h
b

:

b
w3

w2
t, c, h

w5

b

t, c, h

w6

w4
t, c, h

t, c, h

t, c, h

Figure 4: canonical model coffee machine scenario.

2.3 Prime Valuations
say atom p essential formula p atm(0 ) every 0
|=
0 . instance, p1 essential p1 (p1 p2 ). Given , atm!()
CPL
denotes set essential atoms . (If contingent, i.e., tautology
contradiction, atm!() = .)
Given Boolean formula, set formulas 0 |=
0
CPL
0
atm( ) atm!(). ForV instance, p1 V
p2
/ p1 , p1 |=
p p2 atm(p1 p2 ) 6
CPL 1
atm!(p1 ). Clearly, atm( ) = atm!( ), moreover whenever |=CPL 0 case,
atm!() = atm!(0 ) also = 0 .
197

fiVarzinczak

Theorem 2.1 (LeastVAtom-Set Theorem, Parikh, 1999) Let propositional formula. |=CPL , every 0 |=CPL 0 , atm() atm(0 ).
proof theorem given Makinson (2007) state here.
Essentially, theorem establishes every Boolean formula , unique
least set elementary atoms may equivalently expressed using atoms
set. Hence, Cn() = Cn().
Given valuation v, v0 v subvaluation. Given set valuations W, subvaluation
v0 satisfies propositional formula modulo W (noted v0 W ) v
v W v0 v. say subvaluation v essentially satisfies (modulo W),
!
!
noted v W , v W {|`| : ` v} atm!(). v W , call v
essential subvaluation (modulo W).
Definition 2.10 (Prime Subvaluation) Let Boolean formula W set val!
uations. subvaluation v prime subvaluation (modulo W) v

W
!

v0 v v0 W .
prime subvaluation formula thus one weakest states truth
true. Hence, prime subvaluations another way seeing prime implicants (Quine,
1952) . base(, W) denote set prime subvaluations modulo W.
Proposition 2.1 W
Let FmlVand W set valuations. w W, w
w vbase(,W) `v `.
Proof: Right left direction straightforward. left right direction, w ,
0
w . Let w0 w least subset w still
Vsatisfying . Clearly, w prime
subvaluation modulo W, w `w0 `, result follows.
2
2.4 Closeness Models
contracting formula model, perform change structure.
several different ways modifying model (not minimal), need
notion distance models identify closest original one.
going see depth next section, changing model amounts
modifying possible worlds accessibility relation. Hence, distance
two Kn -models depend upon distance sets worlds accessibility
relations. based symmetric difference sets, defined
X = (X \ ) (Y \ X).
Definition 2.11 (Closeness Kn -Models) Let = hW, Ri model.
0 = hW0 , R0 least close 00 = hW00 , R00 i, noted 0 00 ,
either WW0 WW00 ;
WW0 = WW00 RR0 RR00 .
198

fiOn Action Theory Change

extension Burger Heidemas relation (Burger & Heidema, 2002)
modal case. defines lexicographic order set Kn -models. Although simple,
notion closeness turns sufficient purposes here, shall see
sequel. Notice notions distance models could considered
well, namely cardinality symmetric differences Hamming distance. (See Section 4
discussion this.)

3. Semantics Action Theory Change
admitting possibility law failing, one must ensure becomes invalid,
i.e., true least one model dynamic domain formalized.
lots models, may set models (potentially) valid.
Thus contracting amounts making longer valid set models.
operations must carried achieve that? Throwing models
work, since keep valid models remaining set. Thus one
add new models M. models? Well, models true.
models: taking models falsifying different original
models certainly violate principle minimal change.
Hence, shall take model basis manipulate get new model
0
true. modal semantics, removal law model
= hW, Ri means modifying possible worlds accessibility relation
becomes false. operation gives result set models
longer model . several candidates, ones choose?
shall take models minimal modifications original , i.e.,
minimal respect distance models. course,
one 0 minimal respect . case, adding
one new models enough invalidate , take possible combinations
{M 0 } expanding original set models one minimal models.
(Observe approach relates orderly maxichoice contraction Hansson, 1999.)
result set sets models. set models precisely one model
0 falsifying .
might claimed that, such, contraction method described
respect so-called principle categorical matching: input output different
sorts objects, namely set models set sets models. easy see,
however, reasoning stated way output set
models corresponds precisely result one contraction operator, satisfying
referred principle. choice defining result operation set possible
outputs become clear Section 5, going present algorithms
correspond exactly semantic constructions.
3.1 Model Contraction Executability Laws
contract executability law hai> one model, intuitively remove
transitions leaving -worlds. order succeed operation, guarantee
resulting model least one -world departing a-arrow.

199

fiVarzinczak


Definition 3.1 Let = hW, Ri. 0 = hW0 , R0 Mhai>


W0 = W;
R0 R;


;
(w, w0 ) R \ R0 , |=
w
M0

w W0 6|=w hai>.


Observe Mhai>
6= satisfiable W. Moreover, Mhai>


6|= hai>.
provide reader insight operation would carried
Situation Calculus, one look given situation holds
modify interpretation predicate Poss(a) becomes false s.
Like case, may many situations must taken
account. essential difference Kripke structures always finite,
whereas space situations possibly infinite (Reiter, 2001).
get minimal change, want operation removing transitions minimal
respect original model: one remove minimum set transitions
sufficient get desired result.
Definition 3.2 contract(M , hai>) =




min{Mhai>
, }

define sets possible models resulting contraction
executability law set models:
Definition 3.3 Let set models, hai> executability law.
0
0
0
0

hai> = {M : = {M }, contract(M , hai>), M}

running example, consider = {M }, model Figure 4.
agent discovers even token manage buy coffee more,
change models order admit (new) models states token
case buy-transition all. one
world new model enough, taking resulting models whose accessibility
relations maximal guarantees minimal change. Hence
tokenhbuyi> =
0
0
0
0
{M {M1 }, {M2 }, {M3 }}, Mi depicted Figure 5.


Clearly, satisfied M, i.e., |= M, contraction
hai> succeed. line expectations relates
Success Postulate (cf. Section 7.2).
200

fiOn Action Theory Change

w1

w1
t, c, h

t, c, h
b

M10 :

b
w3

w2
t, c, h

w5

w6

w4
t, c, h

t, c, h

b

t, c, h

w6

w4

w5

t, c, h

t, c, h

w3

w2

M20 :

t, c, h

b

t, c, h

t, c, h

t, c, h

w1
t, c, h
b

M30 :

b
w3

w2
t, c, h

t, c, h

w5

w6

w4
t, c, h

t, c, h

t, c, h

Figure 5: Models resulting contracting token hbuyi> model Figure 4.
3.2 Model Contraction Effect Laws
agent discovers may cases buying gets
hot drink, must e.g. give belief effect law token [buy]hot set
models. means token hbuyihot shall admitted least one world
new models set beliefs. Therefore, contract effect law [a]
given model, intuitively add new transitions -worlds worlds
satisfying . shall see, great challenge operation precisely
guarantee minimal change.
example, contracting token [buy]hot model Figure 4 shall
add transitions token-worlds hot-worlds. coffee hot static law
hot coffee, also give us hbuyicoffee token-world (coffee
causally relevant hot, i.e., hot must also coffee). means
allow hbuyihot token-world, also allow hbuyicoffee
world. argument necessarily hold token: allowing hbuyihot
necessarily oblige us allow hbuyitoken respective world.
token relevant hot (as coffee is). means freedom either
allow not.
Hence, running example add transitions token-worlds hot
coffee token-worlds, well hot coffee token. situation depicted
Figure 6. instance, add new buy-arrow world {token, coffee, hot}
one candidates (Figure 7).
Situation Calculus, modification would slightly different,
intuition behind: one look given situation holds
modify interpretation fluents (atoms) do(a, s), situation resulting
performing s. Alternatively, new -situations lead least one -situation.
201

fiVarzinczak

w1
t, c, h
b

:

b
w3

w2
t, c, h

w5

t, c, h

b

w6

w4
t, c, h

t, c, h

t, c, h

Figure 6: Candidate worlds receive transitions coming token-worlds.
w1
t, c, h
b

:

b
w3

w2
t, c, h

w5

b

t, c, h

w6

w4
t, c, h

b

t, c, h

t, c, h

b

Figure 7: Two candidate new buy-arrows falsify token [buy]hot .
Notice however would require addition new whole branches tree-like
first-order model induced Reiter basic action theories (Reiter, 2001).
Back example, observe adding new transition {token, coffee, hot}
would make us lose effect token, true every execution buy original

model (|= token [buy]token). preserve law allowing new
transition hot-world? is, get rid effect hot without losing effects
relevant that? develop approach issue.
adding new transition leaving world w intuitively want preserve many
effects so. achieve this, enough preserve old effects
w (because remaining structure model remains unchanged adding
new transition). course, cannot preserve effects inconsistent (those
lost). Hence suffices preserve effects consistent .
achieve must observe true w target world w0 :
proper effects action world w0 , i.e., changes w w0 (w0 \ w)
new execution must obliged so: either
literals change w w0 necessary w0 (like coffee
example) necessary another effect (independent
, like token) world w0 .
202

fiOn Action Theory Change

non-effects action world w0 , i.e., change w w0
(w w0 ) new execution allowed so: certain
literals never preserved (like token example), pointing new
transition towards world change respect leaving world
(hot coffee token example), may lose effects held w
adding transition.
means things allowed change candidate target world must
forced change, either non-related law
modulo set states W. words, want literals (now) change
w w0 sufficient get modulo W, preserving
maximum effects. Every change beyond intended one. Similarly,
want literals w (now) preserved target world w0
usually preserved given set models. Every preservation beyond
may make us lose law. looks like prime implicants, prime
subvaluations play role: worlds new transition point
whose difference respect departing world literals relevant whose
similarity respect literals know change.
Definition 3.4 (Relevant Target Worlds) Let = hW, Ri model, w, w0 W,
set models M, [a] effect law. w0 relevant target
world w respect [a]




|=
6|= 0 ;
w
w

` w0 \ w:
either v base(, W) v w0 ` v;
0 Fml v0 base( 0 , W) v0 w0 , ` v0 ,

every Mi M, |=w [a] 0
` w w0 :
either v base(, W) v w0 ` v;



Mi 6|=
[a]`;
w

RelTarget(w, [a], , M) denote set relevant target worlds w
respect [a] M.
Note need set models (and suppose contains models
theory want change) preserving effects depends effects
hold models interest us. need take account
local operation changing one model. (The reason need definition

local, one model contraction executability laws Mhai>
removing
transitions way losing effects, every effect law held world
transition removed remains true world resulting model.)
203

fiVarzinczak

Definition 3.5 Let = hW, Ri, M. 0 = hW0 , R0

M[a]

W0 = W;
R R0 ;
(w, w0 ) R0 \ R, w0 RelTarget(w, [a], , M);
M0

w W0 6|=w [a].

Observe M[a]
6= satisfiable W. Moreover,



M[a]
6|= [a].

one world law longer true model enough,
taking resulting models whose accessibility relations minimal respect
original one guarantees minimal change.
Definition 3.6 contract(M , [a]) =




min{M[a]
, }

define possible sets models resulting contracting effect law
set models:
Definition 3.7 Let set models, [a] effect law.
0
0
0
0

[a] = {M : = {M }, contract(M , [a]), M}

Taking = {M }, model Figure 4, contracting token
0
0
0
[buy]hot get
token[buy]hot = {M {M1 }, {M2 }, {M3 }},
Mi0 depicted Figure 8.
cases satisfiable valid , course operator
succeed falsifying [a] (cf. end Section 3.1). Again, works
expected Success Postulate (see also Section 7.2).
3.3 Model Contraction Static Laws
contracting static law model, want admit existence least
one (new) possible state falsifying it. means intuitively add new worlds
original model. (In Situation Calculus setting would correspond allowing
situations satisfying domain constraints.) quite easy.
delicate issue however accessibility relation: new transitions
leave/arrive new world? transition leaves new added world, may lose
executability law. transition leaves it, may lose effect law,
holding add transition pointing new world. hand,
transition arrives new world, intuition? intuitive
unreachable state? (Similar issues would also arise Situation Calculus interpretations,
means independent underlying formalism.)
204

fiOn Action Theory Change

w1

w1
t, c, h

b

M10 :

t, c, h
b

b
w3

w2
t, c, h

w3

w2

M20 :

t, c, h

b

b

t, c, h

b

t, c, h

b
w5

w6

w4
t, c, h

w5

t, c, h

t, c, h

b

t, c, h

w6

w4
t, c, h

t, c, h

w1
t, c, h
b

M30 :

b
w3

w2
t, c, h

b

t, c, h

b
w5

w6

w4
t, c, h

t, c, h

t, c, h

Figure 8: Models resulting contracting token [buy]hot model Figure 4.

discussion shows drastic change static laws might be: change
underlying structure (possible states) world! Changing may
indirect, unexpected (and probability unwanted) consequence loss effect
law(s) executability law(s). choose type(s) laws
may accept lose process postpone change (by operators).
Following tradition RAC community, states executability laws
general difficult formalize effect laws, therefore likely
incorrect (Shanahan, 1997), prefer change accessibility relation,
means preserve effect laws postpone correction executability laws,
required. (Remember approach towards solution qualification
problem cf. Section 2.2 above.)
One may argue things way makes three operators incoherent
sense effect executability laws adopt minimal change approach, giving
stronger theories, whereas static laws adopt cautious approach, giving weaker
theories (see next section). worth noting however largely recognized
RAC community, different laws domain description status:
minimal change approach static law contraction preserves many executability
laws possible, even coherent, would definitely fail cope qualification problem.
Moreover, propagating wrong executability laws, coherent method would definitely
less elaboration tolerant (McCarthy, 1998) one defining regards
modifications theory.
reasons, contention static law contraction cautious.
(For detailed discussion this, see Section 4.2 end Section 5.3.)

205

fiVarzinczak

Definition 3.8 Let = hW, Ri. 0 = hW0 , R0
W W0 ;
R = R0 ;
M0

w W0 6|=w .


Note = |= . Moreover, 6|= .
minimal modifications one model defined usual:

Definition 3.9 contract(M , ) = min{M , }
define sets models resulting contracting static law
given set models:
Definition 3.10 Let set models, static law.
0
0
0
0

= {M : = {M }, contract(M , ), M}

scenario example, initial set models = {M }, model
Figure 4, contracting static law coffee hot would give us resulting
0
0
0
new set models
coffeehot = {M {M1 }, {M2 }}, Mi depicted
Figure 9 below.
w1

w7
t, c, h

b

M10 :

w5

t, c, h
b

w3
b

M20 :

t, c, h

w6

w4
t, c, h

t, c, h

b

w2
t, c, h

w1

w7

t, c, h

t, c, h

t, c, h

b
w3

w2
t, c, h

b

t, c, h

w6

w4

w5
t, c, h

t, c, h

t, c, h

Figure 9: Models resulting contracting coffee hot model Figure 4.
Notice modifying accessibility relation effect laws true
original model preserved resulting models. ensured [buy]
true new world w7 .
executability laws potentially lost, due cautiousness
approach. instance, M10 above, longer case token hbuyi>
true, since world, namely w7 , satisfy anymore. (In M20
executability law still true every possible world.)
worth point out, however, approach indeed line intuition:
learning new state possible, necessarily know behavior
actions new added state. may expect action laws hold new world
(see end Section 5.3), but, information dispose, touching accessibility
relation safest way contracting static laws (cf. Section 4.2 below).
206

fiOn Action Theory Change

4. Interlude
presenting algorithmic counterpart action theory change operators,
section discuss alternatives technical constructions. point
issues alternatives would raise. also provide justifications
design choices made previous sections.
4.1 Distance Notions
defined used model distance based symmetric difference sets (Definition 2.11). distance extension Kripke structures
Winsletts (1988) notion closeness propositional interpretations Possible
Models Approach (PMA). Instead it, however, could considered distance
notions well, like Dalals (1988) distance, Hamming distance (1950), weighted distance. Due space limitations, develop comparison among
distances here. (For details, reader may want refer Schlechtas 2004 book.)
nevertheless show cardinality-based distance, example, may
always get intended result.
Let card(X) denote number elements set X. suppose closeness
Kn -models defined follows:
Definition 4.1 (Cardinality-based Closeness Kn -Models) Let = hW, Ri
model. 0 = hW0 , R0 least close 00 = hW00 , R00 i, noted
0 00 ,
either card(WW0 ) card(WW00 );
card(WW0 ) = card(WW00 ) card(RR0 ) card(RR00 ).
notion distance closely related Dalals (1988) closeness.
contracting static law model usually add one new
possible world, easy see cardinality-based distance get
result contract(M , ) distance Definition 2.11.
comes contraction action laws, changing accessibility
relations, however, cardinality-based distance seem fit intuitions.
witness, consider model Figure 10, law p1 hai> true.


w1

w2
p1 , p2

p1 , p2


:


w3
p1 , p2

Figure 10: model executability law p1 hai>.
207

fiVarzinczak

Then, models resulting contraction p1 hai> model
Mphai> = {M 0 , 00 }, 0 00 depicted Figure 11.
1

w2

w1



w1

p1 , p2

p1 , p2

w2
p1 , p2

p1 , p2



M0 :



00 :

w3

w3

p1 , p2

p1 , p2

Figure 11: Models resulting contracting p1 hai> model Figure 10.
00

Note 00 intended contracted model: 6|= p1 hai>. However,
0
cardinality-based distance get {M }
p1 hai> = {{M , }}.
00
0
00
0
{M , } result since : one transition removed,
00 two.
4.2 Minimal Change v. Cautiousness
usually done literature classical belief revision, defining (traditional)
theory change operator one must always make fundamental decision two opposing principles guiding one: minimizing change, leads
strong modified theories, versus cautious change, leads weak theories.
regard, one might argue action theory change operators incoherent.
adopt first principle contraction effect executability laws,
latter principle contraction static laws.3
turns out, however, view debatable. different perspective one
think three operators coherent following sense: perform
version maxichoice, namely addition precisely single model original models
theory.4
case, sequel give justification behavior operators
show operator contraction static laws cautious
coherent operators contraction effect executability laws. (We
say operator static law contraction coherent respect operators
contraction effect executability laws also performs minimal change respect
types laws, i.e., preserves effect executability laws.)
claimed incoherence come from? contention
inherent problem action theory change itself, flaw definitions.
justification follows. Remembering intuitions semantic constructions,
easy see contraction executability laws knowledge actions
feasibility (the transitions) removed that. contraction effect laws,
3. thank anonymous referee pointed out.
4. thank another anonymous referee pointed out.

208

fiOn Action Theory Change

piece knowledge also added (the new transition), notice one guided
given concrete extra information, namely effect want allow.
Now, contraction static laws, notice extra information whatsoever
given new possible state could guide addition knowledge
feasibility action. thing know new world exist.
Nothing said whether transition leaving arriving
all. property problem per se: problem removing static law
mention executabilities, reflected operator.
Therefore, incoherence already problem, surprising
find proposed operators. designed allowed
given constraints problem. information hands
regarding new added state, coherent version corresponding operator would
defined. (See discussion Section 9 comparison Eiter et al.s 2005
constraint-based method update action theories.)
Proposition 4.1 minimal change operator static law contraction
coherent operators contraction effect executability laws.
Proof: Suppose minimal change based (non-cautious) contraction operator
static laws coherent operators. operator must
contracting Fml formulas type removed (otherwise
coherent operators). means effect executability laws
preserved. particular, operator coherent respect contracting
formula p1 p2 model Figure 12 below.
w1

w1
p1 , p2

:



p1 , p2

w3

M0 :

w2



w2
p1 , p2

p1 , p2



p1 , p2

Figure 12: Adding transition new added world alternative semantics
static law contraction. denotes original model, 0 shows new
added world candidate transition add Ra .
Following intuition contraction Boolean formulas, new world, viz.
valuation {p1 , p2 }, added W . operator question non-cautious,
transition also added new added world {p1 , p2 } , order
preserve executability law p1 hai>. Also operator non-cautious,
effect law p1 [a]p1 preserved. Hence, new transition point
neither world {p1 , p2 } {p1 , p2 } itself. Now, direct new transition
{p1 , p2 } (the world left), get model 0 Figure 12.
209

fiVarzinczak



M0

Observe |= (p1 p2 ) [a]p1 . However, 6|= (p1 p2 ) [a]p1 : operator
makes us lose effect law! means coherent. order us keep
effect law, option direct new transition {p1 , p2 }. then,
transition added all: operator cautious! Hence operator
static law contraction based minimal change coherent operators
laws.
2
result supports contention cannot coherent set minimal
change operators action theory contraction. general result holds
modal-based approaches like ours, applies framework reasoning
actions based transition systems also allows three types
laws consider here.
Furthermore, result also illustrates well difference action theory change
classical belief change. witness, even though contraction static laws amounts
propositional contraction Boolean formulas, remains special case latter.
reason contracting static laws one always asks happens laws
types?, question asked classical propositional contraction
obvious reason simply types formulas.

5. Syntactic Operators Contraction Laws
given semantic construction action theory change, turn
attention definition syntactic operators changing sets formulas describing
dynamic domain.
Nebel (1989) says, [. . . ] finite bases usually represent [. . . ] laws,
forced change theory would like stay close possible original
[. . . ] base. Hence, besides definition syntactical operators, also guarantee
perform minimal change theory level. mean resulting
theory course entail law want contract theory with,
also preserve much previous knowledge possible performing syntactical
manipulations laws original theory. Ideally, knowledge engineers
perspective, modified theory also keep certain degree resemblance
original one: resulting laws slight modifications relevant ones
original action theory.

denote sequel result contracting law set laws T.
5.1 Contracting Executability Laws
case contracting executability law hai> action theory, first
ensure action keeps executability state contexts
antecedent holds, case. achieve strengthening antecedents
relevant executability laws. Second, order get minimality, must make
executable contexts true, viz. -worlds one. Since
possibly many different alternatives that, means several action
theories outcome. Algorithm 1 gives syntactical operator achieve this.
210

fiOn Action Theory Change

easily checked Algorithm 1 always terminates: input action theory
always finite; finiteness Prop follows atm(), IP(S ).
Moreover, entailment problem multimodal K decidable (Harel et al., 2000),
classical propositional logic. Therefore contracting executability laws decidable.
Algorithm 1: Contraction Executability Law
Input: T, hai>
Output:
hai> /* set theories output knowledge engineer */
1

begin

2

:=

hai>

3

|=
hai> 6|=

K
CPL
n

foreach IP(S )

4

forall atm()
V
V
:= pi atm() pi pi atm() pi /* extend valuation */

5
6

pi

6|=
( )
CPL

7

9

else
:= {T}

hai>

11
12
13

/* allowed state */

/* construct theory weaker state */
0 := (T \ Xa ) {(i ( )) hai> : hai> Xa }
0
:=

hai>
hai> {T }

8

10

pi
/

return
hai>
end

straightforward see Algorithm 1 adapted Situation Calculus
action theories well. crucial point however would termination, since entailment
Situation Calculus general undecidable.
running example, contracting executability law token hbuyi>
0
0
0
action theory would give us
tokenhbuyi> = {T1 , T2 , T3 }, where:


coffee hot, coffee [buy]coffee,





token [buy]token, token [buy],
0
T1 =
coffee [buy]coffee, hot [buy]hot,





(token (coffee hot)) hbuyi>


coffee

hot,
coffee

[buy]coffee,






token [buy]token, token [buy],
0
T2 =
coffee [buy]coffee, hot [buy]hot,





(token (coffee hot)) hbuyi>
211

fiVarzinczak






coffee hot, coffee [buy]coffee,
token
[buy]token, token [buy],
T30 =
coffee
[buy]coffee, hot [buy]hot,



(token (coffee hot)) hbuyi>









knowledge engineer choose theory line
intuitions implement required changes (cf. Figure 5).
5.2 Contracting Effect Laws
contracting effect law [a] action theory T, intuitively contract effect laws preclude target worlds. order cope minimality,
must change laws relevant unwanted [a].
Let (Ea, )1 , . . . , (Ea, )n denote minimal subsets (with respect set inclusion) Ea
, (Ea, )i |=
[a], 1 n. words, (Ea, )i support set
Kn
effect law [a] T. make parallel terminology usually adopted
belief change community, shall see (Ea, )i special type kernel (Hansson,
1994) formula [a].
According Herzig Varzinczak (2007), given action theory one always
ensure least one support set [a] exists. let
[
Ea =
(Ea, )i
1in

laws Ea serve guidelines get rid [a] -world allowed
theory T: effect laws weakened allow hai -contexts.
resembles classical kernel contraction (Hansson, 1994): finding minimal sets implying
formula changing them. crucial difference, however, instead completely
removing formula kernel, weaken laws.
modifying support sets, first thing must ensure action
still effect contexts hold, case.
means shall weaken laws Ea, specializing . Now, need preserve
old effects -worlds one. achieve specialize laws
possible valuation (maximal consistent conjunction literals) satisfying one.
Then, left -valuation, must ensure action either old effects
outcome. achieve weakening consequent laws Ea . Finally,
order get minimal change, must ensure literals -valuation
forced change -worlds preserved. stating effect law
form (k `) [a]( `), k -valuation. reason
needed clear: several -valuations, far want one
reachable k -world, force one whose difference
k -valuation minimal.
Situation Calculus terms, syntactical operations would correspond strengthening right-hand side relevant successor state axioms and/or weakening
212

fiOn Action Theory Change

left-hand side. Alternatively, done original effect axioms,
recompiling new successor state axioms afterwards.
output operations described set action theories
output knowledge engineer. Algorithm 2 gives operator.
Algorithm 2: Contraction Effect Law
Input: T, [a]
Output:
[a] /* set theories output knowledge engineer */
1

begin

2

:=

[a]

3

|=K [a] 6|=

CPL
n

foreach IP(S )

4

forall atm()
V
V
:= pi atm() pi pi atm() pi /* extend valuation */

5
6

pi

pi
/

6|=
( )
CPL

7

/* allowed state */

0

foreach IP(S )
0 := \ Ea /* support sets weakened */
0 := 0 {(i ( )) [a]i : [a]i Ea }

8
9
10

11

/* allow state */
0 := 0 {(i ) [a](i 0 ) : [a]i Ea }

12

forall L Lit
|=
( )
CPL

13

V

`L `)



n

0 := 0 {( `) [a]( `)}

16

0
:=

[a]
[a] {T }

17

else
:= {T}

[a]

19

21

6|=
( 0
CPL

6|=
( `) [a]` ` 0
K

15

20

`L `

foreach ` L

14

18

V

return
[a]
end

Again, finiteness action theory atm(),
IP(S ), decidability multimodal K (Harel et al., 2000) well
classical propositional logic, easily verified Algorithm 2 always terminates.
213

fiVarzinczak

Therefore, contracting effect laws decidable. course, complexity computing
support sets well prime implicants quite high (see Section 5.4 later
discussion matter).
example execution Algorithm 2, let us suppose want contract
effect law token [buy]hot action theory running example. First
compute support sets token [buy]hot (i.e., minimal subsets
Ebuy together entail token [buy]hot). following:
token,hot
(Ebuy
)1

token,hot
(Ebuy
)2



coffee [buy]coffee,
coffee [buy]coffee





hot [buy]hot,
coffee [buy]coffee



=

=

possible context antecedent token case,
token,hot
token,hot

)2 . Since = {coffee hot},
)1 (Ebuy
= (Ebuy
weaken effect laws Ebuy
contexts token coffee hot, token coffee hot token coffee hot.


token coffee hot: Algorithm 2 replaces laws Ebuy



(coffee (token coffee hot)) [buy]coffee,
(hot (token coffee hot)) [buy]hot,


(coffee (token coffee hot)) [buy]coffee
preserve effects possible contexts token coffee hot. Now,
order preserve effects token coffee hot-contexts allowing reachable
hot-worlds, algorithm adds laws:


(token coffee hot) [buy](coffee hot),
(token coffee hot) [buy](hot coffee)
Now, search possible combinations laws Ebuy apply token coffee hot
contexts find token [buy]token. token must true every execution
action buy, state law (token coffee hot) [buy](hot token), end
following theory:


coffee hot, token hbuyi>,








token

[buy]token,
token

[buy],







(coffee (token coffee hot)) [buy]coffee,

(hot (token coffee hot)) [buy]hot,
T10 =



(coffee (token coffee hot)) [buy]coffee,









(token

coffee

hot)

[buy](coffee

hot),




(token coffee hot) [buy](hot coffee)
hand, language also atom p theory T,
added law (token coffee hot p) [buy](hot p) meet minimal
change preserving effects relevant (cf. Definition 3.4).
214

fiOn Action Theory Change

execution contexts token coffee hot token coffee hot analogous
0
0
0
algorithm ends
token[buy]hot = {T1 , T2 , T3 }, where:

T20 =

















coffee hot, token hbuyi>,
token [buy]token, token [buy],
(coffee (token coffee hot)) [buy]coffee,
(hot (token coffee hot)) [buy]hot,
(coffee (token coffee hot)) [buy]coffee,
(token coffee hot) [buy](coffee hot)


coffee hot, token hbuyi>,




token [buy]token, token [buy],




(coffee (token coffee hot)) [buy]coffee,
(hot (token coffee hot)) [buy]hot,
T30 =


(coffee
(token coffee hot)) [buy]coffee,





(token
coffee hot) [buy](hot coffee),


(token coffee hot) [buy](coffee hot)




































Looking Figure 8, see correspondence theories
respective models. knowledge engineer look action theories
pick one corresponding expectations.
5.3 Contracting Static Laws
Finally, order contract static law theory, use contraction/erasure
operator classical logic available literature. contracting static
laws means admitting new possible states (cf. semantics), modifying set
static laws may enough multimodal logic case. However, since general
necessarily know behavior actions new discovered state
world, careful approach change theory action laws remain
contexts contracted law case. (The reader invited see
Situation Calculus allowing new situation exist one may need change
precondition axioms well, means problem described independent
logical formalism chosen.)
scenario example, contracting static law coffee hot knowledge
engineer really sure whether action buy still executable not,
weaken set executability laws specializing context coffee hot,
make buy priori inexecutable (coffee hot)-contexts. worth noting
line assumption commonly made RAC community according
executability laws large much likely incorrect right
beginning (Shanahan, 1997). Therefore extrapolating previously unknown states
might (and probability will) result propagation errors and, even worse,
loss effect laws (remember discussion Sections 3.3 4.2). operator given
Algorithm 3 formalizes this.
215

fiVarzinczak

Algorithm 3: Contraction Static Law
Input: T,
Output:
/* set theories output knowledge engineer */
1

begin

2


:=

3

|=CPL
/* call classical contraction */
foreach

4

/* build theory preserving executability old states */
0 := ((T \ ) ) \ Xa

5

0 := 0 {(i ) hai> : hai> Xa } { [a]}

0

:= {T }

6
7
8

else

:= {T}

9
10
11

return

end

running coffee example, contracting static law coffee hot action
0
0
theory produces
coffeehot = {T1 , T2 },

T10 =













T20 =













(token coffee hot),
(token coffee hot) hbuyi>,
coffee [buy]coffee, token [buy]token,
token [buy], coffee [buy]coffee,
hot [buy]hot, (coffee hot) [buy]








(token coffee hot),
(token coffee hot) hbuyi>,
coffee [buy]coffee, token [buy]token,
token [buy], coffee [buy]coffee,
hot [buy]hot, (coffee hot) [buy]




















Observe effect laws affected change: far
pronounce executability action new added world,
effect laws remain true it.
knowledge engineer happy (coffee hot) [buy], contract
formula theory using Algorithm 2. Ideally, besides stating buy executable context coffee hot, want specify outcome context
well. example, could want (coffee hot) hbuyihot true result.
requires theory revision. See Section 8 semantics operation.
216

fiOn Action Theory Change

5.4 Complexity Issues
terminating, algorithms come considerable computational cost: Kn entailment tests global axioms beginning algorithms inside loops
known exptime-complete (Harel
V et al., 2000).
V computation possible
contexts allowed theory, namely pi atm() pi pi atm() pi , atm()
pi

pi
/

IP(S ), clearly exponential. Moreover, computation prime implicants
IP(.) might result exponential growth (Marquis, 2000).
Given theory change carried offline, perspective knowledge engineer important complexity size computed contracted theories: number formulas well length modified ones.
plays important role deciding among several output theories one corresponds
knowledge engineers expectations. matter, whereas length new added
formulas may increase exponentially, respect number laws results
positive: size computed contracted theories linear size original
action theory. (Remember card(X) denotes number elements set X.)
Proposition 5.1 Let action theory, hai> executability law, 0
0

hai> . card(T ) = card(T).
0
Proof: 6|=
hai>,
hai> = {T}, = T, result
Kn
0
follows. Suppose |=
hai> case. 0 = (T \ Xa ) Xa 0 ,
Kn
Xa 0 obtained Xa way (i 0 ) hai> Xa 0
hai> Xa , fixed 0 . follows card(Xa 0 ) = card(Xa ). Now,
card((T \ Xa ) Xa 0 ) = card(T \ Xa )+card(Xa 0 )card((T \ Xa ) Xa 0 ) = card(T)card(Xa )+
card(Xa 0 ) card() = card(T) card(Xa ) + card(Xa ) 0 = card(T).
2

Proposition 5.2 Let action theory, [a] effect law, 0
[a] .
card(T 0 ) card(T) + card(Ea ) + card(Lit).
0
Proof: 6|=
[a],
[a] = {T}, = T, get
Kn
card(T 0 ) = card(T). Since card(T) card(T) + card(Ea ) + card(Lit), result follows.
Suppose |=K [a] case. 0 = (T \ Ea ) Ea 0 Ea 00 Fa , where:
n

0

00

Ea Ea obtained Ea way (i 0 ) [a]i Ea 0
(i 0 ) [a](i 0 ) Ea 00 [a]i Ea , fixed 0 , 0 ;
Fa {(0 `) [a]( `) : ` Lit}, fixed 0 ;
T, Ea 0 , Ea 00 , Fa pairwise disjoint.
Hence card(Ea 0 ) = card(Ea 00 ) = card(Ea ), card(Fa ) card(Lit). card(T 0 ) =
card(T \ Ea ) + card(Ea 0 ) + card(Ea 00 ) + card(Fa ) = card(T \ Ea ) + card(Ea ) + card(Ea ) +
card(Fa ) = card(T) card(Ea ) + card(Ea ) + card(Ea ) + card(Fa ) = card(T) + card(Ea ) +
card(Fa ) card(T) + card(Ea ) + card(Lit).
2
217

fiVarzinczak

Given arbitrary choice contraction operator static laws, without loss
generality resort slightly modified version it, viz. one always gives us
result set static laws cardinality original . (This possible
since, contrary E X , conjunction static laws still static law,
rewriting.) agreeing that, following proposition straightforward:
0
Proposition 5.3 Let action theory, static law, 0
. card(T ) =
card(T) + 1.

Propositions 5.15.3 positive results: knowledge engineer deal
original action theory, able deal output algorithms.
(Observe given 0 conditional frame axioms added Fa contraction
effect law factored single law, resulting theory
cardinality card(T) + card(Ea ) + 1.)
finish section observing size
, set resulting contracted
theories, depends solely set static laws plus law contract with:
Proposition 5.4 Let action theory, let law |=
.
K
n

card(T
) = card(S ),
card(T
) = card(val(S {})), either hai> [a].
Proof: proof follows straightforwardly outermost loops Algorithms 13. 2

6. Correctness Operators
address correctness algorithms respect semantics contraction. Correctness understood completeness adequacy. Adequacy means
algorithms output theories whose models result semantic modifications
models original theory. Conversely, completeness says every model resulting
semantic modifications models original theory indeed model
theory output algorithm.
6.1 Challenges Completeness Adequacy
Let theory = {p1 hai>, (p1 p2 ) [a], [a]p2 } consider model
depicted Figure 13. (Notice |=
(p1 p2 ).) contracting p1 [a]p2 ,
Kn
0
get Figure 13.
0
contracting p1 [a]p2 using Algorithm 2 gives
p [a]p = {T },
1

2



p1 hai>, (p1 p2 ) [a],
(p1 p2 ) [a](p2 p2 ),
T0=


(p1 p2 ) [a](p2 p1 )
Notice formula (p1 p2 ) [a](p2 p1 ) put 0 Algorithm 2
{p1 } Lit 6|=
(p p2 ) 6|=K (p1 p2 ) [a]p1 .
CPL 1
n

218

fiOn Action Theory Change





w1



p1 , p2

w2

w1

p1 , p2

:

M0 :
w3

w2
p1 , p2



p1 , p2


w3

p1 , p2

p1 , p2

Figure 13: model result 0 contracting p1 [a]p2 it.
M0

0
clearly case 6|= 0 theory
p1 [a]p2 model. means
theories contraction operators complete.
issue arises Algorithm 2 tries allow transition p1 p2 -world
p2 -world closest it, viz. {p1 , p2 }, way knowing
V
0
world exist. remedy replacing test 6`
(
`L `)
Kn
V
0
6`CPL ( `L `) , would increase even complexity algorithm.
better option would complete enough allow algorithm determine
worlds new transition could exist.

way round, hold general models 0
result
semantic contraction models . see suppose
one atom p one action a, consider action theory = {p [a], hai>}.
model = h{{p}}, {({p}, {p})}i Figure 14.




w2

w1

w1

:

p

M0 :

p

p

Figure 14: Inadequacy contraction: model model 0 theory
resulting contracting p hai> T.
definitions, contract(M , p hai>) = {M }. (There p-world
0
remove arrow.) hand,
phai> singleton {T }
0 = {p [a], p hai>}. 0 = h{{p}, {p}}, ({p}, {p})i Figure 14
model resulting contracted theory. Clearly, 0 result semantic
contraction p hai> : p valid contraction models T,
valid models 0 . means theories operators
adequate.
problem occurs because, example, worlds forbidden T, e.g.
{p}, preserved 0 . contracting executability effect law,
supposed change possible worlds theory (cf. Section 3).
Fortunately correctness algorithms respect semantics established action theories whose maximal, i.e., set static laws alone
219

fiVarzinczak

characterize worlds possible models theory. principle
modularity (Herzig & Varzinczak, 2005b) briefly review next section.
6.2 Modular Action Theories
quite useful, albeit simple, property domain descriptions reasoning actions
action theory modularity (Herzig & Varzinczak, 2005b).
Definition 6.1 (Modularity) action theory modular every
Boolean formula Fml, |=
, |=CPL .
K
n

example non-modular theory, let us suppose action theory
coffee machine scenario stated


coffee

hot,
hbuyi>,






coffee [buy]coffee,
T=

token [buy]token, token [buy],




coffee [buy]coffee, hot [buy]hot
modified law underlined: (in case wrongly) stated agent
always buy machine. |=K token, 6|=
token.
CPL
n

Since underlying multimodal logic independently axiomatized (see Section 2.1),
use algorithms given Herzig Varzinczak (2005b) check whether action
theory satisfies principle modularity. Whenever case, algorithms
return Boolean formulas entailed theory consequences alone.
theory above, would return {token}: stated hbuyi>,
inexecutability law token [buy] |=
token. 6|=
token,
Kn
CPL
token called implicit static law (Herzig & Varzinczak, 2004) action
theory T.5
Modular action theories several interesting computational properties. example, consistency checked checking consistency static laws :
modular, |=
|=
. Deduction effect laws
Kn
CPL
need executability ones vice versa. Deduction effect sequence actions a1 ; . . . ; (prediction) need take account effect laws actions
a1 , . . . , . applies particular plan validation deciding whether
ha1 ; . . . ; case.
Modularity exclusive property action theories formalized Kn : similar
notions also investigated different contexts formalisms, like regulation consistency deontic logic (Cholvy, 1999), Situation Calculus (Herzig & Varzinczak,
2005a), DL ontologies (Herzig & Varzinczak, 2006), dynamic logic (Zhang, Chopra, & Foo,
2002) also Fluent Calculus (Thielscher, 2010). details modularity
Kn action theories, well role presence solution frame
ramification problems, see work Varzinczak (2006).
5. Implicit static laws closely related veridical paradoxes (Quine, 1962). turns sometimes
intuitive, sometimes not. deep discussion implicit static laws, see
work Varzinczak (2006).

220

fiOn Action Theory Change

Another interesting property modular action theories following:
Theorem 6.1 modular canonical model.
Proof: Let Mcan = hWcan , Rcan canonical frame T.
Mcan

Mcan

(): definition, Mcan |= E . remains show |= X . Let
Mcan
hai> Xa , let w Wcan |=w . Therefore j Fml
Mcan

|=K j [a], must 6|=
w
n

Mcan

|=
(i j ), hence |=
CPL

Mcan

w0 Wcan |= 0
Mcan

|=

w

j , |=
(i j ), modular,
K
n

(i j ). construction Mcan ,
Mcan

[a] Ea |=
w

. Thus Ra (w) 6=

hai>.

(): Suppose modular. must Fml |=

Kn
6|=
. means v val(S ) v 6 . v Wcan (because Wcan
CPL
contains possible valuations ), Mcan model T.
2
6.3 Correctness Modularity
shown Herzig Varzinczak (2007), given action theory formalized
framework available literature allowing expression three basic types
laws, always possible ensure modularity. Moreover, going see
sequel (cf. Section 7.2), computed evolution
action theory. Hence, relying modular theories limitation approach.
following theorem establishes assumption action theory
modular, semantic contraction formula set models produces
models contracted theory
.


Theorem 6.2 Let modular, law. M0
|=
M0

0
0
0
every M, 0
|= every .

2

Proof: See Appendix A.

next theorem establishes way round: modularity models theories

models semantic contraction models T.
M0

0
0
Theorem 6.3 Let modular, law, 0
. |= ,


0
0
M0
|= every M.

2

Proof: See Appendix B.
two theorems get correctness operators:
M0

0
Corollary 6.1 Let modular, law, 0
|=
. |=
K


n

every 0 M0 M0
|= M.
221

fiVarzinczak

Proof:
M0
0
0
(): Let 0 |= 0 . Theorem 6.3, M0



M0

|= M. 0 |=
, |= .
K
n

(): Suppose 0 6|=K . (We show model 0 M0 M0

n



M0

|= M, 6|= .)
Given modular, Lemma B.1 0 modular, too. Then, Lemma B.3,
M0
M0
0 = hval(S 0 ), R0 6|= . Clearly |= 0 , Lemma B.4
result follows.
2

7. Assessment Postulates Change
action theory change operators satisfy classical postulates change?
answering question, one ask: operators behave like revision update
operators? address issue show postulates theory change
satisfied definitions.
7.1 Contraction Erasure?
distinction revision/contraction update/erasure classical theories
historically controversial literature. true case modal theories
describing actions effects. rephrase Katsuno Mendelzons definitions (1992) terms see one method closer.
Katsuno Mendelzons view, contracting law action theory intuitively means description possible behavior dynamic world must
adjusted possibility false. amounts selecting models
closest models allow models result.
contrast, update methods select, model T, set models
closest . Erasing means adding models T; model , add
models closest false. Hence, constructions far
seems operators closer update revision.
Moreover, according Katsuno Mendelzons view (1992), change operators
would also classified update make modifications model independently, i.e., without changing models.6 Besides that, setting different
ordering resulting models induced model theory (see Definitions 3.3, 3.7 3.10), according Katsuno Mendelzon typical property
update/erasure method.
Nevertheless, things get quite different comes postulates theory change.
7.2 Postulates
section analyze behavior action theory change operators respect
AGM-like postulates. follow Katsuno Mendelzons presentation
6. Even contracting effect law one particular model need check models
theory, modified.

222

fiOn Action Theory Change

postulates assess contraction erasure. Let = E X denote action
theory denote law.
0 , 0
Monotonicity Postulate: |=
.
K
n

postulate version Katsuno Mendelzons (C1) (E1) postulates
contraction erasure, respectively, satisfied change operators.
proof Lemma A.1. postulate satisfied operators proposed
Herzig et al. (2006): removing e.g. executability law hai> one may
make [a] valid models resulting theory.
Preservation Postulate: 6|=
, |=
0 , 0
.
K
K
n

n

Katsuno Mendelzons (C2) postulate. operators satisfy far
whenever 6|=
, models resulting theory exactly models T,
Kn
minimal models falsifying .
corresponding version Katsuno Mendelzons (E2) postulate erasure,
i.e., |=
, |=K 0 , 0
, clearly also satisfied operators
Kn
n
special case postulate above. Satisfaction (C2) indicates operators
closer contraction erasure.
6|=K 6|=K , 0 6|=K , 0
.

Success Postulate:

n

n

n

postulate version Katsuno Mendelzons (C3) (E3) postulates.
propositional Fml, operators satisfy it, long classical propositional
change operator satisfies well. general case, however, stated postulate
always satisfied. shown following example: let = {p, hai>, p [a]}.
Note modular consistent. Now, contracting (contingent) formula p hai>
gives us 0 = T. Clearly 0 |=K p hai>. happens because, despite
n
tautology, p hai> trivial formula respect T: since p valid T-models,
p hai> trivially true models (cf. end Section 3.1).
Fortunately, formulas non-trivial consequences theory,
operators guarantee success contraction:
Theorem 7.1 Let consistent, executability effect law
6|=
. modular, 0 6|=K every 0
.
K
n

n

0
Proof: Let us suppose 0
. Since modular,
|=
K
n

M0

Corollary 6.1 tells us |= every 0 M0 M0
, = {M :


|= = hval(S ), Ri}.
M0

00

|= every 0 M0 , even 00 M0 \M |= . 00
00

M, definition 6|= . Hence = , truth
depend accessibility relation Ra . Hence, whether form hai>
[a], , Fml, holds |=CPL (see Definitions 3.1 3.5),
therefore get |=K .
2
n

223

fiVarzinczak

Equivalences Postulate: |=K T1 T2 |=
1 2 , |=K T10 T20 ,
K

0
T10 (T1 )
2 T2 (T2 )1 .

n

n

n

postulate corresponds Katsuno Mendelzons (C4) (E4) postulates.
worth noting equivalence considered always modulo action laws, i.e.,
formulas assumed either static laws, effect laws executability laws, well
equivalents. Moreover remember theories must action theories,
i.e., sets action laws three basic types. modularity assumption
propositional change operator satisfies (C4)/(E4), operations satisfy postulate:
T1 T2 |=K 1 2 ,
Theorem 7.2 Let T1 T2 modular. |=
K
n

n


0
0 T20 , vice-versa.
T10 (T1 )
2 T2 (T2 )1 |=
K 1
n

Proof: proof follows straight results: since |=
T2 |=K 1 2 ,
K 1
n





n

pairwise models. Hence, given |= T1 |= T2 , semantic
contraction 1 2 operations . T1 T2
modular, Corollary 6.1 guarantees get syntactical results. Moreover,
classical operator satisfies (C4)/(E4), follows |=K T10 T20 .
2
n

Recovery Postulate: 0 {} |=
T, 0
.
K
n

action theory counterpart Katsuno Mendelzons (C5) (E5) postulates. rely modularity order satisfy it.
Theorem 7.3 Let modular. 0 {} |=
T, 0
.
K
n

Proof: 6|=K , operators satisfy preservation postulate, 0 = T,
n
result follows monotonicity.
Let |=
, let M0 denote set models 0 . modular, CorolK
n

M0

M0

lary 6.1 every 0 M0 either |= (and |= ) 0 contract(M , )

(and 0 ) |= T.
Let M00 denote set models 0 {}. Clearly M00 M0 , monotonicity.
00

Moreover, every 00 M00 |= , hence 00
/ every


|= T, 00
/ contract(M , ), model T. Thus 00 model
0
{} |=K T.
2
n
W

0
Let
denote disjunction .
W
W
Disjunctive Rule: (T1 T2 )
(T1 )
(T2 )
equivalent

.
version (E8) erasure postulate Katsuno Mendelzon. Clearly
syntactical operators manage contract law disjunction theories: T1 T2
action theory cannot general rewritten one. Nevertheless, proving
holds semantics, correctness operators, get equivalent
operation. fact theories concern modular gives us result.
224

fiOn Action Theory Change

Theorem 7.4 Let T1 T2 modular, law.
_
_
_
(T2 )
(T1 )
|=K
(T1 T2 )
)

(
n

Proof:
W
0W
0W
0W

(T2 )
(T1 )
(): Let 0 |=
(T1 )
. Suppose
|=
(T2 ) . |=
0
M0
W


|=
(T1 ) (the case analogous). (T1 )0 (T1 ) |= (T1 )0 .
0
0
Corollary 6.1, M0
, set models
0
T1 . model resulting contracting models T1 , 0 also
results contracting models T1 T2 , viz. models T1 . CorolM0
0W
0 , |=


|=
(T


)
(T1 T2 )
lary 6.1, (T1 T2 )0 (T1 T2 )
1
2
.

0W

(): Let 0 |=
M0


0
(T1 T2 )
. (T1 T2 ) (T1 T2 )

0
0
|= (T1 T2 )0 . Corollary 6.1, M0
, set
0
models T1 T2 . model resulting contracting models T1 T2 .
Hence 0 results contracting models T1 models T2 . Suppose
former case (the second analogous). Corollary 6.1 (T1 )0 (T1 )

M0
0W
|= (T1 )0 , |=
(T1 )
.
2


thus shown constructions satisfy (E8) postulate. Nevertheless,
far see, immediate whether really expected here. supports
position operators behavior closer contraction erasure.
seen results above, modularity sufficient condition
satisfaction AGM-like postulates action theory contraction. finish state
new postulate:
Preservation Modularity: modular, every 0
modular.
Changing modular theory make non-modular. standard
postulate, think since good property modularity preserved
across changing action theory. so, means whether theory modular
checked one need care future
evolution action theory, i.e., changes made it. operators
satisfy postulate proof given Appendix B.
one may naturally asks whether get characterization result traditional AGM sense, i.e., whether contraction operator satisfying versions
postulates one three contraction operations. Unfortunately, good sense points
towards negative answer: might well operator satisfying postulates that, complying assumptions RAC community (Shanahan,
1997), necessarily one operators defined Section 3 (cf. discussion
general formula contraction Section 10). witness, consider example operator
also modifies worlds contracting effect laws. supports one contentions
present work, viz. classical belief change cannot fully transposed action
theories expected give exactly kind outcome. Similar negative results
also found revision DL ontologies (Flouris, Plexousakis, & Antoniou, 2004)
contraction Horn theories (Booth, Meyer, & Varzinczak, 2009).
225

fiVarzinczak

8. Semantics Action Theory Revision
far analyzed case contraction: knowledge engineer realizes
theory strong therefore weakened. Let us take look
way round, i.e., theory (possibly) liberal agent discovers new laws
world added beliefs, amounts strengthening them.
Suppose action theory scenario example initially stated follows:


coffee hot, token hbuyi>,


coffee [buy]coffee, token [buy],
T=


coffee [buy]coffee, hot [buy]hot
canonical model theory shown Figure 15.
w1
t, c, h
b

b

w2

:

b

w3

b

t, c, h

t, c, h

b
b

w5
t, c, h

w6

w4
t, c, h

t, c, h

Figure 15: Canonical model new initial action domain description.
Looking model Figure 15 see that, example, agent know
loses token every time buys coffee machine. new law
incorporate knowledge base stage action theory evolution.
Contrary contraction, want negation law become satisfiable,
revision want make new law valid. means one eliminate cases
satisfying negation. depicts duality revision contraction: whereas
latter one invalidates formula making negation satisfiable, former one
makes formula valid forcing negation unsatisfiable prior adding new
law theory.
idea behind semantics revision follows: initially set models
given formula (potentially) valid, i.e., (possibly) true
every model M. result want models . Adding -models
help. Moreover, adding models makes us lose laws: corresponding resulting
theory would liberal.
One solution amounts deleting models -models.
course removing solve problem, must delete every
model. that, resulting models models . (This corresponds theory
expansion, resulting theory satisfiable.) However, contains model
, end . Consequence: resulting theory inconsistent. (This
main revision problem.) case solution substitute model
226

fiOn Action Theory Change

nearest modification M? makes true. lets us keep close possible
original models had. But, one model several minimal
(incomparable) modifications validating ? case shall consider them.
result also list models M? , models .
defining revision sets models, present modifications (individual) models are.
8.1 Revising Model Static Law
Suppose coffee deliverer agent discovers hot drink served
machine coffee. case, might want revise beliefs new static
law coffee hot: cannot hold hot drink coffee.
Considering model depicted Figure 15, one see Boolean formula
coffee hot satisfiable (there world model holds). Since
want case, first step remove worlds coffee hot
true. second step guarantee remaining worlds (if any) satisfy new
static law. issue largely addressed literature propositional belief
base revision update (Gardenfors, 1988; Winslett, 1988; Katsuno & Mendelzon, 1992;
Herzig & Rifi, 1999). achieve semantics similar classical
revision operators: basically one change set possible valuations, removing
adding worlds.
example, removing possible worlds {t, c, h} {t, c, h} would job
(there need add new valuations since new incoming law already satisfied
least one world original model, therefore resulting set worlds non-empty).
delicate point removing worlds may consequence loss
executability laws: example, transition world w
say {t, c, h}, removing latter model would make action
concern longer executable w, transition labeled action leaving
it. semantic point view, intuitive: state world
could move longer possible, transition state anymore.
Therefore, transition one had, natural lose it.
Similarly, one could ask accessibility relation new worlds
added, i.e., expansion possible. Following discussion Section 3.3,
prefer add new transitions systematically accessibility relation. Hence shall
postpone correction executability laws, needed. approach may debatable,
information hand, safest way changing static laws. (See
also discussion Sections 3.3 4.2.)
semantics revision one model static law follows:
Definition 8.1 Let = hW, Ri. 0 = hW0 , R0 M? if:
W0 = (W \ val()) W , W val();
R0 R.
227

fiVarzinczak

M0

Clearly unless |=
, |= 0 M? . minimal models
CPL
resulting revising model closest respect :
Definition 8.2 Let model static law. revise(M , ) =



min{M? , }.

example model Figure 15, revise(M , coffee hot) singleton
{M 0 }, 0 shown Figure 16.
w1
t, c, h
b
w2

M0 :

b

t, c, h
b
b

w5
t, c, h

w4
t, c, h

Figure 16: Model resulting revising model Figure 15 coffee hot.

8.2 Revising Model Effect Law
Let us suppose agent eventually discovers buying coffee
keep token anymore. (That design mistake agent still possesses
token even ordering coffee machine). means theory
revised way new effect law token [buy]token holds. Looking
model Figure 15, amounts guaranteeing formula token hbuyitoken
satisfiable none worlds. that, look worlds satisfying
formula (if any) either (i) make token false worlds; (ii) make
hbuyitoken false them. chose first option, essentially flip truth
value literal token respective worlds, changes set valuations
model. chose latter, basically remove buy-arrows leading token-worlds.
case, change accessibility relation made.
example, possible worlds {token, coffee, hot}, {token, coffee, hot}
{token, coffee, hot} satisfy token hbuyitoken change.
Flipping token worlds token would job, would also
consequence introduction new static law: token would valid, i.e.,
agent never token! Another issue approach making token true
everywhere, new incoming law token [buy]token trivially true resulting
model, mean execution action buy token-world
token one. defeats purpose changing action theory basis
observed every execution action consideration lead
token-contexts.
One contentions present work changing action laws never
side effect change static laws (cf. Sections 3 4). Given special status (Shanahan, 1997), change explicitly required. case, world
228

fiOn Action Theory Change

satisfying token hbuyitoken changed hbuyitoken longer true it.
example, remove transitions ({token, coffee, hot}, {token, coffee, hot}),
({token, coffee, hot}, {token, coffee, hot}) ({token, coffee, hot}, {token, coffee, hot}).
semantics one model revision case new effect law is:
?
Definition 8.3 Let = hW, Ri. 0 = hW0 , R0 M[a]
if:

W0 = W;
R0 R;


(w, w0 ) R \ R0 , |=w ;
M0

|= [a].
minimal models resulting revision model new effect law
closest respect order models :
Definition 8.4 Let model [a] effect law. revise(M , [a]) =

?
min{M[a]
, }.
Taking shown Figure 15, revise(M , token [buy]token)
singleton {M 0 } (Figure 17).
w1
t, c, h
b

b
w3

w2

M0 :

t, c, h

t, c, h
b

w5

w6

w4
t, c, h

t, c, h

t, c, h

Figure 17: Model resulting revising model Figure 15 new effect law
token [buy]token.

8.3 Revising Model Executability Law
Let us suppose stage decided grant free coffee everybody.
Faced information, agent revise laws reflect fact buy
also executed token-contexts: token hbuyi> new executability law (and
therefore hbuyi> new models agents beliefs).
Considering model Figure 15, observe (token hbuyi>)
satisfiable . means must throw token [buy] away ensure
new formula becomes true new model, i.e., satisfied worlds.
229

fiVarzinczak

remove token [buy] look worlds satisfying modify
longer satisfy formula. Given worlds {token, coffee, hot}
{token, coffee, hot}, two options: change interpretation token add new
transitions leaving worlds. question arises choice drastic:
change world transition ? Again, think changing worlds content
(the valuation) drastic, existence world foreseen static
law hence assumed is, unless enough information supporting
contrary, case explicitly change static laws (see above). Moreover,
changing truth value token worlds would trivialize new incoming law
token hbuyi> new model, defeating purpose guaranteeing existence
buy-transition token-context. Therefore shall add new buy-arrow
{token, coffee, hot} {token, coffee, hot}.
agreed that, issue is: worlds new transitions
directed to? Recalling reasoning developed Section 3.2, order comply
minimal change, new transitions shall directed worlds relevant targets
token-worlds question. example, {token, coffee, hot}
relevant target world here: two token-worlds violate effect coffee buy,
three token-worlds would make us violate frame axiom token [buy]token.
semantics one model revision new executability law follows:
?
Definition 8.5 Let = hW, Ri. 0 = hW0 , R0 Mhai>
if:

W0 = W;
R R0 ;
(w, w0 ) R0 \ R, w0 RelTarget(w, [a], , M);
M0

|= hai>.
minimal models resulting revising model new executability law
closest respect :
Definition
model hai> executability law. revise(M ,
8.6 Let
?
hai>) = min{Mhai>
, }.
running example, revise(M , token hbuyi>) singleton {M 0 }, 0
depicted Figure 18.
example, observe single relevant target world get
single model result revision.
8.4 Revising Sets Models
seen revision single models means. needed
expansion new law possible due inconsistency. give unified
definition revision set models new law :
230

fiOn Action Theory Change

b
w1
t, c, h
b

b

b

w2

M0 :

b

t, c, h
b

t, c, h

t, c, h

b
b

w5

b
w3

w6

w4
t, c, h

t, c, h

Figure 18: Result revising Figure 15 new executability law token hbuyi>.

Definition 8.7 Let set models law.
(


\ {M : 6|= }, |= ;
?
=

revise(M , ), otherwise.
Observe Definition 8.7 comprises expansion revision: first one, simple
addition new law gives satisfiable theory; latter deeper change needed
get rid inconsistency.

9. Related Work
best knowledge, first work updating action domain description
Li Pereira (1996) narrative-based action description language (Gelfond &
Lifschitz, 1993). Contrary us, however, mainly investigate problem updating
narrative new observed facts (possibly) occurrences actions explain
facts. amounts updating given state/configuration world (in terms,
true possible world) focusing models narrative
actions took place (in terms, models action theory particular sequence
action executions). Clearly models action laws remain same.
Baral Lobo (1997) introduce extensions action languages allow
causal laws stated defeasible. work similar also allow
weakening laws: setting, effect propositions replaced call
defeasible (weakened versions of) effect propositions. approach different
way executability laws dealt with. executability laws explicit
also able contract them. feature important qualification problem
considered: may always discover contexts preclude execution given action
(cf. Introduction).
Liberatore (2000) proposes framework reasoning actions possible express given semantics belief update, like Winsletts (1988) Katsuno
Mendelzons (1992). means formalism, essentially action description lan231

fiVarzinczak

guage, used describe updates (the change propositions one state
world another) expressing laws action theory.
main difference Liberatores work (2000) Li Pereiras (1996)
that, despite concerned, least priori, changing action laws, Liberatores
framework allows abductively introducing action theory new effect propositions
(effect laws, terms) consistently explain occurrence event.
work Eiter et al. (2005) similar also propose framework
oriented updating action laws. mainly investigate case e.g.
new effect law added description (and true models
modified theory). problem dual contraction closer definition
revision (cf. Section 8).
Eiter et al.s framework (2005), action theories described variant narrativebased action description language. Like present work, semantics also terms
transition systems, transitions (action occurrences) linking states (configurations
world). Contrary us, however, minimality condition outcome
update terms inclusion sets laws, means approach syntax
oriented extent.
setting, update action theory seen composed two pieces,
Tu Tm , Tu stands part supposed change Tm contains
laws may modified. terms, contracting static law would
Tm = Xa ; contracting executability law Tm = Xa ; contracting
effects laws Tm = Ea . difference approach always clear
laws change given type contraction, therefore Tu Tm need
explicitly specified prior update.
approach described constraint-based update,
theory change carried relative constraints (a set laws want hold
result). framework, example, changes action laws relative
set static laws (and concentrate models val(S )
worlds). changing law, want keep set states. difference
respect Eiter et al.s (2005) approach also possible update theory
relatively e.g. executability laws: expanding new effect law, one may want
constrain change action concern guaranteed executable
result.7 shown referred work, may require withdrawal static
law. Hence, Eiter et al.s framework, static laws status ours.
Herzig et al. (2006) define method action theory contraction that, despite
similarity current work common underlying motivations, limited
present constructions.
First, referred approach get minimal change. example,
referred work operator contracting executability laws resulting
theory modified set executability laws given
Xa = {(i ) hai> : hai> Xa }
7. could simulate approach two successive modifications T: first adding effect
law executability law (cf. Section 8).

232

fiOn Action Theory Change

which, according semantics, gives theories among whose models resulting
removing transitions -worlds. similar comment made respect
contraction effect laws.
Second, Herzig et al.s (2006) contraction method satisfy postulates
action theory change addressed Section 7. Besides satisfying
monotonicity postulate, satisfy preservation one. witness, suppose
language one atom p, model depicted Figure 19.

w1

:





w2
p

p




w1

:
0



w2
p

p


Figure 19: Counter-example preservation method Herzig et al. (2006).




|= p [a]p 6|= [a]p. contraction operator defined
removing [a]p yields model 0 Figure 19 R0a = WW.
M0

6|= p [a]p, i.e., effect law p [a]p preserved.
Finally, another work related Zhang Ding (2008). Like
ours, approach also giving semantic characterization basic operations
changing Kripke models. Contrary us however, focus model checking,
entailment. Despite definition use operations essence similar
(modifications set possible worlds accessibility relation), work
concerned mainly modifications single model, sets models
do, hence provide operations changing action laws.
that, approach directly comparable ours, since interested
entailment-based revision.

10. Concluding Remarks
work addressed problem changing action domain description
reasoning actions, problem sufficiently investigated literature far.
seen intuitions behind kind theory modification given
semantics action theory change terms distances models captures
notion minimal change. given algorithms contract formula theory
terminate correct respect semantics (Corollary 6.1).
shown importance modularity notion result others.
also extended Varzinczaks investigations (2008) defining semantics
action theory revision based minimal modifications models. corresponding
revision algorithms, reader referred work Varzinczak (2009). One
ongoing research topics assessing revision operators behavior respect
appropriate versions AGM postulates revision (Alchourron et al., 1985)
links contraction counterpart.
algorithms provide set tools used knowledge engineer
interactive possibly iterative way modify action theory. tools guaranteed
233

fiVarzinczak

perform minimal change assisting knowledge engineer implementing
desired modifications. give set options knowledge engineer
decide one line intuitions.
Given action theory change single step operation, knowledge engineer
expected make use contraction/revision operators make series modifications
eventually give fine-grained theory entailing contracted laws entailing
new learned laws domain.
sake presentation, abstracted frame ramification
problems. However definitions could stated formalism suitable solution them, like Castilho et al.s approaches (1999, 2002). regards
qualification problem, ignored here: contracting wrong executability laws
approach towards solution. Indeed, given difficulty stating sufficient conditions
executability action, knowledge engineer writes lets
theory evolve via subsequent revisions.
possible criticism approach developed concerns cautiousness
operator contracting static laws: prefer lose executability laws rather
induce lose effect laws. behavior could make operators interpreted
incoherent. pointed nevertheless line largely accepted
assumptions RAC community, moreover shown impossibility
non-cautious static law contraction operator complies coherent
operators.
Indeed one purposes present work shed light fundamental
differences belief change action domain descriptions logical theories
general. Classical belief change cannot fully transplanted action theories,
shown (cf. Sections 3.2, 4.2, 5.3 8.3).
particular, looking postulates classical belief change (or versions thereof)
one sees enough fully characterize operators action theory change.
achieved fundamental assumptions reasoning actions
extensively used throughout work somehow compiled postulates
supplementing classical ones. immediately clear new postulates
would look like, interesting thread investigation worth pursuing.
might also argued semantic operations respect principle
categorical matching, given input output different sorts objects, viz.
set models set sets models (cf. Definitions 3.3, 3.7 3.10). easy see,
however, semantic constructions could defined way
M0
corresponds result one contraction operator. choice defining
result operation set possible outputs driven definition
algorithms, theory (corresponding set models) given input
output set theories (hence corresponding set set models).
Although semantic operators redefined satisfy principle categorical matching, immediate algorithms (they would nondeterministic). Therefore preferred keep balance semantic
syntactic definitions see clearly direct correspondence.
234

fiOn Action Theory Change

One contentions sticking modular theories (and hence canonical models) big deal: use existent algorithms literature (Herzig &
Varzinczak, 2007) ensure action theory characterized canonical models.
seen modularity, operators satisfy postulates contraction: Modularity one sufficient conditions Success Theorem 7.1.
also sufficient condition Theorem 7.2, and, shown Theorem 7.3, sufficient
condition Recovery. Finally also sufficient condition Disjunctive Rule
hold, shown preserved contraction operators (cf. last paragraph
Section 7.2, proof Appendix B). Preservation modularity important result since
means checked/ensured lifetime action
theory. results support thesis modularity notion fruitful.
forcing formulas explicitly stated respective modules (and thus possibly
making inferable independently different ways), modularity intuitively could
seen diminish elaboration tolerance (McCarthy, 1998). instance, contracting
Boolean formula non-modular theory, seems reasonable expect change
set static laws , theory modular surely forces changing module.
difficult, however, conceive non-modular theories contraction formula
may demand change well. example, suppose = {1 2 } action
theory whose dynamic part (implicitly) infer 2 . case, contracting 1
keeping 2 would necessarily ask change .
point nevertheless cases (modular non-modular) extra work
changing modules stays mechanical level, i.e., algorithms carry
modification, augment significant way amount work
knowledge engineer expected do.
Contrary trend belief change community, focus either belief
bases belief sets (Hansson, 1999), method proposed hybrid one (Delgrande,
2009). one hand, semantics plays crucial role notion minimal change
studied. hand, deal domain descriptions reasoning
actions, sets laws specific types. top that, modularity property (a
syntactical one) fundamental main results.
Following lines, another issue drives future research subject
contract laws Kn -formula. defined, order application
operators matter final result: contract [a] theory T,
result may contracting [a] first removing .
problem would appear general framework formula could
contracted: removing ( [a]) give result ( [a]) .
principle syntax independence (Dalal, 1988).
Related question revision definitions relate contraction
operators. known Levi identity (1977), ? =
{}, general
hold action laws (effect executability ones). reason
contraction operator effect executability law. Indeed
general contraction problem non-classical logics: contraction general
formula (like above) still open problem belief change area. insights
direction given revision definitions, make false every
possible world Kripke model.
235

fiVarzinczak

Definitions 3.1, 3.5 3.8 appear important better understanding problem
contracting general formulas: basically set modifications perform given
model order force falsify general formula comprise removal/addition
transitions/worlds. definition general revision/contraction method benefit
constructions.
Furthermore, given well-known connection multimodal logics Description Logics (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003), believe
definitions may also contribute ontology evolution debugging specific
families DLs.

Acknowledgments
Parts work done authors stay Institut de Recherche en
Informatique de Toulouse (IRIT), France, visit National ICT Australia
(NICTA), Sydney.
author grateful anonymous referees constructive useful remarks, helped improving quality work. paper also benefited
discussions Andreas Herzig Laurent Perrussel.
Special thanks colleagues Meraka Institute Arina Britz, Ken Halland,
Johannes Heidema Tommie Meyer invaluable comments suggestions
earlier versions article.

Appendix A. Proof Theorem 6.2


Let modular, law. M0
|= every M,
M0

0
0
0
0
|= every .

give proof theorem, need following lemma (cf.
Monotonicity Postulate Section 7.2):
Lemma A.1 |=
0.
K
n

Proof: Let action theory, let 0
, given law . going
analyze case.
Let form hai>, Fml. 0
0 = (T \ Xa ) {(i ( )) hai> : hai> Xa }
V
V
IP(S ) = pi atm() pi pi atm() pi , atm().
pi

pi
/



Let = hW, Ri |= T. enough show model new


laws. every (i ( )) hai>, every w W, |=w ( ), |=
.
w


|=K hai>, |= hai>, Ra (w) 6= .
n



Therefore |= 0 .
236

fiOn Action Theory Change

Let form [a], , Fml. 0
(T \ Ea )
{(i ( )) [a]i : [a]i Ea }
0

0 = {(i ) [a](i ) : [a]i Ea }


` L, forVsome L Lit s.t.


( `) [a]( `) : 6` ( 0 `L `) , ` 0


6`Kn ( `) [a]`

Ea =



,
1in (Ea )i ,

IP(S ), =

V
pi atm()
pi

pi

V
pi atm()
pi
/

pi ,

atm(), 0 IP(S ).

Let = hW, Ri |= T. enough show model

added laws. Given (i ( )) [a]i , every w W, |=w ( ),






|=w . |=K [a]i , |= [a]i , |= 0 every w0 W
w
n
(w, w0 ) Ra .


(i ) [a](i 0 ), every w W, |=w , |= 0
w
every w0 W (w, w0 ) Ra .


Now, given ( `) [a]( `), every w W, |=w `, |=
,
w






|=
. Since |=
[a], |= [a], |= 0 every w0 W
Kn
w
w
0
(w, w ) Ra .

Therefore |= 0 .
Let propositional . 0
((T \ ) ) \ Xa
0 = {(i ) hai> : hai> Xa }
{ [a]}
.

Let = hW, Ri |= T. suffices show satisfies added laws.
Since assume behaves like classical contraction operator, like e.g. Katsuno


Mendelzons (1992), |=
, then, |= , |= .
CPL






given (i ) hai>, every w W, |=w , |=
,
w

|= hai>, Ra (w) 6= .

Finally, [a], |= , trivially satisfies [a].

Therefore |= 0 .

2

Proof Theorem 6.2


M0


0
0
Let = {M : |= T}, M0
. show |=
0
0
every .

237

fiVarzinczak

M0

M0

definition, 0 M0 either |= 6|= .
6= ,
M0

M0

0
must 0
. |= T, Lemma A.1 |= done. Let us suppose
M0

6|= . analyze case.
Let form hai> Fml. 0 = hW0 , R0 i, W0 = W,


0
(w, w0 ) Ra }, M.
R0 = R \ R
, Ra = {(w, w ) : |=
w
M0

M0

Let u W0 6|=
hai>, i.e., |=
R0a (u) = .
u
u
V
0
u , must v base(, WV
) v u. Let = `v `. Clearly
prime implicant . Let also = `u\v `, consider
0 = (T \ Xa ) {(i ( )) hai> : hai> Xa }
(Clearly, 0 theory produced Algorithm 1.)
enough show 0 model new added laws. Given (i (A ))


M0

M0

, follows |=w .
( ), |=
hai> 0 , every w W0 , |=
w
w


|= hai>, w0 W w0 Ra (w). need show


0
0
(w, w0 ) R0a . 6|=
, R
, either w = u,
= , (w, w ) Ra . |=
w
w
M0

M0

|=u conclude |=u (i ( )) hai>, w 6= u must



(w, w0 ) R0a , otherwise
Ra R(R \ Sa ) R(R \ Ra ),
00

00
0
00 = hW0 , R \
6|= hai> , contradiction
M0

M0

0 minimal respect . Thus (w, w0 ) R0a , |=
hai>. Hence |= 0 .
w
let form [a], , Boolean. 0 = hW0 , R0 i,
W0 = W, R0 = R R,
,

R,
= {(w, w0 ) : w0 RelTarget(w, [a], , M)}

= hW, Ri M.
M0

M0

[a]. u0 W0 (u, u0 ) R0a
Let u W0 6|=
u

6|=0 . u , v base(, W0 ) v u, u0 , must
u
V
V
V
v 0 base(, W0 ) v 0 u0 . Let = `v `, = `u\v `, 0 = `v0 `.
Clearly (resp. 0 ) prime implicant (resp. ).

let Ea = 1in (Ea, )i let theory
(T \ Ea )
{(i ( )) [a]i : [a]i Ea }
0

0 = {(i ) [a](i ) : [a]i Ea }


` L, forVsome L Lit s.t.


( `) [a]( `) : 6` ( 0 `L `) , ` 0


6`Kn ( `) [a]`

(Clearly, 0 theory produced Algorithm 2.)
238

fiOn Action Theory Change

order show 0 model 0 , enough show model
M0
added laws. Given (i (A )) [a]i 0 , every w W0 , |=w (A ),
M0







|=
, |=w . |= [a]i , |= 0 w0 W (w, w0 ) Ra .
w


w

= , R0a (w) = Ra (w).
need show R0a (w) = Ra (w). 6|=w , R,



M0

M0

|=
, either w = u, |=u conclude |=
(i ( )) [a]i ,
w
u
,
w 6= u, must Ra
= , otherwise would S,
R,



,
,
0
,
00
R(R Sa ) R(R Ra ), = hW , R Sa would
00

6|= [a] 00 0 , contradiction since 0 minimal respect .
M0

Hence R0a (w) = Ra (w), |= 0 w0 (w, w0 ) R0a .
w

M0

M0

Now, given (i ) [a](i 0 ), every w W0 , |=w ,






|=w , |=
. Because, |= [a]i , |= 0 w0 W
w
w

M0

(w, w0 ) Ra , |= 0 every w0 W0 (w, w0 ) R0a \ Ra, . Now, given
w

M0

, |= 0 0 , result follows.
(w, w0 ) R,

w

M0

M0

Now, ( `) [a]( `), every w W0 , |=
`,
w






|=w , |=
. |= [a], |= 0 every w0 W
w
(w, w0 )

w

M0

Ra , |= 0
w

M0

w0

0

W (w, w0 ) R0a \ R,
. remains


show |= 0 ` every w0 W0 (w, w0 ) R,
. Since 0 minimal,

w

M0

M0

enough show |=0 ` every ` Lit |=
`. ` 0 , result
u
u

M0

follows. Otherwise, suppose 6|=0 `.
u

either ` 0 , 0 ` unsatisfiable, case Algorithm 2
put law ( `) [a]( `) 0 , contradiction;
` u0 \ v 0 . case, valuation u00 = (u0 \ {`}) {`}
0
00
0
u00 6 .
: `i u00 }
iV
V must u W , otherwise L = {`
0
0
|=K ( `i L0 `i ) , and, modular, |=CPL ( `i L0 `i ) ,
n
Algorithm 2 put law ( `) [a]( `) 0 , contradiction.
u00 W0 , moreover u00
/ R,
(u), otherwise 0 minimal.

,
u00 \ u u0 \ u, reason u00
/ Ra (u) `0 u u00
Mi V
0
|= `j u `j [a]` every Mi `0
/ v0 v0 base(, W 0 )
V
v0 u00 . Clearly `0 = `, `
/ 0 , |= `j u `j [a]`
every Mi M. |=
( `) [a]`, Algorithm 2
Kn
put law ( `) [a]( `) 0 , contradiction.
M0

Hence |= 0 ` every w0 W0 (w, w0 ) R0a .
w

M0

Putting results together, get |= 0 .
Let propositional . 0 = hW0 , R0 i, W W0 , R0 = R,
minimal respect , i.e., W0 minimal superset W u W0
239

fiVarzinczak

u 6 . assumed syntactical classical contraction operator
sound complete respect semantics moreover minimal, must
M0
W0 = val(S ). Therefore |= .
R0 = R, every effect law remains true 0 .
Now, let
((T \ ) ) \ Xa
0 = {(i ) hai> : hai> Xa }
{ [a]}
(Clearly, 0 theory produced Algorithm 3.)
M0

every (i ) hai> 0 every w W0 , |=
, Ra (w) 6= ,
w
M0



hai>. Given [a], every w W0 , |=
, w = u,
|=
w
w
Ra (w) = .
M0

2

Putting results together, |= 0 .

Appendix B. Proof Theorem 6.3
M0


0
0
0
Let modular, law, 0
. |= ,


0 M0 |= every M.
order prove result, first need show four important lemmas.
Lemma B.1 Let law. modular, every 0
modular.
0
Proof: Let nonclassical, suppose 0
modular.
0
0
0
0
0
0
Fml |=K 6|=
, set static
CPL
n
0
0
laws . Lemma A.1, |=
, |=
0 . nonclassical,
Kn
Kn
0
= . Thus 6|=CPL 0 , therefore modular.

Let Fml.
((T \ ) ) \ Xa
0 = {(i ) hai> : hai> Xa }
{ [a]}
.
Assume modular, let 0 Fml 0 |=
0 6|=
0 .
Kn
CPL
6|=
0 , v val(S ) v 6 0 . v val(S ), 6|=CPL 0 ,
CPL
modular, 6|=K 0 . Lemma A.1, |=K 0 , 0 6|=K 0 , contradiction.
n
n
n
Hence v
/ val(S ). Moreover, must v 6 , otherwise worked expected.

Let = hW, Ri |= 0 . (We extend another model 0 .) Let
0 = hW0 , R0 W0 = W {v} R0 = R. show 0 model
M0

0 , suffices show v satisfies every law 0 . v val(S ), |=
. Given
v
240

fiOn Action Theory Change

M0

[a] 0 , v 6 R0a (v) = , |=v [a]. Now, every [a]i 0 ,
M0

M0

|=v , trivially |=0 every v0 (v, v0 ) R0a . Finally, given
v

M0

(i ) hai> 0 , v 6 , formula trivially holds v. Hence |= 0 ,
M0

v W0 6|=
0 , 0 6|=K 0 , contradiction. Hence 0 Fml
v
n
0 , 0 modular.
0 |=K 0 , |=

2
CPL
n

Lemma B.2 Mcan = hWcan , Rcan model T, every = hW, Ri

|= minimal (with respect set inclusion) extension R0 Rcan \ R
0 = hval(S ), R R0 model T.


Proof: Let Mcan = hWcan , Rcan model T, let = hW, Ri |= T.
M0

Consider 0 = hval(S ), Ri. |= T, R0 = Rcan \ R minimal. Suppose
M0

M0

6|= T. extend 0 model minimal extension . 6|= T,
M0

M0

v val(S ) \ W 6|=
T. 6|=v .
v
Fml, v Wcan , Mcan model T. form [a],
, Fml, v0 val(S ) (v, v0 ) Ra v0 6 , contradiction since
M0

Ra (v) = . Let form hai> Fml. |=
. v Wcan ,
v
Mcan

Mcan

6|=v hai>, 6|= T. Hence, Rcana (v) 6= . Thus taking (v, v0 ) Rcana gives
us minimal R0 = {(v, v0 )} 00 = hval(S ), R R0 model T.
2
Lemma B.3 Let modular, law. |=
every 0 =
K
0

hW,Ri

hval(S ), R |=

n

0

R R model .

Proof:


(): Straightforward, since |=
implies |= every |= T, particular
Kn
extensions model T.




(): Suppose 6|=
. = hW, Ri |= 6|= . modular,
Kn
canonical frame Mcan = hWcan , Rcan model T. Lemma B.2
minimal extension R0 R respect Rcan 0 = hval(S ), R R0 model


T. 6|= , w W 6|=w . propositional Fml
M0

effect law, extension 0 6|=w . form hai>,


|=
Ra (w) = . extension (u, v) R0
w
u val(S ) \ W, worlds W get new departing transition. Thus
M0

2

(R R0 )a (w) = , 6|=w .

0
0
0
Lemma B.4 Let modular, law, 0
. = hval(S ), R model


0 , = {M : = hval(S ), Ri |= T} 0 M0
M0
.
241

fiVarzinczak

M0

M0

Proof: Let 0 = hval(S 0 ), R0 |= 0 . |= T, result follows. Let us
M0

suppose 6|= T. analyze case.
Let form hai>, Fml. Let = {M : = hval(S ), Ri}.
Since hypothesis modular, Lemmas B.2 B.3 follows non-empty
contains models T.
Suppose 0 minimal model 0 , i.e., 00 00 0
M. 0 00 differ executability
inVa given -world,
V
viz. -context, IP(S ) = pi atm() pi pi atm() pi
pi

M0

00

pi
/

atm(). 6|= ( ) hai>, must |= ( ) hai>
00

|= T. Hence 0 minimal respect .
contracting executability laws, 0 = . Hence taking right R minimal

0


0
R
(w, w0 )
= hval(S ), Ri R = R \ Ra , Ra {(w, w ) :|=
w

Ra }, construct M0 = {M 0 } Mhai> .
Let form [a], , Fml. Let = {M : = hval(S ), Ri}.
Since hypothesis modular, Lemmas B.2 B.3 follows non-empty
contains models T.
claim 0 oneVtransition linking
V -world, viz. context
IP(S ) = pi atm() pi pi atm() pi , atm(),
pi

pi
/

0 -world, 0 IP(S ). proof follows: given ` Lit ` holds
-world
( `) [a]( `)
/ 0 , `
/ 0 |=
( `) [a]`.
Kn
world `-successors.
( `) [a]( `) 0 , every 0 -successor `-world.
successively applying reasoning ` holds -world,
end one 0 -successor.
00
Suppose 0 minimal model 0 , i.e., 00 |= 0
00 0 M. 0 00 differ effects
-world: 00 transition linking 0 -world.
00
00
|= (i ) [a]i , |= T. Therefore 0 minimal model 0
respect .
contracting effect laws, 0 = . Thus taking right R minimal R,


0
,
,
0
= hval(S ), Ri R = R Ra , Ra {(w, w ) :|=
w0
w

RelTarget(w, [a], , M)}, construct M0 = {M 0 } M[a] .
Let Fml. Since modular, Lemmas B.2 B.3

= hval(S ), Ri |= T. know val(S ) val(S ). [a] 0 ,
R0a (v) = every -world v added 0 . Hence, minimal, taking =
{M } gives us result.
2
242

fiOn Action Theory Change

Proof Theorem 6.3
hypothesis modular Lemma B.1, follows 0 modular,
too. 0 = hval(S 0 ), Ri model 0 , Lemma B.3. Lemma B.4
result follows.
2

References
Alchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet contraction revision functions. Journal Symbolic Logic, 50, 510
530.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2003).
Description Logic Handbook. Cambridge University Press.
Baral, C., & Lobo, J. (1997). Defeasible specifications action theories. Pollack, M.
(Ed.), Proceedings 15th International Joint Conference Artificial Intelligence
(IJCAI), pp. 14411446. Morgan Kaufmann Publishers.
Booth, R., Meyer, T., & Varzinczak, I. (2009). Next steps propositional Horn contraction. Boutilier, C. (Ed.), Proceedings 21st International Joint Conference
Artificial Intelligence (IJCAI), pp. 702707. AAAI Press.
Burger, I., & Heidema, J. (2002). Merging inference conjecture information. Synthese, 131 (2), 223258.
Castilho, M., Gasquet, O., & Herzig, A. (1999). Formalizing action change modal
logic I: frame problem. Journal Logic Computation, 9 (5), 701735.
Castilho, M., Herzig, A., & Varzinczak, I. (2002). depends context! decidable
logic actions plans based ternary dependence relation. 9th International
Workshop Nonmonotonic Reasoning (NMR).
Cholvy, L. (1999). Checking regulation consistency using SOL-resolution. Proceedings
7th International Conference AI Law, pp. 7379.
Dalal, M. (1988). Investigations theory knowledge base revision: preliminary report.
Smith, R., & Mitchell, T. (Eds.), Proceedings 7th National Conference
Artificial Intelligence (AAAI), pp. 475479. Morgan Kaufmann Publishers.
De Giacomo, G., & Lenzerini, M. (1995). PDL-based framework reasoning actions. Gori, M., & Soda, G. (Eds.), Proceedings 4th Congress Italian Association Artificial Intelligence (IA*AI), No. 992 LNAI, pp. 103114.
Springer-Verlag.
Delgrande, J. (2009). Personal communication. Commonsense09, Toronto.
Demolombe, R., Herzig, A., & Varzinczak, I. (2003). Regression modal logic. Journal
Applied Non-Classical Logic, 13 (2), 165185.
Eiter, T., Erdem, E., Fink, M., & Senko, J. (2005). Updating action domain descriptions.
Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings 19th International Joint
Conference Artificial Intelligence (IJCAI), pp. 418423. Morgan Kaufmann Publishers.
243

fiVarzinczak

Finger, J. (1987). Exploiting constraints design synthesis. Ph.D. thesis, Stanford University.
Flouris, G., Plexousakis, D., & Antoniou, G. (2004). Generalizing AGM postulates.
10th International Workshop Nonmonotonic Reasoning (NMR).
Fuhrmann, A. (1989). modal logic theory change. Logic Theory Change,
pp. 259281.
Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.
MIT Press.
Gelfond, M., & Lifschitz, V. (1993). Representing action change logic programs.
Journal Logic Programming, 17 (2/3&4), 301321.
Giunchiglia, E., Kartha, G., & Lifschitz, V. (1997). Representing action: indeterminacy
ramifications. Artificial Intelligence, 95 (2), 409438.
Hamming, R. (1950). Error detecting error correcting codes. Bell System Technical
Journal, 26 (2), 147160.
Hansson, S. (1994). Kernel contraction. Journal Symbolic Logic, 59 (3), 845859.
Hansson, S. (1999). Textbook Belief Dynamics: Theory Change Database Updating.
Kluwer Academic Publishers.
Harel, D., Tiuryn, J., & Kozen, D. (2000). Dynamic Logic. MIT Press.
Herzig, A., Perrussel, L., & Varzinczak, I. (2006). Elaborating domain descriptions.
Brewka, G., Coradeschi, S., Perini, A., & Traverso, P. (Eds.), Proceedings 17th
European Conference Artificial Intelligence (ECAI), pp. 397401. IOS Press.
Herzig, A., & Rifi, O. (1999). Propositional belief base update minimal change. Artificial
Intelligence, 115 (1), 107138.
Herzig, A., & Varzinczak, I. (2004). Domain descriptions modular. Lopez de
Mantaras, R., & Saitta, L. (Eds.), Proceedings 16th European Conference
Artificial Intelligence (ECAI), pp. 348352. IOS Press.
Herzig, A., & Varzinczak, I. (2005a). Cohesion, coupling meta-theory actions.
Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings 19th International Joint
Conference Artificial Intelligence (IJCAI), pp. 442447. Morgan Kaufmann Publishers.
Herzig, A., & Varzinczak, I. (2005b). modularity theories. Schmidt, R., PrattHartmann, I., Reynolds, M., & Wansing, H. (Eds.), Advances Modal Logic, Vol. 5,
pp. 93109. Kings College Publications.
Herzig, A., & Varzinczak, I. (2006). modularity approach fragment ALC.
Fisher, M., van der Hoek, W., Konev, B., & Lisitsa, A. (Eds.), Proceedings 10th
European Conference Logics Artificial Intelligence (JELIA), No. 4160 LNAI,
pp. 216228. Springer-Verlag.
Herzig, A., & Varzinczak, I. (2007). Metatheory actions: beyond consistency. Artificial
Intelligence, 171, 951984.
244

fiOn Action Theory Change

Jin, Y., & Thielscher, M. (2005). Iterated belief revision, revised. Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings 19th International Joint Conference Artificial
Intelligence (IJCAI), pp. 478483. Morgan Kaufmann Publishers.
Katsuno, H., & Mendelzon, A. (1992). difference updating knowledge
base revising it. Gardenfors, P. (Ed.), Belief revision, pp. 183203. Cambridge
University Press.
Kracht, M., & Wolter, F. (1991). Properties independently axiomatizable bimodal logics.
Journal Symbolic Logic, 56 (4), 14691485.
Levi, I. (1977). Subjunctives, dispositions chances. Synthese, 34, 423455.
Li, R., & Pereira, L. (1996). believed explained. Shrobe, H., & Senator, T. (Eds.), Proceedings 13th National Conference Artificial Intelligence
(AAAI), pp. 550555. AAAI Press/MIT Press.
Liberatore, P. (2000). framework belief update. Proceedings 7th European
Conference Logics Artificial Intelligence (JELIA), pp. 361375.
Makinson, D. (2007). Friendliness sympathy logic. Beziau, J.-Y. (Ed.), Logica
Universalis, Vol. 2, pp. 195224. Springer-Verlag.
Marquis, P. (2000). Consequence finding algorithms. Gabbay, D., & Smets, P. (Eds.),
Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 5:
Algorithms Uncertainty Defeasible Reasoning, edited J. Kohlas S.
Moral, chap. 2, pp. 41145. Kluwer Academic Publishers.
McCarthy, J. (1977). Epistemological problems artificial intelligence. Sridharan, N.
(Ed.), Proceedings 5th International Joint Conference Artificial Intelligence
(IJCAI), pp. 10381044. Morgan Kaufmann Publishers.
McCarthy, J. (1998). Elaboration tolerance. Proceedings 4th International Symposium Logical Formalizations Commonsense Reasoning.
McCarthy, J., & Hayes, P. (1969). philosophical problems standpoint
artificial intelligence. Meltzer, B., & Mitchie, D. (Eds.), Machine Intelligence,
Vol. 4, pp. 463502. Edinburgh University Press.
Nebel, B. (1989). knowledge level analysis belief revision. Brachman, R., Levesque,
H., & Reiter, R. (Eds.), Proceedings 1st International Conference Principles
Knowledge Representation Reasoning (KR), pp. 301311. Morgan Kaufmann
Publishers.
Parikh, R. (1999). Beliefs, belief revision, splitting languages. Moss, L. (Ed.),
Logic, Language Computation, No. 96 CSLI Lecture Notes, pp. 266278. CSLI
Publications.
Popkorn, S. (1994). First Steps Modal Logic. Cambridge University Press.
Quine, W. (1952). problem simplifying truth functions. American Mathematical
Monthly, 59, 521531.
Quine, W. (1962). Paradox. Scientific American, 1, 8496.
245

fiVarzinczak

Reiter, R. (2001). Knowledge Action: Logical Foundations Specifying Implementing Dynamical Systems. MIT Press.
Schlechta, K. (2004). Coherent Systems. Studies Logic Practical Reasoning 2.
Elsevier.
Shanahan, M. (1997). Solving frame problem: mathematical investigation common sense law inertia. MIT Press.
Shapiro, S., Lesperance, Y., & Levesque, H. (2005). Goal change. Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings 19th International Joint Conference Artificial
Intelligence (IJCAI), pp. 582588. Morgan Kaufmann Publishers.
Shapiro, S., Pagnucco, M., Lesperance, Y., & Levesque, H. (2000). Iterated belief change
situation calculus. Cohn, T., Giunchiglia, F., & Selman, B. (Eds.), Proceedings
7th International Conference Principles Knowledge Representation
Reasoning (KR), pp. 527538. Morgan Kaufmann Publishers.
Thielscher, M. (1997). Ramification causality. Artificial Intelligence, 89 (12), 317364.
Thielscher, M. (2010). unifying action calculus. appear Artificial Intelligence.
Varzinczak, I. (2006). good domain description? Evaluating revising action
theories dynamic logic. Ph.D. thesis, Universite Paul Sabatier, Toulouse.
Varzinczak, I. (2008). Action theory contraction minimal change. Lang, J., &
Brewka, G. (Eds.), Proceedings 11th International Conference Principles
Knowledge Representation Reasoning (KR), pp. 651661. AAAI Press.
Varzinczak, I. (2009). revision action laws: algorithmic approach. IJCAI
Workshop Nonmonotonic Reasoning, Action Change (NRAC).
Winslett, M.-A. (1988). Reasoning action using possible models approach. Smith,
R., & Mitchell, T. (Eds.), Proceedings 7th National Conference Artificial
Intelligence (AAAI), pp. 8993. Morgan Kaufmann Publishers.
Zhang, D., Chopra, S., & Foo, N. (2002). Consistency action descriptions. Ishizuka,
M., & Sattar, A. (Eds.), Proceedings 7th Pacific Rim International Conference
Artificial Intelligence, No. 2417 LNCS, pp. 7079. Springer-Verlag.
Zhang, D., & Foo, N. (2001). EPDL: logic causal reasoning. Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI),
pp. 131138. Morgan Kaufmann Publishers.
Zhang, Y., & Ding, Y. (2008). CTL model update system modifications. Journal
Artificial Intelligence Research, 31, 113155.

246

fiJournal Artificial Intelligence Research 37 (2010) 479-525

Submitted 12/09; published 03/10

Multiattribute Auctions Based Generalized Additive
Independence
Yagil Engel

yagile@ie.technion.ac.il

Technion - Israel Institute Technology
Faculty Industrial Engineering & Management
Technion City, Haifa 32000, Israel

Michael P. Wellman

wellman@umich.edu

University Michigan
Division Computer Science & Engineering
2260 Hayward St, Ann Arbor, MI 48109-2121, USA

Abstract
develop multiattribute auctions accommodate generalized additive independent
(GAI) preferences. propose iterative auction mechanism maintains prices
potentially overlapping GAI clusters attributes, thus decreases elicitation computational burden, creates open competition among suppliers multidimensional
domain. significantly, auction guaranteed achieve surplus approximates optimal welfare small additive factor, reasonable equilibrium strategies
traders. main departure GAI auctions previous literature accommodate non-additive trader preferences, hence allowing traders condition evaluation
specific attributes value attributes. time, GAI structure
supports compact representation prices, enabling tractable auction process. perform simulation study, demonstrating quantifying significant efficiency advantage
expressive preference modeling. draw random GAI-structured utility functions
various internal structures, generate additive functions approximate GAI
utility, compare performance auctions using two representations.
find allowing traders express existing dependencies among attributes improves
economic efficiency multiattribute auctions.

1. Introduction
Multiattribute trading mechanisms extend traditional, price-only mechanisms facilitating negotiation set predefined attributes representing various non-price aspects
deal. Rather negotiate fully specified good service, multiattribute
mechanism delays commitment particular configurations extracts sufficient information traders preferences. example, companys procurement department may
run multiattribute auction select supplier hard drives. Supplier offers may
evaluated price offer, also features volume, RPM,
access time, latency, transfer rate, on. addition, suppliers may offer contracts
differing terms warranty, delivery time, service.
order account traders preferences, auction mechanism must extract evaluative information complex domain multidimensional configurations. Constructing
communicating complete preference specification pose severe burden even

c
!2010
AI Access Foundation. rights reserved.

fiEngel & Wellman

moderate number attributes, hence practical multiattribute auctions must either accommodate partial specifications, support compact expression preferences assuming
simplified form. far popular multiattribute form adopt simplest:
additive representation overall value linear combination values associated
attribute. example, several recent proposals iterative multiattribute auctions
(Beil & Wein, 2003; Bichler, 2001; David, Azoulay-Schwartz, & Kraus, 2002; Parkes &
Kalagnanam, 2005) require additive preference representations.
additivity reduces complexity preference specification exponentially (compared general discrete case), precludes expression interdependencies among
attributes. practice, however, interdependencies among natural attributes quite
common. example, hard-drive buyer may exhibit complementary preferences
volume access time (since performance effect salient much data involved), may view strong warranty good substitute high reliability ratings.
Similarly, sellers production characteristics easily violate additivity, example
decreasing access time technically difficult higher-capacity drives. cases
additive value function may able provide adequate approximation real
preferences.
hand, fully general models intractable, multiattribute preferences
typically exhibit structure. goal, therefore, identify subtler yet
widely applicable structured representations, exploit properties preferences
trading mechanisms.
propose iterative auction mechanism based flexible preference
structure. approach inspired design iterative multiattribute procurement
auction additive preferences, due Parkes Kalagnanam (2005) (PK). PK present
two auction designs: first (NLD) makes assumptions traders preferences,
lets sellers bid full multidimensional attribute space. NLD maintains
exponential price structure, suitable small domains. auction (AD)
assumes additive buyer valuation seller cost functions. collects sell bids per attribute
level single discount term. price configuration sum prices
chosen attribute levels minus discount.
auction propose also supports compact price spaces, albeit levels clusters attributes rather singletons. employ preference decomposition based
generalized additive independence (GAI), model flexible enough accommodate interdependencies exact degree accuracy desired, yet providing compact functional
form extent interdependence limited.
First, build direct, formally justified link preference statements priced
outcomes generalized additive decomposition willingness-to-pay (wtp) function.
laying infrastructure, employ representation tool development
multiattribute iterative auction mechanism allows traders express complex
preferences GAI format. study auctions allocational, computational,
practical properties. Next, present simulation study proposed auction mechanism, order practically evaluate economic computational properties GAI
auctions. simulate auctions using random GAI utility functions, including
based constrained preference structures often exhibited applications. simulations
let us quantify benefits modeling preferences accurately using GAI, comparison
480

fiGAI Auctions

using additive approximation. show circumstances, GAI auction
achieves significantly higher surplus auction uses additive approximation
preferences.
providing background multiattribute preferences multiattribute auctions
(Section 2), develop new multiattribute structures wtp functions, supporting generalized additive decompositions (Section 3). describe auction mechanism Section 4,
followed detailed example Section 5, study mechanisms allocational, computational, practical properties Section 6. present simulation framework
Section 7, discuss experimental results Section 8.

2. Background
section provide essential background multiattribute preferences (Sections 2.1
2.2) multiattribute auctions (Section 2.3).
2.1 Multiattribute Preferences Utility
Let denote space possible outcomes, ! preference relation (weak total order)
. Let = {a0 , . . . , }!be set attributes describing . attribute
domain D(a), ni=1 D(ai ). Capital letters denote subsets attributes, small
latin letters (with without numeric subscripts) denote specific attributes, X = A\X.
(and variations " ) indicate specific outcome . instantiation subset
attributes denoted using prime signs (as " ) numerical superscript (as 1 ).
particular, " projection instantiations . represent
instantiation subsets X, time use sequence instantiation symbols,
X 1 2 .
preference relation ! outcomes usually represented numerically value
function v() (Keeney & Raiffa, 1976).
Definition 1 (Value Function). v : % value function representing !
, " , v() v( " ) iff ! " .
Clearly, monotonic transformation v() also value function !.
many cases useful represent, beyond simple preference order outcomes,
notion strength preferences. value function expresses strength preferences
called cardinal value function. measurable value function well-established cardinal
pairs outcomes.
value framework posits existence preference order !
"
"
"
"

! ! , statement (, ) ! (, ) means strength
preference " greater equal " . Krantz, Luce, Suppes,
Tversky (1971) establish set axioms ensuring existence utility function

representing !.
Definition 2 (Measurable Value Function). measurable value function (MVF)
value function u : %, , " , , " , ! " ! " ,
following holds:
(, " ).
u( " ) u() u( " ) u() (, " ) !
(1)

481

fiEngel & Wellman

Hence order differences values u() correspond exactly order
preference differences. Note MVF also used value function representing
( "" , ) iff " ! "" , .
!, ( " , ) !
auction theory mechanism design, traders preferences usually represented
using quasi-linear value function, v(, p) = u()+p, p represents monetary
outcome.1 cardinal value function u() expresses strength preference,
difference u( " ) u( "" ) corresponds additional amount trader willing pay
" relative "" . example, " represents red Mercedes sunroof, ""
denotes blue Subaru sunroof, u( " ) u( "" ) strength preference
Mercedes configuration Subaru. Mercedes costs p" Subaru p"" ,
according v(, p) trader prefers Mercedes deal iff u( " ) u( "" ) p" p"" .
fact, u() easily shown MVF, preference differences correspond differences willingness-to-pay (Engel & Wellman, 2007). reason,
use MVF basis utility work, assume traders willingness-to-pay
(wtp) functions constitute MVF.
Reasoning full outcomes hard several ways. notably, difficult
humans compare outcomes many dimensions, complex machines store
analyze preferences number outcomes exponential number
attributes. therefore useful consider preferences joint product
A, considering rest attributes fixed predefined values.
order also often referred ceteris paribus preference orderone partial outcome
preferred another else equal.
Definition 3 (Conditional Preference). Partial outcome 2 conditionally preferred
"
"
"
partial outcome 1 given , 1 ! 2 . conditional preference order
"
"
"
given denoted !Y ! , hence 1 ! 2 abbreviated 1 !Y ! 2 .
general, conditional preferences may depend particular assignment chosen
rest attributes. precisely, 1 ! 2 , could still find 2 !! 1
""
"
,= . case, one needs maintain conditional preference
orders !Y ! !Y !! , hence general scheme might yield computational
benefits. Fortunately, many cases one identify subsets preference
reversal occur, preference order invariant instantiation
.
Definition 4 (Preferential Independence). preferential independent (PI) ,
"
""
written PI(Y, ), 1 2 , , , 1 !Y ! 2 iff 1 !Y !!
2.
First-order preferential independence (FOPI), independence single attribute
rest, natural assumption many domains. example, typical purchase decisions greater quantity higher quality desirable regardless assignments
attributes. Preferential independence higher order, however, requires invariance
tradeoffs among attributes respect variation others, stringent independence condition. MPI condition, defined below, global set
attributes A, requires possible subsets PI.
1. use term trader referring either buyers sellers.

482

fiGAI Auctions

Definition 5 (Mutual Preferential Independence). Attributes mutually preferential independent (MPI) iff A, P I(Y, ).
Preferential independence greatly simplify form v.
Theorem 1 (Debreu, 1959). preference order set attributes represented
additive value function
v(a1 , . . . , ) =

n
"

vi (ai ),

i=1

iff mutually preferential independent.
Dyer Sarin (1979) extend additivity theory MVF, specify conditions
u() well additive structure above. Effectively, additive forms
used trading mechanisms assume MPI full set attributes, including money
attribute. Intuitively means willingness-to-pay levels attribute attributes
cannot affected instantiation attributes. sweeping condition rarely
holds practice (Von Winterfeldt & Edwards, 1986). Therefore, recent AI literature often
relaxes MPI assumption imposing additivity respect subsets attributes
may overlap.
Definition 6 (Generalized Additive
Independence). Let I1 , . . . , Ig (not necessarily
#
disjoint) subsets A, gi=1 Ii = A. elements I1 , . . . , Ig called generalized
additive independent (GAI) exist functions f1 , . . . , fg that,
u(a1 , . . . , ) =

g
"

fr (Ir ).

(2)

r=1

2.2 Related Work Generalized Independence
definition GAI somewhat nonstandard. literature defines GAI condition
expected utility function (von Neumann & Morgenstern, 1944). well-known
model, particular choice results lottery, probability distribution outcomes. expected utility function represents complete preference order lotteries.
Informally, GAI definition requires preferences lotteries depend
margins subsets I1 , . . . , Ig . form Eq. (2) result definition,
obtained Fishburn (1967). Fishburn introduces functional decomposition,
also provides well-defined form functional constituents f1 , . . . , fg . Graphical
models elicitation procedures GAI decomposable utility developed within
expected utility framework (Bacchus & Grove, 1995; Boutilier, Bacchus, & Brafman, 2001;
Gonzales & Perny, 2004; Braziunas & Boutilier, 2005). addition, generalized additive
utility models employed Hyafil Boutilier (2006) aid direct revelation mechanisms, Robu, Somefun, La Poutre (2005) opponent modeling
bilateral multi-item negotiation.
Bacchus Grove (1995), fact coined term GAI, show decomposition also obtained result collection local, easier detect, binary
483

fiEngel & Wellman

independence conditions. specifically, rely form called conditional additive
independence, which, informally, corresponds GAI decomposition limited two (overlapping) subsets X A. prove condition expressed
separation criterion graph whose nodes correspond A, means perfect
map (Pearl, 1988). Crucially, utility function decomposes GAI form lower dimensional functions, defined maximal clique graph. combined
Fishburns work, result provides well-defined functional form obtained
collection conditional additive independence conditions. result relies
form lotteries basis utility function independence conditions.
expression willingness-to-pay requires cardinal measure preferences, yet
without uncertainty, need expected utility representation. therefore
invoke MVF framework, Section 3, build additive decompositions MVF
developed Dyer Sarin (1979) develop multiattribute preference structures wtp.
development enables us follow footsteps Fishburn (1967) Bacchus
Grove (1995) show well-defined GAI form MVF also obtained using
collection easy-to-detect binary independence conditions.
2.3 Multiattribute Auctions
distinguishing feature multiattribute auction goods defined
vectors attributes. above, use denote set attributes describing domain
. configuration particular attribute vector, . Multiattribute auctions used
primarily procurement, part strategic sourcing processes (Sandholm, 2007).
procurement model single buyer, utility function (representing
willingness-to-pay) ub () purchasing . sellers s1 , . . . , sm utility
functions ci : %, representing cost si supply configurations buyer.
Definition 7 (Multiattribute Allocation Problem). multiattribute allocation problem (Parkes & Kalagnanam, 2005) is:
MAP =

max

i{1,...,m},

ub () ci ().

(3)

allocation (si , ) solving MAP said maximize surplus procurement
problem.
MAP decomposed two subproblems: first find efficient configuration
trader, find trader whose efficient configuration yields highest
surplus. call first part multiattribute matching problem (Engel, Wellman, &
Lochner, 2006).
Definition 8 (Multiattribute Matching Problem). multiattribute matching problem (MMP) buyer b seller si is:
MMP(b, si ) = arg max ub () ci ().


also call configuration selected MMP(b, si ) bilaterally efficient configuration
si .
484

fiGAI Auctions

theoretical work surplus-maximizing multiattribute auctions relates
way foundational work Che (1993). Ches model, good service
characterized single quality attribute, seller independent private
cost function quality. buyer announces scoring rule sellers,
price-quality offers evaluated. Che suggests several types auctions, including
second-score auction, seller bidding highest score wins, must provide
combination price quality achieves second-best score. second-score
mechanism, bidding truthfully equilibrium dominant strategies. particular, Che
shows sellers bid quality maximizes difference buyers
scoring rule cost function; words, respective MMP solution.
Branco (1997) generalizes Ches model results correlated costs.
basic model later generalized several authors account explicitly multiple quality attributes, usually restricting scoring rule additive
attributes (Bichler, 2001; David et al., 2002). Vulkan Jennings (2000) suggest modified version English auctions (iterative auctions require new bids increment
current bid price) bidders required improve current score, rather
price. Sandholm Suri (2006) consider incorporation non-price attributes
multi-item (combinatorial) auctions.
literature surveyed emphasizes auctions require buyer reveal
scoring function prior bidding. order achieve economic efficiency, scoring
function must convey buyers full utility function ub (). major obstacle
practical adaption mechanisms. Procurement auctions rarely isolated event,
buyer-supplier relationships usually evolve change time,
suppliers may retain market power, take advantage information revealed
buyer. Events sometimes conducted recurrent basis, several events may
conducted related goods correlated valuations. addition, buyer may wish
keep secret way utility may discriminating particular suppliers
(Koppius, 2002).
noted Section 1, Parkes Kalagnanam (2005) suggest alternative approach,
employs prices space configurations drive traders efficient configurations. Auction NLD maintains price , sellers bid full
configuration round. Auction AD maintains price level a"i D(ai ). Prices
initially set high. round, sellers bid particular value attribute,
auction selects set levels (again, per attribute) myopically buyer preferred round, is, approximately maximize buyers utility respect
current prices. addition, auction maintains discount factor applied ensure
single seller eventually selected. price configuration defined sum
prices chosen attribute levels minus discount. round, prices
particular levels particular attributes decremented constant ", according set
price change rules, ensuring auction ultimately converges efficient solution.
auctions shown obtain optimal surplus (up "-proportional error),
sellers bid myopically rather strategically (we define concept formally Section 6.1). myopic behavior shown ex-post Nash equilibrium. Auction NLD
fully expressive tractable number attributes large. Auction AD
computationally efficient, expressiveness limited additive preferences (see
485

fiEngel & Wellman

discussion following Theorem 1). traders preferences additive, welfare
achieved auction necessarily optimal; is, solve MAP optimally,
respect inaccurate utility functions. Moreover, clear lack
expressiveness may affect incentives traders act strategically.
Theoretically, one could also use well-known Vickrey-Clake-Grove (VCG) mechanism. Parkes Kalagnanam define sell-side VCG mechanism: traders submit
full utility cost functions, MAP solved auction engine, winning
seller pays according VCG price (definition pricing provided Section 6.1).
auction, traders allowed use compact preference structure, including GAI. However, scheme suffers disadvantages proposals
require full revelation utility.
summarize, previously suggested surplus-maximizing multiattribute procurement
auction time expressive (accommodates interdependencies attributes),
tractable (its computations depend fully exponential domain), preserving
buyers private information, meaning (minimally) require buyer
reveal full utility function extracting bids sellers. proposed mechanism, show theoretically using simulations, possesses attractive properties
criteria.

3. Detection GAI Structure Measurable Value Functions
section provide basis application GAI decomposition procurement
problems. Section 3.1 show GAI obtained collection local, weaker
conditions based invariance willingness-to-pay. Section 3.2 use
example demonstrate process used procurement problems.
3.1 Difference Independence GAI
Dyer Sarin (1979) introduce measurable value analog additive independence,
called difference independence. first step introduce conditional generalization
definition.
Definition 9 (Conditional Difference Independence). Let X, X = ,
define Z = \ X . X conditionally difference independent , denoted
CDI(X, ), Z " D(Z), X 1 , X 2 D(X), 1 , 2 D(Y ),
(X 1 1 Z " , X 2 1 Z " )
(X 1 2 Z " , X 2 2 Z " ),

(4)

1 hold.
symbol
indicates !
definition MVFs (1), CDI condition (4) expressed equivalently
terms measurable value:
u(X 1 1 Z " ) u(X 2 1 Z " ) = u(X 1 2 Z " ) u(X 2 2 Z " )
condition states value, willingness-to-pay, change assignment
X depend current assignment , fixed value Z.
CDI condition leads convenient decomposition MVF.
486

fiGAI Auctions

Lemma 2. Let u(A) MVF representing preference differences, X, Y, Z specified Definition 9. CDI(X, ) iff
u(A) = u(X 0 , Y, Z) + u(X, 0 , Z) u(X 0 , 0 , Z),
arbitrary instantiations X 0 , 0 .
single CDI condition, therefore replace n-ary function u(X, Y, Z)
two lower-dimensional functions u(X 0 , Y, Z) u(X, 0 , Z). reasonable assume
one apply CDI conditions decompose resulting functions.
order take full advantage existing CDI conditions, introduce notion
dependency graph, simplification concept perfect map mentioned
Section 2.2.
Definition 10 (Dependency Graph). Let denote set, R binary relation
2S . graph G = (S, E) dependency graph R S1 , S2 S, holds
(S1 , S2 ) R iff a1 S1 a2 S2 , (a1 , a2 )
/ E.
Hence dependency graph expresses R separation criterion; two subsets
direct connection iff R. dependency graph CDI constructed
simply removing edge (a1 , a2 ) CDI({a1 }, {a2 }); CDI(S1 , S2 )
holds iff CDI({a1 }, {a2 }) holds a1 S1 a2 S2 . use term CDI map
dependency graph induced CDI relation.
next theorem links CDI condition, CDI map, GAI decomposition
A. fact, establishes functional constituents GAI decomposition
MVF functional constituents GAI decomposition expected
utility model, defined Fishburn (1967). adopt following conventional notation.
Let (a01 , . . . , a0n ) predefined vector called reference outcome. A,
function u([I]) stands projection u(A) rest attributes
fixed reference levels.
Theorem 3 (CDI-GAI Theorem). Let G = (A, E) CDI map A, {I1 , . . . , Ig }
set (overlapping) maximal cliques G.
u(A) =

g
"

fr (Ir ),

(5)

r=1


f1 = u([I1 ]),
r = 2, . . . , g,

fr = u([Ir ]) +

r1
"

(6)
(1)j

j=1

"
1i1 <<ij <r

u([

j
$

Iis Ir ]).

s=1

small example, Table 1 exhibits utility function u(x1 , x2 , x3 ). three
attributes boolean domain, D(xi ) = {0, 1}. Let x0i x1i denote assignments 0 1 (respectively) xi . first observe CDI({x1 }, {x3 }) holds because:2
2. Note x02 x12 correspond Z ! Definition 9.

487

fiEngel & Wellman

x1
0
1
0
1
0
1
0
1

x2
0
0
1
1
0
0
1
1

x3
0
0
0
0
1
1
1
1

u(x1 , x2 , x3 )
0
5
2
6
3
8
7
11

u(x1 , x2 , x03 )
0
5
2
6
0
5
2
6

u(x01 , x2 , x3 )
0
0
2
2
3
3
7
7

u(x01 , x2 , x03 )
0
0
2
2
0
0
2
2

u1 (I1 )
0
5
2
6
0
5
2
6

u2 (I2 )
0
0
0
0
3
3
5
5

Table 1: utility function three attributes, decomposable via GAI sum two
functions two attributes each. u1 () depends {x1 , x2 } u2 () depends
{x2 , x3 }.

1. utility difference values x1 given x02 5, x03 x13 . explicitly,
u(x11 , x02 , x03 ) u(x01 , x02 , x03 ) = 5 0 = 5, u(x11 , x02 , x13 ) u(x01 , x02 , x13 ) = 8 3 = 5.
2. Similarly, difference x1 given x12 4, x03 x13 .
Though x1 x3 CDI other, see depend x2 . example,
differences mentioned x1 5 4 given x02 x12 (respectively), hence
difference x1 given fixed x3 depends value x2 . CDI map example
therefore includes edge (x1 , x2 ) edge (x2 , x3 ). maximal cliques graph
I1 = {x1 , x2 } I2 = {x2 , x3 }.
obtain numeric decomposition, first define (x01 , x02 , x03 ) reference values.
Next, (6), get u1 (I1 ) = u([I1 ]) = u(x1 , x2 , x03 ) u2 (I2 ) = u([I2 ]) u([I1 I2 ]) =
u(x01 , x2 , x3 ) u(x01 , x2 , x03 ). functions involved given Table 1. Note
fifth sixth columns obtained appropriate values fourth column;
example, u(x01 , x2 , x3 ) line x1 = 1, x2 = 1, x3 = 0 value u(x1 , x2 , x3 ) line
x1 = 0, x2 = 1, x3 = 0. easy verify indeed u(x, y, z) = u1 (I1 ) + u2 (I2 ).
CDI-GAI Theorem provides operational form GAI, establishing GAI
decomposition obtained collection simple CDI conditions. assumption detection CDI conditions performed incrementally, MVF
decomposed reasonable dimension. CDI conditions, turn, based
invariance preference differences, relatively intuitive detect. particularly
true differences carry direct interpretation, case willingness-to-pay:
check invariance monetary amount buyer willing pay get one outcome
other.
GAI decomposition depicted graphically using clique graph CDI
map, is, graph whose nodes correspond maximal cliques CDI map.
purposes convenient use particular clique graph called tree decomposition (or
junction tree). introduce well-known concept, discuss implications GAI
representation.
Definition 11 (Tree Decomposition). tree decomposition graph G = (N, E)
pair (T, I), = (, E) acyclic graph, = {Ii | } collection
488

fiGAI Auctions

term
MAP
MMP
MVF
PI
FOPI
MPI
GAI
CDI
CDI map
GAI tree

Meaning
Multiattribute Allocation Problem
Multiattribute Matching Problem
Measurable Value Function
Preferential Independence
First-Order Preferential Independence
Mutual Preferential Independence
Generalized Additive Independence
Conditional Difference Independence
graph whose separation criterion CDI
tree decomposition CDI map

Reference
(Parkes & Kalagnanam, 2005)
(Engel et al., 2006)
(Dyer & Sarin, 1979)
(Keeney & Raiffa, 1976)
(Keeney & Raiffa, 1976)
(Bacchus & Grove, 1995)
cf. (Bacchus & Grove, 1995)
cf. (Gonzales & Perny, 2004)

Table 2: Acronym terms, references related literature. Empty references indicate
terms introduced work. terms arranged according topics: (i) multiattribute economic problems, (ii) independence relations, (iii) graphical concepts.
#
subsets N , corresponding node , (i) iI Ii = N , (ii) edge
(n1 , n2 ) E, exists Ii n1 , n2 Ii , (iii) (running intersection)
i, j, k , j path k Ii Ik Ij .
graph tree-decomposed, typically one way. example,
single node I. width tree decomposition maxiI |Ii | 1,
treewidth graph minimum width among possible tree decompositions.
easy show maximal clique G contained within I. Therefore, Theorem 3, utility function decomposes additively subsets = {Ii |
}, = (, E) tree decomposition CDI map. notion GAI tree
adapted work Gonzales Perny (2004), introduce GAI graphical models
expected utility framework.
Definition 12 (GAI Tree). GAI tree u() tree decomposition CDI map
u().
therefore refer elements I1 , . . . , Ig GAI decomposition set
tree decomposition. next subsection provides qualitative example CDI concept,
dependency graph, corresponding GAI tree.
results section lay foundations using GAI decomposition
multiattribute trading mechanisms. results generalize additive MVF theory, justify
application methods developed expected utility framework (Bacchus &
Grove, 1995; Boutilier et al., 2001; Gonzales & Perny, 2004; Braziunas & Boutilier, 2005)
representation monetary value certainty. Table 2 summarizes acronym
terminology introduced point.
3.2 Employing GAI Procurement
section demonstrate process obtaining GAI decomposition
collection CDI conditions. addition, example used motivate approach
489

fiEngel & Wellman

comparison work Parkes Kalagnanam (2005). Consider procurement
department wishes purchase new hard drives (HD) desktops large
number employees. buyer cares several characteristics (attributes) hard
drives particular terms procurement contract. attribute listed
designated attribute name (the first letter), domain. cases (e.g., attribute
I) use arbitrary symbols represent domain elements, abstracting meaningful
interpretation assumed context.
RPM (R) 3600, 4200, 5400 RPM
Transfer rate (T) 3.4, 4.3, 5.7 MBS
Volume (V) 60, 80, 120, 160 GB
Supplier ranking (S) 1, 2, 3, 4, 5
Quality rating (Q) (of HD brand) 1, 2, 3, 4, 5
Delivery time (D) 10, 15, 20, 25, 30, 35 days
Warranty (W) 1, 2, 3 years
Insurance (I) (for case deal signed implemented) 1 , 2 , 3
Payment timeline (P) 10, 30, 90 days
Compatibility (C) (with existing hardware software) 1 , 2 , 3
Consider, example, pair attributes Quality Warranty. value warranty different different values quality; higher quality known
low, lower quality known high. two attributes therefore depend other. Furthermore, might expect Volume complements Quality
Warranty. Larger hard drives prone failures, making quality warranty valuable. Similarly, interdependence Supplier ranking
contract insurance buy, Supplier ranking warranty supplier provides. reasonable dependencies among Delivery, Insurance, Payment timeline
(e.g., later delivery requires better insurance, later payment reduces need insurance),
Volume RPM Transfer rate. Preferences compatibility may
depend attribute. corresponding CDI map depicted Figure 1a.
described Section 2.1, utility function decomposes elements tree
decomposition CDI map. tree decomposition depicted Figure 1b.
example set elements tree decomposition correspond exactly maximal
cliques CDI map. general tree decomposition might include supersets
maximal cliques, decomposition obviously maintained supersets
well.
Non-additive traders, required deal additive price space auction
AD (Parkes & Kalagnanam, 2005), face exposure problem, somewhat analogous traders
combinatorial preferences participate simultaneous auctions (Wellman, Osepayshvili, MacKie-Mason, & Reeves, 2008). Essentially, problem manifest
490

fiGAI Auctions

(a)

(b)

Figure 1: HD procurement problem: (a) CDI map, (b) GAI tree.
two ways. One type exposure occurs one auction round another,
following two-attribute example. sellers conditional preference order attribute
may optimized assignment a1 given attribute b b1 ,
assignment b changes, a1 may become arbitrarily suboptimal. Therefore bidding a1
b1 may result poor allocation seller outbid b1 (and thus must resort
another assignment) left winning a1 . second exposure occurs single round
auction, trader bids multiple configurations. example, suppose configurations (a1 , b1 ) (a2 , b2 ) optimal current prices. bids collected
independently attribute, trader bidding may end configuration
(a1 , b2 ), again, may arbitrarily suboptimal.
prevent exposure sellers part taking simple measures auction
design. First, bids collected anew round, independently previous rounds, hence
first problem avoided. Sellers likewise avoid second problem limiting
bid one configuration per round.
buyers side, solution work require buyer bid
full set optimal configurations round, order ensure auctions convergence (this becomes clearer Section 6.1). prevent buyer exposure, auction design
structures prices according buyers preferences, traders bid clusters
interdependent attributes. terms example above, b interdependent
(meaning CDI({a}, {b}) hold), able bid cluster ab. b
turn depends c, need another cluster bc. still simpler general pricing
structure solicits bids cluster abc. generally, find reasonable CDI
conditions correct buyer, obtain corresponding GAI tree decomposition, solicit bids clusters attributes corresponding GAI elements.
Section 4, describe auction design detail, along example Section 5.
Section 6.1, prove auction terminates (approximately) optimal solution
MAP.

4. GAI Auctions
introducing auction design, reiterate model notation, provide
definition facilitates auction presentation.

491

fiEngel & Wellman

4.1 Notations Definitions
procurement setting, single buyer wishes procure single good, configuration one candidate sellers s1 , . . . , sm . buyer private
valuation function ub : %+ , similarly seller si private cost function, ci .
ub () ci () MVFs, utility differences express differences willingnessto-pay, explained Section 2.1. Assume buyers preferences reflected
GAI structure I1 , . . . , Ig . call assignment GAI element Ir sub-configuration.
use r denote sub-configuration formed projecting configuration element Ir .
Definition 13 (Consistent Cover). collection sub-configurations {1 , . . . , g },
r {1, . . . , g}, r instantiation Ir , consistent cover
r, r " {1, . . . , g}, attribute aj Ir Ir! , r r! agree assignment
aj .
words, consistent cover collection sub-configurations
compose valid configuration. collection {1 , . . . , g } consistent cover
equivalently considered configuration, denote (1 , . . . , g ). example,
consider good three attributes: a, b, c. attributes domain two possible
assignments (e.g., {a1 , a2 } domain a). Let GAI structure I1 = {a, b}, I2 =
{b, c}. Here, sub-configurations assignments form a1 b1 , a1 b2 , b1 c1 , on.
set sub-configurations {a1 b1 , b1 c1 } consistent cover, corresponding configuration
a1 b1 c1 . contrast, set {a1 b1 , b2 c1 } inconsistent.
4.2 GAI Auction
define iterative, descending-price multiattribute auction maintains GAI pricing structure: is, round t, price pt (), corresponding subconfiguration GAI element. price pt () configuration round
defined terms sub-configuration prices global discount term ,


p () =

g
"

pt (r ) .

(7)

r=1

Importantly, elements r may refer overlapping attributes. Bidders submit subbids sub-configurations global discount .3 Sub-bids submitted
round expire next round. sub-bid round configuration r
automatically assigned price pt (r ). set full bids seller contains consistent
covers generated sellers current set sub-bids. existence
full bid configuration represents sellers willingness accept price pt ()
supplying .
start auction, buyer reports (to auction, sellers) complete
valuation function ub (). GAI, expressed
decomposed form (6)
%
local functions (fb,1 , . . . , fb,g ), ub () =
r fb,r (r ). initial prices subconfigurations set level buyers valuations, is, p1 (r ) > fb,r (r )
r . discount initialized zero. auction dynamics descending
3. discount term could replaced uniform price reduction across sub-configurations.

492

fiGAI Auctions

clock auction: round t, bids collected current prices prices
reduced according price rules. seller considered active round set subbids submitted contains least one full bid. round > 1, sellers
active round 1 allowed participate, auction terminates
single seller active. denote set sub-bids submitted si Bit ,
corresponding set full bids
Bit = { = (1 , . . . , g ) | {1 , . . . , g } Bit }.
example Section 4.1, seller could submit sub-bids set sub-configurations
{a1 b1 , b1 c1 }, combines full bid a1 b1 c1 .
auction proceeds two phases. first phase (A), round auction
computes set buyer-preferred sub-configurations Mt : sub-configurations
part configuration within " profit-maximizing buyer
current prices. Formally, first define buyer profit configuration as4
bt () = ub () pt ().
buyer-preferred set sub-configurations defined by:
Mt = {r | bt () max
bt ( " ) ", r = 1, . . . , g}.
!


Section 6.2 show Mt computed efficiently. stress though Mt
set sub-configurations, criterion selecting based profit
full configurations. Profits individual sub-configurations meaningless outside
context configurations.
Phase A, auction adjusts prices round, reducing price every
sub-configuration received bid buyers preferred set. Let "
prespecified price
parameter. Specifically, Phase price change rule
#mdecrement


applied r i=1 Bi \ :
"
pt+1 (r ) pt (r ) .
g

[A]

Let denote set configurations consistent covers Mt :
= { = (1 , . . . , g ) | {1 , . . . , g } Mt }.
auction switches Phase B active sellers least one full bid
buyers preferred set:
i. Bit = Bit ,= .
[SWITCH]
Let round [SWITCH] becomes true. point, auction selects
buyer-optimal full bid seller si .
= arg max (bT ()).
BiT

(8)

4. drop superscript generic statements involving price profit functions, understanding
usage respect (currently) applicable prices.

493

fiEngel & Wellman

Phase B, si may bid . Sub-configuration prices fixed pT ()
phase. adjustment , increased every round ". (7),
increase decreases current price configurations . auction
terminates one seller (if exactly one, designate si ) active. allocation
determined according four distinct cases:
1. sellers drop Phase (i.e., rule [SWITCH] holds). auction
terminates allocation.
2. active sellers drop round Phase B. sellers
dropped last round, auction selects seller si ub (i ) pT (i )
maximal, designates seller winner si . single winner,
appropriate case 3 4 applied.
3. auction terminates Phase B final price buyers valuation,
pT (i ) > ub (i ). still possible exactly one seller (the winning
seller) whose cost buyers valuation, case trade positive
surplus possible. Therefore, auction offers winner si opportunity
supply price ub (i ).
4. auction terminates Phase B final price pT (i ) ub (i ).
ideal situation, auction allocates chosen configuration seller
resulting price.
Collect reported valuation, ub () buyer;
Set high initial prices, p1 (r ) sub-configuration r , set = 0;
[SWITCH], least one active seller
Collect sub-bids sellers;
Compute Mt ;
Apply price change [A];
end
Compute ;
one active seller
Increase ";
Collect bids (i , ) sellers;
end
Implement allocation payment winning seller;
Algorithm 1: GAI-based multiattribute auction.
overall auction described high-level pseudocode Algorithm 1. role
Phase guide traders efficient configurations (MMP solutions),
reducing prices configurations chosen least one seller preferred
buyer. price reduction makes configurations slightly less attractive seller
slightly attractive buyer. Phase B one-dimensional competition
profit remaining seller candidates provide buyer. next section
formalize notions, prove Phase indeed converges Phase B
494

fiGAI Auctions

I1
fb
f1
f2

a1 b1
65
35
35

a2 b1
50
20
20

I2
a1 b2
55
30
25

a2 b2
70
70
25

b1 c1
50
65
55

b2 c1
85
65
110

b1 c2
60
70
70

b2 c2
75
61
95

Table 3: GAI utility functions example domain. fb represents buyers valuation,
f1 f2 costs sellers s1 s2 .

selects seller whose efficient configuration yields (approximately) highest surplus.
Section 6.2 discuss computational tasks associated auction.

5. GAI Auction Example
illustrate auction simple three-attribute scenario, employing two-element
GAI structure I1 = {a, b}, I2 = {b, c}. Table 3 shows GAI utilities buyer
two sellers s1 , s2 . efficient allocation (s1 , a1 b2 c1 ): buyers valuation
55+ 85 = 140 cost s1 configuration (boldface table) 30+ 65 = 95,
hence surplus 45. maximal surplus second-best seller, s2 , 25, achieved
a1 b1 c1 , a2 b1 c1 , a2 b2 c2 . set initial prices I1 75, initial prices
I2 90, " = 8, meaning price reduction sub-configurations ("/g) 4.
sake example assume seller bids round configuration maximizes profit (price minus cost), respect prices current
round. next section provide formal definitions prove incentive properties
strategy.
Table 4 shows progress Phase A. Initially configuration price
(165), sellers bid lowest-cost configurationa2 b1 c1 (with profit 80 s1
90 s2 )realized sub-bids a2 b1 b1 c1 . M1 contains sub-configurations
a2 b2 b2 c1 highest value configuration a2 b2 c1 , yields buyer profit 10.
show next section (Lemma 7), maximum change throughout
Phase A. Price therefore decreased a2 b1 b1 c1 . price change, profit
s1 a2 b1 c1 72, higher profit (74) a1 b2 c2 bids a1 b2
b2 c2 . (round 2) prices go down, reducing profit a1 b2 c2 66 therefore
round 3 s1 prefers a2 b1 c2 (profit 67). Note point configuration a2 b2 c2
yields profit 16 buyer, within " maximal buyers profit (-10),
hence b2 c2 marked M3 .
next price change, configurations a1 b2 c1 a1 b2 c2 become optimal
s1 (profit 66), sub-bids a1 b2 , b2 c1 b2 c2 capture two. configurations
stay optimal another round (5), profit 62. round 5 profit configuration
a1 b2 c1 140 157 = 17, within " maximizing buyers profit, therefore
sub-configuration a1 b2 added M5 . point s1 full bid (in fact two full
bids: a1 b2 c2 a1 b2 c1 ) 5 , longer changes bids price
optimal configurations decrease. Seller s2 however sticks a2 b1 c1

495

fiEngel & Wellman


1

a1 b 1
75

2

75

3

75

4

75

5

75
s2
71

6
7
8
9

I1
a2 b 1
75
1 , s2
71
s2
67
1 , s2
63
s2
59

71
s2
67

67
, s2

59
s2
55
55
s2
51

a1 b 2
75
75
s1
71
71
s1
67
, s1
67
, s1
67
, s1
67
, s1
67
, s1

a2 b 2
75

75

75

75

75

75

75

75

75


b 1 c1
90
1 , s2
86
s2
82
s2
78
s2
74
s2
70
70
s2
66

66
, s2

I2
b 2 c1
90

90

90

90
, s1
90
, s1
90
, s1
90
, s1
90
, s1
90
, s1

b 1 c2
90

b 2 c2
90

90

90
s1
86

86
, s1
86
, s1
86
, s1
86
, s1
86
, s1
86
, s1

90
s1
86
86
86
s2
82
82
s2
78

Table 4: Auction progression Phase A. Sell bids designation Mt (using )
shown price sub-configuration.

first four rounds, switching a1 b1 c1 round 5. takes four rounds s2 Mt
converge (M9 B29 = {a1 b1 c1 }).
round 9, auction sets 1 = a1 b2 c1 (which yields buyer profit a1 b2 c2 )
2 = a1 b1 c1 . second phase, starts point, sellers compete
amount surplus transfer buyer, whose profit consequently becomes positive.
next round (10) = 8, increased 8 subsequent round. Note
p9 (a1 b1 c1 ) = 133, c2 (a1 b1 c1 ) = 90, therefore profit s2 point 43.
round 15, = 48 meaning p15 (a1 b1 c1 ) = 85 causes s2 drop
profit becomes negative. ends auction, sets final allocation (s1 , a1 b2 c1 )
pT (a1 b2 c1 ) = 157 48 = 109. leaves buyer profit 31 s1
profit 14.

6. Analysis
analyze economic properties auction Section 6.1, address practical
computational issues Section 6.2.
6.1 Economic Properties
adopt following assumptions discussion:
A1 optimal (seller, configuration) pair provides non-negative surplus.
A2 ub () real utility function buyer.
496

fiGAI Auctions

optimal solution MAP (3) provides negative surplus sellers bid
cost, auction terminates Phase A, trade occurs, auction
trivially efficient. Therefore Assumption A1 cause loss generality. A2
interpreted follows: given non-truthful buyer report, efficiency results apply
face value buyers report rather true utility.
6.1.1 Properties buyers profit function
{1, . . . , g}, define (partial) profit set sub-configurations
corresponding
"
b ( ) =
(fb,r (r ) p(r )).
r

functions f come GAI breakdown ub (6).
Lemma 4. complement ,
b () = b ( ) + b ( )
Proof. (6) definition b ( ) get
"
"
b () =
(fb,r (r ) p(r )) +
(fb,r (r ) p(r )) = b ( ) + b ( ).
r

r

round 5 example previous section, sub-configuration a1 b2 placed
configuration a1 b2 c1 within " maximal buyer profit 10. Actually,
point a1 b2 c1 added , also a1 b2 c2 whose buyer profit (23)
within " maximum. a1 b2 c2 later selected si could lead
additional efficiency loss, beyond ". following lemma bounds potential loss.
Lemma 5. Let set configurations, within " maximizing profit trader
(buyer seller) given prices. Let = {r | , r {1, . . . , g}}. consistent
cover within g" maximizing profit prices.
particular, includes exactly optimal configurations, consistent cover
exactly optimal well. proof (in Appendix B.1) relies definition GAI
decomposition tree decomposition, uses partial profit function defined
along Lemma 4.
bound tight, GAI tree nontrivial domain
construct example set exists consistent cover whose utility
exactly g" maximal.
result get following corollary.
Corollary 6.
. bt () max
bt ( " ) g"
!


497

fiEngel & Wellman

Proof. Apply Lemma 5 bt : define set configurations within " max! bt ( " ).
Mt , definition, serves lemma. exactly set consistent
covers , hence must within g" optimum max! bt ( " ).
show that, noted example, maximal profit buyer
change Phase A.
Lemma 7. max bt () = max b1 () round Phase A.
Proof. Assume exists " bt+1 ( " ) > bt ( " ). necessarily pt+1 ( " ) =
pt ( " ) > 0. price change Rule [A], meaning w g

sub-configurations " Mt , = w"
g . case, definition ,
bt ( " ) < max bt () ".


Therefore,
bt+1 ( " ) = ( " ) + = ( " ) +

w"
g"
( " ) +
< max bt ().

g
g

true " whose profit improves, therefore max bt () change
Phase A, hence equals value round 1.
6.1.2 Straightforward bidding sellers
turn attention sellers behavior. first define profit function
seller si () = pt () ci ().
Definition 14 (Straightforward Bidder). seller called straightforward bidder (SB)
round bids Bit follows: max () < 0, Bit = . Otherwise
select bti arg max (), set
Bit = {r | bti , r {1, . . . , g}}.
Intuitively, SB sellers follow myopic best response strategy, optimizing profit
respect current prices. approach termed straightforward Milgrom (2000)
sense agents bid myopically, rather strategically anticipating subsequent
price responses.
SB sellers choose optimal configuration bid on; none results proved
affected choice. also important note SB sellers find optimal
full configuration bti , rather optimize GAI element separately. configuration bti
translated set sub-configurations Bit . order calculate bti , seller si needs find
optimum current profit function. Section 6.2 show optimization
problem tractable assumption ui (), too, compact GAI structure.
following immediate corollary definition SB.
Corollary 8. SB seller si ,
t, Bit . () = max
( " ).
!


498

fiGAI Auctions

general, sellers preference structure may coincide auctions price structure. Nevertheless, Corollary 8 holds definition SB, Bit (defined Section 4.2)
contains single configuration submitted bid bti . Alternatively, definition
SB modified, sellers GAI preferences consistent auctions price
structure bid multiple optimal configurations (if exist). sellers bid multiple
configurations, speed convergence. case bti denotes set submitted
configurations, Bit denotes respective collection sub-configurations, Bit set
consistent covers Bit . Lemma 5 (with " = 0) entails Corollary 8 still holds.
However, simplicity analysis retain Definition 14.
6.1.3 Efficiency given SB
Lemma 9 states price system price change rules, Phase leads
buyer sellers mutually efficient configuration. Formally,
interested maximizing function : %, represents surplus ub () ci ().
prices pt ,
() = bt () + ().
Lemma 9. SB seller si , g"-efficient:
(i ) max () g".


Proof. Configuration chosen maximize buyers profit Bit end
Phase A. Bit ,= , configuration available Bit , hence one
must chosen maximize buyers utility. , Bit , get
Corollary 8,
(i ) (),
Corollary 6, get ,
bT (i ) bT () g".
Bit add two inequalities get (i ) () g",
desired result.
Based Phase Bs simple role single-dimensional bidding competition
discount, next assert overall result efficient SB, turn (Section 6.1.4) proves approximately ex-post equilibrium strategy two phases.
Theorem 10. Given truthful buyer SB sellers, surplus final allocation
within (g + 1)" maximal surplus.
Proof Sketch: first establish auction must reach Phase B. that,
show round Phase A, price least one sub-configuration reduced,
whereas Lemma 7, max bt () change. latter enforces lower bound
far prices reduced within Phase A, hence Phase must terminate.
initial prices buyers valuation, seller whose surplus (MMP solution)
positive cannot drop phase, using Assumption A1 show way
499

fiEngel & Wellman

Phase terminate reaching condition [SWITCH]. Next, show
two sellers, surplus first drop auction cannot significantly higher
one stayed longer. ensures winning seller efficient
one, one whose MMP surplus almost maximal, Lemma 9 auction must
obtain (almost) surplus. full proof given Appendix B.2.
bound guaranteed Theorem 10 worst-case bound, shown experimentally following sections auction typically achieves efficiency closer optimum.
example Section 5, difference efficiencies two sellers lower
potential efficiency loss (as (g + 1)" = 24). However, instance still guaranteed s1 wins, either efficient allocation, a1 b2 c2 provides
surplus 39. reason two configurations s1 surplus
within g" = 16 solution MMP(b, s1 ), hence Lemma 9 one must chosen 1 . configurations provide " surplus s2 efficient
configuration, sufficient order win Phase B.
bound Theorem 10 improved CDI map contains disconnected
components. example, fully additive decomposition (as assumed previous
literature) exist, CDI map contains disconnected component attribute.
take advantage disconnectedness create separate tree decomposition
disconnected components. definition adapted apportion "
proportionally across disconnected trees. Formally, redefine Mt follows.
Definition 15 (Buyers Preferred Set). Let G comprised trees G1 , . . . , Gh . Let
j denote projection configuration tree Gj , gj number GAI
elements Gj . Similarly, j denotes projection Gj . Define
"
"
, r Gj }.
Mtj = {r | bt (j ) max

(
)

g
j
j
b
g
j! j
buyers preferred set given Mt =

#h


j=1 Mj .

Let ej = gj 1 denote number edges Gj . define connectivity parameter,
e = maxj=1,...,h ej . turns e + 1 replace g approximation results.
first step replace Corollary 6 tighter bound optimality configurations
.
Corollary 11.
. bt () max
bt ( " ) (e + 1)"
!


Proof. apply Lemma 5 Gj , gj g" instead ", hence consistent cover
Mtj within gj g" gj maxj! j bt (j" ). Lemma 4, get consistent
%
cover Mt (meaning configuration ) within hr=1 gj g" gj max! bt ( " ).
%
e + 1 = maxj=1,...,h gj , bounded g" hr=1 gj (e + 1) = "(e + 1).
obtain tighter efficiency result.
Theorem 12. Given truthful buyer SB sellers, surplus final allocation
within (e + 2)" maximal surplus.
500

fiGAI Auctions

fully additive case loss efficiency reduces 2". extreme,
CDI map connected e + 1 = g, reducing Theorem 12 Theorem 10.
assume preference structure buyer, meaning CDI map fully
connected, e = 0 efficiency loss proportional ".
6.1.4 Sellers incentives use SB
Following Parkes Kalagnanam (2005), relate auction Vickrey-ClarkeGroves (VCG) mechanism establish incentive properties sellers. one-sided
multiattribute VCG auction, buyer reports valuation ub , sellers report cost functions
ci , buyer pays sell-side VCG payment winning seller.
Definition 16 (Sell-Side VCG Payment). Let ( , ) optimal solution MAP.
Let (, i) best solution MAP participate. sell-side VCG
payment
VCG(ub , ci ) = ub ( ) max(0, ub () ci ()).
well known truthful bidding dominant strategy sellers one-sided
VCG auction. Parkes Kalagnanam (2005) showed maximal regret buyers
bidding truthfully mechanism ub ( ) ci ( ) (ub () ci ()), is,
marginal product efficient seller.
typical iterative auctions, VCG outcome exactly achieved,
deviation bounded minimal price change.
Definition 17 (-VCG Payment). sell-side -VCG payment MAP payment p

VCG(ub , ci ) p VCG(ub , ci ) + .
payment guaranteed -VCG, sellers affect payment within
range, hence gain falsely reporting cost bounded 2.
Lemma 13. sellers SB, GAI auction payment sell-side (e + 2)"-VCG.
example Section 5, profit winner (14) less " VCG
profit 20. proof (in Appendix B.4) also covers Case 3 allocation options
Section 4.2, force payment equal ub (i ).
ready final result section, showing approximately
efficient outcome guaranteed Theorem 12 achieved (approximate) ex-post Nash
equilibrium.
Theorem 14. SB (3e + 5)" ex-post Nash equilibrium sellers GAI auction.
is, sellers cannot gain (3e + 5)" deviating SB, given
sellers follow SB.
order exploit even bounded potential gain, sellers need know, given
configuration , whether explicitly selected approximately optimal
buyer, combination sub-configurations approximately optimal configurations. seems highly unlikely sellers information. likely
lose bid myopically optimal configurations.
501

fiEngel & Wellman

6.2 Computation Complexity
advantage GAI auctions additive auction AD (Parkes & Kalagnanam,
2005) economic efficiency: accommodating expressive bidding, efficiency results
respect accurate utility function. contrast, key advantage respect auction employ preference structures, auction NLD (Parkes
& Kalagnanam, 2005), computational efficiency. property show section
computations exponential size largest GAI element, rather
|A|. particular, size price space auction maintains equal
total number sub-configurations. number exponential treewidth (plus one)
original CDI map.5 ensure computational tractability, one define priori
constant C, force treewidth CDI map bounded C ignoring
interdependencies. still much better using additive representation
ignores interdependencies. constant represents tradeoff economic
computational efficiency; larger C supports accurate preference representation,
GAI elements may larger.
!
#
purpose computational analysis, let = gr=1 aj Ir D(aj ), collection sub-configurations. Since grows monotonically t, nave generation
best outcomes sequentially might end enumerating significant portions
domain. Fortunately, enumeration avoided, complexity computation (as well optimization performed seller) grows |I|, is,
computation depends size exponential domain.
Theorem 15. computation Mt performed time O(g|I|2 ). Moreover,
total time spent task throughout auction O(g|I|(|I| + )).
obtain bound , number rounds Phase A, comparing sum
prices sub-configurations rounds 1 .
Theorem 16. number rounds required auction bounded
"
g

p1 (r ) .
"
r

%

Proof. Let =
r p (r ) (the sum prices sub-configurations round i).

Assume < 0 1 . ub () 0, must exist
bi () > 0. chose initial prices , b1 () < 0,
contradicts Lemma
0, hence sum prices cannot reduced
% 7. Therefore,
1
1
= r p (r ) throughout auction. Also, round least one
price reduced g" . leads required result.
bound rather looseits purpose ensure number rounds
depend size non-factored domain. depends number subconfigurations, result dividing initial price minimum price decrement. Usually Phase converges much faster. Let initial negative profit chosen
auctioneer = max b1 (). worst case, Phase needs run
5. use term treewidth subject using optimal tree decomposition.

502

fiGAI Auctions

. b () = m. happens example r I. pt (r ) = fb,r (r ) +
g .
implies closer initial prices reflect buyers valuation, faster Phase
converges. One extreme choice set p1 (r ) = fb,r (r ) +
g . would make Phase
redundant, cost full initial revelation buyers valuation (Section 2.3). option extreme, , I. p1 () = p1 (), auctioneer
range choices determine right tradeoff convergence time information revelation. example Section 5, choice lower initial price
domain I1 provides speedup revealing harmless amount information.
simulations below, also set constant initial prices within GAI element.
Furthermore, many domains natural dependencies mutual traders,
case price structure used auction may also accommodate sellers preference
structures. so, sellers bid multiple equally profitable configurations round,
thus speeding convergence, discussed Section 6.1.
also consider computational complexity SB strategy sellers.
Theorem 17. Let b denote treewidth CDI map ub (), let denote
treewidth CDI map ui (). optimization ui () p() takes time exponential
b + worst case.
Proof. Consider graph G includes union edges two CDI maps.
treewidth G b + worst case. definition, price function p()
decomposed according ub (), hence ui () p() decomposes according additive GAI
factors ui () ub (). Therefore, pair attributes x mutual
factor ui () p(), edge x, G. well known complexity
combinatorial optimization exponential treewidth graphfor example,
using cost networks (Dechter, 1997).
potential concern may communication cost associated descending
auction style. sellers need send bids round.
simple change made avoid much redundant communication: auction
retain sub-bids previous rounds sub-configurations whose price change.
combinations sub-bids different rounds yield suboptimal configurations,
sub-bid tagged number latest round submitted,
consistent combinations round considered full bids.
implementation sellers need resubmit bid price least one
sub-configurations changed.
summarize, GAI auctions shown perform well criteria mentioned Section 2.3: achieve approximate efficiency given reasonable incentive properties,
expressive enough accommodate preferences interdependencies among attributes,
tractable maximal size GAI clusters reasonably bounded,
require full revelation utility. Performance last criterion quantified
experimental part paper.

7. Experimental Design
main idea behind GAI auctions improve efficiency auctions assume
additivity, preferences additive. However, (given fixed ") theoretical
503

fiEngel & Wellman

efficiency guarantee GAI auctions depends e, connectivity parameter GAI
tree. suggests tradeoff: complex models accurately represent true utility,
increase approximation error due higher connectivity. obvious question whether
accurate preference modeling indeed efficient, particular, whether
GAI auctions efficient additive auctions, given preferences
additive. address question experimentally, assume buyers preferences
GAI structure, compare performance GAI auctions model
structure performance auctions restricted additive representation.
latter, use instance GAI auction pricing structure additive,
name additive approximating auction (AP). auction similar principle
auction AD (Parkes & Kalagnanam, 2005).6 best knowledge, AD
proposed instance surplus-maximizing multiattribute auction based additive
preferences, besides require full revelation buyers utility.
experiments, sellers employ SB strategies.
Section 7.1 describe random GAI utilities drawn, Section 7.2
extend scheme generate GAI utility functions exhibit additional structure.
Section 7.3 show obtain additive approximation random functions,
allowing us simulate auction AD. results simulations presented Section 8.
7.1 GAI Random Utility
performed simulations using randomly generated utility functions representing
buyers value function sellers cost functions. random utility generation procedure follows utility elicitation procedure suggested Braziunas Boutilier (2005),
uses two-step process: first create local utility functions GAI element,
normalized range [0, 1]. Next, draw scaling constants represent relative
weight local function overall utility.
formally, let ur (Ir ) = u([Ir ]) denote local utility function Ir , normalized
[0, 1]. Next, let fr (Ir ) defined according GAI functional form Eq. (6),
u([Ir ]) replaced ur (Ir ), hence
f1 = u1 (I1 ),
r = 2, . . . , g,

fr = ur (Ir ) +

r1
"
(1)j
j=1

"
1i1 <<ij <r

ur ([

j
$

Iis Ir ]).

(9)

s=1

Braziunas Boutilier (2005) show GAI-structured utility, exist scaling
constants r [0, 1]
g
"
r fr (Ir ).
(10)
u(A) =
r=1

6. auctions employ additive price space drives bidders efficient configurations. AD
efficient ! buyer sellers additive preferences. GAI auctions !-efficient
given additive buyers preferences, make assumption regarding sellers preference.
structural differences: (i) AD employs complicated price change rules, order allow
sellers ignore attributes, (ii) discounts used stage AD, auction
selects provisional winner iteration.

504

fiGAI Auctions

refer functions ur (Ir ) subutility functions. Note values form
ur ([Iir Iir! ]) drawn used ur (Ir ) ur (Iir! ). representation
lets us draw random GAI functions, given GAI tree structure, using following
steps:
1. Draw random subutility functions ur (Ir ), r = 1, . . . , g range [0,1].
2. Compute fr (), r = 1, . . . , g using (9).
3. Draw random scaling constants r ,
(10).

%g

r=1 r

= 1, compute u(A)

scaling constants represent importance decision maker accords corresponding
GAI elements overall decision. procedure results utilities normalized
[0, 1]. Finally, particular trader draw mean variance , scale u()
range [ , + ], resulting utility functions ub () ui () = 1, . . . , m.
7.2 Structured Subutility
subutility function model may represent valuation subspace.
practice may often find additional structure within GAI element. introduce
two structures consider typical generally applicable, use
simulations, along completely random local functions.
argue Section 2.1, typical purchase sale decisions exhibit FOPI (first
order preferential independence), meaning single attributes natural
ordering quality. example, hard-drive buyers always prefer memory, higher
RPM, longer warranty, on. implement FOPI, let integer values
attribute represent quality. example, belongs GAI element Ir = {a, b},
make sure ur (ai , b" ) ur (aj , b" ) ai > aj , ai , aj D(a), b" D(b).
must course hold attribute FOPI, GAI element Ir
includes a. enforce condition values GAI element
drawn, special-purpose sorting procedure, applied steps 1 2 above.
FOPI condition makes random utility function realistic, particular
appropriate target application. attributes exhibit FOPI, dependencies
among different attributes likely framed complements substitutes.
concepts known primarily context combinatorial preferences, is, preferences
combinations distinct items. multiattribute framework, two attributes
complements improvement value worth sum
improvement separately. Two attributes substitutes way
around. concepts meaningful respect attributes FOPI,
otherwise notion improvement conditional value attributes.
Definition 18 (Complements Substitutes). Let u() measurable value function " . Let a, b " , Z = " \ {a, b}, assume b FOPI
rest attributes. Attributes b called complements ai > ai
(ai , ai D(a)) bj > bj (bj , bj D(b)), Z " D(Z),
u(ai , bj , Z " ) u(ai , bj , Z " ) > u(ai , bj , Z " ) u(ai , bj , Z " ) + u(ai , bj , Z " ) u(ai , bj , Z " ).
505

fiEngel & Wellman

Attributes b substitutes inequality sign (always) reversed.
relationship attributes ruled additive utility function,
admitted weaker independence condition, called mutual utility independence (MUI)
(Keeney & Raiffa, 1976), implies utility function either multiplicative
additive. multiplicative, utility function represented n singledimensional functions, n scaling constants, single parameter k (the MUI-factor)
controls strength complementarity (k > 0) substitutivity (k < 0)
pairs attributes within GAI element (for k = 0 set attributes additive).7
experimental purposes, assume attribute cluster (GAI element) exhibits MUI,
value k all.
elicitation procedure, one would normally extract MUI scaling constants
user, compute k (Keeney & Raiffa, 1976). purposes, first determine
k according relationship wish impose attributes, draw MUI
scaling constants consistent value. explicitly, draw random
scaling constants, iteratively modify constants, set constants
found consistent k. next step compute ur (Ir ) according MUI
formula (Keeney & Raiffa, 1976). ur (Ir ) (for r) range [0, 1], hence
point proceed steps 2 3 above. Note procedure several
distinct sets scaling constants used: g constants used step 3 scale different
GAI elements, whereas MUI constants, per GAI element, scale attributes within
element.
7.3 Additive Approximation
Another issue experiment design additive auction (AP) behaves face
non-additive buyer preferences, specifically would select approximately buyerpreferred sets configurations. approach took come additive
function approximates buyers true utility function, use throughout
auction. aware better strategy, rule possibility one
exists.
%
natural approach generate linear approximation fi () arbitrary function
ub () use linear regression. define indicator variable xij every aij D(ai ),
consider value assignment data point. example, assignment
a1j(1) , . . . , amj(m) creates following data point:

"

"

cij xij = u(a1j(1) , . . . , amj(m) ),

i=1 aij D(ai )

value variable xij 1 j = j(i) 0 otherwise. coefficients cij
result regression represent values used fi (aij ).
problem includes many attributes, possible consider points
. assumption compact GAI representation exists, sensible
expect could use fewer data points regression. indeed found small
7. formalize notion Appendix D.

506

fiGAI Auctions

random sample joint utility yields approximation effective one based
points. precisely, largest domain tested (25 attributes, domain
size 4) found efficiency AP improve increasing number
sampled points beyond 200. show chart supporting claim Appendix E.
experiments use 300 points instances.
method comparison probably overestimates quality additive approximation. general, would true utility function explicitly generate
approximation. Extraction elicitation utility function usually
serious bottleneck multiattribute mechanism. Therefore, major reason use
additive approximation reduce burden elicitation. Hence practice would
try obtain additive function directly, rather obtain full utility approximate it. result process somewhat unpredictable, elicitation
queries may coherent: willingness pay a1 depends value b,
willingness pay a1 know b? therefore consider
experimental generation method biased favor additive approximation.

8. Simulation Results
provide detailed results simulation study. Section 8.1 provides analyses economic efficiency results. Section 8.2 covers computational study, results regarding
revelation private information provided Section 8.3.
8.1 Efficiency GAI Structure
measure efficiency terms percentage MAP solution, surplus
achieved optimal seller-configuration pair. evaluate effect preference modeling efficiency, vary structural parameters buyers GAI preferences: connectivity factor e, size largest GAI element. Performance depends
many additional factors, size attribute domains, number sellers, amount
price decrement ("), distribution utility functions drawn. isolate
primary structural parameters, first tested efficiency varies according
choices side factors, several fixed GAI structures fully random subutility
functions. result tests, picked following parameter values rest
simulations: valuations drawn uniform distribution, buyer mean
set 500. mean seller drawn uniformly [500, 700]. variance set
200 traders. use domain size 3 4 attributes, number
sellers = 5. explanation process leading choices provided
full report (Engel, 2008).
following experiment used roughly fixed GAI structure, g = 6 elements
e = 5, (that is, GAI structure tree, forest), " = 24 (meaning reduction
= 4 per sub-configuration). vary number attributes varying size
element. Figure 2a shows efficiency obtained respect , size largest
GAI element. expected, size GAI elements negligible, effect
efficiency GAI auctions. dramatic effect efficiency AP. = 1,
decomposition fact additive hence AP performs optimally. performance
deteriorates increases.
507

fiEngel & Wellman

(a)

(b)

Figure 2: Efficiency function of: (a) size largest GAI element (), given e = 5,
(b) number GAI elements (e + 1), given = 5.
performed test using utility attributes FOPI.
FOPI restriction, additive approximation much efficient relative unconstrained random utility. FOPI applies strict subset attributes, would
expect efficiency AP fall somewhere efficiency FOPI
unrestricted case. Somewhat surprisingly, imposing FOPI renders GAI auctions slightly
less efficient. Nevertheless, additive approximation achieves lower efficiency compared
accurate preference modeling, differences pass statistical significance
test (P < 0.01), 3. Further, note performance GAI auctions always
improved using smaller value " = g" , whereas hardly improves performance
AP. = 2, statistically significant difference (with confidence level)
already detected 2. used = 2 hereafter.
next experiment (Figure 2b) measures efficiency function e, given fixed
. assume connected GAI trees, e number GAI elements minus one.
tested structures e varying 1 10, elements size 3 5, = 5
structures.8 single element, GAI auction similar NLD (Parkes &
Kalagnanam, 2005), auction assigns price every point joint
domain. e = 0, hence efficiency GAI close perfect. structure
extreme compared additive representation, indeed performance AP
particularly inferior (only 70% efficient).
GAI elements, efficiency GAI auctions declines slow pace.
theoretical potential error (e + 2)", mostly result efficiency loss winning
seller, based Lemma 5. efficiency loss may occur sub-configuration
belongs configuration yields lowest profit allowed buyer-preferred
seta particularly rare case. practice, loss closer e, much smaller
error.
performance AP improves number elements grows maximal
average sizes fixed. Intuitively, changing structure way takes closer
8. find particular tree structure influential results; final structure used
reported results maximum three children per node.

508

fiGAI Auctions

(a)

(b)

Figure 3: (a) Efficiency function k 0 (complements). (b) Efficiency function
k 0 (substitutes).
additive representation. FOPI, see similar phenomenon before. However,
difference GAI FOPI AP FOPI, even ten elements, substantial
statistically significant.
Figures 3a 3b present efficiency function MUI-factor k, complements
substitutes, respectively. used fixed GAI structure four elements, largest
four attributes, imposed k elements. expected,
stronger complementarity among attributes, lower efficiency AP, whereas
relationship affect efficiency GAI auctions. case substitutes,
contrast, additive approximation performs well, efficiency starts deteriorate
extreme values k. roughly, say relationship among attributes
(within GAI element) limited (mild) substitutions, could good idea
use additive approximation. Unfortunately, interpretation parameter k lacks
quantitative scaling: clear intuition actual numbers mean, beyond
qualitative classification mentioned above.
summarize part, experimental results show GAI auctions yield significant efficiency improvement comparison additive auction, almost classes
evaluations. Though efficiency additive auction may come across relatively
high perhaps sufficient, observation misleading several respects. (i)
large procurement events, 510% efficiency differences translate large amounts money.
(ii) wider efficiency loss additive auction (with theoretical bound) may
impact incentives; SB may longer approximate ex-post Nash equilibrium.
(iii) Efficiency expected deteriorate larger problems larger GAI elements,
particular FOPI hold many attributes. (iv) argued Section 7.3, expect practical additive auctions perform worse AP tailored
approximation.
8.2 Computational Analysis
computational tasks required auction simulations performed using algorithms described Appendix C. algorithms suggested applied
509

fiEngel & Wellman

(a)

(b)

Figure 4: Number rounds function of: (a) size largest GAI element (), given
e = 5, (b) number GAI elements (e + 1), given = 5 = 2.
combinatorial optimization problems (Dechter, 1997; Nilsson, 1998), therefore
computational runtime process round particular interest work. Instead,
focus number rounds auction requires. tested number rounds
required auctions GAI AP, fully random FOPI preferences, varying three
parameters: (size largest GAI element), e (connectivity), .
complexity terms number rounds shown Figure 4a (with respect )
Figure 4b (with respect number elements). observe FOPI
GAI auction takes much longer converge, compared case random preferences.
reason FOPI, sellers buyers preferences general seen
opposites: price, specific attribute, buyer prefers higher quality,
whereas sellers prefer lower quality (given fixed values rest attributes),
everyone agrees relative quality attribute values. apparent difference
growth rate (the FOPI case seems steeper curve) somewhat misleading:
= 8 (not shown) GAI random preferences already caught curve
see FOPI case. number rounds, expected, grows exponentially
size largest element. However, observed Figure 4b, number
grow quickly function number elements, supporting theoretical arguments
Section 6.2. Note also variance chosen traders preferences fixed, thus
small number elements variance wider, resulting large number
rounds required GAI FOPI case.
AP, implication increasing respective increase number
attributes. result, complexity AP (not shown) grows slowly
increase . FOPI case, = 2, AP takes average 481 rounds = 1
(6 attributes) 546 rounds = 6 (19 attributes). numbers slightly higher
random preferences (523 628).
high-dimensional multiattribute auctions, expect participation would typically automated software bidding agents (Wellman, Greenwald, & Stone, 2007).
circumstances, auction taking thousands rounds cause
concern. However, reason rounds expensive, might reconsider adopt-

510

fiGAI Auctions

Figure 5: Efficiency function number rounds.
ing additive auctions, sacrifice efficiency order decrease number rounds.
Alternatively, could keep using GAI auctions increase " (and ). final
experiment compares two alternatives. vary level ", order view efficiency function number rounds (Figure 5). GAI structure used
experiment e = 5 = 5.
evident chart, cases GAI achieves better efficiency even fixed
number rounds. exception budget rounds small (under
200), FOPI holds. case need pay rounds order get
higher efficiency.
total computation time, carried GAI auction 10 elements, = 5,
= 3, = 2, rest parameters fixed above, around 11 seconds
average, using Intel Dual Core (2.00 Ghz) CPU, 2048 MB RAM.
8.3 Information Revelation
key difference mechanism proposed previous literature
extent buyer required reveal preference information. GAI auctions,
buyer need reveal private preference information front.
course, price changes reveal buyers information. Another experimental
question therefore whether mechanism significantly reduces overall amount
information revealed buyer.
PK study information revelation buyer seller, additivity
assumption. utility function additive amount information revealed
measured terms constraints linear weights. Sellers infer bounds
buyers set weights, amount information hidden represented
fraction simplex satisfies constraints. simplex analysis
possible GAI utilities. suggest alternative geared towards kind information
revealed GAI auctions.
GAI auctions, buyers private information partially revealed selection buyers preferred set Mt . auction need announce directly;
general sellers infer sub-configuration Mt received bid
(usually sellers observe bids), yet price change

511

fiEngel & Wellman

next round. therefore measure exactly thatfor many sub-configurations r
least one round r Mt Bit i. specifically, define
sub-configuration revealed, within GAI element measure fraction
sub-configurations revealed end auction. measurement overestimates information actually revealed, sellers infer bounds relative
preferences precise values functions fb (). Moreover, assumes
seller observes bids (meaning sellers share bid information other),
unrealistic event practice.
Based criterion, GAI auctions reveal average 15%25% buyers preferences preferences exhibit FOPI, 10%15% subutilities completely
random. seem systematically depend parameter tested.
validates claim advantage GAI auctions promise second-score types
auctions.

9. Conclusions
propose novel exploitation preference structure multiattribute auctions. Rather
assuming full additivity, structure all, model preferences using generalized additive independence (GAI) decomposition. show GAI representation
constructed relatively simple statements willingness-to-pay, develop
iterative auction mechanism directly relying decomposition. auction mechanism
generalizes preference modeling employed Parkes Kalagnanam (2005),
essence retaining information revelation properties. allows range tradeoffs
accuracy preference representation computational complexity
auction, well tradeoff buyer information revelation number
rounds required convergence.
performed simulation study proposed multiattribute auctions, compared
mechanism assumes additive preferences. study validated usefulness
GAI auctions preferences non-additive GAI, allowed us quantify
advantages specific classes preferences. general, found design
yields significantly higher economic efficiency comparison additive auctions.
GAI subutilities exhibit internal structures, FOPI, efficiency loss additive
approximation less severe, cases benefit accurate GAI model still
significant. Using additive approximation may reasonable approach GAI
structure fairly similar additive one, auction must terminate within
small number rounds.
tradeoff expressive compactness preference representation ubiquitous applications involving preferences. one hand, would like ask users
little possible information; other, users preference statements may accurate even meaningful cannot express important dependencies. problems
could useful experimentally compare accuracy GAI additive representations. experimental methodologies used study, particular generation
random structured utility functions, finding additive approximation GAI functions, may therefore prove applicable broader class preference research problems
tradeoff exists.
512

fiGAI Auctions

Acknowledgments
work supported part NSF grants IIS-0205435 IIS-0414710,
STIET program NSF IGERT grant 0114368. Yagil Engel supported part
Aly Kaufman fellowship Technion. thank anonymous reviewers many
useful comments suggestions.

Appendix A. Proofs Section 3.1
Lemma 2.
Let u(A) MVF representing preference differences, let X, Y, Z
define partition A. CDI(X, | Z) iff
u(A) = u(X 0 , Y, Z) + u(X, 0 , Z) u(X 0 , 0 , Z),
arbitrary instantiations X 0 , 0 .
Proof. Let X 0 , 0 arbitrary instantiations.
u(X, Y, Z) = u(X, Y, Z)u(X 0 , Y, Z)+u(X 0 , Y, Z) = u(X, 0 , Z)u(X 0 , 0 , Z)+u(X 0 , Y, Z)
second equality holds iff X 0 , 0 , CDI(X, | Z).
Theorem 3 (CDI-GAI Theorem).
Let G = (A, E) CDI map A,
{I1 , . . . , Ig } set overlapping maximal cliques.
u(A) =

g
"

fr (Ir ),

(A.1)

r=1


f1 = u([I1 ]),
r = 2, . . . , g,

fr = u([Ir ]) +

(A.2)

r1
"

"

(1)j

j=1

u([

1i1 <<ij <r

j
$

Iis Ir ]).

s=1

Proof. actually prove somewhat stronger result.
Claim. Let G CDI map utility function u(). Let Q = {C1 , . . . , Cw } denote set
maximal cliques G. Then,
u(A) =

w
"
(1)k+1
k=1

"
1i1 <<ik w

u([

k
$

Cis ]).

(A.3)

s=1

Let G0 = (A, E 0 ) complete graph nodes G. definition CDI
map, edge (x, y) E 0 \ E implies CDI(x, y). use induction series edge
removals. starting graph G0 , step remove edge E 0 \ E get
0
graph Gi . last step = |E 0 | |E| G|E ||E| = G. prove claim
513

fiEngel & Wellman

holds graph Gi . Since clique G0 , step 0, Q0 = {A}
claim trivially hold. Following process step 1 provides intuition final
decomposition obtained. pick pair nodes (x, y) CDI(x, y). use
notation = \ {a} A. Lemma 2 ,
u(A) = u(x, y, Ax,y )

(A.4)

= u(x0 , y, Ax,y ) + u(x, 0 , Ax,y ) u(x0 , 0 , Ax,y )
= u([Ax ]) + u([Ay ]) u([Ax Ay ]).
set maximal cliques G1 Q1 = {Ax , Ay }. Equation (A.4) shows (A.3)
holds Q1 .
proving induction step, assume (A.3) holds step i, show carry
step + 1. Let (x, y) denote edge removed step + 1. Let C1 , . . . , Cd (WLOG)
indicate sets Qi include x y. Similar (A.4), observe
u([C1 ]) = u([C1x ]) + u([C1y ]) u([C1x C1y ]).

(A.5)

Similarly k = 1, . . . , wi 1, 1 < i1 < < ik wi ,
u([

k
$

s=1

Cis C1 ]) = u([

k
$

s=1

Cis C1x ]) + u([

k
$

Cis C1y ]) u([

s=1

k
$

Cis C1x C1y ]). (A.6)

s=1

(A.3) (assumed hold step) term includes C1 substituted
according (A.5) (A.6). result (A.3) holding set (Qi \ {C1 })
{C1x , C1y }.
repeat operation C2 , . . . , Cd , define resulting collection
Qi+1 = (Qi \ {C1 , . . . , Cd }) {C1x , C1y , . . . , Cdx , Cdy }.
elements Qi+1 subsets elements Qi , maximal cliques Gi .
verify induction property:
element Qi+1 clique Gi+1 , difference Gi
Gi+1 removed edge (x, y), set Qi+1 includes x y.
clique C Qi+1 maximal, subset maximal clique
C Gi , either: (i) C C = C \ {x} (ii) x C C = C \ {y},
(iii)C = C. x longer connected C remains maximal cases.
maximal clique Gi+1 , C C Qi . either = C,
= C \ {x}, = C \ {y}, three cases element Qi+1 .
proves induction step.
result, last step decomposition (A.3) holds set Q = Q|E0 ||E| ,
set maximal cliques G, hence claim proved.
define f1 , . . . , fg according (A.2). claim, get (A.1) holds.

514

fiGAI Auctions

Appendix B. Proofs Section 6.1
B.1 Proving Lemma 5
Lemma 5. Let set configurations, within " maximizing profit
trader given price vector. Let = {r | , r {1, . . . , g}}.
consistent cover within g" maximizing profit prices.
show given suboptimal consistent cover , find suboptimal
member , contradicting premise lemma. traversing GAI
tree depth-first manner, step flip sub-configurations corresponding
elements subtree set sub-configurations source configuration
parent subtree (thus trimming subtree). This, show, results
another consistent cover also sub-optimal. Eventually obtain configuration
still suboptimal.
purpose introduce following notions:
operator turns set sub-configurations, consistent cover,
configuration:
{1 , . . . , g } = (1 , . . . , g ).
Let consistent cover . -source element r configuration
originated (meaning, r = r ).
operation trim replaces sub-configurations given configuration
corresponding set sub-configurations different configuration , according
following rules. Let denote indices GAI elements, corresponding
subtree GAI-tree, whose root GAI element Ii . Let denote consistent
cover . operation -trim defined elements
corresponding -source. Formally, exists ,
r , r = . Now, Let parent Ii , arbitrary element
outside disconnected rest graph. Let source
.
-trim(i , ) = {r | r
/ } {r |r }
replace r r corresponding sub-configuration ,
resulting configuration elements corresponding
-source parent Ii .
Lemma B.1. " = -trim(, ) consistent cover.
Proof. need show pair sub-configurations set {r | r
/ } {r |r
} consistent, assign value attribute appear
corresponding GAI elements.
sub-configurations {r |r } internally consistent mutual
-source . sub-configurations {r | r
/ } internally consistent

sub-configurations . Let r1 r2
/ denote indices GAI elements,
Ir1 Ir2 ,= . Now, Ir1 subtree whose root Ii , whereas Ir2 outside
subtree, path must go Ii parent . Due
515

fiEngel & Wellman

running intersection property GAI tree, Ir1 Ir2 Ii . corresponding subconfigurations must consistent also -source , hence r1
r2 must also consistent.
Lemma B.2. Let defined Lemma 5, let denote consistent cover
. " = -trim(i , ) (for i), ( " ) () + ".
Proof. Let denote single -source {r | r }. Let = =
{1, . . . , g} \ . ( " ) > () + ", (using Lemma 4)
( " ) = (" ) + (" ) > ( ) + ( ) + ",
= " ,

(" ) > ( ) + ".

Define following cover:
= {r" | r } {r | r }
consistent coveragain (as Lemma B.1) possible intersection
element " element (the root = ) parent
. corresponding sub-configurations i" must consistent following
argument: consistent appear together . i" consistent
-source definition -trim. Hence i" assign
values attributes Ii . consistent , must
i" . get
() = (" ) + ( ) > ( ) + ( ) + " = () + ".
last equation follows fact sub-configurations .
contradicts "-optimality .
Proof Lemma 5. Let 1 consistent cover contradicting lemma, meaning
( 1 ) max () g". first reorder GAI elements 1, . . . , g, according
order corresponding backtracking Depth-First-Search: is, starting
leftmost leaf, next move siblings, next parent, general children
node Ii visited, next element visited Ii . perform series g 1
-trim operations, resulting series 1 , . . . , g . that, must show
step operation -trim(i , ) valid, sub-configurations corresponding
mutual -source. Ii leaf, |i | = 1 hence elements
single source. Otherwise, result trimming subtrees children Ii , hence
definition -trim -source ii .
Now, consider resulting g . assumed ( 1 ) < max () g", hence
applications Lemma B.2 g 1 -trim operations, get ( g ) <
max () ". last element g elements mutual -source,
meaning g . Therefore, got contradiction "-optimality .

516

fiGAI Auctions

B.2 Proving Theorem 10
order prove Theorem 10 need several additional claims.
Lemma B.3. price least one sub-configuration must reduced every round
phase A.
Proof. round < phase exists active seller Bit = .
However active round t, Bit ,= . Let Bit . r.r Mt ,
definition . Therefore must r , Mt .
Lemma B.4. auction must reach phase B.
Proof. Lemma B.3 prices must go every round phase A. Lemma 7
ensures lower bound much prices reduced phase A, therefore
auction either terminates phase must reach condition [SWITCH].
set initial prices high max b1 () < 0, max bt () < 0
phase Lemma 7. Assumption A2 efficient allocation ( , ) provides
positive welfare, ( ) = bt ( ) + ( ) > 0. si SB therefore leave
auction ( ) < 0. happen bt ( ) > 0, therefore si
drop phase A. Phase continues long least one seller active,
auction cannot terminate reaching condition [SWITCH].
Finally, following lemma states two sellers, potential surplus
first one drop auction cannot significantly higher potential surplus
one stayed longer.
Lemma B.5. sellers si sj SB, si active least long sj active
phase B,
(i ) max j () (g + 1)".


Proof. SB definition phase B, sj drops > jT (j ). si
drop point (i ) " > jT (j ) ". , get
Corollary 6 that,
bT (i ) + (i ) max bT () + jT (j ) (g + 1)".


Corollary 8, jT (j ) = max jT (). Therefore
(i ) = bT (i ) + (i ) max bT () + max jT () (g + 1)" max j () (g + 1)".






Theorem 10. Given truthful buyer SB sellers, surplus final allocation
within (g + 1)" maximal surplus.
Proof. Lemma B.4 auction terminates allocation (si , ). Lemma
9, theorem immediate case winning seller si efficient seller. Otherwise
efficient seller sj dropped si . result immediate
Lemma B.5.
517

fiEngel & Wellman

B.3 Proving Theorem 12
first adapt Lemma 7, Lemma 9, Lemma B.5 use e + 1 instead g.
Lemma B.6. max bt () change round phase A.
Proof. Let G comprised trees G1 , . . . , Gh , let j" denote projection configuration
" tree Gj , let gj denote number GAI elements Gj .
Assume exists j" bt+1 (j" ) > bt (j" ). necessarily pt+1 (j" ) =

p (j" ) . happen must case w gj sub-configurations

j" Mtj , = w"
g . case, definition Mj ,
"
bt (j" ) < max bt (j ) gj .
j j
g
Therefore,
bt+1 (j" ) = (j" ) + = (j" ) +

w"
gj "
(j" ) +
< max bt (j ).
j j
g
g

true j" whose profit improves, therefore maxj j bt (j ) change
phase A.
max bt () = max




h
"

bt (j ) =

j=1

h
"
j=1

max bt (j ).

j j

last equality holds optimal values disconnected components GAI
tree independent other. result, max bt () well change
phase A.
Lemma B.7. SB seller si , (e + 1)"-efficient:
(i ) max () (e + 1)".


proof identical proof Lemma 9, replacing g e+1 using Corollary 11
instead Corollary 6.
Lemma B.8. sellers si sj SB, si active least long sj active
phase B,
(i ) max j () (e + 2)".


proof identical proof Lemma B.5, using Corollary 11 instead
Corollary 6.
Theorem 12. Given truthful buyer SB sellers, surplus final allocation
within (e + 2)" maximal surplus.
Proof. proof identical proof Theorem 10, replacing Lemmas 9 B.5
lemmas B.7 B.8, respectively.

518

fiGAI Auctions

B.4 Lemma 13 Theorem 14
Lemma 13. sellers SB, GAI auction payment sell-side (e + 2)"-VCG.
Proof. Trivially, consider winning seller si . case final price
buyers valuation payment ub (i ) exactly VCG payment. therefore
assume final price buyers valuation, payment
winning seller pT (i ) . Let sj second best seller. sj drops si ,
discount ", hence,
+ " > jT (j ) = max jT ().
(B.1)


Corollary 6,
ub (i ) pT (i ) max bT () (e + 1)".


Therefore (using (B.1) second inequality)
pT (i ) ub (i ) max b () + (e + 1)" <


ub (i ) max bT () + (e + 2)" max jT () ub (i ) max j () + (e + 2)". (B.2)






Bow sj survived auction discount ",
jT (j ) + ".
Meaning:
pT (j ) cj (j ) ".

(B.3)

Corollary 6
ub (i ) pT (i ) ub (j ) pT (j ) + (e + 1)".
Therefore (using (B.3) second inequality)
pT (i ) ub (i ) ub (j ) + pT (j ) (e + 1)"
ub (i ) (ub (j ) cj (j )) " (e + 1)" ub (i ) max j () (e + 2)". (B.4)


Equations (B.2) (B.4) place payment pT (i ) within (e + 2)" si VCG
payment.
Theorem 14. SB (3e + 5)" ex-post Nash equilibrium sellers GAI auction.
is, sellers cannot gain (3e+5)" deviating SB, given sellers
follow SB.
Let s1 play arbitrary strategy 1 SB sellers s2 , . . . , sn . s1 win
would clearly done worse using SB, therefore assume s1 wins 1 final price
p gains least (3e + 5)" trade. Let 2, . . . n. calculation
(B.2) assumed nothing winning traders strategy, therefore applies well:
p = pT (1 ) ub (1 ) max () + (e + 2)".


(B.5)

Next, define following cost function: c1 (1 ) = p (2e + 3)" c1 ( " ) = , " ,= 1 .
Assume s1 plays SB c1 .
519

fiEngel & Wellman

Claim. playing SB assuming cost c1 , s1 still winner, profit (wrt c1 ())
within (2e + 3)" profit playing 1 .
Proof. Clearly, s1 bids 1 . Let p() denote prices end phase new
final discount.
instance auction, let b () denote buyers profit, let
= 1 (1 ) = p(1 ) c1 (1 ) =
assume moment prices reach s1 limit,
p(1 ) (p (2e + 3)").
(for inequality, use pT (1 ) = ub (1 ) bT (1 ) also (B.5)),
= pT (1 ) p + (2e + 3)" > ub (1 ) (1 ) (ub (1 ) max () + (e + 2)") + (2e + 3)"

b


= max ()


bT (1 )

+ (e + 1)".
(B.6)

Let denote configuration chosen seller si end phase
new instance. Since instance, get bT (i ) bT (1 ) (e + 1)".
Therefore modify (B.6) state,
> (i ) (i ) = pT (i ) ci (i ),

b

(B.7)

meaning prices reached limit s1 , sellers dropped off. shows
s1 wins new instance well. Furthermore, lowest possible price paid s1
= p(1 ) (p (2e + 3)"), hence price least p (2e + 3)".
determined
Proof Theorem 14. Lemma 13:
p V CG(c1 , c2 , . . . , cn ) + (e + 2)".
Truthful reporting dominant strategy sellers one-sided VCG auctions. Therefore
V CG(c1 , c2 , . . . , cn ) V CG(c1 , c2 , . . . , cn ).
result claim get
p p + (2e + 3)" V CG(c1 , c2 , . . . , cn ) + (3e + 5)".
Therefore playing 1 , s1 could gained (3e + 5)" worstcase payoff playing SB respect true cost c1 .

Appendix C. Proofs Section 6.2
Theorem 15. computation Mt performed time O(g|I|2 ). Moreover,
total time spent task throughout auction O(g|I|(|I| + )).
Proof. simplicity notations assume single (connected) GAI-tree.
extension multiple connected components immediate Mjt computed
separately.
functions ub pt GAI form, hence function bt = ub pt
GAI form. noted (Boutilier et al., 2001), functions
520

fiGAI Auctions

GAI form optimized using variable elimination schemes cost networks (Dechter,
1997). fact, GAI structure already tree, case optimization linear
size domain |I|. However, Mt includes sub-configurations
configurations within " max b (). find it, must find maximum bt , add
sub-configurations Mt , find best configuration already (that
is, maximal \ ) on. done following procedure, adapted
work Nilsson (1998):
1. i=1,. . . , g:
Define = { | 1 , . . . , i1 Mt
/ Mt }.
Find = arg maxi bt ().
2. best configuration \ = arg maxi=1,...,g bt ( ) (which means, configuration least one sub-configuration Mt ).
bt ( ) max bt () ", sub-configuration already Mt
added Mt . Otherwise, Mt ready.
procedure performs g optimizations, takes linear size domain. amounts O(g|I|). time procedure done, either least one
sub-configuration added Mt , Mt ready. Therefore number times
procedure done per round bounded number sub-configurations |I| plus one,
giving O(g|I|2 ) bound. Moreover, Mt monotonically increasing auction.
round, start Mt computed previous round. Throughout auction,
application procedure either yields new sub-configuration Mt , terminates
round, total number times procedure performed throughout auction
bounded |I| + , leading overall bound O(g|I|(|I| + )).

Appendix D. Relating MUI condition Complements
Substitutes
definitions utility independence (UI) condition MUI found elsewhere
(Keeney & Raiffa, 1976).
Definition 19. MUI-factor set MUI attributes solution
1+k =

n
&
(1 + kki ).
i=1

Keeney Raiffa (1976) (KR) show one MUI-factor addition
zero (Appendix 6B text). ensures soundness following adaptation
MUI representation theorem:9
Theorem D.1. Let set MUI attributes.
1. MUI-factor zero, u(A) =

%n

i=1 ki ui (ai ).

9. theorem adapted book Keeney Raiffa (1976), Theorem 6.1, page 289.

521

fiEngel & Wellman

2. Otherwise, let k ,= 0 MUI-factor.
!n
[kki ui (ai ) + 1] 1
u(A) = i=1
.
k

(D.1)

KR go point k > 0 define u" (A) = 1 + ku(A), strategically
equivalent function u(), turn (D.1) multiplicative representation.
done similar fashion k < 0. Further, show MUI known exist,
one elicitation query sufficient order determine whether form function
additive multiplicative.
following relationship allow us interpret MUI factor respect complements substitutes. result generalizes formalizes intuition given KR
case MUI two attributes.
Theorem D.2. Let set MUI attributes, MUI-factor k ,= 0.
k > 0 iff pairs attributes complements, k < 0 iff pairs
attributes substitutes.
Proof. proof based work Keeney Raiffa (1976), Theorem 6.1, explained below.
Assume u() normalized u(A0 ) = 0. attribute A, let
= {a}, know U I(a, a). Utility independence form leads following
functional form: exist functions f g that,
u(A) = f (a) + g(a)u(a, a0 )
instantiate form assignment a0 get
u(a0 , a) = f (a) + g(a)u(a0 , a0 ) = f (a)
Hence f (a) = u(a0 , a), g(a) =
u(A0 ) = 0, get

u(A)u(a0 ,a)
u(a,a0 )

g(a) =

(this development done KR).

u(A) u(a0 , a)
.
u(a, a0 ) u(a0 , a0 )

(D.2)

proof Theorem 6.1, KR define MUI-factor follows:
k=

g(a) 1
u(a0 , a)

denominator always positive. Furthermore, shown (D.2), g(a) > 1,
u(A) u(a0 , a) > u(a, a0 ) u(a0 , a0 ). particular means b a, b
complements, inequality holds holding fixed attributes b.
Similarly, g(a) < 1, b substitutes. Putting pieces together,
get desired result.

522

fiGAI Auctions

Figure 6: Efficiency AP function number sampling points used devise
additive approximation.

Appendix E. Optimal Regression Using Small Sample
show experiment supporting claim Section 7.3: larger set sampling
points one used linear regression utility function cannot improve
efficiency AP. Figure 6 shows efficiency AP function number
sampling points used, largest domain used experiments: 25 attributes
= 4 (e = 9 = 5). Similar results shown distributions FOPI
preferences. chart result 150 experiments 10 points x-axis,
largest number tests used.

References
Bacchus, F., & Grove, A. (1995). Graphical models preference utility. 11th
Conference Uncertainty Artificial Intelligence, pp. 310, Montreal.
Beil, D. R., & Wein, L. M. (2003). inverse-optimization-based auction multiattribute
RFQs. Management Science, 49, 15291545.
Bichler, M. (2001). Future e-Markets: Multi-Dimensional Market Mechanisms. Cambridge University Press.
Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: directed graphical
representation conditional utilities. 17th Conference Uncertainty Artificial
Intelligence, pp. 5664, Seattle.
Branco, F. (1997). design multidimensional auctions. RAND Journal Economics,
28 (1), 6381.
Braziunas, D., & Boutilier, C. (2005). Local utility elicitation GAI models. 21st
Conference Uncertainty Artificial Intelligence, pp. 4249, Edinburgh.
Che, Y.-K. (1993). Design competition multidimensional auctions. RAND Journal
Economics, 24 (4), 668680.
David, E., Azoulay-Schwartz, R., & Kraus, S. (2002). English auction protocol multiattribute items. Agent Mediated Electronic Commerce IV: Designing Mechanisms
Systems, Vol. 2531 Lecture Notes Artificial Intelligence, pp. 5268. Springer.
523

fiEngel & Wellman

Debreu, G. (1959). Topological methods cardinal utility theory. Arrow, K. J., Karlin,
S., & Suppes, P. (Eds.), Mathematical Methods Social Sciences, pp. 1626.
Stanford University Press.
Dechter, R. (1997). Mini-buckets: general scheme generating approximations automated reasoning. 15th International Joint Conference Artificial Intelligence,
pp. 12971303, Nagoya.
Dyer, J. S., & Sarin, R. K. (1979). Measurable multiattribute value functions. Operations
Research, 27, 810822.
Engel, Y. (2008). Structured Preference Representation Multiattribute Auctions. Ph.D.
thesis, University Michigan, Ann Arbor, MI.
Engel, Y., & Wellman, M. P. (2007). Generalized value decomposition structured
multiattribute auctions. 8th ACM Conference Electronic Commerce, pp. 227
236, San Diego.
Engel, Y., Wellman, M. P., & Lochner, K. M. (2006). Bid expressiveness clearing
algorithms multiattribute double auctions. 7th ACM Conference Electronic
Commerce, pp. 110119, Ann Arbor.
Fishburn, P. C. (1967). Interdependence additivity multivariate, unidimensional
expected utility theory. International Economic Review, 8, 335342.
Gonzales, C., & Perny, P. (2004). GAI networks utility elicitation. 9th International
Conference Principles Knowledge Representation Reasoning, pp. 224234,
Whistler, BC.
Hyafil, N., & Boutilier, C. (2006). Regret-based incremental partial revelation mechanisms.
21st National Conference Artificial Intelligence, pp. 672678, Boston, MA.
Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences
Value Tradeoffs. Wiley.
Koppius, O. (2002). Information Architecture Electronic Market Performance. Ph.D.
thesis, Erasmus University, Rotterdam, Netherlands.
Krantz, D. H., Luce, R. D., Suppes, P., & Tversky, A. (1971). Foundations Measurement,
Vol. 1. Academic Press, New York.
Milgrom, P. (2000). Putting auction theory work: simultaneous ascending auction.
Journal Political Economy, 108, 245272.
Nilsson, D. (1998). efficient algorithm finding probable configurations
probabilistic expert systems. Statistics Computing, 8 (2), 159173.
Parkes, D. C., & Kalagnanam, J. (2005). Models iterative multiattribute procurement
auctions. Management Science, 51, 435451.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann.
Robu, V., Somefun, D. J. A., & La Poutre, J. A. (2005). Modeling complex multi-issue negotiations using utility graphs. 4th International Joint Conference Autonomous
Agents Multi-Agent Systems, pp. 280287, Utrecht.
524

fiGAI Auctions

Sandholm, T. (2007). Expressive commerce application sourcing: conducted $35 billion generalized combinatorial auctions. AI Magazine, 28 (3), 4558.
Sandholm, T., & Suri, S. (2006). Side constraints non-price attributes markets.
Games Economic Behavior, 55, 321330.
von Neumann, J., & Morgenstern, O. (1944). Theory Games Economic Behavior.
Princeton University Press.
Von Winterfeldt, D., & Edwards, W. (1986). Decision Analysis Behavioral Research.
Cambridge University Press.
Vulkan, N., & Jennings, N. R. (2000). Efficient mechanisms supply services
multi-agent environments. Decision Support Systems, 28, 519.
Wellman, M. P., Greenwald, A., & Stone, P. (2007). Autonomous Bidding Agents: Strategies
Lessons Trading Agent Competition. MIT Press.
Wellman, M. P., Osepayshvili, A., MacKie-Mason, J. K., & Reeves, D. M. (2008). Bidding strategies simultaneous ascending auctions. B. E. Journal Theoretical
Economics (Topics), 8 (1).

525

fiJournal Artificial Intelligence Research 37 (2010) 279-328

Submitted 05/09; published 03/10

Join-Graph Propagation Algorithms
Robert Mateescu

ROMATEES @ MICROSOFT. COM

Microsoft Research
7 J J Thomson Avenue
Cambridge CB3 0FB, UK

Kalev Kask

KKASK @ ICS . UCI . EDU

Donald Bren School Information Computer Science
University California Irvine
Irvine, CA 92697, USA

Vibhav Gogate

VGOGATE @ CS . WASHINGTON . EDU

Computer Science & Engineering
University Washington, Seattle
Seattle, WA 98195, USA

Rina Dechter

DECHTER @ ICS . UCI . EDU

Donald Bren School Information Computer Science
University California Irvine
Irvine, CA 92697, USA

Abstract
paper investigates parameterized approximate message-passing schemes based
bounded inference inspired Pearls belief propagation algorithm (BP). start
bounded inference mini-clustering algorithm move iterative scheme called Iterative
Join-Graph Propagation (IJGP), combines iteration bounded inference. Algorithm
IJGP belongs class Generalized Belief Propagation algorithms, framework allowed
connections approximate algorithms statistical physics shown empirically surpass performance mini-clustering belief propagation, well number stateof-the-art algorithms several classes networks. also provide insight accuracy
iterative BP IJGP relating algorithms well known classes constraint propagation
schemes.

1. Introduction
Probabilistic inference principal task Bayesian networks known NP-hard
problem (Cooper, 1990; Roth, 1996). commonly used exact algorithms jointree clustering (Lauritzen & Spiegelhalter, 1988; Jensen, Lauritzen, & Olesen, 1990) variableelimination (Dechter, 1996, 1999; Zhang, Qi, & Poole, 1994), recently search schemes
(Darwiche, 2001; Bacchus, Dalmao, & Pitassi, 2003; Dechter & Mateescu, 2007) exploit network structure. significant advances made last decade exact algorithms, many
real-life problems big hard, especially structure dense, since
time space exponential treewidth graph. Approximate algorithms therefore
necessary many practical problems, although approximation within given error bounds also
NP-hard (Dagum & Luby, 1993; Roth, 1996).

c
2010
AI Access Foundation. rights reserved.

fiM ATEESCU , K ASK , G OGATE & ECHTER

paper focuses two classes approximation algorithms task belief updating.
inspired Pearls belief propagation algorithm (Pearl, 1988), known exact
trees. distributed algorithm, Pearls belief propagation also applied iteratively
networks contain cycles, yielding Iterative Belief Propagation (IBP), also known loopy belief
propagation. networks contain cycles, IBP longer guaranteed exact,
many cases provides good approximations upon convergence. notable success cases
IBP coding networks (McEliece, MacKay, & Cheng, 1998; McEliece & Yildirim,
2002), version IBP called survey propagation classes satisfiability problems
(Mezard, Parisi, & Zecchina, 2002; Braunstein, Mezard, & Zecchina, 2005).
Although performance belief propagation far well understood general,
one promising avenues towards characterizing behavior came analogies
statistical physics. shown Yedidia, Freeman, Weiss (2000, 2001) belief propagation converge stationary point approximate free energy system, called
Bethe free energy. Moreover, Bethe approximation computed pairs variables terms,
therefore simplest version general Kikuchi (1951) cluster variational method,
computed clusters variables. observation inspired class Generalized
Belief Propagation (GBP) algorithms, work passing messages clusters variables.
mentioned Yedidia et al. (2000), many GBP algorithms correspond
Kikuchi approximation. version based region graphs, called canonical authors,
presented Yedidia et al. (2000, 2001, 2005). algorithm Iterative Join-Graph Propagation
member GBP class, although described language region graphs.
approach similar independently developed McEliece Yildirim
(2002). information BP state art research see recent survey Koller (2010).
first present mini-clustering scheme anytime bounded inference scheme
generalizes mini-bucket idea. viewed belief propagation algorithm tree
obtained relaxation networks structure (using technique variable duplication).
subsequently present Iterative Join-Graph Propagation (IJGP) sends messages
clusters allowed form cyclic structure.
two schemes investigate: (1) quality bounded inference anytime
scheme (using mini-clustering); (2) virtues iterating messages belief propagation type
algorithms, result combining bounded inference iterative message-passing (in IJGP).
background section 2, overview Tree-Decomposition scheme forms basis
rest paper. relaxing two requirements tree-decomposition, connectedness (via mini-clustering) tree structure (by allowing cycles underlying graph),
combine bounded inference iterative message-passing basic tree-decomposition
scheme, elaborated subsequent sections.
Section 3 present partitioning-based anytime algorithm called Mini-Clustering (MC),
generalization Mini-Buckets algorithm (Dechter & Rish, 2003). messagepassing algorithm guided user adjustable parameter called i-bound, offering flexible tradeoff
accuracy efficiency anytime style (in general higher i-bound, better
accuracy). MC algorithm operates tree-decomposition, similar Pearls belief propagation algorithm (Pearl, 1988) converges two passes, tree. contribution
beyond works area (Dechter & Rish, 1997; Dechter, Kask, & Larrosa, 2001) in: (1)
Extending partition-based approximation belief updating mini-buckets general treedecompositions, thus allowing computation updated beliefs variables once.
280

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

extension similar one proposed Dechter et al. (2001), replaces optimization
probabilistic inference. (2) Providing empirical evaluation demonstrates effectiveness
idea tree-decomposition combined partition-based approximation belief updating.
Section 4 introduces Iterative Join-Graph Propagation (IJGP) algorithm. operates
general join-graph decomposition may contain cycles. also provides user adjustable i-bound
parameter defines maximum cluster size graph (and hence bounds complexity),
therefore anytime iterative. algorithm IBP typically presented generalization Pearls Belief Propagation algorithm, show IBP viewed IJGP
smallest i-bound.
also provide insight IJGPs behavior Section 4. Zero-beliefs variable-value pairs
zero conditional probability given evidence. show that: (1) value variable
assessed zero-belief iteration IJGP, remains zero-belief subsequent
iterations; (2) IJGP converges finite number iterations relative set zero-beliefs; and,
importantly (3) set zero-beliefs decided iterative belief propagation
methods sound. Namely zero-belief determined IJGP corresponds true zero conditional probability relative given probability distribution expressed Bayesian network.
Empirical results various classes problems included Section 5, shedding light
performance IJGP(i). see often superior, otherwise comparable,
state-of-the-art algorithms.
paper based part earlier conference papers Dechter, Kask, Mateescu (2002),
Mateescu, Dechter, Kask (2002) Dechter Mateescu (2003).

2. Background
section provide background exact approximate probabilistic inference algorithms
form basis work. present algorithms context directed probabilistic networks, applicable graphical model, including Markov networks.
2.1 Preliminaries
Notations: reasoning problem defined terms set variables taking values finite
domains set functions defined variables. denote variables subsets
variables uppercase letters (e.g., X, Y, Z, S, R . . .) values variables lower case letters
(e.g., x, y, z, s). assignment (X1 = x1 , . . . , Xn = xn ) abbreviated x = (x1 , . . . , xn ).
subset variables S, DS denotes Cartesian product domains variables S. xS
projection x = (x1 , . . . , xn ) subset S. denote functions letters f , g, h, etc.,
scope (set arguments) function f scope(f ).
EFINITION 1 (graphical model) (Kask, Dechter, Larrosa, & Dechter, 2005) graphical model
3-tuple, = hX, D, Fi, where: X = {X1 , . . . , Xn } finite set variables; =
{D1 , . . . , Dn } set respective finite domains values; F = {f1 , . . . , fr } set
positive real-valued discrete functions, defined subset variables Si X, called
scope, denoted scope(f
P ). graphical model typically associated combination
1
operator , (e.g., {, } - product, sum). graphical model represents combination
1. combination operator also defined axiomatically (Shenoy, 1992).

281

fiM ATEESCU , K ASK , G OGATE & ECHTER

functions: ri=1 fi . graphical model associated primal graph captures
structural information model:
EFINITION 2 (primal graph, dual graph) primal graph graphical model undirected graph variables vertices edge connects two vertices whose corresponding variables appear scope function. dual graph graphical model
one-to-one mapping vertices functions graphical model. Two vertices
dual graph connected corresponding functions graphical model share variable.
denote primal graph G = (X, E), X set variables E set
edges.
EFINITION 3 (belief networks) belief (or Bayesian) network graphical model B =
hX, D, G, P i, G = (X, E) directed acyclic graph variables X P = {pi },
pi = {p(Xi | pa (Xi ) ) } conditional probability tables (CPTs) associated variable Xi pa(Xi ) = scope(pi ){Xi } set parents Xi G. Given subset variables
S, write P (s) probability P (S = s), DS . belief network represents
probability distribution X, P (x1 , . . . ., xn ) = ni=1 P (xi |xpa(Xi ) ). evidence set e
instantiated subset variables. primal graph belief network called moral graph.
obtained connecting parents vertex G removing directionality
edges. Equivalently, connects two variables appearing family (a variable
parents CPT).
Two common queries Bayesian networks Belief Updating (BU) Probable Explanation (MPE).
EFINITION 4 (belief network queries) Belief Updating (BU) task find posterior
probability single variable given evidence e, compute P (Xi |e).
Probable Explanation (MPE) task find complete assignment variables maximum probability given evidence, compute argmaxX pi .
2.2 Tree-Decomposition Schemes
Tree-decomposition heart general schemes solving wide range automated
reasoning problems, constraint satisfaction probabilistic inference. basis
many well-known algorithms, join-tree clustering bucket elimination. presentation follow terminology Gottlob, Leone, Scarcello (2000) Kask et al. (2005).
EFINITION 5 (tree-decomposition, cluster-tree) Let B = hX, D, G, P belief network.
tree-decomposition B triple hT, , i, = (V, E) tree, labeling
functions associate vertex v V two sets, (v) X (v) P satisfying:
1. function pi P , exactly one vertex v V pi (v),
scope(pi ) (v).
2. variable Xi X, set {v V |Xi (v)} induces connected subtree .
also called running intersection (or connectedness) property.
often refer node functions cluster use term tree-decomposition
cluster-tree interchangeably.
282

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

EFINITION 6 (treewidth, separator, eliminator) Let = hT, , tree-decomposition
belief network B. treewidth (Arnborg, 1985) maxvV |(v)| 1. treewidth
B minimum treewidth tree-decompositions. Given two adjacent vertices u
v tree-decomposition, separator u v defined sep(u, v) = (u) (v),
eliminator u respect v elim(u, v) = (u) (v). separator-width
max(u,v) |sep(u, v)|. minimum treewidth graph G shown identical related
parameter called induced-width (Dechter & Pearl, 1987).
Join-tree cluster-tree elimination (CTE) Bayesian network constraint satisfaction communities, used tree-decomposition method join-tree decomposition (Lauritzen
& Spiegelhalter, 1988; Dechter & Pearl, 1989), introduced based relational database concepts
(Maier, 1983). decompositions generated embedding networks moral graph G
chordal graph, often using triangulation algorithm using maximal cliques nodes
join-tree. triangulation algorithm assembles join-tree connecting maximal cliques
chordal graph tree. Subsequently, every CPT pi placed one clique containing scope.
Using previous terminology, join-tree decomposition belief network B = hX, D, G, P
0
tree = (V, E), V set cliques chordal graph G contains G, E set
edges form tree cliques, satisfying running intersection property (Maier, 1983).
join-tree satisfies properties tree-decomposition therefore cluster-tree (Kask
et al., 2005). paper, use terms tree-decomposition join-tree decomposition
interchangeably.
variants processing join-trees belief updating (e.g., Jensen et al., 1990;
Shafer & Shenoy, 1990). adopt version Kask et al. (2005), called cluster-treeelimination (CTE), applicable tree-decompositions general geared towards space
savings. message-passing algorithm; task belief updating, messages computed
summation eliminator two clusters product functions originating cluster. algorithm, denoted CTE-BU (see Figure 1), pays special attention
processing observed variables since presence evidence central component belief
updating. cluster sends message neighbor, algorithm operates functions
cluster except message particular neighbor. message contains single combined function individual functions share variables relevant eliminator.
non-individual functions combined product summed eliminator.
Example 1 Figure 2a describes belief network Figure 2b join-tree decomposition it.
Figure 2c shows trace running CTE-BU evidence G = ge , h(u,v) message
cluster u sends cluster v.
HEOREM 1 (complexity CTE-BU) (Dechter et al., 2001; Kask et al., 2005) Given Bayesian
network B = hX, D, G, P tree-decomposition hT, , B, time complexity CTE
BU O(deg (n + N ) dw +1 ) space complexity O(N dsep ), deg maximum
degree node tree-decomposition, n number variables, N number nodes
tree-decomposition, maximum domain size variable, w treewidth sep
maximum separator size.

283

fiM ATEESCU , K ASK , G OGATE & ECHTER

Algorithm CTE Belief-Updating (CTE-BU)
Input: tree-decomposition hT, , i, = (V, E) B = hX, D, G, P i. Evidence variables
var(e).
Output: augmented tree whose nodes clusters containing original CPTs
messages received neighbors. P (Xi , e), Xi X.
Denote H(u,v) message vertex u v, nev (u) neighbors u excluding v,
cluster(u) = (u) {H(v,u) |(v, u) E},
clusterv (u) = cluster(u) excluding message v u.

Compute messages:
every node u , u received messages nev (u), compute message node
v:
1. Process observed variables:
Assign relevant evidence pi (u)
2. Compute combined function:
X

h(u,v) =



f

elim(u,v) f

set functions clusterv (u) whose scope intersects elim(u, v).
Add h(u,v) H(u,v) add individual functions clusterv (u)
Send H(u,v) node v.
Compute P (Xi , e):
every Xi X let u vertex Xi (u). Compute P (Xi , e) =
P
Q
(u){Xi } ( f cluster(u) f )

Figure 1: Algorithm Cluster-Tree-Elimination Belief Updating (CTE-BU).


1

(1) = { A, B, C}
(1) = { p(a ), p(b | ), p(c | a, b)}

1

ABC
BC

B

2
C



( 2) = { B , C , , F }
(2) = { p(d | b), p( f | c, }

2 BCDF

E

BF

3

(3) = {B, E , F }
(3) = { p (e | b, f )}

4

(4) = {E , F , G}
(4) = { p( g | e, f )}

3

F
G

(a)

BEF
EF

4

(b)

EFG

h (1 , 2 ) ( b , c ) =


(b , c ) =


h ( 2 ,1 )

p ( ) p (b | ) p (c | , b )
p ( | b ) p ( f | c , ) h( 3, 2 ) (b , f )

d,f

h( 2 ,3 ) (b , f ) =


h( 3 , 2 ) ( b , f ) =
e

p (d | b)

h( 3 , 4 ) ( e , f ) =

p ( e | b , f ) h( 2 ,3 ) (b , f )

c ,d



p ( f | c, )

h (1 , 2 ) ( b , c )

p ( e | b , f ) h( 4 ,3 ) ( e , f )

b

h( 4 ,3 ) ( e , f ) = p ( G = g e | e , f )

(c)

Figure 2: (a) belief network; (b) join-tree decomposition; (c) Execution CTE-BU.

3. Partition-Based Mini-Clustering
time, especially space complexity, CTE-BU renders algorithm infeasible problems large treewidth. introduce Mini-Clustering, partition-based anytime algorithm
computes bounds approximate values P (Xi , e) every variable Xi .
284

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Procedure MC Belief Updating (MC-BU(i))
2. Compute combined mini-functions:
Make (i)-size mini-cluster partitioning clusterv (u), {mc(1), . . . , mc(p)};
P
Q
h1(u,v) = elim(u,v) f mc(1) f
Q
hi(u,v) = maxelim(u,v) f mc(i) f = 2, . . . , p
add {hi(u,v) |i = 1, . . . , p} H(u,v) . Send H(u,v) v.
Compute upper bounds P (Xi , e) P (Xi , e):
every Xi X let u V cluster Xi (u). Make (i) mini-clusters
cluster(u), {mc(1), . . . , mc(p)}; Compute P (Xi , e) =
P
Q
Qp
Q
( (u)Xi f mc(1) f ) ( k=2 max(u)Xi f mc(k) f ).

Figure 3: Procedure Mini-Clustering Belief Updating (MC-BU).
3.1 Mini-Clustering Algorithm
Combining functions cluster product complexity exponential number
variables, upper bounded induced width. Similar mini-bucket scheme
(Dechter, 1999), rather performing expensive exact computation, partition cluster p mini-clusters mc(1), . . . , mc(p),
Pi variables,
Q accuracy parameter. Instead computing CTE-BU h(u,v) =
elim(u,v)
f (u) f , divide functions
(u)
mc(k), k {1, . . . , p}, rewrite h(u,v) =
P p mini-clusters
Qp Q
P
Q
f
=
mc(k) f . migrating summation operator
elim(u,v)
elim(u,v)
f (u)
Q
P k=1 fQ
p
mini-cluster, yielding k=1 elim(u,v) f mc(k) f , get upper bound h(u,v) .
resulting algorithm called MC-BU(i).
Consequently, combined functions approximated via mini-clusters, follows. Suppose
u V received messages neighbors v (the message v ignored even
received). functions clusterv (u) combined partitioned mini-clusters
{mc(1), . . . , mc(p)}, one containing variables. mini-cluster processed
summation eliminator, resulting combined functions well individual
functions sent v. shown Dechter Rish (2003) upper bound
improved using maximization operator max rather summation operator sum
mini-buckets. Similarly, lower bounds generated replacing sum min (minimization)
mini-buckets. Alternatively, replace sum mean operator (taking sum
dividing number elements sum), case deriving approximation joint
belief instead strict upper bound.
Algorithm MC-BU upper bounds obtained CTE-BU replacing step 2
main loop final part computing upper bounds joint belief procedure given
Figure 3. implementation used experiments reported here, partitioning
done greedy brute-force manner. ordered functions according sizes (number
variables), breaking ties arbitrarily. largest function placed mini-cluster itself. Then,
picked largest remaining function probed mini-clusters order creation,
285

fiM ATEESCU , K ASK , G OGATE & ECHTER

1

ABC

BC

H (1, 2)

h(11, 2 ) (b, c) := p (a ) p (b | ) p(c | a, b)

1
( 2 ,1)

h

H ( 2,1)

(b) := p (d | b) h(13, 2 ) (b, f )
d, f

h(22,1) (c) := max p ( f | c, )
d, f

2

BCDF
1
( 2 , 3)

h

H ( 2 , 3)
BF

3

BEF

EF

c ,d

h(22,3) ( f ) := max p( f | c, )
c,d

1
( 3, 2 )

(b, f ) := p(e | b, f ) h(14,3) (e, f )

H ( 3, 2 )

h

H ( 3, 4 )

h(13, 4 ) (e, f ) := p (e | b, f ) h(12,3) (b) h(22,3) ( f )

H ( 4 , 3)

4

(b) := p (d | b) h(11, 2 ) (b, c)

e

b

1
( 4 , 3)

h

(e, f ) := p(G = g e | e, f )

EFG

Figure 4: Execution MC-BU = 3.
trying find one together new function would variables. new
mini-cluster created whenever existing ones could accommodate new function.
Example 2 Figure 4 shows trace running MC-BU(3) problem Figure 2. First, evidence G = ge assigned CPTs. individual functions sent cluster 1
cluster 2. Cluster 1 contains 3 variables,
(1) = {A, B, C}, therefore partitioned.
P
p(a)

p(b|a) p(c|a, b) computed message
combined function h1(1,2) (b, c) =

1
H(1,2) = {h(1,2) (b, c)} sent node 2. Now, node 2 send message node 3. Again,
individual functions. Cluster 2 contains 4 variables, (2) = {B, C, D, F }, partitioning necessary: MC-BU(3) choose
P mc(1) = {p(d|b), h(1,2) (b,2c)} mc(2) = {p(f |c, d)}.
1
combined functions h(2,3) (b) = c,d p(d|b) h(1,2) (b, c) h(2,3) (f ) = maxc,d p(f |c, d)
computed message H(2,3) = {h1(2,3) (b), h2(2,3) (f )} sent node 3. algorithm continues every node received messages neighbors. upper bound p(a, G = ge )
computed choosing cluster
1, contains variable A. doesnt need partitionP
ing, algorithm computes b,c p(a) p(b|a) p(c|a, b) h1(2,1) (b) h2(2,1) (c). Notice
unlike CTE-BU processes 4 variables cluster 2, MC-BU(3) never processes 3
variables time.
already shown that:
HEOREM 2 (Dechter & Rish, 2003) Given Bayesian network B = hX, D, G, P evidence e, algorithm MC-BU(i) computes upper bound joint probability P (Xi , e)
variable Xi (and values) evidence e.
HEOREM 3 (complexity MC-BU(i)) (Dechter et al., 2001) Given Bayesian network B =
hX, D, G, P tree-decomposition hT, , B, time space complexity MC-BU(i)
O(n hw di ), n number variables, maximum domain size variable
hw = maxuT |{f P |scope(f ) (u) 6= }|, bounds number mini-clusters.
286

fiJ OIN -G RAPH P ROPAGATION LGORITHMS








ff ff
ff





ff ff


fffi fffi




ff" ff"
#$%& 'ff " &



! ! !



ff"" ff""
()* $ && '%&&
!! !! !!


ff ff
!

fffi fffi
!



+

Figure 5: Node duplication semantics MC: (a) trace MC-BU(3); (b) trace CTE-BU.
Semantics Mini-Clustering mini-bucket scheme shown semantics relaxation via node duplication (Kask & Dechter, 2001; Choi, Chavira, & Darwiche, 2007).
extend mini-clustering showing apply messages flow one direction
(inward, leaves root), follows. Given tree-decomposition D, CTE-BU computes
function h(u,v) (the message cluster u sends cluster v), MC-BU(i) partitions cluster u p
mini-clusters u1 , . . . , , processed independently resulting functions h(ui ,v)
sent v. Instead consider different decomposition D0 , like D, exception (a) instead u, clusters u1 , . . . , , children v, variable
appearing single mini-cluster becomes new variable, (b) child w u (in D)
child uk (in D0 ), h(w,u) (in D) assigned uk (in D0 ) partitioning. Note
D0 legal tree-decomposition relative original variables since violates connectedness property: mini-clusters u1 , . . . , contain variables elim(u, v) path
nodes u1 , . . . , (this path goes v) not. However, legal tree-decomposition
relative new variables. straightforward see H(u,v) computed MC-BU(i)
{h(ui ,v) |i = 1, . . . , p} computed CTE-BU D0 direction leaves
root.
want capture semantics outward messages root leaves, need generate different relaxed decomposition (D00 ) MC, defined, allows different partitioning
streams cluster. could course stick decomposition
D0 use CTE directions would lead another variant mini-clustering.
Example 3 Figure 5(a) shows trace bottom-up phase MC-BU(3) network Figure
4. Figure 5(b) shows trace bottom-up phase CTE-BU algorithm problem obtained
problem Figure 4 splitting nodes (into D0 D00 ) F (into F 0 F 00 ).
MC-BU algorithm computes upper bound P (Xi , e) joint probability P (Xi , e).
However, deriving bound conditional probability P (Xi |e) easy exact
287

fiM ATEESCU , K ASK , G OGATE & ECHTER

Random Bayesian N=50 K=2 P=2 C=48
0.20
0.18
0.16

Avg abs error

0.14
0.12
0.10
0.08
#ev=0
#ev=10
#ev=20
#ev=30

0.06
0.04
0.02
0.00
0

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Number iterations

Figure 6: Convergence IBP (50 variables, evidence 0-30 variables).
value P (e) available. try divide (multiply) P (Xi , e) constant,
result
P necessarily upper bound P (Xi |e). easy show normalization,
P (xi , e)/ xi Di P (xi , e), mean operator identical normalization MC-BU output
applying summation operator mini-clusters.
MC-BU(i) improvement Mini-Bucket algorithm MB(i), allows computation P (Xi , e) variables single run, whereas MB(i) computes P (Xi , e)
one variable, single run. computing P (Xi , e) variable, MB(i) run
n times, variable, algorithm call nMB(i). demonstrated Mateescu
et al. (2002) MC-BU(i) linear speed-up nMB(i). given i, accuracy
MC-BU(i) shown worse nMB(i).
3.2 Experimental Evaluation Mini-Clustering
work Mateescu et al. (2002) Kask (2001) provides empirical evaluation MC-BU
reveals impact accuracy parameter quality approximation compares
Iterative Belief Propagation Gibbs sampling scheme. include subset
experiments provide essence results. Additional empirical evaluation
MC-BU given comparing IJGP later paper.
tested performance MC-BU(i) random Noisy-OR networks, random coding networks, general random networks, grid networks, three benchmark CPCS files 54, 360
422 variables respectively (these belief networks medicine, derived Computer based
Patient Case Simulation system, known hard belief updating). type network
ran Iterative Belief Propagation (IBP) - set run 30 iterations, Gibbs Sampling (GS)
MC-BU(i), 2 treewidth w capture anytime behavior MC-BU(i).
random networks generated using parameters (N,K,C,P), N number
variables, K domain size (we used K=2), C number conditional probability
tables P number parents conditional probability table. parents table
picked randomly given topological ordering, conditional probability tables filled
288

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

0
|e| 10
20

NHD
max

IBP

MC-BU(2)

MC-BU(5)

MC-BU(8)

0
0
0
0
0
0
0
0
0

mean
0
0
0
0
0
0
0
0
0
0
0
0

N=50, P=2, 50 instances
Abs. Error
max

1.6E-03
1.1E-03
5.7E-04
1.1E-03
7.7E-04
2.8E-04
3.6E-04
1.7E-04
3.5E-05

mean
9.0E-09
3.4E-04
9.6E-04
1.1E-03
8.4E-04
4.8E-04
9.4E-04
6.9E-04
2.7E-04
3.2E-04
1.5E-04
3.5E-05

Rel. Error

max

1.9E+00
1.4E+00
7.1E-01
1.4E+00
9.3E-01
3.5E-01
4.4E-01
2.0E-01
4.3E-02

mean
1.1E-05
4.2E-01
1.2E+00
1.3E+00
1.0E+00
5.9E-01
1.2E+00
8.4E-01
3.3E-01
3.9E-01
1.9E-01
4.3E-02

Time
max

0.056
0.048
0.039
0.070
0.063
0.058
0.214
0.184
0.123

mean
0.102
0.081
0.062
0.057
0.049
0.039
0.072
0.066
0.057
0.221
0.190
0.127

Table 1: Performance Noisy-OR networks, w = 10: Normalized Hamming Distance, absolute
error, relative error time.

randomly. grid networks structure square, edges directed form diagonal
flow (all parallel edges direction). generated specifying N (a square
integer) K (we used K=2). also varied number evidence nodes, denoted |e|
tables. parameter values reported table. problems, Gibbs sampling
performed consistently poorly include part results here.
experiments focused approximation power MC-BU(i). compared two
versions algorithm. first version, every cluster, used max operator
mini-clusters, except one processed summation. second version,
used operator mean mini-clusters. investigated second version algorithm
two reasons: (1) compare MC-BU(i) IBP Gibbs sampling, also
approximation algorithms, would possible compare bounding scheme; (2)
observed experiments that, although bounds improve i-bound increases, quality
bounds computed MC-BU(i) still poor, upper bounds greater 1 many
cases.2 Notice need maintain sum operator least one mini-clusters.
mean operator simply performs summation divides number elements sum.
example, A, B, C binary variables (taking values 0 1), f (A, B, C) aggregated
function one mini-cluster,
elim = {A, B}, computing message h(C) mean
P
operator gives: 1/4 A,B{0,1} f (A, B, C).
computed exact solution used three different measures accuracy: 1) Normalized
Hamming Distance (NHD) - picked likely value variable approximate
exact, took ratio number disagreements total number variables, averaged number problems ran class; 2) Absolute Error (Abs.
Error) - absolute value difference approximate exact, averaged
values (for variable), variables problems; 3) Relative Error (Rel. Error) -
absolute value difference approximate exact, divided exact,
averaged values (for variable), variables problems. coding networks,
2. Wexler Meek (2008) compared upper/lower bounding properties mini-bucket computing probability
evidence. Rollon Dechter (2010) investigated heuristic schemes mini-bucket partitioning.

289

fiM ATEESCU , K ASK , G OGATE & ECHTER

10
|e| 20
30

N=50, P=3, 25 instances
Abs. Error

NHD
max

mean
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

IBP
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

MC-BU(2)

MC-BU(5)

MC-BU(8)

MC-BU(11)

MC-BU(14)

max

1.3E-03
5.3E-04
2.3E-04
1.0E-03
4.6E-04
2.0E-04
6.6E-04
1.8E-04
3.4E-05
2.6E-04
3.8E-05
6.4E-07
4.2E-05
0
0

mean
1.3E-04
3.6E-04
6.8E-04
9.6E-04
4.0E-04
1.9E-04
8.3E-04
4.1E-04
1.9E-04
5.7E-04
1.8E-04
3.4E-05
2.4E-04
3.8E-05
6.4E-07
4.1E-05
0
0

Rel. Error
max

Time

mean
7.9E-01
2.2E+00
4.2E+00
5.8E+00
2.4E+00
1.2E+00
5.1E+00
2.4E+00
1.2E+00
3.5E+00
1.0E+00
2.1E-01
1.5E+00
2.3E-01
4.0E-03
2.4E-01
0
0

8.2E+00
3.1E+00
1.4E+00
6.4E+00
2.7E+00
1.2E+00
4.0E+00
1.1E+00
2.1E-01
1.6E+00
2.3E-01
4.0E-03
2.5E-01
0
0

max

mean
0.242
0.184
0.121
0.108
0.077
0.064
0.133
0.105
0.095
0.509
0.406
0.308
2.378
1.439
0.624
7.875
2.093
0.638

0.107
0.077
0.064
0.133
0.104
0.098
0.498
0.394
0.300
2.339
1.421
0.613
7.805
2.075
0.630

Table 2: Performance Noisy-OR networks, w = 16: Normalized Hamming Distance, absolute
error, relative error time.
Noisy-OR networks, N=50, P=3, evid=20, w*=16, 25 instances

Noisy-OR networks, N=50, P=3, evid=10, w*=16, 25 instances

1e+0

1e+0

MC
IBP
Gibbs Sampling

MC
IBP
Gibbs Sampling

1e-1

Absolute error

Absolute error

1e-1

1e-2

1e-3

1e-2

1e-3

1e-4

1e-4

1e-5

1e-5
0

2

4

6

8

10

12

14

16

0

2

4

6

8

10

12

14

16

i-bound

i-bound

Figure 7: Absolute error Noisy-OR networks.
report one measure, Bit Error Rate (BER). terms measures defined above, BER
normalized Hamming distance approximate (computed algorithm)
actual input (which case coding networks may different solution given
exact algorithms), denote differently make semantic distinction. also report
time taken algorithm. reported metrics (time, error, etc.) provided Tables,
give mean max values.
Figure 6 show IBP converges 5 iterations. So, experiments
report time 30 iterations, time even better sophisticated termination used.
results typical runs.

290

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Random networks, N=50, P=2, k=2, evid=0, w*=10, 50 instances

Random networks, N=50, P=2, k=2, evid=10, w*=10, 50 instances

0.16

0.16

MC
Gibbs Sampling
IBP

0.14

0.14

MC
Gibbs Sampling
IBP

0.12

0.10

Absolute error

Absolute error

0.12

0.08
0.06
0.04

0.10
0.08
0.06
0.04

0.02

0.02

0.00

0.00

0

2

4

6

8

10

0

2

4

i-bound

6

8

10

i-bound

Figure 8: Absolute error random networks.
BER

= .22
max
mean

IBP
GS
MC-BU(2)
MC-BU(4)
MC-BU(6)
MC-BU(8)

0.000
0.483
0.002
0.001
0.000
0.000

0.000
0.483
0.002
0.001
0.000
0.000

IBP
GS
MC-BU(2)
MC-BU(4)
MC-BU(6)
MC-BU(8)
MC-BU(10)

0.000
0.506
0.006
0.006
0.005
0.002
0.001

0.000
0.506
0.006
0.006
0.005
0.002
0.001

= .26
= .32
= .40
max
mean
max
mean
max
mean
N=100, P=3, 50 instances, w*=7
0.000 0.000 0.002 0.002 0.022 0.022
0.483 0.483 0.483 0.483 0.483 0.483
0.004 0.004 0.024 0.024 0.068 0.068
0.002 0.002 0.018 0.018 0.046 0.045
0.000 0.000 0.004 0.004 0.038 0.038
0.000 0.000 0.002 0.002 0.023 0.023
N=100, P=4, 50 instances, w*=11
0.000 0.000 0.002 0.002 0.013 0.013
0.506 0.506 0.506 0.506 0.506 0.506
0.015 0.015 0.043 0.043 0.093 0.094
0.017 0.017 0.049 0.049 0.104 0.102
0.011 0.011 0.035 0.034 0.071 0.074
0.004 0.004 0.022 0.022 0.059 0.059
0.001 0.001 0.008 0.008 0.033 0.032

= .51
max
mean

Time

0.088
0.483
0.132
0.110
0.106
0.091

0.088
0.483
0.131
0.110
0.106
0.091

0.00
31.36
0.08
0.08
0.12
0.19

0.075
0.506
0.157
0.158
0.151
0.121
0.101

0.075
0.506
0.157
0.158
0.150
0.122
0.102

0.00
39.85
0.19
0.19
0.29
0.71
1.87

Table 3: Bit Error Rate (BER) coding networks.

Random Noisy-OR networks results summarized Tables 1 2, Figure 7. NHD,
IBP MC-BU gave perfect results. measures, noticed IBP
accurate evidence order magnitude. However, evidence added,
IBPs accuracy decreases, MC-BUs increases give similar results. see
MC-BU gets better accuracy parameter increases, shows anytime behavior.
General random networks results summarized Figure 8. similar
random Noisy-OR networks. Again, IBP best result number evidence
variables small. remarkable quickly MC-BU surpasses performance IBP
evidence added (for more, see results Mateescu et al., 2002).
Random coding networks results given Table 3 Figure 9. instances fall within
class linear block codes, ( channel noise level). known IBP accurate
class. Indeed, problems experimented IBP outperformed
MC-BU throughout. anytime behavior MC-BU seen variation numbers
column vividly Figure 9.
291

fiM ATEESCU , K ASK , G OGATE & ECHTER

Coding networks, N=100, P=4, sigma=.51, w*=12, 50 instances

Coding networks, N=100, P=4, sigma=.22, w*=12, 50 instances

0.18

0.007
MC
IBP

0.006

MC
IBP

0.16

0.005

Bit Error Rate

Bit Error Rate

0.14
0.004
0.003
0.002

0.12

0.10

0.001

0.08
0.000

0.06
0

2

4

6

8

10

0

12

2

4

6

8

10

12

i-bound

i-bound

Figure 9: Bit Error Rate (BER) coding networks.
Grid 15x15, evid=10, w*=22, 10 instances

Grid 15x15, evid=10, w*=22, 10 instances

0.06

12
MC
IBP

0.05

MC
IBP

10

Time (seconds)

Absolute error

8
0.04

0.03

0.02

6

4

2
0.01

0

0.00
0

2

4

6

8

10

12

14

16

18

0

2

4

6

i-bound

8

10

12

14

16

18

i-bound

Figure 10: Grid 15x15: absolute error time.
Grid networks results given Figure 10. notice IBP accurate evidence
MC-BU better evidence added. behavior consistently manifested
smaller grid networks experimented (from 7x7 14x14).
CPCS networks results also tested three CPCS benchmark files. results given
Figure 11. interesting notice MC-BU scheme scales fairly large networks,
like real life example CPCS422 (induced width 23). IBP accurate
evidence, surpassed MC-BU evidence added. However, whereas MC-BU
competitive IBP time-wise i-bound small, runtime grows rapidly i-bound
increases. details benchmarks see results Mateescu et al. (2002).
Summary results show that, expected, IBP superior approximations
coding networks. However, random Noisy-OR, general random, grid networks CPCS
networks, presence evidence, mini-clustering scheme often superior even weakest form. empirical results particularly encouraging use un-optimized scheme
exploits universal principle applicable many reasoning tasks.

292

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

CPCS 422, evid=0, w*=23, 1 instance

CPCS 422, evid=10, w*=23, 1 instance

0.05

0.05
MC
IBP

MC
IBP

0.04

Absolute error

Absolute error

0.04

0.03

0.02

0.01

0.03

0.02

0.01

0.00

0.00
2

4

6

8

10

12

14

16

18

2

4

6

i-bound

8

10

12

14

16

18

i-bound

Figure 11: Absolute error CPCS422.

4. Join-Graph Decomposition Propagation
section introduce algorithm Iterative Join-Graph Propagation (IJGP) which, like miniclustering, designed benefit bounded inference, also exploit iterative message-passing
used IBP. Algorithm IJGP viewed iterative version mini-clustering, improving
quality approximation, especially low i-bounds. Given cluster decomposition,
mini-clustering potentially create different partitioning every message sent neighbor.
dynamic partitioning happen incoming message neighbor
excluded realizing partitioning, different set functions split mini-clusters
every message neighbor. define version mini-clustering every cluster
create unique static partition mini-clusters every incoming message included
one mini-clusters. version MC extended IJGP introducing
links mini-clusters cluster, carefully limiting interaction
resulting nodes order eliminate over-counting.
Algorithm IJGP works general join-graph may contain cycles. cluster size
graph user adjustable via i-bound (providing anytime nature), cycles graph
allow iterative application message-passing. Subsection 4.1 introduce join-graphs
discuss properties. Subsection 4.2 describe IJGP algorithm itself.
4.1 Join-Graphs
EFINITION 7 (join-graph decomposition) join-graph decomposition belief network B =
hX, D, G, P triple = hJG, , i, JG = (V, E) graph, labeling
functions associate vertex v V two sets, (v) X (v) P that:
1. pi P , exactly one vertex v V pi (v), scope(pi )
(v).
2. (connectedness) variable Xi X, set {v V |Xi (v)} induces connected
subgraph JG. connectedness requirement also called running intersection property.

293

fiM ATEESCU , K ASK , G OGATE & ECHTER

2,4

1,2,4

2,3,4



C
1,4

2,3,4



3,4

B

2,4

1,2,4

C
1,4

1,3,4

B

a)

3
1,3,4

b)

Figure 12: edge-labeled decomposition.
often refer node V CPT functions cluster3 use term joingraph decomposition cluster-graph interchangeably. Clearly, join-tree decomposition
cluster-tree special case join-graph tree.
clear one problems message propagation cyclic join-graphs overcounting. reduce problem devise scheme, avoids cyclicity respect
single variable. algorithm works edge-labeled join-graphs.
EFINITION 8 (minimal edge-labeled join-graph decompositions) edge-labeled join-graph
decomposition B = hX, D, G, P four-tuple = hJG, , , i, JG = (V, E)
graph, associate vertex v V sets (v) X (v) P
associates edge (v, u) E set ((v, u)) X that:
1. function pi P , exactly one vertex v V pi (v),
scope(pi ) (v).
2. (edge-connectedness) edge (u, v), ((u, v)) (u) (v), Xi X,
two clusters containing Xi connected path whose every edge label includes
Xi .
Finally, edge-labeled join-graph minimal variable deleted label
still satisfying edge-connectedness property.
EFINITION 9 (separator, eliminator edge-labeled join-graphs) Given two adjacent vertices
u v JG, separator u v defined sep(u, v) = ((u, v)), eliminator u
respect v elim(u, v) = (u) ((u, v)). separator width max(u,v) |sep(u, v)|.
Edge-labeled join-graphs made label minimal deleting variables labels
maintaining connectedness (if edge label becomes empty, edge deleted). easy
see that,
Proposition 1 minimal edge-labeled join-graph contain cycle relative single
variable. is, two clusters containing variable connected exactly one path
labeled variable.
Notice every minimal edge-labeled join-graph edge-minimal (no edge deleted),
vice-versa.
3. Note node may associated empty set CPTs.

294

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Example 4 example Figure 12a shows edge minimal join-graph contains cycle
relative variable 4, edges labeled separators. Notice however remove variable 4 label one edge cycles (relative single variables)
connectedness property still maintained.
mini-clustering approximation presented previous section works relaxing jointree requirement exact inference collection join-trees smaller cluster size.
introduces independencies original problem via node duplication applies exact inference relaxed model requiring two message passings. class IJGP algorithms
take different route. choose relax tree-structure requirement use join-graphs
introduce new independencies, apply iterative message-passing resulting cyclic structure.
Indeed, shown join-graph belief network I-map (independency map,
Pearl, 1988) underlying probability distribution relative node-separation. Since plan
use minimally edge-labeled join-graphs address over-counting problems, question
kind independencies captured graphs.
EFINITION 10 (edge-separation edge-labeled join-graphs) Let = hJG, , , i, JG =
(V, E) edge-labeled decomposition Bayesian network B = hX, D, G, P i. Let NW , NY
V two sets nodes, EZ E set edges JG. Let W, Y, Z corresponding
sets variables (W = vNW (v), Z = eEZ (e)). say EZ edge-separates NW
NY path NW NY JG graph whose edges EZ removed.
case also say W separated given Z D, write hW |Z|Y iD . Edgeseparation regular join-graph defined relative separators.
HEOREM 4 edge-labeled join-graph decomposition = hJG, , , belief network
B = hX, D, G, P I-map P relative edge-separation. Namely, edge separation
corresponds conditional independence P .
Proof: Let G moral graph BN . Since G I-map P , enough prove
JG I-map G. Let NW NY disjoint sets nodes NZ set edges JG,
W, Z, corresponding sets variables G. prove:
hNW |NZ |NY iJG = hW |Z|Y iM G
contradiction. Since sets W, Z, may disjoint, actually prove hW
Z|Z|Y ZiM G holds, equivalent hW |Z|Y iM G .
Supposing hW Z|Z|Y ZiM G false, exists path = 1 , 2 , . . . , n1 , = n
G goes variable = 1 W Z variable = n Z without
intersecting variables Z. Let Nv set nodes JG contain variable v X,
let us consider set nodes:
= ni=1 Ni NZ
argue forms connected sub-graph JG. First, running intersection property
ensures every Ni , = 1, . . . , n, remains connected JG removing nodes NZ
(otherwise, must path two disconnected parts original JG,
implies part Z, contradiction). Second, fact (i , i+1 ), =
295

fiM ATEESCU , K ASK , G OGATE & ECHTER

1, . . . , n 1, edge moral graph G implies conditional probability table
(CPT) i+1 , = 1, . . . , n 1 (and perhaps variables). property 1
definition join-graph, follows = 1, . . . , n 1 exists node JG
contains i+1 . proves existence path mutilated join-graph (JG
NZ pulled out) node NW containing = 1 node containing 1 2 (N1
connected), node one containing 2 3 (N2 connected),
reach node NY containing = n . shows hNW |NZ |NY iJG false,
concluding proof contradiction. 2
Interestingly however, deleting variables edge labels removing edges edge-labeled
join-graphs whose clusters fixed increase independencies captured edge-labeled
join-graphs. is,
Proposition 2 two (edge-labeled) join-graphs defined set clusters, sharing (V ,
, ), express exactly set independencies relative edge-separation, set
independencies identical one expressed node separation primal graph
join-graph.
Proof: follows looking primal graph join-graph (obtained connecting
two nodes cluster arc original variables nodes) observing edgeseparation join-graph corresponds node separation primal graph vice-versa. 2
Hence, issue minimizing computational over-counting due cycles appears unrelated problem maximizing independencies via minimal I-mapness. Nevertheless, avoid
over-counting much possible, still prefer join-graphs minimize cycles relative
variable. is, prefer minimal edge-labeled join-graphs.
Relationship region graphs strong relationship join-graphs
region graphs Yedidia et al. (2000, 2001, 2005). approach inspired advances
statistical physics, realized computing partition function essentially
combinatorial problem expresses probabilistic reasoning. result, variational methods
physics could counterparts reasoning algorithms. proved Yedidia et al. (2000,
2001) belief propagation loopy networks converge (when so) stationary
points Bethe free energy. Bethe approximation simplest case
general Kikuchi (1951) cluster variational method. idea group variables together
clusters perform exact computation cluster. One key question aggregate
results, account variables shared clusters. Again, idea
everything counted exactly important. led proposal region
graphs (Yedidia et al., 2001, 2005) associated counting numbers regions.
given possible canonical version graphs support Generalized Belief Propagation
(GBP) algorithms. join-graphs accomplish thing. edge-labeled join-graphs
described region graphs regions clusters labels edges.
tree-ness condition respect every variable ensures over-counting.
similar approach ours, also based join-graphs appeared independently
McEliece Yildirim (2002), based information theoretic perspective.

296

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Algorithm Iterative Join-Graph Propagation (IJGP)
Input arc-labeled join-graph decomposition hJG, , , i, JG = (V, E) B = hX, D, G, P i. Evidence variables var(e).
Output augmented graph whose nodes clusters containing original CPTs messages received
neighbors. Approximations P (Xi |e), Xi X.
Denote h(u,v) message vertex u v, nev (u) neighbors u JG excluding v.
cluster(u) = (u) {h(v,u) |(v, u) E}.
clusterv (u) = cluster(u) excluding message v u.
One iteration IJGP:
every node u JG topological order back,
1. Process observed variables:
Assign relevant evidence pi (u) (u) := (u) var(e), u V
2. Compute individual functions:
Include H(u,v) function clusterv (u) whose scope contain variables elim(u, v).
Denote remaining functions.
P
Q
3. Compute send v combined function: h(u,v) = elim(u,v) f f .
Send h(u,v) individual functions H(u,v) node v.
Endfor
Compute approximation P (Xi |e):
every Xi X let u
P vertex JG
Q Xi (u).
Compute P (Xi , e) = (u){Xi } ( f cluster(u) f )

Figure 13: Algorithm Iterative Join-Graph Propagation (IJGP).
4.2 Algorithm IJGP
Applying CTE iteratively minimal edge-labeled join-graphs yields algorithm Iterative JoinGraph Propagation (IJGP) described Figure 13. One iteration algorithm applies messagepassing topological order join-graph, forward back. node u sends message
(or messages) neighbor node v operates CPTs cluster messages
sent neighbors excluding ones received v. First, individual functions share
variables eliminator collected sent v. rest functions combined
product summed eliminator u v.
Based results Lauritzen Spiegelhalter (1988) Larrosa, Kask, Dechter
(2001) shown that:
HEOREM 5
1. IJGP applied join-tree decomposition reduces join-tree clustering, therefore guaranteed compute exact beliefs one iteration.

2. time complexity one iteration IJGP O(deg (n + N ) dw +1 ) space
complexity O(N ), deg maximum degree node join-graph, n
number variables, N number nodes graph decomposition,
maximum domain size, w maximum cluster size maximum label size.
proof, see properties CTE presented Kask et al. (2005).

297

fiM ATEESCU , K ASK , G OGATE & ECHTER

3


2
B

C

a)



AB

3




B

b)

1

ABC

2
AB





1
AB

ABC

c)

Figure 14: a) belief network; b) dual join-graph singleton labels; c) dual join-graph
join-tree.

special case Iterative Belief Propagation Iterative belief propagation (IBP) iterative application Pearls algorithm defined poly-trees (Pearl, 1988), Bayesian
network. describe IBP instance join-graph propagation dual join-graph.
EFINITION 11 (dual graphs, dual join-graphs) Given set functions F = {f1 , . . . , fl }
scopes S1 , . . . , Sl , dual graph F graph DG = (V, E, L) associates node
function, namely V = F edge connects two nodes whose functions scope share
variable, E = {(fi , fj )|Si Sj 6= }. L set labels edges, edge labeled
shared variables nodes, L = {lij = Si Sj |(i, j) E}. dual join-graph edgelabeled edge subgraph DG satisfies connectedness property. minimal dual join-graph
dual join-graph none edge labels reduced maintaining
connectedness property.
Interestingly, may many minimal dual join-graphs dual graph.
define Iterative Belief Propagation dual join-graph. node sends message edge
whose scope identical label edge. Since Pearls algorithm sends messages whose
scopes singleton variables only, highlight minimal singleton-label dual join-graphs.
Proposition 3 Bayesian network minimal dual join-graph edge labeled
single variable.
Proof: Consider topological ordering nodes acyclic directed graph Bayesian
network = X1 , . . . , Xn . define following dual join-graph. Every node dual graph
D, associated pi connected node pj , j < Xj pa(Xi ). label edge pj
pi variable Xj , namely lij = {Xj }. easy see resulting edge-labeled subgraph
dual graph satisfies connectedness. (Take original acyclic graph G add node
CPT family, namely parents precede ordering. Since G already satisfies
connectedness minimal graph generated.) resulting labeled graph dual graph
singleton labels. 2

Example 5 Consider belief network 3 variables A, B, C CPTs 1.P (C|A, B),
2.P (B|A) 3.P (A), given Figure 14a. Figure 14b shows dual graph singleton labels edges. Figure 14c shows dual graph join-tree, belief propagation
solve problem exactly one iteration (two passes tree).

298

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Algorithm Iterative Belief Propagation (IBP)
Input: edge-labeled dual join-graph DG = (V, E, L) Bayesian network B = hX, D, G, P i. Evidence e.
Output: augmented graph whose nodes include original CPTs messages received neighbors. Approximations P (Xi |e), Xi X. Approximations P (Fi |e), Fi B.
Denote by: hvu message u v; ne(u) neighbors u V ; nev (u) = ne(u) {v}; luv
label (u, v) E; elim(u, v) = scope(u) scope(v).
One iteration IBP
every node u DJ topological order back, do:
1. Process observed variables
Assign evidence variables pi remove labeled edges.
2. Compute send v function:
X

hvu =
(pu
hui )
elim(u,v)

{hu
,inev (u)}

Endfor
Compute approximations P (Fi |e), P (Xi |e):
every Xi QX let u vertex family Fi DJ,
P (Fi , e) = ( hu ,une(i) hui ) pu ;
P
P (Xi , e) = scope(u){Xi } P (Fi , e).

Figure 15: Algorithm Iterative Belief Propagation (IBP).
completeness, present algorithm IBP, special case IJGP, Figure 15.
easy see one iteration IBP time space linear size belief network.
shown IBP applied minimal singleton-labeled dual graph coincides
Pearls belief propagation applied directly acyclic graph representation. Also, dual
join-graph tree IBP converges one iteration (two passes, tree) exact
beliefs.
4.3 Bounded Join-Graph Decompositions
Since want control complexity join-graph algorithms, define decompositions bounded cluster size. number variables cluster bounded i,
time space complexity processing one cluster exponential i. Given join-graph decomposition = hJG, , , i, accuracy complexity (iterative) join-graph propagation
algorithm depends two different width parameters, defined next.
EFINITION 12 (external internal widths) Given edge-labeled join-graph decomposition
= hJG, , , network B = hX, D, G, P i, internal width maxvV |(v)|,
external width treewidth JG graph.
Using terminology state target decomposition clearly. Given graph
G, bounding parameter wish find join-graph decomposition G whose internal
width bounded whose external width minimized. bound controls complexity
join-graph processing external width provides measure accuracy speed
convergence, measures close join-graph join-tree.
299

fiM ATEESCU , K ASK , G OGATE & ECHTER

Algorithm Join-Graph Structuring(i)
1. Apply procedure schematic mini-bucket(i).
2. Associate resulting mini-bucket node join-graph, variables
nodes appearing mini-bucket, original functions minibucket.
3. Keep edges created procedure (called out-edges) label regular
separator.
4. Connect mini-bucket clusters belonging bucket chain in-edges
labeled single variable bucket.

Figure 16: Algorithm Join-Graph Structuring(i).
Procedure Schematic Mini-Bucket(i)
1. Order variables X1 Xn minimizing (heuristically) induced-width, associate bucket variable.
2. Place CPT bucket highest index variable scope.
3. j = n 1 do:
Partition functions bucket(Xj ) mini-buckets variables.
mini-bucket mb create new scope-function (message) f scope(f ) =
{X|X mb} {Xi } place scope(f) bucket highest variable. Maintain
edge mb mini-bucket (created later) f .

Figure 17: Procedure Schematic Mini-Bucket(i).
consider two classes algorithms. One class partition-based. starts given
tree-decomposition partitions clusters decomposition clusters bounded
i. alternative approach grouping-based. starts minimal dual-graph-based join-graph
decomposition (where cluster contains single CPT) groups clusters larger clusters
long resulting clusters exceed given bound. methods one attempt
reduce external width generated graph-decomposition. partition-based approach
inspired mini-bucket idea (Dechter & Rish, 1997) follows.
Given bound i, algorithm Join-Graph Structuring(i) applies procedure Schematic MiniBucket(i), described Figure 17. procedure traces scopes functions would
generated full mini-bucket procedure, avoiding actual computation. procedure ends
collection mini-bucket trees, rooted mini-bucket first variable.
trees minimally edge-labeled. Then, in-edges labeled one variable introduced,
added obtain running intersection property branches trees.
Proposition 4 Algorithm Join-Graph Structuring(i) generates minimal edge-labeled join-graph
decomposition bound i.
Proof: construction join-graph specifies vertices edges join-graph, well
variable function labels vertex. need demonstrate 1) connectedness
property holds, 2) edge-labels minimal.

300

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

G: (GFE)

GFE

P(G|F,E)
EF

E: (EBF)

(EF)

EBF

P(E|B,F)

P(F|C,D)

F: (FCD)

(BF)

BF
F

FCD

BF

CD

D: (DB)

(CD)

P(D|B)

CDB
CB

C: (CAB) (CB)

P(C|A,B)

B
CAB
BA

B: (BA)

(AB)

A: (A)

(A)

(B)

P(B|A)

BA

P(A)



(b)

(a)

Figure 18: Join-graph decompositions.
Connectedness property specifies 2 vertices u v, vertices u v contain
variable X, must path u, w1 , . . . , wm , v u v every vertex
path contains variable X. two cases here. 1) u v correspond 2 mini-buckets
bucket, 2) u v correspond mini-buckets different buckets. case 1
2 cases, 1a) variable X eliminated bucket, 1b) variable X
eliminated bucket. case 1a, mini-bucket must contain X mini-buckets
bucket connected chain, connectedness property holds. case 1b, vertexes u v
connect (respectively) parents, turn connect parents, etc. bucket
scheme variable X eliminated. nodes along chain connect variable X,
connectedness property holds. Case 2 resolves like case 1b.
show edge labels minimal, need prove cycles respect
edge labels. cycle respect variable X, must involve least one in-edge
(edge connecting two mini-buckets bucket). means variable X must variable
eliminated bucket in-edge. means variable X contained
parents mini-buckets bucket. Therefore, order cycle exist, another in-edge
bucket-tree bucket must contain X. However, impossible would
imply variable X eliminated twice. 2

Example 6 Figure 18a shows trace procedure schematic mini-bucket(3) applied problem described Figure 2a. decomposition Figure 18b created algorithm graph
structuring. cluster partitioned F two scopes (FCD) (BF), connected
in-edge labeled F.
range edge-labeled join-graphs shown Figure 19. left side graph
smaller clusters, cycles. type graph IBP works on. right side
tree decomposition, cycles expense bigger clusters. between,
could number join-graphs maximum cluster size traded number
cycles. Intuitively, graphs left present less complexity join-graph algorithms
cluster size smaller, also likely less accurate. graphs right side
301

fiM ATEESCU , K ASK , G OGATE & ECHTER







C

ABC

AB
ABDE

BC



C





ABC

AB

C

BCE

C
BC

BC

ABDE

ABCDE

DE

CE

CDE

C
DE

CE

CE

CDEF

CDEF

CDEF

CDEF
F

FGH

H

FGH

H

FGI

GH

GI

F

H

GHIJ

H

FGH
H
F

F

F
FG

ABCDE

BCE

C
DE

BCE

FGI

GI

F
F

GH

GHIJ

FGI

GH

GI

GHI
GHIJ

FGHI

GHIJ

accuracy
less complexity

Figure 19: Join-graphs.
computationally complex, larger cluster size, likely
accurate.
4.4 Inference Power IJGP
question address subsection propagating messages iteratively help.
IJGP upon convergence superior IJGP one iteration superior MC? One clue
provided considering deterministic constraint networks viewed extreme probabilistic networks. known constraint propagation algorithms, analogous messages sent belief propagation, guaranteed converge guaranteed
improve iteration. propagation scheme IJGP works similar constraint propagation
relative flat network abstraction probability distribution (where non-zero entries
normalized positive constant), propagation guaranteed accurate
abstraction least.
following shed light IJGPs behavior making connections
well-known concept arc-consistency constraint networks (Dechter, 2003). show
that: (a) variable-value pair assessed zero-belief, remains zero-belief
subsequent iterations; (b) variable-value zero-beliefs computed IJGP correct; (c)
terms zero/non-zero beliefs, IJGP converges finite time. also empirically investigated
hypothesis variable-value pair assessed IBP IJGP positive
close zero belief, likely correct. Although experimental results shown
paper contradict hypothesis, examples recent experiments Dechter,
Bidyuk, Mateescu, Rollon (2010) invalidate it.

302

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

4.4.1 IJGP



RC -C ONSISTENCY

belief network define constraint network captures assignments
strictly positive probability. show correspondence IJGP applied belief
network arc-consistency algorithm applied constraint network. Since arc-consistency
algorithms well understood, correspondence proves target claims, may
provide additional insight behavior IJGP. justifies iterative application belief
propagation, also illuminates distance complete.
EFINITION 13 (constraint satisfaction problem) Constraint Satisfaction Problem (CSP)
triple hX, D, Ci, X = {X1 , . . . , Xn } set variables associated set discretevalued domains = {D1 , . . . , Dn } set constraints C = {C1 , . . . , Cm }. constraint
Ci pair hSi , Ri Ri relation Ri DSi defined subset variables Si X
DSi Cartesian product domains variables Si . relation Ri denotes compatible
tuples DSi allowed constraint. projection operator creates new relation, Sj (Ri ) =
{x|x DSj y, DSi \Sj xy Ri }, Sj Si . Constraints combined
join operator 1, resulting new relation, Ri 1 Rj = {x|Si (x) Ri Sj (x) Rj }.
solution assignment values variables x = (x1 , . . . , xn ), xi Di ,
Ci C, xSi Ri . constraint network represents set solutions, 1i Ci .
Given belief network B, define flattening Bayesian network constraint
network called f lat(B), zero entries probability table removed corresponding relation. network f lat(B) defined set variables
set domain values B.
EFINITION 14 (flat network) Given Bayesian network B = hX, D, G, P i, flat network
f lat(B) constraint network, set variables X, every Xi X
CPT P (Xi |pa(Xi )) B define constraint RFi family Xi , Fi = {Xi } pa(Xi )
follows: every assignment x = (xi , xpa(Xi ) ) Fi , (xi , xpa(Xi ) ) RFi iff P (xi |xpa(Xi ) ) > 0.
HEOREM 6 Given belief network B = hX, D, G, P i, X = {X1 , . . . , Xn }, tuple
x = (x1 , . . . , xn ): PB (x) > 0 x sol(f lat(B)), sol(f lat(B)) set solutions
flat constraint network.
Proof: PB (x) > 0 ni=1 P (xi |xpa(Xi ) ) > 0 {1, . . . , n}, P (xi |xpa(Xi ) ) > 0
{1, . . . , n}, (xi , xpa(Xi ) ) RFi x sol(f lat(B)). 2
Constraint propagation class polynomial time algorithms center constraint processing techniques. investigated extensively past three decades
well known versions arc-, path-, i-consistency (Dechter, 1992, 2003).
EFINITION 15 (arc-consistency) (Mackworth, 1977) Given binary constraint network
(X, D, C), network arc-consistent iff every binary constraint Rij C, every value v Di
value u Dj s.t. (v, u) Rij .

303

fiM ATEESCU , K ASK , G OGATE & ECHTER

Note arc-consistency defined binary networks, namely relations involve
two variables. binary constraint network arc-consistent, algorithms
process enforce arc-consistency. algorithms remove values domains
variables violate arc-consistency arc-consistent network generated.
several versions improved performance arc-consistency algorithms, however consider
non-optimal distributed version, call distributed arc-consistency.
EFINITION 16 (distributed arc-consistency algorithm)
algorithm
distributed
arcconsistency message-passing algorithm constraint network. node variable,
maintains current set viable values Di . Let ne(i) set neighbors Xi
constraint graph. Every node Xi sends message node Xj ne(i), consists
values Xj domain consistent current Di , relative constraint Rji
share. Namely, message Xi sends Xj , denoted Dij , is:
Dij j (Rji 1 Di )

(1)

Di Di (1kne(i) Dki )

(2)

addition node computes:

Clearly algorithm synchronized iterations, iteration every node
computes current domain based messages received far neighbors (Eq. 2),
sends new message neighbor (Eq. 1). Alternatively, Equations 1 2 combined.
message Xi sends Xj is:
Dij j (Rji 1 Di 1kne(i) Dki )

(3)

Next define join-graph decomposition flat constraint network
establish correspondence join-graph decomposition Bayesian network B
join-graph decomposition flat network f lat(B). Note constraint networks, edge
labeling ignored.
EFINITION 17 (join-graph decomposition flat network) Given join-graph decomposition = hJG, , , Bayesian network B, join-graph decomposition Df lat =
hJG, , f lat flat constraint network f lat(B) underlying graph structure
JG = (V, E) D, variable-labeling clusters , mapping f lat maps
cluster relations corresponding CPTs, namely Ri f lat (v) iff CPT pi (v).
distributed arc-consistency algorithm Definition 16 applied join-graph decomposition flat network. case, nodes exchange messages clusters
(namely elements set V JG). domain cluster V set tuples
join original relations cluster (namely domain cluster u ./Rf lat (u) R).
constraints binary, involve clusters V neighbors. two clusters u v,
corresponding values tu tv (which tuples representing full assignments variables
cluster) belong relation Ruv (i.e., (tu , tv ) Ru,v ) projections separator (or
labeling ) u v identical, namely ((u,v)) tu = ((u,v)) tv .
304

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

define algorithm relational distributed arc-consistency (RDAC), applies distributed arc-consistency join-graph decomposition constraint network. call relational emphasize nodes exchanging messages fact relations original
problem variables, rather simple variables case arc-consistency algorithms.
EFINITION 18 (relational distributed arc-consistency algorithm: RDAC join-graph)
Given join-graph decomposition constraint network, let Ri Rj relations two
clusters (Ri Rj joins respective constraints cluster), scopes Si
Sj , Si Sj 6= . message Ri sends Rj denoted h(i,j) defined by:
h(i,j) Si Sj (Ri )

(4)

ne(i) = {j|Si Sj 6= } set relations (clusters) share variable Ri .
cluster updates current relation according to:
Ri Ri 1 (1kne(i) h(k,i) )

(5)

Algorithm RDAC iterates change.
Equations 4 5 combined, like Equation 3. message Ri sends Rj
becomes:
h(i,j) Si Sj (Ri 1 (1kne(i) h(k,i) ))

(6)

establish correspondence IJGP, define algorithm IJGP-RDAC applies
RDAC order computation (schedule processing) IJGP.
EFINITION 19 (IJGP-RDAC algorithm) Given Bayesian network B = hX, D, G, P i, let
Df lat = hJG, , f lat , join-graph decomposition flat network f lat(B). algorithm IJGP-RDAC applied decomposition Df lat f lat(B), described
IJGP applied D, following modifications:
Q
1. Instead , use 1.
P
2. Instead , use .
3. end end, update domains variables by:
Di Di Xi ((1vne(u) h(v,u) ) 1 (1R(u) R))

(7)

u cluster containing Xi .
Note algorithm IJGP-RDAC, could first merge constraints cluster u
single constraint Ru =1R(u) R. construction, IJGP-RDAC enforces arc-consistency
join-graph decomposition flat network. join-graph Df lat join-tree,
IJGP-RDAC solves problem namely finds solutions constraint network.

305

fiM ATEESCU , K ASK , G OGATE & ECHTER

Proposition 5 Given join-graph decomposition Df lat = hJG, , f lat , i, JG = (V, E),
flat constraint network f lat(B), corresponding given join-graph decomposition
Bayesian network B = hX, D, G, P i, algorithm IJGP-RDAC applied Df lat enforces arcconsistency join-graph Df lat .
Proof: IJGP-RDAC applied join-graph decomposition Df lat = hJG, , f lat , i, JG =
(V, E), equivalent applying RDAC Definition 18 constraint network vertices V
variables {1R(u) R|u V } relations. 2
Following properties convergence arc-consistency, show that:
Proposition 6 Algorithm IJGP-RDAC converges O(m r) iterations, number
edges join-graph r maximum size separator Dsep(u,v) two clusters.
Proof: follows fact messages (which relations) clusters IJGP-RDAC
change monotonically, tuples successively removed relations separators. Since
size relation separator bounded r edges, O(mr)
iterations needed. 2
following establish equivalence IJGP IJGP-RDAC terms
zero probabilities.
Proposition 7 IJGP IJGP-RDAC applied order computation,
messages computed IJGP identical computed IJGP-RDAC terms zero / nonzero probabilities. is, h(u,v) (x) 6= 0 IJGP iff x h(u,v) IJGP-RDAC.
Proof: proof induction. base case trivially true since messages h IJGP initialized uniform distribution messages h IJGP-RDAC initialized complete relations.
induction step. Suppose hIJGP
(u,v) message sent u v IJGP. show
IJGP RDAC
IJGP RDAC
hIJGP
h(u,v)
message sent IJGP(u,v) (x) 6= 0, x h(u,v)
RDAC u v. Assume claim holds messages received u neighbors.
Let f clusterv (u) IJGP Rf corresponding relation IJGP-RDAC,

P
Q asIJGP
signment values variables elim(u, v). h(u,v) (x) 6= 0 elim(u,v) f f (x) 6= 0
Q
t, f f (x, t) 6= 0 t, f, f (x, t) 6= 0 t, f, scope(Rf ) (x, t) Rf t, elim(u,v) (1Rf
IJGP RDAC
IJGP RDAC
. 2
x h(u,v)
scope(Rf ) (x, t)) h(u,v)

Next show IJGP computing marginal probability P (Xi = xi ) = 0 equivalent
IJGP-RDAC removing xi domain variable Xi .
Proposition 8 IJGP computes P (Xi = xi ) = 0 iff IJGP-RDAC decides xi 6 Di .
Proof: According Proposition 7 messages computed IJGP IJGP-RDAC identical
terms zero probabilities. Let f cluster(u) IJGP Rf corresponding relation
IJGP-RDAC, assignment values variables (u)\Xi . show
IJGP computes P (Xi = xi ) = 0 (upon convergence), IJGP-RDAC computes xi 6 Di .
306

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Q
Q
P
P (Xi = xi ) =
f f (xi , t) = 0 t, f, f (xi , t) = 0
f f (xi ) = 0 t,
X\Xi
t, Rf , scope(Rf ) (xi , t) 6 Rf t, (xi , t) 6 (1Rf Rf (xi , t)) xi 6 Di Xi (1Rf Rf (xi , t))
xi 6 Di . Since arc-consistency sound, decision zero probabilities. 2
Next show P (Xi = xi ) = 0 computed IJGP sound.
HEOREM 7 Whenever IJGP finds P (Xi = xi ) = 0, probability P (Xi ) expressed
Bayesian network conditioned evidence 0 well.
Proof: According Proposition 8, whenever IJGP finds P (Xi = xi ) = 0, value xi removed
domain Di IJGP-RDAC, therefore value xi Di no-good network f lat(B),
Theorem 6 follows PB (Xi = xi ) = 0. 2
following show time takes IJGP find P (Xi = xi ) = 0 bounded.
Proposition 9 IJGP finds P (Xi = xi ) = 0 finite time, is, exists number k,
P (Xi = xi ) = 0 found k iterations.
Proof: follows fact number iterations takes IJGP compute P (Xi =
xi ) = 0 exactly number iterations IJGP-RDAC takes remove xi domain
Di (Proposition 7 Proposition 8), fact IJGP-RDAC runtime bounded (Proposition
6). 2
Previous results also imply IJGP monotonic respect zeros.
Proposition 10 Whenever IJGP finds P (Xi = xi ) = 0, stays 0 subsequent iterations.
Proof: Since know relations IJGP-RDAC monotonically decreasing algorithm
progresses, follows equivalence IJGP-RDAC IJGP (Proposition 7) IJGP
monotonic respect zeros. 2

4.4.2 F INITE P RECISION P ROBLEM
finite precision machines danger underflow interpreted zero
value. provide warning implementation belief propagation allow
creation zero values underflow. show example Figure 20 IBPs messages
converge limit (i.e., infinite number iterations), stabilize finite
number iterations. nodes Hk set value 1, belief Xi variables
function iteration given table Figure 20. 300 iterations, finite precision
computer able represent value Bel(Xi = 3), appears zero,
yielding final updated belief (.5, .5, 0), fact true updated belief (0, 0, 1).
Notice (.5, .5, 0) cannot regarded legitimate fixed point IBP. Namely, would
initialize IBP values (.5, .5, 0), algorithm would maintain them, appearing
fixed point, initializing IBP zero values cannot expected correct.

307

fiM ATEESCU , K ASK , G OGATE & ECHTER

X1

Prior Xi

H3

H1

X3

X2

Xi

P ( Xi )

1

.45

2

.45

3

.1

H2
Hk Xi

CPT Hk

Xj

P ( Hk | Xi , Xj )

1

1

2

1

1

2

1

1

1

3

3

1

1



0

#iter

Bel(Xi = 1) Bel(Xi = 2) Bel(Xi = 3)

1

.45

.45

.1

2

.49721

.49721

.00545

3

.49986

.49986

.00027

100





1e-129

200





1e-260

300

.5

.5

0

True
belief

0

0

1

Figure 20: Example finite precision problem.
initialize zeros forcibly introduce determinism model, IBP always maintain
afterwards.
However, example contradict theory because, mathematically, Bel(Xi = 3)
never becomes true zero, IBP never reaches quiescent state. example shows close
zero belief network arbitrarily inaccurate. case inaccuracy seems due
initial prior belief different posterior ones.
4.4.3 ACCURACY IBP ACROSS B ELIEF ISTRIBUTION
present empirical evaluation accuracy IBPs prediction range belief distribution 0 1. results also extend IJGP. previous section, proved zero
values inferred IBP correct, wanted test hypothesis property extends
small beliefs (namely, close zero). is, IBP infers posterior belief close
zero, likely correct. results presented paper seem support hypothesis, however new experiments Dechter et al. (2010) show true general.
yet good characterization cases hypothesis confirmed.
test hypothesis, computed absolute error IBP per intervals [0, 1]. given
interval [a, b], 0 < b 1, use measures inspired information retrieval: Recall
Absolute Error Precision Absolute Error.
Recall absolute error averaged exact posterior beliefs fall interval
[a, b]. Precision, average taken approximate posterior belief values computed
IBP interval [a, b]. Intuitively, Recall([a,b]) indicates far belief computed
IBP exact, exact [a, b]; Precision([a,b]) indicates far exact
IBPs prediction, value computed IBP [a, b].
experiments show two measures strongly correlated. also show histograms distribution belief interval, exact IBP, also strongly
correlated. results given Figures 21 22. left axis corresponds histograms
(the bars), right axis corresponds absolute error (the lines).
present results two classes problems: coding networks grid network. problems
binary variables, graphs symmetric 0.5 show interval [0, 0.5].
number variables, number iterations induced width w* reported graph.

308

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Recall Abs. Error

noise = 0.20

noise = 0.40

0.4

Absolute Error

0.45

0.3

0
0.35

0.35

0.01

0.2

0%

0.02

0.25

0%

0.03

0.1

0%

0.04

0.15

5%

Precision Abs. Error
0.05

0

10%

5%
0.4

10%

5%

0.45

10%

0.3

15%

0.2

20%

15%

0.25

20%

15%

0.1

20%

0.15

25%

0

30%

25%

0.05

30%

25%

0.4

30%

0.45

35%

0.3

40%

35%

0.35

40%

35%

0.2

40%

0.25

45%

0.1

45%

0.15

45%

0

50%

0.05

IBP Histogram
50%

0.05

Percentage

Exact Histogram
50%

noise = 0.60

Figure 21: Coding, N=200, 1000 instances, w*=15.
Recall Abs. Error

evidence = 0

evidence = 10

0.004
0.003
0.002
0.001

Absolute Error

0.005

0.4

0.45

0.3

0.35

0.2

0.25

0.1

0.15

0
0

0.4

0.45

0.3

0.35

0%
0.2

0%
0.25

5%
0.1

10%

5%
0.15

10%

0

15%

0.05

20%

15%

0.4

20%

0.45

25%

0.3

30%

25%

0.35

30%

0.2

35%

0.25

40%

35%

0.1

40%

0.15

45%

0

45%

Precision Abs. Error

0.05

IBP Histogram

50%

0.05

Percentage

Exact Histogram
50%

evidence = 20

Figure 22: 10x10 grids, 100 instances, w*=15.
Coding networks IBP famously known impressive performance coding networks.
tested linear block codes, 50 nodes per layer 3 parent nodes. Figure 21 shows
results three different values channel noise: 0.2, 0.4 0.6. noise 0.2, beliefs
computed IBP extreme. Recall Precision small, order 1011 . So,
case, beliefs small ( small) IBP able infer correctly, resulting
almost perfect accuracy (IBP indeed perfect case bit error rate). noise
increased, Recall Precision tend get closer bell shape, indicating higher error
values close 0.5 smaller error extreme values. histograms also show less belief
values extreme noise increased, factors account overall decrease
accuracy channel noise increases. networks examples large number
-small probabilities IBP able infer correctly (absolute error small).
Grid networks present results grid networks Figure 22. Contrary case coding
networks, histograms show higher concentration beliefs around 0.5. However, accuracy
still good beliefs close zero. absolute error peaks close 0 maintains plateau,
evidence increased, indicating less accuracy IBP.

5. Experimental Evaluation
anticipated summary Section 3, clearly seen structuring
bounded join-graph, close relationship mini-clustering algorithm MC(i)
309

fiM ATEESCU , K ASK , G OGATE & ECHTER

IBP
#it
1

5

10

MC

#evid
0
5
10
0
5
10
0
5
10
0
5
10

0.02988
0.06178
0.08762
0.00829
0.05182
0.08039
0.00828
0.05182
0.08040

Absolute error
IJGP
i=2
i=5
0.03055 0.02623
0.04434 0.04201
0.05777 0.05409
0.00636 0.00592
0.00886 0.00886
0.01155 0.01073
0.00584 0.00514
0.00774 0.00732
0.00892 0.00808

IBP
i=8
0.02940
0.04554
0.05910
0.00669
0.01123
0.01399
0.00495
0.00708
0.00855

0.04044 0.04287 0.03748
0.05303 0.05171 0.04250
0.06033 0.05489 0.04266

0.06388
0.15005
0.23777
0.01726
0.12589
0.21781
0.01725
0.12590
0.21782

Relative error
IJGP
i=5
0.05677
0.12056
0.14278
0.01239
0.01965
0.02553
0.01069
0.01628
0.01907

i=2
0.15694
0.12340
0.18071
0.01326
0.01967
0.03014
0.01216
0.01727
0.02101

IBP
i=8
0.07153
0.11154
0.15686
0.01398
0.02494
0.03279
0.01030
0.01575
0.02005

0.08811 0.09342 0.08117
0.12375 0.11775 0.09596
0.14702 0.13219 0.10074

0.00213
0.00812
0.01547
0.00021
0.00658
0.01382
0.00021
0.00658
0.01382

KL distance
IJGP
i=5
0.00208
0.00478
0.00768
0.00015
0.00026
0.00042
0.00010
0.00017
0.00024

i=2
0.00391
0.00582
0.00915
0.00014
0.00024
0.00055
0.00012
0.00018
0.00028

IBP
i=8
0.00277
0.00558
0.00899
0.00018
0.00044
0.00073
0.00010
0.00016
0.00029

0.00403 0.00435 0.00369
0.00659 0.00636 0.00477
0.00841 0.00729 0.00503

0.0017
0.0013
0.0013
0.0066
0.0060
0.0048
0.0130
0.0121
0.0109

Time
IJGP
i=5
0.0058
0.0052
0.0036
0.0226
0.0185
0.0138
0.0436
0.0355
0.0271

i=2
0.0036
0.0040
0.0040
0.0145
0.0120
0.0100
0.0254
0.0223
0.0191

i=8
0.0295
0.0200
0.0121
0.1219
0.0840
0.0536
0.2383
0.1639
0.1062

0.0159 0.0173 0.0552
0.0146 0.0158 0.0532
0.0119 0.0143 0.0470

Table 4: Random networks: N=50, K=2, C=45, P=3, 100 instances, w*=16.

IJGP(i). particular, one iteration IJGP(i) similar MC(i). MC sends messages
along clusters form set trees. IJGP additional connections allow
interaction mini-clusters cluster. Since cyclic structure, iterating
facilitated, virtues drawbacks.s
evaluation IJGP(i), focus two different aspects: (a) sensitivity parametric
IJGP(i) i-bound number iterations; (b) comparison IJGP(i) publicly
available state-of-the-art approximation schemes.
5.1 Effect i-bound Number Iterations
tested performance IJGP(i) random networks, M-by-M grids, two benchmark CPCS files 54 360 variables, respectively coding networks. type
networks, ran IBP, MC(i) IJGP(i), giving IBP IJGP(i) number
iterations.
use partitioning method described Section 4.3 construct join-graph. determine
order message computation, recursively pick edge (u,v), node u fewest
incoming messages missing.
network except coding, compute exact solution compare accuracy
using absolute relative error, before, well KL (Kullback-Leibler) distance Pexact (X = a) log(Pexact (X = a)/Papproximation (X = a)) averaged values, variables
problems. coding networks report Bit Error Rate (BER) computed described
Section 3.2. also report time taken algorithm.
random networks generated using parameters (N,K,C,P), N number
variables, K domain size, C number conditional probability tables (CPTs) P
number parents CPT. Parents CPT picked randomly CPT
filled randomly. grid networks, N square number CPT filled randomly.
problem class, also tested different numbers evidence variables. before, coding
networks class linear block codes, channel noise level. Note
limited relatively small sparse problem instances evaluation measures
based comparing exact figures.
Random networks results networks N=50, K=2, C=45 P=3 given Table 4
Figures 23 24. IJGP(i) MC(i) report 3 different values i-bound: 2, 5, 8.
IBP IJGP(i) report results 3 different numbers iterations: 1, 5, 10. report results
310

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Random networks, N=50, K=2, P=3, evid=5, w*=16
0.010

IJGP 1
IJGP 2
IJGP 3
IJGP 5
IJGP 10
IJGP 15
IJGP 20
MC
IBP 1
IBP 2
IBP 3
IBP 5
IBP 10

0.008

KL distance

0.006

0.004

0.002

0.000

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(a) Performance vs. i-bound.
Random networks, N=50, K=2, P=3, evid=5, w*=16
0.010

IBP
IJGP(2)
IJGP(10)

0.008

KL distance

0.006

0.004

0.002

0.000

0

5

10

15

20

25

30

35

Number iterations

(b) Convergence iterations.

Figure 23: Random networks: KL distance.
3 different numbers evidence: 0, 5, 10. Table 4 Figure 23a see IJGP(i)
always better IBP (except i=2 number iterations 1), sometimes order
magnitude, terms absolute error, relative error KL distance. IBP rarely changes 5
iterations, whereas IJGP(i)s solution improved iterations (up 15-20). theory
predicted, accuracy IJGP(i) one iteration MC(i). IJGP(i)
improves number iterations increases, eventually better MC(i) much
order magnitude, although clearly takes time, especially i-bound large.

311

fiM ATEESCU , K ASK , G OGATE & ECHTER

Random networks, N=50, K=2, P=3, evid=5, w*=16
1.0
IJPG 1
IJGP 2
IJGP 3
IJGP 5
IJGP 10
IJGP 15
IJGP 20
MC
IBP 1
IBP 20

Time (seconds)

0.8

0.6

0.4

0.2

0.0

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

Figure 24: Random networks: Time.
IBP
#it
1

5

10

MC

#evid
0
5
10
0
5
10
0
5
10
0
5
10

0.03524
0.05375
0.07094
0.00358
0.03224
0.05503
0.00352
0.03222
0.05503

Absolute error
IJGP
i=2
i=5
0.05550 0.04292
0.05284 0.04012
0.05453 0.04304
0.00393 0.00325
0.00379 0.00319
0.00364 0.00316
0.00352 0.00232
0.00357 0.00248
0.00347 0.00239

IBP
i=8
0.03318
0.03661
0.03966
0.00284
0.00296
0.00314
0.00136
0.00149
0.00141

0.05827 0.04036 0.01579
0.05973 0.03692 0.01355
0.05866 0.03416 0.01075

0.08075
0.16380
0.23624
0.00775
0.11299
0.19403
0.00760
0.11295
0.19401

Relative error
IJGP
i=5
0.10252
0.09889
0.12492
0.00702
0.00710
0.00756
0.00502
0.00549
0.00556

i=2
0.13533
0.13225
0.14588
0.00849
0.00844
0.00841
0.00760
0.00796
0.00804

IBP
i=8
0.07904
0.09116
0.12202
0.00634
0.00669
0.01313
0.00293
0.00330
0.00328

0.13204 0.08833 0.03440
0.13831 0.08213 0.03001
0.14120 0.07791 0.02488

0.00289
0.00725
0.01232
0.00005
0.00483
0.00994
0.00005
0.00483
0.00994

KL distance
IJGP
i=5
0.00602
0.00570
0.00681
0.00007
0.00007
0.00009
0.00003
0.00003
0.00003

i=2
0.00859
0.00802
0.00905
0.00006
0.00006
0.00006
0.00005
0.00005
0.00005

IBP
i=8
0.00454
0.00549
0.00653
0.00010
0.00010
0.00019
0.00001
0.00002
0.00001

0.00650 0.00387 0.00105
0.00696 0.00348 0.00099
0.00694 0.00326 0.00075

0.0010
0.0016
0.0013
0.0049
0.0053
0.0036
0.0090
0.0096
0.0090

Time
IJGP
i=5
0.0106
0.0092
0.0072
0.0347
0.0309
0.0271
0.0671
0.0558
0.0495

i=2
0.0053
0.0041
0.0038
0.0152
0.0131
0.0127
0.0277
0.0246
0.0223

i=8
0.0426
0.0315
0.0256
0.1462
0.1127
0.0913
0.2776
0.2149
0.1716

0.0106 0.0142 0.0382
0.0102 0.0130 0.0342
0.0099 0.0116 0.0321

Table 5: 9x9 grid, K=2, 100 instances, w*=12.

Figure 23a shows comparison algorithms different numbers iterations, using
KL distance. network structure changes different i-bounds, necessarily
see monotonic improvement IJGP i-bound given number iterations (as case
MC). Figure 23b shows IJGP converges iterations smaller KL distance
IBP. expected, time taken IJGP (and MC) varies exponentially i-bound (see
Figure 24).
Grid networks results networks N=81, K=2, 100 instances similar
random networks. reported Table 5 Figure 25, see impact
evidence (0 5 evidence variables) algorithms. IJGP convergence gives
best performance cases, IBPs performance deteriorates evidence
surpassed MC i-bound 5 larger.
CPCS networks results CPCS54 CPCS360 given Table 6 Figure 26,
even pronounced random grid networks. evidence added, IJGP(i)
accurate MC(i), accurate IBP, seen Figure 26a.
Coding networks results given Table 7. tested large networks 400 variables,
treewidth w*=43, IJGP IBP set run 30 iterations (this enough ensure
312

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Grid network, N=81, K=2, evid=5, w*=12
0.010
IJGP 1
IJGP 2
IJGP 3
IJGP 5
IJGP 10
MC
IBP 1
IBP 2
IBP 3
IBP 5
IBP 10

0.008

KL distance

0.006

0.004

0.002

0.000

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(a) Performance vs. i-bound.
Grid network, N=81, K=2, evid=5, w*=12
7e-5
IJGP 20 iterations
(at convergence)
6e-5

KL distance

5e-5

4e-5

3e-5

2e-5

1e-5

0
1

2

3

4

5

6

7

8

9

10

11

i-bound

(b) Fine granularity KL.

Figure 25: Grid 9x9: KL distance.
convergence). IBP known accurate class problems indeed better
MC. However notice IJGP converges slightly smaller BER IBP even small
values i-bound. coding network CPCS360 show scalability IJGP large
size problems. Notice anytime behavior IJGP clear.
summary, see IJGP almost always superior IBP MC(i) sometimes
accurate several orders magnitude. One note IBP cannot improved
time, MC(i) requires large i-bound many hard large networks achieve
reasonable accuracy. question iterative application IJGP instrumental
success. fact, IJGP(2) isolation appears cost-effective variant.

313

fiM ATEESCU , K ASK , G OGATE & ECHTER

IBP
#it

1

5

10

MC

1
10
20
MC

#evid

Absolute error
IJGP
i=2
i=5

Relative error
IJGP
i=5

IBP
i=8

i=2
0.02716
0.05736
0.08475
0.00064
0.04067
0.07302
0.00064
0.04067
0.07302

0.08966
0.09007
0.09156
0.00033
0.00124
0.00215
0.00018
0.00078
0.00123
0.05648
0.05687
0.06002

KL distance
IJGP
i=5

IBP
i=8

CPCS54
0.07761 0.05616
0.07676 0.05856
0.08246 0.06687
0.00255 0.00225
0.00194 0.00203
0.00298 0.00302
0.00029 0.00031
0.00071 0.00080
0.00109 0.00122
0.05128 0.03047
0.05314 0.03713
0.05318 0.03409

Time
IBP

i=2
0.00041
0.00199
0.00357
7.75e-7
0.00161
0.00321
7.75e-7
0.00161
0.00321

0.00583
0.00573
0.00567
0.00000
0.00000
0.00001
0.0000
0.00000
4.0e-6
0.00218
0.00201
0.00216

0.00512
0.00493
0.00506
0.00002
0.00001
0.00003
0.00000
0.00000
3.0e-6
0.00171
0.00186
0.00177

i=8
0.00378
0.00366
0.00390
0.00001
0.00001
0.00002
0.00000
0.00000
4.0e-6
0.00076
0.00098
0.00091

0.0097
0.0072
0.005
0.0371
0.0337
0.0290
0.0736
0.0633
0.0575

i=2

IJGP
i=5

i=8

0.0137
0.0094
0.0047
0.0334
0.0215
0.0144
0.0587
0.0389
0.0251
0.0144
0.0103
0.0094

0.0146
0.0087
0.0052
0.0384
0.0260
0.0178
0.0667
0.0471
0.0297
0.0125
0.0126
0.0090

0.0275
0.0169
0.0115
0.0912
0.0631
0.0378
0.1720
0.1178
0.0723
0.0333
0.0346
0.0295

0
5
10
0
5
10
0
5
10
0
5
10

0.01324
0.02684
0.03915
0.00031
0.01874
0.03348
0.00031
0.01874
0.03348

0.03747
0.03739
0.03843
0.00016
0.00058
0.00101
0.00009
0.00037
0.00058
0.02721
0.02702
0.02825

0.03183
0.03124
0.03426
0.00123
0.00092
0.00139
0.00014
0.00034
0.00051
0.02487
0.02522
0.02504

0.02233
0.02337
0.02747
0.00110
0.00098
0.00144
0.00015
0.00038
0.00057
0.01486
0.01760
0.01600

10
20
10
20
10
20
10
20

0.26421
0.26326
0.01772
0.02413
0.01772
0.02413

0.14222
0.12867
0.00694
0.00466
0.00003
0.00001
0.03389
0.02715

0.13907
0.12937
0.00121
0.00115
3.0e-6
9.0e-6
0.01984
0.01543

CPCS360
0.14334 7.78167 2119.20 2132.78 2133.84 0.17974 0.09297 0.09151 0.09255 0.7172 0.5486 0.5282 0.4593
0.13665 370.444 28720.38 30704.93 31689.59 0.17845 0.08212 0.08269 0.08568 0.6794 0.5547 0.5250 0.4578
0.00258 1.06933 6.07399 0.01005 0.04330 0.017718 0.00203 0.00019 0.00116 7.2205 4.7781 4.5191 3.7906
0.00138 62.99310 26.04308 0.00886 0.01353 0.02027 0.00118 0.00015 0.00036 7.0830 4.8705 4.6468 3.8392
3.0e-6
1.06933 0.00044
8.0e-6
7.0e-6
0.01771
5.0e-6
0.0
0.0
14.4379 9.5783 9.0770 7.6017
9.0e-6
62.9931 0.00014 0.00013 0.00004 0.02027
0.0
0.0
0.0
13.6064 9.4582 9.0423 7.4453
0.01402
0.65600 0.20023 0.11990
0.01299 0.00590 0.00390
2.8077 2.7112 2.5188
0.00957
0.81401 0.17345 0.09113
0.01007 0.00444 0.00234
2.8532 2.7032 2.5297

Table 6: CPCS54 50 instances, w*=15; CPCS360 10 instances, w*=20.


0.22 IJGP
MC
0.28 IJGP
MC
0.32 IJGP
MC
0.40 IJGP
MC
0.51 IJGP
MC
0.65 IJGP
MC

2
0.00005
0.00501
0.00062
0.02170
0.00238
0.04018
0.01202
0.08726
0.07664
0.15396
0.19070
0.21890

IJGP 0.36262
MC 0.25281

Bit Error Rate
i-bound
4
6
8
0.00005 0.00005 0.00005
0.00800 0.00586 0.00462
0.00062 0.00062 0.00062
0.02968 0.02492 0.02048
0.00238 0.00238 0.00238
0.05004 0.04480 0.03878
0.01188 0.01194 0.01210
0.09762 0.09272 0.08766
0.07498 0.07524 0.07578
0.16048 0.15710 0.15452
0.19056 0.19016 0.19030
0.22056 0.21928 0.21904
Time
0.41695 0.86213 2.62307
0.21816 0.31094 0.74851

10
0.00005
0.00392
0.00062
0.01840
0.00238
0.03558
0.01192
0.08334
0.07554
0.15180
0.19056
0.21830

IBP
0.00005
0.00064
0.00242
0.01220
0.07816
0.19142

9.23610 0.019752
2.33257

Table 7: Coding networks: N=400, P=4, 500 instances, 30 iterations, w*=43.

5.2 Comparing IJGP Algorithms
section provide comparison IJGP state-of-the-art publicly available schemes.
comparison based recent evaluation algorithms performed Uncertainty AI
2008 conference4 . present results solving belief updating task (also called task
computing posterior node marginals - MAR). first give brief overview schemes
experimented compared with.
1. EDBP - Edge Deletion Belief Propagation
4. Complete results available http://graphmod.ics.uci.edu/uai08/Evaluation/Report.

314

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

CPCS360, evid=10, w*=20
0.20
IJGP 1
IJGP 10
IJGP 20
MC
IBP 1
IBP 10
IBP 20

0.18
0.16

KL distance

0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(a) Performance vs. i-bound.
CPCS360, evid=10, w*=20
6e-6
IJGP 20 iterations
(at convergence)
5e-6

KL distance

4e-6

3e-6

2e-6

1e-6

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(b) Fine granularity KL.

Figure 26: CPCS360: KL distance.
EDBP (Choi & Darwiche, 2006a, 2006b) approximation algorithm Belief Updating.
solves exactly simplified version original problem, obtained deleting
edges problem graph. Edges deleted selected based two criteria:
quality approximation complexity computation (tree-width reduction). Information
loss lost dependencies compensated introducing auxiliary network parameters.
method corresponds Iterative Belief Propagation (IBP) enough edges deleted
yield poly-tree, corresponds generalized BP otherwise.
2. TLSBP - truncated Loop series Belief propagation algorithm

315

fiM ATEESCU , K ASK , G OGATE & ECHTER

TLSBP based loop series expansion formula Chertkov Chernyak (2006)
specifies series terms need added solution output BP exact
solution recovered. series basically sum so-called generalized loops
graph. Unfortunately, number generalized loops prohibitively
large, series little value. idea TLSBP truncate series decomposing
generalized loops simple smaller loops, thus limiting number loops
summed. evaluation, used implementation TLSBP available work
Gomez, Mooji, Kappen (2007). implementation handle binary networks only.
3. EPIS - Evidence Pre-propagation Importance Sampling
EPIS (Yuan & Druzdzel, 2003) importance sampling algorithm Belief Updating.
well known sampling algorithms perform poorly presented unlikely evidence.
However, samples weighted importance function, good approximation
obtained. algorithm computes approximate importance function using loopy belief
propagation -cutoff heuristic. used implementation EPIS available
authors. implementation works Bayesian networks only.
4. IJGP - Iterative Join-Graph Propagation
evaluation, IJGP(i) first run i=2, convergence, i=3, convergence, etc. i= treewidth (when i-bound=treewidth, join-graph becomes join-tree
IJGP becomes exact). preprocessing, algorithm performed SAT-based variable domain pruning converting zero probabilities problem SAT problem performing singleton-consistency enforcement. problem size may reduce substantially,
cases, preprocessing step may significant impact time-complexity
IJGP, amortized increasing i-bound. However, given i-bound, step improves accuracy IJGP marginally.
5. SampleSearch
SampleSearch (Gogate & Dechter, 2007) specialized importance sampling scheme
graphical models contain zero probabilities CPTs. graphical models,
importance sampling suffers rejection problem generates large number
samples zero weight. SampleSearch circumvents rejection problem
sampling backtrack-free search space every assignment (sample) guaranteed non-zero weight. backtrack-free search space constructed fly
interleaving sampling backtracking style search. Namely, sample supposed
rejected weight zero, algorithm continues instead systematic
backtracking search, non zero weight sample found. evaluation version,
importance distribution SampleSearch constructed output IJGP
i-bound 3. information importance distribution constructed
output IJGP, see work Gogate (2009).
evaluation conducted following benchmarks (see footnote 4 details):
1. UAI06-MPE - UAI-06, 57 instances, Bayesian networks (40 instances used).
2. UAI06-PE - UAI-06, 78 instances, Bayesian networks (58 instances used).
316

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

IJGP
EDBP
TLSBP
EPIS
SampleSearch

WCSPs BN2O Grids Linkage Promedas UAI06-MPE UAI06-PE Relational

































Table 8: Scope experimental study.
Score vs KL distance
1
Score vs KL distance
0.9
0.8

Score

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.2

0.4

0.6

0.8

1

KL distance

Figure 27: Score function KL distance.
3. Relational Bayesian networks - constructed Primula tool, 251 instances, binary variables, large networks large tree-width, high levels determinism (30 instances
used).
4. Linkage networks - 22 instances, tree-width 20-35, Markov networks (5 instances used).
5. Grids - 12x12 50x50, 320 instances, treewidth 12-50.
6. BN2O networks - Two-layer Noisy-OR Bayesian networks, 18 instances, binary variables,
55 variables, treewidth 24-27.
7. WCSPs - Weighted CSPs, 97 instances, Markov networks (18 instances used).
8. Promedas - real-world medical diagnosis, 238 instances, tree-width 1-60, Markov networks
(46 instances used).

Table 8 shows scope experimental study. indicates solver able

handle benchmark type therefore evaluated lack indicates otherwise.
measure performance algorithms terms KL-distance based score. Formally,
score solver problem instance equal 10avgkld avgkld average KL
distance exact marginal (which computed using UCLA Ace solver, see Chavira
& Darwiche, 2008) approximate marginal output solver. solver output
solution, consider KL-distance . score lies 0 1, 1 indicating
solver outputs exact solution 0 indicating solver either output solution
infinite average KL distance. Figure 27 shows score function KL distance.
317

fiM ATEESCU , K ASK , G OGATE & ECHTER

Figures 28-35 report results experiments problem sets.
solver timeout 20 minutes problem instance; solving problem, solver
periodically outputs best solution found far. Using this, compute, solver,
point time, total sum scores problem instances particular set, called
SumScore(t). horizontal axis, time vertical axis, SumScore(t).
higher curve solver is, better (the higher score).
summary, see IJGP shows best performance first four classes networks
(UAI-MPE, UAI-PE, Relational Linkage), tied algorithms two classes (Grid
BN2O), surpassed EDBP last two classes (WCSPs Promedas). EPIS
SampleSearch, importance sampling schemes, often inferior IJGP EDBP.
theory, accuracy importance sampling schemes improve time. However,
rate improvement often unknown practice. hard benchmarks evaluated
on, found rate quite small therefore improvement cannot discerned
Figures. discuss results detail below.
mentioned earlier, TLSBP works binary networks (i.e., two variables per function)
therefore evaluated WCSPs, Linkage, UAI06-MPE UAI06-PE benchmarks.
UAI-MPE UAI-PE instances used UAI 2006 evaluation exact solvers (for
details see report Bilmes & Dechter, 2006). Exact marginals available 40 UAI-MPE
instances 58 UAI-PE instances. results UAI-MPE UAI-PE instances shown
Figures 28 29 respectively. IJGP best performing scheme benchmark sets
reaching SumScore close maximum possible value cases 2 minutes
CPU time. EDBP SampleSearch second best cases.
Relational network instances generated grounding relational Bayesian networks using
Primula tool (Chavira, Darwiche, & Jaeger, 2006). Exact marginals available 30
submitted 251 instances. Figure 30, observe IJGPs SumScore steadily
increases time reaches value close maximum possible score 30
16 minutes CPU time. SampleSearch second best performing scheme. EDBP, TLSBP
EPIS perform quite poorly instances reaching SumScore 10, 13 13 respectively
20 minutes CPU time.
Linkage instances generated converting linkage analysis data Markov network
using Superlink tool (Fishelson & Geiger, 2003). Exact marginals available 5
22 instances. results shown Figure 31. one minute CPU time, IJGPs
SumScore close 5 remains steady thereafter EDBP reaches SumScore 2
20 minutes. SampleSearch second best performing scheme EDBP third best.
results Grid networks shown Figure 32. sink node grid evidence
node. deterministic ratio p parameter specifying fraction nodes deterministic,
is, whose values determined given values parents. evaluation benchmark
set consists 30 instances p = 50%,75% 90% exact marginals available 27
instances only. EPIS, IJGP, SampleSearch EDBP close tie network,
TLSBP lowest performance. hard see, EPIS slightly best performing
scheme, IJGP second best followed SampleSearch EDBP. instances IJGPs
SumScore increases steadily time.
results BN2O instances appear Figure 33. close tie, case
five algorithms. IJGP minuscule decrease SumScore time 17.85 17.7.
Although general improvement accuracy expected IJGP higher i-bound,
318

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Approximate Mar Problem Set uai06-mpe

40
35

Sum Score

30
25
20
15
10
5
0
0

2

4

6

8

10

12

14

16

18

20

Time minutes
SampleSearch

IJGP

EDBP

EPIS

Figure 28: Results UAI-MPE networks. TLSBP plotted cannot handle UAIMPE benchmarks.

Approximate Mar Problem Set uai06-pe

50

Sum Score

40

30

20

10

0
0

2

4

6

8

10

12

14

16

18

20

Time minutes
SampleSearch

IJGP

EDBP

EPIS

Figure 29: Results UAI-PE networks. TLSBP plotted cannot handle UAI-PE
benchmarks.

319

fiM ATEESCU , K ASK , G OGATE & ECHTER

Approximate Mar Problem Set Relational
35

30

Sum Score

25

20

15

10

5

0
0

2

4

6

8

10

12

14

16

18

20

18

20

Time minutes
SampleSearch
IJGP

EDBP
TLSBP

EPIS

Figure 30: Results relational networks.

Approximate Mar Problem Set Linkage
6

5

Sum Score

4

3

2

1

0
0

2

4

6

8

10

12

14

16

Time minutes
SampleSearch

IJGP

EDBP

Figure 31: Results Linkage networks. EPIS TLSBP plotted cannot
handle Linkage networks.

320

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Approximate Mar Problem Set Grids

25

Sum Score

20

15

10

5

0
0

2

4

6

8

10

12

14

16

18

20

16

18

20

Time minutes
SampleSearch
IJGP

EDBP
TLSBP

EPIS

Figure 32: Results Grid networks.

Approximate Mar Problem Set bn2o
18
16

Sum Score

14
12
10
8
6
4
2
0
0

2

4

6

8

10

12

14

Time minutes
SampleSearch
IJGP

EDBP
TLSBP

EPIS

Figure 33: Results BN2O networks. solvers except IJGP quickly converge maximum
possible score 18 therefore indistinguishable Figure.

321

fiM ATEESCU , K ASK , G OGATE & ECHTER

Approximate Mar Problem Set WCSPs
18
16
14

Sum Score

12
10
8
6
4
2
0
0

2

4

6

8

10

12

14

16

18

20

Time minutes
SampleSearch

IJGP

EDBP

Figure 34: Results WCSPs networks. EPIS TLSBP plotted cannot
handle WCSPs.

Approximate Mar Problem Set Promedas
45
40
35

Sum Score

30
25
20
15
10
5
0
0

2

4

6

8

10

12

14

16

18

20

Time minutes
SampleSearch

IJGP

EDBP

TLSBP

Figure 35: Results Promedas networks. EPIS plotted cannot handle Promedas
benchmarks, Markov networks.

322

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

guaranteed, example happen. solvers reach maximum
possible SumScore 18 (or close it) 6 minutes CPU time.
WCSP benchmark set 97 instances. However used 18 instances
exact marginals available. Therefore maximum SumScore algorithm reach
18. results shown Figure 34. EDBP reaches SumScore 17 almost 3 minutes
CPU time IJGP reaches SumScore 13 3 minutes. SumScores
IJGP EDBP remain unchanged interval 3 20 minutes. looking
raw results, found IJGPs score zero 5 instances 18.
singleton consistency component implemented via SAT solver finish 20 minutes
instances. Although singleton consistency step generally helps reduce practical time
complexity IJGP instances, adversely affects WCSP instances.
Promedas instances Noisy-OR binary Bayesian networks (Pearl, 1988). instances
characterized extreme marginals. Namely, given variable, marginals form
(1 , ) small positive constant. Exact marginals available 46
submitted 238 instances. structured problems (see Figure 35), see EDBP
best performing scheme reaching SumScore close 46 7 minutes CPU time
TLSBP IJGP able reach SumScore 40 20 minutes.

6. Related Work
numerous lines research devoted study belief propagation algorithms,
message-passing schemes general. Throughout paper mentioned compared
related work, especially experimental evaluation section. give short summary
developments belief propagation present related schemes mentioned
before. additional information see also recent review Koller (2010).
decade ago, Iterative Belief Propagation (Pearl, 1988) received lot interest
information theory coding community. realized two best error-correcting
decoding algorithms actually performing belief propagation networks cycles.
LDPC code (low-density parity-check) introduced long time ago Gallager (1963), considered one powerful promising schemes often performs impressively close
Shannons limit. Turbo codes (Berrou, Glavieux, & Thitimajshima, 1993) also efficient
practice understood instance belief propagation (McEliece et al., 1998).
considerable progress towards understanding behavior performance BP made
concepts statistical physics. Yedidia et al. (2001) showed IBP strongly related
Bethe-Peierls approximation variational (Gibbs) free energy factor graphs. Bethe
approximation particular case general Kikuchi (1951) approximation. Generalized
Belief Propagation (Yedidia et al., 2005) application Kikuchi approximation works
clusters variables, structures called region graphs. Another algorithm employs
region-based approach Cluster Variation Method (CVM) (Pelizzola, 2005). algorithms
focus selecting good region-graph structure account over-counting (and over-overcounting, etc.) evidence. view generalized belief propagation broadly belief
propagation nodes clusters functions. Within view IJGP, GBP defined
Yedidia et al. (2001), well CVM, special realizations generalized belief propagation.
Belief Propagation Partially Ordered Sets (PBP) (McEliece & Yildirim, 2002) also generalized form Belief Propagation minimizes Bethe-Kikuchi variational free energy,

323

fiM ATEESCU , K ASK , G OGATE & ECHTER

works message-passing algorithm data structures called partially ordered sets,
junction graphs factor graphs examples. one-to-one correspondence
fixed points PBP stationary points free energy. PBP includes special cases many
variants belief propagation. noted before, IJGP basically PBP.
Expectation Propagation (EP) (Minka, 2001) iterative approximation algorithm computing posterior belief Bayesian networks. combines assumed-density filtering (ADF),
extension Kalman filter (used approximate belief states using expectations, mean
variance), IBP, iterates expectations consistent throughout network.
TreeEP (Minka & Qi, 2004) deals cyclic problem reducing problem graph tree subgraph approximating remaining edges. relationship EP GBP discussed
Welling, Minka, Teh (2005).
Survey Propagation (SP) (Braunstein et al., 2005) solves hard satisfiable (SAT) problems using
message-passing algorithm factor graph consisting variable clause nodes. SP inspired
algorithm called Warning Propagation (WP) BP. WP determine tree-problem
SAT, provide solution. BP compute number satisfying assignments
tree-problem, well fraction assignments variable true. two
algorithms used heuristics define SP algorithm, shown efficient
either arbitrary networks. SP still heuristic algorithm guarantee
convergence. SP inspired new concept cavity method statistical physics,
interpreted BP variables take values true false, also extra
dont care value. detailed treatment see book Mezard Montanari (2009).

7. Conclusion
paper investigated family approximation algorithms Bayesian networks,
could also extended general graphical models. started bounded inference algorithms
proposed Mini-Clustering (MC) scheme generalization Mini-Buckets arbitrary tree
decompositions. power lies anytime algorithm governed user adjustable i-bound
parameter. MC start small i-bound keep increasing long given time,
accuracy usually improves time. enough time given it, guaranteed
become exact. One virtues also produce upper lower bounds, route
explored paper.
Inspired success iterative belief propagation (IBP), extended MC iterative
message-passing algorithm called Iterative Join-Graph Propagation (IJGP). IJGP operates general join-graphs contain cycles, sill governed i-bound parameter. Unlike IBP,
IJGP guaranteed become exact given enough time.
also make connections well understood consistency enforcing algorithms constraint
satisfaction, giving strong support iterating messages, giving insight performance
IJGP (IBP). show that: (1) value variable assessed zero-belief
iteration IJGP, remains zero-belief subsequent iterations; (2) IJGP converges
finite number iterations relative set zero-beliefs; and, importantly (3) set
zero-beliefs decided iterative belief propagation methods sound. Namely
zero-belief determined IJGP corresponds true zero conditional probability relative
given probability distribution expressed Bayesian network.

324

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

experimental evaluation IJGP, IBP MC provided, IJGP emerges one
powerful approximate algorithms belief updating Bayesian networks.

References
Arnborg, S. A. (1985). Efficient algorithms combinatorial problems graphs bounded
decomposability - survey. BIT, 25, 223.
Bacchus, F., Dalmao, S., & Pitassi, T. (2003). Value elimination: Bayesian inference via backtracking search. Proceedings Nineteenth Conference Uncertainty Artificial
Intelligence (UAI03), pp. 2028.
Berrou, C., Glavieux, A., & Thitimajshima, P. (1993). Near Shannon limit error-correcting coding:
Turbo codes. Proceedings 1993 International Conference Communications, pp.
10641070.
Bilmes, J., & Dechter, R. (2006). Evaluation probabilistic inference systems UAI06.
http://ssli.ee.washington.edu/ bilmes/uai06InferenceEvaluation/.
Braunstein, A., Mezard, M., & Zecchina, R. (2005). Survey propagation: algorithm satisfiability. Random Struct. Algorithms, 27(2), 201226.
Chavira, M., & Darwiche, A. (2008). probabilistic inference weighted model counting.
Artificial Intelligence, 172(67), 772799.
Chavira, M. D., Darwiche, A., & Jaeger, M. (2006). Compiling relational bayesian networks
exact inference. International Journal Approximate Reasoning, 42(1-2), 420.
Chertkov, M., & Chernyak, V. Y. (2006). Loop series discrete statistical models graphs.
Journal Statistical Mechanics: Theory Experiment, P6009.
Choi, A., Chavira, M., & Darwiche, A. (2007). Node splitting: scheme generating upper
bounds bayesian networks. Proceedings Twenty Third Conference Uncertainty
Artificial Intelligence (UAI07), pp. 5766.
Choi, A., & Darwiche, A. (2006a). edge deletion semantics belief propagation
practical impact approximation quality. Proceedings Twenty-First National
Conference Artificial Intelligence (AAAI06), pp. 11071114.
Choi, A., & Darwiche, A. (2006b). variational approach approximating bayesian networks
edge deletion. Proceedings Twenty Second Conference Uncertainty Artificial
Intelligence (UAI06), pp. 8089.
Cooper, G. F. (1990). computational complexity probabistic inferences. Artificial Intelligence, 42, 393405.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference bayesian belief networks
NP-hard. Artificial Intelligence, 60(1), 141153.
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 125(1-2), 541.
325

fiM ATEESCU , K ASK , G OGATE & ECHTER

Dechter, R. (1992). Constraint networks. Encyclopedia Artificial Intelligence, 276285.
Dechter, R. (1996). Bucket elimination: unifying framework probabilistic inference algorithms. Proceedings Twelfth Conference Uncertainty Artificial Intelligence
(UAI96), pp. 211219.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial Intelligence,
113, 4185.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann Publishers.
Dechter, R., Bidyuk, B., Mateescu, R., & Rollon, E. (2010). power belief propagation:
constraint propagation perspective. Dechter, R., Geffner, H., & Halpern, J. (Eds.), Heuristics, Probabilities Causality: Tribute Judea Pearl.
Dechter, R., Kask, K., & Larrosa, J. (2001). general scheme multiple lower bound computation constraint optimization. Proceedings Seventh International Conference
Principles Practice Constraint Programming (CP01), pp. 346360.
Dechter, R., Kask, K., & Mateescu, R. (2002). Iterative join-graph propagation. Proceedings
Eighteenth Conference Uncertainty Artificial Intelligence (UAI02), pp. 128136.
Dechter, R., & Mateescu, R. (2003). simple insight iterative belief propagations success.
Proceedings Nineteenth Conference Uncertainty Artificial Intelligence (UAI03),
pp. 175183.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces graphical models. Artificial Intelligence, 171(2-3), 73106.
Dechter, R., & Pearl, J. (1987). Network-based heuristics constraint satisfaction problems.
Artificial Intelligence, 34, 138.
Dechter, R., & Pearl, J. (1989). Tree clustering constraint networks. Artificial Intelligence, 38,
353366.
Dechter, R., & Rish, I. (1997). scheme approximating probabilistic inference. Proceedings
Thirteenth Conference Uncertainty Artificial Intelligence (UAI97), pp. 132141.
Dechter, R., & Rish, I. (2003). Mini-buckets: general scheme approximating inference.
Journal ACM, 50(2), 107153.
Fishelson, M., & Geiger, D. (2003). Optimizing exact genetic linkage computations. Proceedings
Seventh Annual International Conference Computational Biology (RECOMB03),
pp. 114121.
Gallager, R. G. (1963). Low-Density Parity-Check Codes. MIT Press, Cambridge, MA.
Gogate, V., & Dechter, R. (2007). SampleSearch: scheme searches consistent samples.
Proceedings Eleventh International Conference Artificial Intelligence Statistics
(AISTATS07), pp. 147154.
326

fiJ OIN -G RAPH P ROPAGATION LGORITHMS

Gogate, V. (2009). Sampling Algorithms Probabilistic Graphical models Determinism.
Ph.D. thesis, School Information Computer Sciences, University California, Irvine.
Gomez, V., Mooji, J. M., & Kappen, H. J. (2007). Truncating loop series expansion belief
propagation. Journal Machine Learning, 8, 19872016.
Gottlob, G., Leone, N., & Scarcello, F. (2000). comparison structural CSP decomposition
methods. Artificial Intelligence, 243282.
Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. (1990). Bayesian updating causal probabilistic
networks local computation. Computational Statistics Quarterly, 4, 269282.
Kask, K. (2001). Approximation algorithms graphical models. Ph.D. thesis, Information
Computer Science, University California, Irvine.
Kask, K., & Dechter, R. (2001). general scheme automatic search heuristics specification
dependencies. Artificial Intelligence, 129(1-2), 91131.
Kask, K., Dechter, R., Larrosa, J., & Dechter, A. (2005). Unifying cluster-tree decompositions
reasoning graphical models. Artificial Intelligence, 166 (1-2), 165193.
Kikuchi, R. (1951). theory cooperative phenomena. Phys. Rev., 81(6), 9881003.
Koller, D. (2010). Belief propagation loopy graphs. Dechter, R., Geffner, H., & Halpern, J.
(Eds.), Heuristics, Probabilities Causality: Tribute Judea Pearl.
Larrosa, J., Kask, K., & Dechter, R. (2001). mini-bucket: scheme approximating
combinatorial optimization tasks. Tech. rep., University California Irvine.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computation probabilities graphical
structures application expert systems. Journal Royal Statistical Society,
Series B, 50(2), 157224.
Mackworth, A. K. (1977). Consistency networks relations. Artificial Intelligence, 8(1), 99
118.
Mezard, M., & Montanari, A. (2009). Information, Physics Computation. Oxford University
Press.
Mezard, M., Parisi, G., & Zecchina, R. (2002). Analytic algorithmic solution random satisfiability problems. Science, 297, 812815.
Maier, D. (1983). Theory Relational Databases. Computer Science Press, Rockville, MD.
Mateescu, R., Dechter, R., & Kask, K. (2002). Tree approximation belief updating. Proceedings Eighteenth National Conference Artificial Intelligence (AAAI02), pp. 553559.
McEliece, R. J., MacKay, D. J. C., & Cheng, J. F. (1998). Turbo decoding instance Pearls
belief propagation algorithm. IEEE J. Selected Areas Communication, 16(2), 140152.
McEliece, R. J., & Yildirim, M. (2002). Belief propagation partially ordered sets. Mathematical Systems Theory Biology, Communications, Computation, Finance, pp. 275300.
327

fiM ATEESCU , K ASK , G OGATE & ECHTER

Minka, T. (2001). Expectation propagation approximate bayesian inference. Proceedings
Seventeenth Annual Conference Uncertainty Artificial Intelligence (UAI01), pp.
362369.
Minka, T., & Qi, Y. (2004). Tree-structured approximations expectation propagation. Advances Neural Information Processing Systems 16 (NIPS03).
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Pelizzola, A. (2005). Cluster variation method statistical physics probabilistic graphical
models. Journal Physics A: Mathematical General, 38(33), R309R339.
Rollon, E., & Dechter, R. (2010). Evaluating partition strategies mini-bucket elimination.
Eleventh International Symposium Artificial Intelligence Mathematics (ISAIM10).
Roth, D. (1996). hardness approximate reasoning. Artificial Intelligence, 82(1-2), 273
302.
Shafer, G. R., & Shenoy, P. P. (1990). Probability propagation. Annals Mathematics Artificial
Intelligence, 2, 327352.
Shenoy, P. P. (1992). Valuation-based systems bayesian decision analysis. Operations Research,
40, 463484.
Welling, M., Minka, T. P., & Teh, Y. W. (2005). Structured region graphs: Morphing ep gbp.
Proceedings Twenty First Conference Uncertainty Artificial Intelligence (UAI05),
pp. 609614.
Wexler, Y., & Meek, C. (2008). MAS: multiplicative approximation scheme probabilistic inference. Proceedings Advances Neural Information Processing Systems 21 (NIPS08),
pp. 17611768.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2000). Generalized belief propagation. Tech. rep.
TR2000-26, Mitsubishi Electric Research Laboratories.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2001). Generalized belief propagation. Advances
Neural Information Processing Systems 13 (NIPS00), pp. 689695.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2005). Constructing free energy approximations
generalized belief propagation algorithms. IEEE Transactions Information Theory, 51,
22822312.
Yuan, C., & Druzdzel, M. J. (2003). importance sampling algorithm based evidence prepropagation. Proceedings Nineteenth Conference Uncertainty Artificial Intelligence (UAI03), pp. 624631.
Zhang, N. L., Qi, R., & Poole, D. (1994). computational theory decision networks. International Journal Approximate Reasoning, 11, 83158.

328

fiJournal Artificial Intelligence Research 37 (2010) 99-139

Submitted 08/09; published 02/10

Interactive Cost Configuration Decision Diagrams
Henrik Reif Andersen

hra@configit.com

Configit A/S
DK-2100 Copenhagen, Denmark

Tarik Hadzic

t.hadzic@4c.ucc.ie

Cork Constraint Computation Centre
University College Cork
Cork, Ireland

David Pisinger

pisinger@man.dtu.dk

DTU Management
Technical University Denmark
DK-2800 Kgs. Lyngby, Denmark

Abstract
many AI domains product configuration, user interactively specify
solution must satisfy set constraints. scenarios, offline compilation
feasible solutions tractable representation important approach delivering efficient backtrack-free user interaction online. particular, binary decision diagrams
(BDDs) successfully used compilation target product service configuration. paper discuss extend BDD-based configuration scenarios
involving cost functions express user preferences.
first show efficient, robust easy implement extension possible
cost function additive, feasible solutions represented using multi-valued decision diagrams (MDDs). also discuss effect MDD size cost function
non-additive encoded explicitly MDD. discuss interactive configuration presence multiple cost functions. prove even simplest form,
multiple-cost configuration NP-hard input MDD. However, solving two-cost
configuration develop pseudo-polynomial scheme fully polynomial approximation scheme. applicability approach demonstrated experiments
real-world configuration models product-catalogue datasets. Response times generally within fraction second even large instances.

1. Introduction
Interactively specifying solution must satisfy number combinatorial restrictions
important problem many AI domains related decision making: buying
product online, selling insurance policy setting piece equipment. Solutions
often modeled assignments variables constraints imposed.
assigning variables without sufficient guidance, user might forced backtrack, since
choices made cannot extended way would satisfy
succeeding constraints. improve usability interaction therefore important
indicate user values participate least one remaining solution.
c
2010
AI Access Foundation. rights reserved.

fiAndersen, Hadzic, & Pisinger

user assigning values guaranteed able reach feasible solution
never forced backtrack. refer task computing values
calculating valid domains (CVD). Since computationally challenging (NP-hard)
problem, short execution times important interactive setting,
suggested compile offline (prior user interaction) set feasible solutions
representation form supports efficient execution CVD online interaction.
Mller, Andersen, Hulgaard (2002) Hadzic, Subbarayan, Jensen, Andersen,
Mller, Hulgaard (2004) investigated approach using binary decision diagrams (BDDs) compilation target. BDDs one data-structures investigated
knowledge compilation community preprocess original problem formulations
tractable representations enhance solving subsequent tasks. CVD one
tasks occurring configuration domain. Knowledge compilation successfully applied number areas planning, diagnosis, model checking etc.
Beside BDDs, number structures, various sublanguages negation normal forms (NNFs) (Darwiche & Marquis, 2002), AND/OR diagrams (Mateescu, Dechter, &
Marinescu, 2008), finite state automata (Vempaty, 1992; Amilhastre, Fargier, & Marquis,
2002) various extensions decision diagrams (Drechsler, 2001; Wegener, 2000; Meinel
& Theobald, 1998) used compilation targets. suitable interactive configuration well. particular, Vempaty (1992) suggested compiling constraints
automaton. However, BDDs investigated data structures tool
support unrivaled emerging representations. many highly optimized
open-source BDD packages (e.g., Somenzi, 1996; Lind-Nielsen, 2001) allow easy
efficient manipulation BDDs. contrast, publicly available, open-source compilers
still developed many newer representations. particular, application BDDs
configuration resulted patent approval (Lichtenberg, Andersen, Hulgaard, Mller, &
Rasmussen, 2001) establishment spinoff company Configit A/S1 .
work paper motivated decision making scenarios solutions
associated cost function, expressing implicitly properties price, quality, failure probability etc. user might prefer one solution another given value
properties. natural way user expresses cost preferences configuration
setting bound minimal maximal cost solution willing accept.
therefore study problem calculating weighted valid domains (wCVD),
eliminate values every valid solution expensive user-provided
maximal cost. present configurator supports efficient cost bounding wide
class additive cost functions. approach easily implementable scales well
instances previously compiled BDDs standard interactive configuration. cornerstone approach reuse robust compilation constraints
BDD, extract corresponding multi-valued decision diagram (MDD).
resulting MDD allows us label edges weights utilize efficient shortest path algorithms label nodes filter expensive values MDD edges. MDD extraction
technique novel, labeling edges decision diagram suggested works well.
generic interpretation (Wilson, 2005), edges decision diagram labeled
elements semiring support algebraic computations relevant probabilistic rea1. http://www.configit.com

100

fiInteractive Cost Configuration Decision Diagrams

soning, optimization etc. Amilhastre et al. (2002) suggest labeling edges automaton
reason abut optimal restorations explanations. general, many knowledge compilation
structures weighted counterparts, many captured framework
valued negation normal forms (VNNFs) (Fargier & Marquis, 2007). structures
utilized probabilistic reasoning, diagnosis, tasks involving reasoning
real-valued rather Boolean functions. principle used wCVD
queries, public tool support weighted variants less available tailored
tasks outside configuration domain.
extend approach support valid domains computation presence
multiple cost functions. user often multiple conflicting objectives,
satisfied simultaneously. Traditional approaches multi-criteria optimization (Figueira,
Greco, & Ehrgott, 2005; Ehrgott & Gandibleux, 2000) typically interact user
way unsuitable configuration setting cost functions combined single
objective interaction step non-dominated solutions sampled displayed user. Based user selections adequate aggregation costs performed
next interaction step. suggest configuration-oriented interaction approach domains bounded respect multiple costs. prove
particularly challenging problem. Computing valid domains MDD presence
two cost functions (2-wCVD) NP-hard, even simplest extension linear inequalities
positive coefficients Boolean variables. Despite negative result, provide
implementation 2-wCVD queries pseudo-polynomial time space develop
fully polynomial time approximation scheme (FPTAS). prove pseudo-polynomial
algorithm hence fully polynomial approximation scheme exists computing domains presence arbitrarily many cost functions since NP-hard problem
strong sense. Finally, demonstrate experimental evaluation applicability wCVD 2-wCVD query large real-world configuration models
product-catalogue datasets. best knowledge, present first interactive configurator supporting configuration wrt. cost restrictions backtrack-free
complete manner. constitutes novel addition existing product-configuration
approaches well approaches within multi-criteria decision making (Figueira et al.,
2005).
remainder paper organized follows. Section 2 describe background
work notation. Section 3 describe approach implementing wCVD query
MDD Section 4 show compile MDD. Section 5 discuss
configuring presence multiple costs. Section 6 present empirical evaluation
approach. Section 7 describe related work finally conclude Section 8.

2. Preliminaries
briefly review important concepts background.
2.1 Constraint Satisfaction Problems
Constraint satisfaction problems (CSPs) form framework modeling solving combinatorial problems, solution problem formulated assignment
101

fiAndersen, Hadzic, & Pisinger

variables satisfy certain constraints. standard form, CSP involves finite
number variables, defined finite domains.
Definition 1 (CSP) constraint satisfaction problem (CSP) triple (X, D, F )
X set variables {x1 , . . . , xn }, = D1 . . .Dn Cartesian product finite
domains D1 , . . . , Dn F = {f1 , ..., fm } set constraints defined variables X.
constraint f function defined subset variables Xf X called scope
f . maps assignment Xf variables {0, 1} 1 indicates f
satisfied 0 indicates f violated assignment. solution assignment
variables X satisfies constraints simultaneously.
Formally, assignment values a1 , . . . , variables x1 , . . . , xn denoted set
pairs = {(x1 , a1 ), . . . , (xn , )}. domain assignment dom() set
variables assigned: dom() = {xi | Di .(xi , a) } variables
assigned, i.e. dom() = X, refer total assignment. say total
assignment valid satisfies rules, denoted |= F . partial
assignment , dom() X valid extended total assignment 0
valid 0 |= F . define solution space Sol set valid total assignments,
i.e. Sol = { | |= F, dom() = X}.
2.2 Interactive Configuration
Interactive configuration important application domain user assisted
specifying valid configuration (of product, service something else) interactively
providing feedback valid options unspecified attributes. problem arises
number domains. example, buying product, user specify number
product attributes. attribute combinations might feasible guidance
provided, user might reach dead-end interacting system.
forced backtrack, might seriously decrease user satisfaction.
many cases, valid configurations implicitly described specifying restrictions
combining product attributes. use CSP model represent restrictions,
CSP solution corresponds valid configuration. configurable attribute
represented variable, attribute option corresponds value
variable domain. Example 1 illustrate simple configuration problem CSP
model.
Example 1 specify T-shirt choose color (black, white, red, blue),
size (small, medium, large) print (Men Black - MIB Save
Whales - STW). choose MIB print color black chosen well,
choose small size STW print (including large picture whale)
cannot selected picture whale fit small shirt. configuration
problem (X, D, F ) T-shirt example consists variables X = {x1 , x2 , x3 } representing
color, size print. Variable domains D1 = {0, 1, 2, 3} (black , white, red , blue), D2 =
{0, 1, 2} (small , medium, large), D3 = {0, 1} (MIB , STW ). two rules translate
F = {f1 , f2 }, f1 x3 = 0 x1 = 0 (MIB black ) f2 (x2 = 0 x3 6= 1)
(small STW ). |D1 ||D2 ||D3 | = 24 possible assignments. Eleven
assignments valid configurations form solution space shown Fig. 1.
102

fiInteractive Cost Configuration Decision Diagrams

(black , small , MIB )
(black , medium, MIB )
(black , medium, STW )
(black , large, MIB )

(black , large, STW )
(white, medium, STW )
(white, large, STW )
(red , medium, STW )

(red , large, STW )
(blue, medium, STW )
(blue, large, STW )

Figure 1: Solution space T-shirt example.
fundamental task concerned paper calculating valid
domains (CVD) query. partial assignment representing previously made user assignments, configurator calculates displays valid domain VD [] Di
unassigned variable xi X \ dom(). domain valid contains
values extended total valid assignment 0 . example,
user selects small T-shirt (x2 = 0), valid domains restricted MIB print
V D3 = {0} black color V D1 = {0}.
Definition 2 (CVD) Given CSP model (X, D, F ), given partial assignment compute valid domains:
VDi [] = {a Di | 0 .(0 |= F {(xi , a)} 0 )}
task main interest since delivers important interaction requirements: backtrackfreeness (user never forced backtrack) completeness (all valid configurations
reachable) (Hadzic et al., 2004). queries relevant supporting
user interaction explanations restorations failure, recommendations
relevant products, etc., CVD essential operation mode interaction
primary importance paper.
2.3 Decision Diagrams
Decision diagrams form family rooted directed acyclic graphs (DAGs) node
u labeled variable xi outgoing edges e labeled value Di .
node may one outgoing edge label. decision diagram
contains one terminal nodes, labeled constant outgoing
edges. well known member family binary decision diagrams (BDDs)
(Bryant, 1986) used manipulating Boolean functions many areas,
verification, model checking, VLSI design (Meinel & Theobald, 1998; Wegener, 2000;
Drechsler, 2001) etc. paper primarily operate following variant
multi-valued decision diagrams:
Definition 3 (MDD) MDD denoted rooted directed acyclic graph (V, E),
V set vertices containing special terminal vertex 1 root r V . Further,
var : V {1, . . . , n + 1} labeling nodes variable index var(1) =
n + 1. edge e E denoted triple (u, u0 , a) start node u, end node u0
associated value a.
work ordered MDDs. total ordering < variables assumed
edges (u, u0 , a), var(u) < var(u0 ). convenience assume variables
103

fiAndersen, Hadzic, & Pisinger

X ordered according indices. Ordered MDDs considered
arranged n layers vertices, layer labeled variable index.
denote Vi set nodes labeled xi , Vi = {u V | var(u) = i}. Similarly,
denote Ei set edges originating Vi , i.e. Ei = {e(u, u0 , a) E |
var(u) = i}. Unless otherwise specified, assume path root
terminal, every variable labels exactly one node.
MDD encodes CSP solution set Sol D1 . . . Dn , defined variables
{x1 , . . . , xn }. check whether assignment = (a1 , . . . , ) D1 . . . Dn Sol
traverse root, every node u labeled variable xi , follow edge
labeled ai . edge solution, i.e., 6 Sol. Otherwise,
traversal eventually ends terminal 1 Sol. denote p : u1
u2
path MDD u1 u2 . Also, edges u u0 sometimes denoted
e : u u0 . value edge e(u, u0 , a) sometimes denoted v(e).
make distinction paths assignments. Hence, set solutions represented
MDD Sol = {p | p : r
1}. fact, every node u Vi associated
subset solutions Sol(u) = {p | p : u
1} Di . . . Dn .
x1
0

1

x2
0
x3

1

x3
0

2

1
x3

0

x3

1 0

1

1

1

x1
2

3

0 1

x2

x2

2

1

x3

x3

x3

x3

1

1

1

1

x2
2

1

1

x2
2

x2

0 2 1
x3

x3

2 3

1 2

x3
0 0 1

x3
1

1

(a) MDD merging.

(b) merged MDD.

Figure 2: uncompressed merged MDD T-Shirt example.
Decision diagrams exponentially smaller size solution set
encode merging isomorphic subgraphs. Two nodes u1 , u2 isomorphic encode
solution set Sol(u1 ) = Sol(u2 ). Figure 2 show fully expanded MDD 2(a)
equivalent merged MDD 2(b) T-shirt solution space. addition merging
isomorphic subgraphs, another compression rule usually utilized: removing redundant
nodes. node u Vi redundant Di outgoing edges, pointing
node u0 . nodes eliminated redirecting incoming edges u u0 deleting u
V . introduces long edges skip layers. edge e(u, u0 , a) long var(u)+1 <
var(u0 ). case, e encodes set solutions: {a} Dvar(u)+1 . . . Dvar(u0 )1 .
refer MDD merging isomorphic nodes removal redundant
nodes taken place reduced MDD, constitutes multi-valued generalization
BDDs typically reduced ordered. reduced MDD T-shirt CSP
shown Figure 3. paper, unless emphasized otherwise, MDD always
assume ordered merged reduced MDD, since exposition simpler, removal
redundant nodes linear effect size. Given variable ordering
104

fiInteractive Cost Configuration Decision Diagrams

unique merged MDD given CSP (X, D, F ) solution set Sol.
size MDD depends critically ordering, could vary exponentially. grow
exponentially number variables, practice, many interesting problems
size surprisingly small.
x1
0 1
x2

2 3
x2

0
x3

12
2 1

0

x3
1

1

Figure 3: reduced MDD T-shirt example.
Interactive Configuration Decision Diagrams. particularly attractive property decision diagrams support efficient execution number important
queries, checking consistency, validity, equivalence, counting, optimization etc.
utilized number application domains problem description
known offline (diagnosis, verification,etc.). particular, calculating valid domains linear
size MDD. Since calculating valid domains NP-hard problem size
input CSP model, possible guarantee interactive response real-time.
fact, unacceptably long worst-case response times empirically observed
purely search-based approach computing valid domains (Subbarayan et al., 2004).
Therefore, compiling CSP solutions off-line (prior user interaction) decision
diagram, efficiently (in size MDD) compute valid domains online
interaction user. important note order user decides
variables completely unconstrained, i.e. depend ordering MDD variables. previous work utilized Binary Decision Diagrams (BDDs) represent
valid configurations CVD queries executed efficiently (Hadzic et al., 2004).
course, BDDs might exponentially large input CSP, many classes
constraints surprisingly compact.

3. Interactive Cost Processing MDDs
main motivation work extending interactive configuration approach
Mller et al. (2002), Hadzic et al. (2004), Subbarayan et al. (2004) situations
addition CSP model (X, D, F ) involving hard constraints, also cost
function:
c : D1 . . . Dn R.
product configuration setting, could product price. uncertainty setting,
cost function might indicate probability occurrence event represented
105

fiAndersen, Hadzic, & Pisinger

solution (failure hardware component, withdrawal bid auction etc.).
decision support context, cost function might indicate user preferences.
number cost-related queries user might interested, e.g. finding optimal
solution, computing probable explanation. We, however, assume user
interested tight control variable values well cost selected solutions.
example, user might desire specific option xi = a, would also care
would assignment affect cost remaining optimal solutions.
communicate information user, allow strike right balance
cost variable values allowing interactively limit maximal cost
product addition assigning variable values. Therefore, paper primarily
concerned implementing weighted CVD (wCVD) query: user-specified maximum
cost K, indicate values unassigned variable domains extended
total assignment valid costs less K. on, assume
user interested bounding maximal cost (limiting minimal cost symmetric).
Definition 4 (wCVD) Given CSP model (X, D, F ), cost function c : R
maximal cost K, given partial assignment weighted CVD (wCVD) query requires
computation valid domains:
VDi [, K] = {a Di | 0 .(0 |= F {(xi , a)} 0 c(0 ) K)}
section assume MDD representation CSP solutions already
generated offline compilation step. postpone discussion MDD compilation
Section 4 discuss delivering efficient online interaction top MDD.
first discuss practicability implementing wCVD queries explicit encoding
costs MDD. provide practical efficient approach implementing
wCVD MDD cost function additive. Finally, discuss
extensions handling expressive cost functions.
3.1 Handling Costs Explicitly
immediate approach interactively handling cost function treat cost
solution attribute, i.e. add variable variables X add constraint
= c(x1 , . . . , xn )

(1)

formulas F enforce equal total cost. resulting configuration model
compiled MDD 0 user able bound cost restricting domain
y.
Assuming variable ordering x1 < . . . < xn original CSP model (X, D, F ),
assuming inserted cost variable i-th position, new variable set X 0
variable ordering x01 < . . . < x0n+1 s.t. x01 = x1 , . . . , x0i1 = xi1 , x0i = x0i+1 =
xi , . . . , x0n+1 = xn . domain Di0 variable x0i set feasible costs C(Sol) =
{c(s) | Sol}. demonstrate MDD 0 may exponentially larger
.
Lemma 1 |Ei0 | |C(Sol)|.
106

fiInteractive Cost Configuration Decision Diagrams

Proof 1 i-th layer MDD 0 corresponding variable y, cost c C(Sol)
must least one path p : r
1 c(p) = c, path, edge e Ei0
i-th layer must labeled v(e) = c. Hence, cost must least
one edge Ei0 . proves lemma.
0
Furthermore, least one layers nodes Vi0 , Vi+1
number nodes greater
p 0
|Ei |. follows following lemma:
0 | |E 0 |.
Lemma 2 i-th layer MDD 0 , |Vi0 | |Vi+1


0 | pairs nodes (u , u ) V 0 V 0 , statement
Proof 2 Since |Vi0 ||Vi+1
1 2
i+1

follows fact pair (u1 , u2 ) one edge e : u1 u2 .
Namely, every solution p3 formed concatenating paths p1 : r
u1 p2 : u2
1
unique cost c(p3 ). However, two edges e1 , e2 : u1 u2 , would
different values v(e1 ) 6= v(e2 ). then, solution c(p3 ) would correspond
two different costs v(e1 ), v(e2 ).

considerations see whenever range possible costs C(Sol)
exponential, resulting MDD 0 would exponentially large well. would
result significantly increased size |V 0 |/|V |, particularly large number
isomorphic nodes would become non-isomorphic variable introduced
(since root paths different costs). extreme instance behavior
presented Example 2. Furthermore, even C(Sol) large, could orders
magnitude increase size 0 due breaking isomorphic nodes MDD
empirically demonstrated Section 6, Table 3, number configuration
instances. major disadvantage otherwise efficient CVD algorithms become
unusable since operate significantly larger structure.
Example 2 Consider model C(X, D, F ) constraints F = {}, Boolean variables Dj = {0, 1}, j = 1 . . . , n. solution space includes assignments Sol = D1
. . . Dn corresponding MDD (V, E) one vertex two
layer,
Pnedgesj1
xj ,
|V | = n + 1, |E| = 2 n. use cost function: c(x1 , . . . , xn ) = j=1 2
exponential number feasible costs C(Sol) = {0, . . . , 2n 1}. Hence, |Ei0 | 2n
0 | greater
i-th layer corresponding variable y, least one layers |Vi0 |, |Vi+1

n/2
2n = 2 .
However, significant node isomorphism , adding variable
necessarily lead significant increase size. extreme instance MDD
isomorphic nodes, example every edge labeled unique value.
MDD, number non-terminal nodes n |Sol|. adding cost variable
y, resulting MDD would add one node per path, leading MDD
(n + 1) |Sol| nodes. translates minor increase size: |V 0 |/|V | = (n + 1)/n.
property empirically demonstrated Section 6, Table 3, product-catalogue
datasets. remainder paper develop techniques tailored instances
large increase size occurs. avoid explicit cost encoding aim exploit
structure cost function implement wCVD.
107

fiAndersen, Hadzic, & Pisinger

3.2 Processing Additive Cost Functions
One main contributions paper practical efficient approach deliver
wCVD queries cost function additive. additive cost function form
c(x1 , . . . , xn ) =

n
X

ci (xi )

i=1

cost ci (ai ) R assigned every variable xi every value domain
ai Di .
Additive functions one important frequently used modeling constructs. number important combinatorial problems modeled integer linear programs often constraints objective function linear, i.e. represent
special cases additive cost functions. multi-attribute utility theory user preferences
certain assumptions aggregated single additive function weighted
summation utilities individual attributes. product configuration context, many
properties additive memory capacity computer total weight.
particular, based experience commercially applying configuration technology,
price product often modeled (weighted) sum prices individual parts.
3.2.1 Labeling Approach
Assuming given MDD representation solution space Sol cost
function c, approach answering wCVD queries based three steps: 1) restricting
MDD wrt. latest user assignment, 2) labeling remaining nodes executing shortest
path algorithms 3) filtering expensive values using node labels.
Restricting MDD. given user assignment xi = ai , xi
unassigned variables, regardless position MDD variable ordering. initialize
MDD pruning removing edges e(u, u0 , a), agreement latest
assignment, i.e. var(u) = 6= ai . might cause number edges
nodes become unreachable terminal root removed last edge
set children edges Ch(u) parent edges P (u0 ). unreachable edge must
removed well. pruning repeated fixpoint reached, i.e.
nodes edges removed. Algorithm 1 implements scheme O(|V | + |E|) time
space using queue Q maintain set edges yet removed.
Note unassigning user assignment xi = ai easily implemented linear
time well. suffices restore copy initial MDD , perform restriction wrt.
partial assignment \ {(xi , ai )} current assignment. Algorithm 1 easily
extended purpose initializing edge removal list Q edges incompatible
wrt. assignments .
Computing Node Labels. Remaining edges e(u, u0 , a) layer Ei implicitly
labeled c(e) = ci (a). second step compute MDD node u V
upstream cost shortest path root r u, denoted U [u], downstream
cost shortest path u terminal 1, denoted D[u]:
(
(
)
)
X
X
U [u] = min
c(e)
(2)
c(e) , D[u] = min
p:r

u

p:u

ep

108

1

ep

fiInteractive Cost Configuration Decision Diagrams

Algorithm 1: Restrict MDD.
Data: MDD (V, E), variable xi , value ai
foreach e Ei , v(e) 6= ai
Q.push(e);
Q 6=
e(u, u0 , a) Q.pop();
delete e ;
Ch(u) =
foreach e : u00 u
Q.push(e);
P (u0 ) =
foreach e : u0 u00
Q.push(e);

Algorithm 2 computes U [u] D[u] labels (|V | + |E|) time space.
Algorithm 2: Update U, labels.
Data: MDD (V, E), Cost function c
D[] = , D[1] = 0;
foreach = n, . . . , 1
foreach u Vi
foreach e : u u0
D[u] = min{D[u], c(e) + D[u0 ]}
U [] = , U [r] = 0;
foreach = 1, . . . , n
foreach u Vi
foreach e : u u0
U [u0 ] = min{U [u0 ], c(e) + U [u]}

Computing Valid Domains. upstream downstream costs U, computed, efficiently compute valid domains VDi wrt. maximal cost bound K
since:
VDi [K] = {v(e) | U [u] + c(e) + D[u0 ] K, e : u u0 , u Vi }
(3)
achieved linear-time traversal (|V | + |E|) shown Algorithm 3.
Algorithm 3: Compute valid domains.
Data: MDD (V, E), Cost function c, Maximal cost K
foreach = 1, . . . , n
V Di = ;
foreach u Vi
foreach e : u u0
U [u] + c[e] + D[u0 ] K
V Di V Di {v(e)};

Hence overall interaction follows. Given current partial assignment , MDD
restricted wrt. Algorithm 1. Labels U, computed Algorithm 2
valid domains computed using Algorithm 3. execution algorithms
109

fiAndersen, Hadzic, & Pisinger

requires (|V | + |E|) time space. Hence, MDD representation solution
space available, interactively enforce additive cost restrictions linear time
space.
3.3 Processing Additive Costs Long Edges
scheme extended MDDs containing long edges. multivalued CSP
models large domains space savings due long edges might significant,
binary models binary decision diagrams (BDDs) significant savings possible.
Furthermore, similar fashion, scheme might adopted versions
decision diagrams contain long edges (with different semantics) zero-suppressed
BDDs long edge implies skipped variables assigned 0.
Recall reduced MDDs, redundant nodes u Vi Di outgoing edges,
pointing node u0 , eliminated. edge e(u, u0 , a) var(u) =
k var(u0 ) = l long k + 1 < l, case, e encodes set solutions:
{a} Dk+1 . . . Dl1 . labeling edges generalized accommodate
edges well. Let domains Dj0 , j = 1, . . . , n represent variable domains updated wrt.
current assignment, i.e. Dj0 = Dj xj unassigned, Dj0 = {[xj ]} otherwise. edge
e(u, u0 , a), (var(u) = k, var(u0 ) = l) removed 6 Dk0 analogous way MDD
pruning previous subsection. Otherwise, labeled
c(e) = ck (a) +

l1
X

j=k+1

min cj (a0 )

a0 Dj0

(4)

cost cheapest assignment xk , . . . , xl1 consistent edge
partial assignment . edges labeled, upstream downstream costs
U, computed (|V | + |E|) time, manner previous subsection.
However, computing valid domains extended. before, sufficient condition
VD existence edge e : u u0 , originating i-th layer u Vi
v(e) =
U [u] + c[e] + D[u0 ] K.
(5)
However, longer necessary condition, even edge satisfying (5),
could exist long edge skipping i-th layer still allows VD . therefore,
layer i, compute cost cheapest path skipping layer:
P [i] = min{U [u] + c(e) + D[u0 ] | e : u u0 E, var(u) < < var(u0 )}

(6)

edge skipping i-th layer, set P [i] = . Let cmin [i] denote cheapest
value Di0 , i.e. cmin [i] = minaDi0 ci (a). determine long edge allowing
VD , unassigned variable xi , following must hold:
P [i] + ci (a) cmin [i] K

(7)

Finally, sufficient necessary condition VD one conditions (5)
(7) holds. variable xi assigned value drawn valid domain previous
step, guaranteed V Di = {[xi ]} calculations necessary. Labels P [i]
110

fiInteractive Cost Configuration Decision Diagrams

Algorithm 4: Update P labels.
Data: MDD (V, E), Cost function c
P [] = ;
foreach = 1, . . . , n
foreach u Vi
foreach e : u u0
foreach j {var(u) + 1, . . . , var(u0 ) 1}
P [j] = min{P [j], U [u] + c(e) + D[u0 ]};

computed Algorithm 4 worst-case O(|E| n) time. Note bound
over-pessimistic assumes every edge |E| skipping every variable X.

auxiliary structures U, D, P computed, valid domains efficiently
extracted using Algorithm 5. unassigned variable xi , value Di valid
domain VDi [K] iff following holds: condition (7) satisfied edge e(u, u0 , a) E
condition (5) satisfied. non-assigned variable i, algorithm first checks
value Di whether supported skipping edge P [i]. Afterwards, scans
i-th layer extracts
P values supported edges Ei . achieved (|D| + |V | + |E|)
time, |D| = ni=1 |Di |.
Algorithm 5: Computing valid domains V Di .
Data: MDD (V, E), cost function C, maximal cost K
foreach = 1, . . . , n
V Di = ;
xi assigned ai
V Di {ai };
continue;
foreach Di
P [i] + ci (a) cmin [i] K
V Di V Di {a};
foreach u Vi
foreach e : u u0
U [u] + c[e] + D[u0 ] K
V Di V Di {v(e)};

Again, overall interaction remains same. Labels P incrementally updated
worst case O(|E| n) time. Valid domains extracted (|D| + |V | + |E|) time.
response changing cost restriction K, auxiliary labels need updated. Valid
domains extracted directly using Algorithm 5 (|D| + |V | + |E|) time.
3.4 Handling Non-Additive Cost Functions
certain interaction settings, cost function additive. example, user preferences might depend entire package features rather selection individual
feature. Similarly, price product need simple sum costs individual
parts, might depend combinations parts selected. general, cost
111

fiAndersen, Hadzic, & Pisinger

function c(x1 , . . . , xn ) might sum non-unary cost functions ci , = 1, . . . , k,
c(x1 , . . . , xn ) =

k
X

ci (Xi )

i=1

cost function ci expresses unique contribution combination features within
subset variables Xi X,

Dj R.
ci :
jXi

3.4.1 Non-Unary Labeling
approach extended handle non-unary costs adopting labeling techniques
used graphical representations (e.g., Wilson,
2005; Mateescu et al.,
Pk
2008). Assume given cost function c(x1 , . . . , xn ) = i=1 ci (Xi ). Let A(i) denote
set cost functions cj xi last variable scope cj :
A(i) = {cj | xi Xj xi0 6 Xj , i0 > i}.
Given assignment a(a1 , . . . , ai ) variables x1 , . . . , xi , evaluate every function cj
Ai . scope cj strict subset {x1 , . . . , xi }, set cj (a) value
u, u
cj (Xj (a)) Xj (a) projection onto Xj . Now, every path p : r
Vi+1 , last edge (in i-th layer) e Ei , label e sum cost functions
become completely instantiated assigning xi = ai :
X
cj (p).
(8)
c(e, p) =
cj A(i)

respect labeling, aP
cost solution represented path p would indeed
sum costs edges: ep c(e, p). order apply approach developed
additive cost functions Section 3.2, edge labeled cost
incoming path. However, possible general. therefore
expand original MDD, creating multiple copies e splitting incoming paths
ensure two paths p1 , p2 sharing copy e0 edge e induce edge
cost c(e0 , p1 ) = c(e0 , p2 ). MDD, denoted Mc , generated using example
search caching isomorphic nodes suggested Wilson (2005), extending
standard apply operator handle weights suggested Mateescu et al. (2008).
3.4.2 Impact Size
increase size Mc relatively cost-oblivious version depends additivity cost function c. example, fully additive cost functions (each scope
Xi contains single variable) Mc = , since label c(e) regardless
incoming path. However, entire cost function c single non-additive component
c1 (X1 ) global scope (X1 = X), edges last MDD layer labeled,
case explicit cost encoding MDD Section 3.1. must least
C(Sol) edges last layer, one feasible cost. Hence, range costs C(Sol)
112

fiInteractive Cost Configuration Decision Diagrams

exponential, size Mc . Furthermore, even C(Sol) limited size, increase Mc might significant due breakup node isomorphisms previous layers.
case explicit cost encoding (Section 3.1) effect demonstrated empirically
Section 6. similar effect size would occur graphical-representations.
example, representations exploiting global CSP structure - weighted cluster trees
(Pargamin, 2003) - adding non-additive cost functions increases size clusters,
required non-additive component ci (Xi ) least one cluster contains
entire scope Xi . Furthermore, criteria node merging Wilson (2005) Mateescu
et al. (2008) refined, since nodes longer isomorphic root
set feasible paths, paths must cost well.
3.4.3 Semiring Costs Probabilistic Queries
Note approach generalized accommodate general aggregation costs discussed Wilson (2005). Cost functions ci need map assignments
Xi variables set real numbers R set equipped operators
, = (A, 0, 1, , ) semiring. MDD property computed
p:r 1 ep c(e). Operator aggregates edge costs operator aggregates path costs.
semiring distributes , global computation done efficiently local node-based aggregations, much shortest path computed. framework based
reasoning paths minimal cost corresponds using = (R+ , 0, 1, min, +)
different semirings could used. particular, taking = (R+ , 0, 1, +, )
handle probabilistic reasoning. cost function ci corresponds conditional probability table, cost edge c(e), e : u u0 Ei corresponds probability

Q
P (xi = v(e)) given assignments p : r
u. cost path c(p) = ep c(e)
probability event represented path,
Pfor given value Di 0can
get marginal probability P (xi = a) computing e(u,u0 ,a)Ei (U [u] c(e) D[u ]).

4. Compiling MDDs
previous section showed implement cost queries solution space
represented MDD. section, discuss generate MDDs
CSP model description (X, D, F ). goal develop efficient easy implement
approach handle instances handled previously BDD-based configuration
(Hadzic et al., 2004).
Variable Ordering. first step choose ordering CSP variables X.
critical since different variable orders could lead exponential differences MDD size.
well investigated problem, especially binary decision diagrams. fixed
formula, deciding ordering resulting BDD would
nodes (for threshold ) NP-hard problem (Bollig & Wegener, 1996). However,
well developed heuristics, either exploit structure input model
use variable swapping existing BDD improve ordering local-search manner
(Meinel & Theobald, 1998). example, fan-in weight heuristics popular
input form combinational circuits. input CSP, reasonable heuristic
choose ordering minimizes path-width corresponding constraint graph,
113

fiAndersen, Hadzic, & Pisinger

MDD worst case exponential path-width (Bodlaender, 1993; Wilson, 2005;
Mateescu et al., 2008). Investigating heuristics variable ordering scope
work, remainder paper assume ordering already given.
experiments use default orderings provided instances.
Compilation Technique. approach first compile CSP model binary
decision diagrams (BDD) exploiting highly optimized stable BDD packages (e.g.,
Somenzi, 1996) afterwards extract corresponding MDD. Dedicated MDD packages
rare, provide limited functionality implementations optimized
BDD packages offer competitive performance (Miller & Drechsler, 2002). interesting
recent alternative generate BDDs search caching isomorphic nodes.
approach suggested Huang Darwiche (2004) compile BDDs CNF
formulas, proved valuable addition standard compilation based pairwise
BDD conjunctions. However, compilation technology still early stages
development open-source implementation publicly available.
4.1 BDD Encoding
Regardless BDD compilation method, finite domain CSP variables X first
encoded Boolean variables. Choosing proper encoding important since
intermediate BDD might large inadequate subsequent extraction. general,
CSP variable xi would encoded ki Boolean variables {xi1 , . . . , xiki }. Di
mapped bit vector enci (a) = (a1 , . . . , aki ) {0, 1}ki different
values 6= a0 get different vectors enci (a) 6= enci (a0 ). several standard Boolean
encodings multi-valued variables (Walsh, 2000). log encoding scheme xi
encoded ki = dlog|Di |e Boolean variables, representing digit binary notation.
multivalued assignment xi = translated set assignments xij = aj
P j1
Pki j1
2 xj < |Di | added
aj . Additionally, domain constraint kj=1
=
j=1 2


forbid bit assignments (a1 , . . . , aki ) encode values outside domain Di .
direct encoding (or 1-hot encoding) also common, especially well suited efficient
propagation searching single solution. scheme, multi-valued variable
xi encoded |Di | Boolean variables {xi1 , . . . , xiki }, variable xij indicates
whether j-th value domain aj Di assigned. variable xi , exactly one
value Di assigned. Therefore, enforce domain constraint xi1 +. . .+xiki = 1
= 1, . . . , n. Hadzic, Hansen, OSullivan (2008) empirically demonstrated
using log encoding rather direct encoding yields smaller BDDs.
Sn Thei set Boolean variables fixed union encoding variables, Xb =
i=1 {x1 , . . . , xki } still specify ordering. common ordering
well suited efficiently answering configuration queries clustered ordering. Here,
Boolean variables {xi1 , . . . , xiki } grouped blocks respect ordering among
finite-domain variables x1 < . . . < xn . is,
xij11 < xij22 i1 < i2 (i1 = i2 j1 < j2 ).
might orderings yield smaller BDDs specific classes constraints.
Bartzis Bultan (2003) shown linear arithmetic constraints represented
114

fiInteractive Cost Configuration Decision Diagrams

compactly Boolean variables xij grouped wrt. bit-position j rather
finite-domain variable xi , i.e. xij11 < xij22 j1 < j2 (j1 = j2 i1 < i2 ). However,
configuration constraints involve linear arithmetic constraints, space savings
reported Bartzis Bultan (2003) significant variable domains
size power two. Furthermore, clustered orderings yield BDDs
preserve essentially combinatorial structure allows us extract MDDs
efficiently seen Section 4.2.
Example 3 Recall T-shirt example D1 = {0, 1, 2, 3}, D2 = {0, 1, 2}, D3 =
{0, 1}. log encoding variables x11 < x12 < x21 < x22 < x31 , inducing variable set
Xb = {1, 2, 3, 4, 5}. log-BDD clustered variable ordering shown Figure 4(a).

x1

x1
x1

0 1 2 3

x2

x2

x2

x2

x2

x2

0
x2

x3
x3

x3

2 1
0

x3
1

1

1

(a) log-BDD.

12

x2

(b) extracted MDD.

Figure 4: log-BDD clustered ordering, extracted MDD T-shirt example. BDD, draw terminal node 1 terminal node 0
incoming edges omitted clarity. node corresponding Boolean
encoding variable xij labeled corresponding CSP variable xi . Edges
labeled 0 1 drawn dashed full lines, respectively.

4.2 MDD Extraction
BDD generated using clustered variable ordering extract corresponding
MDD using method originally suggested Hadzic Andersen (2006)
subsequently expanded Hadzic et al. (2008). following considerations,
use mapping cvar(xij ) = denote CSP variable xi encoding variable
xij and, slight abuse notation, apply cvar also BDD nodes u labeled
xij . terminal nodes, define cvar(0) = cvar(1) = n + 1 (recall BDD two
terminal nodes 0 1 indicating false true respectively). Analogously, use
mapping pos(xij ) = j denote position bit variable encoding.
method based recognizing subset BDD nodes captures core
MDD structure, used directly construct corresponding MDD.
115

fiAndersen, Hadzic, & Pisinger

block BDD layers corresponding CSP variable xi , Li = Vxi . . . Vxi ,
1
ki
suffices consider nodes reachable edge previous block
layers:
Ini = {u Li | (u0 ,u)E cvar(u0 ) < cvar(u)}.

0
0
first layer
Sn+1we take In1 = {r}. resulting MDD (V , E ) contains nodes
0
Ini , V = i=1 Ini constructed using extraction Algorithm 6. edge e(u, u0 , a)
added E 0 whenever traversing BDD B u wrt. encoding ends u0 6= 0.
Traversals executed using Algorithm 7. Starting u, step algorithm
traverses BDD taking low branch corresponding bit ai = 0 high branch
ai = 1. Traversal takes ki steps, terminating soon reaches node
labeled different CSP variable. MDD extracted log-BDD Figure 4(a)
shown Figure 4(b).

Algorithm 6: Extract MDD.
Data: BDD B(V, E)
E 0 {},V 0 {r};
foreach = 1, . . . , n
foreach u Ini
foreach Di
1
u0 Traverse(u, a);
u0 6= 0
E 0 E 0 {(u, u0 , a)};
V 0 V 0 {u0 }
return (V 0 , E 0 );

Algorithm 7: Traverse BDD.
Data: BDD B(V, E), u,
cvar(u);
(a1 , . . . , aki ) enci (v);
repeat
pos(u);
= 0
u low(u);
else
u high(u);
cvar(u) 6= ;
return u;

Since traversal (in lineP
1 Algorithm 6) takes O(dlog|Di |e) steps, running time
MDD extraction
O( ni=1 |Ini | |Di | dlog|Di |e). resulting MDD (V 0 , E 0 )
Pn
O( i=1 |Ini | |Di |) edges
Pnadd |Di | edges every node
0
u Ini . Since keep nodes Ini , |V | = i=1 |Ini | |V |.
4.3 Input Model Implementation Details

important factor usability approach easiness specifying input
CSP model. BDD packages callable libraries default support CSP-like input
language. best knowledge, open-source BDD-compilation tool
116

fiInteractive Cost Configuration Decision Diagrams

accepts input CSP-like model CLab (Jensen, 2007). configuration interface
top BDD package BuDDy (Lind-Nielsen, 2001). CLab constructs BDD
input constraint conjoins get final BDD. Furthermore CLab generates
BDD using log-encoding clustered ordering suits well extraction approach.
Therefore, compilation approach based using CLab specify input model
generate BDD used extraction Algorithm 6.
Note extracting MDD, preprocess efficient online querying.
expand long edges merge isomorphic nodes get merged MDD.
translate efficient form online processing. rename BDD node names
indexes 0, . . . , |V |, root index 0 terminal 1 index |V |.
allows subsequent efficient implementation U labels, well efficient
access children parent edges node. initial experiments got order
magnitude speed-up wCVD queries switched BDD node names (which
required using less efficient mapping U , D, Ch P structures).

5. Interactive Configuration Multiple Costs
number domains, user configure presence multiple cost functions
express often conflicting objectives user wants achieve simultaneously.
example, configuring product, user wants minimize price, maximizing quality, reducing ecological impact, shortening delivery time etc. assume
therefore addition CSP model (X, D, F ) whose solution space represented
merged MDD , given k additive cost functions
ci (x1 , . . . , xn ) =

n
X

cij (xi ), = 1 . . . , k

j=1

expressing multiple objectives. Multi-cost scenarios often considered within multicriteria optimization framework (Figueira et al., 2005; Ehrgott & Gandibleux, 2000).
usually assumed optimal (but unknown) way aggregate multiple
objectives single objective function would lead solution achieves
best balance satisfying various objectives. algorithms sample efficient solutions
(nondominated wrt. objective criteria) display user. user input,
algorithms learn aggregate objectives adequately used
next sampling efficient solutions etc. approaches user asked explicitly
assign
Pkweights wi objectives ci aggregated weighted summation
c = i=1 wi ci .
adopting techniques run compiled representation solution space
would immediately improve complexity guarantees would useful many scenarios multi-criteria techniques traditionally used, believe configuration
setting, explicit control variable values needed. user easily explore
effect assigning various variable values variables well cost functions.
therefore suggest directly extend wCVD query user could explore
effect cost restrictions way explores interactions regular variables. key query want deliver computing valid domains wrt. multiple cost
restrictions:
117

fiAndersen, Hadzic, & Pisinger

Definition 5 (k-wCVD) Given CSP model (X, D, F ), additive cost functions cj : R,
maximal costs Kj , j = 1, . . . , k, given partial assignment , compute:
VD [, {Kj }kj=1 ] = {a Di | 0 .(0 |= F {(xi , a)} 0

k
^

j=1

cj (0 ) Kj )}

particularly interested two-cost configuration likely occur
practice strong connections existing research solving Knapsack problems
multi-criteria optimization. reminder section first discuss
complexity 2-wCVD queries develop practical implementation approach.
discuss general k-wCVD query.
5.1 Complexity 2-wCVD query
assume input problem merged MDD , additive cost
functions c1 , c2 cost bounds K1 , K2 . first question whether possible
restricted forms additive cost functions c1 , c2 implement 2-wCVD polynomial
time. purpose formulate decision-version 2-wCVD problem:
Problem 1 (2-wCVD-SAT) Given CSP (X, D, F ) MDD
PnM representation solution space, given two additive cost functions ci (x) = j=1 cij (xj ), = 1, 2 cost
restrictions K1 , K2 , decide whether F c1 (x) K1 c2 (x) K2 satisfiable.
Unfortunately, answer even constraints involve positive coefficients,
binary domains. show reduce well-known Two-Partition
Problem (TPP) NP-hard (Garey & Johnson, 1979). given set positive
integers = {s1 , . . . , sn }, TPP asks decide whether possible split set
indexes
=P{1, . . . , n} two sets \ sum set same:
P

iI\A si .
iA =

Proposition 3 2-wCVD-SAT problem defined Boolean variables involving
linear cost functions positive coefficients NP-hard.

Proof 3 show stated reduction TPP. order reduce TPP two-cost
configuration introduce 2n binary variables x1 , . . . , x2n
x2i1 = 1 \ x2i = 1. construct MDD F =
{x1 6= x2P
, . . . , x2n1 6= x2n } introduce
P two linear cost functions positive coefficients,
c1 (x) = ni=1 si P
x2i1 c2 (x) = ni=1 si x2i . overall capacity constraints set
K1 = K2 =
iI si /2. setting = {i | x2i1 = 1} easily seen
F c1 (x) K1 c2 (x) K2 satisfiable TPP feasible solution.
Hence, able solve 2-wCVD-SAT Boolean variables positive linear cost
functions polynomial time, would also able solve TPP problem polynomially.
5.2 Pseudo-Polynomial Scheme 2-wCVD
previous subsection demonstrated answering 2-wCVD queries NP-hard even
simplest class positive linear cost functions Boolean domains. Hence,
118

fiInteractive Cost Configuration Decision Diagrams

hope solving 2-wCVD guaranteed polynomial execution time unless P = N P .
However, still want provide practical solution 2-wCVD problem. hope
avoid worst-case performance exploiting specific nature cost-functions
processing. subsection therefore show 2-wCVD solved pseudopolynomial time extending labeling approach Section 3.2. Furthermore,
show adopt advanced techniques used Knapsack problem (Kellerer, Pferschy,
& Pisinger, 2004).
5.2.1 Overall Approach
algorithm runs analogous single-cost approach developed Section 3.2.
restricting MDD wrt. current assignment, calculate upstream downstream
costs U, (which longer constants lists tuples), use check
edge e, whether v(e) valid domain.
given edge e : u u0 , labeled costs c1 (e), c2 (e), follows v(e) V Di iff
paths p : r
u, p0 : u0
1 c1 (p) + c1 (e) + c1 (p0 ) K1
0
c2 (p) + c2 (e) + c2 (p ) K2 . node u suffices store two sets labels:
U [u] = {(c1 (p), c2 (p)) | p : r

u}

D[u] = {(c1 (p), c2 (p)) | p : u

1}

Then, given cost restrictions K1 , K2 , edge e : u u0 , u Vi , domain V Di [K1 , K2 ]
contains v(e) (a1 , a2 ) U [u] (b1 , b2 ) D[u] holds
a1 + c1 (e) + b1 K1 a2 + c2 (e) + b2 K2

(9)

5.2.2 Exploiting Pareto Optimality
single-cost case sufficient store U [u], D[u] minimal value
(the cost shortest path root/terminal), multi-cost case need store multiple
tuples. immediate extension would require storing K1 K2 tuples node.
However, need store non-dominated tuples U lists. two
tuples (a1 , a2 ) (a01 , a02 ) list
a1 a01 a2 a02
may delete (a01 , a02 ) test (9) succeeds (a01 , a02 ) also succeed (a1 , a2 ).
remaining entries costs pareto-optimal solutions. solution pareto-optimal
wrt. solution set cost functions c1 , c2 possible find cheaper solution
respect one cost without increasing other. Path p : r
1 represents
pareto-optimal solution Sol iff node u path, sub-paths p1 : r
u
p2 : u
1 pareto-optimal wrt. sets paths {p : r
u} {p : u
1}
respectively. Hence, node u suffices store:
U [u] = {(c1 (p), c2 (p)) | p : r

u, p0 :r

u (c1 (p)

c1 (p0 ) (c2 (p) c2 (p0 ))}

D[u] = {(c1 (p), c2 (p)) | p : u

1, p0 :u

1 (c1 (p)

c1 (p0 ) (c2 (p) c2 (p0 ))}

119

fiAndersen, Hadzic, & Pisinger

Note due pareto-optimality, a1 {0, . . . , K1 } a2 {0, . . . , K2 }
one tuple U first coordinate a1 second
coordinate a2 . Therefore, node u, U [u] D[u] min{K1 , K2 }
entries. Hence, space requirements algorithmic scheme worst case O(|V |K)
K = min{K1 , K2 }.
5.2.3 Computing U Sets
discuss compute U sets efficiently utilizing advanced
techniques solving Knapsack problems (Kellerer et al., 2004). recursively update U
sets layer layer manner shown Algorithm 8. critical component
recursion step algorithm merging lists lines 2 4. operation
new list formed dominated tuples detected eliminated. order
efficiently, critical keep U lists sorted wrt. first coordinate,
i.e.
(a1 , a2 ) (a01 , a02 ) a1 < a2 .
U sorted, merged O(K) time using list-merging algorithm
Knapsack optimization (Kellerer et al., 2004, Section 3.4).
Algorithm 8: Update U, labels.

1
2

3
4

Data: MDD , Cost functions c1 , c2 , Bounds K1 , K2
U [] = {(, )}, U [r] = {(0, 0)};
foreach = 1, . . . , n
foreach u Vi
foreach e : u u0
;
foreach (a1 , a2 ) U [u]
a1 + c1 (e) K1 a2 + c2 (e) K2
(a1 + c1 (e), a2 + c2 (e));
U [u0 ] ergeLists(S, U [u0 ]);
D[] = {(, )}, D[1] = {(0, 0)};
foreach = n, . . . , 1
foreach u Vi
foreach e : u u0
;
foreach (a1 , a2 ) D[u0 ]
a1 + c1 (e) K1 a2 + c2 (e) K2
(a1 + c1 (e), a2 + c2 (e));
D[u] ergeLists(S, D[u]);

time complexity determined populating list (in lines 1 3) merging
(in lines 2 4). updates takes O(K) worst case. Since perform
updates edge e E, total time complexity Algorithm 8 O(|E| K)
worst case.
120

fiInteractive Cost Configuration Decision Diagrams

5.2.4 Valid Domains Computation
U, sets updated extract valid domains straightforward manner
using Algorithm 9. edge e : u u0 algorithm evaluates whether v(e) V Di
worst case O(|U [u]| |D[u0 ]|) = O(K 2 ) steps. Hence, valid domain extraction takes worst
case O(|E| K 2 ) steps.
Algorithm 9: Compute valid domains.
Data: MDD , Cost functions c1 , c2 , Cost bounds K1 , K2 , Labels U ,D
foreach = 1, . . . , n
VDi ;
foreach u Vi
foreach e : u u0
foreach (a1 , a2 ) U [u], (b1 , b2 ) D[u0 ]
a1 + c1 (e) + b1 K1 a2 + c2 (e) + b2 K2
VDi VDi {v(e)};
break;

However, improve running time valid domains computation exploiting
(1) pareto-optimality (2) fact sets U, sorted. critical observe
given edge e : u u0 , (a1 , a2 ) U [u] suffices perform validity
test (9) tuple (b1 , b2 ) D[u0 ], b1 maximal first coordinate satisfying
a1 + c1 (e) + b1 K1 , i.e.
b1 = max{b1 | (b1 , b2 ) D[u0 ], a1 + c1 (e) + b1 K1 }.

Namely, test succeeds (b01 , b02 ) b01 < b1 , also succeed (b1 , b2 )
since due pareto-optimality, b01 < b1 b2 < b02 hence a2 +c2 (e)+b2 < a2 +c2 (e)+b02
K2 . Since lists sorted, comparing relevant tuples performed efficiently
traversing U [u] increasing order, traversing D[u0 ] decreasing order. Algorithm
10 implements procedure.
Algorithm 10: Extract edge value.

1
2

Data: MDD , Cost constraints c1 , c2 , Bounds K1 , K2 , Edge e : u u0 Ei
a(a1 , a2 ) = U [u].begin();
b(b1 , b2 ) = D[u0 ].end();
6= > b 6=
a1 + c1 (e) + b1 > K1
b(b1 , b2 ) D[u0 ].previous();
continue;
else a1 + c1 (e) + b1 K1 a2 + c2 (e) + b2 K2
VDi VDi {v(e)};
return;
a(a1 , a2 ) U [u].next();

algorithm relies several list operations. Given list L sorted tuples, operations
L.begin() L.end() return first last tuple respectively wrt. list ordering.
121

fiAndersen, Hadzic, & Pisinger

Operations L.next() L.previous() return next previous element
list wrt. ordering. Elements > indicate two special elements appear
last first element list respectively. indicate
passed beyond boundary list. algorithm terminates (line 2) soon
test succeeds. Otherwise, keeps iterating tuples processed either
last tuple U [u] first tuple D[u0 ]. case algorithm terminates
guaranteed v(e) 6 V Di . step, traverse least one element
U [u] D[u0 ]. Hence, total execute U [u] + D[u0 ] 2K operations.
Therefore, time complexity single edge traversal O(K) complexity valid
domains computation Algorithm 9 (after replacing quadratic loop Algorithm
10) O(|E| K) K = min{K1 , K2 }.
conclusion, developed pseudo-polynomial scheme computing valid domains wrt. two cost functions (2-wCVD). space complexity dominated storing U
sets node. worst case store O(|V | K) entries. time
complexity compute U labels extract valid domains takes O(|E| K) steps.
overall interaction similar single-cost approach. assigning variable,
recompute labels well extract domains. tighten cost restrictions
K1 , K2 K10 K1 , K20 K2 need extract domains. However, relax either
cost restrictions, K10 > K1 need recompute labels well.
precisely, labels U, need recomputed K1 > K1max K1max initial
cost restriction last assignment.
5.2.5 Extensions
Note approach can, principle, extended handle general k-wCVD query
fixed k. Lists U would contain set non-dominated k-tuples, ordered that:
(a1 , . . . , ak ) (a01 , . . . , a0k ) iff smallest coordinate j aj 6= a0j holds aj < a0j .
list merging well valid domains extraction would directly generalized
operate ordered sets, although time complexity testing dominans
increase. worst-case complexity would depend size efficient frontier,
k cost functions cost bounds K bounded O(K k1 ). practice however,
could expect number non-dominated tuples much smaller, especially cost
functions smaller scopes smaller coefficients. Note approach
also extended accommodate non-additive cost functions expanding MDD
accommodate non-unary labels fashion discussed Section 3.4.
5.3 Approximation Scheme 2-wCVD
subsection analyze complexity answering 2-wCVD queries approximative
manner, i.e. improve running time guarantees settling approximate
solution. Assume one constraints K2 fixed second constraint may
exceeded small tolerance (1+)K1 . example, user might willing tolerate
small increase price long strict quality restrictions met. section present
fully polynomial time approximation scheme (FPTAS) calculating valid domains
time O(En 1 ) problem. FPTAS satisfy feasible solution
respect original costs fathomed, feasible configuration found
122

fiInteractive Cost Configuration Decision Diagrams

use FPTAS domain restriction satisfy cost constraint within
(1 + )K1 . Finally, FPTAS running time polynomial 1/ input
size.
order develop FPTAS use standard scaling technique (Schuurman &
Woeginger, 2005) originally presented Ibarra Kim (1975). Given , let n
number decision variables. Set = K1 /(n + 1) determine new costs c1 (e) =
bc1 (e)/T c new bounds K1 = dK1 /T e. perform valid domains computation
(label updating domain extraction) described Section 5.2, using scaled weights.
following propositions prove obtained FPTAS scheme.
Proposition 4 running time valid domains computation O( 1 En)
Proof 4 may assume K1 < K2 otherwise may interchange two costs.
running time becomes
1
n+1
) = O(En )
O(E K1 ) = O(EK1 /T ) = O(EK1
K1

since n V polynomial input size O(V + E) precision 1 .
Proposition 5 solution feasible respect original costs, also
feasible respect scaled costs.
P
Proof 5 Assume ep c1 (e) K1 .
X
X
1
1
1X
c1 (e) K1 K1 e = K1
c1 (e) =
bc1 (e)/T c
ep


ep
ep
Proposition 6 solution feasible respect scaled costs c1 (e) satisfies
original constraints within (1 + )K1 .
P
Proof 6 Assume ep c1 (e) C1 .
P
P
P
P
ep c1 (e) + n
ep (bc1 (e)/T c + 1)
ep c1 (e)/T
ep c1 (e) =
K1 + n = dK1 /T e + n (K1 /T + 1) + n

= K1 + (n + 1)

Since = K1 /(n + 1) get
X
c1 (e) K1 + (n + 1)K1 /(n + 1) = (1 + )K1
ep

shows stated.

time complexity improved using techniques Kellerer et al. (2004)
Knapsack Problem, interested showing existence
FPTAS.
considerations previous subsections fully analyzed complexity
answering 2-wCVD queries. first showed NP-hard problem.
developed pseudo-polynomial scheme solving it, finally devised fully polynomial time approximation scheme. Even though cannot provide polynomial running-time
guarantees, based considerations, hope provide reasonable performance
practical instances, demonstrated Section 6.
123

fiAndersen, Hadzic, & Pisinger

5.4 Complexity k-wCVD Query
conclude section discussing complexity general k-wCVD queries.
practical implementation efforts focused implementing 2-wCVD queries, wCVD
queries number cost constraints known advance, completeness
consider generic problem delivering k-wCVD arbitrary k, i.e. k part
input problem.
prove problem pseudo-polynomial scheme unless
NP=P. show decision version problem k-wCVD-SAT NP-hard
strong sense (Garey & Johnson, 1979) reduction bin-packing problem (BPP)
strongly NP-hard (Garey & Johnson, 1979). decision form BPP asks
whether given set numbers s1 , . . . , sn placed k bins size K each. Notice,
cannot use reduction showing NP-hardness 2-wCVD-SAT, since k
part input BPP.
Theorem 7 k-wCVD-SAT problem variable k, strongly NP-hard.
Proof 7 given instance BPP reduce k-wCVD-SAT instance follows:
construct MDD CSP (X, D, F ) n variables X = {x1 , . . . , xn }
domain size k, Di = {1, . . . , k}, = 1, . . . , n. set F = , resulting MDD
allows assignments. n nonterminal nodes u1 , . . . , un corresponding numbers
s1 , . . . , sn . two nodes ui , ui+1 k edges costs (c1 (e), c2 (e), . . . , ck (e))
set
(si , 0, . . . , 0), (0, si , 0, . . . , 0), (0, 0, si , 0, . . . , 0), . . . , (0, . . . , si ),
first node u1 root u1 = r last node un connected terminal
un+1 = 1. overall capacity constraints (K1 , . . . , Kk ) = (K, . . . , K).
easily seen may find path r 1 BPP
feasible solution. Since BPP strongly NP-hard shown k-wCVD-SAT also
strongly NP-hard.

6. Experimental Evaluation
implemented compilation scheme algorithms wCVD 2-wCVD queries.
performed number experiments evaluate applicability approach
well confirm various hypotheses made throughout paper. used two sets
instances whose properties presented Table 1. first set corresponds real-world
configuration problems available configuration benchmarks library CLib2 . CSP
models configuration constraints. correspond highly structured configuration
problems huge number similar solutions. second set instances represents
product-catalogue datasets used Nicholson, Bridge, Wilson (2006). catalogues
defined explicitly, tables solutions. represent much smaller sparser set
solutions.
2. http://www.itu.dk/research/cla/externals/clib/

124

fiInteractive Cost Configuration Decision Diagrams

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault
Travel
Laptops
Cameras
Lettings

Sol
231
224
226
220
220
283
241
1461
683
210
751

X
26
23
34
41
45
124
99
7
14
9
6

dmin
2
2
2
4
2
2
2
4
2
5
2

dmax
61
51
38
34
33
66
42
839
438
165
174

davg
5
5
6
9
8
12
4
134
42
40
45

Table 1: First seven instances real-world configuration problems available configuration benchmarks library CLib. Remaining four instances product catalogues
used Nicholson et al. (2006). instance provide number solutions Sol, number variables X, minimal, maximal average domain
size.

6.1 MDD Size
first set experiments, instance generated log-encoded BDD B using
CLab (Jensen, 2007). extracted corresponding MDD B. Finally,
expanded long edges merged isomorphic nodes generate merged MDD 0 .
compare sizes B, 0 Table 2. structure provide number
nodes V edges E. also provide size BDD B. conclude
table BDDs MDDs exponentially smaller size solution
space configuration instances significantly smaller diverse product
configuration catalogues. Furthermore, see number edges merged
MDDs 0 significantly larger comparison extracted MDDs . Hence, due
simpler online algorithms, using merged MDDs seems well suited online reasoning.
also see multi-valued encoding many cases reduces number nodes
edges comparison BDDs. Even though compilation times less important since
generation MDD performed offline, worth noting largest instance,
Renault, took around 2min 30sec compile instance BDD extract
MDD.
6.1.1 Encoding Cost Explicitly
also investigated impact encoding cost information explicitly MDD.
instance compared size MDD without cost variables (M
c respectively). configuration
benchmarks introduce additional variable
P
[0, 10000] = ni=1 ai xi coefficients ai randomly drawn
interval [0, 50]. put variable last ordering since positions
get MDDs similar size, putting end allows easier theoretical analysis. Since
125

fiAndersen, Hadzic, & Pisinger

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault
Travel
Laptops
Cameras
Lettings

VB
306
3,044
3,129
13,332
16,494
356,696
455,796
8479
9528
4274
2122

EB
612
6,088
6,258
26,664
32,988
713,392
911,592
16,958
19,056
8,548
4,244

KB
5
41
56
237
298
7,945
9,891
154
172
71
36

VM
87
753
853
3,907
4875
100,193
283,033
1469
2033
791
351

EM
202
1,989
1,726
6,136
7989
132,595
334,008
2928
2713
999
1099

VM0
96
767
933
3907
4875
100,272
329,135
1469
2033
791
351

EM0
220
2017
1886
6136
7989
132,889
426,212
2928
2713
999
1099

Table 2: Comparison BDDs MDDs instances Table 1. second,
third fourth column give number non-terminal BDD nodes VB ,
number edges EB size disk BDD kilobytes KB. fifth
sixth column give number vertices VM edges EM MDD
extracted BDD using Algorithm 6 page 116. final two columns
provide number nodes edges merged MDD (M 0 ) long
edges extracted MDD expanded.

product catalogues already contain cost variable (price), produce cost-oblivious
version existentially quantifying y, = c .
Table 3 compare MDDs c . structures provide number
edges well representation size kilobytes. also show size cost range
C(Sol). observe configuration instances high level sharing
compression, introducing cost information explicitly induces order magnitude increase
size even cost range C(Sol) limited (400 times increase Bike2 instance).
MDDs two largest instances could generated. However, product catalogues
much less sharing, removing cost information dramatic effect.
worst case, number edges c two times larger . Hence,
experimental results confirm introducing cost explicitly could dramatic effect
MDD representations highly compressed solution spaces, usually implicitly defined
conjunction combinatorial constraints. However, effect adding explicit cost
information might modest solution space defined explicitly, (sparse) list
database entries, case product catalogues. Furthermore, size
cost range C(Sol) needs significant large increase size take place.
6.2 Response Times wCVD Queries
second set experiments, evaluated performance wCVD queries merged
MDD representations configuration instances. report running times
computing U labels (Algorithm 2) well computing valid domains (Algorithm 3).
Table 4 report average worst-case running times initial merged MDDs
126

fiInteractive Cost Configuration Decision Diagrams

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault
Travel
Laptops
Cameras
Lettings

E
202
1,989
1,726
6,136
7,989
132,595
334,008
1640
1592
725
496

KB
5
41
56
237
298
7,945
9,891
45
80
44
9

Ec
129,514
407,662
693,824
1,099,842
1,479,306
2928
2713
999
1099

KB
4,408
12,767
31,467
57,909
70,900
154
172
71
36

C(Sol)
1,966
1,497
3,008
2,000
2,072
839
438
165
174

Table 3: Effect explicitly encoding cost information. second third column indicate number edges representation size kilobytes cost-oblivious
MDD, fourth fifth column show MDD containing
cost information. Column C(Sol) indicates range available costs
solutions.

Table 2. also report time necessary restrict MDD wrt. assignment
(Algorithm 1). randomly create additive cost function c assigning variable
xi value Di cost ci (a) [0, 50]. Valid domains computed wrt.
maximal cost restriction K set value larger length longest
MDD path wrt. cost function c. ensures longest execution time Algorithm 3.
data-point table average maximum 1000 executions Fedora
9 operating system, using dual Quad core Intel Xeon processor running 2.66 GHz.
one core used instance. Empirical evaluation demonstrates response times
easily within acceptable interaction bounds even largest instances, worst
case MDD nodes labeled within 0.13 seconds, valid domains computed within
0.07 seconds MDD restricted wrt. assignment within 0.28 seconds.
6.3 Response Times 2-wCVD Query
generated analogous statistics 2-wCVD Table 5. tested performance
algorithms computationally demanding circumstances: operate
original (fully-sized) MDD, even though interaction would reduced due
user assignments. Furthermore, cost functions c1 , c2 global scope,
use cost restrictions computing U labels (i.e. ignore condition
line 1 Algorithm 10 hence, U [1] D[r] correspond entire efficient frontier).
Normally, cost functions would involve subset variables fraction
labels efficient frontier (within restrictions K1 , K2 ) would relevant user.
generate cost functions c1 , c2 drawing costs ci (a) randomly [0, 50]. computing
valid domains, use restrictions K1 , K2 larger lengths corresponding longest
127

fiAndersen, Hadzic, & Pisinger

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault

Labeling U,
avg
max
0.0001
0.01
0.0001
0.01
0.0002
0.01
0.0002
0.01
0.0003
0.01
0.0210
0.04
0.0590
0.13

Valid domain
avg
max
0.0001
0.01
0.0001
0.01
0.0001
0.01
0.0002
0.01
0.0003
0.01
0.0110
0.03
0.0310
0.07

Restrict
avg
max
0.0001 0.01
0.0002 0.01
0.0010 0.01
0.0010 0.02
0.0010 0.01
0.0400 0.08
0.1600 0.28

Table 4: Interaction time seconds wCVD queries. report time required computing U labels, valid domain computation restriction wrt. single
assignment.

paths, possible solutions efficient frontier allowed. would lead
longest execution time Algorithm 9.
algorithms easily handle first five instances. largest two instances,
U labels known, calculating valid domains done within fraction
second. Hence, user efficiently explore effect various cost restrictions K1 , K2 wrt.
fixed partial assignment. user assigns variable, recomputing U labels
takes total average less 0.85 seconds, worst case less 1.4 seconds.
already within acceptable interaction times, usability system
enhanced, e.g. using layered display information: always reacting
information fastest compute (such CVD wCVD), user analyzing
it, execute time consuming operations. particular, entire efficient frontier
known soon U labels generated worst case within 0.64 seconds.
stage, user explore cost-space labels computed (on average within
next 0.79 seconds). Note running times reduced number
additional schemes, e.g. computing U labels parallel, two processors
present.

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault

Labeling U
avg
max
0.0001 0.01
0.0010 0.01
0.0010 0.02
0.0030 0.02
0.0050 0.02
0.2070 0.45
0.3470 0.64

Labeling
avg
max
0.0002 0.01
0.0020 0.02
0.0020 0.01
0.0030 0.02
0.0040 0.02
0.3160 0.60
0.4700 0.79

Valid domain
avg
max
0.0001
0.01
0.0001
0.01
0.0001
0.01
0.0005
0.01
0.0008
0.01
0.0300
0.04
0.0700
0.08

Table 5: Interaction time seconds 2-wCVD query.

128

fiInteractive Cost Configuration Decision Diagrams

empirical evaluation demonstrates practical value approach. Even
NP-hard 2-wCVD query implemented response times suitable interactive use,
applied huge configuration instances. Note, however, order achieve
performance critical optimize MDD implementation well utilize advanced
list operation techniques. initial implementation efforts failed so, led
response times measured tens seconds largest instances.

7. Related Work
several directions related work. large variety representations
investigated area knowledge compilation might suitable supporting
interactive decision making cost restrictions. also number approaches
handle multiple cost functions multi-criteria optimization.
7.1 Compiled Knowledge Representation Forms
paper used binary decision diagrams (BDDs) multi-valued decision diagrams
(MDDs) compiled representations CSP model. However, might compiled representations might suitable supporting interactive configuration.
compiled representation supports efficient consistency checking conditioning
would theory support polytime interactive configuration. calculate valid domains
suffices value restrict representation check consistent. representation supports efficient optimization conditioning would support polytime
cost restrictions. would suffice restrict representation value check
minimum smaller threshold value. therefore briefly survey
related compiled representations evaluate suitability framework.
Knowledge-Compilation Structures. Probably well known framework
comparing various compiled forms propositional theories based viewing
special classes negation normal form (NNF) languages (Darwiche & Marquis, 2002).
NNFs directed acyclic graphs internal nodes associated conjunctions ()
disjunctions (), leaf nodes labeled literals (x,x) constants true
false. imposing various restrictions get subclasses NNF languages support
efficient execution various queries transformations. restrictive representations
less succinct i.e. exponentially larger instances, support
larger number queries transformations polytime. comprehensive overview
representations presented Darwiche Marquis (2002).
critical restriction makes NNF languages tractable decomposability.
exploits variable independencies enforcing children -node nonoverlapping
variable scopes. Hence, propositional formula F = F1 F2
var(F1 ) var(F2 ) = , evaluate satisfiability F suffices independently evaluate
F1 F2 . resulting language decomposable negation normal form (DNNF)
already supports polytime two operations critical calculating valid domains: consistency checking conditioning. However, general DNNF compiler exists. Current
compilation approach based exhaustive DPLL search caching isomorphic nodes
(Huang & Darwiche, 2005) constructs subsets DNNF satisfy additional property
129

fiAndersen, Hadzic, & Pisinger

determinism. two children -node mutually exclusive. resulting structure called deterministic decomposable negation normal form (d-DNNF). structure
would interesting target cost-configuration. Boolean CSP models, additive
cost functions could efficiently optimized d-DNNFs. multi-valued models however, research necessary encode finite-domain values way allows
efficient cost processing. tool support compiling d-DNNFs far takes input
CNF formulas, unaware extensions allowing direct compilation general
CSP models.
known knowledge representation forms retrieved enforcing additional
properties. example, enforcing nodes decision nodes
variable encountered path (read-once property) get free
BDDs (FBDDs). enforcing decision nodes appear wrt. fixed ordering get
ordered BDDs (OBDDs). fact, d-DNNF compiler Huang Darwiche (2005)
specialized compile OBDDs, proved valuable alternative way BDD
compilation.
Weighted Multi-Valued Knowledge Compilation Structures. compiled representations propositional theories valued counterparts. Many
seen special cases valued NNFs (VNNF) (Fargier & Marquis, 2007). Roughly, every
valued counterpart obtained changing semantics nodes, logical operators
(such , ) general operators (that could arithmetic, + ). Values
functions represented structures longer {0, 1} R. Furthermore,
functions need defined Boolean domains, could take finite-domain values.
general, subsets VNNF satisfy decomposability operator distributivity support
efficient optimization (Fargier & Marquis, 2007) could, principle, used support
cost configuration.
Construction MDDs based encoding BDDs discussed Srinivasan,
Kam, Malik, Brayton (1990). Amilhastre et al. (2002) augmented automata Vempaty (1992) edge weights reason optimal restorations explanations.
weighted extensions correspond closely weighted MDDs since variant automata
used Vempaty (1992) equivalent merged MDDs (Hadzic et al., 2008). However,
weights used compute different queries generate MDDs based widely
available BDD-packages, Vempaty (1992) report compilation tools used. Semiring
labeled decision diagrams (SLDDs) (Wilson, 2005) label edges (unordered) MDD
values semiring allow computation number queries relevant reasoning
uncertainty. Due relaxed ordering, SLDDs succinct weighted
MDDs therefore attractive target cost-based configuration. However,
proposal seems theoretic, seem implemented. Arithmetic
circuits directed acyclic graphs internal nodes labeled summation
multiplication operators leaf nodes labeled constants variables (Darwiche,
2002). could seen valued extension d-DNNFs hence succinct
SLDDs. Furthermore, support efficient optimization coefficients
positive (in Bayesian context - support efficient computing probable explanations). Compilation technology ACs directly applicable general CSP models,
used primarily representing Bayesian networks. based compiling d-DNNFs
tree clustering approaches (Darwiche, 2002, 2003). context, ACs might use130

fiInteractive Cost Configuration Decision Diagrams

ful optimizing non-additive objective functions multiplicative coefficients
multi-linear functions induced Bayesian networks. However, purely propositional
constraints additive cost function optimized, purely propositional
representation form (such d-DNNF) would adequate. Furthermore, efficient optimization queries based ACs implicitly assume constants (at leaf nodes)
positive, case modeling Bayesian networks, hold general
cost functions.
Global Structure Approaches. number techniques based tree-clustering (Dechter
& Pearl, 1989) variable-elimination (Dechter, 1999) exploit variable independencies
present globally CSP model. time space complexity techniques
turn bounded exponentially size important graph-connectivity notion
tree-width (Bodlaender, 1993). techniques geared towards enhancing search single (optimal) solution (adaptive consistency, bucket elimination etc),
concepts utilized compiling representations solutions. AND/OR
MDDs (Mateescu et al., 2008) restricted Boolean variables subset d-DNNF
formulas, variable labeling respects pseudo-tree obtained variable elimination
order. Due utilization variable independencies -nodes, succinct MDDs therefore attractive compilation target cost-configuration.
Furthermore, already extended handle weighted graphical models support
Bayesian reasoning. However, publicly available tool support limited allow
processing weighted CVD queries. Tree-driven-automata (Fargier & Vilarem, 2004) utilize
tree clustering (Dechter & Pearl, 1989), generate partial variable ordering used
generate automaton. Tree-driven-automata equivalent AND/OR MDDs
restricted Boolean case represent subset d-DNNF languages called
strongly ordered decomposable decision graphs (SO-DDG) (Fargier & Marquis, 2006). Like
AND/OR MDDs succinct MDDs therefore interesting target
cost-configuration. However, tools compiling tree-driven-automata yet become publicly available, far extended handle costs. Weighted
cluster trees Pargamin (2003) weighted extension cluster trees used support
interactive configuration preferences. However, publicly available compilation tool (an internal company-based implementation presented), clusters
represented explicitly without utilizing compressions based local structure decision diagrams compiled representations. Tree-of-BDDs (ToB) (Subbarayan, 2008)
directly exploit tree clustering representing cluster BDD. However,
support conditioning polytime fundamental transformation supporting user
interaction (assigning variables). However, compiled instances
d-DNNF compilation fails, empirical evaluation shows average conditioning
times short.
BDD Extensions. large variety weighted extensions binary decision diagrams, represent real-valued functions f : {0, 1}n R rather Boolean functions
f : {0, 1}n {0, 1}. extensions limited Boolean variables adoption
future would consider encoding techniques multi-valued variables avoid explosion size support cost processing. Comprehensive overviews extensions
presented Drechsler (2001), Wegener (2000), Meinel Theobald (1998). immediate extension form algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona,
131

fiAndersen, Hadzic, & Pisinger

Hachtel, Macii, Pardo, & Somenzi, 1993), also known multi-terminal BDDs (MTBDDs),
essentially BDDs multiple terminal nodes - one cost value.
structure-oblivious approach encoding cost, much approach explicitly encoding
cost variable. size grows quickly increase number terminals. Therefore number BDD extensions introduced based labeling edges weights.
differ mostly cost operators decomposition types associated nodes. Edge-valued
BDDs (EVBDDs) (Lai & Sastry, 1992) label every edge additive cost value c(e)
edge e : u u0 , value val(u) = c(e) + val(u0 ) v(e) = 1 (otherwise
val(u) = val(u0 )). Factored EVBDDs (FEVBDDs) (Tafertshofer & Pedram, 1997) introduce multiplicative weights, v(e) = 1, value val(u) = c(e) + w(e) val(u0 )
(otherwise val(u) = val(u0 )). Affine ADDs (AADDs) Sanner McAllester (2005)
introduce additive multiplicative edge weights edge (regardless v(e)).
val(u) = c(e) + w(e) val(u0 ) every edge. shown AADDs
special case valued NNFs (Fargier & Marquis, 2007).
orthogonal extension BDDs change decomposition type nodes. OBDDs
based Shannon decomposition fu = xi fu0 xi fu1 . change decomposition
type positive Davio (pD) decomposition fu = f0 xi f1 negative Davio(nD) decomposition fu = f0 xi f1 . using pD decomposition get ordered functional decision
diagrams (OFDDs) (Kebschull & Rosenstiel, 1993). structures incomparable
OBDDs, i.e. might exponentially larger smaller OBDDs depending
instance. However, ordered Kronecker functional decision diagrams (OKFDDs)(Drechsler,
Sarabi, Theobald, Becker, & Perkowski, 1994) allow three decomposition types, thus generalizing OBDDs OFDDs. Extending OFDDs additive edge weights leads
binary moment diagrams (BMDs) (Bryant & Chen, 1995), adding also multiplicative
edge weights leads multiplicative binary moment diagrams ( BM Ds). Analogously,
extending OKFDDs additive multiplicative edge weights get Kronecker binary
moment diagrams (KBMDs) K BM Ds respectively (Drechsler, Becker, & Ruppertz,
1996).
unclear whether Boolean structures advanced cost labeling schemes
used directly represent multi-valued CSP models cost functions. However, could
compare generalizations labeling schemes multi-valued structures. multivalued generalization EVBDDs would correspond roughly weighted MDDs. However, introducing additive multiplicative weights AADDs would correspond
generalization labeling scheme could prove useful labeling multilinear cost functions. Namely, introduction multiplicative weights would
subgraph sharing, many nodes would refined accommodate non-additive costs. However, due multiplicative factors, obvious
cashing technique based computing U, directly extended, especially
coefficients negative. case additive cost functions though, schemes
would correspond labeling scheme. structures pay price less
efficient operators (such apply operator) larger memory requirements maintain information. Therefore, compiling Boolean functions, using structures
would pose unnecessary overhead comparison OBDDs. Hence, models
large number propositional (configuration) constraints, additive cost function,
would gain compiling using structures even Boolean case.
132

fiInteractive Cost Configuration Decision Diagrams

cost function non-additive, introducing elaborate cost representations might
prove beneficial reducing memory requirements, might make label computing
technique unapplicable. practical point view, implementations
supporting Boolean versions structures, aware tool supporting
multi-valued generalizations structures input language format used
specifying general propositional constraints.
7.2 Multi-Objective Cost Processing
multiple-cost configuration close approaches within framework multi-criteria
optimization decision maker find solution subject multiple (often conflicting) objectives (Figueira et al., 2005; Ehrgott & Gandibleux, 2000). particular,
MDD-based algorithms close approaches solving multiobjective shortest
path problem, given graph (V, E) arc labeled multiple costs,
goal typically compute set Pareto-optimal (efficient, non-dominated) solutions
(Ehrgott & Gandibleux, 2000; Muller-Hannemann & Weihe, 2001; Tarapata, 2007; Reinhardt & Pisinger, 2009). shown multi-objective shortest path problem
intractable. particular, number Pareto-optimal solutions grow exponentially number vertices |V |, FPTAS (fully polynomial time approximation
scheme) developed approximating set Pareto-optimal solutions. However, way solution space multi-criteria optimization problems explored
significantly different approach. Typically, interaction step subset
Pareto-optimal solutions computed afterwards decision maker interactively navigates set order reach satisfying compromising solution. Interactive
methods multi-criteria optimization usually compute subset solutions efficient
frontier, suggest user evaluation, based input compute new set
solutions (Figueira et al., 2005, Chapter 16). techniques would use user input
better estimate way aggregate multiple objectives, would require
user explicitly assign weights importance objectives. contrast, instead primarily driven costs solutions, interactive approach supports reasoning
variable assignments solutions valid domains computation. inherently different way exploring solution space adequate
users want explicit control variable assignments indicating
importance cost functions.
approaches CSP community model preferences soft constraints
(Meseguer, Rossi, & Shiex, 2006) partially satisfied violated, goal
find satisfying least violating solution. usually presupposes preferences aggregated via algebraic operators, related single-cost
optimization problems. However, approach Rollon Larrosa (2006) deals
multiple costs explicitly. utilizes global structure (i.e. variable independencies)
weighted CSP model compute efficient frontier bucket-based variable elimination. highly related approach utilizes global structure generalized additive
independence (GAI) network presented Dubus, Gonzales, Perny (2009). order
compute efficient frontier, authors use message passing computation mechanism analogous computing buckets. addition, authors develop fully
133

fiAndersen, Hadzic, & Pisinger

polynomial approximation scheme approximate efficient frontier demonstrate
significant improvement performance. However, neither methods exploit
fact solution space hard constraints available compiled representation.
Instead, methods operate unprocessed model specification (whether
weighted CSP GAI network) treating hard soft constraints uniformly
hence allowing scope hard constraints decrease variable independencies
model (and thus decrease performance algorithms). Furthermore, result
computation methods allow full exploration efficient solutions.
value frontier single supporting efficient solution maintained
maintain efficient value set supporting efficient solutions. Hence,
possible efficiently retrieve valid domains even algorithms terminate. would
interesting see however, whether methods could adopted work MDD
representations solution space.
Knapsack constraints special case two-cost configuration problems universally true MDD. Trick (2001) used dynamic programming propagate Knapsack constraints CSP search. Fahle Sellmann (2002) presented approximated filtering
algorithm, based various integer programming bounds Knapsack problem. Sellmann (2004) presented fully polynomial time approximation algorithm approximated
filtering. However, techniques considered constraint propagation context
none considered processing existing MDD structure.

8. Conclusions Future Work
paper presented extension BDD-based interactive configuration configuring presence cost restrictions. guarantee polynomial-time cost configuration
cost function additive feasible solutions represented using multi-valued
decision diagram. process cost restrictions MDD extracted
underlying BDD. therefore strictly extend BDD-based configuration Hadzic et al.
(2004) support cost-bounding additive cost functions without incurring exponential
increase complexity. implementation delivers running times easily satisfy interactive response-time requirements. Furthermore, approach extended support
bounding presence non-additive semiring-based costs.
extended approach considering cost bounding wrt. multiple costs.
proved NP-hard problem input MDD size even processing
two linear inequalities positive coefficients Boolean variables. However,
provided pseudo-polynomial scheme fully polynomial approximation scheme twocost configuration (which, principle, extended k-cost configuration
fixed k). empirical evaluation demonstrated despite inherent hardness
problem still provide satisfying performance interactive setting. interaction
based computing valid domains wrt. multiple cost restrictions novel addition
interaction modes within multiple-criteria decision making (Figueira et al., 2005).
provide explicit control variable assignments well cost functions.
future plan investigate compiled representations delivering
cost configuration might efficient investigate practical approaches processing
non-unary cost functions. particular, plan examine whether existing methods
134

fiInteractive Cost Configuration Decision Diagrams

multiobjective non-unary optimization (e.g., Rollon & Larrosa, 2006; Dubus et al., 2009)
adopted operate MDD representation solution space.

Acknowledgments
would like thank anonymous reviewers extensive comments helped
us improve paper. would also like thank Erik van der Meer providing
T-shirt example. first version paper created Tarik Hadzic
University Copenhagen, updated version made Cork Constraint
Computation Centre support IRCSET/Embark Initiative Postdoctoral Fellowship Scheme.

References
Amilhastre, J., Fargier, H., & Marquis, P. (2002). Consistency Restoration Explanations
Dynamic CSPs-Application Configuration. Artificial Intelligence, 135 (1-2), 199
234.
Bahar, R., Frohm, E., Gaona, C., Hachtel, E., Macii, A., Pardo, A., & Somenzi, F. (1993).
Algebraic decision diagrams applications. IEEE/ACM International Conference CAD, pp. 188191.
Bartzis, C., & Bultan, T. (2003). Construction efficient BDDs bounded arithmetic
constraints. Garavel, H., & Hatcliff, J. (Eds.), TACAS, Vol. 2619 Lecture Notes
Computer Science, pp. 394408. Springer.
Bodlaender, H. L. (1993). tourist guide treewidth. Acta Cybernetica, 11, 123.
Bollig, B., & Wegener, I. (1996). Improving variable ordering OBDDs NP-complete.
Computers, IEEE Transactions on, 45 (9), 9931002.
Bryant, R. E. (1986). Graph-Based Algorithms Boolean Function Manipulation. IEEE
Transactions Computers, 35, 677691.
Bryant, R. E., & Chen, Y.-A. (1995). Verification Arithmetic Circuits Binary
Moment Diagrams. Proceedings 32nd ACM/IEEE Design Automation
Conference, pp. 535541.
Darwiche, A., & Marquis, P. (2002). Knowledge Compilation Map. Journal Artificial
Intelligence Research, 17, 229264.
Darwiche, A. (2002). Logical Approach Factoring Belief Networks. Fensel, D.,
Giunchiglia, F., McGuinness, D., & Williams, M.-A. (Eds.), KR2002: Principles
Knowledge Representation Reasoning, pp. 409420 San Francisco, California.
Morgan Kaufmann.
Darwiche, A. (2003). differential approach inference Bayesian networks. Journal
ACM, 50 (3), 280305.
135

fiAndersen, Hadzic, & Pisinger

Dechter, R. (1999). Bucket Elimination: Unifying Framework Reasoning. Artificial
Intelligence, 113 (1-2), 4185.
Dechter, R., & Pearl, J. (1989). Tree-Clustering Constraint Networks. Artificial Intelligence, 38 (3), 353366.
Drechsler, R., Sarabi, A., Theobald, M., Becker, B., & Perkowski, M. A. (1994). Efficient
representation manipulation switching functions based ordered Kronecker
functional decision diagrams. DAC 94: Proceedings 31st annual conference
Design automation, pp. 415419 New York, NY, USA. ACM.
Drechsler, R. (2001). Binary decision diagrams theory practice. International Journal
Software Tools Technology Transfer (STTT), 3 (2), 112136.
Drechsler, R., Becker, B., & Ruppertz, S. (1996). K*BMDs: New Data Structure
Verification. EDTC 96: Proceedings 1996 European conference Design
Test, p. 2 Washington, DC, USA. IEEE Computer Society.
Dubus, J.-P., Gonzales, C., & Perny, P. (2009). Multiobjective Optimization using GAI
Models. Boutilier, C. (Ed.), IJCAI, pp. 19021907.
Ehrgott, M., & Gandibleux, X. (2000). Survey Annotated Bibliography Multiobjective Combinatorial Optimization. Spektrum, 22, 425460.
Fahle, T., & Sellmann, M. (2002). Cost Based Filtering Constrained Knapsack
Problem. Annals Operations Research, 115, 7393.
Fargier, H., & Marquis, P. (2006). Use Partially Ordered Decision Graphs
Knowledge Compilation Quantified Boolean Formulae. Proceedings AAAI
2006, pp. 4247.
Fargier, H., & Marquis, P. (2007). Valued Negation Normal Form Formulas. Proceedings IJCAI 2007, pp. 360365.
Fargier, H., & Vilarem, M.-C. (2004). Compiling CSPs Tree-Driven Automata
Interactive Solving. Constraints, 9 (4), 263287.
Figueira, J. R., Greco, S., & Ehrgott, M. (2005). Multiple Criteria Decision Analysis: State
Art Surveys. Springer Verlag, Boston, Dordrecht, London.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability-A Guide Theory
NP-Completeness. W H Freeman & Co.
Hadzic, T., Subbarayan, S., Jensen, R. M., Andersen, H. R., Mller, J., & Hulgaard, H.
(2004). Fast Backtrack-Free Product Configuration using Precompiled Solution
Space Representation. Proceedings PETO Conference, pp. 131138. DTUtryk.
Hadzic, T., & Andersen, H. R. (2006). BDD-based Polytime Algorithm Cost-Bounded
Interactive Configuration. Proceedings AAAI 2006, pp. 6267.
136

fiInteractive Cost Configuration Decision Diagrams

Hadzic, T., Hansen, E. R., & OSullivan, B. (2008). Automata, MDDs BDDs
Constraint Satisfaction. Proceedings ECAI 2008 Workshop Inference
Methods based Graphical Structures Knowledge.
Huang, J., & Darwiche, A. (2004). Using DPLL efficient OBDD construction.
Proceedings SAT 2004, pp. 127136.
Huang, J., & Darwiche, A. (2005). DPLL trace: SAT knowledge compilation.
Kaelbling, L. P., & Saffiotti, A. (Eds.), IJCAI, pp. 156162. Professional Book
Center.
Ibarra, O., & Kim, C. (1975). Fast approximation algorithms knapsack sum
subset problem. Journal ACM, 22, 463468.
Jensen, R. M. (2007). CLab: C++ library fast backtrack-free interactive product
configuration. http://www.itu.dk/people/rmj/clab/.
Kebschull, U., & Rosenstiel, W. (1993). Efficient graph-based computation manipulation functional decision diagrams. Design Automation, 1993, European
Event ASIC Design. Proceedings. [4th] European Conference on, 278282.
Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer, Berlin,
Germany.
Lai, Y.-T., & Sastry, S. (1992). Edge-valued binary decision diagrams multi-level hierarchical verification. DAC 92: Proceedings 29th ACM/IEEE conference
Design automation, pp. 608613 Los Alamitos, CA, USA. IEEE Computer Society
Press.
Lichtenberg, J., Andersen, H. R., Hulgaard, H., Mller, J., & Rasmussen, A. S. (2001).
Method configuring product. US Patent No: 7,584,079.
Lind-Nielsen, J. (2001).
BuDDy - Binary
http://sourceforge.net/projects/buddy.

Decision

Diagram

Package.

Mateescu, R., Dechter, R., & Marinescu, R. (2008). AND/OR Multi-Valued Decision Diagrams (AOMDDs) Graphical Models. Journal Artificial Intelligence Research,
33, 465519.
Meinel, C., & Theobald, T. (1998). Algorithms Data Structures VLSI Design.
Springer.
Meseguer, P., Rossi, F., & Shiex, T. (2006). Soft constraints. Rossi, F., van Beek,
P., & Walsh, T. (Eds.), Handbook Constraint Programming, Foundations Artificial Intelligence, chap. 9, pp. 281328. Elsevier Science Publishers, Amsterdam,
Netherlands.
Miller, D. M., & Drechsler, R. (2002). Construction Multiple-Valued Decision
Diagrams. Proceedings 32nd International Symposium Multiple-Valued
Logic (ISMVL02), p. 245 Washington, DC, USA. IEEE Computer Society.
137

fiAndersen, Hadzic, & Pisinger

Mller, J., Andersen, H. R., & Hulgaard, H. (2002). Product configuration internet.
INFORMS Conference Information Systems Technology.
Muller-Hannemann, M., & Weihe, K. (2001). Pareto Shortest Paths Often Feasible
Practice. WAE 01: Proceedings 5th International Workshop Algorithm
Engineering, pp. 185198 London, UK. Springer-Verlag.
Nicholson, R., Bridge, D. G., & Wilson, N. (2006). Decision Diagrams: Fast Flexible
Support Case Retrieval Recommendation. Proceedings ECCBR 2006,
pp. 136150.
Pargamin, B. (2003). Extending Cluster Tree Compilation non-Boolean variables
Product Configuration: Tractable Approach Preference-based Configuration.
IJCAI03 Workshop Configuration.
Reinhardt, L. B., & Pisinger, D. (2009). Multi-Objective Multi-Constrained NonAdditive Shortest Path Problems. Computers Operations Research. Submitted.
Technical report version available at: http://man.dtu.dk/upload/institutter/
ipl/publ/publikationer%202009/rapport%2016.pdf.
Rollon, E., & Larrosa, J. (2006). Bucket elimination multiobjective optimization problems. Journal Heuristics, 12 (4-5), 307328.
Sanner, S., & McAllester, D. A. (2005). Affine Algebraic Decision Diagrams (AADDs)
Application Structured Probabilistic Inference. Proceedings IJCAI 2005,
pp. 13841390.
Schuurman, P., & Woeginger, G. J. (2005). Approximation schemes tutorial.
Moehring, R., Potts, C., Schulz, A., Woeginger, G., & Wolsey, L. (Eds.), Lectures
Scheduling. Forthcoming.
Sellmann, M. (2004). Practice Approximated Consistency Knapsack Constraints.
McGuinness, D. L., & Ferguson, G. (Eds.), AAAI, pp. 179184. AAAI Press /
MIT Press.
Somenzi, F. (1996). CUDD: Colorado university decision diagram package. ftp://vlsi
.colorado.edu/pub/.
Srinivasan, A., Kam, T., Malik, S., & Brayton, R. K. (1990). Algorithms discrete
function manipulation. International Conference CAD, pp. 9295.
Subbarayan, S., Jensen, R. M., Hadzic, T., Andersen, H. R., Hulgaard, H., & Mller, J.
(2004). Comparing two implementations complete backtrack-free interactive
configurator. Proceedings CP04 CSPIA Workshop, pp. 97111.
Subbarayan, S. M. (2008). Exploiting Structures Constraint Solving. Ph.D. thesis,
University Copenhagen, Copenhagen.
Tafertshofer, P., & Pedram, M. (1997). Factored edge-valued binary decision diagrams.
Formal Methods System Design, Vol. 10, pp. 243270. Kluwer.
138

fiInteractive Cost Configuration Decision Diagrams

Tarapata, Z. (2007). Selected multicriteria shortest path problems: analysis complexity, models adaptation standard algorithms. International Journal Applied
Mathematics Computer Science, 17 (2), 269287.
Trick, M. (2001). dynamic programming approach consistency propagation
knapsack constraints. 3rd international workshop integration AI
techniques constraint programming combinatorial optimization problems CPAI-OR, pp. 113124.
Vempaty, N. R. (1992). Solving constraint satisfaction problems using finite state automata.
Proceedings Tenth National Conference Artificial Intelligence, pp. 453
458.
Walsh, T. (2000). SAT v CSP. Dechter, R. (Ed.), Proceedings CP 2000, Lecture Notes
Computer Science, pp. 441456.
Wegener, I. (2000). Branching Programs Binary Decision Diagrams. Society Industrial Applied Mathematics (SIAM).
Wilson, N. (2005). Decision diagrams computation semiring valuations.
Proceedings Nineteenth International Joint Conference Artificial Intelligence
(IJCAI-05), pp. 331336.

139

fiJournal Artificial Intelligence Research 37 (2010) 41-83

Submitted 07/09; published 02/10

Predicting Performance IDA* using
Conditional Distributions
Uzi Zahavi

zahaviu@cs.biu.ac.il

Computer Science Department
Bar-Ilan University, Israel

Ariel Felner

felner@bgu.ac.il

Department Information Systems Engineering
Deutsche Telekom Labs
Ben-Gurion University, Israel

Neil Burch

burch@cs.ualberta.ca

Computing Science Department
University Alberta, Canada

Robert C. Holte

holte@cs.ualberta.ca

Computing Science Department
University Alberta, Canada

Abstract
Korf, Reid, Edelkamp introduced formula predict number nodes IDA*
expand single iteration given consistent heuristic, experimentally demonstrated could make accurate predictions. paper show that, addition requiring heuristic consistent, formulas predictions accurate
levels brute-force search tree heuristic values obey unconditional distribution defined used formula. propose
new formula works well without requirements, i.e., make accurate predictions IDA*s performance inconsistent heuristics heuristic values
level obey unconditional distribution. order achieve introduce
conditional distribution heuristic values generalization unconditional
heuristic distribution. also provide extensions formula handle individual
start states augmentation IDA* bidirectional pathmax (BPMX), technique propagating heuristic values inconsistent heuristics used. Experimental
results demonstrate accuracy new method variations.

1. Introduction Overview
Heuristic search algorithms A* (Hart, Nilsson, & Raphael, 1968) IDA* (Korf,
1985) guided cost function f (n) = g(n) + h(n), g(n) actual distance
start state state n h(n) heuristic function estimating cost n
nearest goal state. heuristic h admissible h(n) dist(n, goal) every state n
goal state goal, dist(n, m) cost least-cost path n m. h(n)
admissible, i.e. always returns lower bound estimate optimal cost, algorithms
guaranteed find optimal path start state goal state one exists.
c
2010
AI Access Foundation. rights reserved.

fiZahavi, Felner, Burch, & Holte

important question ask many nodes expanded algorithms
solve given problem. major advance answering question work done
Korf, Reid, Edelkamp introduced formula predict number nodes IDA*
expand (Korf & Reid, 1998; Korf, Reid, & Edelkamp, 2001). papers, formula
present, predictions makes, referred KRE paper. Prior
KRE, standard method comparing two heuristic functions compare
average values, preference given heuristic larger average (Korf,
1997; Korf & Felner, 2002; Felner, Korf, Meshulam, & Holte, 2007). KRE made substantial
improvement characterizing quality heuristic function distribution
values. developed KRE formula based heuristic distribution
predict number nodes expanded IDA* searching specific heuristic
cost threshold. Finally, compared predictions formula actual
number nodes expanded IDA* different thresholds several benchmark search
spaces showed gave virtually perfect predictions. major advance
analysis search algorithms heuristics.
Despite impressive results, KRE formula two main shortcomings. first
KRE assumes addition admissible given heuristic also consistent.
heuristic h consistent every pair states, n, h(m) h(n) dist(m, n).1
heuristic consistent, heuristic values nodes children thus constrained similar heuristic value node. heuristic inconsistent
consistent, i.e. pair nodes n, h(m) h(n) > dist(m, n). Inconsistency
allows nodes children heuristic values arbitrarily larger smaller
nodes heuristic value. term inconsistency negative connotation
something avoided, recent studies shown inconsistent heuristics easy
define many search applications produce substantial performance improvements
(Felner, Zahavi, Schaeffer, & Holte, 2005; Zahavi, Felner, Schaeffer, & Sturtevant, 2007;
Zahavi, Felner, Holte, & Schaeffer, 2008). reason, important extend
KRE formula accurately predict IDA*s performance inconsistent heuristics,
heuristics likely become increasingly important future applications.
second shortcoming KRE formula works well levels
search tree heuristic distribution follows equilibrium distribution (defined
Section 3.1.2). always holds sufficiently deep levels search tree,
heuristic values converge equilibrium distribution. addition, hold
levels heuristic values set start states distributed according
equilibrium distribution. However, shown (in Section 3.2.2) KRE
formula inaccurate depths practical interest single start states
large sets start states whose values distributed according equilibrium
distribution. cases, heuristic values levels search tree
actually examined IDA* obey equilibrium distribution applying KRE
cases result inaccurate predictions.
main objective paper develop formula accurately predict number
nodes IDA* expand, given cost threshold, given heuristic set start
states, including currently covered KRE. first extend KREs idea
1. general definition graph. case undirected graphs write consistency
definition |h(m) h(n)| dist(m, n).

42

fiPredicting Performance IDA* using Conditional Distributions

heuristic distribution, unconditional, conditional distribution,
probability specific heuristic value constant, KRE, conditioned
certain local properties search space. conditional distribution provides
insights behavior heuristic values search informed
(in context search tree) specific heuristic value produced.
allows better study heuristic behavior.
Based conditional distribution develop new formula, CDP (Conditional Distribution Prediction), predicts IDA*s performance set start states (regardless
heuristic values distributed) desired depth (not necessarily large)
whether heuristic consistent not. CDP recursive structure information
number nodes propagated root leaves search tree.
experiments CDPs predictions least accurate KREs, CDP much
accurate inconsistent heuristics sets start states non-equilibrium
heuristic distributions. basic form, CDP particularly accurate single start
states. describe simple extension improves accuracy setting. Finally,
adapt CDP make predictions IDA* augmented bidirectional pathmax
method (BPMX) (Felner et al., 2005). inconsistent heuristics used, BPMX
useful addition IDA*. prunes many subtrees would otherwise explored,
thereby substantially reducing number nodes IDA* expands.
Throughout paper provide experimental results demonstrating accuracy
CDP scenarios using two benchmark domains used KRE
sliding-tile puzzle Rubiks Cube.
simplicity discussion, assume paper edges cost 1.
true many problem domains. generalization ideas case variable edge
costs straightforward, although practical implementation introduces additional
challenges (briefly described Section 11.2).
paper organized follows. Section 2 presents background material. Section 3
derives KRE formula first principles discusses limitations. Section 4,
notion conditional distribution heuristic values presented. new formula, CDP,
presented Section 4.2. Section 5 discusses subtle important way
experiments differ KREs. Experimental results presented Sections 6 7.
extension CDP formula better handle single start states presented Section 8.
Section 9 proposes technique, based CDP, estimating upper lower bounds
number nodes IDA* expand given unconditional distribution. Section 10
presents extension CDP predicting performance IDA* BPMX applied.
Related work discussed Section 11, conclusions suggestions future work
given Section 12. preliminary version paper appeared (Zahavi, Felner, Burch,
& Holte, 2008).

2. Background
Two application domains used KRE demonstrate accuracy formula.
experiments use exactly domains. section describe
well search algorithm different heuristic functions used
experiments.
43

fiZahavi, Felner, Burch, & Holte

2.1 Problem Domains
Two classic examples AI literature single-agent pathfinding problems
Rubiks Cube sliding-tile puzzle.
2.1.1 Rubiks Cube

Figure 1: 3 3 3 Rubiks Cube
Rubiks Cube invented 1974 Erno Rubik Hungary. standard version
consists 3 3 3 cube (Figure 1), different colored stickers exposed
squares sub-cubes, cubies. 20 movable cubies 6 stable cubies
center face. movable cubies divided eight corner cubies,
three faces each, twelve edge cubies, two faces each. Corner cubies move
among corner positions, edge cubies move among edge positions.
one 6 faces cube rotated 90, 180, 270 degrees relative
rest cube. results 18 possible moves state. Since twisting
face twice row redundant, branching factor first move reduced
15. addition, movements opposite faces independent. example, twisting
left face right face leads state performing moves
opposite order. Pruning redundant moves results search tree asymptotic
branching factor 13.34847 (Korf, 1997).
goal state, squares side cube color. puzzle
scrambled making number random moves, task restore cube
original unscrambled state. 4 1019 different reachable states.
2.1.2 Sliding-tile Puzzles
sliding-tile puzzle consists square frame containing set numbered square tiles,
empty position called blank. legal operators slide tile
horizontally vertically adjacent blank blank position. problem
rearrange tiles random initial configuration particular desired goal
configuration. state space grows exponentially size number tiles increases,
shown finding optimal solutions sliding-tile problem NPcomplete (Ratner & Warmuth, 1986). two common versions sliding-tile
puzzle 3 3 8-puzzle, 4 4 15-puzzle. 8-puzzle contains 9!/2 (181,440)
44

fiPredicting Performance IDA* using Conditional Distributions

1

2

3
7

1

2

4

5

6

3

4

5

8

9

10 11

6

7

8

12 13 14 15

Figure 2: 8-puzzle 15-puzzle goal states
reachable states, 15-puzzle contains 1013 reachable states. goal states
puzzles shown Figure 2.
classic heuristic function sliding-tile puzzles called Manhattan Distance. computed counting number grid units tile displaced
goal position, summing values tiles, excluding blank. Since
tile must move least Manhattan Distance goal position, move changes
location one tile one grid unit, Manhattan Distance lower bound
minimum number moves needed solve problem instance.
2.2 Iterative Deepening A*
Iterative deepening A* (IDA*) (Korf, 1985) performs series depth-first searches, increasing cost threshold time. depth-first search, nodes n f (n)
expanded. Threshold initially set h(s), start node. goal
found using current threshold, search ends successfully. Otherwise, IDA* proceeds
next iteration increasing minimum f value exceeded previous
iteration.
2.3 Pattern Databases (PDBs)
powerful approach obtaining admissible heuristics create simplified version,
abstraction, given state space use exact distances abstract space
estimates distances original state space. type abstractions use
paper sliding-tile puzzles illustrated Figure 3. left side
figure shows 15-puzzle state goal state. right side shows corresponding
abstract states, defined erasing numbers tiles except 2, 3, 6
7. estimate distance goal state 15-puzzle, calculate
exact distance abstract state corresponding abstract goal state.
pattern database (PDB) lookup table stores distance abstract goal
every abstract state (or pattern) (Culberson & Schaeffer, 1994, 1998). PDB built
running breadth-first search2 backwards abstract goal whole abstract
space spanned. compute h(s) state original space, mapped
corresponding abstract state p distance-to-goal p looked PDB.
2. description assumes operators cost. technique easily extended
cases operators different costs.

45

fiZahavi, Felner, Burch, & Holte

PDB lookup

State
11

9

5

10

1

15 12

2

4

14 13

2

3

6

7

8

3

1

2

3

2

3

4

5

6

7

6

7

8

9

10 11

13
6

7

12 13 14 15

Goal State

Goal Pattern

(a)

(b)

Figure 3: Example regular lookups

example, PDB 15-puzzle based tiles 2, 3, 6, 7 would contain entry
every possible way placing four tiles blank 16 puzzle positions.
PDB could implemented 5-dimensional array, P DB, array indexes
locations blank tiles 2, 3, 6, 7 respectively. lookup state
shown Figure 3 would P DB[0][8][12][13][14] (the blank position 0, tile 2
position 8, tile 3 position 12, etc.). paper, accessing PDB state
described referred regular lookup, heuristic value returned
regular lookup referred regular heuristic value.
Pattern databases proven useful finding lower bounds combinatorial
puzzles (Korf, 1997; Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner, Korf, &
Hanan, 2004; Felner et al., 2007). Furthermore, also proven useful
search problems, e.g., multiple sequence alignment (McNaughton, Lu, Schaeffer, & Szafron,
2002; Zhou & Hansen, 2004) planning (Edelkamp, 2001a).
2.4 Geometric Symmetries
common practice exploit special properties state space enable additional
heuristic evaluations. particular, additional PDB lookups performed given
single PDB. example, consider Rubiks Cube suppose PDB based
positions cubies yellow face (the positions cubies dont
matter). Reflecting rotating puzzle enable similar lookups cubies
different color (e.g., green, red, etc.) since puzzle perfectly symmetric respect
color. Thus, 24 symmetric lookups PDB different heuristic
values obtained lookups PDB. heuristic values
admissible given state puzzle.
46

fiPredicting Performance IDA* using Conditional Distributions

another example, consider sliding-tile puzzle. line symmetry main
diagonal (assuming goal location blank upper left corner). configuration tiles reflected main diagonal reflected configuration
shares attributes original one. reflections usually used using
PDBs sliding-tile puzzle (Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner
et al., 2004, 2007) looked PDB.
2.5 Methods Creating Inconsistent Heuristics
consistent heuristics, difference heuristic value neighboring nodes
constrained less equal cost connecting edge. inconsistent
heuristics, constraint difference heuristic values neighboring
nodes much larger cost edge connecting them.
KRE formula designed work consistent heuristics therefore KRE
papers report experiments done consistent heuristics only. contrast, new
formula, CDP, works types heuristics including inconsistent heuristics. Therefore,
paper, addition usual consistent heuristics regular PDB lookups
Manhattan Distance also experiment inconsistent heuristics. previously
described several methods producing inconsistent heuristics (Zahavi et al., 2007). Two
inconsistent heuristics used experiments Random selection
heuristics Dual evaluations.
Random selection heuristics: well-known method overcoming pitfalls
given heuristic employ several heuristics use maximum value (Holte,
Felner, Newton, Meshulam, & Furcy, 2006). example, multiple heuristics
based domain-specific geometric symmetries ones described above.
using geometric symmetries additional storage costs associated
extra evaluations, even evaluations based PDBs.
Although using multiple heuristics results improved heuristic value, therefore
likely reduce number nodes expanded finding solution, also increases
time required calculate heuristic values nodes, might increase
overall running time search. Instead using available heuristics
every heuristic calculation, one could instead choose consult one them,
selection made either randomly systematically. one
heuristic consulted node, time-per-node virtually
one heuristic available. Even individual heuristics consistent,
heuristic values actually used inconsistent different heuristics
consulted different nodes. showed (Zahavi et al., 2007) inconsistency
generally reduces number expanded nodes compared using heuristic
nodes almost low maximum heuristics
computed every node. Rubiks Cube, randomly chose one 24
different lookups PDB arise 24 lines symmetry
cube.
Dual evaluation: permutation state spaces Rubiks Cube, state
exists dual state sd located distance goal (Felner
47

fiZahavi, Felner, Burch, & Holte

et al., 2005; Zahavi, Felner, Holte, & Schaeffer, 2006; Zahavi et al., 2008). Therefore,
admissible heuristic applied sd also admissible s. puzzles studied
paper permutation state spaces, dual state puzzles
calculated reversing role locations objects: regular state uses
set objects indexed current location, dual state set
locations indexed objects contain. using PDBs, dual lookup
look sd PDB. Performing regular PDB lookups states
generated search produces consistent values. However, values produced
performing dual lookup inconsistent identity objects
queried change dramatically two consecutive lookups. Due
diversity, dual heuristic shown preferable regular heuristic (Zahavi
et al., 2007). exact definition explanations dual lookup provided
original papers (Felner et al., 2005; Zahavi et al., 2006, 2008).
important note three PDB lookups (regular, dual, random) consult
PDB. Thus, need amount memory share overall
distribution heuristic values (Zahavi et al., 2007).

3. KRE Formula Limitations
section begins short derivation KRE formula state spaces
state transitions cost 1. KRE describe generalized account
variable edge costs (Korf et al., 2001).
3.1 KRE formula
given state IDA* threshold d, KRE aims predict N (s, d), number nodes
IDA* expand uses start state complete search
IDA* threshold (i.e., searches depth terminate search goal
encountered). written
N (s, d) =


X

Ni (s, d)

(1)

i=0

Ni (s, d) number nodes expanded IDA* level threshold d.
One way decompose Ni (s, d) product two terms
Ni (s, d) = Ni (s) Pex (s, d, i)

(2)

Ni (s) number nodes level BF Ssd , brute-force search tree (i.e.,
tree created breadth first search without heuristic pruning) depth rooted
start state s, Pex (s, d, i) percentage nodes level BF Ssd
expanded IDA* threshold d.
KRE, Ni (s) written Ni , i.e., without dependence start state s.
perfectly correct state spaces uniform branching factor b, Ni (s)
cases simply bi . state spaces non-uniform regular branching structure,
48

fiPredicting Performance IDA* using Conditional Distributions

KRE showed Ni could computed exactly using recurrence equations independent s. However, base cases recurrences KRE depend
using Ni instead Ni (s) reasonable strictly correct.
3.1.1 Conditions Node Expansion IDA*
understand Pex (s, d, i) treated KRE, necessary reflect conditions
required node expansion. node n level BF Ssd expanded IDA*
satisfies two conditions:
1. f (n) = g(n) + h(n) must less equal d. edges unit cost,
g(n) = condition equivalent h(n) i. call nodes satisfy
condition potential nodes potential expanded.
2. n must generated IDA*, i.e., parent (at level 1) must expanded
IDA*.
KRE restricted analysis heuristics consistent proved case
second condition implied first condition. words, given
heuristic consistent, nodes expanded IDA* level BF Ssd threshold
exactly set potential nodes level i.3 observation allows Equation 2
rewritten
Ni (s, d) = Ni (s) PP OT EN IAL (s, i, i)

(3)

PP OT EN IAL (s, i, v) defined percentage nodes level BF Ssd whose
heuristic value less equal v.
Note although PP OT EN IAL(s, i, di) = Pex (s, d, i) given heuristic consistent, PP OT EN IAL (s, i, i) overestimates Pex (s, d, i) heuristic inconsistent,
sometimes large amount (see Section 3.2.1).
3.1.2 Approximating PP OT EN IAL (s, i, v)
KRE use three different approximations PP OT EN IAL(s, i, v). KREs theoretical analysis
PP OT EN IAL(s, i, v) approximated equilibrium distribution, denote
PEQ (v). defined probability node chosen randomly uniformly
among nodes given depth brute-force search tree heuristic value less
equal v, limit large depth (Korf et al. 2001, p. 208). KRE proved that,
limit large d,

X
Ni (s) PEQ (d i)
i=0

would converge N (s, d) given heuristic consistent. final formula (the KRE
formula) therefore:
3. See section 3.2.1 discussion KRE formula consistent heuristics.

49

fiZahavi, Felner, Burch, & Holte

N (s, d) =


X

Ni (s) PEQ (d i)

(4)

i=0

KRE contrasted equilibrium distribution overall distribution,
defined probability state chosen randomly uniformly states
problem heuristic value less equal v (p. 207). Unlike equilibrium
distribution, defined search tree, overall distribution property
state space. overall distribution directly computed pattern database,
one pattern database used entries corresponds number
states original state space, approximated, complex settings,
computing heuristic values large random sample states. KRE argued
Rubiks Cube overall distribution heuristic defined single pattern database
equilibrium distribution, sliding-tile puzzles, two
distributions different.
heuristic used KREs experiments Rubiks Cube defined maximum
three pattern databases. individual pattern database, overall distribution
computed exactly. KREs experiments distributions combined approximate PP OT EN IAL(s, i, v) assuming values three pattern databases
independent.
experiments sliding-tile puzzles, KRE defined three types states based
whether blank located corner position, edge position, interior
position, approximated PP OT EN IAL(s, i, v) weighted combination overall distributions states type. weights used level exact
percentages states different types level.
experiments followed KRE precisely use overall distribution individual Rubiks Cube pattern databases weighted overall distribution described
sliding-tile puzzles. simplicity, reminder paper use phrase
unconditional heuristic distribution4 notation P (v) refer probability
node heuristic less equal v. let exact context determine
distribution P (v) actually denotes, whether equilibrium distribution, overall
distribution, approximation PP OT EN IAL Pex . Likewise use
p(v) (lower case p) denote P (v) P (v 1) (with p(0) = P (0)). p(v) probability
state heuristic value exactly v according distribution P .
3.2 Limitations KRE Formula
KRE formula (Equation 4) two main shortcomings: (1) predictions
accurate given heuristic inconsistent, (2) even consistent heuristics
predictions inaccurate individual start states sets start states whose heuristic
values distributed according unconditional heuristic distribution, P (v).
turn examine detail.
50

fiPredicting Performance IDA* using Conditional Distributions

Consistent
4
3
3

3 3

expanded
generated
generated

5

Inconsistent
4 R
3

3

4 6

3

5
6

4 3 n

Figure 4: Consistent versus inconsistent heuristics
3.2.1 Inconsistent Heuristics
specifically mentioned KRE papers one property required KRE analysis
heuristic consistent. necessary KRE formula aims
count number potential nodes level BF Ssd . consistent heuristics,
heuristic value neighboring states never changes change g-value,
illustrated left side Figure 4 (where number inside node heuristic
value). implies f -value nodes ancestor always less equal
f -value node (i.e., f monotone non-decreasing along path search tree).
Therefore, easy prove consistent heuristics ancestors potential
node also potential nodes (Korf et al., 2001). Consequently IDA* expand
potential nodes BF Ssd . Hence, formula KRE aims count
number potential nodes BF Ssd used predict number nodes IDA*
expand given consistent heuristic.
inconsistent heuristics reasoning apply. heuristic values neighboring states differ much cost edge connects them,
thus f -values along path search tree guaranteed monotonically
non-decreasing. Therefore, ancestors potential node guaranteed
potential nodes themselves, consequence potential node might never
generated. example, consider search tree right side Figure 4. numbers
inside node show nodes heuristic value. Assume start node R
IDA* threshold 5 (a node potential node f -value less equal
5). 3 potential nodes depth 2 (all heuristic value 3). Consider
potential node n. path node node potential node
(f (m) = 1 + 5 = 6 > 5), generated expanded. Therefore, node n
never generated, preventing IDA* expanding it. Since KRE formula counts
number potential nodes, count node n thus overestimate number
expanded nodes inconsistent heuristic used.
amount KRE overestimates number nodes expanded IDA*
inconsistent heuristic large. illustrate this, consider state space
Rubiks Cube PDB heuristic defined locations 6 (out 12) edge
cubies. regular method looking heuristic value PDB produces consistent
heuristic. discussed Section 2.5 two alternative PDB lookups produce inconsistent
4. unconditional distinguish conditional distribution introduce Section 4.1

51

fiZahavi, Felner, Burch, & Holte


8
9
10
11
12
13

KRE
257
3,431
45,801
611,385
8,161,064
108,937,712

Regular
277
3,624
47,546
626,792
8,298,262
110,087,215

Dual
36
518
6,809
92,094
1,225,538
16,333,931

Random Symmetry
26
346
4,608
61,617
823,003
10,907,276

Table 1: Rubiks Cube - Number nodes expanded IDA* using regular, dual,
random-symmetry PDB lookups different IDA* threshold corresponding KRE predictions.

heuristics dual evaluation random selection multiple heuristics.
Rubiks Cube 24 symmetries applied state create new
way perform PDB lookup it. Thus, 24 heuristics Rubiks Cube based
PDB random-symmetry lookup chooses one randomly.
three lookups (regular, dual, random-symmetry) consult PDB
distribution heuristic values, P (v), therefore KRE predict
IDA* expand number nodes regardless whether regular, dual,
random-symmetry lookup done. experimental results Table 1 show
substantially different number nodes actually expanded practice
methods.
row Table 1 presents results specific IDA* threshold (d). result
average 1, 000 random initial states, generated making 180 random
moves goal state. KRE column shows KRE prediction based
unconditional heuristic distribution. last three columns Table 1 show number
nodes IDA* expands performs either regular, dual, random-symmetry lookup
PDB. KRE prediction within 8% actual number nodes expanded
IDA* uses regular (consistent) PDB lookup (third column) substantially
overestimates number nodes expanded IDA* uses dual random-symmetry
inconsistent lookups PDB (fourth fifth columns).
3.2.2 Sets Start States Whose Heuristics Values Obey
unconditional heuristic distribution
explained above, KRE used unconditional heuristic distribution P (v) and,
theoretical analysis, proved use KRE formula would give accurate predictions limit large depth. fact, accurate predictions occur soon
heuristic distribution depth interest closely approximates P (v). happens
large depths definition happen even shallow levels certain
circumstances. reason KRE able produce extremely accurate predictions
experiments using unconditional heuristic distribution P (v) depths
start states experiments report average predictions performances
52

fiPredicting Performance IDA* using Conditional Distributions

large number randomly drawn start states. spaces used KREs experiments,
heuristic distribution large random set start states closely approximated
P (v) distribution used. caused heuristic distributions levels closely
approximate P (v).
However, set start states heuristic values distributed according
P (v), case non-random sets start states single start state,
KRE expected make good predictions small depths. words,
cases unconditional heuristic distribution P (v) expected good
approximation Pex (s, d, i).
Consider case single start state consistent heuristic. distribution
heuristic values search tree close start state highly correlated
heuristic value start state, therefore search trees
start states different heuristic values. example, great deal pruning likely
occur near top search tree start state large heuristic value, resulting
fewer nodes expanded start state small heuristic value. Applying KRE
two states produce prediction, therefore inaccurate
least one them, uses unconditional heuristic distribution P (v)
cases.
h
5
6
7
8
9

IDA*
30,363,829
18,533,503
10,065,838
6,002,025
3,538,964

KRE
8,161,064
8,161,064
8,161,064
8,161,064
8,161,064

Table 2: Results set 1,000 start states h-value shown first column
(regular PDB lookup, IDA* threshold = 12)

Table 2 demonstrates phenomenon Rubiks Cube one regular 6-edge PDB
lookup IDA* threshold = 12. IDA* column shows average number nodes
expanded 1, 000 start states, heuristic value h given row. KRE
ignores heuristic values start states predicts 8,161,064 nodes
expanded IDA* every start state. row = 12 Table 1 shows
accurate prediction performance averaged large random sample start
states, Table 2 see low start states small heuristic values
high ones large heuristic values.
3.2.3 Convergence Heuristic Distributions Large Depths
described above, KRE make accurate predictions level nodes level
actually obey unconditional heuristic distribution P (v). increases, distribution
heuristic values start converge P (v). rate convergence depends upon
state space. believed fairly slow sliding-tile puzzles, faster
53

fiZahavi, Felner, Burch, & Holte

Rubiks Cube. convergence occurs IDA* threshold reached KRE
provide accurate predictions set start states (including single start states).
order experimentally test repeated KRE Rubiks Cube experiment but,
addition using large set random start states, also looked individual
performance two start states, s6 , low heuristic value (6), s11 ,
maximum value heuristic used experiment (11). KRE used
8-6-6 heuristic takes maximum 3 different PDBs (one based 8 corner
cubies two based 6 edge cubies each). heuristic admissible consistent.
billion random states sampled estimate P (v) maximum value 11
average value 8.898.

KRE

10
11
12
13
14
15
16
17

1,510
20,169
269,229
3,593,800
47,971,732
640,349,193
8,547,681,506
114,098,463,567

Multiple start states
IDA*
Ratio
1,501
0.99
20,151
1.00
270,396
1.00
3,564,495
0.99
47,961,699
1.00
642,403,155
1.00
8,599,849,255
1.01
114,773,120,996
1.01

s6
53,262
422,256
3,413,547
29,114,115
259,577,913
2,451,954,240
24,484,797,237
258,031,139,364

Single start state
Ratio
s11
0.03
0.05
8,526
0.08
162,627
0.12
2,602,029
0.18
38,169,381
0.26
542,241,315
0.35
7,551,612,957
0.44 103,934,322,960

Ratio
2.37
1.66
1.38
1.26
1.18
1.13
1.10

Table 3: Rubiks Cube - Max (8,6,6) PDBs
Table 3 presents results. KRE column presents KRE prediction
Multiple start states columns presents actual number states generated (averaged
set random start states) IDA* threshold. columns copied KRE
journal paper (Korf et al., 2001). Ratio columns Table 3 shows value predicted
KRE formula divided actual number nodes generated. ratio found
close 1.0 multiple start states, indicating KREs predictions
accurate.
results two individual start states tested shown Single start
state part table. Note states optimally solved depth 17, but,
KRE, search depth run completion. cases KRE formula
accurate small thresholds accuracy prediction increased threshold
increased. threshold = 17 KRE prediction roughly factor 2 small s6
10% large s11 . large improvement smaller thresholds.
predictions become even accurate depth continues increase.
reason predictions improve larger values deeper depths
heuristic distribution within single level converges unconditional heuristic distribution. Using dashed dotted lines various types, Figure 5(a) shows distribution
heuristic values seen states 0, 1, 2 4 moves away s6 . solid line
Figure 5(a) unconditional heuristic distribution. x-axis corresponds different
heuristic values y-axis shows percentage states specified depth
heuristic values less equal x value. example depth 0 (which includes
54

fiPredicting Performance IDA* using Conditional Distributions

120

120
Unconditional heuristic distribution
Depth = 4
Depth = 2
Depth = 1
Depth = 0

80

100
cumulative percentage

cumulative percentage

100

Unconditional heuristic distribution
Depth = 4
Depth = 2
Depth = 1
Depth = 0

60
40
20

80
60
40
20

0

0
0

2

4

6

8

10

12

0

2

Heuristic value

4

6

8

10

Heuristic value

(a) Heuristic Distributions s6

(b) Heuristic Distributions s11

Figure 5: Convergence heuristic distributions

start state only) heuristic value 6 seen (leftmost curve). depth 1,
heuristic values 5, 6 7 seen (second curve left), on. figure
shows heuristic distribution successive depths converges unconditional
heuristic distribution (rightmost curve Figure 5(a)). depth 17 (not shown), heuristic distribution probably quite close unconditional heuristic distribution, making
KRE prediction quite accurate even single start state.
Figure 5(b) shows heuristic distributions nodes 0, 1, 2, 4 moves
away s11 . case unconditional heuristic distribution left
heuristic distributions shallow depths, heuristic distribution depth 0
rightmost curve figure. Comparing parts (a) (b) Figure 5 see
convergence unconditional heuristic distribution faster s11 s6 ,
explains KRE prediction Table 3 accurate s11 .

4. Conditional Distribution CDP Formula
present new formula CDP (Conditional Distribution Prediction), overcomes two shortcomings KRE described previous section. important feature
CDP extends unconditional heuristic distribution heuristic values P (v)
used KRE conditional distribution.
4.1 Conditional Distribution Heuristic Values
conditional distribution heuristic values denoted P (v|context), context
represents local properties search tree neighborhood node influence
distribution heuristic values nodes children. Specifically, Pn (v) percentage
node ns children heuristic value less equal v, define
P (v|context) average Pn (v) nodes n satisfy conditions defined
55

12

fiZahavi, Felner, Burch, & Holte

context. P (v|context) interpreted probability node heuristic
value less equal v produced node satisfying conditions specified
context expanded. context empty denoted P (v) Section 3.
use p(v|context) (lower case p) denote probability node heuristic value
equal v produced node
P satisfying conditions specified context
expanded. Obviously, P (v|context) = vi=0 p(i|context).
4.1.1 Basic 1-Step Model
conditioning context combination local properties search tree,
including properties node (e.g. heuristic value), operator applied
generate node, properties nodes ancestors search tree, etc. simplest
conditional distribution p(v|vp ), probability node heuristic value equal v
produced node value vp expanded. call 1-step model
value conditioned nodes one step away only. special circumstances,
p(v|vp ) determined exactly analysis state space heuristic,
general must approximated empirically sampling state space.
sampling method p(v|vp ) represented entry [v][vp ] two-dimensional
matrix [0..hmax ][0..hmax ], hmax maximum possible heuristic value. build
matrix first set values matrix 0. randomly generate state
calculate heuristic value vp . that, generate child state one
time, calculate childs heuristic value (v), increment [v][vp ]. repeat
process large number times order generate large sample. Finally, divide
value cell matrix sum column cell belongs to, entry
[v][vp ] represents percentage children generated value v state
value vp expanded.

vp

vp
6

v

7

8

9

10

6

7

8

9

10

6

0.17 0.11 0.06 0.03 0.02

7

0.36 0.38 0.33 0.25 0.19

0.00 0.44 0.70 0.67 0.00

8

0.37 0.44 0.53 0.60 0.62

9

0.00 0.00 0.09 0.32 0.89

9

0.02 0.03 0.05 0.08 0.14

10

0.00 0.00 0.00 0.01 0.11

10

0.00 0.00 0.00 0.01 0.01

6

0.30 0.11 0.00 0.00 0.00

7

0.60 0.45 0.21 0.00 0.00

8

v

(a) Consistent heuristic

(b) Inconsistent heuristic

Figure 6: portion Conditional Distribution matrix Rubiks Cube consistent
inconsistent heuristics

56

fiPredicting Performance IDA* using Conditional Distributions

Figure 6 shows bottom right corner two matrices 6-edge PDB
Rubiks Cube. left matrix (a) shows p(v|vp ) regular (consistent) lookup
PDB right matrix (b) shows p(v|vp ) inconsistent heuristic created
dual lookup PDB. matrix (a) tridiagonal neighboring values
cannot differ 1. example, states heuristic value 8
children heuristics 7, 8 9; occur probabilities 0.21, 0.70 0.09
respectively (see column 8). contrast, matrix (b) tridiagonal. column 8,
example, see 6% time states heuristic value 8 children
heuristic values 6.
4.1.2 Richer Models
IDA* expands node, eliminates children operator pruning.
example, state spaces undirected operators, using studies,
parent node would generated among nodes children IDA* would immediately
prune away. Distribution p(v|vp ) take account. order take
consideration necessary extend context conditional probability include
heuristic value parent node expanded (we refer parent node
gp). denote p(v|vp ,vgp ) call 2-step model conditions
information ancestors two steps away. p(v|vp ,vgp ) gives probability
node heuristic value equal v generated node expanded
heuristic value vp parent node expanded heuristic value
vgp . estimated sampling way done estimate p(v|vp ), except
sample generates random state, gp, neighbors,
neighbors except eliminated operator pruning. Naturally, results
sampling 2-step model stored three-dimensional array.
context conditional distribution extended ways well.
sliding-tile puzzles, KRE conditions overall distribution type state
expanded, type indicates blank corner, edge, interior
location. experiments sliding-tile puzzle below, extend p(v|vp ,vgp )
type information: p(v, t|vp , tp ,vgp , tgp ) gives probability node type
heuristic value equal v generated node expanded heuristic
value vp type tp expanded nodes parent heuristic value vgp type tgp .
4.2 New Prediction Formula, CDP (Conditional Distribution Prediction)
section use conditional distributions described develop CDP, alternative KRE formula predicting number nodes IDA* expand
given heuristic, IDA* threshold, set start states. shown experimentally, new formula CDP overcomes limitations KRE works well inconsistent
heuristics set start states arbitrary IDA* threshold.
overall approach follows. Define Ni (s, d, v) number nodes
IDA* generate level heuristic value equal v start state
IDA* threshold.
Pdi Given Ni (s, d, v), number nodes IDA* expand level
threshold v=0 Ni (s, d, v), and, N (s, d), total number nodes expanded
complete iteration IDA* threshold levels, quantity ultimately
57

fiZahavi, Felner, Burch, & Holte

P P
interested in, di=0 di
v=0 Ni (s, d, v). summations v runs
nodes heuristic values range [0 . . . i] expanded level i.
Ni (s, d, v) could calculated exactly, formula would calculate N (s, d) exactly
whether given heuristic consistent not. However, general method efficiently calculating Ni (s, d, v) exactly. Instead, Ni (s, d, v) estimated recursively
Ni1 (s, d, v) conditional distribution; exact details depend conditional
model used given subsections follow. use Ni (s, d, v)
denote approximation Ni (s, d, v). Section 4.5.1 describe conditions
calculation is, fact, exact, therefore produces perfect predictions N (s, d).
general case predictions may perfect estimates.
present time analytical tools estimating accuracy show
experimentally, estimates often accurate.
4.3 Prediction Using Basic 1-Step Model
basic 1-step conditional distribution p(v|vp ) used, Ni (s, d, v) estimated
recursively follows:
d(i1)

Ni (s, d, v) Ni (s, d, v) =

X

Ni1 (s, d, vp ) bvp p(v|vp )

(5)

vp =0

bvp average branching factor nodes heuristic value vp , estimated
sampling process estimates conditional distribution.5 reasoning
behind equation Ni1 (s, d, vp )bvp total number children IDA* generates
via nodes expands level 1 heuristic value equal vp . multiplied
p(v|vp ) get expected number children heuristic value v. Nodes
level i1 expanded heuristic value less equal (i 1),
hence summation includes vp values range [0 . . . (i 1)]. restricting
vp less equal (i 1) every recursive application formula,
ensure (even inconsistent heuristics) node counted level
ancestors expanded IDA*. base case recursion, N0 (s, d, v), 1
v = h(s) 0 values v.
Based this, number nodes expanded IDA* given start state s, threshold d,
particular heuristic predicted follows:
CDP1 (s, d) =

X
di
X

Ni (s, d, v)

(6)

i=0 v=0

set, S, start states given instead one start state, calculation
identical except base case recursion defined using start states
S. is, define N0 (S, d, v) equal k k states heuristic
value v. rest formula remains (with substituted everywhere).
5. general case equation branching factor depends context defines conditional distribution. Since 1-step model, context heuristic value v, formally
allow branching factor depend it. practice, branching factor usually
heuristic values.

58

fiPredicting Performance IDA* using Conditional Distributions

4.4 Prediction Using Richer Models
2-step conditional distribution p(v|vp , vgp ) used, define Ni (s, d, v, vp )
number nodes IDA* generate level heuristic value equal v
nodes level 1 heuristic value vp start state IDA*
threshold. Ni (s, d, v, vp ) estimated recursively follows:
d(i2)

Ni (s, d, v, vp ) Ni (s, d, v, vp ) =

X

Ni1 (s, d, vp , vgp ) bvp ,vgp p(v|vp , vgp )

(7)

vgp =0

bvp ,vgp average branching factor nodes heuristic value vp parent
heuristic value vgp . base case 2-step model level 1, level 0.
N1 (s, d, v, vp ) 0 vp 6= h(s), number children start state
heuristic value v vp = h(s). Based 2-step model number nodes expanded
IDA* given start state s, threshold d, particular heuristic predicted
follows:
CDP2 (s, d) =

X
di d(i1)
X
X
i=0 v=0

Ni (s, d, v, vp )

(8)

vp =0

set start states instead one, base case N1 (S, d, v, vp ),
number children heuristic value v states heuristic value vp .
Analogous definitions Ni CDP used definition context.
example, using 1-step model set state types, one would define Ni (s, d, v, t)
number nodes type IDA* generate level heuristic value
equal v, estimate recursively follows:
d(i1)

Ni (s, d, v, t) Ni (s, d, v, t) =

X

X

vp =0

tp

Ni1 (s, d, vp , tp ) bvp ,tp p(v, t|vp , tp )

(9)

Based model number nodes expanded IDA* given start state s, threshold
d, particular heuristic predicted follows:
CDP(s, d) =

X
di X
X

Ni (s, d, v, t)

(10)

i=0 v=0 tT

4.5 Prediction Accuracy
accuracy predictions arbitrarily good arbitrarily bad depending
accuracy conditional model used. following subsections examine
extreme cases.
principle, extending context never decrease accuracy predictions
additional information taken account. However, conditional model
estimated sampling, extended context result poorer predictions
fewer samples context. explanation 1-step model
accurate 2-step model rows h = 6 h = 9 Table 7 Section 6.2
below.
59

fiZahavi, Felner, Burch, & Holte

4.5.1 Perfect Predictions
Consider definition context includes heuristic value node expanded (vp contexts defined above) contains sufficient information allow
operator pruning correctly accounted for. use notation (v, x) refer
specific instance context, v heuristic value node expanded
x instantiation information context (e.g., state type
information last model above). general form predictive model
context
CDP(s, d) =

X
di
X
i=0 v=0

X

Ni (s, d, v, x)

(11)

x
(v, x)
instance
context


d(i1)

Ni (s, d, v, x) =

X

X

vp =0

xp
(vp , xp )
instance
context

Ni1 (s, d, vp , xp ) bvp ,xp p(v, x|vp , xp )

(12)

bvp ,xp average branching factor, operator pruning, nodes satisfying
conditions context (vp , xp ), p(v, x|vp , xp ) average nodes n satisfying
conditions context (vp , xp ) pn (v, x), percentage ns children, operator
pruning, satisfy conditions context (v, x).
If, every context (vp , xp ), nodes n satisfying conditions defined (vp , xp )
exactly branching factor bvp ,xp exactly value pn (v, x)
contexts (v, x), simple proof induction starting correctness base cases,
N1 (s, d, v, x), shows Ni (s, d, v, x) = Ni (s, d, v, x) i, i.e., prediction
method correctly calculates exactly many nodes satisfy conditions
context every level search tree. follows CDP(s, d) exactly
number nodes IDA* expand given start state IDA* threshold d.
practical setting predictions 2-step model guaranteed
perfect reasoning following conditions hold:
1. heuristic defined exact distance goal abstract state space,
case single pattern database used.
2. two states, s1 , s2 , map abstract state x set
operators {op1 , ..., opk } apply them,
3. states s1 s2 map abstract state x, operators op {op1 , ..., opk }
apply s1 s2 , s1 child op(s1 ) s2 child op(s2 ) map abstract
state, op(x).
60

fiPredicting Performance IDA* using Conditional Distributions

Define context node heuristic value abstract state maps.
Condition (2) guarantees every context (v, x), nodes satisfying conditions
(v, x) exactly branching factor bv,x . true nodes n1
n2 satisfy conditions context (v, x), map abstract state
x, condition (2) requires exactly set operators apply
both. Conditions (2) (3) together guarantee every context (vp , xp ), nodes
satisfying conditions (vp , xp ) exactly value pn (v, x) v x.
true nodes n1 n2 satisfy conditions context (vp , xp ),
map abstract state xp , set operators applies both, operator
op creates child that, cases, maps specific abstract state, op(xp ). Therefore
percentage children map particular abstract state
n1 n2 .
straightforward implementation prediction method setting associates
counter abstract state, initialized number start states map
abstract state. counter abstract state x updated value
(1 d) adding it, operator op, current value counter
abstract state op(y) = x. algorithm computational complexity
O(d |A| 2 ) |A| number abstract states effective branching
factor abstract space. complexity depends linearly d, contrast
typically exponential dependency number nodes IDA* expand,
sufficiently large prediction arbitrarily faster compute search
itself. example, PDB 15-puzzle based positions 8 tiles
blank (roughly 4 billion abstract states), prediction 1000 start states = 52
takes 6% time required execute search.
exact prediction setting two potential uses. first determine
searching single PDB feasible not. example, calculation might show
even first iteration IDA* (with threshold h(start)) take
year complete. second use prediction compare actual performance
alternative method executed set start states (e.g. taking maximum
set PDBs) performance using single PDB without actually execute
IDA* search single PDB.
4.5.2 Poor Predictions
predictions made conditional model extremely inaccurate distribution
heuristic values independent information supplied context. illustrate
example based 4x3 sliding-tile puzzle two heuristics, PDB based
locations tiles 1-7 blank, heuristic returns 0 every state.
given state blank goal position, position even number
moves goal position, heuristic value state taken PDB.
states heuristic value 0. search tree, heuristic used level
therefore opposite one used level 1.
1-step model situation clearly hopeless predicting heuristic
distribution levels PDB used sufficiently large
distribution level converges unconditional distribution.
61

fiZahavi, Felner, Burch, & Holte

hope 2-step model could make reasonably accurate predictions
PDB, considered itself, defines consistent heuristic therefore distribution heuristic values nodes children somewhat correlated heuristic
value nodes parent.
tested using 4x3 sliding-tile puzzle, small enough could
build 2-step model using states state space error introduced
sampling process. test prediction accuracy model generated
50,000 solvable states random and, explained detail next section, used
state start state combination IDA* threshold IDA* would actually
executed iteration threshold given state start state. means
different number start states might used value d. Num column
Table 4 indicates many start states used value (first column)
included table results 5,000 start states used.
IDA* column shows average number nodes expanded IDA* start
states used Prediction column shows number predicted
2-step model. Ratio column Prediction divided IDA*. One clearly see
improvement predictions increases. even deepest depth
sample provided 5,000 start states, prediction factor 6 smaller
true value. course, using constant heuristic value 0 alternate levels
something one would practice, obtained similar results, essentially
reason, 15-puzzle switching, one level next, pattern
database based tiles 1-7 pattern database based tiles 9-15 (see Section 7.1).


27
28
29
30
31
32
33
34
35
36
37
38
39

IDA*
1,212
1,529
2,340
3,072
4,818
6,607
10,748
15,184
24,613
36,726
60,779
96,077
152,079

CDP2
Prediction Ratio
48
0.04
63
0.04
90
0.04
131
0.04
208
0.04
338
0.05
585
0.05
1,027
0.07
1,896
0.08
3,513
0.10
6,737
0.11
12,941
0.13
25,119
0.17

Num
5,754
7,780
9,086
11,561
12,397
14,109
14,109
14,545
13,492
12,261
10,405
8,355
6,505

Table 4: 4x3 sliding-tile puzzle, alternating good heuristic 0.

5. Experimental Setup
next two sections describe experimental results obtained running IDA*
comparing number nodes expanded number predicted KRE
62

fiPredicting Performance IDA* using Conditional Distributions

CDP. experimented two application domains used KRE, namely, Rubiks
Cube (Section 6) sliding-tile puzzle (Section 7). domain evaluated
accuracy two formulas, consistent inconsistent heuristics, set
solvable start states generated random.
experiments reported here, start states used given IDA* threshold
subject special condition. State used start state combination
threshold IDA* actually performs iteration threshold start
state. example, would use start state = 17 distance 11
goal h(s) > 17. addition, sliding-tile puzzle, start state would
used IDA* threshold h(s) different parity. contrast,
experiments KRE paper restrict choice start states way,
start states used every IDA* threshold .
difference start states chosen large impact number
nodes IDA* expands. Table 5 illustrates 15-Puzzle using Manhattan
Distance heuristic IDA* threshold (first column) 43 50. nodes
column Unrestricted shows number nodes IDA* expanded average
50,000 randomly generated solvable start states. values column close
agreement corresponding results Table 5 KRE paper (Korf et al., 2001).
number column shows many start states satisfy additional condition.
remove start states violate condition, IDA* expands substantially fewer
nodes average, shown nodes column Restricted, difference
increases increases. = 50 almost order magnitude difference
number nodes expanded two settings. difference needs kept
mind making comparisons experimental results reported KRE
papers.


43
44
45
46
47
48
49
50

Unrestricted
nodes
439,942
1,014,941
1,985,565
4,542,249
8,963,747
20,355,110
40,479,725
91,329,281

Restricted
number
nodes
22,525
219,001
22,484
393,406
22,937
688,119
22,266 1,182,522
22,243 2,108,766
21,028 3,508,482
20,389 6,037,064
18,758 9,904,973

Table 5: 15-Puzzle Manhattan Distance. effect nodes expanded start states
randomly chosen subject condition.

63

fiZahavi, Felner, Burch, & Holte

6. Experimental Results Rubiks Cube
begin Rubiks Cube experiments. heuristic used 6-edge PDB
heuristic described (Section 3.2.1). experimented (consistent) regular
lookup (inconsistent) random-symmetry dual lookups PDB.
CDP formula, two models used, CDP1 CDP2 , denote 1-step 2-step
models, respectively.
outlined Section 4.1.1, conditional distribution tables built generating one billion states (each generated applying 180 random moves goal state),
computing neighbors, incorporating heuristic information matrix representing one-step model. two-step model also generated
grandchildren used heuristic information.
addition, order get reliable samples added following two procedures:
generating children grandchildren sampling used pruning
techniques based operator ordering used main search (see
description Section 2.1.1). is, use sequence operators
would generated main search. done looking random
walk led initial state using last operator random walk
basis operator pruning.
order get reliable sample need entry table sufficiently
sampled. entries table low frequency. example, states
heuristic value 0 rare even sample billion states causing
table 0 row generated small sample. Therefore, enriched
entries artificially creating random states heuristic value 0.
under-sampled entries sampled similar way. One technique, example,
creating (with high probability) random state heuristic value x,
perform random walk length x random state heuristic value 0.
6.1 Rubiks Cube Consistent Heuristics
Table 6 compares KRE CDP1 CDP2 . accuracy three prediction methods
compared using regular lookups 6-edge PDB. Results row
averages set 1000 random states. row presents results IDA* iteration


8
9
10
11
12
13

IDA*
277
3,624
47,546
626,792
8,298,262
110,087,215

KRE
Prediction
257
3,431
45,801
611,385
8,161,064
108,937,712

Ratio
0.93
0.95
0.96
0.98
0.98
0.99

CDP1
Prediction Ratio
235
0.85
3,151
0.87
41,599
0.87
546,808
0.87
7,188,863
0.87
94,711,234
0.86

CDP2
Prediction Ratio
257
0.93
3,446
0.95
45,985
0.97
613,332
0.98
8,180,676
0.99
109,133,021
0.99

Table 6: Rubiks Cube consistent heuristic.
64

fiPredicting Performance IDA* using Conditional Distributions

different threshold (d), given first column. second column (IDA*) presents
actual number nodes expanded IDA* threshold. next columns report
predictions accuracy (Ratio) prediction defined ratio
predicted number actual number expanded nodes. reported
KRE paper, KRE formula found accurate consistent heuristic
averaged large set random start states. table shows CDP1 reasonably
accurate systematically underestimates one-step model consider
nodes parent included among children. elaborate below.
CDP2 predictions accurate, slightly accurate KREs.
6.2 Rubiks Cube Start States Specific Heuristic Values
Table 2, presented (Section 3.2.2), related discussion, show KRE might
make accurate predictions start states restricted specific heuristic
value h. particular example shown (IDA* threshold 12) KRE always predict
value 8, 161, 064, exact value depends specific set start states used
IDA* threshold 12 sufficiently large number nodes
independent start states. Table 7 extends Table 2 include predictions CDP.
shows versions CDP substantially outperform KRE particular set
start states.

h
5
6
7
8
9

IDA*
30,363,829
18,533,503
10,065,838
6,002,025
3,538,964

KRE
Prediction Ratio
8,161,064
0.27
8,161,064
0.44
8,161,064
0.81
8,161,064
1.36
8,161,064
2.31

CDP1
Prediction Ratio
48,972,619
1.61
17,300,476
0.93
7,918,821
0.79
5,094,018
0.85
3,946,146
1.12

CDP2
Prediction Ratio
20,771,895
0.68
13,525,425
0.73
9,131,303
0.91
6,743,686
1.12
5,240,425
1.48

Table 7: Results different start state heuristic values (h) regular PDB
IDA* threshold = 12.

6.3 Rubiks Cube Inconsistent Heuristics
experiments repeated inconsistent heuristics. dual randomsymmetry lookups performed 6-edge PDB instead regular lookup, thereby
creating inconsistent heuristic. discussed Section 3.2.1, KRE produces
prediction heuristics (consistent inconsistent) derived single PDB
overestimates inconsistent heuristics. Table 8 shows CDP2 extremely accurate.
prediction always within 2% actual number nodes expanded.
1-step model used CDP1 systematically underestimates actual number
nodes expanded regular dual lookups (see regular lookup Table 6
dual lookup Table 8). understand why, consider happens node
right side Figure 7 expanded. generates two children, node n (assuming
65

fiZahavi, Felner, Burch, & Holte

KRE
Prediction



IDA*

8
9
10
11
12
13

36
518
6,809
92,094
1,225,538
16,333,931

8
9
10
11
12
13

26
346
4,608
61,617
823,003
10,907,276

CDP1
Ratio Prediction Ratio
Dual
257
7.14
31
0.86
3,431
6.62
418
0.81
45,801
6.73
5,556
0.82
611,385
6.64
74,037
0.80
8,161,064
6.66
987,666
0.81
108,937,712
6.67 13,180,960
0.81
Random Symmetry
257
9.88
26
1.00
3,431
9.92
353
1.02
45,801
9.94
4,718
1.02
611,385
9.92
62,990
1.02
8,161,064
9.92
840,849
1.02
108,937,712
9.99 11,224,108
1.03

CDP2
Prediction Ratio
36
508
6,792
90,664
1,210,225
16,154,640

1.00
0.98
1.00
0.98
0.99
0.99

26
346
4,601
61,174
815,444
10,878,227

1.00
1.00
1.00
0.99
0.99
1.00

Table 8: Rubiks Cube dual, random-symmetry (inconsistent) heuristics
operators inverses case Rubiks Cube) copy parent R (shown ms
left child Figure 7). child 2 levels deeper R therefore f -value
2 greater Rs. IDA* threshold 5, child potential node
1-step model conclude generate potential child probability
0.5, whereas fact children remain operator pruning potential
nodes.

4 R
3
4

3 n

Figure 7: 1-step model may underestimate
reason 1-step model underestimate number nodes expanded
random-symmetry lookups done child copy R constrained
heuristic value R different symmetries could chosen
different occurrences R. childs f -value correlation f -value R
explanation CDP1 underestimates apply.
fact, different copies state uncorrelated h-values effect operator
pruning needs taken account reduces number children,
done well within 1-step model calculating branching factor.
may advantages using wider context 2-step model results
random-symmetry heuristic show minor case.
66

fiPredicting Performance IDA* using Conditional Distributions

7. Experimental Results - Sliding-Tile Puzzle
KRE experiments sliding-tile puzzle, three state types used, based
whether blank corner, edge, interior location. used state types
experiments used exact recurrence equations N (s, v, d, t) type-dependent
version KRE formula. heuristic used Manhattan Distance (MD). experimented 2-step CDP includes type system recurrence equations.
Results 1-step CDP included performed poorly early versions experiments.
8-puzzle conditional distribution P (v, t|vp , tp ,vgp , tgp ) needed CDP2
typed unconditional distribution P (v, t) needed type-dependent KRE formula
computed enumerating states 8-puzzle reachable goal.
15-puzzle, possible exhaustive enumeration entire state
space conditional distributions estimated generating ten billion reachable
states random. uniform random sample used estimate P (v, t) KRE,
state sample used gp sampling method described Section 4.1.2
P (v, t|vp , tp ,vgp , tgp ). latter, however, basic sampling method extended
even processing ten billion gp states entries 6-dimensional
matrix missing sampled sufficiently. correct this, generate
gp, children, grandchildren update matrix accordingly, check
matrix already contains data gps great-grandchildren. generate
gps great-grandchildren update corresponding entries matrix. continues
long encounter contexts never seen before. introduces small
statistical bias sample, guarantees sample contains required
data.

h

#States

12
14
16
18
20

11,454
19,426
18,528
10,099
2,719

34
36
38
40
42
44

1,331
2,330
2,999
3,028
2,454
1,507

KRE
IDA*
Prediction
Ratio
8-puzzle depth 22
1,499
1,391
0.93
1,042
1,404
1.35
660
1,419
2.15
377
1,447
3.84
168
1,503
8.95
15-puzzle depth 52
77,028,888 420,858,250
5.46
38,206,986 424,113,561
11.10
16,226,330 428,883,700
26.43
6,310,724 433,096,514
68.63
2,137,488 438,475,079 205.14
620,322 444,543,678 716.63

CDP2
Prediction Ratio
1,809
1,051
544
246
91

1.21
1.01
0.82
0.65
0.54

172,845,559
64,247,275
21,505,426
6,477,903
1,749,231
409,341

2.24
1.68
1.33
1.03
0.82
0.66

Table 9: sliding-tile puzzles consistent heuristic (MD).
Prediction results KRE CDP2 8- 15-puzzles shown Table 9
format above. 8-puzzle predictions made IDA* threshold
67

fiZahavi, Felner, Burch, & Holte

22 row corresponds group 8-puzzle states heuristic
value h (shown first column) IDA* would actually used threshold
22. second column gives number states group. Clearly, shown
IDA* column, states higher initial heuristic values IDA* expanded smaller
number nodes. trend reflected KRE predictions since KRE take
h account. KRE difference attributes different rows
different type distribution given group. Thus, predicted number expanded
nodes KRE similar rows (around 1,400). CDP formula takes heuristic
value start state account able predict number expanded nodes
much better KRE. bottom part Table 9 show results 15-puzzle
IDA* threshold 52. Similar tendencies observed.
7.1 Inconsistent Heuristics Sliding-tile Puzzle
next experiment inconsistent heuristic 8-puzzle. defined two PDBs,
one based location blank tiles 14, based location
blank tiles 58. create inconsistent heuristic, one PDBs consulted
regular lookup. choice PDB made systematically, randomly, based
position blank. Different occurrences state guaranteed
lookup neighboring states guaranteed consult different PDBs
causes inconsistency. results presented Table 10 variety IDA* thresholds.
threshold Num column indicates many start states used.
results show CDPs predictions reasonably accurate, much accurate
KREs overestimate factor 26.


18
19
20
21
22
23
24
25
26
27
28
29

Num
44,243
40,773
60,944
48,888
60,345
40,894
42,031
22,494
18,668
7,036
4,131
762

IDA*
14.5
22.2
27.4
43.3
58.5
95.4
135.7
226.7
327.8
562.0
818.4
1,431.7

KRE
Prediction
80.4
151.5
244.2
459.0
734.4
1,383.6
2,200.6
4,155.3
6,569.9
12,475.0
19,515.7
37,424.6

Ratio
5.56
6.82
8.91
10.59
12.55
14.50
16.21
18.33
20.04
22.20
23.85
26.14

CDP2
Prediction Ratio
10.4
0.72
16.1
0.73
20.2
0.74
32.1
0.74
44.0
0.75
72.5
0.76
103.4
0.76
174.2
0.77
251.0
0.77
432.2
0.77
618.8
0.76
1,074.8
0.75

Table 10: Inconsistent heuristic 8-puzzle.
Similar experiments conducted 15-puzzle. Here, first PDB based
location blank tiles 17, based location
blank tiles 915. Table 11 shows results IDA* thresholds 48 55 (recall
median solution length puzzle 52). numbers shown averages
68

fiPredicting Performance IDA* using Conditional Distributions

50,000 start states. CDP predictions 15-puzzle considerably worse
8-puzzle, KRE predictions degraded much more. reason
inaccuracy predictions discussed Section 4.5.2. Much accurate
predictions produced context extended include heuristic value
pattern databases, one search algorithm actually consults.


48
49
50
51
52
53
54
55

IDA*
231,939.6
388,201.1
644,350.1
1,062,597.5
1,746,025.1
2,773,611.6
4,539,767.0
7,546,286.9

KRE
Prediction
311,462,527.1
664,920,142.2
1,413,202,357.9
3,014,405,997.5
6,404,191,951.4
13,639,455,787.3
29,035,096,650.9
61,899,533,064.7

Ratio
1,342.9
1,712.8
2,193.2
2,836.8
3,667.9
4,917.6
6,395.7
8,202.6

CDP2
Prediction
71,550.2
149,257.5
313,132.4
663,004.4
1,402,898.2
2,985,321.1
6,361,011.5
13,627,941.8

Ratio
0.308
0.384
0.486
0.624
0.803
1.076
1.401
1.806

Table 11: Inconsistent heuristic 15-puzzle.

8. Accurate Predictions Single Start States
seen CDP works well base cases recursive calculation
Ni (s, d, v) seeded large set start states, matter heuristic values
distributed. However, actual number expanded nodes specific single start
state deviate number predicted CDP. conditional distribution reflects
expected values nodes share context, single start state
interest might behave differently average state context.
Consider Rubiks Cube state heuristic value 8. CDP2 predicts IDA*
expand 6, 743, 686 state IDA* threshold 12. Table 2 shows
average (over 1, 000 start states heuristic value 8) 6, 002, 025 states expanded.
Examining results individual start states showed actual number
expanded nodes ranged 2, 398, 072 15, 290, 697 nodes.
order predict number expanded nodes single start state propose
following enhancement CDP. Suppose want predict number expanded
nodes IDA* threshold start state s. First, perform small initial search
depth r. use states depth r seed base cases CDP formula
compute formula IDA* threshold r. cause larger set nodes
used calculating Ni (s, d, v), thereby improving accuracy CDPs predictions.
8.1 Rubiks Cube, 6-edge PDB Heuristic
Table 12 shows results four specific Rubiks Cube states heuristic value 8 (of
regular 6-edge PDB lookup) IDA* threshold set 12. chose
states least greatest number expanded nodes two states around
median. first column shows actual number nodes IDA* expands state.
69

fiZahavi, Felner, Burch, & Holte

next columns show number expanded nodes predicted enhanced CDP2
formula initial search performed depths (r) 0, 2, 5 6. Clearly,
initial searches give much better predictions original CDP2 (with r = 0),
predicts 6, 743, 686 states. initial search depth 6, predictions
accurate.
h
8
8
8
8

IDA*
2,398,072
4,826,154
9,892,376
15,290,697

CDP2 (r=0)
6,743,686
6,743,686
6,743,686
6,743,686

CDP2 (r=2)
4,854,485
7,072,952
8,555,170
9,432,008

CDP2 (r=5)
3,047,836
5,495,475
9,611,325
13,384,290

CDP2 (r=6)
2,696,532
5,184,453
9,763,455
14,482,001

Table 12: Single state (d = 12).

8.2 Rubiks Cube, 8-6-6 Heuristic
Section 3.2.3 presented KRE predictions two start states, s6 , heuristic value
6, s11 , heuristic value 11, Rubiks Cube 8-6-6 heuristic.
repeat experiments CDP1 . Tables 13 14 show results initial
search depth (r) 0 4. tables show CDP1 able achieve substantially
better predictions KRE cases, initial search depth 4 usually
improved CDP1 predictions.

10
11
12
13
14
15
16
17

IDA*
53,262
422,256
3,413,547
29,114,115
259,577,913
2,451,954,240
24,484,797,237
258,031,139,364

KRE
1,510
20,169
269,229
3,593,800
47,971,732
640,349,193
8,547,681,506
114,098,463,567

Ratio
0.03
0.05
0.08
0.12
0.18
0.26
0.35
0.44

CDP1 (r=0)
32,207
246,158
1,979,417
16,690,055
149,319,061
1,435,177,445
14,925,206,678
167,181,670,892

Ratio
0.60
0.58
0.58
0.57
0.58
0.59
0.61
0.65

CDP1 (r=4)
69,770
690,556
5,422,001
42,650,077
345,370,148
2,934,134,125
26,380,507,927
254,622,231,216

Ratio
1.31
1.64
1.59
1.46
1.33
1.20
1.08
0.99

Table 13: 8-6-6 PDB, single start state s6
8.3 Experiments 8-Puzzle - Single Start States
performed experiments enhanced CDP2 formula states 8-puzzle
(consistent) MD heuristic. use term trial refer pair single
start state given IDA* threshold d. trials included possible values
start states IDA* would actually perform search IDA* threshold
d. Predictions made trial separately, relative error, predicted/actual,
trial calculated. results shown Figure 8. four curves
figure, KRE, CDP, enhanced CDP initial search depths (r) 5
70

fiPredicting Performance IDA* using Conditional Distributions


11
12
13
14
15
16
17

IDA*
8,526
162,627
2,602,029
38,169,381
542,241,315
7,551,612,957
103,934,322,960

KRE
20,169
269,229
3,593,800
47,971,732
640,349,193
8,547,681,506
114,098,463,567

Ratio
2.37
1.66
1.38
1.26
1.18
1.13
1.10

CDP1 (r=0)
8,246
191,077
3,188,470
47,281,091
665,292,864
9,125,863,883
123,571,401,411

Ratio
0.97
1.17
1.23
1.24
1.23
1.21
1.19

CDP1 (r=4)
8,904
139,422
2,834,542
45,690,554
614,042,865
8,544,807,943
120,978,148,822

Ratio
1.04
0.86
1.09
1.20
1.13
1.13
1.16

Table 14: 8-6-6 PDB, single start state s11

cumulative percentage

100
80
60
40
KRE
CDP2
CDP2 (radius=5)
CDP2 (radius=10)

20
0
0

1

2

3

4

5

6

7

8

9

10

predicted / actual

Figure 8: Relative error 8-puzzle
10. x-axis relative error. y-axis percentage trials
prediction relative error x less. example, y-value 20% KRE
curve x = 0.5 means KRE underestimated factor 2 20%
trials. rightmost point KRE plot (x = 10, = 94%) indicates 6%
trials KREs prediction 10 times actual number nodes expanded.
contrast CDP much larger percentage highly accurate predictions, 99%
predictions within factor two actual number nodes expanded. figure
clearly shows advantage using enhanced CDP. initial search depth
10, 90% trials predictions within 10% correct number.

9. Performance Range Given Unconditional Distribution
experiments paper used 6-edge PDB Rubiks Cube illustrated fact number nodes IDA* expands given PDB vary tremendously
depending PDB used (Zahavi et al., 2007). see clearly, middle
three columns Table 15 show data already seen Tables 6 8, namely,
number nodes IDA* expands 6-edge PDB used regular manner,
71

fiZahavi, Felner, Burch, & Holte

dual lookups, random-symmetry lookups. IDA* expands ten times fewer
nodes 6-edge PDB consulted random-symmetry lookups
consulted normal way.
raises intriguing question range performance achieved
varying conditional distribution unconditional distribution fixed.

8
9
10
11
12
13
Correlation

CDP
257
3,431
45,801
611,385
8,161,064
108,937,712

Regular
277
3,624
47,546
626,792
8,298,262
110,087,215
0.591

Dual
36
518
6,809
92,094
1,225,538
16,333,931
0.359

Random Symmetry
26
346
4,608
61,617
823,003
10,907,276
0.187

CDP
16
210
2,813
37,553
501,273
6,691,215

Table 15: Range IDA* Performace 6-edge Rubiks Cube PDB

9.1 Upper Limit
upper extreme, results nodes expanded, occurs consistent
heuristic used. IDA* expands potential nodes, maximum
number nodes expanded conditional distribution parent
every potential node level potential node level 1. exact calculation
number potential nodes brute-force tree therefore theoretical upper bound
number nodes IDA* expand given unconditional distribution.
already discussed, one way estimate number potential nodes use
KRE formula. estimate upper bound number nodes IDA* could
expand denoted CDP Table 15.
Alternatively, number potential nodes approximated CDP formula
given conditional distribution. Consider Equation 6. summation consider
possible vp values [0, d(i1)] nodes potential nodes level i1. Thus
nodes expanded IDA* level 1 nodes generate
children level i.6 Now, lets substitute vp [0, hmax ]. consider
nodes level 1, even ones potential nodes. Using
summation calculate number nodes heuristic v level even ones
actually generated IDA* (because parents potential nodes, i.e.
vp > (i 1). shown Equation 13.
Ni (s, v) =

hX
max

Ni1 (s, vp ) bvp p(v|vp )

(13)

vp =0

6. Note heuristic consistent vp values {v 1, v,v + 1} need considered
summation nodes values vp (smaller v 1 larger v + 1) cannot
generate children heuristic value v.

72

fiPredicting Performance IDA* using Conditional Distributions

Using general prediction equation get:

CDP =

X
di
X

Ni (s, v)

(14)

i=0 v=0

gives alternative method approximate number potential nodes.
methods approximate upper bound. practice, however, possible
number expanded nodes slightly exceed approximate bound due noise
small errors sampling calculations.
9.2 Lower Limit
consistent heuristics values neighboring states highly correlated.
extreme cases correlation heuristic values neighboring
nodes. is, heuristic value child node statistically independent heuristic
value parent. means regardless parents heuristic value vp , heuristic
values children distributed according unconditional heuristic distribution,
i.e., p(v|vp ) = p(v).
motivation using estimated lower bound number nodes IDA*
could expand given unconditional distribution empirical observation
number nodes IDA* expands decreases correlation parents heuristic
value childrens heuristic values decreases.
illustrated last row three middle columns Table 15, shows
correlation heuristic values neighboring states different types
lookups done 6-edge PDB. calculated using Pearsons correlation coefficient,
defined n pairs x, values according following equation

Correlationxy

Pn
Pn
xi yi i=1 xi i=1 yi
p Pn
= p Pn
Pn
Pn
n i=1 x2i ( i=1 xi )2 n i=1 yi2 ( i=1 yi )2
n

Pn

i=1

(15)

order calculate correlation, 60, 000 random pairs (xi ,yi ) neighboring states
generated. heuristic values computed used Equation 15. bottom
row Table 15 shows number nodes expanded decreases correlation
neighboring heuristic values decreases. leads us suggest number
nodes expanded reach minimum correlation zero.7
estimated lower bound calculated using CDP formula p(v|vp ) = p(v).
denote CDP. 1-step model would calculated using following
equations:
7. theory, possible heuristic negative correlation parents heuristic value
childrens heuristic values, i.e., parents low heuristic values could tend children
large heuristic values vice versa. believe unlikely occur practice.

73

fiZahavi, Felner, Burch, & Holte

d(i1)

Ni (s, d, v) =

X

Ni1 (s, d, vp ) bvp p(v)

(16)

vp =0

CDP =

di
X
X

Ni (s, d, v)

(17)

i=0 v=0

seen comparing rightmost two columns Table 15, randomsymmetry use 6-edge PDB within factor two estimated minimum
possible number nodes expanded PDB, suggests substantially
improve upon performance one would use different PDB.
Table 16 shows estimated upper lower bounds IDA*s performance, range
IDA* thresholds, three different PDBs Rubiks Cube. bounds calculated
using 1, 000 random start states. table shows that, according estimates, inconsistent heuristics based 5-edge PDB outperform consistent heuristics based
6-edge PDB probably cannot outperform consistent heuristics based 7-edge
PDB since estimated lower bound 5-edge PDB larger estimated upper
bound 7-edge PDB.


8
9
10
11
12
13

5-edge PDB
CDP
CDP
2,869
134
38,355
2,278
511,982
30,623
6,834,185
408,775
91,225,920
5,456,512
1,217,726,395 72,836,079

6-edge PDB
CDP
CDP
257
16
3,431
210
45,801
2,813
611,385
37,553
8,161,064
501,273
108,937,712 6,691,215

7-edge PDB
CDP
CDP
42
10
348
42
4,535
291
60,535
3,829
808,051
51,116
10,786,252 682,311

Table 16: Estimated Bounds Performance three Rubiks Cube PDBs.

10. Predicting Performance IDA* BPMX
inconsistent heuristic, heuristic value child much larger
parent. happens state space undirected edges, childs heuristic
value propagated back parent. causes parents f -value exceed
IDA* threshold entire search subtree rooted parent pruned without
generating remaining children. propagation technique called bidirectional
pathmax (BPMX) (Felner et al., 2005; Zahavi et al., 2007). shown effective
reducing search effort pruning subtrees would otherwise explored.
show modify CDP handle BPMX propagation. Since BPMX applies
state spaces undirected edges, discussion section limited spaces.
74

fiPredicting Performance IDA* using Conditional Distributions

10.1 Bidirectional Pathmax (BPMX)
Traditional pathmax (Mero, 1984) propagates heuristic values parent children,
applied state space. Admissibility preserved subtracting cost
connecting edge heuristic value. basic insight bidirectional pathmax
(BPMX) edges undirected heuristic values propagate neighbors,
includes child node parent. process continue distance
direction. BPMX illustrated Figure 9. left side figure shows
inconsistent heuristic values node two children. Consider left child
heuristic value 5. Since value admissible edges example cost
one, immediate neighbors least 4 moves away goal, neighbors
least 3 moves away, on. left child generated, heuristic value
(h = 5) propagate parent right child. preserve
admissibility, propagation along path reduces h cost traversing path.
results h = 4 root h = 3 right child. using IDA*,
bidirectional propagation may cause many nodes pruned would otherwise
expanded. example, suppose current IDA* threshold 2. Without propagation
h left child, root node (f = g + h = 0 + 2 = 2) right child
(f = g + h = 1 + 1 = 2) would expanded. Using propagation, left child
increase parents h value 4, resulting search node abandoned
without even generating right child.

4

2
5

5

1

3

Figure 9: Propagation values inconsistent heuristics

10.2 CDP Overestimates BPMX Applied
inconsistent heuristic used BPMX applied, CDP overestimate
number expanded nodes count nodes subtrees BPMX
prunes. Section 4.2, defined Ni (s, d, v) number nodes IDA*
generate level heuristic value exactly equal v start state
IDA* threshold. formula given estimating Ni (s, d, v) (Equation 5) was:
d(i1)

Ni (s, d, v) =

X

Ni1 (s, d, vp ) bvp p(v|vp )

vp =0

calculating Ni (s, d, v) Ni1 (s, d, vp ) formula assumes node
expanded children generated. Ni1 (s, d, vp ) multiplied
branching factor bvp . BPMX applied, child may prune parent
rest children generated. happens, assumption children
expanded nodes generated would wrong. example, without BPMX,
75

fiZahavi, Felner, Burch, & Holte

expanding root left tree Figure 9 children generated child
right also expanded. Indeed CDP count two nodes case. BPMX
applied root expanded child right generated (and therefore
expanded). Thus, CDP, counts two nodes, overestimating number
nodes expanded. following section modify equation correct this.
10.3 New Formula Estimating Ni (s, d, v)
Let n node currently expanded. Assume n b children
consider order generated. call order generation order.
Note BPMX applied, probability child generated decreases
move generation order. Children appear late order
larger chance generated since previous children might
cause BPMX cutoff. Let pbx (l) probability child location l order
generated even BPMX applied. definition extend Equation 5
follows:
d(i1) bvp

Ni (s, d, v) =

X X
{Ni1 (s, d, vp ) pbx (l) p(v|vp )}
vp =0

(18)

l=1

Ni (s, d, v) calculated similar way Equation 5, except way count
total number children IDA* generates via nodes expands level 1
heuristic value equal vp . idea iterate possible locations
generation order calculate probability node location l generated.
practice, however, actual context pbx variables besides location l.
also includes IDA* threshold (d), depth parent (i 1) heuristic
value parent (vp ), thus get final formula:
d(i1) bvp

Ni (s, d, v) =

X X
{Ni1 (s, d, vp ) pbx (l, d, 1, vp ) p(v|vp )}
vp =0

(19)

l=1

exactly equal Equation 5 special case pbx (l) = 1 l,
happens BPMX used used consistent heuristic.
10.4 Calculating pbx
simplicity, model assumes heuristic value propagated BPMX
one level tree. means state pruned immediate
children descendants deeper levels. make assumption another
reason besides simplicity description. experiments Rubiks Cube
domains showed indeed almost pruning BPMX caused 1-level BPMX
propagation. generalized formula deeper BPMX propagations similarly
developed include complicated recursive terms low practical value,
least state spaces heuristics studied.
Assume c child n location l generation order. Child c
generated n pruned l 1 children appear c
76

fiPredicting Performance IDA* using Conditional Distributions

generation order. Assume n level threshold d. Since n
expanded, h(n) i. BPMX h(n) increased (and cause BPMX pruning)
child k h(k) > + 1. case, h(k) 1 larger i,
used instead h(n) IDA* decide expand n additional children
generated. Therefore, order child c location l generation order
generated, l 1 predecessors generation order must heuristics less
equal + 1. Assuming heuristic value parent v probability

pbx (l, d, i, v) = {

di+1
X

p(h|v)}l1

(20)

h=0

sum probability relevant heuristic value raise sum
power l 1 since l 1 children appear c.
10.5 Experiments Rubiks Cube BPMX
repeated experiments Rubiks Cube 6-edge PDB BPMX
activated. Since BPMX affects inconsistent heuristics, Dual Random
Symmetry heuristics tested. heuristic tested IDA* thresholds 8
13. results, averaged set 1, 000 random states, presented
Table 17. BPMX columns repeated Table 8. additional columns
show results BPMX. column IDA* + BPMX presents actual number
expanded nodes using BPMX. BPMX reduces number nodes expanded
30% Dual 25% reduction Random Symmetry, making
unmodified CDP2 predictions high amount. CDPbx
2 column
shows modifications introduced section greatly improve accuracy.



IDA*

8
9
10
11
12
13

36
518
6,809
92,094
1,225,538
16,333,931

8
9
10
11
12
13

26
346
4,608
61,617
823,003
10,907,276

BPMX
CDP2

BPMX
Ratio IDA* + BPMX
CDPbx
2
Dual
36
1.00
26
24
508
0.98
353
328
6,792
1.00
4,700
4,387
90,664
0.98
62,405
58,562
1,210,225
0.99
831,362
781,704
16,154,640
0.99
11,091,676 10,434,547
Random Symmetry
26
1.00
19
18
346
1.00
256
240
4,601
1.00
3,432
3,207
61,174
0.99
45,881
42,818
815,444
0.99
608,816
571,556
10,878,227
1.00
8,125,962
7,629,396

Ratio
0.92
0.93
0.93
0.94
0.94
0.94
0.95
0.94
0.93
0.93
0.94
0.94

Table 17: BPMX Rubiks Cube - Dual & Random Symmetry

77

fiZahavi, Felner, Burch, & Holte

11. Related Work
Previous work predicting A* IDA*s performance properties heuristic falls
two main camps. first bases analysis accuracy heuristic,
second bases analysis, done, distribution heuristic values.
next two subsections survey approaches.
11.1 Analysis Based Heuristics Accuracy
One common approach characterize heuristic focusing error heuristic
value (deviation optimal cost). first analysis line, focusing effect
errors performance search algorithms, done Pohl (1970). Many
papers line appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, &
Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid &
Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & Roger,
2008).
works usually assume abstract model space tree every node
exactly b children aim provide asymptotic estimation number expanded
nodes. mainly differ model assumptions (e.g. binary non-binary trees)
case results derived (worst case average case). Worst case analysis
showed correlation
heuristic errors search complexity.
|h(n)h (n)|
found relative error,
, constant, search complexity
h (n)
exponential (in length solution path) absolute error, |h(n) h (n)|,
bounded constant search complexity linear (Pohl, 1977; Gaschnig, 1979). Three
main assumptions used Pohl (1977) branching factor assumed
constant across inputs, single goal state transpositions
search space. assumptions hold, case many standard
benchmark domains planning, general search algorithms A* explore exponential
number states even assumption almost perfect heuristic (i.e., heuristic
whose error bounded small additive constant) (Helmert & Roger, 2008).
Since difficult guarantee precise bounds magnitude errors produced
given heuristic, probabilistic characterization magnitudes suggested (Huyn
et al., 1980; Pearl, 1984). Heuristics modeled random variables (RVs), relative
errors assumed independent identically distributed (IID model). model,
attaining average polynomial A* complexity proved essentially equivalent
requiring values h(n) clustered near h (n) allowed deviation
logarithmic function h (n) itself.
Additional research line conducted Chenoweth Davis (1991). Instead
using IID model, suggested using NC model, places constraints
errors h. model heuristic defined according heuristic values grow respect distance goal, according error.
predicted A* complexity polynomial whenever values h(n)
logarithmicaly clustered near h (n) + (h (n)), arbitrary, non-negative,
non-decreasing function. Heuristics whose values grow slower distance
goal cause exponential complexity. Studies NC model showed replacing
78

fiPredicting Performance IDA* using Conditional Distributions

heuristic h wh w 0 often change A* complexity exponential
polynomial.
works focused tree searches. contrast, Sen et al. (2004) presented
general technique extending analysis average case performance A*
search spaces trees search spaces directed acyclic graphs. analytical results show expected complexity change exponential polynomial
heuristic estimates nodes become accurate restrictions placed
cost matrix. Recent research line, analyzing complexity A* algorithm
presented Dinh et al. (2007). research presented worst average case analysis performance A* approximately accurate heuristics8 search problems
multiple solutions. Bounds presented paper proved dependent
heuristic accuracy distribution solutions.
11.2 Analysis Based Heuristic Distribution
discussed outset paper, KRE suggested alternative approach calculating time complexity IDA* multiple-goal spaces (Korf & Reid, 1998; Korf
et al., 2001). Arguing heuristic accuracy difficult obtain, suggested
deriving analysis unconditional distribution heuristic values, easy
determine least approximately. also came method deriving
closed-form formula Ni , number nodes level brute-force search tree.
method later formalized (Edelkamp, 2001b). Unlike work described
previous subsection, provides big-O complexity analysis, KREs aim (and ours)
exactly predict number nodes IDA* expand.
KRE correctly point that, operators cost, Ni must
defined number nodes reached path cost i, opposed
number nodes edges start state. calculation Ni
general setting studied detail Ruml, slightly different context (Ruml,
2002). solution involves using conditional distribution edge costs bears
strong resemblance conditional distribution heuristic values.
Based work KRE insight PDB heuristics correlation
size PDB heuristic value distribution, new analysis limited
PDB heuristics done (Korf, 2007; Breyer & Korf, 2008). prediction achieved
based branching factor problem size PDB without knowing
actual heuristic distribution. order derive heuristic distribution
size PDB assumed forward backward branching factors
abstract space equal abstract space negligible number cycles.
Since second assumption usually realistic model underestimates number
expanded nodes.
KRE formula developed predict performance IDA* algorithm.
general approach also applied A* long appropriate modifications made
computations Ni P (v) (Korf et al., 2001; Holte & Hernadvolgyi, 2004; Breyer
& Korf, 2008). challenge accounting effect A*s pruning search
tree generates state previously reached path smaller equal
8. heuristic -approximation (1 )h (s) h(s) (1 + )h (s) states search space.

79

fiZahavi, Felner, Burch, & Holte

cost. particularly challenging heuristic inconsistent, case
first time A* generates state guaranteed reached via least-cost
path, state occur A*s search tree. Indeed, worst case,
every state A* enumerate paths state decreasing order cost,
thereby generating exactly search tree IDA* (Martelli, 1977). general,
A*s pruning reduce Ni , especially large i, ways may hard capture
small set recurrence equations. heuristic distribution A*s entire search
tree, taken maximum depth, is, consistent heuristics, overall distribution (Korf
et al., 2001) since state occurs exactly A*s search tree (as observed,
true inconsistent heuristics). imply overall distribution
used good effect level-by-level basis, use KRE formula result
accurate predictions A*s performance 15-puzzle two different consistent
heuristics used together exact calculation Ni A*s search tree (Breyer
& Korf, 2008).

12. Conclusions Future Work
Historically, heuristics characterized average. KRE introduced idea
characterizing heuristics unconditional heuristic distribution presented
formula predict number nodes expanded one iteration IDA* based
unconditional heuristic distribution. work presented paper takes another
step along line. conditional distribution introduced, prediction
formula CDP based it, advance understanding properties heuristic affect
performance IDA*.
CDP method advances KRE improving predictions shallow depths,
wider range sets start states, inconsistent heuristics. also shown
use make accurate prediction single start state IDA* search
uses BPMX heuristic value propagation.
course, sophisticated methods, preprocessing needed
special care must taken gathering data order get reliable sample.
much easier calculate average heuristic calculate 3-dimensional
matrix. hand, latter approach better characterizes heuristic
enables generating accurate predictions larger variety circumstances.
Future work address number issues. yet clear attributes make
best context prediction, influenced choice heuristic
attributes specific domain. Larger contexts (more parameters) probably provide better prediction cost pre-processing. tradeoff needs
studied. Another direction aim extend analysis approach predict
performance search algorithms A*.

13. Acknowledgments
research supported grant number 728/06 305/09 Israeli Science
Foundation (ISF) Ariel Felner. Robert Holte Neil Burch gratefully acknowledge
ongoing support work Canadas Natural Sciences Engineering Research
80

fiPredicting Performance IDA* using Conditional Distributions

Council (NSERC) Albertas Informatics Circle Research Excellence (iCORE).
code Rubiks Cube paper based implementation Richard E. Korf
used seminal work domain(Korf, 1997). thank anonymous reviewer
encouraged us widen experimental results better explain results
KRE relation results. His/her comments clearly improved strength
paper. Thanks also Sandra Zilles careful checking details Section 4.

References
Breyer, T., & Korf, R. (2008). Recent results analyzing performance heuristic
search. Proceedings First International Workshop Search Artificial
Intelligence Robotics (held conjunction AAAI), pp. 2431.
Chenoweth, S. V., & Davis, H. W. (1991). High-performance A* search using rapidly
growing heuristics. Proceedings Twelfth International Joint Conference
Artificial Intelligence (IJCAI-91), pp. 198203.
Culberson, J. C., & Schaeffer, J. (1994). Efficiently searching 15-puzzle. Tech. rep.
94-08, Department Computer Science, University Alberta.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14 (3), 318334.
Dinh, H. T., Russell, A., & Su, Y. (2007). value good advice: complexity
A* search accurate heuristics. Proceedings Twenty-Second Conference
Artificial Intelligence (AAAI-07), pp. 11401145.
Edelkamp, S. (2001a). Planning pattern databases. Proceedings 6th European
Conference Planning (ECP-01), pp. 1334.
Edelkamp, S. (2001b). Prediction regular search tree growth spectral analysis.
Advances Artificial Intelligence, Joint German/Austrian Conference AI,
(KI/OGAI-2001), pp. 154168.
Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. Journal
Artificial Intelligence Research, 22, 279318.
Felner, A., Korf, R. E., Meshulam, R., & Holte, R. C. (2007). Compressed pattern databases.
Journal Artificial Intelligence Research, 30, 213247.
Felner, A., Zahavi, U., Schaeffer, J., & Holte, R. C. (2005). Dual lookups pattern
databases. Proceedings Nineteenth International Joint Conference Artificial Intelligence (IJCAI-05), pp. 103108.
Gaschnig, J. (1979). Performance Measurement Analysis Certain Search Algorithms.
Ph.D. thesis, Carnegie-Mellon University.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination minimum cost paths. IEEE Transactions Systems Science Cybernetics,
SCC-4(2), 100107.
Helmert, M., & Roger, G. (2008). good almost perfect?. Proceedings
Twenty-Third Conference Artificial Intelligence (AAAI-08), pp. 944949.
81

fiZahavi, Felner, Burch, & Holte

Holte, R. C., Felner, A., Newton, J., Meshulam, R., & Furcy, D. (2006). Maximizing
multiple pattern databases speeds heuristic search. Artificial Intelligence, 170 (1617), 11231136.
Holte, R. C., & Hernadvolgyi, I. T. (2004). Steps towards automatic creation search
heuristics. Tech. rep. TR04-02, Computing Science Department, University Alberta.
Huyn, N., Dechter, R., & Pearl, J. (1980). Probabilistic analysis complexity A*.
Artificial Intelligence, 15 (3), 241254.
Karp, R. M., & Pearl, J. (1983). Searching optimal path tree random costs.
Artificial Intelligence, 21 (1-2), 99116.
Korf, R. E. (1985). Depth-first iterative-deepening: optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
Korf, R. E. (1997). Finding optimal solutions Rubiks Cube using pattern databases.
Proceedings Fourteenth Conference Artificial Intelligence (AAAI-97), pp.
700705.
Korf, R. E. (2007). Analyzing performance pattern database heuristics. Proceedings
Twenty-Second Conference Artificial Intelligence (AAAI-07), pp. 11641170.
Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,
134 (1-2), 922.
Korf, R. E., & Reid, M. (1998). Complexity analysis admissible heuristic search.
Proceedings Fifteenth Conference Artificial Intelligence (AAAI-98), pp. 305
310.
Korf, R. E., Reid, M., & Edelkamp, S. (2001). Time complexity Iterative-Deepening-A* .
Artificial Intelligence, 129 (1-2), 199218.
Martelli, A. (1977). complexity admissible search algorithms. Artificial Intelligence, 8, 113.
McDiarmid, C. J. H., & Provan, G. M. (1991). expected-cost analysis backtracking
non-backtracking algorithms. Proceedings Twelfth International Joint
Conference Artificial Intelligence (IJCAI-91), pp. 172177.
McNaughton, M., Lu, P., Schaeffer, J., & Szafron, D. (2002). Memory-efficient A* heuristics
multiple sequence alignment. Proceedings Eighteenth Conference
Artificial Intelligence (AAAI-02), pp. 737743.
Mero, L. (1984). heuristic search algorithm modifiable estimate. Artificial Intelligence, 23 (1), 1327.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving.
Addison & Wesley.
Pohl, I. (1970). Heuristic search viewed path finding graph. Artificial Intelligence,
1 (3), 193204.
Pohl, I. (1977). Practical theoretical considerations heuristic search algorithms.
Machine Intelligence, 8, 5572.
82

fiPredicting Performance IDA* using Conditional Distributions

Ratner, D., & Warmuth, M. K. (1986). Finding shortest solution n n extension
15-puzzle intractable. Proceedings Fifth Conference Artificial
Intelligence (AAAI-86), pp. 168172.
Ruml, W. (2002). Adaptive Tree Search. Ph.D. thesis, Harvard University.
Sen, A. K., Bagchi, A., & Zhang, W. (2004). Average-case analysis best-first search
two representative directed acyclic graphs. Artificial Intelligence, 155 (1-2), 183206.
Zahavi, U., Felner, A., Burch, N., & Holte, R. C. (2008). Predicting performance
IDA* conditional distributions. Proceedings Twenty-Third Conference
Artificial Intelligence (AAAI-08), pp. 381386.
Zahavi, U., Felner, A., Holte, R., & Schaeffer, J. (2006). Dual search permutation
state spaces. Proceedings Twenty-First Conference Artificial Intelligence
(AAAI-06), pp. 10761081.
Zahavi, U., Felner, A., Holte, R. C., & Schaeffer, J. (2008). Duality permutation state
spaces dual search algorithm. Artificial Intelligence, 172 (4-5), 514540.
Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. R. (2007). Inconsistent heuristics.
Proceedings Twenty-Second Conference Artificial Intelligence (AAAI-07),
pp. 12111216.
Zhou, R., & Hansen, E. A. (2004). Space-efficient memory-based heuristics. Proceedings
Nineteenth Conference Artificial Intelligence (AAAI-04), pp. 677682.

83

fi
