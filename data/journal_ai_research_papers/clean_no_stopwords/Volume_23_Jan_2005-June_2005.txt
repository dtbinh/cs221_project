Journal Artificial Intelligence Research 23 (2005) 625-666

Submitted 07/04; published 06/05

Expressive Language Efficient Execution System
Software Agents
gbarish@fetch.com

Greg Barish
Fetch Technologies
2041 Rosecrans Avenue, Suite 245
El Segundo, CA 90245 USA

knoblock@isi.edu

Craig A. Knoblock
University Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Abstract
Software agents used automate many tedious, time-consuming
information processing tasks humans currently complete manually.
However, so, agent plans must capable representing myriad actions
control flows required perform tasks. addition, since tasks require
integrating multiple sources remote information typically, slow, I/O-bound process
desirable make execution efficient possible. address
needs, present flexible software agent plan language highly parallel execution
system enable efficient execution expressive agent plans. plan language
allows complex tasks easily expressed providing variety operators
flexibly processing data well supporting subplans (for modularity) recursion
(for indeterminate looping). executor based streaming dataflow model
execution maximize amount operator data parallelism possible runtime.
implemented language executor system called THESEUS.
results testing THESEUS show streaming dataflow execution yield
significant speedups traditional serial (von Neumann) well non-streaming
dataflow-style execution existing software robot agent execution systems
currently support. addition, show plans written language present
represent certain types subtasks cannot accomplished using languages
supported network query engines. Finally, demonstrate increased
expressivity plan language hamper performance; specifically, show
data integrated multiple remote sources efficiently using
architecture possible state-of-the-art streaming-dataflow network query
engine.

1. Introduction
goal software agents automate tasks require interacting one
accessible software systems. Past research yielded several types agents agent
frameworks capable automating wide range tasks, including: processing sequences
operating system commands (Golden, Etzioni, & Weld, 1994), mediation heterogeneous data
sources (Wiederhold 1996; Bayardo, Bohrer, Brice, Cichocki, Fowler, Helal, Kashyap, Ksiezyk,
Martin, Nodine, Rashid, Rusinkiewicz, Shea, Unnikrishnan, Unruh, & Woelk 1997; Knoblock,
Minton, Ambite, Ashish, Muslea, & Tejada 2001), online comparison shopping (Doorenbos,
2005 AI Access Foundation. Rights Reserved.

fiBARISH & KNOBLOCK

Etzioni, & Weld, 1996), continual financial portfolio analysis (Decker, Sycara, & Zeng, 1996),
airline ticket monitoring (Etzioni, Tuchinda, Knoblock, & Yates, 2004), name few.
Despite software agent heterogeneity, two recurring characteristics (i) wide variety
tasks agents used automate (ii) frequent need process route information
agent execution.
Perhaps domain poses many tantalizing possibilities software agent automation
Web. ubiquity practicality Web suggests many potential benefits
gained automating tasks related sources web. Furthermore, Web ripe
automation given sheer number online applications complete lack
coordination them, agents could address endless list needs problems
solved people use Web practical purposes. Furthermore, like software
agent domains, Web tasks vary widely complexity and, definition, involve routing
processing information part task.
paper, describe software agent plan language execution system enables
one express wide range tasks software agent plan plan
efficiently executed. implemented language executor system called
THESEUS. Throughout paper, discuss THESEUS context Web information
gathering processing, since Web represents domain (if all)
challenges software agents face found.
1.1 Web Information Agents
recent years, Web experienced rapid rate growth, useful
information becoming available online. Today, exists enormous amount online data
people view, also use order accomplish real tasks. Hundreds
thousands people use Web every day research airfares, monitor financial portfolios,
keep date latest news headlines. addition enormity, compelling
Internet practical tool dynamic, up-to-the-minute nature. example,
although information stock quotes ticket availabilities change frequently, many
sources Internet capable reporting updates immediately. reason,
breadth depth information provides, Web become certain
tasks timely necessary medium even daily newspaper, radio, television.
degree complexity gathering information Web varies significantly.
types tasks accomplished manually size data gathered small
need query infrequent. example, finding address restaurant theater
particular city using Yellow Pages type Web site easy enough people themselves.
need automated, since query need done result returned
small easy manage. However, information gathering tasks simple.
often times amount data involved large, answer requires integrating data
multiple sites, answer requires multiple queries period time. example,
consider shopping expensive product period time using multiple sources
updated daily. tasks become quickly tedious require greater amount
manual work, making desirable automate.
1.1.1. COMPLICATED TASKS
One type difficult Web information gathering task involves interleaved gathering
navigation. benefit people use Web browser access online data, many Web
sources display large sets query results spread series web pages connected
Next Page links. example, querying online classified listings source automobiles
sale generate many results. Instead displaying results single long Web page,
many classified listings sites group sets results series hyperlinked pages. order
automatically collect data, system needs interleave navigation gathering

626

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

indeterminate number times: is, needs collect results given page, navigate
next, gather next set results, navigate, on, reaches end set
results. work addressing theoretically incorporate navigation
gathering process (Friedman, Levy, & Millstein, 1999), attention given
efficient execution plans engage type interleaved retrieval.
second example monitoring Web source. Since Web contain
built-in trigger facility, one forced manually check sources updated data. updates
frequent need identify update immediately urgent, becomes desirable
automate monitoring updates, notifying user one conditions
met. example, suppose want alerted soon particular type used car listed
sale one online classified ad sources. Repeated manual checking changes
obviously tedious. Mediators network query engines automate query,
additional software programming languages Java C must written handle
monitoring process itself, something requires conditional execution, comparison past
results, possible notification user, actions.
1.1.2. NEED FLEXIBILITY
examples show automatically querying processing Web data involve
number subtasks, interleaved navigation gathering integration local
databases. needs, traditional database query languages like SQL insufficient
Web. root problem lack flexibility, expressivity, languages -typically, querying supported. complicated types Web information gathering
tasks, described articles (Etzioni & Weld, 1994; Doorenbos et al.,
1997; Chalupsky, Gil, Knoblock, Lerman, Oh, Pynadath, Russ, & Tambe, 2001; Ambite, Barish,
Knoblock, Muslea, Oh, & Minton, 2002; Sycara, Paolucci, van Velsen, & Giampapa, 2003;
Graham, Decker, & Mersic, 2003), usually involve actions beyond needed merely
querying (i.e., beyond filtering combining) require plans capable variety
actions, conditional execution, integration local databases, asynchronous
notification users. short, Web information gathering tasks require expressive query
plan language describe solution.
XQuery (Boag, Chamberlin, Fernandez, Florescu, Robie, & Simeon, 2002), used querying
XML documents, one language offers flexibility. example, XQuery supports
FLWOR expressions allow one easily specify iterate data. XQuery also
supports conditional expressions, UDFs, recursive functions.
Support expressive agent plans also found number software agent robot
agent frameworks, INFOSLEUTH (Bayardo et al., 1997), RETSINA (Sycara et al., 2003),
DECAF (Graham et al., 2003), RAPs (Firby 1994) PRS-LITE (Myers 1996). systems
support concurrent execution operators ability execute complicated types
plans, require conditionals. addition, unlike database systems, software
agent robot agent execution plans contain many different types operators,
limited querying filtering data.
1.1.3. NEED EFFICIENCY
Despite support expressive plans, existing software agent robot agent plan execution
systems lack efficiency problem painfully realized kind large scale
data integration working remote sources operate less optimal speeds.
particular, systems like RETSINA, DECAF, RAPs PRS-LITE ensure high-degree
operator parallelism (independent operators execute concurrently), ensure
type data parallelism (independent elements data processed concurrently).
example, possible one operator systems stream information another.
understandable case robot plan execution systems, address robot

627

fiBARISH & KNOBLOCK

interacts physical world, typically concerned communicating effects,
object X direction north, relatively speaking small amounts local
information. Interestingly, software agent frameworks like DECAF INFOSLEUTH
expressed desire support sort streaming architecture (Bayardo et al., 1997),
research constrained part use data transport layers (such KQML)
contain infrastructure necessary support streaming.
However, software agent process large amounts remote information efficiently,
types parallelism critical. Dataflow-style parallelism important order schedule
independent operations concurrently; streaming important order able process remote
information becomes available make maximum use local processing resources.
Consider agent engages two types image processing image data downloaded
surveillance satellite. normally takes one minute download data another minute
type processing, streaming dataflow execution theoretically reduce overall
execution time two-thirds. Even greater speedups possible different information
processing tasks.
Though existing software agent robot plan execution systems support streaming,
substantial amount previous work gone building architectures database
systems (Wilschut & Apers, 1993) recently network query engines (Ives, Florescu,
Friedman, Levy, & Weld, 1999; Hellerstein, Franklin, Chandrasekaran., Deshpande, Hildrum,
Madden, Raman, & Shah, 2000; Naughton, DeWitt, Maier, Aboulnaga, Chen, Galanis, Kang,
Krishnamurthy, Luo, Prakash, Ramamurthy, Shanmugasundaram, Tian, Tufte, Viglas, Wang,
Zhang, Jackson, Gupta, & Che, 2001). systems employ special iterative-style operators
aware underlying support streaming1 exploit feature minimize
operator blocking. Examples include pipelined hash join (Wilschut & Apers, 1993; Ives, et
al., 1999) eddy data structure (Avnur & Hellerstein, 2000), efficiently routes streaming
data operators. Despite support streaming dataflow model execution, network query
engines lack generality flexibility existing agent frameworks systems. XQuery
provide powerful flexible language Web data gathering manipulation.
However, network query engines support subset XQuery related XML query
processing operators support constructs conditionals recursion2,
essential complex types information processing tasks.
1.2 Contributions
summary, desirable automate gathering processing data Web,
currently possible build agent flexible efficient using existing
technologies. Existing agent execution systems flexible types plans support,
lack ability stream information, critical feature needs built
underlying architecture well individual operators (i.e., operators need implemented
iterators). Network query engines contain support streaming dataflow, lack
expressivity provided existing agent plan languages.
paper, address need combine presenting expressive plan language
efficient execution system software agents. specifically, paper makes two
contributions. first software agent plan language extends features existing agent
plan languages expressive query languages existing information
integration systems network query engines. language proposed consists rich set
operators that, beyond gathering manipulating data, support conditional execution,
management data local persistent sources, asynchronous notification results users,
1

Note: term pipelining common database literature (e.g., Graefe, 1993), although streaming
often used network query engine literature (see recent publications Niagara Telegraph).
2
example, Tukwila supports subset Xquery (Ives et al., 2002).

628

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

integration relational XML sources, extensibility (i.e., user-defined functions).
addition, language modular encourages re-use: existing plans called
plans subplans plans called recursively. operators constructs
provide expressivity necessary address complicated types software agent tasks,
monitoring interleaved navigation gathering Web data.
second contribution paper design executor efficiently processes
agent plans written proposed language. core executor implements streaming
dataflow architecture, data dispatched consuming operators becomes available
operators execute whenever possible. design allows plans realize maximum
degree operational (horizontal) data (vertical) parallelism possible. design also
supports recursive streaming, resulting efficient execution plans require
indeterminate looping, agents interleave navigation gathering information
Web. short, executor supports highly parallel execution software agent plans,
leading significant performance improvement provided expressive, less
efficient agent executors.
implemented plan language executor system called THESEUS.
Throughout paper, refer example plans deployed THESEUS order
better illustrate plan expressivity and, later, validate efficiency claims.
1.3 Organization
rest paper organized follows. Section 2 provides background basic
terminology dataflow computing (Arvind & Nikhil, 1990; Papdopoulos & Culler, 1990;
Gurd & Snelling, 1992), generic information integration (Chawathe, Garcia-Molina, Hammer,
Ireland, Papakonstantinou, Ullman, & Widom, 1994; Arens, Knoblock, & Shen, 1996; Levy,
Rajaraman, & Ordille, 1996; Weiderhold 1996; Genesereth, Keller, & Duschka, 1997)
automated Web information gathering (Knoblock et al., 2001; Ives et al., 1999; Barish &
Knoblock, 2002; Thakkar, Knoblock, & Ambite, 2003; Tuchinda & Knoblock, 2004). Section 3
describes details involved one type complex software agent information gathering task,
example used throughout rest paper. Section 4, describe
proposed plan language detail. Section 5 deals design streaming dataflow
executor provides high degrees horizontal vertical parallelism runtime.
Section 6, present experimental results, describing plan language executor
implemented THESEUS measure provided systems. Section 7,
discuss related work greater detail. Finally, present overall conclusions discuss
future work.

2. Preliminaries
language execution system present paper build upon foundation prior
research related dataflow computing (Dennis 1974) Web information integration
(Wiederhold 1996; Bayardo et al., 1997, Knoblock et al., 2001). Although seemingly orthogonal
disciplines, effective complements parallelism asynchrony provided
dataflow computing lends performance problems associated Web information
gathering.
2.1 Dataflow Computing
pure dataflow model computation first introduced Dennis (1974) alternative
standard von Neumann execution model. foundations share much common past
work computation graphs (Karp & Miller, 1955), process networks (Kahn 1974),
communicating sequential processes (Hoare 1978). Dataflow computing long theoretical
experimental history, first machines proposed early 1970s real

629

fiBARISH & KNOBLOCK

physical systems constructed late 1970s throughout 1980s early 1990s
(Arvind & Nikhil, 1990; Papdopoulos & Culler, 1990; Gurd & Snelling, 1992).
dataflow model computation describes program execution terms data
dependencies instructions. dataflow graph directed acyclic graph (DAG) nodes
edges. nodes called actors. consume produce data tokens along edges
connect actors. actors run concurrently able execute, fire,
time input tokens arrive. Input tokens come initial program input
result earlier execution (i.e., output prior actor firings). potential overall
concurrency execution thus function data dependencies exist program,
degree parallelism referred dataflow limit.
key observation made dataflow computing execution inherently
parallel actors function independently (asynchronously) fire necessary. contrast,
von Neumann execution model involves sequential processing pre-ordered set
instructions. Thus, execution inherently serial. comparing dataflow von Neumann,
subtle difference (yet one heart distinction two) noted
scheduling instructions determined run-time (i.e., dynamic scheduling), whereas
von Neumann system occurs compile-time (i.e., static scheduling). Figure 1 illustrates
difference dataflow von Neumann approaches applied execution
simple program. program requires multiplication two independent additions.
von Neumann style execution, ADD operations must executed sequentially, even
though independent other, instruction counter schedules one
instruction time. contrast, since availability data drives scheduling dataflow
machine, ADD operations executed soon input dependencies fulfilled.
Dataflow systems evolved classic static (Dennis 1974) model dynamic
tagged token models (Arvind & Nikhil, 1990) allowed multiple tokens per arc, hybrid
models combine von Neumann traditional dataflow styles execution (Iannucci, 1988;
Evripidou & Gaudiot, 1991; Gao, 1993). models applied digital signal
processing include boolean dataflow synchronous dataflow (Lee & Messerschmitt, 1987),
resulting architectures known dataflow networks. work described paper
relevant specific hybrid dataflow approach, known threaded dataflow
(Papadopoulos & Traub, 1991), maintains data-driven model execution associates

Figure 1: Comparing von Neuman dataflow computing

630

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

instruction streams individual threads execute von Neumann fashion. distinct
pure von Neumann multithreading sense data, instruction counter, remains
basis scheduling instructions (operators). also distinct pure dataflow
sense execution instruction streams statically scheduled sequential task, unlike
typical dynamic scheduling found dataflow machines. result, threaded dataflow also
viewed data-driven multithreading.
Recent advances processor architecture, Simultaneous Multithreading (SMT)
project (Tullsen, Eggers, & Levy, 1995) demonstrated benefits data-driven
multithreading. SMT-style processors differ conventional CPUs (such Intel Pentium)
partitioning on-chip resources multiple threads execute concurrently, making better
use available functional units amount chip real estate. resulting execution
reduces vertical waste (the wasting cycles) occur sequence instructions
executed using one thread, well horizontal waste (the wasting available functional
units) occur executing multiple threads. so, technique effectively trades
instruction-level parallelism (ILP) benefits thread-level parallelism (TLP) benefits. Instead
deep processor pipeline (which becomes less useful depth increases), SMT
processors contain multiple shorter pipelines, associated single thread. result
can, highly parallel applications, substantially improve scheduling on-chip resources
that, conventional CPUs, would normally starved result I/O stalls well
thread context-switching.
work described applies threaded dataflow design higher level execution
information gathering plan level. Instead executing fine-grained instructions,
interested execution coarse-grained operators. Still, believe threaded dataflow
generally efficient strategy executing I/O-bound information gathering plans integrate
multiple remote sources allows coarse-grained I/O requests (such network requests
multiple Web sources) automatically scheduled parallel. plans similar
systems maintain high degrees concurrent network connections, Web
server database system. Prior studies Web servers (Redstone, Eggers, & Levy, 2000)
database systems (Lo, Barroso, Eggers, Gharachorloo, Levy, & Parekh, 1998) already
shown systems run efficiently SMT-style processors; believe
hold true execution dataflow-style information gathering plans.
2.2 Web-based Information Gathering Integration
Generic information integration systems (Chawathe et al., 1994; Arens et al., 1996; Levy et al.,
1996; Genesereth et al., 1997) concerned problem allowing multiple distributed
information sources queried logical whole. systems typically deal
heterogeneous sources addition traditional databases, provide transparent access
flat files, information agents, structured data sources. high-level domain model maps
domain-level entities attributes underlying sources information provide.
information mediator (Wiederhold 1996) responsible query processing, using domain
model information sources compile query plan. traditional databases, query
processing involves three major phases: (a) parsing query, (b) query plan generation
optimization (c) execution. Query processing information integration involves
phases builds upon traditional query plan optimization techniques addressing cases
involve duplicate, slow, and/or unreliable information sources.
Web-based information integration differs types information integration
focusing specific case information sources Web sites (Knoblock et al., 2001).
adds two additional challenges basic integration problem: (1) retrieving
structured information (i.e., relation) semi-structured source (Web pages written
HTML) (2) querying data organized manner facilitates human visual
consumption, necessarily strictly relational manner. address first challenge, Web

631

fiBARISH & KNOBLOCK

site wrappers used convert semi-structured HTML structured relations, allowing Web
sites queried databases. Wrappers take queries (such expressed
query language like SQL) process data extracted Web site, thus providing
transparent way accessing unstructured information structured. Wrappers
constructed manually automatically, latter using machine learning techniques (Knoblock,
Lerman, Minton, & Muslea 2000; Kushmerick, 2000). wrappers used extract
data many Web sites, sites problematic data extracted
presented. One common case Web site distributes single logical relational answer
multiple physical Web pages, case online classifieds example described
earlier. Automating interleaved navigation gathering required scenarios, yet
received little attention literature. One approach extend traditional query answering
information integration systems incorporate capability navigation (Friedman et al.,
1999). However, solutions mostly address query processing phase remains
open issue regarding execute types information gathering plans efficiently.
recent technology querying Web network query engine (Ives et al.,
1999; Hellerstein et al., 2000; Naughton et al., 2001). systems are, like mediators,
capable querying sets Web sources, greater focus challenges
efficient query plan execution, robustness face network failure large data sets,
processing XML data. Many network query engines rely adaptive execution techniques,
dynamic reordering tuples among query plan operators (Avnur & Hellerstein 2000)
double pipelined hash join (Ives et al., 1999), overcome inherent latency unpredictable
availability Web sites.
important aspect network query engine research focus dataflow-style
execution. Research parallel database systems long regarded dataflow-style query
execution efficient (Grafe, 1993; Wilschut & Apers, 1993). However, applied Web,
dataflow-style processing yield even greater speedups (a) Web sources remote,
base latency access much higher accessing local data (b) Web data
cannot strategically pre-partitioned, shared-nothing architectures (DeWitt & Gray,
1992). Thus, average latency Web data access high, parallelizing capability
dataflow-style execution even compelling traditional parallel database
systems potential speedups greater.

3. Motivating Example
discussed earlier, mediators network query engines allow distributed Web data
queried efficiently, cannot handle complicated types information
gathering tasks query (and thus plan) languages support degree
expressivity required. better motivate discussion, describe detailed example
information gathering problem requires complex plan. Throughout rest
paper, refer example describe details proposed agent plan
language execution system.
example involves using Web search new house buy. Suppose want
use online real estate listings site, Homeseekers (http://www.homeseekers.com),
locate houses meet certain set price, location, room constraints. so,
want query run periodically medium duration time (e.g., weeks)
new updates (i.e., new houses meet criteria) e-mailed us found.
understand automate gathering part task, let us first discuss users
would complete manually. Figures 2a, 2b, 2c show user interface result pages

632

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

Figure 2a: Initial query form Yahoo Real Estate

Figure 2b: Initial results Yahoo Real Estate

Figure 2c: Detailed result Yahoo Real Estate

633

fiBARISH & KNOBLOCK

Homeseekers. query new homes, users initially fill criteria shown Figure 2a
specifically, enter information includes city, state, maximum price, etc. fill
form, submit query site initial set results returned
shown Figure 2b. However, notice page contains results 1 15 22.
get remainder results, "Next" link (circled Figure 2b) must followed page
containing results 16 22. Finally, get details house, users must follow
URL link associated listing. sample detail screen shown Figure 2c. detail
screen useful often contains pictures information, MLS
(multiple listing services) information, house. example, detailed page
house must investigated order identify houses contain number rooms desired.
Users would repeat process period days, weeks, even months.
user must query site periodically keep track new results hand. latter
aspect require great deal work users must note houses result list new
entries identify changes (e.g., selling price updates) houses previously
viewed.
already discussed, possible accomplish part task using existing Web
query techniques, provided mediators network query engines. However,
notice task requires actions beyond gathering filtering data. involves periodic
execution, comparison past results, conditional execution, asynchronous notification
user. actions traditional Web query languages support indeed,
actions involve gathering filtering. Instead query plan language,
needed agent plan language supports operators constructs necessary complete
task.
consider agent plans generally might look. Figure 3 shows abstract plan
monitoring Homeseekers. figure shows, search criteria used input generate
one pages house listing results. URLs house results page
extracted compared houses already existed local database. New houses
web page database subsequently queried details
appended database future queries distinguish new results. extraction
houses given Homeseekers results page, "Next" link (if any) page
followed houses page go process. next-link processing
cycle stops last result page, page without Next link, reached. Then,
details last house gathered, update set new houses found
e-mailed user.
LOAD DAT AB ASE
h ous es
pre viousl een
EXTRACT
hous e URLs

search
criteria

GET h ouse
results p ag e

FILTER
thos e h ouses
pre viousl een

EXTRACT
"ne xt pag e" lin k

GET h ouse
det ail pag e

UPDATE ATAB ASE
ne w ho uses

Figure 3: Abstract plan monitoring Yahoo Real Estate

634

SEND E-MAIL
user

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

4. Expressive Plan Language Software Agents
section, present agent plan language makes possible construct plans
capable complicated tasks. Throughout section, focus information gathering
tasks, Homeseekers example shown Figure 3.
4.1 Plan Representation
language, plans textual representations dataflow graphs describing set input
data, series operations data (and intermediate results leads to), set
output data. discussed earlier, dataflow naturally efficient paradigm information
gathering plans. Graphs consist set operator sequences (flows) data one
operator given flow iteratively processed flows successive operators
flow, eventually merged another flow output plan.
example, Figure 4 illustrates dataflow graph form plan named Example_plan.
shows plan consists six nodes (operators) connected set edges (variables).
solid directed edges (labeled a, b, c, d, f, g) represent stream data, dashed
directed edge (labeled e) represents signal used synchronization purposes.
c


b

Op1


Op2

f
Op4
g

e

Op3

Op5

Figure 4: Graph form Example_plan

Figure 5 shows text form plan. header consists name plan
(example_plan), set input variables (a b), set output variables (f). body
section plan contains set operators. set inputs operator appears
left colon delimiter set outputs appears right delimiter. One
operator (Op3) WAIT clause associated production signal indicated
Figure 4 e. ENABLE clause later operator (Op5) describes consumption
signal.
graph text forms example plan describe following execution.
Variables b plan input variables. Together, trigger execution Op1,
produces variable c. Op2 fires c becomes available, leads output variable
d. Op3 fires upon availability produces signal e. Op4 uses compute f (the
PLAN example_plan
{
INPUT: a, b
OUTPUT: f
BODY
{
Op1
Op2
Op3
Op4
Op5
}

(a, b : c)
(c : d)
(c : ) {ENABLE: e}
(d : f, g)
(g : ) {WAIT : e}

}
Figure 5: Text form Example_plan

635

fiBARISH & KNOBLOCK

plan output variable) g. Finally, availability g signal e triggers execution
Op5.
Note although body part text form plan lists operators linear order,
ordering affect actually executed. Per dataflow model
processing, operators fire whenever individual data dependencies fulfilled. example,
although Op3 follows Op2 order specified plan text, actually executes
logical time Op2. Also note plan output, f, produced plan still running
(i.e., Op5 still processing).
4.1.1. FORMAL DEFINITIONS
Formally, define following:
Definition 1: information gathering plan P represented directed graph
operators Ops nodes connected set variables Vars edges. plan
associated subset Vars plan input variables PlanIn another subset
variables plan output variables PlanOut. specifically, let plan P represented
tuple
P = <Vars, Ops, PlanIn, PlanOut>

Vars = {v1, ..., vn}, n > 0
Ops = {Op1, ..., Opm}, > 0
PlanIn = {va1, ..., vax}, x > 0, s.t. {va1, ..., vax} Vars
PlanOut = {vb1, ..., vby}, >= 0, s.t. {vb1, ..., vby} Vars
Definition 2: plan operator Op encapsulates function Func computes set operator
output variables OpOut set operator input variables OpIn. specifically, let
operator Opi P represented tuple


Opi = <OpIn, OpOut, Func>
OpIn = {vi1, ..., vic}, c > 0, s.t. {vi1, ..., vic} Vars
OpOut = {vo1, ..., vog}, g >= 0, s.t. {vo1, ..., vog} Vars
Func = Function computes {vo1, ..., vog} {vi1, ..., vic}

Furthermore, plan Pa also called another plan Pb operator. case,
plan Pa known subplan.
Definition 3: schedule execution operator instance Opi described firing
rule depends OpIn, optional second set input wait variables OpWait, results
generation OpOut optional second set output enablement variables OpEnable.
initial firing operator conditional availability least one OpIn
OpWait. initial firing, OpEnable variables declared also produced.
OpOut variables produced accordance semantics operator.

specifically, let us define:


(Opi) = <OpIn, OpWait, OpOut, OpEnable>
OpWait = {vw1, ..., vwd}, >= 0, s.t. {vw1, ..., vwd} Vars
OpEnable = {ve1, ..., veh}, h >= 0, s.t. {ve1, ..., veh} Vars

Wait enable variables synchronization mechanisms allow operator execution
conditional beyond normal set input data variables. understand how, let us first
distinguish standard data variable synchronization variable. standard data
variable one contains information meant interpreted, specifically,

636

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

processed function operator encapsulates. example, PlanIn, PlanOut, OpIn,
OpOut consist normal data variables. synchronization variable (earlier called
signal) one consists data meant interpreted rather, variables
merely used additional conditions execution. Since control dataflow systems driven
availability data, synchronization variables dataflow style plans useful
provide control flow flexibility. example, certain static operation occur
time given data flow active, synchronization variables allow us declare behavior.
Definition 3 indicates that, like actors traditional dataflow programs, operators
information gathering plans firing rule describes operator process
input. example, dataflow computer specified (Dennis 1974), actors fire
incoming arcs contain data. plans language described paper, firing
rule slightly different:
operator may fire upon receipt input variable, providing received
wait variables.
Note plans operators require least one input because, firing rule implies,
plans operators without least one input would fire continuously.
4.2 Data Structures
Operators process transmit data terms relations. relation R consists set
attributes (i.e., columns) a1..ac set zero tuples (i.e., rows) t1..tr, tuple ti
containing values vi1..vic. express relations attributes set tuples containing
values attributes as:
R (a1, ..., ac) = {{v11, ..., v1c}, {v21, ..., v2c}, ..., {vr1, ..., vrc}}
attribute relation one five types: char, number, date, relation (embedded),
document (i.e., DOM object).
Embedded relations (Schek & Scholl, 1986) within particular relation Rx treated
opaque objects vij processed operator. However, extracted, become
separate relation Ry processed rest system. Embedded relations
useful allow set values (the non-embedded objects) associated
entire relation. example, operator performs COUNT function relation
determine number tuples contained relation, resulting tuple emitted
operator consist two attributes: (a) embedded relation object (b) value equal
number rows embedded relation. Embedded relations thus allow sets
associated singletons, rather forcing join two. sense, preserve
relationship particular tuple relation without requiring space
additional key repeating data (as join would require).
XML data supported document attribute type. XML one type document
specified Document Object Model (DOM). proposed language contains specific
operators allow DOM objects converted relations, relations converted
DOM objects, DOM objects XML documents queried native form
using XQuery. Thus, language supports querying XML documents native
flattened form.
4.3 Plan Operators
available operators plan language represent rich set functions used
address challenges complex information gathering tasks, monitoring.
Specifically, operators support following classes actions:


data gathering: retrieval data network traditional relational
databases, Oracle DB2.

637

fiBARISH & KNOBLOCK








data manipulation: including standard relational data manipulation, Select
Join, well XML-style manipulations XQuery.
data storage: export updating data traditional relational databases.
conditional execution: routing data based contents run-time.
asynchronous notification: communication intermediate/periodic results
mediums/devices transmitted data queued (e.g., e-mail).
task administration: dynamic scheduling unscheduling plans external
task database.
extensibility: ability embed special type computation (single-row
aggregate) directly streaming dataflow query plan

Though operators differ exact semantics, share similarities
process input generate output. particular, two modes worth noting: automatic
joining output input (a dependent join) packing (embedding) unpacking
(extracting) relations.
information gathering plans, common use data collected one source basis
querying additional sources. Later, often becomes desirable associate input
source output produces. However, join separate step tedious
requires creation another key existing set data plus cost join.
simplify plans improve efficiency execution, many operators language
perform dependent join input tuples onto output tuples produce. dependent
join simply combines contents input tuple output tuple(s) generates,
preserving parity two. example, operator ROUND converts floating
point value column nearest whole integer value. Thus, input data consisted
tuples {{Jack, 89.73}, {Jill, 98.21}} result ROUND operator executes would
{{Jack, 89.73, 90}, {Jill, 98.21, 98}}. Without dependent join, primary key would need
added (if one already exist) separate join would done
ROUND computation. Thus, dependent joins simplify plans reduce total number
operators plan (by reducing number decoupled joins) eliminate need ensure
entity integrity prior processing.
Another processing mode operators involves packing unpacking relations3.
operations relevant context embedded relations. Instead creating
managing two distinct results (which often need joined later), cleaner spaceefficient perform dependent join packed version input relation result
output aggregate-type operator. example, using AVERAGE operator
input data above, result dependent join packed form original relation
would be: {{{Jack, 89.73}, {Jill, 98.21}}, 93.97}. Unpacking would necessary get
original data. short, embedded relations make easy associate aggregates values
led derivation. Packing unpacking useful data handling techniques
facilitate goal.
Table 1 shows entire set operators proposed language. (such
Select Join) well-known semantics (Abiteboul, Hull, & Vianu, 1995) used
database information gathering systems. result, discuss
detail. However, many operators new provide ability express
complicated types plans. focus purpose mechanics
operators.

3

Note: operations also referred NEST UNNEST database literature.

638

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

Operator

Purpose

Fetch extract data web sites relations.
Filters data relation.
Filters attributes relation.
Combines data two relations, based specified condition.
Performs set union two relations.
Finds intersection two relations.
Subtracts one relation another.
Returns tuples unique across one attributes.
Conditionally routes one two streams based existence tuples third
Embeds relation within new relation consisting single tuple.
Extracts embedded relation tuples input relation.
Generates new formatted text attribute based tuple values.
Converts relation XML document.
Converts XML document relation.
Queries XML document attribute tuples input relation using language
specified Xquery standard, returning XML document result attribute
Xquery
contained tuples output relation.
DbImport Scan table local database.
DbQuery Query schema local database using SQL.
DbAppend Appends relation existing table creates table none exists.
DbExport Exports relation single table.
DbUpdate Executes SQL-style update query; results returned.
Uses SMTP communicate email message valid email address.
Email
Sends text message valid cell phone number.
Phone
Faxes data recipient valid fax number.
Fax
Schedule Adds task task database scheduling information.
Unschedule Removes task database.
Executes user-defined function tuple relation.
Apply
Aggregate Executes user-defined function entire relation.
Wrapper
Select
Project
Join
Union
Intersect
Minus
Distinct
Null
Pack
Unpack
Format
Rel2xml
Xml2rel

Table 1: complete set operators

4.3.1. INTERACTING LOCAL DATABASES
two major reasons useful able interact local database systems
plan execution. One reason local database may contain information
wish integrate online information. second reason ability
local database act memory plans run continuously plan run later
time needs use results plan run earlier time.
address needs, database operators DbImport, DbQuery, DbExport, DbAppend
provided. common use operators implement monitoring-style query.
example, suppose wish gradually collect data period time, collection
house data Homeseekers example. accomplish this, DbImport DbQuery
used bring previously queried data plan compared newly queried
data (gathered Wrapper operator) using set-theoretic operators, Minus)
result difference written back database DbAppend DbExport.
4.3.2. SUPPORTING CONDITIONAL EXECUTION
Conditional execution important plans need perform different actions data based
run-time value data. analyze conditionally route data plan, language

639

fiBARISH & KNOBLOCK

supports Null operator. Null acts switch, conditionally routing one set data based
status another set data. Null refers predicate Null, conditional
executes different actions depending result evaluation. data null, action
performed; null, action B performed. accomplish dataflow,
Null operator publishes different sets data, one either case.
example, suppose desirable stock quotes automatically communicated
user every 30 minutes. Normally, quotes retrieved e-mailed. However,
percentage price change stock portfolio greater 20%, quotes
sent via cell phone messaging (since communication immediate). Null
would useful case would allow Select condition process check
price changes exist tuples match filtering criteria allow data trigger
operator communicated results via cell phone. Otherwise, Null would route data
operator communicated information via e-mail. short, Null powerful
dynamic form conditional execution used operators
(like Select) activate/deactivate flows based runtime content data.
input output Null summarized Figure 6. input data analyzed d,
data forwarded upon true (null) dt, data forwarded upon false df. null
(i.e., contains zero tuples), dt copied output variable t. Otherwise, df copied output
f. example, contains three tuples {x1, x2, x3} dt contains five tuples {t1, t2, t3, t4,
t5} df contains two tuples {f1, f2}, variable containing {t1, t2, t3, t4, t5}
output. Consumers f never receive data.


dt
df

Null


f

Figure 6: NULL operator

4.3.3. CALLING USER-DEFINED FUNCTIONS
designing number agent plans, found times agents needed
execute special logic (e.g., business logic) execution. Usually, logic
involve relational information processing plan writer simply wanted able code
standard programming language (such Java C). example, plans written
Electric Elves travel agents (Ambite et al., 2002), necessary agent send
updates users via DARPA CoAbs Agent Grid network. plans, needed
normalize formats date strings produced different Web sources. Instead expanding
operator set unique type logic encountered, developed two special operators
allowed plans make calls arbitrary functions written standard programming
languages. goal operators (a) make easier write plans
required special calculations library calls, (b) encourage non-relational information processing
(which could benefit efficiency dataflow style processing) modularized
outside plan, (c) simplify plans.
two operators, Apply Aggregate, provide extensibility tuple relation
level. Apply calls user-defined single-row functions tuple relational data performs
dependent join input tuple corresponding result. example, user-defined
single-row function called SQRT might return tuple consisting two values: input value
square root. user defined function written standard programming language,
Java, executed per-tuple basis. Thus, type external function
similar use stored procedures UDFs commercial relational database systems.
Aggregate operator calls user-defined multi-row functions performs dependent join
packed form input result. example, COUNT function might return

640

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

relation consisting single tuple two values: first packed form input
second count number distinct rows relation. Apply,
user-defined multi-row function written standard programming language like Java.
However, contrast called per-tuple basis, executed per-relation basis.
4.3.4. XML INTEGRATION
purposes efficiency flexibility, often convenient package transform data
to/from XML mid-plan execution. example, contents large data set often
described compactly leveraging hierarchy XML document. addition,
Web sources (such Web services) already provide query answers XML format. analyze
process data, often simpler efficient deal native form rather
convert relations, process it, convert back XML. However, cases,
relatively small amount XML data might need joined large set relational data.
provide flexible XML manipulation integration, language supports Rel2xml,
Xml2rel, Xquery operators. first two convert relations XML documents viceversa, using straightforward algorithms. Xml2Rel allows one specify iterating element
(which map tuples relation) attribute elements (which map attributes
relation), generates tuples include index referring order original
XML element parsed. Cross product style flattening deeper child elements performed
automatically. Rel2Xml even straightforward: creates parent XML elements
tuple inserts attribute elements children, order appear relation. allow
XML processed native form, support Xquery operator, based XQuery
standard (Boag et al., 2002).
Xml2Rel, Rel2Xml, Xquery complementary terms functionality. Xml2Rel
handles basic conversion XML relational data, noting order data document.
Rel2Xml handles basic conversion back XML, without regards order note
nature streaming dataflow parallelism order processing tuples
deliberately guaranteed. However, order XML document generated Rel2Xml
important, Xquery used post-processing step address requirement. short,
Xml2Rel Rel2Xml focus simple task converting relation document;
complex processing XML accomplished Xquery operator.
4.3.5. ASYNCHRONOUS NOTIFICATION
Many continuously running plans, Homeseekers, involve interactive sessions
users. Instead, users request plan run given schedule expect receive updates
periodic execution plan. updates delivered asynchronous
means, e-mail, cell-phone messaging, facsimile. facilitate notification,
language includes Email, Fax, Phone operators communicating data via devices.
operators works similar fashion. Input data received operator reformatted form suitable transmission target device. data
transmitted: Email sends e-mail message, Fax contacts facsimile server data,
Phone routes data cell phone capable receiving messages.
4.3.6. AUTOMATIC TASK ADMINISTRATION
overall system accompanies language includes task database daemon process
periodically reads task database executes plans according schedule.
architecture shown Figure 7. Task entries consist plan name, set input provide
plan, scheduling information. latter data represented format similar
UNIX crontab entry. format allows minute, hour, day month, month, year
plan supposed run. example, task entry
05 08-17 1,3,5 * * homeseekers.plan

641

fiBARISH & KNOBLOCK

Figure 7: Task administration process

means: run homeseekers.plan five minutes every hour 8am 5pm 1st,
3rd, 5th days every month every year.
tasks scheduled manually, language developed also allows plans
automatically update scheduling plans, including it. so, support two special
scheduling operators, Schedule Unschedule. former allows plan register new plan
run. creates updates plan schedule data task database. input Schedule
consists plan name schedule description, one shown above. operator
produces single tuple output indicates assigned task ID scheduled task.
Unschedule removes scheduled plan task database. Unschedule used
plan remove monitoring activity often used tandem notification
operator. example, plan monitor set available houses market entire
month September, send email end month user containing results,
unschedule execution, schedule new plan (perhaps, example, clean
database stored monitoring data). input Unschedule task ID
scheduled plan output tuple indicating success failure attempt remove
plan task database.
4.4 Subplans
promote reusability, modularity, capability recursion, plan language supports
notion subplans. Recall plans named, consist set input output
streams, set operators. consider series operators amounts complex
function input data, plans present interface operators. particular,
using earlier definitions, possible Opi = P OpIn = PlanIn, OpOut = PlanOut,
OpWait = , OpEnable = , Func = {Op1, ..., Opn}. Thus, plan referenced within
another plan operator. execution, subplan called like
operator would inputs subplan arrive, executed within body subplan
operators subplan. example, consider Example_plan, introduced
earlier, referenced another plan called Parent_plan. Figure 8 illustrates text
form Parent_plan treats Example_plan merely another operator.

642

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

PLAN parent_plan
{
INPUT: w, x
OUTPUT: z
BODY
{
Op6 (w : y)
example_plan (x, : z)
}
}
Figure 8: Text parent_plan

Subplans encourage modularity re-use. written, plan used operator
number future plans. Complicated manipulations data thus abstracted away,
making plan construction simpler efficient.
example, one could develop simple subplan called Persistent_diff, shown Figure 9,
uses existing operators DbQuery, Minus, Null, DbAppend take relation,
compare named relation stored local database. plan determines
update, appends result, returns difference. Many types monitoring style plans
operate updated results incorporate subplan existing plan. Homeseekers
plan could subplan returns house details given set search parameters.
MINUS

relation

diff

DBQUERY

NULL

DBAPPEND

Figure 9: Persistent_diff subplan

4.4.1. RECURSION
addition promoting modularity re-use, subplans make another form control flow
possible: recursion. Recursive execution useful number scenarios related Web
query processing. describe two: reformulating data integration queries iterating
Next Page links.
One application recursion THESEUS involves reformulating data integration queries.
example, efficient version Duschkas Inverse Rules algorithm (Duschka 1997)
implemented using recursive streaming dataflow execution THESEUS (Thakkar & Knoblock,
2003). Support recursion query reformulation allowed Thakkar Knoblock develop
system produced complete answers query reformulation algorithms,
MiniCon (Pottinger & Levy, 2001), support recursion.
Another practical use recursion Web data integration involves iterating list
described multiple documents. described earlier, number online information
gathering tasks require sort looping-style (repeat until) control flow. Results
single query spanned across multiple Web pages. Recursion provides elegant way
address type interleaved information gathering navigation streaming dataflow
environment.
example, processing results search engine query, automated information
gathering system needs collect results page, follow "next page" link, collect
results next page, collect "next page" link page, runs
"next page" links. express von Neumann style programming language,
Do...While loop might used accomplish task. However, implementing types loops

643

fiBARISH & KNOBLOCK

dataflow environment problematic requires cycles within plan. leads
data one loop iteration possibly colliding data different iteration. practice,
loops dataflow graphs require fair amount synchronization additional operators.
Instead, problem solved simply recursion. use subplan reference
means repeat body functionality use Null operator
test, exit condition. resulting simplicity lack synchronization complexity makes
recursion elegant solution addressing cases navigation interleaved retrieval
number iterations looping style information gathering known
runtime. example recursion used, consider abstract plan processing
results search engine query. higher level plan called Query_search_engine, shown
Figure 10a, posts initial query search engine retrieves initial results.
processes results subplan called Gather_and_follow, shown Figure 10b. search
results go Union operator next link eventually used call
Gather_and_follow recursively. results recursive call combined Union
operator first flow.
WRAPPER

search term

GATHER_AND_FOLLOW

initial-result

web pages

Figure 10a: Query_search_engine plan

false

urls

DISTINCT
ne xt -pag e -link

WRAPPER
ne xt-re sults

GATHER_AND_FOLLOW

true

NULL

PROJECT

UNION

url

Figure 10b: Gather_and_follow recursive subplan

4.4.2. REVISITING EXAMPLE
Let us revisit earlier house search example see plan would expressed
proposed plan language. Figure 11a shows one two plans, Get_houses, required
implement abstract real estate plan Figure 3. Get_houses calls subplan Get_urls shown
Figure 11b, nearly identical plan Gather_and_follow, described above. rest
Get_houses works follows:
(a) Wrapper operator fetches initial set houses link next page (if any)
passes Get_urls recursive subplan.
(b) Minus operator determines houses distinct previously seen;
new houses appended persistent store.
(c) Another Wrapper operator investigates detail link house full
set criteria (including picture) returned.
(d) Using details, Select operator filters meet specified search
criteria.
(e) result e-mailed user.

644

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

criteria

PROJECT

WRAPPER

price -inf

house -urls

GET_URLS

WRAPPER

SELECT

raw-ho use -de tails

co nd

FORMAT
"be ds = %s"

Figure 11a: Get_houses plan
WRAPPER
false
urls

DISTINCT
ne xt -p ag e -link

NULL

GET_URLS

next -pag e -link

true

PROJECT

UNION

ho use -url

Figure 11b: Get_urls recursive subplan

5. Efficient Plan Execution Architecture
definition, Web information gathering involves processing data gathered remote sources.
execution information gathering plan, often case multiple
independent requests made different sets remote data. data
independently processed series operations combined output. Network
latencies, bandwidth limitations, slow Web sites, queries yield large result sets
dramatically curtail execution performance information gathering plans. especially
case plan operators executed serially: one issues mentioned
bottleneck execution entire plan.
efficiency standpoint, two problems standard von Neumann execution
information gathering plans. One exploit independence data flows
common plan multiple unrelated requests remote data cannot parallelized. plan
language designed addresses problem somewhat allowing plans expressed
terms minimal data dependencies: still, dictate operators
actually executed.
second efficiency problem von Neumann execution exploit
independence tuples common relation: example, large data set
progressively retrieved remote source, tuples already retrieved could
conceivably operated successive operators plan. often reasonable, since
CPU local system often under-utilized remote data fetched.
remedy problems, designed streaming dataflow execution system
software agent plans. system allows maximum degree operator data parallelism
potentially realized runtime, executing multiple operators concurrently pipelining
data operators throughout execution. network query engines implemented
designs bear similarity present below. However, discussion
extends existing work three ways:


describe details execution (i.e., threads interact firing rules
work). exception (Shah, Madden, Franklin, & Hellerstein, 2001),
able locate similar discussion details execution
systems.

645

fiBARISH & KNOBLOCK




present novel thread-pooling approach execution, multiple threads
shared operators plan. allows significant parallelism without exhausting
resources.
describe recursive streaming dataflow execution implemented using data
coloring approach.

5.1 Dataflow Executor
plan language allows dataflow-style plans coded text, specify
actual execution process works. Thus, complement language efficiently execute
plans, developed true dataflow-style executor. executor allows plans realize
parallelization opportunities independent flows data, thus enabling greater horizontal
parallelism runtime.
executor functions virtual threaded dataflow machine. assigns user-level threads
execute operators ready fire. type execution said virtual dataflow
thread creation assignment done natively CPU, even kernel space
operating system, application program (the executor) running user space.
using threads parallelize execution plan, executor realize better degrees true
parallelism, even single CPU machines. use threads reduces impact
I/O penalties caused currently executing operator. is, multiple threads reduce
effect vertical waste occur single-threaded execution reaches operation
blocks I/O.
example, consider case plan containing two independent Wrapper operators
executed machine single CPU. Suppose Wrapper operators
input fire. operators assigned distinct threads. single CPU execute
code issues network request first Wrapper operator, wait data
returned, finish issuing network request second Wrapper operator. Thus,
matter microseconds, operators issued requests (which typically take
order hundreds milliseconds complete) retrieval data (on remote sites)
parallelized. Thus, overall execution time equal slower
two requests complete. contrasts execution time required serial execution,
equal sum time required request.
5.1.1. PROMOTING BOUNDING PARALLELISM THREAD POOLS
using threaded dataflow benefits, past research dataflow computing operating
systems shown cases parallelism must throttled overhead
thread management (i.e., creation destruction threads) overly taxing.
example, threads created whenever operator ready, cost create add
significant overhead. Also, significant parallelism execution, number
threads employed might result context switching costs outweigh parallelism benefits.
address issues, developed thread pooling architecture allows executor
realize significant parallelism opportunities within fixed bounds.
start plan execution, finite number threads created (this number easily
adjustable external configuration file) arranged thread pool. threads
created, execution begins. data becomes available (either via input
operator production), thread pool assigned execute method consuming
operator data. time operator produces output, hands output zero
threads consumer(s), any, process output. pool contain
available threads, output queued spillover work queue, picked later
threads return queue. behavior occurs operator input events.
Thus, parallelism ensured existence multiple threads pool bounded
latter case, degree true parallelism execution never exceed pool

646

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

size. Demands parallelism beyond number threads pool handled work
queue.
Figure 12 illustrates details thread pool used executor runtime.
figure shows four key parts executor:
thread pool: collection threads ready process input collected
queue. single thread pool partitioned certain
sources guaranteed number threads available operators query
sources. available threads wait new objects queue. Typically, contention
queue machines single CPU issue (even hundreds
threads). However, configuration options exist multiple work queues
created thread pool partitioned across queues.


spillover work queue: data received externally transmitted internally (i.e.,
result operator execution) cannot immediately assigned available
thread collected queue. threads return pool, check
objects queue: are, process them, otherwise thread waits
activated future input. queue asynchronous FIFO queue implemented
circular buffer. queue full, grows incrementally needed. initial
size queue configurable. structure queue element described detail
below.



routing table: data structure describes dataflow plan graph terms
producer/consumer operator method associations. example, Select operator
produces data consumed Project operator, data structure marshals output
Select associated Project input method consume
data. table computed prior execution performance
operator-to-operator I/O impacted runtime repetitive lookups consumers
producers. Instead, pre-computation allows data structure associated
producing method immediately route output data proper set consuming input
methods.

Runtime plan
internal data structure
Operator obje cts

4

3

1
Plan
Input

Routing table

Thre ad
Pool

2a

Thre ads
available ?

Plan
Output

2b

Spillov er
wor k queu e

5

Figure 12: Detailed design executor

647

fiBARISH & KNOBLOCK



set operator objects: collection operator classes (including
input/output methods state data structures). exists one operator object per
instance plan.

queue object consists tuple describes:





session ID
iteration ID
content (i.e., data)
destination operator interface (i.e., function pointer).

session ID used distinguish independent sessions iteration ID distinguish
current call-graph level session, ensures safety concurrent re-entrancy
runtime. IDs provide unique key indexing operator state information. example,
recursive execution, IDs ensure concurrent firing operator
different levels call graph co-mingle state information. Finally, destination
operator interface pointer code thread assigned queue object run.
runtime, system works follows. Initial plan input arrives assigned threads
thread pool (#1 Figure 12), one thread input tuple (#2a), threads
available data added spillover work queue (#2b). assigned thread pool
takes queue object and, based description target, fetches appropriate operator
object execute proper function data (#3). execution
operator, state information previous firings may accessed using (session ID, iteration
ID) pair key. result operator firing may result output. does, operator
uses routing table (#4) determine set consumers output. composes new
data queue objects consumer hands objects (#5) either available
thread thread pool (#2a) deposits work queue (#2b) threads
available. reduce memory demands, producers deep-copy data produce
multiple consumers. Finally, operators produce plan output data route data
plan becomes available.
5.2 Data Streaming
logical level, variables plan language describe relations. However,
provide parallelism thus efficiency runtime, tuples common relation
streamed operators. stream consists stream elements (the tuples relation),
followed end stream (EOS) marker. Thus, communicating relation
producer consumer, producing operators communicate individual tuples consumer operators
follow final tuple EOS token.
Streaming relations operators increases degree vertical parallelism
plan execution. revisiting firing rule described earlier, clarify read:
operator may fire upon receipt input tuple, providing received
first tuple wait variables.
Thus, operator receives single tuple inputs, consume process
tuple. Afterwards, potentially emit output that, turn, consumed
downstream operator output plan. resulting parallelism vertical sense
two operators (e.g., one producer one consumers) concurrently
operate relation data. Remote sources return significant amounts data
efficiently processed streaming, since operator receives network
transmission pass along data processing becomes available rest
data received.

648

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

Support kind streaming implies state must kept operators firings.
operation performed logically entire relation, even though
physically involves tuple relation. operator maintain state
firings, cannot necessarily produce correct results. example, consider set-theoretic
Minus operator takes two inputs, lhs rhs, outputs result lhs - rhs.
operator begin emitting output soon received rhs EOS token. However,
operator must still keep track rhs data receives EOS both; not, may emit
result later found incorrect. see could happen, suppose order
input received instance Minus operator was:
lhs: (Dell)
lhs: (Gateway)
rhs: (HP)
rhs: (Gateway)
rhs: EOS
lhs: (HP)
lhs: EOS
correct output, lhs-rhs,
lhs-rhs: (Dell)
lhs-rhs: EOS
However, achieved waiting EOS emitting output also
keeping track (i.e., maintaining state) inputs. example, lhs data retained,
rhs instance (HP) would memory lhs instance (HP) occurred
tuple would incorrectly emitted.
summary, streaming technique improves efficiency operator I/O
increasing degree vertical parallelism possible runtime. allowing producers
emit tuples soon possible forcing wait consumers receive
producers consumers work fast able. main tradeoff increased
memory, queue required facilitate streaming state needs
maintained firings.
5.2.1. RECURSIVE STREAMING: SIMPLICITY + EFFICIENCY
Streaming complement simplicity many types recursive plans highly efficient
execution. Looping theoretical dataflow systems non-trivial desire singleassignment need synchronization loop iterations. Streaming
complicates this: data different loop iterations collide, requiring mechanism
color data iteration. result, looping becomes even difficult challenge.
address problem, use data coloring approach. time data enters flow,
given session value iteration value (initially 0). Upon re-entrancy, iteration value
incremented. leaving re-entrant module, iteration value decremented. new
value equal 0, flow routed recursive module; otherwise, data flow
continues unravel iteration value 0. tail-recursive situations, system
optimizes process simply decrements iteration value 0 immediately exits
recursive module. two pronged data-coloring approach, similar strategies used
dataflow computing literature, maintains property single assignment level
call graph. Streaming easily fits model without changes. result, many
levels call graph active parallel effectively parallelizing loop.

649

fiBARISH & KNOBLOCK

see works, return Get_houses example Figures 11a 11b.
input tuple arrives, initial page houses fetched. happens, Next link
followed parallel projecting house URLs Union operator
Minus operator. Since Union operator emit results immediately, Minus
operator inputs, data flow continues next Wrapper operator,
queries URL extracts details house. Thus, details houses
first page queried parallel following Next link, exists. Data
next page extracted parallel following Next link second page
on. Meanwhile, results Get_urls subplan (the house URLs) streamed back
first level plan, Union operator. continue details
gathered parallel.
short, recursive streaming powerful capability made possible combination
expressivity THESEUS plan language efficient execution system. result
allows one write plans gather, extract, process data soon possible even
logical set results distributed collection pages (a common case Internet).

6. Experimental Results
demonstrate contributions paper, conducted set experiments highlight
increased expressivity efficient execution supported plan language execution
system. method consists verifying three hypotheses fundamental claims:
Hypothesis 1:

streaming dataflow plan execution system ensures faster plan
execution times possible von Neumann nonstreaming dataflow execution.

Hypothesis 2:

agent plan language described supports plans cannot
represented query languages network query engines.

Hypothesis 3:

additional expressivity permitted plan language described
result increased plan execution time.

brief introduction implemented system used experiments, rest
section divided three subsections, focuses verifying
hypotheses.
6.1 THESEUS Information Gathering System
implemented approach described paper system called THESEUS. THESEUS
written entirely Java (approximately 15,000 lines code) thus runs operating
system Java virtual machine (JVM) ported. ran experiments
described Intel Pentium III 833MHz machine 256MB RAM, running Windows
2000 Professional Edition, using JVM API provided Sun Microsystems Java Standard
Edition (JSE), version 1.4. machine connected Internet via 10Mbps Ethernet
network card.
6.2 Hypothesis 1: Streaming Dataflow Ensures Fast Information Agents
support first hypothesis, measured efficiency Homeseekers information
agent. experiments show without parallelism features plan language
execution system, agents Homeseekers would take significantly longer execute.
graphical plan Homeseekers shown Figures 11a 11b. Note
plan monitor Homeseekers (we get next section), simply
gathers data Web site. textual plans required simply translations

650

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

Figures 11a 11b using plan language described paper. textual form
Get_houses plan shown Figure 13a textual form Get_urls plan shown
Figure 13b.
demonstrate efficiency streaming dataflow provides, ran Homeseekers
Get_houses plan three different configurations THESEUS. first configuration (D-)
consisted thread pool one thread effectively preventing true multi-threaded dataflow
execution also makes streaming irrelevant. resulting execution thus similar
case plan programmed directly (without threads) using language like Java
C++. second THESEUS configuration (D+S-) used multiple threads dataflow-style
processing, stream data operators. Finally, third configuration (D+S+)
consisted running THESEUS normal streaming dataflow mode, enabling types
parallelism. D+S- D+S+ cases, number threads set 15.
Note configurations done purposes running experiments.
practice, THESEUS runs one configuration: streaming dataflow n threads thread
pool (n typically set 10 20). one wants modify number threads
pool need alter configuration file. rarely necessary.
ran configuration three times (interleaved, negate temporary benefits
network source availability) averaged measurements three runs. search
constraints consisted finding houses Irvine, CA priced $500,000
$550,000. query returned 72 results (tuples), spread across 12 pages (6 results per page).
Figure 14 shows average performance results three configurations terms
time took obtain first tuple (beginning output) time took obtain last
tuple (end output). series unpaired t-tests measurements indicates
PLAN get_houses
{
INPUT: criteria
OUTPUT: filtered-house-details
BODY
{
project (criteria, price-range, price-info)
format (beds = %s, beds : bed-info)
wrapper (initial, price-info : result-page-info)
get_urls (house_urls : all-house-urls)
wrapper (detail, all-house-urls : all-house-details)
select (raw-house-details, bed-info : filtered-house-details)
}
}
Figure 13a: Text Get_houses plan
PLAN get_urls
{
INPUT: result-page-info
OUTPUT: combined-urls
BODY
{
project(result-page-info, house-url : curr-urls)
distinct(result-page-info, next-page-link : next-status)
null (next-status, next-status, next-status : next-page-info, next-urls)
wrapper (result-page, next-page-info : next-urls)
union ( curr-urls, next-urls : combined-urls)
}
}
Figure 13b: Text Get_urls plan

651

fiBARISH & KNOBLOCK

statistically significant 0.05 level.4
time first tuple important shows earliest time data becomes
available. Callers information agent plan often interested early results come
back, especially substantial amount data returned time tuples great,
since allows processing results begin soon possible. time last tuple also
important metric associated time data returned.
Callers plan require entire set results, caller executes aggregate
function data, thus interested measurement.
Figure 14 shows, parallelism provided streaming dataflow significant impact.
Typical von Neumann style execution, (D-), cannot leverage opportunities
parallelism suffers heavily cumulative effects I/O delays. D+S- fares
better concurrent I/O requests issued parallel, inability stream data
throughout plan prevents result pages queried parallel. Also,
lack streaming, results obtained early execution (i.e., first tuple) cannot
communicated last tuple ready. Note D+S- case reflects performance
provided plan executed robot plan execution systems like RAPs PRS-LITE,
support operational (horizontal) parallelism data (vertical) parallelism.
Finally, D+S+ case shows streaming alleviate problems, allowing first
tuple output soon possible, supporting ability query result pages
parallel (and process detail pages soon possible, parallel). short, Figure 14 shows
streaming dataflow efficient execution paradigm I/O-bound Web-information
gathering plans require interleaved navigation gathering.
also sought compare execution performance Get_houses plan
performance achieved using another type information gathering system,
network query engine. However, since systems support ability express loops
recursive information gathering, possible simply run plan
executors. address this, calculated theoretical performance network query engine
supported streaming dataflow, ability loop result pages.
solve type challenge sites like Homeseekers pose, systems would need
gather data one result page time. Note loops recursion systems
possible (i.e., possible gather data spread across set pages parallel), given

80000

Time (ms)

70000
60000
50000

D-

40000

D+S-

30000
20000

D+S+

10000
0
First tuple

Last tuple

Figure 14: Performance benefits streaming dataflow
4

Two-tailed P results D+S vs. D+S- D+S- vs. D- time-to-first-tuple cases 0.0001
0.0024 respectively. Two-tailed P results D+S+ vs D+S- D+S- vs. D- time-to-last tuple cases
0.0001 0.0026, respectively:

652

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

type intermediate plan language support, still used drill
details particular result (i.e., gather data set pages) parallel. Thus, network
query engine could leverage dataflow streaming capabilities process single page,
could used parallelize information gathering set linked result pages.
page (and details) would processed one time.
simulate behavior, used THESEUS extract house URLs details one page
time, twelve pages results obtained initial query. average time
required gather details six housing results 3204 ms. Note time
retrieve first detailed result THESEUS D+S+ case: 1852ms. take
time extract six detailed results multiply number pages query (12),
get time last tuple equal (3204 * 12) = 38448ms. Figure 15 shows results
compare D+S+ case THESEUS.
Thus, ad-hoc solution using network query engine could allow first tuple
results returned fast THESEUS inability Next links navigated
immediately would result less loop parallelism and, result, would lead slower
production last tuple data. Therefore, network query engines could used
gather results spread across multiple hyperlinked web pages, inability natively support
mechanism looping negates potential streaming parallelize looping
process.
summary, verify first hypothesis, described expressivity plan
language presented enables complex queries (like Homeseekers) answered efficiently.
results apply Homeseekers, type site reports result list
series hyperlinked pages.
6.3 Hypothesis 2: Better Plan Language Expressivity
support second hypothesis, investigated complex task monitoring
Homeseekers could accomplished using approach versus existing Web query systems.
previously described monitoring cases would useful searching
house process requires weeks, months executing kind query. Thus,
corresponding information gathering plan would query Homeseekers per day send
newly found matches end user e-mail. Again, type problem general
often desirable able monitor many Internet sites produce lists results. However,
requires support plans capable expressing monitoring task, persistence
monitoring data, ability notify users asynchronously.
plan monitor Homeseekers shown Figure 16. plan shown
Figure 13a, additional modifications. particular, uses two database operators
(DbImport DbAppend) integrate local commercial database system persistence
45000

Time (ms)

40000
35000

Theoretical netw ork
query engine

30000

Theseus D+S+

25000
20000
15000
10000
5000
0

First tuple

Last tuple

Figure 15: Comparison hypothetical network query engine
653

fiBARISH & KNOBLOCK

DB-IMPORT

DBAPPEND

ho use s-se e n

criteria

PROJECT

WRAPPER

price -inf

ho use -u rls

GET_URLS

MINUS

WRAPPER

SELECT

raw-hou se -de ails

co nd

EMAIL

FORMAT
"b e ds = %s"

Figure 16: Modifying Homeseekers support monitoring requirements

results. allows future queries return new results stored past results. Notice
initial DbImport triggered synchronization variable. plan also communicates new
results asynchronously users via Email operator.
measure expressivity, consider comparison plan Figure 16
capable produced TELEGRAPH NIAGARA network query engines.
comparison focuses TELEGRAPHCQ (Chandrasekaran, Cooper, Deshpande, Franklin,
Hellerstein, Hong, Krishnamurthy, Madden, Raman, Reiss, & Shah, 2003) NIAGARACQ
(Chen, DeWitt, Tian, & Wang, 2000), modifications original systems
support continuous queries monitoring streaming data sources.
Since
TELEGRAPHCQ NIAGARACQ query languages similar, present detailed
comparison former general comparison latter.
CQ systems allow continuous Select-Project-Join (SPJ) queries expressed.
TELEGRAPHCQ provides SQL-like language extensions expressing operations
windows streaming data. Specifically, language allows one express SPJ style queries
streaming data also includes support loop constructs allow frequency
querying streams. example, treat Homeseekers streaming data source
query per day (for 10 days) houses Manhattan Beach, CA, less
$800,000:
Select street_address, num_rooms, price
Homeseekers
price < 800000 city = Manhattan Beach state = CA
(t=ST; t<ST+10; t++) {
WindowIs(Homeseekers, t-1, t)
}

NIAGARACQ also allows complicated operations, Email, accomplished
calling function declared stored procedure language. format NIAGARACQ
query is:
CREATE CQ_name
XML-QL query
action
{START s_time} {EVERY time_interval} {EXPIRE e_time}

example, query would XML-QL equivalent selecting house information
met query criteria.
action part would something similar
MailTo:user@example.com.
Generally, query language limitations comes flexible
monitoring sources, limitations THESEUS have. First, ability
interleave gathering data navigation (in fact, NIAGARACQ assumes Homeseekers
queried XML source provides single set XML data). Second,
support actions (like e-mail) based differentials data monitored period
time. Although allow one write stored procedure could accomplish action,

654

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

requires separate programming task execution efficient rest query
(this could issue complicated intensive CPU I/O-bound activities per
tuple). Finally, CQ systems, way terminate query
temporal constraints.
6.4 Hypothesis 3: Increased Expressivity Increase Execution Time
Though demonstrated THESEUS performs well complex information
gathering tasks, useful assess whether increased expressivity THESEUS impacts
performance simpler tasks particular, ones network query engines typically process.
this, explored performance THESEUS traditional, database style query
plan online information gathering compared type plan executed
network query engine.
chose single, common type SPJ query involved multiple data sources serve
basis comparison. canonical data integration query. claim
understanding THESEUS compares network query engine respect
performance SPJ query heart efficiency comparison two types
systems. Since types systems execute dataflow-style plans pipelined fashion,
theoretical performance expected differences would due
implementation environment biases (e.g., different LAN infrastructures). Nevertheless,
support efficiency claim, felt important use concrete SPJ query comparison.
experiment, chose reproduce query paper another network query
engine Telegraph. measure performance partial results query processing
technique, Raman Hellerstein ran query gathered data three sources
joined together (Raman & Hellerstein, 2002). specific query involved gathering
information contributors 2000 U.S. Presidential campaign, combined
information neighborhood demographic information crime index information. Table 2
lists sources data provide. Bulk scannable sources data
extracted read directly (i.e., exists static Web page file). Index sources
provide answers based queries via Web forms. Index sources thus sources
require binding patterns. Table 3 shows query used evaluate
performance TELEGRAPH.
important note Raman Hellerstein measured performance query
Table 3 standard pipelined mode compared JuggleEddy partial results
approach. interested results former, measure well
unoptimized network query engine call baseline gathers data
processing traditional, database-style query. optimization, JuggleEddy,
complementary system described here. Since types systems rely streaming
dataflow execution consisting tuples routed iterative-style query operators, would
difficult extend THESEUS support types adaptive query processing
techniques.
Source
FEC

Yahoo
Real
Estate
Crime

Site
www.fec.gov

Type data
Bulk scannable source provides information
(including zip code) contributor candidate
2000 Presidential campaign.
realestsate.yahoo.com Index source returns neighborhood demographic
information particular zip code.
www.apbnews.com

Index source returns crime level ratings
particular zip code.

Table 2: Sources used FEC-Yahoo-Crime query

655

fiBARISH & KNOBLOCK

Query
SELECT F.Name, C.Crime, Y.income
FEC F, Crime C, Yahoo
F.zip = Y.zip F.zip = C.zip
Table 3: SQL query associates crime income statistics
political campaign contributions

wrote simple THESEUS plan allowed query Table 3 executed. used
exactly sources, except found latency Crime source increased
substantially, compared times recorded Raman Hellerstein. Instead, used
another source (Yahoo Real Estate) added artificial delay tuple processed
source, new source performed similarly. Raman Hellersteins results show
performance pipeline plan slow Crime source, 250ms per
tuple. match this, added 150ms delay tuple processing new source,
Yahoo, normally fetching data 100ms per tuple. results shown
Figure 17.
results show THESEUS able execute plan least fast
baseline TELEGRAPH plan, non-optimized result shown Figure 8 paper
Raman Hellerstein, THESEUS execution efficient depending number
threads thread pool. example, THESEUS-3 describes case THESEUS
thread pool contains 3 threads. result run performs slightly worse
TELEGRAPH baseline minor differences could due changes source behavior
different proximities network sources. However, running THESEUS threads
thread pool (i.e., THESEUS-6 THESEUS-10) shows much better performance.
degree vertical parallelism demanded execution better accommodated
threads. noted reason TELEGRAPH perform well
THESEUS-6 THESEUS-10 likely system assigned single thread
operator (Raman 2002). is, THESEUS-6 THESEUS-10 execution involves 6 10
concurrent threads, respectively, whereas TELEGRAPH plan uses 3 concurrent threads.

7. Related Work

Theseus-10 Theseus-6

200000
180000
160000
140000
120000
100000
80000
60000
40000
20000
0

Telegraph

0
12
00

0
10
00

80
00

60
00

40
00

Theseus-3

20
00

0

Cell updates

language system discussed paper relevant efforts focus agent
execution querying Web data. understand work presented context
approaches, consider past work software agent execution, robot agent execution,
network query engines. first area relevant, software agent systems

Time (seconds)

Figure 17: Comparing THESEUS TELEGRAPH baseline (FEC-Yahoo-Crime)
656

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

historically addressed expressivity issues and, recent years, also attempted address
efficiency issues. Robot plan executors represent slightly greater contrast
less experience processing large amounts data. hand, network query engines
explored large-scale remote data processing, though plan/query expressivity tends
quite narrow.
7.1 Software Agent Execution Systems
Internet Softbot (Eztioni & Weld, 1994) software agent automates various
information processing tasks, including UNIX command processing Web information
gathering. support execution incomplete information world, system
interleaves planning execution. XII (Golden et al., 1994) later Puccini (Golden
1998) planners generate partially-ordered plans effects action
known execution verified execution. Softbot makes
clear distinction information goals satisfaction goals, specifically address
need efficiently flexibly handle information processed. example, system
support kind parallel processing information (to capitalize I/O-bound
nature execution). terms expressivity, XII Puccini allow universal
quantification expressed (i.e. iteration possible), requires set
iterated known advance. pointed earlier example Next Page
links, always case set next pages processed discovered
iterating indeterminate, do..while fashion. contrast, although
interleave planning execution, system described support expressive
plan language capable handling next-link type processing, well streaming dataflow
model execution enables efficient large scale information processing. great extent,
contributions research efforts viewed complementary.
research, INFOSLEUTH (Bayardo et al., 1997) recognized importance
concurrent task/action execution, close spirit true dataflow computing.
time, work generally investigated impact streaming combined dataflow.
INFOSLEUTH describes collection agents that, combined working together, present
cohesive view data integration across multiple heterogeneous sources. INFOSLEUTH
centralizes execution Task Execution Agent, coordinates high-level information
gathering subtasks necessary fulfill user queries routing appropriate queries resources
accommodate queries. Task Execution Agent data-driven and, thus, task
fulfillment proceeds dataflow-style manner. addition, multi-threading architecture
supports concurrent, asynchronous communication agents. However, streaming
component exist fact, INFOSLEUTH intends large scale information
processing, specifically notes limitations KQML (the basis message transport
agents) streaming feasible time implementation.
INFOSLEUTH THESEUS similar desire support efficient, large-scale information
processing. However, THESEUS supports streaming operators, well
expressive plan language, capable support complex types plans, including support
conditionals recursion.
contrast INFOSLEUTH, BIG (Lesser, Horling, Klassner, Raja, Wagner, & Zhang, 2000)
general software agent separates components agent planning, scheduling,
execution (among components). BIG agents execute plans based tasks modeled
TMS modeling language. execution, BIG reasons resource tradeoffs attempts
parallelize non-local requests (such Web requests), least terms actions
scheduled. terms expressivity, TMS include support conditionals
looping constructs (see DECAF, below), unlike system described paper. terms
execution, BIG may perform operations concurrently, execute pure
dataflow manner: instead, parallelizes certain operations, based whether

657

fiBARISH & KNOBLOCK

blocking. significantly reduces additional opportunities dataflow-style parallelism.
example, possible parallelize CPU-bound operations (desirable hyperthreaded
processors multi-CPU machines) possible leverage additional I/O-bound parallelism
two different instruction flows. example latter, consider plan uses
common input data query set sources, performing different computations input data
(e.g., form query) remote request. Since I/O-bound operations
parallelized, way execute flows simultaneously, even though flows
eventually end I/O-bound second larger difference BIG THESEUS
latter supports capability stream data operators, maximizing degree
vertical parallelism possible, former not. shown, better vertical
parallelism execution yield significant performance gains.
RETSINA (Sycara et al., 2003) general, multi-agent system attempts automate
wide range tasks, including information processing. RETSINA unique attempts
interleave planning execution (as XII Internet Softbot), also
information gathering. RETSINA agent composed four modules: communication,
planning, scheduling, execution monitoring. modules run separate threads,
communication, planning scheduling occur information gathering (which often
I/O-bound). addition, multiple actions RETSINA executed concurrently,
dataflow-style manner, separate threads. execution, actions communicate
information one another via provision/outcome links (Williamson, Decker, & Sycara,
1996), similar notion operator input output variables described
here. dataflow aspect agent execution RETSINA similar THESEUS,
plan language less expressive (no support conditionals kind looping, including
indeterminate) execution support streaming.
DECAF (Graham et al., 2003) extension RETSINA TMS task language
support agent plans contain if-then-else looping constructs. addition, DECAF
incorporates advanced notion task scheduling views mode operation
analogous operating system example, execution, concerned
number I/O-bound CPU-bound tasks one time, optimize task scheduling.
DECAF employs expressive task language, closer supported THESEUS,
support streaming execution. Again, shown, benefits
increased vertical parallelism streaming make significant difference
processing large amounts data working slow, remote sources, case
common environments like Web. fact, shown going beyond dataflow
limit (maximum vertical horizontal parallelism) though techniques speculative
execution (Barish & Knoblock, 2002; Barish & Knoblock, 2003) yield even greater
performance benefits. Streaming simple feature add execution system; way
operators execute must change (i.e., become iterators), end-of-stream ordering must
managed care, support operator state management needed, addition related
challenges.
7.2 Robot Agent Execution Systems
work THESEUS also related past work robot agent execution systems. main
similarity emphasis providing plan language execution system agents.
main difference, however, robot agent execution systems built primarily robots,
act physical world, lack support critical features software
agents like Web information agents require. discussing specifics, focus two well-known
robot agent executors: RAP system (Firby 1994) PRS-LITE (Myers 1996).
RAP PRS-LITE offer general plan languages execution systems support
concurrent execution actions. Like expressive plan languages, RPL (McDermott
1991), RAP PRS-LITE also support additional action synchronization WAIT-

658

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

clause, triggers action particular signal received. similar
use WAIT ENABLE THESEUS plan language. PRS-LITE supports even greater
expressivity, including notion sequencing goals, enable conditional goals well
parallel sequential goal execution. example, PRS-LITE supports SPLIT
modalities two different ways specify parallel goal execution, former decoupled
parent task latter tightly coupled.
Despite expressivity supported RAPs PRS-LITE, clear plan languages
primarily meant handle needs robots. example, operator execution involves
processing signals, streams tuples, operators. contrast, THESEUS
language executor built stream potentially large amounts relational data.
plan like Homeseekers executed RAPs PRS-LITE, lack streaming would result
significantly worse performance make poor use available resources. say
RAPs PRS-LITE contain design flaws: rather, systems simply better facilitate needs
robots process small amounts local data (such target presence location
information) perform actions physical world. contrast, Web information agents
act physical objects, software objects, Web sites, need deal
problems associated unreliable remote I/O potentially large amounts data.
Streaming thus critical feature agents, allows much faster performance
local resources, CPU, better utilized.
Another significant difference language presented RAPs
PRS-LITE support recursion. understandable robot agent execution systems
lack feature none primary tasks require control flow. fact, neither
PRS-LITE RAP supports kind looping mechanism. contrast, looping often required
Web information agents, frequently need gather logical set data distributed
across indeterminate number pages connected Next page links. Recursive
streaming enables high-performance looping dataflow environment without kind
complicated synchronization.
cannot understated features like streaming recursion make significant
difference terms agent performance. example, execution Homeseekers without
recursive streaming would fare better D+S- example Section 6, performed
much worse D+S+ case.
7.3 Network Query Engines
Network query engines TUKWILA (Ives et al., 1999), TELEGRAPH (Hellerstein et al.,
2000) NIAGARA (Naughton et al., 2001) focused primarily efficient adaptive
execution (Avnur & Hellerstein 2000; Ives et al., 2000; Shanmugasundaram et al., 2000; Raman
& Hellerstein 2002), processing XML data (Ives et al., 2001), continuous queries
(Chen et al., 2000; Chandrasekaran et al., 2003). systems take queries users,
form query plans, execute plans set remote data sources incoming streams.
THESEUS, network query engines rely streaming dataflow efficient, parallel
processing remote data.
work described differs network query engines two ways. first,
important difference, plan language. Plans network query engines consist
mostly relational-style operators required additional adaptive XML-style
processing. example, TUKWILA includes double pipelined hash join dynamic collector
operators adaptive execution (Ives et al., 1999), x-scan web-join operators facilitate
streaming XML data binding tuples. TELEGRAPH contains Eddy operator (Avnur &
Hellerstein 2000) dynamic tuple routing SteMs operator leverage benefits
competing sources access methods. NIAGARA contains Nest operator XML
processing operators managing partial results (Shanmugasundaram et al., 2000).
Outside special operators adaptive execution XML processing, plans network

659

fiBARISH & KNOBLOCK

query engines look similar database style query plans. plans also inaccessible
users alter queries generate plans, plans themselves.
contrast, plan language presented expressive agent plans
accessible. Like network query engines, language described includes relational-style
operators processing XML data. However, also includes operators support
conditional execution, interaction local databases, asynchronous notification, userdefined single-row aggregate functions. plan language developed also supports
subplans modularity, re-use, recursive execution looping-style information gathering.
contrast, network query engines support kinds constructs. result,
systems cannot represent interleaved navigation gathering required tasks
Homeseekers example. Consider Telegraph approach handling Next Page links.
logic iterating set Next Page type links located wrapper itself, separate
query plan5. simplifies wrappers somewhat (each wrapper returns
data particular site), limits flexibility describing gather remote data.
example, one develops Google wrapper Telegraph gathers results search (over
several pages), easy way express requirement stop 10 pages stop
5 links site extracted. short, since logic dealing
Next Page type links decoupled plan, expressivity limited.
addition, build wrapper handles Next Page links Telegraph, one must write custom
Java class referenced engine runtime. contrast, THESEUS language
handle interleaved navigation gathering using recursion loop set Next Page
links, streaming tuples back system extracted, immediate postprocessing conditional checks (i.e., know stop gathering results).
final difference worth noting accessibility. contrast network query
engines, plans language described accessible user. Although
generated query processors (Thakkar et al., 2003) types applications (Tuchinda &
Knoblock, 2004), like plans produced network query engines, also constructed
modified using text editor. provides ability users specify complicated
plans could otherwise represented query. network query engines,
NIAGARACQ (Chen et al., 2000) support means specifying complicated
types actions associated continuous queries, support native system
thus possible execute complex actions middle queries (such actions need
occur certain times, example certain events occur). example, NIAGARACQ
requires one specify actions stored procedure language, introducing barrier (query plan
stored procedure) exist system. Furthermore, logic separate
query plan (i.e., integrated query plan operators) execute
condition met.

8. Conclusion Future Work
Software agents potential automate many types tedious time-consuming tasks
involve interactions one software systems. so, however, requires
agent systems support plans expressive enough capture complexity tasks,
time execute plans efficiently. needed way marry generality
existing software agent robot agent execution systems efficiency network query
engines.
paper, presented expressive plan language efficient approach
execution addresses needs. implemented ideas THESEUS applied
5

See Advanced TESS Wrapper Writing section TESS manual,
http://telegraph.cs.berkeley.edu/tess/advanced.html

660

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

system automate many types Web information processing tasks. Web
compelling domain medium demands agent flexibility efficiency.
existing software agent robot agent plan execution systems support complex plans
consisting many different types operators, systems designed process
information efficiently technologies developed database research communities.
paper, presented plan language execution system combines key aspects
agent execution systems state-of-the-art query engines, software agents
efficiently accomplish complex tasks. plan language described makes possible
build agents accomplish complex tasks supported network query
engines. Agents written using language executed efficiently state-of-theart network query engines efficiently existing agent execution systems. Beyond
work here, also proposed continuing work method speculative
execution information gathering plans (Barish & Knoblock 2002). technique leverages
machine learning techniques analyze data produced early execution accurate
predictions made data needed later execution (Barish & Knoblock
2003). result new form dynamic runtime parallelism lead significant
speedups, beyond dataflow limit allows.
also currently working Agent Wizard (Tuchinda & Knoblock, 2004),
allows user define agents monitoring tasks simply answering set questions
task. Wizard works similar Microsoft Excel Chart Wizard, builds
sophisticated charts asking user set simple questions. Wizard generate
information gathering plans using language described paper schedule
periodic execution.

Acknowledgements
research based upon work supported part National Science Foundation
Award No. IIS-0324955, part Defense Advanced Research Projects Agency (DARPA),
Department Interior, NBC, Acquisition Services Division, Contract No.
NBCHD030010, part Air Force Office Scientific Research grant numbers
F49620-01-1-0053 FA9550-04-1-0105, part United States Air Force contract
number F49620-02-C-0103, part gift Intel Corporation, part gift
Microsoft Corporation.
U.S. Government authorized reproduce distribute reports Governmental
purposes notwithstanding copyright annotation thereon. views conclusions
contained herein authors interpreted necessarily representing
official policies endorsements, either expressed implied,
organizations person connected them.

661

fiBARISH & KNOBLOCK

References
Abiteboul, S., Hull, R. S., & Vianu, V. (1995). Foundations Databases, Addison-Wesley.
Ambite, J.-L, Barish, G., Knoblock, C. A., Muslea, M., Oh, J. & Minton, S. (2002). Getting
There: Interactive Planning Agent Execution Optimizing Travel. Proceedings
14th Innovative Applications Artificial Intelligence (IAAI-2002). Edmonton, Alberta,
Canada.
Arens, Y, Knoblock, C. A., & Shen, W-M. (1996). "Query Reformulation Dynamic
Information Integration." Journal Intelligent Information Systems - Special Issue Intelligent
Information Integration 6(2/3): 99-130.
Arvind, Gostelow, K. P., & Plouffe, W. (1978). Id Report: Asynchronous Programming
Language Computing Machine, University California, 114.
Arvind & Nikhil. R. S. (1990). "Executing Program MIT Tagged-Token Dataflow
Architecture." IEEE Transactions Computers 39(3): 300-318.
Avnur, R. & Hellerstein, J. M. (2000). Eddies: Continuously Adaptive Query Processing.
Proceedings ACM SIGMOD International Conference Management Data. Dallas,
TX: 261-272.
Barish, G. & Knoblock, C. A. (2002). Speculative Execution Information Gathering Plans.
Proceedings Sixth International Conference AI Planning Scheduling (AIPS 2002).
Tolouse, France: 184-193
Barish, G. & Knoblock, C. A. (2003). Learning Value Predictors Speculative Execution
Information Gathering Plans. Proceedings 18th International Joint Conference Artificial
Intelligence (IJCAI 2003). Acapulco, Mexico: 1-8.
Boag, S., Chamberlin, D., Fernandez, M. F., Florescu, D., Robie, J., & Simeon, J. (2002).
XQuery 1.0: XML Query Language. World Wide Web Consortium, http://www.w3.org.
Bayardo Jr., R. J., Bohrer, W., Brice, R. S., Cichocki, A., Fowler, J., Helal, A., Kashyap, V.,
Ksiezyk, T., Martin, G., Nodine, M., Rashid, M., Rusinkiewicz, M., Shea, R., Unnikrishnan, C.,
Unruh, A., & Woelk, D. (1997). InfoSleuth: Semantic Integration Information Open
Dynamic Environments. Proceedings ACM SIGMOD International Conference
Management Data (SIGMOD 1997), Tucson, AZ: 195-206
Chalupsky, H., Gil, Y., Knoblock, C. A., Lerman, K., Oh, J., Pynadath, D., Russ, T. A., &
Tambe, M. (2001). Electric Elves: Applying Agent Technology Support Human Organizations.
Proceedings 13th Innovative Applications Artificial Intelligence (IAAI-2001). Seattle,
WA.
Chandrasekaran, S., Cooper, O., Deshpande, A., Franklin, M. J., Hellerstein, J. M., Hong, W.,
Krishnamurthy, S., Madden, S., Raman, V., Reiss, F., & Shah, M.A. (2003). TelegraphCQ:
Continuous Dataflow Processing Uncertain World. Proceedings First Biennial
Conference Innovative Data Systems Research. Monterey, CA.

662

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

Chawathe, S., Garcia-Molina, H., Hammer, J., Ireland, K., Papakonstantinou, Y., Ullman, J., &
Widom, J. (1994). Tsimmis Project: Integration Heterogenous Information Sources.
Proceedings 16th Meeting Information Processing Society Japan. Tokyo, Japan:
7-18.
Chen, J., DeWitt, D. J., Tian, F., & Wang, Y. (2000). NiagaraCQ: Scalable Continuous Query
System Internet Databases. Proceedings ACM SIGMOD International Conference
Management Data. Dallas, TX: 379-390.
Decker, K., Sycara, K., & Zeng, D. (1996). Designing multi-agent portfolio management
system. Proceedings AAAI Workshop Internet Information Systems.
Dennis, J. B. (1974). "First Version Data Flow Procedure Language." Lecture Notes
Computer Science 19: 362-376.
DeWitt, D. & Gray, J. (1992). "Parallel Database Systems: Future High Performance
Database Systems." Communications ACM 35(6): 85-98.
Doorenbos, R. B., Etzioni, O., & Weld, D.S. (1997). Scalable Comparison-Shopping Agent
World-Wide Web. Proceedings First International Conference Autonomous
Agents, Marina del Rey, CA: 39-48.
Duschka, O.M. (1997). Query Planning Optimization Information Integration. Ph.D.
Thesis, Stanford University, Computer Science Technical Report STAN-CS-TR-97-1598.
Etzioni, O. & Weld, D. S. (1994) "A softbot-based interface internet". Communications
ACM, 37(7):72-76.
Etzioni, O., Tuchinda, R., Knoblock, C.A., & Yates, A. (2003). buy buy: mining
airfare data minimize ticket purchase price. Proceedings Ninth ACM SIGKDD
International Conference Knowledge Discovery Data Mining, 119-128.
Evripidou, P. & Gaudiot, J. L. (1991). Input/Output Operations Hybrid Data-Flow/ControlFlow Systems. Fifth International Parallel Processing Symposium. Anaheim, California:
318-323.
Firby, R. J. (1994). Task Networks Controlling Continuous Processes. Proceedings 2nd
International Conference Artificial Intelligence Planning Systems. Chicago, IL: 49-54.
Friedman, M., Levy A. Y., & Millstein, T. D. (1999). Navigational Plans Data Integration.
Proceedings 16th National Conference Artificial Intelligence (AAAI-1999). Orlando, FL:
67-73.
Gao, G. R. (1993). "An Efficient Hybrid Dataflow Architecture Model." International Journal
Parallel Distributed Computing 19(4): 293-307.
Genesereth, M. R., Keller, A. M., & Duschka, O. M. (1997). Infomaster: Information
Integration System. Proceedings ACM SIGMOD International Conference Management
Data (SIGMOD 1997).. Tucson, AZ: 539-542.

663

fiBARISH & KNOBLOCK

Golden, K., Etzioni, O., & Weld, D. S. (1994). Omnipotence Without Omniscience: Efficient
Sensor Management Planning. Proceedings 12th National Conference Artificial
Intelligence (AAAI-1994). Seattle, WA: 10481054.
Golden, K. (1998). Leap Look: Information Gathering PUCCINI Planner.
Proceedings 4th International Conference Planning Scheduling (AIPS 1998). 70-77
Graham, J. R.., Decker, K., & Mersic M. (2003). "DECAF - Flexible Multi Agent System
Architecture." Autonomous Agents Multi-Agent Systems 7(1-2): 7-27. Kluwer Publishers.
Graefe, G. (1993). "Query evaluation techniques large databases." ACM Computing Surveys
25(2): 73-169.
Gurd, J. R. & Snelling, D. F. (1992). Manchester Data-Flow: Progress Report. Proceedings
6th International Conference Supercomputing. Washington, D.C., United States, ACM
Press: 216-225.
Hellerstein, J. M., Franklin, M. J., Chandrasekaran, S., Deshpande, A., Hildrum, K., Madden, S.,
Raman, V., & Shah, M. A. (2000). "Adaptive Query Processing: Technology Evolution." IEEE
Data Engineering Bulletin 23(2): 7-18.
Hoare, C. A. R. (1978). "Communicating Sequential Processes." Communications ACM
21(8): 666-677.
Iannucci, R. A. (1988). Toward Dataflow/Von Neumann Hybrid Architecture. 15th Annual
International Symposium Computer Architecture. Honolulu, Hawaii, IEEE Computer Society
Press: 131-140.
Ives, Z. G., Florescu, D., Friedman, M., Levy, A., & Weld D. S. (1999). Adaptive Query
Execution System Data Integration. Proceedings ACM SIGMOD International
Conference Management Data (SIGMOD 1999). Philadelphia, PA: 299-310.
Ives, Z. G., Halevy, A. Y., & Weld, D. S. (2001). "Integrating Network-Bound Xml Data." IEEE
Data Engineering Bulletin 24(2): 20-26.
Ives, Z. G., Levy, A. Y., Weld, D. S., Florescu, D., & Friedman, M. (2000). "Adaptive Query
Processing Internet Applications." IEEE Data Engineering Bulletin 23(2): 19-26.
Ives, Z. G., Levy, A. Y., & Weld, D. S. (2002). XML Query Engine Network-bound
Data. VLDB Journal 11(4): 380-402
Kahn, G. (1974). "The Semantics Simple Language Parallel Programming." Information
Processing Letters 74: 471-475.
Karp, R. M. & Miller, R. E. (1955). "Properties Model Parallel Computations:
Determinancy, Termination, Queuing." SIAM Journal Applied Mathematics 14: 1390-1411.
Knoblock, C. A., Lerman, K., Minton, S., & Muslea, I. (2000). "Accurately Reliably
Extracting Data Web: Machine Learning Approach." IEEE Data Engineering Bulletin
23(4): 33-41.

664

fiAN EXPRESSIVE LANGUAGE EFFICIENT EXECUTION SYSTEM SOFTWARE AGENTS

Knoblock, C. A., Minton, S., Ambite, J.-L., Ashish, N., Muslea, I., Philpot, A., & Tejada, S.
(2001). "The Ariadne Approach Web-Based Information Integration." International Journal
Cooperative Information Systems 10(1-2): 145-169.
Kushmerick, N. (2000). "Wrapper Induction: Efficiency Expressiveness." Artificial
Intelligence 118(1-2): 15-68.
Lesser, V., Horling, B., Klassner, F., Raja, A., Wagner, T., & Zhang, S. (2000). "BIG: Agent
Resource-Bounded Information Gathering Decision Making." Artificial Intelligence
Journal, Special Issue Internet Information Agents. 118(1-2): 197-244.
Levy, A. Y., Rajaraman, A., & Ordille, J. J. (1996). Querying Heterogeneous Information
Sources Using Source Descriptions. Proceedings Twenty-Second International Conference
Large Databases. Bombay, India: 251-262.
Lo, J. L., Barroso, L. A., Eggers, S. J., Gharachorloo, K., Levy, H. M., & Parekh, S. S. (1998).
Analysis Database Workload Performance Simultaneous Multithreaded Processors.
Proceedings 25th Annual International Symposium Computer Architecture. Barcelona,
Spain, IEEE Press: 39-50.
McDermott, D. (1991). Reactive Plan Language, Yale University, CSD-RR-864
Myers, K. L. (1996). Procedural Knowledge Approach Task-Level Control. Proceedings
3rd International Conference Ai Planning Scheduling. Edinburgh, UK: 158-165.
Naughton, J. F., DeWitt, D. J., Maier, D., Aboulnaga, A., Chen, J., Galanis, L., Kang, J.,
Krishnamurthy, R., Luo, Q., Prakash, N., Ramamurthy, R., Shanmugasundaram, J., Tian, F.,
Tufte, K., Viglas, S., Wang, Y., Zhang, C., Jackson, B., Gupta, A., & Che, R. (2001). "The
NIAGARA Internet Query System." IEEE Data Engineering Bulletin 24(2): 27-33.
Nikhil, R. S. & Arvind (2001). Implicit Parallel Programming pH, Morgan Kaufmann
Publishers Inc.
Papadopoulos, G. M. & Traub, K. R. (1991). Multithreading: Revisionist View Dataflow
Architectures. Proceedings 18th International Symposium Computer Architecture (ISCA
1991). New York, NY: 342-351.
Papdopoulos, G. M. & Culler, D. E. (1990). Monsoon: Explicit Token Store Architecture.
Proceedings 17th International Symposium Computer Architecture. Seattle,
Washington.
Pottinger, R., & Halevy, A. Y. (2001). MiniCon: Scalable Algorithm Answering Queries
Using Views. VLDB Journal 10(2-3): 182-198
Raman, V. (2002). Personal Communication.
Raman, V. & Hellerstein, J.. M. (2002). Partial Results Online Query Processing. Proceedings
ACM Sigmod International Conference Management Data. Madison, Wisconsin,
ACM Press: 275-286.

665

fiBARISH & KNOBLOCK

Redstone, J. A., Eggers, S. J., & Levy, H. M. (2000). Analysis Operating System Behavior
Simultaneous Multithreaded Architecture. Proceedings Ninth International
Conference Architectural Support Programming Languages Operating Systems.
Cambridge, Massachusetts, ACM Press: 245-256.
Schek, H. J., & Scholl, M. H., (1986). "The Relational Model Relation-Valued Attributes."
Information Systems 11(2): 137-147.
Shah, M. A., Madden, S., Franklin, M. J., & Hellerstein, J. M. (2001). "Java Support DataIntensive Systems: Experiences Building Telegraph Dataflow System." SIGMOD Record
30(4): 103-114.
Shanmugasundaram, J., Tufte, K., DeWitt, D. J., Naughton, J. F., & Maier, D. (2000).
Architecting Network Query Engine Producing Partial Results. Proceedings ACM
SIGMOD 3rd International Workshop Web Databases (WebDB 2000). Dallas, TX: 17-22.
Sycara, K., Paolucci, M., van Velsen, M. & Giampapa, J. (2003). "The RETSINA MAS
Infrastructure. Autonomous Agents Multi-Agent Systems."
7(1-2): 29-48. Kluwer
Publishers.
Thakkar, S. & Knoblock, C. A. (2003). Efficient execution recursive data integration plans.
Workshop Information Integration Web, 18th International Joint Conference
Artificial Intelligence (IJCAI 2003). Acapulco, Mexico.
Thakkar, S., Knoblock, C. A., & Ambite, J.-L. (2003). view integration approach dynamic
composition web services. Workshop Planning Web Services, 13th International
Conference Automated Planning & Scheduling (ICAPS 2003), Trento, Italy.
Tuchinda, R. & Knoblock, C. A. (2004). Agent wizard: building agents answering questions.
Proceedings 2004 International Conference Intelligent User Interfaces, Funchal,
Madeira, Portugal: 340-342.
Tullsen, D. M., Eggers, S., & Levy, H. M. (1995). Simultaneous Multithreading: Maximizing onChip Parallelism. Proceedings 22nd Annual ACM International Symposium Computer
Architecture. Santa Magherita Ligure, Italy: 392-403.
Wiederhold, G. (1996). "Intelligent Integration Information." Journal Intelligent
Information Systems 6(2): 281-291.
Williamson, M., Decker, K., & Sycara, K. (1996). Unified Information Control Flow
Hierarchical Task Networks. Theories Action, Planning, Robot Control: Bridging Gap:
Proceedings 1996 AAAI Workshop. Menlo Park, California, AAAI Press: 142-150.
Wilschut, A. N. & Apers, P. M. G. (1993). "Dataflow Query Execution Parallel MainMemory Environment." Distributed Parallel Databases 1(1): 103-128.

666

fiJournal Artificial Intelligence Research 23 (2005) 167-243

Submitted 07/04; published 02/05

Combining Spatial Temporal Logics:
Expressiveness vs. Complexity
David Gabelaia
Roman Kontchakov
Agi Kurucz

gabelaia@dcs.kcl.ac.uk
romanvk@dcs.kcl.ac.uk
kuag@dcs.kcl.ac.uk

Department Computer Science, Kings College London
Strand, London WC2R 2LS, U.K.

Frank Wolter

frank@csc.liv.ac.uk

Department Computer Science, University Liverpool
Liverpool L69 7ZF, U.K.

Michael Zakharyaschev

mz@dcs.kcl.ac.uk

Department Computer Science, Kings College London
Strand, London WC2R 2LS, U.K.

Abstract
paper, construct investigate hierarchy spatio-temporal formalisms
result various combinations propositional spatial temporal logics
propositional temporal logic PT L, spatial logics RCC-8, BRCC-8, S4u
fragments. obtained results give clear picture trade-off expressiveness
computational realisability within hierarchy. demonstrate different combining principles well spatial temporal primitives produce NP-, PSPACE-,
EXPSPACE-, 2EXPSPACE-complete, even undecidable spatio-temporal logics
components NP- PSPACE-complete.

1. Introduction
Qualitative representation reasoning quite successful dealing
time space. exists wide spectrum temporal logics (see, e.g., Allen, 1983;
Clarke & Emerson, 1981; Manna & Pnueli, 1992; Gabbay, Hodkinson, & Reynolds, 1994;
van Benthem, 1995). variety spatial formalisms (e.g., Clarke, 1981; Egenhofer
& Franzosa, 1991; Randell, Cui, & Cohn, 1992; Asher & Vieu, 1995; Lemon & Pratt,
1998). cases determining computational complexity respective reasoning
problems one important research issues. example, Renz Nebel
(1999) analysed complexity RCC-8, fragment region connection calculus RCC
eight jointly exhaustive pairwise disjoint base relations spatial regions
introduced Egenhofer Franzosa (1991) Randell colleagues (1992); Nebel
Burckert (1995) investigated complexity Allens interval algebra; numerous results
computational complexity point-based propositional linear temporal logic PT L
various flows time obtained Sistla Clarke (1985) Reynolds (2003,
2004). many cases investigations resulted development implementation
effective reasoning algorithms (see, e.g., Wolper, 1985; Smith & Park, 1992; Egenhofer
& Sharma, 1993; Schwendimann, 1998; Fisher, Dixon, & Peim, 2001; Renz & Nebel, 2001;
Hustadt & Konev, 2003).
c
2005
AI Access Foundation. rights reserved.

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

space

.

X

X

X

X

X

.
F
-

0

1

2

3

time

Figure 1: Topological temporal model.
next apparent natural step combine two kinds reasoning.
course, attempts construct spatio-temporal hybrids. example,
intended interpretation Clarkes (1981, 1985) region-based calculus spatio-temporal.
Region connection calculus RCC (Randell et al., 1992) contained function space(X, t)
representing space occupied object X moment time t. Muller (1998a)
developed first-order theory reasoning motion spatial entities. However,
formalisms turn expressive computational point view:
undecidable. Moreover, far know, serious attempts investigate
implement partial (say, incomplete) algorithms capable spatio-temporal reasoning
logics made.
problem constructing spatio-temporal logics better algorithmic properties analysing computational complexity first attacked Wolter Zakharyaschev (2000b); see also popular extended version (Wolter & Zakharyaschev,
2002) conference paper, well (Bennett & Cohn, 1999; Bennett, Cohn, Wolter,
& Zakharyschev, 2002; Gerevini & Nebel, 2002).
main idea underlying papers consider various combinations wellbehaved spatial temporal logics. intended spatio-temporal structures
regarded Cartesian products intended time-line topological (or
other) spaces used model spatial dimension. Figure 1 shows product
(of flow time F = hN, <i two-dimensional Euclidean space T) moving
spatial object X. moving object viewed either 3D spatio-temporal entity
(in particular case) collection snapshots slices entity
moment time; discussion see, e.g., (Muller, 1998b) references therein.
paper, use snapshot terminology understand moving spatial object (or,
precisely, interpret object as) set pairs hX, ti where, point
168

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

flow time, X subset topological spacethe state spatial object
moment t.
expressive power (and consequently computational complexity) combined
spatio-temporal formalisms obviously depends three parameters:
1. expressivity spatial component,
2. expressivity temporal component,
3. interaction two components allowed combined logic.
Regardless chosen component languages, minimal requirement spatiotemporal combination useful ability
express changes time truth-values purely spatial propositions.

(PC)

Typical examples logics meeting spatial propositions truth change principle
combinations RCC-8 Allens interval calculus (Bennett et al., 2002; Gerevini & Nebel,
2002) combinations RCC-8 PT L introduced Wolter Zakharyaschev
(2000b) allow applications temporal operators Boolean combinations RCC-8 relations. Languages satisfying (PC) capture, instance, aspects continuity
change principle (see, e.g., Cohn, 1997)
(A) two images computer screen disconnected now, either remain
disconnected become externally connected one quantum computers time.
Another example following statement geography Europe:
(B) Kaliningrad disconnected EU moment Poland becomes
tangential proper part EU, Kaliningrad EU
externally connected forever.
However, languages meeting (PC) necessarily satisfy second fundamental
spatial object change principle according able
express changes evolutions spatial objects time.

(OC)

logical terms, (PC) refers change truth-values propositions, (OC)
change extensions predicates; see Fig. 2 X moment denotes state
X moment + 1. examples motivating (OC):
(C) Continuity change: cyclones current position overlaps position hour.
(D) Two physical objects cannot occupy space: tomorrow object X
place object today, move tomorrow.
(E) Geographic regions change: space occupied Europe never changes.
(F) Geographic regions change: two years EU extended Romania
Bulgaria.
(G) Fairness conditions regions: raining every part England ever
ever again.
169

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

space

.

X

X
X

X

X





-F

.


t+1

t+2

time

Figure 2: Temporal operators regions.
(H) Mutual exclusion: Earth consists water land, space occupied
water expands, space occupied land shrinks.
clear represent statements refer evolution
spatial objects time (say, compare objects X X)it enough take
account change truth-values propositions speaking spatial objects.
main aim paper investigate trade-off expressive power
computational behaviour spatio-temporal hybrids satisfying (PC) (OC)
principles interpreted various spatio-temporal structures. purpose show
computational obstacles one expect application domain requires
kind interactions temporal spatial operators.
spatio-temporal logics consider combinations fragments PT L
interpreted different flows time fragments propositional spatial logic S4u
(equipped interior closure operators, universal existential quantifiers
points space well Booleans) interpreted topological spaces. choice
motivated following reasons:
component logics well understood established temporal spatial
knowledge representation; supported reasonably effective reasoning
procedures.
definition, implicit explicit temporal quantification necessary capture (OC),
fragments PT L weakest languages quantification know of.
170

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Allens interval calculus, example, provide means quantification
intervals. certainly suitable spatio-temporal hybrids satisfying (PC) (see
Bennett et al., 2002; Gerevini & Nebel, 2002) natural conservative
way combining spatial formalisms meet (OC). hand,
embedded PT L (Blackburn, 1992). natural alternative PT L would
extension Allens calculus means quantification intervals introduced
Halpern Shoham (1986), unfortunately temporal logic turns
highly undecidable.
Although logic S4u originally introduced realm modal logic (see
details), work Bennett (1994), Nutt (1999), Renz (2002) Wolter
Zakharyaschev (2000a) showed regarded unifying language
contains many spatial formalisms like RCC-8, BRCC-8 9-intersections
Egenhofer Herring (1991) fragments.
Apart choice component languages level interaction, expressive power computational complexity spatio-temporal logics strongly depend
restrictions may want impose intended spatio-temporal structures
interpretations spatial objects.
choose among different flows time (say, discrete dense, infinite finite)
among different topological spaces (say, arbitrary, Euclidean Aleksandrov).
time point interpret spatial objects arbitrary subsets topological space, regular closed (or open) ones, polygons, etc.
represent assumption everything eventually comes end,
know when, one restrict class intended models imposing
finite change assumption states spatial object change spatial
configuration infinitely often, liberal finite state assumption according
every spatial object finitely many possible states (although
may change states infinitely often).
paper organised follows. Section 2 introduce full detail component
spatial temporal logics combined later on. particular, besides standard
spatial logics like RCC-8 9-intersections Egenhofer Herring (1991), consider
generalisations framework S4u investigate computational complexity. example, show maximal fragment S4u dealing regular closed
spatial objects turns PSPACE-complete, natural generalisation
9-intersections still NP. Section 3 introduce hierarchy spatio-temporal logics
outlined above, provide topological-temporal semantics, analyse computational properties. First show spatio-temporal logics satisfying (PC)
principle complex components. consider maximal combinations S4u (fragments of) PT L meeting (PC) (OC) see
straightforward approach work: resulting logics turn undecidable.
Finally, systematically investigate trade-off expressivity complexity
spatio-temporal formalisms construct hierarchy decidable logics satisfying (PC)
171

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

(OC) whose complexity ranges PSPACE 2EXPSPACE. results, possible implementations well open problems discussed Section 4.
readers convenience important (un)decidability complexity results obtained
paper summarised Table 1 page 193. technical definitions detailed
proofs found appendices.

2. Propositional Logics Space Time
begin introducing discussing spatial temporal formalisms
going combine later paper.
2.1 Logics Space
dealing number logics suitable qualitative spatial representation
reasoning: well-known RCC-8, BRCC-8 S4u , well certain fragments
last one. intended interpretations logics topological spaces.
topological space pair = hU, Ii U nonempty set, universe
space, interior operator U satisfying standard Kuratowski axioms:
X, U ,
I(X ) = IX IY,

IX IIX,

IX X

IU = U.

operator dual called closure operator denoted C: every X U ,
CX = U I(U X). Thus, IX interior set X, CX closure.
X called open X = IX closed X = CX. complement open set
closed vice versa. boundary set X U defined CX IX. Note X
U X boundary.
2.1.1 S4u
expressive spatial formalism S4u i.e., propositional modal logic S4 extended universal modalities. pedigree logic quite unusual. S4
introduced independently Orlov (1928), Lewis (in Lewis & Langford, 1932), Godel
(1933) without intention reason space. Orlov Godel understood
logic provability (in order provide classical interpretation intuitionistic
logic Brouwer Heyting) Lewis logic necessity possibility, is,
modal logic. Besides Boolean connectives propositional variables, language
S4 contains two modal operators: (it necessary provable) C, dual (it
possible consistent). words, formulas S4 defined follows:


::=

p

|

| 1 u 2

| I,

(1)

p variables. Set C = . denote modal operators C
(rather conventional 2 3) understand, following observation
made several logicians late thirties early forties (Stone, 1937; Tarski, 1938;
Tsao Chen, 1938; McKinsey, 1941), S4 logic topological spaces: interpret
propositional variables subsets topological space, Booleans standard settheoretic operations, C as, respectively, interior closure operators
172

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

space, S4-formula modally consistent satisfiable
topological spacei.e., value empty interpretation.1
precisely, topological model pair form = hT, Ui, = hU, Ii
topological space U, valuation, map associating every variable p set
U(p) U . valuation U inductively extended arbitrary S4-formulas taking:
U( ) = U U( ),

U(1 u 2 ) = U(1 ) U(2 ),

U(I ) = IU( ).

Expressions form (1) interpreted subsets topological spaces;
call spatial terms. particular, propositional variables S4 understood
spatial variables.

language S4u extends S4 universal existential quantifiers 2

3, respectively (known modal logic universal modalities). Given spatial
say part space (represented by) empty (there
term , write 3
means occupies whole space (all points belong ).
least one point ); 2
taking Boolean combinations expressions arrive called spatial
formulas. BNF definition looks follows:2



::= 2

|



| 1 2 ,

. Spatial formulas either true false
= 2
spatial terms. Set 3
topological models. truth-relation |= spatial formula true topological
model Mis defined standard way:

|= 2

iff

U( ) = U ,

|=

iff

6|= ,

|= 1 2

iff

|= 1 |= 2 .

Say spatial formula satisfiable topological model |= .
seemingly simple query language S4u express rather complex relations
sets topological spaces. example, formula
(q @ p) 2
(p @ Cq) 3
p 3
Iq
2

says set q dense nonempty set p, interior (here 1 @ 2
abbreviation 1 u 2 ).
following folklore complexity result proved different settings (see, e.g.,
Nutt, 1999; Areces, Blackburn, & Marx, 2000):
Theorem 2.1. (i) S4u enjoys exponential finite model property; i.e., every satisfiable
spatial formula satisfiable topological space whose size exponential
size .
(ii) Satisfiability spatial formulas topological models PSPACE-complete.
1. Moreover, according McKinsey (1941) McKinsey Tarski (1944), n-dimensional Euclidean
space, n 1, enough satisfy consistent S4-formulas.
2. Formally, language S4u defined weaker standard one, say, Goranko
Passy (1992). However, one easily show precisely expressive power:
see, e.g., (Hughes & Cresswell, 1996) (Aiello & van Benthem, 2002b).

173

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

One way proving theorem first observe every satisfiable spatial formula
satisfied Aleksandrov model, i.e., model based Aleksandrov topological
spacealias standard Kripke frame S4 (see, e.g., McKinsey & Tarski, 1944; Goranko
& Passy, 1992).
remind reader topological space called Aleksandrov space (Alexandroff, 1937) arbitrary (not finite) intersections open sets open. Kripke frame
(or simply frame) S4 pair form G = hV, Ri, V nonempty set
R transitive reflexive relation (i.e., quasi-order ) V . Every frame G induces
interior operator IG V : every X V ,
IG X = {x X | V (xRy X)}.
words, open sets topological space TG = hV, IG upward closed
(or R-closed ) subsets V . minimal neighbourhood point x TG (that
minimal open set contain x) consists points R-accessible x.
well-known (see, e.g., Bourbaki, 1966) TG Aleksandrov space and, conversely,
every Aleksandrov space induced quasi-order.
Now, complete proof, suffices recall S4 PSPACE-hard (Ladner,
1977) use, say, standard tableau technique establish exponential finite model
property construct PSPACE satisfiability checking algorithm spatial formulas.
Although computational complexity S4, logic S4u
expressive. standard example spatial formulas distinguish arbitrary
connected3 topological spaces. Consider, instance, formula
(Cp @ p) 2
(p @ Ip) 3
p
p 2
2

(2)

saying p closed open, nonempty coincide whole space.
satisfied model whose underlying topological space connected,
satisfiable S4-formulas satisfied connected (e.g., Euclidean) spaces.
Another example illustrating expressive power S4u formula
(p @ Cp) 2
(p @ Cp)
p 2
3

(3)

defining nonempty set p p p empty interiors. fact, second
third conjuncts say p p consist boundary points only.
2.1.2 RCC-8 Fragment S4u
qualitative spatial representation reasoning, quite often assumed spatial
terms interpreted regular closed (or open) sets topological spaces (see,
e.g., Davis, 1990; Asher & Vieu, 1995; Gotts, 1996). One reasons imposing
restriction exclude consideration pathological sets p (3). Recall
set X regular closed X = CIX, clearly hold set p satisfying (3).
Another reason ensure space occupied physical body homogeneous
sense contain parts different dimensionality. example,
3. remind reader topological space connected universe cannot represented
union two disjoint nonempty open sets.

174

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

.
X

IX

CIX

.
Figure 3: Regular closure.
subset X Euclidean plane Fig. 3 consists three parts: 2D ellipse hole,
2D circle, 1D curve connecting them. curve disappears form set CIX,
regular closed CICIX = CIX, every X every topological space.
paper, consider several fragments S4u dealing regular closed sets.
call sets regions. Perhaps, best known language devised
speaking regions RCC-8 introduced area Geographical
Information Systems (see Egenhofer & Franzosa, 1991; Smith & Park, 1992)
decidable subset Region Connection Calculus RCC (Randell et al., 1992). syntax
RCC-8 contains eight binary predicates,
DC(X, ) regions X disconnected,
EC(X, ) X externally connected,
EQ(X, ) X equal,
PO(X, ) X partially overlap,
TPP(X, ) X tangential proper part ,
NTPP(X, ) X nontangential proper part ,
inverses last twoTPPi(X, ) NTPPi(X, ),
combined using Boolean connectives. example, given spatial database
describing geography Europe, query whether United Kingdom
Republic Ireland share common border. answer found checking whether
RCC-8 formula EC(UK, RoI) follows database.
arguments RCC-8 predicates called region variables; interpreted
regular closed setsi.e., regionsof topological spaces. satisfiability problem
RCC-8 formulas interpretations NP-complete (Renz & Nebel, 1999).
expressive power RCC-8 rather limited. operates simple regions distinguish connected disconnected ones, regions
without holes, etc. (Egenhofer & Herring, 1991). RCC-8 represent complex relations two regions. Consider, example, three countries (say, Russia,
Lithuania Poland) one adjacent others,
point three meet. express fact may need ternary
predicate like
EC3(Russia, Lithuania, Poland).
(4)
175

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

analyse possible ways extending expressive power RCC-8, convenient view fragment S4u (that RCC-8 embedded S4u first shown
Bennett, 1994). Observe first that, every spatial variable p, spatial term
CIp

(5)

interpreted regular closed set every topological model. So, every region
variable X RCC-8 associate spatial term %X = CIpX , pX spatial
variable, translate RCC-8 predicates spatial formulas taking:
(%
(I%
EC(X, ) = 3
X u %Y ) 3
X u I%Y ),
(%
DC(X, ) = 3
X u %Y ),
(%
(%
EQ(X, ) = 2
X @ %Y ) 2
@ %X ),
(%
(%
(I%
PO(X, ) = 3
X u I%Y ) 2
X @ %Y ) 2
@ %X ),
(%
(%
(%
TPP(X, ) = 2
X @ %Y ) 2
@ %X ) 2
X @ I%Y ),
(%
(%
NTPP(X, ) = 2
X @ I%Y ) 2
@ %X )

(TPPi NTPPi mirror images TPP NTPP, respectively). first
formulas, instance, says two regions externally connected iff intersection
regions empty, whereas intersection interiors is. clear
RCC-8 formula satisfiable topological space translation
S4u defined satisfiable topological model.
translation also shows RCC-8 two regions related terms
truth/falsity atomic spatial formulas form
(% u % ),
2
1
2

(I% u I% ),
2
1
2

(%
2
1 @ %2 )



(%
2
1 @ I%2 ),

%1 %2 spatial terms form (5). example, first formulas
says intersection two regions empty, whereas last one states one region
contained interior another one. words, RCC-8 regarded part
following fragment S4u :
%

::= CIp,



::= %1 u %2




::= 2

|

| I%1 u I%2


| %1 @ %2

| %1 @ I%2 ,

| 1 2 .

distinguish two types spatial terms. form %
called atomic region termsthey represent (regular closed) regions want compare.
Spatial terms form used relate regions (note extensions
necessarily regular closed).
Actually, fragment introduced bit expressive RCC-8: example, contains (appropriately modified) formula (2) satisfied disconnected topological spaces, satisfiable RCC-8 formulas satisfiable
Euclidean space (Renz, 1998). However, convenient us distinguish
two spatial logics. First, turn technical results regarding computational complexity hold even combined temporal
176

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

logics. second, intuitive concise language RCC-8 suitable
illustrations. instance, distinguish region variable X
(% u % ).
region term %X use DC(%1 , %2 ) abbreviation 3
1
2
definition suggests two ways increasing expressive power RCC-8
(while keeping regions regular closed):
(i) allowing complex region terms %,
(ii) allowing ways relating (i.e., complex terms ).
2.1.3 BRCC-8 Fragment S4u
language BRCC-8 Wolter Zakharyaschev (2000a) (see also Balbiani, Tinchev, &
Vakarelov, 2004) extends RCC-8 direction (i). uses eight binary predicates
RCC-8 allows atomic regions also intersections, unions complements. instance, BRCC-8 express fact region (say, Swiss
Alps) intersection two regions (Switzerland Alps case):
EQ(SwissAlps, Switzerland u Alps).

(6)

embed BRCC-8 S4u using almost translation case RCC-8.
difference now, since Boolean combinations regular closed sets
necessarily regular closed, prefix compound spatial terms CI. way
obtain, example, spatial term
CI (Switzerland u Alps)
representing Swiss Alps. manner treat set-theoretic operations,
leads us following definition Boolean region terms:
%

::=

CIp

| CI%

| CI(%1 u %2 ).

words, Boolean region terms denote precisely members well-known
Boolean algebra regular closed sets. (The union expressible via intersection

complement usual way.) simplify notation, given spatial term , write
denote result prefixing CI every subterm ; particular,






p = CIp,
= CI
1 u 2 = CI( 1 u 2 ).

Note (equivalent to) Boolean region term, every spatial term .
Swiss Alps example represented Switzerland u Alps .
interest note Boolean region terms increase complexity
reasoning arbitrary topological models: satisfiability problem BRCC-8 formulas
still NP-complete (however, becomes PSPACE-complete intended models based
connected spaces). hand, BRCC-8 allows restricted comparisons
two regions as, e.g., (6). Nevertheless, shall see below, ternary relations
like (4) still unavailable BRCC-8: require different ways comparing regions;
cf. (ii).
177

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

2.1.4 RC
Egenhofer Herring (1991) proposed relate two regions terms 9-intersections33-matrix specifying emptiness/nonemptiness (nine) possible intersections
interiors, boundaries exteriors regions. Recall that, region X,
three disjoint parts space hU, Ii represented
IX,

X (U IX)



U X,

respectively. generalising approach finite number regions, obtain
following fragment RC S4u :
%

::= Boolean region terms,



::= %




::= 2

| I%
|

|



| 1 u 2 ,

| 1 2 .

words, RC define relations regions terms emptiness/nonemptiness sets formed using arbitrary set-theoretic operations regions
interiors. However, nested applications topological operators allowed (an
example applications required found next section).
Clearly, RCC-8 BRCC-8 fragments RC. Moreover, unlike BRCC-8,
language RC allows us consider complex relations regions. instance,
ternary relation required (4) defined follows:
(%
(I%
(I%
(I%
EC3(X, Y, Z) = 3
X u %Y u %Z ) 3
X u I%Y ) 3
u I%Z ) 3
Z u I%X ).

Another, abstract, example formula
% u u % I%
3
1

i+1 u u I%j u %j+1 u u %k u I%k+1 u u I%n



says
regions %1 , . . . , %i meet somewhere inside region occupied jointly
%i+1 , . . . , %j , outside regions %j+1 , . . . , %k inside %k+1 , . . . , %n .
Although RC expressive RCC-8 BRCC-8, reasoning language still computational complexity:
Theorem 2.2. satisfiability problem RC-formulas arbitrary topological models
NP-complete.
result proved Appendix A. Lemma A.1 shows every satisfiable RCformula satisfied model based Aleksandrov space induced
disjoint union n-broomsi.e., quasi-orders form depicted Fig. 4. Topological
spaces kind rather primitive structure satisfying following property:
(rc) roots n-brooms boundary points, minimal neighbourhood
every boundary pointi.e., n-broom containing pointmust contain
least one internal point least one external point.
178

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

b

b
b
b
*

HH





J
]
H


HHJ
HJ b


depth 0
depth 1

Figure 4: n-broom (for n = 4).
example, spatial formula (3) cannot satisfied model property,
RC.
Lemma A.2, size satisfying model polynomial (in fact, quadratical)
length input RC-formula, nondeterministic polynomial time
algorithm. Actually, proof straightforward generalisation complexity proof
BRCC-8 given Wolter Zakharyaschev (2000a): difference
case BRCC-8 sufficient consider 2-brooms (which called forks).
means, particular, ternary relation (4)which satisfiable model
n-broom, n 3is indeed expressible BRCC-8.
Remark 2.3. topological terms, n-brooms examples so-called door spaces
every subset either open closed. However, modal theory n-brooms defines
wider interesting topological class known submaximal spaces every
dense subset open. Submaximal spaces around since early 1960s
generated interesting challenging problems topology. survey systematic
study spaces see (Arhangelskii & Collins, 1995) references therein.
2.1.5 RC max
One could go even direction (ii) impose restrictions whatsoever
ways relating Boolean region terms. leads us maximal fragment RC max
S4u spatial terms interpreted regular closed sets. syntax defined
follows:
%

::= Boolean region terms,



::= %




::= 2

|
|

| 1 u 2


| I,

| 1 2 ,

understand difference RC RC max , consider RC max -formula








3
q1 u q1 2
q1 u q1 @ C q1 u q2 u q2
.

(7)


says boundary q1 empty
every neighbourhood every
point
boundary contains internal point q1 belongs boundary
q2 (compare property (rc) above). simplest Aleksandrov model satisfying
formula depth 2; shown Fig. 5.
price pay expressivity complexity RC max
full S4u :
179

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

q1 q2

q1 q2

q1 q2

b
b
]
J



J


J b


b

depth 0








q1 q2


]
J


J
J

q1 q2 b

depth 1
depth 2

Figure 5: Model satisfying formula (7).
Theorem 2.4. satisfiability problem RC max -formulas PSPACE-complete.
upper bound follows Theorem 2.1 lower bound proved Appendix A, construct sequence RC max -formulas
satisfiable Aleksandrov space cardinality least exponential length
formula. first formula sequence similar (7) above.
interest note, however, RC max still expressive enough define
pathological sets p (3) clearly regular closed.
conclude section, summarise inclusions spatial languages
introduced above:
RCC-8

$

BRCC-8

$

RC

$

RC max

$

S4u .

discussions spatial logics kind refer reader paper (PrattHartmann, 2002).
2.2 Temporal Logics
said introduction, temporal components spatio-temporal hybrids
(fragments of) propositional temporal logic PT L interpreted various flows time
modelled strict linear orders F = hW, <i, W nonempty set time
points < (connected, transitive irreflexive) precedence relation W .
language PT L based following alphabet:
propositional variables p0 , p1 , . . . ,
Booleans ,
binary temporal operators U (until) (since).
set PT L-formulas defined standard way:


::=

p

|



| 1 2

| 1 U 2

| 1 2 .

PT L-models pairs form = hF, Vi F = hW, <i flow time
V, valuation, map associating variable p set V(p) W time
points (where p supposed true). truth-relation (M, w) |= , arbitrary
PT L-formula w W , defined inductively follows, (u, v) denotes open
interval {w W | u < w < v}:
180

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

(M, w) |= p

w V(p),

iff

(M, w) |=

(M, w) 6|= ,

iff

(M, w) |= 1 2

(M, w) |= 1 (M, w) |= 2 ,

iff

(M, w) |= 1 U 2
u (w, v),

iff

v > w (M, v) |= 2 (M, u) |= 1

(M, w) |= 1 2
u (v, w).

iff

v < w (M, v) |= 2 (M, u) |= 1

PT L-formula satisfied (M, w) |= w W .
took operators U primitive simply important temporal
operators defined via them. example, 3F (sometime future) 2F
(always future) expressible via U
3F = > U ,

2F = 3F ,

(> logical constant true) means
(M, w) |= 3F

iff

v > w (M, v) |= ,

(M, w) |= 2F

iff

(M, v) |= v > w.

intended flows time strict linear orders, next-time operator
definable via U taking
= U



also

( logical constant false) perfectly reflects intuition: F discrete
(M, w) |=

iff

(M, w + 1) |= ,

w + 1 immediate successor w F. reader problems
defining past versions 3F , 2F .
following results due Sistla Clarke (1985) Reynolds (2003, 2004):
Theorem 2.5. satisfiability problem PT L-formulas PSPACE-complete
following classes flows time: strict linear orders, finite strict linear orders,
hN, <i, hZ, <i, hQ, <i, hR, <i.
Note, however, reasoning becomes somewhat simpler take 3F , 2F
past counterparts (but , U S) temporal primitives. Denote PT L2
corresponding fragment PT L. Then, according results Ono Nakamura
(1980), Sistla Clarke (1985), Wolter (1996), have:
Theorem 2.6. satisfiability problem PT L2 -formulas NP-complete
classes flows time mentioned Theorem 2.5.
181

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

3. Combinations Spatial Temporal Logics
section introduce discuss various ways combining logics space time.
First construct spatio-temporal logics satisfying (PC) principle (see introduction) show inherit good computational properties components.
encouraged results, consider maximal combinations S4u
(fragments of) PT L meeting (PC) (OC) see straightforward approach work: end undecidable logics. leads us systematic
investigation trade-off expressivity computational complexity spatiotemporal formalisms. result hierarchy decidable logics satisfying (PC) (OC)
whose complexity ranges PSPACE 2EXPSPACE.
3.1 Spatio-Temporal Logics (PC)
begin investigation combinations spatial temporal logics introduced
considering language PT L[S4u ] temporal operators applied
spatial formulas spatial terms (this way temporalising logic first
introduced Finger Gabbay, 1992). precise syntactic definition PT L[S4u ]-terms
PT L[S4u ]-formulas follows:


::= p




::= 2

|
|

| 1 u 2


| I,

| 1 2

| 1 U 2

| 1 2 .

Note definition PT L[S4u ]-terms coincides definition spatial terms
S4u reflects fact PT L[S4u ] cannot capture change spatial objects
time. imposed restrictions upon temporal operators formulasso
combined language still full expressive power PT L. (Clearly, S4u fragment
PT L[S4u ].)
similar way introduce spatio-temporal logics based spatial
languages dealing with: RCC-8, BRCC-8, RC RC max . example, temporalisation PT L[BRCC-8] BRCC-8 (denoted ST 0 hierarchy Wolter
Zakharyaschev 2002) allows applications temporal operators RCC-8 predicates
Boolean region terms. languages regarded fragments PT L[S4u ]
precisely way spatial components treated fragments S4u .
illustrate expressive power PT L[RCC-8] formalising sentences (A) (B)
introduction:
DC(Image1 , Image2 ) DC(Image1 , Image2 ) EC(Image1 , Image2 ),

DC(Kaliningrad, EU) U TPP(Poland, EU)
2F

(A)
(B)


TPP(Poland, EU) EC(Kaliningrad, EU) .

Sentences (C)(H) cannot expressed language (or even PT L[S4u ]): require
comparisons states spatial objects different time instants.
intended semantics PT L[S4u ] (and spatio-temporal logics considered
paper) rather straightforward. topological temporal model (a tt-model, short)
triple form = hF, T, Ui, F = hW, <i flow time, = hU, Ii
182

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

topological space, U, valuation, map associating every spatial variable p
every time point w W set U(p, w) U space occupied p moment
w; see Fig. 1. valuation U inductively extended arbitrary PT L[S4u ]-terms (i.e.,
spatial terms) precisely way S4u , add time point
parameter:
U( , w) = U U(, w),

U(1 u 2 , w) = U(1 , w) U(2 , w),

U(I, w) = IU(, w).

truth-values PT L[S4u ]-formulas defined way PT L:

(M, w) |= 2

(M, w) |=

iff
iff

U(, w) = U ,
(M, w) 6|= ,

(M, w) |= 1 2

iff

(M, w) |= 1

(M, w) |= 1 U 2
u (w, v),

iff

v > w (M, v) |= 2 (M, u) |= 1

(M, w) |= 1 2
u (v, w).

iff

v < w (M, v) |= 2 (M, u) |= 1



(M, w) |= 2 ,

pure temporal case, operators 2F , 3F , well past counterparts
defined terms U S.
PT L[S4u ]-formula said satisfiable exists tt-model
(M, w) |= time point w.
following optimal complexity result obtained Appendix B.1:
Theorem 3.1. satisfiability problem PT L[S4u ]-formulas tt-models based
arbitrary flows time, (arbitrary) finite flows time, hN, <i, hZ, <i, hQ, <i, hR, <i,
PSPACE-complete.
proof theorem based fact interaction spatial
temporal components PT L[S4u ] restricted. fact, every PT L[S4u ]-formula
one construct PT L-formula replacing every occurrence (spatial) subformula
fresh propositional variable p . Then, given PT L-model N = hF, Vi
2

moment time w, take set
| (N, w) |= p } {2
| (N, w) |= p }
w = {2



spatial formulas. hard see w satisfiable every w F,
tt-model satisfying based flow F. Now, check whether satisfiable,
suffices use suitable nondeterministic algorithm (see, e.g., Sistla & Clarke, 1985;
Reynolds, 2003, 2004) guesses PT L-model then, time point w,
check satisfiability w . done using polynomial space length .
Theorem 3.1 (together Theorem 2.5) shows spatio-temporal logics
form PT L[L], L {RCC-8, BRCC-8, RC, RC max }, also PSPACE-complete
standard flows time.
183

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

let us consider temporalisations spatial logics (NP-complete) fragment PT L2 PT L. Theorems 2.4 3.1, PT L2 [S4u ] PT L2 [RC max ]
PSPACE-complete. However, simpler (NP-complete) spatial components obtain
better result:
Theorem 3.2. satisfiability problem PT L2 [RC]-formulas tt-models based
classes flows time mentioned Theorem 3.1 NP-complete.
proof essentially Theorem 3.1, nondeterministic
polynomial-time algorithms component logics available. follows Theorem 3.2 PT L2 [RCC-8] PT L2 [BRCC-8] NP-complete well.
3.2 Maximal Combinations (PC) (OC)
saw previous section, computational complexity spatio-temporal logics
without (OC) maximum complexity components, reflects
limited interaction spatial temporal operators languages without
means expressing (OC).
maximalist approach constructing spatio-temporal logics capable capturing
(PC) (OC) allow unrestricted applications Booleans, topological
temporal operators form spatio-temporal terms.
Denote PT L S4u spatio-temporal language given following definition:


::= p




::= 2

|
|

| 1 u 2


|

| 1 2

| 1 U 2

| 1 U 2

| 1 2 ,
| 1 2 .

Expressions form called spatio-temporal terms. Unlike previous section,
terms time-dependent. definition expressions form
PT L[S4u ]; called PT L S4u -formulas. languages
Section 3.1, including PT L[S4u ], clearly fragments PT L S4u .
before, introduce temporal operators 2F , 3F , well past
counterparts applicable formulas. Moreover, operators used form
spatio-temporal terms: example,
3F = > U ,

2F = 3F





= U ,

denotes empty set > whole space.
Spatio-temporal formulas supposed represent propositions speaking moving
spatial objects represented spatio-temporal terms. truth-values propositions
spatio-temporal structures vary time, depend points spacesthey
defined precisely way case PT L[S4u ]. understand
temporalised terms?
meaning clear: moment w, denotes space occupied
next moment w + 1 (see Fig. 2). example, write




Cyclone
3
u Cyclone
184

(C)

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

say regions Cyclone
introduction). formula

Cyclone

overlap (thereby formalising sentence (C)

EQ( EU, EU Romania Bulgaria)

(F)

says two years EU (as today) extended Romania Bulgaria.
Note EQ(EU, EU Romania Bulgaria) different meaning EU
may expand shrink year. also hard formalise sentences (D), (E) (H)
introduction:
EQ( X, ) EQ(Y, ),

(D)

2F EQ( Europe, Europe),

(E)

EQ(Earth, W L) EC(W, L) P(W, W ) P( L, L),

(H)

P(X, )X part denotes disjunction EQ(X, ), TPP(X, )
NTPP(X, ).
intended interpretation terms form 3F , 2F (and past counterparts)
bit sophisticated. reflects standard temporal meanings propositions
3F x 2F x , points x topological space:
moment w, term 3F interpreted union spatial extensions
moments v > w;
moment w, term 2F interpreted intersection spatial extensions
moments v > w.
example, consider Fig. 2 moving spatial object X depicted three consecutive
moments time (it change + 2). 3F X union X
X 2F X intersection X X (i.e., X).
another example, take spatial object Rain.
3F Rain moment w occupies space raining time points
v > w (which may different different places). 2F Rain w occupies space
always raining w.
2F 3F Rain w space raining ever ever w,
3F 2F Rain comprises places always raining starting
future moments time.
interpretation shows formalise sentence (G) introduction:
P(England, 2F 3F Rain).

(G)

Now, meaning Rain U Snow? Similarly readings 2F
3F above, adopt following definition:
moment w, spatial extension 1 U 2 consists points x topological
space v > w x belongs 2 moment v x 1
u whenever w < u < v.
185

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

past counterpart Ui.e., operator since Scan used say part
Russia remaining Russian since 1917 connected part Germany
(Konigsberg) became Russian Second World War (Kaliningrad):
DC(Russia Russian Empire, Russia Germany).
models = hF, T, Ui PT L S4u precisely topological temporal
models introduced PT L[S4u ]. However, need additional clauses defining
extensions spatio-temporal terms:

[
\
U(1 U 2 , w) =
U(2 , v)
U(1 , u) ,
v>w

U(1 2 , w) =

[

u(w,v)

U(2 , v)

v<w

\


U(1 , u) .

u(v,w)

also have:
U(3F , w) =

[

U(, v)



U(2F , w) =

v>w

\

U(, v),

v>w

and, discrete F,
U( , w) = U(, w + 1).
truth-values PT L S4u -formulas computed precisely way
case PT L[S4u ]. PT L S4u -formula called satisfiable exists tt-model
(M, w) |= time point w.
first sight may appear computational properties constructed logic badafter all, spatial temporal components PSPACEcomplete. turns out, however, case:
Theorem 3.3. satisfiability problem PT L S4u -formulas tt-models based
flows time hN, <i hZ, <i undecidable.
Without going details proof theorem, one might immediately conjecture use infinitary operators U, 2F 3F construction
spatio-temporal terms makes logic over-expressive. Moreover, whole idea
topological temporal models based infinite flows time may look counterintuitive
context spatio-temporal representation reasoning (unlike, say, models used
represent behaviour reactive computer systems).
different approaches avoid infinity tt-models. radical one
allow finite flows time. cautious approach impose following finite
change assumption models (based infinite flows time):
FCA term change spatial extension infinitely often.
means FCA consider valuations U tt-models hF, T, Ui
satisfy following condition: every spatio-temporal term , pairwise
disjoint intervals I1 , . . . , F = hW, <i W = I1 state
remains constant Ij , i.e., U(, u) = U(, v) u, v Ij . turns out, however,
186

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

case discrete flows time FCA give us anything new compared
arbitrary finite flows time. precisely, one easily show satisfiability
problem PT L S4u -formulas tt-models satisfying FCA based hN, <i
hZ, <i polynomially reducible satisfiability tt-models based finite flows time,
way round. Note also flows time mentioned above, FCA
captured formulas 3F 2F EQ(, F ) (and past counterpart hZ, <i),
every spatio-temporal term .
liberal way reducing infinite unions intersections finite ones
adopt finite state assumption:
FSA Every term may finitely many possible states (although may
change states infinitely often).
Say tt-model hF, T, Ui satisfies FSA if, every spatio-temporal term ,
finitely many sets A1 , . . . , space {U(, w) | w W } = {A1 , . . . , }.
models used, instance, capture periodic fluctuations due season
climate changes, say, daily tide. Similarly FCA finitising flow time, FSA
virtually makes underlying topological space finite. following proposition
proved Appendix B:
Proposition 3.4. PT L S4u -formula satisfiable tt-model FSA based
flow time F iff satisfiable tt-model based F finite (Aleksandrov )
topological space.
Unfortunately, none approaches works PT L S4u still have:
Theorem 3.5. (i) satisfiability problem PT L S4u -formulas tt-models based
(arbitrary) finite flows time undecidable.
(ii) satisfiability problem PT L S4u -formulas tt-models based flows
time hN, <i hZ, <i satisfying FSA undecidable.
next-time operator look harmful infinitary U, 2F , 3F ,
still capture aspects (OC) (see formulas (C), (D), (F) (G) above). let
us consider fragment PT L S4u PT L S4u spatio-temporal terms form:


::=

p

|

| 1 u 2

|

|

.

words, PT L S4u allow applications temporal operators different
form spatio-temporal terms (but still available formula constructors).
means compare states spatial object X bounded set time
points only: time point natural numbers n, 0, compare
state X + n state + m.
fragment definitely less expressive full PT L S4u . instance, according
Lemma B.1, PT L S4u -formulas distinguish arbitrary tt-models
based Aleksandrov topological spaceswe call Aleksandrov tt-models.
hand, set PT L S4u -formulas satisfiable Aleksandrov models
proper subset satisfiable arbitrary tt-models. Consider, example,
PT L S4u -formula
(2 Ip @ I2 p).
2
F
F
187

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

One readily see true every Aleksandrov tt-model, negation
satisfied topological model. suffices take flow F = hN, <i topology
= hR, Ii
standard interior operator real line, select sequence Xn
open sets nN Xn open, e.g., Xn = (1/n, 1/n), put U(p, n) = Xn .
However, even seemingly weak interaction topological temporal operators turns dangerous:
Theorem 3.6. satisfiability problem PT L S4u -formulas tt-models based
flows time hN, <i hZ, <i undecidable. undecidable well tt-models
satisfying FSA based (arbitrary) finite flows time.
Theorem 2.6 might suggest considering fragment PT L2 S4u 2F past
counterpart 2P temporal primitives applicable formulas terms:


::= p




::= 2

|
|

| 1 u 2


|

| 1 2

| 2F

| 2F

| 2P ,

| 2P .

Yet result negative:
Theorem 3.7. satisfiability problem PT L2 S4u -formulas tt-models (with
without FSA) based flows time hN, <i hZ, <i undecidable. undecidable
well tt-models based (arbitrary) finite flows time.
undecidability results (the strongest ones, Theorems 3.6 3.7,
precise) proved Appendix B.2 reduction Posts correspondence problem
known undecidable (Post, 1946). see proofs,
theorems actually hold future fragments corresponding languages.
3.3 Decidable Spatio-Temporal Logics (PC) (OC)
important lesson learn (the proofs of) negative results Section 3.2
full S4u expressive computationally well-behaved combinations fragments
PT L. hand, said Section 2.1.2, qualitative spatial representation
reasoning often requires extensions spatial variables regular closed (i.e., regions).
restriction important constructing decidable spatio-temporal logics
(PC) (OC). First, undecidability proofs Appendix B.2 go
case. second, shown below, decidable combinations PT L
fragments S4u introduced Section 2.1 exist. fact, construct
hierarchy decidable spatio-temporal logics different computational complexity
imposing various restrictions regions themselves, ways compared,
interactions spatial temporal constructors.
begin considering simplest combination PT L RCC-8 capturing (PC)
(OC). logic called PT LRCC-8 (it introduced name ST
1 Wolter
Zakharyaschev, 2002) operates spatio-temporal region terms form
%

::=

CIp

| CI %.

relate terms, allowed use eight binary predicates RCC-8; arbitrary temporal operators Boolean connectives applied produce PT L RCC-8
188

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

formulas. Typical examples formulas (A), (B), (D) (E) above. Note
(C) regarded PT L RCC-8 formula well (two regions overlap iff
neither disconnected externally connected). hand, (F), (H) (G)
PT LRCC-8 formulas first two use operation region terms (G)
uses temporal operators 2F 3F region terms.
before, PT L RCC-8 formulas interpreted topological temporal models (or
tt-models). However, discrete flows time make sense language. Although
interaction topological temporal operators similar PT L S4u
(clearly, PT LRCC-8 fragment PT LS4u ), following rather unexpected
encouraging result:
Theorem 3.8. satisfiability problem PT L RCC-8 formulas tt-models based
hN, <i, hZ, <i (arbitrary) finite flows time PSPACE-complete.
theorem proved Appendix C.5. idea proof similar
Theorem 3.1: consider spatial temporal parts given formula separately.
However, take account interaction parts, use so-called
completion property RCC-8 (cf. Balbiani & Condotta, 2002) respect certain
class C models: given satisfiable set RCC-8 formulas model C satisfying
subset , one extend partial model model C satisfying whole .
happens extend expressive power spatial component allowing
Boolean operators spatio-temporal region terms, i.e., jump RCC-8 BRCC-8?
Define spatio-temporal Boolean region terms taking
%

::=

CIp

| CI%

| CI(%1 u %2 ) | CI %.

Denote PT L BRCC-8 language obtained PT L RCC-8 allowing spatiotemporal Boolean region terms arguments RCC-8 predicates (this language
called ST 1 Wolter Zakharyaschev, 2002). Formulas (A)(F) (H) belong
PT L BRCC-8, (G) uses 2F 3F operators regions PT L
BRCC-8.
Now, another surprise replacement RCC-8 BRCC-8 temporal
context results exponential jump computational complexity (remember
RCC-8 BRCC-8 NP-complete):
Theorem 3.9. satisfiability problem PT L BRCC-8 formulas tt-models based
flows time hN, <i hZ, <i EXPSPACE-complete. EXPSPACE-complete
well models satisfying FSA based (arbitrary) finite flows time.
EXPSPACE upper bound (see Appendix C.3) proved polynomial embedding PT L BRCC-8 one-variable fragment QT L1 first-order temporal logic,
known EXPSPACE-complete (Hodkinson, Kontchakov, Kurucz, Wolter, &
Zakharyaschev, 2003). construct embedding, first show PT L BRCC-8
complete respect Aleksandrov tt-models. fact, prove every satisfiable
formula expressive logic PT L S4u introduced Section 3.2 satisfied
Aleksandrov tt-model (see Lemma B.1 discussion above). Lemma C.1
shows satisfy PT L BRCC-8 formula, suffices take Aleksandrov tt-model
189

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

based partial order depth 1. Lemma C.2, width partial order
bounded 2 (just case BRCC-8), therefore unions forks (or 2-brooms)
enough satisfy PT L BRCC-8 formulas. Aleksandrov tt-models based
unions forks encoded means unary predicates QT L1 .
EXPSPACE lower bound proved Appendix C.1 encoding corridor
tiling problem. also established direct polynomial embedding QT L1
PT LBRCC-8. illustrate idea, consider QT L1 -formula x (P (x) P (x)) saying
that, every point space, either P tomorrow.
statement expressed PT L BRCC-8 formula EQ(P P, E) DC(E, E),
last conjunct makes E empty.
let us make one step space extend BRCC-8 RC, thus obtaining
spatio-temporal language PT L RC following syntax:
%

::= CIp



::= %




::= 2

| CI%

| I%
|

| CI(%1 u %2 ) | CI %,

|



| 1 u 2 ,

| 1 2

| 1 U 2

| 2 2 .

reader surprised (although authors were) extra expressivity results one exponential gap:
Theorem 3.10. satisfiability problem PT L RC-formulas tt-models based
flows time hN, <i hZ, <i 2EXPSPACE-complete. 2EXPSPACE-complete
well models satisfying FSA based (arbitrary) finite flows time.
lower bound established Appendix C.1 upper bound Appendix C.2.
Perhaps, proper time closer look emerging landscape.
exactly causes exponential jumps ? locate precise borders ladder
PSPACEEXPSPACE2EXPSPACE?
analysing proof Theorem 3.8 (see Appendix C.5), note much
added RCC-8. fact, maximal spatio-temporal logic (denoted PT LRC 2 )
proof goes based spatio-temporal terms form
| CI %,

%

::= CIp



::= %



::= 1 u 2 .

| I%

| %

| I%,

hand, even addition predicates form EQ(X, Z) enough
make logic EXPSPACE-hard (see Remark C.3). Thus, PT L RCC-8 (or rather
extension PT L RC 2 ) located pretty close border PSPACE
EXPSPACE spatio-temporal logics.
following fragment RC RC indicates border EXPSPACE
2EXPSPACE may lie:
%

::= Boolean region terms,



::= %

| ,



::= I%

|



::= 1 u u

| 1 u 2 ,

190

| u

| .

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Intuitively, spatial terms interpreted regular closed regular open4
sets, respectively (the interior region regular open, complement regular closed
set regular open (and vice versa), regular closed sets closed unions regular
open ones closed intersections). Thus, regarded generalisation
region terms generalisation interiors regions. words, RC
fragment RC following ways relating regions available:
point regions meet;
region intersects interior another one;
interior region empty.
readily checked BRCC-8 fragment RC . Moreover, proper fragment
(4) belongs latter former. formula





( N orthKorea
2
u SouthKorea ) @ DmZone
(8)
(saying demilitarised zone North Korea South Korea consists
border along adjacent territories) shows RC
proper subset RC:
BRCC-8 $ RC $ RC.
Although RC extends BRCC-8, gives rise spatio-temporal logic
computational complexity:
Theorem 3.11. satisfiability problem PT L RC -formulas tt-models based
flows time hN, <i hZ, <i EXPSPACE-complete. EXPSPACE-complete
well models satisfying FSA based (arbitrary) finite flows time.
lower bound follows immediately Theorem 3.9 proof upper
bound similar Theorem 3.9 (see Appendix C.3). Again, due restriction
possible ways relating regions, polynomially bound width n n-brooms
required satisfy PT LRC -formulas (cf. Lemma C.2). fact, need formulas
similar (8) order increase complexity 2EXPSPACE.
constructed hierarchy decidable spatio-temporal logics still leaves least one
important question: exist decidable spatio-temporal logics allow applications
temporal operators U, 2F , 3F region terms complexity? Consider
languages PT L L, L {BRCC-8, RC , RC}, differ PT L L
definition spatio-temporal region terms:
%

::=

CIp

| CI%

| CI(%1 u %2 ) | CI(%1 U %2 ) | CI(%1 %2 ).

following two theorems provide positive (though partial) answer question:
Theorem 3.12. satisfiability problem PT L BRCC-8 PT L RC -formulas
tt-models based hN, <i hZ, <i satisfying FSA, based (arbitrary) finite
flows time EXPSPACE-complete.
4. Remember set X regular open ICX = X.

191

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Theorem 3.13. satisfiability problem PT L RC-formulas tt-models based
hN, <i hZ, <i satisfying FSA, based (arbitrary) finite flows time
2EXPSPACE-complete.
upper bounds mentioned two theorems proved Appendices C.3
C.2, respectively. lower bounds follow results PT LBRCC-8 (Theorem 3.9)
PT L RC (Theorem 3.10).
appreciate following theorem, reader recall PT L2 RC
NP-complete:
Theorem 3.14. satisfiability problem PT L2 BRCC-8 PT L2 RC -formulas
tt-models based hN, <i hZ, <i satisfying FSA, based (arbitrary) finite
flows time EXPSPACE-complete.
Actually consequence EXPSPACE-hardness QT L1 sole temporal
operator 2F (see Hodkinson et al., 2003).
Unfortunately, little known complexity spatio-temporal languages interpreted tt-models based dense arbitrary flows time. fact,
result know proved using recent work (Hodkinson, 2004; Hodkinson et al.,
2003):
Theorem 3.15. satisfiability problem PT L BRCC-8 PT L RC -formulas
tt-models satisfying FSA based hQ, <i, hR, <i arbitrary flows time belongs
2EXPTIME EXPSPACE-hard.

4. Conclusion
provided in-depth analysis computational complexity various spatiotemporal logics interpreted Cartesian products flows time topological spaces.
results collected Table 1. design languages driven
idea cover basic features spatio-temporal hybrids combining standard logics
time mereotopology, aim see complex reasoning
hybrids could be. try fine-tune languages real-world applications.
contrary, tried keep pure representative possible determine
computational challenges multi-dimensional approach reasoning space
time would face. research objective mind, discuss conclusions
drawn Table 1.
conclusion drawn undecidability results easy: try
implement sound, complete terminating algorithm supposed decide
satisfiability problem PT L S4u , PT L S4u PT L2 S4u never succeed.
decision procedures required, alternative languages devised.
interpretation complexity results decidable logics transparent:
well-known results provide us immediate conclusions regarding
behaviour implemented systems. example, sometimes algorithms running
exponential time worst-case perform better practical problems worst-case
optimal algorithms run polynomial time. Indeed, complexity results
analysed together proofsif significant conclusions required (cf. Nebel, 1996).
192

filanguage

n/a

spatial component L

flow
RCC-8

PT L[L]

N, Z,Q, R,
finite

arbitrary
N, Z

PT L2 L

RC

RC max

NP

PSPACE

PSPACE

(Thm. 2.4)

(Thm. 2.1)

NP

PSPACE

(Thm. 3.2)

(Thm. 3.1)

PSPACE
(Thm. 3.1)

PSPACE
(Thm. 3.8)


finite
EXPSPACE

PSPACE
N, Z+FSA

EXPSPACE

2EXPSPACE

(Thm. 3.9)

(Thm. 3.10)

?

?


finite
EXPSPACE EXPSPACE
2EXPSPACE

NP
EXPSPACE
(Thm. 3.14)
N, Z+FSA
arbitrary


2EXPTIME
2EXPTIME
?
Q, R
NP
EXPSPACE
FSA

undecidable
(Thm. 3.6)

undecidable
(Thm. 3.7)



N, Z

S4u

(Thm. 2.2)

NP

N, Z

PT L L

BRCC-8

N, Z,Q, R,
finite

arbitrary

PT L L

PT L2 [L]

time

L

Combining Spatial Temporal Logics: Expressiveness vs. Complexity

?

?

?

undecidable

?

(Thm. 3.3)


finite
EXPSPACE EXPSPACE

PSPACE
(Thm. 3.12)
N, Z+FSA

arbitrary
2EXPTIME

2EXPTIME

EXPSPACE
Q, R
PSPACE
(Thm. 3.15)
FSA

2EXPSPACE

?

(Thm. 3.13)

?

undecidable
(Thm. 3.5)

?

?

Table 1: Complexity satisfiability problem spatial spatio-temporal logics.

193

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

proofs show sources complexity whether could
relevant practical problems implementation algorithms.
respect proofs actually rather informative. decidability proof
PT L[S4u ] immediately provides us modular algorithm combining known procedures components. EXPSPACE-completeness results PT L BRCC-8
(with FSA) PT L BRCC-8 show extremely close link spatio-temporal
languages one-variable fragment first-order temporal logic. algorithmic
problems investigated context first-order temporal logic are, therefore,
character deal spatio-temporal context. Thus, experience
working algorithms (fragments of) first-order temporal logics (Hodkinson, Wolter,
& Zakharyaschev, 2000; Degtyarev, Fisher, & Konev, 2003; Kontchakov, Lutz, Wolter, &
Zakharyaschev, 2004) pretty good knowledge almost directly
translates insights possible algorithms spatio-temporal logics. PSPACEcompleteness result PT L RCC-8 obtained means reduction (modulo RCC-8
reasoning) PT L. conclude proof sufficient
good solvers RCC-8 PT L obtain reasonable prover PT L RCC-8.
interaction two components turned rather weak.
conclusion, complexity proofs clearly show algorithmic problems solved
dealing spatio-temporal logics presented paper. particular, devising
algorithms logics conceived part general enterprise
developing algorithms propositional one-variable fragment first-order temporal
logic.
comments explanations important results Table 1:
1. undecidability result PT L S4u , PT L S4u PT L2 S4u solves major
open problem Wolter Zakharyaschev (2002). shows that, S4u
suitable candidate efficient pure spatial reasoning (Bennett, 1996; Renz & Nebel,
1998; Aiello & van Benthem, 2002a), temporal extensions satisfying (PC)
(OC) suitable practical spatio-temporal representation reasoning.
2. Logics like PT L BRCC-8 may turn undecidable interpreted
arbitrary topological temporal models. One main origins expressive
power possibility form infinite intersections unions regions. However,
tame computational behaviour logics imposing natural restrictions
classes admissible models FSA.
3. PSPACE upper bound PT L RCC-8 EXPSPACE lower bound
PT L BRCC-8 solve two major open problems Wolter Zakharyaschev
(2002). interest note spatial fragments PT L RCC-8
PT L BRCC-8 computational complexity: NP-complete
arbitrary topological spaces. Thus additional Boolean connectives spatial
regions interacting next-time operator make logic substantially
complex.
4. 2EXPSPACE-completeness result PT L RC FSA PT L RC another example seemingly tiny increase expressiveness results significant
jump complexity.
194

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

5. PSPACE-completeness PT L RCC-8 particularly good news, since shows
combination PT L RCC-8 computational complexity
PT L itself, surprisingly fast systems implemented (Schwendimann, 1998; Hustadt & Konev, 2003). gives us hope practical algorithms
PT LRCC-8 implemented. Indeed, proof shows may possible
encode satisfiability problem PT LRCC-8 satisfiability problem
PT L use PT L provers. note complexity result conjectured Demri DSouza (2002) proof uses ideas Balbiani
Condotta (2002).
6. hand, EXPSPACE lower bounds PT L BRCC-8 FSA
PT LBRCC-8 necessarily mean reasoning logics hopeless.
fact, show regarded fragments one-variable firstorder temporal logic, tableau- resolution-based decision procedures
developed implemented (Degtyarev et al., 2003; Kontchakov et al., 2004).
course, many directions research spatio-temporal knowledge representation reasoning. mention closely related
logics considered above.
paper, confined considering linear flows time. may
interest, however, investigate computational properties spatio-temporal
logics based branching time paradigm (see, e.g., Clarke & Emerson, 1981;
Emerson & Halpern, 1985) order model uncertainty future. Recent
results Hodkinson, Wolter Zakharyaschev (2001, 2002) give hope
logics decidable.
confined considering mereotopological formalisms spatial
dimension. would also interest consider spatial logics directions (Ligozat,
1998), shape (Galton & Meathrel, 1999), size (Zimmermann, 1995), position (Clementini, Di Felice, & Hernandez, 1997), even hybrids (Gerevini & Renz, 2002).
note results direction recently obtained Balbiani
Condotta (2002) Demri DSouza (2002).
Another interesting important perspective spatial spatio-temporal
representation reasoning move arbitrary topological spaces
induced metric spaces introduce explicit and/or implicit numerical parameters.
First encouraging steps direction made work (Kutz, Sturm,
Suzuki, Wolter, & Zakharyaschev, 2003).
conclude paper number open problems:
1. precise computational complexity PT L BRCC-8 FSA
dense flows time arbitrary strict linear orders?
2. logics form PT L L PT L2 L, L {RC, BRCC-8, RCC-8},
decidable without FSA?
195

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

3. combinations PT L PT L2 RC max (satisfying (PC) (OC))
decidable?
4. PT L S4u undecidable dense flows time arbitrary strict linear orders?
5. PT L RCC-8 FSA decidable PSPACE?

Acknowledgments
work paper partially supported U.K. EPSRC grants no. GR/R45369/01,
GR/R42474/01, GR/S61966/01 GR/S63182/01. work third author also
partially supported Hungarian Foundation Scientific Research grants T30314
035192.
Special thanks due referees first version paper whose remarks,
criticism constructive suggestions led many days intensive exciting
research, new results and, hopefully, better paper.

Appendix A. Complexity Spatial Logics
appendix prove Theorems 2.2 2.4. proofs use fact S4u
(as well fragments) complete respect (finite) Aleksandrov topological spaces
(McKinsey & Tarski, 1944; Goranko & Passy, 1992). Recall p. 174 Aleksandrov
(topological ) model pair form = hG, Vi, G = hV, Ri quasi-order
V map set spatial variables 2V . convenient us
unify notation spatial formulas spatial terms write (M, x) |= instead
x V( ), spatial term x point V . particular, definition
interior closure operators Aleksandrov spaces,
(M, x) |=

iff

(M, x) |= C

iff


V xRy (M, y) |= ,

V xRy (M, y) |= .

length `() formula understand number subformulas subterms occurring .
Proof Theorem 2.2. proof follows Lemmas A.1 A.2 show
together every satisfiable RC-formula satisfied Aleksandrov model size
polynomial (in fact, quadratical) length input formula (in words, RC
polynomial finite model property). Thus, nondeterministic polynomial time
algorithm satisfiability problem.
q
fact, Lemma A.1 shows RC complete respect subclass Aleksandrov
spaces, namely, finite disjoint unions finite brooms. Recall p. 179 broom
partial order b form h{r} V0 , Ri, R reflexive closure {r} V0
(see Fig. 4). call r root b points V0 leaves b; also referred
points depth 1 0, respectively. broom b said -broom, ,
|V0 | . particular, call broom finite n-broom, n < .
196

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Lemma A.1. Every satisfiable RC-formula satisfied Aleksandrov model based
finite disjoint union finite brooms.
Proof. well-known, RC-formula satisfiable satisfied finite
Aleksandrov model = hG, Vi, G = hV, Ri. Define new relation R0 V taking R0
reflexive closure R (V1 V0 ),
V0 = {x V | (xRy yRx)}

V1 = V V0 .

(Without loss generality may assume V1 6= V0 one
proper R-predecessor.) Let G0 = hV, R0 M0 = hG0 , Vi. Clearly, G0 partial order
required. prove that, every RC-formula ,
|=

iff

M0 |= .

(9)

First show that, every Boolean region term % every x V ,
(M0 , x) |= %

iff

(M, x) |= %.

(10)

definition, (M0 , x) |= p iff (M, x) |= p, every spatial variable p. readily seen
every V0 every spatial term , (M0 , y) |= iff (M, y) |= . Now, %
Boolean region term % = CI spatial term , clearly have:

(M, x) |= CI iff V xRy z V (yRz (M, z) |= )

iff V0 xR0 (M, y) |=

iff V0 xR0 (M0 , y) |=

iff V0 xR0 (M0 , y) |=
iff

(M0 , x) |= CI.

Next, extend (10) spatial terms form I% % Boolean region term.
(M, x) |= I% (M, y) |= % whenever xRy, so, R0 R, (M0 , x) |= I%.
Conversely, suppose (M0 , x) |= I%. Take xRy z V0 yRz.
claim (M, z) |= %. Indeed, x V1 follows IH xR0 z. x V0
zRx. Since (M0 , x) |= %, IH % = CI , obtain (M, z) |= %. (M, y) |= %
follows yRz % = CI . Thus, (M, x) |= I%.
Finally, easily extend (10) arbitrary spatial terms formulas RC
constructed spatial terms form % I%, % Boolean region term,
using operators depend structure underlying partial order. Thus
(9).
q
Lemma A.2. Every satisfiable RC-formula satisfied Aleksandrov model based
disjoint union `() many 2`()-brooms.
Proof. Remember every RC-formula (equivalent to) Boolean combination
5
,...,3

, spatial term
spatial formulas set = {3
1
}. 3

5. following proof consider

abbreviation 3
primary 2
.
3

197

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

also Boolean (or rather set-theoretic) combination %1 , . . . , %k , I%01 , . . . , I%0m ,
%i %0i Boolean region terms.
follows Lemma A.1 satisfied Aleksandrov model = hG, Vi,

G = hV, Ri finite disjoint union finite brooms. every 3


|= 3 , fix point x V (M, x ) |= . may assume x
.
pairwise distinct roots brooms points form x 3

.
Therefore, G disjoint union `() many finite brooms b , 3

,
Let us construct new model M0 follows. broom b , 3

% , pick
leaf y,% b (if any) (M, y,% ) |= %,
leaf y,% b (if any) (M, y,% ) |= %
remove leaves b . Denote b0 resulting broom. Clearly, 2`() , M0 = hG0 , Vi.
broom. Let G0 = hV 0 , R0 disjoint union b0 , 3

easy see G0 required.
,
Now, show satisfied M0 , suffices prove that, 3


M0 |= 3

iff

.
|= 3

(11)

definition M0 , leaves G0 spatial terms ,
(M0 , y) |=

iff

(M, y) |= .


Next, every root x b , every 3
every % , (M, x ) |= % iff
leaf x Ry (M, y) |= % (simply % = CI, ).
follows construction M0 (M, x ) |= % iff (M0 , x ) |= %, every % .
also follows (M, x ) |= I% implies (M0 , x ) |= I%. Conversely, (M0 , x ) |= I%,
(M, x ) 6|= I% leaf x Ry (M, y) 6|= % contradiction.
Since intersection complement depend structure underlying frame,
(M0 , x ) |= iff (M, x ) |= , every root x b , proves (11).
q

Proof Theorem 2.4. PSPACE upper bound follows Theorem 2.1. proof
PSPACE-hardness reduction validity problem quantified Boolean formulas
known PSPACE-complete (Stockmeyer, 1987). slightly modify
proof Ladner (1977) (that shows PSPACE-hardness S4), order take
account variables RC max -formulas always prefixed CI.
may assume quantified Boolean formulas form
= Q1 p1 . . . Qn pn 0 ,
Qi {, } 0 Boolean formula variables p1 , . . . , pn . well known,
possible truth assignments p1 , . . . , pn arranged leaves full binary
tree depth n. left subtree root contains truth assignments p1 true
right subtree p1 false; branch p2 , p3 , on.
determine whether valid pruning full binary tree: whenever Qi ,
keep subtrees ith level, whenever Qi one them.
198

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

00
q 3 , p 1 , p2 , p 3

r


3@

00
q 3 , p 1 , p2

00
q 3 , p 2 , p3

r
3

00
q 3 , p2

r

r
3


3@

@

@
@
q ,p ,p
2@r - r 2 1 2
6
1

@
@
2 r -r
6

r - r q 1 , p1
YH
H
HH
H

1

r -r

q 2 , p2

q1

*





HH

r rq
H

0

0

Figure 6: Aleksandrov model may satisfy , = p1 p2 p3 0 .
way end tree leaves evaluate 0 true, valid,
otherwise not.
generate leaves binary tree Aleksandrov models help
RC max -formula. precisely, construct RC max -formula
length polynomial length ,
satisfied Aleksandrov model iff valid.
Take fresh spatial variables q0 , . . . , qn , put, = 0, . . . , n,

q0 u q1
= 0;





=
qi1 u qi u qi+1 ,
0 < < n;







qn1 u qn ,
= n.
consider variables p1 . . . , pn
variables, let 00 result
spatial
replacing every occurrence pi pi 0 . Put
^
^




+

+
00




= 3
2
2
0
i1 @ (i )
i1 @ (i u ) 2
n @ ,
Qi =

Qi =

where, = 1, . . . , n,

= C u pi




i+ = C u pi .

Clearly, RC max -formula length polynomial length .
Suppose first valid. Fig. 6 shows structure possible Aleksandrov
model satisfying .
converse direction similar Ladners proof (1977). Suppose
satisfied
Aleksandrov model M. Then, necessary sequence truth values

p1 , . . . , pn , point reflecting sequence (we use structure
spatial terms here). Since, last conjunct , 00 holds
points, obtain quantified Boolean formula must valid.
q
199

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Appendix B. Spatio-Temporal Logics Based S4u
appendix prove Theorems 3.1, 3.2, 3.6 3.7 well Proposition 3.4.
Theorems 3.3 3.5 immediate corollaries Theorem 3.6. first, general
results established used later on.
remind reader Aleksandrov tt-model mean tt-model based
Aleksandrov (topological) space. Every model regarded triple form
K = hF, G, Vi, F = hW, <i flow time, G = hV, Ri quasi-order, V map
associating every spatial variable p every time point w W set V(p, w) V .
Appendix A, instead x V(, w) write (K, hw, xi) |= unify notation
spatio-temporal formulas terms.
Given spatio-temporal formula , denote sub set subformulas
term set spatio-temporal terms occurring .
Lemma B.1. (i) PT L S4u -formula satisfied tt-model FSA based
flow time F, satisfied Aleksandrov tt-model FSA based
F.
(ii) PT L S4u -formula satisfied tt-model based flow time F,
satisfied Aleksandrov tt-model based F well.
Moreover, cases choose Aleksandrov tt-model K = hF, G, Vi satisfying
(with F = hW, <i G = hV, Ri) way w W , x V
spatio-temporal terms , set
Aw,x, = {y V | xRy (K, hw, yi) |= }
contains R-maximal point 6 (provided course Aw,x, 6= ).
Proof. proof uses StoneJonssonTarski representation topological Boolean
algebras (in particular, topological spaces) form general frames (see, e.g., Goldblatt,
1976 Chagrov & Zakharyaschev, 1997).
(i) Suppose satisfied tt-model = hF, T, Ui FSA based
topological space = hU, Ii. Denote V set ultrafilters U . two
ultrafilters x1 , x2 V , put x1 Rx2 iff U (IA x1 x2 ). easy see R
quasi-order V . Define Aleksandrov tt-model K = hF, G, Vi taking G = hV, Ri
V(p, w) = {x V | U(p, w) x}. show induction construction
spatio-temporal term that, w W x V ,
(K, hw, xi) |=

iff

U(, w) x.

(12)

basis induction case Booleans trivial. case = 0
standard (consult Goldblatt, 1976 Chagrov & Zakharyaschev, 1997).
Case = 1 U 2 . Assume (K, hw, xi) |= 1 U 2 . v > w
(K, hv, xi) |= 2 (K, hu, xi) |= 1 u interval (w, v). IH, U(2 , v) x
U(1 , u) x u (w, v). Since
\
U(1 U 2 , w) U(2 , v)
U(1 , u),
u(w,v)

6. point z said R-maximal V if, every z 0 A, z 0 Rz whenever zRz 0 .

200

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

shall U(1 U 2 , w) x show
\
U(1 , u) x.
U(2 , v)

(13)

u(w,v)

view FSA, find time points u1 , . . . , ul (w, v)
\
U(1 , u),
U(1 , u1 ) U(1 , ul ) =
u(w,v)

yields (13) ultrafilters closed finite intersections.
Conversely, let U(1 U 2 , w) x. FSA, time points v1 , . . . vl

[
\
U(1 , u) .
U(1 U 2 , w) =
U(2 , vi )
1il

u(w,vi )

since x ultrafilter,
U(2 , vi )

\

U(1 , u) x,

u(w,vi )

i, 1 l. Therefore, IH, (K, hvi , xi) |= 2 (K, hu, xi) |= 1
u (w, vi ). Hence (K, hw, xi) |= 1 U 2 .
Case = 1 2 considered analogously.
Now, show that, w W spatio-temporal terms ,

(K, w) |= 2

iff

U(, w) = U.

. (K, hw, yi) |= V , so, IH, U(, w)
Suppose (K, w) |= 2
V . U(, w) = U . Conversely, U(, w) = U U(, w)
.
V , which, IH, (K, w) |= 2
follows immediately satisfied K. also clear K satisfies
FSA. proves (i). existence R-maximal points sets form Aw,x, (where
w W , x V spatio-temporal term) follows result Fine (1974); see
also (Chagrov & Zakharyaschev, 1997, Theorem 10.36).

(ii) construction (i). First show induction that, every
spatio-temporal term PT L S4u , (K, hw, xi) |= iff U(, w) x. time, however,
instead U need inductive step .
Case = 0 . (K, hw, xi) |= 0 iff exists immediate successor w0
w (K, hw0 , xi) |= 0 iff, IH, immediate successor w0 w
U( 0 , w0 ) x. remains recall U( 0 , w) = U( 0 , w0 ) whenever w0 immediate
successor w U( 0 , w) = whenever w immediate successor.
remaining part proof (i).
q
Proof Proposition 3.4. implication () follows immediately definition.
() Suppose PT LS4u -formula satisfied tt-model FSA flow
time F = hW, <i. Then, Lemma B.1 (i), satisfiable Aleksandrov tt-model
= hF, G, Vi FSA based quasi-order G = hV, Ri. view FSA,
201

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

every term , finitely many sets A1 , . . . , Ak V {V(, w) | w
W } = {A1 , . . . , Ak }. Therefore, finitely many time points w1 , . . . , wm W
that, every w W , wi , 1 m, V(, w) = V(, wi ) term .
use Lemmon filtration (see, e.g., Chagrov & Zakharyaschev, 1997) construct
tt-model based finite Aleksandrov topological space. First, define equivalence
relation V taking x
(M, hwi , xi) |=

iff

(M, hwi , yi) |= ,

i, 1 m, term .

Denote [x] equivalence class x V . set V / pairwise distinct equivalence
classes clearly finite. Define binary relation V / taking [x]S[y]
(M, hwi , yi) |=

whenever (M, hwi , xi) |= I,

i, 1 m, term .

Clearly, well-defined, reflexive transitive, G0 = hV / , Si finite quasiorder. Let V0 (p, w) = {[x] | x V(p, w)}, every spatial variable p every w W .
Consider tt-model M0 = hF, G0 , V0 i. First show term , x V
w W ,
(M, hw, xi) |=
iff
(M0 , hw, [x]i) |= .
basis induction follows definition V0 , cases intersection complement trivial, temporal operators follow IH.
Suppose (M, hw, xi) |= [x]S[y]. moment wi
(M, hw, zi) |= iff (M, hwi , zi) |= , term z V . definition S,
(M, hwi , yi) |= , (M, hw, yi) |= . Finally, IH, (M0 , hw, [y]i) |= ,
since arbitrary, obtain (M0 , hw, [x]i) |= .
Conversely, let (M0 , hw, [x]i) |= xRy. [x]S[y], (M0 , hw, [y]i) |= ,
which, IH, (M, hw, yi) |= . Thus, (M, hw, xi) |= .
Finally, straightforward induction structure , one show
(M, w) |=

iff

(M0 , w) |= ,

sub w W . follows satisfied M0 .

q

B.1 Temporalisations S4u
Lemma B.2. Let finite set S4u -formulas. finite quasi-order G
every satisfiable subset satisfied Aleksandrov model based G.
Proof. every satisfiable , fix model based finite quasi-order G = hV , R
satisfying . Let n = max{|V | : , satisfiable} let G disjoint
union n full n-ary (transitive) trees depth n whose nodes clusters cardinality
n. difficult see every G p-morphic image G. Therefore, every
satisfiable satisfied Aleksandrov model based G.
q
Proof Theorem 3.1. PSPACE-hardness follows Theorem 2.1 2.5. show
matching upper bound.
Let PT L[S4u ]-formula. Since PT L S4u -formula, Lemma B.1 (ii),
satisfiable tt-model iff satisfiable Aleksandrov tt-model based
202

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

associate fresh propositional
flow time. every (spatial) subformula 2

variable p denote PT L-formula results replacing
p .
subformulas form 2
claim satisfiable Aleksandrov

tt-model flow time F = hW, <i iff

exists temporal model N = hF, Ui satisfying and,
| (N, w) |= p } {2
| (N, w) |= p } spatial
every w W , set w = {2


formulas satisfiable.

implication () obvious. Conversely,
suppose temporal model N
satisfying conditions above. Let = wW w . Lemma B.2, finite
quasi-order G that, every w W , hG, Vw |= w valuation
Vw . clear satisfied Aleksandrov tt-model hF, G, Vi,
V(p, w) = Vw (p), every spatial variable p every w W .
Now, devise decision procedure PT L[S4u ] uses polynomial space
length input formula, one take corresponding nondeterministic PSPACE
algorithm PT L (Sistla & Clarke, 1985; Reynolds, 2004, 2003) modify follows.
algorithm constructs pure temporal model N = hF, Ui every time
produces state time instant w W , additionally checks whether set w
spatial formulas satisfiable. Theorem 2.1, extra test also performed
PSPACE algorithm, increase complexity combined algorithm. q
Proof Theorem 3.2. proof essentially Theorem 3.1,
nondeterministic polynomial-time algorithms component logics available.
q
B.2 Undecidability PT L S4u PT L2 S4u
Note although spatio-temporal languages contain propositional variables,
p regarded proposition.
still simulate them: spatial variable p, formula 2
p, spatial
Thus, follows propositional variable p mean formula 2
variable p (note different typefaces used denote propositional spatial variables).
Proof Theorem 3.6. proof reduction undecidable Posts (1946) correspondence problem PCP, short. formulated follows. Given finite alphabet
finite set P pairs hv1 , u1 , . . . , hvk , uk nonempty finite words


ff


ff
vi = bi1 , . . . , bili ,
ui = ci1 , . . . , ciri
(i = 1, . . . , k)
A, instance PCP, decide whether exist N 1 sequence i1 , . . . ,
indices
vi1 viN = ui1 uiN ,
(14)
concatenation operation. construct (using future-time temporal
operators) PT L S4u -formula A,P
(i) length A,P polynomial function size P ;
(ii) A,P satisfiable tt-model based hN, <i exist N 1
sequence i1 , . . . , indices (14) holds;
203

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

(iii) exist N 1 sequence i1 , . . . , indices (14) holds
A,P satisfiable tt-model FSA based hN, <i;
(iv) A,P satisfiable tt-model based hN, <i iff A,P satisfiable tt-model
based finite flow time.
case hZ, <i follows immediately. Lemma B.1 (ii), suffices consider
Aleksandrov tt-models A,P .
build A,P using spatial variables lefta righta (a A), left, right stripe,
well propositional variables pairi , every pair hvi , ui i, 1 k, range.
variable range required relativise temporal operators 2F 3F order
ensure construct model based finite flow time. variable stripe
used introduce new strict closure operator Aleksandrov spaces taking, every
spatio-temporal term ,


= stripe @ C(stripe u C ) u stripe @ C(stripe u C ) .
Denote Sn sequence n operators S. abbreviations need 1 2
stands (1 @ 2 ) u (2 @ 1 ) 2+
F replaces 2F .
formula A,P defined conjunction
A,P = range stripe pair eq left right ,

range = range 3F range 2F (range 2F range),


_
^
3
range

pair

(pair

pair
)
,
pair = 2+
F


j
F
1ik


(stripe stripe) ,
3F range 2


^
(left
= 3F range
2

right
)

,

stripe =
eq

1i<jk

2+
F

aA

left conjunction (15)(21), 1 k,
^
G


+

2+
lefta ,
F 3 lefta u leftb 2F 2 left
aA

a6=b
a,bA

^

(15)




2+
F pairi 2(lefta @ lefta ) ,

(16)

aA

left 2+ 2
2
F (left @ Sleft),

2+
F
2+
F

(17)

(left @
pairi 2
,
^
((Sj left u Sj+1 left) @ left
2
pairi
b



Sli left)

li j

j<li
left

pairi 3
,

(18)

) ,

(19)
(20)

((left u Sleft) @
2F pairi 2

204

left

) ,

(21)

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity
b

b

b

b

yn4 b
.
..
.
.
.

b

b

b

.
.
.

.
.
.

.
.
.

b

b

b

yn3 +1 b
yn3 b
.
..
.
.
.

yn2 +1 b
yn2 b
.
..
.
.
.

yn1 +1 b
yn1 b

.
..
.
.
li
.
1
b
y1
pairi1
0

b

b

.
.
.

.
.
.

b

b

b

trb


li
3

b
btr


li
4

trb

bili4
vi4

. 4
.
.

br

bi14

br

bili3

. 3
.
.

vi3
rb

br

bi13

rb

br

bili2

. 2
.
.


.
.
l
.
i2
b

rb

rb

br

bi12

trb

rb

rb

br

bili1

vi2

. 1
.
.

vi1
rb

rb

pairi2
1

b = left
r = left
= left u Sleft

rb

pairi3
2
range

pairi4
3

br

bi11

4

...

Figure 7: Model satisfying left , N = 4.

ileft = leftbi u leftbi u S(leftbi u u Sleftbi ) . . .
1

2

3



li

(remember li length word vi ). conjunct right defined replacing
left occurrences left right, lefta righta (for A), li ri ileft
iright , defined similarly. (Note pairi occurs left right .)
Let us prove A,P required. Suppose (M, 0) |= A,P , Aleksandrov
tt-model = hhN, <i , G, Vi G = hV, Ri. Since (M, 0) |= eq , find N ,
1 N < ,
^
(left
(M, N ) |= range
2
(22)
righta ).
aA

view range , (M, j) |= range j, 0 j N . Let i1 , . . . ,
sequence indices that, 1 j N , (M, j 1) |= pairij (pair ensures
unique sequence sort). claim (14) holds sequence.
Since stripe holds 0, have, every V , (M, h0, yi) |= stripe iff
(M, hj, yi) |= stripe j, 0 j N . Denote Rs transitive binary relation
V defined taking xRs z V xRzRy (M, h0, xi) |= stripe
holds iff (M, h0, zi) 6|= stripe. clearly that, every j, 0 j N , every
xV,
(M, hj, xi) |=

iff

V xRs (M, hj, yi) |= .

Call sequence hy1 , . . . , yl (not necessarily distinct) points V Rs -path
V(left, j) length l y1 , . . . , yl V(left, j) y1 Rs y2 Rs . . . Rs yl . every sequence
205

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

z1 , . . . , zl points V(left, j) define
leftwordj (z1 , . . . , zl ) = ha1 , . . . , al ,
ai (uniquely determined (15)) symbols (M, hj, zi i) |= leftai .
show that, every j, 1 j N , following holds:


ff
(a) exists Rs -path y1 , . . . , ynj V(left, j) length nj = li1 + + lij

leftwordj (y1 , . . . , ynj ) = vi1 . . . vij ;
(b) every Rs -path V(left, j) length nj ;


ff
(c) every Rs -path y1 , . . . , ynj V(left, j),
leftwordj (y1 , . . . , ynj ) = vi1 . . . vij .
Indeed, j = 1, (a) (M, 0) |= pairi1 (20), (b) (17) (18),
(c)

(19).ff assume inductively (a)(c) hold j, 1 j < N . Let
y1 , . . . , ynj maximal Rs -path V(left, j). First, (16), y1 , . . . , ynj V(left, j + 1).


ff
Second, since (M, j, ynj ) |= left u Sleft (M, j) |= pairij+1 , (21) implies
ff


exist ynj +1 , . . . , ynj +lij+1 y1 , . . . , ynj +lij+1 Rs -path V(left, j + 1),
required
(a). (b)
ff (c), observe first every Rs -path hy1 , . . . , yl
V(left, j + 1), y1 , . . . , yllij+1 Rs -path V(left, j), (18). l nj+1 must
hold. l = nj+1 leftwordj (y1 , . . . , yllij+1 ) = vi1 . . . vij induction hypothesis, leftwordj+1 (y1 , . . . , yllij+1 ) = vi1 . . . vij (16). hand,
leftwordj+1 (yllij+1 +1 , . . . , yl ) = vij+1 (19), leftwordj+1 (y1 , . . . , yl ) =
vi1 . . . vij vij+1 , required.
repeat argument right side well. every sequence
z1 , . . . , zl points V(right, j), define
rightwordj (z1 , . . . , zl ) = ha1 , . . . , al ,
ai uniquely determined elements (M, hj, zi i) |= rightai .
have, every 1 j N :


ff
(a0 ) Rs -path y1 , . . . , ymj V(right, j) length mj = ri1 + + rij
rightwordj (y1 , . . . , ymj ) = ui1 . . . uij ;
(b0 ) every Rs -path V(right, j) length mj ;


ff
(c0 ) every Rs -path y1 , . . . , ymj V(right, j),
rightwordj (y1 , . . . , ymj ) = ui1 . . . uij .
206

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Now, (15) (22), V(left, N ) = V(right, N ). (a), exists Rs path hy1 , . . . , yl V(left, N ) l = nN leftwordN (y1 , . . . , yl ) = vi1 . . . viN .
(b0 ), nN mN . Similarly, using (a0 ) (b), obtain mN nN ,
nN = mN . Hence, (c0 ), rightwordN (y1 , . . . , yl ) = ui1 . . . uiN . Since, (22),
leftwordN (y1 , . . . , yl ) = rightwordN (y1 , . . . , yl ),
finally obtain vi1 . . . viN = ui1 . . . uiN , required.
Conversely, suppose N 1 sequence i1 , . . . , (14) holds.
show A,P satisfiable Aleksandrov tt-model = hhN, <i , hN, , Vi
FSA. Let nj = li1 + + lij mj = ri1 + + rij every j, 1 j N .
assumption, nN = mN
vi1 . . . viN = ha1 , . . . , anN = ui1 . . . uiN .
Define valuation V taking
V(range, j) true iff 0 j N ,
V(stripe, j) = {2m | < , 0 j N },
V(pairi , j 1) true iff = ij 1 j N ,
V(lefta , j) = {k | 1 k nj , ak = a} 1 j N ,
V(righta , j) = {k | 1 k mj , ak = a} 1 j N ,


V(righta , j).
V(lefta , j) V(right, j) =
V(left, j) =
aA

aA

One easily check valuation (M, 0) |= A,P satisfies
FSA. also readily seen A,P satisfiable tt-model based hN, <i iff
satisfiable tt-model based finite flow time.
q
Proof Theorem 3.7. show modifying formulas proof Theorem 3.6. First, replace stripe
+

2+
F 2(stripe @ 2F stripe) 2F 2(stripe @ 2F stripe).

Then, left conjunction (150 )(210 ), 1 k,
^
G


+

2+
lefta ,
F 3 lefta u leftb 2F 2 left
aA

a6=b
a,bA

^

(150 )



2+
F pairi 2(lefta @ 2F lefta ) ,

(160 )

aA

left 2+ 2
2
F (left @ Sleft),

2+
F
2+
F

(170 )


(left @ 3 Sli left) ,
pairi 2
F
^
(left u 2 left) @ 2 ((Sj left u Sj+1 left) @ left
pairi
2
F
F
b

li j

j<li
left

pairi 2F 3
,

2F

(180 )

) ,

(190 )
(200 )

left
((left u Sleft) @ 2
pairi 2
F
) ,

207

(210 )

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

ileft defined exactly proof Theorem 3.6. Formula right modified
similar way.
q
Remark B.3. fact, set PCP instances without solutions recursively enumerable therefore, proofs show sets PT L S4u PT L2 S4u formulas true models based hN, <i, hZ, <i finite flows time
recursively enumerable either. Therefore, logics recursively axiomatisable.

Appendix C. Spatio-Temporal Logics Based RC
appendix establish lower upper complexity bounds wide range
decidable spatio-temporal combinations and, particular, prove Theorems 3.83.15.
begin straightforward generalisation Lemma A.1 spatio-temporal case:
Lemma C.1. (i) PT L RC-formula satisfiable tt-model FSA based
flow time F satisfiable Aleksandrov tt-model based F finite
disjoint union finite brooms.
(ii) PT L RC-formula satisfiable tt-model based flow time F
satisfiable Aleksandrov tt-model based F (possibly infinite) disjoint union
-brooms.
Proof. (i) Lemma B.1 (i), satisfiable Aleksandrov tt-model based F
finite quasi-order G. rest proof similar Lemma A.1. details
left reader.
(ii) Lemma B.1 (ii), satisfiable Aleksandrov tt-model based F
quasi-order G = hV, Ri. rest proof similar Lemma A.1.
note although G infinite, still every x V V0
xRy. guaranteed condition set Aw,x,> maximal point.
q
Observe Aleksandrov spaces essentially infinite case (ii) Lemma C.1
generalisation Lemma A.2 go through. First, easily enforce topological
space infinite using PT L RCC-8 formula

2+
F NTTP(p, p).

Moreover, formula




( p
p )
3
u p ) 2+
F 2( p @


2+
F2







p u p @ p u p u p

satisfied Aleksandrov tt-model based single -broom, cannot satisfied
Aleksandrov tt-model based union n-brooms finite n.
hand, Aleksandrov tt-models based disjoint unions n-brooms,
n bounded width formula, enough spatio-temporal logics based
RC . Recall spatial terms PT L RC (and PT L RC ) defined follows


::= %

| ,



::= I%

|



::= 1 u u

| 1 u 2 ,

208

| u

| ,

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

% spatio-temporal Boolean region terms PT L BRCC-8 (and PT L
BRCC-8, respectively). hard see that, every tt-model = hF, T, Vi
F = hW, <i, = hU, Ii every w W ,
V(, w) = CIV(, w)



V(, w) = ICV(, w),

(23)

i.e., always interpreted regular closed sets, whereas regular open ones.
define width w() PT L RC -formula maximal number
( u u
conjuncts subformulas form 2
1
), subformulas exist, put
w() = 1 otherwise.
Lemma C.2. (i) PT L RC -formula satisfiable tt-model FSA
based flow time F satisfiable Aleksandrov tt-model based F
finite disjoint union w()-brooms.
(ii) PT L RC -formula satisfiable tt-model based flow time F
satisfiable Aleksandrov tt-model based F (possibly infinite) disjoint
union w()-brooms.
Proof. Lemma C.1, may assume satisfied Aleksandrov tt-model =
hF, G, Vi, F = hW, <i G = hV, Ri disjoint union brooms (in (i), union
brooms finite). Without loss generality may assume composed
,...,3
},7
(using temporal operators Booleans) formulas set = {3
1
n
every one following forms
1 u u ,

u



,

(24)

, defined above.

, fix point x
every 3
every w W (M, w) |= 3
,w V
(M, hw, x,w i) |= . may assume x,w pairwise distinct
.
roots brooms points form x,w w W 3


Therefore, G disjoint union brooms b,w , 3
w W .
Let us construct model M0 = hF, G0 , V0 follows. Given broom b,w , delete
leaves depending form . Three cases possible:
Case = 1 u u : take leaves y1 , . . . , ym b,w (M, hw, yi i) |=
x,w Ryi = 1, . . . , remove leaves different y1 , . . . , ym .
Case = u : take leaf b,w (M, hw, yi) |= x,w Ry remove
leaves. Note that, (23), (M, hw, yi) |= , therefore (M, hw, yi) |= .
Case = : take leaf b,w x,w Ry remove leaves. (23),
(M, hw, yi) |= .
Denote b0,w resulting broom. Clearly, w()-broom. Let G0 = hV 0 , R0
0

disjoint union b,w , 3
w W . clear G
0
required. Finally, define V taking every spatial variable p, every w W
every x V 0 ,
x V0 (p, w)
7. treat

iff

V 0 depth 0 xR0 V(p, w).

abbreviation.
primitive 2
3

209

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

,
show satisfied M0 , first prove that, w W 3


(M0 , w) |= 3

iff

.
(M, w) |= 3

readily proved induction (M0 , hw, xi) |= iff (M, hw, xi) |= ,
points x V 0 depth 0, w W spatio-temporal terms .

Then, construction, also that, formulas 3
w W ,
0


implies
(M, w) |= 3 implies (M , w) |= 3 . remains show (M, w) |= 3
0
0
3

(M , w) |= 3
w W . Suppose (M , hw, xi) |=
. Consider three possible cases :
(M, w) |= 3
Case = 1 u u . Then, every i, 1 m, yi V 0 depth 0
xR0 yi (M0 , hw, yi i) |= . (M, hw, yi i) |= and, (23), (M, hw, xi) |= .
.
Therefore, (M, hw, xi) |= , contrary (M, w) |= 3
0
Case = u . V depth 0 xR0 y, (M0 , hw, yi) |= and,
(23), (M0 , hw, yi) |= . Thus (M0 , hw, yi) |= . (M, hw, yi) |= , contrary
.
(M, w) |= 3
Case = . V 0 depth 0 xR0 and, (23), (M0 , hw, yi) |= .
.
(M, hw, yi) |= , contrary (M, w) |= 3
Now, straightforward induction easily show that, w W
formulas built using temporal operators Booleans,
(M0 , w) |=

iff

(M, w) |= .

follows satisfied M0 .

q

C.1 Lower Complexity Bounds (I)
Proof Theorem 3.10, lower bound. proof reduction arbitrary problem 2EXPSPACE satisfiability problem PT L RC. Let (single-tape,
deterministic) Turing machine halts every input (accepting rejecting it),
f (n)
uses 22
cells tape input length n, polynomial f . Given
Turing machine input x it, construct PT L RC-formula
A,x (using future-time temporal operators)
(i) length A,x polynomial size x;
(ii) A,x satisfiable tt-model based hN, <i accepts x;
(iii) accepts x A,x satisfiable tt-model FSA based hN, <i.
case hZ, <i flow time (with without FSA) follows immediately. case
finite flows time proved relativising temporal operators A,x (say,
propositional variable range proof Theorem 3.6 Appendix B.2
proof lower bound Theorem 3.9 below): obtain formula 0A,x
0A,x satisfiable Aleksandrov tt-model based hN, <i iff satisfiable
Aleksandrov tt-model based quasi-order finite flow time.
way lower bound results theorem follow.
210

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Given Turing machine A, polynomial f , input x = hx1 , . . . , xn above, let
= f (n),
exp(1, d) = 2d exp(2, d) = exp(1, d) 2exp(1,d) .

22

f (n)

exp(2, d).

(25)

plan follows. First, show yardsticks length exp(2, d) (similar
used Stockmeyer, 1974 Halpern Vardi, 1989) encoded PT L RCformulas length polynomial d. yardsticks used define temporal
operator exp(2,d) . Then, using operator, encode computation
input x.
Lemma C.1 (ii), PT LRC-formula A,x satisfied tt-model based flow
time hN, <i, satisfied Aleksandrov tt-model = hhN, <i , G, Ui,
G = hV, Ri disjoint union -brooms. Take model suppose
PT L RC-formula8




aux
(26)
2+
F 2 aux


true moment 0. Since region aux change time, divide
points

V three disjoint sets: external, boundary internal points respect
aux i.e., satisfying








ep(aux) = aux ,
bp(aux) = aux u aux

aux ,
respectively. Note every boundary point non-boundary R-successor, boundary
points depth 1. follows

simply speak external boundary
points mentioning respect aux .
define exp(2,d) operator PT L RC-formula length polynomial
follows:
(a) First, encode yardsticks length d. use different formulas yardsticks
external points yardsticks boundary points.
(b) Then, help d-yardsticks, encode yardsticks length exp(1, d).
use different formulas external boundary points.
(c) Next, help exp(1, d)-yardsticks boundary external points,
encode yardsticks length exp(2, d) boundary points.
(d) Finally, help exp(2, d)-yardsticks boundary points, define polynomial-length exp(2,d) operator applicable propositional variables.
Step (a). Suppose (26) following formula hold 0:


+
ext

2+
F 2 bp(aux) @ 0,d 2F 2 ep(aux) @ 0,d

0,d =



(27)


l



d1
j delim0
,
delim0 delim0 u
delim0 @
j=1

8. Recall 1 2 stands (1 @ 2 ) u (2 @ 1 ). assume u bind stronger .

211

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

ext results
0,d
0,d replacing occurrence delim0 ext delim0 .


Take boundary point z. Suppose v N (M, hv, xi) |= delim0 .
0,d , every time moment w v,


(M, hw, zi) |= delim0
iff
w v (mod d),


is, z, delim0 holds inevery
time instants, starting v.By second

conjunct (27), external points aux behave similarly respect ext delim0 .
Step (b). encode yardsticks length exp(1, d), recall first every number < 2d
represented binary asequence
a0 . . . ad1 bits. mark bits

binary numbers region term bit1
follows. Given boundary point z time
moment v (M, hv, zi) |= delim0 , say interval [w, w + 1],
w = v + j d, j N, encodes number < 2d z, every < d,


(M, hw + i, zi) |= bit1
iff
ai = 1.

Recall binary representation b0 . . . bd1 successor a0 . . . ad1 modulo
following holds: i, 0 < d, ai = bi iff aj = 0, j, < j
use d-intervals starting v encode < 2d numbers way
consecutive intervals encode consecutive (modulo 2d ) numbers, starting 0.
So, suppose (26), (27) following formula hold 0:


ext
ext


2+
2+
F 2 ep(aux) @ 1,d u 1,d ,
F 2 bp(aux) @ 1,d u 1,d

2d
< d.


(28)


1,d

=

1,d

=









lwr1 delim0 bit1 u lwr1
u










zr1 bit1 u delim0 zr1
u delim1 delim0 u zr1 ,





lwr1
bit1 bit1 ,

ext ext result
1,d
1,d 1,d , respectively, attaching prefix ext
1,d
spatial variables (save aux).


Take boundary point z. Suppose v N
(M, hv, zi) |= delim1 .
Then, last conjunct 1,d , (M, hv, zi) |= delim0 . Since, (a), delim0
holds every time instants z, delim0 marks starting moment
d-interval. Then, first conjunct 1,d , every i, 0 < d, have9




(M, hv + i, zi) |= lwr1
iff
(M, hv + j, zi) |= bit1 , j, < j < d.

Therefore, 1,d says consecutive < 2d numbers (starting 0) encoded consecutive d-intervals (starting v). Similarly first conjunct 1,d , second conjunct
ensures that, every i, 0 < d,



(M, hv + i, zi) |= zr1
iff
(M, hv + j, zi) |= bit1 , j, j < d.
9. Since
apply U operator

cannot

form
spatio-temporal
terms,
auxiliary regions


used instead:

lwr1 delim0 bit1 u lwr1 ensures lwr1 behaves bit1 U delim0 .
equality term indeed regarded fixed point characterisation U operator.
Note

also


need require (as fixed point characterisation) lwr1 @ 3F delim0
true eventuality already enforced 0,d .

212

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity



So, last conjunct 1,d , delim1 holds z every exp(1, d) = 2d time
instants, starting
v.
second conjunct (28), external points behave similarly
respect ext delim1 .
Step (c). construct yardsticks length exp(2, d), using exp(1, d)-yardsticks
constructed (b). Suppose (26)(28) following formulas hold 0:

2+
F 2 bp(aux) @






ext delim1
2+
@ (ep(aux) bp(aux)) ,
F 2 ext delim1


2+
F 2 ep(aux) @ 1,d (bit2 ) ,


2+
F 2 bp(aux) @ 2,d u 2,d ,


(29)
(30)
(31)

2,d defined similarly 1,d
1,d (bit2 )

=



jm1 bit2











delim1 u bit2
ext delim1 u jm1 bit2 ,









=
bit2 bit2
u lwr2
bit2 J1,d bit2 ,


= (aux u ext delim1 ) @ jm1 bit2 .
ext

2,d
J1,d bit2



Take boundary point z. Suppose v time moment (M, hv, zi) |= delim2 .
Then, last conjunct 2,d , (M, hv, zi) |= delim1 . know (b) delim1
holds z every exp(1, d) time instants starting v. So, 2,d first
conjunct 2,d intend express consecutive < 2exp(1,d) numbers (starting
0) encoded consecutive
exp(1, d)-intervals starting v. could then,
last conjunct 2,d , delim2 would hold z every exp(2, d) time instants
starting v. problem (and
step (b)) mark
difference

exp(1,d)
bits < 2
binary numbers term bit2 , need show (polynomial


length) term J1,d bit2 actually defines exp(1,d) bit2 sense that, every w v,
(M, hw, zi) |= J1,d bit2



ff


(M, w + exp(1, d), z ) |= bit2 .

iff

(32)

Suppose first (M, hw, zi) |= J1,d bit2 . Then,
(29),
yw
external R-successor


z (of depth 0) (M, hw, yw i) |= ext delim1 , (M, hw, yw i) |= jm1 bit2 .
hand, hard see (M, hw, zi) 6|= J1,d bit2 ,

0
0
external R-successor yw z (of depth 0) (M, hw, yw i) |= ext delim1


0 i) |= jm bit .
(M, hw, yw
2
1


cases, readily checked (M, hw, yi) |= ext delim1 , external
point y, then, (30),


(M, hw, yi) |= jm1 bit2



ff


(M, w + exp(1, d), ) |= bit2 .

iff

(32) follows first conjunct 2,d .
Step (d). position define polynomial-length exp(2,d) operator J2,d
applicable propositional variables. Recall propositional variable p stands spatial
p, p spatial variable associated p. Now, every propositional
formula 2
213

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

variable p intend apply new operator to, introduce fresh spatial variable
jm2 p. Suppose (26)(31) following formulas hold 0:



2+
F 3 bp(aux) u delim2 ,





2+
2+
F 2 p 2 p
F 2 2,d (p),

(33)
(34)

2,d (p) obtained replacing bit2 , jm1 bit2 ext delim1 1,d (bit2 ) p,
jm2 p delim2 , respectively. Let




(bp(aux) u
delim2 ) @ jm2 p .
J2,d p = 2
claim that, every time moment w every propositional variable p,
(M, w) |= J2,d p

(M, w + exp(2, d)) |= p.

iff

(35)

Suppose first (M,
point z

w) |= J2,d p. Then, (33), boundary
(M, hw, zi) |= delim2 , therefore (M, hw, zi) |= jm2 p . hand,

(M, w) 6|= J2,d p, boundary point z 0 (M, hw, z 0 i) |= delim2




(M, hw, z 0 i) |= jm2 p . cases, readily checked (M, hw, zi) |= delim2 ,
boundary point z, then, second conjunct (34),


(M, hw, zi) |= jm2 p



ff

(M, w + exp(2, d), z ) |= p .

iff

(35) follows first conjunct (34).
Finally, position define PT L RC-formula A,x encodes
computation Turing machine input x. Let tape alphabet (with blank
symbol b A) set states (with two halt states syes sno S) A. use
symbol
/ mark left end tape. know space used
f (n)
input x = hx1 , . . . , xn 22 , exp(2, d) (25). represent
configuration computation x finite word
h, a1 , . . . , ai1 , hs, ai , ai+1 , . . . , , b, . . . , bi
length exp(2, d), a1 , . . . , hs, ai represents current state
active cell. transition function takes triples form hai , hs, aj , ak
(for ai {}, aj , ak A, {syes , sno }) similar triples. instance,
(ai , hs, aj , ak ) = hai , aj , hs0 , ak ii
means that, state reading symbol aj , new state s0
head move one cell right. also assume that, ai {}
aj , ak A, (ai , hsyes , aj , ak ) = hai , aj , ak (ai , hsno , aj , ak ) = hai , aj , ak
meaning head removed halted.
Now, every {} (S A), introduce fresh propositional variable p .
Let A,x conjunction (26)(31), (33) instance (34), p , well
214

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

'

$ '

exp(1, d) many l0j
exp(1,d)
l0

c
c
r
c
c
c
H
1

*

HH@





6
@


ep(aux)
aux

HH


@
r0 - rd
bp(aux)

&

...

j
exp(1, d) many lexp(2,d)1
exp(1,d)
lexp(2,d)1


c
r
c
c
c
c
H
1

*

HH@





6
@

ep(aux)
aux

HH


@ rd

-

rexp(2,d)1
% &

|

$

bp(aux)

{z
exp(2, d) many (exp(1, d) + 1)-brooms

%

}

Figure 8: Structure yardsticks.
following formulas:
^

2+
F p p ,

(36)

,A{}SA
6=

p (phs0 ,x1 (px2 ( (pxn pb U p ) ))),
2+
F



af

head

_


phs,ai ,

(37)
(38)

hs,aiSA

^

2+
F



af




head p J2,d p0 p J2,d p 0 p J2,d p 0 , (39)

(,,)=
h0 , 0 , 0


2+
F

^

af



head af head af head pa J2,d pa ,

(40)

aA{}

3F

_

phsyes ,ai 3F

aA

_

phsno ,ai .

(41)

aA

Suppose first A,x holds time moment 0. (36)(40) (35), consecutive
configurations computation input x properly encoded along time
axis. (For instance, p holds every exp(2, d) time moments.) Finally, (41) says
accepts input x.
Conversely, suppose accepts input x. define Aleksandrov tt-model
= hhN, <i , G, Ui FSA satisfies A,x . Let partial order G = hV, Ri
disjoint union exp(2, d) many (exp(1, d) + 1)-brooms (see Fig. 8):
V = {ri | < exp(2, d)} {lij | < exp(2, d), j exp(1, d)},
zRy

iff

z=y

z = ri , = lij , i, j.

Suppose number steps computation x m.
prefix length N = exp(2, d) final configuration (without halting
state) repeats infinity. w N, let
exp(1,d)

U(w, aux) = {li

215

| < exp(2, d)}.

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

easy see boundary points ri , external points
lij , < exp(1, d). put, every w N,
U(w, delim2 ) = {li

exp(1,d)

|iw

(mod exp(2, d))},

exp(1,d)
U(w, delim1 ) = {li
exp(1,d)
U(w, delim0 ) = {li
U(w, ext delim1 ) = {liv |
U(w, ext delim0 ) = {liv |

|iw

(mod exp(1, d))},

|iw

(mod d)},

vw

(mod exp(1, d)), < exp(2, d)},

vw

(mod d), < exp(2, d)}.

valuations variables clear.


exp(1,d)
(M, hw, zi) |= delim2
iff z = ri z = li
w (mod exp(2, d)),


exp(1,d)
(M, hw, zi) |= delim1
iff z = ri z = li
w (mod exp(1, d)),


v
(M, hw, zi) |= ext delim1
iff z = ri z = li v w (mod exp(2, d))
< exp(2, d),
on, required. hard see satisfies FSA (M, 0) |= A,x .

q

Proof Theorem 3.9, lower bound. proof reduction 2n -corridor tiling
problem known EXPSPACE-complete (Chlebus, 1986; van Emde Boas, 1997).
problem formulated follows: given instance = hT, t0 , t1 , ni,
finite set tile types, t0 , t1 n > 0, decide whether N
tiles 2n -grid (or corridor) way t0 placed onto h0, 0i, t1 onto
hm 1, 0i, top bottom sides corridor fixed colour, say, white.
Suppose = hT, t0 , t1 , ni given. aim construct (using future-time
temporal operators) PT L BRCC-8 formula
(i) length polynomial function |T | n;
(ii) satisfiable tt-model based hN, <i N tiles
2n -corridor;
(iii) N tiles 2n -corridor, satisfiable
tt-model FSA based hN, <i;
(iv) satisfiable tt-model based hN, <i iff satisfiable tt-model based
finite flow time.
case hZ, <i follows immediately.
Recall that, Lemma C.2 (ii), satisfied tt-model satisfied
Aleksandrov tt-model = hhN, <i , G, Vi, G = hV, Ri disjoint union brooms. explain meaning subformulas, assume model
given. Throughout proof use restricted subset RCC-8 predicates: spatiotemporal terms 1 2 constructed spatial variables using complement,
intersection next-time operator , need EQ(1 , 2 ) well two abbreviations
P(1 , 2 ) = EQ(1 , 2 ) TPP(1 , 2 ) NTPP(1 , 2 ) E 1 = DC(1 , 1 ) standing 1
216

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

2n

0

(m + 1) 2n

count
range

0



0

1



1

2



2

3



4

3



0

5

6



1



2

7



3

8

9



0



1

10



2

11



3

12



0

13



1

14



2

15



3

16

...



0

Figure 9: Counting formulas = 3 n = 2.
part 2 1 nonempty, respectively. Clearly, language forms subset
PT L BRCC-8 (in fact, show Remark C.3 below, proof goes
even restricted subset langauge).
first step construction (which contain, among many others, spatial
variables ) write formulas forcing sequence y0 , y1 , . . . , ym2n 1
distinct points (of depth 0) V , N, that, < 2n ,
(M, hi, yi i) |= unique tile type . = k 2n + j, k < m, j < 2n ,
use yi (at time i) encode pair hk, ji 2n -grid. Thus,
neighbour hk, j + 1i hk, ji coded point yi+1 time + 1, right
neighbour hk + 1, ji yi+2n moment + 2n (see Fig. 10).
Let q0 , . . . , qn1 pairwise distinct propositional variables


n1
j = qd00 qn1
,

dn1 . . . d0 binary representation j < 2n , q0i = qi q1i = qi , i.
Suppose formula



+
count 0 count U (0 2+
(42)
F count) 2F count
true 0, count fresh propositional variable following
counting formula (the length polynomial n)

^

^ ^
^

qi qk
=
qi qk
(qi qi )
2n 1 0 .
k<n

i<k

i<k

k<i<n

N count true moment (m + 1) 2n false starting
(m+1)2n . sequence 0 , 1 , . . . , 2n 1 repeated m+1 times along time-line,
i.e., count true. Let
range = 3F (count 0 ).
Clearly, range true moment 2n always false (see Fig. 9).
Let equ, p0 , . . . , pn1 e0 , . . . , en1 fresh distinct spatial variables,


n1
j = pd00 u u pn1
,

dn1 . . . d0 binary representation j < 2n , p0i = pi p1i = pi , i.
Suppose (42)
l
^
^


+

2+
e

2
2+
EQ
equ,
q

EQ(e
,
p
)

(43)




F
F
F EQ pi , pi
i<n

i<n

i<n

217

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

true 0. Then, first two conjuncts (43), N V
depth 0, j < 2n (M, hi, yi) |= equ iff (M, i) |= j (M, hi, yi) |= j .
last conjunct (43),


(M, hi, yi) |= equ iff j < 2n (M, i) |= j (M, hk, yi) |= j , k N . ()
generate required sequence points yi using formulas:
range 2+
(range E tile),
F

G l
2+
EQ
tile,
equ
u

u



future
,
F
tT

^

2+
F P future,

(44)
(45)

tT



u future ,

(46)

tT

tile future (for ) fresh spatial variables. Indeed, suppose
conjunction (42)(46) holds time 0 M. Then, first conjunct (44)
(42), (M, 0) |= range 0 and, second conjunct (44), (M, h0, y0 i) |= tile
y0 V . may assume y0 depth 0. Then, (45),
(a0 ) (M, h0, y0 i) |= equ, and, (), (M, hk, y0 i) |= 0 k N;
(b0 ) , (M, h0, y0 i) |= future and, (46), (M, hk, y0 i) |= k > 0.
Next, (42), (M, 1) |= range 1 and, (44), y1 V (again, depth
0) (M, h1, y1 i) |= tile. particular, have:
(a1 ) (M, h1, y1 i) |= equ, and, (), (M, hk, y1 i) |= 1 k N;
(b1 ) , (M, h1, y1 i) |= future and, (46), (M, hk, y1 i) |= k > 1.
(b0 ), (M, h1, y0 i) |= t, , thus y1 6= y0 . consider y1 moment 1
use argument find point y2 V (which different y1 (b1 )),
forth; see Fig. 10. gives us points y0 , y1 , . . . , ym2n 1 (of depth 0) V need.
next aim write formulas could serve pointers right
neighbours given pair corridor (at moment bother top
border). Consider formulas

tile ,
2+
(47)
F EQ up,

+
2F EQ right, equ u equ U tile ,
(48)

+
2F EQ equ U tile, tile equ u equ U tile ,
(49)
up, right equ U tile fresh spatial variables.
i, j < 2n ,
(M, hi, yj i) |=
(M, hi, yj i) |= right

iff

j = + 1;
iff

j = + 2n .
218

claim that,

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

b =
q q= equ U

range

V
3

y11

q

q

q

equ
q

q

q

q

2

y10

q

q

equ
q

q

q

q

1

y9

q

equ
q

q

q

q

0

q
y8 equ

q

q

q

3

y7

q

q

q

equ
q
right

equ
q
right

equ
q
right

equ
q
right

equ
q
right

2

y6

q
q

equ
q
right

1

y5

q

0

y4

equ
q
right

3

y3

q

q

2

y2

q


q

1

y1


q

0

r
y0 equ
tile
0



0

q



equ
r
tile

q

q

q

q

q


q



equ
q
right


r
q equ
b
tile

r
q
q
q equ
b
b
tile

r
q
q equ
b
b
b
tile

r
q equ
b
b
b equ
b
tile
equ
r
b
b
b equ
b
b
tile
b
b
b equ
b
b
b

q

q

b

b

equ
b

b

b

b

equ
b

b

b

b

equ
b

1

2

3

4

5

6

7

8

9

10

11

12



6
right

6


r
q
q
q equ
tile

r
q
q
q equ
b
b
b equ
b
b
b
b
tile

q
q equ
b
b
b equ
b
b
b
b equ
b
r
tile

r
q equ
b
b
b equ
b
b
b
b equ
b
b
tile
equ
r
b
b
b equ
b
b
b
b equ
b
b
b
tile
b
b
b equ
b
b
b
b equ
b
b
b
b

b

1



2



3



0



1



2



3



0

tT
tile



1



2



3





6 6





6
c- right

... |

{z

}

3 22 corridor

0

Figure 10: Satisfying , n = 2, tt-model based space 3 22 points.
former obvious. Let us prove latter. show (M, hi, yj i) |= right,
j = + 2n , first observe (M, hj, yj i) |= equ (M, hi, yj i) |= equ (). follows
(M, hj, yj i) |= tile (49) (M, hj 1, yj i) |= equ U tile. Then, applying
(49) (from right left) sufficiently many times, obtain (M, hi, yj i) |= equ U tile,
(M, hi 1, yj i) 6|= equ U tile, (M, hi, yj i) |= right.
Conversely, suppose (M, hi, yj i) |= right yj . (M, hi, yj i) |= equ and,
() (note + 2n < (m + 1) 2n , count still true + 2n ),
(M, hi + 2n , yj i) |= equ.

()

(M, hi, yj i) |= equ U tile. applying (49) (from left right) sufficiently
many times arrive (M, hi + 2n 1, yj i) |= equ U tile together ()
gives (M, hi + 2n , yj i) |= tile. j = + 2n .
noted every time point extension equ U tile coincides
extension term equ U tile elements sequence y0 , . . . , ym2n , (49)
indeed fixed point characterisation U operator.
Finally, formulas ensure every point 2n -corridor covered
one tile, h0, 0i covered t0 , hm 1, 0i t1 , top bottom sides white
219

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

colours adjacent edges adjacent tiles match:
^
0
2+
F E (t u ),

(50)

t,t0
t6=t0





P(tile, t0 ) 2+


range

3


range

P
tile,

,
0
0
1
F
F


_

n
2+
P tile, ,
F 2 1

(51)
(52)

tT
up(t)=white


_

2+


P
tile,

,
0
F

(53)

tT
down(t)=white

^



0
n


future
,
2+


E


P
up,

2 1
F

(54)

t,t0

up(t)6=down(t0 )

^



0
.
E


P
right,



future
2+
F

(55)

t,t0
right(t)6=left(t0 )

Let conjunction (42)(55). Suppose holds 0 M.
N (M, 2n 1) |= range and, every 2n , (M, i) |= range.
define map : 2n taking
(k, j) =

iff

(M, hi, yi i) |= = k 2n + j.

leave reader check tiling 2n , required.
direction, suppose tiling 2n -corridor ,
> 0. satisfied Aleksandrov tt-model = hhN, <i , hV, Ri , Vi,
V = {y0 , . . . , ym2n 1 }, R minimal reflexive relation V ,
V(t, i) = {yi V | (k, j) = = k 2n + j},
variables interpreted shown Fig. 10. Clearly, satisfies
FSA. Moreover, satisfiable tt-models finite flows time iff satisfiable
tt-models hN, <i. Details left reader.
q
Remark C.3. may interest note language used proof
rather limited. fact, enough extend PSPACE-complete logic PT L RCC-8
predicates form EQ(%1 , %2 %3 ) (where %i atomic spatio-temporal region
terms) make EXPSPACE-hard. show this, transform PT LBRCC-8 formula
constructed following way. First, take fresh spatial variable u (denoting

universe) add conjunct 2+
F EQ(u, u). Next, every spatio-temporal
Boolean region term % , introduce spatial variable neg % (the complement %
+
respect u), add conjuncts 2+
F EQ(u, % neg %) 2F DC(%, neg %), replace
every occurrence % resulting formula neg %. Finally, every spatio-temporal
term form %1 u %2 , introduce fresh spatial variable %1 %2 , add conjuncts
+
+
2+
F P(%1 %2 , %1 ) 2F P(%1 %2 , %2 ) 2F P(%1 , neg %2 %1 %2 )

220

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

replace occurrences %1 u %2 %1 %2 . One readily see (i) length
resulting formula 0T linear length (ii) 0T satisfiable tt-model
based hN, <i (with FSA) iff satisfiable tt-model based hN, <i (with FSA).
C.2 Upper Complexity Bounds (I): Quasimodels PT L RC
appendix define quasimodels PT L RC spirit paper (Hodkinson
et al., 2000) order establish upper complexity bounds Theorems 3.10 3.13.
remind reader spatio-temporal terms PT L RC form:
| I%



::= %

%

::= CIp

|

| CI%

| 1 u 2 ,
| CI(%1 u %2 ) | CI(%1 U %2 ) | CI(%1 %2 ),

PT L RC forms sublanguage PT L RCit differs latter
definition spatio-temporal region terms:
%

::=

CIp

| CI%

| CI(%1 u %2 ) | CI %.

Let PT L RC-formula. Recall p. 200 sub denote set
subformulas term set spatio-temporal terms including
form %. type subset term
every 1 u 2 term ,
every term ,

1 u 2



iff

iff

1 2 t;


/ t.

Clearly, number [() different types bounded 2|term | .
broom type b pair hhT, , ti, hT, broom (with 0
leaves) labelling function associating x type t(x)
following conditions hold:
(bt0) t(x) 6= t(y), pair distinct points x, 0 ;
(bt1) every x 0 ,
every CI(%1 u%2 ) term , CI(%1 u%2 ) t(x) iff %1 t(x) %2 t(x),
every CI% term , CI% t(x) iff %
/ t(x);
(bt2) every I% term ,
(bt3) every % term ,

I% t(x)
% t(x)

iff
iff

% t(y) every , x y;
0 x % t(y).

Broom types b1 = hhT1 , 1 , t1 b2 = hhT2 , 2 , t2 said isomorphic
every x1 T10 , x2 T20 t1 (x1 ) = t2 (x2 )
every x2 T20 , x1 T10 t1 (x1 ) = t2 (x2 ).
Clearly, given two isomorphic broom types b1 b2 , also t1 (r1 ) = t2 (r2 ),
r1 r2 roots b1 b2 , respectively.
quasistate pair hs, mi, Boolean-saturated subset sub
disjoint union hhT, , ti broom types b1 , . . . , bn following conditions
hold:
221

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

(qs0) bi bj isomorphic, 6= j;
sub ,

(qs1) every 2
2

iff

t(x) every x .
[()

Clearly, number ]() quasistates bounded 22
2|sub | .
Fix flow time F = hW, <i. basic structure pair hF, qi, q
function associating w W quasistate q(w) = hsw , mw that,
w W ,
every 1 U 2 sub , 1 U 2 sw iff v > w 2 sv
1 su u (w, v);
every 1 2 sub , 1 2 sw iff v < w 2 sv
1 su u (v, w).
Let hF, qi basic structure , q(w) = hsw , mw mw = hhTw , w , tw
w W . Denote Tw0 set leaves hTw , w Tw1 set roots
brooms it. 1-run hF, qi function r giving w W point
r(w) Tw1 ; coherent saturated 0-run hF, qi function r giving
w W point r(w) Tw0 following conditions hold:
every CI(%1 U %2 ) term , CI(%1 U %2 ) tw (r(w)) iff v > w
%2 tv (r(v)) %1 tu (r(u)) u (w, v);
every CI(%1 %2 ) term , CI(%1 %2 ) tw (r(w)) iff v < w
%2 tv (r(v)) %1 tu (r(u)) u (v, w).
Say quadruple Q = hF, q, R, Ci quasimodel based F hF, qi basic
structure , R = R0 R1 , R1 set 1-runs R0 set coherent
saturated 0-runs hF, qi, C reflexive closure subset R1 R0

(qm2) w0 W sw0 ;
(qm3) every w W every x Tw , r R r(w) = x;
(qm4) r, r0 R, r C r0 r(w) w r0 (w) w W ;
(qm5) r R, w W x Tw0 , r(w) w x r0 R0
r0 (w) = x r C r0 .
quasimodel Q said finitary set R runs finite.
Lemma C.4. PT L RC-formula satisfiable Aleksandrov tt-model based
flow time F (finite) disjoint union (finite) brooms iff (finitary)
quasimodel based F.
Proof. () Let PT L RC-formula Q = hF, q, R, Ci quasimodel ,
F = hW, <i q(w) = hsw , hhTw , w , tw ii w W . construct Aleksandrov
tt-model = hF, G, Vi taking G = hR, Ci and, spatial variable p w W ,
V(p, w) = {r | CIp tw (r(w))}.
222

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Clearly, Q finitary G finite. Thus, remains prove satisfied M.
First, show induction construction region term % term that,
every w W every r R,
(M, hw, ri) |= %

iff

% tw (r(w)).

(56)

basis induction: % = CIp. Let (M, hw, ri) |= %. r0 R
r C r0 (M, hw, r0 i) |= Ip. (qm4), r(w) w r0 (w). Take Tw0 , r0 (w) w y.
(qm5), run r00 R0 r0 C r00 r00 (w) = y. (M, hw, r00 i) |= p
and, definition V, CIp tw (r00 (w)) and, (bt3), % tw (r(w)).
Conversely, % tw (r(w)) then, (bt3), Tw0 r(w) w
% tw (y). (qm5), r00 R0 , r C r00 , r00 (w) = y. CIp tw (r00 (w))
and, definition V, (M, hw, r00 i) |= p. Therefore, (M, hw, ri) |= %.
induction steps % = CI%1 , CI(%1 u %2 ), CI(%1 U %2 ) CI(%1 %2 ) similar,
instead definition V, use (bt1) cases Booleans coherence
saturatedness r00 cases temporal operators.
Next, extend (56) arbitrary spatio-temporal terms term .
Case = I%. Suppose (M, hw, ri) |= I%. Take Tw , r(w) w y. Tw0
then, (qm5), r0 R0 r C r0 r0 (w) = y.
/ Tw0 clearly
= r(w) take r0 = r. (M, hw, r0 i) |= %, which, IH, implies % tw (r0 (w)).
Therefore, % tw (y) every w r(w) and, (bt2), I% tw (r(w)).
Conversely, I% tw (r(w)) then, (bt2), % tw (y), every w r(w). Take
run r0 R r C r0 . (qm4), r(w) w r0 (w), % tw (r0 (w)), which,
IH, (M, hw, r0 i) |= %. Hence, (M, hw, ri) |= I%.
Cases = 1 u 2 1 follow IH definition type.
Finally, show induction construction sub that, every w W ,
(M, w) |=

iff

sw .

(57)

. Suppose (M, w) |= 2
. Take x . (qm3), r R
Case = 2
w
r(w) = x. (M, hw, ri) |= and, IH, tw (r(w)). Therefore, (qs1),
. Conversely, let 2
. Take run r R. (qs1), (r(w)),
2
w
w
w
.
which, IH, (M, hw, ri) |= . Hence, (M, w) |= 2
Cases = 1 2 1 follow IH Boolean-saturatedness sw .
follows (57) (qm2) satisfiable M.

() Let PT L RC-formula suppose satisfied Aleksandrov
tt-model = hF, G, Vi, F = hW, <i G = h, disjoint union brooms.
Denote 0 1 leaves roots brooms G, respectively. every
pair hw, xi W associate type
t(w, x) = { term | (M, hw, xi) |= }.
Fix w W define binary relation follows. x, x0 0 , let x w x0 iff
t(w, x) = t(w, x0 ) and, z, z 0 1 , let z w z 0 iff brooms generated z z 0
isomorphic, i.e.,
x 0 (z x x0 0 (z 0 x0 x w x0 ))
x0 0 (z 0 x0 x 0 (z x x w x0 )).
223

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Clearly, w equivalence relation . Denote [x]w w -equivalence class x
define map fw taking, x ,
(
[x]w ,
x 1 ,
fw (x) =
h[z]w , [x]w , x 0 z 1 z x.
Since G disjoint union brooms, fw well-defined. put
Tw = {fw (x) | x },
u w v

iff

x,

tw (fw (x)) = t(w, x),

x y, u = fw (x) v = fw (y),

x .

definition fw , hTw , w union brooms tw well-defined. Consider
structure hsw , mw i,
mw = hhTw , w , tw



sw = { sub | (M, w) |= }.

readily seen brooms mw (bt0) mw satisfies
(qs0). Moreover, fw p-morphism h, onto hTw , w i, also (bt1)
(bt3) (qs1). So, taking q(w) = hsw , mw w W obtain basic structure
hF, qi satisfying (qm2).
remains define appropriate runs hF, qi. k = 0, 1, let Rk set
maps r : w 7 fw (x) x k . Clearly, R1 R0 sets 1- coherent
saturated 0-runs, respectively. Put R = R0 R1 r, r0 R, r C r0 iff r(w) w r0 (w)
w W . (qm4) holds definition. Let v W Tv .
x fv (x) = y. Clearly, R contains run r : w 7 fw (x), proves
(qm3). Finally, let r R, v W Tv0 r(v) v y.
z, x fw (z) = r(w), every w W , fv (x) = y. clearly z x
x 0 . take run r0 : w 7 fw (x). definition, r C r0 , proves (qm5).
Thus, Q = hF, q, R, Ci quasimodel . Note G finite R finite
well therefore, Q finitary.
q
position establish upper complexity bounds satisfiability
problem PT L RC- PT L RC-formulas tt-models based hN, <i, hZ, <i
arbitrary finite flows time.
Proof Theorem 3.10, upper bound. consider cases hN, <i hZ, <i.
case arbitrary finite flows time tt-models FSA based
hN, <i hZ, <i follow Theorem 3.13.
One readily check propositional temporal logic PT L,
following polynomial reductions PT L RC:
satisfiability tt-models based hZ, <i polynomially reduced satisfiability
tt-models based hN, <i;
satisfiability tt-models based hN, <i polynomially reduced satisfiability
formulas without past-time temporal operators.
224

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

So, follows consider simplest case satisfiability problem,
PT L RC-formulas without past-time temporal operators tt-models based hN, <i.
present nondeterministic 2EXPSPACE satisfiability checking algorithm
similar Sistla Clarke (1985). First, one prove (with help Lemmas C.1 (ii) C.4) analogue (Hodkinson et al., 2000, Theorem 24) states
PT L RC-formula satisfiable tt-model based hN, <i iff l1 , l2 N


2
l1 ](),
0 < l2 |term | 2[() ]() + ]()
balloon-like quasimodel Q = hhN, <i , q, R, Ci q(l1 + n) = q(l1 + l2 + n)
every n N. Although Theorem 24 (Hodkinson et al., 2000) proved
monodic fragment first-order temporal logic, basic idea extracting balloon-like
quasimodel arbitrary one works PT L RC well. difference
quasistates complex: regarded sets sets types (not
sets types) thus, l1 l2 triple exponential length `() .
quasimodel Q guessed 2EXPSPACE algorithm similar
proof (Hodkinson et al., 2003, Theorem 4.1).
q
Proof Theorem 3.13, upper bound. proof similar Theorem 3.10.
Again, one show cases polynomially reducible case satisfiability
PT L RC-formulas without past-time temporal operators tt-models FSA
based hN, <i. take FSA account, prove (using Lemmas C.1 (i)
C.4) analogues Theorems 29 35 (Hodkinson et al., 2000) state
PT L RC-formula satisfiable tt-model FSA based hN, <i iff
finitary balloon-like quasimodel based hN, <i. condition finiteness
set runs also ensured algorithm similar Theorem 3.10.
q
C.3 Upper Complexity Bounds (II):
Embedding First-Order Temporal Logic
appendix introduce first-order temporal language QT L use known
complexity results fragments QT L obtain upper complexity bounds spatiotemporal logics based RC (and therefore, BRCC-8).
alphabet QT L consists individual variables x1 , x2 , . . . , predicate symbols
P1 , P2 , . . . , fixed arity, Booleans, universal x existential x quantifiers variable x, temporal operators U, (with
derivatives , 3F , 2F , etc.). Note language contains neither constant symbols
equality (we simply need obtain complexity results).
QT L interpreted first-order temporal models form = hF, D, Ii,
F = hW, <i flow time, nonempty set, domain M, function
associating every moment time w W first-order structure

E
I(w)
I(w)
I(w) = D, P0 , P1 , . . . ,
I(w)

state moment w, Pi
relation arity Pi .
assignment function set individual variables D. Given
assignment QT L-formula , define truth-relation (M, w) |=a taking
225

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

u = CIp

x1b
depth 0
depth 1

e = CIp

'

xn1
b

x2b

$

xnb

u
e
e
u
...
*

YH
H



@

H @

HH

@ u
H
b

=

Pj1 [db ]Pj2 [db ] . . .Pjn1 [db ]Pjn [db ]
&

%

x0b

db

Figure 11: Representing n-broom b region CIpj point first-order model.
I(w)

(M, w) |=a Pi (x1 , . . . , xm ) iff ha(x1 ), . . . , a(xm )i Pi

,

0

(M, w) |=a x iff (M, w) |=a , every assignment a0 differ
x,
plus standard clauses Booleans temporal operators. say QT Lformula satisfied (M, w) |=a w W assignment D.
free variables among x1 , . . . , xm , instead (M, w) |=a often write
(M, w) |= [d1 , . . . , dm ], di = a(xi ) i, 1 m.
Denote QT L1 one-variable fragment QT L, i.e., set QT L-formulas
contain one individual variable, say, x. Without loss generality may
assume predicate symbols QT L1 unary.
define embedding spatio-temporal languages based RC QT L1 .
Recall that, Lemma C.2 (i), PT L RC -formula width n satisfied
tt-model FSA also satisfiable Aleksandrov tt-model based
flow time finite disjoint union n-brooms. Similarly, PT L RC -formula
width n satisfiable also satisfiable Aleksandrov tt-model based
flow time possibly infinite disjoint union n-brooms.
cover cases, let PT L RC -formula width n. show
construct QT L1 -formula n length linear `() every Aleksandrov tt-model
based (finite) union n-brooms satisfying gives rise first-order temporal model
(with finite domain, respectively) satisfying n vice versa. Thus, polynomially
reduce satisfiability problem spatio-temporal languages QT L1 .
Suppose satisfied Aleksandrov tt-model = hF, G, Vi, F = hW, <i
G (finite infinite) disjoint union n-brooms. every n-broom b G
associate element db first-order domain D. Then, every spatial variable pj ,
fix n different unary predicate symbols Pj1 (x), . . . , Pjn (x) following meaning:
Pji (x) true db moment w W iff i-th leaf b (xib Fig. 11) belongs

region CIp w. Define n distinct translations n , 1 n, encoding truth values
spatio-temporal region terms leaves G taking, spatial variable pj
terms %1 %2 ,


(CIpj )n = Pji (x),






(CI%1 )n = (%1 )n ,








(CI(%1 U %2 ))n = (%1 )n U (%2 )n ,





(CI(%1 u %2 ))n = (%1 )n (%2 )n ,




(CI(%1 %2 ))n = (%1 )n (%2 )n .
226

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Next extend n translations arbitrary spatio-temporal terms . First
0
introduce translation n encode truth value arbitrary spatio-temporal terms
roots n-brooms G: region term %, let
0n

(%)

n
_

=

k

(%)n .

k=1
0

formula shows, particular, n redundant region terms since
0
truth values roots computed defined n . spatio-temporal term
form I%, % region term, take
0n

(I%)

n
^

=

k

(%)n







(I%)n = (%)n

i, 1 n,

k=1

then, spatio-temporal terms 1 2 ,10




(1 )n = (1 )n







(1 u 2 )n = (1 )n (2 )n



i, 0 n.

Finally, define translation n subformulas : spatio-temporal term ,
n

(2 )


0n

= x ( )



n
^

k

x ( )n

k=1

and, spatio-temporal formulas 1 2 ,
(1 )n = 1n ,

(1 2 )n = 1n 2n ,

(1 U 2 )n = 1n U 2n ,

(1 2 )n = 1n 2n ,

Clearly, length n linear n `().
Lemma C.5. PT L RC -formula width n satisfiable Aleksandrov tt-model
based (finite) disjoint union n-brooms iff n satisfiable first-order temporal
model (with finite domain) based flow time.
Proof. () Suppose satisfied Aleksandrov tt-model = hF, G, Vi,
0 1
n
F = hW, <i, G = hV, Ri disjoint
unionffof n-brooms

0 n ffb = hWb , Rb i, Wb = {xb , xb , . . . , xb }
0
1
Rb reflexive closure { xb , xb , . . . , xb , xb } (see Fig. 11).
Construct first-order temporal model N = hF, D, Ii taking set
db n-brooms b G and, every w W ,

E
I(w)
I(w)
I(w)
I(w)
I(w) = D, P11 , . . . , P1n , P21 , . . . , P2n , . . . ,
spatial variable pj i, 1 n,
I(w)

Pji



ff
= {db | (M, w, xib ) |= pj }.

10. brevity, definition follow syntax PT L RC rather PT L RC .

227

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Note finite whenever G finite.
Now, induction construction spatio-temporal region term % ,
easily shown every w W , every n-broom b G every i, 1 n,


ff

(58)
(N, w) |= (%)n [db ] iff (M, w, xib ) |= %.
Next, (58) extended arbitrary spatio-temporal terms i, 0 n:


(N, w) |= ( )n [db ]

ff


iff (M, w, xib ) |= .

(59)

cases i, 1 n, trivially follow (58) fact leaves
successors themselves. Consider = 0. case = % holds simply region
terms interpreted regular closed sets:


ff


ff
(M, w, x0b ) |= %
iff
(M, w, xkb ) |= %, k, 1 k n,
(60)
= I% then, one hand,


ff
(M, w, x0b ) |= I%

iff



ff
(M, w, xkb ) |= %,

k, 0 k n,

0

other, definition n ,
0

(N, w) |= (I%)n [db ]

iff

k

(N, w) |= (%)n [db ],

k, 1 k n,

together (60) IH yields (59). cases Booleans trivial.
Finally, show every sub ,
(N, w) |= n

iff

(M, w) |= .

:
Case = 2
)n
(N, w) |= (2

k

iff

db k {0, 1, . . . , n} (N, w) |= ( )n [db ]


ff
b G k {0, 1, . . . , n} (M, w, xkb ) |=

iff

.
(M, w) |= 2

iff

remaining cases trivial. follows n satisfied N.
() Assume satisfied first-order temporal model N = hF, D, Ii,
F = hW, <i and, every w W ,

E
I(w)
I(w)
I(w)
I(w)
I(w) = D, P11 , . . . , P1n , P21 , . . . , P2n , . . . .
every point associate n-broom bd = hWbd , Rbd sets Wbd ,
D, pairwise disjoint contains n + 1 distinct elements x0bd , . . . , xnbd .
Construct Aleksandrov tt-model = hF, G, Vi taking
G disjoint union n-brooms {bd | D},



V(pj , w) = xibd | (N, w) |= (CIpj )n [d], 0 n .
228

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Clearly, finite G finite well.
straightforward induction one show w W , D, spatio-temporal
region terms %, spatio-temporal terms , subformulas , i, 0 n,


ff

iff
(M, w, xibd ) |= %
(i > 0),
(N, w) |= (%)n [d]


ff


(N, w) |= ( ) [d]
iff
(M, w, xbd ) |= ,
(N, w) |= n

iff

(M, w) |= .

example,
0

(N, w) |= (I%)n [d]

iff
iff
iff

k

(N, w) |= (%)n [d], k, 0 k n
ff


(M, w, xkbd ) |= %, k, 0 k n
ff


(M, w, x0bd ) |= I%.
q

follows satisfied M.
obtain upper complexity bounds combinations PT L RC :

Proof Theorem 3.11, upper bound. Follows Lemmas C.2 (ii) C.5 together
results complexity one-variable fragment QT L (Halpern & Vardi,
1989; Sistla & German, 1987; Hodkinson et al., 2000, 2003).
q
Proof Theorem 3.12, upper bound. Similar proof above.

q

Proof Theorem 3.15, upper bound. proof follows Lemmas C.2 (i) C.5
together upper complexity bound guarded monodic (and one-variable)
fragment QT L (Hodkinson, 2004).
q
C.4 Lower Complexity Bounds (II): Embedding First-Order Temporal Logic
position prove Theorem 3.14 establish lower complexity bounds
spatio-temporal logics based BRCC-8 (and based RC well). Denote
QT L12 one-variable fragment QT L sole temporal operator 2F . define
polynomial embedding QT L12 PT L2 BRCC-8. Note similar embedding
full one-variable fragment QT L1 PT L BRCC-8 regarded alternative
way prove lower complexity bound Theorem 3.12.
QT L12 -formula said basic Q-formula form x (x),
(x) quantifier-free contains propositional variables. QT L12 -sentence Qnormal form built basic Q-formulas using Booleans temporal operator
2F . words, sentences Q-normal form contain nested quantifiers use
unary predicate symbols. following observation come surprise (see,
e.g., Hughes & Cresswell, 1996):
Lemma C.6. every QT L12 -sentence one effectively construct QT L12 -sentence

b Q-normal form satisfiable first-order temporal model flow
time F (and finite domain) iff
b satisfiable first-order temporal model based
F (and finite domain). Moreover, length
b linear length .
229

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Proof. Without loss generality may assume contains occurrences .
transform Q-normal form, first introduce fresh unary predicate symbol Pi (x)
every propositional variable pi replace occurrence pi x Pi (x).
Denote resulting formula 0 . every subformula 0 define formula ]
taking inductively
(P (x))] = P (x),
()] = ] ,

(x )] = Px (x),
(1 2 )] = 1] 2] ,

Px (x) fresh unary predicate symbol. Let

^

2+
x Px (x) x Px (x)

b = x ]0
F

(2F )] = 2F ] ,

x Px (x) x ]




.

xsub0

One readily show induction
b satisfiable first-order temporal model based
F (and finite domain) iff satisfiable first-order temporal model based
F (and finite domain). Moreover,
b Q-normal form.
q
Now, given QT L12 -formula Q-normal form, denote result replacing
occurrences basic Q-formulas x (x) EQ( , >), > region term
representing whole space (for instance, CIu CIu fresh spatial variable u),
translation quantifier-free formulas (x) defined taking:
(P (x)) = CIp,

() = CI ,

(1 2 ) = CI(1 u 2 ),

(2F ) = CI2F ,

P (x) unary predicate symbol p spatial variable standing P (x). Clearly,
belongs PT L2 BRCC-8.
Lemma C.7. QT L12 -sentence Q-normal form satisfiable first-order temporal
model based flow time F finite domain iff satisfiable tt-model
based F satisfying FSA.
Proof. () Suppose Q-normal form = hF, D, Ii first-order temporal


ff
I(w)
model, F = hW, <i and, w W , I(w) = D, P0 , . . . , . Let (M, w0 ) |=
w0 W . Construct Aleksandrov tt-model M0 = hF, G, Vi taking G = hD, Ri,

I(w)
R = {hd, di | D} V(pi , w) = hw, di | Pi
. Note topological
space TG = hD, IG induced G discrete, i.e., X D,
IG X = CG X = X.
follows induction every quantifier-free QT L12 -formula , every w W
every
(M, w) |= [d]

iff

(M0 , hw, di) |= .

Therefore, every basic Q-formula x (x) every w W , (M, w) |= x (x) iff
(M0 , w) |= EQ( , >). follows induction (M0 , w0 ) |= .
230

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

() Suppose satisfied tt-model based F = hW, <i. Lemma C.1 (i),
satisfied Aleksandrov tt-model = hF, G, Vi, G = hV, Ri disjoint
union brooms. Denote V0 V set leaves G define first-order temporal
model M0 = hF, V0 , Ii taking, w W ,


ff
I(w)
I(w)
I(w) = V0 , P0 , . . .

Pi
= V(pi , w) V0 .


Clearly, every X V , IG X V0 = CG X V0 = X V0 , TG = hV, IG
topological space induced G. obtain induction every quantifier-free
QT L12 -formula , w W V0
(M0 , w) |= [d]

iff

(M, hw, di) |= .

regular closed set X V TG coincides V iff contains V0 . So, basic
Q-formulas x (x) w W , (M0 , w) |= x (x) iff (M, w) |= EQ( , >). follows
induction satisfied M0 .
q
Proof Theorem 3.14, lower bound. Lemmas C.6 C.7 satisfiability problem QT L12 -formulas first-order temporal models finite domains based
hN, <i, hZ, <i arbitrary finite flows time polynomially reducible satisfiability PT L2 BRCC-8 formulas tt-models FSA. Since former known
EXPSPACE-hard (Hodkinson et al., 2003) hN, <i hZ, <i, latter also
EXPSPACE-hard cases. noted result Hodkinson
colleagues (2003) readily extended case arbitrary finite flows time
(by reduction finite version corridor tiling problem). gives us lower
complexity bound PT L2 BRCC-8 case finite flows time.
q
C.5 PSPACE-complete Spatio-Temporal Logic
appendix prove Theorem 3.8. fact, show satisfiability problem
PT L RC 2 extension PT L RCC-8is decidable PSPACE, RC 2
sublanguage S4u spatial terms restricted following:
%

::= CIp,



::= %



::= I%



::= 1 2

|

I%,
|

%,
|

1 2

|

.

before, denote spatial terms representing regular closed sets (regions)
representing regular open sets (the interiors regions). Clearly, definition
equivalent definition p. 190 (where make explicit distinction
). easy see RC 2 contains RCC-8, less expressive BRCC-8.
Spatio-temporal terms PT L RC 2 constructed region terms form
%

::=

CIp

|

CI %

way spatial terms RC 2 . Finally, PT L RC 2 -formulas composed
using Booleans temporal operators.
atomic formulas form 2
231

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

reduce satisfiability problem PT LRC 2 PT L. reduction
done number steps.
Let F = hW, <i flow time (as formulation Theorem 3.8)
PT L RC 2 -formula. begin removing next-time operator subterms .
end, let 0 = variable p set
1

=

{p | CI CIp term 0 },

introduce fresh spatial variable p0 , put
^

+
(CI CIp CIp0 ) ,
1 = 1
2+
2
>

2
P F
p1

1 result replacing occurrence CI CIp 0 CIp0
(% % ). Next, p
(%
(% % ) 2
2
1
2
1
2
1 %2 ) stands 2
2

=

{p | CI CIp term 1 },

introduce fresh spatial variable p0 , set
^
+
2 = 2
2+
P 2F

>


(CI CIp CIp0 ) ,
2

p1 2

2 result replacing occurrence CI CIp 1 CIp0 . repeating
process sufficiently many times obtain formula
^

+
(CI CIp CIp0 ) ,
2
>

2
(61)

e =
2+
P F
p

suitable set spatial variables, contains region terms,
is, PT L[RC 2 ]-formula. (Note spatial variable p occurs
either CI CIp
/ term p .) clear length
e
linear length , satisfiable tt-model based F iff
e satisfiable
tt-model based F.
Thus, suffices reduce satisfiability problem PT L RC 2 -formulas
form (61) satisfiability problem PT L-formulas. Let us recall function
Appendix B.1 maps PT L[S4u ]-formulas (in particular, PT L[RC 2 ]-formulas)
, let (2
) = p , p
PT L-formulas. Namely, every atomic RC 2 -formula 2


fresh propositional variable. Then, given PT L[RC 2 ]-formula , define
(2
) .
result replacing every occurrence 2
shown proof
Theorem 3.1, satisfiable tt-model F = hW, <i iff
(s1) exists temporal model N = hF, Ui satisfying and,
(s2) every w W , set
| (N, w) |= p , term } {2
| (N, w) |= p , term }
w = {2
(62)





RC 2 -formulas satisfiable.
232

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

preserve satisfiability whole ,
e ensure somehow
(s3) points satisfying w predecessors successors satisfying w1
w+1 , respectively.
remainder appendix first describe encoding satisfiability
problem sets RC 2 -formulas form (62) Boolean logic, used
part final reduction. prove completion property RC 2 (cf. Balbiani &
Condotta, 2002) class exhaustive models contain sufficiently many points
every type. Roughly, completion property says that, given set form (62)
exhaustive model satisfying subset , one extend valuation model
satisfy whole . property make possible solve problem (s3) above.
worth noting similar construction works stronger languages BRCC-8,
then, enjoy completion property, sets (62) may need exponentially many formulas (in
number spatial variables) and, therefore, reduction PT L exponential
well. RC 2 suffices consider sets (62) quadratic number formulas,
results quadratic reduction.
C.5.1 Properties RC 2 -formulas
finite set = {p1 , . . . , pn } spatial variables, let



AtFm =
2
| RC 2 -term variables .
Clearly, every RC 2 -formula spatial variables Boolean combination spatial
formulas AtFm . also clear |AtFm | 16 ||2 .
width RC 2 -formulas 2 (see p. 209 definition), Lemmas A.1
C.2 (ii), RC 2 -formula satisfiable iff satisfiable Aleksandrov topological model
based disjoint union 2-brooms, alias forks. follows regard every
model disjoint union fork models = hf, vi, f = hW, Ri, W = {x0 , x1 , x2 },
R reflexive closure {hx0 , x1 i, hx0 , x2 i} v valuation spatial variables.
Given 0 , say fork models m1 = hf, v1 m2 = hf, v2 0 -equivalent
write m1 0 m2 , v1 (CIp) = v2 (CIp) every p 0 .
Given AtFm AtFm , say f-consequence
write |=f |= implies |= every fork model based f. said
closed (under
f-consequences) if,
, whenever |=f .

every AtFm
c
| 2
AtFm
Let c = 2
. satisfiable iff closed satisfiable.
means, particular, check whether set w (62) satisfiable, enough
| (N, w) |= p , term }.
consider closure {2

characterise |=f terms Boolean consequence relation |=. know
Appendix C.3, spatial formulas embedded one-variable fragment
first-order logic. precisely, easily shown first-order translations
formulas AtFm (equivalent to) formulas form (which actually Krom
formulas; see, e.g., Borger, Gradel, & Gurevich, 1997):
1
1
2
2
( ))2 = x 2 2
(2
x 12 22 ,
1
2
1
2
1
1
2
2
( ))2 = x 2 2
(2
x 12 22 ,
1
2
1
2

233

(63)
(64)

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

1

1

( ))2 = x 2 2
(2
1
2
1
2

1

2

x 12 22

2

1

x 12 22

2
2
x 12 22 , (65)




i2

(
Pji (x),
= CIpj ,
=
Pji (x), = ICIpj



i2

(
Pji (x),
= ICIpj ,
=
Pji (x), = CIpj ,

= 1, 2.

follows proof Lemma C.5 RC 2 -formula satisfied Aleksandrov
2 satisfied
model based disjoint union forks iff first-order

translation
ff
0
1
2
0
first-order model every fork f = hW, Ri M, W = x , x , x , x Rx1 x0 Rx2 ,
encoded domain element df Pji (x) true df iff (M, xi ) |= CIpj ,
= 1, 2 (see Fig. 11). Since definition closed sets consider Aleksandrov
models based single fork f, domains respective first-order models contain single
element df . means (63)(65) encoded Boolean formulas
2
2
1 2 ,
1
1
2
2
( )) =
(2
1 2 1 2 ,
1
2
2
1
1
1
( )) =
(2
1 2 1 2
1
2
1

1

( )) =
(2
1 2
1
2





2

1

1 2



2
2
1 2 ,




(
qji ,
= CIpj ,
=
qji , = ICIpj


(
qji ,
= ICIpj ,
=
qji , = CIpj ,




= 1, 2.

Thus, every AtFm associate conjunction -translations
formulas following holds:
Claim C.8. every AtFm , |=f

iff

|= .

construct closure AtFm check whether satisfiable,
use following resolution-like inference rules:
()

()

( %)
2
1

(I% )
2
2

()1

( )
2
1
2

%
2

%
2



2(1 )


()

(I% )
2
1
(% )
2
1

2(0


2 )

( )
2
1
2

()2

(% )
2
1
(I% )
2
1


0

= %, = I%;

= %, 0 = %;


= I%, 0 = I%;

together equivalences:
% = 2
I%,
2

% = 2
I%,
2

(% ) = 2
(I% ),
2
1
1

(% ) = 2
(I% ),
2
1
1

% = CIp p , 1 2 form % I%, 1 2 form
I% %. readily checked rules sound, derivable ,
satisfiable. hand, satisfiable regarded
234

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

unsatisfiable set binary unary propositional clauses and, using standard
resolution procedure, one construct derivation empty clause which,
turn, mimicked applications rules (and equivalences) derive
. Moreover, since propositional resolution subsumption complete (see, e.g.,
Slagle, Chang, & Lee, 1969), also derive consequences , thereby obtaining
closure.
encode rules equivalences Boolean formulas variables p ,
AtFm . instance, () () encoded
2



% 2
%
( )
( %) 2
(I% ) 2
2


2
,
2
1
2
1
respectively. Denote conjunction formulas spatial variables
. following:
Claim C.9. every AtFm , closed satisfiable iff Boolean formula
h ^

^
p
p
(66)

AtFm
2



2

satisfiable.
Finally, ensure (s3), need following completion property RC 2 :
Lemma C.10. Let closed subset AtFm , 0 0 = AtFm0 .
(i) 0 closed (ii) every fork model m0 , m0 |= 0 fork model
m0 0 |= .
Proof. Claim (i) clear. show (ii), define characteristic formula m0 0
taking:
(
^
qji , (m0 , xi ) 6|= CIpj ,


=
lji

lji =
qji ,
(m0 , xi ) |= CIpj .
p , i=1,2
j

0

m0 |= 0 follows immediately definitions 0 satisfiable.
aim show also satisfiable, would mean fork model
required. Suppose otherwise. |= . regard set unary
. According
binary clauses clause 2 |0 | literals lji , negations lji
subsumption theorem (Slagle et al., 1969), applying standard resolution rule
, derive clause lj1 i1 lj2 i2 subsumes (i.e., literals occur
). Since closed, lj1 i1 lj2 i2 among clauses ljk ik

k -translations spatial terms spatial variables 0 , conclude lj1 i1 lj2 i2
indeed among clauses 0 , contrary 0 satisfiable.
q
C.5.2 Polynomial Translation PT L RC 2 PT L
position define polynomial (at quadratic) translation
PT L RC 2 PT L. Starting given formula , construct PT L RC 2 formula
e form (61):
^

+
(CI CIp CIp0 ) ,

e =
2+
>2
P 2F
p

235

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

PT L[RC 2 ]-formula. Let 0 = {p0 | p } let denote smallest
set spatial variables containing 0 spatial variables occurring . Given

AtFm
0 formula AtFm 0
2
, denote 2
obtained 2 replacing
0
0
every occurrence p p . Consider PT L-formula


=





+
2+
P 2 F



+
2+
P 2F

>



^


2
0 ) .
( 2

AtFm
2


Lemma C.11. every PT L RC 2 -formula ,
e satisfiable tt-model based
F = hW, <i iff satisfiable temporal model based F.
Proof. () Let (M, w0 ) |= .
e Construct temporal model N = hF, Vi taking,

2 AtFm ,
}.
V(p ) = {w W | (M, w) |= 2
easy see (N, w0 ) |= .
() Let (N, w0 ) |= w0 W . every w W , set
AtFm
w = {2
| (N, w) |= p }.

Let w , w W , set non--equivalent fork models |= w .
Claim C.9, w closed satisfiable, sets w nonempty. use
elements w building blocks exhaustive states tt-model going
construct order satisfy .
First show element w successor w+1 predecessor
w1 (provided w successor predecessor, respectively). precisely,
say pair fork models = hf, vi m0 = hf, v0 suitable write m0
v(CIp0 ) = v0 (CIp), every p .
(succ) Let w , = hf, vi, let w W successor w + 1. third
conjunct ,
w AtFm0

=

0 AtFm 0
{2
| (N, w) |= p 0 }

=

0 AtFm 0
{2
| (N, w + 1) |= p }.

Therefore,
w+1 AtFm

=

AtFm
{2
| (N, w) |= p 0 }.

Now, w , |= w AtFm0 . define fork model m0 = hf, v0
taking v0 (p) = v(p0 ), p (and arbitrary otherwise), m0 |= w+1 AtFm
follows. Since w+1 closed, Lemma C.10, find fork model m00 = hf, v00
m00 m0 m00 |= w+1 . follows m00 m00 -equivalent
fork model w+1 (i.e., may assume m00 w+1 ).
(pred ) Similarly, every , = hf, vi, every w W predecessor
w 1, m00 = hf, v00 m00 w1 m00 m.
clear every fork model w every w W , define
function rm,w gives u W fork model rm,w (u) u rm,w (w) =
236

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

rm,w (u) rm,w (u + 1), whenever u + 1 successor u. Let set
functions rm,w , w W w .
ready define Aleksandrov tt-model = hF, G, Vi satisfying .
e Let
G = hW, Ri disjoint union ||-many forks fr = hWr , Rr i, Wr = {x0r , x1r , x2r }, x0r Rr x1r
x0r Rr x2r , r , let V(p, w) = {xir W | (r(w), xir ) |= CIp}, p
w W . show induction construction sub that, every w W ,
(M, w) |=

iff

(N, w) |= .

. Suppose (M, w) |= 2
(N, w) 6|= p . 2

Case = 2
/ w and, since

. follows
w closed (by Claim C.9 true w), w 6|=f 2
, r r(w) = m,
fork model w |= 2
. Conversely, (N, w) |= p

contrary (M, w) |= 2
then, construction, (M, w) |= 2 .
cases Booleans temporal operators trivial.
second conjunct
e satisfied construction, obtain (M, w0 ) |= .
e
q

References
Aiello, M., & van Benthem, J. (2002a). Logical patterns space. Barker-Plummer,
D., Beaver, D. I., van Benthem, J., & Scotto di Luzio, P. (Eds.), Words, Proofs
Diagrams, pp. 525. CSLI Publications, Stanford.
Aiello, M., & van Benthem, J. (2002b). modal walk space. Journal Applied
Non-Classical Logics, 12 (34), 319364.
Alexandroff, P. (1937). Diskrete Raume. Matematicheskii Sbornik, 2 (44), 501518.
Allen, J. (1983). Maintaining knowledge temporal intervals. Communications
ACM, 26, 832843.
Areces, C., Blackburn, P., & Marx, M. (2000). computational complexity hybrid
temporal logics. Logic Journal IGPL, 8, 653679.
Arhangelskii, A., & Collins, P. (1995). submaximal spaces. Topology Applications, 64, 219241.
Asher, N., & Vieu, L. (1995). Toward geometry common sense: semantics
complete axiomatization mereotopology. Mellish, C. (Ed.), Proceedings
14th International Joint Conference Artificial Intelligence (IJCAI-95), pp. 846
852. Morgan Kaufmann.
Balbiani, P., & Condotta, J.-F. (2002). Computational complexity propositional linear
temporal logics based qualitative spatial temporal reasoning. Armando, A.
(Ed.), Proceedings Frontiers Combining Systems (FroCoS 2002), Vol. 2309
Lecture Notes Computer Science, pp. 162176. Springer.
Balbiani, P., Tinchev, T., & Vakarelov, D. (2004). Modal logics region-based theories
space. Manuscript.
Bennett, B. (1994). Spatial reasoning propositional logic. Proceedings 4th
International Conference Knowledge Representation Reasoning, pp. 5162.
Morgan Kaufmann.
237

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Bennett, B. (1996). Modal logics qualitative spatial reasoning. Logic Journal
IGPL, 4, 2345.
Bennett, B., & Cohn, A. (1999). Multi-dimensional multi-modal logics framework
spatio-temporal reasoning. Proceedings Hot topics Spatio-temporal
reasoning workshop, IJCAI-99, Stockholm.
Bennett, B., Cohn, A., Wolter, F., & Zakharyschev, M. (2002). Multi-dimensional modal
logic framework spatio-temporal reasoning. Applied Intelligence, 17, 239251.
Blackburn, P. (1992). Fine grained theories time. Aurnague, M., Borillo, A., Borillo,
M., & Bras, M. (Eds.), Proceedings 4th European Workshop Semantics
Time, Space, Movement Spatio-Temporal Reasoning, pp. 299320, Chateau
de Bonas, France. Groupe Langue, Raisonnement, Calcul, Toulouse.
Borger, E., Gradel, E., & Gurevich, Y. (1997). Classical Decision Problem. Perspectives
Mathematical Logic. Springer.
Bourbaki, N. (1966). General topology, Part 1. Hermann, Paris Addison-Wesley.
Chagrov, A., & Zakharyaschev, M. (1997). Modal Logic, Vol. 35 Oxford Logic Guides.
Clarendon Press, Oxford.
Chlebus, B. (1986). Domino-tiling games. Journal Computer System Sciences, 32,
374392.
Clarke, B. (1981). calculus individuals based connection. Notre Dame Journal
Formal Logic, 23, 204218.
Clarke, B. (1985). Individuals points. Notre Dame Journal Formal Logic, 26, 6175.
Clarke, E., & Emerson, E. (1981). Design synthesis synchronisation skeletons using
branching time temporal logic. Kozen, D. (Ed.), Logic Programs, Vol. 131
Lecture Notes Computer Science, pp. 5271. Springer.
Clementini, E., Di Felice, P., & Hernandez, D. (1997). Qualitative representation positional information. Artificial Intelligence, 95, 317356.
Cohn, A. (1997). Qualitative spatial representation reasoning techniques. Brewka,
G., Habel, C., & Nebel, B. (Eds.), KI-97: Advances Artificial Intelligence, Vol. 1303
Lecture Notes Computer Science, pp. 130. Springer.
Davis, E. (1990). Representations Commonsense Knowledge. Morgan Kaufmann.
Degtyarev, A., Fisher, M., & Konev, B. (2003). Monodic temporal resolution. Baader,
F. (Ed.), Proceedings 19th International Conference Automated Deduction (CADE-19), Vol. 2741 Lecture Notes Artificial Intelligence, pp. 397411.
Springer.
Demri, S., & DSouza, D. (2002). automata-theoretic approach constraint LTL.
Agrawal, M., & Seth, A. (Eds.), Proceedings 22nd Conference Foundations
Software Technology Theoretical Computer Science (FST TCS 2002), Vol. 2556
Lecture Notes Computer Science, pp. 121132. Springer.
Egenhofer, M., & Franzosa, R. (1991). Point-set topological spatial relations. International
Journal Geographical Information Systems, 5, 161174.
238

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Egenhofer, M., & Herring, J. (1991). Categorizing topological relationships regions,
lines points geographic databases. Tech. rep., University Maine.
Egenhofer, M., & Sharma, J. (1993). Assessing consistency complete incomplete
topological information. Geographical Systems, 1, 4768.
Emerson, E., & Halpern, J. (1985). Decision procedures expressiveness temporal
logic branching time. Journal Computer System Sciences, 30, 124.
Fine, K. (1974). Logics containing K4, part I. Journal Symbolic Logic, 39, 229237.
Finger, M., & Gabbay, D. (1992). Adding temporal dimension logic system. Journal
Logic, Language Information, 2, 203233.
Fisher, M., Dixon, C., & Peim, M. (2001). Clausal temporal resolution. ACM Transactions
Computational Logic (TOCL), 2, 1256.
Gabbay, D., Hodkinson, I., & Reynolds, M. (1994). Temporal Logic: Mathematical Foundations Computational Aspects, Volume 1. Oxford University Press.
Galton, A., & Meathrel, R. (1999). Qualitative outline theory. Dean, T. (Ed.), Proceedings
16th International Joint Conference Artificial Intelligence (IJCAI-99), pp.
10611066. Morgan Kaufmann.
Gerevini, A., & Nebel, B. (2002). Qualitative spatio-temporal reasoning RCC-8 Allens interval calculus: Computational complexity. Proceedings 15th European
Conference Artificial Intelligence (ECAI02), pp. 312316. IOS Press.
Gerevini, A., & Renz, J. (2002). Combining topological size constraints spatial
reasoning. Artificial Intelligence, 137, 142.
Godel, K. (1933). Eine Interpretation des intuitionistischen Aussagenkalkuls. Ergebnisse
eines mathematischen Kolloquiums, 4, 3940.
Goldblatt, R. (1976). Metamathematics modal logic, Part I. Reports Mathematical
Logic, 6, 4178.
Goranko, V., & Passy, S. (1992). Using universal modality: gains questions. Journal
Logic Computation, 2, 530.
Gotts, N. (1996). axiomatic approach topology spatial information systems. Tech.
rep. 96.25, School Computer Studies, University Leeds.
Halpern, J., & Shoham, Y. (1986). propositional modal logic time intervals. Proceedings 1st Annual IEEE Symposium Logic Computer Science (LICS86),
pp. 279292. IEEE Computer Society.
Halpern, J., & Vardi, M. (1989). complexity reasoning knowledge time I:
lower bounds. Journal Computer System Sciences, 38, 195237.
Hodkinson, I. (2004). Complexity monodic packed fragment linear real time.
appear Annals Pure Applied Logic, available http://www.doc.ic.ac.uk/
/~imh/papers/cxmonlin.pdf.
Hodkinson, I., Kontchakov, R., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003).
computational complexity decidable fragments first-order linear temporal
239

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

logics. Reynolds, M., & Sattar, A. (Eds.), Proceedings TIME-ICTL 2003, pp.
9198. IEEE Computer Society.
Hodkinson, I., Wolter, F., & Zakharyaschev, M. (2000). Decidable fragments first-order
temporal logics. Annals Pure Applied Logic, 106, 85134.
Hodkinson, I., Wolter, F., & Zakharyaschev, M. (2001). Monodic fragments first-order
temporal logics: 20002001 A.D. Nieuwenhuis, R., & Voronkov, A. (Eds.), Logic
Programming, Artificial Intelligence Reasoning, Vol. 2250 Lecture Notes
Artificial Intelligence, pp. 123. Springer.
Hodkinson, I., Wolter, F., & Zakharyaschev, M. (2002). Decidable undecidable fragments first-order branching temporal logics. Proceedings 17th Annual
IEEE Symposium Logic Computer Science (LICS 2002), pp. 393402. IEEE
Computer Society.
Hughes, G., & Cresswell, M. (1996). New Introduction Modal Logic. Methuen, London.
Hustadt, U., & Konev, B. (2003). TRP++ 2.0: temporal resolution prover. Baader,
F. (Ed.), Proceedings 19th International Conference Automated Deduction
(CADE-19), Vol. 2741 Lecture Notes Computer Science, pp. 274278. Springer.
Kontchakov, R., Lutz, C., Wolter, F., & Zakharyaschev, M. (2004). Temporalising tableaux.
Studia Logica, 76, 91134.
Kutz, O., Sturm, H., Suzuki, N.-Y., Wolter, F., & Zakharyaschev, M. (2003). Logics
metric spaces. ACM Transactions Computational Logic, 4, 260294.
Ladner, R. (1977). computational complexity provability systems modal logic.
SIAM Journal Computing, 6, 467480.
Lemon, O., & Pratt, I. (1998). incompleteness modal logics space: advancing complete modal logics place. Kracht, M., de Rijke, M., Wansing, H., &
Zakharyaschev, M. (Eds.), Advances Modal Logic, Volume 1, pp. 115132. CSLI
Publications, Stanford.
Lewis, C., & Langford, C. (1932). Symbolic Logic. Appleton-Century-Crofts, New York.
Ligozat, G. (1998). Reasoning cardinal directions. Journal Visual Languages
Computing, 9, 2344.
Manna, Z., & Pnueli, A. (1992). Temporal Logic Reactive Concurrent Systems.
Springer.
McKinsey, J. (1941). solution decision problem Lewis systems S2 S4,
application topology. Journal Symbolic Logic, 6, 117134.
McKinsey, J., & Tarski, A. (1944). algebra topology. Annals Mathematics, 45,
141191.
Muller, P. (1998a). qualitative theory motion based spatio-temporal primitives.
Cohn, A., Schubert, L., & Shapiro, S. (Eds.), Proceedings 6th International
Conference Principles Knowledge Representation Reasoning (KR98), pp.
131142. Morgan Kaufmann.
240

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

Muller, P. (1998b). Space-time primitive space motion. Guarino, N. (Ed.),
Proceedings International Conference Formal Ontology Information Systems (FOIS98), Vol. 46 Frontiers Artificial Intelligence Applications, pp.
6376. IOS Press.
Nebel, B., & Burckert, H. (1995). Reasoning relations: maximal tractable subclass
Allens interval algebra. Journal ACM, 42, 4366.
Nebel, B. (1996). Artificial intelligence: computational perspective. Brewka, G. (Ed.),
Principles Knowledge Representation, pp. 237266. CSLI Publications.
Nutt, W. (1999). translation qualitative spatial reasoning problems modal
logics. Burgard, W., Christaller, T., & Cremers, A. (Eds.), Advances Artificial Intelligence. Proceedings 23rd Annual German Conference Artificial
Intelligence (KI99), Vol. 1701 Lecture Notes Computer Science, pp. 113124.
Springer.
Ono, H., & Nakamura, A. (1980). size refutation Kripke models linear
modal tense logics. Studia Logica, 39, 325333.
Orlov, I. (1928). calculus compatibility propositions. Mathematics USSR,
Sbornik, 35, 263286. (In Russian).
Post, E. (1946). variant recursively unsolvable problem. Bulletin AMS, 52,
264268.
Pratt-Hartmann, I. (2002). topological constraint language component counting.
Journal Applied Non-Classical Logics, 12, 441467.
Randell, D., Cui, Z., & Cohn, A. (1992). spatial logic based regions connection.
Nebel, B., Rich, C., & Swartout, W. (Eds.), Proceedings 3rd International
Conference Principles Knowledge Representation Reasoning (KR92), pp.
165176. Morgan Kaufmann.
Renz, J. (1998). canonical model region connection calculus. Cohn, A., Schubert,
L., & Shapiro, S. (Eds.), Proceedings 6th International Conference Principles Knowledge Representation Reasoning (KR98), pp. 330341. Morgan
Kaufmann.
Renz, J., & Nebel, B. (1998). Spatial reasoning topological information. Freksa, C.,
Habel, C., & Wender, K. (Eds.), Spatial CognitionAn Interdisciplinary Approach
Representation Processing Spatial Knowledge, Vol. 1404 Lecture Notes
Computer Science, pp. 351372. Springer.
Renz, J., & Nebel, B. (1999). complexity qualitative spatial reasoning. Artificial
Intelligence, 108, 69123.
Renz, J. (2002). canonical model region connection calculus. Journal Applied
Non-Classical Logics, 12, 469494.
Renz, J., & Nebel, B. (2001). Efficient methods qualitative spatial reasoning. Journal
Artificial Intelligence Research, 15, 289318.
Reynolds, M. (2003). complexity temporal logic general linear
time. Journal Computer System Science, 66, 393426.
241

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Reynolds, M. (2004). complexity temporal logic reals. Submitted,
available http://www.csse.uwa.edu.au/~mark/research/Online/CORT.htm.
Schwendimann, S. (1998). new one-pass tableau calculus PLTL. de Swart, H. (Ed.),
Proceedings International Conference Automated Reasoning Analytic
Tableaux Related Methods (TABLEAUX-98), Vol. 1397 Lecture Notes Artificial Intelligence, pp. 277291. Springer.
Sistla, A., & Clarke, E. (1985). complexity propositional linear temporal logics.
Journal Association Computing Machinery, 32, 733749.
Sistla, A., & German, S. (1987). Reasoning many processes. Proceedings
Second IEEE Symposium Logic Computer Science (LICS87), pp. 138153. IEEE
Computer Society.
Slagle, J., Chang, C.-L., & Lee, R. (1969). Completeness theorems semantic resolution
consequence-finding. Proceedings 1st International Joint Conference
Artificial Intelligence (IJCAI69), pp. 281286. William Kaufmann.
Smith, T., & Park, K. (1992). algebraic approach spatial reasoning. International
Journal Geographical Information Systems, 6, 177192.
Stockmeyer, L. (1974). Complexity Decision Problems Automata Theory
Logic. Ph.D. thesis, MIT.
Stockmeyer, L. (1987). Classifying computational complexity problems. Journal
Symbolic Logic, 52, 143.
Stone, M. (1937). Application theory Boolean rings general topology. Transactions AMS, 41, 321364.
Tarski, A. (1938). Der Aussagenkalkul und die Topologie. Fundamenta Mathematicae, 31,
103134.
Tsao Chen, T. (1938). Algebraic postulates geometric interpretation Lewis
calculus strict implication. Bulletin AMS, 44, 737744.
van Benthem, J. (1995). Temporal logic. Gabbay, D., Hogger, C., & Robinson, J. (Eds.),
Handbook Logic Artificial Intelligence Logic Programming, Vol. 4, pp. 241
350. Oxford Scientific Publishers.
van Emde Boas, P. (1997). convenience tilings. Sorbi, A. (Ed.), Complexity, Logic
Recursion Theory, Vol. 187 Lecture Notes Pure Applied Mathematics,
pp. 331363. Marcel Dekker Inc.
Wolper, P. (1985). tableau method temporal logic: overview. Logique et Analyse,
28, 119152.
Wolter, F. (1996). Properties tense logics. Mathematical Logic Quarterly, 42, 481500.
Wolter, F., & Zakharyaschev, M. (2000a). Spatial reasoning RCC-8 Boolean region
terms. Horn, W. (Ed.), Proceedings 14th European Conference Artificial
Intelligence (ECAI 2000), pp. 244248. IOS Press.
Wolter, F., & Zakharyaschev, M. (2000b). Spatio-temporal representation reasoning
based RCC-8. Cohn, A., Giunchiglia, F., & Seltman, B. (Eds.), Proceedings
242

fiCombining Spatial Temporal Logics: Expressiveness vs. Complexity

7th Conference Principles Knowledge Representation Reasoning (KR
2000), pp. 314. Morgan Kaufmann.
Wolter, F., & Zakharyaschev, M. (2002). Qualitative spatio-temporal representation
reasoning: computational perspective. Lakemeyer, G., & Nebel, B. (Eds.), Exploring Artificial Intelligence New Millenium, pp. 175216. Morgan Kaufmann.
Zimmermann, K. (1995). Measuring without measures: delta-calculus. Frank, A., &
Kuhn, W. (Eds.), Proceedings 2nd International Conference Spatial Information Theory (COSIT), Vol. 988 Lecture Notes Computer Science, pp. 5967.
Springer.

243

fiJournal Artificial Intelligence Research 23 (2005) 441-531

Submitted 11/04; published 4/05

Generalizing Boolean Satisfiability III: Implementation
Heidi E. Dixon
Matthew L. Ginsberg

dixon@otsys.com
ginsberg@otsys.com

Time Systems, Inc.
1850 Millrace, Suite 1
Eugene, 97403 USA

David Hofer
Eugene M. Luks

hofer@cs.uoregon.edu
luks@cs.uoregon.edu

Computer Information Science
University Oregon
Eugene, 97403 USA

Andrew J. Parkes

parkes@cirl.uoregon.edu

CIRL
1269 University Oregon
Eugene, 97403 USA

Abstract
third three papers describing zap, satisfiability engine substantially
generalizes existing tools retaining performance characteristics modern highperformance solvers. fundamental idea underlying zap many problems passed
engines contain rich internal structure obscured Boolean representation
used; goal define representation structure apparent
exploited improve computational performance. first paper surveyed existing
work (knowingly not) exploited problem structure improve performance
satisfiability engines, second paper showed structure could understood
terms groups permutations acting individual clauses particular Boolean
theory. conclude series discussing techniques needed implement ideas,
reporting performance variety problem instances.

1. Introduction
third series three papers describing zap, satisfiability engine
substantially generalizes existing tools retaining performance characteristics
modern high-performance solvers zChaff (Moskewicz, Madigan, Zhao, Zhang, &
Malik, 2001). first two papers series, made arguments effect that:
Many Boolean satisfiability problems incorporate rich structure reflects properties domain problems arise, recent improvements
performance satisfiability engines understood terms ability
exploit structure (Dixon, Ginsberg, & Parkes, 2004b, refer
zap1).
structure understood terms groups (in algebraic sense)
permutations acting individual clauses (Dixon, Ginsberg, Luks, & Parkes, 2004a,
refer zap2).
c
2005
AI Access Foundation. rights reserved.

fiDixon, Ginsberg, Hofer, Luks & Parkes

showed implementation based ideas could expected combine
attractive computational properties variety recent ideas, including efficient implementations unit propagation (Zhang & Stickel, 2000) extensions Boolean language include cardinality pseudo-Boolean constraints (Barth, 1995; Dixon & Ginsberg,
2000; Hooker, 1988), parity problems (Tseitin, 1970), limited form quantification
known qprop (Ginsberg & Parkes, 2000). paper, discuss implementation
prover based ideas, describe performance pigeonhole, parity
clique coloring problems. classes problems known exponentially difficult
conventional Boolean satisfiability engines, formalization also highlights
group-based nature reasoning involved.
technical point view, difficult three zap papers; need
draw algorithms theoretical constructions zap2 results computational group theory (GAP Group, 2004; Seress, 2003) regarding implementation.
overall plan describing implementation follows:
1. Section 2 review material zap2. begin Section 2.1 presenting
Boolean satisfiability algorithms hope generalize basic algebraic
ideas underlying zap. Section 2.2 describes group-theoretic computations required
zap implementation.
2. Section 3 gives brief necessarily incomplete introduction ideas
computational group theory use.
3. Sections 4 5 describe implementations computations discussed Section 2. basic construction, describe algorithm used give
example computation action. existing implementation something public domain system gap (2004), provide pointer
implementation; concepts needed implement scratch, additional
detail provided.
4. Section 6 extends basic algorithms Section 5 deal unit propagation,
want compute single unit clause instance, list
unit consequences augmented clause.
5. Section 7 discusses implementation Zhang Stickels (2000) watched literal
idea setting.
6. Section 8 describes technique used select among possible resolvents
two augmented clauses. functionality analog conventional
prover, single ground reason truth falsity given
variable. reasons augmented clauses, may variety ways
ground instances clauses combined.
7. describing algorithms, present experimental results regarding performance Sections 9 10. Section 9 reports performance zaps individual
algorithmic components, Section 10 contrasts zaps overall performance
cnf-based predecessors.1 Since focus paper algorithms
1. description zaps input language contained Appendix B.

442

fiZAP 3: Implementation

needed zap, report performance relatively theoretical examples
clearly involve group-based reasoning. Performance wider range problem
classes reported elsewhere.
8. Concluding remarks appear Section 11.
Except Section 3, proofs generally deferred Appendix interests maintaining continuity exposition. Given importance computational group
theory ideas presenting, strongly suggest reader work
proofs Section 3 paper.
long complex paper; make apologies. Zap attempt synthesize
two different fields, complex right: computational group theory
implementations Boolean satisfiability engines. Computational group theory, addition
inherent complexity, likely foreign AI audience. Work complete
algorithms Boolean satisfiability also become increasingly sophisticated
past decade so, introduction substantial nonintuitive modifications
original dpll algorithm relevance-bounded learning (Bayardo & Miranker, 1996;
Bayardo & Schrag, 1997; Ginsberg, 1993) watched literals (Zhang & Stickel, 2000).
bring two fields together, see wide range techniques
computational group theory relevant problems interest us; goal also
simply translate dpll new setting, show recent work
Boolean satisfiability moved across. least one case (Lemma 5.26), also
need extend existing computational group theory results. finally, new
satisfiability techniques possibilities arise synthesis
proposing (Section 8), describe well.
paper intended self-contained. assume throughout reader
familiar material presented zap2; results paper
repeated convenience, accompanying text intended stand alone.
Finally spite disclaimers previous two paragraphs paper
intended complete. goal present practical minimum required
implement effective group-based reasoning system. results obtained,
theoretical described zap2 practical described here, excite us.
excited number issues yet explored. primary goal
present foundation needed interested researchers explore ideas
us.

2. ZAP Fundamentals Basic Structure
overview zap involves summarizing work two distinct areas: existing Boolean
satisfiability engines, group-theoretic elements underlying zap.
2.1 Boolean Satisfiability
begin description architecture modern Boolean satisfiability engines.
start unit propagation procedure, describe follows:

443

fiDixon, Ginsberg, Hofer, Luks & Parkes

Definition 2.1 Given Boolean satisfiability problem described terms set C
clauses, partial assignment assignment values (true false) subset
variables appearing C. represent partial assignment P sequence
consistent literals P = hli appearance vi sequence means vi
set true, appearance vi means vi set false.
annotated partial assignment sequence P = h(li , ci )i ci reason
associated choice li . ci = true, means variable set result
branching decision; otherwise, ci clause entails li virtue choices
previous lj j < i. annotated partial assignment called sound respect
set constraints C C |= ci reason ci . (See zap2 additional details.)
Given (possibly annotated) partial assignment P , denote S(P ) literals
satisfied P , U (P ) set literals unvalued P .
Procedure 2.2 (Unit propagation) compute Unit-Propagate(C, P ) set C
clauses annotated partial assignment P = h(l1 , c1 ), . . . , (ln , cn )i:
1 c C c S(P ) = |c U (P )| 1
2
c U (P ) =
3
li literal c highest index P
4
return htrue, resolve(c, ci )i
5
else l literal c unassigned P
6
P hP, (l, c)i
7 return hfalse, P
result returned depends whether contradiction encountered
propagation, first result returned true contradiction found
false none found. former case, clause c unvalued literals
(line 2), li last literal set c, ci reason li set way caused
c unsatisfiable. resolve c ci return result new nogood
problem question. Otherwise, eventually return partial assignment, augmented
include variables set propagation process.
Given unit propagation, overall inference procedure following:
Procedure 2.3 (Relevance-bounded learning, rbl) Given sat problem C, set
learned nogoods annotated partial assignment P , compute rbl(C, D, P ):

444

fiZAP 3: Implementation

1 hx, yi unit-propagate(C D, P )
2 x = true
3
c
4
c empty
5
return failure
6
else remove successive elements P c unit
7
learn(D, P, c)
8
return rbl(C, D, P )
9
else P
10
P solution C
11
return P
12
else l literal assigned value P
13
return rbl(C, D, hP, (l, true)i)
might expected, procedure recursive. point unit propagation produces contradiction c, use (currently unspecified) learn procedure incorporate c
solvers current state, recurse. c empty, means derived
contradiction procedure fails. backtracking step (line 6), backtrack
c satisfiable, enables unit propagation. technique used
zChaff (Moskewicz et al., 2001). leads increased flexibility choice variable
assigned backtrack complete, generally improves performance.
unit propagation indicate presence contradiction produce solution
problem question, pick unvalued literal, set true, recurse again.
Note dont need set literal l true false; eventually need backtrack
set l false, handled modification P line 6.
Finally, need present procedure used incorporate new nogood
clausal database C. order that, make following definition:
Definition 2.4 Let li clause, denote c, let P partial
assignment. say possible value c P given
poss(c, P ) = |{i|li 6 P }| 1
ambiguity possible, write simply poss(c) instead poss(c, P ).
words, poss(c) number literals either already satisfied valued P ,
reduced one (since clause requires least one true literal).
Note poss(c, P ) = |c [U (P ) S(P )]| 1, since expression one less
number potentially satisfied literals c.
possible value clause essentially measure authors called
irrelevance (Bayardo & Miranker, 1996; Bayardo & Schrag, 1997; Ginsberg, 1993).
unsatisfied clause c poss(c, P ) = 0 used unit propagation; say
clause unit. poss(c, P ) = 1, means change single variable
lead unit propagation, on. notion learning used relevance-bounded
inference captured by:
445

fiDixon, Ginsberg, Hofer, Luks & Parkes

Procedure 2.5 Given set clauses C annotated partial assignment P , compute learn(C, P, c), result adding C clause c removing irrelevant clauses:
1
2

remove C C poss(d, P ) > k
return C {c}

hope familiar; not, please refer zap2 papers
cited fuller explanations.
zap, continue work procedures approximately current form,
replace idea clause (a disjunction literals) augmented clause:
Definition 2.6 augmented clause n-variable Boolean satisfiability problem
pair (c, G) c Boolean clause G group G Wn . (nonaugmented) clause c0 instance augmented clause (c, G) g G
c0 = cg .2 clause c called base instance (c, G).
Roughly speaking, augmented clause consists conventional clause group G
permutations literals theory; intent act clause
element group still get clause part original theory.
group G required subgroup group permutations complementations
(Harrison, 1989) Wn = S2 Sn , permutation g G permute variables
problem flip signs arbitrary subset well. showed zap2
suitably chosen groups correspond cardinality constraints, parity constraints (the group
flips signs even number variables), universal quantification finite
domains.
must lift previous three procedures augmented setting. unit
propagation, example, instead checking see clause c C unit given
assignments P , check see augmented clause (c, G) unit instance.
that, procedure essentially unchanged Procedure 2.2:
Procedure 2.7 (Unit propagation) compute Unit-Propagate(C, P ) set
clauses C annotated partial assignment P = h(l1 , c1 ), . . . , (ln , cn )i:
1
2
3
4
5
6
7

(c, G) C g G cg S(P ) = |cg U (P )| 1
cg U (P ) =
li literal cg highest index P
return htrue, resolve((cg , G), ci )i
else l literal cg unassigned P
P hP, (l, (cg , G))i
return hfalse, P
basic inference procedure also virtually unchanged:

2. zap2 used computational group theory community, denote image clause
c group element g cg instead possibly familiar g(c). explained zap2,
reflects fact composition f g two permutations acts f first g second.

446

fiZAP 3: Implementation

Procedure 2.8 (Relevance-bounded learning, rbl) Given sat problem C, set
learned clauses D, annotated partial assignment P , compute rbl(C, D, P ):
1 hx, yi unit-propagate(C D, P )
2 x = true
3
(c, G)
4
c empty
5
return failure
6
else remove successive elements P c unit
7
learn(D, P, (c, G))
8
return rbl(C, D, P )
9
else P
10
P solution C
11
return P
12
else l literal assigned value P
13
return rbl(C, D, hP, (l, true)i)
line 3, although unit propagation returns augmented clause (c, G), base instance
c still reason backtrack virtue line 6 Procedure 2.7. follows
line 6 Procedure 2.8 unchanged Boolean version.
lift Procedure 2.5 setting, need augmented version Definition 2.4:
Definition 2.9 Let (c, G) augmented clause, P partial assignment.
poss((c, G), P ) mean minimum possible value instance (c, G),
poss((c, G), P ) = min poss(cg , P )
gG

Procedure 2.5 used unchanged, augmented clause instead
simple one. effect Definition 2.9 cause us remove augmented clauses
every instance irrelevant. Presumably, useful retain clause long
relevant instance.
zap2, showed proof engine built around three procedures would
following properties:
Since number generators group made logarithmic group size,
would achieve exponential improvements basic representational efficiency.
Since k-relevant nogoods retained search proceeds, memory requirements remain polynomial size problem solved.
produce polynomially sized proofs pigeonhole clique coloring problems, parity problem.
generalizes first-order inference provided quantifiers universal
domains quantification finite.
stated without proof (and show paper) unit propagation procedure 2.7 implemented way generalizes subsearch (Ginsberg & Parkes,
2000) Zhang Stickels (2000) watched literal idea.
447

fiDixon, Ginsberg, Hofer, Luks & Parkes

2.2 Group-Theoretic Elements
Examining three procedures, elements new relative Boolean engines
following:
1. line 1 unit propagation procedure 2.7, need find unit instances
augmented clause (c, G).
2. line 4 procedure 2.7, need compute resolvent two augmented
clauses.
3. line 1 learning procedure 2.5, need determine augmented clause
relevant instances.
first third needs different second. resolution, need
following definitions:
Definition 2.10 permutation p set p = S, p|S mean
restriction p given set, say p lifting p|S back original
set p acts.
Definition 2.11 set , denote Sym() group permutations .
G Sym() subgroup group , say G acts S.3
Definition 2.12 Suppose G acts set S. x S, orbit x G,
denoted xG , given xG = {xg |g G}. S, G-closure ,
denoted G , set
G = {tg |t g G}

Definition 2.13 K1 , . . . , Kn G1 , . . . , Gn Sym(), say permutation Sym() stable extension G1 , . . . , Gn K1 , . . . , Kn gi Gi
i, |K Gi = gi |K Gi . denote set stable extensions G1 , . . . , Gn




K1 , . . . , Kn stab(Ki , Gi ).
set stable extensions stab(Ki , Gi ) closed composition, therefore
subgroup Sym().
Definition 2.14 Suppose (c1 , G1 ) (c2 , G2 ) augmented clauses. result resolving (c1 , G1 ) (c2 , G2 ), denoted resolve((c1 , G1 ), (c2 , G2 )),
augmented clause (resolve(c1 , c2 ), stab(ci , Gi ) Wn ).
follows definitions computing resolvent two augmented
clauses required Procedure 2.7 essentially matter computing set stable
extensions groups question. return problem Section 4.
two problems viewed instances following:
3. convenience, depart standard usage permit G map points images outside
S.

448

fiZAP 3: Implementation

Definition 2.15 Let c clause, viewed set literals, G group permutations
acting c. fix sets literals U , integer k. say ktransporter problem finding g G cg = |cg U | k,
reporting g exists.
find unit instance (c, G), set set satisfied literals U
set unvalued literals. Taking k = 1 implies searching instance
satisfied one unvalued literal.
find relevant instance, set = U set satisfied unvalued
literals. Taking k relevance bound corresponds search relevant instance.
remainder theoretical material paper therefore focused two
problems: computing stable extensions pair groups, solving k-transporter
problem. discuss techniques used solve two problems, present
brief overview computational group theory generally.

3. Computational Group Theory
group theory large computational group theory specifically (the study effective computational algorithms solve group-theoretic problems) far broad
allow detailed presentations single journal paper. generally refer
Rotmans Introduction Theory Groups (1994) general information,
Seress Permutation Group Algorithms (2003) computational group theory specifically, although many excellent texts areas. also abbreviated
introduction group theory zap2.
cannot substitute references, goal provide enough
general understanding computational group theory possible work
examples follows. mind, three basic ideas hope
convey:
1. Stabilizer chains. underlie fundamental technique whereby large groups
represented efficiently. also underlie many subsequent computations done
using groups.
2. Group decompositions. Given group G subgroup H < G, H used
natural way partition G. partitions partitioned using
subgroup H, on; gradual refinement underpins many search-based
group algorithms developed.
3. Lex-leader search. general, possible establish lexicographic ordering
elements permutation group; searching element group
particular property (as k-transporter problem), assume without loss
generality looking element minimal ordering.
often allows search pruned, since portion search
shown contain minimal element eliminated.

449

fiDixon, Ginsberg, Hofer, Luks & Parkes

3.1 Stabilizer Chains
fact group G described terms exponentially smaller number
generators attractive representational point view, many issues
arise large set clauses represented way. Perhaps fundamental
simple membership: tell fixed clause c0 instance
augmented clause (c, G)?
general, instance 0-transporter problem; need g G
cg , image c g, intersect complement c0 . simpler
clearly related problem assumes fixed permutation g cg = c0 ;
g G not? Given representation G terms simply generators,
obvious determined quickly.
course, G represented via list elements, could sort elements
lexicographically use binary search determine g included. Virtually
problem interest us solved time polynomial size groups involved,
would like better, solving problems time polynomial total size
generators, therefore generally polynomial logarithm size
groups (and polylog size original clausal database). call procedure
polynomial indeed polytime number generators G size
set literals G acts. polynomial procedures
assured zaps representational efficiencies mature computational gains.4
membership problem, determining g G given representation G
terms generators, need coherent way understanding structure
group G itself. suppose G subgroup group Sym() symmetries
set , enumerate elements = {l1 , . . . , ln }.
subset G[2] G fixes l1 h G[2] ,
h
l1 = l1 . easy see G[2] closed composition, since two elements fix
l1 , composition. follows G[2] subgroup G. fact, have:
Definition 3.1 Given group G acting set subset L , point stabilizer
L subgroup GL G g G lg = l every l L. set stabilizer
L subgroup G{L} G g G Lg = L.
defined G[2] point stabilizer l1 , go define G[3]
point stabilizer l2 within G[2] , G[3] fact point stabilizer {l1 , l2 } G.
Similarly, define G[i+1] point stabilizer li G[i] thereby construct
chain stabilizers
G = G[1] G[2] G[n] = 1
last group necessarily trivial n 1 points stabilized,
last point must also.
want describe G terms generators, assume describe
G[i] terms generators, furthermore, generators G[i]
superset generators G[i+1] . G[i+1] subgroup G[i] .
4. development computationally efficient procedures solving permutation group problems appears
begun Sims (1970) pioneering work stabilizer chains.

450

fiZAP 3: Implementation

Definition 3.2 strong generating set group G Sym(l1 , . . . , ln ) set
generators G property
hS G[i] = G[i]
= 1, . . . , n.
usual, hgi denotes group generated gi .
easy see generating set strong case property discussed
above, G[i] generated incrementally G[i+1] generators
fact elements G[i] G[i+1] .
example, suppose G = S4 , symmetric group 4 elements (which
denote 1, 2, 3, 4). hard see S4 generated 4-cycle (1, 2, 3, 4)
transposition (3, 4), strong generating set. G[2] subgroup
S4 stabilizes 1 (and therefore isomorphic S3 , since randomly permute
remaining three points)
hS G[2] = h(3, 4)i = G[3] 6= G[2]

(1)

want strong generating set, need add (2, 3, 4) similar permutation
generating set, (1) becomes
hS G[2] = h(2, 3, 4), (3, 4)i = G[2]
slightly interesting example. Given permutation, always possible
write permutation composition transpositions. One possible construction
maps 1 supposed go, ignores rest construction,
on. Thus example
(1, 2, 3, 4) = (1, 2)(1, 3)(1, 4)

(2)

order composition left right, 1 maps 2 virtue
first transposition left unaffected two, on.
representation permutation terms transpositions unique,
parity number transpositions is; permutation always represented
product even odd number transpositions, both. Furthermore,
product two transposition products lengths l1 l2 obviously represented
product length l1 + l2 , follows product two even permutations
even, have:
Definition 3.3 alternating group order n, denoted , subgroup
even permutations Sn .
strong generating set ? fix first n 2 points,
[n1]
transposition (n 1, n) obviously odd, must
= 1, trivial group.
[i]
smaller i, get subset taking generators Sn operating
necessary transposition (n 1, n) make even. hard
451

fiDixon, Ginsberg, Hofer, Luks & Parkes

see n-cycle odd n even (consider (2) above), given strong
generating set
{(n 1, n), (n 2, n 1, n), . . . , (2, 3, . . . , n), (1, 2, . . . , n)}
Sn , strong generating set n odd
{(n 2, n 1, n), (n 1, n)(n 3, n 2, n 1, n), . . . , (n 1, n)(2, 3, . . . , n), (1, 2, . . . , n)}
n even
{(n 2, n 1, n), (n 1, n)(n 3, n 2, n 1, n), . . . , (2, 3, . . . , n), (n 1, n)(1, 2, . . . , n)}
simplify expressions slightly get
{(n 2, n 1, n), (n 3, n 2, n 1), . . . , (2, 3, . . . , n 1), (1, 2, . . . , n)}
n odd
{(n 2, n 1, n), (n 3, n 2, n 1), . . . , (2, 3, . . . , n), (1, 2, . . . , n 1)}
n even.
Given strong generating set, easy compute size original group G.
this, need following well known definition result:
Definition 3.4 Given groups H G g G, define Hg set hg
h H. g, say Hg (right) coset H G.
Proposition 3.5 Let Hg1 Hg2 two cosets H G. |Hg1 | = |Hg2 |
cosets either identical disjoint.
words, given subgroup H group G, cosets H partition G.
leads to:
Definition 3.6 groups H G, index H G, denoted [G : H], number
distinct cosets H G.
Corollary 3.7 finite group G, [G : H] =

|G|
|H| .

Given cosets partition original group G, natural think
defining equivalence relation G, x x belong
coset H. have:
Proposition 3.8 x xy 1 H.
Proof. xy 1 = h H x coset Hg x = h0 g h0 H,
= h1 x = h1 h0 g coset. Conversely, x = hg = h0 g
coset, xy 1 = hgg 1 h01 = hh01 H.
Many equivalence relations groups form. Indeed, right invariant
equivalence relation elements group G (so x y, xz yz
z G), H G cosets H define equivalence relation.
[i]
Returning stabilizer chains, recall denote liG orbit li G[i]
(i.e, set points G[i] maps li ). have:
452

fiZAP 3: Implementation

Proposition 3.9 Given group G acting set {l1 , . . . , ln } associated stabilizer
chain G[1] G[n] ,
[i]
|G| =
|liG |
(3)


Proof. know
|G| =

|G|
|G[2] | = [G : G[2] ]|G[2] |
|G[2] |

inductively
|G| =


[G[i] : G[i+1] ]


easy see distinct cosets G[i+1] G[i] correspond exactly points
G[i] maps li ,
[i]
[G[i] : G[i+1] ] = |liG |
result follows.
Note expression (3) easy compute given strong generating set.
example, given strong generating set {(1, 2, 3, 4), (2, 3, 4), (3, 4)} S4 , clear
[3]
[2]
S4 = h(3, 4)i orbit 3 size 2. orbit 2 S4 = h(2, 3, 4), (3, 4)i
[1]
size 3, orbit 1 S4 size 4. total size group 4! = 24, hardly
surprise.
A4 , strong generating set {(3, 4)(1, 2, 3, 4), (2, 3, 4)} = {(1, 2, 3), (2, 3, 4)}.
[2]
[1]
orbit 2 A4 = h(2, 3, 4)i clearly size 3, orbit 1 A4 = A4
size 4. |A4 | = 12. general, course, exactly two cosets alternating
group odd permutations constructed multiplying even
permutations fixed transposition t. Thus |An | = n!/2.
evaluate size using strong generators realizing orbit 1
size n, 2 size n 1, on, orbit n 2 size 3.
orbit n 1 size 1, however, since transposition (n 1, n) . Thus
|An | = n!/2 before.
also use strong generating set test membership following way.
Suppose group G described terms strong generating set (and therefore
stabilizer chain G[1] G[n] ), specific permutation . (1) = k,
two possibilities:
1. k orbit 1 G = G[1] , clearly 6 G.
2. k orbit 1 G[1] , select g1 G[1] 1g1 = g1 (1) = k. construct
1 = g11 , fixes 1, determine recursively 1 G[2] .
end process, stabilized elements moved G,
n+1 = 1. so, original G; not, 6 G. procedure known
sifting.
Continuing example, let us see 4-cycle = (1, 2, 3, 4) S4 A4 .
[1]
former, see (1) = 2 (1, 2, 3, 4) S4 . produces 1 = 1,
stop conclude S4 (once again, hardly surprise).
453

fiDixon, Ginsberg, Hofer, Luks & Parkes

[1]

second, know (1, 2, 3) A4 get 1 = (1, 2, 3, 4)(1, 2, 3)1 =
(3, 4). could actually stop, since (3, 4) obviously odd, let us continue
procedure. Since 2 fixed 1 , 2 = 1 . 3 moved 4 2 ,
[3]
A4 trivial group, conclude correctly (1, 2, 3, 4) 6 A4 .
3.2 Coset Decomposition
group problems considering (e.g., k-transporter problem)
subsume described zap1 subsearch (Dixon et al., 2004b; Ginsberg & Parkes,
2000). Subsearch known NP-hard, follows k-transporter must well.
suggests group-theoretic methods solving involve search
way.
search involves potential examination instances augmented
clause (c, G), or, group theoretic terms, potential examination member
group G. computational group theory community often approaches search
problem gradually decomposing G smaller smaller cosets. call
coset decomposition tree produced, root tree entire group G
leaf nodes individual elements G:
Definition 3.10 Let G group, G[1] G[n] stabilizer chain it. coset
decomposition tree G tree whose vertices ith level cosets G[i]
parent particular G[i] g coset G[i1] contains it.
particular level i, cosets correspond points sequence hl1 , . . . , li
mapped, points image li identifying children particular
node level 1.
example, suppose consider augmented clause
(a b, Sym(a, b, c, d))

(4)

corresponding collection ground clauses
ab
ac
ad
bc
bd
cd
Suppose also working assignment b true c
false, trying determine instance (4) unsatisfied. Assuming
take l1 l4 = d, coset decomposition tree associated S4
following:

454

fiZAP 3: Implementation

b, c, d)
sPSym(a,
PP

@

PP

@
PP


PP
@

PP


@
PP


PP (ad)

Sym(b, c, d)
(ab)s
@
P
@s(ac)
Ps































Sym(c, d)s
(bc)
(bd)
(bc)
(bd)
(bc)
(bd)
(bc)
AAs(bd)



1
1
1










B
B
B
B




E
E
E
E
B
B
B
B
E
E
E
E








B
B
B
B




E
E
E
E
B
B
B
B
E
E
E
E








EEs BBs EEs BBs EEs BBs EEs BBs
1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd)
* *
explanation notation surely order. nodes lefthand
edge labeled associated groups; example, node level 2 labeled
Sym(b, c, d) point fixed b, c still allowed
vary.
move across row, find representatives cosets considered. moving across second row, first entry (ab) means taking
coset basic group Sym(b, c, d) obtained multiplying element (ab)
right. coset maps uniformly b.
lower rows, multiply coset representatives associated nodes
leading root. third node third row, labeled (bd), corresponds
coset Sym(c, d) (bd).5 two elements coset (bd) (cd)(bd) = (bdc).
point b uniformly mapped d, fixed, c either fixed mapped b.
fourth point row corresponds coset
Sym(c, d) (ab) = {(ab), (cd)(ab)}
point uniformly mapped b, b uniformly mapped a. c
swapped not.
fifth point coset
Sym(c, d) (bc)(ab) = Sym(c, d) (abc) = {(abc), (abcd)}

(5)

still uniformly mapped b, b uniformly mapped c. c mapped
either d.
fourth line, basic group trivial single member coset
obtained multiplying coset representatives path root. Thus ninth
tenth nodes (marked asterisks tree) correspond permutations (abc)
(abcd) respectively, indeed partition coset (5).
5. here, occasionally denote group multiplication operator explicitly improve
clarity typesetting.

455

fiDixon, Ginsberg, Hofer, Luks & Parkes

Understanding structure used search straightforward. root,
original augmented clause (4) may indeed unsatisfiable instances. move
first child, know image a, instance clause
question x x. Since true assignment question, follows
clause must satisfied. similar way, mapping b also must produce satisfied
clause. search space already reduced to:
b, c, d)
sPSym(a,

@PPP
PP
@
PP
@
PP
PP
@
PP
@
PP
@s(ac)
Ps(ad)















1s
1s
s(bc) AAs(bd)
s(bc) AAs(bd)
B
B
E
E


B
B
E
E


E
E
B
B




B
B
E
E


E
E
B
Es Bs Es BBs











Sym(b, c, d)s

(ab)s

1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd)
map c, first point next row corresponds mapping b b,
producing satisfiable clause. map b (the next node; b mapped c
node c mapped permutation (ac) labeling parent), also get
satisfiable clause. map b d, eventually get unsatisfiable clause, although
clear recognize without expanding two children. case
mapped similar, final search tree is:
Sym(a, b, c, d)
P

@PPP
PP
@
PP
@
PP
PP
@
PP
(ac)
@
PP
@s
Ps(ad)












(bc)
(bd)
(bc)

AAs(bd)
1s
1




B
E
B
E
B
E
B
E
BBs
EEs









Sym(b, c, d)





(ab)s

1 (cd)

456

1 (cd)

fiZAP 3: Implementation

Instead six clauses might need examined instances original
(4), four leaf nodes need considered. internal nodes pruned
pruned without generation, since values need considered
necessarily c (the unsatisfied literals theory). level, then,
search space becomes:
Sym(a, b, c, d)
P
@PPP
PP
@
PP
@
PP
PP
@
PP
(ac)
@
PP
@s
Ps(ad)




AAs(bd)
s(bc)
B
E
B
E
B
E
B
E
B
Bs
EEs

1 (cd)

1 (cd)

3.3 Lex Leaders
Although remaining search space example already examines fewer leaf nodes
original, still appears redundancy. understand one possible
simplification, recall searching group element g cg unsatisfied
given current assignment. Since group element suffices, (if wish)
search group element smallest lexicographic ordering group
itself:
Definition 3.11 Let G Sym() group, = 1 , . . . , n ordering
elements . g1 , g2 G, write g1 < g2 jg1 = jg2
j < ig1 < ig2 .
Since ordering defined Definition 3.11 total order, immediately have:
Lemma 3.12 Sym() ordered set , unique minimal
element.
minimal element typically called lexicographic leader lex leader S.
example, imagine solution (i.e., group element corresponding
unsatisfied instance) right hand node depth three. would
necessarily also analogous solution preceding node depth three,
since two search spaces sense identical. two hypothetical group elements
would identical except images b would swapped. Since group elements
left hand node precede right hand node lexicographic
457

fiDixon, Ginsberg, Hofer, Luks & Parkes

ordering, follows lexicographically least element (which looking
for) right hand node, therefore pruned. search space
becomes:
sPSym(a, b, c, d)
@PPP
PP
@
PP
@
PP
PP
@
PP
(ac)
@
PP
@s
Ps(ad)




AAs(bd)
B
B
B
B
BBs

1 (cd)
particular technique quite general: whenever searching group element particular property, restrict search lex leaders set
elements prune search space basis. Seress (2003) provides
complete discussion context problems typically considered computational
group theory; example context k-transporter problem specifically
found Section 5.5.
Finally, note two remaining leaf nodes equivalent, since refer
instance know images b, overall instance fixed
choices relevant. assuming variables problem ordered
clause considered first, finally prune search depth
three get:
sPSym(a, b, c, d)
@PPP
PP
@
PP
PP
@
PP
@
PP
@
PP
Ps(ad)
@s(ac)




AAs(bd)

single leaf node need considered.
return application ideas zap, stress
scratched surface computational group theory whole. field broad
458

fiZAP 3: Implementation

developing rapidly, implementation zap based ideas appear
Seress gap code. Indeed, name chosen reflect zaps heritage
outgrowth zChaff Gap.6

4. Augmented Resolution
turn zap-specific requirements. First, definition augmented
resolution, involves computing group stable extensions groups appearing
resolvents. Specifically, augmented clauses (c1 , G1 ) (c2 , G2 ) need
compute group G stable extensions G1 G2 . Recalling Definition 2.13,
group permutations property g1 G1
|cG1 = g1 |cG1
1

1


similarly g2 , G2 c2 . viewing clauses ci sets, cG

closure ci Gi (recall Definition 2.12).
example, consider two clauses

(c1 , G1 ) = (a b, h(ad), (be), (bf )i)

(c2 , G2 ) = (c b, h(be), (bg)i)
2
closure c1 G1 {a, b, d, e, f } cG
= {b, c, e, g}. therefore need
2
find permutation restricted {a, b, d, e, f }, element
h(ad), (be), (bf )i, restricted {b, c, e, g} element h(be), (bg)i.
second condition, know c cannot moved , permutation
b, e g acceptable (be) (bg) generate symmetric group Sym(b, e, g).
second restriction impact image a, f .
first condition, know swapped left unchanged,
permutation b, e f acceptable. recall second condition
must also permute b, e g. conditions combine imply cannot move f
g, since move either would break condition other. swap b e
not, group stable extensions h(ad), (be)i, construction
return.

Procedure 4.1 Given augmented clauses (c1 , G1 ) (c2 , G2 ), compute stab(ci , Gi ):
6. authors zChaff Moskewicz, Madigan, Zhao, Zhang Malik; selection Z
include acronym surely unfair Moskewicz, Madigan Malik. Zmap didnt quite
ring it, however, hope implicitly excluded authors accept apologies
choice.

459

fiDixon, Ginsberg, Hofer, Luks & Parkes

1
2
3
4
5
6
7
8
9

G2
1
c closure1 cG
1 , c closure2 c2
g restrict1 G1 |c closure1 , g restrict2 G2 |c closure2
C c closure1 c closure2
g stab1 g restrict1{C } , g stab2 g restrict2{C }
g int g stab1 |C g stab2 |C
{gi } {generators g int}
{l1i } {gi , lifted g stab1 }, {l2i } {gi , lifted g stab2 }
0 } {l |
{l2i
2i c closure2 C }
0 }i
return hg restrict1C , g restrict2C , {l1i l2i

Proposition 4.2 result returned Procedure 4.1 stab(ci , Gi ).
proof Appendix A; here, present example computation use
discuss computational issues surrounding Procedure 4.1. example use
began section, modify G1 h(ad), (be), (bf ), (xy)i instead
earlier h(ad), (be), (bf )i. new points x dont affect set instances
way, thus affect resolution computation, either.
1
1. c closurei cG
1 . amounts computing closures ci Gi ;
described earlier, c closure1 = {a, b, d, e, f } c closure2 = {b, c, e, g}.

2. g restricti Gi |c closurei . Here, restrict group act corresponding c closurei . example, g restrict2 = G2 g restrict1 =
h(ad), (be), (bf )i irrelevant points x removed.
Note always possible restrict group arbitrary set; one cannot
restrict permutation (xy) set {x} need add well.
case, possible restrict Gi c closurei , since latter set closed
action group.
3. C c closure1 c closure2 . construction works considering three
separate sets intersection closures two original clauses (where
computation interesting various must agree), points
closure c1 closure c2 . analysis latter sets
straightforward; need agree element G1 G2 set
question.
step, compute intersection region C . example, C = {b, e}.
4. g stabi g restricti{C } . find subgroup g restricti set stabilizes
C , case subgroup set stabilizes pair {b, e}. g restrict1 =
h(ad), (be), (bf )i, h(ad), (be)i longer swap b f ,
g restrict2 = h(be), (bg)i, get g stab2 = h(be)i.
5. g int g stab1 |C g stab2 |C . Since must simultaneously agree
G1 G2 restricted C (and thus g restrict1 g restrict2
well), restriction C must lie within intersection. example,
g int = h(be)i.
460

fiZAP 3: Implementation

6. {gi } {generators g int}. element g int lead element
group stable extensions provided extend appropriately C back
G2
1
full set cG
1 c2 ; step begins process building extensions.
suffices work generators g int, construct generators
here. {gi } = {(be)}.
7. {lki } {gi , lifted g stabk }. goal build permutation
c closure1 c closure2 that, restricted C , matches generator gi .
lifting gi separately c closure1 c closure2 . lifting
suffices, take (for example)
l11 = (be)(ad)

l21 = (be)
first case, inclusion swap neither precluded required;
could well used l11 = (be).
0 } {l |
8. {l2i
2i c closure2 C }. cannot simply compose l11 l21 get desired
permutation c closure1 c closure2 part permutations acting
intersection c closure1 c closure2 acted twice. case,
would get l11 l21 = (ad) longer captures freedom exchange b e.

deal restricting l21 away C combining l11 .
example, restricting (be) away C = {b, e} produces trivial permutation
0 = ( ).
l21
0 }i. compute final answer
9. Return hg restrict1C , g restrict2C , {l1i l2i
0
three sources: combined l1i l2i working construct, along
elements g restrict1 fix every point closure c2 elements
g restrict2 fix every point closure c1 . latter two sets obviously
consist stable extensions. element g restrict1 point stabilizes closure
c2 point stabilizes points closure c1 (to
g restrict1 restricted) closure c2 ; words,
point stabilizes C .

example,
g restrict1C

= h(ad)i

g restrict2C

= 1

{l1i

0
l2i
}

= {(be)(ad)}

final group returned
h(ad), (be)(ad)i
group identical obvious
h(ad), (be)i
461

fiDixon, Ginsberg, Hofer, Luks & Parkes

swap either (a, d) pair (b, e) pair, see fit. first swap (ad)
sanctioned first resolvent (c1 , G1 ) = (a b, h(ad), (be), (bf )i)
mention relevant variable second (c2 , G2 ) = (c b, h(be), (bg)i). second
swap (be) sanctioned cases.
Computational issues conclude section discussing computational
issues arise implement Procedure 4.1, including complexity various
operations required.

1. c closurei cG
. Efficient algorithms exist computing closure set
group. basic method use flood-fill like approach, adding marking
result acting set single generator, recurring new points
added.

2. g restricti Gi |c closurei . group restricted set stabilizes
restricting generating permutations individually.
3. C c closure1 c closure2 . Set intersection straightforward.
4. g stabi g restricti{C } . Set stabilizer straightforward, known
polynomial total size generators group considered
(Seress, 2003).7 effective implementations work coset decomposition
described Section 3.2; computing G{S} set S, node pruned
maps point inside vice versa. Gap implements (but
see comments end Section 10.2).
5. g int g stab1 |C g stab2 |C . Group intersection also known polynomial total size generators; again, coset decomposition used.
Coset decompositions constructed groups combined,
search spaces pruned appropriately. Gap implements well.
6. {gi } {generators g int}. Groups typically represented terms
generators, reconstructing list generators trivial. Even generators
known, constructing strong generating set known polynomial
number generators constructed.
7. {lki } {gi , lifted g stabk }. Suppose group G acting set ,
subset V permutation h acting V know h
restriction V g G, h = g|V . find g, first construct
stabilizer chain G using ordering puts elements V first.
basically looking g G sifting procedure Section 3.1
produces h point points V fixed. find
g polynomial time inverting sifting procedure itself.
0 } {l |
8. {l2i
2i c closure2 C }. line 2, restriction still easy.

7. Unlike k-transporter problem, mentioned beginning Section 3.2 NP-hard,
neither set stabilizer group intersection (see step 5) likely NP-hard (Babai & Moran, 1988).

462

fiZAP 3: Implementation

0 }i. Since groups typically rep9. Return hg restrict1C , g restrict2C , {l1i l2i
resented generators, need simply take union generators
three arguments. Point stabilizers (needed first two arguments) straightforward compute using stabilizer chains.

5. Unit Propagation (Ir)relevance Test
remarked, main computational requirement augmented satisfiability engine ability solve k-transporter problem: Given augmented clause
(c, G) c viewed set literals, sets U literals
integer k, want find g G cg = |cg U | k, g exists.
5.1 Warmup
begin somewhat simpler problem, assuming U = simply looking
g cg = .
need following definitions:
Definition 5.1 Let H G groups. transversal H G subset G
contains one element coset H. denote transversal (G : H).
Note since H one cosets, transversal must contain (unique) element
H. generally assume identity unique element.
Definition 5.2 Suppose G acts set c . cG denote
elements c fixed G.
search proceeds, gradually fix points clause question.
notation Definition 5.2 let us refer easily points fixed thus
far.
Procedure 5.3 Given groups H G, element G, sets c S, find group
element g = map(G, H, t, c, S) g H cgt = :
1
2
3
4
5
6
7
8
9
10

ctH 6=
return failure
c = cH
return 1
element c cH
t0 (H : H )
r map(G, H , t0 t, c, S)
r 6= failure
return rt0
return failure

463

fiDixon, Ginsberg, Hofer, Luks & Parkes

essentially codification example presented Section 3.2.
terminate search clause fixed remaining group H,
yet included analog lex-leader pruning discussed Section 3.3.
recursive call line 7, retain original group, use subsequent
versions procedure.
precise description procedure would state explicitly G acts c
S, G Sym() c, . elsewhere, believe conditions
obvious context elected clutter procedural descriptions
them.
Proposition 5.4 map(G, G, 1, c, S) returns element g G cg = ,
element exists, returns failure otherwise.
Proof. proof Appendix shows slightly stronger result map(G, H, t, c, S)
returns element g H cgt = element exists.
Given procedure terminates search elements c stabilized
G include lex-leader considerations, search space examined
example Section 3.2 following, replaced variables a, b, c,
x1 , x2 , x3 , x4 avoid confusion current use c represent clause
question.
Sym(x1 , x2 , x3 , x4 )
P
@PPP
PP
@
PP
@
PP
PP
@
PP
@
PP
@s(x1 x3 )
Ps(x1 x4 )




AAs(x2 x4 )
s(x2 x3 )

still important prune node lower right, since larger problem, node
may expanded significant search subtree. discuss pruning Section 5.5.
interests clarity, let us go example explicitly. Recall clause
c = x1 x2 , G = Sym(x1 , x2 , x3 , x4 ) permutes xi arbitrarily, = {x1 , x2 }.
initial pass procedure, cH = ; suppose select x1 stabilize
first. Line 6 selects point x1 mapped; select x1 x2 ,
x1 mapped recursive call fail line 2. suppose pick
x3 image x1 .
cH = {x1 }, need fix image another point; x2 thats left
original clause c. before, selecting x1 x2 image x2 leads failure. x3
already taken (its image x1 ), map x2 x4 . every element
c fixed, next recursive call returns trivial permutation line 4.
combined (x2 x4 ) line 9 caller fix x4 image x2 . original
invocation combines (x1 x3 ) produce final answer (x2 x4 )(x1 x3 ).
464

fiZAP 3: Implementation

5.2 k-Transporter Problem
Extending algorithm solve k-transporter problem straightforward;
addition requiring ctH = line 2, also need keep track number
points (or be) mapped set U make sure wont
forced exceed limit k.
understand this, suppose examining node coset decomposition
tree labeled permutation t, node corresponds permutations gt
various g subgroup considered level. want ensure
g |cgt U | k. Since cgt assumed avoid set completely,
replace slightly stronger
|cgt (S U )| k
turn equivalent

1

|cg (S U )t | k

(6)

(7)

since set (7) simply result operating set (6) permutation
t1 .
present variety ways bound (7) approximated;
moment, simply introduce auxiliary function overlap(H, c, V ), assume
computes lower bound |ch V | h H. Procedure 5.3 becomes:
Procedure 5.5 Given groups H G, element G, sets c, U integer
k, find group element g = transport(G, H, t, c, S, U, k) g H, cgt =
|cgt U | k:
1
2
3
4
5
6
7
8
9
10
11
12

ctH 6=
return failure
1
overlap(H, c, (S U )t ) > k
return failure
c = cH
return 1
element c cH
t0 (H : H )
r transport(G, H , t0 t, c, S, U, k)
r 6= failure
return rt0
return failure

convenience, denote transport(G, G, 1, c, S, U, k) transport(G, c, S, U, k).
top level function corresponding original invocation Procedure 5.5.
Proposition 5.6 Provided |ch V | overlap(H, c, V ) |cH V | h H,
transport(G, c, S, U, k) computed Procedure 5.5 returns element g G
cg = |cg U | k, element exists, returns failure otherwise.
465

fiDixon, Ginsberg, Hofer, Luks & Parkes

second condition overlap (that overlap(H, c, V ) |cH V |) needed ensure
procedure terminates line 4 overlap limit reached, rather
succeeding line 6.
Procedure 5.5 simplified significantly fact need return single g
desired properties, opposed g. examples arising (ir)relevance
calculations, single answer suffices. want compute unit consequences
given literal, need unit instances clause question.
considerations work case, however, defer discussion topic
Section 6.
initial version overlap is:
Procedure 5.7 Given group H, two sets c, V , compute overlap(H, c, V ), lower
bound overlap ch V h H:
1

return |cH V |

defined overlap, may well use replace test line 1 Proce1
dure 5.5 check see overlap(H, c, ) > 0, indicating h H,
1
|ch | > 0 or, equivalently, cht 6= . simple version overlap defined
above, difference two procedures. overlap matures,
change lead additional pruning cases.
5.3 Orbit Pruning
two general ways nodes pruned k-transporter problem.
Lexicographic pruning bit difficult, defer Section 5.5. understand
other, begin following example.
Consider clause c = x1 x2 x3 group G permutes variables
{x1 , x2 , x3 , x4 , x5 , x6 } arbitrarily. = {x1 , x2 , x3 , x4 }, g G cg = ?
Clearly not; isnt enough room image c size three,
way 3-element set avoid 4-element set 6-element
universe {x1 , x2 , x3 , x4 , x5 , x6 }.
bit better many cases. Suppose group G h(x1 x4 ), (x2 x5 ), (x3 x6 )i
swap x1 x4 (or not), x2 x5 , x3 x6 . = {x1 , x4 },
find g G cg = ?
again, answer clearly no. orbit x1 G {x1 , x4 } since {x1 , x4 }
S, x1 image cannot avoid set S.
general case appearing Procedure 5.5, consider initial call,
identity permutation. Given group G, consider orbits points c.
orbit W |W c| > |W S|, prune search. reason
points W c must remain W acted element G;
definition orbit requires. many points W c stay
away S, manage cg = .
general case, 6= 1 necessarily? fixed clause c,
construct image gt , acting first g t. interested

466

fiZAP 3: Implementation

1

whether gt or, equivalently, g . g necessarily orbit
, prune
1
|W c| > |W |
similar reasons, also prune
1

|W c| > |W U | + k
fact, prune

1

|W c| > |W (S U )t | + k
still enough space fit image without either intersecting
putting least k points U .
better still. seen, particular orbit, number points
eventually mapped U least
1

|W c| |W (S U )t |
cases, expression negative; number points mapped
U therefore least
1

max(|W c| |W (S U )t |, 0)
prune node
X
1
max(|W c| |W (S U )t |, 0) > k

(8)

W

sum orbits group.
somewhat convenient rewrite using fact
1

1

|W c| + |W c| = |W | = |W (S U )t | + |W (S U )t |
(8) becomes
X

1

max(|W (S U )t | |W c|, 0) > k

(9)

W

Incorporating type analysis Procedure 5.7 gives:
Procedure 5.8 Given group H, two sets c, V , compute overlap(H, c, V ), lower
bound overlap ch V h H:
1
2
3
4

m0
orbit W H
+ max(|W V | |W c|, 0)
return

Proposition 5.9 Let H group c, V sets acted H. h H,
|ch V | overlap(H, c, V ) |cH V | overlap computed Procedure 5.8.
467

fiDixon, Ginsberg, Hofer, Luks & Parkes

5.4 Block Pruning
pruning described previous section improved further. see why, consider
following example, might arise solving instance pigeonhole problem.
two cardinality constraints:
x1 + x2 + x3 + x4 2

(10)

x5 + x6 + x7 + x8 2

(11)

presumably saying least two four pigeons hole least two
hole n n.8 Rewriting individual cardinality constraints
augmented clauses produces
(x1 x2 x3 , Sym(x1 , x2 , x3 , x4 ))
(x5 x6 x7 , Sym(x5 , x6 , x7 , x8 ))
or, terms generators,
(x1 x2 x3 , h(x1 x2 ), (x2 x3 x4 )i)

(12)

(x5 x6 x7 , h(x5 x6 ), (x6 x7 x8 )i)

(13)

would really like do, however, capture full symmetry single axiom.
realizing obtain (13) (12) switching x1 x5 ,
x2 x6 , x3 x7 (in case want switch x4 x8 well). add
generator (x1 x5 )(x2 x6 )(x3 x7 )(x4 x8 ) overall group, modify permutations
(x1 x2 ) (x2 x3 x4 ) (which generate Sym(x1 , x2 , x3 , x4 )) permute x5 , x6 , x7 , x8
appropriately well. single augmented clause obtain
(x1 x2 x3 , h(x1 x2 )(x5 x6 ), (x2 x3 x4 )(x6 x7 x8 ), (x1 x5 )(x2 x6 )(x3 x7 )(x4 x8 )i)

(14)

hard see indeed capture (12) (13).
suppose x1 x5 false, variables unvalued. (14)
unit instance?
regard pruning condition previous section, group single
orbit, condition (with = 1)
|W (S U )| |W c| > 1

(15)


W

= {x1 , x2 , x3 , x4 , x5 , x6 , x7 , x8 }

=
U

= {x2 , x3 , x4 , x6 , x7 , x8 }

c = {x1 , x2 , x3 }
8. actual pigeonhole instance, variables would negated. dropped negations
convenience.

468

fiZAP 3: Implementation

|W (S U )| = 6, |W c| = 5 (15) fails.
possible conclude immediately unit instances
(14). all, unit instances (10) (11) one variable
clause set, three unvalued variables remain. Equivalently,
unit instance (12) one {x1 , x2 , x3 , x4 } valued, two need
valued make x1 x2 x3 another instance unit. Similarly, unit instance
(13). went wrong?
went wrong pruning heuristic thinks x1 x5
mapped clause instance, case indeed possible instance
question unit. heuristic doesnt realize x1 x5 separate blocks
action group question.
formalize this, let us first make following definition:
Definition 5.10 Suppose G acts set . say G acts transitively
orbit G.
Put somewhat differently, G acts transitively case x,
g G xg = y.
Definition 5.11 Suppose group G acts transitively set . block system
G partitioning sets B1 , . . . , Bn G permutes Bi .
words, g G block Bi , Big = Bj j. j = i,
image Bi g Bi itself. j 6= i, image Bi g disjoint
Bi , since blocks partition .
Every group acting transitively nontrivially set least two block systems:
Definition 5.12 group G acting transitively set , block system B1 , . . . , Bn
called trivial either n = 1 n = |T |.
former case, single block consisting entire set (which obviously
block system). n = |T |, point block; since G permutes points,
obviously permutes blocks.
Lemma 5.13 blocks block system identical size.
example considering, B1 = {x1 , x2 , x3 , x4 } B2 = {x5 , x6 , x7 , x8 }
also block system action group set = {x1 , x2 , x3 , x4 , x5 , x6 , x7 , x8 }.
conceivable clause image unit within overall set , impossible
fewer two unvalued literals within particular block. Instead
looking overall expression
|W (S U )| |W c| > 1
work individual blocks.

469

(16)

fiDixon, Ginsberg, Hofer, Luks & Parkes

clause x1 x2 x3 single block block system, therefore remain
single block acted g G. clause winds block Bi ,
condition (16) replaced
|Bi (S U )| |Bi c| > 1
or, case,
|Bi (S U )| > |Bi c| + 1 = 2
prune two unvalued literals block question.
all, three unvalued literals, must least two
clause instance considered, cannot unit.
course, dont know exactly block eventually contain image c,
still prune
min(|Bi (S U )|) > 2
since case target block generate prune. example
considering,
|Bi (S U )| = 3
block block system.
Generalizing idea straightforward. notational convenience, introduce:
Definition 5.14 Let = {T1 , . . . , Tk } sets,
Ti1 , . . . , Tin n
Pnsuppose min
elements smallest size. denote j=1 |Tij | Ti .
Proposition 5.15 Let G group acting transitively set , let c, V .
Suppose also {B1 , . . . , Bk } block system G c Bi 6= n
blocks {B1 , . . . , Bk }. b size individual block Bi g G,
|cg V | |c| + min
(Bi V ) nb

(17)

Proposition 5.16 block system trivial (in either sense), (17) equivalent
|cg V | |T V | |T c|

(18)

Proposition 5.17 Let {B1 , . . . , Bk } block system group G acting transitively
set . (17) never weaker (18).
event, shown strengthen Procedure 5.8 to:
Procedure 5.18 Given group H, two sets c, V , compute overlap(H, c, V ),
lower bound overlap ch V h H:

470

fiZAP 3: Implementation

1
2
3
4
5
6

m0
orbit W H
{B1 , . . . , Bk } block system W H
n = |{i|Bi c 6= }|
+ max(|c W | + min
(Bi V ) n|B1 |, 0)
return

block system use line 3 procedure? seems
general best answer question, although seen Proposition 5.17
block system better one trivial ones. practice, best choice appears
minimal block system (i.e., one blocks smallest size) c contained
within single block. Procedure 5.18 becomes:
Procedure 5.19 Given group H, two sets c, V , compute overlap(H, c, V ),
lower bound overlap ch V h H:
1
2
3
4
5

m0
orbit W H
{B1 , . . . , Bk } minimal block system W H
c W Bi
+ max(|c W | + min(Bi V ) |B1 |, 0)
return

Proposition 5.20 Let H group c, V sets acted H. h H,
|ch V | overlap(H, c, V ) |cH V | overlap computed Procedure 5.19.
Note block system used depends group H original
clause c. means implementation possible compute block
systems use even changes sets U satisfied
unvalued literals respectively.
Gap includes algorithms finding minimal block systems given set
elements (called seed gap) contained within single block. basic idea
form initial block system points seed one block point
outside seed block own. algorithm repeatedly runs
generators group, seeing generator g maps elements x, one block
xg g different blocks. happens, blocks containing xg g
merged. continues every generator respects candidate block system,
point procedure complete.9
5.5 Lexicographic Pruning
Block pruning help us example end Section 5.1. final space
searched is:
9. faster implementation makes use procedure designed testing equivalence finite automata (Aho, Hopcroft, & Ullman, 1974, chapter 4) takes O(snA(n)) time, size
generating set A(n) inverse Ackerman function.

471

fiDixon, Ginsberg, Hofer, Luks & Parkes

sPSym(a, b, c, d)
@PPP
PP
@
PP
@
PP
PP
@
PP
(ac)
@
PP
@s
Ps(ad)




AAs(bd)
s(bc)

remarked, first leaf node (where mapped c b d) essentially
identical second (where mapped b c). important expand
since complicated examples may involve substantial amount search
nodes leaf nodes figure.
sort situation lexicographic pruning generally applied.
want identify two leaf nodes equivalent way, expand
lexicographically least member equivalence class. particular node n,
need computationally effective way determining n lexicographically least
member equivalence class.
begin identifying conditions two nodes equivalent. understand
this, recall interested image clause c particular group
element g. means dont care particular literal l mapped,
care image entire clause c. also dont care
image literal isnt c.
formal point view, begin extending set stabilizer notation somewhat:
Definition 5.21 permutation group G sets S1 , . . . , Sk acted G, G{S1 ,...,Sk }
mean subgroup G simultaneously set stabilizes Si ; equivalently, G{S1 ,...,Sk } = G{Si } .
computing multiset stabilizer G{S1 ,...,Sk } = G{Si } , need compute individual set stabilizers take intersection. Instead, recall set stabilizers
computed using coset decomposition; stabilized point moved either
set question, given node pruned set stabilizer computation. straightforward modify set stabilizer algorithm stabilized
point moved Si , node question pruned. allows
G{S1 ,...,Sk } computed single traversal Gs decomposition tree.
suppose j permutation G stabilizes set c. cg satisfies
conditions transporter problem, cjg . all, acting j first doesnt
affect set corresponding c, image clause jg therefore identical
image g. means two permutations g h equivalent h = jg
j G{c} , set stabilizer c G. Alternatively, permutation g equivalent
element coset Jg, J = G{c} .
hand, suppose k permutation simultaneously stabilizes
sets U satisfied unvalued literals respectively. possible show
472

fiZAP 3: Implementation

operate k operating successfully g, also dont impact question
whether cg solution transporter problem. upshot
following:
Definition 5.22 Let G group J G K G, let g G. double
coset JgK set elements G form jgk j J k K.
Proposition 5.23 Let G group permutations, c set acted G. Suppose
also U sets acted G. instance k-transporter
problem g G, either every element G{c} gG{S,U } solution I, none is.
understand important, imagine prune overall search tree
permutations g remaining ones minimal double cosets
JgK, J = G{c} K = G{S,U } above. impact solubility
instance k-transporter problem?
not. particular instance solutions, pruning tree obviously
introduce any. particular instance solution g, every element JgK
also solution, specifically minimal element JgK solution, minimal
element pruned assumptions.
see, then, prune node n show every permutation g underneath n minimal double coset JgK. state precise conditions
lets us prune node n, suppose coset decomposition
group G, xj point fixed depth j tree. n node
depth tree, know n corresponds coset Ht G, H stabilizes
xj j i. denote image xj zj . g Ht
minimal double coset JgK J = G{c} K = G{S,U } Proposition 5.23,
node n corresponding Ht pruned.
Jx

,...,xk1

Lemma 5.24 (Leon, 1991) xl xk 1
g Ht first element JgK.

Kz1 ,z2 ,...,zk1

k l zk > min(zl

Jx

),

,...,x

Lemma 5.25 (reported Seress, 2003) Let length orbit xl 1 l1 .
zl among last 1 elements orbit Gz1 ,z2 ,...,zl1 , g Ht first
element JgK.
results give conditions node coset decomposition
pruned searching solution instance k-transporter problem. Let
us consider example each.
begin Lemma 5.24. return example end Section 5.1,
G = Sym(a, b, c, d), c = {a, b} = S, U = . Thus J = K = G{a,b} =
Sym(a, b) Sym(c, d) = h(ab), (cd)i.
Consider node repeatedly remarked pruned depth 1,
fix image d. case, x1 = z1 = d. take k = l
Jx ,...,x
statement lemma, xl xl 1 l1 since 1 Jx1 ,...,xl1 . Thus prune
Kz1 ,z2 ,...,zl1

zl > min(zl

473

)

fiDixon, Ginsberg, Hofer, Luks & Parkes

restricting l = 1 gives us
z1 > min(z1K )

(19)

example, z1 = d, z1K = {c, d} (19) holds (assuming > c ordering).
node pruned, finally get reduced search space:
sPSym(a, b, c, d)
@PPP
PP
@
PP
@
PP
PP
@
PP
@
PP
@s(ac)
Ps(ad)




AAs(bd)

desired.
node pruned Lemma 5.25 well. conditions lemma require
take length orbit J (since l = 1 here), = |{a, b}| = 2.
Thus image cannot among last 2 1 = 1 points orbit G. Since
orbit G {a, b, c, d}, prune node. (The previous
node, maps c, cannot pruned, course.)
particular example simple. nodes examined depth one,
significant overlap groups question. node pruned
either lemma here, lemmas prune different nodes complex cases. Note also
groups J = G{c} K = G{S,U } computed root tree,
group J independent sets U therefore cached augmented
clause (c, G).
Lemmas 5.24 5.25 well known results computational group theory
community. also use following:
Lemma 5.26 Suppose permutation labeling node Ht coset decomgroup
position tree depth k, xti = zi k H = Gx1 ,...,xk residual


JM,x1 ,...,xi1

level. Let set points moved Gx1 ,...,xk . zi > min xi
k, g Ht first element JgK.
example, consider cardinality constraint
x1 + + xm n
corresponding augmented clause (c, G)
c = x1 xmn+1
G = Sym(X), X set xi .
474

fiZAP 3: Implementation

Suppose fix images xi order, considering node
image x1 fixed z1 image x2 fixed z2 , z2 < z1 .
J = G{c} = Sym(x1 , . . . , xmn+1 ) Sym(xmn+2 , . . . , xm ), taking = 1 k = 2
Lemma 5.26 gives us Jxk+1 ,...,xm = Sym(x1 , x2 ) since need fix xj x2 .
Jx

,...,xm

= {z1 , z2 }, since z1 smallest element set,
x1 k+1
enough prune node. See proof Proposition 6.9 another example.
refer Lemmas 5.245.26 pruning lemmas.
Adding lexicographic pruning k-transporter procedure gives us:
Procedure 5.27 Given groups H G, element G, sets c, U integer
k, find group element g = transport(G, H, t, c, S, U, k) g H, cgt =
|cgt U | k:
1
2
3
4
5
6
7
8
9
10
11
12
13
14

1

overlap(H, c, ) > 0
return failure
1
overlap(H, c, (S U )t ) > k
return failure
c = cH
return 1
pruning lemma applied
return failure
element c cH
t0 (H : H )
r transport(G, H , t0 t, c, S, U, k)
r 6= failure
return rt0
return failure

Note test line 7 requires access groups J K, therefore
original group G procedure called. retain copy
group recursive call line 11.
might seem brought much mathematical power bear
k-transporter problem specifically, disagree; recall Figure 1, repeated zap1.
High-performance satisfiability engines, running difficult problems, spend excess
90% CPU time unit propagation, seen instance
k-transporter problem. Effort spent improving efficiency Procedure 5.27 (and
predecessors) expected lead substantial performance improvements
practical application. See also Figure 8 experimental results Section 9.2.
do, however, note lexicographic pruning important, also expensive.
defer line 7 Procedure 5.27. earlier lexicographic prune would
independent U sets, count-based pruning much faster
defer lexicographic check extent possible.

475

fiDixon, Ginsberg, Hofer, Luks & Parkes

100

ZCHAFF data

% time spent

95

90

85

80

75

70

0

10

20

30

40

50

60

70

80

90

total CPU time (sec)

Figure 1: Fraction CPU time spent unit propagation

6. Unit Propagation
Procedure 5.27 designed around need find single permutation g G satisfying
conditions k-transporter problem, technically suffices zaps needs.
unit propagation, however, useful collect unit consequences
augmented clause (c, G) once, opposed collecting via repeated traversals
Gs coset decomposition tree.
work consequences observation, help example
illustrates points going making. end, consider
augmented clause
(a b e, Sym(a, b, c, d) Sym(e, f ))
(20)
situation a, b c false d, e f unvalued. group (20)
allows arbitrary permutations {a, b, c, d} {e, f }, e f unit
consequences instances given augmented clause.
Note cannot simply collect group elements associated unit
instance, since many group elements may correspond clause instance cg
unit literal cg U . example, ( ) (ab) correspond
identical clause b e, clause c e lead conclusion e
given current partial assignment.
goal therefore compute set permutations, associated set
unit conclusions:
Definition 6.1 Let (c, G) augmented clause, P partial assignment. unit
consequences (c, G) given P set literals l g G
cg S(P ) = cg U (P ) = {l}. fixed literal w, unit w-consequences (c, G)
given P set literals l g G w cg , cg S(P ) =
cg U (P ) = {l}.
476

fiZAP 3: Implementation

unit w-consequences involve additional requirement literal w appear
clause instance question. useful discuss watched literals next
section.
example, unit consequences (20) e f . unit c-consequences
same, although longer use identity permutation ( ), since needed c
base instance (20). unit d-consequences (20).
partial assignment annotated, need unit consequences,
reasons well:
Definition 6.2 Let X set pairs hl, gi, g G l literal pair.
X = {hl1 , g1 i, . . . , hln , gn i}, denote {l1 , . . . , ln } L(X).
(c, G) augmented clause P partial assignment, X called annotated set unit consequences (c, G) given P if:
1. cg S(P ) = cg U (P ) = {l} every hl, gi X
2. L(X) set unit consequences (c, G) given P .
returning example, he, ( )i annotated consequence, he, (abc)i.
hf, (ef )i hf, (abc)(ef )i. set {he, (abc)i, hf, (ef )i} annotated set unit
consequences, {he, (abc)i, hf, (ef )i, hf, (abc)(ef )i}. {hf, (ef )i, hf, (abc)(ef )i}
annotated set unit consequences, since e appear consequence.
modify k-transporter procedure search entire tree
accumulating annotated set unit consequences. need careful, however,
pruning lemmas may prune node includes permutation g
minimal double coset JgK. problem g minimal element
JgK may correspond distinct unit consequences. running example, may well
none minimal elements JgK supports f conclusion; accumulate
minimal elements, get full set unit consequences result.
Given successful g minimal double coset, reconstructing relevant
orbits J K easy, begin introducing definitions cater this.
basic idea want minimal g entail, sense, conclusions
drawn permutations double coset JgK.
example, subgroup G simultaneously stabilizes U G{S,U } =
Sym(a, b, c) Sym(e, f ). permutation g1 allows us conclude e,
operate g1 (ef ) g1 G{S,U } conclude f well. formalize follows:
Definition 6.3 Given group G, say hl1 , g1 G-entails hl2 , g2 i, denoted
hl1 , g1 |=G hl2 , g2 i, g G l2 = l1g g2 = g1 g. say
set pairs X G-entails set pairs , writing X |=G , every pair G-entailed
pair X.
skeletal set unit consequences (c, G) given P set X unit consequences
G{S(P ),U (P )} -entails annotated set unit consequences (c, G) given P .
running example, l1 = e g = (ef ) first paragraph, allowing
(for example) he, ( )i G{S,U } -entail hf, (ef )i. Thus see {he, ( )i} skeletal set
unit consequences (20) given partial assignment {a, b, c}.
477

fiDixon, Ginsberg, Hofer, Luks & Parkes

Lemma 6.4 X |=G , L(Y ) L(X)G .
Proof. Every pair form hl1g , g1 gi hl1 , g1 X g G. Thus
associated literal L(X)G .
construct full set unit consequences skeletal set, repeatedly find new
unit conclusions possible:
Procedure 6.5 Given set X pairs hl, gi group G, compute complete(X, G),
X |=G complete(X, G) L(complete(X, G)) = L(X)G :
1
2
3
4
5
6


hl, gi X
l0 lG L(Y )
select h G lh = l0
hl0 , ghi
return

Proposition 6.6 X |=G complete(X, G) L(complete(X, G)) = L(X)G .
apply pruning lemmas search proceeds, eventually returning
skeletal set unit consequences clause question. addition, unit
instance fact unsatisfiable, return failure marker sort.
handle returning two values. first indicates whether contradiction
found, second skeletal set unit consequences.
Procedure 6.7 Given groups H G, element G, sets c, U , find
Transport(G, H, t, c, S, U ), skeletal set unit consequences (c, G) given P :
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

1

overlap(H, c, ) > 0
return hfalse,
1
overlap(H, c, (S U )t ) > 1
return hfalse,
c = cH
ct U =
return htrue, 1i
else return hfalse, hct U, 1ii
pruning lemma applied
return hfalse,

element c cH
t0 (H : H )
hu, V Transport(G, H , t0 t, c, S, U )
u = true
return htrue, V t0
else {hl, gt0 i|hl, gi V }
return hfalse,
478

fiZAP 3: Implementation

Proposition 6.8 Assume |ch V | overlap(H, c, V ) |cH V | h H,
let Transport(G, c, S, U ) computed Procedure 6.7. g G
cg = cg U = , Transport(G, c, S, U ) = htrue, gi g.
g, Transport(G, c, S, U ) = hfalse, Zi, Z skeletal set unit consequences
(c, G) given P .
application pruning lemmas, have:
Proposition 6.9 Let (c, G) augmented clause corresponding cardinality constraint. sets U , Procedure 6.7 expand linear number
nodes finding skeletal set unit consequences (c, G).
original formulation cardinality constraints (as zap1), determining
particular constraint unit (and finding implied literals so) takes time linear
length constraint, since involves simple walk along constraint itself.
therefore seems appropriate linear number nodes expanded case.

7. Watched Literals
one pruning technique yet considered, possibility
finding analog setting Zhang Stickels (2000) watched literal idea.
understand basic idea, suppose checking see clause b c
unit situation b unvalued. follows clause cannot unit,
independent value assigned c.
point, watch literals b; long remain unvalued,
clause cannot unit. practice, data structures representing b include pointer
clause question, unit test needs performed clauses pointed
literals changing value.
continue discuss ideas, useful distinguish among three different
types clauses: satisfied given current partial assignment,
unit, neither:
Definition 7.1 Let C clause, P (possibly annotated) partial assignment.
say C settled P either satisfied unit; otherwise unsettled.
have:
Definition 7.2 Let C clause, P (possibly annotated) partial assignment. C
unsettled P , watching set C P set literals W
|W C U (P )| > 1.
words, W contains least two unvalued literals C C unsettled
current partial assignment.
C satisfied unit? watching set case?
sense, doesnt matter. Assuming notice clause changes
unsettled unit (so either unit propagate detect potential contradiction),
479

fiDixon, Ginsberg, Hofer, Luks & Parkes

settled clauses uninteresting perspective, since never generate
second unit propagation. watch settled clause not, see fit.
another sense, however, matter. One properties would like
watching sets remain valid backtrack. means
settled clause C becomes unsettled backtrack, must two watched
unvalued variables backtrack.
order discuss backtracking formal way, introduce:
Definition 7.3 Let P partial assignment set (possibly augmented) clauses.
say P -closed clause C unit consequence given P .
-closure P minimal, sound -closed extension P , denoted
either PT simply P clear context.
definition closure makes sense intersection two closed partial
assignments closed well. compute closure, simply add unit consequences
one time available. Note still ambiguity;
one unit consequence added point, add unit
consequences order.
Definition 7.4 Let P = hl1 , . . . , ln partial assignment. subassignment P
initial subsequence hl1 , . . . , lj j n. say subassignment P 0 P
backtrack point P either P 0 = P P 0 = P 0 . denote P largest
backtrack point P P itself.
C clause, say P -retraction C, denoted PC , largest
backtrack point P C unsettled.
Note require backtrack point C unsettled, opposed simply
unsatisfied. P closed, difference Definition 7.4 permit
backtrack point C unit. C unit P , retract C
reverting point C became unit. Otherwise, C simply reasserted
unit propagation computes P .
Since P backtrack point P , immediately have:
Lemma 7.5 C unsettled P , PC = P .
example, suppose following annotated partial assignment P :
literal

b
c

e

reason
true
true
b c
true
b e

clause C b e f , P -retraction C ha, b, ci. Removing e sufficient
make C unsettled, ha, b, c, di closed therefore legal backtrack point.
b e theory, retraction fact hai ha, b, ci backtrack
point unit conclusion e drawn.
generalize Definition 7.2 include settled clauses:
480

fiZAP 3: Implementation

Definition 7.6 Let C clause, P annotated partial assignment. watching set
C P set literals W |W C U (PC )| > 1.
words, W contain least two unvalued literals C replace P
P -retraction C. discussed earlier, first point could backtrack
C longer satisfied unit. Continuing earlier example, {e, f } watching
set b e f , {b, e} watching set b e. watching set b e {b, e};
recall definition forces us backtrack way hai.
Lemma 7.7 W watching set C P , superset W .
order watching sets useful, course, must maintain search
proceeds. Ideally, maintenance would involve modifying watching sets infrequently possible, could adjust required variables take new
values, backtracking all. Recall example beginning
section, b unvalued constitute watching set clause b c.
b becomes satisfied, need nothing since clause satisfied {a, b}
still watching set. Note (for example) becomes satisfied, cant remove b
watching set, since would need replace backtrack point
unvalued again. Leaving b watching set required satisfy Definition 7.6
needed ensure sets need adjusted backtrack.
hand, (for example) becomes unsatisfied, need check clause
see whether become unit. clause unit, b set true
unit propagation, maintenance required. clause unsettled, c must
unvalued, replace c set literals watching clause. Finally,
clause already satisfied, unvalued P -retraction clause
watching set need modified.
general, have:
Proposition 7.8 Suppose W watching set C P l literal. Then:
1. W watching set C backtrack point P .
2. C settled hP, li, W watching set C hP, li.
3. C settled hP, li, |(W {l}) C U (PC )| > 1, W {l}
watching set C hP, li.
4. l 6 W C, W watching set C hP, li.
proposition tells us modify watching sets search proceeds.
modification required backtrack (claim 1). modification required
clause satisfied unit (claim 2), also remove newly valued literal
watching set enough unvalued variables present (claim 3). modification
required unless add negation already watched literal (claim 4).
sum, modification watching sets required add negation
watched literal partial assignment watched clause settled; case,
481

fiDixon, Ginsberg, Hofer, Luks & Parkes

add one remaining unvalued literals watching set. addition,
remove literals watching set enough unvalued literals already it. Since
last possibility used zChaff ground systems, example
it.
Suppose are, usual, watching b b c. point, becomes
true. either leave watching set alone virtue condition 4, extend
watching set include c (extending watching set always admissible, virtue
Lemma 7.7), remove watching set. change unneeded ground
prover, useful augmented version 7.10 proposition below.
lift ideas augmented setting, begin modifying Definition 7.6
obvious way get:
Definition 7.9 Let (c, G) augmented clause, P annotated partial assignment.
watching set (c, G) P set literals W watching set every
instance cg (c, G) P .
leads following augmented analog Proposition 7.8. (Although
four clauses Proposition 7.8 four following proposition, clause-forclause correspondence two results.)
Proposition 7.10 Suppose W watching set (c, G) P l literal.
Then:
1. W watching set (c, G) backtrack point P .
2. l 6 W cG , W watching set (c, G) hP, li.
3. |(W V ) cg U (hP, li)| > 1 every g G cg unsettled hP, li,
W V watching set (c, G) hP, li.
4. |(W V ) cg [U (hP, li) (S(P ) S(P ))]| > 1 every g G, W V {l}
watching set (c, G) hP, li.
example, suppose return augmented clause considered
previous section, (abe, Sym(a, b, c, d)Sym(e, f )). Suppose initially watching
a, b, c d, e false, imagine becomes false well.
need augment W |W cg U (P )| > 1 every unsettled instance cg
(c, G) contains a. instances b f , c f f . Since b, c
already W , need add f . f watching set b, c
d, would add three points instead.
case, since clause unit instance (a b e, example), cannot
remove watching set. reason later backtrack past
point, danger watching b unsatisfied clause.
Suppose, however, e unvalued became false. would
add e f watching set would free remove a. sanctioned
Proposition 7.10, since (c, G) settled instances cg S(P ) = g G
well, conditions claims three four equivalent.
482

fiZAP 3: Implementation

e, instead false unvalued, true? add f
watching set, remove new watching set {a, b, c, d, f }? cannot:
instance b e would one watched literal did.
cases, however, remove literal became false
watching set. surely every clause instance still two unvalued literals
watching set. would correspond requirement
|(W V ) cg U (hP, li)| > 1
every instance. stronger condition claim four proposition allows us
slightly better cases satisfied literal clause became satisfied sufficiently
recently know backtrack unvalue it.
fourth conclusion Proposition 7.10 essential effective functioning
overall prover; replace watched literal l become false new
unvalued literal, important stop watching original watched literal l.
last claim proposition allows us (although all)
practical cases. Without fourth conclusion, watching sets would get larger
search proceeded. Eventually, every literal every clause would watched
computational power idea would lost.
use watching sets reduce number clauses must examined line 1 unit propagation procedure 2.7. augmented clause needs
associated watching set initialized updated sanctioned Proposition 7.10.
Initialization straightforward; clause (c, G) c length least two,
need define associated watching set W property |W cg | > 1 every
g G. fact, take W simply cG , union instances cg ,
rely subsequent unit tests gradually reduce size W . (Once again, using
fourth clause Proposition 7.10.) challenge modify Procedure 6.7 way
facilitates maintenance watching sets.
this, let us understand bit detail watching sets used
searching unit instances particular augmented clause. Consider augmented
clause corresponding quantified clause
xy . [q(x) r(y) s]
Q set instances q(x) R set instances r(y), becomes
augmented clause
(q(0) r(0) s, Sym(Q) Sym(R))
(21)
q(0) r(0) elements Q R respectively.
suppose r(y) true y, q(x) unvalued, s, clause
(21) unit instances. Suppose also search unit instances (21) first
stabilizing image r q (s stabilized group Sym(Q) Sym(R)
itself). four possible bindings (which denote 0, 1, 2, 3) three
x (0, 1, 2), search space looks like this:

483

fiDixon, Ginsberg, Hofer, Luks & Parkes

Sym(R)
sPSym(Q)
PP

@

PP

@
PP


PP
@

PP


@
PP


PP (r0 r3 )
Sym(Q)
(r0 r1 )
@
P
@s(r0 r2 )
Ps

































(q0 q2 )
(q0 q2 )
(q0 q2 )
1
1
1
1
AAs(q0 q2 )














(q0 q1 )

(q0 q1 )

(q0 q1 )

(q0 q1 )

interests conserving space, written qi instead q(i) similarly rj .
leaf nodes fails relevant instance q(x) unvalued,
construct new watching set entire clause (21) watches
q(x).
Note causes us lose significant amounts information regarding portions
search space need reexamined. example, responsible literals
leaf node follows:

P
PP

@

PP

@
PP


PP

@

PP

@
PP


PP

@

P
@s
Ps
































q0 ,
q2 ,
q0 ,
q2 ,
q0 ,
q2 ,
q0 ,



AAsq2 ,











q1 ,

q1 ,

q1 ,

q1 ,

simply accumulate literals root search tree, conclude
reason failure watching set {q0 , q1 , q2 , s}. watched literals
changes value, potentially reexamine entire search tree.
address changing order variable stabilization, replacing search
space depicted following one:
Sym(Q) Sym(R)
P
PP


PP

PP


PP


PP

P




Sym(R)


E

EA
E
E
1
EEs AAs(r0 r3 )
(r0 r1 ) (r0 r2 )

(q0 q1 )


AE
EA
E
E
1s EEs AAs(r0 r3 )
(r0 r1 ) (r0 r2 )

484

PP
P

P
Ps(q0 q2 )
EA
EA
E
E
1s
EEs AAs(r0 r3 )
(r0 r1 ) (r0 r2 )

fiZAP 3: Implementation

center node needs reexpansion value q1 changes, since
node q1 appears. search space becomes simply:
Sym(Q) Sym(R)

(q0 q1 )



E

EA
E
E
1
EEs AAs(r0 r3 )
(r0 r1 ) (r0 r2 )

one would expect q1 changes value.
upshot collect new watching set original augmented
clause corresponding (21), also need modify unit propagation procedure
first stabilize points mapped specific watched literal become
unsatisfied.
see keep watching set updated, consider Proposition 7.10. searching
unit instances augmented clause (c, G), need compute set W
|W cg U (P )| > 1 every unsettled instance cg (c, G) contains fixed literal
w. this?
solution lies Procedure 6.7, describes search unit instances.
remaining clause instances particular search node determined
nonunit test line 3, instead simply recognizing every instance
node nonunit, need able identify set unvalued literals meets every
unsettled instance cg least twice. modify overlap procedure 5.19 become:
Procedure 7.11 Given group H, two sets c, V acted H, bound k 0,
compute overlap(H, c, V, k), collection elements V sufficient guarantee
h H, |ch V | > k, collection exists:
1 m0
2 W
3 orbit X H
4
{B1 , . . . , Bk } minimal block system W H
c W Bi
5
= |c X| + min(Bi V ) |B1 |
6
> 0
7
+
8
W W (X V )
9
> k
10
return W
11 return

485

fiDixon, Ginsberg, Hofer, Luks & Parkes

Proposition 7.12 Procedure 7.11 returns nonempty set W Procedure 5.19
returns value excess k. case, |ch W | > k every h H.
finally position replace Procedure 6.7 version uses watched
literals:
Procedure 7.13 Given groups H G, element G, sets c, U , optionally watched element w, find Transport(G, H, t, c, S, U, w), skeletal set unit
w-consequences (c, G) given P :
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

1

w supplied wt 6 cH
return hfalse, ,
1
V overlap(H, c, , 0)
V 6=
return hfalse, ,
1
V overlap(H, c, (S U )t , 1)
V 6=
return hfalse, , V
c = cH
ct U =
return htrue, 1,
else return hfalse, hct U, 1i,
pruning lemma applied
return hfalse, ,
1
element c cH . w supplied w 6 ctH , choose wt H .

W
t0 (H : H )
hu, V, Xi Transport(G, H , t0 t, c, S, U, w)
u = true
return htrue, V t0 ,
else W W X
{hl, gt0 i|hl, gi V }
return hfalse, Y, W

application pruning lemmas line 13, need use restricted group
G{S,U,{w}} , prune group element g w cg basis another
group element jgk w 6 cjgk , since jgk might pruned line 2.
Proposition 7.14 Suppose overlap(H, c, V, k) computed using Procedure 7.11,
otherwise satisfies conclusion Proposition 7.12. g G
w cg cg = cg U = , Transport(G, c, S, U, w) computed Procedure 7.13
returns htrue, g, g. g, Procedure 7.13 returns hfalse, Z, W i,
Z skeletal set unit w-consequences (c, G) given P , W
|W G{S,U,{w}} ch U | > 1 every h H w ch ch unsettled P .
486

fiZAP 3: Implementation

Note pruning lemmas applied relatively late procedure (line 13) even
though successful application prunes space without increasing size watching
set. might seem pruning lemmas applied earlier.
appears case. discussed end Section 5, pruning lemmas
relatively complex check; moving test earlier (to precede line 6, presumably)
actually slows unit propagation procedure factor approximately two, primarily
due need compute set stabilizer G{S,U } even cases simple counting
argument suffices. addition, absolute impact watching sets expected
quite small.
understand why, suppose executing procedure instance
eventually fail. n node pruned either counting argument
(with new contribution Wn set watched literals) lexicographic argument
using another node n0 , since node n0 eventually fail, contribute
watching set Wn0 eventually returned value. possible Wn 6= Wn0
(different elements selected overlap function line 6, example),
expect vast majority cases Wn = Wn0 non-lexicographic
prune impact eventual watching set computed.
Proposition 7.14 implies watching set returned Procedure 7.13 used
update watching set third claim Proposition 7.10. fourth claim,
hope remove l new watching set, need check see
|W cg [U (hP, li) (S(P ) S(P ))]| > 1
g G, W new watching set. determined single call
transport; g G
|cg [W (U (hP, li) (S(P ) S(P )))]| 1

(22)

remove l W . cases, save call transport exploiting
fact (as shown proof Proposition 7.10) (22) cannot satisfied hP, li
unit consequence.
finally position describe watched literals augmented setting.
start, have:
Definition 7.15 watched augmented clause pair h(c, G), W (c, G) augmented clause W watching set (c, G).
Procedure 7.16 (Unit propagation) compute Unit-Propagate(C, P, L) C
set watched augmented clauses, P annotated partial assignment, L set
pairs hl, ri literals l reasons r:

487

fiDixon, Ginsberg, Hofer, Luks & Parkes

1 L 6=
2
hl, ri element L
3
L L hl, ri
4
P hP, hl, rii
5
h(c, G), W C
6
l W
7
hr, H, V Transport(G, c, S(P ), U (P ), l)
8
r = true
9
li literal cH highest index P
10
return htrue, resolve((cH , G), ci )i
11
H 0 complete(H, G{S(P ),U (P ),{l}} )
12
h H 0
13
z literal ch unassigned P
14
hz, r0 L
15
L L hz, ch
16
W W (U (P ) V G{S(P ),U (P ),{l}} )
17
U U (P ) (S(P ) S(P ))
18
H = transport(G, c, , W U, 1, l) = failure
19
W W {l}
20 return hfalse, P
line 18, invoke version transport function accepts additional argument literal required included clause instance sought.
modification similar introduction literal w Transport procedure 7.13.
Proposition 7.17 Let P annotated partial assignment, C set watched augmented clauses, every h(c, G), W C, W watching set (c, G) P .
Let L set unit consequences clauses C. Unit-Propagate(C, P, L) returns
htrue, ci augmented clause c, c nogood P , modified watching
sets C still watching sets P . Otherwise, value returned hfalse, P
watching sets C replaced watching sets P .
Procedure 7.16 modified incorporated fairly obvious way Procedure 2.8, literal recently added partial assignment added L
thereby passed unit propagation procedure.

8. Resolution Revisited
one additional theoretical point need discuss turning attention
experimental matters.
goal augmented resolution produce many (if all) resolvents
sanctioned instances augmented clauses resolved. showed zap2,
however, always possible produce resolvents. another example
phenomenon.
488

fiZAP 3: Implementation

Suppose resolving two clauses
(a c, (ab))

(23)

(b c, (ab))

(24)

(a b, (ab))

(25)


result is10
consider example. instances (23) ac bc; (24) bc
c. Surely better resolvent (a, (ab)) instead (25). general,
never want conclude (c, G) possible conclude (c0 , G) c0 c
set inclusion proper. resolvent c0 properly stronger c.
additional consideration well. Suppose resolving two augmented clauses, choose instances resolving clauses resolvent
(a c, G) (b c, G), b literals two possible resolvents distinct
(ab) 6 G. select?
know general answer, reasonable heuristic make choice based
order literals added current partial assignment. Assuming
resolvent nogood, presumably b false current partial assignment
P . select resolvent allows larger backjump; case, resolvent
involving literal added P first.
considerations direct analog conventional Boolean satisfiability
engine. particular literal l, resolvent reasons l l that;
flexibility possible.11
Definition 8.1 Let (, G) (, H) two augmented clauses resolving literal l,
l l . l-resolvent (, G) (, H) clause obtained
resolving g h g G h H l g l h .
Note group Z resolvent clause (resolve(g , h ), Z) independent
resolvent selected, focus attention strictly syntactic properties
resolvent.
next formalize fact partial assignment P induces natural lexicographic
ordering set nogoods given theory:
Definition 8.2 Let P partial assignment, c ground clause. l literal
c whose negation maximal index P , say falsification depth c
position P literal l. falsification depth zero literal c;
event, falsification depth c denoted c?P .
c1 c2 two nogoods, say c1 falsified earlier c2 P , writing
?P
?P
?P
c1 <P c2 , either c?P
1 < c2 , c1 = c2 c1 lc?P <P c2 lc?P .
1

2

10. result obtained direct computation applying resolution stability property discussed zap2, since groups identical.
11. weak analog present zChaff, replace one nogood n another n0 n0 leads
greater backjump n does. functionality part zChaff code appear
documented.

489

fiDixon, Ginsberg, Hofer, Luks & Parkes

example, suppose P ha, b, c, d, ei. falsification depth c
three, since c third variable assigned P . falsification depth b four.
Thus c <P b d; would rather learn c allows us backjump
c instead d. Similarly c e <P b e; common element
e eliminated, would still rather backtrack c d. general, goal
resolving two augmented clauses select resolvent minimal <P . Note
have:
Lemma 8.3 c1 c2 two nogoods P , c1 <P c2 .
Procedure 8.4 Suppose given two augmented clauses (, G) (, H)
unit partial assignment P = hl1 , . . . , ln i, l l . find <P -minimal
l-resolvent (, G) (, H):
1
2
3
4
5
6
7
8
9
10
11
12
13

U {l, l}
literals cant avoid
f
f
p [( ) U ]?P
p > 0
g transport(G, , {lp , . . . , ln } U, , 0, l)
h transport(H, , {lp , . . . , ln } U, , 0, l)
g = failure h = failure
U U {lp }
else f g
f h
p [(f f ) U ]?P
return resolve(f , f )

basic idea gradually force two clause instances away end
partial assignment; back up, keep track literals unavoidable
associated call transport failed. unavoidable literals accumulated set
U above, continue call transporter function, objection one
clause instances includes elements U . point, refocus attention
deepest variable yet known either avoidable unavoidable;
reach root partial assignment, return instances found.
example. Suppose P = ha, b, c, d, ei before, (, G)
instances c l e l. second clause (, H) single instance
b e l.
resolve <P -minimal instances two augmented clauses, resolve
c l b e l get b c e. better resolve e l
b e l instead get b e. literals b e appear case,
better c d.
Suppose follow example procedure, U initially set
{l, l} (say) therefore f set c l. f set b e l,
since instance (, H). initial value p five, since last literal
U = {b, c, d, e} e.
490

fiZAP 3: Implementation

try find way avoid e appear final resolvent.
looking instance (, G) includes l (the literal resolving)
avoids e (and subsequent literal, arent any). instance given
itself. instance (, H) avoids e, call line 7 fails.
therefore add e U leave clauses f f unchanged. decrement p four,
since e longer (f f ) U .
next pass loop, looking clause instances avoid
{d, e} U = {d}. know well forced include e final result,
dont worry it. hope point exclude d.
Here, successful finding instances. existing instance suffices,
instance e l (, G). becomes new f p gets reduced
two, since (f f ) U = {a, b}.
next pass loop tries avoid b continuing avoid c
(which know avoid current f f so). turns
impossible, b added U p decremented one. Avoiding impossible
well, p decremented zero procedure correctly returns b e.
Proposition 8.5 Suppose given two augmented clauses (, G) (, H)
unit partial assignment P , l l . value
returned Procedure 8.4 <P -minimal l-resolvent (, G) (, H).
procedure implemented somewhat efficiently described above;
f , example, already satisfies condition implicit line 6, need reinvoke
transport function g.
important relatively slender improvement, however, fact
resolution involves repeated calls transport function. general, Boolean
satisfiability engines need worry time used resolution function, since
unit propagation dominates running time. naive implementation Procedure 8.4,
however, involves calls transport unit propagation procedure,
resolution comes dominate zaps overall runtime.
correct this, remember point Procedure 8.4. procedure needed
correctness; needed find improved resolution instances. amount time
spent looking instances less computational savings achieved
them. Put slightly differently, requirement produce resolvent
absolutely minimal <P ordering. resolvent nearly minimal
suffice, especially producing truly minimal instance involves large computational cost.
achieve goal working modified transport function lines 6 7
Procedure 8.4. Instead expanding coset decomposition tree completely, limited
number nodes examined. Zaps current implementation prunes transporter search
100 nodes examined; solving pigeonhole problem, example,
turns sufficient ensure resulting proof length would
strictly <P -minimal resolvents found. also modify pruning
computation, pruning K = GSU instead difficult compute G{S,U } .
Since GSU G{S,U } (stabilizing every element set surely stabilizes set itself),
approximation saves time reduces amount possible pruning. appropriate

491

fiDixon, Ginsberg, Hofer, Luks & Parkes

10
CPU time
1.65e-06 n**4.6

1

secs

0.1

0.01

0.001

1e-04
5

10

15

20

pigeons

Figure 2: CPU time resolution pigeonhole problem
given artificially reduced size overall search tree need produce
answer quickly.

9. Experimental Results: Components
finally position describe experimental performance algorithms
presented. remarked introduction, begin describing performance
zaps algorithmic components, resolution unit propagation algorithms. Performance results complete inference tool build using ideas next section.
experiments performed 2GHz Pentium-M 1GB main memory.
9.1 Resolution
implemented resolution procedure described Section 4, results
pigeonhole problem shown Figure 2. particular example involves resolving
two basic axioms pigeonhole problem containing n pigeons n 1 holes:
(p11 p1,n1 , G)
(p11 p12 , G)

492

fiZAP 3: Implementation

first axiom says pigeon 1 must hole; second, first two
pigeons cannot first hole. group G corresponds global symmetry
group pigeons holes interchanged freely.
resolvent two axioms fact computed without grouptheoretic computation all, using result zap2 group stable extensions
(c1 , G) (c2 , G) always superset group G. algorithm Section 4
computing augmented resolvents include check see groups identical,
implementation include check. test disabled produce
data Figure 2.
plot observed time (in seconds) resolution function number
pigeons involved, time plotted log scale. Memory usage typically approximately 5MB; CPU usage dominated need compute stabilizer chains
groups question. algorithms used take time O(d5 )
size domain group operating (Furst, Hopcroft, & Luks, 1980; Knuth,
1991). case, symmetries pigeons holes stabilized independently therefore expect stabilizer chain computation take time O(n5 ),
n number pigeons. fit data curve axb , best fit occurring
b 4.6. consistent stabilizer chain computation dominating runtime.
reinsert check see groups same, running times reduced
uniformly approximately 35%. Testing group equality involves checking see
generator G1 member G2 vice versa, therefore still involves computing
stabilizer chains groups question. again, need compute stabilizer
chains dominates computation.
9.2 Unit Propagation
Figure 3 give data showing average time needed unit test pigeonhole
problem. naturally occurring unit tests arise run prover
problem question. memory used program remained far less
1GB available; example, maximum usage approximately 20MB 13 pigeons.12
Since unit test NP-complete, customary give mean median
running times; present means Figure 3 mean running times appear
growing polynomially (compare two lines best fit figure),
medians appear modestly smaller means. shown Figure 4,
appears ratio mean median running times growing linearly
problem size.
earlier figure 3 also shows average CPU time failed tests (where clause
question unit instances) successful tests (where unit instances exist);
seen, failed unit tests generally complete far quickly successful
counterparts similar size various pruning heuristics come play. cases,
however, scaling continues appear polynomial problem size.
12. Accurately measuring peak memory usage difficult group operations regularly allocate
free relatively large blocks memory. measured usage simply starting system monitor
observing it, practical problem instances took extended amounts time
complete. reason report memory usage approximately, one problem
instance.

493

fiDixon, Ginsberg, Hofer, Luks & Parkes

10
average
fail
succeed
polynomial fit
exponential fit
1

secs

0.1

0.01

0.001

1e-04
4

6

8

10

12
pigeons

14

16

18

20

Figure 3: CPU time unit test pigeonhole problem

10. Experimental Results: ZAP
conclude discussion zaps experimental performance results problem
instances entirety, opposed performance individual algorithmic components. presenting results, however, let us describe domains
considered expectations regard performance zap existing
systems areas.
examining performance three domains:
1. pigeonhole problem, goal show cannot put n + 1 pigeons n
holes pigeon get hole.
P
P
2. parity problem, goal show iI xi + iJ xi cannot odd
sets J equal (Tseitin, 1970).
3. clique-coloring problem, goal show map containing m-clique
cannot colored n colors n < m.
reasons chosen particular problem classes follows:
1.
easy.
P
P obvious cant put n + 1 pigeons n holes,
iI xi + iJ xi even xi appears exactly twice. also obvious
cant color graph containing m-clique user fewer colors.
494

fiZAP 3: Implementation

2.1
ratio
2

1.9

mean/median ratio

1.8

1.7

1.6

1.5

1.4

1.3

1.2
4

6

8

10

12
pigeons

14

16

18

20

Figure 4: Mean vs. median CPU time unit test pigeonhole problem
last case especially, note solving easy problem.
case trying color specific graph containing m-clique; goal
show graph containing m-clique anywhere cannot colored.
different graph coloring generally.
Put somewhat differently, problems examining P.
Given suitable representations, easy.
2. problems known exponentially difficult resolution-based methods. shown pigeonhole problems Haken (1985) parity problems
Tseitin (1970). Clique-coloring problems known exponentially difficult
resolution, linear programming methods well (Pudlak, 1997).
fact, know implemented system scales polynomially class
problem.
3. Finally, problems involve structure captured group-based
setting.
data present compares zaps performance zChaff; Section 10.4 discusses performance Boolean tools problem classes
discussing. chose zChaff comparison partly
discussed throughout series papers, partly appears best
495

fiDixon, Ginsberg, Hofer, Luks & Parkes

1e+06
zap
zchaff
10000

secs

100

1

0.01

1e-04

1e-06
4

6

8

10

12
pigeons

14

16

18

20

Figure 5: CPU time pigeonhole instances, zap zChaff
overall performance three problem classes considering. (Once again,
see Section 10.4 additional details.)
ZAP expectations proceeding, let us point theoretical basis,
known short group-based proofs exist problems. showed zap2
group-based pigeonhole proofs expected short, also parity problems
short group-based proofs mimic Gaussian elimination. also showed short
group-based proofs existed clique coloring, although proof fairly intricate.
goal determine whether implementation ideas discover short
proofs practice, whether control group-based inference require additional
theoretical ideas yet understand.
Please understand goal point test zap standard NP-complete
search problems Boolean form, graph coloring quasigroup completion problems (Gomes & Selman, 1997). involves significant effort ensuring zaps
constant factors data structures comparable systems; preliminary indications possible modest impact performance
(approximately factor two), work yet complete reported elsewhere.

496

fiZAP 3: Implementation

100000
zap
exponential fit
polynomial fit
10000

1000

secs

100

10

1

0.1

0.01

0.001
4

6

8

10

12
pigeons

14

16

18

20

Figure 6: zap scaling pigeonhole instances
10.1 Pigeonhole Results
Figure 5 shows running times zap zChaff pigeonhole instances. Figure 6 repeats zap data, also including best exponential polynomial fits time
spent. overall running time appears polynomial, varying approximately n8.1
n number pigeons. rough terms, factor O(n5 ) needed
stabilizer chain constructions. branch positive literals, know (see zap2)
O(n) resolutions needed solve problem, resolution
lead O(n2 ) unit propagations. total time thus expected approximately
O(n8 ), assuming unit propagation involves stabilizer chain computations
actual search. observed performance close theoretical value.
practice, zap branches positive literals, negative ones. reason
negative literals appear far clauses positive ones (O(n) clauses
negative literal opposed single clause positive literal), usual
branching heuristic Boolean satisfiability community initially assigns variable
value satisfies many clauses possible.
number nodes expanded zap solving particular instance pigeonhole problem shown Figure 7, also presents similar data zChaff.
number nodes expanded zap fact exactly n2 3n + 1; curiously, also
depth zChaff search next smaller instance n 1 pigeons.
497

fiDixon, Ginsberg, Hofer, Luks & Parkes

1e+07
zap
zchaff
1e+06

100000

nodes

10000

1000

100

10

1
4

6

8

10

12
pigeons

14

16

18

20

Figure 7: Nodes expanded pigeonhole problem
know small size pigeonhole proofs found zap result effectiveness
use <P -optimal resolvents, fundamental argument made
zap proofs pigeonhole problem short.
moving parity problems, allow us comment importance
various algorithmic techniques described. recognize many
algorithms presented quite involved, important demonstrate
associated algorithmic complexity leads legitimate computational gains.
Figure 8 shows time needed solve pigeonhole instances either abandon
pruning lemmas avoid search <P -optimal resolvents. clear
data, techniques essential obtaining overall performance exhibited
system.
abandon search <P -optimal resolvents, proof lengths increase significantly appear remain polynomial n. length increase learned axioms
leads increased running times unit propagation, appears primary
reason performance degradation figure. overall running times scale exponentially.
Abandoning pruning lemmas also leads exponential running times.
expected level; still exponentially many learned ground axioms
cannot prune search unit instances, exponential behavior expected.

498

fiZAP 3: Implementation

1e+06
100000
10000
1000

secs

100
10
1
0.1
0.01
0.001

zap
pruning
resolution instances

1e-04
4

6

8

10

12
pigeons

14

16

18

20

Figure 8: Improvement due pruning lemmas <P -optimal resolution instances.
circles mark zaps performance. xs indicate performance pruning
lemmas disabled unit propagation, boxes give performance resolutions use original base instances clauses resolved, opposed
searching resolving <P -optimal instances.

ways could reduced zaps algorithmic complexity
well. could, example, removed watched literals computational machinery needed maintain them. turns out, change virtually impact
zaps pigeonhole performance provers behavior typically backtrack-free
(Dixon et al., 2004a). general, however, watched literals expected play
important role zap dpll-style prover. overall focus
series papers show group-based augmentations could implemented
without sacrificing ability use recent techniques made Boolean
satisfiability engines effective practice, watched literals certainly numbered
amongst techniques.
also evaluate possibility learning augmented clauses all, perhaps
learning instead ground versions. would avoid need implement Procedure 4.1, would also avoid computational gains zap theoretically
access. learning augmented clauses theoretical reductions proof

499

fiDixon, Ginsberg, Hofer, Luks & Parkes

size obtained; otherwise, proof would necessarily unchanged
dpll-style approach.
10.2 Tseitin Results
next problem class present experimental data one due Tseitin (1970)
shown Urquhart (1987) require resolution proofs exponential length.
problem based graph G. associate Boolean variable edge G,
every vertex v G associated charge 0 1 equal sum mod 2
variables adjacent v. charge entire graph G sum mod 2 charges
vertices. require connected graph G charge one, set
constraints associated vertices unsatisfiable (Tseitin, 1970). graph
problem size four, together associated constraints:
1

0



u
@
b@

u

e
@

c



@
@
@
u

0

f

@
@u

0

a+b+c1
d+e+a0
f +b+d0
c+e+f 0
language zap (see Appendix B),


f
c

b
e
b
e

c


f

%2=
%2=
%2=
%2=

1
0
0
0

;
;
;
;

axiom set unsatisfiable adding axioms gives us
2a + 2b + 2c + 2d + 2e + 2f 1
problems known exponentially difficult resolution-based methods (Urquhart,
1987).
Times solution zap zChaff shown Figure 9. ZChaff clearly scaling
exponentially; best fit zap times 0.00043n1.60 log(n) , n problem
size.

500

fiZAP 3: Implementation

1e+06
zap
zchaff
10000

secs

100

1

0.01

1e-04

1e-06
5

10

15

20

size

Figure 9: CPU time Tseitin instances, zap zChaff. ZChaff scaling exponentially; zap scaling O(n1.6 log(n) ).

Figure 10 shows number nodes expanded two systems. number
search nodes expanded zap appears growing polynomially size
problem (O(n2.6 ), give take), keeping result zap2 showing zap proofs
polynomial length always exist parity problems. pigeonhole instances,
see short proofs exist theory, apparently practice well.
Given polynomial number nodes expanded super-polynomial amount
time consumed, seems likely unit propagation procedure culprit,
taking super-polynomial amount time per unit propagation. shown Figure 11,
fact case. unit test easy all, groups
simply flip even number variables question. want know
augmented clause unit instance, find unvalued variables contains.
one, clause unit. exactly one, clause always unit variable must
valued make parity sum take desired value. seems
reason unit tests scaling nlog(n) .
nlog(n) scaling appears consequence multiset stabilizer computation underlies k-transporter pruning. Here, too, scaling polynomial,
since show polytime (O(n3 )) methods exist set stabilizer groups

501

fiDixon, Ginsberg, Hofer, Luks & Parkes

1e+07
zap
zchaff
1e+06

100000

nodes

10000

1000

100

10

1
5

10

15

20

size

Figure 10: Nodes expanded Tseitin problems. ZChaff scaling exponentially; zap
scaling polynomially O(n2.6 ).

question.13 general methods implemented gap zap exploit
Abelian nature parity groups, however, scaling shown. obvious extension existing implementation would include efficient set stabilizer algorithms
groups.
10.3 Clique Coloring
final problem class present experimental data clique coloring.
class problems related pigeonhole problem far difficult.
mentioned previously, domain graph coloring, two nodes connected edge must assigned different colors. graph clique size m,
obvious graph cannot colored 1 colors. equivalent
instance pigeonhole problem. clique coloring problem, told
graph clique size m, contains clique size m. fact
know exact location clique widens search considerably.
13. argument made either fact groups Abelian, fact
group orbits size two, set stabilizer problem thus converted one linear
algebra Z2 .

502

fiZAP 3: Implementation

1
average
3.21e-05 x**log x

secs

0.1

0.01

0.001

1e-04
5

10

15

20

size

Figure 11: CPU time unit test Tseitin problems. Zap scaling approximately
O(nlog(n) ).

know (other) automated proof system scales polynomially problems
class; resolution linear programming methods inevitably scale exponentially (Pudlak, 1997). showed zap2 zap could produce polynomial-length proofs
theory, suggestions made proofs would easy find practice.
present details zaps performance problem class, let us reiterate
observation clique-coloring problems thought unsatisfiable
instances graph-coloring problems generally. particular instance problem class
describe specific graph needs colored; says graph
contains m-clique needs colored 1 colors.
axiomatization problem follows. use eij describe graph, cij
describe coloring graph, qij describe embedding clique
graph. graph nodes, clique size n + 1, n colors available.
ci1 cin

= 1, . . . ,

(26)

qi1 qim

= 1, . . . , n + 1

(27)

1 < j m, l = 1, . . . , n

(28)

1 < k n + 1, j = 1, . . . ,

(29)

eij cil cjl
qij qkj

503

fiDixon, Ginsberg, Hofer, Luks & Parkes

eij qki qlj

1 < j m, 1 k 6= l n + 1

(30)

eij means edge graph nodes j, cij means graph
node colored jth color, qij means ith element clique
mapped graph node j. Thus first axiom (26) says every graph node color.
(27) says every element clique appears graph. (28) says two
nodes graph cannot color (of n colors available) connected
edge. (29) says two elements clique map node graph.
Finally, (30) says clique indeed clique two clique elements map
disconnected nodes graph.
encoding passed zap group-based, follows:
SORT color 2 ;
SORT node 4 ;
SORT clique 3 ;
PREDICATE edge( node node ) ;
PREDICATE color( node color ) ;
PREDICATE clique( clique node ) ;
GROUP COLOR <
(( color[1 1] color[1 2])
( color[2 1] color[2 2])
( color[3 1] color[3 2])
( color[4 1] color[4 2]))
> ;
GROUP CLIQUE <
(( clique[1 1] clique[2 1])
( clique[1 2] clique[2 2])
( clique[1 3] clique[2 3])
( clique[1 4] clique[2 4]))
(( clique[2 1] clique[3 1])
( clique[2 2] clique[3 2])
( clique[2 3] clique[3 3])
( clique[2 4] clique[3 4]))
> ;
GROUP NODES <
(( edge[1 3] edge[2 3])
( edge[1 4] edge[2 4])
( color[1 1] color[2 1])
( color[1 2] color[2 2])
( clique[1 1] clique[1 2])
( clique[2 1] clique[2 2])
( clique[3 1] clique[3 2]))
(( color[2 1] color[3 1] color[4 1])
( color[2 2] color[3 2] color[4 2])
( edge[1 2] edge[1 3] edge[1 4])
( edge[2 3] edge[3 4] edge[2 4])
( clique[1 2] clique[1 3] clique[1 4])
( clique[2 2] clique[2 3] clique[2 4])
( clique[3 2] clique[3 3] clique[3 4]))
504

fiZAP 3: Implementation

1e+06
zap
zchaff
10000

secs

100

1

0.01

1e-04

1e-06
4

6

8
10
12
graph size (clique size one less)

14

16

18

Figure 12: CPU time clique instances, zap zChaff
> ;
color[1 1] color[1 2] GROUP NODES ;
clique[1 1] clique[1 2] clique[1 3] GROUP CLIQUE ;
-edge[1 2] -color[1 1] -color[2 1] GROUP NODES COLOR ;
-clique[1 1] -clique[2 1] GROUP NODES CLIQUE ;
-clique[1 1] -clique[2 2] edge[1 2] GROUP NODES CLIQUE ;

version 3-clique graph size four, trying
use two colors. first group symmetry colors alone, second
elements clique, third symmetry nodes. axiomatization
identical presented earlier. Note although common symmetry
problem, axiomatization obscures sense, since included
relevant symmetry symmetries particular axiom.
Times solution zap zChaff shown Figure 12. might expected,
zChaff scaling exponentially; zap appears scaling n8.5 . order allow
data presented along single axis, problem instances selected
clique size one smaller graph size.
Figure 13 shows number nodes expanded two systems. again,
number nodes expanded zChaff growing exponentially problem size,
number expanded zap growing polynomially. pigeonhole problem,
505

fiDixon, Ginsberg, Hofer, Luks & Parkes

1e+07
zap
zchaff
1e+06

100000

nodes

10000

1000

100

10

1
4

6

8
10
12
graph size (clique size one less)

14

16

18

Figure 13: Nodes expanded clique problems
see short proofs whose existence guaranteed theory found
practice.
Figures 14 15 display zaps performance somewhat wider range problem
instances clique graph sizes allowed vary independently. number
nodes expanded general
(c + g)2 13c g + 14
2
c size clique g size graph. handful outliers,
notably c = 11, g = 13 instance expanded larger number nodes.
exceptions expanded fewer nodes.
regard total CPU time (Figure 15), time appears scaling (cg)3.89 .
again, c = 11, g = 13 outlier polynomial performance observed generally.
best knowledge, zap first system exhibit polynomial performance
problem class; remarked, approaches proven scale
exponentially.
10.4 Related Work
Finally, compare experimental results obtained using systems
attempt exploit problem structure improve performance satisfiability solvers.
506

fiZAP 3: Implementation

zap
0.075 (x+y)**2.52

1000

100
nodes

10
12
graph size
4

6

8

10

4
12

clique size

Figure 14: Nodes expanded clique problems
section provides high-level summary experimental results number
efforts compares results zap benchmark problems described
previous sections.
Recall benchmark problems highly structured, different type structure. Theoretically, problems allow polynomial-time solutions,
provably hard conventional solvers. solver solves problems efficiently ability exploit range different types problem structure
automates strong proof system. course, interesting, solver must also
practical general purpose solver. example, Tseitin problems solved polynomial
time form Gaussian elimination (Schaefer, 1978), pigeonhole problems
solved polynomial time linear programming method simplex method.
However, neither solutions constitutes practical general purpose solver.
ran number solvers benchmark problems, obtaining following results:

507

fiDixon, Ginsberg, Hofer, Luks & Parkes

zap
3.54e-06 (xy)**3.89

10000
1000
100
10
1
secs

0.1
0.01

12
graph size
4

6

4
8

10

12

clique size

Figure 15: CPU time expended clique problems

zap
zChaff
pbchaff
eqsatz
march eq
resolution
cutting-planes
integer programming

pigeonhole
P
E
P
E
E
E

Tseitin
nlog n
E
E
E
E (P)
E (?)

clique coloring
P
E
E
E
E
E

P

?

E

Rather presenting numerous graphs, summarize results above, simply reporting overall scaling solver problem class. Polynomial-time scaling
indicated P exponential-time scaling E. Scaling shown three
problem classes discussed, two separate encodings considered Tseitin
problems. first encoding Booleanization encoding Section 10.2;
second involves introduction new variables reduce clause length described
below. performance improved introduction, new scaling given parenthetically. final two rows give known proof complexity results resolution
cutting-planes proof systems thus provide lower bounds corresponding rows
them.
508

fiZAP 3: Implementation

Reducing performance results exponential polynomial scaling omits valuable information. Clearly difference n100 n2 scaling something care about,
although polynomial. details specific scaling factors included
discussion follows; goal table merely summarize strength
solvers underlying proof system.
Details solvers appearing table follows:
pbchaff pseudo-Boolean version dpll algorithm. represents problems
pseudo-Boolean form automates cutting-planes proof system. cuttingplanes proof system allows polynomial-length proofs pigeonhole problem
pbchaff able solve problems efficiently. Scaling pbchaff pigeonhole
instances n4.8 , n number pigeons. improvement
n8.1 scaling seen zap. However, performance pbchaff Tseitin
clique coloring problems exponential, since cutting-planes inference able
capture exploit structure problems.
eqsatz (Li, 2000) march eq (Heule & van Maaren, 2004) dpll-based solvers
modified incorporate equivalence reasoning, enable
solve parity problems efficiently. expected, eqsatz march eq
exhibited exponential scaling pigeonhole clique coloring problems, since
solvers designed recognize structure problems. surprising
exponential scaling observed eqsatz march eq initial
encoding Tseitin problems.
Eqsatz scales exponentially recognize structure present
encoding parity problems.14 performance improved modifying cnf encoding reduce size make structure apparent
solver. involves introduction significant number new auxiliary variables, experimental results new encoding discussed below.
March eq recognize structure original encoding, solves
preprocessing phase. exponential scaling due simply fact
size Boolean encoding growing exponentially function graph size (see
Section 10.2).
parity constraint rewritten set parity constraints, length
three (Li, 2000). parity constraint form
x1 + x2 + . . . + xn k
equivalent set parity constraints
x1 + A1 k
A1 + x2 + A2 0
A2 + x3 + A3 0
..
.
An2 + xn1 + An1 0
An1 + xn 0
14. Li, personal communication (2005).

509

(31)

fiDixon, Ginsberg, Hofer, Luks & Parkes

Summing set parity constraints gives
2A1 + 2A2 + + 2An1 + x1 + + xn k
equivalent (31). Tseitin encoding Section 10.2 translated
parity constraints way converted cnf, exponential blowup
size existing cnf encoding avoided. (It clear, however, resolution
produce polynomially sized proof unsatisfiability resulting theory.)
Eqsatz, march eq zap exhibit improved performance new encoding
used; results shown parenthetically Tseitin column table. March eq
solves encoding Tseitin problems virtually instantaneously. Eqsatz substantially outperforms zChaff, reported Li (2003). running times eqsatz,
however, remain exponential system unable solve instance size ten
within 10,000 seconds. performance zap improved well, overall scaling
unchanged.
introduction new variables accepted practice reducing size cnf
encodings, also potential reduce length proofs constructed solvers.
Indeed, classes problems known hard extended resolution, version
resolution introduction new variables permitted. general, however,
introducing new variables order reduce proof length considered cheating
proof complexity perspective; new variables introduced, proof systems
essentially equivalent. addition, general method introducing variables known
know implemented system so. One advantage zap group-based
annotations avoid need syntactic reworkings sort.
Another approach solving highly symmetric problems seen solver sSatz (Li,
Jurkowiak, & Purdom, Jr., 2002). solver also based dpll algorithm,
accepts input problem cnf set matrices describing global symmetry
variables. global symmetry used partition set variable assignments
equivalence classes. addition normal pruning techniques used dpll, search
also pruned eliminating partial assignment minimal
equivalence corresponding global symmetry. sSatz scales polynomially pigeonhole
problems; however, class input symmetry groups allowed sSatz currently
limited applied Tseitin clique coloring problems. clear whether
limitation overcome work matures, included
sSatz table.
solvers tested, zap solver provide efficient solutions test
problems, solver scales polynomially clique coloring. Pbchaff
better scaling pigeonhole problems, march eq better scaling Tseitin
problems; however, solvers exploit narrowly defined type problem structure
therefore perform poorly domains. performance zap also likely
improve basic group primitives underlying zaps procedures optimized.

11. Conclusion Future Work
Zap represents appears new synthesis two distant fields: computational group theory Boolean satisfiability. algorithmic point view,
510

fiZAP 3: Implementation

fields fairly mature complex, synthesis inherits significant algorithmic
complexity result. goal paper present initial versions
algorithms group-based theorem prover need, describe performance
prototype implementation ideas. seen, zap easily outperforms
conventional counterparts difficult problem instances group structure
concealed Boolean axiomatization.
said, important realize results scratch surface
zaps underlying representational shift allows. Tseitin problems, example,
seems likely incorporation sophisticated set stabilizer algorithms allow us
improve zaps performance; fact polynomially many nodes expanded
solving problems bodes well eventual performance system.
improvements also possible. pigeonhole clique coloring problems,
computational performance dominated O(n5 ) stabilizer chain computations
groups question; groups products full symmetry groups. well known
full symmetry groups extremely difficult usual stabilizer chain algorithms,
cases possible produce stabilizer chains directly, taking time
O(n3 ) even O(n2 ) stabilizer chain data structure modified (Jerrum, 1986).
modifications expected improve zaps performance significantly domain.
simply much do. extensions beginning; also
obviously need experiment zap wide range problem instances.
also two general points would like make regarding future work area.
First, left unmentioned problem discovering group structure existing
clausal databases. practical impact would substantial, several reasons.
would make possible apply zap directly problems already encoded
using Boolean axioms, would also make possible discover emergent group
structure appears search begun. example, perhaps symmetry
exists particular problem hidden existing axiomatization;
inferences, symmetry may become apparent still needs noticed.
Second, perhaps important, zap provides us broad stage
work. Progress computational group theory expected lead performance
improvements inference; dually, applying zap wide range reasoning problems
provide new set examples computational group theorists use
test ideas. Lifting heuristics one area AI group-based setting may make
analogs heuristics available other, practical domains. new
syntheses, seems reasonable hope zap allow ideas Boolean satisfiability,
computational group theory search-based AI combined, leading new insights
levels performance areas.

Acknowledgments
would like thank members cirl technical staff Time Systems
assistance ideas series papers. would also like thank
implementers maintainers gap; many elements zap implementation based
directly either implementations appear gap descriptions Seress
book (2003). Finally, would especially like thank anonymous reviewers

511

fiDixon, Ginsberg, Hofer, Luks & Parkes

zap papers care effort put reviewing series papers span
200 journal pages entirety. three papers substantially improved
efforts.
work sponsored part grants Air Force Office Scientific Research (afosr) number F49620-92-J-0384, Air Force Research Laboratory (afrl) number F30602-97-0294, Small Business Technology Transfer Research, Advanced Technology
Institute (sttr-ati) number 20000766, Office Naval Research (onr) number N0001400-C-0233, Defense Advanced Research Projects Agency (darpa) Air Force Research Laboratory, Rome, NY, agreements numbered F30602-95-1-0023, F30602-971-0294, F30602-98-2-0181, F30602-00-2-0534, F33615-02-C-4032, darpa
agreement number HR0011-05-C-0039. views expressed authors.

Appendix A. Proofs
Procedure 4.1 Given augmented clauses (c1 , G1 ) (c2 , G2 ), compute stab(ci , Gi ):
1
2
3
4
5
6
7
8
9

G2
1
c closure1 cG
1 , c closure2 c2
g restrict1 G1 |c closure1 , g restrict2 G2 |c closure2
C c closure1 c closure2
g stab1 g restrict1{C } , g stab2 g restrict2{C }
g int g stab1 |C g stab2 |C
{gi } {generators g int}
{l1i } {gi , lifted g stab1 }, {l2i } {gi , lifted g stab2 }
0 } {l |
{l2i
2i c closure2 C }
0 }i
return hg restrict1C , g restrict2C , {l1i l2i

Proposition 4.2 result returned Procedure 4.1 stab(ci , Gi ).
Proof. show every element group returned stable extension showing
generators line 9 stable extensions; recall set stable extensions
subgroup. show every stable extension returned showing
constructed via procedure.
first claim, argued main text elements g restrictiC
0 } well. element
stable; must show elements {l1i l2i
, however, note |cG1 = l1i |cG1 = gi similarly |cG2 , since agrees
1

1

2

0 stable.
l1i = l2i = gi C l2i outside C . Thus l1i l2i
second claim, suppose stable extension ; consider restriction
G2
G1
G2
1
cG
1 c2 . intersection c1 c2 , must agree elements G1
G2 ; call elements agrees l1 l2 . Restricting l2 away
intersection get l20 , see element l group generated
0 } matches cG1 cG2 .
{l1i l2i
1
2
G2
G1
G2
1
consider l1 . identity cG
1 c2 . Restricting either c1 c2
G2
1
get element G1 G2 point stabilizes cG
1 c2 , elements
included directly line 9 resolution procedure. follows l1 element
hg restrict1C , g restrict2C i,
0
hg restrict1C , g restrict2C , {l1i l2i
}i

512

fiZAP 3: Implementation

Procedure 5.3 Given groups H G, element G, sets c S, find group
element g = map(G, H, t, c, S) g H cgt = :
1
2
3
4
5
6
7
8
9
10

ctH 6=
return failure
c = cH
return 1
element c cH
t0 (H : H )
r map(G, H , t0 t, c, S)
r 6= failure
return rt0
return failure

Proposition 5.4 map(G, G, 1, c, S) returns element g G cg = ,
element exists, returns failure otherwise.
Proof. remarked main text, prove slightly stronger result
map(G, H, t, c, S) returns element g H cgt = element exists.
proposition stated special case = 1.
proof proceeds induction number elements c moved
H. none are, either ct 6= procedure return failure line 2,
ct = return 1 line 4.
inductive step, assume H moves least one point c. Lines 14 dont
affect correctness procedure point, allow early termination
already fixed point moved inside t. interesting case, form
transversal line 6. Every element H represented gt0 g H
t0 transversal. gt0 returned solution, know
inductive hypothesis g found recursive call line 7.
Procedure 5.5 Given groups H G, element G, sets c, U integer
k, find group element g = transport(G, H, t, c, S, U, k) g H, cgt =
|cgt U | k:
1
2
3
4
5
6
7
8
9
10
11
12

ctH 6=
return failure
1
overlap(H, c, (S U )t ) > k
return failure
c = cH
return 1
element c cH
t0 (H : H )
r transport(G, H , t0 t, c, S, U, k)
r 6= failure
return rt0
return failure

513

fiDixon, Ginsberg, Hofer, Luks & Parkes

Proposition 5.6 Provided |ch V | overlap(H, c, V ) |cH V | h H,
transport(G, c, S, U, k) computed Procedure 5.5 returns element g G
cg = |cg U | k, element exists, returns failure otherwise.
1
Proof. remarked main text, |c (S U )t | = |ct (S U )|. since ct
required empty, |ct (S U )| = |ct U |. proof proceeds essentially unchanged
Proposition 5.4.
two conditions overlap function necessary. need know
h
|c V | overlap(H, c, V ) order avoid terminating search early line 3.
need overlap(H, c, V ) |cH V | ensure fixed every element c,
line 3 identify failure |ct U | > k dont return successfully line 6
case.
Procedure 5.8 Given group H, two sets c, V , compute overlap(H, c, V ), lower
bound overlap ch V h H:
1
2
3
4

m0
orbit W H
+ max(|W V | |W c|, 0)
return

Proposition 5.9 Let H group c, V sets acted H. h H,
|ch V | overlap(H, c, V ) |cH V | overlap computed Procedure 5.8.
Proof. subtlety involves contribution fixed points clause make
sum. since fixed point orbit, fixed points contribute either
1 0 sum depending whether already V .
Proposition 5.15 Let G group acting transitively set , let c, V .
Suppose also {B1 , . . . , Bk } block system G c Bi 6= n
blocks {B1 , . . . , Bk }. b size individual block Bi g G,
|cg V | |c| + min
(Bi V ) nb

(32)

Proof. g G, set n blocks collectively contain image cg .
therefore use usual counting argument. Within n blocks, c contain
|c| points, set V contain least min
(Bi V ) points. nb
points available, result follows.
Proposition 5.16 block system trivial (in either sense), (32) equivalent
|cg V | |T V | |T c|

(33)

Proof. Suppose first single block. n = 1, b = |T | one
set take minimum (32), therefore becomes
|cg V | |c| + |T V | |T |
= |T V | |T c|

514

fiZAP 3: Implementation

If, hand, block system trivial point block,
n = |c|, b = 1
min
(Bi V )
smallest number points V must set size n,
min
(Bi V ) = n + |T V | |T |
(32) becomes
|cg V | |c| + |c| + |T V | |T | |c|
= |c| + |T V | |T |
= |T V | |T c|
Proposition 5.17 Let {B1 , . . . , Bk } block system group G acting transitively
set . (32) never weaker (33).
Proof. Comparing (32) (33), see trying show
|c| + min
(Bi V ) nb |T V | |T c|
= |c| + |T V | |T |

min
(Bi V ) nb |T V | |T |
q blocks block system, equivalent
min
min
(Bi V ) nb iq (Bi V ) bq


min
bq nb min
iq (Bi V ) (Bi V )

(34)

lefthand side (34) total amount space q b blocks included
min
(Bi V ), righthand side amount space used V within q b
blocks. Thus (34) follows result proved.
Lemma A.1 Let G group permutations, c set acted G. Suppose also
U sets acted G. j G{c} g G permutation G,

|cg S| = |cjg S|

|cg U | = |cjg U |

Proof. immediate, since cj = c.

515

fiDixon, Ginsberg, Hofer, Luks & Parkes

Lemma A.2 Let G group permutations, c set acted G. Suppose also
U sets acted G. k G{S,U } g G permutation
G,
|cg S| = |cgk S|

|cg U | = |cgk U |

Proof. clearly suffices show result S; U equivalent.
1

|cgk S| = |cg k |
= |cg S|
1

k = k set stabilizer therefore k 1 well (because
set stabilizer group).
Proposition 5.23 Let G group permutations, c set acted G. Suppose
also U sets acted G. instance k-transporter
problem g G, either every element G{c} gG{S,U } solution I, none is.
Proof. Combine lemmas A.1 A.2.
Lemma A.3 Let G, J Sym() (ordered) set {x1 , . . . , xn } suppose
Sym() satisfies xtl = zl 1 l k k n. Suppose fixed
k set Z = J{xi ,...,xk } . Suppose finally

Z

x ,...,x
zi > min xi 1 i1
h Gx1 ,...,xk first element Jh.
Proof. given existence j Zx1 ,...,xi1 zi > xjt
. Consider h = gt
g Gx1 ,...,xk . Since j Z, j stabilizes set {xi , . . . , xk }. Since g stabilizes every
jgt
jt

point set, fixes xi xji . Thus xgt
= xi xi = xi ,
jt
jgt

xgt
= xi = zi > xi = xi
jgt
hand, l < i, g j fix xl , xgt
l = xl . Since jgt thus precedes
gt, gt minimal Jgt.
Lemma 5.26 Suppose permutation labeling node Ht coset decomposition tree depth k, xti = zi k H = Gx1 ,...,xk residual
group


JM,x

,...,x



level. Let set points moved Gx1 ,...,xk . zi > min xi 1 i1
k, g Ht first element JgK.
Proof. direct consequence Lemma A.3. Let permutation JM,x1 ,...,xi1 .
Since fixes every point moved Gx1 ,...,xk , also fixes x1 , . . . , xi1 , follows
must permute remaining points xi , . . . , xk . Thus JM,x1 ,...,xi1 Zx1 ,...,xi1
Z set stabilizer statement Lemma A.3, therefore g first
element Jg. Since Jg JgK, result follows.
Procedure 6.5 Given set X pairs hl, gi group G, compute complete(X, G),
X |=G complete(X, G) L(complete(X, G)) = L(X)G :
516

fiZAP 3: Implementation

1
2
3
4
5
6


hl, gi X
l0 lG L(Y )
select h G lh = l0
hl0 , ghi
return

Proposition 6.6 X |=G complete(X, G) L(complete(X, G)) = L(X)G .
Proof. X |=G complete(X, G) every entry added clearly G-entailed
X. L(complete(X, G)) = L(X)G entire image L(X) G eventually
added.
Procedure 6.7 Given groups H G, element G, sets c, U , find
Transport(G, H, t, c, S, U ), skeletal set unit consequences (c, G) given P :
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

1

overlap(H, c, ) > 0
return hfalse,
1
overlap(H, c, (S U )t ) > 1
return hfalse,
c = cH
ct U =
return htrue, 1i
else return hfalse, hct U, 1ii
pruning lemma applied
return hfalse,

element c cH
t0 (H : H )
hu, V Transport(G, H , t0 t, c, S, U )
u = true
return htrue, V t0
else {hl, gt0 i|hl, gi V }
return hfalse,

Proposition 6.8 Assume |ch V | overlap(H, c, V ) |cH V | h H,
let Transport(G, c, S, U ) computed Procedure 6.7. g G
cg = cg U = , Transport(G, c, S, U ) = htrue, gi g.
g, Transport(G, c, S, U ) = hfalse, Zi, Z skeletal set unit consequences
(c, G) given P .
Proof. Procedure 6.7 identical Procedure 5.27 k = 1 except value returned. g cg = cg U = well, htrue, gi returned
line 7, cause htrue, gt0 returned recursive call(s) line 16
also.
g cg = cg U = , argument proceeds usual
induction number points c moved H. none, know correct
answer returned line 8 usual reasons; remains consider recursive case
517

fiDixon, Ginsberg, Hofer, Luks & Parkes

line 18. know every g cg unit, accumulate result
g 0 minimal JgK J = G{c} K = G{S,U } usual. need
show set hl, gi collected indeed skeletal set unit consequences.
see this, suppose hl, gi annotated unit consequence.
minimal jgk accumulated set pairs accumulated line 17,
associated literal l0 = cjgk U . since j G{c} set stabilizes clause c, cj = c
l0 = cgk U . Thus taking given k G{S,U } produces given unit consequence
element proposed skeleton, returned Procedure 6.7 indeed
skeletal set unit consequences.
Proposition 6.9 Let (c, G) augmented clause corresponding cardinality constraint. sets U , Procedure 6.7 expand linear number
nodes finding skeletal set unit consequences (c, G).
Proof. original cardinality constraint
x1 + + xm n
G Sym(X) X set xi c
x1 xmn+1
first show Leons pruning lemma 5.24 suffices reduce search
quadratic size. basic idea part proof follows.
Suppose expanding particular node, corresponding selection
image point xi c. image xi selected S, prune
immediately. image selected either U X U , image
smallest available point set question lexicographic reasons. addition,
original symmetry literals c used require literals
neither satisfied unvalued selected order expansion.
make argument formally, note first J = G{c} = Sym(c) Sym(X c)
K = G{S,U } = Sym(S) Sym(U ) Sym(X U ). assume without loss generality
points fixed coset decomposition tree xi order n + 1,
continue denote fixed image xi particular search node zi .
denote sequence zi less depth node question,
fixed part image clause c. also set l = |X U |, total number
points valued unsatisfied.
prune node which:
1. 6= . nodes pruned image c meets set
satisfied literals.
2. | U| > 1. above, nodes pruned image c includes two
unsatisfied literals.
3. = hy1 , ..., yj , ui, yi X U S, u U minimal U.
Leons lemma 5.24 k = l requires u = zj+1 min(uKy1 ,...,yj ). since
yi outside U , Ky1 ,...,yj Sym(U ) uKy1 ,...,yj U . Since u assumed
nonminimal U , node pruned.
518

fiZAP 3: Implementation

4. (X U) = hy1 , . . . , yj i, y1 , . . . , yj1 first j 1 elements
X U order, yj X U next element
X U S. argument identical previous paragraph used, since
Ky1 ,...,yj1 includes full symmetry group remaining elements X U S.
follows unpruned nodes search either
= hy1 , . . . , yk k min(l, n + 1),
= hy1 , . . . , yj , u, yj+1 , . . . , yk

(35)

k min(l, n), u minimal element U , yi smallest elements
X U order. need k l many possible values,
k n + 1 k n depth tree clause c
completely instantiated. linear number nodes first type
quadratic number nodes second.
reduce total number nodes searched linear, repeat argument
used discussion example following Lemma 5.26. There, considered node
image x1 z1 x2 z2 , z1 > z2 . Here, consider
slightly general case, = hz1 , . . . , zk1 , zk i, zi sequence except
zk1 > zk .
Lemma 5.26, Gx1 ,...,xk full symmetry group remaining xi ,
= {xk+1 , . . . , xm }. also J = Sym(x1 , . . . , xmn+1 ) Sym(xmn+2 , . . . , xm ).
since k n + 1, taking = k 1 statement lemma gives us
JM,x1 ,...,xi1 = JM,x1 ,...,xk2 Sym(xk1 , xk )
result,
(x

k1
zk1 > xk1

xk )t

= xtk = zk

node pruned.
fixes us position list point sequence among
yi thus reduces number search nodes linear.
Proposition 7.8 Suppose W watching set C P l literal. Then:
1. W watching set C backtrack point P .
2. C settled hP, li, W watching set C hP, li.
3. C settled hP, li, |(W {l}) C U (PC )| > 1, W {l}
watching set C hP, li.
4. l 6 W C, W watching set C hP, li.
0
Proof. None hard. First, note P 0 backtrack point P , Pc
subassignment Pc , watching set C P also watching set
C P 0 .
second claim, C settled hP, li, two possibilities:

519

fiDixon, Ginsberg, Hofer, Luks & Parkes

1. C unsettled P (so addition l P caused C settled),
hP, liC subassignment P (the subassignment proper P 6= P ). Since
C unsettled P , PC = P . Thus U (hP, liC ) U (PC ), W still watching
set.
2. C settled P , hP, liC = PC , W still watching set.
third claim follows second, since W {l} assumed watching
set C P .
fourth claim, suppose l 6 C l 6 C. C U (P ) = C U (hP, li),
W remains watching set. l C, C satisfied (and therefore settled)
l added P . W continues watching set virtue second claim.
remaining case, l C C U potentially smaller l removed
l adjoined P . impact intersection W l
W ; otherwise, W still watching set. W still watching set unless l
C W , proves final claim.
Proposition 7.10 Suppose W watching set (c, G) P l literal.
Then:
1. W watching set (c, G) backtrack point P .
2. l 6 W cG , W watching set (c, G) hP, li.
3. |(W V ) cg U (hP, li)| > 1 every g G cg unsettled hP, li,
W V watching set (c, G) hP, li.
4. |(W V ) cg [U (hP, li) (S(P ) S(P ))]| > 1 every g G, W V {l}
watching set (c, G) hP, li.
Proof. know W watching set every instance (c, G) P , use
Proposition 7.8 show claims follows.
First, Proposition 7.8 states directly W watching set every instance (c, G)
backtrack point P .
Second, l 6 W cG , g G, l 6 W cg . second claim thus
follows fourth claim Proposition 7.8.
remaining two claims interesting. third, suppose cg
instance (c, G). cg settled hP, li, know W still watching
set hP, li. Therefore W V also watching set cg hP, li. cg
unsettled hP, li, condition claim says |(W V ) cg U (hP, li)| > 1,
W V watching set cg hP, li. completes proof third
claim.
fourth final claim, three cases.
1. cg unsettled hP, li, note first cg S(P ) = ,
(W V ) cg [U (hP, li) (S(P ) S(P ))] = (W V ) cg U (hP, li)
W V watching set cg hP, li. Since l 6 U (hP, li),
(W V ) cg U (hP, li) = (W V {l}) cg U (hP, li)
520

fiZAP 3: Implementation

W V {l} watching set well.
2. cg unit hP, li, consider:
(a) l 6 cg , know fourth claim Proposition 7.8 W
watching set cg hP, li. follows W {l} must well, since
l 6 cg . Thus W V {l}.
(b) l cg , cg must form
cg = x1 xk l u
new unit consequence u, xi S(P ). Note also l cannot
either U (hP, li) S(P ). Thus
cg [U (hP, li) (S(P ) S(P ))] = {u}
violation premise claim.
3. Finally, cg satisfied hP, li, know W (and therefore W V ) watching
set cg hP, li; trick show remove l W V safely.
l 6 cg , obviously so.
l cg , need show third claim Proposition 7.8 applied,
need show
|(W V {l}) cg U (Pcg )| > 1

(36)

Given assumption
|(W V ) cg [U (hP, li) (S(P ) S(P ))]| > 1

(37)

note first since l 6 U (hP, li) (S(P ) S(P )), l intersection
(37), therefore equivalent
|(W V {l}) cg [U (hP, li) (S(P ) S(P ))]| > 1
follows (36) follow show
U (Pcg ) U (hP, li) (S(P ) S(P ))

(38)

U (Pcg ) U (hP, li)

(39)


Pcg (proper) subassignment hP, li. also
U (Pcg ) U (P ) S(P ) S(P )

(40)

first inclusion holds since l cg cg satisfied hP, li, cg must
satisfied P well. Thus Pc involves backtrack P , since P
last backtrack point P , Pcg subassignment P U (Pcg ) U (P ).
second inclusion (40) holds literals satisfied P
P must necessarily unvalued P . Combining (39) (40) gives us
(38), result proved.
521

fiDixon, Ginsberg, Hofer, Luks & Parkes

Procedure 7.11 Given group H, two sets c, V acted H, bound k 0,
compute overlap(H, c, V, k), collection elements V sufficient guarantee
h H, |ch V | > k, collection exists:
1 m0
2 W
3 orbit X H
4
{B1 , . . . , Bk } minimal block system W H
c W Bi
5
= |c X| + min(Bi V ) |B1 |
6
> 0
7
+
8
W W (X V )
9
> k
10
return W
11 return
Proposition 7.12 Procedure 7.11 returns nonempty set W Procedure 5.19
returns value excess k. case, |ch W | > k every h H.
Proof. first claim, examine two procedures. clear Procedure 7.11
returns soon Procedure 5.19 concludes minimum overlap greater k;
need simply argue W nonempty. increment W line 8 must
nonempty, since X V = , zero line 6.
second part, imagine replacing V procedure set W returned.
computation unchanged every step, conclusion follows.
Procedure 7.13 Given groups H G, element G, sets c, U , optionally watched element w, find Transport(G, H, t, c, S, U, w), skeletal set unit
w-consequences (c, G) given P :

522

fiZAP 3: Implementation

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

1

w supplied wt 6 cH
return hfalse, ,
1
V overlap(H, c, , 0)
V 6=
return hfalse, ,
1
V overlap(H, c, (S U )t , 1)
V 6=
return hfalse, , V
c = cH
ct U =
return htrue, 1,
else return hfalse, hct U, 1i,
pruning lemma applied
return hfalse, ,
1
element c cH . w supplied w 6 ctH , choose wt H .

W
t0 (H : H )
hu, V, Xi Transport(G, H , t0 t, c, S, U, w)
u = true
return htrue, V t0 ,
else W W X
{hl, gt0 i|hl, gi V }
return hfalse, Y, W

Proposition 7.14 Suppose overlap(H, c, V, k) computed using Procedure 7.11,
otherwise satisfies conclusion Proposition 7.12. g G
w cg cg = cg U = , Transport(G, c, S, U, w) computed Procedure 7.13
returns htrue, g, g. g, Procedure 7.13 returns hfalse, Z, W i,
Z skeletal set unit w-consequences (c, G) given P , W
|W G{S,U,{w}} ch U | > 1 every h H w ch ch unsettled P .
Proof. restriction permutations g w cg enforced first two
lines procedure; note contradiction found line 11, points
c fixed, w cg certain. Note prune basis
without affecting overall correctness procedure, since pruning lemmas
restricted group K = G{S,U,{w}} , leaves watched literal w intact.
difference Procedure 7.14 Procedure 6.7 involves computation set W watched literals. set produced line 8, know
Proposition 7.12 set W sufficient guarantee |W cht U | > 1
every cht current residual search tree. must therefore show h satisfying
conditions proposition covered W G{S,U,{w}} . see this, consider every
point node pruned procedure, show points covered
exclusions statement proposition:
1. prune line 2 occur w 6 cht h H.
523

fiDixon, Ginsberg, Hofer, Luks & Parkes

2. prune line 5 occur cht 6= every h H, cht settled
P .
3. pruning lemma applied, must eventual solution g shown
minimal usual double coset G{c} gG{S,U,{w}} . case,
watching set operated G{S,U,{w}} statement proposition
itself.
Procedure 7.16 (Unit propagation) compute Unit-Propagate(C, P, L) C
set watched augmented clauses, P annotated partial assignment, L set
pairs hl, ri literals l reasons r:
1 L 6=
2
hl, ri element L
3
L L hl, ri
4
P hP, hl, rii
5
h(c, G), W C
6
l W
7
hr, H, V Transport(G, c, S(P ), U (P ), l)
8
r = true
9
li literal cH highest index P
10
return htrue, resolve((cH , G), ci )i
0
11
H complete(H, G{S(P ),U (P ),{l}} )
12
h H 0
13
z literal ch unassigned P
14
hz, r0 L
15
L L hz, ch
16
W W (U (P ) V G{S(P ),U (P ),{l}} )
17
U U (P ) (S(P ) S(P ))
18
H = transport(G, c, , W U, 1, l) = failure
19
W W {l}
20 return hfalse, P
Proposition 7.17 Let P annotated partial assignment, C set watched
augmented clauses, every h(c, G), W C, W watching set (c, G) P .
Let L set unit consequences clauses C. Unit-Propagate(C, P, L) returns
htrue, ci augmented clause c, c nogood P , modified watching
sets C still watching sets P . Otherwise, value returned hfalse, P
watching sets C replaced watching sets P .
Proof. really matter assembling pieces. Procedure 7.16 essentially
loop literals L, much like original procedure 2.7. literal
l, find unit clauses contain l appropriate call Transport
clause l watched. Transport call reveals presence contradiction,
return resolvent reasons l l usual. contradiction found,
adjust partial assignment Procedure 2.7, add new unit consequences

524

fiZAP 3: Implementation

list remains analyzed, update watching set accordance
Propositions 7.10 7.14.
remaining issue removal l watching set W line 19
Procedure 7.16. precisely fourth claim Proposition 7.10
applied. Note call transport, use U (P ) instead U (hP, li), since l
already added P line 4. also require l instance cg , since
otherwise intersection cg obviously unaffected removal l.
Lemma 8.3 c1 c2 two nogoods P , c1 <P c2 .
Proof. immediate. soon last literal c2 c1 (which must happen
eventually literals removed Definition 8.2), falsification depth c2 exceed
c1 .
Procedure 8.4 Suppose given two augmented clauses (, G) (, H)
unit partial assignment P = hl1 , . . . , ln i, l l . find <P -minimal
l-resolvent (, G) (, H):
1
2
3
4
5
6
7
8
9
10
11
12
13

U {l, l}
literals cant avoid
f
f
p [( ) U ]?P
p > 0
g transport(G, , {lp , . . . , ln } U, , 0, l)
h transport(H, , {lp , . . . , ln } U, , 0, l)
g = failure h = failure
U U {lp }
else f g
f h
p [(f f ) U ]?P
return resolve(f , f )

Proposition 8.5 Suppose given two augmented clauses (, G) (, H)
unit partial assignment P , l l . value
returned Procedure 8.4 <P -minimal l-resolvent (, G) (, H).
Proof. need show procedure terminates, returns l-resolvent,
result <P -minimal.
show Procedure 8.4 terminates, show p reduced every iteration
main loop. beginning iteration, know
lp (f f ) U

(41)

end iteration, line 9 selected, f f unchanged lp
added U . means (41) longer satisfied, still satisfied
li > p. Thus p reduced line 12.
If, hand, lines 10 11 selected, know definition g
h lines 6 7 literal l {lp , . . . , ln } U , l 6 (f f ).
words, l {lp , . . . , ln }, l 6 (f f ) U . Thus p
reduced, procedure therefore terminates. returns resolvent clear.
525

fiDixon, Ginsberg, Hofer, Luks & Parkes

see value returned <P -minimal, suppose gm hm
gm hm <P f f . show f f cannot permutations returned
line 13.
Set z = [(f f ) (gm hm )]?P ; last point included f images
produced procedure images provided hypothetical counterexample. Since gm hm <P f f , two sets agree literals index greater
z.
Since lz (f f ), initial value p set line 4 least z; since
procedure terminates p < 0, final value less z.
Consider point procedure p changes value less z one
less z. change lp added U , one transport calls
must failed, impossible (say) image avoid {lp , . . . , ln }U .
know f avoids {lp+1 , . . . , ln } U . Thus gm avoids {lp+1 , . . . , ln } U ,
assuming lp 6 gm , transport(Gl,l , , {lp , . . . , ln } U, , 0)
succeeded all.
follows change p must lines 10 11. also
impossible, since fact successfully managed avoid lz contradicts
assumption z = [(f f ) (gm hm )]?P lz f f .

Appendix B. ZAP Problem Format
Historically, Boolean satisfiability problems typically format variables correspond positive integers, literals nonzero integers (negative integers negated
literals), clauses terminated zeroes. dimacs format precedes actual
clauses problem single line p cnf 220 1122 indicating
220 variables appearing 1,122 clauses problem.
numerical format makes impossible exploit existing understanding
user might problem question; may problem conventional
Boolean tool (since problem structure obscured Boolean encoding
event), felt inappropriate building augmented solver. felt
important user able to:
1. Specify numerical constraints appear cardinality parity constraints,
2. Quantify axioms finite domains,
3. Provide group augmentations explicitly mechanisms insufficient.
discussing specific provisions zap makes areas, remark
zap input file begins list domain specifications, giving names
sizes domain used theory. followed predicate specifications, giving
arity predicate domain type argument. predicates
domains defined, possible refer predicate instances directly (e.g.,
in[1 3] indicating first pigeon third hole) nonground fashion (e.g.,
in[x y]).

526

fiZAP 3: Implementation

Group definition group specified directly, assigned symbolic designator
used augmented clause. group syntax conventional one,
group described terms generators, permutation.
permutation list cycles, cycle space-separated list literals.
augmented clause uses previously defined group form
clause

GROUP

group1



groupn

(ground) clause essentially sequence literals groupi
designator group used. group augmented clause group
collectively generated groupi s.
example, group-based encoding pigeonhole instance involving
four pigeons three holes:
// domain specs
SORT pigeon 4 ;
SORT hole 3 ;
// predicate specs
PREDICATE in(pigeon hole) ;
// group specs
GROUP G < ((in[1
((in[1
(in[1
((in[1
(in[4
((in[1
(in[4

1]
1]
3]
1]
1]
1]
1]

in[2
in[3
in[3
in[1
in[4
in[1
in[4

1]) (in[1 2] in[2
1] in[4 1]) (in[1
3] [4 3]))
2]) (in[2 1] in[2
2]))
3]) (in[2 1] in[2
3])) > ;

2]) (in[1 3] in[2 3]))
2] in[3 2] [4 2])
// permute pigeons
2]) (in[3 1] in[3 2])
// permute holes
3]) (in[3 1] in[3 3])

// group-based encoding
-in[1 1] -in[2 1] GROUP G ;
in[1 1] in[1 2] in[1 3] GROUP G ;

two types domain variables, pigeons (of four) holes (of
three). single predicate indicating given pigeon
particular hole. single group, corresponds symmetries holes
pigeons.
generate group, use four generators. first two correspond symmetry
pigeons, first generator swapping first two pigeons second generator
rotating pigeons one, three four. (Recall permutations (1, 2) (1, 3, 4)
generate symmetry group S4 pigeons.)
second pair generators generate symmetry holes similarly, first
generator swapping first two holes second generator swapping holes one
three. (Once again, (1, 2) (1, 3) generate S3 .)
Turning axioms, first says first hole cannot contain first
two pigeons, therefore hole contain two distinct pigeons virtue
group action. second axiom says first pigeon hole,
therefore every pigeon does.
527

fiDixon, Ginsberg, Hofer, Luks & Parkes

Cardinality parity constraints group specified directly, general
form zap axiom
quantifiers clause result
quantifiers described below. result includes information
desired right hand side axiom, following:
simple terminator, indicating clause Boolean,
comparison operator (>, <=, =, etc.) followed integer, indicating
clause cardinality constraint,
mod-2 operator (%2=) followed integer m, indicating sum
values literals required congruent mod 2.
Quantification

quantifiers form
(x1 , . . . , xk )


(x1 , . . . , xk )
xi variables appear future predicate instances.
addition two classical quantifiers above, also introduce
(x1 , . . . , xk )
quantifier means variables take values cause
quantified predicates instances become identical. example, axiom saying
one pigeon hole becomes
(p1 , p2 , h) . in(p1 , h) in(p2 , h)

(42)

Contrast conventional
(p1 , p2 , h) . in(p1 , h) in(p2 , h)

(43)

specific pigeon p hole h,
in(p, h) in(p, h)

(44)

instance (43) (42). Since (44) equivalent in(p, h), inappropriate
inclusion pigeonhole formulation.
introduction new quantifier understood light discussion
Section 6.1 zap2, argued many cases, quantification given
fact natural provided . quantification also far easier
represent using augmented clauses, avoids many cases need introduce
reason equality. event, zap supports forms universal quantification.
quantifier-based encoding pigeonhole problem:

528

fiZAP 3: Implementation

SORT pigeon 9;
SORT hole 8;
PREDICATE in(pigeon hole);
// quantification-based encoding
NOTEQ (x z) -in[x z] -in[y z] ;
FORALL(z) EXISTS(h) in[z h] ;

nine pigeon instance. two axioms say directly hole z contain
two distinct pigeons x (note use NOTEQ ), every pigeon z
hole h. encoding presumably intuitive previous one.

References
Aho, A. V., Hopcroft, J. E., & Ullman, J. D. (1974). Design Analysis Computer
Algorithms. Addison-Wesley.
Babai, L., & Moran, S. (1988). Arthur-Merlin games: randomized proof system,
hierarchy complexity classes. J. Comput. System Sci., 36, 254276.
Barth, P. (1995). Davis-Putnam based enumeration algorithm linear pseudoboolean optimization. Tech. rep. MPI-I-95-2-003, Max Planck Institut fur Informatik,
Saarbrucken, Germany.
Bayardo, R. J., & Miranker, D. P. (1996). complexity analysis space-bounded learning
algorithms constraint satisfaction problem. Proceedings Thirteenth
National Conference Artificial Intelligence, pp. 298304.
Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques solve real-world
SAT instances. Proceedings Fourteenth National Conference Artificial
Intelligence, pp. 203208.
Dixon, H. E., & Ginsberg, M. L. (2000). Combining satisfiability techniques AI
OR. Knowledge Engrg. Rev., 15, 3145.
Dixon, H. E., Ginsberg, M. L., Luks, E. M., & Parkes, A. J. (2004a). Generalizing Boolean
satisfiability II: Theory. Journal Artificial Intelligence Research, 22, 481534.
Dixon, H. E., Ginsberg, M. L., & Parkes, A. J. (2004b). Generalizing Boolean satisfiability
I: Background survey existing work. Journal Artificial Intelligence Research,
21, 193243.
Furst, M., Hopcroft, J., & Luks, E. (1980). Polynomial time algorithsm permutation
groups. Proceedings 21st Annual IEEE Symp. Foundations Computer Science
(FOCS-80), pp. 3641. IEEE.
GAP Group (2004).
GAP Groups, Algorithms Programming, Version 4.4.
http://www.gap-system.org.
Ginsberg, M. L. (1993). Dynamic backtracking. Journal Artificial Intelligence Research,
1, 2546.
Ginsberg, M. L., & Parkes, A. J. (2000). Search, subsearch QPROP. Proceedings
Seventh International Conference Principles Knowledge Representation
Reasoning, Breckenridge, Colorado.
529

fiDixon, Ginsberg, Hofer, Luks & Parkes

Gomes, C. P., & Selman, B. (1997). Problem structure presence perturbations.
Proceedings Fourteenth National Conference Artificial Intelligence, pp.
221226, Providence, RI.
Haken, A. (1985). intractability resolution. Theoretical Computer Science, 39, 297
308.
Harrison, M. A. (1989). Introduction Switching Automata Theory. McGraw-Hill.
Heule, M., & van Maaren, H. (2004). Aligning CNF- equivalence-reasoning.
Seventh International Conference Theory Applications Satisfiability Testing,
pp. 174180. Also available http://www.satisfiability.org/SAT04/programme/72.pdf.
Hooker, J. N. (1988). Generalized resolution cutting planes. Annals Operations
Research, 12, 217239.
Jerrum, M. (1986). compact representation permutation groups. J. Algorithms, 7,
6078.
Knuth, D. E. (1991). Notes efficient representation permutation groups. Combinatorica, 11, 5768.
Leon, J. (1991). Permutation group algorithms based partitions I: Theory algorithms.
J. Symbolic Comput., 12, 533583.
Li, C. M. (2000). Integrating equivalency reasoning Davis-Putnam procedure.
Proceedings Seventeenth National Conference Artificial Intelligence, pp. 291
296.
Li, C. M. (2003). Equivalent literal propagation Davis-Putnam procedure. Discrete App.
Math., 130 (2), 251276.
Li, C. M., Jurkowiak, B., & Purdom, Jr., P. W. (2002). Integrating symmetry breaking
DLL procedure. Fifth International Symposium Theory Applications
Satisfiability Testing (SAT2002), pp. 149155.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
efficient SAT solver. 39th Design Automation Conference.
Pudlak, P. (1997). Lower bounds resolution cutting planes proofs monotone
computations. J. Symbolic Logic, 62 (3), 981998.
Rotman, J. J. (1994). Introduction Theory Groups. Springer.
Schaefer, T. J. (1978). complexity satisfiability problems. Proceedings
Tenth Annual ACM Symposium Theory Computing, pp. 216226.
Seress, A. (2003). Permutation Group Algorithms, Vol. 152 Cambridge Tracts Mathematics. Cambridge University Press, Cambridge, UK.
Sims, C. C. (1970). Computational methods study permutation groups. Leech, J.
(Ed.), Computational Problems Abstract Algebra, Proc. Conf. Oxford, 1967. Pergamon Press.
Tseitin, G. (1970). complexity derivation propositional calculus. Slisenko,
A. (Ed.), Studies Constructive Mathematics Mathematical Logic, Part 2, pp.
466483. Consultants Bureau.
530

fiZAP 3: Implementation

Urquhart, A. (1987). Hard examples resolution. JACM, 34, 209219.
Zhang, H., & Stickel, M. E. (2000). Implementing Davis-Putnam method. Journal
Automated Reasoning, 24 (1/2), 277296.

531

fiJournal Artificial Intelligence Research 23 (2005) 667-726

Submitted 07/04; published 06/05

Keys, Nominals, Concrete Domains
Carsten Lutz

lutz@tcs.inf.tu-dresden.de

Theoretical Computer Science, TU Dresden
D-01062 Dresden, Germany

Carlos Areces

areces@loria.fr

INRIA Lorraine, Nancy
54602 Villers les Nancy Cedex, France

Ian Horrocks

horrocks@cs.man.ac.uk

Department Computer Science
University Manchester
Oxford Road, Manchester M13 9PL, UK

Ulrike Sattler

sattler@cs.man.ac.uk

Department Computer Science
University Manchester
Oxford Road, Manchester M13 9PL, UK

Abstract
Many description logics (DLs) combine knowledge representation abstract, logical
level interface concrete domains like numbers strings built-in predicates <, +, prefix-of. hybrid DLs turned useful several
application areas, reasoning conceptual database models. propose
extend DLs key constraints allow expression statements like
US citizens uniquely identified social security number. Based idea,
introduce number natural description logics perform detailed analysis
decidability computational complexity. turns naive extensions
key constraints easily lead undecidability, whereas careful extensions yield NExpTime-complete DLs variety useful concrete domains.

1. Motivation
Description logics (DLs) family formalisms allow representation
reasoning conceptual knowledge structured semantically well-understood
manner (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). central
entities representing knowledge concepts, constructed atomic
concept names (unary predicates) role names (binary relations) means concept
role constructors offered particular DL. example, basic propositionally
closed description logic ALC (Schmidt-Schau & Smolka, 1991), describe company
part-time employees full-time managers using concept
Company u employee.Parttime u employee.(Manager Parttime).
example, words beginning uppercase letters denote concept names
employee denotes role name.
c
2005
AI Access Foundation. rights reserved.

fiLutz, Areces, Horrocks, & Sattler

Rather viewed conceptual entities knowledge base, concepts
can, generally, understood central notion various kinds class-centered
formalisms. last decade, observation given rise various new challenging applications description logics reasoning database conceptual models
expressed entity-relationship diagrams object-oriented schemas (Calvanese, Lenzerini,
& Nardi, 1998; Calvanese, De Giacomo, & Lenzerini, 1998) reasoning ontologies
use semantic web (Baader, Horrocks, & Sattler, 2002a; Horrocks, 2002; Horrocks,
Patel-Schneider, & van Harmelen, 2002). new applications have, turn, stimulated
research description logics since expressive power existing DLs insufficient
new tasks. One important extension providing expressive means allow
integration numbers datatypes: suppose, example, want
extend earlier descriptions companies employees include founding year
company hiring year employee. Then, may want describe companies
founded 1970 state hiring year employees prior
founding year employing company. this, obviously need way talk
natural numbers (such 1970) comparisons natural numbers.
Nowadays, standard approach integrate datatypes description logics
extend DLs concrete domains, first proposed Baader Hanschke (1991a)
recently surveyed Lutz (2003). precisely, concrete domain consists set
(such natural numbers) predicates associated fixed extension
set1 (such unary =0 , binary <, ternary +). integration
concrete domains into, say, description logic ALC achieved adding
1. abstract features, i.e. functional roles;
2. concrete features, i.e. (partial) functions associating values concrete domain
(e.g., natural numbers) logical objects;
3. concrete domain-based concept constructor.
DL obtained extending ALC way called ALC(D), denotes
concrete domain viewed parameter logic. example, using
suitable concrete domain describe constraints formulated above: concept
Employee u employer.foundingyear.<1970 u hiringyear, (employer foundingyear).
describes set employees employed company founded 1970
hiring year prior companys founding year. example, term
foundingyear.<1970 instance concrete domain concept constructor (not
confused existential value restriction employee.Parttime), third
conjunct. <1970 unary predicate thus former instance takes one
concrete feature foundingyear argument, second instance uses binary predicate
requiring two arguments: concrete feature hiringyear sequence features
(employer foundingyear) consisting abstract feature employer concrete feature
foundingyear.
Concrete domains rather important many applications DLs, including two
mentioned above:
1. fixed extension predicates often called built-in.

668

fiKeys, Nominals Concrete Domains

standard way using description logics reasoning conceptual database
models translate given model DL representation use DL reasoner FaCT (Horrocks, 1998) RACER (Haarslev & Moller, 2001) compute
consequences information provided explicitly model. includes
detecting inconsistencies inferring additional, implicit containments entities/classes (Calvanese et al., 1998). Since databases store concrete data
like numbers strings, constraints concerning data usually part
conceptual model thus also captured description logic used
reasoning. Indeed, example concepts viewed DL encoding
constraints database companies employees. discussed
Lutz (2002c), description logics concrete domains well-suited conceptual modeling applications involving concrete datatypes.
So-called concrete datatypes play prominent role construction ontologies (Horrocks et al., 2002). Say, example, want construct ontology
used describing car dealers web pages web services.
ontology, concrete datatypes prices, manufacturing years, names car
models doubtlessly important. formulate ontology using DL,
need way represent concrete datatypes. Consequently, almost DLs
proposed ontology language equipped form concrete
domain (Fensel, van Harmelen, Horrocks, McGuinness, & Patel-Schneider, 2001; Horrocks et al., 2002; Dean, Connolly, van Harmelen, Hendler, Horrocks, McGuinness,
Patel-Schneider, & Stein, 2002). Furthermore, since ontology languages provide
inverse abstract roles functional restrictions, users ontology designers
quite surprised find provide inverse concrete functional
featureswhich due fact features correspond concrete key
constraints, reasoning algorithms known whose effect
decidability/complexity yet investigated.
paper, propose enhance expressive power description logics
concrete domains extending concrete key constraints. extension
useful knowledge representation two applications sketched above.
following three examples describe basic idea.
1. Suppose that, knowledge representation application, represent nationalities
concept names US German and, US citizens, store social
security number using concrete feature ssn. would natural state
US citizens uniquely identified social security number, i.e. two distinct
instances
Human u nationality.US
must different values ssn feature. extension DLs concrete
domains, expressed using key assertion 2
(ssn keyfor Human u nationality.US).
2. Readers familiar
Vrelationship DLs first order logic notice key assertion
equivalent x1 x2 .(( i{1,2} (Human(xi )z.(nationality(xi , z)US(z)))(x1 = x2 )) (ssn(x1 ) =
ssn(x2 ))).

669

fiLutz, Areces, Horrocks, & Sattler

2. Returning database companies employees, could useful
equip every employee (i) concrete feature branch storing branch-ID
working (ii) concrete feature id storing personnel-ID. would
natural enforce branch-ID together personnel-ID uniquely
identifies employees, even though personnel-IDs unique.
using composite key assertion
(branch, id keyfor Employee).
3. car dealers ontology, may assume cars well manufacturers
equipped identification numbers every car uniquely identified
combination identification number manufacturers one. express
this, could employ composite key assertion referring sequences features,
case (manufacturer id):
(id, (manufacturer id) keyfor Car).
formally, propose extend DLs provide concrete domains key boxes,
sets key assertions form
(u1 , . . . , un keyfor C),
ui sequence f1 fn g abstract features fj followed single concrete
feature g, C concept. examples illustrate, idea key constraints
natural. Since, moreover, keys play important role databases and, mentioned
above, reasoning database conceptual models important, challenging application
description logics, several approaches extend description logics keys already
investigated (Borgida & Weddell, 1997; Calvanese, De Giacomo, & Lenzerini, 2000;
Khizder, Toman, & Weddell, 2001). distinguishes approach existing ones,
however, idea using concrete domains constructing key constraints, rather
defining keys abstract, logical level.
goal paper provide comprehensive analysis effects decidability computational complexity adding key boxes description logics concrete
domains. end, extend two description logics ALC(D) SHOQ(D) key
boxes, way obtaining ALCK(D) SHOQK(D), respectively. basic DL
concrete domains ALC(D) already discussed above, SHOQ(D) proposed
ontology language (Horrocks & Sattler, 2001). provides wealth expressive
possibilities general concept inclusion axioms (GCIs), transitive roles, role hierarchies, nominals, qualifying number restrictions. Moreover, offers restricted variant
concrete domain constructor disallows use sequences features order
avoid undecidability reasoning. main outcome investigations key
constraints dramatic impact decidability complexity reasoning:
example, whereas satisfiability ALC(D)-concepts known PSpace-complete (Lutz,
2002b), show satisfiability ALCK(D)-concepts w.r.t. key boxes is, general,
undecidable. Decidability regained restrict concepts used key boxes
670

fiKeys, Nominals Concrete Domains

Boolean combinations concept names (Boolean key boxes). Interestingly, satisfiability ALCK(D)-concepts w.r.t. Boolean key boxes still NExpTime-complete even
simple concrete domains. case SHOQ(D) SHOQK(D), leap
complexity somewhat less dramatic since SHOQ(D)-concept satisfiability already ExpTime-complete: again, addition key boxes results NExpTime-complete reasoning
problems.
interesting note exists close connection key assertions
so-called nominals, i.e. concept names one instance,
Pope. Nominals standard means expressivity description logics sometimes
appear disguise one-of operator (Borgida & Patel-Schneider, 1994; Horrocks
et al., 2002). hard see key boxes simulate nominals: if, example,
use concrete domain based natural numbers providing unary predicates
=n equality n , key assertion (g keyfor >), > stands
logical truth, obviously makes concept g.=n behave like nominal, n
. reason, also consider ALCO(D), extension ALC(D) nominals,
ALCOK(D), extension ALCK(D) nominals.3 main result concerning
nominals that, although general lower expressive power key boxes,
already make reasoning NExpTime-hard combined concrete domains: exist
concrete domains ALCO(D)-concept satisfiability NExpTime-complete.
like stress NExpTime-hardness results obtained
paper accordance observation made (Lutz, 2004) PSpace-upper
bound reasoning ALC(D) robust w.r.t. extensions logic: exist
several seemingly harmless extensions ALC(D) (for example acyclic TBoxes
inverse roles) make complexity reasoning leap PSpace-completeness
NExpTime-completeness many natural concrete domains.

N

N

remainder paper organized follows: Section 2, formally introduce
concrete domains, key boxes, DL ALCOK(D) together fragments ALCK(D)
ALCO(D). Moreover, define Boolean key boxes, allow Boolean combinations concept names appear key definitions. Additionally, introduce
important properties key boxes: path-free key boxes prohibit use sequences
features key assertions; unary key boxes, key assertion involves exactly one
sequence features; composite key boxes simply non-unary ones.
Section 3 devoted establishing lower bounds extensions ALC(D) key
boxes nominals. Section 3.1, use reduction Post Correspondence Problem
prove ALCK(D)-concept satisfiability w.r.t. (non-Boolean) key boxes undecidable
large class concrete domains. shift attention Boolean key boxes
since, Section 4, show restriction restores decidability. Section 3.2,
introduce NExpTime-complete variant domino problem three concrete
domains useful reduction problem concept satisfiability DLs
Boolean key boxes nominals. Section 3.3, use concrete domains
prove ALCK(D)-concept satisfiability w.r.t. Boolean, path-free unary key boxes
NExpTime-hard natural concrete domains. Section 3.4, prove
exist concrete domains ALCO(D)-concept satisfiability without reference
3. Note logic SHOQ(D) already provides nominals.

671

fiLutz, Areces, Horrocks, & Sattler

key boxes already NExpTime-hard; show true even concrete
domains computationally simple (PTime) considered isolation.
purpose Section 4 develop reasoning procedures description logics
key boxes prove upper complexity bounds matching NExpTime lower bounds
established previous section. start Section 4.1 tableau algorithm
decides ALCOK(D)-concept satisfiability w.r.t. Boolean key boxes, provided concrete domain key-admissible. Intuitively, concrete domain key-admissible
exists algorithm takes finite conjunction c predicates set
variables, decides whether conjunction satisfiable and, so, chooses solution c
returns information variables take values it. call
algorithm D-tester. chosen tableau algorithm since type reasoning
procedure potential implemented efficient reasoners shown
behave well practice (Horrocks, Sattler, & Tobies, 2000; Haarslev & Moller, 2001).
algorithm implies following upper complexity bound: key-admissible concrete
domain non-deterministic polynomial time D-tester exists, ALCO(D)concept satisfiability w.r.t. Boolean key boxes NExpTime.
Section 4.2, devise tableau algorithm SHOQK(D)-concept satisfiability
w.r.t. path-free key boxes might involve non-Boolean concepts. decidability
ALCOK(D), restricted key boxes Boolean ones. SHOQK(D), restriction possible since SHOQ(D) provides TBoxes, thus longer distinguish
Boolean non-Boolean concepts. hand, follows undecidability proof Baader Hanschke (1992) SHOQ(D) undecidable allow
sequences features concrete domain constructors. Thus restrict key assertions
analogously path-free ones, show yields indeed decidable logic. expressive power orthogonal one ALCOK(D), previous undecidability
results imply combination ALCOK(D) SHOQK(D) undecidable.
by-product correctness proof algorithm, obtain bounded model property SHOQK(D), implies SHOQK(D)-concept satisfiability w.r.t. path-free
key boxes NExpTime key-admissible concrete domain nondeterministic polynomial time D-tester exists.
Section 5, summarize results obtained give outlook possible future
research.

2. Description Logics Concrete Domains
following, introduce description logic ALCOK(D). Let us start defining
concrete domains:
Definition 2.1 (Concrete Domain). concrete domain pair (D , ),
set set predicate names. predicate name P associated
arity n n-ary predicate P nD .
Based concrete domains, define ALCOK(D)-concepts key boxes.
Definition 2.2 (ALCOK(D) Syntax). Let NC , , NR , NcF pairwise disjoint countably infinite sets concept names, nominals, role names, concrete features. Furthermore, assume NR contains countably infinite subset NaF abstract features.
672

fiKeys, Nominals Concrete Domains

path u composition f1 fn g n abstract features f1 , . . . , fn (n 0) concrete
feature g. Let concrete domain. set ALCOK(D)-concepts smallest set

every concept name every nominal concept,
C concepts, R role name, g concrete feature, u1 , . . . , un paths,
P predicate arity n, following expressions also concepts:
C, C u D, C D, R.C, R.C, u1 , . . . , un .P, g.
key assertion expression
(u1 , . . . , uk keyfor C),
u1 , . . . , uk (k 1) paths C concept. finite set key assertions
called key box.
usual, use > abbreviation arbitrary propositional tautology,
abbreviation >, C abbreviation C D, C abbreviation
(C D) u (D C). Throughout paper, also consider several fragments
description logic ALCOK(D). DL ALCO(D) obtained ALCOK(D)
admitting empty key boxes. particular, set ALCO(D)-concepts set
ALCOK(D)-concepts. Furthermore, disallowing use nominals, obtain
fragment ALC(D) ALCO(D) ALCK(D) ALCOK(D).
description logic ALCOK(D) equipped Tarski-style set-theoretic semantics.
Along semantics, introduce two standard inference problems: concept
satisfiability concept subsumption.
Definition 2.3 (ALCOK(D) Semantics). interpretation pair (I , ),
non-empty set, called domain, interpretation function. interpretation
function maps
concept name C subset C ,
nominal N singleton subset N ,
role name R subset RI ,
abstract feature f partial function f ,
concrete feature g partial function g .
673

fiLutz, Areces, Horrocks, & Sattler

u = f1 fn g path, uI (d) defined g (fnI (f1I (d)) ). interpretation
function extended arbitrary concepts follows:
(C)I := \ C
(C u D)I := C DI
(C D)I := C DI
(R.C)I := {d | e (d, e) RI e C }
(R.C)I := {d | e , (d, e) RI , e C }
(u1 , . . . , un .P )I := {d | x1 , . . . , xn : uIi (d) = xi (x1 , . . . , xn ) P }
(g)I := {d | g (d) undefined}.
Let interpretation. model concept C iff C 6= . Moreover,
satisfies key assertion (u1 , . . . , un keyfor C) if, a, b C ,
uI1 (a) = uI1 (b), . . . , uIn (a) = uIn (b) implies = b.
model key box K iff satisfies key assertions K. concept C satisfiable
w.r.t. key box K iff C K common model. C subsumed concept w.r.t.
key box K (written C vK D) iff C DI models K.
well-known that, description logics providing Boolean operators, subsumption
reduced (un)satisfiability vice versa: C vK iff C u unsatisfiable
w.r.t. K C satisfiable w.r.t. K iff C 6vK . allows us concentrate concept
satisfiability devising complexity bounds reasoning description logics: lower
upper complexity bounds concept satisfiability imply corresponding bounds
concept subsumptiononly complementary complexity class.
decision procedures description logics concrete domains devised
without committing particular concrete domain, well-defined interface
decision procedure concrete domain reasoner needed. Usually, interface
based assumption concrete domain admissible (Baader & Hanschke,
1991a; Lutz, 2002a, 2003):
Definition 2.4 (D-conjunction, Admissibility). Let concrete domain V set
variables. D-conjunction (finite) predicate conjunction form
^ (i)
c=
(x0 , . . . , x(i)
ni ) : Pi ,
i<k
(i)

Pi ni -ary predicate < k xj variables V. D-conjunction
c satisfiable iff exists function mapping variables c elements
(i)
(i)
((x0 ), . . . , (xni )) PiD < k. function called solution c.
say concrete domain admissible iff
1. contains unary predicate >D >D
= ;
2. closed negation, i.e., n-ary predicate P , predicate

P arity n P = nD \ P ;
674

fiKeys, Nominals Concrete Domains

3. satisfiability D-conjunctions decidable.
refer satisfiability D-conjunctions D-satisfiability.
shall see, sometimes makes considerable difference w.r.t. complexity decidability restrict key boxes various ways, example disallow paths length greater
one. Therefore, introduce useful notions.
Definition 2.5 (Boolean, Path-free, Simple). key box K called
Boolean concepts appearing (key assertions in) K Boolean combinations
concept names;
path-free if, key assertions (u1 , . . . , un keyfor C) K, u1 , . . . , un NcF ;
simple path-free Boolean;
unary key assertions K unary key assertions, i.e. form (u keyfor C).
concept C called path-free if, subconcepts form u1 , . . . , un .P , u1 , . . . , un
concrete features.
emphasize key box might necessarily Boolean path-free, sometimes
call key box general. Similarly, emphasize key box necessarily
unary key box, sometimes call key box composite.

3. Lower Bounds
section, prove lower complexity bounds description logics concrete domains key boxes and/or nominals. Section 3.1, start showing satisfiability ALCK(D)-concepts w.r.t. (general) key boxes undecidable many interesting
concrete domains. discouraging picture painted result mitigated fact
that, Section 4.1, shall prove restriction Boolean key boxes restores decidability. thus interesting look lower complexity bounds apply
restriction. preparation this, introduce Section 3.2 NExpTime-complete variant domino problem three concrete domains well-suited reductions
problem.
Section 3.3, prove satisfiability path-free ALCK(D)-concepts w.r.t.
simple key boxes NExpTime-hard large class concrete domains that,
many concrete domains, holds even restrict key boxes unary ones. Finally,
consider description logic ALCO(D) Section 3.4 identify several concrete
domains ALCO(D)-concept satisfiability (without key boxes!) NExpTimehard. already mentioned, key boxes nominals closely related: key boxes
express nominals, general powerful.
3.1 Undecidability ALCK(D) General Key Boxes
prove satisfiability ALCK(D)-concepts w.r.t. key boxes undecidable
large class concrete domains allow complex ALCK(D)-concepts occur key
assertions. proof reduction well-known undecidable Post Correspondence
Problem (Post, 1946; Hopcroft & Ullman, 1979).
675

fiLutz, Areces, Horrocks, & Sattler

Definition 3.1 (PCP). instance P Post Correspondence Problem (PCP) given
finite, non-empty list (`1 , r1 ), . . . , (`k , rk ) pairs words alphabet .
sequence integers i1 , . . . , im , 1, called solution P `i1 `im = ri1 rim .
PCP decide whether given instance P solution.
reducing PCP satisfiability DLs, need appropriate concrete
domain. obviously natural use concrete domain based words concatenation.
later see results obtained concrete domain carry
concrete domains based numbers arithmetics. following concrete domain
introduced Lutz (2004). definition presupposes fixed alphabet least
binary.
Definition 3.2 (Concrete domain W). concrete domain W defined setting W :=
defining W smallest set containing following predicates:
unary predicates word nword wordW = W nwordW = ,
W
+
unary predicates = 6= =W
= {} 6= = ,

binary equality predicate = binary inequality predicate 6= obvious
interpretation,
w + , two binary predicates concw nconcw
W
concW
w = {(u, v) | v = uw} nconcw = {(u, v) | v 6= uw}.

readily checked W satisfies properties 1 2 admissibility (see Definition 2.4).
Moreover, W-satisfiability decidable:
Theorem 3.1 (Lutz, 2004). W-satisfiability PTime.
Thus, W admissible even low complexity. important since aim
demonstrate undecidability ALCK(W)-concept satisfiability due
presence keys, due high complexity W-satisfiability.
discuss reduction PCP. given instance (`1 , r1 ), . . . , (`k , rk )
translated ALCK(D)-concept CP key box KP defined Figure 1
P solution iff CP unsatisfiable w.r.t. KP . idea behind reduction
common model CP KP encodes potential solutions P (i.e., sequences i1 , . . . ,
integers ij 1 k) makes sure none fact solution.
Figure 1, f1 , . . . , fk denote abstract features g, `, r denote concrete features.
definition concept Step serves abbreviation confused
so-called TBoxes (see Section 4.2 definition TBoxes). Models CP KP ,
one displayed Figure 2, form infinite k-ary tree whose root
connected extra node x via role R. Intuitively, node tree represents
one partial solution i1 , . . . , , `-successor represents corresponding left concatenation
`i1 `in , r-successor corresponding right concatenation ri1 rin .
enforce existence infinite tree, employ key box KP : consider
example root nodes f1 -successor Figure 2let us call node y. Due Line 3
676

fiKeys, Nominals Concrete Domains

Step :=

u f .(A u g.= u `, r.6=)
u u (`, f `.conc u r, f r.conc


1ik





1ik



`i

ri )

CP := `.= u r.=
u R.(A u g.= u Step)
u Step
KP := {g keyfor Step}
Figure 1: ALCK(W) reduction concept CP key box KP .
R
=

conc`1

`

r

=

x

fk

concrk

f1


g.=

concr1 conc`k
`
f1



r


fk

f1



r

`


fk



Figure 2: example model CP KP .
CP Line 1 Step, (g.= )I . Due Line 2 CP , also
x (g.= )I x (Step)I , x extra node mentioned above. view
key box KP , implies either (i) x = (ii) StepI . easy see
(i) impossible since Line 2 CP Line 1 Step imply x AI (A)I .
Hence StepI and, Line 2 Step, appropriate fi -successors 1 n.
way, construction tree continued ad infinitum. second
line definition Step enforces `I (z) = `i1 `in rI (z) = ri1 rin z
fi1 fin -successor root node. Finally, concept `, r.6= Line 1 Step implies
`I (z) 6= rI (z) holds nodes z tree (except root), implies
potential solution solution.
Since size CP KP clearly polynomial k key box KP unary
key box, obtain following proposition.
Proposition 3.2. satisfiability ALCK(W)-concepts w.r.t. (non-Boolean) path-free
unary key boxes undecidable.
677

fiLutz, Areces, Horrocks, & Sattler

emphasize undecidability result obtained using simple concrete
domain, let us combine Theorem 3.1 Proposition 3.2.
Theorem 3.3. exists concrete domain D-satisfiability PTime
satisfiability ALCK(D)-concepts w.r.t. (non-Boolean) path-free unary key boxes
undecidable.
first sight, concrete domain W might look artificial one may question relevance lower bounds obtained using W. However, straightforward
encode words natural numbers define concatenation words rather simple operations natural numbers (Baader & Hanschke, 1992): word w 6=
alphabet cardinality # interpreted number written base # + 1
symbol 0 digit occur. Hence, use corresponding natural number (e.g., base 10) represent word w, number 0
represent empty word. concatenation two words v w expressed
vw = v (#+1)|w| +w, |w| denotes length word w. Moreover, exponentiation expressed multiple multiplications, multiplication multiple additions,
addition multiple incrementation: shown Section 5.6 (Lutz, 2004)
case ALC(D) extended TBoxes (c.f. Section 4.2) easily adapted
ALC(D) non-Boolean key boxes. observation gives rise following theorem:

N

Theorem 3.4. Let concrete domain , contains unary predicate =0 (=0 )D = {0}, binary equality inequality predicates, binary predicate
incr incrD {(n, x) | n x } = {(k, k + 1) | k }. satisfiability
ALCK(D)-concepts w.r.t. (non-Boolean) path-free unary key boxes undecidable.

N

N

3.2 Domino Problems Concrete Domains
section, introduce NExpTime-complete variant well-known, undecidable
domino problem (Berger, 1966; Knuth, 1968), define three concrete domains D1 ,
D2 , D3 . concrete domains used Sections 3.3 3.4 establish
lower bounds reasoning ALCK(D) Boolean key boxes, reasoning
ALCO(D).
general, domino problem given finite set tile types. Intuitively, tile
types size, type square shape colored edges. unlimited
number tiles type available. NExpTime-hard variant domino
problem use, task tile 2n+1 2n+1 -torus (i.e., 2n+1 2n+1 -rectangle
whose borders glued together) neighboring edges color.

N

Definition 3.3 (Domino System). domino system triple (T, H, V ), (
finite set tile types H, V represent horizontal vertical matching
conditions. Let domino system = a0 , . . . , an1 initial condition, i.e.
n-tuple tiles. mapping : {0, . . . , 2n+1 1} {0, . . . , 2n+1 1} solution
iff, x, < 2n+1 , following holds:
(x, y) = (x 2n+1 1, y) = t0 , (t, t0 ) H
(x, y) = (x, 2n+1 1) = t0 , (t, t0 ) V
678

fiKeys, Nominals Concrete Domains

(i, 0) = ai < n.
denotes addition modulo i.
follows results (Borger, Gradel, & Gurevich, 1997) variant
domino problem NExpTime-complete.
define concrete domain D1 used reduction NExpTimecomplete domino problem ALCK(D1 )-concept satisfiability w.r.t. Boolean key boxes.
Definition 3.4 (Concrete Domain D1 ). concrete domain D1 defined setting
D1 := {0, 1} D1 (smallest) set containing following predicates:
unary predicates >D1 (>D1 )D1 = D1 D1 (D1 )D1 = ;
unary predicates =0 =1 (=i )D1 = {i}, {0, 1}.
second concrete domain D2 used reduction NExpTime-complete
domino problem ALCK(D2 )-concept satisfiability w.r.t. Boolean unary key boxes.
reduction need store vectors bits single concrete domain elements.

N

Definition 3.5 (Concrete Domain D2 ). every n , function v : {0, . . . , n 1}
{0, 1} called bit vector dimension n. use BVn denote theSset bit vectors
dimension n. concrete domain D2 defined setting D2 := i>0 BVi D2
(smallest) set containing following predicates:
unary predicates >D2 (>D2 )D2 = D2 D2 (D2 )D2 = ;
every k,

N < k, unary predicates bit0ik bit1ik
(bitnik )D2 = {v D2 | v BVk v(i) = n},

unary predicates bit0ik bit1ik (bitnik )D2 = D2 \ (bitnik )D2 .
last concrete domain D3 used reduction NExpTime-complete domino
problem ALCO(D3 )-concept satisfiability. reduction, concrete domain D3
contains two kinds elements: firstly, elements D3 represent
whole 2n+1 2n+1 -torus, so-called domino arrays. Secondly, elements D3
represent positions torus. technical reasons discussed later, elements
vectors natural numbers rather bit vectors, following shall call
vectors. domino array function mapping pair vectors (of certain
length) natural number represents tile type.

N

N

Definition 3.6 (Concrete Domain D3 ). every k , function v : {0, . . . , k 1}
called vector dimension k. use VEk denote set vectors dimension k.
every k , function k : VEk VEk
called domino array dimension k.
use DAk denote setSof domino
arrays
dimension k. concrete domain D3

defined setting D3 := i>0 VEi i>0 DAi D3 (smallest) set containing
following predicates:

N

N

unary predicates >D3 (>D3 )D3 = D3 D3 (D3 )D3 = ;
679

fiLutz, Areces, Horrocks, & Sattler

every k,

N < k, unary predicates pos0ik pos1ik
(posnik )D3 = {v D3 | v VEk v(i) = n}

unary predicates pos0ik pos1ik (posnik )D3 = D3 \ (posnik )D3 ;
every k,

N, predicate tileik arity 3

(tileik )D3 = {(vx , vy , d) | vx , vy VEk , DAk , d(vx , vy ) = i}
predicate tileik arity 3 (tileik )D3 = (D3 )3 \ (tileik )D3 .
reason using vectors natural numbers rather bit vectors definition
D3 want D3 -satisfiability low complexity, preferably PTime: consider
D3 -conjunction
pos002 (x) pos002 (y) pos002 (z)
tile72 (x, v, d) tile82 (y, v, d) tile92 (z, v, d).
use bit vectors rather vectors natural numbers, upper line enforces
least two three variables x, y, z must take value. Since
value v fixed, lower line makes conjunction unsatisfiable: tries assign
three different values 7, 8, 9 two different positions domino array. seems
unlikely kind inconsistency detected polynomial time. problem
circumvented using vectors natural numbers definition D3 (but enforcing
bit vectors reduction): case, conjunction clearly
satisfiable.
Proposition 3.5. {1, 2, 3}, concrete domain Di admissible satisfiability Di -conjunctions PTime.
D1 , trivial. D2 , proof found Appendix A. D3 , proof
found (Lutz, Areces, Horrocks, & Sattler, 2002).
3.3 NExpTime-hardness ALCK(D) Boolean Key Boxes
section, prove two NExpTime-lower bounds ALCK(D)-concept satisfiability
w.r.t. Boolean key boxes reducing NExpTime-complete domino problem introduced
previous section. first reduction uses simple concrete domain D1 ,
depends composite key assertions. second reduction uses slightly complex
concrete domain D2 , needs unary key assertions. see, two reductions
yield different, incomparable results.
first reduce NExpTime-complete domino problem ALCK(D1 )-concept satisfiability w.r.t. Boolean composite key boxes. domino system = (T, H, V ) initial
condition = a0 , . . . , an1 translated ALCK(D1 )-concept CD,a displayed
Figure 3. Names TreeX TreeY used abbreviations only. use Ri .C
abbreviation n-fold nesting R. R.C. names xposi yposi used
figure denote concrete features. definition Init concept, n , biti (n)

N

680

fiKeys, Nominals Concrete Domains

TreeX := R.X0 u R.X0 u

u R .(DistX


u R.Xi u R.Xi )

i1

i=1..n

TreeY := DistXn u R.Y0 u R.Y0 u

u R .(DistY u DistX u R.Y u R.Y )
DistX := u ((X R.X ) u (X R.X ))
DistY := u ((Y R.Y ) u (Y R.Y ))
TransXPos := u (X xpos . = ) u (X xpos . = )
TransYPos := u (Y ypos . = ) u (Y ypos . = )


i1

i=1..n

k

k



i=0..k

i=0..n











i=0..k



i=0..n

n













1



1



0





0



Succs := Rx .(TransXPos u TransYPos) u Ry .(TransXPos u TransYPos)

XSuccOk :=
(Yi Rx .Yi ) u (Yi Rx .Yi )
i=0..n


Xj (Xk Rx .Xk ) u (Xk Rx .Xk )

u
u u
u X (X R .X ) u (X R .X )

YSuccOk := u (X R .X ) u (X R .X )
u u (Y R .Y ) u (Y R .Y )
u (Y R .Y ) u (Y R .Y )
Label := u u (D u )
CheckMatch := (D u R .D ) u (D u R .D )

u X u u X u u
Init := u
k=0..n

j=0..k

k=0..n

j=0..k


i=0..n

j=0..k

k=0..n

j=0..k



(i,j)H

i=0..n1



j



k



j

j=0..n,bitj (i)=0

x

k





k

x

k



j

i,jT,i6=j



x

k



k=0..n



j

k



k

k



k

k



k

k

j

(i,j)V
j





j

j=0..n,bitj (i)=1

j

j=0..n

j



ai

CD,a := TreeX u Rn+1 .TreeY
u R2(n+1) .(TransXPos u TransYPos u Succs u XSuccOk u YSuccOk)
u R2(n+1) .(Label u CheckMatch u Init)
Figure 3: ALCK(D1 ) reduction concept CD,a .
supposed denote ith bit binary representation n. claim CD,a
satisfiable w.r.t. key box
{(xpos0 , . . . , xposn , ypos0 , . . . , yposn keyfor >)}
iff exists solution a. substantiate claim, let us go
reduction explain various parts concept CD,a . first step towards under681

fiLutz, Areces, Horrocks, & Sattler

standing structure models CD,a (which key understanding reduction
itself) note purpose first line CD,a enforce tree structure
depth 2(n + 1), whose leaves correspond positions 2n+1 2n+1 -torus.
precisely, TreeX concept guarantees that, every model CD,a , exists binary
tree depth n + 1. Moreover, DistXk concepts (there exists one k {0, . . . , n})
ensure leaves tree binarily numbered (from 0 2n+1 1) concept
names X0 , . . . , Xn . precisely, domain object , set

1 XiI
n

xpsn(d) = i=0 (d) 2 (d) =
0 otherwise.
TreeX DistX concepts ensure exist nodes d0 , . . . , d2n+1 1 level n + 1
tree xpsn(di ) = i. Intuitively, numbering represents horizontal
positions 2n+1 2n+1 -torus. vertical positions coded similar way
Y0 , . . . , Yn concept names. specifically, concepts TreeY, DistX, DistY ensure
every di (i 2n+1 1) root another tree, (i) every node
X0 , . . . , Xn -configuration root node, (ii) leaves numbered binarily
using concept names Y0 , . . . , Yn (note TreeY concept appears CD,a inside
Rn+1 value restriction). Define

1 YiI
n

ypsn(d) = i=0 (d) 2 (d) =
0 otherwise.
set leaf nodes trees enforced TreeY concept, exists,
i, j < 2n+1 , object4 ei,j xpsn(ei,j ) = ypsn(ei,j ) = j, i.e., ei,j
represents position (i, j) 2n+1 2n+1 -torus.
next step translate individual bits numbering ei,j -objects,
represented concept names, concrete domain values.
done TransXPos TransYPos concepts ensure that, ` n,
xposI` (ei,j ) = 0 ei,j X` , xposI` (ei,j ) = 1 ei,j X` , similarly ypos` Y` .
Since model key box
{(xpos0 , . . . , xposn , ypos0 , . . . , yposn keyfor >)},
grid positions uniquely represented domain elements (TransXPos u TransYPos)I ,
i.e., d, e (TransXPos u TransYPos)I xpsn(d) = xpsn(e) ypsn(d) = yxpsn(e),
= e. fact used concepts Succs, XSuccOk, YSuccOk enforce that,
two roles Rx Ry i, j n, following holds:
RxI ({ei,j } ) = {(ei,j , e(i2n+1 1),j }
RyI ({ei,j } ) = {(ei,j , ei,(j2n+1 1) }.

()

Succs concept ensures that, ei,j , exists Rx -successor Ry successor, (TransXPos u TransYPos)I . Let Rx -successor ei,j .
XSuccOk concept ensures xpsn(d) = 2n+1 1 ypsn(d) = j.
4. matter one object.

682

fiKeys, Nominals Concrete Domains

explain this, let us note that, since ei,j (TransXPos u TransYPos)I
grid positions uniquely represented elements (TransXPos u TransYPos)I ,
implies = e(i2n+1 1),j shows upper line () indeed hold.
Let us consider XSuccOk concept detail. essentially
DL-formulation well-known propositional formula
n k1
n k1
^
^
^
_
(
xj = 1) (xk = 1 x0k = 0)
(
xj = 0) (xk = x0k )
k=0 j=0

k=0 j=0

encodes incrementation modulo 2n+1 , i.e., number (binarily) encoded
propositional variables x0 , . . . , xn t0 number encoded propositional
variables x00 , . . . , x0n , t0 = + 1 modulo 2n+1 (see Borger et al., 1997). Taking
account Rx quantifiers XSuccOk, readily checked concept
desired effect: ensure that, every Rx -successor ei,j , xpsn(d) =
xpsn(e(i2n+1 1),j ) = 2n+1 1. explanation YSuccOk enforces lower
line () analogous XSuccOk case.
remains ensure every grid position labeled precisely one tile
initial condition well horizontal vertical matching conditions satisfied.
tiles represented concept names Di (where set tiles )
described tasks accomplished standard way concepts Label, Init,
CheckMatch.
worth noting reduction concept path-free key box simple,
i.e., path-free Boolean. Path-freeness concepts often used tame complexity
description logics concrete domains, although largely sacrifices expressive
power (Lutz, 2003; Baader, Lutz, Sturm, & Wolter, 2002b; Haarslev, Moller, & Wessel, 2001;
Horrocks & Sattler, 2001). example, ALC(D) augmented general TBoxes,
reasoning arbitrary concepts undecidable reasoning path-free concepts
ExpTime-complete admissible D-satisfiability ExpTime (Lutz, 2002a).
taming approach work presence key boxes since,
seen, satisfiability ALC(D)-concepts w.r.t. key boxes (under natural assumptions)
NExpTime-hard, even concept key box path-free.
Since size CD,a used key box clearly polynomial n, obtain
following proposition.
Proposition 3.6. satisfiability path-free ALCK(D1 )-concepts w.r.t. simple key boxes
NExpTime-hard.
shown (non path-free) ALC(D)-concept satisfiability PSpace-complete
D-satisfiability PSpace (Lutz, 2002b). Hence, follows Proposition 3.5
ALC(D1 )-concept satisfiability PSpace-complete. Thus, rather dramatic increase complexity key boxes added ALC(D1 ). stress increase due
key boxes complexity D1 -satisfiability, reformulate
Proposition 3.6:
Theorem 3.7. exists concrete domain D-satisfiability PTime
satisfiability path-free ALCK(D)-concepts w.r.t. simple key boxes NExpTime-hard.
683

fiLutz, Areces, Horrocks, & Sattler

Succs2 := Rx .TransPos u Ry .TransPos
TransPos :=

u
u

i=0..n
i=0..n


(Xi bv.bit1i2(n+1) ) u Xi bv.bit0i2(n+1) ) u

n+i+1
n+i+1
(Yi bv.bit12(n+1)
) u Yi bv.bit02(n+1)
)

CD,a := TreeX u Rn+1 .TreeY
u R2(n+1) .(TransPos u Succs2 u XSuccOk u YSuccOk)
u R2(n+1) .(Label u CheckMatch u Init)
Figure 4: ALCK(D2 ) reduction concept CD,a .
Although, due low expressivity, concrete domain D1 natural
knowledge representation, fragment many concrete domains
proposed literature (Baader & Hanschke, 1992; Haarslev & Moller, 2001; Lutz, 2003,
2002b). Indeed, presented reduction strategy adapted several standard
concrete domains. Let us formulate (very weak) condition concrete domain must
satisfy order presented reduction strategy applicable.
Theorem 3.8. Let concrete domain. exist a, b 6= b P1 , P2
P1D = {a} P2D = {b}, satisfiability path-free ALCK(D)-concepts
w.r.t. simple key boxes NExpTime-hard.
present second NExpTime-hardness result ALCK(D)-concept satisfiability.
time, reduce NExpTime-complete domino problem satisfiability pathfree ALCK(D2 )-concepts w.r.t. simple unary key boxes. reduction similar
previous one discuss differences.
first reduction, represented individual bits grid positions individual
concrete features xposi yposi used composite key box ensure point
torus represented one element. second reduction, use single
concrete feature bv represent entire position (i, j) torus using bit vector
concrete domain D2 . allows us enforce mentioned uniqueness
representations using unary key box.
modified reduction concept CD,a found Figure 4, concepts
TreeX, TreeY, DistXk , DistYk , XSuccOk, YSuccOk, Label, CheckMatch, Init defined
Figure 3. translation position torus encoded X0 , . . . , Xn , Y0 , . . . , Yn
bit vector done TransPos concept straightforward manner. Given
said first reduction, hard see CD,a satisfiable w.r.t. key
box {(bv keyfor >)} iff exists solution a. thus obtain following
proposition.
Proposition 3.9. satisfiability path-free ALCK(D2 )-concepts w.r.t. simple unary
key boxes NExpTime-hard.
Again, relate NExpTime lower bound complexity D2 -satisfiability,
determined Proposition 3.5.
684

fiKeys, Nominals Concrete Domains

Theorem 3.10. exists concrete domain D-satisfiability PTime
satisfiability path-free ALCK(D)-concepts w.r.t. simple unary key boxes NExpTime-hard.
Since elements D2 bit vectors, concrete domain D2 cannot considered
natural choice many application areas. But, reduction, D2 replaced
several natural concrete domains.
central observation use bit vectors injectively translate sequences
bits values concrete domain, i.e., translate sequences 2(n + 1) bits
(represented concept names X0 , . . . , Xn Y0 , . . . , Yn ) elements D2
that, distinct sequences, results translation also distinct. Due
restricted use bit vectors, several ways replace natural numbers.
example, replace TransPos following concept TransPos0 ensures
that, TransPos0I , sI2n+1 (d) = xpsn(d) + 2n+1 ypsn(d):

u


TransPos0 := zero.=0 u
ti .=2i u (X0 s0 .=0 ) u (X0 s0 .=1 ) u
i=1...2n+1


Xi (si1 , zero, si ).+ u Xi (si1 , ti , si ).+ u
i=1..n



Yi(n+1) (si1 , zero, si ).+ u (Yi(n+1) (si1 , ti , si ).+

u
u

i=n+1..2n+1

N

zero, si , ti concrete features, =k (with k ) denotes unary predicate
obvious extension, + denotes ternary addition predicate that, intuitively,
first two arguments addends third one sum.

easy check that, whenever two objects d, e TransPos0 agree
interpretation X0 , . . . , Xn , Y0 , . . . , Yn , sI2n+1 (d) 6= sI2n+1 (e), thus key
box {(s2n+1 keyfor >)} used reduction. size TransPos0 obviously
polynomial n numbers k appearing =k predicates coded binary. thus
obtain following theorem:
Theorem 3.11. Let concrete domain
1.

N ,
N

2. contains, k , predicate =k (=k )D = {k} size (the
representation ) =k logarithmic k,
3. contains predicate + (+)D {(k1 , k2 , x) | k1 , k2
{(k1 , k2 , k1 + k2 ) | k1 , k2 }.

N

N x

} =

satisfiability path-free ALCK(D)-concepts w.r.t. simple unary key boxes
NExpTime-hard.
example, theorem yields NExpTime-lower bounds ALCK(D) instantiated
concrete domains proposed (Baader & Hanschke, 1992; Haarslev & Moller, 2001; Lutz,
2003, 2002b). alternative addition predicate use multiplication injectively
685

fiLutz, Areces, Horrocks, & Sattler

translate sequences bits natural numbers. precisely, let p1 , . . . , p2n+1
first 2n + 1 prime numbers define another version TransPos follows:
TransPos00 := one.=1 u

u
u

i=1..n

u


ti .=pi u (X0 s0 .=0 ) u (X0 s0 .=1 ) u
i=1...2n+1



Xi (si1 , one, si ). u Xi (si1 , ti , si ). u



Yi(n+1) (si1 , one, si ). u Yi(n+1) (si1 , ti , si ).

i=n+1..2n+1

ternary multiplication predicate.
Since factorization natural numbers prime numbers unique,
use key box {(s2n+1 keyfor >)} reduction. Moreover, well-known
kth prime polynomial k (Graham, Knuth, & Patashnik, 1990), thus size
concept TransPos00 polynomial n even numbers k =k predicates coded
unarily. thus obtain another theorem concerning quite natural concrete domains:
Theorem 3.12. Let concrete domain
1.

N ,

N, predicate =k (=k )D = {k},
3. contains predicate ()D {(k1 , k2 , x) | k1 , k2 N x
{(k1 , k2 , k1 k2 ) | k1 , k2 N}.
2. contains, k

} =

satisfiability path-free ALCK(D)-concepts w.r.t. simple unary key boxes
NExpTime-hard.
3.4 NExpTime-hardness ALCO(D)
already pointed Section 1, relationship key boxes nominals
rather close: latter simulated former concrete domain provides
predicates used uniquely describe elements . example, ALCK(D1 )
concept g.=0 behaves nominal use key assertion (g keyfor >).
even define n nominals using n single concrete features unary-key assertions.
logics ALCK(D2 ) ALCK(D3 ), single concrete feature unary key assertions
sufficient simulate arbitrary number nominals: example, ALCK(D2 )
concept C = g.bit002 u g.bit112 uniquely describes bit vector (0, 1) BV2 D2 , i.e.,
C implies g (a) = (0, 1). Obviously, bit vector (of length!)
described similar way.
illustrates that, non-trivial concrete domains D, logic ALCK(D)
(at least) expressive ALCO(D). Although converse hold, expressive
power ALCO(D) still sufficient prove NExpTime-hardness concept satisfiability,
provided suitable concrete domain used. Since ALCO concept satisfiability
PSpace-complete (Areces, Blackburn, & Marx, 1999), yet another example DL
even seemingly harmless extension concrete domains dramatic effect
computational complexity (Lutz, 2003).
686

fiKeys, Nominals Concrete Domains

Nominal := f.N
XSucc :=
YSucc :=

u u X (X X ) u u X (X X )
u u (Y ) u u (Y )

k=0..n

j=0..k

k=0..n

j=0..k

j

0
k

k

j

0
k

k

k=0..n

k=0..n

j

j=0..k

j=0..k

j

i=0..n


n+1

0



n+1

0



n+1

0



n+1

i,jV

u

i=0..n1


n+1



0


i,jH

Init2 :=


n+1




n+1



i=0..n

i=0..n


n+1



i=0..n



u

0
k

k

u (X bvx.pos1 ) u (X bvx.pos0 )

TransYPos := u (Y bvy.pos1
) u (Y bvy.pos0
)

TransXSucc := u (X bvxs.pos1
) u (X bvxs.pos0
)

TransYSucc := u (Y bvys.pos1
) u (Y bvys.pos0
)
CheckHMatch := ((bvx, bvy, f darr).tile
u (bvxs, bvy, f darr).tile
CheckVMatch := ((bvx, bvy, f darr).tile
u (bvx, bvys, f darr).tile
TransXPos :=

0
k

k


n+1

j
n+1 )


n+1

j
n+1 )

Xj u

u

u

Xj u
Yj
j=0..n
j=0..n,bitj (i)=0
j=0..n,bitj (i)=1


(bvx, bvy, f darr).tilean+1



CD,a := TreeX u Rn+1 .TreeY u R2(n+1) .Nominal u
R2(n+1) .(TransXPos u TransYPos u
XSucc u YSucc u TransXSucc u TransYSucc u
Init2 u CheckHMatch u CheckVMatch)
Figure 5: ALCO(D3 ) reduction concept CD,a .

section, reduce NExpTime-complete domino-problem ALCO(D3 )concept satisfiability. Again, let = (T, H, V ) domino system = a0 , . . . , an1
initial condition. modified reduction concept CD,a defined Figure 5, bvx,
bvy, bvxs, bvys, darr denote concrete features, N denotes nominal, concepts
TreeX, TreeY, DistXk , DistYk defined Figure 3. previous reductions,
give detailed explanation reduction strategy show CD,a satisfiable
iff exists solution a. Formal details easily worked
interested reader.
Let model CD,a . explain structure I, convenient start
first line CD,a . previous reductions, TreeX TreeY concepts
used ensure contains tree-shaped substructure depth n + 1 whose leaf
nodes roots additional trees depth n + 1 set leafs
687

fiLutz, Areces, Horrocks, & Sattler

TreeX

TreeY

...

TreeY

...

...

f

f

TreeY

...f

N
darr

Figure 6: structure models CD,a .
latter trees correspond positions 2n+1 2n+1 -torus, i.e., position,
leaf node representing it. torus positions binarily encoded concept
names X0 , . . . , Xn Y0 , . . . , Yn use ei,j refer leaf xpsn(ei,j ) =
ypsn(ei,j ) = j (see Section 3.3).
previous reductions, numbers coded X0 , . . . , Xn Y0 , . . . , Yn
translated concrete domain values, done TransXPos TransYPos
concepts. Note that, contrast ALCK(D2 )-reduction, x-position yposition stored bit vector, rather two distinct ones bvx
bvy. Also contrast previous reduction, actual tiling torus
represented leaf nodes ei,j , rather domino array: last conjunct
first line CD,a ensures every leaf ei,j connected via abstract feature f
(unique) element w N .
domain element w associated domino array via concrete feature darr (as
shall see later, guaranteed CheckHMatch CheckVMatch concepts).
domino array represents tiling 2n+1 2n+1 -torus. Summing up, structure
roughly shown Figure 6.
Since tiling stored domino array, need explain purpose leaf
nodes ei,j : nodes used enforce initial condition horizontal
vertical matching condition. Let us discuss horizontal matching condition (the vertical
matching condition enforced analogously): XSucc concept DL reformulation
propositional logic formula incrementation modulo 2n+1 ensures that,
ei,j , concept names X00 , . . . , Xn0 encode number 2n+1 1, i.e., horizontal
position ei,j horizontal neighbor. addition storage horizontal vertical
position ei,j bvx(ei,j ) bvy(ei,j ), also store horizontal position i2n+1 1 ei,j
horizontal successor bvxs(ei,j ). Finally, CheckHMatch verifies tiles positions
688

fiKeys, Nominals Concrete Domains

(i, j) (i 2n+1 1, j), stored domino array, compatible
horizontal matching condition.
Note CheckHMatch also ensures domain element w (with {w} = N )
domino array attached via concrete feature darr that, position (i, j),
(unique!) tile stored domino array set . initial condition ensured
via Init2 concept similar way. (again) use bitj (i) denote jth bit
binary encoding natural number i.
Using considerations, correctness reduction readily checked.
Moreover, size CD,a polynomial n. Note CD,a path-free:
paths length two appear concepts CheckHMatch, CheckVMatch, Init2. Summing
up, reduction described yields following result:
Proposition 3.13. satisfiability ALCO(D3 )-concepts NExpTime-hard.
Again, relate NExpTime lower bound complexity D3 -satisfiability,
determined Proposition 3.5.
Theorem 3.14. exists concrete domain D-satisfiability PTime
satisfiability ALCO(D)-concepts NExpTime-hard.
Note reduction uses single nominal N . dramatic increase complexity since shown satisfiability ALC(D)-concepts (i.e., without nominals
key boxes) PSpace-complete provided admissible D-satisfiability
PSpace (Lutz, 2002b).
previous sections, note D3 replaced natural concrete
domains NExpTime-hardness proof presented. idea represent whole
domino array single natural number use arithmetic operations access
individual positions: natural number k viewed domino array partitioning
binary representation 2n+1 2n+1 = 22(n+1) sections length dlog(#T )e,
#T denotes cardinality set tile types . section describes tile
single position torus. sections accessed using integer division
reminder operations: k natural number representing torus, tile
posisition computed
(k div 2idlog(#T )e ) mod 2dlog(#T )e + 1.
Thus, introduce ternary predicates div integer division mod computing
remainder division, binary predicate 2x expressing exponentiation basis 2.
modify reduction follows: replace TransXPos TransYPos
TransPos0 concept Section 3.3 translate two numbers encoded X1 , . . . , Xn
Y1 , . . . , Yn single natural number stored concrete feature s2n+1 .
devise new concept Tile[i] (for ) enforcing position identified
feature s2n+1 labeled tile i:
Tile[i] := r.=dlog(#T )e u s2n+1 , r, r0 . u r0 , r00 .2x u one.=1 u r, one, t.+ u t, t0 .2x
u f torus, r00 , u.div u u, t00 , tile.mod u tile.=i .
689

fiLutz, Areces, Horrocks, & Sattler

Here, r, r0 , r00 , t, t0 , u, one, torus, tile concrete features. torus feature counterpart darr feature original reduction, i.e., stores natural number
represents tiling array. use Tile[i] concept obvious way inside
CheckHMatch, CheckVMatch, Init2 concepts. size resulting reduction concept
polynomial n numbers k appearing =k predicates coded binary.
thus obtain following theorem:
Theorem 3.15. Let concrete domain
1.

N ,

2. contains predicates predicate =k (for k
following extensions
(2x )D

{(k, x) | k
(+)D {(k1 , k2 , x) | k1 , k2
()D {(k1 , k2 , x) | k1 , k2
(div)D {(k1 , k2 , x) | k1 , k2
(mod)D {(k1 , k2 , x) | k1 , k2







N
N
N
N
N

(=k )D
x }
x }
x }
x }
x }

=
=
=
=
=
=

N), 2x, +, , div, mod

{k}
{(k, 2k ) | k }
{(k1 , k2 , k1 + k2 ) | k1 , k2 }
{(k1 , k2 , k1 k2 ) | k1 , k2 }
{(k1 , k2 , k1 div k2 ) | k1 , k2 }
{(k1 , k2 , k1 mod k2 ) | k1 , k2 }

N

N
N
N
N

satisfiability ALCO(D)-concepts NExpTime-hard.

4. Reasoning Procedures
section devoted developing reasoning procedures DLs concrete domains,
nominals, keys. start devising tableau algorithm decides satisfiability
ALCOK(D)-concepts w.r.t. Boolean key boxes. algorithm yields NExpTime upper
complexity bound matching lower bounds established Section 3.3.
consider rather powerful description logic SHOQK(D). DL,
extension SHOQ(D) (Horrocks & Sattler, 2001; Pan & Horrocks, 2002), provides
wealth expressive means transitive roles, role hierarchies, nominals, qualifying
number restrictions. Moreover, SHOQK(D) equipped restricted variant
concrete domain constructor key boxes. develop tableau algorithm
deciding satisfiability SHOQK(D)-concepts w.r.t. path-free key boxes. Due
restrictedness SHOQK(D)s concrete domain constructor, even admit general
rather Boolean key boxes. Again, algorithm yields tight NExpTime upper
complexity bound.
4.1 Tableau Algorithm ALCOK(D) Boolean Key Boxes
Tableau algorithms decide satisfiability input concept (in case w.r.t. input
key box) attempting construct model it. precisely, tableau algorithm
starts initial data structure induced input concept repeatedly applies so-called completion rules it. rule application thought attempting
construct model input concept. Finally, either algorithm find obvious contradiction encounter situation contradiction-free
690

fiKeys, Nominals Concrete Domains

completion rules applicable. former case, input concept unsatisfiable,
satisfiable latter.
devising tableau algorithm description logic concrete domains
without committing particular concrete domain, commonly assumed concrete domain admissible, implies decidability satisfiability D-conjunctions.
presence keys, however, enough: D-conjunction satisfiable, also
want know variables take values arbitrary fixed solution.
example, consider concrete domain N = ( , {<n | n }) N-conjunction

N

N

c = <2 (v1 ) <2 (v2 ) <2 (v3 ).
Obviously, one solution c satisfies (v1 ) = (v2 ), another satisfies (v1 ) = (v3 ),
on. tableau algorithm uses identity information passed concrete domain
reasoner since, presence key boxes, impact structure
constructed model. example, information reveals unsatisfiability
R.A u R.(A u B) u R.(A u B) u R.g.<2 w.r.t. (g keyfor >).
formalize requirement, strengthen notion admissibility key-admissibility.
Since tableau algorithm developed section non-deterministic, formulate keyadmissibility non-deterministic way.
Definition 4.1 (Key-admissible). concrete domain key-admissible iff satisfies
following properties:
1. contains name >D ;
2. closed negation;
3. exists algorithm takes input D-conjunction c, returns clash c
unsatisfiable, otherwise non-deterministically outputs equivalence relation
set variables V used c exists solution c
following property: v, v 0 V
(v) = (v 0 ) iff v v 0 .
algorithm showing behaviour described item 3 called D-tester,
equivalence relations called concrete equivalences. say extended Dsatisfiability NP exists D-tester running polynomial time.
Please note key-admissibility less esoteric might seem: concrete domain
admissible provides equality predicate also key-admissible. Due
admissibility, presence equality predicate implies inequality predicate
also available. thus construct D-tester algorithm D-satisfiability:
presented predicate conjunction c, simply guess equivalence relation
set variables
used c. VThen decide (non-extended) satisfiability
V
conjunction c vv0 =(v, v 0 ) v6v0 6=(v, v 0 ), return clash unsatisfiable
otherwise. rather weak condition equality predicate present
691

fiLutz, Areces, Horrocks, & Sattler

(C u D)
(R.C)

C (C D)
R.C
(R.C)

(u1 , . . . , un .P )
(g)

C u
R.C

C

C

u1 , . . . , un .P u1 un
g.>D

Figure 7: NNF rewrite rules.
satisfied almost concrete domains proposed literature (see, e.g. (Lutz, 2003;
Baader & Hanschke, 1991b; Kamp & Wache, 1996; Haarslev, Lutz, & Moller, 1998; Baader
& Sattler, 1998)).
Throughout chapter, assume concrete domain equipped
equality predicate. assumption w.l.o.g. since D-conjunction using equality
translated equivalent one without equality identifying variables according
stated equalities. assumption must confused discussed
previous paragraph: even concrete domain admissible set predicates
thus closed negation, assumption imply presence inequality
predicate.
need prerequisites start presentation tableau
algorithm: concept negation normal form (NNF) negation occurs front
concept names nominals. easily seen that, concrete domain admissible,
every ALCOK(D)-concept converted equivalent one NNF exhaustively applying rewrite rules displayed Figure 7. use C denote result
converting C NNF. key box NNF concepts occurring key assertions
NNF. follows, generally assume input concepts key boxes NNF.
Let C ALCOK(D)-concept K key box. use sub(C) denote set
subconcepts C (including C itself) con(K) denote set concepts appearing

right-hand side key assertions K. set concepts , sub() denotes
set C sub(C). Moreover, write cl(C, K) abbreviation set
sub(C) sub(con(K)) {D
| sub(con(K))}.
start presentation tableau algorithm introducing underlying data
structure.
Definition 4.2 (Completion System). Let Oa Oc disjoint countably infinite
sets abstract concrete nodes. completion tree ALCOK(D)-concept C
key box K finite, labeled tree = (Va , Vc , E, L) nodes Va Vc Va Oa ,
Vc Oc , nodes Vc leaves. tree labeled follows:
node Va labeled subset L(a) cl(C, K);
edge (a, b) E a, b Va labeled role name L(a, b) occurring C
K;
edge (a, x) E Va x Vc labeled concrete feature L(a, x)
occurring C K.
692

fiKeys, Nominals Concrete Domains

Va , use levT (a) denote depth occurs (starting
root node depth 0). completion system ALCOK(D)-concept C key box
K tuple (T, P, , ),
= (Va , Vc , E, L) completion tree C K,
P function mapping P arity n C subset Vcn ,
linear ordering Va levT (a) levT (b) implies b,
equivalence relation Vc .
Let (Va , Vc , E, L) completion tree. node b Va R-successor node Va
(a, b) E L(a, b) = R, node x Vc g-successor (a, x) E
L(a, x) = g. path u, notion u-successor defined obvious way.
Intuitively, relation records equalities concrete nodes found
(non-deterministic) model construction process. recording necessary since equalities concrete nodes induce equalities abstract nodes which, turn,
imply equalities concrete nodes. seen following example: assume completion tree contains, {1, 2}, abstract node ai
concrete g-successor xi concrete g 0 -successor yi . assume key box contains (g keyfor >), D-tester returns x1 x2 . consequence, a1 a2
represent element thus functionality g 0 implies also y1 y2 represent
(concrete) element. deal effects, define equivalence relation
abstract nodes second equivalence relation c concrete nodes.
Definition 4.3 (a c Relations). Let = (T, P, , ) completion system
concept C key box K = (Va , Vc , E, L), let equivalence relation
Va . R NR , node b Va R/-neighbor node Va exists
node c Va c b R-successor c. Similarly, g NcF ,
node x Vc g/-neighbor exists node c Va c x
g-successor c. paths u, notion u/-neighbor defined obvious way.
define sequence equivalence relations 0a 1a Va follows:
0a = {(a, a) Va2 }
{(a, b) Va2 | N N L(a) L(b)}
i+1
= ia

{(a, b) Va2 | c Va f NaF
b f /ia -neighbors c}
{(a, b) Va2 | (u1 , . . . , un keyfor C) K,
ui /ia -neighbors xi 1 n,
ui /ia -neighbors yi b 1 n
C L(a) L(b) xi yi 1 n}.
Finally, set =




i0 .

define

c = {(x, y) Vc2 | Va g NcF
x g/a -neighbors a}.
693

fiLutz, Areces, Horrocks, & Sattler

definition reflects mentioned tight coupling concrete abstract equalities: D-tester finds (or guesses) two concrete nodes equal,
tableau algorithm may use deduce (via computation c ) even
equalities concrete nodes.
Let key-admissible concrete domain. decide satisfiability ALCOK(D)concept C0 w.r.t. Boolean key box K (both NNF), tableau algorithm started
initial completion tree
TC0 = ({a0 }, , , {a0 7 {C0 }})
initial completion system
SC0 = (TC0 , P , , ),
P maps P occurring C0 . introduce operation
used completion rules add new nodes completion trees.
Definition 4.4 (+ Operation). abstract concrete node called fresh completion tree appear T. Let = (T, P, , ) completion system
= (Va , Vc , E, L). use following notions:
Let Va , b Oa fresh T, R NR . write +aRb denote completion
system 0 obtained adding b Va (a, b) E setting
L(a, b) = R L(b) = . Moreover, b inserted b c implies
levT (b) levT (c).
Let Va , x Oc fresh g NcF . write +agx denote completion
system 0 obtained adding x Vc (a, x) E setting
L(a, x) = g.
nesting + operation, omit brackets, writing, example, + aR1 b + bR2 c
(S + aR1 b) + bR2 c. Let u = f1 fn g path. Va x Oc fresh T,
use + aux denote completion system obtained taking distinct
nodes b1 , . . . , bn Oa fresh setting
+ aux := + af1 b1 + + bn1 fn bn + bn gx.
Strictly speaking, + aRb operation non-deterministic since specify
precisely node b inserted . However, since dont care non-determinism,
view + operation deterministic.
completion rules found Figure 8. Note Rt Rch rules
non-deterministic, i.e., one possible outcome (this true dont know
non-determinism). remarks completion rules order: upper
five rules well-known existing tableau algorithms ALC(D)-concept satisfiability
(see, e.g., Lutz, 2002a). use R/ -neighbors u/ -neighbors rules
R, R, Rc deserves comment. Take example R: intuitively, b
two abstract nodes b completion tree, b describe
domain element constructed model (and similarly c relation concrete
694

fiKeys, Nominals Concrete Domains

Ru

C1 u C2 L(a) {C1 , C2 } 6 L(a)
L(a) := L(a) {C1 , C2 }

Rt

C1 C2 L(a) {C1 , C2 } L(a) =
L(a) := L(a) {C} C {C1 , C2 }

R

R.C L(a) R/a -neighbor b C L(b),
set := + aRb fresh b Oa L(b) := {C}

R

R.C L(a), b R/a -neighbor a, C
/ L(b)
set L(b) := L(b) {C}

Rc

u1 , . . . , un .P L(a) exist x1 , . . . , xn Vc
xi ui /a -neighbor 1 n (x1 , . . . , xn ) P(P )
set := (S + au1 x1 + + aun xn ) x1 , . . . , xn Oc fresh
P(P ) := P(P ) {(x1 , . . . , xn )}

Rch

(u1 , . . . , un keyfor C) K exist x1 , . . . , xn Vc
xi ui /a -neighbor 1 n {C, C}

L(a) =
set L(a) := L(a) {D} {C, C}


Rp

L(b) 6 L(a) Va minimal w.r.t. b
set L(a) := L(a) L(b)

Figure 8: Completion rules ALCOK(D).
nodes). Thus b c R-successor a, c also R-successor
b. However, since want completion tree tree, make latter
successorship explicit. compensate this, R rule talks R/a -neighbors
rather R-successors.
lower two rules necessary dealing key boxes. Rch rule
so-called choose rule (Hollunder & Baader, 1991; Horrocks et al., 2000): intuitively,
guesses whether abstract node satisfies C exists key assertion
(u1 , . . . , un keyfor C) K neighbors paths ui .
necessary since possibilities may ramifications: satisfies C, must
taken account construction relation ; satisfy C,
must deal consequences satisfying C
(e.g. case C >).
Rp rule deals equalities abstract nodes recorded relation:
since b means b describe node constructed model,
node labels identical. suffices, however, choose one representative
equivalence class make sure representatives node label contains
labels -equivalent nodes. representative, use node minimal
w.r.t. ordering , introduced solely reason. Rp rule
appropriate copying node labels.
Let us formalize means completion system contain contradiction.
Definition 4.5 (Clash). Let = (T, P, , ) completion system concept C
key box K = (Va , Vc , , ). say completion system concrete
695

fiLutz, Areces, Horrocks, & Sattler

define procedure sat(S)

contains clash
return unsatisfiable
:= test(S )
compute
compute c
=
6 c
contains clash
return unsatisfiable
complete
return satisfiable
0
:= application completion rule
return sat(S 0 )
Figure 9: ALCOK(D) tableau algorithm.
domain satisfiable iff conjunction
^
=

^

P (x1 , . . . , xn )

P used C (x1 ,...,xn )P(P )

^

=(x, y)

xc

satisfiable. said contain clash iff
1. Va NC {A, A} L(a),
2. Va x Vc g L(a) x g/a -neighbor a,
3. concrete domain satisfiable.
contain clash, called clash-free. called complete iff completion
rule applicable S.
tableau algorithm described Figure 9 pseudo-code notation. figure, test
calls D-tester specified Definition 4.1. Let us say words loop.
obviously exist close relationships relations c predicate
conjunction :
c (note c depend thus recomputed
step loop);
definition D-tester, result test(S ) yields relation containing c
(and thus also ).
Using facts, one may check that, step loop, new tuples added
relation, none deleted (see proof Lemma B.2 appendix).
loop needed (i) defined using , (ii), c defined using ,
696

fiKeys, Nominals Concrete Domains

(iii) new concrete equalities c may imply even concrete and/or abstract
equalities, on.
similar concrete-abstract interplay takes place course several recursion steps:
equalities concrete nodes provided D-tester may make new rules applicable
(for example Rp Rc) changes P thus also . may subsequently lead
detection equalities concrete nodes D-tester, on.
considerations show that, presence keys, exists close interplay
concrete domain reasoner tableau algorithm, needed keys
present: without keys, suffices apply concrete domain satisfiability check
completion rules exhaustively applied (Baader & Hanschke, 1991a).
detailed proof termination, soundness, completeness together complexity analysis tableau algorithm defined section given Appendix B.
Theorem 4.1. Let key-admissible concrete domain. extended D-satisfiability
NP, ALCOK(D)-concept satisfiability w.r.t. Boolean key boxes NExpTime.
note that, way presented here, algorithm leaves considerable
room optimizations. One possible optimization concerns re-use f -successors
(for abstract features f ): example, applying R rule concept f.C L(a),
already f -successor b, could simply add C L(b) instead adding
new f -successor c recording b c.
Another candidate optimizations test function. Recall function takes
predicate conjunction c set variables V non-deterministically returns concrete
equivalence, i.e., relation exists solution c vi vj iff
(vi ) = (vj ) (see Definition 4.1). hard devise ALC(D)-concept forces
completion systems exponentially many concrete nodes slightly adapting wellknown ALC-concepts require models exponential size (Halpern & Moses, 1992).
Hence, size input conjunctions c test exponential size input
concept. Even trivial D-conjunctions
c = >D (v1 ) >D (vk )
exponential number distinct concrete equivalences . Thus, number
possible outcomes call test function may double exponential size
input concept. Considering example, natural response problem
require test return minimal concrete equivalences: intuitively, equivalence
minimal variables equivalent whose equality enforced conjunction.
precisely, called minimal exists concrete equivalence 0
{(x, y) | x 0 y} {(x, y) | x y}. conjecture restricting test way
destroy soundness completeness tableau algorithm. However, although
definitely worthwhile optimization, help overcome existence
doubly exponentially many outcomes test worst caseat least concrete
domains D: consider concrete domain N Page 691 conjunctions form
ci = <i (v1 ) <i (v2i ).
readily checked that, 1, number minimal concrete equivalences
ci exponential i. Moreover, hard devise concept Ci size logarithmic
697

fiLutz, Areces, Horrocks, & Sattler

leads completion systems = ci . Hence, still doubly
exponentially many possible outcomes test function.
example discussed, exponential branching test clearly due
discreteness natural numbers. Indeed, use dense structure defining concrete
domains, seems restriction minimal concrete equivalences desired
effect, namely number tests possible outcomes becomes polynomial size
input thus exponential size input concept. example, consider
concrete domain Q, defined follows:
Q set

Q rational numbers;

Q contains unary predicates >Q negation Q , unary predicates =q 6=q
q , binary comparison predicates {<, , =, 6=, , >}, ternary addition
predicate +, negation + (all obvious semantics).

Q

readily checked Q key-admissible (note provides binary equality predicate) thus falls framework. conjecture exists one minimal
concrete equivalence every Q-predicate conjunction c: intuitively, seems possible
(inductively) determine relation set variables V used c (i) x
implies (x) = (y) every solution c (ii) exists solution c
v 6 v 0 implies (v) 6= (v 0 ). Clearly, minimal concrete equivalence. Moreover,
due (i) one.
4.2 Tableau Algorithm SHOQK(D)
Although ALCOK(D) quite powerful DL, lacks several expressive means
found state-of-the-art description logic systems FaCT RACER (Horrocks,
1998; Horrocks et al., 2000; Haarslev & Moller, 2001). section, consider
expressive description logic SHOQK(D) provides concrete domains, key
boxes, nominals, also many means expressivity transitive
roles, role hierarchies, qualifying number restrictions, general TBoxes. Modulo
details, SHOQK(D) viewed extension DL SHOQ(D) key boxes.
SHOQ(D) proposed Horrocks Sattler (2001) (see also Pan & Horrocks, 2002)
tool ontology reasoning context semantic web (Berners-Lee, Hendler,
& Lassila, 2001; Baader et al., 2002a).
One important feature SHOQK(D) so-called TBoxes, i.e. concept equations5
.
form C = used background theory reasoning. Since wellknown combining general TBoxes concrete domain constructor easily leads
undecidability (Baader & Hanschke, 1992; Lutz, 2004), SHOQK(D) offers pathfree variant concrete domain constructori.e. concrete features admitted
inside constructor rather paths arbitrary length. restriction indeed regains
decidability (Haarslev et al., 2001; Horrocks & Sattler, 2001). Path-freeness concrete
domain constructor obviously renders abstract features unnecessary, thus syntactic
type available SHOQK(D).
5. TBox formalisms also allow concept inclusions C v D, re-written
equivalent equations, see Section 2.2.2.5 (Baader et al., 2003).

698

fiKeys, Nominals Concrete Domains

4.2.1 Description Logic SHOQK(D)
Let us define SHOQK(D) formal way, starting syntax.
Definition 4.6 (SHOQK(D) Syntax). role axiom either role inclusion,
form R v R, NR , transitivity axiom Trans(R) R NR . role
box R finite set role axioms. Let v
* reflexive-transitive closure role
inclusions R. role name R called simple v
* R implies Trans(S)
/ R role
names S. Let concrete domain. set SHOQK(D)-concepts smallest set

every concept name every nominal concept,
C concepts, R role name, simple role name, n k natural
numbers, g1 , . . . , gn concrete features, P predicate arity n,
following expressions also concepts:
C, C u D, C D, R.C, R.C, (> k C), (6 k C), g1 , . . . , gn .P, g1 .
.
concept equation expression C = C concepts. TBox finite set
concept equations.
SHOQK(D), consider key boxes differ two aspects ones considered ALCOK(D): following, assume key boxes path-free, admit
complex concepts occur key assertions. Note abstract features paths
occur syntax SHOQK(D)as become clear semantics
defined, former simulated general number restrictions (6 n R C).
usual description logics SHIQ/SHOQ family, require role names
number restrictions simple since admitting arbitrary roles yields undecidability
reasoning (Horrocks et al., 2000; Horrocks & Sattler, 2001). role box R clear
context, usually write Trans(R) instead Trans(R) R. introduce
semantics SHOQK(D) relevant reasoning problems.
Definition 4.7 (SHOQK(D) Semantics). Interpretations = (I , ) defined
Definition 2.3, function extended novel SHOQK(D)-concepts
follows:
(6 k R C)I := {d | ]{e | (d, e) RI } k}
(> k R C)I := {d | ]{e | (d, e) RI } k}.
.
Let interpretation. satisfies concept equation C = C = DI .
model TBox satisfies concept equations . Similarly, satisfies role
inclusion R v RI transitivity axiom Trans(R) RI transitive relation.
model role box R satisfies role inclusions transitivity axioms R.
Let TBox, R role box, K key box. concept C satisfiable w.r.t. ,
R, K iff C, , R, K common model. C subsumed concept w.r.t.
, R, K (written C vT ,R,K D) iff C DI common models , R, K.
699

fiLutz, Areces, Horrocks, & Sattler

Note that, due requirement role names used inside number restrictions
simple, existential universal value restrictions syntactic sugar: contrast
number restrictions, used roles.
well-known that, many expressive description logics, reasoning TBoxes
reduced reasoning without (Schild, 1991; Horrocks & Sattler, 2001):
SHOQK(D), concept C satisfiable w.r.t. , R, K iff concept
R.C u R.

u


DE u
.
D=ET

u

R.N



nominal N used
C, , K

satisfiable w.r.t. R0 , K, empty TBox, R fresh role appearing
C, R, ,
[
{S v R}.
R0 := R {Trans(R)}
role name used
C, , R, K

Since subsumption reduced satisfiability described Section 2, following
consider satisfiability concepts w.r.t. role boxes key boxes, without
TBoxes. also generally assume role boxes R acyclic, i.e. satisfy following
condition: role name R, role names R1 , . . . , Rk R = R1 = Rk
Ri v Ri+1 R 1 < k. hard see restriction since
cycles eliminated: R1 , . . . , Rk cycle R, R1I = = RkI
interpretations I. Thus simply remove cycle R replace every
occurrence R2 , . . . , Rk C, R, K R1 , add Trans(R1 ) if, cycle
elimination, Trans(Ri ) 1 n.
turn attention construction tableau algorithm SHOQK(D),
let us comment minor differences SHOQK(D) introduced
original version SHOQ(D) described (Horrocks & Sattler, 2001). main
difference logic, like extensions investigated (Haarslev et al., 2001; Pan &
Horrocks, 2002), allows n-ary predicates Horrocks Sattler restrict
unary predicates. Moreover, SHOQ(D) introduced (Horrocks & Sattler, 2001) uses
concrete roles rather concrete features, difference concrete roles
necessarily functional. Due non-functionality, original SHOQ(D) admits two
variants T.P T.P concrete domain constructor (where concrete role
P unary predicate). SHOQK(D), simulate universal variant writing
g.P g since concrete features g interpreted partial functions and, contrast
Horrocks Sattler, undefinedness constructor g available. Except
n-ary predicates provide important additional expressivity, view deviations
minor ones since easy see affect decidability complexity
reasoning.
4.2.2 Tableau Algorithm SHOQK(D)
basic intuitions SHOQK(D) tableau algorithm similar ALCOK(D)
algorithm, one exception: deal various expressive means SHOQK(D),
700

fiKeys, Nominals Concrete Domains

(> n R C)
(> 0 R C)
(6 n R C)

(6 (n 1) R C) n 1

(> (n + 1) R C)

Figure 10: SHOQK(D) NNF rewrite rules.
convenient introduce certain abstraction models, so-called tableaux. main
difference tableaux models that, tableaux, roles declared transitive
necessarily described transitive relations. show exists tableau
given concept key box common model. aim
SHOQK(D) algorithm construct tableau input rather trying
construct model. this, algorithm employs completion forests underlying
data structure.
first introduce tableaux. Let us start discussing preliminaries.
ALCOK(D), assume concepts key boxes NNF, i.e. negation occurs
front concept names nominals. use C denote NNF C.
additional NNF rewrite rules SHOQK(D) found Figure 10 complete
given ALCOK(D) Figure 7.
concept D, role box R, key box K, define
cl(D, K) := sub(D) sub(con(K)) {C
| C sub(D) sub(con(K))}
cl(D, R, K) := cl(D, K) {R.C | R v
* S.C cl(D, K)}.
Obviously, cardinality cl(D, R, K) linear size D, R, K.
D,K
denote set role names occurring D, R, K, NcF
follows, write ND,R,K
R
denote sets concrete features occurring K. ready define
tableaux.
Definition 4.8 (Tableau). Let SHOQK(D)-concept NNF, R role box, K
path-free key box NNF. tableau w.r.t. R K tuple (Sa , Sc , L, E, e, P)

Sa , Sc sets abstract concrete individuals,
L : Sa 2cl(D,R,K) maps abstract individual subset cl(D, R, K),
D,R,K

E : Sa Sa 2NR

maps pairs abstract individuals sets roles,

e : Sa ND,K
cF Sc maps pairs abstract individuals concrete features concrete
individuals,
P maps n-ary concrete predicate cl(D, R, K) set n-tuples Sc ,
abstract individual s0 Sa L(s0 ),
s, Sa , C, C1 , C2 cl(D, R, K), R, ND,R,K
,
R
(s, C) := {t Sa | E(s, t) C L(t)},
case that:
701

fiLutz, Areces, Horrocks, & Sattler

(T1) C L(s), C
/ L(s),
(T2) C1 u C2 L(s), C1 L(s) C2 L(s),
(T3) C1 C2 L(s), C1 L(s) C2 L(s),
(T4) R E(s, t) R v
* S, E(s, t),
(T5) R.C L(s) R E(s, t), C L(t),
(T6) R.C L(s), Sa R E(s, t) C L(t),
(T7) S.C L(s) R E(s, t) R v
* Trans(R), R.C L(t),
(T8) (> n C) L(s), ]S (s, C) > n,
(T9) (6 n C) L(s), ]S (s, C) 6 n,
(T10) either (6 n C) L(s) E(s, t) (g1 , . . . , gn keyfor C) K e(t, gi )
defined 1 n, {C, C} L(t) 6= ,
(T11) N L(s) L(t), = t,
(T12) g1 , . . . , gn .P L(s), x1 , . . . , xn Sc e(s, gi ) = xi
(x1 , . . . , xn ) P(P ),
V
V
V
(T13) P used D,K (x1 ,...,xn )P(P ) P (x1 , . . . , xn ) x6=y x 6= satisfiable,
(T14) (g1 , . . . , gn keyfor C) K, C L(s) L(t), e(s, gi ) = e(t, gi ) 1 n,
= t,
(T15) g L(s), e(s, g) undefined.
Note predicate conjunction (T13) uses binary inequality predicate. general,
require concrete domain equipped predicate thus
predicate conjunction necessarily D-conjunction. However, nevertheless safe
use (T13) given form since tableaux used proofs need
concrete domain reasoner capable deciding satisfiability conjunction.
following lemma, whose proof provided Appendix C, shows definition
tableaux provides adequate abstraction models.
Lemma 4.2. Let SHOQK(D)-concept NNF, R role box, K key box
NNF. satisfiable w.r.t. R K iff tableau w.r.t. R K.
Given Lemma 4.2, order decide satisfiability SHOQK(D)-concepts w.r.t. role
key boxes, may use (tableau) algorithm tries construct tableau input.
following, describe algorithm detail.
previous section, algorithm works completion systems. However,
case SHOQK(D) core component completion systems completion forest
rather completion tree. reason completion rules remove
nodes edges completion system way disconnect one tree
two subtrees.
702

fiKeys, Nominals Concrete Domains

Definition 4.9 (Completion System). Let SHOQK(D)-concept NNF, R role
box, K path-free key box NNF. concept (> n R C) cl(D, R, K)
1 n, reserve concept name AnRC
appearing cl(D, R, K) define

extended closure
nRc
cl+ (D, R, K) := cl(D, R, K) {AnRc
| (> n R C) cl(D, R, K)}.
1 , . . . ,

Let Oa Oc disjoint countably infinite sets abstract concrete nodes.
completion forest D, R, K finite forest F = (Va , Vc , E, L) nodes Va Vc
Va Oa , Vc Oc , nodes Vc leaves. forest labelled
follows:
node Va labelled subset L(a) cl+ (D, R, K),
edge (a, b) E a, b Va labeled non-empty set role names
L(a, b) occurring D, R, K,
edge (a, x) E Va x Vc labeled concrete feature L(a, x)
occurring D, R, K.
completion system D, R, K tuple = (F, P, c , )
F = (Va , Vc , E, L) completion forest D, R, K,
P maps n-ary concrete predicate cl(D, R, K) set n-tuples Vc ,
c equivalence relation Vc ,
linear ordering Va .
node Va called R-successor node Va if, R0 R0 v
* R,
R0 L(s, t). node x Vc called g-successor node Va L(s, x) = g. Finally,
.
write =
6 R-successors node AnRC
L(s)

nRC
Aj
L(t) 6= j.
remarks order here. Firstly, contrast ALCOK(D) case, relation
longer required respect level node. due fact (a)
enforce termination artificially mentioned property used
ensure automatic termination, (b) level node might change since node
might become root node completion rules remove nodes edges.
Secondly, relation c returned D-tester, used compute
relation used tableau algorithm. However, need
compute relation c ALCOK(D) case since concepts key
boxes assumed path-free.
Thirdly, new concept names AnRC
used ensure successors node

x generated (> n R C) L(x) merged later due concept (6
n0 R C 0 ) L(x): generated successor labelled different concept AnRC
;

since merging two nodes means unifying node labels, suffices disallow
703

fiLutz, Areces, Horrocks, & Sattler

occurrence distinct concepts AnRC
node label suitable definition

clash.
Since SHOQK(D) provides transitive roles, need cycle-detection mechanism
order guarantee termination algorithm: roughly speaking, encounter
node similar already existing one, node need
explored. Speaking terms (Horrocks et al., 2000; Baader & Sattler, 2000),
employ mechanism called subset blocking.
Definition 4.10 (Blocked). Let reflexive closure . node Va blocked
node Va L(t) L(s), s0 , s0 L(t) L(s0 ).
Note that, unlike done, e.g., (Horrocks et al., 2000), blocking node
necessarily ancestor blocked node, anywhere forest. may even
blocked nodes unblocked successors. modification used later obtain
NExpTime upper bound.
decide satisfiability ALCOK(D)-concept w.r.t. role box R pathfree key box K (where K NNF), tableau algorithm started
initial completion system
SD = (FD , P , , ),
FD = ({s0 }, , , {s0 7 {D}})
P
maps P occurring K .
algorithm repeatedly applies completion rules. actual rules given,
introduce new notions: firstly, define equivalence relation Va
follows: one following conditions satisfied:
N L(s) L(t) nominal N
(g1 . . . , gn keyfor C) K, C L(s)L(t), xi , yi gi E(s, xi )
E(t, yi ) xi c yi 1 n.
Intuitively, two abstract nodes related via relation describe individual
tableau.
Secondly, use following abbreviations formulation rules (written
italic):
remove abstract node incoming outgoing edges, remove
Va (s, t) (t, s) E Va Vc .
Adding g-successor abstract node means nothing exists
g-successor x Vc and, otherwise, adding E(s, x) = g x Oc
yet occur completion forest.
update relation c , D-tester asked decide satisfiability Dconjunction
^
^
:=
P (x1 , . . . , xn )
x=y
P used D,K
(x1 ,...,xn )P(P )

xc

returns, case conjunction satisfiable, updated concrete equivalence c defined Definition 4.1.
704

fiKeys, Nominals Concrete Domains

Concerning predicate conjunction used updates, recall w.l.o.g. assume
concrete domain contain equality predicate discussed Definition 4.1.
completion rules given Figure 11. generally assume new nodes x
introduced completion forest x already existing nodes y.
describing tableau algorithm, comment completion rules.
rules Rt, R6, Rc, Rch non-deterministic, i.e., application one
possible outcome. Rc rule, due update operation performed c
using D-tester: discussed end Section 4.1, computing concrete equivalence
given D-conjunction may result high degree non-determinism. Please note that,
contrast ALCOK(D), need call D-tester ruleand
rule application.
Next, Ra rule takes care abstract nodes related via . Since nodes
-equivalence class denote individual, choose one representative whose
node label contains labels nodes class. representative simply
-minimal node equivalence class Ra rule performs appropriate
copying node labels.
R6 rule rule remove nodes edges: removes surplus R-successor
node (6 n R C) L(s). Since subtree removed, ts successors
new, additional root nodes. behavior reason work completion
forest.
ALCOK(D) case, tableau algorithm stops applying rules finds
obvious contradiction, clash, completion rules applicable.
Definition 4.11 (Clash). Let = (F, P, c , ) completion system D, R K,
F = (Va , Vc , E, L). said contain clash one following conditions
applies:
(C1) concept name NC node Va , {A, A} L(s);
(C2) D-conjunction defined satisfiable;
.
(C3) =
6 Va ;
(C4) Va g NcF , g L(s) g-successor.
completion system containing clash called clash-free. completion system
complete none completion rules applicable.
Due simplicity algorithm, refrain describing pseudo-code notation: algorithm starts initial completion system repeatedly applies
completion rules, checking clashes rule application. clash detected, returns unsatisfiable. complete clash-free completion system found,
algorithm returns satisfiable. Note that, since completion rules
non-deterministic, algorithm also non-deterministic.
Details proof termination, soundness, completeness given Appendix C. Unfortunately, leave complexity algorithm open
problem: hard prove runs double exponential time, clear
whether exponential time also suffices. However, still use algorithm obtain
705

fiLutz, Areces, Horrocks, & Sattler

Ru

C1 u C2 L(s), blocked, {C1 , C2 } 6 L(s),
L(s) := L(s) {C1 , C2 }

Rt

C1 C2 L(s), blocked, {C1 , C2 } L(s) = ,
L(s) := L(s) {C} C {C1 , C2 }

R

R.C L(s), blocked, R-successor C L(t)
create new node t0 t0 Va
set E(s, t) := {R} L(t) := {C}

R>

(> n C) L(s), blocked, n S-successors
.
t1 , . . . , tn C L(ti ) ti =
6 tj 1 < j n,
create n new nodes t1 , . . . , tn s.t. t0 ti 1 n t0 Va ,
set E(s, ti ) := {S} L(ti ) := {C, AnSC
} 1 n


R6

(6 n C) L(s), blocked, n + 1 S-successors t0 , . . . , tn
C L(ti ) 0 n,
choose i, j ti tj , set L(ti ) := L(ti ) L(tj ),
L(s, ti ) := L(s, ti ) L(s, tj ), remove tj incoming
outgoing edges

Rc

g1 , . . . , gn .P L(s), blocked,
gi -successors xi (x1 , . . . , xn ) P(P )
add gi -successor 1 n,
yi gi -successor s, add (y1 , . . . , yn ) P(P ),
update c

R

R.C L(s), blocked,
R-successor C
/ L(t),
L(t) := L(t) {C}

R+

S.C L(s), blocked, R
Trans(R) R v
* S, R-successor R.C
/ L(t),
L(t) := L(t) {R.C}

Rch

S-successor s0 (6 n C) L(s0 )
gi -successors xi 1 n (g1 , . . . gn keyfor C) K
blocked {C, C} L(s) = ,
L(s) := L(s) {E} E {C, C}

Ra

t, L(t) 6 L(s), t, blocked,
set L(s) := L(s) L(t)
Figure 11: completion rules SHOQK(D).

706

fiKeys, Nominals Concrete Domains

tight complexity bound SHOQK(D): following corollary easy by-product
correctness proofs (for proof see Appendix C).
Corollary 4.3. SHOQK(D)-concept satisfiable w.r.t. role box R path-free
key box K, satisfiable w.r.t. R K model size |I | 2m
= # cl+ (D, R, K).
Thus following alternative algorithm deciding satisfiability SHOQK(D)concept w.r.t. role box R path-free key box K: first, guess interpretation
cardinality bounded 2m , using placeholder variables Oc instead
concrete values interpretation concrete features. Let Vc set variables
Oc occuring I. Additionally guess interpretation P concrete domain
predicates: completion forests, P maps n-ary concrete predicate used
K n-ary relation Vc . perform standard (polynomial-time) model checking
ensure model D. this, treat concepts form g1 , . . . , gn .P using
interpretation predicates P. easily checked polynomial time also
model R Kfor latter, assume placeholder variables stand different
values. Finally, use concrete domain D-tester check whether conjunction
^
P (x1 , . . . , xn )
P used inD,K
(x1 ,...,xn )P(P )

satisfiable. Answer yes otherwise. Since algorithm clearly
implemented NExpTime provided D-tester running non-deterministic
polynomial time, obtain following:
Theorem 4.4. Let key-admissible concrete domain extended D-satisfiability
NP, SHOQK(D)-concept satisfiability w.r.t. TBoxes, role boxes, path-free
key boxes NExpTime.

5. Conclusion
paper, identified key constraints interesting extension description
logics concrete domains. Starting observation, introduced number
natural description logics provided comprehensive analysis decidability
complexity reasoning. main observation investigations key boxes
dramatic consequences complexity reasoning: example, PSpacecomplete DL ALC(D) becomes NExpTime-complete extended path-free, unary,
Boolean key boxes undecidable extended path-free, unary, non-Boolean key
boxes. Thus effect key boxes complexity quite different effect
key assertions abstract features allowed (Calvanese et al., 2000):
abstract key assertions said free since increase complexity
expressive description logics.
show restriction Boolean key boxes (in ALCOK(D) case)
path-free key boxes (in SHOQK(D) case) yield decidabile NExpTime-complete
reasoning problems. selected ALC(D) SHOQ(D) basis analysis since,
707

fiLutz, Areces, Horrocks, & Sattler

opinion, fundamental description logics concrete domains.
Going one step further, would interesting combine key boxes extensions
concrete domains, ones presented Lutz (2003, 2004). name one
possibility, extension ALCOK(D) SHOQ(D) inverse roles seems
natural idea. Note inverse roles interact several available means
expressivity: ALC inverse roles PSpace complete (Horrocks, Sattler, & Tobies,
1999), ALCO inverse roles ExpTime-complete (Areces et al., 1999) ALC(D)
inverse roles even NExpTime-complete (Lutz, 2004).
options future research closely related material presented
paper. example, SHOQK(D)-concept satisfiability still decidable drop
requirement key boxes path-free? Moreover, leave exact time
requirements tableau algorithm open problem. algorithm runs (nondeterministic) exponential time, directly yields Theorem 4.4 rather via bounded
model property.

Acknowledgments
would like thank anonymous reviewers valuable comments. paper
extended version (Lutz, Areces, Horrocks, & Sattler, 2003).

Appendix A. Proofs Section 3.2
prove D2 -satisfiability decided PTime.
Proposition A.1. D2 -satisfiability PTime.
Proof. Let c D2 -conjunction. show c satisfiable iff none following
conditions applies:
1. c contains conjunct D2 (x);
2. c contains conjuncts bit0ik (x) bit1ik (x);
3. c contains conjuncts bitnik (x) bitmj` (x) k 6= `;
4. c contains conjuncts bitnik (x) bitnik (x);
5. c contains conjuncts bitnik (x), bit0jk (x), bit1jk (x).
easily seen c unsatisfiable one conditions applies. Assume
Conditions 1 5 apply c let X set variables used c.
x X, set t(x) = k bitnik (x) c n, .6 bitnik (x)
/ c n, i, k ,
set t(x) = r r appearing index r predicate c. mapping
well-defined since c finite, Condition 3 apply, predicates available
bitnik (), D2 (), >D2 (). define solution c follows: x X, set
(x) bit vector v BVt(x) ith bit 1 bit1it(x) (x) c bit0it(x) (x) c,
0 otherwise. remains prove indeed solution c:

N

6. use P (x) c abbreviation P (x) conjunct c.

708

N

fiKeys, Nominals Concrete Domains

Let bit0ik (x) c. t(x) = k thus (x) BVk . Since Condition 2
/ c. Moreover, non-applicability Condition 4 implies
apply, bit1ik (x)

/ c. definition , ith bit (x) thus 0.
bit0k (x)
Let bit1ik (x) c. t(x) = k (x) BVk . definition , ith bit
(x) 1.
Let bit0ik (x) c. t(x) 6= k, (x)
/ BVk . Thus (x) (bit0ik )D2
done. t(x) = k, ith bit (x) 1 definition thus
(x) (bit0ik )D2 .
/ BVk done. t(x) = k,
Let bit1ik (x) c. t(x) 6= k, (x)
j
bitnk (x) c n, j . Since Condition 5 apply, thus

N

/ c. Thus,

/ c. Moreover, non-applicability Condition 4 yields bit1ik (x)
definition , ith bit (x) 0.

bit0ik (x)

obvious listed properties checked polynomial time.

Appendix B. Proofs Section 4.1
prove termination, soundness, completeness ALCOK(D) tableau algorithm
presented Section 4.1, starting termination. start establishing notions
technical lemmas.
Let C concept K key box. use |C| denote
P length C, i.e.
number symbols used write down, |K| denote (u1 ,...,uk keyfor C)K |C|.
path u = f1 fk g, use |u| denote k + 1. role depth concepts defined
inductively follows:
rd(A) = rd(N ) = rd(g) = 0
rd(u1 , . . . , un .P ) = max{|ui | | 1 n} 1
rd(C u D) = rd(C D) = max{rd(C), rd(D)}
rd(R.C) = rd(R.C) = rd(C) + 1.
following series lemmas eventually allow us prove termination.
Lemma B.1. constant k that, tableau algorithm started input
C0 , K = (Va , Vc , E, L) completion tree constructed run algorithm,
k
k
#Va 2|C0 | #Vc 2|C0 | .
Proof. Using induction number rule applications case distinction according
applied rule, straightforward show
C L(a) implies rd(C) |C0 | levT (a)

()

constructed completion trees T. omit details note that, (1) treating
Rch rule, one needs employ fact K Boolean thus adds concepts
role depth 0 node labels, (2) treating Rp rule, use b implies
levT (a) levT (b).
709

fiLutz, Areces, Horrocks, & Sattler

implies upper bound depth constructed completion trees: first,
R Rc rules generate new nodes, application either rule node
Va implies L(a) 6= thus levT (a) |C0 | (). Second, new (abstract
concrete) node b generated application rules node Va clearly satisfies
levT (b) levT (a) + max(1, mpl(C0 )), mpl(C0 ) denotes maximum length paths
C0 (note concepts K may contain paths since Boolean). Since
mpl(C0 ) |C0 |, observations imply depth constructed completion
trees bounded 2 |C0 |.
out-degree. node generated, due application
rule R Rc and, initially, one successor. Let us analyze number
successors generated later applications rules R Rc a: rules
applied concept form R.C u1 , . . . , un .P L(a).
definition cl(C0 , K) since K Boolean, number concepts per node
label bounded #sub(C0 ) |C0 |. Moreover, rule application creates |C0 |
successors. Hence, out-degree constructed completion trees bounded |C0 |2 + 1.

Lemma B.2. constant k that, tableau algorithm started C0 , K,
k
then, every recursion step, loop terminates 2|C0 | steps.
Proof. Fix argument = (T, P, , ) = (Va , Vc , E, L) passed sat function,
let 1 , 2 , . . . sequence concrete equivalences computed loop, let
1c , 2c , . . . corresponding c relations. Since test(S ) calls D-tester,
calls indeed terminates.
show
1 ( 2 ( ,
()
k

implies Lemma B.2: Lemma B.1, exists constant k #Vc 2|C0 | .
k
Hence, #i 22|C0 | which, together (), implies number steps
k
performed loop also bounded 22|C0 | .
proof (). loop reaches i-th step, i1 6= i1
c
step 1. Since i1 i1
definition, implies i1 ( ci1 . definition
c
, easy see i1
1. Hence i1 ( .
c
Lemma B.3. constant k that, tableau algorithm started C0 , K,
k
number recursive calls bounded 2(|C0 |+|K|) .
Proof. obviously suffices establish appropriate upper bound number rule
applications. Ru, Rt, R, Rc rules applied concept
node label. Lemma B.1, number nodes exponential |C0 | + |K|.
Since neither nodes concepts node labels ever deleted, fact node labels
subsets cl(C0 , K) thus implies number applications rules
exponential |C0 | + |K|. holds rules R Rp, applied
every concept C cl(C0 , K) every pair (abstract) nodes. Finally,
number Rch applications exponential |C0 | + |K| since rule
applied every abstract node every key assertion K.
710

fiKeys, Nominals Concrete Domains

Termination obvious consequence Lemmas B.2 B.3.
Corollary B.4 (Termination). tableau algorithm terminates input.
Let us prove soundness algorithm.
Lemma B.5 (Soundness). tableau algorithm returns satisfiable, input concept
C0 satisfiable w.r.t. input key box K.
Proof. tableau algorithm returns satisfiable, exists complete clashfree completion system = (T, P, , ) C0 . Let = (Va , Vc , E, L). definition
tableau algorithm, completion system 0 = (T, P, , 0 ) call
test(S 0 ) returned . Moreover, = c S. Thus, exists solution
0
(x) = (y) iff x c y.
()
0
Clearly, also solution : since
^ second
^ component P ,
solution first part
P (x1 , . . . , xn ) . Moreover,
P used C (x1 ,...,xn )P(P )

conjunct =(x, y) second part , x c definition
thus (x) = (y) ().
use construct interpretation setting


= {a Va | b Va b b a} {w}

AI

= {a | L(a)}

{a | N L(a)} N L(a)
=
{w}
otherwise

NI
RI

= {(a, b) | a0 , b0 Va a0 , b b0 ,
b0 R-successor a0 }

gI

= {(a, (x)) | x g/a -neighbor a}

NC , N , R NR , g NcF . first show well-defined:
N singleton N . Assume exist a, b 6= b
N L(a) L(b). definition (Definition 4.3), N L(a) L(b) implies
b. This, together a, b , yields b b a, contradicting
linear ordering.
f functional f NaF . Assume exist a, b, c
{(a, b), (a, c)} f b 6= c. exist a1 , a2 , b0 , c0 Va a1
a2 , b b0 , c c0 , b0 f -successor a1 , c0 f -successor a2 .
definition , thus b0 c0 implying b c. Since b, c , yields
b c c b, contradiction.
g functional g NcF . Assume exist x, Vc
{(a, (x)), (a, (y))} f (x) 6= (y). x g/a neighbors a. definition c , thus x c implying (x) = (y) (),
contradiction.
711

fiLutz, Areces, Horrocks, & Sattler

show following claim. proof, use notion f1 fk /a -neighbors
(with f1 , . . . fk abstract features), defined analogously u/a -neighbors paths u.
Claim 1: paths u, uI (a) = iff ui / -neighbor
x (x) = .
Proof: Let u = f1 fk g. Using induction easily proved that,
b , fiI ( (f1I (a)) ) = b iff f1 fi /a -neighbor b0
b b0 . Thus particular fkI ( (f1I (a)) ) = b iff f1 fk /a neighbor b0 b b0 . prove claim, hence remains use definition
g together ().
following claim central showing model C0 K.
Claim 2: C cl(C0 , K), C L(a), C .
Since C0 label root node, Claim 2 clearly implies model C0 .
Moreover, use prove satisfies key assertions (u1 , . . . , un keyfor C)
K: fix a, b C uIi (a) = uIi (b) 1 n. Non-applicability Rch yields
{C, C} L(a) 6= . C L(a), Claim 2 implies ( C)I contradiction
C . Thus obtain C L(a). analogous way, argue C L(b). Since
uIi (a) uIi (b) defined 1 n, Claim 1 yields ui /a -neighbor xi
(xi ) = uIi (a) b ui /a -neighbor yi (yi ) = uIi (b) 1 n. Thus
fact uIi (a) = uIi (b) yields (xi ) = (yi ) 1 n. () obtain xi c yi
thus xi yi 1 n. definition , thus get b. Since a, b ,
obtain 6 b b 6 definition thus = b.
remains prove Claim 2, using structural induction:
C concept name nominal. Easy construction I.
C = D. Since C cl(C0 , K), C NNF concept name. Since
clash-free, C L(a) implies
/ L(a). Thus,
/ DI construction I,

yields (D) .
C = u1 , . . . , un .P . Since Rc rule applicable, exist x1 , . . . , xn Vc
xi ui /a -neighbor 1 n (x1 , . . . , xn ) P(P ). Claim 1
yields uIi (a) = (xi ) 1 n. Since (x1 , . . . , xn ) P(P ) solution
, ((x1 ), . . . , (xn )) P thus C .
C = g. Since clash-free, exists x Vc x g/a -neighbor
a. Thus Claim 1 (a, ) g .
C = u E C = E. Straightforward using completeness induction
hypothesis.
C = R.D. Since R rule applicable, R/a -neighbor b
L(b). Let b0 minimal w.r.t. b b0 . definition I,
(a, b0 ) RI . Non-applicability Rp rule yields L(b0 ). induction, get
b0 DI thus C .
712

fiKeys, Nominals Concrete Domains

C = R.D. Let (a, b) RI . definition I, implies exist a0 , b0 Va
minimal w.r.t. a0 , b minimal w.r.t. b b0 , b0
R-successor a0 . Since b0 clearly R/a -neighbor a, non-applicability
R yields L(b0 ), implies L(b) due non-applicability Rp.
induction, get b DI . Since holds independently choice b, obtain
(R.D)I .

Lemma B.6 (Completeness). input concept C0 satisfiable w.r.t. input key box
K, tableau algorithm returns satisfiable.
Proof. Let model C0 K. use guide (the non-deterministic parts
of) algorithm constructs complete clash-free completion system.
completion system = (T, P, , ) = (Va , Vc , E, L) called I-compatible
exist mappings : Va : Vc
(Ca) C L(a) (a) C
(Cb) b R-successor ((a), (b)) RI
(Cc) x g-successor g ((a)) = (x)
(Cd) (x1 , . . . , xn ) P(P ) ( (x1 ), . . . , (xn )) P
(Ce) x (x) = (y).
first establish following claim:
Claim 1: completion system I-compatible, (i) b implies (a) = (b)
(ii) x c implies (x) = (y).
Proof: show induction ia b implies (a) = (b) (see Definition 4.3),
yields (i).
Start. 0a b, exists nominal N N L(a) L(b). (Ca)
obtain (a) N (b) N , yields (a) = (b) definition
semantics.
Step. ia b, distinguish three cases:
1. i1
b, (a) = (b) induction.

2. c Va f NaF b f /i1
-neighbors
i1 c , f -successor
c. Hence, exist c1 , c2 Va c i1
c

1
2


c1 , b f -successor c2 . induction, (c) = (c1 ) = (c2 ).
Thus (Cb) yields {((c), (a)), ((c), (b))} f , implies (a) = (b)
definition semantics.
3. exist (u1 , . . . , un keyfor C) K, ui /ai1 -neighbors xi ui /ai1 neighbors yi b 1 n C L(a)L(b) xi yi 1 n.
(Ca) yields a, b C . Using induction, (Cb), (Cc), straightforward
713

fiLutz, Areces, Horrocks, & Sattler

show uIi ((a)) = (xi ) uIi ((b)) = (yi ) 1 n. (Ce),
implies uIi ((a)) = uIi ((b)) 1 k. Since model key box K,
yields (a) = (b) definition semantics.
Part (ii) Claim 1. x c y, either x Va g NcF
x g/a -neighbors a. former case, (Ce) yields (x) = (y).
latter case, Part (i) claim (Cc) yields {((a), (x)), ((a), (y))} g
implies (x) = (y). finishes proof Claim 1.
show completion rules applied I-compatibility
preserved.
Claim 2: completion system I-compatible rule R applicable S, R
applied I-compatible completion system 0 obtained.
Proof: Let I-compatible completion system, let functions satisfying
(Ca) (Ce), let R completion rule applicable S. make case distinction
according type R.
Ru rule applied concept C1 u C2 L(a). (Ca), C1 u C2 L(a) implies
(a) (C1 u C2 )I hence (a) C1I (a) C2I . Since rule adds C1
C2 L(a), yields completion system I-compatible via .
Rt rule applied C1 tC2 L(a). C1 tC2 L(a) implies (a) C1I (a) C2I .
Since rule adds either C1 C2 L(a), applied yields
completion system I-compatible via .
R rule applied concept R.C L(a), generates new R-successor b
sets L(b) = {C}. (Ca), (a) (R.C)I and, hence, exists
((a), d) RI C . Set 0 := {b 7 d}. readily checked
resulting completion system I-compatible via 0 .
R rule applied concept R.C L(a) adds C L(b) existing
R/a -neighbor b a. Hence, exists a0 a0 b Rsuccessor a0 . Part (i) Claim 1, (a) = (a0 ). Thus, (Ca)
(a0 ) (R.C)I (Cb) yields (((a0 ), (b)) RI . definition semantics,
(b) C thus resulting completion system I-compatible via .
(i)

(i)

Rc rule applied concept u1 , . . . , un .P L(a) ui = f1 fki gi
(i)

1 n. rule application generates new abstract nodes bj xj 1 n
1 j ki
(i)

(i)

(i)

(i)

b1 f1 -successor 1 n,
(i)

bj fj -successor bj1 1 n 2 j ki ,
(i)

xi gi -successor bki 1 n,
(x1 , . . . , xn ) P(P ).
714

fiKeys, Nominals Concrete Domains

(i)

(Ca), (a) (u1 , . . . , un .P )I . Hence, exist dj 1 n
1 j ki 1 , . . . , n
(i)

(i)

(i)

(i)

((a), d1 ) (f1 )I 1 n,
(i)

(dj1 , dj ) (fj )I 1 n 2 j ki ,
(i)

giI (dki ) = 1 n,
(1 , . . . , n ) P .


(i)
(i)
Set 0 := 1in 1jki {bj 7 dj } 0 := 1in {xi 7 }.
resulting completion system I-compatible via 0 0 .
Rch rule applied abstract node key assertion (u1 , . . . , un keyfor C)
K non-deterministically adds either C C. definition semantics,
(a) C (a) ( C)I . Hence, Rch applied resulting
completion system I-compatible via .
Rp rule applied concept C L(a) adds C label L(b) node b
b. (Ca), (a) C . Since Claim 1 yields (a) = (b), follows
resulting completion system I-compatible via .
Finally, show I-compatibility implies clash-freeness.
Claim 3: Every I-compatible completion system clash-free.
Proof: Let = (T, P, , ) I-compatible completion system. Consider three
kinds clash:
Due (Ca), clash form {A, A} L(a) clearly contradicts semantics.
Assume Va x Vc g L(a) x g/a -neighbor
a. exists b Va b x g-successor b. Claim 1,
b implies (a) = (b). Thus, g L(a) (Ca) give (b) (g)I . obtain
contradiction since (Cc) yields ((b), (x)) g .
Properties (Cd) (Ce) Part (ii) Claim 1 imply solution .
Thus, concrete domain satisfiable.
describe guidance tableau algorithm model detail:
ensure that, times, considered completion systems I-compatible.
obviously holds initial completion system
SC0 = (TC0 , P , , ) TC0 = ({a0 }, , , {a0 7 {C}}).
guide non-deterministic test function that, given predicate conjunction
set variables Vc Oc input, returns relation defined setting x
iff (x) = (y) x, V . relation concrete equivalence since solution
(see above). guidance, (Ce) obviously satisfied call test,
properties affected call. According Claim 2, apply
715

fiLutz, Areces, Horrocks, & Sattler

completion rules I-compatibility preserved. Corollary B.4, algorithm
always terminates, hence also guided way. Since, Claim 3, find
clash, algorithm returns satisfiable.
tableau algorithm yields decidability tight upper complexity bound ALCOK(D)concept satisfiability w.r.t. key boxes.
Theorem B.7 (Theorem 4.1 Section 4.1). Let key-admissible concrete domain.
extended D-satisfiability NP, ALCOK(D)-concept satisfiability w.r.t. Boolean
key boxes NExpTime.
Proof. Corollary B.4 Lemmas B.5 B.6 yield decidability ALCOK(D)-concept
satisfiability w.r.t. Boolean key boxes. complexity, Lemma B.3 provides exponential
bound number recursive calls. Hence, remains show single recursion
step needs exponential time. Lemma B.2, loop terminates
exponentially many steps. step, compute relations c ,
used construction predicate conjunction checking termination
loop. Since, Lemma B.1, exists exponential bound number
abstract concrete nodes completion system S, obviously done
exponential time. Moreover, Lemma B.1 implies size exponential.
together fact extended D-satisfiability NP implies call
test needs exponential time. remaining tasks (checking clashes, completeness,
rule applicability) clearly also performed exponential time.

Appendix C. Proofs Section 4.2
first provide proof Lemma 4.2 shows notion tableaux introduced
Section 4.2 adequate abstraction models.
Lemma C.1 (Lemma 4.2 Section 4.2). Let SHOQK(D)-concept NNF, R
role box, K path-free key box NNF. satisfiable w.r.t. R K iff
tableau w.r.t. R K.
Proof. only-if direction, construct tableau common model D,
R, K follows:
Sa :=
Sc := {x | g (s) = x Sa }
L(s) := {C cl(D, R, K) | C }
E(s, t) := {S ND,R,K
| (s, t) }
R
e(s, g) := g (s) g (s) defined
P(P ) := {(x1 , . . . , xn ) Snc | (x1 , . . . , xn ) P }.
easily verified tableau w.r.t. R K: proof satisfies
(T1) (T9) identical corresponding cases (Horrocks et al., 2000; Horrocks &
Sattler, 2001); (T10) holds definition L; (T11) definition L fact
nominals interpreted singleton sets; (T12) definition L, e, P together
716

fiKeys, Nominals Concrete Domains

semantics concepts g1 , . . . , gn .P ; (T13) since identity function Sc clearly
solution listed predicate conjunction; (T14) definition L e together
semantics key constraints; finally (T15) definition L e together
semantics concepts g.
direction, let = (Sa , Sc , L, E, e, P) tableau w.r.t. R K
let solution predicate conjunction (T13). construct model
follows:


:= Sa

AI

:= {s | L(s)}

concept names

NI

:= {s | N L(s)} nominals N
(

R NR \ NcF Trans(R)
v
* R {(s, t) | R E(s, t)}

S6=R
R :=
{(s, t) | R E(s, t)}+
R NR \ NcF Trans(R)

(x)
e(s, g) = x
g (s) :=
g NcF .
undefined e(s, g) undefined
Due (T11), interpretation nominals singleton. Moreover, interpretation
roles well-defined since role boxes acyclic. following claim central proving
indeed model D, R, K:
Claim: C cl(D, R, K), C L(s) implies C .
Proof: proceed induction norm ||C|| C, defined follows:
||A||
||g||
||C1 u C2 ||
||(> n R C)||

:=
:=
:=
:=

||A||
||u1 , . . . , un .P ||
||C1 C2 ||
||(6 n R C)||

:=
:=
:=
:=

0 concept name
0
1 + ||C1 || + ||C2 ||
1 + ||C||

concept names nominals N , claim follows definition AI N .
negation concept names nominals N (note C NNF), claim follows
definition AI N together (T1). Concepts C form C1 u C2 C1 C2
treated using (T2) (T3) together induction hypothesis. existential,
universal, number restrictions, proof analogous one SHIQ (Horrocks
et al., 2000). concepts form C = g1 , . . . gn .P L(s), C immediate
consequence (T12), definition giI , fact (x1 , . . . , xn ) P(P ) implies
((x1 ), . . . , (xn )) P (T13). Finally, concepts C = g, C immediate
consequence definition g together (T15). finishes proof
claim.
definition tableaux, exists s0 Sa C L(s0 ). claim,
s0 C thus model C.
Next, show model R. definition RI , obvious Trans(R)
R implies RI transitive relation. let v R R. Trans(R)
/ R,
RI definition RI . let Trans(R) R (s, t) . E(s, t),
(T4) implies R E(s, t), thus (s, t) RI . Otherwise, 0 v
*
Trans(S 0 ) R (s, t) {(u, v) | 0 E(u, v)}+ . (T4) together 0 v
* R implies
717

fiLutz, Areces, Horrocks, & Sattler

{(u, v) | 0 E(u, v)} {(u, v) | R E(u, v)}, thus Trans(R) R implies
(s, t) RI .
remains show model K. end, let (g1 , . . . , gn keyfor C) K
s, C giI (s) = giI (t) 1 n. Since predicate conjunction
(T13) contains explicit inequalities distinct concrete individuals, implies
e(s, gi ) = e(t, gi ) 1 n. (T10) implies {C, C} L(s) 6= {C, C} L(t) 6= .
C L(s), claim yields ( C)I contradicting C . Thus obtain
C L(s) and, similar way, C L(t). Finally, (T14) implies = t, thus
satisfies K.
proceed prove termination, soundness, completeness tableau algorithm
presented Section 4.2, starting termination. following, use |D, R, K|
denote | cl+ (D, R, K)|. Recall number polynomial size D, R, K.
Lemma C.2 (Termination). Let key-admissible concrete domain. started
SHOQK(D) concept NNF, role box R, path-free key box K NNF,
tableau algorithm terminates.
Proof. Assume D, R, K tableau algorithm terminate. Since key-admissible, means infinite sequence S0 , S1 , . . .
completion systems (a) S0 initial completion system SD (b) Si+1
result applying completion rule Si .
possible R R> rules applied infinitely often: easily seen
rules Ru, Rt, R6, Rc, R, R+ , Rch, Ra applied finitely often
completion systems whose set abstract nodes Va increase since either
add concepts node labels (whose size bounded), add concrete nodes (whose
number bounded linearly number abstract nodes), remove abstract
nodes forest. Hence sub-sequence Si1 , Si2 , . . . S0 , S1 , . . .
Sij result applying R R> rule Sij 1 . Let si` abstract node
R R> rule applied Si` 1 . Since implies
generated s, linear ordering well-founded. Thus, find infinite subsequence Sj1 , Sj2 , . . . Si1 , Si2 , . . . either sj` = sj`+1 ` 1 sj` sj`+1
` 1. former, however, possible since R R> rules
applied per node concept cl(D, R, K): even node removed,
label copying performed R6 rule together clashes type (C3) ensures
R> rule re-applied concept node. Thus second option
remains: subsequence Sj1 , Sj2 , . . . Si1 , Si2 , . . . sj` sj`+1
` 1. Let Lj labeling function Sj . Since abstract node labeled
subset Lj cl+ (D, R, K), nodes sjk sj` k fi ` Ljk (sjk ) = Lj` (sj` ).
node labels increase and, node removed, label conjoined
label node t. Thus node completion system Sj`
sj` Lj` (sj` ) Lj` (t). definition, sj` thus blocked Sj` , contradicting
assumption R R> rule applied sj` Sj` .
Lemma C.3 (Soundness). expansion rules applied SHOQK(D) concept
NNF, role box R, path-free key box K yield complete
clash-free completion forest, tableau w.r.t. R K.
718

fiKeys, Nominals Concrete Domains

Proof. Let = ((Va , Vc , E, L), P, c , ) complete clash-free completion system.
find solution (x) = (y) iff x c y: Rc rule updates
predicate conjunction , rule application c relation updated
using concrete equivalence D-tester returns (note satisfiable due
clash-freeness). According Definition 4.1, thus find solution required.
, define finite tableau = (Sa , Sc , E, L, P) follows:
Sa := {s Va | occurs blocked}
Sc := {(x) | (s, x) E(g) Sa g}
L(s) := L(s) cl(D, R, K) (the intersection due auxiliary concepts AnRC
),

E(s, t) := {R | R-successor blocks R-successor t0 s}

(x)
x g-successor
e(s, g) :=
undefined x g-successor
P := restriction P Sc .
Note function e well-defined due definition adding g-successors.
remains show satisfies (T1)(T14), basically consequence
clash-free complete.
(T1) satisfied since contain clash (C1).
(T2) satisfied since Ru rule cannot applied, thus C1 u C2 L(s) implies
C1 , C2 L(s).
(T3) satisfied since Rt rule cannot applied, thus C1 C2 L(s) implies
{C1 , C2 } L(s) 6= .
(T4), consider s, Sa R E(s, t) R v
* R0 . R E(s, t) implies
blocks R-successor s. definition successor, blocks
R0 -successor s, thus R0 E(s, t).
(T5), let R.C L(s) R E(s, t). R-successor s,
blocked implies C L(t) since R rule cannot applied. blocks
R-successor t0 s, blocked fact R rule cannot
applied yields C L(t0 ), blocking condition implies C L(t).
cases, thus C L(t).
(T6) (T7) satisfied reasons (T5) R replaced R
R+ .
(T8), consider (> n R C) L(s). Hence (> n R C) L(s)
completeness implies existence R-successors t1 , . . . , tn C L(ti )
.
ti =
6 tj 6= j. latter implies, 6= j, existence integers k, `
k 6= `, AnRC
L(ti ), AnRC
L(tj ). (T8) satisfied, remains
k
`
verify
ti block tj : case, blocking condition would imply
{AnRC
, AnRC
} L(ti ).
k
`
719

fiLutz, Areces, Horrocks, & Sattler

block ti tj 6= j: similarly, would imply
{AnRC
, AnRC
} L(t).
k
`
case, would clash (C3), contradiction clash-free.
(T9), consider (6 n R C) L(s). Hence (6 n R C) L(s) and, since
R6 rule cannot applied, n R-successors ti s. Since
ti either blocked blocked exactly one node (due linear
ordering), n ui Sa R E(s, ui ) C L(ui ).
(T10), let (6 n R C) L(s) R E(s, t). Hence (6 n R C) L(s)
either R-successor blocks R-successor s. first case, nonapplicability Rch rule implies {C, C} L(t) 6= . second case,
{C, C} L(t0 ) 6= t0 R-successor blocked t, thus blocking
condition yields {C, C} L(t) 6= . cases, implies {C, C} L(t) 6= .
Next, consider (g1 , . . . , gn keyfor C) K e(s, gi ) defined
i. Hence gi -successor i, thus blocked nonapplicability Rch rule imply {C, C} L(t) 6= .
(T11), consider N L(s) L(t). definition, N L(s) L(t) thus t.
Moreover, totality implies assume without loss generality
= t. Thus non-applicability Ra rule implies L(t) L(s),
thus blocked implies = t.
(T12) satisfied since rule Rc cannot applied.
(T13), clash-freeness implies satisfiability
^

^

P (x1 , . . . , xn ).

P used D,K (x1 ,...,xn )P(P )

choice , (x) = (y) iff x c y, thus (T13) satisfied.
(T14), let (g1 , . . . , gn keyfor C) K, C L(s) L(t), e(s, gi ) = e(t, gi ),
1 n. Thus C L(s) L(t) and, choice e , xi c yi
gi E(s, xi ) E(t, yi ). Hence t. Without loss generality, assume
= t. Thus non-applicability Ra rule implies L(t) L(s),
thus blocked implies = t.
(T15) satisfied definition since contain clash (C4).

Lemma C.4 (Completeness). SHOQK(D)-concept NNF tableau w.r.t.
role box R path-free key box K, expansion rules applied D, R,
K yield complete clash-free completion forest.
720

fiKeys, Nominals Concrete Domains

Proof. Given tableau = (Sa , Sc , L, E, e, P) w.r.t. R K, guide
non-deterministic rules Rt, Rch, Ra way rule application preserves
clash-freeness. together termination Lemma C.2 finishes proof.
Along rule application, perform stepwise construction total mapping
takes abstract nodes completion forest elements Sa concrete nodes
completion forest elements Sc .
L(s) cl(D, R, K) L((s)) Va ,
R-successor s, R E((s), (t)),
x g-successor s, e((s), g) = (x),
x c iff (x) = (y),
.
=
6 t, (s) 6= (t).
mapping satisfying four conditions called correct following. Note
completion system exists correct mapping contain clash: due
(T1) first property, encounter clash (C1). clash (C3) cannot occur
due last property. first third property together (T15) ensure
clash (C4) occur. Finally, clash (C2) cannot occur following reason:
construction P since edges labelled abstract features never removed,
tuple (x1 , . . . , xn ) P(P ), find abstract node paths u1 , . . . , un
u1 , . . . , un .P L(s) xi ui -successor 1 n. Thus, first, second,
third property together (T12) (T13) ensure conjunction
^
P ((x1 ), . . . , (xn ))
P used inD,K
(x1 ,...,xn )P(P )

solution ((x)) 6= ((x)) iff (x) 6= (y). fourth property, setting
0 (x) := ((x)) x Vc thus yields solution 0 .
total mapping inductively defined follows: let solution equation
(T13). Choose node s0 L(s0 ), set (s0 ) := s0 s0 (only) node
initial completion forest. Obviously, correct. show completion
rule applied way either still correct extended
correct mapping.
application rule Ru preserves correctness due (T2).
Due (T3), rule Rt applied correctness preserved.
rule R adds new node R.C L(s), correctness implies R.C
L((s)), thus (T6) implies existence Sa R E((s), t)
C L(t). Thus extending (t) := obviously yields correct mapping.
rule R> adds n nodes ti (> n R C) L(s), correctness implies
(> n R C) L((s)), thus (T8) implies existence t1 , . . . , tn Sa
ti 6= tj 6= j, R E((s), ti ), C L(ti ). Thus extending (ti ) := ti
obviously yields correct mapping.
721

fiLutz, Areces, Horrocks, & Sattler

Assume R6 rule applicable node (6 n R C) L(s)
n R-successors ti C L(ti ). correctness implies (6 n R C)
L((s)), R E((s), (ti )), C L(ti ). Thus, (T9), 6= j
.
(ti ) = (tj ). Again, correctness implies ti =
6 tj and, without loss generality, assume ti tj . Hence applying rule thereby merging L(tj )
L(ti ) preserves correctness.
rule Rc, extended similar way R: new gi -successor xi
added, extending (xi ) := e((s), gi ) yields correct . Moreover,
(T13) ensures c updated way fourth condition
preserved.
R rule, need extended, (T5), (T4), definition
R-successors imply correctness preserved.
R+ rule similar, difference (T7) takes place (T5).
Due (T10), rule Rch applied without violating correctness.
Ra , consider two reasons Ra applicable:
N L(s) L(t). correctness (T11) imply (s) = (t).
(g1 , . . . , gn keyfor C) K, C L(s) L(t), gi E(s, xi ) E(t, yi )
xi c yi 1 n. correctness implies e((s), gi ) = e((t), gi ),
thus (T14) together first property correctness imply (s) = (t).
cases, applying Ra preserves correctness.

immediate consequence Lemmas 4.2, C.2, C.3, C.4, tableau algorithm
always terminates answers satisfiable w.r.t. R K input
concept satisfiable w.r.t. input role box R input key box K. Since concept
satisfiability w.r.t. TBoxes reduced concept satisfiability without TBoxes,
obtain following result:
Proposition C.5. Let key-admissible concrete domain. tableau algorithm
decides satisfiability SHOQK(D) concepts w.r.t. TBoxes, role boxes, path-free key
boxes.
hard verify proof Lemma C.4 together Lemmas 4.2 C.2
yield bounded model property SHOQK(D), bound exponential.
Corollary C.6. SHOQK(D)-concept satisfiable w.r.t. role box R pathfree key box K, satisfiable w.r.t. R K model size |I | 2m
= # cl+ (D, R, K).
722

fiKeys, Nominals Concrete Domains

Proof. SHOQK(D)-concept satisfiable w.r.t. role box R path-free key
box K, Lemma C.4 implies tableau algorithm constructs complete clash-free
completion forest D, R, K. definition blocking, number abstract
nodes completion forest blocked bounded 2m : 6= Va
abstract nodes completion forest L(s) = L(t), either blocks t, blocks s,
blocked another node u. Moreover, easily seen number
concrete successors per abstract node bounded number concrete features C, R,
K. Now, proof Lemma C.4, abstract nodes tableau constructed
complete clash-free completion forest coincide nodes blocked
completion forest. Finally, proof Lemma 4.2 interpretation domain
model constructed tableau coincides abstract nodes tableau.
Summing up, SHOQK(D)-concept satisfiable w.r.t. R K model size
|I | 2m .

References
Areces, C., Blackburn, P., & Marx, M. (1999). road-map complexity hybrid logics.
Flum, J., & Rodrguez-Artalejo, M. (Eds.), Computer Science Logic, No. 1683
Lecture Notes Computer Science, pp. 307321. Springer-Verlag.
Baader, F., Horrocks, I., & Sattler, U. (2002a). Description logics semantic web. KI
Kunstliche Intelligenz, 16 (4), 5759.
Baader, F., Lutz, C., Sturm, H., & Wolter, F. (2002b). Fusions description logics
abstract description systems. Journal Artificial Intelligence Research (JAIR), 16,
158.
Baader, F., & Sattler, U. (1998). Description logics concrete domains aggregation. Prade, H. (Ed.), Proceedings 13th European Conference Artificial
Intelligence (ECAI98), pp. 336340. John Wiley & Sons.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (2003).
Description Logic Handbook: Theory, implementation applications. Cambridge University Press, Cambridge, MA, USA.
Baader, F., & Hanschke, P. (1991a). scheme integrating concrete domains concept
languages. Proceedings 12th International Joint Conference Artificial
Intelligence (IJCAI-91), pp. 452457, Sydney, Australia.
Baader, F., & Hanschke, P. (1991b). scheme integrating concrete domains concept
languages. DFKI research report RR-91-10, German Research Center Artificial
Intelligence (DFKI).
Baader, F., & Hanschke, P. (1992). Extensions concept languages mechanical
engineering application. Proceedings 16th German AI-Conference (GWAI92), Vol. 671 Lecture Notes Computer Science, pp. 132143. Springer-Verlag.
Baader, F., & Sattler, U. (2000). Tableau algorithms description logics. Dyckhoff,
R. (Ed.), Proceedings International Conference Automated Reasoning
Tableaux Related Methods (Tableaux 2000), Vol. 1847 Lecture Notes Artificial
Intelligence, pp. 118. Springer-Verlag.
723

fiLutz, Areces, Horrocks, & Sattler

Berger, R. (1966). undecidability domino problem. Memoirs American
Mathematical Society, 66, 172.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). semantic web. Scientific American,
284 (5), 3443.
Borger, E., Gradel, E., & Gurevich, Y. (1997). Classical Decision Problem. Perspectives
Mathematical Logic. Springer-Verlag.
Borgida, A., & Patel-Schneider, P. F. (1994). semantics complete algorithm
subsumption CLASSIC description logic. Journal Artificial Intelligence Research, 1, 277308.
Borgida, A., & Weddell, G. E. (1997). Adding uniqueness constraints description logics
(preliminary report). Bry, F., Ramakrishnan, R., & Ramamohanarao, K. (Eds.),
Proceedings 5th International Conference Deductive Object-Oriented
Databases (DOOD97), Vol. 1341 LNCS, pp. 85102. Springer.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998). decidability query
containment constraints. Proceedings 17th ACM SIGACT-SIGMODSIGART Symposium Principles Database Systems (PODS98), pp. 149158.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (2000). Keys free description logics.
Baader, F., & Sattler, U. (Eds.), Proceedings 2000 International Workshop
Description Logics (DL2000), No. 33 CEUR-WS (http://ceur-ws.org/), pp. 7988.
Calvanese, D., Lenzerini, M., & Nardi, D. (1998). Description logics conceptual data
modeling. Chomicki, J., & Saake, G. (Eds.), Logics Databases Information
Systems, pp. 229263. Kluwer Academic Publisher.
Dean, M., Connolly, D., van Harmelen, F., Hendler, J., Horrocks, I., McGuinness, D. L.,
Patel-Schneider, P. F., & Stein, L. A. (2002). Web ontology language (OWL) reference
version 1.0. W3C Working Draft.
Fensel, D., van Harmelen, F., Horrocks, I., McGuinness, D. L., & Patel-Schneider, P. F.
(2001). OIL: ontology infrastructure semantic web. IEEE Intelligent
Systems, 16 (2), 3845.
Graham, R. L., Knuth, D. E., & Patashnik, O. (1990). Concrete Mathematics. Addison
Wesley Publ. Co., Reading, Massachussetts.
Haarslev, V., Lutz, C., & Moller, R. (1998). Foundations spatioterminological reasoning
description logics. Cohn, A., Schubert, L., & S.C.Shapiro (Eds.), Proceedings
6th International Conference Principles Knowledge Representation
Reasoning (KR98), pp. 112124. Morgan Kaufman.
Haarslev, V., & Moller, R. (2001). RACER system description. Gore, R., Leitsch,
A., & Nipkow, T. (Eds.), Proceedings 1st International Joint Conference
Automated Reasoning (IJCAR01), No. 2083 Lecture Notes Artificial Intelligence,
pp. 701705. Springer-Verlag.
Haarslev, V., Moller, R., & Wessel, M. (2001). description logic ALCN HR+ extended
concrete domains: practically motivated approach. Gore, R., Leitsch, A.,
724

fiKeys, Nominals Concrete Domains

& Nipkow, T. (Eds.), Proceedings 1st International Joint Conference Automated Reasoning IJCAR01, No. 2083 Lecture Notes Artificial Intelligence, pp.
2944. Springer-Verlag.
Halpern, J. Y., & Moses, Y. (1992). guide completeness complexity modal
logics knowledge belief. Artificial Intelligence, 54 (3), 319380.
Hollunder, B., & Baader, F. (1991). Qualifying number restrictions concept languages.
Proceedings 2nd International Conference Principles Knowledge Representation Reasoning (KR91), pp. 335346, Boston, MA, USA.
Hopcroft, J. E., & Ullman, J. D. (1979). Introduction Automata Theory, Languages
Computation. Addison-Wesley.
Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning expressive description logics. Logic Journal IGPL, 8 (3), 239264.
Horrocks, I. (1998). Using expressive description logic: FaCT fiction?. Proceedings
6th International Conference Principles Knowledge Representation
Reasoning (KR98), pp. 636647.
Horrocks, I. (2002). Reasoning expressive description logics: Theory practice.
Voronkov, A. (Ed.), Proceedings 18th International Conference Automated
Deduction (CADE 2002), No. 2392 Lecture Notes Artificial Intelligence, pp. 115.
Springer.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2002). Reviewing design
DAML+OIL: ontology language semantic web. Proceedings 18th
National Conference Artificial Intelligence (AAAI 2002), pp. 792797.
Horrocks, I., & Sattler, U. (2001). Ontology reasoning SHOQ(D) description logic.
Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial
Intelligence (IJCAI01), pp. 199204. Morgan-Kaufmann.
Horrocks, I., Sattler, U., & Tobies, S. (1999). Practical reasoning expressive description
logics. Ganzinger, H., McAllester, D., & Voronkov, A. (Eds.), Proceedings
6th International Conference Logic Programming Automated Reasoning
(LPAR99), No. 1705 Lecture Notes Artificial Intelligence, pp. 161180. SpringerVerlag.
Kamp, G., & Wache, H. (1996). CTL - description logic expressive concrete domains.
Tech. rep. LKI-M-96/01, Laboratory Artificial Intelligence (LKI), Universitity
Hamburg, Germany.
Khizder, V. L., Toman, D., & Weddell, G. E. (2001). decidability complexity description logics uniqueness constraints. den Bussche, J. V., & Vianu, V. (Eds.),
Proceedings 8th International Conference Database Theory (ICDT2001), Vol.
1973 LNCS, pp. 5467. Springer.
Knuth, D. (1968). Art Computer Programming, Vol. 1. Addison-Wesley.
Lutz, C. (2003). Description logics concrete domainsa survey. Advances Modal
Logics Volume 4, pp. 265296. World Scientific Publishing Co. Pte. LTd.
725

fiLutz, Areces, Horrocks, & Sattler

Lutz, C. (2002a). Complexity Reasoning Concrete Domains. Ph.D. thesis,
LuFG Theoretical Computer Science, RWTH Aachen, Germany.
Lutz, C. (2002b). PSpace reasoning description logic ALCF(D). Logic Journal
IGPL, 10 (5), 535568.
Lutz, C. (2002c). Reasoning entity relationship diagrams complex attribute
dependencies. Horrocks, I., & Tessaris, S. (Eds.), Proceedings International
Workshop Description Logics 2002 (DL2002), No. 53 CEUR-WS (http://ceurws.org/), pp. 185194.
Lutz, C. (2004). NExpTime-complete description logics concrete domains. ACM
Transactions Computational Logic, 5 (4), 669705.
Lutz, C., Areces, C., Horrocks, I., & Sattler, U. (2002). Keys, nominals, concrete
domains. LTCS-report 02-04, Technical University Dresden. See http://lat.inf.tudresden.de/research/reports.html.
Lutz, C., Areces, C., Horrocks, I., & Sattler, U. (2003). Keys, nominals, concrete
domains. Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI03), pp. 349354. Morgan-Kaufmann Publishers.
Pan, J. Z., & Horrocks, I. (2002). Reasoning SHOQ(Dn ) description logic. Horrocks, I., & Tessaris, S. (Eds.), Proceedings International Workshop Description Logics 2002 (DL2002), No. 53 CEUR-WS (http://ceur-ws.org/), pp. 5362.
Post, E. M. (1946). variant recursively unsolvable problem. Bulletin American
Mathematical Society, 52, 264268.
Schild, K. D. (1991). correspondence theory terminological logics: Preliminary report.
Mylopoulos, J., & Reiter, R. (Eds.), Proceedings 12th International Joint
Conference Artificial Intelligence (IJCAI-91), pp. 466471. Morgan Kaufmann.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions complements. Artificial Intelligence, 48 (1), 126.

726

fiJournal Artificial Intelligence Research 23 (2005) 421-440

Submitted 07/04; published 04/05

Practical use Variable Elimination Constraint
Optimization Problems: Still-life Case Study
Javier Larrosa
Enric Morancho
David Niso

larrosa@lsi.upc.edu
enricm@ac.upc.edu
niso57@casal.upc.edu

Universitat Politecnica de Catalunya
Jordi Girona 1-3, 08034 Barcelona, Spain

Abstract
Variable elimination general technique constraint processing. often discarded high space complexity. However, extremely useful
combined techniques. paper study applicability variable elimination challenging problem finding still-lifes. illustrate several alternatives:
variable elimination stand-alone algorithm, interleaved search, source
good quality lower bounds. show techniques best known option
theoretically empirically. experiments able solve n = 20
instance, far beyond reach alternative approaches.

1. Introduction
Many problems arising domains resource allocation (Cabon, de Givry, Lobjois,
Schiex, & Warners, 1999), combinatorial auctions (Sandholm, 1999), bioinformatics
probabilistic reasoning (Pearl, 1988) naturally modeled constraint satisfaction
optimization problems. two main solving schemas search inference. Search
algorithms constitute usual solving approach. transform problem set
subproblems selecting one variable instantiating different alternatives.
Subproblems solved applying recursively transformation rule. recursion
defines search tree normally traversed depth-first manner,
benefit requiring polynomial space. practical efficiency search algorithms
greatly depends ability detect prune redundant subtrees. worst-case,
search algorithms need explore whole search tree. Nevertheless, pruning techniques
make much effective.
Inference algorithms (also known decomposition methods) solve problem sequence transformations reduce problem size, preserving optimal cost.
well known example bucket elimination (BE, also known variable elimination) (Bertele
& Brioschi, 1972; Dechter, 1999). algorithm proceeds selecting one variable
time replacing new constraint summarizes effect chosen variable. main drawback new constraints may large arities require
exponentially time space process store. However, nice property
worst-case time space complexities tightly bounded structural parameter called induced width. exponential space complexity limits severely algorithms
c
2005
AI Access Foundation. rights reserved.

fiLarrosa, Morancho & Niso

practical usefulness. Thus, constraint satisfaction community variable elimination
often disregarded.
paper consider challenging problem finding still-lifes stable
patterns maximum density game life. academic problem recently
included CSPlib repository1 dedicated web page2 set maintain
up-to-date results. Bosch Trick (2002), still-life problem solved using two
different approaches: integer programming constraint programming, based
search. None could solve n = 8 problem within reasonable time.
best results obtained hybrid approach combines two techniques
exploits problem symmetries order reduce search space. algorithm,
solved n = 15 case 8 days cpu. Smith (2002) proposed interesting
alternative using pure constraint programming techniques, solving problem
dual form. work, Smith could improve n = 15 limit. Although explicitly
2
mentioned, two works use algorithms worst-case time complexity O(2(n ) ).
paper show usefulness variable elimination techniques. First apply
plain BE. could expected, observe competitive stateof-the-art alternatives. Next, introduce sophisticated algorithm combines
search variable elimination (following ideas Larrosa & Dechter, 2003) uses
lower bound based mini-buckets (following ideas Kask & Dechter, 2001).
algorithm, solve one minute n = 15 instance. able solve
n = 20 instance, far beyond reach previous techniques. readability
reasons, describe main ideas omit algorithmic details.3
structure paper following: next Section give preliminary
definitions. Section 3 solve problem plain BE. Section 4 introduce
hybrid algorithm obtained results reported Section 5. Section 6
discuss ideas explored article extended domains. Besides,
report additional experimental results. Finally, Section 7 gives conclusions
lines future work.

2. Preliminaries
Section first define still-life problem. Next, define weighted CSP
framework formulate still-life weighted CSP. Finally, review main
solving techniques weighted CSPS.
2.1 Life Still-Life
game life (Gardner, 1970) played infinite checkerboard, square
called cell. cell eight neighbors: eight cells share one two corners
it. player places checkers cells. checker it, cell
alive, else dead. state board evolves iteratively according following
three rules: (1) cell exactly two living neighbors state remains
1. www.csplib.org
2. www.ai.sri.com/~ nysmith/life
3. interested reader find extended version, along source code implementation
www.lsi.upc.edu/~ larrosa/publications

422

fiOn practical use variable elimination

3

1

3

1

1

3

1

Xc

4

2



2

2

B

2



C

E

Figure 1: A: 3 3 still-life. B: constraint graph simple WCSP instance four
variables three cost functions. C: constraint graph assigning variable
x4 . D: constraint graph clustering variables x3 x4 . E: constraint
graph eliminating variable x4 .

next iteration, (2) cell exactly three living neighbors alive
next iteration (3) cell fewer two three living neighbors,
dead next iteration. Although defined terms extremely simple rules,
game life proven mathematically rich attracted interest
mathematicians computer scientists.
still-life problem SL(n) consist finding nn stable pattern maximum density
game life. cells outside pattern assumed dead. Considering
rules game, easy see cell (i, j) must satisfy following three
conditions: (1) cell alive, must exactly two three living neighbors, (2)
cell dead, must three living neighbors, (3) cell grid
boundary (i.e, = 1 = n j = 1 j = n), cannot part sequence three
consecutive living cells along boundary. last condition needed three
consecutive living cells boundary would produce living cells outside grid.
Example 1 Figure 1.A shows solution SL(3). easy verify cells
satisfy previous conditions, hence stable. pattern optimal 6
living cells 3 3 stable pattern 6 living cells exists.

2.2 Weighted CSP
weighted constraint satisfaction problem (WCSP) (Bistarelli, Montanari, & Rossi, 1997)
defined tuple (X, D, F), X = {x1 , . . . , xn } set variables taking values
finite domains Di D. F set weighted constraints (i.e., cost functions).
f F defined subset variables, var(f ), called scope. objective
function sum functions F,
F =

X

f

f F

goal find instantiation variables minimizes objective function.
Example 2 Consider WCSP four variables X = {xi }4i=1 domains Di = {0, 1}
three cost functions: f1 (x1 , x4 ) = x1 + x4 , f2 (x2 , x3 ) = x2 x3 f3 (x2 , x4 ) = x2 + x4 .
423

fiLarrosa, Morancho & Niso

objective function F (x1 , x2 , x3 , x4 ) = x1 + x4 + x2 x3 + x2 + x4 . Clearly, optimal
cost 0, obtained every variable taking value 0.
Constraints given explicitly means tables, implicitly mathematical
expressions computing procedures. Infeasible partial assignments specified constraints assign cost them. assignment value variable xi noted
xi = a. partial assignment tuple = (xi1 = v1 , xi2 = v2 , , xij = vj ). extension
xi = noted (xi = a). WCSPs instances graphically depicted means
interaction constraint graph, one node per variable one edge connecting two nodes appear scope cost function. instance,
Figure 1.B shows constraint graph problem previous example.
2.3 Overview Solving Techniques
Subsection review solving techniques widely used reasoning
constraints.
2.3.1 search
WCSPs typically solved depth-first search. Search algorithms defined
terms instantiating functions,
Definition 1 Let P = (X, D, F) WCSP instance, f function F, xi variable
var(f ), v value Di . Instantiating f xi = v new function scope
var(f ) {xi } returns tuple t, f (t (xi = v)). Instantiating P xi = v
new problem P |xi =v = (X {xi }, {Di }, F 0 ), F 0 obtained instantiating
functions F mention xi xi = v.
instance, instantiating problem Example 2 x4 = 1, produces new
problem three variables {xi }3i=1 three cost functions: f1 (x1 , x4 = 1) = x1 + 1,
f2 (x2 , x3 ) = x2 x3 f3 (x2 , x4 = 1) = x2 + 1. Figure 1.C shows corresponding
constraint graph, obtained original graph removing instantiated variable x4
adjacent edges. Observe new graph depends instantiated variable,
depend value assigned it.
Search algorithms transform current problem P set subproblems. Usually
done selecting one variable xi instantiated different domain values
(P |xi =v1 , P |xi =v2 , , P |xi =vd ). transformation called branching. subproblem process recursively applied, defines tree subproblems. Search
algorithms expand subproblems trivial case achieved: variable left,
pruning condition detected. optimization problems, pruning conditions usually
defined terms lower upper bounds. Search keeps cost best solution
far, upper bound optimal cost. node, lower bound best
cost obtainable underneath computed. lower bound greater equal
upper bound, safe backtrack.
size search tree O(dn ) (being size largest domain) bounds
time complexity. tree traversed depth-first, space complexity polynomial.
424

fiOn practical use variable elimination

2.3.2 clustering
well-known technique constraint processing clustering (Dechter & Pearl, 1989).
merges several variables one meta-variable, preserving problem semantics.
Clustering variables xi xj produces meta-variable xk , whose domain Di Dj . Cost
functions must accordingly clustered. instance, problem Example 2, clustering variables x3 x4 produces variable xc domain Dc = {(0, 0), (0, 1), (1, 0), (1, 1)}.
Cost functions f2 f3 clustered fc (x2 , xc ) = f2 + f3 . new variable
notation fc = x2 xc [1] + x2 + xc [2], xc [i] denotes i-th component xc . Function
f1 needs reformulated f1 (x1 , xc ) = x1 + xc [2]. constraint graph resulting
problem obtained merging clustered variables connecting meta-node
nodes adjacent clustered variables. Figure 1.D shows constraint graph clustering x3 x4 . typical use clustering transform
cyclic constraint graph acyclic one, solved efficiently thereafter.
2.3.3 variable elimination
Variable elimination based following two operations,
Definition 2 sum two functions f g, noted (f + g), new function
scope var(f ) var(g) returns tuple sum costs f g,
(f + g)(t) = f (t) + g(t)
Definition 3 elimination variable xi f , noted f xi , new function
scope var(f ) {xi } returns tuple cost best extension xi ,
(f xi )(t) = min {f (t (xi = a))}
aDi

Observe f unary function (i.e., arity one), eliminating variable
scope produces constant.
Definition 4 Let P = (X, D, F) WCSP instance. Let xi X arbitrary variable
let Bi set cost functions xi scope (Bi called bucket
xi ). define gi
X
gi = (
f ) xi
f Bi

elimination xi transforms P new problem P xi = {X {xi }, {Di }, (F
Bi ) {gi }}. words, P xi obtained replacing xi functions bucket
gi .
P P xi optimal cost because, construction, gi compensates
absence xi . constraint graph P xi obtained forming clique
nodes adjacent node xi removing xi adjacent edges. example,
eliminating x4 problem Example 2 produces new problem three variables
{xi }3i=1 two cost functions: f2 g4 . scope g4 {x1 , x2 } defined as,
425

fiLarrosa, Morancho & Niso

g4 = (f1 + f3 ) x4 = (x1 + x4 + x2 + x4 ) x4 = x1 + x2 . Figure 1.D shows constraint
graph elimination.
previous example, new function g4 could expressed mathematical expression. Unfortunately, general, result summing functions eliminating variables
cannot expressed intensionally, new cost functions must stored extensionally
tables. Consequently, space complexity computing P xi proportional numQ
ber entries gi , is: ( xj var(gi ) |Dj |). Since xj var(gi ) iff xj adjacent xi
Q
constraint graph, previous expression rewritten ( xj N (i,GP ) |Dj |),
GP constraint graph P N (i, GP ) set neighbors xi GP .
time complexity computing P xi space complexity multiplied cost
computing entry gi .
Bucket elimination (BE) works two phases. first phase, eliminates variables
one time reverse order. elimination xi , new gi function computed
added corresponding bucket. elimination x1 produces empty-scope
function (i.e., constant) optimal cost problem. second phase,
considers variables increasing order generates optimal assignment variables.
time space complexity exponential structural parameter
constraint graph, called induced width, captures maximum arity among
gi functions. Without additional overhead also compute number optimal
solutions (see Dechter, 1999, details).
2.3.4 super-buckets
cases, may convenient eliminate set variables simultaneously (Dechter
& Fatah, 2001). elimination set variables performed collecting
set functions mentioning least one variable . Variables functions
replaced new function gY defined as,
gY = (

X

f)

f

set called super-bucket. Note elimination seen
clustering variables meta-variable xY followed elimination.
2.3.5 mini-buckets
space complexity high, approximation, called mini buckets
(Dechter & Rish, 2003), used. Consider elimination xi , associated
bucket Bi = {fi1 , . . . , fik }. would compute,
gi = (

X

f ) xi

f Bi

time space complexity computation depends arity gi . beyond
available resources, partition bucket Bi so-called mini-buckets Bi1 , . . . , Bik
number variables scopes mini-bucket bounded parameter.
compute,
X
gij = (
f ) xi , j = 1..k
f Bij

426

fiOn practical use variable elimination

6

7

6

1

2

2
2
8

9
3

5

1

8

2

8

9

9
3

1

3

1

8

9

5

5

5

4

4
4

4



B

C



Figure 2: constraint graph evolution sequence variable eliminations
instantiations.

gij bounded arity. Since,
gij

gi

zX }|

(

f Bi

{

f ) xi

k z X }|
X

(

{

f ) xi

j=1 f Bij

elimination variables using mini-buckets yields lower bound actual optimal
cost.
2.3.6 combining search variable elimination
plain costly space, combine search (Larrosa & Dechter,
2003). Consider WCSP whose constraint graph depicted Figure 2.A. Suppose
want eliminate variable want compute store constraints
arity higher two. take consideration variables connected
two variables. example, variable x7 one selected.
elimination transforms problem another one whose constraint graph depicted
Figure 2.B. x6 degree decreased two, also eliminated.
new constraint graph depicted Figure 2.C. point, every variable degree
greater two, switch search schema selects variable, say x3 , branches
values produces set subproblems, one value domain.
constraint graph, depicted Figure 2.D. subproblem,
possible eliminate variable x8 x4 . elimination possible eliminate
x2 x9 , subsequently x5 x1 . Eliminations branching done
every subproblem since new constraints eliminated variables replaced
differ one subproblem another. example, one branching made.
Therefore, elimination variables reduced search tree size d9 d,
size domains. example, bounded arity new constraints
two, generalized arbitrary value.

3. Solving Still-life Variable Elimination
SL(n) easily formulated WCSP. natural formulation associates one
variable xij cell (i, j). variable two domain values. xij = 0 cell
427

fiLarrosa, Morancho & Niso

X1
j 2

j 1

j

2

j+1

j+2

X2

1

X3



X4

i+1

X5

+2

X6

B



Figure 3: A: Structure constraint graph SL(n). node center, associated
cell (i, j), linked cells interacts with. shadowed area indicates
scope fij . B (left): Constraint graph SL(6) clustering cells
row variables. B (from left right: Evolution constraint graph
execution BE.

dead, xij = 1 alive. cost function fij variable xij . scope
fij xij neighbors. evaluates stability xij : xij unstable given
neighbors, fij returns ; else fij returns 1 xij .4 objective function minimized
is,
F =

n X
n
X

fij

i=1 j=1

instantiation X represents unstable pattern, F (X) returns ; else returns
number dead cells. fij stored table 29 entries evaluated constant
time.
Figure 3.A illustrates structure constraint graph SL(n). picture shows
arbitrary node xij linked nodes interacts with. instance, edge
xij xi,j+1 xi,j+1 neighbor xij grid and, consequently,
variables scope fij . edge xij xi1,j2
cells neighbors xi1,j1 grid and, therefore, appear scope
fi1,j1 . shadowed area represents scope fij (namely, xij neighbors).
complete graph obtained extending connectivity pattern nodes
graph.
sake clarity, use equivalent compact SL(n) formulation
makes easier describe implement: cluster variables row
single meta-variable. Thus, xi denotes state cells i-th row (namely,
xi = (xi1 , xi2 , . . . , xin ) xij {0, 1}). Accordingly, takes values sequences
n bits or, equivalently, natural numbers interval [0..2n 1]. Cost functions
accordingly clustered: cost function fi associated row i, defined as,
fi =

n
X

fij

j=1

4. Recall that, WCSP, task minimize number dead cells. Therefore, give cost 1
dead cells cost 0 living cells.

428

fiOn practical use variable elimination

internal rows, scope fi {xi1 , xi , xi+1 }. cost function top row, f1 ,
scope {x1 , x2 }. cost function bottom row, fn , scope {xn1 , xn }.
unstable cell xi , fi (xi1 , xi , xi+1 ) = . Else, returns number dead cells
xi . Evaluating fi (n) bits arguments need checked.
new, equivalent, objective function is,
F =

n
X

fi

i=1

Figure 3.B (left) shows constraint graph SL(6) formulation. arbitrary
variable xi connected two variables two variables below.
sequential structure constraint graph makes intuitive. eliminates variables
decreasing orders. elimination xi produces new function gi = (fi1 + gi+1 ) xi
scope {xi2 , xi1 }. Figure 3.B (from left right) shows evolution constraint
graph along elimination variables. Formally, applies recursion transforms
subproblem P P xi , xi variable P highest index. satisfies
following property,
Property 1 Let gi function added replace xi . gi (a, b) cost
best extension (xi2 = a, xi1 = b) eliminated variables (xi , . . . , xn ). Formally,
gi (a, b) =

min

vi Di ,...,vn Dn

{fi1 (a, b, vi ) + fi (b, vi , vi+1 ) +

+fi+1 (vi , vi+1 , vi+2 ) + . . .
+fn1 (vn2 , vn1 , vn ) + fn (vn1 , vn )}
gi (a, b) = , means pattern a, b cannot extended inferior rows
stable pattern. gi (a, b) = k (with k 6= ), means a, b extended
optimal extension k dead cells xi1 xn .
space complexity (n 22n ), due space required store n functions
gi extensionally (2n 2n entries each). Regarding time, computing entry gi
cost (n 2n ) (finding minimum 2n alternatives, computation one
(n)). Since gi 22n entries, total time complexity (n2 23n ). Observe
solving SL(n) exponential improvement search algorithms,
2
time complexity O(2n ).
Table 4 reports empirical results. obtained 2 Ghz Pentium IV
machine 2 Gb memory. first columns reports problem size, second
reports optimal cost number dead cells (in parenthesis, number living
cells), third column reports number optimal solutions. count different
two solutions even one transformed problem symmetry.
fourth column reports CPU time seconds. fifth, sixth seventh
columns report results obtained three approaches tried Bosch Trick
(2002):5 constraint programming (CP), integer programming (IP), sophisticated
algorithm (CP/IP) combines CP IP, exploits problem symmetries.
5. corresponding OPL code available http://mat.gsia.cmu.edu/LIFE.

429

fiLarrosa, Morancho & Niso

n
5
6
7
8
9
10
11
12
13
14
15

opt
9(16)
18(18)
21(28)
28(36)
38(43)
46(54)
57(64)
68(76)
79(90)
92(104)
106(119)

n. sol.
1
48
2
1
76
3590
73
129126
1682
11
?


0
0
0
0
4
27
210
1638
13788
105
*

CP
0
0
4
76
> 600
*
*
*
*
*
*

IP
0
1
3
26
> 600
*
*
*
*
*
*

CP/IP
0
0
0
2
20
60
153
11536
12050
5 105
7 105

Figure 4: Experimental results four different algorithms still-life problem. Times
seconds.

observed clearly outperforms CP IP orders magnitude.
n = 14 case largest instance could solve due exhausting available
space. Comparing CP/IP, observe clear winner. additional
observation scales regularly, execution requiring roughly eight times
time four times space previous, clear accordance
algorithm complexity.

4. Combining Search Variable Elimination
One way overcome high space complexity combine search variable
elimination hybrid approach HYB (Larrosa & Schiex, 2003). idea use search
(i.e, instantiations) order break problem independent smaller parts
variable elimination efficiently performed.
Let us reformulate problem convenient way hybrid algorithm.
sake simplicity without loss generality consider n even. cluster
R
row variables three meta-variables: xC
denotes two central cells row i, xi
n
L
xi denote 2 1 remaining cells right left, respectively (see Figure 5.A).
L
R
Consequently, xC
takes values range [0..3], xi xi take values range
n
1
[0..2 2 1]. Cost functions accordingly clustered,
n

fiL

=

2
X

fiR =

fij ,

n
X
j= n
+1
2

j=1

new, equivalent, objective function is,
F =

n
X

(fiL + fiR )

i=1

430

fij

fiOn practical use variable elimination

Left

X

L
1

Center

X

C
1

Right

X

Left

R
1

Center

Right

X1

X2

X3
X

L


X

C


X

R


X4

X

L
n

X

C
n

X

X5

R
n

X6

B


Left

Center

Right

Left

Center

Right

X1
X1
X2
X2
X3
X3
X4
X4
X5
X5
X6
X6
C



Figure 5: Formulation SL(n) used hybrid algorithm. A: row clustered
three variables. B: Constraint graph SL(6). C: Constraint graph
C
C
assignment xC
n , xn1 xn2 . D: Constraint graph elimination
L
R
xn xn .

431

fiLarrosa, Morancho & Niso

C
L C
L
C
scopes internal row functions, fiL fiR , {xL
i1 , xi1 , xi , xi , xi+1 , xi+1 }
C
R
C
R
C
R
L
R
L
C
C
{xi1 , xi1 , xi , xi , xi+1 , xi+1 }. Top functions f1 f1 scopes {x1 , x1 , xL
2 , x2 }
R C
R
L
R
L
C
L C
{xC
1 , x1 , x2 , x2 }. Bottom functions fn fn scopes {xn1 , xn1 , xn , xn }
R
C
R
C
{xn1 , xn1 , xn , xn }. Figure 5.B shows corresponding constraint graph. imporR
tance formulation xL
xi independent (i.e, edge
constraint graph connecting left right variables).

hybrid algorithm HYB searches central variables eliminates lateral
variables. Variables considered decreasing order index. Thus, algorithm
C
C
starts instantiating xC
n , xn1 xn2 , produces subproblem constraint
R
graph shown Figure 5.C. Observe variable xL
n (respectively, xn ) connected
L
L
R
R
variables xn1 xn2 (respectively, xn1 xn2 ). eliminated producing
L
R
R
R
new function gnL scope {xL
n2 , xn1 } (respectively, gn scope {xn2 , xn1 }).
Figure 5.D shows resulting constraint graph. Lateral variables domains size
n
n
2 2 1 . Hence, elimination space (2n ) time (23 2 ). important note
C
C
eliminations subject current assignment xC
n , xn1 xn2 . Therefore,
recomputed value change. elimination xL
n
C
xR
,

algorithm
would
assign
variable
x


make
possible

elimination

n
n3
L
R
C
xn1 xn1 , on. arbitrary level search, algorithm assigns xi ,
R
makes xL
i+2 xi+2 independent central columns related two
L
variables above. Then, eliminates replacing variables functions gi+2
R scopes {xL , xL } {xR , xR }, respectively. Formally, HYB applies recursion
gi+2

i+1

i+1
transforms subproblem P 4 simpler subproblems {((P |xC =v ) xL ) xR }3v=0 .

i+2
i+2
satisfies following property,

Property 2 Let giL function computed HYB used replace variable xL
.
L
giL (a, b) cost best extension (xL
=
a,
x
=
b)

eliminated
variables
i2
i1
L
R
(xL
, . . . , xn ), conditioned current assignment. Similarly, right side, gi (a, b)
R
R
R
cost best extension (xi2 = a, xi1 = b) eliminated variables (xi , . . . , xR
n ),
conditioned current assignment.

L (a, b) among comA consequence previous Property minimum gi+2
binations b lower bound best cost obtained left
L (a, b)} +
part grid continue current line search. Therefore, mina,b {gi+2
R
mina,b {gi+2 (a, b)} valid lower bound current node used pruning
purposes.

space complexity algorithm (n 2n ), due giL giR functions
need explicitly stored. time complexity O(n 23.5n ), O(4n )
nodes may visited (n variables domains size 4) cost processing
n
node (n 23 2 ) due variable eliminations.
Thus, comparing BE, time complexity increases (n2 23n ) O(n23.5n ).
prize HYB pays space decrement (n 22n ) (n 2n ).
432

fiOn practical use variable elimination

4.1 Refining Lower Bound
well-known average-case efficiency search algorithms depends greatly
lower bound use. algorithm using poor lower bound based giL
giR functions, only.
Kask Dechter (2001) proposed general method incorporate information
yet-unprocessed variables lower bound. Roughly, idea run mini buckets
(MB) prior search save intermediate functions future use. MB executed using
reverse order search instantiate variables. execution MB
completed, search algorithm executed. node, uses mini-bucket functions
compiled look-ahead information. Subsection, show adapted
idea SL(n) integrated HYB.
C
R
Consider SL(n) formulated terms left, central right variables (xL
, xi , xi ).
L
C
R
exact elimination first row variables (x1 , x1 , x1 ) done using super-bucket
B1 = {f1L , f1R , f2L , f2R } computing function,
C
R
h1 = (f1L + f1R + f2L + f2R ) {xL
1 , x1 , x1 }
C
R L C
R
scope h1 {xL
2 , x2 , x2 , x3 , x3 , x3 }. Using mini-buckets idea, partition
L
L
L
R
bucket B1 = {f1 , f2 } B1 = {f1R , f2R }. Then, approximate h1 two smaller
R
functions hL
1 h1 ,
L
L
L C
hL
1 = (f1 + f2 ) {x1 , x1 }
R
R
C
R
hR
1 = (f1 + f2 ) {x1 , x1 }
R
L C
L C
C
R C
R
scopes hL
1 h1 {x2 , x2 , x3 , x3 } {x2 , x2 , x3 , x3 }, respectively.
idea repeated row row increasing order. general, processing row i, yields two
functions,
L
L
L C
hL
= (hi1 + fi+1 ) {xi , xi }
R
R
C
R
hR
= (hi1 + fi+1 ) {xi , xi }
R
L
C
L
C
C
R
C
R
scopes hL
hi {xi+1 , xi+1 , xi+2 , xi+2 } {xi+1 , xi+1 , xi+2 , xi+2 }, respecL
0
0
tively. construction, hi (a, , b, b ) contains cost best extension a, a0 , b, b0
C
L C
processed variables xL
, xi , . . . , x1 , x1 considering left functions only.
0
0
property hR
(a , a, b , b) right functions.
complexity MB space (n2n ) time (n2 21.5n ). Since complexities
smaller complexity HYB, running pre-process affect overall
complexity.
R
MB executed, HYB use information recorded hL
hi functions.
L
R
Consider arbitrary node HYB assigns xC
eliminates xi+2 xi+2 . Let
L
L
L (a, b)
b domain values variables xi xi+1 . Property 2 gi+2
contains best extension a, b attained left part rows + 1
n long current assignment X C maintained. Additionally,
C
C
hL
i1 (a, xi , b, xi+1 ) contains best extension a, b attained left part
L (a, b) + hL (a, xC , b, xC ) lower bound a, b X C
rows 1. Therefore, gi+2
i1

i+1
left part grid. Consequently,
L
C
C
mina,b[0..2 n2 1 1] {gi+2
(a, b) + hL
i1 (a, xi , b, xi+1 )}

433

fiLarrosa, Morancho & Niso

lower bound left part grid current assignment.
reasoning right part that,
L
C
C
mina,b[0..2 n2 1 1] {gi+2
(a, b) + hL
i1 (a, xi , b, xi+1 )} +
R
C
C
+mina,b[0..2 n2 1 1] {gi+2
(a, b) + hR
i1 (xi , a, xi+1 , b)}

lower bound current assignment.
4.2 Refining Upper Bound
efficiency algorithm also depends initial value upper bound.
good upper bound facilitates pruning earlier search tree. Bosch Trick (2002)
suggested modify SL(n) adding additional constraint considering symmetric
patterns, only. Since space solutions becomes considerably smaller, problem
presumably simpler. Clearly, cost optimal symmetric stable pattern upper
bound optimal cost SL(n). observed upper bounds
tight.
Since motivation work use variable elimination techniques,
considered still-lifes symmetric vertical reflection,
efficiently solved using BE. symmetric still-life problem SSL(n) consists finding
n n stable pattern maximum density game life subject vertical reflection
symmetry (namely, state cells (i, j) (i, n j + 1) must same.6
Adapting solve SSL(n) extremely simple: need remove symmetrical
values domains. Let us assume n even number (the odd case similar).
represent symmetric sequences bits length n considering left side
sequence (i.e, first n/2 bits). right part implicit left part. Thus,
n
represent symmetrical sequences n bits integers interval [0..2 2 1]. Reversing
sequence bits noted a. Hence, sequence n/2 bits, corresponding
symmetrical sequence n bits.
complexity BE, applied SSL(n) time (n2 21.5n ) space (n2n ).
Therefore, executing prior HYB setting upper bound optimal cost
affect overall complexity hybrid.
4.3 Exploitation Symmetries
SL(n) highly symmetric problem. stable pattern, possible create
equivalent pattern by: (i) rotating board 90, 180 270 degrees, (ii) reflecting
board horizontally, vertically along one diagonal (iii) combination
rotations reflections.
Symmetries exploited different algorithmic levels. general,
save computation whose outcome equivalent previous computation due
symmetry kept outcome. instance, MB necessary compute
0
0
L
0
0
hR
(a , a, b , b) equal hi (a, , b, b ) due vertical reflection symmetry.
C
C
Another example occurs HYB. Let xn = vn , xC
n1 = vn1 , . . . , xi = vi current
6. Unlike Smiths (2002) work cannot easily exploit larger variety symmetries rotations
diagonal reflections.

434

fiOn practical use variable elimination

n
13
14
15
16
17
18
19
20
22
24
26
28

opt
79(90)
92(104)
106(119)
120(136)
137(152)
153(171)
171(190)
190(210)
?
?
?
?

opt-SSL
79
92
106
120
137
154
172
192
232
276
326
378

CP/IP
12050
5 105
7 105
*
*
*
*
*
*
*
*
*


13788
105
*
*
*
*
*
*
*
*
*
*

HYB
2
2
58
7
1091
2029
56027
2 105
*
*
*
*

HYB LB
2750
7400
2 105
6 105
*
*
*
*
*
*
*
*

HYB UB
2
3
61
49
2612
2311
56865
2 105
*
*
*
*

Figure 6: Experimental results three different algorithms still-life problem. Times
seconds.

C
C
assignment. reversed assignment xC
n = vn , xn1 = vn1 , . . . , xi = vi equivalent due
vertical reflection symmetry. Thus, already considered, algorithm
backtrack. implementation uses tricks others
report would require much lower level description algorithms.

5. Experimental Results
Figure 6 shows empirical performance hybrid algorithm. first column contains
problem size. second column contains optimal value number dead
cells (in parenthesis corresponding number living cells). third column contains
optimal value symmetrical problem SSL(n), obtained executing BE.
observed SSL(n) provides tight upper bounds SL(n). fourth column
reports time obtained CP/IP algorithm (Bosch & Trick, 2002). fifth
column reports times obtained BE. sixth column contains times obtained
hybrid algorithm HYB. seen, performance HYB spectacular.
n = 14 n = 15 instances, require several days CPU, solved HYB
seconds. Instances n = 18 solved less one hour. largest instance
solve n = 20, requires two days CPU (Figure 7 shows optimal
n = 19 n = 20 still-lifes). Regarding space, computer handle executions
HYB n = 22. However, neither n = 21 n = 22 instance could solved
within week CPU. may seem solving n = 20 instance petty progress
respect previous results problem. clearly case. search space
2
2
n = 15 n = 20 instances size 215 = 2225 220 = 2400 , respectively. Thus,
able solve problem search space 2175 times larger before.
Since scales regularly, accurately predict would require 4000 Gb
memory 7 centuries solve n = 20 instance.
435

fiLarrosa, Morancho & Niso

Figure 7: Maximum density still-lifes n = 19 n = 20.

Since HYB combines several techniques, interesting assess impact
one. seventh column reports times obtained HYB without using mini-buckets
information lower bound. seen, algorithm still better plain BE,
performance dramatically affected. information gathered preprocess
improves quality lower bound anticipates pruning. Finally, eighth column
reports times obtained HYB without upper bound initialized SSL(n).
case see importance technique quite limited. reason
HYB, even bad initial upper bound, finds optimum rapidly and,
moment, quality initial upper bound becomes irrelevant.

6. Extension Domains
SL(n) problem well defined structure, hybrid algorithm
proposed makes ad hoc exploitation it. easy find right variables
instantiate eliminate. also easy find variable order mini buckets
produces good quality lower bounds. natural question whether possible apply
similar ideas well structured problems. answer often possible,
although need rely naive consequently less efficient exploitation
problems structure. Section support claim reporting additional experimental results different benchmarks. particular, consider spot5 DIMACS
instances. Spot5 instances optimization problems taken scheduling earth
observation satellite (Bensana, Lemaitre, & Verfaillie, 1999). DIMACS benchmark contains SAT instances several domain. Since concerned optimization tasks,
selected unsatisfiable instances solved Max-SAT task (i.e, given
unsatisfiable SAT instance, find maximum number clauses simultaneously
satisfied), modeled WCSP (de Givry, Larrosa, Meseguer, & Schiex, 2003).
consider aim instances (artificially generated random 3-SAT), pret (graph coloring), ssa
bf (circuit fault analysis).
Figure 8 shows constraint graph one instance domain, visualized
LEDA graph editor. observed graphs obvious pattern
436

fiOn practical use variable elimination

Figure 8: Constraint graph four WCSP instances. top-left corner, clockwise,
aim-100-1-6-no-1, pret60-25, ssa0432-003 Spot5-404.

exploited. Thus, use variable elimination techniques naive way.
solve problems generic WCSP solver toolbar7 (TB). performs depthfirst branch-and-bound search enhanced general-purpose dynamic variable
value ordering heuristics. modified toolbar combine search variable elimination
follows: arbitrary subproblem, every variable degree less 3 eliminated.
variables degree larger equal 3, unassigned variable
heuristically selected domain values heuristically ordered sequentially
instantiated. process recursively applied subproblems. Note
generic version HYB algorithm decision variables
instantiated variables eliminated left heuristic, instead establishing
7. Available http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/SoftCSP.

437

fiLarrosa, Morancho & Niso

hand. refer implementation TBHY B . Toolbar offers variety
lower bounds based different forms local consistency (Larrosa & Schiex, 2003).
One them, directional arc consistency (DAC*), essentially equivalent mini-buckets
size 2 and, therefore, similar spirit lower bound computed HYB. However,
unlike HYB mini-buckets executed pre-process, toolbar executes
DAC* every search state, subject current subproblem. shown Kask
(2000) approach generally efficient. main difference respect
HYB, toolbar executes DAC* subject arbitrary variable ordering (in HYB
good order identified problem structure). lower bounds available
toolbar node consistency (NC*) weaker DAC*, full directional
arc consistency (FDAC*) seen (stronger) refinement DAC*.
F DAC
B
experimented four algorithms: TBN C , TBDAC , TBDAC
HY B TBHY B ,
denotes algorithm lower bound B.
spot5 instances difficult toolbar. Therefore, decreased size
letting toolbar make sequence k greedy assignments driven default variable
value ordering heuristics. result subproblem k less variables.
following, Ik denotes instance k variables greedily assigned toolbar
default parameters.
Table 9 reports result experiments. first column indicates instances
subsequent columns indicate CPU time (in seconds) required different algorithms. time limit 3600 seconds set execution. observed
toolbar weakest lower bound (TBN C ) usually inefficient alternative.
cannot solve spot5 instances also fails several aim ssa instances.
toolbar enhanced mini buckets lower bound (TBDAC ) spot5 problems
solved. domains, new lower bound produce significant effect. add variable elimination (TBDAC
HY B ) problems solved.
general, clear speed-up. worst improvements pret instances
time divided factor 2 best ones obtained spot5 50340
ssa7552-158 instances solved instantly. Typical speed-ups range 5 10.
DAC ) limited
Finally, observe addition stronger lower bound (TBFHY
B
effect problems. execution instance ssa7552-038 clearly accelerated.
Therefore, experiments conclude main techniques used
solve still-life problem also successfully applied domains.

7. Conclusions
paper studied applicability variable elimination problem
finding still-lifes. Finding still-lifes challenging problem developing new solving
techniques interesting task per se. Thus, first contribution paper
observation plain variable elimination (i.e, BE) competitive practice provides
time complexity exponentially better search-based approaches. Besides, developed algorithm able solve n = 20 instance,
clearly improved previous results. second contribution paper
deeper insight. algorithm uses recent techniques based variable elimination.
Since techniques little known rarely applied constraints community,
438

fiOn practical use variable elimination

Problem
Spot5 4040
Spot5 408100
Spot5 412200
Spot5 414260
Spot5 50340
Spot5 505120
Spot5 507200
Spot5 509240
aim-100-1-6-no-1
aim-100-1-6-no-2
aim-100-1-6-no-3
aim-100-1-6-no-4
aim-100-2-0-no-1
aim-100-2-0-no-2
aim-100-2-0-no-3
aim-100-2-0-no-4
bf0432-007
pret60-25
pret60-40
ssa0432-003
ssa2670-141
ssa7552-038
ssa7552-158

BN C
2516
1191
1222
2162
110
110
22
-

BDAC
242
314
223
1533
546
3353
204
684
2007
931
850
1599
120
120
22
-

DAC
BHY
B
40
48
47
221
0
84
58
166
1665
707
1960
2716
830
479
319
738
1206
49
48
5
749
20
0

F DAC
BHY
B
40
43
42
139
0
84
42
121
1427
571
1627
2375
583
285
278
600
1312
56
56
5
767
2
1

Figure 9: Experimental results WCSP instances four different algorithms.
column reports CPU time seconds. Symbol - indicates time limit
3600 seconds reached.

results presented paper add new evidence potential. also shown
variable elimination used beyond academic still-life problem providing
experimental results unstructured realistic problems different domains.

Acknowledgments
authors grateful Barbara Smith, Neil Yorke-Smith anonymous reviewers
useful comments different stages work reported article. Marti
Sanchez kindly made plots Figure 8. research funded Spanish
CICYT project TIC2002-04470-C03-01.

References
Bensana, E., Lemaitre, M., & Verfaillie, G. (1999). Earth observation satellite management.
Constraints, 4(3), 293299.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press.
439

fiLarrosa, Morancho & Niso

Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint satisfaction
optimization. Journal ACM, 44 (2), 201236.
Bosch, R., & Trick, M. (2002). Constraint programming hybrid formulations three
life designs. Proceedings International Workshop Integration AI
Techniques Constraint Programming Combinatorial Optimization Problems,
CP-AI-OR02, pp. 7791.
Cabon, B., de Givry, S., Lobjois, L., Schiex, T., & Warners, J. (1999). Radio link frequency
assignment. Constraints, 4, 7989.
de Givry, S., Larrosa, J., Meseguer, P., & Schiex, T. (2003). Solving max-sat weighted
csp. Proc. 9th CP, pp. 363376, Kinsale, Ireland. LNCS 2833. Springer
Verlag.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113, 4185.
Dechter, R., & Pearl, J. (1989). Tree clustering constraint networks. Artificial Intelligence, 38, 353366.
Dechter, R., & Fatah, Y. E. (2001). Topological parameters time-space tradeoff. Artificial
Intelligence, 125 (12), 93118.
Dechter, R., & Rish, I. (2003). Mini-buckets: general scheme bounded inference.
Journal ACM, 50 (2), 107153.
Gardner, M. (1970). fantastic combinations john conways new solitary game. Scientific American, 223, 120123.
Kask, K. (2000). New search heuristics max-csp. Proc. 6th CP, pp. 262277,
Singapore. LNCS 1894. Springer Verlag.
Kask, K., & Dechter, R. (2001). general scheme automatic generation search
heuristics specification dependencies. Artificial Intelligence, 129, 91131.
Larrosa, J., & Dechter, R. (2003). Boosting search variable elimination constraint
optimization constraint satisfaction problems. Constraints, 8 (3), 303326.
Larrosa, J., & Schiex, T. (2003). quest best form local consistency
weighted csp. Proc. 18th IJCAI, Acapulco, Mexico.
Pearl, J. (1988). Probabilistic Inference Intelligent Systems. Networks Plausible Inference. Morgan Kaufmann, San Mateo, CA.
Sandholm, T. (1999). algorithm optimal winner determination combinatorial
auctions. IJCAI-99, pp. 542547.
Smith, B. (2002). dual graph translation problem life. Proc. CP-2002, pp.
01, Ithaca, USA. LNCS. Springer Verlag.

440

fiJournal Artificial Intelligence Research 23 (2005) 299-330

Submitted 07/04; published 03/05

Combining Knowledge- Corpus-based
Word-Sense-Disambiguation Methods
Andres Montoyo

montoyo@dlsi.ua.es

Dept. Software Computing Systems
University Alicante, Spain

Armando Suarez

armando@dlsi.ua.es

Dept. Software Computing Systems
University Alicante, Spain

German Rigau

rigau@si.ehu.es

IXA Research Group
Computer Science Department
Basque Country University, Donostia

Manuel Palomar

mpalomar@dlsi.ua.es

Dept. Software Computing Systems
University Alicante, Spain

Abstract
paper concentrate resolution lexical ambiguity arises
given word several different meanings. specific task commonly referred
word sense disambiguation (WSD). task WSD consists assigning correct
sense words using electronic dictionary source word definitions. present
two WSD methods based two main methodological approaches research area:
knowledge-based method corpus-based method. hypothesis word-sense
disambiguation requires several knowledge sources order solve semantic ambiguity
words. sources different kinds example, syntagmatic, paradigmatic statistical information. approach combines various sources knowledge,
combinations two WSD methods mentioned above. Mainly, paper concentrates combine methods sources information order achieve
good results disambiguation. Finally, paper presents comprehensive study
experimental work evaluation methods combinations.

1. Introduction
Knowledge technologies aim provide meaning petabytes information content
multilingual societies generate near future. Specifically, wide range
advanced techniques required progressively automate knowledge lifecycle.
include analyzing, automatically representing managing, high-level meanings
large collections content data. However, able build next generation
intelligent open-domain knowledge application systems, need deal concepts
rather words.
c
2005
AI Access Foundation. rights reserved.

fiMontoyo, Suarez, Rigau, & Palomar

1.1 Dealing Word Senses
natural language processing (NLP), word sense disambiguation (WSD) defined
task assigning appropriate meaning (sense) given word text discourse.
example, consider following three sentences:
1. Many cruise missiles fallen Baghdad.
2. Music sales fall 15% year.
3. U.S. officials expected Basra fall early.
system tries determine meanings three sentences need
represent somehow three different senses verb fall. first sentence, missiles
launched Baghdad. second sentence, sales decrease,
third city surrender early. WordNet 2.0 (Miller, 1995; Fellbaum, 1998)1 contains
thirty-two different senses verb fall well twelve different senses noun
fall. Note also first third sentence belong same, military domain,
use verb fall two different meanings.
Thus, WSD system must able assign correct sense given word,
examples, fall, depending context word occurs. example
sentences, are, respectively, senses 1, 2 9, listed below.
1. falldescend free fall influence gravity (The branch fell
tree; unfortunate hiker fell crevasse).
2. descend, fall, go down, come downmove downward necessarily
way (The temperature going down; barometer falling; Real estate
prices coming down).
9. fallbe captured (The cities fell enemy).
Providing innovative technology solve problem one main challenges
language engineering access advanced knowledge technology systems.
1.2 Word-Sense Disambiguation
Word sense ambiguity central problem many established Human Language Technology applications (e.g., machine translation, information extraction, question answering,
information retrieval, text classification, text summarization) (Ide & Veronis, 1998).
also case associated subtasks (e.g., reference resolution, acquisition subcategorization patterns, parsing, and, obviously, semantic interpretation). reason,
many international research groups working WSD, using wide range approaches.
However, date, large-scale, broad-coverage, accurate WSD system built (Snyder & Palmer, 2004). current state-of-the-art accuracy range 6070%, WSD
one important open problems NLP.
1. http://www.cogsci.princeton.edu/wn/

300

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

Even though techniques WSD usually presented stand-alone
techniques, belief, following McRoy (1992), full-fledged lexical ambiguity
resolution require integrate several information sources techniques.
paper, present two complementary WSD methods based two different
methodological approaches, knowledge-based corpus-based methods, well several
methods combine hybrid approaches.
knowledge-based method disambiguates nouns matching context information prescribed knowledge source. WordNet used combines characteristics dictionary structured semantic network, providing definitions
different senses English words defining groups synonymous words
means synsets, represent distinct lexical concepts. WordNet also organizes words
conceptual structure representing number semantic relationships (hyponymy,
hypernymy, meronymy, etc.) among synsets.
corpus-based method implements supervised machine-learning (ML) algorithm
learns annotated sense examples. corpus-based system usually represents
linguistic information context sentence (e.g., usage ambiguous word)
form feature vectors. features may distinct nature: word collocations,
part-of-speech labels, keywords, topic domain information, grammatical relationships,
etc. Based two approaches, main objectives work presented paper
are:
study performance different mechanisms combining information sources
using knowledge-based corpus-based WSD methods together.
show knowledge-based method help corpus-based method better
perform disambiguation process vice versa.
show combination approaches outperforms methods
taken individually, demonstrating two approaches play complementary
roles.
Finally, show approaches applied several languages. particular, perform several experiments Spanish English.
following section summary background word sense disambiguation
presented. Sections 2.1 2.2 describe knowledge-based corpus-based systems
used work. Section 3 describes two WSD methods: specification marks method
maximum entropy-based method. Section 4 presents evaluation results
using different system combinations. Finally, conclusions presented, along
brief discussion work progress.

2. Background WSD
Since 1950s, many approaches proposed assigning senses words
context, although early attempts served models toy systems. Currently,
two main methodological approaches area: knowledge-based corpus-based
methods. Knowledge-based methods use external knowledge resources, define explicit
301

fiMontoyo, Suarez, Rigau, & Palomar

sense distinctions assigning correct sense word context. Corpus-based methods
use machine-learning techniques induce models word usages large collections
text examples. knowledge-based corpus-based methods present different benefits
drawbacks.
2.1 Knowledge-based WSD
Work WSD reached turning point 1980s 1990s large-scale lexical
resources dictionaries, thesauri, corpora became widely available. work
done earlier WSD theoretically interesting practical extremely limited
domains. Since Lesk (1986), many researchers used machine-readable dictionaries
(MRDs) structured source lexical knowledge deal WSD. approaches,
exploiting knowledge contained dictionaries, mainly seek avoid need
large amounts training material. Agirre Martinez (2001b) distinguish ten different
types information useful WSD. located MRDs,
include part speech, semantic word associations, syntactic cues, selectional preferences,
frequency senses, among others.
general, WSD techniques using pre-existing structured lexical knowledge resources
differ in:
lexical resource used (monolingual and/or bilingual MRDs, thesauri, lexical knowledge base, etc.);
information contained resource, exploited method;
property used relate words senses.
Lesk (1986) proposes method guessing correct word sense counting word
overlaps dictionary definitions words context ambiguous word.
Cowie et al. (1992) uses simulated annealing technique overcoming combinatorial
explosion Lesk method. Wilks et al. (1993) use co-occurrence data extracted
MRD construct word-context vectors, thus word-sense vectors, perform large set
experiments test relatedness functions words vector-similarity functions.
approaches measure relatedness words, taking reference structured semantic net. Thus, Sussna (1993) employs notion conceptual distance
network nodes order improve precision document indexing. Agirre Rigau
(1996) present method resolution lexical ambiguity nouns using WordNet noun taxonomy notion conceptual density. Rigau et al. (1997) combine
set knowledge-based algorithms accurately disambiguate definitions MRDs. Mihalcea Moldovan (1999) suggest method attempts disambiguate nouns,
verbs, adverbs, adjectives given text referring senses provided WordNet. Magnini et al. (2002) explore role domain information WSD using WordNet
domains (Magnini & Strapparava, 2000); case, underlying hypothesis
information provided domain labels offers natural way establish semantic relations
among word senses, profitably used disambiguation process.
Although knowledge-based systems proven ready-to-use scalable
tools all-words WSD require sense-annotated data (Montoyo et al.,
302

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

2001), general, supervised, corpus-based algorithms obtained better precision
knowledge-based ones.
2.2 Corpus-based WSD
last fifteen years, empirical statistical approaches significantly increased impact NLP. increasing interest algorithms techniques come
machine-learning (ML) community since applied large variety
NLP tasks remarkable success. reader find excellent introduction ML,
relation NLP, articles Mitchell (1997), Manning Schutze (1999),
Cardie Mooney (1999), respectively. types NLP problems initially addressed
statistical machine-learning techniques language- ambiguity resolution,
correct interpretation selected among set alternatives
particular context (e.g., word-choice selection speech recognition machine translation,
part-of-speech tagging, word-sense disambiguation, co-reference resolution, etc.).
techniques particularly adequate NLP regarded classification
problems, studied extensively ML community. Regarding automatic
WSD, one successful approaches last ten years supervised learning
examples, statistical ML classification models induced semantically annotated corpora. Generally, supervised systems obtained better results
unsupervised ones, conclusion based experimental work international competitions2 . approach uses semantically annotated corpora train machinelearning
(ML) algorithms decide word sense choose contexts. words
annotated corpora tagged manually using semantic classes taken particular
lexical semantic resource (most commonly WordNet). Many standard ML techniques
tried, including Bayesian learning (Bruce & Wiebe, 1994), Maximum Entropy (Suarez
& Palomar, 2002a), exemplar-based learning (Ng, 1997; Hoste et al., 2002), decision lists
(Yarowsky, 1994; Agirre & Martinez, 2001a), neural networks (Towell & Voorhees, 1998),
and, recently, margin-based classifiers like boosting (Escudero et al., 2000) support
vector machines (Cabezas et al., 2001).
Corpus-based methods called supervised learn previously senseannotated data, therefore usually require large amount human intervention
annotate training data (Ng, 1997). Although several attempts made (e.g.,
Leackock et al., 1998; Mihalcea & Moldovan, 1999; Cuadros et al., 2004), knowledge
acquisition bottleneck (too many languages, many words, many senses, many
examples per sense) still open problem poses serious challenges supervised
learning approach WSD.

3. WSD Methods
section present two WSD methods based, respectively, two main methodological approaches outlined above: specification marks method (SM) (Montoyo & Palomar, 2001) knowledge-based method, maximum entropy-based method (ME)
(Suarez & Palomar, 2002b) corpus-based method. selected methods seen
2. http://www.senseval.org

303

fiMontoyo, Suarez, Rigau, & Palomar

representatives methodological approaches. specification marks method
inspired conceptual density method (Agirre & Rigau, 1996) maximum
entropy method also used WSD systems (Dang et al., 2002).
3.1 Specification Marks Method
underlying hypothesis knowledge base method higher similarity
two words, larger amount information shared two concepts.
case, information commonly shared several concepts indicated
specific concept subsumes taxonomy.
input WSD module group nouns W = {w1 , w2 , ..., wn } context. word wi sought WordNet, associated set possible senses
Si = {Si1 , Si2 , ..., Sin }, sense set concepts IS-A taxonomy (hypernymy/hyponymy relations). First, method obtains common concept
senses words form context. concept marked initial specification mark (ISM). initial specification mark resolve ambiguity
word, descend WordNet hierarchy, one level another, assigning
new specification marks. specification mark, number concepts contained
within subhierarchy counted. sense corresponds specification
mark highest number words one chosen sense disambiguated within
given context. Figure 1 illustrates graphically word plant, four different
senses, disambiguated context also words tree, perennial, leaf.
seen initial specification mark resolve lexical ambiguity, since
word plant appears two subhierarchies different senses. specification mark
identified {plant#2, flora#2}, however, contains highest number words (three)
context therefore one chosen resolve sense two word
plant. words tree perennial also disambiguated, choosing sense
one. word leaf appear subhierarchy specification mark {plant#2,
flora#2}, therefore word disambiguated. words beyond
scope disambiguation algorithm. left aside processed
complementary set heuristics (see section 3.1.2).
3.1.1 Disambiguation Algorithm
section, formally describe SM algorithm consists following five
steps:
Step 1:
nouns extracted given context. nouns constitute input context,
Context = {w1 , w2 , ..., wn }. example, Context = {plant, tree, perennial, leaf }.
Step 2:
noun wi context, possible senses Si = {Si1 , Si2 , ..., Sin }
obtained WordNet. sense Sij , hypernym chain obtained stored
order stacks. example, Table 1 shows hypernyms synsets
sense word Plant.
Step 3:
sense appearing stacks, method associates list subsumed senses
304

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

Inicial
Specification
Mark (ISM)

{entity#1}

{object#1}

SM

{life form#1}
SM
{natural object#1}
{plant#2, flora#2} (*)

{substance#1}

{part#4}

{artifact#1}
{person#1}
{material#1}

{section#4}

{plant part#1}
{perennial#1}

{vascular plant#1}

{Structure#1}
{paper#1}

{entertainer#1}
{woody plant#1}

{plant organ#1}

{building complex#1}

{performer#1}

{leaf#3}
{sheet#2}

{leaf#1}
{leaf#2}

{tree#1}

{plant#1}
{actor#1}
{plant#4}

Figure 1: Specification Marks
plant#1
building complex#1
structure#1
artifact#1
object#1
entity#1

plant#2
life form#1
entity#1

plant#3
contrivance#3
scheme#1
plan action#1
plan#1
idea#1
content#5
cognition#1
psychological feature#1

plant#4
actor#1
performer#1
entertainer#1
person#1
life form#1
entity#1

Table 1: Hypernyms synsets plant
context (see Figure 2, illustrates list subsumed senses plant#1
plant#2 ).
Step 4:
Beginning initial specification marks (the top synsets), program descends
recursively hierarchy, one level another, assigning specification mark number context words subsumed.
Figure 3 shows word counts plant#1 plant#4 located within specification mark entity#1, ..., life form#1, flora#2. entity#1 specification mark,
senses #1, #2, #4 maximal word counts (4). Therefore,
possible disambiguate word plant using entity#1 specification mark,
necessary go one level hyponym hierarchy changing
specification mark. Choosing specification mark life form#1, senses #2 #4
plant maximal word counts (3). Finally, possible disambiguate
word plant sense #2 using {plant#2, flora#2} specification mark,
sense higher word density (in case, 3).
305

fiMontoyo, Suarez, Rigau, & Palomar

PLANT:
PLANT#1:
plant#1 plant#1
building complex#1 plant#1
structure#1 plant#1
artifact#1 plant#1
object#1 plant#1, leaf#1, leaf#2, leaf#3
entity#1 plant#1, plant#2, plant#4, tree#1, perennial#1, leaf#1, leaf#2, leaf#3
PLANT#2:
plant#2 plant#2, tree#1, perennial#1
life form#1 plant#2, plant#4, tree#1, perennial#1
entity#1 plant#1, plant#2, plant#4, tree#1, perennial#1, leaf#1, leaf#2, leaf#3

Figure 2: Data Structure Senses Word Plant
PLANT
located within specification mark {entity#1}
PLANT#1 : 4 (plant, tree, perennial, leaf)
PLANT#2 : 4 (plant, tree, perennial, leaf)
PLANT#3 : 1 (plant)
PLANT#4 : 4 (plant, tree, perennial, leaf)

located within specification mark {life form#1}
PLANT#1 : 1 (plant)
PLANT#2 : 3 (plant, tree, perennial)
PLANT#3 : 1 (plant)
PLANT#4 : 3 (plant, tree, perennial)
located within specification mark {plant #2, flora#2}
PLANT#1 : 1 (plant)
PLANT#2 : 3 (plant, tree, perennial)
PLANT#3 : 1 (plant)
PLANT#4 : 1 (plant)

Figure 3: Word Counts Four Senses Word Plant
Step 5:
step, method selects word sense(s) greatest number words
counted Step 4. one sense, one obviously chosen.
one sense, repeat Step 4, moving level within
taxonomy single sense obtained program reach leaf specification
mark. Figure 3 shows word counts sense plant (#1 #4) located
within specification mark entity#1, ..., life form#1, flora#2. word cannot
disambiguated way, necessary continue disambiguation
process applying complementary set heuristics.

3.1.2 Heuristics
specification marks method combined set five knowledge-based heuristics:
hypernym/hyponym, definition, gloss hypernym/hyponym, common specification mark,
domain heuristics. short description methods provided below.
306

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

3.1.3 Hypernym/Hyponym Heuristic
heuristic solves ambiguity words explicitly related WordNet
(i.e., leaf directly related plant, rather follows hypernym chain plus PARTOF relation). hypernyms/hyponyms ambiguous word checked, looking
synsets compounds match word context.
synset hypernym/hyponym chain weighted accordance depth within
subhierarchy. sense greatest weight chosen. Figure 4 shows that,
leaf#1 hyponym plant organ#1 disambiguated (obtain greatest weight,
P
4
5
level
weight(leaf #1) = depth
i=1 ( total levels ) = ( 6 ) + ( 6 ) = 1.5) plant contained within
context leaf.
Context: plant, tree, leaf, perennial
Word non disambiguated: leaf.
Senses: leaf#1, leaf#2, leaf#3.
leaf#1
=> entity, something
=> object, physical object
=> natural object
=> plant part
=> plant organ
=> leaf#1, leafage, foliage

Level 1
Level 2
Level 3
Level 4
Level 5
Level 6

Figure 4: Application Hypernym Heuristic

3.1.4 Definition Heuristic
case, glosses synsets ambiguous word checked looking
contain words context. match increases synset count one.
sense greatest count chosen. Figure 5 shows example
heuristic. sense sister#1 chosen, greatest weight.
Context: person, sister, musician.
Words non disambiguated: sister,musician.
Senses: sister#1, sister#2, sister#3 sister#4.
sister#1 Weight = 2
1. sister, sis -- (a female person parents another person; "my sister married
musician")
sister#3 Weight = 1
3. sister -- (a female person fellow member (of sorority labor union group);
"none sisters would betray her")

Figure 5: Application Definition Heuristic

307

fiMontoyo, Suarez, Rigau, & Palomar

3.1.5 Gloss Hypernym/Hyponym Heuristic
method extends previously defined hypernym/hyponym heuristic using glosses
hypernym/hyponym synsets ambiguous word. disambiguate given word,
glosses hypernym/hyponym synsets checked looking words occurring
context. Coincidences counted. before, synset greatest count
chosen. Figure 6 shows example heuristic. sense plane#1 chosen,
greatest weight.
Context: plane, air
Words non disambiguated: plane
Senses: plane#1, plane#2, plane#3, plane#4, plane#5.
Plane#1: Weight = 1
airplane, aeroplane, plane -- (an aircraft fixed wing powered propellers jets; "the
flight delayed due trouble airplane")
=> aircraft -- (a vehicle fly)
=> craft -- (a vehicle designed navigation water air outer space)
=> vehicle -- (a conveyance transports people objects)
=> conveyance, transport -- (something serves means transportation)
=> instrumentality, instrumentation -- (an artifact (or system artifacts)
instrumental accomplishing end)
=> artifact, artefact -- (a man-made object)
=> object, physical object -- (a physical (tangible visible) entity; "it full
rackets, balls objects")
=> entity, something -- (anything existence (living nonliving))

Figure 6: Application Gloss Hypernym Heuristic

3.1.6 Common Specification Mark Heuristic
cases, senses words disambiguated close
differ subtle differences nuances. Common Specification Mark heuristic reduce
ambiguity word without trying provide full disambiguation. Thus, select
specification mark common senses context words, reporting senses
instead choosing single sense among them. illustrate heuristic, consider
Figure 7. example, word month able discriminate completely among
four senses word year. However, case, presence word month
help select two possible senses word year selecting time period, period
common specification mark. specification mark represents specific common
synset particular set words. Therefore, heuristic selects sense month#1
senses year#1 year#2 instead attempting choose single sense leaving
completely ambiguous.
3.1.7 Domain WSD Heuristic
heuristic uses derived resource, relevant domains (Montoyo et al., 2003),
obtained combining WordNet glosses WordNet Domains (Magnini & Strap308

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

Context: year, month. Words non disambiguated: year. Senses: year#1, year#2, year#3, year#4.
year#1:

year#2:

month#1:

=> abstraction
=> abstraction
=> measure, quantity
=> measure, quantity
=> time period, period
=> time period, period
=> year#1, twelvemonth
=> year#2

=> abstraction
=> measure, quantity
=> time period, period
=> month#1

Figure 7: Example Common Specification Mark Heuristic
parava, 2000)3 . WordNet Domains establish semantic relation word senses
grouping semantic domain (Sports, Medicine, etc.). word bank,
example, ten senses WordNet 2.0, three them, bank#1, bank#3
bank#6 grouped domain label, Economy, whereas bank#2
bank#7 grouped domain labels Geography Geology. domain labels selected set 165 labels hierarchically organized. way, domain
connects words belong different subhierarchies partof-speech.
Relevant domains lexicon derived WordNet glosses using WordNet Domains. fact, use WordNet corpus categorized domain labels.
English word appearing gloses WordNet, obtain list representative domain labels. relevance obtained weighting possible label
Association Ratio formula (AR), w word domain.
AR(w|D) = P (w|D) log

P (w|D)
P (w)

(1)

list also considered weighted vector (or point multidimensional
space). Using word vectors Relevant domains, derive new vectors
represent sets wordsfor instance, contexts glosses. compare
similarity given context possible senses polysemous word
using instance cosine function.
Figure 8 shows example disambiguating word genotype following text:
number ways chromosome structure change,
detrimentally change genotype phenotype organism. First, glosses
word disambiguated context postagged analyzed morphologically.
Second, build context vector (CV) combines one structure relevant representative domains related words text disambiguated.
Third, way, build sense vectors (SV) group relevant
representative domains gloss associated one word senses.
example, genotype#1 (a group organisms sharing specific genetic constitution)
genotype#2 (the particular alleles specified loci present organism). Finally,
order select appropriate sense, made comparison sense vectors
context vector, select senses approximate context vector.
3. http://wndomains.itc.it/

309

fiMontoyo, Suarez, Rigau, & Palomar

example, show sense vector sense genotype#1 select genotype#1
sense, cosine higher.



Bio log
Ecology

Botany
Zoology

CV = Anatomy
Physiology

Chemistry
Geology

Meteorology

...




0.00102837
0.00402855

3.20408e - 06
1.77959e - 05

1.29592e - 05
0.000226531

0.000179857
1.66327e - 05
0.00371308

...

AR



Ecology
Biology

Bowling
SV = Archaeology

Sociology
Alimentation

Linguistics

...


AR

0.084778
0.047627

0.019687
0.016451

0.014251
0.006510

0.005297

...


Sense vector genotype#1.
Context vector

Selected sense
genotype#1 = 0.00804111
genotype#2 = 0.00340548

Figure 8: Example Domain WSD Heuristic
Defining heuristic knowledge-based corpus-based seen controversial heuristic uses WordNet gloses (and WordNet Domains) corpus
derive relevant domains. is, using corpus techniques WordNet. However,
WordNet Domains constructed semi-automatically (prescribed) following hierarchy
WordNet.
3.1.8 Evaluation Specification Marks Method
Obviously, also use different strategies combine set knowledge-based heuristics. instance, heuristics described previous section applied
order passing next heuristic remaining ambiguity previous heuristics
able solve.
order evaluate performance knowledge-based heuristics previously defined, used SemCor collection (Miller et al., 1993), content words
annotated appropriate WordNet sense.In case, used window
fifteen nouns (seven context nouns target noun).
results obtained specification marks method using heuristics applied one one shown Table 2. table shows results polysemous nouns
only, polysemous monosemous nouns combined.
310

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

Heuristics
Precision Recall Coverage
Polysemic monosemic nouns
0.553
0.522
0.943
polysemic nouns
0.377
0.311
0.943

Table 2: Results Using Heuristics Applied Order SemCor

results obtained heuristics applied independently shown Table 3.
shown, heuristics perform differently, providing different precision/recall figures.
Heuristics

Precision
Recall
Coverage
Mono+Poly Polysemic Mono+Poly Polysemic Mono+Poly Polysemic
Spec. Mark Method
0.383
0.300
0.341
0.292
0.975
0.948
Hypernym
0.563
0.420
0.447
0.313
0.795
0.745
Definition
0.480
0.300
0.363
0.209
0.758
0.699
Hyponym
0.556
0.393
0.436
0.285
0.784
0.726
Gloss hypernym
0.555
0.412
0.450
0.316
0.811
0.764
0.617
0.481
0.494
0.358
0.798
0.745
Gloss hyponym
Common specification
0.565
0.423
0.443
0.310
0.784
0.732
Domain WSD
0.585
0.453
0.483
0.330
0.894
0.832

Table 3: Results Using Heuristics Applied Independently
Another possibility combine heuristics using majority voting schema (Rigau
et al., 1997). simple schema, heuristic provides vote, method selects
synset obtains votes. results shown Table 4 illustrate
heuristics working independently, method achieves 39.1% recall polysemous
nouns (with full coverage), represents improvement 8 percentual points
method heuristics applied order (one one).
Precision
Recall
Mono+Poly Polysemic Mono+Poly Polysemic
Voting heuristics
0.567
0.436
0.546
0.391

Table 4: Results using majority voting SemCor
also show Table 5 results domain heuristic applied English
all-words task Senseval-2. table, polysemy reduction caused domain
clustering profitably help WSD. Since domains coarser synsets, word domain
disambiguation (WDD) (Magnini & Strapparava, 2000) obtain better results WSD.
goal perform preliminary domain disambiguation order provide informed
searchspace reduction.
3.1.9 Comparison Knowledge-based Methods
section compare three different knowledge-based methods: conceptual density
(Agirre & Rigau, 1996), variant conceptual density algorithm (Fernandez-Amoros
et al., 2001); Lesk method (Lesk, 1986) ; specification marks method.
311

fiMontoyo, Suarez, Rigau, & Palomar

Level WSD Precision Recall
Sense
0.44
0.32
Domain
0.54
0.43

Table 5: Results Use Domain WSD Heuristic

Table 6 shows recall results three methods applied entire SemCor
collection. best result achieved 39.1% recall. important improvement
respect methods, results still far frequent sense heuristic. Obviously, none knowledge-based techniques heuristics presented
sufficient, isolation, perform accurate WSD. However, empirically demonstrated simple combination knowledge-based heuristics lead improvements
WSD process.
WSD Method
Recall
SM Voting Heuristics
0.391
UNED Method
0.313
SM Cascade Heuristics 0.311
Lesk
0.274
Conceptual Density
0.220

Table 6: Recall results using three different knowledgebased WSD methods

3.2 Maximum Entropy-based Method
Maximum Entropy modeling provides framework integrating information classification many heterogeneous information sources (Manning & Schutze, 1999; Berger
et al., 1996). probability models successfully applied NLP tasks,
POS tagging sentence-boundary detection (Ratnaparkhi, 1998).
WSD method used work based conditional models.
implemented using supervised learning method consists building word-sense classifiers using semantically annotated corpus. classifier obtained means
technique consists set parameters coefficients estimated using optimization procedure. coefficient associated one feature observed training
data. goal obtain probability distribution maximizes entropythat
is, maximum ignorance assumed nothing apart training data considered.
One advantage using framework even knowledge-poor features may applied accurately; framework thus allows virtually unrestricted ability represent
problem-specific knowledge form features (Ratnaparkhi, 1998).
Let us assume set contexts X set classes C. function cl : X C chooses
class c highest conditional probability context x: cl(x) = arg maxc p(c|x).
feature calculated function associated specific class c0 ,
takes form equation (2), cp(x) represents observable characteristic
312

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

context4 . conditional probability p(c|x) defined equation (3),
parameter weight feature i, K number features defined, Z(x)
normalization factor ensures sum conditional probabilities
context equal 1.
f (x, c) =

(

1 c0 = c cp(x) = true
0 otherwise

K
1
f (x,c)
p(c|x) =

Z(x) i=1

(2)

(3)

learning module produces classifiers word using corpus syntactically semantically annotated. module processes learning corpus order
define functions apprise linguistic features context.
example, consider want build classifier noun interest using
POS label previous word feature also following three
examples training corpus:
... widespread interest#1 ...
... best interest#5 ...
... persons expressing interest#1 ...

learning module performs sequential processing corpus, looking
pairs <POS-label, sense>. Then, following pairs used define three functions
(each context vector composed three features).
<adjective,#1>
<adjective,#5>
<verb,#1>
define another type feature merging POS occurrences sense:
< {adjective,verb},#1>
<adjective,#5>
form defining pairs means reduction feature space information
(of kind linguistic data, e.g., POS label position -1) sense contained
one feature. Obviously, form feature function 2 must adapted Equation
4. Thus,

f(c0 ,i) (x, c) =

(

W(c0 ) = {data sense c0 }

(4)

1 c0 = c CP (x) W(c0 )
0 otherwise

4. approach limited binary features, optimization procedure used estimation
parameters, Generalized Iterative Scaling procedure, uses kind features.

313

fiMontoyo, Suarez, Rigau, & Palomar

refer feature function expressed Equation 4 collapsed features.
previous Equation 2 call non-collapsed features. two feature definitions
complementary used together learning phase.
Due nature disambiguation task, number times feature
generated first type function (non-collapsed) activated low,
feature vectors large number null values. new function drastically reduces
number features, minimal degradation evaluation results. way,
new features incorporated learning process, compensating loss
accuracy.
Therefore, classification module carries disambiguation new contexts using
previously stored classification functions. enough information
specific context, several senses may achieve maximum probability
thus classification cannot done properly. cases, frequent sense
corpus assigned. However, heuristic necessary minimum number
contexts set linguistic attributes processed small.
3.2.1 Description Features
set features defined training system described Figure 9
based features described Ng Lee (1996) Escudero et al. (2000).
features represent words, collocations, POS tags local context. collapsed
non-collapsed functions used.
0: word form target word
s: words positions 1, 2, 3

p: POS-tags words positions 1, 2, 3
b: lemmas collocations positions (2, 1), (1, +1), (+1, +2)
c: collocations positions (2, 1), (1, +1), (+1, +2)
k m: lemmas nouns position context, occurring least m% times sense
r : grammatical relation ambiguous word
: word ambiguous word depends

m: multi-word identified parser
L: lemmas content-words positions 1, 2, 3 (collapsed definition)
W : content-words positions 1, 2, 3 (collapsed definition)
S, B, C, P, : collapsed versions (see Equation 4)

Figure 9: Features Used Training System
Actually, item Figure 9 groups several sets features. majority
depend nearest words (e.g., comprises possible features defined words
occurring sample positions w3 , w2 , w1 , w+1 , w+2 , w+3 related ambiguous word). Types nominated capital letters based collapsed function
form; is, features simply recognize attribute belonging training data.
Keyword features (km) inspired Ng Lee work. Noun filtering done using
frequency information nouns co-occurring particular sense. example, let us
314

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

suppose = 10 set 100 examples interest#4 : noun bank found 10 times
position, feature defined.
Moreover, new features also defined using grammatical properties: relationship features (r) refer grammatical relationship ambiguous word
(subject, object, complement, ...) dependency features (d D) extract word
related ambiguous one dependency parse tree.
3.2.2 Evaluation Maximum Entropy Method
subsection present results evaluation training testing data
Senseval-2 Spanish lexicalsample task. corpus parsed using Conexor
Functional Dependency Grammar parser Spanish (Tapanainen & Jarvinen, 1997).
classifiers built training data evaluated test data. Table
7 shows combination groups features works better every POS
work better words together.
Nouns
Verbs
Adjectives


Accuracy Feature selection
0.683
LWSBCk5
0.595
sk5
0.783
LWsBCp
0.671
0LWSBCk5

Table 7: Baseline: Accuracy Results Applying Senseval-2 Spanish Data
work entails exhaustive search looking accurate combination
features. values presented merely informative indicate maximum
accuracy system achieve particular set features.
3.3 Improving accuracy
main goal find method automatically obtain best feature selection
(Veenstra et al., 2000; Mihalcea, 2002; Suarez & Palomar, 2002b) training data.
performed 3-fold cross-validation process. Data divided 3 folds; then, 3 tests
done, one 2 folds training data remaining one testing data.
final result average accuracy. decided three tests small
size training data. Then, tested several combinations features training
data Senseval-2 Spanish lexicalsample analyzed results obtained
word.
order perform 3-fold cross-validation process word, preprocessing
corpus done. word, senses uniformly distributed three
folds (each fold contains one-third examples sense). senses
fewer three examples original corpus file rejected processed.
Table 8 shows best results obtained using three-fold cross-validation training
data. Several feature combinations tested order find best set selected
word. purpose obtain relevant information word
corpus rather applying combination features them. Therefore,
information column Features lists feature selection best result.
315

fiMontoyo, Suarez, Rigau, & Palomar

Word
autoridad,N
bomba,N
canal,N
circuito,N
corazon,N
corona,N
gracia,N
grano,N
hermano,N
masa,N
naturaleza,N
operacion,N
organo,N
partido,N
pasaje,N
programa,N
tabla,N
actuar,V
apoyar,V
apuntar,V

Features
Accur
sbcp
0.589
0LWSBCk5
0.762
sbcprdk3
0.579
0LWSBCk5
0.536
0Sbcpk5
0.781
sbcp
0.722
0sk5
0.634
0LWSBCr
0.681
0Sprd
0.731
LWSBCk5
0.756
sbcprdk3
0.527
0LWSBCk5
0.543
0LWSBCPDk5 0.715
0LWSBCk5
0.839
sk5
0.685
0LWSBCr
0.587
sk5
0.663
sk5
0.514
0sbcprdk3
0.730
0LWsBCPDk5 0.661

MFS
0.503
0.707
0.307
0.392
0.607
0.489
0.295
0.483
0.602
0.455
0.424
0.377
0.515
0.524
0.451
0.486
0.488
0.293
0.635
0.478

Word
clavar,V
conducir,V
copiar,V
coronar,V
explotar,V
saltar,V
tocar,V
tratar,V
usar,V
vencer,V
brillante,A
ciego,A
claro,A
local,A
natural,A
popular,A
simple,A
verde,A
vital,A

Features Accur
sbcprdk3 0.561
LWsBCPD 0.534
0sbcprdk3 0.457
sk5
0.698
0LWSBCk5 0.593
LWsBC
0.403
0sbcprdk3 0.583
sbcpk5
0.527
0Sprd
0.732
sbcprdk3 0.696
sbcprdk3 0.756
0spdk5
0.812
0Sprd
0.919
0LWSBCr 0.798
sbcprdk10 0.471
sbcprdk10 0.865
LWsBCPD 0.776
LWSBCk5 0.601
Sbcp
0.774

MFS
0.449
0.358
0.338
0.327
0.318
0.132
0.313
0.208
0.669
0.618
0.512
0.565
0.854
0.750
0.267
0.632
0.621
0.317
0.441

Table 8: Three-fold Cross-Validation Results Senseval-2 Spanish Training Data: Best Averaged Accuracies per Word

Strings row represent entire set features used training classifier.
example, autoridad obtains best result using nearest words, collocations two lemmas,
collocations two words, POS information is, s, b, c, p features, respectively
(see Figure 9). column Accur (for accuracy) shows number correctly classified
contexts divided total number contexts (because always classifies precision
equal recall). Column MFS shows accuracy obtained frequent sense
selected.
data summarized Table 8 reveal using collapsed features method
useful; collapsed non-collapsed functions used, even word.
example, adjective vital obtains best result Sbcp (the collapsed version
words window (3.. + 3), collocations two lemmas two words window
(2.. + 2), POS labels, window (3.. + 3) too); infer single-word
information less important collocations order disambiguate vital correctly.
target word (feature 0) useful nouns, verbs, adjectives, many
words use best feature selection. general, words
relevant relationship shape senses. hand, POS information (p
P features) selected less often. comparing lemma features word features
(e.g., L versus W , B versus C), complementary majority cases.
Grammatical relationships (r features) wordword dependencies (d features)
seem useful, too, combined types attributes. Moreover, keywords (km
316

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

features) used often, possibly due source size contexts Senseval-2
Spanish lexicalsample data.
Table 9 shows best feature selections part-of-speech words.
data presented Tables 8 9 used build four different sets classifiers order
compare accuracy: MEfix uses overall best feature selection words;
MEbfs trains word best selection features (in Table 8); MEbfs.pos uses
best selection per POS nouns, verbs adjectives, respectively (in Table 9); and,
finally, vME majority voting system input answers preceding
systems.
POS
Nouns
Verbs
Adjectives


Acc
0.620
0.559
0.726
0.615

Features
System
LWSBCk5
sbcprdk3 MEbfs.pos
0spdk5
sbcprdk3
MEfix

Table 9: Three-fold Cross-Validation Results Senseval-2 Spanish Training Data: Best Averaged Accuracies per POS

Table 10 shows comparison four systems. MEfix lower results.
classifier applies set types features words. However, best feature
selection per word (MEbfs) best, probably training examples
necessary. best choice seems select fixed set types features POS
(MEbfs.pos).

Nouns
MEbfs.pos 0.683 MEbfs.pos
vME
0.678 vME
0.661 MEbfs
MEbfs
MEfix
0.646 MEfix
Verbs
Adjectives
0.583 vME
0.774 vME
0.583 MEbfs.pos 0.772 MEbfs.pos
0.583 MEfix
0.771 MEbfs
0.580 MEbfs
0.756 MEfix
MEfix: sbcprdk3 words
MEbfs: word
best feature selection
MEbfs.pos: LWSBCk5 nouns,
sbcprdk3 verbs,
0spdk5 adjectives
vME: majority voting MEfix,
MEbfs.pos, MEbfs
0.677
0.676
0.667
0.658

Table 10: Evaluation Systems
317

fiMontoyo, Suarez, Rigau, & Palomar

MEbfs predicts, word training data, individually selected
features could best ones evaluated testing data, MEbfs.pos
averaged prediction, selection features that, training data, performed good
enough disambiguation majority words belonging particular POS.
averaged prediction applied real testing data, MEbfs.pos performs better
MEbfs.
Another important issue MEbfs.pos obtains accuracy slightly better
best possible evaluation result achieved (see Table 7)that is, best-featureselection per POS strategy training data guarantees improvement ME-based
WSD.
general, verbs difficult learn accuracy method lower
POS; opinion, information (knowledge-based, perhaps) needed
build classifiers. case, voting system (vME) based agreement
three systems, improve accuracy.
Finally Table 11, results method compared systems
competed Senseval-2 Spanish lexicalsample task5 . results obtained
systems excellent nouns adjectives, verbs. However, comparing
POS, systems seem perform comparable best Senseval-2 systems.

0.713
0.682
0.677
0.676
0.670
0.667
0.658
0.627
0.617
0.610
0.595
0.595
0.582
0.578
0.560
0.548
0.524


jhu(R)
jhu
MEbfs.pos
vME
css244
MEbfs
MEfix
umd-sst
duluth 8
duluth 10
duluth Z
duluth 7
duluth 6
duluth X
duluth 9
ua
duluth

0.702
0.683
0.681
0.678
0.661
0.652
0.646
0.621
0.612
0.611
0.603
0.592
0.590
0.586
0.557
0.514
0.464

Nouns
jhu(R)
MEbfs.pos
jhu
vME
MEbfs
css244
MEfix
duluth 8
duluth Z
duluth 10
umd-sst
duluth 6
duluth 7
duluth X
duluth 9
duluth
ua

0.643
0.609
0.595
0.584
0.583
0.583
0.583
0.580
0.515
0.513
0.511
0.498
0.490
0.478
0.477
0.474
0.431

Verbs
jhu(R)
jhu
css244
umd-sst
vME
MEbfs.pos
MEfix
MEbfs
duluth 10
duluth 8
ua
duluth 7
duluth Z
duluth X
duluth 9
duluth 6
duluth

0.802
0.774
0.772
0.772
0.771
0.764
0.756
0.725
0.712
0.706
0.703
0.689
0.689
0.687
0.678
0.655
0.637

Adjectives
jhu(R)
vME
MEbfs.pos
css244
MEbfs
jhu
MEfix
duluth 8
duluth 10
duluth 7
umd-sst
duluth 6
duluth Z
ua
duluth X
duluth 9
duluth

Table 11: Comparison Spanish Senseval-2 systems

5. JHU(R) Johns Hopkins University; CSS244 Stanford University; UMD-SST University
Maryland; Duluth systems University Minnesota - Duluth; UA University Alicante.

318

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

3.4 Comparing Specification Marks Method Maximum Entropy-based
Method
main goal section evaluate Specification Marks Method (Montoyo &
Suarez, 2001) Maximum Entropy-based Method (in particular, MEfix System)
common data set, allow direct comparisons. individual evaluation
method carried noun set (17 nouns) Spanish lexical-sample task
(Rigau et al., 2001) Senseval-26 . Table 12 shows precision, recall coverage
methods.
Precision Recall Coverage
SM
0.464
0.464
0.941

0.646
0.646
1

Table 12: Comparison SM nouns Senseval-2 Spanish lexical sample
order study possible cooperation methods, count cases
that: two methods return correct sense occurrence, least one
methods provides correct sense finally, none provides correct sense.
summary obtained results shown Table 13. results clearly show
large room improvement combining system outcomes. fact,
provide also possible upper bound precision technology, set 0.798
(more 15 percentual points higher current best system). Table 14 presents
complementary view: wins, ties loses SM context
examined. Although performs better SM, 122 cases (15 %) solved
SM method.
Contexts Percentage
OK
240
0.300
398
0.498
One OK
Zero OK
161
0.202

Table 13: Correct classifications SM nouns Senseval-2 Spanish lexical sample

Wins Ties Loses
SM 267 240 122

Table 14: Wins, ties loses SM systems nouns Senseval-2 Spanish lexical
sample

6. Spanish English Senseval-2 corpora, applying Specification Marks method
used whole example context window target noun

319

fiMontoyo, Suarez, Rigau, & Palomar

4. Experimental Work
section attempt confirm hypothesis corpus-based knowledgebased methods improve accuracy other. first subsection shows
results preprocessing test data maximum entropy method (ME) order
help specification marks method (SM). Next, test opposite, preprocessing
test data domain heuristic help maximum entropy method disambiguate
accurately.
last experiment combines vME system (the majority voting system) SM
method. Actually, relies simply adding SM one heuristic voting
scheme.
4.1 Integrating Corpus-based WSD System Knowledge-based WSD
System
experiment designed study evaluate whether integration corpus-based
system within knowledge-based helps improve word-sense disambiguation nouns.
Therefore, help SM labelling nouns context target word.
is, reducing number possible senses nouns context. fact,
reduce search space SM method. ensures sense target word
one related noun senses labelled ME.
case, used noun words English lexical-sample task Senseval2. helps SM labelling words context target word. words
sense tagged using SemCor collection learning corpus. performed three
fold cross-validation nouns 10 occurrences. selected nouns
disambiguated high precision, is, nouns percentage
rates accuracy 90% more. classifiers nouns used disambiguate
testing data. total number different noun classifiers (noun) activated
target word across testing corpus shown Table 15.
Next, SM applied, using heuristics disambiguating target words
testing data, advantage knowing senses nouns formed
context targets words.
Table 15 shows results precision recall SM applied without
first applying ME, is, without fixing sense nouns form
context. small consistent improvement obtained complete
test set (3.56% precision 3.61% recall). Although improvement low,
experiment empirically demonstrates corpus-based method maximum entropy
integrated help knowledge-based system specification marks method.
4.2 Integrating Knowledge-based WSD system Corpus-based WSD
system
case, used domain heuristic improve information
added directly domain features. problem data sparseness WSD
system based features suffers could increased fine-grained sense distinctions
provided WordNet. contrary, domain information significantly reduces
320

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

Target words
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
Total

Without fixed senses fixed senses
noun classifiers Precision
Recall
Precision Recall
63
80
104
37
59
32
59
50
49
136
22
15
14
38
48
38
29
23
40
58
51
25
37
41
31
37
17
37
24

0.475
0.137
0.222
0.421
0.206
0.500
0.500
0.509
0.356
0.038
0.454
0.933
0.875
0.236
0.306
0.184
0.321
0.818
0.375
0.343
0.094
0.269
0.263
0.312
0.200
0.260
0.823
0.228
0.480

0.475
0.123
0.203
0.216
0.190
0.343
0.200
0.509
0.346
0.035
0.454
0.933
0.875
0.230
0.300
0.179
0.310
0.346
0.136
0.338
0.094
0.269
0.263
0.306
0.193
0.240
0.823
0.216
0.480

0.524
0.144
0.232
0.421
0.316
0.521
0.518
0.540
0.369
0.054
0.476
0.933
1
0.297
0.346
0.216
0.321
0.833
0.615
0.359
0.132
0.307
0.289
0.354
0.206
0.282
0.941
0.257
0.541

0.524
0.135
0.220
0.216
0.301
0.375
0.233
0.529
0.360
0.049
0.454
0.933
1
0.282
0.340
0.205
0.310
0.384
0.363
0.353
0.132
0.307
0.289
0.346
0.193
0.260
0.941
0.243
0.520

1294

0.300

0.267

0.336

0.303

Table 15: Precision Recall Results Using SM Disambiguate Words, Without Fixing Noun Sense

word polysemy (i.e., number categories word generally lower number
senses word) results obtained heuristic better precision
obtained whole SM method, turn obtain better recall.
shown subsection 3.1.7, domain heuristic annotate word senses characterized domains. Thus, domains used additional type features
context window 1, 2, 3 target word. addition,
three relevant domains calculated also context incorporated
training form features.
experiment also carried English lexical-sample task data
Senseval-2, used generate two groups classifiers training data.
first group classifiers used corpus without information domains; second,
previously domain disambiguated SM, incorporating domain label
adjacent nouns, three relevant domains context. is, providing
classifier richer set features (adding domain features). However, case,
perform feature selection.
test data disambiguated twice, without SM domain labelling,
using 0lW sbcpdm (see Figure 9) common set features order perform
comparison. results experiment shown Table 16.
table shows 7 29 nouns obtained worse results using domains,
whereas 13 obtained better results. Although, case, obtained small
improvement terms precision (2%)7 .
7. difference proves statistically significant applying test corrected difference
two proportions (Dietterich, 1998; Snedecor & Cochran, 1989)

321

fiMontoyo, Suarez, Rigau, & Palomar

Target words

Without domains domains Improvement

art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew

0.667
0.600
0.625
0.865
0.898
0.567
0.661
0.560
0.408
0.676
0.909
0.800
0.429
0.850
0.708
0.540
0.759
1.000
0.900
0.534
0.569
0.720
0.459
0.463
0.516
0.676
0.765
0.378
0.792

0.778
0.700
0.615
0.919
0.898
0.597
0.695
0.600
0.388
0.669
0.909
0.800
0.500
0.850
0.688
0.620
0.793
0.957
0.900
0.552
0.588
0.720
0.459
0.512
0.452
0.622
0.882
0.378
0.792

0.111
0.100
-0.010
0.054



0.649

0.669

0.020

0.030
0.034
0.040
-0.020
-0.007
0.071
-0.021
0.080
0.034
-0.043
0.017
0.020
0.049
-0.065
-0.054
0.118

Table 16: Precision Results Using Disambiguate Words, Without Domains
(recall precision values equal)

obtained important conclusions relevance domain information
word. general, larger improvements appear words well-differentiated
domains (spade, authority). Conversely, word stress senses belonging
FACTOTUM domain improves all. example, spade, art authority
(with accuracy improvement 10%) domain data seems important source
knowledge information captured types features. words
precision decrease 6.5%, domain information confusing. Three reasons
exposed order explain behavior: clear domain examples
represent correctly context, domains differentiate appropriately
senses, number training examples low perform valid assessment.
cross-validation testing, examples available, could appropriate perform
domain tuning word order determine words must use preprocess
not.
Nevertheless, experiment empirically demonstrates knowledge-based method,
domain heuristic, integrated successfully corpus-based system,
maximum entropy, obtain small improvement.
4.3 Combining Results with(in) Voting System
previous sections, demonstrated possible integrate two different WSD
approaches. section evaluate performance combining knowledge-based
system, specification marks, corpus-based system, maximum entropy,
simple voting schema.
322

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

two previous experiments attempted provide information predisambiguating data. Here, use methods parallel combine
classifications voting system, Senseval-2 Spanish English lexicalsample
tasks.
4.3.1 Senseval-2 Spanish lexicalsample task
vME+SM enrichment vME: added SM classifier combination
three systems vME (see Section 3.3). results Spanish lexicalsample task
Senseval-2 shown Table 17. works nouns, vME+SM
improves accuracy only, obtains score JHU(R) overall
score reaches second place.
0.713
0.684
0.682
0.677
0.676
0.670
0.667
0.658
0.627
0.617
0.610
0.595
0.595
0.582
0.578
0.560
0.548
0.524


jhu(R)
vME+SM
jhu
MEbfs.pos
vME
css244
MEbfs
MEfix
umd-sst
duluth 8
duluth 10
duluth Z
duluth 7
duluth 6
duluth X
duluth 9
ua
duluth

0.702
0.702
0.683
0.681
0.678
0.661
0.652
0.646
0.621
0.612
0.611
0.603
0.592
0.590
0.586
0.557
0.514
0.464

Nouns
jhu(R)
vME+SM
MEbfs.pos
jhu
vME
MEbfs
css244
MEfix
duluth 8
duluth Z
duluth 10
umd-sst
duluth 6
duluth 7
duluth X
duluth 9
duluth
ua

Table 17: vME+SM Spanish lexicalsample task Senseval-2
results show methods like SM combined order achieve
good disambiguation results. results line Pedersen (2002),
also presents comparative evaluation systems participated Spanish
English lexical-sample tasks Senseval-2. focus pair comparisons
systems assess degree agree, measuring difficulty test
instances included tasks. several systems largely agreement,
little benefit combining since redundant simply reinforce
other. However, systems disambiguate instances others not,
systems complementary may possible combine take advantage
different strengths system improve overall accuracy.
results nouns (only applying SM), shown Table 18, indicate SM
low level agreement methods. However, measure optimal
combination quite high, reaching 89% (1.000.11) pairing SM JHU.
323

fiMontoyo, Suarez, Rigau, & Palomar

fact, seven methods achieved highest optimal combination value
paired SM method.
System pair nouns
SM JHU
SM Duluth7
SM DuluthY
SM Duluth8
SM Cs224
SM Umcp
SM Duluth9

OKa One OK
0.29
0.32
0.27
0.34
0.25
0.35
0.28
0.32
0.28
0.32
0.26
0.33
0.26
0.31

b

Zero OK
0.11
0.12
0.12
0.13
0.13
0.14
0.16

c

Kappa
0.06
0.03
0.01
0.08
0.09
0.06
0.14



Table 18: Optimal combination systems participated Spanish
lexicalsample tasks Senseval-2
a.
b.
c.
d.

Percentage instances systems answers correct.
Percentage instances one answer correct.
Percentage instances none answers correct.
kappa statistic (Cohen, 1960) measure agreement multiple systems (or judges)
scaled agreement would expected chance. value 1.00 suggests complete
agreement, 0.00 indicates pure chance agreement.

combination circumstances suggests SM, knowledge-based method,
fundamentally different others (i.e., corpus-based) methods, able
disambiguate certain set instances methods fail. fact, SM different
method uses structure WordNet.
4.3.2 Senseval-2 English lexicalsample task
experiment done Senseval-2 English lexicalsample task data
results shown Table 19. details different systems built
consulted Section 3.2
Again, see Table 19 BFS per POS better per word, mainly
reasons explained Section 3.3.
Nevertheless, improvement nouns using vME+SM system high
Spanish data. differences corpora significant relevance
precision values obtained. example, English data includes
multi-words sense inventory extracted WordNet, Spanish data
smaller dictionary built task specifically, smaller polysemy
degree.
results vME+SM comparable systems presented Senseval-2
best system (Johns Hopkins University) reported 64.2% precision (68.2%, 58.5%
73.9% nouns, verbs adjectives, respectively).
Comparing results obtained section 4.2, also see using
voting system best feature selection Specification Marks vME+SM,
using nonoptimized relevant domain heuristic, obtain similar
performance. is, seems obtain comparable performance combining different
324

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

Nouns Verbs Adjectives
MEfix
0.601 0.656 0.519
0.696
MEbfs 0.606 0.658 0.519
0.696
MEbfs.pos 0.609 0.664 0.519
0.696
vME+SM 0.619 0.667 0.535
0.707
MEfix: 0mcbWsdrvK3 words
MEbfs: word
best feature selection
MEbfs.pos: 0Wsrdm nouns,
0sbcprdmK10 verbs,
0mcbWsdrvK3 adjectives
vME+SM: majority voting MEfix,
MEbfs.pos, MEbfs, Specification Marks

Table 19: Precision Results Using Best Feature Selection Specification Marks
Senseval-2 English lexicalsample task data

classifiers resulting feature selection process using richer set features (adding
domain features) much less computational overhead.
analysis results Senseval-2 English Spanish lexicalsample
tasks demonstrates knowledge-based corpus-based WSD systems cooperate
combined obtain improved WSD systems. results empirically demonstrate combination approaches outperforms individually,
demonstrating approaches could considered complementary.

5. Conclusions
main hypothesis work WSD requires different kinds knowledge sources
(linguistic information, statistical information, structural information, etc.) techniques.
aim paper explore methods collaboration complementary
knowledge-based corpus-based WSD methods. Two complementary methods
presented: specification marks (SM) maximum entropy (ME). Individually,
benefits drawbacks. shown methods collaborate obtain better
results WSD.
order demonstrate hypothesis, three different schemes combining
approaches presented. presented different mechanisms combining
information sources around knowledge-based corpus-based WSD methods.
also shown combination approaches outperforms methods individually, demonstrating approaches could considered complementary. Finally,
shown knowledge-based method help corpus-based method better
perform disambiguation process, vice versa.
order help specification marks method, disambiguates nouns
context target word. selects nouns means previous analysis
training data order identify ones seem highly accurately disambiguated.
325

fiMontoyo, Suarez, Rigau, & Palomar

preprocess fixes nouns reducing search space knowledge-based method.
turn, helped SM providing domain information nouns contexts.
information incorporated learning process form features.
comparing accuracy methods, without contribution
other, demonstrated combining schemes WSD methods possible
successful.
Finally, presented voting system nouns included four classifiers, three
based ME, one based SM. cooperation scheme obtained
best score nouns compared systems submitted Senseval-2 Spanish
lexicalsample task comparable results submitted Senseval-2 English
lexicalsample task.
presently studying possible improvements collaboration
methods, extending information two methods provide
taking advantage merits one.

Acknowledgments
authors wish thank anonymous reviewers Journal Artificial Intelligence Research COLING 2002, 19th International Conference Computational
Linguistics, helpful comments earlier drafts paper. earlier paper (Suarez &
Palomar, 2002b) corpus-based method (subsection 3.2) presented COLING
2002.
research partially funded Spanish Government project CICyT number TIC2000-0664-C02-02 PROFIT number FIT-340100-2004-14 Valencia Government project number GV04B-276 EU funded project MEANING (IST-2001-34460).

References
Agirre, E., & Martinez, D. (2001a). Decision lists english basque. Proceedings
SENSEVAL-2 Workshop. conjunction ACL2001/EACL2001 Toulouse,
France.
Agirre, E., & Martinez, D. (2001b). Knowledge sources word sense disambiguation.
Proceedings International Conference Text, Speech Dialogue (TSD2001)
Selezna Ruda, Czech Republic.
Agirre, E., & Rigau, G. (1996). Word Sense Disambiguation using Conceptual Density.
Proceedings 16th International Conference Computational Linguistic (COLING96 Copenhagen, Denmark.
Berger, A. L., Pietra, S. A. D., & Pietra, V. J. D. (1996). maximum entropy approach
natural language processing. Computational Linguistics, 22 (1), 3971.
326

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

Bruce, R., & Wiebe, J. (1994). Word sense disambiguation using decomposable models.
Proceedings 32nd Annual Meeting Association Computational Linguistics (ACL1994), pp. 139145 Las Cruces, US.
Cabezas, C., Resnik, P., & Stevens, J. (2001). Supervised Sense Tagging using Support
Vector Machines. Proceedings Second International Workshop Evaluating
Word Sense Disambiguation Systems (SENSEVAL-2) Toulouse, France.
Cardie, C., & Mooney, R. J. (1999). Guest editors introduction: Machine learning
natural language. Machine Learning, 34 (1-3), 59.
Cohen, J. (1960). coefficient agreement nominal scales. Educ. Psychol. Meas., 20,
3746.
Cowie, J., Guthrie, J., & Guthrie, L. (1992). Lexical disambiguation using simulated annealing. Proceedings 14th International Conference Computational Linguistic,
COLING92, pp. 359365 Nantes, France.
Cuadros, M., Atserias, J., Castillo, M., & Rigau, G. (2004). Automatic acquisition sense
examples using exretriever. IBERAMIA Workshop Lexical Resources
Web Word Sense Disambiguation. Puebla, Mexico.
Dang, H. T., yi Chia, C., Palmer, M., & Chiou, F.-D. (2002). Simple features chinese
word sense disambiguation. Chen, H.-H., & Lin, C.-Y. (Eds.), Proceedings
19th International Conference Computational Linguistics (COLING2002).
Dietterich, T. G. (1998). Approximate statistical test comparing supervised classification
learning algorithms. Neural Computation, 10 (7), 18951923.
Escudero, G., Marquez, L., & Rigau, G. (2000). Boosting applied word sense disambiguation. Proceedings 12th Conference Machine Learning ECML2000
Barcelona, Spain.
Fellbaum, C. (Ed.). (1998). WordNet. Electronic Lexical Database. MIT Press.
Fernandez-Amoros, D., Gonzalo, J., & Verdejo, F. (2001). Role Conceptual Relations Word Sense Disambiguation. Proceedings 6th International Conference
Application Natural Language Information Systems (NLDB2001)., pp. 8798
Madrid, Spain.
Hoste, V., Daelemans, W., Hendrickx, I., & van den Bosch, A. (2002). Evaluating results
memory-based word-expert approach unrestricted word sense disambiguation.
Proceedings ACL2002 Workshop Word Sense Disambiguation: Recent
Successes Future Directions, pp. 95101 PA, USA.
Ide, N., & Veronis, J. (1998). Introduction Special Issue Word Sense Disambiguation: State Art. Computational Linguistics, 24 (1), 140.
Leackock, C., Chodorow, M., & Miller, G. (1998). Using corpus statistics wordnet
relations sense identification. Computational Linguistics. Special Issue WSD,
24 (1).
327

fiMontoyo, Suarez, Rigau, & Palomar

Lesk, M. (1986). Automated sense disambiguation using machine-readable dictionaries:
tell pine cone ice cream cone. Proceedings 1986 SIGDOC
Conference, Association Computing Machinery, pp. 2426 Toronto, Canada.
Magnini, B., & Strapparava, C. (2000). Experiments Word Domain Disambiguation
Parallel Texts. Proceedings ACL Workshop Word Senses Multilinguality Hong Kong, China.
Magnini, B., Strapparava, C., Pezzulo, G., & Gliozzo, A. (2002). Role Domain
Information Word Sense Disambiguation. Natural Language Engineering, 8 (4),
359373.
Manning, C., & Schutze, H. (Eds.). (1999). Foundations Statistical Natural Language
Processing. MIT Press.
Manning, C. D., & Schutze, H. (1999). Foundations Statistical Natural Language Processing. MIT Press, Cambridge, Massachusetts.
McRoy, S. W. (1992). Using multiple knowledge sources word sense discrimination.
Computational Linguistics, 18 (1), 130.
Mihalcea, R. (2002). Instance based learning automatic feature selection applied
word sense disambiguation. Chen, H.-H., & Lin, C.-Y. (Eds.), Proceedings
19th International Conference Computational Linguistics (COLING2002).
Mihalcea, R., & Moldovan, D. (1999). Method word sense disambiguation unrestricted text. Proceedings 37th Annual Meeting Association
Computational Linguistic, ACL99, pp. 152158 Maryland, Usa.
Miller, G. A. (1995). Wordnet: lexical database english. Communications ACM,
38 (11), 3941.
Miller, G. A., Leacock, C., Tengi, R., & Bunker, T. (1993). Semantic Concordance.
Proceedings ARPA Workshop Human Language Technology, pp. 303308
Plainsboro, New Jersey.
Mitchell, T. M. (Ed.). (1997). Machine Learning. McGraw Hill.
Montoyo, A., & Palomar, M. (2001). Specification Marks Word Sense Disambiguation:
New Development. Gelbukh, A. F. (Ed.), CICLing, Vol. 2004 Lecture Notes
Computer Science, pp. 182191. Springer.
Montoyo, A., & Suarez, A. (2001). University Alicante word sense disambiguation
system.. Preiss, & Yarowsky (Preiss & Yarowsky, 2001), pp. 131134.
Montoyo, A., Palomar, M., & Rigau, G. (2001). WordNet Enrichment Classification
Systems.. Proceedings WordNet Lexical Resources: Applications,
Extensions Customisations Workshop. (NAACL-01) Second Meeting
North American Chapter Association Computational Linguistics, pp. 101
106 Carnegie Mellon University. Pittsburgh, PA, USA.
328

fiCombining Knowledge- Corpus-based Word-Sense-Disambiguation Methods

Montoyo, A., Vazquez, S., & Rigau, G. (2003). Metodo de desambiguacion lexica basada
en el recurso lexico Dominios Relevantes. Procesamiento del Lenguaje Natural, 31,
141148.
Ng, H. (1997). Exemplar-Base Word Sense Disambiguation: Recent Improvements.
Proceedings 2nd Conference Empirical Methods Natural Language
Processing, EMNLP.
Ng, H. T., & Lee, H. B. (1996). Integrating multiple knowledge sources disambiguate word
senses: exemplar-based approach. Joshi, A., & Palmer, M. (Eds.), Proceedings
34th Annual Meeting Association Computational Linguistics San
Francisco. Morgan Kaufmann Publishers.
Pedersen, T. (2002). Assessing System Agreement Instance Difficulty Lexical
Sample Tasks Senseval-2. Proceedings Workshop Word Sense Disambiguation: Recent Successes Future Directions, ACL2002 Philadelphia, USA.
Preiss, J., & Yarowsky, D. (Eds.). (2001). Proceedings SENSEVAL-2, Toulouse, France.
ACL-SIGLEX.
Ratnaparkhi, A. (1998). Maximum Entropy Models Natural Language Ambiguity Resolution. Ph.D. thesis, University Pennsylvania.
Rigau, G., Agirre, E., & Atserias, J. (1997). Combining unsupervised lexical knowledge
methods word sense disambiguation. Proceedings joint 35th Annual Meeting
Association Computational Linguistics 8th Conference European
Chapter Association Computational Linguistics ACL/EACL97 Madrid,
Spain.
Rigau, G., Taule, M., Fernandez, A., & Gonzalo, J. (2001). Framework results
spanish senseval.. Preiss, & Yarowsky (Preiss & Yarowsky, 2001), pp. 4144.
Snedecor, G. W., & Cochran, W. G. (1989). Statistical Methods (8 edition). Iowa State
University Press, Ames, IA.
Snyder, B., & Palmer, M. (2004). english all-words task. Proceedings
3rd ACL workshop Evaluation Systems Semantic Analysis Text
(SENSEVAL-3). Barcelona, Spain.
Suarez, A., & Palomar, M. (2002a). Feature selection analysis maximum entropy-based
wsd. Gelbukh, A. F. (Ed.), CICLing, Vol. 2276 Lecture Notes Computer
Science, pp. 146155. Springer.
Suarez, A., & Palomar, M. (2002b). maximum entropy-based word sense disambiguation
system. Chen, H.-H., & Lin, C.-Y. (Eds.), Proceedings 19th International
Conference Computational Linguistics (COLING2002), pp. 960966.
Sussna, M. (1993). Word sense disamiguation free-text indexing using massive semantic
network. . Proceedings Second International Conference Information
Knowledge Base Management, CIKM93, pp. 6774 Arlington, VA.
329

fiMontoyo, Suarez, Rigau, & Palomar

Tapanainen, P., & Jarvinen, T. (1997). non-projective dependency parser. Proceedings
Fifth Conference Applied Natural Language Processing, pp. 6471.
Towell, G. G., & Voorhees, E. M. (1998). Disambiguating highly ambiguous words. Computational Linguistics, 24 (1), 125145.
Veenstra, J., den Bosch, A. V., Buchholz, S., Daelemans, W., & Zavrel, J. (2000). Memorybased word sense disambiguation. Computers Humanities, Special Issue
SENSEVAL, 34 (12), 171177.
Wilks, Y., Fass, D., Guo, C.-M., McDonald, J., Plate, T., & Slator, B. (1993). Providing machine tractable dictionary tools. Pustejovsky, J. (Ed.), Semantics
lexicon, pp. 341401. Kluwer Academic Publishers.
Yarowsky, D. (1994). Decision lists lexical ambiguity resolution: Application accent
restoration spanish french.. Proceedings 32nd Annual Meeting
Association Computational Linguistics (ACL1994) Las Cruces, NM,.

330

fiJournal Artificial Intelligence Research 23 (2005) 587-623

Submitted 7/04; published 5/05

Improved Search Algorithm
Optimal Multiple-Sequence Alignment
Stefan Schroedl

stefan.schroedl@gmx.de

848 14th St
San Francisco CA 94114
+1 (415) 522-1148

Abstract
Multiple sequence alignment (MSA) ubiquitous problem computational biology.
Although NP -hard find optimal solution arbitrary number sequences,
due importance problem researchers trying push limits exact
algorithms further. Since MSA cast classical path finding problem, attracting growing number AI researchers interested heuristic search algorithms
challenge actual practical relevance.
paper, first review two previous, complementary lines research. Based
Hirschbergs algorithm, Dynamic Programming needs O(kN k1 ) space store
search frontier nodes needed reconstruct solution path, k sequences
length N . Best first search, hand, advantage bounding search
space explored using heuristic. However, necessary maintain
explored nodes final solution order prevent search re-expanding
higher cost. Earlier approaches reduce Closed list either incompatible
pruning methods Open list, must retain least boundary Closed
list.
article, present algorithm attempts combining respective
advantages; like uses heuristic pruning search space, reduces
maximum Open Closed size O(kN k1 ), Dynamic Programming.
underlying idea conduct series searches successively increasing upper bounds,
using DP ordering key Open priority queue. suitable choice
thresholds, practice, running time four times expected.
experiments show algorithm outperforms one currently
successful algorithms optimal multiple sequence alignments, Partial Expansion ,
time memory. Moreover, apply refined heuristic based optimal alignments
pairs sequences, larger subsets. idea new; however,
make practically relevant show equally important bound heuristic
computation appropriately, overhead obliterate possible gain.
Furthermore, discuss number improvements time space efficiency
regard practical implementations.
algorithm, used conjunction higher-dimensional heuristics, able calculate first time optimal alignment almost problems Reference 1
benchmark database BAliBASE .

1. Introduction: Multiple Sequence Alignment
multiple sequence alignment problem (MSA) computational biology consists aligning several sequences, e.g. related genes different organisms, order reveal simic
2005
AI Access Foundation. rights reserved.

fiSchroedl

larities differences across group. Either DNA directly compared,
underlying alphabet consists set {C,G,A,T} four standard nucleotide bases
cytosine, guanine, adenine thymine; compare proteins, case
comprises twenty amino acids.
Roughly speaking, try write sequences one
columns matching letters maximized; thereby gaps (denoted additional
letter ) may inserted either order shift remaining characters
better corresponding positions. Different letters column interpreted
caused point mutations course evolution substituted one amino
acid another one; gaps seen insertions deletions (since direction
change often known, also collectively referred indels). Presumably,
alignment fewest mismatches indels constitutes biologically plausible
explanation.
host applications MSA within computational biology; e.g., determining evolutionary relationship species, detecting functionally active sites
tend preserved best across homologous sequences, predicting threedimensional protein structure.
Formally, one associates cost alignment tries find (mathematically)
optimal alignment, i.e., one minimum cost. designing cost function,
computational efficiency biological meaning taken account.
widely-used definition sum-of-pairs cost function. First, given symmetric
(|| + 1)2 matrix containing penalties (scores) substituting letter another one
(or gap). simplest case, could one mismatch zero match,
biologically relevant scores developed. Dayhoff, Schwartz, Orcutt
(1978) proposed model molecular evolution estimate exchange
probabilities amino acids different amounts evolutionary divergence; gives rise
so-called PAM matrices, PAM250 generally widely used; Jones,
Taylor, Thornton (1992) refined statistics based larger body experimental
data. Based substitution matrix, sum-of-pairs cost alignment defined
sum penalties letter pairs corresponding column positions.
pairwise alignment conveniently depicted path two opposite
corners two-dimensional grid (Needleman Wunsch, 1981): one sequence placed
horizontal axis left right, one vertical axis top
bottom. gap either string, path moves diagonally right; gap
vertical (horizontal) string represented horizontal (vertical) move right (down),
since letter consumed one strings. alignment graph directed
acyclic, (non-border) vertex incoming edges left, top, top-left
adjacent vertices, outgoing edges right, bottom, bottom-right vertices.
Pairwise alignment readily generalized simultaneous alignment multiple
sequences, considering higher-dimensional lattices. example, alignment three
sequences visualized path cube. Fig. 1 illustrates example strings
ABCB, BCD, DB. also shows computation sum-of-pairs cost, hypothetical
substitution matrix. real example (problem 2trx BAliBASE , see Sec. 7.3) given
Fig. 2.
588

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

Alignment:

Substitution matrix:

B C _ B
_ B C _
_ _ _ B

B C _
0 2 4 2 3
B
1 3 3 3
C
2 2 3

1 3
_
0

Cost: 6+7+8+7+7 = 35

end



C

B

B



B



C

B

start

Figure 1: Fictitious alignment problem: Column representation, cost matrix, threedimensional visualization alignment path cube.

number improvements integrated sum-of-pairs cost, like associating
weights sequences, using different substitution matrices sequences varying
evolutionary distance. major issue multiple sequence alignment algorithms
ability handle gaps. Gap penalties made dependent neighbor letters.
Moreover, found (Altschul, 1989) assigning fixed score indel
sometimes produce biologically plausible alignment. Since insertion
sequence x letters likely x separate insertions single letter, gap cost
functions introduced depend length gap. useful approximation
affine gap costs, distinguish opening extension gap charge
+ b x gap length x, appropriate b. Another frequently used modification
waive penalties gaps beginning end sequence.
Technically, order deal affine gap costs longer identify nodes
search graph lattice vertices, since cost associated edge depends
preceding edge path. Therefore, suitable store lattice edges priority
589

fiSchroedl

1thx
1grx
1erv
2trcP

_aeqpvlvyfwaswcgpcqlmsplinlaantysdrlkvvkleidpnpttvkkyk______vegvpal
__mqtvi__fgrsgcpysvrakdlaeklsnerdd_fqyqyvdiraegitkedlqqkagkpvetvp__
agdklvvvdfsatwcgpckmikpffhslsekysn_viflevdvddcqdvasece______vksmptf
_kvttivvniyedgvrgcdalnssleclaaeypm_vkfckira_sntgagdrfs______sdvlptl

1thx
1grx
1erv
2trcP

rlvkgeqildstegvis__kdkllsf_ldthln_________
qifvdqqhiggytdfaawvken_____lda____________
qffkkgqkvgefsgan___kek_____leatine__lv____
lvykggelisnfisvaeqfaedffaadvesflneygllper_

Figure 2: Alignment problem 2trx BAliBASE , computed algorithm settings
described Sec. 7.3.



B

C




g = 53

cost(_,C)=3
gap penalty = 4
g = 60

B


cost(A,_)=3
gap penalty = 4
g = 60

cost(A,C)=4
gap penalty = 0
g = 57



Figure 3: Example computing path costs affine gap function; substitution matrix
Fig. 1 gap opening penalty 4 used.

queue, let transition costs u v, v w sum-of-pairs substitution costs
using one character sequence gap, plus incurred gap penalties
v w followed u v. representation adopted program MSA (Gupta,
Kececioglu, & Schaeffer, 1995). Note state space representation grows
factor 2k . example successor costs calculated, cost matrix
Fig. 1 gap opening penalty 4, shown Fig. 3.
convenience terminology sequel still refer nodes dealing
search algorithm.
590

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

2. Overview
Wang Jiang (1994) shown optimal multiple sequence alignment problem
NP -hard; therefore, cannot hope achieve efficient algorithm arbitrary number
sequences. consequence, alignment tools widely used practice sacrifice
sound theoretical basis exact algorithms, heuristic nature (Chan, Wong, &
Chiu, 1992). wide variety techniques developed. Progressive methods build
alignment gradually, starting closest sequences successively adding
distant ones. Iterative strategies refine initial alignment sequence
improvement steps.
Despite limitation moderate number sequences, however, research
exact algorithms still going on, trying push practical boundaries further. still
form building block heuristic techniques, incorporating existing tools
could improve them. example, algorithm iteratively aligning two groups sequences
time could three more, better avoid local minima. Moreover,
theoretically important gold standard available evaluation comparison,
even problems.
Since MSA cast minimum-cost path finding problem, turns
amenable heuristic search algorithms developed AI community; actually
among currently best approaches. Therefore, many researchers area
often used puzzles games past study heuristic search algorithms, recently
rising interest MSA testbed practical relevance, e.g., (Korf, 1999;
Korf & Zhang, 2000; Yoshizumi, Miura, & Ishida, 2000; Zhou & Hansen, 2003b); study
also led major improvements general search techniques.
pointed definition MSA problem given
one; competes attempts formalizing biological meaning, often
imprecise depends type question biologist investigator pursuing. E.g.,
paper concerned global alignment methods, find alignment
entire sequences. Local methods, contrast, geared towards finding maximally similar
partial sequences, possibly ignoring remainder.
next section, briefly review previous approaches, based dynamic programming incorporating lower upper bounds. Sec. 4, describe new algorithm
combines extends ideas, allows reduce storage Closed
nodes partially recomputing solution path end (Sec. 5). Moreover, turns
algorithms iterative deepening strategy transferred find good balance
computation improved heuristics main search (Sec. 6), issue
previously major obstacle practical application. Sec. 7 presents
experimental comparison Partial Expansion (Yoshizumi, Miura, & Ishida, 2000),
one currently successful approaches. also solve two problems
Reference 1 widely used benchmark database BAliBASE (Thompson, Plewniak, &
Poch, 1999). best knowledge, achieved previously
exact algorithm.
591

fiSchroedl

3. Previous Work
number exact algorithms developed previously compute alignments
moderate number sequences. mostly constrained available
memory, required computation time, both. roughly
group two categories: based dynamic programming paradigm,
proceed primarily breadth-first fashion; best-first search, utilizing lower upper
bounds prune search space. recent research, including new algorithm
introduced Sec. 4, attempts beneficially combine approaches.
3.1 Dijkstras Algorithm Dynamic Programming
Dijkstra (1959) presented general algorithm finding shortest (resp. minimum cost)
path directed graph. uses priority queue (heap) store nodes v together
shortest found distance start node (i.e., top-left corner grid) v
(also called g-value v). Starting priority queue, step,
edge minimum g-value removed priority queue; expansion consists
generating successors (vertices right and/or below) reachable one step,
computing respective g-value adding edge cost previous g-value,
inserting turn priority queue case newly found distance smaller
previous g-value. time node expanded, g-value guaranteed
minimal path cost start node, g (v) = d(s, v). procedure runs
priority queue becomes empty, target node (the bottom-right corner grid)
reached; g-value constitutes optimal solution cost g (t) = d(s, t)
alignment problem. order trace back path corresponding cost, move
backwards start node choosing predecessors minimum cost. nodes either
stored fixed matrix structure corresponding grid, dynamically
generated; latter case, explicitly store node backtrack-pointer
optimal parent.
integer edge costs, priority queue implemented bucket array pointing
doubly linked lists (Dial, 1969), operations performed constant time
(To precise, DeleteMin-operation also needs pointer runs different
g-values once; however, neglect comparison number expansions).
expand vertex, 2k 1 successor vertices generated, since
choice introducing gap sequence. Thus, Dijkstras algorithm solve
multiple sequence alignment problem O(2k N k ) time O(N k ) space k sequences
length N .
means reduce number nodes stored path reconstruction
associating counter node maintains number children whose
backtrack-pointer refers (Gupta et al., 1995). Since node expanded
once, number referring backtrack-pointers decrease, namely,
whenever cheaper path one children found. nodes reference count goes
zero, whether immediately expansion later loses child,
deleted good. way, keep nodes memory least one
descendant currently priority queue. Moreover, auxiliary data structures vertices
592

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

coordinates efficiently stored tries (prefix trees); equipped
reference counters well freed accordingly longer used edge.
complexity Dijkstras algorithm holds dynamic programming (DP);
differs former one scans nodes fixed order known
beforehand (hence, contrary name exploration scheme actually static).
exact order scan vary (e.g., row-wise column-wise), long compatible
topological ordering graph (e.g., two sequences cells left, top,
diagonally top-left explored prior cell). One particular ordering
antidiagonals, diagonals running upper-right lower-left. calculation
antidiagonal node merely amounts summing k coordinates.
Hirschberg (1975) noticed order determine cost optimal alignment g (t), would necessary store whole matrix; instead, proceeding
e.g. rows suffices keep track k time, deleting row soon
next one completed. reduces space requirement one dimension
O(N k ) O(kN k1 ). order recover solution path end, re-computation
lost cell values needed. Divide-and-conquer -strategy applies algorithm twice
half grid each, forward backward direction, meeting fixed
middle row. adding corresponding forward backward distances middle
row finding minimum, one cell lying optimal path recovered.
cell essentially splits problem two smaller subproblems, one upper left
corner it, one lower right corner; recursively solved
using method. two dimensions, computation time doubled,
overhead reduces even higher dimensions.
FastLSA algorithm (Davidson, 2001) refines Hirschbergs algorithm exploiting additionally available memory store one node optimal path,
thereby reducing number re-computations.
3.2 Algorithms Utilizing Bounds
Dijkstras algorithm dynamic programming viewed variants breadthfirst search, achieve best first search expand nodes v order estimate
(lower bound) total cost path passing v. Rather using
g-value Dijkstras algorithm, use f (v) := g(v) + h(v) heap key,
h(v) lower bound cost optimal path v t. h indeed admissible,
first solution found guaranteed optimal (Hart, Nilsson, & Raphael, 1968).
classical best-first search algorithm, algorithm, well known artificial
intelligence community. context, priority queue maintaining generated nodes
often also called Open list, nodes already expanded
removed constitute Closed list. Fig. 4 schematically depicts snapshot
two-dimensional alignment problem, nodes f -value larger current
fmin expanded. Since accuracy heuristic decreases distance
goal, typical onion-shaped distribution results, bulk located closer
start node, tapering towards higher levels.
algorithm significantly reduce total number expanded generated
nodes; therefore, higher dimensions clearly superior dynamic programming. How593

fiSchroedl



ax
um


ia


er
et

Closed
Open
Possible back leak

Le

ls

(a
n

tid

ia
go

na
ls)

X

Start
X X
X
X
X
X

X
X
X
X
X
End

Figure 4: Snapshot best-first search pairwise alignment (schematically).

ever, contrast Hirschberg algorithm, still stores explored nodes
Closed list. Apart keeping track solution path, necessary prevent
search leaking back, following sense.
heuristic h called consistent h(x) h(x0 )+c(x, x0 ), node x child x0 .
consistent heuristic ensures (as case Dijkstras algorithm) time node
expanded, g-value optimal, hence never expanded again. However, try
delete Closed nodes, topologically smaller nodes Open
higher f -value; expanded later stage, lead re-generation
node non-optimal g-value, since first instantiation longer available
duplicate checking. Fig. 4, nodes might subject spurious re-expansion
marked X.
Researchers tried avoid leaks, retaining basic search scheme.
Korf proposed store list forbidden operators node, place parents
deleted node Open f -value infinity (Korf, 1999; Korf & Zhang, 2000). However,
Zhou Hansen (2003a) remark, hard combine algorithm techniques
reduction Open list, moreover storage operators lets size
nodes grow exponentially number sequences. algorithm, keep
track kernel Closed list, defined set nodes
Closed nodes parents; otherwise Closed node said boundary. key
idea boundary nodes maintained, since shield kernel
re-expansions. algorithm gets close memory limit nodes
kernel deleted; backtrack pointer children changed parents
594

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

deleted nodes, become relay nodes them. final reconstruction
optimal solution path, algorithm called recursively relay node bridge
gap missing edges.
addition Closed list, also Open list grow rapidly sequence alignment
problems. Particularly, since original algorithm expansion node generates
children once, whose f -value larger optimal cost g (t) kept
heap end, waste much available space.
upper bound U optimal solution cost g (t) known, nodes v
f (v) > U pruned right away; idea used several articles (Spouge, 1989; Gupta
et al., 1995). One successful approaches Yoshizumi et al.s (2000) Partial
Expansion (PEA ). node stores additional value F , minimum
f -value yet ungenerated children. step, node minimum
F -value expanded, children f = F generated. algorithm
clearly generates nodes f value larger optimal cost, cannot
avoided altogether. However, overhead computation time considerable:
straightforward implementation, want maintain nodes constant size, generating
one edge requires determining f -values successors, interior node
eventually fully expanded computation time order square
number successors, grows O(2k ) number sequences k.
remedy, paper proposed relax condition generating children
f F + C, small C.
alternative general search strategy uses linear space iterative
deepening (IDA ) (Korf, 1985). basic algorithm conducts depth-first search
pre-determined threshold f -value. search, keeps track smallest
f -value generated successor larger threshold. solution found,
provides increased threshold used next search iteration.
Wah Shang (1995) suggested liberal schemes determining next threshold dynamically order minimize number recomputations. IDA efficient
tree structured search spaces. However, difficult detect duplicate expansions without additional memory; Therefore, unfortunately applicable lattice-structured
graphs like sequence alignment problem due combinatorially explosive number
paths two given nodes.
different line research tries restrict search space breadth-first approaches incorporating bounds. Ukkonen (1985) presented algorithm pairwise
alignment problem particularly efficient similar sequences; computation time
scales O(dm), optimal solution cost. First consider problem deciding
whether solution exists whose cost less upper threshold U . restrict
evaluation DP matrix band diagonals minimum number
indels required reach diagonal, times minimum indel cost, exceed U .
general, starting minimum U value, successively double G test
returns solution; increase computation time due recomputations also
bounded factor 2.
Another approach multiple sequence alignment make use lower bounds h
. key idea following: Since nodes f -value lower g (t)
expanded anyway order guarantee optimality, might well explore
595

fiSchroedl

reasonable order, like Dijkstras algorithm DP, knew optimal
cost. Even slightly higher upper bounds still help pruning. Spouge (1989) proposed
bound DP vertices v g(v) + h(v) smaller upper bound g (t).
Linear Bounded Diagonal Alignment (LBD-Align) (Davidson, 2001) uses upper
bound order reduce computation time memory solving pairwise alignment
problem dynamic programming. algorithm calculates DP matrix one antidiagonal time, starting top left corner, working towards bottom-right.
would check bound every expansion, LBD-Align checks
top bottom cell diagonal. e.g. top cell diagonal pruned,
remaining cells row pruned well, since reachable
it; means pruning frontier next row shifted one. Thus,
pruning overhead reduced quadratic linear amount terms
sequence length.
3.3 Obtaining Heuristic Bounds
assumed lower upper bounds, without specifying derive them.
Obtaining inaccurate upper bound g (t) fairly easy, since use cost
valid path lattice. Better estimates e.g. available heuristic linear-time
alignment programs FASTA BLAST (Altschul, Gish, Miller, Myers, & Lipman,
1990), standard method database searches. Davidson (2001) employed
local beam search scheme.
Gusfield (1993) proposed approximation called star-alignment.
sequences aligned, one consensus sequence chosen sum pairwise
alignment costs rest sequences minimal. Using best sequence
center, ones aligned using gap, always gap rule. Gusfield
showed cost optimal alignment greater equal cost star
alignment, divided (2 2/k).
use heuristic estimates, lower bounds k-alignment often based
optimal alignments subsets < k sequences. general, vertex v k-space,
looking lower bound path v target corner t. Consider first
case = 2. cost path is, definition, sum edge costs,
edge cost turn sum pairwise (replacement gap) penalties. multiple
sequence alignment induces pairwise alignment sequences j, simply copying
rows j ignoring columns rows. pairwise alignments
visualized projection alignment onto faces, cf. Fig. 1.
interchange summation order, sum-of-pairs cost sum pairwise
alignment costs respective paths projected face, cannot smaller
optimal pairwise path cost. Thus, construct admissible heuristic hpair
computing, pairwise alignment cell pairwise problem,
cheapest path cost goal node.
optimal solutions pairwise alignment problems needed lower bound h
values usually computed prior main search preprocessing step (Ikeda & Imai,
1994). end, suffices apply ordinary DP procedure; however, since time
interested lowest cost path v t, runs backward direction,
596

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

proceeding lower right corner upper left, expanding possible parents
vertex step.
Let U upper bound cost optimal multiple sequence alignment G.
sum optimal alignment costs Lij = d(sij , tij ) pairwise subproblems i, j
{1, . . . , k}, < j, call L, lower bound G. Carrillo Lipman (1988) pointed
additivity sum-of-pairs cost function, pairwise alignment induced
optimal multiple sequence alignment = U L larger
respective optimal pairwise alignment. bound used restrict number
values computed preprocessing stage stored
calculation heuristic: pair sequences i, j, nodes v feasible
path start node si,j goal node ti,j exists total cost
Li,j + . optimize storage requirements, combine results two
searches. First, forward pass determines relevant node v minimum distance
d(sij , v) start node. subsequent backward pass uses distance like exact
heuristic stores distance d(v, tij ) target node nodes
d(sij , v) + d(v, tij ) d(s, t) + 1 .
Still, larger alignment problems required storage size extensive.
program MSA (Gupta et al., 1995) allows user adjust values CarrilloLipman bound individually pair sequences. makes possible generate
least heuristic alignments time memory doesnt allow complete solution;
moreover, recorded search -bound actually reached.
negative case, optimality found solution still guaranteed; otherwise, user
try run program slightly increased bounds.
general idea precomputing simplified problems storing solutions use
heuristic explored name pattern databases (Culberson & Schaeffer,
1998). However, approaches implicitly assume computational cost
amortized many search instances target. contrast, case MSA,
heuristics instance-specific, strike balance. discuss
greater depth Sec. 6.2.

4. Iterative-Deepening Dynamic Programming
seen, fixed search order dynamic programming several advantages pure best-first selection.
Since Closed nodes never reached search, safe
delete useless ones (those part shortest path current Open
1. slight technical complication arises affine gap costs: recall DP implementations usually charge
gap opening penalty g-value edge e starting gap, edge e0 ending gap
carries extra penalty all. However, since sum pairs heuristics h computed backward
direction, using algorithm would assign penalty path instead e0 .
means heuristic f = g + h would longer guaranteed lower bound, since
contains penalty twice. remedy, necessary make computation symmetric charging
beginning end gap half cost each. case beginning end
sequences handled conveniently starting search dummy diagonal edge
((1, . . . , 1), (0, . . . , 0)), defining target edge dummy diagonal edge ((N, . . . , N ), (N +
1, . . . , N + 1)), similar arrows shown Fig. 1.

597

fiSchroedl

nodes) apply path compression schemes, Hirschberg algorithm.
sophisticated schemes avoiding back leaks required, abovementioned methods core set maintenance dummy node insertion Open.
Besides size Closed list, memory requirement Open list determined maximum number nodes open simultaneously
time algorithm running. f -value used key
priority queue, Open list usually contains nodes f -values range
(fmin , fmin + ); set nodes generally spread across search space,
since g (and accordingly h = (f g)) vary arbitrarily 0 fmin + .
opposed that, DP proceeds along levels antidiagonals rows, iteration
k levels maintained time, hence size
Open list controlled effectively. Fig. 4, pairwise alignment partitioned antidiagonals: maximum number open nodes two adjacent
levels four, total amounts seventeen2 .
practical purposes, running time measured terms
number node expansions, one also take account execution time
needed expansion. arranging exploration order edges
head node (or generally, sharing common coordinate prefix)
dealt one other, much computation cached, edge
generation sped significantly. come back point Sec. 6.
remaining issue static exploration scheme consists adequately bounding
search space using h-values. known minimal terms number node
expansions. knew cost g (t) cheapest solution path beforehand, could
simply proceed level level grid, however immediately prune generated edges
e whenever f (e) > g (t). would ensure generate edges would
generated algorithm , well. upper threshold would additionally help
reduce size Closed list, since node pruned children lie beyond
threshold; additionally, node child parent, give rise
propagating chain ancestor deletions.
propose apply search scheme carries series searches successively larger thresholds, solution found (or run memory patience).
use upper bound parallels IDA algorithm.
resulting algorithm, refer Iterative-Deepening Dynamic Programming (IDDP), sketched Fig. 5. outer loop initializes threshold
lower bound (e.g., h(s)), and, unless solution found, increases upper bound.
manner IDA algorithm, order make sure least one additional edge explored iteration threshold increased correspondingly
least minimum cost fringe edge exceeded previous threshold.
fringe increment maintained variable minNextThresh, initially estimated
upper bound, repeatedly decreased course following expansions.
2. Contrary figure might suggest, open two nodes per level pairwise
alignments, set nodes worse fmin contains holes.

598

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

procedure IDDP(Edge startEdge, Edge targetEdge, int lowerBound, int upperBound)
int thresh = lowerBound
{Outer loop: Iterative deepening phases}
(thresh upperBound)
Heap h = {(startEdge, 0)}
int minNextThresh = upperBound
{Inner loop: Bounded dynamic programming}
(not h.IsEmpty())
Edge e = h.DeleteMin() {Find remove edge minimum level}
(e == targetEdge)
{Optimal alignment found}
return TraceBackPath(startEdge, targetEdge)
end
Expand(e, thresh, minNextThresh)
end
int threshIncr = ComputeThreshIncr() {Compute search threshold next iteration, see text}
thresh = max(thresh + threshIncr, minNextThresh)
end
print(No alignment cost upperBound found)

Figure 5: Algorithm Iterative-Deepening Dynamic Programming.
step inner loop, select remove node priority queue
whose level minimal. explained later Sec. 6, favorable break ties according
lexicographic order target nodes. Since total number possible levels
comparatively small known advance, priority queue implemented using
array linked lists (Dial, 1969); provides constant time operations insertion
deletion.
expansion edge e partial (Fig. 6). child edge might already exist
earlier expansion edge head vertex; test decrease
g-value. Otherwise, generate new edge, temporarily sake calculating f -value; is, f -value exceeds search threshold current iteration,
memory immediately reclaimed. Moreover, case fringe threshold minNextThresh updated. practical implementation, prune unnecessary accesses
partial alignments inside calculation heuristic e.GetH() soon search
threshold already reached.
relaxation child edge within threshold performed subprocedure
UpdateEdge (cf. Fig. 7). similar corresponding relaxation step , updating
childs g- f values, parent pointers, inserting Open, already
contained. However, contrast best-first search, inserted heap according
antidiagonal level head vertex. Note event former parent loses
last child, propagation deletions (Fig. 8) ensure Closed nodes
continue stored belong solution path. Edge deletions also ensue
deletion dependent vertex coordinate data structures (not shown pseudocode).
situation gives rise deletions immediately expansion
node children pointing back (the children might either reachable cheaply
different nodes, f -value might exceed threshold).
599

fiSchroedl

procedure Expand(Edge e, int thresh, int minNextThresh)
Edge child Succ(e)
{Retrieve child tentatively generate yet existing, set boolean variable created
accordingly}
int newG = e.GetG() + GapCost(e, child)
+ child.GetCost()
int newF = newG + child.GetH()
(newF thresh newG < child.GetG())
{Shorter path current best found, estimate within threshold}
child.SetG(newG)
UpdateEdge(e, child, h) {Update search structures}
else (newF > thresh)
minNextThresh =
min(minNextThresh, newF)
{Record minimum pruned edges}
(created)
Delete(child) {Make sure promising edges stored}
end
end
end
(e.ref == 0)
DeleteRec(e) {No promising children could inserted heap}
end

Figure 6: Edge expansion IDDP.
procedure UpdateEdge(Edge parent, Edge child, Heap h)
parent.ref++
child.GetBacktrack().ref
(child.GetBacktrack().ref == 0)
DeleteRec(child.GetBacktrack()) {The former parent lost last child becomes useless}
end
child.SetBacktrack(parent)
(not h.Contains(child))
h.Insert(child, child.GetHead().GetLevel())
end

Figure 7: Edge relaxation IDDP.

correctness algorithm shown analogously soundness proof .
threshold smaller g (t), DP search terminate without encountering
solution; otherwise, nodes pruned cannot part optimal path.
invariant holds always node level lies optimal path
Open list. Therefore, algorithm terminates heap runs empty,
best found solution indeed optimal.
iterative deepening strategy results overhead computation time due reexpansions, trying restrict overhead much possible. precisely,
600

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

procedure DeleteRec(Edge e)
(e.GetBacktrack() 6= nil)
e.GetBacktrack().ref
(e.GetBacktrack().ref == 0)
DeleteRec(e.GetBacktrack())
end
end
Delete(e)

Figure 8: Recursive deletion edges longer part solution path.
procedure TraceBack(Edge startEdge, Edge e)
(e == startEdge)
return {End recursion}
end
(e.GetBackTrack().GetTarget() 6= e.GetSource())
{Relay node: recursive path reconstruction}
IDDP( e.GetBackTrack(), e, e.GetF(), e.GetF())
end
OutputEdge(e)
TraceBack(startEdge, e.GetBackTrack())

Figure 9: Divide-and-Conquer solution reconstruction reverse order.
want minimize ratio
=

nIDDP
,
nA

nIDDP nA denote number expansions IDDP , respectively. One
way (Wah & Shang, 1995) choose threshold sequence 1 , 2 , . . .
number expansions ni stage satisfies
ni = rni1 ,
fixed ratio r. choose r small, number re-expansions hence
computation time grow rapidly, choose big, threshold
last iteration exceed optimal solution cost significantly, explore many
irrelevant edges. Suppose n0 rp < nA n0 rp+1 . algorithm performs p + 1
iterations. worst case, overshoot maximal finds optimal solution
previous threshold, nA = n0 rp + 1. total number expansions
P
r(r p+1 1)
r2

n0 p+1
, ratio becomes approximately r1
. setting
i=0 r = n0
r1
derivative expression zero, find optimal value r 2; number
expansions double one search stage next. achieve doubling,
expand four times many nodes .
Like Wah Shangs (1995) scheme, dynamically adjust threshold using runtime information. Procedure ComputeThreshIncr stores sequence expansion numbers
thresholds previous search stages, uses curve fitting extrapolation
(in first iterations without sufficient data available, small default threshold
applied). found distribution nodes n() f -value smaller equal
601

fiSchroedl

threshold modeled accurately according exponential approach
n() = B .
Consequently, order attempt double number expansions, choose next
threshold according
1
i+1 = +
.
log2 B

5. Sparse Representation Solution Paths
search progresses along antidiagonals, fear back leaks,
free prune Closed nodes. Similarly Zhou Hansens (2003a) work, however,
want delete lazily incrementally forced algorithm
approaching computers memory limit.
deleting edge e, backtrack-pointers child edges refer
redirected respective predecessor e, whose reference count increased accordingly.
resulting sparse solution path representation, backtrack pointers point
optimal ancestors.
termination main search, trace back pointers starting goal
edge; outlined Procedure TraceBack (Fig. 9), prints solution path
reverse order. Whenever edge e points back ancestor e0 direct
parent, apply auxiliary search start edge e0 goal edge e order reconstruct
missing links optimal solution path. search threshold fixed
known solution cost; moreover, auxiliary search prune edges cannot
ancestors e coordinate greater corresponding coordinate
e. Since also shortest distance e e0 known, stop first path
found cost. improve efficiency auxiliary search even further,
heuristic could recomputed suit new target. Therefore, cost restoring
solution path usually marginal compared main search.
edges going prune, order? simplicity, assume
moment Closed list consists single solution path. According Hirschberg
approach, would keep one edge, preferably lying near center search
space (e.g., longest anti-diagonal), order minimize complexity two
auxiliary searches. additional available space allowing store three relay edges,
would divide search space four subspaces equal size (e.g., additionally
storing antidiagonals half-way middle antidiagonal start node resp.
target node). extension, order incrementally save space diminishing
resources would first keep every level, every fourth, on,
start edge, target edge, one edge half-way path would left.
Since general Closed list contains multiple solution paths (more precisely, tree
solution paths), would like density relay edges
them. case k sequences, edge reaching level l head node originate
tail node level l 1, . . . , l k. Thus, every solution path passes
level, deleting every level could result leaving one path completely intact,
extinguishing another totally. Thus, better consider contiguous bands k
602

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

procedure SparsifyClosed()
(int sparse = 1 blog2 N c)
(UsedMemory() > maxMemory exists {Edge e Open | e.GetLastSparse() <
sparse})
Edge pred = e.GetBacktrack()
{Trace back solution path}
(pred 6= nil e.GetLastSparse() < sparse)
e.SetLastSparse(sparse) {Mark avoid repeated trace-back}
(bpred.GetHead().GetLevel() / kc mod 2sparse 6= 0)
{pred lies prunable band: redirect pointer}
e.SetBacktrack(pred.GetBacktrack())
e.GetBacktrack().ref++
pred.ref
(pred.ref == 0)
{e last remaining edge referring pred}
DeleteRec(pred)
end
else
{Not prunable band: continue traversal}
e = e.GetBacktrack()
end
pred = e.GetBacktrack()
end
end
end

Figure 10: Sparsification Closed list restricted memory.

levels each, instead individual levels. Bands size cannot skipped path.
total number antidiagonals alignment problem k sequences length N
k N 1; thus, decrease density blog2 N c steps.
technical implementation issue concerns ability enumerate edges reference given prunable edge, without explicitly storing list. However,
reference counting method described ensures Closed edge reached
following path bottom-up edge Open. procedure sketched Fig. 10.
variable sparse denotes interval level bands maintained
memory. inner loop, paths Open nodes traversed backward direction;
edge e0 falls prunable band, pointer successor e path
redirected respective backtrack pointer. e last edge referencing e0 ,
latter one deleted, path traversal continues start edge. Open
nodes visited memory bound still exceeded, outer loop tries
double number prunable bands increasing sparse.
Procedure SparsifyClosed called regularly search, e.g., expansion.
However, naive version described would incur huge overhead computation
time, particularly algorithms memory consumption close limit. Therefore, optimizations necessary. First, avoid tracing back solution path
(or lower) sparse interval recording edge interval
603

fiSchroedl

traversed last time (initially zero); increased variable sparse
anything left pruning. worst case, edge inspected blog2 N c
times. Secondly, would inefficient actually inspect Open node inner
loop, find solution path traversed previously, higher
sparse value; however, appropriate bookkeeping strategy possible reduce
time search overhead O(k).

6. Use Improved Heuristics
seen, estimator hpair , sum optimal pairwise goal distances, gives
lower bound actual path length. However, powerful heuristics also
conceivable. computation require resources, trade-off prove
worthwhile; tighter estimator is, smaller space main search
needs explore.
6.1 Beyond Pairwise Alignments
Kobayashi Imai (1998) suggested generalize hpair considering optimal solutions
subproblems size > 2. proved following heuristics admissible
informed pairwise estimate.
hall,m sum m-dimensional optimal costs, divided

k2
m2 .

hone,m splits sequences two sets sizes k m; heuristic sum
optimal cost first subset, plus second one, plus sum
2-dimensional optimal costs pairs sequences different subsets. Usually,
chosen close k/2.
improved heuristics reduce main search effort orders magnitudes.
However, contrast pairwise sub-alignments, time space resources devoted compute store higher-dimensional heuristics general longer negligible compared
main search. Kobayashi Imai (1998) noticed even case = 3
triples sequences, impractical compute entire subheuristic hall,m . one
reduction, show suffices restrict oneself nodes path cost
exceed optimal path cost subproblem
!

=

X
k2
U
d(si1 ,...,im , ti1 ,...,im );
m2
,...,i
1



threshold seen generalization Carrillo-Lipman bound. However,
still
incur excessive overhead space computation time computation
k

lower-dimensional subproblems. drawback requires upper bound
U , whose accuracy also algorithms efficiency hinges. could improve bound
applying sophisticated heuristic methods, seems counterintuitive spend
time would rather use calculate exact solution. spite
advantages main search, expensiveness heuristic calculation appears
major obstacle.
604

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

McNaughton, Lu, Schaeffer, Szafron (2002) suggested partition heuristic
(hyper-) cubes using hierarchical oct-tree data structure; contrast full cells,
empty cells retain values surface. main search tries use one
them, interior values recomputed demand. Still, work assumes
node entire heuristic calculated least using dynamic programming.
see one cause dilemma implicit assumption complete computation
necessary. bound refers worst-case, generally include many
nodes actually required main search. However, since dealing
heuristic, actually afford miss values occasionally; might
slow main search, cannot compromise optimality final solution.
Therefore, propose generate heuristics much smaller bound . Whenever
attempt retrieve value m-dimensional subheuristic
fails main

search, simply revert replacing sum 2 optimal pairwise goal distances
covers.
believe IDDP algorithm lends well make productive use higherdimensional heuristics. Firstly importantly, strategy searching adaptively
increasing thresholds transferred -bound well; addressed
detail next section.
Secondly, far practical implementation concerned, important take
account higher-dimensional heuristic affects number node expansions,
also time complexity. time dominated number accesses subalignments. k sequences, worst case edge 2k 1 successors, leading
total
!
k
k
(2 1)

evaluations hall,m . One possible improvement enumerate edges emerging
given vertex lexicographic order, store partial sums heuristics prefix subsets
sequences later re-use. way, allow cache linear size, number
accesses reduced
!
i=k
X
i1
;
2
m1
i=m
correspondingly, quadratic cache need
i=k
X
i=m

!

2



i2
m2

evaluations. instance, aligning 12 sequences using hall,3 , linear cache reduces
evaluations 37 percent within one expansion.
mentioned above, contrast , IDDP gives us freedom choose
particular expansion order edges within given level. Therefore, sort edges
lexicographically according target nodes, much cached prefix information
shared additionally across consecutively expanded edges. higher dimension
subalignments, larger savings. experiments, experienced speedups
eighty percent heuristic evaluation.
605

fiSchroedl

Execution time [s]

100

Main search
Heuristic
Total time

10

1

0.1
0

10

20

30 40 50 60 70
Heuristic miss ratio r [%]

80

90

100

Figure 11: Trade-off heuristic main search: Execution times problem 1tvxA
function heuristic miss ratio.

6.2 Trade-Off Computation Heuristic Main Search
seen, control size precomputed sub-alignments choosing
bound f -values edges generated beyond respective optimal
solution cost. obviously trade-off auxiliary main searches.
instructive consider heuristic miss ratio r, i.e., fraction calculations
heuristic h main search requested entry partial MSA
precomputed. optimum main search achieved heuristic
computed every requested edge (r = 0). Going beyond point generate
unnecessarily large heuristic containing many entries never actually used.
hand, free allocate less effort heuristic, resulting r > 0
consequently decreasing performance main search. Generally, dependence
S-shaped form, exemplified Fig. 11 case problem 1tvxA BAliBASE
(cf. next section). Here, execution time one iteration main search fixed
threshold 45 lower bound shown, includes optimal solution.
Fig. 11 illustrates overall time trade-off auxiliary main search, fix
different levels. minimum total execution time, sum auxiliary
main search, attained r = 0.15 (5.86 seconds). plot corresponding
memory usage trade-off similar shape.
Unfortunately, general know advance right amount auxiliary search.
mentioned above, choosing according Carrillo-Lipman bound ensure
606

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

Execution time [s]

100

10

1

0.1
0

10

20

30

40
50
60
70
Heuristic miss ratio r [%]

80

90

100

Figure 12: Time last iteration main search problem 1tvxA function
heuristic miss ratio.

every requested sub-alignment cost precomputed; however, general
considerably overestimate necessary size heuristic.
remedy, algorithm IDDP gives us opportunity recompute heuristic
threshold iteration main search. way, adaptively strike balance
two.
currently experienced miss rate r rises threshold, suspend
current search, recompute pairwise alignments increased threshold ,
resume main search improved heuristics.
Like main search, accurately predict auxiliary computation time
space threshold using exponential fitting. Due lower dimensionality,
generally increase less steeply; however, constant factor might higher
k
heuristic, due combinatorial number
alignment problems solved.
doubling scheme explained bound overhead within constant
factor effort last iteration. way, also limiting heuristic
computation time fixed fraction main search, ensure expected
upper bound overall execution time stays within constant factor search
time would required using pairwise heuristic.
knew exact relation , r, speedup main search, ideal
strategy would double heuristic whenever expected computation time smaller
time saved main search. However, illustrated Fig. 12, dependence
complex simple exponential growth, varies search depth specifics
problem. Either would need elaborate model search space,
607

fiSchroedl

algorithm would conduct exploratory searches order estimate relation.
leave issue future work, restrict simplified, conservative
heuristic: hypothesize main search made twice fast heuristic
doubling miss rate r rises 25 percent; experiments, found
assumption almost always true. event, since effective branching factor
main search reduced improved heuristic, also ignore history main search
times exponential extrapolation procedure subsequent iterations.

7. Experimental Results
following, compare IDDP one currently successful approaches,
Partial Expansion . empirically explore benefit higher-dimensional heuristics;
finally, show feasibility means benchmark database BAliBASE .
7.1 Comparison Partial Expansion
first series evaluations, ran IDDP set sequences chosen
Yoshizumi et al. (2000) (elongation factors EF-TU EF-1 various species,
high degree similarity). work, substitution costs chosen according
PAM-250 matrix. applied heuristic sum optimal pairwise goal distances.
expansion numbers completely match results, however, since applied
biologically realistic affine gap costs: gaps length x charged 8+8x, except
beginning end sequence, penalty 8 x.
following experiments run RedHat Linux 7.3 Intel XeonT
CPU 3.06 GHz, main memory 2 Gigabytes; used gcc 2.96 compiler.
total space consumption search algorithm determined peak number
Open Closed edges entire running time. Table 1 Fig. 13 give values
series successively larger sets input sequences (with sequences numbered
defined Yoshizumi et al., 2000) 1 4, 1 5, . . ., 1 12.
implementation, basic algorithm could carried 9
sequences, exhausting computers main memory.
Confirming results Yoshizumi et al. (2000), Partial Expansion requires
one percent space. Interestingly, iteration peak total numbers
nodes held memory, nodes actually closed except problem 6. might
explained high degree similarity sequences example. Recall
PEA closes node successors f -value
optimal solution cost; span lower bound small, node least
one bad successor exceeds difference.
IDDP reduces memory requirements factor 6. diagram
also shows maximum size Open list alone. sequences, difference
two dominated linear length store solution path.
problem size increases, however, proportion Closed list total memory drops
12 percent 12 sequences. total number expansions (including
search stages) slightly higher PEA ; however, due optimizations made possible
control expansion order, execution time 12 sequences reduced
third.
608

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

Num
Exp

Time
[sec]

Max
Open

Max
Open +
Closed

626
1599
3267
10781
116261
246955


0.01
0.05
0.25
1.94
49.32
318.58

7805
32178
124541
666098
9314734
35869671

8432
33778
127809
676880
9430996
36116627

3
4
5
6
7
8
9
10
11
12

448
716
2610
6304
23270
330946
780399
5453418
20887627
36078736

PEA
0.01
0.01
0.05
0.33
2.63
87.24
457.98
7203.17
62173.78
237640.14

442
626
1598
3328
10874
118277
249279
1569815
5620926
9265949

442
626
1598
3331
10874
118277
249279
1569815
5620926
9265949

3
4
5
6
7
8
9
10
11
12

496
1367
6776
12770
26026
362779
570898
4419297
21774869
36202456

IDDP
0.01
0.02
0.14
0.59
2.46
73.62
250.48
4101.96
43708.14
158987.80

4
9
171
414
889
13620
21506
160240
860880
1417151

434
443
501
972
1749
19512
30009
192395
997163
1616480

4
5
6
7
8
9

Table 1: Algorithm comparison varying number input sequences (elongation factors
EF-TU EF-1).

Since PEA prune edges, maximum space usage always total number
edges f -value smaller g (t) (call edges relevant edges, since
inspected admissible algorithm). IDDP, hand, Open list
comprise k adjacent levels edges (not counting possible threshold
overshoot, would contribute factor 2). Thus, improvement IDDP
PEA tend increase overall number levels (which sum
609

fiSchroedl

1e+08
1e+07

Edges memory

1e+06
100000
10000
1000
100
A* max open+closed
PEA* max open+closed
IDDP max open+closed
IDDP max open

10
1
3

4

5

6

7
8
9
Number sequences

10

11

12

Figure 13: Memory requirements , IDDP, PEA (elongation factors EF-TU
EF-1).

string lengths), divided number sequences; words, average
sequence length.
Moreover, ratio depends well heuristic suits particular problem.
Fig. 14 shows distribution edges f value smaller equal g (t),
case 9 example sequences. problem quite extreme bulk edges
concentrated small level band 1050 1150. example
even distribution, Fig. 15 depicts situation problem 1cpt Reference 1
benchmark set BAliBASE (Thompson et al., 1999) heuristic hall,3 . case,
proportion overall 19492675 relevant edges maximal among 4 adjacent
levels amounts 0.2 percent. maximum Open size IDDP 7196,
total number edges generated PEA 327259, improvement factor
45.
7.2 Multidimensional Heuristics
set sequences, compared different improved heuristics order get
impression respective potential. Specifically, ran IDDP heuristics hpair ,
hall,3 , hall,4 , hone,k/2 various thresholds . Fig. 16 shows total execution time
computing heuristics, performing main search. case, manually
selected value minimized time. seen times hone,k/2
lie little bit hpair ; sequences (less six), computation
heuristics hall,3 hall,4 dominates overall time. increasing dimensions, how610

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

0.016

Open edges / sum open edges [%]

0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
0

500

1000

1500

2000
Level

2500

3000

3500

4000

Figure 14: Distribution relevant edges levels (elongation factors EF-TU EF-1);
compare schematic projection Fig. 4.

ever, investment starts yield growing returns, hall,3 fastest algorithm,
requiring 5 percent time hpair 12 sequences.
far memory concerned, Fig. 17 reveals maximum size Open
Closed list, chosen values, similar hpair hone,k/2 one hand,
hall,3 hall,4 hand.
12 sequences, hone,6 saves 60 percent edges, hall,3 needs 2.6
percent hall,4 0.4 percent space required pairwise heuristic. Using
IDDP, never ran main memory; even larger test sets could aligned, range
shown diagrams limited patience wait results two
days.
Based experienced burden computing heuristic, Kobayashi Imai (1998)
concluded hone,m preferred hall,m . quite agree judgment. see heuristic hall,m able reduce search space main search
considerably stronger hone,m , beneficial appropriate
amount heuristic computation.
7.3 Benchmark Database BAliBASE
BAliBASE (Thompson et al., 1999) widely used database manually-refined multiple
sequence alignments specifically designed evaluation comparison multiple sequence alignment programs. alignments classified 8 reference sets. Reference 1
contains alignments six equidistant sequences. sequences sim611

fiSchroedl

Open edges / sum open edges [%]

0.00012

0.0001

8e-05

6e-05

4e-05

2e-05

0
0

200

400

600

800
Level

1000

1200

1400

1600

Figure 15: Distribution relevant edges levels, problem 1cpt BAliBASE .
ilar length; grouped 9 classes, indexed sequence length percentage
identical amino acids columns. Note many problems indeed much harder elongation factor examples previous section; despite
consisting fewer sequences, dissimilarities much pronounced.
applied algorithm Reference 1, substitution costs according PET91
matrix (Jones et al., 1992) affine gap costs 9x+8, except leading trailing gaps,
gap opening penalty charged. instances, precomputed pairwise
sub-alignments fixed bound 300 optimal solution; optimal solution
found within bound cases, effort generally marginal compared
overall computation. problems involving three sequences, heuristic
hall,3 applied.
82 alignment problems Reference 1, algorithm could solve 2
problems (namely, 1pamA gal4 ) computer. Detailed results listed Tables 2
10.
Thompson, Plewniak, Poch (1999) compared number widely used heuristic
alignment tools using so-called SP -score; software calculates percentage
correctly aligned pairs within biologically significant motifs. found programs perform equally well sequences medium high amino acid
identity; differences occurred case distant sequences less
25 percent identity, so-called twilight zone. Particularly challenging
group short sequences. subgroup, three highest scoring programs PRRP,
CLUSTALX, SAGA, respective median scores 0.560, 0.687, 0.529.
medium score alignments found experiments amounts 0.558; hence,
good PRRP, beaten CLUSTALX. focused exper612

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

1e+06
100000
10000

Total time [sec]

1000
100
10
1
0.1
2-fold heuristic
div-conq heuristic
3-fold heuristic
4-fold heuristic

0.01
0.001
2

4

6

8
10
Number sequences

12

14

Figure 16: Comparison execution times (including calculation heuristics), elongation
factors EF-TU EF-1.

iments algorithmic feasibility rather solution quality, would worthwhile
attempt improve alignments found program using refined
penalty functions. CLUSTALX, example, uses different PAM matrices depending
evolutionary distance sequences; moreover, assigns weights sequences (based
phylogenetic tree), gap penalties made position-specific. improvements easily integrated basic sum-of-pairs cost function, could
attempt compute optimal alignment respect metrics. leave line
research future work.
Fig. 18 shows maximum number edges stored Open
search, dependence search threshold final iteration. better comparability,
included problems diagram consist 5 sequences. logarithmic
scale emphasizes growth fits exponential curve quite well. Roughly speaking,
increase cost threshold 50 leads ten-fold increase space requirements.
relation similarly applicable number expansions (Fig. 19).
Fig. 20 depicts proportion maximum Open list size combined
maximum size Open Closed. clearly visible due pruning edges
outside possible solution paths, Closed list contributes less less overall
space requirements difficult problems become.
Finally, estimate reduction size Open list compared relevant
edges ratio maximum Open size last iteration IDDP total
number expansions stage, equal number edges f -value
less equal threshold. Considering possible overshoot IDDP, algorithm PEA
613

fiSchroedl

Maximum size open + closed

1e+07

1e+06

100000

10000

1000

2-fold heuristic
div-conq heuristic
3-fold heuristic
4-fold heuristic

100
2

4

6

8
10
Number sequences

12

14

Figure 17: Combined maximum size Open Closed, different heuristics (elongation
factors EF-TU EF-1).

1e+07
1e+06

Max open

100000
10000
1000
100
10

Short
Medium length
Long

1
0

50

100
150
200
Threshold - Lower bound

250

300

Figure 18: Maximum size Open list, dependent final search threshold (BAliBASE ).

614

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

1e+08
1e+07

Expansions

1e+06
100000
10000
1000
100

Short
Medium length
Long

10
0

50

100
150
200
Threshold - Lower bound

250

300

Figure 19: Number expansions final search iteration (BAliBASE ).

80

Max open/ Max open + closed [%]

70
60
50
40
30
20
10

Short
Medium length
Long

0
0

50

100
150
200
Threshold - Lower bound

250

300

Figure 20: Maximum number Open edges, divided combined maximum Open
Closed (BAliBASE ).

615

fiSchroedl

5

Short
Medium length
Long

Max Open / Expansions [%]

4

3

2

1

0
0

50

100
150
200
Threshold - Lower bound

250

300

Figure 21: Percentage reduction Open size (BAliBASE ).
would expand least half nodes. proportion ranges 0.5 5 percent
(cf. Fig. 21). considerable scatter indicates dependence individual problem properties; however, slight average decrease noticed difficult problems.

616

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

8. Conclusion Discussion
presented new search algorithm optimal multiple sequence alignment
combines effective use heuristic bound best-first search ability
dynamic programming approach reduce maximum size Open Closed lists
one order magnitude sequence length. algorithm performs series
searches successively increasing bounds explore search space DP order;
thresholds chosen adaptively expected overhead recomputations
bounded constant factor.
demonstrated algorithm outperform one currently
successful algorithms optimal multiple sequence alignments, Partial Expansion ,
terms computation time memory consumption. Moreover, iterative-deepening
strategy alleviates use partially computed higher-dimensional heuristics. best
knowledge, algorithm first one able solve standard benchmark
alignment problems BAliBASE biologically realistic cost function including affine
gap costs without end gap penalties. quality alignment range
best heuristic programs; concentrated algorithmic feasibility, deem
worthwhile incorporate refined cost metrics better results; study
question future work.
Recently, learned related approaches developed simultaneously independently Zhou Hansen (2003b, 2004). SweepA explores search graph according
layers partial order, still uses f -value selecting nodes within one layer.
Breadth-First Heuristic Search implicitly defines layers graph uniform costs
according breadth-first traversal. algorithms incorporate upper bounds
optimal solution cost pruning; however, idea adaptive threshold determination
limit re-expansion overhead constant factor described. Moreover,
consider flexible use additional memory minimize divide-and-conquer solution
reconstruction phase.
Although described algorithm entirely within framework MSA problem,
straightforward transfer domain state space graph directed
acyclic. Natural candidates include applications ordering imposed
time space coordinates, e.g., finding likely path Markov model.
Two BAliBASE benchmark problems could still solved algorithm
within computers main memory limit. Future work include integration
techniques exploiting secondary memory. expect level-wise exploration scheme
algorithm lends naturally external search algorithms, another currently
active research topic Artificial Intelligence theoretical computer science.

Acknowledgments
author would like thank reviewers article whose comments helped
significantly improving it.

617

fiSchroedl

Appendix

Table 2: Results BAliBASE Reference 1, group short sequences low amino acid
identity. columns denote: number aligned sequences; upper
bound precomputing optimal solutions partial problems last iteration
main search; g (t) optimal solution cost; h(s) lower bound solution cost,
using heuristics; #Exp total number expansions iterations main
search; #Op peak number edges Open list course search;
#Op+Cl peak combined number edges either Open Closed list
search; #Heu peak number subalignment edge costs stored heuristic;
Time: total running time including auxiliary main search, seconds; Mem
peak total memory usage face alignments, heuristic, main search,
KB.
1aboA
1idy
1r69
1tvxA
1ubi
1wit
2trx


5
5
4
4
4
5
4


57
50
20
44
30
69
20

g (t)
9006
8165
6215
5532
7395
14287
7918

h(s)
8898
8075
6183
5488
7357
14176
7899

#Exp
3413786
1732008
634844
1263849
1614286
6231378
63692

#Op
104613
74865
19938
24226
26315
209061
3502

#Op+Cl
176126
121404
41719
48633
54059
351582
5790

#Heu
1654547
970933
88802
476622
289599
2442098
127490

Time
331.029
167.867
22.517
52.860
62.133
578.907
4.572

Mem
15568
10893
3568
5278
5448
27273
1861

Table 3: Short sequences, medium similarity.
1aab
1fjlA
1hfh
1hpi
1csy
1pfc
1tgxA
1ycc
3cyr
451c


4
6
5
4
5
5
4
4
4
5


20
20
30
20
30
30
20
20
48
49

g (t)
6002
13673
16556
5858
14077
15341
4891
8926
8480
11440

h(s)
5984
13625
16504
5835
14026
15277
4856
8903
8431
11333

#Exp
263
900
137914
1560
52718
118543
18987
54049
583260
1213162

#Op
12
106
4852
83
3872
6477
543
1118
13422
38004

618

#Op+Cl
83
155
8465
164
5613
8905
1080
2010
25806
54115

#Heu
4404
19573
70471
5269
56191
55887
5507
77156
193690
583363

Time
0.572
0.985
14.077
0.679
6.165
11.850
1.196
3.780
22.592
111.675

Mem
691
1589
2882
656
2252
2478
649
1644
3076
6529

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

Table 4: Short sequences, high similarity.

5
5
4
5
4
5
5
5
5
5

1aho
1csp
1dox
1fkj
1fmb
1krn
1plc
2fxb
2mhr
9rnt


20
20
20
20
20
20
20
20
20
20

g (t)
8251
8434
7416
13554
7571
9752
12177
6950
14317
12382

h(s)
8187
8427
7405
13515
7568
9747
12152
6950
14306
12367

#Exp
30200
90
782
2621
172
101
454
88
256
350

#Op
2255
2
50
140
4
1
25
2
4
19

#Op+Cl
3074
78
186
222
108
87
103
71
121
108

#Heu
10971
3528
8406
10925
1804
6244
10641
1432
7853
6100

Time
3.175
0.569
0.652
0.945
0.540
0.623
0.728
0.534
0.668
0.695

Mem
1042
784
823
1511
788
1035
1415
617
1558
1250

Table 5: Medium-length sequences, low similarity.

1bbt3
1sbp
1havA
1uky
2hsdA
2pia
3grs
kinase


5
5
5
4
4
4
4
5


160
200
200
94
96
161
126
200

g (t)
30598
42925
31600
18046
21707
22755
20222
45985

h(s)
30277
42512
31234
17915
21604
22616
20061
45520

#Exp
902725789
2144000052
2488806444
179802791
65580608
97669470
107682032
2446667393

#Op
11134608
6839269
10891271
659435
293357
789446
640391
13931051

#Op+Cl
15739188
11882990
16321376
1281339
668926
1673807
1396982
19688961

#Heu
23821767
65341855
58639851
15233338
12497761
25718770
24104710
32422084

Time
43860.175
106907.000
132576.000
7006.560
2646.880
4310.030
4267.880
125170.460

Mem
927735
735053
927735
106184
67788
142318
130425
927734

Table 6: Medium-length sequences, medium similarity.

1ad2
1aym3
1gdoA
1ldg
1mrj
1pgtA
1pii
1ton
2cba


4
4
4
4
4
4
4
5
5


20
20
58
20
20
50
20
102
160

g (t)
16852
19007
20696
25764
20790
17442
20837
32564
40196

h(s)
16843
18978
20613
25736
20751
17398
20825
32428
39914

#Exp
379
466536
10795040
446123
252601
1870204
25256
13571887
60545205

#Op
16
4801
57110
4981
4067
19200
584
351174
1037828

619

#Op+Cl
221
8914
102615
9052
7380
32476
1414
526102
1595955

#Heu
27887
83634
1265777
169038
33942
485947
116670
11549908
19186631

Time
0.959
15.386
363.549
16.115
8.694
73.066
3.089
1373.180
2904.651

Mem
2186
3163
12028
4484
2905
5869
3338
58704
140712

fiSchroedl

Table 7: Medium-length sequences, high similarity.

5
4
5
4
5
4
4
5
4
5

1amk
1ar5A
1ezm
1led
1ppn
1pysA
1thm
1tis
1zin
5ptp


20
20
20
20
20
20
20
20
20
20

g (t)
31473
15209
37396
18795
27203
19242
21470
35444
16562
29776

h(s)
31453
15186
37381
18760
27159
19215
21460
35395
16546
29735

#Exp
447
3985
613
93220
18517
10810
361
31996
771
6558

#Op
7
128
4
2956
489
190
2
448
23
309

#Op+Cl
259
356
324
4951
864
801
293
915
225
539

#Heu
13120
22220
15751
39962
20209
14344
8090
42716
6619
37883

Time
0.825
1.066
0.836
3.761
2.545
1.200
0.682
4.409
0.654
1.767

Mem
3366
1755
3900
2564
2991
2224
2469
4122
1767
3600

Table 8: Long sequences, low similarity.

1ajsA
1cpt
1lvl
1ped
2myr
4enl


4
4
4
3
4
3


160
160
160
50
200
50

g (t)
38382
39745
43997
15351
43414
16146

h(s)
38173
39628
43775
15207
43084
16011

#Exp
318460012
873548
537914936
2566052
3740017645
5169296

#Op
1126697
5260
1335670
7986
7596730
9650

#Op+Cl
2310632
12954
2706940
27718
45488908
30991

#Heu
27102589
10494564
37491416
0
118747184
0

Time
9827.233
223.926
16473.420
20.035
136874.980
41.716

#Heu
18464119
96176
101816849
12801019
1476154
6040375
31318364
5962640
3585721
75819994
38368530
22622910

Time
6815.760
7.829
8795.000
843.402
334.475
348.134
2251.190
505.778
463.962
32965.522
15972.000
733.202

Mem
208951
32119
255123
4447
927735
5589

Table 9: Long sequences, medium similarity.

1ac5
1adj
1bgl
1dlc
1eft
1fieA
1gowA
1pkm
1sesA
2ack
arp
glg


4
4
4
4
4
4
4
4
5
5
5
5


92
20
243
106
56
86
166
89
58
250
143
160

g (t)
37147
32815
78366
47430
31377
53321
38784
36356
57670
76937
54939
74282

h(s)
37020
32785
78215
47337
31301
53241
38632
36256
57557
76466
54696
74059

#Exp
169779871
207072
188429118
14993317
9379999
6905957
45590739
11197890
4755983
994225856
182635167
9251905

#Op
732333
3106
857008
65288
42620
46779
275256
75144
96014
8077412
1291185
87916

620

#Op+Cl
1513853
5145
1744149
126608
72502
90937
544800
140472
136677
12436928
2160263
120180

Mem
124877
4595
291618
43158
13115
26884
99537
27244
27452
765715
193364
72148

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

Table 10: Long sequences, high similarity.

1ad3
1gpb
1gtr
1lcf
1rthA
1taq
3pmg
actin


4
5
5
6
5
5
4
5


20
54
60
160
128
250
51
53

g (t)
33641
101296
55242
149249
69296
133723
42193
48924

h(s)
33604
101231
55133
148854
69133
133321
42133
48826

#Exp
104627
1232707
2037633
181810148
14891538
1693501628
1036943
824295

621

#Op
2218
62184
54496
3235312
71081
9384718
8511
35283

#Op+Cl
3461
98476
91656
3824010
105082
17298456
15540
53009

#Heu
34539
2702949
1916127
28614215
24587882
145223167
777639
777058

Time
4.196
178.610
226.791
15363.051
1721.070
5713.240
50.796
96.147

Mem
3968
25698
18050
294688
70569
1170673
8133
11198

fiSchroedl

References
Altschul, S., Gish, W., Miller, W., Myers, E., & Lipman, D. (1990). Basic local alignment
search tool. Journal Molecular Biology, 215, 403410.
Altschul, S. F. (1989). Gap costs multiple sequence alignment. Journal Theoretical
Biology, 138, 297309.
Carrillo, H., & Lipman, D. (1988). multiple sequence alignment problem biology.
SIAM Journal Applied Mathematics, 5 (48), 10731082.
Chan, S. C., Wong, A. K. C., & Chiu, D. K. Y. (1992). survey multiple sequence
comparison techniques. Bulletin Mathematical Biology, 54 (4), 563598.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14 (4), 318334.
Davidson, A. (2001). fast pruning algorithm optimal sequence alignment. Proceedings 2nd IEEE International Symposium Bioinformatics Bioengineering
(BIBE2001), pp. 4956.
Dayhoff, M. O., Schwartz, R. M., & Orcutt, B. C. (1978). model evolutionary change
proteins. Dayhoff, M. O. (Ed.), Atlas Protein Sequence Structure, pp.
345352, Washington, D.C. National Biomedical Research Foundation.
Dial, R. B. (1969). Shortest-path forest topological ordering. Comm. ACM, 12 (11),
632633.
Dijkstra, E. W. (1959). note two problems connection graphs.. Numerische
Mathematik, 1, 269271.
Gupta, S., Kececioglu, J., & Schaeffer, A. (1995). Improving practical space time
efficiency shortest-paths approach sum-of-pairs multiple sequence alignment.
J. Computational Biology, 2 (3), 459472.
Gusfield, D. (1993). Efficient methods multiple sequence alignment guaranteed
error bounds. Bull. Math. Biol., 55 (1), 141154.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination
minimum path cost. IEEE Trans. Systems Science Cybernetics, 4, 100107.
Hirschberg, D. S. (1975). linear space algorithm computing maximal common subsequences. Comm. ACM, 6 (18), 341343.
Ikeda, T., & Imai, H. (1994). Fast A* algorithms multiple sequence alignment.
Proceedings Genome Informatics Workshop, pp. 9099.
Jones, D. T., Taylor, W. R., & Thornton, J. M. (1992). rapid generation mutation
data matrices protein sequences. CABIOS, 3, 275282.
Kobayashi, H., & Imai, H. (1998). Improvement A* algorithm multiple sequence
alignment. Miyano, S., & Takagi, T. (Eds.), Genome Informatics, pp. 120130,
Tokyo. Universal Academy Press.
Korf, R. E. (1985). Depth-first iterative-deepening: optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
622

fiAn Improved Search Algorithm Optimal Multiple-Sequence Alignment

Korf, R. E. (1999). Divide-and-conquer bidirectional search: First results. Proceedings
Sixteenth International Conference Artificial Intelligence (IJCAI-99), pp.
11811189, Stockholm, Sweden.
Korf, R. E., & Zhang, W. (2000). Divide-and-conquer frontier search applied optimal
sequence alignment. Proceedings Eighteenth National Conference Artificial
Intelligence (AAAI-00), pp. 210216.
McNaughton, M., Lu, P., Schaeffer, J., & Szafron, D. (2002). Memory-efficient A* heuristics
multiple sequence alignment. Proceedings Eighteenth National Conference
Artificial Intelligence (AAAI-02), Edmonton, Alberta, Canada.
Spouge, J. L. (1989). Speeding dynamic programming algorithms finding optimal
lattice paths. SIAM J. Applied Mathematics, 49 (5), 15521566.
Thompson, J. D., Plewniak, F., & Poch, O. (1999). comprehensive comparison multiple
sequence alignment programs. Nucleic Acids Res., 13 (27), 26822690.
Ukkonen, E. (1985). Algorithms approximate string matching. Information Control,
64, 110118.
Wah, B. W., & Shang, Y. (1995). comparison class IDA* search algorithms.
International Journal Tools Artificial Intelligence, 3 (4), 493523.
Wang, L., & Jiang, T. (1994). complexity multiple sequence alignment. Journal
Computational Biology, 1, 337348.
Yoshizumi, T., Miura, T., & Ishida, T. (2000). A* partial expansion large branching
factor problems. AAAI/IAAI, pp. 923929.
Zhou, R., & Hansen, E. A. (2003a). Sparse-memory graph search. 18th International
Joint Conference Artificial Intelligence (IJCAI-03), Acapulco, Mexico.
Zhou, R., & Hansen, E. A. (2003b). Sweep A*: Space-efficient heuristic search partiallyordered graphs. 15th IEEE International Conference Tools Artificial Intelligence, Sacramento, CA.
Zhou, R., & Hansen, E. A. (2004). Breadth-first heuristic search. Fourteenth International Conference Automated Planning Scheduling (ICAPS-04), Whistler, BC,
Canada.

623

fiJournal Artificial Intelligence Research 23 (2005) 1-40

Submitted 05/04; published 01/05

Finding Approximate POMDP Solutions Belief
Compression
Nicholas Roy

nickroy@mit.edu

Massachusetts Institute Technology,
Computer Science Artificial Intelligence Laboratory
Cambridge,

Geoffrey Gordon

ggordon@cs.cmu.edu

Carnegie Mellon University, School Computer Science
Pittsburgh, PA

Sebastian Thrun

thrun@stanford.edu

Stanford University, Computer Science Department
Stanford, CA

Abstract
Standard value function approaches finding policies Partially Observable Markov
Decision Processes (POMDPs) generally considered intractable large models.
intractability algorithms large extent consequence computing
exact, optimal policy entire belief space. However, real-world POMDP
problems, computing optimal policy full belief space often unnecessary
good control even problems complicated policy classes. beliefs experienced
controller often lie near structured, low-dimensional subspace embedded
high-dimensional belief space. Finding good approximation optimal value function
subspace much easier computing full value function.
introduce new method solving large-scale POMDPs reducing dimensionality belief space. use Exponential family Principal Components Analysis (Collins, Dasgupta, & Schapire, 2002) represent sparse, high-dimensional belief spaces
using small sets learned features belief state. plan terms
low-dimensional belief features. planning low-dimensional space, find
policies POMDP models orders magnitude larger models
handled conventional techniques.
demonstrate use algorithm synthetic problem mobile robot
navigation tasks.

1. Introduction
Decision making one central problems artificial intelligence robotics.
robots deployed world accomplish specific tasks, real world
difficult place actactions serious consequences. Figure 1(a) depicts
mobile robot, Pearl, designed operate environment shown Figure 1(b),
Longwood retirement facility Pittsburgh. Real world environments Longwood
characterized uncertainty; sensors cameras range finders noisy
entire world always observable. large number state estimation techniques
explicitly recognize impossibility correctly identifying true state world
(Gutmann, Burgard, Fox, & Konolige, 1998; Olson, 2000; Gutmann & Fox, 2002; Kanazawa,
c
2005
AI Access Foundation. rights reserved.

fiRoy, Gordon, & Thrun

Koller, & Russell, 1995; Isard & Blake, 1998) using probabilistic techniques track
location robot. state estimators Kalman filter (Leonard & DurrantWhyte, 1991) Markov localization (Fox, Burgard, & Thrun, 1999; Thrun, Fox, Burgard,
& Dellaert, 2000) provide (possibly factored, Boyen & Koller, 1998) distribution
possible states world instead single (possibly incorrect) state estimate.

(a)

Figure 1:

(b)

planner mobile robot Pearl, shown (a), must able navigate
reliably real environments Longwood Oakmont retirement facility,
shown (b). white areas map free space, black pixels
obstacles, grey areas regions map uncertainty. Notice
large open spaces, many symmetries lead ambiguity robots
position. map 53.6m 37.9m, resolution 0.1m 0.1m per pixel.

contrast, controllers motion planners, dialogue systems, etc. rarely model
notions uncertainty. state estimate full probability distribution,
controller often uses heuristic extract single best state, distributions
mean mode. planners compensate inevitable estimation errors robust control (Chen, 2000; Bagnell & Schneider, 2001), deployed systems incorporate
full probabilistic state estimate planning. Although most-likely-state method
simple used successfully real applications (Nourbakhsh, Powers, &
Birchfield, 1995), substantial control errors result distribution possible
states uncertain. single state estimate wrong, planner likely choose
unreasonable action.
Figure 2 illustrates difference conventional controllers model
uncertainty. figure, robot must navigate bottom right corner top
left, limited range sensing (up 2m) noisy dead reckoning.1 impoverished
1. purposes example sensing dead reckoning artificially poor,
phenomenon would occur naturally larger-scale environments.

2

fiFinding Approximate POMDP Solutions Belief Compression

sensor data cause robots state estimate become quite uncertain strays
far environmental structures use localize itself. left (Figure 2a)
example trajectory motion planner knowledge uncertainty
state estimate mechanism taking uncertainty account. robots
trajectory diverges desired path, robot incorrectly believes arrived
goal. shown state estimates reflect high uncertainty
robot position. right (Figure 2b) example trajectory controller
model positional uncertainty, take action keep uncertainty small following
walls, arrive reliably goal.

Goal

Goal

Measured Path

Measured Path
True Path
True Path

Start

Start

(a) Conventional controller

Figure 2:

(b) Robust controller

Two possible trajectories navigation Longwood Oakmont environment. robot limited range sensing (up 2m) poor dead-reckoning
odometry. (a) trajectory conventional motion planner uses
single state estimate, minimizes travel distance. (b) trajectory
robust controller models state uncertainty minimize travel
distance uncertainty.

controller Figure 2(b) derived representation called partially observable Markov decision process (POMDP). POMDPs technique making decisions
based probabilistic estimates state world, rather absolute knowledge
true state. POMDP uses priori model world together history
actions taken observations received order infer probability distribution,
belief, possible states world. controller chooses actions, based upon
current belief, maximize reward expects receive time.
advantage using POMDPs decision making resulting policies
handle uncertainty well. POMDP planning process take advantage actions
implicitly reduce uncertainty, even problem specification (e.g., reward function)
explicitly reward actions. disadvantage POMDPs finding
optimal policy computationally intractable. Existing techniques finding exact optimal
3

fiRoy, Gordon, & Thrun

plans POMDPs typically cannot handle problems hundred states
(Hauskrecht, 2000; Zhang & Zhang, 2001). planning problems involving real, physical
systems cannot expressed compactly; would like deploy robots plan
thousands possible states world (e.g., map grid cells), thousands possible
observations (e.g., laser range measurements) actions (e.g., velocities).
paper, describe algorithm finding approximate solutions realworld POMDPs. algorithm arises insight exact POMDP policies use
unnecessarily complex, high-dimensional representations beliefs controller
expect experience. finding low-dimensional representations, planning process
becomes much tractable.
first describe find low-dimensional representations beliefs realworld POMDPs; use variant common dimensionality-reduction technique
called Principal Components Analysis. particular variant use modifies loss
function PCA order better model data probability distributions. Using
low-dimensional representations, describe plan low-dimensional space,
conclude experimental results robot control tasks.

2. Partially Observable Markov Decision Processes
partially observable Markov decision process (POMDP) model deciding
act accessible, stochastic environment known transition model (Russell
Norvig (1995), pg. 500). POMDP described following:









set states = {s1 , s2 , . . . s|S| }
set actions = {a1 , a2 , . . . , a|A| }
set observations Z = {z1 , z2 , . . . , z|Z| }
set transition probabilities (si , a, sj ) = p(sj |si , a)
set observation probabilities O(zi , a, sj ) = p(zi |sj , a)
set rewards R : 7 R
discount factor [0, 1]
initial belief p0 (s)

transition probabilities describe state evolves actions, also represent Markov assumption: next state depends current (unobservable)
state action independent preceding (unobserved) states actions.
reward function describes objective control, discount factor used
ensure reasonable behaviour face unlimited time. optimal policy known
always exist discounted ( < 1) case bounded immediate reward (Howard,
1960).
POMDP policies often computed using value function belief space.
value function V (b) given policy defined long-term expected reward
controller receive starting belief b executing policy horizon time,
may infinite. optimal POMDP policy maximizes value function.
value function POMDP policy finite horizon described using piecewise linear function space beliefs. Many algorithms compute value function
iteratively, evaluating refining current value function estimate
4

fiFinding Approximate POMDP Solutions Belief Compression

refinements improve expected reward policy belief. Figure 3(a)
shows belief space three-state problem. belief space two-dimensional,
shaded simplex. point simplex corresponds particular belief (a threedimensional vector), corners simplex represent beliefs state
known 100% certainty. value function shown Figure 3(b) gives long-term
expected reward policy, starting belief simplex.

1

10
9

0.8

8
7

0.6

6
5

0.4

4
3

0.2

2
0
0

1
0
0.2

0.2
0.4
0.6
0.8
1

0

0.2

0.4

0.6

0.8

0.4

1

0.6
0.8
1

(a) belief space
Figure 3:

0

0.2

0.4

0.6

0.8

1

(b) value function

(a) belief space three-state problem two-dimensional, shaded
simplex. (b) value function defined belief space. purposes
visualization, set beliefs constitutes belief space shown (a)
projected onto XY plane (b); value function rises along
positive Z axis. point belief space corresponds specific
distribution, value function point gives expected reward
policy starting belief. belief space (and therefore value
function) one fewer dimension total number states
problem.

process evaluating refining value function core solving
POMDPs considered intractable. value function defined space
beliefs, continuous high-dimensional; belief space one fewer
dimension number states model. navigation problem map
thousands possible states, computing value function optimization problem
continuous space many thousands dimensions, feasible existing
algorithms.
However, careful consideration real-world problems suggests possible approach
finding approximate value functions. examine beliefs navigating mobile
robot encounters, beliefs share common attributes. beliefs typically
small number modes, particular shape modes fairly generic. modes
move change variance, ways modes change relatively
constrained. fact, even real world navigation problems large belief spaces,
beliefs degrees freedom.
Figure 4(a) illustrates idea: shows typical belief mobile robot might
experience navigating nursing home environment Figure 1(b). visualize
distribution sample set poses (also called particles) according distribution
5

fiRoy, Gordon, & Thrun

plot particles map. distribution unimodal probability mass
mostly concentrated small area. Figure 4(b) shows different kind belief:
probability mass spread wide area, multiple modes, locations
particles bear little relationship map. would difficult find sequence
actions observations would result belief.

particles


(a) common belief

Figure 4:

(b) unlikely belief

Two example probability distributions robot pose. small black dots
particles drawn distribution discrete grid positions. left
distribution robots location relatively certain; kind compact,
unimodal distribution common robot navigation. right
different, implausible distribution. right hand distribution sufficiently
unlikely afford ignore it; even unable distinguish
belief belief result fail identify optimal action,
quality controller unaffected.

real-world beliefs degrees freedom, concentrated near
low-dimensional subset high-dimensional belief spacethat is, beliefs experienced
controller lie near structured, low-dimensional surface embedded belief
space. find surface, representation belief state terms
small set bases features. One benefit representation need
plan terms small set features: finding value functions low-dimensional
spaces typically easier finding value functions high-dimensional spaces.
two potential disadvantages sort representation. first
contains approximation: longer finding complete, optimal POMDP policy.
Instead (as suggested Figure 5) trying find representations belief
rich enough allow good control also sufficiently parsimonious make
6

fiFinding Approximate POMDP Solutions Belief Compression

planning problem tractable. second disadvantage technical one:
making nonlinear transformation belief space, POMDP planning algorithms
assume convex value function longer work. discuss problem detail
Section 6.
Conventional
Path Planner

POMDP

Tractable
Robust

Figure 5:

Intractable
Robust

useful planner lies somewhere continuum MDP-style
approximations full POMDP solution.

3. Dimensionality Reduction
order find low-dimensional representation beliefs, use statistical dimensionality reduction algorithms (Cox & Cox, 1994). algorithms search projection
original high-dimensional representation beliefs lower-dimensional compact representation. is, search low-dimensional surface, embedded
high-dimensional belief space, passes near sample beliefs. consider
evolution beliefs POMDP trajectory inside belief space, assumption trajectories large, real world POMDPs lie near low-dimensional
surface embedded belief space. Figure 6 depicts example low-dimensional surface
embedded belief space three-state POMDP described previous section.
1

0.8

0.6

0.4

0.2

0
0
0.2
0.4
0.6
0.8
1

Figure 6:

0

0.2

0.4

0.6

0.8

1

one-dimensional surface (black line) embedded two-dimensional belief space
(gray triangle). black dot represents single belief probability distribution
experienced controller. beliefs lie near low-dimensional surface.

Ideally, dimensionality reduction involves information lossall aspects data
recovered equally well low-dimensional representation highdimensional one. practice, though, see use lossy representations
belief (that is, representations may allow original data beliefs
recovered without error) still get good control. But, also see finding
representations probability distributions require careful trade-off
7

fiRoy, Gordon, & Thrun

preserving important aspects distributions using dimensions possible.
measure quality representation penalizing reconstruction errors
loss function (Collins et al., 2002). loss function provides quantitative way
measure errors representing data, different loss functions result different
low-dimensional representations.
Principal Components Analysis
One common forms dimensionality reduction Principal Components Analysis (Joliffe, 1986). Given set data, PCA finds linear lower-dimensional representation data variance reconstructed data preserved. Intuitively,
PCA finds low-dimensional hyperplane that, project data onto
hyperplane, variance data changed little possible. transformation
preserves variance seems appealing maximally preserve ability distinguish beliefs far apart Euclidean norm. see below, however,
Euclidean norm appropriate way measure distance beliefs
goal preserve ability choose good actions.
first assume data set n beliefs {b1 , . . . , bn } B, belief bi
B, high-dimensional belief space. write beliefs column vectors
matrix B = [b1 | . . . |bn ], B R|S|n . use PCA compute low-dimensional
representation beliefs factoring B matrices U B,
B = U B .

(1)

equation (1), U R|S|l corresponds matrix bases span low-dimensional
space l < |S| dimensions. B Rnl represents data low-dimensional space.2
geometric perspective, U comprises set bases span hyperplane B
high-dimensional space B; B co-ordinates data hyperplane.
hyperplane dimensionality l exists contains data exactly, PCA find surface given dimensionality best preserves variance data, projecting
data onto hyperplane reconstructing it. Minimizing change variance original data B reconstruction U B equivalent minimizing
sum squared error loss:
L(B, U, B) = kB U B k2F .

(2)

PCA Performance
Figure 7 shows toy problem use evaluate success PCA finding
low-dimensional representations. abstract model two-dimensional state space:
one dimension position along one two circular corridors, one binary variable
determines corridor in. States s1 . . . s100 inclusive correspond one corridor,
states s101 . . . s200 correspond other. reward known position
different corridor; therefore, agent needs discover corridor, move
2. Many descriptions PCA based factorization U SV , U V column-orthonormal
diagonal. could enforce similar constraint identifying B = V S; case columns U
would orthonormal B would orthogonal.

8

fiFinding Approximate POMDP Solutions Belief Compression

appropriate position, declare arrived goal. goal declared
system resets (regardless whether agent actually goal). agent
4 actions: left, right, sense_corridor, declare_goal. observation
transition probabilities given discretized von Mises distributions (Mardia & Jupp,
2000; Shatkay & Kaelbling, 2002), exponential family distribution defined [ : ).
von Mises distribution wrapped analog Gaussian; accounts fact
two ends corridor connected. sum two von Mises variates
another von Mises variate, product two von Mises likelihoods
scaled von Mises likelihood, guarantee true belief distribution always
von Mises distribution corridor action observation.
instance problem consists 200 states, 4 actions 102 observations.
Actions 1 2 move controller left right (with von Mises noise) action
3 returns observation uniquely correctly identifies half maze
agent (the top half bottom half). Observations returned actions 1 2
identify current state modulo 100: probability observation von Mises
distribution mean equal true state (modulo 100). is, observations
indicate approximately agent horizontally.
Max Prob.
Obs. = #1
1

Max Prob.
Obs. = #3
2

Max Prob.
Obs. = #5

3

4

103

104

5

6

105

106

7

Reward
101

102

107

...
...

Max Prob.
Obs. = #100
100

Observation "top"
action #3 prob. 1

Reward

Observation "bottom"
action #3 prob. 1

200

Figure 7: toy maze 200 states.

maze interesting relatively large POMDP standards (200 states)
contains particular kind uncertaintythe agent must use action 3 point
uniquely identify half maze in; remaining actions result observations
contain information corridor agent in. problem large
solved conventional POMDP value iteration, structured heuristic
policies also perform poorly.
collected data set 500 beliefs assessed performance PCA beliefs
problem. data collected using hand-coded controller, alternating
random exploration actions MDP solution, taking current state
maximum-likelihood state belief. Figure 8 shows 4 sample beliefs data
set. Notice beliefs essentially two discretized von Mises distributions
different weights, one half maze. starting belief state
left-most distribution Figure 8: equal probability top bottom corridors,
position along corridor following discretized von Mises distribution concentration
parameter 1.0 (meaning p(state) falls 1/e maximum value move 1/4
way around corridor likely state).
9

fiRoy, Gordon, & Thrun

Sample Belief

Sample Belief

0.04

0.04

0.035

0.035

0.03

0.03

0.025

0.025

0.02
0.015
0.01

0.02
0.015
0.01

0.005

0.005

0

0

-0.005

0

20

40

60

-0.005

80 100 120 140 160 180 200
State

Probability

0.03
0.025

Probability

Probability

Sample Belief
0.04
0.035

0.02
0.015
0.01
0.005
0

0

20

40

60

-0.005

80 100 120 140 160 180 200
State

0

20

40

60

80 100 120 140 160 180 200
State

Sample Belief
0.14
0.12

Probability

0.1
0.08
0.06
0.04
0.02
0

Figure 8:

0

20

40

60

80

100 120 140 160 180 200
State

Sample beliefs toy problem, sample set 500, different (noncontiguous) points time. left-most belief initial belief state.

Figure 9 examines performance PCA representing beliefs data set
computing average error original beliefs B reconstructions U B.3
Figure 9(a) see average squared error (squared L2 ) compared number
bases, Figure 9(b), see average Kullbach-Leibler (KL) divergence.
KL divergence belief b reconstruction r = U b low-dimensional
representation b given
KL(b k r) =

|S|
X

b(si ) ln

i=1



b(si )
r(si )



(3)

Minimizing squared L2 error explicit objective PCA, KL divergence
appropriate measure much two probability distributions differ. 4
Unfortunately, PCA performs poorly representing probability distributions. Despite
fact probability distributions collected data set 3 degrees
freedom, reconstruction error remains relatively high somewhere 10
15 basis functions. examine reconstruction sample belief, see
kinds errors PCA making. Figure 10 shows sample belief (the solid line)
reconstruction (the dotted line). Notice reconstructed belief strange
artifacts: contains ringing (multiple small modes), also negative regions.
PCA purely geometric process; notion original data probability
distributions, therefore free generate reconstructions data contain
negative numbers sum 1.
3. use popular implementation PCA based Golub-Reinsche algorithm (Golub & Reinsch,
1970) available GNU Scientific Library (Galassi, Davies, Theiler, Gough, Jungman, Booth,
& Rossi, 2002).
4. Note computing KL divergence reconstruction original belief,
shift reconstruction non-negative, rescale sum 1.

10

fiFinding Approximate POMDP Solutions Belief Compression

Average L2 Error vs. Number Bases

Average KL Divergence vs. Number Bases

0.02

1.6
Average KL Divergence

1.4
Average L2 Error

0.015
0.01
0.005
0

1.2
1
0.8
0.6
0.4
0.2
0

0

5

10
15
20
Number Bases

25

30

0

(a) Average Squared L-2 Error
Figure 9:

5

10

15
20
Number Bases

25

30

(b) Average KL Divergence

average error original sample set B reconstructions U B.
(a) Squared L2 error, explicitly minimized PCA, (b) KL divergence.
error bars represent standard deviation mean error
500 beliefs.
Example Belief Reconstruction
0.045

Original Belief
Reconstructed Belief

0.04

Probability

0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
-0.005

0

20

40

60

80

100 120 140 160 180 200
State

Figure 10: example belief reconstruction, using 10 bases.

Notice also PCA process making significant errors lowprobability regions belief. particularly unfortunate real-world probability distributions tend characterized compact masses probability, surrounded
large regions zero probability (e.g., Figure 4a). therefore need modify
PCA ensure reconstructions probability distributions, improve representation sparse probability distributions reducing errors made low-probability
events.
question answered loss functions available instead sum
squared errors, equation (2). would like loss function better reflects
need represent probability distributions.
11

fiRoy, Gordon, & Thrun

4. Exponential Family PCA
conventional view PCA geometric one, finding low-dimensional projection
minimizes squared-error loss. alternate view probabilistic one:
data consist samples drawn probability distribution, PCA algorithm
finding parameters generative distribution maximize likelihood
data. squared-error loss function corresponds assumption data
generated Gaussian distribution. Collins et al. (2002) demonstrated PCA
generalized range loss functions modeling data different exponential
families probability distributions Gaussian, binomial, Poisson.
exponential family distribution corresponds different loss function variant
PCA, Collins et al. (2002) refer generalization PCA arbitrary exponential
family data-likelihood models Exponential family PCA E-PCA.
E-PCA model represents reconstructed data using low-dimensional weight
vector b, basis matrix U , link function f :
b f (U b)

(4)

E-PCA model uses different link function, derived data
likelihood model (and corresponding error distribution loss function). link
function mapping data space another space data
linearly represented.
link function f mechanism E-PCA generalizes dimensionality reduction non-linear models. example, identity link function corresponds
Gaussian errors reduces E-PCA regular PCA, sigmoid link function
corresponds Bernoulli errors produces kind logistic PCA 0-1 valued data.
nonlinear link functions correspond non-Gaussian exponential families
distributions.
find parameters E-PCA model maximizing log-likelihood
data model, shown (Collins et al., 2002) equivalent
minimizing generalized Bregman divergence
BF (b k U b) = F (U b) b U b + F (b)

(5)

low-dimensional high-dimensional representations, solve using
convex optimization techniques. (Here F convex function whose derivative f ,
F convex dual F . ignore F purpose minimizing equation 5
since value b fixed.) relationship PCA E-PCA link
functions reminiscent relationship linear regression Generalized Linear
Models (McCullagh & Nelder, 1983).
apply E-PCA belief compression, need choose link function accurately reflects fact beliefs probability distributions. choose link
function
f (U b) = eU b
(6)
P U b
P
hard verify F (U b) =
e F (b) = b ln b b. So, equation 5
becomes
X
X
BF (b k U b) =
eU b b U b + b ln b
b
(7)
12

fiFinding Approximate POMDP Solutions Belief Compression

write b = f (U b), equation 7 becomes
BF (b k U b) = b ln b b ln b +

X

b

X

b = U KL(b k b)

U KL unnormalized KL divergence. Thus, choosing exponential link function (6) corresponds minimizing unnormalized KL divergence original
belief reconstruction. loss function intuitively reasonable choice measuring error reconstructing probability distribution.5 exponential link function
corresponds Poisson error model component reconstructed belief.
choice loss link functions two advantages: first, exponential link
function constrains low-dimensional representation eU b positive. Second, error
model predicts variance belief component proportional expected
value. Since PCA makes significant errors close 0, wish increase penalty
errors small probabilities, error model accomplishes that.
compute loss bi , ignoring terms depend data b, then6

L(B, U, B) =

|B|
X
i=1


eU bi bi U bi .

(8)

introduction link function raises question: instead using complex
machinery E-PCA, could choose non-linear function project data
space linear, use conventional PCA? difficulty
approach course identifying function; general, good link functions E-PCA
related good nonlinear functions application regular PCA. So,
might appear reasonable use PCA find low-dimensional representation log
beliefs, rather use E-PCA exponential link function find representation
beliefs directly, approach performs poorly surface locally
well-approximated log projection. E-PCA viewed minimizing weighted
least-squares chooses distance metric appropriately local. Using conventional
PCA log beliefs also performs poorly situations beliefs contain extremely
small zero probability entries.
P
5. chosen link function eU b / eU b would arrived normalized KL divergence,
perhaps even intuitively reasonable way measure error reconstructing
probability distribution. more-complicated link function would made difficult
derive Newton equations following pages, impossible; experimented
resulting algorithm found produces qualitatively similar results algorithm described
here. Using normalized KL divergence one advantage: allow us get away
one fewer basis function planning, since unnormalized KL divergence E-PCA optimization
must learn basis explicitly represent normalization constant.
6. E-PCA related Lee Seungs (1999) non-negative matrix factorization. One NMF loss
functions presented Lee Seung (1999) penalizes KL-divergence matrix
reconstruction, equation 8; but, NMF loss incorporate link function
E-PCA loss. Another NMF loss function presented Lee Seung (1999) penalizes squared
error constrains factors nonnegative; resulting model example (GL) 2 M,
generalization E-PCA described Gordon (2003).

13

fiRoy, Gordon, & Thrun

Finding E-PCA Parameters
Algorithms conventional PCA guaranteed converge unique answer independent initialization. general, E-PCA property: loss function (8)
may multiple distinct local minima. However, problem finding best B given
B U convex; convex optimization problems well studied unique global
solutions (Rockafellar, 1970). Similarly, problem finding best U given B B
convex. So, possible local minima joint space U B highly constrained,
finding U B require solving general non-convex optimization problem.
Gordon (2003) describes fast, Newtons Method approach computing U B
summarize here. algorithm related Iteratively Reweighted Least Squares,
popular algorithm generalized linear regression (McCullagh & Nelder, 1983). order
use Newtons Method minimize equation (8), need derivative respect U
B:

(U B)

L(B, U, B) =
e

B U B
(9)
U
U
U
= e(U B) B B B
(10)
= (e(U B) B)B

(11)


(U B)


L(B, U, B) =
e

B U B
B
B
B
= U e(U B) U B


= U (e

(U B)

B).

(12)
(13)
(14)

set right hand side equation (14) zero, iteratively compute Bj ,
column B, Newtons method. Let us set q(Bj ) = U (e(U Bj ) Bj ), linearize
Bj find roots q(). gives
j th

Bjnew = Bj
Bjnew Bj

=

q(Bj )
q 0 (Bj )

U (e(U Bj ) Bj )
q 0 (Bj )

(15)
(16)

Note equation 15 formulation Newtons method finding roots q, typically
written
f (xn )
xn+1 = xn 0
.
(17)
f (xn )
need expression q 0 :
q
Bj

=
=


U (e(U Bj ) Bj )
Bj

U e(U Bj )
Bj

= U Dj U
14

(18)
(19)
(20)

fiFinding Approximate POMDP Solutions Belief Compression

define Dj terms diag operator returns diagonal matrix:



Dj = diag(eU Bj ),

(21)


b(s0 ) . . .
0

.. .
..
diag(b) = ...
.
.
0
. . . b(s|S| )

(22)



Combining equation (15) equation (20), get
(U Dj U )(Bjnew Bj ) = U (Bj eU Bj )

(23)

U Dj U Bjnew = (U Dj U )Bj + U Dj Dj1 (Bj eU Bj )
= U Dj (U Bj + Dj1 (Bj eU Bj ),

(24)
(25)

weighted least-squares problem solved standard linear algebra
techniques. order ensure solution numerically well-conditioned, typically
add regularizer divisor,
Bjnew =

U Dj (U Bj + Dj1 (Bj eU Bj )
(U Dj U + 105 Il )

.

(26)

Il l l identity matrix. Similarly, compute new U computing Ui ,
ith row U ,
(Ui B + (Bi eUi B )Di1 )Di B
.
(27)
Uinew =
(BDi B + 105 Il )
E-PCA Algorithm
algorithm automatically finding good low-dimensional representation
B high-dimensional belief set B. algorithm given Table 1; optimization
iterated termination condition reached, finite number iterations,
minimum error achieved.
steps 7 9 raise one issue. Although solving row U column B
separately convex optimization problem, solving two matrices simultaneously
not. therefore subject potential local minima; experiments
find problem, expect need find ways address local
minimum problem order scale even complicated domains.
bases U found, finding low-dimensional representation highdimensional belief convex problem; compute best answer iterating equation (26). Recovering full-dimensional belief b low-dimensional representation b
also straightforward:
x = eU b .
(28)
definition PCA explicitly factor data U , B many
presentations do. three-part representation PCA, contains singular values
15

fiRoy, Gordon, & Thrun

1. Collect set sample beliefs high-dimensional belief space
2. Assemble samples data matrix B = [b1 | . . . |b|B| ]
3. Choose appropriate loss function, L(B, U, B)
4. Fix initial estimate B U randomly
5.
6.

column Bj B,
Compute Bjnew using current U estimate equation (26)

7.
8.

row Ui U ,

9.

Compute Uinew using new B estimate equation (27)

10. L(B, U, B) >

Table 1:

E-PCA Algorithm finding low-dimensional representation POMDP,
including Gordons Newtons method (2003).

decomposition, U B orthonormal. use two-part representation
B f (U B) quantity E-PCA decomposition corresponds
singular values PCA. result, U B general orthonormal.
desired, though, possible orthonormalize U additional step optimization
using conventional PCA adjust B accordingly.

5. E-PCA Performance
Using loss function equation (8) iterative optimization procedure described
equation (26) equation (27) find low-dimensional factorization, look
well dimensionality-reduction procedure performs POMDP examples.
Toy Problem
Recall Figure 9 unable find good representations data
fewer 10 15 bases, even though domain knowledge indicated data
3 degrees freedom (horizontal position mode along corridor, concentration
mode, probability top bottom corridor). Examining one
sample beliefs Figure 10, saw representation worst lowprobability regions. take data set toy example, use E-PCA
find low-dimensional representation compare performance PCA E-PCA.
Figure 11(a) shows E-PCA substantially efficient representing data,
see KL divergence falling close 0 4 bases. Additionally, squared L 2
error 4 bases 4.64 104 . (We need 4 bases perfect reconstruction, rather
3, since must include constant basis function. small amount reconstruction
16

fiFinding Approximate POMDP Solutions Belief Compression

error 4 bases remains stopped optimization procedure fully
converged.)

Average KL Divergence vs. Number Bases (E-PCA)

Example Belief Reconstruction Using 3 Bases

1.6

0.045
0.04
0.035
0.03

1.2
1

Probability

Average KL Divergence

1.4

0.8
0.6
0.4
0
0

5

10

15
20
Number Bases

25

30

(a) Reconstruction Performance

Probability

Probability

0.036
0.034
0.032
0.03
0.028

40

60

80

100 120 140 160 180 200
State

2e-09
1.5e-09
1e-09
5e-10

0.026

0
146

148

150
State

152

154

90

(c) Belief Reconstruction Near
Peak

Figure 11:

20

Example Belief Reconstruction Using 3 Bases
3e-09
Original Belief
Reconstructed Belief
2.5e-09

Original Belief
Reconstructed Belief

0.038

0

(b) Example Belief Reconstruction

Example Belief Reconstruction Using 3 Bases
0.04

0.025
0.02
0.015
0.01
0.005
0
-0.005

0.2

Original Belief
Reconstructed Belief

95

100
State

105

110

(d) Belief Reconstruction LowProbability Region

(a) average KL divergence original sample set reconstructions. KL divergence 0.018 4 bases. error bars represent
standard deviation mean 500 beliefs. (b) example
belief Figure 10 reconstruction using 3 bases. reconstruction
shows small errors peak mode. shown reconstruction
using 4 bases, original belief reconstruction indistinguishable naked eye. (c) (d) show fine detail original belief
reconstruction two parts state space. Although reconstruction
perfect, low-probability area, see error approximately
2 109 .

17

fiRoy, Gordon, & Thrun

Figure 11(b) shows E-PCA reconstruction example belief Figure 10.
see many artifacts present PCA reconstruction absent. Using
3 bases, see E-PCA reconstruction already substantially better PCA
using 10 bases, although small errors peaks (e.g., Figure 11c)
two modes. (Using 4 bases, E-PCA reconstruction indistinguishable naked eye
original belief.) kind accuracy 3 4 bases typical
data set.
Robot Beliefs
Although performance E-PCA finding good representations abstract problem
compelling, would ideally like able use algorithm real-world problems,
robot navigation problem Figure 2. Figures 12 13 show results two
robot navigation problems, performed using physically-realistic simulation (although
artificially limited sensing dead-reckoning). collected sample set 500 beliefs
moving robot around environment using heuristic controller, computed
low-dimensional belief space B according algorithm Table 1. full state space
47.7m 17m, discretized resolution 1m 1m per pixel, total 799 states.
Figure 12(a) shows sample belief, Figure 12(b) reconstruction using 5 bases.
Figure 12(c) see average reconstruction performance E-PCA approach,
measured average KL-divergence sample belief reconstruction.
comparison, performance PCA E-PCA plotted. E-PCA error falls
0.02 5 bases, suggesting 5 bases sufficient good reconstruction.
substantial reduction, allowing us represent beliefs problem using
5 parameters, rather 799 parameters. Notice many states lie regions
outside map; is, states never receive probability mass
removed. removing states would trivial operation, E-PCA correctly
able automatically.
Figure 13, similar results shown different environment. sample set 500
beliefs collected using heuristic controller, low-dimensional belief space
B computed using E-PCA. full state space 53.6m 37.9m, resolution
.5m .5m per pixel. example belief shown Figure 13(a), reconstruction
using 6 bases shown Figure 13(b). reconstruction performance measured
average KL divergence shown Figure 13(c); error falls close 0 around 6
bases, minimal improvement thereafter.

6. Computing POMDP policies
Exponential-family Principal Components Analysis model gives us way find
low-dimensional representation beliefs occur particular problem.
two real-world navigation problems tried, algorithm proved effective
finding low-dimensional representations, showing reductions 800 states
2, 000 states 5 6 bases. 5 6 dimensional belief space allow much
tractable computation value function, able solve much
larger POMDPs could solved previously.
18

fiFinding Approximate POMDP Solutions Belief Compression

KL Divergence Sampled Beliefs Reconstructions

Particles form
bimodal distribution

45

E-PCA
PCA

40

KL Divergence

35

(a) Original Belief

30
25
20
15
10
5
0
1

2

3

4

5

6

7

8

9

Number Bases

(c) Reconstruction performance

(b) Reconstruction

Figure 12:

(a) sample belief robot navigation task. (b) reconstruction
belief learned E-PCA representation using 5 bases. (c) average
KL divergence sample beliefs reconstructions
number bases used. Notice E-PCA error falls close 0 5 bases,
whereas conventional PCA much worse reconstruction error even 9 bases,
improving rapidly.

KL Divergence Sampled Beliefs Reconstructions
4
3.5
KL Divergence

3
2.5
2
1.5
1
0.5
0

(a) sample belief

Figure 13:

(b) reconstruction

0

2

4

6
8
10
Number Bases

(c) Average
performance

12

14

16

reconstruction

(a) sample belief navigation problem Longwood, cf. Figure 2. (b)
reconstruction learned E-PCA representation using 6 bases. (c)
average KL divergence sample beliefs reconstructions
number bases used.

Unfortunately, longer use conventional POMDP value iteration find
optimal policy given low-dimensional set belief space features. POMDP value iteration depends fact value function convex belief space.
19

fiRoy, Gordon, & Thrun

compute non-linear transformation beliefs recover coordinates
low-dimensional belief surface, lose convexity value function (compare Figure 3 Figure 6 see why). result, value function cannot expressed
supremum set hyperplanes low-dimensional belief space.
So, instead using POMDP value iteration, build low-dimensional discrete
belief space MDP use MDP value iteration. Since know form
value function, turn function approximation. Gordon (1995) proved
fitted value iteration algorithm guaranteed find bounded-error approximation
(possibly discounted) MDPs value function, long use combination
function approximator averager. Averagers function approximators
non-expansions max-norm; is, exaggerate errors training data.
experiments below, use regular grids well irregular, variable-resolution grids
based 1-nearest-neighbour discretization, represented set low-dimensional beliefs
B ,
B = {b1 , b2 , . . . , b|B | }.
(29)
approximations averagers; averagers include linear interpolation, knearest-neighbours, local weighted averaging. focus detail exact
mechanism discretizing low-dimensional space, outside scope
paper. resolution regular grid cases chosen empirically; section 7
describe specific variable resolution discretization scheme worked well empirically.
reader consult Munos Moore (2002) Zhou Hansen (2001)
sophisticated representations.
fitted value iteration algorithm uses following update rule compute t-step
lookahead value function V (t 1)-step lookahead value function V t1 :


|B |
X
V (bi ) = max R (bi , a) +
(bi , a, bj ) V t1 (bj )
(30)


j=1

R approximate reward transition functions based dynamics
POMDP, result E-PCA, finite set low-dimensional belief samples
B using function approximator. Note problems described
paper, problem require discounting ( = 1). following sections describe
compute model parameters R .
Computing Reward Function
original reward function R(s, a) represents immediate reward taking action
state s. cannot know, given either low-dimensional high-dimensional belief,
immediate reward be, compute expected reward. therefore
represent reward expected value immediate reward full model,
current belief:
R (b , a) = Eb (R(s, a))

(31)

|S|

=

X
i=1

20

R(si , a)b(si ).

(32)

fiFinding Approximate POMDP Solutions Belief Compression

Equation (32) requires us recover high-dimensional belief b low-dimensional
representation b , shown equation (28).
many problems, reward function R effect giving low immediate
reward belief states high entropy. is, many problems planner
driven towards beliefs centred high-reward states low uncertainty.
property intuitively desirable: beliefs robot worry
immediate bad outcome.
Computing Transition Function
Computing low-dimensional transition function = p(bj |a, bi ) simple
computing low-dimensional reward function R : need consider pairs lowdimensional beliefs, bi bj . original high-dimensional belief space, transition
prior belief bi posterior belief bj described Bayes filter equation:
bj (s) = O(s, a, z)

|S|
X

(sk , a, s)bi (sk )

(33)

k=1

action selected z observation saw; original POMDP
transition probability distribution, original POMDP observation probability
distribution.
Equation (33) describes deterministic transition conditioned upon prior belief,
action observation. transition posterior bj stochastic observation known; is, transition bi bj occurs specific z
generated, probability transition probability generating observation
z. So, separate full transition process deterministic transition b ,
belief acting sensing, stochastic transition b j , full posterior:
ba (s) =

|S|
X

(sj , a, s)bi (sj )

(34)

j=1

bj (s) = O(s, a, z)ba (s).

(35)

Equations 34 35 describe transitions high-dimensional beliefs
original POMDP. Based high-dimensional transitions, compute transitions low-dimensional approximate belief space MDP. Figure 14 depicts process.
figure shows, start low-dimensional belief bi . bi reconstruct
high-dimensional belief b according equation (28). apply action
observation z described equation (34) equation (35) find new belief
b0 . b0 compress low-dimensional representation b0 iterating
equation (26). Finally, since b0 may member sample B low-dimensional
belief states, map b0 nearby bj B according function approximator.
function approximator grid, last step means replacing b0
prototypical bj shares grid cell. generally, function approximator may
represent b0 combination several states, putting weight w(bj , b0 ) bj . (For
example, approximator k-nearest-neighbour, w(bj , b0 ) = k1 closest k
21

fiRoy, Gordon, & Thrun

~*
bi

~
b

~*
bj
Lowdimensional

action


b

ba

b

Highdimensional

observation
z

Figure 14: process computing single transition probability.

samples B .) case replace transition bi b0 several transitions,
bi bj , scale probability one w(bj , b0 ).
transition bi b ba b0 b0 bj assign probability
p(z, j|i, a) =

p(z|ba ) w(bj , b0 )

=

w(bj , b0 )

|S|
X

p(z|sl )ba (sl )

(36)

l=1

total transition probability (bi , a, bj ) sum, observations z, p(z, j|i, a).
Step 3 Table 2 performs computation, shares work computation
(bi , a, bj ) different posterior beliefs bj reachable prior belief
bi action a.
Computing Value Function
reward transition functions computed previous sections, use
value iteration compute value function belief space MDP. full algorithm
given Table 2.

7. Solving Large POMDPs
section, present application algorithm finding policies large
POMDPs.
Toy problem
first tested E-PCA belief features using regular grid representation version
toy problem described earlier. ensure needed small set belief
samples bi , made goal region larger. also used coarser discretization
underlying state space (40 states instead 200) allow us compute low-dimensional
model quickly.
Figure 15 shows comparison policies different algorithms. E-PCA
approximately twice well Maximum-Likelihood heuristic; heuristic guesses
corridor, correct half time. AMDP Heuristic algorithm
Augmented MDP algorithm reported Roy Thrun (1999). controller attempts
22

fiFinding Approximate POMDP Solutions Belief Compression

1. Generate discrete low-dimensional belief space B using E-PCA (cf. Table 1)
2. Compute low-dimensional reward function R :
b B ,
(a) Recover b b
(b) Compute R (b, a) =

P|S|

i=1 R(si , a)b(si ).

3. Compute low-dimensional transition function :
bi B ,
(a) bj : (bi , a, bj ) = 0
(b) Recover bi bi
(c) observation z
(d)

Compute bj Bayes filter equation (33) b.

(e)

Compute b0 bj iterating equation (26).

(f)

bj w(bj , b0 ) > 0
Add p(z, j|i, a) equation (36) (bi , a, bj )

(g)

4. Compute value function B
(a) = 0
(b) bi B : V 0 (bi ) = 0
(c)
(d)

change = 0

(e)

bi B :


P|B |
V (bi ) = maxa R (bi , a) + j=1 (bi , a, bj ) V t1 (bj )

change = change + V (bi ) V t1 (bi )
(f) change > 0

Table 2: Value Iteration E-PCA POMDP

find policy result lowest-entropy belief reaching goal.
controller poorly unable distinguish unimodal belief
knows corridor position within corridor, bimodal
belief knows position corridor. results Figure 15 averaged
10,000 trials.
noted problem sufficiently small conventional PCA fares
reasonably well. next sections, see problems PCA representation
poorly compared E-PCA.
23

fiRoy, Gordon, & Thrun

Average reward vs. Number Bases
120000

E-PCA
PCA

Average Reward

100000
80000
60000
40000

MDP Heuristic

20000
0
-20000

AMDP Heuristic
1

2

3

4

Number Bases

Figure 15:

comparison policy performance using different numbers bases, 10,000
trials, regular grid discretization. Policy performance given total
reward accumulated trials.

Robot Navigation
tested E-PCA POMDP algorithm simulated robot navigation problems two
example environments, Wean Hall corridor shown Figure 16 Longwood retirement facility shown Figure 1(b). model parameters given robot navigation
models (see Fox et al., 1999).
evaluated policy relatively simple problem depicted Figure 16. set
robots initial belief may one two locations corridor,
objective get within 0.1m goal state (each grid cell 0.2m0.2m).
controller received reward +1000 arriving goal state taking at_goal
action; reward 1000 given (incorrectly) taking action non-goal state.
reward 1 motion. states used planning example
500 states along corridor, actions forward backward motion.
Figure 16 shows sample robot trajectory using E-PCA policy 5 basis functions.
Notice robot drives past goal lab door order verify orientation
returning goal; robot know true position, cannot know
fact passing goal. robot started end corridor,
orientation would become apparent way goal.
Figure 17 shows average policy performance three different techniques.
Maximum-Likelihood heuristic could distinguish orientations, therefore approximately 50% time declared goal wrong place. also evaluated policy
learned using best 5 bases conventional PCA. policy performed substantially
better maximum-likelihood heuristic controller incorrectly declare robot arrived goal. However, representation could detect
robot goal, also chose sub-optimal (with respect E-PCA
policy) motion actions regularly. E-PCA outperformed techniques example able model belief accurately, contrast result Figure 15
PCA sufficient representation perform well better E-PCA.
24

fiFinding Approximate POMDP Solutions Belief Compression

True Position

Goal State
Final Estimated
Position

True Start State

Figure 16:

Goal Position

Start Position

example robot trajectory, using policy learned using 5 basis functions.
left start conditions goal. right robot
trajectory. Notice robot drives past goal lab door localize
itself, returning goal.

Policy perfomance Mobile Robot Navigation
400000

Average Reward

300000
200000
100000
0

-268500.0
-1000.0

33233.0

-100000
-200000
-300000

Figure 17:

ML Heuristic

PCA

E-PCA

comparison policy performance using E-PCA, conventional PCA
Maximum Likelihood heuristic, 1,000 trials.

Figure 18(a) shows second example navigation simulation. Notice initial
belief problem bi-modal; good policy take actions disambiguate
modes proceeding goal. Using sample set 500 beliefs, computed
low-dimensional belief space B. Figure 18(b) shows average KL divergence
original reconstructed beliefs. improvement KL divergence error measure
slowed substantially around 6 bases; therefore used 6 bases represent belief
space.
Figure 18(c) shows example execution policy computed using E-PCA.
reward parameters previous navigation example. robot
parameters maximum laser range 2m, high motion model variance. first
action policy chose turn robot around move closer nearest wall.
effect eliminating second distribution mode right. robot
followed essentially coastal trajectory left-hand wall order stay localized,
although uncertainty direction became relatively pronounced. see
uncertainty eventually resolved top image, robot moved
goal.
25

fiRoy, Gordon, & Thrun

KL Divergence Sampled Beliefs Reconstructions
4
3.5

True (hidden) start

3
KL Divergence

Goal

2.5
2
1.5
1
0.5
0

Start distribution modes

(a) Initial Distribution

0

2

4

6
8
10
Number Bases

12

14

16

(b) Reconstruction Performance

Positional Accuracy Goal

8
Distance goal metres

7

5

4.544

4
3
2
1
0

(c) Complete Trajectory

Figure 18:

6.030

6

1.075
E-PCA

AMDP

MDP

(d) Policy Performance

(a) sample navigation problem Longwood, cf. Figure 2. problem
involves multi-modal distributions. (c) average KL divergence
sample beliefs reconstructions number bases used, 500
samples beliefs navigating mobile robot environment. (d) comparison policy performance using E-PCA, conventional MDP AMDP
heuristic.

interesting note policy contains similar coastal attribute
heuristic policies (e.g., Entropy heuristic AMDP, Cassandra, Kaelbling, &
Kurien, 1996; Roy & Thrun, 1999). However, unlike heuristics, E-PCA representation able reach goal accurately (that is, get closer goal).
representation successful able accurately represent beliefs
effects actions beliefs.
26

fiFinding Approximate POMDP Solutions Belief Compression

Finding People

(a) Original Belief

(b) Reconstruction PCA

(c) Reconstruction E-PCA

Figure 19:

performance PCA E-PCA sample belief. map 238 85
grid cells, 0.2m resolution. (a) sample belief. (b) PCA reconstruction,
using 40 bases. (c) E-PCA reconstruction, using 6 bases.

addition synthetic problem robot navigation problems described
previous sections, also tested algorithm complicated POMDP problem,
finding person object moving around environment. problem
motivated Nursebot domain, residents experiencing cognitive decline
sometimes become disoriented start wander. order make better use
health-care providers time, would like use robot Pearl (Figure 1a) find
residents quickly. assume person adversarial.
state space problem much larger previous robot navigation problems: cross-product persons position robots position. However,
assume simplicity robots position known, therefore belief distribution persons position. transitions person state feature
modelled Brownian motion fixed, known velocity, models persons
motion random, independent robot position. (If person moving avoid
captured robot, different transition model would required.) assume
position person unobservable robot close enough see
person (when robot line-of-sight person, maximum range, usually
3 metres); observation model 1% false negatives false positives. reward
function maximal person robot location.
27

fiRoy, Gordon, & Thrun

Figure 19(a) shows example probability distribution occur problem
(not shown robots position). grey dots particles drawn distribution
person could environment. distribution initially uniform
reachable areas (inside black walls). robot receives sensor data,
probability mass extinguished within sensor range robot. robot
moves around, probability mass extinguished, focusing distribution
remaining places person be. However, probability distribution starts
recover mass places robot visits leaves. particle filter,
visualized particles leaking areas previously emptied out.
collected set 500 belief samples using heuristic controller given driving
robot maximum likelihood location person, used E-PCA find good
low-dimensional representation beliefs. Figure 19(b) shows reconstruction
example belief Figure 19(a), using conventional PCA 40 bases. figure
reinforce idea PCA performs poorly representing probability distributions. Figure 19(c) shows reconstruction using E-PCA 6 bases, qualitatively better
representation original belief.
Recall section 6 use function approximator representing value
function. preceding examples used regular grid low-dimensional surface
performed well finding good policies. However, problem finding people empirically requires finer resolution representation would computationally tractable
regular grid. therefore turn different function approximator, 1-nearestneighbour variable resolution representation. add new low-dimensional belief states
model periodically re-evaluating model grid cell, splitting gridcell smaller discrete cells statistic predicted model disagrees
statistic computed experience. number different statistics suggested
testing model data real world (Munos & Moore, 1999),
reduction reward variance, value function disagreement. opted instead
simpler criterion transition probability disagreement. examine policy computed
using fixed representation, also policy computed using incrementally refined
representation. Note fully explored effect different variable resolution representations value function, e.g., using k-nearest-neighbour interpolations
described Hauskrecht (2000). experiments beyond scope
paper, focus utility E-PCA decomposition. variable resolution
representation value function shown scale effectively beyond tens
dimensions best (Munos & Moore, 2002).
problem shares many attributes robot navigation problem, see
Figure 19 figures 20 21 problem generates spatial distributions higher
complexity. somewhat surprising E-PCA able find good representation
beliefs using 6 bases, indeed average KL divergence generally higher
robot navigation task. Regardless, able find good controllers,
example problem PCA performs poorly even large number
bases.
Figure 20 shows example trajectory heuristic control strategy, driving
robot maximum likelihood location person time step. open circle
robot position, starting far right. solid black circle position
28

fiFinding Approximate POMDP Solutions Belief Compression

Figure 20:

(a)

(b)

(c)

(d)

(e)

(f)

example suboptimal person finding policy. grey particles drawn
distribution person might be, initially uniformly distributed (a). black dot true (unobservable) position person.
open circle observable position robot. robots poor
action selection, person able escape previously explored areas.

person, unobservable robot within 3m range. person starts
room corridor (a), moves corridor robot
moved far end corridor (b). robot returns search inside room (c)
(d), person moves unobserved previously searched corridor (e). Although
deliberately chosen example heuristic performs poorly, person
following unlikely adversarial trajectory: times solid black circle remains
regions high probability. robots belief accurately reflects possibility
person slip past, heuristic control algorithm way take possibility
account.
Using policy found low-dimensional belief space described previous
sections, able find much better controller. sample trajectory controller
29

fiRoy, Gordon, & Thrun

Figure 21:

(a)

(b)

(c)

(d)

(e)

(f)

policy computed using E-PCA representation. initial conditions
panel (a) Figure 20. Notice that, unlike previous figure,
strategy ensures probability mass located one place, allowing
robot find person significantly higher probability.

shown Figure 21. robot travels right-most position corridor (a)
part-way corridor (b), returns explore room (c)
(d). example, persons starting position different one given
previous examplethe E-PCA policy would find person point, starting
initial conditions previous example). exploring room eliminating
possibility person inside room (e), policy reduced possible
locations person left-hand end corridor, able find
person reliably location.
Note figures 20 21 target person worst-case start position
planner. person start position Figure 21 Figure 20,
policy would found person panel (d). Similarly, person started
30

fiFinding Approximate POMDP Solutions Belief Compression

end corridor Figure 21, policy shown Figure 20 would found
person panel (b).
Performance Different Policies

Average # Actions Find Person

250

200

150

100
Fully Observable Policy
50

0

Figure 22:

Closest

Densest

MDP

PCA

E-PCA Refined E-PCA

comparison 6 policies person finding simple environment.
baseline fully-observable, i.e., cheating, solution (the solid line). EPCA policy fixed (variable resolution) discretization. Refined E-PCA
discretization additional belief samples added. PCA
policy approximately 6 times worse best E-PCA policy.

Figure 22 shows quantitative comparison performance E-PCA
number heuristic controllers simulation, comparing average time find
person different controllers. solid line depicts baseline performance,
using controller access true state person times (i.e., fully
observable lower bound best possible performance). travel time case
solely function distance person; searching necessary performed.
course, realizable controller reality. controllers are:
Closest: robot driven nearest state non-zero probability.
Densest: robot driven location probability mass
visible.
MDP: robot driven maximum-likelihood state.
PCA: controller found using PCA representation fixed discretization
low-dimensional surface.
E-PCA: E-PCA controller using fixed discretization low-dimensional surface
compute value function.
Refined E-PCA: E-PCA controller using incrementally refined variable resolution
discretization surface computing value function.
performance best E-PCA controller surprisingly close theoretical best
performance, terms time find person, result also demonstrates need
careful choice discretization belief space computing value function.
31

fiRoy, Gordon, & Thrun

initial variable resolution representation proved poor function approximator, however, using iteratively-refined variable resolution discretization, able improve
performance substantially. controller using conventional PCA representation
case computed fixed discretization low-dimensional representation using
40 bases 500 grid points. quality belief representation PCA poor
investigate complex policy approximators.

8. Discussion
experiments demonstrate E-PCA algorithm scale finding low-dimensional surfaces embedded high-dimensional spaces.
Time Complexity
algorithm iterative therefore simple expression total running time
available. data set |B| samples dimensionality n, computing surface size
l, iteration algorithm O(|B|nl 2 + |B|l3 + nl3 ). step Newtons
algorithm dominated set matrix multiplies final step inverting l l
matrix, O(l3 ). U step consists |B| iterations, iteration O(nl)
multiplies O(l3 ) inversion. V step consists n iterations, iteration
O(|B|l) multiplies O(l 3 ) inversion, leading total complexity given above.
Figure 23 shows time compute E-PCA bases 500 sample beliefs,
20,230 states. implementation used Java 1.4.0 Colt 1.0.2, 1 GHz Athlon
CPU 900M RAM. Also shown computation times conventional PCA
decomposition. small state space problems, E-PCA decomposition faster
PCA small number bases, implementation PCA always computes
full decomposition (l = n, l reduced dimensionality n full
dimensionality).
Exponential Family PCA Running Time
35000
30000
Time secs

25000
20000
15000
10000
Conventional PCA = 4151sec

5000
0

Figure 23:

0

2

4

6
8
Number Bases

10

12

time compute E-PCA representations different discretizations
state space.

32

fiFinding Approximate POMDP Solutions Belief Compression

far dominant term running time algorithm time compute
E-PCA bases. bases found low-dimensional space
discretized, running time required value iteration converge policy
problems described order 50 100ms.
Sample Belief Collection
example problems addressed, used standard sample size 500
sample beliefs. Additionally, used hand-coded heuristic controllers sample beliefs
model. practice, found 500 sample beliefs collected using semi-random controller sufficient example problems. However, may able improve overall
performance algorithm future problems iterating phases building
belief space representation (i.e., collecting beliefs generating low-dimensional
representation) computing good controller. initial set beliefs
collected used build initial set bases corresponding policy, continue
evaluate error representation (e.g., K-L divergence current belief
low-dimensional representation). initial representation learned
beliefs, representation may over-fit beliefs; detect situation
noticing representation poor job representing new beliefs. Validation
techniques cross-validation may also useful determining enough beliefs
acquired.
Model Selection
One open questions addressed far choosing appropriate
number bases representation. Unless problem-specific information,
true number degrees freedom belief space (as toy example
section 3), difficult identify appropriate dimensionality underlying surface
control. One common approach examine eigenvalues decomposition,
recovered using orthonormalization step algorithm Table 1.
(This assumes particular link function capable expressing surface
data lies on.) eigenvalues conventional PCA often used determine
appropriate dimensionality underlying surface; certainly reconstruction
lossless use many bases non-zero eigenvalues.
Unfortunately, recall description E-PCA section 4 generate
set singular values, eigenvalues. non-linear projection introduced link
function causes eigenvalues U matrix uninformative contribution
basis representation. Instead using eigenvalues choose appropriate
surface dimensionality, use reconstruction quality, Figure 11. Using reconstruction
quality estimate appropriate dimensionality common choice PCA
dimensionality reduction techniques (Tenenbaum, de Silva, & Langford, 2000). One
alternate choice would evaluate reward policies computed different dimensionalities choose compact representation achieves highest reward,
essentially using control error rather reconstruction quality determine dimensionality.
33

fiRoy, Gordon, & Thrun

Recall discussion section 2 using dimensionality reduction
represent beliefs POMDPs specific kind structure. particular, E-PCA
representation useful representing beliefs relatively sparse
small number degrees freedom. However, E-PCA unable find good lowdimensional representations POMDP models exhibit kind structure
is, beliefs cannot represented lying low-dimensional hyperplane linked
full belief space via appropriate link function. One additional problem
know priori whether specific POMDP appropriate structure. unlikely
general technique determine usefulness E-PCA,
take advantage model selection techniques also determine whether E-PCA
find usefully low dimensional representation specific POMDP. example,
KL divergence set sample beliefs reconstructions large even using
large number bases, problem may right structure.

9. Related Work
Many attempts made use reachability analysis constrain set beliefs
planning (Washington, 1997; Hauskrecht, 2000; Zhou & Hansen, 2001; Pineau, Gordon,
& Thrun, 2003a). reachable set beliefs relatively small, forward search
find set perfectly reasonable approach. policy computed beliefs course optimal, although relatively rare real world problems able
enumerate reachable beliefs. Reachability analysis also used
success heuristic guiding search methods, especially focusing computation
finding function approximators (Washington, 1997; Hansen, 1998). approach,
problem still remains compute low-dimensional representation given finite
set representative beliefs. Discretization belief space explored
number times, regular grid-based discretization (Lovejoy, 1991), regular variable
resolution approaches (Zhou & Hansen, 2001) non-regular variable resolution representations (Brafman, 1997; Hauskrecht, 2000). vein, state abstraction (Boutilier &
Poole, 1996) explored take advantage factored state spaces, particular
interest algorithm Hansen Feng (2000) perform state abstraction
absence prior factorization. far, however, approaches fallen
victim curse dimensionality failed scale dozen
states most.
value-directed POMDP compression algorithm Poupart Boutilier (2002)
dimensionality-reduction technique closer spirit ours, technique.
algorithm computes low-dimensional representation POMDP directly model
parameters R, , finding Krylov subspace reward function belief
propagation. Krylov subspace vector matrix smallest subspace
contains vector closed multiplication matrix. POMDPs,
authors use smallest subspace contains immediate reward vector closed
set linear functions defined state transitions observation model.
major advantage approach optimizes correct criterion: value-directed
compression distinguish beliefs different value. major
disadvantage approach Krylov subspace constrained linear. Using
34

fiFinding Approximate POMDP Solutions Belief Compression

algorithm PCA instead E-PCA, realize much compression
Poupart Boutilier (2002) method: take advantage regularities
transition matrices a,z reward function R. Unfortunately,
seen, beliefs unlikely lie low-dimensional hyperplane, results reported
section 3 indicate linear compression scale size problems wish
address.
Possibly promising approaches finding approximate value-functions
point-based methods, instead optimizing value function entire
belief space, specific beliefs. Cheng (1988) described method backing
value function specific belief points procedure called point-based dynamic
programming (PB-DP). PB-DP steps interleaved standard backups
full value iteration. Zhang Zhang (2001) improved method choosing Witness
points backup belief points, iteratively increasing number points.
essential idea point-based backups significantly cheaper full backup steps.
Indeed, algorithm described Zhang Zhang (2001) out-performs Hansens exact
policy-search method order magnitude small problems. However, need
periodic backups across full belief space still limits applicability algorithms
small abstract problems.
recently, Pineau et al. (2003a) abandoned full value function backups
favour point-based backups point-based value iteration (PBVI) algorithm.
backing discrete belief points, backup operator polynomial instead
exponential (as value iteration), and, even importantly, complexity
value function remains constant. PBVI uses fundamentally different approach finding
POMDP policies, still remains constrained curse dimensionality large state
spaces. However, applied successfully problems least order magnitude
larger predecessors, another example algorithms used make
large POMDPs tractable.
E-PCA possible technique non-linear dimensionality reduction;
exists large body work containing different techniques Self-Organizing Maps (Kohonen, 1982), Generative Topographic Mapping (Bishop, Svensen, & Williams, 1998),
Stochastic Neighbour Embedding (Hinton & Roweis, 2003). Two successful
algorithms emerge recently Isomap (Tenenbaum et al., 2000) Locally Linear Embedding (Roweis & Saul, 2000). Isomap extends PCA-like methods non-linear
surfaces using geodesic distances distance metric data samples, rather
Euclidean distances. Locally Linear Embedding (LLE) considered local alternative
global reduction Isomap represents point weighted combination neighbours operates two phases: computing weights k nearest
neighbours high-dimensional point, reconstructing data lowdimensional co-ordinate frame weights. However, algorithms contain
explicit models kind data (e.g., probability distributions) attempting
model. One interesting line research, however, may extend algorithms using
different loss functions manner PCA extended E-PCA.
35

fiRoy, Gordon, & Thrun

10. Conclusion
Partially Observable Markov Decision Processes considered intractable finding
good controllers real world domains. particular, best algorithms date
finding approximate value function full belief space scaled beyond
hundred states (Pineau et al., 2003a). However, demonstrated real world
POMDPs contain structured belief spaces; finding using structure,
able solve POMDPs order magnitude larger solved conventional
value iteration techniques. Additionally, able solve different kinds POMDPs,
simple highly-structured synthetic problem robot navigation problem
problem factored belief space relatively complicated probability distributions.
algorithm used find structure related Principal Components Analysis
loss function specifically chosen representing probability distributions. real
world POMDPs able solve characterized sparse distributions,
Exponential family PCA algorithm particularly effective compressing data.
exist POMDP problems structure,
dimensionality reduction technique work well; however, question
investigation other, related dimensionality-reduction techniques (e.g., Isomap LocallyLinear Embedding, Tenenbaum et al., 2000; Roweis, Saul, & Hinton, 2002) applied.
number interesting possibilities extending algorithm order
improve efficiency increase domain applicability. loss function
chose dimensionality reduction based reconstruction error,
L(B, U, B) = e(U B) B U B,

(37)

(cf. equation 8). Minimizing reconstruction error allow near-optimal policies
learned. However, would ideally like find compact representation
minimizes control errors. could possibly better approximated taking advantage
transition probability structure. example, dimensionality reduction minimizes
prediction errors would correspond loss function:
L(B, U, B, ) = e(U b) B U b + kB,2...n B,1...n1 k2

(38)

B,1...n1 l n 1 matrix first n 1 column vectors B, B,2...n
l n 1 matrix n 1 column vectors V starting second vector.
effect finding representation allows bt+1 predicted bt ,
caveat B must arranged action. plan address issue
future work.
Another shortcoming approach described work contains
assumption beliefs described using low-dimensional representation.
However, relatively easy construct example problem generates beliefs
lie two distinct low-dimensional surfaces, current formulation would make
apparent dimensionality beliefs appear much higher set beliefs sampled
one surface alone.
work largely motivated finding better representations beliefs,
approach solving large POMDPs. Policy search methods (Meuleau,
36

fiFinding Approximate POMDP Solutions Belief Compression

Peshkin, Kim, & Kaelbling, 1999) hierarchical methods (Pineau, Gordon, & Thrun,
2003b) also able solve large POMDPs. interesting note controllers
based E-PCA representations often essentially independent policy complexity
strongly dependent belief complexity, whereas policy search hierarchical
methods strongly dependent policy complexity largely independent belief
space complexity. seems likely progress solving large POMDPs general lie
combination approaches.
E-PCA algorithm finds low-dimensional representation B full belief space B
sampled data. demonstrated reliance sampled data obstacle
real world problems. Furthermore, using sampled beliefs could asset
large problems generating tracking beliefs considerably easier
planning. may however preferable try compute low-dimensional representation
directly model parameters. Poupart Boutilier (2002) use notion Krylov
subspace this. subspace computed algorithm may correspond exactly
conventional PCA seen instances PCA poor job finding
low-dimensional representations. likely explanation real-world beliefs
lie low-dimensional planes problems, instead curved surfaces.
extremely useful algorithm would one finds subset belief space closed
transition observation function, constrained find planes.

Acknowledgements
Thanks Tom Mitchell, Leslie Kaelbling, Reid Simmons, Drew Bagnell, Aaron Courville,
Mike Montemerlo Joelle Pineau useful comments insight work. Nicholas
Roy funded National Science Foundation ITR grant # IIS-0121426. Geoffrey Gordon funded AFRL contract F3060201C0219, DARPAs MICA program,
AFRL contract F306029820137, DARPAs CoABS program.

References
Bagnell, J. A., & Schneider, J. (2001). Autonomous helicopter control using reinforcement
learning policy search methods. Proceedings IEEE International Conference
Robotics Automation (ICRA), pp. 16151620, Seoul, South Korea. IEEE Press.
Bishop, C., Svensen, M., & Williams, C. (1998). GTM: generative topographic mapping.
Neural Computation, 10 (1), 215234.
Boutilier, C., & Poole, D. (1996). Computing optimal policies partially observable
Markov decision processes using compact representations. Proceedings 13th
National Conference Artificial Intelligence (AAAI-96), pp. 11681175.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings 14th Annual Conference Uncertainty AI (UAI), pp. 3342,
Madison, Wisconsin.
Brafman, R. I. (1997). heuristic variable grid solution method POMDPs. Kuipers,
B. K., & Webber, B. (Eds.), Proceedings 14th National Conference Artificial
Intelligence (AAAI), pp. 727733, Providence, RI.
37

fiRoy, Gordon, & Thrun

Cassandra, A. R., Kaelbling, L., & Kurien, J. A. (1996). Acting uncertainty: Discrete Bayesian models mobile-robot navigation. Proceedings IEEE/RSJ
International Conference Intelligent Robots Systems.
Chen, B. M. (2000). Robust H- Control. Springer-Verlag.
Cheng, H.-T. (1988). Algorithms Partially Observable Markov Decision Processes. Ph.D.
thesis, University British Columbia, Vancouver, Canada.
Collins, M., Dasgupta, S., & Schapire, R. (2002). generalization principal components
analysis exponential family. Dietterich, T. G., Becker, S., & Ghahramani, Z.
(Eds.), Advances Neural Information Processing Systems 14 (NIPS), Cambridge,
MA. MIT Press.
Cox, T., & Cox, M. (1994). Multidimensional Scaling. Chapman & Hall, London.
Fox, D., Burgard, W., & Thrun, S. (1999). Markov localization mobile robots dynamic
environments. Journal Artificial Intelligence Research, 11, 391427.
Galassi, M., Davies, J., Theiler, J., Gough, B., Jungman, G., Booth, M., & Rossi,
F. (2002).
GNU Scientific Library Reference Manual (3rd Edition edition).
http://www.gnu.org/software/gsl/.
Golub, G., & Reinsch, C. (1970). Singular value decomposition least squares solutions.
Numerische Mathematik, pp. 403420.
Gordon, G. (1995). Stable function approximation dynamic programming. Prieditis,
A., & Russell, S. (Eds.), Proceedings 12 International Conference Machine
Learning (ICML), pp. 261268, San Francisco, CA. Morgan Kaufmann.
Gordon, G. (2003). Generalized2 linear2 models. Becker, S., Thrun, S., & Obermayer, K.
(Eds.), Advances Neural Information Processing Systems 15 (NIPS). MIT Press.
Gutmann, J.-S., Burgard, W., Fox, D., & Konolige, K. (1998). experimental comparison
localization methods. Proceedings IEEE/RSJ International Conference
Intelligent Robots Systems, Victoria, Canada.
Gutmann, J.-S., & Fox, D. (2002). experimental comparison localization methods
continued. Proceedings IEEE/RSJ International Conference Intelligent
Robots Systems, Lausanne, Switzerland.
Hansen, E., & Feng, Z. (2000). Dynamic programming POMDPs using factored state
representation. Proceedings Fifth International Conference Artificial
Intelligence Planning Scheduling (AIPS-00), Breckenridge, CO.
Hansen, E. (1998). Solving POMDPs searching policy space. Proceedings
14th Conference Uncertainty Artifical Intelligence (UAI), pp. 211219, Madison,
WI.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov
decision processes. Journal Artificial Intelligence Research, 13, 3394.
Hinton, G., & Roweis, S. (2003). Stochastic neighbor embedding. Becker, S., Thrun,
S., & Obermayer, K. (Eds.), Advances Neural Information Processing Systems 15
(NIPS). MIT Press.
38

fiFinding Approximate POMDP Solutions Belief Compression

Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT.
Isard, M., & Blake, A. (1998). CONDENSATION conditional density propagation
visual tracking. International Journal Computer Vision, 29 (1), 528.
Joliffe, I. T. (1986). Principal Component Analysis. Springer-Verlag.
Kanazawa, K., Koller, D., & Russell, S. (1995). Stochastic simulation algorithms dynamic
probabilistic networks. Proceedings 11th Annual Conference Uncertainty
AI (UAI), pp. 346351, Montreal, Canada.
Kohonen, T. (1982). Self-organized formation topologically correct feature maps. Biological Cybernetics, 48, 5969.
Lee, D. D., & Seung, H. S. (1999). Learning parts objects non-negative matrix
factorization. Nature, 401, 788791.
Leonard, J., & Durrant-Whyte, H. (1991). Mobile robot localization tracking geometric
beacons. IEEE Transactions Robotics Automation, 7 (3), 376382.
Lovejoy, W. S. (1991). Computationally feasible bounds partially observable Markov
decison processes. Operations Research, 39, 192175.
Mardia, K. V., & Jupp, P. E. (2000). Directional Statistics (2nd edition). Wiley, Chichester,
NY.
McCullagh, P., & Nelder, J. A. (1983). Generalized Linear Models (2nd edition). Chapman
Hall, London.
Meuleau, N., Peshkin, L., Kim, K.-E., & Kaelbling, L. P. (1999). Learning finite-state controllers partially observable environments. Laskey, K. B., & Prade, H. (Eds.),
Proceedings Fifteenth International Conference Uncertainty Artificial Intelligence, pp. 427436, Stockholm, Sweden. Morgan Kaufmann.
Munos, R., & Moore, A. (1999). Variable resolution discretization high-accuracy solutions optimal control problems. Dean, T. (Ed.), Proceedings 16th International Joint Conference Artificial Intelligence (IJCAI), pp. 13481355, Stockholm
Sweden. Morgan Kaufmann.
Munos, R., & Moore, A. (2002). Variable resolution discretization optimal control. Machine Learning, 49 (2-3), 291323.
Nourbakhsh, I., Powers, R., & Birchfield, S. (1995). DERVISH office-navigating robot.
AI Magazine, 16 (2), 5360.
Olson, C. F. (2000). Probabilistic self-localization mobile robots. IEEE Transactions
Robotics Automation, 16 (1), 5566.
Pineau, J., Gordon, G., & Thrun, S. (2003a). Point-based value iteration: anytime
algorithm POMDPs. Proceedings 18th International Joint Conference
Artificial Intelligence (IJCAI 2003), Acapulco, Mexico.
Pineau, J., Gordon, G., & Thrun, S. (2003b). Policy-contingent abstraction robust robot
control. Meek, C., & Kjlruff, U. (Eds.), Proceedings 19th Annual Conference
Uncertainty Artificial Intelligence (UAI), Acapulco, Mexico.
39

fiRoy, Gordon, & Thrun

Poupart, P., & Boutilier, C. (2002). Value-directed compression POMDPs. Becker,
S., Thrun, S., & Obermayer, K. (Eds.), Advances Neural Information Processing
Systems 15 (NIPS), Vancouver, Canada. MIT Press.
Rockafellar, R. T. (1970). Convex Analysis. Princeton University Press, New Jersey.
Roweis, S., & Saul, L. (2000). Nonlinear dimensionality reduction locally linear embedding.. Science, 290 (5500), 23232326.
Roweis, S. T., Saul, L. K., & Hinton, G. E. (2002). Global coordination local linear
models. Dietterich, T. G., Becker, S., & Ghahramani, Z. (Eds.), Advances
Neural Information Processing Systems, Vol. 14, Cambridge, MA. MIT Press.
Roy, N., & Thrun, S. (1999). Coastal navigation mobile robots. Solla, S. A., todd
K. Leen, & Muller, K. R. (Eds.), Advances Neural Processing Systems 12 (NIPS),
pp. 10431049, Denver, CO. MIT Press.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: Modern Approach. Prentice Hall.
Shatkay, H., & Kaelbling, L. P. (2002). Learning geometrically-constrained hidden markov
models robot navigation: Bridging geometrical-topological gap. Journal AI
Research.
Tenenbaum, J. B., de Silva, V., & Langford, J. C. (2000). global geometric framework
nonlinear dimensionality reduction. Science, 290 (5500), 23192323.
Thrun, S., Fox, D., Burgard, W., & Dellaert, F. (2000). Robust Monte Carlo localization
mobile robots. Artificial Intelligence, 128 (1-2), 99141.
Washington, R. (1997). BI-POMDP: Bounded, incremental partially-observable Markovmodel planning. Proceedings 4th European Conference Planning (ECP).
Zhang, N. L., & Zhang, W. (2001). Speeding convergence value iteration partially observable Markov decision processes. Journal Artificial Intelligence Research,
14, 128.
Zhou, R., & Hansen, E. (2001). improved grid-based approximation algorithm
POMDPs. Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI), pp. 707716, Seattle, Washington. Morgan
Kaufmann.

40

fiJournal Artificial Intelligence Research 23 (2005) 367-420

Submitted 07/04; published 04/05

Hybrid BDI-POMDP Framework Multiagent Teaming
Ranjit Nair

ranjit.nair@honeywell.com

Automation Control Solutions
Honeywell Laboratories, Minneapolis, MN 55416

Milind Tambe

tambe@usc.edu

Department Computer Science
University Southern California, Los Angeles, CA 90089

Abstract
Many current large-scale multiagent team implementations characterized
following belief-desire-intention (BDI) paradigm, explicit representation team
plans. Despite promise, current BDI team approaches lack tools quantitative
performance analysis uncertainty. Distributed partially observable Markov decision
problems (POMDPs) well suited analysis, complexity finding optimal
policies models highly intractable. key contribution article
hybrid BDI-POMDP approach, BDI team plans exploited improve POMDP
tractability POMDP analysis improves BDI team plan performance.
Concretely, focus role allocation, fundamental problem BDI teams:
agents allocate different roles team. article provides three key contributions. First, describe role allocation technique takes account future
uncertainties domain; prior work multiagent role allocation failed address
uncertainties. end, introduce RMTDP (Role-based Markov Team Decision Problem), new distributed POMDP model analysis role allocations.
technique gains tractability significantly curtailing RMTDP policy search; particular, BDI team plans provide incomplete RMTDP policies, RMTDP policy search
fills gaps incomplete policies searching best role allocation.
second key contribution novel decomposition technique improve RMTDP
policy search efficiency. Even though limited searching role allocations, still
combinatorially many role allocations, evaluating RMTDP identify best
extremely difficult. decomposition technique exploits structure BDI team
plans significantly prune search space role allocations. third key contribution
significantly faster policy evaluation algorithm suited BDI-POMDP hybrid approach. Finally, also present experimental results two domains: mission rehearsal
simulation RoboCupRescue disaster rescue simulation.

1. Introduction
Teamwork, whether among software agents, robots (and people) critical capability
large number multiagent domains ranging mission rehearsal simulations,
RoboCup soccer disaster rescue, personal assistant teams. Already large number multiagent teams developed range domains (Pynadath & Tambe,
2003; Yen, Yin, Ioerger, Miller, Xu, & Volz, 2001; Stone & Veloso, 1999; Jennings, 1995;
Grosz, Hunsberger, & Kraus, 1999; Decker & Lesser, 1993; Tambe, Pynadath, & Chauvat,
2000; da Silva & Demazeau, 2002). existing practical approaches characterized situated within general belief-desire-intention (BDI) approach, paradigm
c
2005
AI Access Foundation. rights reserved.

fiNair & Tambe

designing multiagent systems, made increasingly popular due programming frameworks (Tambe et al., 2000; Decker & Lesser, 1993; Tidhar, 1993b) facilitate design
large-scale teams. Within approach, inspired explicitly implicitly BDI logics,
agents explicitly represent reason team goals plans (Wooldridge, 2002).
article focuses analysis BDI teams, provide feedback aid human
developers possibly agents participating team, team performance
complex dynamic domains improved. particular, focuses critical
challenge role allocation building teams (Tidhar, Rao, & Sonenberg, 1996; Hunsberger
& Grosz, 2000), i.e. agents allocate various roles team. instance,
mission rehearsal simulations (Tambe et al., 2000), need select numbers
types helicopter agents allocate different roles team. Similarly, disaster
rescue (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjoh, & Shimada, 1999), role
allocation refers allocating fire engines ambulances fires greatly impact
team performance. domains, performance team
linked important metrics loss human life property thus critical
analyze team performance suggest improvements.
BDI frameworks facilitate human design large scale teams, key difficulty
analyzing role allocation teams due uncertainty arises complex
domains. example, actions may fail world state may partially observable
agents owing physical properties environment imperfect sensing. Role
allocation demands future uncertainties taken account, e.g. fact
agent may fail execution may may replaced another must taken
account determining role allocation. Yet current role allocation algorithms address uncertainty (see Section 7.4). Indeed, uncertainty requires
quantitative comparison different role allocations. However, tools quantitative
evaluations BDI teams currently absent. Thus, given uncertainties, may
required experimentally recreate large number possible scenarios (in real domain
simulations) evaluate compare different role allocations.
Fortunately, emergence distributed Partially Observable Markov Decision Problems (POMDPs) provides models (Bernstein, Zilberstein, & Immerman, 2000; Boutilier,
1996; Pynadath & Tambe, 2002; Xuan, Lesser, & Zilberstein, 2001) used
quantitative analysis agent teams uncertain domains. Distributed POMDPs represent class formal models powerful enough express uncertainty
dynamic domains arising result non-determinism partial observability
principle, used generate evaluate complete policies multiagent team.
However, two shortcomings models prevents application
analysis role allocation. First, previous work analysis focused communication (Pynadath & Tambe, 2002; Xuan et al., 2001), rather role allocation
coordination decisions. Second, shown Bernstein et al. (2000), problem
deriving optimal policy generally computationally intractable (the corresponding
decision problem NEXP-complete). Thus, applying optimal policies analysis highly
intractable.
address first difficulty, derive RMTDP (Role-based Multiagent Team Decision
Problem), distributed POMDP framework quantitatively analyzing role allocations.
Using framework, show that, general, problem finding optimal role
368

fiHybrid BDI-POMDP Framework Multiagent Teaming

completed policy =
additions BDI team plan

BDI team plan
RMTDP
Search Policy Space

Incomplete policy
BDI Interpreter

Domain
RMTDP model

Figure 1: Integration BDI POMDP.

allocation policy computationally intractable (the corresponding decision problem still
NEXP-complete). shows improving tractability analysis techniques role
allocation critically important issue.
Therefore, order make quantitative analysis multiagent teams using RMTDP
tractable, second contribution provides hybrid BDI-POMDP approach
combines native strengths BDI POMDP approaches, i.e., ability BDI
frameworks encode large-scale team plans POMDP ability quantitatively
evaluate plans. hybrid approach based three key interactions improve
tractability RMTDP optimality BDI agent teams. first interaction
shown Figure 1. particular, suppose wish analyze BDI agent team (each agent
consisting BDI team plan domain independent interpreter helps coordinate
plans) acting domain. shown Figure 1, model domain via
RMTDP, rely BDI team plan interpreter providing incomplete policy
RMTDP. RMTDP model evaluates different completions incomplete
policy provides optimally completed policy feedback BDI system. Thus,
RMTDP fills gaps incompletely specified BDI team plan optimally.
gaps concentrate role allocations, method applied
key coordination decisions. restricting optimization role allocation decisions
fixing policy points, able come restricted policy
space. use RMTDPs effectively search restricted space order find
optimal role allocation.
restricted policy search one key positive interaction hybrid approach,
second interaction consists efficient policy representation used converting
BDI team plan interpreter corresponding policy (see Figure 1) new
algorithm policy evaluation. general, agents policy distributed POMDP
indexed observation history (Bernstein et al., 2000; Pynadath & Tambe, 2002).
369

fiNair & Tambe

However, BDI system, agent performs action selection based set
privately held beliefs obtained agents observations applying belief
revision function. order evaluate teams performance, sufficient RMTDP
index agents policies belief state (represented privately held beliefs)
instead observation histories. shift representation results considerable
savings amount time needed evaluate policy space required
represent policy.
third key interaction hybrid approach exploits BDI team plan structure increasing efficiency RMTDP-based analysis. Even though RMTDP
policy space restricted filling gaps incomplete policies, many policies may result
given large number possible role allocations. Thus enumerating evaluating
possible policy given domain difficult. Instead, provide branch-and-bound algorithm exploits task decomposition among sub-teams team significantly prune
search space provide correctness proof worst-case analysis algorithm.
order empirically validate approach, applied RMTDP allocation
BDI teams two concrete domains: mission rehearsal simulations (Tambe et al., 2000)
RoboCupRescue (Kitano et al., 1999). first present (significant) speed-up
gained three interactions mentioned above. Next, domains, compared
role allocations found approach state-of-the-art techniques allocate
roles without uncertainty reasoning. comparison shows importance reasoning
uncertainty determining role allocation complex multiagent domains.
RoboCupRescue domain, also compared allocations found allocations chosen
humans actual RoboCupRescue simulation environment. results showed
role allocation technique presented article capable performing human
expert levels RoboCupRescue domain.
article organized follows: Section 2 presents background motivation.
Section 3, introduce RMTDP model present key complexity results. Section
4 explains BDI team plan evaluated using RMTDP. Section 5 describes
analysis methodology finding optimal role allocation, Section 6 presents
empirical evaluation methodology. Section 7, present related work
Section 8, list conclusions.

2. Background
section first describes two domains consider article: abstract
mission rehearsal domain (Tambe et al., 2000) RoboCupRescue domain (Kitano
et al., 1999). domain requires us allocate roles agents team. Next, teamoriented programming (TOP), framework describing team plans described
context two domains. focus TOP, discussed Section 7.1,
techniques would applicable frameworks tasking teams (Stone & Veloso,
1999; Decker & Lesser, 1993).
2.1 Domains
first domain consider based mission rehearsal simulations (Tambe et al.,
2000). expository purposes, intentionally simplified. scenario
370

fiHybrid BDI-POMDP Framework Multiagent Teaming

follows: helicopter team executing mission transporting valuable cargo point
X point enemy terrain (see Figure 2). three paths X
different lengths different risk due enemy fire. One scouting sub-teams must
sent (one path X Y), larger size scouting sub-team
safer is. scouts clear one path X Y, transports
move safely along path. However, scouts may fail along path, may
need replaced transport cost transporting cargo. Owing partial
observability, transports may receive observation scout failed
route cleared. wish transport amount cargo quickest
possible manner within mission deadline.
key role allocation decision given fixed number helicopters,
allocated scouting transport roles? Allocating scouts means
scouting task likely succeed, fewer helicopters left
used transport cargo consequently less reward. However, allocating
scouts could result mission failing altogether. Also, allocating scouts,
routes scouts sent on? shortest route would preferable
risky. Sending scouts route decreases likelihood failure
individual scout; however, might beneficial send different routes, e.g.
scouts risky short route others safe longer route.
Thus many role allocations consider. Evaluating difficult
role allocation must look-ahead consider future implications uncertainty, e.g. scout
helicopters fail scouting may need replaced transport. Furthermore, failure success scout may visible transport helicopters hence
transport may replace scout transports may never fly destination.
scout
transports

X

route 1

route 2



enemy gun
route 3

Figure 2: Mission rehearsal domain.

second example scenario (see Figure 3), set RoboCupRescue disaster
simulation environment (Kitano et al., 1999), consists five fire engines three different
fire stations (two stations 1 & 3 last station 2) five ambulances
stationed ambulance center. Two fires (in top left bottom right corners
map) start need extinguished fire engines. fire extinguished,
ambulance agents need save surviving civilians. number civilians
371

fiNair & Tambe

location known ahead time, although total number civilians known.
time passes, high likelihood health civilians deteriorate fires
increase intensity. Yet agents need rescue many civilians possible
minimal damage buildings. first part goal scenario therefore
first determine fire engines assign fire. fire engines gathered
information number civilians fire, transmitted ambulances.
next part goal allocate ambulances particular fire rescue
civilians trapped there. However, ambulances cannot rescue civilians fires fully
extinguished. Here, partial observability (each agent view objects within visual
range), uncertainty related fire intensity, well location civilians
health add significantly difficulty.

C1
F3
F2
F1
C2


Figure 3: RoboCupRescue Scenario: C1 C2 denote two fire locations, F1, F2
F3 denote fire stations 1, 2 3 respectively denotes ambulance
center.

2.2 Team-Oriented Programming
aim team-oriented programming (TOP) (Pynadath & Tambe, 2003; Tambe et al.,
2000; Tidhar, 1993b) framework provide human developers (or automated symbolic
planners) useful abstraction tasking teams. domains described
Section 2.1, consists three key aspects team: (i) team organization hierarchy
consisting roles; (ii) team (reactive) plan hierarchy; (iii) assignment roles
sub-plans plan hierarchy. developer need specify low-level coordination
details. Instead, TOP interpreter (the underlying coordination infrastructure) automatically enables agents decide communicate reallocate
372

fiHybrid BDI-POMDP Framework Multiagent Teaming

roles upon failure. TOP abstraction enables humans rapidly provide team plans
large-scale teams, unfortunately, qualitative assessment team performance
feasible. Thus, key TOP weakness inability quantitatively evaluate optimize
team performance, e.g., allocating roles agents qualitative matching capabilities may feasible. discussed later, hybrid BDI-POMDP model addresses
weakness providing techniques quantitative evaluation.
concrete example, consider TOP mission rehearsal domain. first
specify team organization hierarchy (see Figure 4(a)). Task Force highest level
team organization consists two roles Scouting Transport,
Scouting sub-team roles three scouting sub-sub-teams. Next specify
hierarchy reactive team plans (Figure 4(b)). Reactive team plans explicitly express
joint activities relevant team consist of: (i) pre-conditions plan
proposed; (ii) termination conditions plan ended; (iii)
team-level actions executed part plan (an example plan discussed
shortly). Figure 4(b), highest level plan Execute Mission three sub-plans:
DoScouting make one path X safe transports, DoTransport
move transports along scouted path, RemainingScouts scouts
reached destination yet get there.
Execute Mission [Task Force]
DoScouting
[Task Force]

RemainingScouts
DoTransport
[Scouting Team] [Transport Team]

Task Force
ScoutRoutes
WaitAtBase
[Transport Team] [Scouting Team]

Scouting Team

Transport Team

SctTeamA SctTeamB SctTeamC

ScoutRoute1 ScoutRoute2 ScoutRoute3
[SctTeamA] [SctTeamB] [SctTeamC]

(a)

(b)

Figure 4: TOP mission rehearsal domain a: Organization hierarchy; b: Plan hierarchy.

Figure 4(b) also shows coordination relationships: relationship indicated
solid arc, relationship indicated dashed arc. Thus, WaitAtBase ScoutRoutes must done least one ScoutRoute1,
ScoutRoute2 ScoutRoute3 need performed. also temporal dependence relationship among sub-plans, implies sub-teams assigned perform
DoTransport RemainingScouts cannot DoScouting plan completed. However, DoTransport RemainingScouts execute parallel. Finally,
assign roles plans Figure 4(b) shows assignment brackets adjacent plans.
instance, Task Force team assigned jointly perform Execute Mission SctTeamA assigned ScoutRoute1.
team plan corresponding Execute Mission shown Figure 5.
seen, team plan consists context, pre-conditions, post-conditions, body constraints. context describes conditions must fulfilled parent plan
pre-conditions particular conditions cause sub-plan begin exe373

fiNair & Tambe

cution. Thus, Execute Mission, pre-condition team mutually believes
(MB)1 start location. post-conditions divided Achieved,
Unachievable Irrelevant conditions sub-plan terminated.
body consists sub-plans exist within team plan. Lastly, constraints describe
temporal constraints exist sub-plans body. description
plans plan hierarchy Figure 4(b) given Appendix A.
ExecuteMission:
Context:
Pre-conditions: (MB <TaskForce> location(TaskForce) = START)
Achieved: (MB <TaskForce> (Achieved(DoScouting) Achieved(DoTransport))) (time
> (MB <TaskForce>
Achieved(RemainingScouts) ( helo ScoutingTeam, alive(helo)
location(helo) 6= END)))
Unachievable: (MB <TaskForce> Unachievable(DoScouting)) (MB <TaskForce>
Unachievable(DoTransport)
(Achieved(RemainingScouts) ( helo ScoutingTeam, alive(helo)
location(helo) 6= END)))
Irrelevant:
Body:
DoScouting
DoTransport
RemainingScouts
Constraints: DoScouting DoTransport, DoScouting RemainingScouts

Figure 5: Example team plan. MB refers mutual belief.
HTN (Dix, Muoz-Avila, Nau, & Zhang, 2003; Erol, Hendler, & Nau, 1994),
plan hierarchy TOP gives decomposition task smaller tasks. However,
language TOPs richer language early HTN planning (Erol et al., 1994)
contained simple ordering constraints. seen example, plan hierarchy
TOPs also contain relationships like OR. addition, like recent
work HTN planning (Dix et al., 2003), sub-plans TOPs contain pre-conditions
post-conditions, thus allowing conditional plan execution. main differences
TOPs HTN planning are: (i) TOPs contain organization hierarchy addition
plan hierarchy, (ii) TOP interpreter ensures team executes plans coherently.
seen later, TOPs analyzed expressiveness including conditional
execution; however, since analysis focus fixed time horizon, loops
task description unrolled time horizon.
1. Mutual Belief (Wooldridge, 2002), shown (MB hteami x) Figure 5, refers private belief held
agent team believe fact x true, agents
team believe x true, every agent believes every agent believes x
true on. infinite levels nesting difficult realize practice. Thus, practical
BDI implementations, purposes article, mutual belief approximated private
belief held agent agents team believe x true.

374

fiHybrid BDI-POMDP Framework Multiagent Teaming

new observation
agent

Belief Update
function

Private beliefs
agent

Figure 6: Mapping observations beliefs.

execution, agent copy TOP. agent also maintains set
private beliefs, set propositions agent believes true (see
Figure 6). agent receives new beliefs, i.e. observations (including communication),
belief update function used update set privately held beliefs. instance,
upon seeing last scout crashed, transport may update privately held beliefs
include belief CriticalFailure(DoScouting). practical BDI systems, belief
update computation low complexity (e.g. constant linear time). beliefs
updated, agent selects plan execute matching beliefs preconditions plans. basic execution cycle similar standard reactive planning
systems PRS (Georgeff & Lansky, 1986).
team plan execution, observations form communications often arise
coordination actions executed TOP interpreter. instance, TOP
interpreters exploited BDI theories teamwork, Levesque et al.s theory
joint intentions (Levesque, Cohen, & Nunes, 1990) require agent
comes privately believe fact terminates current team plan (i.e. matches
achievement unachievability conditions team plan), communicates fact
rest team. performing coordination actions automatically, TOP
interpreter enables coherence initiation termination team plans within TOP.
details examples TOPs seen work Pynadath
Tambe (2003), Tambe et al. (2000) Tidhar (1993b).
concretely illustrate key challenges role allocation mentioned
earlier. First, human developer must allocate available agents organization hierarchy (Figure 4(a)), find best role allocation. However, combinatorially many
allocations choose (Hunsberger & Grosz, 2000; Tambe et al., 2000). instance,
starting 6 homogeneous helicopters results 84 different ways deciding
many agents assign scouting transport sub-team. problem exacerbated fact best allocation varies significantly based domain variations.
example, Figure 7 shows three different assignments agents team organization hierarchy, found analysis best given setting failure
observation probabilities (details Section 6). example, increasing probability
failures routes resulted number transports best allocation changing
four (see Figure 7(b)) three (see Figure 7(a)), additional scout added
SctTeamB. failures possible all, number transports increased
five (see Figure 7(c)). analysis takes step towards selecting best among
allocations.
375

fiNair & Tambe

Task Force
Scouting Team

Task Force

Transport Team=3

Scouting Team

SctTeamA=2 SctTeamB=1 SctTeamC=0

Transport Team=4

SctTeamA=2 SctTeamB=0 SctTeamC=0

(a) Medium probability

(b) Low probability
Task Force

Scouting Team

Transport Team=5

SctTeamA=0 SctTeamB=0 SctTeamC=1

(c) Zero probability

Figure 7: Best role allocations different probabilities scout failure.

Figure 8 shows TOP RoboCupRescue scenario. seen, plan hierarchy scenario consists pair ExtinguishFire RescueCivilians plans
done parallel, decompose individual plans. (These individual plans get fire engines ambulances move streets using specific
search algorithms, however, individual plans relevant discussions
article; interested readers refer description RoboCupRescue team
entered RoboCup competitions 2001 (Nair, Ito, Tambe, & Marsella, 2002).)
organizational hierarchy consists Task Force comprising two Engine sub-teams, one
fire Ambulance Team, engine teams assigned extinguishing
fires ambulance team assigned rescuing civilians. particular TOP,
assignment ambulances AmbulanceTeamA AmbulanceTeamB conditioned
communication c, indicated AmbulanceTeamA|c AmbulanceTeamB|c.
c described detail figure, refers communication received fire engines describes number civilians present fire.
problem engines assign Engine Team possible value c,
ambulances assign Ambulance Team. Note engines differing
capabilities owing differing distances fires ambulances identical
capabilities.
Task Force
EngineTeamA

EngineTeamB

AmbulanceTeam

AmbulanceTeamA |c

AmbulanceTeamB |c

(a)
ExecuteMission
[Task Force]
ExtinguishFire1
[EngineTeamA]

RescueCivilians1
[AmbulanceTeamA]

ExtinguishFire2
[EngineTeamB]

RescueCivilians2
[AmbulanceTeamB]

(b)

Figure 8: TOP RoboCupRescue scenario a: Organization hierarchy; b: Plan hierarchy.

376

fiHybrid BDI-POMDP Framework Multiagent Teaming

3. Role-based Multiagent Team Decision Problem
Multiagent Team Decision Problem (MTDP) (Pynadath & Tambe, 2002) inspired
economic theory teams (Marschak & Radner, 1972; Ho, 1980; Yoshikawa, 1978).
order quantitative analysis key coordination decisions multiagent teams,
extend MTDP analysis coordination actions interest. example,
COM-MTDP (Pynadath & Tambe, 2002) extension MTDP analysis communication. article, illustrate general methodology analysis aspects
coordination present RMTDP model quantitative analysis role allocation
reallocation concrete example. contrast BDI systems introduced previous section, RMTDP enables explicit quantitative optimization team performance. Note
that, use MTDP, possible distributed POMDP models could potentially also
serve basis (Bernstein et al., 2000; Xuan et al., 2001).
3.1 Multiagent Team Decision Problem
Given team n agents, MTDP (Pynadath & Tambe, 2002) defined tuple:
hS, A, P, , O, Ri. consists finite set states = 1 j ,
1 j m, feature world state. agent perform action
set actions Ai , 1in Ai = A. P (s, < a1 , . . . , >, ) gives probability
transitioning state state given agents perform actions < a1 , . . . , >
jointly. agent receives observation (1in = ) based function
O(s, < a1 , . . . , >, 1 , . . . , n ), gives probability agents receive
observations, 1 , . . . , n given world state perform < a1 , . . . , >
jointly. agents receive single joint reward R(s, < a1 , . . . , >) based state
joint action < a1 , . . . , >. joint reward shared equally members
private reward individual agents receive actions. Thus,
agents motivated behave team, taking actions jointly yield
maximum expected reward.
agent MTDP chooses actions based local policy, ,
mapping observation history actions. Thus, time t, agent perform action
(i0 , . . . , ). contrasts single-agent POMDP, index agents
policy belief state probability distribution world state (Kaelbling, Littman,
& Cassandra, 1998), shown sufficient statistic order compute
optimal policy (Sondik, 1971). Unfortunately, cannot directly use single-agent POMDP
techniques (Kaelbling et al., 1998) maintaining updating belief states (Kaelbling et al.,
1998) MTDP unlike single agent POMDP, MTDP, agents observation
depends actions, also unknown actions agents. Thus,
distributed POMDP models (Bernstein et al., 2000; Xuan et al., 2001),
MTDP, local policies indexed observation histories. =< 1 , . . . , n > refers
joint policy team agents.
3.2 Extension Explicit Coordination
Beginning MTDP, next step methodology make explicit separation
domain-level actions coordination actions interest. Earlier work intro377

fiNair & Tambe

duced COM-MTDP model (Pynadath & Tambe, 2002), coordination action
fixed communication action, got separated out. However, coordination actions could also separated domain-level actions order investigate
impact. Thus, investigate role allocation reallocations, actions allocating agents
roles reallocate roles separated out. end, define RMTDP
(Role-based Multiagent Team Decision Problem) tuple hS, A, P, , O, R, RLi
new component, RL. particular, RL = {r1 , . . . , rs } set roles agents
undertake. instance role rj may assigned agent fulfill it.
actions agent distinguishable two types:
Role-Taking actions: = {irj } contains role-taking actions agent i. irj
means agent takes role rj RL.

Role-Execution Actions: = rj RL irj contains execution actions agent
irj set agent actions executing role rj RL
addition define set states = 1 roles, feature roles (a vector) gives current role agent taken on. reason
introducing new feature assist us mapping BDI team plan
RMTDP. Thus time agent performs new role-taking action successfully, value
feature roles updated reflect change. key
model agents initial role-taking action also subsequent role reallocation. Modeling
allocation reallocation important accurate analysis BDI teams. Note
agent observe part feature pertaining current role
may observe parts pertaining agents roles.
introduction roles allows us represent specialized behaviors associated
role, e.g. transport vs. scout role. filling particular role, rj , agent
perform role-execution actions, irj , may different roleexecution actions irl role rl . Thus, feature roles used filter actions
role-execution actions correspond agents current role permitted.
worst case, filtering affect computational complexity (see Theorem 1
below) practice, significantly improve performance trying find
optimal policy team, since number domain actions agent choose
restricted role agent taken on. Also, different roles
produce varied effects world state (modeled via transition probabilities, P )
teams reward. Thus, policies must ensure agents role capabilities
benefit team most.
MTDP, agent chooses action perform indexing local policy
observation history. epoch agents could role-taking
actions others role-execution actions. Thus, agents local policy
divided local role-taking role-execution policies observation
histories, i0 , . . . , , either (i0 , . . . , ) = null (i0 , . . . , ) = null. =<
1 , . . . , n > refers joint role-taking policy team agents =<
1 , . . . , n > refers joint role-execution policy.
378

fiHybrid BDI-POMDP Framework Multiagent Teaming

article explicitly model communicative actions special action.
Thus communication treated like role-execution action communication
received agents treated observations.2
3.3 Complexity Results RMTDP
Section 2.2 qualitatively emphasized difficulty role allocation, RMTDP helps
us understanding complexity precisely. goal RMTDP come
joint policies maximize total expected reward finite horizon
. Note agents change roles according local role-taking policies.
agents role-execution policy subsequent change would contain actions pertaining
new role. following theorem illustrates complexity finding optimal joint
policies.
Theorem 1 decision problem determining exist policies, ,
RMTDP, yield expected reward least K finite horizon NEXPcomplete.
Proof sketch: Proof follows reduction MTDP (Pynadath & Tambe, 2002)
to/from RMTDP. reduce MTDP RMTDP, set RMTDPs role taking actions, ,
null set RMTDPs role-execution actions, , MTDPs set actions, A.
reduce RMTDP
MTDP, generate new MTDP set actions,

equal . Finding required policy MTDP NEXP-complete (Pynadath &
Tambe, 2002).
theorem shows us, solving RMTDP optimal joint role-taking roleexecution policies even finite horizon highly intractable. Hence, focus
complexity determining optimal role-taking policy, given fixed role-execution
policy. fixed role-execution policy, mean action selection agent
predetermined role executing.
Theorem 2 decision problem determining exists role-taking policy, ,
RMTDP, yields expected reward least K together fixed role-execution
policy , finite horizon NEXP-complete.
Proof sketch: reduce MTDP RMTDP different role-taking
role-execution action corresponding action MTDP. Hence, RMTDP
role-taking action irj agent take role rj created action aj Ai
MTDP role rj contains single role-execution action, i.e. |irj | = 1.
RMTDP, construct transition function role-taking action always
succeeds affected state feature roles . role-execution action irj ,
transition probability MTDP action, aj Ai corresponding
last role-taking action irj . fixed role-execution policy simply perform
action, irj , corresponding last successful role-taking action, irj . Thus,
decision problem RMTDP fixed role-execution policy least hard
2. explicit analysis communication please refer work done Pynadath Tambe (2002)
Goldman et al. (2003).

379

fiNair & Tambe

decision problem MTDP. Furthermore, given Theorem 1, conclude
NEXP-Completeness.
result suggests even fixing role-execution policy, solving RMTDP
optimal role-taking policy still intractable. Note Theorem 2 refers completely
general globally optimal role-taking policy, number agents change roles
point time. Given result, general globally optimal role-taking policy
likely doubly exponential complexity, may left choice run
brute-force policy search, i.e. enumerate role-taking policies evaluate
them, together determinethe run-time finding globally optimal policy.
number policies

||

||T 1
||1

n

, i.e. doubly exponential number observation

histories number agents. Thus, RMTDP enables quantitative evaluation
teams policies, computing optimal policies intractable; furthermore, given low level
abstraction, contrast TOP, difficult human understand optimal policy.
contrast RMTDP TOP root hybrid model described
following section.

4. Hybrid BDI-POMDP Approach
explained TOP RMTDP, present detailed view
hybrid methodology quantitatively evaluate TOP. first provide detailed
interpretation Figure 1. BDI team plans essentially TOP plans, BDI
interpreter TOP coordination layer. shown Figure 1, RMTDP model
constructed corresponding domain TOP interpreter converted
corresponding (incomplete) RMTDP policy. analyze TOP using
analysis techniques rely evaluating RMTDP policy using RMTDP model
domain.
Thus, hybrid approach combines strengths TOPs (enabling humans
specify TOPs coordinate large-scale teams) strengths RMTDP (enabling
quantitative evaluation different role allocations). one hand, synergistic
interaction enables RMTDPs improve performance TOP-based BDI teams.
hand, identified least six specific ways TOPs make easier
build RMTDPs efficiently search RMTDP policies: two discussed
section, four next section. particular, six ways are:
1. TOPs exploited constructing RMTDP models domain (Section 4.1);
2. TOPs exploited present incomplete policies RMTDPs, restricting RMTDP
policy search (Section 5.1);
3. TOP belief representation exploited enabling faster RMTDP policy evaluation
(Section 4.2);
4. TOP organization hierarchy exploited hierarchically grouping RMTDP policies
(Section 5.1);
5. TOP plan hierarchy exploited decomposing RMTDPs (Section 5.3);
380

fiHybrid BDI-POMDP Framework Multiagent Teaming

6. TOP plan hierarchies also exploited cutting observation belief
histories RMTDPs (Section 5.3).
end result efficient policy search completed RMTDP policy improves
TOP performance. exploit TOP framework, frameworks tasking
teams, e.g. Decker Lesser (1993) Stone Veloso (1999) could benefit
similar synergistic interaction.
4.1 Guidelines Constructing RMTDP
shown Figure 1, analysis approach uses input RMTDP model domain,
well incomplete RMTDP policy. Fortunately, TOP serve
direct mapping RMTDP policy, also utilized actually constructing
RMTDP model domain. particular, TOP used determine
domain features important model. addition, structure TOP
exploited decomposing construction RMTDP.
elements RMTDP tuple, hS, A, P, , O, R, RLi, defined using procedure relies TOP well underlying domain. procedure
automated, key contribution recognizing exploitation TOP structures
constructing RMTDP model. First, order determine set states, S,
critical model variables tested pre-conditions, termination conditions
context components (i.e. sub-plans) TOP. Note state needs
model features tested TOP; TOP pre-condition expresses complex test
feature, test modeled state, instead gets used defining
incomplete policy input RMTDP. Next define set roles, RL, leaf-level
roles organization hierarchy TOP. Furthermore, specified Section 3.2,
define state feature roles vector containing current role agent.
defined RL roles , define actions, follows. role rj RL,
define corresponding role-taking action, irj succeed fail depending
agent performs action state action performed in.
role-execution actions, irj agent role rj , allowed role according
TOP.
Thus, defined S, RL based TOP. illustrate steps, consider
plans Figure 4(b). pre-conditions leaf-level plan ScoutRoute1 (See
Appendix A), instance, tests start location helicopters start location X,
termination conditions test scouts end location Y. Thus, locations
helicopters modeled features set states RMTDP. Using
organization hierarchy, define set roles RL role corresponding
four different kinds leaf-level roles, i.e. RL = {memberSctT eamA, memberSctT eamB,
memberSctT eamC, memberT ransportT eam}. role-taking role-execution actions
defined follows:
role-taking action defined corresponding four roles RL, i.e.
becoming member one three scouting teams transport team.
domain specifies transport change scout thus role-taking
action, jointTransportTeam, fail agent i, current role agent scout.
381

fiNair & Tambe

Role-execution actions obtained TOP plans corresponding agents
role. mission rehearsal scenario, agent, fulfilling scout role (members
SctTeamA, SctTeamB SctTeamC), always goes forward, making current
position safe, reaches destination execution action
consider move-making-safe. agent transport role (members Transport
Team) waits X obtains observation signal one scouting sub-team
reached hence role-execution actions wait move-forward.
must define , P, O, R. obtain set observations agent
directly domain. instance, transport helos may observe status scout
helos (normal destroyed), well signal path safe. Finally, determining
functions, P, O, R requires combination human domain expertise empirical
data domain behavior. However, shown later Section 6, even approximate
model transitional observational uncertainty sufficient deliver significant benefits. Defining reward transition function may sometimes require additional state
variables modeled, implicitly modeled TOP. mission
rehearsal domain, time scouting transport mission completed
determined amount reward. Thus, time implicitly modeled TOP
needed explicitly modeled RMTDP.
Since interested analyzing particular TOP respect uncertainty,
procedure constructing RMTDP model simplified exploiting hierarchical decomposition TOP order decompose construction RMTDP
model. high-level components TOP often represent plans executed different
sub-teams, may loosely interact other. Within component,
sub-team members may exhibit tight interaction, focus loose coupling
across components, end results one component feed another,
components independently contribute team goal. Thus, procedure constructing RMTDP exploits loose coupling components plan hierarchy
order build RMTDP model represented combination smaller RMTDPs (factors). Note decomposition infeasible, approach still applies except
benefits hierarchical decomposition unavailable.
classify sibling components either parallel sequentially executed (contains temporal constraint). Components executed parallel could either independent
dependent. independent components, define RMTDPs
components sub-team executing one component cannot affect transitions, observations reward obtained sub-teams executing components. procedure determining elements RMTDP tuple component k,
hSk , Ak , Pk , k , Ok , Rk , RLk i, identical procedure described earlier constructing
overall RMTDP. However, component smaller set relevant variables
roles hence specifying elements corresponding RMTDP easier.
combine RMTDPs independent components obtain
RMTDP corresponding higher-level component. higher level component l,
whose child
components independent, set states, Sl = x FSl x
FSl = k s.t. Child(k,l)=true FSk FSl FSk sets features set
states Sl set states Sk . state sl Sl said correspond state
sk Sk x FSk , sl [x ] = sk [x ], i.e. state sl value state sk
382

fiHybrid BDI-POMDP Framework Multiagent Teaming


defined follows, Pl (sl , al , sl ) =
Q features state sk . transition function

k s.t. Child(k,l)=true Pk (sk , ak , sk ), sl sl component l corresponds states
sk sk component k ak joint action performed sub-team assigned component k corresponding joint action al performed sub-team
assigned
component l. observation function defined similarly Ol (sl , al , l ) =
Q
Ok (sk , ak , k ). reward function component l defined
k s.t. Child(k,l)=true
P
Rl (sl , al ) = k s.t. Child(k,l)=true Rk (sk , ak ).

case sequentially executed components (those connected temporal constraint), components loosely coupled since end states preceding component
specify start states succeeding component. Thus, since one component
active time, transition function defined follows, Pl (sl , al , sl ) = Pk (sk , ak , sk ),
component k active child component, sk sk represent states
component k corresponding states sl sl component l ak joint action
performed sub-team assigned component k corresponding joint action
al performed sub-team corresponding component l. Similarly, define
Ol (sl , al , l ) = Ok (sk , ak , k ) Rl (sl , al ) = Rk (sk , ak ), k active child
component.

Consider following example mission rehearsal domain components
exhibit sequential dependence parallel independence. Concretely, component
DoScouting executed first followed DoTransport RemainingScouts,
parallel independent hence, either DoScouting active DoTransport
RemainingScouts active point execution. Hence, transition, observation reward functions parent Execute Mission given corresponding
functions either DoScouting combination corresponding functions
DoTransport RemainingScouts.
use top-down approach order determine construct factored RMTDP
plan hierarchy. shown Algorithm 1, replace particular sub-plan
constituent sub-plans either independent sequentially executed. not,
RMTDP defined using particular sub-plan. process applied recursively
starting root component plan hierarchy. concrete example, consider
mission rehearsal simulation domain hierarchy illustrated Figure 4(b).
Given temporal constraints DoScouting DoTransport, DoScouting RemainingScouts, exploited sequential decomposition, DoTransport
RemainingScouts parallel independent components. Hence, replace
ExecuteMission DoScouting, DoTransport RemainingScouts. apply process DoScouting. constituent components DoScouting
neither independent sequentially executed thus DoScouting cannot replaced
constituent components. Thus, RMTDP mission rehearsal domain comprised
smaller RMTDPs DoScouting, DoTransport RemainingScouts.
Thus, using TOP identify relevant variables building factored RMTDP
utilizing structure TOP decompose construction procedure, reduce load
domain expert model construction. Furthermore, shown Section 5.3,
factored model greatly improves performance search best role allocation.
383

fiNair & Tambe

Algorithm 1 Build-RMTDP(TOP top, Sub-plan subplan)
1: children subplanchildren() {subplanchildren() returns sub-plans within subplan}
2: children = null children (loosely coupled independent)
3:
rmtdp Define-RMTDP(subplan) {not automated}
4:
return rmtdp
5: else
6:
child children
7:
factors[child] Build-RMTDP(top,child)
8:
rmtdp ConstructFromFactors(factors)
9:
return rmtdp

4.2 Exploiting TOP Beliefs Evaluation RMTDP Policies
present technique exploiting TOPs speeding evaluation RMTDP
policies. explain improvement, first describe original algorithm
determining expected reward joint policy, local policies agent
indexed entire observation histories (Pynadath & Tambe, 2002; Nair, Pynadath,
Yokoo, Tambe, & Marsella, 2003a). Here, obtain RMTDP policy TOP
follows. obtain (~
), i.e. action performed agent observation history

~i , action performed agent following TOP set privately
held beliefs corresponding observation history, ~it . compute expected reward
RMTDP policy projecting teams execution possible branches
different world states different observations. time step, compute
expected value joint policy, =< 1 , . . . , n >, team starting given state, st ,
given set past observations,
~ 1t , . . . ,
~ nt , follows:
X


ff
ff





ff

~ nt ) = R(st , 1 (~
Vt (st , ~1t , . . . ,
1t ), . . . , n (~nt ) ) +
P , 1
~ 1t , . . . , n
~ nt , st+1
st+1

X



ff

ff


ff
st+1, 1
~ 1t , . . . , n
~ nt , 1t+1, . . . , nt+1 Vt+1 st+1 , ~1t+1 , . . . ,
~ nt+1

(1)

t+1

expected reward joint policy given V0 (s0 , < null, . . . , null >) s0
start state. time step t, computation Vt performs summation
possible world states agent observations time complexity (|S| ||).
computation
repeated states observation histories length t, i.e.

|S| ||t times. Therefore,
given time horizon , overall complexity algo
2

+1
rithm |S| ||
.
discussed Section 2.2, team-oriented program, agents action selection
based currently held private beliefs (note mutual beliefs modeled
privately held beliefs agents per footnote 2). similar technique
exploited mapping TOP RMTDP policy. Indeed, evaluation RMTDP
policy corresponds TOP speeded agents local policy indexed
private beliefs, . refer , TOP-congruent belief state agent
384

fiHybrid BDI-POMDP Framework Multiagent Teaming

RMTDP. Note belief state probability distribution world
states single agent POMDP, rather privately held beliefs (from BDI
program) agent time t. similar idea representing policy
finite-state controller (Hansen & Zhou, 2003; Poupart & Boutilier, 2003). case,
private beliefs would map states finite-state controller.
Belief-based RMTDP policy evaluation leads speedup multiple observation
histories map belief state, . speedup key illustration exploitation
synergistic interactions TOP RMTDP. instance, belief representation techniques used TOP reflected RMTDP, resulting faster policy evaluation
help us optimize TOP performance. detailed example belief state presented later
brief explanation belief-based RMTDP policies evaluated.
evaluation using observation histories, compute expected reward
belief-based policy projecting teams execution possible branches
different world states different observations. time step, compute
expected value joint policy, =< 1 , . . . , n >, team starting given state, st ,
given team belief state, < 1t , . . . , nt > follows:


ff


ff X

ff



Vt (st , 1t . . . nt ) = R(st , 1 (1t ), . . . , n (nt ) ) + P st , 1 1t , . . . , n nt , st+1
st+1

X
ff


ff




ff
st+1, 1 1t , . . . , n nt , 1t+1, . . . , nt+1 Vt+1 st+1 , 1t+1 , . . . , nt+1

t+1

(2)


t+1

= BeliefUpdateFunction



, it+1



complexity computing function (expression 2) (|S| ||) BF , BF
represents complexity belief update function, BeliefUpdateFunction.
time step computation value function done every state possible
reachable belief states. Let |i | = max1tT (|it |) represent maximum number
possible belief states agent point time, |it | number
belief states agent t. Therefore complexity algorithm
given O(|S|2 || (|1 | . . . |n |) ) BF . Note that, algorithm
exponent unlike algorithm expression 1. Thus, evaluation method
give large time savings if: (i) quantity (|1 | . . . |n |) much less ||T
(ii) belief update cost low. practical BDI systems, multiple observation histories
map often onto belief state, thus usually, (|1 | . . . |n |) much less
||T . Furthermore, since belief update function mirrors practical BDI systems,
complexity also low polynomial constant. Indeed, experimental results
show significant speedups result switching TOP-congruent belief states
. However, absolute worst case, belief update function may simply append
new observation history past observations (i.e., TOP-congruent beliefs
equivalent keeping entire observation histories) thus belief-based evaluation
complexity observation history-based evaluation.
turn example belief-based policy evaluation mission rehearsal
domain. time step, transport helicopters may receive observation
385

fiNair & Tambe

whether scout failed based observation function. use observationhistory representation policy, transport agent would maintain complete
history observations could receive time step. example, setting
two scout helicopters, one route 1 route 2, particular transport
helicopter may several different observation histories length two. every time step,
transports may receive observation scout alive failed.
Thus, time = 2, transport helicopter might one following observation histories length two, < {sct1OnRoute1Alive, sct2OnRoute2Alive}1 , {sct1OnRoute1F ailed,
sct2OnRoute2F ailed}2 >, < {sct1OnRoute1Alive, sct2OnRoute2F ailed}1 , {sct1OnRoute1
F ailed}2 >, < {sct1OnRoute1F ailed, sct2OnRoute2Alive}1 , {sct2OnRoute2F ailed}2 >,
etc. However, action selection transport helicopters depends whether
critical failure (i.e. last remaining scout crashed) taken place change
role. Whether failure critical determined passing observation
belief-update function. exact order observations received
precise times failure non-failure observations received relevant
determining critical failure taken place consequently whether transport
change role scout. Thus, many observation histories map onto
belief states. example, three observation histories map belief
CriticalF ailure(DoScouting) i.e. critical failure taken place. results significant speedups using belief-based evaluation, Equation 2 needs executed
smaller number belief states, linear domains, opposed observation
history-based evaluation, Equation 1 executed exponential number observation histories (||T ). actual speedup obtained mission rehearsal domain
demonstrated empirically Section 6.

5. Optimizing Role Allocation
Section 4 focused mapping domain interest onto RMTDP algorithms
policy evaluation, section focuses efficient techniques RMTDP policy search,
service improving BDI/TOP team plans. TOP essence provides incomplete,
fixed policy, policy search optimizes decisions left open incomplete policy;
policy thus completed optimizes original TOP (see Figure 1). enabling RMTDP
focus search incomplete policies, providing ready-made decompositions,
TOPs assist RMTDPs quickly searching policy space, illustrated
section. focus, particular, problem role allocation (Hunsberger & Grosz,
2000; Modi, Shen, Tambe, & Yokoo, 2003; Tidhar et al., 1996; Fatima & Wooldridge, 2001),
critical problem teams. TOP provides incomplete policy, keeping open
role allocation decision agent, RMTDP policy search provides optimal
role-taking action role allocation decision points. contrast previous
role allocation approaches, approach determines best role allocation, taking
consideration uncertainty domain future costs. Although demonstrated
solving role allocation problem, methodology general enough apply
coordination decisions.
386

fiHybrid BDI-POMDP Framework Multiagent Teaming

5.1 Hierarchical Grouping RMTDP Policies
mentioned earlier, address role allocation, TOP provides policy complete,
except role allocation decisions. RMTDP policy search optimally fills
role allocation decisions. understand RMTDP policy search, useful gain
understanding role allocation search space. First, note role allocation focuses
deciding many types agents allocate different roles organization
hierarchy. role allocation decision may made time = 0 may made
later time conditioned available observations. Figure 9 shows partially expanded role
allocation space defined TOP organization hierarchy Figure 4(a) six helicopters.
node role allocation space completely specifies allocation agents roles
corresponding level organization hierarchy (ignore now, number
right node). instance, root node role allocation space specifies
six helicopters assigned Task Force (level one) organization hierarchy
leftmost leaf node (at level three) Figure 9 specifies one helicopter assigned
SctTeamA, zero SctTeamB, zero SctTeamC five helicopters Transport Team.
Thus, see, leaf node role allocation space complete, valid role
allocation agents roles organization hierarchy.
order determine one leaf node (role allocation) superior another evaluate
using RMTDP constructing RMTDP policy each. particular
example, role allocation specified leaf node corresponds role-taking actions
agent execute time = 0. example, case leftmost leaf
Figure 9, time = 0, one agent (recall Section 2.2 homogeneous team
hence specific agent matter) become member SctTeamA
agents become members Transport Team. Thus, one agent i, roletaking policy include (null) = joinSctT eamA agents, j, j 6= i,
include j (null) = joinT ransportT eam. case, assume rest
role-taking policy, i.e. roles reallocated scout fails, obtained role
reallocation algorithm BDI/TOP interpreter, STEAM algorithm (Tambe
et al., 2000). Thus example, role reallocation indeed performed STEAM
algorithm, STEAMs reallocation policy included incomplete policy
RMTDP initially provided. Thus, best role allocation computed keeping
mind STEAMs reallocation policy. STEAM, given failure agent playing RoleF ,
agent playing RoleR replace if:
Criticality (RoleF ) Criticality (RoleR ) > 0
Criticality (x) = 1 x critical; = 0 otherwise

Thus, based agents observations, critical failure taken place,
replacing agents decision replace computed using expression
included incomplete policy input RMTDP. Since incomplete
policy completed role allocation leaf node using technique above,
able construct policy RMTDP corresponds role allocation.
domains like RoboCupRescue, allocation decisions made time
= 0. domains, possible role allocation conditioned observations
(or communication) obtained course execution. instance,
shown Figure 8(a), RoboCupRescue scenario, ambulances allocated
sub-team AmbulanceTeamA AmbulanceTeamB information location
387

fiNair & Tambe

6

0

6

[0]
6

1

6

[4167]
5

2

6

[3420]
4

3

6

[2773]
3

4

6

[1926]
2

5

6

[1179]
1

6

6

[432]
0

6 1359.57
6 2926.08
6 1500.12
6 613.81
2 4
2 4
1 5
1 5
1 1 0
0 0 2
0 0 1
1 0 0

Figure 9: Partially expanded role allocation space mission rehearsal domain(six helos).

civilians conveyed fire engines. allocation ambulances
conditioned communication, i.e. number civilians location.
Figure 10 shows partially expanded role allocation scaled-down rescue scenario
three civilians, two ambulances two fire engines (one station 1
station 2). Figure, 1;1;2 depicts fact two ambulances,
one fire engine station. shown, level allocation fire engines
EngineTeamA EngineTeamB gives number engines assigned
EngineTeam station. next level (leaf level) different leaf nodes
possible assignment ambulances AmbulanceTeamA AmbulanceTeamB depending
upon value communication c. Since three civilians exclude
case civilians present particular fire, two possible messages i.e.
one civilian fire 1 two civilians fire 1 (c = 1 2).
TaskForce=1;1;2
1;1;2

1;1;2

EngineTeamA=0;1 EngineTeamB=1;0 AmbTeam=2

c=1

EngineTeamA=1;0 EngineTeamB=0;1 AmbTeam=2

1;1;2

1;1;2

0;1 1;0 2

0;1 1;0 2

c=2

c=1

AmbTeamA=2 AmbTeamB=0 AmbTeamA=1 AmbTeamB=1

c=2

AmbTeamA=1 AmbTeamB=1 AmbTeamA=1 AmbTeamB=1

Figure 10: Partially expanded role allocation space Rescue domain (one fire engine
station 1, one fire engine station 2, two ambulances, three civilians).
thus able exploit TOP organization hierarchy create hierarchical
grouping RMTDP policies. particular, leaf node represents complete
RMTDP policy (with role allocation specified leaf node), parent node
represents group policies. Evaluating policy specified leaf node equivalent
evaluating specific role allocation taking future uncertainties account. could
388

fiHybrid BDI-POMDP Framework Multiagent Teaming

brute force search role allocations, evaluating order determine
best role allocation. However, number possible role allocations exponential
leaf roles organization hierarchy. Thus, must prune search space.
5.2 Pruning Role Allocation Space
prune space valid role allocations using upper bounds (MaxEstimates)
parents leaves role allocation space admissible heuristics (Section 5.3).
leaf role allocation space represents completely specified policy MaxEstimate upper bound maximum value policies parent node
evaluated using RMTDP. obtain MaxEstimates parent nodes (shown
brackets right parent node Figure 9), use branch-and-bound style
pruning (see Algorithm 2). discuss Algorithm 2 below, note essence
performs branch-and-bound style pruning; key novelty step 2 discuss
Section 5.3.
branch-and-bound algorithm works follows: First, sort parent nodes
estimates start evaluating children parent highest MaxEstimate (Algorithm 2: steps 3-13). Evaluate(RMTDP, child) refers evaluation
leaf-level policy, child, using RMTDP model. evaluation leaf-level policies (step
13) done using either methods described Section 4. case
role allocation space Figure 9, would start evaluating leaves parent
node one helicopter Scouting Team five Transport Team. value
evaluating leaf node shown right leaf node. obtained
value best leaf node (Algorithm 2: steps 14,15), case 1500.12, compare
MaxEstimates parents role allocation space (Algorithm 2:
steps 16-18). see Figure 9 would result pruning three parent nodes
(leftmost parent right two parents) avoid evaluation 65 84 leaf-level
policies. Next, would proceed evaluate leaf nodes parent
two helos Scouting Team four Transport Team. would result pruning
remaining unexpanded parent nodes return leaf highest value,
case node corresponding two helos allocated SctTeamA four
Transport Team. Although demonstrated 3-level hierarchy, methodology
applying deeper hierarchies straightforward.
5.3 Exploiting TOP Calculate Upper Bounds Parents
discuss upper bounds parents, called MaxEstimates, calculated parent. MaxEstimate parent defined strict upper bound
maximum expected reward leaf nodes it. necessary
MaxEstimate upper bound else might end pruning potentially useful role
allocations. order calculate MaxEstimate parent could evaluate
leaf nodes using RMTDP, would nullify benefit subsequent pruning. We, therefore, turn TOP plan hierarchy (see Figure 4(b)) break
evaluation parent node components, evaluated separately thus
decomposing problem. words, approach exploits structure BDI
program construct small-scale RMTDPs unlike decomposition techniques
389

fiNair & Tambe

Algorithm 2 Branch-and-bound algorithm policy search.
1: Parents list parent nodes
2: Compute MAXEXP(Parents) {Algorithm 3}
3: Sort Parents decreasing order MAXEXP
4: bestVal
5: parent Parents
6:
done[parent] false; pruned[parent] false
7: parent Parents
8:
done[parent] = false pruned[parent] = false
9:
child parentnextChild() {child leaf-level policy parent}
10:
child = null
11:
done[parent] true
12:
else
13:
childVal Evaluate(RMTDP,child)
14:
childVal > bestVal
15:
bestVal childVal;best child
16:
parent1 Parents
17:
MAXEXP[parent1] < bestVal
18:
pruned[parent1] true
19: return best

assume decomposition ultimately rely domain experts identify interactions
agents reward transition functions (Dean & Lin, 1995; Guestrin, Venkataraman,
& Koller, 2002).
parent role allocation space, use small-scale RMTDPs evaluate values TOP component. Fortunately, discussed Section 4.1,
exploited small-scale RMTDPs corresponding TOP components constructing larger
scale RMTDPs. put small-scale RMTDPs use again, evaluating policies within
component obtain upper bounds. Note like evaluation leaf-level
policies, evaluation components parent node done using either
observation histories (see Equation 1) belief states (see Equation 2). describe
section using observation history-based evaluation method computing values
components parent, summed obtain MaxEstimate (an
upper bound childrens values). Thus, whereas parent role allocation space
represents group policies, TOP components (sub-plans) allow component-wise
evaluation group obtain upper bound expected reward policy
within group.
Algorithm 3 exploits smaller-scale RMTDP components, discussed Section 4.1,
obtain upper bounds parents. First, order evaluate MaxEstimate
parent node role allocation space, identify start states component
evaluate RMTDPs. explain step using parent node Figure 9
Scouting Team = two helos, Transport Team = four helos (see Figure 11). first
component preceding components, start states corresponds
start states policy TOP mapped onto. next
390

fiHybrid BDI-POMDP Framework Multiagent Teaming

components next component one linked sequential dependence
start states end states preceding component. However, explained later
section, significantly reduce list start states component
evaluated.
Algorithm 3 MAXEXP method calculating upper bounds parents role allocation space.
1: parent search space
2:
MAXEXP[parent] 0
3:
component corresponding factors RMTDP Section 4.1
4:
component preceding component j
5:
Obtain start states, states[i] endStates[j]
6:
states[i] removeIrrelevantFeatures(states[i]) {discard features present
Si }
7:
Obtain corresponding observation histories start OHistories[i]
endOHistories[j]
8:
OHistories[i] removeIrrelevantObservations(OHistories[i])
9:
else
10:
Obtain start states, states[i]
11:
Observation histories start OHistories[i] null
12:
maxEval[i] 0
13:
leaf-level policies parent
14:
maxEval[i] max(maxEval[i], maxsi states[i],ohi OHistories[i](Evaluate(RM DPi ,
si , ohi , )))
+
15:
MAXEXP[parent] maxEval[i]
Similarly, starting observation histories component observation histories completing preceding component (no observation history first
component). BDI plans normally refer entire observation histories rely
key beliefs typically referred pre-conditions component.
starting observation history shortened include relevant observations,
thus obtaining reduced list starting observation sequences. Divergence private observations problematic, e.g. cause agents trigger different team plans.
indicated earlier Section 2.2, TOP interpreters guarantee coherence
key aspects observation histories. instance, discussed earlier, TOP interpreter
ensures coherence key beliefs initiating terminating team plans TOP; thus
avoiding divergence observation histories.
order compute maximum value particular component, evaluate
possible leaf-level policies within component possible start states observation histories obtain maximum (Algorithm 3:steps 13-14). evaluation,
store end states ending observation histories used
evaluation subsequent components. shown Figure 11, evaluation
DoScouting component parent node two helicopters assigned
Scouting Team four helos Transport Team, leaf-level policies correspond
possible ways helicopters could assigned teams SctTeamA, SctTeamB, Sct391

fiNair & Tambe

TeamC Transport Team, e.g. one helo SctTeamB, one helo SctTeamC four
helos Transport Team, two helos SctTeamA four helos Transport Team, etc.
role allocation tells agents role take first step. remainder
role-taking policy specified role replacement policy TOP infrastructure
role-execution policy specified DoScouting component TOP.
obtain MaxEstimate parent node role allocation space, simply
sum maximum values obtained component (Algorithm 3:steps 15), e.g.
maximum values component (see right component Figure 11)
summed obtain MaxEstimate (84 + 3330 + 36 = 3420). seen Figure 9, third
node left indeed upper bound 3420.
calculation MaxEstimate parent nodes much faster
evaluating leaf nodes cases two reasons. Firstly, parent nodes
evaluated component-wise. Thus, multiple leaf-level policies within one component result
end state, remove duplicates get start states next component. Since component contains state features relevant it, number
duplicates greatly increased. duplication evaluation effort cannot avoided
leaf nodes, policy evaluated independently start finish. instance, DoScouting component, role allocations, SctTeamA=1, SctTeamB=1,
SctTeamC=0, TransportTeam=4 SctTeamA=1, SctTeamB=0, SctTeamC=1, TransportTeam=4, end states common eliminating irrelevant features
scout SctTeamB former allocation scout SctTeamC latter allocation fail. feature elimination (Algorithm 3:steps 6),
state features retained DoTransport scouted route number transports
(some transports may replaced failed scouts) shown Figure 11.
second reason computation MaxEstimates parents much faster
number starting observation sequences much less number ending observation histories preceding components. observations
observation histories component relevant succeeding components (Algorithm 3:steps 8). Thus, function removeIrrelevantObservations reduces number
starting observation histories observation histories preceding component.
refer methodology obtaining MaxEstimates parent MAXEXP. variation this, maximum expected reward failures (NOFAIL),
obtained similar fashion except assume probability agent failing 0. able make assumption evaluating parent node, since
focus obtaining upper bounds parents, obtaining exact value.
result less branching hence evaluation component proceed much
quicker. NOFAIL heuristic works evaluation policy without failures
occurring higher evaluation policy failures possible.
normally case domains. evaluation NOFAIL heuristics
role allocation space six helicopters shown square brackets Figure 9.
following theorem shows MAXEXP method finding upper bounds
indeed finds upper bound thus yields admissible search heuristic branchand-bound search role allocation space.
Theorem 3 MAXEXP method always yield upper bound.
392

fiHybrid BDI-POMDP Framework Multiagent Teaming

[84]
DoScouting
[ScoutingTeam=2,TransportTeam=4]

Alloc:
SctTeamA=2
SctTeamB=0
SctTeamC=0
TransportTeam=4

Alloc:
SctTeamA=0
SctTeamB=1
SctTeamC=1
TransportTeam=4

[3300]
DoTransport
[TransportTeam=4]

StartState:
RouteScouted=1
Transports=4

[36]
RemainingScouts
[ScoutTeam=2]

StartState:
RouteScouted=1
Transports=3

StartState:
RouteScouted=1
Transports=0

Figure 11: Component-wise decomposition parent exploiting TOP.

Proof: See Appendix C.
Theorem 3, conclude branch-and-bound policy search algorithm
always find best role allocation, since MaxEstimates parents true
upper bounds. Also, help Theorem 4, show worst case,
branch-and-bound policy search complexity brute force search.
Theorem 4 Worst-case complexity evaluating single parent node using MAXEXP
evaluating every leaf node within constant factor.
Proof sketch:
worst case complexity MAXEXP arises when:
1. Let ESj end states component j executing policy removing
features irrelevant succeeding component k. Similarly, let ESj
end states component j executing policy
Tremoving features
irrelevant succeeding component k. ESj ESj = null
duplication end states occur.
2. Let OHj ending observation histories component j executing policy
removing observations irrelevant succeeding component
k. Similarly, let OHj ending observation histories component j executing policy removing observation
histories irrelevant

succeeding component k. OHj OHj = null duplication
observation histories occur. Note belief-based evaluation used
would replace observation histories TOP congruent belief states
(see Sect 4).
case, computational advantage evaluating components
MaxEstimate separately. Thus, equivalent evaluating child node
parent. Thus, worst case, MAXEXP computation parent
evaluating children within constant factor.
addition, worst case, pruning result using MAXEXP every
leaf node need evaluated. equivalent evaluating leaf node twice.
393

fiNair & Tambe

Thus, worst case complexity branch-and-bound search using MAXEXP
finding best role allocation evaluating every leaf node. refer
brute-force approach NOPRUNE. Thus, worst case complexity MAXEXP
NOPRUNE. However, owing pruning savings decomposition computation MaxEstimates, significant savings likely average
case. Section 6 highlights savings mission rehearsal RoboCupRescue
domains.

6. Experimental Results
section presents four sets results context two domains introduced
Section 2.1, viz. mission rehearsal RoboCupRescue (Kitano et al., 1999). First,
investigated empirically speedups result using TOP-congruent belief
states (belief-based evaluation) observation history-based evaluation using
algorithm Section 5 brute-force search. focus determining
best assignment agents roles; assume fixed TOP TOP infrastructure.
Second, conducted experiments investigate benefits considering uncertainty
determining role allocations. this, compared allocations found RMTDP
role allocation algorithm (i) allocations consider kind uncertainty,
(ii) allocations consider observational uncertainty consider action
uncertainty. Third, conducted experiments domains determine sensitivity
results changes model. Fourth, compare performance allocations
found RMTDP role allocation algorithm allocations human subjects
complex domains RoboCupRescue simulations.
6.1 Results Mission Rehearsal Domain
mission rehearsal domain, TOP one discussed Section 2.2.
seen Figure 4(a), organization hierarchy requires determining number agents
allocated three scouting sub-teams remaining helos must allocated
transport sub-team. Different numbers initial helicopters attempted, varying
three ten. details RMTDP constructed domain given
Appendix B. probability failure scout time step routes 1, 2 3
0.1, 0.15 0.2, respectively. probability transport observing alive scout
routes 1, 2 3 0.95, 0.94 0.93, respectively. False positives possible,
i.e. transport observe scout alive failed. probability
transport observing scout failure routes 1, 2 3 0.98, 0.97 0.96, respectively.
too, false positives possible hence transport observe failure
unless actually taken place.
Figure 12 shows results comparing different methods searching role
allocation space. show four methods. method adds new speedup techniques
previous:
1. NOPRUNE-OBS: brute force evaluation every role allocation determine
best. Here, agent maintains complete observation history evaluation
algorithm Equation 1 used. ten agents, RMTDP projected
394

fiHybrid BDI-POMDP Framework Multiagent Teaming

order 10,000 reachable states order 100,000 observation histories
per role allocation evaluated (thus largest experiment category limited
seven agents).
2. NOPRUNE-BEL: brute force evaluation every role allocation. difference
method NOPRUNE-OBS use belief-based evaluation
algorithm (see Equation 2).
3. MAXEXP: branch-and-bound search algorithm described Section 5.2
uses upper bounds evaluation parent nodes find best allocation.
Evaluation parent leaf nodes uses belief-based evaluation.
4. NOFAIL: modification branch-and-bound heuristic mentioned Section 5.3.
essence MAXEXP, except upper bounds computed making
assumption agents fail. heuristic correct domains
total expected reward failures always less failures present
give significant speedups agent failures one primary sources
stochasticity. method, too, evaluation parent leaf nodes uses
belief-based evaluation. (Note upper bounds computed using
no-failure assumption changes assumed actual domains.)
Figure 12(a), Y-axis number nodes role allocation space evaluated
(includes leaf nodes well parent nodes), Figure 12(b) Y-axis represents
runtime seconds logarithmic scale. figures, vary number agents
X-axis. Experimental results previous work using distributed POMDPs often
restricted two agents; exploiting hybrid models, able vary number
agents three ten shown Figure 12(a). clearly seen Figure 12(a),
pruning, significant reductions obtained MAXEXP NOFAIL NOPRUNEBEL terms numbers nodes evaluated. reduction grows quadratically
10-fold ten agents.3 NOPRUNE-OBS identical NOPRUNE-BEL terms
number nodes evaluated, since methods leaf-level policies evaluated,
method evaluation differs. important note although NOFAIL
MAXEXP result number nodes evaluated domains,
necessarily true always. general, NOFAIL evaluate least many nodes
MAXEXP since estimate least high MAXEXP estimate. However,
upper bounds computed quicker NOFAIL.
Figure 12(b) shows NOPRUNE-BEL method provides significant speedup
NOPRUNE-OBS actual run-time. instance, 12-fold speedup using
NOPRUNE-BEL instead NOPRUNE-OBS seven agent case (NOPRUNE-OBS
could executed within day problem settings greater seven agents).
empirically demonstrates computational savings possible using belief-based evaluation instead observation history-based evaluation (see Section 4). reason,
use belief-based evaluation MAXEXP NOFAIL approaches also
3. number nodes NOPRUNE eight agents obtained experiments, rest
calculated using formula [m]n /n! = (m + n 1) . . . m/n!, represents number
heterogeneous role types n number homogeneous agents. [m]n = (m + n 1) . . .
referred rising factorial.

395

fiNair & Tambe

remaining experiments paper. MAXEXP heuristic results 16-fold speedup
NOPRUNE-BEL eight agent case.
NOFAIL heuristic quick compute upper bounds far outperforms
MAXEXP heuristic (47-fold speedup MAXEXP ten agents). Speedups
MAXEXP NOFAIL continually increase increasing number agents. speedup
NOFAIL method MAXEXP marked because, domain, ignoring
failures results much less branching.
350

NOFAIL, MAXEXP

Number nodes

300

NOPRUNE-OBS,
NOPRUNE-BEL

250
200
150
100
50
0

3

4

5

6

7

8

9

10

Number agents
100000

MAXEXP
NOFAIL
NOPRUNE-BEL
NOPRUNE-OBS

Time secs (log scale)

10000
1000
100
10
1
0.1
0.01

3

4

5

6

7

8

9

10

Number agents

Figure 12: Performance role allocation space search mission rehearsal domain, a) (left)
Number nodes evaluated, b) (right)Run-time seconds log scale.

Next, conducted experiments illustrating importance RMTDPs reasoning
action observation uncertainties role allocations. this, compared
allocations found RMTDP role allocation algorithm allocations found using two
different methods (see Figure 13):
1. Role allocation via constraint optimization (COP) (Modi et al., 2003; Mailler & Lesser,
2004) allocation approach: COP approach4 , leaf-level sub-teams or4. Modi et al.s work (2003) focused decentralized COP, investigation emphasis
resulting role allocation generated COP, decentralization per se.

396

fiHybrid BDI-POMDP Framework Multiagent Teaming

ganization hierarchy treated variables number helicopters
domain variable (thus, domain may 1, 2, 3,..helicopters).
reward allocating agents sub-teams expressed terms constraints:
Allocating helicopter scout route assigned reward corresponding
routes distance ignoring possibility failure (i.e. ignoring transition
probability). Allocating helicopters subteam obtained proportionally higher reward.
Allocating helicopter transport role assigned large reward transporting cargo destination. Allocating helicopters subteam
obtained proportionally higher reward.
allocating least one scout role assigned reward negative infinity
Exceeding total number agents assigned reward negative infinity
2. RMTDP complete observability: approach, consider transition
probability, ignore partial observability; achieved assuming complete observability RMTDP. MTDP complete observability equivalent
Markov Decision Problem (MDP) (Pynadath & Tambe, 2002) actions
joint actions. We, thus, refer allocation method MDP method.
Figure 13(a) shows comparison RMTDP-based allocation MDP allocation COP allocation increasing number helicopters (X-axis). compare
using expected number transports get destination (Y-axis) metric
comparison since primary objective domain. seen, considering forms uncertainty (RMTDP) performs better considering transition
uncertainty (MDP) turn performs better considering uncertainty (COP).
Figure 13(b) shows actual allocations found three methods four helicopters
six helicopters. case four helicopters (first three bars), RMTDP MDP
identical, two helicopters scouting route 2 two helicopters taking transport role.
COP allocation however consists one scout route 3 three transports.
allocation proves myopic results fewer transports getting destination
safely. case six helicopters, COP chooses one scout helicopter route 3,
shortest route. MDP approach results two scouts route 1,
longest route albeit safest. RMTDP approach, also considers observational
uncertainty chooses additional scout route 2, order take care cases
failures scouts go undetected transports.
noted performance RMTDP-based allocation depend
values elements RMTDP model. However, next experiment
revealed, getting values exactly correct necessary. order test sensitivity
performance allocations actual model values, introduced error
various parameters model see allocations found using incorrect model
would perform original model (without errors). emulates situation
model correctly represent domain. Figure 14 shows expected number
transports reach destination (Y-axis) mission rehearsal scenario six
helicopters error (X-axis) introduced various parameters model. instance,
397

fiNair & Tambe

7

Number transports

6
5

RMTDP
COP
MDP

4
3
2
1
0

4

5

6

7

8

Number agents
7
6 helos

RM

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

Rt3
Rt2
xxx
xxxRt1
xxx
Transports
xxxx
xxxx
xxxx
xxxx
xxxx

xxxx
xxxx
xxxx
xxxx
xxxx

DP

TD

P

0

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxx



1

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

TD
P

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx

RM

2

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx


DP

3

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx

P

4

CO

Number helos

5

P

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

CO

4 helos

6

Figure 13: a) Comparison performance different allocation methods, b)Allocations
found using different allocation methods.

398

fiHybrid BDI-POMDP Framework Multiagent Teaming

percentage error failure rate route 1 (route1-failure-rate) -15%
(i.e. erroneous failure rate 85% actual failure rate) 10%, difference
number transports reached destination (3.498). However
percentage error greater 10%, allocation found conservative resulting
fewer transports getting destination. Similarly, percentage error less
-15%, allocation found risky, scouts assigned, resulting
failures. general, Figure 14 shows model insensitive errors 5 10%
model parameters mission rehearsal domain, model parameters
outside range, non-optimal allocations would result. comparing non-optimal
allocations COP, find always perform better COP range
errors tested (+/-25%) failure rate well observability routes. instance,
error 25% failure rate route 1, RMTDP managed 2.554 transports
safely reach destination, COP managed get 1.997 transports reach safely.
comparing non-optimal allocations MDP, also find performed better
MDP within range +/- 25% error observability routes. Thus,
although allocations found using incorrect model non-optimal performed
better COP MDP large ranges errors model. shows getting
model exactly correct necessary find good allocations. thus able
obtain benefits RMTDP even without insisting accurate model.
4

route1-failure-rate
route-2-failure-rate
route3-failure-rate
route1-observability

Number Transports

3.5
3
2.5
2
1.5
1
0.5
0
-25 -20 -15 -10 -5

0

5

10

15

20

25

Percentage error

Figure 14: Model sensitivity mission rehearsal domain.

6.2 Results RoboCupRescue Domain
6.2.1 Speedups RoboCupRescue Domain
next set experiments, highlight computational savings obtained
RoboCupRescue domain. scenario experiment consisted two fires different
locations city. fires different initially unknown number civilians
it, however total number civilians distribution locations
civilians chosen known ahead time. experiment, fix number
civilians five set distribution used choose civilians locations uniform.
number fire engines set five, located three different fire stations described
399

fiNair & Tambe

Section 2.1 vary number ambulances, co-located ambulance center,
two seven. reason chose change number ambulances
small number fire engines unable extinguish fires, changing problem completely.
goal determine fire engines allocate fire information
civilians transmitted, many ambulances send fire location.
Figure 15 highlights savings terms number nodes evaluated actual
runtime increase number agents. show results NOPRUNE-BEL
MAXEXP. NOPRUNE-OBS could run slowness. NOFAIL
heuristic identical MAXEXP since agents cannot fail scenario. RMTDP
case 30,000 reachable states.
Figures 15(a) 15(b), increase number ambulances along Xaxis. Figure 15(a), show number nodes evaluated (parent nodes + leaf nodes)5
logarithmic scale. seen, MAXEXP method results 89-fold
decrease number nodes evaluated compared NOPRUNE-BEL seven
ambulances, decrease becomes pronounced number ambulances
increased. Figure 15(b) shows time seconds logarithmic scale Y-axis
compares run-times MAXEXP NOPRUNE-BEL methods finding best
role allocation. NOPRUNE-BEL method could find best allocation within
day number ambulances increased beyond four. four ambulances (and
five fire engines), MAXEXP resulted 29-fold speedup NOPRUNE-BEL.
6.2.2 Allocation RoboCupRescue
next set experiments shows practical utility role allocation analysis
complex domains. able show significant performance improvements actual
RoboCupRescue domain using role allocations generated analysis. First,
construct RMTDP rescue scenario, described Section 2.1, taking guidance
TOP underlying domain (as described Section 4.1). use
MAXEXP heuristic determine best role allocation. compared RMTDP
allocation allocations chosen human subjects. goal comparing RMTDP
allocations human subjects mainly show RMTDP capable performing
near human expert levels domain. addition, order determine
reasoning uncertainty actually impacts allocations, compared RMTDP
allocations allocations determined two additional allocation methods:
1. RescueISI: Allocations used RoboCupRescue agents entered
RoboCupRescue competitions 2001(RescueISI) (Nair et al., 2002),
finished third place. agents used local reasoning decision making,
ignoring transitional well observational uncertainty.
2. RMTDP complete observability: discussed earlier, complete observability
RMTDP leads MDP, refer method MDP method.
5. number nodes evaluated using NOPRUNE-BEL computed (f1 + 1) (f2 + 1) (f3 + 1)
(a + 1)c+1 , f1 , f2 f3 number fire engines station 1, 2 3, respectively,
number ambulances c number civilians. node provides complete conditional
role allocation, assuming different numbers civilians fire station.

400

fiHybrid BDI-POMDP Framework Multiagent Teaming

Number nodes (log scale)

10000000

MAXEXP
NOPRUNE

1000000
100000
10000
1000
100
10
1

2

3

4

5

6

7

Number ambulances

Run time secs (log scale)

100000

MAXEXP
NOPRUNE

10000

1000

100

10

1

2

3

4

5

Number ambulances

6

7

Figure 15: Performance role allocation space search RoboCupRescue, a: (left) Number
nodes evaluated log scale, b: (right) Run-time seconds log
scale.

401

fiNair & Tambe

Note comparisons performed using RoboCupRescue simulator
multiple runs deal stochasticity6 . scenario described Section 6.2.1.
fix number fire engines, ambulances civilians five each. experiment,
consider two settings, location civilians drawn from:
Uniform distribution 25% cases four civilians fire 1 one civilian
fire 2, 25% three civilians fire 1 two fire 2, 25% two civilians
fire 1 three fire 2 remaining 25% one civilian fire 1
four civilians fire 2. speedup results Section 6.2.1 obtained using
distribution.
Skewed distribution 80% cases four civilians fire 1 one civilian
fire 2 remaining 20% one civilian fire 1 four civilians fire 2.
Note consider case civilians located fire
optimal ambulance allocation simply assign ambulances fire
civilians located. skewed distribution chosen highlight cases
becomes difficult humans reason allocation choose.
three human subjects used experiment researchers USC. three
familiar RoboCupRescue. given time study setup
given time limit provide allocations. subject told allocations
going judged first basis number civilian lives lost next
damage sustained due fire. exactly criteria used RoboCupRescue (Kitano
et al., 1999).
compared RMTDP allocation human subjects
RoboCupRescue simulator RescueISI MDP. Figure 16, compared
performance allocations basis number civilians died
average damage two buildings (lower values better criteria). two
criteria main two criteria used RoboCupRescue (Kitano et al., 1999). values shown Figure 16 obtained averaging forty simulator runs uniform
distribution twenty runs skewed distribution allocation. average
values plotted account stochasticity domain. Error bars provided
show standard error allocation method.
seen Figure 16(a), RMTDP allocation better five
allocations terms lower number civilians dead (although human3 quite close).
example, averaging forty runs, RMTDP allocation resulted 1.95 civilian deaths
human2s allocation resulted 2.55 civilian deaths. terms average building
damage, six allocations almost indifferentiable, humans actually performing marginally better. Using skewed distribution, difference allocations
much perceptible (see Figure 16(b)). particular, notice RMTDP
allocation much better humans terms number civilians dead. Here,
human3 particularly badly bad allocation fire engines. resulted
damage buildings consequently number civilians dead.
6. mission rehearsal domain, could run actual mission rehearsal simulator since
simulator public domain longer accessible, hence difference tested role
allocations mission rehearsal RoboCupRescue domains.

402

fiHybrid BDI-POMDP Framework Multiagent Teaming

Comparing RMTDP RescueISI MDP approach showed reasoning
transitional uncertainty (MDP) better static reactive allocation method
(RescueISI) well reasoning transitional observational uncertainty. uniform distribution case, found RMTDP better MDP
RescueISI, MDP method performing better RescueISI. skewed distribution case, improvement allocations using RMTDP greater. Averaging twenty
simulation runs, RMTDP allocations resulted 1.54 civilians deaths MDP resulted
1.98 RescueISI 3.52. allocation method used RescueISI often resulted
one fires allocated fire engines. allocations determined
MDP approach turned human1.
two-tailed t-test performed order test statistical significance means
allocations Figure 16. means number civilians dead RMTDP
allocation human allocations found statistically different (confidence
> 96%) uniform well skewed distributions. difference fire
damage statistically significant uniform case, however, difference
RMTDP allocation human3 fire damage statistically significant (> 96%)
skewed case.
6

Civilians casualties
Building damage

5
4
3
2
1

DP

ue







3
sc





2



hu

hu





1


hu

RM

TD

P

0

6

Civilians casualties
Building damage

5
4
3
2
1

DP





sc
ue



3


hu

2


hu

1


hu

RM
TD

P

0

Figure 16: Comparison performance RoboCupRescue, a: (left) uniform, b: (right)
skewed.

403

fiNair & Tambe

Considering average performance different allocations highlight
individual cases marked differences seen performance. Figure 17,
present comparison particular settings allocation methods showed
bigger difference RMTDP terms allocations. standard error shown
error bars allocation. Figures 17(a) 17(b) compare allocations uniform
civilian distributions setting one civilian fire 1 four civilians
fire 2 (1-4 civilian setting) four civilians fire 1 one fire 2 (4-1 civilian setting)
respectively. seen figure, RMTDP allocation results fewer civilian
casualties slightly damage buildings due fire (difference fire damage
statistically significant damage values close). Figures 17(c)
17(d) compare allocations skewed civilian distribution. key difference
arises human3. seen, human3 results damage due fire.
human3 allocated fire engines one buildings, turn resulted
building burnt completely. Consequently, civilians located fire
location could rescued ambulances. Thus, see specific instances
allocation done using RMTDP-based allocation algorithm superior allocations
human comes with.

3.5

Civilians casualties
Building damage

3

4.5

Civilians casualties
Building damage

4
3.5

2.5

3

2

2.5
2

1.5

1.5

1

3.5

DP



ue



3
sc

2






hu



hu

P




TD

Civilians casualties
Building damage

3

hu

RM



DP




3

ue




hu

sc




hu





hu

TD
RM

2

0
1

0

P

0.5
1

1
0.5

4.5

Civilians casualties
Building damage

4
3.5

2.5

3
2

2.5
2

1.5

1.5

1

1
0.5

0.5
DP




sc
ue



3



hu



2
hu

hu

P
RM
TD

DP



ue


3




sc

hu

2


hu

1


hu

TD
P
RM



1

0

0

Figure 17: Comparison performance RoboCupRescue particular settings, a: (topleft) uniform 1-4 civilian setting b:(top-right) uniform 4-1 civilian setting, c:
(bottom-left) skewed 1-4 civilian setting d:(bottom-right) skewed 4-1 civilian
setting.

404

fiHybrid BDI-POMDP Framework Multiagent Teaming

Table 1 shows allocations fire 1 (agents assigned fire 1 allocated fire
2) found RMTDP role allocation algorithm used human subjects
skewed 4-1 civilian setting (we consider case since shows difference).
particular, table highlights differences various allocators skewed
4-1 civilian setting helps account differences seen performance
actual simulator. seen Figure 17(d), main difference performance
terms number civilians saved. Recall scenario, four
civilians fire 1, one fire 2. human subjects MDP chose
send one ambulance fire 2 (number ambulances allocated f ire 2 = 5
number ambulances allocated f ire 1). lone ambulance unable rescue
civilian fire 1, resulting humans MDP saving fewer civilians. RescueISI chose
send ambulances fire 2 using greedy selection method based proximity
civilians resulting civilians fire 1 dying7 . terms fire engine allocation,
human3 sent four fire engines fire 1 civilians likely located
(number engines allocated f ire 2 = 5 number engines allocated f ire 1).
Unfortunately, backfired since lone fire engine fire 2 able extinguish
fire there, causing fire spread parts city.
Distribution
Skewed 4-1

Engines station 1
Engines station 2
Engines station 3
Ambulances

RMTDP
0
1
1
3

human1
2
1
0
4

human2
2
1
0
4

human3
1
1
2
4

RescueISI
2
1
0
0

MDP
2
1
0
4

Table 1: Allocations ambulances fire engines fire 1.
experiments show allocations found RMTDP role allocation algorithm performs significantly better allocations chosen human subjects RescueISI
MDP cases (and significantly worse case). particular
distribution civilians uniform, difficult humans come
allocation difference human allocations RMTDP allocation
becomes significant. conclude RMTDP allocation performs
near-human expertise.
last experiment done using RoboCupRescue simulator, introduced error
RMTDP model order determine sensitive model errors
parameters model. Figure 18 compares allocations found, five
ambulances, 5 fire engines 5 civilians, terms number civilian casualties (Yaxis) error (X-axis) introduced probability fire spread probability
civilian health deterioration. seen increasing error probability fire
spread 20% higher results allocations save fewer civilians fire brigades
choose concentrate effort one fires. resulting allocation
found value terms number civilians casualties used
RescueISI, consider uncertainty. Reducing error probability
fire impact allocations found. Increasing error probability
7. strategy ambulances going closest civilian worked fairly well ambulances
usually well spread

405

fiNair & Tambe

civilian health deterioration 15% higher caused civilians sacrificed.
allocation found value terms number civilians casualties
used RescueISI. Decreasing error probability civilian health deterioration
-5% lower (more negative) caused number ambulances allocated fire
number civilians fire (same human1).
3

Civilian casualties

2.5
2

fire-rate
civilian-health

1.5
1
0.5
0

-25 -20 -15 -10 -5

0

5

10

15

20

25

Percentage error

Figure 18: Model sensitivity RoboCupRescue scenario.

7. Related Work
four related areas research, wish highlight. First,
considerable amount work done field multiagent teamwork (Section 7.1).
second related area research use decision theoretic models, particular
distributed POMDPs (Section 7.2). third area related work describe (Section 7.3)
hybrid systems used Markov Decision Process BDI approaches. Finally,
Section 7.4, related work role allocation reallocation multiagent teams
described.
7.1 BDI-based Teamwork
Several formal teamwork theories Joint Intentions (Cohen & Levesque, 1991),
SharedPlans (Grosz & Kraus, 1996) proposed tried capture essence
multiagent teamwork logic Beliefs-Desires-Intentions. Next, practical models
teamwork COLLAGEN (Rich & Sidner, 1997), GRATE* (Jennings, 1995),
STEAM (Tambe, 1997) built teamwork theories (Cohen & Levesque, 1991; Grosz
& Kraus, 1996) attempted capture aspects teamwork reusable
across domains. addition, complement practical teamwork models, teamoriented programming approach (Pynadath & Tambe, 2003; Tidhar, 1993a, 1993b)
introduced allow large number agents programmed teams. approach
expanded applied variety domains (Pynadath & Tambe, 2003; Yen
et al., 2001; da Silva & Demazeau, 2002). approaches building practical multia406

fiHybrid BDI-POMDP Framework Multiagent Teaming

gent systems (Stone & Veloso, 1999; Decker & Lesser, 1993), explicitly based
team-oriented programming, could considered family.
research reported article complements research teamwork introducing hybrid BDI-POMDP models exploit synergy BDI POMDP
approaches. particular, TOP teamwork models traditionally addressed
uncertainty cost. hybrid model provides capability, illustrated
benefits reasoning via detailed experiments.
article uses team-oriented programming (Tambe et al., 2000; da Silva &
Demazeau, 2002; Tidhar, 1993a, 1993b) example BDI approach, relevant
similar techniques modeling tasking collectives agents, Decker
Lessers (1993) TAEMS approach. particular, TAEMS language provides abstraction tasking collaborative groups agents similar TOP, GPGP infrastructure used executing TAEMS-based tasks analogous TOP interpreter
infrastructure shown Figure 1. Lesser et al. explored use distributed
MDPs analyses GPGP coordination (Xuan & Lesser, 2002), exploited
use TAEMS structures decomposition abstraction searching optimal policies
distributed MDPs, suggested article. Thus, article complements Lesser
et al.s work illustrating significant avenue efficiency improvements
analyses.
7.2 Distributed POMDP Models
Distributed POMDP models represent collection formal models expressive
enough capture uncertainty domain costs rewards associated
states actions. Given group agents, problem deriving separate policies maximize joint reward modeled using distributed POMDP
models. particular, DEC-POMDP (Decentralized POMDP) (Bernstein et al., 2000)
MTDP (Multiagent Team Decision Problem) (Pynadath & Tambe, 2002) generalizations POMDPs case multiple, distributed agents, basing
actions separate observations. frameworks allow us formulate
constitutes optimal policy multiagent team principle derive policy.
However, exceptions, effective algorithms deriving policies distributed
POMDPs developed. Significant progress achieved efficient
single-agent POMDP policy generation algorithms (Monahan, 1982; Cassandra, Littman,
& Zhang, 1997; Kaelbling et al., 1998). However, unlikely research directly
carried distributed case. Finding optimal policies distributed POMDPs
NEXP-complete (Bernstein et al., 2000). contrast, finding optimal policy single
agent POMDP PSPACE-complete (Papadimitriou & Tsitsiklis, 1987). Bernstein et
al. note (Bernstein et al., 2000), suggests fundamental difference nature
problems. distributed problem cannot treated one separate POMDPs
individual policies generated individual agents possible cross-agent
interactions reward, transition observation functions. (For one action one
agent, may many different rewards possible, based actions agents
may take.)
407

fiNair & Tambe

Three approaches used solve distributed POMDPs. One approach
typically taken make simplifying assumptions domain. instance,
Guestrin et al. (2002), assumed agent completely observe world state.
addition, assumed reward function (and transition function) team
expressed sum (product) reward (transition) functions agents
team. Becker et al. (2003) assume domain factored agent
completely observable local state also domain transition-independent
(one agent cannot affect another agents local state).
second approach taken simplify nature policies considered
agents. example, Chades et al. (2002) restrict agent policies memoryless
(reactive) policies, thereby simplifying problem solving multiple MDPs. Peshkin et
al. (2000) take different approach using gradient descent search find local optimum
finite-controllers bounded memory. Nair et al. (2003a) present algorithm finding
locally optimal policy space unrestricted finite-horizon policies. third
approach, taken Hansen et al. (2004), involves trying determine globally optimal
solution without making simplifying assumptions domain. approach,
attempt prune space possible complete policies eliminating dominated
policies. Although brave frontal assault problem, method expected
face significant difficulties scaling due fundamental complexity obtaining
globally optimal solution.
key difference work research focused hybrid systems
leverage advantages BDI team plans, used practical systems,
distributed POMDPs quantitatively reason uncertainty cost. particular,
use TOPs specify large-scale team plans complex domains use RMTDPs
finding best role allocation teams.
7.3 Hybrid BDI-POMDP Approaches
POMDP models used context analysis single agent (Schut,
Wooldridge, & Parsons, 2001) multiagent (Pynadath & Tambe, 2002; Xuan et al., 2001)
behavior. Schut et al. compare various strategies intention reconsideration (deciding
deliberate intentions) modeling BDI system using POMDP.
key differences work approach apply analysis single
agent case consider issues exploiting BDI system structure improving
POMDP efficiency.
Xuan Lesser (2001) Pynadath Tambe (2002), analyze multiagent
communication. Xuan Lesser dealt finding evaluating various communication policies, Pynadath Tambe used COM-MTDP model deal problem comparing various communication strategies empirically analytically.
approach general explain approach analyzing coordination actions including communication. concretely demonstrate approach analysis role
allocation. Additional key differences earlier work Pynadath Tambe (2002)
follows: (i) RMTDP, illustrate techniques exploit team plan decomposition
speeding policy search, absent COM-MTDP, (ii) also introduce techniques
belief-based evaluation absent previous work. Nonetheless, combining RMTDP
408

fiHybrid BDI-POMDP Framework Multiagent Teaming

COM-MTDP interesting avenue research preliminary steps
direction presented Nair, Tambe Marsella (2003b).
Among hybrid systems focused analysis, Scerri et al. (2002) employ Markov
Decision Processes within team-oriented programs adjustable autonomy. key difference work MDPs used execute particular
sub-plan within TOPs plan hierarchy making improvements TOP.
DTGolog (Boutilier, Reiter, Soutchanski, & Thrun, 2000) provides first-order language
limits MDP policy search via logical constraints actions. Although shares
work key idea synergistic interactions MDPs Golog, differs
work focuses single agent MDPs fully observable domains,
exploit plan structure improving MDP performance. ISAAC (Nair, Tambe, Marsella,
& Raines, 2004), system analyzing multiagent teams, also employs decision theoretic
methods analyzing multiagent teams. work, probabilistic finite automaton
(PFA) represents probability distribution key patterns teams behavior
learned logs teams behaviors. key difference work
analysis performed without access actual team plans agents
executing hence advice provided cannot directly applied improving team,
need human developer change team behavior per advice generated.

7.4 Role Allocation Reallocation
several different approaches problem role allocation reallocation.
example, Tidhar et al. (1996) Tambe et al. (2000) performed role allocation based
matching capabilities, Hunsberger Grosz (2000) proposed use combinatorial auctions decide roles assigned. Modi et al. (2003) showed
role allocation modeled distributed constraint optimization problem
applied problem tracking multiple moving targets using distributed sensors.
Shehory Kraus (1998) suggested use coalition formation algorithms deciding
quickly agent took role. Fatima Wooldridge (2001) use auctions
decide task allocation. important note competing techniques
free problem model problem, even though model
transition probabilities. approaches reforming team reconfiguration methods due Dunin-Keplicz Verbrugge (2001), self-adapting organizations Horling
Lesser (2001) dynamic re-organizing groups (Barber & Martin, 2001). Scerri et
al. (2003) present role (re)allocation algorithm allows autonomy role reallocation
shift human supervisor agents.
key difference prior work use stochastic models (RMTDPs)
evaluate allocations: enables us compute benefits role allocation, taking
account uncertainty costs reallocation upon failure. example, mission
rehearsal domain, uncertainties considered, one scout would
allocated, leading costly future reallocations even mission failure. Instead,
lookahead, depending probability failure, multiple scouts sent one
routes, resulting fewer future reallocations higher expected reward.
409

fiNair & Tambe

8. Conclusion
BDI approach agent teamwork provided successful applications, tools
techniques provide quantitative analyses team coordination team behaviors uncertainty lacking. emerging field distributed POMDPs provides
decision theoretic method quantitatively obtaining optimal policy team
agents, faces serious intractability challenge. Therefore, article leverages
benefits BDI POMDP approaches analyze improve key coordination
decisions within BDI-based team plans using POMDP-based methods. order demonstrate analysis methods, concentrated role allocation fundamental aspect
agent teamwork provided three key contributions. First, introduced RMTDP,
distributed POMDP based framework, analysis role allocation. Second, article
presented RMTDP-based methodology optimizing key coordination decisions within
BDI team plan given domain. Concretely, article described methodology
finding best role allocation fixed team plan. Given combinatorially many
role allocations, introduced methods exploit task decompositions among sub-teams
significantly prune search space role allocations.
Third, hybrid BDI-POMDP approach uncovered several synergistic interactions
BDI team plans distributed POMDPs:
1. TOPs useful constructing RMTDP model domain, identifying
features need modeled well decomposing model construction
according structure TOP. RMTDP model could used
evaluate TOP.
2. TOPs restricted policy search providing RMTDPs incomplete policies
limited number open decisions.
3. BDI approach helped coming novel efficient belief-based representation policies suited hybrid BDI-POMDP approach corresponding
algorithm evaluating policies. resulted faster evaluation also
compact policy representation.
4. structure TOP exploited decompose problem evaluating
abstract policies, resulting significant pruning search optimal role
allocations.
constructed RMTDPs two domains RoboCupRescue mission rehearsal
simulation determined best role allocation domains. Furthermore,
illustrated significant speedups RMTDP policy search due techniques introduced
article. Detailed experiments revealed advantages approach state-ofthe-art role allocation approaches failed reason uncertainty.
key agenda future work continue scale-up RMTDPs even larger
scale agent teams. scale-up require efficiency improvements. propose
continue exploit interaction BDI POMDP approaches achieving
scale-up. instance, besides disaster rescue, distributed sensor nets large area
monitoring applications could benefit scale-up.
410

fiHybrid BDI-POMDP Framework Multiagent Teaming

Acknowledgments
research supported NSF grant #0208580. would like thank Jim Blythe,
Anthony Cassandra, Hyuckchul Jung, Spiros Kapetanakis, Sven Koenig, Michael Littman,
Stacy Marsella, David Pynadath Paul Scerri discussions related article.
would also like thank reviewers article whose comments helped
significantly improving article.

Appendix A. TOP details
section, describe TOP helicopter scenario. details
subplan Figure 4(b) shown below:
ExecuteMission:
Context:
Pre-conditions: (MB <TaskForce> location(TaskForce) = START)
Achieved: (MB <TaskForce> (Achieved(DoScouting) Achieved(DoTransport)))
(time > (MB <TaskForce> Achieved(RemainingScouts)
( helo ScoutingTeam, alive(helo) location(helo) 6= END)))
Unachievable: (MB <TaskForce> Unachievable(DoScouting))
(MB <TaskForce> (Unachievable(DoTransport)
(Achieved(RemainingScouts)
( helo ScoutingTeam, alive(helo) location(helo) 6= END))))
Irrelevant:
Body:
DoScouting
DoTransport
RemainingScouts
Constraints:
DoScouting DoTransport
DoScouting RemainingScouts
DoScouting:
Context:ExecuteMission <TaskForce>
Pre-conditions:
Achieved:
Unachievable:
Irrelevant:
Body:
WaitAtBase
ScoutRoutes
Constraints:
WaitAtBase ScoutRoutes
WaitAtBase:
Context: DoScouting <TaskForce>
Pre-conditions:
Achieved:
Unachievable: (MB <TransportTeam> helo TransportTeam, alive(helo))
411

fiNair & Tambe

Irrelevant:
Body:
no-op



ScoutRoutes:
Context: DoScouting <TaskForce>
Achieved:
Unachievable:
Irrelevant:(MB <ScoutingTeam> helo TransportTeam, alive(helo))
Body:
ScoutRoute1
ScoutRoute2
ScoutRoute3
Constraints:
ScoutRoute1 ScoutRoute2 ScoutRoute3
ScoutRoute1:
Context: ScoutRoutes <ScoutingTeam>
Pre-conditions:
Achieved: (MB <SctTeamA> helo SctTeamA, location(helo) = END)
Unachievable: time > (MB <SctTeamA> helo SctTeamA, alive(helo))
Irrelevant:
Body:
(location(SctTeamA) = START) route(SctTeamA) 1
(location(SctTeamA) 6= END) move-forward
ScoutRoute2:
Context: ScoutRoutes <ScoutingTeam>
Pre-conditions:
Achieved: (MB <SctTeamB> helo SctTeamB, location(helo) = END)
Unachievable: time > (MB <SctTeamB> helo SctTeamB, alive(helo))
Irrelevant:
Body:
(location(SctTeamB) = START) route(SctTeamB) 2
(location(SctTeamB) 6= END) move-forward
ScoutRoute2:
Context: ScoutRoutes <ScoutingTeam>
Pre-conditions:
Achieved: (MB <SctTeamA> helo SctTeamA, location(helo) = END)
Unachievable: time > (MB <SctTeamA> helo SctTeamA, alive(helo))
Irrelevant:
Body:
(location(SctTeamA) = START) route(SctTeamA) 1
(location(SctTeamA) 6= END) move-forward
DoTransport:
Context: ExecuteMission <TaskForce>
Pre-conditions:
412

fiHybrid BDI-POMDP Framework Multiagent Teaming

Achieved: (MB <TransportTeam> location(TransportTeam) = END)
Unachievable: time > (MB <TransportTeam> helo TransportTeam, alive(helo))
Irrelevant:
Body:
(location(TransportTeam) = start)
(MB <TransportTeam> Achieved(ScoutRoute1))
route(TransportTeam) 1
elseif (MB <TransportTeam> Achieved(ScoutRoute2))
route(TransportTeam) 2
elseif (MB <TransportTeam> Achieved(ScoutRoute3))
route(TransportTeam) 3
(route(TransportTeam) 6= null) (location(TransportTeam) 6= END)
move-forward
RemainingScouts:
Context: ExecuteMission <TaskForce>
Pre-conditions:
Achieved: (MB <ScoutingTeam> location(ScoutingTeam) = END)
Unachievable: time > (MB <ScoutingTeam> ( helo ScoutingTeam
alive(helo) location(helo) 6= END))
Irrelevant:
Body:
(location(ScoutingTeam) 6= END) move-forward

predicate Achieved(tplan) true Achieved conditions tplan true. Similarly, predicates Unachievable(tplan) Irrelevant(tplan) true Unachievable conditions Irrelevant conditions tplan true, respectively. predicate
(location(team) = END) true members team END.
Figure 4(b) also shows coordination relationships: relationship indicated
solid arc, relationship indicated dotted arc. coordination relationships indicate unachievability, achievability irrelevance conditions
enforced TOP infrastructure. relationship team sub-plans
means team sub-plans fail, parent team plan fail. Also,
parent team plan achieved, child sub-plans must achieved. Thus,
DoScouting, WaitAtBase ScoutRoutes must done:
Achieved: (MB <TaskForce> Achieved(WaitAtBase) Achieved(ScoutRoutes))
Unachievable: (MB <TaskForce> Unachievable(WaitAtBase)
Unachievable(ScoutRoutes))

relationship means subplans must fail parent fail success
subplans means parent plan succeeded. Thus, ScoutingRoutes,
least one ScoutRoute1, ScoutRoute2 ScoutRoute3 need performed:
(MB <ScoutingTeam> Achieved(ScoutRoute1)
Achieved(ScoutRoute2) Achieved(ScoutRoute3))
Unachievable: (MB <TaskForce> Unachievable(ScoutRoute1)
Unachievable(ScoutRoute2) Unachievable(ScoutRoute3))
Achieved:

413

fiNair & Tambe

Also relationship affects irrelevance conditions subplans joins.
parent unachievable subplans still executing become irrelevant.
Thus, WaitAtBase:
Irrelevant:

(MB <TaskForce> Unachievable(ScoutRoutes))

Similarly ScoutingRoutes:
Irrelevant:

(MB <TaskForce> Unachievable(ScoutRoutes))

.
Finally, assign roles plans Figure 4(b) shows assignment brackets adjacent plans. instance, Task Force team assigned jointly perform Execute
Mission.

Appendix B. RMTDP details
section, present details RMTDP constructed TOP Figure 4.
S: get features state attributes tested preconditions
achieved, unachievable irrelevant conditions body team plans
individual agent plans. Thus relevant state variables are:location
helicopter, role helicopter,route helicopter, status helicopter
(alive not) time. team n helicopters, state given tuple
< time, role1 , . . . , rolen , loc1 , . . . , locn , route1 , . . . , routen , status1 , . . . , statusn >.
A: consider actions primitive actions agent perform
within individual plans. TOP infrastructure enforces mutual belief
communication actions. Since analyzing cost focus
research consider communication implicit model effect
communication directly observation function.
consider 2 kinds actions role-taking role-execution actions. assume
initial allocation specify roles agents. specifies whether
agent scout transport scout scout team assigned to.
scout cannot become transport change team initial allocation
transport change role taking one role-taking actions.The role-taking
role-execution actions agent given by:
i,memberT ransportT eam = {joinSctT eamA, joinSctT eamB, joinSctT eamC}
i,memberSctT eamA = i,memberSctT eamB = i,memberSctT eamCx =
i,memberT ransportT eam = {chooseRoute, moveF orward}
i,memberSctT eamA = i,memberSctT eamB = i,memberSctT eamC = {moveF orward}
P : obtain transition function help human expert
simulations simulator available. domain, helicopters crash (be shot
down) START, END already scouted location. probability
scouts get shot depends route on, i.e. probability
crash route1 p1 , probability crash route2 p2 probability crash
route3 p3 many scouts spot. assume
414

fiHybrid BDI-POMDP Framework Multiagent Teaming

probability transport shot unscouted location 1
scouted location 0. probability multiple crashes obtained
multiplying probabilities individual crashes.
action, moveForward, effect routei = null loci = END
statusi = dead. cases, location agent gets incremented.
assume role-taking actions scoutRoutex always succeed role
performing agent transport assigned route already.
: transport START observe status agents
probability depending positions. helicopter particular route
observe helicopters route completely cannot observe helicopters
routes.
O: observation function gives probability group agents receive
particular joint observation. domain assume observations one agent
independent observations agents, given current state
previous joint action. Thus probability joint observation computed
multiplying probabilities individual agents observations.
probability transport START observing status alive scout
route 1 0.95. probability transport START observing nothing
alive scout 0.05 since dont false negatives. Similarly scout
route 1 crashes, probability visible transport START 0.98
probability transport doesnt see failure 0.02. Similarly
probabilities observing alive scout route 2 route 3 0.94 0.93
respectively probabilities observing crash route 2 route 3
0.97 0.96 respectively.
R: reward function obtained help human expert helps
assign value various states cost performing various actions.
analysis, assume actions moveForward chooseRoute cost.
consider negative reward (cost) replacement action, scoutRoutex,
R , negative reward failure helicopter RF , reward
scout reaching END Rscout reward transport reaching END
Rtransport . E.g. R = 10, RF = 50, Rscout = 5, Rtransport = 75.
RL: roles individual agents take TOP organization hierarchy.
RL = {transport, scoutOnRoute1, scoutOnRoute2, scoutOnRoute3}.

Appendix C. Theorems
Theorem 3 MAXEXP method always yield upper bound.
Proof sketch:
Let policy leaf-level policy highest expected reward particular parent node, i, restricted policy space.
V = maxChildren(i) V
415

(3)

fiNair & Tambe

Since reward function specified separately component, separate expected reward V rewards constituent components given
starting states starting observation histories components. Let
team plan divided components components parallel
independent sequentially executed.
X
V
maxstates[j],oHistories[j]Vj
1jm

expected value obtained component j, 1 j cannot greater
highest value obtained j using policy.
maxstates[j],oHistories[j]Vj maxChildren(i) maxstates[j],oHistories[j](Vj )

(4)

Hence,
V

X

maxChildren(i) maxstates[j],oHistories[j](Vj )

1jm

V MaxEstimate(i)

(5)



References
Barber, S., & Martin, C. (2001). Dynamic reorganization decision-making groups.
Proceedings Fifth International Conference Autonomous Agents (Agents-01),
pp. 513520.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent
decentralized Markov decision processes. Proceedings Second International
Joint Conference Autonomous Agents Multi Agent Systems (AAMAS-03), pp.
4148.
Bernstein, D. S., Zilberstein, S., & Immerman, N. (2000). complexity decentralized control MDPs. Proceedings Sixteenth Conference Uncertainty
Artificial Intelligence(UAI-00), pp. 3237.
Boutilier, C. (1996). Planning, learning & coordination multiagent decision processes.
Proceedings Sixth Conference Theoretical Aspects Rationality Knowledge (TARK-96), pp. 195210.
Boutilier, C., Reiter, R., Soutchanski, M., & Thrun, S. (2000). Decision-theoretic, highlevel agent programming situation calculus. Proceedings Seventeenth
National Conference Artificial Intelligence (AAAI-00), pp. 355362.
Cassandra, A., Littman, M., & Zhang, N. (1997). Incremental pruning: simple, fast,
exact method partially observable Markov decision processes. Proceedings
Thirteenth Annual Conference Uncertainty Artificial Intelligence (UAI-97),
pp. 5461.
416

fiHybrid BDI-POMDP Framework Multiagent Teaming

Chades, I., Scherrer, B., & Charpillet, F. (2002). heuristic approach solving
decentralized-pomdp: Assessment pursuit problem. Proceedings 2002
ACM Symposium Applied Computing (SAC-02), pp. 5762.
Cohen, P. R., & Levesque, H. J. (1991). Teamwork. Nous, 25 (4), 487512.
da Silva, J. L. T., & Demazeau, Y. (2002). Vowels co-ordination model. Proceedings
First International Joint Conference Autonomous Agents Multiagent
Systems (AAMAS-2002), pp. 11291136.
Dean, T., & Lin, S. H. (1995). Decomposition techniques planning stochastic domains. Proceedings Fourteenth International Joint Conference Artificial
Intelligence (IJCAI-95), pp. 11211129.
Decker, K., & Lesser, V. (1993). Quantitative modeling complex computational task
environments. Proceedings Eleventh National Conference Artificial Intelligence (AAAI-93), pp. 217224.
Dix, J., Muoz-Avila, H., Nau, D. S., & Zhang, L. (2003). Impacting shop: Putting
ai planner multi-agent environment. Annals Mathematics Artificial
Intelligence, 37 (4), 381407.
Dunin-Keplicz, B., & Verbrugge, R. (2001). reconfiguration algorithm distributed
problem solving. Engineering Simulation, 18, 227246.
Erol, K., Hendler, J., & Nau, D. S. (1994). HTN planning: Complexity expressivity.
Proceedings Twelfth National Conference Artificial Intelligence (AAAI-94),
pp. 11231128.
Fatima, S. S., & Wooldridge, M. (2001). Adaptive task resource allocation multiagent systems. Proceedings Fifth International Conference Autonomous
Agents (Agents-01), pp. 537544.
Georgeff, M. P., & Lansky, A. L. (1986). Procedural knowledge. Proceedings IEEE
special issue knowledge representation, 74, 13831398.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange cooperative
multi-agent systems. Proceedings Second International Joint Conference
Autonomous Agents Multi Agent Systems (AAMAS-03), pp. 137144.
Grosz, B., Hunsberger, L., & Kraus, S. (1999). Planning acting together. AI Magazine,
20 (4), 2334.
Grosz, B., & Kraus, S. (1996). Collaborative plans complex group action. Artificial
Intelligence, 86 (2), 269357.
Guestrin, C., Venkataraman, S., & Koller, D. (2002). Context specific multiagent coordination planning factored MDPs. Proceedings Eighteenth National
Conference Artificial Intelligence (AAAI-02), pp. 253259.
Hansen, E., & Zhou, R. (2003). Synthesis hierarchical finite-state controllers pomdps.
Proceedings Thirteenth International Conference Automated Planning
Scheduling (ICAPS-03), pp. 113122.
417

fiNair & Tambe

Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially
observable stochastic games. Proceedings Nineteenth National Conference
Artificial Intelligence (AAAI-04), pp. 709715.
Ho, Y.-C. (1980). Team decision theory information structures. Proceedings
IEEE, 68 (6), 644654.
Horling, B., Benyo, B., & Lesser, V. (2001). Using self-diagnosis adapt organizational
structures. Proceedings Fifth International Conference Autonomous
Agents (Agents-01), pp. 529536.
Hunsberger, L., & Grosz, B. (2000). combinatorial auction collaborative planning.
Proceedings Fourth International Conference Multiagent Systems (ICMAS2000), pp. 151158.
Jennings, N. (1995). Controlling cooperative problem solving industrial multi-agent
systems using joint intentions. Artificial Intelligence, 75 (2), 195240.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101 (2), 99134.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjoh, A., & Shimada,
S. (1999). RoboCup-Rescue: Search rescue large scale disasters domain
multiagent research. Proceedings IEEE Conference Systems, Men,
Cybernetics (SMC-99), pp. 739743.
Levesque, H. J., Cohen, P. R., & Nunes, J. (1990). acting together. Proceedings
National Conference Artificial Intelligence, pp. 9499. Menlo Park, Calif.: AAAI
press.
Mailler, R. T., & Lesser, V. (2004). Solving distributed constraint optimization problems using cooperative mediation. Proceedings Third International Joint Conference
Agents Multiagent Systems (AAMAS-04), pp. 438445.
Marschak, J., & Radner, R. (1972). Economic Theory Teams. Cowles Foundation
Yale University Press, New Haven, CT.
Modi, P. J., Shen, W.-M., Tambe, M., & Yokoo, M. (2003). asynchronous complete
method distributed constraint optimization. Proceedings Second International Joint Conference Agents Multiagent Systems (AAMAS-03), pp.
161168.
Monahan, G. (1982). survey partially observable Markov decision processes: Theory,
models algorithms. Management Science, 101 (1), 116.
Nair, R., Ito, T., Tambe, M., & Marsella, S. (2002). Task allocation rescue simulation
domain. RoboCup 2001: Robot Soccer World Cup V, Vol. 2377 Lecture Notes
Computer Science, pp. 751754. Springer-Verlag, Heidelberg, Germany.
Nair, R., Pynadath, D., Yokoo, M., Tambe, M., & Marsella, S. (2003a). Taming decentralized
POMDPs: Towards efficient policy computation multiagent settings. Proceedings
Eighteenth International Joint Conference Artificial Intelligence (IJCAI-03),
pp. 705711.
418

fiHybrid BDI-POMDP Framework Multiagent Teaming

Nair, R., Tambe, M., & Marsella, S. (2003b). Team formation reformation multiagent domains like RoboCupRescue. Kaminka, G., Lima, P., & Roja, R. (Eds.),
Proceedings RoboCup-2002 International Symposium, pp. 150161. Lecture Notes
Computer Science, Springer Verlag.
Nair, R., Tambe, M., Marsella, S., & Raines, T. (2004). Automated assistants analyze
team behavior. Journal Autonomous Agents Multi-Agent Systems, 8 (1), 69
111.
Papadimitriou, C., & Tsitsiklis, J. (1987). Complexity Markov decision processes. Mathematics Operations Research, 12 (3), 441450.
Peshkin, L., Meuleau, N., Kim, K.-E., & Kaelbling, L. (2000). Learning cooperate via
policy search. Proceedings Sixteenth Conference Uncertainty Artificial
Intelligence (UAI-00), pp. 489496.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. Proceedings
Advances Neural Information Processing Systems 16 (NIPS).
Pynadath, D. V., & Tambe, M. (2002). communicative multiagent team decision
problem: Analyzing teamwork theories models. Journal Artificial Intelligence
Research, 16, 389423.
Pynadath, D. V., & Tambe, M. (2003). Automated teamwork among heterogeneous software agents humans. Journal Autonomous Agents Multi-Agent Systems
(JAAMAS), 7, 71100.
Rich, C., & Sidner, C. (1997). COLLAGEN: agents collaborate people.
Proceedings First International Conference Autonomous Agents (Agents97), pp. 284291.
Scerri, P., Johnson, L., Pynadath, D., Rosenbloom, P., Si, M., Schurr, N., & Tambe, M.
(2003). prototype infrastructure distributed robot, agent, person teams.
Proceedings Second International Joint Conference Agents Multiagent
Systems (AAMAS-03), pp. 433440.
Scerri, P., Pynadath, D. V., & Tambe, M. (2002). Towards adjustable autonomy
real-world. Journal Artificial Intelligence (JAIR), 17, 171228.
Schut, M. C., Wooldridge, M., & Parsons, S. (2001). Reasoning intentions uncertain domains. Proceedings Sixth European Conference Symbolic
Quantitative Approaches Reasoning Uncertainty (ECSQARU-2001), pp. 84
95.
Shehory, O., & Kraus, S. (1998). Methods task allocation via agent coalition formation.
Artificial Intelligence, 101 (1-2), 165200.
Sondik, E. J. (1971). optimal control partially observable Markov processes. Ph.D.
Thesis, Stanford.
Stone, P., & Veloso, M. (1999). Task decomposition, dynamic role assignment, lowbandwidth communication real-time strategic teamwork. Artificial Intelligence,
110 (2), 241273.
419

fiNair & Tambe

Tambe, M. (1997). Towards flexible teamwork. Journal Artificial Intelligence Research,
7, 83124.
Tambe, M., Pynadath, D., & Chauvat, N. (2000). Building dynamic agent organizations
cyberspace. IEEE Internet Computing, 4 (2), 6573.
Tidhar, G. (1993a). Team-oriented programming: Preliminary report. Tech. rep. 41, Australian Artificial Intelligence Institute.
Tidhar, G. (1993b). Team-oriented programming: Social structures. Tech. rep. 47, Australian Artificial Intelligence Institute.
Tidhar, G., Rao, A., & Sonenberg, E. (1996). Guided team selection. Proceedings
Second International Conference Multi-agent Systems (ICMAS-96), pp. 369376.
Wooldridge, M. (2002). Introduction Multiagent Systems. John Wiley & Sons.
Xuan, P., & Lesser, V. (2002). Multi-agent policies: centralized ones decentralized ones. Proceedings First International Joint Conference Agents
Multiagent Systems (AAMAS-02), pp. 10981105.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multiagent
cooperation. Proceedings Fifth International Conference Autonomous
Agents (Agents-01), pp. 616623.
Yen, J., Yin, J., Ioerger, T. R., Miller, M. S., Xu, D., & Volz, R. A. (2001). Cast: Collaborative agents simulating teamwork. Proceedings Seventeenth International
Joint Conference Artificial Intelligence (IJCAI-01), pp. 11351144.
Yoshikawa, T. (1978). Decomposition dynamic team decision problems. IEEE Transactions Automatic Control, AC-23 (4), 627632.

420

fiJournal Artificial Intelligence Research 23 (2005) 79-122

Submitted 2/04; published 2/05

Reinforcement Learning Agents Many Sensors
Actuators Acting Categorizable Environments
Josep Porta

porta@science.uva.nl

IAS Group, Informatics Institute
University Amsterdam
Kruislaan 403, 1098SJ, Amsterdam, Netherlands

Enric Celaya

celaya@iri.upc.edu

Institut de Robotica Informatica Industrial
Spanish Council Scientific Research (CSIC)
Llorens Artigas 4-6, 08028, Barcelona, Spain

Abstract
paper, confront problem applying reinforcement learning agents
perceive environment many sensors perform parallel actions using
many actuators case complex autonomous robots. argue reinforcement
learning successfully applied case strong assumptions made
characteristics environment learning performed,
relevant sensor readings motor commands readily identified. introduction
assumptions leads strongly-biased learning systems eventually lose
generality traditional reinforcement-learning algorithms.
line, observe that, realistic situations, reward received robot
depends reduced subset executed actions reduced subset
sensor inputs (possibly different situation action) relevant
predict reward. formalize property called categorizability assumption
present algorithm takes advantage categorizability environment,
allowing decrease learning time respect existing reinforcement-learning
algorithms. Results application algorithm couple simulated realisticrobotic problems (landmark-based navigation six-legged robot gait generation)
reported validate approach compare existing flat generalizationbased reinforcement-learning approaches.

1. Introduction
division knowledge-based behavior-based artificial intelligence
fundamental achieving successful applications within field autonomous robots (Arkin,
1998). However, now, division repercussions reinforcement learning. Within artificial intelligence, reinforcement learning formalized
general way borrowing ideas dynamic programming decision-theory fields.
Within formalization, objective reinforcement-learning methods establish
correct mapping set abstract observations (formalized states) set
high level actions, without worried sets states actions
defined (for introduction reinforcement learning check Kaelbling, Littman,
& Moore, 1996; Sutton & Barto, 1998, among many others). Algorithms developed within
general framework used different fields without modification.
c
2005
AI Access Foundation. rights reserved.

fiPorta & Celaya

particular application, definition sets states actions responsibility
programmer supposed part reinforcement-learning problem.
However, clearly pointed Brooks (1991), autonomous robots major hurdles
related perception action representations. reason, robotic
task, traditional reinforcement-learning research assumes major problem
(connecting states actions) simpler assumes given (the definition
states actions). consequence existing reinforcement-learning methods
best suited problems fall symbolic artificial intelligence domain
belong robotics. Due generality existing reinforcement-learning
algorithms, robotic problem analyzed re-formulated tackled
available reinforcement-learning tools but, many cases, re-formulation
awkward introducing unnecessary complexity learning process. alternative
explore paper new reinforcement-learning algorithm applied
robotic problems are, without re-formulation.
Brooks (1991) remarked, dealing real environment necessarily problem
since real environments properties exploited reduce complexity
robots controller. Brooks works, find simple robot controllers achieve
good performance particular environments. clearly contrast generality
pursued within reinforcement learning. Following idea parallel Brooks,
paper, present new reinforcement-learning algorithm takes advantage specific
environment-related property (that call categorizability) efficiently learn achieve
given task. formalize categorizability property present representation
system (partial rules) exploit property. remarkable feature representation
system allows generalization spaces sensors actions, using
uniform mechanism. ability generalize state action spaces
fundamental successfully apply reinforcement learning autonomous robots.
paper organized follows. First, Section 2, formalize reinforcement learning point view use field autonomous robotics describe
problems make flat (and, cases, also generalization-based) reinforcementlearning algorithms adequate case. Section 3 presents categorizability assumption plausible robotics environments. Then, Section 4, describe
alternative reinforcement-learning algorithm exploits categorizability assumption circumvent problems present existing approaches. Section 5, analyze
points contact proposal already existing work. Next, Section 6,
present experiments validate approach. experiments performed
simulations mimic realistic robotic applications categorizability assumption
likely valid. Finally, Section 7, conclude analyzing strengths
weaknesses proposed learning system.
Additionally, Appendix provides detailed description partial-rule learning
algorithm introduced paper, Appendix B devoted enhancement
algorithm make execution efficient, Appendix C summarizes notation
use throughout paper.
80

fiReinforcement Learning Categorizable Environments

2. Problem Formalization
simplicity, assume robot perceives environment set binary
feature detectors1 F = {fdi | = 1..nf }. feature detector devised process
identifies specific combinations present (and possibly past) sensor readings.
use feature detectors common robotics. field, feature detectors
defined programmer attending special characteristics environment,
robot sensors, task executed order extract potentially useful information
(presence landmarks obstacles, . . . ) raw sensor readings.
similar way, instead working directly space actions provided
robot motors (that define low-level way controlling robot), common
practice define set elementary actions EA = {eai |i = 1..ne }. elementary action
specific sequence/combination motor commands defined programmer attending
characteristics robot task achieved. simplify, assume
elementary actions form (mi k) (i [1..nm ]) mi motor k
value range valid inputs motor mi . framework quite flexible since
motor mi either one physical motors robot high-level, abstract
motor combines movements actual motors. formalization, given
moment, robot execute parallel many elementary actions available motors.
robot controller seen procedure executes (combinations elementary)
actions response specific situations (i.e., activation specific feature detectors)
objective achieving given task. Reinforcement-learning approaches automatically
define controller using information provided reward signal. context
reinforcement learning, controller called policy learner.
objective value-function-based reinforcement-learning algorithms (the
common reinforcement-learning algorithms) predict reward directly
indirectly obtained execution action (i.e., combination elementary
actions) possible situation, described combination active inactive feature
detectors. prediction available, action executed situation
one maximum reward expected.
predict reward, classic reinforcement-learning algorithms rely Markov
assumption, requires state signal carry enough information determine effects
actions given situation.2 Additionally, non-generalizing reinforcement-learning
algorithms assume states system must learned independently. So,
information gathered effects action given state s, denoted Q(s, a),
cannot safely transferred similar states actions. assumption, cost
reinforcement-learning algorithm general problem
(ns na ),
ns number states na number actions.
action tried least state. Since state defined observed
1. Non-binary feature detectors providing discrete range values readily binarized.
2. Non-Markovian problems, confronted, converted Markovian ones.
scope paper, although one relevant points achieve successful
real-world reinforcement-learning application.

81

fiPorta & Celaya

combination feature detectors, potential number states
ns = 2 nf ,
nf number feature detectors. Consequently,
(ns na ) = (2nf na ),
exponential number feature detectors. Since number feature detectors used robotic applications tends high, non-generalizing reinforcement learning
becomes impractical realistic problems. well known curse dimensionality
introduced Bellman (1957), whose research presaged work reinforcement
learning.
Although size action set (na ) important size state set (ns )
curse dimensionality, less attention paid actions reinforcement-learning
literature. However, robot many degrees freedom execute many elementary
actions simultaneously makes cost learning algorithms also increase
exponentially number motors robot (nm ).
Suppose address task two different sets feature detectors F 1
F D2 F D1 F D2 . Using plain reinforcement-learning algorithm, cost
finding proper policy would larger using larger set features (F 2 ).
even one features F D2 F D1 stronger correlation reward
features F D1 . Non-generalizing reinforcement-learning algorithms
able take advantage situation, and, even better input information,
performance decreases. similar argument made actions addition
feature detectors.
Generalizing reinforcement-learning algorithms using gradient-descent
techniques (Widrow & Hoff, 1960), coarse codings (Hinton, McClelland, & Rumelhart,
1986), radial-basis functions (Poggio & Girosi, 1990), tile coding (Sutton, 1996) decision
trees (Chapman & Kaelbling, 1991; McCallum, 1995) partially palliate problem
since deal large state spaces. However, approach complex realistic
problems, number dimensions state-space grows point making use
generalization techniques impractical function approximation
techniques must used (Sutton & Barto, 1998, page 209).
Adding relevant inputs actions task make task easier least
difficult. methods whose complexity depends relevance available inputs actions number would scale well real domain problems.
Examples systems fulfilling property are, instance, Kanerva coding system presented Kanerva (1988) random representation method Sutton Whitehead
(1993). systems rely large collections fixed prototypes (i.e., combinations
feature detectors) selected random, proposal search appropriate prototypes, using strong bias search performed reasonable time.
strong bias based categorizability assumption plausible assumption
case autonomous robots, allows large speed learning process.
Additionally, existing systems address problem determining relevance
actions, since assume learning agent single actuator (that is, obviously,
82

fiReinforcement Learning Categorizable Environments

relevant one). simple set adequate robotics. approach (presented below), combinations feature detectors elementary actions considered
using unified framework.

3. Categorizability Assumption
experience developing controllers autonomous robots, observe that, many
realistic situations, reward received robot depends reduced subset
actions executed robot sensor inputs irrelevant
predict reward. Thus, example, value resulting action grasping
object front robot depend object is: object robot
bring user, electrified cable, unimportant object. However, result
probably whether robot moving cameras grasping
object, day night, robot is, time, checking distance
nearest wall, see red light nearby (aspects, them, may
become important circumstances).
agent observes acts environment reduced fraction available inputs actuators considered time, say agent
categorizable environment.
Categorizability binary predicate graded property. completely
categorizable case, would necessary pay attention one sensor/motor
situation. extreme spectrum, motors carefully
coordinated achieve task effect action could predicted
taking account value feature detectors, would say environment
categorizable all.
Since robots large collection sensors providing heterogeneous collection
inputs many actuators affecting quite different degrees freedom, hypothesis
that, robotic problems, environments highly categorizable and, cases,
algorithm biased categorizability assumption would result advantageous.

4. Reinforcement Learning Categorizable Environments: Partial
Rule Approach
implement algorithm able exploit potential categorizability environment,
need representation system able transfer information similar situations
also similar actions.
Clustering techniques successive subdivisions state space (as, instance,
presented McCallum, 1995) focus perception side problem aim
determining reward expected given state considering
feature detectors perceived state. subset relevant feature detectors
used compute expected reward state possible action (the Q(s, a)
function). However, way posing problem curse dimensionality problem
completely avoided since features relevant one action
another produces unnecessary (from point view action)
differentiation equivalent situations, decreasing learning speed. problem
83

fiPorta & Celaya

avoided finding specific set relevant feature detectors action.
case, Q function computed Q(fs (a), a), state definition function
action consideration. technique used, instance, Mahadevan
Connell (1992). Unfortunately, problem confronting, enough since,
case, actions composed combinations elementary actions also want
transfer reward information similar combinations actions. Therefore,
estimate Q(fs (a), a) taking account elementary actions compose
a. However, principle, relevance elementary actions function situation (or,
equivalently, state): given elementary action relevant situations
others. reason, function approximate becomes Q(f (a), fa (s))
cross-dependency state defined function action, f (a),
action defined function state, fa (s). proposal detail next solves
cross-dependency working Cartesian product spaces feature detectors
elementary actions combinations.
formalize proposal, introduce definitions.
say agent perceives (or observes) partial view order k, v(fd i1 , . . . , fdik ),
k nf whenever predicate fdi1 ... fdik holds.3 Obviously, many partial views
perceived time.
given moment, agent executes action issues different command
one agents motors = {ea1 , . . . , eanm }, nm number motors.
partial command order k, noted c(eai1 , . . . , eaik ), k nm , executed whenever
elementary actions {eai1 , . . . , eaik } executed simultaneously. say partial
command c action accordance c subset a. Note execution
given action supposes execution partial commands accordance
it.
partial rule w defined pair w = (v, c), v partial view c
partial command. say partial rule w = (v, c) active v observed, w
used whenever partial view v perceived partial command c executed.
partial rule covers sub-area Cartesian product feature detectors elementary
actions and, thus, defines situation-action rule used partially determine
actions robot many situations (all partial view rule
active). order partial rule defined sum order partial view
order partial command compose rule.
associate quantiy qw partial rule. qw estimation value (i.e.,
discounted cumulative reward) obtained executing c v observed
time t:

X
qw =
t+i rt+i ,
i=0

rt+i reward received learner time step + rule w used time
t. So, partial rule interpreted as: partial view v observed execution
partial command c results value qw .
3. partial view also include negations feature detectors since non-detection feature
relevant detection.

84

fiReinforcement Learning Categorizable Environments

objective learning process deriving set partial rules adjusting
corresponding qw values desired task properly achieved.
apparent drawback partial-rule representation number possible
partial rules much larger number state action pairs: number
partial rules defined set nf binary feature detectors nm binary
motors 3nf +nm , number different states action pairs 2nf +nm .
arbitrary problems confronted (as case synthetic learning situations),
partial-rule approach could useful. However, problems confronted robots
arbitrary since, mentioned, environments present regularities properties (as
categorizability) exploited reduce complexity controller necessary
achieve given task.
Using partial-rule framework, categorizability assumption formally defined
as:
Definition 1 say environment/task highly categorizable exists set
low-order partial rules allows us predict reward accuracy
statistics possible state-action combination considered. lower order
rules controller higher categorizability environment/task.
extent categorizability assumption fulfilled, number partial rules
necessary control robot becomes much smaller number state-action pairs
defined using sets feature detectors elementary actions
partial views partial commands based. Additionally, categorizability implies
rules necessary controller mostly lower order
easily exploited bias search space partial rules. So, environment
categorizable, use partial-rule approach suppose important increase
learning speed reduction use memory respect traditional
non-generalizing reinforcement-learning algorithms.
following sections, describe possible estimate effect
action given fixed set partial rules. evaluation, repeated actions, used
determine best action executed given moment. Next, detail
possible adjust value predictions fixed set partial rules. Finally, describe
categorizability assumption allows us use incremental strategy generation
new partial rules. strategy results faster learning existing generalizing
non-generalizing reinforcement-learning algorithms. procedures described highlevel form make explanation clear. Details implementation found
Appendix A.
4.1 Value Prediction using Partial Rules
given situation, many partial views simultaneously active triggering subset
partial rules controller C. call subset active partial rules denote
C 0 . evaluate given action take account rules C 0
partial command accordance a. denote subset C 0 (a). Note that,
approach, refer action, mean corresponding set elementary actions
(one per motor) single element, general case reinforcement learning.
85

fiPorta & Celaya

Every rule w = (v, c) C 0 (a) provides value prediction a: qw associated
partial rule. averaged value provides information accuracy
prediction. also pointed Wilson (1995), favor use partial
rules high accuracy value prediction or, say it, rules high relevance.
seems clear relevance rule (w ) depends distribution values
around qw . Distributions low dispersion indicative coherent value predictions
and, so, highly relevant rule. measure dispersion maintain error estimation
ew approximation qw . Another factor (not used Wilson, 1995) taken
account relevance determination confidence qw ew statistics: low
confidence (i.e., insufficiently sampled) measures qw ew reduce relevance
rule. confidence value prediction given rule (cw ) number
interval [0, 1], initialized 0, increasing partial rule used (i.e., rule
active partial command executed). confidence would decrease
value model given partial rule consistently wrong.
Using confidence, approximate real error value prediction partial
rule w
w = ew cw + e (1 cw ),
value e average error value prediction. Observe importance
e reduced confidence increases and, consequently, w converges ew .
definitions, relevance partial rule defined
w =

1
.
1 + w

Note exact formula relevance important far w1 w2
w1 w2 . formula provides value range [0, 1] could directly
used scale factor, necessary.
problem then, derive single value prediction using qw statistics
rules C 0 (a) corresponding relevance value, w ? Two possible solutions
come mind: using weighted sum values predicted partial rules using
relevance weighting factor, using competitive approach,
relevant partial rule used determine predicted value. weighted sum assumes
linear relation inputs (the value prediction provided individual rule)
output (the value prediction a). assumption proved powerful many
systems but, general, compatible categorizability assumption since,
although one partial rules involved sum low order, taking
account means using large set different feature detectors elementary
actions predict effect given action. reason, learning system uses
winner-take-all solution value prediction relevant partial rule
taken account predict value action. So, action determine
winner rule
w =winner (C 0 , a) =arg

max {w0 },

w0 C 0 (a)

use range likely value rule, Iw = [qw 2w , qw + 2w ], randomly
determine value prediction action a. probability distribution inside interval
depends distribution assume value.
86

fiReinforcement Learning Categorizable Environments

procedure outlined used time step obtain value prediction
action. action maximal value one want robot execute
next.
Observe obtain probabilistic value prediction: situation
statistics, get different value predictions action. way,
action obtains maximal evaluation always one maximal q w
and, consequently, favor exploration promising actions. probabilistic action selection provides exploratory mechanism uses information typical
reinforcement-learning exploration mechanisms (the error confidence value predictions available reinforcement-learning algorithms) result
sophisticated exploration schema (see Wilson, 1996, survey different exploration
mechanisms reinforcement learning).
4.2 Partial Rules Value Adjustment
adjust value predictions rules C 0 (a) last executed action.
rule adjusted, update qw , ew , cw statistics.
effect action accordance partial command c attending
partial rule w = (v, c) defined (using Bellman-like equation)

qw
= rw +

X

p(w, C 0 ) v (C 0 ),

C 0

r w average reward obtained immediately executing c v observed,
discount factor used balance importance immediate respect delayed
reward, v (C 0 ) represents goodness (or value) situation rules C 0 active,
p(w, C 0 ) probability reaching situation execution c v
observed. value situation assessed using best action executable
situation

v (C 0 ) = max
{qw
|w = winner(C 0 , a0 )},
0


since gives us information well robot perform (at most)
situation.
many existing reinforcement-learning approaches, values q w ew
rules adjusted modified using temporal difference rule
error measure. Rules direct relation
progressively approach qw
received reward would provide value prediction (qw ) coherent actually
obtained one and, consequently, statistics adjustment, prediction error
decreased. Contrariwise, rules related observed reward would predict value
different obtained one error statistics increased. way,
rule really important generation received reward, relevance increased
decreased. Rules low relevance chances used drive
robot and, extreme cases, could removed controller.
confidence cw also adjusted. adjustment depends confidence measured. related number samples used qw ew
statistics, cw simply slightly incremented every time statistics rule w
87

fiPorta & Celaya

updated. However, also decrease confidence value model given partial
rule consistently wrong (i.e., value observed systematically interval w ).
Observe learning rule equivalent used state-based reinforcementlearning methods. instance, Q-learning (Watkins & Dayan, 1992), Q (s, a),
state action, defined
X
p(s, a, s0 ) V (s0 ),
Q (s, a) = r w +
s0

p(s, a, s0 ) probability transition s0 executed
V (s0 ) = max
{Q (s0 , a0 )}
0


approach, set rules active given situation C 0 plays role state
instead
and, thus, v (C 0 ) V (s0 ) equivalent. hand, estimate qw

Q (s, a), rule w includes information (partial) state actions
Q (s, a) play similar role. value prediction given rule, q ,
making qw
w
corresponds average value predictions cells Cartesian product
feature detectors elementary actions covered rule. case complete
rules (i.e., rules involving feature detectors actions motors), sub-area
covered rule includes one cell Cartesian product and, therefore,
controller includes complete rules, described learning rule exactly
used Q-learning. particular case, C 0 (a) one rule that, consequently,
winner rule. statistics rule (and updated
way) Q(s, a) entry table used Q-learning. Thus, learning rule
generalization learning rule normally used reinforcement learning.
4.3 Controller Initialization Partial Rule Creation/Elimination
Since assume working categorizable environment, use incremental
strategy learn adequate set partial rules: initialize controller rules
lowest order generate new partial rules necessary (i.e., cases
correctly categorized using available set rules). So, initial controller contain,
instance, rules order two include one feature detector one elementary
action ((v(fdi ), c(aej )), (v(fdi ), c(aej )) i, j). case, sensible include
empty rule (the rule order 0, w ) initial controller. rule always active
provides average value average error value prediction. Additionally,
knowledge user task achieved easily introduced initial
controller form partial rules. available, estimation value predictions
user-defined rules also included. hand-crafted rules (and value
predictions) correct learning process accelerated. correct,
learning algorithm would take care correcting them.
create new rule large error value prediction detected. new
rule defined combination two rules C 0 (a), rules forecast
effects last executed action, a, current situation. selecting couple
rules combined, favor selection value prediction close
88

fiReinforcement Learning Categorizable Environments

actually observed one, since likely involve features elementary actions
(partially) relevant value prediction try refine.
problem possible determine priori whether incorrectly
predicted value would correctly predicted rule adjustments really
necessary create new partial rule account received reward. So, create new
rules large error value prediction, possible create unnecessary
rules. existence (almost) redundant rules necessarily negative, since
provide robustness controller, called degeneracy effect introduced Edelman
(1989). must avoided generate rule twice, since useful
all. Two rules identical respect lexicographic criteria (they contain
feature detectors elementary actions) also respect semantic ones (they
get active situations propose equivalent actions). identical rules
created, detected removed soon possible. Preserving
rules proved useful avoids number rules controller growing
reasonable limit.
Since create new rules significant error value prediction,
necessary, could end generating complete rules (provided limit
number rules controller). case, assuming specific
rule accurate value prediction, system would behave normal tablebased reinforcement-learning algorithm: specific rules (i.e., relevant
ones) would used evaluate actions and, explained before, statistics
rules would exactly table-based reinforcement-learning algorithms.
Thus, limit, system deal type problems non-generalizing
reinforcement-learning algorithms. However, regard limit situation improbable impose limits number rules controllers. Observe
asymptotic convergence table-based reinforcement learning possible
use winner-takes-all strategy action evaluation. weighted-sum strategy,
value estimation non-complete rules possibly present controller would
added complete rules leading action evaluation different
table-based reinforcement-learning algorithms.

5. Partial Rule Approach Context
categorizability assumption closely related complexity theory principles
Minimum Description Length (MDL) used authors Schmidhuber (2002) bias learning algorithms. complexity results try formalize
well-known Occams Razor principle enforces choosing simplest model
set otherwise equivalent models.
Boutilier, Dean, Hanks (1999) presents good review representation methods
reduce computational complexity planning algorithms exploiting particular
characteristics given environment. representation based partial rules seen
another representation systems. However, partial rule representation
formalism that, without bias introduced categorizability assumption, would
efficient enough applied realistic applications.
89

fiPorta & Celaya

partial-rule formalism seen generalization XCS classifier
systems described Wilson (1995). XCS learning system aims determining set
classifiers (that combinations features associated action) associated value relevance predictions. main difference approach
Wilsons work pursues generic learner bias learning process using
categorizability assumption. allows us use incremental rule-generation strategy
likely efficient robotic problems. Additionally, categorizability assumption also modifies way value given action evaluated: Wilsons
approach uses weighted sum predictions classifier advocating action
determine expected effect action, while, fulfill categorizability assumption (i.e., minimize number feature detectors elementary actions involved
given evaluation), propose use winner-takes-all strategy. critical point
since winner-takes-all strategy takes full advantage categorizability assumption
allows partial-rule system asymptotically converge table-based
reinforcement-learning system. case weighted sum strategy used.
Furthermore, XCS formalism generalization action space and,
already commented, requirement robotic-like applications.
general, reinforcement learning pay attention necessity generalizing
space actions, although exceptions exists. instance, work Maes
Brooks (1990) includes possible execution elementary actions parallel. However
system include mechanism detecting interactions actions and,
thus, coordination actions relies sensory conditions. instance, system
difficulties detecting execution two actions results always (i.e., independently
active/inactive feature detectors) positive/negative reward.
CASCADE algorithm Kaelbling (1993) learns bit complex action
separately. algorithm presents clear sequential structure learning
given action bit depends previously learned ones. approach
predefined order learning outputs result flexible learning
schema.
multiagent learning (Claus & Boutilier, 1998; Sen, 1994; Tan, 1997) objective
learn optimal behavior group agents trying cooperatively solve given
task. Thus, field, case, multiple actions issued parallel considered. However, one main issues multiagent learning, coordination
different learners irrelevant case since one learner.
Finally, way define complex actions elementary actions
points common works reinforcement learning macro-actions defined
learner confronts different tasks (Sutton, Precup, & Singh, 1999; Drummond, 2002).
However, useful combinations elementary actions detected algorithm
guaranteed relevant task hand (although likely also relevant
related tasks).

6. Experiments
show results applying learning algorithm two robotics-like simulated problems: robot landmark-based navigation legged robot walking. first problem
90

fiReinforcement Learning Categorizable Environments

Flowers

Bushes

Boat

Tree

Lake

Goal





A7
A6



A5






A4



ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff




ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff


ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff
ff

fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi
A1
fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi
A2


A3
fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi
fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi








fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi




Rock

Bushes

Start

North

Bush

Figure 1: Landscape simple landmark-based navigation task. landscape divided areas (the dashed ovals) subsets landmarks visible.

simpler (although includes delayed reward) use clearly describe
workings algorithm. second problem approaches realistic robotic application,
objective long term. use two examples compare performance
learning system generalizing non-generalizing reinforcement-learning
algorithms. confronted problems different enough show generality
proposed learning system.
6.1 Simulated Landmark-Based Navigation
confront simple simulated landmark-based navigation task forest-like environment shown Figure 1. objective learner go start position
(marked cross bottom figure) goal position
food (marked cross top right corner environment). agent
neither walk lake escape depicted terrain.
agent make use binary landmark (i.e., feature) detectors identify
position environment decide action execute next. example,
landmark detectors agent are:
1. Rock detector: Active rock seen.
2. Boat detector: Active boat seen.
3. Flower detector: Active bunch flowers seen.
91

fiPorta & Celaya

4. Tree detector: Active tree seen.
5. Bush detector: Active whenever bush seen.
6. Water detector: Active water nearby.
7. Bird detector: Active bird flying agent.
8. Cow detector: Active cow nearby.
9. Sun detector: Active sun shining.
10. Cloud detector: Active cloudy.
detectors, first 5 relevant task. water detector always
active, rest landmark detectors become active random. 10 landmark
detectors differentiate 210 = 1024 situations.
simplify problem clustering possible positions learner environment 7 areas (shown Figure 1): area includes positions
set relevant landmarks seen.
far actions concerned, use three actions West-East movement
robot: move West (denoted W ), stay place (), move East (E).
three indicate movement along North-South dimension (move North
N , stay latitude , move South S). two independent groups
three actions combined giving rise 9 different actions (move North-West, North,
North-East, etc.). assume agent executes one actions,
stop nearest area terrain direction movement reached.
agent tries move lake terrain, remains
position was. Figure 1 shows possible transitions contiguous areas
environment.
described landmark detectors elementary actions maximum possible order given rule 12, define 944784 (310 42 ) syntactically different
partial rules. taking account rules one feature detector one elementary action (that ones initially included controller) 90 different
partial rules.
agent receives reward (with value 100) reaches goal. Consequently,
problem delayed reward since agent must transmit information provided reward signal actions situations directly related
observation reward.
parameters partial-rule learning algorithm used task = 0.9,
= 0.99, = 5, = 0.1, = 5, = 200 and, = 0.95 (see Appendix detailed
description parameters). Observe that, maximum number partial rules
= 200 initial controller containing 90 rules, little room left generation
rules order higher 2.
learning organized sequence trials. trial consists placing
learner starting position letting move goal reached, allowing
execution 150 actions reach goal. performing optimally, three
actions required reach objective starting position.
92

fiReinforcement Learning Categorizable Environments

180
160

Steps Goal

140
120
100
80
60
40
20
0

0

50

100

150

200

250

Trial
PR Algorithm

XCS

Figure 2: Performance landmark-based navigation task. Results shown average 10 runs.

Figure 2 shows that, 40 learning trials, agent approaches optimal behavior
(represented flat dashed line = 3).
dashed line Figure 2 performance XCS problem. perform
test, used implementation Wilsons XCS developed Butz (1999).
make XCS work search space partial-rule algorithm, modified
XCS implementation able deal non-binary actions. modification,
parameter adjustment, introduced original code. results presented
corresponds average 10 runs using set parameters gave better
result. Nominally, parameters were: learning rate = 0.1, decay rate = 0.9,
maximum number classifiers = 200 (however, initial set empty), genetic
algorithm applied average every 5 time steps, deletion experience 5, subsume
experience 15, fall rate 0.1, minimum error 0.01, prediction threshold
0.5, crossover probability 0.8, mutation probability 0.04 initial dont
care probability 1/3. prediction fitness new classifiers initialized 10
error 0. detailed explanation meaning parameters provided
Wilson (1995) also comments code Butz (1999).
see XCS reaches performance partial-rule approach,
using four times trials. difference performance partially explained
XCSs lack generalization action space. However factor relevant
case since action space two dimensions. main factor explains
better performance partial-rule approach bias introduced categorizability
93

fiPorta & Celaya


1

Position
A2

V
81

2

A4

90

3

A6

100

Action
(W, N )
(W, )
(, N )
(E, N )
(E, )
(E, )

Winner Rule
w1 = (v(Rock, Boat), c(W, N ))
w2 = (v(Rock, W ater), c(W ))
w3 = (v(Boat, ree), c(N ))
w4 = (v(T ree), c(E, N ))
w5 = (v(Rock, Boat), c(E))
w6 = (v(Bush), c(E, ))

qw
80.63
79.73
89.61
90.0
86.71
100.0

ew
1.16
2.19
2.04
0.0
4.58
0.0

Guess
79.87
77.65
88.88
89.86
79.56
99.87

Table 1: Partial execution trace landmark-based navigation task. Elementary action means movement along corresponding dimension. time step
t, action highest guess executed. time step 3, goal
reached.

assumption present XCS system that, case, allows
efficient learning process. XCS powerful partial-rule approach sense
XCS makes assumption categorizability environment,
assume high. result XCS learning process includes identification
degree categorizability environment case is, sense,
pre-defined. generality XCS, however, produces slower learning process.
initialize classifiers XCS high dont care probability initialize
rules partial-rule algorithm generalization used action space
(i.e., rules include command motor), two systems become closer.
case, main (but only) difference two approaches
assumption relation inputs value: XCS assumes linear
relation, assume environment categorizable, or, same, assume
value depend inputs. Due difference, confronted
problem, two systems would learn policy values
action, values would computed using different rules different associated
values, independently parameter/rule initialization used case.
system smaller learning time would assumption closer
reality. results obtained particular example presented show
categorizability assumption valid hypothesis would case
robotics-like applications.
Table 1 shows evaluation actions different situations agent encounters path start goal 50 learning trials. Analyzing trace,
extract insight partial-rule learning algorithm works.
instance, time step 1, see rule w2 = (v(Rock, W ater), c(W )) used
determine value action (W, ). Since landmark detector Water always active,
rule equivalent w = (v(Rock), c(W )), one rules used generate w 2 .
examine statistics w find qw = 74.70 ew = 15.02. Obviously,
value distributions qw qw2 look different (74.70 vs. 79.73 15.02 vs. 2.19).
w2 generated later stages learning and, thus, statistics
updated using subsample values used adjusts statistics w.
94

fiReinforcement Learning Categorizable Environments

particular case, qw updated 250 times qw2 updated 27
times. learning continues, distributions become similar rule w 2
eventually eliminated.
Table 1, see sometimes non-optimal actions get
evaluation close optimal ones. reason, agent executes, times,
non-optimal actions increases number steps necessary reach goal.
general, adjustment statistics rules solve problem but,
particular case, need create new rules fix situation. instance, time step 2,
value rule w4 increased towards 90, value rules active time step
proposing actions accordance action rule w4 also converge toward 90.
So, long term, rule proposing action (N ) get value close 90.
absence specific rules, rule used estimate value action
(, N ) and, due probabilistic nature action selection procedure,
action can, eventually, executed delaying agent reaching goal 1 time
step. However, execution (, N ) results error value prediction and, thus,
creation new rules better characterize situation. soon specific rule
action (, N ) generated, error longer repeated.
time step 3, see rule w6 = (v(Bush), c(E, )) value 100 error
0 guess rule 99.87. maximum confidence () lower
1.0 (0.99 case) makes agent keep always certain degree
exploration.
agent receives reward task totally achieved, function value
situation computed V (s) = n1 r n distance (in actions)
situation target one r reward finally obtained. Table 1, see
situations get correct evaluation: 80.63( 81 = 100 0.9 2 ) A2, 90(= 100 0.9)
A4, 100 A6.
Observe problem solved using 200 partial rules 9216
possible situation-action combinations domain. So, say problem
certainly categorizable. main conclusion extract toy example
that, particular case confronted problem categorizable, presented
algorithm able determine relevant rules adjust values (including
effect delayed reward) optimal action determined
situation.
6.2 Gait Generation Six-Legged Robot
also applied algorithm task learning generate appropriate gait (i.e.,
sequence steps) six-legged robot (Figure 3). apply learning algorithm
real robot would possible, dangerous: initial phases learning robot
would fall many times damaging motors. reason used simulator
learning and, afterward, applied learned policy real robot.
problem learning walk six legged robot chosen many authors
paradigmatic robotic-learning problem. instance, Maes Brooks (1990)
implemented specific method based immediate reward derive preconditions
leg perform step. Pendrith Ryan (1996) used simplified version
95

fiPorta & Celaya

six-legged walking problem test algorithm able deal Non-Markovian spaces
states Kirchner (1998) presented hierarchical version Q-learning learn
low-level movements leg, well coordination scheme low-level
learned behaviors. Ilg, Muhlfriedel, Berns (1997) introduced learning architecture
based self-organizing neural networks, Kodjabachia Meyer (1998) proposed
evolutionary strategy develop neural network control gait robot. Vallejo
Ramos (2000) used parallel genetic algorithm architecture Parker (2000) described
evolutionary computation robot executes best controller found
given moment new optimal controller computed off-line simulation.
algorithms usually tested flat terrain aim generating periodic gaits (i.e.,
gaits sequence steps repeated cyclically). However, general locomotion
(turns, irregular terrain, etc) problem free gait generation needs considered.

Figure 3: Genghis II walking robot 2D simulation environment.
simulator (see Figure 3) allows controller command leg robot
two independent degrees freedom (horizontal vertical) able detect
robot unstable position (in robot happens two neighboring legs
air simultaneously). Using simulator, implemented behaviors described
Celaya Porta (1996) except charge gait generation. Therefore,
task learned consists deciding every moment legs must step (that is, leave
ground move advanced position), must descend stay
ground support propel body.
defined set 12 feature detectors that, due experience legged robots,
knew could useful different situations gait-generation task:
air(x): Active leg x air.
Advanced(x): Active leg x advanced neighboring leg clockwise
circuit around robot.
Attending activation non-activation 12 feature detectors,
differentiate 4096 different situations.
action side, work two different elementary actions per leg: one
issues step leg another descends leg touches ground.
96

fiReinforcement Learning Categorizable Environments

Thus, cardinality set elementary actions 12 and, time step, robot
issues action containing 6 elementary elements (one per leg). Thus, think
leg virtual motor accepts two possible values, 0 remain contact
ground 1 perform step.
reward signal includes two aspects:
Stability: action causes robot fall down, reward 50 given.
Efficiency: robot fall down, reward equal distance
advanced robot given. Observe legs descend recover contact
ground advance robot obtained movement necessary
able get reward next time steps. So, problem delayed
reward.
efficient stable gait tripod gait two sets three non-adjacent
legs step alternately. Using gait, robot would obtain reward 0 (when one group
three legs lifted advanced) followed reward 50 (when legs contact
ground move backward reaction advance legs moved previous
time step). Thus, optimal average reward 25.
experiments, robot set initial posture legs contact
ground random advance position.
Figure 4 shows results applying partial-rule algorithm compared obtained using standard Q-learning 4096 distinct states 64 different actions.
partial-rule algorithm, used following set parameters: = 0.2, =
0.99, = 22, = 0.1, = 150, = 10000 and, = 0.95 (see Appendix description
parameters). Q-learning, learning rate set = 0.5 use
action selection rule performs exploratory actions probability 0.1.
Figure 4, see stability subproblem (i.e., falling down,
corresponds getting reward greater zero) learned quickly. because,
stability subproblem, take advantage generalization provided using
separate elementary actions and, single rule, avoid executing several dangerous
actions. However, advance subproblem (i.e., getting reward close 25) learned
slowly. little generalization possible learning system must generate
specific rules. words, sub-problem less categorizable stability
one.
landmark-based navigation example discussed previous section,
observe controller contains (slightly) overly general rules responsible
non optimal performance robot. However, dont regard problem
since interested efficiently learning correct enough policy
frequent situations finding optimal behaviors particular cases.
Figure 5 shows performance Q-learning longer run using different exploration rates. shows Q-learning eventually converge optimal policy
many iterations approach (about factor 10). Observe lower
exploration rate allows algorithm achieve higher performance (around 19
learning rate 0.1 around 24 learning rate 0.01) using longer period.
careful adjustment exploration rate combine initial faster learning
97

fiPorta & Celaya

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

1000

2000
3000
Time Slice

PR Algorithm

4000

5000

QLearning

Figure 4: Performance partial-rule approach compared standard Q-learning.
Results smoothed average 10 experiments.

better convergence long term. Experiments Q-learning using learning rates
0.5 showed insignificant differences compared results shown here.
advantage algorithm non-generalizing ones increased problems
sensors provide information related task. test point,
set experiment 6 feature detectors become active randomly
added 12 initial ones. new features, number possible combinations
feature activations increases, number states considered Q-learning.
Figure 6 shows comparison algorithm Q-learning problem.
Q-learning able learn reasonable gait strategy 5000 time steps shown
figure, performance partial-rule algorithm almost
before. means partial-rule algorithm able detect sets features
relevant use effectively determine robots behavior. remarkable
that, case, ratio memory used algorithm respect used
non-generalizing algorithms 0.2%. exemplifies performance
non-generalizing algorithms degrades number features increases,
necessarily case using partial-rule approach.
importance generation partial rules improvement categorization seen comparing results obtained problem without
mechanism (Figure 7). results show task cannot learned using
partial rules order 2. aspect gait-generation problem learned
rules order 2 avoid lifting leg one neighboring legs already
98

fiReinforcement Learning Categorizable Environments

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

50000

Exploration 0.1

100000
150000
Time Slice

200000

Exploration 0.01

250000

References

Figure 5: Performance Q-learning algorithm different exploration rates.
reference values 19 24 upper bound performance attainable
using exploration rate 0.1 0.01.

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

1000

2000
3000
Time Slice

PR Algorithm

4000

5000

QLearning

Figure 6: Performance algorithm compared Q-learning irrelevant
features.

99

fiPorta & Celaya

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

1000

2000
3000
Time Slice

Without Generation

4000

5000

Generation

Figure 7: Performance without partial-rule generation procedure.
air. instance, rule
v(In air(1)) c(Step(2)),
forecasts highly relevant negative reward prevents leg 2 raised
leg 1 air.
Rules order higher 2 (i.e., provided robot initial controller)
necessary, instance, avoid raising two neighboring legs simultaneously. rule like
v(In air(1)) c(Step(1), Step(2))
becomes active robot evaluates action implies raising leg 1 leg 2
time. Since value prediction rule negative relevance
high, action evaluation would discarded, preventing robot falling
down. Similar rules generated pair neighboring legs. make
robot advance, need generate rules even higher order.
Figure 8, see performance algorithm start learning
process correct rule set (i.e., rule set learned previous experiment),
statistics initialized 0. experiment, compare complexity
learning values rules compared complexity learning rules
value time. see values rules need
learned learning process two times faster normal application
algorithm.
final experiment, issue frequent changes heading direction robot
(generated randomly every 10 time steps). way, periodic gaits become suboptimal
100

fiReinforcement Learning Categorizable Environments

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

500

1000

1500

2000
2500
Time Slice

PR correct rule set

3000

3500

4000

PR Algorithm

Figure 8: Performance partial-rule approach learning started correct
rule set compared standard approach rules also learned.

controller produce free gait, i.e., gait includes sequence steps
without periodic repetition.
case, focus advance subproblem and, thus, introduced handcrafted rules initial controller prevent robot falling down. rules
form:
leg lifted execution action results value 50 confidence 1,
actions lift one two legs contiguous i.
set parameters used case was: = 0.2, = 0.99, = 5, = 0.1,
= 150, = 10000 and, = 0.95.
Figure 9 shows average results obtained using partial-rule learning algorithm
compared obtained best hand-coded gait-generation strategy. figure, horizontal dashed line shows average performance using best gait-generation
strategy implemented (Celaya & Porta, 1998). seen learned gaitgeneration strategy (the increasing continuous line) produces performance similar
best hand-coded strategy that, cases, even outperforms it. Figure 10
shows situation learned controller produces better behavior hand
coded one. Using hand-coded strategy, robot starts walk raising two legs (3
6) and, time steps reaches state tripod gait generated. Initially,
leg 2 advanced legs 1 4 and, general, suboptimal execute step
leg neighboring legs less advances itself. particular case
however, general rule hold. learned strategy detects exception
101

fiPorta & Celaya

25

Average Reward

20
15
10
5
0

0

1000

2000
3000
Time Slice

PR Algorithm

4000

5000

Hand Coded

Figure 9: Performance partial-rule approach learning free gait.
generates tripod gait beginning resulting larger advance robot
initial stages movement.

7. Conclusions
paper, introduced categorizability assumption states robot
driven achieve given task using simple rules: i.e., rules including reduced
set feature detectors elementary actions. assumption supported
experience within behavior-based approach controllers formed sets rules
relatively simple conditions actions. shown learning algorithm
based categorizability assumption allows large speed learning process
many realistic robotic applications respect existing algorithms.
exploit categorizability assumption observations action spaces,
introduced new representation formalism based concept partial rules
concepts independent states independent actions kernel
many existing reinforcement-learning approaches.
introduction partial-rule concept provides large flexibility problems
formalized. structure algorithms, confront problems
generalization perception side (usually considered reinforcement learning),
action side (usually considered), them.
generalization possible via partial rules, use complete rules:
rules involving available inputs outputs. case, partial-rule approach
equivalent non-generalizing reinforcement learning. algorithm presented
102

fiReinforcement Learning Categorizable Environments

Leg Numbering
1
2

0

0

Step 3,6

Step 2,3,6

25

32

3

4

5

6

Step 1,4,5

Step 1,4,5

59

82

Step 2,3,6

Step 2,3,6

109

132

Figure 10: hand-programmed gait strategy (top sequence) vs. learned one (bottom
sequence). advance position robot snapshot indicated
picture.

can, necessary, generate complete rules and, consequently, can, principle, solve
problem solved using traditional reinforcement-learning algorithm. However,
take categorizability assumption valid so, generation complete rules
extreme case likely occur limit situation. Therefore,
approach, forego generality order increase efficiently learning process
class problems want address.
Another advantage partial-rule framework allows easy robust
introduction initial knowledge learning process form rules easily understood programmer. contrast usual reinforcement-learning
algorithms introduction initial knowledge is, general, rather difficult.
partial-rule approach, subtle change emphasis main goal
learning: work reinforcement learning emphasis learning
value action state, main purpose learn relevance (subsets of)
elementary actions feature detectors. relevant subsets elementary actions
feature detectors identified, learning becomes straightforward.
103

fiPorta & Celaya

main limitation work possible know priori (except trivial cases) whether environment categorizable given robot. Non-generalizing
reinforcement learning implicitly assumes environment non-categorizable
that, consequently, possible combination features actions taken
account separately. approach assumes opposite: environment
categorizable and, so, reduced combinations features actions need taken
account. drawback using non-generalizing approach robotic tasks
become intractable curse dimensionality. generalization techniques
problem partially alleviated, enough general. approach take
radical approach order much less affected curse dimensionality:
introduce strong bias learning process drastically limit use combinations
features actions.
tested partial-rule learning algorithm many robotic-inspired problems
two discussed paper (landmark based-navigation sixlegged robot gait generation) categorizability assumption proved valid
cases tested. algorithm out-performs generalizing non-generalizing reinforcementlearning algorithms memory requirements convergence time. Additionally,
shown approach scales well number inputs increases,
performance existing algorithms largely degraded. important result
lets us think could possible use approach control complex robots,
use existing approaches discarded.
work presented paper, extract two main proposals. First,
apply reinforcement learning agents many sensors actuators,
concentrate efforts determining relevance inputs outputs and, second,
achieve efficient learning complex environments could necessary introduce
additional assumptions reinforcement-learning algorithms, even risk losing
generality.

Acknowledgments
authors would like express gratitude anonymous reviewers paper.
contributions toward improving quality paper relevant enough
considered, sense, co-authors paper. shortcomings still paper
attributed nominal authors.
second author partially supported Spanish Ministerio de Ciencia Tecnologa FEDER funds, project DPI2003-05193-C02-01 Plan
Nacional de I+D+I.

104

fiReinforcement Learning Categorizable Environments

Appendix A: Partial-Rule Learning Algorithm
appendix, describe detail approach described main body
paper.

Partial Rule Learning Algorithm
(Initialize)
F Set features detectors
EA Set elementary actions
C {w } {(v(fd), c(ea)), (v(fd), c(ea))|fd F D, ea EA}
w C
qw 0
ew 0
iw 0
endfor
e0
episode
C 0 {w C|w active}
Repeat (for step episode):
(Action Selection)
Action Evaluation
(Computes guess(a0 ) a0 )
0
arg max
{guess(a )}
0


Execute
(System Update)
ra Reward generated
0
Cant
C0
0
C {w C|w active}
Statistics Update
Partial-Rule Management
terminal situation
enddo

Figure 11: partial-rule learning algorithm. Text inside parentheses comments.
Action Evaluation, Statistics Update, Partial-Rule Management procedures
described next.

partial-rule learning algorithm (whose top level form shown Figure 11) stores
following information partial rule
value (i.e., discounted cumulative reward) estimation qw ,
error estimation ew ,
confidence index iw .
105

fiPorta & Celaya

1.0
0.9





0.8
0.7

cw

0.6
0.5
0.4
0.3
0.2
0.1
1

2

3

4

5

iw

6

7



8

9

10

Figure 12: Confidence function =7 =0.8.
estimate confidence qw ew use confidence index iw that, roughly
speaking, keeps track number times partial rule used. confidence
derived iw using confidence function following way:
cw =confidence function(iw ),
confidence function non-decreasing function range [0, ].
less 1 since, way, system always keeps certain degree exploration and,
consequently, able adapt changes environment. Different confidence schemes
implemented changing confidence function. implementation, use
sigmoid-like function (see Figure 12) increases slowly low values w reducing
confidence provided first obtained rewards. way avoid premature
increase confidence (and, thus, decrease error exploration)
insufficiently-sampled rules. parameter () determines point function
reaches top value .
Additionally, confidence index used define learning rate (i.e., weight
new observed rewards statistics update). purpose implement MAM
function (Venturini, 1994) rule:
mw = max{, 1/(iw + 1)}.
Using MAM-based updating rule, that, lower confidence, higher
effect last observed rewards statistics, faster adaptation
statistics. adaptive learning rate strategy related presented Sutton (1991)
Kaelbling (1993), contrasts traditional reinforcement-learning algorithms
constant learning rate used.
initialization phase, algorithm enters continuous loop task
episode consisting estimating possible effects actions, executing promis106

fiReinforcement Learning Categorizable Environments

Action Evaluation
action a0
w winner(C 0 , a0 )
guess(a0 ) qw + 2 random(w , w )
endfor

Figure 13: Action Evaluation procedure.
ing one, updating system performance improves future. system
update includes statistics update partial-rule management.
Action Evaluation
simplest procedure get estimated value actions brute-force approach
consisting independent evaluation one them. simple cases, approach
would enough but, number valid combinations elementary actions (i.e.,
actions) large, separate evaluation action would take long time, increasing
time robot decision decreasing reactivity control. avoid this,
Appendix B presents efficient procedure get value action.
Figure 13 summarizes action-evaluation procedure using partial rules. value
action guessed using relevant rule action (i.e., winner rule).
winner rule computed
winner (C 0 , a) =arg

max {w },

wC 0 (a)

w relevance rule w
w =

1
.
1 + w

value estimation using winner rule selected random (uniformly)
interval
Iw = [qw 2w , qw + 2w ],

w = ew cw + e (1 cw ).
Here, e average error value prediction (i.e., value error prediction
empty rule, w ).
Statistics Update
statistics-update procedure (Figure 14), qw ew adjusted rules
active previous time step proposed partial command accordance
(the last executed action).
107

fiPorta & Celaya

Statistics Update
terminal situation
v0
else
v max
{qw |w = winner(C 0 , a0 )}
0


endif
q ra + v
0
w = (v, c) Cant
c accordance
q Iw
iw iw + 1
else
iw min( 1, iw 1)
endif
qw qw (1 mw ) + q mw
ew ew (1 mw ) + |qw q| mw
endif
endfor
e ew

Figure 14: Statistics update procedure.

qw ew updated using learning rate (mw ) computed using MAM
function, initially 1, consequently, initial values qw ew
influence future values variables. initial values become relevant
using constant learning rate, many existing reinforcement-learning algorithms do.
observed effects last executed action agree current estimated
interval value (Iw ), confidence index increased one unit. Otherwise,
confidence index decreased allowing faster adaptation statistics last
obtained, surprising values reward.
Partial-Rule Management
procedure (Figure 15) includes generation new partial rules removal
previously generated ones proved useless.
implementation, apply heuristic produces generation new partial
rules value prediction error exceeds e. way, concentrate efforts
improve categorization situations larger errors value prediction.
Every time wrong prediction made, new partial rules generated
0 (a). Recall set includes
combination pairs rules included set Cant
rules active previous time step accordance executed action a. Thus,
rules related situation-action whose value prediction need
improve.
108

fiReinforcement Learning Categorizable Environments

combination two partial rules w1 w2 consists new partial rule partial
view includes features included partial views either w1 w2
partial command includes elementary actions partial commands either
w1 w2 . words, feature set w1 w2 union feature sets w1
w2 elementary actions w1 w2 union w1
0 (a), simultaneously active
w2 . Note that, since w1 w2 Cant
accordance action and, thus, incompatible
(i.e., include inconsistent features elementary actions).
partial-rule creation, bias system favor combination rules
(wi ) whose value prediction (qwi ) closer observed one (q). Finally, generation
rules lexicographically equivalent already existing ones allowed.
According categorizability assumption, low-order partial rules required
achieve task hand. reason, improve efficiency, limit number
partial rules maximum . However, partial-rule generation procedure always
generating new rules (concentrating situations larger error). Therefore,
need create new rules room them, must eliminate less useful
partial rules.
partial rule removed value prediction similar rule
situations.
similarity two rules measured using normalized degree intersection value distributions number times rules used
simultaneously:
similarity(w, w 0 ) =

U (w w0 )
kIw Iw0 k
,
max{kIw k, kIw0 k} min{U (w), U (w 0 )}

U (w) indicates number times rule w actually used.
similarity assessment pair partial rules controller expensive
and, general, determining similarity rule respect
generated (that rules tried refine new rule created)
sufficient. Thus, based similarity measure, define redundancy
partial rule w = (w1 w2 ) as:
redundancy(w) = max{similarity(w, w1 ), similarity(w, w2 )}.
Observe w = (w1 w2 ), w w1 = w U (w) U (w1 ).
Therefore
U (w w1 )
U (w)
U (w)
=
=
= 1.
min{U (w), U (w1 )}
min{U (w), U (w1 )}
U (w)
reasoning done w2 and, consequently,
redundancy(w) = max{

kIw Iw2 k
kIw Iw1 k
,
}.
max{kIw k, kIw1 k} max{kIw k, kIw2 k}

need create new rules maximum number rules ()
reached, partial rules redundancy given threshold () eliminated.
Since redundancy partial rule estimated observing number
109

fiPorta & Celaya

Partial Rule Management
0
w winner(Cant
, a)
|qw q| > e
(If time create new rules)
(Partial Rule Elimination)
(Test room new rules)
kCk >
(Rule elimination based redundancy)
C C {w C | redundancy(w) > }
(Rule elimination based creation error)
kCk > (If still room)
SC partial rules C with:
- Lowest creation error(w),
- creation error(w) < |qw q|
C C SC
endif
endif
(Partial Rule Generation)
t0
kCk < <
(Create new rule w 0 )
0
(a)
Select two different rules w1 , w2 Cant
preferring minimize
|qwi q| cwi + e (1 cwi )
w0 = (w1 w2 )
creation error(w 0 ) |qw q|
(Insert new rule controller)
C C {w 0 }
tt+1
endwhile
endif

Figure 15: Partial Rule Management procedure. value q calculated Statistics
Update procedure last executed action.

times, redundancy partial rules low confidence indexes set 0,
immediately removed creation.
Observe that, compute redundancy rule w, use partial rules
w derived. reason, rule w 0 cannot removed controller C
exists rule w C w = w 0 w00 . Additionally, way eliminate
first useless rules higher order.

110

fiReinforcement Learning Categorizable Environments

Appendix B: Efficient Action Evaluation
non-generalizing reinforcement learning cost executing single learning step
neglected. However, algorithms generalization spaces sensors and/or actuators
simple execution time iteration increased substantially.
extreme case, increase limit reactivity learner
dangerous working autonomous robot.
expensive procedure algorithm computing value
actions (i.e., valid combinations elementary actions). cost procedure
especially critical since used twice step: get guess action
(in Action Evaluation procedure detailed Figure 13) get goodness
new achieved situation action execution (when computing v value
Statistics Update procedure detailed Figure 14). trivial re-order algorithm
avoid double use expensive procedure learning step: select
action executed next time evaluate goodness new
achieved situation. drawback re-order action selected without
taking account information provided last reward value (the goodness
situation assessed value adjustment). However, problem tasks
require many learning steps.
Even use action-evaluation procedure per learning step,
optimize much possible since brute-force approach described before,
evaluates action sequentially, feasible simple problems.
action-evaluation method presented next based observation many
actions would value since highest relevant partial rule given
moment would provide value actions accordance partial
command rule. separate computation value two actions would end
evaluated using rule waste time. avoided performing
action evaluation attending set active rules first place set
possible actions, brute-force approach does.
Figure 16 shows general form algorithm propose. algorithm, partial
rules considered one time, ordered relevant rule least relevant
one. partial command rule consideration (cow ) used process
actions accordance partial command. already processed sub-set
actions need considered action-evaluation procedure.
rules processed, update current situation assessment (v) action
executed next (a) attending, respectively, value prediction (qw ) guess (gw )
rules.
Observe partial rules maintained sorted relevance statistics update
procedure, since procedure rule relevance modified. relevance
rule changed, position list also modified accordingly. way
re-sort list rules every time want apply procedure
described.
elementary actions form (m k) motor k value
range possible values motor, algorithm implemented
especially efficient way since need explicitly compute set actions A.
111

fiPorta & Celaya

Action Evaluation
(Initialization)
L List active rules sorted relevance.
EA Set elementary actions
Set combinations EA
v
(Situation assessment)

(Optimal action)
g
(Optimal action value prediction)
(Process)
w first element(L)

cow partial command w
gw qw + 2 random(w , w )
Aw {a A|cow accordance a}
qw > v
v qw
endif
gw > g
g gw
cow
endif
Aw
w next element(L)
6=

Figure 16: General form proposed situation-assessment action-selection procedure.

case (see Figure 17 18), construct decision tree using motors decision
attributes groups leaf actions evaluated partial
rule (all actions removed set iteration algorithm Figure 16).
internal node tree classifies action according one motor commands included action. internal nodes store following information:
Partial command: partial command accordance action classified node. partial command constructed collecting
motors whose values fixed nodes root tree node
consideration.
Motor: motor used node classify actions. node open (i.e.,
still decided motor attend) motor value set .
node closed deciding motor pay attention (and adding
corresponding subtrees) converting node leaf.
112

fiReinforcement Learning Categorizable Environments

Action Evaluation
(Initialization)
L List active rules sorted relevance.
v

g
tree new node(c )
open 1
closed 0
(Process)
w first element(L)

gw qw + 2 random(w , w )
Include Rule(tree, w, gw )
w next element(L)
closed = open

Figure 17: Top level algorithm efficient action evaluation algorithm. end
algorithm, v goodness current situation used Statistics
Update algorithm (see Figure 14), action executed next guess
expected value. Include Rule procedure detailed next figure.

Subtrees: list subtrees start node. subtree
associated value corresponds one possible actions executable motor node. actions included given subtree elementary action
(m k) motor node k value corresponding
subtree.
leaves tree information value actions classified
leaf. information represented following set attributes leaf:
Value: expected value actions classified leaf. maximum
value leaves used assess goodness, v, new achieved situation.
Guess: value altered noise exploratory reasons. leaf maximal
guess set actions select action executed next.
Relevance: relevance value predictions (of value guess).
Partial command: partial command accordance actions
classified leaf. case internal nodes, partial command
constructed collecting motors whose values fixed root
tree leaf consideration.
113

fiPorta & Celaya

Include rule(n, w, gw )
not(is leaf(n))
cow command(w)
con command(n)
motor(n) 6=
(Closed Node: Search compatible sub-nodes)
ea cow motor(ea) = motor(n)
Include Rule(get subtree(value(ea), n), w, gw )
else
subtrees(n)
Include Rule(s, w, gw )
endfor
endif
else
(Open Node: Specialize node)
cow con 6=
(Extend node)
ea action in(cow con )
set motor(n, motor(ea))
closed closed + 1
k values(motor(ea))
new subtree(n, {k, new node(con (motor(ea) k))})
open open + 1
endfor
Include Rule(n, w, gw )
else
(Transform node leaf )
transform leaf(n, qw , gw , w , cow )
closed closed + 1
qw > v
v qw
endif
gw > guess
g gw
cow
endif
endif
endif
endif

Figure 18: Include rule algorithm searches nodes node n partial command compatible partial command rule w extends nodes
insert leave tree.

given moment, inclusion new partial rule tree produces specialization open nodes compatible rule (see Figure 18). say open
node n compatible given rule w partial command node con
partial command rule cow assign different values motor.
specialization open node result extension node (i.e., new branches
114

fiReinforcement Learning Categorizable Environments

Partial rules
Partial View
Partial Command
RU Ev
(m1 v1 ) (m2 v1 )
RU Ev
(m1 v1 )
RU Ev
(m2 v1 ) (m3 v1 )
RU Ev
(m2 v1 )
RU Ev
(m1 v0 )
RU Ev
(m2 v0 )
RU Ev
(m3 v1 )
RU Ev
(m3 v0 )

q

e
5
7
8
3
2
10
1
6


0.1
0.9
2.0
3.1
3.5
3.6
4.0
4.5

0.83
0.52
0.33
0.24
0.22
0.21
0.20
0.18

guess
5.1
6.5
6.0
6.2
5.3
4.1
5.2
12.7

Table 2: Set rules controller. values q e stored guess
computed them. define partial views RU Ev indicate
active current time step.

added tree node) transformation node leaf.
node extended partial command rule affects motors included
partial command node. means motor values taken
account tree used action evaluation according
rule consideration. node extended, one motors present
layers tree used generate layer open nodes current node.
that, node considered closed inclusion rule procedure repeated
node (with different effects node closed). motors affected
partial command rule also affected partial command node,
node transformed leaf storing value, guess, relevance attributes
extracted information associated rule.
process stopped soon detect nodes closed (i.e.
external nodes tree leaves). case, rules still processed
effect tree form and, consequently useful action evaluation. rule
consistently used action evaluation, removed controller.
toy-size example illustrate tree-based action-evaluation algorithm. Suppose
robot three motors accept two different values (named v 0
v1 ). produces set 8 different actions. Suppose that, given moment, robot
controller includes set rules shown Table 2. Action Evaluation algorithm
(Figure 17), rules processed least relevant one expanding
initially empty tree using algorithm Figure 18. inclusion rule tree results
extension tree (see stages B, E Figure 19) closing branches
converting open nodes leaves (stages C F). particular case tree becomes
completely closed processing 5 rules 8 active rules controller.
end process, tree five leaves. Three include two actions
two represent single action. Using tree say value
situation tree constructed, v, 8 (this given leaf circled
solid line figure). Additionally, next action executed form
115

fiPorta & Celaya

Motor: m1
Command:
TRUE c

B


Open
Node

v1

v0

Motor: m2
Command:
(m1,v1)
v0

Open
Node

v1
Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Open
Node

C

Motor: m2
Command:
(m1,v1)
v0

Open
Node

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

E

Open
Node

v0

v1

v1

Open
Node

Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Motor: m3
Command:
(m1,v0)
(m2,v1)
v0
v1

v1
Motor: m3
Command:
(m1,v0)
(m2,v1)
v0
Value: 3
Guess: 6.2
Relevance: 0.24
Command:
(m1,v0)
(m2,v1)
(m3,v0)

v1

v1

v0

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

Motor: m2
Command:
(m1,v1)
v0

Motor: m2
Command:
(m1,v0)
v1

v0
Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Value: 2
Guess: 5.3
Relevance: 0.22
Command:
(m1,v0)
(m2,v0)

Value: 8
Guess: 6.0
Relevance: 0.33
Command:
(m1,v0)
(m2,v1)
(m3,v1)

Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Motor: m1
Command:
TRUE c

v1
Motor: m2
Command:
(m1,v1)
v0

v1

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

Value: 8
Guess: 6.0
Relevance: 0.33
Command:
(m1,v0)
(m2,v1)
(m3,v1)

Open
Node

F

Motor: m2
Command:
(m1,v0)

Motor: m2
Command:
(m1,v1)
v0

Motor: m2
Command:
(m1,v0)

Motor: m1
Command:
TRUE c
v0

v1

v0

v1

v0

v0

Motor: m1
Command:
TRUE c



Motor: m1
Command:
TRUE c

v1
Motor: m3
Command:
(m1,v0)
(m2,v1)
v0
Value: 3
Guess: 6.2
Relevance: 0.24
Command:
(m1,v0)
(m2,v1)
(m3,v0)

v1

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

v1
Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Value: 8
Guess: 6.0
Relevance: 0.33
Command:
(m1,v0)
(m2,v1)
(m3,v1)

Figure 19: Six different stages construction tree action evaluation.
stage corresponds insertion one rule Table 2.

116

fiReinforcement Learning Categorizable Environments

8
7

log(Time)

6
5
4
3
2
1
0

0

1

2
3
Number Void Motors

Brute Force Evaluation

4

5

TreeBased Evaluation

Figure 20: Log execution time (in seconds) brute-force approach vs. treebased one.

(m1 v1 , m2 v0 , m3 ]) ] represents possible action. optimal action
given leaf circled dashed line leaf larger guess value.
cost algorithm largely depends specific set partial rules
processed. worst case, cost algorithm is:
O(nr lnm ),
nr number rules, nm number motors and, l maximal range values
accepted motors. because, worst case, insert given rule,
visit nodes maximally expanded tree (i.e., tree node l subtrees
final nodes branches still opened). number nodes
tree
nm
X
lnm +1 1
li =
= O(lnm ).
l1
i=0

transform cost expression taking account l nm total number
possible combinations elementary actions (nc ) or, words, total amount
actions. Therefore, cost presented algorithm
O(nr nc ).
hand, cost brute-force approach always
(nr nc ).
117

fiPorta & Celaya

So, worst case, cost presented algorithm order cost
brute-force approach. However, since l rules would enough close
maximally expanded tree (one rule different values motor used last
still-open layer tree), cost tree-based algorithm would be, average,
much smaller brute-force approach.
Figure 20 exemplifies different performance brute-force action-evaluation procedure tree-based one. figure shows time taken execution
toy example Section 6.1. experiment, defined void motors motors
whose actions effect environment. seen, number void
motors increases, cost tree-based evaluation significantly less
brute-force approach.

118

fiReinforcement Learning Categorizable Environments

Appendix C: Notation
Uppercase used sets, Greek letters represent parameters algorithms.

Set states.
0
s,
Individual states. Full views.
ns
Number states.
F = {fdi | = 1..nf }
Set feature detectors.
Partial view order k.
v(fdi1 , . . . , fdik )

Set actions robot.
na
Number actions.
EA = {eai | = 1..ne }
Set elementary actions.
nm
Number motors robot.
eai = (mi k)
Elementary action assigns value k motor mi .
c(eai1 , . . . , eaik )
Partial command order k.
= (ea1 , . . . , eanm )
Action. Combination elementary actions. Full command.
w = (v, c)
Partial rule composed partial view v partial command c.
w
empty partial rule.
w1 w 2
Composition two partial rules.
C = {wi | = 1..nr }
Controller set partial rules.

Maximum number elements C.
0
0
C , Cant
Subset rules active given time step previous one.
C 0 (a)
Active rules partial command accordance a.
qw
Expected value partial rule w.
ew
Expected error value estimation partial rule w.
e
Average error value prediction.
iw
Confidence index.
cw
Confidence statistics partial rule w.

Top value confidence.

Index confidence function reaches value .
w = ew cw + e (1 cw ) Error return prediction partial rule w.
w = 1/(1 + w )
Relevance rule w.
Iw = [qw 2w ]
Value interval partial rule w.
mw
Updating ratio statistics partial rule w.

Learning rate. Top value mw .
U (w)
Number times rule w used.
0
winner(C , a)
relevant active partial rule w.r.t. action a.
guess(a)
reliable value estimation action a.
ra
Reward received execution a.

Discount factor.
v
Goodness given situation.
q = ra + v
Value executing action given situation.

Number new partial rules created time.

Redundancy threshold used partial-rule elimination.

119

fiPorta & Celaya

References
Arkin, R. C. (1998). Behavior-Based Robotics. Intelligent Robotics Autonomous Agents.
MIT Press.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Brooks, R. A. (1991). Intelligence without representation. Artificial Intelligence, 47, 139
159.
Butz,

M. (1999).
C-XCS: implementation
(http://www.cs.bath.ac.uk/ amb/LCSWEB/computer.htm).



XCS



C.

Celaya, E., & Porta, J. M. (1996). Control six-legged robot walking abrupt terrain.
Proceedings IEEE International Conference Robotics Automation,
pp. 27312736.
Celaya, E., & Porta, J. M. (1998). control structure locomotion legged
robot difficult terrain. IEEE Robotics Automation Magazine, Special Issue
Walking Robots, 5 (2), 4351.
Chapman, D., & Kaelbling, L. P. (1991). Input generalization delayed reinforcement
learning: algorithm performance comparisons. Proceedings International Joint Conference Artificial Intelligence, pp. 726731.
Claus, C., & Boutilier, C. (1998). dynamics reinforcement learning cooperative
multiagent systems. Proceedings Fifteenth National Conference Artificial
Intelligence, pp. 746752. American Association Artificial Intelligence.
Drummond, C. (2002). Accelerating reinforcement learning composing solutions automatically identified subtasks. Journal Artificial Intelligence Research, 16, 59104.
Edelman, G. M. (1989). Neuronal Darwinism. Oxford University Press.
Hinton, G., McClelland, J., & Rumelhart, D. (1986). Parallel Distributed Processing: Explorations Microstructure Cognition. Volume 1: Foundations, chap. Distributed
Representations. MIT Press, Cambridge, MA.
Ilg, W., Muhlfriedel, T., & Berns, K. (1997). Hybrid learning architecture based neural
networks adaptive control walking machine. Proceedings 1997 IEEE
International Conference Robotics Automation, pp. 26262631.
Kaelbling, L. P. (1993). Learning Embedded Systems. Bradford Book. MIT Press,
Cambridge MA.
Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: survey.
Journal Artificial Intelligence Research, 4, 237 285.
Kanerva, P. (1988). Sparse Distributed Memory. MIT Press, Cambridge, MA.
Kirchner, F. (1998). Q-learning complex behaviors six-legged walking machine.
Robotics Autonomous Systems, 25, 253262.
120

fiReinforcement Learning Categorizable Environments

Kodjabachia, J., & Meyer, J. A. (1998). Evolution development modular control
architectures 1-d locomotion six-legged animats. Connection Science, 2, 211
237.
Maes, P., & Brooks, R. A. (1990). Learning coordinate behaviors. Proceedings
AAAI-90, pp. 796802.
Mahadevan, S., & Connell, J. H. (1992). Automatic programming behavior-based robots
using reinforcement learning. Artificial Intelligence, 55, 311363.
McCallum, A. K. (1995). Reinforcement Learning Selective Perception Hidden
State. Ph.D. thesis, Department Computer Science.
Parker, G. B. (2000). Co-evolving model parameters anytime learning evolutionary
robotics. Robotics Autonomous Systems, 33, 1330.
Pendrith, M. D., & Ryan, M. R. K. (1996). C-trace: new algorithm reinforcement
learning robotic control. Proceedings 1996 International Workshop
Learning Autonomous Robots (Robotlearn96).
Poggio, T., & Girosi, F. (1990). Regularization algorithms learning equivalent
multilayer networks. Science, pp. 978982.
Schmidhuber, J. (2002). speed prior: new simplicity measure yielding near-optimal
computable predictions. Proceedings 15th Annual Conference Computational Learning Theory (COLT 2OO2). Lecture Notes Artificial Intelligence.
Springer., pp. 216228.
Sen, S. (1994). Learning coordinate without sharing information. Proceedings
Twelfth National Conference Artificial Intelligence, pp. 426431. American
Association Artificial Intelligence.
Sutton, R. S. (1991). Reinforcement learning architectures animats. Meyer, J. A., &
Wilson, S. W. (Eds.), Proceedings First International Conference Simulation Adaptive Behavior. Animals Animats, pp. 288296. MIT Press,
Bradford Books.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. Bradford
Book. MIT Press.
Sutton, R. S., & Whitehead, S. D. (1993). Online learning random representations.
Proceedings Eleventh International Conference Machine Learning, pp.
314321. Morgan Kaufman, San Francisco, CA.
Sutton, R. (1996). Generalization reinforcement learning: Successful examples using
sparse coarse coding. Proceedings 1995 Conference Advances Neural
Information Processing, pp. 10381044.
Sutton, R., Precup, D., & Singh, S. (1999). MDPs semi-MDPs: framework
temporal abstraction reinforcement learning. Artificial Intelligence, 12, 181211.
Tan, M. (1997). Multi-agent reinforcement learning: Independent vs. cooperative agents.
Reading Agents, pp. 487494. Morgan Kaufmann Publishers Inc.
121

fiPorta & Celaya

Vallejo, E. E., & Ramos, F. (2000). distributed genetic programming architecture
evolution robust insect locomotion controllers. Meyer, J. A., Berthoz, A.,
Floreano, D., Roitblat, H. L., & Wilson, S. W. (Eds.), Supplement Proceedings
Sixth International Conference Simulation Adaptive Behavior: Animals
Animats, pp. 235244. International Society Adaptive Behavior.
Venturini, G. (1994). Apprentissage Adaptatif et Apprentissage Supervise par Algorithme
Genetique. Ph.D. thesis.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279292.
Widrow, B., & Hoff, M. (1960). Adaptive switching circuits. Western Electronic Show
Convention, Volume 4, pp. 96104. Institute Radio Engineers (now IEEE).
Wilson, S. W. (1995). Classifier fitness based accuracy. Evolutionary Computation, 3,
149175.
Wilson, S. W. (1996). Explore/exploit strategies autonomy. Animals Animats 4: Proceedings 4th International Conference Simulation Adaptive
Behavior, pp. 325332.

122

fiJournal Artificial Intelligence Research 23 (2005) 245-297

Submitted 12/03; published 03/05

Graduality Argumentation
Claudette Cayrol
Marie-Christine Lagasquie-Schiex

ccayrol@irit.fr
lagasq@irit.fr

IRIT-UPS, 118 route de Narbonne
31062 Toulouse Cedex, FRANCE

Abstract
Argumentation based exchange valuation interacting arguments, followed
selection acceptable (for example, order take decision,
make choice). Starting framework proposed Dung 1995, purpose
introduce graduality selection best arguments, i.e. able
partition set arguments two usual subsets selected
non-selected arguments order represent different levels selection. basic idea
argument acceptable preferred attackers. First,
discuss general principles underlying gradual valuation arguments based
interactions. Following principles, define several valuation models abstract
argumentation system. Then, introduce graduality concept acceptability
arguments. propose new acceptability classes refinement existing classes
taking advantage available gradual valuation.

1. Introduction
shown Dung (1995), argumentation frameworks provide unifying powerful
tool study several formal systems developed common-sense reasoning, well
giving semantics logic programs. Argumentation based exchange
valuation interacting arguments support opinions assertions. applied,
among others, legal domain, collective decision support systems negotiation
support.
fundamental characteristic argumentation system interaction arguments. particular, relation attack may exist arguments. example,
argument takes form logical proof, arguments proposition arguments
proposition advanced. case, attack relation relies logical
inconsistency.
argumentation process usually divided two steps: valuation relative
strength arguments, followed selection acceptable arguments.
valuation step, usual distinguish two different types valuations:
intrinsic valuation: here, value argument independent interactions
arguments. enables simply express extent argument
increases confidence statement supports (see Pollock, 1992; Krause, Ambler, Elvang, & Fox, 1995; Parsons, 1997; Prakken & Sartor, 1997; Amgoud & Cayrol,
1998; Kohlas, Haenni, & Berzati, 2000; Pollock, 2001).
c
2005
AI Access Foundation. rights reserved.

fiCayrol, Lagasquie-Schiex

example, work Krause et al. (1995), using following knowledge base,
composed (formula, probability) pairs {(1 , 0.8), (2 , 0.8), (3 , 0.8), ((1 2
4 ), 1), ((1 3 4 ), 1)}, two arguments produced1 :
A1 =< {1 , 2 , (1 2 4 )}, 4 >
A2 =< {1 , 3 , (1 3 4 )}, 4 >.

arguments weight 0.8 0.8 1 = 0.64, formula 4
weight 0.64 + 0.64 0.512 = 0.7682 .
interaction-based valuation: value argument depends attackers
(the arguments attacking it), attackers attackers (the defenders), etc. 3
Several approaches proposed along line (see Dung, 1995; Amgoud &
Cayrol, 1998; Jakobovits & Vermeir, 1999; Besnard & Hunter, 2001) differ
sets values used. Usually, two values considered. However,
proposals use two values (three values Jakobovits & Vermeir,
1999, infinity values Besnard & Hunter, 2001).
example, work Besnard Hunter (2001), set values
interval real line [0, 1]. case, set arguments4 {A1 , A2 , A3 }
considering A1 attacks A2 attacks A3 , value argument A1
(resp. A2 , A3 ) 1 (resp. 21 , 32 ).
Intrinsic valuation interaction-based valuation often used separately, according considered applications. recent works however consider combination
approaches (see Amgoud & Cayrol, 1998; Karacapilidis & Papadias, 2001; Pollock,
2001).
Considering selection acceptable arguments, usual distinguish
two approaches:
individual acceptability: here, acceptability argument depends
properties. example, argument said acceptable
attacker (in case, interaction arguments considered,
see Elvang-Goransson et al., 1993). context intrinsic valuation, argument also said acceptable better attackers
(see Amgoud & Cayrol, 1998).
collective acceptability: case, acceptability set arguments explicitly
defined. example, acceptable, set arguments may contain two
1. Here, arguments form Explanation-Conclusion Pair. one possible way
compute arguments (see also Lin & Shoham, 1989; Vreeswijk, 1997; Pollock, 1992; Prakken & Sartor,
1997; Simari & Loui, 1992; Elvang-Goransson, Fox, & Krause, 1993; Kohlas et al., 2000; Amgoud &
Cayrol, 2002).
2. Weights probabilities, weight argument probability conjunction
formulae argument, weight 4 probability disjunction A1 A2 .
3. Here, consider interactions corresponding attacks arguments. exist also
types interactions (for example, arguments reinforce arguments instead attacking them, see Karacapilidis & Papadias, 2001; Verheij, 2002). kind interaction, graduality
considered.
4. Here, initial knowledge base useless.

246

fiGraduality argumentation

arguments one attacks (interactions arguments used).
Dungs (1995) framework well suited kind approach allows
binary classification: argument belongs belong acceptable set.
clear except intrinsic valuations, proposals allow gradual
notion valuation acceptability (i.e. low number levels describe values
acceptability usually binary). aim therefore introduce graduality
two steps.
However, processes valuation selection often linked together.
case selection done basis value arguments5 selection
defines binary valuation arguments. therefore:
first consider discuss general principles concerning definition gradual
interaction-based valuation define valuation models abstract
argumentation system,
then, introduce notion graduality definition acceptability using
previously defined gradual valuations, also classical mechanisms.
graduality already introduced argumentation systems. instance,
work Pollock (2001), degrees justification beliefs computed. Arguments
sequences conclusive and/or prima-facie inferences. Arguments collected graph
node represents conclusion argument, support link ties node nodes
inferred, attack link indicates attack nodes. degree
justification belief computed strength arguments concluding
belief strength arguments concluding attacker belief.
work takes place abstract framework since consider argument
structure. valuation models based interactions arguments directly
apply arguments.
use framework defined Dung (1995): set arguments binary attack
relation arguments. also use graphical representation argumentation systems (see Section 2). gradualisation interaction-based valuations presented
Section 3. Then, Section 4, consider different mechanisms leading gradual
acceptability, sometimes relying gradual valuations defined Section 3.
conclude Section 5.
proofs properties stated Sections 3 4 given Appendix A.

2. Dungs (1995) framework graphical representation
consider abstract framework introduced Dung (1995). argumentation system
<A, R> set arguments binary relation R called attack relation:
consider Ai Aj A, Ai RAj means Ai attacks Aj Aj attacked Ai (also
denoted (Ai , Aj ) R).
5. example, using Besnard Hunters (2001) valuation, decide arguments whose
value > 0.5 selected, 0.5 mean value set values; Another possibility,
different valuations (interaction-based intrinsic), accept argument value better
value attackers.

247

fiCayrol, Lagasquie-Schiex

argumentation system well-founded infinite sequence 0 , A1 ,
. . . , , . . . i, Ai Ai+1 RAi .
Here, interested structure arguments consider arbitrary
attack relation.
Notation: <A, R> defines directed graph G called attack graph. Consider A,
set R (A) set arguments attacking A6 set R+ (A) set
arguments attacked A7 .
Example 1
system <A = {A1 , A2 , A3 , A4 }, R = {(A2 , A3 ), (A4 , A3 ), (A1 , A2 )}> defines following graph G root8 A3 :
A1

A2
A3
A4

Definition 1 (Graphical representation argumentation system) Let G
attack graph associated argumentation system <A, R>, define:
Leaf attack graph leaf G argument without attackers9 .
Path attack graph path B sequence arguments C = A1
. . . that:
= A1 ,
A1 RA2 ,
...,
An1 RAn ,
= B.
length path n 1 (the number edges used path)
denoted lC .
special case path10 whose length 0.
set paths B denoted C(A, B).
6. R (A) = {Ai A|Ai RA}.
7. R+ (A) = {Ai A|ARAi }.
8. word root used informal sense (it means graph paths
leading node). term terms (leaf, branch, path, . . . ) used
document standard graph theory may different definition. usual terms
argumentation domain. Please see Definition 1 order know precise meaning document.
definitions simply take account fact directed edges graph link attackers
attacked argument).
9. leaf iff R (A) = .
10. assume exists infinity paths. assumption greatly simplifies handling
leaves later paper.

248

fiGraduality argumentation

Dependence, independence, root-dependence path
Consider 2 paths CA C(A1 , ) CB C(B1 , Bm ).
two paths said dependent iff Ai CA , Bj CB Ai = Bj .
Otherwise independent.
two paths said root-dependent iff = Bm Ai 6= CA ,
6 Bj CB Ai = Bj .
Cycles attack graph cycle11 path C = A1 . . . A1 i, j
[1, n], 6= j, Ai 6= Aj .
cycle C isolated iff C, 6 B BRA B 6 C.
Two cycles CA = A1 . . . A1 CB = B1 . . . Bm B1 interconnected
iff [1, n], j [1, m] Ai = Bj .
use notions direct indirect attackers defenders. notions introduced
inspired related definitions first introduced Dung (1995) strictly
equivalent12 .
Definition 2 (Direct/Indirect Attackers/Defenders argument) Consider
A:
direct attackers elements R (A).
direct defenders direct attackers elements R (A).
indirect attackers elements Ai defined by:
C C(Ai , A) lC = 2k + 1, k 1.
indirect defenders elements Ai defined by:
C C(Ai , A) lC = 2k, k 2.
argument attacker (direct indirect) argument B, say
attacks B (or B attacked A). way, argument defender
(direct indirect) argument B, defends B (or B defended A).
Note attacker also defender (for example, A1 attacks A2 attacks
A3 , A1 also attacks A3 ). way, direct attacker indirect attacker
(for example, A1 attacks A2 attacks A3 attacks A4 , A1 also attacks A4 )
thing may occur defenders.
Definition 3 (Attack branch defence branch argument) Consider
A, attack branch (resp. defence branch) path G leaf whose
length odd (resp. even). say root attack branch (resp. defence
branch).
11. definition cycle corresponds definition elementary cycle graph theory (an
elementary cycle contain 2 edges initial extremity, ending extremity).
12. Dungs (1995) work, direct attackers (resp. defenders) also indirect attackers (resp. defenders)
true definitions.

249

fiCayrol, Lagasquie-Schiex

Note notion defence basis usual notion reinstatement (B attacks
C, attacks B C reinstated A). paper, reinstatement taken
account indirectly, value argument C possibility selecting
C increased thanks presence A.
notions illustrated following example:
Example 2

graph G, see:
path C2 whose length 2 (C2 B1 A),
2 cycles A1 A3 A2 A1 A1 A3 A4 A1 , length
3, isolated (note A1 A3 A2 A1
A3 A4 A1 cycle definition),
two previous cycles interconnected (in A1 A3 ),
paths D1 C1 B1 C3 B2 independent,
paths D1 C1 B1 C3 B2 root-dependent
paths D1 C1 B1 C2 B1 dependent,
D1 , C2 , E1 leaves G,
D1 C1 B1 attack branch whose length
3, C2 B1 defence branch whose length 2,
C2 , B1 B2 direct attackers A,
C1 , C2 (which already direct attacker A) C3
direct defenders A,
D1 D2 two indirect attackers A,
E1 indirect defender A.

A3

A4

A1

A2


B2

B1

C1

D1

C2

C3

D2

E1

3. Graduality interaction-based valuations
consider two different valuation methods taking account quality attackers
defenders argument order define value argument using
interaction arguments13 :
first approach, value argument depends values direct
attackers argument. Therefore, defenders taken account
attackers. approach called local.
second approach, value argument represents set attack
defence branches argument. approach called global.
main difference two approaches illustrated following example:


C1

C

B

B

C2

13. pursue work initiated (Cayrol & Lagasquie-Schiex, 2003c) propose improvements.

250

fiGraduality argumentation

local approach, B two direct attackers (C2 C1 ) whereas B 0 one
(C 0 ). Thus B 0 better B (since B 0 suffers one attack whereas B suffers two attacks).
global approach, two branches (one attack one defence) lead B whereas
one branch attack leads B 0 . Thus B better B 0 (since least one
defence whereas B 0 none). case, C1 loses negative status attacker, since
fact carrying defence B.
3.1 Local approach (generic valuation)
existing proposals already considered examples local valuations.
Jakobovits Vermeirs (1999) approach, labelling set arguments assigns
status (accepted, rejected, undecided) argument using labels set {+, , ?}.
+ (resp. , ?) represents accepted (resp. rejected, undecided) status. Intuitively,
argument labelled ? supported weakened.
Definition 4 (Jakobovits Vermeirs labellings, 1999) Let <A, R> argumentation system. complete labelling <A, R> function Lab : {+, ?, }
that:
1. Lab(A) {?, } B R (A) Lab(B) {+, ?}
2. Lab(A) {+, ?} B R (A) R+ (A), Lab(B) {?, }
underlying intuition argument weakened (label ?) one
direct attackers supported (condition 1); argument get support
direct attackers weakened argument supported (label + ?) weakens
arguments attacks (condition 2). So:





attacker Lab(A) = +.
Lab(A) =? B R (A) Lab(B) =?.
(B R (A), Lab(B) = ) Lab(A) = +.
Lab(A) = + B R (A) R+ (A), Lab(B) = .

Every argumentation system completely labelled. associated semantics
acceptable set arguments iff exists complete labelling Lab <A, R>
= {A|Lab(A) = +}.
types labellings introduced Jakobovits Vermeir (1999) among
so-called rooted labelling induces corresponding rooted semantics. idea
reject arguments attacked accepted arguments: attack undecided
argument rooted since undecided attacker may become rejected.
Definition 5 (Jakobovits Vermeirs labellings, 1999 continuation)
complete labelling Lab rooted iff A, Lab(A) = B R (A)
Lab(B) = +.
rooted semantics enables clarify links semantics introduced
Jakobovits Vermeir (1999) semantics introduced Dung (1995).
251

fiCayrol, Lagasquie-Schiex

Example 3 following example:


An1

A2

A1

n even, obtain Lab(An ) = Lab(An2 ) = . . . = Lab(A2 ) = + Lab(An1 ) =
Lab(An3 ) = . . . = Lab(A1 ) = .
n odd, obtain Lab(An ) = Lab(An2 ) = . . . = Lab(A1 ) = + Lab(An1 ) =
Lab(An3 ) = . . . = Lab(A2 ) =
Another type local valuation introduced recently Besnard Hunter (2001)
deductive arguments. approach characterised follows. argument
structured pair hsupport, conclusioni, support consistent set formulae
enables prove formula conclusion. attack relation considered strict
cycles allowed. notion tree arguments allows concise
exhaustive representation attackers defenders given argument, root tree.
function, called categoriser, assigns value tree arguments. value
represents relative strength argument (root tree) given attackers
defenders. Another function, called accumulator, synthesises values assigned
argument trees whose root argument (resp. against) given conclusion.
phase categorisation therefore corresponds interaction-based valuation. Besnard
Hunter (2001) introduce following function Cat:
R (A) = , Cat(A) = 1
R (A) 6= R (A) = {A1 , . . . , }, Cat(A) =

1
1+Cat(A1 )+...+Cat(An )

Intuitively, larger number direct attackers argument, lower value.
larger number defenders argument, larger value.
Example 3 (continuation) obtain:
Cat(A
n ) = 1, Cat(An1 ) = 0.5, Cat(An2 ) = 0.66, Cat(An3 ) = 0.6, . . . , Cat(A1 ) =

( 5 1)/2 n (this value inverse golden ratio14 ).
So, have:
n even Cat(An1 ) . . . Cat(A3 ) Cat(A1 ) Cat(A2 ) . . . Cat(An ) = 1
n odd Cat(An1 ) . . . Cat(A2 ) Cat(A1 ) Cat(A3 ) . . . Cat(An ) = 1
approach local valuations generalisation two previous proposals
sense Besnard Hunters (2001) Cat function Jakobovits Vermeirs (1999)
labellings instances approach.
main idea value argument obtained composition two
functions:
one aggregating values direct attackers argument; so,
function computes value direct attack;
computing effect direct attack value argument:
value direct attack increases value argument decreases,
value direct attack decreases value argument increases.
14. golden ratio famous number since antiquity several interesting properties
several domains (architecture, example).

252

fiGraduality argumentation

Let (W, ) totally ordered set minimum element (VMin ) subset V W ,
contains VMin maximum element VMax .
Definition 6 (Generic gradual valuation) Let <A, R> argumentation system.
valuation function v : V that:
1. A, v(A) VMin
2. A, R (A) = , v(A) = VMax
3. A, R (A) = {A1 , . . . , } 6= , v(A) = g(h(v(A1 ), . . . , v(An )))
h : V W (V denotes set finite sequences elements V )
h(x) = x
h() = VMin
permutation (xi1 , . . . , xin ) (x1 , . . . , xn ), h(xi1 , . . . , xin ) = h(x1 , . . . , xn )
h(x1 , . . . , xn , xn+1 ) h(x1 , . . . , xn )
xi x0i h(x1 , . . . , xi , . . . , xn ) h(x1 , . . . , x0i , . . . , xn )
g : W V
g(VMin ) = VMax
g(VMax ) < VMax
g non-increasing (if x g(x) g(y))
Note h(x1 , . . . , xn ) max(x1 , . . . , xn ) logical consequence properties
function h.
first property function g explains behaviour local valuation case
argument root one branch (like Example 3):
Property 1 function g satisfies n 1:
g(VMax ) g 3 (VMax ) . . . g 2n+1 (VMax ) g 2n (VMax ) . . . g 2 (VMax ) VMax
Moreover, g strictly non-increasing g(VMax ) > VMin , previous inequalities
become strict.
second property shows local valuation induces ordering relation arguments:
Property 2 (Complete preordering) Let v valuation sense Definition 6.
v induces complete15 preordering set arguments defined by: B iff
v(A) v(B).
third property handles cycles:
15. complete preordering means two elements comparable.

253

fiCayrol, Lagasquie-Schiex

Property 3 (Value cycle) Let C isolated cycle attack graph, whose
length n. n odd, arguments cycle value value
fixpoint function g. n even, value argument cycle
fixpoint function g n .
following property shows underlying principles satisfied local valuations
defined according schema:
Property 4 (Underlying principles) gradual valuation given Definition 6 respects following principles:
P1 valuation maximal argument without attackers non maximal
attacked undefended argument.
P2 valuation argument function valuation direct attackers (the
direct attack).
P3 valuation argument non-increasing function valuation direct
attack.
P4 attacker argument contributes increase valuation direct
attack argument.
last properties explain Jakobovits Vermeir (1999) Besnard Hunter
(2001) propose instances local valuation described Definition 6:
Property 5 (Link Jakobovits & Vermeir, 1999)
Every rooted labelling <A, R> sense Jakobovits Vermeir (1999)
defined instance generic valuation that:
V = W = {, ?, +} < ? < +,
VMin = ,
VMax = +,
g defined g() = +, g(+) = , g(?) =?
h function max.
Property 6 (Link Besnard & Hunter, 2001) gradual valuation Besnard
Hunter (2001) defined instance generic valuation that:
V = [0, 1],
W = [0, [,
VMin = 0,
VMax = 1,
1
g : W V defined g(x) = 1+x
h defined h(x1 , . . . , xn ) = x1 + . . . + xn .
254

fiGraduality argumentation

Note that, work Besnard Hunter (2001), valued graphs acyclic. However, easy show valuation proposed Besnard Hunter (2001)
generalised graphs cycles (in case, must solve second degree equations see
Example 5).

















































B1B2B3









































C1

C3





C2
C4























































D1












D2
D3

































































E1















































































Example 4 Consider following graph:

B4

example, generic valuation, obtain:
v(E1 ) = v(D2 ) = v(D3 ) = v(C4 ) = v(B4 ) = VMax
v(D1 ) = v(C2 ) = v(C3 ) = v(B3 ) = g(VMax )
v(C1 ) = v(B2 ) = g 2 (VMax )
v(B1 ) = g(h(g 2 (VMax ), g(VMax )))
v(A) = g(h(g(h(g 2 (VMax ), g(VMax ))), g 2 (VMax ), g(VMax ), VMax ))
So, have:
E 1 , 2 , 3 , C 4 , B4

C 1 , B2

1 , C 2 , C 3 , B3
However, constraints v(A) v(B1 ) insufficient compare B1
arguments.
problem exists reduce example hatched part graph
previous figure; obtain E1 , D2 C1 D1 , C2 , B1 cannot compared
arguments16 .
Now, use instance generic valuation proposed Besnard Hunter (2001):
v(E1 ) = v(D2 ) = v(D3 ) = v(C4 ) = v(B4 ) = 1,
v(D1 ) = v(C2 ) = v(C3 ) = v(B3 ) = 12 ,
v(C1 ) = v(B2 ) = 23 ,
16. v(A) = g 2 (h(g 2 (VMax , g(VMax ))) v(B1 ) = g(h(g 2 (VMax ), g(VMax ))).

255

fiCayrol, Lagasquie-Schiex

6
,
v(B1 ) = 13
78
v(A) = 283 .

So, have:
E 1 , 2 , 3 , C 4 , B4

C 1 , B2

1 , C 2 , C 3 , B3

B1


However, reduce example hatched part graph, value
13
19 . So, v(A) better v(B1 ) v(D1 ), also v(C1 ) (A becomes better
defender).
Example 5 (Isolated cycle) Consider following graph reduced isolated cycle:


B

.

generic valuation gives v(A) = v(B) = fixpoint g 2 .
use instance proposed Besnard Hunter (2001), v(A) v(B) solutions
following second degree equation:
x2 + x 1 = 0.

1+ 5
So, obtain: v(A) = v(B) =
0.618 (the inverse golden ratio again).
2
3.2 Global approach (with tuples)
consider second approach valuation step, called global approach. Here,
key idea value must describe subgraph whose root A. So,
want memorise length branch leading tuple (for attack branch,
odd integer, defence branch, even integer).
approach, main constraint must able identify branches
leading argument compute lengths. easy case
acyclic graph. therefore introduce first global gradual valuation acyclic graphs.
Then, next sections, extend proposition case graphs cycles,
study properties global gradual valuation.
3.2.1 Gradual valuation tuples acyclic graphs
First, order record lengths branches leading arguments, use
notion tuples define operations tuples:
256

fiGraduality argumentation

Definition 7 (Tuple) tuple sequence integers. tuple (0, . . . , 0, . . .)
|
{z
}
denoted 0 . tuple (1, . . . , 1, . . .) denoted 1 .
|
{z
}





Notation 1 denotes set tuples built positive integers.
Definition 8 (Operations tuples) two kinds operations tuples:
concatenation two tuples defined function ? :
0 ? = ? 0 = 6= ()

(x1 , . . . , xn , . . .) ? (x01 , . . . , x0n , . . .) = Sort(x1 , . . . , xn , . . . , x01 , . . . , x0n , . . .)
Sort function orders tuple increasing values.
addition tuple integer defined function :




0 k = (k)
() k = ()

(x1 , . . . , xn ) k = (x1 + k, . . . , xn + k)

(x1 , . . . , xn , . . .) k = (x1 + k, . . . , xn + k, . . .) (x1 , . . . , xn , . . .) 6= 0
Note allow infinite tuples, among reasons, needed later
order compute ordering relations described Section 3.2.4 (in particular
graph cyclic).
operations tuples following properties:
Property 7 (Properties ? )
concatenation ? commutative associative.
tuple integers k k 0 , (t k) k 0 = (k + k 0 ).
integer k tuples t0 different 017 , (t ? t0 ) k = (t k) ? (t0 k).
order valuate arguments, split set lengths branches leading
argument two subsets, one lengths defence branches (even integers)
one lengths attack branches (odd integers). captured
notion tupled values:
Definition 9 (Tupled value) tupled value pair tuples vt = [vtp , vti ] with:
vtp tuple even integers ordered increased values; tuple called even
component vt;
vti tuple odd integers ordered increased values; tuple called odd
component vt.
17. Otherwise false : (0 ? (p)) k = (p + k), whereas (0 k) ? ((p) k) = (k) ? (p + k) = (k, p + k).

257

fiCayrol, Lagasquie-Schiex

Notation 2 V denotes subset tupled values (so, vt V, vt pair
tuples satisfying Definition 9).
Using notion tupled-values, define computation process gradual
valuation tuples18 case acyclic graphs.
Definition 10 (Valuation tuples acyclic graphs) Let <A, R> argumentation system without cycles. valuation tuples function v : V
that:
leaf

v(A) = [0 , ()].

direct attackers denoted B1 , . . . , Bn , . . .
v(A) = [vp (A), vi (A)] with:

vp (A) = (vi (B1 )1)?. . .?(vi (Bn )1)?. . .
vi (A) = (vp (B1 )1)?. . .?(vp (Bn )1)?. . .

Notes: choice value [0 , ()] leaves justified fact value
argument memorises lengths branches leading argument. Using
constraint, either vp (A) vi (A) may empty both19 .
Note also set direct attackers argument infinite (this property
used take account argumentation graph cycles).
Example 6 graph, valuation tuples gives following results:

B2

B1

C1

D1

graph G, have:

C2

C3

D2

v(D1 ) = v(C2 ) = v(E1 ) = [0 , ()],
v(C1 ) = v(D2 ) = [(), (1)],
v(C3 ) = [(2), ()],
v(B1 ) = [(2), (1)],
v(B2 ) = [(), (3)],
v(A) = [(2, 4), (1, 3)].

E1

18. definition different definition given (Cayrol & Lagasquie-Schiex, 2003c). ideas
formalisation different.
19. proof following:.
leaf, least one tuples empty, exists least one branch
whose length > 0 leading (see Definitions 8 10).
And, leaf, also exists least one defence branch path
allowed length 0 (in fact, infinity paths see Definition 1) attack
branch leading leaf (see Definition 10).
So, value leaf [0 , ()], impossible vp (A) = vi (A) = ().

258

fiGraduality argumentation

3.2.2 Study cycles
Handling cycles raises important issues: notion branch always useful
cycle (for example, unattacked cycle like Examples 5 7), notion
useful, length branch defined different ways.
Let us consider different examples:
Example 7 (Unattacked cycle) graph reduced unattacked cycle B
attacks argument C:


B

C

notion branch useless case, leaf graph.
two possibilities:
First, one consider cycle like infinite branch; (resp. B)
root one branch whose length . parity length branch
undefined, impossible say branch attack branch defence
branch.
second possibility consider cycle like infinity branches;
(resp. B) root infinity attack branches defence branches whose
lengths known finite.
second possibility means cycle may two representations acyclic
also infinite graphs (one root one root B).
rewriting process cycle:
B4

A4

B1

B5

B6

A1

A2

A3

B2

B3

B4

A1

A5

A6

B1

B2

B3

A2

A3

A4

B



Ai Bi must new arguments created rewriting process cycle.
Example 8 (Attacked cycle) cycle B attacked least one argument
belong cycle (here, attacker unattacked argument D):
259

fiCayrol, Lagasquie-Schiex





B

C

E

case, notion branch useful exists one leaf graph,
difficulty compute length branch. Example 7, consider either
one infinite branch (so, impossible know branch attack
defence branch), infinity attack branches defence branches
whose lengths known finite.
second case, graph rewritten following structures:




A6

A3





B3

A4

A5



B3

A1

A2



B1

B2

B1

B2

A1

A2

A3



B

C

E

Ai Bi must new arguments created rewriting process graph.
previous examples, chosen manage cycle infinity attack
branches defence branches whose lengths known finite would like
able apply Definition 10 cases (acyclic graphs graphs cycles). However,
need rewriting process graph cycles acyclic graph. two
different cases, one unattacked cycles one attacked cycles:
Definition 11 (Rewriting unattacked cycle) Let C = A0 A1 . . . An1 A0
unattacked cycle. graph G contains C rewritten follows:
260

fiGraduality argumentation

1. cycle C removed,
2. replaced infinite acyclic graphs, one Ai , = 0 . . . n 1:

%
Ai 11

%
Ai 21

Ai 22

Ai
...
...
...
...
...

-

Ai n1
1

Ai n1
2
...

Ai n1
n1

Ai n1

Ai n2
...

Ai nn1

Ai nn

-

Ai n+1
1

Ai n+1
2
...

Ai n+1
n1

Ai n+1
n

Ai n+1
n+1

...
...
...
...
...
...
...
...
...
...
...

3. edges Ai argument belong C kept.
Example 7 Unattacked cycle (continuation) graph G containing unattacked
cycle B argument C, attacked A, rewritten follows:
C


%
A11

%
A21

A22

B
A31

A32

A33

%
B11

...
...
...
...
...
...

%
B12

B22

B13

B23

B33

...
...
...
...
...
...

Alk Bkl new arguments.
Definition 12 (Rewriting attacked cycle) Let C = A0 A1 . . . An1 A0
attacked cycle, direct attacker Ai denoted Bi , exists. graph G
contains C rewritten follows:
1. cycle C removed,
2. replaced infinite acyclic graphs, one Ai = 0 . . . n 1:
261

fiCayrol, Lagasquie-Schiex

%
Bi

%
Ai 11


B(i1+n) mod n

Ai
...
...
...
...
...

Ai n1
1

Ai n1
2
...

Ai n1
n1


Ai n1

Ai n2
...

Ai nn1

Ai nn

Bi

B(i+1) mod n

Ai n+1
1

Ai n+1
2
...

Ai n+1
n1

Ai n+1
n

Ai n+1
n+1


B(i1+n) mod n

...
...
...
...
...
...
...
...
...
...
...
...
...

(the branches leading Bk exist iff Bk exists20 ).
3. edges Ai argument belong C kept.
4. edges Bi argument belong C kept.
Example 8 Attacked cycle (continuation) graph G containing cycle
B attacked argument argument C (resp. E) attacked
(resp. B) rewritten follows:
E

B

C


%



A21

A22



A41

A42

A43

A44



%
B11



...
...
...
...
...
...
...
...
...
...

Alk Bkl new arguments.
20. operator mod modulo function.

262


B13

B23

B33



B15

B25

B35

B45

B55



...
...
...
...
...
...
...
...
...
...
...
...

fiGraduality argumentation

Note: exist several cycles graph, two cases.
interconnected, rewrite cycle, valuation resulting
graph rewriting depend order cycles select rewrite
valuation process uses length branches.
interconnected, considered metacyle turn attacked unattacked previous methodology used leading
complex rewriting process formalized (see details examples
Appendix B).
3.2.3 gradual valuation tuples general graphs
Using definitions given Sections 3.2.1 3.2.2, gradual valuation tuples
given Definition 10 applicable arbitrary graphs rewriting process.
Let us apply rewriting process Definition 10 different examples.
Example 7 Unattacked cycle (continuation)
Consider following graph:


B

C

rewriting graph given Section 3.2.2.
Definition 10 produces:
vp (A) = (vi (A11 ) 1) ? . . . ? (vi (An1 ) 1) ? . . .
vi (A) = (vp (A11 ) 1) ? . . . ? (vp (An1 ) 1) ? . . .
Applying Definition 10 different arguments rewritten graph produces following
equalities:
v(Ann ) = [0 , ()] n 1
v(Ann1 ) = [(), (1)] n 2


v(Am
n ) = [vp (An+2 ) 2, vi (An+2 ) 2] n 1 n + 2

So, using equalities formulae giving vp (A) vi (A), define two sequences tuples : sequence (xk , k 1) infinite tuples even integers, sequence
(yk , k 1) infinite tuples odd integers
n
xk = (2) ? (vi (A2k+1
2k1 ) 1) ? . . . ? (vi (A2k1 ) 1) ? . . .
n
yk = (1) ? (vp (A2k+1
2k1 ) 1) ? . . . ? (vp (A2k1 ) 1) ? . . .

263

fiCayrol, Lagasquie-Schiex

results stated Property 7, easy prove vp (A) = x1 k 1,
xk = (2) ? (xk+1 2).
Similarly, vi (A) = y1 k 1, yk = (1) ? (yk+1 2).
equations enable prove :
even integer p p > 0, p belongs tuple xi , 1.
odd integer p, p belongs tuple yi , 1.

proof done induction p.
So, v(A) = v(B) = [(2, 4, 6, . . .), (1, 3, 5, . . .)].
Then, v(C) = [(2, 4, 6, . . .), (3, 5, 7, . . .)].
Note results readily extended unattacked cycle length n,
n 2.
Property 8 (Properties unattacked cycles)
unattacked cycle, argument cycle, v(A) = [(2, 4, 6, . . .), (1, 3, 5, . . .)].
Example 8 Attacked cycle (continuation)

Consider following graph:





B

C

E

rewriting graph given Section 3.2.2.
Definition 10 produces:
vp (A) = (vi (D) 1) ? (vi (A21 ) 1) ? . . . ? (vi (A2n
1 ) 1) ? . . .
vi (A) = (vp (D) 1) ? (vp (A21 ) 1) ? . . . ? (vp (A2n
1 ) 1) ? . . .
also
v(D) = [0 , ()]
v(Ann ) = [(), (1)] n 2
done treatment Example 7, formulae giving vp (A) vi (A) rewritten
order bring light interesting sequences tuples.
264

fiGraduality argumentation

2(k+p)

x0k = (vi (A2k
2k1 ) 1) ? . . . ? (vi (A2k1 ) 1) ? . . .
2(k+p)

yk0 = (1) ? (vp (A2k
2k1 ) 1) ? . . . ? (vp (A2k1 ) 1) ? . . .
Then, easy prove vp (A) = x01 k 1, x0k = (x0k+1 2).
0
2).
Similarly, vi (A) = y10 k 1, yk0 = (1) ? (yk+1

first equation enables prove x01 empty tuple21 .
second equation already solved produces y10 = (1, 3, 5, . . .).
So, v(A) = [(), (1, 3, 5, . . .)]. B, reason A, v(B) = [(2, 4, 6, . . .), ()].
Then, v(C) = [(2, 4, 6, . . .), ()], v(E) = [(), (3, 5, 7 . . .)].
Notation: order simplify writing, repeat values inside tuples
(we indicate value many times appears). example:
[(2, 4, 4, 6, 6, 6, 8, 8, 8, 8 . . .), (3, 5, 5, 7, 7, 7, 9, 9, 9, 9 . . .)]
denoted
[(2, |{z}
4 , |{z}
6 , |{z}
8 , . . .), (3, |{z}
5 , |{z}
7 , |{z}
9 , . . .)]
2

3

4

2

3

4

Conclusion cycles Cycles expensive since values obtained infinite.
appendix B, introduce algorithm computing tupled values. uses
process value propagation parameterised maximum number runs
cycle. number used order stop propagation mechanism
obtain finite (thus incomplete) tupled values.
3.2.4 Comparison tupled values
section, define comparison relation arguments (so, particular tupled values), using following idea: argument better argument
B iff better defence (for it) lower attack (against it).
first idea use lexicographic ordering tuples. lexicographic ordering
denoted lex defined by:
21. proof following:.
x01 contains even integers.
k, x0k 6= 0 since x0k result addition tuple integer.
x01 empty, let e1 denote least even integer present x01 . x01 = x02 2, x02 empty
e2 denote least integer present x02 . e1 = e2 + 2. So, able build
sequence positive even integers e1 , e2 , . . ., strictly decreasing. impossible. So,
x01 = ().

265

fiCayrol, Lagasquie-Schiex

Definition 13 (Lexicographic ordering tuples)
Let (x1 , . . . , xn , . . .) (y1 , . . . , ym , . . .) 2 finite infinite tuples .
(x1 , . . . , xn , . . .) <lex (y1 , . . . , ym , . . .) iff 1 that:
j < i, xj = yj
yi exists and:
either tuple (x1 , . . . , xn , . . .) finite number elements equal 1
(so, xi exist),
xi exists xi < yi .
(x1 , . . . , xn , . . .) =lex (y1 , . . . , ym , . . .) iff tuples contain number p {}
elements i, 1 p, xi = yi .
So, define: (x1 , . . . , xn , . . .) lex (y1 , . . . , ym , . . .) iff
(x1 , . . . , xn , . . .) =lex (y1 , . . . , ym , . . .) (x1 , . . . , xn , . . .) <lex (y1 , . . . , ym , . . .).
ordering <lex generalisation classical lexicographic ordering (see Xuong,
1992) case infinite tuples. ordering complete well-founded (there
exist infinite sequences strictly non-increasing: (0) <lex (0, 0) <lex . . . <lex
(0, . . . , 0, . . .) <lex . . . <lex (0, 1)).
Since even values odd values tupled value argument play
role, cannot use classical lexicographic comparison. So, compare tupled
values two steps:
first step compares number attack branches number defence
branches argument. So, two criteria (one defence
attack). criteria aggregated using cautious method: conclude
one arguments defence branches (it better according defence
criterion) less attack branches argument (it also better according
attack criterion). Note conclude positively criteria
agree: one arguments defence branches (it better according
defence criterion) attack branches argument (it worse
according attack criterion), arguments considered incomparable.
Else, arguments number defence branches number
attack branches, second step compares quality attacks
quality defences using length branch. comparison made
lexicographic principle (see Definition 13) gives two criteria
aggregated using cautious method. case disagreement, arguments
considered incomparable.
Let us consider examples:
[(2), (1)] better [(2), (1, 1)] less attack branches first
tupled value second tupled value, numbers defence branches
(first step).
[(2), (1)] incomparable [(2, 2), (1, 1)] less defence branches
less attack branches first tupled value second tupled value (first
step).
266

fiGraduality argumentation

[(2), (3)] better [(2), (1)] weaker attack branches first
tupled value second tupled value (the attack branch first tupled
value longer one second tupled value), defence branches
(second step, using lexicographic comparison applied even parts
odd parts tupled values).
[(2), (3)] better [(4), (3)] stronger defence branches
first tupled value second tupled value (the defence branch shorter
first tupled value second tupled value), attack branches
(second step).
[(2), (1)] incomparable [(4), (3)] worse attack branches
better defence branches first tupled value second tupled value
(second step).
comparison arguments done using Algorithm 1 implements principle
double comparison (first quantitative, qualitative) two criteria (one defence
criterion one attack criterion) using cautious method.
Algorithm 1: Comparison two tupled values
% Description parameters:
% v, w: 2 tupled values
% Notations:
%
|vp | (resp. |wp |): number elements even component v (resp. w)
%
vp (resp. wp ) infinite |vp | (resp. |wp |) taken equal
%
|vi | (resp. |wi |): number elements odd component v (resp. w)
%
vi (resp. wi ) infinite |vi | (resp. |wi |) taken equal
%
usual, denote strict relation associated defined by:
%
v w iff v w not(w v).

%
%
%
%
%
%
%
%
%

begin
v = w v w w v
% Case 1 %
2
else
3
|vi | = |wi | |vp | = |wp |
% lexicographic comparisons vp wp vi wi %
4
vp lex wp vi lex wi v w
% case 2 %
5
else
6
vp lex wp vi lex wi v w
% case 3 %
7
else v 6 w v 6 w
% Incomparable tupled values. case 4 %
1

8
9
10
11
12

else
|vi | |wi | |vp | |wp | v w
% case 5 %
else
|vi | |wi | |vp | |wp | v w
% case 6 %
else v 6 w v 6 w
% Incomparable tupled values. Case 7 %

end

Algorithm 1 defines partial preordering set v(A):
Property 9 (Partial preordering) Algorithm 1 defines partial preordering
set v(A).
267

fiCayrol, Lagasquie-Schiex

tupled value [0 , ()] maximal value partial preordering .
tupled value [(), 1 ] minimal value partial preordering .
Notation: partial preordering set v(A) induces partial preordering
arguments (the partial preordering denoted like partial preordering
v(A)): B v(A) v(B)22 .
order present underlying principles satisfied global valuation, first
consider different ways modifying defence part attack part argument:
Definition 14 (Adding/removing branch argument)
Let argument whose tupled value v(A) = [vp (A), vi (A)] vp (A) = (xp1 , . . . , xpn )
vi (A) = (xi1 , . . . , xim ) (vp (A) vi (A) may empty simultaneously).
Adding (resp. removing) defence branch defined by:
vp (A) becomes Sort(xp1 , . . . , xpn , xpn+1 ) xpn+1 length added branch (resp.
j [1..n] vp (A) becomes (xp1 , . . . , xpj1 , xpj+1 , . . . , xpn )).
thing vi (A) adding (resp. removing) attack branch A.
Definition 15 (Increasing/decreasing length branch argument)
Let argument whose tupled value v(A) = [vp (A), vi (A)] vp (A) = (xp1 , . . . , xpn )
vi (A) = (xi1 , . . . , xim ) (vp (A) vi (A) may empty simultaneously).
Increasing (resp. decreasing) length defence branch defined by:
p
p
0p
p
j [1..n] vp (A) becomes (xp1 , . . . , xpj1 , x0p
j , xj+1 , . . . , xn ) xj > xj (resp.
p
0p
p
x0p
j < xj ) parity xj parity xj .
thing vi (A) increasing (resp. decreasing) attack branch A.
Definition 16 (Improvement/degradation defences/attacks)
Let argument whose tupled value v(A) = [vp (A), vi (A)] (vp (A) vi (A) may
empty simultaneously). define:
improvement (resp. degradation) defence consists
adding defence branch initially vp (A) 6= 0 (resp. removing
defence branch A);
decreasing (resp. increasing) length defence branch A;
removing defence branch leading (resp. adding defence branch leading initially vp (A) = 0 );
improvement (resp. degradation) attack consists
adding (resp. removing) attack branch A;
decreasing (resp. increasing) length attack branch A.
Property 10 (Underlying principles) Let v valuation tuples (Definition 10)
associated Algorithm 1, v respects following principles:
P10 valuation maximal argument without attackers non maximal
argument attacked (whether defended not).
22. also use notation B defined by: B iff B.

268

fiGraduality argumentation

P20 valuation argument takes account branches rooted
argument.
P30 improvement defence degradation attack argument leads
increase value argument.
P40 improvement attack degradation defence argument leads
decrease value argument.
Example 4 (continuation)

valuation tuples, obtain:

v(E1 ) = v(D2 ) = v(D3 ) = v(C4 ) = v(B4 ) = [0 , ()],
v(D1 ) = v(C2 ) = v(C3 ) = v(B3 ) = [(), (1)],
v(C1 ) = v(B2 ) = [(2), ()],
v(B1 ) = [(2), (3)],
v(A) = [(2, 4), (1, 3, 3)].
So, have:
E 1 , 2 , 3 , C 4 , B4

C 1 , B2

B1

1 , C 2 , C 3 , B3

also

E 1 , 2 , 3 , C 4 , B4



incomparable almost arguments (except leaves graph).
Similarly, hatched part graph, obtain following results:
E1 , D2 C 1 B 1 1 , C 2
comparable arguments (in particular, worse
defender C1 direct attacker B1 ).
3.3 Main differences local global valuations
Cayrol Lagasquie-Schiex (2003c) give comparison approaches existing approaches (Dung, 1995; Jakobovits & Vermeir, 1999; Besnard & Hunter, 2001),
also comparison local approaches global approach. improvement
global approach proposed paper modify main results
comparison.
Let us recall example essential point differentiates (this example
already presented beginning Section 3):


C1

C

B
C2

269

B

fiCayrol, Lagasquie-Schiex

local approach, B 0 better B (since B 0 suffers one attack whereas B suffers
two attacks).
global approach, B better B 0 (since least defence whereas B 0
none). case, C1 loses negative status attacker, since fact carrying
defence B.
following table synthesises results different proposed valuations:
global approach
arguments
ing
branches

havattack



arguments
attack
branches

defence
branches



arguments
defence
branches



arguments
never attacked

local approach
arguments
arguments
havonly one attacked
ing

one


direct
attacker
unattacked direct
(possibly
defended)
attacker
arguments several attacked direct attackers (possibly defended)

arguments
several unattacked
direct attackers



arguments
never attacked

difference local approaches global approach also illustrated
following property:
Property 11 (Independence branches global approach)
Let argument following direct attackers:
A1 whose value v(A1 ) = [(a1p1 , . . . , a1pm ), (a1i1 , . . . , a1im )],
1
1
...,
whose value v(An ) = [(anp1 , . . . , anpmn ), (ani1 , . . . , animn )].
Let A0 argument following direct attackers:
A1p1 whose value v(A1p1 ) = [(a1p1 )()],
...,
A1pm whose value v(A1pm ) = [(a1pm )()],
1

1

1

A1i1 whose value v(A1i1 ) = [()(a1i1 )],
...,
A1im whose value v(A1im ) = [()(a1im )],
1
1
1
...,
Anp1 whose value v(Anp1 ) = [(anp1 )()],
...,
270

fiGraduality argumentation

Anpmn whose value v(Anpmn ) = [(anpmn )()],
Ani1 whose value v(Ani1 ) = [()(ani1 )],
...,
Animn whose value v(Animn ) = [()(animn )].
v(A) = v(A0 ).
property illustrates independence branches computation
values global approach, even branches graphically independent.
following example, A0 value [(2, 2)()] though root
different subgraphs:
C1

C1
B

B1



C2

C2

B2

property satisfied local approach since, using underlying principles
local approach (see Property 4), value argument must least
good (and sometimes better than23 ) value argument A0 (A one direct
attacker, A0 two direct attackers).
3.4 Conclusion valuation step
proposed two different gradual valuation models able make
distinction different arguments using preordering associated valuation
model. valuations used selection arguments (see Section 4).

4. Graduality acceptability
section, shift selection step introduce graduality notion
acceptability24 .
basic idea select argument depending non-selection direct attackers.
Following idea, propose two different methods:
first method consists refining classical partition issued Dungs collective acceptability; refinement may achieved using gradual valuations
defined Section 3.
second method takes place individual acceptability consists defining
new acceptability using gradual valuations defined Section 3.
4.1 Dungs (1995) collective acceptability
framework collective acceptability, consider acceptability set
arguments. acceptability defined respect properties sets
satisfy properties called acceptable sets extensions. argument
said acceptable belongs extension.
23. valuation proposed Besnard Hunter (2001), obtain: v(A) = 34 v(A0 ) = 12 .
24. work presented workshop (Cayrol & Lagasquie-Schiex, 2003b).

271

fiCayrol, Lagasquie-Schiex

Definition 17 (Basic properties extensions following Dung, 1995)
Let <A, R> argumentation system, have:
Conflict-free set set E conflict-free 6 A, B E ARB.
Collective defence Consider E A, A. E collectively defends
B A, BRA, C E CRB. E defends elements
E, E collectively defends A.
Dung (1995) defines several semantics collective acceptability: mainly, admissible
semantics, preferred semantics stable semantics (with corresponding extensions:
admissible sets, preferred extensions stable extensions).
Definition 18 (Some semantics extensions following Dung, 1995) Let <A, R>
argumentation system.
Admissible semantics (admissible set) set E admissible E
conflict-free E defends elements.
Preferred semantics (preferred extension) set E preferred extension
E maximal set inclusion among admissible sets.
Stable semantics (stable extension) set E stable extension E
conflict-free E attacks argument belong E (A \ E,
B E BRA).
Note definitions, attacker given argument considered
separately (the direct attack whole considered). Dung (1995) proves that:
admissible set <A, R> included preferred extension <A, R>.
always exists least one preferred extension <A, R>.
<A, R> well-founded one preferred extension also
stable extension.
stable extension also preferred extension (the converse false).
always stable extension.
Property 12 set leaves (i.e. {A|R (A) = }) included every preferred extension every stable extension.
4.2 Different levels collective acceptability
given semantics, following Dung, acceptability argument depends
membership extension semantics. consider three possible cases 25 :
25. terminology used section also used domain nonmonotonic reasoning (see Pinkas
& Loui, 1992): word uni comes word universal synonym word skeptical,
word exi comes word existential synonym word credulous.
chosen use words uni exi recall logical quantificators (for all) (exists
least one).

272

fiGraduality argumentation

argument uni-accepted, belongs extensions semantics,
argument exi-accepted, belongs least one extension
semantics,
argument not-accepted belong extension
semantics.
However, three levels seem insufficient. example, concluded
case two arguments B exi-accepted ARB BRA?
So, introduce new definition takes account situation argument
w.r.t. attackers. refines class exi-accepted arguments given
semantics S.
Definition 19 (Cleanly-accepted argument) Consider A, cleanly-accepted
belongs least one extension B BRA, B
belong extension S.
Thus, capture idea argument better accepted, attackers
not-accepted.
Property 13 Consider semantics extension conflictfree. uni-accepted cleanly-accepted. converse false.
notion cleanly-accepted argument refines class exi-accepted arguments.
semantics argument A, following states:
uni-accepted, belongs extensions (so, also
cleanly-accepted);
cleanly-accepted (so, definition also exi-accepted); note
possible argument also uni-accepted;
only-exi-accepted, cleanly-accepted, exi-accepted;
not-accepted belong extension S.
Example 9 Consider following argumentation system.
two preferred extensions {D, C2 , A, G}
J
{D, C2 , E, G, I}. So, preferred semantics, acI
ceptability
levels following:
E

G
H



B

F

C1

C2

D, C2 G uni-accepted,
cleanly-accepted uni-accepted,
E only-exi-accepted,
B, C1 , F , H J not-accepted.

Note that, cases one extension, first three levels acceptability coincide26 . case:
26. one extension fact belongs extensions equivalent
fact belongs least one extension. Moreover, one extension containing A,
attackers belong extension. So, cleanly-accepted.

273

fiCayrol, Lagasquie-Schiex

preferred semantics, even cycle (see Doutre, 2002).
basic semantics (another semantics proposed Dung see Dung, 1995;
Doutre, 2002 presented one extension).
Looking closely, prove following result (proof Appendix A):
Property 14 stable semantics, class uni-accepted arguments coincides
class cleanly-accepted arguments.
Then, using result issued work Dunne Bench-Capon (2001, 2002)
reused Doutre (2002) shows that, odd cycle, preferred
extensions stable27 , apply Property 14 obtain following consequence:
Consequence 1 preferred semantics, odd cycle, class
uni-accepted arguments coincides class cleanly-accepted arguments.
Finally, exploitation gradual interaction-based valuations (see Section 3) allows
us define new levels collective acceptability.
Let v gradual valuation let associated preordering (partial complete)
A. preordering used inside acceptability level (for example, level
exi-accepted arguments) order identify arguments better accepted
others.
Example 9 (continuation)
graph:

Two different gradual valuations applied

0,674

0,590

0,482

J



E

0,694

H

G 0,666

0,441
0,4 B


1

C1

F

0,5

C2 1

0.5

Besnard & Hunters (2001) valuation
instance generic valuation proposed Besnard Hunter (2001) (see
Section 3.1), obtain following comparisons:
D, C2 E G J C1 , F H B
27. corresponds consistent argumentation system proposed Dung (1995).

274

fiGraduality argumentation

{
{

[(6,8,10,12,...),
2 3
{
{
{

(7,9,10,11,...)]
2 2 3

J

[(4,6,8,10,...),
(3,5,7,9,...)]

E
H

2 2 3

2 3

[(2,4,6,8,...),
(3,5,7,9...)]


G [(2),()]

{
{
{

{
{

[(4,6,8,10,...),
2 3
(5,7,9,11,...)]

{
{
{

[(6,8,10,12,...),
2 2 3
(5,7,9,11,...)]
{
{



[(2),(1)]



B

C1

[(0,...,0),()]

F [(),(1)]

C2 [(0,...,0),()]

[(),(1)]

Valuation tuples

global valuation tuples presented Section 3.2, obtain following comparisons:
D, C2 G B F, C1
D, C2 E
D, C2 H E
D, C2
D, C2 J
So, arguments belonging cycle incomparable G, B, F , C 1 and, even
them, comparison results.
apply preordering induced valuation without respecting acceptability
levels defined section, counter-intuitive situations may happen. Example 9,
obtain:
valuation Besnard Hunter (2001) preferred semantics,
E G despite fact G uni-accepted E only-exi-accepted.
valuation tuples preferred semantics, H E despite
fact E only-exi-accepted H not-accepted.
counter-intuitive situations illustrate difference acceptability definition valuation definitions (even use interaction arguments,
use way).
275

fiCayrol, Lagasquie-Schiex

4.3 Towards gradual individual acceptability
individual acceptability based comparison argument attackers.
first proposal select argument attacker
(see Elvang-Goransson et al., 1993).
later extended Amgoud Cayrol (1998) where, using preference
relation arguments (an intrinsic valuation), argument accepted
preferred attackers.
Following proposal, propose mechanism interaction-based
valuation.
Given v gradual valuation, preordering induced v directly used order
compare, acceptability point view, argument attackers 28 .
defines new class acceptable arguments: well-defended arguments.
Definition 20 (Well-defended argument) Consider A, well-defended (for v)
B BRA, B 6 A.
Thus, capture idea argument better accepted least good
direct attackers (or incomparable case partial ordering).
set well-defended arguments depend valuation used.
Using new notion, set arguments partitioned three classes:
first class contains arguments attacked,
second class contains arguments attacked well-defended,
third class contains arguments (attacked well-defended).
Note set well-defended arguments corresponds union two first
classes. refinement uses gradual valuation inside classes
Section 4.2.
Example 9 presented Section 4.2, well-defended arguments are:
D, C2 , G, H (A incomparable B better E) valuation
tuples,
though valuation Besnard Hunter (2001) well-defended arguments
D, C2 , G, E (E better A).
Note also that, semantics Dung (1995), Definition 20 considers attackers
one one. suitable valuation handles direct attack whole
(as valuation Besnard Hunter (2001) see counterexamples presented
Section 4.4).
28. idea also used notion defeat proposed Bench-Capon (2002). So, link
well-defended argument argument attacked sense BenchCapon (2002) direct attackers. Note that, work Bench-Capon (2002), valuation
extra knowledge added argumentation framework. contrast, here, v-preference extracted
attack graph.

276

fiGraduality argumentation

4.4 Compatibility acceptability gradual valuation
Following previous sections, set arguments partitioned two different
ways:
First, given semantics gradual valuation v, possible use partition
issued Dung (1995) refined:
Uni
accepted
Cleanly
accepted

Exi
accepted

OnlyExi
accepted


accepted

Refinement level gradual valuation v

Second, given gradual valuation v, possible use partition induced
notion well-defended arguments:
Attaked
Welldefended
Arguments

Attaked
Arguments


















WellDefended





valuation
v
















Unattacked
Arguments

natural interesting question is: possible find semantics gradual
valuation v associated partitions compatibilities?
following examples show class well-defended arguments correspond class cleanly-accepted arguments (in cases, uni-accepted
arguments even well-defended).
277

fiCayrol, Lagasquie-Schiex

4.4.1 Examples showing non-compatibility general case
give examples usual valuation (the global valuation tuples 2 instances
generic local valuation: Besnard & Hunter, 2001; Jakobovits & Vermeir, 1999)
classical semantics acceptability (preferred semantics stable semantics
Dung, 1995).
Cleanly-accepted argument well-defended: 3 examples (each using distinct valuation: one global valuation two two well-known instances
local valuation):
argument cleanly-accepted well-defended:


0.4

0.5

B1

B2

B3
0.5

C2

C3
1

0.5

C1
1

1

1 preferred stable extension = { C1, C2, C3, A}
B1, B2, B3 belong preferred extension
Bi
forall = 1, 2, 3

argument cleanly-accepted well-defended:
?

C

2 preferred stable extensions : {C,A} {D,A}
B doesnt belong preferred extension
B





B
?

?

argument cleanly-accepted well-defended:
[(4,4),(3)]

[(0,...0),()]

B

C

[(),(1)]

[(2),()]


[(),(3)]
H
[(),(3)]

E

F
[(4),(5,5)]

G
[(6,6),(5)]


[(6),(7,7)]

1 preferred stable extension = {A,C,F,I}
G doesnt belong preferred extension
G


Well-defended argument cleanly-accepted: Similarly, three valuations,
have:
argument C well-defended cleanly-accepted:
?

+


?

B
C
?

1 preferred stable extension : {D, B}
C
B
C doesnt belong preferred stable extension

argument F well-defended cleanly-accepted:
278

fiGraduality argumentation

0.618

2 preferred stable extensions = {A,H,E} {B,H,F}
F belongs preferred stable extension
F
E E belongs also preferred stable extension
E attacks F

0.618
B



E

G
0.5

0.472

H
1

F
0.679

argument G well-defended cleanly-accepted:
[(4,4),(3)]

[(0,...,0),()]

B

C

[(),(1)]

[(2),()]


[(),(3)]

E

F
[(4),(5,5)]

G
[(6,6),(5)]


[(6),(7,7)]

H
[(),(3)]

1 preferred stable extension = {A,C,F,I}
G
F
G doesnt belong preferred stable extension

4.4.2 Particular cases leading compatibility
context argumentation system finite relation R without cycles 29 ,
stable preferred semantics provide one extension levels uni-accepted,
exi-accepted, cleanly-accepted coincide.
context, least two particular cases leading compatibility.
First case: deals global valuation tuples.
Theorem 1 Let G graph associated <A, R>, <A, R> argumentation
system finite relation R without cycles satisfying following condition:

Xi , leaf G, one path Xi A, Xi1 . . . Xili Xi1 = Xi
li length path (if li even, path defence branch A, else
attack branch),
paths Xi root-dependent A,
Ai A, Xj leaf G Ai belongs path Xj A.
Let v valuation tuples. Let semantics {preferred, stable}.
1. B A, B 6= A, B (exi, uni, cleanly) accepted iff B well-defended v.
2. (exi, uni, cleanly) accepted well-defended v (the converse
false).
3. well-defended v branches leading defence branches
(exi, uni, cleanly) accepted S.
29. So, (A, R) well-founded.

279

fiCayrol, Lagasquie-Schiex

Note Theorem 1 is, general, satisfied local valuation. See following
counterexample valuation Besnard Hunter (2001):
0,4

0,5

1

B1

C1

0,5

B2

1 C2

0,5

B3

1 C3

graph satisfies condition stated Theorem 1. set well-defended arguments
{C1 , C2 , C3 } (so, well-defended). Nevertheless, {C1 , C2 , C3 , A} preferred
extension.
Second case: second case concerns generic local valuation:
Theorem 2 Let <A, R> argumentation system finite relation R without
cycles. Let semantics {preferred, stable}. Let v generic local valuation
satisfying following condition ():
(i = 1 . . . n, g(xi ) xi ) (g(h(x1 , . . . , xn )) h(x1 , . . . , xn ))
()
A, (exi, uni, cleanly) accepted iff well-defended v.
theorem direct consequence following lemma:
Lemma 1 Let <A, R> argumentation system finite relation R without cycles.
Let semantics {preferred, stable}. Let v generic local valuation satisfying
condition ().
(i) exi-accepted one direct attacker B B.
(ii) B not-accepted B one direct attacker C C B.
Remark: condition () stated Theorem 2 is:
false local valuation proposed Besnard Hunter (2001) shown
following graph:
0,4

0,5

1

know g(x) =

1
1+x



B1

C1

0,5

B2

0,5

B3

1 C2

1 C3
h(x1 , . . . , xn ) = ni=1 xi

(see Property 6). get:

= 1 . . . 3, xi = v(Bi ) = 0.5,
= 1 . . . 3, g(xi ) = 0.66, g(xi ) xi ,
nevertheless g(h(x1 , x2 , x3 )) = v(A) = 0.4 6 h(x1 , x2 , x3 ) = 1.5.
280

fiGraduality argumentation

false local valuations defined h n > 1 h(x1 , . . . , xn ) >
max(x1 , . . . , xn ) (for functions g strictly non-increasing): see previous graph
h(x1 , x2 , x3 ) = 1.5 max(x1 , x2 , x3 ) = 0.5.
true local valuations defined h = max (for functions g): h = max
g(h(x1 , . . . , xn )) = g(max(x1 , . . . , xn )) = g(xj ), xj maximum xi ;
and, assumption, g(xi ) xi , xi , particular xj ; so, get:
g(h(x1 , . . . , xn )) = g(xj ) xj = max(x1 , . . . , xn ) = h(x1 , . . . , xn ).

5. Conclusion
paper, introduced graduality two main related issues argumentation
systems:
valuation arguments,
acceptability arguments.
Regarding first issue, defined two formalisms introducing interaction-based
gradual valuation arguments.
First, generic gradual valuation covers existing proposals (for example Besnard
& Hunter, 2001 Jakobovits & Vermeir, 1999). approach essentially local
since computes value argument value direct attackers.
Then, approach based labelling takes form pair tuples;
labelling memorises structure graph representing interactions (the
attack graph), associating branch length (number edges
leaf current node) attack graph (if length branch even
integer, branch defence branch current node, otherwise branch
attack branch current node). approach said global since
computes value argument using whole attack graph influencing
argument.
shown valuations induces preordering set arguments, brought light main differences two approaches.
Regarding second issue, two distinct approaches proposed:
First, context collective acceptability Dung (1995): three levels
acceptability (uni-accepted, exi-accepted, not-accepted) already defined.
graduality introduced collective acceptability using notion cleanlyaccepted arguments (those whose direct attackers not-accepted).
Then, context individual acceptability: using previously defined gradual
valuations, new notion well-defended arguments introduced (those
preferred direct attackers sense given gradual valuation
v).
first concept induces refinement level exi-accepted two sublevels (cleanlyaccepted arguments only-exi-accepted arguments). gradual valuation allows graduality inside level collective acceptability.
281

fiCayrol, Lagasquie-Schiex

second concept induces two new levels acceptability (well-defended arguments
not-well-defended arguments). gradual valuation also allows graduality inside
level individual acceptability.
Regarding initial purpose introducing graduality definition acceptability,
adopted basic principle:
acceptability strongly related interactions arguments (represented
graph interactions),
argument acceptable preferred direct attackers.
Then, followed two different directions. One based refinement existing
partition remains framework Dungs work. one based
original concept well-defended, deserves investigation, particular
computational point view.

Acknowledgements
Thanks reviewers interesting constructive comments.
Thanks Thomas Schiex help.

Appendix A. proofs
section, give proofs properties presented Sections 3 4.
Proof
(of Property 1) induction VMin g(VMax ) < VMax applying
function g twice.


Proof
(of Property 2) valuation function v associates argument
value v(A) belonging set V subset completely ordered set
W.


Proof
(of Property 3) Let C = An1 . . . A2 A1 cycle:
n even: n = 2k v(A1 ) = g(v(A2 )) = . . . = g 2k1 (v(A2k )) =
g 2k (v(A1 )); so, v(A1 ) fixpoint g 2k = g n . Ai ,
1 2k.
However, Ai may different values: example, n = 2,
valuation Jakobovits Vermeir (1999), v(A1 ) = + v(A2 ) =
g(+) = g() = +. Ai value,
value fixpoint g (because v(A1 ) = g(v(A2 )) = g(v(A1 ))).
282

fiGraduality argumentation

n odd: n = 2k + 1 v(A1 ) = g(v(A2 )) = . . . = g 2k (v(A2k+1 )) =
g 2k+1 (v(A1 )); so, v(A1 ) fixpoint g 2k+1 = g n .
Ai , 1 2k + 1.
Since function g non-increasing, function g 2k+1 also nonincreasing apply following result: non-increasing function fixpoints, fixpoints identical30 . So, v(A1 ) = . . . =
v(A2k+1 ). But, v(A1 ) = g(v(A2 )) = g(v(A1 )), v(A1 ) fixpoint g.
So, 1 2k + 1, v(Ai ) fixpoint g.

Proof
(of Property 4)
P1 satisfied because: A, direct attacker (R (A) empty),
v(A) = VMax g(VMax ) < VMax .
P2 satisfied R (A) = {A1 , . . . , }, h(v(A1 ), . . . , v(An )) evaluates
direct attack A.
P3 satisfied function g supposed non-increasing.
P4 satisfied due properties function h.



Proof
(of Property 5) valuation proposed Jakobovits Vermeir (1999)
following:
Let <A, R> argumentation system. complete labelling <A, R>
function Et : {+, ?, } that:
1. Et(A) {?, } B R (A) Et(B) {+, ?}

2. Et(A) {+, ?} B R (A) R+ (A), Et(B) {?, }
Moreover, Jakobovits Vermeir (1999) also define complete rooted labelling
Et with: A, Et(A) = B R (A) Et(B) = +.

translation Et local gradual valuation easy:

g defined g() = +, g(+) = , g(?) =? h function max.



Proof
(of Property 6) Besnard Hunter (2001) introduce following function
Cat (in context deductive arguments acyclic graph):
R (A) = , Cat(A) = 1
30. Proof: let g non-increasing function, let two fixpoints g. 6= , may suppose
> , g() g() (since g non-increasing), (since fixpoints g),
contradiction assumption > .

283

fiCayrol, Lagasquie-Schiex

R (A) 6= R (A) = {A1 , . . . , }, Cat(A) =

1
1+Cat(A1 )+...+Cat(An )

translation Cat gradual valuation is: V = [0, 1], W = [0, [,
1
h
VMin = 0 VMax = 1 g : W V defined g(x) = 1+x
defined h({x1 , . . . , xn }) = x1 + + xn .

Proof
(of Property 7) Let = (x1 , . . . , xn , . . .), t0 = (y1 , . . . , yn , . . .), t00 = (z1 , . . . , zn , . . .)
tuples.
Commutativity ?: ? t0 = t0 ? two cases:
t0 = 0 , property given Definition 8.
t0 6= 0 :
? t0 = Sort(x1 , . . . , xn , . . . , y1 , . . . , yn , . . .)
= Sort(y1 , . . . , yn , . . . , x1 , . . . , xn , . . .)
= t0 ?
Associativity ?: (t ? t0 ) ? t00 = ? (t0 ? t00 ) two cases:
t0 t00 = 0 , simplify expression. example,
= 0 :
(t ? t0 ) ? t00 = t0 ? t00
= ? (t0 ? t00 )
t, t0 t00 6= 0 :
(t ? t0 ) ? t00 = Sort(x1 , . . . , xn , . . . , y1 , . . . , yn , . . . , z1 , . . . , zn , . . .)
= ? (t0 ? t00 )
Property : (t k) k 0 = (k + k 0 ) have:
(t k) k 0 = (x1 + k, . . . , xn + k, . . .) k 0

= (x1 + k + k 0 , . . . , xn + k + k 0 , . . .)

= (k + k 0 )
Distributivity: (t ? t0 ) k = (t k) ? (t0 k) have:
(t ? t0 ) k = Sort(x1 , . . . , xn , . . . , x01 , . . . , x0n , . . .) k

= Sort(x1 + k, . . . , xn + k, . . . , x01 + k, . . . , x0n + k, . . .)

= (t k) ? (t0 k)


284

fiGraduality argumentation

Proof
(of Property 9) First, show relation defined Algorithm 1
partial ordering:
Let u, v, w three tupled values, relation defined Algorithm 1 is:
reflexive: u u u = u, u u u u (case 1
Algorithm 1);
transitive: suppose u v v w consider
possible cases:
u = v:
v = w: u = w u w,
|vi | |wi | |vp | > |wp |: |vi | = |ui | |wi |
|vp | = |up | > |wp |, u w,
|vi | < |wi | |vp | |wp |: |vi | = |ui | < |wi |
|vp | = |up | |wp |, u w,
|vi | = |wi | |vp | = |wp | vp lex wp
vi lex wi : |vi | = |ui | = |wi | |vp | = |up | = |wp |
vp = lex wp vi = ui lex wi , u w;
|ui | |vi | |up | > |vp |:
v = w: |ui | |vi | = |wi | |up | > |vp | = |wp |
u w,
|vi | |wi | |vp | > |wp |: |ui | |vi | |wi |
|up | > |vp | > |wp |, u w,
|vi | < |wi | |vp | |wp |: |ui | |vi | < |wi |
|up | > |vp | |wp |, u w,
|vi | = |wi | |vp | = |wp |: |ui | |vi | = |wi |
|up | > |vp | = |wp |, u w;
|ui | < |vi | |up | |vp |:
v = w: |ui | < |vi | = |wi | |up | |vp | = |wp |
u w,
|vi | |wi | |vp | > |wp |: |ui | < |vi | |wi |
|up | |vp | > |wp |, u w,
|vi | < |wi | |vp | |wp |: |ui | < |vi | < |wi |
|up | |vp | |wp |, u w,
|vi | = |wi | |vp | = |wp |: |ui | < |vi | = |wi |
|up | |vp | = |wp |, u w;
|ui | = |vi | |up | = |vp | lex vp ui lex
vi :
v = w: |ui | = |vi | = |wi | |up | = |vp | = |wp |
lex vp = wp ui lex vi = wi u w,
|vi | |wi | |vp | > |wp |: |ui | = |vi | |wi |
|up | = |vp | > |wp |, u w,
285

fiCayrol, Lagasquie-Schiex

|vi | < |wi | |vp | |wp |: |ui | = |vi | < |wi |
|up | = |vp | |wp |, u w,
|vi | = |wi | |vp | = |wp | vp lex wp
vi lex wi : |ui | = |vi | = |wi | |up | = |vp | = |wp |
lex vp lex wp ui lex vi lex wi ,
u w.
cases, u w.
Now, consider maximal minimal values:
tupled value [0 , ()] unique maximal element preordering
: let v tupled value v 6= [0 , ()], |vp | |vi | 0.
Compare [0 , ()] v Algorithm 1: [0 , ()] 6= v case number
1 used; then, |()| = 0 |vi | |0 | = |vp | two
cases:
|vp | = |vi | = 0, case 3 Algorithm 1 applied
[0 , ()] v,
else |vp | |vi | 0, case 5 Algorithm 1 applied
[0 , ()] v.
tupled value [(), 1 ] unique minimal element preordering
: let v tupled value v 6= [(), 1 ], |vi | |vp | 0.
Compare [(), 1 ] v Algorithm 1: [(), 1 ] 6= v case number
1 used; then, |()| = 0 |vp | |1 | = |vi | two
cases:
|vi | = |vp | = 0, case 2 Algorithm 1 applied
[(), 1 ] v,
else |vi | |vp | 0, case 6 Algorithm 1 applied
[(), 1 ] v.

Proof
(of Property 10) principle P10 satisfied Definition 10
fact [0 , ()] unique maximal element v(A) (see Property 9).
principle P20 satisfied Definition 10.
principles P30 P40 satisfied: possible cases improvement/degradation defence/attack given argument (see Definition 16)
applied case case31 . case leads new argument. Using Algorithm 1, comparison argument application
case shows principle P30 (or P40 , depending applied case)
31. work case case order avoid complex cases several simultaneous simple
modifications. example, modification length branch changes status
branch (an even integer replaced odd integer) complex case corresponding two simple cases:
removal branch given status, addition new branch different status.

286

fiGraduality argumentation

satisfied.



Proof
(of Property 11) Definition 10.



Proof
(of Property 12) First, consider case preferred extensions: Let
E preferred extension A, assume E contain
unattacked arguments A. So, let unattacked argument
6 E.
Consider E {A}:
E {A} conflict-free then, unattacked argument E
preferred extension, E {A} collectively defends itself, E {A}
admissible E E {A}. contradicts fact E preferred
extension.
E {A} contains least one conflict, then:

B E BRA. impossible since unattacked.
B E ARB. But, since unattacked, @C E
CRA. So, E collectively defend B,
contradiction fact E preferred extension.

So, assumption E contain unattacked arguments
cannot hold.
Now, consider stable extensions: Let E stable extension A, assume
E contain unattacked arguments A. So, let
unattacked argument 6 E.
Since 6 E exists E another argument B attacks A;
impossible since unattacked.
So, assumption E contain unattacked arguments
cannot hold.


Proof
(of Property 13) argument one direct attackers cannot belong
extension sense Dung (1995) extension must
conflict-free. So, since uni-accepted, means belongs
extensions, none direct attackers belongs extensions.
converse, use following counterexample case preferred
semantics:
287

fiCayrol, Lagasquie-Schiex

F



K

C

B
H

J

E

G

two preferred extensions
{K, H, G} {A, E, K, H}. argument cleanly-accepted (B
C belong preferred extension, belongs least one
two extensions). But,
uni-accepted belong
preferred extensions.



Proof
(of Property 14) First, uni-accepted cleanly-accepted result
Property 13.
Conversely, let cleanly-accepted argument, exists least one stable extension E E B, BRA, B 6 E 0 , E 0 stable extension.
Using reductio ad absurdum, assume exists stable extension
E 00 6 E 00 ; but, 6 E 00 , means B E 00 BRA,
so, direct attacker B belongs stable extension; so, contradiction assumption (A cleanly-accepted); so, E 00 exist
uni-accepted.


Proof
(of Theorem 1)
1. consider arguments B B 6= A. Let Xi leaf,
path C C(Xi , A) Xi1 . . . Xili Xi1 = Xi li denoting
length path (if li even, path defence branch A, else
attack branch).
constraints Xi1 Xili following:
Xi1 Xi3 . . . Xili Xili 1 . . . Xi4 Xi2 li odd 1

Xi1 Xi3 . . . Xili 1 Xili . . . Xi4 Xi2 li even 2
So, path Xi1 . . . Xili , set well-defended arguments
{Xi1 , Xi3 , . . . , Xili } li odd, {Xi1 , Xi3 , . . . , Xili 1 } otherwise (this
set arguments value strictly better
direct attackers). set denoted Accepi .
definition, set conflict-free, defends elements (because
contains leaf path arguments defended
leaf) attacks arguments path. try
288

fiGraduality argumentation

include another argument path X {Xi1 , . . . , Xili }\ Accepi ,
obtain conflict (because arguments path attacked
elements Accepi ). So, {Xi1 , . . . , Xili }, Accepi
preferred stable extension.
Consider A0 = \ {A}, R0 restriction R A032
Union Accep= Accepi , Union Accep preferred
stable extension <A0 , R0 >.
So, B A, B 6= A, B accepted iff B well-defended.

2. Now, consider A. accepted Union Accep {A}
preferred stable extension <A, R>. So, i, Xili belong
extension. Then, i, Xili 1 Xili . Therefore, branch leading
defence branch A. So, i, v(Xili ) = [()(li 1)]. So, v(A) =
[(l1 , l2 , . . . , ln )()]. Then, i, v(A) v(Xili ). Therefore, well-defended.
Using following example, show converse false:
[(2,2)(3)]

A1

[()(1)]

B1

C1
[(0...0)()]

[()(1)]

B2

[()(1)]

C2
[(0...0)()]

[(2)()]

B3

C3
[(0...0)()]

well-defended (A B1 , B2 incomparable A1 )
accepted.
3. Now, well-defended branches leading defence
branches A, Union Accep {A} conflict-free defended
direct attackers (because Xili 1 Union Accep
branch i). So, Union Accep {A} preferred stable extension
<A, R> accepted.


Proof
(of Lemma 1) Let <A, R> argumentation system finite relation
R without cycles (so, one non empty preferred stable extension
denoted E). know that:
exi-accepted direct attacker denoted B B
not-accepted,
32. R0 restriction R A0 R0 = {(a, b)|aRb, A0 , b A0 }.

289

fiCayrol, Lagasquie-Schiex

B not-accepted exists least one argument C
CRB C exi-accepted (because B belong E E
stable, C must E). So, fortiori, B not-accepted
one direct attacker C, C exi-accepted.
proof done induction depth proof tree C.
Basic case (i): exi-accepted one direct attacker B (BRA)
C1 . . . Cn direct attackers B; so, proof tree whose
depth 2 one unattacked Ci , example C1 ; so:
v(B) = g(h(v(C1 ), . . . , v(Cn )))
g(v(C1 ))

h(v(C1 ), . . . , v(Cn )) h(v(C1 )) = v(C1 )

g non-increasing

g(VMax )

v(C1 ) = VMax

so:

v(A) = g(v(B))
g 2 (VMax )
But, Property 1 says g 2 (VMax ) g(VMax ), v(A) v(B).
Basic case (ii): CRB C direct attacker B; so,
proof tree whose depth 0 C, i.e. C unattacked; so, v(C) = VMax
v(B) = g(VMax ) v(C) (following Definition 6).
General case (i): exi-accepted one direct attacker B
(BRA) C1 . . . Cn direct attackers B, one Ci exiaccepted, example C1 ; consider subgraph leading C1
add C1 RBRA, assume:
g(v(C1 )) v(C1 ) (induction assumption issued (ii))
So:

v(B) = g(h(v(C1 ), . . . , v(Cn )))
g(v(C1 ))
v(C1 )

reasons basic case
induction assumption

h(v(C1 ), . . . , v(Cn ))

property h

non-increasing g:

v(A) = g(v(B))
g(h(v(C1 ), . . . , v(Cn ))) = v(B)
290

fiGraduality argumentation

General case (ii): B not-accepted, C exi-accepted; assume
C several direct attackers D1 . . . Dp not-accepted
(because C exi-accepted); consider subgraph leading Di
add Di RCRB assume:
= 1 . . . p, g(v(Di )) v(Di ) (induction assumption issued (i))
so:
v(C) = g(h(v(D1 ), . . . , v(Dp )))
h(v(D1 ), . . . , v(Dp ))

application condition ()
since induction assumption
corresponds premise ()

so:
v(B) = g(v(C))
g(h(v(D1 ), . . . , v(Dp ))) = v(C)

Proof
(of Theorem 2) Assume () true consider exiaccepted. Let Bi , = 1 . . . n, direct attackers A. Then, =
1 . . . n, subgraph leading Bi completed Bi RA, apply
lemma obtain: g(v(Bi )) v(Bi ), = 1 . . . n. Thus, have:
v(A) = g(h(v(B1 ), . . . , v(Bn )))
h(v(B1 ), . . . , v(Bn ))

v(Bi ), = 1 . . . n

applying ()
property h

So, well-defended.
converse, let well-defended. Let B1 , . . . , Bn direct
attackers assume exi-accepted. Then, exists
least one direct attacker Bi Bi exi-accepted (because
one preferred stable extension). apply (ii) lemma
subgraph leading Bi completed Bi RA obtain g(v(Bi )) v(Bi ).
So, exists Bi direct attacker that:

v(A) = g(h(v(B1 ), . . . , v(Bn )))
g(v(Bi ))

v(Bi )

property h non-increasing g
using lemma
291

fiCayrol, Lagasquie-Schiex

contradiction well-defended. So, exi-accepted.



Appendix B. Computation tupled values
propose algorithm computing tupled values arbitrary graph (cyclic
acyclic, cycles may isolated not). algorithm uses principle propagation
values: argument evaluated values direct attackers known.
must consider cycles meta-arguments evaluated direct
attackers cycle (i.e. direct attackers one elements cycle
belong cycle) evaluated.
beginning process follows: consider arguments
initial value [0 , ()], leaves graph marked final
values. Thus, following partition graph G:
Gv : part graph already evaluated (at beginning, part contains
leaves graph),
Gv : part graph evaluated (at beginning, part contains
arguments graph G except leaves).
algorithm also relies special data structure denoted L giving list
cycles graph main characteristics:
list arguments belong cycle,
list arguments belong cycle direct attackers outside
cycle (these arguments called inputs cycle; used
order propagate values across cycle case non isolated cycle);
list empty case isolated cycle.
Remark: sake efficiency, interconnected cycles (see Definition 1)
considered whole algorithm used like meta-cycle. example,
two cycles B B C B direct attacker outside
cycles, described data structure L one meta-cycle
following lists:
A, B, C,
nothing (because isolated meta-cycle).
order avoid ambiguity, meta-cycles defined mcycles:
Definition 21 (mcycle) Let G attack graph. Let CC set cycles
G. Let CC 0 CC CC 0 = {C1 , . . . , Cn } set cycles.
Let ACC 0 set: {Aj Ci CC 0 Aj Ci }.
CC 0 satisfies following properties:
Aj , Ak ACC 0 , path Aj Ak element (arguments edges
arguments) path belongs cycles CC 0 ,
Ck CC \ CC 0 , 6 Ci CC 0 Ck interconnected Ci .
292

fiGraduality argumentation

union Ci belonging CC 0 mcycle.
Thus, make partition CC using notion interconnection cycles,
element partition different mcycle. See following example:


J
C

B

E

F

G



K

L

graph, 6 cycles:
{J},
{I, J, K},
{K, L},
{B, C, D},
{C, E},
{F, G}.
3 mcycles:
{I, J, K, L},
{B, C, D, E},
{F, G}.
Algorithm 2 main algorithm used computing tupled values.
function Add-Node (respectively Remove-Node) whose parameters subgraph
Gx attack graph node s, adds (resp. removes) (resp. of) Gx .
functions described (Cayrol & Lagasquie-Schiex, 2003a).
Algorithm 2 applied example step rewriting (see Figure 1). Note
order make understanding results easier, created new
arguments (as Definitions 11 12), course, would necessary rigorous
formalization.

293

fiCayrol, Lagasquie-Schiex

Algorithm 2: Algorithm computing tupled values
% Description parameters:
%
G: attack graph (partitioned Gv Gv )
%
L: data structure describing mcycles
%
n: number propagation steps mcycles
% Used variables:
%
A: current argument (to evaluated)
%
C: current mcycle (to evaluated) (containing A)
%
LAD: list direct attackers C
%
Bi : current direct attackers A, C

begin
least one argument Gv
2
= Choose-Argument(Gv )
3
belong mcycle C described L
4
Bi R (A), Bi already evaluated
5
Gv = Add-Node(Gv ,Evaluate-Node(A, R (A), 1))

%
%
%
%
%
%
%
%
%

1

Gv = Remove-Node(Gv , A)

6
7
8
9
10
11
12
13
14
15

% value
% value
% direct attackers
% add 1
% see Definition 10

else
C isolated
Gv = Add-Mcycle(Gv ,Evaluate-Mcycle-Isolated(G, C, n))
Gv = Remove-Mcycle(Gv , C)

else
LAD = Find-Direct-Attackers-Mcycle(C, G)
Bi LAD, Bi already evaluated
Gv = Add-Mcycle(Gv ,
Evaluate-Mcycle-Not-Isolated(G, C, LAD,n))
Gv = Remove-Mcycle(Gv , C)

return G
end

16

294

%
%
%
%
%

fiGraduality argumentation



C

B



E

G

F

previous argumentation graph rewritten follows:









B

B

B

C



C



C



B

B



B

B

E

C

C

E

C

C

C











B

B



B

C



C

B



C



B

B

C

B

B

E



C

C

C

C

C

B

B

B

....

....
B

C

....


....
E

G
F

results valuation obtained one propagation step are:
v(A) = [(0, . . . , 0)()],
v(B) = [(6, 8, 8, . . .)(1, 3, 5, . . .)],
v(C) = [(2, 4, 6, . . .)(5, . . .)],
v(D) = [(6, . . .)(3, 5, . . .)],
v(E) = [(4, 6, . . .)()],
v(F ) = [(8, . . .)(3, 5, 5, 7, 7, . . .)],
v(G) = [(2, 4, 6, . . .)(7, . . .)],
Figure 1: Example rewriting

295

fiCayrol, Lagasquie-Schiex

References
Amgoud, L., & Cayrol, C. (1998). acceptability arguments preference-based
argumentation. Cooper, G. F., & Moral, S. (Eds.), Proc. 14th Uncertainty
Artificial Intelligence, pp. 17, Madison, Wisconsin. Morgan-Kaufmann.
Amgoud, L., & Cayrol, C. (2002). Inferring inconsistency preference-based argumentation frameworks. Journal Automated Reasoning, 29, 125169.
Bench-Capon, T. J. (2002). Value based argumentation frameworks. Benferhat, &
Giunchiglia (Eds.), Proc. 9th International Workshop Nonmonotonic Reasoning (session Argument, Dialogue Decision), pp. 444453, Toulouse, France.
Besnard, P., & Hunter, A. (2001). logic-based theory deductive arguments. Artificial
Intelligence, 128 (1-2), 203235.
Cayrol, C., & Lagasquie-Schiex, M.-C. (2003a). Critique et amelioration de levaluation
graduelle par tuples pour le traitement des circuits. Rapport de recherche 2003-13-R,
Institut de Recherche en Informatique de Toulouse (I.R.I.T.), France.
Cayrol, C., & Lagasquie-Schiex, M.-C. (2003b). Gradual acceptability argumentation
systems. Proc. 3rd CMNA (International workshop computational models
natural argument), pp. 5558, Acapulco, Mexique.
Cayrol, C., & Lagasquie-Schiex, M.-C. (2003c). Gradual handling contradiction argumentation frameworks. Bouchon-Meunier, B., L.Foulloy, & Yager, R. (Eds.),
Intelligent Systems Information Processing: representation Applications,
chap. Reasoning, pp. 179190. Elsevier.
Doutre, S. (2002). Autour de la semantique preferee des systemes dargumentation. These,
Universite Paul Sabatier, IRIT.
Dung, P. M. (1995). acceptability arguments fundamental role nonmonotonic reasoning, logic programming n-person games. Artificial Intelligence,
77, 321357.
Dunne, P. E., & Bench-Capon, T. J. (2001). Coherence finite argument systems. Technical
report 01-006, University Liverpool, Department Computer Science (U.L.C.S.).
Dunne, P. E., & Bench-Capon, T. J. (2002). Coherence finite argument system. Artificial
Intelligence, 141 (1-2), 187203.
Elvang-Goransson, M., Fox, J., & Krause, P. (1993). Dialectic reasoning inconsistent
information. Heckerman, D., & Mamdani, A. (Eds.), Proc. 9th UAI, pp.
114121, Washington, DC. Morgan-Kaufmann.
Jakobovits, H., & Vermeir, D. (1999). Robust semantics argumentation frameworks.
Journal logic computation, 9(2), 215261.
Karacapilidis, N., & Papadias, D. (2001). Computer supported argumentation collaborative decision making: hermes system. Information systems, 26 (4), 259277.
Kohlas, J., Haenni, R., & Berzati, D. (2000). Probabilistic argumentation systems
abduction. Proc. 8th International Workshop Non-Monotonic Reasoning
- special session Uncertainty Frameworks Non-Monotonic Reasoning, pp. 391
398, Breckenridge, Colorado.
296

fiGraduality argumentation

Krause, P., Ambler, S., Elvang, M., & Fox, J. (1995). logic argumentation reasoning
uncertainty. Computational Intelligence, 11 (1), 113131.
Lin, F., & Shoham, Y. (1989). Argument systems - uniform basis non-monotonic
reasoning. Proc. first International Conference Principles Knowledge
Representation Reasoning (KR), pp. 245255.
Parsons, S. (1997). Normative argumentation qualitative probability. Proc.
first International Joint Conference Qualitative quantitative practical reasoning, ECSQARU-FAPR, LNAI 1244, pp. 466480, Germany.
Pinkas, G., & Loui, R. P. (1992). Reasoning inconsistency: taxonomy principles
resolving conflict. Allen, J., Fikes, R., & Sandewall, E. (Eds.), Proc. 3rd
KR, pp. 709719, Cambridge, MA. Morgan-Kaufmann.
Pollock, J. L. (1992). reason defeasibly. Artificial Intelligence, 57, 142.
Pollock, J. L. (2001). Defeasible reasoning variable degrees justification. Artificial
Intelligence, 133, 233282.
Prakken, H., & Sartor, G. (1997). Argument-based extended logic programming defeasible priorities. Journal Applied Non-Classical Logics, 7, 2575.
Simari, G., & Loui, R. (1992). mathematical treatment defeasible reasoning
implementation. Artificial Intelligence, 53, 125157.
Verheij, B. (2002). existence multiplicity extension dialectical argumentation. Benferhat, S., & Giunchiglia, E. (Eds.), Proceedings 9th International
Workshop Non-Monotonic Reasoning (NMR2002), pp. 416425.
Vreeswijk, G. (1997). Abstract argumentation systems. Artificial Intelligence, 90, 225279.
Xuong, N. (1992). Mathematiques discretes et informatique. Masson.

297

fiJournal Artificial Intelligence Research 23 (2005) 533-585

Submitted 4/04; published 5/05

Using Memory Transform Search Planning Graph
Terry Zimmerman

WIZIM@CS.CMU.EDU

Robotics Institute, Carnegie Mellon University
Pittsburgh, PA 15213-3890

Subbarao Kambhampati

RAO@ASU.EDU

Department Computer Science & Engineering
Arizona State University, Tempe AZ 85287-5406

Abstract
Graphplan algorithm generating optimal make-span plans containing parallel sets actions remains one effective ways generate plans. However, despite enhancements range fronts, approach currently dominated terms speed, state space
planners employ distance-based heuristics quickly generate serial plans. report family strategies employ available memory construct search trace learn various
aspects Graphplans iterative search episodes order expedite search subsequent episodes.
planning approaches partitioned two classes according type extent
search experience captured trace. planners using aggressive tracing method
able avoid much Graphplans redundant search effort, planners second class trade
aspect favor much higher degree freedom Graphplan traversing space
states generated regression search planning graph. tactic favored second approach, exploiting search trace transform depth-first, IDA* nature Graphplans
search iterative state space view, shown powerful. demonstrate
distance-based, state space heuristics adapted informed traversal search trace used
second class planners develop augmentation targeted specifically planning graph
search. Guided heuristic, step-optimal version planner class clearly
dominates even highly enhanced version Graphplan. adopting beam search search
trace show virtually optimal parallel plans generated speeds quite competitive
modern heuristic state space planner.

1. Introduction
Graphplan introduced 1995 (Blum & Furst, 1995) became one fastest programs
solving benchmark planning problems time and, accounts, constituted radically different approach automated planning. Despite recent dominance heuristic state-search
planners Graphplan-style planners, Graphplan approach still one effective ways
generate so-called optimal parallel plans. State-space planners drowned exponential
branching factors search space parallel plans (the exponential branching result fact
planner needs consider subset non-interfering actions). 8 years since
introduction, Graphplan system enhanced numerous fronts, ranging planning
graph construction efficiencies reduce size build time one orders magnitude (Smith & Weld, 1998; Long & Fox, 1999), search speedup techniques variable
value ordering, dependency-directed backtracking, explanation based learning (Kambhampati,
2000). spite advances, Graphplan ceded lead planning speed variety heuristic-guided planners (Bonet & Geffner, 1999; Nguyen & Kambhampati, 2000; Gerevini & Serina,
2002). Notably, several exploit planning graph powerful state-space heuristics,
2005 AI Access Foundation. rights reserved.

fiZIMMERMAN & KAMBHAMPATI

eschewing search graph itself. Nonetheless, Graphplan approach remains perhaps fastest parallel planning mainly way combines iterative deepening A* (IDA*,
Korf, 1985) search style highly efficient CSP-based incremental generation applicable action
subsets.
investigate use available memory surmount Graphplans major
drawbacks, redundant search effort need exhaustively search k-length planning
graph proceeding k+1 length graph. time wish retain attractive features Graphplans IDA* search rapid generation parallel action steps ability
find step optimal plans. approach describe remains rooted iterative search planning
graph greatly expedites search building maintaining concise search trace.
Graphplan alternates two phases; one data structure called planning graph
incrementally extended, backward phase planning graph searched extract
valid plan. first regression search phase space explored given episode closely
correlated conducted preceding episode. strategy pursue work employ appropriately designed trace search conducted episode n (which failed find solution) identify avoid aspects search provably unchanged episode n+1,
focus effort features may evolved. identified precisely features dynamic across Graphplan search episodes construct search traces capture exploit features different degrees. Depending design search trace may provide benefits 1)
avoidance much Graphplans redundant search effort, 2) learning iterative search experience improve heuristics constraints embodied planning graph, 3)
realizing much higher degree freedom Graphplan, traversing space states generated regression search process. show third advantage particularly key
search trace effectiveness, allows planner focus attention promising areas
search space.
issue much memory right amount use boost algorithms performance
cuts across range computational approaches search paging process operating systems, Internet browsing database processing operations. investigation explore several alternative search trace based methods differ markedly terms memory demands.
describe four approaches paper. Figure 1 depicts pedigree family search
trace-based planners, well primary impetus leading evolution system
predecessor. figure also suggests relative degree planner steps away
original IDA* search process underlying Graphplan. two tracks correspond two genres
search trace developed;


left track: EGBG planners (Explanation Guided Backward search Graphplan) employ
comprehensive search trace focused minimizing redundant search.



right track: PEGG planners (Pilot Explanation Guided Graphplan) use skeletal
trace, incurring Graphplans redundant search effort exchange reduced memory
demands increased ability exploit state space view search space.

EGBG planner (Zimmerman & Kambhampati, 1999) adopts memory intensive structure
search trace seeks primarily minimize redundant consistency-checking across Graphplans
search iterations. proves effective range smaller problems memory constraints

534

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

XY

Search
Trace
Exploiting Graphplan
Symmetry & Redundancy

Exploiting
state space view

EGBG
Leveraging CSP & memory efficiency

so-PEGG
me-EGBG
Trading step-optimality
speedup episodes

PEGG

Figure 1: Applying available memory step away Graphplan search process;
family search trace-based planners
impede ability scale up. Noting Graphplans search process viewed specialized
form CSP search (Kambhampati, 2000), explore middle ground terms memory usage augmenting EGBG several methods known effective speedup techniques CSP
problems.
primary interest techniques, however, impact memory reduction describe accomplish beyond search speedup benefit afford. implemented planner, me-EGBG, markedly outperforms EGBG speed capabilities, variety
problems still lie beyond planners reach due memory constraints.
search trace structure used PEGG track planners trades minimization redundant
search exchange much smaller memory footprint. addition greatly reduced memory
demands, PEGG search trace structure exploited intrinsic state space view
essentially Graphplans CSP-oriented search space. significant speedup advantage approach
Graphplan EGBG track planners derives ability employ distance-based
heuristics power many current generation state-space planners (Bonet & Geffner, 1999;
Nguyen & Kambhampati, 2000; Hoffman, 2001). adapt heuristics task identifying
promising states visit search trace implement approach first so-PEGG
planner (step-optimal PEGG, Zimmerman & Kambhampati, 2003). So-PEGG outperforms even
highly enhanced version Graphplan two orders magnitude terms speed,
maintaining guarantee finding step-optimal plan.
Finally explore adoption beam search approach visiting state space implicit
PEGG-style trace. employ distance-based heuristics extracted planning graph
itself, direct order search trace states visited, also prune restrict
space heuristically best set states, according user-specified metric. show
planning graph leveraged provide measure likelihood previously generated regression state might spawn new search branches higher planning graph level.
535

fiZIMMERMAN & KAMBHAMPATI

term metric flux employ effective filter states skipped even
though might appear promising based distance-based heuristic. Implemented PEGG
system (Zimmerman & Kambhampati, 2003), approach exploiting search trace produces
two-fold benefit previous approaches; 1) reduction search trace memory demands
2) effective release Graphplans exhaustive search planning graph search episodes. PEGG exhibits speedups ranging 300x enhanced version Graphplan
quite competitive recent state space planner using similar heuristics. adopting beam
search PEGG necessarily sacrifices guarantee step-optimality empirical evidence indicates
secondary heuristics remarkably effective ensuring make-span solutions produced
virtually optimal.
fact systems successfully employ search trace noteworthy. general,
tactic adopting search trace algorithms explicitly generate node-states iterative
search episodes, found infeasible due memory demands exponential
depth solution. Sections 2 3 describe tight integration search trace
planning graph permits EGBG PEGG planners largely circumvent issue.
planning graph structure costly construct, terms memory time;
well-known problems even domains problematic planners employ it. (PostGraphplan planners employ planning graph purpose include STAN, Long & Fox,
1999, Blackbox, Kautz & Selman, 1999, IPP, Koehler et al., 1997, AltAlt, Nguyen & Kambhampati, 2000, LPG Gerevini & Serina, 2002). planning systems described share
memory overhead course, interestingly, found search trace memory demands
PEGG class planners significantly limited range problems solve.
remainder paper organized follows: Section 2 provides brief overview
planning graph Graphplans search process. discussion CSP nature manner process viewed IDA* search motivates potential employing available memory accelerate solution extraction. Section 3 addresses two primary challenges attempting build use search trace advantage Graphplan: 1) done within
reasonable memory constraints given Graphplans CSP-style search planning graph? and, 2)
trace available, effectively used? section briefly describes EGBG
(Zimmerman & Kambhampati, 1999), first system use search trace guide Graphplans
search, outlines limitations method (Details algorithm contained Appendix
A.) Section 4 summarizes investigations variety memory reduction techniques reports impact combination six performance EGBG. PEGG planners
discussed Section 5 performance so-PEGG PEGG (using beam search) compared enhanced version Graphplan, EGBG, modern, serial state-space planner. Section
6 contains discussion findings Section 7 compares work related research. Finally,
Section 8 wraps conclusions.
2. Background & Motivation: Planning Graphs Nature Direct
Graph Search
outline Graphplan algorithm discuss traits suggesting judicious use additional
memory might greatly improve performance. touch three related views Graphplans
search; 1) form CSP, 2) IDA* search and, 3) state space aspect.
536

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

Actions
Level 1

Initial
State
W

Planning Graph
Propositions
Propositions
Actions
Level 1
Level 2
Level 2
nop
nop

~W
W

nop

a1


Actions
Level 3

~W

nop

~W

W

nop

W

a1

a1

nop

~Y


nop

nop

nop

X
~Y

Z
H

a2

nop



a3

a3

nop

H

nop



nop

~Y


nop

H

Propositions
Level 3

a3



a4
a5

a5

nop

J

a5

J

nop

J

Domain Actions



a1

IHW
HJ

a2

X

action descriptions:



a3

~W
IJ

a4

Z


a5

J ~Y

action-ID [effects]
[preconditions]

Figure 2: Planning graph representation three levels Alpha domain
2.1 Construction Search Planning Graph
Graphplan algorithm employs two interleaved phases forward phase, data structure
called planning graph incrementally extended, backward phase planning graph
searched extract valid plan. planning graph consists two alternating structures, called
proposition lists action lists. bottom Figure 2 depicted simple domain refer
Alpha domain use illustration study. figure shows four action proposition levels planning graph engendered simple initial state given domain. start
initial state zeroth level proposition list. Given k-level planning graph, extension
graph structure level k+1 involves introducing actions whose preconditions present
kth level proposition list. addition actions domain model, operation actions
introduced, one condition kth level proposition list (abbreviated nop papers
figures, also termed persists others). nop-C action C precondition C
effect. Given kth level actions, proposition list level k+1 constructed union
effects introduced actions. planning graph maintains dependency links

537

fiZIMMERMAN & KAMBHAMPATI

actions level k+1, preconditions level k proposition list, effects level
k+1 proposition list.
planning graph construction binary "mutex'' constraints computed propagated.
Figure 2, arcs denote mutex relations pairs propositions pairs actions.
propagation starts level 1 labeling mutex pairs actions statically interfering
(static mutex), preconditions effects logically inconsistent. Mutexes
propagated level forward using two simple propagation rules. Two propositions
level k marked mutex actions level k support one proposition mutex actions support second proposition. Two actions level 2 mutex statically
interfering precondition first action mutually exclusive precondition second . (We term latter dynamic mutex, since constraint may relax higher planning graph
level).1 propositions also either static mutex (one negates other) dynamic mutex (all actions supporting one proposition mutex actions supporting other).
reduce Figure 2 clutter mutex arcs propositions negations omitted.
search phase k-level planning graph involves checking see sub-graph
planning graph corresponds valid solution problem. Figure 3 depicts Graphplan search
manner similar CSP variable-value assignment process. Beginning propositions
corresponding goals level k, incrementally select set actions level k action
list support goals, two actions selected supporting two different goals
mutually exclusive (if are, backtrack try change selection actions). essentially CSP problem goal propositions level variables, actions establish
proposition values, mutex conditions constitute constraints. search proceeds
depth-first fashion: goals level supported, recursively call search process k-1 level planning graph, preconditions actions selected level k
goals k-1 level search. search succeeds reach level 0 (the initial state)
solution extracted unwinding recursive goal assignment calls. process viewed
system solving Dynamic CSPs (DCSP) (Mittal & Falkenhainer, 1990; Kambhampati 2000),
wherein standard CSP formalism augmented concept variables appear
(a.k.a. get activated) variables assigned.
interleaved planning graph extension search phases, graph may extended
stasis condition, changes occur actions, propositions, mutex conditions.
sufficient condition defining level-off level new actions introduced
existing mutex conditions propositions go away. refer planning graph levels
level-off static levels. Note although graph becomes static point, finding
solution may require many episodes composed adding identical static levels conducting
regression search problem goals.
Like many fielded CSP solvers, Graphplan's search process benefits simple form nogood learning. set (sub)goals level k determined unsolvable, memoized level hash table. Subsequently, backward search process later enters level k
set subgoals first checked hash table, match found search
1

static mutex condition also called eternal mutex dynamic mutex termed conditional mutex (Smith
& Weld, 1998).

538

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

J nop



a3



a5

W

J



a5



a5

J

a3

YJ






a3

W

Z nop



J


J

a4

nop

nop

H nop


a1
a5

J


a3

nop



J

nop

a2

X

a3

nop



Level 1

a1

X




YH J

a5



W

a4

Ha1 nop

a5



nop

nop

a1



nop


J



a1

nop







a3

Z nop

WXYZ



H nop


a1

nop J
a5

nop

nop

WYHIJ

Initial State

a5


Goal State

nop

a1

a2

a3


a3

nop

H
a1

a5

Level 2

Level 3

YHI

Icon explanation:
Action a5 assigned give
Assigned action
Assigned action
J

goal J
static mutex
dynamic mutex
a5
previous assigned action
previous assigned action
a1
Goal already satisfied
Set regressed subgoals

previously assigned action a1
satisfied next lower level

Figure 3: CSP-style trace Graphplans regression search Figure 2 planning graph
process backtracks. constitutes one three conditions backtracking: two others arise
attempts assign static mutex actions dynamic mutex actions (See Figure 3 legend).
next discuss Graphplans search higher-level view abstracts away CSP nature.
2.2 Graphplan State Space Search
abstract perspective, Graphplan viewed conducting regression state space
search problem goals initial state. view, states generated expanded subgoals result CSP process given set subgoals finds consistent
set actions satisfying subgoals planning graph level (c.f. Kambhampati & Sanchez,
2000). view state-generator function effectively Graphplans CSP-style goal assignment routine seeks non-mutex set actions given set subgoals within given planning
graph level. view depicted Figure 4, top graph casts CSP-style search trace
539

fiZIMMERMAN & KAMBHAMPATI

1

Init
State

W


2

Proposition Levels

6

2 valid sets action
assignments satisfy
goals WXYZ level 7

J a1

1

2

Init
State

W

7

W
a2 a4
H
Goal

W
X

Z

H a1, a2

4

6

Proposition Levels

S11

S6

S5

S9

S8

7

S4
W

H


S7

S10
S18

S12

8

S14

S13

S15


J

Goal

W
X

Z


S19

S17

S16


H


S20

S21
1

2

3
S23
S24

S22

W

8

S8

S27

S25

S7

W

H


S14

S13

S26

S15


J


H


S29
S19

S17

9

S5

S11

S12

7

S4

S10
S18

S30

S6
S9

S28

Init
State



Proposition Levels

S16
S21

Goal
W
X

Z

S20

Figure 4: Graphplans regression search space: Three consecutive search episodes
Figure 3 high-level state-space search trace. terms box depict set (positive)
subgoals result action assignment process goals higher-level state
box linked.2
Recognizing state-space aspect Graphplans search helps understanding connection
IDA* search. First noted briefly discussed (Bonet & Geffner, 1999), highlight expand
upon relationship here. three correspondences algorithms:
1. Graphplans episodic search process nodes generated previous episode regenerated new episode (and possibly new nodes), corresponds IDA*s iterative
search. Graphplan nodes states (sets subgoals) result regres2

Figure 4 facilitates discussion search trace next section, conjuring hypothetical problem
first search episode begins level 7 planning graph instead level 3, Figure 3.

540

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

sion search given plan graph level succeeds. perspective node-generator
function effectively Graphplans CSP-style goal assignment routine seeks non-mutex
set actions given set propositions within given planning graph level.
2. state space view Graphplans search (ala Figure 4), within given search episode/iteration algorithm conducts search depth-first fashion IDA*. ensures
space requirements linear depth solution node.
3. upper bound iteratively deepened ala IDA* node-state heuristic f-value;
f = g + h. context h distance terms associated planning graph levels
state generated Graphplans regression search initial state3 g cost reaching
state goal state terms number CSP epochs (i.e. numerical difference highest graph level states level).
purposes, perhaps important observation implicit f-value bound
given iteration length planning graph associated iteration. is,
node-state, associated planning graph level determines distance initial state (h)
cost reach goal state (g), total must always equal length plan graph.
heuristic clearly admissible; shorter distance goal Graphplan
exhaustively searches shorter length planning graphs (any) previous iterations. heuristic
implicit Graphplan algorithm guarantees step-optimal solution returned. Note
perspective nodes visited given Graphplan search iteration implicitly
f-value: g + h = length planning graph. consider implications property
address informed traversal Graphplans search space Section 5.
primary shortcoming standard IDA* approach search regenerates many
nodes iterations. long recognized IDA*s difficulties
problem spaces traced using little memory (Russell, 1992; Sen & Bagchi, 1989).
information carried one iteration next upper bound f-value. Graphplan partially addresses shortcoming memo caches store no-goods -states found
inconsistent successive episodes. However, IDA* nature search make inefficient planner problems goal propositions appear non-mutex planning graph
many levels valid plan actually extracted.
second shortcoming IDA* nature Graphplans search node-states generated
given Graphplan episode f-value (i.e. length graph). such, within
iteration (search episode) discernible preference visiting one state another.
next discuss use available memory target shortcomings Graphplans search.
3. Efficient Use Search Trace Guide Planning Graph Search
search space Graphplan explores given search episode defined constrained three
factors: problem goals, plan graph associated episode, cache memoized nogood states created previous search episodes. Typical IDA* search considerable
3

Bonet & Geffner define hG (the Graphplan h-value) somewhat differently first level goals
state appear non-mutex memoized. definition (which necessarily first level
Sm goals appear non-mutex) produces informed admissible estimate cases.
guarantees states generated Graphplan f-value equal planning graph length,
property primary interest us.

541

fiZIMMERMAN & KAMBHAMPATI

similarity (i.e. redundancy) search space successive episodes plan graph extended.
fact, discussed below, backward search conducted level k+1 graph essentially
replay search conducted previous level k certain well-defined extensions.
specifically, essentially every set subgoals generated backward search episode n, starting
level k, regenerated Graphplan episode n+1 starting level k+1 (unless solution
found first).4
returning Figure 4 entirety, note depicts state space tree structure corresponding Graphplans search three consecutive iterations. top graph, discussed above, represents subgoal states generated course Graphplans first attempt satisfy WXYZ
goal problem resembling running example. (It implied W,X,Y,Z propositions
present planning graph level 7 first level pair
mutex.) second search episode (the middle Figure 4 graph), states generated again,
one level higher. addition, states expanded generate number children,
shown darker shade. (Since Figure 4 hypothetical variation Alpha domain problem
detailed Figures 2 3, states created beyond first episode labeled state numbers representing order generated.) Finally, third episode, Graphplan regenerates states previous two episodes attempting satisfy WXYZ level 9, ultimately finds solution (the assigned actions associated figures double outlined subgoal
sets) generating states shown darkest shading bottom graph Figure 4.
Noting extent consecutive iterations Graphplans search overlap, investigate
application additional memory store trace explored search tree. first implemented
approach, EGBG (which summarized following subsection), seeks leverage appropriately designed search trace avoid much inter-episode redundant search effort possible
(Zimmerman & Kambhampati, 1999).
3.1 Aggressive Use Memory Tracing Search: EGBG Planner
Like types CSP-based algorithms, Graphplan consumes computational effort
given problem checking constraints. instrumented version planner reveals typically,
60 - 90% cpu run-time spent creating checking action proposition mutexes -both
planning graph construction search process. (Mutex relations incorporated planning graph primary constraints CSP view Graphplan, Kambhampati, 2000)
such, obvious starting point seeking efficiency improvements planner
primary tactic adopted EGBG. provide overview approach, referring
interested reader Appendix details.
EGBG exploits four features planning graph Graphplans search process:

4

set actions establish given proposition level k+1 always superset
establishing proposition level k.

Strictly speaking, always case due impact Graphplans memoizing process. problems
particular branch search tree generated search episode n rooted planning graph level k may revisited
episode n+1 level k+1 due no-good proposition set memoized level k+1. However, memo merely acts
avoid redundant search neglecting relatively rare cases serves simplify visualization symmetry
across Graphplans search episodes. .

542

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH



constraints (mutexes) active level k monotonically decrease increasing
planning graph levels. is, mutex active level k may may continue
active level k+1 becomes inactive never gets re-activated future levels.



Two actions level statically mutex (i.e. effects preconditions conflict
other) mutex succeeding levels.



problem goal set satisfied level k set searched
level k+1 planning graph extended. is, subgoal set present level k
two propositions mutex, remain future levels.

Given appropriate trace search conducted episode n (which failed find solution)
would like ignore aspects search provably unchanged episode n+1, focus effort features may evolved. previous search failed extract solution
k-length planning graph, search k+1 length graph succeed one
following conditions holds:
1. dynamic mutex condition pair actions whose concurrent assignment
attempted episode n longer holds episode n+1.
2. subgoal generated regression search episode n planning graph level
k, action establishes episode n+1 first appears level k+1.
3. episode n regression state (subgoal set) level k matched cached memo
level memo-match generated level k+1 episode n+1.
(The discussion Appendix formalizes conditions.) instance one
conditions hold, complete policy must resume backward search search parameters associated instance previous episode, n. resumed partial search episodes
either find solution generate additional trace subgoal sets augment parent trace. specialized search trace used direct future backward search episodes problem,
viewed explanation failure search process episode. hereafter use
terms pilot explanation (PE) search trace interchangeably. following definitions useful describing search process:
Search segment: essentially state, specifically set planning graph level-specific subgoals
generated regression search goal state (which first search segment).
EGBG search segment Sn , generated planning graph level k contains:
subgoal set propositions satisfied
pointer parent search segment (Sp ), (the state level k+1 gave rise Sn)
list actions assigned Sp resulted subgoals Sn
pointer PE level (as defined below) associated Sn
sequential list results action consistency-checking process attempt satisfy Sns subgoals. possible trace results given consistency check are: static mutex,
dynamic mutex, action consistent prior assigned actions. Trace results
stored list bit vectors efficiency.
search segment therefore represents state plus path information, often use search
segment state interchangeably. such, boxes Figure 4 (whether state goals
explicitly shown not) viewed search segments.

543

fiZIMMERMAN & KAMBHAMPATI

Pilot explanation (PE): search trace. consists entire linked set search segments
representing search space visited Graphplan backward search episode. convenient
visualize Figure 4: tiered structure separate caches segments associated search
planning graph level. adopt convention numbering PE levels reverse
order plan graph: top PE level 0 (it contains single search segment whose goals
problem goals) level number incremented move towards initial state.
solution found, PE necessarily extend highest plan graph level initial state,
shown third graph Figure 4.
PE transposition: state first generated search episode n associated specific
planning graph level, say k. premise using search trace guide search episode n+1
based idea re-associating PE search segment (state) generated (or updated) episode n
next higher planning graph level. is, define transposing PE as: search
segment PE associated planning graph level k search episode n, associate
level k+1 episode n+1.
Given definitions, note states PE search episode n plan graph
level k, loosely constitute minimal set 5 states visited backward search conducted episode n+1 level k+1. (This bound visualized sliding fixed tree search
segments first graph Figure 4 one level.)
3.2 Conducting Search EGBG Search Trace
EGBG builds initial pilot explanation first regression search episode tracing
search process augmented version Graphplans assign-goals routine. solution
possible k-length planning graph, PE transposed one level, key features previous search replayed significant new search effort occurs points one
three conditions described holds. new search process PE augmented according search space visited.
EGBG search algorithm exploits search trace essentially bi-modal fashion: alternates
informed selection state search trace previous experience focused CSP-type
search states subgoals. discussion EGBGs bi-modal algorithm revolves around
second mode; minimizing redundant search effort state chosen visitation.
describe PEGGs use search trace Section 5 see greater potential
dramatic efficiency increases lies first mode; selection promising state
search trace.
choosing state visit, EGBG uses trace previous episode focus
aspects entailed search could possibly changed. search segment Si
planning graph level k+1, visitation 4step process:
1. Perform memo check ensure subgoals Si valid level k+1
2. Replay previous episodes action assignment sequence subgoals Si, using
segments ordered trace vectors. Mutex checking conducted pairs actions
dynamic mutex level k. actions longer dynamic mutex, add can5

possible Graphplans memoizing process preclude states regenerated subsequent episode.
See footnote 2 brief explanation conditions may occur.

544

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

didate action Sis list consistent assignments resume Graphplan-style search remaining goals. Si ,is augmented PE extended process. Whenever Sis goals
successfully assigned, entailing new set subgoals satisfied lower level k, child
search segment created, linked Si , added PE.
3. Si subgoal replay sequence, check also new actions appearing level k+1
establish subgoal. New actions inconsistent previously assigned action
logged Sis assignments. new actions conflict previously assigned, assign resume Graphplan-style search point step 2.
4. Memoize Sis goals level k+1 solution found via search process steps 2 3.
long segments PE visited manner, planner guaranteed find
optimal plan search episode Graphplan. Hereafter refer PE search segment
visited extended via backward search find valid plan, seed segment. addition,
segments part plan extracted PE call plan segments. Thus, third
graph Figure 4, S18 apparent seed segment plan segments (in bottom order) are;
S30, S29, S18, S17, S16, S15, labeled segments YH, YHI, goal state WXYZ.
principle freedom traverse search states encapsulated PE order
longer restricted (non-informed) depth-first nature Graphplans search process.
Unfortunately, EGBG incurs high overhead associated visiting search segments order bottom (in terms PE levels). ancestor state represented PE
visited state itself, EGBGs search process would regenerate state
descendents (unless first finds solution). non-trivial cost associated generating
assignment trace information EGBGs search segments; search advantage lies reusing trace data without regenerate it.
hand, top-down visitation segments PE levels degenerate mode.
search process essentially mimics Graphplans, since episode begins search
problem goal set, (with exception replay top-level search segments assignments)
regenerates states generated previous episode -plus possibly new states-
regression search. search trace provides significant advantage top-down visitation
policy.
bottom-up policy, hand, intuitive appeal since lowest levels PE correspond portions search space lie closest initial state (in terms plan steps).
state one lower levels fact extended solution, planner avoids search
effort Graphplan would expend reaching state top-level problem goals.
Adopting bottom-up visitation policy amounts layering secondary heuristic primary
IDA* heuristic, planning graph length iteratively deepened. Recalling Section 2.2 states PE f-value terms primary heuristic, essentially biasing favor states low h-values. Support policy comes work
heuristic guided state-space planning (Bonet & Geffner, 1999; Nguyen & Kambhampati, 2000)
weighting h factor 5 relative g component heuristic f-value generally improved performance. However, unlike state-space planning systems, primary heuristic, EGBG employs secondary heuristic guarantee step optimality

545

fiZIMMERMAN & KAMBHAMPATI

Standard Graphplan

Speedup Ratios

EGBG

Problem
Time

Bktrks

Mutex
Chks

7919

2.7x

3.2x

5.5x

3,400 K

1020

10.0x

11.4x

22x

240 K

548 K

2722

24.5x

33x

42x

977 K

8901 K

6611

5.1x

6.0x

9.1x

Total
Time

Backtracks

Mutex
Checks

Total
Time

Backtracks

Mutex
Checks

Size
PE

BW-Large-B
(18/18)

213

2823 K

121,400 K

79

880 K

21,900 K

Rocket-ext-a
(7/36)

402

8128 K

74,900 K

40

712 K

Tower-5
(31/31)

811

7907 K

23040 K

33

Ferry-6
(39/39)

319

5909 K

81000 K

62

Table 1: Comparison EGBG standard Graphplan.
Numbers parentheses give number time steps / number actions respectively. Search backtracks
mutex checks performed search shown. "Size PE" pilot explanation size terms
final number search segments. Standard Graphplan Lisp version Smith Peot.

depend admissibility. found bottom-up visitation efficient mode
EGBG default order EGBG results reported study.
3.3 EGBG Experimental Results
Table 1 shows performance results reported first version EGBG (Zimmerman &
Kambhampati, 1999). Amongst search trace designs tried, version memory
intensive records greatest extent search experience. Runtime, number search
backtracks, number search mutex checks performed compared Lisp implementation original Graphplan algorithm. EGBG exhibits clear advantage Graphplan
small set problems;




total problem runtime: 2.7 - 24.5x improvement
Number backtracks search: 3.2 - 33x improvement
Number mutex checking operations search: 5.5 - 42x improvement

Since total time is, course, highly dependent machine well coding language 6
(EGBG performance particularly sensitive available memory), backtrack mutex checking
metrics provide better comparative measure search efficiency. Graphplan, mutex checking
far biggest consumer computation time and, such, latter metric perhaps
complete indicator search process improvements. problem-to-problem variation
EGBGs effectiveness attributed static/dynamic mutex ratio characterizing Graphplans
action assignment routine. action assignments rejected due pair-wise statically mutex
actions, greater advantage enjoyed system doesnt need retest them. Tower-ofHanoi problems fall classification.
noted original study (Zimmerman & Kambhampati, 1999) range problems
6

planners developed report coded Allegro Lisp run Pentium 900 mhz, 384 RAM.
Runtimes include plangraph construction time exclude garbage collection time. Values Table 1 differ
published 1999 problems re-run platform. also reflect changes tracking statistics.

546

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

handled implementation significantly restricted amount memory available
program runtime. example, PE consisting almost 8,000 search segments,
modest sized BW-Large-B problem challenges available memory limit test machine.
consider next approach (me-EGBG Figure 1) occupies middle ground terms memory
demands amongst search trace approaches investigated.
4. Engineering Reduce EGBG Memory Requirements: me-EGBG Planner
memory demands associated Graphplans search process major concern, since
conducts depth-first search search space requirements linear depth solution node.
Since seek avoid redundancy inherent IDA* episodes Graphplans search using
search trace, must deal much different memory-demand profile. search trace design
employed EGBG memory requirements exponential depth solution. However, search trace grows direct proportion search space actually visited, techniques
prune search also act greatly reduce memory demands.
examined variety methods respect issue, eventually implemented suite
seven together proven instrumental helping EGBG (and later, PEGG) overcome memorybound limitations. Six known techniques planning CSP fields: variable ordering, value ordering, explanation based learning (EBL), dependency directed backtracking (DDB),
domain preprocessing invariant analysis, transitioning bi-partite planning graph. Four
six effective methods CSP speedup techniques, however interest lies primarily
impact search trace memory demands. challenging aspects adapting
methods planning graph search trace context, focus paper. Thus details
motivation implementation methods relegated Appendix B.
seventh method, novel variant variable ordering call EBL-based reordering, exploits
fact using EBL search trace available. Although method readily implemented PEGG, strict ordering trace vectors required EGBG search trace make
costly implement planner. such, memory-efficient EGBG (me-EGBG) use
EBL-based reordering defer discussion PEGG introduced Section 5.
4.1 Impact Enhancements EGBG Memory Demands
two major modes first six techniques impact memory demand me-EGBG:
1) Reduction size pilot explanation (search trace), either number search segments
(states), average trace content within segments, 2) Reduction requirements
structures compete pilot explanation available memory (i.e. planning graph
memo caches). Admittedly, two dimensions independent, since number
memos (though size) linear number search segments. nonetheless consider
partition discussion facilitate comparison methods impact search
trace.
general, impact enhancements search process depends significantly,
particular problem, also presence (or absence) methods.
single configuration techniques proves optimal across wide range problems. Indeed, due
computational overhead associated methods, generally possible find class
problems planner performance degrades due presence method. chose
547

fi90
DDB

40

50

60

70

80

six combination
( me-EGBG )

30

EBL

20

Domain preprocess /
Invariant Analysis
Value Ordering

10

% reduction PE (search trace) memory requirement

100

ZIMMERMAN & KAMBHAMPATI

Variable Ordering

10

20

30

Bi-partite graph

40

50

60

70

80

90

100

% reduction planning graph, memo cache memory requirements

Figure 5: Memory demand impact along two dimensions six memory reduction/speedup
techniques. Plots applied independently suite (within EGBG).
set techniques then, based joint average impact me-EGBG / PEGG memory footprint
extensive variety problems.
Figure 5 illustrates method impact memory reduction relative two dimensions above, method operates isolation others. plot reflects results based
twelve problems three domains (logistics, blocksworld, tower-of-hanoi), chosen include
mix problems entailing large planning graphs, problems requiring extensive search, problems
requiring both. horizontal axis plots percent reduction end-of-run memory footprint
combined memo caches planning graph. ratios along ordinate assessed based
runs Graphplan (no search trace employed) memo cache planning graph
globally defined structures significant size remain Lisp interpreted environment
run completion.7 Similarly, vertical axis plots percent reduction space required PE
end EGBG runs without method activated, planning graph
memo cache structures purged working memory.
plot crossbars method depict spread reduction values seen across twelve
problems along dimensions, intersection average. bi-partite planning
graph, surprisingly, impacts graph aspect, five six methods seen
impact search trace size graph/memo cache size. these, DDB greatest influence PE size little impact graph memo cache size, EBL modest influence former larger impact latter (due smaller memos creates
7

Allegro Common Lisp global scavenging function used purge target global data structures
workspace.

548

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

production general memos, engender backtracks). Domain preprocessing/ invariant analysis major impact graph size PE size due processes
extraction invariants operator preconditions. highly domain dependent, little effect case blocksworld problems, great consequence tower-ofHanoi logistics problems.
six methods combined complement evidenced crossbars plotting space reduction six employed once. twelve problems average reduction
PE size approaches 90% average reduction planning graph/memo cache aspect exceeds
80%. single method isolation averages 55% reduction along dimensions.
runtime reduction associated methods isolation also highly dependent
problem methods active. general, relative time reduction
two methods correlate closely relative memory reduction. However, found
similarly, techniques broadly complement net speedup accrues.
techniques listed (and been) used improve Graphplans performance also, terms speed. order focus impact planning search trace, use
version Graphplan enhanced six methods comparisons me-EGBG
PEGG study (We hereafter refer enhanced version Graphplan GP-e).
4.2 Experimental Results me-EGBG
Table 2 illustrates impact six augmentations discussed previous section EGBGs
(and Graphplans) performance, terms space runtime. Standard Graphplan, GP-e,
EGBG, me-EGBG compared across 37 benchmark problems wide range domains, including problems first three AIPS planning competitions held date. problems
selected satisfy three objectives: subset standard Graphplan EGBG could solve
comparison me-EGBG, different subsets exceed memory limitations three
planners terms either planning graph PE size, subset gives rough impression
search time limitations.
surprisingly, memory efficient EGBG clearly outperforms early version problems attempted. importantly, me-EGBG able solve variety problems beyond reach
standard Graphplan EGBG. 37 problems, standard Graphplan solves 12, original EGBG solves 14, GP-e solves 32, me-EGBG solves 32. Wherever me-EGBG GP-e solve
problem, me-EGBG faster factor 62x, averages ~4x speedup. Standard
Graphplan (on twelve problems solve), bested me-EGBG factors ranging 3x
1000x.
striking improvement memory efficient version EGBG first version
simply due speedup associated five techniques discussed previous section,
directly tied impact search trace memory requirements. Table 2 indicates one three reasons instance problem solved planner: 1) s: planner still search
30 cpu minutes, 2) pg: memory exhausted exceeded 30 minutes planning graph building phase, 3) pe: memory exhausted search due pilot explanation extension. third reason clearly favors me-EGBG size PE (reported terms search segments time
problem solved) indicates generates retains trace 100x fewer states
EGBG. translates much broader reach me-EGBG; exhausts memory 14%
549

fiZIMMERMAN & KAMBHAMPATI

Table 2 problems compared 49% first version EGBG. Regardless, GP-e solves three
problems me-EGBG fails 30 minutes due search trace memory demands
table also illustrates dramatic impact speedup techniques Graphplan itself.
enhanced version, GP-e, well 10x faster original version problems
solve 30 minutes, solve many problems entirely beyond standard Graphplans reach.
Nonetheless, me-EGBG modestly outperforms GP-e majority problems
solve. Since EGBG (and PEGG) planners derive strength using PE shortcut
Graphplans episodic search process, advantage realized problems multiple search
episodes high fraction runtime devoted search. Thus, speedup seen grid-y-1
problems mystery, movie, mprime domains solution extracted
soon planning graph reaches level containing problem goals non-mutex state.
bottom-up order EGBG visits PE search segments turns surprisingly effective many problems. Table 2 problems found great majority PE final
episode contains seed segment (a state search reach initial state) within
deepest two three PE levels. supports intuition discussed Section 3.2 suggests
advantage low h-value bias observed heuristic state-space planners (Bonet & Geffner,
1999; Nguyen & Kambhampati, 2000) trans-lates search planning graph.
Results even memory efficient version EGBG reveal two primary weaknesses:
1. action assignment trace vectors allow EGBG avoid redundant search somewhat
costly generate, make significant demands available memory problems elicit large
search (e.g. Table 2 problems: log-y-4, 8puzzle-1, freecell-2-1), difficult revise
search experience alters drastically subsequent visits.
2. Despite surprising effectiveness many problems, bottom visitation PE search
segments inefficient others. Table 2 problems freecell-2-1 essentially
schedule domain problems, planning graph gets extended level
solution extracted, solution arises via new search branch generated root
search segment (i.e. problem goal state). Thus, seed segment PE topmost search segment, bottom-up visitation PE states costly Graphplans
top-down approach.
first shortcoming particularly manifest problems allow EGBG exploit
PE (e.g. problems solution extracted first search episode). hit EGBG
takes problems relative Graphplan closely tied overhead associated building
search trace. compelling tactic address second shortcoming traverse search space
implicit PE according state space heuristics. might wish, example, exploit
variety state-space heuristics revolutionized state space planners recent years
(Bonet & Geffner, 1999; Nguyen & Kambhampati, 2000; Gerevini & Serina, 2002). However,
noted Section 3.2, depart policy visiting EGBG search segments level-bylevel, bottom-up order, face costly bookkeeping high memory management overhead.
informed traversal state-space view Graphplans search space taken next,
argue perhaps key benefit afforded trace search planning graph.

550

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

Problem

(steps/actions)

bw-large-B (18/18)
huge-fct
(18/18)
rocket-ext-a (7/34)
att-log-a
(11/79)
gripper-8 (15/23)
Tower-6
(63/63)
Tower-7 (127/127)
8puzzle-1 (31/31)
8puzzle-2 (30/30)
TSP-12
(12/12)
AIPS 1998
grid-y-1 (14/14)
grid-y-2 (??/??)
gripper-x-3 (15/23)
gripper-x-4 (19/29)
gripper-x-5 (23/35)
log-y-4
(11/56)
mprime-x-29 (4/6)
movie-x-30 (2/7)
mysty-x-30 (6/14)
AIPS 2000
blocks-10-1 (32/32)
blocks-12-0 (34/34)
logistics-10-0 (15/56)
logistics-11-0 (13/56)
logistics-12-1 (15/77)
freecell-2-1 (6/10)
schedule-8-5 (4/14)
schedule-9-2 (5/13)
AIPS 2002
depot-6512 (10/26)
depot-7654a (10/28)
driverlog-2-3-6a (10/24)
driverlog-2-3-6e (12/28)
roverprob1425 (10/32)
roverprob1423 (9/30)
strips-sat-x-5 (7/22)
strips-sat-x-9 (6/35)
ztravel-3-8a (7/25)
ztravel-3-7a (10/21)

EGBG

Graphplan
cpu sec

Stnd.
126
165

GP-e

cpu sec

(enhanced)

11.4
13.0

3.5

12.2
125
14.2

43.1

158
667
57.1
304
48.3

454
Graphplan
GP-e
388
16.7
pg
pg
291
16.1

190


pg
470
15.7
5.5
.1
.05
83
13.5
Graphplan
GP-e

101

24.2

30.0

78.6



98.0
pg
63.5
pg
58.1
Graphplan
GP-e
239
5.1

32.5
1280
2.8

169

18.9

170
313
47.0



972



79
98
40.3
pe
88
39.1

pe
pe
pe
393
pg
200
pe
pe
pg
6.6
.06
85

size
PE

7919
8410
1020
9790
3303

EGBG
19
9888

4
2
32
EGBG

pe
pe

pe
pe
pe
pg
pg
219

807

979
pe
272

pe
pe

EGBG
4272
1569
10028
4111

me-EGBG

SPEEDUP
(me-EGBG
vs. GP-e)

2090
2964
174
1115
2313
80
166
pe
>16000
26.9
10392
97.0
7155
me-EGBG
16.9
15
pg
8.4
2299
65.7
6351
433
13572
pe
>25000
5.5
4
.05
2
13.5
19
me-EGBG
16.1
6788
14.5
3220
16.3
1259
10.0
1117
1205
7101
pe
>12000
42.9
6
46.8
6
me-EGBG
4.1
456
14.8
1199
1.0
230
83.3
7691
10.3
1522
94.7
10217
23.0
2717
84.4
306
15.6
1353
>20000
pe

1.2x
1.4x
1.9x
1.7x
1.8x
5.7x
7.9x
(pe)
1.8x
4.7x
Speedup
1x
~
1.9x
2.9x
> 5x
(pe)
1x
1x
1x
Speedup
6.3x
1.7x
1.8x
7.9x
> 2x
(pe)
1.5x
1.2x
Speedup
1.25x
2.2x
2.8x
2x
1.8x
1.8x
2.0x
>21x
62x
~

(memory efficient EGBG)
cpu sec
size
PE

9.2
9.1
1.8
7.2
7.9
7.6
20.0

Table 2: Search step-optimal plans: EGBG, me-EGBG, standard & enhanced Graphplan
Standard Graphplan: Lisp version Smith Peot
GP-e: Graphplan enhanced per Section 4.1 me-EGBG: memory efficient EGBG
Size PE final search trace size terms number "search segments"
Search failure modes: pg Exceeded 30 mins. memory constraints graph building
pe Exceeded memory limit search due size PE
Exceeded 30 mins. search
Parentheses adjacent cpu time give (# steps / # actions) solution.
551

fiZIMMERMAN & KAMBHAMPATI

5. Focusing State Space View: so-PEGG PEGG Planners
costs associated EGBGs generation use search trace directly attributable
storage, updating, replay CSP value assignments search segments subgoals.
therefore investigated stripped version search trace abandons tactic focuses
instead embodied state space information. show PEGG planners employing
search trace (both so-PEGG, step-optimal version PEGG, version using beam search),
outperform EGBG planners larger problems. key difference EGBGs pilot explanation pared down, skeletal PE used PEGG planners, elimination detailed
mutex-checking information contained bit vectors former (i.e. last item bullet
list EGBG search segment contents Section 3.1). PEGG planners apply state-space
heuristics rank PE search segments based associated subgoal sets (states) free
visit state space informed manner. tradeoff PE state visited
planner must regenerate CSP effort finding consistent action assignments subgoals.
Figure 6 illustrates PEGG advantage small hypothetical search trace final search episode. search segments PE onset episode appear solid lines plan
segments (states extendable valid plan) shown double-lined boxes. figure reflects
fact typically may many latent plan segments diverse branches search trace
solution-bearing episode. Clearly planner discriminate plan segment states
states PE could solve problem quickly planner restricted bottom-up traversal (deepest PE level first). State space heuristics endow PEGG planners capability.
so-PEGG planner visits every search segment PE search episode (comparable Graphplans exhaustive search given length graph) thereby guaranteeing returned
plans step-optimal. such, advantage heuristic-guided traversal realized
final episode. many problems, computational effort expended Graphplan last search
episode greatly exceeds previous episodes combined, still powerful advantage. However, scale problems larger terms number size search
episodes, cost exhaustive search even intermediate episodes becomes prohibitive.

3
Init
State

Goal

1

W

W
X

Z

2



4
.
.

.
.

.
.

1

2

3

.
.

.
.
.

Proposition Levels

7

8

Figure 6: PE final search episode hypothetical problem. Search segments PE
onset search appear solid lines, double-lined boxes represent plan segments,
dashed lined boxes states newly generated regression search episode.
Visitation order dictated secondary heuristic shown via numbering.
552

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

planner refer simply PEGG employs beam search, applying search trace heuristics
intermediate search episodes visit select subset PE segments. PEGG
trades step-optimality guarantee often greatly reduced solution times.
several challenges must dealt effectively use pared search trace
employed so-PEGG PEGG, including adaptation augmentation distance-based heuristics guide search trace traversal dealing memory management problems induced
tactic skipping search space. describe addressed issues
give complete description algorithm, first present results provide perspective effectiveness planners.
5.1 Experimental Results so-PEGG PEGG
Table 3 compares Graphplan (standard GP-e), me-EGBG, so-PEGG, PEGG
problems Table 2, adds variety larger problems latter two systems
handle. Table 2 problems easily solved GP-e me-EGBG (e.g. AIPS-98
movie mystery domains) omitted Table 3. Here, planners employ variable
value ordering (i.e. except standard Graphplan), configured use value ordering based
planning graph level action first appears goal ordering based proposition distance
determined adjusted-sum heuristic (which defined below). variety
parameters so-PEGG PEGG planners optimal configurations tend
problem-dependent. defer discussion Sections 5.3, 5.4, 5.6 note
Table 3 results following parameter settings used based good performance average
across variety domains problems:
Secondary heuristic visiting states: adjusted-sum w0=1 (eqn 5-1)
Beam search: visit best 20% (lowest f-value) search segments per search episode,
minimum 25 maximum 50. Search segments flux lower 10% average
visited regardless heuristic rank. (wcf = .01, see section 5.6.1)
Focusing first GP-e, me-EGBG, so-PEGG columns, clearly see impact
tradeoff storing exploiting intra-segment action assignment information PE.
set 37 problems, 16 result me-EGBG exceeding available memory due size
PE one pushes limit so-PEGG. Seven problems cause me-EGBG run
memory actually solved so-PEGG remainder exceed time limit
search. addition, so-PEGG handles five problems table GP-e fails on. problems
typically entail extensive search final episode, PE efficiently shortcuts full-graph
search conducted GP-e. speedup advantage so-PEGG relative GP-e ranges
modest slowdown three problems almost 87x Zeno-Travel problems, average
5x. (Note speedup values reported table so-PEGG.)
Generally, planner using search trace perform GP-e single search episode problems grid-y-1, cost building trace recovered. low overhead associated building so-PEGGs search trace means suffers little relative GP-e case.
problems me-EGBG so-PEGG solve, me-EGBG upper hand due
ability avoid redundant consistency-checking effort. fact me-EGBGs advantage soPEGG greater problems attributable so-PEGGs ability move PE
search space final search episode (versus me-EGBGs bottom-up traversal) lower
553

fiZIMMERMAN & KAMBHAMPATI

Graphplan

Problem

cpu sec (steps/acts)

Stnd.
bw-large-B
bw-large-C
bw-large-D
att-log-a
att-log-b
Gripper-8
Gripper-15
Tower-7
Tower-9
8puzzle-1
8puzzle-2
TSP-12
AIPS 1998
grid-y-1
gripper-x-5
gripper-x-8
log-y-5
AIPS 2000
blocks-10-1
blocks-12-0
blocks-16-2
logistics-10-0
logistics-12-1
logistics-14-0
freecell-2-1
freecell-3-5
schedule-8-9
AIPS 2002
depot-7654a
depot-4321
depot-1212
driverlog-2-3-6e
driverlog-3-3-6b
roverprob1423
roverprob4135
roverprob8271
sat-x-5
sat-x-9
ztravel-3-8a
ztravel-3-7a

194.8








2444
1546

Stnd GP
388


pg
Stnd GP

~

~


pg
pg
pg
Stnd GP








313




GP-e

me-EGBG
cpu sec
(steps/acts)

(enhanced )

11.4 (18/18)
(28/28)
(38/38)
31.8 (11/79)

14.2 (15/23)

158 (127/127)
(511/511)
57.1 (31/31)
48.3 (30/30)
454 (12/12)
GP-e
16.7 (14/14)


470 (16/41)
GP-e
95.4 (32/32)
26.6 (34/34)

30.0 (15/56)


98.0 (6/10)
1885 (7/16)
300 (5/12)
GP-e
32.5 (10/28)


166 (12/28)

170 (9/30)


45 (7/22)

972 (7/25)


9.2

so-PEGG
heur:istic:
adjsum

cpu sec
(steps/acts)

7.0
1104
pe
7.2
2.9 (11/72)
pe

7.9
30.6
pe

20.0
14.3
232
118
pe
31.1
26.9
31.3
97.0
390
me-EGBG
so-PEGG
17.9
16.8
433
512
pe

pe
361
me-EGBG
so-PEGG
16.1
18.7
14.5
23.0
pe

16.6
21
1205 (15/77) 1101 (15/75)
pe

pe
102
pe
511
615
719
me-EGBG
so-PEGG
14.8
12.9




83.3
109
pe
1437 (11/39)
pe
63.4
pe

pe

43.0
27.0
918
9.9
15.6
11.2
pe

pe
pe

PEGG

heur: adjsum-u

cpu sec
(steps/acts)

Speedup
(PEGG
vs. GP-e)

4.1
24.2
388
2.2
21.6
5.5
46.7
6.1
23.6
9.2
7.0
6.9

(18/18)
(28/28)
(38/38)
(11/62)
(13/64)
(15/23)
(31/45)
(127/127)
(511/511)
(31/31)
(32/32)
(12/12)
PEGG
16.8 (14/14)
110 (23/35)
520 (35/53)
30.5 (16/34)
PEGG
6.9 (32/32)
9.4 (34/34)
28.1 (56/56)
7.3 (15/53)
17.4 (15/75)
678 (13/74)
19.5 (6/10)
101 (7/17)
719 (5/12)
PEGG
13.2 (10/26)
42.6 (14/37)
79.1 (22/53)
80.6 (12/26)
169 (14/45)
15.0 (9/26)
379 (12 / 43)
220 (11 / 39)
25.1 (7 / 22)
9.9
(6 / 35)
15.1 (9/26)
101 (10/23)

2.8x
> 74x
> 4.6x
14.5x
> 83x
2.6x
> 38.5x
26x
> 76x
6.2x
6.9x
51x
Speedup
1x
> 16x
> 3.5x
15.4x
Speedup
13.8x
2.8x
> 64 x
4.1x
> 103x
> 2.7x
>92x
18.7x
(.42x)
Speedup
2.7x
>42x
>22.8x
2.1x
> 10.7x
11.3x
> 4.7x
> 8.2x
1.7x
>182x
119x
> 18x

Table 3: so-PEGG PEGG comparison Graphplan, GP-e, me-EGBG
GP-e: Graphplan enhanced per Section 4.1 me-EGBG: memory efficient EGBG
so-PEGG: step-optimal, search via PE, segments ordered adjusted-sum-u heuristic
PEGG: beam search, best 20% segments PE ordered adjusted-sum-u heuristic
Parentheses give (# steps/ # actions) plan. Boldface values exceed known
step-optimal.
See Table 2 definitions s, pg, pe
554

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

overhead due concise search trace. Note obvious reason prefer one state
traversal order non solution-bearing episodes since step-optimal planners visit
states PE search episodes. 8
turning attention PEGG results, apparent beam search greatly extends
size problems handled. PEGG solves ten larger problems Table 3 could
solved either so-PEGG enhanced Graphplan. Speed-wise PEGG handily outperforms
planners every problem except schedule-8-9, GP-e factor 2.3x advantage. indicated tables right-hand column, speedup PEGG GP-e ranges .42x
182x. conservative bound PEGGs maximum advantage relative GP-e since speedup
values seventeen problems GP-e fails solve conservatively assessed time
limit 1800 seconds.
defer analysis results Section 6 order first describe PEGG algorithm
advantages extracts search trace.
5.2 Algorithm PEGG Planners
high-level algorithm so-PEGG PEGG given Figure 7. Graphplan, search begins planning graph extended level problem goals first appear
binary mutex conditions. (The routine, find_1st_level_with_goals virtually
Graphplans defined here). first search episode conducted Graphplan fashion,
except assign_goals assign_next_level_goals routines Figure 8 initialize PE
create search segments hold states generated regression search process. assign_goals pseudo-code outlines process compiling conflict sets (see Appendix B) means
implementing DDB EBL action assignment search. assign_next_level_goals
routine illustrates role top-level conflict set recording minimal no-good search
state completed (EBL) depicts variable ordering need done state
(when search segment created). child segment created linked parent (extending
PE) assign_next_level_goals whenever parent goals successfully assigned. assign_next_level_goals routine determines subgoals child search segment regressing
parents goals actions assigned checks see either initial state
reached remaining goals. so, success signaled returning child search segment used extract ordered actions plan.
Subsequent first episode, PEGG_plan enters outer loop employs PE conduct
successive search episodes. episode, newly generated search segments previous
episode evaluated according state space heuristic, ranked, merged already ordered
PE. inner loop search segment visited turn passing subgoals Graphplanlike assign_goals routine.
exit conditions inner loop primarily differentiate so-PEGG PEGG.
Whereas so-PEGG visit every search segment whose goals found match memo,
PEGG restricts visitation best subset, based user-specified criterion. such, expansion
planning graph deferred segment chosen visitation transposes planning graph level exceeding current graph length. consequence, problems PEGG
8

fact found advantages respect traversal order even intermediate search episodes problems. However, highly problem-dependent, consider study.

555

fiZIMMERMAN & KAMBHAMPATI

PEGG_PLAN (Ops, Init, Goals) /*{ Ops, Init, Goals} constitutes planning problem */
/* build plangraph, PG, level n goals first occur non-mutex state*/

Let PG find_1st_level_with_goals( Ops, Init, Goals )
PG reached level-off goals present non-mutex state Return FAIL
Let n number levels PG
Reorder Goals according variable ordering method
Let SS0 new search segment fields:
goalsGoals, parent root, PE-level 0, parent-actions {}
Let PE pilot explanation structure fields: ranked-segs {SS0}, new-segs {}

/* Conduct Graphplan-style backward search n-length planning graph, storing trace PE..*/

Let search-reslt assign_goals(Goals, {}, n, SS0, PG, PE)
search-reslt search segment /* Success */
Let Plan extract plan actions ancestors linked search-reslt
Return Plan
else /* n-length solution possible ...use PE search longer length solution */
loop forever
n n+1
/* rank newly generated states merge existing ordered PE segments list */
PE<ranked-segs> merge sort(PE<ranked-segs> U heuristic_sort( PE<new-segs> ) )

loop unvisited search segments PE[ranked-segs] (optionally: segments
heuristic threshold)
Let SS highest ranked, unvisited segment PEs ranked-segs
Let k = n (PE-level SS)
/*... planning graph level SS based transposed PE */
k = n PG extend_plangraph(PG) /*.. delays extending graph unavoidable! */
optionally: flux metric SS goals < user-specified threshold continue loop.
SS goals memos level k PG remove SS PEs ranked-segs
else /* visit search segment SS... */
search-reslt assign_goals (SS<goals>, {}, k, SS, PG, PE)
search-reslt search segment /* Success...*/
Let Plan extract plan actions ancestors linked search-reslt
Return Plan
else search-reslt conflict set..
add conflict set level k memos PG /* memoize minimal nogood */
reorder SS goals goals conflict set appear first /* EBL-based reordering */
end
end-loop
end

Figure 7: Top-level algorithm PEGG so-PEGG planners.
planners may able extract step-optimal solution building one less level Graphplan-based planners.9

9

Interestingly, PEGG beam search could conceivably extract optimal solution planning graph arbitrary number levels shorter required Graphplan. Consider case PE, average, extends least
one level deeper episode subset PE search segments visited always resides deepest levels PE.
arbitrary number search episodes might completed without extending planning graph. Based experiments problems date however, advantage seldom saves one planning graph level extension.

556

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

Conduct DDB & EBL-enhanced Graphplan-style search building search trace
arguments> G: goals still assigned, A: action set already assigned, k: PG level,
SS1: search segment, PG: planning graph, PE: pilot explanation (search trace)
ASSIGN_GOALS (G, A, k, SS1, PG, PE)
Let g goal selected G
Let Ag actions PG level k support g, ordered value-ordering heuristic
Let cs {g} /* initialize conflict set DDB */
loop act Ag
Let search-reslt = {}
act mutex action
Let b goal conflicted action assigned support
cs cs U {b} /* augment conflict set continue loop*/
else /* act conflict actions already */
G-{g} empty
/* continue goal assignment level */
search-reslt assign_goals (G-{g}, U{act}, k, SS1, PG, PE)
else /* SS1 goals left satisfy...setup search next lower level */
search-reslt assign_next_level-goals (A U{act}, k, SS1, PG, PE)
search-reslt conflict set, check contains current goal..
g search-reslt
/* absorb returned conflict set & try next action */
cs cs U search-reslt
else Return search-reslt /*just return conflict set */
else search-reslt search segment: /* Success.. */
Return search-reslt
end loop (actions)
Return cs /* soln reached .. compiled conflict set returned*/
end-if
end
Set search graph level k-1 given SS1 goals satisfied actions level k
ASSIGN_NEXT_LEVEL_GOALS (A, k, SS1, PG, PE)
Let nextgoals regress SS1 goals (the actions assigned satisfy goals)
nextgoals empty k = 0 (its initial state)
Return SS1 /* Success */
else memos level k-1 PG nextgoals
Return conflict set /* backtrack due nogood */
else /* initiate search next lower PG level*/
Let SS2 new search segment holding nextgoals, pointer SS1, & actions assigned SS1
Add SS2 PE new-segs list
Let search-reslt assign_goals (nextgoals, {}, k-1, SS2, PG, PE)
search-reslt search segment /* Success.. */
Return search-reslt
else search-reslt conflict set: /*memoize minimal nogood return conflict set */
add conflict set level k-1 memos PG
reorder SS2 goals goals conflict set appear first /* .. EBL-based reordering */
Return search-reslt
end-if
end

Figure 8: PEGG / so-PEGG regression search algorithm Graphplan-style regression search
subgoals concurrently building search trace (PE)
557

fiZIMMERMAN & KAMBHAMPATI

Note PEGGs algorithm combines state-space CSP-based aspects search:
chooses expansion promising state based previous search iteration
state space heuristics. PEGG so-PEGG free traverse states search trace
order.
selected state expanded Graphplans CSP-style, depth-first fashion, making full use
CSP speedup techniques outlined above.
first aspect clearly distinguishes PEGG EGBG: traversal state space PE
longer constrained bottom-up level-by-level. EGBG, management
memory associated search trace challenge PEGG stray bottom-up traversal, less daunting. easier outline address first discuss
development adaptation heuristics search trace traversal.
5.3 Informed Traversal Search Trace Space
HSP HSP-R state space planners (Bonet & Geffner, 1999) introduced idea using
reachability propositions sets propositions (states) assess difficulty degree relaxed version problem. concept underlies powerful distance based heuristics selecting promising state visit. Subsequent work demonstrated planning graph
function rich source heuristics (Nguyen & Kambhampati, 2000). Since planning
graph already available PEGG, adapt extend heuristics latter work serve
secondary heuristic role direct PEGGs traversal search trace states. Again, primary heuristic planning graph length iteratively deepened (Section 2.2), step-optimality
guarantee so-PEGG planner depend admissibility secondary heuristic.
important differences heuristic ranking states generated state space planner ordering search segments (states) PEGGs search trace. example, state space
planner chooses visit given state PEGG planners often must consider whether
revisit state many consecutive search episodes. Ideally, heuristic rank states search
trace reflect level-by-level evolutions planning graph, since transposition process
associates search segment higher level successive episode. higher planning
graph level given state associated with, effective regression search space
changes complex function number new actions appear graph, number
dynamic mutexes relax, no-goods memo caches. Moreover, unlike state space
planners queue previously unvisited states, states search trace include children
state generated last visited. Ideally value visiting state assessed independently value associated children, since assessed turn. Referring back search trace depicted Figure 6, desire heuristic can, example, discriminate #4 ranked search segment ancestor, top goal segment (WXYZ).
would like heuristic assessment segment WXYZ discount value associated children already present trace, ranked based potential generating new local search branches.
next discuss adaptation known planning graph based heuristics effective use
search trace.

558

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

5.3.1 ADOPTION DISTANCE-BASED STATE SPACE HEURISTICS
heuristic value state, S, generated backward search problem goals expressed as:
5-1)

f ( ) = g ( ) + w0 * h( )
where: g(S) distance problem goals (e.g. terms steps)
h(S) distance estimate initial state (e.g. steps)
w0 optional weighting factor

value g state generated search (e.g. states PE) easily assessed
cumulative cost assigned actions point. h values consider taken
distance heuristics adapted exploit planning graph (Nguyen & Kambhampati,
2000). One heuristic readily extractable planning graph based notion
level set propositions:
Set Level heuristic: Given set propositions, denote lev(S) index first level
leveled serial planning graph propositions appear non-mutex one another. (If singleton, lev(S) index first level singleton element
occurs.) level exists, lev(S) = .
admissible heuristic embodies lower bound number actions needed achieve
initial state also captures negative interactions actions (due planning graph binary mutexes). Nguyen & Kambhampati, 2000 study, set level heuristic
found moderately effective backward state space (BSS) planner AltAlt, tended result many states f-value. directing search PEGGs search trace
somewhat effective, still suffers lower level discrimination
heuristics examined -especially problems engender planning graph relatively
levels. Nonetheless, noted Appendix B discussion memory efficiency improvements
use planning graph construction default heuristic value ordering, due low
computational cost synergy building using bi-partite planning graph.
inadmissible heuristics investigated Nguyen & Kambhampati, 2000 work based
computing heuristic cost h(p) single proposition iteratively fixed point follows.
proposition p assigned cost 0 initial state otherwise. action, a, adds
p, h(p) updated as:
5-2) h(p) := min{ h(p), 1+h(Prec(a) }
h(Prec(a)) sum h values preconditions action a.
Given estimate propositions h-value, variety heuristic estimates state
studied, including summing h values subgoal taking maximum subgoal hvalues. study focus heuristic termed adjusted-sum (Nguyen & Kambhampati, 2000), combines set-level heuristic measure sum h-values states
goals. Though powerful heuristic tested them, computationally cheap planning graph based planner found quite effective BSS planners tested.
Adjusted-sum heuristic: Define lev(p) first level p appears plan graph
lev(S) first level plan graph propositions state appear nonmutexed one another. adjusted-sum heuristic may stated as:
559

fiZIMMERMAN & KAMBHAMPATI

5-3)

hadjsum ( ) :=

h( p ) + ( lev(S ) max lev( p ) )


pi

pi



2-part heuristic; summation, estimate cost achieving assumption goals independent, estimate cost incurred negative interactions
amongst actions must assigned achieve goals. latter factor estimated taking
difference planning graph level propositions first become non-mutex
level propositions first appear together graph.
complex heuristics proposed include measure positive interactions subgoals state, is, extent action establishes one relevant subgoal. so-called relaxed plan distance-based heuristics focus positive interactions,
several studies demonstrated power backward forward state-space planners
(Nguyen & Kambhampati, 2000; Hoffman, 2001). However, reported former study, primary advantage adding positive interactions adjusted-sum heuristic produce shorter
make-span plans expense modest increase planning time. Since PEGGs IDA* search
already ensures optimal make-span little incentive incur expense relaxed plan
calculation, restricted work simpler adjusted-sum heuristic eqn 5-3.
adjusted-sum heuristic adapted search planning graph leveraging
information PEGGs search trace. takes form heuristic updating dynamically improve h value estimate states PE. lev(S) term adjusted-sum heuristic represents
first planning graph level subgoals state appear binary non-mutex
other. However, regression search graph level k fails given episode, search
process essentially discovered n-ary mutex condition subset goals
level k (This subset conflict set, C, gets memoized PEGG algorithm Figures 7
8). point lev(S) value updated k+1, indicating k+1 conservative estimate
first level goals appear n-ary non-mutex state. desirable property
ranking search trace states; longer state resides search trace, often h-value gets
increased, less appealing becomes candidate visit again. is, heuristic update
biases states visited failed extend solution. use
augmented adjusted-sum heuristic PEGG runs work refer adjusted-sum-u.
Experimentally, find advantage given heuristic ordering PE states highly
domain dependent (but less sensitive particular domain problem). example, compared
simple bottom-up visitation strategy, adjusted-sum-u heuristic improves so-PEGG runtimes
order magnitude domains (e.g. Freecell and, Satellite) degrading
factor 2x 7x others (e.g. Zenotravel). Figure 9 depicts performance adjusted-sum-u
heuristic relative bottom-up heuristic so-PEGG several sets problems. heuristics compared terms so-PEGGs average computation time percentage GP-es
final search episode -the important measure exhaustive search planning graph.
informed heuristic find seed segment sooner but, event many
(typical logistics domains), find one lies planning graph level closer
initial state. less informed heuristic may cause PEGG end conducting search final
episode GP-e, may many states PE would regenerated Graphplan
final regression search finds solution. direct measure power

560

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

bottom-up

adjusted-sum-u

Depot
Schedule

Problem Set

ZenoTravel
Freecell
Driverlog
Satellite
Logistics
Blocks
TSP
0.00

20.00

40.00

60.00

80.00 100.00 120.00 140.00

% Graphplan's search cost final episode

Figure 9: Heuristic accuracy so-PEGG final search episode --relative GP-e
search segment selection heuristic. Since performance vary considerably specific problem results figure averages three representative examples domain10.
5.4 Memory Management Arbitrary Search Trace Traversal Order

return memory management problems induced strategy skipping
search space. Consider PE time final search episode Figure 6. search segments visited order deepest PE level first, encounter problem regenerating states already contained PE. visitation order depicted numbered segments
figure could result fairly informed heuristic (the 4th segment chooses visit plan
segment), implies many states already resident PE regenerated. includes,
example, yet unvisited descendents third segment visited. Unchecked, process
significantly inflate search trace memory demands well overhead associated regenerating search segments. addition, heuristic information state lost state regenerated instead revisited extant PE search segment. due adjustedsum-u secondary heuristic PEGG learns improved n-ary mutex level search segments goals
updates f-value accordingly search episode.
address issue hashing every search segment generated associated PE state hash
table according canonically ordered goal set. One hash table built PE level.
Prior initiating regression search subgoal set search segment, Sn , PEGG first checks
planning graph memo caches and, relevant memo found, checks PE state hash table
see Sns goals already embodied existing PE search segment relevant PE level.
10

Problem sets used- Blocksworld: bw-large-b, blocks-10-1 12-0 Logistics: att-log-a, logistics-10-0, 12-0, Gripper:
gripper8, gripper-x-3, x-5, Depot: depotprob6512, -5646, 6587, Driverlog: dlog-3-36a, -2-3-6a, 2-3-6e, Zenotravel:
ztravel-3-8a, -3-8b, 3-7b, Freecell: freecell-2-1, -2-2, -3-5, Satellite: strips-sat-x-4, x-5, x-9.

561

fiZIMMERMAN & KAMBHAMPATI

search segment, Se,, returned PE state check, Se made child Sn (if already) establishing link, search proceed Se goals.11
Another search trace memory management issue associated fact PEGG visits
subset PE states -a set call active PE. tempting pursue minimal memory
footprint strategy retaining memory active search segments PE. However unlike
Graphplan, initial state reached PEGG cannot extract solution unwinding complete sequence action assignment calls since may begun regression search arbitrary state branch search trace tree. PEGG depends instead link child
search segment parent extract plan actions solution found. must
retain minimum, active search segments ancestor segments root node.
Beyond requirement retain search segments tied active PE, many strategies
might used managing inactive portion. study attempted reduce
PE memory requirements manner, instead focusing might termed search space
field view. beam search, heuristic effectiveness depends informed is,
search trace states available rank. reduced memory footprint PEGGs skeletal
search trace allows us adopt strategy retaining memory search segments generated.
segments f-values updated ranked, giving beam search wide selection states contending active status given search episode.
5.5 Learning Order States Subgoals

PEGG planners employ EBL search trace, allows overlay yet
sophisticated version variable ordering top distance-based ordering heuristic. guiding
principle variable ordering search fail early, failure inevitable. terms Graphplan-style search regressed state, translates Since state goals must assigned action, best attempt satisfy difficult goals first. adjusted-sum heuristic described
above, applied single goal, provides estimate difficulty based structure
planning graph. However, EBL provides additional information difficulty goal achievement
based directly search experience. wit, conflict set returned PEGGs assign_goals routine search goal set explicitly identifies goals responsible search failure. intuition behind EBL-based reordering technique then,
goals likely difficult assign search segment revisited next
search episode. constitutes dynamic form variable ordering that, unlike distance-based
ordering, search segments goals may reordered successive search episodes based
recent search experience.
Figure 10 compares influence adjusted sum variable ordering EBL-based reordering
methods memory demand, manner similar Figure 5. impact EBL-based reordering EGBGs performance reported PEGG tightly integrates various CSP efficiency methods, independent influence cannot readily assessed.12 isolate impact
EBL-based reordering EBL activating EBL using produced con11
12

interests simplicity, Figure 8 algorithm outline memory management process.
Given success various memory-efficiency methods within EGBG, versions PEGG implement
default. graph analogous Figure 5 PEGG planner would differ terms actual memory reduction values, confident overall benefits methods would persist, would relative benefit relationship
methods.

562

fi30
20

Variable Ordering (adjsum) & EBLReordering
Variable ordering
EBL- Reordering
10

% reduction PE (search trace) memory requirement

USING MEMORY TRANSFORM SEARCH PLANNING GRAPH

10
20
30
% reduction planning graph, memo cache memory requirements

Figure 10: Memory demand impact along two dimensions adjusted-sum
variable ordering EBL-based reordering techniques applied
independently together.
flict sets reordering, memoization. average reduction search trace memory
12-problem sample seen 18% EBL-based reordering alone. compares
favorably 22% average reduction distance-based ordering, especially since, unlike
adjusted sum ordering, EBL-based reordering takes effect 2nd search episode. plot
also reveals two modes ordering quite complimentary.
Across variety problems domains found following approach effective
combining distance-based variable ordering EBL-based reordering: 1) newly created search
segments goals ordered according distance-based heuristic. 2) visit search
segment, subset goals appear conflict set reordered appear first. 3) goals
conflict set ordered distance-based heuristic appended non-conflict goals,
also set distance-based order.
indicated Figure 10, hybrid form variable ordering boosts average memory reduction almost 30%, also significantly reduces wide fluctuation performance
either method isolation. re-emphasize search experience-informed goal ordering
available search algorithm maintains memory states visited. therefore
portable Graphplan-based planner know of.
5.6 Trading Guaranteed Step-Optimality Speed Reach:
PEGG Beam Search

Many difficult benchmark problems Graphplans IDA* style search 20
search episodes reaching episode solution extracted. cumulative search time tied episodes large portion total search time and, indicated
Table 3, so-PEGG exhausts search time limits well reaching episode solution
extracted. strategy exhaustively searching planning graph, episode
solution bearing level, gives step-optimal guarantee Graphplans solutions exact
563

fiZIMMERMAN & KAMBHAMPATI

high cost ensure is, all, one aspect plan quality. explore PEGG, nonexhaustive search version so-PEGG, extent search episodes truncated
producing plans virtually makespan Graphplans solution.
PEGG shortcuts time spent search intermediate episodes using secondary
heuristic direct order PE states visited prune search space visited
episode. beam search seeks visit promising PE states, measured
f-values user-specified limit. addition, beam search important dual benefit
PEGG reduces memory demands search trace and, depending
problem, even planning graph. PEGG algorithm Figure 8, loop statement
point beam search f-value threshold optionally applied PE states candidates visitation. first segment exceeding threshold reached sorted queue
search episode ends.
devise effective threshold test must reconcile competing goals: minimizing search nonsolution bearing episodes maximizing likelihood PE retains visits (preferably
early possible), search segment thats extendable solution graph reaches first
level extant solution. narrower window states visited, difficult
heuristic ranks states ensure includes plan segment, i.e. one part stepoptimal plan. PEGG return step-optimal plan long search strategy leads visit
plan segment (including top, root segment PE) belonging plan latent PE, search first solution-bearing planning graph. heuristics job selecting window
search segments visit made less daunting many problems many step-optimal
plans latent solution-bearing level.
next describe effective planning graph based metric augments state space heuristic
choosing set PE states visit search episode.
5.6.1 MINING PLANNING GRAPH FILTER BEAM
Beyond heuristic updating introduced Section 3, distance-based heuristics virtually
insensitive planning graph evolution search segment transposed successive levels. Since
search trace contains children states generated regression search state episode n, heuristic preference include states trace visit episode n+1
reflect chance directly generate new promising search branches. child states
search episode n competitors S, ideally heuristics rank reflect
sense value visiting state beyond importance children.
Consider sensitivity adjusted-sum heuristic (or distance-based heuristics)
possible differences implicit regression search space set propositions, S, planning graph level k versus level k+1. Given propositions present binary non-mutex
level k, cost summation factor equation 5-3 could conceivably change
evaluated level k+1. would require two conditions: new action must establish one
propositions first time level k+1 actions precondition costs must sum less
precondition costs establisher proposition. practice happens infrequently since later action appears graph construction process, higher cost tends
be. Consequently h-values states based distance-based heuristics remain remarkably constant planning graph levels beyond propositions appear binary non564

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

mutex13. desire means compensating static h-value state transposed planning graph level promising new branches regression search open up.
likelihood state visited episode n graph level k give rise new child states
visited episode n+1 level k+1 rooted graph dynamics summarized Observations A-1
A-2 Appendix A. Three planning graph memo cache properties determine whether regression search subgoal set evolve successive episodes:
1. new actions level k+1 establish subgoal
2. dynamic mutexes level k actions establishing subgoals relax
level k+1
3. no-good memos encountered regression search state episode n
encountered level k+1 (and also converse).
set measures potential new search branches result visiting state PE
refer flux; intuition higher flux, likely search
given state differ seen previous search episode (and captured PE). none
three factors applies state consideration, point visiting it, new
search result relative previous episode.
first factor readily assessed state (thanks part bi-partite graph structure). second flux factor unfortunately expensive assess; direct measure requires storing
pairs attempted action assignments goals inconsistent episode n retesting new planning graph level. However, graph mechanics relaxation
dynamic mutex two actions level k requires relaxation dynamic mutex condition
pair preconditions level k-1 (one precondition action). relaxation, turn, either due one new establishing actions preconditions level k-1
recursively, relaxations existing actions establishing preconditions. such, number new
actions establishing subgoals state PE (factor 1 above) provide measure
flux S, also predictor flux due factor 2 parent (and higher ancestors) S.
Thus, turns simply tracking number new actions state subgoal current level propagating appropriately weighted measure parent, compile useful
estimate flux factors 1 2 above.
third flux factor unwieldy costly estimate; exact measure requires
storing child states generated regression search level k caused backtracking due
cached memos, retesting see memos present level k+1.14 Ignoring
factor, sum two flux measures depend new actions derive filtering metric
used assist largely static adjusted-sum distance-based heuristic culling
beam. resulting (inexact) metric sensitive evolution search potential state transposed higher planning graph levels:

13

This, part, explains observation (Nguyen & Kambhampati, 2000) AltAlt state space planner performance
generally degrades little planning graph used extract heuristic values built level problem goals appear non-mutex, rather extending level-off.
14
Note long using EBL/DDB, sufficient test whether memo exists child state.
no-good goals contribute conflict set used direct search within whenever backtracking occurs.

565

fiZIMMERMAN & KAMBHAMPATI

5-4)

flux( ) =

newacts( p )


pi

|S|

+ wcf

childflux(s )

si c



where: pi proposition state
newacts(pi) number new actions establish proposition pi
associated planning graph level
| | normalization factor; number propositions
Sc set child states currently represented search trace
childflux(si) sum two flux terms eqn 5-4 applied child state si
wcf weights contribution flux child states parent state

number new actions establishing subgoals state normalized relative number subgoals state.
report elsewhere (Zimmerman, 2003) use flux directly augment secondary
heuristic. Depending domain weighting flux contribution adjusted-sum heuristic, speedups order magnitude observed15. However impact highly domain
dependent since primarily concerned performance general purpose planner,
study consider use beam filter.
beam search, flux measure strongly impact every search episode, influences
states actually included active PE. used mode, search segments assessed
flux specified threshold skipped even f-value places active PE.
Flux proves broadly effective across domains used mode. mentioned
Section 5.1, use flux cutoff search episode 10% average flux search segments PE; segment value visited regardless heuristic rank.
setting impact speedup PEGG column problems Table 3 ranges nil factor
9x PEGGs performance without flux filter. Higher settings dramatically speed
solution search, often expense greater solution makespan.
5.6.2 PEGGS ABILITY FIND STEP-OPTIMAL PLANS
variety parameters associated beam search approach described admits considerable flexibility biasing PEGG towards producing plans different quality. Shorter makespan plans
favored extensive search PE states episode heuristically truncated
search tends generate non-optimal plans quickly, often containing redundant unnecessary
actions. settings used study clearly bias PEGG solutions towards step-optimality:
step-optimal plan produced enhanced Graphplan matched PEGG four 37
problems reported Table 3, indicated annotated steps actions numbers given parenthesis next successful GP-e PEGG runs16. (PEGG solutions longer makespan
step-optimal boldface step/action values.) four problems, PEGG returns solutions
15

example, compared simple bottom-up visitation strategy, flux-augmented adjusted-sum heuristic improves
so-PEGG runtimes 11x domains (e.g. Freecell and, Satellite) degrading much 2x 7x
others (e.g. Zenotravel).

16

one guaranteed step-optimal planners (GP-e, me-EGBG so-PEGG) finds solution steps
actions reported one them, since makespan.

566

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

Problem

N-best first
PEGG
[N=100, state-space search] [ adjusted-sum-u heuristic beam search
100 best search segments]

SATPLAN
(optimal)

bw-large-a

8

6

(.1 s)

6

bw-large-b

12

9 (4.5 s)

9

bw-large-c

21

14 (39.0 s)

14

bw-large-d

25

18 (412 s)

18

Table 4: Quality comparison (in terms plan steps) PEGG N-best beam search
forward state space planner (Bonet et al., 1997).
within four steps optimum, spite highly pruned search. proved fairly robust
property PEGGs beam search settings across problems tested date.
PEGG adjusted-sum-u secondary heuristic often finds plans fewer actions
GP-e parallel domains, Graphplan hybrid system also impressive serial domains
blocksworld (which exactly Graphplans forte).
tactic trading optimal plan length favor reduced search effort well known
planning community. comparison, PEGGs beam search approach biased towards producing
high quality plan possibly expense runtime. example, paper focusing
action selection mechanism planning, Bonet et. al. briefly describe work N-best
first algorithm (Bonet, Loerincs, & Geffner, 1997). employ distance-based heuristic
conduct beam search forward state space planning. report small set results case
100 best states retained queue considered search.
Table 4 reproduces results alongside PEGGs performance problems using beam
search. Here, approximate N-best algorithm, PEGG also run 100 states visited
intermediate search episode. 1997 study compared N-best first approach SATPLAN,
produces optimal length plan, make point approach could produce plans reasonably close optimal much less search. N-best first code available run
test platform, PEGGs runtime reported. Focusing plan makespan, clear even
serial domain, parallel planner PEGG produces much shorter plan N-best first
state space approach, fact finds optimum length plan generated SATPLAN cases.
recently LPG (Gerevini & Serina, 2002), another planner whose search tightly integrated
planning graph, awarded top honors AIPS-2002 planning competition, due
ability quickly produce high quality plans across variety domains currently interest.
Figure 11 scatter plot, solution quality terms steps LPG PEGG compared
optimal 22 problems three domains 2002 AIPS planning competition. chose
particular problems optimal solution known, interested comparing
quality baseline. LPGs results particularly apt case, planner also nonexhaustively searches planning graph level extending it, although search process
differs markedly PEGGs. LPG, too, biased produce plans higher quality (generally
expense speed) report competition results quality mode. terms
number actions solutions neither planner consistently dominates problems
PEGG clearly excels step-optimality. maximum deviation optimum four steps
567

fiZIMMERMAN & KAMBHAMPATI

18

LPG: dlog

16

LPG: depot

Steps optimal

14

LPG: ztravel

12

PEGG: dlog

10

PEGG: depot

8

PEGG: ztravel

6
4
2
0
0

2

4

6

8

10

Problem number

12

14

16

18

Figure 11: Makespan comparison PEGG LPG -Departure step-optimal
plan length. (LPG data taken AIPS 2002 competition results.)
plot points solutions lie right optimal makespan axis. possible
sets actions within LPGs solutions could conducted parallel algorithms quality
mode heuristic insensitive .
noted LPG produced solutions difficult problems domains
PEGG currently solve within reasonable time limit. investigating characteristics
problems make difficult PEGG.
5.6.3 PEGG COMPARED HEURISTIC STATE SPACE SEARCH
attempted run PEGG head-to-head speed recent IPC planners, part
due platform difficulties (PEGG written Lisp competition planners generally
coded C published results based execution competition machines) partly
due focus near-optimal makespan parallel plans rather speed. Given PEGGs close
coupling planning graph, relevant comparisons parallel planners
also employ graph form. comparisons, would like isolate search component runtime planning graph construction, since variety routines produce essentially graph widely different expenditures computational time memory. reported runtimes LPG planner AIPS-02 competition generally much
smaller PEGGs, difficult isolate impact graph construction platform-related
effects, mention disparity makespan plans produced.
Table 5 compares PEGG Lisp version fast distance-based heuristic state space planner using problems Table 3. AltAlt (Srivastava et al., 2001), like PEGG, depends
planning graph derive powerful heuristics uses direct regression search
problem goals. facilitates planner performance comparison based differences search without confusing graph construction time issues. last column Table 5 reports AltAlt performance
(runtime makespan) two effective heuristics developed planner (Nguyen &
Kambhampati, 2000), first adjusted-sum heuristic described Section 5.3.1.

568

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

Problem
bw-large-B
bw-large-C
bw-large-D
rocket-ext-a
att-log-a
att-log-b
Gripper-8
Gripper-15
Gripper-20
Tower-7
Tower-9
8puzzle-1
8puzzle-2
TSP-12
AIPS 1998
grid-y-1
gripper-x-5
gripper-x-8
log-y-5
mprime-1
AIPS 2000
blocks-10-1
blocks-12-0
blocks-16-2
logistics-10-0
logistics-12-1
freecell-2-1
schedule-8-9
AIPS 2002
depot-6512
depot-1212
driverlog-2-3-6e
driverlog-4-4-8
roverprob1423
roverprob4135
roverprob8271
sat-x-5
sat-x-9
ztravel-3-7a
ztravel-3-8a

PEGG
heuristic: adjusted-sum-u
cpu sec (steps/acts)
4.1
24.2
388
1.1
2.2
21.6
5.5
46.7
1110.8
6.1
23.6
9.2
7.0
6.9

(18/18)
(28/28)
(38/38)
(7/34)
(11/62)
(13/64)
(15/23)
(36/45)
(40/59)
(127/127)
(511/511)
(31/31)
(32/32)
(12/12)
PEGG
16.8 (14/14)
110
(23/35)
520
(35/53)
30.5 (16/34)
2.1 (4/6)
PEGG
6.9 (32/32)
9.4 (34/34)
40.9 (56/56)
7.3 (15/ 53)
17.4 (15/75)
19.1 (6/10)
297
(5/12)
PEGG
2.1 (14/31)
79.1 (22/53)
80.6 (12/26)
889
(23/38)
15
(9/28)
379
(12 / 43)
220
(11 / 39)
25.1 (7/22)
9.1 (6/35)
101
(10/23)
15.1 (9/26)

Alt Alt (Lisp version)
cpu sec ( / acts)
heuristics:
combo
adjusum2
67.1 (/ 18 ) 19.5 (/28 )
608 (/ 28) 100.9 (/38)
950 (/ 38)
~
23.6 (/ 40) 1.26 (/ 34)
16.7 ( /56) 2.27( / 64)
189 (/ 72) 85.0 (/77)
6.6 (/ 23)
*
10.1 (/ 45)
6.98 (/45)
38.2 (/ 59) 20.9 (/59)
7.0 (/127)
*
28.0 (/511)
*
33.7 ( / 31) 9.5 ( /39)
28.3 (/ 30)
5.5 (/ 48)
21.1 (/12) 18.9 (/12)
Alt Alt
17.4 (/14)
17.5 (/14)
9.9 (/35)
8.0 (/37)
73 (/48)
25.0 (/53)
44 (/38)
29.0 (/42)
722.6 (/ 4)
79.6 (/ 4)
Alt Alt
13.3 (/32)
7.1 (/36)
17.0 (/34)
61.9 (/56)
31.5 (/53)
80 (/77)
49 (/12)
123 (/15)
Alt Alt
1.2 (/33)
290 (/61)
50.9 (/28)
461 (/44)
2.0 ( /33)
292 ( /45)
300 ( / 45)
3.1 (/25)
5.9 (/ 35)
77 (/28)
15.4 (/31)

Table 5: PEGG state space planner using variations adjusted-sum heuristic
PEGG: bounded PE search, best 20% search segments visited search episode, ordered
adjusted-sum-u state space heuristic
AltAlt: Lisp version, state space planner two effective planning graph distance-based
heuristics: adjusum2 combo (combo results reported problems since
adjusum2 produces plans competitive PEGG terms makespan.)

569

fiZIMMERMAN & KAMBHAMPATI

Surprisingly, majority problems PEGG returns parallel, generally step-optimal plan faster
AltAlt returns serial plan. (AltAlt cannot construct plan parallel actions, however recent
work highly modified version AltAlt does, fact, construct plans -Nigenda & Kambhampati, 2003). PEGG plans also seen comparable length, terms number actions, best AltAlt plans.
6. Discussion Results

distinguishing feature EGBG PEGG planners relative planners exploit
planning graph, aggressive use available memory learn online episodic search
experience expedite search subsequent episodes. Although employ search trace
structure log experience, EGBG PEGG systems differ content granularity search experience track aggressiveness use memory. also differ
confront common problem faced learning systems; utility learned information
versus cost storing accessing needed.
first efforts focused primarily using search trace learn mutex-related redundancies
episodic search process. Although resulting planners, EGBG me-EGBG, avoid virtually redundant mutex checking based search experience embodied PEs, empirically
find limited class problems winning strategy. utility tracking
mutex checking experience search function number times information
subsequently used. Specifically:
EPS ( p )

6 1) U mt ( p)

PE

visit

( e)

PE

add

(e)

e =1
EPS ( p )
e =1

where: Umt utility tracking mutex checking experience
p planning problem
EPS(p) number search episodes problem p
PEvisit (e) number PE search segments visited search episode e
PEadd (e)is number new search segments added PE episode e
Thus payback EGBGs incurred overhead tracing consistency-checking experience
search depends number times sets revisited relative total number subgoal
sets generated (and added PE) problem run. characteristic explains less
2x speedups observed me-EGBG many Table 2 problems. approach handicap single search episode problems. also ineffectual problems final search episode search generates large number states relative previous episodes seed segment(s)
top levels PE (due need bottom-up visitation search segments EGBGs
search trace).
PE thought snapshot regression search reachable (RS reachable) states
search episode. is, regression search process generates state level k
planning graph, state reachable search higher levels graph future search
episodes. Essentially, search segments PE represent RS reachable states,
570

fispeedup wrt enhanced Graphplan

USING MEMORY TRANSFORM SEARCH PLANNING GRAPH

so-PEGG: log

1000.0

PEGG: log
so-PEGG: ztravel
PEGG:ztravel

100.0

10.0

1.0
1
0.1

2

3

4

5

6

7

search episodes problem

Figure 12: Speedup vs. number search episodes: Logistics '00 Zenotravel '02 domains
candidate set partial plans segments state current tail state plan. Table 3 5 results indicate utility learning states RS reachable given search
episode generally outweighs utility learning details episodes consistency-checking,
require much less memory. Freed need regenerate RS reachable states IDA* fashion search episode, PEGG visit states heuristically preferred order.
Tables 2, 3 5 shed light several classes problems problematic search trace
guided search planning graph:
1. Domains high branching factors operator descriptions thwart DDB EBL
(e.g. larger Schedule, Satellite, Zenotravel domain problems)
2. Problems significant fraction runtime consumed planning graph construction.
(e.g. Grid domain,, dlog-2-3-6e, freecell-2-1)
3. Problems one two search episodes ( grid-y-1, schedule-8-5)
problems first class, Graphplan-style CSP assignment search prone bogging
certain PE states selected visitation. second class search time
reduction dominated large graph construction time (a problem shared planner
builds complete planning graph). Problems third class give PEGG sufficient
opportunity exploit PE, since built first episode (and first episode PE typically
small) benefit subsequent episodes. aspect PEGGs behavior illustrated
Figure 12. speedup factors so-PEGG PEGG (under beam search) plotted
series problems ordered according number search episodes Graphplan would conduct prior finding solution. data gathered running GP-e, so-PEGG, PEGG
planners two different domains (the Logistics domain AIPS-00 planning competition,
Zenotravel domain AIPS-02 competition) averaging speedups observed
problems number observed search episodes. downturn PEGG/ Ztravel
curve seven episodes surprising given one problem
many factors beyond number search episodes impact solution time. Noting speed571

fiZIMMERMAN & KAMBHAMPATI

ups plotted logarithmic scale, power search trace given multiple search episode problems evident. PEGG using beam search handily outperforms so-PEGG problems three
search episodes, largely shortcuts exhaustive search intermediate episodes.
several avenues addressing above-listed limitations PEGG explored anticipate investigating. example, unlike N-best first state space planner reported
Table 4, PEGG enforces user-specified limit state f-values selecting PE search
segments visit. search segment chosen visitation, Graphplan-style regression search
state goals continues either solution found sub-branches fail. greedy
approach would also apply heuristic bound regression search. is, could
backtrack whenever state generated exceeds f-value threshold applied search segments
visited. translates Greedy Best First Search (GBFS) algorithm employed
HSP-r (Bonet & Geffner, 1999) state space search, form hill-climbing search planning graph.
Experimentally find PEGG adapted enforce PE state f-value limit
regression search, improvements unpredictable best. Speedups factor 100
observed cases (all logistics problems) many cases runtimes increased search failed
entirely within time limit. addition, quality (make-span) returned solutions suffered
across broad range problems. two factors may explain result: 1) PEGGs regression search greatly expedited DDB EBL, regressed conflict set rely
undefined regression search space state fully explored, fvalue limit enforced. Without conducting search informed basis returning anything full set subgoals state, essentially forces search towards chronological backtracking. 2) Assessing f-value newly generated state compare fvalue bound based states generated previous episodes problematic.
heuristic values PE states determine f-value bound increased PEGGs use
search experience improve h-value estimates (Section 5.1.2).
Degradation solution quality shift PEGG closer greedy search approach may indicator PEGGs ability return step-optimal plans (as evidenced Table 3 results) rooted
interleaving best-state selection PE Graphplan-style depth-first search
states subgoals.
7. Related Work

focus related alternative strategies employing search heuristics planning, generating parallel plans, making use memory expedite search. Related work pertaining
search techniques, efficiencies, data structures enable EGBG PEGG successfully
employ search trace cited arose considered here.17
noted Section 2.2, shortcoming IDA* search (and Graphplan) inadequate use
available memory: information carried one iteration next upper
bound f-value. Exploitation search trace directly addresses shortcoming serving
memory states visited search space previous episode order reduce redun17

Support methodologies include memory efficient bi-partite planning graph models, explanation based learning dependency directed backtracking context planning graph search, variable value ordering strategies, evolution
extraction distance-based heuristics planning graph.

572

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

dant regeneration. respect PEGGs search closely related methods MREC (Sen,
Anup & Bagchi, 1989), MA*, SMA* (Russell, 1992) lie middle ground
memory intensive A* IDA*s scant use memory. central concern algorithms
using prescribed amount available memory efficiently possible. Like EGBG PEGG,
retain much search experience memory permits avoid repeating regenerating
nodes, depend heuristic order nodes memory visitation. Unlike search trace
based algorithms though, three algorithms backup deleted nodes f-value parent node. ensures deleted branch re-expanded promising node remains open list. implemented extended memory management PEGG
(though would straight-forward so) primarily because, least beam search, PEGG
seldom confronted PE-related memory limitations.
EGBG PEGG first planners directly interleave CSP state space views problem search, related approaches synthesize different views planning problem.
Blackbox system (Kautz & Selman 1999) constructs planning graph instead exploiting
CSP nature, converted SAT encoding extension k-step solution
sought. GP-CSP (Do & Kambhampati, 2000), similarly alternates extending planning
graph converting it, transforms graph CSP format seeks satisfy constraint
set search phase.
beam search concept employed context propositional satisfiability GSAT (Selman, Levesque, & Mitchell, 1992) option Blackbox planner (Kautz & Selman, 1999).
systems greedy local search conducted assessing episode, n-best flips
variable values randomly generated truth assignment (Where best flips lead
greatest number satisfied clauses). n flips fail find solution, GSAT restarts new
random variable assignment tries n-best flips. several important differences
relative PEGGs visitation n-best search trace states. search trace captures state aspect engendered Graphplans regression search problem goals such, PEGG exploits
reachability information implicit planning graph. conducting search purely propositional level, SAT solvers leverage global view problem constraints cannot exploit
state-space information. Whereas GSAT (and Blackbox) improve performance based
experience one n-best search episode next, PEGG learns variety modes; improving heuristic estimate states visited, reordering state goals based prior search experience, memorizing general no-goods based use EBL.
Like PEGG, LPG system (Gerevini & Serina, 2002) heavily exploits structure planning graph, leverages variety heuristics expedite search, generates parallel plans. However,
LPG conducts greedy local search space composed subgraphs given length planning
graph, PEGG combines state space view search experience Graphplans CSP-style
search graph itself. LPG systematically search planning graph heuristically
moving extend it, guarantee step-optimality forfeited. PEGG operate either
step-optimal mode modes trade optimality speed varying degrees.
currently investigating interesting parallel LPGs ability simultaneously consider
candidate partial plans different lengths. principle, nothing prevents PEGG
simultaneously considering given PE search segment Sn, terms heuristic rankings
transposed onto various levels planning graph. tantamount simultaneously consider573

fiZIMMERMAN & KAMBHAMPATI

ing arbitrary number candidate partial plans different implied lengths extend first
(each partial plan Sn tail state). search trace proves useful
regard state contains transposed desired number levels -subject
ability extend planning graph needed- heuristics re-evaluated level. Referring back Figure 4, first search episode pictured (top), YJ state PE could
expanded multiple distinct states transposing graph level 5 levels 6, 7, higher,
heuristically evaluating level. graph-level indexed instances YJ
simultaneously compared. Ideally wed like move directly visiting YJ planning graph level 7,
since point becomes plan segment problem (bottom graph Figure 4). secondary heuristic discriminate solution potential state sequential levels
transposed to, effective means shortcutting Graphplans level-bylevel search process. flux adjunct likely one key boosting sensitivity distancebased heuristic regard.
Generating assessing arbitrarily large number graph-level transposed instances PE
states would prohibitive terms memory requirements store multiple versions
PE. However simply store level-specific heuristic information search segments
single PE values indexed associated planning graph levels. Challenging issues include
things range plan lengths considered one time potential plans
steps consisting entirely persists actions.
havent examined PEGG context real-time planning here, use search
trace reflects flavor real-time search methods, LRTA* (Korf, 1990)
variants B-LRTA* (Bonet, Loerincs, & Geffner, 1997), -a variant applies distance-based
heuristic oriented planning problems. Real-time search algorithms interleave search execution,
performing action limited local search. LRTA* employs search heuristic based
finding less-than-optimal solution improving heuristic estimate series iterations.
associates h-value every state estimate goal distance state (similar h-values
A*). always first updates h-value current state uses h-values successors move successor believed minimum-cost path current state goal.
Unlike traditional search methods, act real-time also amortize learning consecutive planning episodes solves planning task repeatedly. allows find suboptimal plan fast improve plan converges minimum-cost plan.
Like LRTA*, PEGG search process iteratively improves h-value estimates states
generated determines optimal make-span plan. Unlike LRTA*, PEGG doesnt actually
find sub-optimal plan first. Instead converges minimum-cost plan either exhaustively extending candidate partial plans monotonically increasing length (so-PEGG) extending
promising candidates according secondary heuristic (PEGG beam search). realtime version PEGG closely related LRTA* might based method described above,
search segments simultaneously transposed onto multiple planning graph levels.
mode PEGG would biased search quickly plan length, search anytime
fashion progressively shorter length planning graphs lower cost plans.
methodology direct relevance work reported elsewhere multi-PEGG
(Zimmerman & Kambhampati, 2002; Zimmerman 2003), version PEGG operates anytime fashion, seeking optimize multiple plan quality criteria. Currently multi-PEGG first re-

574

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

turns optimal make-span plan, exploits search trace novel way efficiently
stream plans monotonically improve terms quality metrics. discussed paper,
important step away multi-PEGGs bias towards make-span plan quality metric would
modification. Co-mingling versions state transposed onto multiple planning
graph levels would enable planner concurrently consider visitation candidate search segments might seed segments latent plans various lengths.
8. Conclusions

investigated presented family methods make efficient use available memory
learn different aspects Graphplans iterative search episodes order expedite search
subsequent episodes. motivation, design, performance four different planners build
exploit search trace described. methods differ significantly either information
content trace manner leverage it. However, cases high-level
impact transform IDA* nature Graphplans search capturing aspect search
experience first episode using guide search subsequent episodes, dynamically updating along way.
EGBG planners employ aggressive mode tracing search experience PEGG
planners. track use action assignment consistency checking performed search
subgoal set (state) minimize effort expended state next visited. EGBG approach found memory intensive, motivating incorporation variety techniques
planning CSP fields which, apart well-known speedup benefits, shown
dramatic impact search trace planning graph memory demands. resulting planner,
me-EGBG, frequently two orders magnitude faster either standard Graphplan EGBG
problems handle, generally fastest guaranteed step-optimal approaches
investigated. comparisons GP-e, version Graphplan enhanced space saving
speedup techniques, me-EGBG solves problems average 5 times faster.
PEGG planners adopt skeletal search trace, design conducive informed traversal search space. Ultimately proves powerful approach exploiting
episodic search experience. adapt distance-based, state space heuristics support informed traversal states implicit search trace describe metric call flux effectively
focuses search states worth visiting. flux measure sensitive potential search
trace state seed new search branches transposed higher planning graph levels. also
describe new techniques leverage search experience captured search trace
demonstrate effectiveness.
so-PEGG planner, like me-EGBG, produces guaranteed optimal parallel plans similarly
averages 5x speedup GP-e. greatly reduced memory demands allow so-PEGG handles
one 16 problems me-EGBG exceeds available memory. compelling evidence speedup potential search trace guided planner provided PEGG beam
search. Since longer exhaustively searches planning graph episode, PEGG sacrifices
guarantee returning optimal make-span plan. Nonetheless, even beam search limited
best 20% PE states episode, PEGG returns step-optimal plan almost 90%
test bed problems comes within steps optimal others. speedups

575

fiZIMMERMAN & KAMBHAMPATI

ranging almost two orders magnitude GP-e, quite competitively modern state
space planner (which finds serial plans).
code PEGG planners (including GP-e) instructions running various
modes available download http://rakaposhi.eas.asu.edu/pegg.html
Acknowledgements

research improved many discussions Binh Minh Do, XuanLong Nguyen, Romeo
Sanchez Nigenda William Cushing. Thanks also David Smith anonymous reviewers,
whose copious suggestions greatly improved presentation paper. research supported
part NSF grants IRI-9801676 IIS-0308139, DARPA AASERT Grant DAAH04-96-10247 NASA grants NAG2-1461 NCC-1225.

Appendix A: EGBG Planner

insight behind EGBGs use search trace based characterization Graphplans
search given beginning Section 3.1 entailed observations:
Observation A-1) intra-level CSP-style search process conducted Graphplan set propositions (subgoals) , planning graph level k+1 episode n+1 identical search process
level k episode n IF:
1. mutexes pairs actions establishers propositions level k remain
mutex level k+1. (this concerns dynamic mutexes; static mutexes persist definition)
2. actions establishing proposition level k+1 also present level k.
Observation A-2) trace Graphplans search episode n+1, set goals G, planning
graph level m+1, identical episode n search level IF:
1. two conditions observation A-1 hold every subgoal set (state) generated Graphplan
episode n+1 regression search G.
2. every subgoal set planning graph level j search episode n matching
level j memo, exists equivalent memo level j+1 generated episode n+1.
Conversely, every subgoal set level j search episode n matching level j memo
existed time generated, also matching memo level j+1 time generated episode n+1.

Now, suppose search trace states (including no-good states) generated Graphplans regression search problem goals planning graph level episode n. search
failed extract solution m-length planning graph (i.e. reach initial state), necessary condition extract solution m+1 length graph one conditions
observations A-1 A-2 fails hold states episode n search trace.
observations A-1 A-2 mind, exploit search trace new episode
sound complete manner focusing search effort three situations could lead
solution: 1) state variables newly extended value ranges (i.e. search segment goals
least one new establishing action newly associated graph level), 2) points previous search episode backtracked due violation dynamic constraint (i.e. two actions
576

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

W

nop a1
OK OK
1



7

nop a3
OK SM

nop a3
OK SM

2

nop a1
OK OK

H

3



6

8

nop a3 a1
OK SM OK
4 nop a55

J

DM SM

nop a5 nop a5
DM SM DM SM

nop a5
DM SM

Figure A1: Bit vector representation search trace WYHIJ state Figure 3.
Semantics: OK > assigned (no action conflicts) SM > action static mutex previous assign
DM > action dynamic mutex previous assign
NG > no-conflict action results no-good state lower graph level

dynamic mutex), 3) states matched cached memo episode n. assignment mutex checking operations involved satisfying set subgoals static across search
episodes.
experimented several search trace designs capturing key decision points. design
adopted EGBG employs ordered sequence bit vectors, vector contains results
Graphplans CSP-style action assignment process related given subgoal search segment.
Efficient action assignment replay possible trace uses vectors two-bit tags represent
four possible assignment outcomes: 1) dynamic mutex, 2) static mutex, 3) conflict, 4) complete, consistent set assignments rejected next level due memoized no-good. Figure A1 illustrates sequence eight bit vectors used capture search experience search segment state goals WYHIJ Figure 3 Alpha problem. propositional goals (the variables) appear left sets bit vectors (depicted segmented bars)
encode outcome possible action assignment (the values). possible establishing
action goal appears bit vector tag.
numbered edges reflect order trace vectors initially created first
goal action tried. Note whenever candidate action goal conflict free respect
previously assigned actions (indicated OK figure), action checking goal suspended, process jumps next goal, new bit vector initialized goals possible
establishers. edge numbering also reflects order vectors popped
search segment trace list segment revisited next episode. scheme work,
bit vectors must pushed onto search segment trace list actions goal tried,
reverse numbered edge order. Long edges skip one goals indicate
goals already established previously assigned actions.
long order actions appearing establishers list planning graph proposition remains constant, bit vectors used replay search next episode next
higher planning graph level. graph building routine EGBG enforces constraint.

577

fiZIMMERMAN & KAMBHAMPATI

EGBG Algorithm

high-level EGBG algorithm given Figure A2. Graphplan, search planning
graph occurs extended level problem goals first appear
binary mutex conditions. (the call find_1st_level_with_goals). first search episode conducted
Graphplan fashion except assign_goals routines Figure A3 create search segments
hold states trace information generated regression search process. necessary
trace information search segment captured trace vectors described above. segments stored PE structure indexed according level generated
(where current highest planning graph level corresponds 0 contains problem goals).
Subsequent first episode, EGBG_plan enters outer loop employs PE conduct
successive episodes (Referred search trace guided). search strategy alternates
selection visitation promising state trace previous experience (select_searchseg_from_PE routine), focused CSP-type search states subgoals (the replay_trace_goals assign_goals routines Figures A3 A4).
episode, inner loop visits PE search segments level-by-level, bottom-up fashion
(for reasons discussed Section 3). extend_plangraph routine called state
visited corresponds level beyond current graph length.
replay_trace_goals routine counterpart Graphplans assign_goals routine, except
avoids latters full-blown mutex checking stepping trace vectors captured
previous search experience given state. Unlike assign_goals, branch child
states already contained PE. conditional checking trace vectors establishing
actions initiates new search calling assign_goals two conditions: 1) dynamic mutexes
previous episodes longer hold 2) new establishing actions appear subgoal (These
tried establishers replayed.) dynamic mutex longer holds new
establishing action considered trace vector modified accordingly EGBG resumes Graphplans CSP-style search, adding new trace vectors search segment process.

578

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

EGBG-PLAN ( Ops, Init, Goals) /* {Ops,Init,Goals} planning problem */
/* build plangraph, PG, level n goals first occur non-mutex state*/

Let PG find_1st_level_with_goals( Ops, Init, Goals )
PG reached level-off goals present non-mutex state Return FAIL
Let n number levels PG
Let SS0 new search segment fields:
goalsGoals, parentroot, PE-level 0, parent-actions {} trace {}
Let PE pilot explanation structure fields hold search segments plangraph level
PE[0] {SS0} /* 0 top level PE */
/* Conduct Graphplan-style backward search n-length plangraph, store trace PE...*/

Let search-reslt assign_goals(Goals, {}, n, SS0, PG, PE)
search-reslt search segment /* Success */
Plan extract plan actions ancestors linked search-reslt
Return Plan
else /* n-length solution possible ...use PE search longer length solution */
loop forever
n n+1
loop pe-lev ranging number deepest level PE 0 (top level)
let k planning graph level associated PE level pe-lev
= n pe-lev /* ..essentially translates PE one planning graph level */
pe-lev = 0 PG extend_plangraph(PG) /*.. must extend plangraph point */
loop search segments PE[pe-lev]
SS select_searchseg_from_PE[pe-lev]
SSassigns SS<trace> /* get ordered, goal-by-goal trace vectors search segment */
SS<trace> {} /* Clear search segment trace vectors field */
SS<goals> memos(k, PG) /*check nogoods level k */
goals match nogood level, loop next search segment
else /* use SS trace avoid redundant search effort SS goals.. */
search-reslt replay_trace_goals (SS<goals>, {}, k, SSassigns, SS, PG, PE)
search-reslt search segment
Plan extract plan actions ancestors linked search-reslt
Return Plan
else Add SS<goals> memos(k, PG) /*memoize nogood */
end loop (search segments)
end loop (PE levels)
end-loop (PG level)
end

Figure A2: EGBG planner top level algorithm

579

fiZIMMERMAN & KAMBHAMPATI

Conduct Graphplan-style search subgoal set planning graph level k
arguments> G: goals still assigned, A: actions already assigned, k: PG level,
SS1: search segment, PG: planning graph, PE: pilot explanation (search trace)
ASSIGN_GOALS (G, A, k, SS1, PG, PE)
G empty k = 0 (the initial state) Return SS1 /* Success */
else /* goals left satisfy*/
Let g-assigns ={} /* trace vector hold ordered action assignment tags */
Let g goal selected G
Let Ag = actions PG level k support g, ordered value-ordering heuristic
loop act Ag
Let search-reslt = {}
action dynamic mutex act append dm tag g-assigns
else action static mutex act append sm tag g-assigns
else /* act conflict actions already */
G empty
/* done G goals, setup search next lower level */
search-reslt assign_next_level_goals (A U{act}, k, SS1, PG, PE)
search-reslt nogood append ng tag g-assigns
else append ok tag g-assigns /* search occurred lower level */
else /* search continues level next goal */
append ok tag g-assigns
search-reslt assign_goals (G-{g}, U{act}, k, SS1, PG, PE)
end-if
search-reslt search segment Return search-reslt /* Success */
/* else loop try another action*/

end-if
end loop (actions)
push g-assigns trace field SS1
Return nil /* solution found */
end-if
end

/*add trace data search segment */

Setup search graph level k-1 given actions satisfy goals SS1 level k
ASSIGN_NEXT_LEVEL_GOALS (A, k, SS1, PG, PE)
Let nextgoals regress SS1 goals assigned actions
nextgoals memos PG level k-1 Return nogood /* backtrack nogood goals */
else /* initiate search next lower PG level*/
Let SS2 new search segment fields:
goalsnextgoals, parentSS1, parent-actionsA, trace{}
Add SS2 PE level: (maximum PG level) (k-1)
Let search-reslt assign_goals (nextgoals, {}, k-1, SS2, PG, PE)
search-reslt nil /* search level k-1 failed */
Add nextgoals memos level k-1 PG /*memoize nogood */
Return search-reslt
end-if
end
ASSIGN_NEW_ACTIONS (G, A, Ag, g, g-assigns, k, SS, PG, PE)
/* Routine essentially assign_goals, except attempts satisfy goal g actions
new actions (i.e. first appearing recent plangraph extension */

Figure A3: EGBGs non-guided regression search algorithm

580

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

Regression search using search trace (PE) replay
arguments> G: goals still assigned, A: actions already assigned, k: PG level,
SS1: search segment, PG: planning graph, PE: pilot explanation (search trace)
REPLAY_TRACE_GOALS (G, A, k, SS1, PG, PE)
G empty /* SS1 goals branch successfully assigned last episode...*/
Return /* .. continue level k replay, ignoring search replay next lower level */
else /* goals left satisfy*/
Let g select goal G
Let g-assigns pop front trace vector SS<trace>
Let Ag set actions level k PG support g
/* replay assignments g previous episode, rechecking may changed..*/

loop tag g-assigns
Let search-reslt {}, Let act pop action Ag
tag = ok /* act conflict actions last episode.. go next goal */
search-reslt replay_trace_goals (G-{g}, U{act}, k, SSassigns, SS1, PG, PE)
else-if tag =ng loop /* last action assign level & act mutex last
episode --So next-level regressed goals reside child search segment already visited */

else tag = sm loop /* act static mutex action last episode */
else tag = dm /* act dynamic mutex action last episode retest it...*/
dynamic mutex persists loop
else change tag ok g-assigns vector /* act longer mutex actions */
G-{g} empty /* resume backward search level */
search-reslt assign_goals (G-{g}, U{act}, k, SS1, PG,PE)
else /* goals left satisfy SS1, setup search lower level */
search-reslt assign_next_level_goals (A U{act}, k, SS1, PG, PE)
search-reslt =nogood change g-assigns vector tag ng
end-if
search-reslt search segment Return search-reslt (Success)
else loop (check next action)
end-loop /* establishment possibilities prior episode tried ..Now check new actions */
Ag still contains actions /* new actions establishing g level .. attempt assign */
search-reslt assign_new_actions(G, A, Ag, g, g-assigns, k, SS1, PG, PE)
search-reslt search segment Return search-reslt /* Success */
else push g-assigns SS1<trace>
Return nil /* solution found search stemming SS1 goals */
end-if
end

Figure A4: EGBGs search-trace guided algorithm

581

fiZIMMERMAN & KAMBHAMPATI

Appendix B: Exploiting CSP Speedup Methods Reduce Memory Demands

Background implementation details provided six techniques planning
CSP fields proved key controlling memory demands search trace based
planners. variable ordering, value ordering, explanation based learning (EBL), dependency
directed backtracking (DDB), domain preprocessing invariant analysis, replacing redundant multi-level planning graph bi-partite version.
Domain preprocessing invariant analysis:
speedups attainable preprocessing domain problem specifications well
documented (Fox & Long, 1998; Gerevini & Schubert, 1996). Static analysis prior planning
process used infer certain invariant conditions implicit domain theory and/or problem
specification. domain preprocessing me-EGBG PEGG fairly basic, focusing identification extraction invariants action descriptions, typing constructs, subsequent rewrite
domain form efficiently handled planning graph build routines. implementation discriminates static (or permanent) mutex relations dynamic mutex relations
(in mutex condition may eventually relax) actions proposition pairs. information used expedite graph construction me-EGBGs replay action assignments search segment visited.
Domain preprocessing significantly reduce memory requirements extent identifies
propositions need explicitly represented level graph. (Examples
terms extracted action preconditions -and hence get explicitly represented
planning graph levels- include (SMALLER ?X ?Y) term MOVE action towers Hanoi domain typing terms (AUTO ?X) (PLACE ?Y) logistics domains.) benefit
compounded EGBG PEGG since propositions removed action preconditions directly reduce size subgoal sets generated regression search episodes,
hence size search trace.
Bi-partite planning graph:
original Graphplan maintains level-by-level action, proposition, mutex information
distinct structures level, thereby duplicating -often many times over- information contained previous levels. multi-level planning graph efficiently represented indexed
two-part structure finite differencing techniques employed focus aspects
graph structure possibly change extension. leads rapid construction
concise planning graph (Fox & Long 1998; Smith & Weld, 1998).
me-EGBG PEGG, bi-partite graph offers benefit beyond reduced memory demands faster graph construction time; PE transposition process described section 3.1 reduced simply incrementing search segments graph level index. straightforward
multi-level graph built Graphplan, since proposition (and action) referenced
search segments unique data structure itself.
Explanation Based Learning Dependency Directed Backtracking:
application explanation based learning (EBL) dependency directed backtracking (DDB)
investigated preliminary way (Zimmerman & Kambhampati, 1999), primary
interest speedup benefits. techniques shown result modest speedups
582

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

several small problems complexity integrating maintenance PE replay
vectors limited size problem could handled. since succeeded implementing
robust version methods, results reported reflect that.
EBL DDB based explaining failures leaf-nodes search tree, propagating explanations upwards search tree (Kambhampati, 1998). DDB involves using propagation failure explanations support intelligent backtracking, EBL involves
storing interior-node failure explanations, pruning future search nodes. approach implements complimentary techniques Graphplan reported (Kambhampati, 2000)
speedups ranged ~2x blocksworld problems ~100x ferry domain problems.
refer study full description EBL/DDB Graphplan context, note aspects particularly relevant me-EGBG PEGG.
conflict directed back-jumping (Prosser, 1993), failure explanations compactly represented terms conflict sets identify specific action/goal assignments gave rise
backtracking. liberates search chronological backtracking, allowing jump back
recent variable taking part conflict set. attempts satisfy set subgoals (a
state) fail, conflict set regressed back represents useful minimal no-good memoization. (See PEGG algorithm Figures 8 9 depiction process.) conflict set
memo usually shorter hence general one generated stored standard
Graphplan. Additionally, EBL-augmented Graphplan generally requires less memory memo
caches.
Less obvious speedup benefit perhaps, role EBL DDB often play dramatically reducing memory footprint pilot explanation. Together EBL DDB shortcut
search process steering away areas search space provably devoid solutions.
Search trace memory demands decrease proportionally.
me-EGBG PEGG outfitted EBL/DDB non-PE directed Graphplan-style search. me-EGBG however, use EBL/DDB replay action assignment results PE search segment due complexity retract parts assignment
vectors whenever conflict set new episode entails new replay order.
Value Variable Ordering:
Value variable ordering also well known speedup methods CSP solvers. context
Graphplans regression search given planning graph level k, variables regressed subgoals values possible actions give propositions level k graph.
original paper, Blum Furst (1997) argue variable value ordering heuristics
particularly useful improving Graphplan, mainly exhaustive search required levels
solution bearing level anyway. Nonetheless, impact dynamic variable ordering
(DVO) Graphplan performance examined (Kambhampati, 2000), modest speedups
achieved using standard CSP technique selecting assignment subgoal (variable)
least number remaining establishers (values). impressive results reported
later study (Nguyen & Kambhampati, 2000) distance-based heuristics rooted planning
graph exploited order subgoals goal establishers. configuration, Graphplan
exhibits speedups ranging 1.3 100x, depending particular heuristic problem.

583

fiZIMMERMAN & KAMBHAMPATI

study fix variable ordering according adjusted sum heuristic value ordering
according set level heuristic, found combination reasonably robust across
range test bed problems. heuristics described Section 5 used
direct traversal PE states discussed. Section 4.1 describes highly problem-dependent
performance distance-based variable value ordering search trace-based planners.
manner EGBG/PEGG builds maintains planning graph search trace
structures actually reduces cost variable value ordering. default order Graphplan considers establishers (values) satisfying proposition (variable) given level set
order appear planning graph structure. graph construction me-EGBG
PEGG set order correspond desired value ordering heuristic, ordering computed once. part, PE constructed search record
heuristically-best ordering regression states goals, variable ordering also done
given state. contrasts versions Graphplan outfitted
variable value ordering (Kambhampati, 2000) ordering reassessed time state
regenerated successive search episodes.

References
Blum, A. & Furst, M.L. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90(1-2).
Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanism
planning. Proceedings AAAI-97.
Bonet, B. & Geffner, H. (1999). Planning heuristic search: New results. Proceedings
ECP-99.
Do, M.B. & Kambhampati, S. (2000). Solving Planning-Graph compiling CSP.
Proceedings AIPS-00.
Fox, M., & Long, D. (1998). automatic inference state invariants TIM. Journal
Artificial Intelligence Research, 9, 317-371.
Frost, D. & Dechter, R. (1994). search best constraint satisfaction search. Proceedings
AAAI-94.
Gerevini , A., & Schubert, L. (1996). Accelerating Partial Order Planners: techniques
effective search control pruning. Journal Artificial Intelligence Research 5, 95-137.
Gerevini, A. & Serina, I., (2002). LPG: planner based local search planning graphs
action costs. Proceedings AIPS-02.
Haslum, P., & Geffner, H. (2000). Admissible Heuristics Optimal Planning. Proceedings.
AIPS-00.
Hoffman, J. (2001) heuristic domain independent planning use enforced hillclimbing algorithm. Technical Report No. 133, Albert Ludwigs University.
Kambhampati, S. (1998). relations Intelligent Backtracking Failure-driven
Explanation Based Learning Constraint Satisfaction Planning. Artificial Intelligence,
105(1-2).
Kambhampati, S. (2000). Planning Graph (dynamic) CSP: Exploiting EBL, DDB
CSP search techniques Graphplan. Journal Artificial Intelligence Research, 12, 1-34.
Kambhampati, S. & Sanchez, R. (2000). Distance-based Goal-ordering heuristics Graphplan.
Proceedings AIPS-00.
584

fiUSING MEMORY TRANSFORM SEARCH PLANNING GRAPH

Kambhampati, S., Parker, E., & Lambrecht, E. (1997). Understanding extending Graphplan.
Proceedings ECP-97.
Kautz, H. & Selman, B. (1996). Pushing envelope:
stochastic search. Proceedings AAAI-96.

Planning, prepositional logic

Kautz, H. & Selman, B. (1999). Unifying SAT-based Graph-based Planning. Proceedings
IJCAI-99, Vol 1.
Koehler, D., Nebel, B., Hoffman, J., & Dimopoulos, Y., (1997). Extending planning graphs
ADL subset. Proceedings ECP-97, 273-285.
Korf, R. (1985). Depth-first iterative-deepening: optimal admissible tree search. Artificial
Intelligence, 27(1), 97-109.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189-211.
Long, D. & Fox, M. (1999). Efficient implementation plan graph STAN. Journal
Artificial Intelligence Research, 10, 87-115.
Mittal, S., & Falkenhainer, B. (1990). Dynamic constraint satisfaction problems. Proceedings
AAAI-90.
McDermott, D. (1999). Using regression graphs control search planning. Artificial
Intelligence, 109(1-2), 111-160.
Nigenda, R., & Kambhampati, S. (2003). AltAltp: Online Parallelization Plans Heuristic
State Search. Journal Artificial Intelligence Research, 19, 631-657.
Nguyen, X. & Kambhampati, S. (2000). Extracting effective admissible state space heuristics
planning graph. Proceedings AAAI-00.
Prosser, P. (1993). Domain filtering degrade intelligent backtracking search. Proceedings
IJCAI-93.
Russell, S.J., (1992). Efficient memory-bounded search methods. Proceedings ECAI 92.
Sen, A.K., & Bagchi, A., (1989). Fast recursive formulations best-first search allow
controlled use memory. Proceedings IJCAI-89.
Selman, B, Levesque, H., & Mitchell, D. (1992). new method solving hard satisfiability
problems. Proceedings AAAI-92.
Smith, D., Weld, D. (1998). Incremental Graphplan. Technical Report 98-09-06. Univ. Wash.
Srivastava, B., Nguyen, X., Kambhampati, S., Do, M., Nambiar, U. Nie, Z., Nigenda, R.,
Zimmerman, T. (2001). AltAlt: Combining Graphplan Heuristic State Search. AI
Magazine, 22(3), American Association Artificial Intelligence, Fall 2001.
Zimmerman, T. (2003). Exploiting memory search high quality plans planning
graph. PhD dissertation, Arizona State University.
Zimmerman, T. & Kambhampati, S. (1999). Exploiting Symmetry Planning-graph via
Explanation-Guided Search. Proceedings AAAI-99.
Zimmerman, T., Kambhampati, S. (2002). Generating parallel plans satisfying multiple criteria
anytime fashion. Proceedings workshop Planning Scheduling Multiple
Criteria, AIPS-02.
Zimmerman, T. & Kambhampati, S. (2003). Using available memory transform Graphplans
search. Poster paper Proceedings IJCAI-03.

585

fiJournal Artificial Intelligence Research 23 (2005) 331 -- 366

Submitted 06/04; published 03/05

Learning Labeled Unlabeled Data: Empirical Study
Across Techniques Domains
Nitesh V. Chawla
Department Computer Science & Engg.,
University Notre Dame,
46556, USA
Grigoris Karakoulas
Department Computer Science
University Toronto
Toronto, Ontario
Canada M5S 1A4

NCHAWLA@CSE.ND.EDU

GRIGORIS@CS.TORONTO.EDU

Abstract
increased interest devising learning techniques combine unlabeled data
labeled data i.e. semi-supervised learning. However, best knowledge, study
performed across various techniques different types amounts labeled
unlabeled data. Moreover, published work semi-supervised learning techniques
assumes labeled unlabeled data come distribution. possible
labeling process associated selection bias distributions data points
labeled unlabeled sets different. correcting bias result biased function
approximation potentially poor performance. paper, present empirical study
various semi-supervised learning techniques variety datasets. attempt answer various
questions effect independence relevance amongst features, effect size
labeled unlabeled sets effect noise. also investigate impact
sample-selection bias semi -supervised learning techniques study implement
bivariate probit technique particularly designed correct bias.

1. Introduction
availability vast amounts data applications made imperative need combine
unsupervised supervised learning (Shahshahni & Landgrebe, 1994; Miller & Uyar, 1997;
Nigam et al., 2000; Seeger, 2000; Ghani et al., 2003). cost assigning labels
data expensive, and/or data might labels due selection
bias. applications include, limited to, text classification, credit scoring, fraud
intrusion detection. underlying challenge formulate learning task uses
labeled unlabeled data generalization learned model improved.
recent years various techniques proposed utilizing unlabeled data (Miller &
Uyar, 1997; Blum & Mitchell, 1998; Goldman & Zhou, 2000; Nigam et al., 2000; Blum & Chawla,
2001; Bennett et al., 2002; Joachims, 2003). However, best knowledge,
empirical study evaluating different techniques across domains data distributions. goal
2005 AI Access Foundation. rights reserved.

fiCHAWLA & KARAKOULAS

paper examine various scenarios dealing labeled unlabeled data conjunction
multiple learning techniques.
Incorporating unlabeled data might always useful, data characteristics and/or
technique particulars might dictate labeled data sufficient even presence
corpus unlabeled data. conflicting experiences reported literature.
one hand, NIPS01 competition semi-supervised learning required contestants achieve
better classification results cases supervised learning (Kremer & Stacey,
2001). implies expectation cases semi-supervised techniques could help.
However, results competition meet expectation. Shahshahni
Landgrebe (1994) note unlabeled data delay occurrence Hughes phenomenon
classif ication performance improved. Zhang Oles (2000), using Fisher
Information criterion, show parametric models unlabeled data always help.
hand, Cozman et al. (2002; 2003), using asymptotic analysis, show unlabeled data
fact decrease classification accuracy. concluded modeling assumptions labeled
data incorrect, unlabeled data would increase classification error, assuming labeled
unlabeled data come distribution. formulate asymptotic
behavior semi-supervised techniques scenarios labeled unlabeled data come
different distributions. addition, evaluate algorithm one real dataset,
varying amount labeled unlabeled data.
aforementioned semi-supervised learning techniques distinguish different
reasons data missing. techniques assume data missing
completely random (MCAR), i.e. P(labeled=1| x, y) = P(labeled=1) class label
assigned labeled=1 given example labeled (Little & Rubin, 1987). case
labeling process function variables occurring feature space. labeled
unlabeled data assumed come distribution.
labeling process function feature vector x class label missing
random (MAR), i.e. P(labeled=1| x, y) = P(labeled=1|x). scenario MAR labeling occur,
example, credit scoring applications since many creditors use quantitative models
approving customers. case class label observed (deterministic) function
g variables x exceeds threshold value, i.e. g(x)c, c constant.
definition MAR follows P(y=1| x, labeled=1) = P(y=1| x, labeled=0)=P(y = 1|x), i.e.
fixed value x distribution observed distribution missing
y. However, due aforementioned screening, conditional distribution x given
labeled unlabeled data, i.e. bias. Thus, modeling techniques aim
learn conditional distributions one needs correct bias incorporating information
unlabeled data.
contrast, labeling depends y, class label missing random (MNAR), i.e.
P(labeled=1| x, y) P(labeled=0| x, y). case sample-selection bias constructing
labeled set corrected (Heckman, 1979). example, sample -selection bias
occur credit scoring applications selection (approval) customers (labels)
performed based human judgment rather deterministic model (Greene, 1998; Crook &
Banasik, 2002; Feelders, 2000). Sample -selection bias also occur datasets marketing
332

fiLearning Labeled Unlabeled Data

campaigns. MNAR scenario labeled unlabeled data come different distributions
associated censoring mechanism, i.e. P(y=1| x, labeled=1) P(y=1| x, labeled=0).
scenario learning labeled data give estimates biased downwards.
selection bias associated labeling process necessary model underlying missing
data mechanism. Shahshahani Landgrebe (1994) also note training sample
representative distribution population unlabeled data might help.
paper present empirical study existing techniques learning
labeled unlabeled data three different missing data mechanisms, i.e. MCAR, MAR
MNAR. best knowledge, effect unlabeled data different missing data
mechanisms previously addressed semi-supervised learning literature. also
introduce two techniques Econometrics, namely reweighting bivariate probit,
semi-supervised learning. try answer various questions important learning
labele unlabeled datasets, as:






process data become labeled vs. unlabeled?
many unlabeled vs. labeled examples dataset?
effect label noise semi-supervised learning techniques?
characteristics feature space dictate successful
combination labeled unlabeled data?
effect sample-selection bias semi-supervised learning methods?

experiments chosen datasets unbalanced class distributions, since
typical real-world applications unlabeled data, information filtering, credit
scoring, customer marketing drug design. datasets, accuracy misleading
metric treats kinds errors, false positives false negatives, equally. Thus, use
AUC, Area Receiver Operating Curve (ROC), performance metric (Swets, 1988;
Bradley, 1987). AUC become popular performance measure learning unbalanced
datasets (Provost & Fawcett, 2000; Chawla et al., 2002).
paper organized follows. Section 2 present overview techniques
evaluated work. techniques mainly applicable MCAR MAR type
data. Section 3 present two techniques purport deal sample -selection bias
MNAR data. Section 4 describe experimental framework Section 5 analyze
results experiments. conclude paper discussion main findings.

2. Semi-Supervised Learning Methods
techniques evaluate learning labeled unlabeled data are: Co-training (Blum &
Mitchell, 1998; Goldman & Zhou, 2000), Reweighting (Crook & Banasik, 2002), ASSEMBLE
(Bennett et al., 2002) Common-component mixture EM (Ghahramani & Jordan, 1994;
Miller & Uyar, 1997). chose variant co-training algorithm proposed Goldman
Zhou (2000) since make assumption redundant independent views data.
datasets considered, "natural" feature split would offer
redundant views. One could try random feature split, in-line
original assumption Blum Mitchell (1998). Essentially, two different algorithms looking
333

fiCHAWLA & KARAKOULAS

data potentially provide better random information process
labeling. construction, co-training ASSEMBLE assume labeled unlabeled data
sampled distribution, namely based MCAR assumption.
re-weighting common-component mixture techniques based assumption data
MAR type. Thus, applied cases conditional distribution x given
labeled unlabeled data. Details techniques given rest
Section.
sample -selection correction MNAR data use Bivariate Probit technique
(Heckman, 1979; Greene, 1998) Sample -Select, adapted version technique used
Zadrozny Elkan (2000). techniques presented Section 3.
worth pointing selected popular algorithms learning
labeled unlabeled data. However, list algorithms study meant
exhaustive. main goal evaluate behavior algorithms
different amounts, distributions, characteristics labeled unlabeled data.
used Nave Bayes algorithm underlying supervised learner
aforementioned semi-supervised learning methods. chose Nave Bayes due popularity
semi-supervised learning framework well need generative model
common-component mixture technique. Thus, choice base learner helped us achieve
consistency comparability experimental framework. co-training
experiments, design require two classifiers, also used C4.5 decision tree release 8
(Quinlan, 1992). Please note rest paper, use Nave Bayes supervised
learner, interchangeably.
next introduce notation used describing various methods paper.
Consider classification problem K classes k , k = 0,1,...,K 1 . assume
training set consists two subsets: X = {X L , X U } ,

X L = {( x1 , y1 ), ( x2 , 2 ),..., ( xl , yl )} labeled subset,
X U = {xl +1 , x l + 2 ,..., xl + u } unlabeled subset,
l u number examples labeled unlabeled sets, respectively,
x n-dimensional feature vector.
2.1 Co-training
Co-training, proposed Blum Mitchell (1998), assumes exist two independent
compatible feature sets views data. is, feature set (or view) defining problem
independently sufficient learning classification purposes. instance, web page
description partitioned two feature subsets: words exist web page words
exist links leading web page. classifier learned redundant
feature subsets used label data thus expand others training set.
informative assigning random labels unlabeled data. However,
real-world application, finding independent redundant feature splits unrealistic,
lead deterioration performance (Nigam & Ghani, 2001).
Goldman Zhou (2000) proposed co-training strategy assume feature
independence redundancy. Instead, learn two different classifiers (using two different
334

fiLearning Labeled Unlabeled Data

supervised learning algorithms) dataset. idea behind strategy since two
algorit hms use different representations hypotheses learn two diverse models
complement labeling unlabeled data enlarging training set
other. Goldman Zhou derive confidence intervals deciding unlabeled examples
classifier label. adopt co-training strategy allows us apply various
real-world datasets thus compare semi-supervised learning techniques.
illustration purposes, let us assume two classifiers B. labels data B,
B labels data unlabeled data left none labeled due lack
enough confidence label. decision label data classifier taken
basis statistical techniques. Following Goldman Zhou (2000) construct confidence
intervals B using 10-fold cross-validation. Let lA , lB , h , h B , lower upper
confidence intervals B, respectively. Additionally, upper lower confidence
intervals class k defined lAk, h Ak, l Bk, h Bk. intervals define confidence
assigned classifiers labeling mechanism. addition, amount data
labeled subject noise control mechanism.
Let us consider co-training round labeling data B (the follow
similarly). Let XL original labeled training set (which B), XLB data
labeled B, wB conservative estimate mislabeling errors XLB , = |XL XLB |
sample size, = wB /|XL XLB | noise rate. relationship sample
size m, classification noise rate , hypothesis error given

m=

k
2 (1 2 )

,

k constant, assumed 1

relationship used decide amount additional data labeled compensate
increase classification noise rate. Thus, based expressions ,
conservative estimate 1/ 2 classifier B, q B , given
q B = X L X LB



1




2wB

2
X L X LB








2

Similarly, compute conservative estimate 1/2 , qk, class k labeled set:

q k = X L X LB X Uk


2( w B + w k )
1
X L X LB X Uk


2


,



| X Uk | = Examples X U mapped class k
w k = (1 l k ) | X Uk |

335

fiCHAWLA & KARAKOULAS

Thus, classifier labeling process consists two tests:
(i)
(ii)

h Ak > lB
q k > qB

first test ensures class k used classifier label data accuracy least
good accuracy classifier B. second test help prevent degradation performance
classifier B due increased noise labels. unlabeled examples satisfy criteria
la beled classifier classifier B vice versa. process repeated
unlabeled examples satisfy criteria either classifier. end co-training procedure
(no unlabeled examples labeled), classifiers learned corresponding final labeled sets
evaluated test set.
experiments used two different classification algorithms Nave Bayes C4.5
decision tree. modified C4.5 decision tree program convert leaf frequencies
probabilities using Laplace correction (Provost & Domingos, 2003). final prediction,
probability estimates Nave Bayes C4.5 averaged.
2.2 ASSEMBLE
ASSEMBLE algorithm (Bennett et al., 2002) NIPS 2001 Unlabeled data competition.
intuition behind algorithm one build ensemble works consistently
unlabeled data maximizing margin function space labeled unlabeled data.
allow margin used labeled unlabeled data, Bennett et al. introduce
concept pseudo-class. pseudo-class unlabeled data point defined
predicted class ensemble point. One variations ASSEMBLE algorithm
based AdaBoost (Freund & Schapire, 1996) adapted case mixture labeled
unlabeled data. used ASSEMBLE.AdaBoost algorithm used authors
NIPS Challenge.
ASSEMBLE.AdaBoost starts procedure assigning pseudo-classes instances
unlabeled set, allows supervised learning procedure algorithm applied. initial
pseudo-classes assigned using 1-nearest-neighbor algorithm, ASSEMBLE-1NN,
simply assigning majority class instance unlabeled datasets, ASSEMBLE-Class0.
treat majority class class 0 minority class class 1 datasets. size
training set boosting iterations size labeled set. Figure 1 shows
pseudo code ASSEMBLE.AdaBoost algorithm.
pseudo code, L signifies labeled set, U signifies unlabeled set, l (size labeled
set) sample size within iteration weight assigned labeled unlabeled data
distribution D0 . initial misclassification costs biased towards labeled data, due
confidence labels. subsequent iterations algorithm, a, indicates relative
weight given type error either labeled data unlabeled data error. pseudo-code,
f indicates supervised learning algorithm, Nave Bayes experiments. indicates
number iterations, f (xi ) = 1 instance xi correctly classified f (xi ) = -1,
incorrectly classified; error computed iteration t, Dt sampling
distribution.
336

fiLearning Labeled Unlabeled Data

1. l :=| L | u :=| U |
/ l
2. D1 ( i) :=
(1 ) / l

L

U

3. yi := c, c class 1NN U class 0 U,
original class L
4. f 1 = NaiveBayes (L + U)
5. := 1
6. Let := f (x ), = 1,..., L + U

7. = Dt [yi i], = 1,..., L + U


8. > 0.5 Stop
1 -
9. w = 0.5 * log


10. Let Ft := Ft -1 + w f
11. Let = Ft (x ) U
12. Dt +1 =

e yi Ft +1 ( xi )
e

yi Ft +1 ( xi )





13. = Sample(L + U, Y, Dt + 1)
14. f +1 = NaiveBayes (S)
15. end
16. return Ft+1
Figure 1: ASSEMBLE.AdaBoost pseudo-code
2.3 Re-weighting
Re-weighting (Crook & Banasik, 2002) popular simple technique reject-inferencing
credit scoring, considered semi-supervised learning technique. uses
information examples approved credit applications, i.e. labeled data, infer
conditional class probabilities rejected applications, i.e. unlabeled data.
apply re-weighting one assumes unlabeled data MAR, i.e.

P(y = 1 | x,labeled = 1) = P(y = 1 | x,labeled = 0) = P(y = 1 | x)
is, x, distribution labeled cases label distribution
missing (in unlabeled population).
several variants re-weighting. According one adopt here, also called
extrapolation, goal first estimate distribution classes la beled data within
337

fiCHAWLA & KARAKOULAS

score group, extrapolate distribution unlabeled data. Thus, model learned
labeled training set applied unlabeled data. labeled unlabeled data
grouped score, i.e. posterior class probability model. probabilities
percentile -binned, bin forms score group. unlabeled data assigned labels based
distribution classes corresponding score group labeled data. Thus, key
assumption corresponding score-bands labeled unlabeled data
distribution classes. formulated as:

P( k | j , X L ) = P( yk | j , X U )
kjL / X Lj = Ukj / X Uj
j score band group j, kjL number labeled examples belonging class yk
group j, U
kj proportion unlabeled examples could class k score group j.
number labeled cases j weighted (|XLj +XUj |)/|XLj |, probability sampling
weight.
explain re-weighting concept, let us consider example given Table 1. Let 0 1
two classes datasets. score group (0.6 - 0.7) bin posterior class
probabilities model. group weight example Table 1 computed
(XL+XU )/XL = 1.2. Therefore, weighting number class0 class1 group, get class0
= 12; class1 = 108. Thus, label random 2 (=12-10) data points unlabeled set
class0, 18 (=108-90) data points unlabeled set class1.
labeling unlabeled examples learn new model expanded training set
(inclusive labeled unlabeled data).
Score Group
0.6 - 0.7

Unlabeled
20

Labeled

Class 0

Class 1

100

10

90

Table 1: Re-weighting example

2.4 Common Components Using EM
idea behind technique eliminate bias estimation generative model
labeled/unlabeled data MAR type. reason bias case data
distributions x given labeled unlabeled sets different. remove bias
estimating generative model labeled unlabeled data modeling missing
labels hidden variable within mixture model framework.
particular, suppose data generated components (clusters),
components well modeled densities p ( x | j , j ) , j = 1,2,..., , j denoting
corresponding parameter vector. feature vectors generated according
density:


p ( x | ) = j p ( x | j , j )
j =1

338

fiLearning Labeled Unlabeled Data

j p( j ) mixing proportions sum one.
joint data log-likelihood incorporates labeled unlabeled data takes form:

L( ) =





( xi , yi ) X l

log j p( xi | j , j ) p ( yi | xi , j , j ) +
j =1



xi X u



log j p ( xi | j , j )
j =1

Note likelihood function contains "supervised" term, derived X l labeled
data, "unsupervised" term, based X u unlabeled data.
Consider applying following simplistic assumption: posterior probability class
label conditionally independent feature vector given mixture component, i.e.
p ( | xi , j , j ) = p( yi | j ) . type model makes assumption class conditional
densities function common component (CC) mixture (Ghahramani & Jordan, 1994;
Miller & Uyar, 1997). general, interested applying CC mixture model solve
classification problem. Therefore need compute posterior probability class label
given observation feature vector. posterior takes form:

p ( | xi , ) =



p ( j | xi , ) p ( | xi , j, j ) =
j =1

p( x | j , )
j

j
p( | j )



p
(
x
|
l
,

)
j =1 l =1 l

l


One could also consider applying separate component (SC) mixture model, class
conditional density modeled mixture components. model essentially presented
Nigam et al. (2000) case labeled/unlabeled data document classification.
models, well powerful ones condition input feature vector, united
mixture-of-experts framework (Jacobs et al., 1991). application framework
semi-supervised setting studied Karakoulas Salakhutdinov (2003).

2.4.1 Model Training
Assume p ( x | j , j ) parameterized Gaussian mixture component distribution
continuous data. define parameter vector component, j, j = ( j , j )
j = 1, 2,..., ; j mean, j covariance matrix jth component.
experiments constrain covariance matrix diagonal. also two
model parameters estimated: mixing proportion components, j = p ( j ) ,
| j = p ( | j ) posterior class probability.

According general mixture model assumptions, hidden variable directly related
data generated mixture components. Due labeled/unlabeled nature
data, additional hidden variable missing class labels unlabeled data.
hidden variables introduced log-likelihood L() .
general method maximum likelihood estimation model parameters presence
hidden variables Expectation-Maximization (EM) algorithm (Dempster et al., 1977).
EM algorithm alternates estimating expectations hidden variables (incomplete data)
given current model parameters refitting model parameters given estimated,
complete data. derive fixed-point EM iterations updating model parameters.
339

fiCHAWLA & KARAKOULAS

mixture component, j, feature vector, i, compute expected
responsibility component via one following two equations, depending whether
ith vector belongs labeled unlabeled set (E-Step):

tj yi | j p( xi | j , tj )

p ( j | xi , yi , ) =




lt |l p (x | l , lt )


l =1

p ( j | xi , ) =


tj p ( x | j , tj )



l =1

lt

xi X L

xi X U

p ( x | l , lt )

Given equations solution model parameters takes form (M-Step):


1


p
(
j
|
x
,

,

)
+
p
(
j
|
x
,

)





N ( x , )X
xi X u
l


tj+1 =

+1
k| j



p ( j | xi , , )

=

xi X l = k

p ( j | x , , )

( xi , yi )X l

tj +1 =







p
(
j
|
x
,

,

)
x
+
p
(
j
|
x
,

)
x






+1
N j ( xi , yi )X l
xi X u


tj+1 =








p
(
j
|
x
,

,

)

+
p
(
j
|
x
,

)




ij

ij
N tj+1 ( xi , yi )X l
xi X u


1

1

define N =| X l | + | X u | , Sij ( xi j )( xi j ) .




Iterating E-step M-step guarantees increase likelihood function.
discrete-valued data, instead using mixture Gaussians, apply mixture multinomials
(Kontkanen et al., 1996).
general, number components tuned via cross-validation. However, large
number experiments reported work makes application tuning across
experiments impractical, computational overhead would entail. Furthermore,
purpose paper provide insight learning labeled unlabeled data
different techniques, rather find technique performs best. reason
report results experiments using two, six, twelve twenty-four components.
340

fiLearning Labeled Unlabeled Data

discuss point Section 4.

3. Dealing Sample Selection Bias
censoring mechanism rules assignment label instances data
model learned labeled data sample selection bias. case decision
labeling due unobserved features create dependency assigning label
class label itself. Thus data MNAR type. example, suppose interested
assigning relevance text corpus, e.g., web pages, someone subjectively decided
documents label. estimates probability relevance underestimated,
training set might representative complete population. result
increase estimation bias model, thus increasing error.
present two techniques, previously proposed literature, dealing MNAR data.
3.1 Bivariate Probit
Heckman (1979) first studied sample -selection bias modeling labor supply field
Econometrics. Heckman's sample selection model canonical form consists linear
regression model binary probit selection model. probit model variant regression
model latent variable normally distributed. Heckman's method two-step
estimation process, probit model selection equation regression model
observation equation. selection equation represents parameters classification
labeled unlabeled data namely purpose explicitly model censoring mechanism
correct bias observation equation represents actual values (to regress on)
labeled data. regression model computed cases satisfy selection
equation therefore observed. Based probit parameters correct
selection bias, linear regression model developed labeled cases.
following equations present regression probit models. probit model
estimated y2 . dependent variable y2 represents whether data labeled not; y2 = 1
data labeled, y2 = 0 data unlabeled. equation y1 , variable interest,
regression equation. dependent variable y1 known label value labeled data.
latent variables u 1 u 2 assumed bivariate normally distributed correlation .
observation equation selected cases (the cases satisfy y2 >0). Let us
denote y1 * observed values y1 . have:

y1 = ' x1 + u1
2 = ' x 2 + u2
2

u1 , u2 N [ 0,0, u1 , ]
y1 = y1* , 2

>0

y1 missing y2 0
estimate unbiased latent variables u 1 u2 uncorrelated,
341

fiCHAWLA & KARAKOULAS

case data MNAR. taking account bias results. bias
regression depends sign (biased upwards positive, downwards
negative). amount bias depends magnitude . crux Heckmans
method estimate conditional expectation u 1 given u 2 use additional variable
regression equation y1 . Let us denote univariate normal density CDF u 1
, respectively. Then,

E[ y1* | x1 , 2 > 0] = ' x1 + u
1
( x2 ' )
=
( x2 ' )
also called Inverse Mills Ratio (IMR), calculated parameter estimates.
Heckman's canonical form requires one models regression based. However,
paper deal semi-supervised classification problems. problems class assigned
labeled data, y1 = 0 1, y2 = 1. Given two equations Heckmans method,
labeled data observed y2 = 1. Thus, interested E[y1 |x, 2 = 1] P(y1 |x, 2 = 1).
example, predict whether someone default loan (bad customer), model needs
learnt accepted cases. Thus, scenarios one use bivariate probit model
two outcomes, default accept (Crook & Banasik, 2002; Greene, 1998). bivariate probit
model written follows:

y1 = x1 +u1
2 = x 2 +u 2
y1 = y1* 2 = 1
Cov(u1 , u 2 ) =
= 0 sample -selection bias associated datasets, latent
variables correlated using single probit model sufficient. log-likelihood
bivariate model written (where 2 bivariate normal CDF univariate
normal CDF).

Log ( L ) =

2 =1, y1 =1

log F 2 [ 1 x1 , 2 x 2 , ]

+

2 =1, y1 =1

log F 2 [ 1 x 1 , 2 x 2 , ]

2=0 log F [ 2 x 2 ]
apply bivariate probit model semi-supervised learning framework introducing
selection bias various datasets.
3.2 Sample -Select
Zadrozny Elkan (2000) applied sample -selection model KDDCup-98 donors' dataset
assign mailing costs potential donors. sample sele ction bias one tries predict
donation amount donors data. Using Nave Bayes probabilistic version C4.5,
342

fiLearning Labeled Unlabeled Data

computed probabilistic estimates membership person ''donate'' ''not donate''
category, i.e. P(labeled=1|x). imputed membership probability additional
feature linear regression model predicts amount donation. Zadrozny Elkan
cast problem semi-supervised learning one. adapted method
semi-supervised classification setting follows:

= P( labeled = 1 | x, L U )
n +1
P( = 1 |x x
, L)
n +1
x

L labeled training set, U unlabeled set, labeled=1 denotes instance labeled,
labeled = 0 denotes instance unlabeled, label assigned instance
labeled set. new training set labeled unlabeled datasets (L U ) constructed, wherein
class label 1 assigned labeled data class label 0 assigned unlabeled data. Thus,
probabilistic mode l learned classify data labeled unlabeled data. probabilities,
P(labeled = 1|x), assigned labeled data included another feature labeled data
only. Then, classifier learned modified labeled data (with additional feature).
classifier learns "actual" labels existing labeled data.
experiments, learn Nave Bayes classifier L U, impute posterior
probabilities another feature L, relearn Nave Bayes classifier L. rest
paper call method Sample-Select.
apply Sample -Select method along semi-supervised techniques,
apply bivariate probit model datasets particularly constructed
sample-selection bias. Sample -Select appropriate MAR case rather
MNAR case due limiting assumption makes. specifically, MNAR case

P(labeled,y|x,,)=P(y|x,labeled,)*P(labeled|x,)
However, since Sample -Select assumes
P(y|x,labeled=0,)= P(y|x,labeled=1,)
reduces problem MAR.

4. Experimental Set-up
goal experiments investigate following questions using aforementioned
techniques variety datasets:
(i)
(ii)
(iii)
(iv)
(v)

effect independence dependence among features?
much unlabeled vs. labeled data help?
effect label noise semi-supervised techniques?
effect sample -selection bias semi-supervised techniques?
semi-supervised learning always provide improvement supervised
learning?

questions designed series experiments real-world artificial
343

fiCHAWLA & KARAKOULAS

datasets. latter datasets used providing insights effects unlabeled data within
controlled experimental framework. Since datasets come fixed
labeled/unlabeled split, randomly divided ten splits built ten models
technique. reported results average performance ten models technique.
Details splitting given Section 4.1. datasets came pre-defined test
set. used test sets evaluate performance model. definition AUC
performance measure given Section 4.2.
mentioned earlier, purpose work provide insight learning labeled
unlabeled data different techniques, rather find technique performs
best. latter would required fine-tuning technique. However, understand
sensitivity various techniques respect key parameters, run experiments
different parameter settings. Thus, common-component technique, tried 2, 6, 12, 24
components; ASSEMBLE-1NN ASSEMBLE-Class0 tried 0.4, 0.7 1.0 values
, parameter controls misclassification cost; co-training tried 90%, 95%
99% confidence intervals deciding label. Appendix (Tables 6 9) presents
results four techniques, respectively. common-component technique 6
components gave consistently good performance across datasets. found marginal
difference performance co-training different confidence intervals. found small
sensitivity value lower amounts labeled data, labeled data increases
differences values various datasets becomes smaller. Moreover, datasets
exhibited sensitivity value a. observations agree Bennett et al. (2002) choice
might critical cost function.
Thus, analyzing questions set parameters techniques follows: 6
components common-component mixture, = 1 ASSEMBLE-1NN
ASSEMBLE-Class0, confidence 95% co-training.
4.1 Datasets
evaluated methods discussed Sections 2 3 various datasets, artificial
real-world, order answer questions. Table 2 summarizes datasets.
datasets binary classes unbalanced class distributions. feature makes
datasets relevant real-world applications well difficult
semi-supervised techniques, since techniques inductive bias towards majority
class. study effects feature relevance noise constructed 5 artificial datasets
different feature characteristics presence noise. modified artificial data generation
code provided Isabelle Guyon NIPS 2001 Workshop Variable Feature Selection
(Guyon, 2001). convention used datasets is: A_B_C_D,
indicates percentage independent features,
B indicates percentage relevant independent features,
C indicates percentage noise datasets,
indicates percentage positive (or minority) class datasets.
independent features artificial datasets drawn N(0,1). Random noise added
344

fiLearning Labeled Unlabeled Data

features N(0,0.1). features rescaled shifted randomly. relevant
features centered rescaled standard deviation 1. class labels assigned
according linear classification random weight vector, drawn N(0,1), using
useful features, centered mean. mislabeling noise, specified, added
randomly changing labels datasets. naming artificial datasets Table 2
self-explanatory using convention outlined earlier.
Datasets

Training Size
|L+U|

Testing
Size

Class-distribution
(majority:minority)

Features

30_30_00_05

8000

4000

95:5

30

30_80_00_05

8000

4000

95:5

30

80_30_00_05

8000

4000

95:5

30

80_80_00_05

8000

4000

95:5

30

30_80_05_05

8000

4000

95:5

30

30_80_10_05

8000

4000

95:5

30

30_80_20_05

8000

4000

95:5

30

SATIMAGE (UCI)

4435

2000

90.64:9.36

36

WAVEFORM (UCI)

3330

1666

67:33

21

ADULT (UCI)

32560

16281

76:24

14

2710

71.7:28.3

12

800

61.75:38.25

25

0.6:99.4

200

NEURON (NIPS)

L

530 +
2130 U

HORSE-SHOE (NIPS)

400 L +
400 U

KDDCUP-98 (UCI)

4843 L +

96367

90569 U
Table 2: Datasets details

Three real-world datasets waveform, satimage, adult UCI repository
(Blake et al., 1999). transformed original six-class satimage datasets binary class
problem taking class 4 goal class (y=1), since least prevalent one (9.73%),
merging remaining classes single one. Two datasets come NIPS 2001
(Kremer & Stacey, 2001) competition semi-supervised learning pre-defined
labeled/unlabeled subsets.
randomly divided ten times artificial UCI datasets five different
labeled-unlabeled partitions: (1,99)%, (10,90)%, (33,67)%, (67,33)%, (90,10)%. provided us
diverse test bed varying labeled unlabeled amounts. report results averaged
345

fiCHAWLA & KARAKOULAS

ten random runs.
last real-world dataset KDDCup-98 Cup (Heittich & Bay, 1999; Zadrozny &
Elkan, 2000). typical case sample selection bias, i.e. MNAR. converted original
KDDCup-98 donors regression problem classification problem consistent
semi-supervised classification setting experiments. considered respondents
donation mailing campaign labeled set non-respondents unlabeled set.
transformed labeled set classification problem, assigning label 1
individuals make donation greater $2, label 0 individuals
make donation less $2. gave us difficult dataset since due sample
selection bias built dataset labeled unlabeled sets markedly different class
distributions. positive class training set 99.5%, testing set 5%.
also created biased version Adult dataset (30_80_00_05) artificial dataset
MAR labels order study effect missing label mechanism techniques.
specifically, biased Adult dataset constructed labeled unlabeled partitions
training set conditioning education individual. Using censoring
mechanism individuals without post high-school education put unlabeled
set, rest labeled set. Thus, converted original classification problem
problem return wages adults post high-school education. (30_80_00_05)
dataset divided labeled unlabeled sets conditioning two features that:
(xin <= ci || x jn <= cj ), n U. introduced selection bias construction labeled
unlabeled sets. datasets test set changed. discuss three
biased datasets Section 5.3.
4.2 Learning Performance Measurement
experiments use Nave Bayes algorithm base learner.
co-training experiments use C4.5 decision tree learner addition Nave Bayes. apply
Nave Bayes algorithm, pre-discretize continuous features using Fayyad Irani's
entropy heuristic (Fayyad & Irani, 1993).
use Area Receiver Operating Characteristic (ROC) curve (AUC)
performance metric experiments (Hand, 1997). Due imbalanced class distribution
datasets studied paper chose use AUC standard classification accuracy
metric, classification accuracy misleading unbalanced datasets. compare
performance model random model define AUC

AUC := 2 * auc 1
auc original area AUC normalized one. Thus, formulation, AUC
normalized respect diagonal line ROC space represents random performance.
ROC curve model diagonal line, AUC worse random hence
negative.

346

fiLearning Labeled Unlabeled Data

5. Empirical Analysis
following present analysis results experiments. structure
analysis around questions stated Section 4. Figures 2 6 show AUCs various
datasets. Figures show performance semi-supervised technique,
(supervised) Nave Bayes classifier. x-axis shows percentage labeled/unlabeled data
dataset (Labeled, Unlabeled)%, y-axis shows AUC applying
semi-supervised supervised techniques. datasets come
pre-defined labeled/unlabeled split average AUC 10 random (labeled/unlabeled split)
runs. present results charts using following convention order techniques:








Supervised: Supervised Nave Bayes
ASS1NN: ASSEMBLE-1NN
ASSCLS0: ASSEMBLE-Class0
Samp-Sel: Sample-Select
Reweight: Reweighting
Co-train: Co-training
CC: Common-Component Mixture

5.1 Effect Independence Dependence Among Features?
first investigate effect feature independence semi-supervised learning.
purpose use artificial datasets. consider first four artificial datasets Table 2
noise. Since datasets constructed varying amounts feature
independence relevance, offer diverse test-bed. results shown Figure 2.
results averaged 10 different runs (labeled, unlabeled)% splits.
two graphs left Figure 2 show effect increasing amount feature
independence, whereas two graphs right show effect increasing amount
feature relevance amongst independent features. worth pointing amount
feature independence increased without increasing amount feature relevance (top-left
bottom-left) adding unlabeled data hurts performance. amount independence
relevance increases (bottom-right graph) semi-supervised learning techniques improve
performance supervised learner different amounts labeled/unlabeled data.
case Naive Bayes model underlying semi-supervised techniques better
approximation true generative model.
semi-supervised techniques either comparable better learning
classifier labeled datasets. largest improvement average AUC observed
(1,99)% case across four combinations feature independence relevance, one would
expect. However, (some of) results statistically significant (random)
labeled unlabeled splits, labeled set contained one two examples class 1, thus
making highly skewed training set. cases, performance worse random.
led high variance AUC, causing confidence intervals overlap. lowest gain
observed using semi-supervised learning techniques learning supervised Nave Bayes
classifier case artificial datasets 30% independent 30% relevant
347

fiCHAWLA & KARAKOULAS

30_30_00_05

30_80_00_05

1

1
0.8

0.8

0.6
0.6
0.4
0.4
AUC

AUC

0.2

0.2

0
-0.2

0
-0.4
-0.2

-0.6

-0.4

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

(1,99)

Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweight
Co-train
CC

80_30_00_05
1

1

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2
AUC

AUC

-0.8

0

-0.2

-0.4

-0.4

-0.6

-0.6

-0.8

-0.8
(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

80_80_00_05

0

-0.2

-1

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

-1

(90,10)

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

Figure 2 : Artificial datasets without noise. Effect different amounts feature independence (top bottom)
relevance (left right) different percentages labeled unlabeled data.
348

fiLearning Labeled Unlabeled Data

features. Naive Bayes model achieves relatively high AUC even 1/3 rd
instances labeled set, addition unlabeled data help much.
Regarding specific techniques, re-weighting consistently lower learning Nave Bayes
classifier labeled set only. re-weighting, labels assigned unlabeled data
function P(y|x). smaller amounts labeled data function could overfitting given
rich feature space. also evident much lower performance Naive Bayes model
testing set. Sample -Select perform better supervised learner even
(1,99)% case.
higher amounts independence relevance, ASSEMBLE-1NN ASSEMBLE-Class0
(almost) always better supervised technique. Co-training also sensitive amount
feature relevance independence. higher amounts independence relevance,
co-training becomes sensitive amount labeled unlabeled data mixture.
common-component mixture approach provides large improvement average AUC
across experiments amount labeled data small, i.e. (1,99)%, one would
expect. common-component mixture model seems sensitive amount
independent features (top vs. bottom graphs Figure 2) semi-supervised techniques
considered paper. probably features correlated
might useful clustering. goal clustering group feature vectors
cohesive distinct (Fisher, 1987). Thus, correlated features likely relevant
clustering independent features. fact, Talavera (2000) proposes feature-dependency
measure based Fisher's results selecting features clustering. Thus, decrease
performance common-component mixture technique (top vs. bottom graphs Figure 2)
attributed increase number independent features.
5.2 Much Unlabeled vs. Labeled Data Help?
discuss trade -off labeled unlabeled data, present results UCI
NIPS competition datasets. Figure 3 shows result three UCI datasets satimage,
waveform, adult. evident graphs that, artificial datasets, maximal
gain semi-supervised learning supervised learning occurs smaller amounts labeled
data. lower amounts labeled data, common-mixture model gives biggest gain
Naive Bayes amongst semi-supervised learning techniques. Also, common component
relatively stable performance amount labeled data increased. note
waveform dataset helped unlabeled data even (90,10)% split, using
ASSEMBLE-1NN, ASSEMBLE-Class0 common component. adult dataset,
semi-supervised techniques help amount labeled/unlabeled split. general,
addition labeled data helped co-training, re-weighting, Sample -Select. Co-training noted
largest improvement compared techniques increased (90,10)% split, namely
labeled data.
Figure 4 shows results two NIPS datasets neuron horseshoe. neuron
dataset, ASSEMBLE-1NN provides marginal improvement Nave Bayes. Co-training
technique helps horseshoe dataset. worth pointing horseshoe (50,
50)% split labeled unlabeled data.
349

fiCHAWLA & KARAKOULAS

Satimage

Waveform

0.9

1

0.8

0.8

0.7
0.6
0.6
0.4
AUC

AUC

0.5
0.4

0.2

0.3
0
0.2
-0.2

0.1
0

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

-0.4

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

Adult
0.9
0.8
0.7



Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweight
Co-train
CC

0.6

AUC

0.5
0.4
0.3
0.2
0.1
0

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

Figure 3: Effect different percentages labeled unlabeled data three UCI datasets.

350

(90,10)

fiLearning Labeled Unlabeled Data

0.7

Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweight
Co-train
CC

0.6

0.5

AUC

0.4

0.3

0.2

0.1

0

Neuron

Horse-shoe
Data set

Figure 4: Performance comparison two datasets NIPS semi -supervised learning
competition.

Thus, amount labeled unlabeled data much domain dependent. consistent
observation drawn experiments. Nevertheless, average
semi-supervised techniques common-component ASSEMBLE-1NN show improvement
supervised learner datasets, particularly (1,99)% split.
prior work states unlabeled data help small amount
labeled data (Nigam et al., 2000; Cozman et al., 2003). However, observe even
small amount labeled data, depending technique unlabeled data might help (see
adult Figure 3). even large amount labeled data unlabeled data help (see
waveform Figure 3).
5.3 Effect Label Noise Semi-supervised Techniques?
considered question label noise could detrimental learning. Therefore
initial label estimates semi-supervised learning technique based labeled data
accurate adding unlabeled data might provide minimal gain, any. understand
sensitivity semi-supervised techniques amount noise, added 5%, 10%, 20%
mislabeling noise 30_80_*_05 artificial datasets, * denotes level mislabeling
noise.
Figure 5 shows effect noise semi-supervised techniques. before, unlabeled
data help amount labeled data relatively small, i.e. (1,99)%. again,
common-component mixture ASSEMBLE-1NN ones exhibiting biggest
improvement Naive Bayes, particularly (1,99)%. (10,90)%, ASSEMBLE-Class0 also
improves Nave Bayes. observed across noise levels. worth pointing
datasets 20% mislabeling noise adding unlabeled data improves performance even

351

fiCHAWLA & KARAKOULAS

30_80_05_05

30_80_10_05

1

0.8

0.8

0.6

0.6

0.4

0.4
0.2
AUC

AUC

0.2
0

0
-0.2

-0.2
-0.4

-0.4

-0.6

-0.6
-0.8

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

-0.8

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

30_80_20_05
0.6
0.4
Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweight
Co-train
CC

0.2

AUC

0
-0.2
-0.4
-0.6
-0.8
-1

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

Figure 5: Effect noise artificial datasets varying percentages labeled unlabeled data.

352

(90,10)

fiLearning Labeled Unlabeled Data

(10,90)% labeled/unlabeled mix. contrast one would expect since mislabeling
noise could caused model learned labeled data deviate underlying
distribution. Hence, could eliminated potential improvement adding unlabeled
data. improvement probably supervised learner overfitting (1,99)%
(10,90)% datasets 20% mislabeling noise, due high level noise small amount
labeled data. overfitting occur lower levels noise. Thus, two datasets
unlabeled data acting regularizer preventing semi-supervised learner
overfitting.
5.4 Effect Sample-selection Bias Semi-supervised Techniques?
used three datasets sample selection bias adult, 30_80_00_05, KDDCup-98.
Section 4.1 detailed procedure introducing MAR label bias first two datasets.
KDDCup-98 dataset comes built-in MNAR bias. Table 3 summarizes sizes class
distributions training set three datasets. worth noting different class
distributions labeled unlabeled partitions training set.
Datasets

Training Size (Labeled;
Unlabeled)

Class Distribution (Labeled;
Unlabeled)

Adult

(17807; 14754)

(66.75, 33.25; 87, 13)

30_80_00_05

(2033; 5967)

(88.15, 11.85; 97.33, 2.67)

KDDCup-98

(4843; 90569)

(0.5, 99.5; 100, 0)

Table 3: Composition training set biased datasets

Table 4 shows effect sample selection bias feature distributions adult
30_80_00_05 datasets. specifically, compared feature distributions
labeled unlabeled sets two datasets two different missing data scenarios:
biased random. used Kolmogorov-Smirnov statistical test continuous features
Chi-squared statistical test nominal features. results show that, expected, MCAR
data statistically significant difference feature distributions labeled
unlabeled sets. contrast differences feature distributions two sets
bias missing data mechanism.

Adult
30_80_00_05

MAR MNAR data
13 14 different
20 30 different

MCAR data
0 different
0 different

Table 4: Differences feature distributions labeled unlabeled data different missing data
mechanisms.

353

fiCHAWLA & KARAKOULAS

Figure 6 shows supervised learner (Nave Bayes) well semi-supervised
techniques perform reasonably good level two MAR datasets, perform poorly
MNAR dataset. particular, co-training ASSEMBLE-1NN, moderately well
assumption MCAR, exhibit significantly worse performance. fact, ASSEMBLE-Class0
much better ASSEMBLE-1NN 30_80_00_05 dataset. due selection
bias labeled unlabeled sets different feature distributions. Thus, 1NN weaker
classification model initialization class labels ASSEMBLE dataset biased.
1.2
Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweigh
Co-train
CC
Probit
Bivar Probit

1

0.8

AUC

0.6

0.4

0.2

0

-0.2

30-30-00-05

Adult
Dataset

KDDCup-98

Figure 6: AUC performance biased datasets.

Re-weighting better supervised classifier adult 30_80_00_05 MAR
datasets fails KDDCup-98 MNAR dataset. re-weighting suitable
MNAR datasets since randomly assigns labels based biased posterior probability
distribution. Sample -Select performance comparable supervised classifier. Thus,
exhibits poor performance KDDCup-98 MNAR dataset. explained Section 3.2,
Sample-Select appropriate MNAR datasets. common-component mixture algorithm
seem sensitive sample selection bias, even MNAR datasets.
common-component technique better semi-supervised techniques considered
paper presence bias. Although common-component technique scores less
supervised classifier 30_80_00_05 dataset, performance consistent
observed without sample selection bias. exception bivariate probit
common-components technique best performance amongst semi-supervised techniques
across three datasets. supervised Nave Bayes learner significantly affected
introduced selection bias, performance decreases 0.89 0.82 (approximately)
corresponding amounts labeled unlabeled data 30_80_00_05 dataset.
similar effect bias adult dataset.
354

fiLearning Labeled Unlabeled Data

noteworthy improvement provided bivariate probit model probit
model three datasets. bivariate probit successfully corrects sample selection bias
training set. provides evidence using biased dataset prediction
general population detrimental classification performance, especially case
MNAR dataset. Part reason probit bivariate probit perform well artificial
datasets versus techniques assumption log-normal feature distribution
satisfied data. However, perform well Adult dataset since consists
nominal real-valued features. KDDCup-98 datasets, bivariate model best
performance amongst techniques biggest improvement probit degree
selection bias datasets. dataset comes selection bias built in,
true MNAR case. two datasets MAR bias.
Based results believe presence sample -selection bias availability
unlabeled data would help since better representation population could
induced. course, choosing appropriate learning technique data, like bivariate
probit even common-component mixture, also critical.
5.5 Semi-Supervised Learning Always Provide Improvement Supervised
Learning?
Figure 7 summarizes performance semi-supervised technique across datasets.
y-axis average AUC (over 10 runs) datasets partitioned labeled
unlabeled sets 10 random times, datasets AUC obtained available
labeled-unlabeled split biased labeled-unlabeled split. bar box-plot shows
median performance across datasets labeled-unlabeled partition. observed
figure, ASSEMBLE-1NN common components never worse random. Although,
median bar common component lower median bar supervised model, box
compact all. highlights relatively less sensitivity common components
size labeled unlabeled data. average, techniques usually better
learning labeled data one observe distribution outliers.
Table 5 contains statistical significance comparisons various
semi-supervised learning methods supervised learning technique (Nave Bayes). derive
grouped AUCs obtained 10 different random runs technique across
datasets. Thus, 100 observation points (10 datasets times 10 random runs)
technique. Since one observation point NIPS biased datasets,
include analysis. Using Kruskal-Wallis statistical test applied multiple
comparison procedure comparing different techniques. Kruskal-Wallis non-parametric
one-way ANOVA test assume values coming normal distribution.
test performs analysis variance based ranks different values
means. report Wins-Losses (W-T-L) counts semi-supervised learning technique
supervised learning method. 1% labeled data, ASSEMBLE-1NN
common-components mixture wins ties 1 0 loss. common-component
model well larger amounts labeled data, smaller amounts labeled

355

fiCHAWLA & KARAKOULAS

data prevalent scenario seems help. general semi-supervised techniques tie
supervised one.

1
0.8
0.6
0.4
0.2

U
C

0
-0.2
-0.4
-0.6
-0.8
-1
Supervised ASS-1NN

ASS-CLS0

CoTrain
Technique

Reweigh

Sample-Sel

CC

Figure 7: Box plots summarizing AUC's obtained various techniques across datasets. y-axis indicates
AUC spread, x-axis indicates corresponding method. box lines lower quartile, median
(red line) upper quartile. + symbol denotes outliers.

(Labeled, Unlabeled)%
(1,99)

(10,90)

(33,67)

(67,33)

(90,10)

Method

W-T-L

W-T-L

W-T-L

W-T-L

W-T-L

Assemble -1NN

5-4-1

1-5-4

1-7-2

1-7-2

2-7-1

Assemble -Class0

0-9-1

2-7-1

2-7-1

1-6-3

2-6-2

Re-weighting

0-10-0

0-7-3

1-4-5

0-6-4

0-9-1

Sample-Select

0-10-0

0-10-0

0-10-0

0-10-0

1-9-0

Co-training

0-10-0

0-10-0

0-10-0

2-8-0

4-6-0

Common Component

6-4-0

2-4-4

1-1-8

1-2-7

1-4-5

Table 5: Win-Tie-Loss (W-T-L) tally various techniques versus supervised learning varying
proportions labeled unlabeled data.

356

fiLearning Labeled Unlabeled Data

6. Conclusions
presented various existing techniques machine learning econometrics learning
labeled unlabeled data. introduced concept sample -selection bias
prevalent real-world labeled sets potentially detrimental supervised learning.
reason also introduced bias correction technique econometrics bivariate probit.
empirically evaluated technique set five main objectives (hypotheses)
collection artificial real-world datasets. dataset varied proportion labeled
unlabeled data compared performance technique performance
supervised classifier. tried include datasets various domains different
characteristics order make study generalizable possible. observed, different
characteristics data favorable different kinds semi-supervised learning frameworks.
Moreover, one may need perform parameter tuning optimizing performance specific
technique since optimization focus paper.
main conclusions five objectives enlisted follows:
effect independence dependence amongst features?
investigated objective using artificial datasets. observed fewer
independent relevant features (30_*) higher amounts labeled data,
semi-supervised techniques generally provide much improvement supervised
learning method. Given 3 4 10 independent features relevant, fewer
labeled examples usually sufficient learning classifier. amount feature
independence increased without increasing amount relevant features adding unlabeled
data hurts performance. amount independence relevance increases
techniques, particularly ASSEMBLE-1NN ASSEMBLE-Class0, improve performance
supervised technique. common-component mixture technique seems
sensitive (less effective) amount independent features increases
semi-supervised techniques. correlated features relevant
clustering.
much unlabeled vs. labeled data help?
objective used artificial real-world datasets. observe
consistent pattern lower amounts labeled data, unlabeled data would always help
regardless technique domain. saw even small amount labeled
data unlabeled data might help depending technique domain. Moreover, even
large amount labeled data, unlabeled data still help (for example, see
Waveform). average common-component mixture ASSEMBLE-1NN helped
(1,99)% labeled/unlabeled mix. Also, common component relatively stable
performance amount labeled data increased. is, performance
deviate much scored lower amounts labeled data. increased
amount labeled data, wins ASSEMBLE-1NN common component changed
ties losses, compared supervised learner. contrary, addition
labeled data helped co-training, re-weighting, Sample -Select. techniques ranked
357

fiCHAWLA & KARAKOULAS

low (1,99)% split. Co-training noted largest improvement compared
techniques increased (90,10)% split, namely labeled data.
effect label noise semi-supervised learning?
investigated objective using artificial datasets. Noise detrimental
effect semi-supervised learning. contrary, fewer semi-supervised models
performed worse single supervised model compared no-noise cases.
despite significant reduction performance single Naive Bayes model. believe
single Naive Bayes learner overfitting noise level increases.
cases unlabeled data acting regularizer preventing semi-supervised learner
overfit. fact, 20% mislabeling noise adding unlabeled data improves performance even
(10,90)% labeled/unlabeled mix.
effect sample-selection bias semi-supervised learning?
objective used two real-world datasets transformed MAR
real-world MNAR dataset. Selection bias, especially MNAR case, detrimental
performance supervised learner semi-supervised techniques.
type problem choosing appropriate learning technique, bivariate
probit specifically addresses selection bias even common-component mixture,
critical. observed ASSEMBLE, reasonably well datasets
labels missing completely random, perform well biased datasets
(MAR MNAR). expected, Sample -Select well MNAR dataset. one
technique reasonably well fairly consistent common component
technique. Bivariate probit best performance amongst techniques MNAR
dataset. presence selection bias unlabeled data conjunction
appropriate technique seem always improve performance.
semi-supervised techniques considered consistently better supervised
learning?
single technique consistently better supervised Nave Bayes
classifier labeled/unlabeled splits. (1,99)% mix, ASSEMBLE-1NN
common-component mixture generally better Nave Bayes.
semi-supervised techniques considered neither better worse Nave Bayes.
increase amount labeled data, wins ASSEMBLE-1NN common component
move ties losses. Common component losses added labeled data,
particularly artificial datasets. Sample -select comparable Nave Bayes
classifier, sometimes slightly better. Re-weighting generally worse Nave Bayes.
Co-training noted largest improvement (over Nave Bayes) compared techniques
increased percentage labeled data.
Semi-supervised learning still open problem, especially context MNAR. Current
techniques behave differently depending nature datasets. Understanding type
missing data mechanism data assumptions key devising appropriate
358

fiLearning Labeled Unlabeled Data

semi-supervised learning technique improving performance adding unlabeled data.
work, dealt datasets varying amounts imbalance class distribution.
purpose, utilized AUC performance measure. extension work could
utilization performance metrics, e.g. classification error, analysis effects
unlabeled data.

Acknowledgments
Thanks Ruslan Salakhutdinov help experiments. Also, thanks Brian
Chambers, Danny Roobaert anonymous reviewers comments suggestions
earlier versions paper.

Appendix
Tables 6 9 present results experiments different parameter settings
semi-supervised techniques: number components common-component mixture,
parameter ASSEMBLE, significance level confidence intervals co-training.

359

fiCHAWLA & KARAKOULAS

Dataset

30_30_00_05

30_80_00_05

80_30_00_05

80_80_00_05

Adult

Satimage

Waveform

Horseshoe
Neuron

(Labeled,
Unlabeled)%
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)

2
0.5488
0.6870
0.6891
0.6911
0.6927
0.4105
0.5145
0.5150
0.5160
0.5167
0.2253
0.2268
0.2317
0.2379
0.2431
0.1323
0.2222
0.2289
0.2392
0.2454
0.6590
0.5736
0.7465
0.7160
0.7578
0.0190
0.0264
0.0890
0.0575
0.0832
0.7109
0.7135
0.7225
0.7311
0.7348
0.3052
0.5570

Number Mixture Components
6
12
AUC
0.6904
0.3536
0.8369
0.7245
0.8377
0.7277
0.8754
0.7558
0.8833
0.7458
0.6753
0.2107
0.7454
0.4874
0.7605
0.5344
0.7760
0.5520
0.7881
0.5577
0.3914
0.0982
0.6728
0.1567
0.6954
0.2499
0.7057
0.2516
0.7138
0.2224
0.5054
0.3014
0.6821
0.4553
0.6921
0.4806
0.7310
0.5116
0.7595
0.5099
0.7954
0.6805
0.8210
0.7377
0.8237
0.7523
0.8378
0.7607
0.8432
0.7634
0.7972
0.6919
0.8362
0.8199
0.8454
0.8259
0.8544
0.8295
0.8565
0.8264
0.9074
0.8986
0.9053
0.9384
0.9209
0.9394
0.9334
0.9437
0.9407
0.9455
0.4000
0.3300
0.6500
0.6539

24
0.4529
0.7676
0.8092
0.8057
0.8171
0.3138
0.4834
0.5430
0.5670
0.5831
0.1046
0.2190
0.2674
0.3247
0.3560
0.2713
0.4386
0.5073
0.5302
0.5710
0.7076
0.7453
0.7676
0.7693
0.7807
0.6447
0.8299
0.8438
0.8519
0.8520
0.8758
0.9298
0.9393
0.9443
0.9449
0.3400
0.6600

Table 6. Sensitivity common components respect number components different datasets
labeled/unlabeled split. bold entries show components maximum AUC.

360

fiLearning Labeled Unlabeled Data

Dataset

30_30_00_05

30_80_00_05

80_30_00_05

80_80_00_05

Adult

Satimage

Waveform

Horseshoe
Neuron

(Labeled,
Unlabeled)%
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)

0.4
0.7244
0.8621
0.9295
0.9557
0.9599
0.5538
0.7047
0.8502
0.8989
0.8992
0.2730
0.8824
0.9305
0.9416
0.9409
0.4760
0.8610
0.9046
0.9207
0.9127
0.3059
0.7754
0.8027
0.8210
0.8323
0.6940
0.8566
0.8582
0.8402
0.8165
0.8206
0.8517
0.8998
0.9041
0.8923
0.0397
0.7108

Value
0.7
AUC
0.7602
0.8485
0.9111
0.9496
0.9595
0.5673
0.6690
0.8011
0.8927
0.9004
0.3085
0.8741
0.9388
0.9427
0.9452
0.4563
0.8283
0.9011
0.9261
0.9197
0.2955
0.7687
0.7997
0.8192
0.8313
0.6797
0.8545
0.8600
0.8483
0.8214
0.8297
0.8381
0.8947
0.9119
0.8969
0.1778
0.6956

1
0.6989
0.8386
0.9024
0.9440
0.9587
0.5542
0.6534
0.7603
0.8840
0.8990
0.3011
0.8677
0.9409
0.9484
0.9440
0.4531
0.8039
0.8969
0.9289
0.9174
0.2773
0.3055
0.7959
0.8157
0.831
0.7133
0.8517
0.8600
0.8506
0.8258
0.8160
0.8327
0.8815
0.9137
0.8989
0.2347
0.6929

Table 7. Sensitivity ASSEMBLE-1NN respect value different datasets
labeled/unlabeled split. bold entries show value maximum AUC.

361

fiCHAWLA & KARAKOULAS

Dataset

30_30_00_05

30_80_00_05

80_30_00_05

80_80_00_05

Adult

Satimage

Waveform

Horseshoe
Neuron

(Labeled,
Unlabeled)%
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)

0.4
0.2521
0.9082
0.9397
0.9565
0.9605
-0.1739
0.8398
0.8734
0.8985
0.9007
-0.7405
0.8874
0.9334
0.9410
0.9412
-0.6767
0.8618
0.9073
0.9202
0.9152
0.6875
0.7754
0.8022
0.8202
0.8323
0.2113
0.8285
0.8613
0.8321
0.8107
0.0017
0.8424
0.8902
0.8945
0.8884
0.2254
0.6572

Value
0.7
AUC
0.0139
0.8931
0.9302
0.9515
0.9587
-0.3721
0.8105
0.8466
0.8947
0.9025
-0.6104
0.8760
0.9447
0.9477
0.9427
-0.8587
0.8507
0.9112
0.9257
0.9137
0.6743
0.7687
0.7991
0.8182
0.8313
0.2024
0.8255
0.8619
0.8455
0.8162
-0.2123
0.8247
0.8794
0.9057
0.8902
0.1641
0.6754

1
-0.3505
0.8849
0.9235
0.9473
0.9578
-0.6730
0.7946
0.8208
0.8877
0.8979
-0.8355
0.8677
0.9465
0.9499
0.9441
-0.8990
0.8096
0.9112
0.9290
0.9201
0.6573
0.7671
0.7975
0.8148
0.8306
0.2024
0.8192
0.8624
0.8525
0.8245
-0.3601
0.8158
0.8718
0.9067
0.8928
0.3711
0.6436

Table 8. Sensitivity ASSEMBLE-Class0 respect value different datasets
labeled/unlabeled split. bold entries show value maximum AUC.

362

fiLearning Labeled Unlabeled Data

Dataset

30_30_00_05

30_80_00_05

80_30_00_05

80_80_00_05

Adult

Satimage

Waveform

Horseshoe
Neuron

(Labeled,
Unlabeled)%
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)

90%
0.3255
0.9338
0.9651
0.9820
0.9852
-0.1281
0.8169
0.8858
0.9132
0.9224
-0.3532
0.6624
0.8734
0.9229
0.9331
-0.5479
0.4828
0.8145
0.8734
0.8839
0.7208
0.7871
0.8050
0.8131
0.8264
0.5213
0.8308
0.8566
0.8625
0.8650
0.8000
0.8400
0.8679
0.8856
0.8931
0.4735
0.6234

Confidence Value
95%
AUC
0.3255
0.9343
0.9571
0.9799
0.9851
-0.1281
0.8129
0.8748
0.9121
0.9227
-0.3532
0.6694
0.8734
0.9229
0.9331
-0.5479
0.4828
0.8145
0.8734
0.8839
0.7208
0.7871
0.8050
0.8131
0.8264
0.5213
0.8346
0.8566
0.8625
0.8650
0.8000
0.8391
0.8681
0.8855
0.8927
0.4735
0.6234

99%
0.3255
0.9316
0.9538
0.9768
0.9844
-0.1281
0.8051
0.8709
0.9099
0.9200
-0.3532
0.7108
0.8734
0.9229
0.9331
-0.5479
0.4828
0.8145
0.8734
0.8839
0.7208
0.7871
0.8050
0.8131
0.8264
0.5213
0.8346
0.8566
0.8625
0.8650
0.8000
0.8377
0.8681
0.8859
0.8930
0.4735
0.6234

Table 9. Sensitivity Co-training respect confidence value different datasets
labeled/unlabeled split. bold entries show value maximum AUC.

363

fiCHAWLA & KARAKOULAS

References
Bennett, K., Demiriz, A. & Maclin, R. (2002). Exploiting unlabeled data ensemble methods.
Proceedings Sixth International Conference Knowledge Discovery Databases, pp.
289-296, Edmonton, Canada.
Blake, C., Keogh, E., & Merz, C.J. (1999). UCI repository machine learning databases. (URL:
http://www.ics.uci.edu/~mlearn/MLRepository.html)
Blum, A. & Chawla, S. (2001). Learning Labeled Unlabeled Data using Graph Mincuts.
Proceedings Eighteenth International Conference Machine Learning, pp. 19-26,
San Francisco, CA.
Blum, A. & Mitchell, T. (1998). Combining labeled unlabeled data co-training.
Proceedings Workshop Computational Learning Theory, pp. 92-100, Madison, WI.
Bradley, A.P. (1997). Use Area ROC Curve Evaluation Machine
Learning Algorithms. Pattern Recognition, 30(6), 145-1159.
Chawla, N.V., Bowyer, K.W., Hall, L.O. & Kegelmeyer, W.P. (2002). SMOTE: Synthetic
Minority Over-sampling Technique. Journal Artificial Intelligence Research, 16, 321-357.
Cozman, F., Cohen, I., & Cirelo, M. (2002). Unlabeled data degrade classification performance
generative classifiers. Proceedings Fifteenth International Florida Artificial Intelligence
Society Conference, pp. 327-331, Pensacola, FL.
Cozman, F., Cohen, I., & Cirelo, M. (2003). Semi-supervised learning mixture models.
Proceedings Twentieth International Conference Machine Learning, pp. 99-106,
Washington, DC.
Crook, J. & Banasik, J (2002). Sample selection bias credit scoring models. International
Conference Credit Risk Modeling Decisioning, Philadelphia, PA.
Dempster, A., Laird, N. & Rubin, D. (1977). Maximum Likelihood incomplete data via
EM algorithm. Journal Royal Statistical Society, Series B, 39(1), 1-38.
Fayyad, U.M. & Irani, K.B. (1993). Multi-Interval discretization continuous-valued attributes
classification learning. Proceedings Thirteenth International Joint Conference AI,
pp 1022-1027, San Francisco, CA.
Feelders, A.J. (2000). Credit scoring reject inferencing mixture models. International
Journal Intelligent Systems Accounting, Finance & Management, 9, 1-8.
Freund, Y. & Schapire, R. (1996). Experiments new boosting algorithm. Proceedings
Thirteenth International Conference Machine Learning, pp. 148-156, Bari, Italy.
Ghahramani, Z. & Jordan, M.I. (1994). Learning incomplete data. Technical Report 108, MIT
Center Biological Computational Learning.
Ghani, R., Jones, R. & Rosenberg, C. (2003). Workshop Continuum Labeled
364

fiLearning Labeled Unlabeled Data

Unlabeled Data Machine Learning Data Mining. Twenty-first International
Conference Machine Learning, Washington, D.C.
Goldman, S. & Zhou, Y. (2000). Enhancing supervised learning unlabeled data.
Proceedings Seventeenth International Conference Machine Learning, pp.327-334,
San Francisco, CA.
Greene, W. (1998). Sample selection credit-scoring models. Japan World Economy, 10:
299-316.
Guyon, I. (2001). NIPS 2001 Workshop variable feature Selection.
http://www.clopinet.com/isabelle/Projects/NIPS2001/.
Hand, D.J. (1997). Construction assessment classification rules. Chichester: John Wiley
Sons.
Hettich, S. Bay, S. D. (1999). UCI KDD Archive, http://kdd.ics.uci.edu. University
California Irvine, Department Information Computer Science.
Heckman, J. (1979). Sample selection bias specification error. Econometrica, 47, 153-161.
Jacobs, B.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E. (1991). Adaptive mixtures local
experts. Neural Computation, 3(1), 79-87.
Joachims, T. (2003). Transductive Learning via Spectral Graph Partitioning. Proceedings
Twentieth International Conference Machine Learning, pp. 290-297, Washington, DC.
Karakoulas, G & Salakhutdinov, R. (2003). Semi-supervised Mixture Experts Classification.
Proceedings Fourth IEEE International Conference Data Mining, pp. 138-145,
Brighton UK.
Kontkanen, P., Myllymaki, P., & Tirri, H. (1996). Constructing bayesian finite mixture models
EM algorithm. Technical Report NC-TR-97-003, ESPRIT: Neural Computational
Learning (NeuroCOLT).
Kremer, S. & Stacey, D. (2001). NIPS 2001 Workshop Competition unlabeled data
supervised learning. http://q.cis.guelph.ca/~skremer/NIPS2001/.
Little, R.J.A & Rubin, D.R. (1987). Statistical Analysis Missing Data. Wiley: New York.
Miller, D. & Uyar, S. (1997). mixture experts classifier learning based labeled
unlabeled data. Advances Neural Information Processing Systems 9, pp. 571-578, MIT
Press.
Nigam, K., McCallum, A., Thrun, S., & Mitchell, T. (2000). Text classification labeled
unlabeled documents using EM. Machine Learning, 39(2/3), 103-134.
Nigam, K. & Ghani, R. (2000). Analyzing effectiveness applicability co-training.
Proceedings Ninth International Conference Information Knowledge Management
pp. 86-93.
365

fiCHAWLA & KARAKOULAS

Provost, F. & Fawcett, T. (2001). Robust classification imprecise environments. Machine
Learning, 42, 203-231.
Provost, F. & Domingos, P. (2003). Tree induction probability based ranking. Machine
Learning, 52, 199-215.
Quinlan R. (1992) C4.5: Programs Machine Learning. San Mateo, CA: Morgan Kaufmann.
Seeger, M. (2000). Learning labeled unlabeled data. Technical report, Institute ANC,
Edinburgh, UK. http://www.dai.ed.ac.uk/~seeger/papers.html.
Shahshahni, B. & Landgrebe, D. (1994). effect unlabeled samples reducing small
sample size problem mitigating Hughes phenomenon. IEEE Transactions
Geoscience Remote Sensing, 32(5), 1087-1095.
Talavera, L. (2000). Dependency-based feature selection clustering symbolic data. Intelligent
Data Analysis, 4(1), 19-28.
Swets, J. (1988). Measuring Accuracy Diagnostic Systems. Science, 240, 1285-1293.
Zadrozny, B. & Elkan, C. (2000). Learning making decisions costs probabilities
unknown. Proceedings Seventh International Conference Knowledge
Discovery Data Mining, pp 204-213, San Francisco, CA.
Zhang, T., & Oles, F.J. (2000). probability analysis value unlabeled data
classification problems. Proceedings Seventeenth International conference Machine
Learning, pp 1191-1198, Stanford, CA.

366

fiJournal Artificial Intelligence Research 23 (2005) 123-165

Submitted 10/03; published 2/05

Restricted Value Iteration: Theory Algorithms
Weihong Zhang

wzhang@cs.wustl.edu

Department Computer Science
Washington University, Saint Louis, MO 63130 USA

Nevin L. Zhang

lzhang@cs.ust.hk

Department Computer Science
Hong Kong University Science & Technology
Clear Water Bay Road, Kowloon, Hong Kong, CHINA

Abstract
Value iteration popular algorithm finding near optimal policies POMDPs.
inefficient due need account entire belief space, necessitates
solution large numbers linear programs. paper, study value iteration
restricted belief subsets. show that, together properly chosen belief subsets,
restricted value iteration yields near-optimal policies give condition determining whether given belief subset would bring savings space time. also
apply restricted value iteration two interesting classes POMDPs, namely informative
POMDPs near-discernible POMDPs.

1. Introduction
Partially Observable Markov Decision Processes (POMDPs) provide general framework
sequential decision-making tasks effects agents actions nondeterministic states world environment known certainty. Due
model generality, POMDPs found variety potential applications reality (Monahan, 1982; Cassandra, 1998b). However, solving POMDPs computationally intractable.
Extensive efforts devoted developing efficient algorithms finding solutions
POMDPs (Parr & Russell, 1995; Cassandra, Littman, & Zhang, 1997; Cassandra, 1998a;
Hansen, 1998; Zhang, 2001).
Value iteration popular algorithm solving POMDPs. Two central concepts
value iteration belief state value function. belief state, probability distribution
state space, measures probability environment state.
possible belief states constitute belief space. value function specifies payoff cost
belief state belief space. Value iteration proceeds iterative fashion.
iteration, referred dynamic programming (DP) update, computes new value
function current one. algorithm terminates, final value function
used agents action selection. Value iteration computationally expensive because,
iteration, updates current value function entire belief space,
necessitates solution large number linear programs.
One generic strategy accelerate value iteration restrict value iteration, is, DP
updates, subset belief space. simplicity, subset belief space referred
belief subset. Existing value iteration algorithms working belief subsets include
family grid-based algorithms DP updates calculate values finite grid (Lovejoy,
c
2005
AI Access Foundation. rights reserved.

fiZhang & Zhang

1991; Hauskrecht, 1997; Zhou & Hansen, 2001), several (maybe anytime) algorithms
DP updates calculate values growing belief subset (Dean, Kaelbling, Kirman,
& Nicholson, 1993; Washington, 1997; Hansen & Ziberstein, 1998; Hansen, 1998; Bonet &
Geffner, 2000). restricting value iteration belief subset, complexity value
functions reduced also DP updates efficient. advantages
observed several researchers (Hauskrecht & Fraser, 1998; Roy & Gordon, 2002; Zhang
& Zhang, 2001b; Pineau, Gordon, & Thrun, 2003).
fundamental issue restricted value iteration select belief subset.
efficiency value iteration quality generated value functions strongly depend
selected belief subset. one extreme case, subset chosen singleton
set, value iteration efficient, quality value functions arbitrarily poor.
extreme case, subset belief space, quality value functions
retained, algorithm inefficient. exists tradeoff size
belief subset quality value functions.
paper, show indeed possible value iteration work
belief subset also retain quality value functions. achieved deliberately
selecting belief subset value iteration. Sometimes, refer algorithm working
selected belief subset subset value iteration. (For distinction, restricted value
iteration refers value iteration working belief subset.) efficiency subset
value iteration depends size selected subset. characterize condition
priori determine whether subset proper 1 respect belief space given
POMDP. case, subset value iteration carries space time advantages.
also study two special POMDP classes, namely informative POMDPs neardiscernible POMDPs. informative POMDP assumes agent good albeit
imperfect idea world states time point. informative POMDP,
exists natural belief subset value iteration restricted efficient
standard value iteration (Zhang & Liu, 1997). near-discernible POMDP assumes
agent good idea world states while. near-discernible
POMDP, propose restricted value iteration algorithm starts small belief
subset grows gradually. algorithm terminates proper tradeoff size
subset policy quality found. near-discernibility, algorithm
able find good tradeoff subset grows large.
algorithms developed paper tested variety small maze problems designed possess various properties desired, number problems adapted
existing research created office environment. results show
exploiting problem characteristics, restricted value iterations solve larger POMDPs
standard value iteration. show algorithmic performances vary
properties selected belief subset maze problems. small problems facilitate exposition properties chosen belief subsets. Meanwhile, experiments
provide clues POMDP classes amenable perspective algorithms.
rest paper organized follows. next section, introduce
POMDP model value iteration. two subsequent sections, present subset
value iteration algorithm analyze theoretical properties. particular, Section
1. Set proper subset set B (1) subset B, (2) exists least one element B
belong A.

124

fiRestricted Value Iteration: Theory Algorithms

3, show select belief subset selected subset related
belief space. Section 4, describe subset value iteration algorithm discuss
able achieve near optimality. Section 5, examine informative POMDPs
show algorithm exploits informativeness related general subset
value iteration (Zhang & Liu, 1997). Section 6, examine near-discernible POMDPs
develop anytime algorithm. empirically demonstrate algorithm able
compute value functions high quality. Section 7, survey related work
research.

2. POMDPs Value Iteration
section gives brief overview POMDP model value iteration.
2.1 POMDPs
POMDP sequential decision model agent acts stochastic environment
partial knowledge state environment. set possible states
environment referred state space denoted S. point
time, environment one possible states. agent directly observe
state. Rather, receives observation it. denote set possible
observations Z. receiving observation, agent chooses action set
possible actions executes action. Thereafter, agent receives immediate
reward environment evolves stochastically next state.
Mathematically, POMDP specified by: three sets S, Z, A; reward function
r(s, a) A; transition probability function P (s0 |s, a); observation
probability function P (z|s0 , a) z Z s0 S. reward function characterizes
dependency immediate reward current state current action a.
transition probability characterizes dependency next state s0 current state
current action a. observation probability characterizes dependency
observation z next time point next state s0 current action a.
2.2 Policies Value Functions
Since current observation necessarily fully reveal identity current
state, agent needs consider previous observations actions choosing
action. Information current state contained current observation, previous
observations, previous actions summarized probability distribution
state space (Astrom, 1965). probability distribution sometimes called belief
state denoted b. possible state s, b(s) probability current
state s. set possible belief states called belief space. denote B.
policy prescribes action possible belief state. words,
mapping B A. Associated policy value function V . belief
state b, V (b) expected total discounted reward agent receives following
P

policy starting b, i.e., V (b) = E,b [
t=0 rt ] rt reward received
time (0<1) discount factor. known exists policy

V (b)V (b) policy belief state b (Puterman, 1994).
125

fiZhang & Zhang

policy called optimal policy. value function optimal policy called
optimal value function. denote V . positive number , policy
-optimal V (b) + V (b) b B.
2.3 Value Iteration
explain value iteration, need consider belief state evolves time. Let b
current belief state. belief state next point time determined
current belief state, current action a, next observation z. denote (b, a, z).
state s0 , (b, a, z) given
P

P (z,

(b, a, z)(s0 ) =

0 |s, a)b(s)

P (z|b, a)

,

(1)

P

P (z, s0 |s, a)=P (z|s0 , a)P (s0 |s, a) P (z|b, a)= s,s0 P (z, s0 |s, a)b(s) renormalization constant. notation suggests, constant also interpreted
probability observing z taking action belief state b.
concept belief state, POMDP model transformed belief space
MDP follows.
state space B action space A.
Given belief state b action a, transition model specifies transition
probability follows.
(

0

P (b |b, a) =

P (z|b, a) b0 = (b, a, z) z,
0
otherwise.

Given belief state b action a, reward model specifies immediate reward
P
r(b, a) r(b, a) = sS b(s)r(s, a).
Due reformulation, task solving POMDP accomplished solving
reformulated MDP. proven reformulated MDP stationary
optimal policy, found stochastic dynamic programming (Bellman, 1957;
Puterman, 1994).
Value iteration dynamic programming algorithm finding -optimal policies
MDP. starts initial value function V0 iterates using following formula:
Vn+1 (b) = max[r(b, a) +


X

P (z|b, a)Vn ( (b, a, z))] b B

(2)

z

Vn referred nth-step value function. known Vn geometrically
converges V n goes infinity.
given value function V , policy said V -improving
(b) = arg max[r(b, a) +


X

P (z|b, a)V ( (b, a, z))]

b B.

(3)

z

following theorem tells one terminate value iteration given precision requirement (Puterman, 1994). stopping criterion depends quantity maxbB |Vn (b)
Vn1 (b)|, maximum difference Vn Vn1 belief space.
quantity often called Bellman residual Vn Vn1 (Puterman, 1994).
126

fiRestricted Value Iteration: Theory Algorithms

Theorem 1 maxb |Vn (b) Vn1 (b)| (1 )/(2), Vn1 -improving policy
-optimal.

Since infinitely many belief states, value functions cannot explicitly represented. Fortunately, value functions one encounters process value iteration
admit implicit finite representations (Sondik, 1971).
2.4 Technical Notational Considerations
convenience, view functions state space vectors size |S|. use lower
case Greek letters refer vectors script letters V U refer sets
vectors. contrast, upper case letters V U always refer value functions,
functions belief space B.
set V vectors induces piecewise linear convex value function (say f ) follows:
f (b) = maxV b b B b inner product b. convenience,
shall abuse notation use V denote set vectors value function
induced set. convention, quantity f (b) written V(b).
vector set useless removal affect function set induces.
useful otherwise. set vectors minimal contains useless vectors. Let
vector set V. known useful least one belief
state b b > 0 b, 0 V\{}. belief state called witness point
testifies fact useful (Kaelbling, Littman, & Cassandra, 1998).
determine usefulness vector set, sufficient solve one linear program.
compute minimal set given set V vectors, sufficient solve |V| linear
programs. procedure computing minimal set given set vectors often
referred pruning set.
2.5 Finite Representation Value Functions Value Iteration
value function V represented set vectors equals value function induced
set. value function representable finite set vectors,
unique minimal set represents function (Littman, Cassandra, & Kaelbling, 1995).
Sondik (1971) shown value function representable finite set
vectors, subsequent value functions derived DP updates. process
obtaining minimal representation Vn+1 minimal representation Vn
usually referred dynamic programming (DP) update.
practice, value iteration POMDPs carried directly terms value
functions themselves. Rather, carried terms sets vectors represent
value functions. One begins initial set vectors V0 (often set zero-vector).
iteration, one performs DP update previous minimal set Vn vectors
obtains new minimal set Vn+1 vectors. One continues Bellman residual
maxb |Vn+1 (b) Vn (b)|, determined solving sequence linear programs, falls
threshold.
127

fiZhang & Zhang

3. Belief Subset Selection
section, show select belief subset value iteration. describe
condition determining whether selected subset proper w.r.t. belief space.
addition, discuss minimal representation value functions w.r.t. selected subset.
next section, develop subset value iteration algorithm show able
achieve near optimality.
3.1 Subset Selection
belief subset selection rests belief updating. Let agents current belief b.
next belief state (b, a, z) performs action receives observation z. vary
belief state b belief space B, obtain set { (b, a, z)|b B}. Abusing
notation, denote set (B, a, z). words, matter belief state agent
starts with, receives z performing a, next belief state must (B, a, z).
union a,z (B, a, z) takes account sets belief states possible combinations actions observations. contains belief states agent
encounter. words, agents belief state time point must belong
set regardless initial belief state, performed actions received observations.
denote set (B, A, Z) simply (B). closed set sense action
lead agent belief states outside (B) agent starts belief state it.
Furthermore, belief subset set (B) belief space B closed.
Lemma 1 set (B) closed. Moreover, (B) B0 B, B0 closed.
apparent, set (B) subset belief space B. definition application
reachability analysis (Boutilier, Brafman, & Geib, 1998; Dean et al., 1993).
terminology reachability analysis, subset (B, a, z) comprises one-step reachable
belief states agent performs action receives observation z, subset (B)
comprises one-step reachable belief states regardless performed actions received
observations. Although belief subset (B) set one-step reachable belief states,
appealing property, shown next subsection, value iteration working
preserve quality generated value functions.
3.2 Subset Representation
Subset representation addresses represent subsets (B, a, z) (B). this,
introduce concept belief simplex.
Definition 1 Let B = {b1 , b2 , ..., bk } set belief states. belief simplex generated
P
P
B set belief states { ki=1 bi |i 0 ki=1 = 1.0}.
set B said basis belief simplex . definition, belief
simplex (or simply simplex) set convex combinations belief states
basis. Following standard terms linear algebra, also talk minimum
basis simplex. convenience, use notation B denote basis given simplex
. Additionally, simplex basis {b1 , b2 , , bk } denoted (b1 , b2 , , bk ).
result z, subset (B, a, z) simplex. intuition
follows. Let number states POMDP n. {1, 2, . . . , n}, bi unit
128

fiRestricted Value Iteration: Theory Algorithms

vector, i.e., bi (s) equals 1.0 = 0.0 otherwise. belief state bi , P (z|bi , a) > 0,
(bi , a, z) belief state (B, a, z); P (z|bi , a) = 0, belief update equation
(bi , a, z) undefined. belief space B, trivial note belief state
represented convex combination belief states {b1 , b2 , , bn }. Correspondingly,
belief subset (B, a, z), belief state represented convex combination
belief states { (bi , a, z)|P (z|bi , a) > 0}. Hence, { (bi , a, z)|P (z|bi , a) > 0} basis
(B, a, z). convenience, denote basis B (B,a,z) .
Theorem 2 pair [a, z], subset (B, a, z) simplex.
Proof: See Appendix A.
2
theorem, subset (B) union simplices. Although subset
(B) linearly representable own, union linearly representable sets.
Later section, property crucial exploited finding minimal
representing sets value functions w.r.t. belief subset (B).
concretize ideas subset representation, give POMDP example visualize simplices actions observations. presenting example, mention
shall use additional purposes later paper. First, shall use
show difference two conditions determining whether subset (B)
proper subset belief space. Second, shall use demonstrate fundamental
differences two restricted value iteration algorithms.
Example POMDP three states {s1 , s2 , s3 }, two actions {a1 , a2 } two observations {z1 , z2 }. define transition observation model action a1 . models
a2 defined similarly. shorten notations, use pij denote transition
probability P (sj |si , a1 ) qij denote observation probability P (zj |si , a1 ). assume (1) state si , probability pi1 equal pi2 , i.e., pi1 = pi2 ; (2)
state si , observations z1 z2 received probability, i.e., qi1 = qi2 = 0.5
i; (3) p11 > p21 > p31 . assumptions, matrix


Pa1 z1

0.5p11

0.5p11
=
0.5(1 2p11 )

0.5p21
0.5p21
0.5(1 2p21 )



0.5p31

0.5p31
.
0.5(1 2p31 )

(4)

first assumption, first two rows matrix same. third
row, probability pi3 replaced 1 pi1 pi2 , i.e., 1 2pi1 .
compute basis belief subset (B, a1 , z1 ). Let basis belief space
B set {(1.0, 0, 0)T , (0, 1.0, 0)T , (0, 0, 1.0)T }. (For matrix vector A, denotes
transpose.) action a1 observation z1 , next belief states, denoted Ai s, are:
A1
A2
A3

= ((1.0, 0, 0)T , a1 , z1 ) = (p11 , p11 , 1.0 2p11 )T
= ((0, 1.0, 0)T , a1 , z1 ) = (p21 , p21 , 1.0 2p21 )T
= ((0, 0, 1.0)T , a1 , z1 ) = (p31 , p31 , 1.0 2p31 )T

(5)

Interestingly, shown A2 convex combination A1 A3 . fact,
verified
((0, 1.0, 0)T , a1 , z1 ) = 1 ((1.0, 0, 0)T , a1 , z1 ) + 2 ((0, 0, 1.0)T , a1 , z1 )
129

fiZhang & Zhang

p31
p21
1 = pp21
2 = pp11
. Thus 1 + 2 = 1.0. third assumption ensures
11 p31
11 p31
1 2 greater 0.0. A2 convex combination A1 A3 ,
three belief states A1 A3 lie straight line.
Figure 1 visualizes belief space B left simplex (B, a1 , z1 )
right. right chart based parameters: p11 = 0.5, p21 = 0.4 p31 = 0.1.
belief states Ai following: A1 = (0.5, 0.5, 0.0)T , A2 = (0.4, 0.4, 0.2)T
A3 = (0.1, 0.1, 0.8)T . see belief space triangle area belief simplex
line segment area. next subsection, shall return point show
why.

b(s3)

b(s3)

11111111
00000000
(0,0,1.0)11111111
00000000

(0,0,1.0)

11111111
00000000
11111111
00000000
11111111
00000000
b(s1)
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
(1.0,0,0)
11111111
00000000
00000000
(0,0,0) 11111111
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
00000000
11111111

(0,1.0,0)

* A3
b(s1)
A2
(1.0,0,0)
*
(0,0,0)
* A1

b(s2)

(0,1.0,0)

b(s2)

Figure 1: graphical representation belief space belief simplex. See text explanations.
show belief subset (B), continue define transition observation
models action a2 .2 may follow three assumptions a1 define models
a2 , give different set transition probabilities a2 differs a1 .
second assumption observation z1 z2 received probability, matrix Pai ,z1 identical Pai ,z2 {1, 2}. So, simplex (B, ai , z1 )
identical (B, ai , z2 ) i. such, subset (B) consists two line segments
entire belief space.
2
3.3 Belief Subset Belief Space
discuss relationship set (B) belief space B. Since set (B)
union simplices, helps show simplex (B, a, z) related belief
space B. action observation z, turns matrix derived
transition observation models plays central role determining simplex.
matrix, denoted Paz , dimension |S| |S| entry (s, s0 ) joint
probability P (s0 , z|s, a), i.e.,





Paz =

P (s01 , z|s1 , a)
P (s02 , z|s1 , a)

0
P (sn , z|s1 , a)

P (s01 , z|s2 , a)
P (s02 , z|s2 , a)

0
P (sn , z|s2 , a)






P (s01 , z|sn , a)
P (s02 , z|sn , a)

0
P (sn , z|sn , a)




.


2. components POMDP affect discussions omitted convenience.

130

fiRestricted Value Iteration: Theory Algorithms

matrix used relate next belief (b, a, z) current b. b
(b, a, z) viewed column vector form, belief update Equation (1) rewritten
1
(b, a, z) = p(z|b,a)
Paz b. Hence, (b, a, z) transformation b matrix Paz
called transformational matrix. following lemma characterizes condition
simplex (B, a, z) belief space B.
Lemma 2 [a, z], exists bijection simplex (B, a, z)
space B matrix Paz invertible 3 .
1
Paz b (B, a, z) set
seen fact (b, a, z) = p(z|b,a)
transformed belief states belief space. Consequently, simplex (B, a, z)
proper subset B matrix Paz degenerate. note matrix Paz degenerate
exists state s0 P (z|s0 , a) = 0.0, i.e., state agent
never receive observation z (if action performed). entries
row corresponding s0 0.0 matrix. case, lemma, set
(B, a, z) must proper subset belief space B.

Corollary 1 state P (z|s, a) = 0.0, set (B, a, z) proper
subset belief space B.
However, (B, a, z) proper subset necessarily imply exists
state P (z|s, a) = 0.0. Consequently, determine belief simplex
proper, corollary provides sufficient condition; contrast, Lemma 2 provides
sufficient necessary condition. illustrated continuing discussions
POMDP example.
Example (Continued) consider simplex (B, a1 , z1 ). second assumption,
z1 observed state. So, exist state P (z1 |s, a1 ) = 0.0.
hand, matrix Pa1 z1 degenerate two rows.
Lemma 2, simplex (B, a1 , z1 ) proper set belief space. fact, seen
Figure 1, line segment, viewed degenerate belief space.
2
proceed discover relationship belief set (B, a, z) belief
space B. Since (B, a, z) union simplices (B, a, z), proper subset
belief space simplex. turn, requires transformational matrix
degenerate.
Theorem 3 subset (B) proper subset belief space B transformational
matrices actions observations degenerate.
3.4 Subset Value Functions
discuss value functions whose domains belief subset (B, a, z) (B). simplicity,
refer subset value functions. problem examine is, given set
vectors representing subset value function, compute minimal set w.r.t. belief
subset. first consider case subset simplex.
3. matrix invertible determinant non-zero. degenerate otherwise.

131

fiZhang & Zhang

order calculate minimal set vectors, one needs determine usefulness
vector set w.r.t. simplex. Let vector set V simplex (B, a, z).
vector useful w.r.t. (B, a, z) belief state b simplex
b b + x x sufficiently small positive number vector
set V{}. Moreover, belief state b exists, since simplex, b must
P
representable belief states basis B (B,a,z) , i.e., b = (bi , a, z).
P
replace b (bi , a, z) b b + x, condition determining usefulness
equivalent this: whether exists series nonnegative numbers
vector V,


X

(bi , a, z)



X

(bi , a, z) + x.



Rewriting inequality,
X

[ (bi , a, z)]



X

[ (bi , a, z)] + x.

(6)



determine usefulness, procedure simplexLP Table 1 used.
optimality linear program reached, one checks objective x. positive,
exists belief state belief simplex (B, a, z) belief state dominates
P
vectors. belief state witness point . represented (bi , a, z)
solutions (values variables) linear program. case,
P
belief state (bi , a, z) returned. cases, belief state returned
useless vector.
simplexLP(, V, B (B,a,z) ):
1. Variables: x,
2. Maximize: x.
3. Constraints:
P
P
4.
[ (bi , a, z)]
[ (bi , a, z)] + x V {}
i, (bi , a, z) B (B,a,z)
P
5.

= 1, 0 i.
simplexPrune(V, B (B,a,z) ):
1. U V, V
2. U
3.
b simplexLP(, U, B (B,a,z) )
4.
b 6= null
5.
V V {}
6. Return V
Table 1: procedure compute minimal set vectors simplex
determine vectors usefulness given set, one linear program needs solved.
vector useless, removal change value function set induces.
132

fiRestricted Value Iteration: Theory Algorithms

Therefore, compute minimal set given set V w.r.t. simplex, one needs solve
|V| linear programs.
procedure implemented simplexPrune Table 1. input two arguments:
set vectors V 4 basis simplex B (B,a,z) . set U initialized
set V set V empty line 1. Useful vectors added set V
sequel. vector set U, line 3 procedure simplexLP called determine
uselessness. returns belief state, vector added set V line 5. Eventually,
set V becomes minimal representation U w.r.t. simplex (B, a, z).
compute minimal set vectors w.r.t. subset (B), one needs determine
vectors usefulness w.r.t. subset. turn, one needs determine usefulness w.r.t.
simplex. Again, let set V vector . useful w.r.t. simplex,
must useful w.r.t. subset. However, useless w.r.t. simplex, may useful
w.r.t. another simplex. Hence, simplex, identified useful,
need check subsequent simplices. simplices examined,
useless w.r.t. simplices, useless w.r.t. subset. removing useless
vectors w.r.t. subset, one obtains minimal set.

4. Subset Value Iteration
section, first describe value iteration algorithm belief subset (B).
show algorithm able achieve near optimality. Finally, analyze
complexity report empirical studies.
4.1 Belief Subset MDP
subset (B) closed, able define so-called belief subset MDP (or
simply subset MDP). state space chosen subset (B) components
MDP transformed original POMDP (Section 2.3).
difference two MDPs lies state spaces: state space belief
subset MDP subset state space belief space MDP.
4.2 Subset DP Updates
(B)

MDP theory, subset MDP admits following DP update equation Vn
represents nth-step value function.
(B)

Vn+1 (b) = max{r(b, a) +


X

P (z|b, a)Vn (B) ( (b, a, z))} b (B).

(7)

z
(B)

Following equation, implicit DP update computes minimal set Vn+1 represent (B)

(B)

(B)

ing value function Vn+1 Vn
representing Vn . Note domains value
functions belief subset (B). simplicity, step called subset DP update.
Implicit subset DP updates carried standard DP updates. Here, present
two-pass algorithm due conceptual simplicity (Monahan, 1982). constructs
4. simplicity, assume set V contain duplicate vectors. Duplicates removed
simple componentwise check.

133

fiZhang & Zhang

next set vectors two steps enumeration step enumerating possible vectors
reduction step removing useless vectors. following, focus enumeration
step. reduction step, since usefulness vector w.r.t. subset (B),
techniques preceding section used.
(B)
(B)
Given set Vn , vector representing set Vn+1 defined pair
(B)

action mapping set Z observations set Vn . precise,
action mapping , vector, denoted a, , defined follows 5 . S,
a, (s) = r(s, a) +

XX
z

P (s0 |s, a)P (z|s0 , a)z (s0 )

(8)

s0

z mapped vector observation z.
enumerate possible combinations actions mappings above, define
various vectors. vectors form set
{a, |a A, : Z Vn (B) & z, z Vn (B) }.

(9)
(B)

(B)

set denoted Vn+1 . MDP theory, represents value function Vn+1
(B)

set Vn

(B)

represents Vn

.

(B)

(B)

(B)

Lemma 3 set Vn+1 represents value function Vn+1 Vn

(B)

represents Vn

.

DP update works collective fashion directly computes value
functions (B). alternative way conduct DP updates compute value functions individual simplices one one. rationale that, letting DP update
work finer-grained belief subsets, could efficient collective ver (B,a,z)
sion. DP update individual fashion constructs collection {Vn+1 } vector sets
(B,a,z)

(B,a,z)

(B)

given collection {Vn
|a A, z Z} Vn
represents Vn

(B,a0 ,z 0 )
simplex (B, a, z). consider construct set Vn+1
one simplex (B, a0 , z 0 ).
(B,a0 ,z 0 )

Likewise, vector a, Vn+1
defined action mapping .
fact (b, a, z) must (B, a, z) b implies z, z restricted
(B,a,z)
vector set Vn
. altering actions mappings, one obtains following
set:
{a, |a A,

: Z a,z Vn (B,a,z) , & z, z Vn (B,a,z) }.

(10)

differs (8) observation z, mapped vector restricted
(B,a,z)
(B,a0 ,z 0 )
set Vn
. set denoted Vn+1
. obtain minimal representation,
one removes useless vectors set w.r.t. simplex (B, a0 , z 0 ). value function
(B,a0 ,z 0 )
(B)
set Vn+1
induces equal value function Vn
belief simplex
(B, a0 , z 0 ).
5. procedure defining vector actually constructs (n+1)th-step policy tree. (See, e.g., Zhang
Liu 1997, details.)

134

fiRestricted Value Iteration: Theory Algorithms

(B,a,z)

Lemma 4 action observation z, set Vn+1
function
plex.

(B)
Vn+1

simplex (B, a, z)

(B,a,z)
Vn

represents value
(B)

represents Vn

sim-

Although subset DP updates carried either collectively individually,
essentially equivalent terms value functions induced.
(B,a,z)

Theorem 4 Let U = a,z Vn+1

(B)

. b (B), U(b) = Vn+1 (b).

worthwhile note two pairs actions observations, simplices
(B, a1 , z1 ) (B, a2 , z2 ) might disjoint. remarks order
(B,a ,z )
case. First, Theorem 4, b intersection simplices, Vn+1 1 1 (b) =
(B,a ,z )

(B)

Vn+1 2 2 (b). sets represent Vn+1 (B). Second, subset DP
update carried individually, may generate vectors collective version.
(B,a ,z )
(B,a ,z )
two sets Vn+1 1 1 Vn+1 2 2 may contain duplicate vectors.
Finally, note achieve computational savings, sophisticated algorithms
standard DP updates applied subset DP updates. Let us take incremental pruning,
one efficient algorithms, example (Cassandra et al., 1997; Zhang & Liu,
1997). standard incremental pruning, pruning operations w.r.t. belief
space; however, used subset DP updates, pruning operations w.r.t.
belief subsets.
4.3 Analysis
analyze several theoretical properties subset value iteration algorithm. main
results include: value functions generated subset value iteration equivalent
standard value iteration sense; achieve near optimality, value iteration needs
account least belief subset (B); value function generated subset value
iteration used near optimal decision-making entire belief space
algorithm appropriately terminated.
4.3.1 Belief Subset, Value Functions Value Iterations
(B)

Subset value iteration generates series {Vn } value functions. initial value
(B)
function V0
initial V0 standard value iteration (B), subset value
iteration generates series value functions standard value iteration (B).
(B)

Theorem 5 V0
b (B).

(B)

(b) = V0 (b) b (B), Vn

(b) = Vn (b) n

Proof: first consider one DP update computing value function Vn+1 current
Vn DP Equation (2). right hand side, since (b, a, z) must belong subset
(B), notation Vn ( (., ., .)) interpreted value function subset (B) rather
belief space B. Comparing DP Equation (2) belief space B Equation (7)
(B)
belief subset (B), see Vn+1 Vn+1 represent value function (B)
(B)

Vn Vn

.
135

fiZhang & Zhang

theorem true n = 0 given condition. true n > 0 induction.
2

(B)

interestingly, value function Vn
step n used derive value
function Vn+1 standard value iteration. see why, first note subset value function V (B) used define value function (say V ) one-step lookahead operation
follows:
V (b) = max{r(b, a) +


X

P (z|b, a)V (B) ( (b, a, z))} b B.

(11)

z

so-defined V called V (B) -improving value function. Second, comparing Equations
(B)
(B)
(11) (2), see Vn -improving value function actually Vn+1 Vn

equal Vn (B).
Consequently, although subset value iteration works (B), value functions generated standard value iteration derived. sense, say (B) sufficient
belief subset since enables subset value iteration preserve standard value functions
without loss.
Since subset value iteration retains quality value functions, regarded
exact algorithm. One interesting question is, value iteration intends retain quality,
work proper subset (B)? general, answer no. reason follows.
(B)
compute Vn+1 , one needs keep values Vn
belief states (B). Otherwise,
one accounts proper subset B0 (B), proven exists belief state
b B, action observation z (b, a, z) belong B0 .
known value update Vn+1 (b) depends values possible next belief
(B)
states. Due unavailability Vn ( (b, a, z)), value Vn+1 (b) cannot calculated
exactly. Consequently, value iteration works proper subset (B), cannot
exact. words, approximate algorithm. make exact, value
iteration needs consider least (B). sense, subset (B) said minimal
sufficient set.
Informally use Figure 2 illustrate relationship belief subsets value
iteration. figure, circles represent belief sets. minimum belief subset value
iteration retain quality (B), maximum subset belief space B itself.
value iteration works belief subset B0 (denoted dashed circles) (B)
B, quality also retained. However, works proper belief subset (B),
general unable retain quality value functions.

(B)

B

B

Figure 2: relationship belief subsets value iteration

136

fiRestricted Value Iteration: Theory Algorithms

4.3.2 Stopping criterion decision making
Subset value iteration starts initial value function. continues, Bellman
(B)
(B)
residual maxb (B) |Vn (b) Vn1 (b)|, maximum difference two value functions subset (B), must become smaller. MDP theory, quantity falls
(B)
(1 )/(2), value function Vn1 -optimal w.r.t. subset MDP. following,
show -optimality extended entire belief space appropriately
terminating subset value iteration algorithm.
(B)
Let output value function Vn1 . used define policy belief
(B)

state B Equation (3) V replaced Vn1 . policy said
(B)

Vn1 -improving. Note policy prescribes action belief state belief
space. following theorem tells one terminate subset value iteration
(B)
Vn1 -improving policy -optimal belief space.
(B)

(B)

(B)

Theorem 6 maxb (B) |Vn (b)Vn1 (b)| (1 )/(22 |Z|), Vn1 -improving
policy -optimal entire belief space B.
Proof: See Appendix A.
2
theorem important two reasons. First, although subset value iteration outputs
subset value function, -optimal value functions entire space induced
(B)
one-step lookahead operation. Second, implies Vn1 -improving policy optimal condition met. know (B) consists possible belief states
agent encounters initial belief state. However, assumption
initial belief state. may may belong set. theorem means agent
still able select near optimal action initial belief state even
subset. fact, agent always select near optimal action belief state
entire belief space.
Finally, note guarantee -optimality, compared condition
Theorem 1, subset value iteration uses restrictive condition. convenience,
sometimes called strict stopping criterion. contrast, condition standard value
iteration called loose stopping criterion.
4.4 Complexity
put use POMDP, subset value iteration algorithm would take two steps:
determining algorithm bring savings time running subset
value iteration can. first step needs compute |A||Z| determinants |Paz |. Since
complexity computing |Paz | |S|3 , first step complexity O(|A||Z||S|3 ).
polynomial part complexity subset value iteration. second step
much harder first step. known finding optimal policy even
simplified finite horizon POMDP PSPACE-complete (Papadimitriou & Tsitsiklis, 1987;
Burago, de Rougemont, & Slissekno, 1996; Littman, Goldsmith, & Mundhenk, 1998). Recently, proven finding optimal policy infinite-horizon POMDP
incomputable (Madani., Hanks, & Condon, 1999).
compare subset value iteration standard value iteration. Standard DP
updates improve values space B, subset DP updates improve values
137

fiZhang & Zhang

(B)

subset (B). initial set V0
equal initial set V0 standard value iteration,
(B)
(B)
(B) subset B, vectors V1
must V1 , V1
subset
(B)
V1 . Inductively, Vn
subset Vn n. analysis suggests two advantages
subset value iteration subset (B) proper subset belief space B. First,
fewer vectors needed represent value function belief subset.
representational advantage space. Vn+1 , size large |A||Vn ||Z| .
(B)

(B) |Z|

Vn+1 , size large |A||Vn | . Clearly, subset DP update generates fewer
vectors. Second, fewer vectors means lesser degree time complexity since computing
vectors needs solve linear programs. computational advantage time.
However, advantages strongly depend upon size subset (B).
simplex (B, a, z) belief space B DP updates conducted
individual fashion, subset DP updates could |A||Z| times slower standard DP
updates. worst case complexity. Fortunately, discussions previous
section (Theorem 3), know given POMDP able determine whether
selected subset (B) proper subset belief space solving it.
Although Theorem 3 gives condition determine subset value iteration
efficient standard value iteration POMDP, answer question
much savings algorithm bring about, turns difficult
problem theoretical analysis. difficulty lies size set (B)
also vectors representing step optimal value functions. Let us assume
(B) proper subset belief space. imagine least two cases. one case,
iteration step value function useful vectors subset (B),
subset value iteration efficient. case, iteration
step value function useful vectors subset (B), subset value iteration
complexity standard value iteration asymptotic sense. general, given
POMDP, difficult predict vectors scatter around belief subsets
belief space. Consequently, hard predict much saving subset value iteration
algorithm bring POMDP without solving it.
4.5 Empirical Studies
present empirical results two variants designed maze problem
problems standard test-bed subsection. common settings experiments
paper follows. experiments conducted UltraSparc II machine
dual CPUs 256MB RAM. codes written C executed
UNIX operating system Sola 2.6. solving linear programs, use commercial
package CPLEX V6.0. discount factor set 0.95 round-off precision set
106 . stated otherwise, quality requirement set 0.01. use
incremental pruning compute representing sets value functions belief space
belief subsets.
compare performances subset standard value iteration. simplicity,
denote respectively ssVI VI. iteration, compare VI ssVI
two measures: sizes sets representing value functions total time DP updates.
138

fiRestricted Value Iteration: Theory Algorithms

4.5.1 Maze Problem
maze problem specified Figure 3. 10 locations goal location 9.
robot agent execute four move actions change position, optionally look
action observe surroundings declare action announce success goalattainment. move actions achieve intended effects probability 0.8,
might effects probability 0.1 (the agents position remains unchanged)
lead overshooting probability 0.1. Moving maze walls leaves agent
original location. actions change agents position. time point,
robot receives null observation giving useful information all, reads four sensors
reason current position. sensor informs robot whether
wall nothing along direction. figure thick lines stand walls thin lines
nothing (open). instance, agent location 2, ideally string owow (in
order East, South, West North) received. Specific parameters instantiated
relevant empirical analysis. robot required maximize infinite discounted
sum rewards.

1

2

9

10

7

8

3

4

5

6

Figure 3: maze problem
Two variants maze designed test ssVI VI. denoted
maze1 maze2. maze1, ssVI efficient; maze2, ssVI less efficient.
Case I: (B) B
maze1 problem state space 10 locations, action space size 5 (four
move one declare) observation space size 6 (strings four letters).
ideal string received certainty action performed. agent declares
goal location 9, receives reward 1 unit; location 10, receives
reward 1. combinations actions observations lead reward.
collect results Figure 4. first chart figure depicts total time
DP updates log-scale VI loose stopping criterion ssVI strict one
(Section 4.3.2). compute 0.01-optimal value function, VI took 20,000 seconds 162
iterations ssVI strict stopping criterion took 900 seconds 197 iterations.
note ssVI needs iterations still takes much less time. performance
difference big. Moreover, iterations means value function generated
ssVI closer optimality.
surprising result take look matrix Paz action
observation z. know matrix impacts size simplex (B, a, z).
dimension matrix 10 10. entry Paz (i, j) product
transition probability P (sj |si , a) observation probability P (z|sj , a). Let us assume
139

fisizes representing sets(logscale)

Zhang & Zhang

CPU seconds(logscale)

100000
10000
1000
100
10

VI
ssVI

1
0.1
0.01
0 20 40 60 80 100 120 140 160 180 200
number iterations

10000

1000

100

10

VI
ssVI

1
0 20 40 60 80 100 120 140 160 180 200
number iterations

Figure 4: Comparative studies VI ssVI Maze1
observation owow. Hence, possible locations may 2 5. Regardless actions
executed, entries row 2 5 Paz non-zero. Therefore, matrix highly
sparse non-invertible simplex (B, a, z) much smaller B. analysis
holds similarly combinations actions observations. Hence, ssVI accounts
small portion belief space. explains ssVI efficient
VI. addition, expect sets generated ssVI much smaller
VI.
confirmed second chart figure. depicts sizes sets
representing value functions generated ssVI VI iteration. counting
(B)
size Vn , collect sum sizes representing sets |A||Z| simplices.
note iteration VI always generates much vectors ssVI.
sizes curves increase sharply first iterations stabilize. size VI
reaches peak 2466 iteration 11 maximum size ssVI 139 iteration
10. size VI 20 times many ssVI. magnitude consistent
performance difference. sizes stabilize, sizes sets generated
VI around 130 around 50 ssVI.
Case II: (B) = B
problem maze2 designed show ssVI could less efficient VI
selected belief subset (B) equal belief space B. problem state space
10 locations, action space size 6 (four moves, one stay one declare)
observation space size 7 (6 strings null telling nothing). action stay
change agents position. maze2 complications observation model. Due
hardware limitations, move action, probability 0.1, agent receives
wrong report string owow collected owww woww wowo. declare
action executed, agent always receives null observation. addition, agent
executes stay, receives either null observation probability 0.9 ideal string
surrounding locations probability 0.1.
reward model accordingly changed reflect new design considerations.
assume agent needs pay information states. purpose,
140

fiRestricted Value Iteration: Theory Algorithms

sizes representing sets(logscale)

agent executes stay, really nothing thus yields cost (i.e., negative reward).
contrast, move actions always cause cost 2. Depending locations
executes declare, receives rewards costs: location state 9, receives
reward 1; state 10, receives cost 1; otherwise, leads rewards.
stay action attractive yields cost leads useful observation
states small likelihood.
empirical results collected Figure 5. First, note VI ssVI
able run 11 iterations within reasonable time limit (8 hours). first chart
figure presents time costs along iterations. run 11 iterations, ssVI takes 53,000
seconds VI takes around 30,900 seconds. Therefore, ssVI slower VI
problem. However, magnitude performance difference big. explain this, let
us consider matrix Paz action stay observation null. transition matrix
identity state lead null observation probability 0.9
stay executed. Therefore, matrix Paz invertible simplex (B, a, z)
belief space B. ssVI needs account additional simplices
combinations actions observations, ssVI must less efficient VI. explains
performance difference time ssVI VI.

CPU seconds(logscale)

100000
10000

ssVI
VI

1000
100
10
1
0.1
0.01
0

2

4
6
8
10
number iterations

12

10000

ssVI
VI

1000

100

10

1
0

2

4
6
8
number iterations

10

12

Figure 5: Comparative studies VI ssVI maze2
also anticipated ssVI generate vectors VI
(B)
iteration size Vn
defined sum individual sets it.
confirmed demonstrated second chart Figure 5. curve ssVI always
upper side VI. 11th iteration, ssVI generates 3,300 vectors
VI generates around 1,700 vectors.
4.5.2 Experiments Test-Bed
validate performance subset value iteration different problem domains,
collected results algorithm standard test-bed maintained Tony
Cassandra 6 . literature, eight problems commonly referred 4x3CO,
Cheese, 4x4, Part Painting, Tiger, Shuttle, Network, Aircraft. Table 2 presents detailed
6. See URL http://pomdp.org/pomdp/examples/index.shtml

141

fiZhang & Zhang

|S|
|Z|
|A|
VI(Time)
ssVI(Time)
VI(#)
ssVI(#)
subspace

4x3CO
11
4
11
3.52
63.28
4
43/1
yes

Cheese
11
4
7
14.06
85.44
14
32/2
yes

4x4
16
2
4
27.47
85.44
20
42/10
yes

Paint
4
4
2
38.75
85.20
9
22/9


Tiger
2
2
3
82.40
145.21
9
22/9


Shuttle
8
2
3
6130.69
1437.32
208
98/45
yes

Network
7
2
4
13283.15
2810.21
491
201/50
yes

Aircraft
12
5
6
1723193.34
425786.49
2071
3236/428
yes

Table 2: Comparative studies ssVI VI standard test-bed

statistics problems. table, Rows 24 give sizes problem parameters,
namely number states, observations actions. Row 5 6 show CPU seconds
standard subset value iteration algorithms compute 0.01-optimal policy
problem. Row 7 shows number vectors representing 0.01-optimal
value function standard value iteration. experiments, implemented subset
value iteration individual fashion. Row 8, entry takes form /, denoting
total number vectors |A| |Z| simplices maximum number vectors
among simplices subset value iteration terminates. last row shows
whether belief subset (B) proper subset belief space.
discussing performances subset value iteration algorithm, categorize
tested problems three classes. first class, subset (B) actually
belief space. Subset value iteration must less efficient standard value iteration.
reason follows: exists least one belief simplex value iteration
complexity standard value iteration; moreover, subset value iteration
needs account simplices. Example problems tiger paint. Let us take
paint problem instance. results show two simplices
belief space. 0.01-optimal value functions represented 9 vectors,
number vectors representing 0.01-optimal value function
entire belief space. second class tested problems, set (B)
proper subset belief space, meanwhile numbers vectors representing
value functions belief space individual simplices small. Subset value
iteration may efficient standard value iteration overhead
accounting large number simplices. Example problems include 4X3CO, cheese
4X4. Let us take 4X3CO instance. 0.01-optimal value function entire
belief space represented 4 vectors, whereas 0.01-optimal value function
simplex represented 1 vector. Since subset value iteration account
44 simplices, subset value iteration less efficient standard value iteration.
third class tested problems, set (B) proper subset belief space,
meanwhile numbers vectors representing value functions belief space
moderately large. subset value iteration algorithm efficient standard
value iteration algorithm. Examples include shuttle, network aircraft. Let us take
network instance. 0.01-optimal value function belief space represented
142

fiRestricted Value Iteration: Theory Algorithms

491 vectors, whereas 0.01-optimal value function (B) represented less
201 vectors (note duplicates across belief simplices). maximum size
representing sets simplices 50. case, expect savings brought
subset value iteration outweighs overhead accounting simplices.
result shows subset value iteration 5 times faster standard value iteration.
Combining results maze problem, see computational
savings brought subset value iteration vary different problem domains. Theorem
3 used determine whether subset value iteration bring computational
savings POMDP. event belief set (B) proper subset belief
space, magnitude savings needs determined empirical evaluation.

5. Informative POMDPs
section, study special POMDP class, namely informative POMDPs.
POMDP class, natural belief subsets value iteration work with.
show formally define subsets. value iteration belief subsets
described (Zhang & Liu, 1997), focus compare algorithm
general subset value iteration developed previous section.
5.1 Motivation
noted authors, reality agent often good, although imperfect, idea
locations (Roy & Gordon, 2002). instance, mobile robots real world
systems local uncertainty, rarely encounter global uncertainty. Let us exemplify
using maze Figure 3. Suppose time point agent receives string
four letters certainty. total, 6 observations, owww, owow, owoo, wwow,
wowo woww regardless executed actions. enumerate possible observations
set locations agent receives observations, end
following table.
observations
owww
owoo
wowo

states
{1 }
{ 3,4 }
{ 7,8}

observations
owow
wwow
woww

states
{ 2, 5}
{6}
{ 9,10 }

hand, strings used infer agents locations. instance,
string owoo received, world must location 3 4. Hence, observation owoo
restricts world small range world states. fact, observation restrict
world two states although world ten. reason, POMDP
said informative.
general, agent perceives world via observations. Starting state,
agent executes action receives observation z, world states categorized
two classes observation model: states agent states cannot.
Formally, former {s|s P (z|s, a) > 0}. set denoted az . use
set define informativeness. [a, z] pair said informative size |S az |
much smaller |S|. observation z informative [a, z] informative every
action giving rise z. POMDP informative observations informative.
143

fiZhang & Zhang

informative POMDPs, since observation restricts world small set
states, agent knows world cannot state outside small set.
words, states outside set, agent zero beliefs. Consequently,
observation also restrict belief states belief subset.
5.2 Belief Subset Selection
informative POMDPs, select belief subset (B) before. Combining informativeness assumption Corollary 1, know (B) proper subset belief
space. So, value iteration (B) carries space time savings. section,
choose alternative belief subset value iteration. Compared subset (B),
subset choose yields several advantages. First, conceptually simple
geometrically intuitive. Second, facilitates employing low dimensional representation
vectors. Third, may lead additional savings time observation models
POMDP independent actions. latter two advantages shown later.
define belief subset (say (B)), first define subset (B, a, z) action
observation pair. Then, belief subset (B) formed taking union (B, a, z)
action observation pairs. specific,
X

(B, a, z) = {b|

b(s) = 1.0, az , b(s) 0}

(12)

sS az



(B) = a,z (B, a, z).

trivial see (B, a, z) belief simplex. proven belief state
b, (b, a, z) must (B, a, z). Therefore, (B, a, z) subset (B, a, z). Consequently,
(B) subset (B). summarized lemma below. lemma useful
discuss value iteration algorithm working belief subset (B).
Lemma 5 POMDP, (B) (B).
interest compare -simplex -simplex pair z. Although
simplices generated list belief states, -simplex intuitive geometric
meaning. belief state basis (B, a, z) unit vector, i.e., probability
mass one state. Therefore, belief state basis must boundary point
belief space. contrast, belief state basis -simplex interior point.
See Figure 1 example, A2 , A3 interior points A1 boundary point
belief space.
5.3 Value Iteration (B)
theoretical perspective, feasibility conducting value iteration (B)
justified Lemma 1. Combined Lemma 5, subset (B) closed set. Hence,
MDP theory applicable defining DP update equation. discussions
relationship value iteration Section 4.3, value iteration working (B) retains
quality value functions.
exploit informative feature value iteration (B). briefly outline
subset value iteration algorithm refer readers detailed description (Zhang
144

fiRestricted Value Iteration: Theory Algorithms

& Liu, 1997). basic idea reduce dimensions vectors representing sets
value functions. Note pair [a, z], since beliefs states outside az zero,
vector representing set value function simplex (B, a, z) needs
|S az | components. individual fashion, DP update (B) computes collection
(B,a,z)
(B,a,z)
(B,a,z)
{Vn+1 } collection {Vn
} Vn
nth-step value function
az
vectors |S | dimensions. procedure conducting DP update parallel
Section 4.2 except (B, a, z) replaced (B, a, z). enumeration
step, building vector belief simplex (B, a0 , z 0 ) using Equation (10), need
0 0
define components corresponding set z . reduction step,
0
0
(B,a ,z )
constructed set Vn+1
, pruning procedure called remove useless vectors obtain
minimal representation set. Note lower dimension feature also used
cut number variables setting linear programs.
Interestingly, DP updates (B) account larger subset (B).
Hopefully, since DP updates (B) explicitly employ economy representation,
could efficient. addition, DP updates (B) another advantage
event observation models POMDP independent actions, i.e.,
probabilities P (z|s, a) independent a. Hence, given observation z, simplices
(B, a, z) actions. Therefore, DP updates (B) account
|Z| -simplices. However, DP updates (B) usually need account |A||Z|
-simplices observation determines different -simplices combined
different actions.
5.4 Empirical Studies
conducted experiments compare VI, ssVI infoVI, refers value iteration exploiting low-dimension feature. experiments maze1 (defined Section
4.5) found elsewhere (Zhang & Zhang, 2001; Zhang, 2001). results, together
existing results (Zhang & Liu, 1997), showed value iteration (B)
significantly efficient standard value iteration. reference, mention
feasible integrate point-based technique value iteration (B) order
take advantage reducing iteration number accelerating iterative steps
(Zhang & Zhang, 2001b). demonstrate this, include results 96-state POMDP
Appendix B.
5.5 Restricted Value Iteration Dimension Reduction
compare value iteration algorithms previous section.
comparison, would like emphasize working belief subsets imply
working low-dimensional vectors.
Although algorithms work belief subsets, mechanisms exploited achieve
computational gains different. general value iteration works belief
subset (B) dimension representing vectors number states,
whereas value iteration (B) works superset (B) dimension
vectors smaller number states. facilitate demonstrating reduced
belief set low-dimensional representation respectively contribute computational gains, experimented carefully designed maze problem amenable
145

fiZhang & Zhang

algorithms. However, worth pointing working reduced belief set
mean vectors represented low dimensions. illustrate
point continuing discussions example Section 3.3. example shows
primary advantage value iteration (B) stems size chosen
belief subset rather dimension reduction representing vectors.
Example (Continued) POMDP example presented Section 3.2, subset (B)
consists two line segments entire belief space. Clearly value iteration
(B) efficient standard value iteration. However, one runs value iteration
(B) POMDP anyhow, algorithm less efficient standard value
iteration algorithm. follows (1) set ai zj equal set states
action ai observation zj second assumption, (2) -simplex actually
belief space definition Equation (12). solve POMDP,
susbet value iteration algorithm definitely better choice value iteration
algorithm informative POMDPs.
2

6. Near-Discernible POMDPs
section, study near-discernible POMDPs. POMDP class, develop
anytime value iteration algorithm working growing belief subsets.
6.1 Motivation
discernible POMDP assumes uncertainty world states
vanishes particular action executed observations pertain action fully
reveal identities world (Hansen, 1998). research near-discernible POMDPs
motivated two aspects. One arises origin applying POMDP
framework planning uncertainty. achieve goal location, agent
change positions performing goal-achieving actions also reason
surroundings performing information-gathering actions. However, one time point
agent cannot simultaneously move positions observe environments. instance,
information-gathering action performed, agent cannot move positions meanwhile. aspect motivating concept near-discernibility arises existing
research community. Near-discernible POMDPs generalize discernible POMDPs
even information-gathering action performed, agent get rough,
rather exact, idea world states uncertainty vanishes sense.
revise maze problem fix ideas first motivation. action space consists
six actions: four moving actions, look declare. move actions declare
performed, observation null received agent gets information all.
look performed, ideal string received agent gets imperfect information since
different locations might yield string. one hand, achieve goal location,
agent change positions. hand, declare goal attainment
confidence, perform look reason environment. Arbitrarily declaring
goal attainment leads penalty. Consequently, time point agent faces
problem choosing move look.
146

fiRestricted Value Iteration: Theory Algorithms

note subset value iteration algorithm usually yields computational advantage near-discernible POMDPs. give example subset (B)
belief space B assumptions. Suppose maze square grid.
Locations numbered row indices locations increase
left right. assume move action achieves intended effects high likelihood, may effect (i.e., agents location remains unchanged) may lead
overshooting small probability. assumptions, transition matrix
action east upper-triangular invertible. location null received
positive probability move, transformational matrix Peast,null invertible.
Theorem 3, belief subset (B, east, null) equal belief space B.
solution near-discernible POMDPs rests intuition agent needs
interleave goal-achieving actions information-gathering actions. typical sequence
executed actions consist several goal-achieving actions informationgathering action. difficulty frequently agent execute informationgathering action. section, consider action observation sequences containing goal-achieving actions incrementally. show sequences
used determine belief simplices. sequences added, union belief
simplices grows. following, give technical preparations describe
algorithm designed near-discernible POMDPs. order put discussions
general context, shall use information-rich information-poor actions instead
information-gathering goal-achieving actions respectively.
6.2 Histories, Belief Subsets Value Functions
history sequence ordered pairs actions observations. usually denote
history h. number pairs actions observations referred length
history. history length l denoted [a1 , z1 , , al , zl ]. agents initial belief
state b history h length l realized, belief state updated
time step. notation (b, h) denotes belief time point l. set (B, h)
defined bB (b, h), consisting possible belief states agent
step l starts belief history h realized. Note h length 1(say
h = [a, z]), (B, h) degenerates previous notation (B, a, z).
Lemma 6 history h, belief subset (B, h) simplex.
set histories usually denoted H. belief subset (B, H) denotes union
simplices histories set H, i.e., hH (B, h). Value functions simplex
(B, h) belief subset (B, H) referred V (B,h) V (B,H) respectively. Given
set V (B,h) representing value function V (B,h) , procedure simplexPrune(V, B (B,h) )
Table 1 computes minimal representation V (B,h) . context history,
occurrences basis B (B,a,z) replaced B (B,h) .
6.3 Space Progressive Value Iteration
describe space progressive value iteration (SPVI) algorithm. anytime algorithm, SPVI begins belief subset gradually grows it. certain stopping
147

fiZhang & Zhang

criterion met, SPVI terminates returns set vectors agents decision making.
6.3.1 Algorithmic structure
SPVI interleaves value iteration (computing value function belief subset) subset
expansion (expanding current belief subset larger one). belief subsets SPVI
introduced sets histories. Subset expansion achieved incorporating
histories. convenience, set histories determining i-th belief subset denoted
Hi . belief subset determined Hi (B, Hi ). value function constructed
SPVI Hi V (B,Hi ) .
pseudo-code Table 3 implements SPVI. set histories H0 (and therefore
belief subset (B, H0 )), value function V (B,H0 ) quality precision initialized
line 1. step regarded 0th-step expansion belief subset. Note
set initial value function minimum reward pairs actions states.
(This convergence issue discussed later.) Value iteration current subset
(B, Hi ) conducted line 3, belief subset expanded subset (B, Hi+1 )
constructing superset Hi+1 current set Hi line 4. Value function V (B,Hi )
current belief subset set initial value function next subset
line 5. stopping condition satisfied line 7, SPVI goes next iteration;
otherwise, terminates returns latest value function V (B,Hi1 ) .
ensure efficiency SPVI, initial belief subset chosen small.
end, set H0 {[a, z] | AIR , z ZIR } AIR set information-rich
actions ZIR set observations led actions. subset (B, H0 )
small due discernability property.
sequel, discuss value iteration belief subset, subset expansion
stopping criterion detail.
6.3.2 Value iteration belief subset
Given set V vectors, set H histories precision threshold , value iteration
computes improved value function belief subset (B, H). accomplished
conducting sequence DP updates. following, discuss implicit DP updates,
convergence issue stopping criterion value iteration step.
implicit DP update computes new value function current one belief
subset (B, H). Let Uj (U0 = V) denote j-step value function. Thus, DP update
computes value function Uj+1 Uj . procedure computing Uj+1 Uj parallel
collective DP update Section 4. particular, defining vector a, given
(B)
action mapping Equation (9), occurrences Vn
replaced Uj .
enumerating actions mappings, defined vectors form set Uj+1 . minimal
representation obtained removing useless vectors w.r.t. subset (B, H).
convergence issue arises subset (B, H) may closed set.
guarantee convergence value iteration, set Uj+1 union set Uj+1
Uj DP update. Together fact initial value function set
minimum reward actions states, sequence {Uj } monotonically
increases terms induced value functions. hand, value functions
148

fiRestricted Value Iteration: Theory Algorithms

SPVI:
1. 0, initialize H0 , V (B,H0 ) minsS,aA r(s, a), (1 )/2
2.
3.
V (B,Hi ) subsetVI(V (B,Hi ) , Hi , )
4.
< Hi+1 , (B, Hi+1 ) > expandSubset(V (B,Hi ) , Hi )
5.
V (B,Hi+1 ) V (B,Hi )
6.
ii+1
7. (stopping condition met)
8. Return V (B,Hi1 )
subsetVI(V, H, ):
1. j 0, U0 V
2.
3.
Uj+1 subsetDPUpdate(Uj , (B, H))
4.
Uj+1 Uj+1 Uj
5.
j j+1
6. ( maxb (B,H) |Uj (b) Uj1 (b)| )
7. Return Uj1
expandSubset(V, H)
1. H0 H
2. set V
3.
.history maximal H .action information-poor
4.
[a, z] AIP ZIP
5.
H0 H0 {[h, a, z]}
0
6. Return < H , (B, H0 ) >
Table 3: Space progressive value iteration (SPVI)
sequence upper bounded optimal value function. Consequently, value iteration
(B, H) must converge. result, Bellman residual value functions,
maxb (B,H) |Uj+1 (b)Uj (b)|, becomes smaller (B, H) value iteration continues.
residual falls threshold , value iteration terminates.
value iteration step implemented procedure subsetVI Table 3. Given
set V vectors, set H histories threshold , procedure computes improved
value function belief subset (B, H). Value function U0 set input set V
line 1. new value function Uj+1 computed DP update line 3. guarantee
convergence, Uj+1 set union Uj Uj+1 line 4. stopping criterion
tested line 6. met, latest value function Uj1 returned.
6.3.3 Subset expansion
Given set V vectors set H histories, subset expansion step expands belief
subset (B, H) larger one. achieved generating superset H0 H. new
149

fiZhang & Zhang

belief subset (B, H0 ) thus superset (B, H). Hence, key subset expansion
generate history set H0 . following, propose two approaches generating
history set using intuition near-discernible POMDPs. approaches generate
new histories exploiting vectors V. begin analysis vectors
set V show use generate histories.
Let vector set V. Remember defined pair action
mapping . convenience, action said associated action .
addition, useful set V w.r.t. belief subset (B, H), must exist history
h H useful w.r.t. belief simplex (B, h). history h said
associated history . associated history vector used generate new
histories extending history, i.e., appending pairs informative-poor actions
observations history. Let AIP set information-poor actions ZIP
set observations led actions. Extending history h results set
{[h, a, z]|a AIP , z ZIP }. set contains |AIP ||ZIP | histories. history set
called extension history h.
generate H0 set H set V, one generic approach works follows.
vector set V examined. associated history long associated
action information-poor, produce extensions associated history. extension
added H0 H0 . (The reason associated action vector
information-rich associated history sufficiently long.) ensure H0
superset H, set H0 H beginning. Apparently, approach generating
histories suffers exponential increase size |H0 | |AIP | |ZIP |.
worst case vectors V associated information-poor actions, size
|H0 | |H||AIP ||ZIP |. Consequently, i-step subset expansion, |Hi+1 |
large |AIR ||ZIR |(|AIP ||ZIP |)i |AIR ||ZIR | size initial history set.
alleviate problem, use heuristic approach generating H0 hope
size H0 increases moderately. exhaustive approach extends histories
associated vectors prescribing informative-poor actions. heuristic approach
extend histories. Instead extends maximal histories set
H. (A history said maximal set none extensions set.)
change made approach. indicated experiments,
restriction effectively cut size history sets. Nonetheless, heuristic
approach shares worst-case complexity exhaustive approach.
subset expansion step implemented procedure subsetExpansion Table
3. Given set V vectors set H histories, computes expanded set H0
expanded belief subset (B, H0 ). set H0 initialized H line 1. vector
V line 2, associated history maximal H action information-poor
(line 3), extensions associated history added H0 (line 5). expanded
set H0 also expanded belief subset (B, H0 ) returned line 6.
6.3.4 Stopping criterion Decision-Making
anytime algorithm, SPVI terminated hard deadline reached. Another
stopping criterion interest set follows. Given sufficiently large amount
time, SPVI would account many histories possible. (near) optimal policy
150

fiRestricted Value Iteration: Theory Algorithms

POMDP requires information-rich actions executed sequence informationpoor actions, SPVI able compute value function belief subset,
consists possible belief states agent encounters guided
policy. sufficiently many expansions history sets hence belief subsets,
vector associated maximal history prescribes information-rich action.
vectors representing set prescribe information-rich actions, SPVI terminates.
(near) optimal policy desired structure sequence actions, output value
function near optimal final belief subset.
SPVI terminates, value function V (B,Hi1 ) used decision making.
Similarly Equation (3), V (B,Hi1 ) -improving policy defined belief space.
6.3.5 Efficiency SPVI
efficiency SPVI depends selected belief subsets. belief subsets
close belief space size, SPVI must inefficient. Fortunately, approach
belief subset expansion ensures initial belief subset small subsequent
subsets grow slowly. First, since H0 set pairs information-rich actions
observations, initial belief (B, H0 ) relatively small. Second, subsequent belief
subsets (B, Hi ) grow quickly. reason follows. extending history,
information-poor pairs added end. Hence, first action observation pair
histories set Hi must information-rich. Therefore, history h set Hi ,
(B, h) small size. Meanwhile, due heuristic generating history sets, sizes
|Hi | would increase fast. characteristics make SPVI efficient compared
standard value iteration algorithm.
Although analysis empirically confirmed experiments below,
worthy mention worst case number belief simplices grows exponentially
number |AIP ||ZIP |. Since history determines belief simplex, worst
case number belief simplices i-step subset expansion
number histories, i.e., |AIR ||ZIR |(|AIP ||ZIP |)i (see third paragraph Section 6.3.3).
6.4 Empirical Results
Since SPVI works anytime manner, primary interest demonstrate
quality generated value functions varies time cost. However, availability
optimal solutions strongly depends tractability problems. near
optimality available, compare directly value function generated SPVI
simulations. Otherwise, simply compare value functions SPVI
approximate algorithm QMDP (Littman et al., 1995; Hauskrecht, 2000). Although
comparison strict formal sense, provide clues quality value
functions.
report results two variants base maze problem office navigation
problem. one variant, SPVI terminated finite number iterations output
value function near optimal; variant, SPVI quickly find high-quality
value function time goes (Zhang & Zhang, 2001a; Zhang, 2001). rest
section, report results office navigation problem.
151

fiZhang & Zhang

environment modeled floor plan authors home department.
layout shown Figure 6. 35 states: 34 locations plus one terminal state.
action space size 6 (four move, one look one beep replacing declare
maze problem). action except look leads null observation. introduce
observations, note figure, black bars represents doors grey bars represent
walls display boards. look action yields observation strings four letters
location indicating, four directions, door (d), empty
wall (w), wall display board (b), nothing (o). total, 22 different
strings. Hence, plus null observation, observation space size 23. Transition
probabilities moves specified identically maze problem. Neither look
beep changes states environment. location, look produces ideal
string location probability 0.75. probability 0.05, produces null
observation. Also probability 0.05, produces string ideal
location differs ideal string current location 1 character.
robot receives reward 50 beeping location 22 reward -10
beeping location (we dont want robot make lot noise). move
actions bring reward -2 lead robot bumping walls doors.
rewards otherwise. reward look action always -1. robot
needs get location 22 beep someone main office come
hand robot mail.

3
2
1

6

11

16

19

5

10

15

18

9

14

17

4

7

8

12

13

Main
Office

20

21

22

23

34

25

26

27
28
29
34

33

32

30
31

Figure 6: HKUST-CSD office environment
conduct simulations generated value functions existing exact
algorithm find near optimal value function. simulation consists 1000 trials.
trial starts random initial belief state allowed run 100 steps.
average reward across trials used measurement quality policies
derived value functions.
Figure 7 presents results quality time costs. see SPVI
found policy whose average reward 19.6 80,000 seconds. SPVI manually
terminated running 24 hours. found algorithm conducted three
steps subset expansion. data, first second expansion steps,
152

fiRestricted Value Iteration: Theory Algorithms

rewards simulation 18.4. far 19.6 obtained third expansion
step although difficult say close polices optimal. Compared
solutions generated QMDP, policies generated SPVI clearly better.

Quality Value Functions

20
19
18
17
16
15

SPVI
QMDP

14
13
12
100

1000
10000
Time Seconds (in log-scale)

100000

Figure 7: Performance SPVI office navigation problem
reference, Table 4 gives detailed statistics number histories, iterations
vectors subset expansion. note number iterations
third column: conducting value iteration subsets, also use point-based
improvements (Zhang, 2001). column, number point-based steps excluded.
fourth column number vectors provides idea SPVI takes long
time problem. generates great number vectors.
third expansion, uses 7,225 vectors represent value function belief subset.

i-step expansion
0
1
2

histories#
53
86
131

iterations#
4
6
7

vectors#
464
5178
7225

rewards
18.44
18.44
19.65

time
163
16157
78183

Table 4: Statistics HKUST-CSD environment SPVI

7. Related Work
paper, propose restricted value iteration algorithms accelerate value iteration
POMDPs. Two basic ideas behind restricted value iterations (1) reducing complexity DP updates (2) reducing complexity value functions. section,
discuss related work two categories. addition, give overview special
POMDPs literature algorithms exploiting problem characteristics.
153

fiZhang & Zhang

7.1 Reducing Complexity DP Updates
broad sense, approaches reducing complexity DP updates roughly categorized two classes: approaches conducting value updates (stationary) belief subset
approaches conducting value updates growing subset, although boundary
two classes ambiguous cases.
first class includes family grid-based algorithms, algorithms based reachability analysis, algorithms using state-space decomposition others. Grid-based algorithms
update values finite grid extrapolate values non-grid belief states (Lovejoy,
1991; Hauskrecht, 1997; Zhou & Hansen, 2001). However, guarantee optimality,
grid size often exponential dimension state space. tackle POMDPs
large state spaces, reachability analysis generally applicable technique. agent
informed initial belief, belief states encounter form finite set case
finite decision horizon. belief states structured decision tree AND/OR
tree (Washington, 1997; Hansen & Ziberstein, 1998; Hansen, 1998; Bonet & Geffner, 2000).
Although sometimes near optimality achieved initial belief state (Hansen,
1998), algorithms cited articles cannot applied case unknown
initial belief. State-space decomposition effective way alleviate curse dimensionality. approach successfully applied MDPs (Dean & Lin, 1995; Dean,
Givan, & Kim, 1998; Parr, 1998; Koller & Parr, 2000). Typically, solve MDP, one
solves number small MDPs uses solutions approximate original
MDP. However, state-space decomposition approach cannot directly generalize
POMDP context inherent difficulty incurred continuum belief
space.
theory algorithms restricted value iteration significant differences
approaches. well chosen belief subset, restricted value iterations
achieve convergence optimality. differs grid-based algorithms computes
vector-based representations value functions. Despite difference, possible
grid-based algorithms benefit theory belief subset selection. possibility
yet investigated. instance, choosing grid points, one choose
within belief subset (B). reason follows. Since belief states outside set
never reachable, values directly contribute value updates beliefs
grid. regard differences aforementioned algorithms ours,
approach assumption agents initial belief, although belief subset
chosen via reachability analysis. algorithm differs decomposition techniques
solves reformulated MDP instead set small MDPs.
Approaches conducting value updates growing belief subset include real-time dynamic programming (RTDP) POMDP context (Barto, Bradtke, & Singh, 1995;
Geffner & Bonet, 1998), synthetic projection algorithm (Drummond & Bresina, 1990)
envelope algorithm Plexus planner MDP context (Dean et al., 1993).
Naturally, run anytime algorithms. RTDP, value updates carried
belief subset, grows agent explores belief space. main difference
SPVI algorithms expand belief/state subset
choose beliefs/states value updates. subset expansion, SPVI adds
belief simplices, often contains infinite number belief states,
154

fiRestricted Value Iteration: Theory Algorithms

algorithms mostly add finite number belief states. (It also noted reachability
analysis used expansions.) value updates, SPVI improves values
entire belief subset, algorithms typically select limited number beliefs
states current subset.
7.2 Reducing Complexity Value Functions
Another idea behind restricted value iteration concerned representational complexity value functions. Intuitively, representing set value function belief
subset contains fewer vectors value function belief space.
fact observed (Boutilier & Poole, 1996; Hauskrecht & Fraser, 1998),
POMDPs represented compactly. states depicted set variables,
classified observable variables hidden variables. also noted
belief states cannot reached certain combinations observable variables hidden variables. fact exploited approximating solution medical
treatment example (Hauskrecht & Fraser, 1998). Recent work along thread includes
state-space compression technique exploiting representational advantage (Poupart &
Boutilier, 2002), technique Principle Component Analysis (PCA) aiming reducing complexity value functions (Roy & Gordon, 2002). However, unclear whether
feasible combine state-space compression subset value iteration know
conduct value iteration belief space induced compressed state space.
7.3 Solving Special POMDPs
Since solving POMDP generally computationally intractable, advisable study
POMDPs special characteristics. hope characteristics may exploited find near optimal solutions efficiently. Special POMDPs examined
literature include regional-observable POMDPs (Zhang & Liu, 1997), memory-resetting
discernible POMDPs (Hansen, 1998), even-odd POMDPs (Zubek & Dietterich, 2000)
generalized near-discernible POMDPs (Zhang, 2001). Interestingly, POMDPs
assume existence informative actions observations somehow agent
able get information world. following, briefly discuss
assumptions behind informative POMDPs near-discernible POMDPs review existing work closely related them. concluding subsection, also mentioned
couple extensions current work.
informative POMDP assumes observation restricts world small
set states. assumption validated problem instances compact representations state space. literature, POMDP examples actually informative
POMDPs. One example slotted Aloha protocol problem (Bertsekas & Gallagher, 1995;
Cassandra, 1998a), state system consists number backlogged messages channel status. channel status observable possible assignments
form observation space. However, system access number backlogged
messages. maximum number backlogged messages set n possible values channel status, number states n. particular assignment
channel status restrict system states n. similar problem
155

fiZhang & Zhang

characteristic also exists non-stationary environment model proposed reinforcement
learning (Choi, Yeung, & Zhang, 1999).
regional observable POMDP assumes point time agent restricted
handful world states (Zhang & Liu, 1997). assumption leads value iteration
algorithm works belief subset also exploits low dimensional representation vectors. used algorithm solve informative POMDPs. However, would
like discuss several differences. First, conceptually assumptions different
two POMDP classes. regional observable POMDPs, agent restricted set
states (i.e., region), states set geometrically neighboring ones. However, informative POMDPs, agent restricted set states, states
set obtained formally analyzing observation model POMDP. possible
states set spatially distant one another. Second, algorithms
two POMDP classes work quite different way. ease presentation, use infoVI
roVI respectively denote value iteration informative POMDPs
regional observation POMDPs. infoVI, number state sets product
number actions number observations, roVI, number regions
subjectively chosen. addition, observations roVI augmented. augmented
observation consists original observation specific region. So, number augmented observations product number original observations number
regions. Hence, roVI account many observations infoVI.
fact useful comparing efficiency infoVI roVI. Imagine happens
roVI works region system, consists state sets defined infoVI,
informative POMDP. infoVI accounts fewer observations roVI,
efficient. Finally, quality value function returned infoVI
guaranteed entire belief space terminates strict stopping criterion.
However, quality value function roVI original description problematic
even considered belief subset.
POMDP class examined paper near-discernible POMDPs. near
discernible POMDP assumes actions classified information-rich ones
information-poor ones. assumption reasonable several realistic domains. first
domain path planning problems (Cassandra, 1998a). actions categorized
goal-achieving information-gathering ones, discussed earlier. Another application
domain machine maintenance problems (Smallwood & Sondik, 1973; Hansen, 1998),
agent usually execute following set actions: manufacture, examine,
inspect replace. Among actions, inspect information-rich remaining
three actions information-poor.
near discernible POMDP generalization memory-resetting (discernible)
POMDP, assumes exists actions resetting world unique state.
actions performed, agent knows world must definite state.
initial belief state known optimal policy must execute one actions periodically, number belief states agent visits finite. Accordingly, DP updates
finite set beliefs much cheaper. However, discernibility assumption
relaxed, agent may visit infinite number states DP updates become
expensive. therefore developed anytime algorithm seeking tradeoff
solution quality size belief subset.
156

fiRestricted Value Iteration: Theory Algorithms

also experimented one extension using SPVI approximate solutions
general POMDPs (Zhang, 2001). approximation scheme employs thresholding
technique. Given POMDP threshold, POMDP transformed new one,
differs original one observation model. observation model
transformed POMDP obtained ignoring probabilities (in original model) less
threshold 7 . transformed POMDP near discernible, solution
found SPVI used approximate original POMDP. designed
another maze problem informative action/observation pair therefore
expected amenable SPVI (Zhang, 2001). However, transformed POMDP
amenable SPVI. experiments show SPVI quickly find high quality solution
transformed POMDP. another case, transformed POMDP informative,
algorithm exploiting low dimensional representations informative POMDPs
applied.

8. Conclusions
paper, studied value iterations working belief subset. applied reachability
analysis select particular subset. subset (1)closed actions lead
agent belief states outside it; (2)sufficient value function defined
extended belief space; (3)minimal value iteration needs consider
least subset intends achieve quality value functions. subset
closed enables one formulate subset MDP. addressed issues representing
subset pruning set vectors w.r.t. subset. described subset value
iteration algorithm. given POMDP, whether subset proper determined
priori. case, subset value iteration carries advantages representation
space efficiency time. also studied informative POMDPs near-discernible
POMDPs. informative POMDPs, natural belief subsets value iteration
work with. near-discernible POMDPs, developed anytime value iteration
algorithm seeking tradeoff policy quality size belief subsets.

Acknowledgments
Research partially supported Hong Kong Research Grants Council grant
HKUST6088 / 01E. authors thank Tony Cassandra Eric Hansen sharing us
programs. first author would like thank Eric Hansen in-depth discussions
belief subset selection low dimensional representation, Judy Goldsmith valuable
comments earlier writeup ideas paper. also grateful three
anonymous reviewers provided insightful comments suggestions paper.

Appendix A. Proofs
Theorem 2 pair [a, z], subset (B, a, z) simplex.
7. complete definition approximate observation model, one needs re-normalize model
parameters action state, probabilities observations sum 1.0.

157

fiZhang & Zhang

Proof: Suppose bi belief state bi (s) = 1.0 = si 0 otherwise.
seen {b1 , b2 , , bn } basis belief space B. belief state b(=
P
(b(s1 ), b(s2 ), , b(sn )) represented ni=1 b(si )bi .
Let k cardinality set { (bi , a, z)|P (z|bi , a) > 0}. Without loss generality,
enumerate set { (b1 , a, z), , (bk , a, z)}. suffices show (B, a, z) =
( (b1 , a, z), (b2 , a, z), , (bk , a, z)). prove it, prove:
(1) (B, a, z) ( (b1 , a, z), (b2 , a, z), , (bk , a, z))
(2) ( (b1 , a, z), (b2 , a, z), , (bk , a, z)) (B, a, z).
First, prove (1). suffices show belief state b0 (B, a, z) must belong
simplex . Since b0 (B, a, z), must exist belief state b B
b0 = (b, a, z). define constants follows.
{1, , k}, Cbi probability observing z action executed
P
belief state bi . Formally, Cbi = s0 ,s P (z|s0 , a)P (s0 |s, a)bi (s).
Cb probability observing z action performed b. Formally, Cb =
P
0
0
s0 ,s P (z|s , a)P (s |s, a)b(s).
{1, , k}, define = Cbi /Cb .
P

Given constants, going prove b0 = (bi , a, z). true, i.e., b0
represented convex combination vectors basis, (1) proven.
start b0 = (b, a, z). (b, a, z) replaced definition, state s0 ,
b0 (s0 ) =

1 X
P (z|s0 , a)P (s0 |s, a)b(s)
Cb

definition belief state bi , rewrite equation
b0 (s0 ) =

1 X X
P (z|s0 , a)P (s0 |s, a)bi (s).
Cb i{1,,k}

Trivially,
X

b0 (s0 ) =

1 X
Cb

P (z|s0 , a)P (s0 |s, a)bi (s)



Cbi

Cbi



.

definition (bi , a, z), rewriting equation,
b0 (s0 ) =

X Cb


(



Cb

) (bi , a, z)(s0 ).

definition , equation yields
b0 (s0 ) =

X

(bi , a, z)(s0 ).



158

fiRestricted Value Iteration: Theory Algorithms

b0 (bi , a, z) regarded column vectors, equation means
b0 =

X

(bi , a, z).



Therefore, prove belief state b b0 = (b, a, z), b0
represented convex combination vectors basis. means b0 must
simplex .
prove (2), prove belief state b0 simplex must subset
(B, a, z). suffices show exists belief state b B b0 = (b, a, z).
P
Since b0 , must exist set nonnegative b0 = ki=1 (bi , a, z).
replace (bi , a, z) definition, then: state s0 ,
0

0

b (s ) =
denote

X


P

P
P (z|s0 , a)P (s0 |s, a)bi (s)
P
.
0
0
s0 ,s P (z|s

0
0
s0 ,s P (z|s , a)P (s |s, a)bi (s)

b0 (s0 ) =

XX


, a)P (s |s, a)bi (s)

constant Cbi ,

P (z|s0 , a)P (s0 |s, a)bi (s)




.
Cbi

Exchanging summation order making use definition bs (i),

X
P (z|s0 , a)P (s0 |s, a).
b0 (s0 ) =
C
bs

define belief state b follows: s,
/Cbs
.
b(s) = X
/Cbs


seen

P
P (z|s0 , a)P (s0 |s, a)b(s)
.
b (s ) = P
0
0
0

0

s0 ,s P (z|s

, a)P (s |s, a)b(s)

Therefore, proved b0 exists belief state b b0 = (b, a, z).
Consequently, b0 (B, a, z).
2
(B)

(B)

(B)

Theorem 6 maxb (B) |Vn (b)Vn1 (b)| (1 )/(22 |Z|), Vn1 -improving
policy -optimal entire belief space B.
Proof: suffices show maxbB |Vn+1 (b) Vn (b)| (1 )/(2). b B,
=






|Vn+1 (b) Vn (b)|
P
P
(B,a,z)
(B,a,z)
| maxa {r(b, a)+ z Vn
( (b, a, z))} maxa {r(b, a)+ z Vn1
( (b, a, z))}|


P
P
(B,a ,z)
(B,a ,z)




|r(b, ) + z Vn
( (b, , z)) r(b, ) z Vn1
( (b, , z))|
P
(B,a ,z)
(B,a ,z)


| z (Vn
( (b, , z)) Vn1
( (b, , z)))|
(B,a ,z)
(B,a ,z)
|Z| maxz |Vn
( (b, , z)) Vn1
( (b, , z))|
|Z|(1 )/(22 |Z|)
(1 )/(2)
159

(1)
(2)
(3)
(4)
(5)
(6)

fiZhang & Zhang


Step (1), value functions replaced definitions;
(B)

Step (2), Vn
improving;

(B)

-improving action b necessarily Vn1 -

Step (5), given condition used;
steps trivial.

2

Appendix B. Informative POMDPs: Elevator Problem
appendix describes 96-state informative POMDP empirical results value
iteration (B). problem adapted existing research (Choi et al., 1999).
purpose show restricted value iteration able solve larger POMDPs
standard value iteration.
Problem Formulation
elevator operates two-floor residential building. three patterns
passengers arrival: high arrival rate first floor low second floor; low
arrival rate first floor high second floor; equal arrival rates. time varies
morning night day, patterns change according probability
distribution. keep track pick-up drop-off requests, elevator sets four
buttons control panel: two buttons record pick-up drop-off requests
first floor, two buttons keep information second floor. elevator
also aware floor on. order fulfill requests floor, elevator
first moves upwards downwards reaches floor; then, elevator stays
floor passengers finish entering exiting. objective elevator
minimize certain penalty cost long run.
problem formulated POMDP framework. state consists six
components: arrival pattern, pick-up requests two floors, drop-off requests
two floors elevators position. use six variables denote components
respectively. state assignment variables. arrival pattern takes three
possible values three different patterns. passengers waiting lobby
first floor, pick-up request set; otherwise, unset. passengers
elevator intending get first floor, drop-off request first floor set;
otherwise, unset. Similarly, second floor, variables pick-up/drop-off
requests set accordingly. elevator first floor, position set first;
second floor, position set second. number states 322222 = 96.
observation five components; components state except
arrival pattern. many 2 2 2 2 2 = 32 observations. elevator may
execute one three actions, namely go.up, go.down stay. restriction is,
first floor, cannot perform go.down; second floor, action
go.up cannot performed.
160

fiRestricted Value Iteration: Theory Algorithms

uncertainty stems probabilities changes arrival patterns.
elevator executes go.up, component evolves follows. arrival pattern
changes according predetermined probability distribution. components pickup/drop-off requests remain. position changes first second. effects
action go.down described similarly. elevator performs action stay,
arrival pattern changes similarly. requests floor fulfilled
corresponding variables reset. instance, passenger would like get
first floor, elevator first floor performs stay, passenger able get
off. say elevator fulfills drop-off requests first floor. another
instance, passengers like enter elevator second floor,
elevator performs action stay second floor. say pickup request
second floor fulfilled case. also allowable elevator fulfill two
requests one time point. example, pick-up drop-off requests
first floor, elevator performs action stay, passengers enter exit
within one time point. say fulfills two requests. Note action stay
performed, elevator fulfill request. Since variable arrival pattern changes
time moment, elevator changes states probabilistically performing
action.
elevator informed partial knowledge state transition. elevator
performs action, knows changes components states: variables pick-up
drop-off floor position. However, since know arrival
pattern component state, observations cannot reveal identities
states. partial observability. However, since three possible
arrival patterns, observation reveals elevator must three possible
states. Therefore, POMDP informative.
performance elevator measured different ways diverse applications. define measure minimize unsatisfactory degree service elevator
provides. encode reward model. time point, elevator serves
one four requests: pick-up requests first/second floor, drop-off requests
first/second floor. performing action, 4 requests unfulfilled,
elevator receives penalty 0.25. instance, elevator un-fulfills either pick-up
drop-off request(if set) first floor, receives penalty 0.25 2 = 0.5.
objective elevator minimize total discounted penalty long run.
convenience, use A.i denote arrival patterns = 1, 2, 3. experiments,
transition probabilities set following table. Basically, pattern remains
fixed probability 0.90 changes another 0.05.

A.1
A.2
A.3

A.1
0.90
0.05
0.05

A.2
0.05
0.90
0.05

161

A.3
0.05
0.05
0.90

fisizes representing sets(logscale)

Zhang & Zhang

CPU seconds(logscale)

1e+06
100000
10000
1000
VI
ssVI
infoVI
infoVIPB

100
10
1
0.1
0

5

10 15 20 25
number iterations

30

35

100000
10000
1000
100

VI
ssVI
infoVI
infoVIPB

10
1
0

5

10 15 20 25
number iterations

30

35

Figure 8: Performance VI, ssVI, infoVI infoVIPB Elevator
Empirical Studies
collect time costs actual number vectors generated iteration
algorithms VI, ssVI, infoVI infoVIPB referring infoVI integrated pointbased procedure (Zhang, 2001). results presented Figure 8.
first chart figure shows time costs iterations. algorithms
set compute 0.1-optimal value function. infoVIPB, exclude iterations
point-based improvements. Overall, see VI ssVI means solve
problem, infoVI likely solve given sufficient time infoVIPB able solve
easily. infoVIPB runs, uses loose stopping criterion.
strict one used, threshold close round-off precision parameter.
first seven iterations, ssVI takes 190,000 seconds, infoVI 32 seconds.
performance difference drastic. infoVI proceeds, takes 1,100 seconds
one iteration. evident infoVI able compute near optimal value function
given sufficient time. point-based technique integrated, infoVIPB able
terminate 94 seconds five steps DP updates (B). Since
algorithms cannot terminate within reasonable time limit, compare data
6th iteration among them. last iteration able gather statistics VI.
iteration, VI takes 76,000 seconds, ssVI 6,000 seconds, infoVIPB 8 seconds.
second chart Figure 8 depicts number vectors generated iteration
tested algorithms. ssVI, collect sum numbers vectors representing
value functions |A| |Z| -simplices. infoVI infoVIPB, collect sum
numbers representing vectors |Z| -simplices. problem
observation models independent actions.
chart, see VI generates significantly vectors ssVI
infoVI. experiments, infoVIPB terminates, produces 1,132 vectors.
reason above, compare numbers vectors 6th iterations
algorithms. iteration, VI generates 12,000 vectors. ssVI infoVI,
number 252 136 respectively. DP updates proceed, conceivable
number vectors generated VI increase sharply hence DP updates
extremely inefficient. infoVIPB, since final number generated vectors rather
162

fiRestricted Value Iteration: Theory Algorithms

small, together fact point-based improvement effectively reduces number
iterations (B), possible compute near optimal value function within
rather small time limit turns out.

References
Astrom, K. J. (1965). Optimal control Markov decision processes incomplete state
estimation. Journal Mathematical Analysis Applications, 10, 403406.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic
programming. Artificial Intelligence, 72, 81138.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Gallagher, R. G. (1995). Data Networks. Prentice Hall.
Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic search
belief space. Proceedings 6th International Conference Artificial Intelligence Planning Systems (AIPS), pp. 5261. AAAI Press.
Boutilier, C., Brafman, R. I., & Geib, C. (1998). Structured reachability analysis Markov
decision processes. Proceedings 14th Conference Uncertainty Artificial
Intelligence (UAI), pp. 2432.
Boutilier, C., & Poole, D. (1996). Computing optimal policies partially observable
decision processes using compact representations. Thirteenth National Conference
Artificial Intelligence (AAAI), pp. 11681175. Portland, Oregon.
Burago, D., de Rougemont, M., & Slissekno, A. (1996). complexity partially
observed Markov decision processes. Theoretical Computer Science, 157 (2), 161183.
Cassandra, A. R. (1998a). Exact approximate algorithms partially observable Markov
decision processes. Ph.D. thesis, Department Computer science, Brown university.
Cassandra, A. R. (1998b). survey POMDP applications. Working Notes AAAI
1998 Fall Symposium Planning Partially Observable Markov Decision Processes, pp. 1724.
Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple,
fast, exact method partially observable Markov decision processes. Proceedings
13th Conference Uncertainty Artificial Intelligence, pp. 5461.
Choi, S. P. M., Yeung, D. Y., & Zhang, N. L. (1999). environment model nonstationary reinforcement learning. Advances Neural Information Processing Systems
12, pp. 987993.
Dean, T., Givan, R., & Kim, K. (1998). Solving planning problems large state
action spaces. Proceedings 4th International Conference Artificial Intelligence Planning Systems (AIPS), pp. 102110. Pittsburgh, Pennsylvania.
Dean, T. L., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning deadlines
stochastic domains. Proceedings 9th National Conference Artificial
Intelligence (AAAI), pp. 574579.
163

fiZhang & Zhang

Dean, T. L., & Lin, S. H. (1995). Decomposition techniques planning stochastic
domains. Proceedings 14th International Joint Conference Artificial Intelligence (IJCAI), pp. 11211127.
Drummond, M., & Bresina, J. (1990). Anytime synthetic projection: maximizing probability goal satisfaction. Proceedings National Conference Artificial Intelligence (AAAI), pp. 138144.
Geffner, H., & Bonet, B. (1998). Solving large POMDPs using real time dynamic programming. Working Notes Fall AAAI Symposium POMDPs, pp. 6168.
Hansen, E. A. (1998). Finite memory control partially observable systems. Ph.D. thesis,
Dept Computer Science, University Massachusetts Amherst.
Hansen, E. A., & Ziberstein, S. (1998). Heuristic search cyclic AND/OR graphs.
Proceedings National Conference Artificial Intelligence (AAAI), pp. 412417.
Hauskrecht, M. (1997). Incremental methods computing bounds partially observable Markov decision processes. Proceedings National Conference Artificial
Intelligence (AAAI), pp. 734739.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov
decision processes. Journal Artificial Intelligence Research, 13, 3394.
Hauskrecht, M., & Fraser, H. (1998). Modeling treatment ischemic heart disease
partially observable Markov decision processes. American Medical Informatics
Association annual symposium Computer Applications Health Care, pp. 538
542. Orlando, Florida.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101 (1-2).
Koller, D., & Parr, R. (2000). Policy iteration factored MDPs. Proceedings
Sixteenth Conference Uncertainty Artificial Intelligence (UAI), pp. 326334.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Efficient dynamic programming updates partially observable Markov decision processes. Tech. rep. CS-95-19,
Department Computer Science, Brown University.
Littman, M. L., Goldsmith, J., & Mundhenk, M. (1998). computational complexity
probabilistic planning. Journal Artificial Intelligence Research, 9, 136.
Lovejoy, W. S. (1991). Computationally feasible bounds partially observed Markov
decision processes. Operations Research, 39 (1), 162175.
Madani., O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planning infinite horizon partially observable Markov decision problems..
Monahan, G. E. (1982). survey partially observable Markov decision processes: theory,
models, algorithms. Management Science, 28 (1), 116.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov decision processes. Mathematics Operations Research, 12 (3), 441450.
Parr, R. (1998). Flexible decomposition algorithms weakly coupled Markov decision
problems. Proceedings 14th Conference Uncertainty Artificial Intelligence (UAI), pp. 422430.
164

fiRestricted Value Iteration: Theory Algorithms

Parr, R., & Russell, S. (1995). Approximating optimal policies partially observable
stochastic domains. Proceedings 14th International Joint Conference
Artificial Intelligence (IJCAI), pp. 10881094.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime algorithm POMDPs. International Joint Conference Artificial Intelligence
(IJCAI), pp. 10251032.
Poupart, P., & Boutilier, C. (2002). Value-directed compresseion POMDPs. Proceedings Advances Neural Information Processing Systems (NIPS), pp. 15471554.
Puterman, M. L. (1994). Markov decision processes: discrete stochastic dynamic programming. Wiley, New York, NY.
Roy, N., & Gordon, G. (2002). Exponential family PCA belief compression POMDPs.
Proceedings Advances Neural Information Processing Systems (NIPS), pp.
16351642.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable
Markov processes finite horizon. Operations Research, 21, 10711088.
Sondik, E. J. (1971). optimal control partially observable decision processes. Ph.D.
thesis, Stanford University, Stanford, California, USA.
Washington, R. (1997). BI-POMDP: Bounded, incremental partially-observable Markovmodel planning. Proceedings 4th European Conference Planning (ECP),
pp. 440451.
Zhang, N. L., & Liu, W. (1997). model approximation scheme planning partially
observable stochastic domains. Journal Artificial Intelligence Research, 7, 199230.
Zhang, N. L., & Zhang, W. (2001a). Space-progressive value iteration: anytime algorithm
class POMDPs. Sixth European Conference Symbolic Quantitative
Approaches Reasoning Uncertainty (ECSQARU), pp. 7283.
Zhang, N. L., & Zhang, W. (2001b). Speeding convergence value iteration
partially observable Markov decision processes. Journal Artificial Intelligence Research, 14, 2951.
Zhang, W. (2001). Algorithms partially observable Markov decision processes. Ph.D.
thesis, Department Computer Science, Hong Kong University Science
Technology.
Zhang, W., & Zhang, N. L. (2001). Solving informative partially observable Markov decision
processes. Proceedings 6th European Conference Planning (ECP).
Zhou, R., & Hansen, E. (2001). improved grid-based approximation algorithm
POMDPs. Proceedings 17th International Joint Conference Artificial
Intelligence, pp. 707716.
Zubek, V. B., & Dietterich, T. G. (2000). POMDP approximation algorithm anticipates need observe. Proceedings PRICAI-2000, pp. 521532. Lecture
Notes Computer Science, New York: Springer-Verlag.

165

fiJournal Artificial Intelligence Research 23 (2005) 4178

Submitted 07/04; published 01/05

Research Note
Extremal Behaviour Multiagent Contract Negotiation
Paul E. Dunne

ped@csc.liv.ac.uk

Department Computer Science
University Liverpool, Liverpool, UK

Abstract
examine properties model resource allocation several agents exchange resources order optimise individual holdings. schemes discussed relate well-known negotiation protocols proposed earlier work consider number
alternative notions rationality covering quantitative measures, e.g. cooperative
individual rationality qualitative forms, e.g. Pigou-Dalton transfers.
known imposing particular rationality structural restrictions may result
reallocations resource set becoming unrealisable, paper address
issue number restricted rational deals may required implement particular reallocation possible so. construct examples showing
number may exponential (in number resources m), even agent
utility functions monotonic. show k agents may achieve single
deal reallocation requiring exponentially many rational deals k 1 agents
participate, reallocation unrealisable sequences rational deals
k 2 agents involved.

1. Introduction
Mechanisms negotiating allocation resources within group agents form important body work within study multiagent systems. Typical abstract models
derive game-theoretic perspectives economics among issues
addressed strategies agents use obtain particular subset resources available, e.g. (Kraus, 2001; Rosenschein & Zlotkin, 1994; Sandholm, 1999), protocols
process settling upon allocation resources among agents involved
agreed, e.g. (Dignum & Greaves, 2000; Dunne, 2003; Dunne & McBurney, 2003; McBurney
et al., 2002).
setting concerned encapsulated following definition.
Definition 1 resource allocation setting defined triple hA, R, Ui
= {A1 , A2 , . . . , }

;

R = {r1 , r2 , . . . , rm }

are, respectively, set (at least two) agents collection (non-shareable) resources.
utility function, u, mapping subsets R rational values. agent Ai
associated particular utility function ui , U hu1 , u2 , . . . , un i.
allocation P R partition hP1 , P2 , . . . , Pn R. value ui (Pi ) called
utility resources assigned Ai . utility function, u, monotone whenever
holds u(S ) u(T ), i.e. value assigned u set resources, ,
never less value u attaches subset, .
c
2005
AI Access Foundation. rights reserved.

fiDunne

Two major applications abstract view Definition 1 exploited
e-commerce distributed task realisation. first R represents collection
commodities offered sale individual agents seek acquire subset these,
value agent attaches specific set described agents utility function.
task planning, resource set describes collection sub-tasks performed
order realise complex task, e.g. complex task may transport goods
central warehouse set cities. example R describes locations
goods must dispatched given allocation defines places
agent must arrange deliveries. utility functions cases model cost agent
associates carrying alloted sub-tasks.
Within general context Definition 1, number issues arise stemming
observation unlikely initial allocation seen satisfactory
either respect views agents system respect divers global
considerations. Thus, proposing changes initial assignment individual agents
seek obtain better allocation. scenario raises two immediate questions:
evaluate given partition thus basis forming improved optimal allocations;
and, issue underlying main results paper, restrictions imposed
form proposed deals may take.
shall subsequently review widely studied approaches defining
conditions allocations seen better others. purposes
introduction simply observe criteria may either quantitative
qualitative nature. example former approach wherein
value allocation P simply sum values given agents utility
P
functions subsets R apportioned within P , i.e. ni=1 ui (Pi ):
so-called utilitarian social welfare, avoid repetition denote u (P ).
natural aim agents within commodity trading context seek allocation
u maximised. One example qualitative criterion envy freeness: informally,
allocation, P , envy-free agent assigns greater utility resource set (Pj ) held
another agent respect resource set (Pi ) actually
allocated, i.e. distinct pair hi , j i, ui (Pi ) ui (Pj ).
general terms two approaches considered treating
question finite collection resources might distributed among set agents
order optimise criterion interest: contract-net based methods, e.g. (Dunne
et al., 2003; Endriss et al., 2003; Endriss & Maudet, 2004b; Sandholm, 1998, 1999) deriving
work Smith (1980); combinatorial auctions, e.g. (Parkes & Ungar, 2000a,
2000b; Sandholm et al., 2001; Sandholm, 2002; Sandholm & Suri, 2003; Tennenholz, 2000;
Yokoo et al., 2004, amongst others). significant difference extent
centralized controlling agent determines eventual distribution resources
among agents.
One may view strategy underlying combinatorial auctions investing computational effort pre-processing stage following given allocation determined.
Thus controlling agent (the auctioneer) supplied set bids pairs hSj , pj
wherein Sj subset available resources pj price agent Aj prepared
pay order acquire Sj . problem faced auctioneer decide bids

42

fiExtremal Behaviour Multiagent Contract Negotiation

accept order maximise overall profit subject constraint item
obtained one agent.
shall refer contract-net schemes typically eschew precomputation
stage subordination controlling arbiter employed auction mechanisms, seeking
instead realise suitable allocation agreed sequence deals. contract-net (in
general instantiation) scenarios resources distributed among n agents
complete directed graph n vertices (each associated distinct
allocation). way possible deal hP , Qi represented edge directed
vertex labelled P labelled Q. Viewed thus, identifying sequence deals
interpreted search process which, principle, individual agents may conduct
autonomous fashion.
Centralized schemes effective contexts participants cooperate (in
sense accepting auctioneers arbritration). environments within agents
highly self-interested extent aims conflict auction process
high degree uncertainty outcome, working towards
final allocation, agents involved may prepared proceed cautiously: is,
agent accept proposed reallocation satisfied would result
immediate improvement perspective. cases, process moving
initial allocation, Pinit , eventual reallocation Pfin sequence local rational
deals, e.g. agent might refuse accept deals reduced u possibility
suffers uncompensated loss utility. key issue following: deal
protocol allows moves stage agent Aj offers single resource
another agent Aj rational reallocation hPinit , Pfin always implemented; if,
however, every single move must rational hPinit , Pfin may realisable.
may, informally, regard view agents myopic, sense
unwilling accept short-term loss (a deal hP , Qi might incur loss
utility) despite prospect long-term gain (assuming u (Pfin ) > u (Pinit ) holds).
number reasons agent may adopt views, e.g. consider
following simple protocol agreeing reallocation.
reallocation resources agreed sequence stages,
involves communication two agents, Ai Aj . communication
consists Ai issuing proposal Aj form (buy, r , p), offering purchase
r Aj payment p; (sell , r , p), offering transfer r Aj return
payment p. response Aj simply accept (following
deal implemented) reject.
This, course, simple negotiation structure, however consider operation within
two agent setting one agent, A1 say, wishes bring allocation Pfin
(and thus devise plan sequence deals realise initial allocation
Pinit ) agent, A2 , know Pfin . addition, assume A1
agent makes proposals final allocation fixed either A1 satisfied
soon A2 rejects offer.
A2 could better Pfin realised, may case proposals
A2 accept lose, e.g. agents may sceptical
bona fides others accept deals perceive
43

fiDunne

immediate benefit. several reasons agent may embrace attitudes
within schema outlined: deal implemented A2 may lose utility
proposals made A1 loss permanent. note even
enrich basic protocol A1 describe Pfin , A2 may still reject offers
suffers loss, since unwilling rely subsequent deals would ameliorate
loss actually proposed. Although position taken A2 setting
described may appear unduly cautious, would claim reflect real behaviour
certain contexts. Outside arena automated allocation negotiation multiagent
systems, many examples actions individuals promised long-term gains
insufficient engender acceptance short term loss. Consider chain letter
schemes (or subtle manifestation pyramid selling enterprises):
natural lifetime bounded size population circulate, may
break reached. Faced request send $10 five names
head list forward letter ten others adding name despite
possibility significant gain temporary loss $50, ignore blandishments
seen overly sceptical cautious: may reluctance accept one
eventually receive sufficient recompense return suspicion name order
manipulated.
summary, identify two important influences lead contexts
agents prefer move towards reallocation via sequence rational deals. Firstly,
agents self-interested operating unstable environment, e.g. chain
letter setting, agent cannot reliably predict exact point chain fail.
second factor computational restrictions may limit decisions individual
agent make whether accept proposed deal. example settings
deals involve one resource time, A2 may reject proposal accept
resource, r , since r useful following sequence deals: number
deals small A2 could decide accept proposed deal since
sufficient computational power determine context r value;
number large however, A2 may lack sufficient power scan search
space future possibilities would allow accept r . Notice extreme
case, A2 makes decision solely whether r immediate use, i.e. A2 myopic.
powerful A2 may able consider whether r useful k deals
take place: case, A2 could still refuse accept r since, although use, A2 cannot
determine bounded look ahead.
total scenario described, A1 wishes bring allocation Pfin
faced view adopted A2 limitations imposed deal protocol,
effective plan A1 could adopt find sequence rational deals
propose A2 .
aim article show combining structural restrictions (e.g. one
resource time involved local reallocation) rationality restrictions result
settings sequence realise reallocation hP , Qi must involve exponentially
many (in |R|) separate stages. refine ideas next sub-section.

44

fiExtremal Behaviour Multiagent Contract Negotiation

1.1 Preliminary Definitions
begin, first formalise concepts deal contract path.
Definition 2 Let hA, R, Ui resource allocation setting. deal pair hP , Qi
P = hP1 , . . . , Pn Q = hQ1 , . . . , Qn distinct partitions R. effect implementing deal hP , Qi allocation resources specified P replaced
specified Q. Following notation (Endriss & Maudet, 2004b) deal = hP , Qi,
use indicate subset involved, i.e. Ak Pk 6= Qk .
Let = hP , Qi deal. contract path realising sequence allocations
= hP (1) , P (2) , . . . , P (t1) , P (t)
P = P (1) P (t) = Q. length , denoted || 1, i.e. number
deals .
two methods use reduce number deals single
agent may consider seeking move allocation another, thereby
avoiding need choose exponentially many alternatives: structural rationality
constraints. Structural constraints limit permitted deals bound
number resources and/or number agents involved, take consideration
view agent may whether allocation improved. contrast, rationality
constraints restrict deals hP , Qi Q improves upon P according
particular criteria. article consider two classes structural constraint: Ocontracts, defined considered (Sandholm, 1998), shall refer (k )contracts.
Definition 3 Let = hP , Qi deal involving reallocation R among A.
a. one contract (O-contract)
O1. = {i , j }.
O2. unique resource r Pi Pj Qi = Pi {r } Qj = Pj \ {r }
(with r Pj ) Qj = Pj {r } Qi = Pi \ {r } (with r Pi )
b. value k 2, deal = hP , Qi (k )-contract 2 |A | k
iA Qi = iA Pi .
Thus, O-contracts involve transfer exactly one resource particular agent
another, resulting number deals compatible given allocation exactly
(n 1)m: resources reassigned current owner
n 1 agents.
Rationality constraints arise number different ways. example,
standpoint individual agent Ai given deal hP , Qi may three different outcomes:
ui (Pi ) < ui (Qi ), i.e. Ai values allocation Qi superior Pi ; ui (Pi ) = ui (Qi ), i.e.
Ai indifferent Pi Qi ; ui (Pi ) > ui (Qi ), i.e. Ai worse
deal. global optima utilitarian social welfare maximised,
question incentive agent accept deal hP , Qi
45

fiDunne

left less valuable resource holding. standard approach latter question
introduce notion pay-off function, i.e. order Ai accept deal
suffers reduction utility, Ai receives payment sufficient compensate
loss. course compensation must made agents system
providing wish pay excess gain. defining notions pay-off
interpretation transaction agent Ai makes payment, : < 0
Ai given return accepting deal; > 0 Ai contributes
amount distributed among agents whose pay-off negative.
notion sensible transfer captured concept individual rationality,
often defined terms appropriate pay-off vector existing. difficult,
however, show definitions equivalent following.
Definition 4 deal hP , Qi individually rational (IR) u (Q) > u (P ).
shall consider alternative bases rationality constraints later: primarily
interest within so-called money free settings (so compensatory payment loss
utility option).
central issue interest paper concerns properties contract-net
graph allowed deals must satisfy structural rationality constraint.
Thus, consider arbitrary predicates deals hP , Qi cases interest
combining structural rationality condition have,
Definition 5 predicate distinct pairs allocations, contract path
hP (1) , P (2) , . . . , P (t1) , P (t)
realising hP , Qi -path 1 < t, hP (i) , P (i+1) -deal,
(P (i) , P (i+1) ) holds. say complete deal may realised -path.
We, further, say complete respect -deals (where predicate
distinct pairs allocations) deal () holds may realised -path.
main interest earlier studies ideas areas identifying
necessary and/or sufficient conditions deals complete respect particular
criteria, e.g. (Sandholm, 1998); establishing convergence termination properties, e.g. Endriss et al. (2003), Endriss Maudet (2004b) consider deal types, ,
every maximal1 -path ends Pareto optimal allocation, i.e. one
reallocation agent improves utility lead another agent suffering
loss. Sandholm (1998) examines restrictions e.g. (P , Q) = >
hP , Qi O-contract, may affect existence contract paths realise deals.
particular interest, viewpoint heuristics exploring contract-net graph,
cases (P , Q) = > deal hP , Qi individually rational.
case O-contracts following known:
Theorem 1
a. O-contracts complete.
1. Maximal sense hP (1) , . . . , P (t) path, every allocation, Q, (P (t) , Q)
hold.

46

fiExtremal Behaviour Multiagent Contract Negotiation

b. IR O-contracts complete respect IR deals.
consideration algorithmic complexity issues presented (Dunne et al., 2003)
one difficulty attempting formulate reallocation plans rational O-contracts
already apparent, is:
Theorem 2 Even case n = 2 monotone utility functions problem
deciding IR O-contract path exists realise IR deal hP , Qi nphard.
Thus deciding rational plan possible already computationally hard.
article demonstrate that, even appropriate rational plan exists, extreme cases,
may significant problems: number deals required could exponential
number resources, affecting time take schema outlined
conclude space agent dedicate storing it. Thus proof
Theorem 1 (b), Sandholm observes IR O-contract path exists given
IR deal, may case length exceeds m, i.e. agent passes resource
another accepts resource later stage.
typical form results derive summarised as:
structural constraint (O-contract (k )-contract) rationality
constraint, e.g. (P , Q) holds hP , Qi individually rational, resource allocation settings hAn , Rm , Ui deal hP , Qi satisfying
following.
a. hP , Qi -deal.
b. hP , Qi realised contract path every deal satisfies
structural constraint rationality constraint .
c. Every contract path length least g(m).
example, show instances shortest IR O-contract path
length exponential m.2 next section interested lower bounds
values following functions: introduce general terms avoid unnecessary
subsequent repetition.
Definition 6 Let hA, R, Ui resource allocation setting. Additionally let
two predicates deals. deal = hP , Qi partial function Lopt (, hA, R, Ui, )
length shortest -contract path realising hP , Qi path exists (and
undefined path possible). partial function Lmax (hA, R, Ui, , )
Lmax (hA, R, Ui, , ) =

max
Lopt (, hA, R, Ui, )
-deals

Finally, partial function max (n, m, , )
max (n, m, , ) =

max

U=hu1 ,u2 ,...,un

Lmax (hAn , Rm , Ui, , )

consideration restricted -deals = hP , Qi realising -path
exists.
2. Sandholm (1998) gives upper bound length paths also exponential m,
explicitly state lower bound already referred to.

47

fiDunne

three measures, Lopt , Lmax max distinguish different aspects regarding length
contract-paths. function Lopt concerned -paths realising single deal hP , Qi
given resource allocation setting hA, R, Ui: property interest number
deals shortest, i.e. optimal length, -path. stress Lopt partial function
whose value undefined event hP , Qi cannot realised -path
setting hA, R, Ui. function Lmax defined terms Lopt , context
specific resource allocation setting. behaviour interest Lmax , however,
simply length -paths realising specific hP , Qi worst-case value Lopt
deals -deals. note qualification Lmax defined -deals
capable realised -paths, thus consider cases
appropriate contract path exists. Thus, case -deal setting
hA, R, Ui realised -path value Lmax (hA, R, Ui, , ) undefined, i.e.
Lmax also partial function. may interpret upper bound Lmax following
terms: Lmax (hA, R, Ui, , ) K -deal -path exists
realised -path length K .
main interest centre max concerned behaviour Lmax
function n ranges n-tuples utility functions hu : 2R Qin .
approach obtaining lower bounds function constructive, i.e. h,
considered, show utility functions U may defined setting
resources yield lower bound max (n, m, , ). contrast measures Lopt
Lmax , function max described terms single fixed resource allocation
setting. is, however, still partial function: depending hn, m, , may case
every n agent, resource allocation setting, regardless choice utility
functions made, -deal, hP , Qi capable realised -path,
cases value max (n, m, , ) undefined.3
noted, point, definition max allows arbitrary utility functions
employed constructing worst-case instances. reasonable terms
general lower bound results, apparent given constructions utility
functions actually employed highly artificial (and unlikely feature real application
settings). shall attempt address objection considering bounds
following variant max :
max
mono (n, m, , ) =

U=hu1 ,u2 ,...,un

max
Lmax (hAn , Rm , Ui, , )
: ui monotone

Thus, max
mono deals resource allocation settings within utility functions
must satisfy monotonicity constraint.
main results article presented next sections. consider two
general classes contract path: O-contract paths various rationality conditions
3. recognising possibility max (n, m, , ) could undefined, claiming
behaviour arises instantiations h, considered subsequently: fact
max
clear constructions that, denoting max
(n, m, , ) fixed
, (n, m) function
instantiation h, i, restricted deal types rationality conditions examined, function
max
, (n, m) total function. Whether possible formulate sensible choices h,
max
, (n, m) undefined values hn, mi (and, so, demonstrating examples such) is,
primarily, question combinatorial interest, whose development central concerns
current article.

48

fiExtremal Behaviour Multiagent Contract Negotiation

Section 2; and, similarly, (k )-contract paths arbitrary values k 2 Section 3.
results concerned construction resource allocation settings hA, Rm , Ui
given rationality requirement, e.g. deals individually rational,
deal hP , Qi satisfies rationality condition, realised rational
O-contract path (respectively, (k )-contract path), number deals required
paths exponential m. additionally obtain slightly weaker (but still
exponential) lower bounds rational O-contract paths within settings monotone utility
functions, i.e. measure max
mono , outlining similar results may derived
(k )-contract paths.
resource allocation settings constructed demonstrating properties
(k )-contract paths, constructed deal hP , Qi realisable single (k + 1)contract unrealisable rational (k 1)-contract path. discuss related work,
particular recent study (Endriss & Maudet, 2004a) addresses similar issues
considered present article, Section 4. Conclusions directions
work presented final section.

2. Lower Bounds Path Length O-contracts
section consider issue contract path length structural restriction
requires individual deals O-contracts. first give overview construction
method, following subsections analysing cases unrestricted utility functions
and, subsequently, monotone utility functions.
2.1 Overview
strategy employed proving results involves two parts: given class restricted contract paths proceed follows obtaining lower bounds max (n, m, , ).
a. contract-net graph partitioning resources among n agents, construct
path, = hP (1) , P (2) , . . . , P (t) realising deal hP (1) , P (t) i. structural
constraint, 0 influencing proved that:
a1. contract path 0 -path, i.e. 1 < t, deal hP (i) , P (i+1
satisfies structural constraint 0 .
a2. pair allocations P (i) P (i+j ) occurring , j 2
deal hP (i) , P (i+j ) 0 -deal.
Thus (a1) ensures suitable contract path, (a2) guarantee
exactly one allocation, P (i+1) , reached within given
allocation P (i) means 0 -deal.
b. Define utility functions Un = hu1 , . . . , un following properties
b1. deal hP (1) , P (t) -deal.
b2. rationality constraint, 00 influencing , every deal hP (i) , P (i+1)
00 -deal.

49

fiDunne

b3. every allocation P (i) contract path every allocation Q
P (i+1) deal hP (i) , Qi -deal, i.e. violates either stuctural
constraint 0 rationality constraint 00 .
Thus, (a1) (b2) ensure hP (1) , P (t) defined value respect
function Lopt -deal hP (1) , P (t) i, i.e. -path realising deal possible.
properties given (a2) (b3) indicate (within constructed resource
allocation setting) path unique -path realising hP (1) , P (t) i. follows
1, length path, gives lower bound value Lmax hence
lower bound max (n, m, , ).
continuing useful fix notational details.
use Hm denote m-dimensional hypercube. Interpreted directed graph, Hm
2m vertices identified distinct m-bit label. Using = a1 a2 . . .
denote arbitrary label, edges Hm formed
{ h, : differ exactly one bit position}
identify m-bit labels = a1 a2 . . . subsets Rm , via ri
ai = 1. Similarly, subset R described binary word, (S ), length m,
i.e. (S ) = b1 b2 . . . bm bi = 1 ri . label use || denote
number bits value 1, || size subset . m-bit
labels, 2m-bit label, Rm Tm disjoint sets, describes
union subset Rm subset Tm . Finally = a1 a2 . . .
m-bit label denotes label formed changing 0 values 1
vice versa. way, subset Rm described describes set
Rm \ . avoid excess superscripts will, ambiguity arises, use
denote m-bit label subset Rm described it, e.g. write rather
.
n = 2 contract-net graph induced O-contracts viewed mdimensional hypercube Hm : m-bit label, associated vertex Hm describing
allocation h, hA1 , A2 i. way set IR O-contracts define subgraph,
Gm Hm directed path (P ) (Q) Gm corresponding possible IR
O-contract path allocation hP , R \ P allocation hQ, R \ Qi.
2.2 O-contract Paths Unrestricted Utility Functions
first result clarifies one issue presentation (Sandholm, 1998, Proposition 2):
upper bound exponential proved length IR O-contract
paths, i.e. terms notation, (Sandholm, 1998, Proposition 2) establishes upper
bound max (n, m, , ). prove similar order lower bound.
Theorem 3 Let (P , Q) predicate holds whenever hP , Qi IR O-contract
(P , Q) holds whenever hP , Qi IR. 7
max (2, m, , )

50



77
256



2m 2

fiExtremal Behaviour Multiagent Contract Negotiation

Proof. Consider path C = h1 , 2 , . . . , Hm , following property4
1 < j (j + 2) (i j differ least 2 positions)

(SC)

e.g. = 4
, {r1 }, {r1 , r3 }, {r1 , r2 , r3 }, {r2 , r3 }, {r2 , r3 , r4 }, {r2 , r4 }, {r1 , r2 , r4 }
path corresponds sequence h0000, 1000, 1010, 1110, 0110, 0111, 0101, 1101i.
Choose C (m) longest path property could formed Hm ,
letting = hP (1) , P (2) , . . . , P (t) sequence allocations P (i) = hi , i.
define utility functions u1 u2 Rm ,
(

u1 () + u2 () =

k
0




= k
6 {1 , 2 , . . . , }

choice, contract path describes unique IR O-contract path realising
IR deal hP (1) , P (t) i: IR O-contract path immediate, since
u (P (i+1) ) = + 1 > = u (P (i) )
unique follows fact 1 + 2 j t, deal
hP (i) , P (j ) O-contract (hence short-cuts possible),
P (i) exactly one IR O-contract follow it, i.e. P (i+1) .5
preceding argument follows lower bound length C (m) ,
i.e. sequence satisfying condition (SC), lower bound max (2, m, , ).
paths Hm originally studied Kautz (1958) context coding theory
lower bound length (77/256)2m 2 established (Abbott & Katchalski,
1991).
2
Example 1 Using path
C (4)

=
=

h0000, 1000, 1010, 1110, 0110, 0111, 0101, 1101i
h1 , 2 , 3 , 4 , 5 , 6 , 7 , 8

resource allocation setting h{a1 , a2 }, {r1 , r2 , r3 , r4 }, hu1 , u2 ii, utility functions
specified Table 1 u (h1 , 1 i) = 1 u (h8 , 8 i) = 8. Furthermore,
C (4) describes unique IR O-contract path realising reallocation hh1 , 1 i, h8 , 8 ii
number alternative formulations rationality also considered.
example
Definition 7 Let = hP , Qi deal.
4. defines so-called snake-in-the-box codes introduced (Kautz, 1958).
5. example = 4, sequence h0000, 1000, 1001, 1101i, although defining O-contract path
gives rise deal IR, namely corresponding h1000, 1001i.

51

fiDunne


0000
0001
0010
0011
0100
0101
0110
0111

R\S
1111
1110
1101
1100
1011
1010
1001
1000

u1 (S )
1
0
0
0
0
4
3
3

u2 (R \ )
0
0
0
0
0
3
2
3

u
1
0
0
0
0
7
5
6

1

7
5
6


1000
1001
1010
1011
1100
1101
1110
1111

R\S
0111
0110
0101
0100
0011
0010
0001
0000

u1 (S )
1
0
2
0
0
4
2
0

u2 (R \ )
1
0
1
0
0
4
2
0

u
2
0
3
0
0
8
4
0

2
3

8
4

Table 1: Utility function definitions = 4 example.
a. cooperatively rational every agent, Ai , ui (Qi ) ui (Pi ) least
one agent, Aj , uj (Qj ) > uj (Pj ).
b. equitable miniA ui (Qi ) > miniA ui (Pi ).
c. Pigou-Dalton deal = {i , j }, ui (Pi ) + uj (Pj ) = ui (Qi ) + uj (Qj )
|ui (Qi ) uj (Qj )| < |ui (Pi ) uj (Pj )| (where | . . . | absolute value).
number views take concerning rationality conditions given Definition 7. One shared feature that, unlike concept individual rationality
provision compensate agents suffer loss utility needed, i.e. individual
rationality presumes money-based system, forms defined Definition 7 allow concepts rationality given money-free enviroments. Thus, cooperatively
rational deal, agent involved suffers loss utility least one better off. may
noted given characterisation Definition 4 immediate cooperatively rational deal perforce also individually rational; converse, however, clearly
hold general. settings, equitable deal may neither cooperatively
individually rational. One may interpret deals one method reducing inequality
values agents place allocations: involved equitable deal,
ensured agent places least value current allocation obtain
resource set valued highly. may, course, case agents
suffer loss utility: condition deal equitable limits great loss
could be. Finally concept Pigou-Dalton deal originates studied
depth within theory exchange economies. one many approaches
proposed, order describe deals reduce inequality members
agent society, e.g. (Endriss & Maudet, 2004b). terms definition given,
deals encapsulate so-called Pigou-Dalton principle economic theory:
transfer income wealthy individual poorer one reduce disparity
them. note that, principle, could define related rationality concepts
based several extensions principle suggested, e.g. (Atkinson, 1970;
Chateauneaf et al., 2002; Kolm, 1976).
Using O-contract path constructed Theorem 3, need vary
definitions utility functions employed order obtain,
Corollary 1 cases below,

52

fiExtremal Behaviour Multiagent Contract Negotiation

a. () holds cooperatively rational O-contract.
() holds cooperatively rational.
b. () holds equitable O-contract.
() holds equitable.
c. () holds Pigou-Dalton O-contract.
() holds Pigou-Dalton deal.



max



(2, m, , )

77
2m 2
256


Proof. employ exactly sequence allocations described proof
Theorem 3 modify utility functions hu1 , u2 case.
a. Choose hu1 , u2 u2 () = 0 R
(

u1 () =

k
0




= k
6 {1 , . . . , }

resulting O-contract path cooperatively rational: utility enjoyed A2 remains constant enjoyed A1 increases 1 deal. deviation
contract path (employing alternative O-contract) result loss
utility A1 .
b. Choose hu1 , u2 u2 () = u1 ()
(

u1 () =

k
0




= k
6 {1 , . . . , }

O-contract path equitable: A1 A2 increase respective utility
values 1 deal. Again, O-contract deviating result
agents losing utility.
c. Choose hu1 , u2
(

u1 () =

k
0




= k
6 {1 , . . . , }

(

;

u2 () =

2m k
2m




= k
6 {1 , . . . , }

see O-contract path consists Pigou-Dalton deals, suffices note
u1 (i ) + u2 (i ) = 2m 1 t. addition, |u2 (i+1 ) u1 (i+1 )| = 2m 2i 2
strictly less |u2 (i ) u1 (i )| = 2m 2i . Finally, O-contract hP , Qi
deviates sequence Pigou-Dalton deal since
|u2 (Q2 ) u1 (Q1 )| = 2m > |u2 (P2 ) u1 (P1 )|
violates one conditions required Pigou-Dalton deals.
construction two agent settings, easily extends larger numbers.
53

2

fiDunne

Corollary 2 choices h, considered Theorem 3 Corollary 1,
n 2,


77
max
(n, m, , )
2m 2
256
Proof. Fix allocations A1 given 1 , A2 allocated 1 , Aj assigned
3 j n. Using identical utility functions hu1 , u2 previous cases,
employ uj : uj () = 1, uj (S ) = 0 whenever 6= (h, Theorem 3); uj (S ) = 0
(Corollary 1(a)); uj () = 2m , uj (S ) = 0 whenever 6= (Corollary 1(b)); and, finally,
uj (S ) = 2m , (Corollary 1(c)). Considering realisation -deal hP (1) , P (t)
-contract path admissible path defined related proofs. gives
lower bound stated.
2
note, point, consequences Corollary 1 respect (Endriss &
Maudet, 2004b, Theorems 1, 3), state
Fact 1 recall -path, hP (1) , . . . , P (t) maximal allocation Q, hP (t) , Qi
-deal.
a. hP (1) , . . . , P (t) maximal path cooperatively rational deals P (t)
Pareto optimal.
b. hP (1) , . . . , P (t) maximal path equitable deals P (t) maximises
value e (P ) = min1in ui (Pi ), i.e. so-called egalitarian social welfare.
sequence cooperatively rational deals Corollary 1(a) terminates Pareto
optimal allocation P (t) : allocation A2 always utility 0 allocation
A1 whose utility exceed t. Similarly, sequence equitable deals Corollary 1(b)
terminates allocation P (t) , e (P (t) ) = maximum attained
instance defined. cases, however, optima reached sequences
exponentially many (in m) deals: thus, although Fact 1 guarantees convergence particular
deal sequences optimal states may case, illustrated Corollary 1(ab),
process convergence takes considerable time.
2.3 O-contract Paths Monotone Utility Functions
conclude results concerning O-contracts presenting lower bound max
mono , i.e.
length paths utility functions required monotone.
principle one could attempt construct appropriate monotone utility functions
would desired properties respect path used Theorem 3. is, however,
far clear whether construction possible. attempt resolve
question here. Whether exact translation could accomplished is, ultimately, question
purely combinatorial interest: since aim demonstrate exponential length
contract paths needed monotone utility functions not, primarily, concerned
obtaining optimal bound.

54

fiExtremal Behaviour Multiagent Contract Negotiation

Theorem 4 (P , Q) (P , Q) defined Theorem 3 14

max
mono (2, m, , )





77
m/2 3


128 2



even








odd

77
128



2(m1)/2 3

Proof. describe details case even: result
odd obtained simple modification shall merely provide outline.
Let = 2s 7. path
= h1 , 2 , . . . ,
Hs (where describes subset Rs s-bit label), path double(s ) H2s
defined
double(s )

=
=

h 1 1 , 2 2 , . . . , , i+1 i+1 , . . . ,
h1 , 3 , . . . , 2i1 , 2i+1 , . . . , 2t1

(The reason successive indices increasing 2 become clear subsequently)
course, double(s ) describe O-contract path6 : is, however, difficult
interpolate appropriate allocations, 2i , order convert path. Consider
subsets 2i (with 1 < t) defined follows:
(

2i =

i+1
i+1




i+1
i+1

consider path, ext(s ), within H2s given
ext(s ) = h1 , 2 , 3 , . . . , 2(t1) , 2t1
satisfies,
a. property (SC) Theorem 3 Hs ext(s ) property (SC) H2s .
b. j odd |j | = s.
c. j even |j | = + 1.
(a) bounds proved (Abbott & Katchalski, 1991) deduce ext(s )
chosen P (i) denoting allocation hi ,
d. ext(s ) describes O-contract path P (1) P (2t1) .
e. pair hi , j j + 2, deal hP (i) , P (j ) O-contract.
f. chosen proof Theorem 3 number deals ext(s )
given statement present theorem.
6. terms classification described Sandholm (1998), contains swap deals (S -contracts):
deal swaps exactly one item 2i1 item 2i1 order give 2i+1 .

55

fiDunne

therefore fix path Theorem 3 order complete proof
need construct utility functions hu1 , u2 monotone ext(s )
defines unique IR O-contract path realising reallocation hP (1) , P (2t1) i.
choice u2 relatively simple. Given R2s ,
u2 (S ) =



0

2t + 1


2t + 2





|S | 2
|S | = 1
|S |

number allocations . behaviour u2 clearly monotone.
construction u1 rather complicated. main idea make use
fact size set occurring ext(s ) tightly constrained: |i |
either + 1 according whether odd even. first demonstrate
set size + 1 two strict subsets (of size s) occurring within ext(s ):
thus, every size + 1 exactly 2 1 0 subsets size ext(s ). see
suppose contrary. Let , 2i1 , 2j 1 , 2k 1 || = + 1
2i1 ; 2j 1 ; 2k 1
Noting 2i1 = property (SC) must case (at
least) two s-bit labels {i , j , k } differ least two positions. Without loss
generality suppose true k . result deduce sets 2i1
2k 1 2 elements common, i.e. |2i1 2k 1 | 2: 2i1 =
2k 1 = k k position differs k , differs k
exactly position. total |2i1 \ 2k 1 | 2, i.e. (at least) two elements
2i1 occur 2k 1 ; way |2k 1 \ 2i1 | 2, i.e.
(at least) two elements 2k 1 occur 2i1 . set , however,
+ 1 members cannot 2i1 2k 1 subsets: would require
2i1 2k 1 2i1 \ 2k 1 2k 1 \ 2i1
but, seen,
| 2i1 2k 1 2i1 \ 2k 1 2k 1 \ 2i1 | + 2
One immediate consequence argument given set size +1
exactly two strict subsets occurring ext(s ) = 2i1 2i+1 = 2i
value 1 < t. characterise subset R2s size + 1
falling one three categories.
C1. Good sets, given { : = 2i }.
C2. Digressions, consisting
{ : 2i1 , 6= 2i < t}
C3. Inaccessible sets, consisting
{ : neither Good Digression}
56

fiExtremal Behaviour Multiagent Contract Negotiation

Good sets describing allocations A1 within path defined ext(s );
Digressions allocations could reached using O-contract set
size ext(s ), i.e. 2i1 , differ set actually occurs ext(s ), i.e.
2i . Finally, Inaccessible sets occur ext(s ) cannot reached
via O-contract set ext(s ). note view set size + 1
could reached O-contract 2t1 inaccessible: principle
possible extend O-contract path beyond 2t1 , however, choose complicate
construction way.
define u1

u1 () =


2i 1





2i + 1


2i

0




0




2t 1








= 2i1
= 2i
|| = + 1 Digression 2i1
|| 1
|| = 6 ext(s )
Inaccessible || + 2

remains prove choices hu1 , u2 O-contract path hP (1) , . . . , P (2t1)
defined ext(s ) unique IR O-contract path realising IR deal hP (1) , P (2t1)
u1 monotone.
show hP (1) , . . . , P (2t1) IR need demonstrate
1 j < 2t 1 u1 (j ) + u2 (j ) < u1 (j +1 ) + u2 (j +1 )
via definition hu1 , u2
u1 (2i1 ) + u2 (2i1 )

=
<
=
<
=

2(t + ) + 1
u1 (2i ) + u2 (2i )
2(t + ) + 2
u1 (2i+1 ) + u2 (2i+1 )
2(t + ) + 3

Thus, via Definition 4, follows ext(s ) gives rise IR O-contract path.
see path unique IR O-contract path implementing hP (1) , P (2t1) i,
consider position P (j ) = hj , j allocation Q P (j +1) P (j 1) . may
assumed deal hP (j ) , Qi O-contract. j = 2i 1 u (P (2i1) ) = 2(t +i )+1
|j | = s. Hence |Q1 | {s 1, +1}. former case, u1 (Q1 ) = 0 u2 (Q2 ) = 2t +2
u (Q) = 2t + 2 thus hP (j ) , Qi IR. latter case u1 (Q1 ) = 2i
since Q1 Digression 2i1 u2 (Q2 ) = 2t + 1 giving u (Q) = 2(t + ) + 1.
hP (j ) , Qi fails IR since Q fails give increase value u . left
case j = 2i u (P (2i) ) = 2(t + ) + 2 |j | = + 1. Since hP (j ) , Qi assumed
O-contract gives |Q1 | {s, + 2}. first possibility Q1 could
set ext(s ): 2i1 2i+1 subsets 2i two
subsets occurring ext(s ). follows, therefore, u1 (Q1 ) = 0 giving u (Q) = 2t + 2
hP (j ) , Qi IR. second possibility, u1 (Q1 ) = 2t 1 u2 (Q2 ) = 0
|Q2 | = 2 deal would result overall loss. deduce P (j )
IR O-contract consistent deal hP (j ) , P (j +1) i.
57

fiDunne

final stage prove utility function u1 indeed monotone function.
Suppose subsets R2s . need show u1 (S ) u1 (T ).
may assume |S | = s, occurs set within ext(s ), |T | = + 1.
|S | < |S | = occur ext(s ) u1 (S ) = 0 required
inequality holds; |S | + 1 order possible would need
|T | + 2, would give u1 (T ) = 2t 1 maximum value
subset assigned u1 . left |S | = s, |T | = + 1 ext(s )
consider. already shown two subsets occur
ext(s ). Consider different possibilities:
a. = 2i exactly two subsets occur ext(s ): 2i1 2i+1 . Since
u1 (2i ) = 2i + 1 least max{u1 (2i1 ), u1 (2i+1 )}, either
2i1 2i+1 u1 (S ) u1 (T ) required.
b. Digression = 2i1 , u1 (T ) = 2i u1 (S ) = 2i 1 and, again,
u1 (S ) u1 (T ).
deduce u1 monotone completing lower bound proof max
mono even values
m.
conclude observing similar construction used = 2s + 1 odd:
use path ext(s ) described modifying one resource (rm ) always
held A2 . minor modifications utility function definitions needed.
2
Example 2 = 3, choose 3 = h000, 001, 101, 111, 110i = 5.
gives double(3 )
h000111, 001110, 101010, 111000, 110001i
O-contract path defined ext(3 )
=

h000111, 001111, 001110, 101110, 101010, 111010, 111000, 111001, 110001i
h1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9

Considering 15 subsets size + 1 = 4, gives
Good
Digression
Inaccessible

=
=
=

{001111, 101110, 111010, 111001}
{010111, 100111, 101011, 011110, 111100}
{011011, 011101, 101101, 110110, 110011, 110101}

Notice sets {110011, 110101} Inaccessible: principle could
continue 9 = 110001 using either, however, order simplify construction
path halted 9 .
Following construction presented Theorem 4, gives following utility function
definitions R = {r1 , r2 , r3 , r4 , r5 , r6 }.

u2 (S ) =



0

11

12

58





|S | 1
|S | = 2
|S | 3

fiExtremal Behaviour Multiagent Contract Negotiation

u1 obtain

u1 (S ) =





































































0
0
1
2
2
3
3
4
5
5
6
7
7
8
9
9
9



















|S | 2
|S | = 3 6 {000111, 001110, 101010, 111000, 110001}
= 000111 (1 )
= 010111 (digression 1 )
= 100111 (digression 1 )
= 001111 (2 )
= 001110 (3 )
= 011110 (digression 3 )
= 101110 (4 )
= 101010 (5 )
= 101011 (digression 5 )
= 111010 (6 )
= 111000 (7 )
= 111100 (digression 7 )
= 111001 (8 )
= 110001 (9 )
|S | 5 {011011, 011101, 101101, 110110, 110011, 110101}

monotone utility functions, hu1 , u2 i, employed proving Theorem 4 defined
path arising ext(s ) IR: event either agent suffering loss utility
gain made sufficient provide compensatory payment. natural question
arises whether bound obtained Theorem 4 shown apply
rationality conditions preclude monetary payment, e.g. cases concept
rationality one given Definition 7. next result shows set
rationality condition enforce cooperatively rational equitable deals bound
Theorem 4 still holds.
Theorem 5 cases 14
a. () holds cooperatively rational O-contract.
() holds cooperatively rational.
b. () holds equitable O-contract.
() holds equitable.

max
mono (2, m, , )





77
m/2 3


128 2



even








odd

77
128



2(m1)/2 3

Proof. illustrate constructions case even, noting
modification deal odd values outlined end proof Theorem 4.
path ext(s ) used cases.

59

fiDunne

(a), require hu1 , u2 defined monotone functions ext(s )
unique cooperatively rational O-contract path realise cooperatively rational
deal hP (1) , P (2t1) P (j ) = hj , j i. case set hu1 , u2 be,

hu1 (), u2 ()i =


hi ,





hi
+ 1,




hi , 1i


h0, 2t 1i




h0, 2t 1i




h2t 1, 0i








= 2i1
= 2i
|| = + 1 Digression 2i1
|| 1
|| = 6 ext(s )
Inaccessible || + 2

Since,
hu1 (2i1 ), u2 (2i1 )i
hu1 (2i ), u2 (2i )i
hu1 (2i+1 ), u2 (2i+1 )i

=
=
=

hi ,
hi + 1,
hi + 1, + 1i

certainly case hP (1) , P (2t1) deals O-contract path defined
ext(s ) cooperatively rational. Furthermore Q = h, allocation
P (j +1) deal hP (j ) , Qi fail cooperatively rational O-contract.
suppose contrary letting hP (j ) , Qi without loss generality O-contract,
Q 6 {P (j 1) , P (j +1) } rule former case since already shown
deal cooperatively rational. j = 2i 1 hu1 (j ), u2 (j )i = hi ,
|| {s 1, + 1}: former case leads loss utility A1 ; latter,
(since Digression 2i1 ) loss utility A2 . Similarly, j = 2i
hu1 (j ), u2 (j )i = hi + 1, || {s, + 2}: first 6 ext(s ) leading loss
utility A1 ; second results loss utility A2 . follows path defined
ext(s ) unique cooperatively rational O-contract path realises hP (1) , P (2t1) i.
remains show choices hu1 , u2 define monotone utility functions.
Consider u1 suppose subsets R2s . |S | 1,
occur ext(s ) u1 (S ) = 0. |T | + 2 Inaccessible
u1 (T ) = 2t 1 maximum value attainable u1 . may assume
|S | = s, occurs ext(s ), i.e. = 2i1 , , |T | = + 1 either
Good set Digression. definition u1 , u1 (S ) = : {2i , 2i2 }
u1 (T ) = u1 (S ); Digression 2i1 u1 (T ) = = u1 (S ). deduce
u1 (S ) u1 (T ), i.e. utility function monotone.
consider u2 subsets R2s . |T | + 1
R2s \ occur ext(s ) u2 (T ) = 2t 1 maximal value. |S | 2
R2s \ Inaccessible u2 (S ) = 0. Thus may assume = 2i1 giving
u2 (T ) = |S | = 1, R2s \ either Digression one Good sets
{2i , 2i2 }. R2s \ Digression u2 (S ) = 1; Good set 2i2
u2 (S ) = 1 < u2 (T ); Good set 2i u2 (S ) = = u2 (T ). follows
u2 monotone completing proof part (a).

60

fiExtremal Behaviour Multiagent Contract Negotiation

(b) use,

hu1 (), u2 ()i =


h2i 1, 2i





h2i + 1, 2i












h2i , 2i 1i
h0, 2t 1i
h0, 2t 1i
h2t 1, 0i








= 2i1
= 2i
|| = + 1 Digression 2i1
|| 1
|| = 6 ext(s )
Inaccessible || + 2

choices give ext(s ) unique equitable O-contract path realise equitable
deal hP (1) , P (2t1) i, since
min{u1 (2i1 ), u2 (2i1 )}
min{u1 (2i ), u2 (2i )}
min{u1 (2i+1 ), u2 (2i+1 )}

=
=
=

2i 1
2i
2i + 1

deal hP (j ) , P (j +1) equitable. Q = h, allocation P (j +1)
deal hP (j ) , Qi equitable O-contract. Assume hP (j ) , Qi Ocontract, Q 6 {P (j 1) , P (j +1) }. j = 2i 1, P (j ) = h2i1 , 2i1
min{u1 (2i1 ), u2 (2i1 )} = 2i 1 || {s 1, + 1}. first
min{u1 (), u2 ()} = 0; second min{u1 (), u2 ()} = 2i 1 since must
Digression. leaves j = 2i P (j ) = h2i , 2i min{u1 (2i ), u2 (2i )} = 2i .
this, || {s, + 2}: || = min{u1 (), u2 ()} 2i 1 (with equality
= 2i1 ); || = + 2 min{u1 (), u2 ()} = 0. total establish ext(s )
unique equitable O-contract path realising equitable deal hP (1) , P (2t1) i.
choices hu1 , u2 describe monotone utility functions shown
similar argument part (a).
2
Example 3 = 3 using O-contract path ext(3 ) previous example,
i.e.
=

h000111, 001111, 001110, 101110, 101010, 111010, 111000, 111001, 110001i
h1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9

hu1 , u2 (a) obtain

hu1 (S ), u2 (R \ )i =















































h0, 9i
h0, 9i
h1, 1i
h1, 0i
h1, 0i
h2, 1i
h2, 2i
h2, 1i
h3, 2i
h3, 3i
h3, 2i
h4, 3i
h4, 4i
h4, 3i
h5, 4i
h5, 5i
h9, 0i



















|S | 2
|S | = 3 6 {000111, 001110, 101010, 111000, 110001}
= 000111 (1 )
= 010111 digression 1
= 100111 digression 1
= 001111 (2 )
= 001110 (3 )
= 011110 digression 3
= 101110 (4 )
= 101010 (5 )
= 101011 digression 5
= 111010 (6 )
= 111000 (7 )
= 111100 digression 7
= 111001 (8 )
= 110001 (9 )
|S | 5 {011011, 011101, 101101, 110110, 110011, 110101}

61

fiDunne

Similarly, (b)

hu1 (S ), u2 (R \ )i =















































h0, 9i
h0, 9i
h1, 2i
h2, 1i
h2, 1i
h3, 2i
h3, 4i
h4, 3i
h5, 4i
h5, 6i
h6, 5i
h7, 6i
h7, 8i
h8, 7i
h9, 8i
h9, 10i
h9, 0i



















|S | 2
|S | = 3 6 {000111, 001110, 101010, 111000, 110001}
= 000111 (1 )
= 010111 digression 1
= 100111 digression 1
= 001111 (2 )
= 001110 (3 )
= 011110 digression 3
= 101110 (4 )
= 101010 (5 )
= 101011 digression 5
= 111010 (6 )
= 111000 (7 )
= 111100 digression 7
= 111001 (8 )
= 110001 (9 )
|S | 5 {011011, 011101, 101101, 110110, 110011, 110101}

demonstrate similar extremal behaviours contract path length
rationality constraints money-based (individual rationality) money-free (cooperative rationality, equitable) settings irrespective whether monotonicity properties
assumed, interesting parallels contexts monotonicity relevant. particular observe common complexity results already
noted (Dunne et al., 2003) deciding allocation Pareto optimal, allocation maximises u , IR O-contract path exists requiring utility functions
monotone result setting computationally tractable.

3. (k )-contract paths
turn similar issues respect (k )-contracts, recalling one respect
offer form deal fit classification Sandholm (1998).
classification defines four forms contract type: O-contracts, considered previous
section; -contracts, involve exactly 2 agents swapping single resources; C -contracts,
one agent tranfers least two resources another; -contracts
three agents reallocate resource holding amongst themselves. definition
(k )-contracts permits two agents exchange resources (thus -contracts
Sandholms (1998) scheme) deals permitted restricted O, , C contracts. one regard, however, (k )-contracts general -contracts since
preset bound (k ) specified number agents involved.
main result (k )-contract paths following development Theorem 3.
Theorem 6 Let k (P , Q) predicate
holds whenever hP , Qi IR (k )k
contract. k 3, n k
, resource allocation setting
2
hA, R, Ui IR deal = hP , Qi which,
Lopt (, hA, R, Ui, k )
Lopt (, hA, R, Ui, k 1 )
Lopt (, hA, R, Ui, k 2 )

=


62

1
2b2m/k (k 1)c

1
undefined

(a)
(b)
(c)

fiExtremal Behaviour Multiagent Contract Negotiation

presenting proof, comment formulation theorem statement
give overview proof structure.
first note lower bounds (where defined) phrased terms
function Lopt opposed max used various results O-contract paths
Section 2.2. is, course, case bound claimed Lopt (, hA, R, Ui, k 1 )
also lower bound max (n, m, k 1 , ) n k (P , Q) holds whenever
deal hP , Qi IR. statement Theorem 6, however, claims rather this,
namely specific resource allocation setting hA, R, Ui defined n k
m, together IR deal hP , Qi way that: hP , Qi achieved
single (k )-contract cannot realised IR (k 2)-contract path. Recalling
Lopt partial function, latter property equivalent claim made part
(c) deal hP , Qi theorem statement. Furthermore, deal although
achievable IR (k 1)-contract path realised one whose length
given part (b) theorem statement.
Regarding proof itself, number notational complexities
attempted ameliorate making simplifying assumptions concerning relationship size resource set R k number agents
needed realise hP
, Qi
single IR deal. particular, shall assume
exact multiple k2 . observe employing similar device used
proof
Theorem 4 deal cases
property:
k
= 2 + q integer values 1 1 q < k2 , simply employ exactly
construction using q resources missing q resources Rm allocated A1 never reallocated within (k 1)-contract path. approach
accounts rounding operation (b. . .c) exponent term lower bound.
shall also assume number agents exactly k . Within proof use
running example k = 4 = 18 = 3 6 illustrate specific features.
first give outline structure.
Given hA, R, Ui resource allocation setting involving k agents resources,
aim define IR (k 1)-contract path
= hP (1) , P (2) , . . . , P (t)
realises IR (k ) deal hP (1) , P (t) i. use index particular allocations
within , 1 t.
order simplify presentation employ
setting k agents
= {A0 , A1 , . . . , Ak 1 }. Recalling = k2 , resource set Rm formed
union



k
2



pairwise disjoint sets size s. Given distinct values j
{i,j }

{i,j }

{i,j }

0 < j k 1, use Ri,j denote one subsets {r1 , r2
, . . . , rs }
{i,j
}
resources form R
.
two main ideas underpinning structure (k 1)-contract .
Firstly, initial subsequent allocations, resource set R{i,j } partitioned
Ai Aj reallocation resources Ai Aj takes place
within deal hP (d) , P (d+1) involve resources set. Thus, every al(d)
location P (d) pair {i , j }, h 6 {i , j } Ph R{i,j } = . Furthermore,
63

fiDunne

= hP (d) , P (d+1) Ai Aj involved, i.e. {Ai , Aj } , reallocation R{i,j } Ai Aj O-contract. is, either exactly one
(d)
(d+1)
element R{i,j } moved Pi become member allocation Pj

(d)

exactly one element R{i,j } moved Pj

become member allocation

(d+1)
Pi . Intotal, every (k 1)-contract consists simultaneous implementation
1
k
O-contracts: single O-contract distinct pairs {Ai , Aj } agents
2
k 1 agents .

second key idea exploit one well-known property s-dimensional hypercube network: every 2, Hs contains Hamiltonian cycle, i.e. simple directed cycle
formed using edges Hs containing 2s vertices.7 Now, suppose
(v ) = v (0) , v (1) , . . . , v (i) , . . . , v (2

1)

, v (0)

Hamiltonian cycle hypercube Hs
(w ) = w (0) , w (1) , . . . , w (i) , . . . , w (2

1)

, w (0)

Hamiltonian cycle w (i) obtained complementing bit v (i) .
described overview Section 2.1 interpret s-bit label v = v1 v2 . . . vs
{i,j }
describing particular subset R{i,j } , i.e. subset rk
occurs
{i,j
}
vk = 1. Similarly subset R
may define unique s-bit word. suppose
(d)
Pi allocation held Ai allocation P (d) . deal = hP (d) , P (d+1)
(d)
(d+1)
affect Pi R{i,j } following way: 6 j 6 Pi
R{i,j } =
(d)
(d+1)
(d)
Pi R{i,j } Pj
R{i,j } = Pj R{i,j } . Otherwise {i , j }
(d)

(d)

(complementary) holdings Pi R{i,j } Pj R{i,j } define (complementary) s-bit
labels vertices Hs : correspond places hv (h) , w (h) Hamiltonian cycles,
(d+1)
(d+1)
(d+1)
(d+1)
Pi
Pj
s-bit labels defined Pi
R{i,j } Pj
R{i,j }
produce s-bit labels v (h+1) w (h+1) , i.e. vertices succeed v (h) w (h)
Hamiltonian cycles. total, j , Ai initially holds either subset R{i,j }
maps v (0) maps w (0) and, conclusion (k 1)-path, holds


subset maps v (2 1) (or w (2 1) ). final detail progression
Hamiltonian cycles conducted series rounds round comprising k (k 1)deals.
(d)
(d+1) occurs path
noted
(k1)-contract, hP , P
k 1
interpreted set
distinct O-contracts. important property utility
2
functions employed unless p k 1 beno individually
rational (p)
1
contract path realises deal hP (d) , P (d+1) i, i.e. k
O-contract
deals must
2
occur simultaneously order progression P (d) P (d+1) IR. Although
required deal could realised sequence O-contracts (or, generally,
suitable (k 2)-contract path), realisations describe IR contract path.
7. shown easy inductive argument. = 2, sequence h00, 01, 11, 10, 00i defines
Hamiltonian cycle H2 . Inductively assume h1 , 2 , . . . , p , 1 (with p = 2s ) cycle
Hs h01 , 11 , 1p , 1p1 , . . . , 12 , 02 . . . , 0p , 01 defines Hamiltonian cycle Hs+1 .

64

fiExtremal Behaviour Multiagent Contract Negotiation

construction utility functions guarantee behaviour provides principal
component showing IR deal hP (1) , P (t) cannot realised IR (k 2)contract path: Q allocation hP (1) , Qi (k 2)-contract
hP (1) , Qi IR.
proceed proof Theorem 6.
Proof. (of Theorem 6) Fix = {A0 , A1 , . . . , Ak 1 }. R consists
sets resources
{i,j } {i,j }
R{i,j } = {r1 , r2 , . . . , rs{i,j } }



k
2



pairwise disjoint

k = 4 = 3 yield = {A0 , A1 , A2 , A3 }
R{0,1}
R{0,2}
R{0,3}
R{1,2}
R{1,3}
R{2,3}

=
=
=
=
=
=

{0,1}

{0,1}

{0,1}

, r2
, r3
}
{r1
{0,2} {0,2} {0,2}
{r1
, r2
, r3
}
{0,3} {0,3} {0,3}
{r1
, r2
, r3
}
{1,2} {1,2} {1,2}
, r2
, r3
}
{r1
{1,3} {1,3} {1,3}
{r1
, r2
, r3
}
{2,3} {2,3} {2,3}
{r1
, r2
, r3
}

use two ordering structures defining (k 1)-contract path.
a.
(v ) = v (0) , v (1) , . . . , v (i) , . . . , v (2

1)

, v (0)

Hamiltonian cycle Hs , without loss generality, v (0) = 111 . . . 11.
b.
(w ) = w (0) , w (1) , . . . , w (i) , . . . , w (2

1)

, w (0)

complementary Hamiltonian cycle this, w (0) = 000 . . . 00.
Thus k = 4 = 3 obtain
a.
b.

(v ) = h111, 110, 010, 011, 001, 000, 100, 101i
(w ) = h000, 001, 101, 100, 110, 111, 011, 010i

describe (k 1)-contract path.
= hP (1) , P (2) , . . . , P (t)

Initial Allocation: P (1) .
Define k k Boolean matrix, B = [bi,j ] (with 0 , j k 1)

bi,j =





b

j ,i

b
i,j 1

65





=j
>j
<j

fiDunne

1 k ,
(1)

Pi

=

i1
[

{ R {j ,i} : bi,j = >}

j =0

k[
1

{ R {i,j } : bi,j = >}

j =i+1

Thus, example,





B =



>


>


>


>



>

>








Yielding starting allocation
(1)

P0
(1)
P1
(1)
P2
(1)
P3

=
=
=
=

R{0,1} R{0,3}
R{1,2}
R{0,2} R{2,3}
R{1,3}

=
=
=
=

h111, 000, 111i
h000, 111, 000i
h111, 000, 111i
h000, 111, 000i






R{0,1} R{0,2} R{0,3}
R{0,1} R{1,2} R{1,3}
R{0,2} R{1,2} R{2,3}
R{0,3} R{1,3} R{2,3}

(1)

third column Pi indicating 3-bit labels characterising subsets
R{i,j } three values j assume.
Rounds: initial allocation changed series rounds
Q 1, Q 2, . . . , Q z
involves exactly k distinct (k 1)-contracts. use Q x ,p indicate
allocation resulting stage p round x 0 p k 1. note following:
a. initial allocation, P (1) denoted Q 0,k 1 .
b. Q x ,0 obtained using single (k 1)-contract Q x 1,k 1 (when x 1).
c. Q x ,p obtained using single (k 1)-contract Q x ,p1 (when 0 < p k 1).
final item notation cube position respect j allocation
P , denoted (i , j , P ). Letting u s-bit string describing Pi R{i,j } allocation
(1)
P , (i , j , P ) index u Hamiltonian cycle (v ) (when R{i,j } Pi )
(1)
Hamiltonian cycle (w ) (when R{i,j } Pj ). P = Q x ,p allocation
sequence construction employ notation (i , j , x , p), noting one invariant
path (i , j , x , p) = (j , , x , p), property certainly holds true P (1) =
Q 0,k 1 since (i , j , 0, k 1) = (j , , 0, k 1) = 0.
sequence allocations built follows. Since Q 1,0 immediate successor
initial allocation Q 0,k 1 , suffices describe Q x ,p formed Q x ,p1 (when
p > 0) Q x +1,0 Q x ,k 1 . Let Q y,q allocation formed Q x ,p .
deal = hQ x ,p , Q y,q (k 1) contract = \ {Aq }. pair
{i , j } (i , j , x , p) = (j , , x , p) allocation Q x ,p . moving Q y,q
exactly one element R{i,j } reallocated Ai Aj way Q y,q ,
66

fiExtremal Behaviour Multiagent Contract Negotiation

(i , j , y, q) = (i , j , x , p)+1, since Ai Aj tracing complementary Hamiltonian cycles
respect R{i,j } ensures (j , , y, q) = (j , , x , p) + 1, thereby maintaining
invariant property.
Noting distinct pair hi , j i, either R{i,j } allocated Ai P (1)
R{i,j } allocated Aj P (1) , description outlined indicates allocation
P (d) = Q x ,p completely specified follows.
cube position, (i , j , x , p), satisfies,

(i , j , x , p) =


0





1 + (i , j , x 1, k 1)

(i , j , x 1, k 1)




1 + (i , j , x , p 1)




(i , j , x , p 1)







x = 0 p = k 1
x 1, p = 0, p 6 {i , j }
x 1, p = 0, p {i , j }
1 p k 1, p 6 {i , j }
1 p k 1, p {i , j }

, subset R{i,j } held Ai allocation Q x ,p is,

v ((i,j ,x ,p))
w ((i,j ,x ,p))




(1)

R{i,j } Pi
(1)
R{i,j } Pj

(where recall s-bit labels hypercube Hs identified subsets
R{i,j } .)
tables illustrates process example.


1
2
3
4
5
6
7
8
9
..
.

x
0
1
1
1
1
2
2
2
2
..
.

p
3
0
1
2
3
0
1
2
3
..
.

j
01
111
111
111
110
010
010
010
011
001

A0
j j
02 03
000 111
000 111
001 110
001 010
101 010
101 011
100 001
100 001
110 001
..
.
Subsets

j
10
000
000
000
001
101
101
101
100
110

A1
j
12
111
110
110
110
010
011
011
011
001
..
.

j
13
000
001
001
101
101
100
100
110
110

j
20
111
111
110
110
010
010
011
011
001

R{i,j } held Ai

67

A2
j j
21 23
000 111
001 110
001 010
001 010
101 010
100 011
100 001
100 001
110 001
..
.
Q x ,p (k

j
30
000
000
001
101
101
101
100
110
110
= 4,

A3
(d1)
,P (d)
j j AhP
31 32
111 000

110 001 {A1 , A2 , A3 }
110 101 {A0 , A2 , A3 }
010 101 {A0 , A1 , A3 }
010 101 {A0 , A1 , A2 }
011 100 {A1 , A2 , A3 }
011 110 {A0 , A2 , A3 }
001 110 {A0 , A1 , A3 }
001 110 {A0 , A1 , A2 }
..
..
.
.
= 3)

fiDunne


1
2
3
4
5
6
7
8
9
..
.

x
0
1
1
1
1
2
2
2
2
..
.

p
3
0
1
2
3
0
1
2
3
..
.

A0
A1
A2
A3
(d1) ,P (d)
j j j j j j j j j j j j AhP
01 02 03 10 12 13 20 21 23 30 31 32
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
1
1
0
1
1
0
1
1 {A1 , A2 , A3 }
0
1
1
0
1
1
1
1
2
1
1
2 {A0 , A2 , A3 }
1
1
2
1
1
2
1
1
2
2
2
2 {A0 , A1 , A3 }
2
2
2
2
2
2
2
2
2
2
2
2 {A0 , A1 , A2 }
2
2
2
2
3
3
2
3
3
2
3
3 {A1 , A2 , A3 }
2
3
3
2
3
3
3
3
4
3
3
4 {A0 , A2 , A3 }
3
3
4
3
3
4
3
3
4
4
4
4 {A0 , A1 , A3 }
4
4
4
4
4
4
4
4
4
4
4
4 {A0 , A1 , A2 }
..
..
..
..
..
.
.
.
.
.
Cube Positions (i , j , x , p) (k = 4, = 3)

certainly case process applying successive rounds k deals could
continued, however, wish long possible go
allocation P (d) sequence another P (d+r ) r 2 via (k 1)-contract.
Q x ,p Q y,q distinct allocations generated process
deal = hQ x ,p , Q y,q (k 1)-contract Ai , Qix ,p = Qiy,q .
follows hP (d) , P (d+r ) (k 1)-contract r > 1,
(d+r )
(d)
j 6= , Pi
R{i,j } = Pi R{i,j } .
(d+r )
(d)
determine minimum value r > 1 Pi
= Pi , observe
without loss generality need consider case = = 0, i.e. determine
(1)
minimum number deals P0 reappears. First note round, Q x ,
(0, j , x 1, k 1) = p (0, j , x , k 1) = p + k 2, i.e. round advances cube
position k 2 places: (0, j , x 1, k 1) = (0, j , x , 0) (0, j , x , j ) = (0, j , x , j 1).
(1)
also observe P0 = Q00,k 1 6= Q0x ,p p 0 < p < k 1, since
(0, 1, x , p) = (0, 2, x , p) = . . . = (0, k 1, x , p)
cases p = 0 p = k 1. follows value r > 1 must form
qk q must q(k 2) exact multiple 2s . observation
see that,
(1)

min{ r > 1 : P0

(1+r )

= P0

} = min{ qk : q(k 2) multiple 2s }

Now, k odd q = 2s minimal value, r = k 2s . k even
may uniquely written form z 2l + 2 z odd giving q 1 (if l s)
2sl (if l s), give r = k r = z 2s + 2sl+1 , e.g. k = 4 = 3,
(1)
(17)
get k = 1 21 + 2 r = 23 + 231+1 = 16 example P0 = P0 may
easily verified. total,
r




k2

k


2s





k odd
k = z 2l + 2, z odd, l
k = z 2l + 2, z odd l
68

fiExtremal Behaviour Multiagent Contract Negotiation

immediately give r 2s (in second case k 2s , inequality holds

trivially), thus

continue chain (k 1) contracts least 2 moves.
Recalling = k2 , gives length (k 1)-contract path
= hP (1) , P (2) , . . . , P (t)
written terms k least8
m/



2

k
2


2m

1 = 2 k (k 1) 1

remains define appropriate utility functions U = hu0 , . . . , uk 1 order ensure
unique IR (k 1)-contract path realising IR (k )-deal hP (1) , P (t) i.
defining U convenient denote path
= hQ 0,k 1 , Q 1,0 , Q 1,1 , . . . , Q 1,k 1 , . . . , Q x ,p , . . . , Q r ,k 1
and, since rk 2s , may without loss generality, focus first 2s allocations
contract path.
Recalling (i , j , x , p) index s-bit label u corresponding Qix ,p R{i,j }
relevant Hamiltonian cycle i.e. (v ) R{i,j } Qi0,k , (w ) R{i,j } Qj0,k 1
note following properties sequence allocations defined hold
distinct j .
P1. x , p (i , j , x , p) = (j , , x , p)
P2. Q y,q immediate successor Q x ,p (i , j , y, q) (i , j , x , p) + 1
equality q 6 {i , j }.
P3. 0 , j 0 0 0 , j 0 k 1, (i , j , x , k 1) = (i 0 , j 0 , x , k 1).
first two properties already established description . third
follows observation within round Q x , cube position advanced
exactly k 2 progressing Q x 1,0 Q x ,k 1 .
utility function ui given, Rm ,
( P

j 6=i
2km

ui (S ) =

(i , j , x , p)

= Qix ,p 0 x r , 0 p k 1
otherwise

claim that, choices,
= hQ 0,k 1 , Q 1,0 , Q 1,1 , . . . , Q 1,k 1 , . . . , Q x ,p , . . . , Q r ,k 1
unique IR (k 1)-contract path realising IR (k )-deal hQ 0,k 1 , Q r ,k 1 i. Certainly, IR (k 1)-contract path: deal = hQ x ,p , Q y,q path
|A | = k 1 since agent Ai = \ {Aq } utility Qiy,q increased
8. omit
operation b. . .c exponent, significant exact
rounding

multiple

k
2

, event device described overview proof applied.

69

fiDunne

exactly k 2, i.e. cube position respect j whenever q 6 {i , j }
increased, follows u (Q y,q ) > u (Q x ,p ) hence hQ x ,p , Q y,q IR.
show unique IR (k 1)-contract path continuation Q 0,k 1
Suppose = hQ x ,p , P deal deviates contract path (having followed
allocation Q x ,p ). Certainly following must hold P :
, Pi j 6=i R{i,j } ; k -tuple pairs h(x0 , p0 ), . . . , (xk 1 , pk 1 )i
Pi = Qixi ,pi , either fail case , ui (Pi ) = 2km
consequent effect u (P ) < 0 thence IR. Now, Q y,q allocation
would succeed Q x ,p P 6= Q y,q , thus least one agent, Qixi ,pi 6= Qiy,q .
cannot case Qixi ,pi corresponds allocation occurring strictly later
Qiy,q since allocations could realised (k 1)-contract. addition,
since Pi = Qixi ,pi must case |A | = k 1 since exactly k 1 cube positions
holding Ai must change. follows two possibilities (yi , pi ):
Pi reverts allocation immediately preceding Qix ,p advances holding Qiy,q .
suffices observe deal agents satisfy first
remainder proceed accordance second either give rise valid
allocation cannot realised (k 1)-contract. hand P corresponds
allocation preceding Q x ,p IR. deduce, therefore, IR
(k 1) deal consistent Q x ,p prescribed Q y,q .
completes analysis needed proof part (b) theorem.
clear since system contains k agents, deal hP , Qi effected
single (k )-contract, thereby establishing part (a). part (c) IR deal
hP (1) , P (t) cannot realised using individually rational (k 2)-contract path,
suffices observe since class IR (k 2)-contracts subset class
IR (k 1)-contracts, case IR (k 2)-contract path existed
implement hP (1) , P (t) i, would imply unique IR (k 1)-contract
path. have, however, proved unique, part (c) theorem follows. 2
obtain similar development Corollary 1
Corollary 3 k 3, n k ,



k
2



cases below,

a. k () holds cooperatively rational (k )-contract.
() holds cooperatively rational.
b. k () holds equitable (k )-contract.
() holds equitable.
resource allocation setting hA, R, Ui -deal = hP , Qi
Lopt (, hA, R, Ui, k )
Lopt (, hA, R, Ui, k 1 )
Lopt (, hA, R, Ui, k 2 )

=


1
2b2m/k (k 1)c 1
undefined

(a)
(b)
(c)

Proof. proof Corollary 1 relation Theorem 3, case employ
contract path proof Theorem 6, varying definition U = hu1 , u2 , . . . , uk
order establish result. Thus let


=
=

hP (1) , P (2) , . . . , P (r ) , . . . , P (t)
hQ 0,k 1 , Q 1,0 , . . . , Q x ,p , . . . , Q z ,r
70

fiExtremal Behaviour Multiagent Contract Negotiation

(k 1)-contract path realising (k )-deal hP (1) , P (t) described proof
Theorem 6, path length 2b2m/k (k 1) 1.
a. utility functions U = hu0 , . . . , uk 1 Theorem 6 ensure hP (1) , P (t)
cooperatively rational cooperatively rational (k 1)-contract
path realising hP (1) , P (t) i: utility held Ai never decreases value
least one agent (in fact exactly k 1) whose utility increases value. Furthermore
unique cooperatively rational (k 1)-contract path realising hP (1) , P (t)
since, argument used Theorem 6, deviation result
agent suffering loss utility.
b. Set utility functions U = hu0 , . . . , uk 1 as,

ui (S ) =



1





xk 2 + k



2

(x 1)k + k + p


(x 1)k 2 + k + p + 1





xk 2 + 1


xk 2 + 1 + p















6= Qix ,p Q x ,p
= Qix ,k 1
= Q0x ,p , p < k 1 = 0
= Qix ,p , p < 1 6= 0.
= Qix ,i1 = Qix ,i 6= 0.
= Qix ,p , p > 6= 0

see choices admit equitable (k 1)-contract path realising
equitable deal hQ 0,k 1 , Q z ,r i, first note
min

0ik 1

{ui (Qiz ,r )} > 1 =

min

0ik 1

{ui (Qi0,k 1 )}

thus, hQ 0,k 1 , Q z ,r indeed equitable. Consider deal = hQ x ,p , Q y,q occurring
within . suffices show
min

0ik 1

{ui (Qix ,p )} 6= uq (Qqx ,p )

since Aq 6 , agents ui (Qiy,q ) > ui (Qix ,p ). two possibilities:
q = 0 (in case p = k 1 = x + 1); q > 0 (in case p = q 1).
Consider first these: u0 (Q0x ,k 1 ) = xk 2 + k , however,
,k 1
)
min{ui (Qix ,k 1 )} = xk 2 + 1 = uk 1 (Qkx1

hence every deal hQ x ,k 1 , Q x +1,0 forming part equitable.
remaining case, uq (Qqx ,q1 ) = xk 2 + 1
min{ui (Qix ,q1 )}


=
<
=
<
=

u0 (Q0x ,q1 )
(x 1)k 2 + k + q 1
xk 2 (k 2 2k + 1)
xk 2 (k 1)2
xk 2 + 1
uq (Qqx ,q1 )

thus remaining deals hQ x ,q1 , Q x ,q within equitable. similar
argument employed Theorem 6 follows unique equitable
(k 1)-contract path realising hQ 0,k 1 , Q z ,r i.
2

71

fiDunne

Monotone Utility Functions (k )-contract paths
device used develop Theorem 3 obtain path Theorem 4 applied
rather intricate construction Theorem 6, thereby allowing exponential lower
bounds max
mono (n, m, k , ) derived. merely outline approach rather
present detailed technical exposition. recall became relatively straightforward
define suitable monotone utility functions ensured subset sizes
interest i.e. allocations arising O-contract path forced fall
quite restricted range. main difficulty arises applying similar methods
path Theorem 6 following: proof Theorem 4 consider two agents
converting setting resources Theorem 3 ext(s ) 2s
resources Theorem 4 achieved combining complementary allocations, i.e. Rs
Ts . exploit two facts, however, develop path multi ()
monotone
utility functions could defined: resource set Rm Theorem 6 consists


k
disjoint sets size s; deal path involves reallocation R{i,j }
2
Ai Aj {i , j } . Thus letting Tm formed
{i,j } size s, suppose
(d)

(d)
Pi

(d)



k
2



disjoint sets,

described
(d)

(d)

(d)

i,0 i,1 i,i1 i,i+1 i,k 1
(d)

i,j s-bit label corresponding subset R {i,j } held Ai P (d) .
Consider sequence allocations,
multi () = hC (1) , C (2) , . . . , C (t)
(d)

resource allocation setting k agents 2m resources Rm Tm Ci
characterised
(d) (d)
(d)
(d)
(d)
i,0 i,1 i,i1 i,i+1 i,k 1
(d)

this, i,j , indicates subset R{i,j } {i,j } described 2s-bit label,
(d)

i,j
(d)

(d)

(d)

= i,j i,j
(d)

i.e. i,j selects subset R{i,j } i,j subset {i,j } .
immediate construction allocation C (d) multi ()
(d)
Ai , always case |Ci | = (k 1)s. follows, therefore, subsets
relevant definition monotone utility functions analogous
result Theorem 6 path multi () could derived, size (k 1)s:
Rm Tm |S | < (k 1)s, fix ui (S ) small enough negative value; similarly
|S | > (k 1)s ui (S ) set large enough positive value.9
description preceding paragraphs, summarised following result, whose proof omitted: extending outline given formal lower bound
9. worth noting interpolation stage used Theorem 4 needed forming multi():
deal hC (d) , C (d+1) (k 1)-contract. recall going Theorem 3 ext(s )
intermediate stage double(s ) O-contract path.

72

fiExtremal Behaviour Multiagent Contract Negotiation

proof, largely technical exercise employing much analysis already introduced,
since nothing signifcantly new required analysis shall give detailed
presentation it.
Theorem 7 Let k (P , Q) predicate
holds whenever hP , Qi IR (k )contract. k 3, n k 2 k2 , resource allocation setting
hA, R, Ui every u U monotone, IR deal = hP , Qi which,
Lopt (, hA, R, Ui, k )
Lopt (, hA, R, Ui, k 1 )
Lopt (, hA, R, Ui, k 2 )

=


1
2bm/k (k 1)c

1
undefined

(a)
(b)
(c)

4. Related Work
principal focus article considered property contract paths realising rational reallocations hP , Qi constituent deals required conform structural
restriction satisfy rationality constraint. Section 2 structural restriction limited
deals involving single resource, i.e. O-contracts. rationality constraint
forcing deals strictly improve utilitarian social welfare, i.e. individually rational
(IR) following properties.
a. resource allocation settings hA, R, Ui within IR reallocations
hP , Qi cannot realised sequence IR O-contracts. (Sandholm, 1998,
Proposition 2)
b. Every IR reallocation, hP , Qi, realised IR O-contract path,
realised IR O-contract path length n (n 1)m. (Sandholm, 1998,
Proposition 2)
c. Given hA, R, Ui together IR reallocation hP , Qi problem deciding
hP , Qi implemented IR O-contract path nphard, even |A| = 2
utility functions monotone. (Dunne et al., 2003, Theorem 11).
d. resource allocation settings hA, R, Ui within IR reallocations
hP , Qi realised IR O-contract path, path
length exponential m. holds even case |A| = 2 utility functions
monotone. (Theorem 3 Theorem 4 Section 2)
recent article Endriss Maudet (2004a) analyse contract path length also considering
O-contracts various rationality constraints. Although approach rather
different perspective, central question addressed many rational deals required
reach optimal allocation?, (Endriss & Maudet, 2004a, Table 1, p. 629) closely
related issues discussed above. One significant difference analysis rational Ocontracts Sandholms (1998) treatment results Section 2 (Endriss
& Maudet, 2004a) utility functions restricted every rational reallocation
hP , Qi realised rational O-contract path. two main restrictions examined
P
requiring utility functions additive, i.e. every R, u(S ) = r u(r );
73

fiDunne

and, requiring value returned either 0 1, so-called 0 1 utility functions.
Additive utility functions considered case IR O-contracts (Endriss & Maudet,
2004a, Theorems 3, 9), whereas 01 utility functions cooperatively rational O-contracts
max
(Endriss & Maudet, 2004a, Theorems 4, 11). Using max
add (n, m, , ) 01 (n, m, , )
denote functions introduced Definition 6 utility functions additive
(respectively 01), cf. definition max
mono , 1 (P , Q) holding hP , Qi IR
O-contract; 2 (P , Q) holding hP , Qi cooperatively rational O-contract (P , Q)
true hP , Qi IR, may formulate Theorems 9 11 (Endriss & Maudet, 2004a)
terms framework used Definition 6,
max
add (n, m, 1 , )
max
01 (n, m, 2 , )

=
=




(Endriss & Maudet, 2004a, Theorem 9)
(Endriss & Maudet, 2004a, Theorem 11)

can, course, equally couch Theorems 3 4 Section 2 terms shortestpath convention adopted (Endriss & Maudet, 2004a), provided domains
utility reallocation instances restricted appropriate O-contract
path exists. Thus, obtain following development (Endriss & Maudet, 2004a,
Table 1) case O-contracts.
Utility Functions
Rationality
Shortest Path
Complete

Additive
IR

Yes

0-1
CR

Yes

Unrestricted
IR
(2m )


Monotone
IR
(2m/2 )


Unrestricted
CR
(2m )


Monotone
CR
(2m/2 )


Table 2: many O-contract rational deals required reach allocation?
Extension Table 1 (Endriss & Maudet, 2004a, p. 629)

5. Conclusions Work
aim article develop earlier studies Sandholm (1998) concerning
scope limits particular practical contract forms. Sandholm (1998)
established insisting individual rationality addition structural restriction
prescribed O-contracts leads scenarios incomplete (in sense
individually rational deals cannot realised individually rational O-contracts)
focus respect deals realised restricted contract paths,
intention determining extent combination structural rationality conditions increases number deals required. shown that, using number
natural definitions rationality, settings involving resources, rational O-contract
paths length (2m ) needed, whereas without rationality restriction individual
deals, O-contracts suffice realise deal. also considered class
deals (k )-contracts examined (Sandholm, 1998), establishing
cases that, particular rationality conditions imposed, (k 1)-contract
2
paths length (22m/k ) needed realise deal achieved single
(k )-contract.
note analyses primarily focused worst-case lower bounds
path length appropriate paths exist, several questions
74

fiExtremal Behaviour Multiagent Contract Negotiation

practical interest merit discussion. may noted path structures
associated utility functions rather artificial, directed attaining path
specific length meeting given rationality criterion. seen, however, Theorems 4
5 outlined discussion concluding Section 3 issue exponential length
contract paths continues arise even require utility functions satisfy
monotonicity condition. identify two classes open question arise
results.
Firstly, focusing IR O-contract paths, would interest identify natural
restrictions utility functions would ensure that, deal hP , Qi implemented
IR O-contract path, realised one whose length polynomially
bounded m, e.g. additivity mentioned preceding section. interpret
Theorem 4, indicating monotonicity guarantee short IR contract paths.
note, however, restrictions suffice. use rather trivial
example, number distinct values u assume p
constant p IR O-contract path length exceeding p : successive deals
must strictly increase u take K different values IR contract
path length exceeding K . well practical interest, classes utility
function property considered would also interest regarding one
complexity issue. result proved (Dunne et al., 2003) establishing deciding
IR O-contract path exists np-hard, gives lower bound computational complexity
problem. present, (non-trivial) upper bound problems complexity
demonstrated. results Theorems 3 4 indicate decision
problem np (thus complexity would npcomplete rather nphard)
required polynomial length existence certificate may something
path itself.10 note proof nphardness (Dunne et al., 2003) constructs
instance u take O(m) distinct values: thus, example
restriction ensuring present IR O-contract paths short,
result (Dunne et al., 2003) indicates question deciding existence might
remain computationally hard.
Considering restrictions form utility functions one approach could
taken regarding finding tractable cases. alternative would gain insight
average path length likely be. attempting address question,
however, number challenging issues arise. immediate concerns,
course, notion modeling distribution utility function given definitions
rationality terms value agents attach resource holdings. principle
average-case analysis scenarios involving exactly two agents could carried
purely graph-theoretic terms, i.e. without complication considering utility functions
directly. unclear, however, whether graph-theoretic analysis obviating need
consideration literal utility functions, extended beyond settings involving
exactly two agents. One difficulty arising three agents utility
10. use may rather must needed convention representing utility functions
employed (Dunne et al., 2003).

75

fiDunne

functions allocative externalities, i.e. given allocation hX , , Z three agents,
u1 (X ) unchanged Z redistributed among A2 A3 .11
one final set issues may merit study raise following.
constructions, individual deals contract path must satisfy structural
condition (be O-contract involve k agents), rationality constraint.
Focusing O-contracts following extremes: (Sandholm, 1998),
O-contracts suffice realise rational deal; results above, (2m ) rational
O-contracts needed realise rational deals. number mechanisms
employ relax condition every single deal O-contract
rational. example, allow path contain number deals Ocontracts (but must still IR) insist deals O-contracts allow
irrational. Thus, latter case, go extent allowing irrational
O-contracts, rational deal realised efficiently. would interest
examine issues effect allowing constant number, t, irrational deals
questions whether situations irrational contracts yield
short contract path 1 force one exponential length. particular interest,
application viewpoint, following: define ((m), O)-path O-contract
path containing (m) O-contracts individually rational. know
(m) = 0 individually rational (0, O)-paths complete respect
individually rational deals; similarly (m) = (m, O)-paths complete
respect individually rational deals. question interest would establish
(m) = o(m) ((m), O)-paths complete respect
individually rational deals maximum length contract path bounded
polynomial function m.

Acknowledgements
author thanks reviewers earlier version article valuable comments suggestions contributed significantly content organisation.
work reported article carried support EPSRC Grant
GR/R60836/01.

References
Abbott, H. L., & Katchalski, M. (1991). construction snake box codes.
Utilitas Mathematica, 40, 97116.
Atkinson, A. (1970). measurement inequality. Jnl. Econ. Theory, 2, 244263.
Chateauneaf, A., Gajdos, T., & Wilthien, P.-H. (2002). principle strong diminishing
transfer. Jnl. Econ. Theory, 103, 311333.
11. preliminary investigation complexity-theoretic questions arising settings allocative
externalities presented (Dunne, 2004) referred contextdependent:
utility functions appear neglected computational algorithmic analysis resource
allocation problems, although idea well-known game-theoretic models economics
term allocative externality originates.

76

fiExtremal Behaviour Multiagent Contract Negotiation

Dignum, F., & Greaves, M. (2000). Issues Agent Communication, Vol. 1916 LNCS.
Springer-Verlag.
Dunne, P. (2003). Prevarication dispute protocols. Proc. Ninth International Conf.
A.I. Law (ICAIL03), pp. 1221, Edinburgh. ACM Press.
Dunne, P. (2004). Context dependence multiagent resource allocation. Proc. ECAI04,
pp. 100001, Valencia.
Dunne, P., & McBurney, P. (2003). Optimal utterances dialogue protocols. Proc. Second International Joint Conf. Autonomous Agents Multiagent Systems (AAMAS03), pp. 608615. ACM Press.
Dunne, P., Wooldridge, M., & Laurence, M. (2003). complexity contract negotiation.
Tech. rep. ULCS-03-002, Dept. Computer Science, Univ. Liverpool. (to appear
Artificial Intelligence).
Endriss, U., & Maudet, N. (2004a). communication complexity multilateral trading. Proc. Third International Joint Conf. Autonomous Agents Multiagent
Systems (AAMAS04), pp. 622629.
Endriss, U., & Maudet, N. (2004b). Welfare engineering multiagent systems. Omicini,
A., Petta, P., & Pitt, J. (Eds.), Proc. Fourth International Workshop Engineering
Societies Agents World (ESAW-2003), Vol. 3071 LNAI, pp. 93106. SpringerVerlag.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2003). optimal outcomes negotiations
resources. Proc. Second International Joint Conf. Autonomous Agents
Multiagent Systems (AAMAS03), pp. 177184. ACM Press.
Kautz, W. H. (1958). Unit distance error checking codes. IRE Trans. Electronic Computers, 7, 179180.
Kolm, S.-C. (1976). Unequal inequalities. Jnl. Econ. Theory, 13, 82111.
Kraus, S. (2001). Strategic negotiation multiagent environments. MIT Press.
McBurney, P., Parsons, S., & Wooldridge, M. (2002). Desiderata argumentation protocols. Proc. First International. Joint Conf. Autonomous Agents Multiagent
Systems (AAMAS02), pp. 402409. ACM Press.
Parkes, D. C., & Ungar, L. H. (2000a). Iterative combinatorial auctions: theory practice.
Proc. 17th National Conf. Artificial Intelligence (AAAI-00), pp. 7481.
Parkes, D. C., & Ungar, L. H. (2000b). Preventing strategic manipulation iterative
auctions: proxy agents price adjustment. Proc. 17th National Conf. Artificial
Intelligence (AAAI-00), pp. 8289.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules Encounter. MIT Press.
Sandholm, T. W. (1998). Contract types satisficing task allocation: theoretical results.
AAAI Spring Symposium: Satisficing Models.
Sandholm, T. W. (1999). Distributed rational decision making. Wei, G. (Ed.), Multiagent Systems, pp. 201258. MIT Press.

77

fiDunne

Sandholm, T. W. (2002). Algorithm optimal winner determination combinatorial
auctions. Artificial Intelligence, 135, 154.
Sandholm, T. W., & Suri, S. (2003). Bob: Improved winner determination combinatorial
auctions generalizations. Artificial Intelligence, 145, 3358.
Sandholm, T. W., Suri, S., Gilpin, A., & Levine, D. (2001). Cabob: fast optimal algorithm
combinatorial auctions.. Proc. IJCAI-01, pp. 11021108.
Smith, R. G. (1980). contract net protocol: high-level communication control
distributed problem solver. IEEE Trans. Computers, C-29 (12), 11041113.
Tennenholz, M. (2000). tractable combinatorial auctions. Proc. 17th National
Conf. Artificial Intelligence (AAAI-00).
Yokoo, M., Sakurai, Y., & Matsubara, S. (2004). effect false-name bids combinatorial auctions: new fraud internet auctions. Games Economic Behavior,
46 (1), 174188.

78

fi
