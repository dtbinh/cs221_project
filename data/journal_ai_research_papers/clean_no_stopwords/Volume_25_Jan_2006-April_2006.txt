Journal Artificial Intelligence Research 25 (2006) 503527Submitted 6/04; published 4/06Fault Tolerant Boolean SatisfiabilityAmitabha Royaroy@cs.bc.eduComputer Science Department,Boston College, Chestnut Hill, 02467.Abstract-model satisfying assignment Boolean formula small alteration,single bit flip, repaired flips small number bits, yieldingnew satisfying assignment. satisfying assignments represent robust solutionsoptimization problems (e.g., scheduling) possible recover unforeseenevents (e.g., resource becoming unavailable). concept -models introducedGinsberg, Parkes, Roy (1998), proved finding -models generalBoolean formulas NP-complete. paper, extend result studyingcomplexity finding -models classes Boolean formulas knownpolynomial time satisfiability solvers. particular, examine 2-SAT, Horn-SAT, AffineSAT, dual-Horn-SAT, 0-valid 1-valid SAT. see wide variation complexityfinding -models, e.g., 2-SAT Affine-SAT polynomial time tests -models,testing whether Horn-SAT formula one NP-complete.1. Introductionimportant problem artificial intelligence community concerns allocationresources near minimal cost. optimal solution problem mightrendered infeasible due unforeseen event (for example, resource becoming unavailable task exceeding allocated deadline). Hence, motivation searchoptimal solutions immune events. paper, considercomplexity finding robust solutions, allow fixed small numberbad events, added condition bad events rectified makingsmall change solution. solutions, call -models, introducedGinsberg et al. (1998), explored Bailleux Marquis (1999). approachfault tolerance extended constraint-satisfaction problems (CSPs) (Hebrard,Hnich, & Walsh, 2004b, 2004a) applications combinatorial auctions (Holland &OSullivan, 2004). Hoos ONeill (2000) consider approach robustnessframework dynamic satisfiability (which call DynSAT) goal ablerevise optimal solutions constantly changing input problem.extend initial complexity results Ginsberg et al. (1998) lookingtheoretical complexity tractable instances satisfiability (SAT) identified Schaefersdichotomy theorem (Schaefer, 1978). dichotomy theorem proves polynomialtime solvable instances SAT 2-SAT, Horn-SAT, dual-Horn-SAT, Affine-SAT, 0-validSAT 1-valid SAT form NP-complete. goal study complexity finding -models tractable problems identified dichotomy theorem.show wide variation complexity type (2-SAT vs Horn-SAT) parameter(the number repairs allowed break).c2006AI Access Foundation. rights reserved.fiRoyFormally, -model Boolean formula, called supermodels Ginsberg et al. (1998),satisfying assignment (satisfying assignments usually called models)bit assignment flipped (from 0 1 vice versa), one following conditionshold:(i) either new assignment model(ii) least one bit flipped obtain another model.Flipping bit -model called break, corresponding bad event. bitflipped get another satisfying assignment repair (we allow breaks mayneed repair). also study generalization concept: (r, s)-models satisfyingassignments breaks every set r bits need repairs (to avoidtrivialities, require repair bits different break bits).let -SAT refer decision question whether input Boolean formula-model. restrict form input Boolean formula, refercorresponding decision questions -2-SAT, -Horn-SAT etc. higher degree variantsproblems (r, s)-SAT etc. consider r fixed integers.following problems proved NP-complete:- (r, s)-SAT (Ginsberg et al., 1998), (1, s)-2-SAT > 1,- (1, s)-Horn-SAT, (1, s)-dual-Horn-SAT,- (r, s)-0-valid-SAT (r, s)-1-valid-SAT.contrast, prove following problems P:- (1, 1)-2-SAT, (r, s)-Affine-SAT.definition -models require new model obtained repairingbreak -model -model. define -models -modelsevery break needs one repair obtain another -model. models representgreatest degree fault tolerance achieved problem. refercorresponding decision problems -SAT, -2-SAT etc. prove -SATNEXP (non-deterministic exponential time) NP-hard, -2-SAT P-Affine-SAT P.Remark. Since goal paper study problems Schaefers tractableclass respect fault tolerance, yardstick measure complexity membershipP. Hence, concern finding exact running times within P.Optimizing runtimes may well prove important practical applications (at leastrare instances find polynomial time algorithms).Organization paper: Section 2, introduce define problem establishnotation. Section 3, study complexity finding -models general Booleanformulas. Section 4, consider complexity finding -models restricted classesformulas: consider 2-SAT (Section 4.1), Horn-SAT (Section 4.2), 0-valid-SAT, 1-validSAT (Section 4.3) Affine-SAT (Section 4.4). conclude section future work(Section 5).504fiFault Tolerant Boolean Satisfiability2. Definitions Notationssection, establish notation used rest paper formallydefine problems wish study.Boolean variable take two values true false write 1 0respectively. literal either variable v negation, denoted v (a variable oftencalled pure literal ). clause disjunction ( ) literals (for example, v1 v2 v3clause). Boolean formula function set Boolean variables V ={v1 , v2 , . . . , vn } {0, 1}. computational problems, assume Boolean formulasinput canonical fashion: usually conjunction ( ) clauses (in case, sayconjunctive normal form (CNF)).consider various forms CNF formulas. 2-SAT formula Boolean formulaCNF 2 literals per clause (more generally, k-CNF formula k-SAT formulaCNF formula k literals per clause). Horn-SAT formula Boolean formula CNFclause one positive literal (each clause called Horn clause).Equivalently, Horn clause written implication ((v1 v2 . . . vr ) u)u, v1 , v2 , . . . , vr pure literals r 0. dual-Horn-SAT formula CNF formulaclause one negative literal. Affine-SAT formula CNF formulaclause exclusive-or () literals negation exclusive-orliterals (such clause satisfied exactly odd number literals set1). Equivalently, clause Affine-SAT formula written linear equationfinite field {0, 1} 2 elements.assignment function X : V {0, 1} assigns truth value (true false)variable V . Given assignment truth values V , Boolean formuladefined V also inherits truth value (we denote (X)), applying rulesBoolean logic. model assignment X (X) true. often treatassignment X n-bit vector i-th bit, denoted X(i), 1 n,truth value variable vi . slight abuse notation, let X(l) denote valueliteral l assignment X.0-valid-SAT (resp. 1-valid-SAT) formula one satisfied assignmentevery variable set 0 (resp. 1).propositional satisfiability problem defined follows:Problem (SAT).Instance: Boolean formula .Question: model ?SAT canonical example NP -complete decision problem (for definitionscomplexity class NP completeness, see e.g., Garey & Johnson, 1979; Papadimitriou,1994). Many computational difficult problems artificial intelligence SAT encodings(for example, planning (Kautz & Selman, 1992)) finding heuristic algorithmssolving SAT important research area artificial intelligence. Polynomial timealgorithms known SAT input instance either Horn-SAT, dual-Horn-SAT,2-SAT, Affine-SAT, 0-valid-SAT 1-valid-SAT. Schaefer (1978) provedcases SAT solvable polynomial time, every case NP-complete505fiRoy(Schaefers theorem applies general situation called generalized satisfiabilitytruth value clause determined set constraints specifiedrelation).introduce concept fault-tolerant models. Given n-bit assignment X,operation flips i-th bit X (from 0 1, vice versa). operation producesnew assignment denote (X). Similarly, flip two distinct bits (say bitsj), write new assignment ij (X) generally, (X) represents Xbits flipped (where subset coordinates {1, 2, . . . , n}).Definition 2.1. -model Boolean formula model X i,1 n, either(i) assignment (X) model(ii) bit j, 1 j n 6= j, ij (X) model.words, -model model bit flipped (we call break ),one bit flip required produce new model. second bit flip calledrepair.Example 2.1. Let H(n, k) Boolean formula defined n variables v1 , v2 , . . . , vn ,whose models n-bit assignments exactly k bits set 1. example:!nnn_^^H(n, 1) =vivivji=1i=1j=1j6=ifirst clause specifies least one bit model 1 successive clausespecifies i-th bit 1, every bit set 0 1 n.model H(n, 1) -model: break 0-bit unique repair (the bit set 1)break 1-bit (n 1) possible repairs (any one 0-bits).following decision problem interpreted fault-tolerant analogue SAT:Problem (-SAT).Instance: Boolean formula .Question: -model ?problem -SAT variants (when restrict form input Booleanformula) focus paper.extend notion single repairability repairability sequence breaksmodel.Definition 2.2. (r, s)-model Boolean formula model everychoice r bit flips (the break set) model, disjoint setbits (the repair set) flipped obtain another model .506fiFault Tolerant Boolean SatisfiabilityRemark. (i) view r fixed constants unless otherwise mentioned. avoidredundancies, required repair set disjoint break set. Sincerequire bits repair, also allow case repairfewer repairs needed.(ii) definition, (1, 1)-models -models continue refer-models notational simplicity.(iii) Similar definition -SAT, define decision problem (r, s)-SATasks whether input Boolean formula (r, s)-model.Example 2.2. model H(n, k) (see, e.g., Example 2.1) also (k, k) modelk n/2.Assumptions: discussions, assume every variable input Booleanformula appears positive negative literals input Boolean formulaclausal form variable appearing clause (i.e.,clause form v1 v1 v2 ). also assume instance -SAT (orvariants), clause consists single literal, since case inputformula cannot -model.Consider -model X Boolean formula suppose model repairsbreak X. definition (Definition 2.1) -models require-model. enforce every break X repaired -model,X tolerant single break, repair. thus define degreefault tolerance. setting, models fault tolerant degree 0. Then, -modelsfault-tolerant degree 1. generally, degree-k fault-tolerant models (whichcall k -models) consist k1 fault-tolerant models every break repairedk1 model. give formal definition below.Definition 2.3. Let Boolean formula. define k (r, s)-models inductively: 0 (r, s)models models . k 1, k (r, s)-models k1 (r, s)-models Xevery break r coordinates X, disjoint setcoordinates X flipped get k1 (r, s)-model .define corresponding decision problem k (r, s)-SAT, asks whether inputBoolean formula k (r, s)-model. Observe also definition k (r, s)-model(r, s)-model i, 0 k 1.Example 2.3. Let n 6 even let Boolean formula:(v1 = v2 ) (v3 = v4 ) (vn1 = vn ) (4_H(n, k))k=0models vectors either 0, 2 4 variables set 1. variables{v2i1 , v2i } truth value (and forces breaks uniquerepairs).507fiRoyclaim X = (0, 0, . . . , 0) 2 (1, 1)-model . break (without lossgenerality, assume coordinate 1) repaired flip coordinate 2 (and vice versa).new vector (1, 1, 0, 0, . . . , 0) -model. break coordinate (say,bit 3) unique repair (bit 4) give model (1, 1, 1, 1, 0, . . . , 0) 4 1s. modellonger repairable, since model 4 1s, break coordinate0 (e.g., bit 5) repair.Let n 2 even. Consider formula(v1 = v2 ) (v3 = v4 ) (vn1 = vn )2n/2 models. Observe model -model. models k (1, 1)models every integer k 0. call models (1, 1)-models (as usual, r = 1= 1, denote (r, s)-models -models simplicity).Definition 2.4. Let Boolean formula defined n Boolean variables.model k (r, s) model k 0 called (r, s)-model.Observe set -models form set models satisfiesfollowing properties:(i) vector -model, i.e., break bit needs 1 repair.(ii) bit vector broken, repair (if repairneeded) new vector also member .call sets -models stable sets . stable sets studiedcombinatorial setting Luks Roy (2005).Remark. existence families models satisfy conditions (i) (ii)may used give alternate definition -models perhaps natural.However, notion degrees repairability -models appear limitdegrees, apparent definition, hence use formulation leadingDefinition 2.4.corresponding decision problem, named -SAT, asks whether input Booleanformula -model. Note yes answer question implies existenceone family models, particular, set above.Complexity Classes: refer Papadimitriou (1994) definitions basic complexityclasses like P NP. language L said NEXP non-deterministicTuring machine (NDTM) decides L exponential time (exponential lengthinput). language L said NP-hard polynomial time reductionSAT L. language NP-complete NP NP-hard. complexityclass NL (non-deterministic log space), contained P, consists languagesaccepted non-deterministic Turing machines using space logarithmic sizeinput. complexity classes Pk defined follows: P1 NP, Pk k 2set languages accepted NDTM access oracle TM Pk1 .508fiFault Tolerant Boolean Satisfiability3. Complexity Finding -modelssection, study computational complexity finding -models generalBoolean formulas.Theorem 3.1. (Ginsberg et al., 1998) decision problem (r, s)-SAT NP-complete.Remark. proof technique used Ginsberg et al. (1998) prove Theorem 3.1 usedprove NP-hardness results paper, e.g., Theorem 3.2 Theorem 4.19.Theorem 3.2. decision problem -SAT NEXP NP-hard.Proof. Since NDTM guess stable set models (which could exponential size)check satisfies required conditions stability exponential time, -SATNEXP.reduce SAT -SAT using reduction used proof Theorem 3.1Ginsberg et al. (1998): given instance SAT, Boolean formula n variablesv1 , v2 , . . . , vn , construct instance -SAT: formula 0 = vn+1 vn+1new variable (to put 0 CNF form, add variable vn+1 clauseCNF formula ).Suppose model X. show 0 -model showingstable set models . Extend X model 0 setting vn+1 = 0. Let Xi = (X)1 n. Extend assignment Xi model Yi 0 setting vn+1 = 1.let= {Y, Y1 , Y2 , . . . , Yn }.show stable set. Suppose bit j 6= i, 1 j n Yibroken, repair flipping i-th bit (in case, get repaired vectorYj ). i-th bit Yi broken, repair (n + 1)-th bit (and vice versa),case repaired vector . instead i-th bit broken, 1 n,repair (n + 1)-th bit (we obtain Yi repaired vector case).(n + 1)-th bit broken, repair flipping first n bits. Hencestable set models 0 -model (in fact, exhibited n + 1models).show 0 -model, model. 0 -model,must -model (n + 1)-th coordinate set 0. restrictionassignment v1 , v2 , . . . , vn model . completes reductionSAT.Remark. Note every -model k -model k 1, NP-hardness-SAT (Theorem 3.2) imply NP-hardness k -SAT (Theorem 3.3 below).reduction used Theorem 3.2 however adapted prove Theorem 3.3.Theorem 3.3. k -SAT NP-complete, k 0.Proof. k = 0, Cooks Theorem (Garey & Johnson, 1979), assume k 1.First observe k -SAT NP. NDTM guess assignment Xcheck k (1, 1)-model: check whether X k (1, 1)-model, suffices509fiRoyconsider possible nk break sets, check repair exists break appliedsequence break set. Since k fixed, done polynomial time.prove k -SAT NP-hard, use, again, proof technique used Ginsberg et al. (1998) prove Theorem 3.1. Given instance SAT, defined n variablesv1 , v2 , . . . , vn , construct 0 = vn+1 (and modify 0 CNF formula), vn+1newly introduced variable. argument used Theorem 3.2 used provesatisfiable iff 0 k -model. particular, construct stable set models0 single model . Since -model k -model, provessatisfiable, 0 -model. direction also follows: 0 k -modelmodel vn+1 set 0. restriction model v1 , . . . , vnmodel .4. Finding -models Restricted Boolean Formulassection, consider complexity (r, s)-SAT restricted classes SATformulas known polynomial-time algorithms satisfiability: 2-SAT,Horn-SAT, dual-Horn-SAT, 0-valid SAT, 1-valid SAT Affine-SAT. observeproblems different complexity testing fault tolerance. example, 2-SATAffine-SAT polynomial time tests existence -models (see Section 4.14.4) whereas problem NP-complete Horn-SAT (Section 4.2).4.1 Finding -models 2-SATprove finding -models 2-SAT formulas polynomial time. givetwo independent proofs: first proof (Section 4.1.1) exploits structure formulasecond proof (suggested referee) uses CSP (constraint satisfaction problem)techniques (Section 4.1.2). contrast, show finding (1, s)-models 2-SATformulas NP-complete 2 (Section 4.1.3). However, also show finding-models 2-SAT formulas polynomial time (Section 4.1.4).4.1.1 Polynomial time algorithm (1, 1)-2-SATNotation: Let instance 2-SAT. Following notation Papadimitriou (1994),define directed graph G() = (V, E) follows: vertices graphliterals clause li lj (where li , lj literals), two directed edges(li , lj ) ( lj , li ) E. path G() ordered sequence vertices (l1 , l2 , . . . , lr )(li , li+1 ) E 1 r 1. define simple path G() path(l1 , l2 , . . . , lr ) literals li involve distinct variables, i.e., li 6= lj li 6= lj6= j, 1 i, j r. simple cycle G() simple path allow startend vertices identical. source vertex (resp. sink vertex ) G() vertexin-degree (resp. out-degree) 0. vertex l G() said k-ancestor (resp.k-descendant) exists simple path (l, l1 , l2 , . . . , lk ) (resp. (l1 , l2 , . . . , lk , l)) lengthk G().following well-known lemma provides necessary sufficient condition2-SAT formula satisfiable.510fiFault Tolerant Boolean SatisfiabilityLemma 4.1. (Papadimitriou, 1994) 2-SAT formula unsatisfiable iff variable x appearing path x x path x xG().-model, G() restrictions.Lemma 4.2. 2-SAT formula -model, path l lvertex l G().Proof. path l l G(), satisfying assignment setl false. flip value literal l (by flipping associated variable),cannot repair get model .Remark. Lemma 4.2 establishes necessary condition satisfiable 2-SAT formula-model. Unlike Lemma 4.1, condition sufficient: consider, example,2-SAT formula (which also illustrates many constraints satisfied-model exists):(v1 v2 ) (v2 v3 ) (v3 v4 ) (v4 v5 ).-model formula set v1 false (otherwise every variable settrue break v5 requires one repair). Similarly v5 set 1, v20 v4 1. choice v3 allow single repair break v1 v5 .formula thus -model, yet satisfies necessary condition Lemma 4.2.establish necessary sufficient condition model 2-SAT formula-model.Lemma 4.3. Let satisfiable 2-SAT formula. Suppose path ll vertex l G(). Let X model . X -modelsatisfies following conditions:(C1) Let P = (l1 , l2 , l3 ) simple path G() length 2. X(l1 ) = 0 X(l3 ) = 1.(C2) (l1 , l2 ) (l1 , l3 ) edges G(), X(l1 ), X(l2 ), X(l3 ) cannot 0.Proof. () Suppose X -model . Let P = (l1 , l2 , l3 ) simple path length 2.X(l1 ) = 1, X(l2 ) = X(l3 ) = 1, otherwise X cannot model . break l3requires values l1 l2 flipped X cannot -model, contradiction.X(l1 ) = 0. Similar arguments show X(l3 ) = 1. Condition (C2) holds similarly:X(l1 ), X(l2 ), X(l3 ) false, break l1 would require two repairs (both l2l3 ). Hence one set true.() Let X model satisfies conditions (C1) (C2). showX actually -model. Suppose not; say break variable v repairableone bit flip. Assume without loss generality, X(v) = 0break, v set 1. must least one clause form v l l literal,X(l) = 0, otherwise break need response. 1clause, say clauses v l v l0 X(l) = X(l0 ) = 0, X violates condition (C2),contradicting hypothesis. exactly one clause form v l X(l) = 0511fiRoymoreover, must case flipping l produce model (then onerepair would sufficed). since flip variable associated l repairsclause v l, must clauses break l repaired. clause mustform l l0 literal l0 X(l0 ) = 0. know l0 cannot v sincewould path v v G(), violates hypothesis.assumption clause distinct literals implies l0 6= l. Hence (v, l, l0 )simple path X(v) = 0 X(l0 ) = 0, contradicting condition (C1). Hence X-model.Remark. (i) -model, indeed case (v, u) (w, u)edges G(), u, v w cannot set true -model (sincebreak u repairable single flip). need include conditionexplicitly Lemma 4.3, condition happens ( u, v)( u, w) satisfy condition (C2) Lemma 4.3.(ii) -model, condition (C1) extended specify values literals(vertices) path length 3 (the maximum possible length, see Corollary 4.4below) follows: (u1 , u2 , u3 , u4 ) simple path, apply condition (C1) twiceget X(u1 ) = X(u2 ) = 0 X(u3 ) = X(u4 ) = 1. Thus includecondition explicitly.Lemma 4.3 consequences G():Corollary 4.4. 2-SAT formula -model, G() satisfies followingproperties:(i) longest simple path G() length 3.(ii) longest simple cycle G() length 2.(iii) vertex v take part 1 simple cycle.Proof. Suppose simple path (l1 , l2 , l3 , l4 , l5 ) length 4 G(). X model , Lemma 4.3 implies X(l3 ) = 1 apply (C2) segment (l1 , l2 , l3 )X(l3 ) = 0 apply (C2) segment (l3 , l4 , l5 ). Hence -model cannotexist. conditions follow similar arguments.Pseudo-code algorithm given Algorithm (1). Observe Algorithm (1)polynomial time reduction -2-SAT satisfiability question new 2-SATformula B . Proof correctness follows.first need prove following easy lemma.Lemma 4.5. 2-SAT formula -model, -model sourcevertex (respectively, sink vertex) G() set false (resp. true).Proof. Modify -model X setting sink vertex 1 (and hence sourcevertex 0). Let new assignment X 0 . Clearly, X 0 still model (settingantecedent p, consequent q, 0, 1 respectively, satisfies every implication p q).512fiFault Tolerant Boolean SatisfiabilityAlgorithm 1 Algorithm -2-SAT1:2:Input: 2-SAT formulaOutput: True (1, 1)-model, false otherwisesatisfiablereturn false.5: end3:4:/* Check necessary condition holds (Lemma 4.2) */Construct G()8: path G() l l literal l9:return false.10: end6:7:11:B/* Enforce condition (C1) Lemma 4.3 */13: 2-ancestor vertex l G()14:B B (l)15: end12:16:17:18:19:20:21:22:/* Force source (resp. sink) vertex value 0 (resp. 1) */source vertices l G()B B (l)endsink vertices l G()B B (l)end/* Enforce condition (C2) Lemma 4.3 */24: 1-ancestors l G()25:pairs distinct vertices l1 , l226:(l, l1 ), (l, l2 ) edges G()27:B B (l1 l2 )28:end29:end30: end23:31:32:33:34:35:B satisfiablereturn trueelsereturn falseend513fiRoyshow model satisfies condition (C1) (C2) Lemma 4.3, thus proving-model. condition (C1) violated, simple path (l1 , l2 , l3 )G() X 0 (l1 ) = 1 X 0 (l3 ) = 0. X 0 (l1 ) = 1, X(l1 ) = 1 (suppose letX(l1 ) = 0: since (l1 , l2 ) edge G(), l1 sink vertex, value wouldchanged). Similarly, X 0 (l3 ) = 0 would imply X(l3 ) = 0. Thus X wouldviolate condition (C1) respect simple path (l1 , l2 , l3 ) could-model (a contradiction). Condition (C2) similarly holds.Algorithm (1) adds literals input 2-SAT formula enforce variable assignmentsmust hold -model (see Lines 1215, 2430 body Algorithm (1)).Since guaranteed Lemma 4.3 conditions necessary sufficientcondition existence -model, satisfiability resulting Boolean formulawould imply -model. simplify proof correctness (which simplyCorollary 4.6 below), enforce source label vertices get default values prescribedLemma 4.5.Corollary 4.6. formula B satisfiable iff -model.Proof. Immediate Lemma 4.3 Lemma 4.5.Example 4.1. Let 2-SAT formula:(v1 v2 ) (v2 v3 )(v1 v4 ) (v4 v3 )(v1 v5 ) (v5 v3 )Algorithm (1) constructs BB =(v1 ) (v3 )(added lines 1316 Algorithm (1))(v2 v4 ) (v2 v5 ) (v4 v5 )(added lines 2431 )(v2 v4 ) (v2 v5 ) (v4 v5 )(added lines 24-31 )Note construction G(), v3 2-ancestor. Since two variablesv2 , v4 , v5 set value, B unsatisfiable. Hence-model.Theorem 4.7. polynomial time, one determine 2-SAT formula -modelfind one exists.Proof. Satisfiability 2-SAT formula P (Papadimitriou, 1994). stepsprocedure consist looping simple paths length 3, done time O(n3 )n number variables.Remark. possible characterize space complexity (1, 1)-2-SAT.fact, (1, 1)-2-SAT complete NL (non-deterministic log space). see (1, 1)-2SAT NL, observe Algorithm (1) executed space logarithmic input.Completeness established via log-space reduction 2-SAT. Since resultrelevant present context, leave details out.514fiFault Tolerant Boolean Satisfiability4.1.2 alternative proof Theorem 4.7alternative proof Theorem 4.7 suggested one reviewers. possiblecast satisfiability problem constraint satisfaction problem (CSP) binaryvariables. transformation, particularly input instance 2-SAT problem,produces CSP local consistency (consistency subproblems involving fewervariables) ensures presence global solution. framework, assertingBoolean formula -model becomes particularly convenient.Notation: Let Boolean formula CNF. subset variables, let (S)denote subformula consisting clauses involve variables S.Definition 4.1. formula said k-consistent every subset k1 variables,every model (S) extended model (S {v}) every variable v (i.e.,larger subformula involving one variable). formula strong k-consistenti-consistent i, 1 k.Remark. concept k-consistency equivalent formulations (Jeavons, Cohen,& Cooper, 1998; Dechter, 1992). Since goal paper study satisfiabilityexclusively, rephrase definitions theorems apply present context.Theorem 4.8. (Dechter, 1992) Let 2-SAT formula. following hold:(a) strong 3-consistent, satisfiable 2 element set S, (S)satisfiable.(b) polynomial time (see e.g., (Jeavons et al., 1998)) one check whetherstrong 3-consistent. satisfiable strong 3-consistent, one addextra clauses (also 2-CNF) polynomial time resulting 2-SATformula strong 3-consistent.Remark. generally, given input Boolean formula , one establish k-consistencyadding extra constraints change set models. done iteratingpossible k-element subsets variables solving subproblem variables. Clauses added restrict values subset k 1 variablesvalues extended another variable. set k 1 variables nonewhose assignments extended, conclude unsatisfiable. not,extra clauses added make k-consistent. Enforcing strong k-consistency(for fixed k) accomplished polynomial time (Jeavons et al., 1998; Dechter, 1992).special case 2-SAT formula extra clauses also binaryb exactlyend strong 3-consistent 2-SAT formula (which denote )models (and hence, set -models).Notation: ordered pair variables (u, v), let (u, v) denote set models({u, v}).Theorem 4.8 (b) implies assume without loss generality inputstrong 3-consistent 2-SAT formula . Theorem 4.8 also implies assignment X515fiRoymodel iff (X(u), X(v)) (u, v) pairs (u, v). Clearly, constructsets (u, v) polynomial time (there (n2 ) variable tuples, nnumber variables, set (u, v) consists models 2-SAT formula2 variables). slight abuse notation, denote (u, v) set{(, )| (, ) (u, v)}.Let u variable . Let u,0 = (u) u,1 = (u). either u,0u,1 unsatisfiable, clear cannot -model. Assumeu,0 u,1 satisfiable letu,0 u,1 corresponding strong 3-consistentformulas. Let Nu set variable pairs (v, w) Md(v, w) Md(v, w) = .u,0u,1Lemma 4.9. Suppose Nu 6= variable u. -model,variable v, v 6= u, v belongs every pair Nu .Proof. flip value u -model , repair flipping onevariable forced flip one variable pair Nu . meansrepair variable every pair Nu .Lemma 4.9 implies may assume pairs Nu common member.similarly show:Lemma 4.10. Suppose v variable appears every pair Nu .following hold:(i) exists wMd(v, w) Md(v, w) =u,0u,1-model X set X(u) = 0.(ii) exists wMd(v, w) Md(v, w) = ,u,0u,1-model X set X(u) = 1.Thus either two conditions Lemma 4.10 force value variable u-model . Together Lemmas 4.9 4.10 enable us set values variablesforced (cf. Lemma 4.3). setting values variables, derivecontradiction cannot -model.Algorithm (2) provides detailed description algorithm.Theorem 4.11. Algorithm (2) decides (1, 1)-2-SAT polynomial time.Proof. Enforcing 3-consistency polynomial time (Dechter, 1992). outer loopLine 3 executes n times n number variables. Within body loop,calls made enforce satisfiability 3-consistency, along calls construct Nuvariable u consideration. step takes polynomial time, hence claimfollows.Remark. Algorithm (2) solves yes/no problem testing whether input 2SAT formula -model, simple matter modify algorithm outputs-model model exists. forced variable assignments along satisfyingassignment remaining 2-SAT formula -model input formula.516fiFault Tolerant Boolean SatisfiabilityAlgorithm 2 Algorithm (1, 1)-2-SATInput: strong 3-consistent 2-SAT formula2: Output: True (1, 1)-model, false otherwise1:3:4:5:6:7:8:every variable uu,0 u,1 unsatisfiableOutput false.endFind sets Md(v, w) Md(v, w) variables v, w.u,0u,1Compute N = set pairs (v, w)Md(v, w) Md(v, w) = .u,0u,19:10:11:12:13:14:15:16:17:18:19:20:21:22:pairs N common member, output false.N 6=common member v,variable w Md(v, w) Md(v, w) =u,0u,1set = (u)endvariable w Md(v, w) Md(v, w) =u,0u,1set = uendendCheck satisfiable, output false.satisfiable, add extra clauses make 3-consistent.endOutput true517fiRoy4.1.3 Complexity (1, s)-2-SAT 2Theorem 4.12. problem (1, s)-2-SAT NP-complete > 1.Proof. Clearly problem NP: NDTM guess assignment checkmodel every break, bits flippedget model (since fixed priori, leads O(ns ) possible repair sets,polynomial number choices).prove NP-completeness via reduction (s + 1)-SAT. Let= C1 C2 . . . Cminstance (s + 1)-SAT clause Ci disjunction + 1 literals:vi,1 vi,2 . . . vi,s+1 .construct instance 0 (1, s)-2-SAT follows: clause Ci ,construct appropriate 2-SAT formula Ci0 . resulting instance (1, s)-2-SATconjunction 2-SAT formulas. Thus,^T0 =Ci01imCi0 2-SAT formula defined clause Ci follows:^Ci0 =(zi vi,j )1j(s+1)^(vi,j i,j,1 )(4.1)1j(s+1)^^(i,j,k i,j,k+1 )1js+1 1k(s1)introduced 1+s(s+1) new variables: zi i,j,k 1 j s+1, 1 kdefine gadget Ci0 . gadget Ci0 best understood via Figure (1).Let model X. Extend model 0 setting zi = 0 1i,j,k = 1 1 m, 1 j + 1, 1 k s. claim(1, s)-model 0 . Suppose flip variable corresponding literal l.case analysis many repairs needed:[l = zi ] Since vi,1 vi,2 . . . vi,s+1 set true model X, need flipfalse literals {vi,1 , . . . , vi,s+1 }. Observe repairs necessary.[l = i,j,k ] Need flip i,k,k0 1 k 0 < k might need flip variablecorresponding vi,j vi,j set true X. repair affect truthvalue clauses 0 . Hence flip variables.[l = variable occurring ] flip value literals involving l.set every ai,j,k = 1 zi = 0, repairs needed 0 , implication(clause) 0 still remains true.518fiFault Tolerant Boolean Satisfiabilityzivi,1vi,jvi,s+1i,1,1i,j,1i,s+1,1i,1,ki,j,ki,s+1,ki,1,si,j,si,s+1,sFigure 1: Gadget 2-SATsuppose 0 (1, s)-model. show model. Notemodel zi = 0 (otherwise zi = 1, vi,j = i,j,k = 1 needrepairs flip value i,1,s ). literals {vi,1 , vi,2 , . . . , vi,s+1 } cannotset 0, since break zi would necessitate + 1 repairs. Hence least oneliterals {vi,1 , vi,2 , . . . , vi,s+1 } set 1. words, clause Ci satisfied.Since zi = 0 i, must model.4.1.4 Complexity -2-SATsection, show -2-SAT polynomial time.Let input 2-SAT formula n variables. construct graph G()described Section 4.1.1. Since -model definition also -model, mustpath restrictions set forth Lemma 4.3 Lemma 4.2. -model,G() restrictions.Lemma 4.13. Let 2-SAT formula -model. every non-trivial simplepath G() length 1.Proof. Suppose (l1 , l2 , l3 ) simple path G() length 2. Let X -model. Lemma 4.3, know X(l1 ) = 0, X(l3 ) = 1case -models. means break X(l1 ) cannot repaired get another-model. Hence, X cannot -model, contradiction.519fiRoyRemark. Note G() may cycles (l1 , l2 , l1 ), however situation, Lemma 4.13implies {l1 , l2 } must form one connected component. -model exists assignsvalue l1 l2 respective variables form break-repair pairindependent remaining variables. thus remove cycles consideration.without loss generality, assume G() cycles.Let R vertices G() in-degree 0 B vertices out-degree0. Since vertex cannot positive in-degree positive out-degree, createsbipartition R B vertices G(), R, B disjoint vertex sets edgesG() form (l, l0 ) l R l0 B.Note (l, l0 ) edge G(), out-degree l 0: otherwise,would path length 2 cycle, excluded. Hence l R iffl B. also observe isolated vertices G() since every clausedisjunction distinct literals. gives complete graph theoretic characterizationstructure G() -model.let Y0 assignment sets every literal R false (0) (hence sets) everyliteral B true (since assumed every variable appears positivenegative literals).Lemma 4.14. assignment Y0 -model.Proof. exhibit stable set C models contains Y0 . Let B (respectively,R ) denote restriction assignment onto literals B (respectively, R).LetC = {Y | B contains one literal set false }.Note B contains one false literal, R contains one trueliteral. Clearly Y0 C.show C stable set. Let C, 6= Y0 . Suppose setsliteral l R true l false B. value literal l flipped, getY0 (a model C) repairs needed. different variable flipped,creates new literal l0 R set true (and l0 false B) new assignment.repair flipping value l true false, thereby allowing one positiveliteral R. Thus break repairable another model C. break Y0 Cneed repairs. Hence C stable set Y0 -model.Theorem 4.15. -2-SAT P .Proof. graph G() constructed polynomial time (in time linear size). conditions needed existence -model checked polynomial time:using depth-first search, one check longest simple path G() length 1check whether subgraph G without 2-cycles bipartite.4.2 Finding -models Horn-SAT dual Horn-SATRecall instance Horn-SAT Boolean formula CNF clause contains1 positive literal. 2-SAT, polynomial time algorithm find model520fiFault Tolerant Boolean SatisfiabilityHorn formula (see, e.g., Papadimitriou (1994)). However, unlike situation 2SAT, finding (1, s)-models Horn formulas NP complete 1. prooffact easily modified show problem NP-complete dualHorn-SAT.first prove technical lemma used NP-completeness proof.Define Boolean formula = (x, y, 1 , . . . , 2s ) variables x, y, 1 , . . . , 2s follows:(x, y, 1 , . . . , 2s ) =s1^(i i+1 )i=1(s x) (s y)(4.2)(x s+1 ) (y s+1 )2s1^(i i+1 )i=s+1formula best visualized Figure (2). Observe variable xappears head tail chain implications length s.Figure 2: Gadgetx12s+1s+22scrucial property gadget use follows:Lemma 4.16. Let X model . X (1, s)-model iff satisfies x y.Proof. () Let X model . x holds X (i.e., x get opposite truthvalues X), X set 0 (because either x set 0)j j + 1 true (because either x set 1). breakrequires repairs j < j exactly one x (the variableset 0). Similarly break + 1 requires repairs j , + 1 j 1exactly one x (the variable set 1). break x need repairs.Since never need repairs every break, X (1, s)-model.() (1, s)-model X set j 0 1 j j1 + 1 j 2s (otherwise repairs needed breaksvariables). X(x) = X(y) = 0, break 1 (from 0 1) would requirerepairs 2 , 3 , . . . , well x y, total + 1 repairs. Hencex cannot false. Similarly, x cannot true break2s would require + 1 repairs. Hence X satisfies x y.Theorem 4.17. (1, s)-Horn-SAT NP-complete 1.521fiRoyProof.VWe prove via reduction 3-SAT. Let instance 3-SAT,=i=1 Ci defined n variables x1 , x2 , . . . , xn clause Ci disjunction 3distinct literals. Clearly assume every variable appears positivenegative literals (if not, may set pure literal true false appropriatelyconsider resulting formula ).first apply intermediate transformation . replace positive literal(say xj ) Ci negative literal, x0j , x0j new variable occurring .new clause, positive literal, denoted Ci0 . Rememberingglobal assumption every variable input Boolean formulas appear positivenegative literals, see transformation introduce variables x0j everyvariable xj . maintain logical equivalence, also need enforce x0j xjnew formula: add following clauses: ( x0j xj ) (x0j xj ). Notetwo clauses imply model new Boolean formula, xj x0j cannottruth value.Thus obtain^^ nT0 =Ci0( x0i xi ) (x0i xi )1im1inNote 0 almost Horn (since every clause Ci0 Horn), non-Horn clausesclauses form (xi x0i ). introduced n new variables 2n new clauses,0 + 2n clauses defined 2n variables. Clearly 0 satisfiable iffsatisfiable.construct instance 00 (1, s)-Horn-SAT 0 00 (1, s)model iff 0 satisfiable. first introduce + 1 new variables A1 , A2 , . . . , As+1 .clause Ci0 = vi,1 vi,2 vi,3 , construct formula i,1 consisting single clause(note step, vi,j variable form xk form x0kk, 1 k n):i,1 = ( zi wi,1 wi,2 wi,3 )(4.3)zi , wi,1 , wi,2 , wi,3 new variables introduced clause Ci0 . step introduces 4 new variables per clause Ci0 total 4m new variables. next step createsformulas places restrictions new variables ties variables vi,j original clause. introduce new variables i,j,k clause Ci0 ,1 j 3, 1 k 1, variables forming intermediate variables chainimplications length vi,j wi,j below:i,2 =(vi,1 i,1,1 ) (i,1,1 i,1,2 ) (i,1,s1 wi,1 )(vi,2 i,2,1 ) (i,2,1 i,2,2 ) (i,2,s1 wi,2 )(4.4)(vi,3 i,3,1 ) (i,3,1 i,3,2 ) (i,3,s1 wi,3 )reader may wish compare gadget i,2 similar gadget Ci0 Equation (4.1) shown Figure (1) used proof Theorem 4.12.522fiFault Tolerant Boolean Satisfiabilityalso make zi , one new variables introduced i,1 , appear headchain implications length + 1 shown formula i,3 :i,3 = (zi A1 ) (A1 A2 ) . . . (As As+1 )define formula Ci00 constructed clause Ci0 , 1 m, 0 :Ci00 = i,1 i,2 i,3Note Ci00 Horn introduced new variables i,j,k , wi,j , zi total3(s 1) + 3 + 1 = 3s + 1 new variables. new variables Ai global, i.e, reusedformulas Ci00 various i.clauses form (x0i xi ) (x0i xi ) 0 , 1 n,introduce new variables i,j 1 j 2s construct gadget =(xi , x0i , i,1 , i,2 , . . . , i,2s ) defined Equation (4.2).instance (1, s)-Horn-SAT then:00 =^Ci001im^1infirst show 0 satisfiable, 00 -model. Suppose 0 modelExtend assignment X 00 variables 00 setting valuesnewly introduced variables follows:X 0.Ai = 1 1 + 1,zi = 0 1 m,wi,j = 1 j, 1 1 j 3,i,j,k = 1 i, j 1 m, 1 j 3, 1 k 1,i,j = 0 j, 1 j i, 1 n,i,j = 1 j, + 1 j 2s i, 1 n.Since X 00 satisfies clause 00 , model 00 . show X 00actually (1, s)-model. Suppose variable v 00 flipped. casecase analysis possible repairs break.[v = xi x0i ] repairs needed since implication remains satisfied 0 .[v = Ai i, 1 + 1 ] repairs needed A1 , A2 , . . . , Ai1 (since zi = 0i, need flipped) 1 ( s) repairs.[v = i,j 1 n, 1 j ] repairs i,k j +1 k s. SinceX 0 model 0 , exactly one xi x0i set false need flipvariable. leads j + 1 repairs.[v = i,j 1 n, + 1 j 2s ] repairs needed i,k + 1k < j one xi x0i (since X 0 model 0 one xi , x0i set trueX 0 ) j repairs.523fiRoy[v = wi,j 1 m, 1 j 3 ] repairs needed i,j,k 1 k s1vi,j (if X 0 (vi,j ) = 1), repairs.[v = i,j,k 1 m, 1 j 3, 1 k 1 ] repairs needed i,j,k01 k 0 k 1 vi,j (if X 0 (vi,j ) = 1) k 1 repairs.[v = zi ] break alone whose repair crucially depends satisfiability 0 .Note break changes zi 0 1 makes clause ( zi wi,1wi,2 wi,3 ) false since wi,j true X 00 . repairs include onewi,j s, consequently might trigger flips i,j,k vi,j .choice wi,j involve repair process indicated vi,j set 0X 0 . Since X 0 model, note also least one vi,j set 0. Without lossgenerality, assume X 0 (vi,1 ) = 0 repair break zi flipping wi,1 , i,1,j1 j 1 exactly repairs.suppose 00 -model X 00 . show 0 satisfiable. Specifically, claimrestriction X 00 variables 0 model 0 . Lemma 4.16,know i,j = 0 1 n, 1 j i,j = 1 1 n, + 1 j 2salso x0i xi 0 satisfied i, 1 n. Note 00 , wi,jend chain implications:k,1 k,2 k,s vi,j i,j,1 i,j,s1 wi,j(4.5)vi,j either xk x0k k, 1 k n. Note variables chaindifferent gadgets k i,2 . implies X 00 (wi,j ) = 1 sinceotherwise X 00 would set variables chain 0 would violateLemma 4.16. Since X 00 model 00 , must X 00 (zi ) = 1 i, otherwisezi wi,1 wi,2 wi,3 false. zi flipped, guaranteed repairflips make clause zi wi,1 wi,2 wi,3 true. involveflipping least one wi,j , j = 1, 2, 3. vi,1 , vi,2 vi,3 set true X 00(which would turn implied X 00 (i,j,k ) = 1 1 j 3, 1 k 1)flip would require additional repairs, total s+1 repairs break zi .must vi,j false j, 1 j 3. words, Ci0 = vi,1 vi,2 vi,3satisfied X 00 . Hence restriction X 00 0 satisfies clauses 0 . Thus 0satisfiable.0 satisfiable iff 00 (1, s)-model. Since satisfiable iff 0 satisfiableSAT instance, accomplishes reduction SAT. reduction clearlypolynomial time reduction. Since (1, s)-Horn-SAT clearly NP fixed r s,proves NP-complete.Recall dual-Horn formula Boolean formula CNF clauseone negative literal. surprisingly, dual-Horn-SAT formulas behave similarlyHorn-SAT comes finding -models.Theorem 4.18. (1, s)-dual-Horn-SAT NP-complete.proof theorem similar Theorem 4.17: replace Equation (4.3) i,1 = (zi wi,1 wi,2 wi,3 ) change direction implications i,3Equation (4.4).524fiFault Tolerant Boolean Satisfiability4.3 Finding -models 0-valid, 1-valid SAT formulasRecall 0-valid (resp. 1-valid) Boolean formula one satisfied modelevery variable set 0 (resp. 1). consider complexity finding faulttolerant models input 0-valid (or 1-valid) formula refer correspondingdecision questions (r, s)-0-valid-SAT, (r, s)-1-valid-SAT, -0-valid-SAT etc.knowledge input Boolean formula satisfied particular assignmentprovide information presence fault-tolerant models. Hence wouldexpect (correctly) finding models NP-hard. first prove:Theorem 4.19. decision problem (r, s)-0-valid-SAT NP-complete.Proof. proof, refer proof Theorem 3.1 which, slight modification,works problem well. reduce SAT. Let SAT instance, constructinstance (r, s)-0-valid-SAT, 0 = new variable appearing. Observe 0 0-valid (its value matters). proof 0-model iff satisfiable identical proof Theorem 3.1: satisfiablemodel X, extend model X 0 0 setting value 1.break consisting r variables X 0 require repair r variables involve y.involve y, flipping value 1 0 makes 0 true, hence onerepair suffices. Hence X 0 (r, s)-model. 0 (r, s)-model, must modelset 1. restriction model variables makes true, hencesatisfiable.Similarly, easy verify proofs Theorem 3.1, Theorem 3.2 workinput formula 0-valid 1-valid formula. Hence following:Theorem 4.20. decision problem (r, s)-1-valid-SAT NP-complete. problem(1, 1)-0-valid-SAT (1, 1)-1-valid-SAT NEXP NP-hard.4.4 Finding -models Affine-SATAnother class Boolean formulas polynomial time satisfiability checkers AffineSAT: formulas conjunction clauses, clause exclusiveor (denoted ) distinct literals (a b = 1 iff exactly one Boolean variables a, bset 1).Example 4.2. example Affine-SAT formula(x1 x2 x3 x4 = 1) (x3 x4 = 0)formula -model X = (1, 0, 0, 0). fact, X easily seen -model (whichtrue -models Affine-SAT formulas, shall shortly see).One find satisfying assignment formula affine form variant Gaussianelimination. prove finding -models affine formulas also polynomialtime.Lemma 4.21. Affine-SAT formula -model iff satisfiable everyvariable v V appearing exists variable w = w(v) v w appearexactly clauses.525fiRoyProof. Let X -model . variable v flipped, clauses v appearsbecome false, repair need flip variable appears exactlyclauses (and others). Thus variable pairing must exist. reverse directioneasily proved: variable pairing exists, variables form break-repairpair.Since conditions Lemma 4.21 easy check polynomial time,following theorem:Theorem 4.22. (1, 1)-Affine-SAT P.can, fact, slightly strengthen theorem. first state analogue Lemma 4.21,variable pairings easily generalized.Definition: parity integer n n mod 2.Lemma 4.23. Affine-SAT formula (r, s)-model iff satisfiable everyset R r variables, exists set S, R = variables,clauses C , parity number variables R appearing Cparity number variables appearing C.prove:Theorem 4.24. (r, s)-Affine-SAT P .Proof. Since r fixed constants, conditions Lemma 4.23 checkedpolynomial time: choice set R R r, (there O(nr ) sets),cycle possible set |S| s, R = (there O(ns ) sets),check see conditions Lemma 4.23 satisfied (in particular, test whetherparity variables R appearing clause = parity variables appearingclause, also accomplished polynomial time).Hence (r, s)-Affine-SAT polynomial time.Theorem 4.22 implies -model actually -model, since pairings(u, w(v)) exist, model become -model (with {v, w(v)} forming break-repairpairs).Hence Affine-SAT formula -model iff -model, hence finding model Affine-SAT formulas also polynomial time.thus following theorem:Theorem 4.25. -Affine-SAT P.5. Future Workcomplexity (r, s)-SAT r part input opposed fixedconstants known. problem complexity class p3 , completeclass? status problem restricted Boolean formulas like 2-SAT, HornSAT etc., r specified input similarly open. present,also know (r, s)-SAT decided polynomial space r, fixed constants.Finally, practical modification concept -models would involve weakeningcondition allow high percentage breaks repairable.526fiFault Tolerant Boolean SatisfiabilityAcknowledgementsauthor grateful Eugene M. Luks encouragement advice. also thankanonymous referees detailed comments suggestions.ReferencesBailleux, O., & Marquis, P. (1999). Distance-sat: Complexity algorithms. ProceedingsSixteenth National Conference Artificial Intelligence (AAAI99), pp. 642647.Dechter, R. (1992). local global consistency. Artificial Intelligence, 55 (1), 87108.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-completeness. W. H. Freeman Company, New York.Ginsberg, M., Parkes, A., & Roy, A. (1998). Supermodels robustness. ProceedingsFifteenth National Conference American Association Artificial Intelligence tenth conference Innovative Applications Artificial Intelligence,1998, Madison, WI, pp. 334339.Hebrard, E., Hnich, B., & Walsh, T. (2004a). Robust solutions constraint satisfactionoptimization. Proceedings sixteenth European Conference ArtificialIntelligence, ECAI, pp. 186190, Valencia, Spain.Hebrard, E., Hnich, B., & Walsh, T. (2004b). Super solutions constraint programming.Proceedings Internation Conference Integration AI TechniquesConstraint Programming Combinatorial Optimization Problems (CPAIOR), pp.157172.Holland, A., & OSullivan, B. (2004). Super solutions combinatorial auctions. Proceedings CSCLP 2004: Joint Annual Workshop ERCIM/CoLogNet ConstraintSolving Constraint Logic Programming.Hoos, H., & ONeill, K. (2000). Stochastic local search methods dynamic sat initialinvestigation. AAAI-2000 Workshop Leveraging Probability UncertaintyComputation, pp. 2226.Jeavons, P. G., Cohen, D. A., & Cooper, M. C. (1998). Constraints, consistency closure.Artificial Intelligence, 101 (12), 251265.Kautz, H. A., & Selman, B. (1992). Planning satisfiability. Proceedings TenthEuropean Conference Artificial Intelligence (ECAI92), pp. 359363.Luks, E. M., & Roy, A. (2005). Combinatorics singly-repairable families. ElectronicJournal Combinatorics, 12 (1), Research Paper 59, 17 pp. (electronic).Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.Schaefer, T. J. (1978). complexity satisfiability problems. STOC 78: Proceedingstenth annual ACM symposium Theory computing, pp. 216226. ACMPress.527fiJournal Artificial Intelligence Research 25 (2006) 159-185Submitted 5/05; published 2/06Dynamic Local Search Maximum Clique ProblemWayne Pullanw.pullan@griffith.edu.auSchool Information Communication Technology,Griffith University,Gold Coast, QLD, AustraliaHolger H. Hooshoos@cs.ubc.caDepartment Computer ScienceUniversity British Columbia2366 Main Mall, Vancouver, BC, V6T 1Z4 CanadaAbstractpaper, introduce DLS-MC, new stochastic local search algorithm maximum clique problem. DLS-MC alternates phases iterative improvement,suitable vertices added current clique, plateau search,vertices current clique swapped vertices contained current clique.selection vertices solely based vertex penalties dynamically adjustedsearch, perturbation mechanism used overcome search stagnation.behaviour DLS-MC controlled single parameter, penalty delay, controls frequency vertex penalties reduced. show empirically DLSMC achieves substantial performance improvements state-of-the-art algorithmsmaximum clique problem large range commonly used DIMACS benchmarkinstances.1. Introductionmaximum clique problem (MAX-CLIQUE) calls finding maximum sized subgraph pairwise adjacent vertices given graph. MAX-CLIQUE prominent combinatorial optimisation problem many applications, example, information retrieval,experimental design, signal transmission computer vision (Balus & Yu, 1986).recently, applications bioinformatics become important (Pevzner & Sze, 2000; Ji,Xu, & Stormo, 2004). search variant MAX-CLIQUE stated follows: Givenundirected graph G = (V, E), V set vertices E set edges,find maximum size clique G, clique G subset vertices, C V ,pairs vertices C connected edge, i.e., v, v C, {v, v } E,size clique C number vertices C. MAX-CLIQUE N P-hardassociated decision problem N P-complete (Garey & Johnson, 1979); furthermore, inapproximable sense deterministic polynomial-time algorithmfind cliques size |V |1 > 0, unless N P = ZPP (Hastad, 1999).1best polynomial-time approximation algorithm MAX-CLIQUE achieves approximation ratio O(|V |/(log |V |)2 ) (Boppana & Halldorsson, 1992). Therefore, large hardinstances MAX-CLIQUE typically solved using heuristic approaches, particular,1. ZPP class problems solved expected polynomial time probabilistic algorithmzero error probability.c2006AI Access Foundation. rights reserved.fiPullan & Hoosgreedy construction algorithms stochastic local search (SLS) algorithms simulated annealing, genetic algorithms tabu search. (For overviewmethods solving MAX-CLIQUE, see Bomze, Budinich, Pardalos, & Pelillo, 1999.)may noted maximum clique problem equivalent independent set problem well minimum vertex cover problem, algorithm MAX-CLIQUEdirectly applied equally fundamental application relevant problems(Bomze et al., 1999).recent literature MAX-CLIQUE algorithms, seems that, somewhat unsurprisingly, single best algorithm. Although algorithms empiricallyevaluated benchmark instances Second DIMACS Challenge (Johnson & Trick,1996), quite difficult compare experimental results studies, mostlydifferences respective experimental protocols run-time environments. Nevertheless, particularly considering comparative results reported Grosso et al. (Grosso,Locatelli, & Croce, 2004), seems five heuristic MAX-CLIQUE algorithmsachieve state-of-the-art performance.Reactive Local Search (RLS) (Battiti & Protasi, 2001) derived ReactiveTabu Search (Battiti & Tecchiolli, 1994), advanced general tabu search methodautomatically adapts tabu tenure parameter (which controls amount diversification) search process; RLS also uses dynamic restart strategy provideadditional long-term diversification.QUALEX-MS (Busygin, 2002) deterministic iterated greedy construction algorithm uses vertex weights derived nonlinear programming formulation MAXCLIQUE.recent Deep Adaptive Greedy Search (DAGS) algorithm (Grosso et al., 2004)also uses iterated greedy construction procedure vertex weights; weightsDAGS, however, initialised uniformly updated every iteration greedyconstruction procedure. DAGS, weighted iterated greedy construction procedureexecuted iterative improvement phase permits limited amount plateausearch. Empirical performance results indicate DAGS superior QUALEX-MSMAX-CLIQUE instances DIMACS benchmark sets, hardinstances reach performance RLS (Grosso et al., 2004).k-opt algorithm (Katayama, Hamamoto, & Narihisa, 2004) based conceptually simple variable depth search procedure uses elementary search stepsvertex added removed current clique; evidenceperforms better RLS many instances DIMACS benchmark sets (Katayamaet al., 2004), performance relative DAGS unclear.Finally, Edge-AC+LS (Solnon & Fenet, 2004), recent ant colony optimisation algorithm MAX-CLIQUE uses elitist subsidiary local search procedure, appearsreach (or exceed) performance DAGS RLS least DIMACSinstances.work, introduce new SLS algorithm MAX-CLIQUE algorithm dubbedDynamic Local Search Max Clique, DLS-MC, based combination constructive search perturbative local search, makes use penalty values associatedvertices graph, dynamically determined searchhelp algorithm avoid search stagnation.160fiDynamic Local Search Max-Clique ProblemBased extensive computational experiments, show DLS-MC outperformsstate-of-the-art MAX-CLIQUE search algorithms, particular DAGS, broadrange widely studied benchmark instances, hence represents improvementheuristic MAX-CLIQUE solving algorithms. also present detailed results behaviour DLS-MC offer insights roles single parameter dynamicvertex penalties. note use vertex penalties DLS-MC inspireddynamic weights DAGS and, generally, current state-of-the-art Dynamic LocalSearch (DLS) algorithms well-known combinatorial problems, SATMAX-SAT (Hutter, Tompkins, & Hoos, 2002; Tompkins & Hoos, 2003; Thornton, Pham,Bain, & Ferreira, 2004; Pullan & Zhao, 2004); general introduction DLS, see alsowork (Hoos & Stutzle, 2004). results therefore provide evidenceeffectiveness broad applicability algorithmic approach.remainder article structured follows. first describe DLS-MCalgorithm key aspects efficient implementation. Next, present empirical performance results establish DLS-MC new state-of-the-art heuristic MAX-CLIQUEsolving. followed detailed investigation behaviour DLS-MCfactors determining performance. Finally, summarise main contributionswork, insights gained study outline directions future research.2. DLS-MC AlgorithmLike DAGS algorithm Grosso et al., new DLS-MC algorithm based fundamental idea augmenting combination iterative improvement plateau searchvertex penalties modified search. iterative improvement procedure used algorithms based greedy construction mechanism startstrivial clique consisting single vertex successively expands clique C addingvertices adjacent vertices C. expansion impossible,may still exist vertices connected one vertices C. includingvertex v C removing single vertex C connected v, new cliquenumber vertices obtained. type search called plateausearch. noted one plateau search steps, expansioncurrent clique may become possible; therefore, DLS-MC alternates phasesexpansion plateau search.purpose vertex penalties provide additional diversification searchprocess, otherwise could easily stagnate situations current cliquevertices common optimal solution given MAX-CLIQUE instance.Perhaps obvious approach avoiding kind search stagnation simplyrestart constructive search process different initial vertex. However, evenrandom (or systematic) variation choice initial vertex, still riskheuristic guidance built greedy construction mechanism causes bias towardslimited set suboptimal cliques. Therefore, DAGS DLS-MC utilise numericalweights associated vertices; weights modulate heuristic selection functionused greedy construction procedure way vertices repeatedly occurcliques obtained constructive search process discouraged usedfuture constructions. Following intuition, consistent general approach161fiPullan & Hoosdynamic local search (DLS), based idea, paper, refernumerical weights vertex penalties.Based general considerations, DLS-MC algorithm works follows (see alsoalgorithm outline Figure 1): picking initial vertex given graph Guniformly random setting current clique C set consisting singlevertex, vertex penalties initialised zero. Then, search alternatesiterative improvement phase, suitable vertices repeatedly addedcurrent clique C, plateau search phase, repeatedly one vertex Cswapped vertex currently contained C.two subsidiary search procedures implementing iterative improvementplateau search phases, expand plateauSearch, shown Figure 2. Note both,expand plateauSearch select vertex added current clique C usingpenalties associated candidate vertices. case expand, selectionmade set NI (C) vertices connected vertices Cedge G; call set improving neighbour set C. plateauSearch,hand, vertex added C selected level neighbour set C, NL (C),comprises vertices connected vertices C except one vertex,say v , subsequently removed C.Note procedures always maintain current clique C; expand terminatesimproving neighbour set C becomes empty, plateauSearch terminateseither NI (C) longer empty NL (C) becomes empty. Also, order reduceincidence unproductive plateau search phases, DLS-MC implements plateau searchtermination condition (Katayama et al., 2004) recording current clique (C )start plateau search phase terminating plateauSearch overlaprecorded clique C current clique C.end plateau search phase, vertex penalties updated incrementingpenalty values vertices current clique, C, one. Additionally, every pdpenalty value update cycles (where pd parameter called penalty delay), non-zerovertex penalties decremented one. latter mechanism prevents penalty valuesbecoming large allows DLS-MC forget penalty values time.updating penalties, current clique perturbed one two ways.penalty delay greater one, i.e., penalties decreased occasionally, currentclique reduced last vertex v added it. removed verticesincreased penalty values, unlikely added back current cliquesubsequent iterative improvement phase. equivalent restarting searchv. However, penalty delay one corresponds behaviour penaltieseffectively used (since increase vertex penalty immediately undone),keeping even single vertex current clique C carries high likelihood reconstructingC subsequent iterative improvement phase. Therefore, achieve diversificationsearch, penalty delay one, C perturbed adding vertex vchosen uniformly random given graph G removing vertices Cconnected v.stated above, penalty values used selection vertex givenneighbour set S. precisely, selectMinPenalty(S) selects vertex choosinguniformly random set vertices minimal penalty values. vertex162fiDynamic Local Search Max-Clique Problemprocedure DLS-MC(G, tcs, pd, maxSteps)input: graph G = (V, E); integers tcs (target clique size), pd (penalty delay), maxStepsoutput: clique G size least tcs failedbeginnumSteps := 0;C := {random(V )};initPenalties;numSteps < maxSteps(C, v) := expand(G, C);|C| = tcs return(C); endC := C;(C, v) := plateauSearch(G, C, C );NI (C) 6=(C, v) := expand(G, C);|C| = tcs return(C); end(C, v) := plateauSearch(G, C, C );endupdatePenalties(pd );pd > 1C := {v};elsev := random(V );C := C {v};remove vertices C connected v G;endendreturn(failed);endFigure 1: Outline DLS-MC algorithm; details, see text.selected S, becomes unavailable subsequent selections penaltiesupdated perturbation performed. prevents plateau searchphase repeatedly visiting clique. Also, safeguard prevent penaltyvalues becoming large, vertices penalty value greater 10 neverselected.order implement DLS-MC efficiently, sets maintained using two array datastructures. first these, vertex list array, contains vertices currentlyset; second one, vertex index array, indexed vertex number containsindex vertex vertex list array (or 1, vertex set).additions set performed adding end vertex list array updatingvertex index array. Deletions set performed overwriting vertex listentry vertex deleted last entry vertex list updatingvertex index array. Furthermore, vertices swapped currentclique plateau search phase, intersection current cliquerecorded clique simply maintained recording size current cliquestart plateau search decrementing one every time vertex swapped163fiPullan & Hoosprocedure expand(G, C)input: graph G = (V, E); vertex set C V (clique)output: vertex set C V (expanded clique); vertex v (most recently added vertex)beginNI (C) 6=v := selectMinPenalty(NI (C));C := C {v};numSteps := numSteps + 1;end while;return((C, v));endprocedure plateauSearch(G, C, C )input: graph G = (V, E); vertex sets C V (clique), C C (recorded clique)output: vertex set C V (modified clique); vertex v (most recently added vertex)beginNI (C) = NL (C) 6= C C 6=v := selectMinPenalty(NL (C));C := C {v};remove vertex C connected v G;numSteps := numSteps + 1;end while;return((C, v));endFigure 2: Subsidiary search procedures DLS-MC; details, see text.current clique. Finally, array elements accessed using pointers rathervia direct indexing array. 2Finally, may noted order keep time-complexity individualsearch steps minimal, selection improving level neighbour setsattempt maximise size set respective search step, rather choosesvertex minimal penalty uniformly random; keeping commonintuition that, context SLS algorithms, often preferable perform manyrelatively simple, efficiently computable search steps rather fewer complex searchsteps.3. Empirical Performance Resultsorder evaluate performance behaviour DLS-MC, performed extensive computational experiments MAX-CLIQUE instances Second DIMACSImplementation Challenge (19921993)3 , also used extensively benchmarking purposes recent literature MAX-CLIQUE algorithms. 80 DIMACSMAX-CLIQUE instances generated problems coding theory, fault diagnosisproblems, Kellers conjecture tilings using hypercubes Steiner triple problem,2. Several techniques based implementation details Henry Kautzs highly efficient WalkSAT code, see http://www.cs.washington.edu/homes/kautz/walksat.3. http://dimacs.rutgers.edu/Challenges/164fiDynamic Local Search Max-Clique Problemaddition randomly generated graphs graphs maximum cliquehidden incorporating low-degree vertices. problem instances range sizeless 50 vertices 1 000 edges greater 3 300 vertices 5 000 000 edges.experiments study performed dedicated 2.2 GHz Pentium IV machine 512KB L2 cache 512MB RAM, running Redhat Linux 3.2.2-5 usingg++ C++ compiler -O2 option. execute DIMACS Machine Benchmark4 ,machine required 0.72 CPU seconds r300.5, 4.47 CPU seconds r400.5 17.44CPU seconds r500.5. following, unless explicitly stated otherwise, CPU timesrefer reference machine.following sections, first present results series experimentsaimed providing detailed assessment performance DLS-MC. Then, reportadditional experimental results facilitate direct comparison DLS-MCstate-of-the-art MAX-CLIQUE algorithms.3.1 DLS-MC Performanceevaluate performance DLS-MC DIMACS benchmark instances, performed 100 independent runs instance, using target clique sizes (tcs) corresponding respective provably optimal clique sizes or, cases provablyoptimal solutions unknown, largest known clique sizes. order assess peakperformance DLS-MC, conducted experiment multiple valuespenalty delay parameter, pd, report best performance obtained. behaviourDLS-MC suboptimal pd values method used identify optimal pd valuediscussed Section 4.2. remaining parameter DLS-MC, maxSteps,set 100 000 000, order maximise probability reaching target clique sizeevery run.results experiments displayed Table 1. benchmarkinstance show DLS-MC performance results (averaged 100 independent runs)complete set 80 DIMACS benchmark instances. Note DLS-MC finds optimal(or best known) solutions success rate 100% 100 runs per instance 7780 instances; cases target clique size reached consistentlywithin alotted maximum number search steps (maxSteps) are:C2000.9, 93 100 runs successful giving maximum clique size (averageclique size, minimum clique size) 78 (77.93, 77);MANN a81, 96 100 runs obtained cliques size 1098, remainingruns produced cliques size 1097;MANN a45, runs achieved maximum clique size 344.three cases, reported CPU time statistics successful runsshown parentheses Table 1. Furthermore, expected time required DLS-MCreach target clique size less 1 CPU second 67 80 instances,4. dmclique, ftp://dimacs.rutgers.edu directory /pub/dsj/clique165fiPullan & HoosInstancebrock200 1brock200 2brock200 3brock200 4brock400 1brock400 2brock400 3brock400 4brock800 1brock800 2brock800 3brock800 4DSJC1000 5DSJC500 5hamming10-2hamming10-4hamming6-2hamming6-4hamming8-2hamming8-4johnson16-2-4johnson32-2-4johnson8-2-4johnson8-4-4MANN a27MANN a45MANN a81MANN a9san1000san200 0.7 1san200 0.7 2san200 0.9 1san200 0.9 2san200 0.9 3san400 0.5 1san400 0.7 1san400 0.7 2san400 0.7 3san400 0.9 1sanr200 0.7BR2112*1517*2729*3133*2324252615*13*5124032412816*816414126*345*1099161530187060441340302210018pd2222151515154545454522555555555533338522222222222CPU(s)0.01820.02420.03670.04682.22990.47740.17580.067356.497115.733521.91978.88070.7990.01380.00080.0089<<0.0003<<<<<0.0476(51.9602)(264.0094)<8.36360.00290.06840.00030.00020.00150.16410.10880.21110.42490.00290.002Steps14091118752180230508955520205440747582893610691276304477542649211731725916962913112919034332443171532141976(16956750)(27840958)21521086172733661415347156426235296355735811390518201342Sols.21111111111125422100283100921001006629100(100)(96)991121111111113Instancesanr200 0.9sanr400 0.5sanr400 0.7C1000.9C125.9C2000.5C2000.9C250.9C4000.5C500.9c-fat200-1c-fat200-2c-fat200-5c-fat500-1c-fat500-10c-fat500-2c-fat500-5gen200 p0.9 44gen200 p0.9 55gen400 p0.9 55gen400 p0.9 65gen400 p0.9 75keller4keller5keller6p hat1000-1p hat1000-2p hat1000-3p hat1500-1p hat1500-2p hat1500-3p hat300-1p hat300-2p hat300-3p hat500-1p hat500-2p hat500-3p hat700-1p hat700-2p hat700-3BR4213216834*167844*185712245814126266444*55*55657511*275910466812*65948*25*36*9365011*44*62pd2221111111111111111111111111111111111111CPU(s)0.01270.03930.0234.440.00010.9697(193.224)0.0009181.23390.12720.00020.0010.00020.00040.00150.00040.0020.0010.00030.02680.0010.0005<0.0201170.48290.00340.00240.00622.70640.00610.01030.00070.00020.00070.0010.00050.00230.01940.0010.0015Steps1573999188475141744015850052(29992770)84555055367282824291118452764930110773691845571640231406711984412230415157912687273018281338747611420010751767251525Sols.18461709493(91)85933141319318344111981001008287231909813421048143627285Table 1: DLS-MC performance results, averaged 100 independent runs, complete set DIMACS benchmark instances. maximum known clique sizeinstance shown BR column (marked asterisk provenoptimal); pd optimised DLS-MC penalty delay instance; CPU(s)run-time CPU seconds, averaged successful runs, instance. Average CPU times less 0.0001 seconds shown < ; Stepsnumber vertices added clique, averaged successful runs,instance; Sols. total number distinct maximum sized cliques foundinstance. runs achieved best known cliques size shownexception of: C2000.9, 93 100 runs successful giving maximumclique size (average clique size, minimum clique size) 78(77.93, 77); MANN a81,96 100 runs obtained 1098 giving 1098(1097.96, 1097); MANN a45,runs achieved maximum clique size 344.166fiDynamic Local Search Max-Clique Problemexpected run-time 10 CPU seconds required 8 13 remaininginstances, least 800 vertices. Finally, variation coefficients (stddev/mean) run-time distributions (measured search steps, order overcomeinaccuracies inherent extremely small CPU times) instances 100% success rate obtained found reach average maximum values 0.86 1.59,respectively.may interesting note time-complexity search steps DLS-MCgenerally low. indicative example, brock800 1 800 vertices, 207 505edges maximum clique size 23 vertices, DLS-MC performs, average, 189 235search steps (i.e., additions current clique) per CPU second. Generally, timecomplexity DLS-MC steps increases size improving (NI ) level (NL )neighbour sets well as, lesser degree, maximum clique size. relationshipseen Table 2 shows, (randomly generated) DIMACS C.9brock 1 instances, performance DLS-MC terms search steps per CPUsecond decreases number vertices (and hence size NI , NL ) increases.InstanceC125.9C250.9C500.9C1000.9C2000.9brock200 1brock400 1brock800 1Vertices12525050010002000200400800Edges69632798411233245007917995321483459723207505BR3444576878212723DLS-MC pd1111121545Steps / Second1587399939966572553319243155223774231428504189236Table 2: Average number DLS-MC search steps per CPU second (on reference machine) 100 runs DIMACS C.9 brock 1 instances. BRDLS-MC pd figures Table 1 also shown, factors directimpact performance DLS-MC. is, BR increases, greateroverhead maintaining sets within DLS-MC; furthermore, larger pd valuescause higher overhead maintaing penalties, vertices tendpenalised. C.9 instances randomly generated edge probability0.9, brock 1 instances constructed hide maximumclique considerably lower densities (i.e., average number edges pervertex). scaling average number search steps per CPU second performed DLS-MC C.9 instances only, running reference machine,approximated 9 107 n0.8266 , n number verticesgiven graph (this approximation achieves R2 value 0.9941).detailed analysis DLS-MCs performance terms implementation-independentmeasures run-time, search steps iteration counts, beyond scopework, could yield useful insights future.3.2 Comparative Resultsresults reported previous section demonstrate clearly DLS-MC achievesexcellent performance standard DIMACS benchmark instances. However, com167fiPullan & Hoosparative analysis results, compared results found literaturestate-of-the-art MAX-CLIQUE algorithms, straight-forward taskdifferences in:Computing Hardware: date, computing hardware basically documented terms CPU speed allows basic means comparison(i.e., scaling based computer CPU speed which, example, takes account features, memory caching, memory size, hardware architecture,etc.). Unfortunately, algorithms, realistic option availableus comparison.Result Reporting Methodology: empirical results performanceMAX-CLIQUE algorithms found literature form statisticsclique size obtained fixed run-time. conduct performance comparisonsdata, care must taken avoid inconclusive situations algorithmachieves larger clique sizes another algorithm B, cost higher runtimes. important realise relative performance B varysubstantially run-time; may reach higher clique sizes B relativelyshort run-times, opposite could case longer run-times. Finally, seeminglysmall differences clique size may fact represent major differences performance,since (as many hard optimisation problems) finding slightly sub-optimal cliquestypically substantially easier finding maximal cliques. example, C2000.9,average time needed find clique size 77 (with 100% success rate) 6.419CPU seconds, whereas reaching maximum clique size 78 (with 93% successrate) requires average (over successful runs only) 193.224 CPU seconds.Termination Criteria: MAX-CLIQUE algorithms (such DAGS)terminate upon reaching given target clique size, instead run givennumber search steps fixed amount CPU time, even optimal cliqueencountered early search. would obviously highly unfair directly compare published results algorithms DLS-MC, terminatessoon finds user supplied target clique size.Therefore, confirm DLS-MC represents significant improvement previousstate-of-the-art MAX-CLIQUE algorithms, conducted experiments analysesdesigned yield performance results DLS-MC directly comparedresults MAX-CLIQUE algorithms. particular, compared DLS-MCfollowing MAX-CLIQUE algorithms: DAGS (Grosso et al., 2004), GRASP (Resende,Feo, & Smith, 1998) (using results contained Grosso et al., 2004), k-opt (Katayamaet al., 2004), RLS (Battiti & Protasi, 2001), GENE (Marchiori, 2002), ITER (Marchiori,2002) QUALEX-MS (Busygin, 2002). rank performance MAX-CLIQUEalgorithms determine dominant algorithm benchmark instances,used set criteria based, primarily, quality solution then,deemed equivalent, CPU time requirements algorithms.criteria shown, order application, Table 3.168fiDynamic Local Search Max-Clique Problem1. algorithm algorithm find largest known maximum clique instanceranked dominant algorithm instance.2. one algorithm achieves 100% success rate instance algorithm lowestaverage (scaled) CPU time becomes dominant algorithm instance.3. single algorithm achieves 100% success rate instance algorithm becomes dominantalgorithm instance.4. algorithm achieves 100% success rate instance, algorithm achieves largestsize clique, highest average clique size lowest average CPU time becomesdominant algorithm instance.5. If, instance, algorithm meets four criteria listed above, conclusiondrawn dominant algorithm instance.Table 3: criteria used ranking MAX-CLIQUE algorithms.Instancebrock200 1brock200 2brock200 3brock200 4brock400 1brock400 2brock400 3brock400 4brock800 1brock800 2brock800 3brock800 4C1000.9C2000.9C4000.5C500.9gen200 p0.9 44gen400 p0.9 55gen400 p0.9 65gen400 p0.9 75keller6MANN a45p hat1000-3p hat1500-1san200 0.7 2san400 0.7 3sanr200 0.9DLS-MCClique sizeCPU(s)210.0182120.0242150.0367170.0468272.2299290.4774310.1758330.06732356.49712415.73352521.9197268.8807684.4478(77.93,77)193.22418 181.2339570.1272440.001550.0268650.001750.000559 170.482934451.9602680.0062122.7064180.0684220.4249420.0127DAGSClique sizeSCPU(s)210.256120.064150.06417(16.8,16)0.19227(25.35,24)1.79229(28.1,24)1.79231(30.7,25)1.792331.79223(20.95,20),10.62424(20.8,20)10.75225(22.2,21)10.8826(22.6,20)10.81668(65.95,65)94.84876(75.4,74)1167.3618(17.5,17)2066.5656(55.85,55)8.6444(41.15,40)0.57653(51.8,51)4.60865(55.4,51)4.67275(55.2,52)4.99257(56.4,56)7888.64344(343.95) 1229.63268(67.85,67)71.87212(11.75,11)19.90418(17.9,17)0.19222(21.7,19)1.2842(41.85,41)0.576GRASPClique sizeSCPU(s)214.992121.4081442.56173.3282514.9762515.23231(26.2,25)14.8482515.23221322132.9622(21.85,21)34.1122133.15267(66.1,65)154.36875(74.3,73)466.36818(17.75,17)466.9445680.89644(41.95,41)11.77653(52.25,52)35.26465(64.3,63)34.5674(72.3,69)36.1655(53.5,53) 1073.792336(334.5,334)301.88868237.5681123.42418(16.55,15)3.26421(18.8,17)9.8564212.608Table 4: Performance comparison DLS-MC, DAGS GRASP selected DIMACSinstances. SCPU columns contain scaled DAGS GRASP averagerun-times CPU seconds; DAGS GRASP results based 20 runs perinstance, DLS-MC results based 100 runs per instance. casesbest known result found runs, clique size entries formatmaximum clique size (average clique size, minimum clique size). DLS-MCdominant algorithm instances table.169fiPullan & HoosTable 4 contrasts performance results DAGS GRASP literature (Grossoet al., 2004) respective performance results DLS-MC. Since DAGSGRASP runs performed 1.4 GHz Pentium IV CPU, DLS-MC ran2.2 GHz Pentium IV reference machine, scaled CPU times factor 0.64.(Note based assumption linear scaling run-time CPU clockspeed; reality, speedup typically significantly smaller.) Using ranking criteria,data shows DLS-MC dominates DAGS GRASP benchmarkinstances listed Table 4. confirm ranking, modified DAGS terminatedsoon given target clique size reached (this termination condition usedDLS-MC) performed direct comparison DLS-MC 80 DIMACS instances,running algorithms reference machine. seen resultsexperiment, shown Table 5, DLS-MC dominates DAGS one instance (theexception san1000).Table 6 shows performance results DLS-MC compared results k-opt (Katayamaet al., 2004), GENE (Marchiori, 2002), ITER (Marchiori, 2002) RLS (Battiti & Protasi,2001) literature. roughly compensate differences CPU speed, scaledCPU times k-opt, GENE ITER factor 0.91 (these obtained2.0 GHz Pentium IV) RLS (measured 450 MHz Pentium II CPU)0.21. Using ranking criteria Table 3, RLS dominant algorithm instanceskeller6 MANN a45, k-opt dominant algorithm MANN a81 DLS-MCdominant algorithm, exception C2000.9, remainder DIMACSinstances listed Table 6. identify dominant algorithm C2000.9, experiment performed, running DLS-MC maxSteps parameter (which controlsmaximum allowable run-time) reduced point average clique sizeDLS-MC exceeded reported RLS. experiment, DLS-MC reachedoptimum clique size 78 58 100 independent runs average minimumclique size 77.58 77, respectively average run-time 85 CPU sec (takingaccount runs). establishes DLS-MC dominant RLS k-opt instanceC2000.9.Analagous experiments performed directly compare performance DLSMC k-opt selected DIMACS benchmark instances; results, shown Table 7,confirm DLS-MC dominates k-opt instances.Finally, Table 8 shows performance results DLS-MC comparison resultsQUALEX-MS literature (Busygin, 2002); CPU times QUALEX-MSscaled factor 0.64 compensate differences CPU speed (1.4 GHzPentium IV CPU vs 2.2 GHz Pentium IV reference machine). Using rankingcriteria Table 3, QUALEX-MS dominates DLS-MC instances brock400 1, brock800 1,brock800 2 brock800 3, DLS-MC dominates QUALEX-MS remaining 7680 DIMACS instances.170fiDynamic Local Search Max-Clique ProblemInstancebrock200 1brock200 2brock200 3brock200 4brock400 1brock400 2brock400 3brock400 4brock800 1brock800 2brock800 3brock800 4DSJC1000 5DSJC500 5C1000.9C125.9C2000.9C2000.5C250.9C4000.5C500.9c-fat200-1c-fat200-2c-fat200-5c-fat500-1c-fat500-10c-fat500-2c-fat500-5gen200 p0.9 44gen200 p0.9 55gen400 p0.9 55gen400 p0.9 65gen400 p0.9 75hamming10-2hamming10-4hamming6-2hamming6-4hamming8-2hamming8-4johnson16-2-4DLS-MCSuccess CPU(s)1000.01821000.02421000.03671000.04681002.22991000.47741000.17581000.067310056.497110015.733510021.91971008.88071000.7991000.01381004.441000.000193193.2241000.96971000.0009100 181.23391000.12721000.00021000.0011000.00021000.00041000.00151000.00041000.0021000.0011000.00031000.02681000.0011000.00051000.00081000.0089100<100<1000.0003100<100<DAGSSuccess CPU(s)930.1987980.12521000.1615820.2534353.1418752.3596922.2429991.653920.01022018.7471919.12764516.9227807.2381000.113952.871000.00245 2.87060810017.9247990.1725416.20641000.00021000.00041000.00121000.00051000.00671000.00091000.0028140.99781000.026709.0372277.1492148.60181000.11231003.88121000.0003100<1000.00391000.00061000.0003Instancejohnson32-2-4johnson8-2-4johnson8-4-4keller4keller5keller6MANN a27MANN a45MANN a81MANN a9p hat1000-1p hat1000-2p hat1000-3p hat1500-1p hat1500-2p hat1500-3p hat300-1p hat300-2p hat300-3p hat500-1p hat500-2p hat500-3p hat700-1p hat700-2p hat700-3san1000san200 0.7 1san200 0.7 2san200 0.9 1san200 0.9 2san200 0.9 3san400 0.5 1san400 0.7 1san400 0.7 2san400 0.7 3san400 0.9 1sanr200 0.7sanr200 0.9sanr400 0.5sanr400 0.7DLS-MCSuccess CPU(s)100<100<100<100<1000.0201100 170.48291000.047610051.960296 264.0094100<1000.00341000.00241000.00621002.70641000.00611000.01031000.00071000.00021000.00071000.0011000.00051000.00231000.01941000.0011000.00151008.36361000.00291000.06841000.00031000.00021000.00151000.16411000.10881000.21111000.42491000.00291000.0021000.01271000.03931000.023DAGSSuccess CPU(s)1000.0042100<1000.00011000.00091000.0791000.1886948.1941000.00031000.03531000.09848137.26915.6091000.40251006.32551000.00781000.00331000.06091000.00991000.02151000.42361000.12171000.04151000.10861000.9671000.0029920.10011000.00231000.03681000.05721000.03361000.00891000.0402900.53331000.03221000.0239830.3745930.2311000.1345Table 5: Success rates average CPU times DLS-MC DAGS (based 100 runsper instance). 80 DIMACS instances, DLS-MC superior success rate31 instances and, exception san1000, required less CPUtime DAGS instances. Entries signify runsterminated excessive CPU time requirements. obtain meaningfulcomparison DLS-MC DAGS, MANN a45 MANN a81, 3441098 respectively used best known results producing table.DLS-MC DAGS, average CPU time successful runs only.Using ranking criteria study, DAGS dominant algorithmsan1000 instance, DLS-MC dominant algorithm instances.171fiPullan & HoosDLS-MCInstanceClique sizebrock200 212brock200 417brock400 229brock400 433brock800 224brock800 426C1000.968C125.934C2000.516C2000.978(77.9,77)C250.944C4000.518C500.957DSJC1000 515DSJC500 513gen200 p0.9 4444gen200 p0.9 5555gen400 p0.9 5555gen400 p0.9 6565gen400 p0.9 7575hamming10-440hamming8-416keller411keller527keller659MANN a27126MANN a45344MANN a81 1098(1097.96,1097)p hat1500-112p hat1500-265p hat1500-394p hat300-18p hat300-225p hat300-336p hat700-111p hat700-244p hat700-362k-optRLSCPU(s)Clique sizeSCPU(s)Clique sizeSCPU(s)0.0242110.02184122.017050.0468160.01911174.093110.477425(24.6,24)0.28028 29(26.063,25)8.839110.0673250.18291 33(32.423,25) 22.8139815.733521(20.8,20)2.16034210.995198.880721(20.5,20)2.50796211.406164.44676.3063688.74860.0001340.00091340.000840.969716 13.01846162.09496193.22477(75.1,74) 66.14608 78(77.575,77) 172.905180.0009440.05642440.00609181.233917 65.2788518 458.448690.127257(56.1,56)0.82264570.656040.799155.77941151.355130.0138130.12103130.040740.001440.06643440.007770.0003550.00273550.003360.026853(52.3,51)0.56238550.252840.001650.24934650.01050.0005750.16926750.010710.0089400.58422400.01638<160.00182160.00063<110.00091110.000420.0201270.07371270.03591170.482957(55.5,55) 125.0321859 39.860940.04761260.032761260.6543651.9602344(343.6,343)5.34716 345(343.6,343)83.7417264.0094 1099(1098.1,1098)84.9031098 594.47222.706412 15.43997126.357540.0061650.42224650.033180.0103942.093940.040320.000780.0063780.003780.0002250.00546250.001260.0007360.0273360.004410.0194110.57876110.039060.001440.04914440.005880.0015620.08008620.00735GENEITERAvg.Avg.Clique size Clique size10.510.515.415.522.523.223.623.119.319.118.91961.661.633.83414.214.268.268.742.84315.415.652.252.713.313.512.212.139.739.550.848.849.749.153.751.260.262.737.738.8161611112626.351.852.7125.6126342.4343.11096.3109710.810.463.863.992.49388252534.635.19.89.943.543.660.461.8Table 6: Performance DLS-MC, k-opt, RLS, GENE ITER selected DIMACSinstances. SCPU columns contain scaled average run-time CPU secondsk-opt RLS; DLS-MC RLS results based 100 runs per instance,k-opt, GENE ITER results based 10 runs per instance. Usingranking criteria study, RLS dominant algorithm instancesMANN a45 keller6, DLS-MC dominant algorithminstances.172fiDynamic Local Search Max-Clique ProblemDLS-MCInstanceClique size CPU(s)brock400 2 25(24.69,24) 0.1527brock400 425 0.0616brock800 2 21(20.86,20) 1.7235brock800 4 21(20.65,20) 1.0058k-optDLS-MCk-optClique size SCPU(s) Instance Clique size CPU(s) Clique size SCPU(s)25(24.6,24)0.280 C1000.9 67(66.07,64) 0.037367(66,65)6.306250.183 C2000.9 77(75.33,74) 0.6317 77(75.1,74)66.14621(20.8,20)2.160 C4000.517 1.30051765.27921(20.5,20)2.508keller6 57(55.76,54) 2.6796 57(55.5,55) 125.032Table 7: Performance DLS-MC k-opt DLS-MC parameter maxStepsreduced point clique size results comparablek-opt. CPU(s) values DLS-MC include unsuccessful runs; DLS-MCresults based 100 runs k-opt results 10 runs (per instance).DLS-MCQUALEX-MSDLS-MCQUALEX-MSInstanceClique size CPU(s) Clique size SCPU(s)InstanceClique sizeCPU(s) Clique size SCPU(s)brock200 1210.0182210.64 johnson32-2-416<165.12brock200 2120.024212< 0.64 johnson8-2-44<4< 0.64brock200 3150.0367150.64 johnson8-4-414<14< 0.64brock200 4170.046817< 0.64keller411<110.64brock400 1272.2299271.28keller5270.02012610.24brock400 2290.4774291.92keller659 170.482953826.24brock400 3310.1758311.28MANN a271260.04761250.64brock400 4330.0673331.28MANN a45344 51.960234210.88brock800 123 56.49712311.52MANN a81 1098(1097.96,1097) 264.00941096305.28brock800 224 15.73352411.52MANN a916<16< 0.64brock800 325 21.91972511.52p hat1000-1100.00341017.92brock800 4268.88072611.52p hat1000-2460.00244521.76C1000.9684.446417.28p hat1000-3680.00626520.48C125.9340.000134< 0.64p hat1500-1122.70641260.8C2000.5160.969716177.92p hat1500-2650.00616471.04C2000.9 78(77.93,77) 193.22472137.6p hat1500-3940.01039169.12C250.9440.0009440.64p hat300-180.000780.64C4000.518 181.2339171500.8p hat300-2250.0002250.64C500.9570.1272552.56p hat300-3360.0007350.64c-fat200-1120.000212< 0.64p hat500-190.00191.92c-fat200-2240.00124< 0.64p hat500-2360.0005362.56c-fat200-5580.000258< 0.64p hat500-3500.0023482.56c-fat500-1140.0004140.64p hat700-1110.0194116.4c-fat500-101260.00151261.28p hat700-2440.001447.68c-fat500-2260.0004261.28p hat700-3620.0015627.04c-fat500-5640.002641.28san1000158.36361516.0DSJC1000 5150.7991423.04 san200 0.7 1300.0029300.64DSJC500 5130.0138133.2 san200 0.7 2180.068418< 0.64gen200 p0.9 44440.00142< 0.64 san200 0.9 1700.000370< 0.64gen200 p0.9 55550.0003550.64 san200 0.9 2600.0002600.64gen400 p0.9 55550.0268511.28 san200 0.9 3440.001540< 0.64gen400 p0.9 65650.001651.28 san400 0.5 1130.1641131.28gen400 p0.9 75750.0005751.28 san400 0.7 1400.1088401.92hamming10-25120.000851224.32 san400 0.7 2300.2111301.28hamming10-4400.00893628.8 san400 0.7 3220.4249181.28hamming6-232<32< 0.64 san400 0.9 11000.00291001.28hamming6-44<4< 0.64sanr200 0.7180.002180.64hamming8-21280.0003128< 0.64sanr200 0.9420.012741< 0.64hamming8-416<160.64sanr400 0.5130.0393131.28johnson16-2-48<8< 0.64sanr400 0.7210.023201.28Table 8: Performance DLS-MC QUALEX-MS. SCPU column containsscaled run-time QUALEX-MS CPU seconds; DLS-MC results based100 runs per instance. Using ranking criteria study, QUALEX-MSdominant algorithm instances brock400 1, brock800 1, brock800 2brock800 3, DLS-MC dominant algorithm instances.173fiPullan & HoosOverall, results comparative performance evaluations summarisedfollows:QUALEX-MS dominant brock400 1, brock800 1, brock800 2 brock800 3DIMACS instances.RLS dominant algorithm MANN a45 keller6 DIMACS instances.DAGS dominant algorithm san1000 DIMACS instance.k-opt dominant algorithm MANN a81 DIMACS instance.DLS-MC dominant algorithm remaining 72 DIMACS instances.addition, within alotted run-time number runs, DLS-MC obtained current best known results DIMACS instances exceptions MANN a45MANN a81.4. Discussiongain deeper understanding run-time behaviour DLS-MC efficacyunderlying mechanisms, performed additional empirical analyses. Specifically,studied variability run-time multiple independent runs DLS-MCproblem instance; role vertex penalties general and, particular,impact penalty delay parameter performance behaviour DLS-MC;frequency pertubation well role perturbation mechanism.investigations performed using two DIMACS instances, C1000.9 brock800 1.instances selected because, firstly, reasonable size difficulty. Secondly, C1000.9 randomly generated instance vertices optimal maximumclique predominantly higher vertex degree average vertex degree (intuitivelywould seem reasonable that, randomly generated problem, vertices optimalmaximum clique would tend higher vertex degrees). brock800 1,hand, vertices optimal maximum clique predominantly lower-than-averagevertex degree. (Note DIMACS brock instances created attempt defeatgreedy algorithms used vertex degree selecting vertices Brockington & Culberson,1996).fundamental difference highlighted results quantitative analysis maximum cliques instances, showed that, C1000.9, averagedmaximal cliques found DLS-MC, average vertex degree vertices maximal cliques 906 (standard deviation 9) compared 900 (9) averagedvertices; brock800 1, corresponding figures 515 (11) 519 (13) respectively.4.1 Variability Run-Timevariability run-time multiple independent runs given problem important aspect behaviour SLS algorithms DLS-MC. Following methology Hoos Stutzle (2004), studied aspect based run-time distributions(RTDs) DLS-MC two reference instances.174fiDynamic Local Search Max-Clique Problemseen empirical RTD graphs shown Figure 3 (each based100 independent runs reached respective best known clique size), DLS-MCshows large variability run-time. Closer investigation shows RTDs quitewell approximated exponential distributions (a Kolmogorov-Smirnov goodness-of-fit testfailed reject null hypothesis sampled run-times stem exponentialdistributions shown figure standard confidence level = 0.05 p-values0.16 0.62). observation consistent similar results highperformance SLS algorithms, e.g., SAT (Hoos & Stutzle, 2000) scheduling problems(Watson, Whitley, & Howe, 2005). consequence, performing multiple independentruns DLS-MC parallel result close-to-optimal parallelisation speedup (Hoos& Stutzle, 2004). Similar observation made difficult DIMACSinstances.11empirical RLD DLS-MCed[2.5*105]0.90.80.80.70.70.60.6P(solve)P(solve)0.90.50.40.50.40.30.30.20.20.10.101000100001000001e+006empirical RTD DLS-MCed[0.85]00.0011e+0070.01run-time [search steps]11empirical RLD DLS-MCed[0.7*107]0.90.80.80.70.70.60.6P(solve)P(solve)0.90.50.40.30.20.10.11e+0061e+007run-time [search steps]101e+0081e+009empirical RTD DLS-MCed[35]0.40.210000010.50.30100000.1run-time [CPU sec]00.010.1110run-time [CPU sec]1001000Figure 3: Run-time distributions DLS-MC applied C1000.9 (top) brock800 1(bottom), measured search steps (left) CPU seconds (right) reference machine (based 100 independent runs reached bestknown clique size); empirical RTDs well approximated exponentialdistributions, labelled ed[m](x) = 1 2x/m plots.4.2 Penalty Delay Parameter Vertex Penaltiespenalty delay parameter pd specifies number penalty increase iterations mustoccur DLS-MC penalty decrease (by 1) vertices currently175fiPullan & HoosVertex frequencypenalty. MAX-CLIQUE problem, pd basically provides mechanism focusinglower degree vertices constructing current cliques. pd = 1 (i.e., penalties),frequency vertices improving neighbour / level neighbour setsbasically solely dependent degree. Increasing pd overcomes bias towardshigher degree vertices, allows penalty values increase (as oftencurrent clique), inhibits selection current clique. turnallows lower degree vertices become part current clique. effect penaltydelay parameter illustrated Figure 4, shows correlation degreevertices frequency included current clique immediately priorperturbation performed within DLS-MC.C1000.9 pd = 10.40.2Vertex frequency0860.487888990Vertex degree919293brock800_1 pd = 10.30.20.1058606264Vertex degree6668706264Vertex degree666870Vertex frequency0.25brock800_1 pd = 450.20.150.10.055860Figure 4: Correlation vertex degree frequency verticespresent clique immediately prior DLS-MC perturbation.C1000.9 brock800 1, pd = 1, higher degree vertices tendhigher frequency present clique immediately prior DLS-MCperturbation. brock800 1, pd = 45, frequency presentclique immediately prior DLS-MC perturbation almost independentvertex degree.Currently, pd needs tuned family (or, case brock instances,sub-family) instances. general, could done principled way based RTDgraphs, DLS-MC, reasonably robust regard exact valueparameter (as shown Figures 5 6), actual tuning process simple, almostinteractive process normally require evaluating RTD graphs. Still, fine-tuningbased RTD data could possibly result further, minor performance improvements.176fiDynamic Local Search Max-Clique Problem100% Success rate9590858075700102030Penalty delay4050600102030Penalty delay405060Median processor time300250200150100500Figure 5: Success rate median CPU time DLS-MC function penalty delayparameter, pd, benchmark instance brock800 1. data point based100 independent runs.Cumulative success rate10080pd = 35pd = 45pd = 5060402004105610710Steps81010Cumulative success rate10080pd = 35pd = 45pd = 506040200110010110Processor time (seconds)210Figure 6: Run-time distributions DLS-MC brock800 1 penalty delays 35, 4550, measuring run-time search steps (top) CPU seconds (bottom).performance penalty delay 45 clearly dominates 35 50.177fiPullan & Hooseffect penalty delay parameter vertex penalties clearly illustratedFigure 7, shows cumulative distributions number penalised verticesperturbation DLS-MC, representative runs DLS-MC DIMACS brock800 1instance, varying values parameter pd. Note brock800 1, optimalpd value 45 corresponds point where, average, 90% verticespenalised. role pd parameter illustrated Figure 8, shows(sorted) frequency vertices present current clique immediatelyprior perturbation C1000.9 brock800 1. Note instances,using higher penalty delay settings significanly reduces bias towards including certainvertices current clique. previously demonstrated, without vertex penalties (i.e.,pd = 1), DLS-MC prefers include high-degree vertices current clique,case problem instances like C1000.9, optimal cliques tend consist verticeshigher-than-average degrees, effective strategy. instances brock800 1,however, optimal clique contains many vertices lower-than-average degree,heuristic bias towards high-degree vertices misleading needs counteracted, e.g.,means vertex penalties.100pd = 5pd = 10pd = 15pd = 20pd = 25pd = 30pd = 35pd = 40pd = 45pd = 50pd = 559080Cumulative frequency7060504030201000100200300400500Penalised vertices600700800Figure 7: Cumulative distributions number penalised vertices measuredsearch perturbation representative independent runs DLS-MC DIMACS brock800 1 instance penalty delay parameter pd varied (the leftcurve corresponds pd = 5). Note approx. optimal penaltydelay pd = 45 (solid line), average 90% vertices penalised (i.e.,penalty value greater zero).Generally, reducing bias cliques visited, vertex penalties help diversifysearch DLS-MC. time, penalties appear provide learningmechanism DLS-MC identifies vertices included178fiDynamic Local Search Max-Clique ProblemC1000.9% frequency vertex clique0.5pd = 1pd = 100.40.30.20.100100200300400500Vertex6007008009001000brock800_1% frequency vertex clique0.4pd = 1pd = 450.30.20.100100200300400Vertex500600700800Figure 8: Sorted frequency vertices present current clique immediately prior DLS-MC perturbation C1000.9 (top) brock800 1(bottom), based representative run problem instance. Noteusing penalty delay values pd > 1, bias towards using certain verticesfrequently others substantially reduced.current clique. agreement recent results SAPS, high-performancedynamic local search algorithm SAT (Hoos & Stutzle, 2004).4.3 Perturbation Mechanism Search Mobilityprevent search stagnation, DLS-MC uses perturbation mechanism executedwhenever plateau search procedure failed lead cliqueexpanded. Since mechanism causes major changes current clique, relativelyhigh time complexity. therefore interesting investigate frequently rathercostly disruptive perturbation steps performed. Figure 9 shows distributionnumber improving search steps (i.e., clique expansions) plateau steps (i.e., vertexswaps) successive perturbation phases representative run DLS-MCC1000.9 instance. Analogous results brock800 1 shown Figure 10. figuresbasically show result interactions improving plateau search steps,perturbation mechanism problem structure.179fiPullan & Hoosc1000.9Cumulative frequency100pd = 1pd = 2pd = 108060402000102030405060Improving steps7080901002030405060Plateau swaps708090100Cumulative frequency100pd = 1pd = 2pd = 10806040200010Figure 9: Number improving search steps plateau swaps successive perturbation phases DLS-MC C1000.9. graphs show cumulative distributions measures collected representative independent runspd value; solid lines correspond approx. optimal penalty delayinstance, pd = 1.brock800_1Cumulative frequency100pd = 1pd = 2pd = 4580604020005101520Improving steps25303540101520Plateau swaps25303540Cumulative frequency100pd = 1pd = 2pd = 4580604020005Figure 10: Number improving search steps plateau swaps successive perturbation phases DLS-MC brock800 1. graphs show cumulativedistributions measures collected representative independent runspd value; solid lines correspond approx. optimal penalty delayinstance, pd = 45.180fiDynamic Local Search Max-Clique Problemseen data, compared higher penalty delay values, pd = 1results significantly shorter plateau phases somewhat longer improvement phases.time, differences behaviour DLS-MC observed various penaltydelay values greater one relatively small. One explanation phenomenon liesfact pd = 1, effectively vertex penalties used, consequently,selection improving level neighbours sets search step less constrained.Intuitively, make easier find exits plateaus underlying searchlandscape follow gradients larger number search steps.Whether renders search efficient clearly depends topologygiven search landscape. Instance C1000.9 least 70 optimal solutions (see Table 1),construction, optimal cliques higher-than-average vertex degree. suggestsrespective search landscape relatively high fitness-distance correlation,would explain problem instance relatively easy solve also usingless radical perturbation mechanism associated pd = 1 (which adds randomly chosenvertex v current clique removes vertices connected v) provides sufficientdiversification search process. Instance brock800 1, hand, appearssingle optimal solution many near-optimal solutions (i.e., large nonoptimal cliques cannot extended), since construction, optimal cliquelower-than-average vertex degree. suggests respective search landscaperelatively low fitness-distance correlation, therefore, radical perturbationmechanism used pd > 1 (which restarts clique construction recentlyadded vertex uses vertex penalties diversification) required order obtaingood performance; hypothesis also agreement relatively high costsolving problem instance.investigate efficacy perturbation DLS-MC diversification mechanism, measured relative mobility search, defined Hamming distancecurrent cliques (i.e., number different vertices) consecutive perturbationsdivided two times maximum clique size, representative runs DLS-MC instances C1000.9 brock800 1 (this mobility measure closely related usedprevious studies (Schuurmans & Southey, 2000)). seen Figure 11,large difference mobility two variants perturbation mechanismpd = 1 pd > 1; former restarts search randomly chosen vertexconsequently leads large variability Hamming distance previous clique,latter restarts recently added vertex, using vertex penaltiesincrease search diversification, hence shows consistently much higher mobility. Notevertex penalties used (i.e., pd > 1), pd value significant effectsearch mobility. time, previously observed (see Figure 5), performanceDLS-MC significantly depend penalty update delay pd. demonstratesorder achieve peak performance, increased mobility afforded usevertex penalties needs combined correct amount additional diversificationachieved using specific penalty update delay.181fiPullan & HoosC1000.9Cumulative frequency100Delay 1Delay 2Delay 1080604020000.050.10.150.20.250.3Relative mobility0.350.40.450.50.350.40.450.5brock800_1Cumulative frequency100Delay 1Delay 2Delay 4580604020000.050.10.150.20.250.3Relative mobilityFigure 11: Mobility search consecutive perturbation phases DLS-MC instances C1000.9 (top) brock800 1 (bottom). Mobility measured termsrelative Hamming distance, i.e., number different vertices respective cliques divided two times maximum clique size. graphsshow cumulative distributions relative mobility measurements collectedrepresentative independent runs pd value problem instance;solid lines correspond respective approx. optimal pd values.5. Conclusions Future Workdemonstrated applying general paradigm dynamic local searchmaximum clique problem, state art MAX-CLIQUE solving improved.new algorithm, DLS-MC, similarity previous MAX-CLIQUE algorithms,particular recently introduced DAGS algorithm: algorithms use vertex penaltiesguide heuristic selection vertices searching maximum cliques. However,unlike DAGS, initial phase unweighted greedy construction search, DLS-MCuses updates vertex penalties throughout entire search process. Furthermore,weight updates DAGS monotone while, DLS-MC, vertex penalties subjectincreases well occasional decreases, effectively allows algorithmforget vertex penalties time. Furthermore, DLS-MC selects vertex addedcurrent clique step solely based penalty, vertex selectionDAGS based total weight neighbouring vertices hence implicitely usesvertex degree heuristic guidance. fact DLS-MC, although conceptually slightlysimpler, outperforms DAGS one standard DIMACS benchmark instancescombination excellent performance compared high-performance MAX182fiDynamic Local Search Max-Clique ProblemCLIQUE algorithms clearly demonstrates value underlying paradigm dynamiclocal search non-monotone penalty dynamics.work presented article extended several directions. particular,would interesting investigate extent use multiplicative penalty updatemechanisms DLS-MC instead current additive mechanism lead performance improvements. also believe current implementation DLS-MCoptimised. example, selection vertex added currentclique, implementation DLS-MC performs complete scan either improvingplateaus sets build list vertices lowest penalties; would probablyefficient maintain list means incremental update scheme. Anotherinteresting direction future research develop mechanisms automaticallyadjusting DLS-MCs penalty delay parameter search, similar scheme useddynamically adapting tabu tenure parameter RLS (Battiti & Protasi, 2001)Reactive Tabu Search (Battiti & Tecchiolli, 1994), mechanism used controllingnoise parameter Adaptive Novelty+ (Hoos, 2002). Finally, given excellent performance DLS-MC standard MAX-CLIQUE instances reported suggestsunderlying dynamic local search method substantial potential provide basishigh-performance algorithms combinatorial optimisation problems, particularlyweighted versions MAX-CLIQUE conceptually related clustering problems.Acknowledgmentsauthors would like thank Liang Zhao participation performinginitial experiments paper.ReferencesBalus, E., & Yu, C. (1986). Finding maximum clique arbitary graph. SIAM JournalComputing, 15 (4), 10541068.Battiti, R., & Protasi, M. (2001). Reactive local search maximum clique problem.Algorithmica, 29, 610637.Battiti, R., & Tecchiolli, G. (1994). reactive tabu search. ORSA Journal Computing,6 (2), 126140.Bomze, I., Budinich, M., Pardalos, P., & Pelillo, M. (1999). maximum clique problem.D.Z. Du, P. P. (Ed.), Handbook Combinatorial Optimization, Vol. A, pp. 174.Boppana, R., & Halldorsson, M. (1992). Approximating maximum independent setsexcluding subgraphs. Bit, 32, 180196.Brockington, M., & Culberson, J. (1996). Camouflaging independent sets quasi-randomgraphs. D.S. Johnson, M. T. (Ed.), Cliques, Coloring Satisfiability: SecondDIMACS Implementation Challenge, Vol. 26 DIMACS Series. American Mathematical Society.183fiPullan & HoosBusygin, S. (2002). new trust region technique maximum clique problem. Internalreport, http://www.busygin.dp.ua.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryN P-Completeness. Freeman, San Francisco, CA, USA.Grosso, A., Locatelli, M., & Croce, F. D. (2004). Combining swaps node weightsadaptive greedy approach maximum clique problem. Journal Heuristics,10, 135152.Hastad, J. (1999). Clique hard approximate within n1 . Acta Mathematica, 182,105142.Hoos, H. H. (2002). adaptive noise mechanism WalkSAT. ProceedingsEighteenth National Conference Artificial Intelligence, pp. 655660. AAAI Press /MIT Press, Menlo Park, CA, USA.Hoos, H. H., & Stutzle, T. (2004). Stochastic Local Search: Foundations Applications.Morgan Kaufmann Publishers, USA.Hoos, H., & Stutzle, T. (2000). Local search algorithms SAT: empirical evaluation.Gent, I., v.Maaren, H., & Walsh, T. (Eds.), SAT 2000, pp. 4386. IOS Press.Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling probabilistic smoothing: Efficient dynamic local search SAT. Hentenryck, P. V. (Ed.), PrinciplesPractice Constraint Programming CP 2002, Vol. 2470 Lecture NotesComputer Science, pp. 233248. Springer Verlag, Berlin, Germany.Ji, Y., Xu, X., & Stormo, G. D. (2004). graph theoretical approach predicting common RNA secondary structure motifs including pseudoknots unaligned sequences.Bioinformatics, 20 (10), 15911602.Johnson, D., & Trick, M. (Eds.). (1996). Cliques, Coloring Satisfiability: Second DIMACS Implementation Challenge, Vol. 26 DIMACS Series. American MathematicalSociety.Katayama, K., Hamamoto, A., & Narihisa, H. (2004). Solving maximum clique problem k-opt local search. Proceedings 2004 ACM Symposium Appliedcomputing, pp. 10211025.Marchiori, E. (2002). Genetic, iterated multistart local search maximum cliqueproblem. Applications Evolutionary Computing, Vol. 2279 Lecture NotesComputer Science, pp. 112121. Springer Verlag, Berlin, Germany.Pevzner, P. A., & Sze, S.-H. (2000). Combinatorial approaches finding subtle signalsDNA sequences. Proceedings Eighth International Conference IntelligentSystems Molecular Biology, pp. 269278. AAAI Press.Pullan, W., & Zhao, L. (2004). Resolvent clause weighting local search. Tawfik, A. Y.,& Goodwin, S. D. (Eds.), Advances Artificial Intelligence, 17th ConferenceCanadian Society Computational Studies Intelligence, Vol. 3060 Lecture NotesComputer Science, pp. 233247. Springer Verlag, Berlin, Germany.184fiDynamic Local Search Max-Clique ProblemResende, M., Feo, T., & Smith, S. (1998). Algorithm 786: FORTRAN subroutine approximate solution maximum independent set problem using GRASP. ACMTransactions Mathematical Software, 24, 386394.Schuurmans, D., & Southey, F. (2000). Local search characteristics incomplete SATprocedures. Proceedings Seventeenth National Conference Artificial Intelligence, pp. 297302. AAAI Press / MIT Press, Menlo Park, CA, USA.Solnon, C., & Fenet, S. (2004). study aco capabilities solving maximum cliqueproblem. Journal Heuristics, appear.Thornton, J., Pham, D. N., Bain, S., & Ferreira, V. (2004). Additive versus multiplicativeclause weighting SAT. Proceedings 19th National Conference ArtificialIntelligence (AAAI-04), pp. 191196. AAAI Press / MIT Press, Menlo Park, CA,USA.Tompkins, D., & Hoos, H. (2003). Scaling probabilistic smoothing: Dynamic localsearch unweighted MAX-SAT. Xiang, Y., & Chaib-draa, B. (Eds.), AdvancesArtificial Intelligence, 16th Conference Canadian Society ComputationalStudies Intelligence, Vol. 2671 Lecture Notes Computer Science, pp. 145159.Springer Verlag, Berlin, Germany.Watson, J., Whitley, L., & Howe, A. (2005). Linking search space structure, run-timedynamics, problem difficulty: step toward demystifying tabu search. JournalArtificial Intelligence, 24, 221261.185fiJournal Artificial Intelligence Research 25 (2006) 389424Submitted 09/05; published 03/06Graphical Modeling Preference ImportanceRonen I. Brafmanbrafman@cs.stanford.eduDepartment Computer ScienceStanford UniversityStanford CA 94305Carmel Domshlakdcarmel@ie.technion.ac.ilFaculty Industrial Engineering ManagementTechnion - Israel Institute TechnologyHaifa, Israel 32000Solomon E. Shimonyshimony@cs.bgu.ac.ilDepartment Computer ScienceBen-Gurion UniversityBeer Sheva, Israel 84105Abstractrecent years, CP-nets emerged useful tool supporting preference elicitation, reasoning, representation. CP-nets capture support reasoning qualitative conditional preference statements, statements relatively natural usersexpress. paper, extend CP-nets formalism handle another classnatural qualitative statements one often uses expressing preferences daily life statements relative importance attributes. resulting formalism, TCP-nets, maintainsspirit CP-nets, remains focused using simple natural preferencestatements, uses ceteris paribus semantics, utilizes graphical representationinformation reason consistency perform, possibly constrained, optimization using it. extra expressiveness provides allows us better model tradeoffsusers would like make, faithfully representing preferences.1. Introductionability make decisions assess potential courses action corner-stonenumerous AI applications, including expert systems, autonomous agents, decision-supportsystems, recommender systems, configuration software, constrained optimization applications. make good decisions, must able assess compare different alternatives. Sometimes, comparison performed implicitly, many recommendersystems (Burke, 2000; Resnick & Varian, 1997). frequently, explicit informationdecision-makers preferences required.classical decision theory decision analysis utility functions used representdecision-makers preferences. However, process obtaining type informationrequired generate good utility function involved, time-consuming requires nonnegligible effort part user (French, 1986). Sometimes effort necessarypossible, many applications user cannot engaged lengthy periodtime cannot supported human decision analyst. instance, caseon-line product recommendation systems software decision-support applications.c2006AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiBrafman, Domshlak, & Shimonyutility function cannot need obtained, one resort other,qualitative forms preference representation. Ideally, qualitative informationeasily obtainable user non-intrusive means. is,able extract information natural relatively simple statements preferenceprovided user, elicitation process amenable automation.addition, automated reasoning qualitative preference informationsemantically effective computationally efficient.One framework preference representation addresses concernsConditional Preference Networks (CP-nets) (Boutilier et al. 1999, 2004a). CP-netsgraphical preference representation model grounded notion conditional preferential independence. preference elicitation CP-nets, decision maker (directlyindirectly) describes preference values one variable depends valuevariables. example, may state preference dessert dependsmain-course well whether alcoholic beverage. turn, preference alcoholic beverage may depend main course time day.information described graphical structure nodes represent variablesinterest edges capture direct preferential dependence relations variables.node annotated conditional preference table (CPT) describing userspreference alternative values node given different values parent nodes.CP-nets capture class intuitive useful natural language statements formprefer value x0 variable X given = y0 Z = z0 . statementsrequire complex introspection quantitative assessment.practical perspective, another class preference statementsless intuitive important, yet captured CP-net model. statementsform: important value X better valuebetter. call relative importance statements. instance, one might saylength journey important choice airline.refined notion importance, though still intuitive easy communicate, conditional relative importance: length journey importantchoice airline need give talk following day. Otherwise, choice airlineimportant. latter statement form: better assignment Ximportant better assignment given Z = z0 . Notice informationrelative importance different information preferential independence.instance, example above, users preference airline dependduration journey because, e.g., compares airlines based service,security levels, quality frequent flyer program. Informally, using statementsrelative importance user expresses preference compromises mayrequired. information important customized product configuration applications (Sabin & Weigel, 1998; Haag, 1998; Freuder & OSullivan, 2001), production,supply, constraints posed product space producer,constraints typically even unknown customer. Indeed, many applications, various resource (e.g., money, time, bandwidth) constraints exist, main computationaltask finding solution feasible preferentially dominatedsolution.390fiTCP-Netspaper consider enhancing expressive power CP-nets introducinginformation importance relations, obtaining preference-representation structurecall TCP-nets (for tradeoffs-enhanced CP-nets). capturing informationconditional preferential independence conditional relative importance, TCP-netsprovide richer framework representing user preferences, allowing stronger conclusionsdrawn, yet remaining committed use intuitive, qualitative information. time, show added relative importance informationsignificant impact consistency specified relation, techniques usedreasoning it. Focusing computational issues, show graphical structure mixed set preference statements captured TCP-net oftenexploited order achieve efficiency consistency testing preferentialreasoning.paper organized follows: Section 2 describes notions underlying TCP-nets:preference relations, preferential independence, relative importance. Section 3define TCP-nets, specify semantics, provide number examples. Section 4characterize class conditionally acyclic TCP-nets whose consistency guaranteedthen, Section 5 discuss complexity identifying members class.Section 6 present algorithm outcome optimization conditionally acyclic TCPnets, discuss related tasks reasoning preferences given TCP-net.conclude discussion related future work Section 7.2. Preference Orders, Independence, Relative Importancesection describe semantic concepts underlying TCP-nets: preference orders,preferential independence, conditional preferential independence, well relative importance conditional relative importance.2.1 Preference Independencemodel preference relation strict partial order. Thus, use terms preferenceorder strict partial order interchangeably. strict partial order binary relationoutcomes anti-reflexive, anti-symmetric transitive. Given two outcomeso, o0 , write o0 denote strictly preferred o0 .choice implies two outcomes cannot equally preferred. choicefollows fact language preferences use paper allowstatements indifference (as opposed incomparability), thus needusing weak orderings. Incorporating statements indifference pretty straightforward,explained Boutilier et al. (2004a), introduces much overhead formallytreat throughout paper.types outcomes concerned consist possible assignments setvariables. formally, assume given set V = {X1 , . . . , Xn } variablescorresponding domains D(X1 ), . . . , D(Xn ). set possible outcomes D(V) =D(X1 ) D(Xn ), use D() denote domain set variables well.example, context problem configuring personal computer (PC),variables may processor type, screen size, operating system etc., screen sizedomain {17in, 19in, 21in}, operating system domain {LINUX, Windows98,391fiBrafman, Domshlak, & ShimonyWindowsXP}, etc. complete assignment set variables specifies outcomeparticular PC configuration. Thus, preference relation outcomes specifiesstrict partial order possible PC configurations.number possible outcomes exponential n, set possible orderingsdoubly exponential n. Therefore, explicit specification representation ordering realistic, thus must describe implicitly usingcompact representation model. notion preferential independence plays key rolerepresentations. Intuitively, X V preferentially independent = V Xassignments Y, preference X values identical.Definition 1 Let x1 , x2 D(X) X V, y1 , y2 D(Y), = V X.say X preferentially independent iff, x1 , x2 , y1 , y2x1 y1 x2 y1 iff x1 y2 x2 y2(1)example, PC configuration example, user may assess screen size preferentially independent processor type operating system. could caseuser always prefers larger screen smaller screen, independent selectionprocessor and/or OS.Preferential independence strong property, therefore common.refined notion conditional preferential independence. Intuitively, X conditionally preferentially independent given Z every fixed assignmentZ, ranking X values independent value Y.Definition 2 Let X, Z partition V let z D(Z). X conditionallypreferentially independent given z iff, x1 , x2 , y1 , y2x1 y1 z x2 y1 z iff x1 y2 z x2 y2 z,(2)X conditionally preferentially independent given Z iff X conditionally preferentially independent given every assignment z D(Z).Returning PC example, user may assess operating system independentfeatures given processor type. is, always prefers LINUX given AMDprocessor WindowsXP given Intel processor (e.g., might believeWindowsXP optimized Intel processor, whereas LINUX otherwise better). Notenotions preferential independence conditional preferential independenceamong number standard well-known notions independence multi-attributeutility theory (Keeney & Raiffa, 1976).2.2 Relative ImportanceAlthough statements preferential independence natural useful, orderings obtained relying alone relatively weak. understand this, consider twopreferentially independent boolean attributes B values a1 , a2 b1 , b2 , respectively. B preferentially independent, specify preference ordervalues, say a1 a2 , independently value B. Similarly, preference392fiTCP-NetsB values, say b1 b2 , independent value A. deduce a1 b1preferred outcome a2 b2 least preferred outcome. However,know relative order a1 b2 a2 b1 . typically case considerindependent variables: rank one given fixed value other, often,cannot compare outcomes values different. One type informationaddress (though necessarily all) comparisons informationrelative importance. instance, state important B, meansprefer improvement improvement B. case, knowa1 b2 a2 b1 , totally order set outcomes a1 b1 a1 b2 a2 b1 a2 b2 .One may ask important us order a1 b2 a2 b1 all, knowa1 b1 preferred outcome. However, many typical scenarios,auxiliary user constraints prevent us providing user preferred(unconstrained) outcome. simple common example budget constraints,resource limitations, bandwidth buffer size (as adaptive richmedia systems described Brafman Friedman (2005) also common. cases,important know attributes user cares strongly, trymaintain good values attributes, compromising others.Returning PC configuration example, suppose attributes operating system processor type mutually preferentially independent. might say processor type important operating system, e.g, believe effectprocessors type system performance significant effectoperating system.Definition 3 Let pair variables X mutually preferentially independent givenW = V {X, }. say X important , denoted X ,every assignment w D(W) every xi , xj D(X), ya , yb D(Y ), xi xjgiven w, that:xi ya w xj yb w.(3)Note Eq. 3 holds even yb ya given w. instance, Xbinary variables, x1 x2 y1 y2 hold given w, X iffx1 y2 w x2 y1 w w D(W). Notice strict notion importancereduction preferred reduction X. Clearly, idea refinedproviding actual ordering elements D(XY ), discuss extensionSection 3.4. addition, one consider relative importance assessments amongtwo variables. However, feel benefit capturing statements small:believe statements relative importance referring two attributesnatural users articulate, inclusion would significantly reducecomputational advantages graphical modeling. Therefore, work focusrelative importance statements referring pairs attributes.Relative importance information natural enhancement independence information.such, relative importance retains desirable property - corresponds statementsnaive user would find simple clear evaluate articulate. Moreover,generalized naturally notion conditional relative importance. instance, supposerelative importance processor type operating system depends primaryusage PC. example, PC used primarily graphical applications,393fiBrafman, Domshlak, & Shimonychoice operating system important processor certainimportant software packages graphic design available LINUX. However,applications, processor type important applicationsWindows LINUX exist. Thus, say X important given zalways prefer reduce value rather value X, whenever z holds.Definition 4 Let X pair variables V, let Z W = V {X, }.say X important given z D(Z) iff, every assignment w0W0 = V ({X, } Z) have:xi ya zw0 xj yb zw0(4)whenever xi xj given zw0 . denote relation X z . Finally,z D(Z) either X z , z X, say relative importanceX conditioned Z, write RI(X, |Z).3. TCP-netsTCP-net (for Tradeoff-enhanced CP-nets) model extension CP-nets (Boutilieret al., 2004a) encodes conditional relative importance statements, well conditional preference statements supported CP-nets. primary usage TCP-netgraphical structure consistency analysis provided preference statements,classification complexity developing efficient algorithms various reasoning tasksstatements. particular, later show, structure acyclic(for suitable definition notion!), set preference statements representedTCP-net guaranteed consistent is, strict total orderoutcomes satisfies preference statements. follows formally defineTCP-net model. subsumes CP-net model, immediately definegeneral model rather proceed stages.3.1 TCP-net DefinitionTCP-nets annotated graphs three types edges. nodes TCP-net correspond problem variables V. first type (directed) edges comesoriginal CP-nets model captures direct preferential dependencies, is, edgeX implies user different preferences values given differentvalues X. second (directed) edge type captures relative importance relations.existence edge X implies X important . third(undirected) edge type captures conditional importance relations: edgenodes X exists exists non-empty variable subset Z V {X, }RI(X, |Z) holds. Without loss generality, follows, set Z assumedminimal set variables upon relative importance X depends.CP-nets, node X TCP-net annotated conditional preferencetable (CPT). table associates preferences D(X) every possible value assignmentparents X (denoted P a(X)). addition, TCP-nets, undirected edgeannotated conditional importance table (CIT). CIT associated edge394fiTCP-Nets(X, ) describes relative importance X given value correspondingimportance-conditioning variables Z.Definition 5 TCP-net N tuple hV, cp, i, ci, cpt, citi where:(1) V set nodes, corresponding problem variables {X1 , . . . , Xn }.(2) cp set directed cp-arcs {1 , . . . , k } (where cp stands conditional preference).cp-arc hXi , Xj N iff preferences values Xj depend actualvalue Xi . X V, let P a(X) = {X 0 |hX 0 , Xi cp}.(3) set directed i-arcs {1 , . . . , l } (where stands importance). i-arc (Xi , Xj )N iff Xi Xj .(4) ci set undirected ci-arcs {1 , . . . , } (where ci stands conditional importance). ci-arc (Xi , Xj ) N iff RI(Xi , Xj |Z) Z V{Xi , Xj }.1call Z selector set (Xi , Xj ) denote S(Xi , Xj ).(5) cpt associates CPT every node X V, CP (X) mappingD(P a(X)) (i.e., assignments Xs parent nodes) strict partial orders D(X).(6) cit associates every ci-arc = (Xi , Xj ) (possibly partial) mapping CIT ()(S(Xi , Xj )) orders set {Xi , Xj }.2TCP-net sets ci (and therefore also cit) empty, also CP-net.Thus, elements i, ci, cit describe absolute conditional importanceattributes provided TCP-nets, beyond conditional preference information capturedCP-nets.3.2 TCP-net Semanticssemantics TCP-net defined terms set strict partial orders consistentset constraints imposed preference importance information capturedTCP-net. intuitive idea rather straightforward: (1) strict partial ordersatisfies conditional preferences variable X two complete assignmentsdiffer value X ordered consistently ordering X valuesCPT X. Recall ordering depend parent X graph.(2) strict partial order satisfies assertion X important giventwo complete assignments differ value X only, prefersassignment provides X better value. (3) strict partial order satisfiesassertion X important given assignment z variable set Zgiven two complete assignments differ value X only, (bothof) Z assigned z, prefers assignment provides X better value.1. Observe every i-arc (Xi , Xj ) seen representing RI(Xi , Xj |). However, clear distinctioni-arcs ci-arc simplifies specification many forthcoming notions claims (e.g., Lemma 3Section 4, well related notion root variables.)2. is, relative importance relation Xi Xj may specified certain valuesselector set.395fiBrafman, Domshlak, & Shimonydefined formally below. use Xu denote preference relationvalues X given assignment u U P a(X).Definition 6 Consider TCP-net N = hV, cp, i, ci, cpt, citi.1. Let W = V ({X} P a(X)) let p D(P a(X)). preference (=strict partial)order D(V) satisfies Xp iff xi pw xj pw, every w D(W), wheneverXxi p xj holds.2. preference order D(V) satisfies CP (X) cpt iff satisfies Xp everyassignment p P a(X).3. preference order D(V) satisfies X iff every w D(W)W = V {X, }, xi ya w xj yb w whenever xi Xw xj .4. preference order D(V) satisfies X z iff every w D(W)W = V ({X, } Z), xi ya zw xj yb zw whenever xi Xzw xj .5. preference order D(V) satisfies CIT () ci-arc = (X, ) citsatisfies X z whenever entry table conditioned z ranks Ximportant.preference order D(V) satisfies TCP-net N = hV, cp, i, ci, cpt, citi iff:(1) every X V, satisfies CP (X),(2) every i-arc = (Xi , Xj ) i, satisfies X ,(3) every ci-arc = (Xi , Xj ) ci, satisfies CIT ().Definition 7 TCP-net satisfiable iff strict partial order D(V)satisfies it; o0 implied TCP-net N iff holds preference ordersD(V) satisfy N .Lemma 1 Preferential entailment respect satisfiable TCP-net transitive. is,N |= o0 N |= o0 o00 , N |= o00 .Proof: N |= o0 N |= o0 o00 , o0 o0 o00 preference orderssatisfying N . ordering transitive, must o00 satisfyingorderings.Note that, strictly speaking, use term satisfiable rather consistent respect set preference statements, given provide model theory,proof theory. However, since corresponding proof theory follows completely straightforward manner semantics combined transitivity, raisesproblem.396fiTCP-Nets3.3 TCP-net Examplesprovided formal specification TCP-nets model, let us illustrate TCPnets examples. simplicity presentation, following examplesvariables binary, although semantics TCP-nets given Definitions 6 7respect arbitrary finite domains.Example 1 (Evening Dress) Figure 1(a) presents CP-net consists three variablesJ, P , S, standing jacket, pants, shirt, respectively. prefer black whitecolor jacket pants, preference shirt color (red/white)conditioned color combination jacket pants: color,white shirt make dress colorless, therefore, red shirt preferable. Otherwise,jacket pants different colors, red shirt probably make eveningdress flashy, therefore, white shirt preferable. solid lines Figure 1(c) showpreference relation induced directly information captured CP-net;top bottom elements worst best outcomes, respectively,arrows directed less preferred preferred outcomes.J b JwJw Pw SwPb Pw?>=<89:;@ABCGFEDJ/P////////?>=<89:;J b PbJ w PbJ b PwJ w PwSr SwSw SrSw SrSr Sw(a)?>=<89:;GFED/@ABCJ/P////////?>=<89:;Jw Pw SrJw Pb Sr _ _ _ _/ iJb Pw SriiiiiiiiiiiiiiiitiiJb Pw LSw o_ _ _ _Jw Pb SwLLLLLLLLLLLLLLL%Jb Pb Sw}Jb Pb Sr(b)(c)Figure 1: Evening Dress CP-net & TCP-net.Figure 1(b) depicts TCP-net extends CP-net adding i-arc J P ,i.e., black jacket (unconditionally) important black pants.397fiBrafman, Domshlak, & Shimonyinduces additional relations among outcomes, captured dashed lines Figure 1(c).3reader may rightfully ask whether statement importance Example 1redundant: According preference, seems always wear black suitred shirt. However, preferences clear, various constraints may makeoutcomes, including preferred one, infeasible. instance, may cleanblack jacket, case preferred feasible alternative white jacket, blackpants, white shirt. Alternatively, suppose clean clothes velvetblack jacket white pants, silk white jacket black pants. wife forbidsmix velvet silk, compromise, wear either black (velvet)jacket white (velvet) pants, white (silk) jacket black (silk) pants.case, fact prefer wearing preferred jacket wearing preferred pantsdetermines higher desirability velvet combination. Now, wife prepareevening dress late work writing paper, information helpchoose among available options outfit would like most.Indeed, noted earlier, many applications involve limited resources, money,time, bandwidth, memory, etc. many instances, optimal assignment violatesresource constraints, must compromise accept less desirable, feasible assignment. TCP-nets capture information allows us make informed compromises.Example 2 (Flight USA) Figure 2(a) illustrates complicated CP-net, describing preference flight options conference USA, Israel.network consists five variables, standing various parameters flight:Day Flight variable distinguishes flights leaving day (D1d )two days (D2d ) conference, respectively. Since married,really busy work, prefer leave day conference.Airline variable represents airline. prefer fly British Airways (Aba )KLM (Aklm ).Departure Time variable distinguishes morning/noon (Tm ) evening/night(Tn ) flights. Among flights leaving two days conference prefer evening/nightflight, allow work longer day flight. However, amongflights leaving day conference prefer morning/noon flight,would like hours conference opening order resthotel.Stop-over variable distinguishes direct (S0s ) indirect (S1s ) flights,respectively. day flights awake time and, smoker, preferstop-over Europe (so smoking break). However, night flightssleep, leading preference direct flights, since shorter.Ticket Class variable C stands ticket class. night flight, prefer siteconomy class (Ce ) (I dont care sleep, seats significantlycheaper), day flight prefer pay seat business class (Cb ) (Beingawake, better appreciate good seat, food, wine).398fiTCP-Nets@ABCGFEDD1d D2dD1dD2dTm TnTn TmTmTnS1s S0sS0s S1sAba AklmTmTnCb CeCe Cb@ABCGFED89:;?>=<T000000000?>=<89:;@ABCGFEDC89:;?>=<(a)89:;?>=<?>=</ 89:;T000000000T,A@ABCGFED89:;?>=<CTm AklmTm AbaTn AbaSCCSC(b)Figure 2: Flight USA CP-net & TCP-net Example 2.CP-net Figure 2(a) captures preference statements, underlyingpreferential dependencies, Figure 2(b) presents TCP-net extends CP-netcapture relative importance relations parameters flight. First,i-arc A, getting suitable flying time importantgetting preferred airline. Second, ci-arc C,relative importance C depends values A:31. KLM day flight, intermediate stop Amsterdam importantflying business class (I feel KLMs business class goodcost/performance ratio, visiting casino Amsterdams airport soundslike good idea.)2. British Airways night flight, fact flight direct importantgetting cheaper economy seat (I ready pay business class,order spend even one minute Heathrow airport night).3. British Airways day flight, business class importantshort intermediate break (it hard find nice smoking area Heathrow).CIT ci-arc also shown Figure 2(b). 33.4 Relative Importance Non-binary Variablesread far, reader may rightfully ask whether notion relative (conditional)importance ceteris paribus, specified Section 2.2 (Eq. 3 4), strong3. clarity, ci-arc Figure 2(b) schematically labeled importance-conditioning variablesA.399fiBrafman, Domshlak, & ShimonyD1d D2dD1dD2dTm TnTn Tm@ABCGFED?>=<89:;?>=<89:;T6666666666SCAba AklmTm AbaTm AklmTn AbaTn AbaS1s Cb S0s Cb S1s Ce S0s CeS1s Cb S1s Ce S0s Cb S0s CeS0s Ce S0s Cb S1s Ce S1s CbS0s Ce S0s Cb S1s CbS0s Ce S1s Ce S1s CbFigure 3: network obtained clustering variables C Example 2.variables binary. example, consider refined notion departure time(variable ) Example 2, suppose two companies flyingIsrael USA (variable A). case, one may prefer better flight time, evenrequires compromise airline, long compromise significant.instance, get better flight time, one may willing compromise acceptairline among ranks top places context.generally, notion importance, well refined notions it,really means specifying ordering assignments variable pairs. sense,one could reduce TCP-nets CP-nets combining variablesimportance relation. Thus, instance, Flight USA example, couldcombine variables C (see Figure 3). resulting variable, SCdomain Cartesian product domains C. preferences valuesSC conditioned , current parent C, well A,belongs selector set CIT. general, selector set (and parents of) pairvariables viewed conditioning preferences value combinationspair. Hence, clustering help us already case binary variables certainorderings assignments two binary variables cannot specified TCP-net.However, clearly issue case non-binary variables,number combinations pairs values much larger.bottom line complex importance relations pairs variablescaptured. main questions how. strict importance relation use capturescertain relations compact manner. such, specification (e.g., termsnatural language statements) easy. rule possibility expressingrefined relations. Various linguistic constructs could used express relations.However, technically, captured clustering relevant variables,resulting representation would TCP-net, possibly simply CP-net. course,quite possible relations alternative compact representation couldhelp make reasoning efficient simply collapsing them,useful question future research examine.400fiTCP-Nets?>=<89:;89:;?>=<?>=<89:;89:;/ ?>=<000000000T,A?>=<89:;?>=<89:;CTm AklmTm AbaTn Aba89:;?>=<?>=</ 89:;00000 ttttt0000ttttyt89:;?>=<89:;?>=<CSCCSSC(a)()*+/.-,(b)()*+/.-,()*+/.-,/.-,()*+/.-,/ ()*+T555ooooo5woooo 5()*+()*+/.-,/ /.-,C/.-,()*+/.-,/ ()*+T555ooooo5woooo 5/.-,()*+/.-,()*+C/.-,()*+/.-,/ ()*+T555ooooo5woooo 5/.-,()*+/.-,()*+C(Tm Aklm )-directed(Tn Aklm )-directed(Tm Aba )-directed()*+/.-,'&%$!"#!"#/ '&%$T2qqfiA2fiqq2fi q2 fifififi qxfio qqq /.-,'&%$!"#()*+C(Tn Aba )-directed(c)Figure 4: (a) Flight USA TCP-net. (b) dependency graph. (c), Four w-directedgraphs.4. Conditionally Acyclic TCP-netsReturning notion TCP-net satisfiability, observe Definition 7 provides practical tools verifying satisfiability given TCP-net. Tackling issue, sectionintroduce large class TCP-nets whose members guaranteed satisfiable.refer class TCP-nets conditionally acyclic.Let us begin notion dependency graph induced TCP-net.Definition 8 dependency graph N ? TCP-net N contains nodes edgesN . Additionally, every ci-arc (Xi , Xj ) N every Xk S(Xi , Xj ), N ? containspair directed edges (Xk , Xi ) (Xk , Xj ), edges already N .Figure 4(b) depicts dependency graph TCP-net Flight USAexample, repeated convenience Figure 4(a). next definition, recallselector set ci-arc set nodes whose value determines directionarc. Recall also, assign value selector set, are, essence, orienting401fiBrafman, Domshlak, & Shimonyconditional importance edges. generally, selector sets assigned,transform N N ? . motivates following definition.Definition 9 Let S(N ) union selector sets N . Given assignment wnodes S(N ), w-directed graph N ? consists nodes directed edgesN ? . addition directed edge Xi Xj edge alreadyN ? , (Xi , Xj ) ci-arc N CIT (Xi , Xj ) specifies Xi Xj given w.Figure 4(c) presents four w-directed graphs TCP-net FlightUSA example. Note that, KLM night flights, relative importance Cspecified, thus edge C (Tn Aklm )-directed graphN ? .Using Definitions 8 9, specify class conditionally acyclic TCP-nets,show satisfiable4 .Definition 10 TCP-net N conditionally acyclic if, every assignment w S(N ),induced w-directed graphs N ? acyclic.show every conditionally acyclic TCP-net satisfiable, beginproviding two auxiliary lemmas.Lemma 2 property conditional acyclicity TCP-nets hereditary. is, given twoTCP-nets N = hV, cp, i, ci, cpt, citi N 0 = hV0 , cp0 , i0 , ci0 , cpt0 , cit0 i,1. N conditionally acyclic,2. V0 V, cp0 cp, i0 i, ci0 ci, cpt0 cpt, cit0 cit,N 0 also conditionally acyclic.Proof:proof straightforward Definition 10 since removing nodes and/oredges N , well removing preference importance information CPTsCITs N , remove cycles w-directed graphs N ? . Hence, Nconditionally acyclic, subnet N .Lemma 3 Every conditionally acyclic TCP-net N = hV, cp, i, ci, cpt, citi contains least onevariable X V, that, V \ {X}, hY, Xi 6 cp, (Y, X) 6 i,(X, ) 6 ci.Proof: prove existence root variable X N , consider dependencygraph N ? . Since N conditionally acyclic, node X 0 N ? neitherincoming directed undirected edges associated it. see latter, observe(i) every endpoint undirected edge N ? also incoming directed edge,4. authors would like thank Nic Wilson pointing error original definitionconditionally acyclic TCP-nets (Brafman & Domshlak, 2002).402fiTCP-Nets(ii) least one node N ? incoming directed edges, otherwiseconditional acyclicity N trivially violated. However, node X 0 alsoroot node N since edge set N ? superset N .Theorem 1 Every conditionally acyclic TCP-net satisfiable.Proof: prove constructively building satisfying preference ordering. fact,inductive hypothesis stronger: conditionally acyclic TCP-net stricttotal order satisfies it. proof induction number problem variables.result trivially holds one variable definition CPTs, since simply usestrict total order consistent CPT (and trivially satisfying Definition 6.)Assume theorem holds conditionally acyclic TCP-nets fewern variables. Let N TCP-net n variables, X one root variables N .(The existence root X guaranteed Lemma 3.) Let D(X) = {x1 , . . . , xk }domain chosen root variable X, let x1 . . . xk total ordering D(X)consistent (possibly partial) preferential ordering dictated CP (X)N . xi , 1 k, construct TCP-net Ni , n 1 variables V {X}removing X original network, and:1. variable , cp-arc hX, N , revise CPTrestricting row X = xi .2. ci-arc = (Y1 , Y2 ), X S(), revise CIT restrictingrow X = xi . If, result restriction, rows new CIT expressrelative importance Y1 Y2 , replace Ni correspondingi-arc, i.e., either (Y1 , Y2 ) (Y2 , Y1 ). Alternatively, CIT becomes empty,simply removed Ni .3. Remove variable X, together cp-arcs form hX, i, i-arcsform (X, ).Lemma 2 conditional acyclicity N implies conditional acyclicityreduced TCP-nets Ni . Therefore, inductive hypothesis constructpreference ordering reduced networks Ni . constructpreferential ordering original network N follows. Every outcome X = xjranked preferred outcome X = xi , 1 < j k. outcomesidentical X value, xi , ranked according ordering associated Ni (ignoringvalue X). Clearly, construction, ordering defined strict total order:obtained taking set strict total orders ordering them, respectively.Definition 6, easy see strict total order satisfies N .close look proof Theorem 1 reveals key property conditionallyacyclic TCP-nets induce ordering nodes network.ordering fixed, context dependent. Different assignments variablesprefix ordering yield different suffixes. Put differently, ordering depends403fiBrafman, Domshlak, & Shimonyvalues variables, captures relative importance variableparticular context. particular, nodes appear earlier orderingimportant particular context.observation helps explain rationale definition dependencygraph (Definition 8). sense, graph captures constraints orderingvariables. TCP-net conditionally acyclic constraints satisfiable. useperspective explain choices made definition dependency graphmay seem arbitrary. First, consider direction (unconditional) importance edgesimportant less important variable. simply goes linedesire use topological ordering important variables appear first.Second, consider direction CP-net edges parent children. turnsCP-nets, induced importance relationship parents children: parentsimportant children (see (Boutilier et al., 2004a)). Thus, edgesdependency graph must point parent child.Finally, order make sense idea context-dependent ordering, mustorder variables selector set ci-arc nodes connected arc.motivation last choice may bit less clear. following example showsnecessity (i.e., Theorem 1 cannot provided stronger notion TCP-netacyclicity obtained defining w-directed graphs N rather N ? ).ccBC89:;?>=<ABBAaabb: cc: ccC@ABCGFEDB@ABCGFEDCConsider TCP-net depicted above. TCP-net N defined three booleanvariables V = {A, B, C}, cp = {hA, Ci}, ci = {(A, B)} S(A, B) = {C},= . Clearly, two possible w-directed graphs N (not N ? ) acyclic.Now, suppose exists strict partial order 0 D(V) satisfies N .Definition 6,(1) abc 0 abc (from CP (C)),(2) abc 0 abc (from CIT ((A, B)) CP (B)),(3) abc 0 abc (from CP (C)),(4) abc 0 abc (from CIT ((A, B)) CP (A)).However, implies 0 anti-symmetric, contradicting assumption 0strict partial order.404fiTCP-Nets5. Verifying Conditional Acyclicitycontrast standard acyclicity directed graphs, property conditional acyclicitycannot easily tested general. Naive verification acyclicity every w-directedgraph require time exponential size S(N ). study complexityverifying conditional acyclicity, discuss hard polynomial subclassesproblem, provide sufficient and/or necessary conditions conditional acyclicityeasily checked certain subclasses TCP-nets.Let N TCP-net. cycles undirected graph underlying N ?(i.e., graph obtained N ? making directed edges undirected edges),clearly w-directed graphs N ? acyclic, property N ? simplecheck. Alternatively, suppose underlying undirected graph N ? containcycles. projection cycle back N ? contains directed arcs orienteddifferent directions cycle (one clockwise another counter-clockwise),w-directed graphs N ? still guaranteed acyclic. instance, subset (ofsize > 2) variables {T, A, S, C} running example Figure 4 forms cycleundirected graph underlying N ? , yet cycle satisfies aforementionedcriterion. sufficient condition conditional acyclicity also checked (loworder) polynomial time.remaining cases dependency graph N ? contains definesemi-directed cycles, rest section study cases closely.Definition 11 Let mixed set directed undirected edges, AU undirected graph underlying (that is, graph obtained dropping orientationdirected edges.) say semi-directed cycle(1) AU forms simple cycle (that is, AU consists single connected componentvertices degree 2 w.r.t. AU ).(2) edges directed.(3) directed edges point direction along AU (i.e., clockwisecounter-clockwise).assignment w selector sets ci-arcs semi-directed cycle N ?induces direction ci-arcs. say semi-directed cycle conditionallyacyclic assignment w obtain directed cycle A. Otherwise,called conditionally directed. Figure 5 illustrates semi-directed cycle (basedvariables running example) two possible configurations CITs makesemi-directed cycle conditionally directed conditionally acyclic, respectively.Using notions, Lemma 4 shows testing conditional acyclicity TCP-netsnaturally decomposable.Lemma 4 TCP-net N conditionally acyclic every semi-directed cycle N ?conditionally acyclic.Proof: proof straightforward: variable assignment makes onesemi-directed cycles N ? conditionally directed, cycle need examined.405fiBrafman, Domshlak, & Shimony?>=<89:;D1d89:;?>=<D1dSC(a)?>=<89:;89:;?>=<CD1dD1dSC(b)Figure 5: semi-directed cycle: (a) conditionally directed, (b) conditionally acyclic.Conversely, consider one semi-directed cycles N ? . assignment S(A)makes conditionally directed, additional assignments variables selectorsets change property.decomposition presented Lemma 4 allows us prove first complexity resulttesting conditional acyclicity. Theorem 2 shows determining TCP-netconditionally acyclic coNP-hard.Theorem 2 Given binary-valued TCP-net N , decision problem: conditionallydirected cycle N ? , NP-complete, even every ci-arc N |S()| = 1.Proof: proof hardness reduction 3-sat. Given 3-cnf formula F,construct following TCP-net N . every variable Xi every clause Cj F,construct boolean variable Xi variable Cj N , respectively (we retainnames, simplicity). addition, every clause Cj , construct three boolean variablesLj,k , 1 k 3, corresponding literals appearing Cj . Let n numberclauses F. TCP-net N somewhat degenerate, since cp-arcs. However,i-arc (Cj , Lj,k ) clause Cj every literal Lj,k Cj . addition,every literal Lj,k Cj , ci-arc (Lj,k , C(j+1) mod n ), whose selector variablevariable Xi represented Lj,k . relative importance Lj,k C(j+1) mod nselector Xi follows: Lj,k positive literal, variable Lj,k importantC(j+1) mod n Xi true, less important Xi false. negative literals,dependence selector variable reversed. completes construction - clearlypolynomial-time operation. Figure 6 illustrates subnet N corresponding clauseCj = (x1 x2 x3 ), Lj,1 , Lj,2 , Lj,3 correspond x1 , x2 , x3 , respectively.claim N ? , dependency graph network N constructed,conditionally directed cycle F satisfiable5 . easy see pathCj C(j+1) mod n values variables participating CjCj satisfied. Thus, assignment creates directed path C0 C05. particular construction, directed edges N ? outgoing selector variables Xieffect existence conditionally directed cycles N ? . Therefore, simply considerTCP-net N instead dependency graph N ? .406fiTCP-NetsONMLHIJKLj,1;;B;;;;X1;;;;;;X2PQRSHIJKWVUTPQRS/ ONMLCj+1Lj,2CjWVUT::::::::::X3::HIJKONMLLj,3GFED@ABCONMLHIJK@ABCGFED@ABCGFEDLj,1XXX1r 2sy 3;;Brrr;;;rrssyy;;rrr sssysyyrr;; rrsss yy;r;rrsss yyyr;rr; sss yyyys yyxrrrPQRSWVUTPQRSWVUTHIJKONML/Cj+1 yyyLj,2Cj::yyy::yyy::::yyyy::::yy|yyyHIJKONMLLj,3(a)(b)Figure 6: (a) TCP-net subnet Cj = (x1 x2 x3 ), (b) dependency graph.assignment satisfies clauses, problems equivalent - hence decisionproblem NP-hard. Deciding existence conditional directed cycle NP: Indeed,verifying existence semi-directed cycle given assignment S(A) (the unionselector sets ci-arcs A) done polynomial time. Thus, problemNP-complete.One reason complexity general problem, emerges proofTheorem 2, possibility number semi-directed cycles TCP-net dependency graph exponential size network. example, networkreduction 3n semi-directed cycles, due three possible paths generatedsubnet depicted Figure 6(a). Thus, natural consider networksnumber semi-directed cycles large. follows, call TCP-net Nm-cycle bounded number different semi-directed cycles dependency graph N ?m.Lemma 4 follows that, given m-cycle bounded TCP-net N , polynomialsize N , reduce testing conditional acyclicity N ? separatetests conditional acyclicity every semi-directed cycle N ? . naive checkconditional acyclicity semi-directed cycle requires time exponential size S(A)S(A) union selector sets ci-arcs A. Thus, S(A) smallsemi-directed cycle N ? , conditional acyclicity N ? checked quickly.fact, often determine semi-directed cycle conditionally directed/acycliceven efficiently enumerating possible assignments S(A).Lemma 5 Let semi-directed cycle N ? . conditionally acyclic, containspair ci-arcs , j S(i ) S(j ) 6= .words, selector sets ci-arcs pairwise disjoint,conditionally directed. Thus, Lemma 5 provides necessary condition conditionalacyclicity checked time polynomial number variables.407fiBrafman, Domshlak, & ShimonyProof (Lemma 5) selector sets ci-arcs pairwise disjoint, triviallyexists assignment S(A) orienting ci-arcs one direction.developing sufficient conditions conditional acyclicity, let us introduceuseful notation. First, given ci-arc = (X, ), say assignment w subset0 S() orients rows CIT () consistent w express relativeimportance X , any. words, w orients if, given w, relativeimportance X independent S() \ 0 . Second, semi-directed cyclecontains directed edges, refer (by definition, unique) directiondirection A.Lemma 6 semi-directed cycle conditionally acyclic contains pair ci-arcs , jeither:(a) contains directed edges, every assignment w S(i ) S(j ), either joriented w direction opposite direction A.(b) edges undirected, every assignment w S(i ) S(j ), joriented w opposite directions respect A.Proof:Follows immediately conditions lemma.Lemma 6 provides sufficient condition conditional acyclicitychecked time exponential maximal size selector set intersection pairci-arcs A. Note size TCP-net least large exponentialterm, description CIT exponential size correspondingselector set. Thus, checking condition linear size network.Definition 12 Given semi-directed cycle A, let shared(A) denote union pairwiseintersections selector sets ci-arcs A:[shared(A) =S(i ) S(j ),jLemma 7(a) semi-directed cycle contains directed edges, conditionally acyclicif, assignment u shared(A), exists ci-arc u orientedu direction opposite direction A.(b) semi-directed cycle contains ci-arcs, conditionally acyclicif, assignment u shared(A), exist two ci-arcs u1 , u2 orientedu opposite directions respect A.Proof: sufficiency condition clear, since subsumes conditionLemma 6. Thus, left proving necessity. start second case408fiTCP-Netscontains ci-arcs. Assume contrary conditionally acyclic,exists assignment u shared(A) pair ci-arcs oriented uopposite directions respect A.ci-arc A, let () = S() \ shared(A). Consider following disjointpartition = Aiu Aciu induced u A: ci-arc A, u orients ,Aiu . Otherwise, direction independent () given u,Aciu . make two observations:1. initial (contradicting) assumption implies (now directed) edges Aiuagree direction respect A.2. ci-arc () = , Aiu , sinceselectors instantiated u.Aciu = , first observation trivially contradicts initial assumptionconditionally acyclic. Alternatively, Aciu 6= , then, definition shared(A),(i ) (j ) = pair ci-arcs , j Aciu . meansassign (non-empty, second observation) (i ) independently,thus extend u assignment S(A) orient ci-arcs Aciu eitherdirection Aiu Aiu 6= , arbitrary joint direction Aiu = . Again,contradicts assumption conditionally acyclic. Hence, provedcondition necessary second case. proof first case containsdirected edges similar.general, size shared(A) O(|V|). Since check set assignmentsshared(A), implies problem may hard. Theorem 3 showsindeed case.Theorem 3 Given binary-valued, 1-cycle bounded TCP-net N , decision problem:conditionally directed cycle N ? , NP-complete, even every ci-arc N|S()| 3.Proof: proof hardness reduction 3-sat. Given 3-cnf formula F,construct following TCP-net N . every variable Xi every clause Cj F,construct boolean variables Xi Cj N , respectively. addition, add single dummyvariable C, i-arc (C, C1 ). Let n number clauses F. 1 j n 1,n 1 ci-arcs Ej = (Cj , Cj+1 ). addition, ci-arc En = (Cn , C).1 j n, CIT Ej determined clause Cj , follows. selector set Ejset variables appearing Cj , relative importance variablesEj determined follows: Cj less important Cj+1 valuesvariables selector set Cj false. (For j = n, read C instead Cj+1 ).constructed TCP-net N 1-cycle bounded, one semi-directedcycle dependency graph N ? , namely C, C1 , . . . , Cn , C. claim semidirected cycle conditionally directed F satisfiable. easy seedirected path C C exists ci-arcs directed Cj Cj+1 ,occurs exactly variable assignment makes clause Cj satisfiable. Hence,409fiBrafman, Domshlak, & Shimonydirected cycle occurs N exactly assignment makes clauses satisfiable,making two problems equivalent. Thus decision problem NP-hard. Finally,deciding existence conditional directed cycle NP (see proof Theorem 3),problem NP-complete.Observe proof Theorem 3 work size selectorsets bounded 2, 2-sat P. immediate question whetherlatter case problem becomes tractable, binary-valued TCP-nets answeraffirmative.Theorem 4 Given binary-valued, m-cycle bounded TCP-net N , polynomialsize N and, every ci-arc N |S()| 2, decision problem:conditional directed cycle N ? , P.Proof: proof uses reduction conditional acyclicity testing satisfiability. Letsemi-directed cycle |S()| k every ci-arc A. reduce conditionalacyclicity testing problem equivalent k-sat problem instance. particular, since 2sat solvable linear time (Even, Itai, & Shamir, 1976), together Lemma 4proves claim.First, assume least one directed edge (either i-arc cp-arc). definitionsemi-directed cycles, directed edges point direction, specifyingpossible cyclic orientation A. ci-arc A, let selector setS(i ) = {Xi,1 , ..., Xi,k }.6 Clearly, conditionally directed ci-arcsdirected consistently .Given semi-directed cycle A, create corresponding k-cnf formula F,F satisfiable conditionally directed. Let us call CIT (i ) entriesconsistent term -entries. Since S() = {Xi,1 , ..., Xi,k } N binaryvalued, represent non- entries CIT (i ) conjunction disjunctions,i.e., CNF form. number disjunctions equal number non- entriesCIT (i ), disjunction comprised k literals. Thus, representationCIT (i ) k-CNF formula, size linear size CIT (i ). (In fact, sizeresulting formula sometimes significantly smaller, one frequently simplifycomponent CNF fragments, property needed here.)Finally, compose CNF representations CIT (i ), every A, resultingk-CNF formula size linear combined number table entries. constructionF clearly linear-time operation. Likewise, easy see F satisfiableassignment S(A) converting directed cycle.minor unresolved issue semi-directed cycles consisting ci-arcs only. Givensemi-directed cycle A, reduce problem two sub-problems directedarc. Let A0 A00 cycles created inserting one dummy variable one i-arcclockwise A0 , counter-clockwise A00 . Now, conditionally directedeither A0 A00 (or both) conditionally directed.6. |S(i )| < k, impact compact reduction below.410fiTCP-Netssummarize analysis verifying conditional acyclicity, one must first identifysemi-directed cycles dependency graph TCP-net. Next, one must showgiven assignment w importance-conditioning variables semi-directedcycle, w-directed graph acyclic. problem coNP-hard general networks7 ,interesting classes networks tractable. casenumber semi-directed cycles large either size shared(A)cycle size selector set large. Note practice, onewould expect small selector sets statements X important= B = b . . . Z = z appear complexone would expect hear. Thus, Lemma 6, Lemma 7 (for semi-directed cycles smallshared(A)), Theorem 4 theoretical interest. Naturally, extendingtoolbox TCP-net subclasses efficiently tested consistency clearlytheoretical practical interest.6. Reasoning Conditionally Acyclic TCP-netsautomated consistency verification core part preference elicitation stage,efficiency reasoning user preferences one main desiderata modelpreference representation. particular importance task preference-basedoptimization constrained optimization, discuss first part section.Another important task, provides important component algorithmconstrained optimization present, outcome comparison discussed second partsection.6.1 Generating Optimal AssignmentsFollowing notation Boutilier et al. (2004a), x assignments disjointsubsets X variable set V, respectively, denote combination xxy. X = X = V, call xy completion assignment x, denoteComp(x) set completions x.One central properties original CP-net model (Boutilier et al., 2004a)that, given acyclic CP-net N (possibly empty) partial assignment xvariables, simple determine outcome consistent x (a completion x)preferentially optimal respect N . corresponding linear time forward sweepprocedure follows: Traverse variables topological order induced N ,set unassigned variable preferred value given parents values.immediate observation procedure works also conditionallyacyclic TCP-nets: relative importance relations play role case,network traversed according topological order induced CP-net partgiven TCP-net. fact, Corollary 1 holds TCP-net directed cyclesconsisting cp-arcs.Corollary 1 Given conditionally acyclic TCP-net (possibly empty) partial assignment xvariables, forward sweep procedure constructs preferred outcome Comp(x).7. actually means network large, probably solve reasonableamount time.411fiBrafman, Domshlak, & Shimonystrong computational property outcome optimization respect acyclic CPnets (and conditionally acyclic TCP-nets) hold TCP-net variablesconstrained set hard constraints, C. case, determining set preferentially non-dominated8 feasible outcomes trivial. acyclic CP-nets, branchbound algorithm determining optimal feasible outcomes introduced Boutilier,Brafman, Domshlak, Hoos, Poole (2004b). algorithm important anytimeproperty outcome added current set non-dominated outcomes,never removed. important implication property first generated assignment satisfies set hard constraints also preferentially non-dominated.words, finding one non-dominated solution algorithm boils solvingunderlying CSP certain variable value ordering strategies.develop extension/modification algorithm Boutilier et al. (2004b)conditionally acyclic TCP-nets. extended algorithm Search-TCP retains anytime property shown Figure 7. key difference processing acyclicCP-net conditionally acyclic TCP-net semantics former implicitly induces single partial order importance variables (where node precedes descendants) (Boutilier et al., 2004a), semantics latter induceshierarchically-structured set partial orders: partial order correspondssingle assignment set selector variables network, or, specifically,certain w-directed graph.Formally, problem defined conditionally acyclic TCP-net Norig , sethard constraints Corig , posed variables Norig . Search-TCP algorithm (depictedFigure 7) recursive, recursive call receives three parameters:1. TCP-net N , subnet original conditionally acyclic TCP-net Norig ,2. set C hard constraints among variables N , subset originalset constraints Corig obtained restricting Corig variables N ,3. assignment K variables Norig N . follows, referassignment K context.initial call Search-TCP done Norig , Corig , {}, respectively.Basically, Search-TCP algorithm starts empty set solutions, graduallyextends adding new non-dominated solutions Corig . stage algorithm,current set solutions serves lower bound future candidates; new candidatepoint compared solutions generated point. candidatedominated member current solution set, added set.Search-TCP algorithm guided graphical structure Norig . proceedsassigning values variables top-down manner, assuring outcomesgenerated order satisfies (i.e., consistent with) N . recursive callSearch-TCP procedure TCP-net N , eliminated variable X one rootvariables N (line 1). Recall that, Lemma 3, conditional acyclicity N guaranteesexistence root variable X. values X considered according8. outcome said non-dominated respect preference order set outcomeso0 o0 o.412fiTCP-NetsSearch-TCP (N , C, K)Input: Conditionally acyclic TCP-net N ,Hard constraints C variables N ,Assignment K variables Norig \ N .Output: Set all, non-dominated w.r.t. N , solutions C.1. Choose variable X s.t. cp-arc hY, Xi,i-arc (Y, X), (X, ) N .2. Let x1 . . . xk total order D(X) consistent preferenceordering D(X) assignment P a(X) K.3. Initialize set local results R =4. (i = 1; k; + +)5. X = xi6. Strengthen constraints C X = xi obtain Ci7. Cj Ci j < Ci inconsistent8.continue next iterationelse9.Let K0 partial assignment induced X = xi Ci10.Ni = Reduce (N ,K0 )11.Let Ni1 , . . . , Nim components Ni , connectedeither edges Ni constraints Ci .12.(j = 1; j m; j + +)13.Rji = Search-TCP(Nij , Ci , K K0 )14.Rji 6= j15.foreach K0 R1i Rm016K 6 K holds o0 R add R17. return RFigure 7: Search-TCP algorithm conditionally acyclic TCP-net based constrainedoptimization.preference ordering induced D(X) assignment provided context KP a(X) (where P a(X) defined respect Norig ). Note K necessarily containsassignment P a(X) since X root variable currently considered subnet NNorig . additional variable assignment X = xi converts current set constraintsC strictly non-weaker constraint set Ci . result propagation X = xi ,values variables (at least, value X) fixed automatically, partialassignment K0 extends current context K recursive processing next variable.Reduce procedure, presented Figure 8, refines TCP-net N respect K0 :variable assigned K0 , reduce CPTs CITs involvingvariable, remove variable network. reduction CITs may removeconditioning relative importance variables, thus convert ci-arcs413fiBrafman, Domshlak, & Shimonyi-arcs, and/or remove ci-arcs completely. main point that, contrastCP-nets, pair X values xi , xj , variable elimination orderings processingnetworks Ni Nj , resulting propagating Ci Cj , respectively, may disagreeordering variables.Reduce (N , K0 )1. foreach {X = xi } K02. foreach cp-arc hX, N3.Restrict CPT rows dictated X = xi .4. foreach ci-arc = (Y1 , Y2 ) N s.t. X S()5.Restrict CIT rows dictated X = xi .6.if, given restricted CIT , relative importanceY1 Y2 independent S(),7.CIT empty8.Replace corresponding i-arc.9.else Remove .10. Remove N edges involving X.11. return N .Figure 8: Reduce procedure.partial assignment K0 causes current CP-net become disconnectedrespect edges network inter-variable hard constraints,connected component invokes independent search (lines 11-16).optimization variables within component independent variables outsidecomponent. addition, strengthening set constraints C X = xi Ci(line 6), pruning takes place search tree (lines 7-8): set constraintsCi strictly restrictive set constraints Cj = C {X = xj }j < i, search X = xi continued. reason pruningshown feasible outcome involving X = xi dominated (i.e., lesspreferable than) feasible outcome b involving X = xj thus cannotset non-dominated solutions original set constraints9 . Therefore, searchdepth-first branch-and-bound, set non-dominated solutions generated farproper subset required set non-dominated solutions problem,thus corresponds current lower bound.potentially non-dominated solutions particular subgraph returnedassignment X = xi , solution compared non-dominated solutionsinvolving preferred (in current context K) assignments X = xj , j < (line 16).solution X = xi added set non-dominated solutions currentsubgraph context passes non-domination test. semantics9. pruning introduced Boutilier et al. (2004b) acyclic CP-nets, remains validway conditionally acyclic TCP-nets. proof soundness pruning techniquerefer reader Lemma 2 (Boutilier et al., 2004b).414fiTCP-NetsTCP-nets, given context K, solution involving X = xi preferredsolution involving X = xj , j < i. Thus, generated global set R never shrinks.Theorem 5 Given conditionally acyclic TCP-net N set hard constraints Cvariables N , outcome belongs set R generated algorithm Search-TCPconsistent C, outcome o0 consistent CN |= o0 o.Proof: Let RC desired set preferentially non-dominated solution C.prove theorem, show that:1. Completeness: preferentially non-dominated solution C pruned out, is,R RC ,2. Soundness: resulting set R contains preferentially dominated solution C,is, R RC .(1) solutions C pruned Search-TCP two places, namely searchspace pruning lines 7-8, non-dominance test step line 16. first case,correctness pruning technique used lines 7-8 given Lemma 2 (Boutilieret al., 2004b), thus pruning violate completeness Search-TCP.second case, explicitly generated solution rejected due failure nondominance test, 6 RC apparent since rejection based presentingconcrete solution o0 N |= o0 o. Hence, R RC .(2) show R RC enough prove newly generated solution cannot dominateexisting solution, is, added generated set solutions o0case N |= o0 . proof induction number problemvariables. First, claim trivially holds one-variable TCP-net, ordersolutions examined line 16 coincides total order selected singlevariable network line 2. Now, assume claim holds conditionallyacyclic TCP-nets fewer n variables. Let N TCP-net n variables, Cset hard constraints variables, X root variable N selectedline 1. Let R = {o1 , . . . , } output Search-TCP N C,elements R numbered according order non-dominance examinationline 16. Now, assume exists pair assignments oi , oj R, < j, yetN |= oj oi .First, suppose oi oj provide value X, oi = xl o0ioj = xl o0j , xl D(X). case, however, o0i o0j belong outputrecursive call Search-TCP Nl Cl , thus, inductive hypothesis, o0io0j preferentially incomparable. Likewise, Nl obtained line 10 reducing Nrespect xl , thus variables Nl preferentially independent X. Hence,preferential incomparability o0i o0j implies preferential incomparability oi oj ,thus N |= oj oi cannot case.Alternatively, suppose oi oj provide two different values X, oi = xl o0ioj = xm o0j , xl , xm D(X), D(X) numbered according total ordering415fiBrafman, Domshlak, & Shimonyvalues selected line 2. Observe that, construction Search-TCP, < j triviallyimplies l < m. However, using arguments identical constructive proofTheorem 6, exists least one preference order complete assignmentsvariables N oi oj . Hence, cannot case N |= oj oi ,thus contradiction assumption N |= oj oi complete.Note that, interested getting one non-dominated solution given sethard constraints (which often case), output first feasible outcome generatedSearch-TCP. comparisons pairs outcomes requirednothing compare first generated solution. However, interested gettingall, even non-dominated solutions, efficiency preferential comparisonpairs outcomes becomes important factor entire complexitySearch-TCP algorithm. Hence, next section consider preferential comparisonsclosely.6.2 Dominance Testing TCP-netsOne fundamental queries preference-representation formalism whetheroutcome dominates (i.e., strictly preferred to) outcome o0 . discussedabove, dominance queries required whenever wish generate onenon-dominated solution set hard constrains. Much like CP-nets, dominancequery hN , o, o0 respect TCP-net treated search improvingflipping sequence (purported) less preferred outcome o0 (purported)preferred outcome sequence successively preferred outcomes,flip sequence directly sanctioned given TCP-net. Formally,improving flipping sequence context TCP-nets defined follows:Definition 13 sequence outcomeso0 = o0 o1 om1 om =improving flipping sequence respect TCP-net N if, 0 < m,either1. (CP-flips) outcome oi different outcome oi+1 value exactly onevariable Xj , oi [j] oi+1 [j] given (identical) values P a(Xj ) oi oi+1 ,2. (I-flips) outcome oi different outcome oi+1 value exactly twovariables Xj Xk , oi [j] oi+1 [j] oi [k] oi+1 [k] given (identical) valuesP a(Xj ) P a(Xk ) oi oi+1 , Xj Xk given RI(Xj , Xk |Z)(identical) values Z oi oi+1 .10Clearly, value flip flipping sequence sanctioned TCP-net N ,CP-flips exactly flips allowed CP-nets (Boutilier et al., 2004a).10. implicitly assumed neither node parent other. implicit consequencestandard semantics conditional preferences node important children. Thus,need specify explicitly.416fiTCP-NetsTheorem 6 Given TCP-net N pair outcomes o0 , N |= o0improving flipping sequence respect N o0 o.Proof:= Given improving flipping sequence F:o0 = o0 o1 om1 om =o0 respect N , Definition 13, N |= oi oi+1 improvingflip F. proposition follows transitivity preferential entailmentrespect TCP-nets (Lemma 1).= Let G graph preferential ordering induced N , i.e., nodes G standoutcomes, directed edge o1 o2 improvingCP-flip I-flip o1 o2 , sanctioned N . Clearly, directed paths G equivalentimproving flipping sequences respect N .First, show preference ordering respects paths G (that is,path o1 o2 G, o2 o1 ) satisfies N . Assumecontrary respects paths G, satisfy N . Then, definitionsatisfiability (Definition 7), must exist either:1. variable X, assignment p D(P a(X)), values x, x0 D(X), assignmentw remaining variables W = V (X P a(X)), pxw px0 w,CP (X) dictates x0 x given p,2. importance arc pair variables X , assignment z D(S())(if i-arc, S() = ), values x, x0 D(X), y, 0 D(Y ), assignment wremaining variables W = V ({X, } S()), pxyw px0 0 w,(i) CP (X) dictates x0 x, (ii) (possibly empty) CIT dictatesX given z.However, first case, N specifies x0 x given p, CP-flip px0 w pxw,contradicting fact extends G. Similarly, second case, N specify x0 xgiven w, X given z, I-flip px0 0 w pxyw, contradictingfact extends G.Now, construction G, improving flipping sequence o0o, directed path G o0 o. Therefore, exist preferenceordering respecting paths G o0 o. However, basedobservation preference orderings respecting paths G, also satisfies N ,implies N 6|= o0 .Various methods used search flipping sequence. particular, believe least techniques, developed task respect CPnets Domshlak Brafman (2002), Domshlak (2002), Boutilier et al. (2004a)applied TCP-net model issue left future research. However,general, dominance testing respect CP-nets (and thus TCP-nets) known417fiBrafman, Domshlak, & ShimonyNP-hard (Boutilier et al., 2004a), thus practice one may possibly consider performingapproximate constrained optimization, using Search-TCP algorithm dominancetesting based one tractable refinements TCP-nets discussedBrafman, Domshlak, Kogan (2004a).7. DiscussionCP-nets (Boutilier et al., 1999, 2004a) relatively new graphical model representationreasoning preferences. development, however, already stimulated researchseveral directions (e.g., see (Brafman & Chernyavsky, 2005; Brafman & Dimopoulos, 2004;Brewka, 2002; Boutilier et al., 2001; Domshlak et al., 2003; Rossi et al., 2004; Lang, 2002;Wilson, 2004b, 2004a)). paper introduced qualitative notions absoluteconditional relative importance pairs variables extended CP-netmodel capture corresponding preference statements. extended model calledTCP-nets. identified wide class TCP-nets satisfiable, notably classconditionally acyclic TCP-nets, analyzed complexity algorithms testing membership class networks. also studied reasoning TCP-nets, focusingoutcome optimization conditionally acyclic TCP-nets without hard constraints.work opens several directions future research. First, important open theoretical question precise complexity dominance testing TCP-nets. contextCP-nets problem studied Domshlak (2002), Boutilier et al. (2004a),Goldsmith et al. (2005). Another question consistency TCP-netsconditionally acyclic. preliminary study issue context cyclic CP-netsdone Domshlak Brafman (2002) Goldsmith et al. (2005).growing research preference modeling motivated need preferenceelicitation, representation, reasoning techniques diverse areas AI user-centricinformation systems. particular, one main application areas mindautomatic personalized product configuration (Sabin & Weigel, 1998). Thus,remaining part section, first consider process preference elicitationTCP-nets, listing practical challenges addressed makeprocess appealing users en-masse. Then, relate work approachespreference-based optimization.7.1 Preference Elicitation TCP-nets (and Logical ModelsPreference)process preference elicitation known complex account takenformal model users preferences also numerous important factorshuman-computer interaction (e.g., see (Faltings, Pu, Torrens, & Viappiani, 2004; Pu &Faltings, 2004)). paper focus formalism structuring analyzingusers preferences, although (probably offline) applications, formalism couldactually used drive input process, much like Bayes network used helpexperts express beliefs.Depending application, schematic process constructing TCP-net wouldcommence asking decision maker identify variables interest, presentinguser, fixed. example, application CP-net adaptive418fiTCP-Netsdocument presentation (Domshlak, Brafman, & Shimony, 2001; Brafman, Domshlak, &Shimony, 2004b), content provider chooses set content elements, correspondset variables. online shopper-assistant agent, variables likelyfixed (e.g., agent online PC customizer) (Brafman et al., 2004a). Next,user asked consider variable, value variables influencepreferences values variable. point cp-arcs CPTs introduced.Next, user asked consider relative importance relations, ci-arcsadded. ci-arc, corresponding CIT filled.Clearly, one may prefer keep preference elicitation process user-driven, allowing user simply provide us set preference statements. setstatements fits language expressible TCP-nets model, specific TCPnet underlying statements constructed simple analysis referentsconditionals statements. TCP-net extraction statementssimpler statements provided another formal language, obtainedvia carefully designed, structured user interface. However, user obviouslynatural provide statements natural language. Hence, interesting practical question related elicitation qualitative preferences model acquisition speechand/or text (Asher & Morreau, 1995; Glass, 1999; Bethard, Yu, Thornton, Hatzivassiloglou,& Jurafsky, 2004). Observe intuitiveness qualitative preferential statementsclosely related fact straightforward representation natural language everyday life. addition, collections typical preferential statements seemform linguistic domain priori constrained special manner. mayallow us develop specialized techniques tools understanding correspondinglanguage. offline online language understanding considered, since usereither describe preferences offline, self-contained text, asked online,part interactive process (possibly mixed) preference elicitation preference-basedconstrained optimization.Yet another possible approach eliciting TCP-nets, well alternative logicalmodels preferences, would allow user expressing pair-wise comparisonscompletely specified choices, construct TCP-net consistent input.scope quantitative models preference representation, example-basedmodel generation adopted numerous user-centric optimization systems (e.g.,see (Linden, Hanks, & Lesh, 1997; Blythe, 2002).) However, devising frameworklearning qualitative models preference seems somewhat challenging. theory,nothing prevents us adopting example-based generation TCP-nets since latterseen compact representation preference relation space choices.question, however, whether reasonably small set pair-wise choice comparisonsprovide us sufficient basis learning TCP-net consistenttraining examples, compact TCP-net generalize justifiable mannerbeyond provided examples. best knowledge, far questionstudied logical preference-representation models, hence clearly poseschallenging venue future research.1111. Note that, interested compact modeling pair-wise comparisons choices,numerous techniques area machine learning found useful. instance, onelearn decision tree classifying ordered pairs choices preferred (first choice second choice)419fiBrafman, Domshlak, & Shimony7.2 Related Workshow Section 6, extending CP-nets TCP-nets appealing mainly scopedecision scenarios space syntactically possible choices (either explicitlyimplicitly) constrained hard constraints. review related approachespreference-based optimization appeared literature.primary example preference-based optimization soft-constraints formalism(e.g., see Bistarelli et al. (1997)), developed model constraint satisfaction problemseither over-constrained (and thus unsolvable according standard meaningsatisfaction) (Freuder & Wallace, 1992), suffer imprecise knowledge actualconstraints (Fargier & Lang, 1993). formalism, constrained optimization problemrepresented set preference orders assignments subsets variables, togetheroperator combining preference relations subsets variablespreference relation assignments whole set variables. subsetvariables corresponds soft constraint satisfied different extentdifferent variable assignments. much flexibility local preferenceorders specified, combined. Various soft-constraints models,weighted (Bistarelli et al., 1999), fuzzy (Schiex, 1992), probabilistic (Fargier & Lang,1993), lexicographic (Fargier et al., 1993) CSPs, discussed literature softconstraint satisfaction.conceptual difference approach soft-constraints formalismlatter based tightly coupled representation preferences constraints,representation two paradigms completely decoupled. Informally, softconstraints machinery developed optimization partial constraint satisfaction,dealing optimization face constraint satisfaction. instance,personalized product configuration, two parties involved typically: manufacturer consumer. manufacturer brings forth product expertise,set hard constraints possible system configurations operating environment.user expresses preferences properties final product. Typically numerousconfigurations satisfy production constraints, manufacturer strives provideuser maximal satisfaction finding one preferred, feasible productconfiguration. naturally leads decoupled approach.Freuder OSullivan (2001) proposed framework interactive sessions overconstrained problems. session, constraint solver discoverssolution current set constraints available, user asked consider tradeoffs. example, following Freuder OSullivan (2001), suppose set userrequirements photo-camera configuration tool weight cameraless 10 ounces zoom lens least 10X. camera meetsrequirements, user may specify tradeoffs increase weight limit 14ounces, zoom lens least 10X (possibly using suggestions automatically generated tool). turn, tradeoffs used refining currentset requirements, system goes new constraint satisfaction process.preferred. However, classification guarantee general resulting binaryrelation space choices anti-symmetric assumption preference transitivity (ajoint property considered extremely natural literature preference structures (Hansson,2001).)420fiTCP-Netstradeoffs exploited Freuder OSullivan (2001) correspond informationcaptured TCP-nets i-arcs. However, instead treating informationincremental compromising update set hard constraints done FreuderOSullivan (2001), TCP-net based constrained optimization presented Section 6,exploit information guide constraint solver preferable feasible solutions.hand, motivation ideas behind work Freuder OSullivanwell works interactive search (see works, e.g., interactive goalprogramming (Dyer, 1972), interactive optimization based example critique (Pu &Faltings, 2004)) open venue future research interactive preference-based constrainedoptimization TCP-nets, elicitation user preferences interleavedsearch feasible solution.notion lexicographic orders/preferences (Fishburn, 1974; Schiex et al., 1995;Freuder et al., 2003) closely related notion importance. idea lexicographic ordering often used qualitative approaches multi-criteria decision making.Basically, implies one item better another important (lexicographically earlier) criteria differ, considered better overall, regardlesspoorly may criteria. Thus, four criteria (or attributes)A, B, C, D, thus ordered, well miserably B, C D, whereas o0slightly worse much better criteria, still deemed better. termsnotion variable importance, lexicographic ordering attributes denotes specialform relative importance attribute versus set attributes. Thus, exampleabove, important B, C combined; B important Ccombined, C important D. note Wilson (2004b) providesnice language capture statements more. Wilson allows statementsform = preferred = all-else-being-equal, except B C. is, giventwo outcomes differ A, B C only, one assigns preferredone assigns a0 , regardless value B C outcomes. Hence,richer language particular capture lexicographic preferences.believe lexicographic ordering attributes typically strong,think flexibility provided Wilsons language could quite useful. However,one starts analyzing relationships sets attributes, utility graphical modelsanalysis power becomes questionable. Indeed, aware graphicalanalysis Wilsons approach, except special case covered TCP-nets. Moreover,intuition relative importance sets notion users much lesscomfortable specifying many applications. However, hypothesis requires empiricalverification, well general study exact expressive power TCP-nets, i.e.,characterizing partial orders expressible using language. believeimportant avenue future research.AcknowledgmentsRonen Brafman Solomon Shimony partly supported Paul Ivanier CenterRobotics Production Management. Ronen Brafman partly supported NSFgrants SES-0527650 IIS-0534662. Ronen Brafmans permanent address is: DepartmentComputer Science, Ben Gurion University, Israel.421fiBrafman, Domshlak, & ShimonyReferencesAsher, N., & Morreau, M. (1995). generic sentences mean. Carlson, G., &Pelletier, F. J. (Eds.), Generic Book, pp. 300338. Chicago University Press.Bethard, S., Yu, H., Thornton, A., Hatzivassiloglou, V., & Jurafsky, D. (2004). Automaticextraction opinion propositions holders. Proceedings AAAISpring Symposium Exploring Attitude Affect Text: Theories Applications.Bistarelli, S., Fargier, H., Montanari, U., Rossi, F., Schiex, T., & Verfaillie, G. (1999).Semiring-based CSPs valued CSPs: Frameworks, properties, comparison.Constraints, 4 (3), 275316.Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint solvingoptimization. Journal ACM, 44 (2), 201236.Blythe, J. (2002). Visual exploration incremental utility elicitation. ProceedingsNational Conference Artificial Intelligence (AAAI), pp. 526532.Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: directed graphicalrepresentation conditional utilities. Proceedings Seventeenth ConferenceUncertainty Artificial Intelligence, pp. 5664.Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004a). CP-nets: toolrepresenting reasoning conditional ceteris paribus preference statements.Journal Artificial Intelligence Research (JAIR), 21, 135191.Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004b). Preference-basedconstrained optimization CP-nets. Computational Intelligence (Special IssuePreferences AI CP), 20 (2), 137157.Boutilier, C., Brafman, R., Hoos, H., & Poole, D. (1999). Reasoning conditional ceterisparibus preference statements. Proceedings Fifteenth Annual ConferenceUncertainty Artificial Intelligence, pp. 7180. Morgan Kaufmann Publishers.Brafman, R., & Chernyavsky, Y. (2005). Planning goal preferences constraints.Proceedings International Conference Automated Planning Scheduling,pp. 182191, Monterey, CA.Brafman, R., & Domshlak, C. (2002). Introducing variable importance tradeoffs CPnets. Proceedings Eighteenth Annual Conference Uncertainty ArtificialIntelligence, pp. 6976, Edmonton, Canada.Brafman, R., Domshlak, C., & Kogan, T. (2004a). Compact value-function representationsqualitative preferences. Proceedings Twentieth Annual ConferenceUncertainty Artificial Intelligence, pp. 5158, Banff, Canada.Brafman, R., Domshlak, C., & Shimony, S. E. (2004b). Qualitative decision makingadaptive presentation structured information. ACM Transactions InformationSystems, 22 (4), 503539.Brafman, R. I., & Dimopoulos, Y. (2004). Extended semantics optimization algorithmscp-networks. Computational Intelligence (Special Issue Preferences AICP), 20 (2), 218245.422fiTCP-NetsBrafman, R. I., & Friedman, D. (2005). Adaptive rich media presentations via preferencebased constrained optimization. Proceedings IJCAI-05 Workshop Advances Preference Handling, pp. 1924, Edinburgh, Scotland.Brewka, G. (2002). Logic programming ordered disjunction. ProceedingsEighteenth National Conference Artificial Intelligence, pp. 100105, Edmonton,Canada. AAAI Press.Burke, R. (2000). Knowledge-based recommender systems. Kent, A. (Ed.), EncyclopediaLibrary Information Systems, Vol. 69, pp. 180200. Marcel Dekker, New York.Domshlak, C. (2002). Modeling Reasoning Preferences CP-nets. Ph.D.thesis, Ben-Gurion University, Israel.Domshlak, C., & Brafman, R. (2002). CP-nets - reasoning consistency testing.Proceedings Eighth International Conference Principles Knowledge Representation Reasoning, pp. 121132, Toulouse, France.Domshlak, C., Brafman, R., & Shimony, S. E. (2001). Preference-based configurationweb page content. Proceedings Seventeenth International Joint ConferenceArtificial Intelligence, pp. 14511456, Seattle.Domshlak, C., Rossi, F., Venable, K. B., & Walsh, T. (2003). Reasoning softconstraints conditional preferences: Complexity results approximation techniques. Proceedings Eighteenth International Joint Conference ArtificialIntelligence, pp. 215220, Acapulco, Mexico.Dyer, J. S. (1972). Interactive goal programming. Management Science, 19, 6270.Even, S., Itai, A., & Shamir, A. (1976). complexity timetable multicommodityflow problems. SIAM Journal Computing, 5, 691703.Faltings, B., Pu, P., Torrens, M., & Viappiani, P. (2004). Designing example-critiquing interaction. Proceedings International Conference Intelligent User Interfaces,pp. 2229, Funchal, Madeira, Portugal.Fargier, H., & Lang, J. (1993). Uncertainty constraint satisfaction problems: probabilistic approach. Proceedings European Conference Symbolic QualitativeApproaches Reasoning Uncertainty, Vol. 747 LNCS, pp. 97104.Fargier, H., Lang, J., & Schiex, T. (1993). Selecting preferred solutions fuzzy constraintsatisfaction problems. Proceedings First European Congress FuzzyIntelligent Technologies, pp. 11281134.Fishburn, P. (1974). Lexicographic orders, utilities, decision rules: survey. Management Science, 20 (11), 14421471.French, S. (1986). Decision Theory. Halsted Press, New York.Freuder, E., & OSullivan, B. (2001). Generating tradeoffs interactive constraint-basedconfiguration. Proceedings 7th International Conference PrinciplesPractice Constraint Programming, pp. 590594, Paphos, Cyprus.Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58, 2170.423fiBrafman, Domshlak, & ShimonyFreuder, E. C., Wallace, R. J., & Heffernan, R. (2003). Ordinal constraint satisfaction.Proceedings Fifth International Workshop Soft Constraints.Glass, J. (1999). Challenges spoken dialogue systems. Proceedings IEEE ASRUWorkshop, Keystone, CO.Goldsmith, J., Lang, J., Truszczynski, M., & Wilson, N. (2005). computational complexity dominance consistency CP-nets. Proceedings Nineteenth International Joint Conference Artificial Intelligence, pp. 144149, Edinburgh, Scotland.Haag, A. (1998). Sales configuration business processes. IEEE Intelligent SystemsApplications, 13 (4), 7885.Hansson, S. O. (2001). Preference logic. Gabbay, D. M., & Guenthner, F. (Eds.),Handbook Philosophical Logic (2 edition)., Vol. 4, pp. 319394. Kluwer.Keeney, R. L., & Raiffa, H. (1976). Decision Multiple Objectives: Preferences ValueTradeoffs. Wiley.Lang, J. (2002). preference representation combinatorial vote. ProceedingsEight International Conference Principles Knowledge RepresentationReasoning (KR), pp. 277288.Linden, G., Hanks, S., & Lesh, N. (1997). Interactive assessment user preference models:automated travel assistant. Proceedings Sixth International ConferenceUser Modeling, pp. 6778.Pu, P., & Faltings, B. (2004). Decision tradeoff using example critiquing constraintprogramming. Constraints: International Journal, 9 (4), 289310.Resnick, P., & Varian, H. R. (Eds.). (1997). Special Issue Recommender Systems, Vol. 40Communications ACM.Rossi, F., Venable, K. B., & Walsh, T. (2004). mCP nets: Representing reasoningpreferences multiple agents. Proceedings Nineteenth National ConferenceArtificial Intelligence, pp. 729734, San Jose, CL.Sabin, D., & Weigel, R. (1998). Product conguration frameworks - survey. IEEE Intelligent Systems Applications, 13 (4), 4249.Schiex, T. (1992). Possibilistic cosntraint satisfaction, handle soft constraints.Proceedings Eighth Conference Uncertainty Artificial Intelligence, pp. 269275.Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued constraint satisfaction problems: Hardeasy problems. Proceedings Fourteenth International Joint ConferenceArtificial Intelligence, pp. 631637.Wilson, N. (2004a). Consistency constrained optimisation conditional preferences.Proceedings Sixteenth European Conference Artificial Intelligence, pp.888894, Valencia.Wilson, N. (2004b). Extending CP-nets stronger conditional preference statements.Proceedings Nineteenth National Conference Artificial Intelligence, pp.735741, San Jose, CL.424fiJournal Artificial Intelligence Research 25 (2006) 529-576Submitted 4/05; published 4/06Asynchronous Partial Overlay: New Algorithm SolvingDistributed Constraint Satisfaction ProblemsRoger Maillermailler@ai.sri.comSRI International333 Ravenswood DrMenlo Park, CA 94025 USAVictor R. Lesserlesser@cs.umass.eduUniversity Massachusetts, Department Computer Science140 Governors DriveAmherst, 01003 USAAbstractDistributed Constraint Satisfaction (DCSP) long considered importantproblem multi-agent systems research. many real-world problemsrepresented constraint satisfaction problems often presentdistributed form. article, present new complete, distributed algorithm calledasynchronous partial overlay (APO) solving DCSPs based cooperative mediation process. primary ideas behind algorithm agents, actingmediator, centralize small, relevant portions DCSP, centralized subproblems overlap, agents increase size subproblems along critical pathswithin DCSP problem solving unfolds. present empirical evidence showsAPO outperforms known, complete DCSP techniques.1. IntroductionDistributed constraint satisfaction problem become useful representationused describe number problems multi-agent systems including distributedresource allocation (Conry, Kuwabara, Lesser, & Meyer, 1991) distributed scheduling(Sycara, Roth, Sadeh, & Fox, 1991). researchers cooperative multi-agent systemsfocused developing methods solving problems based one key assumption. Particularly, agents involved problem solving process autonomous.means agents willing exchange information directly relevantshared problem retain ability refuse solution obviously conflictsinternal goal.researchers believe focus agent autonomy precludes use centralization forces agents reveal internal constraints goalsmay, reasons privacy pure computational complexity, impossible achieve.Several algorithms developed explicit purpose allowing agentsretain autonomy even involved shared problem exhibitsinterdependencies. Probably best known algorithms fit descriptionfound work Yokoo et al. form distributed breakout (DBA) (Yokoo &Hirayama, 1996), asynchronous backtracking (ABT) (Yokoo, Durfee, Ishida, & Kuwabara,1992), asynchronous weak-commitment (AWC) (Yokoo & Hirayama, 2000).c2006AI Access Foundation. rights reserved.fiMailler & LesserUnfortunately, common drawback algorithms effortprovide agents complete privacy, algorithms prevent agentsmaking informed decisions global effects changing local allocation, schedule, value, etc. example, AWC, agents try value wait another agenttell work nogood message. this, agents neverlearn true reason another agent set agents unable accept value,learn value combination values doesnt work.addition, techniques suffer complete distribution control. words, agent makes decisions based incomplete often inaccurateview world. result leads unnecessary thrashing problemsolving agents trying adapt behavior agents,turn trying adapt them. Pathologically, behavior counter-productiveconvergence protocol(Fernandez, Bejar, Krishnamachari, Gomes, & Selman, 2003).iterative trial error approach discovering implicit implied constraintswithin problem causes agents pass exponential number messages actually reveals great deal information agents constraints domain values(Yokoo, Suzuki, & Hirayama, 2002). fact, order complete, agents using AWCwilling reveal shared constraints domain values. key thingnote statement AWC still allows agents retain autonomyeven forced reveal information variables constraints formglobal constraint network.paper, present cooperative mediation based DCSP protocol, called Asynchronous Partial Overlay (APO). Cooperative mediation represents new methodologylies somewhere centralized distributed problem solving usesdynamically constructed, partial centralization. allows cooperative mediation basedalgorithms, like APO, utilize speed current state-of-the-art centralized solverstaking advantage opportunities parallelism dynamically identifying relevantproblem structure.APO works agents asynchronously take role mediator. agentacts mediator, computes solution portion overall problem recommends value changes agents involved mediation session. If, resultrecommendations, causes conflicts agents outside session, linkspreventing repeating mistake future sessions.Like AWC, APO provides agents great deal autonomy allowing anyonetake mediator notice undesirable state currentsolution shared problem. adding autonomy, agents also ignorerecommendations changing local solution made agents. similarway AWC, APO sound complete agents willing revealdomains constraints shared variables allows agents obscurestates, domains, constraints strictly local variables.rest article, present formalization DCSP problem (section2). section 3, describe underlying assumptions motivation work.present APO algorithm (section 4.1) give example protocolsexecution simple 3-coloring problem (section 4.2). go give proofssoundness completeness algorithm (section 4.3). section 5, present530fiAsynchronous Partial Overlay: New Algorithm DCSPresults extensive testing compares APO AWC within distributed graphcoloring domain complete compatibility version SensorDCSP domain (Bejar,Krishnamachari, Gomes, & Selman, 2001) across variety metrics including numbercycles, messages, bytes transmitted, serial runtime. cases, showAPO significantly outperforms AWC (Yokoo, 1995; Hirayama & Yokoo, 2000). Section6 summarizes article discusses future research directions.2. Distributed Constraint SatisfactionConstraint Satisfaction Problem (CSP) consists following:set n variables V = {x1 , . . . , xn }.discrete, finite domains variables = {D1 , . . . , Dn }.set constraints R = {R1 , . . . , Rm } Ri (di1 , . . . , dij ) predicateCartesian product Di1 Dij returns true iff value assignmentsvariables satisfies constraint.problem find assignment = {d1 , . . . , dn |di Di }constraints R satisfied. CSP shown NP-complete, making formsearch necessity.distributed case, DCSP, using variable-based decomposition, agent assigned one variables along constraints variables. goalagent, local perspective, ensure constraints variablessatisfied. Clearly, agents goal independent goals agentssystem. fact, simplest cases, goals agents stronglyinterrelated. example, order one agent satisfy local constraints, anotheragent, potentially directly related constraint, may change valuevariable.article, sake clarity, restrict case agentassigned single variable given knowledge constraints variable.Since agent assigned single variable, refer agent namevariable manages. Also, restrict considering binary constraintsform Ri (di1 , di2 ). Since APO uses centralization core, easysee algorithm would work restrictions removed. pointdiscussed part algorithm description section 4.1.4.Throughout article, use term constraint graph refer graph formedrepresenting variables nodes constraints edges. Also, variables neighborsvariables shares constraints.531fiMailler & Lesser3. Assumptions Motivation3.1 Assumptionsfollowing assumptions made environments agentsprotocol designed:1. Agents situated, autonomous, computing entities. such, capablesensing environment, making local decisions based model intentionality, acting decisions. Agents rationally resource bounded.result, agents must communicate gain information others state,intentions, decisions, etc.2. Agents within multi-agent system share one joint goals. paper,goal Boolean nature stemming DCSP formulation.3. work focuses cooperative problem solving, agents cooperative.necessarily imply share state, intentions, etc.agents, are, degree, willing exchange information solve jointgoals. also imply change intentions, state, decisionsbased demands another agent. Agents still maintain autonomyability refuse revise decisions agents based local state,intentions, decisions, etc.4. agent capability computing solutions joint goal basedpotentially limited rationality. follows naturally ability agentsmake decisions, i.e., every agent capable computing solutionportion joint goal based desires.3.2 Motivation Mediation-Based Problem SolvingWebsters dictionary defines act mediating follows:Mediate: 1. act intermediary; especially work opposing sidesorder resolve (as dispute) bring (as settlement). 2. bringabout, influence, transmit acting intermediate controlling agentmechanism. (Merriam-Webster, 1995)definition, mediation implies degree centralizing shared problemorder group individuals derive conflict free solution. Clearly situationsparticipants willing (cooperative), mediation powerful paradigm solvingdisputes. rather strange, considering this, little done lookingmediation cooperative method solving DCSPs.Probably, earliest mediation-based approach solving conflicts amongst agentsairspace management application(Cammarata, McArthur, & Steeb, 1983). workinvestigates using various conflict resolution strategies deconflict airspace distributedair traffic control system. author proposes method solving disputesinvolved agents elect leader solve problem. elected, leader becomes532fiAsynchronous Partial Overlay: New Algorithm DCSPS1S2{1,2}{1,2}Figure 1: simple distributed problem two variables.responsible recognizing dispute, devising plan correct it, actingplan. Various election schemes tested, unfortunately, leader authoritymodify actions order resolve conflicts. obviously leads situationsplan suboptimal.(Hayden, Carrick, & Yang, 1999), authors describe mediation one number possible coordination mechanisms. work, mediator acts intermediaryagents also act coordinate behavior. intermediary, mediator routes messages, provides directory services, etc. provides loose couplingagents, since need know mediator. mediator also actcoordinate agents behavior tight interdependencies.research work mediation-based problem solving involved settling disputescompetitive semi-competitive agents. Probably one best examplesusing mediation manner found PERSUADER system(Sycara, 1988).PERSUADER designed settle conflicts adversarial parties involvedlabor dispute. PERSUADER uses case-based reasoning suggest concessions orderconverge satisfactory solution. Another example using mediation wayfound system called Designer Fabricator Interpreter (DFI) (Werkman, 1990).DFI, mediation used resolve conflicts one series problem solving steps.Whenever first step fails, case iterative negotiation, mediator agent stepstries convince agents relax constraints. fails, mediator mandatesfinal solution.may several reasons mediation deeply exploredcooperative problem solving method. First, researchers focused strongly usingdistributed computing way exploiting concurrency distribute computationneeded solve hard problems (Rao & Kumar, 1993). this, even partiallyand/or temporarily centralizing sections problem viewed contradictorycentral goal. Second, researchers often claimed part powerdistributed methods lies ability techniques solve problems naturallydistributed. example, supply chain problems generally central monitoringauthority. Again, directly sharing reasons particular choice made formconstraint seem contradict use distributed methods. Lastly, researchers oftenclaim reasons privacy security problem solved distributedfashion. Clearly, sharing information solve problem compromises agents abilityprivate and/or violates security manner.Although, parallelism, natural distribution, security, privacy, may seem like goodjustifications entirely distributed problem solving, actuality, whenever probleminterdependencies distributed problem solvers, degree centralizationinformation sharing must take place order derive conflict-free solution.533fiMailler & LesserConsider, simple example, problem figure 1. figure, two problemsolvers, one variable, share common goal different value oneanother. agents two allowable values: {1, 2}. Now, order solveproblem, agent must individually decide different valueagent. this, least, one agent must transmit value other.this, removes half privacy (by revealing one possible values), eliminatessecurity (because agent could make send values tellingvalue good), partially centralizes problem solving (agent S2 computesolutions based solution S1 presented decide problem solved agentS1 relies S2 solve it.) even simple example, achieving totally distributedproblem solving impossible.fact, look details current approaches solving DCSPs,observe significant amount centralization occurring. approachesperform centralization incrementally problem solving unfolds attemptrestrict amount internal information shared. Unfortunately, problemsinterdependencies among problem solvers, revealing agents information (suchpotential values variables) unavoidable. fact, solutions derivedone agents conceals information regarding shared constraintvariable based incomplete information therefore may always sound.follows then, since cannot avoid amount centralization, mediationnatural method solving problems contain interdependencies among distributedproblem solvers.4. Asynchronous Partial Overlaycooperative mediation based protocol, key ideas behind creation APOalgorithmUsing mediation, agents solve subproblems DCSP using internal search.local subproblems overlap allow rapid convergenceproblem solving.Agents should, time, increase size subproblem work alongcritical paths within CSP. increases overlap agents ensurescompleteness search.4.1 AlgorithmFigures 2, 3, 4, 5, 6 present basic APO algorithm. algorithm works constructing good list maintaining structure called agent view. agent viewholds names, values, domains, constraints variables agent linked.good list holds names variables known connected ownerpath constraint graph.problem solving unfolds, agent tries solve subproblem centralized within good list determine unsolvable indicates entire globalproblem over-constrained. this, agents take role mediator attempt534fiAsynchronous Partial Overlay: New Algorithm DCSPprocedure initializedi random Di ;pi sizeof (neighbors) + 1;mi true;mediate false;add xi good list;send (init, (xi , pi , di , mi , Di , Ci )) neighbors;initList neighbors;end initialize;received (init, (xj , pj , dj , mj , Dj , Cj ))Add (xj , pj , dj , mj , Dj , Cj ) agent view;xj neighbor xk good listadd xj good list;/ good listadd xl agent view xlconnected good list;pi sizeof (good list);end if;xj/ initListsend (init, (xi , pi , di , mi , Di , Ci )) xj ;elseremove xj initList;check agent view;end do;Figure 2: APO procedures initialization linking.change values variables within mediation session achieve satisfiedsubsystem. cannot achieved without causing violation agents outsidesession, mediator links agents assuming somehow relatedmediators variable. process continues one agents finds unsatisfiablesubsystem, conflicts removed.order facilitate problem solving process, agent dynamic prioritybased size good list (if two agents sized good listtie broken using lexicographical ordering names). Priorities usedagents decide mediates session conflicts arises. Priority orderingimportant two reasons. First, priorities ensure agent knowledgegets make decisions. improves efficiency algorithm decreasingeffects myopic decision making. Second, priorities improve effectivenessmediation process lower priority agents expect higher priority agents mediate.improves likelihood lower priority agents available mediationrequest sent.535fiMailler & Lesserreceived (ok?, (xj , pj , dj , mj ))update agent view (xj , pj , dj , mj );check agent view;end do;procedure check agent viewinitList 6= mediate 6=falsereturn;m0i hasConf lict(xi );m0i j (pj > pi mj = = true)(d0i Di ) (d0i agent view conflict)di conflicts exclusively lower priority neighborsdi d0i ;send (ok?, (xi , pi , di , mi )) xj agent view;elsemediate;else mi 6= m0imi m0i ;send (ok?, (xi , pi , di , mi )) xj agent view;end if;end check agent view;Figure 3: procedures local resolution, updating agent viewgood list.4.1.1 Initialization (Figure 2)startup, agents provided value (they pick randomly one isntassigned) constraints variable. Initialization proceedsagents send init message neighbors. initialization message includesvariables name (xi ), priority (pi ), current value(di ), agents desire mediate (mi ),domain (Di ), constraints (Ci ). array initList records names agentsinitialization messages sent to, reason become immediatelyapparent.agent receives initialization message (either initializationlater link request), records information agent view addsvariable good list can. variable added good listneighbor another variable already good list. ensures graph createdvariables good list always remains connected, focuses agents internalproblem solving variables knows interdependency with. initListchecked see message link request response link request.agent initList, means message response, agent removes536fiAsynchronous Partial Overlay: New Algorithm DCSPprocedure mediatepref erences ;counter 0;xj good listsend (evaluate?, (xi , pi )) xj ;counter ++;end do;mediate true;end mediate;receive (wait!, (xj , pj ))update agent view (xj , pj );counter - -;counter == 0 choose solution;end do;receive (evaluate!, (xj , pj , labeled Dj ))record (xj , labeled Dj ) preferences;update agent view (xj , pj );counter - -;counter == 0 choose solution;end do;Figure 4: procedures mediating APO session.name initList nothing further. agent initListmeans request, response init generated sent.important note agents contained good list subsetagents contained agent view. done maintain integrity good listallow links bidirectional. understand point, consider casesingle agent repeatedly mediated extended local subproblem longpath constraint graph. so, links agents may limitedview therefore unaware indirect connection mediator. orderlink bidirectional, receiver link request store namerequester agent view, cannot add good list pathidentified. seen section 4.3, bi-directionality links important ensureprotocols soundness.4.1.2 Checking agent view (Figure 3)initialization messages received, agents execute check agent viewprocedure (at end figure 2). procedure, current agent view (which containsassigned, known variable values) checked identify conflicts variableowned agent neighbors. If, check (called hasConflict537fiMailler & Lesserprocedure choose solutionselect solution using Branch Bound search that:1. satisfies constraints agents good list2. minimizes violations agents outside sessionsatisfies constraintsbroadcast solution;xj agent viewxj pref erencesd0j violates xk xk/ agent viewsend (init, (xi , pi , di , mi , Di , Ci )) xk ;add xk initList;end if;send (accept!, (d0j , xi , pi , di , mi )) xj ;update agent view xjelsesend (ok?, (xi , pi , di , mi )) xj ;end if;end do;mediate false;check agent view;end choose solution;Figure 5: procedure choosing solution APO mediation.figure), agent finds conflict one neighbors toldhigher priority agent want mediate, assumes role mediator.agent tell higher priority agent wants mediate flagmentioned previous section. Whenever agent checks agent view recomputesvalue flag based whether existing conflicts neighbors.flag set true indicates agent wishes mediate givenopportunity. mechanism acts like two-phase commit protocol, commonly seendatabase systems, ensures protocol live-lock dead-lock free.agent becomes mediator, first attempts rectify conflict(s)neighbors changing variable. simple, effective technique preventsmediation sessions occurring unnecessarily, stabilizes system saves messages time. mediator finds value removes conflict, makes changesends ok? message agents agent view. cannot find nonconflicting value, starts mediation session. ok? message similar initmessage, contains information priority, current value, etc. variable.4.1.3 Mediation (Figures 4, 5, 6)complex certainly interesting part protocol mediation.previously mentioned section, agent decides mediate conflict538fiAsynchronous Partial Overlay: New Algorithm DCSPreceived (evaluate?, (xj , pj ))mj true;mediate == true k (pk > pj mk = = true)send (wait!, (xi , pi ));elsemediate true;label Di names agentswould violated setting di d;send (evaluate!, (xi , pi , labeled Di ));end if;end do;received (accept!, (d, xj , pj , dj , mj ))di d;mediate false;send (ok?, (xi , pi , di , mi )) xj agent view;update agent view (xj , pj , dj , mj );check agent view;end do;Figure 6: Procedures receiving APO session.one neighbors expecting session request higher priorityagent. mediation starts mediator sending evaluate? messagesagents good list. purpose message two-fold. First, informsreceiving agent mediation begin tries obtain lockagent. lock, referred mediate figures, prevents agent engagingtwo sessions simultaneously local value change coursesession. second purpose message obtain information agenteffects making change local value. key point. obtaininginformation, mediator gains information variables constraints outsidelocal view without directly immediately link agents. allowsmediator understand greater impact decision also used determineextend view makes final decision.agent receives mediation request, responds either wait!evaluate! message. wait message indicates requester agentcurrently involved session expecting request agent higher priorityrequester, fact could itself. agent available, labelsdomain elements names agents would conflictasked take value. information returned evaluate! message.size evaluate! message strongly related number variablessize agents domain. cases either extremely large, numbertechniques used reduce overall size message. example techniques539fiMailler & Lesserinclude standard message compression, limiting domain elements returnedones actually create conflict simply sending relevant value/variable pairsmediator actually labeling. fact means largest evaluate! messageever actually needed polynomial number agents (O(V )). implementation,graph coloring, largest possible evaluate! message O(|D | + |V |).noted agents need return namesprivacy security reasons. effects completeness algorithm,completeness relies one agents eventually centralizing entireproblem worst case. mentioned section 3.2, whenever agent attemptscompletely hide information shared variable constraint distributed problem,completeness necessarily effected.mediator received either wait! evaluate! message agentssent request to, chooses solution. mediator determines receivedresponses using counter variable set size good listevaluate? messages first sent. mediator receives either wait!evaluate! message, decrements counter. reaches 0, agentsreplied.Agents sent wait! message dropped mediation agentssent evaluate! message labeled domains specified message recordedused search process. mediator uses current values alonglabeled domains received evaluate! messages conduct centralized search.Currently, solutions generated using Branch Bound search (Freuder & Wallace,1992) constraints good list must satisfied number outsideconflicts minimized. similar min-conflict heuristic (Minton,Johnston, Philips, & Laird, 1992). Notice although search takes variablesconstraints good list consideration, solution generates may adherevariable values agents dropped session. variablesactually considered outside session impact able changevalues calculated part min-conflict heuristic. causes search considercurrent values dropped variables weak-constraints final solution.addition, domain variables good list orderedvariables current value first element. causes search use currentvalue assignments first path search tree tendency minimizechanges made current assignments. heuristics, combined together,form lock key mechanism simultaneously exploits work previouslydone mediators acts minimize number changes assignments.presented section 5, simple feed-forward mechanisms, combinedlimited centralization needed solve problems, account considerable improvementsalgorithms runtime performance.satisfying assignments found search, mediator announcesproblem unsatisfiable algorithm terminates. solution chosen,accept! messages sent agents session, who, turn, adopt proposedanswer.mediator also sends ok messages agents agent view,whatever reason session. simply keeps agents agent views up540fiAsynchronous Partial Overlay: New Algorithm DCSPto-date, important determining solution reached. Lastly, usinginformation provided evaluate! messages, mediator sends init messagesagent outside agent view, caused conflict choosingsolution. linking step extends mediators view along paths likelycritical solving problem identifying over-constrained condition. step alsoensures completeness protocol.Although termination detection explicitly part APO protocol, techniquesimilar (Wellman & Walsh, 1999) could easily added detect quiescence amongstagents.4.1.4 Multiple Variables n-ary ConstraintsRemoving restrictions presented section 2 fairly straightforward process.APO uses linking part problem solving process, working n-ary constraintssimply involves linking n agents within constraint initializationpost-mediation linking needs occur. Priorities scheme identicalused binary constraints.Removing single agent per variable restriction also difficult factone strengths approach. using spanning tree algorithm initialization,agents quickly identify interdependencies internal variablesuse create separate good lists disconnected componentsinternal constraint graph. essence, startup, agents would treatdecomposed problems separate problem, using separate flag, priority,good list, etc. problem solving unfolds, agent discovers connectionsinternal variables (through external constraints), decomposed problems couldmerged together utilize single structure information.technique advantages able ensure consistency dependent internal variables attempting mediate (because local checkingmediation), allows agent handle independent variables separate problems.Using situation aware technique one shown yield best resultsprevious work(Mammen & Lesser, 1998). addition, technique allows agentshide variables strictly internal. pre-computation decomposedproblems, agents construct constraints encapsulate subproblemsn-ary constraints n number variables external links. derived constraints sent part init message whenever agent receiveslink request one external variables.4.2 ExampleConsider 3-coloring problem presented figure 7. problem, 8 agents,variable 12 edges constraints them. 3-coloringproblem, variable assigned one three available colors {Black, Red,Blue}. goal find assignment colors variables twovariables, connected edge, color.example, four constraints violation: (ND0,ND1), (ND1,ND3), (ND2,ND4),(ND6,ND7). Following algorithm, upon startup agent adds541fiMailler & LesserFigure 7: example 3-coloring problem 8 nodes 12 edges.good list sends init message neighbors. Upon receiving messages,agents add neighbors good list able identifyshared constraint themselves.startup completed, agents checks agent view.agents, except ND5, find conflicts. ND0 (priority 3) waits ND1mediate (priority 5). ND6 ND7, priority 4, wait ND4 (priority 5, tieND3 broken using lexicographical ordering). ND1, equal number agentsgood list, lower lexicographical order, waits ND4 start mediation. ND3,knowing highest priority amongst neighbors, first checks see resolveconflict changing value, case, cannot. ND3 starts sessioninvolves ND1, ND5, ND6, ND7. sends evaluate? message. ND4highest priority amongst neighbors, unable resolve conflict locally, alsostarts session sending evaluate? messages ND1, ND2, ND6, ND7.agents mediation receives evaluate? message, firstcheck see expecting mediation higher priority agent. case,ND1, ND6, ND7 expecting ND4 tell ND3 wait. labeldomain elements names variables would conflictresult adopting value. information sent evaluate! message.following labeled domains agents sent ND4:ND1 - Black causes conflicts; Red conflicts ND0 ND3; Blue conflictsND2 ND4ND2 - Black causes conflicts; Red conflicts ND0 ND3; Blue conflictsND4ND6 - Black conflicts ND7; Red conflicts ND3; Blue conflicts ND4ND7 - Black conflicts ND6; Red conflicts ND3; Blue conflicts ND4542fiAsynchronous Partial Overlay: New Algorithm DCSPFigure 8: state sample problem ND3 leads first mediation.following responses sent ND3:ND1 - wait!ND5 - Black causes conflicts; Red conflicts ND3; Blue causes conflictsND6 - wait!ND7 - wait!responses received, mediators, ND3 ND4, conduct branchbound searches attempt find satisfying assignment subproblemsminimizes amount conflict would created outside mediation. eithercannot find least one satisfying assignment, broadcasts solution cannotfound.example, ND3, limited information has, computes satisfyingsolution changes color remain consistent would also changedcolors ND6 ND7. Since told ND6 ND7 wait, changes color,sends accept ! message ND5 ok? messages ND1, ND6 ND7.information, ND4 finds solution thinks solve subproblem withoutcreating outside conflicts. changes color red, ND7 blue, ND1 blackleaving problem state shown figure 8.ND1, ND4, ND5, ND6 ND7 inform agents agent view newvalues, check conflicts. time, ND1, ND3, ND6 notice valuesconflict. ND3, highest priority, becomes mediator mediates sessionND1, ND5, ND6, ND7. Following protocol, ND3 sends evaluate?messages receiving agents label respond. following labeled domainsreturned:ND1 - Black conflicts ND3; Red conflicts ND0 ND4; Blue conflictsND2543fiMailler & LesserFigure 9: final solution ND2 leads second mediation.ND5 - Black conflicts ND3; Red causes conflicts; Blue causes conflictsND6 - Black conflicts ND3; Red conflicts ND4; Blue conflicts ND7ND7 - Black conflicts ND3 ND6; Red conflicts ND4; Blue causesconflictsND3, receiving messages, conducts search finds solution solvessubproblem. chooses change color red. ND1, ND3, ND5, ND6, ND7check agent view find conflicts. Since, point, none agentsconflict, problem solved (see figure 9).4.3 Soundness Completenesssection show APO algorithm sound complete.proofs, assumed communications reliable, meaning message sentxi xj xj receive message finite amount time. also assumexi sends message m1 sends message m2 xj , m1 receivedm2 . Lastly, assume centralized solver used algorithm soundcomplete. prove soundness completeness, helps principallemmas established.Lemma 1 Links bidirectional. i.e. xi xj agent view eventually xjxi agent view.Proof:Assume xi xj agent view xi agent view xj .order xi xj agent view, xi must received init messagepoint xj . two cases.Case 1: xj initList xi . case, xi must sent xj init messagefirst, meaning xj received init message therefore xi agent view,contradiction.544fiAsynchronous Partial Overlay: New Algorithm DCSPCase 2: xj initList xi . case, xi receives init messagexj , responds init message. means reliable communicationassumption holds, eventually xj receive xi init message add xi agent view.Also contradiction.Lemma 2 agent xi linked xj xj changes value, xi eventuallyinformed change update agent view.Proof:Assume xi value agent view xj incorrect. would meanpoint xj altered value without informing xi . two cases:Case 1: xj know needed send xi update. i.e. xi xjagent view. Contradicts lemma 1.Case 2: xj inform agents agent view changes value.clear code cannot happen. Agents change valuescheck agent view, choose solution, accept! procedures. cases, informsagents within agent view either sending ok? accept!message change value occurred. contradiction.Lemma 3 xi conflict one neighbors, expect mediationanother higher priority agent agent view, currently session,act mediator.Proof:Directly procedure check agent view.Lemma 4 xi mediates session solution, constraintsagents involved mediation satisfied.Proof:Assume two agents xj xk (either could xi ),mediated xi mediation conflict xj xk .two ways could happened.Case 1: One agents must value xi assignpart mediation.Assume xj and/or xk value xi assign. know since ximediated session including xj xk , xi receive wait! messageeither xj xk . means could mediating. also meansmust set mediate flags true xi sent evaluate?message. Since times agent change value mediate flagfalse, mediating, told mediator, xj and/or xk couldchanged values xi told to, contradicts assumption.Case 2: xi assigned value caused conflict one another.Lets assume xi assigned conflicting values. means xi chosesolution take account constraints xj xk . But, knowxi chooses satisfying solutions include constraintsagents good list. leads contradiction.545fiMailler & Lesserlemma important says mediator successfully concludedsession, conflicts exist constraints outsidemediation. viewed mediator pushing constraint violations outsideview. addition, mediators get information violationspushed establish links agents, time, gain context.important point considering completeness algorithm.Theorem 1 APO algorithm sound. i.e. reaches stable state eitherfound answer solution exists.Proof:order sound, agents stop reached answer.condition would stop without found answer oneagents expecting mediation request higher priority agentsend it. words, protocol deadlocked.Lets say 3 agents, xi , xj , xk pi < pj pk < pj (i could equal k)xk conflict xj . two cases xj would mediate sessionincluded xi , xi expecting to:Case 1: xi mj = true agent view actual value false.Assume xi mj = true agent view true value mj = false.would mean point xj changed value mj false without informingxi . one place xj changes value mj , check agent viewprocedure (see figure 3). Note procedure, whenever flag changes valuetrue false, agent sends ok message agents agent view. Sincelemma 1 know xi agent view xj , xi must received messagesaying mj = false, contradicting assumption.Case 2: xj believes xi mediating xi believe be.i.e. xj thinks mi = true pi > pj .previous case, know xj believes mi = true mustcase. need show pi < pj . Lets say p0i priority xj believesxi assume xj believes p0i > pj when, fact pi < pj . meanspoint xi sent message xj informing current priority p0i . Sinceknow priorities increase time (the good list gets larger), knowp0i pi (xj always correct value underestimates priority xi ). Since pj > pipi p0i pj > p0i contradicts assumption.also important point considering algorithm behaves. proofsays agents always either know underestimate true value neighborspriorities. this, agents attempt mediate fact sometimes,shouldnt. side effect attempt, however, correct prioritiesexchanged mistake doesnt get repeated. important thing mentioncase priority values become equal. case, tie broken usingalphabetical order names agents. ensures always waybreak ties.Definition 1 Oscillation condition occurs subset V 0 V agentsinfinitely cycling allowable values without reaching solution. words,agents live-locked546fiAsynchronous Partial Overlay: New Algorithm DCSPdefinition, order considered part oscillation, agent withinsubset must changing value (if stable, oscillating) must connectedmembers subset constraint (otherwise, actually partoscillation).Theorem 2 APO algorithm complete. i.e. solution exists, algorithmfind it. solution exist, report fact.Proof:solution exist whenever problem over-constrained. problemover-constrained, algorithm eventually produce good list variableswithin associated constraints lead solution. Since subset variablesunsatisfiable, entire problem unsatisfiable, therefore, solution possible.algorithm terminates failure condition reached.Since shown Theorem 1 whenever algorithm reaches stablestate, problem solved finds subset variables unsatisfiable terminates, need show reaches one two states finitetime. way agents reach stable state oneagents system oscillation.two cases consider, easy case single agent oscillating(|V 0 | = 1) case one agent oscillating (|V 0 | > 1).Case 1: agent xi caught infinite loop agentsstable.Lets assume xi infinite processing loop. means matterchanges value to, conflict one neighbors, changedvalue something doesnt conflict neighbors, would solutionstop. changes value conflict xj higher priority it,xj mediate xi , contradicting assumption agents stable.xi changes value conflict lower priority agent, lemma 3,act mediator neighbors. Since assumed agentsstable state, agents xi good list participate sessionlemma 4, agent xi conflicts removed. means xistable state contradicting assumption infinite loop.Case 2: Two agents oscillation.Lets say set agents V 0 V oscillation. consideragent xi within V 0 . know conditions xi changes valuesolve conflicts (a contradiction x wouldntconsidered part oscillation), mediator, receiver mediationagent V 0 . interesting case agent acts mediator.Consider case xi mediator call set agents mediatingVi . know according definition 1 mediation, least oneconflict must created remain otherwise oscillation would stop problemwould solved. fact, know remaining conflicts must containagent set V 0 Vi lemma 4. also know violated constraintmember Vi , xi link agent part constraints547fiMailler & Lessermember Vi . next time xi mediates, set Vi include membersnumber agents set V 0 Vi reduced. fact, whenever xi mediates setV 0 Vi reduced (assuming told wait! one agents.case, takes longer reduce set, proof still holds). Eventually, O(|V | 2 )mediations, xi within V 0 must Vi = V 0 (every agent within set mustmediated |V 0 | times order happen). agent mediates pushviolations outside set V 0 solve subproblem lemma 4. Eitherconditions contradicts oscillation assumption. Therefore, algorithm complete.QEDfairly clear that, domains exponential, algorithms worsecase runtime exponential. space complexity algorithm is, however, polynomial,agents retain names, priorities, values, constraints agents.5. Evaluationgreat deal testing evaluation conducted APO algorithm. Almostexclusively, test done comparing APO algorithm currently fastestknown, complete algorithm solving DCSPs called Asynchronous Weak Commitment(AWC) protocol. section describe AWC protocol (section 5.1),describe distributed 3-coloring domain present results extensive testing donedomain (section 5.2). testing compares two algorithms across varietymetrics, including cycle time, number messages, serial runtime.Next, describe tracking domain (section 5.3) present results testingdomain well. domain, modified core search algorithm APOtake advantage polynomial complexity problem. variant called, APOFlow, also described.5.1 Asynchronous Weak Commitment (AWC) ProtocolAWC protocol (Yokoo, 1995) one first algorithms used solving DCSPs.Like APO algorithm, AWC based variable decomposition. Also, like APO, AWCassigns agent priority value dynamically changes. AWC, however, usesweak-commitment heuristic (Yokoo, 1994) assign priorities valuesgets name.Upon startup, agents selects value variable sends ok? messagesneighbors (agents shares constraint with). message includes variablesvalue priority (they start 0).agent receives ok? message, updates agent view checksnogood list violated nogoods. nogood composed set nogood pairsdescribe combination agents values lead unsatisfiable condition. Initially, nogoods agents nogood list constraints variable.checking nogood list, agents check violations higher priority nogoods. priority nogood defined priority lowest priority variablenogood. value greater priority agents variable, nogoodhigher priority. Based results check, one three things happen:548fiAsynchronous Partial Overlay: New Algorithm DCSP1. higher priority nogoods violated, agent nothing.2. higher priority nogoods violated repairedsimply changing agents variable value, agent changes value sendsok? messages agents agent view. multiple possiblesatisfying values, agent chooses one minimizes number violatedlower priority nogoods.3. violated higher priority nogoods cannot repaired changingvalue variable, agent generates new nogood. nogoodpreviously generated nogood, nothing. Otherwise, sendsnew nogood every agent variable contained nogood raisespriority value variable. Finally, changes variable value one causesleast amount conflict sends ok? messages.Upon receiving nogood message another agent, agent adds nogoodnogood list rechecks nogood violations. new nogood includes namesagents agent view links them. linking step essentialcompleteness search(Yokoo et al., 1992), causes agents communicatenogoods ok? messages agents direct neighbors constraintgraph. overall effect increase messages reduction amountprivacy provided agents communicate potential domain valuesinformation constraints exchange ok? nogood messageslarger number agents.One recent advances AWC protocol addition resolventbased nogood learning (Hirayama & Yokoo, 2000) adaptation classical nogoodlearning methods (Ginsberg, 1993; Cha & Iwana, 1996; Frost & Dechter, 1994).resolvent method used whenever agent finds needs generate newnogood. Agents generate new nogoods domain values violationleast one higher priority nogood already nogood list. resolvent methodworks selecting one higher priority nogoods domain valuesaggregating together new nogood. almost identical resolventpropositional logic referred resolvent-based learning. AWCprotocol used testing incorporates resolvent-based nogood learning.5.2 Distributed Graph ColoringFollowing directly definition CSP, graph coloring problem, also knownk-colorability problem, consists following:set n variables V = {x1 , . . . , xn }.set possible colors variables = {D1 , . . . , Dn } Diexactly k allowable colors.set constraints R = {R1 , . . . , Rm } Ri (di , dj ) predicate implements equals relationship. predicate returns true iff value assignedxi differs value assigned xj .549fiMailler & Lesser100APOAWCCycles806040200102030405060708090100VariablesFigure 10: Comparison number cycles needed solve satisfiable, low-density 3coloring problems various sizes AWC APO.problem find assignment = {d1 , . . . , dn |di Di }constraints R satisfied. Like general CSP, graph coloring shownNP-complete values k > 2.test APO algorithm, implemented AWC APO algorithms conducted experiments distributed 3-coloring domain. distributed 3-coloring problem 3-coloring problem n variables binary constraints agentgiven single variable. conducted 3 sets graph coloring based experiments comparealgorithms computation communication costs.5.2.1 Satisfiable Graphsfirst set experiments, created solvable graph instances = 2.0n (lowdensity), = 2.3n (medium-density), = 2.7n (high-density) according methodpresented (Minton et al., 1992). Generating graphs way involves partitioningvariables k equal-sized groups. Edges added selecting two groupsrandom adding edge random member group. method ensuresresulting graphs satisfiable, also tests limited likely easiersubset possible graphs. tests done traditionally usedresearchers DCSPs.particular values chosen represent three major regionswithin phase-transition 3-colorability (Culberson & Gent, 2001). phase transitionCSP defined based order parameter, case average node degreed. transition occurs point random graphs created order valueyield half satisfiable half unsatisfiable instances. Values order parameter550fiAsynchronous Partial Overlay: New Algorithm DCSP200APOAWCCycles150100500102030405060708090100VariablesFigure 11: Comparison number cycles needed solve satisfiable, medium-density3-coloring problems various sizes AWC APO.lower transition point (more 50% instance satisfiable) referredleft transition. opposite true values right.Phase transitions important strongly correlated overall difficulty finding solution graph (Cheeseman, Kanefsky, & Taylor, 1991; Monasson,Zecchina, Kirkpatrick, Selman, & Troyansky, 1999; Culberson & Gent, 2001). Withinphase transition, randomly created instances typically difficult solve. Interestingly,problems right left phase transitions tend much easier.3-colorability, value = 2.0 left phase transition. region,randomly created graphs likely satisfiable usually easy find solve.= 2.3, middle phase transition graph 50% chancesatisfiable usually hard solve. = 2.7n, right phase transition,graphs likely unsatisfiable and, again, also easier solve.number papers (Yokoo & Hirayama, 2000, 1996; Hirayama & Yokoo, 2000)reported = 2.7n within critical phase transition 3-colorability. seemscaused misinterpretation previous work area(Cheeseman et al.,1991). Although Cheeseman, Kanefsky, Taylor reported = 2.7n withincritical region 3-colorability, using reduced graphs analysis.reduced graph one trivially colorable nodes non-relevant edgesremoved. example, one easily remove node two edges3-coloring problem always trivially colored. Additionally, nodespossess unique domain element neighbors also easily removed.later work, Culberson Gent identified critical region approximately= 2.3 therefore included tests(Culberson & Gent, 2001). One note,however, phase transitions typically done completely random graphs551fiMailler & Lesser2.02.32.7Nodes153045607590Overall153045607590Overall153045607590OverallAPOMean17.8227.0739.9753.2459.8380.75APOStDev8.1517.1125.7932.3235.3554.30AWCMean17.3824.6243.7669.9680.3282.92AWCStDev12.7019.2330.9849.03103.7661.0115.0434.0147.7292.73114.02160.885.5516.8126.5872.4675.84125.1220.4941.30109.99135.60185.84189.0411.2729.5874.85146.57119.9491.2713.8327.2842.4752.1564.5487.143.5610.1018.0123.1226.2642.8220.2939.9962.9286.89104.09127.0410.3224.0835.2343.6946.6264.97p(AW C AP O)0.770.270.340.010.070.810.010.000.040.000.010.000.070.000.000.000.000.000.000.000.00Table 1: Comparison number cycles needed solve satisfiable 3-coloring problemsvarious sizes densities AWC APO.552fiAsynchronous Partial Overlay: New Algorithm DCSPAPO140AWC120Cycles100806040200102030405060708090100VariablesFigure 12: Comparison number cycles needed solve satisfiable, high-density3-coloring problems various sizes AWC APO.definition involves satisfiable unsatisfiable instances,hard apply phase-tranisition results graphs created using technique describedbeginning section generates satisfiable instances. detailedphase transition analysis done graph generation technique fact,believe graphs tend easier randomly created satisfiable onessize order.evaluate relative strengths weakness approaches, measurednumber cycles number messages used course solvingproblems. cycle, incoming messages delivered, agent allowedprocess information, messages created processingadded outgoing queue delivered beginning next cycle. actualexecution time given one agent cycle varies according amount workneeded process incoming messages. random seeds used creategraph instance variable instantiation saved used algorithmsfairness.comparison AWC APO, randomly generated 10 graphs sizen = 15, 30, 45, 60, 75, 90 = 2.0n, 2.3n, 2.7n instance generated 10 initialvariable assignments. Therefore, combination n m, ran 100 trials makingtotal 1800 trials. results experiment seen figures 10 15table 1. mention results testing AWC obtainedexperiments agree previous results (Hirayama & Yokoo, 2000) verifying correctnessimplementation.first glance, figure 10 appears indicate satisfiable low-density graph instances, AWC APO perform almost identically terms cycles completion. Look553fiMailler & LesserAPOAWCNodes153045607590153045607590% LinksMean32.9317.2411.979.307.426.5155.7632.8927.0026.4721.9920.11% LinksStDev3.522.591.841.300.950.9612.318.887.899.127.677.89% CentralMean60.5342.5333.2729.0024.6024.9379.7360.9756.4956.9853.1950.38% CentralStDev9.128.066.617.316.736.1611.4510.9811.5714.2313.2913.78Table 2: Link statistics satisfiable, low-density problems.APOAWCNodes153045607590153045607590% LinksMean37.4621.2414.9012.8511.1710.0763.1942.8452.0543.6947.7744.04% LinksStDev3.452.852.272.992.453.1012.0611.8313.5214.5914.1010.84% CentralMean67.6051.3743.6742.9843.7640.4783.5672.2080.6774.9379.4177.98% CentralStDev8.269.3211.4013.0612.6914.558.9612.0911.2414.5612.1810.52Table 3: Link statistics satisfiable, medium-density problems.554fiAsynchronous Partial Overlay: New Algorithm DCSPAPOAWCNodes153045607590153045607590% LinksMean43.2824.2918.0713.8611.8510.8678.8758.0354.0653.0149.6347.72% LinksStDev2.862.612.061.771.571.8512.2911.6613.4412.7711.3613.81% CentralMean74.5359.3052.6247.7846.3750.7393.6086.2782.6783.4781.9180.32% CentralStDev8.349.7510.2111.3211.3214.487.759.5413.3411.049.9315.06Table 4: Link statistics satisfiable, high-density problems.ing associated table (Table 1), however, reveals overall, pairwise T-testindicates 99% confidence, APO outperforms AWC graphs.density, average degree, graph increases, difference becomesapparent. Figures 11 12 show APO begins scale much efficientlyAWC. attributed ability APO rapidly identify strong interdependencies variables derive solutions using centralized searchpartial subproblem.Tables 2 4, partially verify statement. see, average, less50% possible number links (n (n 1)) used APO solving problems (%Links column). addition, maximum amount centralization (% Central column)occuring within single agent (i.e. number agents agent view) remains fairlylow. highest degree centralization occurs small, high-density graphs. Intuitively,makes lot sense graphs, single node likely highdegree start. Combine fact dynamic priority orderingresult large amounts central problem solving.profound differences algorithms seen figures 13, 14, 15table 5. APO uses least order magnitude less messages AWC. Table 6 showsmessage savings lead large savings number bytes transmittedwell. Even though APO uses twice many bytes per message AWC (the messagesoptimized all), total amount information passed around significantlyless almost every case.Again, looking linking structure AWC produces gives insightsuses many messages APO. agents communicateagents linked whenever value changes, large number changesoccur single cycle, AWC tremendous amount thrashing behavior. APO,hand, avoids problem process mediating implicitly creates555fiMailler & Lesser2.02.32.7Nodes153045607590Overall153045607590Overall153045607590OverallAPOMean361.501117.152078.723387.134304.226742.14APOStDev179.57844.331552.982084.692651.154482.54AWCMean882.192431.716926.8618504.1721219.0133125.57AWCStDev967.213182.517395.2222281.5922714.9839766.56379.151640.083299.058773.1614368.8725826.74188.69931.222155.459613.8412066.3229172.661205.506325.2144191.8970104.74178683.02201145.37923.856914.7944693.3369050.66173493.21143236.26433.641623.893859.995838.369507.6016455.59164.12787.591921.513140.534486.0410679.921667.719014.0228964.4366857.87116016.71196239.221301.038104.3422900.8953221.0582857.63163722.90p(AW C AP O)0.000.000.000.000.000.000.010.000.000.000.000.000.000.000.000.000.000.000.000.000.00Table 5: Comparison number messages needed solve satisfiable 3-coloring problems various sizes densities AWC APO.556fiAsynchronous Partial Overlay: New Algorithm DCSP2.02.32.7Nodes153045607590Overall153045607590Overall153045607590OverallAPOMean10560.7632314.9759895.1097126.79123565.94192265.35APOStDev4909.0923138.5542740.8257428.7073493.72123384.05AWCMean11590.3832409.9395061.77259529.42294502.71466084.60AWCStDev13765.1945336.65106391.51326087.70328696.59581963.7211370.1347539.2095098.49247417.78401618.24712035.134951.7525486.7459312.25262844.89327990.65782835.8316260.1988946.59644007.011018059.112626178.312935211.4513237.31101077.04675192.261029273.232606377.802138087.1713415.5148542.24112541.02170174.55272391.95465571.424280.1521331.9652729.1085705.12122177.07288265.8222393.61125072.76405535.81945039.321641250.632793725.7818578.25116518.80327349.47773937.601204185.342397839.47p(AW C AP O)0.490.980.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00Table 6: Comparison number bytes transmitted APO AWC satisfiablegraph instances various sizes density.557fiMailler & Lesser35000APOAWC30000Messages2500020000150001000050000102030405060708090100VariablesFigure 13: Comparison number messages needed solve satisfiable, low-density3-coloring problems various sizes AWC APO.250000APOAWCMessages200000150000100000500000102030405060708090100VariablesFigure 14: Comparison number messages needed solve satisfiable, mediumdensity 3-coloring problems various sizes AWC APO.regions stability problem landscape mediator decides solution.addition, APO uses partial centralization solve problems, avoids uselarge number messages discover implied constraints trial error.558fiAsynchronous Partial Overlay: New Algorithm DCSP200000APOAWCMessages150000100000500000102030405060708090100VariablesFigure 15: Comparison number messages needed solve satisfiable, high-density3-coloring problems various sizes AWC APO.% Satisfiable10.80.60.40.201.822.22.42.62.8DensityFigure 16: Phase transition curve 60 node randomly generated graphs used testing.see next two experiments, high degree centralization causedunfocused linking degrades AWCs performance even solving randomlygenerated, possibly unsatisfiable, graph instances.559fiMailler & Lesser800APO700AWCCycles60050040030020010001.822.22.42.62.8DensityFigure 17: Number cycles needed solve completely random 60 variable problemsvarious density using AWC APO.Density1.82.02.12.32.52.72.9OverallAPOMean49.8888.77116.79116.4156.2127.6217.74APOStDev41.9883.79107.21264.6545.1225.6613.69% APOSolved100100100100100100100AWCMean52.51189.42377.54660.65640.66537.99476.20AWCStDev77.06237.17364.55362.80335.65324.87271.63% AWCSolved100968055658392p(AW C AP O)0.620.000.000.000.000.000.000.00Table 7: Number cycles needed solve completely random 60 variable problemsvarious density using AWC APO.560fiAsynchronous Partial Overlay: New Algorithm DCSP1.4e+06APOAWC1.2e+06Messages1e+0680000060000040000020000001.822.22.42.62.8DensityFigure 18: Number messages needed solve completely random 60 variable problemsvarious density using AWC APO.5.2.2 Random Graphssecond set experiments, generated completely random 60 node graphsaverage degrees = 1.8 2.9. series conducted test completenessalgorithms, verify correctness implementations, study effectsphase transition performance. value d, generated 200 randomgraphs single set initial values. Graphs generate randomly choosingtwo nodes connecting them. edge already existed, another pair chosen.phase transition curve instances seen figure 16.total, 1400 graphs generated tested. Due time constraints, stoppedexecution AWC 1000 cycles completed (APO never reached 1000).results experiments shown figures 17 18 tables 7 8.graphs, APO significantly outperforms AWC simplest problems(see figure 17). results directly attributed AWCs poor performanceunsatisfiable problem instances (Fernandez et al., 2003). fact, regionphase transition, AWC unable complete 45% graphs within 1000 cycles.addition, solve problems, AWC uses least order magnitudemessages APO. results seen figure 18. looking table 9, easysee occurs. AWC high degree linking centralization. fact,= 2.9 graphs, AWC reaches average 93% centralization 75% completeinter-agent linking.contrast this, APO loose linking throughout entire phase transitioncentralizes average around 50% entire problem. results encouraging reinforce idea partial overlays extending along critical paths yieldsimprovements convergence solutions.561fiMailler & LesserDensity1.82.02.12.32.52.72.9OverallAPOMean2822.617508.3312642.6815614.378219.744196.582736.20APOStDev3040.399577.8616193.5615761.907415.764201.802286.39AWCMean12741.58126658.29356993.39882813.451080277.251047001.181000217.83AWCStDev47165.70269976.18444899.21566715.73661909.63738367.27699199.90p(AW C AP O)0.000.000.000.000.000.000.000.00Table 8: Number messages needed solve completely random 60 variable problemsvarious density using AWC APO.APOAWCDensity1.82.02.12.32.52.72.91.82.02.12.32.52.72.9% LinksMean8.0910.9313.3115.5614.3713.1913.1216.2834.3446.5264.1370.0672.9575.19% LinksStDev1.503.284.494.943.743.322.706.5813.4413.9511.4310.5110.659.62% CentralMean26.1936.9246.6855.9153.8647.3345.2641.0865.0075.2486.0689.4691.7192.78% CentralStDev6.9413.7715.8618.1818.2519.5917.5512.4214.4110.665.614.553.805.63Table 9: Link statistics 60 node random problems.5.2.3 Runtime Teststhird set experiments, directly compared serial runtime performanceAWC APO. Serial runtime measured using following formula:serialtime =cyclesXXi=0 aagents562time(a, i)fiSecondsAsynchronous Partial Overlay: New Algorithm DCSP23900141608390497029401740103061036021012070402010APOAWCBacktracking152535455565NodesSecondsFigure 19: Comparison number seconds needed solve random, low-density 3coloring problems various sizes AWC, APO, centralized Backtracking.92350516802892016180905050602830158088049027015080402010APOAWCBacktracking152535455565NodesFigure 20: Comparison number seconds needed solve random, medium-density3-coloring problems various sizes AWC, APO, centralized Backtracking.563fiSecondsMailler & Lesser362023601540100065042027017011070402010APOAWCBacktracking152535455565NodesFigure 21: Comparison number seconds needed solve random, high-density 3coloring problems various sizes AWC, APO, centralized Backtracking.2.02.32.7Nodes153045601530456015304560APOMean0.260.781.272.020.180.982.1910.510.090.400.665.47APOStDev0.210.661.181.740.230.882.2612.970.060.390.635.44AWCMean2.7210.52257.88890.123.96112.951236.2732616.243.5161.43460.564239.16AWCStDev4.4018.991252.254288.433.23144.241777.5162111.522.9059.77690.984114.30BTMean0.021.65245.1427183.640.030.86150.7392173.710.020.2935.582997.04BTStDev0.024.14587.3156053.260.011.07241.02222327.590.010.3657.754379.97Table 10: Comparison number seconds needed solve random 3-coloring problemsvarious sizes densities using AWC, APO, centralized Backtracking.564fiAsynchronous Partial Overlay: New Algorithm DCSPtotal accumulated runtime needed solve problem oneagent allowed process time.experiments, generated random graphs, time varying sizedensity graph. generated 25 graphs values n = 15, 30, 45, 60densities = 2.0, 2.3, 2.7, total 300 test cases. show performancedifference APO AWC caused speed central solver, rancentralized backtracking algorithm graph instances. Although, APO usesbranch bound algorithm, backtracking algorithm used test provides bestcase lower bound runtime APOs internal solver.programs used test run identical 2.4GHz Pentium 4768 Mbytes RAM. machines entirely dedicated testsminimal amount interference competing processes. addition, computational costassigned message passing simulator passes messages cycles.algorithms were, however, penalized amount time took process messages.Although realize specific implementation algorithm greatly effectruntime performance, every possible effort made optimize AWC implementationused experiments effort fair.results test series seen figures 19, 20, 21. notescale used graphs logarithmic. looking results, twothings become apparent. Obviously, first APO outperforms AWCevery case. Second, APO actually outperforms centralized solver graphs larger45 nodes. indicates two things. First, solver currently APOpoor second APOs runtime performance direct result speedcentralized solver using. fact, tests show improved performanceAPO AWC caused APOs ability take advantage problems structure.replace centralized solver used tests state-of-the-artsolver, would expect two things. first would expect serial runtimeAPO algorithm decrease simply speedup caused centralized solver.second, importantly, centralized solver would always outperformAPO. current CSP solvers take advantage problem structure unlikesolver used tests. way making claim APO improvescentralized solver. simply stating APO outperforms AWC reasonsspeed current internal solver.5.3 Tracking Domaintest APOs adaptability various centralized solvers, created implementationcomplete-compatibility version SensorDCSP formulation (Bejar et al., 2001;Krishnamachari, Bejar, & Wicker, 2002; Fernandez et al., 2003). domain,number sensors number targets randomly placed within environment.range restrictions, sensors within distance dist seetarget. goal find assignment sensors targets targetthree sensors tracking it.Following directly definition CSP, SensorDCSP problem consistsfollowing:565fiMailler & LesserFigure 22: example tracking problem. 30 targets (labeledname) 224 sensors (black dots). Lines connecting sensors targets indicate sensor assigned tracking target.566fiAsynchronous Partial Overlay: New Algorithm DCSPset n targets = {T1 , . . . , Tn }.set possible sensors see targets = {D 1 , . . . , Dn }.set constraints R = {R1 , . . . , Rm } Ri (ai , aj ) predicate implements intersects relationship. predicate returns true iff sensorsassigned Ti elements common sensors assigned Tj .problem find assignment = {a1 , . . . , } constraintsR satisfied ai set |Dci | sensors Di c = min(|Di |, 3).indicates target requires 3 sensors, enough available, sensors,less 3.Since, implementation, sensors compatible one another,overall complexity problem polynomial, using reduction feasible flowbipartite graph(Krishnamachari, 2002). this, centralized solver usedAPO agents changed modified version Ford-Fulkerson maximum flowalgorithm (Ford & Fulkerson, 1962; Cormen, Leiserson, & Rivest, 1999),proven run polynomial time.example tracking problem seen figure 22. example,224 sensors (black dots) placed ordered pattern environment.30 targets (labeled names) randomly placed startup. linesconnecting sensors targets indicate sensor assigned target. Noteinstance problem satisfiable.5.3.1 Modifying APO Tracking Domaintracking domain closely related general CSP formulation,changes made either AWC APO tests. did, however, decide testadaptability APO new centralized problem solver. this, changedcentralized problem solver Ford Fulkerson max-flow algorithm figure 23. FordFulkerson works repeatedly finding paths remaining capacity residualflow network augmenting flows along paths. algorithm terminatesadditional paths found. detailed explanation algorithm well proofoptimality found (Cormen et al., 1999).Like mapping bipartite graphs max-flow, SensorDCSP problem also easilymapped max-flow. figures 24 25 see mapping simple sensorallocation problem max-flow problem. Notice capacity flowsensors targets 1. ensures sensor cannot usedsingle target. Also, notice capacity targets 3. fact, valuemin(|Di |, 3).use algorithm within APO, mediator simply translates problemnetwork flow graph G using following rules whenever runs choose solutionprocedure figure 5:1. Add nodes G.2. Ti add node Ti edge (Ti , t) capacity min(|Di |, 3) G.567fiMailler & LesserFord-Fulkerson (G, s, t)edge (u, v) E[G]f [u, v] 0;f [v, u] 0;end do;exists path presidual network Gfcf (p) min{cf (u, v) : (u, v) p};edge (u, v) pf [u, v] f [u, v] + cf (p);f [v, u] f [u, v];end do;end do;end Ford-Fulkerson;Figure 23: Ford-Fulkerson maximum flow algorithm.S1S2T1S3S4T2S5S6Figure 24: simple sensor target allocation problem.3. unique sensor Si domains Ti , add node Si , edge (s, Si )capacity 1, edge (Si , Ti ) capacity 1 G.568fiAsynchronous Partial Overlay: New Algorithm DCSPS111S211S3111T13113S41S511T21S6Figure 25: flow network simple target allocation problem figure 24.executes Ford-Fulkerson algorithm. algorithm finishes, mediatorchecks residual capacity edges targets t. edgesresidual flow, problem unsatisfiable. Otherwise, assignment derivedfinding (Si , Ti ) edges flow 1.One nicest characteristics Ford-Fulkerson algorithm works regardless order paths residual network chosen. implementation,used breadth-first search which, addition identifying paths residual network, minimized cost path. Cost sense refers amount externalconflict created sensor assigned target. modification maintainsmin-conflict heuristic integral part extending mediators local view.5.3.2 Resultstest APO AWC domain, ran test series used 200f 200fenvironment 224 sensors placed ordered grid-based pattern. chose placesensors ordered fashion reduce variance obtained within results. rantest series varied sensor target ratio 10:1 3.8:1 (22 59 targets)increments 0.2 across spectrum mostly satisfiable mostly unsatisfiableinstances (see figure 26). conducted 250 trial runs random target placementvalues get good statistical sampling.total, 6750 test cases used. comparison, measured number messagescycles taken algorithms find solution. random seeds usedplace targets saved, APO AWC tested using identical problem569fiMailler & Lesser0.90.8% Satisfiable0.70.60.50.40.30.20.10202530354045505560TargetsFigure 26: Phase transition curve 224 sensor environment used testing.APOAWCCycles20151050202530354045505560TargetsFigure 27: Number cycles needed solve random target configurations field 224sensors using AWC APO.instances. correctness algorithms verified cross-checking solutions(satisfiable/unsatisfiable) obtained tests, matched identically.seen figure 27 28 tables 11 12, APO outperforms AWCsimplest cases. Part reason minimum 3 cycles takes APOfinish mediation session. problems sparsely connected interdependencies,cost tends dominate. All-in-all, T-tests indicate, APO significantly betterAWC terms cycles completion number messages used problemsdomain.570fiAsynchronous Partial Overlay: New Algorithm DCSPTargets222324252627282930313233343536373940414345474951535659OverallAPOMean6.366.657.126.556.807.097.387.107.557.186.887.627.477.568.087.487.556.457.455.964.805.154.533.524.123.143.28APOStDev2.333.394.723.244.285.025.884.895.996.116.317.597.817.499.898.3810.8710.5413.117.797.258.315.902.005.820.592.26AWCMean5.887.328.837.159.6510.8511.059.2413.1512.4211.7312.3215.8814.7415.7020.7016.1215.7417.5616.1017.6118.5215.3314.3413.1310.457.46AWCStDev6.6110.5519.9610.5615.7819.8915.8913.7625.5822.2218.2124.0428.8229.0125.4639.6326.4321.6630.9822.9128.5030.2725.2822.6022.5520.8110.47p(AW C AP O)0.290.360.180.390.010.000.000.020.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00Table 11: Number cycles needed solve random target configurations field 224sensors using AWC APO.571fiMailler & LesserTargets222324252627282930313233343536373940414345474951535659OverallAPOMean78.2889.52105.53102.92116.36128.58149.23145.72167.50169.30174.32212.59218.74221.93258.41258.95303.64293.24342.33274.39277.26311.91303.66269.37333.42296.36339.21APOStDev32.5854.0190.3657.1886.5898.63144.8193.24144.27152.83152.89237.60246.18230.44354.13342.97501.10649.42724.64267.95414.19405.1299.13110.57390.0845.54202.98AWCMean95.68133.12184.19149.18245.99263.32279.49231.98378.89404.40362.63410.05811.58613.64671.00947.95815.32884.32912.651279.971334.381471.821487.651571.461804.471895.231765.18AWCStDev133.53237.74616.36298.85566.03587.30493.05335.98874.99997.82570.41923.252243.791422.331333.502116.981373.291407.991517.102194.842470.132172.132503.622157.522815.943731.983676.16p(AW C AP O)0.040.000.040.020.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00Table 12: Number messages needed solve random target configurations targetsfield 224 sensors using AWC APO.572fiAsynchronous Partial Overlay: New Algorithm DCSPAPOMessages2000AWC150010005000202530354045505560TargetsFigure 28: Number messages needed solve random target configurations targetsfield 224 sensors using AWC APO.6. Conclusions Future Directionsarticle, presented new complete, distributed constraint satisfaction protocolcalled Asynchronous Partial Overlay (APO). Like AWC, APO allows agents retainautonomy obscure completely hide internal variables constraints. addition, agents refuse solutions posed mediator, instead takingmediator reason unhappy proposed solution. alsopresented example execution simple problem (section 4.2) provedsoundness completeness algorithm (section 4.3). extensive empiricaltesting 10,250 graph instances graph coloring tracking domain, alsoshowed APO significantly outperforms currently best known distributed constraintsatisfaction algorithm, AWC (Yokoo, 1995). tests shown APO betterAWC terms cycles completion, message usage, runtime performance.also shown runtime characteristics directly attributed speedcentralized solver.APOs performance enhancements attributed number things. First,APO exhibits hill-climbing nature early search becomes focusedcontrolled time goes on. Like hill-climbing techniques often leads satisfiablesolution early search. Second, using partial overlaying informationagents use decision making, APO exploits work previously donemediators. forms lock key mechanism promotes solution stability.Lastly, importantly, APO uses dynamic, partial centralization, agentswork smaller, highly relevant portions overall problem. identifying areasdecomposability, search space greatly reduced which, cases, improvesefficiency centralized search algorithm.vast number improvements planned APO future. Probablyimportant improve centralized solver uses. article, inefficientsolver chosen show strengths distributed portions APO. expect573fiMailler & Lesseradditional improvements algorithms runtime performance obtainedusing faster centralized search engine. addition, modern solvers often use methodslike graph reductions, unit propagation backbone guided search. conceivableinformation gained centralized search engine could used prune domainsvariables consistency reasons variables centralized subproblemrelevance reasons. expect focus efforts agents additionallyreducing search time communications usage algorithm.Along improvements selective use memory recording nogoods.Unlike AWC uses nogoods ensure complete search, APOs completeness reliesone agents centralizing entire problem worst case. keydifference, APO improved simply remembering small, powerful subsetnogoods discovers mediation session session. would allow algorithmimprove future search exploiting work done previously.clear APO, cooperative mediation methodologywhole, opens new areas future exploration new questions answereddistributed problem solving. believe work shows great deal promiseaddressing vast number problems represents bridge centralizeddistributed problem solving techniques.AcknowledgmentsSpecial thanks Bryan Horling design implementation Farm simulationenvironment experiment run Shlomo Zilberstein, Bart Selman,Neil Immerman, Jose Vidal making numerous suggestions developmentwork. Lastly, authors would like thank JAIR reviewers helpfulfeedback suggestions Carlos Ansotegui Jean-Charles Regin lengthydiscussion final revision article.effort represented paper sponsored Defense Advanced Research Projects Agency (DARPA) Air Force Research Laboratory, Air Force MaterielCommand, USAF, agreement number F30602-99-2-0525. views conclusionscontained herein authors interpreted necessarily representing official policies endorsements, either expressed implied, DefenseAdvanced Research Projects Agency (DARPA), Air Force Research Laboratory, U.S.Government. U.S. Government authorized reproduce distribute reprintsGovernmental purposes notwithstanding copyright annotation thereon.ReferencesBejar, R., Krishnamachari, B., Gomes, C., & Selman, B. (2001). Distributed constraintsatisfaction wireless sensor tracking system. Workshop Distributed Constraint Reasoning, International Joint Conference Artificial Intelligence, Seattle,Washington.Cammarata, S., McArthur, D., & Steeb, R. (1983). Strategies cooperation distributedproblem solving. Proceedings 8th International Joint Conference Artificial574fiAsynchronous Partial Overlay: New Algorithm DCSPIntelligence (IJCAI-83), Vol. 2, pp. 767770.Cha, B., & Iwana, K. (1996). Adding new clauses faster local search. ProceedingsThirteenth National Conference Artificial Intelligence (AAAI), pp. 332337.Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). really hard problems are.Proceedings 12th International Joint Conference Artificial Intelligence(IJCAI-91), pp. 331337.Conry, S. E., Kuwabara, K., Lesser, V. R., & Meyer, R. A. (1991). Multistage negotiationdistributed constraint satisfaction. IEEE Transactions Systems, Man,Cybernetics, 21 (6).Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1999). Introduction Algorithms.McGraw-Hill.Culberson, J., & Gent, I. (2001). Frozen development graph coloring. Theoretical Computer Science, 265 (12), 227264.Fernandez, C., Bejar, R., Krishnamachari, B., Gomes, C., & Selman, B. (2003). DistributedSensor Networks: Multiagent Perspective, chap. Communication ComputationDistributed CSP Algorithms, pp. 299317. Kluwer Academic Publishers.Ford, L. R., & Fulkerson, D. (1962). Flows Networks. Princeton University Press.Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58 (13), 2170.Frost, D., & Dechter, R. (1994). Dead-end driven learning. Proceedings TwelfthNatioanl Conference Artificial Intelligence, pp. 294300.Ginsberg, M. L. (1993). Dynamic backtracking. Journal Artificial Intelligence Research,1, 2546.Hayden, S., Carrick, C., & Yang, Q. (1999). Architectural design patterns multi-agentcoordination. Proceedings International Conference Agent Systems, Seattle, WA.Hirayama, K., & Yokoo, M. (2000). effect nogood learning distributed constraintsatisfaction. 20th International Conference Distributed Computing Systems(ICDCS), pp. 169177.Krishnamachari, B., Bejar, R., & Wicker, S. (2002). Distributed problem solvingboundaries self-configuration multi-hop wireless networks. Hawaii International Conference System Sciences (HICSS-35).Krishnamachari, B. (2002). Phase Transitions, Structure, Compleixty Wireless Networks. Ph.D. thesis, Cornell University, Ithaca, NY.Mammen, D. L., & Lesser, V. R. (1998). Problem Structure Subproblem SharingMulti-Agent Systems. Third International Conference Multi-Agent Systems, 174181.Merriam-Webster (Ed.). (1995). Merriam-Webster Dictionary (Home Office edition). Springfield, IL.575fiMailler & LesserMinton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts:heuristic repair method constraint satisfaction scheduling problems. ArtificialIntelligence, 58 (1-3), 161205.Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computational complexity characteristic phase transitions. Nature, 400,133137.Rao, V. N., & Kumar, V. (1993). efficiency parallel backtracking. IEEE Transactions Parallel Distributed Systems, 4 (4), 427437.Sycara, K., Roth, S., Sadeh, N., & Fox, M. (1991). Distributed constrained heuristic search.IEEE Transactions Systems, Man, Cybernetics, 21 (6), 14461461.Sycara, K. (1988). Resolving goal conflicts via negotiation. Proceedings SeventhNational Conference Artificial Intelligence, pp. 245250.Wellman, M., & Walsh, W. (1999). Distributed quiescence detection multiagent negotiation. AAAI-99 Workshop Negotiation: Settling Conflicts IdentifyingOpportunities.Werkman, K. J. (1990). Knowledge-based model negotiation using shared perspectives.Proceedings 10th International Workshop Distributed Artificial intelligence,Bandera, TX.Yokoo, M. (1994). Weak-commitment search solving constraint satisfaction problems.Proceedings 12th National Conference Artificial Intelligence (AAAI-94);Vol. 1, pp. 313318, Seattle, WA, USA. AAAI Press, 1994.Yokoo, M. (1995). Asynchronous weak-commitment search solving distributed constraintsatisfaction problems.. Proceedings First International Conference Principles Practice Constraint Programming (CP-95), Lecture Notes ComputerScience 976, pp. 88102. Springer-Verlag.Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction formalizing distributed problem solving. International ConferenceDistributed Computing Systems, pp. 614621.Yokoo, M., & Hirayama, K. (1996). Distributed breakout algorithm solving distributedconstraint satisfaction problems.. International Conference Multi-Agent Systems(ICMAS).Yokoo, M., & Hirayama, K. (2000). Algorithms distributed constraint satisfaction:review. Autonomous Agents Multi-Agent Systems, 3 (2), 198212.Yokoo, M., Suzuki, K., & Hirayama, K. (2002). Secure distributed constraint satisfaction: Reaching agreement without revealing private information. ProceedingEighth International Conference Principles Practice Constraint Programming (CP).576fiJournal Artificial Intelligence Research 25 (2006) 349-387Submitted 06/05; published 03/06Representing Conversations Scalable OverhearingGery GutnikGal A. Kaminkagutnikg@cs.biu.ac.ilgalk@cs.biu.ac.ilComputer Science DepartmentBar Ilan UniversityRamat Gan 52900, IsraelAbstractOpen distributed multi-agent systems gaining interest academic communityindustry. open settings, agents often coordinated using standardizedagent conversation protocols. representation protocols (for analysis, validation, monitoring, etc) important aspect multi-agent applications. Recently, Petrinets shown interesting approach representation, radicallydifferent approaches using Petri nets proposed. However, relative strengthsweaknesses examined. Moreover, scalability suitabilitydifferent tasks addressed. paper addresses challenges. First,analyze existing Petri net representations terms scalability appropriateness overhearing, important task monitoring open multi-agent systems. Then,building insights gained, introduce novel representation using Colored Petrinets explicitly represent legal joint conversation states messages. representation approach offers significant improvements scalability particularly suitableoverhearing. Furthermore, show new representation offers comprehensive coverage conversation features FIPA conversation standards. also presentprocedure transforming AUML conversation protocol diagrams (a standard humanreadable representation), Colored Petri net representation.1. IntroductionOpen distributed multi-agent systems (MAS) composed multiple, independently-builtagents carry mutually-dependent tasks. order allow inter-operability agentsdifferent designs implementation, agents often coordinate using standardized interaction protocols, conversations. Indeed, multi-agent community investingsignificant effort developing standardized Agent Communication Languages (ACL) facilitate sophisticated multi-agent systems (Finin, Labrou, & Mayfield, 1997; Kone, Shimazu,& Nakajima, 2000; ChaibDraa, 2002; FIPA site, 2003). standards define communicative acts, top them, interaction protocols, ranging simple queriesstate another agent, complex negotiations auctions bidding contracts.instance, FIPA Contract Net Interaction Protocol (FIPA Specifications, 2003b) definesconcrete set message sequences allows interacting agents use contractnet protocol negotiations.Various formalisms proposed describe standards (e.g., Smith & Cohen,1996; Parunak, 1996; Odell, Parunak, & Bauer, 2000, 2001b; AUML site, 2003). particular, AUMLAgent Unified Modelling Languageis currently used FIPA-ACL standardsc2006AI Access Foundation. rights reserved.fiGutnik & Kaminka(FIPA Specifications, 2003a, 2003b, 2003c, 2003d; Odell, Parunak, & Bauer, 2001a) 1 . UML2.0 (AUML site, 2003), new emerging standard influenced AUML, potentialbecome FIPA-ACL standard (and forthcoming IEEE standard) future. However, moment, large set FIPA specifications remains formalized using AUML.AUML intended human readability visualization, interaction protocolsideally represented way amenable automated analysis, validationverification, online monitoring, etc.Lately, increasing interest using Petri nets (Petri Nets site, 2003) modellingmulti-agent interaction protocols (Cost, 1999; Cost, Chen, Finin, Labrou, & Peng, 1999,2000; Lin, Norrie, Shen, & Kremer, 2000; Nowostawski, Purvis, & Cranefield, 2001; Purvis,Hwang, Purvis, Cranefield, & Schievink, 2002; Cranefield, Purvis, Nowostawski, & Hwang,2002; Ramos, Frausto, & Camargo, 2002; Mazouzi, Fallah-Seghrouchni, & Haddad, 2002;Poutakidis, Padgham, & Winikoff, 2002). broad literature using Petri netsanalyze various aspects distributed systems (e.g. deadlock detection shownKhomenco & Koutny, 2000), recent work specific uses Petri netsmulti-agent systems, e.g., validation testing (Desel, Oberweis, & Zimmer, 1997),automated debugging monitoring (Poutakidis et al., 2002), dynamic interpretationinteraction protocols (Cranefield et al., 2002; de Silva, Winikoff, & Liu, 2003), modellingagents behavior induced participation conversation (Ling & Loke, 2003)interaction protocols refinement allowing modular construction complex conversations(Hameurlain, 2003).However, key questions remain open use Petri nets conversation representation. First, radically different approaches representation using Petri netsproposed, relative strengths weaknesses investigated. Second,many investigations addressed restricted subsets features needed representing complex conversations standardized FIPA (see detailed discussionprevious work Section 2). Finally, procedures proposed translatinghuman-readable AUML protocol descriptions corresponding machine-readable Petrinets.paper addresses open challenges context scalable overhearing. Here,overhearing agent passively tracks many concurrent conversations involving multiple participants, based solely exchanged messages, participantoverheard conversations (Novick & Ward, 1993; Busetta, Serafini, Singh, & Zini,2001; Kaminka, Pynadath, & Tambe, 2002; Poutakidis et al., 2002; Busetta, Dona, & Nori,2002; Legras, 2002; Gutnik & Kaminka, 2004a; Rossi & Busetta, 2004). Overhearing useful visualization progress monitoring (Kaminka et al., 2002), detecting failuresinteractions (Poutakidis et al., 2002), maintaining organizational situational awareness (Novick & Ward, 1993; Legras, 2002; Rossi & Busetta, 2004) non-obtrusivelyidentifying opportunities offering assistance (Busetta et al., 2001, 2002). instance,overhearing agent may monitor conversation contractor agent engaged multiplecontract-net protocols different bidders bid callers, order detect failures.begin analysis Petri net representations, respect scalabilityoverhearing. classify representation choices along two dimensions affecting scalability:1. (FIPA Specifications, 2003c) currently deprecated. However, use specification since describesmany important features needed modelling multi-agent interactions.350fiRepresenting Conversations Scalable Overhearing(i) technique used represent multiple concurrent conversations; (ii) choicerepresenting either individual joint interaction states. show runtime complexity monitoring conversations using different approaches same, choicesalong two dimensions significantly different space requirements, thuschoices scalable (in number conversations) others. also arguerepresentations suitable overhearing require use explicit message places, thoughsubset previously-explored techniques utilized those.Building insights gained, paper presents novel representation usesColored Petri nets (CP-nets) places explicitly denote messages, valid jointconversation states. representation particularly suited overhearing numberconversations scaled-up. show representation used representessentially features FIPA AUML conversation standards, including simple complex interaction building blocks, communicative act attributes message guardscardinalities, nesting, temporal aspects deadlines duration.realize advantages machine-readable representations, debugging(Poutakidis et al., 2002), existing human-readable protocol descriptions must convertedcorresponding Petri net representations. final contribution paper,provide skeleton semi-automated procedure converting FIPA conversation protocolsAUML Petri nets, demonstrate use complex FIPA protocol.procedure fully automated, takes first step towards addressing open challenge.paper organized follows. Section 2 presents motivation work.Sections 3 6 present proposed representation addressing FIPA conversation features including basic interaction building blocks (Section 3), message attributes(Section 4), nested & interleaved interactions (Section 5), temporal aspects (Section 6).Section 7 ties features together: presents skeleton algorithm transformingAUML protocol diagram Petri net representation, demonstrates use challenging FIPA conversation protocol. Section 8 concludes. paper rounds threeappendixes. first provides quick review Petri nets. Then, complete coverageFIPA interactions, Appendix B provides additional interaction building blocks. Appendix Cpresents Petri net complex conversation protocol, integrates many featuresdeveloped representation technique.2. Representations Scalable OverhearingOverhearing involves monitoring conversations progress, tracking messagesexchanged participants (Gutnik & Kaminka, 2004a). interested representations facilitate scalable overhearing, tracking many concurrent conversations,many agents. focus open settings, complex internal state control logic agents known advance, therefore exclude discussions Petri netrepresentations explicitly model agent internals (e.g., Moldt & Wienberg, 1997; Xu& Shatz, 2001). Instead, treat agents black boxes, consider representationscommit agents conversation state (i.e., role progress conversation).suitability representation scalable overhearing affected several facets.First, since overhearing based tracking messages, representation must ableexplicitly represent passing message (communicative act) one agent another351fiGutnik & Kaminka(Section 2.1). Second, representation must facilitate tracking multiple concurrentconversations. tracking runtime bounded number messages(since case, messages overheard processed), space requirements may differsignificantly (see Sections 2.22.3).2.1 Message-monitoring versus state-monitoringdistinguish two settings tracking progress conversations, dependinginformation available tracking agent. first type setting, referstate monitoring, tracking agent access internal state conversationone participants, necessarily messages exchanged.settings involves message monitoring, tracking agent accessmessages exchanged (which externally observable), cannot directly observeinternal state conversation participant. Overhearing form messagemonitoring.Representations support state monitoring use places denote conversationstates participants. Tokens placed places (the net marking) denotecurrent state. sending receiving message participant explicitlyrepresented, instead implied moving tokens (through transition firings) newstate places. Thus, representation essentially assumes internal conversationstate participants directly observable monitoring agent. Previous work utilizingstate monitoring includes work Cost (1999), Cost et al. (1999, 2000), Lin et al. (2000),Mazouzi et al. (2002), Ramos et al. (2002).representation present paper intended overhearing tasks, cannotassume conversation states overheard agents observable. Instead, mustsupport message monitoring, addition using tokens state places (to denotecurrent conversation state), representation uses message places, tokens placedcorresponding message overheard. conversation-state place messageplace connected via transition state place denoting new conversation state.Tokens placed originating placesindicating message received appropriateconversation statewill cause transition fire, tokens placednew conversation state place. Thus new conversation state inferred "observing"message. Previous investigations, used explicit message places, include workCost (1999), Cost et al. (1999, 2000), Nowostawski et al. (2001), Purvis et al. (2002),Cranefield et al. (2002), Poutakidis et al. (2002)2 . discussed depth below.2.2 Representing Single ConversationTwo representation variants popular within utilize conversation places (inaddition message places): Individual state representations use separate places tokensstate participant (each role). Thus, overall state conversationrepresented different tokens marking multiple places. Joint state representations usesingle place joint conversation state participants. placement token2. Cost (1999), Cost et al. (1999, 2000) present examples state- message- monitoring representations.352fiRepresenting Conversations Scalable Overhearingwithin place represents overhearing agents belief participantsappropriate joint state.previous representations use individual states. these, different markings distinguish conversation state one agent sent message, stateagent received it. net conversation role essentially built separately,merged nets, connected via fusion places similar means.Cost (1999), Cost et al. (1999, 2000) used CP-nets individual state placesrepresenting KQML FIPA interaction protocols. Transitions represent message events,CP-net features, token colors arc expressions, used represent AUMLmessage attributes sequence expressions. authors also point deadlines (atemporal aspect interaction) modelled, implementation details provided.Cost (1999) also proposed using hierarchical CP-nets represent hierarchical multi-agentconversations.Purvis et al. (2002), Cranefield et al. (2002) represented conversation roles separateCP-nets, places denote interaction messages states, transitions represent operations performed corresponding communicative acts send, receive,process. Special in/out places used pass net tokens different CP-nets,special get/put transitions, simulating actual transmission correspondingcommunicative acts.principle, individual-state representations require two places role, everymessage. given message, would two individual places sender (beforesending sending), similarly two receiver (before receivingreceiving). possible conversation statesvalid notcan represented.single message two roles, two places role (four places total), fourpossible conversation states: message sent received, sent received, sentincorrectly believed received, sent received. statesrepresented different markings. instance, conversation state messagesent received denoted token after-sending place senderanother token before-receiving place receiver. summarizedfollowing proposition:Proposition 1 Given conversation R roles total possible messages,individual state representation space complexity O(M R).representations represent roles conversation state separately,many applications overhearing require representation valid conversation states(message sent received, sent received). Indeed, specifications interaction protocols often assume use underlying synchronization protocols guaranteedelivery messages (Paurobally & Cunningham, 2003; Paurobally, Cunningham, & Jennings, 2003). assumption, every message, two joint statesregardless number roles. example, single message three rolesasender two receivers, two places two possible markings: tokensending/receiving place represents conversation state messageyet sent sender (and two receivers waiting it), tokensending/receiving place denotes message sent receivedreceivers.353fiGutnik & KaminkaNowostawski et al. (2001) utilize CP-nets places denote joint conversation states.also utilize places representing communicative acts. Poutakidis et al. (2002) proposedrepresentation based Place-Transition nets (PT-nets)a restricted representationPetri nets color. presented several interaction building blocks,could fit together model additional conversation protocols. general, followingproposition holds respect representations:Proposition 2 Given conversation R roles total possible messages,joint state representation represents legal states space complexity O(M ).condition representing valid states critical complexity analysis.joint conversation statesvalid invalidare represented, space complexity wouldO(M R ). case, individual-state representation would advantage.would case, instance, assume use synchronization protocols,e.g., overhearing agent may wish track exact system state evenmessage underway (i.e., sent yet received).2.3 Representing Multiple Concurrent ConversationsPropositions 1 2 address space complexity representing single conversation. However, large scale systems overhearing agent may required monitormultiple conversations parallel. instance, overhearing agent may monitoringmiddle agent carrying multiple parallel instances single interaction protocolmultiple partners, e.g., brokering (FIPA Specifications, 2003a).previous investigations propose duplicate appropriate Petri net representation monitored conversation (Nowostawski et al., 2001; Poutakidis et al., 2002).approach, every conversation tracked separate Petri-net, thus numberPetri nets (and associated tokens) grows number conversations (Proposition 3). instance, Nowostawski et al. (2001) shows example contract-netprotocol carried three different contractors, using three duplicate CP-nets.captured following proposition:Proposition 3 representation creates multiple instances conversation Petri netrepresent C conversations, requires O(C) net structures, O(C) bits tokens.investigations take different approach, single CP-net structure usedmonitor conversations protocol. tokens associated conversationsdifferentiated token color (Cost, 1999; Cost et al., 1999, 2000; Lin et al., 2000;Mazouzi et al., 2002; Cranefield et al., 2002; Purvis et al., 2002; Ramos et al., 2002).example, assigning token color tuple type hsender, receiveri, agentdifferentiate multiple tokens place thus track conversations different pairsagents3 . Color tokens use multiple bits per token; log C bits required differentiate C conversations. Therefore, number bits required track C conversationsusing C tokens C log C. leads following proposition.3. See Section 4 distinguish different conversations agents.354fiRepresenting Conversations Scalable OverhearingProposition 4 representation uses color tokens represent C multiple instancesconversation, requires O(1) net structures, O(C log C) bits tokens.Due constants involved, space requirements Proposition 3 practicemuch expensive Proposition 4. Proposition 3 refers creationO(C) Petri networks, duplicated place transition data structures. contrast,Proposition 4 refers bits required representing C color tokens single CP net.Moreover, practical settings, sufficiently large constant bound numberconversations may found, essentially reduce O(log C) factor O(1).Based Propositions 14, possible make concrete predictions scalabilitydifferent approaches respect number conversations, roles. Table 1 showsspace complexity different approaches modelling C conversationsprotocol, maximum R roles, messages, assumptionunderlying synchronization protocols. table also cites relevant previous work.IndividualStates(Proposition 1)JointStates(Proposition 2)Representing Multiple Conversations (of Protocol)Multiple CP- PT-netsUsing color tokens, single CP-net(Proposition 3)(Proposition 4)Space: O(M R + C log C)Cost (1999), Cost et al. (1999, 2000),Space: O(M RC)Lin et al. (2000), Cranefield et al. (2002),Purvis et al. (2002), Ramos et al. (2002),Mazouzi et al. (2002)Space: O(M C)Space: O(M + C log C)Nowostawski et al. (2001),paperPoutakidis et al. (2002)Table 1: Scalability different representationsBuilding insights gained Table 1, propose representation using CP-netsplaces explicitly represent joint conversation states (corresponding lower-rightcell Table 1), tokens color used distinguish concurrent conversations (asupper-right cell Table 1). such, related works features,table demonstrates, novel synthesis.representation uses similar structures found works Nowostawskiet al. (2001) Poutakidis et al. (2002). However, contrast previous investigations, rely token color CP-nets model concurrent conversations, spacecomplexity O(M + C log C). also show (Sections 36) used covervariety conversation features covered investigations. features includerepresentation full set FIPA interaction building blocks, communicative act attributes(such message guards, sequence expressions, etc.), compact modelling concurrent conversations, nested interleaved interactions, temporal aspects.355fiGutnik & Kaminka3. Representing Simple & Complex Interaction Building Blockssection introduces fundamentals representation, demonstrates various simple complex AUML interaction messages, used FIPA conversation standards(FIPA Specifications, 2003c), implemented using proposed CP-net representation. begin simple conversation, shown Figure 1-a using AUML protocoldiagram. Here, agent1 sends asynchronous message msg agent2 .fffifffifffifffi(a) AUML representationfi !"# $ %&' (%)'* " # $' ( '%+'(b) CP-net representationFigure 1: Asynchronous message interaction.represent agent conversation protocols, define two types places, correspondingmessages conversation states. first type net places, called message places,used describe conversation communicative acts. Tokens placed message places indicateassociated communicative act overheard. second type net places,agent places, associated valid joint conversation states interacting agents.Tokens placed agent places indicate current joint state conversation withininteraction protocol.Transitions represent transmission receipt communicative acts agents.Assuming underlying synchronization protocols, transition always originates within jointstate place message place, targets joint conversation state (more onepossiblesee below). Normally, current conversation state known (markedtoken), must wait overhearing matching message (denoted tokenconnected message place). token marked, transition fires, automaticallymarking new conversation state.Figure 1-b presents CP net representation earlier example Figure 1-a. CPnet Figure 1-b three places one transition connecting them. A1 B1A2 B2 places agent places, msg place message place. B capitalletters used denote agent1 agent2 individual interaction states respectively(we indicated individual joint interaction states AUML diagramFigure 1-a, omit annotations later figures). Thus, A1 B1 place indicatesjoint interaction state agent1 ready send msg communicative act agent2(A1 ) agent2 waiting receive corresponding message (B1 ). msg messageplace corresponds msg communicative act sent two agents. Thus,transmission msg communicative act causes agents transition A2 B2356fiRepresenting Conversations Scalable Overhearingplace. place corresponds joint interaction state agent1 already sentmsg communicative act agent2 (A2 ) agent2 received (B2 ).CP-net implementation Figure 1-b also introduces use token colorsrepresent additional information interaction states communicative acts.token color sets defined net declaration, i.e. dashed box Figure 1-b.syntax follows standard CPN ML notation (Wikstrom, 1987; Milner, Harper, &Tofte, 1990; Jensen, 1997a). AGEN color identifies agents participatinginteraction, used construct two compound color sets.INTER-STATE color set associated agent places, represents agentsappropriate joint interaction states. record ha1 , a2 i, a1 a2 AGENcolor elements distinguishing interacting agents. apply INTER-STATE colorset model multiple concurrent conversations using CP-net. second colorset SG, describing interaction communicative acts associated message places.SG color token record , ar i, ar correspond senderreceiver agents associated communicative act. cases, additional elements,conversation identification, may used. See Section 4 additional details.Figure 1-b, A1 B1 A2 B2 places associated INTER-STATEcolor set, msg place associated SG color set. place color setwritten italic capital letters next corresponding place. Furthermore, user AGEN color type variables denote net arc expressions. Thus, givenoutput arc expression A1 B1 msg places hs, ri, relements agent place token must correspond r elements messageplace token. Consequently, net transition occurs agents messagecorrespond interacting agents. A2 B2 place input arc expression hr, si followingunderlying intuition agent2 going send next interaction communicativeact.Figure 2-a shows AUML representation another interaction building block, synchronous message passing, denoted filled solid arrowhead. Here, msg communicative act sent synchronously agent1 agent2 , meaning acknowledgementmsg communicative act must always received agent1 interaction mayproceed.corresponding CP-net representation shown Figure 2-b. interaction startsA1 B1 place terminates A2 B2 place. A1 B1 place represents jointinteraction state agent1 ready send msg communicative act agent2 (A1 )agent2 waiting receive corresponding message (B1 ). A2 B2 place denotesjoint interaction state, agent1 already sent msg communicative actagent2 (A2 ) agent2 received (B2 ). However, since CP-net diagram representssynchronous message passing, msg communicative act transmission cannot causeagents transition directly A1 B1 place A2 B2 place. therefore defineintermediate A01 B10 agent place. place represents joint interaction state agent2received msg communicative act ready send acknowledgement(B1 ), agent1 waiting acknowledgement (A01 ). Taken together, msgcommunicative act causes agents transition A1 B1 place A01 B10 place,acknowledgement msg message causes agents transitionA01 B10 place A2 B2 place.357fiGutnik & Kaminkafffififffifffifi!"#$% & '( ) ( "* % & ' ( ) ( "+ ( "(a) AUML representation(b) CP-net representationFigure 2: Synchronous message interaction.Transitions typical multi-agent interaction protocols composed interactionbuilding blocks, two presented above. Additional interaction buildingblocks, fairly straightforward (or appeared previous work, e.g., Poutakidiset al., 2002) presented Appendix B. remainder section, present twocomplex interactions building blocks generally common multi-agent interactions:XOR-decision OR-parallel.begin XOR-decision interaction. AUML representation buildingblock shown Figure 3-a. sender agent agent1 either send message msg1agent2 message msg2 agent3 , send msg1 msg2 . non-filleddiamond x inside AUML notation constraint.ff fi!"#$ %&'()' ("#$#*%&!'()' ()'+(,$ %&() (-' . / (ffff(a) AUML representation(b) CP-net representationFigure 3: XOR-decision messages interaction.Figure 3-b shows corresponding CP-net. Again, A, B C capital lettersused denote interaction states agent1 , agent2 agent3 , respectively.358fiRepresenting Conversations Scalable Overhearinginteraction starts A1 B1 C1 place terminates either A2 B2 placeA2 C2 place. A1 B1 C1 place represents joint interaction state agent1 readysend either msg1 communicative act agent2 msg2 communicative act agent3(A1 ); agent2 agent3 waiting receive corresponding msg1 /msg2 message(B1 /C1 ). represent A1 B1 C1 place color set, extend INTER-STATE colorset denote joint interaction state three interacting agents, i.e. using INTERSTATE-3 color set. msg1 communicative act causes agents transition A2 B2place. A2 B2 place represents joint interaction state agent1 sent msg1message (A2 ), agent2 received (B2 ). Similarly, msg2 communicative act causesagents agent1 agent3 transition A2 C2 place. Exclusiveness achieved sincesingle agent token A1 B1 C1 place used either activating A1 B1 C1 A2 B2transition activating A1 B1 C1 A2 C2 transition, both.similar complex interaction OR-parallel messages interaction. AUML representation presented Figure 4-a. sender agent, agent1 , send message msg1agent2 message msg2 agent3 , both. non-filled diamond AUML notationconstraint.fffififffifffifffi(a) AUML representation!"#$%& '()*+) *"#$%&%, '()*+) *+)-*". & '(*+*"/) 01 *"fffi(b) CP-net representationFigure 4: OR-parallel messages interaction.Figure 4-b shows CP-net representation OR-parallel interaction. interaction starts A1 B1 C1 place terminated A2 B2 place,A2 C2 place, both. represent inclusiveness interaction protocol, definetwo intermediate places, A01 B1 place A001 C1 place. A01 B1 place representsjoint interaction state agent1 ready send msg1 communicative act agent2(A01 ) agent2 waiting receive message (B1 ). A001 C1 place similar meaning, respect agent3 . normally done Petri nets, transition connectingA1 B1 C1 place intermediate places duplicates single token A1 B1 C1 placetwo tokens going A01 B1 A001 C1 places. Consequently, two partsOR-parallel interaction independently executed.4. Representing Interaction Attributesextend representation allow additional interaction aspects, useful describing multi-agent conversation protocols. First, show represent interaction359fiGutnik & Kaminkamessage attributes, guards, sequence expressions, cardinalities content (FIPASpecifications, 2003c). explore depth representation multiple concurrentconversations (on CP net).Figure 5-a shows simple agent interaction using AUML protocol diagram.interaction similar one presented Figure 1-a previous section. However,Figure 5-a uses AUML message guard-conditionmarked [condition]thatfollowing semantics: communicative act sent agent1 agent2condition true.fffifffi fffifffi(a) AUML representation! " #$!%& " #$'( ! ! " #$fi ) !*+,! ! " -./0 !1 .2 0 !$3 , " -0 !1 0 !10!%&10'( ! !$4. 0 !$ 4. 0!%&$4. 0'( ! !$(b) CP-net representationFigure 5: Message guard-conditionguard-condition implementation Petri net representation uses transitionguards (Figure 5-b), native feature CP nets. AUML guard condition mappeddirectly CP-net transition guard. CP-net transition guard indicatednet inscription next corresponding transition using square brackets. transitionguard guarantees transition enabled transition guard true.Figure 5-b, also extend color tokens include informationcommunicative act used content. extend SG color set definitionrecord hs, r, t, ci, r elements interpretation previoussection (sender receiver), c elements define message type content,respectively. element new color P E, determines communicative acttypes. c element new color CON EN , represents communicative actcontent argument list (e.g. reply-to, reply-by etc).addition new elements also allows additional potential uses. instance,facilitate representation multiple concurrent conversations agents(s r), possible add conversation identification field SGINTER-STATE colors. simplicity, refrain examplespaper.Two additional AUML communicative act attributes modelled CPrepresentation message sequence-expression message cardinality. sequenceexpressions denote constraint message sent sender agent. numbersequence-expressions defined FIPA conversation standards (FIPA Specifications, 2003c):denotes message sent exactly times; n..m denotes message sentanywhere n times; denotes message sent arbitrary number360fiRepresenting Conversations Scalable Overhearingtimes. additional important sequence expression broadcast, i.e. message sentagents.explain representation sequence-expressions CP-nets, using broadcastexample (Figure 6-b). sequence expressions easily derived example.define INTER-STATE-CARD color set. color set tuple (ha1 , a2 i, i) consistingtwo elements. first tuple element INTER-STATE color element, denotesinteracting agents previously defined. second tuple element integercounts number messages already sent agent, i.e. message cardinality.element initially assigned 0. INTER-STATE-CARD color set appliedS1 R1 place, R capital letters used denote senderreceiver individual interaction states respectively S1 R1 indicates initial jointinteraction state interacting agents. two additional colors, used Figure 6-b,BROADCAST-LIST ARGET colors. BROADCAST-LIST color definessender broadcast list designated receivers, assuming sender mustlist carry role. ARGET color defines indexes broadcast list.fffi!!fffifi ff'()( * + ,'()( ./ + ,'()( 01 * * + ,'()( 2* 3 + %'(4 56 *756 *# $%&'()( 08 + 9:'()( 2* 3 30 8 + ; ( 4<':" "2* 3 7 08fffi'()( = + %'(4 6 *7 6 *7fi:6./ 7' 601 * *'()( > 180 2 + * @ :A,3?B 5 ) $% + , '()( + 94%C > 180 3? 2ff@ :A !DDD$%3 B 5 6 *- B 5 6= - B 5 608fi(a) AUML representation(b) CP-net representationFigure 6: Broadcast sequence expression.According broadcast sequence-expression semantics, sender agent sendsmsg1 communicative act receivers broadcast list. CP-net introduced Figure 6-b models behavior.4 interaction starts S1 R1 place,representing joint interaction state sender ready send msg1 communicative act receiver (S1 ) receiver waiting receive corresponding msg1message (R1 ). S1 R1 place initial marking single token, set initialization expression (underlined, next corresponding place). initialization expression 1(hs, ARGET (0)i, 0)given standard CPN ML notationdetermines S1 R1places initial marking multi-set containing single token (hs, ARGET (0)i, 0). Thus,first designated receiver assigned agent index 0 broadcast list,message cardinality counter initiated 0.4. implement broadcast iterative procedure sending corresponding communicative act separately designated recipients.361fiGutnik & Kaminkamsg1 message place initially contains multiple tokens. tokens represents msg1 communicative act addressed different designated receiverbroadcast list. Figure 6-b, initialization expression, corresponding msg1 message place, omitted. S1 R1 place token appropriate msg1 place tokentogether enable corresponding transition. Consequently, transition may fire thusmsg1 communicative act transmission simulated.msg1 communicative act sent incrementally every designated receiverbroadcast list. incoming arc expression (hs, ri, i) incremented transitionoutgoing (hs, ARGET (i + 1)i, + 1) arc expression, causing receiver agentindex + 1 broadcast list selected. transition guard constraint < size,i.e. < |broadcast list|, ensures msg1 message sent |broadcast list|times. msg1 communicative act causes agents transition S2 R2 place.place represents joint interaction state sender already sent msg1communicative act receiver waiting receive msg2 message (S2 )receiver received msg1 message ready send msg2 communicative actsender (R2 ). Finally, msg2 message causes agents transition S3 R3place. S3 R3 place denotes joint interaction state sender received msg2communicative act receiver terminated (S3 ), receiver already sentmsg2 message sender terminated well (R3 ).use Figure 6-b demonstrate use token color represent multiple concurrentconversations using CP-net. instance, let us assume sender agentcalled agent1 broadcast list contains following agents: agent2 , agent3 , agent4 ,agent5 agent6 . also assume agent1 already sent msg1 communicative act agents broadcast list. However, received msg2reply message agent3 agent6 . Thus, CP-net current marking completeinteraction protocol described follows: S2 R2 place marked hagent2 , agent1 i,hagent4 , agent1 i, hagent5 , agent1 i, S3 R3 place contains tokens hagent1 , agent3hagent1 , agent6 i.Example. construct CP-net representation FIPA Query InteractionProtocol (FIPA Specifications, 2003d), shown AUML form Figure 7, demonstratebuilding blocks presented Sections 3 4 put together. interactionprotocol, Initiator requests P articipant perform inform action using one twoquery communicative acts, query-if query-ref. P articipant processes querymakes decision whether accept ref use query request. Initiator may requestP articipant respond either accept ref use message, simplicity,assume always case. case query request accepted,P articipant informs Initiator query results. P articipant fails,communicates f ailure. successful response, P articipant replies onetwo versions inform (inform-t/f inform-result) depending type initial queryrequest.CP-net representation FIPA Query Interaction Protocol presented Figure 8. interaction starts I1 P1 place (we use P capital lettersdenote Initiator P articipant roles). I1 P1 place represents jointinteraction state (i) Initiator agent ready send either query-if communicative act, query-ref message, P articipant (I1 ); (ii) P articipant wait362fiRepresenting Conversations Scalable Overhearingfffifffifffi fifi ff fifi ff fifififififififf fiff fifififi fffffi fiFigure 7: FIPA Query Interaction Protocol - AUML representation.ing receive corresponding message (P1 ). Initiator send either query-ifquery-ref communicative act. assume acts belong class,query communicative act class. Thus, implement messages using singleQuery message place, check message type using following transition guard:[#t msg = query-if #t msg = query-ref]. query communicative act causesinteracting agents transition I2 P2 place. place represents joint interactionstate Initiator sent query communicative act waiting receiveresponse message (I2 ), P articipant received query communicative actdeciding whether send agree ref use response message Initiator (P2 ).ref use communicative act causes agents transition I3 P3 place, agreemessage causes agents transition I4 P4 place.P articipant decision whether send agree ref use communicativeact represented using XOR-decision building block introduced earlier (Figure 3-b).I3 P3 place represents joint interaction state Initiator received ref usecommunicative act terminated (I3 ) P articipant sent ref use messageterminated well (P3 ). I4 P4 place represents joint interaction state Initiatorreceived agree communicative act waiting response363fiGutnik & Kaminka++fffiff3 ? 6 ( 9 6, B! "@Aff!#' " (" ) *+, -.$!ff fiff%ff%fffi' " (" 23 +, *+, -.' " ("7,/ *8 ' 723 +, * +, .ff+, *$4,, * ' "5 6 7 ) *+,86 7 ) *+, .%' " (" 9 4 ) ' "5 7 ) *+, 87 ) *+,8: 6 7 ) *+, .fffi' " (" ,/ * ! 0 ! 0111.&&: 6 79 4 ) .: 6 7,/ * .3 ;,?!">6 (C ?!" D! 6 ?5 ! Efffffifffi==3 ;,ff< <" C ?! " ( 6 ?5! E#3 ;,Figure 8: FIPA Query Interaction Protocol - CP-net representation.P articipant (I4 ) P articipant sent agree message decidingresponse send Initiator (P4 ). point, P articipant agent may send onefollowing communicative acts: inform-t/f, inform-result f ailure. choicerepresented using another XOR-decision building block, inform-t/f informresult communicative acts represented using single Inf orm message place. f ailurecommunicative act causes transition I5 P5 place, inf orm message causestransition I6 P6 place. I5 P5 place represents joint interaction stateP articipant sent f ailure message terminated (P5 ), Initiator receivedf ailure terminated (I5 ). I6 P6 place represents joint interaction stateP articipant sent inf orm message terminated (P6 ), Initiator receivedinf orm terminated (I6 ).implementation [query-if ] [query-ref ] message guard conditions requires detailed discussion. implemented usual manner view factdepend original request communicative act. Thus, create special intermediate place contains original message type marked "Original essage ype"figure. case inf orm communicative act sent, transition guard verifiesinf orm message appropriate original query type. Thus, inform-t/fcommunicative act sent original query type query-ifinform-result message sent original query type query-ref.364fiRepresenting Conversations Scalable Overhearing5. Representing Nested & Interleaved Interactionssection, extend CP-net representation previous sections model nestedinterleaved interaction protocols. focus nested interaction protocols. Nevertheless, discussion also addressed interleaved interaction protocols similarfashion.FIPA conversation standards (FIPA Specifications, 2003c) emphasize importancenested interleaved protocols modelling complex interactions. First, allows reuse interaction protocols different nested interactions. Second, nesting increasesreadability interaction protocols.AUML notation annotates nested interleaved protocols round corner rectangles (Odell et al., 2001a; FIPA Specifications, 2003c). Figure 9-a shows examplenested protocol5 , Figure 9-b illustrates interleaved protocol. Nested protocolsone compartments. first compartment name compartment.name compartment holds (optional) name nested protocol. nested protocolname written upper left-hand corner rectangle, i.e. commitment Figure 9a. second compartment, guard compartment, holds (optional) nested protocolguard. guard compartment written lower left-hand corner rectangle, e.g.[commit] Figure 9-a. Nested protocols without guards equivalent nested protocols[true] guard.fffifffififffi fi(a) Nested protocol(b) Interleave protocolFigure 9: AUML nested interleaved protocols examples.Figure 10 describes implementation nested interaction protocol presentedFigure 9-a extending CP-net representation using hierarchies, relying standard CP-net methods (see Appendix A). hierarchical CP-net representation containsthree elements: superpage, subpage page hierarchy graph. CP-net superpagerepresents main interaction protocol containing nested interaction, CP-netsubpage models corresponding nested interaction protocol, i.e. Commitment Inter5. Figure 9-a appears FIPA conversation standards (FIPA Specifications, 2003c). Nonetheless, noterequest-good request-pay communicative acts part FIPA-ACL standards.365fiGutnik & Kaminkaaction Protocol. page hierarchy graph describes superpage decomposedsubpages.# ff ff fffffffffffiff ff ff fffffffffffifffffi# ffffffffffff ff ffff ffffff ffff fffffffi!"Figure 10: Nested protocol implementation using hierarchical CP-nets.Let us consider detail process modelling nested interaction protocolFigure 9-a using hierarchical CP-net, resulting net described Figure 10. First,identify starting ending points nested interaction protocol. starting pointnested interaction protocol Buyer1 sends Request-Good communicative actSeller1 . ending point Buyer1 receives Request-Pay communicative actSeller1 . model nested protocol end-points CP-net socket nodessuperpage, i.e. Interaction P rotocol: B11 S11 Request-Good input socketnodes B13 S13 output socket node.nested interaction protocol, Commitment Interaction P rotocol, representedusing separate CP-net, following principles outlined Sections 3 4. netsubpage main interaction protocol superpage. nested interaction protocolstarting ending points subpage correspond net port nodes. B1 S1Request-Good places subpage input port nodes, B3 S3 place outputport node. nodes tagged IN/OUT port type tags correspondingly.Then, substitution transition, denoted using HS (Hierarchy Substitution), connects corresponding socket places superpage. substitution transition conceals nested interaction protocol implementation net superpage, i.e.Interaction P rotocol. nested protocol name guard compartmentsmapped directly substitution transition name guard respectively. Consequently,Figure 10 define substitution transition name Commitment substitutionguard determined [commit].superpage subpage interface provided using hierarchy inscription.hierarchy inscription indicated using dashed box next substitution transition. first line hierarchy inscription determines subpage identity, i.e.366fiRepresenting Conversations Scalable OverhearingCommitment Interaction P rotocol example. Moreover, indicates substitution transition replaces corresponding subpage detailed implementation superpage. remaining hierarchy inscription lines introduce superpage subpage portassignment. port assignment relates socket node superpage port nodesubpage. substitution transition input socket nodes related IN-taggedport nodes. Analogously, substitution transition output socket nodes correspondOUT-tagged port nodes. Therefore, port assignment Figure 10 assigns net socketport nodes following fashion: B11 S11 B1 S1 , Request-Good Request-GoodB13 S13 B3 S3 .Finally, page hierarchy graph describes decomposition hierarchy (nesting)different protocols (pages). CP-net pages, Interaction P rotocolCommitment Interaction P rotocol, correspond page hierarchy graph nodes(Figure 10). arc inscription indicates substitution transition, i.e. Commitment.6. Representing Temporal Aspects InteractionsTwo temporal interaction aspects specified FIPA (FIPA Specifications, 2003c).section, show timed CP-nets (see also Appendix A) applied modellingagent interactions involve temporal aspects, interaction duration, deadlinesmessage exchange, etc.first aspect, duration, interaction activity time period. Two periodsdistinguished: transmission time response time. transmission time indicatestime interval communicative act, sent one agent receiveddesignated receiver agent. response time period denotes time intervalcorresponding receiver agent performing task response incomingcommunicative act.second temporal aspect deadlines. Deadlines denote time limitcommunicative act must sent. Otherwise, corresponding communicative actconsidered invalid. issues addressed previous investigationsrelated agent interactions modelling using Petri nets.6propose utilize timed CP-nets techniques represent temporal aspectsagent interactions. so, assume global clock.7 begin deadlines. Figure 11-a introduces AUML representation message deadlines. deadline keywordvariation communicative act sequence expressions described Section 4.sets time constraint start transmission associated communicative act.Figure 11-a, agent1 must send msg communicative act agent2 defineddeadline. deadline expires, msg communicative act considered invalid.Figure 11-b shows timed CP-net implementation deadline sequence expression.timed CP-net Figure 11-b defines additional MSG-TIME color set associatednet message places. MSG-TIME color set extends SG color set, describedSection 4, adding time stamp attribute message token. Thus, communicative6. Cost et al. (1999, 2000) mention deadlines without presenting implementation details.7. Implementing it, use private clock overhearing agent global clock Petrinet representation. Thus, time stamp message overhearers time correspondingmessage overheard.367fiGutnik & Kaminkaff fi! ! "#$ % &'! ! ()# % &'! ! *+ $#$ % &'! ! ,$#-./ # % !ff01 "#$23 1 "#$'fifffi! ! 4 /" % !1 "#$2 1 "#$21()#2 1*+ $#$'! ! 4 /". ,4 #% 4 /" '5 1 "#$' 5 1()#'ff5 1*+ $#$'fi5 % &'(a) AUML representation(b) CP-net representationFigure 11: Deadline sequence expression.act token record hs, r, t, ci@[T ts]. @[..] expression denotes corresponding tokentime stamp, whereas token time value indicated starting capital T. Accordingly, described message token ts time stamp. communicative act time limitdefined using val deadline parameter. Therefore, deadline sequence expressionsemantics simulated using following transition guard: [T ts < deadline]. transition guard, comparing msg time stamp deadline parameter, guaranteesexpired msg communicative act received.turn representing interaction duration. AUML representation shownFigure 12-a. AUML time intensive message notation used denote communicative act transmission time. rule communicative act arrows illustrated horizontally.indicates message transmission time neglected. However, casemessage transmission time significant, communicative act drawn slanted downwards.vertical distance, arrowhead arrow tail, denotes message transmission time. Thus, communicative act msg1 , sent agent1 agent2 , t1transmission time.fffifffiff!"#$%& '()*+ ,)-+2 2341 1 15 67$%& % #.ff#fffi ff#$%& / '(fiff. & '(2 234 156 7+ , + ,+ , +!". &% #.. & / '(ff0)1+ 0) +fi ff88+!"0)(a) AUML representation(b) CP-net representationFigure 12: Interaction duration.368fiRepresenting Conversations Scalable Overhearingresponse time Figure 12-a indicated interaction thread length.incoming msg1 communicative act causes agent2 perform task sendingresponse msg2 message. corresponding interaction thread duration denotedt2 time period. Thus, time period specifies agent2 response time incomingmsg1 communicative act.CP-net implementation interaction duration time periods shown Figure 12-b. communicative act transmission time illustrated using timed CP-nets@+ operator. net transitions simulate communicative act transmissionagents. Therefore, representing transmission time t1 , CP-net transition adds t1time period incoming message token time stamp. Accordingly, transition @ + t1output arc expression denotes t1 delay time stamp outgoing token. Thus,corresponding transition takes t1 time units consequently msg1 communicative act transmission time.contrast communicative act transmission time, agent interaction response timerepresented implicitly. Previously, defined MSG-TIME color set indicatesmessage token time stamps. Analogously, Figure 12-b introduce additional INTERSTATE-TIME color set. color set associated net agent places presentspossibility attach time stamps agent tokens well. Now, let us assume A2 B2msg2 places contain single token each. circled 1 next corresponding place,together multi-set inscription, indicates place current marking. Thus, agentmessage place tokens ts1 ts2 time stamps respectively. ts1 timestamp denotes time agent2 received msg1 communicative act sentagent1 . ts2 time stamp indicates time agent2 ready send msg2response message agent1 . Thus, agent2 response time t2 (Figure 12-a) ts2 ts1 .7. Algorithm Concluding Examplefinal contribution paper skeleton procedure transforming AUMLconversation protocol diagram two interacting agents CP-net representation.procedure semi-automatedit relies human fill detailsbut alsoautomated aspects. apply procedure complex multi-agent conversation protocolinvolves many interaction building blocks already discussed.procedure shown Algorithm 1. algorithm input AUML protocoldiagram algorithm creates, output, corresponding CP-net representation.CP-net constructed iterations using queue. algorithm essentially createsconversation net exploring interaction protocol breadth-first avoiding cycles.Lines 1-2 create initiate algorithm queue, output CP-net, respectively.queue, denoted S, holds initiating agent places current iteration.places correspond interaction states initiate conversation interacting agents. lines 4-5, initial agent place A1 B1 created insertedqueue. A1 B1 place represents joint initial interaction state two agents. Lines7-23 contain main loop.enter main loop line 8 set curr variable first initiating agentplace queue. Lines 10-13 create CP-net components corresponding currentiteration follows. First, line 10, message places, associated curr agent place,369fiGutnik & KaminkaAlgorithm 1 Create Conversation Net(input:AU L,output:CP N )1: new queue2: CP N new CP net3:4:5:A1 B1 new agent place color informationS.enqueue(A1 B1 )6:7:8:emptycurr S.dequeue()9:10:11:12:13:P CreateM essageP laces(AU L, curr)RP CreateResultingAgentP laces(AU L, curr, P )(T R, AR) CreateT ransitionsAndArcs(AU L, curr, P, RP )F ixColor(AU L, CP N, P, RP, R, AR)14:15:16:17:18:19:20:21:22:23:place p RPp created current iterationcontinuep terminating placeS.enqueue(p)CP N.places = CP N.places P SRPCP.transitions = CP N.transitionsTRCP N.arcs = CP N.arcs AR24:25:return CP Ncreated using CreateM essageP laces procedure (which detail here).procedure extracts communicative acts associated given interactionstate, AUML diagram. places correspond communicative acts,take agents joint interaction state curr successor(s). line 11,CreateResultingAgentP laces procedure creates agent places correspond interactionstate changes result communicative acts associated curr agent place (againbased AUML diagram). Then, CreateT ransitionsAndArcs procedure (line 12),places connected using principles described Sections 36. Thus, CP-netstructure (net places, transitions arcs) created. Finally, line 13, F ixColor procedure adds token color elements CP-net structure, support deadlines, cardinality,communicative act attributes.Lines 15-19 determine resulting agent places inserted queueiteration. non-terminating agent places, i.e. places correspondinteraction states terminate interaction, inserted queue lines 18-19.However, one exception (lines 16-17): resulting agent place, alreadyhandled algorithm, inserted back queue since inserting causeinfinite loop. Thereafter, completing current iteration, output CP-net, denoted370fiRepresenting Conversations Scalable OverhearingCP N variable, updated according current iteration CP-net components lines21-23. main loop iterates long queue empty. resulting CP-netreturnedline 25.fi fifififffififfff fififf fifffifffiffFigure 13: FIPA Contract Net Interaction Protocol using AUML.demonstrate algorithm, use FIPA Contract Net InteractionProtocol (FIPA Specifications, 2003b) (Figure 13). protocol allows interacting agentsnegotiate. Initiator agent issues calls proposals using cf p communicative act.P articipants may refuse counter-propose given deadline sending eitherref use propose message respectively. ref use message terminates interaction.contrast, propose message continues corresponding interaction.deadline expires, Initiator accept P articipant response messages. evaluates received P articipant proposals selects one, several,agents perform requested task. Accepted proposal result sendingaccept-proposal messages, remaining proposals rejected using reject-proposalmessage. Reject-proposal terminates interaction corresponding P articipant.hand, accept-proposal message commits P articipant perform requested task. successful completion, P articipant informs Initiator sending eitherinform-done inform-result communicative act. However, case P articipantfailed accomplish task, communicates f ailure message.371fiGutnik & Kaminkause algorithm introduced create CP-net, representsFIPA Contract Net Interaction Protocol. corresponding CP-net model constructedfour iterations algorithm. Figure 14 shows CP-net representation seconditeration algorithm, Figure 15 shows CP-net representation fourthfinal iteration.Contract Net Interaction Protocol starts I1 P1 place, represents joint interaction state Initiator ready send cf p communicative act (I1 ) P articipantwaiting corresponding cf p message (P1 ). I1 P1 place created insertedqueue iterations main loop begin.First iteration. curr variable set I1 P1 place. algorithm createsnet places, associated I1 P1 place, i.e. Cf p message place,I2 P2 resulting agent place. I2 P2 place denotes interaction state Initiatoralready sent cf p communicative act P articipant waiting response (I2 ) P articipant received cf p message decidingappropriate response (P2 ). created using CreateM essageP lacesCreateResultingAgentP laces procedures, respectively.Then, CreateT ransitionsAndArcs procedure line 12, connects three placesusing simple asynchronous message building block shown Figure 1-b (Section 3).line 13, color sets places determined, algorithm also handlescardinality cf p communicative act, putting appropriate sequence expressiontransition, using principles presented Figure 6-b (Section 4). Accordingly,color set, associated I1 P1 place, changed INTER-STATE-CARD color set.Since I2 P2 place terminating place, inserted queue.Second iteration. curr set I2 P2 place. P articipant agent send,response, either ref use propose communicative act. Ref use P ropose messageplaces created CreateM essageP laces (line 10), resulting places I3 P3 I4 P4 ,corresponding results ref use propose communicative acts, respectively,created CreateResultingAgentP laces (line 11). I3 P3 place represents jointinteraction state P articipant sent ref use message terminated (P3 ),Initiator received it, terminated (I3 ). I4 P4 place represents joint stateP articipant sent propose message (P4 ), Initiator receivedmessage considering response (I4 ).line 12, I2 P2 , Ref use, I3 P3 , P ropose I4 P4 places connected usingXOR-decision building block presented Figure 3-b (Section 3). Then, F ixColorprocedure (line 13), adds appropriate token color attributes, allow deadline sequenceexpression (on ref use propose messages) implemented shownFigure 11-b (Section 6). I3 P3 place denotes terminating state, whereas I4 P4place continues interaction. Thus, lines 18-19, I4 P4 place insertedqueue, next iteration algorithm. state net end seconditeration algorithm presented Figure 14.Third iteration. curr set I4 P4 . Here, Initiator response P articipantproposal either accept-proposal reject-proposal. CreateM essageP laces procedure line 10 thus creates corresponding Accept-Proposal Reject-Proposal messageplaces. accept-proposal reject-proposal messages cause interacting agentstransition I5 P5 I6 P6 places, respectively. agent places created using372fiRepresenting Conversations Scalable Overhearing3 0 & ( )43 083 0 ; 4$%& '( )**ff fi fffi3 0!./. 01 "fi fffi4)'<=&)<& ' @ 54!"!./.01"ff# #) 5 3 . /"- -!"ff5 67&& ()4>/fffi)4)'<=3 0 & '@ 5 1! 7+ $+ $' ,4fi& '( )fi ff3 05 67) 5 3 9 , 9:::72 2)'<=& ()4->?7.,3!&)?& '@73 0 A= ( 5 3 . & ( )4 ? & ( ) 4 ?>>!8 )? 3 ;4 )4 7>>& '() <B =5 & ( )4C ! 673 05 67E/ 03 0& '()5 1.F& '()C ! * :::E / & ( ) 4 7 E />E / & '@7>E / 0 ./. 015 67<B =< $7A=(7>Figure 14: FIPA Contract Net Interaction Protocol using CP-net 2nd iteration.CreateResultingAgentP laces procedure (line 11). I5 P5 place denotes interactionstate Initiator sent reject-proposal message terminated interaction (I5 ), P articipant received message terminated well (P5 ).contrast, I6 P6 place represents interaction state Initiator sent acceptproposal message waiting response (I6 ), P articipant receivedaccept-proposal communicative act performing requested task sendingresponse (P6 ). Initiator agent sends exclusively either accept-proposal rejectproposal message. Thus, I4 P4 , Reject-Proposal, I5 P5 , Accept-Proposal I6 P6 placesconnected using XOR-decision block (in CreateT ransitionsAndArcs procedure,line 12).F ixColor procedure line 13 operates follows: According interactionprotocol semantics, Initiator agent evaluates received P articipant proposalsdeadline passes. thereafter, appropriate reject-proposal accept-proposalcommunicative acts sent. Thus, F ixColor assigns MSG-TIME color set RejectProposal Accept-Proposal message places, creates [T ts >= deadline] transition guard associated transitions. transition guard guarantees Initiatorcannot send response deadline expires, valid P articipant responsesreceived. resulting I5 P5 agent place denotes terminating interaction state,whereas I6 P6 agent place continues interaction. Thus, I6 P6 agent placeinserted queue.Fourth iteration. curr set I6 P6 . place associated three communicative acts: inform-done, inform-result f ailure. inform-done informresult messages instances inf orm communicative act class. Thus, CreateMessagePlaces (line 10) creates two message places, Inf orm F ailure. line 11,CreateResultingAgentP laces creates I7 P7 I8 P8 agent places. f ailure communicative act causes interacting agents transition I7 P7 agent place, inf ormmessages cause agents transition I8 P8 agent place. I7 P7 place representsjoint interaction state P articipant sent f ailure message terminated (P7 ),373fiGutnik & KaminkaInitiator received f ailure communicative act terminated (I7 ).hand, I8 P8 place denotes interaction state P articipant sent inf ormmessage (either inform-done inform-result) terminated (P8 ), Initiatorreceived inf orm communicative act terminated (I8 ). inf orm f ailure communicative acts sent exclusively. Thus CreateT ransitionsAndArcs (line 12) connectsI6 P6 , F ailure, I7 P7 , Inf orm I8 P8 places using XOR-decision building block.Then, F ixColor assigns [#t msg = inform-done #t msg = inform-result] transitionguard transition associated Inf orm message place. Since I7 P7I8 P8 agent places represent terminating interaction states, insertedqueue, remains empty end current iteration. signifies endconversion. complete conversation CP-net resulting iteration algorithmshown Figure 15.$%& '( )* *fffi ff, $ , $..fi##3fi'6 4!5 0 138 8fi++;0 1-fifi=4 1L0 1& 44!5)'5Dfi ff&)5& 'G 75I/-4!&)F& 'G?FE& () =@ )F4 EC=7 & ( )=)=F?J ! K >?& '( )7 2/M& '()J !K * BBB 5 $??L 05IEH D( ?L 0 E& 'G?!"L 0 1 /0/ 127 >?! 7/0/ 12"fffi2fi9 9)'5D7 >?L 0 E& ()=& '( )4 10 1ff?!EF0 E& ()=.4 1 H D( 7 4 / E& ()=!"ff) 7 4 / 0 E& ( ) ==! 7/0/ 12"7 >?&!!"4 1/0/ 12 "ff)=)'5D=4 1 & 'G 7 2! ?fi ff!ff!"/0/12"fiff"fi4 17 >?@ ) 7 4 - ABBB?4 1 C =fffi ff4 1& '( )' -4 1 & ( )=fi:<!7 25/ 2 <!725- 1!"ff:fiFigure 15: FIPA Contract Net Interaction Protocol using CP-net 4th (and final)iteration.procedure outline guide conversion many 2-agent conversation protocols AUML CP-net equivalents. However, sufficiently developedaddress general n-agent case. Appendix C presents complex example 3-agent conversation protocol, successfully converted manually, without guidancealgorithm. example incorporates many advanced features CP-net representationtechnique would beyond scope many previous investigations.374fiRepresenting Conversations Scalable Overhearing8. Summary Conclusionsrecent years, open distributed MAS applications gained broad acceptancemulti-agent academic community real-world industry. result, increasing attention directed multi-agent conversation representation techniques.particular, Petri nets recently shown provide viable representation approach(Cost et al., 1999, 2000; Nowostawski et al., 2001; Mazouzi et al., 2002).However, radically different approaches proposed using Petri nets modelling multi-agent conversations. Yet, relative strengths weaknesses proposedtechniques examined. work introduces novel classification previous investigations compares investigations addressing scalabilityappropriateness overhearing tasks.Based insights gained analysis, developed novel representation,uses CP-nets places explicitly represent joint interaction states messages.representation technique offers significant improvements (compared previous approaches) terms scalability, particularly suitable monitoring via overhearing.systematically show representation covers essentially features requiredmodel complex multi-agent conversations, defined FIPA conversation standards (FIPA Specifications, 2003c). include simple & complex interaction buildingblocks (Section 3 & Appendix B), communicative act attributes multiple concurrentconversations using CP-net (Section 4), nested & interleaved interactions usinghierarchical CP-nets (Section 5) temporal interaction attributes using timed CP-nets(Section 6). developed techniques demonstrated, throughout paper,complex interaction protocols defined FIPA conversation standards (see particularexample presented Appendix C). Previous approaches could handleexamples (though reduced scalability), shown coverrequired features.Finally, paper presented skeleton procedure semi-automatically convertingAUML protocol diagrams (the chosen FIPA representation standard) equivalent CPnet representation. demonstrated use challenging FIPA conversation protocol, difficult represent using previous approaches.believe work assist motivate continuing research multi-agentconversations including issues performance analysis, validation verification (Desel et al., 1997), agent conversation visualization, automated monitoring (Kaminka et al.,2002; Busetta et al., 2001, 2002), deadlock detection (Khomenco & Koutny, 2000), debugging (Poutakidis et al., 2002) dynamic interpretation interaction protocols (Cranefieldet al., 2002; de Silva et al., 2003). Naturally, issues remain open future work.example, presented procedure addresses AUML protocol diagrams representing twoagent roles. plan investigate n-agent version future.Acknowledgmentsauthors would like thank anonymous JAIR reviewers many useful informative comments. Minor subsets work also published LNAI book chapter(Gutnik & Kaminka, 2004b). K. Ushi deserves many thanks.375fiGutnik & KaminkaAppendix A. Brief Introduction Petri NetsPetri nets (Petri Nets site, 2003) widespread, established methodology representingreasoning distributed systems, combining graphical representation comprehensive mathematical theory. One version Petri nets called Place/Transition nets(PT-nets) (Reisig, 1985). PT-net bipartite directed graph node eitherplace transition (Figure 16). net places transitions indicatedcircles rectangles respectively. PT-net arcs support place transitiontransition place connections, never connections two places twotransitions. arc direction determines input/output characteristics placetransition connected. Thus, given arc, P , connecting place P transition ,say place P input place transition vice versa transitionoutput transition place P . P arc considered output arc place Pinput arc transition .(a) firing(b) firingFigure 16: PT-net example.PT-net place may marked small black dots called tokens. arc expressioninteger, determines number tokens associated corresponding arc.convention, arc expression equal 1 omitted. specific transition enabledinput places marking satisfies appropriate arc expressions. example,consider arc P arc connect place P transition . Thus, givenarc arc expression 2, say transition enabledplace P marked two tokens. case transition enabled, may fire/occur.transition occurrence removes tokens transition input places puts tokenstransition output places specified arc expressions correspondinginput/output arcs. Thus, Figures 16-a 16-b, demonstrate PT-net markingtransition firing respectively.Although computationally equivalent, different version Petri nets, called ColoredPetri nets (CP-nets) (Jensen, 1997a, 1997b, 1997c), offers greater flexibility compactlyrepresenting complex systems. Similarly PT-net model, CP-nets consist net places,net transitions arcs connecting them. However, CP-nets, tokens singlebits, complex, structured, information carriers. type additional information carried token, called token color, may simple (e.g., integerstring), complex (e.g. record tuple). place declared place color set376fiRepresenting Conversations Scalable Overhearingmatch tokens particular colors. CP-net place marking token multi-set (i.e.,set member may appear once) corresponding appropriate placecolor set. CP-net arcs pass token multi-sets places transitions. CP-net arcexpressions evaluate token multi-sets may involve complex calculation procedurestoken variables declared associated corresponding arcs.CP-net model introduces additional extensions PT-nets. Transition guardsboolean expressions, constrain transition firings. transition guard associatedtransition tests tokens pass transition, enable transitionfirings guard successfully matched (i.e., test evaluates true). CP-nettransition guards, together places color sets arc expressions, appear partnet inscriptions CP-net.order visualize manage complexity large CP-nets, hierarchical CP-nets(Huber, Jensen, & Shapiro, 1991; Jensen, 1997a) allow hierarchical representations CPnets, sub-CP nets re-used higher-level CP nets, abstracted awaythem. Hierarchical CP-nets built pages, CP-nets. Superpagespresent higher level hierarchy, CP-nets refer subpages, additiontransitions places. subpage may also function superpage subpages.way, multiple hierarchy levels used hierarchical CP-net structure.relationship superpage subpage defined substitution transition, substitutes corresponding subpage instance CP-net superpage structuretransition superpage. substitution transition hierarchy inscription suppliesexact mapping superpage places connected substitution transition (calledsocket nodes), subpage places (called port nodes). port types determinecharacteristics socket node port node mappings. complete CP-net hierarchicalstructure presented using page hierarchy graph, directed graph vertices correspond pages, directed edges correspond direct superpage-subpage relationships.Timed CP-nets (Jensen, 1997b) extend CP-nets support representation temporal aspects using global clock. Timed CP-net tokens additional color attributecalled time stamp, refers earliest time token may used. Timestamps used arc expression transition guards, enable timed-transitionsatisfies two conditions: (i) transition color enabled, i.e. satisfiesconstraints defined arc expression transition guards; (ii) tokens ready,i.e. time global clock equal greater tokens time stamps.transition fire.Appendix B. Additional Examples Conversation RepresentationBuilding Blocksappendix presents additional interaction building blocks already described Section 3. first AND-parallel messages interaction (AUML representation shown Figure 17-a). Here, sender agent1 sends msg1 messageagent2 msg2 message agent3 . However, order two communicative actsunconstrained.representation AND-parallel CP-net representation shown Figure 17-b.A1 B1 C1 , A2 B2 , A2 C2 , msg1 msg2 places defined similarly Figures 3-b377fiGutnik & Kaminkaff fi010ff fiff fiffff!"#$ %& ' ()' (!"#$#*%& ' ()' ()'+(,$ %& ()(-' ./ ((a) AUML representation(b) CP-net representationFigure 17: AND-parallel messages interaction.4-b Section 3. However, also define two additional intermediate agent places, A01 B2 C1A001 B1 C2 . A01 B2 C1 place represents joint interaction state agent1 sentmsg1 message agent2 ready send msg2 communicative act agent3(A1 ), agent2 received msg1 message (B2 ) agent3 waiting receive msg2communicative act (C1 ). A001 B1 C2 place represents joint interaction stateagent1 ready send msg1 message agent2 already sent msg2 communicative act agent3 (A001 ), agent2 waiting receive msg1 message (B1 ) agent3received msg2 communicative act (C2 ). places enable agent1 sendcommunicative acts concurrently. Four transitions connect appropriate places respectively. behavior transitions connecting A01 B2 C1 A2 B2 A001 B1 C2 A2 C2similar described above. transitions A1 B1 C1 A01 B2 C1 A1 B1 C1 A001 B1 C2triggered receiving messages msg1 msg2 , respectively. However, transitions consume message token since used triggering transitionsA01 B2 C1 A2 B2 A001 B1 C2 A2 C2 . achieved adding appropriate messageplace output place corresponding transition.second AUML interaction building block, shown Figure 18-a, messagesequence interaction, similar AND-parallel. However, message sequenceinteraction defines explicitly order transmitted messages. Using 1/msg12/msg2 notation, Figure 18-a specifies msg1 message sentsending msg2 .Figure 18-b shows corresponding CP-net representation. A1 B1 C1 , A2 B2 , A2 C2 ,msg1 msg2 places defined before. However, CP-net implementation presentsadditional intermediate agent placeA01 B2 C1which identical corresponding378fiRepresenting Conversations Scalable Overhearingfffifffffifffffiff0fffiff!"#$%& ' ( ) * + )* "#$%& %, ' ( ) * +)* +)- * ". & ' ( * + * "/ ) * "(a) AUML representation(b) CP-net representationFigure 18: Sequence messages interaction.intermediate agent place Figure 17-b. A01 B2 C1 defined output placeA1 B1 C1 A2 B2 transition. thus guarantees msg2 communicative actsent (represented A01 B2 C1 A2 C2 transition) upon completion msg1transmission (the A1 B1 C1 A2 B2 transition).last interaction present synchronized messages interaction, shown Figure 19-a. Here, agent3 simultaneously receives msg1 agent1 msg2 agent2 .AUML, constraint annotated merging two communicative act arrowshorizontal bar single output arrow.fffifffifffifi1fffi/fffi..../0../!"#$ %& '( )'(!"#$ #* %& '( )'( )'+(,$ %& ( ) (- ' .((a) AUML representation(b) CP-net representationFigure 19: Synchronized messages interaction.379fiGutnik & KaminkaFigure 19-b illustrates CP-net implementation synchronized messages interaction.previous examples, define A1 C1 , B1 C1 , msg1 , msg2 A2 B2 C2 places.additionally define two intermediate agent places, A2 C10 B2 C100 . A2 C10 place represents joint interaction state agent1 sent msg1 agent3 (A2 ), agent3received it, however agent3 also waiting receive msg2 (C10 ). B2 C100 place representsjoint interaction state agent2 sent msg2 agent3 (B2 ), agent3received it, however agent3 also waiting receive msg1 (C100 ). places guaranteeinteraction transition A2 B2 C2 state msg1 msg2received agent3 .Appendix C. Example Complex Interaction Protocolpresent example complex 3-agent conversation protocol, manually converted CP-net representation using building blocks paper. conversationprotocol addressed FIPA Brokering Interaction Protocol (FIPA Specifications,2003a). interaction protocol incorporates many advanced conversation featuresrepresentation nesting, communicative act sequence expression, message guardsetc. AUML representation shown Figure 20.Initiator agent begins interaction sending proxy message Brokeragent. proxy communicative act contains requested proxied-communicative-actpart argument list. Broker agent processes request responds eitheragree ref use message. Communication ref use message terminates interaction.Broker agent agreed function proxy, locates agents matchingInitiator request. agent found, Broker agent communicatesfailure-no-match message interaction terminates. Otherwise, Broker agentbegins interactions matching agents. agent, Broker informsInitiator, sending either inform-done-proxy failure-proxy communicative act.failure-proxy communicative act terminates sub-protocol interaction matchingagent question. inform-done-proxy message continues interaction. subprotocol progresses, Broker forwards received responses Initiator agent usingreply-message-sub-protocol communicative acts. However, failuresexplicitly returned sub-protocol interaction (e.g., agent executingsub-protocol failed). case Broker agent detects failure, communicatesfailure-brokering message, terminates sub-protocol interaction.CP-net representation FIPA Brokering Interaction Protocol shown Figure 21. Brokering Interaction Protocol starts I1 B1 place. I1 B1 place represents joint interaction state Initiator ready send proxy communicativeact (I1 ) Broker waiting receive (B1 ). proxy communicative act causesinteracting agents transition I2 B2 . place denotes interaction stateInitiator already sent proxy message Broker (I2 ) Broker received (B2 ).Broker agent send, response, either ref use agree communicative act.CP-net component implemented using XOR-decision building block presentedSection 3. ref use message causes agents transition I3 B3 place thusterminate interaction. place corresponds Broker sending ref use messageterminating (B3 ), Initiator receiving message terminating (I3 ).380fiRepresenting Conversations Scalable Overhearingfffi ff!!" ##!!!!Figure 20: FIPA Brokering Interaction Protocol - AUML representation.hand, agree communicative act causes agents transition I4 B4 place,represents joint interaction state Broker sent agree messageInitiator (and trying locate receivers proxied message),Initiator received agree message.Broker agents search suitable receivers may result two alternatives. First,case matching agents found, interaction terminates I5 B5 agent place.joint interaction place corresponds interaction state Broker sentfailure-no-match communicative act (B5 ), Initiator received message terminated (I5 ). second alternative suitable agents found. Then, Brokerstarts sending proxied-communicative-act messages agents established listdesignated receivers, i.e. TARGET-LIST. first proxied-communicative-act message causes interacting agents transition I4 B6 P1 place. I4 B6 P1 place denotesjoint interaction state three agents: Initiator, Broker P articipant (the receiver).381fiGutnik & Kaminkafffi71/ff- 8 7 29 7> /B * 71/ 8 7 29 7> /2 9KB! "#$% & '(fffi$% #/0% % # & 1!2 3 "# $%4 2 3 " #$% ($% #/0% % #/5 & 12 3 "# $%4 2 3 " #$%49ff!fffiff! - .$% #$% & '(fffi! %) # & * + +,,,(ff2 3 "# $% (! - . 6 & 789 (!$% #/0% % #/5/- 6 &$% # /0% % #/541- 6 ($/:2 9IJ ffH2 7!/9/% *L! : 0"& 1 3 "# $%4=3 " #$%49 3%) #471/3-. $% # $% (- 8 7 29 7> /GffK % "#% K? L ? Lfffifffi>2 ! & '(! % " #%&781 % " #%/K * 7LE fiF9ffG! % " #%/ ; 0% & " #$% < 79 = ,,(; 0% < 79 = ? ,,, / @(>2 3: 0"(>2 * 3 " #$% (A7 281 9 &B9 CK % " #% K7N @L* 7N @L>2 9 3%) #(>2 73- 6(*H2 7!/Pff*71/- 8 7 29 7> /fffi8 /6 8/*fffiff9R /0S/9 !Q0*R / 892 9 7 8 /9 !fffiffX/ XX (W71/- 8 7 2 9 7> /*PU78XY Z/9 !H2 7!/Vfffiff fiffff*XY Z/[W([(W*9/R (XY Z/ Z Z (R /0S/*! /: 2 /*0S/**fffifffiffff9 !Figure 21: FIPA Brokering Interaction Protocol - CP-net representation.Initiator individual state remains unchanged (I4 ) since proxied-communicative-actmessage starts interaction Broker P articipant. Broker individualstate (B6 ) denotes designated agents found proxied-communicative382fiRepresenting Conversations Scalable Overhearingact messages ready sent, P articipant waiting receive interactioninitiating communicative act (P1 ). proxied-communicative-act message place alsoconnected output place transition. message place used partCP-net XOR-decision structure, enables Broker agent send either failure-nomatch proxied-communicative-act, respectively. Thus, token denoting proxiedcommunicative-act message, must consumed transition.Thus, multiple proxied-communicative-act messages sent P articipants.implemented similarly broadcast sequence expression implementation (Section 4).Furthermore, proxied-communicative-act type verified type requestedproxied communicative act, obtained original proxy message content.use Proxied-Communicative-Act-Type message type place implement CPnet component similarly Figure 8. proxied-communicative-act message causesinteracting agents transition I4 B7 P1 B6 P1 places.B6 P1 place corresponds interaction Broker P articipantagents. represents joint interaction state Broker ready send proxiedcommunicative-act message P articipant (B6 ), P articipant waiting message(P1 ). fact, B6 P1 place initiates nested interaction protocol results B10 P3place. B10 P3 place represents joint interaction state P articipant sentreply-message communicative act terminated (P3 ), Broker receivedmessage (B10 ). example, chosen FIPA Query Interaction Protocol (FIPASpecifications, 2003d) (Figures 78) interaction sub-protocol. CP-net component,implementing nested interaction sub-protocol, modeled using principles describedSection 5. Consequently, interaction sub-protocol concealed using Query-SubProtocol substitution transition. B6 P1 , proxied-communicative-act B10 P3 placesdetermine substitution transition socket nodes. socket nodes assigned CPnet port nodes Figure 8 follows. B6 P1 proxied-communicative-act placesassigned I1 P1 query input port nodes, B10 P3 place assignedI3 P3 , I5 P5 I6 P6 output port nodes.turn I4 B7 P1 place. contrast B6 P1 place, place correspondsmain interaction protocol. I4 B7 P1 place represents joint interaction stateInitiator waiting Broker respond (I4 ), Broker ready send appropriate response communicative act (B7 ), best Initiators knowledge interactionP articipant yet begun (P1 ). Broker agent send one two messages,either failure-proxy inform-done-proxy, depending whether succeededsend proxied-communicative-act message P articipant. failure-proxy messagecauses agents terminate interaction corresponding P articipant agenttransition I6 B8 P1 place. place denotes joint interaction state Initiatorreceived failure-proxy communicative act terminated (I6 ), Broker sentfailure-proxy message terminated well (B8 ) interaction P articipantagent never started (P1 ). hand, inform-done-proxy causes agentstransition I7 B9 P2 place. I7 B9 P2 place represents interaction state Brokersent inform-done-proxy message (B9 ), Initiator received (I7 ), P articipantbegun interaction Broker agent (P2 ). Again, represented usingXOR-decision building block.383fiGutnik & KaminkaFinally, Broker agent either send reply-message-sub-protocol failurebrokering communicative act. failure-brokering message causes interacting agentstransition I8 B11 P2 place. place indicates Broker sent failure-brokeringmessage terminated (B11 ), Initiator received message terminated (I8 ),P articipant terminated interaction Broker agent (P2 ). replymessage-sub-protocol communicative act causes agents transition I9 B12 P3 place.I9 B12 P3 place indicates Broker sent reply-message-sub-protocol messageterminated (B12 ), Initiator received message terminated (I9 ), P articipantsuccessfully completed nested sub-protocol Broker agent terminatedwell (P3 ). Thus, B10 P3 place, denoting successful completion nested sub-protocol,also corresponding transition input place.ReferencesAUML site (2003). Agent unified modeling language, www.auml.org..Busetta, P., Dona, A., & Nori, M. (2002). Channelled multicast group communications.Proceedings AAMAS-02.Busetta, P., Serafini, L., Singh, D., & Zini, F. (2001). Extending multi-agent cooperationoverhearing. Proceedings CoopIS-01.ChaibDraa, B. (2002). Trends agent communication languages. Computational Intelligence, 18 (2), 89101.Cost, R. S. (1999). framework developing conversational agents. Ph.D. thesis, Department Computer Science, University Maryland.Cost, R. S., Chen, Y., Finin, T., Labrou, Y., & Peng, Y. (1999). Modeling agent conversationscoloured Petri nets. Proceedings Workshop Specifying Implementing Conversation Policies, Third International Conference Autonomous Agents(Agents-99), Seattle, Washington.Cost, R. S., Chen, Y., Finin, T., Labrou, Y., & Peng, Y. (2000). Using coloured petri netsconversation modeling. Dignum, F., & Greaves, M. (Eds.), Issues AgentCommunications, Lecture notes Computer Science, pp. 178192. Springer-Verlag.Cranefield, S., Purvis, M., Nowostawski, M., & Hwang, P. (2002). Ontologies interaction protocols. Proceedings Workshop Ontologies Agent Systems,First International Joint Conference Autonomous Agents & Multi-Agent Systems(AAMAS-02), Bologna, Italy.de Silva, L. P., Winikoff, M., & Liu, W. (2003). Extending agents transmitting protocolsopen systems. Proceedings Workshop Challenges Open Agent Systems, Second International Joint Conference Autonomous Agents & Multi-AgentSystems (AAMAS-03), Melbourne, Australia.Desel, J., Oberweis, A., & Zimmer, T. (1997). Validation information system models: Petrinets test case generation. Proceedings 1997 IEEE International Conference Systems, Man Cybernetics: Computational Cybernetics Simulation,pp. 34013406, Orlando, Florida.384fiRepresenting Conversations Scalable OverhearingFinin, T., Labrou, Y., & Mayfield, J. (1997). KQML agent communication language.Bradshaw, J. (Ed.), Software Agents. MIT Press.FIPA site (2003). Fipa - Foundation Intelligent Physical Agents, www.fipa.org..FIPA Specifications (2003a). Fipa Brokering Interaction Protocol Specification, version H,www.fipa.org/specs/fipa0000033/..FIPA Specifications (2003b). Fipa Contract Net Interaction Protocol Specification, versionH, www.fipa.org/specs/fipa0000029/..FIPA Specifications (2003c). Fipa Interaction Protocol Library Specification, version E,www.fipa.org/specs/fipa0000025/..FIPA Specifications (2003d). Fipa Query Interaction Protocol Specification, version H,www.fipa.org/specs/fipa0000027/..Gutnik, G., & Kaminka, G. (2004a). Towards formal approach overhearing: Algorithmsconversation identification. Proceedings AAMAS-04.Gutnik, G., & Kaminka, K. A. (2004b). scalable Petri net representation interactionprotocols overhearing.. van Eijk, R. M., Huget, M., & Dignum, F. (Eds.), AgentCommunication LNAI 3396: International Workshop Agent Communication, AC2004, New York, NY, USA, pp. 5064. Springer-Verlag.Hameurlain, N. (2003). MIP-Nets: Refinement open protocols modeling analysiscomplex interactions multi-agent systems. Proceedings 3rd InternationalCentral Eastern European Conference Multi-Agent Systems (CEEMAS-03), pp.423434, Prague, Czech Republic.Huber, P., Jensen, K., & Shapiro, R. M. (1991). Hierarchies Coloured Petri nets.Jensen, K., & Rozenberg, G. (Eds.), High-level Petri Nets: Theory Application,pp. 215243. Springer-Verlag.Jensen, K. (1997a). Coloured Petri Nets. Basic Concepts, Analysis Methods PracticalUse, Vol. 1. Springer-Verlag.Jensen, K. (1997b). Coloured Petri Nets. Basic Concepts, Analysis Methods PracticalUse, Vol. 2. Springer-Verlag.Jensen, K. (1997c). Coloured Petri Nets. Basic Concepts, Analysis Methods PracticalUse, Vol. 3. Springer-Verlag.Kaminka, G., Pynadath, D., & Tambe, M. (2002). Monitoring teams overhearing:multi-agent plan-recognition approach. JAIR, 17, 83135.Khomenco, V., & Koutny, M. (2000). LP deadlock checking using partial order dependencies. Proceedings 11th International Conference Concurrency Theory(CONCUR-00), pp. 410425, Pennsylvania State University, Pennsylvania.Kone, M. T., Shimazu, A., & Nakajima, T. (2000). state art agent communication languages. Knowledge Information Systems, 2, 258284.Legras, F. (2002). Using overhearing local group formation. Proceedings AAMAS02.385fiGutnik & KaminkaLin, F., Norrie, D. H., Shen, W., & Kremer, R. (2000). schema-based approach specifying conversation policies. Dignum, F., & Greaves, M. (Eds.), Issues AgentCommunications, Lecture notes Computer Science, pp. 193204. Springer-Verlag.Ling, S., & Loke, S. W. (2003). MIP-Nets: compositional model multi-agent interaction.Proceedings 3rd International Central Eastern European ConferenceMulti-Agent Systems (CEEMAS-03), pp. 6172, Prague, Czech Republic.Mazouzi, H., Fallah-Seghrouchni, A. E., & Haddad, S. (2002). Open protocol designcomplex interactions multi-agent systems. Proceedings First InternationalJoint Conference Autonomous Agents & Multi-Agent Systems (AAMAS-02), pp.517526, Bologna, Italy.Milner, R., Harper, R., & Tofte, M. (1990). Definition Standard ML. MIT Press.Moldt, D., & Wienberg, F. (1997). Multi-agent systems based Coloured Petri nets.Proceedings 18th International Conference Application Theory PetriNets (ICATPN-97), pp. 82101, Toulouse, France.Novick, D., & Ward, K. (1993). Mutual beliefs multiple conversants: computationalmodel collaboration air traffic control. Proceedings AAAI-93, pp. 196201.Nowostawski, M., Purvis, M., & Cranefield, S. (2001). layered approach modelingagent conversations. Proceedings Second International Workshop Infrastructure Agents, MAS Scalable MAS, Fifth International ConferenceAutonomous Agents, pp. 163170, Montreal, Canada.Odell, J., Parunak, H. V. D., & Bauer, B. (2000). Extending UML design multiagent systems. Proceedings AAAI-2000 Workshop Agent-Oriented Information Systems (AOIS-00).Odell, J., Parunak, H. V. D., & Bauer, B. (2001a). Agent UML: formalism specifyingmulti-agent interactions. Ciancarini, P., & Wooldridge, M. (Eds.), Agent-OrientedSoftware Engineering, pp. 91103. Springer-Verlag, Berlin.Odell, J., Parunak, H. V. D., & Bauer, B. (2001b). Representing agent interaction protocols UML. Ciancarini, P., & Wooldridge, M. (Eds.), Agent-Oriented SoftwareEngineering, pp. 121140. Springer-Verlag, Berlin.Parunak, H. V. D. (1996). Visualizing agent conversations: Using enhances Dooley graphsagent design analysis. Proceedings Second International ConferenceMulti-Agent Systems (ICMAS-96).Paurobally, S., & Cunningham, J. (2003). Achieving common interaction protocols openagent environments. Proceedings Workshop Challenges Open AgentSystems, Second International Joint Conference Autonomous Agents & MultiAgent Systems (AAMAS-03), Melbourne, Australia.Paurobally, S., Cunningham, J., & Jennings, N. R. (2003). Ensuring consistencyjoint beliefs interacting agents. Proceedings Second International JointConference Autonomous Agents & Multi-Agent Systems (AAMAS-03), Melbourne,Australia.386fiRepresenting Conversations Scalable OverhearingPetri Nets site (2003). Petri nets world: Online services international petri netscommunity, www.daimi.au.dk/petrinets..Poutakidis, D., Padgham, L., & Winikoff, M. (2002). Debugging multi-agent systems usingdesign artifacts: case interaction protocols. Proceedings First International Joint Conference Autonomous Agents & Multi-Agent Systems (AAMAS-02),pp. 960967, Bologna, Italy.Purvis, M. K., Hwang, P., Purvis, M. A., Cranefield, S. J., & Schievink, M. (2002). Interaction protocols network environmental problem solvers. Proceedings2002 iEMSs International Meeting:Integrated Assessment Decision Support(iEMSs 2002), pp. 318323, Lugano, Switzerland.Ramos, F., Frausto, J., & Camargo, F. (2002). methodology modeling interactionscooperative information systems using Coloured Petri nets. International JournalSoftware Engineering Knowledge Engineering, 12 (6), 619636.Reisig, W. (1985). Petri Nets: Introduction. Springer-Verlag.Rossi, S., & Busetta, P. (2004). Towards monitoring group interactions social rolesvia overhearing. Proceedings CIA-04, pp. 4761, Erfurt, Germany.Smith, I. A., & Cohen, P. R. (1996). Toward semantics agent communicationslanguage based speech-acts. Proceedings AAAI-96.Wikstrom, A. (1987). Functional Programming using Standard ML. International SeriesComputer Science. Prentice-Hall.Xu, H., & Shatz, S. M. (2001). agent-based Petri net model applicationseller/buyer design electronic commerce. Proceedings 5th InternationalSymposium Autonomous Decentralized Systems (ISAD-01), pp. 1118, Dallas,Texas, USA.387fi fffiff ff!ff#"%$&(')$**+,-$..0/1$+234567 8%*9:ff*&<;>=5"8%*$:ff*+?@BADCFE(GIH4JDKMLON(PDCQHRSFHT7RVUXWDCFEYPDKYW[Z\N(]^`_INbadceNb^fCFTgWhjilkmilkgn-opQqsrqut-vYwexzy{n-kg|~} -r#kjg-y{xeon-kkr#kgjItbrrtkxYnsfirnqso#F0 fi ffF0 fi ggffs-Yg Q 4`sFf{s)0! (40)0)4Q1s%!<40)>>04sff>Q04s0<0! s)> Y>0(<fiI>ff>040 4<u04) 0 s>0s0 >g%10 sl 004s0<0! s!<>ffff!ss740f0Y>ff>04b>4ff(0ffs#>44D0s4ffsI>usff 0 s>>0e40)0)>DF)0 0f1sYs0 0I40)0)0< 00 efis40)>7 0 l040!> 7 > <1[s\40)0)D4>>)!lfi 4<zsF0(s0 >Q10 se ef04) 0 s>0`!<>ffff!ss-0)g>40<!4ff0 4fiff4ff 0fffi0I1s(s0 0)10 s>!0)> zfiI!<>ffff I004) <!ssF(04) <z!<>ffff0fi<4>!` 7s0< >!0)> s)sff 0> ff<>4>0401s00> 1s s4 > <f>Is 0f40)0))<e7 0D>!0)>40)0)44fi1sV444 s0<efiz04) <e!<>fffffi0<! 0Ys0fs4ff 0`40)0)>)`!>Q0Y04) <!<>ffff00I 0 ss040)0))(4 > 0<fis>0 zIs0 >04sff>)>4ff4(7 7)fffi<`>f!Y <-> -4 7)ff!< 7b)>0407)!`)40)4>07 Ffs>0)!sz)s%sz0s> !<e0040 0 s>Q)> gs0 0 7)fff>0)>4ff>!0) 0< 74) <!<>ffffY)F1sY0f(Q<0 sb z!s7>0<!(s> 4sg>g> b> !)F04s<>)ff>ffs040 < 0 s%>07s> - 7)ff04) <!<>ffff4><0<f0`<0 ssI040>-<s!0<se0fs0 >>e04) <I!<>ffffe!s<4 I0s> 74004) <!<>ffff)<0 ss> l!> bff 0<)sff 0 s> ff<>4Y0>> )>0s%ff 0<7>D!> ff 0<04e0s%!> !4<0!s7ff 0<4fffi(!#"%$'&)(+*,*+-.!"/&10243+"%57643,898:3,445:';=<ff6>?&1"%57"%576@AB&"/&10%&CD"FEff6.?G8:3,4'&102H1IKJL9M-N3,4C } -JL9M-'@PEQ$45:R2$S3,8:H/6T?G3+0%"%5:R157?G3+"/&CU5:V"%$'&D(+*,*(WRX6>?&1"%57"%576@Y5:HZ3["/&>?6,023,8]\^JL_L!9`\W?G8:3,4'&10@6,?4"%5:>a3,8Bbc:d1cfecV>a3+g,&H/?G3, h"a5:HiG3,H/&CT6T"/&>?6,023,8j0%&1;,0%&H%H%576WH/&3+02R2$WEQ57"%$T3,k3,C4>a5:H%H%57iG87&$'&l'025:H/"%5:RMR13,8:87&Cm $ JL$'&ff"/&>?6,023,8'm $ $'&l'025:H/"%5:RM5:H 3,n5:4H/"%3,4RX&ff6,o4"%$'&`;,&'&1023,8'm'prqtsvuxw+y2(zy1{1{1{:|o}3,>a5:87~6,o$'&l'025:H/"%5:R1H1@ffEQ$45:R2$k5:HC'&X '&CTiz~W3D?G3+023,>&1"/&102571&CW0%&8:3'3+"%576k6,oB"%$'&=6,?4"%5:>a3,8QRX6H/"o}l44RX"%576W6,&10Z"%$'&.H/&3+02R2$SH/?G3,RX& } - 5:H5:C'&"%5:R13,8Q"/6[JL9M-D&X'RX&1?4")"%$43+")57"l4H/&H3D0%&RX&"%87~C'&1,&876,?&C>&1"%$'6^C@4R13,8:87&C.d%X7z%%+d/X@4"/65:>?40%6,&B"%$'&]m $ $'&l'025:H/"%5:Rnq}Y3,H%8:l4>Z@G(+*,*+-3|_L&1;,0%&H%H%576?G8:3,4'&102HMR13+0%0%~6l'"ff3KH/&3+02R2$6,&10jH/&1"%H6,o;,63,8:H1@'H/"%3+0%"%5:';ot0%6>"%$'&Q;,63,8G;57,&5:"%$'&P?G8:3,445:';?40%6,iG87&> JL$'&P0%&8:3'3+"%576"%$43+"L87&3,C4Hff"/6"%$'&m'pr$'&l'025:H/"%5:R1Hff5:Hff"/63,H%H%l4>&P"%$43+"j"%$'&RX6H/"M6,o3,~nH/&1"M6,o>6,0%&j"%$43,s;,63,8:HM&zl43,8:H "%$'&LRX6H/"M6,o "%$'&L>6H/"MRX6H/"%87~H%l'iGH/&1" 6,oH%571&js JL$'&$'&l'025:H/"%5:RR13,l44C'&10%&H/"%5:>a3+"/&]"%$'&nRX6H/"B6,o3;,63,8H/&1"1@57o"%$'&10%&3+0%&5:"/&1023,RX"%5764HP5:,687^5:';>6,0%&G 2 Wf22XXG/2 /`jh]}2/ L%/%F2j2+%F !A2+2j2 !XX!4`!2F% ! h[! %/Y/%F FP,hhA!2!,+ff 1 Y+hjh z,/Q +2,h/Gf$**+ 8 0" 8fi44"%$43,=s;,63,8:H1@ iGl'"Q57"QR13,N'&1,&10Y6,&10%&H/"%5:>a3+"/& !"%$'&]JL9M-3,4C } - "/&>?6,023,8?G8:3,4'&102H1@4"%$'&RX6H/"P6,off3H/&1"P6,oM;,63,8:HB5:HQ"%$'&n>a5:45:>]l4>"/6,"%3,8 &X^&R1l'"%576N"%5:>&,@6,0B>a3+g,&H/?G3,@6,o3,~?G8:3,="%$43+"3,R2$457&1,&HB"%$'&n;,63,8:H1@iGl'"Y"%$'&H%3,>&0%&8:3'3+"%576.R13,.i&nl4H/&CN"/6C'&10257,&n$'&l'025:H/"%5:R1HQot6,0B0%&1;,0%&H%H%576?G8:3,445:';EQ57"%$=C45&10%&"Q?G8:3,=H/"/02l4RX"%l'0%&HY3,4CNRX6H/"P>&3,H%l'0%&H1@Mc c:@H/&zl'&"%5:3,8A?G8:3,4HLEQ57"%$=H%l4>6,o3,RX"%576SRX6H/"%Hq}Y3,H%8:l4>Z@jff6'&1"1@LY&X'&10@Y(+*,*|2@Q6,0"/6&H/"%5:>a3+"/&N0%&H/6l'02RX&NRX64H%l4>?4"%576q}Y3,H%8:l4>Y&X'&10@M(+*,*'w| 6,02>a3,8:87~,@"%$'&)m'p$'&l'025:H/"%5:R+@ot6,0n3,~s@ R13,Di&C'&X '&CD3,HK"%$'&H/68:l'"%576K"/6B3Y0%&8:3'3+"%576n6,o4"%$'&`6,?4"%5:>a3,8^RX6H/" &zl43+"%576qtg^'6EQn3,H "%$'&jff&8:8:>a3,K&zl43+"%576 |EQ$45:R2$R2$43+023,RX"/&102571&H"%$'&n6,?4"%5:>a3,8RX6H/"Bo}l44RX"%576N6,&10"%$'&aH/&3+02R2$#H/?G3,RX& RX6>?G87&1"/&aH/68:l'"%576N"/6Z"%$'&0%&8:3^&C[&zl43+"%576W5:HRX6>?Gl'"/&CT&X^?G8:5:R157"%87~,@iz~[H/687^5:';3.;,&'&1023,8:571&CTH%$'6,0%"/&H/"a?G3+"%$[?40%6,iG87&>Z@?402576,0K"/6.H/&3+02R2$3,4CH/"/6,0%&C5:D3="%3+iG87&EQ$45:R2$D5:HKl4H/&CD"/6.R13,8:R1l48:3+"/&$'&l'025:H/"%5:R+3,8:l'&H6,oQH/"%3+"/&HC4l'025:';nH/&3+02R2$JL$'&]?G3+023,>&1"/&10Ps6+&102HB3"/023,C'&X6+i&1"FEff&1&N"%$'&n3,R1R1l'023,RX~=6,o"%$'&n$'&l'025:H/"%5:R3,4CN57"%HYRX6>n?Gl'"%3+"%57643,8RX6H/"1I"%$'&)$457;$'&10s@ "%$'&>6,0%&)H%l'i4;,63,85:"/&1023,RX"%5764H]3+0%&"%3+g,&[5:"/6N3,R1RX6l4"3,4C"%$'&R1876H/&10P"%$'&n$'&l'025:H/"%5:RK5:HQ"/6)"%$'&n"/02l'&nRX6H/"B6,o3,8:8A;,63,8:HB5:N3)H/"%3+"/&,@EQ$45:87&6."%$'&]6,"%$'&10$43,4C@RX6>?Gl'"%5:';B"%$'&LH/68:l'"%576"/6"%$'&L0%&8:3^&CaRX6H/"M&zl43+"%5765:H?687~^'6>a5:3,8z5:"%$'&QH%571&j6,o"%$'&L?40%6,iG87&>qt"%$'&=zl4>Ki&10n6,oB3+"/6>aH|]iGl'"n&X^?6'&"%5:3,8j5:[s ff&R13,l4H/&"%$'&R1l'0%0%&"a>&1"%$'6^CTot6,0RX6>?Gl'"!5:';)"%$'&$'&l'025:H/"%5:R]RX6>?Gl'"/&H3D2+Q :1eFH/68:l'"%576."/6"%$'&0%&8:3^&C#RX6H/"&zl43+"%576@A"%$'&a$'&l'025:H/"%5:R&X'$457iG57"%Hot6,0>6H/"]?G8:3,445:';?40%6,iG87&>aHK3S/C45:>a5:45:H%$45:';)>a3+0%;5:43,8;3,5:4^Ia64RX&)s;,6z&Hn6,&103RX&10%"%3,5:="%$'0%&H%$'68:CDqt"F~z?G5:R13,8:87~,@4sur(|j"%$'&]5:>?40%6,&>&"Qi40%6l';$"Liz~Z"%$'&Kl4H/&K6,oMm'p`AY6,&10m'pi&RX6>&HjH%>a3,8:87&10`ot6,0L5:4RX0%&3,H%5:';]s JL$45:HffRX6>KiG5:'&H`"/6a>a3+g,&B"%$'&B>&1"%$'6^CZRX6H/"j&X&RX"%57,&,@G5:"%$'&H/&4H/&`"%$43+""%$'&L$'&l'025:H/"%5:RM0%&C4l4RX&HH/&3+02R2$"%5:>&L>6,0%&`"%$43,n"%$'&j"%5:>&`0%&zl4570%&C]"/6RX6>?Gl'"/&j57"1@6487~ot6,0YH%>a3,8:8+3,8:l'&HL6,osqt"F~z?G5:R13,8:87~,@GsU(| Q6Eff&1,&10@"%$'&]m $ $'&l'025:H/"%5:RP5:HL6,ot"/&="/6z6Eff&3+g JL$'&zl'&H/"%576)3,C4C'0%&H%H/&C$'&10%&P5:H`57o3n>6,0%&B3,R1R1l'023+"/&Y3,4CRX6H/"j&X&RX"%57,&Y$'&l'025:H/"%5:RLR13,)i&PC'&10257,&C5:D"%$'&m'pot023,>&1Eff6,0%g JL$'&5:C'&3=6,oQ0%&8:3^&C[H/&3+02R2$T5:HK"/6RX6>?Gl'"/&m'pqtot6,0$457;$'&10]sN|6487~'+dXe}+t "/63,65:CZ"%$'&B&X^?6'&"%5:3,85:4RX0%&3,H/&5:)RX6>?Gl'"%3+"%57643,8RX6H/" JL$'&3,87"/&10243+"%57,&BEff6l48:C)6,oRX6l'02H/&Bi&P"/6a3+iG3,4C'6)"%$'&m'prot023,>&1Eff6,0%g3,4CZ876z6,g3+"j6,"%$'&10Q3+?4?40%63,R2$'&Hj"/6aC'&10257^5:';n3,C4>a5:H%H%5iG87&B$'&l'025:H/"%5:R1H`ot6,0L6,?4"%5:>a3,8"/&>?6,023,8?G8:3,445:';'@ziGl'"j"%$'&10%&3+0%&K'6,"Q>a3,~"/6ai&Bot6l44CI &X'5:H/"%5:';>a3+g,&H/?G3,^6,?4"%5:>a3,84"/&>?6,023,8'?G8:3,4'&102H &57"%$'&10l4H/&L"%$'&L"/&>?6,023,8Gm $ $'&l'025:H/"%5:RBq%c c:@'<`9JB@,P5:C43,8Y&X'&10@G(+*,*+-z|ff6,0L6,i4"%3,5:)&H/"%5:>a3+"/&Hjot0%6>3n"/&>?6,023,8?G8:3,445:';K;,023+?G$q%c c:@4JYP9@ \^>a57"%$)&8:C@jw,,z@M3,4CJL9`\z~^H1@MP3+0%025:C'6'@MP43,5:4C45:3^@`3+0%i&10@(+*,*'w|2@MEQ$45:R2$D3,8:H/6=&4RX6^C'&HK"%$'&)m $$'&l'025:H/"%5:RY"%$'6l';$=RX6>?Gl'"/&C=5:3aC45&10%&"Lo}3,H%$4576 JL$'&KC'6>a3,5:^F5:4C'&1?&4C'&"`$'&l'025:H/"%5:R1Hjl4H/&C5:6,"%$'&10`"/&>?6,023,8G?G8:3,4'&102H1@H%l4R2$)3,H`A9`P9q}6'; 6@G(+*,*|6,0`F'JA&Jxq}JA025:4zl43+0%"1@'(+*,*|2@3+0%&al4H/&C."/6&H/"%5:>a3+"/&"%$'&aC45:H/"%3,4RX&"/6Z"%$'&a'&3+0%&H/"KH/68:l'"%576.5:N"%$'&aH/&3+02R2$DH/?G3,RX&,@A023+"%$'&10B"%$43,"%$'&KRX6H/"]qhc}c:@4>a3+g,&H/?G3, |ff6,o "%$43+"QH/68:l'"%576JL$'&)0%&8:3'3+"%576Tl44C'&10287~^5:';="%$'&=m'p$'&l'025:H/"%5:R1HnR13,[i&)&X^?G8:3,5:'&C[5:"/&102>aH6,oY"%$'&ZH/&3+02R2$H/?G3,RX&,@A023+"%$'&10"%$43,5:."/&102>aHB6,o`H/68:l'"%576.RX6H/"1IK3,~H/&1"B6,o`>6,0%&"%$43,s;,63,8:H5:HZ/H/?G8:57"%)5:"/6?40%6,iG87&>aHQ6,os;,63,8:HP&3,R2$@EQ$45:R2$.3+0%&H/687,&C.5:4C'&1?&4C'&"%87~#qt"%$'&H/?G8:57"Y5:HP'6,"B3?G3+0%"%57"%57645:';'@H%5:4RX&+tH%l'iGH/&1"%HY6,o`H%571&ns3+0%&aH/687,&C | JL$'&n0%&8:3^&CRX6H/"B&zl43+"%5765:HB3,8:H/6)"%$'&6,?4"%5:>a3,8 RX6H/"&zl43+"%576Not6,0B"%$45:HQ0%&8:3^&C.H/&3+02R2$#H/?G3,RX&,@EQ$45:R2$= 8:8R13,8:8"%$'&nshd%,d%2+H/?G3,RX& JL$'&]0%&8:3^&CH/&3+02R2$>&1"%$'6^C.RX64H%5:H/"%HY5:NH/687^5:';"%$'&n?G8:3,445:';a?40%6,iG87&>qhc}c:@H/&3+02R2$45:';ot0%6>"%$'&n"/6,?87&1,&8;,63,8:H| 5:]"%$'&js)0%&1;,0%&H%H%576aH/?G3,RX& l'025:';P"%$'&LH/&3+02R2$@?G3+0%"%H 6,o "%$'&LH/68:l'"%576n"/6B"%$'&L0%&8:3^&CRX6H/"&zl43+"%5763+0%&PC45:H%RX6,&10%&C@'3,4Ca"%$'&H/&P3+0%&PH/"/6,0%&C)5:3"%3+iG87&Yot6,0`8:3+"/&10`l4H/&,@%l4H/"ff3,H`5:a"%$'&Q?40%&1^576l4H3+?4?40%63,R2$ ff&R13,l4H/&`"%$'&0%&8:3^&C]H/&3+02R2$]6487~P^5:H%57"%H '+dXez6,o4"%$'&s)0%&1;,0%&H%H%576]H/&3+02R2$]H/?G3,RX&,@57"R13,i&L&X^?&RX"/&C"/6Ki&L3+iG87&L"/6nC'6KH/6]>6,0%&Qzl45:R%g^87~]"%$43,>&1"%$'6^C4HM"%$43+"iGl45:8:Cn3KH/68:l'"%576"/6K"%$'&YRX6H/"&zl43+"%576not6,0M"%$'&j&"%570%&`s)0%&1;,0%&H%H%576H/?G3,RX& <ff64H/&zl'&"%87~,@57"MR13,ni&j3+?4?G8:57&C]ot6,0M$457;$'&10+3,8:l'&H>fi (^z7G4 % : 4^-G-Ns4-G^1 f6,o \^5:4RX&B"%$'&0%&8:3^&C=H/&3+02R2$N5:HLC'6'&ot0%6>"%$'&K;,63,8:HL6,o"%$'&?G8:3,445:';n?40%6,iG87&>Z@'"%$'&K?G3+0%"L6,o"%$'&)m'pH/68:l'"%576"%$43+"]5:HKRX6>?Gl'"/&C5:HK3,8:H/6N8:57g,&87~Ni&a"%$'&>6H/"K0%&87&1+3,"K?G3+0%" JL$'&RX6>?G87&1"/&3,4C.?G3+0%"%5:3,8Mm'p$'&l'025:H/"%5:R1HPRX6>?Gl'"/&C.ot6,0KC45&10%&"Psiz~N"%$'&"FEff6=>&1"%$'6^C4HBR13,i&RX6>KiG5:'&Ciz~>a3'5:>a573+"%576@z0%&H%l487"%5:';]5:3n>6,0%&B3,R1R1l'023+"/&B 43,8 $'&l'025:H/"%5:R+@^$'6,?&1o}l48:87~3+"j3nRX6>?Gl'"%3+"%57643,8RX6H/"L"%$43+"Q5:HL'6,"j;,0%&3+"/&10Y"%$43,57"%Hj+3,8:l'&JL$45:Hn?G3+?&10>a3+g,&H"FEff6>a3,5:WRX6"/0257iGl'"%5764H1I 5702H/"1@`57"?40%6^5:C'&Ha3DC'&1"%3,5:87&CT?40%&H/&"%3+"%5766,oP"%$'&Z0%&8:3^&CWH/&3+02R2$k>&1"%$'6^CW3,4CT$'6E"%$45:H>&1"%$'6^CW5:H3+?4?G8:57&C"/6DR18:3,H%H%5:R13,8L3,4CT"/&>?6,023,80%&1;,0%&H%H%576[?G8:3,445:'; OD 87"%$'6l';$["%$'&>&1"%$'6^CT5:Hn?40%&H/&"/&CT5:["%$'&RX6"/&X^"a6,oY?G8:3,445:';'@M57"5:Hzl457"/&N;,&'&1023,8P3,4CV>a3~ki&.3+?4?G8:5:R13+iG87&"/6[6,"%$'&10ZH/&3+02R2$S?40%6,iG87&>aH3,H)Eff&8:8 !4C'&1&C@QH%5:>a5:8:3+0"/&R2$445:zl'&H$43,&Yi&1&3+?4?G8:57&Cn"/6K?G8:3,445:';B3,4Ca6,"%$'&10`H%5:';87&L3+;,&"`H/&3+02R2$?40%6,iG87&>aHQq}9M0257&C457"%5:H1@w,,z`l4';$43,44HKv\^R2$43+&X&10@(+*,*'w|K3,4C#"/6.RX64H/"/023,5:"K6,?4"%5:>a573+"%576VqP5:4H/i&10%;NY3+0%,&1~,@w,,(z&10%o}3,5:8:8:57&,@&>a3,57"/0%&,@+S\^R2$457&X@'w,,| JL$'&M0%&8:3+"%576K"/6Y"%$'&H/&ff5:C'&3,H3,4C"/&R2$445:zl'&H5:HA3,8:H/6C45:H%R1l4H%H/&C \z&RX64C@57"?40%&H/&"%H"%$'&`0%&H%l487"%HA6,o 3,]&X^"/&4C'&Cn3,43,87~^H%5:HA6,oG"%$'&ff0%&8:3+"%57,&`?&10%ot6,02>a3,4RX&6,oLJL9M-N3,4C } - 5:#"%$'&C'6>a3,5:4H6,oj"%$'&?G8:3,445:';ZRX6>?&1"%57"%576 JL$'&?G5:RX"%l'0%&a"%$43+"K&>&10%;,&Hot0%6>"%$45:H3,43,87~^H%5:HM5:HH/6>&1EQ$43+"ffC45&10%&"Mot0%6>"%$43+"ff;57,&aiz~"%$'&YRX6>?&1"%57"%576a0%&H%l487"%H !a?G3+0%""%$45:HQ5:HYC4l'&K"/6"%$'&]"%5:>&X?&10/?40%6,iG87&>8:5:>a57"L5:>?6H/&C=5:"%$'&nRX6>?&1"%57"%576@ H%5:4RX&K"%$'&n3,C'+3,"%3+;,&6,o } - 6,&10aJL9M-N5:H]>a3,5:487~N6/$43+02C4?40%6,iG87&>aH1@EQ$45:R2$#0%&zl4570%&3N876,"K6,oj"%5:>&"/6.H/687,&ot6,0i6,"%$=?G8:3,4'&102H !=?G3+0%"1@57"Y5:HY3,8:H/6i&R13,l4H/&]"%$'&],&102H%576N6,o } - l4H/&C.5:"%$'&nRX6>?&1"%57"%576NE`3,HiGl';,;,~ JL$'&>a3,5:0%&H%l487"6,oj"%$'&3,43,87~^H%5:H1@$'6Eff&1,&10@M5:HK3NR2$43+023,RX"/&102573+"%5766,oj"%$'&C'6>a3,5:4H5:EQ$45:R2$n0%&8:3^&CaH/&3+02R2$aR13,ai&`&X^?&RX"/&Ca"/6i&jRX6H/"M&X&RX"%57,&,IM5:H%l4R2$aC'6>a3,5:4H1@,&X^?G3,4C45:';PH%>a3,8:8H/"%3+"/&HM5:H RX6>?Gl'"%3+"%57643,8:87~KR2$'&3+?&10"%$43,]&X^?G3,4C45:';P8:3+0%;,&`H/"%3+"/&H1@z3,4CnH%>a3,8:8^H/"%3+"/&H"/&4C]"/6$43,&H%>a3,8:8'H%l4R1RX&H%H/6,0ffH/"%3+"/&H h"5:HM3,8:H/6KH%$'6EQ"%$43+"M"%$'&H/&QRX0257"/&1025:3BR13,ai&KqtEff&3+g^87~4| zl43,"%5G&Ciz~n"FEff6>&3,H%l'0%&H1@5:,687^5:';Y"%$'&`0%&8:3+"%57,&`0%&1;,0%&H%H%576ni4023,4R2$45:';Qo}3,RX"/6,02HM3,4C]"%$'&LH%571&ff6,oH/"%3+"/&H;,&'&1023+"/&Ciz~0%&1;,0%&H%H%576@45:)"%$'&6,0257;5:43,83,4CZ0%&8:3^&CH/&3+02R2$=H/?G3,RX&HQAFJ $'&JL9M-?G8:3,4'&10B 4C4HB"/&>?6,023,8?G8:3,4HBot6,0a\^JL_L!9`\=?40%6,iG87&>aHBEQ57"%$#C4l'023+"%57,&3,RX"%5764H JL$'&L?G8:3,4Hot6l44C3+0%&j6,?4"%5:>a3,8bc:d1cfec>a3+g,&H/?G3,@4c}c:@"%$'&j"/6,"%3,84&X^&R1l'"%576"%5:>&j6,o "%$'&j?G8:3, $ @3,4C"%$'&?G8:3,4'&10M5:HM3,8:H/6K3+iG87&j"/6K&4H%l'0%&j"%$43+"?G8:3,4HC'6K'6,"M^5768:3+"/&QRX&10%"%3,5:ag^5:4C4H 6,o0%&H/6l'02RX&QRX64H/"/023,5:"%HJL$'&n>a3,5:=Eff6,0%g^5:';?4025:4R157?G87&Hj6,oJL9M-)3+0%&3ot6,02>]l48:3+"%576=6,off30%&1;,0%&H%H%576.H/&3+02R2$.H/?G3,RX&not6,0"/&>?6,023,8ff?G8:3,445:';=3,4CD"%$'&m'po}3,>a5:87~.6,oY3,C4>a5:H%H%57iG87&$'&l'025:H/"%5:R1H1@i40%6l';$"n"/6,;,&1"%$'&10a"%$'0%6l';$"%$'&] LH/&3+02R2$3,87;,6,0257"%$4> JL$'&H/&,@3,4CN"%$'&]6,&1023,8:83+02R2$457"/&RX"%l'0%&]6,o"%$'&]?G8:3,4'&10@ 3+0%&ni40257 & G~C'&H%RX0257i&C]5:K"%$45:HH/&RX"%576z>6,0%&`C'&1"%3,5:8:H6n"%$'&ff?G8:3,4'&10 R13,ni&ffot6l44C]5:]&3+028:57&10?G3+?&102Hjq}Y3,H%8:l4>Y&X'&10@M(+*,*,*^@M(+*,*'w+ffY3,H%8:l4>Z@(+*,*+-,i| JA6N?40%6^5:C'&aiG3,R%gz;,0%6l44C#ot6,0n3=R187&3+0%&10nC'&H%RX0257?4"%5766,o0%&8:3^&CH/&3+02R2$=5:"%$'&K'&X^"LH/&RX"%576@GH/&3+02R2$H/?G3,RX&K3,4C$'&l'025:H/"%5:R1H`3+0%&B&X^?G8:3,5:'&CG02H/"jot6,0L"%$'&H%5:>?G87&10jR13,H/&K6,oH/&zl'&"%5:3,8?G8:3,445:';'@^ot68:876Eff&CZiz~)"%$'&570L3,C43+?4"%576"/6a"%$'&"/&>?6,023,8R13,H/& O` 8:H/6'@RX&10%"%3,5:"/&R2$445:R13,8 C'&1"%3,5:8:HM"%$43+"`3+?4?&3+0ff5:>?6,0%"%3,"5:a&X^?G8:3,5:45:';B"%$'&Qi&$43^576l'06,o } - 0%&8:3+"%57,&"/6JL9M-a5:)"%$'&KRX6>?&1"%57"%576C'6>a3,5:4H`EQ5:8:8Gi&$457;$48:57;$"/&C1fffj ,P2/ !Xh/+%jP 2L2X!+2 L L2 !,!hLKP!L21 !`+%fi4L2j!2A!j,2F2z 2 FB hAj +%A T+%A,!! +/Y2fi^1 Q7!!)1 1 %FZz]L2XQ 2j2!!!F2 Pfi^1 P 22 %2L j j!!!%a] hjj!+2!!j 1ff 2 h/2 ` L2X!+2Y 1 1>fi44-q>qsrtkmxeon-kkr#kg"!#F%$Qgkrn-ofInq>&Q3,H%H%l4>&`"%$'&LH/"%3,4C43+02Cn?40%6,?6H%57"%57643,8'\^JL_L!9`\K>6^C'&8z6,o ?G8:3,445:'; OA ?G8:3,445:';Q?40%6,iG87&>q'&]|RX64H%5:H/"%HM6,o3H/&1"6,o3+"/6>aH1@^3KH/&1"M6,o3,RX"%5764H3,4Ca"FEff6]H%l'iGH/&1"%H6,o3+"/6>aH1I "%$'6H/&Q"/02l'&Q5:n"%$'&Y5:457"%5:3,8H/"%3+"/&.'q (^|B3,4C"%$'6H/&0%&zl4570%&C."/6i&a"/02l'&a5:"%$'&a;,63,8ffH/"%3+"/&.*q )K| ,+ 3,R2$3,RX"%576.-.5:HC'&H%RX0257i&Ciz~3.H/&1"n6,oQ?40%&RX64C457"%5763+"/6>aH=q /ff01'q -'|/|2@EQ$45:R2$[$43,&Z"/6$'68:C[5:3.H/"%3+"/&Zot6,0"%$'&Z3,RX"%576["/6i&a&X^&R1l'"%3+iG87&,@ 3,4CDH/&1"%HK6,oj3+"/6>aH]>a3,C'&a"/02l'&N'q -%232 'q -'|/|K3,4Co}3,8:H/&N'q 2%154/'q -'|/|Biz~"%$'&3,RX"%576 OZH/68:l'"%576.?G8:3,.5:H3,&X^&R1l'"%3+iG87&aH/&zl'&4RX&a6,0KH%R2$'&C4l487&n6,oj3,RX"%5764HB&4C45:';Z5:.3H/"%3+"/&aEQ$'&10%&a3,8:8;,63,83+"/6>aH`$'68:C JL$'&Q&X'3,RX"ff?G8:3,aot6,02> C'&1?&4C4H6"%$'&Y>&3,H%l'0%&Q6,?4"%5:>a571&CI 5:a"%$'&PH/&zl'&"%5:3,8R13,H/&,@3RX6H/"P5:HQ3,H%H/6^R15:3+"/&C="/6&3,R2$.3,RX"%576[q'6 7985:1'q -'<| ;S*|2@3?G8:3,5:HQ3H/&zl'&4RX&]6,oM3,RX"%5764H1@3,4C"%$'&KH%l4>6,o "%$'&570LRX6H/"%HQ5:Hj"%$'&RX6H/"Q6,o"%$'&?G8:3,_L&1;,0%&H%H%5765:HK3=?G8:3,445:';>&1"%$'6^CD5:#EQ$45:R2$#"%$'&)H/&3+02R2$ot6,0n3?G8:3,D5:H]>a3,C'&5:#"%$'&H/?G3,RX&6,o.!?G8:3,S"%3,5:8:H%^@Y?G3+0%"%5:3,8P?G8:3,4H"%$43+"3,R2$457&1,&#"%$'&;,63,8:HZ?40%6^5:C'&CS"%$43+"Z"%$'&?40%&RX64C457"%5764H6,o"%$'&)?G3+0%"%5:3,8?G8:3,[3+0%&Z>&1" \z&3+02R2$[&4C4HKEQ$'&[3N?G8:3,D"%3,5:8ffEQ$'6H/&)?40%&RX64C457"%5764HK3+0%&Z3,870%&3,C'~H%3+"%5:H!G&CDiz~"%$'&)5:457"%5:3,8MH/"%3+"/&Z5:Hot6l44C O= 6,0aH/&zl'&"%5:3,8?G8:3,445:';'@"%$'&?40%&RX64C457"%5764H?40%6^5:C'&3H%>l =R157&"aH%l4>a>a3+0%~D6,oY"%$'&Z?G8:3,["%3,5:8 JL$zl4H1@3#H/&zl'&"%5:3,8`0%&1;,0%&H%H%576TH/"%3+"/&N5:H3H/&1"1<@ 8@6,o3+"/6>aH1@0%&1?40%&H/&"%5:';H%l'i4;,63,8:HQ"/6)i&]3,R2$457&1,&C OP N3,RX"%576?-R13,Ni&]l4H/&C="/60%&1;,0%&H%HP3H/"%3+"/& 85@2%154/'q -'ff| AB8PDu Cz@'3,4C"%$'&Y0%&H%l487"6,o0%&1;,0%&H%H%5:';B8Y"%$'0%6l';,$ -a5:EH 85FGu q 8HG@-%232 'q -'|/ff| IJ/ff01'q -'| JL$'&H/&3+02R2$=H/"%3+0%"%HLot0%6>"%$'&H/&1"L6,o ;,63,8:KH ) 3,4CZ&4C4HjEQ$'&Z3aH/"%3+"/L& 8MN(5:H`0%&3,R2$'&CO-q>qsrtkmxeon-kkr#kg"!Dw`bt-n-ofInq>!D"%$'&)R13,H/&Z6,oL"/&>?6,023,8?G8:3,445:';'@A&3,R2$W3,RX"%576[$43,Hn3NC4l'023+"%576Sq'2Pff04q'-'|;*| JL$'&?G8:3,5:H3NH%R2$'&C4l487&,@EQ$'&10%&)3,RX"%5764Hn>a3~D&X^&R1l'"/&Z5:D?G3+023,8:87&8Yq}H%l'i^/&RX"]"/6.0%&H/6l'02RX&)3,4C[RX6>?G3+"%57iG5:8:57"F~RX64H/"/023,5:"%H|2@^3,4C"%$'&Q6,i^/&RX"%57,&Q"/6]>a5:45:>a571&j5:H"%$'&Q"/6,"%3,8G&X^&R1l'"%576a"%5:>&,@6,0ff>a3+g,&H/?G3, S$'&3,RX"%576=C4l'023+"%5764Hj3+0%&3,8:8&zl43,8"/6Nw+@4"%$'&H/?&R15:3,8R13,H/&K6,o?G3+023,8:87&8?G8:3,445:';K0%&H%l487"%HQSRTQSR'UWVYX[Z]\S^.Z"_N`JaBaBbcQSRTQedfZ]g h3ikj _mlff^JL9M-K3,4C } - C'6]'6,"H%l'?4?6,0%"M3,~6,o"%$'&Y'&1Ekot&3+"%l'0%&H5:"/0%6^C4l4RX&Ca5:9 P ( (z@^"%$'&Q?40%6,iG87&>H/?&R15 R13+"%5768:3,';l43+;,&]ot6,0Q"%$'&n(+*,*+-)RX6>?&1"%57"%576q + C'&87g+3,>?.Q6+>a3,4@(+*,*+-z| JL$'&?G8:3,^'&102HjH%l'?4?6,0%"`C4l'023+"%57,&B3,RX"%5764H1@'6,iz^576l4H%87~,@'iGl'"`"%$'&H/&3+0%&5:"/&10%?40%&1"/&CZ5:)3>a3,4'&10j"%$43+"LC45&102Hot0%6>"%$'&N9 P ( w.H/?&R15 R13+"%576q 6U6';'@Q(+*,*| Ox 6,0)?4023,RX"%5:R13,8Q?Gl'0%?6H/&H1@`JL9M-[3,4C} - 3,R1RX&1?4""%$'&9 P ( waH/~^"%3n l4>&1025:RnH/"%3+"/&a+3+025:3+iG87&Haq}R13,8:87&C l'&"%H%Z5:9 P ( w|3+0%&KH%l'?4?6,0%"/&C6487~)5:ZRX&10%"%3,5:Zot6,02>aHj6,o l4H/&JL$'&KH/&>a3,"%5:R1HQ"%$43+"YJL9M-3,4C } - 3,H%H%l4>&ot6,0YC4l'023+"%57,&K3,RX"%5764HY3+0%&K&H%H/&"%5:3,8:87~"%$'6H/&]5:^"/0%6^C4l4RX&Caiz~)\^>a57"%$a3,4C&8:C.q!w,,|ot6,0ff"%$'&YJYP9?G8:3,4'&10 6,0j3,3,RX"%576p-n"/6]i&L&X^&R1l'"%3+iG87&6,&10]3"%5:>&5:"/&10%+3,r8 q :sy :tN2Pff04'q -'*| u@A3+"/6>aH5:o/ff01'q -'|Y>]l4H/"Pi&]"/02l'&3+f" :@3,4C.?&102H%5:H/"/&"Y?40%&XRX64C457"%5764Haq}3+"/6>aHK5:,/1v04'q -'|YDu /ff01'q -'| Gw2%154/'q -'|/|B>]l4H/"B0%&>a3,5:."/02l'&6,&10K"%$'&a&"%570%&a5:"/&10%+3,8+ &RX"%Hn6,oL"%$'&)3,RX"%576"%3+g,&Z?G8:3,RX&3+"H/6>&?65:"]5:D"%$'&)5:"/&102576,0]6,oL"%$'&)5:"/&10%+3,8@3,4CR13,[i&0%&8:57&C6)"/6$'68:C)3+"j"%$'&B&4C)?65:" JjEff6a3,RX"%5764H1@ -3,4Cx-3Ft@43+0%&3,H%H%l4>&C)"/6i&a2+Q',e}* yX:X@ 5:"%$'&]H/&4H/&]"%$43+"Q"%$'&1~=R13,=i&K&X^&R1l'"/&C.5:6,&1028:3+?4?G5:';5:"/&10%+3,8:HLEQ57"%$'6l'"Q5:"/&10%ot&1025:';aEQ57"%$&3,R2$6,"%$'&10`5='&57"%$'&10ff3,RX"%576)C'&87&1"/&Hff3,3+"/6> "%$43+"`5:H3K?40%&RX64C457"%5766,o6,0`3,C4C'&Caiz~a"%$'&Q6,"%$'&10@ c}c:@5z2%154/'q -'"| A{/ff01'q -3F:|M|u 2%154/'q -'"| Ap-%232 'q -3Ft|Du C3,4CZ^5:RX&B,&102H%3JL$45:HA5:"/&10%?40%&1"%3+"%576K6,o C4l'023+"%57,&ff3,RX"%5764H0%&H/?&RX"%H"%$'&n/'6P>6^5:';Y"%3+0%;,&1"%Y02l487&6,oG9 P ( w+@iGl'"ff5:3]C45&10%&"E`3~ IM5:4H/"/&3,C6,o0%&zl457025:';B?G8:3,4HM"/6n&X^?G8:5:R157"%87~nH/&1?G3+023+"/&P3,)3,RX"%576C'&1?&4C45:';6D3=RX64C457"%576ot0%6>"%$'&&X&RX"]"%$43+"K&H/"%3+iG8:5:H%$'&HB"%$'&RX64C457"%576@A"%$'&H/&>a3,"%5:R1H0%&zl4570%&HB"%$43+"~}>fi (^z7G@557;l'0%&aw+I4 % : 4^-G-Ns4-G^1 fp s@ ~,5 s@5 ~255x 52 5 5B5C'6>a3,5: | JL$'&P?G8:3,"/&>?6,023,8?G8:3,qt"%$'&H/68:l'"%576"/6?40%6,iG87&>ff%ot0%6>"%$'&K3S]3,8:H/6.RX6"%3,5:4H]"FEff6#3,RX"%5764He3Z3,4C@EQ$45:R2$3+0%&)'6,"]^5:H%57iG87&ai&R13,l4H/&"%$'&1~$43,&B1&10%6aC4l'023+"%576 RX"%5764Hos%933,4CNs%93+0%&H/&1?G3+023+"/&C)i&R13,l4H/&6,o3n0%&H/6l'02RX&KRX6> 5:RX" JL$'&>a3+g,&H/?G3,6,o "%$'&B?G8:3,Z5:HQ9, (5x5X^+zzaeY 7t[=e}ttGeFXd++ JL$45:HY>a3+g,&HC4l'023+"%57,&n3,RX"%5764HBH/"/025:RX"%87~=87&H%HY&X^?40%&H%H%57,&"%$43,)5:9 P ( w+@'EQ$'&10%&Y&X&RX"%HjR13,i&PH/?&R15G&Ca"/6n"%3+g,&B?G8:3,RX&Y&X'3,RX"%87~3+"ff"%$'&BH/"%3+0%"ff6,0`&4C6,o3,3,RX"%576 !a?G3+0%"%5:R1l48:3+0@57"C'6z&H'6,"ffH%l'?4?6,0%"M3,RX"%5764HM"%$43+"`>a3+g,&Y3KRX64C457"%576"/02l'&Q6487~nC4l'025:';"%$'&570ff&X^&R1l'"%576#qhc}c:@'3,C4C3,Z3+"/6>3+"`"%$'&PH/"%3+0%"`6,oA"%$'&P3,RX"%576Z3,4C)C'&87&1"/&P57"`3+;3,5:)3+"`"%$'&P&4C |2@EQ$45:R2$?40%&1,&"/&C=JL9M-3,4C } - ot0%6>H/687^5:';n"%$'&BRX6>?G5:87&C),&102H%5764H`6,oA?40%6,iG87&>aHffEQ57"%$)"%5:>&C5:457"%5:3,88:57"/&1023,8:H!)?4025:4R157?G87&,@z57"L5:HjRX&10%"%3,5:487~?6H%H%57iG87&Y"/6C'&1^5:H/&B3"/&>?6,023,80%&1;,0%&H%H%576H/&3+02R2$H/?G3,RX&ot6,0L"%$'&9 P ( wQ5:"/&10%?40%&1"%3+"%576n6,oC4l'023+"%57,&L3,RX"%5764H1@z3,87"%$'6l';$H/"%3+"/&H5:n"%$45:HH/?G3,RX&LEff6l48:Cni&`o}3+0>6,0%&RX6>?G87&XZH/"/02l4RX"%l'0%&H1@4C4l'&B"/6a"%$'&K'&1&CZ"/6a0%&1"%3,5:>6,0%&6,o "%$'&B?G8:3,)"%3,5:85:)"%$'&KH/"%3+"/&)qt&'6l';$Z"/65:4R18:l4C'&B"%$'&K&4C?65:"L6,oM3,8:86^;,65:';3,RX"%5764H| JL$'&KA9`P9 q}6';) 6@(+*,*|Q3,4C=JL9`\z~^HqP3+0%025:C'6'@ 6@j6';'@Q(+*,*(|?G8:3,4'&102Hi6,"%$kl4H/&"%$'&=9 P ( wNH/&>a3,"%5:R1H1@L3,4Ck3+0%&=i6,"%$Y023+?G$'?G8:3,.C'&10257+3+"%57,&HB3,4C"%$zl4HR13+0%0%~N6l'"3ZH/&3+02R2$#0%&H/&>KiG8:5:';0%&1;,0%&H%H%576#5:N"%$'&570H/68:l'"%576&X^"/023,RX"%576?G$43,H/&qt"%$'6l';$Zi6,"%$Z?G8:3,4'&102Hff&>Ki6^C'~)>6^C45 R13+"%5764Hff"/6a"%$'&?Gl'0%&87~aiG3,R%gFR2$43,5:45:';H/68:l'"%576)&X^"/023,RX"%576Nl4H/&C5:=Y023+?G$'?G8:3, | Q6Eff&1,&10@5:Z"%$'&?G8:3,445:';nC'6>a3,5:4H`"%$43+"Y$43,&Ki&1&l4H/&CW5:T"%$'&"FEff6?G8:3,445:';.RX6>?&1"%57"%5764HaH%5:4RX&"%$'&=5:"/0%6^C4l4RX"%576[6,oP"/&>?6,023,8L?G8:3,445:';.5:"/69 P M@`3,4CV3,8:H/65:k>6H/"6,o"%$'&=&X'3,>?G87&NC'6>a3,5:4H"%$43+")$43,&.3+?4?&3+0%&CV5:W"%$'&.8:57"/&1023+"%l'0%&,@"%$'&)>a3,5:Dl4H/&6,oL"%$'&H/"/0%6';,&109 P ( w)H/&>a3,"%5:R1H]6,oLC4l'023+"%57,&3,RX"%5764Hn$43,HKi&1&D"/6N&4RX6^C'&RX&10%"%3,5:=ot&3+"%l'0%&H1@H%l4R2$=3,HQ"%$'&]"%5:>&C=5:457"%5:3,88:57"/&1023,8:HLl4H/&C=5:=H/6>&]C'6>a3,5:,&102H%5764HQ5:"%$'&]8:3,H/"RX6>?&1"%57"%576@6,0]/'6^F5:'&10%"%Po}3,RX"%HYqhc}c:@o}3,RX"%HM"%$43+"ffC'6'6,"M?&102H%5:H/" 6,&10ff"%5:>&Ll4487&H%H>a3,5:"%3,5:'&Ciz~[3,W3,RX"%576 | h"a>a3~[,&10%~Eff&8:8`i&Z&3,H%57&10a"/6D3,C4CTH/6>&6,oY"%$'&H/&ot&3+"%l'0%&HaC4570%&RX"%87~D"/6#"%$'&"/&>?6,023,8 0%&1;,0%&H%H%576.ot6,02>]l48:3+"%576l4H/&C.iz~N"%$'&aJL9M-3,4C } - ?G8:3,4'&102H1@"%$'6l';$."%$45:HB$43,HB~,&1""/6ai&B?Gl'"j"/6a"%$'&B"/&H/"n l4>&1025:RH/"%3+"/&ff+3+025:3+iG87&H"%$43+" 3+0%&`l4H/&C)qtiz~3,RX"%5764H|5:KRX&10%"%3,5:]H/?&R15 RME`3~^H 3+0%&`5:"/&10%?40%&1"/&C3,H)0%&H/6l'02RX&Hqt6,0RX6H/">&3,H%l'0%&H1@Y5:VH/&zl'&"%5:3,8Y?G8:3,445:';z|3,4CSH%l'?4?6,0%"/&CWiz~k"%$'&.?G8:3,4'&102H1@"%$'6l';$=EQ57"%$NH/6>&]0%&H/"/025:RX"%5764H JL$'&]l4'0%&H/"/025:RX"/&CNl4H/&K6,ozl4>&1025:RH/"%3+"/&n+3+025:3+iG87&HY3,8:876Eff&C=iz~9 P ( w]5:HQ'6,"YH%l'?4?6,0%"/&C O` >6,0%&KC'&1"%3,5:87&C=C45:H%R1l4H%H%576ZR13,=i&ot6l44C=5:Z"%$'&K?G3+?&10L6NJL9M3,4C } - 5:)"%$'&RX6>?&1"%57"%576Zi6z6,g^87&1"Kq}Y3,H%8:l4>Z@4(+*,*+-,i|QSRTQSRTQf^ffg "Z "j h@ ^S" ^ svi Z"_JA&>?6,023,80%&1;,0%&H%H%576@'%l4H/"8:57g,&aH/&zl'&"%5:3,8M0%&1;,0%&H%H%576@5:H3H/&3+02R2$5:."%$'&H/?G3,RX&6,o`?G8:3,"%3,5:8:HQ6Eff&1,&10@ 5:)"%$'&B"/&>?6,023,8R13,H/&"%$'&H/&1"j6,o ?40%&RX64C457"%576)3+"/6>aHQ5:Hj'6a876';,&10LH%>l =R157&"j"/6H%l4>n>a3+02571&Y3?G8:3,a"%3,5:8IH/"%3+"/&H`$43,&Y"/6]i&Qi&L&X^"/&4C'&CEQ57"%$3,RX"%5764HffRX64R1l'0%0%&"ffEQ57"%$"%$'&PH%l'i4;,63,8:H3,4CT"%$'&"%5:>a5:';6,oP"%$'6H/&=3,RX"%5764Ha0%&8:3+"%57,&"/6D"%$'&=H%l'i4;,63,8:H <ff64H%5:C'&10"%$'&&X'3,>?G87&?G8:3,T5:57;l'0%&nw+@'H/?&R15 R13,8:87~n"%$'&Z/H/"%3+"/&a3+"`"%5:>&B(,+*^IH%5:4RX&Y"%$45:Hff5:Hff"%$'&PH/"%3+0%"%5:';]?65:"ff6,oA3,RX"%576Ns%ff>fi443@57"%H?40%&RX64C457"%5764H >]l4H/"i&j;,63,8:HM"/6i&L3,R2$457&1,&C3+"M"%$45:H ?65:"`l'"M"%$'&L3,RX"%5764HYq}5:4R18:l4C45:';'6+6,?GH|&H/"%3+iG8:5:H%$45:';K"%$'6H/&RX64C457"%5764H`>]l4H/"`i&BRX6>?G3+"%57iG87&PEQ57"%$"%$'&3,RX"%576s%ff@^EQ$45:R2$H/"%3+0%"%HKwl4457"%H`6,o"%5:>&B&3+028:57&10Q3,4C)EQ$'6H/&&X^&R1l'"%576H/?G3,4HL3,RX0%6H%HL"%$45:H`?65:"JL$zl4H1@'3]"/&>?6,023,8 0%&1;,0%&H%H%576)H/&3+02R2$H/"%3+"/&5:H`3]?G3,57<0 8Pu 'q ay ]|2@4EQ$'&10%& 5:H`3H/&1"`6,o3+"/6>aH3,4C{xu z'q - |2y1{1{1{y'q -3 4| `5:HA3YH/&1"A6,o43,RX"%5764H-3'EQ57"%$B"%5:>&ff5:4RX0%&>&"%mH JL$45:H0%&1?40%&H/&"%H3n?G3+0%"%5:3,8 ?G8:3,.qt"%3,5:8t|EQ$'&10%&Y"%$'&3+"/6>aHL5:p>]l4H/"j$'68:C)3,4C)&3,R2$3,RX"%576Dq'-3hy F|ff5:,$43,H`i&1&H/"%3+0%"/&C A"%5:>&Pl4457"%HM&3+028:57&10 9l'"`3,'6,"%$'&10jE`3~,@43,&X^&R1l'"%3+iG87&P?G8:3,.q}H%R2$'&C4l487&|QX ,ffH/"%3+"/&8u'q ay ]|L3+"Q"%5:>& :j5"%$'&KO ?G8:3,>a3+g,&HY3,8:8A3+"/6>aHY5: "/02l'&]3+J" :j3,4C=H%R2$'&C4l487&HQ3,RX"%576-33+""%5:>f& :mG ot6,0L&3,R2$['q -3hy Fc| @S$'&&X^?G3,4C45:';)3H/"%3+"/,& 8u'q ay ]|2@ H%l4R1RX&H%H/6,0]H/"%3+"/&LH 85F u'q F}y fFf|Y3+0%&RX64H/"/02l4RX"/&Ciz~R2$'6z6H%5:';q}'6^FC'&1"/&102>a5:45:H/"%5:R13,8:87~4|ot6,0P&3,R2$3+"/6O> /ee3,N&H/"%3+iG8:5:H%$'&10qhc}c:@30%&1;l48:3+0B3,RX"%5766,0j'6+6,x? -aEQ57"%f$ /@@-%232 'q -'|/|2@GH%l4R2$"%$43+"jR2$'6H/&)3,RX"%5764Hj3+0%&PRX6>?G3+"%57iG87&q}3,H`C'&X '&C5:Z\z&RX"%576( ( w|nEQ57"%$[&3,R2$W6,"%$'&103,4CTEQ57"%$T3,8:8`3,RX"%5764H5:@`3,4CW3,C'+3,4R15:';."%5:>&Z"/6#"%$'&'&X^"?65:"EQ$'&10%&B3,Z3,RX"%576H/"%3+0%"%HKq}H%5:4RX&P"%$45:H`5:Hj3n0%&1;,0%&H%H%576)H/&3+02R2$@/3,C'+3,4R15:';3,4C[/'&X^"%a3+0%&5:"%$'&C4570%&RX"%576[6,oY"%$'&i&1;5:445:';=6,oP"%$'&C'&1,&876,?G5:';.?G8:3, | 9M0%&RX64C457"%5764H]6,oB3,8:8j3,RX"%5764Ha3,4CW'6+6,?GHBH/"%3+0%"%5:';3+"B"%$45:HP?65:"Pi&RX6>& FEQ$45:87&]0%&>a3,5:45:';)3,RX"%5764HqtEQ57"%$N"%$'&570B"%5:>&5:4RX0%&>&"%H3,C%l4H/"/&C |i&RX6>{& fF H/"%3+"/[& 8Pu 'q ay ]|j5:Hff 43,857mxDu C3,4CxMN(JL$'&L&X'3,RX"ffC'&1"%3,5:8:H6,o"%$'&L"/&>?6,023,8'0%&1;,0%&H%H%576H/&3+02R2$3+0%&Y'6,"5:>?6,0%"%3,"ot6,0"%$'&L0%&H/"M6,o"%$45:H?G3+?&10L3,4C$43,&Ki&1&ZC'&H%RX0257i&C)&8:H/&1EQ$'&10%&aq}Y3,H%8:l4>Y&X'&10@ (+*,*'w|QSRTQSR } ff\ }"ik \wdK]\!n3P"/&>?6,023,8z?G8:3,]"%$'&10%&j5:H l4H%l43,8:87~H/6>&/H%8:3,R%g^^@Gc}c:@H/6>&L3,RX"%5764HR13,ni&jH%$457ot"/&CKot6,0%E`3+02Cn6,0iG3,R%gzE`3+02C=5:)"%5:>&BEQ57"%$'6l'"LR2$43,';5:';"%$'&KH/"/02l4RX"%l'0%&B6,0Q>a3+g,&H/?G3,6,o "%$'&B?G8:3, OM 0257;$"!FH%$457ot"/&C?G8:3,5:HL6'&]5:EQ$45:R2$=3,8:8H%l4R2$N>6+3+iG87&]3,RX"%5764HY3+0%&]H%R2$'&C4l487&C=3,HY8:3+"/&]3,HQ?6H%H%57iG87& En 6^0257;$"!H%$457ot"/&C?G8:3,4HKR13,Di&a&X'R18:l4C'&Cot0%6>RX64H%5:C'&1023+"%576EQ57"%$'6l'"&4C43,';,&1025:';6,?4"%5:>a3,8:57"F~ OZ 65:';"%$45:Hn&8:5:>a5:43+"/&Hn0%&C4l44C43,"ni4023,4R2$'&Ha5:["%$'&H/&3+02R2$kH/?G3,RX&,@ffEQ$45:R2$[6,ot"/&kH/?&1&C4Hl'?W?G8:3,445:';H%57;45 R13,"%87~ .JL$45:HaR13,ki&=3,R2$457&1,&Ckiz~W3+?4?G87~^5:';"%$'&ot68:876EQ5:';#02l487&,I=S$'&W&X^?G3,4C45:';#3DH/"%3+"/z& 85FBu'q F}y fFf|EQ57"%$?40%&C'&RX&H%H/6,K0 8Pu 'q ay ]|2@G3,3,RX"%576-RX6>?G3+"%57iG87&PEQ57"%$Z3,8:83,RX"%5764HL5:p>a3~N ,ei&Yl4H/&C"/6]&H/"%3+iG8:5:H%$3,3+"/6>5:,85F'EQ$'&3,8:84"%$'&P3+"/6>aH`5:oFG"%$43+E" -3,C4C4Hff$43,&Pi&1&6,i4"%3,5:'&Cot0%6> 8Biz~'6+6,?GH JL$'&Y0%&3,H/6Z5:Hff"%$43+<" -RX6l48:C)$43,&Bi&1&)l4H/&C"/6aH%l'?4?6,0%""%$'&BH%3,>&3+"/6>aHj5:a@G3,4CZ"%$zl4HLRX6l48:CZO $43,&Ki&1&ZH%$457ot"/&C)"/6a"%$'&0257;$"Kq}C'&8:3~,&C |;3,5:@MC'&1"%3,5:8:H]R13,i&ot6l44CD&8:H/&1EQ$'&10%&3,4C3+0%&)'6,"n5:>?6,0%"%3," S$43+"fK5:>?6,0%"%3,"K"/6'6,"/&)5:HK"%$43+"]"%$'&0257;$"!FH%$457ot"%5:';Z02l487&a0%&1ot&102HK"/6."%$'&?40%&C'&RX&H%H/6,0n6,oL"%$'&H/"%3+"/&)i&5:';&X^?G3,4C'&CJL$45:Ha>&3,4Ha"%$43+"EQ$'&T"%$'&=02l487&5:H3+?4?G8:57&C@"%$'&?6H%H%57iG87&ZH%l4R1RX&H%H/6,02H"/6'@L3,4CW"%$'&10%&1ot6,0%&="%$'&6,?4"%5:>a3,8 RX6H/"P6,oh@30%&1;,0%&H%H%576.H/"%3+"/&a>a3~=i&nC45&10%&"PC'&1?&4C45:';6N"%$'&n?G3+"%$N"%$'0%6l';$NEQ$45:R2$"%$'&)H/"%3+"/&ZE`3,H]0%&3,R2$'&C JL$zl4H1@"%$'&)876Eff&10ni6l44C#6"%$'&)RX6H/"n6,oQ3.H/"%3+"/&)6,i4"%3,5:'&CEQ$'&D"%$'&H/"%3+"/&B5:H&X^?G3,4C'&CiGl'"ff'6,"`H/687,&C.q}3,H`57"ffEQ5:8:8'i&Y5:3, L H/&3+02R2$ |>a3~ai&Y5:+3,8:5:C3,H`3]876Eff&10i6l44Cot6,0L"%$'&KH%3,>&KH/"%3+"/&KEQ$'&)0%&3,R2$'&C^5:33aC45&10%&"j?G3+"%$[2Qh!! L, 12^ L!j,hhF2 2! 2h h!! L jA L 22!! ! L!}F2 /Y2/% Q2, `' ks 1 h f /+4F%h+2Q%A %!}, jx/%F, 1j2 +%jL%h` `+%`2YL21n, `}hF2 1 2 !1/, n j`2`22h 4 nj ff! h/Bj/2 1K 1 f B /n }/24 h f L +%A Bff!2! /%F` h f z h %hff' ks !!,h Y/%F!%2+Q% ! j! !1A~>fi (^z7G4 % : 4^-G-Ns4-G^1 fTOi|{rq>qsro gfirq rq9!B#F%$Qgkrn-ofInq>&1"Lm q+8 |MC'&'6,"/&P"%$'&P6,?4"%5:>a3,8 RX6H/"`o}l44RX"%576@ c}c:@z"%$'&Po}l44RX"%576"%$43+"j3,H%H%57;4H"/6n&3,R2$H/"%3+"/&8B5:"%$'&H/&3+02R2$H/?G3,RX&B"%$'&>a5:45:>a3,8GRX6H/"L6,o 3,~?G3+"%$)ot0%6>B8 "/6a3] 43,8H/"%3+"/&q}3aH/"%3+"/&{85FMN4( @45:"%$'&0%&1;,0%&H%H%576#?G8:3,445:';H/?G3,RX&| JL$'&ao}l44RX"%576m q+8 |B5:HKR2$43+023,RX"/&102571&C[iz~"%$'&ff&8:8:>a3,#&zl43+"%576q}ff&8:8:>a3,@w,, |2I*57o8MN(q8+|Muq!w|>a5:*T9*~* ' , q85:F |"twz q8 y85fF |EQ$'&10%&r85PS6 ,6 q+8 |A5:H"%$'&`H/&1" 6,o H%l4R1RX&H%H/6,0MH/"%3+"/&H "/6{8 @^c}c:@,"%$'&jH/&1" 6,oGH/"%3+"/&H"%$43+"R13,ni&`RX64H/"/02l4RX"/&Cot0%6>K8 iz~)0%&1;,0%&H%H%576@ 3,4Cz q8 y85fF |j5:Hj"%$'&N/C'&87"%3RX6H/"%^@c}c:@G"%$'&K5:4RX0%&3,H/&]5:3,R1R1l4>]l48:3+"/&CNRX6H/"i&1"FEff&1&p8Q3,4C,85F !"%$'&YH/&zl'&"%5:3,84H/&1"/"%5:';'@z"%$45:H&zl43,8:H"%$'&QRX6H/"ff6,o"%$'&Q3,RX"%576l4H/&C"/6K0%&1;,0%&H%Hot0%6>P8 "/685F OH+ zl43+"%576.wPR2$43+023,RX"/&102571&HLm q+8 |M6487~6)H/"%3+"/&HJP8 "%$43+"`3+0%&P0%&3,R2$43+iG87&,I"%$'&BRX6H/"`6,o3,l4'0%&3,R2$43+iG87&H/"%3+"/&]5:HjC'&X '&C)"/6ai&5:^ 457"/&ff&R13,l4H/&n3,R2$457&1^5:';3a0%&1;,0%&H%H%576=H/"%3+"/&Zqhc}c:@ H/&1"Q6,o;,63,8:H|<]8 5:>?G8:57&Hj3,R2$457&1^5:';3,8:8A3+"/6>aHY5:8 @43,4CZ"%$'&10%&1ot6,0%&K3,~)H%l'iGH/&1"j6,oH8 @'"%$'&6,?4"%5:>a3,8RX6H/"Lo}l44RX"%576ZH%3+"%5:H!G&Hj"%$'&K5:'&zl43,8:57"F~q+8 |c *TS>aq(|3*T q8 F |pot6,0`3,~as H%H%l4>a5:';"%$43+"ff"%$45:H5:'&zl43,8:57"F~5:H3,RX"%l43,8:87~a3,&zl43,8:57"F~a5:HM"%$'&Y0%&8:3'3+"%576"%$43+"ff;57,&H"%$'&]m'p$'&l'025:H/"%5:R1H1I0%&1EL0257"%5:';n&zl43+"%576Dq!w|jl4H%5:';Zq(|j3,HQ3,Z&zl43,8:57"F~0%&H%l487"%Hj5:*p q8+|Mu >a5:>a357o8fMN(*T9*~* ' , m'pnq85F:|"twzq8y85Ff| 57o< 8>Ws*TSs *k p m'pnq85F:|q|XR 6>?G87&1"/&ffH/68:l'"%576"/6Q"%$45:H&zl43+"%576@+5:"%$'&ot6,02>6,o43,K&X^?G8:5:R157""%3+iG87&6,oGm'pnq8+|ot6,03,8:8H/&1"%HAEQ57"%$8>A s@R13,#i&aRX6>?Gl'"/&C#iz~H/687^5:';Z3Z;,&'&1023,8:571&CDH%5:';87&XFH/6l'02RX&XF3,8:8"%3+0%;,&1"%HKH%$'6,0%"/&H/"?G3+"%$?40%6,iG87&> +3+0257&1"F~D6,oQ3,87;,6,0257"%$4>aHq}3,8:8+3+025:3+"%5764HK6,oQC'~^43,>a5:Ra?40%6,;,023,>a>a5:';=6,0];,&'&1023,8:571&CH%$'6,0%"/&H/"A?G3+"%$ |R13,Ki&l4H/&C"/6YH/687,&"%$45:H?40%6,iG87&>Z@3,HC'&H%RX0257i&CBiz~,@Gc c:@+A5:l1e+7cLq(+*,*(| JL9M3,4C } - l4H/&3+3+025:3+"%576#6,o`"%$'&ZY&'&1023,8:571&CDff&8:8:>a3,^ 6,02CVqP |3,87;,6,0257"%$4> <ff6>?Gl'"%5:';3RX6>?G87&1"/&]H/68:l'"%576Z"/6&zl43+"%576q|L5:Hj?687~^'6>a5:3,85:"%$'&]zl4>Ki&10L6,oM3+"/6>aHQiGl'"L&X^?6'&"%5:3,85:s@H%5:>?G87~i&R13,l4H/&K"%$'&nzl4>Ki&10Q6,oMH%l'iGH/&1"%HQ6,oMH%571&Ks6,0P87&H%HQ;,0%6EQHQ&X^?6'&"%5:3,8:87~)EQ57"%$sJL$45:Hj8:5:>a57"%Hff"%$'&RX6>?G87&1"/&KH/68:l'"%576Z3+?4?40%63,R2$"/6H%>a3,8:8+3,8:l'&H`6,o sq}5:)?4023,RX"%5:RX&,@4sV(|QSRR'U[_ b "_ ^.E j"h j \ Z"_ j _"N\ } ^.{^" i# \ le j]ffh ^JL$'&KH/68:l'"%576Z"/6&zl43+"%576[q|j5:HQH/"/6,0%&C=5:=3a"%3+iG87&qtEQ$45:R2$ZEQ5:8:8i&0%&1ot&10%0%&C"/63,HQ"%$'&a' ^dfXe}e3 yX:2| JL$'&H/"/6,0%&CH/68:l'"%576@A$'6Eff&1,&10@MRX6>?4025:H/&HB6487~.+3,8:l'&HB6,oQm'pnq 8+|Pot6,0]H/&1"%LH 8H%l4R2$#"%$43+"8>zks JA6a6,i4"%3,5:Z"%$'&]$'&l'025:H/"%5:RQ+3,8:l'&6,o3,=3+0%iG57"/023+0%~H/"%3+"/&,@ "%$'&K8:3,H/"LR18:3,l4H/&6,o &zl43+"%576q|5:HY&1+3,8:l43+"/&CS!6^F8:5:'&^@3,4C.C4l'025:';a"%$45:HQ&1+3,8:l43+"%576."%$'&]+3,8:l'&]6,o`m'pnq 85F:|Lot6,03,~?85FAH%l4R2$N"%$43+"85F*zWs5:H`6,i4"%3,5:'&CZiz~)876z6,g^5:';57"Ll'?=5:)"%$'&B"%3+iG87&!o}3,RX"1@"%$'&L$'&l'025:H/"%5:Rff"%3+iG87&L5:>?G87&>&"/&C5:JL9M-3,4C } - 5:H3B;,&'&1023,84>a3+?4?G5:';Pot0%6>H/&1"%H6,oG3+"/6>aH "/6Y"%$'&570 3,H%H/6^R15:3+"/&Cn+3,8:l'&,@,3,4CK"%$'&`$'&l'025:H/"%5:R+3,8:l'&ff6,oG3YH/"%3+"/<& 8j5:HA"%$'&`>a3'5:>a3,8+3,8:l'&ff6,o3,~KH%l'iGH/&1"6,So 8ff"%$43+"5:HH/"/6,0%&C]5:"%$'&ff"%3+iG87& !6,"%$'&10 Eff6,02C4H1@+57>o nq 8+|C'&'6,"/&H"%$'&+3,8:l'&ffH/"/6,0%&CKot6,08@^"%$'&B$'&l'025:H/"%5:RL+3,8:l'&Y6,o3nH/"%3+"/{& 8B5:Hff;57,&)iz~)mAq 8+|MuS>a3K vnq 85F: | 85FM8y nq 85F:|&X'5:H/"%vH S$'&~>fi44qt3iv|q}3|q}R|3v3v57;l'0%&K(zI_L&8:3'3+"%576Z6,o"/&>?6,023,80%&1;,0%&H%H%576H/"%3+"/&H3,8:8'3,4Cn6487~]H/&1"%H6,oH%571&`s6,087&H%H3+0%&LH/"/6,0%&Ca5:n"%$'&j"%3+iG87&Kq}3,HM5:H "%$'&LR13,H/&LEQ$'&am'pV5:HRX6>?Gl'"/&CRX6>?G87&1"/&87~4|"%$45:HffRX65:4R15:C'&HEQ57"%$&1+3,8:l43+"%5:';]"%$'&P8:3,H/"`R18:3,l4H/&Y6,oA&zl43+"%576q| Q6Eff&1,&10@4"%$'&Pl4H/&6,o3B;,&'&1023,8G$'&l'025:H/"%5:R"%3+iG87&Q5:>?G8:57&H"%$43+"3,HH/6z6a3,H3B+3,8:l'&jot6,0P+4L3+"/6> H/&1c" 8Q5:HMH/"/6,0%&Ca5:"%$'&"%3+iG87&,@ 57"i&RX6>&H]5:>a>&C45:3+"/&87~.5:4R18:l4C'&C5:#3,8:8MH%l'iGH/&zl'&"&1+3,8:l43+"%5764H6,oLH/"%3+"/&H]RX6"%3,5:45:';8 !Z?G3+0%"%5:R1l48:3+0@^iz~)H/"/6,025:';?G3+0%"%Hj6,o "%$'&KH/68:l'"%576)"/6m'p @'ot6,0YH/6>&K$457;$'&10jxFt@G5:)"%$'&ot6,02>6,ol'?C43+"/&H6,oG"%$'&`+3,8:l'&H 6,o H/6>&jH%571&`xFz3+"/6>H/&1"%H1@"%$'&j$'&l'025:H/"%5:RM&1+3,8:l43+"%5765:>?G8:5:R157"%87~PRX6>?Gl'"/&H"%$'&K>a3'5:>]l4>6,om'px3,4C)"%$'&?G3+0%"%5:3,8:87~RX6>?Gl'"/&CNm'pJL$'&$'&l'025:H/"%5:RQ"%3+iG87&5:Hj5:>?G87&>&"/&CZ3,HL3JA0257&q}H/&1&)c c $'6'@GQ6,?RX0%6,ot"1@G Y8:8:>a3,@w9 ,|H/6n"%$43+"`"%$'&P&1+3,8:l43+"%5766,o3,)3+"/6>H/&1<" 8BR13,)i&PC'6'&P5:"%5:>&B8:5:'&3+0`5:"%$'&Bzl4>Ki&10ff6,oH%l'iGH/&1"%H6,f8)"%$43+"&X'5:H/"a5:"%$'&)"%3+iG87v&,&WH/6'@"%$'&10%&Z5:HH/6>&)6,&102$'&3,CkRX6>?G3+0%&C["/6D3N"%3+iG87&3,4C+&1+3,8:l43+"%576Z?40%6^RX&C4l'0%&BC'&H%57;'&C)ot6,0Q3n4^&C>a3'5:>a3,8H%l'iGH/&1"LH%571&i|{rq>qsr9!gfirq r q Iw`bt-n-ofInq>JA6aC'&X '&Bm'pot6,0j"/&>?6,023,8 0%&1;,0%&H%H%576?G8:3,445:';'@6'&'&1&C4H`6487~a"/6aC'&X '&P3nH%l457"%3+iG87&P>&3,H%l'0%&6,oYH%571&ot6,0n"/&>?6,023,8ff0%&1;,0%&H%H%576H/"%3+"/&Ha3,4C"%$'&?40%6^RX&1&C[3,H5:D"%$'&ZH/&zl'&"%5:3,8ffR13,H/& _L&R13,8:8"%$43+"P3"/&>?6,023,8A0%&1;,0%&H%H%576NH/"%3+"/&RX64H%5:H/"%HQ6,oM"FEff6ZRX6>?6'&"%H1"@ 8]u'q ay ]|2@EQ$'&10%L& 5:HY3H/&1"6,oL3+"/6>aH]3,4C.3=H/&1"]6,ojH%R2$'&C4l487&CD3,RX"%5764HKEQ57"%$"%5:>&5:4RX0%&>&"%H JL$'&a6,iz^576l4HKR13,4C45:C43+"/&5:HB"/6.C'&X '& 8>u p9t , @3,4C#5:4C'&1&C@l4H%5:';Z"%$45:H>&3,H%l'0%&5:&zl43+"%576Vq|3+i6,&0%&H%l487"%H5:3ZR2$43+023,RX"/&102573+"%576#6,o`3Z876Eff&10i6l44CNo}l44RX"%576N6"%$'&"/&>?6,023,8 0%&1;,0%&H%H%576H/?G3,RX& !."%$45:HR13,H/&,@'$'6Eff&1,&10@'C4l'&Q"/6]"%$'&Q?40%&H/&4RX&Q6,o3"%5:>&Y5:4RX0%&>&"EB5:&3,R2$#q -'E| @@z"%$'&YH/&1"ff6,oH/"%3+"/&HEQ57"%w$ 8>4s5:HQ?6,"/&"%5:3,8:87~Z5:^ 457"/&,@ 3,4C="%$'&10%&1ot6,0%&n"%$'&nH/68:l'"%576="/6"%$45:HQ&zl43+"%576.R13,.'6,"Pi&RX6>?Gl'"/&CZ&X^?G8:5:R157"%87~JA6[6,i4"%3,5:S3[l4H%3+iG87&NRX6H/")&zl43+"%576@Y3o}l'0%"%$'&100%&8:3'3+"%576S5:H'&1&C'&CIH%5:4RX&.3D?G8:3,k"%$43+"3,R2$457&1,&HK"%$'&H/"%3+"/,& 8uv'q ay ]|2@ot6,L0u z'q - |2y1{1{1{y'q -3 '| @ 3+"K"%5:>B& :P>]l4H/"K3,R2$457&1,&"%$'&?40%&RX64C457"%5764H6,oj&3,R2$T3,RX"%576-3L3+"n"%5:>o& :cGN!@3,4CD"%$'&H/&)>]l4H/"]0%&>a3,5:#"/02l'&)l4"%5:8 :Kl4487&H%H`! 2 2 /%M j`!2 +% n K+h j2j ]2AQ!hF2 ]! % t]2!1 !}2/` `F2 4A %F }h}2! L2 G +h/ff1ff} !2+ff j !j!1F%24! } F2 M/%%Vh 2 } FK%j 2,/% KE/P h /2'2Fh/Z!2K%VhH}2/ BMF2 !XhBh+% L P2 /K2j} Q B !Bff}2 ,}2/ Q2 z,hh2 /2 2 ff !j! } 2 !%!j/71P!j hff ]2 cDs 1+% K2M1P! %//%F,F 1! 2!!1ML !V !!!XhffBhff ff}2/G2 ffhQ7 ! + PLh!2!,+ B h /2 !F+/X2 /K,!!]}2/L !Xh/ ff/2 1ff! } `F2 ^ ] M!!1 2P^ M2} 2 F2, PF2 /Q Km/%F1fi (^z7G4 % : 4^-G-Ns4-G^1 fC'&87&1"/&CZiz~p-3!@'"%$'&6,?4"%5:>a3,8RX6H/"Lo}l44RX"%576H%3+"%5:H!G&Hq'ay]|&>a3/ff01q'-3|2yCwtwfiffqt-z|<, ' , 'q'ay]| DIq|/ff01q'-3|2yCK{' ,8 uq s/"y zq'- yw|2yq'- $ y2(|+|2@MC'&X#&X'3,>?G87&>a3~#R18:3+0257ot~N"%$'&?4025:4R157?G87&,IK<ff64H%5:C'&10B"%$'&H/"%3+"/&x)?G5:RX"/&Cn5: 57;l'0%&j('q}3| OM ?G8:3,]3,R2$457&1^5:';Y"%$45:HH/"%3+"/&L3+" "%5:>&c: >]l4H/" 3,R2$457&1,&j"%$'&ff?40%&RX64C457"%5764HA6,o- $ 3+"H:3G(z@H/6Km q8+|>]l4H/"i&j3+"87&3,H/"m q /ff01 q'- $ |2yC |)( ho3,RX"%576- $ 5:HB/87&1ot"6l'"%^@z3,HM5: 57;l'0%&('qti|2@57" R13,]i&ffH/&1&]"%$43+" "%$'&`H%3,>&ff?G8:3,]3,8:H/6P3,R2$457&1,&H "%$'&/65:"?40%&RX64C457"%5764H6,oG3,RX"%5764H- 3,4C- $ 3+":G.w+@^H/6]m q+8 |>]l4H/"Mi&Q3+"ff87&3,H/"ffm q /ff01 q'- |3IE/ff01 q'- $ |2yC |3#w 5:43,8:87~,@57o i6,"%$3,RX"%5764Hff3+0%&87&1ot" 6l'"Lq 57;l'0%&Q('q}R|/|2@z57"5:H R187&3+0M"%$43+" "%$'&j?G8:3,n3,8:H/6B3,R2$457&1,&HMH%5:>]l487"%3,'&16l4H%87~B"%$'&j?40%&RX64C457"%5764H6,o "%$'&B"FEff63,RX"%5764HL3,4C3+"/6>/ @GH/6m q+8 |`>]l4H/"ji&3+"Q87&3,H/"Ym q s/"<I/ff01 q'- | I{/ff01 q'- $ |2yC |ff~D"/0%&3+"%5:';.5:'&zl43,8:57"%57&Hqt-z|PVq|K3,Hn&zl43,8:57"%57&H1@3="/&>?6,023,8ff0%&1;,0%&H%H%576[H/"%3+"/&Z5:HK0%&8:3^&C"/6[3DH/&1"6,oH/"%3+"/&H)5:WEQ$45:R2$u zC @Bc}c:@jH/"%3+"/&H)RX6"%3,5:45:';D6487~[;,63,8:H3,4Ck'6RX64R1l'0%0%&"'3,RX"%5764H JA6]&3,R2$ZH%l4R2$)H/"%3+"/&,@'0%&8:3'3+"%576.q(|R13,i&Y3+?4?G8:57&C@0%&H%l487"%5:';K5:3,&zl43+"%576)C'&X 45:';"/&>?6,023,8'm'pK@H%5:>a5:8:3+0A"/6aq| JL$45:H&zl43+"%576$43,H 3Q 457"/&ff&X^?G8:5:R157"AH/68:l'"%576@,RX6"%3,5:45:';P3,8:8^H/"%3+"/&H8Ku 'q ay C|LEQ57"%$ p4Us OQ ;3,5:@ >6,0%&nC'&1"%3,5:8:HQR13,Ni&ot6l44C&8:H/&1EQ$'&10%&)q}Y3,H%8:l4>v Y&X'&10@(+*,*'w|L 5:H3PEff&8:8^g^'6EQa3,C4>a5:H%H%57iG87&ff$'&l'025:H/"%5:RffH/&3+02R2$a3,87;,6,0257"%$4>q}H/&1&nc cB6,0%oh@Gw9,z@Gw,,| JL$'&3,87;,6,0257"%$4>Eff6,0%g^H`iz~)3H/&10257&H`6,oRX6H/"!i6l44C'&CZC'&1?4"%$^G02H/"jH/&3+02R2$'&H JL$'&BRX6H/"L0%&1"%l'02'&Ciz~"%$'&:8 3,H/"BRX6>?G87&1"/&C.C'&1?4"%$^G02H/"PH/&3+02R2$#5:HY3Z876Eff&10Bi6l44C=6N"%$'&RX6H/"B6,off3,~NH/68:l'"%576 JL$'&10%&1ot6,0%&,@"%$'&3,87;,6,0257"%$4>R13,#&3,H%5:87~Ni&a>6^C45G&C."/6"%3+g,&3,Dl'?4?&10B8:5:>a57"P6#H/68:l'"%576RX6H/"1@3,4C"/6&X'57"EQ57"%$)o}3,5:8:l'0%&P64RX&K57"L$43,Hj?40%6,&Z"%$43+"Q'6H/68:l'"%576)EQ57"%$Z3RX6H/"QEQ57"%$45:"%$45:Hj8:5:>a57"ff&X'5:H/"%H)&X^"/&4H%576)6,oA"%$'&B L3,87;,6,0257"%$4> ot6,0LH/&3+02R2$45:'; KnYP_;,023+?G$4Hj5:Hff"%$'&>a3,5:"/6z68iz~EQ$45:R2$="%$'&]0%&8:3^&CNH/&3+02R2$.>&1"%$'6^CN5:HY5:>?G87&>&"/&C JL$'&K&X^"/&4C'&C.3,87;,6,0257"%$4>5:HQ?40%&H/&"/&CN5:\z&RX"%576= (L 5:HL3H/6+FR13,8:87&C=8:5:'&3+0QH/?G3,RX&]3,87;,6,0257"%$4>ZIff57"QH/"/6,0%&HQ6487~)"%$'&?G3+"%$"/6"%$'&KR1l'0%0%&"Q'6^C'&JL$'&3,87;,6,0257"%$4>R13,#i&H/?&1&C'&CDl'?#iz~#l4H%5:';>&>6,0%~5:"%$'&ot6,02>6,oL3Z"/023,4H/?6H%57"%576"%3+iG87&,@EQ$45:R2$0%&RX6,02C4HKl'?C43+"/&C.&H/"%5:>a3+"/&CRX6H/"%HK6,oj'6^C'&HB"%$43+"]$43,&i&1&&X^?G3,4C'&CiGl'"K'6,"KH/687,&Cq}_L&5:'ot&8:CVN3+02H%8:3,4C@Bw,-z| JL$'&="%3+iG87&N5:H6,o3#4^&CV8:5:>a57"/&CWH%571&,@LH/6['6,"3,8:8Q&X^?G3,4C'&Cl44H/687,&C#'6^C'&H3+0%&H/"/6,0%&C + S$'&'&1,&10]"%$'&H/&3+02R2$#0%&3,R2$'&H]3='6^C'&"%$43+"K5:H5:"%$'&a"%3+iG87&a"%$'&l'?C43+"/&CRX6H/"j&H/"%5:>a3+"/&Pot6,0j"%$'&B'6^C'&q}C45:H%RX6,&10%&C)EQ$'&"%$'&P'6^C'&YE`3,H`?40%&1^576l4H%87~&X^?G3,4C'&C |M5:H12ff+%jh2+%!1!!M+2M,!!n j +/] !K !#"%$M !QY}F%J'&k!#"%$MY+2 /' ] ]72hff, !.' !#"%M$ tQ!^`!jth ]ff 1+2+K L2,2s1 +2 ! !!+K+%2 ff}F%ffks ,ffh2!!!1M2h ($ffff j +/B2PPff M+2 A2'+%!1!!! A/L +2,h/) M1!#*,+ - !hM!j h!! 2M L2 t2ff/%Fz !`}F%! +%%`2}2/:! F%ff j Qf h+2+/Y !!1h/B2X2 mt4fi Q2+./10 2 ,F2 j !j!1/Y2L! /+21F2 ks }F%!%ff}2/ K% , K!2!,+ QL! +2K2 !`1P! } P B` +2Ph B!jF% `}F%Ms1 +2 tY!}jXh Y+%}2/P}F%M +!/Q2j2M,! j 1X/P,FtP/224! !hh!!M X!P}2 `!! hAff122^ /%FY!j4A3 ! ! 658P7 % 2+z:9;29 1F1=<fi44l4H/&C=5:4H/"/&3,CZ6,oM57"%HQ$'&l'025:H/"%5:RP+3,8:l'&,@G3,8:876EQ5:';"%$'&]3,87;,6,0257"%$4>"/63,65:C=0%&XFH/&3+02R2$45:';'6^C'&Hj"%$43+"3+0%&0%&3,R2$43+iG87&^5:3H/&1,&1023,8?G3+"%$4HLC4l'025:';]"%$'&KH%3,>&K57"/&1023+"%576!>OwexzyJL$'&JL9M-a?G8:3,4'&10j?40%&RX6>?Gl'"/&HL"%$'&"/&>?6,023,8Am $ $'&l'025:H/"%5:RP3,HQC'&H%RX0257i&CZ3+i6,&]3,4Cl4H/&HQ57"L5:3, LH/&3+02R2$W5:D"%$'&)"/&>?6,023,80%&1;,0%&H%H%576[H/?G3,RX& _Q57;$"!FH%$457ot"nR1l'"%Hn3+0%&Zl4H/&C"/6.&8:5:>a5:43+"/&0%&C4l44C43,"?G3+"%$4Hot0%6> "%$'&QH/&3+02R2$H/?G3,RX&,@z3,4C3B"/023,4H/?6H%57"%576]"%3+iG87&Q5:HMl4H/&C"/6]H/?&1&Cal'?aH/&3+02R2$q}Y3,H%8:l4>xY&X'&10@'(+*,*'w| JL$'&Y>a3,5:H/"/&1?GH6,o"%$'&Q?G8:3,4'&10ff3+0%&Q6l'"%8:5:'&C5: 57;l'0%&Pqt6?G3+;,&(3- |2@ >a3,5:487~"/65:8:8:l4H/"/023+"/&PH%5:>a5:8:3+0257"F~3,4CC45&10%&4RX&bc:d1cfec"%$'& } - ?G8:3,4'&10? A@CB DM CEGFGb!s!IHKJbAHLJNMOF=J6,0`>a3,~?G8:3,445:';P?40%6,iG87&>aH"%$'&Ym $ $'&l'025:H/"%5:R`5:HM"/6z6KEff&3+g >6,0%&Y3,R1R1l'023+"/&P$'&l'025:H/"%5:RjR13,i&6,i4"%3,5:'&Ciz~RX64H%5:C'&1025:';a$457;$'&10L+3,8:l'&HL6,o"%$'&Ks?G3+023,>&1"/&10@ iGl'"Q3,~>&1"%$'6^Cot6,0PRX6>?Gl'"%5:';3NRX6>?G87&1"/&)H/68:l'"%576#"/6."%$'&Zm'p&zl43+"%576[H%R13,87&HK&X^?6'&"%5:3,8:87~5:Ds@>a3+g^5:';N57"]5:>?4023,RX"%5:R13,8ot6,0s ;( O[ RX6>?G87&1"/&H/68:l'"%576[5:Hnl4H/&1o}l48ffi&R13,l4H/&Z57"$'&87?GHnC'&1"/&RX"l4'0%&3,R2$43+iG87&)H/"%3+"/&HNq}5:?G3+0%"%5:R1l48:3+0@m $ C'&1"/&RX"%H3H%57;45 R13,"?G3+0%"6,o "%$'&LH/"%3+"%5:RL>]l'"/&Xn0%&8:3+"%5764H5:n3B?G8:3,445:';Q?40%6,iG87&>|2@iGl'"3,8:H/6]E`3,H/"/&1o}l484i&R13,l4H/&Y6,ot"/&>a3,~6,o"%$'&Y3+"/6>H/&1"%H`3+0%&Y'6,"ff0%&87&1+3,"ot6,0ff&1+3,8:l43+"%5:';]H/"%3+"/&H3,RX"%l43,8:87~]&4RX6l4"/&10%&CaEQ$45:87&jH/&3+02R2$45:';Bot6,03H/68:l'"%576n"/6K"%$'&j?G8:3,445:';Y?40%6,iG87&> 3+"$43,4C _L&R13,8:8"%$43+"P"%$'&$'&l'025:H/"%5:RB&1+3,8:l43+"%576N6,off3)H/"%3+"/&=q}3)H/&1"P6,oM;,63,8:H|Y>a3+g,&HBl4H/&]6,o"%$'&n&H/"%5:>a3+"/&C.RX6H/"B6,o3,~.H%l'iGH/&1"P6,off"%$'&aH/"%3+"/&a"%$43+"K5:HPg^'6EQWq}H/"/6,0%&C#5:."%$'&a$'&l'025:H/"%5:R"%3+iG87&| Oa HB8:3+0%;,&10K3+"/6>H/&1"%H3+0%&RX64H%5:C'&10%&C@Mc}c:@A3,Hs 5:4RX0%&3,H/&H1@"%$'&1~.i&RX6>&ai6,"%$>6,0%&zl4>&10%6l4HB3,4C#>6,0%&H/?&R15 R+@3,4CZ"%$zl4Hj"%$'&Bot023,RX"%576Z6,o "%$'&KRX6>?G87&1"/&H/68:l'"%576)"%$43+"Q5:HL3,RX"%l43,8:87~)l4H/&1o}l48C'&RX0%&3,H/&HJA6Dl4H/&=m'pot6,0$457;$'&10s@`R187&3+0287~3E`3~T5:Ha'&1&C'&CT"/6DRX6>?Gl'"/&Z"%$'&=$'&l'025:H/"%5:R3+"3#RX6H/"?40%6,?6,0%"%57643+"/&Z"/6"%$'&+3,8:l'&)6,oP"%$'&5:>?40%6,&>&" _L&8:3^&CWH/&3+02R2$k3,5:>aH"/6D3,R2$457&1,&"%$45:Hniz~RX6>?Gl'"%5:';B6487~3?G3+0%"M6,o"%$'&Pm'pSH/68:l'"%576@3,4C3B?G3+0%""%$43+"ff5:HM8:57g,&87~]"/6Ki&L0%&87&1+3,"Mot6,0ffH/687^5:';"%$'&;57,&Z?G8:3,445:';]?40%6,iG87&>onQPQ|W#FnfiH!B#F%$Qgkrn-oInq>HB&X^?G8:3,5:'&CN&3+028:57&10@"%$'&m'p$'&l'025:H/"%5:R]R13,#i&H/&1&#3,HB"%$'&6,?4"%5:>a3,8RX6H/"o}l44RX"%5765:."%$'&s)0%&1;,0%&H%H%576kH/?G3,RX&,@L3#0%&8:3^&CkH/&3+02R2$VH/?G3,RX&=EQ$'&10%&=H/&1"%H6,o>6,0%&="%$43,Ws ;,63,8:H3+0%&NH/?G8:57"a5:"/6?40%6,iG87&>aHL6,oMs;,63,8:H1@&3,R2$.6,oMEQ$45:R2$N5:HYH/687,&CN5:4C'&1?&4C'&"%87~ JL$zl4H1@ "%$'&]s)0%&1;,0%&H%H%576NH/?G3,RX&5:H)3, KnYP_;,023+?G$IH/"%3+"/&H)EQ57"%$Vs 6,0)ot&1Eff&103+"/6>aHZ3+0%&DP_jF'6^C'&H)3,4CS3+0%&.&X^?G3,4C'&Ciz~T'6,02>a3,8L0%&1;,0%&H%H%576@ffEQ$45:87&H/"%3+"/&HEQ57"%$W>6,0%&="%$43,Ws 3+"/6>aH3+0%& KnY F'6^C'&H1@`EQ$45:R2$k3+0%&&X^?G3,4C'&C=iz~=H/687^5:';a&3,R2$H%l'iGH/&1"Q6,oH%571&]s JL$'&]RX6H/"P6,o3,P_jF'6^C'&]5:HY>a5:45:>a571&CZ6,&103,8:857"%HnH%l4R1RX&H%H/6,02H1@EQ$45:87&"%$'&)RX6H/"n6,oY3, KnY F'6^C'&Z5:H]>a3'5:>a571&C ?+ '3,>?G87&HK6,onqt?G3+0%"]6,o|"%$45:H;,023+?G$@ ot6,0P3)(0%&1;,0%&H%H%576.H/?G3,RX&,@3+0%&nH%$'6EQN5: 57;l'0%&HQ-)3,4C.Nqt"%$'&]&X'3,>?G87&]5:HYC'&H%RX0257i&C5:C'&1"%3,5:85:Z\z&RX"%576 ( w| HjR13,)i&BH/&1&@^"%$'&P;,023+?G$)5:H`'6,"jH/"/025:RX"%87~8:3~,&10%&C@45:"%$43+"YP_jF'6^C'&H>a3~ZH/6>&1"%5:>&HL$43,&]H%l4R1RX&H%H/6,02Hj"%$43+"Q3+0%&K3,8:H/6)P_jF'6^C'&HJL$'&LC45&10%&"3,87;,6,0257"%$4>aHl4H/&Cn"/6B6,i4"%3,5:aRX6>?G87&1"/&LH/68:l'"%5764H"/6"%$'&Qm'pk&zl43+"%576aR13,a3,8:8^i&H/&1&3,Hff+3+025:3+"%5764HM6,o3=!i6,"/"/6>nFl'?G]8:3+i&8:5:';B6,o"%$'&Y'6^C'&H6,o"%$45:HM;,023+?G$@zH/"%3+0%"%5:';Kot0%6> '6^C'&HEQ57"%$ZRX6H/"j1&10%63,4C)?40%6,?G3+;3+"%5:';aRX6H/"%HL"/6?G3+0%&"L'6^C'&Hj3,R1RX6,02C45:';"/6"%$45:Hj>a5: >a3?4025:4R157?G87&JL$'&?40%6,?G3+;3+"%576T5:HnRX6>?G87&1"/&,@Qc}c:@M?40%6^RX&1&C4Hnl4"%5:8&1,&10%~Uq}H/687+3+iG87&|K'6^C'&)5:"%$'&);,023+?G$[$43,Hi&1&Z8:3+i&87&CEQ57"%$)57"%H`6,?4"%5:>a3,8RX6H/"]q}3,87"%$'6l';$Z5:ZH/6>&B6,o"%$'&3,87;,6,0257"%$4>aH1@'5:4R18:l4C45:';"%$'&]P1fifi (^z7G4 % : 4^-G-Ns4-G^1 f5:>?G87&>&"%3+"%576#l4H/&CDiz~JL9M-N3,4C } - @ 6487~."%$'&RX6H/"%HK6,oYP_jF'6^C'&HK3+0%&)3,RX"%l43,8:87~H/"/6,0%&C |_L&8:3^&C=H/&3+02R2$=&X^?G876,0%&Hj"%$'&Ks)0%&1;,0%&H%H%576H/?G3,RX&]5:3>6,0%&ot6^R1l4H/&Co}3,H%$4576@'EQ57"%$Z"%$'&K3,5:>6,oC45:H%RX6,&1025:';a"%$'&6,?4"%5:>a3,8ARX6H/"nqt6,0Y3,N5:>?40%6,&C876Eff&10Yi6l44C |M6,oMH/"%3+"/&HQ0%&87&1+3,"Q"/6"%$'&KH/&3+02R2$ot6,0j3nH/68:l'"%576"/6n"%$'&P;,63,8:H`6,o"%$'&P;57,&?G8:3,445:';?40%6,iG87&> JL$45:H5:H`3,R2$457&1,&C)iz~H/&3+02R2$45:';n"%$'&s)0%&1;,0%&H%H%576ZH/?G3,RX&Bot6,0Q3,)6,?4"%5:>a3,8H/68:l'"%576"/6a3n?G3+0%"%5:R1l48:3+0jH/"%3+"/&,I"%$'&RX6H/"j6,o"%$45:HjH/68:l'"%576)5:H"%$'&]m'p$'&l'025:H/"%5:RY+3,8:l'&B6,o "%$43+"QH/"%3+"/& JL$'&K3,87;,6,0257"%$4>C'&H%RX0257i&CZ5:)"%$'&K'&X^"QH/&RX"%576qt L |R13+0%0257&H6l'"M"%$45:HMH/&3+02R2$D!"/6,?'FC'6EQ4^@^H/"%3+0%"%5:';ot0%6> "%$'&QH/"%3+"/&YRX6,0%0%&H/?64C45:';Y"/6K"%$'&L;,63,8:HM6,o"%$'&?G8:3,445:';]?40%6,iG87&>Q&l'025:H/"%5:R1HjC'&10257,&C)iz~)H/&3+02R2$45:';5:Z3,3+iGH/"/023,RX"%576Z6,o"%$'&H/&3+02R2$=H/?G3,RX&$43,&i&1&H/"%l4C457&C&X^"/&4H%57,&87~5: Lq}H/&1&c c^P3,H%R2$4457;'@w +z'9M0257&C457"%5:H1@4w,,zG<`l487i&102H/6ar\^R2$43+&X&10@w,,| !?G3+0%"%5:R1l48:3+0@57"P$43,HYi&1&.H%$'6EQN"%$43+"BH%l4R2$.$'&l'025:H/"%5:R1HYR13,.6487~Zi&nRX6H/"B&X&RX"%57,&al44C'&10PRX&10%"%3,5:RX64C457"%5764H1IP"%$'&;,&'&1023,8:571&C"%$'&16,0%&>6,offM3,87"/6,0%"%3=H/"%3+"/&H"%$43+"K5:."%$'&aRX6l'02H/&6,oj3, H/&3+02R2$;l45:C'&C)iz~Z3a$'&l'025:H/"%5:RPC'&10257,&CZiz~)H/&3+02R2$45:';aiG8:5:4C487~a5:ZH/6>&K3+iGH/"/023,RX"%5766,o "%$'&KH/&3+02R2$=H/?G3,RX&,@&1,&10%~[H/"%3+"/&Z"%$43+"Eff6l48:CDi&)&X^?G3,4C'&Ciz~3NiG8:5:4CDH/&3+02R2$W5:"%$'&)6,0257;5:43,8ffH/&3+02R2$WH/?G3,RX&Z>]l4H/"i&K&X^?G3,4C'&C&57"%$'&10Y5:"%$'&n3+iGH/"/023,RX"PH/?G3,RX&K6,0Piz~Z"%$'&H/&3+02R2$.5:Z"%$'&]6,0257;5:43,8H/?G3,RX&Zq}Q687"/&,@9&10%&1,S@ R5:>a>&10@4TN3,R 643,8:C@Aw,,| JL$45:H`5:>?G8:57&H"%$43+"L57oA"%$'&3+iGH/"/023,RX"%5765:Hj3,Z&>Ki&C4C45:';qt"%$'&nH/&1"Y6,oMH/"%3+"/&HB5:"%$'&]3+iGH/"/023,RX"BH/?G3,RX&]5:HQ"%$'&]H%3,>&n3,HP5:"%$'&K6,0257;5:43,8AH/&3+02R2$.H/?G3,RX&|2@H%l4R2$N3$'&l'025:H/"%5:RnR13,'&1,&10Ki&RX6H/"K&X&RX"%57,&qtM3,87"/6,0%"%3^@jw9 -z| JL$'&as)0%&8:3'3+"%576#6,o`"%$'&0%&1;,0%&H%H%576?G8:3,445:';=H/&3+02R2$WH/?G3,RX&5:Hn3,[&>Ki&C4C45:';'@H%5:4RX&)&1,&10%~[H/"%3+"/&5:"%$'&Z'6,02>a3,8ff0%&1;,0%&H%H%576[H/?G3,RX&RX6,0%0%&H/?64C4H"/6&X'3,RX"%87~k6'&.H/"%3+"/&Wq}RX6"%3,5:45:';"%$'&.H%3,>&.H/&1")6,oKH%l'i4;,63,8Y3+"/6>aH|5:k"%$'&Ns)0%&1;,0%&H%H%576#H/?G3,RX& !#H/?G57"/&a6,o`"%$45:H1@A"%$'&10%&3+0%&0%&3,H/64H"/6=i&8:57&1,&"%$43+"K0%&8:3^&C#H/&3+02R2$R13,Di&RX6H/"`&X&RX"%57,&,IJL$'&P3,87;,6,0257"%$4> l4H/&C"/6H/&3+02R2$"%$'&Ps)0%&1;,0%&H%H%576H/?G3,RX&BC45:H%RX6,&102HBq}3,4C)H/"/6,0%&H`5:"%$'&j$'&l'025:H/"%5:R"%3+iG87&|"%$'&`"/02l'&jm'pW+3,8:l'&,@+6,0M3P876Eff&10 i6l44CK6]"%$45:H+3,8:l'&ff;,0%&3+"/&10M"%$43,]"%$43+" ;57,&iz~n"%$'&QR1l'0%0%&"M$'&l'025:H/"%5:Rff"%3+iG87&,@ot6,0M&1,&10%~P_jF'6^C'&j&X^?G3,4C'&CaC4l'025:';P"%$'&QRX6l'02H/&j6,o"%$'&L0%&8:3^&CH/&3+02R2$ JL$'& KnYP_H/"/02l4RX"%l'0%&6,o "%$'&s)0%&1;,0%&H%H%576H/?G3,RX&,@ 3,4C"%$'&o}3,RX"Q"%$43+"L"%$'&=!6^F8:5:'&$'&l'025:H/"%5:R]>a3+g,&Hl4H/&6,o`3,8:80%&87&1+3,"5:'ot6,02>a3+"%576N?40%&H/&"5:."%$'&a$'&l'025:H/"%5:R"%3+iG87&,@A5:>?G8:57&HQ"%$43+"3,N5:>?40%6,&>&"Q6,o"%$'&]&H/"%5:>a3+"/&CNRX6H/"Y6,oM3,P_jF'6^C'&K>a3~~^57&8:C5:>a>&C45:3+"/&87~Z3,N5:>?40%6,&C&H/"%5:>a3+"/&=6,oB"%$'&NRX6H/"6,o>a3,~ KnY F'6^C'&H.q}3,8:8QH/"%3+"/&H"%$43+")3+0%&NH%l'?&102H/&1"%H6,oB"%$'&N5:>?40%6,&CH/"%3+"/&|2@'EQ57"%$'6l'"ff3,~3,C4C457"%57643,84H/&3+02R2$)&X6,0%" 5:43,8:87~,@i&R13,l4H/&P_jF'6^C'&Hff5:a"%$'&Ys)0%&1;,0%&H%H%576H/?G3,RX&Q3+0%&LH/"%3+"/&H6,o8:5:>a57"/&CnH%571&,@&3,R2$'6^C'&j&X^?G3,4H%5765:n"%$'&js)0%&1;,0%&H%H%576aH/?G3,RX&Q5:H8:57g,&87~K"/6i&RX6>?Gl'"%3+"%57643,8:87~R2$'&3+?&10ff"%$43,a"%$'&Y3,&1023+;,&5:"%$'&Y'6,02>a3,840%&1;,0%&H%H%576H/?G3,RX&,@^H%5:4RX&L"%$'&Yzl4>Ki&106,oAH%l4R1RX&H%H/6,02H`;,&'&1023+"/&C)EQ$'&0%&1;,0%&H%H%5:';n3nH/"%3+"/&P;,&'&1023,8:87~5:4RX0%&3,H/&HffEQ57"%$"%$'&Pzl4>Ki&10ff6,o;,63,83+"/6>aHQ5:)"%$'&KH/"%3+"/&iKUIJA6nH/&3+02R2$)"%$'&Y0%&8:3^&C0%&1;,0%&H%H%576H/?G3,RX&,@ } - l4H/&H`3,)3,87;,6,0257"%$4> R13,8:87&C L Hff"%$'&P43,>&H%l';,;,&H/"%H1@L57"5:Ha3,k3,C43+?4"%576W6,oP L"/6DH/&3+02R2$45:'; KnY P_;,023+?G$4H1@Yc}c:@j57"aR13+0%0257&Ha6l'"3C'&1?4"%$^G02H/"1@'57"/&1023+"%57,&KC'&1&1?&45:';H/&3+02R2$ L 5:Hj3,C4>a5:H%H%57iG87&,@^5:)"%$'&H/&4H/&B"%$43+"Q57oA;l45:C'&C)iz~3,Z3,C4>a5:H%H%57iG87&L$'&l'025:H/"%5:R+@^57"ff0%&1"%l'024H"%$'&P6,?4"%5:>a3,8H/68:l'"%576RX6H/"j6,o"%$'&BH/"%3+0%"%5:';H/"%3+"/& !o}3,RX"1@457"4C4H"%$'&L6,?4"%5:>a3,84RX6H/"ff6,o&1,&10%~P_jF'6^C'&L"%$43+"ff5:HMH/687,&C5:"%$'&QRX6l'02H/&Q6,o"%$'&YH/&3+02R2$ Q6Eff&1,&10@57"]C'6z&HK'6,"]g,&1&1?&'6l';$5:'ot6,02>a3+"%576#ot6,0]"%$'&6,?4"%5:>a3,8H/68:l'"%576#57"%H/&87o`"/6Ni&&X^"/023,RX"/&C@MH/6N57"R13,Z'6,"ffi&Pl4H/&C"/6n 4CH/68:l'"%5764H"/6 KnYP_H/&3+02R2$)?40%6,iG87&>aH h"ffEff6,0%g^Hffot6,0j"%$'&Y?Gl'0%?6H/&Q6,o5:>?40%6^5:';n"%$'&$'&l'025:H/"%5:R+@'$'6Eff&1,&10@ H%5:4RX&Bot6,0L"%$45:H`6487~"%$'&6,?4"%5:>a3,8M2XeA'&1&C4Hj"/6ai&Pg^'6EQ1fi44V ;WYX:Zv[\ V =]KW_^V W ;`fiabv;cedKf5fiavhgVji W :kv vlvdnm V WgVpo W qfim;a V :kv vlvsrtlcKlu`@;`fiabv;cAWK^Vv W:kv vlvdX:Zv[fiwtZxy V =]{:kv vlvAWgzVj{ W v;kv;l:kv vlvQgzVj| WYX:Zv[fiwtZxy V =]W_^Vj} W tfIf~:l~fia V WK^Vj~ W;`fiabv;cdo ;k5hgV :uWv;kv;lshgzV ;W tf V ,`v;c:lyfi`fiabv;cfi5uauWI^V W;`fiabv;cdo ;k5hgViWv;kv;l?`v;c@;`fiatkv~;`tlz;`vQgzV W tf Vu W_^Yfio;fiZutlu`cvVvWf` V vv:m?:k @=`f@,:k:mx;m~ = r;dBW_^V{Wl5q;`v`f=6dX:Zv[\ V =fi]KWgfixfiafiaeX:Zv[\q;m;`vav9pV|Wtf V l5q?;`v`f=Wn^fix='lu`@;`fiabv;cV}Wv;kv;lel5q;`v`f@=figzzV~W;`fiabv;cd V fiafiax:k ~x;`fiabv;cAWgV ;uWl5q;`v`f@ndB`bvfiafiax=4l5q;`v`f@=pQgV ~;Wtf V ;`fiabv;cAWK^V W`v V =]l5q;`v`f@Wo:lsyfi`fiabv;cfi5uahgzVi Wv;kv;lel5q;`v`f@=gzV W ave^fiK[tutlu`cvV v Wf` V vv:m?=f:l:k V WfiWK^V{ Wtf V cva5 V =]*=WK_m V =W_r;doW_^V| Wl5q;`vp;mv`tk;m?=6d_cva5 V =]*=WKeX:Zv[fiwtZxy V =fi]sncva5 V =]*=WfiWgV} Wtf V ;`fiabv;cAWK^V~ Wl5q;`v`f_dnl5q;`vp;mv`tk;m?=figVji uW`v V =]l5q;`v`f@Wo:lsyfi`fiabv;cfi5uahgVji ;Wv;kv;lIl5q;`v`f@=gzzVji Wave^Vjifii Wl5q;`vp;mv`tk;m?=6d_cva5 V =]*=WKnm V =WgzzVji;o Wl5q;`v`f@ndB9:ls`bvfiafiax=4l5q;`vp;mv`tk;m?=pQgVjiv W`v V =]l5q@;`vs`fW,:lI5kv~ ~ t5uahgVjifi{ Wv;kv;lel5q;`v`f@=gzz57;l'0%&KzIJL$'&B L3,87;,6,0257"%$4>qtEQ57"%$ZH/687,&C"%3+iG87&|1fi (^z7G4 % : 4^-G-Ns4-G^1 fJL$'&`3,87;,6,0257"%$4>r5:HH/g,&1"%R2$'&Ca5: 57;l'0%&` JL$'&ff>a3,5:]C45&10%&4RX&ffot0%6>r L 5:H5:K"%$'& P \KH%l'i'0%6l'"%5:'&,IEQ$'&)&X^?G3,4C45:';n3, KnY F'6^C'&,@G57"j0%&R1l'02H%57,&87~5:,6,g,&HL"%$'&>a3,5:)?40%6^RX&C4l'0%&P L @023+"%$'&10]"%$43,D"%$'& P \o}l44RX"%576 JL$zl4H1@ot6,0]&3,R2$TH%l4R1RX&H%H/6,0]"/6.3, KnY F'6^C'&,@ "%$'&)3,87;,6,0257"%$4>?&10%ot6,02>aHA3PH/&10257&HA6,oGH/&3+02R2$'&H EQ57"%$]5:4RX0%&3,H%5:';YRX6H/" i6l44C@H/"%3+0%"%5:';Yot0%6>"%$'&`$'&l'025:H/"%5:R&H/"%5:>a3+"/&6,oj"%$'&)H%l4R1RX&H%H/6,0n'6^C'&NqtEQ$45:R2$#ot6,0nH/6>&)H%l4R1RX&H%H/6,02H]>a3~#i&H%>a3,8:87&10K"%$43,D"%$43+"K6,oL"%$'& KnY'6^C'&)57"%H/&87o|3,4CD 45:H%$45:';EQ$'&[3.H/68:l'"%5765:H]ot6l44C6,0n"%$'&ZRX6H/"i6l44CD6,oQ"%$'&)?40%&C'&RX&H%H/6,0K6, ?4nY"%5: >aF3,'68^RXC'6&BH/"L5:H6,&Xo'"%RX$'&1&P&C'&X&^C ?GO 3,JL4C'$4&5:HCZ&'64H^%C'l'&,0%&@4H3,4C"%$4)3+&"`z"%l4$'3,&P8RX"/66aH/"j"%$'0%&B&1"%6,l'?402"%'&5:>aC3,5:8HffRX3,687H/E`"Q357~^oHj"%$'3n&876'6Eff^&1C'0`&Bi5:Hj6l4H/64C87,&6C "%ff$'~ &H/"/6,025:';]l'?C43+"/&CRX6H/"%H`6,o P_jF'6^C'&Hff5:"%$'&P$'&l'025:H/"%5:Rj"%3+iG87&,@^"%$'&PH/&3+02R2$ZRX6>?Gl'"/&H`3]?G3+0%"ff6,o"%$'&m'p$'&l'025:H/"%5:Ra3,H]3NH%5:C'&&X&RX"n3,4C@3,Hn'6,"/&CD&3+028:57&10@ "%$'&+3,8:l'&H]H/"/6,0%&C[5:#"%$'&"%3+iG87&i&RX6>&5:>a>&C45:3+"/&87~=3+3,5:8:3+iG87&not6,0l4H/&a5:.H%l'iGH/&zl'&"B$'&l'025:H/"%5:RK&1+3,8:l43+"%5764H L H/"/6,?GHH/&3+02R2$45:';"%$'&H%l4R1RX&H%H/6,02Hn6,oP3, KnY F'6^C'&3,HaH/6z6T3,H6'&Z5:Hnot6l44C"/6#$43,&=3.RX6H/"a;,0%&3+"/&10"%$43,["%$'&R1l'0%0%&"`i6l44C@^H%5:4RX&Y"%$45:Hj5:>?G8:57&HM"%$'&BRX6H/"L6,oA"%$'& KnY F'6^C'&B5:Hj3,8:H/6n;,0%&3+"/&10L"%$43,)"%$'&Bi6l44CQ6Eff&1,&10@H%5:4RX&"%$'&`3,87;,6,0257"%$4>?&10%ot6,02>aH0%&1?&3+"/&CnC'&1?4"%$^G02H/"AH/&3+02R2$'&H EQ57"%$K5:4RX0%&3,H%5:';Li6l44C4H1@0%&>a3,5:45:';nH%l4R1RX&H%H/6,02HL6,o"%$'& KnY F'6^C'&EQ5:8:8G&1,&"%l43,8:87~)3,8:H/6i&H/687,&C S$'&3,Zs)FH/68:l'"%576$43,H`i&1&ot6l44C@^3,8:8 H%l4R1RX&H%H/6,02H`"/6&1,&10%~ KnY F'6^C'&B3+?4?&3+025:';]5:"%$'&BH/68:l'"%576"/0%&1&$43,&Bi&1&H/&3+02R2$'&C@ff3,4C"%$'&570l'?C43+"/&C[RX6H/"%HaH/"/6,0%&C JL$45:HK&4H%l'0%&H]"%$43+""%$'&)0%&H%l487"%5:';N$'&l'025:H/"%5:R+@`c}c:@"%$43+"QC'&X '&C)iz~"%$'&K$'&l'025:H/"%5:RQ"%3+iG87&K3+ot"/&10Q"%$'&B0%&8:3^&CH/&3+02R2$=5:Hff 45:H%$'&C@'5:HjH/"%5:8:8RX64H%5:H/"/&"ff&R13,l4H/&"%$'&H%l4R1RX&H%H/6,0]'6^C'&HB6,o KnY F'6^C'&H]3+0%&H%l'iGH/&1"%H1@A L ot0%&zl'&"%87~.&4RX6l4"/&102H"%$'&KH%3,>&H/"%3+"/&)q}H/&1"L6,o ;,63,8:H|`>6,0%&"%$43,Z64RX&KC4l'025:';nH/&3+02R2$ JL$'&3,87;,6,0257"%$4>R13,i&H/?&1&C'&Cl'?@H%57;45 R13,"%87~,@iz~.H/"/6,025:';ZH/687,&C'6^C'&Haqti6,"%$ KnY F'6^C'&H3,4CDP_jF'6^C'&H|L"/6,;,&1"%$'&10KEQ57"%$"%$'&570n6,?4"%5:>a3,8ffRX6H/"a3,4C[H%$'6,0%"!FR1l'"/"%5:';."%$'&ZH/&3+02R2$[EQ$'&[57"]0%&3,R2$'&Ha3.'6^C'&"%$43+"$43,Hn3,870%&3,C'~i&1&TH/687,&C 2 !TC45&10%&4RX&Z"/6"%$'&876Eff&10i6l44C4H]H/"/6,0%&Ck5:"%$'&$'&l'025:H/"%5:R"%3+iG87&,@ffEQ$45:R2$T3+0%&+3,8:5:C3,8:H/65:"%$'&PxF 0%&1;,0%&H%H%576)H/&3+02R2$ZH/?G3,RX&Bot6,0j3,~xFS;Ws3,H`Eff&8:8 3,Hj5:"%$'&Y6,0257;5:43,8 H/&3+02R2$H/?G3,RX&,@ "%$'&]5:'ot6,02>a3+"%5765:"%$'&]H/687,&C"%3+iG87&]5:HL+3,8:5:CZ6487~)ot6,0Q"%$'&]R1l'0%0%&"Qs)0%&1;,0%&H%H%576=H/&3+02R2$q}H%5:4RX&H/"%3+"/&HK6,ojH%571&axFt@ot6,0KxF;s 3+0%&0%&8:3^&C#5:"%$'&as)0%&1;,0%&H%H%576DH/?G3,RX&aiGl'"K'6,"K5:"%$'&sxF 0%&1;,0%&H%H%576H/?G3,RX&|n 6,"/&n"%$43+"B3)H/"%3,4C43+02C="/023,4H/?6H%57"%576"%3+iG87&,@EQ$45:R2$0%&RX6,02C4HPl'?C43+"/&CNRX6H/"P&H/"%5:>a3+"/&HY6,offl4^H/687,&CW'6^C'&H1@5:Hn6,oP'6l4H/&5: L H%5:4RX&Zl'?C43+"/&C&H/"%5:>a3+"/&H6,oP_jF'6^C'&H3+0%&H/"/6,0%&CW5:"%$'&$'&l'025:H/"%5:R"%3+iG87&,@ffEQ$45:87&"%$'&$'&l'025:H/"%5:R&H/"%5:>a3+"/&Z6,oP3, KnY F'6^C'&=5:H3,87E`3~^H;57,&Tiz~"%$'&>a3'5:>]l4>6,o57"%HLH%571&BsH%l4R1RX&H%H/6,02HRTQSR'UWVL_N j g h ^6,03,W5:8:8:l4H/"/023+"%576#6,oY"%$'&Zl4H/&)6,oY0%&8:3^&CTH/&3+02R2$T"/6#5:>?40%6,&$'&l'025:H/"%5:Ra+3,8:l'&H1@RX64H%5:C'&10"%$'&ot68:876EQ5:';)H%5:>?G87&K?40%6,iG87&>ot0%6>"%$'&\^JL_L!9`\Z,&102H%576.6,o"%$'&\^3+"/&8:8:57"/&nC'6>a3,5:@5:"/0%6^C4l4RX&CN5:"%$'&](+*,*(a?G8:3,445:';nRX6>?&1"%57"%576 JL$'&B?40%6,iG87&>RX64RX&1024HQ3aH%3+"/&8:8:57"/&EQ$'6H/&B;,63,85:Hj"/63,R1zl4570%&5:>a3+;,&HM6,oC45&10%&"3,H/"/0%6'6>a5:R13,84"%3+0%;,&1"%HPqt0%&1?40%&H/&"/&Ciz~n"%$'&Q?40%&C45:R13+"/p& fi5,S,| JA6]C'6KH/6'@57"%H5:4H/"/02l4>&">]l4H/"G02H/"i&?6Eff&10%&CB6q5",| 3,4CR13,8:57i4023+"/&C5q h,|2@3,4CB"%$'&H%3+"/&8:8:57"/&M>]l4H/""%l'02DH/6="%$43+"]57"K5:H?65:"%5:';5:"%$'&C'&H%570%&CDC4570%&RX"%576kq5Qu,S,| !4H/"/02l4>&"KR13,8:57i4023+"%5760%&zl4570%&H"%$'&`H%3+"/&8:8:57"/&"/6Pi&?65:"%5:';Q3+"3YH/?&R15 RMR13,8:57i4023+"%576"%3+0%;,&1"Qq}5:"%$45:HA&X'3,>?G87&,@,C4570%&RX"%5761 X/jF2 XAF2, LF2 j !j!1/`2M! /L+21F2tL/22z! !! P}2/Y j Q%Xh ! Aj/2+%Aj/%F!AL!Q, !,/%/+1!2A !hA!2!h!A2^M2 2 PLL2 ]/] ] Mh2j ` Mj +% j + /Y` !j,2F2 L2 ] ff !/]!h1fi44{(img d4),(img d5),(img d6)}: 4 (3){(img d4),(img d5)}: 4 (3)(tk_img d4){(img d4),(img d6)}: 3{(point d4),(on),(cal),(img d5)}: 3 + 1{(img d5),(img d6)}: 3(tk_img d5){(point d5),(on),(cal),(img d4)}: 3 + 157;l'0%&B-'IQ9 3+0%"K6,oj"%$'&)(F_L&1;,0%&H%H%576D"/0%&1&qt&X^?G3,4C'&C#"/6N3=RX6H/"]i6l44C6,oL|Bot6,0K"%$'&&X'3,>?G87&\^3+"/&8:8:57"/&?40%6,iG87&> OjKnY F'6^C'&HY3+0%&]C'&1?G5:RX"/&Ciz~Z0%&RX"%3,';87&H1@P_jF'6^C'&HLiz~Z&8:8:57?GH/&HJL$'&QRX6H/"6,o&3,R2$'6^C'&Q5:HEL0257"/"/&a3,HK!&H/"%5:>a3+"/&xC tU3,R1R1l4>]l48:3+"/&C4 OM 6,0`'6^C'&HEQ$'6H/&&H/"%5:>a3+"/&CRX6H/"$43,HMi&1&al'?C43+"/&C3+ot"/&10&X^?G3,4H%576@"%$'&KqmX|&H/"%5:>a3+"/&Qi&1ot6,0%&j&X^?G3,^H%576Z5:Hj;57,&5:)?G3+0%&"%$'&H%5:Hz|\^5:4RX&"%$45:HQ5:HL"%$'&\^JL_L!9`\,&102H%576=6,o"%$'&]C'6>a3,5:@ 3,8:8A3,RX"%5764HY3+0%&n3,H%H%l4>&C"/6)$43,&nl4457"RX6H/"JA6g,&1&1?SH%571&=6,oB"%$'&=&X'3,>?G87&N>a3,43+;,&3+iG87&,@L87&1"1 H3,H%H%l4>&N3DRX6>?G87&1"/&.H/68:l'"%576W$43,Hi&1&RX6>?Gl'"/&CT6487~Dot6,0m3,4C["%$43+"a0%&8:3^&CWH/&3+02R2$W5:Hl4H/&CT"/6DRX6>?Gl'"/&3.?G3+0%"%5:3,8Lm $ H/68:l'"%57657;l'0%&HA-B3,4CnYH%$'6Eqt?G3+0%"6,o|"%$'&j(0%&8:3^&CnH/?G3,RX&`&X^?G876,0%&CKiz~"%$'&G02H/" 3,4C]H/&RX64Cn57"/&1023+"%576@0%&H/?&RX"%57,&87~,@'6,o3,Z L H/&3+02R2$=H/"%3+0%"%5:';ot0%6>"%$'&?40%6,iG87&> ;,63,8:H!T"%$'&)G02H/"a57"/&1023+"%576q 57;l'0%&Z-z|] L Q P \W5:HR13,8:87&CTEQ57"%$T3RX6H/"i6l44C6,oBz@j3,H"%$45:H5:Ha"%$'&&H/"%5:>a3+"/&CVRX6H/"6,oP"%$'&NH/"%3+0%"%5:';DH/"%3+"/&N;57,&kiz~["%$'&=?40%&RX6>?Gl'"/&CVm$'&l'025:H/"%5:R JL$'&0%6z6,"P'6^C'&n5:HY3, KnY F'6^C'&,@H/6EQ$'&N57"P5:HQ&X^?G3,4C'&CN L 5:HYR13,8:87&CNot6,0P&3,R2$H%571&(H%l'iGH/&1"q}8:5:'&Haq!w|YTq!5w |B5: 57;l'0%&| JL$'&G02H/"H%l4R2$#H%l'iGH/&1""/6i&;,&'&1023+"/&C5:[H fi5S3fi5Q JL$45:HffH/"%3+"/&B3,8:H/6n$43,H`3,&H/"%5:>a3+"/&CZRX6H/"`6,oz@'H/6] L Q P \)5:HffR13,8:87&CEQ57"%$a"%$45:Hi6l44C5:"%$'&KG02H/"Y57"/&1023+"%576@ iGl'"L"%$'&]"FEff6)?6H%H%57iG87&B0%&1;,0%&H%H%5764HQ6,oM"%$45:HQH/"%3+"/&ni6,"%$N87&3,C="/6)H/"%3+"/&HPEQ57"%$N3$457;$'&10QRX6H/"Q&H/"%5:>a3+"/&)q}3,&H/"%5:>a3+"/&C=RX6H/"Q6,on?G8:l4Hj3,=3,R1R1l4>]l48:3+"/&CNRX6H/"Q6,ojw| JL$'&'&1ERX6H/"5:HQ?40%6,?G3+;3+"/&C.iG3,R%g="/6)"%$'&n?G3+0%&"PH/"%3+"/&,@EQ$'&10%&K"%$'&5:>?40%6,&CNRX6H/"B&H/"%5:>a3+"/&qt-z|L6,o"%$'&n3+"/6>H/&1" fi5S3fi5Q5:HKH/"/6,0%&C5:"%$'&$'&l'025:H/"%5:Rn"%3+iG87&3,4C#0%&1"%l'02'&CVq}8:5:'&Hq,|YWq,|5: 57;l'0%&a| \^5:4RX&n"%$45:HP?Gl'"%HP"%$'&&H/"%5:>a3+"/&C#RX6H/"6,o`"%$'&aH/"%3+"/&'6E3+i6,&a"%$'&ai6l44CN6,off"%$'&L R13,8:8Mq}8:5:'&aqt-z|j5: 57;l'0%&|`'6>6,0%&57"/&1023+"%5764HL3+0%&C'6'& JL$'&'&1ERX6H/"Y5:H`0%&1"%l'02'&C)"/6"%$'&B L Q P \)?40%6^RX&C4l'0%&P&X^?G3,4C45:';]"%$'&B0%6z6,"L'6^C'&,@'EQ$45:R2$)3,8:H/60%&1"%l'024H`H%5:4RX&B"%$'&B0%6z6,"L'6^C'&5:Hj3, KnY F'6^C'&K3,4C57"j'6Er$43,HL3,l44H/687,&CH%l4R1RX&H%H/6,0]q}8:5:'&Hq!~w ,|Mq!5w |j5: 57;l'0%&| JL$45:H45:H%$'&Hff"%$'&BG02H/"j57"/&1023+"%576!N"%$'&nH/&RX64C57"/&1023+"%576[q 57;l'0%&a|L L Q P \.5:HYR13,8:87&CNEQ57"%$N3i6l44C6,o- h"Y?40%6^RX&1&C4H8:57g,&j"%$'&`G02H/"1@iGl'"M'6Ek"%$'&j&H/"%5:>a3+"/&CRX6H/"M6,o"%$'& K& QuS3"3 h3fi5nY F'6^C'fQQ5:HEQ57"%$45:K"%$'&Li6l44C@H/6B"%$45:HM'6^C'&L5:H &X^?G3,4C'&C JL$'&`G02H/"MH%571&Q(BH%l'iGH/&1"ot6,0MEQ$45:R2$n L5:H`R13,8:87&C)5:JH QuS3"@EQ57"%$)3,Z5:457"%5:3,84&H/"%5:>a3+"/&CZRX6H/"j6,o`w JL$'&YG02H/"`57"/&1023+"%576o}3,5:8:H"/6B 4C3H/68:l'"%576not6,0"%$45:HH/"%3+"/&,@ziGl'"MH%5:4RX&j"%$'&L'&1EkRX6H/"6,o(B5:HH/"%5:8:8^EQ57"%$45:]"%$'&ji6l44C5:>?6H/&Ciz~"%$'&K?G3+0%&" KnY F'6^C'&,@3H/&RX64C.57"/&1023+"%576.5:HQC'6'&]EQ$45:R2$Z 4C4HQ3H/68:l'"%576 JL$'&]'&1ExRX6H/"6,oL"%$'&Z3+"/6>H/&1," QuS3")5:H]H/"/6,0%&C[5:D"%$'&)$'&l'025:H/"%5:Ra"%3+iG87&)3,4C5:3,C4C457"%576@ "%$'&H/687,&CH/"%3+"/&HBq}3,876';EQ57"%$"%$'&5706,?4"%5:>a3,84H/68:l'"%576aRX6H/"|3+0%&Y3,8:84H/"/6,0%&C5:"%$'&QH/687,&Ca"%3+iG87&]q}8:5:'&H1}fi (^z7G4 % : 4^-G-Ns4-G^1 f{(img d4),(img d5),(img d6)}: 5 (4){(img d4),(img d5)}: 5 (4)(tk_img d4)(sw_on){(point d4),(off)}: 1 (1) + 1{(point d0),(img d5)}: 3 + 1{(point d5),(on),(cal),(img d4)}: 4 (3) + 1{(point d4),(img d5)}: 4 (3)(turn d0 d4)...(turn d1 d4){(point d1),(img d5)}: 3 + 1{(img d5),(img d6)}: 3(tk_img d5){(point d4),(on),(cal),(img d5)}: 4 (3) + 1{(point d4),(on)}: 2 (1){(img d4),(img d6)}: 3...(tk_img d5)...{(point d4),(point d5),(on),(cal)}: 3 (2) + 1(turn d6 d4)...{(point d6),(off)}: 0 + 257;l'0%&KzIQ9 3+0%"K6,oj"%$'&)(F_L&1;,0%&H%H%576D"/0%&1&qt&X^?G3,4C'&C#"/6N3=RX6H/"]i6l44C6,o`-z|Bot6,0K"%$'&&X'3,>?G87&\^3+"/&8:8:57"/&?40%6,iG87&> OjKnY F'6^C'&HY3+0%&]C'&1?G5:RX"/&Ciz~Z0%&RX"%3,';87&H1@P_jF'6^C'&HLiz~Z&8:8:57?GH/&HJL$'&RX6H/"B6,off&3,R2$D'6^C'&n5:HPEL0257"/"/&.3,H!&H/"%5:>a3+"/&C 3,R1R1l4>]l48:3+"/&C4 Bn 6,"/&a"%$43+"B"%$'&3,R1R1l4>]l48:3+"/&C[RX6H/"a5:HK6487~#3,876';N"%$'&?G3+"%$ot0%6>"%$'&Z'&3+0%&H/"n3,4RX&H/"/6,0 KnY F'6^C'&6 ,0B'6^C'&HQEQ$'6H/&K&H/"%5:>a3+"/&C.RX6H/"P$43,HQi&1&Nl'?C43+"/&C=3+ot"/&10P&X^?G3,4H%576@G"%$'&K&H/"%5:>a3+"/&i&1ot6,0%&&X^?G3,4H%5765:Hj;57,&=5:)?G3+0%&"%$'&H%5:H1I"%$45:Hj&H/"%5:>a3+"/&K5:4R18:l4C'&Hjl'?C43+"/&HL>a3,C'&K5:"%$'&?40%&1^576l4H`57"/&1023+"%576Dq}H%$'6EQ=5: 57;l'0%&P-z|q (9|L[q^w|P5: 57;l'0%&a| \^5:4RX&n"%$'&nG02H/"BH%l4R1RX&H%H/6,06,off"%$'& KnY F'6^C'&aE`3,HH/687,&C.&X^?G3,4H%576RX6"%5:zl'&HEQ57"%$."%$'&a'&X^"KH%l'iGH/&1"1@QuS3fi5CQ JL$45:HBH/"%3+"/&$43,HKH/&1,&1023,8?6H%H%57iG87&0%&1;,0%&H%H%5764H1@^H/6>&Y6,oEQ$45:R2$87&3,C"/6P_jF'6^C'&HiGl'"ffH/6>&P"/6 KnY F'6^C'&H 8:8@z$'6Eff&1,&10@'0%&1"%l'023N>a5:45:>]l4>qt&H/"%5:>a3+"/&NC t3,R1R1l4>]l48:3+"/&C |KRX6H/"n6,oL-'@MH/63,5:>?40%6,&C[RX6H/"Zqtot6,0n"%$'&Z3+"/6>H/&1"QuS3fi5CQ+|A5:H H/"/6,0%&C5:K"%$'&L$'&l'025:H/"%5:RM"%3+iG87&`3,4Cn"%$'&`?G3+0%&" K nY F'6^C'&`0%&>a3,5:4Hl44H/687,&C H%5:>a5:8:3+0?40%6^RX&H%H$43+?4?&4HEQ$'&]57"%HH%57iG8:5:';j'6^C'&,%@ QuQ3"3 h3fi5S@5:H&X^?G3,4C'&C@3,4CB"%$'&RX6H/"A6,o^"%$'&M3+"/6>H/&1" fi5S3fi5Q5:Hl'?C43+"/&CP64RX&ff>6,0%&,@"/6JL$'&K?40%6^RX&H%HQRX6"%5:zl'&HQ"%$'0%6l';$N3aot&1Ex>6,0%&]57"/&1023+"%5764H1@ l4"%5:83,8:8"%$'&]H%571&n(aH%l'iGH/&1"%HQ6,o"%$'&"/6,?'F87&1,&8;,63,8H/&1"P$43,&ni&1&NH/687,&C@"%$'&n>6H/"PRX6H/"%87~=3+"B3RX6H/"P6,EOB "P"%$45:HL?65:"1@l'?C43+"/&C&H/"%5:>a3+"/&HB6,oj,)H%571&a()3+"/6>H/&1"%H$43,&i&1&H/"/6,0%&C#5:N"%$'&$'&l'025:H/"%5:RK"%3+iG87&,@AH%8:57;$"%87~87&H%HP"%$43,$43,87o"%$'&Yzl4>Ki&10ff"%$43+"ffEff6l48:C$43,&Pi&1&H/"/6,0%&C)57o3]RX6>?G87&1"/&Bm $ H/68:l'"%576$43,Ci&1&RX6>?Gl'"/&ConQPQ|W#FnfiH!Iw`bt-n-ofInq>H E`3,H"%$'&QR13,H/&YEQ57"%$"%$'&Ym'pS$'&l'025:H/"%5:Rj57"%H/&87oh@3,C43+?4"%5:';0%&8:3^&CH/&3+02R2$a"/6]"%$'&L"/&>?6,023,8GR13,H/&Y5:HH%5:>?G87&Q5:a?4025:4R157?G87&,@iGl'"ffH/6>&1EQ$43+"`RX6>?G8:5:R13+"/&C5:?4023,RX"%5:RX& OM 5702H/"1@z"%$'&Y0%&8:3'3+"%576)5:"/0%6^C4l4RX&Ciz~K&zl43+"%5764HLqt-z|q| 3+?4?40%6'5:>a3+"/&H3P"/&>?6,023,8z0%&1;,0%&H%H%576H/"%3+"/J& 8Pu 'q ay ]|iz~]3]1ez6,oH/"%3+"/&HEQ57"%$'6l'"`3,RX"%5764H1@c}c:@^6,o"%$'&Pot6,02>'q F}y C| JA6g,&1&1?=>a3+"/"/&102HLH%5:>?G87&,@z6487~a&zl43+"%576#q|ff5:Hjl4H/&C19fi44; oQV `uasWI^u ytA\ V `uasWe^;`fiabvKm,IxQ]`v@:lK5kv~ ~ t5uahg ; `fiabv_m,exQ]`v:lI5kv~ ~ t5uahg` edX:Zvu\ V `uasS` fiavWgsd gzqfim; V lu`rv` fi :l?;`tlc~~;`tl WK^X:Zv[\ V `uasS` fiav=]B:lf~:lfiAWgtf Vva v;c `uaslu`@;`fiabv;cAWK^f5v;agfiI`~t~:l~fia `uasskfil;`fiab5uazd[@,gz` edeX:Zvu\ V `uasS` fiavWgz57;l'0%&KzIJL$'&JL9M-a3,4C} -?G8:3,445:';]?40%6^RX&C4l'0%&H5:)"%$'&0%&8:3^&CH/&3+02R2$IM"%$43+"Q5:H1@'"%$'&H%571&B6,o3aH/"%3+"/&K5:HLC'&X '&CZ3,Hq'ay]|vzu |I /ff01q'-'|A {' ,q|q'ay]|`Vmq*,|\z&RX64C@ &1,&.H/63H/"%3+"/&]6,oMH%571&]87&H%HQ"%$43,=s>a3~=H/"%5:8:8$43,&n3'6^&>?4"F~ RX6>?6'&"1@3,4CH%l4R2$]3QH/"%3+"/&`R13,]'6,"Ai&H/"/6,0%&C]5:"%$'&ff$'&l'025:H/"%5:R "%3+iG87&PqtEQ$45:R2$K>a3+?GH6487~B3+"/6>xH/&1"%HA"/6Y3,H%H/6^R15:3+"/&CRX6H/"%H|n &57"%$'&10LR13,)"%$'&P6,?4"%5:>a3,8RX6H/"`6,0L876Eff&10ji6l44Cot6l44Cot6,0LH%l4R2$)3H/"%3+"/&Bi&PH/"/6,0%&CZ3,H`"%$'&RX6H/"j6,oA"%$'&BRX6,0%0%&H/?64C45:';]3+"/6>H/&1"qt0257;$"j$43,4CH%5:C'&Y6,oA&zl43+"%576#q|/|2@GH%5:4RX&Y"%$'&P6,?4"%5:>a3,8RX6H/"6,o3,R2$457&1^5:';]"%$45:Hff3+"/6>H/&1"j>a3~i&P876Eff&10 Q6Eff&1,&10@G3]?G8:3,"%$43+"j3,R2$457&1,&<H wI_ ' , /ff01'q -'|3+{" :B3,8:H/6=3,R2$457&1,&H]"%$'&H/"%3+"/&'q ay ]|3+"]>6H/"]>a3 ' , "%5:>&l4457"%H8:3+"/&10@"%$'0%6l';$D5:'&10%"%5:3^@c}c:@|I /ff01q'-'|2ySC G ' >a 3,' ,JL$zl4H1@^"/6>a3,5:"%3,5:)"%$'&3,C4>a5:H%H%57iG5:8:57"F~n6,o"%$'&$'&l'025:H/"%5:RQo}l44RX"%576)C'&X '&C)iz~"%$'&RX6"/&"%HQ6,o"%$'&$'&l'025:H/"%5:RQ"%3+iG87&,@'"%$'&8:3+0%;,&H/K" K3,>6';3,8:83,RX"%5764HL5:p5:HjH%l'i4"/023,RX"/&CZot0%6>"%$'&RX6H/"Li&1ot6,0%&B57"L5:HH/"/6,0%&CY'ot6,0%"%l443+"/&87~,@,i6,"%$]6,o4"%$'&H/&jH%5:>?G8:5 R13+"%5764HEff&3+g,&"%$'&j$'&l'025:H/"%5:R+3,8:l'&Hot6l44CKiz~0%&8:3^&CH/&3+02R2$ S$43+"5:HnEff6,02H/&,@`H%5:4RX&Z3#RX6H/"l44C'&10/F3+?4?40%6'5:>a3+"%5765:Ha3+?4?G8:57&CDEQ$'&TH/"/6,025:';#H/"%3+"/&HRX6"%3,5:45:';nRX64R1l'0%0%&"j3,RX"%5764H1@'iGl'"ff'6,"jC4l'025:';K"%$'&BH/&3+02R2$@'"%$'&P$'&l'025:H/"%5:RQC'&X '&Ciz~a"%$'&P"%3+iG87&3+ot"/&100%&8:3^&CUH/&3+02R2$R13,Ui&5:4RX64H%5:H/"/&" 8:H/6'@Y0257;$"!FH%$457ot")R1l'"%HR13,U'6,"Zi&l4H/&CU5:V"%$'&0%&8:3^&CH/&3+02R2$ H]>&"%576'&CD&3+028:57&10@5:#"%$'&H/&3+02R2$[H/?G3,RX&?402l4'&Ciz~0257;$"!FH%$457ot"KR1l'"%H1@"%$'&?6H%H%57iG87&KH%l4R1RX&H%H/6,02HY"/6Z3)H/"%3+"/&,@3,4CN"%$'&10%&1ot6,0%&n"%$'&nRX6H/"P0%&1"%l'02'&C=EQ$'&="%$'&H/"%3+"/&5:HY&X^?G3,4C'&Cqt0%&1;,0%&H%H/&C |QiGl'"B'6,"H/687,&C@A>a3~.i&C45&10%&"BC'&1?&4C45:';6"%$'&n?G3+"%$"%$'0%6l';$.EQ$45:R2$.57"BE`3,H0%&3,R2$'&C OM ;3,5:@"%$45:HR13,'6,"Mi&jH/"/6,0%&C5:n"%$'&L$'&l'025:H/"%5:Rff"%3+iG87&,@3,4Ca57"MR13,'6,"i&L57;'6,0%&CaH%5:4RX&"%$45:HjRX6l48:CZ^5768:3+"/&B"%$'&K3,C4>a5:H%H%57iG5:8:57"F~n6,o "%$'&$'&l'025:H/"%5:R1fi (^z7G4 % : 4^-G-Ns4-G^1 fyO} -JL$'& } - ?G8:3,445:';L?40%6^RX&C4l'0%&Bq}H%$'6EQ5: 57;l'0%&L|ARX64H%5:H/"%H 6,oG"%$'0%&1&j>a3,5:nH/"/&1?GH1IA"%$'&ffG02H/" 5:H"/6?40%&RX6>?Gl'"/&L"%$'&L"/&>?6,023,8Gm $ $'&l'025:H/"%5:R+@,"%$'&QH/&RX64Ca"/6K?&10%ot6,02> 3KH/&10257&H6,os)0%&8:3^&CH/&3+02R2$'&H1@ot6,0ns uzy1{1{1{f@M5:D6,02C'&10n"/65:>?40%6,&"%$'&)$'&l'025:H/"%5:R+@3,4CD"%$'& 43,8ff5:Hn3, L H/&3+02R2$T5:D"%$'&"/&>?6,023,80%&1;,0%&H%H%576NH/?G3,RX&,@;l45:C'&C=iz~"%$'&RX6>?Gl'"/&C.$'&l'025:H/"%5:Rn 6,"/&n"%$43+"P"%$'&]G02H/"Y3,4C.8:3,H/"6,o"%$'&Q"%$'0%&1&YH/"/&1?GH3+0%&P5:C'&"%5:R13,8'"/6]"%$'6H/&Q6,oJL9M-'I "%$'&L6487~C45&10%&4RX&Y5:H"%$'&Y5:"/&102>&C45:3+"/&YH/"/&1?@"%$'&BH/&10257&H6,oA0%&8:3^&CH/&3+02R2$'&H JL$'&Y?Gl'0%?6H/&Q6,o"%$'&H/&BH/&3+02R2$'&Hj5:Hff"/6C45:H%RX6,&10@'3,4C)H/"/6,0%&B5:"%$'&$'&l'025:H/"%5:R"%3+iG87&,@5:>?40%6,&C.RX6H/"P&H/"%5:>a3+"/&HP6,offH/"%3+"/&Haqhc}c:@3+"/6>H/&1"%H|L6,offH%571&]s HP5:4C45:R13+"/&C5: 57;l'0%&z@0%&8:3^&C.H/&3+02R2$'&H3+0%&R13+0%0257&C=6l'"Pot6,0Psu zy1{1{1{f@l4"%5:8H/6>&H/"/6,?4?G5:';RX64C457"%5765:HjH%3+"%5:H!G&C JL$'&10%&3+0%&KH/&1,&1023,80%&3,H/643+iG87&H/"/6,?4?G5:';RX64C457"%5764H`"%$43+"QR13,i&Bl4H/&CIq/|)H/"/6,?NEQ$'&"%$'&n8:3,H/"Qs)0%&1;,0%&H%H%576=H/&3+02R2$.C'6z&HQ'6,"Y&4RX6l4"/&10P3,~ KnY F'6^C'&)q}5:EQ$45:R2$R13,H/&K"%$'&B0%&8:3^&CH/68:l'"%576Z5:HL5:)o}3,RX"Q3aH/68:l'"%576)"/6a"%$'&B6,0257;5:43,8?40%6,iG87&>|2q y2|)H/"/6,?N3+"Q3n4^&C#B d+d;57,&Zsq/|)H/"/6,?EQ$'&a"%$'&YRX6H/"6,o"%$'&Qs)FH/68:l'"%576ot6l44Ca5:HM"%$'&YH%3,>&Y3,H"%$43+"6,o"%$'&nqtGw|FH/68:l'"%576qt6,0Y$'&l'025:H/"%5:RQ&H/"%5:>a3+"/&|2q/|)H/"/6,?N3+ot"/&10Y3RX&10%"%3,5:=3,>6l4"j6,o "%5:>&,@4zl4>Ki&10j6,o &X^?G3,4C'&C'6^C'&H1@^6,0YH%5:>a5:8:3+05:>?G87&>&"%H"%$'&aG02H/"K"%$'0%&1& @+ 3,R2$0%&H%l487"%HK5:D3NC45&10%&"]RX6^G;l'023+"%576D6,oj"%$'&?G8:3,4'&10@,3 4Cl4H%l43,8:87~3,8:H/65:Z3C45&10%&4RX&5:Z?&10%ot6,02>a3,4RX& !Z"%$'&KRX6>?&1"%57"%576@G3n4^&C8:5:>a57"j3+"QsvuE`3,Hl4H/&C OB+ 'RX&1?4"BEQ$'&10%&a57"B5:HP&X^?G8:5:R157"%87~H/"%3+"/&C#6,"%$'&10%EQ5:H/&,@"%$45:HB5:HP"%$'&aRX6^G;l'023+"%576l4H/&C5:"%$'&&X^?&1025:>&"%H`?40%&H/&"/&C5:)"%$'&K'&X^"LH/&RX"%576=3,HLEff&8:8} -F-+Z J(F @CB F@JL$45:HH/&RX"%576?40%&H/&"%H`3]RX6>?G3+025:H/66,o"%$'&P0%&8:3+"%57,&Y?&10%ot6,02>a3,4RX&Q6,oAJL9M-n3,4C } - 6"%$'&PC'6+>a3,5:4HQ3,4C?40%6,iG87&>H/&1"%HQ"%$43+"YEff&10%&]l4H/&C=5:"%$'&n(+*,*+-)?G8:3,445:';RX6>?&1"%57"%576@ 3,4C=3,N3,43,87~^H%5:H6,o "%$'&0%&H%l487"%H JL$'&B0%&H%l487"%H`?40%&H/&"/&C=$'&10%&K3+0%&ot0%6>0%&102l4445:';]i6,"%$Z?G8:3,4'&102H`6"%$'&KRX6>?&X"%57"%576?40%6,iG87&>vH/&1"%H1@ '6,"B"%$'&3,RX"%l43,8M0%&H%l487"%HPot0%6>"%$'&RX6>?&1"%57"%576 JL$45:HP5:HBot6,0"FEff6=0%&3,H/64H1I5702H/"1@3,H3,870%&3,C'~n>&"%576'&C@&10%0%6,02HM5:n"%$'& } - 5:>?G87&>&"%3+"%576a>a3,C'&L57"%H?&10%ot6,02>a3,4RX&L5:n"%$'&RX6>?&1"%57"%576H/6>&1EQ$43+"Eff6,02H/&L"%$43,EQ$43+"57"5:H3,RX"%l43,8:87~R13+?G3+iG87&L6,o \z&RX64C@"%$'&Q0%&1?&3+"/&C02l44HEff&10%&>a3,C'&nEQ57"%$N3)>6,0%&n;,&'&10%6l4HY"%5:>&8:5:>a57"L"%$43,N"%$43+"5:>?6H/&C=C4l'025:';"%$'&nRX6>?&1"%57"%576N"/66,i4"%3,5:>6,0%&KC43+"%3a3,4CZ&43+iG87&3ni&1"/"/&10YRX6>?G3+025:H/6 9 8:H/6'@4H/6>&&X^?&1025:>&"%H`Eff&10%&02l4)EQ57"%$3,87"/&10243+"%57,&LRX6^G;l'023+"%5764H6,o "%$'&`?G8:3,4'&102H &1"%3,5:87&CC'&H%RX0257?4"%5764H6,oG"%$'&LRX6>?&1"%57"%576nC'6>a3,5:4H3+0%&;57,&Ziz~)Q6+>a3,4@ + C'&87g+3,>?1eff+7cPq(+*,*+-'@ |~u tn-r#kJL$'&S~uYC'6>a3,5:]>6^C'&8:HA"/023,4H/?6,0%"%3+"%576K6,o!iG3+"%R2$'&H%Y6,oG?&1"/0%687&l4>?40%6^C4l4RX"%HA"%$'0%6l';$"wIg3?G57?&8:5:'&)'&1"FEff6,0%g JL$'&>a3,5:WC45&10%&4RX&ot0%6> 6,"%$'&10"/023,4H/?6,0%"%3+"%576WC'6>a3,5:4Ha5:H"%$43+""%$'&9 Qh,h j!1 hYL IfiT jQ j ff,h !!a]2+nB2j/2F !P'n 1 P h`L2F ff+2n+%/] ]L!j,h 3`41h LX Fn+2L%1!!2%27Y`2+a/X%76Nj!j2B K2F27j`ff ff1!!2MhMP21F2ff2h! !!M%24! ://+A%/QffY!XhF2z }F2!! P+%F2 !72! ff %XhF2 AtL2X!+2`2^h,h j!1/1fi1410001000100100Lower Bound H*10000HSP* : Time (seconds)HSP*a: Time (seconds)10000441011101001000TP4: Time (seconds)q}3|10000101110100qti|1000TP4: Time (seconds)100001312TP4 (Search)HSP*a (Rel. Search)HSP* (Final Search)1110101001000Time (seconds)q}R|1000057;l'0%&LIP\z68:l'"%576)"%5:>&HLot6,0QJL9M-3,4C } - 6?40%6,iG87&>aHjH/687,&C=5:Z"%$'&S~uaC'6>a3,5:q}3|YEQ57"%$'6l'"P"%3,'g+3+;,&0%&H/"/025:RX"%576@3,4Ckqti|QEQ57"%$."%3,'g+3+;,&0%&H/"/025:RX"%5764H JL$'&,&10%"%5:R13,88:5:'&B"/6"%$'&0257;$"Q5:)G;l'0%&qti|`5:4C45:R13+"/&Hj"%$'&K"%5:>&X6l'"Q8:5:>a57"qt"%$zl4H1@G"%$'&K"%$'0%&1&?65:"%H6]"%$'&`8:5:'&RX6,0%0%&H/?64CK"/6Y?40%6,iG87&>r5:4H/"%3,4RX&H H/687,&CK6487~Biz~ } - | q}R| + ,68:l'"%576K6,o"%$'&P876Eff&10ffi6l44C6)H/68:l'"%576RX6H/"`C4l'025:';B0%&8:3^&C3,4C'6,02>a3,8q}'6^0%&8:3^&C |H/&3+02R2$5:?40%6,iG87&> ffn6,oA"%$'K& S~unC'6>a3,5:qt,&102H%576)EQ57"%$'6l'"`"%3,'g+3+;,&B0%&H/"/025:RX"%576 |\z"%3+02H5:4C45:R13+"/&MEQ$'&10%&H/68:l'"%5764HA3+0%&ot6l44C Omn 6,"/&ff"%$43+" 3,8:8"%5:>&H%R13,87&H 3+0%&ff876,;3+0257"%$4>a5:R?G57?&8:5:'&HM>]l4H/"`i&Q 8:87&C3+"j3,8:8 "%5:>&H1@'H/6nEQ$'&6'&BiG3+"%R2$)&"/&102Hj3]?G57?&nq}5:Hn!?Gl4H%$'&C4|3,'6,"%$'&10iG3+"%R2$N>]l4H/"Q87&3,&]"%$'&?G57?&B3+"Q"%$'&K6,"%$'&10Q&4Cqti&=!?6,?4?&C4| JL$'&KC'6>a3,5:RX6>&HY5:Z"FEff6,&10/H%5764H1@^6'&EQ57"%$)0%&H/"/025:RX"%5764H`6W!"%3,'g+3+;,&.q}H/?G3,RX&ot6,0Y5:"/&102>&C45:3+0%~H/"/6,023+;,&|L3,4C)6'&EQ57"%$'6l'"H%l4R2$Z0%&H/"/025:RX"%5764H87"%$'6l';$'&57"%$'&10JL9M-Q'6,0 } - 3,R2$457&1,&ff,&10%~P;,6z6^C0%&H%l487"%H5:B"%$45:HC'6>a3,5:@57"A5:H3,&X'3,>?G87&6,oP3C'6>a3,5:EQ$'&10%& } - ?&10%ot6,02>aH]i&1"/"/&10"%$43,WJL9M- O[ 57;l'0%&oH ^q}3|n3,4CN^qti|]RX6>?G3+0%&Z"%$'&02l4"%5:>&H`6,o "%$'&B"FEff6?G8:3,4'&102Hff6Z"%$'&KH/&1"L6,o ?40%6,iG87&>aH`H/687,&CZiz~Z3+"Q87&3,H/"j6'&& ^q}R|YRX6>?G3+0%&HP"%$'&]i&$43^576l'0Y6,o"%$'&]"FEff6Z?G8:3,4'&102HL6N6'&n&X'3,>?G87&]?40%6,iG87&>Z@ ff57;l'0%ot0%6>"%$'&.C'6>a3,5:k,&102H%576kEQ57"%$'6l'""%3,'g+3+;,&.0%&H/"/025:RX"%576@Q5:k>6,0%&.C'&1"%3,5:8 JL$45:Ha?40%6^5:C'&H3,5:8:8:l4H/"/023+"%57,&&X'3,>?G87&]6,o0%&8:3^&C.H/&3+02R2$.EQ$'&N57"YEff6,0%g^HB3,HP5:"/&4C'&C \^5:4RX&Ki6,"%$N?G8:3,4'&102HQl4H/&57"/&1023+"%57,&ffC'&1&1?&45:';LH/&3+02R2$'&H1@,"%$'&i&H/"Ag^'6EQK876Eff&10i6l44CB6"%$'&ffRX6H/"6,o'"%$'&?40%6,iG87&>H/68:l'"%576EQ5:8:8i&5:4RX0%&3,H%5:';'@ H/"%3+0%"%5:';Not0%6>"%$'&)5:457"%5:3,8m $ &H/"%5:>a3+"/&,@"%$'0%6l';$3NH/&10257&HK6,o]qt0%&8:3^&C[3,4C'6^0%&8:3^&C |H/&3+02R2$'&H`EQ57"%$)5:4RX0%&3,H%5:';Ki6l44C@^l4"%5:8 3nH/68:l'"%576)5:Hffot6l44CI"%$'&P;,023+?G$)?G876,"%Hff"%$45:H&1,68:l'"%576a6,o"%$'&YH/68:l'"%576aRX6H/"`876Eff&10ffi6l44C3+;3,5:4H/""%5:>& HffR13,i&QH/&1&@'0%&1;,0%&H%H%576H/&3+02R2$0%&3,R2$'&H3.H/68:l'"%576SqtEQ57"%$RX6H/"w(|o}3,H/"/&10"%$43,"%$'&Z'6,02>a3,80%&1;,0%&H%H%576[H/&3+02R2$TC45:H%RX6,&102H]"%$43+""%$'&10%&]5:HL'6H/68:l'"%576ZEQ57"%$45:)"%$'&KH%3,>&]RX6H/"Yi6l44C JL$'&B 43,8q}'6^0%&8:3^&C |ff0%&1;,0%&H%H%576=H/&3+02R2$5: } - 5:HL3,8:H/6ao}3,H/"/&10Q"%$43,"%$43+"L6,oMJL9M-=q}3,HY5:4C45:R13+"/&C)iz~)"%$'&KH%876,?&B6,o "%$'&KR1l'0%,&|2@ C4l'&B"/6"%$'&$'&l'025:H/"%5:RY5:>?40%6,&>&"%HQH/"/6,0%&CC4l'025:';n"%$'&B0%&8:3^&CH/&3+02R2$"wIgQh3n-kg|tn-r#kgq< &10%"%3,5:g^5:4C4HM6,oA>6^C'&8GR2$'&R%g^5:';n?40%6,iG87&>aH1@zH%l4R2$3,Hff"%$'&PC'&1"/&RX"%576)6,oAC'&3,C4876^R%g^Hff3,4C3,H%H/&10%"%576ff^5768:3+"%5764H1@ff3+0%&&H%H/&"%5:3,8:87~zl'&H/"%5764H6,oY0%&3,R2$43+iG5:8:57"F~Uqt6,0l4'0%&3,R2$43+iG5:8:57"F~4|K5:TH/"%3+"/&X"/023,4H%57"%576;,023+?G$4H JL$'r& Qh3]C'6>a3,5:5:H"%$'&Y0%&H%l487"M6,o"/023,4H%8:3+"%5:';KH%l4R2$)>6^C'&8GR2$'&R%g^5:';]?40%6,iG87&>aH1@ot6,0H/~^H/"/&>>6^C'&8:H`&X^?40%&H%H/&C5:)"%$'& `d/+X7KH/?&R15 R13+"%576Z8:3,';l43+;,&,@G5:"/6a9 P JL$'&B?40%6,iG87&>aH~fi (^z7G4 % : 4^-G-Ns4-G^1 f10000HSP*a: Time (seconds)HSP*a: Time (seconds)10000100010010101001000TP4: Time (seconds)1001110000100TP4: Time (seconds)q}3|10000qti|57;l'0%&{zIP\z68:l'"%576)"%5:>&Hjot6,0QJL9M-3,4C } - 6Z?40%6,iG87&>aHjH/687,&C=5:#q}3|`"%$'&KQh3aC'6>a3,5:Tq S==,SZH%l'iGH/&1"|3,4CUqti|"%$'& DC'6>a3,5:UqvShN5:4H/"%3,4RX&)H%l'iGH/&1"| JL$'&,&10%"%5:R13,8Q8:5:'&)"/6#"%$'&Z0257;$"5:4C45:R13+"/&Hn"%$'&"%5:>&X6l'"8:5:>a57"Zqt"%$zl4H1@ff?65:"%Hn6W"%$'&=8:5:'&RX6,0%0%&H/?64C)"/6a?40%6,iG87&>5:4H/"%3,4RX&HLH/687,&CZ6487~iz~ } - |l4H/&CW5:T"%$'&NRX6>?&1"%57"%576W3+0%&N5:4H/"%3,4RX&H6,oB"FEff6C45&10%&"C'&3,C4876^R%gTC'&1"/&RX"%576k?40%6,iG87&>aHqt"%$'&/C45:45:';]?G$45:876H/6,?G$'&102H%]3,4CW!6,?4"%5:R13,8"/&87&1;,023+?G$4?40%6,iG87&>aH|M6,o5:4RX0%&3,H%5:';H%571&JL$'H& KC'6>a3,5:n>6^C'&8:HA"%$'&`?40%6,iG87&>6,o40%&RX6^G;l'025:';P3Yo}3,l487"F~?6Eff&10M'&1"FEff6,0%g]"/6P0%&H%l'?4?G87~RX64H%l4>&102Hn3&RX"/&C[iz~#"%$'&)o}3,l487" Y4RX&10%"%3,5:"F~[RX64RX&10245:';="%$'&Z5:457"%5:3,8MH/"%3+"/&Z6,oL"%$'&)?40%6,iG87&>qt"%$'&zl4>Ki&10K3,4C#876^R13+"%576#6,o`o}3,l487"%H|2@l4'0%&8:5:3+iG87&n3,RX"%5764HK3,4C?G3+0%"%5:3,8@AH/6>&1"%5:>&H&1,&Do}3,8:H/&,@6,iGH/&10%+3+"%5764H3+0%&Z5:>?6,0%"%3,"]ot&3+"%l'0%&H6,oQ"%$'&Z3+?4?G8:5:R13+"%576@iGl'"]"%$'&H/&3,H/?&RX"%HnEff&10%&H%5:>?G8:5G&C3E`3~ot0%6>"%$'&]C'6>a3,5:l4H/&C=5:Z"%$'&KRX6>?&1"%57"%576 JL$'&KC'6>a3,5:C45:C$'6Eff&1,&10B>a3+g,&]H%57;45 R13,"l4H/&=6,o xRX64H/"/02l4RX"%H)3,4Ck"%$'&N'&1EvC'&10257,&Ck?40%&C45:R13+"/&Hot&3+"%l'0%&N6,o9 P ( ( JL$'&RX64H/"/02l4RX"%H3,4CnC'&10257,&C]?40%&C45:R13+"/&H R13,ni&`RX6>?G5:87&Cn3E`3~,@iGl'"6487~K3+"M3,]&X^?6'&"%5:3,8^5:4RX0%&3,H/&5:k?40%6,iG87&>H%571& JL$'&10%&1ot6,0%&N6487~T"%$'&.H%>a3,8:87&H/")5:4H/"%3,4RX&HEff&10%&3+3,5:8:3+iG87&.5:k?G8:3,5:S\^JL_L!9`\ot6,02>]l48:3+"%576@3,4C[i&R13,l4H/&Z6,oY"%$45:H]"%$'&1~Eff&10%&Z"%$'&Z6487~5:4H/"%3,4RX&Hn"%$43+"aJL9M-#3,4C } - RX6l48:C3+"/"/&>?4"L"/6H/687,&JL$',& Qh33,4C. C'6>a3,5:4H3+0%&'6^"/&>?6,023,8@ff5:"%$'&=H/&4H/&Z"%$43+"a3,RX"%576kC4l'023+"%5764H3+0%&.'6,")RX64H%5:C'&10%&C@jiGl'"'&57"%$'&103+0%&N"%$'&1~kH/"/025:RX"%87~TH/&zl'&"%5:3,8@c}c:@L3,RX"%5764H)R13,V"%3+g,&.?G8:3,RX&RX64R1l'0%0%&"%87~ ff&R13,l4H/&)6,oj"%$45:H1@ JL9M-N3,4C } - Eff&10%&02l4D5:#?G3+023,8:87&8@A023+"%$'&10]"%$43,#"/&>?6,023,8@?G8:3,445:';>6^C'&6D?40%6,iG87&>aH]5:#"%$'&H/&ZC'6>a3,5:4HX * JL$'&0%&H%l487"%H6,oL"%$'&"FEff6?G8:3,4'&102H1@ H%$'6EQ5: 57;l'0%[& z@ 3+0%&]H%5:>a5:8:3+0j"/6"%$'6H/&K&X'$457iG57"/&C5:Z"%$'f& S~uC'6>a3,5:I } - 5:Hji&1"/"/&10Y"%$43,JL9M-P6,&1023,8:8@H/687^5:';P>6,0%&ff?40%6,iG87&>aH5:]i6,"%$nC'6>a3,5:4H3,4CnH/687^5:';Y"%$'&`$43+02C'&105:4H/"%3,4RX&Ho}3,H/"/&10@EQ$45:87&PJL9M-5:H`o}3,H/"/&10Y3+"QH/687^5:';n&3,H/~Z5:4H/"%3,4RX&H"TOwIg 9QhhQ tn-r#kJL$'&L9QhhQKC'6>a3,5:>6^C'&8:HffH%3+"/&8:8:57"/&Hff"%3,H/g,&C)EQ57"%$>a3+g^5:';]3,H/"/0%6'6>a5:R13,8 6,iGH/&10%+3+"%5764H OMH%5:>?G8:5G&C\^JL_L!9`\,&102H%5766,o"%$'&BC'6>a3,5:E`3,HjC'&H%RX0257i&C5:Z\z&RX"%576 ( w !"%$'&P;,&'&1023,8C'6+/ fi'%F2 !1 2 4,!! 21/2 2!j,2F2X 2 +%! !2 2h z+/X4 ^F% !hF2 P j /% 2A /2M% j !j!1/7 !1 /2 + Y,2P 2h!~,<fi444h10000HSP*a: TimeTime (seconds)1h100010010TP4HSP*1CPTp01p02p03p04p05p06p07Problem (source parameters)1m1s0.1s0.1sp08q}3|1s1mTP4: Time1h4hqti|57;l'0%&KzIP\z68:l'"%576"%5:>&HYot6,0PJL9M-)3,4C } 6=?40%6,iG87&>aHYH/687,&C.5:"%$'&,9QhhQC'6>a3,5:5:8:87&CVqtiG8:3,R%g4|?65:"%H]0%&1?40%&H/&"n5:4H/"%3,4RX&H]i&876';5:';"/6."%$'&ZRX6>?&1"%57"%576?40%6,iG87&>H/&1"1@EQ$45:87&0%&>a3,5:45:';=?65:"%Hn3+0%&Zot0%6>"%$'&H/&1"n6,oY3,C4C457"%57643,8?40%6,iG87&>aH];,&'&1023+"/&C"%$'&RX6>?&1"%57"%576+H/&13,"jR23,$r4C!EQH%5:$'C'6&EQZHffRX"%6$'8:l4&P>aH/6.58:l':"%N576G";l'%5:0%>&=&q}H3ot|Y6,0%0`&1"%?4$'0%&P&H/H/&&1""`%6,HBoA65:'&4H/"%?43,4R0%6,XiG&Hff87&;,>v&'&ot10%0263+>v"/&C)EQ57"%$a"%$'&BH%3,>&?G3+023,>&1"/&102H.qt;,0%6l'?&Ck5:"/6H%l'iRX68:l4>a4Hniz~T?G8:3,4'&10X| P487~[H/687,&CV5:4H/"%3,4RX&H3+0%&H%$'6EQ@4H/6a'6,"Q3,8:8RX68:l4>a4Hj$43,&"%$'&H%3,>&Kzl4>Ki&10j6,o?65:"%H OM 57;l'0%&aqti|`RX6>?G3+0%&HJL9M-n3,4C } - C4570%&RX"%87~ JL$'&Y,&10%"%5:R13,88:5:'&Q"/6n"%$'&P0257;$"`5:4C45:R13+"/&Hff"%$'&Y"%5:>&X6l'"j8:5:>a57"qt"%$zl4H1@4?65:"%H`6Z"%$'&K8:5:'&P3+0%&K5:4H/"%3,4RX&HLH/687,&CZiz~ } - iGl'"L'6,"Liz~)JL9M-z|>a3,5:@^"%$'&10%&R13,Zi&B>6,0%&P"%$43,Z6'&H%3+"/&8:8:57"/&,@^&3,R2$&zl457?4?&CEQ57"%$)>6,0%&B"%$43,)6'&5:4H/"/02l4>&"1@3,4CNC45&10%&"P5:4H/"/02l4>&"%HQ$43,&C45&10%&"P5:>a3+;5:';R13+?G3+iG5:8:57"%57&H]q}R13,8:87&CV/>6^C'&H%|2@EQ$45:R2$=>a3~6,&1028:3+?qti&1"FEff&1&=5:4H/"/02l4>&"%Hff3,4C)i&1"FEff&1&=H%3+"/&8:8:57"/&H| H+ 3,R2$Z;,63,85:H`"/6a$43,&K3,Z5:>a3+;,&B"%3+g,&6,oA3]H/?&R15 RL"%3+0%;,&1"1@45:3]H/?&R15 RQ>6^C'& H`5:a"%$'&K\^JL_L!9`\n,&102H%576@z"%3+g^5:';n3,5:>a3+;,&Y0%&zl4570%&H"%$'&a0%&87&1+3,"K5:4H/"/02l4>&"P"/6=i&?6Eff&10%&C#6D3,4C#R13,8:57i4023+"/&C@A3,4C#"/6=R13,8:57i4023+"/&3,#5:4H/"/02l4>&""%$'&H%3+"/&8:8:57"/&>]l4H/"i&?65:"/&C"/6E`3+02C4H]3R13,8:57i4023+"%576"%3+0%;,&1" Jl'0245:';)"%5:>&HBi&1"FEff&1&C45&10/&"C4570%&RX"%5764HY+3+0%~ HB3,#3,C4C457"%57643,8RX6>?G8:5:R13+"%576@3+">6H/"B6'&a5:4H/"/02l4>&"Y6i63+02C.&3,R2$H%3+"/&8:8:57"/&BR13,Zi&P?6Eff&10%&CZ63+"L3,~"%5:>& JL$zl4H1@^"/6>a5:45:>a571&Q6,&1023,8:8&X^&R1l'"%576Z"%5:>&P0%&zl4570%&H3R13+0%&1o}l48H/&87&RX"%576=6,oEQ$45:R2$H%3+"/&8:8:57"/&)q}3,4CN5:4H/"/02l4>&"|ff"/6l4H/&Kot6,0Y&3,R2$N6,iGH/&10%+3+"%576@ 3,4C="%$'&6,02C'&10Q5:)EQ$45:R2$)&3,R2$NH%3+"/&8:8:57"/&BR13+0%0257&Hj6l'"L"%$'&6,iGH/&10%+3+"%5764Hj57"L$43,HLi&1&Z3,H%H%57;'&CJL$45:HC'6>a3,5:V5:H)$43+02CVot6,0)i6,"%$SJL9M-T3,4C } - @Qot6,0H/&1,&1023,8P0%&3,H/64H1I 5702H/"1@Q3,HZ3,870%&3,C'~>&"%576'&C@z"%$'&BRX6,0%&P6,o"%$'&BC'6>a3,5:5:Hff3nRX6>KiG5:43+"%5763,)3,H%H%57;4>&"ff?40%6,iG87&> 3,4C)3]JY\^9F8:57g,&?40%6,iG87&>Z@^i6,"%$)6,o EQ$45:R2$Z3+0%&$43+02C)6,?4"%5:>a573+"%576)?40%6,iG87&>aH 8:H/6'@'"%$'&]m $ $'&l'025:H/"%5:RQ"/&4C4Hj"/6ai&?G3+0%"%5:R1l48:3+0287~]Eff&3+ga6)JY\^93,4C0%&8:3+"/&C?40%6,iG87&>aHYqt"%$'&PH%3,>&QEff&3+g^'&H%H`$43,Hff3,8:H/6]i&1&'6,"/&Ciz~\^>a57"%$=q(+*,*+-z|Mot6,0"%$'&Q?G8:3,445:';Y;,023+?G$$'&l'025:H/"%5:R+@EQ$45:R2$a5:HM&H%H/&"%5:3,8:87~]"%$'&YH%3,>&Y3,Hffm $ | \z&RX64C@3,RX"%576=C4l'023+"%5764H`5:)"%$45:HjC'6>a3,5:ZC45&10jiz~Z8:3+0%;,&3,>6l4"%HL3,4C3+0%&K3+"L"%$'&H%3,>&"%5:>&H/?&R15G&CEQ57"%$T3.,&10%~[$457;$0%&H/68:l'"%576 O[ 6,0&X'3,>?G87&,@ff5:?40%6,iG87&> ff>.6'&3,RX"%576k$43,H3C4l'023+"%576[6,ow+{ (+*+-#3,4C[3,'6,"%$'&103.C4l'023+"%576#6,,(z{ , S$'&[l4H%5:'; LEQ57"%$D"/&>?6,023,80%&1;,0%&H%H%576@"%$'&RX6H/"Bi6l44C="/&4C4HP"/65:4RX0%&3,H/&niz~="%$'&n;R1Ckqt;,0%&3+"/&H/"KRX6>a>6C457^5:H/6,0X|j6,off3,RX"%576#C4l'023+"%5764HY5:~fi (^z7G4 % : 4^-G-Ns4-G^1 f&3,R2$D57"/&1023+"%576@&X'RX&1?4"Kot6,0B"%$'&G02H/"Pot&1E57"/&1023+"%5764HX/ !"%$'&x9QhhQZC'6>a3,5:@"%$'&;R1C#6,o3,RX"%576ZC4l'023+"%5764H`5:Hff"F~z?G5:R13,8:87~,&10%~)H%>a3,8:8 qt6)"%$'&P6,02C'&10j6,o * * | <ff6>KiG5:'&CEQ57"%$"%$'&PEff&3+g^'&H%H6,oP"%$'&.m $ $'&l'025:H/"%5:R+@EQ$45:R2$W>&3,4Ha"%$'&NC45&10%&4RX&i&1"FEff&1&W"%$'&N5:457"%5:3,8`$'&l'025:H/"%5:R)&H/"%5:>a3+"/&=6,o"%$'&H/68:l'"%576NRX6H/"aq}>a3+g,&H/?G3, |L6,off3?40%6,iG87&>3,4CN"%$'&3,RX"%l43,8 6,?4"%5:>a3,8RX6H/"B5:HY6,ot"/&8:3+0%;,&,@"%$45:H0%&H%l487"%H5:K3,n3,8:>6H/" 3,H/"/0%6'6>a5:R13,8zzl4>Ki&106,o4 L57"/&1023+"%5764HAi&5:';Q0%&zl4570%&Ci&1ot6,0%&ff3YH/68:l'"%5765:Hffot6l44C JA63,65:C)"%$45:HBq}H/6>&1EQ$43+"j3+0%"%5 R15:3,8t|?40%6,iG87&>Z@^3,RX"%576ZC4l'023+"%5764HEff&10%&P0%6l44C'&Cl'?)"/6"%$'&P'&3+0%&H/"`5:"/&1;,&10j5:a"%$'&Q&X^?&1025:>&"%HffC'6'&Y5:a"%$45:HffC'6>a3,5: JL$45:H5:4RX0%&3,H/&H"%$'&P>a3+g,&H/?G3,6,o"%$'&j?G8:3,4Hot6l44C@,iGl'"'6,"M,&10%~n>]l4R2$]K6a3,&1023+;,&Qiz~(z{ @z3,4C3+">6H/"iz~z{ q}RX6>?G3+025:H/6>a3,C'&B6"%$'&B?40%6,iG87&>aH`"%$43+"QRX6l48:C)i&H/687,&CZEQ57"%$)6,0257;5:43,8C4l'023+"%5764H|% $l'&"/6Z"%$'&nEff&3+g^'&H%HB6,o"%$'&m $ $'&l'025:H/"%5:RK5:N"%$45:HPC'6>a3,5:@"%$'&&X6,0%"5:,&H/"/&C.iz~ } - 5:RX6>?Gl'"%5:';B3>6,0%&L3,R1R1l'023+"/&Q$'&l'025:H/"%5:RffR13,ai&`&X^?&RX"/&C"/6?G3~n6+ @0%&H%l487"%5:';P5:3Bi&1"/"/&10M6,&1023,8:802l4"%5:>&Yot6,0 } - RX6>?G3+0%&C)"/6aJL9M- JL$45:Hff5:Hj5:4C'&1&C"%$'&R13,H/&,I3,87"%$'6l';$ } - H/687,&H`6487~"%$'&G,&H%>a3,8:87&H/"P?40%6,iG87&>aHB5:N"%$'&H/&1"q}H%$'6EQ#3,HBiG8:3,R%g.?65:"%HB5: 57;l'0%&a'qti|/|2@ JL9M-H/687,&H6487~ot6l'0L6,o"%$'6H/&,@G3,4C5:HjH%8:57;$"%87~H%876Eff&10L6>6H/"L6,o"%$'&> JL$'&H/&B0%&H%l487"%H1@'$'6Eff&1,&10@ 3+0%&K'6,"Qzl457"/&0%&1?40%&H/&"%3+"%57,&JL$'& 9QhhQBC'6>a3,5:a$43,H38:3+0%;,&Qzl4>Ki&10M6,o?40%6,iG87&> ?G3+023,>&1"/&102H1I"%$'&Yzl4>Ki&106,o;,63,8:H3,4C"%$'&Qzl4>Ki&106,oH%3+"/&8:8:57"/&H1@5:4H/"/02l4>&"%HM3,4C"%$'&Q5:4H/"/02l4>&"R13+?G3+iG5:8:57"%57&H1@G1ec:@EQ$45:R2$C'&1"/&10/>a5:'&B"%$'&Kzl4>Ki&10L6,o E`3~^HQ"/63,R2$457&1,&K&3,R2$=;,63,8 9M0%6,iG87&>5:4H/"%3,4RX&HQl4H/&C5:Z"%$'&KRX6>?&1"%57"%576Eff&10%&Q;,&'&1023+"/&C023,4C'6>a87~,@EQ57"%$+3+0%~^5:';B?G3+023,>&1"/&10`H/&1"/"%5:';HX . JL$'&LRX6>?&1"%57"%576a?40%6,iG87&> H/&1"1@EQ$45:R2$D$43,H]"/6N6+&10R2$43,8:87&';5:';=?40%6,iG87&>aH"/6.3=EQ5:C'&+3+0257&1"F~#6,oL?G8:3,4'&102Hqti6,"%$D6,?4"%5:>a3,8ff3,4CH%l'i6,?4"%5:>a3,8t|EQ$45:87&Bot6,0Y?4023,RX"%5:R13,8A0%&3,H/64HP'6,"Qi&5:';a"/6z6)8:3+0%;,&,@H%R13,87&HYl'?N"%$'&]C45&10%&"Q?G3+023,>n&1"/&102HPzl457"/&KH/"/&1&1?G87~,@3,4CZ>6,0%&]5:>?6,0%"%3,"%87~ZRX6"%3,5:4HQ6487~Z6'&K?40%6,iG87&>5:4H/"%3,4RX&Kot6,0Y&3,R2$H/&1"j6,o ?G3+023,>&1"/&102HLl4H/&C Q6Eff&1,&10@G"%$'&$43+02C4'&H%H`6,o 3n?40%6,iG87&>5:4H/"%3,4RX&>a3~)C'&1?&4CZ3,HL>]l4R2$q}57o'6,">6,0%&|Q6."%$'&n023,4C'6>&87&>&"%HB6,o"%$'&?40%6,iG87&>;,&'&1023+"%576kqtEQ$45:R2$.5:4R18:l4C'&,@Mc c:@"%$'&"%l'0245:';]"%5:>&Hji&1"FEff&1&"%3+0%;,&1"%HY3,4CZ"%$'&3,RX"%l43,8A3,8:876^R13+"%576)6,oR13+?G3+iG5:8:57"%57&H`3,4CR13,8:57i4023+"%576"%3+0/;,&1"%H"/65:4H/"/02l4>&"%H|Q3,HB6"%$'&aH/&1"/"%5:';HB6,off"%$'&aRX6"/0%68:8:3+iG87&n?G3+023,>&1"/&102H JA6=5:,&H/"%57;3+"/&a"%$'&5:>?6,0%"%3,4RX&L6,o"%$'&Q023,4C'6>?40%6,iG87&> &87&>&"%Hffot6,0ff?40%6,iG87&>$43+02C4'&H%H1@3,4C"/6]6,i4"%3,5:3i40%63,C'&10iG3,H%5:HPot6,0"%$'&aRX6>?G3+025:H/6.i&1"FEff&1&DJL9M-3,4C } - @"/&#3,C4C457"%57643,8?40%6,iG87&>aHYEff&10%&a;,&'&1023+"/&Cq}l4H%5:';"%$'&P3+3,5:8:3+iG87&Q?40%6,iG87&>;,&'&1023+"/6,0X|ot6,0`&3,R2$6,o"%$'&Q?G3+023,>&1"/&10jH/&1"/"%5:';H`RX6,0%0%&H/?64C45:';B"/6"%$'&n&57;$"H%>a3,8:87&H/"Y?40%6,iG87&>aHY5:N"%$'&RX6>?&1"%57"%576.H/&1" JL$'&nC45:H/"/0257iGl'"%5766,offH/68:l'"%576N"%5:>&HYot6,0JL9M-'@ } - 3,4CN<`9Jqt"%$'&6487~6,?4"%5:>a3,8"/&>?6,023,8?G8:3,4'&10ji&H%5:C'&HjJL9M-a3,4C } - "/6?G3+0%"%5:R15fi4]2+_./102 /%L2h F% j2ffF% +2 P1nQ/a2A F% +2 '2+Q `j/212/%!}F% +2F+%O&ff2+&2 1!h2+'2+% 2h*,+!jM`/K22h F% M2+ff h!j!1 ff!},+``,+ ]/2FhF% L h/2/j!}2A /2}!} j+% 22h+2+/`M+/ MM!}2,%XL,+] L! hF% :2ff,h`}F2+%F[mQ/%F,F]+%ff 1!1 n+2,!M,j7P2Fh2F/P2 2h F% 27+! F2 % !hA24F2F ` /Y!hF% /%FP+2!%1 L2 t/2,Q!}2/1B f}F2B j /% F!j FaYL2X!+22Y~L24 F2X!2` 2 ,h,+ YF2F~ t2+5 f,+/%F' Y`2 +22h F% G7!G 2 P22222^j24hF2 F4 2z/ 4!j,h 2z vafiavvL2 hffff Y/%F]}F2!!ff /PF2X` ! j+2]Q 2 [m`/%F] Y2 +2F% !G f}F2F!j A2 /2 `21YL2 !h X!! Y!hF2 2Y,!1!}!T!hF%2/2P, +Y% mv fi tlfil:l 5;m'v!kfiAv;`s ~~;`tlA !~2 +%F2jhh%ff,h 2'%! !!L% ff#ff,h2^ }j!1 ,h%!ff,h'2 h!1Gh% `j!!2F2ff,h'2,F%h!2+ff,h42,h% jX2 !~>fi44?G3+"/&P5:a"%$'&PRX6>?&1"%57"%576 | 6&3,R2$6,o"%$'&Y0%&H%l487"%5:';B?40%6,iG87&>H/&1"%H`5:HffH%$'6EQ5: 57;l'0%&P'q}3| JL$'&5:4H/"%3,4RX&Hj"%$43+"jEff&10%&?G3+0%"j6,o"%$'&RX6>?&1"%57"%576)?40%6,iG87&>H/&1"L3+0%&KH%$'6EQ)iz~ 8:87&CqtiG8:3,R%g4|ff?65:"%H<`87&3+0287~,@'"%$'&+3+025:3+"%5765:)?40%6,iG87&>$43+02C4'&H%HL5:HLRX64H%5:C'&1023+iG87&B3,4CZ6,o "%$'&?40%6,iG87&>aHj5:)"%$'&KRX6>n?&1"%57"%576[H/&1"aH/6>&3+0%&Z,&10%~&3,H/~[3,4CTH/6>&3+0%&Z,&10%~$43+02C@M0%&8:3+"%57,&Z"/6"%$'&H/&1"6,oY?40%6,iG87&>aH;,&'&1023+"/&CEQ57"%$)"%$'&KH%3,>&?G3+023,>&1"/&102H57;l'0%&n'qti|LRX6>?G3+0%&HPJL9M-)3,4C } - 6="%$'&]&X^"/&4C'&C=?40%6,iG87&>H/&1" } - H/687,&HB, 6,o"%$45:H`H/&1"1@'EQ$45:87&YJL9M-H/687,&HL^uw q}3H%l'iGH/&1"ff6,o"%$'6H/&BH/687,&C)iz~ } - | Q6Eff&1,&10@ 3,HjR13,)i&BH/&1&5:"%$'&YG;l'0%&,@z"%$'&B0%&8:3+"%57,&Y?&10%ot6,02>a3,4RX&P6,oA"%$'&P"FEff6?G8:3,4'&102Hff5:H`3,8:H/6$457;$487~n+3+0257&C@'>]l4R2$Z>6,0%&H/6a"%$43,Z"%$'&B0%&H%l487"%H`6"%$'&RX6>?&1"%57"%576Z?40%6,iG87&>H/&1"QH%l';,;,&H/"%HQ % tn-r#kJL$'&[Q%ZC'6>a3,5:N>6^C'&8:HQ"%$'&n>6,&>&"%HB6,o3,5702RX023+ot"Y6N"%$'&];,0%6l44CN3+"P3,.3,570%?6,0%""wIgJ $'&L;,63,85:HB"/6;l45:C'&a3+0%0257^5:';)3,5702RX023+ot""/6?G3+0%g^5:';Z?6H%57"%5764HP3,4C#C'&1?G3+0%"%5:';3,5702RX023+ot"B"/6=3H%l457"%3+iG87&02l4E`3~]ot6,0M"%3+g,&16+ @^3,876';B"%$'&L3,570%?6,0%"'&1"FEff6,0%gn6,o "%3'57E`3~^H JL$'&L>a3,5:RX6>?G8:5:R13+"%5765:H "/6Bg,&1&1?"%$'&P3,5702RX023+ot"`H%3+ot&87~aH/&1?G3+023+"/&CIM3+"`>6H/"`6'&P3,5702RX023+ot"ffR13,6^R1R1l'?z~3K02l4E`3~a6,0`"%3'57E`3~H/&1;>&"3+"Q3,~"%5:>&,@43,4CZC'&1?&4C45:';]6)"%$'&H%571&B6,o"%$'&3,5702RX023+ot"L3,4C)"%$'&K8:3~,6l'"j6,o"%$'&3,570%?6,0%"j'&3+0%iz~H/&1;>&"%HQ>a3~)i&BiG876^R%g,&C3,HLEff&8:8JL9M-ZH/687,&HB6487~w6l'"P6,off"%$'&+*)?40%6,iG87&>v5:4H/"%3,4RX&HB5:N"%$45:HPC'6>a3,5: O] 6,0"%$'&5:4H/"%3,4RX&HH/687,&Cniz~]JL9M-'@"%$'&jzl4>Ki&10 6,o'6^C'&H&X^?G3,4C'&Cn5:nH/&3+02R2$a5:H,&10%~nH%>a3,8:8z0%&8:3+"%57,&`"/6B"%$'&jH/68:l'"%576C'&1?4"%$qt"%$'6l';$[ot6,0a"%$'&Z8:3+0%;,&105:4H/"%3,4RX&H1@'6^C'&)&X^?G3,4H%576T5:H],&10%~[H%876E@0%&H%l487"%5:';.5:[3.?6z6,002l4"%5:>&Z6,&1023,8:8t| JL$45:H5:>?G8:57&Hn"%$43+"aot6,0"%$'&H/&?40%6,iG87&> 5:4H/"%3,4RX&H"%$'&.m $ $'&l'025:H/"%5:R)5:Ha,&10%~3,R1R1l'023+"/&,@43,4C"%$zl4Hff"%$'&1~3+0%&B5:3]H/&4H/&)!&3,H/~^^4ot6,0jH%l4R2$)5:4H/"%3,4RX&H1@ } - R13,Z'6,"ffi&Y&X^?&RX"/&C"/6Zi&]i&1"/"/&10@H%5:4RX&n"%$'&H/&3+02R2$.&X6,0%"57"B5:,&H/"%HB5:"/6ZRX6>?Gl'"%5:';)3)>6,0%&3,R1R1l'023+"/&$'&l'025:H/"%5:RK5:H8:3+0%;,&87~]E`3,H/"/&C@iGl'"57"M3,8:H/65:4C45:R13+"/&H "%$43+"3>6,0%&L3,R1R1l'023+"/&Y$'&l'025:H/"%5:Rff5:HM'&1&C'&Cn"/6KH/687,&/$43+02C4?40%6,iG87&>5:4H/"%3,4RX&HQ6Eff&1,&10@ } - H/687,&HP6487z~ a?40%6,iG87&>aH1@3ZH%l'iGH/&1"P6,o"%$'6H/&aH/687,&C.iz~NJL9M-'@A3,4CN"%3+g,&Ho}3+0>6,0%&B"%5:>&Pot6,0L&3,R2$ 57;l'0%&w1*4q}3|jH%$'6EQH`"%$'&B"%5:>& } - H/?&4C4H`5:0%&1;,0%&H%H%576H/&3+02R2$3,4CZ5:"%$'&Q 43,8q}'6^0%&8:3^&C |H/&3+02R2$)ot6,0ff&3,R2$)6,o"%$'f& Q%5:4H/"%3,4RX&Hff57"ffH/687,&H OM 6,0`0%&1ot&10%&4RX&,@^"%$'&H/&3+02R2$="%5:>&ot6,0YJL9M-5:HL3,8:H/65:4R18:l4C'&C <`87&3+0287~,@4"%$'&0%&8:3^&C=H/&3+02R2$NRX64H%l4>&HQ3876,"L6,o "%5:>&K5:"%$45:HjC'6>a3,5:@43,4CZ6+&102Hj,&10%~Z8:57"/"%87&B5:Z"%$'&BE`3~Z6,o $'&l'025:H/"%5:RP5:>?40%6,&>&"L5:Z0%&1"%l'02 JL$43+"L"%$'&$'&l'025:H/"%5:RB5:>?40%6,&>&"Y5:HQH%>a3,8:8q}R1876H/&K"/6'6^&X'5:H/"/&"|L5:HL&3,H%5:87~&X^?G8:3,5:'&C@GH%5:4RX&,@G3,HY3,870%&3,C'~6,iGH/&10%,&C@A"%$'&m $ $'&l'025:H/"%5:R]5:HB3,870%&3,C'~N,&10%~3,R1R1l'023+"/&a6"%$'&H/&a?G3+0%"%5:R1l48:3+0B?40%6,iG87&>v5:4H/"%3,4RX&HJL$'&zl'&H/"%576@'"%$'&@45:H`EQ$~"%$'&0%&8:3^&CH/&3+02R2$=5:HjH/6a"%5:>&RX64H%l4>a5:';JL$'&)3+?4?G3+0%&"n0%&3,H/6T5:H]"%$43+"a5:D"%$45:HnC'6>a3,5:@MH/&3+02R2$T5:"%$'&0%&1;,0%&H%H%576TH/?G3,RX&Z5:Hn>6,0%&&X^?&4H%57,&]"%$43,H/&3+02R2$5:="%$'&'6,02>a3,80%&1;,0%&H%H%576.H/?G3,RX& JL$45:HY5:HYRX6"/023+0%~N"/6Z"%$'&n3,H%H%l4>?4"%576H/"%3+"/&CD5:D\z&RX"%576D w+@A"%$43+""%$'&aRX6H/"6,o`&X^?G3,4C45:';)3H/"%3+"/&H%$'6l48:CNi&aH%>a3,8:87&10B5:."%$'&0%&8:3^&C0%&1;,0%&H%H%576H/?G3,RX&,@+C4l'&"/6Y3LH%>a3,8:87&10i4023,4R2$45:';`o}3,RX"/6,0 J3+iG87&Lw1*4qti|C45:H/?G8:3~^HH/6>&R2$43+023,RX"/&1025:H/"%5:R1H6,o"%$'&]'6,02>a3,8A3,4C.0%&1;,0%&H%H%576NH/?G3,RX&HQot6,f0 Q%)5:4H/"%3,4RXf& ff.qt"%$'&nH%>a3,8:87&H/"Q5:4H/"%3,4RX&]'6,"H/687,&Ciz~JL9M-z| OM 3+"%3n5:HRX68:87&RX"/&C)C4l'025:';B"%$'&QG02H/"Pqto}3,5:87&C | 57"/&1023+"%5766,o LL \z"%3+"/&H5:#"%$'&)'6,02>a3,80%&1;,0%&H%H%576DH/?G3,RX&ZRX6"%3,5:@ 6[3,&1023+;,&,@ff3N8:3+0%;,&)zl4>Ki&10K6,oQH%l'i4;,63,8:H1@ EQ$45:87&a5:"%$'&a0%&1;,0%&H%H%576.H/?G3,RX&,@H/"%3+"/&HRX6,0%0%&H/?64C45:';a"/6NP_jF'6^C'&HY3+0%&niz~=C'&X 457"%576N8:5:>a57"/&C=5:NH%571&<ff64H/&zl'&"%87~,@'"%$'&Pi4023,4R2$45:';no}3,RX"/6,0L6,oMP_jF'6^C'&Hj5:Z0%&1;,0%&H%H%576Z5:HjH%>a3,8:87&10q}H%5:4RX&P"%$'&R2$'65:RX&6,oL&H/"%3+iG8:5:H%$'&10Kot6,0]&3,R2$TH%l'i4;,63,8ff5:H]3=?6,"/&"%5:3,8i4023,4R2$D?65:"|2@ iGl'"K'6,"]iz~D>]l4R2$I"%$'&)>a3,~H%l'i4;,63,8:Ha5:T"%$'&='6,02>a3,8`0%&1;,0%&H%H%576W5:"/&1023,RX"1@j0%&H%l487"%5:';5:[0%&8:3+"%57,&87~ot&1ERX64H%5:H/"/&"R2$'65:RX&H~1fi (^z7G100004 % : 4^-G-Ns4-G^1 fnHSP*a (Rel. Search)HSP* (Final Search)1000TP4 (Search)6,02>a3,8F_L&1;,0%&H%H%576P_K nY8>9z{85F 8>w+{ *(z{ +i4023,4R2$45:';o}3,RX"/6,0w+{w+{ *z{ *Time (sec.)1001010.10.01p01p02p03q}3|p04p05p10p11*^{qti|57;l'0%&aw1*^I]q}3|JL5:>&WH/?&"D5:0%&1;,0%&H%H%576 H/&3+02R2$3,4C5:x 43,8=q}'6^0%&8:3^&C |NH/&3+02R2$6Q%N5:4H/"%3,4RX&HH/687,&Ciz~ } - JL$'&aH/&3+02R2$#"%5:>&ot6,0KJL9M-5:HB3,8:H/6H%$'6EQot6,0RX6>?G3+025:H/6n 6,"/&L"%$'&L876,;3+0257"%$4>a5:R"%5:>&jH%R13,87&,I H/&3+02R2$"%5:>&H ot6,0 } - 3,4CJL9M-3+0%&'&3+0287~N5:C'&"%5:R13,8@EQ$45:87&]"%$'&a0%&1;,0%&H%H%576H/&3+02R2$#RX64H%l4>&HBH/&1,&1023,86,02C'&102HP6,off>a3+;+457"%l4C'&B>6,0%&B"%5:>& qti|j<`$43+023,RX"/&1025:H/"%5:R1H`6,o "%$'&'6,02>a3,83,4C0%&1;,0%&H%H%576H/?G3,RX&Hjot6,0Q%Z5:4H/"%3,4RX& ff4{I 8>'5:HL"%$'&]3,&1023+;,&aH/"%3+"/&nH%571&,H 85F* 85F '"%$'&]3,&1023+;,&023+"%5766,oH%l4R1RX&H%H/6,0LH/"%3+"/&H%571&P"/6n"%$'&BH%571&Y6,oA"%$'&P?40%&C'&RX&H%H/6,0jH/"%3+"/& 3+"%3a5:HffRX68:87&RX"/&CZC4l'025:';"%$'&BG02H/"Kqto}3,5:87&C |57"/&1023+"%576Z6,o LL:8 H/6'@ff"%$'&Z0257;$"!FH%$457ot"R1l'"02l487&,@EQ$45:R2$[&8:5:>a5:43+"/&HH/6>&Z0%&C4l44C43,"ni4023,4R2$'&H1@`5:Hl4H/&CW5:["%$'&'6,02>a3,8P0%&1;,0%&H%H%576UH/?G3,RX&,@PiGl'"Z'6,"EQ$'&V&X^?G3,4C45:';kP_jF'6^C'&HZ5:U0%&1;,0%&H%H%576 Q6Eff&1,&10@0%&1;,0%&H%H%576"/&4C4H"/6Y>a3+g,&ffH/"%3+"/&HP!;,0%6EQ^@'c}c:@H%l4R1RX&H%H/6,0 H/"%3+"/&HA;,&'&1023,8:87~BRX6"%3,5:K>6,0%&ffH%l'i4;,63,8:H"%$43,)"%$'&570ff?40%&C'&RX&H%H/6,02H1@43,4CEQ$45:87&Q"%$45:Hff&X&RX"L5:H`zl457"/&P>6^C'&1023+"/&B5:)'6,02>a3,8 0%&1;,0%&H%H%576@zEQ$'&10%&H%l4R1RX&H%H/6,02H`$43,&,@'6)3,&1023+;,&,@ >6,0%&BH%l'i4;,63,8:H1@^57"ff5:H`>]l4R2$)>6,0%&Y?40%6'6l44RX&Cot6,0`"%$'&PH%>a3,8:87&10H/"%3+"/&HMRX6,0%0%&H/?64C45:';Y"/6KP_jF'6^C'&H5:]"%$'&L0%&1;,0%&H%H%576H/?G3,RX&,@EQ$'6H/&jH%l4R1RX&H%H/6,02HM3+0%&`63,&1023+;,&(z{ +e}tj8:3+0%;,&10 OM HL30%&H%l487"1@'H%l4R1RX&H%H/6,02HL"/6)P_jF'6^C'&Hj3+0%&K3,8:8 KnY F'6^C'&H1@4EQ57"%$Z3,3,&1023+;,&6,o3+i6l'J" z{ H%l'i4;,63,8:HL3,4C*^{ H%l4R1RX&H%H/6,02H]q}H%l'iGH/&1"%Hj6,o H%571&]|JA6WH%l4>a>a3+02571&,@Y&3,R2$U&X^?G3,4C'&CrP_jF'6^C'&5:U0%&1;,0%&H%H%576U0%&H%l487"%Hqt^5:3T3,U5:"/&102>&C45:3+"/&/8:3~,&102=6,o KnY F'6^C'&H|B5:#3,D3,&1023+;,&)6,oJ9{ (='&1EP_jF'6^C'&H Op+ ,&#"%$'6l';$D>6H/"K6,o`"%$'&>q*-'{ ( a|3+0%&`ot6l44C5:]"%$'&j L H/687,&Cn"%3+iG87&,@3,4Cn"%$'&10%&1ot6,0%&LC'6 "$43,&L"/6Bi&jH/&3+02R2$'&C@"%$'6H/&"%$43+"Y0%&>a3,5:~^57&8:C3,=&X&RX"%57,&.2P_j"/6+!P_Qi4023,4R2$45:';ao}3,RX"/6,0P6,oLwz{ Nq(,z{ 6,c9{ (|2@"/6i&RX6>?G3+0%&C#EQ57"%$."%$'&ai4023,4R2$45:';)o}3,RX"/6,0K6,oBw+{ )ot6,0K'6,02>a3,80%&1;,0%&H%H%576 Oa ;3,5:@A"%$'&a?40%6,iG87&>5:H'6,"j"%$'&$457;$)i4023,4R2$45:';]o}3,RX"/6,0Y5:Z57"%H/&87ohI57"L5:H`"%$43+"j"%$'&i4023,4R2$45:';]o}3,RX"/6,0Y5:)"%$'&B0%&8:3^&CZH/&3+02R2$H/?G3,RX&Y5:HMo}3+0Q7+'Xd"%$43,57"5:HMot6,0`'6,02>a3,840%&1;,0%&H%H%576@z3,4Ca"%$43+"ffH/&3+02R2$)5:"%$'&P0%&1;,0%&H%H%576H/?G3,RX&5:HMRX64H/&zl'&"%87~n>6,0%&L&X^?&4H%57,&`"%$43,aH/&3+02R2$5:n"%$'&L'6,02>a3,8'0%&1;,0%&H%H%576aH/?G3,RX&,@023+"%$'&10M"%$43,a87&H%H~~fiTypeType IIType III10010010001011101001000TP4: Time (seconds)q}3|100001000100100010000HSP* : Time (seconds)HSP* : Time (seconds)10000HSP* : Time (seconds)10000441011101001000TP4: Time (seconds)qti|100001011101001000TP4: Time (seconds)10000q}R|57;l'0%&aw,w+IP\z68:l'"%576."%5:>&HPot6,0KJL9M-3,4C."%$'0%&1&aC45&10%&"RX6^G;l'023+"%5764HP6,o } - 6?40%6,iG87&>aHH/687,&C5:"%$'{& 3SaC'6>a3,5:IKq}3| } - EQ57"%$=s)0%&1;,0%&H%H%576.8:5:>a57"/&C"/6)su 6487~qti|#/l448:5:>a57"/&C4 } - qt?&10%ot6,02>aHas)0%&1;,0%&H%H%5764Hot6,0)5:4RX0%&3,H%5:';#s l4"%5:8L&57"%$'&10)3'6^0%&8:3^&C.H/68:l'"%576N5:HYot6l44C@ 6,0B"%$'&n&H/"%5:>a3+"/&C.RX6H/"B6,o"%$'&]"/6,?#87&1,&8;,63,8:HBC'6z&H'6,"5:4RX0%&3,H/&|2`q}R|%- } - q}3,87E`3~^HB?&10%ot6,02>aHBQ3,4CN-+0%&1;,0%&H%H%576 | JL$'&8:5:'&H"/6"%$'&K0257;$"Y3,4C"/6,?.5:ZG;l'0%&H]qti|j3,4C[q}R|L5:4C45:R13+"/&"%$'&K"%5:>&X6l'"Y8:5:>a57" /J~z?&KN%%!0%&1ot&102HP"/6"%$'&aR18:3,H%H%5 R13+"%576N6,off"%$'&?40%6,iG87&>v5:4H/"%3,4RX&HBC'&H%RX0257i&C.5:#\z&RX"%576- =qt?G3+;,&](, ,|"wIg3S tn-r#kJL$'f& 3SC'6>a3,5:=>6^C'&8:HL"%$'&[NJY\R13,8:8AH/&1"!Fl'?.?40%6^RX&C4l'0%&ot6,0PC43+"%33+?4?G8:5:R13+"%5764HL5:=>6,iG5:87&"/&87&1?G$'6'&H JL$'&)C'6>a3,5:[5:Hn3,RX"%l43,8:87~D3.H%R2$'&C4l48:5:';=?40%6,iG87&>Z@H%5:>a5:8:3+0K"/6?G6EQH%$'6,? JL$'&ZR13,8:8H/&1"!Fl'??40%6^RX&C4l'0%&QRX64H%5:H/"%HM5:a&57;$"C45:H%RX0%&1"/&QH/"/&1?GHot6,0&3,R2$)3+?4?G8:5:R13+"%576@,6,02C'&10%&Caiz~n?40%&RX&C'&4RX&RX64H/"/023,5:"%H JL$'&KC4l'023+"%576)6,o3H/"/&1?NC'&1?&4C4H`6"%$'&"F~z?&6,oH/"/&1?N3,HLEff&8:83,HL"%$'&K3+?4?G8:5:R13+"%576S$'&#H/&1,&1023,8M3+?4?G8:5:R13+"%5764HB3+0%&i&5:';ZH/&1"Kl'?@H/"/&1?GHB?&10%"%3,5:45:';"/6=C45&10%&"B3+?4?G8:5:R13+"%5764HBR13,i&L&X^&R1l'"/&C5:?G3+023,8:87&8@H%l'i^/&RX"M"/6K0%&H/6l'02RX&Q3+3,5:8:3+iG5:8:57"F~ IA"%$'&10%&Y3+0%&Kwzl4>&1025:R`0%&H/6l'02RX&H1@^3,4C&3,R2$DH/"/&1?#l4H/&H3ZRX&10%"%3,5:3,>6l4"1@EQ$45:R2$C'&1?&4C4HY6."%$'&a3+?4?G8:5:R13+"%576@6,o`H/6>&aH%l'iGH/&1"P6,off0%&XH/6l'02RX&HMC4l'025:';Q&X^&R1l'"%576=qt6487~nP6,oG"%$'&wP0%&H/6l'02RX&HM3+0%&L3,RX"%l43,8:87~K6,&102H%l'iGH%RX0257i&C | _L&H/6l'02RX&H3+0%&=!0%&l4H%3+iG87&^@c}c:@'"%$'&K3,>6l4"Ql4H/&C)i&RX6>&HQ3+3,5:8:3+iG87&3+;3,5:Z64RX&"%$'&KH/"/&1?=$43,H` 45:H%$'&C"LG02H/"L;8:3,4RX&,@G"%$45:HL3+?4?&3+02HL"/6i&3a?&10%ot&RX"QC'6>a3,5:Zot6,0 } - IffJL$'&?40%&H/&4RX&K6,o 0%&l4H%3+iG87&0%&H/6l'02RX&#8:5:>a57"%3+"%5764H>a3+g,&H57"Z>6,0%&8:57g,&87~W"%$43+"Z"%$'&10%&3+0%&#$457;$'&10/6,02C'&10Z>]l'"%l43,8P&X'R18:l4H%5764Hi&1"FEff&1&T3,RX"%5764Hqhc}c:@M"%$'&10%&Z>a3~i&&'6l';$6,oY3N0%&H/6l'02RX&)"/6#R13+0%0%~#6l'"n"FEff6#3,RX"%5764Hl4H%5:';"%$'&0%&H/6l'02RX&RX64R1l'0%0%&"%87~,@ffiGl'"a'6,"a"%$'0%&1&,@ff6,0a"%$'0%&1&iGl'"a'6,"ot6l'0@1e| JL$45:HH%l';,;,&H/"%Ha"%$'&m'p $'&l'025:H/"%5:R1HL3+0%&]>6,0%&n8:57g,&87~"/6)5:>?40%6,&KEQ57"%$=5:4RX0%&3,H%5:';s@H%5:4RX&]m'p RX64H%5:C'&102HQ3+"P>6H/"QsH%l'i4;,63,8:Hj3,4CZ"%$'&10%&1ot6,0%&3+"Q>6H/"LsRX64R1l'0%0%&"Q3,RX"%5764H "L"%$'&H%3,>&B"%5:>&,@4C4l'&B"/6a"%$'&H%5:>?G87&H/"/02l4RX"%l'0%&j6,o?40%&RX&C'&4RX&YRX64H/"/023,5:"%H1@"%$'&!;,0%6EQ5:';H/"%3+"/&Hff3,4Ca"%$'&L0%&H%l487"%5:';Pi4023,4R2$45:';Po}3,RX"/6,0iG876EjFl'?5:)0%&8:3^&CH/&3+02R2$"%$43+"L6^R1R1l'0%0%&C5:)"%$'{& Q%C'6>a3,5:@'3+0%&Kl448:57g,&87~_L&H%l487"%H1@$'6Eff&1,&10@C45:H%3+;,0%&1&,IQJL9M-)3,4C } - H/687,&]"%$'&nH%3,>&nH/&1"P6,oM?40%6,iG87&>5:4H/"%3,4RX&Hq,6l'"]6,oP+*|2@5:D"%5:>&Hn3,HnH%$'6EQ5: 57;l'0%&.w,wq}3| JL9M-.5:HKo}3,H/"/&10"%$43, } - 5:3.>a3/6,0257"F~#6,oR13,H/&H1@G"%$'6l';$"%$'&]C45&10%&4RX&5:Hj0%&8:3+"%57,&87~ZH%>a3,8:8@'EQ$45:87&B5:Z"%$'&KR13,H/&HQEQ$'&10%& } - 5:HL"%$'&o}3,H/"/&H/"6,o"%$'&B"FEff6'@4"%$'&C45&10%&4RX&B5:Hff;,0%&3+"/&10 JA6aH/&1&BEQ$~,@'"%$'&B?40%6,iG87&>5:4H/"%3,4RX&HjR13,Zi&BC457^5:C'&C5:"/6"%$'&ot68:876EQ5:';]"%$'0%&1&"F~z?&H1I~5}fi (^z7G4 % : 4^-G-Ns4-G^1 fIT!4H/"%3,4RX&H5:]EQ$45:R2$3,8:8^RX6"/&H/"/&Ca0%&H/6l'02RX&HM3+0%&j3+3,5:8:3+iG87&j5:nH%l>=R157&"zl43,"%57"F~ JL$45:HwYpQbn> &3,4Hj"%$'&10%&5:Hj'60%&H/6l'02RX&RX6> 5:RX"j3+"Q3,8:8@'3,4CZ"%$zl4H`"%$43+"Ym uUm JL$'&10%&B3+0%&w5n5:4H/"%3,4RX&Hj6,o"%$45:H`"F~z?&K3,>6';a"%$'6H/&KH/687,&C@43,4CZ"%$'&1~)3+0%&K5:4C45:R13+"/&C)iz~[5: 57;l'0%&aw,wu $ iGl'"m ; $ @Mc}c:@"%$'&10%&3+0%&0%&H/6l'02RX&aRX6> 5:RX"%H1@iGl'""%$'&H/&N5:,687,&N>6,0%&="%$43,k"%$'0%&1&NRX64R1l'0%0%&"3,RX"%5764H3,4Ck3+0%&N"%$'&10%&1ot6,0%&N'6,"C'&1"/&RX"/&CViz~km .JL$'&10%&]3+0%&)w1*)5:4H/"%3,4RX&HQ6,oM"%$45:HL"F~z?&n3,>6';"%$'6H/&H/687,&C@3,4C="%$'&1~=3+0%&n5:4C45:R13+"/&Ciz~k 5:57;l'0%&aw,w. ;m $wYpQb8AA I!4H/"%3,4RX&HB5:=EQ$45:R2$.mJL$'&10%&n3+0%&Zw,wn5:4H/"%3,4RX&HY6,oM"%$45:HY"F~z?&n3,>6';)"%$'6H/&H/687,&C@43,4CZ"%$'&1~Z3+0%&K5:4C45:R13+"/&C)iz~[; a5: 57;l'0%&aw,wPB"F~z?&5:4H/"%3,4RX&H1@ } - R187&3+0287~Q?G3~^HA3,B6,&102$'&3,CKot6,0ARX6>?Gl'"%5:';L3,l44'&RX&H%H%3+025:87~YH/"/0%6';$'&l'025:H/"%5:R+@^"%$'6l';$Z57"L5:H`0%&8:3+"%57,&87~)H%>a3,8:8 6,"/&K"%$43+"j"%$'&H/&K3,R1RX6l4"Qot6,0Q3n"%$45702C)6,o"%$'&5:4H/"%3,4RX&H5:D"%$'&?40%6,iG87&>H/&1"Zq!5w =6l'"n6,oY+*|2@3,4C'&3+0287~#$43,87oj6,oL"%$'&)H/687,&C[5:4H/"%3,4RX&H P5:4H/"%3,4RX&H6,o`"F~z?&HB%Y3,4C%%@ } - &X^?G3,4C4HBot&1Eff&10K'6^C'&H5:."%$'&n 43,8Lq}'6^0%&8:3^&C |PH/&3+02R2$#"%$43,#JL9MC'6z&HnC4l'025:';57"%H]H/&3+02R2$@3,Hn>]l4R2$[3,H- ot&1Eff&10n6[3,&1023+;,& JL$45:HKH%$'6EQHK"%$43+"n"%$'&)$'&l'025:H/"%5:R5:>?40%6,&>&"`0%&H%l487"%5:';ot0%6>0%&1;,0%&H%H%576)5:H`3+"j87&3,H/"ff6,oH/6>&P+3,8:l'&,@z"%$'6l';$)5:0%6l';$487~ . 02C6,o"%$'&K5:4H/"%3,4RX&Hj'6,"L&'6l';$Z"/6RX6>?&4H%3+"/&ot6,0L"%$'&KRX6H/"L6,o ?&10%ot6,02>a5:';]"%$'&0%&8:3^&CZH/&3+02R2$_L&R13,8:8 "%$43+"ff"%$'& } - ?G8:3,4'&10ff5:a"%$'&BRX6>?&1"%57"%576@^3,4C5:"%$'&Y&X^?&1025:>&"%H?40%&H/&"/&C)$'&10%&H/6.o}3+0@ME`3,Hn0%&H/"/025:RX"/&C"/6?&10%ot6,02>a5:';6487~0%&1;,0%&H%H%576TH/&3+02R2$ OD ?6H%H%57iG87&a&X^?G8:3,43+"%576ot6,0"%$'&0%&8:3+"%57,&87~.Eff&3+g#0%&H%l487"%H"%$'&?G8:3,4'&10?40%6^C4l4RX&H6#"F~z?&%B3,4CD%%P5:4H/"%3,4RX&H]5:H"%$43+"K"%$45:H0%&H/"/025:RX"%576#?40%&1,&"%HK0%&8:3^&CH/&3+02R2$ot0%6>i&5:';o}l48:87~=&X^?G87657"/&C Q6Eff&1,&10@M"%$45:H"%$'&16,0%~#C'6z&H'6,"M$'68:C 57;l'0%&HLw,wqti|3,4CZw,wq}R|H%$'6EW0%&H%l487"%Hot6,0M"FEff6K3,87"/&10243+"%57,&LRX6^G;l'023+"%5764H 6,o } - 5:w,wqti|Q3,V/l448:5:>a57"/&C4RX6^G;l'023+"%576@ EQ$45:R2$=R13+0%0257&HQ6l'"Ys)0%&1;,0%&H%H%576NH/&3+02R2$'&HYot6,0Psuzy1{1{1{l4"%5:8&57"%$'&103P'6^0%&8:3^&CnH/68:l'"%576]5:Hot6l44C@+6,0 "%$'&`6,?4"%5:>a3,8zs)FH/68:l'"%576]RX6H/"5:Hot6l44CK"/6Bi&ff"%$'&H%3,>&j3,H "%$'&BqtGw|FH/68:l'"%576RX6H/"1@3,4Cn5:w,wq}R|3)%-RX6^G;l'023+"%576@+EQ$45:R2$n3,87E`3~^H ?&10%ot6,02>aH`3,4CZ-+0%&1;,0%&H%H%576=H/&3+02R2$'&H O` HQR13,i&KH/&1&=5:)"%$'&G;l'0%&H1@'i6,"%$3,87"/&10243+"%57,&]RX6^G;l'023+"%5764H5:4R1l'03D8:3+0%;,&106,&102$'&3,CVot6,0"%$'&=0%&8:3^&CVH/&3+02R2$'&H.q}l448:5:>a57"/&C } - &1,&k"%5:>&Ha6l'"6k"FEff65:4H/"%3,4RX&HEQ$45:87&C'65:';[0%&1;,0%&H%H%576VH/&3+02R2$ | Ox 8:H/6'@j"%$'&=;3,5:kot0%6>"%$'&.3,C4C457"%57643,8L$'&l'025:H/"%5:R5:'ot6,02>a3+"%576T5:Hzl457"/&ZH%>a3,8:8IZRX6>?G3+025:';#3+;3,5:T"%$'&=zl4>Ki&106,oP'6^C'&Hn&X^?G3,4C'&CW5:["%$'&) 43,8q}'6^0%&8:3^&C |P0%&1;,0%&H%H%576H/&3+02R2$"/6N"%$'&)zl4>Ki&10K6,oL'6^C'&HK&X^?G3,4C'&C#iz~#JL9M-'@"%$'&l448:5:>a57"/&C3,4Ck-[RX6^G;l'023+"%5764H&X^?G3,4CT6k3,&1023+;,&.-4w+{ - 3,4CW-4uw ot&1Eff&10'6^C'&H1@ff0%&H/?&RX"%57,&87~qt"/6i&KRX6>?G3+0%&C="/6"%$'&n- 3,&1023+;,&aH%3^5:';5:&X^?G3,4C'&C='6^C'&HL6,i4"%3,5:'&C=iz~ } - 0%&H/"/025:RX"/&C"/60%&1;,0%&H%H%576H/&3+02R2$6487~4|'6,"%$'&10Q?6H%H%57iG87&P&X^?G8:3,43+"%5765:Hj"%$43+"Q"%$'&K"/023,4H/?6H%57"%576)"%3+iG87&,@4EQ$45:R2$3,8:H/6H/"/6,0%&HYl'?C43+"/&H6,oB&H/"%5:>a3+"/&CVRX6H/"1@j"%$'6l';$kot6,0EQ$'687&=H/"%3+"/&H023+"%$'&10"%$43,kH%l'iGH/&1"%Ha6,oB;,63,8:H1@j"/6[H/6>&=&X^"/&"RX6>?&4H%3+"/&HQot6,0Q"%$'&Eff&3+g,&10P$'&l'025:H/"%5:RBl4H/&CZiz~ZJL9M- O` ;3,5:@ $'6Eff&1,&10@ "%$'&&X^?G8:3,43+"%576Z"%l'024H6l'"'6,""/6[$'68:CINEQ57"%$W"%$'&="/023,4H/?6H%57"%576T"%3+iG87&=C45:H%3+iG87&CW5:Wi6,"%$W?G8:3,4'&102H1@ff"%$'&NH%3^5:';H5:zl4>Ki&106,oj'6^C'&HB&X^?G3,4C'&C#5:."%$'& 43,8MH/&3+02R2$#iz~ } - RX6>?G3+0%&C#"/6"%$'&zl4>Ki&106,oj'6^C'&H&X^?G3,4C'&CDiz~JL9M-N6"F~z?&%K3,4CD%%5:4H/"%3,4RX&Hn5:H]3,RX"%l43,8:87~D87&H%HK"%$43,EQ$'&4CD"/023,4H/?6H%57"%576"%3+iG87&HB3+0%&l4H/&C@3,&1023+;5:';Z6487~D5w qt"%$'6l';$ } - 5:N"%$45:HY&X^?&1025:>&"YH/687,&HP"FEff6?40%6,iG87&>aH"%$43+"QJL9M-o}3,5:8:H`"/6H/687,&|2@ 3,4C)"%$'&KC45&10%&4RX&3,8:H/6i&RX6>&HQ>]l4R2$>6,0%&+3+0257&C_L&R13,8:8"%$43+"Y3azl4>Ki&10j6,oH%5:>?G8:5 R13+"%5764HEff&10%&K5:"/0%6^C4l4RX&CZ5:Z"%$'&Bot6,02>]l48:3+"%576Z6,o "/&>?6,023,8s)0%&1;,0%&H%H%576@B5:V6,02C'&10"/6W&43+iG87&RX6>?G87&1"/&DH/68:l'"%5764H)"/6W"%$'&0%&8:3^&CRX6H/"&zl43+"%576U"/6Wi&RX6>?Gl'"/&CZ3,4C)H/"/6,0%&CZ5:"%$'&B$'&l'025:H/"%5:RL"%3+iG87& JL$zl4H1@^3]0%&>a3,5:45:';K?6H%H%57iG87&L&X^?G8:3,43+"%576ot6,0j"%$'&Ix!4H/"%3,4RX&H5:NEQ$45:R2$mwYpQb.~fi44H%>a3,8:8+3,8:l'&]6,oM"%$'&n5:>?40%6,&>&"Y6,oM"%$'&n$'&l'025:H/"%5:R5:="%$'&K 43,8AH/&3+02R2$5:HQ"%$'&H/&nH%5:>?G8:5 R13+"%5764H1@H%5:4RX&B"%$'&1~)876Eff&10Q"%$'&B&H/"%5:>a3+"/&HQH/"/6,0%&C=5:)"%$'&K$'&l'025:H/"%5:RQ"%3+iG87&<ff64RX&10245:';`"%$'&M"%5:>&6,&102$'&3,Cot6,00%&8:3^&CH/&3+02R2$@+5:P?G3+0%"%5:R1l48:3+0ot6,0"%$'&M$457;$'&10s)0%&1;,0%&H%H%576H/&3+02R2$'&H1@"%$'&ff&X^?G8:3,43+"%576]3+?4?&3+02H3+;3,5:n"/6Yi&`3Y$457;$'&10i4023,4R2$45:';Qo}3,RX"/6,0M5:"%$'&`0%&8:3^&C]H/&3+02R2$H/?G3,RX&,@"%$'6l';$="%$'&H%57"%l43+"%576N5:HYH/6>&1EQ$43+"PC45&10%&"Y"%$43,.5:"%$'& Q%ZC'6>a3,5: JL$'&]023+"%576i&1"FEff&1&Z"%$'&H%571&B6,o H%l4R1RX&H%H/6,0QH/"%3+"/&HQ3,4CZ"%$'&570`?40%&C'&RX&H%H/6,02HL5:Hj876E@43,&1023+;5:';*^{ n5:Z'6,02>a3,80%&1;,0%&H%H%576#3,4C#*^{ ^waot6,0P_jF'6^C'&H5:#0%&1;,0%&H%H%576Vq}3,4CDH/"%3~^H0%6l';$487~N"%$'&H%3,>&3,8:H/65:"%$'&-+ff3,4C0%&1;,0%&H%H%576H/?G3,RX&H|`H/6 KnY F'6^C'&HQ3+0%&B0%&8:3+"%57,&87~)H%R13+02RX& `l'"j"%$'&K3,&1023+;,&Ki4023,4R2$45:';o}3,RX"/6,0ot6,0`P_jF'6^C'&HM5:a0%&1;,0%&H%H%576a5:H(z{ -q}5:4RX0%&3,H%5:';P"/6-'{ 9 3,4Coz{ ( B5:n-+3,4C0%&1;,0%&H%H%576@0%&H/?&RX"%57,&87~4|ffRX6>?G3+0%&C"/63,3,&1023+;,&]6,ojw+{ ,a5:Z'6,02>a3,80%&1;,0%&H%H%576 JL$'&B0%&3,H/65:)"%$'& 3SC'6>a3,5:k5:Ha"%$'&N0257;$"!FH%$457ot"R1l'"%H1I.0%&R13,8:8Q"%$43+""%$'&H/&=&8:5:>a5:43+"/&0%&C4l44C43,"ai4023,4R2$'&Hot0%6>"%$'&H/&3+02R2$UH/?G3,RX&,@L"%$zl4H0%&C4l4R15:';#"%$'&Ni4023,4R2$45:';Do}3,RX"/6,0@QiGl'"R13,S'6,"i&.l4H/&CkEQ$'&k0%&1;,0%&H%H%5:';P_jF'6^C'&Hj5:0%&8:3^&CZH/&3+02R2$=H%5:4RX&P"%$45:Hj>a57;$"jR13,l4H/&B"%$'&RX6>?Gl'"/&C$'&l'025:H/"%5:RL"/6ai&RX6>&5:43,C^>a5:H%H%57iG87& JL$'&Qi4023,4R2$45:';Ko}3,RX"/6,0jot6,0j'6,02>a3,8G0%&1;,0%&H%H%576H/&3+02R2$)EQ57"%$'6l'"0257;$"!FH%$457ot"R1l'"%H`5:H`(z{ ,JL$'&C45&10%&4RX&>a3~ZH/&1&>H%>a3,8:8@^iGl'"L57"L$43,HL3;,0%&3+"Q&X&RX"1IffJL9M-EQ57"%$'6l'"j0257;$"!FH%$457ot"jR1l'"%Hjo}3,5:8:H"/6]H/687,&Q3,8:84iGl'""FEff6]"F~z?&L% 3,4Ca%%5:4H/"%3,4RX&HYq}5:a>a3,~aR13,H/&Hff'6,"M&1,&a 45:H%$45:';Y"%$'&jG02H/" P \57"/&1023+"%576 |"!>Oilkgn-opQqsrqun-kg|Itk]o#gqsrtkgq!]H/&1,&1023,86,o4"%$'&`RX6>?&1"%57"%576KC'6>a3,5:4H1@ } - C'6z&H3,R2$457&1,&`i&1"/"/&100%&H%l487"%HA"%$43,]JL9M-'@,5:4C45:R13+"%5:';"%$43+"M0%&8:3^&CaH/&3+02R2$R13,ai&L3,& =R157&">&1"%$'6^C6,oRX6>?Gl'"%5:';3>6,0%&L3,R1R1l'023+"/&Y$'&l'025:H/"%5:RffEQ$45:87&H/"%3~^5:';k5:S"%$'&Dm'pot023,>&1Eff6,0%g !U"%$'&#C'6>a3,5:4H)EQ$'&10%&#57"Zo}3,5:8:Hk"%$'& Q%k3,4C3SC'6>a3,5:4H57"LC'6z&HLH/6i&R13,l4H/&B0%&8:3^&CH/&3+02R2$~^57&8:C4H`30%&8:3+"%57,&87~H%>a3,8:85:>?40%6,&>&"j6,&10Y"%$'&$ $'&l'025:H/"%5:R+@'3+"Q3aC45:H/?40%6,?6,0%"%57643+"/&87~a8:3+0%;,&KRX6>?Gl'"%3+"%57643,8RX6H/"!"%$'& Q%aC'6>a3,5:@z"%$'&P"FEff6?40%6,iG87&>aHff3+0%&P"%57;$"%87~RX64'&RX"/&CIM"%$'&B$'&l'025:H/"%5:RQ5:>?40%6,&X>&"Z5:H'&1;8:57;57iG87&=H%5:>?G87~i&R13,l4H/&.0%&8:3^&CVH/&3+02R2$U5:HH/6[&X^?&4H%57,&="%$43+")"%$'&N6487~W?40%6,iG87&>5:4H/"%3,4RX&H6EQ$45:R2$57"M 45:H%$'&HEQ57"%$45:"%$'&Q"%5:>&Y8:5:>a57"3+0%&Q"%$'6H/&Y,&10%~H%5:>?G87&L5:4H/"%3,4RX&Qot6,0ffEQ$45:R2$"%$'&)m $ $'&l'025:H/"%5:R5:HK3,870%&3,C'~R1876H/&"/6=?&10%ot&RX" !"%$'B& 3SZC'6>a3,5:@A"%$'&0%&3,H/6Dot6,0K"%$'&?6z6,0$'&l'025:H/"%5:RQ5:>?40%6,&>&"L5:H`H/"%5:8:8 H/6>&1EQ$43+"`6,o3>K~^H/"/&10%~ IM3zl4>Ki&10ff6,o $~z?6,"%$'&H/&HffEff&10%&B"/&H/"/&C@3,4C0%&1o}l'"/&C OD 0%&>a3,5:45:';=?G8:3,l4H%57iG87&&X^?G8:3,43+"%576[5:H]"%$43+""%$'&ZH%5:>?G8:5 R13+"%5764H5:"/0%6^C4l4RX&C5:ot6,02>]l48:3+"%576)6,o "/&>?6,023,8s)0%&1;,0%&H%H%5763+0%&?G3+0%"%5:R1l48:3+0287~C43,>a3+;5:';a5:)"%$45:HjC'6>a3,5:!Zi6,"%$C'6>a3,5:4H1@'$'6Eff&1,&10@G"%$'&&X^?G8:3,43+"%576Zot6,0L"%$'&0%&8:3+"%57,&87~)8:3+0%;,&B6,&102$'&3,C=ot6,0L0%&8:3^&CH/&3+02R2$]3+?4?&3+02H"/6Qi&"%$43+"57"AH%l^&102Hot0%6>r3L$457;$'&10i4023,4R2$45:';jo}3,RX"/6,0 "%$43,"%$'&'6,02>a3,80%&1;,0%&H%H%576H/&3+02R2$@ EQ$45:R2$DR13,l4H/&HK&X^?G3,4H%5766,oYP_jF'6^C'&HK5:"%$'&0%&8:3^&CDH/&3+02R2$"/6=i&RX6>?Gl'"%3+"%57643,8:87~>6,0%&n&X^?&4H%57,&K"%$43,'6^C'&]&X^?G3,4H%576N5:="%$'&'6,02>a3,8 H/&3+02R2$Wqt&1,&57o>a3,~6,o"%$'&n;,&'&1023+"/&CH%l4R1RX&H%H/6,02H3+0%&Q'6,"H/&3+02R2$'&C | OM 57;l'0%&w(BH%l4>a>a3+02571&HMH/6>&QH/&3+02R2$H/?G3,RX&QR2$43+023,RX"/&1025:H/"%5:R1Hot6,03,8:8"%$'&YRX6>?&1"%57"%576C'6>a3,5:4H !"%$'&YC'6>a3,5:4HMEQ$'&10%&Q0%&8:3^&CH/&3+02R2$)5:HM&X^?&4H%57,&,@z"%$45:H5:HMi&R13,l4H/&H/"%3+"/&Ha"/&4C["/6;,0%6EEQ$'&[0%&1;,0%&H%H/&C@Qc}c:@ 85F 8>5:Hn8:3+0%;,&3,4CT3,Ha3.0%&H%l487"]"%$'&10%&3+0%&>a3,~F'6^'C&HQE75%"]$>,3~]%H4l1RXR&%H/H,620LHqQ%KH/?GnY3,RX &P$43,&B3K$457;$'&10ffi4023,4R2$45:';o}3,RX"/6,0`"%$43,54:a"|2@+%6,$'0 &Pi'6&,R1023,>al4H/3,&Q84P0%&1_j;,0%F&'6H%^H%C'576&HH5:/]"?G3,%$'RX&,&ff@'0%3,&4C8:3^&3+Cn0%&Q0%&1"%$';,0%&1&0%H%&1H%ot6,5760%&RX6>?Gl'"%3+"%57643,8:87~.>6,0%&&X^?&4H%57,&a"/6=&X^?G3,4CVTq 3S| !#"%$'&C'6>a3,5:4HBEQ$'&10%&a0%&8:3^&CDH/&3+02R2$5:HYH%l4R1RX&H%H/o}l48@G6N"%$'&]6,"%$'&10B$43,4CH@ 85F* 8>G5:HL"F~z?G5:R13,8:87~R1876H/&]"/6w+@ c}c:@H%>a3,8:8AH/"%3+"/&HH/"%3~=H%>a3,8:8EQ$'&n0%&1;,0%&H%H/&C@3,4Cn0%&1;,0%&H%H%576n6,oP_jF'6^C'&H5:]"%$'&j0%&8:3^&CH/?G3,RX&Q5:H RX6>?Gl'"%3+"%57643,8:87~]R2$'&3+?&10"%$43,'6^C'&a&X^?G3,4H%576D5:"%$'&'6,02>a3,8M0%&1;,0%&H%H%576H/?G3,RX&,@3,H]5:4C45:R13+"/&C#iz~3N876Eff&10Zqt6,0]0%6l';$487~~5fi (^z7G4 % : 4^-G-Ns4-G^1 fn8>85F* 8>i4023,4R2$45:';no}3,RX"/6,08>85F* 8>6,02>a3,8_L&1;,0%&H%H%576Q%9z {w+{ *w+{S~uz{ +i4023,4R2$45:';no}3,RX"/6,08>85F* 8>8>85F* 8>i4023,4R2$45:';no}3,RX"/6,08>85F* 8>i4023,4R2$45:';no}3,RX"/6,08>85F* 8>z { *(z{ +w+{ **^{w+{ (^wwz{:w(z{ ,w+{ ,z{:wz{ (wX-'{w+{:~w(^w+{ ((z{ ,(z{:~wz{ +*+*^{z{ *w+{ *(-'{(z{ ,w+{ -(wz{:w{ +(z{ ,w+{ *z{ *'w{ ,(z{ ,(*^{ ^w(z{ -*z{ ,Qh3qTS==,S|i4023,4R2$45:';no}3,RX"/6,0i4023,4R2$45:';no}3,RX"/6,0F_L&1;,0%&H%H%576P_K nY9QhhQz{ 9w+{ *+z{ 93Sz{:w1**^{w+{ ,57;l'0%&aw(zIP\z6>&nR2$43+023,RX"/&1025:H/"%5:R1HY6,oM"%$'&]'6,02>a3,8A0%&1;,0%&H%H%576N3,4C.0%&1;,0%&H%H%576NH/&3+02R2$.H/?G3,RX&HP5:"%$'&ffC'6>a3,5:4HRX64H%5:C'&10%&CI"%$'&ff3,&1023+;,&`H/"%3+"/&`H%571&Yq 8> |2@"%$'&ff3,&1023+;,&ff023+"%576L6,o4H%l4R1RX&H%H/6,0H/"%3+"/&KH%571&P"/6"%$'&H%571&P6,oA"%$'&B?40%&C'&RX&H%H/6,0LH/"%3+"/&q 85F* 8> |`3,4C"%$'&Bi4023,4R2$45:';]o}3,RX"/6,0& S~uG5@ Qh34~@ ]3,4Cp9QhhQPC'6>a3,5:4H1@,"%$'&jzl4>Ki&102HH%$'6EQ6,0"%$'c3+0%&"%$'&3,&1023+;,&H6,&10 H/687,&CB?40%6,iG87&>5:4H/"%3,4RX&H 6,0A"%$'& 3S`C'6>a3,5:@"%$'&3,&1023+;,&5:HL6,&10BH/687,&C"F~z?&K%`3,4C%%`5:4H/"%3,4RX&HL6487~#q}H/&1&\z&RX"%576=- | OL 6,0Y"%$'[& Q%C'6>a3,5:@LC43+"%3D5:Hot0%6>3H%5:';87&Dqto}3,5:87&C |57"/&1023+"%576k6k3H%5:';87&?40%6,iG87&> 5:4H/"%3,4RX&Tq ff^|~5fi44&zl43,8t|i4023,4R2$45:';No}3,RX"/6,0 YoQRX6l'02H/&,@"%$'&H/&Dq}3,&1023+;,&C |nzl4>Ki&102Hn3+0%&'6,"n?&10%ot&RX"n?40%&C45:RX"/6,02H6,oj?&10%ot6,02>a3,4RX&,I5:#"%$'B& Qh3=C'6>a3,5:@ot6,0]&X'3,>?G87&,@ "%$'&z 85F* 8>023+"%576.5:HKzl457"/&8:3+0%;,&iGl'"6l'"/?&10%ot6,02>aHjJL9M-a3,~zE`3~Dq}3,HQH%$'6EQ5: 57;l'0%{& 'q}3|/|} -h"ff5:H5:4H/"/02l4RX"%57,&L"/6n876z6,ga>6,0%&YR1876H/&87~a3+""%$'&PH/"%3+"/&H`5:a"%$'& Q%nC'6>a3,5:@z3,4CaEQ$~"%$'&1~;,0%6EEQ$'&[0%&1;,0%&H%H/&C OW 6,0a&X'3,>?G87&,@ff"%$'&W/H/"%3+"/&#6,oP&3,R2$k3,5702RX023+ot"a5:Hq}5:[&3,R2$WEff6,028:CTH/"%3+"/&|C'&H%RX0257i&Ciz~"%$'0%&1&RX6>?6'&"%H1IM6'&B"/&8:8:H`EQ$'&10%&P"%$'&3,5702RX023+ot"L5:H`?6H%57"%576'&C)5:"%$'&'&1"FEff6,0%g)6,o3,570%?6,0%"02l4E`3~^HM3,4Cn"%3'57E`3~^H1@6'&jEQ$45:R2$nC4570%&RX"%57657"5:H o}3,R15:';'@3,4Cn6'&jEQ$'&1"%$'&10M57"5:H ?G3+0%g,&C@i&5:';)?Gl4H%$'&C.6,0K>6^5:';l44C'&1057"%HB6EQ?6Eff&101* Oa 8:>6H/"&1,&10%~.6,?&1023+"/6,0"%$43+"KR2$43,';,&HK6'&6,o"%$'&H/&n$43,HY3,=&X&RX"P6,0P?40%&RX64C457"%576Z6="%$'&]6,"%$'&10Y"FEff6Z3,HQEff&8:8 OQ 6,0P&X'3,>?G87&,@3,~5:4H/"%3,4RX&6,o"%$'f& Qa6,?&1023+"/6,0@EQ$45:R2$R2$43,';,&HY"%$'&]?6H%57"%576Z6,oM3,N3,5702RX023+ot"1@ 0%&zl4570%&Hj"%$'&n3,5702RX023+ot"Q"/6i&>6^5:';3,4Co}3,R15:';n3]?G3+0%"%5:R1l48:3+0`C4570%&RX"%576@^3,4CZ>a3~3,8:H/6R2$43,';,&P"%$'&Po}3,R15:'; JL$zl4H1@^0%&1;,0%&H%H%5:';3PH/"%3+"/&LRX6"%3,5:45:';P6487~3P;,63,8'3+"/6> i&876';5:';Q"/6P6'&`6,oG"%$'&jRX6>?6'&"%H5:]>6H/"R13,H/&H0%&H%l487"%H5:Z3H/"%3+"/&KRX6"%3,5:45:';n;,63,8A3+"/6>aHji&876';5:';]"/63,8:8"%$'0%&1& OM RX64R18:l4H%576)6'&>a3~)C'023E5:H`"%$43+"H/?G8:57"/"%5:';Q8:3+0%;,&`H/"%3+"/&HLq KnY F'6^C'&H|A5:"/6PH%>a3,8:87&10H/"%3+"/&HLqP_jF'6^C'&H|iG3,H/&CK6487~B6]"%$'&`zl4>Ki&106D3+"/6>aH]5:H'6,"]3,87E`3~^HK"%$'&a0257;$"KR2$'65:RX& OZ D3,87"/&10243+"%57,&aEff6l48:Ci&a"/6NC457^5:C'&3+"/6>aH]5:"%$'&?G8:3,445:';L?40%6,iG87&>x5:"/6P;,0%6l'?GH6,off!0%&8:3+"/&C4B3+"/6>aHM3,4C]"%3+g,&L"%$'&jzl4>Ki&10 6,oG;,0%6l'?GH0%&1?40%&H/&"/&C5:Z3aH/"%3+"/&K"/6ai&B57"%HLH%571&'6,"%$'&10Y6,iGH/&10%+3+"%576"%$43+"YR13,=i&K>a3,C'&]5:HL"%$43+"Q"%$'&KiGl487g6,o"%5:>&]H/?&"Q5:Z0%&8:3^&C=H/&3+02R2$5:HLH/?&"L5:)"%$'&B 43,8H/&3+02R2$N57"/&1023+"%576@'EQ$'&3aH/68:l'"%576)&X'5:H/"%HLEQ57"%$45:"%$'&KR1l'0%0%&"LRX6H/"Qi6l44CJL$45:Ha57"/&1023+"%576k5:H'6,"6487~"%$'&N>6H/"&X^?&4H%57,&,@`iGl'"3,8:H/6D"%$'&N87&3,H/"l4H/&1o}l48@`H%5:4RX&0%&8:3+"%57,&87~ot&1E $'&l'025:H/"%5:R5:>?40%6,&>&"%HP3+0%&nC45:H%RX6,&10%&C5:=57" JL$45:HQ3,8:H/60%&8:3+"/&HY"/6)"%$'&]i4023,4R2$45:';o}3,RX"/6,0@H/?&R15 R13,8:87~."%$'&o}3,RX"n"%$43+" KnY F'6^C'&Hn$43,&>a3,~D>6,0%&)H%l4R1RX&H%H/6,02H]"%$43,TP_jF'6^C'&H1Iot6,03,F'6^'Cn&/"Z6n&/H678,&.C,3:88 57"%HPH%l4R1RX&H%H/6,02HB>]l4H/"Pi&nH/687,&C@H/6Z5:N"%$'&] 43,857"/&1023+"%576@3,8:8 H%l4RKRX&nYH%H/ 6,02HQ6,oM&1,&10%~F'6^'C]&3+0%&]H/&3+02R2$'&C Q6Eff&1,&10@"%$'&K?Gl'0%?6H/&B6,o0%&8:3^&CNH/&3+02R2$.5:HQ'6,"Q"/6nYK4C3H/68:l'"%5765:Z"%$'&Ks)0%&1;,0%&H%H%576=H/?G3,RX&,@GiGl'"L"/6a 4CH%571&sH/"%3+"/&HnqP_jF'6^C'&H|`EQ$'6H/&KRX6H/"5:HPl44C'&10%&H/"%5:>a3+"/&C=iz~"%$'&$'&l'025:H/"%5:R+@ 3,4C.>6,0%&3,R1R1l'023+"/&aRX6H/"P&H/"%5:>a3+"/&HPot6,0B"%$'&H/& JL$'&10%&1ot6,0%&57"`>a3~)'6,"j3,RX"%l43,8:87~ai&P'&RX&H%H%3+0%~"/6H/&3+02R2$3,8:8G"%$'&BH%l4R1RX&H%H/6,02H`"/6&1,&10%~ KnY F'6^C'& OM Z3,87"/&10/43+"%57,&YEff6l48:Cai&Y3)qtsy 4|0%&1;,0%&H%H%576)H/&3+02R2$)5:aEQ$45:R2$a6487~"%$'6& >6H/"n!?40%6>a5:H%5:';H%l4R1RX&H%H/6,02H"/6Z&1,&10%~ KF'6^'C&+3%0&XR64H%:5'C1&%0&C@^~75&:84C:5';,3'6,%"'$1&04C:5>&4H%576ot6,057"/&1023+"%57,&87~=0%&X 45:';"%$'&nY$'&l'025:H/"%5:Rnqt"%$45:HL5:HjC45:H%R1l4H%H/&C)o}l'0%"%$'&10L5:)"%$'&K'&X^"LH/&RX"%576 |5:43,8:87~,@j57"H%$'6l48:CTi&?65:"/&Ck6l'""%$43+"&1,&V"%$'6l';$ } - 5:H1@j6V3,&1023+;,&,@Yi&1"/"/&10"%$43,JL9M-'@ 0%&H%l487"%HB6,oji6,"%$D?G8:3,4'&102HB6#"%$'&)RX6>?&1"%57"%576DC'6>a3,5:4HH/"%5:8:8M3+?4?&3+0K023+"%$'&10]?6z6,0 JL$'&6487~Z6,"%$'&10P6,?4"%5:>a3,8"/&>?6,023,8A?G8:3,4'&10Q"/6?G3+0%"%5:R157?G3+"/&]5:"%$'&nRX6>?&1"%57"%576=E`3,H<`9JqtP5:C43,8Y&X'&10@^(+*,*+-z|2@'EQ$45:R2$@5:>6H/"ffC'6>a3,5:4H1@3,R2$457&1,&C>]l4R2$ai&1"/"/&100%&H%l487"%H"%$43,JL9M-qt"%$'&L0%&H%l487"%H6"%$'&a&X^"/&4C'&wC 9QhhQ)?40%6,iG87&>H/&1"1@ 3,4C#3,#5:'ot6,02>a3,8 RX6>?G3+025:H/6i&1"FEff&1&DC43+"%3Zot0%6>"%$'&KRX6>?&1"%57"%576Z3,4CZ"%$'&0%&H%l487"%H`?40%&H/&"/&C=$'&10%&,@G5:4C45:R13+"/&P"%$43+"Q57"j6l'"/?&10%ot6,02>aH } - 3,HjEff&8:8t|!["%$'&'6^"/&>?6,023,8 Qh3.3,4C. C'6>a3,5:4H1@M6,"%$'&10a6,?4"%5:>a3,8`?G8:3,4'&102H]3,8:H/66l'"/?&10%ot6,02>q}RX6>?&1"%57"%576a0%&H%l487"%HM3+0%&Q?40%&H/&"/&Caiz~aQ6+>a3,43,4C + C'&87g+3,>?h@ | <`9JW3,8:H/6]l4H/&HM"%$'&} -"/&>?6,023,8m $ $'&l'025:H/"%5:R+@'iGl'"YH/&3+02R2$'&HP3)<L\^9kot6,02>]l48:3+"%576Z6,o?G3+0%"%5:3,86,02C'&10Y?G8:3,445:';'I$'&l'025:H/"%5:R&H/"%5:>a3+"/&HY3,4CZ"%$'&KR1l'0%0%&"QRX6H/"Li6l44CZ3+0%&Kot6,02>]l48:3+"/&C3,HQRX64H/"/023,5:"%H1@G3,4C=RX6H/"Li6l44C)^5768:3"%5764Hj3+0%&B5:'ot&10%0%&Ciz~RX64H/"/023,5:"j?40%6,?G3+;3+"%576@'3,65:C45:';n"%$'&B'&1&C)"/6&X^?G8:5:R157"%87~&1+3,8:l43+"/&H/"%3+"/&H3,4C&43+iG8:5:';a&3+028:57&10YC'&1"/&RX"%576 JL$'&?G3+0%"%5:3,86,02C'&10Yi4023,4R2$45:';a3,4C="%$'&]l4H/&K6,o & =R157&"Q?40%6,?G3!!Yt!j,!1%h2z, jA,ht`+%h2h j, Q LAhL21ff/2F+2 2 `}F%9 ks X % 21!4!1 L21`}F%h2h ` V v;lvv~5f;lvAW 2A/2F v~5f 2+Q`n /2 !QM2 hF% A2 ,2M ,!! 25}fi (^z7G4 % : 4^-G-Ns4-G^1 f;3+"%5763+0%&Qi6,"%$5:>?6,0%"%3,"Mot6,0"%$'&Q& =R157&4RX~6,o"%$'&B<`9JW?G8:3,4'&10@,iGl'"57"Eff6l48:C?40%6,iG3+iG87~n3,8:H/6i&'&XG"Bot0%6>3Z>6,0%&3,R1R1l'023+"/&$'&l'025:H/"%5:R JL$zl4H1@"%$'&H/&3+02R2$DH%R2$'&>&a6,oQ<`9J 3,4C"%$'&5:C'&3)6,o5:>?40%6^5:';$'&l'025:H/"%5:R1Hff"%$'0%6l';$H/&3+02R2$=3+0%&RX6>?G87&>&"%3+0%~,@ 3,4CZ>a3~)i&B?6H%H%57iG87&Y"/6RX6>KiG5:'&F,FG ( FFJL$'&a5:C'&3)6,o`l4H%5:';ZH/&3+02R2$#"/6=C'&10257,&6,0K5:>?40%6,&a$'&l'025:H/"%5:R1HB5:HB'6,"K'&1E JL$45:HPH/&RX"%576#0%&1^57&1EQH3=H/&87&RX"%5766,oj0%&8:3+"/&CD>&1"%$'6^C4H S57"%$#"%$'&a&X'RX&1?4"%576#6,oj"%$'&C45:H%R1l4H%H%576N6,oj?G3+"/"/&102DC43+"%3+iG3,H/&$'&l'025:H/"%5:R1H1@A"%$'&aot6^R1l4H]5:"%$45:HKH/&RX"%5765:HB6$'6E "%$'&H/&.qt6,0nH%5:>a5:8:3+0X|Y>&1"%$'6^C4HKR13,Di&3,C43+?4"/&C3,4CZ&X^?G87657"/&C)"/65:>?40%6,&0%&8:3^&CZH/&3+02R2$O#FnfiDnq>|mgfirq rq1& 0257^5:';$'&l'025:H/"%5:R1HBiz~H/687^5:';3,3+iGH/"/023,RX"/&C@ 6,0]0%&8:3^&C@A,&102H%576#6,oj"%$'&H/&3+02R2$D?40%6,iG87&>5:H'6,"Q3'&1E5:C'&3^@G3,4C='&57"%$'&10Q5:Hj"%$'&]5:C'&36,ol4H%5:';aH/&3+02R2$="/6H/687,&C"%$'&K3+iGH/"/023,RX"Q?40%6,iG87&>q}H/&1&c cP3,H%R2$4457;'@w +z9&3+028@w9 -'9M0257&C457"%5:H1@w,,|0%&RX&"1@'3,4CaH%l4R1RX&H%H/o}l48@+3+025:3,"M6a"%$45:H"%$'&>&Q5:H ',eeFXdN,e3 y%P' ^dfXe}XLq<`l487i&102H/6\^R2$43+&X&10@Lw,,zjQ&10243,C' 6 87;,~^5`Q687"/&,@ff(+*,*,*| JL$'&H/&3+0%&ZC'&X '&C#iz~D3+iGH/"/023,RX"%5:';.3E`3~?G3+0%"6,oG"%$'&`?40%6,iG87&>r3,4CnH/687^5:';Q6487~"%$'&`?G3+0%""%$43+" 0%&>a3,5:4H`qt"%$'&!?G3+"/"/&1024| JL$'&`3+iGH/"/023,RX"%5765:>?G8:5:R157"%87~ZC'&X '&HB3)?40%6/&RX"%576.ot0%6>v"%$'&?40%6,iG87&>vH/&3+02R2$#H/?G3,RX&a5:"/63ZH%>a3,8:87&10BH/&3+02R2$#H/?G3,RX&,I6,?4"%5:>a3,8H/68:l'"%576DRX6H/"n5:#"%$45:HK3+iGH/"/023,RX"nH/?G3,RX&)5:H]3=876Eff&10ni6l44C6D6,?4"%5:>a3,8H/68:l'"%576DRX6H/"n5:"%$'&B6,0257;5:43,8 H/&3+02R2$H/?G3,RX&,@43,4C)iz~>a3+g^5:';n"%$45:H`H/?G3,RX&H%>a3,8:8 &'6l';$)"%$'&B6,?4"%5:>a3,8RX6H/"L6,oA&1,&10%~H/"%3+"/&P5:a"%$'&Y3+iGH/"/023,RX"ff?40%6,iG87&>H/?G3,RX&PR13,ot6l44Caiz~iG8:5:4CH/&3+02R2$@^3,4CH/"/6,0%&C5:3"%3+iG87&YH/6K"%$43+"H/"%3+"/&K&1+3,8:l43+"%576R13,i&C'6'&Biz~)3aH%5:>?G87&P"%3+iG87&876z6,g^l'?Dq}$'&4RX&"%$'&K43,>&?G3+"/"/&102C43+"%3+iG3,H/&|Q&l'025:H/"%5:RB&H/"%5:>a3+"/&HQot0%6>>]l487"%57?G87&3+iGH/"/023,RX"%5764HYR13,=i&KRX6>KiG5:'&Ciz~)"%3+g^5:';"%$'&570Y>a3'5:>]l4>q}5:DH/6>&R13,H/&HK"%$'&570]H%l4>ZH/&1& &8:'&10O@ B6,0%oh@ Y3,43,@(+*,*+-z| 9 3+"/"/&102C43+"%3+iG3,H/&)$'&l'025:H/"%5:R1H$43,&i&1&UH%l4R1RX&H%H/o}l48:87~W3+?4?G8:57&Ck"/6W3Tzl4>Ki&106,onH/"%3,4C43+02CSH/&3+02R2$U?40%6,iG87&>aHq<`l487i&102H/6V\^R2$43+&X&10@Mw,,z &8:'&10P&1" 3,8 @(+*,*+-z|2@A3,4C.3,8:H/6"/6ZH/&zl'&"%5:3,8\^JL_L!9`\)?G8:3,445:';Nq + C'&87g+3,>?@(+*,*'w| JL$'&a5:C'&3)6,o`?G3+"/"/&102#C43+"%3+iG3,H/&H]>a3~3+?4?&3+0,&10%~H%5:>a5:8:3+0P"/60%&8:3^&CDH/&3+02R2$Vq}5:4C'&1&C@"/6"%$'&C'&X 457"%5766,o"%$'&Km'pr$'&l'025:H/"%5:R1H`5:;,&'&1023,8t|ff5:"%$43+"L"%$'&P?40%6,iG87&>5:H/H/?G8:57"%n5:"/6H%5:>?G87&10?40%6,iG87&>aHAEQ$45:R2$n3+0%&LH/687,&Cn5:4C'&1?&4C'&"%87~,@+3,4Cn"%$'&jH/68:l'"%576nRX6H/"%Hot6,0 "%$'&H/&Ll4H/&Cn3,H3B$'&l'025:H/"%5:R&H/"%5:>a3+"/&Not6,0"%$'&NRX6>?G87&1"/&=?40%6,iG87&> JL$'&10%&N5:H1@j$'6Eff&1,&10@Y3DRX02l4R15:3,8QC45&10%&4RX&,@j5:W"%$43+""%$'&m'p0%&8:3'3+"%576W?&10%ot6,02>aH"%$45:HH/?G8:57")d%%v ^d2 ,X +@L"/6D&1,&10%~WH/"%3+"/&N6,oH%571&=>6,0%&"%$43,ks@`EQ$45:87&"%$'&a3+iGH/"/023,RX"%576."%$43+"C'&X '&HP"%$'&n?G3+"/"/&102#5:.3)?G3+"/"/&102#C43+"%3+iG3,H/&a$'&l'025:H/"%5:RK5:HY4^&CWq}3,8:H/6'@"%$'&3+iGH/"/023,RX"%576n"%$43+"C'&X '&H3P9 5:H 5:K;,&'&1023,8'3j d/1 %e}+4@ c}c:@,3P>a3,~"/6+6'&L>a3+?4?G5:';'@+EQ$45:87&"%$'&m'p 0%&8:3'3+"%576D5:H3,VXoy2%+tz,@`c}c:@A&3,R2$H/"%3+"/&)5:."%$'&a6,0257;5:43,8 0%&1;,0%&H%H%576DH/&3+02R2$DH/?G3,RX&RX6,0%0%&H/?64C4Hj"/63H%5:';87&H/"%3+"/&n5:Z"%$'&K0%&8:3^&C=H/&3+02R2$NH/?G3,RX&| O<+ ,&N57o&H/"%5:>a3+"/&HQot0%6>>]l487"%57?G87&?G3+"/"/&102.C43+"%3+iG3,H/&Haq}3+iGH/"/023,RX"%5764H|Q3+0%&RX6>KiG5:'&C@ "%$'&nRX6>KiG5:43+"%576N6,oM+3,8:l'&Hqtiz~=>a3'5:>a575:';6,0H%l4>a>a5:';z|B6^R1R1l'02H6487~3+"a"%$'&Z0%6z6,"1@Yc}c:@"%$'&H/"%3+"/&=i&5:';N&1+3,8:l43+"/&C@`3,4CT'6,"3,876';"%$'&H/68:l'"%576W?G3+"%$4Hot0%6> "%$45:HH/"%3+"/&5:W"%$'&NC45&10%&"3+iGH/"/023,RX"%5764H JL$45:HC45&10%&4RX&=>&3,4H"%$43+"5:[H/6>&?40%6,iG87&>aH1@"%$'&=m'p$'&l'025:H/"%5:RR13,Ti&Z>6,0%&3,R1R1l'023+"/&"%$43,W3,~?G3+"/"/&102WC43+"%3+iG3,H/&6,o0%&3,H/643+iG87&H%571&aq}Y3,H%8:l4>Z@4ff6'&1"YY&X'&10@G(+*,*z@G?40%6^5:C'&B3,Z&X'3,>?G87&| PZ"%$'&B6,"%$'&10Q$43,4C@"%$'&?6H%H%57iG5:8:57"F~6,o`3,C4>a5:H%H%57iG87~ZH%l4>a>a5:';+3,8:l'&HYot0%6>>]l487"%57?G87&K?G3+"/"/&102#C43+"%3+iG3,H/&H>&3,4HP"%$43+"5:kH/6>&N?40%6,iG87&>aH1@`3RX68:87&RX"%576k6,o3,C4C457"%57,&?G3+"/"/&102SC43+"%3+iG3,H/&H)R13,kot6,02>3D$'&l'025:H/"%5:R>6,0%&3,R1R1l'023+"/&a"%$43,Dm'p@ot6,0K3,~N0%&3,H/643+iG87&n+3,8:l'&n6,offs q}3+;3,5:@AY3,H%8:l4>Z@ff6'&1"KY&X'&10@(+*,*z@;57,&K3,Z&X'3,>?G87&|5}<fi44!H/6>&E`3~^H1@0%&8:3^&C[H/&3+02R2$[$43,H]>6,0%&)5:DRX6>a>6EQ57"%$#"%$'&)5:C'&36,o`',eeFXdW%+d/X'@C'&1,&876,?&C5:D"%$'&RX6"/&X^"6,oL"%$'&\z6,g,6,iG3,?Gl'187&,@ EQ$45:R2$3+0%&)>6,0%&)C'~^43,>a5:R=ql4';$43,44H]\^R2$43+&X&10@4(+*,*'w| A57g,&P5:3K?G3+"/"/&102)C43+"%3+iG3,H/&B$'&l'025:H/"%5:R+@z3K?G3+"/"/&102)H/&3+02R2$Z3+iGH/"/023,RX"%H`3E`3~?G3+0%"6,o'"%$'&?40%6,iG87&>3,4CKH/687,&HA"%$'&M0%&>a3,5:45:';Kq}H%>a3,8:8t| ?40%6,iG87&>U"/6Q6,i4"%3,5:K3,]5:>?40%6,&CK876Eff&10i6l44C@iGl'"B"%$'&a?G3+"/"/&102kqhc}c:@A"%$'&a?G3+0%"6,off"%$'&a?40%6,iG87&>v"%$43+"K5:HBg,&1?4"iz~."%$'&3+iGH/"/023,RX"%576 |P5:HBH/&87&RX"/&CEQ$'&'&1,&103?G3+0%"%5:R1l48:3+0aH/"%3+"/&=&X^?G3,4H%576rqX/>6,&|a5:HaRX64H%5:C'&10%&C 9 3+"/"/&1024H"%$43+"$43,&=i&1&H/&3+02R2$'&C=3+0%&KH/"/6,0%&C@G3,876';EQ57"%$)"%$'&570Ql'?C43+"/&CZRX6H/"1@G3,4CZ"%3+g,&N5:"/6a3,R1RX6l4"Y5:)"%$'&K$'&l'025:H/"%5:R&1+3,8:l43+"%576=qtiz~a>a3'5:>a573+"%576 |A6,o3,~'&1EVH/"%3+"/&Q"%$43+"Y2+Ge+t'M"%$'&QH%3,>&L?G3+"/"/&102@&4RX6l4"/&10%&Ciz~n"%$'&YH/&3+02R2$ JL$'&L?G3+"/"/&1024H&X^?G876,0%&Ciz~n?G3+"/"/&102H/&3+02R2$'&Hff3+0%&Qot6l44C"%$'0%6l';$3,5:4RX0%&>&"%3,8?40%6^RX&H%H1IJL$'&LG02H/"M?G3+"/"/&102)RX64H%5:H/"%HM6,o6487~n"%$'&Y?G3+0%"6,o"%$'&Y?40%6,iG87&>qX/H/"/6'&|M"%$43+"`5:HC4570%&RX"%87~3&RX"/&CViz~["%$'&N>6,&.l44C'&10RX64H%5:C'&1023+"%576 JL$'&='&X^"?G3+"/"/&102W&X^"/&4C4H"%$'&?40%&1^576l4HEQ57"%$H/"/6'&HK"%$43+"K5:"%$'&R1l'0%0%&"KH/"%3+"/&RX6> 5:RX"EQ57"%$"%$'&H/68:l'"%576.ot6l44C#5:"%$'&?40%&RX&C45:';Z?G3+"/"/&102H/&3+02R2$@G3,4CZ"%$45:Hj5:Hj0%&1?&3+"/&Cl4"%5:8'6a>6,0%&KRX6> 5:RX"%HL3+0%&Bot6l44CHY>&"%576'&CN5:"%$'&K?40%&1^576l4HQH/&RX"%576@ "%$'&]$457;$=RX6>?Gl'"%3+"%57643,8RX6H/"Y6,o"%$'&Ks)0%&1;,0%&H%H%576H/&3+02R2$)5:HM6,ot"/&)C4l'&L"/6]"%$'&Yo}3,RX"ff"%$43+" KnY F'6^C'&Hff$43,&B>a3,~aH%l4R1RX&H%H/6,02H1@^3,4C>6H/"6,o"%$'&Q"%5:>&57"5:Hn'6,"3,RX"%l43,8:87~D'&RX&H%H%3+0%~"/6H/&3+02R2$T"%$'&>3,8:8ffot6,0&1,&10%~ KnY F'6^C'&,IZ3,T3,87"/&10243+"%57,&Z5:H]"/6H/&3+02R2$#6487~="%$'&a>6H/"=!?40%6>a5:H%5:';H%l4R1RX&H%H/6,02H1@AEQ$'&10%&a3)?40%6>a5:H%5:';)H%l4R1RX&H%H/6,0K5:H3,P_jF'6^C'&EQ$'6H/&aRX6H/"5:HB8:57g,&87~="/6Zi&l44C'&10%&H/"%5:>a3+"/&C.iz~N"%$'&aR1l'0%0%&"B$'&l'025:H/"%5:R+@3,4C"%$'&10%&1ot6,0%&a8:57g,&87~"/65:4RX0%&3,H/&EQ$'&)"%$'&K'6^C'&5:Hj&X^?G3,4C'&C A5:>a57"%5:';]"%$'&Kzl4>Ki&10L6,oH%l4R1RX&H%H/6,02HLH/&3+02R2$'&C=ot6,0L&1,&10%~l4457ot6,02>a87~#"/63+">6H/I" [0%&H%l487"%Ha5:W3,qtsy 4|0%&1;,0%&H%H%576VH/?G3,RX&,@j3,4Ck3#H/&10257&Ha6,oK qtsnYy 4F|'60%^&1C';,&N0%&H%H%576.H/&3+02R2$'&HYEQ57"%$=5:4RX0%&3,H%5:';as3,4CR13,Ni&K6,0%;3,4571&CN5:=C45&10%&"QE`3~^H1Ijot6,0&X'3,>?G87&,@ff"%$'&Z?G8:3,4'&10RX6l48:C[?&10%ot6,02> qtsyw|0%&1;,0%&H%H%576kH/&3+02R2$'&Haot6,0as uzy1{1{1{ffl4"%5:8jH/6>&H%l457"%3+iG87&BH/"/6,?4?G5:';RX64C457"%5765:Hj>&1"1@G"%$'&Dqtsy2(|0%&1;,0%&H%H%576=H/&3+02R2$'&HQot6,0LsvuUzy1{1{1{f@ 1e JL$45:H5:H`,&10%~)H%5:>a5:8:3+0ff"/6a57"/&1023+"%57,&Bi40%63,C'&45:';nH/&3+02R2$qP5:4H/i&10%;xY3+0%,&1~,@w,,(| O` Z3,87"/&10243+"%57,&5:H"/6=8:5:>a57"Y"%$'&&X^?G3,4H%576.6,o KnY F'6^C'&HK'6^Fl4457ot6,02>a87~,@ ot6,0&X'3,>?G87&,@"/6=H/&3+02R2$D3,8:8H%l4R1RX&H%H/6,02HH%3+"%5:H/ot~^5:';BH/6>&LRX0257"/&102576not6,0i&5:';P?40%6>a5:H%5:';'@,H%5:>a5:8:3+0"/6"%$'&L57"/&1023+"%57,&jEQ5:C'&45:';YH/"/023+"/&1;,~al4H/&C5:)"%$'&KRX6"/&X^"Q6,o;3,>&X"/0%&1&nH/&3+02R2$q<`3+1&43,&,@(+*,*'w|JL$'&KRX6> 5:RX"!FC4570%&RX"/&C=H/"/023+"/&1;,~l4H/&C"/6a 4C)?G3+"/"/&1024HY5:)?G3+"/"/&102NH/&3+02R2$'&HYC'&>64H/"/023+"/&HY3E`3~a"/6nC45:H/"%5:';l45:H%$]?40%6>a5:H%5:';H%l4R1RX&H%H/6,02H <ff64H%5:C'&10H/&zl'&"%5:3,84?G8:3,445:';BEQ$'&10%&Y3, KnY F'6^C'&5:HYH%5:>?G87~Z3)H/&1"Y6,off>6,0%&]"%$43,Ns q}H%l'i4;,63,8t|L3+"/6>aH1@3,4CN"%$'&H%l4R1RX&H%H/6,02HP3+0%&n3,8:8 H%571&]sH%l'iGH/&1"%H6,o"%$45:HQH/&1"1IjH%l'iGH/&1"%HQ>6,0%&]8:57g,&87~"/6)$43,&n3$457;$'&10QRX6H/"Y"%$43,="%$'&K&H/"%5:>a3+"/&K;57,&=iz~=m''p PR13,i&Q5:C'&"%5G&Caiz~aH/687^5:';B&3,R2$ZH%571&QG#wLH%l'iGH/&1"ff3,4Ca&X'3,>a5:45:';"%$'&YH/68:l'"%576aot6,0ffRX6> 5:RX"%HEQ57"%$0%&>a3,5:45:';]3+"/6>aHj5:"%$'&PH/"%3+"/&q}3RX6> 5:RX"ffi&5:';]ot6,0`&X'3,>?G87&P3,Z3,RX"%576)"%$43+"jC'&87&1"/&H`"%$'&B3+"/6>Z@6,0Q3,RX"%5764Hj5:4RX6>?G3+"%57iG87&YEQ57"%$)3,3,RX"%576Z'&1&C'&CZ"/6&H/"%3+iG8:5:H%$"%$'&3+"/6>| !)o}3,RX"1@4"%$45:H`>&1"%$'6^C$43,Hffi&1&l4H/&C)5:"%$'&PRX6"/&X^"j6,o3]C45&10%&"`>&1"%$'6^Cot6,0j5:>?40%6^5:';"%$'&Bm'p$'&l'025:H/"%5:R1H"%$'0%6l';$8:5:>a57"/&CH/&3+02R2$Zq}Y3,H%8:l4>Z@,(+*,*+-3| \z6>&`R13+0%&ff>]l4H/"Ai&"%3+g,&]"/6Y&4H%l'0%&M"%$43+""%$'&ffH/&3+02R2$'&H '&1&C'&C"/6] 4C"%$'&P?40%6>a5:H%5:';KH/&1"%Hj3+0%&B'6,"`>6,0%&P&X^?&4H%57,&Y"%$43,)H/&3+02R2$45:';]&1,&10%~H/&1"1@'iGl'"`57o"%$'&m 'p+3,8:l'&jE`3,HM3,8:H/6RX6>?Gl'"/&Ciz~]0%&8:3^&CaH/&3+02R2$@"%$'&LH%571&jDG=wjH%l'iGH/&1"%HLqt6,0ff3+"M87&3,H/"H/6>&j6,o"%$'&>|$43,&]3,870%&3,C'~i&1&ZH/&3+02R2$'&C@G3,4CRX6> 5:RX"%Hjot6l44CZC4l'025:';]?40%&1^576l4HjH/&3+02R2$'&HYR13,Zi&H%3,&CiKUIQq9[#nUn wnqw`q n-kg|Ugilo-t-firquv!t-liKsUln# nfiFJL$'&a\4<LYJ KYn P_"/0%&1&H/&3+02R2$3,87;,6,0257"%$4>Z@C'&1,&876,?&C.>a3,5:487~Zot6,0BH/&3+02R2$45:';);3,>&n"/0%&1&H1@"/0257&HA"/6Y0%&C4l4RX&"%$'&`zl4>Ki&10A6,o4'6^C'&HA&1+3,8:l43+"/&C]iz~PG02H/"A"/&H/"%5:';Qot6,0&3,R2$'6^C'&ff57o'57"R13,n3&RX" "%$'&5}fi (^z7G4 % : 4^-G-Ns4-G^1 f+3,8:l'&B6,oM57"%Hj?G3+0%&"Li&1ot6,0%&&1+3,8:l43+"%5:';"%$'&]'6^C'&B&X'3,RX"%87~Dq}9&3+028@Aw9-z| JL$'&"/&H/"Q5:Hj?&10%ot6,02>&Ciz~3N?40%6^RX&C4l'0%&,@MR13,8:87&C[H%5:>?G87~U/JA&H/"%^@EQ$45:R2$"%3+g,&Ha3,H3+0%;l4>&"%H3.'6^C'&)3,4C[3."%$'0%&H%$'68:C+3,8:l'&,@3,4CNC'&1"/&102>a5:'&HQ57o"%$'&]+3,8:l'&K6,oM"%$'&]'6^C'&]5:HQ;,0%&3+"/&10qt6,0P&zl43,8t|`"%$43,="%$'&]"%$'0%&H%$'68:C@Giz~0%&R1l'02H%57,&87~]"/&H/"%5:';K"%$'&Q'6^C'&, HH%l4R1RX&H%H/6,02HYqt"/6n3KH/?&R15G&CC'&1?4"%$ |2@iGl'"M6487~nl4"%5:8'"%$'&Y5:'&zl43,8:57"F~5:H?40%6,& JL$'&j?40%6^RX&C4l'0%&jR13,a&3,H%5:87~]i&L>6^C45G&C]"/60%&1"%l'02a3P;,0%&3+"/&10ff+3,8:l'&jot6,0M"%$'&Q'6^C'&jEQ$'&H%l4R2$3+3,8:l'&5:Hjot6l44C@'"%$'6l';$Z"%$45:HL>a3~ZH/"%5:8:8 i&K87&H%H`"%$43,"%$'&K'6^C'&HL3,RX"%l43,8+3,8:l'&,@43,4C57"L$43,Hi&1&)H%$'6EQ"%$43+"`"%$'&BJA&H/"`?40%6^RX&C4l'0%&,@z&4$43,4RX&CEQ57"%$)>&>6,0%~5:"%$'&Pot6,02> 6,o3]"/023,4H/?6H%57"%576"%3+iG87&,@4R13,)i&Bl4H/&C)57"/&1023+"%57,&87~"/6;57,&3,)& =R157&"L3,87;,6,0257"%$4> "%$43+"LC'&1"/&102>a5:'&Hff"%$'&B&X'3,RX"j+3,8:l'&6,o3'6^C'&q}98:3,3+"1@\^R2$43+&X&10@ 95 %8:H1@' KO @Aw,,|JL$'& P \aH%l'i40%6l'"%5:'&j6,o L 5:HM,&10%~aH%5:>a5:8:3+0M"/6n3KC'&1?4"%$^Fl4i6l44C'&Cn,&102H%5766,o"%$'&YJA&H/"?40%6^RX&C4l'0%&,@3,4C"%$zl4H"%$'&j L 3,87;,6,0257"%$4>5:HMH%5:>a5:8:3+0 "/6KH%l4R2$a3,a57"/&1023+"%57,&Q3+?4?G8:5:R13+"%576n6,oJA&H/"JL$'&B>a3,5:)C45&10%&4RX&B8:57&H`5:"%$43+"j L Q P \Z3+?4?G8:57&Hff57"/&1023+"%57,&C'&1&1?&45:';Zqtiz~R13,8:8:5:';] L |"/6"%$'&ZH%l4R1RX&H%H/6,02H6,o KnY F'6^C'&H1@EQ$'&10%&3,HJA&H/"aR13,8:8:Hn57"%H/&87oL0%&R1l'02H%57,&87~#EQ57"%$"%$'&ZH%3,>&RX6H/"i6l44C HL30%&H%l487"1@^ L 4C4Hff"%$'&B6,?4"%5:>a3,8RX6H/"L6,o 3,~)H/687,&CNP_jF'6^C'&,@^EQ$45:R2$ZJA&H/"LC'6z&H'6,"]qt"%$'6l';$Z"%$'&$457;$'&10LRX6H/"Q0%&1"%l'02'&C)iz~q}>6^C45G&C |JA&H/"LEQ$'&)"%$'&RX6H/"Q6,o 3a'6^C'&B5:Hj?40%6,&"/6a&X'RX&1&C="%$'&B"%$'0%&H%$'68:CZ5:HjH/"%5:8:83876Eff&10Qi6l44C6Z"%$'&K'6^C'&Hj6,?4"%5:>a3,8RX6H/"|_L&RX&"%87~,@Qff6'&1")3,4CSY&X'&10q(+*,*+-z|a?40%&H/&"/&CV3#;,&'&1023,8YC'&1?4"%$^G02H/"H/&3+02R2$V3,87;,6,0257"%$4>ot6,0 KnYP_j;,023+?G$4H1@ R13,8:87&C# P \ @EQ$45:R2$#5:H3,8:H/6,&10%~H%5:>a5:8:3+0B"/6 L A57g,&a L @57"4C4Hj"%$'&6,?4"%5:>a3,8ARX6H/"Q6,o&1,&10%~H/687,&C='6^C'&,@G3,4C=5:>?40%6,&C=876Eff&10Qi6l44C4Hj6='6^C'&HL"%$43+"Y3+0%&&X^?G876,0%&C.iGl'"B'6,"H/687,&C P \ @$'6Eff&1,&10@ H/"/6,?GHH/&3+02R2$45:';)"%$'&aH%l4R1RX&H%H/6,02HB6,o`3, KnY F'6^C'&3,HH/6z63,HP6'&6,o"%$'&>5:HPot6l44CN"/6$43,&a3ZRX6H/"B;,0%&3+"/&10K"%$43,."%$'&R1l'0%0%&"P&H/"%5:>a3+"/&ot6,0B"%$43+"'6^C'&,@jEQ$'&10%&3,H L ?&10%ot6,02>aH57"/&1023+"%57,&.C'&1&1?&45:';#H/&3+02R2$'&H)l4"%5:8L"%$'&N'6^C'&N5:HH/687,&Ck6,0H%$'6EQa"/6n$43,&P3]RX6H/"ff;,0%&3+"/&10`"%$43,"%$'&YR1l'0%0%&"&H/"%5:>a3+"/&CRX6H/"ff6,o"%$'&Q?40%&C'&RX&H%H/6,0 KnY F'6^C'&87;,6,0257"%$4>ot6,0s)0%&1;,0%&H%H%576DH/&3+02R2$D$43,&H%$'6EQ#"%$43+"K57"+5:HK^'6?,&1"K025:>>6,&"0%&%HP & EQ=57R1"%57$#&"3,K#"%$4h"/3,&1#023+"%57,& JA&H/"]3,#&L O= X^?&1025:>&"%3,8MRX6>?G3+025:H/6#i&1"FEff&1&D L 3,4C#"%$'&P \)3,87;,6,0257"%$4>0%&>a3,5:4H`"/6ai&C'6'&L @h"/&1023+"%57,&nJA&H/"Y3,4C= P \3,8:8?&10%ot6,02>"/6,?'FC'6EQ@C'&1?4"%$^G02H/"L57"/&1023+"%57,&nC'&1&1?&45:';H/&3+02R2$'&H1@BiGl'"'6'&6,on"%$'&H/&DR2$43+023,RX"/&1025:H/"%5:R1H6,on"%$'&#3,87;,6,0257"%$4>aH3+0%&#&H%H/&"%5:3,8Bot6,0"%$'&#"%$'&570l4H/&a5:RX6>?Gl'"%5:';3,#5:>?40%6,&C#$'&l'025:H/"%5:R ~ KnYP_ H/&3+02R2$D3,87;,6,0257"%$4>R13,#i&l4H/&C"/6R13+0%0%~#6l'"]"%$'&0%&8:3^&CH/&3+02R2$@M3,Hn876';.3,Hn57"]C45:H%RX6,&102H]"%$'&6,?4"%5:>a3,8ffRX6H/"Zqt6,0a3=;,0%&3+"/&10a876Eff&10i6l44C |`6,o&1,&10%~=&X^?G3,4C'&C#P_jF'6^C'& 6,0B&X'3,>?G87&,@H/"%3,4C43+02C q n 5:8:H%H/6@ w,9 |P3,4CN"%$'&Y&'&1023,8:571&C %57g^H/"/023Z3,87;,6,0257"%$4>viz~ N3+0%"/&8:8:5 3,4C8=6"%3,43+025Qq!w +|Pi6,"%$C'6Z"%$45:H1@3,4Ci6,"%$6+&10Y3?6H%H%57iG5:8:57"F~6,o "/023,C45:';;,0%&3+"/&10P>&>6,0%~0%&zl4570%&>&"%Hjot6,0Q87&H%HLH/&3+02R2$="%5:>&qt"%$'6l';$"%$'&0%&H%l487"%H`6,off6'&1"Q3,4CNY&X'&10@ (+*,*+-'@ 5:4C45:R13+"/&B"%$43+"j"%$45:HL>a3~Z'6,"ji&B"%$'&KR13,H/&|H/&3+02R2$k$43,Hi&1&T>6H/"%87~[5:,&H/"%57;3+"/&CW5:["%$'& n3+0%&36,oP;3,>&?G8:3~^5:'; JL$'&K nYP _ Pqt6,_O0 N5:^N3G|H/&3+02R2$]H/?G3,RX&HA0%&1?40%&H/&"%5:';L"FEff6+?G8:3~,&10;3,>&H3+0%&ffH/6>&1EQ$43+"C45&10%&"K nYot0%6>"%$'&Ks)0%&1;,0%&H%H%576NH/?G3,RX&,I`"%$'&10%&]5:HQ'6RX64RX&1?4"Y6,oMH/68:l'"%576RX6H/"1@6,"%$'&10Y"%$43,"%$'&]Eff6 876H/"C45:H/"%5:4RX"%576@M3,4C[ot6,0>6H/";3,>&Ha57"5:H5:'ot&3,H%57iG87&"/6#H/&3+02R2$Tot6,03RX6>?G87&1"/&H/68:l'"%576@M023+"%$'&10"%$'&H/&3+02R2$3,5:>aHff"/6a5:>?40%6,&B"%$'&B3,R1R1l'023,RX~)6,o3$'&l'025:H/"%5:RQ&H/"%5:>a3+"/&B6,oA"%$'&l4H/&1o}l48:'&H%H6,o 3n>6,&6,0]?6H%57"%576D5:#"%$'&;3,>& JL$zl4H1@;3,>&X"/0%&1&H/&3+02R2$'&Hn3+0%&)C'&1?4"%$^i6l44C'&C@023+"%$'&10n"%$43,RX6H/"!i6l44C'&C@3,4C.+3,8:l'&HP3+"B"%$'&a87&3+off'6^C'&HY6,o"%$'&"/0%&1&a3+0%&n;57,&.iz~N3ZH/"%3+"%5:R$'&l'025:H/"%5:Ro}l44RX"%576s)0%&1;,0%&H%H%576]R13,Ki&Mot6,02>]l48:3+"/&C]5:"%$45:HE`3~,@,iz~B"%3+g^5:';Q"%$'&ffH%l4>6,oG3,R1R1l4>]l48:3+"/&C]3,4CK&H/"%5:>a3+"/&CRX6H/"]3,HK"%$'&H/"%3+"%5:R$'&l'025:H/"%5:R]o}l44RX"%576 C'&1?4"%$^i6l44C'&C.H/&3+02R2$DEQ57"%$#3=H/"%3,4C43+02C;3,>&X"/0%&1&H/&3+02R2$W3,87;,6,0257"%$4> q}9&3+028@Qw9 -'Q98:3,3+"n&1"3,8 @Qw,,|nR13,[i&Zl4H/&C"/6#5:>?40%6,&)"%$'&Z3,R1R1l'023,RX~6,o"%$'&n&H/"%5:>a3+"/&C#RX6H/"B6,o"%$'&n0%6z6,"'6^C'& JL$45:H1@$'6Eff&1,&10@Ao}3,5:8:HY"/6Z3,R2$457&1,&a"%$'&>a3,5:N6,i^/&RX"%57,&5}fi446,off0%&8:3^&CH/&3+02R2$@AEQ$45:R2$5:HP"/6C45:H%RX6,&10q}3,4C#H/"/6,0%&|P5:>?40%6,&CRX6H/"&H/"%5:>a3+"/&HBot6,0"%$'&aH%571&nsH/"%3+"/&HP&4RX6l4"/&10%&CC4l'025:';"%$'&H/&3+02R2$@H/6)"%$45:HQ>&1"%$'6^CNEff6l48:CN$43,&n"/6)i&]l4H/&CN5:N3C45&10%&"E`3~,@c c:@ 3,HP3C'&1?4"%$^i6l44C'&C876z6,gF3,$'&3,C="/6)5:>?40%6,&K"%$'&]3,R1R1l'023,RX~6,oM$'&l'025:H/"%5:RB&1+3,8:l43+"%5764H6,oMH/"%3+"/&HP5:Z"%$'&]'6,02>a3,80%&1;,0%&H%H%576=H/&3+02R2$ P"%$'&K6,"%$'&10P$43,4C@GH%5:4RX&"%$'&]5:>?40%6,&C=$'&l'025:H/"%5:R+3,8:l'&j5:H5:]"%$45:H>6^C'&`6,ol4H/&j'6,"MH/"/6,0%&CiGl'" 6487~Kl4H/&Cnot6,0M"%$'&jH/"%3+"/&Lot0%6> EQ$45:R2$]"%$'&L876z6,gF3,$'&3,CH/&3+02R2$6,0257;5:43+"/&H1@57"P5:HP'6,"B'&RX&H%H%3+0%~N"/6ZH%5:>?G8:57ot~)"%$'&ns)0%&1;,0%&H%H%576.H/?G3,RX&,@A3,4C.3?6,"/&"%5:3,8:87~>6,0%&?6Eff&10%o}l48 0%&8:3'3+"%576R13,i&l4H/&CQF(+ 8 F B !AFSJL$'&j"FEff6K?G8:3,4'&102H &"/&10%&C5:"%$'&Y(+*,*+-K!"/&10243+"%57643,8498:3,445:';K<ff6>?&1"%57"%576@JL9M-K3,4C } - @3+0%&P,&10%~H%5:>a5:8:3+0IA"%$'&Y6487~C45&10%&4RX&P5:H"%$43+" } - 5:,&H/"%HjH/6>&P&X6,0%"j5:"/6nRX6>?Gl'"%5:';n3n>6,0%&3,R1R1l'023+"/&j$'&l'025:H/"%5:R+@"%$'0%6l';$]3YH/&10257&HA6,o4H/&3+02R2$'&H5:0%&8:3^&CK0%&1;,0%&H%H%576]H/?G3,RX&Hjqt"%$'&ffs)0%&1;,0%&H%H%576H/?G3,RX&H|YEQ$45:R2$3+0%&C'&10257,&C.ot0%6>"%$'&aH%3,>&a0%&8:3'3+"%576#3,HB"%$'&m $ $'&l'025:H/"%5:R]l4H/&Ciz~.JL9M- !^C'&1&C@ "%$'&]>6,"%57+3+"%576=ot6,0Y&"/&1025:';i6,"%$=?G8:3,4'&102HjE`3,HY"/6)>a3+g,&nl4H/&K6,oM"%$'&]RX6>?&1"%57"%576N3,HY3,&X^?&1025:>&"%3,8 &1+3,8:l43+"%576#6,off"%$'&a0%&8:3^&C#H/&3+02R2$D"/&R2$445:zl'&,@3,HEff&8:83,HKRX6>?G3+025:';Zi6,"%$?G8:3,^'&102HQ"/66,"%$'&10BH/"%3+"/&X6,of"%$'&XF3+0%"B6,?4"%5:>a3,8A"/&>?6,023,8?G8:3,4'&102H OQ 6,0BH/&1,&1023,80%&3,H/64HY$'6Eff&1,&10@"%$'&RX6>?&1"%57"%576Z0%&H%l487"%HLC45:CZ'6,"Q?40%6^5:C'&B"%$'&KRX6>?G87&1"/&K?G5:RX"%l'0%&B6,o "%$'&K0%&8:3+"%576Zi&1"FEff&1& } - 3,4CJL9M-HPC'&>64H/"/023+"/&C$'&10%&,@ } - R13,N?40%6^C4l4RX&]i&1"/"/&10P0%&H%l487"%HQ"%$43,.JL9M-)5:NH/6>&n6,o"%$'&nRX6>n?&1"%57"%576TC'6>a3,5:4H1@M"%$'6l';$W5:T'6#C'6>a3,5:TC'6z&H } - RX6>?G87&1"/&87~C'6>a5:43+"/&Z6,&10)JL9M- h"a5:H>a3,5:487~=6#?40%6,iG87&>aHB"%$43+"]3+0%&$43+02C@Aot6,0Ki6,"%$#?G8:3,4'&102H1@"%$43+""%$'&$'&l'025:H/"%5:Rn5:>?40%6,&>&"K0%&XH%l487"%5:';jot0%6>r0%&8:3^&CKH/&3+02R2$]~^57&8:C4H3,]3,C'+3,"%3+;,& !]H/6>&ffC'6>a3,5:4H1@"%$'&ff5:>?40%6,&>&"A6,&10"%$'&$ $'&l'025:H/"%5:RL5:H`'6,"`&'6l';$)"/6RX6>?&4H%3+"/&Bot6,0`"%$'&P"%5:>&BH/?&"`RX6>?Gl'"%5:';n57" OM >6,0%&BC'&1"%3,5:87&C3,43,87~^H%5:H 0%&H%l487"/&C5:a3qtEff&3+g4|R2$43+023,RX"/&102573+"%576a6,o"%$'&QC'6>a3,5:4H5:nEQ$45:R2$0%&8:3^&CaH/&3+02R2$R13,ai&&X^?&RX"/&C."/6Zi&RX6H/"B&X&RX"%57,&,IB5:.H%l4R2$C'6>a3,5:4H1@&X^?G3,4C45:';H%>a3,8:8 H/"%3+"/&H5:HPRX6>?Gl'"%3+"%57643,8:87~R2$'&3+?&10P"%$43,=&X^?G3,4C45:';8:3+0%;,&nH/"%3+"/&H1@3,4CNH%>a3,8:8AH/"%3+"/&HP"/&4C="/6)$43,&H%>a3,8:8AH%l4R1RX&H%H/6,0PH/"%3+"/&H!"%$'&C'6>a3,5:4HBEQ$'&10%&a57"5:HB"/6z6=&X^?&4H%57,&,@6#"%$'&a6,"%$'&10]$43,4C@"%$'&RX&"/023,8M?40%6,iG87&>5:HP"%$43+""%$'&Li4023,4R2$45:';Bo}3,RX"/6,0ff6,os)0%&1;,0%&H%H%5765:H`7+'Xd"%$43,a"%$43+"M6,o'6,02>a3,8'0%&1;,0%&H%H%576aH/&3+02R2$@^H/6"%$43+"H/&3+02R2$45:';#5:"%$'&Z0%&8:3^&CTH/?G3,RX&=5:HnRX6>?Gl'"%3+"%57643,8:87~ka+d%&X^?&4H%57,&#q}zl457"/&RX6"/023+0%~"/6#"%$'&5:C'&3)6,off6,i4"%3,5:45:';Z3$'&l'025:H/"%5:RK&H/"%5:>a3+"/&aiz~.H/687^5:';Z3W/H%5:>?G8:5G&C4a?40%6,iG87&>| JjEff6=>&3,H%l'0%&H1@! cff"%$'&Ki4023,4R2$45:';no}3,RX"/6,0Y6,offP_jF'6^C'&HL5:Z0%&8:3^&CH/&3+02R2$=0%&8:3+"%57,&"/6i4023,4R2$45:';o}3,RX"/6,0P5:Z"%$'&6,0257;5:43,8H/&3+02R2$NH/?G3,RX&]3,4C"%$'&K0%&8:3+"%57,&]H%571&6,o"%$'&KH%l4R1RX&H%H/6,02HQ6,offP_jF'6^C'&HQ5:Z0%&8:3^&C=H/&3+02R2$@EQ$'&10%&`ot6l44CK"/6Bi&ff;,6z6^C5:4C45:R13+"/6,02H6,o $'6ETEff&8:8 } - ?&10%ot6,02>&C@,RX6>?G3+0%&Cn"/6BJL9M-'@5:n3Y;57,&C'6>a3,5: !a"%$'&Q&X^?&1025:>&"%H1@"%$'&H/&P>&3,H%l'0%&HEff&10%&Q"%3+g,&)iz~&X^?G876,025:';B"%$'&YH/&3+02R2$)H/?G3,RX&H1@^iGl'"57">a3~3,8:H/6i&L?6H%H%57iG87&`"/6]&H/"%5:>a3+"/&Q"%$'&>vq}57o'6,""/6]R13,8:R1l48:3+"/&Y"%$'&>&X'3,RX"%87~4|ot0%6>"%$'&YC'6>a3,5:C'&H%RX0257?4"%576JL$'&B3,43,87~^H%5:H`6,oA"%$'&C'6>a3,5:4Hj5:EQ$45:R2$)0%&8:3^&CZH/&3+02R2$Zo}3,5:8:H`"/6i&P&X&RX"%57,&K3,8:H/6?65:"%H`6l'"?6H%H%57iG5:8:57"%57&Hffot6,0P5:>?40%6,&>&"1@3,4CNH/&1,&1023,8 5:C'&3,HQot6,0P?6,"/&"%5:3,85:>?40%6,&>&"%HQ"/6)"%$'&]>&1"%$'6^CR13,Ui&Not6l44CS5:V0%&8:3+"/&C/5:4RX0%&>&"%3,8:[H/&3+02R2$H%R2$'&>&H5:V"%$'&8:57"/&1023+"%l'0%& JL$'&H/&5:4R18:l4C'&8:5:>a57"%5:';ZH/&3+02R2$"/6.3NH%>a3,8:87&10Kot023,RX"%576D6,oj"%$'&0%&8:3^&CH/?G3,RX&,@Ml4H%5:';=RX6> 5:RX"%HK"/6.C4570%&RX"KH/&3+02R2$"/6a"%$'&KH/"%3+"/&HY>6,0%&K8:57g,&87~"/6i&$43,&K"%$'&570Q$'&l'025:H/"%5:RY+3,8:l'&HL5:>?40%6,&C@43,4C3,87"/&10243+"%57,&KH/&3+02R2$3,87;,6,0257"%$4>aH`ot6,0 KnYP_;,023+?G$4H JL$45:H`5:Hj6'&C4570%&RX"%576)ot6,0Lo}l'"%l'0%&C'&1,&876,?G>&"%H5:43,8:87~,@0%&H%l487"%HMR187&3+0287~nC'&>64H/"/023+"/&Q"%$43+"3,87"%$'6l';$5:>?40%6^5:';P"%$'&Y$'&l'025:H/"%5:R`5:>?40%6,&HM?&10/ot6,02>a3,4RX&=5:[H/6>&=C'6>a3,5:4H1@ff3,876'&57"a5:H'6,"&'6l';$T"/6D3,R2$457&1,&=;,6z6^CT?&10%ot6,02>a3,4RX&)0%&8:5:3+iG87~5}fi (^z7G4 % : 4^-G-Ns4-G^1 f3,RX0%6H%HP3,8:8"%$'&nRX6>?&1"%57"%576=C'6>a3,5:4H <`9JqtP5:C43,8Y&X'&10@(+*,*+-z|2@"%$'&]6487~)6,"%$'&10P6,?4"%5:>a3,8"/&>?6,023,8?G8:3,4'&10j"/6a?G3+0%"%5:R157?G3+"/&B5:)"%$'&KRX6>?&1"%57"%576@'3+?4?&3+02Hj"/6a~^57&8:Ci&1"/"/&10Q0%&H%l487"%Hj5:Z>6H/"RX6>?&1"%57"%576ZC'6>a3,5:4HBqt"%$'6l';$'6?40%&R15:H/&BRX6>?G3+025:H/6Z$43,H`i&1&Z>a3,C'&| <`9JB@'8:57g,&BJL9M-'@4l4H/&H"%$'&]"/&>?6,023,8m $ $'&l'025:H/"%5:R+@GiGl'"Y?&10%ot6,02>aHQ3)'6^FC4570%&RX"%57643,8AH/&3+02R2$@3,4C.l4H/&HPRX64H/"/023,5:"Y0%&1?'0%&H/&"%3+"%5763,4CN?40%6,?G3+;3+"%576."/&R2$445:zl'&HY"/6Z5:'ot&10PRX6H/"Bi6l44C^5768:3+"%5764H JL$'&n5:'& =R157&4RX~6,oC4570%&RX"%57643,8`H/&3+02R2$W5:"/&>?6,023,8ff?G8:3,445:';N$43,Hni&1&['6,"/&CTi&1ot6,0%&,I"%$zl4H<`9J3,4C } - R13,i&]H%3,5:C"/6)5:>?40%6,&K6="FEff6ZC45&10%&"QEff&3+g^'&H%H/&HY6,oMJL9M- <ff6>KiG5:45:';"%$'&H/&]5:>?40%6,&>&"%HP5:H3,'6,"%$'&10YR2$43,8:87&';,&Bot6,0L"%$'&o}l'"%l'0%&,Qkgto|ksqe& RX"/6,0Y&X'&10n$43,HK$43,CD3,D5:>?6,0%"%3,"?G3+0%"K5:"%$'&C'&1,&876,?G>&"K6,oj0%&8:3^&CDH/&3+02R2$H%5:4RX&57"%H5:4RX&1?4"%576=3,4C"%$'0%6l';$=zl4>&10%6l4HQC45:H%R1l4H%H%5764H ,643,HB+3+024H/"/0A6 >Z@ 9&10 n ~ziG876>Z@ !_x3,H%H/6+R15:3+"/&Y&C457"/6,0N3+025:3 63,4C"%$'&Y3,'6~^>6l4HM0%&1^57&1Eff&102Hff3,8:84?40%6^5:C'&C,&10%~a$'&87?4o}l48'RX6>a>&"%Hff6C'023+ot"%Hj6,o"%$'&?G3+?&10 OM 8:8o}3,l487"%Hj3+0%&6,o RX6l'02H/&K>a5:'&F1FF hF-$'6'@ K@MQ6,?RX0%6,ot"1@M @YY8:8:>a3,@ q!w9,| OC ,e e}d4e'^d%=+ +d}et] O[ C4C45:H/6^&H%87&1~ff&8:8:>a3,@4_ q!w, ,| O' + +n`d/%,d/+nntz 9M025:4RX&1"/6Y457,&102H%57"F~)9M0%&H%Hff6'&1"1@ @Y&X'&10@ q(+*,*+-z| OK N3,87;,6,0257"%$4>i&1"/"/&10P"%$43, !`d/c(h+etsK,e}+ ++ Xd%X +sjh @'?4? w-].w%-<`3+1&43,&,@+J q(+*,*'w| h"/&1023+"%57,&EQ5:C'&45:'; !`d/c et2GeFXd ,e}+ + + Xd%X j+YdXe} ff1+2GeFXt 7zX Kjh @4?4? ,(,]Z,9(<`l487i&102H/6@ff @L\^R2$43+&X&10@L q!w,,| \z&3+02R2$45:';EQ57"%$k?G3+"/"/&102UC43+"%3+iG3,H/&H ! + +++ Xd%X +s2@'68 w1* ^w6,fiffL G@'?4? -*(]-4w \z?4025:';,&10?@ \ q(+*,*'w| 98:3,445:';nEQ57"%$Z?G3+"/"/&102=C43+"%3+iG3,H/&H !`d/c +etE^d/24%+ + Xd%X+ C'&87g+3,+>s`7+44tzn @4?4? wKZ(-6+>a3,4@j q(+*,*+-z| 9 P ( (zI`"%$'&N8:3,';l43+;,&Not6,0"%$'&NR18:3,H%H%5:R13,8L?G3+0%"6,o+ C'&87g+3,!9`><?@`- \ @`!4Qet2GeFXd ,e}+ + `7+44tz +Q41e}}e}+:1e}@ ?4? ()# O= +3,5:8:3+iG87&a3+"%Oh~t% >hQSfi19h&8:'&10@ KO @ B6,0%oh@4_ @ Y3,43,@\ q(+*,*+-z| Oj C4C457"%57,&P?G3+"/"/&102=C43+"%3+iG3,H/&K$'&l'025:H/"%5:R1H ' ^d + 1"!L%+d/X@ @( +KZ^5w@ @Mv6';'@ ]O q(+*,*| 9 P ( w+I &X^"/&4H%576["/69 P Uot6,0&X^?40%&H%H%5:';N"/&>?6,023,86?G8:3,445:';nC'6>a3,5:4H ' ^d + 1 6"!L%+d/X@ h@ ^wPNw(-P3+0%025:C'6'@ KO @ 6@ @G 6';'@ ]O q(+*,*(| "/&>?6,023,8 ?G8:3,445:';]H/~^H/"/&>ot6,0QC4l'023+"%57,&B3,RX"%5764H6,o9 P ( w !`d/#1 Net'#,et$E^d%%24%+ + Xd%X +CYdXe} ff1+#2GeFXt 7zX %h A&@4?4? 9 ,]),+*P3+0%025:C'6'@ KO @P43,5:4C45:3^@ +LO @`3+0%i&10@ ffO q(+*,*'w| JL5:>&X6,?4"%5:>a3,8?G8:3,445:';n5:Z"/&>?6,023,8?40%6,i'87&>aH !`d/fic +etE^d/24%+ + Xd%X +s`7+44tzn @4?4? , -*(5}fi44P3,H%R2$4457;'@ q!w+| OT ?40%6,iG87&>H%5:>a5:8:3+0257"F~3+?4?40%63,R2$T"/6#C'&1^5:H%5:';N$'&l'025:H/"%5:R1H1I 5702H/"n0%&H%l487"%H!`d/c +et_2GeFXd ,e}+ +' '+tGe +Xd%X K+YdXe} ff1+2GeFXt 7zX njh )(@^?4?+*'wPZ+3*P5:4H/i&10%;'@ @Y3+0%,&1~,@ q!w,,(| h"/&1023+"%57,&i40%63,C'&45:'; YdXe} ff1+ 2GeFXt 7zX X@*q(h|2@, KZ9 ,Y3,H%8:l4>Z@9 q(+*,*+-3| !>?40%6^5:';K$'&l'025:H/"%5:R1H "%$'0%6l';$H/&3+02R2$ !I`d/+c E^d/24%+ + Xd%X B+h ,-@4?4? w1*^wP.w1*,(Y3,H%8:l4>Z@z9 q(+*,*+-,i| JL9M-' *+-a3,4C } - !4et_2GeFXd ,e}+ +S `7+44tz +Q41e}}e}+.:1e}@?4? 9 K-* +3,5:8:3+iG87&B3+#" %h~t% >hQ,fi19hY3,H%8:l4>Z@B9 @ff6'&1"1@K @] Y&X'&10@ q(+*,*| n &1E3,C4>a5:H%H%57iG87&$'&l'025:H/"%5:R1H=ot6,0DC'6>a3,5:^5:4C'&1?&4C'&"?G8:3,445:'; !I`d/c h+etK,e}+ + + Xd%X +Ijh @?4? w,w,.w,w9Y3,H%8:l4>Z@a9 @) Y&X'&10@ q(+*,*,*| C4>a5:H%H%57iG87&S$'&l'025:H/"%5:R1H[ot6,0k6,?4"%5:>a3,8Z?G8:3,445:'; !`d//c ,et82GeFXd ,e}+ + + Xd%X =+ YdXe} ff1+(2GeFXt 7zX n`7+44tz+X'% ^tzj:#@4?4? wX-*n.wX- YY `9M0%&H%HY3,H%8:l4>Z@z9 @4 Y&X'&10@^ q(+*,*'w| Q&l'025:H/"%5:RL?G8:3,445:';BEQ57"%$"%5:>&P3,4C0%&H/6l'02RX&H !`d/c +etE^d/24%+ + Xd%X n+`7+44tzn @G?4? w(^wP.w,(Q&10243,C' 6 87;,~^5@ @vQ687"/&,@_ q(+*,*,*| N+ ^?&1025:>&"%H]EQ57"%$T3,l'"/6>a3+"%5:R13,8:87~RX0%&3+"/&Ck>&>6,0%~iG3,H/&CV$'&l'025:H/"%5:R1H !2 `d/c yXe}d/e}+ %0!Lj 1+d[^7,e}+ %+ Gff d/,ta,e}+ %04et2GeFXd ,e}+ +L G+Q' ^ Q1!'Nh@ ?4? 9( ^wPZ(,+*Q6+>a3,4@' @' + C'&87g+3,>?@'\ q(+*,*| JL$'&PR18:3,H%H%5:R13,8G?G3+0%"ff6,o!9`<-'I 6,&10%^57&1E JA63+?4?&3+05:)"%$'2& ' ^d + 1 1!L%+d/Xqt"%$45:HY\z?&R15:3,8JA023,R%g4|Q6+>a3,4@` @ + C'&87g+3,>?@`\ @ + ';87&10%"1@_ @ffA57?6,023,RX&,@ ffO JL$4;5 &1 iG3,l^@`\ @`JA0 l' ;'@`\ q(+*,*+-z|JA6E`3+02C4HL0%&3,8:5:H/"%5:RQi&4R2$4>a3+0%g^H`ot6,0L?G8:3,445:';'IzJL$'&C'6>a3,5:4H`l4H/&CZ5:)"%$'&R18:3,H%H%5:R13,8?G3+0%"j6,o!9`<- !#4ets2GeFXd ,e}+ +`7+44tz +Q41e}}e}+:1e}@?4? DwX- +3,5:8:3+iG87&]3+"%h~t% >hQSfi19hQ687"/&,@ff_ @ff9&10%&1,@@ R5:>a>&10@M_ @` N3,R 643,8:C@ KO q!w,,| Y57&1023+02R2$45:R13,8\z&3+02R2$45:';#3+iGH/"/023,RX"%576T$457&1023+02R2$457&Hn & =R157&"%87~ ! `d/3c 4etK,e}+ + + Xd%X N+YdXe} ff1+S 2GeFXt 7zX _jh (5@4?4? ,+*nZ,,l4';$43,44H1@ KO @j\^R2$43+&X&10@j q(+*,*'w| \z6,g,6,iG3,I + 4$43,4R15:';;,&'&1023,8YH%5:';87&XF3+;,&")H/&3+02R2$>&1"%$'6^C4HLl4H%5:';C'6>a3,5:)g^'6EQ87&C';,& YdXe} ff1+ 2GeFXt 7zX X6@(q!wh(|2@(^w])(,^wB6,0%oh@'_ q!w9 ,| &1?4"%$^G02H/"j57"/&1023+"%57,&XFC'&1&1?&45:';'I )6,?4"%5:>a3,83,C4>a5:H%H%57iG87&L"/0%&1&H/&3+02R2$ YdXe}1+S 2GeFXt 7zX X@ 7q!w|2@ .w1*B6,0%oh@_ q!w,,| 0%"%5 R15:3,8 5:"/&8:8:57;,&4RX&H/&3+02R2$D3,87;,6,0257"%$4>aH !8K+ 3 y% .1 +d}et]+'e,e}+4@R2$43+? , <`_L<S9M0%&H%H9 '%+dZ1 +QSA5:l,@ : A@ B6z&457;'@\ @+ l'02RX~,@ ]O q(+*,*(| \z?&1&C45:';Ll'?"%$'&R13,8:R1l48:3+"%576B6,o4$'&l'025:H/"%5:R1Hot6,0$'&l'025:H/"%5:RH/&3+02R2$^iG3,H/&C?G8:3,445:'; ! `d/1c ;etK,e}+ + + Xd%X +YdXe} ff1+2GeFXt 7zXjh A&@4?4? 3- --^w6';'@ ]O @A 6L@ q(+*,*| [+ ^?G87657"%5:';3;,023+?G$'?G8:3,=ot023,>&1Eff6,0%gN5:="/&>?6,023,8?G8:3,445:'; !4et2GeFXd ,e}+ + + Xd%X +K'e+a,eF%`7+44tz#+X'% ^ tz j # -4,@?4? ^wYZ,(5}~}fi (^z7G4 % : 4^-G-Ns4-G^1 fN3+0%"/&8:8:5@ KO @C=6"%3,43+025@ q!w+| C4C457"%57,& KnY P_W;,023+?G$4H !n`d/c+4,d/2GeFXd ,e}+ +'+tGe + Xd%X +sYdXe} ff1+2GeFXt 7zX Kjh <4,@4?4? wQ.w,wn 5:8:H%H/6H/@68:l'Kn "%O576 4HO q!w!,9 `| d/O \zcL&3+ =O02R2:$4 5:'; )+?4z0%,6,d%iG287&>n@G?4FH/? 687^w5:(,';].3,4Cw+N* ;3,>&X?G8:3~^5:';)"/0%&1&HPot6,0B>a5:45:>a3,8ARX6H/"9&3+028@M q!w9-z| 8] ^dfXe}X&>2GeFXt 7zXGe%+d/X e}d/,eF,61+d +QS'eFXd_`d/3yX:X + tz4C C45:H/6^&H%87&1~98:3,3+"1@ KO @+\^R2$43+&X&10@+ @95 %8:H1@1 @ KO @C q!w,,| ff&H/"!G02H/"4^&C^FC'&1?4"%$>a5:45:>a3P3,87;,6,0257"%$4>aHYdXe} ff1+S 2GeFXt 7zX X@'; q!wh(|2@(,,]Z(,,9M0257&C457"%5:H1@ KO+LO q!w,,| N3,R2$45:'&LC45:H%RX6,&10%~]6,oG&X&RX"%57,&Q3,C4>a5:H%H%57iG87&ff$'&l'025:H/"%5:R1H O7? XtffA%+d4tz,6@@Aw,~w K.wX-4w_L&5:'ot&8:C@ KO @GN3+02H%8:3,4C@^J q!w,-z|+ 4$43,4RX&C)57"/&1023+"%57,&XFC'&1&1?&45:';nH/&3+02R2$ fifi 9 d/+'11e}+'n+sL,eeFXds+ f]+ ? Xt62GeFXt 7zX X6@ 5 A*q ,|2"@ *'wYzw1*\^>a57"%$@ ]O q(+*,*+-z| <`$'6z6H%5:';6,i^/&RX"%57,&H5:[6,&10/FH%l'iGH%RX0257?4"%576[?G8:3,445:'; !C`d/0c &4et24eFXd ,e}+ + + Xd%X a+K'e+a,eF%_`7+44tzA@X'% ^ tz8j # ,-@G?4? ,,n-*'w\^>a57"%$@ ]O @&8:C@ ]O q!w,,| JA&>?6,023,8ff?G8:3,445:';=EQ57"%$[>]l'"%l43,8ff&X'R18:l4H%5760%&3,H/645:'; !`d/"c 5 +et2GeFXd ,e}+ +B'+tGe + Xd%X Z+GYdXe} ff1+O2GeFXt 7zXjh ((@A?4?,(,]Z,,JA025:4zl43+0%"1@M_ q(+*,*| 43,87~z5:';0%&3,R2$43+iG5:8:57"F~[EQ57"%$45:[?G8:3,WH/?G3,RX& ! # -4 e+d/++'1+dXe} ^n@ ?4? w(,(].w(,M3,87"/6,0%"%3^O@ q!w9 -z| 0%&H%l487"P6#"%$'&RX6>?Gl'"%3+"%57643,8MRX6>?G87&X'57"F~.6,oj$'&l'025:H/"%5:R]&H/"%5:>a3+"/&HKot6,0"%$'&1X C@ 4<'@43- KZ,3,87;,6,0257"%$4> 2 1+da,e}+&10%o}3,5:8:8:57&,@4 @'&>a3,57"/0%&,@ @4\^R2$457&X@'J q!w,,| _Ql4H%H%5:3,C'68:8H/&3+02R2$)ot6,0LH/687^5:';nRX64H/"/023,5:"6,?4"%5:>a573+"%576?40%6,iG87&>aH !`d//c 4etGK,e}+ + + Xd%X =+ YdXe} ff2+2GeFXt 7zXjh (5@4?4? 5w ^wP.5wP5:C43,8@ff @L Y&X'&10@L q(+*,*+-z| ff023,4R2$45:';[3,4CS?402l445:';'I S6,?4"%5:>a3,8Y"/&>?6,023,8P9`<`?G8:3,4'&10iG3,H/&CV6SRX64H/"/023,5:"?40%6,;,023,>a>a5:'; !`d/Dc (,etK,e}+ + + Xd%X #+YdXe} ff1+S 2GeFXt 7zX _jh ,-@4?4? *nZ 95}fiJournal Artificial Intelligence Research 25 (2006) 457-502Submitted 11/05; published 4/06Continuation Method Nash Equilibria StructuredGamesBen Blumbblum@cs.berkeley.eduUniversity California, BerkeleyDepartment Electrical Engineering Computer ScienceBerkeley, CA 94720Christian R. Sheltoncshelton@cs.ucr.eduUniversity California, RiversideDepartment Computer Science EngineeringRiverside, CA 92521Daphne Kollerkoller@cs.stanford.eduStanford UniversityDepartment Computer ScienceStanford, CA 94305AbstractStructured game representations recently attracted interest models multiagent artificial intelligence scenarios, rational behavior commonly characterizedNash equilibria. paper presents efficient, exact algorithms computing Nash equilibria structured game representations, including graphical games multi-agentinfluence diagrams (MAIDs). algorithms derived continuation methodnormal-form extensive-form games due Govindan Wilson; follow trajectory space perturbed games equilibria, exploiting game structurefast computation Jacobian payoff function. theoreticallyguaranteed find least one equilibrium game, may find more. approachprovides first efficient algorithm computing exact equilibria graphical gamesarbitrary topology, first algorithm exploit fine-grained structural propertiesMAIDs. Experimental results presented demonstrating effectiveness algorithms comparing predecessors. running time graphical gamealgorithm similar to, often better than, running time previous approximatealgorithms. algorithm MAIDs effectively solve games much largersolvable previous methods.1. Introductionattempting reason interactions multiple agents, artificial intelligencecommunity recently developed interest game theory, tool economics. Gametheory general mathematical formalism representation complex multiagent scenarios, called games, agents choose actions receive payoffsdepend outcome game. number new game representationsintroduced past years exploit structure represent games efficiently.representations inspired graphical models probabilistic reasoningartificial intelligence literature, include graphical games (Kearns, Littman, & Singh,c2006AI Access Foundation. rights reserved.fiBlum, Shelton, & Koller2001), multi-agent influence diagrams (MAIDs) (Koller & Milch, 2001), G nets (La Mura,2000), action-graph games (Bhat & Leyton-Brown, 2004).goal describe rational behavior game. game theory, descriptionbehavior agents game referred strategy profile: joint assignmentstrategies agent. basic criterion look strategy profileoptimal agent, taken individually: agent able improve utilitychanging strategy. fundamental game theoretic notion Nash equilibrium (Nash,1951) satisfies criterion precisely. Nash equilibrium strategy profileagent improve payoff deviating unilaterally changing strategyagents hold fixed. types game theoretic solutions,Nash equilibrium fundamental often agreed minimum solutionrequirement.Computing equilibria difficult several reasons. First, game representationsgrow quite large. However, many games would interestedsolving require full generality description leads large representationsize. structured game representations introduced AI exploit structural propertiesgames represent compactly. Typically, structure involves localityinteraction agents concerned behavior subset agents.One would hope compact representations might lead efficient computation equilibria would possible standard game-theoretic solution algorithms(such described McKelvey & McLennan, 1996). Unfortunately, even compact representations, games quite hard solve; present result showing findingNash equilibria beyond single trivial one NP-hard types structured gamesconsider.paper, describe set algorithms computing equilibria structuredgames perform quite well, empirically. algorithms family continuationmethods. begin solution trivial perturbed game, track solutionperturbation incrementally undone, following trajectory space equilibriaperturbed games equilibrium original game found. algorithmsbased recent work Govindan Wilson (2002, 2003, 2004) (GW hereafter),applies standard game representations (normal-form extensive-form).algorithms GW great interest computational game theory communityright; Nudelman et al. (2004) tested leading algorithmsfound them, certain cases, effective available. However,algorithms unstructured games, infeasible large games.show game structure exploited perform key computational stepalgorithms GW, also give alternative presentation work.methods address graphical games MAIDs. Several recent paperspresented methods finding equilibria graphical games. Many proposed algorithms (Kearns et al., 2001; Littman, Kearns, & Singh, 2002; Vickrey & Koller, 2002; Ortiz& Kearns, 2003) focused finding approximate equilibria, agent mayfact small incentive deviate. sorts algorithms problematic:approximations must crude reasonable running times, guaranteeexact equilibrium neighborhood approximate one. Algorithms findexact equilibria restricted narrow class games (Kearns et al., 2001).458fiA Continuation Method Nash Equilibria Structured Gamespresent first efficient algorithm finding exact equilibria graphical games arbitrary structure. present experimental results showing running timealgorithm similar to, often better than, running time previous approximatealgorithms. Moreover, algorithm capable using approximate algorithms startingpoints finding exact equilibria.literature MAIDs limited. algorithm Koller Milch (2001)takes advantage certain coarse-grained structure MAIDs, otherwise fallsback generating solving standard extensive-form games. Methods related typesstructured games (La Mura, 2000) also limited coarse-grained structure,currently unimplemented. Approximate approaches MAIDs (Vickrey, 2002) comewithout implementation details timing results. provide first exact algorithmtake advantage fine-grained structure MAIDs. present experimental resultsdemonstrating algorithm solve MAIDs significantly outside scopeprevious methods.1.1 Outline Guide Background Materialresults require background several distinct areas, including game theory, continuationmethods, representations graphical games, representation inference Bayesiannetworks. Clearly, outside scope paper provide detailed reviewtopics. attempted provide, topics, sufficient backgroundallow results understood.begin overview game theory Section 2, describing strategy representations payoffs normal-form games (single-move games) extensive-formgames (games multiple moves time). concepts utilized paperpresented section, thorough treatment available standardtext Fudenberg Tirole (1991). Section 3 introduce two structured gamerepresentations addressed paper: graphical games (derived normal-form games)MAIDs (derived extensive-form games). Section 4 give result complexity computing equilibria graphical games MAIDs, proof deferredAppendix B. next outline continuation methods, general scheme algorithmsuse compute equilibria, Section 5. Continuation methods form broad computationalframework, presentation therefore necessarily limited scope; Watson (2000)provides thorough grounding. Section 6 describe particulars applyingcontinuation methods normal-form games extensive-form games. presentationnew, methods exactly GW.Section 7, present main contribution: exploiting structure performalgorithms GW efficiently graphical games MAIDs. show Bayesiannetwork inference MAIDs used perform key computational step GWalgorithm efficiently, taking advantage finer-grained structure previously possible.algorithm utilizes, subroutine, clique tree inference algorithm Bayesiannetworks. Although present clique tree method full, describeproperties method allow used within algorithm; also provideenough detail allow implementation algorithm using standard clique treepackage black box. comprehensive introduction inference Bayesian459fiBlum, Shelton, & Kollernetworks, refer reader reference Cowell, Dawid, Lauritzen, Spiegelhalter(1999). Section 8, present running-time results variety graphical gamesMAIDs. conclude Section 9.2. Game Theorybegin briefly reviewing concepts game theory used paper, referringtext Fudenberg Tirole (1991) good introduction. use notationemployed GW. readers familiar game theory may wish skip directlytable notation Appendix A.game defines interaction set N = {n1 , n2 , . . . , n|N | } agents. agentn N set n available strategies, strategy determines agents behaviorgame. precise definition set n dependsQ game representation,discuss below. strategy profile = (n1 , n2 , . . . , n|N | ) nN n defines strategyn n agent n N . Given strategy profile , game defines expectedpayoff Gn () agent n N . use n refer set strategy profilesagents N \ {n} (agents n) n n refer one profile;generalize notation n,n0 set strategy profiles two agents.strategy profile, n0 n strategy agent n, (n0 , n ) new strategyprofile n deviates play n0 , agents act according .solution game prescription strategy profile agents. paper,use Nash equilibria solution concept strategy profiles agentprofit deviating unilaterally. agent knew others playing accordingequilibrium profile (and would change behavior), would incentivedeviate. Using notation outlined here, define Nash equilibriumstrategy profile that, n N strategies n0 n ,Gn (n , n ) Gn (n0 , n ).also define notion approximate equilibrium, agents incentive deviate small. -equilibrium strategy profile agentimprove expected payoff unilaterally deviating .words, n N strategies n0 n , Gn (n0 , n ) Gn (n , n ) .Unfortunately, finding -equilibrium necessarily step toward finding exactequilibrium: fact -equilibrium guarantee existence exactequilibrium neighborhood .2.1 Normal-Form Gamesnormal-form game defines simultaneous-move multi-agent scenario. agent independently selects action receives payoff depends actions selectedagents. precisely, let G normal-form game set N agents.agent n N hasQdiscrete action set payoff array Gn entries everyaction profile = nN is, joint actions = (an1 , an2 , . . . , an|N | )agents. use refer joint actions agents N \ {n}.460fiA Continuation Method Nash Equilibria Structured Games2.1.1 Strategy Representationagents restricted choosing actions deterministically, equilibrium guaranteed exist. If, however, agents allowed independently randomize actions,seminal result game theory (Nash, 1951) guarantees existence mixed strategyequilibrium. mixed strategy n probability distribution .strategy set n therefore defined probability simplex mixedstrategies. support mixed strategy set actions non-zeroprobability. strategy n agent n said pure strategy singleaction support pure strategies correspondexactly deterministic actionsQ. set mixed strategy profiles nN n , product simplices. mixedstrategy single agent represented vector probabilities, oneaction. notational simplicity later on, concatenate allPthese vectors regardmixed strategy profile single m-vector, = nN |An |. vectorindexed actions nN , action , probability agentn plays action a. (Note that, notational convenience, every action associatedparticular agent; different agents cannot take action.)2.1.2 Payoffsmixed strategy profile induces joint distribution action profiles, computeexpectation payoffs respect distribution. let Gn () representexpected payoff agent n agents behave according strategy profile .calculate valueXGn () =Gn (a)ak .(1)aAkNgeneral case (a fully mixed strategy profile, every ), sum includesevery entry game array Gn , exponentially large number agents.2.2 Extensive-Form Gamesextensive-form game represented tree. game proceeds sequentiallyroot. non-leaf node tree corresponds choice either agentnature; outgoing branches represent possible actions taken node.natures choice nodes, game definition includes probability distributionoutgoing branches (these points game something happens randomlyworld large). leaf z Z tree outcome, associatedvector payoffs G(z), Gn (z) denotes payoff agent n leaf z. choicesagents nature dictate path tree followed.choice nodes belonging agent partitioned information sets;information set set states among agent cannot distinguish. Thus, agentsstrategy must dictate behavior nodes information set. setagent ns information sets denoted , set actions available informationset denoted A(i). define agent history Hn (y) node treeagent n sequence containing pairs (i, a) information sets belonging ntraversed path root (excluding information set461fiBlum, Shelton, & Koller0.7a20.80.7b2b10.6a1 a2(0, 2)0.9a10.3b10.20.10.41.0a3 a4(1, 4)(6, 7)0.3b20.00.5a5 a6(6, 0)(3, 3)0.5a7 a8(1, 7)(8, 0)(2, 6)Figure 1: simple 2-agent extensive-form game.contained), action selected n one. Since actions unique informationsets (the action cant taken two different information sets), also omitinformation sets represent history ordered tuple actions only. Two nodesagent-n history paths used reach indistinguishable n,although paths may differ ways, natures decisions decisionsagents. make common assumption perfect recall : agent forgetinformation known choices made previous decisions. precisely, two nodesy, 0 information set agent n, Hn (y) = Hn (y 0 ).Example 1. game tree shown Figure 1, two agents, Alice Bob. Alicefirst chooses actions a1 a2 , Bob next chooses b1 b2 , Alice choosestwo set {a01 , a02 , . . . , a08 } (which pair depends Bobs choice). Informationsets indicated nodes connected dashed lines. Bob unaware Alices actions,nodes information set. Alice aware bottom levelinitial action Bobs action, nodes distinct information set.Edges labeled probability agent whose action follow it;note actions taken nodes information set must probabilitydistribution associated them. eight possible outcomes game,labeled pair payoffs Alice Bob, respectively.2.2.1 Strategy RepresentationUnlike case normal-form games, several quite different choices strategyrepresentation extensive-form games. One convenient formulation terms behaviorstrategies. behavior profile b assigns information set distribution462fiA Continuation Method Nash Equilibria Structured Gamesactions A(i). probability agent n takes action information setwritten b(a|i). node i, also write b(a|y) abbreviationb(a|i).methods primarily employ variant sequence form representation (Koller &Megiddo, 1992; von Stengel, 1996; Romanovskii, 1962), built upon behaviorstrategy representation. sequence form, strategy n agent n representedrealization plan, vector real values. value, realization probability,realization plan corresponds distinct history (or sequence) Hn (y) agent n has,nodes game tree. sequences may partial records nsbehavior game proper prefixes larger sequences. strategy representationemployed GW (and ourselves) equivalent sequence form representationrestricted terminal sequences: agent-n histories least one leaf node.shall henceforth refer modified strategy representation simply sequence form,sake simplicity.agent n, then, consider realization plan n vector realizationprobabilities terminal sequences. outcome z, (Hn (z)), abbreviated n (z),probability agent ns choices allow realization outcome zQ words,product agent ns behavior probabilities along history Hn (z), (i,a)Hn (z) b(a|i).Several different outcomes may associated terminal sequence,agent n may fewer realization probabilities leaves tree. setrealization plans agent n therefore subset IR`n , `n , number distinctterminal sequences agent n, number leaves tree.Example 2. example above, Alice eight terminal sequences, onea01 , a02 , . . . , a08 four information sets bottom level. history onelast action (a1 , a03 ). realization probability (a1 , a03 ) equal b(a1 )b(a03 |a1 , b2 ) =0.1 0.6 = 0.06. Bob two last actions, whose realization probabilities exactlybehavior probabilities.realization probabilities non-zero, realization plans behavior strategiesone-to-one correspondence. (When probabilities zero, many possible behavior strategy profiles might correspond realization plan, described Koller& Megiddo, 1992; affect work presented here.)Q behavior strategyprofile b, easily calculate realization probability n (z) = (i,a)Hn (z) b(a|i).understand reverse transformation, note also map behavior strategiesfull realization plans defined non-terminal sequences(as originally definedQKoller & Megiddo, 1992) defining n (h) = (i,a)h b(a|i); intuitively, n (h)probability agent ns choices allow realization partial sequence h. Usingobservation, compute behavior strategy extended realization plan: (partial) sequence (h, a) extends sequence h one action, namely action information setbelonging agent n, compute b(a|i) = nn(h,a)(h) . extended realizationprobabilities computed terminal realization probabilities recursiveprocedure starting leaves tree working upward: atPinformation setagent-n history h (determined uniquely perfect recall), n (h) = aA(i) n (h, a).several different information sets agent-n history h, n (h)computed multiple ways. order (terminal) realization plan valid,463fiBlum, Shelton, & Kollermust satisfy constraint choices information sets agent-n history h mustgive rise value n (h). formally, partial sequence h,P constraintsP pairs information sets i1 i2 Hn (i1 ) = Hn (i2 ) = h,(h,a)=aA(i1 ) naA(i2 ) n (h, a). game tree Example 1, consider Alicesrealization probability (a1 ). expressed either (a1 , a01 ) + (a1 , a02 ) =0.1 0.2 + 0.1 0.8 (a1 , a03 ) + (a1 , a04 ) = 0.1 0.6 + 0.1 0.4, two sums mustsame.recursively defining realization probability sum realization probabilities longer sequences, constraints expressed terms terminal realizationprobabilities; fact, constraints linear probabilities. severalconstraints: probabilities must nonnegative, and, agent n, n () = 1,(the empty sequence) agent-n history first information set agent nencounters. latter constraint simply enforces probabilities sum one. Together,linear constraints define convex polytope legal terminal realization plans.2.2.2 Payoffsagents play according , payoff agent n extensive-form gameXGn () =Gn (z)k (z) ,(2)zZkNaugmented N include nature notational convenience.simply expected sum payoffs leaves. agentQ k, k (z)product probabilities controlled n along path z; thus, kN k (z)multiplication probabilities along path z, precisely probabilityz occurring. Importantly, expression similar multi-linear form payoffnormal-form game, using realization plans rather mixed strategies.Extensive-form games expressed (inefficiently) normal-form games,guaranteed equilibrium mixed strategies. extensive-form gamesatisfying perfect recall, mixed strategy profile represented payoff-equivalentbehavior profile, hence realization plan (Kuhn, 1953).3. Structured Game Representationsartificial intelligence community recently introduced structured representationsexploit independence relations games order represent compactly.methods address two representations: graphical games (Kearns et al., 2001),structured class normal-form games, MAIDs (Koller & Milch, 2001), structuredclass extensive-form games.3.1 Graphical Gamessize payoff arrays required describe normal-form game grows exponentiallynumber agents. order avoid blow-up, Kearns et al. (2001) introducedframework graphical games, structured representation inspired probabilistic graphical models. Graphical games capture local structure multi-agent interactions,464fiA Continuation Method Nash Equilibria Structured Gamesallowing compact representation scenarios agents payoff affectedsmall subset agents. Examples interactions structure occurs include agents interact along organization hierarchies agents interact accordinggeographic proximity.graphical game similar definition normal-form game, representationaugmented inclusion interaction graph node agent.original definition assumed undirected graph, easily generalizes directed graphs.edge agent n0 agent n graph indicates agent ns payoffs dependaction agent n0 . precisely, define Famn set agents consistingn parents graph. Agent ns payoff function Gn array indexedactions agents Famn . Thus, description game exponentialin-degree graph total number agents. case, usefn Afn refer strategy profiles action profiles, respectively, agentsFamn \ {n}.Example 3. Suppose 2L landowners along road running north south decidingwhether build factory, residential neighborhood, shopping mall plots.plots laid along road 2-by-L grid; half agents east side(e1 , . . . , eL ) half west side (w1 , . . . , wL ). agents payoff dependsbuilds neighbors north, south, across road build.example, agent wants build residential neighborhood next factory. agentspayoff matrix indexed actions four agents (fewer ends road)34 entries, opposed full 32L entries required equivalent normal formgame. (This example due Vickrey & Koller, 2002.)3.2 Multi-Agent Influence Diagramsdescription length extensive-form games also grow exponentially number agents. many situations, large tree represented compactly.Multi-agent influence diagrams (MAIDs) (Koller & Milch, 2001) allow structured representation games involving time information extending influence diagrams (Howard& Matheson, 1984) multi-agent case.MAIDs influence diagrams derive much syntax semanticsBayesian network framework. MAID compactly represents certain type extensiveform game much way Bayesian network compactly represents jointprobability distribution. thorough treatment Bayesian networks, referreader reference Cowell et al. (1999).3.2.1 MAID RepresentationLike Bayesian network, MAID defines directed acyclic graph whose nodes correspondrandom variables. random variables partitioned sets: set X chancevariables whose values chosen nature, represented graph ovals;agent n, set Dn decision variables whose values chosen agent n, representedrectangles; agent n, set Un utility variables, represented diamonds.Chance decision variables have, domains, finite sets possible actions.refer domain random variable V dom(V ). chance decision variable465fiBlum, Shelton, & KollerV , graph defines parent set PaV variables whose values choice Vdepend. Utility variables finite sets real payoff values domains,permitted children graph; represent components agentspayoffs, game state.game definition supplies chance variable X conditional probabilitydistribution (CPD) P (X|PaX ), conditioned values parent variables X.semantics chance variable identical semantics random variableBayesian network; CPD specifies probability action dom(X)selected nature, given actions taken Xs parents. game definition also suppliesutility function utility node U . utility function maps instantiationpa dom(PaU ) deterministically real value U (pa). notational algorithmicconvenience, regard utility function CPD P (U |PaU ) which,pa dom(PaU ), value U (pa) probability 1 P (U |pa) valuesprobability 0 (the domain U simply finite set possible utility values).end game, agent ns total payoff sum utility received Uni Un(here index variable). Note component Uni agent ns payoff dependssubset variables MAID; idea compactly decompose payoffadditive pieces.3.2.2 Strategy Representationcounterpart CPD decision node decision rule. decision ruledecision variable Dni Dn function, specified n, mapping instantiationpa dom(PaDni ) probability distribution possible actions dom(Dni ).decision rule identical form conditional probability distribution, referusing notation P (Dni |PaDni ). semantics chance node, decisionrule specifies probability agent n take particular action dom(Dni ),seen actions taken Dni parents. assignment decision rules Dni Dncomprises strategy agent n. agent n chooses strategy, ns behavior Dnidepends actions taken Dni parents. PaDni therefore regardedset nodes whose values visible n makes choice Dni . Agent ns choicestrategy may well take nodes account; actual game play, nodesexcept PaDni invisible n.Example 4. extensive-form game considered Example 1 representedMAID shown Figure 2(a). Alice Bob initial decision make withoutinformation previous actions; Alice another decision makeaware Bobs action own. Alice Bob one utility node(the two condensed single node graph, sake brevity), whose payoffstructure wholly general (dependent every action game) thus whose possiblevalues exactly values payoff vectors extensive-form game.Example 5. Figure 2(b) shows complicated MAID somewhat realisticscenario. Here, three landowners along road deciding whether build storehouse. payoff depends happens adjacent along road.decision proceeds two stages: planning stage building stage. second466fiA Continuation Method Nash Equilibria Structured GamesP1P2BP3E1E2C1C2B1ABC3B2R1L2(a)B3R2L3(b)Figure 2: (a) simple MAID equivalent extensive form game Figure 1. (b)two-stage road game three agents.landowner, instance, two decision variables P2 B2 . receives certainpenalty utility node C2 builds opposite planned build.planning, learns something neighbor left planned.chance node E1 represents noisy espionage; transmits action taken P1 .learning value E1 , may second landowners interests deviateplan, even means incurring penalty. interest start trenddistinguishes previous builders subsequent builders follow: utilitynode L2 rewards building opposite built B1 , utility nodeR2 rewards third landowner builds thing B3 .Note MAID exhibits perfect recall, choice made planning stagevisible agent makes next choice building stage.3.2.3 Payoffsparticular strategy profile is, tuple strategies playersdecision nodes CPDs specified. Since chance utility nodes endowed CPDsalready, MAID therefore induces fully-specified Bayesian network B variablesV = X U directed graph MAID. chain rule Bayesiannetworks,QB induces joint probability distribution P variables VP (V) = V V P (V |PaV ), CPDs chance utility variables given MAIDdefinition CPDs decision variables given . game G representedMAID, expected payoff agent n receives expectation ns utilitynode values respect distribution:XGn () =EP [Uni ]Uni Un=XXUni Un udom(Uni )467u P (u).fiBlum, Shelton, & Kollershow Section 7 related expectations calculated efficientlyusing Bayesian network inference algorithms, giving substantial performance increasecalculation payoffs extensive-form game.3.2.4 Extensive Form Strategy Representations MAIDsMAID provides compact definition extensive-form game. note that, althoughcorrespondence MAIDs extensive form games provides intuitionMAIDs, details mapping relevant remainder discussion.therefore briefly review construction, referring work Koller Milch(2001) details.game tree associated MAID full, balanced tree, path corresponding complete assignment chance decision nodes network.node tree corresponds either chance node decision node oneplayers, outgoing branch possible action node. nodesdepth tree correspond MAID node. assume nodesalong path tree ordered consistently ordering implied directededges MAID, MAID node X parent MAID node , treebranches X branches . information sets tree nodes associateddecision node Dni correspond assignments parents PaDni : tree nodescorresponding Dni assignment PaDni single information set.note that, construction, assignment PaDni determined earlier tree,partition information sets well-defined. example, simple MAIDFigure 2(a) expands much larger game tree saw earlier Figure 1.Translating opposite direction, extensive-form games MAIDs,always natural. game tree unbalanced, cannot simply reverseprocess. However, care, possible construct MAID largergiven extensive-form game, may exponentially smaller number agents.details fairly technical, omit interest brevity.Despite fact MAID typically much compact equivalentextensive-form game, strategy representations two turn equivalentequal size. decision rule decision variable Dni assigns distribution actionsjoint assignment PaDni , behavior strategy assigns distribution actionsinformation set extensive form game discussed above, assignmentparents Dni information set. strategy profile MAID set decisionrules every decision variable therefore equivalent set behavior strategiesevery information set, simply behavior profile.make assumption perfect recall, then, since MAID strategies simplybehavior strategies, represent sequence form. Perfect recall requiresagent forget anything learned course game. MAIDformalism, perfect recall assumption equivalent following constraint: agentn two decision nodes Dni Dnj , second occurring first,parents Dni (the information n aware making decision Dni ) Dni mustparents Dnj . implies agent ns final decision node Dnd has, parents, nsprevious decision nodes parents. joint assignment Dnd PaDnd precisely468fiA Continuation Method Nash Equilibria Structured Gamesdetermines agent ns sequence information sets actions leading outcomegame agent-n history outcome.realization probability particular sequence computed multiplyingbehavior strategy probabilities actions sequence. MAIDs, sequence corresponds joint assignment Dnd PaDnd , behavior strategy probabilitiessequence entries consistent assignment decision rules agent n.therefore derive agent ns realization probabilities multiplyingtogether, conditional probability distributions, decision rules agent ns decision nodes sequence multiplying conditional probability distributions,entries whose assignments consistent multiplied. Conversely,given realization plan, derive behavior strategies hence decision rulesaccording method outlined extensive-form games.simple MAID example Figure 2(a), terminal sequencesequivalent extensive-form game. road example Figure 2(b), agent 2 8terminal sequences; one joint assignment final decision node (B2 )parents (E1 P2 ). associated realization probabilities given multiplyingdecision rules P2 B2 .4. Computational Complexitydeveloping algorithms compute equilibria efficiently, question naturally ariseswell one expect algorithms perform. complexity computingNash equilibria studied time. Gilboa Zemel (1989) first showedNP-hard find one Nash equilibrium normal-form game,Conitzer Sandholm (2003) recently utilized simpler reduction arrive resultseveral others vein. recent hardness results pertain restrictedsubclasses normal-form games (e.g., Chu & Halpern, 2001; Codenotti & Stefankovic,2005). However, results apply 2-agent normal-form games. trueproving certain subclass class problems NP-hard also proves entireclass NP-hard (because NP-hardness measure worst-case complexity),proof might tell us little complexity problems outside subclass.issue particularly apparent problem computing equilibria, gamesgrow along two distinct axes: number agents, number actions per agent.hardness results Conitzer Sandholm (2003) apply number actionsper agent increases. 2-agent normal-form games (fully connected) graphicalgames, results apply graphical games.However, interested hardness graphical games numberagents increases, rather number actions per agent. graphical gameslarge numbers agents capture structure gamesgraphical game representation designed. order prove resultsasymptotic hardness computing equilibria along interesting (in setting)axis representation size, require different reduction. proof, like numberprevious hardness proofs games (e.g., Chu & Halpern, 2001; Conitzer & Sandholm, 2003;Codenotti & Stefankovic, 2005), reduces 3SAT equilibrium computation. However,previous proofs, variables 3SAT instances mapped actions (or sets actions)469fiBlum, Shelton, & Kollergame 2 players, whereas reduction mapped agents. Althoughdiffering approach, reduction much spirit reduction appearingwork Conitzer Sandholm (2003), many corollaries main resultalso follow (in form adapted graphical games).Theorem 6. constant 5, k 2, problem deciding whethergraphical game family size k actions per playerone Nash equilibrium NP-hard.Proof. Deferred Appendix B.reduction, games one equilibria least one purestrategy equilibrium. immediately gives usCorollary 7. NP-hard determine whether graphical game one Nashequilibrium discretized strategies even coarsest possible granularity.Finally, graphical games represented (trivial) MAIDs,agent single parentless decision node single utility node, agentsutility node has, parents, decision nodes graphical game family agent,obtain following corollary.Corollary 8. NP-hard determine whether MAID constant family size least6 one Nash equilibrium.5. Continuation MethodsContinuation methods form basis algorithms solving structured game representations. begin high-level overview continuation methods,referring reader work Watson (2000) detailed discussion.Continuation methods work solving simpler perturbed problem tracingsolution magnitude perturbation decreases, converging solutionoriginal problem. precisely, let scalar parameterizing continuumperturbed problems. = 0, perturbed problem original one; = 1,perturbed problem one solution known. Let w represent vectorreal values solution. perturbed problem defined , characterizesolutions equation F (w, ) = 0, F real-valued vector functiondimension w (so 0 vector zeros). function F w solutionproblem perturbed F (w, ) = 0.continuation method traces solutions along level set solution pairs (w, )satisfying F (w, ) = 0. Specifically, solution pair (w, ), would like tracesolution nearby solution. Differential changes w must cancelF remains equal 0.(w, ) changes direction unit vector u, F changedirectionF u, F Jacobian F (which also written w F F ).want find direction u F remains unchanged, i.e., equal 0. Thus, needsolve matrix equationdww F F=0.(3)470fiA Continuation Method Nash Equilibria Structured GamesEquivalently, changes dw along path must obey w F dw = F d. Ratherinverting matrix w F solving equation, use adjoint adj(w F ),still defined w F null space rank 1. adjoint matrix cofactors:element (i, j) (1)i+j times determinant sub-matrix rowcolumn j removed. inverse defined, adj(w F ) = det(w F )[w F ]1 .practice, therefore set dw = adj(w F ) F = det(w F ). Jacobian[w F F ] null-space rank 1 everywhere, curve uniquely defined.function F constructed curve starting = 1 guaranteedcross = 0, point corresponding value w solution originalproblem. continuation method begins known solution = 1 . null-spaceJacobian F current solution (w, ) defines direction, along solutionmoved small amount. Jacobian recalculated process repeats,tracing curve = 0. cost step computation least cubicsize w, due required matrix operations. However, Jacobian maygeneral much difficult compute. Watson (2000) provides simple examplescontinuation methods.6. Continuation Methods Gamesreview work GW applying continuation method task finding equilibria games. provide continuation methods normal-formextensive-form games. algorithms form basis extension structuredgames, described next section. continuation methods perturb game giving agents fixed bonuses, scaled , actions, independently whateverelse happens game. bonuses large enough (and unique), dominateoriginal game structure, agents need consider opponents actions.thus unique pure-strategy equilibrium easily determined bonuses = 1.continuation method used follow path space equilibriumprofiles resulting perturbed game, decreasing zero; point,corresponding strategy profile equilibrium original game.6.1 Continuation Method Normal-Form Gamesmake intuition precise, beginning normal-form games.6.1.1 Perturbationsperturbation vector b vector values chosen random, one actiongame. bonus ba given agent n owning action playing a, independentlywhatever else happens game. Applying perturbation target game G givesus new game, denote G b, which, , ,(G b)n (a, t) = Gn (a, t) + ba . b made sufficiently large, G b uniqueequilibrium, agent plays pure strategy ba maximal.471fiBlum, Shelton, & Koller6.1.2 Characterization Equilibriaorder apply Equation (3), need characterize equilibria perturbed gameszeros function F . Using structure theorem Kohlberg Mertens (1986), GWshow continuation method path deriving equilibrium characterizationleads convergence perturbation vectors except set measure zero.present equilibrium characterization here; proofs characterizationmethods convergence given Govindan Wilson (2003).first define auxiliary vector function V G (), indexed actions, payoffsagent deviating play single action. call V G deviation function.element VaG () corresponding single action a, owned agent n, payoffagent n deviates mixed strategy profile playing pure strategyaction a:X(4)VaG () =Gn (a, t)tk .tAnkN \{n}also viewed component agent ns payoff derives action a,strategy profile . Since bonuses given actions independently ,effect bonuses V G independent . VaG measures payoff deviatingplaying a, bonuses given precisely deviation, V Gb () = V G () + b.also utilize retraction operator R : IRm defined Gul, Pearce, Stachetti (1993), maps arbitrary m-vector w point space mixedstrategies nearest w Euclidean distance. Given operator, equilibriumcharacterization follows.Lemma 9. (Gul et al., 1993) strategy profile G, = R(V G () + ) iffequilibrium.Although omit proof, give intuition result true.Suppose fully-mixed equilibrium; is, every action non-zero probability.single agent n, VaG () must actions , nincentive deviate play single one them. Let Vn vector entriesV G () corresponding actions n, let n defined similarly. Vn scalar multiple1, all-ones vector, simplex n ns mixed strategies defined 1T x = 1,Vn orthogonal n . V G () therefore orthogonal , retracting + V G () ontogives precisely . reverse direction, fully-mixed strategy profile satisfying= R(V G () + ), V G () must orthogonal polytope mixed strategies.Then, agent, every pure strategy payoff. Therefore, factequilibrium. little care must taken dealing actions support.refer Gul et al. (1993) details.According Lemma 9, define equilibrium solution equation= R( + V G ()). hand, = R(w) w IRm ,equivalent condition w = R(w) + V G (R(w)); equilibrium iff conditionsatisfied, easily verified. therefore search point w IRmsatisfies equality, case R(w) guaranteed equilibrium.form continuation equationF (w, ) = w R(w) V G (R (w)) + b .(5)472fiA Continuation Method Nash Equilibria Structured GamesV G + b deviation function perturbed game G b, F (w, )zero R(w) equilibrium G b. = 0 game unperturbed,F (w, 0) = 0 iff R(w) equilibrium G.6.1.3 Computationexpensive step continuation method calculation Jacobian w F ,required computation maintains constraint Equation (3). Here,w F = (I + V G )R, identity matrix. hard partcalculation V G . pure strategies a0 An0 , n0 6= n, valuelocation (a, a0 ) V G () equal expected payoff agent n playspure strategy a, agent n0 plays pure strategy a0 , agents act accordingstrategy profile :XGn (a, t)tka0tAnkN \{n}X0=Gn (a, , t)tk .GVa,a0 () =tAn,n0(6)kN \{n,n0 }G () = 0.a0 , Va,a0Computing Equation(6) requires large number multiplications; sumQspace An,n0 = kN \{n,n0 } Ai , exponentially large number agents.6.2 Continuation Method Extensive-Form Gamesmethod applies extensive-form games, using sequence form strategy representation.6.2.1 Perturbationsnormal-form games, game perturbed bonus vector b. Agent n owningsequence h paid additional bonus bh playing h, independently whatever elsehappens game. Applying perturbation gives us new game G b which,z Z, (G b)n (z) = Gn (z) + bHn (z) .bonuses large enough unique, GW show perturbedgame unique pure-strategy equilibrium (one realization probabilities0 1). However, calculating simple case normal-form games.Behavior strategies must calculated leaves upward recursive procedure,step agent owns node question chooses action resultssequence largest bonus. Since actions recursivelydetermined, action node question determines outcome. realizationplans derived behavior profile method outlined Section 2.2.1.473fiBlum, Shelton, & Koller6.2.2 Characterization Equilibriamore, first define vector function capturing benefit deviating givenstrategy profile, indexed sequences:Xk (z),(7)VhG () =Gn (z)zZhkN \{n}Zh set leaves consistent sequence h. interpretationV G natural case normal-form games, possible agentplay one sequence exclusion others; possible actions partiallydetermined actions agents. case, VhG () regardedportion payoff agent n receives playing sequence h, unscaled agent nsprobability playing sequence. normal-form games, vector bonusesadded directly V G , V Gb = V G + b.retraction operator R realization plans defined way normalform strategies: takes general vector projects onto nearest point validregion realization plans. constraints defining space linear, discussedSection 2.2.1 . therefore express constraint matrix C C = 0valid profiles . addition, probabilities must greater equal zero.calculate w, must find minimizing (w )T (w ), (squared) Euclidean distancew , subject C = 0 0. quadratic program (QP),solved efficiently using standard methods. Jacobian retraction easilycomputable set active constraints.equilibrium characterization realization plans surprisingly similarmixed strategies normal-form games; GW show that, before, equilibriacharacterized = R( + V G ()), R retraction sequence formV G deviation function. continuation equation F takes exactly formwell.6.2.3 Computationkey property reduced sequence-form strategy representation deviation function multi-linear function extensive-form parameters, shownEquation (7). elements Jacobian V G thus also general structure. particular, element corresponding sequence h agent n sequence h0agent n0XGn (z)k (z)h0zZhkN \{n}X=Gn (z)k (z)GVh,h0 () =zZh,h0(8)kN \{n,n0 }Zh,h0 set leaves consistent sequences h (for agent n)h0 (for agent n0 ). Zh,h0 empty set (and hence V G = 0) h h0 incompatible.Equation (8) precisely analogous Equation (6) normal-form games. sumoutcomes utility outcome multiplied strategy probabilities474fiA Continuation Method Nash Equilibria Structured Games321Figure 3: abstract diagram path. horizontal axis represents verticalaxis represents space strategy profiles (actually multidimensional).algorithm starts right = 1 follows dynamical system= 0 point 1, found equilibrium original game.continue trace path find equilibria labeled 2 3.agents. Note sum leaves tree, may exponentiallynumerous number agents.One additional subtlety, must addressed method equilibrium computation extensive-form games, relates zero-probability actions. actions induceprobability zero entire trajectories tree, possibly leading equilibria basedunrealizable threats. Additionally, information sets occur zero probability,agents behave arbitrarily without disturbing equilibrium criterion, resulting continuum equilibria possible bifurcation continuation path. preventsmethods converging. therefore constrain realization probabilities greaterequal small > 0. is, fact, requirement GWs equilibriumcharacterization hold. algorithm thus looks -perfect equilibrium (Fudenberg& Tirole, 1991): strategy profile component constrained ,agents strategy best response among satisfying constraint. Noteentirely different -equilibrium. -perfect equilibrium always exists,long large make set legal strategies empty. -perfect equilibrium interpreted equilibrium perturbed game agents smallprobability choosing unintended action. limit -perfect equilibria approaches0 perfect equilibrium (Fudenberg & Tirole, 1991): refinement basic notionNash equilibrium. approaches 0, equilibria found GWs algorithm thereforeconverge exact perfect equilibrium, continuity variation continuationmethod path. small enough, perfect equilibrium vicinityfound -perfect equilibrium, easily found local search.475fiBlum, Shelton, & Koller6.3 Path Propertiescase normal-form games, GW show, using structure theorem KohlbergMertens (1986), path algorithm one-manifold without boundaryprobability one choices b. provide analogous structure theoremguarantees property extensive-form games. Figure 3(a) shows abstractrepresentation path followed continuation method. GW show pathmust cross = 0 hyperplane least once, yielding equilibrium. fact, pathmay cross multiple times, yielding many equilibria single run. path musteventually continue = side, find odd number equilibria runcompletion.normal-form extensive-form games, path piece-wise polynomial,piece corresponding different support set strategy profile. piecescalled support cells. path smooth cell boundaries due discontinuitiesJacobian retraction operator, hence w F , support changes. Caremust taken step boundaries exactly following path; point,Jacobian new support calculated path tracednew support cell.case two agents, path piece-wise linear and, rather taking steps,algorithm jump corner corner along path. algorithm appliedtwo-agent game particular bonus vector used (in single entry nonzero), steps support cell support cell algorithm takes identicalpivots Lemke-Howson algorithm (Lemke & Howson, 1964) two-agent generalsum games, two algorithms find precisely set solutions (Govindan &Wilson, 2002). Thus, continuation method strict generalization LemkeHowson algorithm allows different perturbation rays games twoagents.process described detail pseudo-code algorithm, presentedFigure 4.6.4 Computational IssuesGuarantees convergence apply long stay path defined dynamical system continuation method. However, computational purposes, discretesteps must taken. result, error inevitably accumulates path traced,F becomes slightly non-zero. GW use several simple techniques combat problem.adopt techniques, introduce one own: employ adaptive stepsize, taking smaller steps error accumulates quickly larger ones not.F nearly linear (as is, example, actions supportcurrent strategy profile), technique speeds computation significantly.GW use two different techniques remove error accumulated. Supposepoint (w, ) wish minimize magnitude F (w, ) = w V G (R(w)) +b+R(w). two values might change: w, b. change first withoutaffecting guarantee convergence, every steps run local Newton methodsearch w minimizing |F (w, )|. search decrease error sufficiently,perform GW call wobble: change perturbation vector (wobble476fiA Continuation Method Nash Equilibria Structured Gamescontinuation path) make current solution consistent. set b = [w V G (R(w))R(w)]/, equilibrium characterization equation immediately satisfied. Changingperturbation vector invalidates theoretical guarantees convergence. However,nonetheless attractive option immediately reduces error zero.local Newton method wobbles described detail GovindanWilson (2003).techniques potentially send algorithm cycle, practiceoccasionally do. However, necessary keeping algorithm path.algorithm cycles, random restarts decrease step size improve convergence.sophisticated path-following algorithms might also used, general could improvesuccess rate execution time algorithm.6.5 Iterated Polymatrix Approximationperturbed games may large number equilibria, pathmay wind back forth number them, continuation algorithmtake trace way back solution original game. speedalgorithm using initialization procedure based iterated polymatrix approximation(IPA) algorithm GW. polymatrix game normal-form game payoffsagent n equal sum payoffs set two-agent games, involvingn another agent. polymatrix games linear combination two-agentnormal-form games, reduce linear complementarity problem solvedquickly using Lemke-Howson algorithm (Lemke & Howson, 1964).agent n N polymatrix game, payoff array matrix B n indexednactions agent n agent; actions a0 An0 , Ba,a0000payoff n receives playing game agent n , n plays . Agentnspayoffs receives games agent,P totalPpayoff sumnn0 6=naAn ,a0 An0 a0 Ba,a0 . Given normal-form game G strategy profile ,construct polymatrix game P whose payoff function JacobianGs settingGn(9)Ba,a0 = Va,a0 () .game P linearization G around : Jacobian everywhere. GWshow equilibrium G equilibrium P . followsequation V G () = V G () /(|N | 1), holds . seeholds, consider single element indexed :XXX(V G () )a =a0Gn (a, a0 , t)tkn0 N \{n} a0 An0=XXn0 N \{n}tAnkN \{n,n0 }tAn,n0Gn (a, t)tkkN \{n}G= (|N | 1)V ()a .equilibrium characterization equation therefore written= R + V G () (|N | 1) .477fiBlum, Shelton, & KollerG P value V , thus equilibrium characterizationfunction. satisfies one satisfies other.define mapping p : p() equilibrium P (specifically,first equilibrium found Lemke-Howson algorithm). p() = ,equilibrium G. IPA procedure Govindan Wilson (2004) aims findfixed point. begins randomly chosen strategy profile , calculates p()running Lemke-Howson algorithm; adjusts toward p() using approximatederivative estimate p built past two iterations. p() sufficientlyclose, terminates approximate equilibrium.IPA guaranteed converge. However, practice, quickly moves neargood solution. possible point calculate perturbed game closeoriginal game (essentially, one differs amount Gs polymatrixapproximation differs G) found approximate equilibrium factexact equilibrium. continuation method run starting pointfind exact equilibrium original game. continuation method guaranteedconverge starting point. However, practice always foundconverge, long IPA configured search high quality equilibrium approximations.Although theoretical results required quality, IPA refine startingpoint continuation method fails. results show IPA quick-startsubstantially reduces overall running time algorithm.fact use approximate algorithm quick-start ours, alsowithout guarantees convergence. Given approximate equilibrium , inverseimage R defined set linear constraints. let w := V G () + ,use standard QP methods retract w nearest point w0 satisfyingconstraints, let b := w0 w. = R(w0 ) = R(V G () + + b),continuation method path. Alternatively, choose b wobbling, caseset b := [w V G (R(w)) R(w)]/.7. Exploiting Structurealgorithms continuation method foundation game representation,calculation V G Step 2(b)i pseudo-code Figure 4 differentconsumes time. normal-form (in worst case) extensiveform games, requires exponential time number agents. However, showsection, using structured representation graphical game MAID,effectively exploit structure game drastically reduce computationaltime required.7.1 Graphical GamesSince graphical game also normal-form game, definition deviation functionV G Equation (4) same: VaG () payoff agent n deviatingplay deterministically. However, due structure graphical game, choicestrategy agent outside family n affect agent n0 payoff.observation allows us compute payoff locally.478fiA Continuation Method Nash Equilibria Structured Gamesinput game G:1. Set = 1, choose initial b either quick-start procedure (e.g., IPA) randomizing. Setw = V G () + b + .2. greater (negative) threshold (i.e., still good chance pickinganother equilibrium):(a) Initialize current support cell: set steps counter number steps takecrossing cell, depending current amount error. F linear nearly linear (if,example, strategy profile nearly pure, 2 agents), set steps = 1cross entire cell.(b) steps 1:i. Compute V G ().ii. Set w F (w, ) = (V G () + I)R(w) (we already know F = b). Set dw =adj(w F ) b = det(w F ). satisfy Equation (3).iii. Set equal distance wed go direction dw reach next supportboundary. scale dw /steps.iv. change signs course step, record equilibrium point0.v. Set w := w + dw(/steps) := + d(/steps).vi. sufficient error accumulated, use local Newton method find w minimizing|F (w, )|. reduce error enough, increase steps, thereby decreasing stepsize. already increased steps, perform wobble reassign b.vii. Set steps := steps 1.Figure 4: Pseudo-code cont algorithm.7.1.1 Jacobian Graphical Gamesbegin definition V G normal-form games (modified slightly accountlocal payoff arrays). Recall Afn set action profiles agents Famnn, let AFamn set action profiles agents Famn .divide sum full action profiles two sets, switchingnormal-form version Gn graphical game version Gn , follows:XVaG () =Gn (a, t)tktAn=XuAfnkN \{n}Gn (a, u)ukXv j .(10)kFamn \{n} vAFamn jN \FamnNote latter sum product simply sum probability distribution, hencealways equal 1 due constraints . thus eliminated withoutchanging value V G takes valid strategy profiles. However, partial derivativesrespect strategies agents Famn non-zero, enter computation V G .Suppose wish compute row Jacobian matrix corresponding actionagent n. must compute entries action a0 agent n0 N . trivialG = 0, since appear anywhere expressioncase n0 = n Va,a0VaG (). next compute entries action a0 agent n0 Famn .479fiBlum, Shelton, & Kollercase,GVa,a0 () =XGn (a, u)a0f=Gn (a, u)uAfn=Xa0v j(11)uk 1kFamn \{n}Gn (a, a0 , t)tAfn,n0XvAFamn jN \FamnkFamn \{n}uAnXukkFamnn0 Famn .tk ,(12)\{n,n0 }next compute entry single action a0 agent n0/ Famn . derivativeEquation (11) takes different form case; variable question secondsummation, first,XXGVa,av jGn (a, u)uk0 () =a0f=XGn (a, u)=XukGn (a, u)uAfnXvAFamnkFamn \{n}uAfnvAFamn jN \FamnkFamn \{n}uAnuk 1,a0v jjN \Famnn0 6 Famn .(13)kFamn \{n}Notice calculation depend a0 ; therefore, actionagent Famn . need compute elements row.copy value columns actions belonging agents Famn .7.1.2 Computational ComplexityDue graphical game structure, computation V G () takes time exponentialmaximal family size game, hence takes time polynomial numberagents family size constant. particular, methods lead followingtheorem complexity continuation method graphical games.Theorem 10. time complexity computing Jacobian deviation functionV G () graphical game O(f df |N | + d2 |N |2 ), f maximal family sizemaximal number actions per agent.Proof. Consider single row Jacobian, corresponding single action ownedsingle agent n. d(f 1) entries row actions ownedGmembers Famn . one action a0 , computation Jacobian element Va,a0f2according Equation (12) takes time O(d ). total cost entries thereforeO((f 1)df 1 ). d(|N | f ) entries actions owned non-familyG a0 same. calculated timemembers. value Va,a0f1O(d ), copied across row time d(|N | f ). all, computational costrow O(f df 1 + d|N |). d|N | rows, total computationalcost O(|N |f df + d2 |N |2 ).480fiA Continuation Method Nash Equilibria Structured GamesP1P2P3B1B2B3B(a)(b)Figure 5: strategic relevance graphs MAIDs (a) Figure 2(a) (b) Figure 2(b).iteration algorithm calculates V G () once; therefore provedsingle iteration takes time polynomial |N | f constant (in fact, matrix operations makecomplexity cubic |N |). However, normal-form games, theoreticalresults many steps continuation method required convergence.7.2 MAIDsgraphical games, exploitation structure straightforward. turndifficult problem exploiting structure MAIDs. take advantage twodistinct sets structural properties. first, coarse-grained structural measure knownstrategic relevance (Koller & Milch, 2001), used previous computationalmethods. decomposing MAID according strategic relevance relations,exploit finer-grained structure using extensive-form continuation method GWsolve components equivalent extensive-form game. next two sections,describe two kinds structure.7.2.1 Strategic RelevanceIntuitively, decision node Dni strategically relevant another decision node Dnj 0 agentn0 , order optimize decision rule Dnj 0 , needs know agent ns decision ruleDni . relevance relation induces directed graph known relevance graph,decision nodes appear edge node Dnj 0 node Dni present iff Dnistrategically relevant Dnj 0 . event relevance graph acyclic, decisionrules optimized sequentially reverse topological order; childrennode Dni decision rules set, decision rule Dni optimizedwithout regard nodes.cycles exist relevance graph, however, steps must taken. Withinstrongly connected component (SCC), set nodes directed pathtwo nodes exists relevance graph, decision rules cannot optimized sequentiallylinear ordering nodes SCC, node must optimized one481fiBlum, Shelton, & Kollerchildren, impossible. Koller Milch (2001) show MAIDdecomposed SCCs, solved individually.example, relevance graph MAID Figure 2(a), shown Figure 5(a),one SCC consisting B, another consisting A0 . MAID, wouldfirst optimize decision rule A0 , optimal decision rule A0 relydecision rules B makes decision A0 , Alice already knowsactions taken B, need know decision rules led them.would turn A0 chance node CPD specified optimized decisionrule optimize decision rules A0 B. relevance graph Figure 2(b),shown Figure 5(b), forms single strongly connected component.computational method Koller Milch (2001) stops strategic relevance:SCC converted equivalent extensive-form game solved using standardmethods. algorithm viewed augmentation method: MAIDdecomposed SCCs, solve SCCs using methods, takingadvantage finer-grained MAID structure within find equilibria efficiently.MAIDs test algorithms (including road MAID Figure 2b)strongly connected relevance graphs, cannot decomposed (see Figure 5bFigure 10).7.2.2 Jacobian MAIDsMAID equivalent extensive-form game, deviation function V Gone defined Equation (8). Now, however, compute payoffs makeJacobian V G efficiently. Consider payoff Gn (z) agent n outcome z.outcome z simply assignment x variables MAID. realizationprobability n (z) productQ probabilities decisions agent nassignment x, product kN k (z) realization probabilities simply jointprobabilityPof assignment.expected payoff agent n receive strategyQprofile , zZ Gn (z) kN k (z), therefore expectation Gn (z). expectationrespect distribution P defined Bayesian network B whose structureMAID, decision node CPDs determined .entries V G strictly expected payoffs, however. Equation (8)rewrittenQX Gn (z)kN k (z)GVh,h0 () =.(14)n (z)n0 (z)zZh,h0expectation quantity Gn (z)/[n (z)n0 (z)]. payoff Gn (z) sum agentns utility nodes. Due linearity expectation, perform computation separatelyagent ns utility nodes, simply add separate contributions.therefore restrict attention computing contribution single utilitynode Un agent n. Furthermore, value n (z) depends valuesset nodes n consisting ns decision nodes parents. Thus, insteadcomputing probabilities assignments variables, need computemarginal joint distribution Un , n , n0 . distribution,compute contribution Un expectation Equation (14) every pair terminalsequences belonging agents n n0 .482fiA Continuation Method Nash Equilibria Structured GamesP1P2P3E1E2C1C2B1C3B2R1L2P1 E1B1 B2E1 P2B1 B2P2 E2B2 B3E2 P3B2 B3B3R2L3(a)(b)Figure 6: (a) two-stage road MAID three agents shown divided cliques.four cliques surrounded dashed line, three decision nodeschance node. (b) resultant clique tree.7.2.3 Using Bayesian Network Inferenceanalysis reduces required computations significantly. Rather computingseparate expectation every pair sequences h, h0 , might first seemednecessary, need compute one marginal joint distribution variables {Un }n n0 every pair agents n, n0 . marginal joint distribution one definedBayesian network B . Naively, computation requires execute Bayesiannetwork inference |N |2 times: ordered pair agents n, n0 . fact,exploit structure MAID perform computation much efficiently.basis method standard clique tree algorithm Lauritzen Spiegelhalter(1998). clique tree algorithm fairly complex, detailed presentation outsidescope paper. choose treat algorithm black box, describingproperties relevant understanding used withincomputation. note details suffice allow method implementedusing one many off-the-shelf implementations clique tree algorithm. readerwishing understand clique tree algorithm derivation detail referredreference Cowell et al. (1999) complete description.clique tree Bayesian network B data structure defined undirectedtree set nodes C. node Ci C corresponds subset variablesB, typically called clique. clique tree satisfies certain important properties.must family preserving: node X B, exists clique Ci C(X PaX ) Ci . also satisfies separation requirement: C2 lies unique pathC1 C3 , then, joint distribution defined B, variables C1 mustconditionally independent C3 given C2 .division 3-agent road MAID cliques shown Figure 7.2.3(a).MAID 4 cliques. Notice every family contained clique (including familieschance nodes utility nodes). clique tree MAID shown Figure 7.2.3(b).483fiBlum, Shelton, & Kollerclique maintains data structure called potential, table entryjoint assignment variables clique. table sort generally calledfactor. Inference algorithms typically use two basic operations factors: factor product,factor marginalization. F G two factors (possibly overlapping) setsvariables X , respectively, define product FG new factorX . entry FG particular assignment variables Xproduct entries F G corresponding restriction assignment X, respectively. notion multiplication corresponds way conditionalprobability distributions multiplied. also marginalize, sum, variable Xfactor F X wayPwe would sum variable jointprobability distribution. result factor XPF variables X\{X}.entry particular assignment variables X F equal sum entriesF compatible assignment one value X.factor entry every joint assignment variables, sizepotential Ci exponential |Ci |. clique tree inference algorithm proceedspassing messages, factors, one clique another tree. messagesused update potential receiving clique factor multiplication.process messages sent directions edge tree,tree said calibrated ; point, potential every clique Ci contains preciselyjoint distribution variables Ci according B (for details, referreference Cowell et al., 1999).use clique tree algorithm perform inference B . Consider finaldecision node agent n. Due perfect recall assumption, ns previous decisionsparents also parents decision node. family preservationproperty therefore implies n fully contained clique. also impliesfamily utility node contained clique. expectation Equation (14)thus requires computation joint distribution three cliques tree: onecontaining PaUn , one containing n , one containing n0 . need computejoint distribution every pair agents n, n0 .first key insight reduce problem one computing joint marginal distribution pairs cliques tree. Assume computed PB (Ci , Cj )every pair cliques Ci , Cj . Now, consider triple cliques Ci , Cj , Ck . twocases: either one cliques path two, not. firstcase, assume without loss generality Cj path Ci Ck . case,separation requirement, PB (Ci , Cj , Ck ) = PB (Ci , Cj )PB (Cj , Ck )/PB (Cj ).second case, exists unique clique C lies path paircliques. Again, separation property, C renders cliques conditionallyindependent, computePB (Ci , Cj , Ck ) =X PB (Ci , C )PB (Cj , C )PB (Ck , C )PB (C )2C.(15)Thus, reduced problem one computing marginals pairscliques calibrated clique-tree. use dynamic programming execute processefficiently. construct table contains PB (Ci , Cj ) pair cliques Ci , Cj .construct table order length path Ci Cj . base case Ci484fiA Continuation Method Nash Equilibria Structured GamesCj adjacent tree. case, PB (Ci , Cj ) = PB (Ci )PB (Cj )/PB (CiCj ). probability expressions numerator simply clique potentialscalibrated tree. denominator obtained marginalizing either twocliques. fact, expression computed byproduct calibration process,marginalization required. cliques Ci Cj adjacent, let Cknode adjacent Cj path Ci Cj . clique Ck one step closerCi , so, construction, already computed P (Ci , Ck ). applyseparation property again:PB (Ci , Cj ) =X PB (Ci , Ck )PB (Ck , Cj )PB (Ck )Ck.(16)7.2.4 Computational ComplexityTheorem 11. computation V G () performed time O(`2 d3 + u|N |d4 ),` number cliques clique tree G, size largest clique(the number entries potential), |N | number agents, u totalnumber utility nodes game.Proof. cost calibrating clique tree B O(`d). cost computingEquation (16) single pair cliques O(d3 ), must compute factorvariables three cliques summing out. must perform computation O(`2 )times, pair cliques, total cost O(`2 d3 ). compute marginaljoint probabilities triples cliques PaUni , n , n0 every utility node Uni everyagent n0 n. u(|N | 1) triples. Computing factorvariables three cliques may first require computing factor variables fourcliques, cost O(d4 ). Given factor, computing expected value utilitynode takes time O(d3 ), affect asymptotic running time. total costcomputing marginal joint probabilities expected utilities therefore O(u|N |d4 ),total cost computing V G () O(`2 d3 + u|N |d4 ).method, shown single iteration continuation methodaccomplished time exponential induced width graph numbervariables largest clique clique tree. induced width optimal cliquetree one smallest maximal clique called treewidth network.Although finding optimal clique tree is, itself, NP-hard problem, good heuristicalgorithms known (Cowell et al., 1999). games interactions agentshighly structured (the road MAID, example), size largest cliqueconstant even number agents grows. case, complexity computingJacobian grows quadratically number cliques, hence also numberagents. Note matrix adjoint operation takes time cubic m, least|N |, single step along path actually cubic computational cost.485fiBlum, Shelton, & Koller1800400contIPA+contVK160014003001200250secondssecondscontIPA+contVK35010008002001506001004005020000204060# agents8001001015(a)202530# agents354045(b)47x 100.01Cumulative60.009Terminating run0.00850.007seconds/iteration# iterationscontcubic fit430.0060.0050.0040.00320.00210.0010101520# agents0525(c)101520# agents25(d)Figure 7: Results 2-by-L road game rock-paper-scissors payoffs: (a) running time.Results road game random payoffs: (b) running time; (c) numberiterations cont; (d) average time per iteration cont.8. Resultsperformed run-time tests algorithms wide variety graphical gamesMAIDs. Tests performed Intel Xeon processor running 3 GHz 2GB RAM, although memory never taxed calculations.8.1 Graphical Gamesgraphical games, compared two versions algorithm: cont, simple continuation method, IPA+cont, continuation method IPA initialization. testedhybrid equilibrium refinement algorithm Vickrey Koller (2002) (VK hereafter)486fiA Continuation Method Nash Equilibria Structured Games46005contIPA+contVK500Cumulative4.5Terminating run43.5# iterations400secondsx 1030020032.521.511000.5051015202530# agents354004551015(a)2025# agents303540(b)42506x 10contCumulativeIPA+cont5200Terminating runVK# iterationsseconds41501003250015101520# agents2530350(c)101520# agents25(d)Figure 8: Results ring game random payoffs: (a) running time; (b) numberiterations cont. Results L-by-L grid game random payoffs: (c) runningtime; (d) number iterations cont.comparison, parameters used. VK algorithm returns-equilibria; exact methods exist comparable own.algorithms run two classes games defined Vickrey Koller (2002)two additional classes. road game Example 3, denoting situationagents must build land plots along road, played 2-by-L grid; agent threeactions, payoffs depend actions (grid) neighbors. Following VK,ran algorithm road games additive rock-paper-scissors payoffs: agentspayoffs sum payoffs independent rock-paper-scissors gamesneighbors. game is, fact, polymatrix game, hence easy solve usingmethods. order test algorithms typical examples, experimented487fiBlum, Shelton, & Kollerroad games entries payoff matrix agent chosen uniformlyrandom [0, 1]. also experimented ring graph three actions peragent random payoffs. Finally, order test games increasing treewidth,experimented grid games random payoffs. defined mannerroad games, except game graph L-by-L grid.class games, chose set game sizes run on. each, selected(randomly cases payoffs random) set 20 test games solve.solved game using cont, IPA+cont, VK. cont, started differentrandom perturbation vector time recorded time number iterationsnecessary reach first equilibrium. IPA+cont, started different initialstrategy profile IPA time recorded total time IPA cont reachfirst equilibrium.equilibria found algorithm error 1012 , essentially machineprecision. hybrid refinement algorithm VK found -equilibria average error104 road games rock-paper-scissors payoffs, 0.01 road games gridgames random payoffs, 0.03 ring games random payoffs, althoughequilibria error high 0.05 road games 0.1 ring games.smaller games, algorithms always converged equilibrium. largergames, cont IPA detected entered cycle terminated without findingequilibrium. maintaining hash table support cells passedalready, cont IPA able detect entered support cellsecond time. Although sure sign entered cycle, strongindicator. potential cycles detected, algorithms restarted newrandom initialization values. Note cycles execution cont never arisealgorithm stray path dictated theory GW, randomrestarts reflect failure follow path accurately.equilibrium eventually found, cumulative time randomrestarts recorded. error bars running time graphs show variance duenumber random restarts required, choices initialization values, and, randomgames, choice game.Random restarts required 29% games tested. average, 2.2 restartsnecessary games. Note figure skewed larger games,occasionally required many restarts; largest games sometimes required 8 9 restarts.large graphical games (10 random road games 8 random ring games), IPAconverge 10 restarts; cases record results IPA+cont. contalways found equilibrium within 10 restarts. results shown Figures 7(a,b,c,d)Figures 8(a,b,c).random roads, also plotted number iterations time per iterationcont Figures 7(c,d). number iterations varies based gameperturbation vector chosen. However, time per iteration almost exactly cubic,predicted. note that, IPA used quick-start, cont invariably convergedimmediately (within second) time spent IPA algorithm.road games, methods efficient smaller games, become costly. Due polymatrix nature rock-paper-scissors road games,IPA+cont algorithm solves immediately Lemke-Howson algorithm,488fiA Continuation Method Nash Equilibria Structured Gamestherefore significantly less expensive VK. random ring games, algorithmsefficient VK smaller games (up 2030 agents), IPA+cont performing considerably better cont. However, road games, running timealgorithms grows rapidly VK, larger games, becomeimpractical. Nevertheless, algorithms performed well games 45 agents3 actions per agent, previously intractable exact algorithms.L-by-L grid games, algorithm performed much better VK algorithm (see Figures 8(c,d)), without IPA quick-start. reflects fact running-timecomplexity algorithms depend treewidth graph.# equilibria80604020020101581065# players42# runsFigure 9: number unique equilibria found function size gamenumber runs algorithm, averaged ten random ring games.also examined number equilibria found IPA+cont algorithm. ranIPA+cont ring graphical game differing numbers agents. numberagents, fixed 10 random games, ran algorithm 10 times game, recordedcumulative number unique equilibria found. average number equilibria found10 games number agents plotted figure 9. small games (withpresumably small number equilibria), number equilibria found quickly saturated.large games, almost linear increase number equilibria foundsubsequent random restart, implying run algorithm produced newset solutions.8.2 MAIDsprevious computational method MAIDs (Koller & Milch, 2001) stopped strategicrelevance: SCC converted equivalent extensive-form game solved usingstandard methods. algorithm takes advantage structure game already decomposed according strategic relevance. test cases thereforeselected relevance graphs consisting single strongly connected component.489fiBlum, Shelton, & KollerNABABNBCBC(a)BC(b)Figure 10: (a) chain game (b) strategic relevance graph case threeagents (A, B, C).order ascertain much difference enhancements made, comparedresults MAID algorithm, MAID cont, achieved converting gameextensive-form running EF cont, extensive-form version cont specifiedGW, Gambit (McKelvey, McLennan, & Turocy, 2004), standard game theory softwarepackage. time required conversion extensive form included results.ran algorithms two classes games, varying sizes. first,refer chain game, alternates decision chance nodes (see Figure 10).decision node belongs different agent. agent two utility nodes,connected decision node neighbors (except end agents,one utility node single neighbor). three actions decision node.probability tables payoff matrices chosen uniformly random. secondclass two-stage road building game Example 5, shown Figure 2(b).class, chose payoffs carefully, hand, ensure non-trivial mixed strategy equilibria.ran chain games sizes 2 21, road games sizes2 9. size, randomly selected 20 perturbation vectors 20 games (all20 road games same, since payoffs set hand, 20 chain gamespayoffs randomly assigned). tested algorithms games, initializedperturbation vectors, averaged across test cases. timing results appearFigures 11(a,b). error bars reflect variance due choice game (in chaingames), choice perturbation vector, number random restarts required.cases, graphical game tests, MAID cont failed find equilibrium, terminating early detected entered cycle. cases,restarted new perturbation vector successfully terminated.equilibrium eventually found, cumulative time random restartsrecorded. course test runs, two chain games required random restart.size 7. algorithms failed frequently road games; spikeroad games size 8 reflects fact games size required, average, 1.2490fiA Continuation Method Nash Equilibria Structured Games200900contEF contgambit180800160contEF contgambit700140600secondsseconds1201005004008030060200401002002468101214# agents1618200222345(a)6# agents789789(b)10000.4contcubic fit9000.358000.3seconds/iteration# iterations7006005004000.250.20.153000.12000.05100023456# agents78029(c)3456# agents(d)Figure 11: Results MAIDs: (a) Running times chain MAID. Results two-stageroad MAID: (b) running time; (c) number iterations; (d) time per iteration.random restarts equilibrium found. Strangely, MAID cont muchsuccessful road game size 9, succeeding without random restarts twocases.tested Gambit EF cont smaller games, time memoryrequirements testing larger ones beyond means. results show that,EF cont faster algorithm Gambit extensive-form games, inadequatelarger MAIDs able solve MAID cont. surprising;road game size 9 26 decision chance nodes, equivalent extensive-form gametree 226 67 million outcome nodes. MAIDs size, Bayesian networkinference techniques used become necessary.491fiBlum, Shelton, & KollerMAIDs, realization probabilities constrained least 104 (i.e.,found -perfect equilibria = 104 ). accuracy equilibria within 1012 ,machine precision.graphical games, recorded number iterations convergence welltime per iteration MAID cont. results appear Figures 11(c,d). timeper iteration fit well cubic curve, accordance theoretical predictions.variance primarily due execution retraction operator, whose running timedepends number strategies support.9. Discussion Conclusionsdescribed two adaptations continuation method algorithms GW,purpose accelerated execution structured games. results showalgorithms represent significant advances state art equilibrium computationgraphical games MAIDs.9.1 Related Work Graphical Gameslast years, several papers addressed issue finding equilibria structured games. graphical games, exact algorithms proposed far apply gamesinteraction structure undirected tree, agent twopossible actions. Kearns et al. (2001) provide exponential-time algorithm computeexact equilibria game, Littman et al. (2002) provide polynomial-timealgorithm compute single exact equilibrium. limited set games,algorithms may preferable own, since come running-time guarantees.However, yet tested whether algorithms are, fact, efficient practice. Moreover, methods applicable fully general games, results indicateperform well.effort focused computation -equilibria general graphicalgames. number algorithms recently proposed task.use discretized space mixed strategies: probabilities must selected gridsimplex, made arbitrarily fine. computational reasons, however,grid must typically quite coarse, number grid points consider growsexponentially number actions per agent. methods (implicitlyexplicitly) define equilibrium set constraints discretized strategy space,use constraint solving method: Kearns et al. (2001) use tree-propagationalgorithm (KLS); Vickrey Koller (2002) use standard CSP variable elimination methods(VK1); Ortiz Kearns (2003) use arc-consistency constraint propagation followedsearch (OK). Vickrey Koller (2002) also propose gradient ascent algorithm (VK2),provide hybrid refinement method can, computation, reduceequilibrium error.exact methods, KLS algorithm restricted tree-structured games,comes without experimental running time results (although guaranteed runpolynomial time). Kearns et al. (2001) give suggestion working non-tree graphconstructing junction tree passing messages therein. However, necessarycomputations clear potentially expensive.492fiA Continuation Method Nash Equilibria Structured GamesVK1 algorithm applicable graphical games arbitrary topology,number actions per agent. takes time exponential treewidth graph.treewidth constant, scales linearly number agents; however,results show quickly becomes infeasible treewidth expands (as gridgame).methods come complexity guarantees, depend treewidthgraph. others (OK VK2, well algorithm) insensitive treewidthsingle iteration takes time polynomial size game representation (andhence exponential maximum degree graph). However, requireunknown number iterations converge. Corollary 7 shows that, general, computationequilibria discretized strategies games fixed degree hard. Thus, lackcomplexity guarantees methods surprising.Nonetheless, experimental results OK seem promising indicate that,average, relatively iterations required convergence. Results indicate OKcapable solving grid games least 100 agents (although caseslarge 0.2, much better random fully mixed strategy profile). However,running time results provided.VK2 also exhibits strong experimental results. Vickrey Koller (2002) successfully found -equilibria games 400 agents, errors 2% maximalpayoff.main drawback algorithms compute -equilibria. equilibrium may sufficient certain applications: utility functionsapproximate, agent certainly might satisfied -best response; makeassumption slightly costly agents change minds, agent mightneed incentive greater deviate. However, -equilibria bring setproblems. primary one guarantee exact equilibriumneighborhood -equilibrium. make difficult find -equilibriasmall values ; attempts refine given -equilibrium may fail. lack nearbyNash equilibrium also implies certain instability. agent unsatisfied-equilibrium, play may deviate quite far it. Finally, -equilibria numerousNash equilibria (uncountably so, general). exacerbates difficulty agentfaces choosing equilibrium play.algorithms computing -equilibria frequently faster own, especiallyapproximations crude games 50 agents. However,exact equilibria found algorithms satisfying solutions, resultsshow performance algorithm comparable approximate methodscases. Surprisingly, many games, running time results showfastest available, particularly case games large treewidth, gridgame test cases. Furthermore, since use approximate equilibriumstarting point algorithm, advances approximate methods complementmethod. hybrid algorithm Vickrey Koller (2002) turns unsuitedpurpose, tends remove pure strategies support,interesting see whether methods (including listed above) mighteffective. remains seen small must methods reliably refineapproximate equilibrium.493fiBlum, Shelton, & Koller9.2 Related Work MAIDsKoller Milch (2001) (KM) define notion dependence agents decisions(s-relevance), provide algorithm decompose solve MAIDs basedfairly coarse independence structure. algorithm able exploit finer-grainedstructure, resolving open problem left KM. general, method automatically exploit structure obtained decomposing game relevancecomponents, methods best regarded complement KM; decomposition according s-relevance, algorithm applied find equilibriaefficiently decomposed problems. Running time results indicate methodssignificantly faster previous standard algorithms extensive-form games.unsurprising, since game representation test cases exponentially largernumber players converted extensive-form.Vickrey (2002) proposes approximate hill-climbing algorithm MAIDs takesadvantage sort fine-grained structure do: Bayesian network inferenceemployed calculate expected utility one component score function singleiteration. constraint-satisfaction approach also proposed. However, proposalsnever implemented, hard determine quality equilibria would findquickly would find them.La Mura (2000) proposes continuation method finding one equilibriaG net, representation similar MAIDs. proposal exploitslimited set structural properties (a strict subset exploited KM).proposal also never implemented, several issues regarding non-converging pathsseem unresolved.algorithm therefore first able exploit finer-grained structureMAID. Moreover, algorithm, applied conjunction decomposition methodKM, able take advantage full known independence structure MAID.potential drawback requirement strategies -perturbed. However, decreasingincurs additional computational cost, although limits imposed machineprecision. Perfect equilibria highly desirable refinement Nash equilibria, definedlimit sequence -perturbed equilibria goes zero thereforecomputed effectively algorithm little additional computational cost.sense, use perturbed strategies advantageous. implementedlocal search algorithm find exact perfect equilibrium neighborhood found-perturbed equilibrium, although straightforward so.9.3 Conclusion Workpresented two related algorithms computing exact equilibria structuredgames. algorithms based methods GW, perform key computationalsteps methods much efficiently exploiting game structure. approachyields first exact algorithm take advantage structure general graphical gamesfirst algorithm take full advantage independence structure MAID.algorithms capable computing exact equilibria games large numbersagents, previously intractable exact methods.494fiA Continuation Method Nash Equilibria Structured Gamesalgorithms come without theoretical running time bounds, noticed certain interesting trends. graphical game MAID version algorithm,iteration executes time polynomial number agents, examinednumber iterations required convergence. adaptive step size technique decreasesnumber random restarts required find equilibrium, increases numberiterations required cross support cell larger games. adaptive step sizedisabled, noticed number iterations required, averaged across gamesrandom payoffs, seems grow approximately linearly. Intuitively, makes sensenumber iterations least linear: starting pure strategy profile,linear number actions (in number agents) must enter support order usreach general strategy profile. support boundary requires least one iterationalgorithm. somewhat surprising, however, number iterations requiredgrow quickly. interesting open problem analyze numberiterations required convergence.large games, tendency algorithm cycle increases. phenomenonattributed, partially, cumulative effect wobbling: great numberwobbles, possible path altered sufficiently passequilibrium. noticed games seem intrinsically harderothers, requiring many random restarts convergence. large games,overall running time algorithm therefore quite unpredictable.algorithms might improved number ways. importantly, continuation method would profit greatly sophisticated path-following methods;number cases, cont MAID cont failed find equilibrium strayedfar path. Better path-following techniques might greatly increase reliabilityalgorithms, particularly obviated need wobbles, negate GWstheoretical guarantee convergence continuation method.also number theoretical questions algorithms GWremain unresolved. Nothing known worst-case average-case running timeIPA, theoretical bounds exist number iterations required cont.interesting speculate choice perturbation ray might affect executionalgorithm. algorithm directed toward particular equilibria interesteither careful selection perturbation ray change continuationmethod? way selecting perturbation rays equilibria found?way selecting perturbation ray speed execution time?Several improvements might made MAID cont. adapted IPA useMAIDs, possible so, making use generalized Lemke algorithmKoller, Megiddo, von Stengel (1996) solve intermediate linearized MAIDs.computation V G might also accelerated using variant all-pairs clique treealgorithm computes potentials pairs sepsets sets variables sharedadjacent cliques rather pairs cliques.work suggests several interesting avenues research. fact,initial publication results (Blum, Shelton, & Koller, 2003), least oneapplication techniques already developed: Bhat Leyton-Brown (2004)shown adaptation cont used efficiently solve new class structured games called action-graph games (a generalization local effect games presented495fiBlum, Shelton, & KollerLeyton-Brown & Tennenholtz, 2003). believe games, structuredrepresentations, show great promise enablers new applications game theory.several advantages unstructured counterparts: well-suited gameslarge number agents, determined fewer parameters, making feasiblehuman researchers fully specify meaningful way, built-in structuremakes intuitive medium frame structured, real-world scenarios.However, avoid computational intractability general problem, new classstructured games requires new algorithm equilibrium computation. hypothesizecont IPA excellent starting point addressing need.Acknowledgments. work supported ONR MURI Grant N00014-00-1-0637,Air Force contract F30602-00-2-0598 DARPAs TASK program. Specialthanks Robert Wilson, kindly taking time guide us detailswork Srihari Govindan, David Vickrey, aiding us testing algorithmsalongside his. also thank anonymous referees helpful comments.496fiA Continuation Method Nash Equilibria Structured GamesAppendix A. Table NotationNotation gamesNset agentsnstrategy agent nnstrategy space agent nstrategy profilespace strategy profilesnstrategy profile restricted agents nnspace strategy profiles agents n(n , n ) strategy profile agent n plays strategy n agents actaccording nGn ()expected payoff agent n strategy profileGV ()vector deviation functionRretraction operator mapping points closest valid strategy profileFcontinuation method objective functionscale factor perturbation continuation methodwfree variable continuation methodNotation normal-form gamesaction agent nset available actions agent naction profileset action profilesaction profile restricted agents nspace action profiles agents nNotation extensive-form gameszleaf node game tree (outcome)Zset outcomesinformation setset information sets agent nA(i)set actions available information setHn (y)sequence (history) agent n determined nodeZhset outcomes consistent sequence (history) hb(a|i)probability behavior profile b agent n choose actionn (z)realization probability outcome z agent nNotation graphical gamesFamnset agent n agent ns parentsfnstrategy profiles agents Famn nfspace action profiles agents Famn nNotation MAIDsDnidecision node index belonging agent nUniutility node index belonging agent nPaXparents node Xdom(S) joint domain variables set497fiBlum, Shelton, & KollerC1C2C3bbcbccFigure 12: Reduction 3SAT instance (a b c) (a b c) (a b c)graphical game.Appendix B. Proof Theorem 6Proof. proof reduction 3SAT. given 3SAT instance, constructgraphical game whose equilibria encode satisfying assignments variables.Let C = {c1 , c2 , . . . , cm } clauses 3SAT instance question, let V ={v1 , v1 , v2 , v2 , . . . , vn , vn } set literals. variable appears one clause,immediately assigned satisfy clause; therefore, assume variablesappear least two clauses.construct (undirected) graphical game. clause, ci , createagent Ci connected Ci1 Ci+1 (except C1 Cm , one clauseneighbor). also create agents Vi` literal ` ci (there 3). If,example, ci clause (v1 v2 ), agents Viv1 Viv2 . connectCi . every variable v, group agents Viv Vjv connect line,way connected clauses other. order unimportant.Clause agents 5 neighbors (two clauses either side threeliterals) literal agents 3 neighbors (two literals either sideone clause). completely specifies game topology. example, Figure 12 showsgraphical game corresponding 3SAT problem (abc)(abc)(abc).define actions payoff structure. agent interpretedBoolean variable, two actions, true false, correspond Booleanvalues true false. Intuitively, clause Ci plays true, satisfied. agentViv plays true, v non-negated variable, v assigned true. Vjvplays true, v assigned false.payoff matrix clause agent Ci designed ensure one clauseunsatisfied, entire 3SAT instance marked unsatisfied. best expressedpseudo-code, follows:Ciclause neighbors play false1 playing falsepayoff0 playing trueelse least one Ci literals plays true (Ci satisfied)498fiA Continuation Method Nash Equilibria Structured Gamespayoff22playing falseplaying trueelse(Ci unsatisfied)1 playing falsepayoff0 playing trueendpayoff matrix literal agent Vi` designed encourage agreementliterals along line variable v(`) associated `. describedpseudo-code follows:parentclause Ci plays false1 playing consistently false assignment v(`)payoff0 playing oppositeelse Vi` sliteral neighbors play consistently single assignment v(`)2 playing consistently neighborspayoff0 playing oppositeelse2 playing consistently false assignment v(`)payoff0 playing oppositeendformula satisfying assignment, pure equilibriumliteral consistent assignment clauses play true; fact,agents receive higher payoffs case equilibrium, satisfyingassignments correspond equilibria maximum social welfare.parent clauses play false, clearly equilibrium non-negated literalsmust play false negated literals must play true. trivial equilibrium.remains show trivial equilibrium equilibrium unsatisfiable formulas, i.e. non-trivial equilibrium used construct satisfying assignment.first prove two simple claims.Claim 11.1. Nash equilibrium, either clauses play true probability oneclauses play false probability one.Proof. case advantageous clause choose true false, neighborclause takes action false, fact disadvantageous so. Thus, clausenon-zero probability playing false equilibrium, neighbors, consequentlyclauses, must play false probability one. Therefore, possible equilibriaclauses playing false clauses playing true.follows immediately claim every non-trivial equilibrium clausesplaying true probability one.Claim 11.2. non-trivial Nash equilibrium, line literals variablev, literals play pure strategies must choose consistently singleassignment v.499fiBlum, Shelton, & KollerProof. Since equilibrium non-trivial, clauses play true. Suppose oneliterals, V ` , employs pure strategy corresponding false assignment v. sufficesshow fact literals line must pure strategies corresponding false00assignment v. Consider neighbor V ` V ` . Either V ` neighbors (oneV ` ) play consistently false assignment v, case V ` must also playconsistently false assignment v, neighbors play inconsistently, case0else clause V ` payoff matrix applies V ` must, again, play consistentlyfalse assignment v. may proceed way line manner.literals line must therefore pure strategies consistent false assignmentv, contradicting literals.Suppose non-trivial equilibrium. Claim 11.1, clauses must playtrue probability 1. literals pure strategies, clearequilibrium corresponds satisfying assignment: literals must consistentassignment Claim 11.2, clauses must satisfied. subtleties ariseconsider mixed strategy equilibria.Note first clause, payoff choosing true choosing false case satisfying assignment literals, less caseunsatisfying assignment. Therefore, unsatisfying assignment non-zeroprobability, clause must play false.Consider single clause Ci , assumed choosing true equilibrium. mixedstrategies Ci literals induce distribution jointCi plays true,W actions.`joint action non-zero probability must satisfy ` Vi . literal Vi` mixedstrategy, consider happen change strategy either one possiblepure strategies (true false). joint actions non-zero probabilityones remain subset originals, still satisfyW removed,` . Essentially, value ` affect satisfiability C , assignedV`arbitrarily.Thus, literal line certain variable mixed strategy, assignvariable either true false (and give literal line correspondingpure strategy) without making clauses connected literals unsatisfied.fact, literals line pure strategies consistentother: indeed literals pure strategies, assign variable accordingthem. Claim 11.2, always case.observe briefly constructed graphical game finite numberequilibria, even peculiarities 3SAT instance give rise equilibria mixedstrategies. clauses play false, one equilibrium. clauses play true,remove graph trim payoff matrices literalsaccordingly. line literals case generic graphical game, finite setequilibria. equilibria original game must subset direct productfinite sets.500fiA Continuation Method Nash Equilibria Structured GamesReferencesBhat, N. A. R., & Leyton-Brown, K. (2004). Computing Nash equilibria action-graphgames. Proceedings Twentieth International Conference UncertaintyArtificial Intelligence.Blum, B., Shelton, C., & Koller, D. (2003). continuation method Nash equilibriastructured games. Proceedings Eighteenth International Joint ConferenceArtificial Intelligence, pp. 757764.Chu, F., & Halpern, J. (2001). np-completeness finding optimal strategygames common payoff. International Journal Game Theory, 30, 99106.Codenotti, B., & Stefankovic, D. (2005). computational complexity nash equilibria(0,1) bimatrix games. Information Processing Letters, 94, 145150.Conitzer, V., & Sandholm, T. (2003). Complexity results Nash equilibria. Proceedings Eighteenth International Joint Conference Artificial Intelligence,pp. 765771.Cowell, R. G., Dawid, A. P., Lauritzen, S. L., & Spiegelhalter, D. J. (1999). ProbabilisticNetworks Expert Systems. Springer-Verlag.Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.Gilboa, I., & Zemel, E. (1989). Nash correlated equilibria: complexity considerations. Games Economic Behavior, 1, 8093.Govindan, S., & Wilson, R. (2002). Structure theorems game trees. ProceedingsNational Academy Sciences, 99 (13), 90779080.Govindan, S., & Wilson, R. (2003). global Newton method compute Nash equilibria.Journal Economic Theory, 110, 6586.Govindan, S., & Wilson, R. (2004). Computing Nash equilibria iterated polymatrixapproximation. Journal Economic Dynamics Control, 28, 12291241.Gul, F., Pearce, D., & Stachetti, E. (1993). bound proportion pure strategyequilibria generic games. Mathematics Operations Research, 18, 548552.Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. Howard, R. A., & Matheson, J. E. (Eds.), Readings Principles Applications Decision Analysis,Vol. 2, pp. 719762. Strategic Decision Group. article dated 1981.Kearns, M., Littman, M. L., & Singh, S. (2001). Graphical models game theory.Proceedings Seventeenth International Conference Uncertainty ArtificialIntelligence, pp. 253260.Kohlberg, E., & Mertens, J.-F. (1986). strategic stability equilibria. Econometrica,54 (5), 10031038.Koller, D., & Megiddo, N. (1992). complexity two-person zero-sum games extensiveform. Games Economic Bahavior, 4, 528552.Koller, D., Megiddo, N., & von Stengel, B. (1996). Efficient computation equilibriaextensive two-person games. Games Economic Behavior, 14, 247259.501fiBlum, Shelton, & KollerKoller, D., & Milch, B. (2001). Multi-agent influence diagrams representing solvinggames. Proceedings Seventeenth International Joint Conference ArtificialIntelligence, pp. 10271034.Kuhn, H. W. (1953). Extensive games problem information. ContributionsTheory Games II, eds. H. W. Kuhn A. W. Tucker, Vol. 28, pp. 193216.Princeton University Press, Princeton, NJ.La Mura, P. (2000). Game networks. Proceedings Sixteenth International Conference Uncertainty Artificial Intelligence, pp. 335342.Lauritzen, S. L., & Spiegelhalter, D. J. (1998). Local computations probabilitiesgraphical structures application expert systems. Journal RoyalStatistical Society, B 50 (2), 157224.Lemke, C. E., & Howson, Jr., J. T. (1964). Equilibrium points bimatrix games. JournalSociety Applied Mathematics, 12 (2), 413423.Leyton-Brown, K., & Tennenholtz, M. (2003). Local-effect games. ProceedingsEighteenth International Joint Conference Artificial Intelligence, pp. 772777.Littman, M. L., Kearns, M., & Singh, S. (2002). efficient exact algorithm singlyconnected graphical games. Advances Neural Information Processing Systems14, Vol. 2, pp. 817823.McKelvey, R. D., & McLennan, A. (1996). Computation equilibria finite games.Handbook Computational Economics, Vol. 1, pp. 87142. Elsevier Science.McKelvey, R. D., McLennan, A. M., & Turocy, T. L. (2004). Gambit: Software toolsgame theory, version 0.97.07.. http://econweb.tamu.edu/gambit.Nash, J. (1951). Non-cooperative games. Annals Mathematics, 52 (2), 286295.Nudelman, E., Wortman, J., Shoham, Y., & Leyton-Brown, K. (2004). Run GAMUT:comprehensive approach evaluating game-theoretic algorithms. Third International Conference Autonomous Agents Multi-Agent Systems.Ortiz, L. E., & Kearns, M. (2003). Nash propagation loopy graphical games. AdvancesNeural Information Processing Systems 15, Vol. 1, pp. 793800.Romanovskii, I. (1962). Reduction game complete memory matrix game.Doklady Akademii Nauk, SSSR 144, 6264. [English translation: Soviet Mathematics3, pages 678681].Vickrey, D. (2002). Multiagent algorithms solving structured games. Undergraduatehonors thesis, Stanford University.Vickrey, D., & Koller, D. (2002). Multi-agent algorithms solving graphical games.Proceedings Eighteenth National Conference Artificial Intelligence (AAAI),pp. 345351.von Stengel, B. (1996). Efficient computation behavior strategies. Games EconomicBehavior, 14, 220246.Watson, L. T. (2000). Theory globally convergent probability-one homotopies nonlinear programming. SIAM Journal Optimization, 11 (3), 761780.502fiJournal Artificial Intelligence Research 25 (2006) 1-15Submitted 08/05; published 01/06Engineering NoteEngineering Conformant Probabilistic PlannerNilufer OnderGarrett C. WhelanLi Linilufer@mtu.edugcwhelan@mtu.edulili@mtu.eduDepartment Computer ScienceMichigan Technological University1400 Townsend DriveHoughton, MI 49931Abstractpresent partial-order, conformant, probabilistic planner, Probapop competed blind track Probabilistic Planning Competition IPC-4. explainadapt distance based heuristics use probabilistic domains. Probapopalso incorporates heuristics based probability success. explain successesdifficulties encountered design implementation Probapop.1. IntroductionProbapop conformant probabilistic planner took part probabilistic track4th International Planning Competition (IPC-4). conformant plannercompeted. conformant probabilistic planning paradigm (Hyafil & Bacchus, 2003)actions state initialization probabilistic, i.e., several possibleoutcomes annotated probability occurrence. addition, planning problemconformant, i.e., planner construct best plan possible without assumingresults actions performed observed. example conformantprobabilistic planning problem, consider student applying graduate studies. Supposeapplication needs include several forms prepared student singleletter recommendation written professor (one letter sufficient oneletter acceptable). assume typical professor students department80% probability sending letter time. problem student asksone professor letter, probability complete application 0.8.student observes professor sent letter due date, waycomplete application would late ask another professor. Thus,observation actions useless way student increase chancesgetting letter ask one professor send letter. 9 professorsasked, probability getting letter 0.999997 close 1. Obviously,asking many people costly, therefore student weigh benefits increasedprobability costs asking several people.conformant probabilistic planners task find best sequence actionspossible results actions predefined probabilities cannot observed.regard, conformant probabilistic planners classified non-observable Markovc2006AI Access Foundation. rights reserved.fiOnder, Whelan & Lidecision processes (NOMDPs) (Boutilier, Dean, & Hanks, 1999). Fully-observable MDPs(FOMDP) extreme MDPs agent complete cost-freesensors indicate current state. Planners adopt FOMDP frameworkgenerate policies functions states actions. NOMDP based plannersgenerate unconditional sequences actions based predictive model,environment cannot observed (Boutilier et al., 1999). middle ground partiallyobservable MDPs (POMDPs) contingency plans domain observable execution actions may depend results observations (Kaelbling,Littman, & Cassandra, 1998; Majercik & Littman, 1999; Onder & Pollack, 1999; Hansen& Feng, 2000; Karlsson, 2001; Hoffmann & Brafman, 2005). also conformantplanners model imperfect actions may multiple possible resultsmodel probability information (Ferraris & Giunchiglia, 2000; Bertoli, Cimatti, & Roveri,2001; Brafman & Hoffmann, 2004).work Probapop motivated incentive partial-order planningviable option conformant probabilistic planning. main reasons threefold.First, partial-order planners worked well parametric lifted actions,useful coding large domains. Second, due least commitment strategy stepordering, partial-order planning (POP) produces plans highly parallelizable. Third,many planners handle rich temporal constraints based POPparadigm (Smith, Frank, & Jonsson, 2000). Given advantages, intuitiondesign Probapop bring together two paradigms model states explicitly:POP planners represent states search space plans, blindplanners cannot observe state observation actions available.basic approach form base plans using deterministic partial-order planningtechniques, estimate best way improve plans. Recently, Repop(Nguyen & Kambhampati, 2001) Vhpop (Younes & Simmons, 2003) plannersdemonstrated heuristics speed non-partial-order planners usedscale partial-order planning. show distance-based heuristics (McDermott,1999; Bonet & Geffner, 2001) implemented using relaxed plan graphs partial-orderplanners Repop Vhpop employed probabilistic domains.heuristics coupled selective plan improvement heuristics incremental planningtechniques result significant advantages. result, Probapop makes partial-orderplanning feasible probabilistic domains. work Probapop invaluableunderstanding identifying key solutions issues probabilistic conformantplanning.2. Probapop Partial-Order Planningpartial-order probabilistic planning, implemented Buridan (Kushmerick, Hanks,& Weld, 1995) probabilistic planning algorithm top Vhpop (Younes & Simmons,2003), recent partial-order planner. partially ordered plan 6-tuple, <STEPS,BIND, ORD, LINKS, OPEN, UNSAFE>, representing sets actions, binding constraints, ordering constraints, causal links, open conditions, unsafe links, respectively. bindingconstraint constraint action parameters action parameters groundliterals. ordering constraint Si Sj represents fact step Si precedes Sj .2fiEngineering Conformant Probabilistic Plannercausal link triple < Si , p, Sj >, Si producer step, Sj consumer stepp represents condition supported Si Sj . open condition pair < p, >,p condition needed step S. causal link < Si , p, Sj > unsafe plancontains threatening step Sk Sk p among effects, Sk may interveneSi Sj . Open conditions unsafe links collectively referred flaws.planning problem quadruple < D, I, G, >, where, domain theory consisting(probabilistic) operators, initial state probability distribution states, Gset literals must true end execution, termination criterionprobability threshold time limit. objective planner findmaximal probability plan takes agent G. several plansprobability success, one least number steps cost preferred.Probapop algorithm shown Figure 1 based classical POP algorithm(Russell & Norvig, 2003; Younes & Simmons, 2003). first constructs initial planconverting initial goal dummy initial goal steps, using firstlast steps plan empty body. refines plans search queuemeets termination criterion. termination criterion implementedinclude time limit (e.g., stop 5 minutes), memory limit (e.g., stop 256MB),probability threshold (e.g., stop finding plan 0.9 higher probability),lack significant progress (e.g., stop probability success cannot increased). possible specify multiple termination criterion use earliest onebecomes true. termination criterion met plan highest probabilityreturned.Plan refinement operations involve repairing flaws. open condition closedadding new step domain theory, reusing step already plan. unsafelink handled promotion, demotion, separation (when lifted actions used)operations, confrontation (Penberthy & Weld, 1992). techniques partVhpop implementation. Consider step Sk threatening causal link < Si , p, Sj >.Promotion involves adding extra ordering constraint Sk comes Sj (Sj Skadded ORD). Demotion involves adding extra ordering constraint kcomes Si (Sk Si added ORD). Separation involves adding extra inequalityconstraint BIND Sk threatening effect longer unify p. Finally,actions multiple effects, confrontation used making commitmentnon-threatening effects Sk , i.e., effects Sk contain propositionunifies p. Note deterministic domains, action multipleeffects due multiple secondary preconditions (when conditions). probabilistic domains,probabilistic actions always multiple effects.search conducted using A* algorithm guided ranking functionprovides f value. usual plan , f () = g() + h(), g() costplan, h() estimated cost completing it. ranking function usedMerge step algorithm order plans search queue. competitionProbapop used distance based heuristic (ADD) explained next section.flaw selection strategy Select-Flaw method, used Vhpops static, givespriority static open conditions, i.e., condition whose value altered actiondomain theory. flaws plan contain static open conditionsthreats handled next; lowest priority given remaining open conditions.3fiOnder, Whelan & Lifunction Probapop (D, initial, goal, T)returns solution plan, failure** plans Make-Minimal-Plan(initial, goal)** BestPlan null** loop**** termination criterion met return BestPlan**** plans empty return failure**** plan Remove-Front(plans)**** Solution?(plan) return plan**** plans Merge(plans, Refine-Plan(plan))** endfunction Refine-Plan (plan)returns set plans (possibly null)** Flaws(plan) empty**** ProbSuccess (plan) > ProbSuccess (BestPlan)******* BestPlan plan**** plan Reopen-Conditions(plan)** flaw Select-Flaw(plan)** flaw open condition choose:****** return Reuse-Step(plan, flaw)****** return Add-New-Step(plan, flaw)** flaw threat choose:****** return Demotion(plan, flaw)****** return Promotion(plan, flaw)****** return Separation(plan, flaw)****** return Confrontation(plan, flaw)Figure 1: probabilistic POP algorithm.comment heuristics flaw selection techniques following discussioncompetition results.deterministic POP algorithm, plan considered completeflaws, i.e., OPEN = UNSAFE = . probabilistic domains, possibility completeplans insufficient probability success (e.g., 1 ) improved.Probapop improves plans conducting search reopening conditionsfail explained next section. Probapop viewed first searchingplan complete deterministic sense, searching way improveplan. current implementation, discard search queue findingfirst plan subsequent improvements made first complete plan found.future, plan implement multiple search queues order able jumpdifferent plans improvements. Figure 2a, show initial plancorresponds student application domain mentioned first section. openconditions sending forms (forms-sent) getting letter reference (letter-sent).Probapop uses Vhpop guided ranking flaw selection heuristics producecomplete plan 80% probability success shown Figure 2b. straight line showscausal link two actions zigzag line refers causal link plan4fiEngineering Conformant Probabilistic Plannerfragment omitted clarity exposition. Probapop reopens conditionletter-sent (Figure 3a) resumes search using heuristics comeimproved plan involves asking two professors shown Figure 3b. AssumingASK-PROFx action probabilistic effects, probability success0.8 first complete plan 0.8 + 0.2 0.8 second complete plan. Severaliterations reopen search leads Probapop find plan probability0.999997. plan cannot improved single precision arithmetic.INITIALINITIALASK PROF1formssentlettersentformssentlettersentGOALGOAL(a)(b)Figure 2: Starting empty plan finding first plan.INITIALINITIALASK PROF1formssentlettersentASK PROF1lettersentformssentGOALlettersentASK PROF2lettersentGOAL(a)(b)Figure 3: Starting complete plan finding improved plan.3. Distance Based Ranking ProbapopVhpop deterministic partial order-planner described Younes Simmons (2003)supports distance based heuristics provide estimate total number new actionsneeded close open condition. starting search, planner builds planninggraph (Blum & Furst, 1997), literals initial state first level,continues expand graph reaches level goal literals present.planning graph different Graphplans planning graph sense5fiOnder, Whelan & Lirelaxed, i.e., delete lists ignored thus mutex relationships computed (Bonet& Geffner, 2001).order able generate relaxed planning graph multiple probabilisticeffects present, one would need split many plan graphs leavesprobabilistic action. avoid potential blow up, split action domaintheory many deterministic actions number nonempty effect lists. splitaction represents different way original action would work. Figure 4, showaction A1, two probabilistic effects b P Q true, one effect cP true Q false, effect otherwise. split action corresponds oneset non-empty effects. Probapop, plan graph uses split actions, plansconstructed always contain full original action planner correctly assessprobability success. using split actions, compute good estimatenumber actions needed complete plan use distance based heuristics.A1PQprec: P, Qprec: P, QA11~PA12b~Qprec: P, ~Q0.70.3bcA13cFigure 4: Probabilistic action A1 split deterministic actions A1-1, A1-2, A1-3.important distinction deterministic partial-order planning probabilisticpartial-order planning multiple support plan literals. deterministic case,open condition permanently removed list flaws resolved.probabilistic case, reopened planner search additional stepsincrease probability literal. Buridan system implements techniquereopening previously closed conditions complete plan resuming searchfind another complete plan. implementation employs selective reopening (SR)conditions guaranteed achieved reopened. words,literals supported probability 1 reopened. Note checkingprobability literals costly probabilistic plans, save cost performingcheck mandatory assessment complete plans. Obviously, avoiding redundantsearches advantage planner. current implementation reopensupported literals probability less 1. leave selection amongnew set preconditions flaw selection heuristic. implementationcontain probability based heuristics.important note neither split actions selective reopening techniquechange base soundness completeness properties Buridan algorithm. splitactions used relaxed plan graph, reopening technique blockalternatives sought would already covered plan searchqueue.6fiEngineering Conformant Probabilistic Planner4. Probapop IPC-4Probapop among 7 domain-independent planners competed probabilistictrack IPC-4. domain-independent mean planner uses PPDDL description domain solve planning problem employ previously codedcontrol information. Table 1 show brief description planners (Edelkamp,Hoffman, Littman, & Younes, 2004; Younes, Littman, Weissman, & Asmuth, 2005; Bonet& Geffner, 2005; Fern, Yoon, & Givan, 2006; Thiebaux, Gretton, Slaney, Price, & Kabanza,2006). competition conducted follows: planner given set 24problems written probabilistic PDDL (PPDDL) allotted 5 minutes solveproblem. this, server simulated possible way executing plan sendingsequence states starting initial state planners responded stateaction based solution found. 30 simulations conductedproblem. goal-based problems success measured whether goal reachedend simulation. reward-based problems total reward calculated.set 24 problems included types.competition included various domains listed below:Blocksworld: Includes pick put actions action fail.6 problems 5, 8, 11, 15, 18 21 blocks given. goal build onetowers blocks.Colored Blocksworld: actions Blocksworld domain.block one three colors. goal towers specified using existentialquantifiers, e.g., green block table, red block greenblock.Exploding Blocksworld: similar Blocksworld domain first put-downaction permanently destroy bottom object (block table). Replanningrepetition based approaches fail easily due irreversible nature explosion.Boxworld: box transportation problem load, unload, drive fly actions.drive action fail taking truck wrong city.Fileworld: objective includes actions put papers files matchingtype. type paper found using observation actionprobabilistics outcomes.Tireworld: actions include moving several cities tire go flattrip.Towers Hanoise: variation Towers Hanoi problem discsmoved singles doubles discs may slip move.Zeno travel: travel domain includes actions related flying. actionsboarding flying fail.noted competition domains designed full observabilityneeded changed incorporate blind planner. instance, PICKUP action7fiOnder, Whelan & LiPlanner (code)UMass (C)NMRDPP (G1)Classy (J2)FF-rePlan (J3)mGPT (P)Probapop (Q)CERT (R)DescriptionSymbolic heuristic search based symbolic AO* loops (LAO*)symbolic real-time dynamic programming (RTDP)Solving decision problems non-Markovian (and hence Markovian)rewardsApproximate policy iteration inductive machine learning usingrandom-walk problemsDeterministic replanner using Fast ForwardLabeled real-time dynamic programming (LRTDP) lower boundsextracted deterministic relaxations MDPPOP-style plan-space A* search distance based heuristics failureanalysisHeuristic state space search structured policy iteration algorithm,factored MDPs, reachability analysisTable 1: Domain-independent planners listed order competition code.Blocksworld domain precondition requires block pickedheld arm. action two probabilistic effects, one resultingblock held, held. planner assumesobservability, plan involving PICKUP action cannot improved actioncannot executed unless preconditions hold. Thus, Probapop planner cannotinsert second PICKUP action cover case first one fails. helpcompetition organizers, implemented workaround actionsexecuted conditions hold effect rather causing error.Probapop (competition name Q) attempted 4 24 problems. two plannersattempted problems Classy (J2) FF-rePlan (J3).planners attempted 3 10 problems listed Table 2. Probapop attemptedsmall number problems due three reasons. First, started building Probapop,Vhpops version 2.0. performance Vhpop significantly improved bettermemory handling techniques version 2.2 time competitionconvert implementation newer version. Second, competition Blocksworlddomains included universally quantified preconditions supported Vhpop.implementation preconditions including FORALL keyword efficient.Third, implementation disables feature Vhpop allows use multiplesearch queues different heuristics. prohibited us constructing several searchqueues different heuristic using one finishes earliest.therefore pick single heuristic run competition problems. result,picked ADD ranking metric static flaw selection technique ranproblems combination.competition results announced, observed three domainindependent planners, namely Classy (J2), FF-rePlan (J3), mGPT (P),able solve largest Blocksworld problems whereas Probapop able solve5-blocks problem (the competition included domains 5, 8, 11, 15, 18, 218fiEngineering Conformant Probabilistic PlannerPlannerUmass (C)NMRDPP (G1)Classy (J2)FF-rePlan (J3)mGPT (P)Probapop (Q)CERT (R)#problems4718241043bw-nc-r-530303030301130tire-nr30971679tire-r3030303060zeno3030030127Table 2: number successes 30 trials obtained planners usedomain knowledge. problems attempted Probapop (Q) listed.dash means planner attempt problem. Bw-nc-r-5Blocksworld problem 5 blocks. Tire-nr tire-r goal rewardbased problems Tireworld domain. Zeno problem using Zenotravel domain problem.blocks). Therefore, looked ways improving performance Probapopproblems. first reimplemented Probapop Vhpops newer version 2.2. Second,brought language competition Blocksworld domain closer STRIPS.particular, removed FORALL preconditions conditions. example,replaced PPDDL PICK-UP action shown Figure 5 two actions shownFigure 6. However, version upgrade language simplification sufficientenable Probapop solve 8-blocks problem. explained before, Probapops strategyfirst find base plan improve plan possible failure points, thereforefinding base plan crucial. next looked heuristics flaw selectionstrategies make Blocksworld problems solvable. begin discussingexplaining Vhpops ADD heuristic detail.(:action pick-up-block-from* :parameters (?top - block ?bottom)* :effect (when (and (not (= ?top ?bottom)) (on-top-of ?top ?bottom)****************** (forall (?b - block) (not (holding ?b)))****************** (forall (?b - block) (not (on-top-of ?b ?top))))************ (and (decrease (reward) 1)************ (probabilistic 0.75 (and (holding ?top) (not (on-top-of ?top ?bottom)))*************** ******** 0.25 (when (not (= ?bottom table))*************** ************* (and (not (on-top-of ?top ?bottom)) (on-top-of ?top table)))))))Figure 5: PPDDLs PICK-UP actionADD heuristic achieves good performance computing sum step costsopen conditions relaxed planning graph, i.e., heuristic cost plancomputed h() = hadd (OP EN ()). cost achieving literal q levelfirst action achieves q: hadd (q) = minaGA(q) hadd (a) GA(q) 6= , GA(q)9fiOnder, Whelan & Li(:action pick-up* :parameters (?x)* :precondition (and (clear ?x) (ontable ?x) (handempty))* :effect* ** (probabilistic 0.75* **** (and (not (ontable ?x)) (not (clear ?x)) (not (handempty)) (holding ?x))))(:action unstack* :parameters (?x ?y)* :precondition (and (on-top-of ?x ?y) (clear ?x) (handempty))* :effect* ** (probabilistic 0.75* **** (and (holding ?x) (clear ?y) (not (clear ?x)) (not (handempty)) (not (on-top-of ?x ?y)))))Figure 6: Simplified form PPDDLs PICK-UP action.action effect q. Note hadd (q) 0 q holds initially,q never holds. level action first level preconditions become true:hadd (a) = 1 + hadd (P REC(a)). ADDR heuristic modification ADD heuristictakes action reuse account, thus addition conditions described above,heuristic cost literal q 0 plan already contains action achieve q.observed ADDR effective ADD Blocksworld domaintested variety flaw selection strategies implemented Vhpop together ADDR.show flaw selection strategies tried Table 3. adopt notation givenPollack et al. (1997) revised Younes Simmons (2003). notation,strategy ordered list selection criteria LR refers least refinementsfirst, MCadd refers cost computed using ADD, MWadd refers workusing ADD. Open conditions divided three categories use heuristics.static open condition open condition whose literal providedinitial state, i.e., action literal effect. local open condition refersopen conditions recently added action used maintain focusachievement single goal. unsafe open condition refers open condition whosecausal link would threatened.five main strategies prioritize flaws differently. ucpop strategygives priority threats, static strategy gives priority static open conditions, lcfrstrategy handles flaws order least expected cost, mc strategy orders open conditionsrespect cost extracted relaxed planning graph, mw strategy ordersopen conditions respect expected work extracted relaxed planning graph.strategy loc annotation gives priority local open conditions among openconditions, strategy conf annotation gives priority unsafe open conditionsamong open conditions. refer reader paper Younes Simmons(2003) thorough description heuristics well experimental resultsdomains.depict results experiments Blocksworld problems firstthird lines Table 4 (the second fourth lines Tables 4 5 explained later).10fiEngineering Conformant Probabilistic PlannerStrategyucpopstaticlcfrlcfr-loclcfr-conflcfr-loc-confmcmc-dsepmc-locmc-dsepmwmw-dsepmw-locmw-loc-dsepDescription{n,s} LIFO / {o} LIFO{t} LIFO / {n,s} LIFO / {o} LIFO{n,s,o} LR{n,s,l} LR{n,s,u} LR / {o} LR{n,s,u} LR / {l} LR{n,s} LR / {o} MCadd{n} LR / {o} MCadd / {s} LR{n,s} LR / {l} MCadd{n} LR / {l} MCadd / {s} LR{n,s} LR / {o} MWadd{n} LR / {o} MWadd / {s} LR{n,s} LR / {l} MWadd{n} LR / {l} MWadd / {s} LRTable 3: description variety flaw selection strategies Vhpop. n nonseparable threat, separable threat, open condition, staticopen condition, l local open condition, u unsafe open condition.seen lcfr mc strategies work problem 8 blocks.larger problems solvable. actions lifted, tried make searchspace smaller delaying separable threats. Peot Smith (1993) explain delayingseparable threats may result decreased branching factor may manyways add inequality constraints separation. delay might also helpthreat disappear variables bound. modified best working strategies,namely variants mc mw, implemented delay separable threats (in Table 3shown dsep suffix.) show planning times experimentswithout dsep Table 5 (we repeat columns Table 4 comparison).results show time improvement seen 5-blocks problem. problems8 blocks show increase time threat must checked seeseparable. Delaying threats made 8-blocks problem solvable using mc-loc, mw,mw-loc strategies. However, larger problems solvable strategy.results experiments various heuristics strategies showsearch time increases dramatically going 5 8 blocks larger problemssolvable. able find heuristic combination solve larger problems.noticed competition Blocksworld problems list goal towers top bottomplanner spends lot time dead end plans original goal orderpreserved. tower built top bottom, initial goals almost alwaysundone achieve later goals. also concluded interaction cannotdetected heuristics used designed consider subgoalsisolation. Koehler Hoffmann (2000) describe polynomial time algorithmorder goals minimize type undoing. algorithm operates ground11fiOnder, Whelan & Li55o88oucpopstaticlcfr80070001055Klcfrloc6050lcfrconf57010lcfrloc-conf90770mc10013K42Kmcloc5040mw0041Kmwloc2030mwloc-conf220120Table 4: Time (msec) required find base plan Blocksworld problems 58 blocks.55o88omc10013K42Kmc-dsep0073K104Kmc-loc4050mc-loc-dsep203022Kmw0041Kmw-dsep0073K103Kmw-loc2030mw-loc-dsep302022KTable 5: Time (msec) required find base plan delaying separable threatsBlocksworld problems 5 8 blocks.action descriptions generated action schemas implementedFF planning system (Hoffman & Nebel, 2001). used algorithm ordertop-level goals repeated experiments ordering essentially buildstowers bottom top. results ordered goals shown lines 24 tables 4 5. Ordering goals mixed results. example, 8 blocksproblem, made lcfr heuristic usable mw heuristic usable. However,lowest time increased 13K 41K milliseconds larger problems stillsolvable.final strategy combine planning approach used FF plannerPOP-style search. particular, ordered top-level goals using FFs ordering algorithm ran Vhpop n times problems n top level goals. first problemfirst goal Vhpop returned plan, steps simulated findresulting state. second problem resulting state initial state goals 12 goal 1 would preserved redone goal 2 would achieved.used strategy default heuristics Vhpop solve problem 21 blocks,total time 70 milliseconds phases taking 0 milliseconds. KoehlerHoffmann (2000) explain approach works well invertible planning problems,i.e., problems Blocksworld actions reversible. case, tradeoffpossibility less optimal plans plan ith goal set working+ 1st goal. second tradeoff getting several partially-ordered plansbreakpoints problems rather single maximally parallel plan. believeworthwhile work algorithm combines individual plans preserveleast commitment ordering. Possible strategies causally link action preconditions12fiEngineering Conformant Probabilistic Plannerlatest producers use approach Edelkamp (2004) parallelize sequential plansusing critical path analysis.5. Conclusion Future Workpresented design implementation Probapop, partial-order, probabilistic,conformant planner. described distance-based condition-probability basedheuristics used. discussed advantages disadvantages using incremental algorithm goals first ordered submitted one one. shortterm plans involve implementing multiple search queues different base plans reincorporating ADL constructs PPDDL. future work involves three threads. one,looking improving performance Probapop adding probability information planning graph probability open conditions optimisticallyestimated. also considering addition domain specific information (Kuter &Nau, 2005) probabilistic domains. second thread, exploring middleground observability full observability considering POMDP-like problems partial-order setting. Finally, would like incorporate hill climbing techniquesprobabilistic framework. current Probapop 2.0 software availablewww.cs.mtu.edu/nilufer.Acknowledgmentswork supported Research Excellence Fund grant Nilufer OnderMichigan Technological University. thank JAIR IPC-4 special track editor David E.Smith, anonymous reviewers helpful comments.ReferencesBertoli, P., Cimatti, A., & Roveri, M. (2001). Heuristic search + symbolic model checking= efficient conformant planning. Proceedings Eighteenth International JointConference Artificial Intelligence (IJCAI-01), pp. 467472.Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. ArtificialIntelligence, 90, 281300.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (12), 533.Bonet, B., & Geffner, H. (2005). mGPT: probabilistic planner based heuristic search.Journal Artificial Intelligence Research, 24, 933944.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,11, 194.Brafman, R. I., & Hoffmann, J. (2004). Conformant planning via heuristic forward search:new approach. Proceedings Fourteenth International Conference AutomatedPlanning & Scheduling (ICAPS-04), pp. 355364.13fiOnder, Whelan & LiEdelkamp, S. (2004). Extended critical paths temporal planning. Workshop Integrating Planning Scheduling International Conference Automated PlanningScheduling (ICAPS-04), pp. 3845.Edelkamp, S., Hoffman, J., Littman, M., & Younes, H. (2004). International planning competition. Proceedings Fourteenth International Conference Automated Planning& Scheduling (ICAPS-04).Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration policy languagebias: Solving relational Markov decision processes. Journal Artificial IntelligenceResearch, 25.Ferraris, P., & Giunchiglia, E. (2000). Planning satisfiability nondeterministic domains. Proceedings Seventeenth National Conference Artificial Intelligence(AAAI-00), pp. 748754.Hansen, E. A., & Feng, Z. (2000). Dynamic programming POMDPs using factoredstate representation. Proceedings Fifth International Conference ArtificialIntelligence Planning & Scheduling (AIPS-00), pp. 130139.Hoffman, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 253302.Hoffmann, J., & Brafman, R. I. (2005). Contingent planning via heuristic forward searchimplicit belief states. Proceedings Fifteenth International ConferenceAutomated Planning & Scheduling (ICAPS-05), pp. 7180.Hyafil, N., & Bacchus, F. (2003). Conformant probabilistic planning via CSPs. Proceedings Thirteenth International Conference Automated Planning & Scheduling(ICAPS-03), pp. 205214.Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning actingpartially observable stochastic domains. Artificial Intelligence, 101, 99134.Karlsson, L. (2001). Conditional progressive planning uncertainty. ProceedingsEighteenth International Joint Conference Artificial Intelligence (IJCAI-01), pp.431436.Koehler, J., & Hoffmann, J. (2000). reasonable forced goal orderings useagenda-driven planning algorithm. Journal Artificial Intelligence Research,12, 339386.Kushmerick, N., Hanks, S., & Weld, D. S. (1995). algorithm probabilistic planning.Artificial Intelligence, 76, 239286.Kuter, U., & Nau, D. (2005). Using domain-configurable search control probabilistic planners. Proceedings Twentieth National Conference Artificial Intelligence(AAAI-05).Majercik, S. M., & Littman, M. L. (1999). Contingent planning uncertainty viastochastic satisfiability. Proceedings Sixteenth National Conference Artificial Intelligence (AAAI-99), pp. 549556.McDermott, D. (1999). Using regression-match graphs control search planning. Artificial Intelligence, 109 (1-2), 111159.14fiEngineering Conformant Probabilistic PlannerNguyen, X., & Kambhampati, S. (2001). Reviving partial order planning. ProceedingsEighteenth International Joint Conference Artificial Intelligence (IJCAI-01), pp.459464.Onder, N., & Pollack, M. E. (1999). Conditional, probabilistic planning: unifying algorithm effective search control mechanisms. Proceedings SixteenthNational Conference Artificial Intelligence (AAAI-99), pp. 577584.Penberthy, J. S., & Weld, D. S. (1992). UCPOP: sound, complete, partial order plannerADL. Proceedings Third International Conference Principles KnowledgeRepresentation & Reasoning (KR-92), pp. 103114.Peot, M. A., & Smith, D. E. (1993). Threat-removal strategies partial order planning.Proceedings Eleventh National Conference Artificial Intelligence (AAAI-93),pp. 492499.Pollack, M. E., Joslin, D., & Paolucci, M. (1997). Flaw selection strategies partial-orderplanning. Journal Artificial Intelligence Research, 6, 223262.Russell, S. J., & Norvig, P. (2003). Artificial Intelligence: Modern Approach, SecondEdition. Pearson Education, Upper Saddle River, NJ.Smith, D. E., Frank, J., & Jonsson, A. K. (2000). Bridging gap planningscheduling. Knowledge Engineering Review, 15 (1), 4783.Thiebaux, S., Gretton, C., Slaney, J., Price, D., & Kabanza, F. (2006). Decision-theoreticplanning non-Markovian rewards. Journal Artificial Intelligence Research, 25.Younes, H. L., Littman, M. L., Weissman, D., & Asmuth, J. (2005). first probabilistictrack international planning competition. Journal Artificial IntelligenceResearch, 24, 851887.Younes, H., & Simmons, R. (2003). VHPOP: Versatile heuristic partial order planner.Journal Artificial Intelligence Research, 20, 405430.15fiJournal Artificial Intelligence Research 25 (2006) 315348Submitted 08/05; published 03/06Negotiating Socially Optimal Allocations ResourcesUlle Endrissulle@illc.uva.nlILLC, University Amsterdam1018 TV Amsterdam, NetherlandsNicolas Maudetmaudet@lamsade.dauphine.frLAMSADE, Universite Paris-Dauphine75775 Paris Cedex 16, FranceFariba Sadrifs@doc.ic.ac.ukDepartment Computing, Imperial College LondonLondon SW7 2AZ, UKFrancesca Tonift@doc.ic.ac.ukDepartment Computing, Imperial College LondonLondon SW7 2AZ, UKAbstractmultiagent system may thought artificial society autonomous softwareagents apply concepts borrowed welfare economics social choice theoryassess social welfare agent society. paper, study abstractnegotiation framework agents agree multilateral deals exchange bundlesindivisible resources. analyse deals affect social welfare differentinstances basic framework different interpretations concept social welfareitself. particular, show certain classes deals sufficient necessaryguarantee socially optimal allocation resources reached eventually.1. Introductionmultiagent system may thought artificial society autonomous softwareagents. Negotiation distribution resources (or tasks) amongst agents inhabiting society important area research artificial intelligence computerscience (Rosenschein & Zlotkin, 1994; Kraus, 2001; Chavez, Moukas, & Maes, 1997; Sandholm, 1999). number variants problem studied literature.consider case artificial society agents where, begin with, agentholds bundle indivisible resources assigns certain utility. Agents maynegotiate order agree redistribution resourcesbenefit either agent society inhabit.Rather concerned specific strategies negotiation, analyseredistribution resources means negotiation affects well-being agent societywhole. end, make use formal tools measuring social welfare developedwelfare economics social choice theory (Moulin, 1988; Arrow, Sen, & Suzumura,2002). multiagent systems literature, utilitarian interpretation conceptsocial welfare usually taken granted (Rosenschein & Zlotkin, 1994; Sandholm, 1999;Wooldridge, 2002), i.e. whatever increases average welfare agents inhabitingsociety taken beneficial society well. case welfare economics,c2006AI Access Foundation. rights reserved.fiEndriss, Maudet, Sadri, & Toniinstance, different notions social welfare studied comparedother. Here, concept egalitarian social welfare takes particularly prominentrole (Sen, 1970; Rawls, 1971; Moulin, 1988; Arrow et al., 2002). model, socialwelfare tied individual welfare weakest member society, facilitatesincorporation notion fairness resource allocation process.discussion respective advantages drawbacks different notions social welfaresocial sciences tends dominated ethical considerations,1 contextsocieties artificial software agents choice suitable formal tool modellingsocial welfare boils clear-cut (albeit necessarily simple) technical designdecision (Endriss & Maudet, 2004). Indeed, different applications may call differentsocial criteria. instance, application studied Lematre, Verfaillie, Bataille(1999), agents need agree access earth observation satellitefunded jointly owners agents, important onereceives fair share common resource. Here, society governed egalitarianprinciples may appropriate. electronic commerce application runningInternet agents little commitments towards other,hand, egalitarian principles seem little relevance. scenario, utilitarian socialwelfare would provide appropriate reflection overall profit generated. Besidesutilitarian egalitarian social welfare, also going discuss notions ParetoLorenz optimality (Moulin, 1988), well envy-freeness (Brams & Taylor, 1996).paper, study effect negotiation resources societynumber different interpretations concept social welfare. particular, showcertain classes deals regarding exchange resources allow us guaranteesocially optimal allocation resources reached eventually. convergence resultsmay interpreted emergence particular global behaviour (at level society)reaction local behaviour governed negotiation strategies individual agents(which determine kinds deals agents prepared accept). work describedcomplementary large body literature mechanism design game-theoreticalmodels negotiation multiagent systems (see e.g. Rosenschein & Zlotkin, 1994; Kraus,2001; Fatima, Wooldridge, & Jennings, 2004). work typically concernednegotiation local level (how design mechanisms provide incentiveindividual agents adopt certain negotiation strategy?), address negotiationglobal level analysing actions taken agents locally affect overall systemsocial point view.shall see, truly multilateral deals involving number agents wellnumber resources may necessary able negotiate socially optimal allocationsresources. certainly true long use arbitrary utility functions modelpreferences individual agents. application domains, however, utilityfunctions may assumed subject certain restrictions (such additive),able obtain stronger results show also structurally simpler classes deals(in particular, deals involving single resource time) sufficient negotiatesocially optimal allocations. Nevertheless, seemingly strong restrictions agents1. famous example Rawls veil ignorance, thought experiment designed establish constitutessociety (Rawls, 1971).316fiNegotiating Socially Optimal Allocations Resourcesutility functions (such restriction dichotomous preferences) able showreduction structural complexity negotiation possible.approach multiagent resource allocation distributed nature. general,allocation procedure used find suitable allocation resources could either centralised distributed. centralised case, single entity decides final allocationresources amongst agents, possibly elicited agents preferences alternative allocations. Typical examples combinatorial auctions (Cramton, Shoham, &Steinberg, 2006). central entity auctioneer reporting preferencestakes form bidding. truly distributed approaches, hand, allocationsemerge result sequence local negotiation steps. approachesadvantages disadvantages. Possibly important argument favour auctionbased mechanisms concerns simplicity communication protocols required implement mechanisms. Another reason popularity centralised mechanismsrecent push design powerful algorithms combinatorial auctions that,first time, perform reasonably well practice (Fujishima, Leyton-Brown, & Shoham,1999; Sandholm, 2002). course, techniques are, principle, also applicabledistributed case, research area yet reached level maturitycombinatorial auctions. important argument centralised approachesmay difficult find agent could assume role auctioneer (forinstance, view computational capabilities view trustworthiness).line research pursued paper inspired Sandholms worksufficient necessary contract (i.e. deal) types distributed task allocation (Sandholm,1998). Since then, developed present authors, colleagues,others context resource allocation problems (Bouveret & Lang, 2005; Chevaleyre,Endriss, Estivie, & Maudet, 2004; Chevaleyre, Endriss, Lang, & Maudet, 2005a; Chevaleyre,Endriss, & Maudet, 2005b; Dunne, 2005; Dunne, Laurence, & Wooldridge, 2004; Dunne,Wooldridge, & Laurence, 2005; Endriss & Maudet, 2004, 2005; Endriss, Maudet, Sadri,& Toni, 2003a, 2003b). particular, extended Sandholms framework alsoaddressing negotiation systems without compensatory side payments (Endriss et al., 2003a),well agent societies concept social welfare given different interpretationutilitarian programme (Endriss et al., 2003b; Endriss & Maudet, 2004).present paper provides comprehensive overview fundamental results, mostlyconvergence optimal allocation respect different notions social welfare,active timely area ongoing research.remainder paper organised follows. Section 2 introduces basic negotiation framework resource reallocation going consider. gives definitionscentral notions allocation, deal, utility, discusses possible restrictionsclass admissible deals (both structural terms acceptability individualagents). Section 2 also introduces various concepts social preference goingconsider paper. Subsequent sections analyse specific instances basic negotiation framework (characterised, particular, different criteria acceptabilityproposed deal) respect specific notions social welfare. first instance,agents assumed rational (and myopic) sense never accepting dealwould result negative payoff. Section 3 analyses first variant modelrational negotiation, allows monetary side payments increase range317fiEndriss, Maudet, Sadri, & Toniacceptable deals. shall see, model facilitates negotiation processes maximiseutilitarian social welfare. side payments possible, cannot guarantee outcomesmaximal social welfare, still possible negotiate Pareto optimal allocations.variant rational model studied Section 4. Section 3 4 also investigate restrictions range utility functions agents may use modelpreferences affect convergence results.second part paper apply methodology agent societiesconcept social welfare given different kind interpretation commonlycase multiagent systems literature. Firstly, Section 5 analyse frameworkresource allocation negotiation context egalitarian agent societies.Section 6 discusses variant framework combines ideas utilitarianegalitarian programme enables agents negotiate Lorenz optimal allocationsresources. Finally, Section 7 introduces idea using elitist model social welfareapplications societies agents merely means enabling least one agentachieve goal. section also reviews concept envy-freeness discussesways measuring different degrees envy.Section 8 summarises results concludes brief discussion conceptwelfare engineering, i.e. idea choosing tailor-made definitions social welfaredifferent applications designing agents behaviour profiles accordingly.2. Preliminariesbasic scenario resource allocation negotiation studied paperartificial society inhabited number agents, initially holds certainnumber resources. agents typically ascribe different values (utilities) different bundles resources. may engage negotiation agree reallocationresources, example, order improve respective individual welfare(i.e. increase utility). Furthermore, assume interest systemdesigner distributed negotiation processes somehow also result positivepayoff society whole.2.1 Basic Definitionsinstance abstract negotiation framework consists finite set (at least two)agents finite set resources R. Resources indivisible non-sharable.allocation resources partitioning R amongst agents A.Definition 1 (Allocations) allocationresources function subsetsR A(i) A(j) = { } 6= j iA A(i) = R.example, given allocation A(i) = {r3 , r7 }, agent would resources r3r7 . Given particular allocation resources, agents may agree (multilateral) dealexchange resources currently hold. general case, numbersagents resources could involved single deal. abstract point view,deal takes us one allocation resources next. is, may characterisedeal pair allocations.318fiNegotiating Socially Optimal Allocations ResourcesDefinition 2 (Deals) deal pair = (A, A0 ) A0 allocationsresources 6= A0 .set agents involved deal = (A, A0 ) given = {i | A(i) 6= A0 (i)}.composition two deals defined follows: 1 = (A, A0 ) 2 = (A0 , A00 ),1 2 = (A, A00 ). given deal composition two deals concern disjoint setsagents, deal said independently decomposable.Definition 3 (Independently decomposable deals) deal called independentlydecomposable iff exist deals 1 2 = 1 2 A1 A2 = { }.Observe = (A, A0 ) independently decomposable exists intermediateallocation B different A0 intersection {i | A(i) 6= B(i)}{i | B(i) 6= A0 (i)} empty, i.e. union {i | A(i) = B(i)}{i | B(i) = A0 (i)} full set agents A. Hence, = (A, A0 ) independentlydecomposable implies exists allocation B different A0either B(i) = A(i) B(i) = A0 (i) agents (we going use factproofs necessity theorems later on).value agent ascribes particular set resources R modelledmeans utility function, is, function sets resources real numbers.going consider general utility functions (without restrictions) severalspecific classes functions.Definition 4 (Utility functions) Every agent equipped utility functionui : 2R R. going consider following restricted classes utility functions:ui non-negative iff ui (R) 0 R R.ui positive iff non-negative ui (R) 6= 0 R R R 6= { }.ui monotonic iff R1 R2 implies ui (R1 ) ui (R2 ) R1 , R2 R.Pui additive iff ui (R) = rR ui ({r}) R R.ui 0-1 function iff additive ui ({r}) = 0 ui ({r}) = 1 r R.ui dichotomous iff ui (R) = 0 ui (R) = 1 R R.Recall that, given allocation A, set A(i) bundle resources held agentsituation. usually going abbreviate ui (A) = ui (A(i)) utility valueassigned agent bundle.2.2 Deal Types Rationality Criteriapaper investigate kinds negotiation outcomes agents achieve usingdifferent classes deals. class deals may characterised structural constraints(number agents resources involved, etc.) rationality constraints (relatingchanges utility experienced agents involved).Following Sandholm (1998), distinguish number structurally different typesdeals. basic 1-deals, single item passed one agent another.319fiEndriss, Maudet, Sadri, & ToniDefinition 5 (1-deals) 1-deal deal involving reallocation exactly one resource.corresponds classical form contract typically found Contract Netprotocol (Smith, 1980). Deals one agent passes set resources anotheragent called cluster deals. Deals one agent gives single item another agentreturns another single item called swap deals. Sometimes also necessaryexchange resources two agents. Sandholms terminology,multiagent deal deal could involve number agents, agent passesone resource agents taking part. Finally, deals combinefeatures cluster multiagent deal type called combined deals Sandholm.could involve number agents number resources. Therefore, everydeal , sense Definition 2, combined deal. remainder paper,speaking deals without specifying type, always going refercombined deals (without structural restrictions).2agent may may find particular deal acceptable. Whether agentaccept given deal depends rationality criterion applies evaluating deals.selfish agent may, instance, accept deals = (A, A0 ) strictly improvepersonal welfare: ui (A) < ui (A0 ). call criteria this, dependutilities agent question, personal rationality criteria. wantadmit arbitrary rationality criteria, classes deals characterised usingpersonal rationality criteria alone somewhat narrow purposes. Instead,going consider rationality criteria local sense dependingutility levels agents involved deal concerned.Definition 6 (Local rationality criteria) class deals said characterisedlocal rationality criterion iff possible define predicate 2ARRdeal = (A, A0 ) belongs iff ({(i, ui (A), ui (A0 )) | }) holds true.is, mapping set triples one agent name two reals (utilities)truth values. locality aspect comes applying set triplesagents whose bundle changes . Therefore, instance, class dealsincrease utility previously poorest agent characterisable localrationality criterion (because condition checked inspecting utilitiesagents system).2.3 Socially Optimal Allocations Resourcesalready mentioned introduction, may think multiagent system societyautonomous software agents. agents make local decisions dealspropose accept, also analyse system global societal pointview may thus prefer certain allocations resources others. end, welfareeconomics provides formal tools assess distribution resources amongstmembers society affects well-being society whole (Sen, 1970; Moulin, 1988;Arrow et al., 2002).2. ontology deal types discussed is, course, exhaustive. would, instance, alsointerest consider class bilateral deals (involving exactly two agents number items).320fiNegotiating Socially Optimal Allocations ResourcesGiven preference profiles individual agents society (which, framework, represented means utility functions), social welfare orderingalternative allocations resources formalises notion societys preferences. Nextgoing formally introduce important social welfare orderings consideredpaper (some additional concepts social welfare discussed towards endpaper). cases, social welfare best defined terms collective utility function.One example notion utilitarian social welfare.Definition 7 (Utilitarian social welfare) utilitarian social welfare swu (A)allocation resources defined follows:Xswu (A) =ui (A)iAObserve maximising collective utility function swu amounts maximisingaverage utility enjoyed agents system. Asking maximal utilitarian socialwelfare strong requirement. somewhat weaker concept Pareto optimality. allocation Pareto optimal iff allocation higher utilitariansocial welfare would worse agents system (i.e. wouldstrictly better least one agent without worse others).Definition 8 (Pareto optimality) allocation called Pareto optimal iffallocation A0 swu (A) < swu (A0 ) ui (A) ui (A0 ) A.first goal egalitarian society increase welfare weakestmember (Rawls, 1971; Sen, 1970). words, measure social welfaresociety measuring welfare agent currently worst off.Definition 9 (Egalitarian social welfare) egalitarian social welfare swe (A)allocation resources defined follows:swe (A) = min{ui (A) | A}egalitarian collective utility function swe gives rise social welfare orderingalternative allocations resources: A0 strictly preferred iff swe (A) < swe (A0 ).ordering sometimes called maximin-ordering. maximin-ordering takesaccount welfare currently weakest agent, insensitive utility fluctuationsrest society. allow finer distinction social welfare differentallocations introduce so-called leximin-ordering.society n agents, let {u1 , . . . , un } set utility functionssociety. every allocation determines utility vector hu1 (A), . . . , un (A)i length n.rearrange elements vector increasing order obtain ordered utilityvector allocation A, going denote ~u(A). number ~ui (A) ithelement vector (for 1 |A|). is, ~u1 (A) instance, utility valueassigned allocation currently weakest agent. declare lexicographicordering vectors real numbers (such ~u(A)) usual way: ~x lexicographicallyprecedes ~y iff ~x (proper) prefix ~y ~x ~y share common (proper) prefix lengthk (which may 0) ~xk+1 < ~yk+1 .321fiEndriss, Maudet, Sadri, & ToniDefinition 10 (Leximin-ordering) leximin-ordering alternative allocationsresources defined follows:A0~u(A) lexicographically precedes ~u(A0 )iffwrite A0 iff either A0 ~u(A) = ~u(A0 ). allocation resources calledleximin-maximal iff allocation A0 A0 .Finally, introduce concept Lorenz domination, social welfare orderingcombines utilitarian egalitarian aspects social welfare. basic idea endorsedeals result improvement respect utilitarian welfare without causingloss egalitarian welfare, vice versa.Definition 11 (Lorenz domination) Let A0 allocations society nagents. Lorenz dominated A0 iffkX~ui (A)i=1kX~ui (A0 )i=1k 1 k n and, furthermore, inequality strict least one k.k 1 k n, sum referred definition sumutility values assigned respective allocation resources k weakest agents.k = 1, sum equivalent egalitarian social welfare allocation. k = n,equivalent utilitarian social welfare. allocation resources called Lorenzoptimal iff Lorenz dominated allocation.illustrate social welfare concepts (and use ordered utilityvectors) means example. Consider society three agents two resources,agents utility functions given following table:u1 ({ }) = 0u1 ({r1 }) = 5u1 ({r2 }) = 3u1 ({r1 , r2 }) = 8u2 ({ }) = 0u2 ({r1 }) = 4u2 ({r2 }) = 2u2 ({r1 , r2 }) = 17u3 ({ }) = 0u3 ({r1 }) = 2u3 ({r2 }) = 6u3 ({r1 , r2 }) = 7First all, observe egalitarian social welfare 0 possible allocationscenario, least one agents would get resources all. Letallocation agent 2 holds full bundle resources. Observeallocation maximal utilitarian social welfare. corresponding utility vectorh0, 17, 0i, i.e. ~u(A) = h0, 0, 17i. Furthermore, let A0 allocation agent 1 getsr1 , agent 2 gets r2 , agent 3 content empty bundle. getordered utility vector h0, 2, 5i. initial element either vector 0, 0 < 2, i.e.~u(A) lexicographically precedes ~u(A0 ). Hence, get A0 , i.e. A0 would sociallypreferred allocation respect leximin-ordering. Furthermore, A0Pareto optimal neither Lorenz-dominated other. Starting allocation A0 ,agents 1 2 swapping respective bundles would result allocationordered utility vector h0, 3, 4i, i.e. move would result Lorenz improvement.322fiNegotiating Socially Optimal Allocations Resources3. Rational Negotiation Side Paymentssection, going discuss first instance general framework resourceallocation negotiation set earlier. particular variant, shall refermodel rational negotiation side payments (or simply money), equivalentframework put forward Sandholm agents negotiate order reallocatetasks (Sandholm, 1998). variant framework, aim negotiateallocations maximal utilitarian social welfare.3.1 Individual Rationalityinstance negotiation framework, deal may accompanied numbermonetary side payments compensate agents involved accepting lossutility. Rather specifying pair agents much money formersupposed pay latter, simply say much money agent either paysreceives. modelled using call payment function.Definition 12 (Payment functions) payment function function p realnumbers satisfying following condition:Xp(i) = 0iAHere, p(i) > 0 means agent pays amount p(i), p(i) < 0 meansreceives amount p(i). definition payment function, sum payments0, i.e. overall amount money present system change.3rational negotiation model, agents self-interested sense proposing accepting deals strictly increase welfare (for justificationapproach refer Sandholm, 1998). myopic notion individual rationality mayformalised follows.Definition 13 (Individual rationality) deal = (A, A0 ) called individually rationaliff exists payment function p ui (A0 ) ui (A) > p(i) A, exceptpossibly p(i) = 0 agents A(i) = A0 (i).is, agent prepared accept deal iff pay less gainutility get paid loss utility, respectively. agentsaffected deal, i.e. case A(i) = A0 (i), may payment all. example,ui (A) = 8 ui (A0 ) = 5, utility agent would reduced 3 unitsaccept deal = (A, A0 ). Agent agree deal accompaniedside payment 3 units; is, payment function p satisfies 3 > p(i).given deal, usually range possible side payments. agentsmanage agree particular one matter consideration abstract leveldiscussing framework here. assume deal go aheadlong exists suitable payment function p. point3. overall amount money present system stays constant throughout negotiation process,makes sense take account evaluation social welfare.323fiEndriss, Maudet, Sadri, & Toniassumption may justified circumstances. instance, utility functionspublicly known agents risk-takers, potential deal may identifiedsuch, agents may understate interest deal ordermaximise expected payoff (Myerson & Satterthwaite, 1983). Therefore, theoreticalresults reachability socially optimal allocations reported applyassumption strategic considerations prevent agents makingmutually beneficial deals.3.2 Exampleexample, consider system two agents, agent 1 agent 2, set tworesources R = {r1 , r2 }. following table specifies values utility functions u1u2 every subset {r1 , r2 }:u1 ({ }) = 0u1 ({r1 }) = 2u1 ({r2 }) = 3u1 ({r1 , r2 }) = 7u2 ({ }) = 0u2 ({r1 }) = 3u2 ({r2 }) = 3u2 ({r1 , r2 }) = 8Also suppose agent 1 initially holds full set resources {r1 , r2 } agent 2resources begin with.utilitarian social welfare initial allocation 7, could 8, namelyagent 2 resources. going see next, simple class 1-deals alonealways sufficient guarantee optimal outcome negotiation process (if agentsabide individual rationality criterion acceptability deal). example,possible 1-deals would pass either r1 r2 agent 1 agent 2. eithercase, loss utility incurred agent 1 (5 4, respectively) would outweigh gainagent 2 (3 either deal), payment function would make dealsindividually rational. cluster deal passing {r1 , r2 } agent 1 2,hand, would individually rational agent 2 paid agent 1 amount of, say, 7.5 units.Similarly example above, also construct scenarios swap dealsmultiagent deals necessary (i.e. cluster deals alone would sufficientguarantee maximal social welfare). also follows Theorem 2, goingpresent later section. Several concrete examples given Sandholm (1998).3.3 Linking Individual Rationality Social Welfarefollowing result, first stated form Endriss et al. (2003a), says deal (withmoney) individually rational iff increases utilitarian social welfare. mainly goinguse lemma give simple proof Sandholms main result sufficient contracttypes (Sandholm, 1998), also found useful applications right (Dunneet al., 2005; Dunne, 2005; Endriss & Maudet, 2005).Lemma 1 (Individually rational deals utilitarian social welfare) deal =(A, A0 ) individually rational iff swu (A) < swu (A0 ).324fiNegotiating Socially Optimal Allocations ResourcesProof. : definition, = (A, A0 ) individually rational iff exists paymentfunction p ui (A0 ) ui (A) > p(i) holds A, except possibly p(i) = 0case A(i) = A0 (i). add inequations agents get:X(ui (A0 ) ui (A)) >iAXp(i)iAdefinition payment function, righthand side equates 0 while, definitionutilitarian social welfare, lefthand side equals swu (A0 ) swu (A). Hence, really getswu (A) < swu (A0 ) claimed.: let swu (A) < swu (A0 ). show = (A, A0 ) individuallyrational deal. done prove exists payment function pui (A0 ) ui (A) > p(i) A. define function p : R follows:p(i) = ui (A0 ) ui (A)swu (A0 ) swu (A)|A|(for A)PFirst, observe p really payment function, get iA p(i) = 0. alsoget ui (A0 ) ui (A) > p(i) A, swu (A0 ) swu (A0 ) > 0. Hence,must indeed individually rational deal.2Lemma 1 suggests function swu indeed provide appropriate measuresocial well-being societies agents use notion individual rationality (as givenDefinition 13) guide behaviour negotiation. also shows individualrationality indeed local rationality criterion sense Definition 6.3.4 Maximising Utilitarian Social Welfarenext aim show sequence deals rational negotiation modelside payments converge allocation maximal utilitarian social welfare;is, class individually rational deals (as given Definition 13) sufficientguarantee optimal outcomes agent societies measuring welfare according utilitarianprogramme (Definition 7). originally shown Sandholm (1998)context framework rational agents negotiate order reallocate tasksglobal aim minimise overall costs carrying tasks.Theorem 1 (Maximal utilitarian social welfare) sequence individually rational deals eventually result allocation maximal utilitarian social welfare.Proof. Given set agents well set resources R requiredfinite, finite number distinct allocations resources. Furthermore, Lemma 1, individually rational deal strictly increase utilitarian socialwelfare. Hence, negotiation must terminate finite number deals. sakecontradiction, assume terminal allocation maximal utilitariansocial welfare, i.e. exists another allocation A0 swu (A) < swu (A0 ). then,Lemma 1, deal = (A, A0 ) would individually rational thereby possible,contradicts earlier assumption terminal allocation.2325fiEndriss, Maudet, Sadri, & Tonifirst sight, result may seem almost trivial. notion multilateral deal withoutstructural restrictions powerful one. single deal allows numberresources moved number agents. point view,particularly surprising always reach optimal allocation (even singlestep!). Furthermore, finding suitable deal complex task, may alwaysviable practice. crucial point Theorem 1 sequence dealsresult optimal allocation. is, whatever deals agreed early stagesnegotiation, system never get stuck local optimum finding allocationmaximal social welfare remains option throughout (provided, course, agentsactually able identify deal theoretically possible). Given restrictiondeals individually rational agents involved, social welfare must increaseevery single deal. Therefore, negotiation always pays off, even stop earlydue computational limitations.issue complexity still important one. full range deals largemanaged practice, important investigate close get findingoptimal allocation restrict set allowed deals certain simple patterns.Andersson Sandholm (2000), instance, conducted number experimentssequencing certain contract/deal types reach best possible allocations withinlimited amount time. complexity-theoretic analysis problem decidingwhether possible reach optimal allocation means structurally simple typesdeals (in particular 1-deals), refer recent work Dunne et al. (2005).3.5 Necessary Dealsnext theorem improves upon Sandholms main result regarding necessary contracttypes (Sandholm, 1998), extending cases either utility functionsmonotonic utility functions dichotomous.4 Sandholms original result, translatedterminology, states system (consisting set agentsset resources R) (not independently decomposable) deal system,possible construct utility functions choose initial allocation resourcesnecessary reach optimal allocation, agents agree individuallyrational deals. findings insufficiency certain types contracts reportedSandholm (1998) may considered corollaries this. instance, fact that,say, cluster deals alone sufficient guarantee optimal outcomes followstheorem take particular swap deal system question.Theorem 2 (Necessary deals side payments) Let sets agents resources fixed. every deal independently decomposable, existutility functions initial allocation sequence individually rationaldeals leading allocation maximal utilitarian social welfare must include .continues case even either utility functions required monotonicutility functions required dichotomous.4. fact, theorem sharpens also also corrects mistake previous expositionsresult (Sandholm, 1998; Endriss et al., 2003a), restriction deals independentlydecomposable omitted.326fiNegotiating Socially Optimal Allocations ResourcesProof. Given set agents set resources R, let = (A, A0 ) 6= A0deal system. need show collection utility functionsinitial allocation necessary reach allocation maximal socialwelfare. would case A0 maximal social welfare, second highestsocial welfare, initial allocation resources.first prove existence collection functions case utilityfunctions required monotonic. Fix 0 < < 1. 6= A0 ,must agent j A(j) 6= A0 (j). define utility functions uiagents sets resources R R follows:|R| + R = A0 (i) (R = A(i) 6= j)ui (R) =|R|otherwiseObserve ui monotonic utility function every A. get swu (A0 ) = |R|+|A|swu (A) = swu (A0 ) . = (A, A0 ) individually decomposable,exists allocation B different A0 either B(i) = A(i) B(i) =A0 (i) agents A. Hence, swu (B) swu (A) allocation B.is, A0 (unique) allocation maximal social welfare allocationhigher social welfare A. Therefore, make initial allocation= (A, A0 ) would deal increasing social welfare. Lemma 1, meanswould individually rational (and thereby possible) deal. Hence,indeed necessary achieve maximal utilitarian social welfare.proof case dichotomous utility functions similar; needshow suitable collection dichotomous utility functions constructed. Again,let j agent A(j) 6= A0 (j). use following collection functions:1 R = A0 (i) (R = A(i) 6= j)ui (R) =0 otherwiseget swu (A0 ) = |A|, swu (A) = swu (A0 )1 swu (B) swu (A) allocationsB. Hence, socially optimal and, initial allocation, woulddeal individually rational.2stress set deals independently decomposable includesdeals involving number agents and/or number resources. Hence, Theorem 2,negotiation protocol puts restrictions structural complexity dealsmay proposed fail guarantee optimal outcomes, even constraintseither time computational resources. emphasises high complexitynegotiation framework (see also Dunne et al., 2005; Chevaleyre et al., 2004; Endriss &Maudet, 2005; Dunne, 2005). fact necessity (almost) full range dealspersists, even utility functions subject certain restrictions makes resulteven striking. true particular case dichotomous functions,sense simplest class utility functions (as distinguishgood bad bundles).see restriction deals independently decomposable matters,consider scenario four agents two resources. deal moving r1agent 1 agent 2, r2 agent 3 agent 4 individually rational,327fiEndriss, Maudet, Sadri, & Tonieither one two subdeals moving either r1 agent 1 agent 2 r2agent 3 agent 4. Hence, deal (which independently decomposable) cannotnecessary sense Theorem 2 (with reference proof above, caseallocations B either B(i) = A(i) B(i) = A0 (i) agents A,i.e. could get swu (B) = swu (A0 )).3.6 Additive ScenariosTheorem 2 negative result, shows deals complexity mayrequired guarantee optimal outcomes negotiation. partly consequencehigh degree generality framework. Section 2.1, defined utility functionsarbitrary functions sets resources real numbers. many application domainsmay unnecessarily general even inappropriate may able obtainstronger results specific classes utility functions subject certain restrictions.course, already seen case either monotonicity (possiblynatural restriction) dichotomy (possibly severe restriction).Here, going consider case additive utility functions, appropriate domains combining resources result synergy effects (insense increasing agents welfare). refer systems agents additiveutility functions additive scenarios. following theorem shows additivescenarios 1-deals sufficient guarantee outcomes maximal social welfare.5Theorem 3 (Additive scenarios) additive scenarios, sequence individually rational 1-deals eventually result allocation maximal utilitarian social welfare.Proof. Termination shown Theorem 1. going show that, whenevercurrent allocation maximal social welfare, still possible 1-dealindividually rational.additive domains, utilitarian social welfare given allocation may computedadding appropriate utility values single resources R.allocation A, let fA function mapping resource r R agentholds r situation (that is, fA (r) = iff r A(i)). utilitarian social welfareallocation given following formula:Xswu (A) =ufA (r) ({r})rRsuppose negotiation terminated allocationindividually rational 1-deals possible. Furthermore, sake contradiction, assumeallocation maximal social welfare, i.e. exists another allocationA0 swu (A) < swu (A0 ). then, characterisation social welfareadditive scenarios, must least one resource r R ufA (r) ({r}) <ufA0 (r) ({r}). is, 1-deal passing r agent fA (r) agent fA0 (r) wouldincrease social welfare. Therefore, Lemma 1, must individually rational deal,i.e. contrary earlier assumption, cannot terminal allocation. Hence, mustallocation maximal utilitarian social welfare.25. also observed T. Sandholm (personal communication, September 2002).328fiNegotiating Socially Optimal Allocations Resourcesconclude section briefly mentioning two recent results extend,different ways, result stated Theorem 3 (a detailed discussion, however, wouldbeyond scope present paper). first results shows rationaldeals involving k resources sufficient convergence allocationmaximal social welfare whenever utility functions additively separablerespect common partition R i.e. synergies across different parts partitionpossible overall utility defined sum utilities different setspartition (Fishburn, 1970) set partition k elements(Chevaleyre et al., 2005a).second result concerns maximality property utility functions respect1-deals. Chevaleyre et al. (2005b) show class modular utility functions,slightly general class additive functions considered (namely,possible assign non-zero utility empty bundle), maximal senseclass functions strictly including class modular functions would still possibleguarantee agents using utility functions larger class negotiatingindividually rational 1-deals eventually reach allocation maximal utilitariansocial welfare cases.4. Rational Negotiation without Side Paymentsimplicit assumption made framework presented far everyagent got unlimited amount money available able pay agentswhenever required deal would increase utilitarian social welfare. Concretely,initial allocation A0 allocation maximal utilitarian social welfare,agent may require amount money difference ui (A0 ) ui (A)able get negotiation process. context task contracting,framework proposed originally (Sandholm, 1998), may justifiable,resource allocation problems seems questionable make assumptionsunlimited availability one particular resource, namely money. section, thereforeinvestigate extent theoretical results discussed previous section persistapply consider negotiation processes without monetary side payments.4.1 Examplescenario without money, is, allow compensatory payments,cannot always guarantee outcome maximal utilitarian social welfare. see this,consider following simple problem system two agents, agent 1 agent 2,single resource r. agents utility functions defined follows:u1 ({ }) = 0u1 ({r}) = 4u2 ({ }) = 0u2 ({r}) = 7suppose agent 1 initially owns resource. passing r agent 1 agent 2would increase utilitarian social welfare amount 3. framework money,agent 2 could pay agent 1, say, amount 5.5 units deal would individuallyrational them. Without money (i.e. p 0), however, individually rationaldeal possible negotiation must terminate non-optimal allocation.329fiEndriss, Maudet, Sadri, & Tonimaximising social welfare generally possible, instead going investigatewhether Pareto optimal outcome (see Definition 8) possible framework withoutmoney, types deals sufficient guarantee this.4.2 Cooperative Rationalitybecome clear due course, order get desired convergence result, needrelax notion individual rationality little. framework without money,also want agents agree deal, least maintains utility (that is, strictincrease necessary). However, still going require least one agent strictlyincrease utility. could, instance, agent proposing deal question.call deals conforming criterion cooperatively rational.6Definition 14 (Cooperative rationality) deal = (A, A0 ) called cooperatively rational iff ui (A) ui (A0 ) agent j uj (A) < uj (A0 ).analogy Lemma 1, still swu (A) < swu (A0 ) deal = (A, A0 )cooperatively rational, vice versa. call instance negotiation frameworkdeals cooperatively rational (and hence include monetary component)model rational negotiation without side payments.4.3 Ensuring Pareto Optimal Outcomesnext theorem show, class cooperatively rational deals sufficientguarantee Pareto optimal outcome money-free negotiation. constitutes analogueTheorem 1 model rational negotiation without side payments.Theorem 4 (Pareto optimal outcomes) sequence cooperatively rational dealseventually result Pareto optimal allocation resources.Proof. Every cooperatively rational deal strictly increases utilitarian social welfare (thisneed condition least one agent behaves truly individually rationaldeal). Together fact finitely many different allocationsresources, implies negotiation process eventually terminate.sake contradiction, assume negotiation ends allocation A, Paretooptimal. latter means exists another allocation A0 swu (A) < swu (A0 )ui (A) ui (A0 ) A. ui (A) = ui (A0 ) A, alsoswu (A) = swu (A0 ); is, must least one j uj (A) < uj (A0 ).deal = (A, A0 ) would cooperatively rational, contradicts assumptionterminal allocation.2Observe proof would gone deals required strictlyrational (without side payments), would necessitate ui (A) < ui (A0 ) A.Cooperative rationality means, instance, agents would prepared give awayresources assign utility value 0, without expecting anything return.6. Note cooperatively rational agents still essentially rational. willingness cooperateextends cases benefit others without loss utility themselves.330fiNegotiating Socially Optimal Allocations Resourcesframework money, another agent could always offer agent infinitesimallysmall amount money, would accept deal.Therefore, proposed weakened notion rationality seems indeed reasonableprice pay giving money.4.4 Necessity Resultnext result shows, also framework without side payments, dealsstructural complexity may necessary order able guarantee optimal outcomenegotiation.7 Theorem 5 improves upon previous results (Endriss et al., 2003a)showing necessity property persists also either utility functions belongclass monotonic functions utility functions belong class dichotomousfunctions.Theorem 5 (Necessary deals without side payments) Let sets agents resources fixed. every deal independently decomposable, existutility functions initial allocation sequence cooperatively rationaldeals leading Pareto optimal allocation would include . continuescase even either utility functions required monotonic utilityfunctions required dichotomous.Proof. details proof omitted possible simply reuse constructionused proof Theorem 2. Observe utility functions defined alsoguarantee ui (A) ui (A0 ) A, i.e. Pareto optimal, A0 is.make initial allocation, = (A, A0 ) would cooperatively rationaldeal (as every deal would decrease social welfare).24.5 0-1 Scenariosconclude study rational negotiation framework without side paymentsidentifying class utility functions able achieve reduction structuralcomplexity.8 Consider scenario agents use additive utility functions assigneither 0 1 every single resource (this call 0-1 functions).9 mayappropriate simply wish distinguish whether agent needs particularresource (to execute given plan, example). is, instance, caseagents defined work Sadri, Toni, Torroni (2001). following theoremshows, 0-1 scenarios (i.e. systems utility functions 0-1 functions),7. theorem corrects mistake original statement result (Endriss et al., 2003a),restriction deals independently decomposable omitted.8. dichotomous functions special case non-negative functions, full range (not independently decomposable) deals also necessary scenarios non-negative functions. Interestingly,changes restrict positive utility functions. result Theorem 5 wouldhold anymore, deal would involve particular agent (with positive utility function)giving away resources without receiving anything return could never cooperatively rational.Hence, Theorem 4, deal could never necessary achieve Pareto optimal allocation either.9. Recall distinction 0-1 functions dichotomous functions. latter assign either 0 1bundle, former assign either 0 1 individual resource (the utilities bundlesfollow fact 0-1 functions additive).331fiEndriss, Maudet, Sadri, & Toni1-deals sufficient guarantee convergence allocation maximal utilitariansocial welfare, even framework without monetary side payments (where dealsrequired cooperatively rational).Theorem 6 (0-1 scenarios) 0-1 scenarios, sequence cooperatively rational 1deals eventually result allocation maximal utilitarian social welfare.Proof. Termination shown proof Theorem 4. allocationmaximal social welfare must case agent holds resource rui ({r}) = 0 another agent j system uj ({r}) = 1. Passing rj would cooperatively rational deal, either negotiation yet terminatedindeed situation maximal utilitarian social welfare.2result may interpreted formal justification negotiation strategiesproposed Sadri et al. (2001).5. Egalitarian Agent Societiessection, going apply methodology used studyoptimal outcomes negotiation systems designed according utilitarian principlesfirst part paper analysis egalitarian agent societies. classicalcounterpart utilitarian collective utility function swu egalitarian collectiveutility function swe introduced Definition 9 (Moulin, 1988; Sen, 1970; Rawls, 1971).Therefore, going study design agent societies negotiationguaranteed converge allocation resources maximal egalitarian social welfare.first aim identify suitable criterion agents inhabiting egalitarianagent society may use decide whether accept particular deal. Clearly, cooperatively rational deals, instance, would ideal choice, Pareto optimalallocations typically optimal egalitarian point view (Moulin, 1988).5.1 Pigou-Dalton Transfers Equitable Dealssearching economics literature class deals would benefit societyegalitarian system soon encounter Pigou-Dalton transfers. Pigou-Daltonprinciple states whenever utility transfer two agents takes placereduces difference utility two, transfer consideredsocially beneficial (Moulin, 1988). context framework, Pigou-Dalton transfer(between agents j) defined follows.Definition 15 (Pigou-Dalton transfers) deal = (A, A0 ) called Pigou-Daltontransfer iff satisfies following criteria:two agents j involved deal: = {i, j}.deal mean-preserving: ui (A) + uj (A) = ui (A0 ) + uj (A0 ).deal reduces inequality: |ui (A0 ) uj (A0 )| < |ui (A) uj (A)|.332fiNegotiating Socially Optimal Allocations Resourcessecond condition definition could relaxed postulate ui (A) + uj (A)ui (A0 ) + uj (A0 ), also allow inequality-reducing deals increase overall utility.Pigou-Dalton transfers capture certain egalitarian principles; sufficientacceptability criteria guarantee negotiation outcomes maximal egalitarian socialwelfare? Consider following example:u1 ({ }) = 0u1 ({r1 }) = 3u1 ({r2 }) = 12u1 ({r1 , r2 }) = 15u2 ({ }) = 0u2 ({r1 }) = 5u2 ({r2 }) = 7u2 ({r1 , r2 }) = 17first agent attributes relatively low utility value r1 high one r2 . Furthermore, value resources together simply sum individual utilities, i.e.agent 1 using additive utility function (no synergy effects). second agent ascribesmedium value either resource high value full set. supposeinitial allocation resources A(1) = {r1 } A(2) = {r2 }. inequalityindex allocation |u1 (A) u2 (A)| = 4. easily check inequalityfact minimal allocation (which means inequality-reducing deal,certainly Pigou-Dalton transfer, given allocation). However, allocation A0A0 (1) = {r2 } A0 (2) = {r1 } would result higher level egalitarian social welfare(namely 5 instead 3). Hence, Pigou-Dalton transfers alone sufficient guaranteeoptimal outcomes negotiations egalitarian agent societies. need generalacceptability criterion.Intuitively, agents operating according egalitarian principles helpfellow agents worse (as long affordwithout ending even worse). means, purpose exchangeresources improve welfare weakest agent involved respectivedeal. formalise idea introducing class equitable deals.Definition 16 (Equitable deals) deal = (A, A0 ) called equitable iff satisfiesfollowing criterion:min{ui (A) | } < min{ui (A0 ) | }Recall = {i | A(i) 6= A0 (i)} denotes set agents involved deal .Given = (A, A0 ) deal require 6= A0 , never empty set(i.e. minima referred definition well-defined). Note equitabilitylocal rationality criterion sense Definition 6.easy see Pigou-Dalton transfer also equitable deal,always result improvement weaker one two agents concerned.converse, however, hold (not even restrict deals involvingtwo agents). fact, equitable deals may even increase inequality agentsconcerned, namely cases happier agent gains utility weaker does.literature multiagent systems, autonomy agent (one centralfeatures distinguishing multiagent systems distributed systems) sometimesequated pure selfishness. interpretation agent paradigm,333fiEndriss, Maudet, Sadri, & Toninotion equitability would, course, make little sense. believe, however,useful distinguish different degrees autonomy. agent may well autonomousdecision general, still required follow certain rules imposed society (andagreed agent entering society).5.2 Local Actions Global Effectsgoing prove two lemmas provide connection localacceptability criterion given notion equitability two egalitarian socialwelfare orderings introduced Section 2.3 (i.e. maximin-ordering induced swewell leximin-ordering).first lemma shows global changes reflected locally. deal happensincrease (global) egalitarian social welfare, is, results rise respectmaximin-ordering, deal fact equitable deal.Lemma 2 (Maximin-rise implies equitability) A0 allocationsswe (A) < swe (A0 ), = (A, A0 ) equitable deal.Proof. Let A0 allocations swe (A) < swe (A0 ) let = (A, A0 ).agent minimal utility allocation must involved , egalitarian socialwelfare, thereby agents individual utility, higher allocation A0 . is,min{ui (A) | } = swe (A). Furthermore, A, certainlyswe (A0 ) min{ui (A0 ) | }. Given original assumption swe (A) < swe (A0 ),obtain inequation min{ui (A) | } < min{ui (A0 ) | }. showsindeed equitable deal.2Observe converse hold; every equitable deal necessarily increaseegalitarian social welfare (although equitable deal never decrease egalitarian socialwelfare either). is, instance, case agents currently betterinvolved deal. fact, argued already end Section 2.2,class deals characterisable local rationality criterion (see Definition 6) wouldalways result increase egalitarian social welfare.able detect changes welfare resulting equitable deal requirefiner differentiation alternative allocations resources given leximinordering. fact, shall see next, equitable deal shown result strictimprovement respect leximin-ordering.Lemma 3 (Equitability implies leximin-rise) = (A, A0 ) equitable deal,A0 .Proof. Let = (A, A0 ) deal satisfies equitability criterion define =min{ui (A) | }. value may considered partitioning ordered utilityvector ~u(A) three subvectors: Firstly, ~u(A) got (possibly empty) prefix ~u(A)<elements strictly lower . middle, got subvector ~u(A)=(with least one element) elements equal . Finally, ~u(A) got suffix~u(A)> (which may empty) elements strictly greater .334fiNegotiating Socially Optimal Allocations Resourcesdefinition , deal cannot affect agents whose utility values belong ~u(A)< .Furthermore, definition equitability, < min{ui (A0 ) | }, meansagents involved end utility value strictlygreater , least one agents come ~u(A)= . collectinformation ~u(A0 ), ordered utility vector second allocation A0 . Firstly,prefix ~u(A0 )< identical ~u(A)< . followed (possibly empty)subvector ~u(A0 )= elements equal must strictly shorter~u(A)= . remaining elements ~u(A0 ) strictly greater . follows~u(A) lexicographically precedes ~u(A0 ), i.e. A0 holds claimed.2Again, converse hold, i.e. every deal resulting leximin-rise necessarilyequitable. Counterexamples deals utility value weakest agent involvedstays constant, despite improvement respect leximin-orderinglevel society.well-known result welfare economics states every Pigou-Dalton utility transferresults leximin-rise (Moulin, 1988). Given observed earlier every dealamounts Pigou-Dalton transfer also equitable deal, resultalso regarded simple corollary Lemma 3.5.3 Maximising Egalitarian Social Welfarenext aim prove convergence result egalitarian framework (in analogyTheorems 1 4). going show systems agents negotiate equitabledeals always converge towards allocation maximal egalitarian social welfare.Theorem 7 (Maximal egalitarian social welfare) sequence equitable dealseventually result allocation resources maximal egalitarian social welfare.Proof. Lemma 3, equitable deal result strict rise respect leximinordering (which irreflexive transitive). Hence, finite numberdistinct allocations, negotiation terminate finite number deals.suppose negotiation terminated equitable deals possible. Letcorresponding terminal allocation resources. claim allocationmaximal egalitarian social welfare. sake contradiction, assume not, i.e.assume exists another allocation A0 system swe (A) < swe (A0 ).then, Lemma 2, deal = (A, A0 ) equitable deal. Hence, stillpossible deal, namely , contradicts earlier assumption terminalallocation. shows allocation maximal egalitarian social welfare,proves claim.2purely practical point view, Theorem 7 may lesser interestcorresponding results utilitarian systems, refer acceptabilitycriterion depends single agent. course, coincides intuitions egalitarian societies: maximising social welfare possible meanscooperation sharing information agents preferences.reached allocation maximal egalitarian social welfare, maycase still equitable deals possible, although would increase social335fiEndriss, Maudet, Sadri, & Toniwelfare (but would still cause leximin-rise). demonstratedmeans simple example. Consider system three agents two resources.following table fixes utility functions:u1 ({ }) = 0u1 ({r1 }) = 5u1 ({r2 }) = 0u1 ({r1 , r2 }) = 5u2 ({ }) = 6u2 ({r1 }) = 7u2 ({r2 }) = 6.5u2 ({r1 , r2 }) = 7.5u3 ({ }) = 8u3 ({r1 }) = 9u3 ({r2 }) = 8.5u3 ({r1 , r2 }) = 9.5possible interpretation functions would following. Agent 3 fairly wellcase; obtaining either resources r1 r2 great impactpersonal welfare. true agent 2, although slightly less well beginwith. Agent 1 poorest agent attaches great value r1 , interestr2 . Suppose agent 3 initially holds resources. corresponds ordered utilityvector h0, 6, 9.5i. Passing r1 agent 1 would lead new allocation orderedutility vector h5, 6, 8.5i increase egalitarian social welfare 5, maximumachievable system. However, still another equitable deal couldimplemented latter allocation: agent 3 could offer r2 agent 2. course,deal affect agent 1. resulting allocation would ordered utilityvector h5, 6.5, 8i, corresponds leximin-maximal allocation.able detect situations social welfare maximum already reachedequitable deals still possible, able stop negotiation (assuminginterested maximising swe quickly possible), however, would requirenon-local rationality criterion. criterion takes welfare agents involvedparticular deal account could sharp enough always tell us whether givendeal would increase minimum utility society (see also discussion Lemma 2).could define class strongly equitable deals like equitable deals toprequire (currently) weakest agent involved deal. wouldsharper criterion, would also spirit distributivity locality,every single agent would involved every single deal (in sense everyoneannounce utility order able determine weakest).5.4 Necessity Resultnext theorem show, restrict set admissible dealsequitable, every single deal (that independently decomposable) maynecessary guarantee optimal result (that is, sequence equitable deals excludingcould possibly result allocation maximal egalitarian social welfare). Furthermore,theorem improves upon previous result (Endriss et al., 2003b) showingholds even utility functions required dichotomous.10Theorem 8 (Necessary deals egalitarian systems) Let sets agents resources fixed. every deal independently decomposable, exist10. theorem also corrects mistake original statement result (Endriss et al., 2003b),restriction deals independently decomposable omitted.336fiNegotiating Socially Optimal Allocations Resourcesutility functions initial allocation sequence equitable deals leading allocation maximal egalitarian social welfare would include .continues case even utility functions required dichotomous.Proof. Given set agents set resources R, let = (A, A0 ) dealsystem. 6= A0 , (at least one) agent j A(j) 6= A0 (j).use particular j fix suitable (dichotomous) utility functions ui agentssets resources R R follows:ui (R) =1 R = A0 (i) (R = A(i) 6= j)0 otherwiseis, allocation A0 every agent assigns utility value 1 resources holds.true allocation A, sole exception agent j, assignsvalue 0. allocation, agents assign value 0 set resources,unless set either allocation A0 . independentlydecomposable, happen least one agent every allocation differentA0 . Hence, every allocation least one agent assign utility value0 allocated bundle. get swe (A0 ) = 1, swe (A) = 0, swe (B) = 0 everyallocation B, i.e. A0 allocation maximal egalitarian social welfare.ordered utility vector A0 form h1, . . . , 1i, formh0, 1, . . . , 1i, allocation got form h0, . . .i, i.e. A0B allocations B B 6= B 6= A0 . Therefore, makeinitial allocation resources, deal would result strict riserespect leximin-ordering. Thus, Lemma 3, would also equitabledeal. Hence, set admissible deals restricted equitable deals indeednecessary reach allocation maximal egalitarian social welfare.2result shows, again, structurally simple class deals (suchclass deals involving two agents time) would sufficient guaranteeoptimal outcome negotiation. case even agents limitedoptions modelling preferences (as case dichotomous utility functions).11negative necessity result shared two instances negotiation framework considered, currently positive results sufficiency1-deals restricted domains egalitarian setting (see Theorems 3 6). instance, difficult construct counterexamples show even agentsusing additive 0-1 functions, complex deals involving agents time mayrequired reach allocation maximal egalitarian social welfare (a concrete examplemay found Endriss et al., 2003b).11. However, observe unlike two variants framework rational negotiation,necessity result scenarios monotonic utility functions (see Theorems 2 5). Usingcollection monotonic utility functions proof Theorem 2 would allow us drawconclusions regarding respective levels egalitarian social welfare A0 one hand,allocations B other.337fiEndriss, Maudet, Sadri, & Toni6. Negotiating Lorenz Optimal Allocationssection, going analyse framework resource allocation negotiationview notion Lorenz optimal allocations introduced Definition 11. beginsomewhat general discussion possible local rationality criteria acceptabilitygiven deal.6.1 Local Rationality Criteria Separabilityfar, studied three different variants negotiation framework: (i) rational negotiation side payments (aiming maximising utilitarian social welfare); (ii) rationalnegotiation without side payments (aiming Pareto optimal outcomes); (iii) negotiation egalitarian agent societies. first two instances framework, agentseither individually rational cooperatively rational, natural choicesformalise widely made assumptions agents purely self-interested myopic (for scenarios without monetary side payments, respectively).third variant framework, applies egalitarian agent societies, attractive conceptual technical reasons. Conceptually, egalitarian social welfareinterest, largely neglected multiagent systems literaturedespite classical counterpart widely used notion utilitarian social welfare. Technically, analysis egalitarian agent societies interesting,egalitarian social welfare admit definition local rationality criteriondirectly captures class deals resulting increase respect metric.took detour via leximin-ordering prove termination.class social welfare orderings captured deals conforminglocal rationality criterion closely related class separable social welfare orderings(Moulin, 1988). nutshell, social welfare ordering separable iff dependsagents changing utility whether given deal result increase socialwelfare. Compare notion local rationality criterion (Definition 6);depends agents changing bundle whether deal acceptable. changeutility presupposes change bundle (but vice versa). Hence, every separablesocial welfare ordering corresponds class deals characterised local rationalitycriterion (but vice versa). means every separable social welfare ordering givesrise local rationality criterion proving general convergence theorem becomesstraightforward.12 leximin-ordering, instance, separable (Moulin, 1988),going discuss social welfare ordering paper.13Similarly, ordering alternative allocations induced notion Lorenz domination (Definition 11) also separable. Hence, definition appropriate classdeals proof general convergence result would yield significant newinsights either. However, analysis effects previously intro12. Indeed, case framework rational negotiation side payments, central argumentproof Theorem 1 Lemma 1, shows individual rationality fact equivalentlocal rationality criterion induced swu .13. suitable rationality criterion would simply amount lexicographic comparison ordered utilityvectors subsociety agents involved deal question.338fiNegotiating Socially Optimal Allocations Resourcesduced rationality criteria agent societies social well-being assessed termsLorenz condition rather instructive, going see next.6.2 Lorenz Domination Existing Rationality Criteriagoing try establish connections global welfare measure inducednotion Lorenz domination one hand, various local criteriaacceptability proposed deal individual agents may choose apply other.instance, immediate consequence Definitions 11 14 that, whenever = (A, A0 )cooperatively rational deal, must Lorenz dominated A0 . may easilyverified, deal amounts Pigou-Dalton transfer (see Definition 15) also resultLorenz improvement. hand, difficult construct examplesshow case class equitable deals anymore (see Definition 16).is, equitable deals result Lorenz improvement, others not.next goal check whether possible combine existing rationality criteriadefine class deals captures notion Lorenz improvements faras, two allocations A0 Lorenz dominated A0 , existssequence deals (or possibly even single deal) belonging class leadingA0 . Given cooperatively rational deals Pigou-Dalton transfers alwaysresult Lorenz improvement, union two classes deals may seem likepromising candidate. fact, according result reported Moulin (1988, Lemma 2.3),case Lorenz improvement implemented means sequencePareto improvements (i.e. cooperatively rational exchanges) Pigou-Dalton transfers.important stress seemingly general result apply negotiationframework. see this, consider following example:u1 ({ }) = 0u1 ({r1 }) = 6u1 ({r2 }) = 1u1 ({r1 , r2 }) = 7u2 ({ }) = 0u2 ({r1 }) = 1u2 ({r2 }) = 6u2 ({r1 , r2 }) = 7u3 ({ }) = 0u3 ({r1 }) = 1u3 ({r2 }) = 1u3 ({r1 , r2 }) = 10Let allocation agent 3 owns resources, i.e. ~u(A) = h0, 0, 10iutilitarian social welfare currently 10. Allocation Pareto optimal,allocation would strictly worse agent 3. Hence, cooperatively rationaldeal would applicable situation. also observe deal involvingtwo agents would best result new allocation utilitarian social welfare 7 (thiswould deal consisting either passing resources one agents,passing preferred resource either agent 1 agent 2, respectively). Hence,deal involving two agents (and particular Pigou-Dalton transfer) could possiblyresult Lorenz improvement. However, allocation Lorenz dominatesA, namely allocation assigning one first two agents respectivelypreferred resource. allocation A0 A0 (1) = {r1 }, A0 (2) = {r2 } A0 (3) = { }got ordered utility vector h0, 6, 6i.reason general result reported Moulin applicable domaincannot use Pigou-Dalton transfers implement arbitrary utility transfers here.Moulin assumes every possible utility vector constitutes feasible agreement.339fiEndriss, Maudet, Sadri, & Tonicontext resource allocation, would mean allocation every possibleutility vector. framework, agents negotiate finite number indivisibleresources, however, range feasible allocations limited. instance,example feasible allocation (ordered) utility vector h0, 4, 6i. Moulinssystem, agents could first move h0, 0, 10i h0, 4, 6i (a Pigou-Dalton transfer)h0, 4, 6i h0, 6, 6i (a Pareto improvement). system, hand,possible.6.3 Simple Pareto-Pigou-Dalton Deals 0-1 Scenarioscannot use existing rationality criteria compose criterion captures notionLorenz improvement (and would allow us prove general convergence theorem),going investigate far get scenario restricted utility functions.Recall definition 0-1 scenarios utility functions used indicatewhether agent need particular resource. shall see next, 0-1scenarios, aforementioned result Moulin apply. fact, even sharpenlittle showing Pigou-Dalton transfers cooperatively rational deals involvingsingle resource two agents required guarantee negotiation outcomesLorenz optimal. first give formal definition class deals.Definition 17 (Simple Pareto-Pigou-Dalton deals) deal called simplePareto-Pigou-Dalton deal iff 1-deal either cooperatively rational PigouDalton transfer.going show class deals sufficient guarantee Lorenz optimaloutcomes negotiations 0-1 scenariosTheorem 9 (Lorenz optimal outcomes) 0-1 scenarios, sequence simplePareto-Pigou-Dalton deals eventually result Lorenz optimal allocation resources.Proof. pointed earlier, deal either cooperatively rational PigouDalton transfer result Lorenz improvement (not case 0-1 scenarios).Hence, given finite number distinct allocations resources,finite number deals system reached allocation simplePareto-Pigou-Dalton deals possible; is, negotiation must terminate. Now,sake contradiction, let us assume terminal allocation optimal, i.e.exists another allocation A0 Lorenz dominates A. Amongst things, impliesswu (A) swu (A0 ), i.e. distinguish two cases: either (i) strictincrease utilitarian welfare, (ii) remained constant. 0-1 scenarios, formerpossible (at least) one resource r R two agents i, jui ({r}) = 0 uj ({r}) = 1 well r A(i) r A0 (j), i.e. r movedagent (who need it) agent j (who need it). 1-deal movingr j would cooperatively rational hence also simple Pareto-PigouDalton deal. contradicts assumption terminal allocation.let us assume utilitarian social welfare remained constant, i.e. swu (A) =swu (A0 ). Let k smallest index ~uk (A) < ~uk (A0 ). (This first kinequality Definition 11 strict.) Observe cannot k = |A|,340fiNegotiating Socially Optimal Allocations Resourceswould contradict swu (A) = swu (A0 ). shall call agents contributing firstk entries ordered utility vector ~u(A) poor agents remaining onesrich agents. Then, 0-1 scenario, must resource r R ownedrich agent allocation poor agent j allocation A0 neededagents, i.e. ui ({r}) = 1 uj ({r}) = 1. moving resourceagent agent j would constitute Pigou-Dalton transfer (and hence also simplePareto-Pigou-Dalton deal) allocation A, contradicts earlier assumptionterminal.2summary, shown (i) allocation resources simplePareto-Pigou-Dalton deals possible must Lorenz optimal allocation (ii)allocation always reached implementing finite number simple ParetoPigou-Dalton deals. earlier convergence results, agents need worrydeals implement, long simple Pareto-Pigou-Dalton deals.convergence global optimum guaranteed theorem.7. Variationssection, going briefly consider two notions social preferencediscuss context framework resource allocation negotiation. Firstly,going introduce idea elitist agent societies, social welfare tiedwelfare agent currently best off. going discuss societiesenvy-free allocations resources desirable.7.1 Elitist Agent SocietiesEarlier discussed maximin-ordering induced egalitarian collective utilityfunction swe (see Definition 9). ordering actually particular case class socialwelfare orderings, sometimes called k-rank dictators (Moulin, 1988), particularagent (the one corresponding kth element ordered utility vector) chosenrepresentative society. Amongst class orderings, another particularlyinteresting case welfare society evaluated basis happiestagent (as opposed unhappiest agent, case egalitarian welfare). callelitist approach measuring social welfare.Definition 18 (Elitist social welfare) elitist social welfare swel (A) allocationresources defined follows:swel (A) = max{ui (A) | A}elitist agent society, agents would cooperate order support champion (thecurrently happiest agent). approach may seem somewhat unethical farhuman society concerned, believe could indeed appropriatecertain societies artificial agents. applications, distributed multiagent systemmay merely serve means helping single agent system achieve goal.However, may always known advance agent likely achievegoal therefore supported peers. typical scenario could341fiEndriss, Maudet, Sadri, & Tonisystem designer launches different agents goal, aim least oneagent achieves goal matter happens others. egalitarian agentsocieties, contradict idea agents autonomous entities. Agents mayphysically distributed make autonomous decisions variety issueswhilst also adhering certain social principles, case elitist ones.technical point view, designing criterion would allow agents inhabiting elitist agent society decide locally whether accept particular dealsimilar egalitarian case. analogy case equitable deals definedearlier, suitable deal would increase maximal individual welfare amongstagents involved one deal. egalitarian case, class dealscharacterised local rationality criterion would exactly capture range dealsresulting increase elitist social welfare (because every agent system wouldconsulted first determine currently best off). prove convergence,would resort auxiliary social welfare ordering (similarly useleximin-ordering proof Theorem 7).course, many cases much simpler way finding allocationmaximal elitist social welfare. instance, agents use monotonic utility functions,moving resources agent assigning highest utility value full bundleR would optimal elitist point view. generally, always findelitist optimum checking whose utility function got highest peak. is,highest possible elitist social welfare easily determined centralised manner,distributed approach still provide useful framework studying processactually reaching optimal allocation.7.2 Reducing Envy amongst Agentsfinal example interesting approach measuring social welfare agent societyissue envy-freeness (Brams & Taylor, 1996). particular allocation resources,agent may envious another agent would prefer agents set resourcesown. Ideally, allocation envy-free.Definition 19 (Envy-freeness) allocation resources called envy-free iffui (A(i)) ui (A(j)) agents i, j A.Like egalitarian social welfare, related fair division resources amongst agents.Envy-freeness desirable (though always achievable) societies self-interested agentscases agents collaborate longer period time.case, agent believe ripped off, would incentiveleave coalition may disadvantageous agents societywhole. words, envy-freeness plays important role respect stabilitygroup. Unfortunately, envy-free allocations always exist. simple example wouldsystem two agents single resource, valued them.whichever agent holds single resource envied agent.Furthermore, aiming agreeing envy-free allocation resources alwayscompatible with, say, negotiating Pareto optimal outcomes. Consider following exampletwo agents identical preferences alternative bundles resources:342fiNegotiating Socially Optimal Allocations Resourcesu1 ({ }) = 0u1 ({r1 }) = 1u1 ({r2 }) = 2u1 ({r1 , r2 }) = 0u2 ({ }) = 0u2 ({r1 }) = 1u2 ({r2 }) = 2u2 ({r1 , r2 }) = 0example, either one two allocations one agent owns resourcesnone would envy-free (as agent would prefer ones bundleown). However, allocation would Pareto optimal. hand,allocation agent owns single resource would Pareto optimal,envy-free (because agent holding r1 would rather r2 ).stress envy defined sole basis agents private preferences,i.e. need take agents utility functions account. Still, whetheragent envious depend resources holds, also resourcescould hold whether agents currently hold preferred bundle.somewhat paradoxical situation makes envy-freeness far less amenable methodologynotions social welfare discussed paper.able measure different degrees envy, could, example, count numberagents envious given allocation. Another option would computeagent experiences envy difference ui (A(i)) ui (A(j))agent j envies most. sum differences would alsoprovide indication degree overall envy (and thereby social welfare).spirit egalitarianism, third option would identify degree envy societydegree envy experienced agent envious (Lipton, Markakis,Mossel, & Saberi, 2004). However, possible define local acceptability criterionterms utility functions agents involved deal (and those)indicates whether deal question would reduce envy according metric.simple consequence fact deal may affect degree envy experiencedagent involved deal (because could lead one participatingagents ending bundle preferred non-concerned agent question).8. Conclusionstudied abstract negotiation framework members agent societyarrange multilateral deals exchange bundles indivisible resources, analysedresulting changes resource distribution affect society respect differentsocial welfare orderings.scenarios agents act rationally sense never accepting dealwould (even temporarily) decrease level welfare, seen systemsside payments possible guarantee outcomes maximal utilitarian social welfare,systems without side payments allow, least, negotiation Pareto optimalallocations. also considered two examples special domains restricted utilityfunctions, namely additive 0-1 scenarios. cases, able proveconvergence socially optimal allocation resources also negotiation protocolsallow deals involving single resources pair agents (so-called1-deals). case agent societies welfare measured terms egalitariancollective utility function, put forward class equitable deals shown343fiEndriss, Maudet, Sadri, & Toninegotiation processes agents use equitability acceptability criterion alsoconverge towards optimal state. Another result states that, relatively simple0-1 scenarios, Lorenz optimal allocations achieved using one-to-one negotiationimplementing 1-deals either inequality-reducing increase welfareagents involved. also discussed case elitist agent societies socialwelfare tied welfare successful agent. finally, pointeddifficulties associated designing agents would able negotiateallocations resources degree envy agents society minimal.Specifically, proved following technical results:class individually rational deals14 sufficient negotiate allocationsmaximal utilitarian social welfare (Theorem 1).domains additive utility functions, class individually rational 1-dealssufficient negotiate allocations maximal utilitarian social welfare (Theorem 3).class cooperatively rational deals sufficient negotiate Pareto optimal allocations (Theorem 4).domains 0-1 utility functions, class cooperatively rational 1-dealssufficient negotiate allocations maximal utilitarian social welfare (Theorem 6).class equitable deals sufficient negotiate allocations maximal egalitarian social welfare (Theorem 7).domains 0-1 utility functions, class simple Pareto-Pigou-Dalton deals(which 1-deals) sufficient negotiate Lorenz optimal allocations (Theorem 9).three convergence results apply deals without structural restrictions(rather 1-deals), also proved corresponding necessity results (Theorems 2, 5,8). theorems show given deal (defined pair allocations)independently decomposable may necessary able negotiate optimal allocationresources (with respect chosen notion social welfare), deals requiredconform rationality criterion question. consequence results,negotiation protocol allow representation deals involvingnumber agents number resources could ever enable agents (whose behaviourconstrained various rationality criteria) negotiate socially optimal allocationcases. Rather surprisingly, three necessity results continue apply evenagents differentiate resource bundles would happywould happy (using dichotomous utility functions). Theorems 25 also apply case agents required use monotonic utility functions.natural question arises considering convergence results concernscomplexity negotiation framework. difficult agents agree dealmany deals required system converges optimal state? latterquestions recently addressed Endriss Maudet (2005). paper14. Recall individually rational deals may include monetary side payments.344fiNegotiating Socially Optimal Allocations Resourcesestablishes upper bounds number deals required reach optimal allocations resources referred four convergence theorems model rationalnegotiation (i.e. Theorems 1, 3, 4, 6). also discusses different aspects complexity involved general level (such distinction communicationcomplexity system, i.e. amount information agents need exchangereach optimal allocation, computational complexity reasoning tasks facedevery single agent). Dunne (2005) addresses related problem studies numberdeals meeting certain structural requirements (in particular 1-deals) requiredreach given target allocation (whenever possible recall necessityresults show excluding certain deal patterns typically bar agents reachingoptimal allocations).earlier work, Dunne et al. (2005) studied complexity deciding whetherone-resource-at-a-time trading side payments sufficient reach given allocation(with improved utilitarian social welfare). problem shown NP-hard.complexity results concern computational complexity finding socially optimal allocation, independently concrete negotiation mechanism used. mentionedearlier, results closely related computational complexity winner determination problem combinatorial auctions (Rothkopf, Pekec, & Harstad, 1998; Cramtonet al., 2006). Recently, NP-completeness results optimisation problemderived respect several different ways representing utility functions (Dunne et al.,2005; Chevaleyre et al., 2004). Bouveret Lang (2005) also address computationalcomplexity deciding whether allocation exists envy-free Pareto optimal.Besides presenting technical results, argued wide spectrum socialwelfare orderings (rather induced well-known utilitarian collectivewelfare function concept Pareto optimality) interest agent-basedapplications. context typical electronic commerce application, participating agents responsibilities towards other, system designer may wishensure Pareto optimality guarantee agents get maximal payoff wheneverpossible without making agents worse off. applications fairtreatment participants vital (e.g. cases system infrastructure jointlyowned agents), egalitarian approach measuring social welfare mayappropriate. Many applications fact likely warrant mixture utilitarianegalitarian principles. Here, systems enable Lorenz optimal agreements may turntechnology choice. applications, however, may require social welfaremeasured ways foreseen models typically studied social sciences.proposed notion elitist welfare would example. Elitism little roomhuman society, ethical considerations paramount, particular computingapplication considerations may well dropped changed.discussion suggests approach multiagent systems design call welfareengineering (Endriss & Maudet, 2004). involves, firstly, application-driven choice (orpossibly invention) suitable social welfare ordering and, secondly, design agentbehaviour profiles negotiation mechanisms permit (or even guarantee) sociallyoptimal outcomes interactions agents system. discussed earlier,designing agent behaviour profiles necessarily contradict idea autonomy345fiEndriss, Maudet, Sadri, & Toniagent, autonomy always understood relative normsgoverning society agent operates. stress that,studying distributed approach multiagent resource allocation paper, generalidea exploring full range social welfare orderings developing agent-basedapplications also applies centralised mechanisms (such combinatorial auctions).hope develop methodology welfare engineering future work.possible directions future work include identification social welfareorderings definition corresponding deal acceptability criteria; continuationcomplexity-theoretic analysis negotiation framework; design practicaltrading mechanisms (including protocols strategies) would allow agentsagree multilateral deals involving two agents time.Acknowledgmentswould like thank Jerome Lang various anonymous referees valuablecomments. research partially supported European Commissionpart SOCS project (IST-2001-32530).ReferencesAndersson, M., & Sandholm, T. W. (2000). Contract type sequencing reallocative negotiation. Proceedings 20th International Conference Distributed ComputingSystems (ICDCS-2000), pp. 154160. IEEE.Arrow, K. J., Sen, A. K., & Suzumura, K. (Eds.). (2002). Handbook Social ChoiceWelfare, Vol. 1. North-Holland.Bouveret, S., & Lang, J. (2005). Efficiency envy-freeness fair division indivisiblegoods: Logical representation complexity. Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI-2005), pp. 935940. MorganKaufmann Publishers.Brams, S. J., & Taylor, A. D. (1996). Fair Division: Cake-cutting Dispute Resolution. Cambridge University Press.Chavez, A., Moukas, A., & Maes, P. (1997). Challenger: multi-agent system distributedresource allocation. Proceedings 1st International Conference AutonomousAgents (Agents-1997), pp. 323331. ACM Press.Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2004). Multiagent resource allocationk-additive utility functions. Proceedings DIMACS-LAMSADE Workshop Computer Science Decision Theory, Vol. 3 Annales du LAMSADE,pp. 83100.Chevaleyre, Y., Endriss, U., Lang, J., & Maudet, N. (2005a). Negotiating small bundlesresources. Proceedings 4th International Joint Conference AutonomousAgents Multiagent Systems (AAMAS-2005), pp. 296302. ACM Press.Chevaleyre, Y., Endriss, U., & Maudet, N. (2005b). maximal classes utility functionsefficient one-to-one negotiation. Proceedings 19th International Joint346fiNegotiating Socially Optimal Allocations ResourcesConference Artificial Intelligence (IJCAI-2005), pp. 941946. Morgan KaufmannPublishers.Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2006). Combinatorial Auctions. MITPress.Dunne, P. E. (2005). Extremal behaviour multiagent contract negotiation. JournalArtificial Intelligence Research, 23, 4178.Dunne, P. E., Wooldridge, M., & Laurence, M. (2005). complexity contract negotiation. Artificial Intelligence, 164 (12), 2346.Dunne, P. E., Laurence, M., & Wooldridge, M. (2004). Tractability results automaticcontracting. Proceedings 16th Eureopean Conference Artificial Intelligence(ECAI-2004), pp. 10031004. IOS Press.Endriss, U., & Maudet, N. (2004). Welfare engineering multiagent systems. EngineeringSocieties Agents World IV, Vol. 3071 LNAI, pp. 93106. Springer-Verlag.Endriss, U., & Maudet, N. (2005). communication complexity multilateral trading:Extended report. Journal Autonomous Agents Multiagent Systems, 11 (1), 91107.Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2003a). optimal outcomes negotiations resources. Proceedings 2nd International Joint ConferenceAutonomous Agents Multiagent Systems (AAMAS-2003), pp. 177184. ACMPress.Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2003b). Resource allocation egalitarian agent societies. Secondes Journees Francophones sur les Modeles FormelsdInteraction (MFI-2003), pp. 101110. Cepadues-Editions.Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2004). agenda-based frameworkmulti-issues negotiation. Artificial Intelligence, 152 (1), 145.Fishburn, P. C. (1970). Utility Theory Decision Making. John Wiley Sons.Fujishima, Y., Leyton-Brown, K., & Shoham, Y. (1999). Taming computational complexity combinatorial auctions: Optimal approximate approaches. Proceedings 16th International Joint Conference Artificial Intelligence (IJCAI1999), pp. 548553. Morgan Kaufmann Publishers.Kraus, S. (2001). Strategic Negotiation Multiagent Environments. MIT Press.Lematre, M., Verfaillie, G., & Bataille, N. (1999). Exploiting common property resourcefairness constraint: case study. Proceedings 16th International JointConference Artificial Intelligence (IJCAI-1999), pp. 206211. Morgan KaufmannPublishers.Lipton, R. J., Markakis, E., Mossel, E., & Saberi, A. (2004). approximately fair allocations indivisible goods. Proceedings 5th ACM Conference ElectronicCommerce (EC-2004), pp. 125131. ACM Press.Moulin, H. (1988). Axioms Cooperative Decision Making. Cambridge University Press.347fiEndriss, Maudet, Sadri, & ToniMyerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms bilateral trading.Journal Economic Theory, 29 (2), 265281.Rawls, J. (1971). Theory Justice. Oxford University Press.Rosenschein, J. S., & Zlotkin, G. (1994). Rules Encounter. MIT Press.Rothkopf, M. H., Pekec, A., & Harstad, R. M. (1998). Computationally manageable combinational auctions. Management Science, 44 (8), 11311147.Sadri, F., Toni, F., & Torroni, P. (2001). Dialogues negotiation: Agent varieties dialogue sequences. Proceedings 8th International Workshop Agent Theories,Architectures, Languages (ATAL-2001), pp. 405421. Springer-Verlag.Sandholm, T. W. (2002). Algorithm optimal winner determination combinatorial auctions. Artificial Intelligence, 135, 154.Sandholm, T. W. (1998). Contract types satisficing task allocation: Theoretical results.Proceedings AAAI Spring Symposium: Satisficing Models.Sandholm, T. W. (1999). Distributed rational decision making. Wei, G. (Ed.), Multiagent Systems: Modern Approach Distributed Artificial Intelligence, pp. 201258.MIT Press.Sen, A. K. (1970). Collective Choice Social Welfare. Holden Day.Smith, R. G. (1980). contract net protocol: High-level communication controldistributed problem solver. IEEE Transactions Computers, C-29 (12), 11041113.Wooldridge, M. (2002). Introduction MultiAgent Systems. John Wiley Sons.348fiJournal Artificial Intelligence Research 25 (2006) 75-118Submitted 01/05; published 1/06Approximate Policy Iteration Policy Language Bias:Solving Relational Markov Decision ProcessesAlan Fernafern@cs.orst.eduSchool Electrical Engineering Computer Science, Oregon State UniversitySungwook Yoonsy@purdue.eduRobert Givangivan@purdue.eduSchool Electrical Computer Engineering, Purdue UniversityAbstractstudy approach policy selection large relational Markov Decision Processes(MDPs). consider variant approximate policy iteration (API) replacesusual value-function learning step learning step policy space. advantageousdomains good policies easier represent learn correspondingvalue functions, often case relational MDPs interested in.order apply API problems, introduce relational policy languagecorresponding learner. addition, introduce new bootstrapping routine goalbased planning domains, based random walks. bootstrapping necessarymany large relational MDPs, reward extremely sparse, API ineffectivedomains initialized uninformed policy. experiments showresulting system able find good policies number classical planning domainsstochastic variants solving extremely large relational MDPs.experiments also point limitations approach, suggesting future work.1. IntroductionMany planning domains naturally represented terms objects relationsamong them. Accordingly, AI researchers long studied algorithms planninglearning-to-plan relational state action spaces. include, example, classicalSTRIPS domains blocks world logistics.common criticism domains algorithms assumption idealized,deterministic world model. This, part, led AI researchers study planninglearning within decision-theoretic framework, explicitly handles stochastic environments generalized reward-based objectives. However, work basedexplicit propositional state-space models, far demonstrated scalabilitylarge relational domains commonly addressed classical planning.Intelligent agents must able simultaneously deal complexity arisingrelational structure complexity arising uncertainty. primary goalresearch move toward agents bridging gap classicaldecision-theoretic techniques.paper, describe straightforward practical method solving large,relational MDPs. work viewed form relational reinforcement learning(RRL) assume strong simulation model environment. is, assumeaccess black-box simulator, provide (relationally represented)c2006AI Access Foundation. rights reserved.fiFern, Yoon, & Givanstate/action pair receive sample appropriate next-state reward distributions. goal interact simulator order learn policy achieving highexpected reward. separate challenge, considered here, combine workmethods learning environment simulator avoid dependence providedsimulator.Dynamic-programming approaches finding optimal control policies MDPs (Bellman, 1957; Howard, 1960), using explicit (flat) state space representations, breakstate space becomes extremely large. recent work extends algorithmsuse propositional (Boutilier & Dearden, 1996; Dean & Givan, 1997; Dean, Givan, &Leach, 1997; Boutilier, Dearden, & Goldszmidt, 2000; Givan, Dean, & Greig, 2003; Guestrin,Koller, Parr, & Venkataraman, 2003b) well relational (Boutilier, Reiter, & Price, 2001;Guestrin, Koller, Gearhart, & Kanodia, 2003a) state-space representations. extensions significantly expanded set approachable problems, yet showncapacity solve large classical planning problems benchmark problemsused planning competitions (Bacchus, 2001), let alone stochastic variants. One possible reason methods based calculating representing valuefunctions. familiar STRIPS planning domains (among others), useful value functionsdifficult represent compactly, manipulation becomes bottle-neck.techniques purely deductivethat is, value function guaranteed certain level accuracy. Rather, work, focus inductivetechniques make guarantees practice. existing inductive forms approximate policy iteration (API) utilize machine learning select compactly representedapproximate value functions iteration dynamic programming (Bertsekas & Tsitsiklis, 1996). machine learning algorithm, selection hypothesis space,space value functions, critical performance. example space used frequentlyspace linear combinations human-selected feature set.knowledge, previous work applies form APIbenchmark problems classical planning, stochastic variants.1 Again, onereason high complexity typical value functions large relationaldomains, making difficult specify good value-function spaces facilitate learning.Comparably, often much easier compactly specify good policies, accordinglygood policy spaces learning. observation basis recent work inductive policy selection relational planning domains, deterministic (Khardon, 1999a;Martin & Geffner, 2000), probabilistic (Yoon, Fern, & Givan, 2002). techniquesshow useful policies learned using policy-space bias described generic(relational) knowledge representation language. incorporate ideas variant API, achieves significant success without representing learning approximatevalue functions. course, natural direction future work combine policy-spacetechniques value-function techniques, leverage advantages both.Given initial policy, approach uses simulation technique policy rollout(Tesauro & Galperin, 1996) generate trajectories improved policy. trajectories given classification learner, searches classifier, policy,matches trajectory data, resulting approximately improved policy. two1. Recent work relational reinforcement learning applied STRIPS problems much simplergoals typical benchmark planning domains, discussed Section 8.76fiAPI Policy Language Biassteps iterated improvement observed. resulting algorithmviewed form API iteration carried without inducing approximatevalue functions.avoiding value function learning, algorithm helps address representationalchallenge applying API relational planning domains. However, another fundamentalchallenge that, non-trivial relational domains, API requires form bootstrapping. particular, STRIPS planning domains reward, correspondsachieving goal condition, sparsely distributed unlikely reached random exploration. Thus, initializing API random uninformed policy, likely resultreward signal hence guidance policy improvement. One approach bootstrapping rely user provide good initial policy heuristic gives guidancetoward achieving reward. Rather, work develop new automatic bootstrappingapproach goal-based planning domains, require user intervention.bootstrapping approach based idea random-walk problem distributions.given planning domain, blocks world, distribution randomly generatesproblem (i.e., initial state goal) selecting random initial stateexecuting sequence n random actions, taking goal condition subsetproperties resulting state. problem difficulty typically increases n,small n (short random walks) even random policies uncover reward. Intuitively,good policy problems walk length n used bootstrap API problemsslightly longer walk lengths. bootstrapping approach iterates idea, startingrandom policy small n, gradually increasing walk lengthlearn policy long random walks. long-random-walk policies clearly capturemuch domain knowledge, used various ways. Here, show empiricallypolicies often perform well problem distributions relational domains usedrecent deterministic probabilistic planning competitions.implementation bootstrapped API approach took second place 3 competitors hand-tailored track 2004 International Probabilistic Planning Competition.2 knowledge first machine-learning based system enteredplanning competition, either deterministic probabilistic.Here, give evaluation system number probabilistic deterministicrelational planning domains, including AIPS-2000 competition benchmarks, benchmarks hand-tailored track 2004 Probabilistic Planning Competition.results show system often able learn policies domains performwell long-random-walk problems. addition, policies often perform wellplanning-competition problem distributions, comparing favorably state-of-theart planner FF deterministic domains. experiments also highlight numberlimitations current system, point interesting directions future work.remainder paper proceeds follows. Section 2, introduce problemsetup then, Section 3, present new variant API. Section 4, provide2. Note, however, approach hand-tailored. Rather, given domain definition, systemlearns policy offline, automatically, applied problem domain.entered hand-tailored track track facilitated use offline learning,providing domains problem generators competition. entrants humanwritten domain.77fiFern, Yoon, & Givantechnical analysis algorithm, giving performance bounds policy-improvementstep. Sections 5 6, describe implemented instantiation API approachrelational planning domains. includes description generic policy languagerelational domains, classification learner language, novel bootstrappingtechnique goal-based domains. Section 7 presents empirical results, finallySections 8 9 discuss related work future directions.2. Problem Setupformulate work framework Markov Decision Processes (MDPs).primary motivation develop algorithms relational planning domains, firstdescribe problem setup approach general, action-simulatorbased MDP representation. Later, Section 5, describe particular representation planning domainsrelational MDPs corresponding relational instantiation approach.Following adapting Kearns, Mansour, Ng (2002) Bertsekas Tsitsiklis(1996), represent MDP using generative model hS, A, T, R, Ii, finiteset states, finite, ordered set actions, randomized action-simulationalgorithm that, given state action a, returns next state s0 according unknownprobability distribution PT (s0 |s, a). component R reward function mapsreal-numbers, R(s, a) representing reward taking action state s,randomized initial-state algorithm inputs returns state accordingunknown distribution P0 (s). sometimes treat (s, a) random variablesdistributions P0 () PT (|s, a) respectively.MDP = hS, A, T, R, Ii, policy (possibly stochastic) mappingA. value function , denoted V (s), represents expected, cumulative, discountedreward following policy starting state s, unique solutionV (s) = E[R(s, (s)) + V (T (s, (s)))](1)0 < 1 discount factor. Q-value function Q (s, a) representsexpected, cumulative, discounted reward taking action state following ,givenQ (s, a) = R(s, a) + E[V (T (s, a))](2)measure quality policy objective function V () = E[V (I)], givingexpected value obtained policy starting randomly drawn initialstate. common objective MDP planning reinforcement learning findoptimal policy = argmax V (). However, automated technique, including onepresent here, date able guarantee finding optimal policy relationalplanning domains consider, reasonable running time.well known fact given current policy , define new improvedpolicyPI (s) = argmaxaA Q (s, a)(3)value function PI guaranteed 1) worsestate s, 2) strictly improve state optimal. Policy iteration78fiAPI Policy Language Biasalgorithm computing optimal policies iterating policy improvement (PI)initial policy reach fixed point, guaranteed optimal policy.iteration policy improvement involves two steps: 1) Policy evaluation computevalue function V current policy , 2) Policy selection, where, given Vstep 1, select action maximizes Q (s, a) state, defining new improvedpolicy.Finite Horizons. Since API variant based simulation, must boundsimulation trajectories horizon h, technical analysis Section 4 use notionfinite-horizon discounted reward. h-horizon value function Vh recursively definedV0 (s) = 0,Vh (s) = E[R(s, (s)) + Vh1 (T (s, (s)))](4)giving expected discounted reward obtained following h steps s. also(T (s, a))], h-horizondefine h-horizon Q-function Qh (s, a) = R(s, a) + E[Vh1objective function V h () = E[Vh (I)]. well known, effect using finitehorizon made arbitrarily small. particular, statesactions a, approximation error decreases exponentially h,|V (s) Vh (s)| h Vmax ,|Q (s, a) Qh (s, a)| h Vmax ,RmaxVmax =,1Rmax maximum absolute value reward action state.also get |V h () V ()| h Vmax .3. Approximate Policy Iteration Policy Language BiasExact solution techniques, policy iteration, typically intractable large statespace MDPs, arising relational planning domains. section,introduce new variant approximate policy iteration (API) intended domains.First, review generic form API used prior work, based learning approximatevalue functions. Next, motivated fact value functions often difficult learnrelational domains, describe API variant, avoids learning value functionsinstead learns policies directly state-action mappings.3.1 API Approximate Value FunctionsAPI, described Bertsekas Tsitsiklis (1996), uses combination Monte-Carlosimulation inductive machine learning heuristically approximate policy iterationlarge state-space MDPs. Given current policy , iteration API approximatespolicy evaluation policy selection, resulting approximately improved policy .First, policy-evaluation step constructs training set samples V smallrepresentative set states. sample computed using simulation, estimating V (s)policy state drawing number sample trajectories starting79fiFern, Yoon, & Givanaveraging cumulative, discounted reward along trajectories. Next,policy-selection step uses function approximator (e.g., neural network) learnapproximation V V based training data. V serves representation, selects actions using sampled one-step lookahead based V ,(s) = arg max R(s, a) + E[V (T (s, a))].aAcommon variant procedure learns approximation Q rather V .API exploits function approximators generalization ability avoid evaluatingstate state space, instead directly evaluating small number training states.Thus, use API assumes states perhaps actions represented factoredform (typically, feature vector) facilitates generalizing properties training dataentire state action spaces. Note case perfect generalization (i.e.,V (s) = V (s) states s), equal exact policy improvementPI , thus API simulates exact policy iteration. However, practice, generalizationperfect, typically guarantees policy improvement3 nevertheless,API often converges usefully (Tesauro, 1992; Tsitsiklis & Van Roy, 1996).success API procedure depends critically ability representlearn good value-function approximations. MDPs, arisingrelational planning domains, often difficult specify space value functionslearning mechanism facilitate good generalization. example, work relationalreinforcement learning (Dzeroski, DeRaedt, & Driessens, 2001) shown learningapproximate value functions classical domains, blocks world, problematic.4 spite this, often relatively easy compactly specify good policies usinglanguage (relational) state-action mappings. suggests languages mayprovide useful policy-space biases learning API. However, prior API methodsbased approximating value functions hence leverage biases.motivation, consider form API directly learns policies without directlyrepresenting approximating value functions.3.2 Using Policy Language Biaspolicy simply classifier, possibly stochastic, maps states actions. APIapproach based view, motived recent work casts policy selectionstandard classification learning problem. particular, given ability observetrajectories target policy, use machine learning select policy, classifier,mimics target closely possible. Khardon (1999b) studied learning settingprovided PAC-like learnability results, showing certain assumptions, smallnumber trajectories sufficient learn policy whose value closetarget. addition, recent empirical work, relational planning domains (Khardon, 1999a;Martin & Geffner, 2000; Yoon et al., 2002), shown using expressive languages3. strong assumptions, API shown converge infinite limit near-optimalvalue function. See Proposition 6.2 Bertsekas Tsitsiklis (1996).4. particular, RRL work considered variety value-function representation including relationalregression trees, instance based methods, graph kernels, none generalized wellvarying numbers objects.80fiAPI Policy Language Biasspecifying state-action mappings, good policies learned sample trajectoriesgood policies.results suggest that, given policy , somehow generate trajectoriesimproved policy, learn approximately improved policy basedtrajectories. idea basis approach. Figure 1 gives pseudo-code APIvariant, starts initial policy 0 produces sequence approximatelyimproved policies. iteration involves two primary steps: First, given currentpolicy , procedure Improved-Trajectories (approximately) generates trajectoriesimproved policy 0 = PI . Second, trajectories used training dataprocedure Learn-Policy, returns approximation 0 . describestep detail.Step 1: Generating Improved Trajectories. Given base policy , simulation technique policy rollout (Tesauro & Galperin, 1996; Bertsekas & Tsitsiklis, 1996)computes approximation improved policy 0 = PI , 0 resultapplying one step policy iteration . Furthermore, given state s, policy rolloutcomputes (s) without need solve 0 states, thus providestractable way approximately simulate improved policy 0 large state-space MDPs.Often 0 significantly better , hence , lead substantiallyimproved performance small cost. Policy rollout provided significant benefitsnumber application domains, including example Backgammon (Tesauro & Galperin,1996), instruction scheduling (McGovern, Moss, & Barto, 2002), network-congestion control(Wu, Chong, & Givan, 2001), Solitaire (Yan, Diaconis, Rusmevichientong, & Van Roy,2004).Policy rollout computes (s), estimate 0 (s), estimating Q (s, a)action taking maximizing action (s) suggested Equation 3.Q (s, a) estimated drawing w trajectories length h, trajectoryresult starting s, taking action a, following actions selectedh 1 steps. estimate Q (s, a) taken average cumulativediscounted reward along trajectory. sampling width w horizon h specifieduser, control trade increased computation time large values,reduced accuracy small values. Note rollout applies stochasticdeterministic policies due variance Q-value estimates, rollout policystochastic even deterministic base policies.procedure Improved-Trajectories uses rollout generate n length h trajectories, beginning randomly drawn initial state. Rather recordingstates actions along trajectory, store additional information usedpolicy-learning algorithm. particular, ith element trajectory formhsi , (si ), Q(si , a1 ), . . . , Q(si , )i, giving ith state si along trajectory, actionselected current (unimproved) policy si , Q-value estimates Q(si , a)action. Note given Q-value information si learning algorithmdetermine approximately improved action (s), maximizing actions, desired.Step 2: Learn Policy. Intuitively, want Learn-Policy select new policyclosely matches training trajectories. experiments, use relatively simplelearning algorithms based greedy search within space policies specified policylanguage bias. Sections 5.2 5.3 detail policy-language learning bias used81fiFern, Yoon, & Givantechnique, associated learning algorithm. Section 4 providetechnical analysis idealized version algorithm, providing guidance regardingrequired number training trajectories. note labeling training statetrajectories associated Q-values action, rather simplybest action, enable learner make informed trade-offs, focusing accuracystates wrong decisions high costs, empirically useful. Also,inclusion (s) training data enables learner adjust data relative ,desirede.g., learner uses bias focuses states large improvementappears possible.Finally, note API effective, important initial policy0 provide guidance toward improvement, i.e., 0 must bootstrap API process.example, goal-based planning domains 0 reach goal sampledstates. Section 6 discuss important issue bootstrapping introducenew bootstrapping technique.4. Technical Analysissection, consider variant policy improvement step main API loop,learns improved policy given base policy . show select samplingwidth w, horizon h, training set size n that, certain assumptions, qualitylearned policy close quality 0 policy iteration improvement. Similarresults shown previous forms API based approximate value functions(Bertsekas & Tsitsiklis, 1996), however, assumptions much different nature.5analysis divided two parts. First, following Khardon (1999b), considersample complexity policy learning. is, consider many trajectoriestarget policy must observed learner guarantee good approximationtarget. Second, show apply result, deterministic policies,problem learning rollout policies, stochastic. Throughoutassume context MDP = hS, A, T, R, Ii.4.1 Learning Deterministic Policiestrajectory length h sequence (s0 , a0 , s1 , a1 , . . . , ah1 , sh ) alternating states siactions ai . say deterministic policy consistent trajectory (s1 , a1 , . . . , sh )0 < h, (si ) = ai . define Dh distribution setlength h trajectories, Dh (t) probability generates trajectory= (s0 , a0 , s1 , a1 , . . . , ah1 , sh ) according following process: first draw s0 accordinginitial state distribution I, draw si+1 (si , (si )) 0 < h. NoteDh (t) non-zero consistent t.policy improvement step first generates trajectories rollout policy (see Section 3.2), via procedure Improved-Trajectories, learns approximation5. particular, Bertsekas Tsitsiklis (1996) assumes bound L norm value functionapproximation, i.e., state approximation almost perfect. Rather assumeimproved policy 0 comes finite class policies consistent learner.cases policy improvement guaranteed given additional assumption minimumQ-advantage MDP (see below).82fiAPI Policy Language BiasAPI (n, w, h, M, 0 , )// training set size n, sampling width w, horizon h,// MDP = hS, {a1 , . . . , }, T, R, Ii, initial policy 0 , discount factor .0 ;loopImproved-Trajectories(n, w, h, M, );Learn-Policy(T );satisfied ;// e.g., change smallReturn ;Improved-Trajectories(n, w, h, M, )// training set size n, sampling width w,// horizon h, MDP , current policy;repeat n times // generate n trajectories improved policynil;state drawn I; // draw random initial state= 1 hhQ(s, a1 ), . . . , Q(s, )i Policy-Rollout(, s, w, h, ); // Q (s, a) estimateshs, (s), Q(s, a1 ), . . . , Q(s, ))i; // concatenate new sample onto trajectoryaction maximizing Q(s, a); // action improved policy statestate sampled (s, a); // simulate action improved policyt;Return ;Policy-Rollout (, s, w, h, )// Compute Q (s, a) estimates hQ(s, a1 ), . . . , Q(s, )i// policy , state s, sampling width w, horizon h, MDPaction aiQ(s, ai ) 0;repeat w times // Q(s, ai ) average w trajectoriesR R(s, ai ); s0 state sampled (s, ai ); // take action ai= 1 h 1 // take h 1 steps accumulating discounted reward RR R + R(s0 , (s0 ));s0 state sampled (s0 , (s0 ))Q(s, ai ) Q(s, ai ) + R; // include trajectory averageQ(s, ai )Q(s,ai );wReturn hQ(s, a1 ), . . . , Q(s, )iFigure 1: Pseudo-code API algorithm. See Section 5.3 instantiation LearnPolicy called Learn-Decision-List.. Note rollout policy serves stochastic approximation 0 = PIpolicy iteration improvement . Thus, Improved-Trajectories viewed at0tempting draw trajectories Dh , learning step viewed learning83fiFern, Yoon, & Givan0approximation 0 . Imagining moment draw trajectories Dh ,fundamental question many trajectories sufficient ensure learnedpolicy good 0 . Khardon (1999b) studied question casedeterministic policies undiscounted goal-based planning domains (i.e., MDPsreward received goal states). give straightforward adaptationmain result problem setting general reward functions measurequality policy V ().learning-problem formulation similar spirit standard framework Probably Approximately Correct (PAC) learning. particular, assume targetpolicy comes finite class deterministic policies H. example, H may correspondset policies described bounded-length decision lists. addition,assume learner consistenti.e., returns policy H consistenttraining trajectories. assumptions, relatively small numbertrajectories (logarithmic |H|) sufficient ensure high probabilitylearned policy good target.Proposition 1. Let H finite class deterministic policies. H,set n = 1 ln |H|trajectories drawn independently Dh , 1 probabilityevery H consistent trajectories satisfies V () V () 2Vmax ( + h ).proof proposition Appendix. computational complexityfinding consistent policy depends policy class H. Polynomial-time algorithmsgiven interesting classes bounded-length decision listshowever,algorithms typically expensive policy classes consider practice. Rather,described Section 5.3, use learner based greedy heuristic search, oftenworks well practice.assumption target policy comes fixed size class H oftenviolated. However, pointed Khardon (1999b), straightforward giveextension Proposition 1 setting learner considers increasingly complexpolicies consistent one found. case, sample complexity relatedencoding size target policy rather size H, thus allowing uselarge expressive policy classes without necessarily paying full sample-complexityprice Proposition 1.4.2 Learning Rollout Policiesproof Proposition 1 relies critically fact policy class H containsdeterministic policies. However, main API loop, target policies computed viarollout hence stochastic due uncertainty introduced finite sampling. Thus,cannot directly use Proposition 1 context learning trajectories producedrollout. deal problem describe variant Improved-Trajectoriesreliably generate training trajectories deterministic policy 0 = PI (seeEquation 3), guaranteed improve improvement possible.Given base policy , first define (s) set actions maximizeQ (s, a). Note 0 (s) = min (s), minimum taken respect action ordering provided MDP. Importantly policy deterministic thus84fiAPI Policy Language Biasgenerate trajectories it, apply result learn close approximation. order generate trajectories 0 slightly modify Improved-Trajectories.modification introduced analysis only, experiments based procedures given Figure 1. modification replace action maximization stepImproved-Trajectories (second last statement loop), chooses nextaction execute, following two stepsA(, s) {a0 | maxa Q(s, a) Q(s, a0 ) }min A(, s)Q(s, a) estimate Qh (s, a) computed policy rollout using sampling widthw, newly introduced parameter.Note A(, s) = (s), selected action equal 0 (s). condition true every state encountered modified Improved-Trajectorieseffectively generate trajectories 0 . Thus, would like bound probabilityA(, s) 6= (s) small value appropriately choosing sampling width w,horizon h, . Unfortunately, choice parameters depends MDP.is, given particular parameter values, MDP eventA(, s) 6= (s) non-negligible probability state. reason firstdefine Q-advantage MDP show select appropriate parameter valuesgiven lower-bound .Given MDP policy , let 0 set states 0 ifftwo actions a0 Q (s, a) 6= Q (s, a0 ), i.e., actions distinctQ-values. Also state 0 define a1 (s) a2 (s) best action secondbest action respectively measured Q (s, a). Q-advantage defined =minsS 0 a1 (s) a2 (s), measures minimum Q-value gap optimalsub-optimal action state space. Given lower-bound Q-advantageMDP following proposition indicates select parameter values ensureA(, s) = (s) high probability.Proposition 2.MDP Q-advantage least , 0 < 0 < 1,h > log8Vmax8Vmax2w >=2ln|A|0state s, A(, s) = (s) probability least 1 0 .proof given Appendix. Thus, parameter values satisfying conditions, MDP Q-advantage least guaranteed probabilityleast 1 0 A(, s) = A(, s). means Improved-Trajectories correctly select action 0 (s) probability least 1 0 . Note proposition85fiFern, Yoon, & Givanagrees intuition h w increase decreasing Q-advantageincreasing Vmax also w increase decreasing 0 .6order generate n length h trajectories 0 , modified Improved-Trajectoriesroutine must compute set A(, ) n h states, yielding n h opportunities makeerror. ensure error made, modified procedure sets sampling width w. guarantees error free training set created probabilityusing 0 = 2nhleast 1 2 .Combining observation assumption 0 H apply Proposition01 follows. First, generate n = 1 ln 2|H|trajectories using modified ImprovedTrajectories routine (with 0 = 2nh). Next, learn policy trajectories usingconsistent learner. know probability generating imperfect training setbounded 2 , chosen value n, failure probability learneralso bounded 2 . Thus, get probability least 1 , learned policysatisfies V () V ( 0 ) 2Vmax ( + h ), giving approximation guarantee relativeimproved policy 0 . summarized following proposition.Proposition 3. Let H finite class deterministic policies, 0 < < 1, 0 < < 1.MDP Q-advantageleast , policy PI H, set11n > ln 2|H|trajectories produced modified Improved-Trajectories usingparameters satisfying,=2h > log8Vmax8Vmax 2 2nh|A|w >lnleast 1 probability every H consistent trajectories satisfiesV () V (PI ) 2Vmax ( + h ).One notable aspect result logarithmic dependencenumber actions |A| 1 . However, practical utility hindered dependencetypically known practice, exponentially smallplanning horizon. Unfortunately, dependence appears unavoidable typeapproach try learn trajectories PI produced rollout.particular setting parameters, always MDPsmall enough Q-advantage, value rollout policy arbitrarily worsePI .5. API Relational Planningwork motivated goal solving relational MDPs. particular, interested finding policies relational MDPs represent classical planning domains6. first glance appears lower-bound h decreases increasing Vmax decreasing .However, opposite true since base logarithm discount factor, strictly lessone. Also note since upper-bounded 2Vmax bound h always positive.86fiAPI Policy Language Biasstochastic variants. policies applied problem instanceplanning domain, hence viewed form domain-specific control knowledge.section, first describe straightforward way view classical planning domains(not single problem instances) relationally factored MDPs. Next, describerelational policy space policies compactly represented taxonomic decisionlists. Finally, present heuristic learning algorithm policy space.5.1 Planning Domains MDPs.say MDP hS, A, T, R, Ii relational defined giving finiteset objects O, finite set predicates P , finite set action types . factpredicate applied appropriate number objects, e.g., on(a, b) blocks-worldfact. state set facts, interpreted representing true facts state.state space contains possible sets facts. action action type appliedappropriate number objects, e.g., putdown(a) blocks-world action, actionspace set actions.classical planning domain describes set problem instances related structure,problem instance gives initial world state goal. example, blocksworld classical planning domain, problem instance specifies initial blockconfiguration set goal conditions. Classical planners attempt find solutionsspecific problem instances domain. Rather, goal solve entire planning domainsfinding policy applied problem instances. described below,straightforward view classical planning domain relational MDP MDPstate corresponds problem instance.State Action Spaces. classical planning domain specifies set actiontypes , world predicates W , possible world objects O. Together defineMDP action space. state MDP corresponds single problem instance (i.e.,world state goal) planning domain specifying current worldgoal. achieve letting set relational MDP predicates P = W G,G set goal predicates. set goal predicates contains predicateworld predicate W , named prepending g onto correspondingworld predicate name (e.g., goal predicate gclear corresponds world predicateclear). definition P see MDP states sets goal worldfacts, indicating true world facts problem instance goal conditions.important note, described below, MDP actions change worldfacts goal facts. Thus, large relational MDP viewed collectiondisconnected sub-MDPs, sub-MDP corresponds distinct goal condition.Reward Function. Given MDP state objective reach another MDP stategoal facts subset corresponding world factsi.e., reach world statesatisfies goal. call states goal states MDP. example,MDP state{on-table(a), on(a, b), clear(b), gclear(b)}goal state blocks-world MDP, would goal state without world factclear(b). represent objective reaching goal state quickly defining R assignreward zero actions taken goal states negative rewards actions87fiFern, Yoon, & Givanstates, representing cost taking actions. Typically, classical planningdomains, action costs uniformly -1, however, framework allows cost varyacross actions.Transition Function. classical planning domain provides action simulator(e.g., defined STRIPS rules) that, given world state action, returns new worldstate. define MDP transition function simulator modified treat goalstates terminal preserve without change goal predicates MDP state. Sinceclassical planning domains typically large number actions, action definitionsusually accompanied preconditions indicate legal actions given state,usually legal actions small subset possible actions. assumetreats actions legal no-ops. simplicity, relational MDP definitionexplicitly represent action preconditions, however, assume algorithmsaccess preconditions thus need consider legal actions. example,restrict rollout legal actions given state.Initial State Distribution. Finally, initial state distribution programgenerates legal problem instances (MDP states) planning domain. example, problem domains planning competitions commonly distributed problemgenerators.definitions, good policy one reach goal states via low-costaction sequences initial states drawn I. Note policies mappingsproblem instances actions thus sensitive goal conditions.way, learned policies able generalize across different goals. next describelanguage representing generalized policies.5.2 Taxonomic Decision List Policies.single argument action types, many useful rules planning domains take formapply action type object class C (Martin & Geffner, 2000). example,blocks world, pick clear block belongs table table,logistics world, unload object destination. Using conceptlanguage describing object classes, Martin Geffner (2000) introduced usedecision lists rules useful learning bias, showing promising experimentsdeterministic blocks world. motivation, consider policy space similarone used originally Martin Geffner, generalized handle multiple actionarguments. Also, historical reasons, concept language based upon taxonomicsyntax (McAllester, 1991; McAllester & Givan, 1993), rather description logicused Martin Geffner.Comparison Predicates. relational MDPs world goal predicates,corresponding classical planning domains, often useful polices comparecurrent state goal. end, introduce new set predicates, calledcomparison predicates, derived world goal predicates.world predicate p corresponding goal predicate gp, introduce new comparisonpredicate cp defined conjunction p gp. is, comparison-predicatefact true corresponding world goal predicates facts true.88fiAPI Policy Language Biasexample, blocks world, comparison-predicate fact con(a, b) indicatesb current state goali.e., on(a, b) gon(a, b) true.Taxonomic Syntax. Taxonomic syntax provides language writing class expressions represent sets objects properties interest serve fundamentalpieces build policies. Class expressions built MDP predicates(including comparison predicates applicable) variables. policy representation,variables used denote action arguments, runtime instantiatedobjects. simplicity consider predicates arity one two, callprimitive classes relations, respectively. domain contains predicates aritythree more, automatically convert multiple auxiliary binary predicates. Givenlist variables X = (x1 , . . . , xk ), class expressions given by,C[X] ::= C0 | xi | a-thing | C[X] | (R C[X]) | (min R)R ::= R0 | R 1 | RC[X] class expression, R relation expression, C0 primitive class, R0primitive relation, xi variable X. Note that, classical planning domains,primitive classes relations world, goal, comparison predicates. define depth d(C[X]) class expression C[X] one C[X] either primitiveclass, a-thing, variable, (min R), otherwise define d(C[X]) d(R C[X])d(C[X]) + 1, R relation expression C[X] class expression. givenrelational MDP denote Cd [X] set class expressions C[X] depthless.Intuitively class expression (R C[X]) denotes set objects relatedrelation R object set C[X]. expression (R C[X]) denotesset objects related R chain object C[X]thisconstructor important representing recursive concepts (e.g., blocks a).expression (min R) denotes set objects minimal relation R.formally, let MDP state = (o1 , . . . , ok ) variable assignment,assigns object oi variable xi . interpretation C[X] relativeset objects denoted C[X]s,O . primitive class C0 interpreted setobjects predicate symbol C0 true s. Likewise, primitive relation R0interpreted set object tuples relation R0 holds s. classexpression a-thing denotes set objects s. class expression xi , xivariable, interpreted singleton set {oi }. interpretation compoundexpressions given by,(C[X])s,O = {o | 6 C[X]s,O }(R C[X])s,O = {o | o0 C[X]s,O s.t. (o0 , o) Rs,O }(min R)s,O = {o | o0 s.t. (o, o0 ) Rs,O , 6 o0 s.t. (o0 , o) Rs,O }(R )s,O = ID {(o1 , ov ) | o2 , . . . , ov1 s.t. (oi , oi+1 ) Rs,O 1 < v}(R1 )s,O = {(o, o0 ) | (o0 , o) Rs,O }C[X] class expression, R relation expression, ID identity relation.examples useful blocks-world concepts, given primitive classes clear, gclear,holding, con-table, along primitive relations on, gon, con, are:89fiFern, Yoon, & Givan(gon1 holding) depth two, denotes block want blockheld.(on (on gclear)) depth three, denotes blocks currently blockswant make clear.(con con-table) depth two, denotes set blocks well constructedtowers. see note block bv class existssequence blocks b1 , . . . , bv b1 table goalcurrent state (i.e. con-table(b1 )) bi+1 bi goal current state(i.e. con(bi , bi+1 )) 1 < v.(gon (con con-table)) depth three, denotes blocks belong topcurrently well constructed tower.Decision List Policies represent policies decision lists action-selection rules.rule form a(x1 , . . . , xk ) : L1 , L2 , . . . Lm , k-argument action type,Li literals, xi action-argument variables. denote listaction argument variables X = (x1 , . . . , xk ). literal form x C[X],C[X] taxonomic syntax class expression x action-argument variable.Given MDP state list action-argument objects = (o1 , . . . , ok ), sayliteral xi C[X] true given iff oi C[X]s,O . say ruleR = a(x1 , . . . , xk ) : L1 , L2 , . . . Lm allows action a(o1 , . . . ok ) iff literal ruletrue given O. Note literals rule action type a,possible actions type allowed rule. rule viewed placing mutualconstraints tuples objects action type applied to. Notesingle rule may allow actions many actions one type. Given decision listrules say action allowed list allowed rule list,previous rule allows actions. Again, decision list may allow actionsmultiple actions one type. decision list L MDP defines deterministic policy[L] MDP. L allows actions state s, [L](s) least7 legal actions; otherwise, [L](s) least legal action allowed L. importantnote since [L] considers legal actions, specified action preconditions,rules need encode preconditions, allows simpler rules learning.words, think rule implicitly containing preconditionsaction type.example taxonomic decision list policy consider simple blocks-world domaingoal condition always clear red blocks. primitive classesdomain red, clear, holding, single relation on. followingpolicy solve problem domain.putdown(x1 ) : x1 holdingpickup(x1 ) : x1 clear, x1 (on (on red))7. action ordering relational MDP defined lexicographically terms orderings actiontypes objects.90fiAPI Policy Language Biasfirst rule cause agent putdown block held. Otherwise,block held, find block x1 clear red block (expressed(on (on red))) pick up. Appendix B gives examples complex policieslearned system experiments.5.3 Learning Taxonomic Decision Listsgiven relational MDP, define Rd,l set action-selection ruleslength l literals whose class expression depth d. Also, letHd,l denote policy space defined decision lists whose rules Rd,l . Sincenumber depth-bounded class expressions finite finite number rules,hence Hd,l finite, though exponentially large. implementation Learn-Policy,used main API loop, learns policy Hd,l user specified values l.use Rivest-style decision-list learning approach (Rivest, 1987)an approach alsotaken Martin Geffner (2000) learning class-based policies. primary differenceMartin Geffner (2000) technique method selecting individualrules decision list. use greedy, heuristic search, previous work usedexhaustive enumeration approach. difference allows us find rulescomplex, potential cost failing find good simple rules enumerationmight discover.Recall Section 3, training set given Learn-Policy contains trajectoriesrollout policy. learning algorithm, however, sensitive trajectorystructure (i.e., order trajectory elements) thus, simplify discussion,take input learner training set contains uniontrajectory elements. means trajectory set contains n length htrajectories, contain total n h training examples. described Section 3,training example form hs, (s), Q(s, a1 ), . . . , Q(s, )i, state,(s) action selected previous policy, Q(s, ai ) Q-value estimateQ (s, ai ). Note experiments training examples contain valueslegal actions state.Given training set D, natural learning goal find decision-list policytraining example selects action maximum estimated Q-value. learninggoal, however, problematic practice often several best (or closebest) actions measured true Q-function. case, due random sampling,particular action looks best according Q-value estimates training setarbitrary. Attempting learn concise policy matches arbitrary actionsdifficult best likely impossible.One approach (Lagoudakis & Parr, 2003) avoiding problem use statisticaltests determine actions clearly best (positive examples) onesclearly best (negative examples). learner asked findpolicy consistent positive negative examples. approachshown empirical success, potential shortcoming throwing awayQ-value information. particular, may always possible find policyexactly matches training data. cases, would like learner make informedtrade-offs regarding sub-optimal actionsi.e., prefer sub-optimal actions larger91fiFern, Yoon, & GivanLearn-Decision-List (D, d, l, b)// training set D, concept depth d, rule length l, beam width bL nil;(D empty)R Learn-Rule(D, d, l, b);{d | R covers d};L Extend-List(L, R); // add R end listReturn L;Learn-Rule(D, d, l, b)// training set D, concept depth d, rule length l, beam width baction type// compute rule action typeRa Beam-Search(D, d, l, b, a);Return argmaxa Hvalue(Ra , D);Beam-Search (D, d, l, b, a)// training set D, concept depth d, rule length l, beam width b, action typek arity a; X (x1 , . . . , xk );//L {(x C) | x X, C Cd [X]}; //X sequence action-argument variablesconstruct set depth bounded candidate literalsB0 { a(X) : nil }; 1; // initialize beam single rule literalsloopG = Bi1 {R Rd,l | R = Add-Literal(R0 , l), R0 Bi1 , l L};Bi Beam-Select(G, b, D); //select best b heuristic values+ 1;Bi1 = Bi ; //loop improvement heuristicReturn argmaxRBi Hvalue(R, D) //return best rule final beamFigure 2: Pseudo-code learning decision list Hd,l given training data D.procedure Add-Literal(R, l) simply returns rule literal l added endrule R. procedure Beam-Select(G, w, D) selects best b rules G differentheuristic values. procedure Hvalue(R, D) returns heuristic value rule R relativetraining data described text.Q-values. motivation, describe cost-sensitive decision-list learnersensitive full set Q-values D. learning goal roughly find decisionlist selects actions large cumulative Q-value training set.Learning List Rules. say decision list L covers training examplehs, (s), Q(s, a1 ), . . . , Q(s, )i L suggests action state s. Given set trainingexamples D, search decision list selects actions high Q-value viaiterative set-covering approach carried Learn-Decision-List. Decision-list rules92fiAPI Policy Language Biasconstructed one time order list covers training examples.Pseudo-code algorithm given Figure 2. Initially, decision list null listcover training examples. iteration, search high qualityrule R quality measured relative set currently uncovered training examples.selected rule appended current decision-list, training examples newlycovered selected rule removed training set. process repeatslist covers training examples. success approach depends heavilyfunction Learn-Rule, selects good rule relative uncovered trainingexamplestypically good rule one selects actions best (or close best)Q-value also covers significant number examples.Learning Individual Rules. input rule learner Learn-Rule settraining examples, along depth length parameters l, beam width b.action type a, rule learner calls routine Beam-Search find good ruleRa Rd,l action type a. Learn-Rule returns rule Ra highest valuemeasured heuristic, described later section.given action type a, procedure Beam-Search generates beam B0 , B1 . . .,Bi set rules Rd,l action type a. sets evolve specializingrules previous sets adding literals them, guided heuristic function. Searchbegins general rule a(X) : nil, allows action type state.Search iteration produces set Bi contains b rules highest different heuristicvalues among following set8G = Bi1 {R Rd,l | R = Add-Literal(R0 , l), R0 Bi1 , l L}L set possible literals depth less. set includescurrent best rules (those Bi1 ) also rule Rd,l formed addingnew literal rule Bi1 . search ends improvement heuristic valueoccurs, Bi = Bi1 . Beam-Search returns best rule Bi accordingheuristic.Heuristic Function. training instance hs, (s), Q(s, a1 ), . . . , Q(s, )i, define Q-advantage taking action ai instead (s) state (s, ai ) = Q(s, ai )Q(s, (s)). Likewise, Q-advantage rule R sum Q-advantages actionsallowed R s. Given rule R set training examples D, heuristic functionHvalue(R, D) equal number training examples rule covers pluscumulative Q-advantage rule training examples.9 Using Q-advantage ratherQ-value focuses learner toward instances large improvement previous policy possible. Naturally, one could consider using different weights coverageQ-advantage terms, possibly tuning weight automatically using validation data.8. Since many rules Rd,l equivalent, must prevent beam filling semanticallyequivalent rules. Rather deal problem via expensive equivalence testing take ad-hoc,practically effective approach. assume rules coincidentally heuristicvalue, ones must equivalent. Thus, construct beams whose membersdifferent heuristic values. choose rules value preferring shorter rules,arbitrarily.9. coverage term included, covering zero Q-advantage examplecovering it. zero Q-advantage good (e.g., previous policy optimal state).93fiFern, Yoon, & Givan6. Random Walk Bootstrappingtwo issues critical success API technique. First, APIfundamentally limited expressiveness policy language strengthlearner, dictates ability capture improved policy described trainingdata iteration. Second, API yield improvement Improved-Trajectoriessuccessfully generates training data describes improved policy. large classicalplanning domains, initializing API uninformed random policy typically resultessentially random training data, helpful policy improvement.example, consider MDP corresponding 20-block blocks world initialproblem distribution generates random initial goal states. case, randompolicy unlikely reach goal state within practical horizon time. Hence,rollout trajectories unlikely reach goal, providing guidance toward learningimproved policy (i.e., policy reliably reach goal).interested solving large domains this, providing guiding inputsAPI critical. Fern, Yoon, Givan (2003), showed bootstrapping APIdomain-independent heuristic planner FF (Hoffmann & Nebel, 2001), APIable uncover good policies blocks world, simplified logistics world (no planes),stochastic variants. approach, however, limited heuristics abilityprovide useful guidance, vary widely across domains.describe new bootstrapping procedure goal-based planning domains, basedrandom walks, guiding API toward good policies. planning system,evaluated Section 7, based integrating procedure API order findpolicies goal-based planning domains. non-goal-based MDPs, bootstrappingprocedure directly applied, bootstrapping mechanisms must usednecessary. might include providing initial non-trivial policy, providing heuristicfunction, form reward shaping (Mataric, 1994). Below, first describeidea random-walk distributions. Next, describe use distributionscontext bootstrapping API, giving new algorithm LRW-API.6.1 Random Walk DistributionsThroughout consider MDP = hS, A, T, R, Ii correspond goal-based planning domains, described Section 5.1. Recall state correspondsplanning problem, specifying world state (via world facts) set goal conditions (viagoal facts). use terms MDP state planning problem interchangeably.Note that, context, distribution planning problems. conveniencedenote MDP states tuples = (w, g), w g sets world factsgoal facts respectively.Given MDP state = (w, g) set goal predicates G, define s|GMDP state (w, g 0 ) g 0 contains goal facts g applications predicateG. Given set goal predicates G, define n-step random-walk problemdistribution RW n (M, G) following stochastic algorithm:1. Draw random state s0 = (w0 , g0 ) initial state distribution I.94fiAPI Policy Language Bias2. Starting s0 take n uniformly random actions10 , giving state sequence (s0 , . . . , sn ),sn = (wn , g0 ) (recall actions change goal facts). uniformlyrandom action selection, assume extra no-op action (that changestate) selected fixed probability, reasons explained below.3. Let g set goal facts corresponding world facts wn , e.g.,wn = {on(a, b), clear(a)}, g = {gon(a, b), gclear(a)}. Return planningproblem (MDP state) (s0 , g)|G output.sometimes abbreviate RW n (M, G) RW n G clear context.Intuitively, perform well distribution policy must able achieve factsinvolving goal predicates typically result n-step random walkinitial state. restricting set goal predicates G specify types factsinterested achievinge.g., blocks world may interestedachieving facts involving predicate.random-walk distributions provide natural way span range problem difficulties. Since longer random walks tend take us initial state, smalln typically expect planning problems generated RW n becomedifficult n grows. However, n becomes large, problems generated require farfewer n steps solvei.e., direct paths initial stateend state long random walk. Eventually, since finite, problem difficultystop increasing n.question raised idea whether, large n, good performance RW nensures good performance problem distributions interest domain.domains, simple blocks world11 , good random-walk performanceseem yield good performance distributions interest. domains,grid world (with keys locked doors), intuitively, random walk unlikelyuncover problem requires unlocking sequence doors. Indeed, since RW ninsensitive goal distribution underlying planning domain, random-walkdistribution may quite different.believe good performance long random walks often useful,addressing one component difficulty many planning benchmarks. successfullyaddress problems components difficulty, planner need deploy orthogonal technology landmark extraction setting subgoals (Hoffman, Porteous, &Sebastia, 2004). example, grid world, could automatically set subgoalpossessing key first door, long random-walk policy could provide usefulmacro getting key.purpose developing bootstrapping technique API, limit focusfinding good policies long random walks. experiments, define longspecifying large walk length N . Theoretically, inclusion no-op actiondefinition RW ensures induced random-walk Markov chain12 aperiodic,10. practice, select random actions set applicable actions state si , providedsimulator makes possible identify set.11. blocks world large n, RW n generates various pairs random block configurations, typicallypairing states far apartclearly, policy performs well distribution capturedsignificant information blocks world.12. dont formalize chain here, various formalizations work well.95fiFern, Yoon, & Givanthus distribution states reached increasingly long random walks convergesstationary distribution13 . Thus RW = limn RW n well-defined, takegood performance RW goal.6.2 Random-Walk BootstrappingMDP , define [I 0 ] MDP identical initial statedistribution replaced 0 . also define success ratio SR(, [I]) [I]probability solves problem drawn I. Also treating random variable,average length AL(, [I]) [I] conditional expectation solutionlength problems drawn given solves I. Typically solution lengthproblem taken number actions, however, action costs uniform,length taken sum action costs. Note MDP formulationclassical planning domains, given Section 5.1, policy achieves high V ()also high success ratio low average cost.Given MDP set goal predicates G, system attempts find goodpolicy [RW N ], N selected large enough adequately approximateRW , still allowing tractable completion learning. Naively, given initialrandom policy 0 , could try apply API directly. However, already discussed,work general, since interested planning domains RW producesextremely large difficult problems random policies provide ineffective startingpoint.However, small n (e.g., n = 1), RW n typically generates easy problems,likely API, starting even random initial policy, reliably find goodpolicy RW n . Furthermore, expect policy n performs well RW n ,also provide reasonably good, perhaps perfect, guidance problems drawnRW moderately larger n. Thus, expect able findgood policy RW bootstrapping API initial policy n . suggests naturaliterative bootstrapping technique find good policy large n (in particular, n = N ).Figure 3 gives pseudo-code procedure LRW-API integrates APIrandom-walk bootstrapping find policy long-random-walk problem distribution.Intuitively, algorithm viewed iterating two stages: first, findinghard enough distribution current policy (by increasing n); and, then, finding goodpolicy hard distribution using API. algorithm maintains current policycurrent walk length n (initially n = 1). long success ratio RWnsuccess threshold , constant close one, simply iterate stepsapproximate policy improvement. achieve success ratio policy ,if-statement increases n success ratio RW n falls . is,performs well enough current n-step distribution move distributionslightly harder. constant determines much harder set small enoughlikely used bootstrap policy improvement harder distribution.(The simpler method increasing n 1 whenever success ratio achieved also13. Markov chain may irreducible, stationary distribution may reachedinitial states; however, considering one initial state, described I.96fiAPI Policy Language BiasLRW-API (N, G, n, w, h, M, 0 , )// max random-walk length N , goal predicates G// training set size n, sampling width w, horizon h,// MDP , initial policy 0 , discount factor .0 ; n 1;loopc (n) >SR// Find harder n-step distribution .c (i) < , N none;n least [n, N ] s.t. SR0 = [RW n (M, G)];Improved-Trajectories(n, w, h, 0 , );Learn-Policy(T );satisfiedReturn ;c (n) estimates success ratio planningFigure 3: Pseudo-code LRW-API. SRdomain problems drawn RW n (M, G) drawing set problems returningfraction solved . Constants described text.find good policies whenever method does. take much longer, may runAPI repeatedly training set already good policy.)n becomes equal maximum walk length N , n = N futureiterations. important note even find policy good success ratioRW N may still possible improve average length policy. Thus,continue API distribution satisfied success ratioaverage length current policy.7. Relational Planning Experimentssection, evaluate LRW-API technique relational MDPs correspondingdeterministic stochastic classical planning domains. first give results numberdeterministic benchmark domains, showing promising results comparison stateof-the-art planner FF (Hoffmann & Nebel, 2001), also highlighting limitationsapproach. Next, give results several stochastic planning domains includingdomain-specific track 2004 International Probabilistic Planning Competition(IPPC). domain definitions problem generators used experimentsavailable upon request.experiments, use policy learner described Section 5.3 learntaxonomic decision list policies. cases, number training trajectories 100,policies restricted rules depth bound length bound l. discount97fiFern, Yoon, & Givanfactor always one, LRW-API always initialized policy selectsrandom actions. utilize maximum-walk-length parameter N = 10, 000 setequal 0.9 0.1 respectively.7.1 Deterministic Planning Experimentsperform experiments seven familiar STRIPS planning domains including usedAIPS-2000 planning competition, used evaluate TL-Plan BacchusKabanza (2000), Gripper domain. domain standard problem generatoraccepts parameters, control size difficulty randomly generatedproblems. list domain parameters associated them. detaileddescription domains found Hoffmann Nebel (2001).Blocks World (n) : standard blocks worlds n blocks.Freecell (s, c, f, l) : version Solitaire suits, c cards per suit, f freecells,l columns.Logistics (a,c,l,p) : logistics transportation domain airplanes, c cities, llocations, p packages.Schedule (p) : job shop scheduling domain p parts.Elevator (f, p) : elevator scheduling f floors p people.Gripper (b) : robotic gripper domain b balls.Briefcase (i) : transportation domain items.LRW Experiments. first set experiments evaluates ability LRW-APIfind good policies RW . utilize sampling width one rollout, sincedeterministic domains. Recall iteration LRW-API compute(approximately) improved policy may also increase walk length n find harderproblem distribution. continued iterating LRW-API observedimprovement. training time per iteration approximately five hours.14 Thoughinitial training period significant, policy learned used solve newproblems quickly, terminating seconds solution one found, evenlarge problems.Figure 4 provides data iteration LRW-API seven domainsindicated parameter settings. first column, domain, indicatesiteration number (e.g., Blocks World run 8 iterations). second columnrecords walk length n used learning corresponding iteration. thirdfourth columns record SR AL policy learned corresponding iteration14. timing information relatively unoptimized Scheme implementation. reimplementationC would likely result 5-10 fold speed-up.98finRW nSRALiter. #iter. #API Policy Language BiasRWSRALnBlocks World (20)12345678414545454543343340.920.940.560.780.880.980.840.99FF2.05.615.015.033.725.145.637.800.100.170.320.650.900.8710.9658303030303030300.970.970.650.720.900.810.780.900.93FF1.42.77.07.16.76.76.86.97.70.080.260.780.850.850.890.870.890.931RWSRALLogistics (1,2,2,6)041.442.840.247.043.950.143.349.012345678910434445Freecell (4,2,2,4)123456789RW nSRAL3.66.37.07.06.36.66.86.67.95.454545454545454545454545450.860.860.810.860.760.760.860.760.700.810.740.900.92FF3.16.56.96.86.15.96.26.96.16.16.46.96.60.250.280.310.280.280.320.390.310.190.250.250.390.38111.37.28.48.97.88.49.111.07.87.69.09.39.4130.481127343600.2110383028Schedule (20)12140.791FF13.45Briefcase (10)Elevator (20,10)12014.0FF112623111313123515150.910.891FF1.44.23.0Gripper (10)1101FF3.8Figure 4: Results iteration LRW-API seven deterministic planning domains.iteration, show walk length n used learning, along success ratio(SR) average length (AL) learned policy RW n RW . finalpolicy shown domain performs = 0.9 SR walks length N = 10, 000(with exception Logistics), iteration improve performance.benchmark also show SR AL planner FF problems drawnRW .measured 100 problems drawn RW n corresponding value n (i.e.,distribution used learning). SR exceeds , next iteration seeksincreased walk length n. fifth sixth columns record SR AL99fiFern, Yoon, & Givanpolicy, measured 100 problems drawn LRW target distribution RW ,experiments approximated RW N N = 10, 000.So, example, see Blocks World total 8 iterations,learn first one iteration n = 4, one iteration n = 14, four iterationsn = 54, two iterations n = 334. point see resultingpolicy performs well RW . iterations n = N , shown, showedimprovement policy found iteration eight. domains, also observedimprovement iterating n = N , thus show iterations.note domains except Logistics (see below) achieve policies good performanceRW N learning much shorter RW n distributions, indicating indeedselected large enough value N capture RW , desired.General Observations. several domains, learner bootstraps quicklyshort random-walk problems, finding policy works well even much longerrandom-walk problems. include Schedule, Briefcase, Gripper, Elevator. Typically, large problems domains many somewhat independent subproblemsshort solutions, short random walks generate instances different typicalsubproblems. domains, best LRW policy found small numberiterations performs comparably FF RW . note FF consideredgood domain-independent planner domains, consider successfulresult.two domains, Logistics15 Freecell, planner unable find policysuccess ratio one RW . believe result limited knowledge representation allowed policies following reasons. First, cannot write goodpolicies domains within current policy language. example, logistics, oneimportant concept set containing packages trucks truckpackages goal city. However, domain defined way conceptcannot expressed within language used experiments. Second, final learneddecision lists Logistics Freecell, Appendix B, contain much largernumber specific rules lists learned domains. indicateslearner difficulty finding general rules, within language restrictions,applicable large portions training data, resulting poor generalization. Third,success ratio (not shown) sampling-based rollout policy, i.e., improved policysimulated Improved-Trajectories, substantially higher resultinglearned policy becomes policy next iteration. indicates LearnDecision-List learning much weaker policy sampling-based policy generatingtraining data, indicating weakness either policy language learning algorithm. example, logistics domain, iteration eight, training data learningiteration-nine policy generated sampling rollout policy achieves success ratio0.97 100 training problems drawn RW 45 distribution, learnediteration-nine policy achieves success ratio 0.70, shown figure iterationnine. Extending policy language incorporate expressiveness appearsrequired domains require sophisticated learning algorithm,point future work.15. Logistics, planner generates long sequence policies similar, oscillating success ratioelided table ellipsis space reasons.100fiAPI Policy Language BiasDomainBlocksFFSR AL0.81600.28 158Size(20)(50)SR11AL54151Freecell(4,2,2,4)(4,13,4,8)0.3601510.4710112Logistics(1,2,2,6)(3,10,2,30)0.8706116158Elevator(60,30)1112198Schedule(50)11751212Briefcase(10)(50)11301621029Gripper(50)11491149Figure 5: Results standard problem distributions seven benchmarks. Success ratio(SR) average length (AL) provided FF policy learned LRWproblem distribution. given domain, learned LRW policy usedproblem size shown.remaining domain, Blocks World, bootstrapping provided increasinglylong random walks appears particularly useful. policies learned walklengths 4, 14, 54, 334 increasingly effective target LRW distribution RW .walks length 54 334, takes multiple iterations master provided leveldifficulty beyond previous walk length. Finally, upon mastering walk length 334,resulting policy appears perform well walk length. learned policy modestlysuperior FF RW success ratio average length.Evaluation Original Problem Distributions. domain denotebest learned LRW policyi.e., policy, domain, highestperformance RW , shown Figure 4. taxonomic decision lists correspondingdomain given Appendix B. Figure 5 shows performance ,comparison FF, original intended problem distributions domains.measured success ratio systems giving time limit 100 seconds solveproblem. attempted select largest problem sizes previously usedevaluation domain-specific planners, either AIPS-2000 Bacchus Kabanza(2000), well show smaller problem size cases one plannersshow performed poorly large size. case, use problem generatorsprovided domains, evaluate 100 problems size.Overall, results indicate learned, reactive policies competitivedomain-independent planner FF. important remember policieslearned domain-independent fashion, thus LRW-API viewed generalapproach generating domain-specific reactive planners. two domains, Blocks World101fiFern, Yoon, & GivanBriefcase, learned policies substantially outperform FF success ratio, especiallylarge domain sizes. three domains, Elevator, Schedule, Gripper, two approaches perform quite similarly success ratio, approach superior averagelength Schedule FF superior average length Elevator.two domains, Logistics Freecell, FF substantially outperforms learned policies success ratio. believe partly due inadequate policy language,discussed above. also believe, however, another reason poor performancelong-random-walk distribution RW correspond well standardproblem distributions. seems particularly true Freecell. policy learnedFreecell (4,2,2,4) achieved success ratio 93 percent RW , however, standard distribution achieved 36 percent. suggests RW generates problemssignificantly easier standard distribution. supported factsolutions produced FF standard distribution average twice longproduced RW . One likely reason easy random walksend dead states Freecell, actions applicable. Thus random walkdistribution typically produce many problems goals correspond deadstates. standard distribution hand treat dead states goals.7.2 Probabilistic Planning Experimentspresent experiments three probabilistic domains described probabilistic planning domain language PPDDL (Younes, 2003).Ground Logistics (c, p) : probabilistic version logistics airplanes, ccities p packages. driving action probability failure domain.Colored Blocks World (n) : probabilistic blocks world n colored blocks,goals involve constructing towers certain color patterns. probabilitymoved blocks fall floor.Boxworld (c, p) : probabilistic version full logistics c cities p packages.Transportation actions probability going wrong direction.Ground Logistics domain originally Boutilier et al. (2001), also usedevaluation Yoon et al. (2002). Colored Blocks World Boxworld domainsdomains used hand-tailored track IPPC LRW-API techniqueentered. hand-tailored track, participants provided problem generatorsdomain competition allowed incorporate domain knowledgeplanner use competition time. provided problem generators LRW-APIlearned policies domains, entered competition.also conducted experiments probabilistic domains Yoon et al.(2002), including variants blocks world variant Ground Logistics,appeared Fern et al. (2003). However, show results sincequalitatively identical deterministic blocks world results describedGround Logistics results show below.three probabilistic domains, conducted LRW experiments usingprocedure above. parameters given LRW-API except102finSRRW nALiter. #iter. #API Policy Language BiasRWSRALBoxworld (10,5)110 0.734.3210 0.932.3320 0.914.4440 0.966.15 170 0.6230.837.96 170 0.497 170 0.6329.329.18 170 0.639 170 0.4836.4Standard Distribution (15,15)nSRRW nALSRRWALGround Logistics (3,4,4,3)0.030.130.170.310.250.170.210.180.17061.558.455.950.452.255.75555.355.315 0.95210 0.973 1601Standard Distribution2.712.066.41(5,7,7,20)0.170.8411168.917.57.220Colored Blocks World (10)123452 0.861.75 0.898.440 0.9211.7100 0.7637.5100 0.9420.0Standard Distribution (50)0.190.810.850.770.950.9593.640.832.738.521.9123Figure 6: Results iteration LRW-API three probabilistic planning domains.iteration, show walk length n used learning, along success ratio(SR) average length (AL) learned policy RW n RW .benchmark, show performance standard problem distribution policy whoseperformance best RW .sampling width used rollout set w = 10, set 0.85 orderaccount stochasticity domains. results experiments shownFigure 6. tables form Figure 4 last row givendomain gives performance standard distribution, i.e., problems drawndomains problem generator. Colored Blocks World problem generatorproduces problems whose goals specified using existential quantifiers. example,simple goal may exists blocks x x red, blue x y.Since policy language cannot directly handle existentially quantified goals preprocessplanning problems produced problem generator remove them. doneassigning particular block names existential variables, ensuring staticproperties block (in case color) satisfied static properties variableassigned to. domain, finding assignment trivial, resultingassignment taken goal, giving planning problem learned policyapplied. Since blocks world states fully connected, resulting goal alwaysguaranteed achievable.Boxworld, LRW-API able find good policy RW standarddistribution. Again, deterministic Logistics Freecell, believeprimarily restricted policy languages currently used learner.Here, domains, see decision list learned Boxworld contains manyspecific rules, indicating learner able generalize well beyond103fiFern, Yoon, & Givantraining trajectories. Ground Logistics, see LRW-API quickly finds goodpolicy RW standard distribution.Colored Blocks World, also see LRW-API able quickly find goodpolicy RW standard distribution. However, unlike deterministic(uncolored) blocks world, success ratio observed less one, solving 95percent problems. unclear, LRW-API able find perfect policy.relatively easy hand-code policy Colored Blocks World using languagelearner, hence inadequate knowledge representation answer. predicatesaction types domain deterministic counterpartstochastic variants previously considered. difference apparentlyinteracts badly learners search bias, causing fail find perfect policy.Nevertheless, two results, along probabilistic planning results shownhere, indicate good policy expressible language, LRW-APIfind good policies complex relational MDPs. makes LRW-API onetechniques simultaneously cope complexity resulting stochasticityrelational structure domains these.8. Related WorkBoutilier et al. (2001) presented first exact solution technique relational MDPsbased structured dynamic programming. However, practical implementationapproach provided, primarily due need simplification first-orderlogic formulas. ideas, however, served basis logic-programming-basedsystem (Kersting, Van Otterlo, & DeRaedt, 2004) successfully applied blocksworld problems involving simple goals simplified logistics world. style approachinherently limited domains exact value functions and/or policiescompactly represented chosen knowledge representation. Unfortunately,generally case types domains consider here, particularly planninghorizon grows. Nevertheless, providing techniques directly reasonMDP model important direction. Note API approach essentially ignoresunderlying MDP model, simply interacts MDP simulator black box.interesting research direction consider principled approximations techniques discover good policies difficult domains. consideredGuestrin et al. (2003a), class-based MDP value function representationused compute approximate value function could generalize across different setsobjects. Promising empirical results shown multi-agent tactical battle domain.Presently class-based representation support representation features commonly found classical planning domains (e.g., relational factson(a, b) change time), thus directly applicable contexts. However, extending work richer representations interesting direction. abilityreason globally domain may give advantages compared API.approach closely related work relational reinforcement learning (RRL) (Dzeroski et al., 2001), form online API learns relational value-function approximations. Q-value functions learned form relational decision trees (Q-trees)used learn corresponding policies (P -trees). RRL results clearly demonstrate104fiAPI Policy Language Biasdifficulty learning value-function approximations relational domains. Compared P trees, Q-trees tend generalize poorly much larger. RRL yet demonstratedscalability problems complex considered hereprevious RRL blocks-worldexperiments include relatively simple goals16 , lead value functions muchless complex ones here. reason, suspect RRL would difficultydomains consider, precisely value-function approximation stepavoid; however, needs experimentally tested.note, however, API approach advantage using unconstrainedsimulator, whereas RRL learns irreversible world experience (pure RL). usingsimulator, able estimate Q-values actions training state,providing us rich training data. Without simulator, RRL able directlyestimate Q-value action training statethus, RRL learns Q-treeprovide estimates Q-value information needed learn P -tree. way, valuefunction learning serves critical role simulator unavailable. believe,many relational planning problems, possible learn model simulatorworld experiencein case, API approach incorporated planningcomponent RRL. Otherwise, finding ways either avoid learning effectivelylearn relational value-functions RRL interesting research direction.Researchers classical planning long studied techniques learning improveplanning performance. collection survey work learning planning domains see Minton (1993) Zimmerman Kambhampati (2003). Two primary approaches learn domain-specific control rules guiding search-based planners e.g.,Minton, Carbonell, Knoblock, Kuokka, Etzioni, Gil (1989), Veloso, Carbonell, Perez,Borrajo, Fink, Blythe (1995), Estlin Mooney (1996), Huang, Selman, Kautz(2000), Ambite, Knoblock, Minton (2000), Aler, Borrajo, Isasi (2002), and,closely related, learn domain-specific reactive control policies (Khardon, 1999a; Martin& Geffner, 2000; Yoon et al., 2002).Regarding latter, work novel using API iteratively improve stand-alonecontrol policies. Regarding former, theory, search-based planners iterativelyimproved continually adding newly learned control knowledgehowever, difficult avoid utility problem (Minton, 1988), i.e., swamped low utility rules.Critically, policy-language bias confronts issue preferring simpler policies.learning approach also tied base planner (let alone tied single particular base planner), unlike previous work. Rather, require domain simulator.ultimate goal systems allow planning large, difficult problemsbeyond reach domain-independent planning technology. Clearly, learningachieve goal requires form bootstrapping almost previous systemsrelied human purpose. far, common human-bootstrappingapproach learning small problems. Here, human provides small problemdistribution learner, limiting number objects (e.g., using 2-5 blocksblocks world), control knowledge learned small problems. approachwork, human must ensure small distribution good control knowledgesmall problems also good large target distribution. contrast, long16. complex blocks-world goal RRL achieve on(A, B) n block environment.consider blocks-world goals involve n blocks.105fiFern, Yoon, & Givanrandom-walk bootstrapping approach applied without human assistance directlylarge planning domains. However, already pointed out, goal performing wellLRW distribution may always correspond well particular target problemdistribution.bootstrapping approach similar spirit bootstrapping framework learning exercises(Natarajan, 1989; Reddy & Tadepalli, 1997). Here, learner provided planning problems, exercises, order increasing difficulty. learningeasier problems, learner able use new knowledge, skills, order bootstrap learning harder problems. work, however, previously reliedhuman provide exercises, typically requires insight planning domainunderlying form control knowledge planner. work viewedautomatic instantiation learning exercises, specifically designed learning LRWpolicies.random-walk bootstrapping similar approach used Micro-Hillary(Finkelstein & Markovitch, 1998), macro-learning system problem solving.work, instead generating problems via random walks starting initial state, randomwalks generated backward goal states. approach assumes actionsinvertible given set backward actions. assumptions hold,backward random-walk approach may preferable provided goaldistribution match well goals generated forward random walks.course, cases forward random walks may preferable. Micro-Hillaryempirically tested N N sliding-puzzle domain; however, discussed work,remain challenges applying system complex domains parameterized actions recursive structure, familiar STRIPS domains. bestknowledge, idea learning random walks previously exploredcontext STRIPS planning domains.idea searching good policy directly policy space rather value-functionspace primary motivation policy-gradient RL algorithms. However, algorithmslargely explored context parametric policy spaces. approachdemonstrated impressive success number domains, appears difficult definepolicy spaces types planning problem considered here.API approach viewed type reduction planning reinforcementlearning classification learning. is, solve MDP generating solvingseries cost-sensitive classification problems. Recently, severalproposals reducing reinforcement learning classification. Dietterich Wang (2001)proposed reinforcement learning approach based batch value function approximation.One proposed approximations enforced learned approximation assignbest action highest value, type classifier learning. Lagoudakis Parr(2003) proposed classification-based API approach closely related ours. primary difference form classification problem produced iteration.generate standard multi-class classification problems, whereas generate cost-sensitiveproblems. Bagnell, Kakade, Ng, Schneider (2003) introduced closely related algorithm learning non-stationary policies reinforcement learning. specified horizontime h, approach learns sequence h policies. iteration, policiesheld fixed except one, optimized forming classification problem via policy106fiAPI Policy Language Biasrollout17 . Finally, Langford Zadrozny (2004) provide formal reduction reinforcement learning classification, showing -accurate classification learning impliesnear-optimal reinforcement learning. approach uses optimistic variant sparsesampling generate h classification problems, one horizon time step.9. Summary Future Workintroduced new variant API learns policies directly, without representingapproximate value functions. allowed us utilize relational policy languagelearning compact policy representations. also introduced new API bootstrappingtechnique goal-based planning domains. experiments show LRW-APIalgorithm, combines techniques, able find good policies varietyrelational MDPs corresponding classical planning domains stochastic variants.know previous MDP technique successfully applied problemsthese.experiments also pointed number weaknesses current approach. First,bootstrapping technique, based long random walks, always correspondwell problem distribution interest. Investigating automatic bootstrappingtechniques interesting direction, related general problems explorationreward shaping reinforcement learning. Second, seen limitationscurrent policy language learner partly responsible failuressystem. cases, must either: 1) depend human provide useful featuressystem, 2) extend policy language develop advanced learning techniques. Policy-language extensions considering include various extensionsknowledge representation used represent sets objects domain (in particular,route-finding maps/grids), well non-reactive policies incorporate searchdecision-making.consider ever complex planning domains, inevitable brute-forceenumeration approach learning policies trajectories scale. Presentlypolicy learner, well entire API technique, makes attempt use definitiondomain one available. believe developing learner exploitinformation bias search good policies important direction future work.Recently, Gretton Thiebaux (2004) taken step direction using logicalregression (based domain model) generate candidate rules learner. Developing tractable variations approach promising research direction. addition,exploring ways incorporating domain model approach modelblind approaches critical. Ultimately, scalable AI planning systems need combineexperience stronger forms explicit reasoning.17. initial state distribution dictated policies previous time steps, held fixed.Likewise actions selected along rollout trajectories dictated policies future time steps,also held fixed.107fiFern, Yoon, & GivanAcknowledgmentswould like thank Lin Zhu originally suggesting idea using random walksbootstrapping. would also like thank reviewers editors helpingvastly improve paper. work supported part NSF grants 9977981-IIS0093100-IIS.Appendix A. Omitted ProofsProposition 1. Let H finite class deterministic policies. H,set n = 1 ln |H|trajectories drawn independently Dh , 1 probabilityevery H consistent trajectories satisfies V () V () 2Vmax ( + h ).Proof: first introduce basic properties notation used below.deterministic policy , consistent trajectory t, Dh (t) entirelydetermined underlying MDP transition dynamics. implies two deterministic policies 0 consistent trajectory Dh (t) = Dh (t).denote v(t) cumulative discounted reward accumulated executing trajectoryPt. policy , V h () = Dh (t) v(t) summation takenlength h trajectories (or simply consistent ). Finally setPtrajectories let Dh () = t0 Dh (t) giving cumulative probabilitygenerating trajectories .Consider particular H H consistent n trajectories. let denote set length h trajectories consistentdenote set trajectories consistent . Following Khardon (1999b)first give standard argument showing high probability Dh () > 1 . seeconsider probability consistent n = 1 ln |H|trajectoriesgiven Dh () 1 . probability occurs (1 )n < en = |H|.Thus probability choosing |H| |H|= . Thus, probabilityleast 1 know Dh () > 1 . Note Dh () = Dh ().given condition Dh () > 1 show V h () V h () 2Vmaxconsidering difference two value functions.V h () V h () =XDh (t) v(t)=X=v(t) +X(Dh (t) Dh (t)) v(t)Dh (t)Dh (t) v(t)Dh (t)XXv(t) + 0Dh (t)) + Dh ( )]Dh () + 1 Dh ()]Vmax [Dh (= Vmax [1X2Vmax108Xv(t)Dh (t) v(t)fiAPI Policy Language Biasthird lines follows since Dh (t) = Dh (t) consistent t.last line follows substituting assumption Dh () = Dh () > 1 previousline. Combining result approximation due using finite horizon,V () V () V h () V h () + 2 h Vmaxget probability least 1 , V () V () 2Vmax ( + h ), completesproof. 2Proposition 2.MDP Q-advantage least , 0 < 0 < 1,h > log8Vmax8Vmax2w >=2ln|A|0state s, A(, s) = (s) probability least 1 0 .Proof: Given real valued random variable X bounded absolute value Xmaxaverage X w independently drawn samples X,q additive Chernoff bound statesprobability least 1 , |E[X] X| Xmax wln .Note Qh (s, a) expectation random variable X(s, a) = R(s, a) +Vh1 (T (s, a)) Q(s, a) simply average w independent samples X(s, a).0Chernoff bound tells us probability least 1 |A|, |Qh (s, a) Q(s, a)|q0Vmax ln |A|ln, |A| number actions. Substituting choice wwget probability least 1 0 , |Qh (s, a) Q(s, a)| < 8 satisfied actionssimultaneously. also know |Q (s, a) Qh (s, a)| h Vmax , choiceh gives, |Q (s, a) Qh (s, a)| < 8 . Combining relationships getprobability least 1 0 , |Q (s, a) Q(s, a)| < 4 holds actions simultaneously.use bound show high probability Q-value estimatesactions (s) within 2 range other, actions outsiderange. particular, consider action (s) action a0 .a0 (s) Q (s, a) = Q (s, a0 ). bound get|Q(s, a) Q(s, a0 )| < 2 . Otherwise a0 6 (s) assumption MDPQ-advantage get Q (s, a) Q (s, a0 ) . Using bound impliesQ(s, a) Q(s, a0 ) > 2 . relationships definition A(, s) implyprobability least 1 0 A(, s) = (s). 2Appendix B. Learned Policiesgive final taxonomic-decision-list policies learned domainexperiments. Rather write rules form a(x1 , . . . , xk ) : L1 L2 Lm109fiFern, Yoon, & Givandrop variables head simply write, : L1 L2 Lm . additionuse notation R short-hand (R1 ) R relation. interpreting policies, important remember rule action type a,preconditions action type implicitly included constraints. Thus, rulesoften allow actions legal, actions never consideredsystem.Gripper1. MOVE: (X1 (NOT (GAT (CARRY1 GRIPPER)))) (X2 (NOT (GAT (AT1 AT-ROBBY)))) (X2 (GAT (NOT(CAT1 ROOM)))) (X1 (CAT BALL))2. DROP: (X1 (GAT1 AT-ROBBY))3. PICK: (X1 (GAT1 (GAT (CARRY1 GRIPPER)))) (X1 (GAT1 (NOT AT-ROBBY)))4. PICK: (X2 (AT (NOT (GAT1 ROOM)))) (X1 (GAT1 (NOT AT-ROBBY)))5. PICK: (X1 (GAT1 (NOT AT-ROBBY)))Briefcase1. PUT-IN: (X1 (GAT1 (NOT IS-AT)))2. MOVE: (X2 (AT (NOT (CAT1 LOCATION)))) (X2 (NOT (AT (GAT1 CIS-AT))))3. MOVE: (X2 (GAT IN)) (X1 (NOT (CAT IN)))4. TAKE-OUT: (X1 (CAT1 IS-AT))5. MOVE: (X2 GIS-AT)6. MOVE: (X2 (AT (GAT1 CIS-AT)))7. PUT-IN: (X1 UNIVERSAL)Schedule1. DO-IMMERSION-PAINT: (X1 (NOT (PAINTED1 X2 ))) (X1 (GPAINTED1 X2 ))2. DO-DRILL-PRESS: (X1 (GHAS-HOLEO1 X3 )) (X1 (GHAS-HOLEW1 X2 ))3. DO-LATHE: (X1 (NOT (SHAPE1 CYLINDRICAL))) (X1 (GSHAPE1 CYLINDRICAL))4. DO-DRILL-PRESS: (X1 (GHAS-HOLEW1 X2 ))5. DO-DRILL-PRESS: (X1 (GHAS-HOLEO1 X3 ))6. DO-GRIND: (X1 (NOT (SURFACE-CONDITION1 SMOOTH))) (X1 (GSURFACE-CONDITION1 SMOOTH))7. DO-POLISH: (X1 (NOT (SURFACE-CONDITION1 POLISHED))) (X1 (GSURFACE-CONDITION1 POLISHED))8. DO-TIME-STEP:Elevator1. DEPART: (X2 GSERVED)2. DOWN: (X2 (DESTIN BOARDED)) (X2 (DESTIN GSERVED))3. UP: (X2 (DESTIN BOARDED)) (X2 (DESTIN GSERVED)) (X2 (ABOVE (ORIGIN BOARDED))) (X1 (NOT(DESTIN BOARDED)))4. BOARD: (X2 (NOT CSERVED)) (X2 GSERVED)5. UP: (X2 (ORIGIN GSERVED)) (X2 (NOT (DESTIN BOARDED))) (X2 (NOT (DESTIN GSERVED))) (X2(ORIGIN (NOT CSERVED))) (X2 (ABOVE (DESTIN PASSENGER))) (X1 (NOT (DESTIN BOARDED)))6. DOWN: (X2 (ORIGIN GSERVED)) (X2 (ORIGIN (NOT CSERVED))) (X1 (NOT (DESTIN BOARDED)))110fiAPI Policy Language Bias7. UP: (X2 (NOT (ORIGIN BOARDED))) (X2 (NOT (DESTIN BOARDED)))FreeCell1. SENDTOHOME: (X1 (CANSTACK1 (CANSTACK (SUIT1 (SUIT INCELL))))) (X5 (NOT GHOME))2. MOVE-B: (X2 (NOT (CANSTACK (ON GHOME)))) (X2 (CANSTACK GHOME)) (X2 (VALUE1 (NOTCOLSPACE))) (X1 (CANSTACK1 (SUIT1 (SUIT BOTTOMCOL))))3. MOVE: (X1 (CANSTACK1 (ON (CANSTACK1 (ON1 GHOME))))) (X3 (CANSTACK (ON (SUIT1 (SUIT BOTTOMCOL))))) (X1 (ON1 BOTTOMCOL)) (X1 (CANSTACK1 (ON GHOME))) (X3 (ON1 (CANSTACK1(ON1 (NOT (CANSTACK (VALUE1 CELLSPACE))))))) (X1 (NOT (CANSTACK1 (SUIT1 (SUIT INCELL)))))(X3 (CANSTACK BOTTOMCOL)) (X1 (SUIT1 (SUIT (ON1 (NOT (CANSTACK (VALUE1 CELLSPACE)))))))(X1 (VALUE1 (NOT COLSPACE))) (0 (ON1 (NOT (CANSTACK1 (SUIT1 (SUIT INCELL)))))) (X1 (NOT(CANSTACK1 CHOME)))4. SENDTOHOME-B: (X4 (NOT GHOME))5. SENDTOHOME: (X1 (ON1 (CANSTACK (CANSTACK1 (SUIT1 (SUIT INCELL)))))) (X5 (NOT GHOME))6. SENDTOHOME: (X1 (ON1 (ON1 GHOME))) (X1 (CANSTACK1 (NOT GHOME))) (X1 (CANSTACK1 (NOT(ON1 GHOME)))) (X5 (NOT GHOME))7. MOVE-B: (X1 (NOT (CANSTACK1 GHOME))) (X2 (VALUE1 (NOT COLSPACE))) (X1 (CANSTACK1(SUIT1 (SUIT BOTTOMCOL))))8. SENDTOFREE: (X1 (ON1 (ON1 GHOME))) (X1 (NOT GHOME))9. SENDTOHOME: (X5 (CANSTACK1 (CANSTACK (ON GHOME)))) (X5 (NOT GHOME))10. SENDTOHOME: (0 GHOME) (X5 (VALUE1 (NOT COLSPACE))) (X5 (NOT (CANSTACK1 (ON1 (NOTGHOME))))) (X1 (ON1 (NOT (ON1 GHOME)))) (X5 (NOT GHOME))11. NEWCOLFROMFREECELL: (X1 GHOME)12. SENDTOHOME: (X5 (CANSTACK1 (ON GHOME))) (X1 GHOME) (X5 (NOT GHOME))13. MOVE-B: (X1 (VALUE1 (VALUE HOME))) (X2 (VALUE1 (NOT COLSPACE))) (X1 (CANSTACK1 (SUIT1(SUIT BOTTOMCOL))))14. SENDTOHOME: (X1 (CANSTACK1 (ON1 (CANSTACK1 (SUIT1 (SUIT INCELL)))))) (X5 (NOT GHOME))15. SENDTOHOME: (X1 (ON1 (ON1 (CANSTACK1 (ON1 (NOT GHOME)))))) (X5 (NOT GHOME))16. SENDTOFREE: (X1 (CANSTACK1 (ON (ON1 GHOME)))) (X1 (SUIT1 (SUIT BOTTOMCOL))) (X1 (ON1BOTTOMCOL))17. MOVE: (X3 (ON1 (CANSTACK1 CLEAR))) (X1 (ON1 (CANSTACK (ON1 (NOT (CANSTACK (VALUE1CELLSPACE))))))) (X3 (NOT GHOME)) (X1 GHOME) (X3 (CANSTACK BOTTOMCOL)) (X3 (ON1(CANSTACK1 (ON1 (NOT (CANSTACK (VALUE1 CELLSPACE))))))) (X1 (NOT (CANSTACK1 (SUIT1(SUIT INCELL))))) (X1 (ON1 BOTTOMCOL)) (X1 (SUIT1 (SUIT (ON1 (NOT (CANSTACK (VALUE1CELLSPACE))))))) (X1 (VALUE1 (NOT COLSPACE))) (X1 (ON1 (NOT (CANSTACK1 (SUIT1 (SUITINCELL)))))) (X1 (NOT (CANSTACK1 CHOME)))18. MOVE: (X1 (SUIT1 (SUIT CHOME))) (X3 (NOT GHOME)) (X3 (NOT (ON1 GHOME))) (X1 (ON1(CANSTACK1 BOTTOMCOL)))19. SENDTOHOME: (X1 (CANSTACK (ON (CANSTACK (ON GHOME))))) (X1 GHOME) (X5 (NOT GHOME))20. SENDTOHOME: (X1 (CANSTACK1 (ON (CANSTACK1 (ON1 GHOME))))) (X1 (NOT (SUIT1 (SUIT BOTTOMCOL)))) (X5 (NOT GHOME))21. SENDTOFREE: (X1 (CANSTACK (ON (CANSTACK (VALUE1 CELLSPACE))))) (X1 (CANSTACK CHOME))22. SENDTOHOME: (X1 (CANSTACK1 (SUIT1 (SUIT INCELL)))) (X1 (ON1 (NOT (CANSTACK (VALUE1CELLSPACE))))) (X5 (NOT GHOME))23. SENDTONEWCOL: (X1 (CANSTACK (CANSTACK1 (ON1 GHOME))))24. SENDTOFREE: (X1 (CANSTACK (ON1 (CANSTACK1 (ON1 GHOME))))) (X1 (NOT (CANSTACK GHOME)))(X1 (NOT (ON1 GHOME))) (X1 (ON1 (NOT (CANSTACK1 (SUIT1 (SUIT INCELL))))))111fiFern, Yoon, & Givan25. SENDTOFREE: (X1 (ON1 (CANSTACK (CANSTACK1 (ON1 GHOME))))) (X1 (NOT (CANSTACK BOTTOMCOL))) (X1 (NOT (CANSTACK1 (CANSTACK (ON GHOME)))))26. SENDTOFREE: (X1 (CANSTACK (ON1 (CANSTACK1 (ON1 (NOT GHOME)))))) (X1 (NOT (CANSTACKGHOME))) (X1 (CANSTACK (NOT (SUIT1 (SUIT BOTTOMCOL)))))27. SENDTOHOME: (X1 (CANSTACK1 (CANSTACK (ON1 GHOME)))) (X1 (ON1 (CANSTACK1 (ON1 (NOTGHOME))))) (X1 (NOT GHOME)) (X5 (NOT GHOME))28. SENDTOFREE: (X1 (CANSTACK (ON1 (CANSTACK1 (ON1 (NOT GHOME)))))) (X1 (CANSTACK (CANSTACK1(ON1 GHOME)))) (X1 (NOT GHOME)) (X1 (ON1 (CANSTACK1 (ON1 (NOT (CANSTACK (VALUE1CELLSPACE)))))))29. SENDTOFREE: (X1 (CANSTACK CHOME)) (X1 (SUIT1 (SUIT (CANSTACK1 (ON1 GHOME)))))30. SENDTOHOME: (X1 GHOME) (X1 (SUIT1 (SUIT BOTTOMCOL))) (X1 (CANSTACK1 (NOT (ON1GHOME)))) (X5 (NOT GHOME))31. SENDTOFREE: (X1 (CANSTACK1 (ON1 GHOME))) (X1 (CANSTACK1 (ON1 (NOT GHOME))))32. SENDTOFREE: (X1 (CANSTACK (ON1 GHOME))) (X1 (NOT GHOME)) (X1 (ON1 (CANSTACK1 (ON1(NOT GHOME)))))33. SENDTOHOME: (X1 (ON1 (CANSTACK1 BOTTOMCOL))) (X1 (CANSTACK1 (NOT GHOME))) (X5(NOT GHOME))34. SENDTOFREE: (X1 (CANSTACK (ON (CANSTACK1 (ON1 (NOT GHOME)))))) (X1 (NOT (SUIT1 (SUITBOTTOMCOL)))) (X1 (NOT GHOME))35. SENDTOHOME: (X1 (NOT (CANSTACK1 GHOME))) (X1 (NOT (SUIT1 (SUIT BOTTOMCOL)))) (X5(NOT GHOME))36. SENDTOFREE: (X1 (NOT (ON1 GHOME))) (X1 (CANSTACK (CANSTACK1 (ON1 (NOT GHOME)))))37. SENDTOFREE-B: (X1 (NOT GHOME))38. SENDTOFREE: (X1 UNIVERSAL)Logistics1. FLY-AIRPLANE: (X1 (IN (GAT1 AIRPORT))) (X1 (NOT (IN (GAT1 (AT AIRPLANE))))) (X3 (NOT (GAT(IN1 TRUCK)))) (X1 (NOT (IN (GAT1 (NOT AIRPORT)))))2. LOAD-TRUCK: (X2 (IN (NOT (GAT1 (NOT AIRPORT))))) (X1 (GAT1 (GAT (IN1 TRUCK)))) (X1 (NOT(CAT1 LOCATION)))3. DRIVE-TRUCK: (X3 (AT (AT1 (GAT (IN1 TRUCK))))) (X3 (IN-CITY1 (IN-CITY (AT AIRPLANE)))) (X1(AT1 (NOT (GAT (IN1 TRUCK)))))4. UNLOAD-TRUCK: (X1 (GAT1 (AT (IN OBJ)))) (X1 (GAT1 (AT OBJ))) (X1 (NOT (GAT1 (AT AIRPLANE)))) (X2 (AT1 (GAT (IN1 TRUCK)))) (X1 (GAT1 (AT TRUCK)))5. FLY-AIRPLANE: (X3 (GAT (IN1 AIRPLANE))) (X1 (IN (NOT (GAT1 (AT TRUCK))))) (X1 (AT1 (NOT(GAT (IN1 TRUCK)))))6. UNLOAD-AIRPLANE: (X2 (NOT (IN (GAT1 (NOT AIRPORT))))) (X1 (GAT1 (AT AIRPLANE)))7. LOAD-TRUCK: (X2 (IN (NOT (GAT1 LOCATION)))) (X1 (NOT (GAT1 (AT TRUCK)))) (X1 (GAT1LOCATION))8. UNLOAD-TRUCK: (X1 (GAT1 (AT TRUCK))) (X2 (AT1 AIRPORT)) (X2 (NOT (IN (GAT1 (NOT AIRPORT))))) (X1 (GAT1 (AT AIRPLANE)))9. FLY-AIRPLANE: (X3 (AT (AT1 (GAT (IN1 TRUCK))))) (X1 (AT1 (GAT (GAT1 LOCATION)))) (X1(NOT (AT1 (CAT OBJ))))10. DRIVE-TRUCK: (X1 (IN (GAT1 LOCATION))) (X1 (AT1 (NOT (GAT (IN1 TRUCK))))) (X1 (AT1 (NOT(AT AIRPLANE))))11. UNLOAD-TRUCK: (X2 (AT1 (GAT (GAT1 (NOT AIRPORT))))) (X1 (NOT (GAT1 AIRPORT)))12. FLY-AIRPLANE: (X3 (NOT (GAT (GAT1 LOCATION)))) (X1 (AT1 (GAT (AT1 (CAT OBJ))))) (X3 (AT(NOT (GAT1 (AT AIRPLANE))))) (X3 (AT OBJ)) (X1 (NOT (IN (GAT1 AIRPORT)))) (X3 (NOT (AT(IN OBJ))))112fiAPI Policy Language Bias13. UNLOAD-TRUCK: (X1 (GAT1 AIRPORT))14. LOAD-TRUCK: (X1 (AT1 (CAT (GAT1 (AT AIRPLANE))))) (X1 (NOT (GAT1 LOCATION)))15. LOAD-TRUCK: (X1 (GAT1 (CAT (GAT1 (AT AIRPLANE))))) (X1 (NOT (GAT1 (AT TRUCK)))) (X1(GAT1 (AT (GAT1 (AT AIRPLANE)))))16. LOAD-TRUCK: (X1 (GAT1 (NOT AIRPORT))) (X1 (NOT (GAT1 (AT TRUCK))))17. FLY-AIRPLANE: (X3 (AT (GAT1 (AT AIRPLANE)))) (X1 (AT1 (CAT OBJ)))18. FLY-AIRPLANE: (X3 (NOT (GAT (AT1 (CAT OBJ))))) (X1 (AT1 (GAT (AT1 (CAT OBJ))))) (X1 (AT1(GAT (GAT1 (AT TRUCK)))))19. LOAD-TRUCK: (X1 (GAT1 (AT AIRPLANE))) (X1 (NOT (GAT1 (AT TRUCK)))) (X1 (AT1 (CAT OBJ)))20. LOAD-AIRPLANE: (X1 (GAT1 AIRPORT)) (X1 (NOT (CAT1 LOCATION))) (X1 (GAT1 (NOT (ATAIRPLANE)))) (X2 (NOT (IN (GAT1 (NOT AIRPORT)))))21. FLY-AIRPLANE: (X3 (AT (GAT1 (AT AIRPLANE)))) (X3 (NOT (AT TRUCK)))22. LOAD-TRUCK: (X1 (AT1 (CAT (GAT1 (NOT AIRPORT))))) (X1 (GAT1 AIRPORT))23. DRIVE-TRUCK: (X3 (NOT (AT OBJ))) (X1 (NOT (AT1 (CAT OBJ)))) (X1 (AT1 (GAT (GAT1 LOCATION))))24. LOAD-TRUCK: (X1 (GAT1 (CAT (CAT1 AIRPORT)))) (X1 (NOT (CAT1 LOCATION)))25. FLY-AIRPLANE: (X3 (AT (GAT1 (AT AIRPLANE)))) (X1 (AT1 (AT OBJ)))26. DRIVE-TRUCK: (X1 (IN OBJ))27. DRIVE-TRUCK: (X1 (AT1 (GAT (GAT1 AIRPORT)))) (X3 (AT (GAT1 AIRPORT))) (X1 (AT1 (NOT(AT AIRPLANE))))28. FLY-AIRPLANE: (X3 (CAT (GAT1 (AT TRUCK)))) (X1 (AT1 (GAT (GAT1 LOCATION))))29. LOAD-TRUCK: (X1 (GAT1 (AT OBJ))) (X1 (NOT (CAT1 LOCATION)))30. DRIVE-TRUCK: (X3 (AT (GAT1 (AT AIRPLANE)))) (X1 (NOT (AT1 (CAT OBJ))))31. DRIVE-TRUCK: (X3 (AT AIRPLANE)) (X3 (AT (GAT1 (AT TRUCK))))32. UNLOAD-AIRPLANE: (X2 (NOT (AT1 (CAT OBJ)))) (X1 (GAT1 (NOT AIRPORT)))33. DRIVE-TRUCK: (X3 (AT (GAT1 (AT TRUCK))))34. LOAD-TRUCK: (X1 (AT1 (NOT AIRPORT))) (X1 (GAT1 AIRPORT))35. FLY-AIRPLANE: (X3 (AT (GAT1 LOCATION)))36. FLY-AIRPLANE: (X1 (IN OBJ)) (X3 (NOT (GAT (GAT1 LOCATION)))) (X1 (NOT (IN (GAT1 AIRPORT))))(X3 (NOT (AT (IN OBJ)))) (X1 (AT1 (GAT (AT1 (CAT OBJ))))))37. DRIVE-TRUCK: (X1 (AT1 (AT AIRPLANE)))38. LOAD-AIRPLANE: (X1 (GAT1 (NOT AIRPORT)))Blocks World1. STACK: (X2 (GON HOLDING)) (X2 (CON (MIN GON))) (X1 (GON ON-TABLE))2. PUTDOWN:3. UNSTACK: (X1 (ON (ON (MIN GON)))) (X2 (CON (ON (MIN GON))))4. UNSTACK: (X2 (ON1 (GON CLEAR))) (X2 (GON (ON (MIN GON)))) (X1 (ON (GON ON-TABLE)))(X1 (GON (NOT CLEAR)))5. PICKUP: (X1 (GON1 (CON (MIN GON)))) (X1 (GON1 CLEAR)) (X1 (GON1 (CON ON-TABLE)))6. UNSTACK: (X2 (CON (GON1 CLEAR))) (X1 (GON1 (ON (MIN GON)))) (X1 (GON1 (CONCLEAR)))113fiFern, Yoon, & Givan7. UNSTACK: (X1 (NOT (GON (MIN GON))))8. UNSTACK: (X2 (GON ON-TABLE)) (X1 (GON1 (CON (MIN GON)))) (X1 (GON1 CLEAR))9. UNSTACK: (X1 (NOT (CON (MIN GON)))) (X2 (ON (GON1 ON-TABLE))) (X2 (GON (NOT ONTABLE))) (X1 (GON (GON ON-TABLE))) (X1 (GON (NOT CLEAR)))10. UNSTACK: (X2 (NOT (CON CLEAR))) (X1 (GON1 (CON ON-TABLE)))11. UNSTACK: (X1 (GON1 CLEAR)) (X1 (ON (ON (MIN GON)))Ground Logistics1. LOAD: (X2 (NOT (IN (GIN1 CITY)))) (X1 (NOT (CIN1 CITY))) (X1 (GIN1 CITY))2. UNLOAD: (X1 (GIN1 X3 ))3. DRIVE: (X1 (IN (GIN1 X3 )))4. DRIVE: (X3 (NOT (GIN BLOCK))) (X3 (IN (GIN1 CITY))) (X1 CAR) (X2 CLEAR)5. DRIVE: (X3 (IN (GIN1 RAIN))) (X1 TRUCK)Colored Blocks World1. PICK-UP-BLOCK-FROM: (X2 (NOT (CON-TOP-OF TABLE))) (X2 (GON-TOP-OF1 (ON-TOP-OF BLOCK)))2. PUT-DOWN-BLOCK-ON: (X2 (CON-TOP-OF1 (CON-TOP-OF1 BLOCK))) (X2 (GON-TOP-OF HOLDING))(X2 (CON-TOP-OF TABLE))3. PICK-UP-BLOCK-FROM: (X2 (NOT (CON-TOP-OF BLOCK))) (X1 (ON-TOP-OF (GON-TOP-OF1 TABLE)))(X2 (GON-TOP-OF (GON-TOP-OF1 BLOCK))) (X2 (NOT (CON-TOP-OF1 BLOCK))) (X2 (ON-TOPOF1 (GON-TOP-OF BLOCK))) (X1 (GON-TOP-OF (GON-TOP-OF1 BLOCK)))4. PICK-UP-BLOCK-FROM: (X1 (NOT (CON-TOP-OF TABLE))) (X1 (GON-TOP-OF1 (CON-TOP-OF TABLE))) (X1 (GON-TOP-OF (ON-TOP-OF1 BLOCK)))5. PUT-DOWN-BLOCK-ON: (X2 (CON-TOP-OF1 (ON-TOP-OF1 TABLE))) (X2 (GON-TOP-OF HOLDING)) (X2(CON-TOP-OF TABLE))6. PUT-DOWN-BLOCK-ON: (X2 (CON-TOP-OF (ON-TOP-OF BLOCK))) (X1 (GON-TOP-OF1 (GON-TOP-OF1BLOCK)))7. PUT-DOWN-BLOCK-ON: (X2 (GON-TOP-OF HOLDING)) (X2 (CON-TOP-OF TABLE))8. PUT-DOWN-BLOCK-ON: (X2 TABLE)9. PICK-UP-BLOCK-FROM: (X2 (NOT (CON-TOP-OF TABLE))) (X2 (GON-TOP-OF1 (CON-TOP-OF TABLE)))10. PICK-UP-BLOCK-FROM: (X1 (GON-TOP-OF1 (CON-TOP-OF1 TABLE))) (X2 TABLE) (X1 (GON-TOP-OF(GON-TOP-OF BLOCK))) (X1 (GON-TOP-OF (ON-TOP-OF1 TABLE)))11. PICK-UP-BLOCK-FROM: (X2 (ON-TOP-OF (CON-TOP-OF BLOCK))) (X1 (GON-TOP-OF1 (CON-TOP-OF1TABLE)))12. PICK-UP-BLOCK-FROM: (X2 (ON-TOP-OF1 BLOCK)) (X2 (NOT (CON-TOP-OF TABLE))) (X2 (GONTOP-OF (ON-TOP-OF1 BLOCK))) (X2 (GON-TOP-OF (ON-TOP-OF1 BLOCK)))13. PICK-UP-BLOCK-FROM: (X1 (GON-TOP-OF1 (GON-TOP-OF1 TABLE)))Boxworld1. DRIVE-TRUCK: (X2 (GBOX-AT-CITY (BOX-AT-CITY1 X3 ))) (X3 (NOT (CAN-FLY (TRUCK-AT-CITY (NOTPREVIOUS))))) (X3 (CAN-DRIVE1 PREVIOUS)) (X2 (NOT (CAN-FLY (TRUCK-AT-CITY (NOT PREVIOUS))))) (X3 (NOT (CAN-FLY (BOX-AT-CITY BOX)))) (X2 (CAN-DRIVE (CAN-DRIVE (BOX-AT-CITY BOX))))(X3 (NOT (CAN-FLY (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))))2. UNLOAD-BOX-FROM-TRUCK-IN-CITY: (X1 (GBOX-AT-CITY1 (TRUCK-AT-CITY PREVIOUS))) (X3 (GBOXAT-CITY BOX)) (X3 (NOT (BOX-AT-CITY PREVIOUS))) (X1 (GBOX-AT-CITY1 (CAN-DRIVE1 (CANDRIVE1 (CAN-FLY CITY))))) (X2 (BOX-ON-TRUCK (GBOX-AT-CITY1 PREVIOUS)))3. DRIVE-TRUCK: (X1 (BOX-ON-TRUCK (GBOX-AT-CITY1 X3 ))) (X2 (NOT (CAN-DRIVE (TRUCK-AT-CITY(BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))))114fiAPI Policy Language Bias4. DRIVE-TRUCK: (X3 (CAN-DRIVE (BOX-AT-CITY PREVIOUS))) (X2 (CAN-FLY (CAN-DRIVE1 (BOX-AT-CITYBOX)))) (X3 (CAN-DRIVE (CAN-FLY (TRUCK-AT-CITY TRUCK)))) (X2 (NOT (CAN-DRIVE (TRUCK-AT-CITY(BOX-ON-TRUCK (GBOX-AT-CITY1 CITY)))))) (X2 PREVIOUS) (X2 (CAN-DRIVE (CAN-DRIVE X3 ))) (X3(NOT (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))) (X3 (NOT (CAN-FLY PREVIOUS)))(X3 (CAN-DRIVE (NOT (BOX-AT-CITY BOX)))) (X2 (CAN-DRIVE (CAN-DRIVE1 X3 ))) (X3 (CAN-DRIVE(NOT (TRUCK-AT-CITY TRUCK))))5. LOAD-BOX-ON-TRUCK-IN-CITY: (X1 (GBOX-AT-CITY1 (CAN-DRIVE (TRUCK-AT-CITY TRUCK)))) (X3 (NOT(PLANE-AT-CITY PREVIOUS))) (X3 (CAN-DRIVE (CAN-DRIVE1 (CAN-FLY CITY)))) (X3 (CAN-DRIVE1(NOT (TRUCK-AT-CITY (NOT PREVIOUS)))))6. UNLOAD-BOX-FROM-TRUCK-IN-CITY: (X3 (GBOX-AT-CITY (BOX-ON-TRUCK1 TRUCK))) (X3 (NOT (CANFLY (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY)))))) (X1 (GBOX-AT-CITY1 CITY))7. DRIVE-TRUCK: (X1 (BOX-ON-TRUCK (GBOX-AT-CITY1 PREVIOUS))) (X3 (CAN-DRIVE (GBOX-AT-CITY(GBOX-AT-CITY1 PREVIOUS)))) (X3 (NOT (PLANE-AT-CITY PLANE))) (X2 (NOT (CAN-FLY (GBOX-ATCITY (GBOX-AT-CITY1 PREVIOUS)))))8. FLY-PLANE: (X1 (BOX-ON-PLANE (GBOX-AT-CITY1 X3 )))9. UNLOAD-BOX-FROM-PLANE-IN-CITY: (X1 (GBOX-AT-CITY1 PREVIOUS))10. FLY-PLANE: (X2 (NOT (CAN-DRIVE (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY)))))) (X2(GBOX-AT-CITY BOX)) (X3 (NOT (PLANE-AT-CITY PREVIOUS))) (X1 (NOT PREVIOUS))11. LOAD-BOX-ON-PLANE-IN-CITY: (X1 (GBOX-AT-CITY1 (CAN-FLY PREVIOUS))) (X3 (NOT (TRUCK-AT-CITY(NOT PREVIOUS)))) (X3 (NOT (CAN-DRIVE (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))))12. DRIVE-TRUCK: (X1 (BOX-ON-TRUCK (GBOX-AT-CITY1 X3 ))) (X2 (NOT (CAN-DRIVE (CAN-FLY PREVIOUS)))) (X2 (CAN-DRIVE1 (CAN-FLY CITY)))13. LOAD-BOX-ON-TRUCK-IN-CITY: (X1 (GBOX-AT-CITY1 PREVIOUS))ReferencesAler, R., Borrajo, D., & Isasi, P. (2002). Using genetic programming learn improvecontrol knowledge. Artificial Intelligence, 141 (1-2), 2956.Ambite, J. L., Knoblock, C. A., & Minton, S. (2000). Learning plan rewriting rules.Artificial Intelligence Planning Systems, pp. 312.Bacchus, F. (2001). AIPS 00 planning competition. AI Magazine, 22(3)(3), 5762.Bacchus, F., & Kabanza, F. (2000). Using temporal logics express search control knowledge planning. Artificial Intelligence, 16, 123191.Bagnell, J., Kakade, S., Ng, A., & Schneider, J. (2003). Policy search dynamic programming. Proceedings 16th Conference Advances Neural InformationProcessing.Bellman, R. (1957). Dynamic Programming. Princeton University Press.Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.Boutilier, C., & Dearden, R. (1996). Approximating value trees structured dynamicprogramming. Saitta, L. (Ed.), International Conference Machine Learning.Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programmingfactored representations. Artificial Intelligence, 121 (1-2), 49107.Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-orderMDPs. International Joint Conference Artificial Intelligence.115fiFern, Yoon, & GivanDean, T., & Givan, R. (1997). Model minimization markov decision processes. NationalConference Artificial Intelligence, pp. 106111.Dean, T., Givan, R., & Leach, S. (1997). Model reduction techniques computing approximately optimal solutions Markov decision processes. Conference UncertaintyArtificial Intelligence, pp. 124131.Dietterich, T., & Wang, X. (2001). Batch value function approximation via support vectors.Proceedings Conference Advances Neural Information Processing.Dzeroski, S., DeRaedt, L., & Driessens, K. (2001). Relational reinforcement learning. Machine Learning, 43, 752.Estlin, T. A., & Mooney, R. J. (1996). Multi-strategy learning search control partialorder planning. National Conference Artificial Intelligence.Fern, A., Yoon, S., & Givan, R. (2003). Approximate policy iteration policy language bias. Proceedings 16th Conference Advances Neural InformationProcessing.Finkelstein, L., & Markovitch, S. (1998). selective macro-learning algorithmapplication NxN sliding-tile puzzle. Journal Artificial Intelligence Research,8, 223263.Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions model minimizationMarkov decision processes. Artificial Intelligence, 147 (1-2), 163223.Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression inductive policyselection. Conference Uncertainty Artificial Intelligence.Guestrin, C., Koller, D., Gearhart, C., & Kanodia, N. (2003a). Generalizing plans newenvironments relational mdps. International Joint Conference Artificial Intelligence.Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003b). Efficient solution algorithmsfactored mdps. Journal Artificial Intelligence Research, 19, 399468.Hoffman, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. JournalArtificial Intelligence Research, 22, 215278.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 263302.Howard, R. (1960). Dynamic Programming Markov Decision Processes. MIT Press.Huang, Y.-C., Selman, B., & Kautz, H. (2000). Learning declarative control rulesconstraint-based planning. International Conference Machine Learning, pp.415422.Kearns, M. J., Mansour, Y., & Ng, A. Y. (2002). sparse sampling algorithm nearoptimal planning large markov decision processes. Machine Learning, 49 (23),193208.Kersting, K., Van Otterlo, M., & DeRaedt, L. (2004). Bellman goes relational. ProceedingsTwenty-First International Conference Machine Learning.116fiAPI Policy Language BiasKhardon, R. (1999a). Learning action strategies planning domains. Artificial Intelligence, 113 (1-2), 125148.Khardon, R. (1999b). Learning take actions. Machine Learning, 35 (1), 5790.Lagoudakis, M., & Parr, R. (2003). Reinforcement learning classification: Leveragingmodern classifiers. International Conference Machine Learning.Langford, J., & Zadrozny, B. (2004). Reducing t-step reinforcement learning classification.http://hunch.net/jl/projects/reductions/RL class/colt submission.ps.Martin, M., & Geffner, H. (2000). Learning generalized policies planning domains usingconcept languages. International Conference Principles Knowledge Representation Reasoning.Mataric, M. (1994). Reward functions accelarated learning. Proceedings International Conference Machine Learning.McAllester, D., & Givan, R. (1993). Taxonomic syntax first order inference. JournalACM, 40 (2), 246283.McAllester, D. (1991). Observations cognitive judgements. National ConferenceArtificial Intelligence.McGovern, A., Moss, E., & Barto, A. (2002). Building basic block instruction schedulerusing reinforcement learning rollouts. Machine Learning, 49 (2/3), 141160.Minton, S. (1988). Quantitative results concerning utility explanation-based learning.National Conference Artificial Intelligence.Minton, S. (Ed.). (1993). Machine Learning Methods Planning. Morgan Kaufmann.Minton, S., Carbonell, J., Knoblock, C. A., Kuokka, D. R., Etzioni, O., & Gil, Y. (1989).Explanation-based learning: problem solving perspective. Artificial Intelligence, 40,63118.Natarajan, B. K. (1989). learning exercises. Annual Workshop ComputationalLearning Theory.Reddy, C., & Tadepalli, P. (1997). Learning goal-decomposition rules using exercises.International Conference Machine Learning, pp. 278286. Morgan Kaufmann.Rivest, R. (1987). Learning decision lists. Machine Learning, 2 (3), 229246.Tesauro, G. (1992). Practical issues temporal difference learning. Machine Learning, 8,257277.Tesauro, G., & Galperin, G. (1996). On-line policy improvement using monte-carlo search.Conference Advances Neural Information Processing.Tsitsiklis, J., & Van Roy, B. (1996). Feature-based methods large scale DP. MachineLearning, 22, 5994.Veloso, M., Carbonell, J., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integratingplanning learning: PRODIGY architecture. Journal ExperimentalTheoretical AI, 7 (1).Wu, G., Chong, E., & Givan, R. (2001). Congestion control via online sampling. Infocom.117fiFern, Yoon, & GivanYan, X., Diaconis, P., Rusmevichientong, P., & Van Roy, B. (2004). Solitaire: Man versusmachine. Conference Advances Neural Information Processing.Yoon, S., Fern, A., & Givan, R. (2002). Inductive policy selection first-order MDPs.Conference Uncertainty Artificial Intelligence.Younes, H. (2003). Extending pddl model stochastic decision processes. ProceedingsInternational Conference Automated Planning Scheduling WorkshopPDDL.Zimmerman, T., & Kambhampati, S. (2003). Learning-assisted automated planning: Looking back, taking stock, going forward. AI Magazine, 24(2)(2), 7396.118fiJournal Artificial Intelligence Research 25 (2006) 187-231Submitted 03/05; published 02/06Approach Temporal Planning SchedulingDomains Predictable Exogenous EventsAlfonso GereviniAlessandro SaettiIvan Serinagerevini@ing.unibs.itsaetti@ing.unibs.itserina@ing.unibs.itDipartimento di Elettronica per lAutomazioneUniversita degli Studi di BresciaVia Branze 38, I-25123 Brescia, ItalyAbstracttreatment exogenous events planning practically important many realworld domains preconditions certain plan actions affected events.paper focus planning temporal domains exogenous events happenknown times, imposing constraint certain actions plan must executedpredefined time windows. actions durations, handling temporal constraints adds extra difficulty planning. propose approach planningdomains integrates constraint-based temporal reasoning graph-basedplanning framework using local search. techniques implemented plannertook part 4th International Planning Competition (IPC-4). statistical analysisresults IPC-4 demonstrates effectiveness approach termsCPU-time plan quality. Additional experiments show good performancetemporal reasoning techniques integrated planner.1. Introductionmany real-world planning domains, execution certain actions occurpredefined time windows one necessary conditions hold. instance,car refueled gas station gas station open, space telescopetake picture certain planet region region observable. truthconditions determined exogenous events happen known times,cannot influenced actions available planning agent (e.g.,closing gas station planet movement).Several frameworks supporting action durations time windows proposed(e.g., Vere, 1983; Muscettola, 1994; Laborie & Ghallab, 1995; Schwartz & Pollack, 2004;Kavuluri & U, 2004; Sanchez, Tang, & Mali, 2004). However, domaindependent systems fast enough large-scale problems. paper, proposenew approach planning temporal features, integrating constraint-basedtemporal reasoning graph-based planning framework.last two versions domain definition language International planning competition (IPC) support action durations predictable (deterministic) exogenousevents (Fox & Long, 2003; Edelkamp & Hoffmann, 2004). PDDL2.1, predictable exogenous events implicitly represented (Fox, Long, & Halsey, 2004), PDDL2.2explicitly represented timed initial literals, one two new PDDLc2006AI Access Foundation. rights reserved.fiGerevini, Saetti & Serinafeatures 2004 competition (IPC-4) focused. Timed initial literals specifieddescription initial state planning problem assertions form(at L), real number, L ground literal whose predicateappear effects domain action. obvious meaning (at L) Ltrue time t. set assertions involving ground predicate definessequence disjoint time windows timed predicate holds. examplewell-known ZenoTravel domain (Penberthy, 1993; Long & Fox, 2003a)(at(at(at(at8 (open-fuelstation city1))12 (not (open-fuelstation city1)))15 (open-fuelstation city1))20 (not (open-fuelstation city1))).assertions define two time windows (open-fuelstation city1) true,i.e., 8 12 (excluded) 15 20 (excluded). timed initial literal relevantplanning process precondition domain action, call timedprecondition action. timed precondition action seen temporalscheduling constraint action, defining feasible time window(s) actionexecuted. actions plan durations timed preconditions, computingvalid plan requires planning reasoning time integrated, order checkwhether execution planned actions satisfy scheduling constraints.action plan cannot scheduled, plan valid must revised.main contributions work are: (i) new representation temporal plansaction durations timed preconditions, called Temporally-Disjunctive Action Graph,(TDA-graph) integrating disjunctive constraint-based temporal reasoning recentgraph-based approach planning; (ii) polynomial method solving disjunctive temporal reasoning problems arise context; (iii) new local search techniquesguide planning process using representation; (iv) experimental analysisevaluating performance methods implemented planner called lpg-td,took part IPC-4 showing good performance many benchmark problems.td extension name planner abbreviation timed initial literalsderived predicates, two main new features PDDL2.2.1 lpg-td, techniqueshandling timed initial literals quite different techniques handling derivedpredicates. first ones concern representing temporal plans predictable exogenousevents fast temporal reasoning action scheduling planning; second onesconcern incorporating rule-based inference system efficient reasoning derivedpredicates planning. timed initial literals derived predicates requirechange heuristics guiding search planner, radically different way.paper, focus timed initial literals, significant usefulextension PDDL2.1. Moreover, analysis results IPC-4 shows lpg-tdtop performer benchmark problems involving feature. treatment derivedpredicates lpg-td presented another recent paper (Gerevini et al., 2005b).1. Derived predicates allow us express concise natural way indirect action effects. Informally, predicates appear effect action, truth determineddomain rules specified part domain description.188fiAn Approach Temporal Planning Schedulingpaper organized follows. Section 2, necessary background,introduce TDA-graph representation method solving disjunctive temporalreasoning problems arise context. Section 3, describe new localsearch heuristics planning space TDA-graphs. Section 4, presentexperimental analysis illustrating efficiency approach. Section 5, discussrelated work. Finally, Section 6 give conclusions.2. Temporally Disjunctive Action GraphLike partial-order causal-link planning, (e.g., Penberthy & Weld, 1992; McAllester &Rosenblitt, 1991; Nguyen & Kambhampati, 2001), framework search spacepartial plans. search state partial temporal plan representTemporally-Disjunctive Action Graph (TDA-graph). TDA-graph extensionlinear action graph representation (Gerevini, Saetti, & Serina, 2003) integrates disjunctive temporal constraints handling timed initial literals. linear action graphvariant well-known planning graph (Blum & Furst, 1997). section,necessary background linear action graphs disjunctive temporal constraints,introduce TDA-graphs, propose techniques temporal reasoningcontext representation used next section.2.1 Background: Linear Action Graph Disjunctive Temporal Constraintslinear action graph (LA-graph) planning problem directed acyclic leveledgraph alternating fact level, action level. Fact levels contain fact nodes,labeled ground predicate . fact node f level l associatedno-op action node level l representing dummy action predicate fprecondition effect. action level contains one action node labeledname domain action represents, no-op nodes correspondinglevel.action node labeled level l connected incoming edges fact nodeslevel l representing preconditions (precondition nodes), outgoing edgesfact nodes level l + 1 representing effects (effect nodes). initial levelcontains special action node astart , last level special action node aend .effect nodes astart represent positive facts initial state , preconditionnodes aend goals .pair action nodes (possibly no-op nodes) constrained persistent mutexrelation (Fox & Long, 2003), i.e., mutually exclusive relation holding every levelgraph, imposing involved actions never occur parallel valid plan.relations efficiently precomputed using algorithm proposed previouswork (Gerevini et al., 2003).LA-graph also contains set ordering constraints actions (partial) plan represented graph. constraints (i) constraints imposedsearch deal mutually exclusive actions: action level l mutexaction node b level l, constrained finish start b; (ii)constraints actions implied causal structure plan: action189fiGerevini, Saetti & Serinaused achieve precondition action b, constrained finish startb.effects action node automatically propagated next levelsgraph corresponding no-ops, interfering (mutex) actionblocking propagation, last level graph reached (Gerevini et al.,2003). rest paper, assume LA-graph incorporates propagation.Disjunctive Temporal Problem (DTP) (Stergiou & Koubarakis, 2000; Tsamardinos& Pollack, 2003) pair hP, Ci, P set time point variables, C setdisjunctive constraints c1 cn , ci form yi xi ki , xi yi P, kireal number (i = 1...n). C contains unary constraints, DTP called SimpleTemporal Problem (STP) (Dechter, Meiri, & Pearl, 1991).DTP consistent DTP solution. solution DTPassignment real values variables DTP consistent every constraintDTP. Computing solution DTP NP-hard problem (Dechter et al., 1991),computing solution STP accomplished polynomial time. GivenSTP special start time variable preceding others, computesolution STP variable shortest possible distance O(n c)time, n variables c constraints STP (Dechter et al., 1991; Gerevini & Cristani,1997). call solution optimal solution STP. Clearly, DTP consistentchoose constraint DTP disjunct obtaining consistentSTP, solution STP also solution original DTP.Finally, STP consistent distance graph STPcontain negative cycles (Dechter et al., 1991). distance graph STP hP, Cidirected labeled graph vertex labeled p p P, edge v Pw P labeled k constraint w v k C.2.2 Augmenting LA-graph Disjunctive Temporal ConstraintsLet p timed precondition set W (p) time windows. following, x x+indicate start time end time x, respectively, x either time windowaction. Moreover, al indicates action node level l LA-graph consideration.clarity presentation, describe techniques focusing action preconditionsmust hold whole execution action (except end pointaction), operator effects hold end action execution, i.e., PDDLconditions type all, PDDL effects type end (Fox & Long, 2003). 2order represent plans actions durations time windowsexecution, augment ordering constraints LA-graph (i) action durationconstraints (ii) action scheduling constraints. Duration constraints forma+ = Dur(a),Dur(a) denotes duration action (for special actions start aend ,++Dur(astart ) = Dur(aend ) = 0, sincestart = astart aend = aend ). Durationconstraints supported representation proposed previous work (Gerevini2. methods planner support types operator condition effect specifiedPDDL 2.1 2.2.190fiAn Approach Temporal Planning SchedulingLevel 1(0)p1Level 2Level 3Goal level()p1p1p1p1pmutexp2(0)(50)p5(50)p5(50)p5a3a1(0)astart(0)p3p3p3(0)p7p3mutex()astartp6(90)(70)a1aend(70)p8(70)p8pp10[15](75)[50](0)p(90)a2a3(70)p8aenda2(0)p4(0)p4(0)p4[70](0)(70)p9(70)p9(70)p9(70)025507590125p9Figure 1: example LA-graph nodes labeled -values (in round brackets),Gantt chart actions labeling nodes LA-graph. Squarenodes action nodes; circle nodes fact nodes. Action nodes also markedduration represented actions (in square brackets). Unsupportedprecondition nodes labeled (). Dashed edges form chains no-ops blockedmutex actions. Grey areas Gantt chart represent time windowstimed precondition p a3 .et al., 2003), representation treatment scheduling constraints majorcontribution work.Let plan represented LA-graph A. easy see set C formedordering constraints duration constraints actionsencoded STP. instance, ai used support precondition node aj ,a+aj 0 C; ai aj two mutex actions , ai ordered aj ,a+aj 0 C. Moreover, every action , following STP-constraintsC:a+ Dur(a), a+ Dur(a),equivalent a+ = Dur(a). scheduling constraint imposes constraintexecution action must occur time windows associated timedprecondition action. Syntactically, disjunctive constraint c1 cn ,ci form(yi xhi ) (vi ui ki ),u, vi , xi , yi action start times action end times, hi , ki R . every actiontimed precondition p, following disjunctive constraint added C:191fiGerevini, Saetti & Serina_wW (p)+3+a+a+.start wstart wDefinition 1 temporally disjunctive action graph (TDA-graph) 4-tuple hA, , P, Cilinear action graph;assignment real values nodes A;P set time point variables corresponding start times end timesactions labeling action nodes A;C set ordering constraints, duration constraints scheduling constraintsinvolving variables P.TDA-graph hA, , P, Ci represents (partial) plan formed actions labelingaction nodes start times assigned . Figure 1 gives LA-graph-values simple TDA-graph containing five action nodes (astart , a1 , a2 , a3 , aend )several fact nodes representing ten facts. ordering constraints duration constraintsC are:4+a+1 a3 0, a2 a3 0,+++a1 a1 = 50, a22 = 70, a3 a3 = 15.Assuming p timed precondition a3 windows [25, 50) [75, 125),scheduling constraint C is:+++++((a+start a3 25) (a3 astart 50)) ((astart a3 75) (a3 astart 125)).pair hP, Ci defines DTP D.5 Let Ds set scheduling constraints D.represents set STPs, consists constraintsDs one disjunct (pair STP-constraints) disjunction subset s0Ds (Ds0 Ds ). call consistent STP induced STP D. inducedSTP contains disjunct every disjunction Ds (i.e., Ds0 = Ds ), say(consistent) STP complete induced STP D.values assigned action nodes action start times corresponding optimal solution induced STP. call start times scheduleactions A. value labeling fact node f earliest time = Ta + Dur(a)3. Note that, p timed condition action a, end timeexogenous event making p false happens, PDDL p required true end(Fox & Long, 2003).+4. brevity, examples omit constraints a+start ai 0 ai aend 0, actionai , well duration constraints astart aend , duration zero.5. disjunctive constraints C exactly DTP-form. However, easy see everydisjunctive constraint C translated equivalent conjunction constraints exact DTPform. use compact notation clarity efficiency reasons.192fiAn Approach Temporal Planning Schedulingsupports f A, starts Ta . induced STP deriveschedule incomplete, may violate scheduling constraint action nodes,say unscheduled current TDA-graph.following definitions present notions optimality complete induced STPoptimal schedule, used next section.Definition 2 Given DTP point variable p, complete induced STPoptimal induced STP p iff solution assigning p value lessequal value assigned p every solution every complete inducedSTP D.Definition 3 Given DTP TDA-graph G, optimal schedule actionsG optimal solution optimal induced STPend .Note optimal solution minimizes makespan represented (possiblypartial) plan. DTP previous example (Figure 1) two induced STPs: onetime window p (S1 ), one including pair STP-constraints imposingtime window [75, 125) p (S2 ). STP obtained imposing time window [25, 50)p induced STP DTP, consistent. S1 partial inducedSTP D, S2 complete optimal start time aend . temporal valuesderived optimal solution S2 assigned action nodes++TDA-graph are:start = astart = 0, a1 = 0, a2 = 0, a3 = 75, aend = aend = 90.2.3 Solving DTP TDA-graphgeneral, computing complete induced STP DTP (if exists) NP-hard problemsolved backtracking algorithm (Stergiou & Koubarakis, 2000; Tsamardinos& Pollack, 2003). However, given particular structure temporal constraintsforming TDA-graph, show task accomplished polynomial timebacktrack-free algorithm. Moreover, algorithm computes optimal induced STPend .following, assume time window timed precondition shorterduration action (otherwise, time window removedavailable precondition and, time window remains, action cannotused valid plan). Moreover, without loss generality, assumeaction one timed precondition. easy see always replaceset timed conditions action single equivalent timed precondition,whose time windows obtained intersecting windows forming different originaltimed conditions a. Also set start timed conditions set end timedconditions compiled single equivalent timed preconditions. achievedtranslating conditions conditions type all. idea similarone presented Edelkamp (2004), difference onetime window associated timed condition, Edelkamp assumes timedcondition associated unique time window. Specifically, every start timedcondition p action translated equivalent timed condition p 0 typereplacing scheduling constraint p,193fiGerevini, Saetti & SerinappDur(a)Dur(a)qDur(a)rrx035 40506080100120150180Figure 2: example set timed conditions compiled single timed precondition (x). solid boxes represent time windows associated timedconditions p (of type start), q (of type end), r (of type all)action a. solid box extended dashed box indicates extensiontime window translation corresponding timed conditiontimed condition a._wW (p)+a+a+,start < wstart < wforcing occur one time windows,_wW (p)6++a+a+start < w + Dur(a) .start < wSimilarly, every end timed condition p translated equivalenttimed condition replacing scheduling constraint_wW (p)+++a+a+,start < wstart < wforcing a+ occur one time windows,_wW (p)+++a+.start < w + Dur(a) astart < wClearly, translation timed conditions domain action single timedprecondition action accomplished preprocessing step polynomial time.Figure 2 shows example. Assume action duration 20 timed conditionsp type start, q type end r type all. Let [0, 50) [100, 150)time windows p, [35, 80) time window q, finally [40, 60) [120, 180)time windows r. compile timed conditions new timed condition xtime window [40, 60).6. Note timed conditions type start end need use < instead . However,properties algorithms STPs easily generalized STPs extended <-constraints(e.g., Gerevini & Cristani, 1997).194fiAn Approach Temporal Planning SchedulingSolve-DTP(X, S)Input: set X meta-variables meta CSP DTP, partial solution meta CSP;Output: Either solution meta CSP fail.1.2.3.4.5.6.7.8.9.10.X = stop return S;x SelectVariable(X); X 0 X {x};D(x) 6=SelectValue(D(x));0 {x d}; D(x) D(x) {d};D0 (x) D(x); /* Saving domain values */ForwardCheck-DTP(X 0 , 0 )Solve-DTP(X 0 , 0 );D(x) 0 (x); /* Restoring domain values */return fail; /* backtracking */ForwardCheck-DTP(X, S)Input: set X meta-variables, (partial) solution S;Output: Either true false.1.2.3.4.5.6.forall x Xforall D(x)Consistency-STP(S {x d})D(x) D(x) {d};D(x) = return false; /* dead-end */return true.Figure 3: Basic algorithm solving DTP. D(x) global variable whose valuecurrent domain meta-variable x. Consistency-STP(S) returns true,STP formed variable values (partial) solution solution, falseotherwise.observed Stergiou Kourbarakis (2000) Tsamardinos Pollack (2003),DTP seen meta CSP: variables meta CSP constraintsoriginal CSP, values (meta) variables disjuncts formingconstraints original CSP. constraints meta CSP explicitlystated. Instead, implicitly defined follows: assignment valuesmeta-variables satisfies constraints meta CSP iff forms consistent STP (aninduced STP DTP). solution meta CSP complete induced STPDTP.Figure 3 shows algorithm solving meta CSP DTP (Tsamardinos &Pollack, 2003), variant forward-checking backtracking algorithm solvinggeneral CSPs. appropriately choosing next meta-variable instantiate (functionSelectVariable) value (function SelectValue), show algorithm findssolution backtracking (if one exists). Moreover, simple modification Solve195fiGerevini, Saetti & SerinaDTP, derive algorithm backtrack free even input meta CSPsolution. achieved exploiting information LA-graphTDA-graph decompose DTP sequence growing DTPsD1 D2 ... Dlast =(i) last number levels A, (ii) variables Vi Di (i = 1..last)variables corresponding action nodes level i, (iii)constraints Di constraints involving variables Vi . E.g.,+++DTP Figure 1, point variables D3 a+start , a1 , a1 , a2 , a2 , a3 , a3 , setconstraints D3++++{ a+1 a3 0, a2 a3 0, a1 a1 = 50, a2 a2 = 70, a3 a3 = 15,+++++((a+start a3 25) (a3 astart 50)) ((astart a3 75) (a3 astart 125))}.decomposed DTP, derive ordered partition set metavariables meta CSP original DTPX = X1 X2 ... Xlast ,Xi set meta-variables corresponding constraints Di Di1 ,> 1, D1 otherwise. ordered partition used define orderSelectVariable chooses next variable instantiate, crucial avoid backtracking. Specifically, every variable single domain value (i.e., ordering constraint,duration constraint, scheduling constraint one time window) selectedevery variable one possible value (i.e., scheduling constraintone time window); moreover, xi Xi , xj Xj < j, xi selectedxj .order avoid backtracking, order SelectValue chooses valuemeta-variable important well: given meta-variable one value(time window) current domain, choose value corresponding earliestavailable time window. E.g., current domain selected meta-variablepossible values[i=1..m+++a+,start wi astart wiSelectValue chooses j-th value |wj | < |wh |, every h {1, ..., m},j {1, ..., m}, h 6= j.following give simple example illustrating order SelectVariableSelectValue select meta-variables meta-values, respectively. ConsiderTDA-graph Figure 1 additional time window [150, 200) timed precondition p a3 . DTP extended TDA-graph six meta-variables (x1 , x2 , . . . , x6 ),whose domains (the disjuncts corresponding constraints original CSP) are:x1 : {a+1 a3 0}+x2 : {a2 a3 0}196fiAn Approach Temporal Planning Schedulingx3 :x4 :x5 :x6 :{a+1 a1 = 50}+{a2 a2 = 70}{a+3 a3 = 15}++++++{(astart3 25) (a3 astart 50), (astart a3 75) (a3 astart 125),+++(astart a3 150) (a3 astart 200)}.exploiting level structure TDA-graph, derive ordered partitionmeta-variables formed following sets:X1 = {x3 }, X2 = {x4 }, X3 = {x1 , x2 , x5 , x6 }.Since x3 belongs X1 x4 belongs X2 , SelectVariable selects x3 selectingx4 . Similarly, function selects x4 meta-variables X3 . algorithminstantiates x6 , first meta-value x6 (i.e., first time window timed precondition a3 ) removed domain forward checking, SelectValue selects+++++(a+start a3 75) (a3 astart 125) (astart a3 150) (a3 astart 200),first meta-value corresponds time window starting time 75,second one corresponds time window starting time 150.using techniques selecting next meta-variable instantiatevalue, prove following theorem.Theorem 1 Given DTP TDA-graph, meta CSP X solvable,Solve-DTP finds solution X backtracking. Moreover, solution optimalinduced STPend .Proof. proof two key points: way meta-variables selected instantiatedSelectVariable SelectValue, respectively; particular type constraints D,disjunctive constraints specific form encoding set disjoint time windows,and, construction D,j < j |=j < ai ,(1)set ordering constraints duration constraints D,(aj )endpoint ai (aj ). property (1), cannot imply restrictionmaximum distance endpoint ai endpoint aj (while, course,lower bound distance). I.e., positive quantity uj < j |= (aj ai u).(2)Let assume SelectVariable chooses meta-variable x cannot consistentlyinstantiated value D(x) (and means reached backtracking point).show cannot case.SelectVariable chooses meta-variables STP-constraints metavariable scheduling constraint one value (time window). Let Xset meta-variables associated scheduling constraints D.x must meta-variable X , assuming meta CSP Xsolvable. use forward checking subroutine guarantees least one value xconsistent respect meta-variables instantiated current partial197fiGerevini, Saetti & Serinasolution S. Hence, case step 7 Solve-DTP ForwardCheck-DTPreturns false every value (time window) D(x), i.e., every D(x)exists another uninstantiated meta-variable x0 X that, every d0 D(x0 ),check Consistency-STP(S {x0 d0 }) executed forward checking subroutine returnsfalse. However, X solution (D consistent), cannot case(i) value chosen SelectValue instantiate x previously instantiated metavariables (step 4) earliest available time window current domainmeta-variable consideration, least commitment assignment,(ii) one scheduling constraint (meta-variable X ) levelTDA-graph.Let a0 action constrained scheduling constraint associated x 0 . SinceSelectVariable selects x x0 , (ii) a0 level following levelaction constrained scheduling constraint associated x. Thus, property (2),x0 could instantiated, would every time windowa0 constrains a0 start early: current partial solution X augmentedpossible values x implies start time a0 endlast time window a0 . then, (i) assumption X solvable guaranteecannot case.Moreover, since value every instantiated meta-variable propagated forwardchecking unassigned variables, first value assigned metavariable value assigned variable solution found CSP (ifany) easy see first value chosen SelectValue(D(x)) feasible(ForwardCheck-DTP(X 0 , 0 ) returns false), every next value chosen xfeasible.Finally, since value chosen SelectValue meta-variable correspondsearliest available window current domain meta-variable, followssolution computed algorithm complete optimal induced STPend . 2consequence previous theorem, Solve-DTP performs backtracking (step 10),input meta CSP solution. Thus, obtain general backtrack-freealgorithm DTP TDA-graph simply replacing step 1010. stop return fail.correctness modified algorithm, called Solve-DTP+ , followsTheorem 1. next theorem states runtime complexity Solve-DTP + polynomial.Theorem 2 Given TDA-graph G DTP D, Solve-DTP+ processes meta CSPcorresponding polynomial time respect number action nodes Gmaximum number time windows scheduling constraint D. 77. noted main goal give complexity bound polynomial. useimproved forward checking techniques (e.g., Tsamardinos & Pollack, 2003) could lead complexitybound lower one given proof theorem.198fiAn Approach Temporal Planning SchedulingProof. time complexity depends number times ForwardCheck-DTP executed, time complexity. contains linear number variables respectnumber n domain action nodes LA-graph TDA-graph, O(n 2 ) ordering constraints, O(n) duration constraints scheduling constraints. Hence,meta CSP O(n2 ) meta-variables (one variable constraint originalCSP). Let maximum number time windows scheduling constraint D.ForwardCheck-DTP executed times meta-variable x, i.e., O( n 2 ) timestotal. Consistency-STP decides satisfiability STP involving O(n) variables,accomplished O(n3 ) time (Dechter et al., 1991; Gerevini & Cristani, 1997). (Notevariables STP processed Consistency-STP variablesoriginal CSP, i.e., starting time end time actions plan.)Finally, Consistency-STP run O( n2 ) times run ForwardCheck-DTP.follows runtime complexity Solve-DTP+ O( 2 n7 ). 2exploiting structure temporal constraints forming DTP TDAgraph, make following additional changes Solve-DTP+ improving efficiencyalgorithm.Instead starting empty assignment (no meta-variable instantiated),initially every meta-variable associated ordering constraint durationconstraint instantiated value, X contains meta-variables associatedscheduling constraints. observed proof Theorem 1, metaCSP solvable, values assigned meta-variables initial formconsistent STP.Forward checking performed meta-variable.proof Theorem 1 shown that, meta CSP solvable,first value chosen SelectValue feasible (i.e., ForwardCheck-DTP returnstrue). Thus, first value feasible, stop algorithm return failmeta CSP solvable. Moreover, omit steps 6 9save restore domain values meta-variables.Finally, improved algorithm made incremental exploiting particularway update DTP TDA-graph planning (i.e.,search solution TDA-graph described next section). describednext section, search step either addition new action node certainlevel l, removal action node l. cases, suffices recomputesub-solution meta-variables subsets Xl , Xl+1 , ..., Xlast . valuesassigned meta-variables assignment last solutioncomputed updating DTP, part input algorithm.Moreover, order use local search techniques described next section,need another change basic algorithm: algorithm detects Xsolution, instead returning failure, (i) keeps processing remaining meta-variables,(ii) terminates, returns (partial) induced STP Si formed valuesassigned meta-variables. optimal solution Si defines -assignmentTDA-graph.199fiGerevini, Saetti & Serinanext section, SG denotes induced STP DTP TDA-graph G computed method.3. Local Search Techniques TDA-GraphsTDA-graph hA, , P, Ci contain two types flaw: unsupported precondition nodesA, called propositional flaws, action nodes scheduled , calledtemporal flaws. level contains flaw, say level flawed. example,time window p TDA-graph Figure 1 [25, 50), level 3 wouldflawed, start time a3 would 70, violates scheduling constrainta3 imposing action must executed [25, 50).TDA-graph flawed level represents valid plan called solution graph.section, present new heuristics finding solution graph search spaceTDA-graphs. heuristics used guide local search procedure, called Walkplan,originally proposed Gerevini Serina (1999) heartsearch engine planner.initial TDA-graph contains astart aend . search step identifiesneighborhood N (G) (successor states) current TDA-graph G (search state),set TDA-graphs obtained G adding helpful action node removingharmful action node attempt repair earliest flawed level G. 8following, sake brevity refer action node TDA-graph,implicitly referring action node LA-graph TDA-graph. Similarlylevel TDA-graph. Moreover, remind reader l denotes actionlevel l, la denotes level action a.Definition 4 Given flawed level l TDA-graph G, action node helpful l iffinsertion G level l would remove propositional flaw l.Definition 5 Given flawed level l TDA-graph G, action node level lharmful l iff removal G would remove propositional flaw l, woulddecrease -value al , al unscheduled.Examples helpful action node harmful action nodeaction node representing action effect p1 helpful level 3 TDA-graphFigure 1 added level 2 3 (bear mind insertion action nodelevel 3 determines expansion TDA-graph postponing a3 level 4; detailsgiven end examples). Action node a3 Figure 1 harmful level 3,precondition node p1 unsupported; action node a1 harmful level 3,blocks no-op propagation p1 level 1, would support precondition node p1level 3. Moreover, assuming W (p) = {[25, 50)}, a3 unscheduled plan representedLA-graph. Action node a2 harmful level 3, removal a28. designed several flaw selection strategies described experimentally evaluatedrecent paper (Gerevini, Saetti, & Serina, 2004). strategy preferring flaws earliest levelgraph tends perform better others, used default strategy planner.details discussion strategy given aforementioned paper.200fiAn Approach Temporal Planning Schedulingwould decrease temporal value a3 . contrary, a1 harmful level 3,removal would affect possible scheduling a3 . Notice actionnode helpful harmful: a3 harmful level 3, helpfulgoal level (because supports precondition node p10 aend ).add action node level l empty, LA-graph extendedone level, action nodes l shifted forward one level (i.e., movednext level), new action inserted level l . Similarly, removeaction node level l, graph shrunk removing level l. additionaldetails process given another paper (Gerevini et al., 2003). Moreover,pointed previous section, addition (removal) action node requires usupdate DTP G adding (removing) appropriate ordering constraintsactions LA-graph G, duration constraint a, schedulingconstraint (if any). updated DTP, use method describedprevious section revise , compute possibly new schedule actions G(i.e., optimal solution SG ).elements N (G) evaluated using heuristic evaluation function E consistingtwo weighted terms, estimating additional search cost temporal cost, i.e.,number search steps required repair new flaws introduced, contributionmakespan represented plan, respectively. element N (G) lowestcombined cost selected using noise parameter randomizing search escapelocal minima (Gerevini et al., 2003). addition, order escape local minima,new version planner uses short tabu list (Glover & Laguna, 1997). restsection, focus search cost term E. techniques useevaluation temporal cost (automatic) setting term weights Esimilar introduced previous work (Gerevini et al., 2003).search cost adding helpful action node repair flawed level l Gestimated constructing relaxed temporal plan achieving(1) unsupported precondition nodes a, denoted Pre(a)(2) propositional flaws remaining l adding a, denoted Unsup(l),(3) supported precondition nodes action nodes G would becomeunsupported adding a, denoted Threats(a).Moreover, estimate number additional temporal flaws additionG would determine, i.e., count number(I) action nodes G would become unscheduled adding G,(II) unsatisfied timed preconditions a, unscheduled TDA-graph extended ,(III) action nodes scheduling constraint estimate cannot satisfiedcontext G.search cost adding G number actions plus (I), (II) (III),new terms heuristic evaluation. Note action nodes (I)201fiGerevini, Saetti & Serinaaend (90)Goal level(90)p9p8[15]a3 (75)(70)p8Actionb1b2b3b4(70)(70)p10p(70)p9(70)p9p5(70)(70)(70)(50)p1p7Est lower bound001550(35)[5]qp9p8p6Relaxed PlanLevel 3()Actionb1b2b3b4N um acts0015anew (30)(20)p5 (50)p1p3Level 2p1p1p5mutexp1(0)[50]a1 (0)Level 1(0)(50)(0)p2mutexp6p3p3p3(0)(0)a2 [70](0)()p4p4p4(30)q1q2[10](20)(0)b3(20)q3(0)[15]p5(0)b1 (0)astart (0)(0)p3[100]qp4b4 (50)(0)q4p5(50)[20]b2 (0)p4(0)(50)p5Figure 4: example relaxed temporal plan . Square nodes represent action nodes,nodes represent fact nodes; solid nodes correspond nodes{anew }; dotted nodes correspond precondition nodes action nodesconsidered construction ; gray dotted nodesselected inclusion . Action nodes marked durationrepresented actions (in square brackets) estimated start time (inround brackets). meaning Num acts described text; lowerbounds earliest action start times (Est lower bound) computedalgorithm Appendix A.would ordered (because used achieve onepreconditions, action nodes mutex a) that, given estimated endtime duration a, would excessively increase start time. (II)consider original formulation timed preconditions (i.e., formulationpossible compilation one merged new precondition, discussed Section 2.3).Finally, check scheduling constraint action , consider estimated endtime relaxed subplan used achieve preconditions action.Example relaxed temporal plan additional temporal flaws (IIII)Figure 4 gives example evaluating addition anew level 2 LAgraph left side figure (the graph one used Figure 1),202fiAn Approach Temporal Planning SchedulingRelaxedTimePlan(G, I, A)Input: set goal facts (G), initial state relaxed plan (I), set reusable actions (A);Output: set actions Acts forming relaxed plan G earliest timefacts G achieved.1.2.3.4.5.6.7.8.9.10.11.12.13.Acts A; F aActs Add (a);AX {T (g) | g G F g G I};G G I;G F 6=g fact G F ;b BestAction(g);hA, t0 RelaxedTimePlan(Pre(b), I, Acts);(b) ComputeEFT (b, t0 );AX{t, (b)};forall f Add(b)(f ) (f ),ST (b) + Dur(b) ;Acts {b}; F aActs Add (a);return hActs, ti.Figure 5: Algorithm computing relaxed temporal plan. ComputeEFT (b, 0 ) returnsestimated earliest finishing time b consistent schedulingconstraint b (if any), t0 + Dur(b) (for example seeAppendix A). Add (a) denotes set positive effects a.helpful action node unsupported precondition p6 . goals unsupportedpreconditions q1 q2 anew ; initial state formed fact nodessupported level 2. actions anew , b2 b3. numbers nameactions facts relaxed plan indicate order RelaxedTimePlanconsiders them. estimated start time end time b3 20 30, respectively.Assume timed precondition q anew associated time window [0, 20).Concerning point (I), action node G would become unscheduled addinganew G. Concerning point (II), anew unscheduled one timed preconditionunsatisfied (q). Concerning point (III), b3 cannot scheduledcontext current TDA-graph G. Finally, since contains three actions,sum (I), (II) (III) 2, search cost adding anew G level 25.evaluation TDA-graph derived removing harmful action nodeflawed level l similar, achievingprecondition nodes supported would become unsupported removingla < l, unsupported precondition nodes level l become supported removing a.203fiGerevini, Saetti & SerinaRegarding second point, note l = la , flaws l eliminated because,remove action, also (automatically) remove precondition nodes. While,la < l, removal could leave flaws level l.Plan relaxed sense derivation ignores possible (negative) interference actions , actions may unscheduled. derivationtakes account actions already current partial plan (the plan representedTDA-graph G). particular, actions current plan used defineinitial state , obtained applying actions G level la 1, orderedaccording corresponding levels. Moreover, fact f marked temporalvalue, (f ), corresponding time f becomes true (and remains )current subplan formed actions level la 1.relaxed plan constructed using backward process, called RelaxedTimePlan (seeFigure 5), extension RelaxedPlan algorithm proposed previouswork (Gerevini et al., 2003). algorithm outputs two values: set actions forming(sub)relaxed plan, estimated earliest finishing time (used defined temporalcost term E). set actions Acts forming derived running RelaxedTimePlantwice: first goals Pre(a), initial state empty set reusable actions;goals Unsup(l ) hreats(a), initial state Threats(a) Add (a), set reusableactions formed actions computed first run plus a.main novelty extended algorithm computing concerns choiceactions forming relaxed plan. action b chosen achieve (sub)goal g actionminimizing sumestimated minimum number additional actions required support propositional preconditions (Num acts(b, I)),number supported precondition nodes LA-graph would becomeunsupported adding b G (Threats(b)),number timed preconditions b estimate would unsatisfied Gextended (TimedPre(b));number action nodes scheduled estimate would become unscheduled adding b G (TimeThreats(b)).formally, action chosen BestAction(g) step 6 RelaxedTimePlanachieve (sub)goal g action satisfying0000ARGMIN Num acts(a , I) + |Threats(a )| + |TimedPre(a )| + |TimeThreats(a )| ,{a0 Ag }Ag = {a0 | g Add (a0 ), set domain actions whose preconditionsreachable I}.Num acts(b, I) computed algorithm given Appendix A; Threats(b) computed previous method deriving (Gerevini et al., 2003), i.e., consideringnegative interactions (through mutex relations) b precondition nodessupported levels al ; TimedPre(b) TimeThreats(b) new componentsaction selection method, computed follows.204fiAn Approach Temporal Planning Schedulingorder compute TimedPre(b), estimate earliest start time b (Est(b))earliest finishing time b (Ef t(b)). Using values, count numbertimed preconditions b cannot satisfied. Ef t(b) defined Est(b) + Dur(b),Est(b) maximumlower bound possible earliest start time b (Est lower bound b), computedreachability analysis algorithm given Appendix A;-values action nodes ci current TDA-graph G, < la ,mutex b addition b G would occur addition c+b 0DTP G;maximum estimated lower bound time preconditionsb achieved relaxed plan; estimate computed causal structurerelaxed plan, duration scheduling constraints actions,-values facts initial state I.Example TimedPreexample Figure 4, estimated start time b3 maximum 15,Est lower bound b3, 20, maximum time estimatedtimes preconditions b3 supported (p4 supported initial statetime 0, q3 supported time 20). Notice a1 mutex b3,second point definition Est(b3) apply here. Since estimated earlieststart time b3 20 duration b3 10, Ef t(b3) = 20 + 10. Thus, assumeq associated time window [0,20), timed precondition q b3cannot scheduled, i.e., q TimedPre(b3).order compute TimeThreats(b), use following notion time slackaction nodes.Definition 6 Given two action nodes ai aj TDA-graph hA, , P, CiC |= a+< aj , Slack(ai , aj ) maximum time -value aiconsistently increased SG without violating time window chosen scheduling aj .order estimate whether action b time threat action node kcurrent TDA-graph extended action node adding repairing levell (l < k), check(b , a) > Slack(a, ak )holds, b portion relaxed plan computed far, (b , a)estimated delay adding actions b G would cause start time a.Examples time slack TimeThreatslack anew a3 TDA-graph Figure 4 extended anew 35,even anew started 35, a3 could still executed time window[75, 125) (imposed timed precondition p); anew started 35 + , a3would finish 125+ (determined summing start time anew , Dur(anew ), Dur(a2 ),205fiGerevini, Saetti & SerinaDur(a3 )), scheduling constraint a3 would violated. Assumeevaluating inclusion b4 relaxed plan Figure 4 achieving q2.(b4 , anew ) = 150,i.e. estimated delay portion plan formed b4 would add endtime anew 150. Since slack anew a3 35,Slack(anew , a3 ) < (b4 , anew ),a3 TimeThreats(b4). contrary, sinceSlack(anew , a3 ) > (b3 , anew ) = 30a3 6 TimeThreats(b3).conclude section, observe way consider scheduling constraintsevaluation search neighborhood similarity well-knowntechnique used scheduling. example, suppose evaluating TDA-graphsobtained adding helpful action node one among alternative possible levelsgraph, current TDA-graph contains another action node c mutexa. search neighborhood contains two TDA-graphs corresponding (1) addinglevel lc (2) adding level lc , (1) violates less schedulingconstraints (2), then, according points (I)(III), (1) preferred (2). similarheuristic method, called constraint-based analysis, proposed Erschler, RoubellatVernhes (1976) decide whether action scheduled anotherconflicting action, also used scheduling work guiding searchtoward consistent scheduling tasks involved problem (e.g., Smith & Cheng,1993).4. Experimental Resultsimplemented approach planner called lpg-td, obtained 2nd prizemetric-temporal track (satisficing planners) 4th International Planning Competition (IPC-4). lpg-td incremental planner, sense produces sequencevalid plans improves quality previous ones. Plan qualitymeasured using metric expression specified planning problem description.incremental process lpg-td described another paper (Gerevini et al., 2003).Essentially, process iterates search solution graph additional constraintlower bound plan quality, determined quality previouslygenerated plans. lpg-td written C available http://lpg.ing.unibs.it.section, present results experimental study two main goals:testing efficiency approach temporal planning predictable exogenousevents comparing performance lpg-td recent plannersIPC-4 attempted benchmark problems involving timed initial literals (Edelkamp,Hoffmann, Littman, & Younes, 2004);206fiAn Approach Temporal Planning SchedulingPlannerlpg-tdsgplanp-mepcrikeylpg-ipc3downward (diag)downwardmarvinyahspmacro-fffaproadmappertilsapaoptopSolved8451090983643063803602242551898152634Attempted1074141558859459443243243227933219318616650Success ratio79%77%17%61%52%88%83%52%91%57%42%28%38%8%Planning capabilities IPC-4Propositional + DP, Metric-Temporal +TILPropositional + DP, Metric-Temporal +TILPropositional, Metric-Temporal +TILPropositional, Metric-TemporalPropositional, Metric-TemporalPropositional + DPPropositional + DPPropositional + DPPropositionalPropositionalPropositionalPropositionalTILTILTable 1: Number problems attempted/solved success ratio (satisficing) planners took part IPC-4. DP means derived predicates; TIL means timedinitial literals; Propositional means STRIPS ADL. planning capabilities PDDL2.2 features test problems attempted plannerIPC-4.testing effectiveness proposed temporal reasoning techniques integratedplanning process understand, particular, impact overallperformance system, compare existing techniques.first analysis, consider test problems variant IPC-4 metrictemporal domains involving timed initial literals. comparison lpg-td IPC-4planners considering variants IPC-4 metric-temporal domains givenAppendix B. Additional results available web site planner.second experiments, use new domains problems obtained extendingtwo well-known benchmark domains (and relative problems) IPC-3 timedinitial literals (Long & Fox, 2003a).9tests conducted Intel Xeon(tm) 3 GHz, 1 Gbytes RAM. ran lpg-tddefault settings every problem attempted.4.1 LPG-td IPC-4 Plannerssection, use official results IPC-4 compare performance lpg-tdplanners took part competition. performance lpg-tdcorresponds single run. CPU-time limit run 30 minutes,termination forced. lpg-td.s indicates CPU-time required planner derivefirst plan; lpg-td.bq indicates best quality plan found within CPU-time limit.9. description IPC-4 domains relative variants, reader visitofficial web site IPC-4 (http://ls5-www.cs.uni-dortmund.de/edelkamp/ipc-4/index.html).extended versions IPC-3 domains used experiments availablehttp://zeus.ing.unibs.it/lpg/TestsIPC3-TIL.tgz.207fiGerevini, Saetti & Serinafocusing analysis IPC-4 domains involving timed initial literals,Table 1 give brief overview results IPC-4 (satisficing) planners,terms planning capabilities problems attempted/solved planner. tablesummarizes results domain variants IPC-4. lpg-td sgplan (Chen, Hsu,& Wah B., 2004) planners supporting major features PDDL2.1PDDL2.2. planners good success ratio (close 80%). downward (Helmert,2004) yahsp (Vidal, 2004) success ratio better lpg-td sgplan,handle propositional domains (downward supports derived predicates,yahsp not). sgplan attempted problems lpg-td also testedcompiled version variants derived predicates timed initial literals. 10Moreover, lpg-td attempt numerical variant two versions Promeladomain ADL variant PSR-large, use equality numericalpreconditions conditional effects, currently planner support.Figure 6 shows performance lpg-td variants three domains involvingpredictable exogenous events respect (satisficing) planners IPC-4 supporting timed initial literals: sgplan, p-mep (Sanchez et al., 2004) tilsapa (Kavuluri& U, 2004). Airport (upper plots figure), lpg-td solves 45 problems 50,sgplan 43, p-mep 12, tilsapa 7. terms CPU-time, lpg-td performs much betterp-mep tilsapa. lpg-td faster sgplan nearly problems (exceptproblems 1 43). particular, gap performance problems 2131 nearlyone order magnitude. Regarding plan quality, performance lpg-td similarperformance p-mep tilsapa, while, overall, sgplan finds plan worse quality(with exception problems 41 43, sgplan performs slightly better,easiest problems lpg-td sgplan perform similarly).lpg-td tilsapa planners IPC-4 attempted variantPipesWorld timed initial literals (central plots Figure 6). lpg-td solves 23 problems 30, tilsapa solves 3 problems. domain variant lpg-td performsmuch better tilsapa.flaw version Umts (bottom plots Figure 6), lpg-td solves 50 problems,sgplan solves 27 problems (p-mep tilsapa attempt domain variant).Moreover, lpg-td one order magnitude faster sgplan every problemsolved. Compared IPC-4 benchmark problems, Umts problems generallyeasier solve. test problems, main challenge finding plans good quality.Overall, best quality plans lpg-td much better sgplan plans, exceptsimplest problems two planners generate plans similar quality. basicversion Umts without flawed actions, sgplan solves problems lpg-td, termsplan quality lpg-td performs much better.Figure 7 shows results Wilcoxon sign-rank test, also known Wilcoxonmatched pairs test (Wilcoxon & Wilcox, 1964), comparing performance lpg-tdplanners attempted benchmark problems IPC-4 involving timed initial literals. test used Long Fox (2003a) comparing performance10. versions generated planners support features PDDL2.2.competition test lpg-td problems compiled domains plannersupports original version domains. lpg-td attempted every problem (uncompiled)IPC-4 domains could attempt terms planning language supports.208fiAn Approach Temporal Planning SchedulingAirport-WindowsMilliseconds1e+07LPG-td.s (45 solved)P-MEP (12 solved)SGPlan (43 solved)TilSapa (7 solved)1e+06Airport-WindowsMakespan1400LPG-td.bq (45 solved)P-MEP (12 solved)SGPlan (43 solved)TilSapa (7 solved)12001000100000800100006001000400100200100051015202530354045PipesWorldNoTankage-DeadlinesMilliseconds1e+07051015202530354045PipesWorldNoTankage-DeadlinesMakespan30LPG-td.bq (23 solved)TilSapa (3 solved)LPG-td.s (23 solved)TilSapa (3 solved)1e+06251000002010000151000101005100051015202530UmtsFlaw-WindowsMilliseconds10000051015202530UmtsFlaw-WindowsMakespan1900LPG-td.s (50 solved)SGPlan (27 solved)LPG-td.bq (50 solved)SGPlan (27 solved)18501800100017501700165010016001550150010145005101520253035404550051015202530354045Figure 6: CPU-time plan quality lpg-td, p-mep, sgplan, tilsapa threeIPC-4 domains timed initial literals. x-axis problemnames simplified numbers. plots left, y-axisCPU-milliseconds (logarithmic scale); plots right, y-axisplan makespan (the lower better).20950fiGerevini, Saetti & Serinalpg-td.s vs p-mep5.841< 0.00145lpg-td.bq vs p-mep< 0.00112CPU-Time Analysislpg-td.s vs sgplan3.162(0.0016)197lpg-td.s vs tilsapa10.118< 0.001136Plan Quality Analysislpg-td.bq vs sgplan lpg-td.bq vs tilsapa9.8376.901< 0.001< 0.00115463Figure 7: Results Wilcoxon test performance lpg-td comparedIPC-4 (satisficing) planners terms CPU-times plan quality benchmark problems timed initial literals.lpg-td.ssgplantilsapap-mepCPU-Timelpg-td.bqsgplantilsapap-mepB:consistently better BB:better Bsignificant number times(confidence level 99.84%)Plan QualityFigure 8: Partial order performance IPC-4 (satisficing) planners accordingWilcoxon test benchmark problems timed initial literals.dashed arrow indicates performance relationship holds confidencelevel slightly less 99.9%.IPC-3 planners. CPU-time analysis, consider problems attemptedcompared planners solved least one (when plannersolve problem, corresponding CPU-time IEEE arithmetic representationpositive infinity). plan quality (makespan) analysis, consider problemssolved compared planners.210fiAn Approach Temporal Planning Schedulingorder carry Wilcoxon test, planning problem computeddifference CPU-times two planners compared, defining samplestest CPU-time analysis. Similarly, test concerning plan qualityanalysis computed differences makespan plans generatedtwo planners. absolute values differences ranked increasing numbersstarting lowest value. (The lowest value ranked 1, next lowest valueranked 2, on.) sum ranks positive differences, sumranks negative differences. performance two planners significantlydifferent, number positive differences approximately equalnumber negative differences, sum ranks set positivedifferences approximately equal sum ranks set. Intuitively,test considers weighted sum number times one planner performs betterother. sum weighted test uses performance gap assign rankperformance difference.cell Figure 7 gives result comparison performancelpg-td another IPC-4 planner. number samples sufficiently large,T-distribution used Wilcoxon test approximatively normal distribution.Therefore, cells figure contain z-value p-value characterizingnormal distribution. higher z-value, significant differenceperformance is. p-value represents level significance performance gap.use confidence level 99.9%; hence, p-value lower 0.001,performance compared planners statistically different. informationappears left (right) side cell, first (second) planner named titlecell performs better planner.11 analysis comparing CPUtime, value cell number problems solved least one planner;analysis comparing plan quality, number problems solvedplanners.Figure 8 shows graphical description relative performance IPC-4 satisficingplanners according Wilcoxon test benchmark problems timed initialliterals. solid arrow planner planner B (or cluster planners B)indicates performance statistically different performance B,performs better B (every planner B). dashed arrow Bindicates better B significant number times, significantWilcoxon relationship B confidence level 99.9% (onhand, relationship holds confidence level slightly less 99.9%). resultsanalysis say lpg-td consistently faster tilsapa p-mep,faster sgplan significant number times. terms plan quality, lpg-td performsconsistently better p-mep, sgplan tilsapa.Although lpg-td guarantee optimal plans, interesting compareperformance optimal planners took part IPC-4, especially see goodlpg-tds plans are. Figure 9 shows performance lpg-td best resultsresults optimal IPC-4 planners (AllOthers-Opt) temporal variantsAirport Umts (without flawed actions). plots plan quality (makespan)11. p-value cell comparing lpg-td p-mep omitted number problems solvedlpg-td p-mep high enough approximate T-distribution normal distribution.211fiGerevini, Saetti & SerinaAirport-TimeMilliseconds1e+07Airport-TimeMakespan1000LPG-td.s (44 solved)LPG-td.bq (44 solved)AllOthers-Opt (21 solved)LPG-td.s (44 solved)LPG-td.bq (44 solved)AllOthers-Opt (21 solved)9001e+06800700100000600100005004001000300200100100100051015202530354045UMTS-TimeMilliseconds1e+070510152030354045UMTS-TimeMakespan900LPG-td.s (50 solved)LPG-td.bq (50 solved)AllOthers-Opt (38 solved)25LPG-td.s (50 solved)LPG-td.bq (50 solved)AllOthers-Opt (38 solved)8501e+06800100000750100007006501000600100550105000510152025303540455005101520253035404550Figure 9: Performance lpg-td best optimal planners IPC-4(AllOthers-Opt) Airport-Time Umts-Time: CPU-time logarithmic scale(left plots) plan makespan (right plots). x-axis problemnames simplified numbers.show that, nearly every problem domains, best quality plan found lpg-tdoptimal solution, first plan found lpg-td generally good solution.plots CPU-time show lpg-td finds plan much quicklyoptimal planner, CPU-time required lpg-td find best plan oftenlower CPU-time required AllOthers-Opt (except problems 12, 16, 1820 Airport). noted lpg-td.bq last plan sequencecomputed plans increasing quality (and CPU-time). intermediate planssequence could already good quality. particular, shown plan quality plotAirport, first plan (lpg-td.s) solving problem 12 near-optimal quality,computed much quickly lpg-td.bq plan AllOthers-Opt plan.212fiAn Approach Temporal Planning SchedulingFigure 10: Plan quality distance solutions found lpg-td corresponding optimal solutions. x-axis, classes quality distance(e.g., 1025% means plan generated lpg-td worseoptimal plan factor 0.1 0.25). y-axis,percentage solved problems classes.Finally, Figure 10 gives results general analysis plan quality distance,considering metric-temporal STRIPS variants IPC-4 domains. 12 analysisuses problems solved least one IPC-4 optimal planner. also importantnote consider plans generated incremental process lpg-td usingCPU-time CPU-time required fastest optimal planner (AllOthersOpt). Overall, results Figure 10 provide significant empirical evidence supportingclaim often incremental local search approach allows us compute plansgood quality using less CPU-time optimal approach. particular,bars 0%1% class plot metric-temporal problems showpercentage test problems best quality plan lpg-td (lpg-td.bq)optimal nearly optimal (i.e., plan quality worse optimal factor0 0.01, 0 meaning difference) 90%. Moreover, often first plancomputed lpg-td (lpg-td.s) good quality: 60% plans qualityoptimal nearly optimal, 25% quality worseoptimal factor greater 0.5.Interestingly, plot right Figure 10 shows similar results concerning goodquality lpg-tds plans also STRIPS problems IPC-4 (with lower percentagelpg-td.s plans 0%1% class, slightly higher percentagelpg-td.bqs plans > 50% class).4.2 Temporal Reasoning LPG-tdconducted two main experiments. first aimed testing performancelpg-td number windows timed initial literals varies problems12. STRIPS problems, plan quality metric number actions plan.213fiGerevini, Saetti & Serinainitial state goals. second experiment focused temporalreasoning techniques main goals empirically evaluating performance,understanding impact overall performance lpg-td.experiments used two well-known IPC-3 domains, modifiedinclude timed initial literals: Rovers ZenoTravel. version Rovers timedinitial literals obtained IPC-3 temporal version follows. problemspecification, waypoint, added collection pairs timed initial literalstype(at t1 (in sun waypoint0))(at t2 (not (in sun waypoint0)))t1 < t2 . pairs defines time window involved literal.operator specification file, recharge operator precondition(over (in sun ?w))imposes constraint recharging actions applied roversun (?w operator parameter representing waypoint recharging action.)modified version ZenoTravel obtained similarly. problem specification,city added collection pairs timed initial literals type(at t1 (open-station city0))(at t2 (not (open-station city0)))operator specification file, added timed precondition(over (open-station ?c))refuel operator, ?c operator parameter representing cityrefuel action executed.Given planning problem collection time windows W timed literal ,noted that, general, difficulty solving affected three parameters:number windows W , size, way distributed timeline.13 considered two methods generating test problems taking accountparameters ( indicates original IPC-3 problem either Rovers ZenoTraveldomain, n indicates number windows W ):(I) Let best (shortest makespan) plan among generated lpg-tdsolving within certain CPU-time limit, makespan . timeinterval [0, t] divided 2n 1 sub-intervals equal size. time windowstimed literal extended problem 0 odd sub-intervals [0, t],i.e.,nhhh3t2t, 2n1,.W = 0, 2n1, 2n1, . . . , (2n2)t2n1(II) Let maximum duration action timed precondition .time interval = [0, (2n 1)] divided 2n 1 sub-intervals duration d.13. general, parameters influence hardness temporal reasoning planning,also logical part planning process (i.e., selection actions forming plan,lpg-td done using heuristics taking exogenous events account).214fiAn Approach Temporal Planning SchedulingRovers-WindowsMilliseconds10000ZenoTravel-WindowsMilliseconds1e+061 time window per waypoint10 time windows per waypoint1 time window per city10 time windows per city100000100010000100010010010100246810121416182002468101214161820Figure 11: Performance lpg-td Rovers ZenoTravel domains extendedtimed initial literals (1 10 time windows timed literal). testproblems generated using method I. x-axis problemnames simplified numbers; y-axis CPU-milliseconds (logarithmic scale).Similarly method (I), time windows extended problem 0odd sub-intervals .Notice use first method number windows relativelysmall because, many time windows small size, extended problembecome unsolvable (no window large enough schedule necessary actiontimed precondition). second method designed avoid problem,used test techniques planning problems involving many time windows.Figures 11 12 give results first experiment. CPU-times plotsmedian values five runs problem. results Figure 11, useIPC-3 test problems modified method I, results Figure 12 useIPC-3 test problems modified method II. cases lpg-td solves problems.plots Figure 11 indicate performance degradation numberwindows increases 1 10 generally moderate, except two cases. plotsFigure 12 indicate that, number windows increases exponentially 110,000, approach scales well benchmark problems considered. instance,consider first ZenoTravel problem. 1 window lpg-td solves problem 10milliseconds, 10 windows 20 milliseconds, 100 windows 30 milliseconds,1000 windows 100 milliseconds, 10,000 windows 1 second.Moreover, observed performance degradation mainly determined heavierpre-processing phase (parsing instantiation operators).Tables 2 3 give results concerning experiment temporal reasoningtechniques implemented lpg-td. consider problems 10 time windows(for timed fluent) used tests Figure 11, examine computational215fiGerevini, Saetti & SerinaPerformance LPG-td Rovers-TimeWindowsMilliseconds100000Performance LPG-td ZenoTravel-TimeWindowsMilliseconds1e+061 time window per waypoint10 time windows per waypoint100 time windows per waypoint1000 time windows per waypoint10,000 time windows per waypoint1 time window per city10 time windows per city100 time windows per city1000 time windows per city10,000 time windows per city10000010000100001000100010010010100246810121416182002468101214161820Figure 12: Performance lpg-td Rovers ZenoTravel domains extendedtimed initial literals (110,000 time windows timed literal). testproblems generated using method II. x-axis problemnames simplified numbers; y-axis CPU-milliseconds (logarithmic scale).cost temporal reasoning planning problems. approach temporalplanning, search step defines set temporal constraints formed orderingscheduling constraints current TDA-graph. Table 2 gives statistical informationDTPs using compact constraint representation lpg-td classicalDTP representation. action TDA-graph, two temporal variables(the start/end times action), except astart aend (for which, pointed out,use one variable). number scheduling constraints numberordering constraints depend actions current TDA-graph,actions (causally exclusively) related other, respectively (weone scheduling constraint action timed precondition TDA-graph).Notice representation scheduling constraints much compactclassical DTP formulation.14table also gives information average number DTPs (i.e., search steps)generated planning, indicating many satisfiable (indicatedSat. DTPs).Table 3 gives CPU-time required temporal reasoning techniques implementedlpg-td (Solve-DTP+ ) tsat++ (Armando, Castellini, Giunchiglia, & Maratea,2004), state-of-the-art general DTP solver. DTPs consideredTable 2, i.e., sets temporal constraints TDA-graph search14. classical DTP-translation scheduling constraint contains exponential number disjunctsrespect number time windows scheduling constraint. example, let qtimed precondition Wq = {[25, 50), [75, 125)}. scheduling constraint determined q+translated four classical DTP constraints (as abbreviates astart ): (a+25 75),+++++++++5075),(a50125),(a25(a+125).216fiAn Approach Temporal Planning SchedulingProblemsRoversProblem 1Problem 5Problem 10Problem 15Problem 20ZenoTravelProblem 1Problem 5Problem 10Problem 15Problem 20Variablesmax mean2856949820618.430.065.858.8105.083611417228262083.4122.4194.6SC 10 windows (DC)maxmean12234(1024)(2048)(2048)(3072)(4096)1 (1024)3 (3072)16 (16,384)24 (24,576)42 (43,008)DTPs(Sat. DTPs)0.13 (136.5)0.33 (341.3)1.41 (1447)1.01 (1037)1.45 (1489)15 (15)27 (27)104 (47)77 (55)108 (108)0.33 (341.3)0.88 (910.2)10.5 (10,769)16.3 (16,673)24.9 (25,536)3 (3)18 (18)1162 (175)291 (128)750 (637)Table 2: Characteristics DTPs generated planning lpg-td solvingproblems Rovers ZenoTravel domains: maximum/mean number variables (2nd/3rd columns); maximum/mean number scheduling constraints (SC) non-unary disjunctions (DC) DTP-form translation (4th/5th columns); number DTPs satisfiable DTPs solved lpg-td(6th column).step planning process. noted comparison Solve-DTP +tsat++ means intended determine one better other. Indeedtsat++ developed manage much larger class DTPs. However, bestknowledge exists specialized DTP-solver handling scheduling constraints could used. goal comparison experimentally showexisting general DTP solvers, although designed work efficiently general case,adequate managing class DTPs arise planning framework.Hence, important develop specialized techniques which, empirically demonstrated results Table 3, much efficient. instance, consider problem15 Rovers domain. indicated last column Table 2, lpg-td solvesproblem 77 search steps, defines 77 DTPs. data Table 3 showtotal CPU-time spent lpg-td solving temporal reasoning problemsnegligible (< 106 seconds), tsat++ requires 16.8 CPU-seconds total (notewhole temporal planning problem solved lpg-td 0.25 seconds). 15 Overall,specialized temporal reasoning technique several orders magnitude fasterefficient general DTP, terms CPU-time solving single DTP, CPU-timesolving DTPs generated planning.15. CPU-time tsat++ includes neither generation explicit (classical) DTPs TDAgraph, parsing time. Moreover, tsat++ decides satisfiability input DTPs,Solve-DTP+ also finds schedule optimal, DTP satisfiable.217fiGerevini, Saetti & SerinaProblemsRoversProblem 1Problem 5Problem 10Problem 15Problem 20ZenoTravelProblem 1Problem 5Problem 10Problem 15Problem 20CPU-seconds Temporal ReasoningSolve-DTP+tsat++maxmeantotalmaxmeantotalTotalCPU-Timelpg-td< 106< 106< 106< 1060.01< 106< 106< 106< 1060.0008< 106< 106< 106< 1060.030.0050.0450.540.543.170.0020.0020.0390.0280.100.090.1412.716.8107.10.020.030.300.253.03< 106< 1060.010.010.01< 106< 1060.000170.000140.00065< 106< 1060.20.040.50.0010.042.744.6323.90.00030.0049.83.924.20.010.21601818,877177,5950.020.0522.013.9376.2Table 3: Performance Solve-DTP+ tsat++ DTPs generated planninglpg-td solving problems Rovers ZenoTravel domains:maximum, mean total CPU-seconds. last column gives total CPUtime lpg-td solving planning problem. tsat++ run using defaultsettings.Finally, experimentally tested effectiveness improvements Solve-DTP +making algorithm incremental described end Section 2 (suchimprovements included implementation Solve-DTP+ Table 3). particularobserved that, problems Table 3, average CPU-time basic (nonincremental) version Solve-DTP+ one three orders magnitude higherincremental version. However, basic version still always significantly fastertsat++ (from one four orders magnitude).5. Related WorkSeveral researchers addressed temporal reasoning context DTP framework. general techniques aimed efficiently solving DTP proposed(e.g., Armando et al., 2004; Tsamardinos & Pollack, 2003), worst-case complexityremains exponential. Section 4, presented experimental results indicatingsimple use state-of-the-art DTP solver adequate solving subclassDTPs arise context.Various planning approaches supporting temporal features considered paperproposed. One first planners capable handling predictableexogenous events deviser (Vere, 1983), developed nonlin (Tate, 1977).deviser temporal partial order planner using network activities called plannetwork. starting plan generation, plan network contains exogenous events218fiAn Approach Temporal Planning Schedulingexplicit nodes network. plan generation, activities added networkordered respect scheduled events, depending relevance eventsactivities. similar explicit treatment exogenous events could also adoptedcontext action-graph representation: initial action graph contains specialaction nodes representing predicted exogenous events. However, simple methoddisadvantages respect method, treats exogenous eventstemporal level representation rather logical (causal) level. particular,high number timed initial literals, explicit representationexogenous events action graph could lead large graphs, causing memoryconsumption problems possibly heavy CPU-time cost heuristic evaluation(possibly large) search neighborhood.late 80s early 90s temporal planners handling exogenous eventsdeveloped. general, systems use input descriptions planning problem/domain significantly different PDDL descriptions accepted modernfully-automated planners. One successful among hsts (Frederking &Muscettola, 1992; Muscettola, 1994), representation problem solving frameworkprovides integrated view planning scheduling. hsts represents predictable exogenous events non-controllable state variables. lpg-td hsts managetemporal constraints, two systems use considerably different approaches temporalplanning (lpg-td adopts classical state-transition view change, hsts adoptshistories view change, Ghallab, Nau, & Traverso, 2003), baseddifferent plan representations search techniques.zeno (Penberthy, 1993; Penberthy & Weld, 1994) one first domain-independentplanners supports rich class metric-temporal features, including exogenous events.zeno powerful extension causal-link partial-order planner ucpop (Penberthy &Weld, 1992). However, terms computational performance, planner competitive recent temporal planners.IxTeT (Ghallab & Laruelle, 1994; Laborie & Ghallab, 1995) another causal-link planner uses techniques ideas scheduling, temporal constraint reasoning,graph algorithms. IxTeT supports expressive language temporal description actions, including timed preconditions features cannotexpressed PDDL2.2. expressive power language obtained cost increased semantic complexity (Fox & Long, 2005). observed Ghallab, Nau Traverso(2003), IxTeT embodies compromise expressiveness complex temporal domains, planning efficiency; however, planner still remains noncompetitiverecent temporal planners.Smith Weld (1999) studied extension Graphplan-style planningmanaging temporal domains. proposed extension tgp planner makespossible represent predictable exogenous events. tgp supports subclassdurative actions expressible PDDL2.1, prevents cases concurrencyPDDL2.1 admitted. tgp optimal planner (under assumed conservativemodel action concurrency), lpg-td near-optimal (satisficing) planner. maindrawback tgp scale adequately.recently, Edelkamp (2004) proposed method planning timed initialliterals based compiling action timed preconditions time window as219fiGerevini, Saetti & Serinasociated action, defining interval action scheduled.gives efficient, polynomial algorithm based critical path analysis computingoptimal action schedule sequential plans generated using compiled representation. techniques presented Edelkamp assume unique time windowtimed precondition. techniques propose general, senseaction representation treats multiple time windows associated timed precondition,temporal reasoning method computes optimal schedules partially ordered planspreserving polynomiality.Cresswell Coddington (2004) proposed extension lpgp planner (Long& Fox, 2003b) handle timed initial literals, represented special deadlineactions. literal asserted hold time represented deadline actionstarting time initial state, duration t. deadline actionsplan construction translated particular linear inequalities that, togetherequalities inequalities generated plan representation, managedgeneral linear programming solver. lpg-td uses different representationencode timed initial literals special actions, temporal schedulingconstraints associated actions plan managed efficient algorithmderived specializing general DTP solver.order handle problems timed initial literals sapa planner (Do & Kambhampati, 2003), Do, Kambhampati Zimmerman (2004) proposed forward searchheuristic based relaxed plans, constructed exploiting technique similartime slack analysis used scheduling (Smith & Cheng, 1993). Given set candidateactions choosing action add relaxed plan construction, techniquecomputes minimum slack candidate action actions currentlyrelaxed plan. candidate action highest minimum slack preferred. lpg-tduses different time slack analysis, exploited different way. methodselecting actions forming relaxed plan uses time slacks counting numberscheduling constraints would violated adding candidate action: prefercandidate actions cause lowest number violations. Moreover, sapaslack analysis limited actions relaxed plan, method also considersactions real plan construction.dt-pop recent planner (Schwartz & Pollack, 2004) extending POP-styleplanning action model involving disjunctive temporal constraints. languagedt-pop elegant express rich class temporal features,indirectly (and less elegantly) expressed PDDL2.2 (Fox et al., 2004). treatmenttemporal constraints required manage predictable exogenous events dt-popappears less efficient planner, since dt-pop uses general DTP solverenhanced efficiency techniques, lpg-td uses polynomial solver specializedsubclass DTPs arise representation. dt-pop handles mutex actions(threats) posting explicit temporal disjunctive constraints imposing disjointnessmutex actions, lpg-td implicitly decides disjunctions search timechoosing level graph actions inserted, asserting appropriateprecedence constraints. Moreover, search procedure heuristics dt-pop lpgtd significantly different.220fiAn Approach Temporal Planning SchedulingIPC-4, planners reasoned timed initial literals tilsapa (Kavuluri &U, 2004), sgplan (Chen et al., 2004), p-mep (Sanchez et al., 2004) lpg-td. firsttwo planners, time writing, best knowledge available literaturesufficiently detailed description clearly understand possible similaritiesdifferences lpg-td treatment predictable exogenous events. Regardingp-mep, planner uses forward state-space search guided relaxed plan heuristic which,differently relaxed plans lpg-td, constructed without taking accounttemporal aspects relaxed plan real plan construction (the makespanconstructed relaxed plans considered comparative evaluation).6. Conclusionspresented techniques temporal planning domains certain fluentsmade true false known times predictable exogenous events cannotinfluenced actions available planner. external events present manyrealistic domains, planner take account guarantee correctnesssynthesized plans, generate plans good optimal quality (makespan),use effective search heuristics fast planning.approach, causal structure plan represented graph-based representation called TDA-graph, action ordering scheduling constraints managedefficient constraint-based reasoning, plan search based stochastic local searchprocedure. proposed algorithm managing temporal constraintsTDA-graph specialization general CSP-based method solving DTPs.algorithm polynomial worst-case complexity and, combined planrepresentation, practice efficient. also presented local searchtechniques temporal planning using new TDA-graph representation. techniques improve accuracy heuristic methods adopted previous versionlpg, extend consider action scheduling constraints evaluationsearch neighborhood, based relaxed temporal plans exploiting (dynamic)reachability information.techniques implemented planner lpg-td. experimentallyinvestigated performance planner statistical analysis IPC-4 resultsusing Wilcoxons test. results analysis show planner performs wellcompared recent temporal planners supporting predictable exogenous events,terms CPU-time find valid plan quality best plan generated. Moreover,comparison plans computed lpg-td generated optimal plannersIPC-4 shows often lpg-td generates plans good optimal quality.Finally, additional experiments indicate temporal reasoning techniques manageclass DTPs arise context efficiently.directions future work temporal planning within framework are:extension local search heuristics temporal reasoning techniques explicitly handle action effects limited persistence delays; treatment predictable exogenousevents affecting numerical fluents discrete continuous way; development tech221fiGerevini, Saetti & Serinaniques supporting controllable exogenous events;16 management actionsvariable durations (Fox & Long, 2003), i.e., actions whose durations specifiedinequalities constraining lower upper bounds, whose actual duration decidedplanner.Moreover, intend study integration framework techniquesgoal partitioning subplan composition successfully used sgplan(Chen et al., 2004) IPC-4, application approach plan revision.latter already partially explored, simple strips domains usingless powerful search techniques (Gerevini & Serina, 2000).Acknowledgmentspaper revised extended version paper appearing ProceedingsNineteenth International Joint Conference Artificial Intelligence (Gerevini, Saetti, &Serina, 2005a). research supported part MIUR Grant anemone. workIvan Serina part carried Department Computer InformationSciences University Strathclyde (Glasgow, UK), supported Marie CurieFellowship N HPMF-CT-2002-02149. would like thank anonymous reviewershelpful comments, Paolo Toninelli extended parser lpg-td handlenew language features PDDL2.2.Appendix A: Reachability Informationtechniques described paper computing action evaluation function useheuristic reachability information minimum number actions required reachpreconditions domain action (N um acts) lower bound earliestfinishing time (Ef t) reachable actions (the actions whose preconditions reachable).following, S(l) denotes state defined facts corresponding fact nodessupported level l current TDA-graph. l = 1, S(l) represents initial stateplanning problem (I).action a, lpg-td pre-computes N um acts(a, I), i.e., estimated minimumnumber actions required reach preconditions I, Ef t(a, I), i.e.,estimated earliest finishing time (if reachable I). Similarly, fact freachable I, lpg-td computes estimated minimum number actions requiredreach f (N um acts(f, I)) estimated earliest time f madetrue plan starting (Et(f, I)). l > 1, N um acts(a, S(l)) Ef t(a, S(l))computed search, depend action nodescurrent TDA-graph levels preceding l. Since search many action nodesadded removed, operations N um acts(a, S(l)) Ef t(a, S(l))could change (if operation concerns level preceding l), importantcomputed efficiently.16. Consider instance transportation domain shuttle bus train station extrarun airport midnight booked advance. shuttle booking domain actionavailable planner, event night stop shuttle controlled planner.222fiAn Approach Temporal Planning SchedulingReachabilityInformation(I, O)Input: initial state planning problem consideration (I) ground instances(actions) operators (O);Output: action a, estimate number actions (N um acts(a, I)) required reachpreconditions I, estimate earliest finishing time (Ef t(a, I)).1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16.17.18.19.20.21.22.23.24.forall facts f /* set facts precomputed operator instantiation phase */fN um acts(f, I) Et(f, I) 0; Action(f, I) astart ;else N um acts(f, I) Et(f, I) ;forall actions N um acts(a, I) Ef t(a, I) Lf t(a) ;F I; Fnew I; O; Arev ;( Fnew 6= Arev 6= )F F Fnew ; Fnew ; Arev ; Arev ;A0 = {a | P re(a) F } emptyaction A0 ;ComputeEFT(a, AX Et(f, I));f P re(a)< Ef t(a, I) Ef t(a, I) t;Lf t(a) ComputeLFT(a);Ef t(a, I) Lf t(a) /* scheduled */ra RequiredActions(I, P re(a));N um acts(a, I) > ra N um acts(a, I) ra;forall f Add(a)Et(f, I) >Et(f, I) t;Arev Arev {a0 | f P re(a0 )};N um acts(f, I) > (ra + 1)N um acts(f, I) ra + 1; Action(f, I) a;Fnew Fnew Add(a) F ;{a};RequiredActions(I, G)Input: set facts set action preconditions G;Output: estimate min number actions required achieve facts G (ACTS).1.2.3.4.5.6.7.8.ACT ;G G I;G 6=g element G;Action(g, I);ACT ACT {a};G G P re(a) bACT Add(b);return(|ACT S|).Figure 13: Algorithms computing heuristic information search costtime reaching set facts G I.223fiGerevini, Saetti & SerinaFigure 13 gives ReachabilityInformation, algorithm used lpg-td computingN um acts(a, I), Ef t(a, I), N um acts(f, I) Et(f, I). ReachabilityInformation similarreachability algorithm used version lpg took part 2002 planningcompetition (lpg-ipc3), significant differences. main differences are:(i) order estimate earliest finishing time domain actions, ReachabilityInformation takes account scheduling constraints, consideredprevious version algorithm;(ii) algorithm used lpg-ipc3 applies domain action once, ReachabilityInformation apply once.Notice (i) improves accuracy estimated finishing time actions(Ef t), important piece information used search neighborhoodevaluation selecting actions forming temporal relaxed plans (see Section 3).Moreover, (i) allows us identify domain actions cannot scheduledtime windows associated timed preconditions, prunedaway.Regarding (ii), forward process computing reachability information,action re-applied whenever estimated earliest time one preconditionsdecreased. important two reasons. one hand, reconsidering actions alreadyapplied useful lead better estimate action finishing times;hand, also necessary guarantee correctness reachabilityalgorithm. latter because, overestimate earliest finishing time actionscheduling constraint, could incorrectly conclude action cannotscheduled (and would consider action inapplicable). action necessaryvalid plan, incorrect estimate earliest finishing time could leadincorrect conclusion planning problem unsolvable. words, estimatedfinishing time action scheduling constraint lower bound actualearliest finishing time.ReachabilityInformation could used update N um acts(a, S(l)) Ef t(a, S(l))action insertion/removal, l > 1 (when l > 1, instead I, input algorithmS(l)). However, order make updating process efficient, revision doneselective focused way. Instead revising reachability informationgraph modification (search step), evaluating search neighborhoodchoosing estimated best modification. Specifically, repairing flawed level l,update reachability information actions facts levels precedingl updated yet. (For instance, suppose ith search step addaction level 5, (i + 1)th step add another action level 10.(i + 1)th step need consider updating reachability information levels610, since information levels 15 already updated ith step.)sufficient search neighborhood repairing flawed level consideration(l) contain graph modifications concerning levels preceding l.describing steps ReachabilityInformation, need introduce notation. Add (a) denotes set positive effects a; Pre(a) denotes set(non-timed) preconditions a; Arev denotes set actions already applied whose224fiAn Approach Temporal Planning Schedulingreachability could revised estimated earliest time preconditions revised application. Given action node currentearliest start time computed maximum earliest times preconditions reachable, ComputeEFT (a, t) function computing earliest finishingtime consistent scheduling constraint (if any)+ Dur(a) .17 ComputeLFT (a) function computing latest finishing timeaction a, i.e., returns upper bound last time windowscheduled (if one exists), returns timed precondition.example, let action preconditions true initialstate (i.e., = 0), duration 50, scheduling constraint imposingaction executed interval [25, 100). ComputeEFT (a, t) returns 75,ComputeLFT (a, t) returns 100. Thus, scheduling constraint satisfied.contrary, earliest start time 500, ComputeEFT (a, t) returns 550cannot scheduled [25, 100).sake clarity, first describe steps ReachabilityInformation used deriveN um acts, comment computation Ef t. steps 14,every fact f , algorithm initializes N um acts(f, I) 0, f I, otherwise(indicating f reachable); while, step 5, N um acts(a, I) initialized(indicating reachable I). Then, steps 724 algorithm iterativelyconstructs set F facts reachable I, starting F = I,terminating F cannot extended set Arev actions reconsiderempty. set available actions initialized set possible actions(step 6); reduced application (step 24), augmented setactions Arev (step 8) action application. modify estimated timeprecondition action becomes reachable, added rev (step 20).internal while-loop (steps 924) applies actions current F , possibly derivingnew set facts Fnew step 23. Fnew Arev empty, F extendedFnew , extended Arev , internal loop repeated. action A0(the subset actions currently applicable F ) applied, reachabilityinformation effects revised follows. First estimate minimum numberra actions required achieve P re(a) using subroutine RequiredActions (step15). use ra possibly update N um acts(a, I) N um acts(f, I) effectf (steps 1516, 2122). number actions required achieve preconditionslower current value N um acts(a, I), N um acts(a, I) set ra.Moreover, application leads lower estimate f , i.e., ra + 1 lesscurrent value N um acts(f, I), N um acts(f, I) set ra + 1. addition, datastructure indicating current best action achieve f (Action(f, I)) set(step 22). information used subroutine RequiredActions.fact f initial state, value Action(f, I) astart (step 3).subroutine RequiredActions one reachability algorithm lpg-ipc3.subroutine uses Action derive ra backward process starting inputset action preconditions (G), ending G I. subroutine incrementallyconstructs set actions (ACTS) achieving facts G preconditions17. scheduling constraint associated a, existing scheduling constraints cannotsatisfied starting action t, ComputeEFT (a, t) returns + Dur(a).225fiGerevini, Saetti & Serinaactions already selected (using Action). iteration set G revised addingpreconditions last action selected, removing facts belongingeffects actions already selected (step 7). Termination RequiredActions guaranteedevery element G reachable I.briefly describe computation temporal information. Eft(a, I), computed way similar N um acts(a, I). steps 14, ReachabilityInformation initializesestimated earliest time (Et(f, I)) fact f becomes reachable 0, f I,otherwise; moreover, algorithm sets Ef t(a, I) Lf t(a, I) . Then, everyapplication action forward process described above, estimate earliestfinishing time Ef adding duration (current) maximum estimated earliesttime preconditions a, taking account scheduling constraintsusing ComputeEFT (a) (step 11). addition, compute latest finishing time Lfusing ComputeLFT (a) (step 13). earliest finishing time actiongreater latest finishing time, timed preconditions cannot satisfiedI, steps 1523 executed (see if-statement step 14). effect fcurrent temporal value higher earliest finishing time a, steps 1819set Et(f, I) t, step 20 adds Arev (because decreased estimatedearliestx time f , revision could decrease estimated start time actionprecondition f ).Appendix B: Wilcoxon Test Metric-Temporal Domains IPC-4appendix, present results Wilcoxon sign-rank test performancelpg-td satisficing IPC-4 planners attempted metric-temporaldomains. performance evaluated terms CPU-times plan quality.cell first two tables gives result comparison performancelpg-td another IPC-4 planner. number samples sufficiently large,T-distribution used Wilcoxon test approximatively normal distribution. Hence,cell Figure give z-value p-value characterizing normaldistribution. higher z-value, significant difference performanceis. p-value represents level significance difference performance.use confidence level 99.9%; therefore, p-value lower 0.001,performance two planners statistically different. information appearsleft (right) side cell, first (second) planner named title cellperforms better other. analysis comparing CPU-time, valuecell number problems solved least one planner; analysiscomparing plan quality, number problems solved planners.pictures tables show partial order performance comparedplanners terms CPU-time plan quality. solid edge planner anotherplanner B (or cluster planners B) indicates performance statisticallydifferent performance B, performs better B (every plannerB). dashed edge B indicates better B significant numbertimes, significant Wilcoxon relationship confidence level99.9%.226fiAn Approach Temporal Planning Schedulinglpg-td.s vs crikey11.275< 0.001169Analysis CPU-Timelpg-td.s vs p-meplpg-td.s vs sgplan11.1320.387< 0.001(0.699)215513lpg-td.s vs tilsapa12.324< 0.001136lpg-td.bq vs crikey10.500< 0.001173Analysis Plan Qualitylpg-td.bq vs p-mep lpg-td.bq vs sgplan4.01616.879< 0.001< 0.00121452lpg-td.bq vs tilsapa6.901< 0.00163lpg-td.scrikeyp-meptilsapasgplanCPU-Timesgplanlpg-td.bqp-mepcrikeyB:consistently better BtilsapaB:better B significant number times(confidence level 94.78%)Plan Quality227fiGerevini, Saetti & SerinaReferencesArmando, A., Castellini, C., Giunchiglia, E., & Maratea, M. (2004). SAT-based decisionprocedure boolean combination difference constraints. ProceedingsSeventh International Conference Theory Applications Satisfiability Testing(SAT-04), Berlin, Heidelberg, New York. Springer-Verlag. SAT 2004 LNCS Volume.Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. ArtificialIntelligence, 90, pp. 281300.Chen, Y., Hsu, C., & Wah B., W. (2004). SGPlan: Subgoal partitioning resolutionplanning. Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.),Abstract Booklet Competing Planners ICAPS-04, pp. 3032.Cresswell, S., & Coddington, A. (2004). Adapting LPGP plan deadlines. Proceedings Sixteenth European Conference Artificial Intelligence (ECAI-04),pp. 983984, Amsterdam, Netherlands. IOS Press.Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49, pp. 6195.Do, M., B., Kambhampati, S., & Zimmerman, T. (2004). Planning - scheduling connectionsexogenous events. Proceedings ICAPS-04 Workshop IntegratingPlanning Scheduling, pp. 3237.Do, M., & Kambhampati, S. (2003). SAPA: multi-objective metric temporal planner.Journal Artificial Intelligence Research (JAIR), 20, pp. 155194.Edelkamp, S. (2004). Extended critical paths temporal planning. ProceedingsICAPS-04 Workshop Integrating Planning Scheduling, pp. 3845.Edelkamp, S., & Hoffmann, J. (2004). PDDL2.2: language classic part4th international planning competition. Technical report 195, Institut fur Informatik,Freiburg, Germany.Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (2004) Abstract Bookletcompeting planners ICAPS-04.Erschler, J., Roubellat, F., & Vernhes, J. P. (1976). Finding essential characteristicsfeasible solutions scheduling problem. Operations Research (OR), 24, pp.772782.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporalplanning domains. Journal Artificial Intelligence Research (JAIR), 20, pp. 61124.Fox, M., & Long, D. (2005). Planning time. Fisher, M., Gabbay, D., & Vila, L. (Eds.),Handbook Temporal Reasoning Artificial Intelligence, pp. 497536. Elsevier Science Publishers, New York, NY, USA.Fox, M., Long, D., & Halsey, K. (2004). investigation expressive powerPDDL2.1. Proceedings Sixteenth European Conference Artificial Intelligence (ECAI-04), pp. 338342, Amsterdam, Netherlands. IOS Press.Frederking, R., E., & Muscettola, N. (1992). Temporal planning transportation planning scheduling. IEEE International Conference Robotics Automation(ICRA-92), pp. 11251230. IEEE Computer Society Press.228fiAn Approach Temporal Planning SchedulingGerevini, A., & Cristani, M. (1997). finding solution temporal constraint satisfactionproblems. Proceedings Fifteenth International Joint Conference ArtificialIntelligence (IJCAI-97), Vol. 2, pp. 14601465, San Francisco, CA, USA. MorganKaufmann Publishers.Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local searchtemporal action graphs. Journal Artificial Intelligence Research (JAIR), 20, pp.239290.Gerevini, A., Saetti, A., & Serina, I. (2004). empirical analysis heuristic featureslocal search LPG. Proceedings Fourteenth International ConferenceAutomated Planning Scheduling (ICAPS-04), pp. 171180, Menlo Park, CA,USA. AAAI Press.Gerevini, A., Saetti, A., & Serina, I. (2005a). Integrating planning temporal reasoning domains durations time windows. Proceedings NineteenthInternational Joint Conference Artificial Intelligence (IJCAI-05), pp. 12261235,Menlo Park, CA, USA. International Joint Conference Artificial Intelligence Inc.Gerevini, A., Saetti, A., Serina, I., & Toninelli, P. (2005b). Fast planning domainsderived predicates: approach based rule-action graphs local search.Proceedings Twentieth National Conference Artificial Intelligence (AAAI05), pp. 11571162, Menlo Park, CA, USA. AAAI Press.Gerevini, A., & Serina, I. (1999). Fast planning greedy action graphs. ProceedingsSixteenth National Conference Artificial Intelligence (AAAI-99), pp. 503510, Menlo Park, CA, USA. AAAI Press/MIT Press.Gerevini, A., & Serina, I. (2000). Fast plan adaptation planning graphs: Localsystematic search techniques. Proceedings Fifth International ConferenceArtificial Intelligence Planning Scheduling (AIPS-00), pp. 112121, Menlo Park,CA, USA. AAAI Press/MIT Press.Ghallab, M., & Laruelle, H. (1994). Representation control IxTeT, temporal planner. Proceedings Second International Conference Artificial IntelligencePlanning Systems (AIPS-94), pp. 6167, Menlo Park, CA, USA. AAAI press.Ghallab, M., Nau, D., & Traverso, P. (2003). Automated Planning: Theory Practice.Morgan Kaufmann Publishers, San Francisco, CA, USA.Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer Academic Publishers, Boston, USA.Helmert, M. (2004). planning heuristic based causal graph analysis. ProceedingsFourteenth International Conference Automated Planning Scheduling(ICAPS-04), pp. 161170, Menlo Park, CA, USA. AAAI Press.Kavuluri, B. R., & U, S. (2004). Tilsapa - timed initial literals using SAPA. Edelkamp, S.,Hoffmann, J., Littman, M., & Younes, H. (Eds.), Abstract Booklet CompetingPlanners ICAPS-04, pp. 4647.Laborie, P., & Ghallab, M. (1995). Planning sharable resource constraints. Proceedings Fourteenth International Joint Conference Artificial Intelligence(IJCAI-95), Vol. 2, pp. 16431651, San Francisco, CA, USA. Morgan Kaufmann Publishers.229fiGerevini, Saetti & SerinaLong, D., & Fox, M. (2003a). 3rd international planning competition: Resultsanalysis. Journal Artificial Intelligence Research (JAIR), 20, pp. 159.Long, D., & Fox, M. (2003b). Exploiting graphplan framework temporal planning.Proceedings Thirteenth International Conference Automated PlanningScheduling (ICAPS-03), pp. 5261, Menlo Park, CA, USA. AAAI Press.McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. ProceedingsNinth National Conference Artificial Intelligence (AAAI-91), pp. 634639,Menlo Park, CA, USA. AAAI Press.Muscettola, N. (1994). HSTS: Integrating planning scheduling. Zweben, & Fox(Eds.), Intelligent Scheduling, pp. 169212, San Francisco, CA, USA. Morgan Kaufmann Publishers.Nguyen, X., & Kambhampati, S. (2001). Reviving partial order planning. ProceedingsSeventeenth International Joint Conference Artificial Intelligence (IJCAI-01),Vol. 1, pp. 459464, San Francisco, CA, USA. Morgan Kaufmann Publishers.Penberthy, J., & Weld, D. (1992). UCPOP: sound, complete, partial order plannerADL. Proceedings Third International Conference Principles KnowledgeRepresentation Reasoning (KR92), pp. 103114, San Mateo, CA, USA. MorganKaufmann Publishers.Penberthy, J., & Weld, D. (1994). Temporal planning continuous change. ProceedingsTwelfth National Conference Artificial Intelligence (AAAI-94), pp. 10101015, Menlo Park, CA, USA. AAAI Press/MIT Press.Penberthy, J., S. (1993). Planning Continuous Change. Ph.D. thesis, UniversityWashington, Seattle, WA, USA. Available technical report UW-CSE-93-12-01.Sanchez, J., Tang, M., & Mali, A., D. (2004). P-MEP: Parallel expressive planner.Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.), Abstract BookletCompeting Planners ICAPS-04, pp. 5355.Schwartz, P., J., & Pollack, M., E. (2004). Planning disjunctive temporal constraints.Proceedings ICAPS-04 Workshop Integrating Planning Scheduling,pp. 6774.Smith, D., & Weld, D. (1999). Temporal planning mutual exclusive reasoning.Proceedings Sixteenth International Joint Conference Artificial Intelligence(IJCAI-99), pp. 326337, San Francisco, CA, USA. Morgan Kaufmann Publishers.Smith, S., & Cheng, C. (1993). Slack-based heuristics constraint satisfaction scheduling.Proceedings Eleventh National Conference Artificial Intelligence (AAAI93), pp. 139144, Menlo Park, CA, USA. AAAI Press/The MIT press.Stergiou, K., & Koubarakis, M. (2000). Backtracking algorithms disjunctions temporalconstraints. Artificial Intelligence, 120 (1), pp. 81117.Tate, A. (1977). Generating project networks. Proceedings Fifth InternationalJoint Conference Artificial Intelligence (IJCAI-77), pp. 888889, Cambridge, MA,USA. MIT, William Kaufmann.230fiAn Approach Temporal Planning SchedulingTsamardinos, I., & Pollack, M. E. (2003). Efficient solution techniques disjunctivetemporal reasoning problems. Artificial Intelligence, 151 (1-2), pp. 4389.Vere, S. A. (1983). Planning time: Windows durations activities goals. IEEETransactions Pattern Analysis Machine Intelligence, 5 (3), pp. 246267.Vidal, V. (2004). lookahead strategy heuristic search planning. ProceedingsFourteenth International Conference Automated Planning Scheduling (ICAPS04), pp. 150159, Menlo Park, CA, USA. AAAI Press.Wilcoxon, F., & Wilcox, R. A. (1964). Rapid Approximate Statistical Procedures.American Cyanamid Co., Pearl River, NY, USA.231fiJournal Artificial Intelligence Research 25 (2006) 425456Submitted 12/04; published 4/06Logical Hidden Markov ModelsKristian KerstingLuc De Raedtkersting@informatik.uni-freiburg.dederaedt@informatik.uni-freiburg.deInstitute Computer ScienceAlbert-Ludwigs-Universitat FreiburgGeorges-Koehler-Allee 079D-79110 Freiburg, GermanyTapani Raikotapani.raiko@hut.fiLaboratory Computer Information ScienceHelsinki University TechnologyP.O. Box 5400FIN-02015 HUT, FinlandAbstractLogical hidden Markov models (LOHMMs) upgrade traditional hidden Markov modelsdeal sequences structured symbols form logical atoms, rather flatcharacters.note formally introduces LOHMMs presents solutions three central inference problems LOHMMs: evaluation, likely hidden state sequence parameter estimation. resulting representation algorithms experimentally evaluatedproblems domain bioinformatics.1. IntroductionHidden Markov models (HMMs) (Rabiner & Juang, 1986) extremely popular analyzing sequential data. Application areas include computational biology, user modelling,speech recognition, empirical natural language processing, robotics. Despite successes, HMMs major weakness: handle sequences flat, i.e., unstructured symbols. Yet, many applications symbols occurring sequences structured. Consider, e.g., sequences UNIX commands, may parametersemacs lohmms.tex, ls, latex lohmms.tex, . . .Thus, commands essentially structured.Tasks considered UNIX command sequences include predictionnext command sequence (Davison & Hirsh, 1998), classification commandsequence user category (Korvemaker & Greiner, 2000; Jacobs & Blockeel, 2001),anomaly detection (Lane, 1999). Traditional HMMs cannot easily deal typestructured sequences. Indeed, applying HMMs requires either 1) ignoring structurecommands (i.e., parameters), 2) taking possible parameters explicitlyaccount. former approach results serious information loss; latter leadscombinatorial explosion number symbols parameters HMMconsequence inhibits generalization.sketched problem HMMs akin problem dealing structured examples traditional machine learning algorithms studied fields inductive logic programming (Muggleton & De Raedt, 1994) multi-relational learnc2006AI Access Foundation. rights reserved.fiKersting, De Raedt, & Raikoing (Dzeroski & Lavrac, 2001). paper, propose (inductive) logic programmingframework, Logical HMMs (LOHMMs), upgrades HMMs deal structure.key idea underlying LOHMMs employ logical atoms structured (output state)symbols. Using logical atoms, UNIX command sequence representedemacs(lohmms.tex), ls, latex(lohmms.tex), . . . two important motivationsusing logical atoms symbol level. First, variables atoms allow one makeabstraction specific symbols. E.g., logical atom emacs(X, tex) represents files XLATEX user tex could edit using emacs. Second, unification allows one share information among states. E.g., sequence emacs(X, tex), latex(X, tex) denotesfile used argument Emacs LATEX.paper organized follows. reviewing logical preliminaries, introduceLOHMMs define semantics Section 3; Section 4, upgrade basicHMM inference algorithms use LOHMMs; investigate benefits LOHMMsSection 5: show LOHMMs strictly expressive HMMs,design order magnitude smaller corresponding propositionalinstantiations, unification yield models, better fit data. Section 6,empirically investigate benefits LOHMMs real world data. concluding,discuss related work Section 7. Proofs theorems found Appendix.2. Logical Preliminariesfirst-order alphabet set relation symbols r arity 0, written r/m,set functor symbols f arity n 0, written f/n. n = 0 f called constant,= 0 p called propositional variable. (We assume least one constantgiven.) atom r(t1 , . . . , tn ) relation symbol r followed bracketed n-tupleterms ti . term variable V functor symbol f(t1 , . . . , tk ) immediately followedbracketed k-tuple terms ti . Variables written upper-case, constant, functor predicate symbols lower-case. symbol denote anonymous variablesread treated distinct, new variables time encountered. iterativeclause formula form H B H (called head) B (called body) logicalatoms. substitution = {V1 /t1 , . . . , Vn /tn }, e.g. {X/tex}, assignment terms tivariables Vi . Applying substitution term, atom clause e yields instantiated term, atom, clause e occurrences variables V simultaneouslyreplaced term ti , e.g. ls(X) emacs(F, X){X/tex} yields ls(tex) emacs(F, tex).substitution called unifier finite set atoms singleton. unifiercalled general unifier (MGU) if, unifier S, existssubstitution = . term, atom clause E called ground containsvariables, i.e., vars(E) = . Herbrand base , denoted hb , setground atoms constructed predicate functor symbols . set G (A)atom consists ground atoms belong hb .3. Logical Hidden Markov Modelslogical component traditional HMM corresponds Mealy machine (Hopcroft& Ullman, 1979), i.e., finite state machine output symbols associated426fiLogical Hidden Markov Modelstransitions. essentially propositional representation symbols usedrepresent states output symbols flat, i.e. structured. key idea underlyingLOHMMs replace flat symbols abstract symbols. abstract symboldefinition logical atom. abstract represents set ground, i.e.,variable-free atoms alphabet , denoted G (A). Ground atoms playrole traditional symbols used HMMs.Example 1 Consider alphabet 1 constant symbols tex, dvi, hmm1,lohmm1, relation symbols emacs/2, ls/1, xdvi/1, latex/2. atomemacs(File, tex) represents set {emacs(hmm1, tex), emacs(lohmm1, tex)}. assumealphabet typed avoid useless instantiations emacs(tex, tex)).use atoms instead flat symbols allows us analyze logical structured sequencesemacs(hmm1, tex), latex(hmm1, tex), xdvi(hmm1, dvi).Definition 1 Abstract transition expressions form p : HB p [0, 1],H, B atoms. variables implicitly assumed universally quantified,i.e., scope variables single abstract transition.atoms H B represent abstract states represents abstract output symbol.semantics abstract transition p : HB one one statesG (B), say BB , one go probability p one states G (HB ), say HB H ,emitting symbol G (OB H ), say OB H .latex(File)Example 2 Consider c 0.8 : xdvi(File, dvi) latex(File, tex). generalH, B share predicate. due nature running example. Assume state latex(hmm1, tex), i.e.B = {File/hmm1}. c specifies probability 0.8 next stateG1 (xdvi(hmm1, dvi)) = {xdvi(hmm1, dvi)} ( i.e., probability 0.8next state xdvi(hmm1, dvi)), one symbols G 1 (latex(hmm1)) ={latex(hmm1)} ( i.e., latex(hmm1)) emitted. Abstract states might alsocomplex latex(file(FileStem, FileExtension), User)example simple H empty. situation becomes complicated substitutions empty. Then, resultingstate output symbol sets necessarily singletons. Indeed, transilatex(File)tion 0.8 : emacs(File0 , dvi) latex(File, tex) resulting state set wouldG1 (emacs(File0 , dvi)) = {emacs(hmm1, tex), emacs(lohmm1, tex)}. Thus transitionnon-deterministic two possible resulting states. therefore needmechanism assign probabilities possible alternatives.Definition 2 selection distribution specifies abstract state observationsymbol alphabet distribution ( | A) G (A).continue example, let (emacs(hmm1, tex) | emacs(File0 , tex)) = 0.4(emacs(lohmm1, tex) | emacs(File0 , tex)) = 0.6. would probability 0.4 0.8 = 0.32 next state emacs(hmm1, tex) 0.48emacs(lohmm1, tex).427fiKersting, De Raedt, & RaikoTaking account, meaning abstract transition p : HB summarized follows. Let BB G (B), HB H G (HB ) OB H G (OB H ).model makes transition state BB HB H emits symbol OB H probabilityp (HB H | HB ) (OB H | OB H ).(1)represent , probabilistic representation - principle - used, e.g. Bayesiannetwork Markov chain. Throughout remainder present paper, however,use nave Bayes approach. precisely, associate argumentr/mr/mrelation r/m finite domain Diconstants probability distribution Pir/mDi . Let vars(A) = {V1 , . . . , Vl } variables occurring atom r/m,let = {V1 /s1 , . . . Vl /sl } substitution grounding A. Vj consideredr/mrandom variable domain Darg(Vj ) argument arg(Vj ) appears first in. Then,Qr/m(A | A) = lj=1 Parg(Vj ) (sj ). E.g. (emacs(hmm1, tex) | emacs(F, E)), computedemacs/2emacs/2product P1(hmm1) P2(tex).Thus far semantics single abstract transition defined. LOHMMusually consists multiple abstract transitions creates complication.Example 3 Consideremacs(File)0.8 : latex(File, tex) emacs(File, tex)emacs(File)0.4 : dvi(File) emacs(File, User).two abstract transitions makeconflicting statements state resulting emacs(hmm1, tex). Indeed, accordingfirst transition, probability 0.8 resulting state latex(hmm1, tex)according second one assigns 0.4 xdvi(hmm1).essentially two ways deal situation. one hand, one might wantcombine normalize two transitions assign probability 23 respectively 13 .hand, one might want one rule firing. paper, choselatter option allows us consider transitions independently, simplifieslearning, yields locally interpretable models. employ subsumption (or generality) relation among B-parts two abstract transitions. Indeed, B-partfirst transition B1 = emacs(File, tex) specific second transition B2 = emacs(File, User) exists substitution = {User/tex}B2 = B1 , i.e., B2 subsumes B1 . Therefore G1 (B1 ) G1 (B2 ) first transitionregarded informative second one. therefore preferredsecond one starting emacs(hmm1, tex). also say first transition specific second one. Remark generality relation imposespartial order set transitions. considerations lead strategyconsidering maximally specific transitions apply state order determinesuccessor states. implements kind exception handling default reasoningakin Katzs (1987) back-off n-gram models. back-off n-gram models,detailed model deemed provide sufficiently reliable information currentcontext used. is, one encounters n-gram sufficiently reliable,back-off use (n 1)-gram; reliable either back-off level n 2, etc.conflict resolution strategy work properly provided bodies maximally specific transitions (matching given state) represent abstract state.428fiLogical Hidden Markov Modelsstartls : 0.40.450.55emacs(F) : 0.7ls(U0 )emacs(F, U)emacs(F) : 0.3ls : 0.6emacs(F) : 0.3emacs(F0 , U)latex(F) : 0.2emacs(F) : 0.1latex(F) : 0.2latex(F, tex)emacs(F, tex)emacs(F) : 0.6latex(F) : 0.6Figure 1: logical hidden Markov model.enforced requiring generality relation B-parts closedgreatest lower bound (glb) predicate, i.e., pair B1 , B2 bodies,= mgu(B1 , B2 ) exists, another body B (called lower bound) subsumes B1(therefore also B2 ) subsumed B1 , B2 , lower boundsubsumed B. E.g., body second abstract transition exampleemacs(hmm1, User) set abstract transitions would closed glb.Finally, order specify prior distribution states, assume finite setclauses form p : H start using distinguished start symbol pprobability LOHMM start state G (H).able formally define logical hidden Markov models.Definition 3 logical hidden Markov model (LOHMM) tuple (, , , )logical alphabet, selection probability , set abstract transitions,set abstract transitions encoding prior distribution. Let B set atomsoccur body parts transitions . assume B closed glb requireXB B :p = 1.0(2)p:HBprobabilities p clauses sum 1.0 .HMMs special cases LOHMMs contains relation symbols arityzero selection probability irrelevant. Thus, LOHMMs directly generalize HMMs.LOHMMs also represented graphically. Figure 1 contains example. underlying language 2 consists 1 together constant symbol denotesuser employ LATEX. graphical notation, nodes represent abstract statesblack tipped arrows denote abstract transitions. White tipped arrows used represent meta knowledge. precisely, white tipped, dashed arrows represent generalitysubsumption ordering abstract states. follow transition abstract stateoutgoing white tipped, dotted arrow dotted arrow always followed.Dotted arrows needed abstract state occur different cirlatex(File)cumstances. Consider transition p : latex(File0 , User0 ) latex(File, User).429fiKersting, De Raedt, & Raiko0.6start0.45em(F, U)em(F, t)stateabstract stateabstract state0.4lsls(t)em(f1 )em(f1 , t)1.0la(F, t)la(f1 , t)abstract statestatels(U0 )em(f2 )state abstract state0.6la(f1 )0.7em(f2 , o)stateem(F, U)abstract stateem(F0 , U)abstract stateFigure 2: Generatingobservationsequenceemacs(hmm1), latex(hmm1),emacs(lohmm1), ls LOHMM Figure 1. command emacsabbreviated em, f1 denotes filename hmm1, f2 represents lohmm1, denotestex user, user. White tipped solid arrows indicate selections.Even though atoms head body transition syntactically differentrepresent abstract state. accurately represent meaning transitioncannot use black tipped arrow latex(File, User) itself, would actulatex(File)ally represent abstract transition p : latex(File, User) latex(File, User).Furthermore, graphical representation clarifies LOHMMs generative models. Let us explain model Figure 1 would generate observation sequenceemacs(hmm1), latex(hmm1), emacs(lohmm1), ls (cf. Figure 2). chooses initial abstract state, say emacs(F, U). Since variables F U uninstantiated, modelsamples state emacs(hmm1, tex) G2 using . indicated dashed arrow, emacs(F, tex) specific emacs(F, U). Moreover, emacs(hmm1, tex) matchesemacs(F, tex). Thus, model enters emacs(F, tex). Since value F alreadyinstantiated previous abstract state, emacs(hmm1, tex) sampled probability1.0. Now, model goes latex(F, tex), emitting emacs(hmm1) abstractobservation emacs(F) already fully instantiated. Again, since F already instantiated,latex(hmm1, tex) sampled probability 1.0. Next, move emacs(F 0 , U), emitting latex(hmm1). Variables F0 U emacs(F0 , U) yet bound; so, values, saylohmm1 others, sampled . dotted arrow brings us back emacs(F, U).variables implicitly universally quantified abstract transitions, scopevariables restricted single abstract transitions. turn, F treated distinct,new variable, automatically unified F0 , bound lohmm1. contrast,variable U already instantiated. Emitting emacs(lohmm1), model makes transitionls(U0 ). Assume samples tex U0 . Then, remains ls(U0 ) probability0.4 . Considering possible samples, allows one prove following theorem.Theorem 1 (Semantics) logical hidden Markov model language definesdiscrete time stochastic process, i.e., sequence random variables hX it=1,2,... ,domainXt hb() hb(). induced probability measure Cartesian productNhb() hb() exists unique > 0 limit .concluding section, let us address design choices underlying LOHMMs.First, LOHMMs introduced Mealy machines, i.e., output symbolsassociated transitions. Mealy machines fit logical setting quite intuitivelydirectly encode conditional probability P (O, S0 |S) making transition S0430fiLogical Hidden Markov Modelsemitting observation O. Logical hidden Markov models define distributionXP (O, S0 |S) =p (S0 | HB ) (O | O0 B H )O0p:HBO0sum runs abstract transitions H B B specific S.Observations correspond (partially) observed proof steps and, hence, provide informationshared among heads bodies abstract transitions. contrast, HMMs usuallyintroduced Moore machines. Here, output symbols associated states implicitlyassuming S0 independent. Thus, P (O, S0 | S) factorizes P (O | S) P (S0 | S).makes difficult observe information shared among heads bodies.turn, Moore-LOHMMs less intuitive harder understand. detaileddiscussion issue, refer Appendix B essentially showpropositional case Mealy- Moore-LOHMMs equivalent.Second, nave Bayes approach selection distribution reduces model complexity expense lower expressivity: functors neglected variablestreated independently. Adapting expressive approaches interesting future lineresearch. instance, Bayesian networks allow one represent factorial HMMs (Ghahramani & Jordan, 1997). Factorial HMMs viewed LOHMMs, hiddenstates summarized 2 k-ary abstract state. first k arguments encode kstate variables, last k arguments serve memory previous joint state.i-th argument conditioned + k-th argument. Markov chains allow onesample compound terms finite depth s(s(s(0))) model e.g. misspelledfilenames. akin generalized HMMs (Kulp, Haussler, Reese, & Eeckman, 1996),node may output finite sequence symbols rather single symbol.Finally, LOHMMs introduced present paper specify probability distribution sequences given length. Reconsider LOHMM Figure 1. Already probabilities observation sequences length 1, i.e., ls, emacs(hmm1),Pemacs(lohmm1)) sum 1. precisely, > 0 holds x1 ,...,xt P (X1 =x1 , . .P. , Xt P= xt ) = 1.0 . order model distribution sequences variable length,i.e., t>0 x1 ,...,xt P (X1 = x1 , . . . , Xt = xt ) = 1.0 may add distinguished end state.end state absorbing whenever model makes transition state,terminates observation sequence generated.4. Three Inference Problems LOHMMsHMMs, three inference problems interest. Let LOHMM let= O1 , O2 , . . . , OT , > 0, finite sequence ground observations:(1) Evaluation: Determine probability P (O | ) sequence generatedmodel .(2) likely state sequence: Determine hidden state sequencelikely produced observation sequence O, i.e. = arg maxS P (S | O, ) .(3) Parameter estimation: Given set = {O1 , . . . , Ok } observation sequences, determine likely parameters abstract transitions selection| ) .distribution , i.e. = arg max P (O431fiPSfrag replacementsKersting, De Raedt, & Raikosc(1)abstract selection abstracttransitiontransitionselectionabstract selectionsc(2)transitionsc(Y)ls(o)ls(o)ls(o)ls(U)ls(t)hc(1)ls(o)ls(t)ls(t)hc(2)ls(t)ls(U)starthc(X)...ls(U)em(f2,o)sc(Z)em(f1,o)em(F,U)em(f1,t)em(F,o)em(f2,t)latex(f1,t)latex(f1,t) latex(f2,t)em(F,U)O1em(F,U)O2O2abstract stateS2S1S0em(F,o)statesFigure 3: Trellis induced LOHMM Figure 1. sets reachable states time0, 1, . . . denoted S0 , S1 , . . . contrast HMMs, additionallayer states sampled abstract states.address problems turn upgrading existing solutionsHMMs. realized computing grounded trellis Figure 3. possibleground successor states given state computed first selecting applicableabstract transitions applying selection probabilities (while taking accountsubstitutions) ground resulting states. two-step factorization coalescedone step HMMs.evaluate O, consider probability partial observation sequence 1 , O2 , . . . , Ot(ground) state time t, 0 < , given model = (, , , )(S) := P (O1 , O2 , . . . , Ot , qt = | )qt = denotes system state time t. HMMs, (S) computed using dynamic programming approach. = 0, set 0 (S) = P (q0 = | ) ,i.e., 0 (S) probability starting state and, > 0, compute (S) basedt1 (S0 ):1: S0 := {start}2: = 1, 2, . . . ,3:St =4:foreach St15:6:7:8:9:/* initialize set reachable states*//* initialize set reachable states clock t*/foreach maximally specific p : HB s.t. B = mgu(S, B) existsforeach S0 = HB H G (HB ) s.t. Ot1 unifies OB HS0 6 StSt := St {S0 }(S0 ) := 0.0(S0 ) := (S0 ) + t1 (S) pP11: return P (O | ) = SST (S)10:432(S0 | HB ) (Ot1 | OB H )fiLogical Hidden Markov Modelsassume sake simplicity start abstract transition p : Hstart . Furthermore, boxed parts specify differences HMM formula:unification taken account.PClearly, HMMs P (O | ) = SST (S) holds. computational complexityforward procedure O(T (|B| + g)) = O(T s2 ) = maxt=1,2,...,T |St | ,maximal number outgoing abstract transitions regard abstract state,g maximal number ground instances abstract state. completelyanalogous manner, one devise backward procedure compute(S) = P (Ot+1 , Ot+2 , . . . , OT | qt = S, ) .useful solving Problem (3).forward procedure, straightforward adapt Viterbi algorithmsolution Problem (2), i.e., computing likely state sequence. Let (S)denote highest probability along single path time accounts firstobservations ends state S, i.e.,(S) =maxS0 ,S1 ,...,St1P (S0 , S1 , . . . , St1 , St = S, O1 , . . . , Ot1 |M ) .procedure finding likely state sequence basically follows forward procedure. Instead summing ground transition probabilities line 10, maximizethem. precisely, proceed follows:1:S0 := {start}/* initialize set reachable states*/2: = 1, 2, . . . ,3:St =/* initialize set reachable states clock t*/foreach St14:5:foreach maximally specific p : HB s.t. B = mgu(S, B) existsforeach S0 = HB H G (HB ) s.t. Ot1 unifies OB H6:S0 6 St7:8:St := St {S0 }(S, S0 ) := 0.09:10:(S, S0 ) := (S, S0 ) + t1 (S) p (S0 | HB ) (Ot1 | OB H )11:foreach S0 St12:(S0 ) = maxSSt1 (S, S0 )13:(S0 ) = arg maxSSt1 (S, S0 )Here, (S, S0 ) stores probability making transition S0 (S0 ) (with1 (S) = start states S) keeps track state maximizing probability alongsingle path time accounts first observations ends state 0 .likely hidden state sequence computedST +1 = arg max +1 (S)St=SST +1(St+1 )= T, 1, . . . , 1 .One also consider problem (2) abstract level. Instead consideringcontributions different abstract transitions single ground transition state433fiKersting, De Raedt, & Raikostate S0 line 10, one might also consider likely abstract transition only.realized replacing line 10 forward procedure(S0 ) := max(t (S0 ), t1 (S) p (S0 | HB ) (Ot1 | OB H )) .solves problem finding (20 ) likely state abstract transitionsequence:Determine sequence states abstract transitions GT =S0 , T0 , S1 , T1 , S2 , . . . , ST , TT , ST+1 exists substitutions Si+1Si Ti likely produced observation sequence O, i.e.GT = arg maxGT P (GT | O, ) .Thus, logical hidden Markov models also pose new types inference problems.parameter estimation, estimate maximum likelihood transitionprobabilities selection distributions. estimate former, upgrade well-knownBaum-Welch algorithm (Baum, 1972) estimating maximum likelihood parametersHMMs probabilistic context-free grammars.HMMs, Baum-Welch algorithm computes improved estimate p tranOsition probability (ground) transition p : HB taking ratiop= P(T)H0O0B(T0 )(3)expected number (T) times making transitions time givenmodel observation sequence O, total number times transitionsmade B time given O.Basically applies abstract transition. However,little bit careful direct access (T). Let (gcl, T)GOprobability following abstract transition via ground instance gcl p : GH GBtime t, i.e.,(gcl, T) =(GB) p t+1 (GH)(GH | HB ) (Ot1 | OB H ) ,P (O | )(4)B , H forward procedure (see above) P (O | ) probabilitymodel generated sequence O. Again, boxed terms constitute maindifference corresponding HMM formula. order apply Equation (3) computeimproved estimates probabilities associated abstract transitions, set(T) =Xt=1(T) =XX(gcl, T)t=1 gclinner sum runs ground instances T.leads following re-estimation method, assume setsreachable states reused computations - -values:434fiLogical Hidden Markov Models1:2:3:4:5:6:7:8:9:/* initialization expected counts */foreach(T) := /* 0 using pseudocounts *//* compute expected counts */= 0, 1, . . . ,foreach Stforeach max. specific p : HB s.t. B = mgu(S, B) existsforeach S0 = HB H G (HB ) s.t. S0 St+1 mgu(Ot , OB H ) exists(T) := (T) + (S) p t+1 (S0 ) P (O | ) (S0 | HB ) (Ot1 | OB H )Here, equation (4) found line 9. line 3, set pseudocounts small samplesize regularizers. methods avoid biased underestimate probabilities evenzero probabilities m-estimates (see e.g., Mitchell, 1997) easily adapted.estimate selection probabilities, recall follows nave Bayes scheme. Therefore, estimated probability domain element domain rationumber times selected number times d0 selected.procedure computing -values thus reused.Altogether, Baum-Welch algorithm works follows: converged, (1) estimate abstract transition probabilities, (2) selection probabilities. Sinceinstance EM algorithm, increases likelihood data every update,according McLachlan Krishnan (1997), guaranteed reach stationarypoint. standard techniques overcome limitations EM algorithms applicable.computational complexity (per iteration) O(k ( + d)) = O(k s2 + k d)k number sequences, complexity computing -values (see above),sum sizes domains associated predicates. Recently, KerstingRaiko (2005) combined Baum-Welch algorithm structure search modelselection logical hidden Markov models using inductive logic programming (Muggleton& De Raedt, 1994) refinement operators. refinement operators account differentabstraction levels explored.5. Advantages LOHMMssection, investigate benefits LOHMMs: (1) LOHMMs strictlyexpressive HMMs, (2), using abstraction, logical variables unificationbeneficial. specifically, (2), show(B1) LOHMMs design smaller propositional instantiations,(B2) unification yield better log-likelihood estimates.5.1 Expressivity LOHMMsWhereas HMMs specify probability distributions regular languages, LOHMMs specifyprobability distributions expressive languages.435fiKersting, De Raedt, & RaikoTheorem 2 (consistent) probabilistic context-free grammar (PCFG) Glanguage L exists LOHMM s.t. PG (w) = PM (w) w L.proof (see Appendix C) makes use abstract states unbounded depth.precisely, functors used implement stack. Without functors, LOHMMs cannotencode PCFGs and, Herbrand base finite, proven alwaysexists equivalent HMM.Furthermore, functors allowed, LOHMMs strictly expressive PCFGs.specify probability distributions languages context-sensitive:1.0 :stack(s(0), s(0))0.8 :stack(s(X), s(X))0.2 : unstack(s(X), s(X))b1.0 :unstack(X, Y)c1.0 :unstack(s(0), Y)end1.0 :endstartstack(X, X)stack(X, X)unstack(s(X), Y)unstack(s(0), s(Y))unstack(s(0), s(0))LOHMM defines distribution {an bn cn | n > 0}.Finally, use logical variables also enables one deal identifiers. Identifiersspecial types constants denote objects. Indeed, recall UNIX commandsequence emacs lohmms.tex, ls, latex lohmms.tex, . . . introduction. filenamelohmms.tex identifier. Usually, specific identifiers matter ratherfact object occurs multiple times sequence. LOHMMs easily dealidentifiers setting selection probability constant argumentsidentifiers occur. Unification takes care necessary variable bindings.5.2 Benefits Abstraction Variables UnificationReconsider domain UNIX command sequences. Unix users oftenly reuse newly created directory subsequent commands mkdir(vt100x), cd(vt100x), ls(vt100x) .Unification allow us elegantly employ information allows us specify that, observing created directory, model makes transition statenewly created directory used:p1 : cd(Dir, mkdir) mkdir(Dir, com)p2 : cd( , mkdir) mkdir(Dir, com)first transition followed, cd command move newly created directory;second transition followed, specified directory cd move to. Thus,LOHMM captures reuse created directories argument future commands.Moreover, LOHMM encodes simplest possible case show benefits unification. time, observation sequence uniquely determines state sequence,functors used. Therefore, left abstract output symbols associatedabstract transitions. total, LOHMM U , modelling reuse directories, consists542 parameters still covers 451000 (ground) states, see Appendixcomplete model. compression number parameters supports (B1).empirically investigate benefits unification, compare U variant NU variables shared, i.e., unification used instance436fiLogical Hidden Markov Modelsfirst transition allowed, see Appendix D. N 164 parameters less U .computed following zero-one win function(1 log PU (O) log PN (O) > 0f (O) =0 otherwiseleave-one-out cross-validated Unix shell logs collected Greenberg (1988). Overall,data consists 168 users four groups: computer scientists, nonprogrammers, novicesothers. 300000 commands logged average 110 sessionsper user. present results subset data. considered computerscientist sessions least single mkdir command appears. yield 283 logicalsequences total 3286 ground atoms. LOO win 81.63%. LOO statisticsalso favor U :UNtrainingO)O) log PPU (Olog P (OO)N (O11361.01795.313157.0testlog P (O) log PPNU (O)(O)42.87.9150.7Thus, although U 164 parameters N , shows better generalization performance. result supports (B2). pattern often found U 10.15 : cd(Dir, mkdir) mkdir(Dir, com)0.08 : cd( , mkdir) mkdir(Dir, com)favoring changing directory made. knowledge cannot captured N0.25 : cd( , mkdir) mkdir(Dir, com).results clearly show abstraction variables unification beneficialapplications, i.e., (B1) (B2) hold.6. Real World Applicationsintentions investigate whether LOHMMs applied real worlddomains. precisely, investigate whether benefits (B1) (B2) alsoexploited real world application domains. Additionally, investigate whether(B3) LOHMMs competitive ILP algorithms also utilize unificationabstraction variables,(B4) LOHMMs handle tree-structured data similar PCFGs.aim, conducted experiments two bioinformatics application domains: proteinfold recognition (Kersting, Raiko, Kramer, & De Raedt, 2003) mRNA signal structuredetection (Horvath, Wrobel, & Bohnebeck, 2001). application domains multiclassproblems five different classes each.1. sum probabilities (0.15 + 0.08 = 0.23 6= 0.25) use pseudo countssubliminal non-determinism (w.r.t. abstract states) U , i.e., case firsttransition fires, second one also fires.437fiKersting, De Raedt, & Raiko6.1 Methodologyorder tackle multiclass problem LOHMMs, followed plug-in estimateapproach. Let {c1 , c2 , . . . , ck } set possible classes. Given finite set trainingexamples {(xi , yi )}ni=1 X {c1 , c2 , . . . , cn }, one tries find f : X {c1 , c2 , . . . , ck }f (x) = argmaxc{c1 ,c2 ,...,ck }P (x | M, c ) P (c) .(5)low approximation error training data well unseen examples.Equation (5), denotes model structure classes, c denotesmaximum likelihood parameters class c estimated training examplesyi = c only, P (c) prior class distribution.implemented Baum-Welch algorithm (with pseudocounts m, see line 3) maximum likelihood parameter estimation using Prolog system Yap-4.4.4. experiments,set = 1 let Baum-Welch algorithm stop change log-likelihoodless 0.1 one iteration next. experiments ran Pentium-IV3.2 GHz Linux machine.6.2 Protein Fold RecognitionProtein fold recognition concerned proteins fold nature, i.e., threedimensional structures. important problem biological functions proteinsdepend way fold. common approach use database searches find proteins (of known fold) similar newly discovered protein (of unknown fold). facilitateprotein fold recognition, several expert-based classification schemes proteinsdeveloped group current set known protein structures according similarityfolds. instance, structural classification proteins (Hubbard, Murzin, Brenner, & Chotia, 1997) (SCOP) database hierarchically organizes proteins accordingstructures evolutionary origin. machine learning perspective, SCOP inducesclassification problem: given protein unknown fold, assign best matching groupclassification scheme. protein fold classification problem investigatedTurcotte, Muggleton, Sternberg (2001) based inductive logic programming(ILP) system PROGOL Kersting et al. (2003) based LOHMMs.secondary structure protein domains2 elegantly represented logical sequences. example, secondary structure Ribosomal protein L4 representedst(null, 2), he(right, alpha, 6), st(plus, 2), he(right, alpha, 4), st(plus, 2),he(right, alpha, 4), st(plus, 3), he(right, alpha, 4), st(plus, 1), he(hright, alpha, 6)Helices certain type, orientation length he(HelixType, HelixOrientation, Length),strands certain orientation length st(StrandOrientation, Length) atomslogical predicates. application traditional HMMs sequences requires oneeither ignore structure helices strands, results loss information,take possible combinations (of arguments orientation length) account,leads combinatorial explosion number parameters2. domain viewed sub-section protein appears number distantly relatedproteins fold independently rest protein.438fiLogical Hidden Markov ModelsendBlock B length 3Block s(B) length 2Dynamics within blockDynamics within blockblock(B, s(P))block(s(B), s(P))block(B, P)block(s(B), P)Transition next blockTransition next blockblock(s(B), s(0))block(B, s(s(s(0))))block(B, 0)block(s(B), 0)Figure 4: Scheme left-to-right LOHMM block model.results reported Kersting et al. (2003) indicate LOHMMs well-suitedprotein fold classification: number parameters LOHMM ordermagnitude smaller number corresponding HMM (120 versus approximately62000) generalization performance, 74% accuracy, comparable Turcotteet al.s (2001) result based ILP system Progol, 75% accuracy. Kersting et al.(2003), however, cross-validate results investigate commonbioinformatics impact primary sequence similarity classification accuracy.instance, two commonly requested ASTRAL subsets subset sequencesless 95% identity (95 cut) less 40% identity(40 cut). Motivated this, conducted following new experiments.data consists logical sequences secondary structure protein domains.work Kersting et al. (2003), task predict one five populatedSCOP folds alpha beta proteins (a/b): TIM beta/alpha-barrel (fold 1), NAD(P)binding Rossmann-fold domains (fold 2), Ribosomal protein L4 (fold 23), Cysteine hydrolase(fold 37), Phosphotyrosine protein phosphatases I-like (fold 55). class a/bproteins consists proteins mainly parallel beta sheets (beta-alpha-beta units).data extracted automatically ASTRAL dataset version 1.65 (Chandonia,Hon, Walker, Lo Conte, P.Koehl, & Brenner, 2004) 95 cut 40 cut.work Kersting et al. (2003), consider strands helices only, i.e., coilsisolated strands discarded. 95 cut, yields 816 logical sequences consistingtotal 22210 ground atoms. number sequences classes listed 293,151, 87, 195, 90. 40 cut, yields 523 logical sequences consisting total14986 ground atoms. number sequences classes listed 182, 100, 66, 122,53.LOHMM structure: used LOHMM structure follows left-to-right block topology,see Figure 4, model blocks consecutive helices (resp. strands). Blocksize s, say 3, model remain block = 3 time steps. similaridea used model haplotypes (Koivisto, Perola, Varilo, Hennah, Ekelund, Lukk,Peltonen, Ukkonen, & Mannila, 2002; Koivisto, Kivioja, Mannila, Rastas, & Ukkonen,2004). contrast common HMM block models (Won, Prugel-Bennett, & Krogh, 2004),439fiKersting, De Raedt, & Raikotransition parameters shared within block one ensure modelmakes transition next state s(Block ) end block; exampleexactly 3 intra-block transitions. Furthermore, specific abstract transitionshelix types strand orientations model priori distribution, intra-inter-block transitions. number blocks sizes chosen accordingempirical distribution sequence lengths data beginningending protein domains likely captured detail. yield following blockstructure1 2...19 2027 28...40 4146 4761 6276 77numbers denote positions within protein domains. Furthermore, notelast block gathers remaining transitions. blocks modelled usinghidden abstract stateshc(HelixType, HelixOrientation, Length, Block ) sc(StrandOrientation, Length, Block ) .Here, Length denotes number consecutive bases structure element consists of.length discretized 10 bins original lengths uniformallydistributed. total, LOHMM 295 parameters. corresponding HMM withoutparameter sharing 65200 parameters. clearly confirms (B1).Results: performed 10-fold cross-validation. 95 cut dataset, accuracy76% took approx. 25 minutes per cross-validation iteration; 40 cut, accuracy73% took approx. 12 minutes per cross-validation iteration. results validateKersting et al.s (2003) results and, turn, clearly show (B3) holds. Moreover,novel results 40 cut dataset indicate similarities detected LOHMMsprotein domain structures accompanied high sequence similarity.6.3 mRNA Signal Structure DetectionmRNA sequences consist bases (guanine, adenine, uracil, cytosine) fold intramolecularly form number short base-paired stems (Durbin, Eddy, Krogh, & Mitchison,1998). base-paired structure called secondary structure, cf. Figures 5 6.secondary structure contains special subsequences called signal structures responsible special biological functions, RNA-protein interactions cellular transport.function signal structure class based common characteristic bindingsite class elements. elements necessarily identical similar.vary topology (tree structure), size (number constituting bases), basesequence.goal experiments recognize instances signal structures classesmRNA molecules. first application relational learning recognize signal structure class mRNA molecules described works Bohnebeck, Horvath,Wrobel (1998) Horvath et al. (2001), relational instance-based learnerRIBL applied. dataset 3 used similar one described Horvath3. dataset described work Horvath et al. (2001) could obtainoriginal dataset. compare smaller data set used Horvath et al., consisted440fiLogical Hidden Markov Modelset al. (2001). consisted 93 mRNA secondary structure sequences. precisely,composed 15 5 SECIS (Selenocysteine Insertion Sequence), 27 IRE (Iron ResponsiveElement), 36 TAR (Trans Activating Region) 10 histone stem loops constituting fiveclasses.secondary structure composed different building blocks stacking region,hairpin loops, interior loops etc. contrast secondary structure proteins formschains, secondary structure mRNA forms tree. trees easily handledusing HMMs, mRNA secondary structure data challenging proteins.Moreover, Horvath et al. (2001) report making tree structure available RIBLbackground knowledge influence classification accuracy. precisely,using simple chain representation RIBL achieved 77.2% leave-one-out cross-validation(LOO) accuracy whereas using tree structure background knowledge RIBL achieved95.4% LOO accuracy.followed Horvath et al.s experimental setup, is, adapted data representations LOHMMs compared chain model tree model.Chain Representation: chain representation (see also Figure 5),signalstructuresdescribedsingle(TypeSingle, Position, Acid )helical(TypeHelical , Position, Acid , Acid ).Depending type, structure element represented either single/3 helical/4.first argumentTypeSingle (resp.TypeHelical ) specifies type structure element, i.e.,single, bulge3, bulge5, hairpin (resp. stem). argument Position position sequence element within corresponding structure element counted down,i.e.4 , {n13 (0), n12 (0), . . . , n1 (0)}. maximal position set 13maximal position observed data. last argument encodes observed nucleotide(pair).used LOHMM structure follows left-to-right block structure shownFigure 4. underlying idea model blocks consecutive helical structure elements. hidden states modelled using single(TypeSingle, Position, Acid , Block )helical(TypeHelical , Position, Acid , Acid , Block ). Block consecutive helical (resp. single) structure elements, model remain Block transitionsingle element. transition single (resp. helical) element occurs Positionn(0). positions n(Position), transitions helical (resp. single)structure elements helical (resp. single) structure elements Position capturing dynamics nucleotide pairs (resp. nucleotides) within structure elements. instance,66 signal structures close data set. larger data set (with 400 structures) Horvathet al. report error rate 3.8% .4. nm (0) shorthand recursive application functor n 0 times, i.e., position m.441fiKersting, De Raedt, & Raikohelical(stem, n(0), c, g).helical(stem, n(n(0)), c, g).helical(stem, n(n(n(0))), c, g).single(bulge5, n(0), a).single(bulge5, n(n(0)), a).single(bulge5, n(n(n(0))), g).helical(stem, n(0), c, g).helical(stem, n(n(0)), c, g).single(bulge5, n(0), a).helical(stem, n(0), a, a).helical(stem, n(n(0)), u, a).helical(stem, n(n(n(0))), u, g).helical(stem, n(n(n(n(0)))), u, a).helical(stem, n(n(n(n(n(0))))), c, a).helical(stem, n(n(n(n(n(n(0)))))), u, a).helical(stem, n(n(n(n(n(n(n(0))))))), a, u).uucccggggccsingle(hairpin, n(n(n(0))), a).single(hairpin, n(n(0)), u).single(hairpin, n(0), u).single(bulge3, n(0), a).gguuucuguFigure 5: chain representation SECIS signal structure. ground atomsordered clockwise starting helical(stem, n(n(n(n(n(n(n(0))))))), a, u)lower left-hand side corner.transitions block n(0) position n(n(0))pa :he(stem,n(0),X,Y): he(stem, n(0), X, Y, n(0)) he(stem, n(n(0)), X, Y, n(0)))pb :he(stem,n(0),X,Y)b:he(stem, n(0), Y, X, n(0)) he(stem, n(n(0)), X, Y, n(0)))c:he(stem, n(0), X, , n(0)) he(stem, n(n(0)), X, Y, n(0)))d:he(stem, n(0), , Y, n(0)) he(stem, n(n(0)), X, Y, n(0)))e:he(stem, n(0), , , n(0)) he(stem, n(n(0)), X, Y, n(0)))pc :he(stem,n(0),X,Y)pd :he(stem,n(0),X,Y)pe :he(stem,n(0),X,Y)total, 5 possible blocks maximal number blocks consecutivehelical structure elements observed data. Overall, LOHMM 702 parameters.contrast, corresponding HMM 16600 transitions validating (B1).Results: LOO test log-likelihood 63.7, EM iteration took average26 seconds.Without unification-based transitions b-d, i.e., using abstract transitionspa :he(stem,n(0),X,Y): he(stem, n(0), X, Y, n(0)) he(stem, n(n(0)), X, Y, n(0)))e:pe :he(stem,n(0),X,Y)he(stem, n(0), , , n(0)) he(stem, n(n(0)), X, Y, n(0))),model 506 parameters. LOO test log-likelihood 64.21, EM iteration took average 20 seconds. difference LOO test log-likelihood statisticallysignificant (paired t-test, p = 0.01).Omitting even transition a, LOO test log-likelihood dropped 66.06,average time per EM iteration 18 seconds. model 341 parameters.difference average LOO log-likelihood statistically significant (paired t-test, p = 0.001).results clearly show unification yield better LOO test log-likelihoods, i.e.,(B2) holds.442fiLogical Hidden Markov Modelsnucleotide pair((c, g)).nucleotide pair((c, g)).nucleotide pair((c, g)).helical(s(s(s(s(s(0))))), s(s(s(0))), [c], stem, n(n(n(0)))).nucleotide(a).nucleotide(a).nucleotide(g).single(s(s(s(s(0)))), s(s(s(0))), [], bulge5, n(n(n(0)))).nucleotide pair((c, g)).nucleotide pair((c, g)).helical(s(s(s(0))), s(0), [c, c, c], stem, n(n(0))).nucleotide(a).single(s(s(0)), s(0), [], bulge5, n(0)).nucleotide pair((a, a)).nucleotide pair((u, a)).nucleotide pair((u, g)).nucleotide pair((u, a)).nucleotide pair((c, a)).nucleotide pair((u, a)).nucleotide pair((a, u)).helical(s(0), 0, [c, c], stem, n(n(n(n(n(n(n(0)))))))).uucccggggccggsingle(s(s(s(s(s(s(0)))))), s(s(s(s(s(0))))),[], hairpin, n(n(n(0)))).nucleotide(a).nucleotide(u).nucleotide(u).single(s(s(s(s(s(s(s(0))))))), s(s(s(0))),[], bulge3, n(0)).nucleotide(a).uuucugu0s(0)s(s(0))s(s(s(0)))s(s(s(s(0))))s(s(s(s(s(s(s(0)))))))s(s(s(s(s(0))))s(s(s(s(s(s(0))))))root(0, root, [c]).Figure 6: tree representation SECIS signal structure. (a) logical sequence,i.e., sequence ground atoms representing SECIS signal structure.ground atoms ordered clockwise starting root(0, root, [c]) lowerleft-hand side corner. (b) tree formed secondary structure elements.Tree Representation: tree representation (see Figure 6 (a)), idea capturetree structure formed secondary structure elements, see Figure 6 (b).training instance described sequence ground factsroot(0, root, #Children),helical(ID, ParentID, #Children, Type, Size),nucleotide pair(BasePair ),single(ID, ParentID, #Children, Type, Size),nucleotide(Base) .Here, ID ParentID natural numbers 0, s(0), s(s(0)), . . . encoding childparent relation, #Children denotes number5 children [], [c], [c, c], . . ., Typetype structure element stem, hairpin, . . ., Size natural number0, n(0), n(n(0)), . . . Atoms root(0, root, #Children) used root topology.maximal #Children 9 maximal Size 13 maximal valueobserved data.trees easily handled using HMMs, used LOHMM basicallyencodes PCFG. Due Theorem 2, possible. used LOHMM structurefound Appendix E. processes mRNA trees in-order. Unification usedparsing tree. chain representation, used Position argument hiddenstates encode dynamics nucleotides (nucleotide pairs) within secondary structure5. Here, use Prolog short hand notation [] lists. list either constant [] representingempty list, compound term functor ./2 two arguments, respectively headtail list. Thus [a, b, c] compound term .(a, .(b, .(c, []))).443fiKersting, De Raedt, & Raikoelements. maximal Position 13. contrast chain representation,nucleotide pairs (a, u) treated constants. Thus, argument BasePairconsists 16 elements.Results: LOO test log-likelihood 55.56. Thus, exploiting tree structureyields better probabilistic models. average, EM iteration took 14 seconds. Overall,result shows (B4) holds.Although Baum-Welch algorithm attempts maximize different objective function, namely likelihood data, interesting compare LOHMMs RIBLterms classification accuracy.Classification Accuracy: chain representation, LOO accuraciesLOHMMs 99% (92/93). considerable improvement RIBLs 77.2% (51/66)LOO accuracy representation. tree representation, LOHMM alsoachieved LOO accuracy 99% (92/93). comparable RIBLs LOO accuracy97% (64/66) kind representation.Thus, already chain LOHMMs show marked increases LOO accuracy compared RIBL (Horvath et al., 2001). order achieve similar LOO accuracies, Horvathet al. (2001) make tree structure available RIBL background knowledge.LOHMMs, significant influence LOO test log-likelihood,LOO accuracies. clearly supports (B3). Moreover, according Horvath et al.,mRNA application also considered success terms application domain,although primary goal experiments. exist also alternativeparameter estimation techniques models, covariance models (Eddy &Durbin, 1994) pair hidden Markov models (Sakakibara, 2003), mightused well basis comparison. However, LOHMMs employ (inductive) logic programming principles, appropriate compare systems within paradigmRIBL.7. Related WorkLOHMMs combine two different research directions. one hand, relatedseveral extensions HMMs probabilistic grammars. hand, alsorelated recent interest combining inductive logic programming principlesprobability theory (De Raedt & Kersting, 2003, 2004).first type approaches, underlying idea upgrade HMMs probabilisticgrammars represent structured state spaces.Hierarchical HMMs (Fine, Singer, & Tishby, 1998), factorial HMMs (Ghahramani &Jordan, 1997), HMMs based tree automata (Frasconi, Soda, & Vullo, 2002) decompose state variables smaller units. hierarchical HMMs statesHMMs, factorial HMMs factored k state variables depend oneanother observation, tree based HMMs represented probabilitydistributions defined tree structures. key difference LOHMMsapproaches employ logical concept unification. Unification essential444fiLogical Hidden Markov Modelsallows us introduce abstract transitions, consist detailedstates. experimental evidence shows, sharing information among abstract statesmeans unification lead accurate model estimation. holds relational Markov models (RMMs) (Anderson, Domingos, & Weld, 2002) LOHMMsclosely related. RMMs, states different types, type describeddifferent set variables. domain variable hierarchically structured.main differences LOHMMs RMMs RMMs either supportvariable binding unification hidden states.equivalent HMMs context-free languages probabilistic context-free grammars (PCFGs). Like HMMs, consider sequences logical atomsemploy unification. Nevertheless, formal resemblance Baum-Welchalgorithms LOHMMs PCFGs. case LOHMM encodes PCFGalgorithms identical theoretical point view. re-estimate parametersratio expected number times transition (resp. production) usedexpected number times transition (resp. production) might used. proofTheorem 2 assumes PCFG given Greibach normal form6 (GNF) usespushdown automaton parse sentences. grammars GNF, pushdown automatacommon parsing. contrast, actual computations Baum-Welch algorithmPCFGs, called Inside-Outside algorithm (Baker, 1979; Lari & Young, 1990),usually formulated grammars Chomsky normal form7 . Inside-Outside algorithmmake use efficient CYK algorithm (Hopcroft & Ullman, 1979) parsing strings.alternative learning PCFGs strings learn structured dataskeletons, derivation trees nonterminal nodes removed (Levy &Joshi, 1978). Skeletons exactly set trees accepted skeletal tree automata (STA).Informally, STA, given tree input, processes tree bottom up, assigningstate node based states nodes children. STA accepts tree iffassigns final state root tree. Due automata-based characterizationskeletons derivation trees, learning problem (P)CFGs reducedproblem STA. particular, STA techniques adapted learning treegrammars (P)CFGs (Sakakibara, 1992; Sakakibara et al., 1994) efficiently.PCFGs extended several ways. closely related LOHMMsunification-based grammars extensively studied computational linguistics. Examples (stochastic) attribute-value grammars (Abney, 1997), probabilistic feature grammars (Goodman, 1997), head-driven phrase structure grammars (Pollard & Sag,1994), lexical-functional grammars (Bresnan, 2001). learning within frameworks, methods undirected graphical models used; see work Johnson (2003)description recent work. key difference LOHMMs nonterminals replaced structured, complex entities. Thus, observation sequencesflat symbols atoms modelled. Goodmans probabilistic feature grammarsexception. treat terminals nonterminals vectors features. abstractionmade, i.e., feature vectors ground instances, unification employed.6. grammar GNF iff productions form aV variable, exactly oneterminal V string none variables.7. grammar CNF iff every production form B, C A, B C variables,terminal.445fiKersting, De Raedt, & Raikoconmkdirconmkdirmvlscdmvconvt100xvt100xlsnewvt100xvt100xvt100xnew(a)cdvt100x(b)vt100xvt100xFigure 7: (a) atom logical sequence mkdir(vt100x), mv(new, vt100x),ls(vt100x), cd(vt100x) forms tree. shaded nodes denote shared labelsamong trees. (b) sequence represented single tree. predicate con/2 represents concatenation operator.Therefore, number parameters needs estimated becomes easily large,data sparsity serious problem. Goodman applied smoothing overcome problem.LOHMMs generally related (stochastic) tree automata (see e.g., Carrasco, Oncina, Calera-Rubio, 2001). Reconsider Unix command sequencemkdir(vt100x), mv(new, vt100x), ls(vt100x), cd(vt100x) . atom forms tree, seeFigure 7 (a), and, indeed, whole sequence atoms also forms (degenerated) tree,see Figure 7 (b). Tree automata process single trees vertically, e.g., bottom-up. stateautomaton assigned every node tree. state depends node labelstates associated siblings node. focus sequentialdomains. contrast, LOHMMs intended learning sequential domains.process sequences trees horizontally, i.e., left right. Furthermore, unificationused share information consecutive sequence elements. Figure 7 (b)illustrates, tree automata employ information allowing higher-ordertransitions, i.e., states depend node labels states associatedpredecessors 1, 2, . . . levels tree.second type approaches, attention devoted developing highlyexpressive formalisms, e.g. PCUP (Eisele, 1994), PCLP (Riezler, 1998), SLPs (Muggleton, 1996), PLPs (Ngo & Haddawy, 1997), RBNs (Jaeger, 1997), PRMs (Friedman,Getoor, Koller, & Pfeffer, 1999), PRISM (Sato & Kameya, 2001), BLPs (Kersting & DeRaedt, 2001b, 2001a), DPRMs (Sanghai, Domingos, & Weld, 2003). LOHMMsseen attempt towards downgrading highly expressive frameworks. Indeed, applying main idea underlying LOHMMs non-regular probabilistic grammar, i.e., replacingflat symbols atoms, yields principle stochastic logic programs (Muggleton, 1996).consequence, LOHMMs represent interesting position expressiveness scale.Whereas retain essential logical features expressive formalisms,seem easier understand, adapt learn. akin many contemporary consid446fiLogical Hidden Markov Modelserations inductive logic programming (Muggleton & De Raedt, 1994) multi-relationaldata mining (Dzeroski & Lavrac, 2001).8. ConclusionsLogical hidden Markov models, new formalism representing probability distributionssequences logical atoms, introduced solutions three centralinference problems (evaluation, likely state sequence parameter estimation)provided. Experiments demonstrated unification improve generalizationaccuracy, number parameters LOHMM order magnitude smallernumber parameters corresponding HMM, solutions presentedperform well practice also LOHMMs possess several advantages traditionalHMMs applications involving structured sequences.Acknowledgments authors thank Andreas Karwath Johannes Horstmanninteresting collaborations protein data; Ingo Thon interesting collaborationanalyzing Unix command sequences; Saul Greenberg providing Unix command sequence data. authors would also like thank anonymous reviewers comments considerably improved paper. research partly supportedEuropean Union IST programme contract numbers IST-2001-33053 FP6-508861(Application Probabilistic Inductive Logic Programming (APrIL) II). Tapani Raikosupported Marie Curie fellowship DAISY, HPMT-CT-2001-00251.Appendix A. Proof Theorem 1Let = (, , , ) LOHMM. show specifies time discrete stochasticprocess, i.e., sequence random variables hXt it=1,2,... , domains randomvariable Xt hb(), Herbrand base , define immediate state operatorTM -operator current emission operator EM -operator.Definition 4 (TM -Operator, EM -Operator ) operators TM : 2hb 2hb EM :2hb 2hbTM (I) = {HB H | (p : HB) : BB I, HB H G (H)}EM (I) = {OB H | (p : HB) : BB I, HB G G (H)OB H G (O)}i+1({start}))= 1, 2, 3, . . ., set TM({start}) := TM (TM1TM ({start}) := TM ({start}) specifies state set clock forms random varii ({start}) specifies possible symbols emitted transitioningable Yi . set UM+ 1. forms variable Ui . Yi (resp. Ui ) extended randomvariable Zi (resp. Ui ) hb :P (Zi = z) =({start})0.0 : z 6 TMP (Yi = z) : otherwise447fiKersting, De Raedt, & RaikoPSfrag replacementsZ1Z2Z3U1U2...U3Figure 8: Discrete time stochastic process induced LOHMM. nodes Z Uirepresent random variables hb .Figure 8 depicts influence relation among Zi Ui . Using standard argumentsprobability theory notingP (Ui = Ui | Zi+1 = zi+1 , Zi = zi ) =P (Zi+1 | Zi ) =XP (Zi+1 , ui | Zi )P (Zi+1 = zi+1 , Ui = ui | Zi )Pui P (Zi+1 , ui | Zi )uiprobability distributions due equation (1), easy show Kolmogorovs extension theorem (see Bauer, 1991; FristedtGray, 1997) holds. Thus,Ntspecifies unique probability distribution(ZUi ) > 0i=1limit .Appendix B. Moore Representations LOHMMsHMMs, Moore representations, i.e., output symbols associated states Mealyrepresentations, i.e., output symbols associated transitions, equivalent.appendix, investigate extend also holds LOHMMs.Let L Mealy-LOHMM according definition 3. following, derivenotation equivalent LOHMM L0 Moore representation abstracttransitions abstract emissions (see below). predicate b/n L extended b/n+1 L0 . domains first n arguments b/n. last argumentstore observation emitted. precisely, abstract transitiono(v1 ,...,vk )p : h(w1 , . . . , wl ) b(u1 , . . . , un )L, abstract transitionp : h(w1 , . . . , wl , o(v01 , . . . , v0k )) b(u1 , . . . , un , )L0 . primes o(v01 , . . . , v0k ) denote replaced free 8 variables o(v1 , . . . , vk )distinguished constant symbol, say #. Due this, holds(h(w1 , . . . , wl )) = (h(w1 , . . . , wl , o(v01 , . . . , v0k ))) ,8. variable X vars(o(v1 , . . . , vk )) free iff X 6 vars(h(w1 , . . . , wl )) vars(b(u1 , . . . , un )).448(6)fiLogical Hidden Markov ModelsL0 output distribution specified using abstract emissions expressionsform1.0 : o(v1 , . . . , vk ) h(w1 , . . . , wl , o(v01 , . . . , v0k )) .(7)semantics abstract transition L0state S0t G0 (b(u1 , . . . , un , )) system make transitionS0t+1 G0 (h(w1 , . . . , wl , o(v01 , . . . , v0k ))) probabilityp (S0t+1 | h(w1 , . . . , wl , o(v01 , . . . , v0k )) | S0t )state(8)S0t = mgu(S0t , b(u1 , . . . , un , )). Due Equation (6), Equation (8) rewrittenp (S0t+1 | h(w1 , . . . , wl ) | S0t ) .Due equation (7), system emit output symbol ot+1 G0 (o(v1 , . . . , vk ))state S0t+1 probability(ot+1 | o(v1 , . . . , vk )S0t+1 S0t )S0t+1 = mgu(h(w1 , . . . , wl , o(v01 , . . . , v0k )), S0t+1 ). Due construction L0 ,exists triple (St , St+1 , Ot+1 ) L triple (S0t , S0t+1 , Ot+1 ), > 0, L0 (and viseversa). Hence,both LOHMMs assign overall transition probability.L L0 differ way initialize sequences h(S0t , S0t+1 , Ot+1 it=0,2...,T (resp.h(St , St+1 , Ot+1 it=0,2...,T ). Whereas L starts state S0 makes transition S1emitting O1 , Moore-LOHMM L0 supposed emit symbol O0 S00 makingtransition S01 . compensate using prior distribution. existencecorrect prior distribution L0 seen follows. L, finitely manystates reachable time = 1, i.e, PL (q0 = S) > 0 holds finite set groundstates S. probability PL (q0 = s) computed similar 1 (S). set = 1 line6, neglecting condition Ot1 line 10, dropping (Ot1 | OB H ) line 14.Completely listing states S1 together PL (q0 = S), i.e., PL (q0 = S) : start ,constitutes prior distribution L0 .argumentation basically followed approach transform Mealy machineMoore machine (see e.g., Hopcroft Ullman, 1979). Furthermore, mappingMoore-LOHMM introduced present section Mealy-LOHMM straightforward.Appendix C. Proof Theorem 2Let terminal alphabet N nonterminal alphabet. probabilistic context-freegrammar (PCFG) G consists distinguished start symbol N plus finite setproductionsP form p : X , X N , (N ) p [0, 1].X N , :X p = 1. PCFG defines stochastic process sentential forms states,leftmost rewriting steps transitions. denote single rewriting operationgrammar single arrow . result one ore rewriting operationsable rewrite (N ) sequence (N ) nonterminals terminals,write . probability rewriting product probability449fiKersting, De Raedt, & Raikovalues associated productions used derivation. assume G consistent, i.e.,sum probabilities derivations sum 1.0.assume PCFG G Greibach normal form. follows Abneyet al.s (1999) Theorem 6 G consistent. Thus, every production P Gform p : X aY1 . . . Yn n 0. order encode G LOHMM ,introduce (1) non-terminal symbol X G constant symbol nX (2)terminal symbol G constant symbol t. production P G, includeabstract transition form p : stack([nY1 , . . . , nYn |S])stack([nX|S]), n > 0,p : stack(S)stack([nX|S]), n = 0. Furthermore, include 1.0 : stack([s]) startend1.0 : end stack([]). straightforward prove induction Gequivalent.Appendix D. Logical Hidden Markov Model Unix CommandSequencesLOHMMs described model Unix command sequences triggered mkdir.aim, transformed original Greenberg data sequence logical atomscom, mkdir(Dir, LastCom), ls(Dir, LastCom), cd(Dir, Dir, LastCom), cp(Dir, Dir, LastCom)mv(Dir, Dir, LastCom). domain LastCom {start, com, mkdir, ls, cd, cp, mv}.domain Dir consisted argument entries mkdir, ls, cd, cp, mv originaldataset. Switches, pipes, etc. neglected, paths made absolute. yields212 constants domain Dir. original commands, mkdir, ls, cd,cp, mv, represented com. mkdir appear within 10 time stepscommand C {ls, cd, cp,mv}, C represented com. Overall, yields451000 ground states covered Markov model.unification LOHMM U basically implements second order Markov model, i.e.,probability making transition depends upon current state previousstate. 542 parameters following structure:com start.mkdir(Dir, start) start.com com.mkdir(Dir, com) com.end com.Furthermore, C {start, com}mkdir(Dir, com)mkdir( , com)comendls(Dir, mkdir)ls( , mkdir)cd(Dir, mkdir)mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).cd( , mkdir)cp( , Dir, mkdir)cp(Dir, , mkdir)cp( , , mkdir)mv( , Dir, mkdir)mv(Dir, , mkdir)mv( , , mkdir)450mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).mkdir(Dir,C).fiLogical Hidden Markov Modelstogether C {mkdir, ls, cd, cp, mv} C 1 {cd, ls} (resp.C2 {cp, mv})mkdir(Dir, com)mkdir( , com)comendls(Dir,C1 )ls( ,C1 )cd(Dir,C1 )cd( ,C1 )cp( , Dir,C1 )cp(Dir, ,C1 )cp( , ,C1 )mv( , Dir,C1 )mv(Dir, ,C1 )mv( , ,C1 )C1 (Dir,C). mkdir( , com)comC1 (Dir,C).C1 (Dir,C).endls(From,C2 )C1 (Dir,C).ls(To,C2 )C1 (Dir,C).C1 (Dir,C).ls( ,C2 )C1 (Dir,C).cd(From,C2 )C1 (Dir,C).cd(To,C2 )C1 (Dir,C).cd( ,C2 )C1 (Dir,C). cp(From, ,C2 )C1 (Dir,C).cp( , To,C2 )C1 (Dir,C).cp( , ,C2 )C1 (Dir,C). mv(From, ,C2 )mv( , To,C2 )C1 (Dir,C).mv( , ,C2 )C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).C2 (From, To,C).states fully observable, omitted output symbols associatedclauses, and, sake simplicity, omitted associated probability values.unification LOHMM N variant U variables sharedmkdir( , com) cp(From, To,C).ls(com cp(From, To,C).cd(end cp(From, To,C). cp( ,mv( ,, cp), cp), cp), cp)cp(From, To,C).cp(From, To,C).cp(From, To,C).cp(From, To,C).transitions affected, N 164 parameters less U , i.e., 378.Appendix E. Tree-based LOHMM mRNA SequencesLOHMM processes nodes mRNA trees in-order. structure LOHMMshown end section. copies shaded parts. Termsabbreviated using starting alphanumerical; tr stands tree, helical, sisingle, nuc nucleotide, nuc p nucleotide pair.domain #Children covers maximal branching factor found data, i.e.,{[c], [c, c], . . . , [c, c, c, c, c, c, c, c, c]}; domain Type consists types occurringdata, i.e., {stem, single, bulge3, bulge5, hairpin}; Size, domain coversmaximal length secondary structure element data, i.e., longest sequenceconsecutive bases respectively base pairs constituting secondary structure element.length encoded {n1 (0), n2 (0), . . . , n13 (0)} nm (0) denotes recursiveapplication functor n times. Base BasePair , domains 4 basesrespectively 16 base pairs. total, 491 parameters.451fimy start1.0 : root(0, root, X)Copies tr(Id, [c], [Pa [C]|R]), tr(Id, [c, c], [Pa [C]|R]),tr(Id, [c, c, c], [Pa [C]|R])0.25 : he(s(Id), Pa, [], T, L)tr(Id, , [Pa [C]|R])tr(0, X, [0 X])tr(Id, [c, c, c], [Pa [C1, C2|Cs]|R])0.25 : he(s(Id), Pa, B, T, L)0.25 : he(s(Id), Pa, [], T, L)0.25 : he(s(Id), Pa, B, T, L)tr(Id, , [Pa [C1, C2|Cs]|R])0.25 : si(s(Id), Pa, B, T, L)0.25 : si(s(Id), Pa, [], T, L)0.25 : si(s(Id), Pa, B, T, L)0.25 : si(s(Id), Pa, [], T, L)se(T, L, s(Id), B, [s(Id) B|R])se(T, L, s(Id), [], R)Copies tr(Id, [c], [Pa [C1, C2|Cs]|R]), tr(Id, [c, c], [Pa [C1, C2|Cs]|R]),treemodelse(T, L, s(Id), [], [Pa [C2|Cs]|R]) se(T, L, s(Id), B, [s(Id) B, Pa [C2|Cs]|R])Copies type single, bulge3, bulge5Copies n(n(0)) n(n(n(0)))Copies length sequence n(n(0)), n(n(n(0))), n(n(n(n(0))))se(stem, n(A), Id, B, S)se(hairpin, n(A), Id, B, S)Copies nuc p(a, g), . . . , nuc p(u, u)Copies nuc(g), nuc(c), nuc(u)0.25 : nuc(a)0.0625 : nuc p(a, a)se(hairpin, A, Id, B, S)se(hairpin, n(0), Id, B, S)se(stem, A, Id, B, S)se(hairpin, n(0), s( ), , [])0.25 : nuc(a)se(stem, n(0), s( ), , [])0.0625 : nuc p(a, a)0.25 : nuc(a)se(stem, n(0), Id, B, S)0.0625 : nuc p(a, a)Copies nuc p(a, g), . . . , nuc p(u, u)Copies nuc(g), nuc(c), nuc(u)endtr(Id, B, S)sequencemodelKersting, De Raedt, & RaikoFigure 9: mRNA LOHMM structure. symbol denotes anonymous variablesread treated distinct, new variables time encountered.copies shaded part. Terms abbreviated using startingalphanumerical; tr stands tree, se structure element, helical,si single, nuc nucleotide, nuc p nucleotide pair.References452Abney, S. (1997). Stochastic Attribute-Value Grammars. Computational Linguistics, 23 (4),597618.1.0startfiLogical Hidden Markov ModelsAbney, S., McAllester, D., & Pereira, F. (1999). Relating probabilistic grammars automata. Proceedings 37th Annual Meeting Association ComputationalLinguistics (ACL-1999), pp. 542549. Morgan Kaufmann.Anderson, C., Domingos, P., & Weld, D. (2002). Relational Markov Models Application Adaptive Web Navigation. Proceedings Eighth InternationalConference Knowledge Discovery Data Mining (KDD-2002), pp. 143152 Edmonton, Canada. ACM Press.Baker, J. (1979). Trainable grammars speech recognition. Speech communicationpaper presented th 97th Meeting Acoustical Society America, pp. 547550Boston, MA.Bauer, H. (1991). Wahrscheinlichkeitstheorie (4. edition). Walter de Gruyter, Berlin, NewYork.Baum, L. (1972). inequality associated maximization technique statistical estimation probabilistic functions markov processes. Inequalities, 3, 18.Bohnebeck, U., Horvath, T., & Wrobel, S. (1998). Term comparison first-order similaritymeasures. Proceedings Eigth International Conference Inductive LogicProgramming (ILP-98), Vol. 1446 LNCS, pp. 6579. Springer.Bresnan, J. (2001). Lexical-Functional Syntax. Blackwell, Malden, MA.Carrasco, R., Oncina, J., & Calera-Rubio, J. (2001). Stochastic inference regular treelanguages. Machine Learning, 44 (1/2), 185197.Chandonia, J., Hon, G., Walker, N., Lo Conte, L., P.Koehl, & Brenner, S. (2004).ASTRAL compendium 2004. Nucleic Acids Research, 32, D189D192.Davison, B., & Hirsh, H. (1998). Predicting Sequences User Actions. PredictingFuture: AI Approaches Time-Series Analysis, pp. 512. AAAI Press.De Raedt, L., & Kersting, K. (2003). Probabilistic Logic Learning. ACM-SIGKDD Explorations: Special issue Multi-Relational Data Mining, 5 (1), 3148.De Raedt, L., & Kersting, K. (2004). Probabilistic Inductive Logic Programming.Ben-David, S., Case, J., & Maruoka, A. (Eds.), Proceedings 15th InternationalConference Algorithmic Learning Theory (ALT-2004), Vol. 3244 LNCS, pp.1936 Padova, Italy. Springer.Durbin, R., Eddy, S., Krogh, A., & Mitchison, G. (1998). Biological sequence analysis:Probabilistic models proteins nucleic acids. Cambridge University Press.Dzeroski, S., & Lavrac, N. (Eds.). (2001). Relational data mining. Springer-Verlag, Berlin.Eddy, S., & Durbin, R. (1994). RNA sequence analysis using covariance models. NucleicAcids Res., 22 (11), 20792088.453fiKersting, De Raedt, & RaikoEisele, A. (1994). Towards probabilistic extensions contraint-based grammars.Dorne, J. (Ed.), Computational Aspects Constraint-Based Linguistics Decription-II.DYNA-2 deliverable R1.2.B.Fine, S., Singer, Y., & Tishby, N. (1998). hierarchical hidden markov model: analysisapplications. Machine Learning, 32, 4162.Frasconi, P., Soda, G., & Vullo, A. (2002). Hidden markov models text categorizationmulti-page documents. Journal Intelligent Information Systems, 18, 195217.Friedman, N., Getoor, L., Koller, D., & Pfeffer, A. (1999). Learning probabilistic relationalmodels. Proceedings Sixteenth International Joint Conference Artificial Intelligence (IJCAI-1999), pp. 13001307. Morgan Kaufmann.Fristedt, B., & Gray, L. (1997). Modern Approach Probability Theory. Probabilityapplications. Birkhauser Boston.Ghahramani, Z., & Jordan, M. (1997). Factorial hidden Markov models. Machine Learning,29, 245273.Goodman, J. (1997). Probabilistic feature grammars. Proceedings Fifth International Workshop Parsing Technologies (IWPT-97) Boston, MA, USA.Greenberg, S. (1988). Using Unix: collected traces 168 users. Tech. rep., Dept.Computer Science, University Calgary, Alberta.Hopcroft, J., & Ullman, J. (1979). Introduction Automata Theory, Languages,Computation. Addison-Wesley Publishing Company.Horvath, T., Wrobel, S., & Bohnebeck, U. (2001). Relational Instance-Based learningLists Terms. Machine Learning, 43 (1/2), 5380.Hubbard, T., Murzin, A., Brenner, S., & Chotia, C. (1997). SCOP : structural classificationproteins database. NAR, 27 (1), 236239.Jacobs, N., & Blockeel, H. (2001). Learning Shell: Automated Macro Construction.User Modeling 2001, pp. 3443.Jaeger, M. (1997). Relational Bayesian networks. Proceedings Thirteenth Conference Uncertainty Artificial Intelligence (UAI), pp. 266273. Morgan Kaufmann.Katz, S. (1987). Estimation probabilities sparse data hte language model component speech recognizer. IEEE Transactions Acoustics, Speech, SignalProcessing (ASSP), 35, 400401.Kersting, K., & De Raedt, L. (2001a). Adaptive Bayesian Logic Programs. Rouveirol,C., & Sebag, M. (Eds.), Proceedings 11th International Conference InductiveLogic Programming (ILP-01), Vol. 2157 LNAI, pp. 118131. Springer.454fiLogical Hidden Markov ModelsKersting, K., & De Raedt, L. (2001b). Towards Combining Inductive Logic ProgrammingBayesian Networks. Rouveirol, C., & Sebag, M. (Eds.), Proceedings11th International Conference Inductive Logic Programming (ILP-01), Vol. 2157LNAI, pp. 118131. Springer.Kersting, K., & Raiko, T. (2005). Say EM Selecting Probabilistic Models LogicalSequences. Bacchus, F., & Jaakkola, T. (Eds.), Proceedings 21st ConferenceUncertainty Artificial Intelligence, UAI 2005, pp. 300307 Edinburgh, Scotland.Kersting, K., Raiko, T., Kramer, S., & De Raedt, L. (2003). Towards discovering structural signatures protein folds based logical hidden markov models. Altman,R., Dunker, A., Hunter, L., Jung, T., & Klein, T. (Eds.), Proceedings Pacific Symposium Biocomputing (PSB-03), pp. 192203 Kauai, Hawaii, USA. WorldScientific.Koivisto, M., Kivioja, T., Mannila, H., Rastas, P., & Ukkonen, E. (2004). Hidden MarkovModelling Techniques Haplotype Analysis. Ben-David, S., Case, J., & Maruoka,A. (Eds.), Proceedings 15th International Conference Algorithmic Learning Theory (ALT-04), Vol. 3244 LNCS, pp. 3752. Springer.Koivisto, M., Perola, M., Varilo, T., Hennah, W., Ekelund, J., Lukk, M., Peltonen, L.,Ukkonen, E., & Mannila, H. (2002). MDL method finding haplotype blocksestimating strength haplotype block boundaries. Altman, R., Dunker,A., Hunter, L., Jung, T., & Klein, T. (Eds.), Proceedings Pacific SymposiumBiocomputing (PSB-02), pp. 502513. World Scientific.Korvemaker, B., & Greiner, R. (2000). Predicting UNIX command files: Adjusting userpatterns. Adaptive User Interfaces: Papers 2000 AAAI Spring Symposium,pp. 5964.Kulp, D., Haussler, D., Reese, M., & Eeckman, F. (1996). Generalized Hidden MarkovModel Recognition Human Genes DNA. States, D., Agarwal, P.,Gaasterland, T., Hunter, L., & Smith, R. (Eds.), Proceedings Fourth International Conference Intelligent Systems Molecular Biology,(ISMB-96), pp. 134142 St. Louis, MO, USA. AAAI.Lane, T. (1999). Hidden Markov Models Human/Computer Interface Modeling.Rudstrom, A. (Ed.), Proceedings IJCAI-99 Workshop Learning Users,pp. 3544 Stockholm, Sweden.Lari, K., & Young, S. (1990). estimation stochastic context-free grammars usinginside-outside algorithm. Computer Speech Language, 4, 3556.Levy, L., & Joshi, A. (1978). Skeletal structural descriptions. Information Control,2 (2), 192211.McLachlan, G., & Krishnan, T. (1997). EM Algorithm Extensions. Wiley, NewYork.455fiKersting, De Raedt, & RaikoMitchell, T. M. (1997). Machine Learning. McGraw-Hill Companies, Inc.Muggleton, S. (1996). Stochastic logic programs. De Raedt, L. (Ed.), AdvancesInductive Logic Programming, pp. 254264. IOS Press.Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods.Journal Logic Programming, 19 (20), 629679.Ngo, L., & Haddawy, P. (1997). Answering queries context-sensitive probabilisticknowledge bases. Theoretical Computer Science, 171, 147177.Pollard, C., & Sag, I. (1994). Head-driven Phrase Structure Grammar. UniversityChicago Press, Chicago.Rabiner, L., & Juang, B. (1986). Introduction Hidden Markov Models. IEEE ASSPMagazine, 3 (1), 416.Riezler, S. (1998). Statistical inference probabilistic modelling constraint-basednlp. Schrder, B., Lenders, W., & und T. Portele, W. H. (Eds.), Proceedings4th Conference Natural Language Processing (KONVENS-98). Also CoRRcs.CL/9905010.Sakakibara, Y. (1992). Efficient learning context-free grammars positive structuralexamples. Information Computation, 97 (1), 2360.Sakakibara, Y. (2003). Pair hidden markov models tree structures. Bioinformatics,19 (Suppl.1), i232i240.Sakakibara, Y., Brown, M., Hughey, R., Mian, I., Sjolander, K., & Underwood, R. (1994).Stochastic context-free grammars tRNA modelling. Nucleic Acids Research,22 (23), 51125120.Sanghai, S., Domingos, P., & Weld, D. (2003). Dynamic probabilistic relational models.Gottlob, G., & Walsh, T. (Eds.), Proceedings Eighteenth International JointConference Artificial Intelligence (IJCAI-03), pp. 992997 Acapulco, Mexico. Morgan Kaufmann.Sato, T., & Kameya, Y. (2001). Parameter learning logic programs symbolic-statisticalmodeling. Journal Artificial Intelligence Research (JAIR), 15, 391454.Scholkopf, B., & Warmuth, M. (Eds.). (2003). Learning Parsing Stochastic UnificationBased Grammars, Vol. 2777 LNCS. Springer.Turcotte, M., Muggleton, S., & Sternberg, M. (2001). effect relational backgroundknowledge learning protein three-dimensional fold signatures. Machine Learning,43 (1/2), 8195.Won, K., Prugel-Bennett, A., & Krogh, A. (2004). Block Hidden Markov Model Biological Sequence Analysis. Negoita, M., Howlett, R., & Jain, L. (Eds.), ProceedingsEighth International Conference Knowledge-Based Intelligent InformationEngineering Systems (KES-04), Vol. 3213 LNCS, pp. 6470. Springer.456fiJournal Artificial Intelligence Research 25 (2006) 269-314Submitted 5/05; published 2/06Distributed Reasoning Peer-to-Peer Setting:Application Semantic WebPhilippe AdjimanPhilippe ChatalicFrancois GoasdoueMarie-Christine RoussetLaurent Simonadjiman@lri.frchatalic@lri.frfg@lri.frmcr@lri.frsimon@lri.frLRI-PCRI, Batiment 490CNRS & Universite Paris-Sud XI INRIA Futurs91405 Orsay Cedex, FranceAbstractpeer-to-peer inference system, peer reason locally also solicitacquaintances, peers sharing part vocabulary. paper,consider peer-to-peer inference systems local theory peer setpropositional clauses defined upon local vocabulary. important characteristic peerto-peer inference systems global theory (the union peer theories)known (as opposed partition-based reasoning systems). main contributionpaper provide first consequence finding algorithm peer-to-peer setting: DeCA.anytime computes consequences gradually solicited peer peersdistant. exhibit sufficient condition acquaintance graphpeer-to-peer inference system guaranteeing completeness algorithm. Anotherimportant contribution apply general distributed reasoning setting settingSemantic Web Somewhere semantic peer-to-peer data managementsystem. last contribution paper provide experimental analysisscalability peer-to-peer infrastructure propose, large networks 1000peers.1. IntroductionRecently peer-to-peer systems received considerable attention underlying infrastructure appropriate scalable flexible distributed applicationsInternet. peer-to-peer system, centralized control hierarchical organization: peer equivalent functionality cooperates peers ordersolve collective task. Peer-to-peer systems evolved simple keyword-basedpeer-to-peer file sharing systems like Napster (http://www.napster.com) Gnutella(http://gnutella.wego.com) semantic peer-to-peer data management systems likeEdutella (Nejdl, Wolf, Qu, Decker, Sintek, & al., 2002) Piazza (Halevy, Ives, Tatarinov, & Mork, 2003a), handle semantic data description support complex queriesdata retrieval. systems, complexity answering queries directly relatedexpressivity formalism used state semantic mappings peersschemas (Halevy, Ives, Suciu, & Tatarinov, 2003b).c!2006AI Access Foundation. rights reserved.fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonpaper, interested peer-to-peer inference systems peeranswer queries reasoning local (propositional) theory also ask queriespeers semantically related sharing part vocabulary.framework encompasses several applications like peer-to-peer information integrationsystems intelligent agents, peer knowledge (about dataexpertise domain) partial knowledge peers. setting,solicited perform reasoning task, peer, cannot solve completely task locally,must able distribute appropriate reasoning subtasks among acquainted peers.leads step step splitting initial task among peers relevantsolve parts it. outputs different splitted tasks must recomposedconstruct outputs initial task.consider peer-to-peer inference systems local theory peercomposed set propositional clauses defined upon set propositional variables(called local vocabulary). peer may share part vocabularypeers. investigate reasoning task finding consequences certain form (e.g.,clauses involving certain variables) given input formula expressed using localvocabulary peer. Note reasoning tasks like finding implicants certainform given input formula equivalently reduced consequence finding task.important emphasize problem distributed reasoning considerpaper quite different problem reasoning partitions obtaineddecomposition theory (Dechter & Rish, 1994; Amir & McIlraith, 2000).problem, centralized large theory given structure exploited computebest partitioning order optimize use partition-based reasoning algorithm.problem, whole theory (i.e., union local theories) knownpartition imposed peer-to-peer architecture. illustrate example(Section 2), algorithms based transmitting clauses partitions spiritwork Amir McIlraith (2000), Dechter Rish (1994) del Val (1999)appropriate consequence finding problem. algorithm splits clausesshare variables several peers. piece splitted clause transmittedcorresponding theory find consequences. consequences foundpiece splitted clause must recomposed get consequences clausesplitted.main contribution paper provide first consequence finding algorithmpeer-to-peer setting: DeCA. anytime computes consequences graduallysolicited peer peers distant. exhibit sufficient condition acquaintance graph peer-to-peer inference system guaranteeingcompleteness algorithm.Another important contribution apply general distributed reasoning settingsetting Semantic Web Somewhere semantic peer-to-peer datamanagement system. Somewhere based simple data model made taxonomiesatomic classes mappings classes different taxonomies, thinkappropriate common semantic support needed future semantic webapplications. Somewhere data model encoded propositional logicquery answering Somewhere equivalently reduced distributed reasoninglogical propositional theories.270fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Weblast contribution paper provide experimental analysis scalability approach, deployed large networks. far, scalability systemlike Piazza goes 80 peers. Piazza uses expressive languageone used approach, results rely wide range optimizations (mappingscomposition, paths pruning Tatarinov & Halevy, 2004) made possible centralizedstorage schemas mappings global server. contrast, stuckfully decentralized approach performed experiments networks 1000 peers.important point characterizing peer-to-peer systems dynamicity: peersjoin leave system moment. Therefore, feasible bringinformation single server order reason locally using standard reasoningalgorithms. would costly gather data available systemwould useless task changing peers connected network.dynamicity peer-to-peer inference systems imposes revisit reasoning problemorder address completely decentralized manner.paper organized follows. Section 2 defines formally peer-to-peer inferenceproblem address paper. Section 3, describe distributed consequencefinding algorithm state properties. describe Somewhere Section 4.Section 5 reports experimental study scalability peer-to-peer infrastructure.Related work summarized Section 6. conclude short discussion Section 7.2. Consequence Finding Peer-to-peer Inference Systemspeer-to-peer inference system (P2PIS) network peer theories. peer Pfinite set propositional formulas language LP . consider case LPlanguage clauses without duplicated literals built finite setpropositional variables VP , called vocabulary P . Peers semantically relatedsharing variables peers. shared variable two peers intersectionvocabularies two peers. impose however variablescommon vocabularies two peers shared two peers: two peers mayaware variables common them.P2PIS, peer knowledge global P2PIS theory. peer knowslocal theory variables shares peers P2PIS (itsacquaintances). necessarily knows variables commonpeers (including acquaintances). new peer joins P2PIS simplydeclares acquaintances P2PIS, i.e., peers knows sharing variables with,declares corresponding shared variables.2.1 Syntax SemanticsP2PIS formalized using notion acquaintance graph followingconsider P2PIS acquaintance graphs equivalent.271fiAdjiman, Chatalic, Goasdoue, Rousset, & SimonDefinition 1 (Acquaintance graph) Let P = {Pi }i=1..n collection clausal theories respective vocabularies VPi , let V = i=1..n VPi . acquaintance graph Vgraph = (P, acq) P set vertices acq V P P setlabelled edges every (v, Pi , Pj ) acq, %= j v VPi VPj .labelled edge (v, Pi , Pj ) expresses peers Pi Pj know sharingvariable v. peer P literal l, acq(l, P ) denotes set peers sharingP variable l.contrast approaches (Ghidini & Serafini, 2000; Calvanese, De Giacomo,Lenzerini, & Rosati, 2004), adopt epistemic modal semantics interpretingP2PIS interpret standard semantics propositional logic.Definition 2 (Semantics P2PIS) Let = (P, acq) P2PIS P = {Pi }i=1..n ,!interpretation P assignement variables i=1..n Pi truef alse. particular, variable common two theories Pi Pj givenP2PIS interpreted value two theories.model clause c iff one literals c evaluated true I.model set clauses (i.e., local theory, union local theories,whole P2PIS) iff model clauses set.P2PIS satisfiable iff model.consequence relation P2PIS standard consequence relation |=: givenP2PIS P, clause c, P |= c iff every model P model c.2.2 Consequence Finding Problemtheory P , consider subset target variables V P VP , supposed representvariables interest application, (e.g., observable facts model-based diagnosisapplication, classes storing data information integration application). goal is,given clause provided input given peer, find possible consequencesbelonging target language input clause union peer theories.point input clause uses vocabulary queried peer,expected consequences may involve target variables different peers. targetlanguages handled algorithm defined terms target variables requireshared variable target status peers sharing it. worth notingrequirement local property: peers sharing variables given peeracquaintances and, definition, direct neighbours acquaintancegraph.Definition 3 (Target Language) Let = (P, acq) P2PIS, every peer P ,let V P set target variables (v, Pi , Pj ) acq v V Pi iff)v V Pj . set SP peers P, define target language arget(SP!language clauses (including empty clause) involving variables P SP V P .272fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Webreasoning problem interested compute logical consequencesinput clause given P2PIS. corresponds notion proper prime implicatesclause w.r.t. clausal (distributed) theory, formally defined Definition 4.Definition 4 (Proper prime implicate clause w.r.t. clausal theory) Let Pclausal theory q clause. clause said be:implicate q w.r.t. P iff P {q} |= m.prime implicate q w.r.t. P iff implicate q w.r.t. P ,clause m# implicate q w.r.t. P , m# |= m# m.proper prime implicate q w.r.t. P iff prime implicate q w.r.t. PP %|= m.problem finding prime implicates new clause theory, a.k.a. -primeimplicates, corresponds exactly problem computing proper prime implicatesclause w.r.t. clausal theory. extensively studied centralized case(see work Marquis, 2000, survey). Note deciding whether clause-prime implicate clausal theory BH2 -complete (Marquis, 2000), i.e., N PcoN P . problem address may viewed refinement, restrictingcomputation proper prime implicates given target language. corresponds(L, )-prime implicates work Marquis (2000) complexity.Definition 5 (The consequence finding problem P2PIS) Let = (P, acq)P 2P IS, P = {Pi }i=1..n collection clausal theories respective targetvariables. consequence finding problem is, given peer P , acquaintances,!clause q LP , find set proper prime implicates q w.r.t. i=1..n Pibelong arget(P).algorithmic point view, consequence finding problem P2PIS newsignificantly different consequence finding problem single global theory.According semantics, order complete, peer-to-peer consequence findingalgorithm must obtain results standard consequence finding algorithmapplied union local theories, without global input:partial local input made theory single peer acquaintances.reasoning must distributed appropriately among different theories withoutglobal view whole P2PIS. full peer-to-peer setting, consequence findingalgorithm cannot centralized (because would mean super-peer controllingreasoning). Therefore, must design algorithm running independently peerpossibly distributing part reasoning controls acquainted peers: peercontrol whole reasoning.Among possible consequences distinguish local consequences, involving targetvariables solicited peer, remote consequences, involve target variablessingle peer distant solicited peer, combined consequences involve targetvariables several peers.273fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon2.3 Examplefollowing example illustrates main characteristics message-based distributedalgorithm running peer, presented detail Section 3.Let us consider 4 interacting peers. P1 describes tour operator. theory expressescurrent F ar destinations either Chile Kenya. far destinationsinternational destinations (Int) expensive (Exp). peer P2 concernedpolice regulations expresses passport required (P ass) international destinations. P3 focuses sanitary conditions travelers. expresses that, Kenya,yellow fever vaccination (Y ellowF ev) strongly recommended strong protection paludism taken (P alu) accomodation occurs Lodges. P4describes travel accommodation conditions : Lodge Kenya Hotel Chile. alsoexpresses anti-paludism protection required, accommodations equippedappropriate anti-mosquito protections (AntiM ). respective theories peerdescribed Figure 1 nodes acquaintance graph. Shared variables mentioned edge labels. Target variables defined : V P1 = {Exp}, V P2 = {P ass},V P3 = {Lodge, ellowF ev, P alu} V P4 = {Lodge, Hotel, P alu, AntiM }.P1 :Far ExpFar Chile KenyaIntP2 :Int PassFar IntKenyaKenya,ChileP3 :Kenya YellowFevLodge Kenya PaluP4 :Kenya LodgeChile HotelPalu AntiMLodge,PaluFigure 1: Acquaintance graph tour operator exampleillustrate behavior algorithm input clause Far providedpeer P1 user. Describing precisely behavior distributed algorithmnetwork peers easy. following present propagation reasoningtree structure, nodes correspond peers branchesmaterialize different reasoning paths induced initial input clause. Edgeslabelled left side literals propagated along paths and/or rightside consequences transmitted back. downward arrow edge indicatesstep literal propagated one peer neighbor. instance,initial step represented following tree :F arP1 :Local consequences literal propagated peer explicited withinpeer node. Target literals outlined using grey background, well transmitted274fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Webback consequences. Vertical arrows preceding consequences distinguish last returnedconsequences earlier ones. Although successive trees presented increasingdepth, reasoning paths explored synchronously parallel, readerkeep mind messages exchanged asynchronous wayorder consequents produced cannot predicted.example, consequences Far derived local reasoning P1 Exp, IntChile Kenya. Since Exp arget(P1 ) local consequence Far. Inttarget literal shared P2 , therefore transmitted P2 . clauseChile Kenya also made shared variables. clauses processed algorithmusing split/recombination approach. shared literal processed independently,transmitted appropriate neighbors. literal associated queue datastructure, transmitted back consequences stored. soon least one consequent obtained literal, respective queued consequents literalrecombined, produce consequences initial clause. recombination processcontinues, new consequences literal produced. Note since literalprocessed asynchronously, order recombined consequences producedunpredictable. Here, component Chile transmitted P4 Kenya transmittedP3 P4 . Let us note peer P4 appears two times tree, two differentliterals propagated peer, induces two different reasoning paths.F arP1 :ExpIntP2 :ChileIntChile KenyaKenya KenyaP4 :P3 :P4 :Exp transmitted back user first (local) consequence Far.propagation Int P2 produces clause Pass, arget(P2 )shared therefore, cannot propagated.clause Chile, transmitted P4 , produces Hotel arget(P4 )shared cannot propagated.transmitted P3 , clause Kenya produces YellowFev well clauseLodge Palu. three variables arget(P3 ). Lodge Palu also sharedvariables therefore, splitting second clause, corresponding literalstransmitted (independently) P4 .transmitted P4 , Kenya produces Lodge, arget(P4 ) alsoshared therefore transmitted P3 .275fiAdjiman, Chatalic, Goasdoue, Rousset, & SimonExpF arP1 :ExpIntP2 :PassChile KenyaIntChileP4 :HotelKenyaKenyaP3 :YellowFevLodgeP4 :P4 :Lodge PaluP aluP4 :LodgeLodgeP3 :clause Pass, produced P2 , transmitted back P1 consequence Intuser remote consequence Far.clause Hotel, produced P4 , transmitted back P1 queuedconsequent Chile, since combined consequences Kenya.two local consequences Kenya obtained P3 contain target variables.transmitted back P1 queued there. may combinedHotel produce two new combined consequences Far : Hotel YellowFevHotel Lodge Palu, transmitted back user.Similarly P4 , Lodge local target consequent Kenya, transmitted backP1 consequent Kenya, combined Hotel produce newconsequence Far that, turn, transmitted back user.Simultaneously, reasoning propagates network peers. propagation Lodge Palu P4 respectively produces Kenya, target literalshared thus propagated P1 , well AntiM, target literal,shared. detail propagation Lodge right branchreasoning tree.276fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic WebExpPassHotel YellowFevF arHotel Lodge PaluHotel LodgeP1 :ExpPassP2 :PassP4 :HotelChile KenyaIntYellowFevHotelLodge PaluP3 :LodgeP4 :YellowFevLodgeP4 :KenyaLodge PaluP aluP4 :AntiMLodgeLodgeP3 :...KenyaP1 :Note deepest node P1 asked produce implicates Kenya,complementary literal Kenya still process. see Section 3situations handled algorithm mean histories keeping trackreasoning branches ending transmitted literal. history containstwo complementary literals, corresponding reasoning branch closed emptyclause ! returned consequence literals history.example, consequence produced P1 Kenya thus !, sentback P4 P3 . combination P3 Palu thus obtain Palunew consequent Kenya, subsumes previously obtained Lodge Palu.transmitted back P1 combined Hotel obtain Hotel Palu subsumespreviously obtained consequent Hotel Lodge Palu. Since AntiM shared variableconsequent Palu P4 . transmitted back P3 combination!, thus obtain AntiM which, turn, returned P1 combination Hotel, thusgiving Hotel AntiM new consequent Far.277fiAdjiman, Chatalic, Goasdoue, Rousset, & SimonExp PassHotel YellowFevHotel LodgeHotel PaluF arHotel AntiMHotel ...P1 :ExpChile KenyaIntYellowFevPassP2 :PassP4 :HotelLodgePaluHotel...AntiMP3 :P4 :YellowFevLodge PaluAntiM!P4 :KenyaP4 :AntiMLodge...P3 :...!P1 :detailed production consequences Lodge P3 rightbranch. reader could check similar way also produces AntiM (whichalready produced P3 /P4 third branch). Eventually, whole setconsequences Far {Exp, Pass, HotelLodge, HotelPalu, HotelAntiM, HotelYellowFev}.Among consequences, important note (e.g., HotelYellowFev)involve target variables different peers. implicates could obtainedpartition-based algorithms like (Amir & McIlraith, 2000). made possiblethanks split/recombination strategy algorithm.3. Distributed Consequence Finding Algorithmmessage passing distributed algorithm implemented described Section3.2. show terminates computes results recursivealgorithm described Section 3.1. exhibit property acquaintance graphguarantees completeness recursive algorithm, therefore message passingdistributed algorithm (since algorithms compute results).algorithms, use following notations :literal q, Resolvent(q, P ) denotes set clauses obtained resolutionset P {q} P alone. call clauses proper resolvents qw.r.t. P ,literal q, q denotes complementary literal,clause c peer P , S(c) (resp. L(c)) denotes disjunction literalsc whose variables shared (resp. shared) acquaintance P .278fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Webcondition S(c) = ! thus expresses c contain variable sharedacquaintance P ,history hist sequence triples (l, P, c) (where l literal, P peer, cclause). history [(ln , Pn , cn ), . . . , (l1 , P1 , c1 ), (l0 , P0 , c0 )] represents branchreasoning initiated propagation literal l0 within peer P0 , eithercontains clause l0 c0 (in case c0 may splitted differentliterals among l1 propagated P1 ), (in case l0 = c0 l0propagated P1 , thus l0 = l1 ): every [0..n 1], ci consequenceli Pi , li+1 literal ci , propagated Pi+1 ,! distribution operator sets clauses: S1 ! ! Sn = {c1 cn|c1 S1 , . . . , cn Sn }. L = {l1 , . . . , lp }, use notation !lL Sl denoteSl1 ! ! Slp .3.1 Recursive Consequence Finding AlgorithmLet = (P, acq) P2PIS, P one peers, q literal whose variable belongsvocabulary P . RCF (q, P ) computes implicates literal q w.r.t. P, startingcomputation local consequences q, i.e., implicates q w.r.t. P ,recursively following acquaintances visited peers. ensure termination,necessary keep track literals already processed peers. donerecursive algorithm RCF H(q, SP, hist), hist history reasoning branchending propagation literal q SP , set acquaintanceslast peer added history.Algorithm 1: Recursive consequence finding algorithmRCF (q, P )(1)return RCF H(q, {P }, )RCF H(q, SP, hist)(1)if exists P SP s.t. q P every P SP , (q, P, ) hist return(2)else (q, , ) hist return {!}(3)else every P SP local(P ) {q} Resolvent(q, P )(4)if exists P SP s.t. ! local(P ) return {!}(5)else every P SP local(P ) {c local(P )|L(c) arget(P!)}(6)if every P SP every c local(P ), S(c) = !, return P SP local(P )(7)else!(8) result P SP {c local(P )|S(c) arget(P )}(9) foreach P SP c local(P ) s.t. S(c) %= !(10) q c P , P P \{q c}(11) foreach literal l S(c)(12)answer(l) RCF H(l, acq(l, P ), [(q, P, c)|hist])(13) disjcomb (!lS(c) answer(l)) ! {L(c)}(14) result result disjcomb(15) return result279fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonestablish properties algorithm. Theorem 1 states algorithmguaranteed terminate sound. Theorem 2 exhibits conditionacquaintance graph algorithm complete. properties soundnesscompleteness, consider topology content P2PIS changealgorithm running. Therefore, properties following meaningP2PIS: algorithm sound (respectively, complete) iff every P2PIS, resultsreturned RCF (q, P ), P peer P2PIS q literal whose variablebelongs vocabulary P , implicates (respectively, include proper primeimplicates) q w.r.t. union peers P2PIS, changeP2PIS algorithm running.sufficient condition exhibited Theorem 2 completeness algorithmglobal property acquaintance graph: two peers variable commonmust either acquainted (i.e., must share variable) must related pathacquaintances sharing variable. First, important emphasize evenproperty global, checked running algorithm.verified, algorithm remains sound complete. Second, worth noticingmodeling/encoding applications general peer-to-peer propositional reasoningsetting may result acquaintance graphs satisfying global property construction.particular, shown Section 4 (Proposition 2), case propositionalencoding Semantic Web applications deal Somewhere.Theorem 1 Let P peer P2PIS q literal belonging vocabulary P .RCF (q, P ) sound terminates.Proof: Soundness: need prove every result returned RCF (q, P ) belongstarget language implicate q w.r.t. P, P unionpeers P2PIS. so, prove induction number rc recursive callsRCF H(q, SP, hist) every result returned RCF H(q, SP, hist) (where historyhist, empty, form [(ln , Pn , cn ), . . . , (l0 , P0 , c0 )]) implicate q w.r.t.P {ln , . . . , l0 } belongs target language.rc = 0: either one conditions Line (1), Line (2), Line (4) Line (6)satisfied.- condition Line (1) satisfied, algorithm returns empty result.- either exists peer P (q, , ) hist ! local(P ):cases, ! returned algorithm (in respectively Line (2) Line (4))indeed implicate q w.r.t. P {ln , . . . , l0 } belonging target language.- Let r result returned algorithm Line (6): exists P SPr local(P ), obvioulsy implicate q w.r.t. P {ln , . . . , l0 } (as qresolvent q clause P ), belongs target language.Suppose induction hypothesis true rc p, let SP set peersP2PIS q literal (belonging vocabulary peers SP )RCF H(q, SP, hist) requires p + 1 recursive calls terminate. Let r resultreturned RCF H(q, SP, hist).280fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web- r local(P ) P SP S(r) = ! S(r) arget(P ),obviously implicate q w.r.t. P {ln , . . . , l0 } belonging target language.- r % local(P ) P SP , obtained Line (13): exist P SPclause c P form S(c) L(c) S(c) = ll1 llk r = r1rk L(c), every ri result returned RCF H(lli , acq(lli , P ), [(q, P, c)|hist])(Line (12)). According induction hypothesis (the number recursive callsRCF H(lli , acq(lli , P ), [(q, P, c)|hist]) every lli less equal p), everyri belongs target language implicate lli w.r.t. P\{q c}{q, ln , . . . , l0 }, or, equivalently, implicate q w.r.t. P\{q c} {lli , ln , . . . , l0 }.Therefore, r1 rk belongs target language implicate q w.r.t.P\{q c} {S(c), ln , . . . , l0 }. Since L(c) belongs target language c =S(c)L(c), r (i.e, r1 rk L(c)) belongs target language, implicateq w.r.t. P\{q c} {c, ln , . . . , l0 }, fortiori q w.r.t. P {c, ln , . . . , l0 } Sincec local(P ), c implicate q w.r.t. P, therefore r implicate qw.r.t. P {ln , . . . , l0 }.Termination: recursive call, new triple (sl, P, c) added history.algorithm terminate, history would infinite, possible sincenumber peers, literals clauses within P2PIS finite.!following theorem exhibits sufficient condition algorithm complete.Theorem 2 Let = (P, acq) P2PIS. every P , P # v VP VP ! existspath P P # , edges labelled v, every literalq LP , RCF (q, P ) computes proper prime implicates q w.r.t. P belongarget(P).Proof: fact, prove RCF (q, P ) computes least prime proper resolventsq w.r.t. P , i.e., elements Resolvent(q, P ) strictly subsumedelements Resolvent(q, P ).first show proper prime implicates q w.r.t. P prime proper resolventsq w.r.t. P . Let proper prime implicate q w.r.t. P . definition P {q} |=P %|= m. completeness resolution w.r.t. prime implicates, obtainedresolution set P {q} P alone, i.e., proper resolvent q w.r.t.P . Let us suppose strictly subsumed another element m# Resolvent(q, P ).means P {q} |= m# |= % m# , contradicts primeimplicate q w.r.t. P .prove induction maximum number rc recursive calls involvingliteral triggering RCF H(q, SP, hist) RCF H(q, SP, hist) computesproper prime implicates belonging target language q w.r.t. P(hist), P(hist)obtained P replacing li ci li li %= ci . Thus:P(hist) = P hist empty,otherwise:P(hist) = P\{li ci |(li , Pi , ci ) hist s.t. li %= ci } {li |(li , Pi , ci ) hist s.t. li %= ci }281fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonhistory hist empty, form [(ln , Pn , cn ), . . . , (l0 , P0 , c0 )]. Accordingalgorithm, RCF H(q, SP, hist) triggered, least n + 1previous calls algorithm RCF H: RCF H(l0 , SP0 , ) RCF H(li , SPi , histi )[1..n], histi =[(li1 , Pi , ci1 ) . . . , (l0 , P0 , c0 )]), Pi SPi every [0..n].Since P2PIS cyclic, may case call RCF H(q, SP, hist),(q, P, q) hist. case, previous calls RCF H involving q,i.e., form RCF H(q, SPi , histi ).rc = 0: either one conditions Line (1), Line (2), Line (4) Line (6)satisfied.- first condition satisfied, since rc = 0, cannot case everyP SP , (q, P, ) hist, therefore exists P SP q P :case, proper prime implicate q w.r.t. P(hist), q P(hist)prime implicates q w.r.t. theory containing q consequencestheory.- either (q, , ) hist ! local(P ) given peer P SP : cases,! prime implicate q w.r.t. P(hist) therefore, proper primeimplicate, one too. returned algorithm (respectively Line (2)Line (4)).- every P SP , every resolvent q w.r.t. P shared variableacquaintance P : P satisfies property stated theorem, meansevery prime implicate q w.r.t. P variable commontheory P. AccordingLemma 1, set proper resolvents q w.r.t. P(hist)!included P SP local(P ), thus particular every proper prime implicateq w.r.t. P(hist), target language, returned algorithm(Line(6)).Suppose induction hypothesis true rc p, let SP set peersP2PIS satisfying property stated theorem, RCF H(q, SP, hist)requires atmost p + 1 recursive calls involving q. Since least one recursivecall, condition Line (1) satisfied. Let target languageproper prime implicate q w.r.t. P(hist), P(hist) = P\{li ci |(li , Pi , ci )hist s.t. li %= ci } {li |(li , Pi , ci ) hist s.t. li %= ci }. Let us show belongsresult returned RCF H(q, SP, hist).- proper resolvent q w.r.t. given P SP , local(P )returned algorithm since target language.-If proper resolvent q w.r.t. given P SP , then, according!Lemma 1, either (i) q variable !common clauses P(hist)\ P SP P ,(ii) exists! clause q c P SP P c variables commonP(hist)\ P SP P proper resolvent c w.r.t. P(hist)\{q c}{q}. addition, prime proper resolvent c w.r.t. P(hist)\{q c} {q}.Let us suppose case. exists clause m#Resolvent(c, P(hist)\{q c} {q}), m# |= m# % m. soundness,P(hist)\{q c} {q} {c} |= m# . Since P(hist)\{q c} {q} {c} P(hist) {q},282fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic WebP(hist) {q} |= m# m# |= m# % m, contradicts primeimplicate q w.r.t. P(hist).(i) first case, according property stated theorem, variableq shared peers P2PIS SP , therefore q involvediteration loop Line (9). According induction hypothesis (thenumber recursive calls obtain answer(q) Line (12) less equalp) answer(q) includes set proper prime resolvents q w.r.t. P(hist# ),target language, hist# = [(q, P, q)|hist], thus P(hist# ) = P(hist).Therefore, answer(q) includes set proper prime resolvents q w.r.t. P(hist),particular m.(ii) second case, according property stated theorem, c sharesvariables peers P2PIS SP . addition, sincetarget language, local variables c target variables. Therefore cinvolved iteration loop Line (9). According induction hypothesis(the number recursive calls obtain answer(l) Line (12) less equalp), every l S(c), answer(l) includes set proper prime resolventsl w.r.t. P(hist# ) thus particular, set proper prime implicatesl w.r.t. P(hist# ) target language. Since hist# = [(q, P, c)|hist]q %= c (because duplicate literals clauses consider),P(hist# ) = P(hist)\{q c} {q}. apply Lemma 2 infer DisjComp,computed Line (13), includes set proper prime implicates c w.r.t.P(hist)\{q c} {q}, target language, particular m.!Lemma 1 Let P set clauses q literal. Let P # P contains clausesvariable common q. proper resolvent q w.r.t. P , :either proper resolvent q w.r.t. P # ,variable q common clauses P \P # ,exists clause q c P # c variables common clausesP \P # proper resolvent c w.r.t. P \{q c} {q}.Proof: Let proper resolvent q w.r.t. P . different q, existsclause q c P proper resolvent c w.r.t. P {q}. Since P \{q c}{q} P {q}, proper resolvent c w.r.t. P \{q c} {q}.clause exist P # , exists P \P # therefore variable qcommon clauses P \P # .exists clause q c P # proper resolvent c w.r.t.P \{q c} {q}, proper resolvent q w.r.t. P # , everyproof must exist clause c# P \P # either q q c mustresolved. Therefore, either q c variables common clauses P \P # . !283fiAdjiman, Chatalic, Goasdoue, Rousset, & SimonLemma 2 Let P set clauses, let c = l1 ln clause. every properprime implicate c w.r.t. P , exists m1 , . . . , mn m1 mn ,every [1..n], mi proper prime implicate li w.r.t. P .Proof: Let proper prime implicate c w.r.t. P . every literal li , let od(li )set models P make li true. od(li ) = , means !proper prime implicate li w.r.t. P . every od(li ) %= , every model od(li ) model P {c}, model ; therefore,proper implicate li w.r.t. P , and, definition proper prime implicates, existsproper prime implicate mi li w.r.t. P mi |= m. Consequently, existsm1 , . . . , mn m1 mn |= m, every [1..n], mi proper primeimplicate li w.r.t. P (mi may !). Since P {l1 ln } |= m1 mn ,proper implicate l1 ln w.r.t. P , necessarily get m1 mn . !3.2 Message-based Consequence Finding Algorithmsection, exhibit result transformation previous recursive algorithm DeCA: message-based Decentralized Consequence finding Algorithm runninglocally peer. DeCA composed three procedures, one triggeredreception message. procedure ReceiveForthMessage triggeredreception f orth message m(Sender, Receiver, f orth, hist, l) sent peer Senderpeer Receiver executes procedure: demand Sender,shares variable l, processes literal l. procedure ReceiveBackMessagetriggered reception back message m(Sender, Receiver, back, hist, r) sentpeer Sender peer Receiver executes procedure: processes consequence r (which clause variables target variables) sent back Senderliteral l (last added history) ; may combine consequences literals clause l. procedure ReceiveFinalMessagetriggered reception f inal message m(Sender, Receiver, f inal, hist, true):peer Sender notifies peer Receiver computation consequences literall (last added history) completed. procedures handle two data structuresstored peer: cons(l, hist) caches consequences l computed reasoningbranch corresponding hist ; final(q, hist) set true propagation q withinreasoning branch history hist completed.reasoning initiated user (denoted particular peer U ser) sendinggiven peer P message m(U ser, P, f orth, , q). triggers peer P local execution procedure ReceiveForthMessage(m(U ser, P, f orth, , q)). descriptionprocedures, since locally executed peer receives message,denote Self receiver peer.284fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic WebAlgorithm 2: DeCA message passing procedure propagating literals forthReceiveForthMessage(m(Sender, Self, f orth, hist, q))(1) (q, , ) hist(2) send m(Self, Sender, back, [(q, Self, !)|hist], !)(3) send m(Self, Sender, f inal, [(q, Self, true)|hist], true)(4) else q Self (q, Self, ) hist(5) send m(Self, Sender, f inal, [(q, Self, true)|hist], true)(6) else(7) local(Self ) {q} Resolvent(q, Self )(8) ! local(Self )(9)send m(Self, Sender, back, [(q, Self, !)|hist], !)(10) send m(Self, Sender, f inal, [(q, Self, true)|hist], true)(11) else(12) local(Self ) {c local(Self )| L(c) arget(Self )}(13)every c local(Self ), S(c) = !(14)foreach c local(Self )(15)send m(Self, Sender, back, [(q, Self, c)|hist], c)(16)send m(Self, Sender, f inal, [(q, Self, true)|hist], true)(17) else(18)foreach c local(Self )(19)S(c) = !(20)send m(Self, Sender, back, [(q, Self, c)|hist], c)(21)else(22)foreach literal l S(c)(23)l arget(Self )(24)cons(l, [(q, Self, c)|hist]) {l}(25)else(26)cons(l, [(q, Self, c)|hist])(27)final(l, [(q, Self, c)|hist]) f alse(28)foreach RP acq(l, Self )(29)send m(Self, RP, f orth, [(q, Self, c)|hist], l)Algorithm 3: DeCA message passing procedure processing return consequencesReceiveBackMessage(m(Sender, Self, back, hist, r))(1) hist form [(l! , Sender, c! ), (q, Self, c)|hist! ](2) cons(l! , hist) cons (l! , hist) {r}(3) result !lS(c)\{l! } cons(l, hist) ! {L(c) r}(4) hist! = , U U ser else U first peer P ! hist!(5) foreach cs result(6) send m(Self, U, back, [(q, Self, c)|hist!], cs)Algorithm 4: DeCA message passing procedure notifying terminationReceiveFinalMessage(m(Sender, Self, f inal, hist, true))(1) hist form [(l! , Sender, true), (q, Self, c)|hist!](2) final(l! , hist) true(3) every l S(c), final(l, hist) = true(4) hist! = U U ser else U first peer P ! hist!(5) send m(Self, U, f inal, [(q, Self, true)|hist!], true)(6) foreach l S(c)(7)cons(l, [(l, Sender, ), (q, Self, c)|hist! ])285fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonfollowing theorem states two important results: first, message-based distributedalgorithm computes results algorithm Section 3.1, thus, completeconditions Theorem 2 ; second user notified terminationoccurs, crucial anytime algorithm.Theorem 3 Let r result returned RCF (q, P ). P receives user message m(U ser, P, f orth, , q), message m(P, U ser, back, [(q, P, )], r) produced.r last result returned RCF (q, P ), user notified terminationmessage m(P, U ser, f inal, [(q, P, true)], true).Proof: prove induction number recursive calls RCF H(q, SP, hist) that:(1) result r returned RCF H(q, SP, hist), exists P SPP bound send message m(P, S, back, [(q, P, )|hist], r) receiving messagem(S, P, f orth, hist, q),(2) r last result returned RCF H(q, SP, hist), peers P SPbound send message m(P, S, f inal, [(q, P, true)|hist], true), first peerhistory.rc = 0: either one conditions Lines (1), (2), (4) (6) algorithmRCF H(q, SP, hist) satisfied. shown proof Theorem 2conditions Lines (2) (4) satisfied, ! result returned algorithm. condition Line (2) algorithm RCF H(q, SP, hist) correspondscondition Line (1) algorithm ReceiveForthMessage(m(S, P, f orth, hist, q))P SP , triggers sending message m(P, S, back, [(q, P, !)|hist], !)(Line (2)) message m(P, S, f inal, [(q, P, true)|hist], true) (Line(3)). condition Line (4) algorithm RCF H(q, SP, hist) satisfied, exists P SP! P . condition corresponds condition Line (8) algorithmReceiveForthMessage(m(S, P, f orth, hist, q)), triggers sending messagem(P, S, back, [(q, P, !)|hist], !) (Line (9) message m(P, S, f inal, [(q, P, true)|hist], true)(Line (10)). condition (1) algorithm RCF H(q, SP, hist), result returned (see proof Theorem 2), corresponds condition Line (4) algorithmReceiveForthMessage(m(S, P, f orth, hist, q)), every P SP , triggerssending final message (Line (5)). Finally, condition Line (6) algorithm RCF H(q, SP, hist) satisfied, exists P SP r local(P ).condition Line (6) algorithm RCF H(q, SP, hist) corresponds conditionLine (13) ReceiveForthMessage(m(S, P, f orth, hist, q)), triggers sendingmessages m(P, S, back, [(q, P, c)|hist], c), c clause local(P ) (Line (15)),particular message m(P, S, back, [(q, P, r)|hist], r). triggers sendingfinal message (Line (16)) P . r last result returned RCF H(q, SP, hist),final messages sent every P SP .Suppose induction hypothesis true rc p, let = (P, acq) P2PISRCF H(q, SP, hist) requires p + 1 recursive calls terminate.- exists P SP r local(P ), r last result returnedalgorithm, r one clauses c involved iteration loop Line(18) algorithm ReceiveForthMessage(m(S, P, f orth, hist, q)), verifyingcondition Line (19), triggers sending message m(P, S, back, [(q, P, r)|hist], r) (Line (20)).286fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web-If exists P SP clause c : l1 lk L(c) local(P ) cinvolved iteration loop Line (9) algorithm RCF H(q, P, hist), relement r1 rk L(c) (!lS(c) answer(l)) ! {L(c)} computed Line (12),answer(l) obtained result RCF H(l, acq(l, P ), [(q, P, c)|hist]) (Line(13)), requires p less p recursive calls. induction, literal li S(c),exists RPi acq(li , P ) RPi sends message m(RPi , P, back, [(li , RPi , ),(q, P, c)|hist], ri ) received message m(P, RPi , f orth, [(q, P, c)|hist], li ). loopLine (11) algorithm RCF H(q, SP, hist) corresponds loop Line (22)algorithm ReceiveForthMessage(m(S, P, f orth,hist,q)), triggers sendingmessages m(P, RPi , f orth, [(q, P, c)|hist], li ) literal li S(c) (Line (29)).Therefore, according induction hypothesis, every li S(c), RPi sends messagem(RPi , P, back, [(li , RPi , ), (q, P, c)|hist], ri ). last messages (let us saym(RPj , P, back, [(lj , RPj , ), (q, P, c)|hist], rj )) processed, r produced Line (3)ReceiveBackMessage(m(RPj , P , back, [(lj , RPj , ), (q, P, c)|hist], rj )), existspeer U P bound send message m(P, U, back, [(q, P, c)|hist], r) (Line(6)).- r last result returned algorithm RCF H(q, SP , hist), every PSP , every c local(P ), every l S(c), RCF H(l, acq(l, P ), [(q, P, c)|hist])finished, and, induction, every peer RP acq(l, P ) sent message m(RP, P, f inal,[(l, RP, true), (q, P, c)|hist], true). Therefore, condition Line (3) algorithmReceiveFinalMessage(m(RP, P, f inal, [(l, RP, ), (q, P, c)|hist], true)) satisfied,triggers sending message m(P, U, f inal, [(q, P, true)|hist], true) (Line (5)).!sake simplicity, recursive distributed algorithms presentedapplying literals. mean formulas consider limitedliterals. Clauses handled splitting literals using !operator recompose results obtained literal.also important notice ! returned algorithm proper primeimplicate lines (1) (3) (8) (10) ReceiveForthMessage.case, corollary theorems, P2PIS detected unsatisfiableinput clause. Therefore, algorithm exploited checking satisfiabilityP2PIS join new peer theory.4. Application Semantic Web: somewhere Peer-to-peer DataManagement SystemSemantic Web (Berners-Lee, Hendler, & Lassila, 2001) envisions world wide distributed architecture data computational resources easily inter-operate basedsemantic marking web resources using ontologies. Ontologies formalizationsemantics application domains (e.g., tourism, biology, medicine) definition classes relations modeling domain objects properties consideredmeaningful application. concepts, tools techniques deployedfar Semantic Web community correspond big beautiful idea highexpressivity needed describing domain ontologies. result, applied,287fiAdjiman, Chatalic, Goasdoue, Rousset, & Simoncurrent Semantic Web technologies mostly used building thematic portalsscale Web. contrast, Somewhere promotes small beautifulvision Semantic Web (Rousset, 2004) based simple personalized ontologies (e.g.,taxonomies atomic classes) distributed large scale. visionSemantic Web introduced Plu, Bellec, Agosto, van de Velde (2003), userimposes others ontology logical mappings ontologies make possiblecreation web people personalized semantic marking data cohabitsnicely collaborative exchange data. view, Web huge peer-to-peerdata management system based simple distributed ontologies mappings.Peer-to-peer data management systems proposed recently (Halevy et al.,2003b; Ooi, Shu, & Tan, 2003; Arenas, Kantere, Kementsietsidis, Kiringa, Miller, & Mylopoulos, 2003; Bernstein, Giunchiglia, Kementsietsidis, Mylopoulos, Serafini, & Zaihraheu,2002; Calvanese et al., 2004) generalize centralized approach information integration systems based single mediators. peer-to-peer data management system,central mediator: peer ontology data services, mediatepeers ask answer queries. existing systems vary according(a) expressive power underlying data model (b) way different peerssemantically connected. characteristics impact allowed queriesdistributed processing.Edutella (Nejdl et al., 2002), peer stores locally data (educational resources)described RDF relative reference ontologies (e.g., Dmoz - http://dmoz.org).instance, peer declare data related concept dmoz taxonomy corresponding path Computers/Programming/Languages/Java,data export author date properties. overlay network underlying Edutella hypercube super-peers peers directly connected.super-peer mediator data peers connected it. queried,first task check query matches schema: case, transmitsquery peers connected it, likely store data answeringquery ; otherwise, routes query neighbour super-peers accordingstrategy exploiting hypercube topology guaranteeing worst-case logarithmic timereaching relevant super-peer.contrast Edutella, Piazza (Halevy et al., 2003b, 2003a) considerdata distributed different peers must described relatively existingreference schemas. Piazza, peer data schema mediatepeers declaring mappings schema schemaspeers. topology network fixed (as hypercube Edutella)accounts existence mappings peers: two peers logically connectedexists mapping two schemas. underlying data model firstversion Piazza (Halevy et al., 2003b) relational mappings relationalpeer schemas inclusion equivalence statements conjunctive queries.mapping formalism encompasses local-as-views global-as-views (Halevy, 2000)formalisms used information integration systems based single mediators. pricepay query answering undecidable except restrictions imposedmappings topology network (Halevy et al., 2003b). currentlyimplemented version Piazza (Halevy et al., 2003a) relies tree-based data model:288fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Webdata XML mappings equivalence inclusion statements XMLqueries. Query answering implemented based practical (but complete) algorithmsXML query containment rewriting. scalability Piazza far go80 peers published experiments relies wide rangeoptimizations (mappings composition, Madhavan & Halevy, 2003, paths pruning, Tatarinov& Halevy, 2004), made possible centralized storage schemas mappingsglobal server.Somewhere, made choice fully distributed: neithersuper-peers (as Edutella) central server global view overlaynetwork (as Piazza). addition, aim scaling thousands peers.make possible, chosen simple class-based data model dataset resource identifiers (e.g., URIs), schemas (simple) definitions classespossibly constrained inclusion, disjunction equivalence statements, mappingsinclusion, disjunction equivalence statements classes different peer schemas.data model accordance W3C recommendations since capturedpropositional fragment OWL ontology language (http://www.w3.org/TR/owlsemantics). Note OWL makes possible, declarative import mechanism,retrieve ontologies physically distributed. Using transitive mechanism peerdata management systems amounts worst case centralized single peerwhole set peer ontologies, reason locally. feeling large networksmechanism cannot scale satisfactorily. Moreover, dynamicitypeer-to-peer settings, imports would re-actualized time peerjoins quit network.Section 4.1 defines Somewhere data model, illustrative examplegiven Section 4.2. Section 4.3, show query rewriting Somewhere, thusquery answering, reduced propositional encoding distributed reasoningpropositional logic.4.1 Somewhere Data modelSomewhere, new peer joins network peers knows (its acquaintances) declaring mappings ontology ontologiesacquaintances. Queries posed given peer using local ontology. answersexpected instances local classes possibly instances classespeers distant queried peer infered peer ontologiesmappings satisfy query. Local ontologies, storage descriptions mappingsdefined using fragment OWL DL description logic fragmentOntology Web Language recommended W3C. call OWL PL fragment OWLDL consider Somewhere, PL stands propositional logic. OWL PLfragment OWL DL reduced disjunction, conjunction negation constructorsbuilding class descriptions.4.1.1 Peer ontologiespeer ontology made set class definitions possibly set equivalence,inclusion disjointness axioms class descriptions. class description either289fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonuniversal class (/), empty class (), atomic class union (1), intersection (2)complement () class descriptions.name atomic classes unique peer: use notation P :A identifying atomic class ontology peer P . vocabulary peer P setnames atomic classes.Class descriptionsLogical notationuniversal class/empty classP :Aatomic classconjunctionD1 2 D2disjunctionD1 1 D2negationOWL notationhingN othingclassIDintersectionOf (D1 D2)unionOf (D1 D2)complementOf (D)Axioms class definitionsLogical notationOWL notationCompleteP :AClass(P :A complete D)PartialP :A 3Class(P :A partial D)Axioms class descriptionsLogical notationOWL notationequivalenceD1 D2EquivalentClasses(D1 D2)inclusionD1 3 D2SubClassOf (D1 D2)disjointnessD1 2 D2DisjointClasses(D1 D2)Taxonomies atomic classes (possibly enriched disjointness statementsatomic classes) particular cases allowed ontologies Somewhere . specification made set inclusion (and disjointness) axioms involving atomic classes only:class definition using (conjunction, disjunction negation) constructors.4.1.2 Peer storage descriptionsspecification data stored locally peer P done declarationatomic extensional classes defined terms atomic classes peer ontology,assertional statements relating data identifiers (e.g., URIs) extensional classes.restrict axioms defining extensional classes inclusion statementsatomic extensional class description combining atomic classes ontology.impose restriction order fit Local-as-Views approach open-worldassumption within information integration setting (Halevy, 2000). usenotation P :V iewA denote extensional class V iewA peer P .Storage descriptiondeclaration extensional classes:OWL notationLogical notationP :V iewA 3 CSubClassOf (P :V iewA C)assertional statements:OWL notationLogical notationP :V iewA(a)individual(a type(P :V iewA))290fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web4.1.3 MappingsMappings disjointness, equivalence inclusion statements involving atomic classesdifferent peers. express semantic correspondence may existontologies different peers.4.1.4 Schema Somewhere networkSomewhere network, schema centralized distributed uniondifferent peer ontologies mappings. important point peerpartial knowledge schema: knows local ontology mappingsacquaintances.Let P Somewhere peer-to-peer network made collection peers {Pi }i=1..n .peer Pi , let Oi , Vi Mi sets axioms defining respectively localontology Pi , declaration extensional classes set mappings stated Piclasses Oi classes! ontologies acquaintances Pi . schemaP, denoted S(P), union i=1..n Oi Vi Mi ontologies, declarationextensional classes sets mappings peers.4.1.5 Semanticssemantics isis standard semantics first order logic defined terms interpretations. interpretation pair (I , .I ) non-empty set, called domaininterpretation, .I interpretation function assigns subset everyclass identifier element every data identifier.interpretation model distributed! schema Somewhere peer-topeer network P = {Pi }i=1..n iff axiom i=1..n Oi Vi Mi satisfied I.Interpretations axioms rely interpretations class descriptions inductivelydefined follows:/I = , =(C1 1 C2 )I = C1I C2I(C1 2 C2 )I = C1I C2I(C)I = \CAxioms satisfied following holds:C 3 satisfied iff C DIC satisfied iff C = DIC 2 satisfied iff C =Somewhere peer-to-peer network satisfiable iff (distributed) schemamodel.Given Somewhere peer-to-peer network P = {Pi }i=1..n , class description C subsumes class description iff model S(P) DI C .4.2 Illustrative Exampleillustrate Somewhere data model small example four peers modeling fourpersons Ann, Bob, Chris Dora, bookmarking URLs restaurantsknow like, according taxonomy categorizing restaurants.291fiAdjiman, Chatalic, Goasdoue, Rousset, & SimonAnn, working restaurant critic, organizes restaurant URLs accordingfollowing classes:class Ann:G restaurants considered offering good cooking, amongdistinguishes subclass Ann:R rated: Ann:R 3 Ann:Gclass Ann:R union three disjoint classes Ann:S1, Ann:S2, Ann:S3 corresponding respectively restaurants rated 1, 2 3 stars:Ann:R Ann:S1 1 Ann:S2 1 Ann:S3Ann:S1 2 Ann:S2 Ann:S1 2 Ann:S3Ann:S2 2 Ann:S3classes Ann:I Ann:O, respectively corresponding Indian Orientalrestaurantsclasses Ann:C, Ann:T Ann:V subclasses Ann:O denoting Chinese, Ta Vietnamese restaurants respectively: Ann:C 3 Ann:O, Ann:T 3 Ann:O,Ann:V 3 Ann:OSuppose data stored Ann accepts make available deals restaurants various specialties, rated 2 stars among rated restaurants. extensional classes declared Ann then:Ann:V iewS2 3 Ann:S2, Ann:V iewC 3 Ann:C, Ann:V iewV 3 Ann:V ,Ann:V iewT 3 Ann:T , Ann:V iewI 3 Ann:IBob, fond Asian cooking likes high quality, organizes restaurant URLsaccording following classes:class Bob:A Asian restaurantsclass Bob:Q high quality restaurants knowsSuppose wants make available every data stored. extensionalclasses declares Bob:V iewA Bob:V iewQ (as subclasses Bob:A Bob:Q):Bob:V iewA 3 Bob:A, Bob:V iewQ 3 Bob:QChris fond fish restaurants recently discovered places servingnice cantonese cuisine. organizes data respect following classes:class Chris:F fish restaurants,class Chris:CA Cantonese restaurantsSuppose declares extensional classes Chris:V iewF Chris:V iewCA subclasses Chris:F Chris:CA respectively:Chris:V iewF 3 Chris:F , Chris:V iewCA 3 Chris:CADora organizes restaurants URLs around class Dora:DP preferred restaurants, among distinguishes subclass Dora:P pizzerias subclassDora:SF seafood restaurants.Suppose URLs stores concerns pizzerias: extensional classdeclare Dora:V iewP subclass Dora:P : Dora:V iewP 3Dora:PAnn, Bob, Chris Dora express know using mappingsstating properties class inclusion equivalence.292fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic WebAnn confident Bobs taste agrees include Bob selection goodrestaurants stating Bob:Q 3 Ann:G. Finally, thinks Bobs Asian restaurantsencompass Oriental restaurant concept: Ann:O 3 Bob:ABob knows calls Asian cooking corresponds exactly Ann classifiesOriental cooking. may expressed using equivalence statement : Bob:AAnn:O (note difference perception Bob Ann regarding mappingsBob:A Ann:O)Chris considers calls fish specialties particular case Dora seafoodspecialties: Chris:F 3 Dora:SFDora counts Ann Bob obtain good Asian restaurants : Bob:A 2 Ann:G3 Dora:DPFigure 2 describes resulting overlay network. order alleviate notations,omit local peer name prefix except mappings. Edges labeled classidentifiers shared mappings peers.Doraontology :DP 3 /,P 3 DP , SF 3 DP,V iewP 3 Pmappings :Bob:A 2 Ann:G 3 Dora:DPAnn:GBob:ADora:SFChrisontology :F 3 /, CA 3 /,V iewF 3 F ,V iewCA 3 CAmappings :Chris:F 3 Dora:SFBobontology :3 /, Q 3 /,V iewA 3 A,V iewQ 3 Qmappings :Bob:A Ann:OBob:Q,Bob:A,Ann:OAnnontology :G 3 /, 3 /, 3 /,R 3 G,(S1 1 S2 1 S3) R,S1 2 S2 ,S1 2 S3 ,S2 2 S3 ,(C 1 V 1 ) 3 O,V iewC 3 C,V iewV 3 V ,V iewT 3 ,V iewI 3 I,V iewS2 3 S2mappings :Ann:O 3 Bob:A,Bob:Q 3 Ann:GFigure 2: restaurants network4.3 Query Rewriting Somewhere Propositional EncodingSomewhere, user interrogates peer-to-peer network one peerchoice, uses vocabulary peer express query. Therefore, querieslogical combinations classes given peer ontology.corresponding answer sets expressed intention terms combinationsextensional classes rewritings query. point extensional classes293fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonseveral distant peers participate rewritings, thus answer queryposed given peer.Definition 6 (Rewritings) Given Somewhere peer-to-peer network P = {Pi }i=1..n ,logical combination Qe extensional classes rewriting query Q iff Q subsumes Qew.r.t. P.Qe proper rewriting exists model S(P) QIe %= . Qeconjunctive rewriting rewriting conjunction extensional classes.Qe maximal (conjunctive) rewriting exist another (conjunctive) rewriting Q#e Q (strictly) subsuming Qe w.r.t. P.general, finding answers peer data management system critical issue(Halevy et al., 2003b). setting however, case answersobtained using rewritings query: shown (Goasdoue & Rousset, 2004)query finite number maximal conjunctive rewritings, answers(a.k.a. certain answers) obtained union answer sets rewritings.query answering point view, notion proper rewriting relevantguarantees non empty set answers. query proper rewriting,wont get answer.Somewhere setting, query rewriting equivalently reduced distributedreasoning logical propositional theories straighforward propositional encodingquery distributed schema Somewhere network. consists transforming query schema statement propositional formula using class identifierspropositional variables.propositional encoding class description D, thus query, propositional formula P rop(D) obtained inductively follows:P rop(/) = true, P rop() = f alseP rop(A) = A, atomic classP rop(D1 2 D2 ) = P rop(D1 ) P rop(D2 )P rop(D1 1 D2 ) = P rop(D1 ) P rop(D2 )P rop(D) = (P rop(D))propositional encoding schema Somewhere peer-to-peer network Pdistributed propositional theory P rop(S) made formulas obtained inductivelyaxioms follows:P rop(C 3 D) = P rop(C) P rop(D)P rop(C D) = P rop(C) P rop(D)P rop(C 2 ) = P rop(C) P rop(D)on, simplicity, use propositional clausal form notationqueries Somewhere peer-to-peer network schemas. illustration, let us considerpropositional encoding example presented Section 4.2. applicationtransformation rules, conversion produced formula clausal form suppressiontautologies, obtain (Figure 3) new acquaintance graph peer schemadescribed propositional theory.state two propositions showing query rewriting Somewherereduced consequence finding P2PIS presented previous sections.294fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic WebDora : :Dora:V iewP Dora:PDora:P Dora:DPDora:SF Dora:DPBob:A Ann:G Dora:DPAnn:GBob:ADora:SFBob :Bob:V iewA Bob:ABob:V iewQ Bob:QBob:A Ann:OAnn:O Bob:AChris :Chris:V iewF Chris:FChris:V iewCA Chris:CAChris:F Dora:SFBob:Q,Bob:A,Ann:OAnn :Ann:S1 Ann:S2Ann:S1 Ann:S3Ann:S2 Ann:S3Ann:S1 Ann:RAnn:S2 Ann:RAnn:S3 Ann:RAnn:R Ann:S1 Ann:S2 Ann:S3Ann:V iewS2 S2Ann:R Ann:GBob:Q Ann:GAnn:O Bob:AAnn:C Ann:OAnn:V Ann:OAnn:T Ann:OAnn:V iewC Ann:C Ann:V iewV Ann:VAnn:V iewT Ann:TAnn:V iewI Ann:IChris:CA Ann:CChris:CAFigure 3: propositional encoding restaurant networkProposition 1 states propositional encoding transfers satisfiability establishes connection proper (maximal) conjunctive rewritings proper (prime)implicates.Proposition 2 states P2PIS resulting propositional encoding Somewhere schema fulfills construction property exhibited Theorem 2 sufficientcondition completeness algorithms described Section 3 compute properprime implicates clause w.r.t distributed propositional theories.Proposition 1 Let P Somewhere peer-to-peer network let P rop(S(P))propositional encoding schema. Let set extensional classes.S(P) satisfiable iff P rop(S(P)) satisfiable.qe proper maximal conjunctive rewriting query q iff P rop(qe) properprime implicate P rop(q) w.r.t. P rop(S(P)) variables extensionalclasses.Proof: first exhibit properties used proof proposition. LetP Somewhere network, S(P) schema P rop(S(P)) propositional encoding.interpretation S(P), element domain interpretation ,define interpretation po (I) P rop(S(P)) follows: every propositional variablev P rop(S(P)) (v name atomic class S(P)), v po (I) = true iff v .interpretation J P rop(S(P)), define interpretation i(J) S(P) follows:domain i(J) {true}, every atomic class S(P) (A namepropositional variable P rop(S(P))), AJ = true Ai(J) = {true} else Ai(J) = .easy show following properties, every interpretation S(P) (and every), every interpretation J P rop(S(P)):1. every class description C S(P):295fiAdjiman, Chatalic, Goasdoue, Rousset, & Simona) C P rop(C)po (I) = trueb) true C i(J) P rop(C)J = true2. model S(P) po (I) model P rop(S(P))3. i(J) model S(P) J model P rop(S(P)).Suppose S(P) satisfiable let model let element: according Property 2, po (I) model P rop(S(P)), thusP rop(S(P)) satisfiable. converse way, let J propositional modelP rop(S(P)). According Property 3, i(J) model S(P), thusS(P) satisfiable.Suppose P rop(qe ) proper prime implicate P rop(q) w.r.t. P rop(S(P)),variables extensional classes, let us show qe propermaximal conjunctive rewriting q.Let us first show qe rewriting q. Suppose case:exists interpretation S(P) qeI % q thus elementqeI % q . According Property 2, po (I) model P rop(S(P)),according Property 1.a, (P rop(qe ))po (I) = true (P rop(q))po (I) %= true, i.e.,(P rop(qe ))po (I) = f alse (P rop(q))po (I) = true. impossible sincewould contradict fact P rop(qe ) proper prime implicate {P rop(q)}P rop(S(P)). Therefore, qe must rewriting q.Let us show qe conjunctive rewriting q, i.e., qe conjunctionextensional classes. S(P), extensional classes appear inclusion axioms.Therefore, propositional encoding S(P) ensures extensional classesappear variables P rop(S(P)) negative literals. Resolution soundcomplete prime implicate computation, P rop(qe ) obtained resultfinite chain resolutions, starting clauses P rop(S(P)) P rop(q).Since P rop(S(P)) extensional classes appear negative literals,appear negative literals computed implicates, particular P rop(qe).Therefore, qe conjunction extensional classes.Let us show qe proper rewriting q, i.e., satisfiablemodel S(P). Since P rop(qe) proper prime implicate P rop(q) w.r.t.P rop(S(P)), exists model J P rop(S(P)) s.t. P rop(qe )J = f alsethus P rop(qe )J = true. Property 3, i(J) model S(P) accordingProperty 1.b, true (qe )i(J) . Therefore exists model S(P )qe satisfiable, thus qe proper rewriting q.Finally, let us show qe maximal conjunctive rewriting q. Supposecase. Then, exists conjunctive rewriting qe# qqe |= qe# qe % qe# . means exists interpretationelement q # Ie , thus q , % qeI . According Property 1.a, P rop(qe# )po (I) = true, P rop(q)po (I) = true, P rop(qe )po (I) = f alse, i.e.,P rop(qe# )po (I) = f alse, P rop(q)po (I) = f alse, P rop(qe )po (I) = true.296fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Webimpossible since contradicts P rop(qe ) prime implicate P rop(q) w.r.t.P rop(S(P)). Therefore, qe maximal conjunctive rewriting q.Let us prove converse direction. Suppose qe proper maximal conjunctive rewriting query q let us show hat P rop(qe) proper prime implicateP rop(q) w.r.t. P rop(S(P)). definition proper rewriting, every modelS(P) qeI q , equivalently (q)I (qe )I exists model # S(P)!qeI %= .Let us first show P rop(qe ) implicate P rop(q) w.r.t P rop(S(P)). Suppose case, i.e., {P rop(q)}P rop(S(P)) %|= P rop(qe ). Then,exists model J {P rop(q)} P rop(S(P)) P rop(qe )J = f alse.According Property 3, i(J) model S(P). AccordingProperty 1.b, true (q)i(J) (qe )i(J) = . impossible since contradicts(q)i(J) (qe )i(J) . Therefore, P rop(qe ) implicate P rop(q) w.r.t.P rop(S(P)).Let us show P rop(qe ) proper implicate P rop(q) w.r.t. P rop(S(P)),i.e., P rop(qe) implicate P rop(S(P)) alone. definition proper!!rewriting, exists model # S(P) qeI %= . Let element qeI .According Property 2, po (I # ) model P rop(S(P)), according Property!!1.a, (P rop(qe ))po (I ) = true, i.e., (P rop(qe ))po (I ) = f alse. Therefore, P rop(qe )implicate P rop(S(P)).Finally, let us show P rop(qe ) prime implicate P rop(q) w.r.t. P rop(S(P)).Let us show c clause P rop(S(P )) {P rop(q)} |= c c |=P rop(qe ), c P rop(qe ). Since c |= P rop(qe ) P rop(qe ) disjunction negation extensional classes, c disjunction subset literals P rop(qe ). Let qe# conjunction extensional classes c,c = P rop(qe# ). proved previously P rop(qe# ) implicateP rop(q) w.r.t. P rop(S(P )) qe# rewriting q, similarly P rop(qe )implicate P rop(qe# ) w.r.t. P rop(S(P )) qe# subsumes qe . Therefore, qe#rewriting qe subsumes qe . Since qe maximal conjunctive rewritingq, qe# qe , thus qe# qe P rop(qe# ) P rop(qe ), i.e. c P rop(qe ).!Proposition 2 Let P Somewhere peer-to-peer network let P rop(S(P))propositional encoding schema. Let = (P rop(S(P)), acq) correspondingP2PIS, set labelled edges acq that: (P:A, P, P) acq iff P:Ainvolved mapping P P (i.e, P = P P = P). every P , P #v VP VP ! exists path P P # edges labelledv.Proof: Let P P # two peers variable v common. Since vocabularieslocal ontologies different peers disjoint, v necessarily variable P ## :A involved297fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonmapping declared P acquaintances P1 (and thus P ## = P P ## =P1 ), P # acquaintances P1# (in case P ## = P # P ## = P1# ).- v form P ## :A P ## = P (respectively P ## = P # ), P :A atomicclass P (respectively P # :A atomic class P # ) involved mappingP P # , therefore, edge (and thus path) P P # labelledv (P :A P # :A respectively) .- v form P ## :A P ## distinct P P # , P ## :A atomicclass P ## , involved mapping P ## P mappingP ## P # . Therefore, exists edge P ## P labelled v edgeP ## P # labelled v, thus path P P # edgeslabelled v.!two propositions, follows message-based distributed consequencefinding algorithm Section 3.2 used compute maximal conjunctive rewritingsquery. algorithm computes set proper prime implicates literal w.r.t.distributed propositional clausal theory. Therefore, applied distributed theoryresulting propositional encoding schema Somewhere network,extensional classes symbols target variables, triggered literal q, computesfact negation maximal conjunctive rewritings atomic query q. resultalso holds arbitrary query since, setting, maximal rewritingsquery obtained combining maximal rewritings atomic components.corollary two propositions that, setting, query answering BH2 complete w.r.t. query complexity polynomial w.r.t. data complexity.5. Experimental Analysissection devoted experimental study performances distributedconsequence finding algorithm described Section 3, deployed real peer-to-peerinference systems. Particularly, aim experiments study scalability issuesSomewhere infrastructure Semantic Web. experimentsperformed networks 1000 peers. goal thus study practical complexityreasoning networks size answer questions as: deepwide reasoning spread network peers? network copetraffic load? fast results obtained? extent results integrateinformation distinct peers? etc.far, large real corpus distributed clausal theories still missing. Since deployingnew real applications scale requires significant amount time, chosenperform experiments artificially generated instances peer-to-peer networks.instances characterized size form local theories correspondingpeer network, well size topology acquaintancegraph.Since aim use Somewhere infrastructure Semantic Web applications,focus benchmarking instances suitable characteristics. particular generate local theories supposed encode realistic ontologies mappings, could298fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Webwritten people categorize data. Acquaintances peers generatedway topology resulting graph looks realistic respectacquaintances people Web. Therefore focus generation acquaintance graphs so-called small world property, admitted (Newman, 2000)general property social networks (including Web).following, first detail Section 5.1 generation process benchmarkinstances, involved parameters allowed vary experiments. Section 5.2, first series experiments studies hardness local reasoningwithin single peer evaluating number size computed proper prime implicates. allows us realize intrinsic complexity task thus,reasoning large scale networks theories. also helps us justify choiceparameter values experiments. Finally, Section 5.3 reports experimental results obtained concerning scalability Somewhere largenetworks peers.5.1 Benchmark GenerationGenerating satisfactory instances peer-to-peer networks framework meansgenerating realistic propositional theories peer, well appropriate structureacquaintance graph. latter induced variables shared peertheories. setting Semantic Web, correspond names atomic classesinvolved mappings peer ontologies.5.1.1 Generation local theoriesmake following assumptions ontologies mappings likelydeployed large scale future Semantic Web: ontologies taxonomiesatomic classes (possibly disjointness statements pairs atomic classes) ;mappings ontologies likely state simple inclusion equivalencetwo atomic classes two different peers, want excludecomplex mappings involving logical combination classes.consequence, propositional encoding taxonomy results set clauseslength 2. mappings result well clauses length 2. complex mappingsmight result longer clauses, since set clauses may equivalently rewrittenset clauses length 3, restrict case encoded clauseslength 3. Clauses encoding mappings (called mapping clauses) thus clauseslength 2 3, (2-clauses 3-clauses short). denote %3cnf ratio3-clauses total number mapping clauses. ratio reflects way degreecomplexity mappings. experiments study variations parametersignificant impact hardness reasoning.local theories generated two steps. first generate set random2-clauses number n propositional variables randomly select number (t n)target variables (corresponding names extensional classes). Mapping clausesgenerated, according given value %3cnf added theories. Sincemapping clauses induce structure acquaintance graph, way generateddiscussed below. Peer theories thus composed 2-clauses 3-clauses.299fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonliterature propositional reasoning, theories correspond so-called 2 + p formulas,p denotes proportion 3-clauses whole set clauses (note p%3cnf different ratios).5.1.2 Generation acquaintance graphorder focus realistic acquaintance graphs, chosen generate randomgraphs small worlds properties, proposed studied Watts Strogatz(1998), well Newman (2000). graphs two properties encounteredsocial networks: first, short path length pair nodes, observed socialrelationship (for instance, widely accepted six-degrees separation humans)and, second, high clustering ratio, measurement number common neighborsshared two neighbors (for instance, likely two friends share common subsetfriends).generate graphs, first generate pairs peers connected. Following work Watts Strogatz (1998), start called k-regular ringstructure np nodes, i.e., graph nodes may arranged ring,node connected k closest neighbors. Edges graphrandomly rewired, given probability pr, replacing one (or both) connectedpeers another peer. shown regular graphs (pr = 0)uniform random graphs (pr = 1), graphs generated pr = 0.1 small worldproperties. acquaintance graphs used experiments generatedway, pr = 0.1. Moreover, since aim evaluate scalability approach,chosen focus networks significant size. instances, fixednumber peers np 1000 number k edges per peer 10.topology network generated, local theories peergenerated. Portion theories encoding taxonomies first generated previouslydescribed. Then, edge generated graph mapping clauses added.simplicity, chosen add fixed number q mapping clauses edge.Mapping clauses randomly generated picking one variable two peerstheories negating probability 0.5. %3cnf probability, thirdliteral (randomly chosen two peers) added clause produce mappingclauses length 3. consequence, average number variables shared twoconnected peers (2 + %3cnf ) q.5.2 Hardness Local Reasoning within Single Peersetting, first part reasoning performed peer consists computingproper prime implicates received litteral w.r.t. local theory peer.order grasp hardness local reasoning first conducted experimentalstudy evaluate number form implicates, also, since localtheories 2 + p clausal theories, evaluate impact ratio p values.experiments performed using modified version Zres (Simon & del Val,2001).Prime implicates already studied 3-CNF random formulas (Schrag &Crawford, 1996). corresponds case p = 100%. first take300fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web3CNF m=30 clauses, 1500 experiments3CNF m=30 clauses, 1500 experiments1700Number prime implicates600500Cumulative Distribution Function (CDF)n=12 varsn=14 varsn=16 varsn=18 varsn=20 varsn=22 varsn=24 varsn=26 varsn=28 vars4003002001000.90.80.7n=12 varsn=16 varsn=20 varsn=24 varsn=28 vars0.60.50.40.30.20.100246810001012Size prime implicates110210310410510610Total number literalsFigure 4: Prime Implicates uniform random 3-CNF theoryreference comparison proper prime implicates. consider 3-CNF theories= 30 clauses n variables (n ranging 12 28). left part Figure 4 presentscharacteristics prime implicates different values n. curve describesprime implicates distribute according length. curves correspond averagevalues 1500 experiments. instance, n = 28 variables, average680 prime implicates length 7. right part Figure 4 describes sizewhole set prime implicates means cumulative distribution functiontotal number L literals. point (x, y) curve must readN = 1500 runs, y.N led L value smaller x. representationconvenient exhibiting exponential distributions. point stresssmall formulas, median value size whole set prime implicatesalready reaches ten thousand literals. rare runs (less 5%),set prime implicates one hundred thousand literals. also observeaugmenting number variables increases difficulty since results longerlonger prime implicates produced final result. Note simple random3CNF formula 30 clauses may already generate thousands prime implicates. Sincecomputing implicates random formulas known require lots subsumption checks(Simon & del Val, 2001), final set prime implicates may hard obtainpractice.results new (Schrag & Crawford, 1996), interesting compareobtained similar conditions computing proper prime implicates,described Figure 5. observe curve shapes similar previouslyobtained values one order magnitude smaller. Note n = 28,median value size whole set proper prime implicates already onethousand literals. similarly, large values n, majority proper prime implicates long. Intuitively, one may explain phenomenon fact proper primeimplicates prime implicates initial theory augmented additional literal.literal presumably induces clauses reductions theory consequencesubsumptions. problem thus becomes simplified much versionprime implicates problem. experiments, first conclusion that, even301fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon3CNF m=30 clauses, 100.n experiments3CNF m=30 clauses, 100.n experiments190Number proper prime implicates807060Cumulative Distribution Function (CDF)n=10 varsn=12 varsn=14 varsn=16 varsn=18 varsn=20 varsn=22 varsn=24 varsn=26 varsn=28 vars504030201000.90.80.7n=12 varsn=16 varsn=20 varsn=24 varsn=28 vars0.60.50.40.30.20.10246810001012110Size proper prime implicates210310410510Total number literalsFigure 5: Proper Prime Implicates uniform random 3-CNF theory2+p CNF m=30 clauses, 1000 experiments2+p CNF m=30 clauses, 1000 experiments1800Number prime implicates700600500Cumulative Distribution Function (CDF)p =0 %p =10 %p =20 %p =30 %p =40 %p =50 %p =60 %p =70 %p =80 %p =90 %p =100 %40030020010000.90.80.70.6p =0 %p =10 %p =20 %p =30 %p =40 %p =50 %p =60 %p =70 %p =80 %p =90 %p =100 %0.50.40.30.20.10246810001012Size prime implicates110210310410510Size prime implicatesFigure 6: Prime Implicates uniform random 2 + p-CNF theory (m = 30, n = 28)small 3-CNF theories, number proper prime implicates may quite large,may quite big.Let us focus experiments local 2 + p CNF theories, smallervalues p, supposed better correspond encoding applications partknowledge structured tree dag (which encoded 2-clauses). Figure6 describes prime implicates 2 + p CNF theory = 30 clauses n = 28variables, values p ranging 0% 100% (the curve corresponding casep = 100% Figure 4). Similarly, Figure 7 describes characteristicsproper prime implicates different values p. previously, observecurves similar shapes. cumulative distribution function (CDF) curves,appears hardness (measured total size prime/proper prime implicates)2 + p CNF formula grows exponentially value p. Even small valuesp problem may hard. p increases, larger larger clauses quickly appearresult.Figure 8 studies characteristics proper prime implicates fixed smallvalue p = 10%, increasing sizes theory number variables n.302fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web2+p CNF m=30 clauses, 100.n experiments2+p CNF m=30 clauses, 100.n experiments1100Number proper prime implicates908070Cumulative Distribution Function (CDF)p =0 %p =10 %p =20 %p =30 %p =40 %p =50 %p =60 %p =70 %p =80 %p =90 %p =100 %60504030200.80.7p =0 %p =10 %p =20 %p =30 %p =40 %p =50 %p =60 %p =70 %p =80 %p =90 %p =100 %0.60.50.40.30.20.11000.90246810001012Size proper prime implicates121031010410Size proper prime implicatesFigure 7: Proper Prime Implicates uniform random 2+p-CNF theory (m = 30, n = 28)2+p CNF p=10%, 1000 experiments2+p CNF p=10%, 100.n experiments10.9Cumulative Distribution Function (CDF)Cumulative Distribution Function (CDF)10.80.70.60.5=30, n =28=40, n =37=50, n =46=60, n =55=70, n =64=80, n =73=90, n =82=100, n =910.40.30.20.101100.950.90.850.8=30, n =28=40, n =37=50, n =46=60, n =55=70, n =64=80, n =73=90, n =82=100, n =910.750.70.650.60.55210310410Size prime implicates0.5010110210310Size proper prime implicatesFigure 8: Size Prime Proper Prime Implicates uniform random 2+p-CNF theoryfixed pchosen small value p focus characteristics larger theories (up= 100 clauses). worth noticing even = 100 p = 10%, problemseems simpler = 30 p = 100% (note right part Figure 6y-axis rescaled [0.5 1]). kinds peer theories seem reasonablebehavior integration perspective: half queries short smallpart queries still hard (the exponential distribution still observed).5.3 Scalability Distributed Reasoningprevious section clearly shown local computation performed peermay really hard, even small simple theories. focusing levelwhole Somewhere networks, splitting/recombining strategy followed distributedconsequence finding algorithm clearly adds another source combinatorial explosion.order evaluate scalability Somewhere networks, performed two kinds303fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonexperiments. first one aims studying behavior Somewhere networksquery processing. consists counting number messages circulating networknumber peers solicited processing query. particular,measured distribution depth query processing well potential widthquery. depth processing (depth short) query maximum lengthreasoning branches developed distributed algorithm returning answerquery. width query measures average number neighbors solicitedpeer reasoning. second kind experiments aims evaluatingprocessing time number answers obtained query.experiments, local theories 1000 peers networkgenerated described Section 5.1, fixed values = n = 70 = 40.numbers close would obtain encoding taxonomies formbalanced trees, depth 3 5, class 2 3sub-classes, extensional classes correspond leaves tree.peer theories contains addition 10 (1 %3cnf ) q mapping clauses length 210 q %3cnf mapping clauses length 3. Since seen Section 5.2proportion 3-clauses local theory strong impact hardness localcomputing performed (sub)query peer, studied variationstwo related parameters: q %3cnf increasing complexity. Note distributedtheories considered experiments quite large since size correspondingcentralized theories ranges 80 000 130 000 clauses 70 000 variables.5.3.1 Behavior Distributed Query ProcessingLet us study impact number q mapping clauses per edge ratio%3cnf mapping clauses length 3, depth queries. purposemeasured, pair (q, %3cnf ), depth 1000 random queries1 . Since know (cf.Section 5.2) local computing may sometimes hard therefore may requirelot time, necessary introduce additional timeout parameter.query thus tagged remaining time live, which, peer, decreasedlocal processing time, propagating induced subqueries. experiments,timeout value set 30 seconds.Figure 9 shows cumulative distribution functions corresponding pair (q, %3cnf ).point figure reports run, distinct query. four leftmost curves correspond cases query depth remains relatively small. instance, q = 2%3cnf = 0 none 1000 queries depth greater 7. Altogetherfour leftmost curves none queries depth greater 36. suggestsalgorithm behaves well networks.value %3cnf increases, queries longer depths. instance,q = 3 %3cnf = 20, observe 22% queries depth greater100 (the maximum 134). three rightmost curves similar shape,composed three phases: sharp growth, corresponding small depth queries, followed1convenience, since time direct issue (except timeout impact),experiments performed single computer, running 1000 peers. made buildingreports relying peer traces much easier.304fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Webm=70, n=70, t=40, 1000 experimentsCumulative Distribution Function (CDF)10.90.80.7q=2, %3cnf=0q=2, %3cnf=20q=3, %3cnf=0q=3, %3cnf=5q=3, %3cnf=10q=3, %3cnf=15q=3, %3cnf=200.60.50.4020406080100120140Depth queriesFigure 9: Cumulative distribution function depth 1000 queries. q numbermapping clauses per edge, %3cnf ratio 3-clauses mappings.scale re-centered [0.4 1.0].plateau, slower growth. small depth query processing plateaucharacteristics exponential distribution values: processing easy,little remaining hard. slow growth observed due timeout, sideeffect bound query depth. Without timeout, previous experimentssuggest would exist queries requiring long reasoning branches.point outlined curve corresponding hardest cases (q = 3 %3cnf = 20)query depth 20 60. suggests hard processingappears, hard. One may notice last case corresponds local theoriesclose one cases studied section 5.2. matter fact, q = 3%3cnf = 20 local theories close 2 + p theories = 100 clausesp = 6%.experiments detail here, checked exponentialdistribution values cannot observed acquaintance graphs strong structure ring (this corresponds rewiring probability p = 0 network generator).exponential distribution observed random graphs (corresponding p = 1), suspect behavior due short path lengthtwo peers, property shared random small world graphs. However, twotypes graphs differ clustering coefficient, property direct impactalgorithm behavior.depth query length history handled algorithm,given peer appear several times since processing subqueries (resulting severalsplitting clauses entailed initial query) solicit peer several times.305fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonm=70, n=70, t=40, 20 000 experimentsCumulative Distribution Function (CDF)10.950.90.850.80.750.70.650.6q=2, %3cnf=0q=3, %3cnf=20q=3, %3cnf=100q=5, %3cnf=1000.550.50510152025Width queriesFigure 10: Cumulative Distribution Function queries width without timeouts.curve summarise 20000 runs. scale re-centered [0.5 1.0],X axis [0 25].also measured integration degree queries, number distinctpeers involved query processing. observed kind exponentialdistributions values depth queries, 20% smaller values.means one fifth history peers repeated ones. phenomenonobserved random acquaintance graphs seems closely related smallworld topology. important point difference small worldrandom graphs could observed large experimental data, large numberpeers.also studied wide reasoning propagates network peersquery processing. purpose evaluated average number neighborspeers solicited peer solving query. estimated valuegenerating 20000 random queries random peers, counting them,number induced subqueries neighbor peers. Figure 10 shows, different pairs(q, %3cnf ), corresponding cumulative distribution functions. instance, q = 2%3cnf = 0, 75% queries solved locally 15% remainingones solved asking one neighbor. q = 5 %3cnf = 100, 25%queries solicit least 10 neighbors. course, 25% subqueries may also solicitleast 10 peers on.summarize, experiments pointed direct impact %3cnf valuehardness query processing, surprising considering hardnessclauses length 3 prime implicates computation. experiments also suggestexponential distribution query depths, due short path length two peers306fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Webacquaintance graphs, important repetition solicited peers, duelarge clustering coefficient small world acquaintance graphs.5.3.2 Time Number Answersreport time performance study algorithm deployed realcluster 75 heterogeneous computers2 . Based observations previous sections,chosen focus 5 differents kinds acquaintance graphs, denoted Easy,Easy, Medium, Hard Hard (see Table 1). One main goals sectionestimate limits processing algorithm faces hard(and even hard) Somewhere networks. Again, experiments, settimeout value 30s.Network1st ans.10th ans.100th ans.1000thans.% timeout#answers%unsatEasyq=2%3cnf = 00.04s (100%)0.06s (14.3%)0.07s0%5.174.4%Easyq=3%3cnf = 201.26s (99.6%)1.37s (25.6%)2.11s (12.7%)4.17s (6.80%)5.56s13.9%3643.64%Mediumq=3%3cnf = 1001.58s (95.6%)0.99s (33.3%)0.84s (27.0%)4.59s (21.2%)14.6s37.5%10063.76%Hardq=5%3cnf = 1001.39s (89.3%)1.13s (12.0%)4.09s (10.7%)11.35s (7.15%)21.23s66.9%10041.84%Hardq = 10%3cnf = 1002.66s (49.7%)5.38s (29.9%)11.0s (9.0%)16.6s (1.80%)27.74s86.9%651.81%Table 1: Characteristics query processing ranging easy hard cases.values reported Table 1 mean values three hundred differentrandom queries. column indicates time needed produce respectively 1st ,10th , 100th 1000th answer query. mean time (in seconds) followedpercentage initial queries taken account average. instance,medium case q = 3, 12.7% queries produced 100 answers,100th answer given average 2.11 seconds (the average takeaccount queries produce least 100 answers). row correspondsmean time needed produce answers, including queries lead timeout,percentage reported %timeout row. last two rows report meannumber answers ratio proved unsatisfiable queries w.r.t. Somewherenetwork (some unsatisfiable queries w.r.t. network may counted sinceinconsistency might found timeout).Unsurprisingly, timeout occurs Easy case. known satisfiabilitychecking prime implicates computation tractable sets clauses length 2.Moreover, high partitioning global theory induced low value q (numbermappings per peer) often witness easy cases reasoning centralized theories.2computers Linux Athlon units, 1GB RAM. 26 ran 1.4GHz, 9 1.8GHz40 2 GHz. computer running around 14 randomly assigned peers307fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonpoint outline case 5 answers average,produced quickly algorithm (in less 0.1 seconds).Let us point that, even Medium Hard instances, algorithm produceslot answers. instance, obtained average 1006 answers q = 3,1004 answers q = 5. addition, hard instances, 90% runs producedleast one answer. noticeable Hard case (q = 10), half queriesproduce least one answer, even 13% complete without timeout. Letus note yet checking satisfiability corresponding centralized theoriesalso hard. matter fact formula corresponding centralized versiondistributed theories n=70 000 variables m=120 000 clauses, 50 000length 3. ratio 3-clauses 2 + p theories thus p = 0.416.shown (Monasson, Zecchina, Kirkpatrick, Selman, & Troyansky, 1999) that,2 + p random formulas, one restrict locality variables, SAT/UNSATtransition continuous p < p0 (p0 = 0.41) discontinuous p > p0 , like 3-SATinstances. Intuitively, p > p0 , random 2 + p-SAT problem shares characteristicsrandom 3-SAT problems, well known canonical NP-Complete problem.Let us recall generation model induces high clustering variables insidepeer. Therefore cannot claim corresponding centralized theories exactlycararacteristics uniform 2+ p-SAT random formulas. However, one focusvalues parameters n, characteristics Hard network,transition phase SAT UNSAT instance occurs m/n=1.69. Here,m/n=1.71, close enough transition phase suspecthard instances may found.summarize, deployed real cluster heterogeneous computers, algorithm scales well. Even Hard instances share characteristicslarge 2 + p-SAT formula crossover 2-SAT/3-SAT SAT/UNSATtransitions, algorithm able return many answers reasonable time.6. Related workSection 6.1, situate work w.r.t. existing work related distributed reasoningdistributed logics, Section 6.2 summarize distinguishing points Somewhere among existing peer data management systems.6.1 Related Work Distributed Reasoningmessage passing distributed algorithm described Section 3 proceedssplitting clauses distributing work corresponding piece clauseappropriate neighbor peers network. idea splitting formulas severalparts may found back called splitting rule natural deduction calculus,introduced middle 1930s two independent works Gentzen (1935, 1969)Jaskowski (1934). algorithm may viewed distributed version OrderedLinear Deduction (Chang & Lee, 1973) produce new target clauses. principleextended Siegel (1987) order produce implicates given clause belongingtarget language, extended first order case Inoue (1992).308fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Webalready pointed differences work approach AmirMcIlraith (2000). peer-to-peer setting, tree decomposition acquaintancegraph possible. addition, shown introductory example,algorithm Amir McIlraith (2000) complete general case properprime implicate computation. However, Goasdoue Rousset (2003) showncompleteness guaranteed family P2PIS peer/super-peers topology.describes encode P2PIS peer/super-peers partitioned propositionaltheory order use consequence finding algorithm Amir McIlraith (2000).model-based diagnosis framework distributed embedded systems (Provan, 2002)based work Amir McIlraith (2000). think benefit approachapply real peer-to-peer setting global knowledge shared.forms distributed reasoning procedures may found multiagent frameworks,several agents try cooperate solve complex problems. Problems addressedway generally decomposed several interacting subproblems,addressed one agent. case Distributed Constraint Satisfaction Problems(DCSP, Yokoo, Durfee, Ishida, & Kuwabara, 1998). Given set variables,associated given domain, set constraints theses variables, problem assign variable one value respective domain, wayconstraints satisfied. distributed case, variable associated agent.Constraints may either concern set variables relevant agent variables relevant different agents. first case, may considered characterizinglocal theory agent, second case may assimilated mapping constraints agents. mapping constraint assigned one agent. problemaddressed DCSP easier problem consequence finding since satisfiability problem, NP-complete. centralized CSP solved using combinationbacktrack search consistency techniques, algorithms used solve DCSP use asynchronous versions backtracking (Yokoo, Durfee, Ishida, & Kuwabara, 1992; Yokoo et al.,1998) consistency techniques (Silaghi, Sam-Haroud, & Faltings, 2000). Basically, agentsproceed exchanging invalid partial affectations, converging globally consistentsolution. Similar ideas may also found distributed ATMS (Mason & Johnson, 1989),agents exchange nogood sets order converge globally consistent set justifications. Let us note contrast peer-to-peer vision, methods aimsharing global knowledge among agents.Probabilistic reasoning bayesian networks (Pearl, 1988) also given rise severaladaptations suited distributed reasoning (e.g., message passing algorithm Pfeffer& Tai, 2005). However problem addressed context different, since consistsupdating set posteriori beliefs, according observed evidence set conditionalprobabilities. conditional probabilities form P (x|u1 , ..., un ) describeprobability event x u1 . . . un observed. describe valueinteractions viewed mappings conjunction literals singleliteral, oriented nature conditioning.Another work using oriented mappings framework distributed first order logic(Ghidini & Serafini, 2000), collection first order theories communicatebridge rules. approach adopts epistemic semantics connectionspeer theories reflected mappings respective domains inter309fiAdjiman, Chatalic, Goasdoue, Rousset, & Simonpretation involved peers. Based work, distributed description logicsintroduced Borgida Serafini (2003) distributed tableau method describedreasoning distributed description logics proposed Serafini Tamilin(2004b). reasoning algorithm implemented Drago (Serafini & Tamilin,2004a) bridge rules restricted inclusion statements atomic concepts.6.2 Related Work Peer Data Management Systemspointed Section 4, Somewhere peer data management systemdistinguishes Edutella (Nejdl et al., 2002) fact needsuper-peers, Piazza (Halevy et al., 2003b, 2003a) requirecentral server global view overlay network.semantic point view, Somewhereuses propositional language mappings correspond unrestricted formulas. semantics standard propositional semantics query answering always decidable. peer data management systemsinvestigated Halevy et al. (2003b) use relational language mappings correspond inclusion statements conjonctive queries. semantics used workstandard FOL semantics, query answering shown undecidablegeneral case. Restricting acyclic mappings renders query answering decidable Piazza,checking property requires global knowledge network topologyperformed central server.peer data management system considered work Calvanese et al. (2004)similar Halevy et al. (2003b) proposes alternative semantics basedepistemic logic. semantics shown query answering always decidable(even cyclic mappings). Answers obtained according semantics correspondsubset would obtained according standard FOL semantics. However,best knowledge, results implemented.systems point view, recent work around coDB peer data management system (Franconi, Kuper, Lopatenko, & Zaihrayeu, 2004) supports dynamic networksfirst step distributed algorithm let node know network topology.contrast, Somewhere node know topology network.KadoP system (Abiteboul, Manolescu, & Preda, 2004) infastructure baseddistributed hash tables constructing querying peer-to-peer warehouses XMLresources semantically enriched taxonomies mappings. mappings considered simple inclusion statements atomic classes. Compared KadoP (andalso Drago, Serafini & Tamilin, 2004a), mapping language dealtSomewhere expressive simple inclusion statements atomic classes.important difference makes Somewhere able combine elements answerscoming different sources answering query, KadoP Drago cannot do.Somewhere implements simpler setting (not implemented) vision peerpeer data management systems proposed Bernstein et al. (2002) relational model.310fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web7. Conclusioncontributions paper theoretical practical. provided firstdistributed consequence finding algorithm peer-to-peer setting, exhibitedsufficient condition completeness. developed P2PIS architectureimplements algorithm first experimental results look promising.architecture used joint project France Telecom, aims enriching peerto-peer web applications reasoning services (e.g., Someone, Plu et al., 2003).far, restricted algorithm deal vocabulary-based target language.However, adapted sophisticated target languages (e.g., implicatesgiven, maximal, length, language based literals variables).done adding simple tag messages encode desired target language.Another possible extension algorithm allow compact representationimplicates, proposed Simon del Val (2001). work relies efficientclause-distribution operator. adapted extending messages algorithmorder send compressed sets clauses instead one clause case right now,without changing deep architecture algorithm.Semantic Web direction, plan deal distributed RDF(S) resourcesshared large scale. RDF(S) (Antoniou & van Harmelen, 2004) W3C standardannotating web resources, think encoded propositonal setting.distributed reasoning direction, plan consider sophisticated reasoningorder deal real multi-agent setting, possible inconsistenciesagents must handled.Acknowledgnentspaper revised extended version ECAI short paper (Adjiman, Chatalic,Goasdoue, Rousset, & Simon, 2004) IJCAI paper (Adjiman, Chatalic, Goasdoue,Rousset, & Simon, 2005).ReferencesAbiteboul, S., Manolescu, I., & Preda, N. (2004). Constructing querying peer-to-peerwarehouses xml resources. Proceedings the2nd International VLDB WorkshopSemantic Web Databases.Adjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2004). Distributedreasoning peer-to-peer setting. de Mantaras, R. L., & Saitta, L. (Eds.), Proceedings ECAI 2004 (16th European Conference Artificial Intelligence), pp. 945946.ECCAI, IOS Press.Adjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2005). Scalabilitystudy peer-to-peer consequence finding. Proceedings IJCAI05 (19th International Joint Conference Artificial Intelligence), pp. 351356. IJCAI.Amir, E., & McIlraith, S. (2000). Partition-based logical reasoning. Proceedings KR00(7th International Conference Principles Knowledge Representation Reasoning), pp. 389400, Breckenridge, Colorado, USA. Morgan Kaufmann Publishers.311fiAdjiman, Chatalic, Goasdoue, Rousset, & SimonAntoniou, G., & van Harmelen, F. (2004). Semantic Web Primer. MIT Press.Arenas, M., Kantere, V., Kementsietsidis, A., Kiringa, I., Miller, R., & Mylopoulos, J.(2003). hyperion project: data integration data coordination. ACMSIGMOD Record, 32 (3), 5358.Berners-Lee, T., Hendler, J., & Lassila, O. (2001). semantic web. Scientific American,284 (5), 3543.Bernstein, P., Giunchiglia, F., Kementsietsidis, A., Mylopoulos, J., Serafini, L., & Zaihraheu,I. (2002). Data management peer-to-peer computing: vision. ProceedingsWebDB 2002 (5th International Workshop Web Databases).Borgida, A., & Serafini, L. (2003). Distributed description logics: Assimilating informationpeer sources. Journal Data Semantics, 1, 153184.Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2004). Logical foundationspeer-to-peer data integration. Deutsch, A. (Ed.), Proceedings PODS 2004 (20thSymposium Principles Database Systems), pp. 241251, Paris, France. ACM.Chang, C. L., & Lee, R. C. (1973). Symbolic Logic Mechanical Theorem Proving.Computer Science Classics. Academic Press.Dechter, R., & Rish, I. (1994). Directed resolution: davis-putnam procedure revisited.Proceedings KR94 (4th International Conference Principles KnowledgeRepresentation Reasoning), pp. 134145, Bonn, Germany. Morgan Kaufmann.del Val, A. (1999). new method consequence finding compilation restrictedlanguages. Proceedings AAAI 1999 (16th National Conference ArtificialIntelligence), pp. 259264, Orlando, FL, USA. AAAI Press / MIT Press.Franconi, E., Kuper, G., Lopatenko, A., & Zaihrayeu, I. (2004). Queries updatescoDB peer peer database system. Proceedings VLDB04 (30th International Conference Large Databases), pp. 12771280, Toronto, Canada. MorganKaufmann.Gentzen, G. (1935). Untersuchungen uber das logisches Schlieen.Zeitschrift, 1, 176210.MathematischeGentzen, G. (1969). Collected Works, edited M.E. Szabo. North-Holland, Amsterdam.Ghidini, C., & Serafini, L. (2000). Frontiers Combining Systems 2, chap. DistributedFirst Order Logics, pp. 121139. No. 7 Studies Logic Computation. ResearchStudies Press Ltd.Goasdoue, F., & Rousset, M.-C. (2003). Querying distributed data distributedontologies: simple scalable approach. IEEE Intelligent Systems, pp. 6065.Goasdoue, F., & Rousset, M.-C. (2004). Answering queries using views: krdb perspectivesemantic web. ACM Transactions Internet Technology (TOIT), 4 (3), 255288.Halevy, A. Y. (2000). Logic-based artificial intelligence, chap. Logic-based techniquesdata integration, pp. 575595. Kluwer Academic Publishers.312fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic WebHalevy, A. Y., Ives, Z., Tatarinov, I., & Mork, P. (2003a). Piazza: data management infrastructure semantic web applications. Proceedings WWW 2003 (12th International World Wide Web Conference), pp. 556567. ACM Press.Halevy, A. Y., Ives, Z. G., Suciu, D., & Tatarinov, I. (2003b). Schema mediation peerdata management systems. Dayal, U., Ramamritham, K., & Vijayaraman, T. M.(Eds.), Proceedings ICDE03 (International Conference Data Engineering), pp.505518, Bangalore, India. IEEE Computer Society.Inoue, K. (1992). Linear resolution consequence finding. Artificial Intelligence, 2-3 (56),301353.Jaskowski, S. (1934). rules suppositions formal logic. Studia Logica, 1, 532.Reprinted in: S. McCall (ed.), Polish Logic 19201939, Clarendon Press, Oxford, pp.232258.Madhavan, J., & Halevy, A. Y. (2003). Composing mappings among data sources.Proceedings VLDB03 (29th International Conference Large Databases),pp. 572583, Berlin, Germany. Morgan Kaufman.Marquis, P. (2000). Handbook Defeasible Reasoning Uncertainty Management Systems, vol 5: Algorithms Defeasible Uncertain Reasoning, Vol. 5, chap. Consequence Finding Algorithms, pp. 41145. Kluwer Academic Publisher.Mason, C., & Johnson, R. (1989). Distributed Artificial Intelligence II, chap. DATMS:framework distributed assumption based reasoning, pp. 293317. Pitman.Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). 2+p-sat:Relation typical-case complexity nature phase transition. RandomStructure Algorithms, 15, 414435.Nejdl, W., Wolf, B., Qu, C., Decker, S., Sintek, M., & al. (2002). Edutella: p2p networkinginfrastructure based rdf. Proceedings WWW 2002 (11th International WorldWide Web Conference), pp. 604615. ACM.Newman, M. E. J. (2000). Models small world. Journal Statistical Physics, 101,819841.Ooi, B., Shu, Y., & Tan, K.-L. (2003). Relational data sharing peer data managementsystems. ACM SIGMOD Record, 23 (2).Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.Pfeffer, A., & Tai, T. (2005). Asynchronous dynamic bayesian networks. Bacchus, F.,& Jaakkola, T. (Eds.), Proceedings UAI05 (21th Conference UncertaintyArtificial Intelligence).Plu, M., Bellec, P., Agosto, L., & van de Velde, W. (2003). web people: dual viewwww. Proceedings WWW 2003 (12th International World Wide WebConference), Vol. Alternate Paper Tracks, p. 379, Budapest, Hungary.Provan, G. (2002). model-based diagnosis framework distributed embedded systems.Fensel, D., Giunchiglia, F., McGuinness, D. L., & Williams, M.-A. (Eds.), Proceedings KR02 (8th International Conference Principles Knowledge Representation Reasoning), pp. 341352. Morgan Kaufmann.313fiAdjiman, Chatalic, Goasdoue, Rousset, & SimonRousset, M.-C. (2004). Small beautiful semantic web. McIlaith, S., Plexousakis, D., & van Harmelen, F. (Eds.), Proceedings ISWC 2004 (3rd InternationalSemantic Web Conference), Vol. 3298 Lectures Notes Computer Science, pp.616, Hiroshima, Japan. Springer Verlag.Schrag, R., & Crawford, J. (1996). Implicates prime implicates random 3-sat. Artificial Intelligence, 81 (1-2), 199222.Serafini, L., & Tamilin, A. (2004a). Drago: Distributed reasoning architecture semantic web. Tech. rep., ITC-IRST.Serafini, L., & Tamilin, A. (2004b). Local tableaux reasoning distributed descriptionlogics.. Haarslev, V., & Moller, R. (Eds.), Proceedings DL04 (InternationalWorkshop Description Logics), Vol. 104 CEUR Workshop Proceedings, Whistler,British Columbia, Canada.Siegel, P. (1987). Representation et utilisation de la connaissance en calcul propositionnel.Ph.D. thesis, Universite dAix-Marseille II, Marseille, France.Silaghi, M.-C., Sam-Haroud, D., & Faltings, B. (2000). Asynchronous search aggregations. Proceedings AAAI 2000 (17th National Conference ArtificialIntelligence), pp. 917922. AAAI Press / MIT Press.Simon, L., & del Val, A. (2001). Efficient consequence finding. Nebel, B. (Ed.), Proceedings IJCAI01 (17th International Joint Conference Artificial Intelligence), pp.359365, Seattle, Washington, USA. Morgan Kaufmann.Tatarinov, I., & Halevy, A. Y. (2004). Efficient query reformulation peer data managementsystems. Proceedings SIGMOD04 (International Conference ManagementData), pp. 539550, New York, NY, USA. ACM Press.Watts, D. J., & Strogatz, S. H. (1998). Collective dynamics small-world networks.Nature, 393, 440442.Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction formalizing distributed problem solving. Proceedings ICDS92 (12thIEEE International Conference Distributed Computing Systems), pp. 614621.Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1998). distributed constraintsatisfaction problem: Formalization algorithms. IEEE Transactions KnowledgeData Engineering, 10 (5), 673685.314fiJournal Artificial Intelligence Research 25 (2006) 119-157Submitted 04/05; published 02/06Learning Real-Time Search: Unifying FrameworkVadim BulitkoGreg LeeBULITKO @ UALBERTA . CAGREGLEE @ CS . UALBERTA . CADepartment Computing ScienceUniversity AlbertaEdmonton, Alberta T6G 2E8, CANADAAbstractReal-time search methods suited tasks agent interacting initiallyunknown environment real time. simultaneous planning learning problems, agentselect actions limited amount time, sensing local part environment centered agents current location. Real-time heuristic search agents select actions usinglimited lookahead search evaluating frontier states heuristic function. repeated experiences, refine heuristic values states avoid infinite loops convergebetter solutions. wide spread settings autonomous software hardware agentsled explosion real-time search algorithms last two decades. potentialuser confronted hodgepodge algorithms, also faces choice control parametersuse. paper address problems. first contribution introduction simple three-parameter framework (named LRTS) extracts core ideas behind many existingalgorithms. prove LRTA*, -LRTA* , SLA*, -Trap algorithms special casesframework. Thus, unified extended additional features. Second, provecompleteness convergence algorithm covered LRTS framework. Third, proveseveral upper-bounds relating control parameters solution quality. Finally, analyzeinfluence three control parameters empirically realistic scalable domains real-timenavigation initially unknown maps commercial role-playing game well routingad hoc sensor networks.1. Motivationpaper, consider simultaneous planning learning problem. One motivating application lies navigation initially unknown map real-time constraints. example,consider robot driving work every morning. Imagine robot newcomer town.first route robot finds may optimal traffic jams, road conditions,factors initially unknown. passage time, robot continues learn eventuallyconverges nearly optimal commute. Note planning learning happen robotdriving therefore subject time constraints.Present-day mobile robots often plagued localization problems power limitations,simulation counter-parts already allow researchers focus planning learningproblem. instance, RoboCup Rescue simulation league (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjou, & Shimada, 1999) requires real-time planning learning multipleagents mapping unknown terrain. Pathfinding done real time various crises, involvingfire spread human victims trapped rubble, progress agents plan.Similarly, many current-generation real-time strategy games employ priori known maps. Fullknowledge maps enables complete search methods A* (Hart, Nilsson, & Raphael,c2006AI Access Foundation. rights reserved.fiB ULITKO & L EE1968) Dijkstras algorithm (Dijkstra, 1959). Prior availability maps allows pathfindingengines pre-compute various data speed on-line navigation. Examples data includevisibility graphs (Woodcock, 2000), influence maps (Pottinger, 2000), space triangulation (Kallmann, Bieri, & Thalmann, 2003), state abstraction hierarchies (Holte, Drummond, Perez, Zimmer,& MacDonald, 1994; Holte, 1996; Botea, Muller, & Schaeffer, 2004) route waypoints (Reece,Krauss, & Dumanoir, 2000). However, forthcoming generations commercial academicgames (Buro, 2002) require agent cope initially unknown maps via explorationlearning game, therefore greatly limit applicability complete searchalgorithms pre-computation techniques.Incremental search methods dynamic A* (D*) (Stenz, 1995) D* Lite (Koenig &Likhachev, 2002) deal initially unknown maps widely used robotics, includingDARPAs Unmanned Ground Vehicle program, Mars rover, mobile robot prototypes (Herbert, McLachlan, & Chang, 1999; Thayer, Digney, Diaz, Stentz, Nabbe, & Hebert, 2000).work well robots movements slow respect planning speed (Koenig, 2004).real-time strategy games, however, AI engine responsible hundreds thousandsagents traversing map simultaneously planning cost becomes major factor. illustrate: even smaller scale six-year old Age Empires 2 (Ensemble-Studios, 1999),60-70% simulation time spent pathfinding (Pottinger, 2000). gives rise followingquestions:1. planning time per move, particularly first-move delay, minimizedagent moves smoothly responds user requests nearly instantly?2. Given real-time execution, local sensory information, initially unknown terrain,agent learn near-optimal path and, time, minimize learning timememory required?rest paper organized follows. first introduce family real-time search algorithms designed address questions. make first contribution defining simpleparameterized framework unifies extends several popular real-time search algorithms.second contribution lies theoretical analysis resulting framework wherein proveconvergence completeness well several performance bounds. Finally, evaluateinfluence control parameters two different domains: single agent navigation unknownmaps routing ad hoc sensor networks. Detailed pseudocode needed reimplement algorithms well follow theorem proofs presented Appendix A. Theorem proofsfound Appendix B.2. Real-time Heuristic Search Related Worksituate survey real-time heuristic search literature framework agent-centeredsearch (Koenig, 2001). traditional off-line search methods first plan path startgoal state move agent along path, agent-centered search interleaves planningexecution. Furthermore, planning restricted areas search space around currentstate agent physical location mobile robot current board positiongame. agent-centered search methods thus satisfy two requirements: (i) times,agent single state changed via taking actions and, therefore, incurringexecution cost; (ii) agent see states around current state.120fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKReal-time heuristic search methods subset agent-centered search methods.distinguished following two additional properties: (i) planning time per moveupper-bounded user-supplied constant (hence real-time); (ii) associate heuristic functionstate (hence heuristic). former requirement motivated need act quicklycontrol tasks flying helicopter (Ng, Coates, Diel, Ganapathi, Schulte, Tse, Berger, & Liang,2004) playing real-time strategy computer game (Buro, 2002). latter property allowsagent avoid infinite cycles map learning proper distances state goalstate. Unlike learning map using planning (Stenz, 1995; Koenig & Likhachev, 2002;Chimura & Tokoro, 1994; Sturtevant, 2005), acquiring high-quality heuristic values lets agentselect right action quickly thereby improving reaction time. heuristic valuesalso allows agent pick next best action primary choice become unavailable(e.g., due road block map). Remarkably, learning state state-action valuesprevailing technique Reinforcement Learning (Sutton, 2005).Learning real-time search algorithms, LRTA* (Korf, 1990), interleave planningexecution on-line decision-making setting. planning time per action executedagent bounded, algorithms used control policies autonomous agents, evenunknown and/or non-stationary environment (Koenig & Simmons, 1998; Koenig, 1999; Ishida& Korf, 1991, 1995; Ishida, 1997). particular, ability make decisions smallscale local search makes algorithms well suited routing large-scale wireless networkssimple sensors actuators node aware nearby neighbors globalmap exists. scenarios, limited amount computation per node suitlow computing power energy requirements, also network collectively learnstopology time (Yu, Govindan, & Estrin, 2001; Shang, Fromherz, Zhang, & Crawford, 2003).Since pioneering LRTA* (Korf, 1990), research field learning real-time heuristicsearch developed several major directions. Ishida Korf (1991) investigated modifications LRTA* non-stationary environments. Shimbo Ishida (2003) studied convergencesuboptimal solutions well mechanisms bounding amount state space exploration.Furcy Koenig (2000) considered different learning mechanism speeding convergence.Russell Wefald (1991) researched decision-theoretic approach balancing partial planningexecution. Shue Zamani (1993) Bulitko (2004) proposed backtracking componentsuggesting yet another way control exploration environment.Note original LRTA* algorithm viewed special case real-time dynamic programming (Barto, Bradtke, & Singh, 1995), general, real-time heuristic search methodsseveral notable differences reinforcement learning methods. First, usually assumedeterministic environment thus take advantage aggressive value update rules. Second, employ non-trivial initial heuristics converge even faster heuristicadmissible (Pearl, 1984) never decreasing heuristic values states. Third, real-time heuristicsearch methods often use extensive sophisticated local search methods -greedycontrol policy commonly used reinforcement learning. Examples include dynamically selectedlookahead search space DTA* (Russell & Wefald, 1991), additional heuristic tie-breakingFALCONS (Furcy & Koenig, 2000), heuristic upper-bounds safe space explorationbounded LRTA* (Shimbo & Ishida, 2003). improves quality decision-making compensating inaccuracies heuristic function speeds learning process. Finally,backtracking extensions (Shue & Zamani, 1993; Bulitko, 2004) give agent another mechanismmaintain consistency heuristic.121fiB ULITKO & L EEmultitude learning real-time search algorithms (LRTA*, -LRTA* , -LRTA*, FALCONS, eFALCONS, SLA*, -Trap, SLA*T, DTA*, etc.) available user disorientingsince decide algorithm use also algorithm parametersmajor impact performance. problem complicated factempirical studies done different test beds making results incomparable directly.illustrate: Furcy Koenig (2000) Shimbo Ishida (2003) appear use testbed(the 8-puzzle). Yet, results algorithm (LRTA*) differ substantially. closer inspection reveals two different goal states used leading major discrepanciesperformance. compound problems, performance metrics measuredmyopic lookahead search depth one. contrast game-playing practicecompetitive systems gain substantial benefit deeper search horizon (Schaeffer, Culberson,Treloar, Knight, Lu, & Szafron, 1992; Hsu, Campbell, & Hoane, 1995; Buro, 1995).take step towards unified view learning real-time search make four contributions. First, introduce simple three-parameter framework (named LRTS) includes LRTA*,-LRTA* , SLA* -Trap special cases. by-product generalization extensionfirst three algorithms variable depth lookahead. Second, prove completeness convergence combination parameters. Third, prove non-trivial theoretical boundsinfluence parameters. Fourth, present large-scale empirical evaluation practicallyrelevant domains.3. Problem Formulationsection, formally introduce learning real-time search problem settings metricsusing.Definition 3.1 search space defined tuple (S, A, c, s0 , Sg , h0 ) finite setstates, finite set actions, c : R+ cost function c(s, a) incurredcost taking action state s, s0 initial state, Sg set goal states, h0initial heuristic (e.g., Manhattan distance).adopt assumptions Shue Zamani (1993), Shimbo Ishida (2003) (i)every action every state exists reverse action (possibly different cost), (ii) everyapplicable action leads another state (i.e., self-loops), (iii) least one goal state Sgreachable s0 . Assumption (i) needed backtracking mechanism used SLA* (Shue& Zamani, 1993). holds many combinatorial path-planning problems. applicabilitydomains reversing action may require non-trivial sequence actions subjectfuture research. Assumption (ii) adopted sake proof simplicity. words,results presented paper hold even self-loops (of positive cost) present.real-time search algorithms unifying, assume environment stationarydeterministic. Extension algorithms dynamic/stochastic domains subject currentresearch.Definition 3.2 execution cost traveling state s1 state s2 denoted dist(s1 , s2 )defined minimal cumulative execution cost agent incur traveling s1s2 . Throughout paper, assume dist satisfies standard triangle inequality:s1 , s2 , s3 [dist(s1 , s3 ) dist(s1 , s2 ) + dist(s2 , s3 )]. Then, state true execution122fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKcost h defined minimal execution cost nearest goal: h (s) = minsg Sg dist(s, sg ).heuristic approximation h true execution cost called admissible iff [h(s) h (s)].value h state referred heuristic value state s. assumeheuristic value initial heuristic function 0 goal state. latter assumptionholds trivially initial heuristic function admissible. depth child state state s0reachable minimum actions (denoted ks, s0 k = d). depth neighborhoodstate defined S(s, d) = {sd | ks, sd k = d}.Definition 3.3 Search agents operate starting fixed start state s0 executing actions suggested control policies. trial defined sequence states algorithm visitsstart state first goal state encounters. trial completed, agent resetstart state next trial begins. Final trial defined first trial learning (i.e.,updates heuristic function) occurs.1 convergence run sequence trials firsttrial final trial.4. Performance Measuresproblem instance fully specified search space includes start goal states.agent run convergence following statistics collected:execution convergence cost sum execution costs actions taken agentconvergence process (i.e., convergence run);planning cost average cost planning per action convergence process. Planningcost action number states agent considered decide taking action;total convergence cost total planning convergence cost actions convergenceplus execution convergence cost scaled factor called planning speed. scalingfactor represents amount planning (measured number nodes considered)agent would able time takes execute unit travel. instance,computer on-board AIBO robodog may able plan 10,000 states timetakes AIBO traverse 1 foot distance ground. Correspondingly, planningspeed 10,000 states per foot. commonly used way combining executionplanning costs single metric (Russell & Wefald, 1991; Koenig, 2004);2memory total amount memory (measured number state values) agent usedstore heuristic function convergence run. Note initial heuristicusually represented compact algorithmic form opposed table. Therefore, memoryrequired store heuristic values modified learning;first-move delay (lag) amount planning time (measured milliseconds3 ) agent takesdeciding first move. important metric real-time strategy games,1. Note random tie-breaking used, learning actually take place learning-free trial. use fixedtie-breaking throughout paper simplicity.2. Note unlike total planning cost, total convergence cost allows us model domains actionsexpensive disproportionally running time (e.g., taking damage running wall). Additionally, usestandard real-time search framework assume planning execution simultaneous interleaved.3. timings taken PowerMac G5 running 2GHz. Apple gcc 3.3 compiler used.123fiB ULITKO & L EEwherein hundreds thousands units tasked simultaneously yet reactusers commands quickly possible;suboptimality final solution defined percentage execution costfinal trial exceeds best possible solution. instance, pathfinding agentexecution cost 120 final trial shortest path cost 100,suboptimality 20%.5. Application Domainspaper, illustrate algorithms two realistic testbeds: real-time navigation unknownterrain routing ad hoc wireless sensor networks. former domain standardsingle-agent search use throughout paper illustrate points. latterdomain relative newcomer field real-time heuristic search. use laterpaper (Section 8) demonstrate learning single-search methods handle practicallyimportant yet different task.Note domains desired attributes listed introduction. Indeed,domains, state space initially unknown, agent single current state changed viaexecuting actions, agent sense local part state space centered agentscurrent state, planning time per action minimized, repeated trials.5.1 Real-time navigation next-generation video gamesagent tasked travel start state (xs , ys ) single goal state (xg , yg ).coordinates two-dimensional rectangular grid. state, eight moves availableleading eight immediate neighbors. straight move (i.e.,north, south, west, east)travel cost 1 diagonal move travel cost 2. state mappassable occupied wall. latter case, agent unable move it. Initially,map entirety unknown agent. state (x, y) agent see status(occupied/free) neighborhood visibility radius v: {(x0 , 0 ) | |x0 x| v & |y 0 y|v}. initial heuristic use so-called octile distance defined length shortestpath two locations cells passable. translation Manhattan distance ontocase eight moves easily computed closed form.Note classical A* search inapplicable due initially unknown map. Specifically,impossible agent plan path state (x, y) unless either positioned withinvisibility radius state visited state prior trial. simple solution problemgenerate initial path assumption unknown areas map containoccupied states (the free space assumption Koenig, Tovey, & Smirnov, 2003). octiledistance heuristic, initial path close straight line since map assumedempty. agent follows existing path runs occupied state. travel,updates explored portion map memory. current path blocked, A*invoked generate new complete path current position goal. processrepeats agent arrives goal. reset start state new trial begins.convergence run ends new states seen.44. Note framework easily extended case multiple start goal states.124fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKincrease planning efficiency, several methods re-using information subsequent planning episodes suggested. Two popular versions D* (Stenz, 1995) D* Lite (Koenig& Likhachev, 2002). Powerful are, enhancements reduce first-move lag time.Specifically, agent given destination coordinates, conduct A* searchposition destination move. Even small maps, delay substantial.contrast LRTA* (Korf, 1990), performs small local search selectfirst move. result, several orders magnitude agents respond users requesttime takes one A* agent.present-day games circumvent problem using fully observable maps precomputing auxiliary data structures beforehand, conduct experiments research prototype, called Hierarchical Open Graph (HOG), developed University Alberta NathanSturtevant. allows one load maps commercial role-playing real-time strategy gamesBaldurs Gate (BioWare, 1999) WarCraft III (Blizzard, 2002). use five maps rangingsize 214 192 states (of 2765 traversable) 235 204 (with 16142 traversablestates). largest map shown Figure 1. total 1000 problem instances five mapsrandomly chosen shortest path lengths fall ten bins: 1-10, 11-20, . . . , 91-100100 problems per bin. use suite 1000 problems throughout paper.Figure 1: Left: one five test maps Baldurs Gate commercial role-playing gameBioWare. Right: close-up showing agent dot middle.6. LRTS: Unifying Frameworksection introduce primary contribution paper unifying frameworkreal-time heuristic search incremental fashion. Namely, start basealgorithm, LRTA*, analyze three extensions: deeper lookahead, optimality weight,backtracking. extension illustrated hand-traced micro-example empirical resultsreal-time pathfinding domain. unifying algorithm constitutes sections finale.6.1 Learning Real-Time A* (LRTA*)LRTA* introduced Korf (1990), first best known learning real-time heuristic searchalgorithm . key idea lies interleaving acting backing heuristic values (Figure 2).55. clarity, pseudocode paper describes single trial only.125fiB ULITKO & L EESpecifically, current state s, LRTA* lookahead one considers immediate neighbors (lines 4-5 Figure 2). neighbor state, two values computed: execution costgetting current state (henceforth denoted g) estimated execution costgetting closest goal state neighbor state (henceforth denoted h). g knownprecisely, h heuristic estimate. LRTA* travels state lowest f = g + h value(line 7). Additionally, updates heuristic value current state minimum f -valuegreater heuristic value current state (line 6).6LRTA*1 initialize heuristic: h h02 reset current state: sstart3 6 Sg4generate children one move away state5find state s0 lowest f = g + h6update h(s) f (s0 ) f (s0 ) greater7execute action get s08 endFigure 2: LRTA* algorithm lookahead one.Korf (1990) showed finite domains goal state reachable stateaction costs non-zero, LRTA* finds goal every trial. Additionally, initial heuristicfunction admissible LRTA* converge optimal solution finite numbertrials. Compared A* search, LRTA* lookahead one considerably shorter firstmove lag finds suboptimal solutions much faster. However, converging optimal (e.g.,lowest execution cost) solution expensive terms number trials totalexecution cost. Table 1 lists measurements averaged 1000 convergence runs five gamemaps pathfinding domain (details Section 5.1). differences become pronouncedlarger problems (Figure 3).Table 1: LRTA* lookahead one vs. A* pathfinding.AlgorithmA*LRTA*First-move lag3.09 ms0.05 msConvergence execution cost15993466.2 Extension 1: Deeper Lookahead SearchLRTA* follows heuristic landscape local minimum. avoids getting stuck raisingheuristic values eventually eliminating local minimum. Thus, local heuristic minima,caused inaccuracies initial heuristic values, eliminated process learning.Ishida (1997) referred inaccuracies heuristic heuristic depressions. studiedbasic case LRTA* lookahead one. Heuristic depressions later generalized case weighted LRTA* arbitrary lookahead (Bulitko, 2004) name-traps.6. condition necessary heuristic consistent f -values non-decreasing along lookaheadbranches. general, need decrement h-value state initial heuristic admissible.126fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK483.5LRTA*A*65432A*2.521.510.510LRTA*3Convergence execution costFirst move lag (ms)7x 10010203040506070Shortest solution length80901000010203040506070Shortest solution length8090100Figure 3: Differences LRTA* A* pathfinding problems scale up.process filling heuristic depressions, agent incur substantial execution costmoving back forth inside depression. happens due fact LRTA*lookahead one myopic conducts minimum amount planning per move. planningcheaper execution, natural solution increase amount planning per movehope eliminating heuristic local minima lower execution cost.LRTA*, additional planning per move implemented deeper lookahead search.heuristic update action selection rules introduced previous section extendedarbitrary lookahead depth manner inspired standard mini-max search games (Korf,1990). Korf called new rule mini-min empirically observed deeper lookahead decreases execution cost increases planning cost per move. phenomenon illustrated hand-traceable example Figure 4. six states shown circleactions shown edges. action cost 1. initial heuristic h0 admissibleinaccurate. Heuristic values first trial shown numbers state(circle).goalstartgoalh0:012113h1:0123335 movesstarth0:012113h1:0123133 movesFigure 4: Initial final heuristics LRTA* lookahead one (left) two (right).LRTA* lookahead one two converge final heuristics one trial. However,additional ply lookahead takes advantage two extra heuristic values and, result,reduces execution cost 5 3 moves. hand, planning cost moveincreases 2 4 nodes.general, planning per action compensates inaccuracies heuristic functionallows one select better moves. Table 2 demonstrates effect averaged 1000pathfinding problems. reduction execution cost due deeper lookahead becomespronounced larger problems (Figure 5).Lookahead search LRTA*-like algorithms received considerable attention literature.Russell Wefald (1991) Koenig (2004) analyzed selection lookahead depth optimalterms total cost (i.e., weighted combination planning execution costs). Bulitko,127fiB ULITKO & L EETable 2: Effects deeper lookahead LRTA*.Lookahead13579First move lag0.05 ms0.26 ms0.38 ms0.68 ms0.92 msExecution convergence cost934677956559540544234Convergence execution cost3.5x 10d=13d=3d=5d=72.52d=91.510.500204060Solution length80100Figure 5: Convergence LRTA* different lookaheads problems scale up.Li, Greiner, Levner (2003) Bulitko (2003) examined pathological cases deeper lookaheadincreasing execution planning costs.6.3 Extension 2: Heuristic WeightScalability LRTA* large problems determined two attributes. First, usesinitial optimistic heuristic function unknown areas search space informed,realistic function explored areas. results so-called optimism face uncertainty agent eagerly explores novel parts space. Consequently, convergencetime lengthened solution quality oscillates significantly consecutive trials (Shimbo& Ishida, 2003). Second, complete convergence process intractable terms runningtime amount memory required store heuristic optimal solutions sought.Indeed, even simple problems intractable terms finding optimal solutions (e.g.,generalized sliding tile puzzle Ratner & Warmuth, 1986).weighted version LRTA* proposed Shimbo Ishida (2003) systematic waygiving optimality converged solution. authors argued optimal solutionsrarely needed practice, achieved large domains. Consequently, user mustsettle suboptimal satisficing solutions. Weighted LRTA* algorithm (LRTA*)run inadmissible initial heuristic. inadmissibility bounded every state h(s)value required upper-bounded (1 + )h (s) h (s) true distance goal.resulting -admissibility leads -optimality solution -LRTA* converges.giving optimality -controlled fashion, algorithm gains faster convergence, smallermemory requirements, higher stability. Note 0-LRTA* equivalent original LRTA*.Shimbo Ishida (2003) used Manhattan distance scaled (1 + ) initial heuristich0 . underlying idea illustrated five-state domain (Figure 6). left, initial128fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKheuristic listed immediately five states (shown circles). action (shownedge) execution cost 1. Every trial LRTA* involve four moves updateheuristic values. values trial listed successive rows. 16 moves (4trials), LRTA* converge perfect heuristic. right, start heuristicmultiplied (1+) = 2. shown there, 1-LRTA* takes one trial (i.e., 4 moves) converge.goalinitial heuristic:final heuristic:start0011201122goal4 movesstartinitial heuristic:00224final heuristic:012344 moves4 moves01223012334 moves012344 movesFigure 6: Heuristic values successive trials of: LRTA* (left) 1-LRTA* (right).general, scaling initial (admissible) heuristic (1 +), amount underestimationadmissible initial heuristic relative perfect heuristic reduced many states. Therefore,number updates (line 6 LRTA*, Figure 2) needed increase initial heuristic valuestate final value reduced. Correspondingly, execution convergence cost loweredlearning sped up. Figure 7 illustrates correlation pathfinding domain. discrepancyamount underestimation initial heuristic (1 + )h0 averaged states maphand (s S):avgsgoal , sSh (s, sgoal )(1 + )h0 (s, sgoal ).h (s, sgoal )(6.1)denotes non-negative subtraction: ab equals b result positive 0 otherwise.usual, h perfect heuristic h0 octile distance. heuristics takenrespect 1000 random goal states (sgoal ) defined Section 5.1.Convergence execution cost1000080006000400020000024681012Discrepancy (%)14161820Figure 7: Convergence cost -LRTA* vs. weighted initial heuristic discrepancy. pointsgraph correspond values (1 + ): 3.3, 2.0, 1.5, 1.1, 1.0.negative side, scaling uniform states sometimes result scaledvalues exceeding perfect heuristic values. inadmissibility lead suboptimal solutions129fiB ULITKO & L EEillustrated Figure 8. Specifically, initial heuristic admissible (the values five statesshown circles left part figure). actions (shown arrows)execution cost 1. Thus, LRTA* converges optimal path (i.e., top state).hand, scaling heuristic (1 + ) = 3 (shown right) leads 2-LRTA* convergelonger suboptimal path (through bottom two states).312600StartStart00Goal0Goal0Figure 8: 2-LRTA* converges suboptimal path.summary, scaling initial (admissible) heuristic 1 + tends reduce underestimation error heuristic thereby speeding convergence. weighted heuristic becomesprogressively overestimating, suboptimality final solutions increases and, eventually,overall convergence slows (due less informative weighted heuristic). Table 3 illustratestrends pathfinding domain. again, effect pronounced larger problems(Figure 9).Note similar effect observed weighted A* search wherein weight wput heuristic h. w exceeds 1, fewer nodes expanded cost suboptimalsolutions (Korf, 1993). Table 4 lists results running weighted A* pathfinding problemsused Table 3.x 1061+ = 1.251.51+ = 1.251+ = 551+ = 5Suboptimality (%)Convergence execution cost4210.5432100204060Solution length8010000204060Solution length80100Figure 9: Impact heuristic weight pathfinding problems scale up.6.4 Extension 3: BacktrackingLRTA* -LRTA* learn updating heuristic function advancing statespace. SLA* (Shue & Zamani, 1993) introduced backtracking mechanism. is, upon makingupdate heuristic value current state, agent backtracks previous state.provides agent two opportunities: (i) update heuristic value previous state(ii) possibly select different action previous state. alternative scheme would update130fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKTable 3: Effects scaling initial heuristic -LRTA* .Heuristic weight (1 + )10.05.03.32.01.51.11.0 (LRTA*)Execution convergence cost2832200219082271363965209346Suboptimality3.17%2.76%2.20%1.52%0.97%0.25%0.00%Table 4: Effects scaling initial heuristic weighted A*.Heuristic weight10.05.03.32.01.51.11.0 (A*)Planning convergence cost14431568173220743725522010590Suboptimality3.48%3.08%2.37%1.69%1.16%0.30%0.00%heuristic values previously visited states memory (i.e., without physically moving agentthere). approach exploited, example, temporal difference methods eligibilitytraces (Watkins, 1989; Sutton & Barto, 1998). give agent opportunity (ii) above.Naturally, backtracking moves counted execution cost.goalinitial heuristic:final heuristic:start01123012230123301234goal4 movesstartinitial heuristic:01123final heuristic:012348 moves4 moves4 movesFigure 10: Heuristic values successive trials of: LRTA* (left) SLA* (right).underlying intuition illustrated simple example Figure 10. again,consider one-dimensional five-state domain. action execution cost one. initialheuristic accurate left two states one lower right three states. trial,LRTA* raise heuristic value single state. Therefore, three trials (12 moves) neededmake heuristic perfect. SLA*, hand, gets middle state 2 moves, updatesvalue 1 2, backtracks second state right, increases value 2 3,backtracks right state, increases value 3 4. agent take 4moves towards goal, following perfect heuristic. result, first trial longer (8vs. 4 moves) overall number moves convergence reduced (from 12 8).SLA* algorithm nearly identical LRTA* lookahead one (Figure 2).difference upon increasing state value line 6, agent takes action leading131fiB ULITKO & L EEprevious state state s0 line 7 LRTA*. previous state exist (i.e.,algorithm reached start state) action taken heuristic value still updated.SLA* backtracks every time heuristic updated (i.e., learning took place). causessubstantial execution cost first trial. fact, prove Theorem 7.8, learningoccurs first trial. larger problems, user may want suboptimal solutionconvergence process completes. address problem, SLA*T, introduced (Shue, Li, &Zamani, 2001), gives user control amount learning done per trial. Namely,SLA*T backtracks exhausts user-specified learning quota.two primary effects backtracking metrics consider paper. First,larger values learning quota decrease execution cost trial less backtracking occurs.extreme, learning quota set infinity, transforming SLA*T backtracking-freeLRTA* lookahead one. end spectrum (with learning quota zero),SLA*T always backtracks upon updating heuristic value becomes equivalent SLA*.backtracking tends speed convergence process (i.e., decrease convergence executioncost) fewer trials needed converge (recall example Figure 10). Empirically,first effect backtracking clearly seen 8-puzzle (Table 5) well pathfindingtask small maps (Table 6). still open question trend observed largerpathfinding tasks (Table 7).Table 5: Effects backtracking 8-puzzle.Learning quota0 (SLA*)713(LRTA*)First trial execution cost1846424375371Convergence execution cost2321527665781658002Memory required728187512059425206Table 6: Effects backtracking pathfinding domain small maps.Learning quota0 (SLA*)10501000(LRTA*)First trial execution cost434413398390235Convergence execution cost457487592810935second effect backtracking reduction amount memory required storelearned heuristic values.7 backtracking, agent tends update heuristic values previously visited states. updates require additional memory allocated. memorysaving effect seen across problem sizes Table 5 Table 7.6.5 Combining Extensionsprevious sections introduced three extensions base algorithm LRTA*. Table 8 lists sixalgorithms use extensions. first entry summarizes arbitrary-depth LRTA*7. Recall initial heuristic values stored table come effectively computable functionManhattan distance.132fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKTable 7: Effects backtracking pathfinding domain large maps.Learning quota0 (SLA*)10100100010000(LRTA*)Convergence execution cost974110315111371022194719346Memory required174178207258299303Table 8: Real-time heuristic methods compared along several dimensions.AlgorithmLRTA*-LRTA*SLA*SLA*T-TrapLRTSLookaheadarbitraryarbitraryone plyone plyarbitraryarbitraryLearning rulemini-min, frontiermini-min, frontiermini-min, frontiermini-min, frontiermax min, statesmax min, statesExecutionone actionone actionone actionone actionactionsactionsOptimalityoptimal-optimaloptimaloptimal-optimal-optimalBacktrackingnonenonealwayscontrolledalwayscontrolleduses mini-min backup rule propagate heuristic values search frontier interior states lookahead tree. executes one action converges optimal solution.backtracking used. Weighted LRTA* (-LRTA* ) converges -optimal solution identical LRTA* columns table. SLA* SLA*T myopic versions LRTA*use backtracking: unlimited controlled learning quota respectively. Backtrackingindependently introduced another algorithm, called -Trap (Bulitko, 2004), also usedheuristic weight parameter = 1/(1 + ) effect weighted LRTA*. Additionally,-Trap employed aggressive heuristic update rule, denoted max min table,used heuristic values states (as opposed frontier lookahead search only). Insteadtaking single action lookahead search episodes, applied actions amortizeplanning cost.combine three extensions single algorithm called Learning Real-TimeSearch LRTS. shown last row table and, simplified fashion, Figure 11.detailed pseudo-code necessary re-implement algorithm well follow theoremproofs found Figure 20, Appendix A. step operation LRTSexplain control parameters.line 1, trial initialized setting current state agent start state sstartresetting amount learning done trial (u). long goal state sgoal reached,LRTS agent interleaves planning (lines 3 4), learning (line 5), execution (lines 6-11).planning state, LRTS uses model environment (implicitly updated newstates comes agents radius visibility) generate child states current statemoves away. lookahead level (i = 1, 2, . . . , d), LRTS finds promising state(si ) minimizes weighted estimate total distance:si = arg mins0 S(s,i)g(s0 ) + h(s0 ) ,133(6.2)fiB ULITKO & L EEg(s0 ) shortest distance current state child s0 h(s0 ) heuristicestimate distance child state s0 goal state sgoal . Throughout paperbreaking ties systematic fashion detailed later paper. Note distance g(s0 )current state child state s0 weighted control parameter (0, 1].behavior obtained multiplying initial heuristic 1+ = 1/ -LRTA* exceptmakes initial heuristic inadmissible (Theorem 7.2).LRTS(d, , )1 initialize: sstart , u 02 6 Sg3generate children moves away, = 1 . . .4level i, find state si lowest f = g + h5update h(s) max f (si ) greater1id6increase amount learning u |h|7u8execute moves get state sd9else10execute moves backtrack previous state, set u =11 end12 endFigure 11: LRTS algorithm combines three extensions LRTA*.lookahead search conducted line 5 complexity O(bd ) child state reachedb branching factor. maps, however, complexity much reducedO(dn ) n dimensionality map. Thus, path-planning sensornetwork routing task, lookahead complexity O(d2 ).Like -Trap , LRTS uses so-called max min heuristic update (or learning) rule line 5.Specifically, initial heuristic admissible, agent want increase aggressively possible keeping admissible. Lemma 7.1 prove setting heuristic valueh current state maximum minima found ply preserves admissibilityh times.line 6 amount increase heuristic value current state (h(s)) addedcumulative amount learning done far trial (u). new amount learningexceed learning quota agent execute moves promising frontierstate sd seen lookahead (line 8). moves added pathundone later backtracking. new amount learning exceeds learning quotaagent backtrack previous state (line 10). so, undo lastactions. last actions thus removed path traveled far trial. Notebacktracking actions count execution cost.encourage reader revisit hand-traceable examples found Sections 6.2,6.3, 6.4 show LRTS operation simple domains. weighting example (Section 6.3),set 1/(1+) enable LRTS behave -LRTA* . concludes presentationLRTS algorithm highlight properties theoretical empirical results.134fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK7. Theoretical ResultsLRTS framework includes LRTA*, weighted LRTA*, SLA*, -Trap special cases (Table 8). fact presented formally following theorems. proofs found Appendix B. Note following assumptions mandatory real-time search,needed proofs. Specifically, search space assumed deterministic stationary. Additionally, agents current state affected agents actions. Finally, agentpossesses sufficient knowledge environment conduct local-search (i.e., lookahead)depth current state.Theorem 7.1 LRTS(d = 1, = 1, = ) equivalent LRTA* lookahead one.1, = ) initialized admissible heuristic h0 equivTheorem 7.2 LRTS(d = 1, = 1+alent -LRTA* initialized (1 + )h0 .Theorem 7.3 LRTS(d = 1, = 1, = 0) equivalent SLA*.Theorem 7.4 LRTS(d, , = 0) equivalent -Trap(d, ).past, convergence completeness proven special cases (LRTA*, LRTA* , SLA*). prove properties valid values lookaheadN, 1, heuristic weight R, (0, 1], learning quota R {}, 0.Definition 7.1 tie-breaking scheme employed two states equal f = g + hvalues. Among candidate states equal f -values, random tie-breaking selects one random.scheme used (Shimbo & Ishida, 2003). Systematic tie-breaking used FurcyKoenig (2000) uses fixed action ordering per current state. orderings are, however, generatedrandomly search problem. Finally, fixed tie-breaking always uses action ordering.use latter scheme experiments simplest.Lemma 7.1 (Admissibility) valid T, , admissible initial heuristic h0 , heuristic function h maintained admissible times LRTS(d, , ) search.Theorem 7.5 (Completeness) valid T, , admissible initial heuristic h0 ,LRTS(d, , ) arrives goal state every trial.Theorem 7.6 (Convergence) valid T, , admissible initial heuristic h0 ,LRTS(d, , ) systematic fixed tie-breaking converges final solution finitenumber trials. makes zero updates heuristic function final trial. subsequenttrial identical final trial.Theorem 7.7 (Suboptimality) valid T, , admissible initial heuristic h0 , ex0)ecution cost final trial LRTS(d, , ) upper-bounded h (s. words,suboptimality final solution worse 1 1.non-trivial upper-bound h (s0 )+T imposed solution produced LRTSfirst trial. requires, however, minor extension LRTS algorithm. Thus, listextension theorem Figure 21, Appendix Theorem B.1, Appendix B respectively.135fiB ULITKO & L EETheorem 7.8 (One trial convergence) valid , admissible initial heuristic h0 ,second trial LRTS(T = 0) systematic fixed tie-breaking guaranteed final. Thus,learning exploration done first trial.8. Combinations Parametersintroduced three-parameter algorithm unifying several previously proposed extensionsbase LRTA*. also demonstrated theoretically empirically influenceparameter independently two parameters Sections 6.2, 6.3, 6.4. summaryinfluences found Table 9. up-arrow/down-arrow means increase/decreasemetric. Notation p b indicates parameter p increases b. illustrate:intersection row labeled convergence execution cost column labeled 1states convergence execution cost decreases goes up.Table 9: Summary influences LRTS parameters pathfinding domain.metric / parameterconvergence execution costplanning cost per moveconvergence memoryfirst-move lagsuboptimality final solution1dsmall01small , largeinfluenceinfluence0Tsmall , largeinfluenceinfluenceinfluence8.1 Interaction among parameterssection consider effects parameter combinations. Figure 12 shows impactlookahead depth function heuristic weight learning quota . Specifically,value , left plot shows difference convergence execution costslookahead values = 1 = 10:impact-of-d(, ) = execution-cost(d = 1, , ) execution-cost(d = 10, , ).Reduction conv. execution cost d=1 d=101Reduction conv. memory d=1 d=104x 10312.5Heuristic weight ()0.720.61.50.50.410.30.20.5Heuristic weight ()0.90.80.91600.81400.71200.61000.5800.4600.3400.2200.10.1051050 100 500 1000 5000 100001e+13Learning quota (T)(8.1)051050100 500 1000 5000 10000 1e+13Learning quota (T)Figure 12: Impact lookahead different values heuristic weight learning quota .Brighter shades indicate higher impact increasing 1 10. Likewise, right plotdemonstrates impact memory required convergence. Brighter shades indicate136fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKstronger impact (i.e., larger differences). point averaged 1000 convergenceruns pathfinding domain. observe larger values learning quota higher valuesheuristic weight magnify reduction memory due increasing lookahead depth. Noteplot show memory requirements different values rather reductionmemory requirements increases 1 10.Figure 13 shows impact function conditions. Learningquota seem affect impact suboptimality final solution. However, deeperlookahead make less effective respect. believe deeper lookaheadcompensates inaccuracies heuristic function. Therefore, weighting heuristicmakes less difference.Reduction suboptimality gamma=0.3 gamma=1.0Reduction conv. execution cost gamma=1.0 gamma=0.31060009500098400087300062000Lookahead depth (d)Lookahead depth (d)1010005042.121.971.861.751.6410001.533200021.4230001.3110510500100 500 1000 5000 10000 1e+13Learning quota (T)51050 100 500 1000 5000 100001e+13Learning quota (T)Figure 13: Impact heuristic weight different values lookahead learning quota .Figure 14 shows impact function 1000 pathfinding problems.Reducing learning quota 500 0 results increased amount backtracking.discussed previously, backtracking tends save memory speed convergence.figure shows trends demonstrates affect impact backtracking. leftplot shows backtracking effective speeding convergence lower heuristicweights ( = 0.1). contrary, right plot indicates higher heuristic weights (close= 1) make backtracking effective reducing memory required convergence.Reduction conv. execution cost T=500 T=010Reduction conv. memory T=500 T=04x 101100.570650.54Lookahead depth (d)Lookahead depth (d)85087406305420331210.160990.20.30.4 0.5 0.6 0.7Heuristic weight ()0.80.9110210.10.20.30.40.5 0.6 0.7Heuristic weight ()0.80.91Figure 14: Impact learning quota different values heuristic weight lookahead d.137fiB ULITKO & L EEOverall, influences non-trivial and, according experiments, domain-specific.explained readily, others appear result interaction threeparameters structure problem space. Consequently, selection optimal parametersspecific performance metric concrete domain presently matter trial error.manual tuning control parameters typical scenario Artificial Intelligence, futureresearch investigate automatic parameter adjustment.8.2 Minimizing performance measuressection, list parameter combinations optimizing performance measuresexplain underlying rationale. best combinations parameters sought followingparameter space: lookahead {1, 2, . . . , 10}, heuristic weight {0.1, 0.2, . . . , 1.0},learning quota {0, 5, 10, 50, 100, 500, 1000, 5000, 10000, }. one thousandcombinations parameters, ran LRTS convergence test suite 1000 pathfindingproblems described Section 5.1. results shown Table 10 explained below.Table 10: LRTS control parameters minimizing performance metrics.Metricexecution convergence costplanning convergence costplanning cost per movetotal convergence cost (speed < 185.638)total convergence cost (speed 185.638)suboptimalitymemoryLookahead101111010Heuristic weight0.30.30.30.31.00.3Learning quota0Execution convergence cost lowest quality individual moves highest.Thus, deeper lookahead leading planning per move favored. High values undesirable LRTS seeks near-optimal solutions takes longer converge. low valuesalso undesirable weighted initial heuristic becomes less less informative (Table 3).Hence, best value lays somewhere middle (0.3). Finally, backtracking preferredconvergence fastest larger maps used experiment (Table 7).Planning convergence cost defined total planning cost moves convergenceachieved. Deeper lookahead leads faster convergence quadratically increases planningcost every move (cf., Section 6.5). Hence, overall, shallow lookahead preferred. alsoexplains optimal parameter combination minimizing planning cost per move LRTSdepends only.Total convergence cost. minimize weighted sum planning executioncosts. common objective previously studied (Russell & Wefald, 1991; Koenig, 2004).adopt settings latter bring two metrics (the execution cost planningcost) single scalar expressed number states. Namely, total convergence costdefined convergence execution cost multiplied planning speed plus total planningcost (Section 4). instance, agent converged 1000 units execution cost, consideredtotal 5000 states planning (i.e., average five states per unit execution cost),planning speed 200 (i.e., 200 states considered planning one unit travelexecuted), total convergence cost 1000200+5000 = 205000, clearly dominated138fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKexecution component. planning per move tends decrease convergence execution costthus preferred planning speed high (i.e., planning cheap relative executionexecution cost dominates total cost). Conversely, less planning per move preferredplanning speed low. Russell Wefald (1991) Koenig (2004) usedreasoning select optimal lookahead depth LRTA*-based real-time search algorithms.pathfinding domain, best values fastest convergence 0.3discussed above. best value lookahead depends planning speed: deeper lookahead (10vs. 1) preferred planning speed sufficiently high planning per moveafforded.Suboptimality zero = 1 regardless .Memory. optimal parameter combination (10, 0.3, 0) consistent previous analysis: backtracking (T = 0) reduces memory consumption, lower heuristic weight leadsless exploration (since LRTS satisfied suboptimal solutions), since heuristic valuesstored actions apart, higher lookahead preferred.Adjusting LRTS control parameters allows agent achieve convergencepreviously impossible due memory limitations. following, adopt settings ShimboIshida (2003) Bulitko (2004) consider one hundred 15-puzzles first published Korf(1985). known FALCONS LRTA* lookahead one unable convergeoptimal solutions 100 puzzles memory limited 40 million states (Shimbo &Ishida, 2003). hand, convergence suboptimal solutions achieved-LRTA* (Shimbo & Ishida, 2003) -Trap (Bulitko, 2004).Table 11: Convergence Korfs one hundred 15-puzzles four million states memory.AlgorithmFALCONSLRTA*LRTSLRTSLRTS12400max0.290.60.7Suboptimalityconvergence problemconvergence problem50%10%4%make task challenging reducing memory limit ten fold forty fourmillion states. unable find set d, T, parameters allowed LRTSconverge optimal solutions 100 instances, algorithm converge solutionswere, average, 4% worse optimal 100 puzzles (Table 11). Lower valueshigher values increase memory cost efficiency allow increase leadshigher quality solutions.summary, adjusting control parameters LRTS significantly affects performance.analysis individual combined influence parameters several practically relevantperformance metrics gives practitioner better intuition application-specific parameter tuning.Perhaps importantly, opens interesting line research automatic selection optimalparameters basis effectively measurable properties domain target performancemetric.139fiB ULITKO & L EE8.3 Application Domain #2: Routing Sensor Networkssecond application LRTS considered routing ad hoc wireless sensor networks. Sensornodes generally limited computational power energy, thus simple, energy efficient routingroutines typically used within sensor networks. Various applications exist, including military(Shang et al., 2003), environmental monitoring (Braginsky & Estrin, 2002) reconnaissancemissions (Yan, He, & Stankovic, 2003). such, routing sensor networks interest largecommunity users.Distance Vector Routing (DVR) algorithm widely used network routing (Royer & Toh,1999). particular, DVR employed within Border Gateway Protocol used route trafficentire Internet. DVR, network node maintains heuristic estimate distancedestination/goal node. observe DVR conceptually similar LRTA*lookahead one. Thus, applying LRTS network routing domain essentially equivalentextending DVR heuristic weight backtracking mechanism controlledlearning quota . Note analogue deeper lookahead DVR since networknode learn explicit model network and, consequently, aware networknodes beyond immediate neighbors. Hence, limited one.order demonstrate impact two DVR extensions, chose setup usedrecent application real-time heuristic search network routing (Shang et al., 2003). Specifically,network sensor nodes assumed dropped sky monitor territory, landingrandom dispersion, thus necessitating ad hoc routing order allow data transmissions.consider case fixed destination transmission, simulating source nodeproviding information central information hub (e.g., satellite up-link).Note LRTA* pathfinding domain access heuristic values neighborsextra cost since heuristic values stored centralized memory. casead hoc networks, wherein node maintains estimate number hops goal node(the h heuristic). naive application LRTA* would query neighbor nodes order retrieveh values. would, however, substantially increase amount network traffic, therebydepleting nodes energy sources faster making network easily detectable. adoptedstandard solution node stores heuristic value also last knownheuristic values immediate neighbors. Then, whenever node updates heuristic value,notifies neighbors change broadcasting short control message. last majordifference pathfinding nodes know either geographical positioncoordinates destination node. Thus, Euclidian distance available initialheuristic initialized one nodes except destination node set zero.experiments reported below, N sensor nodes randomly distributed Xtwo-dimensional grid two nodes occupy location. node fixed transmission radius determines number neighbors node broadcast messages (andwhose heuristic values store). specify single problem instance, distinct start (source)node goal (destination hub) nodes randomly selected N nodes. start nodeneeds transmit data message goal node. so, send message oneneighbors re-transmit message another node, etc. transmissiontwo neighboring nodes incurs execution cost one hop. message reaches destination/goal node, new trial begins start goal nodes. Convergence occursheuristic values updated trial.140fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKobjective find shortest route start node goal node learning betterheuristic values repeated trials. take measurements four metrics paralleling useddomain pathfinding:convergence execution cost total number hops traveled messages convergence;first trial execution cost number hops traveled first trial;suboptimality final solution excess execution cost final trialshortest path possible. instance, network converged route 120 hopsshortest route 100 hops, suboptimality 20%;convergence traffic defined total volume information transmitted throughout network convergence. data message counts one. Short control messages (usednotify nodes neighbors update nodes heuristic function) count one third.Interference modeled simulation. assume scheduling performed independentlyrouting strategy, different schedules severely affect performancerouting algorithms. Note that, since transmission delay timings modeled, woulddifficult model interference accurately.Four batches networks 50, 200, 400, 800 nodes considered. nodesrandomly positioned square grid dimensions 20 20, 30 30, 50 50,80 80 respectively. batch consisted 100 randomly generated networks. network,25 convergence runs performed parameter values running = {0.1, 0.3, 0.5, 0.7, 1}= {0, 10, 100, 1000, 100000}. following, present results batch800-node networks. trends observed smaller networks well.start demonstrating influence two control parameters isolation. Table 12shows influence heuristic weight backtracking. expected, smaller valuesspeed convergence increase suboptimality final solution. resultsparallel ones reported Table 3 pathfinding domain.Table 12: Effects heuristic weighting network routing.Heuristic weight1.0 (DVR)0.70.50.30.1Execution convergence cost81888188810679727957Suboptimality0%0%0%0.29%0.33%Table 13 demonstrates influence learning quota heuristic weight setone. Smaller values increase amount backtracking speed convergence costlengthen first trial. parallels results 8-puzzle (Table 5) well pathfindingsmall maps (Table 6).consider impact parameter combination. fixed lookahead depth onemakes two-dimensional parameter space easier visualize contour plots. Thus, insteadplotting impact parameter metric function two control parameters,141fiB ULITKO & L EETable 13: Effects backtracking network routing.Learning quota105 (DVR)103102100First trial execution cost5491188495859566117Convergence execution cost81888032645562306138First trial execution cost (hops)Convergence execution cost (hops)11Heuristic weight ()Heuristic weight ()50000.740000.5300020000.30.775000.570000.3650010000.10101001000Learning quota (T)0.11000000101001000Learning quota (T)100000Figure 15: First-trial convergence execution costs network routing.plot metric itself. Figure 15 shows first trial convergence execution costscombinations . Brighter areas indicates higher costs.Figure 16 shows suboptimality final solution total traffic convergence run,averaged 100 networks 800 nodes each. Again, brighter areas indicate higher values.Suboptimality final solution (%)Convergence traffic14x 1010.20.50.150.10.3Heuristic weight ()Heuristic weight ()0.250.750.74.50.540.30.053.50.10.10101001000Learning quota (T)0100000101001000Learning quota (T)100000Figure 16: Suboptimality final solution total traffic network routing.summary, extending DVR algorithm heuristic weight backtracking mechanisms LRTS, reduction convergence execution cost total network trafficachieved. result, near-optimal routes found faster, lower energy consumption.promising result raises several questions research. First, additional routingconstraints, ones investigated (Shang et al., 2003), affect performance LRTS?Second, benefits LRTS scale case several messages destina142fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKtion passing network simultaneously? latter question also applies path-planningmultiple memory-sharing units common situation squad-based games.9. Summary Future Researchpaper, considered simultaneous planning learning problems real-timepathfinding initially unknown environments well routing ad hoc wireless sensor networks. Learning real-time search methods plan conducting limited local search learnmaintaining refining state-based heuristic function. Thus, react quickly userscommands yet converge better solutions repeated experiences.analyzed three complementary extensions base LRTA* algorithm showedeffects hand-traceable examples, theoretical bounds, extensive empirical evaluation twodifferent domains. proposed simple algorithm, called LRTS, unifying extensionsstudied problem choosing optimal parameter combinations.Current future work includes extending LRTS automated state space abstraction mechanisms, function approximation methods heuristic, automated parameter selection techniques. also interest investigate convergence process team LRTS-basedagents shared memory extend LRTS moving targets stochastic/dynamic environments video games, robotics, networking.AcknowledgementsInput Valeriy Bulitko, Sven Koenig, Rich Korf, David Furcy, Nathan Sturtevant, MasashiShimbo, Rob Holte, Douglas Aberdeen, Reza Zamani, Stuart Russell, Maryia Kazakevichgreatly appreciated. Nathan Sturtevant kindly provided supported Hierarchical Open Graphsimulator. Additionally, Valeriy Bulitko, David Furcy, Sven Koenig, Rich Korf, Scott Thiessen,Ilya Levner taken time proof read early draft paper. Great thanks JAIR reviewersworking us improving manuscript. grateful support NSERC,University Alberta, Alberta Ingenuity Centre Machine Learning, Jonathan Schaeffer.143fiB ULITKO & L EEAppendix A. Detailed Pseudocodesection list pseudocode algorithms level detail necessary implementwell follow proofs listed Appendix B.LRTA*UTPUT: series actions leading s0 goal state1234567h undefined set h h0reset current state: s06 Sggenerate depth 1 neighborhood S(s, 1) = {s0 | ks, s0 k = 1}compute h0 (s) = 0 min (dist(s, s0 ) + h(s0 ))S(s,1)h0 (s) > h(s) update h(s) h0 (s)update current state arg 0 min (dist(s, s0 ) + h(s0 ))S(s,1)Figure 17: LRTA* algorithm.SLA*UTPUT: series actions leading s0 goal state123456789101112h undefined set h h0reset current state: s0reset path traveled far: path6 Sggenerate depth 1 neighborhood S(s, 1) = {s0 | ks, s0 k = 1}compute h0 (s) = 0 min (dist(s, s0 ) + h(s0 ))S(s,1)h0 (s) > h(s)update h(s) h0 (s)update current state pop(path) [or remain state path = ]elsepush onto stack pathupdate current state arg 0 min (dist(s, s0 ) + h(s0 ))S(s,1)Figure 18: SLA* algorithm.144fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK-Trap(d, )NPUT:d: lookahead depth: optimality weightUTPUT: path: series actions leading s0 goal state1234567891011121314h undefined set h h0reset current state: s0reset stack: pathreset cumulative learning amount u 06 Sgset min{d, min{k | S(s, k) = }}reset set Gk=1generate depth k neighborhood S(s, k) = {s0 | ks, s0 k = k}S(s, k) Sg 6= update G G {k}compute fmin (s, k) = 0 min ( dist(s, s0 ) + h(s0 ))S(s,k)compute smin (s, k) = argmins0 S(s,k)endcompute h0 (s)max fmin (s, k)G = ,1kdmax1kmin{G}1516( dist(s, s0 ) + h(s0 ))fmin (s, k)otherwise.h0 (s) h(s)push onto stack path17(G = ,smin (s, d)update current statearg min fmin (s, k) otherwise.1819202122elseupdate h(s) h0 (s)backtrack: pop(path) [or remain state path = ].endendkGFigure 19: -Trap algorithm.145fiB ULITKO & L EELRTS(d, , )NPUT:d0 : lookahead depth: optimality weight: learning thresholdUTPUT: path: series actions leading s0 goal state1234567891011121314h undefined set h h0reset current state: s0reset stack: pathreset cumulative learning amount u 06 Sgset min{d0 , min{k | S(s, k) = }}reset set Gk=1generate depth k neighborhood S(s, k) = {s0 | ks, s0 k = k}S(s, k) Sg 6= update G G {k}compute fmin (s, k) = 0 min ( dist(s, s0 ) + h(s0 ))S(s,k)compute smin (s, k) = argmins0 S(s,k)endcompute h0 (s)max fmin (s, k)max1kmin{G}23G = ,1kd1516171819202122( dist(s, s0 ) + h(s0 ))fmin (s, k)otherwise.h0 (s) > h(s)compute amount learning move: ` h0 (s) h(s)update h(s) h0 (s)elsereset ` = 0endu + `push onto stack path(G = ,smin (s, d)update current state:arg min fmin (s, k) otherwise.kG2425262728accumulate learning amount u u + `elsebacktrack: pop(path) [or remain state path = ].endendFigure 20: LRTS algorithm.23a23b23cs0 [s = s0 & s0 path]remove s0 following states pathendFigure 21: Additional lines facilitating incremental pruning LRTS. inserted lines 23 24 Figure 20.146fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKAppendix B. Theorem ProofsTheorem 7.1 LRTS(d = 1, = 1, = ) equivalent LRTA* lookahead one.Proof. show state LRTS(d = 1, = 1, = ) (i) takes actionLRTA* (ii) updates h function way LRTA* does.(i) state s, LRTS(d = 1, = 1, = ) goes state smin (s, 1) (line 23, Figure 20)equivalent state arg min (dist(s, s1 ) + h(s1 )) LRTA* move (lines1 S(s,1)7, Figure 17).8(ii) LRTS(d = 1, = 1, = ) updates h(s) h0 (s) line 17 Figure 20 h0 (s) >h(s) (line 15) equivalent LRTA*s update condition line 6, Figure 17.1, = ) initialized admissible heuristic h0 equivaTheorem 7.2 LRTS(d = 1, = 1+lent -LRTA* initialized (1 + )h0 .Proof. two algorithms begin maintain different heuristic functions. Let hLRTS1heuristic function LRTS(d = 1, = 1+, = ) maintains iteration t.9 Likewise, let h-LRTA*denote heuristic function -LRTA* maintains iteration t. also denote sLRTS1, = ) iteration t. Likewise, s-LRTA*currentcurrent state LRTS(d = 1, = 1+state -LRTA* iteration t. show induction iteration number t:h-LRTA*= (1 + )hLRTS.(B.1)1Base step: since -LRTA* initialized (1 + )h0 LRTS(d = 1, = 1+, = )initialized h0 , equation B.1 trivially holds = 0.Inductive step: suppose equation B.1 holds iteration t. show holds iteration+ 1.First, show algorithms bound state st . Suppose not: sLRTS6=-LRTA*st. since start state s0 must earliest t0 <-LRTA* . means state different actions takensLRTS= s-LRTA*sLRTSt0t0t0t0 +1 6= st0 +1LRTS -LRTA* . LRTS takes action line 23 Figure 20 would move state:argmins0 S(st0 ,1)10LRTS 0dist(st0 , ) + ht0 (s ) .1+(B.2)-LRTA* moves following state (line 7 Figure 17):argmins0 S(st0 ,1)dist(st0 , s0 ) + h-LRTA*(s0 ) .t0(B.3)Since h-LRTA*= (1 + )hLRTS(as t0 < equation B.1 holds time inductivet0t0hypothesis), action B.2 B.3.8. Throughout paper assume tie-breaking done consistent way among algorithms.9. One measure iterations counting number times condition executed (line 5 Figure 20line 3 Figure 17).147fiB ULITKO & L EEupdate h LRTS(d = 1, =expression h0 (s) arrive at:hLRTSt+1 (st )11+ ,====mins0 S(st ,1)= ) occurs line 17, Figure 20. Substituting10LRTS 0dist(st , ) + ht(s )1+1mindist(st , s0 ) + (1 + )hLRTS(s0 )1 + s0 S(st ,1)1mindist(st , s0 ) + h-LRTA*(s0 )01 + S(st ,1)1 -LRTA*h(st )1 + t+1(B.4)(B.5)(B.6)(B.7)concludes inductive proof.Theorem 7.3 LRTS(d = 1, = 1, = 0) equivalent SLA*.Proof. = 0, condition u + ` line 21 Figure 20 hold h(s)updated (i.e., h0 (s) h(s) line 15). case forward move executed line23 equivalent SLA*s forward move lines 11, 12 Figure 18. h(s) indeed updatedLRTS SLA* backtrack executing line 26 Figure 20 line 9 Figure 18respectively. proof concluded observation h0 (s) computed LRTS(d = 1, =1, = 0) equivalent h0 (s) computed SLA* line 6 Figure 18.Theorem 7.4 LRTS(d, , = 0) equivalent -Trap(d, ).Proof. Substituting = 0 line 21 Figure 20, effectively obtain -Trap (Figure 19).Lemma 7.1 (Admissibility) valid T, , admissible initial heuristic h0 , heuristicfunction h maintained admissible times LRTS(d, , ) search.Proof. prove lemma induction iteration number t. = 0 statementtrivially holds since initial heuristic h0 admissible. Suppose ht admissibleshow ht+1 admissible. Heuristic ht+1 different ht state st updatedlines 14 17 Figure 20. Combining expressions together, obtain:ht+1 (st ) = maxmin ( dist(st , s0 ) + ht (s0 )),1kd s0 S(st ,k)(B.8)goal states discovered lookahead (G = ) and:ht+1 (st ) = minmin ( dist(st , s0 ) + ht (s0 )),kG s0 S(st ,k)(B.9)otherwise. Suppose max min min min reached state :ht+1 (st ) = dist(st , ) + ht (s ), S(st , m), 1 d.(B.10)shortest path state st closest goal state intersects neighborhood S(st , m) depthm. Let us denote state S(st , m) belonging shortest path (Figure 22).148fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKS(st,d)S(st,m)sts*goalFigure 22: proof admissibility.Since ht admissible states,arg min ( dist(st , s) + ht (s)) conclude that:1,chosensS(st ,m)ht+1 (st ) = dist(st , ) + ht (s )(B.11)dist(st , ) + ht (s )(B.12)dist(st , ) + h (s )(B.13)= h (st ).(B.14)Thus, ht+1 admissible heuristic concludes induction.Theorem 7.5 (Completeness) valid T, , admissible initial heuristic h0 ,LRTS(d, , ) arrives goal state every trial.Proof. Since heuristic h maintained admissible times update increases valuepositive amount lower-bounded constant, finite number updates madetrial. Let last update made state su1 , u 0 sequence states s0 , s1 , . . . traversedtrial. show sequence states su , su+1 , . . . guaranteed terminategoal state. words, LRTS cannot loop finite state space forever avoiding goalstates. updates made su , su+1 , . . . , must hold that10 j u:h(sj )maxmin ( dist(sj , s0 ) + h(s0 )),1kd s0 S(sj ,k)h(sj ) dist(sj , sj+1 ) + h(sj+1 ),h(sj ) h(sj+1 ) dist(sj , sj+1 ).(B.15)(B.16)(B.17)Since > 0 states 6= b [dist(a, b) > 0], follows h(su ) > h(su+1 ) > h(su+2 ) >. . . . state space finite, series states must terminate goal state h(sn ) = 0.Thus, valid parameter values, LRTS ends goal state every trial.10. Similar reasoning applies case G 6= min min used instead max min.149fiB ULITKO & L EETheorem 7.6 (Convergence) valid T, , admissible initial heuristic h0 ,LRTS(d, , ) systematic fixed tie-breaking converges final solution finitenumber trials. makes zero updates heuristic function final trial. subsequenttrial identical final trial.Proof. proof Theorem 7.5 clear trial zero updatesheuristic function h. call trial final trial demonstrate subsequenttrial identical final trial. Suppose not. final trial s0 , s1 , . . . , snearliest state sj next trial different final trial state sj+1 . impliessubsequent trial LRTS took different action state sj action tookstate final trial. updates heuristic function, action mustselected line 23 Figure 20. Expanding expression, get:11sj+1 = smin (sj , d) = argmin ( dist(sj , s0 ) + h(s0 )).s0 S(sj ,d)(B.18)Since ties broken systematic fixed fashion, choice state sj+1 uniquecontradicts assumption existence subsequent trial different final trial.Theorem 7.7 valid T, , admissible initial heuristic h0 , converged solution0)cost12 LRTS(d, , ) upper-bounded h (s.Proof. First, let us denote converged solution cost (i.e., execution cost final trial)\ theorem 7.6 follows final trial updates heuristicsolution.function. Suppose s0 , . . . , sn states traversed final trial. Since updatesmade heuristic function, condition h0 (si ) h(si ) line 15 Figure 20 must heldsi , = 0, . . . , n. Substituting expression h0 (si ) line 14, arrive at:13max1kdmins0 S(s,k)dist(si , s0 ) + h(s0 ) h(si ).(B.19)dist(si , s0 ) + h(s0 )(B.20)Since:si+1 =mins0 S(s,d)conclude that:dist(si , si+1 ) + h(si+1 ) h(si )dist(si , si+1 ) h(si ) h(si+1 ).(B.21)(B.22)Summing telescoping inequalities = 0, . . . , n 1, derive:n1Xi=0dist(si , si+1 )h(s0 ) h(sn ).11. case G 6= handled similarly.12. Henceforth converged solution cost defined execution cost final trial.13. again, case G 6= handled similarly.150(B.23)fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKSince sn Sg , h(sn ) = 0. sum distances travelled travel cost final trialconverged solution cost definition.14 Thus, conclude converged solution cost0)upper bounded h(s 0 ) h (s.Theorem 7.8 valid , admissible initial heuristic h0 , second trial LRTS(T =0) systematic fixed tie-breaking guaranteed final. Thus, learning explorationdone first trial.Proof. Consider sequence states [s0 , s1 , . . . , sn ] traversed first trial. s0start state sn goal state found. transition st st+1 , = 0, . . . , n 1carried via forward move (line 23 Figure 20) backtracking move (line 26). move= 0, . . . , n 1, heuristic function may changed (line 17) state st . useht denote heuristic function update line 17. update results ht+1 . Noteupdate changes h state st ht (s) = ht+1 (s) 6= st . particular, sincest+1 6= st 15 ht+1 (st+1 ) = ht (st+1 ).Since = 0, n moves divided two groups: forward moves changeh function backtracking moves do. prune sequence states[s0 , s1 , . . . , sn ] using backtracking stack. Namely, beginning stack contains s0 . Everyforward move 0 n1 add st+1 stack. Every backtracking move 0 n1remove st+1 stack. Carrying procedure = 0, . . . , n 1, derive sequencestates: p0 = [s0 , . . . , sm ] leading initial state s0 goal state sm . Clearly, n.Observe since backtracking moves removed original move sequence,moves left forward moves updates h. Thus, let us define bh-value statevisited trial as:(h(s)recent backtracking move state s;bh(s) =initial h(s) backtracking move taken trial.Since = 0, state si , = 0, . . . , updates h lastbacktrackingh move fromi state (if any). words, h final heuristic =0, . . . , h(si ) = bh(si ) . also means every k-neighborhood (1 k d) state0si p state s0 bh(s0 ) + dist(si , s0 ) bh(si ).show updates h done second trial. Supposecase. must exist earliest state sj p0 , 0 j < LRTS secondtrial updated h(sj ) greater bh(sj ). reasoned above, final value h(sj )bfirst trial h(sj ) need increase it. assumption, LRTS increaseh(sj ) second trial. Consequently, least one state one sj neighborhoods updatedfirst trial (as updates second trial happened yet).Formally, h(s0 ), s0 S(sj , k) 1 k d, increased first trialstate sj visited p0 . Additionally, means result update musthold that:hnew (s0 ) + k > h(sj ).(B.24)\ =14. Peasy check sequence states final trial repetitions. Therefore, solutionn1dist(s,).i+1i=015. Except possibly case backtracking st = s0 affect proof.151fiB ULITKO & L EEs0S(sj,k)p'sjsms'Figure 23: proof Theorem 7.8.Thus, first trial, state s0 must arrived via sequence forward moves startingstate sj (shown dashed curve Figure 23). forward moves h-updates,h(scurrent ) > h(snext ) holds. means sequence states traversed forwardmoves started sj ended s0 . Since hnew (s0 ) + k > h(sj ), LRTS mustbacktrack s0 least sj . would updated h(sj ) hnew (sj ) hnew (s0 ) + kfirst trial. Consequently, state s0 cannot cause update h(sj ) second trialinequality B.24 longer hold, leading contradiction. Thus, updates h carriedsecond trial.Let us define solution(i) execution cost path p start goal statesthat: (i) p lies fully set states traversed agent trial (ii) p minimizesexecution cost.Theorem B.1 valid T, , admissible initial heuristic h0 , first trialLRTS(d, , ) systematic fixed tie-breaking incremental pruning (see Figure 21)sults solution(1) h (s0 )+T .Proof. prove theorem three steps.Notation. Consider sequence states [s0 , s1 , . . . , sn ] traversed first trial.state s0 start state state si current state (s) i-th iteration line 27 LRTS(Figure 20). State sn goal state reached end first trial. Within iteration (0n1), two states computed follows. beginning iteration (line 6), si currentstate s. forward move (line 23), state set si+1 = smin (si , d).16 Let us denoteheuristic value state si beginning iteration hi (si ). possible update line17, new heuristic value hi+1 (si ). Also, let us denote amount learning ` iteration`i . precisely:(hi+1 (si ) hi (si ) h0 (s) > h(s) line 15,`i =(B.25)0otherwise.notation, easy see following inequality holds:dist(si , si+1 ) + hi (si+1 ) hi (si ) + `i16. case seeing goal state, i.e., G 6= , dealt similar fashion.152(B.26)fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKforward move (line 23) happens. Note iteration i, state si stateheuristic value updated. Thus, sj 6= si [hi+1 (sj ) = hi (sj )]. particular17 ,hi+1 (si+1 ) = hi (si+1 ) allows transform inequality B.26 into:dist(si , si+1 ) + hi+1 (si+1 ) hi (si ) + `i .(B.27)Forward moves. consider case revisiting states forward moves.Namely, suppose sequence states [s0 , s1 , . . . , sn ], state si+k = si k > 0smallest k equation holds. also assume backtracking (line 26) tookplace states si si+k . pruning module (lines 23a 23c Figure 21)purge states si si+k . Therefore, move si+k si+k+1 immediately followmove si1 si making new sequence: si1 si (= si+k ) si+k+1 .state si1 , inequality B.27 is:dist(si1 , si ) + hi (si ) hi1 (si1 ) + `i1 .(B.28)state si+k , inequality B.27 is:dist(si+k , si+k+1 ) + hi+k+1 (si+k+1 ) hi+k (si+k ) + `i+k .(B.29)Remembering si = si+k increase h(si ) possible algorithm leftstate si , conclude hi (si )+`i = hi+k (si+k ). consistent pruned move sequencesi1 si+k si+k+1 , re-write inequality B.28 as:dist(si1 , si+k ) + hi+k (si+k ) `i hi1 (si1 ) + `i1 .(B.30)Adding B.29 B.30 re-grouping terms, arrive at:[dist(si1 , si+k ) + dist(si+k , si+k+1 )]hi1 (si1 ) hi+k+1 (si+k+1 ) + `i1 + `i + `i+k .(B.31)means weighted sum distances sequence states [si1 , si+k , si+k+1 ]upper bounded differences h first last states plus sum learning amountsthree states. easy show absence backtracking, inequality B.31generalizes entire pruned sequence [s0 , s1 , . . . , sn ] sequence. Note pruning donesequence manifest gaps enumeration states. similarsubsequence si1 , si , si+1 , . . . , si+k , si+k+1 used earlier proof becoming si1 , si+k , si+k+1 .Let us denote pruned sequence indices I. inequality B.31 becomes:Xdist(si , si+1 ) h0 (s0 ) hn (sn ) +n1X`i .(B.32)i=0iINoticing h0 (s0 ) h (s0 ) hn (sn ) = 0 sn goal state, arrive at:Xdist(si , si+1 ) h (s0 ) +n1Xi=0iI17. special case si = s0 dealt simple fashion.153`i .(B.33)fiB ULITKO & L EEFinally, observing total amount learning upper bounded learning quotasum distances along pruned state sequence exactly solution(1), derivedesired upper bound:solution(1)h (s0 ) +.(B.34)Backtracking. finale proof lies showing backtracking (line 26 LRTS)affect derivation previous section. Suppose, LRTS traversed states si si+1si+2 . . . backtracked state si iteration i+k (i.e., si+k = si ). pruned solutionsequence states si1 si+k removed path (line 26) creating gapenumeration: . . . , si1 , si+k , si+k+1 , . . . . Observing states si1 , si = si+k , si+k+1 (i)backtracked (ii) among states removed path backtracking,18conclude backtracking affect heuristic value three states. Therefore,previous derivation still holds concludes proof theorem.ReferencesBarto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72(1), 81138.BioWare (1999). Baldurs Gate. http://www.bioware.com/games/baldur_gate.Blizzard (2002). Warcraft 3: Reign chaos. http://www.blizzard.com/war3.Botea, A., Muller, M., & Schaeffer, J. (2004). Near Optimal Hierarchical Path-Finding. JournalGame Development, 1(1), 728.Braginsky, D., & Estrin, D. (2002). Rumor routing algorithm sensor networks. ProceedingsFirst ACM Workshop Sensor Networks Applications, pp. 2231.Bulitko, V. (2003). Lookahead pathologies meta-level control real-time heuristic search.Proceedings 15th Euromicro Conference Real-Time Systems, pp. 1316, Porto,Portugal.Bulitko, V., Li, L., Greiner, R., & Levner, I. (2003). Lookahead pathologies single agent search.Proceedings International Joint Conference Artificial Intelligence, pp. 15311533,Acapulco, Mexico.Bulitko, V. (2004). Learning adaptive real-time search. Tech. rep. http: // arxiv. org / abs / cs.AI/ 0407016, Computer Science Research Repository (CoRR).Buro, M. (1995). Probcut: effective selective extension alpha-beta algorithm. ICCAJournal, 18, 7176.Buro, M. (2002). ORTS: hack-free RTS game environment. Proceedings InternationalComputers Games Conference, p. 12, Edmonton, Canada.Chimura, F., & Tokoro, M. (1994). Trailblazer search: new method searching capturing moving targets. Proceedings National Conference Artificial Intelligence, pp.13471352.18. fact three states cannot among states removed backtracking due incremental pruningmechanism lines 23a, 23b, 23c Figure 21. pruning mechanism clearly separates updates heuristicfunction made forward backward moves.154fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKDijkstra, E. W. (1959). note two problems connexion graphs.. Numerische Mathematik,1, 269271.Ensemble-Studios (1999). Age empires II: Age kings. http: // www.microsoft.com / games /age2.Furcy, D., & Koenig, S. (2000). Speeding convergence real-time search. ProceedingsNational Conference Artificial Intelligence, pp. 891897.Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determinationminimum cost paths. IEEE Transactions Systems Science Cybernetics, 4(2), 100107.Herbert, M., McLachlan, R., & Chang, P. (1999). Experiments driving modes urban robots.Proceedings SPIE Mobile Robots.Holte, R., Drummond, C., Perez, M., Zimmer, R., & MacDonald, A. (1994). Searching abstractions: unifying framework new high-performance algorithm. ProceedingsCanadian Artificial Intelligence Conference, pp. 263270.Holte, R. (1996). Hierarchical A*: Searching abstraction hierarchies efficiently. ProceedingsNational Conference Artificial Intelligence, pp. 530 535.Hsu, F., Campbell, M., & Hoane, A. (1995). Deep Blue system overview. Proceedings 9thACM International Conference Supercomputing, pp. 240244.Ishida, T. (1997). Real-time Search Learning Autonomous Agents. Kluwer Academic Publishers.Ishida, T., & Korf, R. (1991). Moving target search. Proceedings International JointConference Artificial Intelligence, pp. 204210.Ishida, T., & Korf, R. (1995). realtime search changing goals. IEEE Transactions PatternAnalysis Machine Intelligence, 17(6), 609619.Kallmann, M., Bieri, H., & Thalmann, D. (2003). Fully dynamic constrained Delaunay triangulations. Brunnett, G., Hamann, B., Mueller, H., & Linsen, L. (Eds.), Geometric ModellingScientific Visualization, pp. 241 257. Springer-Verlag, Heidelberg, Germany.Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjou, A., & Shimada, S. (1999).Robocup rescue: Search rescue large-scale disasters domain autonomous agentsresearch. Proceedings IEEE Conference Man, Systems, Cybernetics.Koenig, S. (1999). Exploring unknown environments real-time search reinforcement learning. Proceedings Neural Information Processing Systems, pp. 10031009.Koenig, S., & Likhachev, M. (2002). D* Lite. Proceedings National ConferenceArtificial Intelligence, pp. 476483.Koenig, S., Tovey, C., & Smirnov, Y. (2003). Performance bounds planning unknown terrain.Artificial Intelligence, 147, 253279.Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22(4), 109132.Koenig, S. (2004). comparison fast search methods real-time situated agents. Proceedings Third International Joint Conference Autonomous Agents MultiagentSystems - Volume 2, pp. 864 871.155fiB ULITKO & L EEKoenig, S., & Simmons, R. (1998). Solving robot navigation problems initial pose uncertaintyusing real-time heuristic search. Proceedings International Conference ArtificialIntelligence Planning Systems, pp. 144 153.Korf, R. (1985). Depth-first iterative deepening : optimal admissible tree search. ArtificialIntelligence, 27(3), 97109.Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189211.Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.Ng, A. Y., Coates, A., Diel, M., Ganapathi, V., Schulte, J., Tse, B., Berger, E., & Liang, E. (2004).Inverted autonomous helicopter flight via reinforcement learning. Proceedings International Symposium Experimental Robotics.Pearl, J. (1984). Heuristics. Addison-Wesley.Pottinger, D. C. (2000). Terrain analysis realtime strategy games. Proceedings ComputerGame Developers Conference.Ratner, D., & Warmuth, M. (1986). Finding shortest solution N N extension15-puzzle intractable. Proceedings National Conference Artificial Intelligence,pp. 168172.Reece, D., Krauss, M., & Dumanoir, P. (2000). Tactical movement planning individual combatants. Proceedings 9th Conference Computer Generated Forces BehavioralRepresentation.Royer, E., & Toh, C. (1999). review current routing protocols ad hoc mobile wirelessnetworks. IEEE Personal Communications, Vol. 6, pp. 4655.Russell, S., & Wefald, E. (1991). right thing: Studies limited rationality. MIT Press.Schaeffer, J., Culberson, J., Treloar, N., Knight, B., Lu, P., & Szafron, D. (1992). world championship caliber checkers program. Artificial Intelligence, 53(2-3), 273290.Shang, Y., Fromherz, M. P., Zhang, Y., & Crawford, L. S. (2003). Constraint-based routingad-hoc networks. Proceedings IEEE International Conference Information Technology: Research Education, pp. 306310, Newark, NJ, USA.Shimbo, M., & Ishida, T. (2003). Controlling learning process real-time heuristic search.Artificial Intelligence, 146(1), 141.Shue, L.-Y., Li, S.-T., & Zamani, R. (2001). intelligent heuristic algorithm project schedulingproblems. Proceedings Thirty Second Annual Meeting Decision SciencesInstitute, San Francisco.Shue, L.-Y., & Zamani, R. (1993). admissible heuristic search algorithm. Proceedings7th International Symposium Methodologies Intelligent Systems, Vol. 689 LNAI, pp.6975. Springer Verlag.Stenz, A. (1995). focussed D* algorithm real-time replanning. ProceedingsInternational Conference Artificial Intelligence, pp. 16521659.Sturtevant, N. (2005). Path refinement A* search. Tech. rep., University Alberta.Sutton, R. (2005). value function hypothesis. http:// rlai.cs.ualberta.ca/ RLAI/ valuefunctionhypothesis. html.156fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORKSutton, R., & Barto, A. (1998). Reinforcement Learning: Introduction. MIT Press.Thayer, S., Digney, B., Diaz, M., Stentz, A., Nabbe, B., & Hebert, M. (2000). Distributed roboticmapping extreme environments. Proceedings SPIE: Mobile Robots XV Telemanipulator Telepresence Technologies VII.Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Kings College, CambridgeUniversity.Woodcock, S. (2000). AI SDKs help?. Game Developer magazine.Yan, T., He, T., & Stankovic, J. A. (2003). Differentiated surveillance sensor networks.Proceedings 1st international conference Embedded networked sensor systems, pp.5162.Yu, Y., Govindan, R., & Estrin, D. (2001). Geographical energy aware routing: recursive datadissemination protocol wireless sensor networks. Tech. rep. UCLA/CSD-TR-01-0023,UCLA Computer Science Department.157fiJournal Artificial Intelligence Research 25 (2006) 17-74Submitted 12/04; published 01/06Decision-Theoretic Planning non-Markovian RewardsSylvie ThiebauxCharles GrettonJohn SlaneyDavid PriceSylvie.Thiebaux@anu.edu.auCharles.Gretton@anu.edu.auJohn.Slaney@anu.edu.auDavid.Price@anu.edu.auNational ICT Australia &Australian National UniversityCanberra, ACT 0200, AustraliaFroduald Kabanzakabanza@usherbrooke.caDepartement dInformatiqueUniversite de SherbrookeSherbrooke, Quebec J1K 2R1, CanadaAbstractdecision process rewards depend history rather merely current state called decision process non-Markovian rewards (NMRDP). decisiontheoretic planning, many desirable behaviours naturally expressed properties execution sequences rather properties states, NMRDPs formnatural model commonly adopted fully Markovian decision process (MDP) model.tractable solution methods developed MDPs directly applypresence non-Markovian rewards, number solution methods NMRDPsproposed literature. exploit compact specification non-Markovianreward function temporal logic, automatically translate NMRDP equivalent MDP solved using efficient MDP solution methods. paper presentsnmrdpp(Non-Markovian Reward Decision Process Planner), software platformdevelopment experimentation methods decision-theoretic planning nonMarkovian rewards. current version nmrdpp implements, single interface,family methods based existing well new approaches describe detail. include dynamic programming, heuristic search, structured methods. Usingnmrdpp, compare methods identify certain problem features affectperformance. nmrdpps treatment non-Markovian rewards inspired treatmentdomain-specific search control knowledge TLPlan planner, incorporatesspecial case. First International Probabilistic Planning Competition, nmrdppable compete perform well domain-independent hand-codedtracks, using search control knowledge latter.c2006AI Access Foundation. rights reserved.fiThiebaux, Gretton, Slaney, Price & Kabanza1. Introduction1.1 ProblemMarkov decision processes (MDPs) widely accepted preferred modeldecision-theoretic planning problems (Boutilier, Dean, & Hanks, 1999). fundamentalassumption behind MDP formulation system dynamics alsoreward function Markovian. Therefore, information needed determine rewardgiven state must encoded state itself.requirement always easy meet planning problems, many desirablebehaviours naturally expressed properties execution sequences (see e.g., Drummond, 1989; Haddawy & Hanks, 1992; Bacchus & Kabanza, 1998; Pistore & Traverso,2001). Typical cases include rewards maintenance property, periodicachievement goal, achievement goal within given number stepsrequest made, even simply first achievement goalbecomes irrelevant afterwards.instance, consider health care robot assists ederly disabled peopleachieving simple goals reminding important tasks (e.g. taking pill),entertaining them, checking transporting objects (e.g. checking stovestemperature bringing coffee), escorting them, searching (e.g. glassesnurse) (Cesta et al., 2003). domain, might want reward robot makingsure given patient takes pill exactly every 8 hours (and penalise failsprevent patient within time frame!), mayreward repeatedly visiting rooms ward given order reportingproblem detects, may also receive reward patients request answeredwithin appropriate time-frame, etc. Another example elevator control domain(Koehler & Schuster, 2000), elevator must get passengers origindestination efficiently possible, attempting satisfying rangeconditions providing priority services critical customers. domain,trajectories elevator desirable others, makes natural encodeproblem assigning rewards trajectories.decision process rewards depend sequence states passedrather merely current state called decision process non-Markovianrewards (NMRDP) (Bacchus, Boutilier, & Grove, 1996). difficulty NMRDPsefficient MDP solution methods directly apply them. traditional waycircumvent problem formulate NMRDP equivalent MDP, whose statesresult augmenting original NMRDP extra information capturingenough history make reward Markovian. Hand crafting MDP howeverdifficult general. exacerbated fact size MDPimpacts effectiveness many solution methods. Therefore, interestautomating translation MDP, starting natural specification nonMarkovian rewards systems dynamics (Bacchus et al., 1996; Bacchus, Boutilier,& Grove, 1997). problem focus on.18fiDecision-Theoretic Planning non-Markovian Rewards1.2 Existing Approachessolving NMRDPs setting, central issue define non-Markovian rewardspecification language translation MDP adapted class MDP solutionmethods representations would like use type problems hand.precisely, tradeoff effort spent translation, e.g. producingsmall equivalent MDP without many irrelevant history distinctions, effort requiredsolve it. Appropriate resolution tradeoff depends type representationssolution methods envisioned MDP. instance, structured representationssolution methods ability ignore irrelevant information may copecrude translation, state-based (flat) representations methods requiresophisticated translation producing MDP small feasible.two previous proposals within line research rely past linear temporallogic (PLTL) formulae specify behaviours rewarded (Bacchus et al., 1996, 1997).nice feature PLTL yields straightforward semantics non-Markovianrewards, lends range translations crudest finest. twoproposals adopt different translations adapted two different types solutionmethods representations. first (Bacchus et al., 1996) targets classical state-basedsolution methods policy iteration (Howard, 1960) generate complete policiescost enumerating states entire MDP. Consequently, adopts expensivetranslation attempts produce minimal MDP. contrast, second translation(Bacchus et al., 1997) efficient crude, targets structured solution methodsrepresentations (see e.g., Hoey, St-Aubin, Hu, & Boutilier, 1999; Boutilier, Dearden, &Goldszmidt, 2000; Feng & Hansen, 2002), require explicit state enumeration.1.3 New Approachfirst contribution paper provide language translation adaptedanother class solution methods proven quite effective dealing largeMDPs, namely anytime state-based heuristic search methods LAO* (Hansen &Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003), ancestors (Barto, Bardtke, &Singh, 1995; Dean, Kaelbling, Kirman, & Nicholson, 1995; Thiebaux, Hertzberg, Shoaff,& Schneider, 1995). methods typically start compact representationMDP based probabilistic planning operators, search forward initial state,constructing new states expanding envelope policy time permits. mayproduce approximate even incomplete policy, explicitly construct explorefraction MDP. Neither two previous proposals well-suitedsolution methods, first cost translation (most performedprior solution phase) annihilates benefits anytime algorithms, secondsize MDP obtained obstacle applicability state-basedmethods. Since cost translation size MDP resultsseverely impact quality policy obtainable deadline, needappropriate resolution tradeoff two.approach following main features. translation entirely embeddedanytime solution method, full control given parts MDPexplicitly constructed explored. MDP obtained minimal,19fiThiebaux, Gretton, Slaney, Price & Kabanzaminimal size achievable without stepping outside anytime framework, i.e.,without enumerating parts state space solution method would necessarilyexplore. formalise relaxed notion minimality, call blind minimalityreference fact require lookahead (beyond fringe).appropriate context anytime state-based solution methods, wantminimal MDP achievable without expensive pre-processing.rewarding behaviours specified PLTL, appearway achieving relaxed notion minimality powerful blind minimality withoutprohibitive translation. Therefore instead PLTL, adopt variant future lineartemporal logic (FLTL) specification language, extend handle rewards.language complex semantics PLTL, enables natural translation blind-minimal MDP simple progression reward formulae. Moreover,search control knowledge expressed FLTL (Bacchus & Kabanza, 2000) fits particularlynicely framework, used dramatically reduce fraction searchspace explored solution method.1.4 New Systemsecond contribution nmrdpp, first reported implementation NMRDP solutionmethods. nmrdpp designed software platform development experimentation common interface. Given description actions domain, nmrdpplets user play compare various encoding styles non-Markovian rewardssearch control knowledge, various translations resulting NMRDP MDP,various MDP solution methods. solving problem, made recordrange statistics space time behaviour algorithms. also supportsgraphical display MDPs policies generated.nmrdpps primary interest treatment non-Markovian rewards,also competitive platform decision-theoretic planning purely Markovian rewards.First International Probabilistic Planning Competition, nmrdpp able enroldomain-independent hand-coded tracks, attempting problems featuringcontest. Thanks use search control-knowledge, scored second placehand-coded track featured probabilistic variants blocks world logisticsproblems. surprisingly, also scored second domain-independent subtrack consisting problems taken blocks world logistic domains.latter problems released participants prior competition.1.5 New Experimental Analysisthird contribution experimental analysis factors affect performanceNMRDP solution methods. Using nmrdpp, compared behavioursinfluence parameters structure degree uncertainty dynamics,type rewards syntax used described them, reachability conditionstracked, relevance rewards optimal policy. able identify numbergeneral trends behaviours methods provide advice concerningbest suited certain circumstances. experiments also lead us rule one20fiDecision-Theoretic Planning non-Markovian Rewardsmethods systematically underperforming, identify issues claimminimality made one PLTL approaches.1.6 Organisation Paperpaper organised follows. Section 2 begins background material MDPs,NMRDPs, existing approaches. Section 3 describes new approach Section 4presents nmrdpp. Sections 5 6 report experimental analysis various approaches. Section 7 explains used nmrdpp competition. Section 8 concludesremarks related future work. Appendix B gives proofs theorems.material presented compiled series recent conference workshoppapers (Thiebaux, Kabanza, & Slaney, 2002a, 2002b; Gretton, Price, & Thiebaux, 2003a,2003b). Details logic use represent rewards may found 2005 paper(Slaney, 2005).2. Background2.1 MDPs, NMRDPs, Equivalencestart notation definitions. Given finite set states, writeset finite sequences states S, set possibly infinitestate sequences. stands possibly infinite state sequencenatural number, mean state index , (i) mean prefixh0 , . . . , . ; 0 denotes concatenation 0 .2.1.1 MDPsMarkov decision process type consider 5-tuple hS, s0 , A, Pr, Ri,finite set fully observable states, s0 initial state, finite set actions(A(s) denotes subset actions applicable S), {Pr(s, a, ) | S, A(s)}family probability distributions S, Pr(s, a, s0 ) probabilitystate s0 performing action state s, R : 7 IR reward functionR(s) immediate reward state s. well known MDPcompactly represented using dynamic Bayesian networks (Dean & Kanazawa, 1989;Boutilier et al., 1999) probabilistic extensions traditional planning languages (see e.g.,Kushmerick, Hanks, & Weld, 1995; Thiebaux et al., 1995; Younes & Littman, 2004).stationary policy MDP function : 7 A, (s) A(s)action executed state s. value V policy s0 , seekmaximise, sum expected future rewards infinite horizon, discountedfar future occur:V (s0 ) = lim EnXnR(i ) | , 0 = s0i=00 < 1 discount factor controlling contribution distant rewards.21fiThiebaux, Gretton, Slaney, Price & Kabanza................................................................................ 0 ............................@R.....................................................initial state s0 , p false two actionspossible: causes transition s1 probability0.1, change probability 0.9, btransition probabilities 0.5. state s1 , p true,actions c (stay go) lead s1 s0respectively probability 1.reward received first time p true,subsequently. is, rewarded state sequences are:hs0 , s1hs0 , s0 , s1hs0 , s0 , s0 , s1hs0 , s0 , s0 , s0 , s1etc....................................................................................................................................................................... .................. .............. ...... ................ 1 .............................0.9b6@1.00.10.50.5~s =I.....@.....c.............................1.0 ..................Figure 1: Simple NMRDP2.1.2 NMRDPsdecision process non-Markovian rewards identical MDP exceptdomain reward function . idea process passedstate sequence (i) stage i, reward R((i)) received stage i. Figure 1gives example. Like reward function, policy NMRDP depends history,mapping A. before, value policy expectationdiscounted cumulative reward infinite horizon:XnV (s0 ) = lim ER((i)) | , 0 = s0ni=0edecision process = hS, s0 , A, Pr, Ri state S, let D(s)standset state sequences rooted feasible actions D, is:eD(s)= { | 0 = A(i ) Pr(i , a, i+1 ) > 0}. Note definitioneD(s) depend R therefore applies MDPs NMRDPs.2.1.3 Equivalenceclever algorithms developed solve MDPs cannot directly applied NMRDPs.One way dealing problem translate NMRDP equivalent MDPexpanded state space (Bacchus et al., 1996). expanded states MDP(e-states, short) augment states NMRDP encoding additional informationsufficient make reward history-independent. instance, want rewardfirst achievement goal g NMRDP, states equivalent MDP wouldcarry one extra bit information recording whether g already true. e-stateseen labelled state NMRDP (via function Definition 1 below)history information. dynamics NMRDPs Markovian, actionsprobabilistic effects MDP exactly NMRDP. following definition,adapted given Bacchus et al. (1996), makes concept equivalent MDPprecise. Figure 2 gives example.22fiDecision-Theoretic Planning non-Markovian Rewards.................................................................................................................................... 0 ........ 2 .......................................................................................................................................................................................... ................. ............... ........ 0 ........................ 3 ..................@R ?0.90.1b6@1.0~s =c....@.............................1.00.5..... ................................................................................ ................................................................................................................. 0 ......... 0 ........ 1 ........ 0 ..................................................................................... ...................................................................................... ...................0.50.50.5...........b/c@.......................0.1@0.9Figure 2: MDP Equivalent NMRDP Figure 1. (s00 ) = (s02 ) = s0 . (s01 ) =(s03 ) = s1 . initial state s00 . State s01 rewarded; states not.Definition 1 MDP D0=hS 0 , s00 , A0 , Pr0, R0 equivalent NMRDP = hS, s0 , A, Pr, Riexists mapping : 0 7 that:11. (s00 ) = s0 .2. s0 0 , A0 (s0 ) = A( (s0 )).3. s1 , s2 S, A(s1 ) Pr(s1 , a, s2 ) > 0, s01 0(s01 ) = s1 , exists unique s02 0 , (s02 ) = s2 ,a0 A0 (s01 ), Pr0 (s01 , a0 , s02 ) = Pr(s1 , a0 , s2 ).e 0 ) 0e 0 (s0 ) (0 ) =4. feasible state sequence D(s0i, have: R0 (0i ) = R((i)) i.Items 13 ensure bijection feasible state sequences NMRDPfeasible e-state sequences MDP. Therefore, stationary policy MDPreinterpreted non-stationary policy NMRDP. Furthermore, item 4 ensurestwo policies identical values, consequently, solving NMRDP optimallyreduces producing equivalent MDP solving optimally (Bacchus et al., 1996):Proposition 1 Let NMRDP, D0 equivalent MDP it, 0 policye 0 ) ((i)) = 0 (0 ),D0 . Let function defined sequence prefixes (i) D(s0j (j ) = j . policy V (s0 ) = V0 (s00 ).1. Technically, definition allows sets actions A0 different, actiondiffer must inapplicable reachable states NMRDP e-states equivalentMDP. practical purposes, A0 seen identical.23fiThiebaux, Gretton, Slaney, Price & Kabanza2.2 Existing Approachesexisting approaches NMRDPs (Bacchus et al., 1996, 1997) use temporal logicpast (PLTL) compactly represent non-Markovian rewards exploit compactrepresentation translate NMRDP MDP amenable off-the-shelf solutionmethods. However, target different classes MDP representations solution methods, consequently, adopt different styles translations.Bacchus et al. (1996) target state-based MDP representations. equivalent MDPfirst generated entirely involves enumeration e-states transitionsthem. Then, solved using traditional dynamic programming methodsvalue policy iteration. methods extremely sensitive numberstates, attention paid producing minimal equivalent MDP (with least numberstates). first simple translation call pltlsim produces large MDPpost-processed minimisation solved. Another, call pltlmin,directly results minimal MDP, relies expensive pre-processing phase.second approach (Bacchus et al., 1997), call pltlstr, targets structuredMDP representations: transition model, policies, reward value functions represented compact form, e.g. trees algebraic decision diagrams (ADDs) (Hoey et al.,1999; Boutilier et al., 2000). instance, probability given proposition (statevariable) true execution action specified tree whose internalnodes labelled state variables whose previous values given variable depends, whose arcs labelled possible previous values (> ) variables,whose leaves labelled probabilities. translation amounts augmentingstructured MDP new temporal variables tracking relevant properties statesequences, together compact representation (1) dynamics, e.g. treesprevious values relevant variables, (2) non-Markovian reward functionterms variables current values. Then, structured solution methods structuredpolicy iteration SPUDD algorithm run resulting structured MDP. Neithertranslation solution methods explicitly enumerates states.review approaches detail. reader referred respectivepapers additional information.2.2.1 Representing Rewards PLTLsyntax PLTL, language chosen represent rewarding behaviours,propositional logic, augmented operators (previously) (since) (see Emerson, 1990). Whereas classical propositional logic formula denotes set states (a subsetS), PLTL formula denotes set finite sequences states (a subset ). formulawithout temporal modality expresses property must true current state, i.e.,last state finite sequence. f specifies f holds previous state (thestate one last). f1 f2 , requires f2 true point sequence, and, unless point present, f1 held ever since. formally,modelling relation |= stating whether formula f holds finite sequence (i) definedrecursively follows:(i) |= p iff p , p P, set atomic propositions24fiDecision-Theoretic Planning non-Markovian Rewards(i) |= f iff (i) 6|= f(i) |= f1 f2 iff (i) |= f1 (i) |= f2(i) |= f iff > 0 (i 1) |= f(i) |= f1 f2 iff j i, (j) |= f2 k, j < k i, (k) |= f1- f > f meaning f trueS, one define useful operators- f meaning f always true. E.g, g- g denotespoint, fifset finite sequences ending state g true first time sequence.- k fuseful abbreviation k (k times ago), k iterations modality,ki=1 f (f true k last steps), fik f ki=1 f (f truek last steps).Non-Markovian reward functions described set pairs (fi : ri ) fiPLTL reward formula ri real, semantics reward assignedsequence sum ri sequence model fi . Below, letF denote set reward formulae fi description reward function. Bacchuset al. (1996) give list behaviours might useful reward, togetherexpression PLTL. instance, f atemporal formula, (f : r) rewardsr units achievement f whenever happens. Markovian reward.- f : r) rewards every state following (and including) achievement f ,contrast (- f : r) rewards first occurrence f . (f fik f : r) rewards occurrence(ff every k steps. (n : r) rewards nth state, independentlyproperties. (2 f1 f2 f3 : r) rewards occurrence f1 immediately followedf2 f3 . reactive planning, so-called response formulae describeachievement f triggered condition (or command) c particularly useful.- c : r) every state f true following first issuewritten (fcommand rewarded. Alternatively, written (f (f c) : r)first occurrence f rewarded command. common- k c : r)reward achievement f within k steps trigger; write example (freward states f holds.theoretical point view, known (Lichtenstein, Pnueli, & Zuck, 1985)behaviours representable PLTL exactly corresponding star-free regularlanguages. Non star-free behaviours (pp) (reward even number statescontaining p) therefore representable. Nor, course, non-regular behaviourspn q n (e.g. reward taking equal numbers steps left right). shallspeculate severe restriction purposes planning.2.2.2 Principles Behind Translationsthree translations MDP (pltlsim, pltlmin, pltlstr) rely equivalence f1 f2 f2 (f1 (f1 f2 )), decompose temporal modalitiesrequirement last state sequence (i), requirementprefix (i 1) sequence. precisely, given state formula f , one com-25fiThiebaux, Gretton, Slaney, Price & Kabanzapute in2 O(||f ||) new formula Reg(f, s) called regression f s. Regressionproperty that, > 0, f true finite sequence (i) ending = iffReg(f, s) true prefix (i 1). is, Reg(f, s) represents musttrue previously f true now. Reg defined follows:Reg(p, s) = > iff p otherwise, p PReg(f, s) = Reg(f, s)Reg(f1 f2 , s) = Reg(f1 , s) Reg(f2 , s)Reg(f, s) = fReg(f1 f2 , s) = Reg(f2 , s) (Reg(f1 , s) (f1 f2 ))instance, take state p holds q not, take f = (q) (p q),meaning q must false 1 step ago, must held pointpast p must held since q last did. Reg(f, s) = q (p q), is,f hold now, previous stage, q false p q requirementstill hold. p q false s, Reg(f, s) = , indicating fcannot satisfied, regardless came earlier sequence.notational convenience, X set formulae write X X{x | x X}.translations exploit PLTL representation rewards follows. expandedstate (e-state) generated MDP seen labelled set Sub(F )subformulae reward formulae F (and negations). subformulae must(1) true paths leading e-state, (2) sufficient determine currenttruth reward formulae F , needed compute current reward. Ideallyalso (3) small enough enable that, i.e. containsubformulae draw history distinctions irrelevant determining rewardone point another. Note however worst-case, number distinctionsneeded, even minimal equivalent MDP, may exponential ||F ||. happensinstance formula k f , requires k additional bits information memorisingtruth f last k steps.2.2.3 pltlsimchoice s, Bacchus et al. (1996) consider two cases. simple case,call pltlsim, MDP obeying properties (1) (2) produced simply labellinge-state set subformulae Sub(F ) true sequence leadinge-state. MDP generated forward, starting initial e-state labelleds0 set 0 Sub(F ) subformulae true sequencehs0 i. successors e-state labelled NMRDP state subformula setgenerated follows: labelled successor s0 NMRDPset subformulae { 0 Sub(F ) | |= Reg( 0 , s0 )}.instance, consider NMRDP shown Figure 3. set F = {qp} consistssingle reward formula. set Sub(F ) consists subformulae reward formula,2. size ||f || reward formula measured length size ||F || set reward formulaeF measured sum lengths formulae F .26fiDecision-Theoretic Planning non-Markovian Rewardsstart_statea(0.16)pa(1) b(1)a(0.04) b(0.2)a(0.16)a(0.64)b(0.8)qa(0.2) b(1)a(0.8)p, qa(1) b(1)initial state, p q false.p false, action independently setsp q true probability 0.8.p q false, action b sets q trueprobability 0.8. actionseffect otherwise. reward obtainedwhenever q p. optimal policyapply b q gets produced, makingsure avoid state left-hand side,apply p gets produced,apply b indifferently forever.Figure 3: Another Simple NMRDPnegations, Sub(F ) = {p, q, p, p, q p, p, q, p, p, (qp)}. equivalent MDP produced pltlsim shown Figure 4.2.2.4 pltlminUnfortunately, MDPs produced pltlsim far minimal. Although couldpostprocessed minimisation invoking MDP solution method,expansion may still constitute serious bottleneck. Therefore, Bacchus et al. (1996) considercomplex two-phase translation, call pltlmin, capable producingMDP also satisfying property (3). Here, preprocessing phase iterates statesS, computes, state s, set l(s) subformulae, function lsolution fixpoint equation l(s) = F {Reg( 0 , s0 ) | 0 l(s0 ), s0 successor s}.subformulae l(s) candidates inclusion sets labelling respectivee-states labelled s. is, subsequent expansion phase above, taking0 l(s0 ) 0 l(s0 ) instead 0 Sub(F ) 0 Sub(F ). subformulael(s) exactly relevant way feasible execution sequences startinge-states labelled rewarded, leads expansion phase produce minimalequivalent MDP.Figure 5 shows equivalent MDP produced pltlmin NMRDP exampleFigure 3, together function l labels built. ObserveMDP smaller pltlsim MDP: reach state left-hand sidep true q false, point tracking values subformulae,q cannot become true reward formula cannot either. reflected factl({p}) contains reward formula.worst case, computing l requires space, number iterations S,exponential ||F ||. Hence question arises whether gain expansionphase worth extra complexity preprocessing phase. one questionsexperimental analysis Section 5 try answer.2.2.5 pltlstrpltlstr translation seen symbolic version pltlsim. setadded temporal variables contains purely temporal subformulae PTSub(F ) rewardformulae F , modality prepended (unless already there): = { |27fiThiebaux, Gretton, Slaney, Price & Kabanzastart_statef6,f7,f8,f9,f10Reward=0a(0.16)a(0.04) b(0.2)a(0.16)pf1,f7,f8,f9,f10Reward=0a(0.64)qf2,f6,f8,f9,f10Reward=0a(1) b(1)pf1,f3,f4,f7,f10Reward=0following subformulae Sub(F ) labele-states:f1 : pf2 : qf3 : pf4 : pf5 : q pf6 : pf7 : qf8 : pf9 : pf10 : (q p)a(0.2) b(1)a(0.8)pf1,f3,f7,f9,f10Reward=0a(1)b(0.8)p, qf1,f2,f8,f9,f10Reward=0b(1)a(1)a(1) b(1)b(1)p, qf1,f2,f3,f9,f10Reward=0a(1) b(1)p, qf1,f2,f3,f4,f5Reward=1a(1) b(1)Figure 4: Equivalent MDP Produced pltlsimstart_statef4,f5,f6Reward=0a(0.16)pf4Reward=0a(1) b(1)a(0.04) b(0.2)a(0.16)b(0.8)qf4,f5,f6Reward=0a(0.64)function l given by:l({}) = {q p, p, p}l({p}) = {q p}l({q}) = {q p, p, p}l({p, q}) = {q p, p, p}a(0.2) b(1)a(0.8)following formulae label e-states:f1 : q pf2 : pf3 : pf4 : (q p)f5 : pf6 : pp, qf3,f4,f5Reward=0a(1)b(1)p, qf2,f3,f4Reward=0a(1)b(1)p, qf1,f2,f3Reward=1a(1) b(1)Figure 5: Equivalent MDP Produced pltlmin28fiDecision-Theoretic Planning non-Markovian Rewardsqp1.00prv prv pprv p0.001. dynamics p1.000.002. dynamics p0.001.003. rewardFigure 6: ADDs Produced pltlstr. prv (previously) standsPTSub(F ), 6= 0 } { | PTSub(F )}. repeatedly applying equivalencef1 f2 f2 (f1 (f1 f2 )) subformula PTSub(F ), express currentvalue, hence reward formulae, function current values formulaestate variables, required compact representation transitionreward models.NMRDP example Figure 3, set purely temporal variables PTSub(F ) ={p, p}, identical PTSub(F ). Figure 6 shows ADDs formingpart symbolic MDP produced pltlstr: ADDs describing dynamicstemporal variables, i.e., ADDs describing effects actions brespective values, ADD describing reward.complex illustration, consider example (Bacchus et al., 1997)- (p (q r))} {> (p (q r))}F = {PTSub(F ) = {> (p (q r)), p (q r), r}set temporal variables used= {t1 : (> (p (q r))), t2 : (p (q r)), t3 : r}Using equivalences, reward decomposed expressed meanspropositions p, q temporal variables t1 , t2 , t3 follows:> (p (q r))(p (q r)) (> (p (q r)))(q r) (p (p (q r))) t1(q t3 ) (p t2 ) t1pltlsim, underlying MDP produced pltlstr far minimalencoded history features even vary one state next. However, sizeproblematic state-based approaches, structured solution methodsenumerate states able dynamically ignore variables becomeirrelevant policy construction. instance, solving MDP, may29fiThiebaux, Gretton, Slaney, Price & Kabanzaable determine temporal variables become irrelevant situationtrack, although possible principle, costly realised good policy.dynamic analysis rewards contrast pltlmins static analysis (Bacchus et al.,1996) must encode enough history determine reward reachable futurestates policy.One question arises circumstances analysis irrelevance structured solution methods, especially dynamic aspects, really effective.another question experimental analysis try address.3. fltl: Forward-Looking Approachnoted Section 1 above, two key issues facing approaches NMRDPsspecify reward functions compactly exploit compact representationautomatically translate NMRDP equivalent MDP amenable chosensolution method. Accordingly, goals provide reward function specificationlanguage translation adapted anytime state-based solution methods.brief reminder relevant features methods, consider two goalsturn. describe syntax semantics language, notion formulaprogression language form basis translation, translationitself, properties, embedding solution method. call approachfltl. finish section discussion features distinguish fltlexisting approaches.3.1 Anytime State-Based Solution Methodsmain drawback traditional dynamic programming algorithms policy iteration(Howard, 1960) explicitly enumerate states reachable s0entire MDP. interest state-based solution methods, mayproduce incomplete policies, enumerate fraction states policy iterationrequires.Let E() denote envelope policy , set states reachable(with non-zero probability) initial state s0 policy. definedE(), say policy complete, incomplete otherwise.set states E() undefined called fringe policy.fringe states taken absorbing, value heuristic. common featureanytime state-based algorithms perform forward search, starting s0repeatedly expanding envelope current policy one step forward adding onefringe states. provided admissible heuristic values fringe states,eventually converge optimal policy without necessarily needing exploreentire state space. fact, since planning operators used compactly representstate space, may even need construct small subset MDPreturning optimal policy. interrupted convergence, returnpossibly incomplete often useful policy.methods include envelope expansion algorithm (Dean et al., 1995),deploys policy iteration judiciously chosen larger larger envelopes, using successive policy seed calculation next. recent LAO algorithm (Hansen30fiDecision-Theoretic Planning non-Markovian Rewards& Zilberstein, 2001) combines dynamic programming heuristic searchviewed clever implementation particular case envelope expansion algorithm,fringe states given admissible heuristic values, policy iteration runconvergence envelope expansions, clever implementation runspolicy iteration states whose optimal value actually affected new fringestate added envelope. Another example backtracking forward searchspace (possibly incomplete) policies rooted s0 (Thiebaux et al., 1995), performed interrupted, point best policy found far returned. Real-timedynamic programming (RTDP) (Barto et al., 1995) another popular anytime algorithmMDPs learning real-time (Korf, 1990) deterministic domains,asymptotic convergence guarantees. RTDP envelope made samplepaths visited frequency determined current greedy policytransition probabilities domain. RTDP run on-line, off-line given numbersteps interrupted. variant called LRTDP (Bonet & Geffner, 2003) incorporatesmechanisms focus search states whose value yet converged, resultingconvergence speed finite time convergence guarantees.fltl translation present targets anytime algorithms, althoughcould also used traditional methods value policy iteration.3.2 Language SemanticsCompactly representing non-Markovian reward functions reduces compactly representingbehaviours interest, behaviour mean set finite sequences states(a subset ), e.g. {hs0 , s1 i, hs0 , s0 , s1 i, hs0 , s0 , s0 , s1 . . .} Figure 1. Recallreward issued end prefix (i) set. behaviours compactlyrepresented, straightforward represent non-Markovian reward functions mappingsbehaviours real numbers shall defer looking Section 3.6.represent behaviours compactly, adopt version future linear temporal logic(FLTL) (see Emerson, 1990), augmented propositional constant $, intendedread behaviour want reward happened reward received now.language $FLTL begins set basic propositions P giving rise literals:L ::= P | P | > | | $> stand true false, respectively. connectives classical, temporal modalities (next) U (weak until), giving formulae:F ::= L | F F | F F | F | F U Fweak: f1 U f2 means f1 true f2 is, ever. Unlikecommonly used strong until, imply f2 eventually true.allows us define useful operator (always): f f U (f always truek f k iterations modality (fon). also adopt notationsWktrue exactly k steps), k f i=1 f (f true within next k steps),Vk f ki=1 f (f true throughout next k steps).Although negation officially occurs literals, i.e., formulae negationnormal form (NNF), allow write formulae involving usual way,31fiThiebaux, Gretton, Slaney, Price & Kabanzaprovided equivalent NNF. every formula equivalent,literal $ eventualities (f true time)expressible. restrictions deliberate. use notationlogic theorise allocation rewards, would indeed need means sayrewards received express features liveness (always,reward eventually), fact using mechanism ensuringrewards given be, restricted purpose eventualitiesnegated dollar needed. fact, including would create technical difficultiesrelating formulae behaviours represent.semantics language similar FLTL, important difference:interpretation constant $ depends behaviour B want reward(whatever is), modelling relation |= must indexed B. therefore write(, i) |=B f mean formula f holds i-th stage arbitrary sequence ,relative behaviour B. Defining |=B first step description semantics:(, i) |=B $ iff (i) B(, i) |=B >(, i) 6|=B(, i) |=B p, p P, iff p(, i) |=B p, p P, iff p 6(, i) |=B f1 f2 iff (, i) |=B f1 (, i) |=B f2(, i) |=B f1 f2 iff (, i) |=B f1 (, i) |=B f2(, i) |=Bfiff (, + 1) |=B f(, i) |=B f1 U f2 iff k (j, j k (, j) 6|=B f2 ) (, k) |=B f1Note except subscript B first rule, standard FLTLsemantics, therefore $-free formulae keep FLTL meaning. FLTL,say |=B f iff (, 0) |=B f , |=B f iff |=B f .modelling relation |=B seen specifying formula holds,reading takes B input. next final step use |=B relation define,formula f , behaviour Bf represents, must rather assumef holds, solve B. instance, let f (p $), i.e., get rewardedevery time p true. would like Bf set finite sequences endingstate containing p. arbitrary f , take Bf set prefixesrewarded f hold sequences:Definition 2 Bf {B | |=B f }understand Definition 2, recall B contains prefixes end getreward $ evaluates true. Since f supposed describe way rewardsreceived arbitrary sequence, interested behaviours B make $true way make f hold without imposing constraints evolutionworld. However, may many behaviours property, take32fiDecision-Theoretic Planning non-Markovian Rewardsintersection,3 ensuring Bf reward prefix prefixevery behaviour satisfying f . pathological cases (see Section 3.4), makes Bfcoincide (set-inclusion) minimal behaviour B |=B f . reasonstingy semantics, making rewards minimal, f actually say rewardsallocated prefixes required truth. instance, (p $) saysreward given every time p true, even though generous distributionrewards would consistent it.3.3 Examplesintuitively clear many behaviours specified means $FLTL formulae.simple way general translate past future tense expressions,4 examples used illustrate PLTL Section 2.2 expressiblenaturally $FLTL, follows.classical goal formula g saying goal p rewarded whenever happenseasily expressed: (p $). already noted, Bg set finite sequences statesp holds last state. care p achieved get rewardedstate on, write (p $). behaviour formula representsset finite state sequences least one state p holds. contrast,formula p U (p $) stipulates first occurrence p rewarded (i.e.specifies behaviour Figure 1). reward occurrence p every ksteps, write (( k+1 p k p) k+1 $).response formulae, achievement p triggered command c,write (c (p $)) reward every state p true following firstissue command. reward first occurrence p command, write(c (p U (p$))). bounded variants reward goal achievementwithin k steps trigger command, write example (c k (p $)) rewardstates p holds.also worth noting express simple behaviours involving past tense operators.stipulate reward p always true, write $ U p. say rewardedp true since q was, write (q ($ U p)).Finally, often find useful reward holding p occurrence q.neatest expression q U ((p q) (q $)).3.4 Reward Normality$FLTL therefore quite expressive. Unfortunately, rather expressive,contains formulae describe unnatural allocations rewards. instance,may make rewards depend future behaviours rather past, may3.B |=B f , case $-free f logical theorem,Bf i.e. following normal set-theoretic conventions. limiting case harm, since$-free formulae describe attribution rewards.4. open question whether set representable behaviours $FLTL PLTL,star-free regular languages. Even behaviours same, little hopepractical translation one exists.33fiThiebaux, Gretton, Slaney, Price & Kabanzaleave open choice several behaviours rewarded.5 exampledependence future p $, stipulates reward p going holdnext. call formula reward-unstable. reward-stable f amountswhether particular prefix needs rewarded order make f true dependfuture sequence. example open choice behavior reward(p $) (p $) says either reward achievements goal preward achievements p determine which. call formula rewardindeterminate. reward-determinate f amounts set behavioursmodelling f , i.e. {B | |=B f }, unique minimum. not, Bf insufficient (toosmall) make f true.investigating $FLTL (Slaney, 2005), examine notions reward-stabilityreward-determinacy depth, motivate claim formulae rewardstable reward-determinate call reward-normal preciselycapture notion funny business. intuition ask readernote, needed rest paper. reference then, define:Definition 3 f reward-normal iff every every B , |=B f iffevery i, (i) Bf (i) B.property reward-normality decidable (Slaney, 2005). Appendix givesimple syntactic constructions guaranteed result reward-normal formulae.reward-abnormal formulae may interesting, present purposes restrict attentionreward-normal ones. Indeed, stipulate part method reward-normalformulae used represent behaviours. Naturally, formulae Section 3.3normal.3.5 $FLTL Formula Progressiondefined language represent behaviours rewarded, turnproblem computing, given reward formula, minimum allocation rewards statesactually encountered execution sequence, way satisfy formula.ultimately wish use anytime solution methods generate state sequencesincrementally via forward search, computation best done fly, sequencegenerated. therefore devise incremental algorithm based model-checkingtechnique normally used check whether state sequence model FLTL formula(Bacchus & Kabanza, 1998). technique known formula progressionprogresses pushes formula sequence.progression technique shown Algorithm 1. essence, computes modelling relation |=B given Section 3.2. However,unlike definition |=B , designeduseful states sequence become available one time, defersevaluation part formula refers future point nextstate becomes available. Let state, say , last state sequence prefix (i)5. difficulties inherent use linear-time formalisms contexts principledirectionality must enforced. shared instance formalisms developed reasoningactions Event Calculus LTL action theories (see e.g., Calvanese, De Giacomo, &Vardi, 2002).34fiDecision-Theoretic Planning non-Markovian Rewardsgenerated far, let b boolean true iff (i) behaviour Brewarded. Let $FLTL formula f describe allocation rewards possiblefutures. progression f given b, written Prog(b, s, f ), new formuladescribe allocation rewards possible futures next state, givenpassed s. Crucially, function Prog Markovian, dependingcurrent state single boolean value b. Note Prog computablelinear time length f , $-free formulae, collapses FLTL formulaprogression (Bacchus & Kabanza, 1998), regardless value b. assume Progincorporates usual simplification sentential constants >: f simplifies, f > simplifies f , etc.Algorithm 1 $FLTL ProgressionProg(true, s, $)= >Prog(false, s, $)=Prog(b, s, >)= >Prog(b, s, )=Prog(b, s, p)= > iff p otherwiseProg(b, s, p)= > iff p 6 otherwiseProg(b, s, f1 f2 ) = Prog(b, s, f1 ) Prog(b, s, f2 )Prog(b, s, f1 f2 ) = Prog(b, s, f1 ) Prog(b, s, f2 )Prog(b, s, f )= fProg(b, s, f1 U f2 ) = Prog(b, s, f2 ) (Prog(b, s, f1 ) f1 U f2 )Rew(s, f )$Prog(s, f )= true iff Prog(false, s, f ) == Prog(Rew(s, f ), s, f )fundamental property Prog following. b ((i) B):Property 1 (, i) |=B f iff (, + 1) |=B Prog(b, , f )Proof:See Appendix B.Like |=B , function Prog seems require B (or least b) input, courseprogression applied practice f one new state time ,really want compute appropriate B, namely representedf . So, similarly Section 3.2, turn second step, use Progdecide fly whether newly generated sequence prefix (i) Bfallocated reward. purpose functions $Prog Rew, also givenAlgorithm 1. Given f , function $Prog Algorithm 1 defines infinite sequenceformulae hf0 , f1 , . . .i obvious way:f0 = ffi+1 = $Prog(i , fi )decide whether prefix (i) rewarded, Rew first tries progressingformula fi boolean flag set false. gives consistent result,need reward prefix continue without rewarding (i), result35fiThiebaux, Gretton, Slaney, Price & Kabanzaknow (i) must rewarded order satisfy f . case,obtain fi+1 must progress fi again, time boolean flag setvalue true. sum up, behaviour corresponding f {(i)|Rew(i , fi )}.illustrate behaviour $FLTL progression, consider formula f = p U (p $)stating reward received first time p true. Let state pholds, Prog(false, s, f ) = ( p U (p $)) . Therefore, since formulaprogressed , Rew(s, f ) true reward received. $Prog(s, f ) = Prog(true, s, f ) => ( p U (p $)) >, reward formula fades away affect subsequentprogression steps. If, hand, p false s, Prog(false, s, f ) = (>p U (p$)) p U (p$)). Therefore, since formula progressed , Rew(s, f )false reward received. $Prog(s, f ) = Prog(false, s, f ) = p U (p$), rewardformula persists subsequent progression steps.following theorem states weak assumptions, rewards correctly allocated progression:Theorem 1 Let f reward-normal, let hf0 , f1 , . . .i result progressingsuccessive states sequence using function $Prog. Then, providedfi , Rew(i , fi ) iff (i) Bf .Proof: See Appendix Bpremise theorem f never progresses . Indeed fi =i, means even rewarding (i) suffice make f true, something mustgone wrong: earlier stage, boolean Rew made falsemade true. usual explanation original f reward-normal.instance p $, reward unstable, progresses next state ptrue there: regardless 0 , f0 = p $ = p $, Rew(0 , f0 ) = false, f1 = p,p 1 f2 = . However, (admittedly bizarre) possibilities exist:example, although p $ reward-unstable, substitution instance > $,also progresses steps, logically equivalent $ reward-normal.progression method deliver correct minimal behaviour cases(even reward-normal cases) would backtrack choice valuesboolean flags. interest efficiency, choose allow backtracking. Instead,algorithm raises exception whenever reward formula progresses , informsuser sequence caused problem. onus thus placed domainmodeller select sensible reward formulae avoid possible progression .noted worst case, detecting reward-normality cannot easierdecision problem $FLTL expected simplesyntactic criterion reward-normality. practice, however, commonsense precautionsavoiding making rewards depend explicitly future tense expressions sufficekeep things normal routine cases. generous class syntactically recognisablereward-normal formulae, see Appendix A.3.6 Reward Functionslanguage defined far, able compactly represent behaviours.extension non-Markovian reward function straightforward. represent36fiDecision-Theoretic Planning non-Markovian Rewardsfunction set6 $FLTL IR formulae associated real valued rewards.call reward function specification. formula f associated reward r ,write (f : r) . rewards assumed independent additive,reward function R represented given by:X{r | (i) Bf }Definition 4 R ((i)) =(f :r)E.g, {p U (p $) : 5.2, (q $) : 7.3}, get reward 5.2 first time pholds, reward 7.3 first time q holds onwards, reward 12.5conditions met, 0 otherwise.Again, progress reward function specification compute rewardstages . before, progression defines sequence h0 , 1 , . . .i reward functionspecifications, i+1 = RProg(i , ), RProg function applies Progformulae reward function specification:RProg(s, ) = {(Prog(s, f ) : r) | (f : r) }Then, total reward received stage simply sum real-valued rewardsgranted progression function behaviours represented formulae :X{r | Rew(i , f )}(f :r)iproceeding way, get expected analog Theorem 1, states progressioncorrectly computes non-Markovian reward functions:Theorem 2 Let reward-normal7 reward function specification, let h0 , 1 . . .iresult progressing successive statesX sequence using functionRProg. Then, provided ( : r) 6 i,{r | Rew(i , f )} = R ((i)).(f :r)iProof:Immediate Theorem 1.3.7 Translation MDPexploit compact representation non-Markovian reward function rewardfunction specification translate NMRDP equivalent MDP amenable statebased anytime solution methods. Recall Section 2 e-state MDPlabelled state NMRDP history information sufficient determineimmediate reward. case compact representation reward function specification0 , additional information summarised progression 0sequence states passed through. e-state form hs, i,6. Strictly speaking, multiset, convenience represent set, rewards multipleoccurrences formula multiset summed.7. extend definition reward-normality reward specification functions obvious way,requiring reward formulae involved reward normal.37fiThiebaux, Gretton, Slaney, Price & Kabanzastate, $FLTL IR reward function specification (obtained progression).Two e-states hs, ht, equal = t, immediate rewards same,results progressing semantically equivalent.8Definition 5 Let = hS, s0 , A, Pr, Ri NMRDP, 0 reward function specification representing R (i.e., R0 = R, see Definition 4). translate MDPD0 = hS 0 , s00 , A0 , Pr0 , R0 defined follows:1. 0 2$FLTL IR2. s00 = hs0 , 03. A0 (hs, i) = A(s)Pr(s, a, s0 ) 0 = RProg(s, )0otherwise006 (hs, i), Pr (hs, i, a, ) undefinedX5. R0 (hs, i) ={r | Rew(s, f )}4. A0 (hs, i), Pr0 (hs, i, a, hs0 , 0 i) =(f :r)6. s0 0 , s0 reachable A0 s00 .Item 1 says e-states labelled state reward function specification. Item2 says initial e-state labelled initial state original rewardfunction specification. Item 3 says action applicable e-state applicablestate labelling it. Item 4 explains successor e-states probabilitiescomputed. Given action applicable e-state hs, i, successor e-statelabelled successor state s0 via NMRDP progressions. probability e-state Pr(s, a, s0 ) NMRDP. Notecost computing Pr0 linear computing Pr sum lengthsformulae . Item 5 motivated (see Section 3.6). Finally, since items 15leave open choice many MDPs differing unreachable states contain,item 6 excludes irrelevant extensions. easy show translation leadsequivalent MDP, defined Definition 1. Obviously, function requiredDefinition1 given (hs, i) = s, proof matter checking conditions.practical implementation, labelling one step ahead definition:label initial e-state RProg(s0 , 0 ) compute current reward current reward specification label progression predecessor reward specificationscurrent state rather predecessor states. apparent below,potential reduce number states generated MDP.Figure 7 shows equivalent MDP produced $FLTL version NMRDPexample Figure 3. Recall example, PLTL reward formula q p.$FLTL, allocation rewards described ((p q) $). figure also8. Care needed notion semantic equivalence. rewards additive, determiningequivalence may involve arithmetic well theorem proving. example, reward function specification {(p $ : 3), (q $ : 2)} equivalent {((p q) $ : 5), ((p q) $ : 3), ((p q) $ : 2)}although one-one correspondence formulae two sets.38fiDecision-Theoretic Planning non-Markovian Rewardsstart_statef1Reward=0a(0.16)pf1,f2Reward=0a(1)pf1,f2,f3Reward=0a(0.04) b(0.2)a(0.16)a(0.64)qf1Reward=0b(1)a(1) b(1)b(0.8)a(0.2) b(1)a(0.8)following formulae label e-states:f1 : ((p q) $)f2 : q $f3 : q $p, qf1,f2Reward=0a(1) b(1)p, qf1,f2,f3Reward=0a(1) b(1)p, qf1,f2,f3Reward=1a(1) b(1)Figure 7: Equivalent MDP Produced fltlshows relevant formulae labelling e-states, obtained progression rewardformula. Note without progressing one step ahead, would 3 e-states state{p} left-hand side, labelled {f1 }, {f1 , f2 }, {f1 , f2 , f3 }, respectively.3.8 Blind Minimalitysize MDP obtained, i.e. number e-states contains key issueus, amenable state-based solution methods. Ideally, would likeMDP minimal size. However, know method building minimalequivalent MDP incrementally, adding parts required solution method. sinceworst case even minimal equivalent MDP larger NMRDPfactor exponential length reward formulae (Bacchus et al., 1996), constructingentirely would nullify interest anytime solution methods.However, explain, Definition 5 leads equivalent MDP exhibiting relaxednotion minimality, amenable incremental construction. inspection,may observe wherever e-state hs, successor hs0 , 0 via action a,means order succeed rewarding behaviours described meansexecution sequences start going s0 via a, necessary futurestarting s0 succeeds rewarding behaviours described 0 . hs,minimal equivalent MDP, really execution sequences succeedingrewarding behaviours described , hs0 , 0 must also minimal MDP.is, construction progression introduce e-states priori needed.Note e-state priori needed may really needed: may factexecution sequence using available actions exhibits given behaviour.39fiThiebaux, Gretton, Slaney, Price & Kabanzainstance, consider response formula (p ( k q k $)), i.e., every time trigger ptrue, rewarded k steps later provided q true then. Obviously, whether ptrue stage affects way future states rewarded. However,transition relation happens property k steps state satisfying p,state satisfying q reached, posteriori p irrelevant, needlabel e-states differently according whether p true observe occurrenceexample Figure 7, leads fltl produce extra statebottom left Figure. detect cases, would look perhaps quite deepfeasible futures, cannot constructing e-states fly. Hencerelaxed notion call blind minimality always coincide absoluteminimality.formalise difference true blind minimality. purpose,convenient define functions mapping e-states e functionsIR intuitively assigning rewards sequences NMRDP starting (e). RecallDefinition 1 maps e-state MDP underlying NMRDP state.Definition 6 Let NMRDP. Let 0 set e-states equivalent MDP D0f0 (s0 )D. Let e reachable e-state 0 . Let 0 (i) sequence e-states0e 0 ) obtained0 (i) = e. Let (i) corresponding sequence D(ssense that, j i, (j) = (0j ). , define(e) : 7(e) : 7R((i 1); ) 0 =0otherwisee i)R((i 1); ) D(0otherwiseunreachable e-state e, define (e)() (e)() 0 .Note carefully difference . former describes rewards assignedcontinuations given state sequence, latter confines rewards feasiblecontinuations. Note also well-defined despite indeterminacychoice 0 (i), since clause 4 Definition 1, choices lead valuesR.Theorem 3 Let 0 set e-states equivalent MDP D0 = hS, s0 , A, Pr, Ri.D0 minimal iff every e-state 0 reachable 0 contains two distinct e-states s01s02 (s01 ) = (s02 ) (s01 ) = (s02 ).Proof:See Appendix B.Blind minimality similar, except that, since looking ahead, distinctiondrawn feasible trajectories others future s:Definition 7 Let 0 set e-states equivalent MDP D0 = hS, s0 , A, Pr, Ri.D0 blind minimal iff every e-state 0 reachable 0 contains two distinct estates s01 s02 (s01 ) = (s02 ) (s01 ) = (s02 ).40fiDecision-Theoretic Planning non-Markovian RewardsTheorem 4 Let D0 translation Definition 5. D0 blind minimalequivalent MDP D.Proof:See Appendix B.size difference blind-minimal minimal MDPs dependprecise interaction rewards dynamics problem hand, making theoretical analyses difficult experimental results rather anecdotal. However, experimentsSection 5 6 show computation time point view, often preferable work blind-minimal MDP invest overhead computingtruly minimal one.Finally, recall syntactically different semantically equivalent reward functionspecifications define e-state. Therefore, neither minimality blind minimalityachieved general without equivalence check least complex theoremproving LTL. pratical implementations, avoid theorem proving favour embedding (fast) formula simplification progression regression algorithms.means principle approximate minimality blind minimality,appears enough practical purposes.3.9 Embedded Solution/ConstructionBlind minimality essentially best achievable anytime state-based solution methods typically extend envelope one step forward without looking deeperfuture. translation blind-minimal MDP trivially embeddedsolution methods. results on-line construction MDP: method entirelydrives construction parts MDP feels need explore,leave others implicit. time short, suboptimal even incomplete policy mayreturned, fraction state expanded state spaces might constructed.Note solution method raise exception soon one reward formulae progresses , i.e., soon expanded state hs, built ( : r) ,since acts detector unsuitable reward function specifications.extent enabled blind minimality, approach allows dynamic analysisreward formulae, much pltlstr (Bacchus et al., 1997). Indeed, executionsequences feasible particular policy actually explored solution method contribute analysis rewards policy. Specifically, reward formulae generatedprogression given policy determined prefixes execution sequencesfeasible policy. dynamic analysis particularly useful, since relevancereward formulae particular policies (e.g. optimal policy) cannot detected priori.forward-chaining planner TLPlan (Bacchus & Kabanza, 2000) introduced ideausing FLTL specify domain-specific search control knowledge formula progressionprune unpromising sequential plans (plans violating knowledge) deterministicsearch spaces. shown provide enormous time gains, leading TLPlanwin 2002 planning competition hand-tailored track. approach basedprogression, provides elegant way exploit search control knowledge, yetcontext decision-theoretic planning. results dramatic reduction41fiThiebaux, Gretton, Slaney, Price & Kabanzafraction MDP constructed explored, therefore substantially betterpolicies deadline.achieve follows. specify, via $-free formula c0 , properties knowmust verified paths feasible promising policies. simply progress c0alongside reward function specification, making e-states triples hs, , ci c $-freeformula obtained progression. prevent solution method applying actionleads control knowledge violated, action applicability condition (item3 Definition 5) becomes: A0 (hs, , ci) iff A(s) c 6= (the changesstraightforward). instance, effect control knowledge formula (p q)remove consideration feasible path p followed q. detectedsoon violation occurs, formula progresses . Although paper focusesnon-Markovian rewards rather dynamics, noted $-free formulaealso used express non-Markovian constraints systems dynamics,incorporated approach exactly control knowledge.3.10 DiscussionExisting approaches (Bacchus et al., 1996, 1997) advocate use PLTL finitepast specify non-Markovian rewards. PLTL style specification, describepast conditions get rewarded now, $FLTL describeconditions present future future states rewarded.behaviours rewards may scheme, naturalness thinking onestyle depends case. Letting kids strawberry dessertgood day fits naturally past-oriented account rewards, whereaspromising may watch movie tidy room (indeed, making sensewhole notion promising) goes naturally $FLTL. One advantage PLTLformulation trivially enforces principle present rewards dependfuture states. $FLTL, responsibility placed domain modeller. bestoffer exception mechanism recognise mistakes effects appear,syntactic restrictions. hand, greater expressive power $FLTL openspossibility considering richer class decision processes, e.g. uncertaintyrewards received (the dessert movie) (some time next week,rains).rate, believe $FLTL better suited PLTL solving NMRDPsusing anytime state-based solution methods. pltlsim translation could easily embedded solution method, loses structure original formulaeconsidering subformulae individually. Consequently, expanded state space easilybecomes exponentially bigger blind-minimal one. problematicsolution methods consider, size severely affects performance solutionquality. pre-processing phase pltlmin uses PLTL formula regression find setssubformulae potential labels possible predecessor states, subsequentgeneration phase builds MDP representing histories make difference way actually feasible execution sequences rewarded.recover structure original formula, best case, MDP producedexponentially smaller blind-minimal one. However, prohibitive cost42fiDecision-Theoretic Planning non-Markovian Rewardspre-processing phase makes unsuitable anytime solution methods. consider method based PLTL regression achieve meaningful relaxednotion minimality without costly pre-processing phase. fltl approach based$FLTL progression precisely that, letting solution method resolvetradeoff quality cost principled way intermediate two extremesuggestions above.structured representation solution methods targeted Bacchus et al. (1997)differ anytime state-based solution methods fltl primarily aims at, particularrequire explicit state enumeration all. Here, non-minimalityproblematic state-based approaches. virtue size MDP produced,pltlstr translation is, pltlsim, clearly unsuitable anytime state-based methods.9another sense, too, fltl represents middle way, combining advantages conferredstate-based structured approaches, e.g. pltlmin one side, pltlstrother. former fltl inherits meaningful notion minimality. latter,approximate solution methods used perform restricted dynamic analysisreward formulae. particular, formula progression enables even state-based methodsexploit structure $FLTL space. However, gap blindtrue minimality indicates progression alone insufficient always fully exploitstructure. hope pltlstr able take advantage full structurereward function, also possibility fail exploit even much structurefltl, efficiently. empirical comparison three approaches needed answerquestion identify domain features favoring one other.4. NMRDPPfirst step towards decent comparison different approaches frameworkincludes all. Non-Markovian Reward Decision Process Planner, nmrdpp,platform development experimentation approaches NMRDPs.provides implementation approaches described common framework,within single system, common input language. nmrdpp available on-line,see http://rsise.anu.edu.au/~charlesg/nmrdpp. worth noting Bacchus et al.(1996, 1997) report implementation approaches.4.1 Input languageinput language enables specification actions, initial states, rewards, searchcontrol-knowledge. format action specification essentiallySPUDD system (Hoey et al., 1999). reward specification one formulae,associated name real number. formulae either PLTL $FLTL.Control knowledge given language chosen reward. Controlknowledge formulae verified sequence states feasiblegenerated policies. Initial states simply specified part control knowledgeexplicit assignments propositions.9. would interesting, hand, use pltlstr conjunction symbolic versionsmethods, e.g. Symbolic LAO* (Feng & Hansen, 2002) Symbolic RTDP (Feng, Hansen, & Zilberstein,2003).43fiThiebaux, Gretton, Slaney, Price & Kabanzaaction flipheads (0.5)endactionaction tiltheads (heads (0.9) (0.1))endactionheads = ff[first, 5.0]? heads ~prv (pdi heads)[seq, 1.0]? (prv^2 heads) (prv heads) ~headsFigure 8: Input Coin Example. prv (previously) stands-.pdi (past diamond) standsinstance, consider simple example consisting coin showing either headstails (heads). two actions performed. flip action changescoin show heads tails 50% probability. tilt action changes 10%probability, otherwise leaving is. initial state tails. get reward 5.0- heads PLTL) reward 1.0first head (this written headstime achieve sequence heads, heads, tails (2 heads heads heads PLTL).input language, NMRDP described shown Figure 8.4.2 Common frameworkcommon framework underlying nmrdpp takes advantage fact NMRDPsolution methods can, general, divided distinct phases preprocessing,expansion, solving. first two optional.pltlsim, preprocessing simply computes set Sub(F ) subformulae rewardformulae. pltlmin, also includes computing labels l(s) state s.pltlstr, preprocessing involves computing set temporal variables wellADDs dynamics rewards. fltl require preprocessing.Expansion optional generation entire equivalent MDP prior solving.Whether off-line expansion sensible depends MDP solution method used.state-based value policy iteration used, MDP needs expanded anyway.If, hand, anytime search algorithm structured method used,definitely bad idea. experiments, often used expansion solely purposemeasuring size generated MDP.Solving MDP done using number methods. Currently, nmrdpp providesimplementations classical dynamic programming methods, namely state-based valuepolicy iteration (Howard, 1960), heuristic search methods: state-based LAO* (Hansen &Zilberstein, 2001) using either value policy iteration subroutine, one structuredmethod, namely SPUDD (Hoey et al., 1999). Prime candidates future developments(L)RTDP (Bonet & Geffner, 2003), symbolic LAO* (Feng & Hansen, 2002), symbolicRTDP (Feng et al., 2003).44fiDecision-Theoretic Planning non-Markovian Rewardsload coin NMRDPpltlstr preprocessing> loadWorld(coin)> preprocess(sPltl)> startCPUtimer> spudd(0.99, 0.0001)> stopCPUtimer> readCPUtimer1.22000> iterationCount1277> displayDot(valueToDot)Expected value18.8718.62report number iterationsdisplay ADD value function(prv heads)(prv (prv pdi heads))23.87report solving timeheads(prv heads)(prv (prv pdi heads))solve MDP SPUDD(, )23.62(prv^2 heads)(prv pdi heads)18.2523.15(prv pdi heads)19.2524.15display policy> displayDot(policyToDot)Optimal policyheads(prv heads)flip>>>6>tiltpltlmin preprocessingcompletely expand MDPreport MDP sizepreprocess(mPltl)expanddomainStateSizeprintDomain ("") | show-domain.rbReward=0flip(0.5)flip(0.5)display postcript rendering MDPtilt(0.9)tilt(0.1)headsReward=5flip(0.5)headsReward=0tilt(0.1)flip(0.5)tilt(0.9)tilt(0.9)flip(0.5)tilt(0.1)flip(0.5)Reward=1tilt(0.9)flip(0.5)tilt(0.9)flip(0.5)tilt(0.1)tilt(0.1)flip(0.5)Reward=0flip(0.5)flip(0.5)flip(0.5)tilt(0.9)tilt(0.1)headsReward=0solve MDP VI(, )report number iterations> valIt(0.99, 0.0001)> iterationCount1277> getPolicy...output policy (textual)Figure 9: Sample Session45fiThiebaux, Gretton, Slaney, Price & Kabanza4.3 Approaches coveredAltogether, various types preprocessing, choice whether expand,MDP solution methods, give rise quite number NMRDP approaches, including,limited previously mentioned (see e.g. pltlstr(a) below). combinations possible. E.g., state-based processing variants incompatible structuredsolution methods (the converse possible principle, however). Also, presentstructured form preprocessing $FLTL formulae.pltlstr(a) example interesting variant pltlstr, obtainconsidering additional preprocessing, whereby state space explored (without explicitlyenumerating it) produce BDD representation e-states reachable startstate. done starting BDD representing start e-state, repeatedlyapplying action. Non-zero probabilities converted ones result or-edlast result. action adds reachable e-states BDD,sure represents reachable e-state space. used additional controlknowledge restrict search. noted without phase pltlstr makesassumptions start state, thus left possible disadvantage. Similarstructured reachability analysis techniques used symbolic implementationLAO* (Feng & Hansen, 2002). However, important aspecttemporal variables also included BDD.4.4 nmrdpp Systemnmrdpp controlled command language, read either file interactively. command language provides commands different phases (preprocessing,expansion, solution) methods, commands inspect resulting policy valuefunctions, e.g. rendering via DOT (AT&T Labs-Research, 2000), well supportingcommands timing memory usage. sample session, coin NMRDPsuccessively solved pltlstr pltlmin shown Figure 9.nmrdpp implemented C++, makes use number supporting libraries.particular, relies heavily CUDD package manipulating ADDs (Somenzi,2001): action specification trees converted stored ADDs system,moreover structured algorithms rely heavily CUDD ADD computations.state-based algorithms make use MTL Matrix Template Library matrixoperations. MTL takes advantage modern processor features MMX SSEprovides efficient sparse matrix operations. believe implementationsMDP solution methods comparable state art. instance, foundimplementation SPUDD comparable performance (within factor 2)reference implementation (Hoey et al., 1999). hand, believe datastructures used regression progression temporal formulae could optimised.5. Experimental Analysisfaced three substantially different approaches easy compare,performance depend domain features varied structuretransition model, type, syntax, length temporal reward formula, presence46fiDecision-Theoretic Planning non-Markovian Rewardsrewards unreachable irrelevant optimal policy, availability good heuristicscontrol-knowledge, etc, interactions factors. section,report experimental investigation influence factors tryanswer questions raised previously:101. dynamics domain predominant factor affecting performance?2. type reward major factor?3. syntax used describe rewards major factor?4. overall best method?5. overall worst method?6. preprocessing phase pltlmin pay, compared pltlsim?7. simplicity fltl translation compensate blind-minimality,benefit true minimality outweigh cost pltlmin preprocessing?8. dynamic analyses rewards pltlstr fltl effective?9. one analyses powerful, rather complementary?cases all, able identify systematic patterns. resultssection obtained using Pentium4 2.6GHz GNU/Linux 2.4.20 machine 500MBram.5.1 Preliminary RemarksClearly, fltl pltlstr(a) great potential exploiting domain-specific heuristics control-knowledge; pltlmin less so. avoid obscuring results, thereforerefrained incorporating features experiments. running LAO*,heuristic value state crudest possible (the sum reward valuesproblem). Performance results interpreted light necessarilyreflect practical abilities methods able exploit features.begin general observations. One question raised whethergain PLTL expansion phase worth expensive preprocessing performedpltlmin, i.e. whether pltlmin typically outperforms pltlsim. definitively answerquestion: pathological exceptions, preprocessing pays. found expansionbottleneck, post-hoc minimisation MDP produced pltlsimhelp much. pltlsim therefore little practical interest, decidedreport results performance, often order magnitude worsepltlmin. Unsurprisingly, also found pltlstr would typically scale larger statespaces, inevitably leading outperform state-based methods. However, effectuniform: structured solution methods sometimes impose excessive memory requirementsmakes uncompetitive certain cases, example n f , large n,features reward formula.10. executive summary answers executive reader. 1. no, 2. yes, 3. yes, 4. pltlstrfltl, 5. pltlsim, 6. yes, 7. yes no, respectively, 8. yes, 9. yes, respectively.47fiThiebaux, Gretton, Slaney, Price & Kabanza5.2 DomainsExperiments performed four hand-coded domains (propositions + dynamics)random domains. hand-coded domain n propositions pi , dynamicsmakes every state possible eventually reachable initial statepropositions false. first two domains, spudd-linear spudd-expondiscussed Hoey et al. (1999); two others own.intention spudd-linear take advantage best case behaviourSPUDD. proposition pi , action ai sets pi true propositionspj , 1 j < false. spudd-expon, used Hoey et al. (1999) demonstrateworst case behaviour SPUDD. proposition pi , action ai sets pitrue propositions pj , 1 j < true (and sets pi false otherwise),sets latter propositions false. third domain, called on/off, one turn-onone turn-off action per proposition. turn-on-pi action probabilisticallysucceeds setting pi true pi false. turn-off action similar. fourthdomain, called complete, fully connected reflexive domain. proposition piaction ai sets pi true probability i/(n + 1) (and false otherwise)pj , j 6= true false probability 0.5. Note ai cause transition2n states.Random domains size n also involve n propositions. method generatingdynamics detailed appendix C. Let us summarise saying ablegenerate random dynamics exhibiting given degree structure given degreeuncertainty. Lack structure essentially measures bushiness internal partADDs representing actions, uncertainty measures bushiness leaves.5.3 Influence Dynamicsinteraction dynamics reward certainly affects performancedifferent approaches, though strikingly factors reward type (seebelow). found reward scheme, varying degree structureuncertainty generally change relative success different approaches.instance, Figures 10 11 show average run time methods functiondegree structure, resp. degree uncertainty, random problems size n = 6reward n > (the state encountered stage n rewarded, regardless properties11 ).Run-time increases slightly degrees, significant change relativeperformance. typical graphs obtain rewards.Clearly, counterexamples observation exist. notable casesextreme dynamics, instance spudd-expon domain. Although small valuesn, n = 6, pltlstr approaches faster others handling rewardn > virtually type dynamics encountered, perform poorlyreward spudd-expon. explained fact small fractionspudd-expon states reachable first n steps. n steps, fltl immediatelyrecognises reward consequence, formula progressed >.pltlmin discovers fact expensive preprocessing. pltlstr,hand, remains concerned prospect reward, pltlsim would.11.n $$FLTL48fiAverage CPU time (sec)Decision-Theoretic Planning non-Markovian Rewards302520151050.10.30.50.70.91.1Structure (0:Structured, ... 1:Unstructured)FLTLPLTLMINPLTLSTRUCTPLTLSTRUCT(A)Figure 10: Changing Degree StructureAverage CPU time (sec)3525201510500.20.40.60.811.2Uncertainty (0:Certain, ... 1:Uncertain)FLTLPLTLMINPLTLSTRUCTPLTLSTRUCT(A)Figure 11: Changing Degree Uncertainty5.4 Influence Reward Typestype reward appears stronger influence performance dynamics.unsurprising, reward type significantly affects size generated MDP:certain rewards make size minimal equivalent MDP increase constantnumber states constant factor, others make increase factor exponentiallength formula. Table 1 illustrates this. third column reports sizeminimal equivalent MDP induced formulae left hand side.12legitimate question whether direct correlation size increase(in)appropriateness different methods. instance, might expect state-basedmethods particularly well conjunction reward types inducing small MDP12. figures necessarily valid non-completely connected NMRDPs. Unfortunately, evencompletely connected domains, appear much cheaper way determine MDPsize generate count states.49fiThiebaux, Gretton, Slaney, Price & Kabanzatypefirst time pipi sequence start statetwo consecutive pipi n times agoformula- ni=1 pi )(ni=1 pi ) ((ni=1 pi ) n >n1i=1(pi pi+1 )n ni=1 pisizeO(1)||S||O(n)||S||O(nk )||S||O(2n )||S||fastestpltlstr(a)fltlpltlstrpltlstrslowestpltlminpltlstrfltlpltlminTable 1: Influence Reward Type MDP Size Method PerformanceAverage CPU time (sec)100060040020022.533.544.555.5nAPPROACHES prvInAPPROACHES prvOutFigure 12: Changing Syntaxotherwise badly comparison structured methods. Interestingly, alwayscase. instance, Table 1 whose last two columns report fastest slowestmethods range hand-coded domains 1 n 12, first row contradictsexpectation. Moreover, although pltlstr fastest last row, larger valuesn (not represented table), aborts lack memory, unlikemethods.obvious observations arising experiments pltlstr nearlyalways fastest runs memory. Perhaps interesting resultssecond row, expose inability methods based PLTL dealrewards specified long sequences events. converting reward formulaset subformulae, lose information order events,recovered laboriously reasoning. $FLTL progression contrast takes events onetime, preserving relevant structure step. experimentation led usobserve PLTL based algorithms perform poorly reward specified using- k f , fik f (f true k steps ago, within last kformulae form k f ,steps, last k steps).5.5 Influence SyntaxUnsurprisingly, find syntax used express rewards, affects lengthformula, major influence run time. typical example effectcaptured Figure 12. graph demonstrates re-expressing prvOut n (ni=1 pi )50fiDecision-Theoretic Planning non-Markovian RewardsState count/(2^n)119753102468101214nPLTLMINFLTLFigure 13: Effect Multiple Rewards MDP sizeTotal CPU time (sec)1500100050002468101214nFLTLPLTLMINPLTLSTRUCTPLTLSTRUCT(A)Figure 14: Effect Multiple Rewards Run TimeprvIn ni=1 n pi , thereby creating n times temporal subformulae, altersrunning time PLTL methods. fltl affected $FLTL progression requires twoiterations reward formula. graph represents averages runningtimes methods, complete domain.serious concern relation PLTL approaches handling rewardspecifications containing multiple reward elements. notably found pltlminnecessarily produce minimal equivalent MDP situation. demonstrate, consider set reward formulae {f1 , f2 , . . . , fn }, associatedreal value r. Given this, PLTL approaches distinguish unnecessarily pastbehaviours lead identical future rewards. may occur rewarde-state determined truth value f1 f2 . formula necessarily requiree-states distinguish cases {f1 >, f2 } {f1 , f2 >}hold; however, given specification, pltlmin makes distinction. example,51fiThiebaux, Gretton, Slaney, Price & Kabanzataking fi = pi , Figure 13 shows fltl leads MDP whose size 3 timesNMRDP. contrast, relative size MDP produced pltlminlinear n, number rewards propositions. results obtainedhand-coded domains except spudd-expon. Figure 14 shows run-times functionn complete. fltl dominates overtaken pltlstr(A) large valuesn, MDP becomes large explicit exploration practical. obtainminimal equivalent MDP using pltlmin, bloated reward specification form{( ni=1 (pi nj=1,j6=i pj ) : r), . . . , ( ni=1 pi : n r)} necessary, which, virtueexponential length, adequate solution.5.6 Influence Reachabilityapproaches claim ability ignore variables irrelevantcondition track unreachable:13 pltlmin detects preprocessing,pltlstr exploits ability structured solution methods ignore them, fltl ignores progression never exposes them. However, given mechanismsavoiding irrelevance different, expect corresponding differences effects.experimental investigation, found differences performance best illustrated looking response formulae, assert trigger condition c reachedreward received upon achievement goal g in, resp. within, k steps.- k c, $FLTL, (c k (g $)), resp.PLTL, written g k c, resp. g(c k (g $))goal unreachable, PLTL approaches perform well. always false,goal g lead behavioural distinctions. hand, constructingMDP, fltl considers successive progressions k g without able detectunreachable actually fails happen. exactly blindness blindminimality amounts to. Figure 15 illustrates difference performance functionnumber n propositions involved spudd-linear domain, rewardform g n c, g unreachable.fltl shines trigger unreachable. Since c never happens, formulaalways progress itself, goal, however complicated, never tracked generated MDP. situation PLTL approaches still consider k c subformulae,discover, expensive preprocessing pltlmin, reachability analysis pltlstr(a), never pltlstr, irrelevant. illustrated Figure 16,spudd-linear reward form g n c, c unreachable.5.7 Dynamic IrrelevanceEarlier claimed one advantage pltlstr fltl pltlmin pltlsimformer perform dynamic analysis rewards capable detecting irrelevancevariables particular policies, e.g. optimal policy. experiments confirmclaim. However, reachability, whether goal triggering conditionresponse formula becomes irrelevant plays important role determining whether13. sometimes speak conditions goals reachable achievable rather feasible,although may temporally extended. keep line conventional vocabularyphrase reachability analysis.52fiDecision-Theoretic Planning non-Markovian RewardsTotal CPU time (sec)350250150100502468101214nFLTLPLTLMINPLTLSTRUCTPLTLSTRUCT(A)Figure 15: Response Formula Unachievable GoalTotal CPU time (sec)350250150100501357911nFLTLPLTLMINPLTLSTRUCTPLTLSTRUCT(A)Figure 16: Response Formula Unachievable Triggerpltlstr fltl approach taken: pltlstr able dynamically ignore goal,fltl able dynamically ignore trigger.illustrated Figures 17 18. figures, domain consideredon/off n = 6 propositions, response formula g n c before,g c achievable. response formula assigned fixed reward. study effectdynamic irrelevance goal, Figure 17, achievement g rewarded valuer (i.e. (g : r) PLTL). Figure 18, hand, study effectdynamic irrelevance trigger achievement c rewarded value r.figures show runtime methods r increases.Achieving goal, resp. trigger, made less attractive r increasespoint response formula becomes irrelevant optimal policy.happens, run-time pltlstr resp. fltl, exhibits abrupt durable improvement.figures show fltl able pick irrelevance trigger, pltlstr ableexploit irrelevance goal. expected, pltlmin whose analysis static pick53fiThiebaux, Gretton, Slaney, Price & KabanzaTotal CPU time (sec)20015010050050100150200250300350rPLTLMINPLTLSTRUCTFLTLPLTLSTRUCT (A)Average CPU time (sec)Figure 17: Response Formula Unrewarding Goal20015010050050100150200250300350rPLTLMINFLTLPLTLSTRUCTPLTLSTRUCT(A)Figure 18: Response Formula Unrewarding Triggereither performs consistently badly. Note figures, pltlstr progressivelytakes longer compute r increases value iteration requires additional iterationsconverge.5.8 Summaryexperiments artificial domains, found pltlstr fltl preferable statebased PLTL approaches cases. one insists using latter, stronglyrecommend preprocessing. fltl technique choice reward requires trackinglong sequence events desired behaviour composed many elementsidentical rewards. response formulae, advise use pltlstr probabilityreaching goal low achieving goal costly, conversely, adviseuse fltl probability reaching triggering condition low reachingcostly. cases, attention paid syntax reward formulae54fiDecision-Theoretic Planning non-Markovian Rewardsparticular minimising length. Indeed, could expected, found syntaxformulae type non-Markovian reward encode predominantfactor determining difficulty problem, much featuresMarkovian dynamics domain.6. Concrete Exampleexperiments far focused artificial problems aimed characterisingstrengths weaknesses various approaches. look concrete exampleorder give sense size interesting problems techniquessolve. example derived Miconic elevator classical planning benchmark(Koehler & Schuster, 2000). elevator must get number passengers originfloor destination. Initially, elevator arbitrary floor passengerserved boarded elevator. version problem, one singleaction causes elevator service given floor, effect unservedpassengers whose origin serviced floor board elevator, boarded passengerswhose destination serviced floor unboard become served. task planelevator movement passengers eventually served.14two variants Miconic. simple variant, reward receivedtime passenger becomes served. hard variant, elevator also attemptsprovide range priority services passengers special requirements: many passengersprefer travelling single direction (either down) destination, certainpassengers might offered non-stop travel destination, finally, passengersdisabilities young children supervised inside elevatorpassenger (the supervisor) assigned them. omit VIP conflicting groupservices present original hard Miconic problem, reward formulaecreate additional difficulties.formulation problem makes use propositions PDDL description Miconic used 2000 International Planning Competition: dynamic propositionsrecord floor elevator currently whether passengers served boarded,static propositions record origin destination floors passengers, wellcategories (non-stop, direct-travel, supervisor, supervised) passengers fall in. However,formulation differs PDDL description two interesting ways. Firstly, sinceuse rewards instead goals, able find preferred solution evengoals cannot simultaneously satisfied. Secondly, priority services naturallydescribed terms non-Markovian rewards, able use action description simple hard versions, whereas PDDL description hard miconicrequires additional actions (up, down) complex preconditions monitor satisfaction priority service constraints. reward schemes Miconic encapsulatedfour different types reward formula.1. simple variant, reward received first time passenger Pi served:14. experimented stochastic variants Miconic passengers small probabilitydesembarking wrong floor. However, find useful present results deterministicversion since closer Miconic deterministic planning benchmark since, shownbefore, rewards far crucial impact dynamics relative performance methods.55fiThiebaux, Gretton, Slaney, Price & KabanzaPLTL:ServedPi fi ServedPi$FLTL:ServedPi U (ServedPi $)2. Next, reward received time non-stop passenger Pi served one stepboarding elevator:PLTL:N onStopPi BoardedPi ServedPi ServedPi$FLTL:((N onStopPi BoardedPi ServedPi ServedPi ) $)3. Then, reward received time supervised passenger Pi servedaccompanied times inside elevator supervisor15 Pj :PLTL:$FLTL:SupervisedPi SupervisorPj Pi ServedPifi ServedPi fi(BoardedPi BoardedPj )ServedPi U ((BoardedPi SupervisedPi (BoardedPj SupervisorPj Pi )ServedPi ) (ServededPi $))4. Finally, reward received time direct travel passenger Pi servedtravelled one direction since boarding, e.g., case going up:DirectPW W ServedPi ServedPi(( j k>j (AtF loork AtF loorj )) (BoardedPi BoardedPi ))W W$FLTL: ((DirectPi BoardedPi ) (ServedPi U ((( j k>i AtF loorj AtF loork )ServedPi ) (servedPi $))))PLTL:similarly case going down.Experiments section run Dual Pentium4 3.4GHz GNU/Linux 2.6.11machine 1GB ram. first experimented simple variant, giving reward50 time passenger first served. Figure 19 shows CPU time takenvarious approaches solve random problems increasing number n floorspassengers, Figure 20 shows number states expanded so. datapoint corresponds one random problem. fair structured approach,ran pltlstr(a) able exploit reachability start state. first observationalthough pltlstr(a) best small values n, quickly runs memory.pltlstr(a) pltlsim need track formulae form fi ServedPipltlsim not, conjecture run memory earlier.second observation attempts PLTL minimisation pay much here.pltlmin reduced memory tracks fewer subformulae, sizeMDP produces identical size pltlsim MDP largerfltl MDP. size increase due fact PLTL approaches label differentlye-states passengers served, depending become served(for passengers, reward formula true e-state). contrast, fltlimplementation progression one step ahead labels e-states reward15. understand $FLTL formula, observe get reward iff (BoardedPi SupervisedPi )(BoardedPj SupervisorPj Pi ) holds ServedPi becomes true, recall formula q U ((pq) (q $)) rewards holding p occurrence q.56fiDecision-Theoretic Planning non-Markovian RewardsTotal CPU time (sec)70004000200010002468101214nFLTLPLTLSIMPLTLMINPLTLSTR(A)Figure 19: Simple Miconic - Run Time45State count/(2^n)40353025201510502468101214nFLTLPLTLSIM, PLTLMINFigure 20: Simple Miconic - Number Expanded Statesformulae relevant passengers still need served, formulaeprogressed >. gain number expanded states materialises run time gains,resulting fltl eventually taking lead.second experiment illustrates benefits using even extremely simple admissible heuristic conjunction fltl. heuristic applicable discounted stochasticshortest path problems, discounts rewards shortest time futurepossible. simply amounts assigning fringe state value 50 timesnumber still unserved passengers (discounted once), results avoiding floorspassenger waiting destination boarded passenger.Figures 21 22 compare run time number states expanded fltl usedconjunction value iteration (valIt) used conjunction LAO*57fiThiebaux, Gretton, Slaney, Price & KabanzaTotal CPU time (sec)35000200001000050002468101214nFLTLLAO(h)FLTLLAO(u)FLTLvalItFigure 21: Effect Simple Heuristic Run TimeState count/(2^n)504030201002468101214nFLTLLAO(h)FLTLvalIt,FLTLLAO(u)Figure 22: Effect Simple Heuristic Number Expanded Statessearch informed heuristic (LAO(h)). Uninformed LAO* (LAO*(u), i.e. LAO*heuristic 50 n node) also included reference point showoverhead induced heuristic search. seen graphs, heuristic searchgenerates significantly fewer states eventually pays terms run time.final experiment, considered hard variant, giving reward 50service (1), reward 2 non-stop travel (2), reward 5 appropriate supervision(3), reward 10 direct travel (2). Regardless number n floorspassengers, problems feature single non-stop traveller, third passengers requiresupervision, half passengers care traveling direct. CPU time numberstates expanded shown Figures 23 24, respectively. simple case,pltlsim pltlstr quickly run memory. Formulae type (2) (3) createmany additional variables track approaches, problem seem58fiDecision-Theoretic Planning non-Markovian RewardsTotal CPU time (sec)14000800040002000234567nFLTLPLTLSIMPLTLMINPLTLSTRUCT(A)Figure 23: Hard Miconic - Run TimeState count/(2^n)100806040200234567nFLTLPLTLSIMPLTLMINFigure 24: Hard Miconic - Number Expanded Statesexhibit enough structure help pltlstr. fltl remains fastest. Here,seem much due size generated MDP slightlypltlmin MDP, rather overhead incurred minimisation. Anotherobservation arising experiment small instances handledcomparison classical planning version problem solved state artoptimal classical planners. example, 2000 International Planning Competition,PropPlan planner (Fourman, 2000) optimally solved instances hard Miconic20 passengers 40 floors 1000 seconds much less powerful machine.59fiThiebaux, Gretton, Slaney, Price & Kabanza7. nmrdpp Probabilistic Planning Competitionreport behaviour nmrdpp probabilistic track 4th International Planning Competition (IPC-4). Since competition feature non-Markovianrewards, original motivation taking part compare solution methodsimplemented nmrdpp Markovian setting. objective largely underestimatedchallenges raised merely getting planner ready competition, especiallycompetition first kind. end, decided successfully preparing nmrdpp attempt problems competition using one solution method (and possiblysearch control knowledge), would honorable result.crucial problem encountered translation PPDDL (Younes &Littman, 2004), probabilistic variant PDDL used input language competition, nmrdpps ADD-based input language. translating PPDDL ADDspossible theory, devising translation practical enough needcompetition (small number variables, small, quickly generated, easily manipulableADDs) another matter. mtbdd, translator kindly made available participantscompetition organisers, always able achieve required efficiency.times, translation quick nmrdpp unable use generated ADDs efficiently. Consequently, implemented state-based translator top PDDL parserbackup, opted state-based solution method since rely ADDscould operate translators.version nmrdpp entered competition following:1. Attempt get translation ADDs using mtbdd, proves infeasible,abort rely state-based translator instead.2. Run fltl expansion state space, taking search control knowledge accountavailable. Break 10mn complete.3. Run value iteration convergence. Failing achieve useful result (e.g.expansion complete enough even reach goal state), go back step 2.4. Run many 30 trials possible remaining time,16 following generated policy defined, falling back non-deterministic search controlpolicy available.Step 1 trying maximise instances original ADD-basednmrdpp version could run intact. Step 3, decided use LAO*run good heuristic, often incurs significant overhead compared valueiteration.problems featured competition classified goal-based rewardbased problems. goal-based problems, (positive) reward received goalstate reached. reward-based problems, action performance may also incur (usuallynegative) reward. Another orthogonal distinction made problems16. given problem, planners 15mn run whatever computation saw appropriate (including parsing, pre-processing, policy generation any), execute 30 trial runs generatedpolicy initial state goal state.60fiDecision-Theoretic Planning non-Markovian Rewardsdomains communicated advance participants domainswere. latter consisted variants blocks world logistics (or box world)problems, gave participating planners opportunity exploit knowledgedomain, much hand-coded deterministic planning track.decided enroll nmrdpp control-knowledge mode domain-independentmode. difference two modes first uses FLTL searchcontrol knowledge written known domains additional input. main concernwriting control knowledge achieve reasonable compromise sizeeffectiveness formulae. blocks world domain, two actionspickup-from putdown-to 25% chance dropping block onto table,control knowledge used encoded variant well-known GN1 near-optimal strategydeterministic blocks world planning (Slaney & Thiebaux, 2001): whenever possible,try putting clear block goal position, otherwise put arbitrary clear blocktable. blocks get dropped table whenever action fails,success probabilities rewards identical across actions, optimal policiesproblem essentially made optimal sequences actions deterministic blocksworld little need sophisticated strategy.17 colored blocksworld domain, several blocks share color goal referscolor blocks, control knowledge selected arbitrary goal state non-coloredblocks world consistent colored goal specification, used strategynon-colored blocks world. performance strategy depends entirelygoal-state selected therefore arbitrarily bad.Logistics problems IPC-2 distinguish airports locations withincity; trucks drive two locations city planes flytwo airports. contrast, box world features cities,airport, accessible truck. priori, map truckplane connections arbitrary. goal get packages city origincity destination. Moving truck 20% chance resulting reaching onethree cities closest departure city rather intended one. size boxworld search space turned quite challenging nmrdpp. Therefore, writingsearch control knowledge, gave optimality consideration favored maximalpruning. helped fact box world generator produces problemsfollowing structure. Cities divided clusters, composedleast one airport city. Furthermore cluster least one hamiltonian circuittrucks follow. control knowledge used forced planes one, trucksone cluster idle. cluster, truck allowed move couldattempt driving along chosen hamiltonian circuit, picking dropping parcelswent.planners participating competition shown Table 2. Planners E, G2,J1, J2 domain-specific: either tuned blocks box worlds, usedomain-specific search control knowledge, learn examples. participatingplanners domain-independent.17. sophisticated near-optimal strategies deterministic blocks world exist (see Slaney & Thiebaux,2001), much complex encode might caused time performance problems.61fiThiebaux, Gretton, Slaney, Price & KabanzaPart.CE*G1G2*J1*J2*J3PQRDescriptionsymbolic LAO*first-order heuristic search fluent calculusnmrdpp without control knowledgenmrdpp control knowledgeinterpreter hand written classy policieslearns classy policies random walksversion ff replanning upon failuremgpt: lrtdp automatically extracted heuristicsProbaProp: conformant probabilistic plannerstructured reachability analysis structured PIReference(Feng & Hansen, 2002)(Karabaev & Skvortsova, 2005)paperpaper(Fern et al., 2004)(Fern et al., 2004)(Hoffmann & Nebel, 2001)(Bonet & Geffner, 2005)(Onder et al., 2006)(Teichteil-Konigsbuch & Fabiani, 2005)Table 2: Competition Participants. Domain-specific planners starreddomprobG2*J1*J2*E*J3G1RPCQ5100100100100100bw-c-nr811100 100100 100100 100100 100100 100bw-nc-nr8100100100100100bx-nr5-10 10-10100 10010010010067100expl-bw11hanoise5-3zeno1-2-3-7tire-nr30-49505710090100100323303053?231003total60060056740063218017715310026Table 3: Results Goal-Based Problems. Domain-specific planners starred. Entriespercentage runs goal reached. blank indicatesplanner unable attempt problem. indicates plannerattempted problem never able achieve goal. ? indicatesresult unavailable (due bug evaluation software, coupleresults initially announced found invalid).domprobJ1*G2*E*J2*J3PCG1RQ5497495496497496bw-c-r811487 481486 480492 486486 482487 48254944954954954944944954954941808489490490490490488bw-nc-r11 15 1821480 470 462 458480 468 352 286480 468481466 397455459bx-r5-10 10-10 10-15419317129438 376376425184346279file30-4tire-r30-43658?11total518348462459422944752087495495494191Table 4: Results Reward-Based Problems. Domain-specific planners starred. Entriesaverage reward achieved 30 runs. blank indicatesplanner unable attempt problem. indicates plannerattempted problem achieve strictly positive reward. ? indicatesresult unavailable.62fiDecision-Theoretic Planning non-Markovian RewardsTables 3 4 show results competition, extracted competition overview paper (Younes, Littman, Weissmann, & Asmuth, 2005)competition web site http://www.cs.rutgers.edu/~mlittman/topics/ipc04-pt/.first tables concerns goal-based problems second reward-based problems. entries tables represent goal-achievement percentage average reward achieved various planner versions (left-column) various problems (toptwo rows). Planners top part tables domain-specific. Problemsknown domains lie left-hand side tables. colored blocks world problemsbw-c-nr (goal-based version) bw-c-r (reward version) 5, 8, 11 blocks.non-colored blocks world problems bw-nc-nr (goal-based version) 8 blocks, bwnc-r (reward-based version) 5, 8, 11, 15, 18, 21 blocks. box world problemsbx-nr (goal-based) bx-r (reward-based), 5 10 cities 10 15 boxes. Problems unknown domains lie right hand side tables. comprise:expl-bw, exploding version 11 block blocks world problem puttingblock may destroy object put on, zeno, probabilistic variant zeno traveldomain problem IPC-3 1 plane, 2 persons, 3 cities 7 fuel levels, hanoise,probabilistic variant tower hanoi problem 5 disks 3 rods, file, problemputting 30 files 5 randomly chosen folders, tire, variant tire world problem30 cities spare tires 4 them, tire may go flat driving.planner nmrdpp G1 G2 version, able attempt problems, achieving strictly positive reward 4 them. even ff (J3), competition overallwinner, able successfully attempt many problems. nmrdpp performed particularly well goal-based problems, achieving goal 100% runs except expl-bw,hanoise, tire-nr (note three problems, goal achievement probabilityoptimal policy exceed 65%). planner outperformed nmrdppscale. pointed before, ff behaves well probabilistic version blocks boxworld optimal policies close deterministic problemHoffmann (2002) analyses reasons ff heuristic works well traditional planning benchmarks blocks world logistics. hand, ff unablesolve unknown problems different structure require substantialprobabilistic reasoning, although problems easily solved number participating planners. expected, large discrepancy version nmrdppallowed use search control (G2) domain-independent version (G1).latter performs okay unknown goal-based domains, able solveknown ones. fact, except ff, none participating domain-independentplanners able solve problems.reward-based case, nmrdpp control knoweldge behaves well knownproblems. human-encoded policies (J1) performed better. Without control knowledge nmrdpp unable scale problems, participants ffmgpt are. Furthermore nmrdpp appears perform poorly two unknown problems.cases, might due fact fails generate optimal policy: suboptimal policies easily high negative score domains (see Younes et al., 2005).r-tire, know nmrdpp indeed generate suboptimal policy. Additionally,could nmrdpp unlucky sampling-based policy evaluation process:63fiThiebaux, Gretton, Slaney, Price & Kabanzatire-r particular, high variance costs various trajectoriesoptimal policy.Alltogether, competition results suggest control knowledge likely essential solving larger problems (Markovian not) nmrdpp, that,observed deterministic planners, approaches making use control knowledgequite powerful.8. Conclusion, Related, Future Workpaper, examined problem solving decision processes nonMarkovian rewards. described existing approaches exploit compact representation reward function automatically translate NMRDP equivalentprocess amenable MDP solution methods. computational model underlyingframework traced back work relationship linear temporal logicautomata areas automated verification model-checking (Vardi, 2003;Wolper, 1987). remaining framework, proposed new representationnon-Markovian reward functions translation MDPs aimed making bestpossible use state-based anytime heuristic search solution method. representation extends future linear temporal logic express rewards. translationeffect embedding model-checking solution method. results MDPminimal size achievable without stepping outside anytime framework, consequentlybetter policies deadline. described nmrdpp, software platformimplements approaches common interface, proved useful toolexperimental analysis. system analysis first kind.able identify number general trends behaviours methodsprovide advice best suited certain circumstances. obviousreasons, analysis focused artificial domains. Additional work examinewider range domains practical interest, see form results takecontext. Ultimately, would like analysis help nmrdpp automatically selectappropriate method. Unfortunately, difficulty translatingPLTL $FLTL, likely nmrdpp would still maintain PLTL$FLTL version reward formulae.detailed comparison approach solving NMRDPs existing methods (Bacchus et al., 1996, 1997) found Sections 3.10 5. Two important aspects futurework would help take comparison further. One settle question appropriateness translation structured solution methods. Symbolic implementationssolution methods consider, e.g. symbolic LAO* (Feng & Hansen, 2002), wellformula progression context symbolic state representations (Pistore & Traverso,2001) could investigated purpose. take advantage greaterexpressive power $FLTL consider richer class decision processes, instanceuncertainty rewards received when. Many extensions languagepossible: adding eventualities, unrestricted negation, first-class reward propositions,quantitative time, etc. course, dealing via progression without backtrackinganother matter.64fiDecision-Theoretic Planning non-Markovian Rewardsinvestigate precise relationship line work recent workplanning temporally extended goals non-deterministic domains. particularinterest weak temporally extended goals expressible Eagle language(Dal Lago et al., 2002), temporally extended goals expressible -CTL* (Baral &Zhao, 2004). Eagle enables expression attempted reachability maintenance goalsform try-reach p try-maintain p, add goals do-reach pdo-maintain p already expressible CTL. idea generated policymake every attempt satisfying proposition p. Furthermore, Eagle includes recovery goalsform g1 fail g2 , meaning goal g2 must achieved whenever goal g1 fails,cyclic goals form repeat g, meaning g achieved cyclicallyfails. semantics goals given terms variants Buchi tree automatapreferred transitions. Dal Lago et al. (2002) present planning algorithm basedsymbolic model-checking generates policies achieving goals. Baral Zhao(2004) describe -CTL*, alternative framework expressing subset Eagle goalsvariety others. -CTL* variant CTL* allows formulae involvingtwo types path quantifiers: quantifiers tied paths feasible generatedpolicy, usual, also quantifiers generally tied paths feasibledomain actions. Baral Zhao (2004) present planning algorithm.would interesting know whether Eagle -CTL* goals encoded nonMarkovian rewards framework. immediate consequence would nmrdppcould used plan them. generally, would like examine respectivemerits non-deterministic planning temporally extended goals decision-theoreticplanning non-Markovian rewards.pure probabilistic setting (no rewards), recent related research includes workplanning controller synthesis probabilistic temporally extended goals expressibleprobabilistic temporal logics CSL PCTL (Younes & Simmons, 2004; Baier et al.,2004). logics enable expressing statements probability policy satisfying given temporal goal exceeding given threshold. instance, Younes Simmons(2004) describe general probabilistic planning framework, involving concurrency, continuous time, temporally extended goals, rich enough model generalised semi-Markovprocesses. solution algorithms directly comparable presented here.Another exciting future work area investigation temporal logic formalismsspecifying heuristic functions NMRDPs generally search problemstemporally extended goals. Good heuristics important solution methodstargeting, surely value ought depend history. methodsdescribed could applicable description processing heuristics. Relatedproblem extending search control knowledge fully operatepresence temporally extended goals, rewards, stochastic actions. first issuebranching probabilistic logics CTL PCTL variants preferredFLTL describing search control knowledge, stochastic actionsinvolved, search control often needs refer possible futures evenprobabilities.18 Another major problem GOALP modality,key specification reusable search control knowledge interpreted respect18. would argue, hand, CTL necessary representing non-Markovian rewards.65fiThiebaux, Gretton, Slaney, Price & Kabanzafixed reachability goal19 (Bacchus & Kabanza, 2000), such, applicabledomains temporally extended goals, let alone rewards. Kabanza Thiebaux (2005)present first approach search control presence temporally extended goalsdeterministic domains, much remains done system like nmrdpp ablesupport meaningful extension GOALP.Finally, let us mention related work area databases uses similar approachpltlstr extend database auxiliary relations containing sufficient informationcheck temporal integrity constraints (Chomicki, 1995). issues somewhat differentraised NMRDPs: ever one sequence databases, matterssize auxiliary relations avoiding making redundant distinctions.AcknowledgementsMany thanks Fahiem Bacchus, Rajeev Gore, Marco Pistore, Ron van der Meyden, MosheVardi, Lenore Zuck useful discussions comments, well anonymousreviewers David Smith thorough reading paper excellentsuggestions. Sylvie Thiebaux, Charles Gretton, John Slaney, David Price thank National ICT Australia support. NICTA funded Australian GovernmentsBacking Australias Ability initiative, part Australian Research Council. Froduald Kabanza supported Canadian Natural Sciences Engineering ResearchCouncil (NSERC).Appendix A. Class Reward-Normal Formulaeexisting decision procedure (Slaney, 2005) determining whether formula rewardnormal guaranteed terminate finitely, involves construction comparisonautomata rather intricate practice. therefore useful give simple syntacticcharacterisation set constructors obtaining reward-normal formulae even thoughformulae constructible.say formula material iff contains $ temporal operatorsis, material formulae boolean combinations atoms.consider four operations behaviours representable formulae $FLTL. Firstly,behaviour may delayed specified number timesteps. Secondly, may madeconditional material trigger. Thirdly, may started repeatedly materialtermination condition met. Fourthly, two behaviours may combined formunion. operations easily realised syntactically corresponding operationsformulae. material formula:delay[f ] =fcond[m, f ] = floop[m, f ] = f Uunion[f1 , f2 ] = f1 f219. f atemporal formula, GOALP(f ) true iff f true goal states.66fiDecision-Theoretic Planning non-Markovian Rewardsshown (Slaney, 2005) set reward-normal formulae closed delay,cond (for material m), loop (for material m) union, also closure{$} operations represents class behaviours closed intersectionconcatenation well union.Many familiar reward-normal formulae obtainable $ applying four operations. example, (p $) loop[, cond[p, $]]. Sometimes paraphrase necessary.example, ((p q) $) required form antecedentconditional, equivalent (p (q $)) loop[, cond[p, delay[cond[q, $]]]].cases easy. example formula p U (p$) stipulates rewardfirst time p happens form suggested. capturebehaviour using operations requires formula like (p $) ( (p $) U p).Appendix B. Proofs TheoremsProperty 1 b ((i) B), (, i) |=B f iff (, + 1) |=B Prog(b, , f ).Proof:Induction structure f . several base cases, fairly trivial.f = > f = nothing prove, progress holdeverywhere nowhere respectively. f = p f holds progresses >holds i+1 f hold progresseshold i+1 . case f = p similar. last base case, f = $. followingequivalent:(, i) |=B f(i) BbProg(b, , f ) = >(, + 1) |=B Prog(b, , f )Induction case 1: f = g h. following equivalent:(, i) |=B f(, i) |=B g (, i) |=B h(, + 1) |=B Prog(b, , g) (, + 1) |=B Prog(b, , h) (by induction hypothesis)(, + 1) |=B Prog(b, , g) Prog(b, , h)(, + 1) |=B Prog(b, , f )Induction case 2: f = g h. Analogous case 1.Induction case 3: f = g. Trivial inspection definitions.Induction case 4: f = g U h. f logically equivalent h (g (g U h)cases 1, 2 3 holds stage behaviour B iff Prog(b, , f ) holds stage i+1.Theorem 1 Let f reward-normal, let hf0 , f1 , . . .i result progressingsuccessive states sequence . Then, provided fi , Rew(i , fi )iff (i) Bf .67fiThiebaux, Gretton, Slaney, Price & KabanzaProof: First, definition reward-normality, f reward-normal |=B f iffi, (i) Bf (i) B. Next, |=B f progressing f accordingB (that is, letting bi true iff (i) B) cannot lead contradictionProperty 1, progression truth-preserving.remains, then, show 6|=B f progressing f according Bmust lead eventually . proof induction structure fusual base case f literal (an atom, negated atom >, $) trivial.Case f = g h. Suppose 6|=B f . either 6|=B g 6|=B h, inductionhypothesis either g h progresses eventually , hence conjunction.Case f = g h. Suppose 6|=B f . 6|=B g 6|=B h, inductionhypothesis g h progresses eventually . Suppose without loss generalityg progress h does. point g progressedformula g 0 f progressed g 0 simplifies g 0 . Since g 0 also progresseseventually, f .Case f = g. Suppose 6|=B f . Let = 0 ; let B 0 = {|0 ; B}.6|=B 0 g, induction hypothesis g progressed according B 0 eventuallyreaches . progression f according B exactlyfirst step, leads .Case f = g U h. Suppose 6|=B f . j (, j) 6|=B gj, (, i) 6|=B h. proceed induction j. base case j = 0, 6|=B g6|=B h whence main induction hypothesis g h eventually progress. Thus h (g f 0 ) progresses eventually f 0 , particular f 0 = f ,establishing base case. induction case, suppose |=B g (and course 6|=B h).Since f equivalent h (g f ) 6|=B f , 6|=B h |=B g, clearly 6|=B f .B 0 previous case, therefore, 6|=B 0 f failure occurs stage j 1. Therefore hypothesis induction j applies, f progressedaccording B 0 goes eventually , f progressed according B goessimilarly .Theorem 3 Let 0 set e-states equivalent MDP D0 = hS, s0 , A, Pr, Ri.D0 minimal iff every e-state 0 reachable 0 contains two distinct e-states s01s02 (s01 ) = (s02 ) (s01 ) = (s02 ).Proof: Proof construction canonical equivalent MDP Dc . Let sete 0 ) partitioned equivalence classes,finite prefixes state sequences D(se 0 ), R(1(i); ) =1(i) 2(j) iff 1i = 2j 1(i); D(sR(2(j); ). Let [(i)] denote equivalence class (i). Let E setequivalence classes. Let function takes [(i)] E A(i ).(i) (j) A([(i)]), let ([(i)], a, [(j)]) Pr(i , a, s) [(j)] =[(i); hsi]. Otherwise let ([(i)], a, [(j)]) = 0. Let R([(i)]) R((i)). notefollowing four facts:1. functions A, R well-defined.2. Dc = hE, [hs0 i], A, , Ri equivalent MDP ([(i)]) = .68fiDecision-Theoretic Planning non-Markovian Rewards3. equivalent MDP D00 mapping subset statesD00 onto E.4. D0 satisfies condition every e-state 0 reachable 0 contains twodistinct e-states s01 s02 (s01 ) = (s02 ) (s01 ) = (s02 ) iff Dc isomorphicD0 .fact 1 amounts 1(i) 2(j) mattertwo sequences used define A, R equivalence class. casessimply 1i = 2j . case R, special case = h1iequality rewards extensions.Fact 2 matter checking four conditions Definition 1 hold. these,conditions 1 ( ([s0 ]) = s0 ) 2 (A([(i)]) = A(i )) hold trivially construction.e 0 ), R([(i)]) = R((i))Condition 4 says feasible state sequence D(si. also given construction. Condition 3 states:s1 , s2 S, A(s1 ) Pr(s1 , a, s2 ) > 0,e 0 ) = s1 , exists unique [(j)] E, j = s2 ,(i) D(sA([(i)]), ([(i)], a, [[j]]) = Pr(s1 , a, s2 ).e 0 ) = s1 . required (j) (i); hs2 i,Suppose Pr(s1 , , s2 ) > 0, (i) D(scourse A([(i)]) = A(i ), required condition reads:[(i); hs2 i] unique element X E (X) = s2A(i ), ([(i)], a, X) = Pr(s1 , a, s2 ).establish existence, need A(i ) ([(i)], a, [(i); hs2 i]) = Pr(i , a, s2 ),immediate definition above. establish uniqueness, suppose(X) = s2 ([(i)], a, X) = Pr(s1 , a, s2 ) actions A(i ). Since Pr(s1 , , s2 ) >0, transition probability [(i)] X nonzero action, definition, X [(i); hs2 i].Fact 3 readily observed. Let equivalent MDP D. states s1s2 D, state X (X) = s1 one state(Y ) = s2 action A(s1 ) gives nonzero probabilitytransition X . follows uniqueness part condition 3 Definition 1together fact transition function probability distribution (sums 1).Therefore given finite state sequence (i) one state reachedstart state following (i). Therefore induces equivalence relation: (i) (j) iff lead state (the sequencesfeasible may regarded equivalent ). reachable stateassociated nonempty equivalence class finite sequences states D. Workingdefinitions, may observe sub-relation (if (i) (j)(i) (j)). Hence function takes equivalence classfeasible sequence (i) [(i)] induces mapping h (an epimorphism fact)reachable subset states onto E.establish Fact 4, must shown case D0 mappingreversed, equivalence class [(i)] Dc corresponds exactly one element69fiThiebaux, Gretton, Slaney, Price & Kabanzae 0)D0 . Suppose (for contradiction). exist sequences 1(i) 2(j) D(s01(i) 2(j) following two sequences s0 arrive two differentelements s01 s02 D0 (s01 ) = 1i = 2j = (s02 ) (s01 ) 6= (s02 ). Thereforeeexists sequence (k) D(s)R(1(i 1); (k)) 6= R(2(j 1); (k)).contradicts condition 1(i) 2(j).Theorem 3 follows immediately facts 14.Theorem 4 Let D0 translation Definition 5. D0 blind minimalequivalent MDP D.Proof: Reachability e-states obvious, constructedreached. e-state pair hs, state reward functionspecification. fact, = (hs, i) determines distribution rewardscontinuations sequencesreach hs, i. is, 0 = s,Preward (f :r) {r | Bf }. D0 blind minimal, existdistinct e-states hs, hs, 0 sum . makes0 semantically equivalent, contradicting supposition distinct.Appendix C. Random Problem DomainsRandom problem domains produced first creating random action specificationdefining domain dynamics. experiments conducted20 also involvedproducing, second step, random reward specification desired propertiesrelation generated dynamics.random generation domain dynamics takes parameters number npropositions domain number actions produced, startsassigning effects action proposition affected exactly oneaction. example, 5 actions 14 propositions, first 4 actions may affect3 propositions each, 5th one 2, affected propositions different.action initial effects, continue add effects one time,sufficient proportion state space reachable see proportion reachable parameterbelow. additional effect generated picking random action randomproposition, producing random decision diagram according uncertaintystructure parameters below:Uncertainty parameter probability non zero/one value leaf node.uncertainty 1 result leaf nodes random values uniformdistribution. uncertainty 0 result leaf nodes values 0 1equal probability.Structure (or influence) parameter probability decision diagram containingparticular proposition. influence 1 result decision diagrams20. None included paper, however.70fiDecision-Theoretic Planning non-Markovian Rewardsincluding propositions (and unlikely significant structure), 0result decision diagrams depend values propositions.Proportion Reachable parameter lower bound proportion entire 2nstate space reachable start state. algorithm adds behaviourlower bound reached. value 1 result algorithm runningactions sufficient allow entire state space reachable.reward specification produced regard generated dynamicsspecified number rewards reachable specified number unreachable.First, decision diagram produced represent states reachablenot, given domain dynamics. Next, random path taken rootdecision diagram true terminal generating attainable reward, falseterminal producing unattainable reward. propositions encounteredpath, negated not, form conjunction reward formula. processrepeated desired number reachable unreachable rewards obtained.ReferencesAT&T Labs-Research (2000). Graphviz. Available http://www.research.att.com/sw/tools/graphviz/.Bacchus, F., Boutilier, C., & Grove, A. (1996). Rewarding behaviors. Proc. AmericanNational Conference Artificial Intelligence (AAAI), pp. 11601167.Bacchus, F., Boutilier, C., & Grove, A. (1997). Structured solution methods nonMarkovian decision processes. Proc. American National Conference ArtificialIntelligence (AAAI), pp. 112117.Bacchus, F., & Kabanza, F. (1998). Planning temporally extended goals. AnnalsMathematics Artificial Intelligence, 22, 527.Bacchus, F., & Kabanza, F. (2000). Using temporal logic express search control knowledgeplanning. Artificial Intelligence, 116 (1-2).Baier, C., Groer, M., Leucker, M., Bollig, B., & Ciesinski, F. (2004). Controller synthesisprobabilistic systems (extended abstract). Proc. IFIP International ConferenceTheoretical Computer Science (IFIP TCS).Baral, C., & Zhao, J. (2004). Goal specification presence nondeterministic actions.Proc. European Conference Artificial Intelligence (ECAI), pp. 273277.Barto, A., Bardtke, S., & Singh, S. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72, 81138.Bonet, B., & Geffner, H. (2003). Labeled RTDP: Improving convergence real-timedynamic programming. Proc. International Conference Automated PlanningScheduling (ICAPS), pp. 1221.71fiThiebaux, Gretton, Slaney, Price & KabanzaBonet, B., & Geffner, H. (2005). mGPT: probabilistic planner based heuristic search.Journal Artificial Intelligence Research, 24, 933944.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,Vol. 11, pp. 194.Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programmingfactored representations. Artificial Intelligence, 121 (1-2), 49107.Calvanese, D., De Giacomo, G., & Vardi, M. (2002). Reasoning actions planning LTL action theories. Proc. International Conference PrinciplesKnowledge Representation Reasoning (KR), pp. 493602.Cesta, A., Bahadori, S., G, C., Grisetti, G., Giuliani, M., Loochi, L., Leone, G., Nardi, D.,Oddi, A., Pecora, F., Rasconi, R., Saggase, A., & Scopelliti, M. (2003). RoboCareproject. Cognitive systems care elderly. Proc. International ConferenceAging, Disability Independence (ICADI).Chomicki, J. (1995). Efficient checking temporal integrity constraints using boundedhistory encoding. ACM Transactions Database Systems, 20 (2), 149186.Dal Lago, U., Pistore, M., & Traverso, P. (2002). Planning language extendedgoals. Proc. American National Conference Artificial Intelligence (AAAI), pp.447454.Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1995). Planning time constraints stochastic domains. Artificial Intelligence, 76, 3574.Dean, T., & Kanazawa, K. (1989). model reasoning persistance causation.Computational Intelligence, 5, 142150.Drummond, M. (1989). Situated control rules. Proc. International ConferencePrinciples Knowledge Representation Reasoning (KR), pp. 103113.Emerson, E. A. (1990). Temporal modal logic. Handbook Theoretical ComputerScience, Vol. B, pp. 9971072. Elsevier MIT Press.Feng, Z., & Hansen, E. (2002). Symbolic LAO search factored Markov decision processes. Proc. American National Conference Artificial Intelligence (AAAI), pp.455460.Feng, Z., Hansen, E., & Zilberstein, S. (2003). Symbolic generalization on-line planning.Proc. Conference Uncertainty Artificial Intelligence (UAI), pp. 209216.Fern, A., Yoon, S., & Givan, R. (2004). Learning domain-specific knowledge randomwalks. Proc. International Conference Automated Planning Scheduling(ICAPS), pp. 191198.Fourman, M. (2000). Propositional planning. Proc. AIPS Workshop Model-TheoreticApproaches Planning, pp. 1017.72fiDecision-Theoretic Planning non-Markovian RewardsGretton, C., Price, D., & Thiebaux, S. (2003a). Implementation comparison solutionmethods decision processes non-Markovian rewards. Proc. ConferenceUncertainty Artificial Intelligence (UAI), pp. 289296.Gretton, C., Price, D., & Thiebaux, S. (2003b). NMRDPP: system decision-theoreticplanning non-Markovian rewards. Proc. ICAPS Workshop PlanningUncertainty Incomplete Information, pp. 4856.Haddawy, P., & Hanks, S. (1992). Representations decision-theoretic planning: Utilityfunctions deadline goals. Proc. International Conference PrinciplesKnowledge Representation Reasoning (KR), pp. 7182.Hansen, E., & Zilberstein, S. (2001). LAO : heuristic search algorithm finds solutionsloops. Artificial Intelligence, 129, 3562.Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: stochastic planning usingdecision diagrams. Proc. Conference Uncertainty Artificial Intelligence (UAI),pp. 279288.Hoffmann, J. (2002). Local search topology planning benchmarks: theoretical analysis.Proc. International Conference AI Planning Scheduling (AIPS), pp. 92100.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 253302.Howard, R. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge,MA.Kabanza, F., & Thiebaux, S. (2005). Search control planning temporally extendedgoals. Proc. International Conference Automated Planning Scheduling(ICAPS), pp. 130139.Karabaev, E., & Skvortsova, O. (2005). Heuristic Search Algorithm Solving FirstOrder MDPs. Proc. Conference Uncertainty Artificial Intelligence (UAI),pp. 292299.Koehler, J., & Schuster, K. (2000). Elevator control planning problem. Proc.International Conference AI Planning Scheduling (AIPS), pp. 331338.Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189211.Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning.Artificial Intelligence, 76, 239286.Lichtenstein, O., Pnueli, A., & Zuck, L. (1985). glory past. Proc. ConferenceLogics Programs, pp. 196218. LNCS, volume 193.Onder, N., Whelan, G. C., & Li, L. (2006). Engineering conformant probabilistic planner.Journal Artificial Intelligence Research, 25, 115.73fiThiebaux, Gretton, Slaney, Price & KabanzaPistore, M., & Traverso, P. (2001). Planning model-checking extended goalsnon-deterministic domains. Proc. International Joint Conference Artificial Intelligence (IJCAI-01), pp. 479484.Slaney, J. (2005). Semi-positive LTL uninterpreted past operator. Logic JournalIGPL, 13, 211229.Slaney, J., & Thiebaux, S. (2001). Blocks world revisited. Artificial Intelligence, 125,119153.Somenzi, F. (2001).CUDD: CU Decision Diagram Package.ftp://vlsi.colorado.edu/pub/.AvailableTeichteil-Konigsbuch, F., & Fabiani, P. (2005). Symbolic heuristic policy iteration algorithms structured decision-theoretic exploration problems. Proc. ICAPS workshop Planning Uncertainty Autonomous Systems.Thiebaux, S., Hertzberg, J., Shoaff, W., & Schneider, M. (1995). stochastic modelactions plans anytime planning uncertainty. International JournalIntelligent Systems, 10 (2), 155183.Thiebaux, S., Kabanza, F., & Slaney, J. (2002a). Anytime state-based solution methodsdecision processes non-Markovian rewards. Proc. Conference UncertaintyArtificial Intelligence (UAI), pp. 501510.Thiebaux, S., Kabanza, F., & Slaney, J. (2002b). model-checking approach decisiontheoretic planning non-Markovian rewards. Proc. ECAI Workshop ModelChecking Artificial Intelligence (MoChArt-02), pp. 101108.Vardi, M. (2003). Automated verification = graph, logic, automata. Proc. International Joint Conference Artificial Intelligence (IJCAI), pp. 603606. Invitedpaper.Wolper, P. (1987). relation programs computations models temporallogic. Proc. Temporal Logic Specification, LNCS 398, pp. 75123.Younes, H. L. S., & Littman, M. (2004). PPDDL1.0: extension PDDL expressingplanning domains probabilistic effects. Tech. rep. CMU-CS-04-167, SchoolComputer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania.Younes, H. L. S., Littman, M., Weissmann, D., & Asmuth, J. (2005). first probabilistictrack International Planning Competition. Journal Artificial IntelligenceResearch, Vol. 24, pp. 851887.Younes, H., & Simmons, R. G. (2004). Policy generation continuous-time stochasticdomains concurrency. Proc. International Conference Automated PlanningScheduling (ICAPS), pp. 325333.74fi