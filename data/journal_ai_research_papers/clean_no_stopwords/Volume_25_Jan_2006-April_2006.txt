Journal Artificial Intelligence Research 25 (2006) 503527

Submitted 6/04; published 4/06

Fault Tolerant Boolean Satisfiability
Amitabha Roy

aroy@cs.bc.edu

Computer Science Department,
Boston College, Chestnut Hill, 02467.

Abstract
-model satisfying assignment Boolean formula small alteration,
single bit flip, repaired flips small number bits, yielding
new satisfying assignment. satisfying assignments represent robust solutions
optimization problems (e.g., scheduling) possible recover unforeseen
events (e.g., resource becoming unavailable). concept -models introduced
Ginsberg, Parkes, Roy (1998), proved finding -models general
Boolean formulas NP-complete. paper, extend result studying
complexity finding -models classes Boolean formulas known
polynomial time satisfiability solvers. particular, examine 2-SAT, Horn-SAT, AffineSAT, dual-Horn-SAT, 0-valid 1-valid SAT. see wide variation complexity
finding -models, e.g., 2-SAT Affine-SAT polynomial time tests -models,
testing whether Horn-SAT formula one NP-complete.

1. Introduction
important problem artificial intelligence community concerns allocation
resources near minimal cost. optimal solution problem might
rendered infeasible due unforeseen event (for example, resource becoming unavailable task exceeding allocated deadline). Hence, motivation search
optimal solutions immune events. paper, consider
complexity finding robust solutions, allow fixed small number
bad events, added condition bad events rectified making
small change solution. solutions, call -models, introduced
Ginsberg et al. (1998), explored Bailleux Marquis (1999). approach
fault tolerance extended constraint-satisfaction problems (CSPs) (Hebrard,
Hnich, & Walsh, 2004b, 2004a) applications combinatorial auctions (Holland &
OSullivan, 2004). Hoos ONeill (2000) consider approach robustness
framework dynamic satisfiability (which call DynSAT) goal able
revise optimal solutions constantly changing input problem.
extend initial complexity results Ginsberg et al. (1998) looking
theoretical complexity tractable instances satisfiability (SAT) identified Schaefers
dichotomy theorem (Schaefer, 1978). dichotomy theorem proves polynomial
time solvable instances SAT 2-SAT, Horn-SAT, dual-Horn-SAT, Affine-SAT, 0-valid
SAT 1-valid SAT form NP-complete. goal study complexity finding -models tractable problems identified dichotomy theorem.
show wide variation complexity type (2-SAT vs Horn-SAT) parameter
(the number repairs allowed break).
c
2006
AI Access Foundation. rights reserved.

fiRoy

Formally, -model Boolean formula, called supermodels Ginsberg et al. (1998),
satisfying assignment (satisfying assignments usually called models)
bit assignment flipped (from 0 1 vice versa), one following conditions
hold:
(i) either new assignment model
(ii) least one bit flipped obtain another model.
Flipping bit -model called break, corresponding bad event. bit
flipped get another satisfying assignment repair (we allow breaks may
need repair). also study generalization concept: (r, s)-models satisfying
assignments breaks every set r bits need repairs (to avoid
trivialities, require repair bits different break bits).
let -SAT refer decision question whether input Boolean formula
-model. restrict form input Boolean formula, refer
corresponding decision questions -2-SAT, -Horn-SAT etc. higher degree variants
problems (r, s)-SAT etc. consider r fixed integers.
following problems proved NP-complete:
- (r, s)-SAT (Ginsberg et al., 1998), (1, s)-2-SAT > 1,
- (1, s)-Horn-SAT, (1, s)-dual-Horn-SAT,
- (r, s)-0-valid-SAT (r, s)-1-valid-SAT.
contrast, prove following problems P:
- (1, 1)-2-SAT, (r, s)-Affine-SAT.
definition -models require new model obtained repairing
break -model -model. define -models -models
every break needs one repair obtain another -model. models represent
greatest degree fault tolerance achieved problem. refer
corresponding decision problems -SAT, -2-SAT etc. prove -SAT
NEXP (non-deterministic exponential time) NP-hard, -2-SAT P
-Affine-SAT P.
Remark. Since goal paper study problems Schaefers tractable
class respect fault tolerance, yardstick measure complexity membership
P. Hence, concern finding exact running times within P.
Optimizing runtimes may well prove important practical applications (at least
rare instances find polynomial time algorithms).
Organization paper: Section 2, introduce define problem establish
notation. Section 3, study complexity finding -models general Boolean
formulas. Section 4, consider complexity finding -models restricted classes
formulas: consider 2-SAT (Section 4.1), Horn-SAT (Section 4.2), 0-valid-SAT, 1-validSAT (Section 4.3) Affine-SAT (Section 4.4). conclude section future work
(Section 5).
504

fiFault Tolerant Boolean Satisfiability

2. Definitions Notations
section, establish notation used rest paper formally
define problems wish study.
Boolean variable take two values true false write 1 0
respectively. literal either variable v negation, denoted v (a variable often
called pure literal ). clause disjunction ( ) literals (for example, v1 v2 v3
clause). Boolean formula function set Boolean variables V =
{v1 , v2 , . . . , vn } {0, 1}. computational problems, assume Boolean formulas
input canonical fashion: usually conjunction ( ) clauses (in case, say
conjunctive normal form (CNF)).
consider various forms CNF formulas. 2-SAT formula Boolean formula
CNF 2 literals per clause (more generally, k-CNF formula k-SAT formula
CNF formula k literals per clause). Horn-SAT formula Boolean formula CNF
clause one positive literal (each clause called Horn clause).
Equivalently, Horn clause written implication ((v1 v2 . . . vr ) u)
u, v1 , v2 , . . . , vr pure literals r 0. dual-Horn-SAT formula CNF formula
clause one negative literal. Affine-SAT formula CNF formula
clause exclusive-or () literals negation exclusive-or
literals (such clause satisfied exactly odd number literals set
1). Equivalently, clause Affine-SAT formula written linear equation
finite field {0, 1} 2 elements.
assignment function X : V {0, 1} assigns truth value (true false)
variable V . Given assignment truth values V , Boolean formula
defined V also inherits truth value (we denote (X)), applying rules
Boolean logic. model assignment X (X) true. often treat
assignment X n-bit vector i-th bit, denoted X(i), 1 n,
truth value variable vi . slight abuse notation, let X(l) denote value
literal l assignment X.
0-valid-SAT (resp. 1-valid-SAT) formula one satisfied assignment
every variable set 0 (resp. 1).
propositional satisfiability problem defined follows:
Problem (SAT).
Instance: Boolean formula .
Question: model ?
SAT canonical example NP -complete decision problem (for definitions
complexity class NP completeness, see e.g., Garey & Johnson, 1979; Papadimitriou,
1994). Many computational difficult problems artificial intelligence SAT encodings
(for example, planning (Kautz & Selman, 1992)) finding heuristic algorithms
solving SAT important research area artificial intelligence. Polynomial time
algorithms known SAT input instance either Horn-SAT, dual-Horn-SAT,
2-SAT, Affine-SAT, 0-valid-SAT 1-valid-SAT. Schaefer (1978) proved
cases SAT solvable polynomial time, every case NP-complete
505

fiRoy

(Schaefers theorem applies general situation called generalized satisfiability
truth value clause determined set constraints specified
relation).
introduce concept fault-tolerant models. Given n-bit assignment X,
operation flips i-th bit X (from 0 1, vice versa). operation produces
new assignment denote (X). Similarly, flip two distinct bits (say bits
j), write new assignment ij (X) generally, (X) represents X
bits flipped (where subset coordinates {1, 2, . . . , n}).
Definition 2.1. -model Boolean formula model X i,
1 n, either
(i) assignment (X) model
(ii) bit j, 1 j n 6= j, ij (X) model.
words, -model model bit flipped (we call break ),
one bit flip required produce new model. second bit flip called
repair.
Example 2.1. Let H(n, k) Boolean formula defined n variables v1 , v2 , . . . , vn ,
whose models n-bit assignments exactly k bits set 1. example:





!


n
n
n
_
^
^

H(n, 1) =
vi
vi
vj





i=1
i=1
j=1

j6=i

first clause specifies least one bit model 1 successive clause
specifies i-th bit 1, every bit set 0 1 n.
model H(n, 1) -model: break 0-bit unique repair (the bit set 1)
break 1-bit (n 1) possible repairs (any one 0-bits).
following decision problem interpreted fault-tolerant analogue SAT:
Problem (-SAT).
Instance: Boolean formula .
Question: -model ?
problem -SAT variants (when restrict form input Boolean
formula) focus paper.
extend notion single repairability repairability sequence breaks
model.
Definition 2.2. (r, s)-model Boolean formula model every
choice r bit flips (the break set) model, disjoint set
bits (the repair set) flipped obtain another model .
506

fiFault Tolerant Boolean Satisfiability

Remark. (i) view r fixed constants unless otherwise mentioned. avoid
redundancies, required repair set disjoint break set. Since
require bits repair, also allow case repair
fewer repairs needed.
(ii) definition, (1, 1)-models -models continue refer
-models notational simplicity.
(iii) Similar definition -SAT, define decision problem (r, s)-SAT
asks whether input Boolean formula (r, s)-model.
Example 2.2. model H(n, k) (see, e.g., Example 2.1) also (k, k) model
k n/2.
Assumptions: discussions, assume every variable input Boolean
formula appears positive negative literals input Boolean formula
clausal form variable appearing clause (i.e.,
clause form v1 v1 v2 ). also assume instance -SAT (or
variants), clause consists single literal, since case input
formula cannot -model.
Consider -model X Boolean formula suppose model repairs
break X. definition (Definition 2.1) -models require
-model. enforce every break X repaired -model,
X tolerant single break, repair. thus define degree
fault tolerance. setting, models fault tolerant degree 0. Then, -models
fault-tolerant degree 1. generally, degree-k fault-tolerant models (which
call k -models) consist k1 fault-tolerant models every break repaired
k1 model. give formal definition below.
Definition 2.3. Let Boolean formula. define k (r, s)-models inductively: 0 (r, s)models models . k 1, k (r, s)-models k1 (r, s)-models X
every break r coordinates X, disjoint set
coordinates X flipped get k1 (r, s)-model .
define corresponding decision problem k (r, s)-SAT, asks whether input
Boolean formula k (r, s)-model. Observe also definition k (r, s)-model
(r, s)-model i, 0 k 1.
Example 2.3. Let n 6 even let Boolean formula:
(v1 = v2 ) (v3 = v4 ) (vn1 = vn ) (

4
_

H(n, k))

k=0

models vectors either 0, 2 4 variables set 1. variables
{v2i1 , v2i } truth value (and forces breaks unique
repairs).
507

fiRoy

claim X = (0, 0, . . . , 0) 2 (1, 1)-model . break (without loss
generality, assume coordinate 1) repaired flip coordinate 2 (and vice versa).
new vector (1, 1, 0, 0, . . . , 0) -model. break coordinate (say,
bit 3) unique repair (bit 4) give model (1, 1, 1, 1, 0, . . . , 0) 4 1s. model
longer repairable, since model 4 1s, break coordinate
0 (e.g., bit 5) repair.
Let n 2 even. Consider formula
(v1 = v2 ) (v3 = v4 ) (vn1 = vn )
2n/2 models. Observe model -model. models k (1, 1)models every integer k 0. call models (1, 1)-models (as usual, r = 1
= 1, denote (r, s)-models -models simplicity).
Definition 2.4. Let Boolean formula defined n Boolean variables.
model k (r, s) model k 0 called (r, s)-model.
Observe set -models form set models satisfies
following properties:
(i) vector -model, i.e., break bit needs 1 repair.
(ii) bit vector broken, repair (if repair
needed) new vector also member .
call sets -models stable sets . stable sets studied
combinatorial setting Luks Roy (2005).
Remark. existence families models satisfy conditions (i) (ii)
may used give alternate definition -models perhaps natural.
However, notion degrees repairability -models appear limit
degrees, apparent definition, hence use formulation leading
Definition 2.4.
corresponding decision problem, named -SAT, asks whether input Boolean
formula -model. Note yes answer question implies existence
one family models, particular, set above.
Complexity Classes: refer Papadimitriou (1994) definitions basic complexity
classes like P NP. language L said NEXP non-deterministic
Turing machine (NDTM) decides L exponential time (exponential length
input). language L said NP-hard polynomial time reduction
SAT L. language NP-complete NP NP-hard. complexity
class NL (non-deterministic log space), contained P, consists languages
accepted non-deterministic Turing machines using space logarithmic size
input. complexity classes Pk defined follows: P1 NP, Pk k 2
set languages accepted NDTM access oracle TM Pk1 .
508

fiFault Tolerant Boolean Satisfiability

3. Complexity Finding -models
section, study computational complexity finding -models general
Boolean formulas.
Theorem 3.1. (Ginsberg et al., 1998) decision problem (r, s)-SAT NP-complete.
Remark. proof technique used Ginsberg et al. (1998) prove Theorem 3.1 used
prove NP-hardness results paper, e.g., Theorem 3.2 Theorem 4.19.
Theorem 3.2. decision problem -SAT NEXP NP-hard.
Proof. Since NDTM guess stable set models (which could exponential size)
check satisfies required conditions stability exponential time, -SAT
NEXP.
reduce SAT -SAT using reduction used proof Theorem 3.1
Ginsberg et al. (1998): given instance SAT, Boolean formula n variables
v1 , v2 , . . . , vn , construct instance -SAT: formula 0 = vn+1 vn+1
new variable (to put 0 CNF form, add variable vn+1 clause
CNF formula ).
Suppose model X. show 0 -model showing
stable set models . Extend X model 0 setting vn+1 = 0. Let Xi = (X)
1 n. Extend assignment Xi model Yi 0 setting vn+1 = 1.
let
= {Y, Y1 , Y2 , . . . , Yn }.
show stable set. Suppose bit j 6= i, 1 j n Yi
broken, repair flipping i-th bit (in case, get repaired vector
Yj ). i-th bit Yi broken, repair (n + 1)-th bit (and vice versa),
case repaired vector . instead i-th bit broken, 1 n,
repair (n + 1)-th bit (we obtain Yi repaired vector case).
(n + 1)-th bit broken, repair flipping first n bits. Hence
stable set models 0 -model (in fact, exhibited n + 1
models).
show 0 -model, model. 0 -model,
must -model (n + 1)-th coordinate set 0. restriction
assignment v1 , v2 , . . . , vn model . completes reduction
SAT.
Remark. Note every -model k -model k 1, NP-hardness
-SAT (Theorem 3.2) imply NP-hardness k -SAT (Theorem 3.3 below).
reduction used Theorem 3.2 however adapted prove Theorem 3.3.
Theorem 3.3. k -SAT NP-complete, k 0.
Proof. k = 0, Cooks Theorem (Garey & Johnson, 1979), assume k 1.
First observe k -SAT NP. NDTM guess assignment X
check k (1, 1)-model: check whether X k (1, 1)-model, suffices
509

fiRoy

consider possible nk break sets, check repair exists break applied
sequence break set. Since k fixed, done polynomial time.
prove k -SAT NP-hard, use, again, proof technique used Ginsberg et al. (1998) prove Theorem 3.1. Given instance SAT, defined n variables
v1 , v2 , . . . , vn , construct 0 = vn+1 (and modify 0 CNF formula), vn+1
newly introduced variable. argument used Theorem 3.2 used prove
satisfiable iff 0 k -model. particular, construct stable set models
0 single model . Since -model k -model, proves
satisfiable, 0 -model. direction also follows: 0 k -model
model vn+1 set 0. restriction model v1 , . . . , vn
model .

4. Finding -models Restricted Boolean Formulas
section, consider complexity (r, s)-SAT restricted classes SAT
formulas known polynomial-time algorithms satisfiability: 2-SAT,
Horn-SAT, dual-Horn-SAT, 0-valid SAT, 1-valid SAT Affine-SAT. observe
problems different complexity testing fault tolerance. example, 2-SAT
Affine-SAT polynomial time tests existence -models (see Section 4.1
4.4) whereas problem NP-complete Horn-SAT (Section 4.2).
4.1 Finding -models 2-SAT
prove finding -models 2-SAT formulas polynomial time. give
two independent proofs: first proof (Section 4.1.1) exploits structure formula
second proof (suggested referee) uses CSP (constraint satisfaction problem)
techniques (Section 4.1.2). contrast, show finding (1, s)-models 2-SAT
formulas NP-complete 2 (Section 4.1.3). However, also show finding
-models 2-SAT formulas polynomial time (Section 4.1.4).

4.1.1 Polynomial time algorithm (1, 1)-2-SAT
Notation: Let instance 2-SAT. Following notation Papadimitriou (1994),
define directed graph G() = (V, E) follows: vertices graph
literals clause li lj (where li , lj literals), two directed edges
(li , lj ) ( lj , li ) E. path G() ordered sequence vertices (l1 , l2 , . . . , lr )
(li , li+1 ) E 1 r 1. define simple path G() path
(l1 , l2 , . . . , lr ) literals li involve distinct variables, i.e., li 6= lj li 6= lj
6= j, 1 i, j r. simple cycle G() simple path allow start
end vertices identical. source vertex (resp. sink vertex ) G() vertex
in-degree (resp. out-degree) 0. vertex l G() said k-ancestor (resp.
k-descendant) exists simple path (l, l1 , l2 , . . . , lk ) (resp. (l1 , l2 , . . . , lk , l)) length
k G().
following well-known lemma provides necessary sufficient condition
2-SAT formula satisfiable.
510

fiFault Tolerant Boolean Satisfiability

Lemma 4.1. (Papadimitriou, 1994) 2-SAT formula unsatisfiable iff variable x appearing path x x path x x
G().
-model, G() restrictions.
Lemma 4.2. 2-SAT formula -model, path l l
vertex l G().
Proof. path l l G(), satisfying assignment set
l false. flip value literal l (by flipping associated variable),
cannot repair get model .
Remark. Lemma 4.2 establishes necessary condition satisfiable 2-SAT formula
-model. Unlike Lemma 4.1, condition sufficient: consider, example,
2-SAT formula (which also illustrates many constraints satisfied
-model exists):
(v1 v2 ) (v2 v3 ) (v3 v4 ) (v4 v5 ).
-model formula set v1 false (otherwise every variable set
true break v5 requires one repair). Similarly v5 set 1, v2
0 v4 1. choice v3 allow single repair break v1 v5 .
formula thus -model, yet satisfies necessary condition Lemma 4.2.
establish necessary sufficient condition model 2-SAT formula
-model.
Lemma 4.3. Let satisfiable 2-SAT formula. Suppose path l
l vertex l G(). Let X model . X -model
satisfies following conditions:
(C1) Let P = (l1 , l2 , l3 ) simple path G() length 2. X(l1 ) = 0 X(l3 ) = 1.
(C2) (l1 , l2 ) (l1 , l3 ) edges G(), X(l1 ), X(l2 ), X(l3 ) cannot 0.
Proof. () Suppose X -model . Let P = (l1 , l2 , l3 ) simple path length 2.
X(l1 ) = 1, X(l2 ) = X(l3 ) = 1, otherwise X cannot model . break l3
requires values l1 l2 flipped X cannot -model, contradiction.
X(l1 ) = 0. Similar arguments show X(l3 ) = 1. Condition (C2) holds similarly:
X(l1 ), X(l2 ), X(l3 ) false, break l1 would require two repairs (both l2
l3 ). Hence one set true.
() Let X model satisfies conditions (C1) (C2). show
X actually -model. Suppose not; say break variable v repairable
one bit flip. Assume without loss generality, X(v) = 0
break, v set 1. must least one clause form v l l literal,
X(l) = 0, otherwise break need response. 1
clause, say clauses v l v l0 X(l) = X(l0 ) = 0, X violates condition (C2),
contradicting hypothesis. exactly one clause form v l X(l) = 0
511

fiRoy

moreover, must case flipping l produce model (then one
repair would sufficed). since flip variable associated l repairs
clause v l, must clauses break l repaired. clause must
form l l0 literal l0 X(l0 ) = 0. know l0 cannot v since
would path v v G(), violates hypothesis.
assumption clause distinct literals implies l0 6= l. Hence (v, l, l0 )
simple path X(v) = 0 X(l0 ) = 0, contradicting condition (C1). Hence X
-model.
Remark. (i) -model, indeed case (v, u) (w, u)
edges G(), u, v w cannot set true -model (since
break u repairable single flip). need include condition
explicitly Lemma 4.3, condition happens ( u, v)
( u, w) satisfy condition (C2) Lemma 4.3.
(ii) -model, condition (C1) extended specify values literals
(vertices) path length 3 (the maximum possible length, see Corollary 4.4
below) follows: (u1 , u2 , u3 , u4 ) simple path, apply condition (C1) twice
get X(u1 ) = X(u2 ) = 0 X(u3 ) = X(u4 ) = 1. Thus include
condition explicitly.
Lemma 4.3 consequences G():
Corollary 4.4. 2-SAT formula -model, G() satisfies following
properties:
(i) longest simple path G() length 3.
(ii) longest simple cycle G() length 2.
(iii) vertex v take part 1 simple cycle.
Proof. Suppose simple path (l1 , l2 , l3 , l4 , l5 ) length 4 G(). X model , Lemma 4.3 implies X(l3 ) = 1 apply (C2) segment (l1 , l2 , l3 )
X(l3 ) = 0 apply (C2) segment (l3 , l4 , l5 ). Hence -model cannot
exist. conditions follow similar arguments.
Pseudo-code algorithm given Algorithm (1). Observe Algorithm (1)
polynomial time reduction -2-SAT satisfiability question new 2-SAT
formula B . Proof correctness follows.
first need prove following easy lemma.
Lemma 4.5. 2-SAT formula -model, -model source
vertex (respectively, sink vertex) G() set false (resp. true).
Proof. Modify -model X setting sink vertex 1 (and hence source
vertex 0). Let new assignment X 0 . Clearly, X 0 still model (setting
antecedent p, consequent q, 0, 1 respectively, satisfies every implication p q).
512

fiFault Tolerant Boolean Satisfiability

Algorithm 1 Algorithm -2-SAT
1:
2:

Input: 2-SAT formula
Output: True (1, 1)-model, false otherwise

satisfiable
return false.
5: end
3:

4:

/* Check necessary condition holds (Lemma 4.2) */
Construct G()
8: path G() l l literal l
9:
return false.
10: end
6:
7:

11:

B

/* Enforce condition (C1) Lemma 4.3 */
13: 2-ancestor vertex l G()
14:
B B (l)
15: end
12:

16:
17:
18:
19:
20:
21:
22:

/* Force source (resp. sink) vertex value 0 (resp. 1) */
source vertices l G()
B B (l)
end
sink vertices l G()
B B (l)
end

/* Enforce condition (C2) Lemma 4.3 */
24: 1-ancestors l G()
25:
pairs distinct vertices l1 , l2
26:
(l, l1 ), (l, l2 ) edges G()
27:
B B (l1 l2 )
28:
end
29:
end
30: end
23:

31:
32:
33:
34:
35:

B satisfiable
return true
else
return false
end

513

fiRoy

show model satisfies condition (C1) (C2) Lemma 4.3, thus proving
-model. condition (C1) violated, simple path (l1 , l2 , l3 )
G() X 0 (l1 ) = 1 X 0 (l3 ) = 0. X 0 (l1 ) = 1, X(l1 ) = 1 (suppose let
X(l1 ) = 0: since (l1 , l2 ) edge G(), l1 sink vertex, value would
changed). Similarly, X 0 (l3 ) = 0 would imply X(l3 ) = 0. Thus X would
violate condition (C1) respect simple path (l1 , l2 , l3 ) could
-model (a contradiction). Condition (C2) similarly holds.
Algorithm (1) adds literals input 2-SAT formula enforce variable assignments
must hold -model (see Lines 1215, 2430 body Algorithm (1)).
Since guaranteed Lemma 4.3 conditions necessary sufficient
condition existence -model, satisfiability resulting Boolean formula
would imply -model. simplify proof correctness (which simply
Corollary 4.6 below), enforce source label vertices get default values prescribed
Lemma 4.5.
Corollary 4.6. formula B satisfiable iff -model.
Proof. Immediate Lemma 4.3 Lemma 4.5.
Example 4.1. Let 2-SAT formula:
(v1 v2 ) (v2 v3 )
(v1 v4 ) (v4 v3 )
(v1 v5 ) (v5 v3 )
Algorithm (1) constructs B
B =
(v1 ) (v3 )

(added lines 1316 Algorithm (1))

(v2 v4 ) (v2 v5 ) (v4 v5 )

(added lines 2431 )

(v2 v4 ) (v2 v5 ) (v4 v5 )

(added lines 24-31 )

Note construction G(), v3 2-ancestor. Since two variables
v2 , v4 , v5 set value, B unsatisfiable. Hence
-model.
Theorem 4.7. polynomial time, one determine 2-SAT formula -model
find one exists.
Proof. Satisfiability 2-SAT formula P (Papadimitriou, 1994). steps
procedure consist looping simple paths length 3, done time O(n3 )
n number variables.
Remark. possible characterize space complexity (1, 1)-2-SAT.
fact, (1, 1)-2-SAT complete NL (non-deterministic log space). see (1, 1)-2SAT NL, observe Algorithm (1) executed space logarithmic input.
Completeness established via log-space reduction 2-SAT. Since result
relevant present context, leave details out.
514

fiFault Tolerant Boolean Satisfiability

4.1.2 alternative proof Theorem 4.7
alternative proof Theorem 4.7 suggested one reviewers. possible
cast satisfiability problem constraint satisfaction problem (CSP) binary
variables. transformation, particularly input instance 2-SAT problem,
produces CSP local consistency (consistency subproblems involving fewer
variables) ensures presence global solution. framework, asserting
Boolean formula -model becomes particularly convenient.
Notation: Let Boolean formula CNF. subset variables, let (S)
denote subformula consisting clauses involve variables S.
Definition 4.1. formula said k-consistent every subset k1 variables,
every model (S) extended model (S {v}) every variable v (i.e.,
larger subformula involving one variable). formula strong k-consistent
i-consistent i, 1 k.
Remark. concept k-consistency equivalent formulations (Jeavons, Cohen,
& Cooper, 1998; Dechter, 1992). Since goal paper study satisfiability
exclusively, rephrase definitions theorems apply present context.
Theorem 4.8. (Dechter, 1992) Let 2-SAT formula. following hold:
(a) strong 3-consistent, satisfiable 2 element set S, (S)
satisfiable.
(b) polynomial time (see e.g., (Jeavons et al., 1998)) one check whether
strong 3-consistent. satisfiable strong 3-consistent, one add
extra clauses (also 2-CNF) polynomial time resulting 2-SAT
formula strong 3-consistent.
Remark. generally, given input Boolean formula , one establish k-consistency
adding extra constraints change set models. done iterating
possible k-element subsets variables solving subproblem variables. Clauses added restrict values subset k 1 variables
values extended another variable. set k 1 variables none
whose assignments extended, conclude unsatisfiable. not,
extra clauses added make k-consistent. Enforcing strong k-consistency
(for fixed k) accomplished polynomial time (Jeavons et al., 1998; Dechter, 1992).
special case 2-SAT formula extra clauses also binary
b exactly
end strong 3-consistent 2-SAT formula (which denote )
models (and hence, set -models).
Notation: ordered pair variables (u, v), let (u, v) denote set models
({u, v}).
Theorem 4.8 (b) implies assume without loss generality input
strong 3-consistent 2-SAT formula . Theorem 4.8 also implies assignment X
515

fiRoy

model iff (X(u), X(v)) (u, v) pairs (u, v). Clearly, construct
sets (u, v) polynomial time (there (n2 ) variable tuples, n
number variables, set (u, v) consists models 2-SAT formula
2 variables). slight abuse notation, denote (u, v) set
{(, )| (, ) (u, v)}.
Let u variable . Let u,0 = (u) u,1 = (u). either u,0
u,1 unsatisfiable, clear cannot -model. Assume

u,0 u,1 satisfiable let
u,0 u,1 corresponding strong 3-consistent
formulas. Let Nu set variable pairs (v, w) Md
(v, w) Md
(v, w) = .
u,0
u,1
Lemma 4.9. Suppose Nu 6= variable u. -model,
variable v, v 6= u, v belongs every pair Nu .
Proof. flip value u -model , repair flipping one
variable forced flip one variable pair Nu . means
repair variable every pair Nu .
Lemma 4.9 implies may assume pairs Nu common member.
similarly show:
Lemma 4.10. Suppose v variable appears every pair Nu .
following hold:
(i) exists w
Md
(v, w) Md
(v, w) =
u,0
u,1
-model X set X(u) = 0.
(ii) exists w
Md
(v, w) Md
(v, w) = ,
u,0
u,1
-model X set X(u) = 1.
Thus either two conditions Lemma 4.10 force value variable u
-model . Together Lemmas 4.9 4.10 enable us set values variables
forced (cf. Lemma 4.3). setting values variables, derive
contradiction cannot -model.
Algorithm (2) provides detailed description algorithm.
Theorem 4.11. Algorithm (2) decides (1, 1)-2-SAT polynomial time.
Proof. Enforcing 3-consistency polynomial time (Dechter, 1992). outer loop
Line 3 executes n times n number variables. Within body loop,
calls made enforce satisfiability 3-consistency, along calls construct Nu
variable u consideration. step takes polynomial time, hence claim
follows.
Remark. Algorithm (2) solves yes/no problem testing whether input 2SAT formula -model, simple matter modify algorithm outputs
-model model exists. forced variable assignments along satisfying
assignment remaining 2-SAT formula -model input formula.
516

fiFault Tolerant Boolean Satisfiability

Algorithm 2 Algorithm (1, 1)-2-SAT
Input: strong 3-consistent 2-SAT formula
2: Output: True (1, 1)-model, false otherwise
1:

3:
4:
5:
6:
7:
8:

every variable u
u,0 u,1 unsatisfiable
Output false.
end
Find sets Md
(v, w) Md
(v, w) variables v, w.
u,0
u,1
Compute N = set pairs (v, w)
Md
(v, w) Md
(v, w) = .
u,0
u,1

9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

pairs N common member, output false.
N 6=
common member v,
variable w Md
(v, w) Md
(v, w) =
u,0
u,1
set = (u)
end
variable w Md
(v, w) Md
(v, w) =
u,0
u,1
set = u
end
end
Check satisfiable, output false.
satisfiable, add extra clauses make 3-consistent.
end
Output true

517

fiRoy

4.1.3 Complexity (1, s)-2-SAT 2
Theorem 4.12. problem (1, s)-2-SAT NP-complete > 1.
Proof. Clearly problem NP: NDTM guess assignment check
model every break, bits flipped
get model (since fixed priori, leads O(ns ) possible repair sets,
polynomial number choices).
prove NP-completeness via reduction (s + 1)-SAT. Let
= C1 C2 . . . Cm
instance (s + 1)-SAT clause Ci disjunction + 1 literals:
vi,1 vi,2 . . . vi,s+1 .
construct instance 0 (1, s)-2-SAT follows: clause Ci ,
construct appropriate 2-SAT formula Ci0 . resulting instance (1, s)-2-SAT
conjunction 2-SAT formulas. Thus,
^
T0 =
Ci0
1im

Ci0 2-SAT formula defined clause Ci follows:
^
Ci0 =
(zi vi,j )
1j(s+1)

^

(vi,j i,j,1 )

(4.1)

1j(s+1)

^

^

(i,j,k i,j,k+1 )

1js+1 1k(s1)

introduced 1+s(s+1) new variables: zi i,j,k 1 j s+1, 1 k
define gadget Ci0 . gadget Ci0 best understood via Figure (1).
Let model X. Extend model 0 setting zi = 0 1
i,j,k = 1 1 m, 1 j + 1, 1 k s. claim
(1, s)-model 0 . Suppose flip variable corresponding literal l.
case analysis many repairs needed:
[l = zi ] Since vi,1 vi,2 . . . vi,s+1 set true model X, need flip
false literals {vi,1 , . . . , vi,s+1 }. Observe repairs necessary.
[l = i,j,k ] Need flip i,k,k0 1 k 0 < k might need flip variable
corresponding vi,j vi,j set true X. repair affect truth
value clauses 0 . Hence flip variables.
[l = variable occurring ] flip value literals involving l.
set every ai,j,k = 1 zi = 0, repairs needed 0 , implication
(clause) 0 still remains true.
518

fiFault Tolerant Boolean Satisfiability

zi
vi,1

vi,j

vi,s+1

i,1,1

i,j,1

i,s+1,1

i,1,k

i,j,k

i,s+1,k

i,1,s

i,j,s

i,s+1,s

Figure 1: Gadget 2-SAT
suppose 0 (1, s)-model. show model. Note
model zi = 0 (otherwise zi = 1, vi,j = i,j,k = 1 need
repairs flip value i,1,s ). literals {vi,1 , vi,2 , . . . , vi,s+1 } cannot
set 0, since break zi would necessitate + 1 repairs. Hence least one
literals {vi,1 , vi,2 , . . . , vi,s+1 } set 1. words, clause Ci satisfied.
Since zi = 0 i, must model.

4.1.4 Complexity -2-SAT
section, show -2-SAT polynomial time.
Let input 2-SAT formula n variables. construct graph G()
described Section 4.1.1. Since -model definition also -model, must
path restrictions set forth Lemma 4.3 Lemma 4.2. -model,
G() restrictions.
Lemma 4.13. Let 2-SAT formula -model. every non-trivial simple
path G() length 1.
Proof. Suppose (l1 , l2 , l3 ) simple path G() length 2. Let X -model
. Lemma 4.3, know X(l1 ) = 0, X(l3 ) = 1
case -models. means break X(l1 ) cannot repaired get another
-model. Hence, X cannot -model, contradiction.

519

fiRoy

Remark. Note G() may cycles (l1 , l2 , l1 ), however situation, Lemma 4.13
implies {l1 , l2 } must form one connected component. -model exists assigns
value l1 l2 respective variables form break-repair pair
independent remaining variables. thus remove cycles consideration.
without loss generality, assume G() cycles.
Let R vertices G() in-degree 0 B vertices out-degree
0. Since vertex cannot positive in-degree positive out-degree, creates
bipartition R B vertices G(), R, B disjoint vertex sets edges
G() form (l, l0 ) l R l0 B.
Note (l, l0 ) edge G(), out-degree l 0: otherwise,
would path length 2 cycle, excluded. Hence l R iff
l B. also observe isolated vertices G() since every clause
disjunction distinct literals. gives complete graph theoretic characterization
structure G() -model.
let Y0 assignment sets every literal R false (0) (hence sets) every
literal B true (since assumed every variable appears positive
negative literals).
Lemma 4.14. assignment Y0 -model.
Proof. exhibit stable set C models contains Y0 . Let B (respectively,
R ) denote restriction assignment onto literals B (respectively, R).
Let
C = {Y | B contains one literal set false }.
Note B contains one false literal, R contains one true
literal. Clearly Y0 C.
show C stable set. Let C, 6= Y0 . Suppose sets
literal l R true l false B. value literal l flipped, get
Y0 (a model C) repairs needed. different variable flipped,
creates new literal l0 R set true (and l0 false B) new assignment.
repair flipping value l true false, thereby allowing one positive
literal R. Thus break repairable another model C. break Y0 C
need repairs. Hence C stable set Y0 -model.
Theorem 4.15. -2-SAT P .
Proof. graph G() constructed polynomial time (in time linear size
). conditions needed existence -model checked polynomial time:
using depth-first search, one check longest simple path G() length 1
check whether subgraph G without 2-cycles bipartite.
4.2 Finding -models Horn-SAT dual Horn-SAT
Recall instance Horn-SAT Boolean formula CNF clause contains
1 positive literal. 2-SAT, polynomial time algorithm find model
520

fiFault Tolerant Boolean Satisfiability

Horn formula (see, e.g., Papadimitriou (1994)). However, unlike situation 2SAT, finding (1, s)-models Horn formulas NP complete 1. proof
fact easily modified show problem NP-complete dual
Horn-SAT.
first prove technical lemma used NP-completeness proof.
Define Boolean formula = (x, y, 1 , . . . , 2s ) variables x, y, 1 , . . . , 2s follows:

(x, y, 1 , . . . , 2s ) =

s1
^

(i i+1 )

i=1

(s x) (s y)

(4.2)

(x s+1 ) (y s+1 )
2s1
^

(i i+1 )

i=s+1

formula best visualized Figure (2). Observe variable x
appears head tail chain implications length s.
Figure 2: Gadget

x
1

2



s+1

s+2

2s


crucial property gadget use follows:
Lemma 4.16. Let X model . X (1, s)-model iff satisfies x y.
Proof. () Let X model . x holds X (i.e., x get opposite truth
values X), X set 0 (because either x set 0)
j j + 1 true (because either x set 1). break
requires repairs j < j exactly one x (the variable
set 0). Similarly break + 1 requires repairs j , + 1 j 1
exactly one x (the variable set 1). break x need repairs.
Since never need repairs every break, X (1, s)-model.
() (1, s)-model X set j 0 1 j j
1 + 1 j 2s (otherwise repairs needed breaks
variables). X(x) = X(y) = 0, break 1 (from 0 1) would require
repairs 2 , 3 , . . . , well x y, total + 1 repairs. Hence
x cannot false. Similarly, x cannot true break
2s would require + 1 repairs. Hence X satisfies x y.
Theorem 4.17. (1, s)-Horn-SAT NP-complete 1.
521

fiRoy

Proof.VWe prove via reduction 3-SAT. Let instance 3-SAT,
=
i=1 Ci defined n variables x1 , x2 , . . . , xn clause Ci disjunction 3
distinct literals. Clearly assume every variable appears positive
negative literals (if not, may set pure literal true false appropriately
consider resulting formula ).
first apply intermediate transformation . replace positive literal
(say xj ) Ci negative literal, x0j , x0j new variable occurring .
new clause, positive literal, denoted Ci0 . Remembering
global assumption every variable input Boolean formulas appear positive
negative literals, see transformation introduce variables x0j every
variable xj . maintain logical equivalence, also need enforce x0j xj
new formula: add following clauses: ( x0j xj ) (x0j xj ). Note
two clauses imply model new Boolean formula, xj x0j cannot
truth value.
Thus obtain

^
^ n
T0 =
Ci0
( x0i xi ) (x0i xi )
1im

1in

Note 0 almost Horn (since every clause Ci0 Horn), non-Horn clauses
clauses form (xi x0i ). introduced n new variables 2n new clauses,
0 + 2n clauses defined 2n variables. Clearly 0 satisfiable iff
satisfiable.
construct instance 00 (1, s)-Horn-SAT 0 00 (1, s)model iff 0 satisfiable. first introduce + 1 new variables A1 , A2 , . . . , As+1 .
clause Ci0 = vi,1 vi,2 vi,3 , construct formula i,1 consisting single clause
(note step, vi,j variable form xk form x0k
k, 1 k n):

i,1 = ( zi wi,1 wi,2 wi,3 )

(4.3)

zi , wi,1 , wi,2 , wi,3 new variables introduced clause Ci0 . step introduces 4 new variables per clause Ci0 total 4m new variables. next step creates
formulas places restrictions new variables ties variables vi,j original clause. introduce new variables i,j,k clause Ci0 ,
1 j 3, 1 k 1, variables forming intermediate variables chain
implications length vi,j wi,j below:
i,2 =(vi,1 i,1,1 ) (i,1,1 i,1,2 ) (i,1,s1 wi,1 )
(vi,2 i,2,1 ) (i,2,1 i,2,2 ) (i,2,s1 wi,2 )

(4.4)

(vi,3 i,3,1 ) (i,3,1 i,3,2 ) (i,3,s1 wi,3 )
reader may wish compare gadget i,2 similar gadget Ci0 Equation (4.1) shown Figure (1) used proof Theorem 4.12.
522

fiFault Tolerant Boolean Satisfiability

also make zi , one new variables introduced i,1 , appear head
chain implications length + 1 shown formula i,3 :
i,3 = (zi A1 ) (A1 A2 ) . . . (As As+1 )
define formula Ci00 constructed clause Ci0 , 1 m, 0 :
Ci00 = i,1 i,2 i,3
Note Ci00 Horn introduced new variables i,j,k , wi,j , zi total
3(s 1) + 3 + 1 = 3s + 1 new variables. new variables Ai global, i.e, reused
formulas Ci00 various i.
clauses form (x0i xi ) (x0i xi ) 0 , 1 n,
introduce new variables i,j 1 j 2s construct gadget =
(xi , x0i , i,1 , i,2 , . . . , i,2s ) defined Equation (4.2).
instance (1, s)-Horn-SAT then:
00 =

^

Ci00

1im

^



1in

first show 0 satisfiable, 00 -model. Suppose 0 model
Extend assignment X 00 variables 00 setting values
newly introduced variables follows:
X 0.

Ai = 1 1 + 1,
zi = 0 1 m,
wi,j = 1 j, 1 1 j 3,
i,j,k = 1 i, j 1 m, 1 j 3, 1 k 1,
i,j = 0 j, 1 j i, 1 n,
i,j = 1 j, + 1 j 2s i, 1 n.
Since X 00 satisfies clause 00 , model 00 . show X 00
actually (1, s)-model. Suppose variable v 00 flipped. case
case analysis possible repairs break.
[v = xi x0i ] repairs needed since implication remains satisfied 0 .
[v = Ai i, 1 + 1 ] repairs needed A1 , A2 , . . . , Ai1 (since zi = 0
i, need flipped) 1 ( s) repairs.
[v = i,j 1 n, 1 j ] repairs i,k j +1 k s. Since
X 0 model 0 , exactly one xi x0i set false need flip
variable. leads j + 1 repairs.
[v = i,j 1 n, + 1 j 2s ] repairs needed i,k + 1
k < j one xi x0i (since X 0 model 0 one xi , x0i set true
X 0 ) j repairs.
523

fiRoy

[v = wi,j 1 m, 1 j 3 ] repairs needed i,j,k 1 k s1
vi,j (if X 0 (vi,j ) = 1), repairs.
[v = i,j,k 1 m, 1 j 3, 1 k 1 ] repairs needed i,j,k0
1 k 0 k 1 vi,j (if X 0 (vi,j ) = 1) k 1 repairs.
[v = zi ] break alone whose repair crucially depends satisfiability 0 .
Note break changes zi 0 1 makes clause ( zi wi,1
wi,2 wi,3 ) false since wi,j true X 00 . repairs include one
wi,j s, consequently might trigger flips i,j,k vi,j .
choice wi,j involve repair process indicated vi,j set 0
X 0 . Since X 0 model, note also least one vi,j set 0. Without loss
generality, assume X 0 (vi,1 ) = 0 repair break zi flipping wi,1 , i,1,j
1 j 1 exactly repairs.
suppose 00 -model X 00 . show 0 satisfiable. Specifically, claim
restriction X 00 variables 0 model 0 . Lemma 4.16,
know i,j = 0 1 n, 1 j i,j = 1 1 n, + 1 j 2s
also x0i xi 0 satisfied i, 1 n. Note 00 , wi,j
end chain implications:
k,1 k,2 k,s vi,j i,j,1 i,j,s1 wi,j

(4.5)

vi,j either xk x0k k, 1 k n. Note variables chain
different gadgets k i,2 . implies X 00 (wi,j ) = 1 since
otherwise X 00 would set variables chain 0 would violate
Lemma 4.16. Since X 00 model 00 , must X 00 (zi ) = 1 i, otherwise
zi wi,1 wi,2 wi,3 false. zi flipped, guaranteed repair
flips make clause zi wi,1 wi,2 wi,3 true. involve
flipping least one wi,j , j = 1, 2, 3. vi,1 , vi,2 vi,3 set true X 00
(which would turn implied X 00 (i,j,k ) = 1 1 j 3, 1 k 1)
flip would require additional repairs, total s+1 repairs break zi .
must vi,j false j, 1 j 3. words, Ci0 = vi,1 vi,2 vi,3
satisfied X 00 . Hence restriction X 00 0 satisfies clauses 0 . Thus 0
satisfiable.
0 satisfiable iff 00 (1, s)-model. Since satisfiable iff 0 satisfiable
SAT instance, accomplishes reduction SAT. reduction clearly
polynomial time reduction. Since (1, s)-Horn-SAT clearly NP fixed r s,
proves NP-complete.
Recall dual-Horn formula Boolean formula CNF clause
one negative literal. surprisingly, dual-Horn-SAT formulas behave similarly
Horn-SAT comes finding -models.
Theorem 4.18. (1, s)-dual-Horn-SAT NP-complete.
proof theorem similar Theorem 4.17: replace Equation (4.3) i,1 = (zi wi,1 wi,2 wi,3 ) change direction implications i,3
Equation (4.4).
524

fiFault Tolerant Boolean Satisfiability

4.3 Finding -models 0-valid, 1-valid SAT formulas
Recall 0-valid (resp. 1-valid) Boolean formula one satisfied model
every variable set 0 (resp. 1). consider complexity finding faulttolerant models input 0-valid (or 1-valid) formula refer corresponding
decision questions (r, s)-0-valid-SAT, (r, s)-1-valid-SAT, -0-valid-SAT etc.
knowledge input Boolean formula satisfied particular assignment
provide information presence fault-tolerant models. Hence would
expect (correctly) finding models NP-hard. first prove:
Theorem 4.19. decision problem (r, s)-0-valid-SAT NP-complete.
Proof. proof, refer proof Theorem 3.1 which, slight modification,
works problem well. reduce SAT. Let SAT instance, construct
instance (r, s)-0-valid-SAT, 0 = new variable appearing
. Observe 0 0-valid (its value matters). proof 0
-model iff satisfiable identical proof Theorem 3.1: satisfiable
model X, extend model X 0 0 setting value 1.
break consisting r variables X 0 require repair r variables involve y.
involve y, flipping value 1 0 makes 0 true, hence one
repair suffices. Hence X 0 (r, s)-model. 0 (r, s)-model, must model
set 1. restriction model variables makes true, hence
satisfiable.
Similarly, easy verify proofs Theorem 3.1, Theorem 3.2 work
input formula 0-valid 1-valid formula. Hence following:
Theorem 4.20. decision problem (r, s)-1-valid-SAT NP-complete. problem
(1, 1)-0-valid-SAT (1, 1)-1-valid-SAT NEXP NP-hard.
4.4 Finding -models Affine-SAT
Another class Boolean formulas polynomial time satisfiability checkers AffineSAT: formulas conjunction clauses, clause exclusiveor (denoted ) distinct literals (a b = 1 iff exactly one Boolean variables a, b
set 1).
Example 4.2. example Affine-SAT formula
(x1 x2 x3 x4 = 1) (x3 x4 = 0)
formula -model X = (1, 0, 0, 0). fact, X easily seen -model (which
true -models Affine-SAT formulas, shall shortly see).
One find satisfying assignment formula affine form variant Gaussian
elimination. prove finding -models affine formulas also polynomial
time.
Lemma 4.21. Affine-SAT formula -model iff satisfiable every
variable v V appearing exists variable w = w(v) v w appear
exactly clauses.
525

fiRoy

Proof. Let X -model . variable v flipped, clauses v appears
become false, repair need flip variable appears exactly
clauses (and others). Thus variable pairing must exist. reverse direction
easily proved: variable pairing exists, variables form break-repair
pair.
Since conditions Lemma 4.21 easy check polynomial time,
following theorem:
Theorem 4.22. (1, 1)-Affine-SAT P.
can, fact, slightly strengthen theorem. first state analogue Lemma 4.21,
variable pairings easily generalized.
Definition: parity integer n n mod 2.
Lemma 4.23. Affine-SAT formula (r, s)-model iff satisfiable every
set R r variables, exists set S, R = variables,
clauses C , parity number variables R appearing C
parity number variables appearing C.
prove:
Theorem 4.24. (r, s)-Affine-SAT P .
Proof. Since r fixed constants, conditions Lemma 4.23 checked
polynomial time: choice set R R r, (there O(nr ) sets),
cycle possible set |S| s, R = (there O(ns ) sets),
check see conditions Lemma 4.23 satisfied (in particular, test whether
parity variables R appearing clause = parity variables appearing
clause, also accomplished polynomial time).
Hence (r, s)-Affine-SAT polynomial time.
Theorem 4.22 implies -model actually -model, since pairings
(u, w(v)) exist, model become -model (with {v, w(v)} forming break-repair
pairs).
Hence Affine-SAT formula -model iff -model, hence finding model Affine-SAT formulas also polynomial time.
thus following theorem:
Theorem 4.25. -Affine-SAT P.

5. Future Work
complexity (r, s)-SAT r part input opposed fixed
constants known. problem complexity class p3 , complete
class? status problem restricted Boolean formulas like 2-SAT, HornSAT etc., r specified input similarly open. present,
also know (r, s)-SAT decided polynomial space r, fixed constants.
Finally, practical modification concept -models would involve weakening
condition allow high percentage breaks repairable.
526

fiFault Tolerant Boolean Satisfiability

Acknowledgements
author grateful Eugene M. Luks encouragement advice. also thank
anonymous referees detailed comments suggestions.

References
Bailleux, O., & Marquis, P. (1999). Distance-sat: Complexity algorithms. Proceedings
Sixteenth National Conference Artificial Intelligence (AAAI99), pp. 642
647.
Dechter, R. (1992). local global consistency. Artificial Intelligence, 55 (1), 87108.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-completeness. W. H. Freeman Company, New York.
Ginsberg, M., Parkes, A., & Roy, A. (1998). Supermodels robustness. Proceedings
Fifteenth National Conference American Association Artificial Intelligence tenth conference Innovative Applications Artificial Intelligence,
1998, Madison, WI, pp. 334339.
Hebrard, E., Hnich, B., & Walsh, T. (2004a). Robust solutions constraint satisfaction
optimization. Proceedings sixteenth European Conference Artificial
Intelligence, ECAI, pp. 186190, Valencia, Spain.
Hebrard, E., Hnich, B., & Walsh, T. (2004b). Super solutions constraint programming.
Proceedings Internation Conference Integration AI Techniques
Constraint Programming Combinatorial Optimization Problems (CPAIOR), pp.
157172.
Holland, A., & OSullivan, B. (2004). Super solutions combinatorial auctions. Proceedings CSCLP 2004: Joint Annual Workshop ERCIM/CoLogNet Constraint
Solving Constraint Logic Programming.
Hoos, H., & ONeill, K. (2000). Stochastic local search methods dynamic sat initial
investigation. AAAI-2000 Workshop Leveraging Probability Uncertainty
Computation, pp. 2226.
Jeavons, P. G., Cohen, D. A., & Cooper, M. C. (1998). Constraints, consistency closure.
Artificial Intelligence, 101 (12), 251265.
Kautz, H. A., & Selman, B. (1992). Planning satisfiability. Proceedings Tenth
European Conference Artificial Intelligence (ECAI92), pp. 359363.
Luks, E. M., & Roy, A. (2005). Combinatorics singly-repairable families. Electronic
Journal Combinatorics, 12 (1), Research Paper 59, 17 pp. (electronic).
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Schaefer, T. J. (1978). complexity satisfiability problems. STOC 78: Proceedings
tenth annual ACM symposium Theory computing, pp. 216226. ACM
Press.

527

fiJournal Artificial Intelligence Research 25 (2006) 159-185

Submitted 5/05; published 2/06

Dynamic Local Search Maximum Clique Problem
Wayne Pullan

w.pullan@griffith.edu.au

School Information Communication Technology,
Griffith University,
Gold Coast, QLD, Australia

Holger H. Hoos

hoos@cs.ubc.ca

Department Computer Science
University British Columbia
2366 Main Mall, Vancouver, BC, V6T 1Z4 Canada

Abstract
paper, introduce DLS-MC, new stochastic local search algorithm maximum clique problem. DLS-MC alternates phases iterative improvement,
suitable vertices added current clique, plateau search,
vertices current clique swapped vertices contained current clique.
selection vertices solely based vertex penalties dynamically adjusted
search, perturbation mechanism used overcome search stagnation.
behaviour DLS-MC controlled single parameter, penalty delay, controls frequency vertex penalties reduced. show empirically DLSMC achieves substantial performance improvements state-of-the-art algorithms
maximum clique problem large range commonly used DIMACS benchmark
instances.

1. Introduction
maximum clique problem (MAX-CLIQUE) calls finding maximum sized subgraph pairwise adjacent vertices given graph. MAX-CLIQUE prominent combinatorial optimisation problem many applications, example, information retrieval,
experimental design, signal transmission computer vision (Balus & Yu, 1986).
recently, applications bioinformatics become important (Pevzner & Sze, 2000; Ji,
Xu, & Stormo, 2004). search variant MAX-CLIQUE stated follows: Given
undirected graph G = (V, E), V set vertices E set edges,
find maximum size clique G, clique G subset vertices, C V ,
pairs vertices C connected edge, i.e., v, v C, {v, v } E,
size clique C number vertices C. MAX-CLIQUE N P-hard
associated decision problem N P-complete (Garey & Johnson, 1979); furthermore, inapproximable sense deterministic polynomial-time algorithm
find cliques size |V |1 > 0, unless N P = ZPP (Hastad, 1999).1
best polynomial-time approximation algorithm MAX-CLIQUE achieves approximation ratio O(|V |/(log |V |)2 ) (Boppana & Halldorsson, 1992). Therefore, large hard
instances MAX-CLIQUE typically solved using heuristic approaches, particular,
1. ZPP class problems solved expected polynomial time probabilistic algorithm
zero error probability.
c
2006
AI Access Foundation. rights reserved.

fiPullan & Hoos

greedy construction algorithms stochastic local search (SLS) algorithms simulated annealing, genetic algorithms tabu search. (For overview
methods solving MAX-CLIQUE, see Bomze, Budinich, Pardalos, & Pelillo, 1999.)
may noted maximum clique problem equivalent independent set problem well minimum vertex cover problem, algorithm MAX-CLIQUE
directly applied equally fundamental application relevant problems
(Bomze et al., 1999).
recent literature MAX-CLIQUE algorithms, seems that, somewhat unsurprisingly, single best algorithm. Although algorithms empirically
evaluated benchmark instances Second DIMACS Challenge (Johnson & Trick,
1996), quite difficult compare experimental results studies, mostly
differences respective experimental protocols run-time environments. Nevertheless, particularly considering comparative results reported Grosso et al. (Grosso,
Locatelli, & Croce, 2004), seems five heuristic MAX-CLIQUE algorithms
achieve state-of-the-art performance.
Reactive Local Search (RLS) (Battiti & Protasi, 2001) derived Reactive
Tabu Search (Battiti & Tecchiolli, 1994), advanced general tabu search method
automatically adapts tabu tenure parameter (which controls amount diversification) search process; RLS also uses dynamic restart strategy provide
additional long-term diversification.
QUALEX-MS (Busygin, 2002) deterministic iterated greedy construction algorithm uses vertex weights derived nonlinear programming formulation MAXCLIQUE.
recent Deep Adaptive Greedy Search (DAGS) algorithm (Grosso et al., 2004)
also uses iterated greedy construction procedure vertex weights; weights
DAGS, however, initialised uniformly updated every iteration greedy
construction procedure. DAGS, weighted iterated greedy construction procedure
executed iterative improvement phase permits limited amount plateau
search. Empirical performance results indicate DAGS superior QUALEX-MS
MAX-CLIQUE instances DIMACS benchmark sets, hard
instances reach performance RLS (Grosso et al., 2004).
k-opt algorithm (Katayama, Hamamoto, & Narihisa, 2004) based conceptually simple variable depth search procedure uses elementary search steps
vertex added removed current clique; evidence
performs better RLS many instances DIMACS benchmark sets (Katayama
et al., 2004), performance relative DAGS unclear.
Finally, Edge-AC+LS (Solnon & Fenet, 2004), recent ant colony optimisation algorithm MAX-CLIQUE uses elitist subsidiary local search procedure, appears
reach (or exceed) performance DAGS RLS least DIMACS
instances.
work, introduce new SLS algorithm MAX-CLIQUE algorithm dubbed
Dynamic Local Search Max Clique, DLS-MC, based combination constructive search perturbative local search, makes use penalty values associated
vertices graph, dynamically determined search
help algorithm avoid search stagnation.
160

fiDynamic Local Search Max-Clique Problem

Based extensive computational experiments, show DLS-MC outperforms
state-of-the-art MAX-CLIQUE search algorithms, particular DAGS, broad
range widely studied benchmark instances, hence represents improvement
heuristic MAX-CLIQUE solving algorithms. also present detailed results behaviour DLS-MC offer insights roles single parameter dynamic
vertex penalties. note use vertex penalties DLS-MC inspired
dynamic weights DAGS and, generally, current state-of-the-art Dynamic Local
Search (DLS) algorithms well-known combinatorial problems, SAT
MAX-SAT (Hutter, Tompkins, & Hoos, 2002; Tompkins & Hoos, 2003; Thornton, Pham,
Bain, & Ferreira, 2004; Pullan & Zhao, 2004); general introduction DLS, see also
work (Hoos & Stutzle, 2004). results therefore provide evidence
effectiveness broad applicability algorithmic approach.
remainder article structured follows. first describe DLS-MC
algorithm key aspects efficient implementation. Next, present empirical performance results establish DLS-MC new state-of-the-art heuristic MAX-CLIQUE
solving. followed detailed investigation behaviour DLS-MC
factors determining performance. Finally, summarise main contributions
work, insights gained study outline directions future research.

2. DLS-MC Algorithm
Like DAGS algorithm Grosso et al., new DLS-MC algorithm based fundamental idea augmenting combination iterative improvement plateau search
vertex penalties modified search. iterative improvement procedure used algorithms based greedy construction mechanism starts
trivial clique consisting single vertex successively expands clique C adding
vertices adjacent vertices C. expansion impossible,
may still exist vertices connected one vertices C. including
vertex v C removing single vertex C connected v, new clique
number vertices obtained. type search called plateau
search. noted one plateau search steps, expansion
current clique may become possible; therefore, DLS-MC alternates phases
expansion plateau search.
purpose vertex penalties provide additional diversification search
process, otherwise could easily stagnate situations current clique
vertices common optimal solution given MAX-CLIQUE instance.
Perhaps obvious approach avoiding kind search stagnation simply
restart constructive search process different initial vertex. However, even
random (or systematic) variation choice initial vertex, still risk
heuristic guidance built greedy construction mechanism causes bias towards
limited set suboptimal cliques. Therefore, DAGS DLS-MC utilise numerical
weights associated vertices; weights modulate heuristic selection function
used greedy construction procedure way vertices repeatedly occur
cliques obtained constructive search process discouraged used
future constructions. Following intuition, consistent general approach
161

fiPullan & Hoos

dynamic local search (DLS), based idea, paper, refer
numerical weights vertex penalties.
Based general considerations, DLS-MC algorithm works follows (see also
algorithm outline Figure 1): picking initial vertex given graph G
uniformly random setting current clique C set consisting single
vertex, vertex penalties initialised zero. Then, search alternates
iterative improvement phase, suitable vertices repeatedly added
current clique C, plateau search phase, repeatedly one vertex C
swapped vertex currently contained C.
two subsidiary search procedures implementing iterative improvement
plateau search phases, expand plateauSearch, shown Figure 2. Note both,
expand plateauSearch select vertex added current clique C using
penalties associated candidate vertices. case expand, selection
made set NI (C) vertices connected vertices C
edge G; call set improving neighbour set C. plateauSearch,
hand, vertex added C selected level neighbour set C, NL (C),
comprises vertices connected vertices C except one vertex,
say v , subsequently removed C.
Note procedures always maintain current clique C; expand terminates
improving neighbour set C becomes empty, plateauSearch terminates
either NI (C) longer empty NL (C) becomes empty. Also, order reduce
incidence unproductive plateau search phases, DLS-MC implements plateau search
termination condition (Katayama et al., 2004) recording current clique (C )
start plateau search phase terminating plateauSearch overlap
recorded clique C current clique C.
end plateau search phase, vertex penalties updated incrementing
penalty values vertices current clique, C, one. Additionally, every pd
penalty value update cycles (where pd parameter called penalty delay), non-zero
vertex penalties decremented one. latter mechanism prevents penalty values
becoming large allows DLS-MC forget penalty values time.
updating penalties, current clique perturbed one two ways.
penalty delay greater one, i.e., penalties decreased occasionally, current
clique reduced last vertex v added it. removed vertices
increased penalty values, unlikely added back current clique
subsequent iterative improvement phase. equivalent restarting search
v. However, penalty delay one corresponds behaviour penalties
effectively used (since increase vertex penalty immediately undone),
keeping even single vertex current clique C carries high likelihood reconstructing
C subsequent iterative improvement phase. Therefore, achieve diversification
search, penalty delay one, C perturbed adding vertex v
chosen uniformly random given graph G removing vertices C
connected v.
stated above, penalty values used selection vertex given
neighbour set S. precisely, selectMinPenalty(S) selects vertex choosing
uniformly random set vertices minimal penalty values. vertex
162

fiDynamic Local Search Max-Clique Problem

procedure DLS-MC(G, tcs, pd, maxSteps)
input: graph G = (V, E); integers tcs (target clique size), pd (penalty delay), maxSteps
output: clique G size least tcs failed
begin
numSteps := 0;
C := {random(V )};
initPenalties;
numSteps < maxSteps
(C, v) := expand(G, C);
|C| = tcs return(C); end
C := C;
(C, v) := plateauSearch(G, C, C );
NI (C) 6=
(C, v) := expand(G, C);
|C| = tcs return(C); end
(C, v) := plateauSearch(G, C, C );
end
updatePenalties(pd );
pd > 1
C := {v};
else
v := random(V );
C := C {v};
remove vertices C connected v G;
end
end
return(failed);
end

Figure 1: Outline DLS-MC algorithm; details, see text.

selected S, becomes unavailable subsequent selections penalties
updated perturbation performed. prevents plateau search
phase repeatedly visiting clique. Also, safeguard prevent penalty
values becoming large, vertices penalty value greater 10 never
selected.
order implement DLS-MC efficiently, sets maintained using two array data
structures. first these, vertex list array, contains vertices currently
set; second one, vertex index array, indexed vertex number contains
index vertex vertex list array (or 1, vertex set).
additions set performed adding end vertex list array updating
vertex index array. Deletions set performed overwriting vertex list
entry vertex deleted last entry vertex list updating
vertex index array. Furthermore, vertices swapped current
clique plateau search phase, intersection current clique
recorded clique simply maintained recording size current clique
start plateau search decrementing one every time vertex swapped
163

fiPullan & Hoos

procedure expand(G, C)
input: graph G = (V, E); vertex set C V (clique)
output: vertex set C V (expanded clique); vertex v (most recently added vertex)
begin
NI (C) 6=
v := selectMinPenalty(NI (C));
C := C {v};
numSteps := numSteps + 1;
end while;
return((C, v));
end

procedure plateauSearch(G, C, C )
input: graph G = (V, E); vertex sets C V (clique), C C (recorded clique)
output: vertex set C V (modified clique); vertex v (most recently added vertex)
begin
NI (C) = NL (C) 6= C C 6=
v := selectMinPenalty(NL (C));
C := C {v};
remove vertex C connected v G;
numSteps := numSteps + 1;
end while;
return((C, v));
end

Figure 2: Subsidiary search procedures DLS-MC; details, see text.
current clique. Finally, array elements accessed using pointers rather
via direct indexing array. 2
Finally, may noted order keep time-complexity individual
search steps minimal, selection improving level neighbour sets
attempt maximise size set respective search step, rather chooses
vertex minimal penalty uniformly random; keeping common
intuition that, context SLS algorithms, often preferable perform many
relatively simple, efficiently computable search steps rather fewer complex search
steps.

3. Empirical Performance Results
order evaluate performance behaviour DLS-MC, performed extensive computational experiments MAX-CLIQUE instances Second DIMACS
Implementation Challenge (19921993)3 , also used extensively benchmarking purposes recent literature MAX-CLIQUE algorithms. 80 DIMACS
MAX-CLIQUE instances generated problems coding theory, fault diagnosis
problems, Kellers conjecture tilings using hypercubes Steiner triple problem,
2. Several techniques based implementation details Henry Kautzs highly efficient WalkSAT code, see http://www.cs.washington.edu/homes/kautz/walksat.
3. http://dimacs.rutgers.edu/Challenges/

164

fiDynamic Local Search Max-Clique Problem

addition randomly generated graphs graphs maximum clique
hidden incorporating low-degree vertices. problem instances range size
less 50 vertices 1 000 edges greater 3 300 vertices 5 000 000 edges.
experiments study performed dedicated 2.2 GHz Pentium IV machine 512KB L2 cache 512MB RAM, running Redhat Linux 3.2.2-5 using
g++ C++ compiler -O2 option. execute DIMACS Machine Benchmark4 ,
machine required 0.72 CPU seconds r300.5, 4.47 CPU seconds r400.5 17.44
CPU seconds r500.5. following, unless explicitly stated otherwise, CPU times
refer reference machine.
following sections, first present results series experiments
aimed providing detailed assessment performance DLS-MC. Then, report
additional experimental results facilitate direct comparison DLS-MC
state-of-the-art MAX-CLIQUE algorithms.
3.1 DLS-MC Performance
evaluate performance DLS-MC DIMACS benchmark instances, performed 100 independent runs instance, using target clique sizes (tcs) corresponding respective provably optimal clique sizes or, cases provably
optimal solutions unknown, largest known clique sizes. order assess peak
performance DLS-MC, conducted experiment multiple values
penalty delay parameter, pd, report best performance obtained. behaviour
DLS-MC suboptimal pd values method used identify optimal pd value
discussed Section 4.2. remaining parameter DLS-MC, maxSteps,
set 100 000 000, order maximise probability reaching target clique size
every run.
results experiments displayed Table 1. benchmark
instance show DLS-MC performance results (averaged 100 independent runs)
complete set 80 DIMACS benchmark instances. Note DLS-MC finds optimal
(or best known) solutions success rate 100% 100 runs per instance 77
80 instances; cases target clique size reached consistently
within alotted maximum number search steps (maxSteps) are:
C2000.9, 93 100 runs successful giving maximum clique size (average
clique size, minimum clique size) 78 (77.93, 77);
MANN a81, 96 100 runs obtained cliques size 1098, remaining
runs produced cliques size 1097;
MANN a45, runs achieved maximum clique size 344.
three cases, reported CPU time statistics successful runs
shown parentheses Table 1. Furthermore, expected time required DLS-MC
reach target clique size less 1 CPU second 67 80 instances,

4. dmclique, ftp://dimacs.rutgers.edu directory /pub/dsj/clique

165

fiPullan & Hoos

Instance
brock200 1
brock200 2
brock200 3
brock200 4
brock400 1
brock400 2
brock400 3
brock400 4
brock800 1
brock800 2
brock800 3
brock800 4
DSJC1000 5
DSJC500 5
hamming10-2
hamming10-4
hamming6-2
hamming6-4
hamming8-2
hamming8-4
johnson16-2-4
johnson32-2-4
johnson8-2-4
johnson8-4-4
MANN a27
MANN a45
MANN a81
MANN a9
san1000
san200 0.7 1
san200 0.7 2
san200 0.9 1
san200 0.9 2
san200 0.9 3
san400 0.5 1
san400 0.7 1
san400 0.7 2
san400 0.7 3
san400 0.9 1
sanr200 0.7

BR
21
12*
15
17*
27
29*
31
33*
23
24
25
26
15*
13*
512
40
32
4
128
16*
8
16
4
14
126*
345*
1099
16
15
30
18
70
60
44
13
40
30
22
100
18

pd
2
2
2
2
15
15
15
15
45
45
45
45
2
2
5
5
5
5
5
5
5
5
5
5
3
3
3
3
85
2
2
2
2
2
2
2
2
2
2
2

CPU(s)
0.0182
0.0242
0.0367
0.0468
2.2299
0.4774
0.1758
0.0673
56.4971
15.7335
21.9197
8.8807
0.799
0.0138
0.0008
0.0089
<
<
0.0003
<
<
<
<
<
0.0476
(51.9602)
(264.0094)
<
8.3636
0.0029
0.0684
0.0003
0.0002
0.0015
0.1641
0.1088
0.2111
0.4249
0.0029
0.002

Steps
14091
11875
21802
30508
955520
205440
74758
28936
10691276
3044775
4264921
1731725
91696
2913
1129
1903
43
3
244
31
7
15
3
21
41976
(16956750)
(27840958)
21
521086
1727
33661
415
347
1564
26235
29635
57358
113905
1820
1342

Sols.
2
1
1
1
1
1
1
1
1
1
1
1
25
42
2
100
2
83
100
92
100
100
66
29
100
(100)
(96)
99
1
1
2
1
1
1
1
1
1
1
1
13

Instance
sanr200 0.9
sanr400 0.5
sanr400 0.7
C1000.9
C125.9
C2000.5
C2000.9
C250.9
C4000.5
C500.9
c-fat200-1
c-fat200-2
c-fat200-5
c-fat500-1
c-fat500-10
c-fat500-2
c-fat500-5
gen200 p0.9 44
gen200 p0.9 55
gen400 p0.9 55
gen400 p0.9 65
gen400 p0.9 75
keller4
keller5
keller6
p hat1000-1
p hat1000-2
p hat1000-3
p hat1500-1
p hat1500-2
p hat1500-3
p hat300-1
p hat300-2
p hat300-3
p hat500-1
p hat500-2
p hat500-3
p hat700-1
p hat700-2
p hat700-3

BR
42
13
21
68
34*
16
78
44*
18
57
12
24
58
14
126
26
64
44*
55*
55
65
75
11*
27
59
10
46
68
12*
65
94
8*
25*
36*
9
36
50
11*
44*
62

pd
2
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

CPU(s)
0.0127
0.0393
0.023
4.44
0.0001
0.9697
(193.224)
0.0009
181.2339
0.1272
0.0002
0.001
0.0002
0.0004
0.0015
0.0004
0.002
0.001
0.0003
0.0268
0.001
0.0005
<
0.0201
170.4829
0.0034
0.0024
0.0062
2.7064
0.0061
0.0103
0.0007
0.0002
0.0007
0.001
0.0005
0.0023
0.0194
0.001
0.0015

Steps
15739
9918
8475
1417440
158
50052
(29992770)
845
5505536
72828
24
291
118
45
276
49
301
1077
369
18455
716
402
31
4067
11984412
230
415
1579
126872
730
1828
133
87
476
114
200
1075
1767
251
525

Sols.
18
4
61
70
94
93
(91)
85
93
3
14
1
3
19
3
18
3
4
4
1
1
1
98
100
100
82
87
23
1
90
98
13
42
10
48
14
36
2
72
85

Table 1: DLS-MC performance results, averaged 100 independent runs, complete set DIMACS benchmark instances. maximum known clique size
instance shown BR column (marked asterisk proven
optimal); pd optimised DLS-MC penalty delay instance; CPU(s)
run-time CPU seconds, averaged successful runs, instance. Average CPU times less 0.0001 seconds shown < ; Steps
number vertices added clique, averaged successful runs,
instance; Sols. total number distinct maximum sized cliques found
instance. runs achieved best known cliques size shown
exception of: C2000.9, 93 100 runs successful giving maximum
clique size (average clique size, minimum clique size) 78(77.93, 77); MANN a81,
96 100 runs obtained 1098 giving 1098(1097.96, 1097); MANN a45,
runs achieved maximum clique size 344.

166

fiDynamic Local Search Max-Clique Problem

expected run-time 10 CPU seconds required 8 13 remaining
instances, least 800 vertices. Finally, variation coefficients (stddev/mean) run-time distributions (measured search steps, order overcome
inaccuracies inherent extremely small CPU times) instances 100% success rate obtained found reach average maximum values 0.86 1.59,
respectively.
may interesting note time-complexity search steps DLS-MC
generally low. indicative example, brock800 1 800 vertices, 207 505
edges maximum clique size 23 vertices, DLS-MC performs, average, 189 235
search steps (i.e., additions current clique) per CPU second. Generally, timecomplexity DLS-MC steps increases size improving (NI ) level (NL )
neighbour sets well as, lesser degree, maximum clique size. relationship
seen Table 2 shows, (randomly generated) DIMACS C.9
brock 1 instances, performance DLS-MC terms search steps per CPU
second decreases number vertices (and hence size NI , NL ) increases.
Instance
C125.9
C250.9
C500.9
C1000.9
C2000.9
brock200 1
brock400 1
brock800 1

Vertices
125
250
500
1000
2000
200
400
800

Edges
6963
27984
112332
450079
1799532
14834
59723
207505

BR
34
44
57
68
78
21
27
23

DLS-MC pd
1
1
1
1
1
2
15
45

Steps / Second
1587399
939966
572553
319243
155223
774231
428504
189236

Table 2: Average number DLS-MC search steps per CPU second (on reference machine) 100 runs DIMACS C.9 brock 1 instances. BR
DLS-MC pd figures Table 1 also shown, factors direct
impact performance DLS-MC. is, BR increases, greater
overhead maintaining sets within DLS-MC; furthermore, larger pd values
cause higher overhead maintaing penalties, vertices tend
penalised. C.9 instances randomly generated edge probability
0.9, brock 1 instances constructed hide maximum
clique considerably lower densities (i.e., average number edges per
vertex). scaling average number search steps per CPU second performed DLS-MC C.9 instances only, running reference machine,
approximated 9 107 n0.8266 , n number vertices
given graph (this approximation achieves R2 value 0.9941).
detailed analysis DLS-MCs performance terms implementation-independent
measures run-time, search steps iteration counts, beyond scope
work, could yield useful insights future.
3.2 Comparative Results
results reported previous section demonstrate clearly DLS-MC achieves
excellent performance standard DIMACS benchmark instances. However, com167

fiPullan & Hoos

parative analysis results, compared results found literature
state-of-the-art MAX-CLIQUE algorithms, straight-forward task
differences in:
Computing Hardware: date, computing hardware basically documented terms CPU speed allows basic means comparison
(i.e., scaling based computer CPU speed which, example, takes account features, memory caching, memory size, hardware architecture,
etc.). Unfortunately, algorithms, realistic option available
us comparison.
Result Reporting Methodology: empirical results performance
MAX-CLIQUE algorithms found literature form statistics
clique size obtained fixed run-time. conduct performance comparisons
data, care must taken avoid inconclusive situations algorithm
achieves larger clique sizes another algorithm B, cost higher runtimes. important realise relative performance B vary
substantially run-time; may reach higher clique sizes B relatively
short run-times, opposite could case longer run-times. Finally, seemingly
small differences clique size may fact represent major differences performance,
since (as many hard optimisation problems) finding slightly sub-optimal cliques
typically substantially easier finding maximal cliques. example, C2000.9,
average time needed find clique size 77 (with 100% success rate) 6.419
CPU seconds, whereas reaching maximum clique size 78 (with 93% success
rate) requires average (over successful runs only) 193.224 CPU seconds.
Termination Criteria: MAX-CLIQUE algorithms (such DAGS)
terminate upon reaching given target clique size, instead run given
number search steps fixed amount CPU time, even optimal clique
encountered early search. would obviously highly unfair directly compare published results algorithms DLS-MC, terminates
soon finds user supplied target clique size.
Therefore, confirm DLS-MC represents significant improvement previous
state-of-the-art MAX-CLIQUE algorithms, conducted experiments analyses
designed yield performance results DLS-MC directly compared
results MAX-CLIQUE algorithms. particular, compared DLS-MC
following MAX-CLIQUE algorithms: DAGS (Grosso et al., 2004), GRASP (Resende,
Feo, & Smith, 1998) (using results contained Grosso et al., 2004), k-opt (Katayama
et al., 2004), RLS (Battiti & Protasi, 2001), GENE (Marchiori, 2002), ITER (Marchiori,
2002) QUALEX-MS (Busygin, 2002). rank performance MAX-CLIQUE
algorithms determine dominant algorithm benchmark instances,
used set criteria based, primarily, quality solution then,
deemed equivalent, CPU time requirements algorithms.
criteria shown, order application, Table 3.

168

fiDynamic Local Search Max-Clique Problem

1. algorithm algorithm find largest known maximum clique instance
ranked dominant algorithm instance.
2. one algorithm achieves 100% success rate instance algorithm lowest
average (scaled) CPU time becomes dominant algorithm instance.
3. single algorithm achieves 100% success rate instance algorithm becomes dominant
algorithm instance.
4. algorithm achieves 100% success rate instance, algorithm achieves largest
size clique, highest average clique size lowest average CPU time becomes
dominant algorithm instance.
5. If, instance, algorithm meets four criteria listed above, conclusion
drawn dominant algorithm instance.

Table 3: criteria used ranking MAX-CLIQUE algorithms.

Instance
brock200 1
brock200 2
brock200 3
brock200 4
brock400 1
brock400 2
brock400 3
brock400 4
brock800 1
brock800 2
brock800 3
brock800 4
C1000.9
C2000.9
C4000.5
C500.9
gen200 p0.9 44
gen400 p0.9 55
gen400 p0.9 65
gen400 p0.9 75
keller6
MANN a45
p hat1000-3
p hat1500-1
san200 0.7 2
san400 0.7 3
sanr200 0.9

DLS-MC
Clique size
CPU(s)
21
0.0182
12
0.0242
15
0.0367
17
0.0468
27
2.2299
29
0.4774
31
0.1758
33
0.0673
23
56.4971
24
15.7335
25
21.9197
26
8.8807
68
4.44
78(77.93,77)
193.224
18 181.2339
57
0.1272
44
0.001
55
0.0268
65
0.001
75
0.0005
59 170.4829
344
51.9602
68
0.0062
12
2.7064
18
0.0684
22
0.4249
42
0.0127

DAGS
Clique size
SCPU(s)
21
0.256
12
0.064
15
0.064
17(16.8,16)
0.192
27(25.35,24)
1.792
29(28.1,24)
1.792
31(30.7,25)
1.792
33
1.792
23(20.95,20),
10.624
24(20.8,20)
10.752
25(22.2,21)
10.88
26(22.6,20)
10.816
68(65.95,65)
94.848
76(75.4,74)
1167.36
18(17.5,17)
2066.56
56(55.85,55)
8.64
44(41.15,40)
0.576
53(51.8,51)
4.608
65(55.4,51)
4.672
75(55.2,52)
4.992
57(56.4,56)
7888.64
344(343.95) 1229.632
68(67.85,67)
71.872
12(11.75,11)
19.904
18(17.9,17)
0.192
22(21.7,19)
1.28
42(41.85,41)
0.576

GRASP
Clique size
SCPU(s)
21
4.992
12
1.408
14
42.56
17
3.328
25
14.976
25
15.232
31(26.2,25)
14.848
25
15.232
21
32
21
32.96
22(21.85,21)
34.112
21
33.152
67(66.1,65)
154.368
75(74.3,73)
466.368
18(17.75,17)
466.944
56
80.896
44(41.95,41)
11.776
53(52.25,52)
35.264
65(64.3,63)
34.56
74(72.3,69)
36.16
55(53.5,53) 1073.792
336(334.5,334)
301.888
68
237.568
11
23.424
18(16.55,15)
3.264
21(18.8,17)
9.856
42
12.608

Table 4: Performance comparison DLS-MC, DAGS GRASP selected DIMACS
instances. SCPU columns contain scaled DAGS GRASP average
run-times CPU seconds; DAGS GRASP results based 20 runs per
instance, DLS-MC results based 100 runs per instance. cases
best known result found runs, clique size entries format
maximum clique size (average clique size, minimum clique size). DLS-MC
dominant algorithm instances table.

169

fiPullan & Hoos

Table 4 contrasts performance results DAGS GRASP literature (Grosso
et al., 2004) respective performance results DLS-MC. Since DAGS
GRASP runs performed 1.4 GHz Pentium IV CPU, DLS-MC ran
2.2 GHz Pentium IV reference machine, scaled CPU times factor 0.64.
(Note based assumption linear scaling run-time CPU clock
speed; reality, speedup typically significantly smaller.) Using ranking criteria,
data shows DLS-MC dominates DAGS GRASP benchmark
instances listed Table 4. confirm ranking, modified DAGS terminated
soon given target clique size reached (this termination condition used
DLS-MC) performed direct comparison DLS-MC 80 DIMACS instances,
running algorithms reference machine. seen results
experiment, shown Table 5, DLS-MC dominates DAGS one instance (the
exception san1000).
Table 6 shows performance results DLS-MC compared results k-opt (Katayama
et al., 2004), GENE (Marchiori, 2002), ITER (Marchiori, 2002) RLS (Battiti & Protasi,
2001) literature. roughly compensate differences CPU speed, scaled
CPU times k-opt, GENE ITER factor 0.91 (these obtained
2.0 GHz Pentium IV) RLS (measured 450 MHz Pentium II CPU)
0.21. Using ranking criteria Table 3, RLS dominant algorithm instances
keller6 MANN a45, k-opt dominant algorithm MANN a81 DLS-MC
dominant algorithm, exception C2000.9, remainder DIMACS
instances listed Table 6. identify dominant algorithm C2000.9, experiment performed, running DLS-MC maxSteps parameter (which controls
maximum allowable run-time) reduced point average clique size
DLS-MC exceeded reported RLS. experiment, DLS-MC reached
optimum clique size 78 58 100 independent runs average minimum
clique size 77.58 77, respectively average run-time 85 CPU sec (taking
account runs). establishes DLS-MC dominant RLS k-opt instance
C2000.9.
Analagous experiments performed directly compare performance DLSMC k-opt selected DIMACS benchmark instances; results, shown Table 7,
confirm DLS-MC dominates k-opt instances.
Finally, Table 8 shows performance results DLS-MC comparison results
QUALEX-MS literature (Busygin, 2002); CPU times QUALEX-MS
scaled factor 0.64 compensate differences CPU speed (1.4 GHz
Pentium IV CPU vs 2.2 GHz Pentium IV reference machine). Using ranking
criteria Table 3, QUALEX-MS dominates DLS-MC instances brock400 1, brock800 1,
brock800 2 brock800 3, DLS-MC dominates QUALEX-MS remaining 76
80 DIMACS instances.

170

fiDynamic Local Search Max-Clique Problem

Instance
brock200 1
brock200 2
brock200 3
brock200 4
brock400 1
brock400 2
brock400 3
brock400 4
brock800 1
brock800 2
brock800 3
brock800 4
DSJC1000 5
DSJC500 5
C1000.9
C125.9
C2000.9
C2000.5
C250.9
C4000.5
C500.9
c-fat200-1
c-fat200-2
c-fat200-5
c-fat500-1
c-fat500-10
c-fat500-2
c-fat500-5
gen200 p0.9 44
gen200 p0.9 55
gen400 p0.9 55
gen400 p0.9 65
gen400 p0.9 75
hamming10-2
hamming10-4
hamming6-2
hamming6-4
hamming8-2
hamming8-4
johnson16-2-4

DLS-MC
Success CPU(s)
100
0.0182
100
0.0242
100
0.0367
100
0.0468
100
2.2299
100
0.4774
100
0.1758
100
0.0673
100
56.4971
100
15.7335
100
21.9197
100
8.8807
100
0.799
100
0.0138
100
4.44
100
0.0001
93
193.224
100
0.9697
100
0.0009
100 181.2339
100
0.1272
100
0.0002
100
0.001
100
0.0002
100
0.0004
100
0.0015
100
0.0004
100
0.002
100
0.001
100
0.0003
100
0.0268
100
0.001
100
0.0005
100
0.0008
100
0.0089
100
<
100
<
100
0.0003
100
<
100
<

DAGS
Success CPU(s)
93
0.1987
98
0.1252
100
0.1615
82
0.2534
35
3.1418
75
2.3596
92
2.2429
99
1.653
9
20.0102
20
18.747
19
19.1276
45
16.9227
80
7.238
100
0.1139
5
2.87
100
0.0024
5 2.870608
100
17.9247
99
0.1725


4
16.2064
100
0.0002
100
0.0004
100
0.0012
100
0.0005
100
0.0067
100
0.0009
100
0.0028
14
0.9978
100
0.0267
0
9.0372
27
7.1492
14
8.6018
100
0.1123
100
3.8812
100
0.0003
100
<
100
0.0039
100
0.0006
100
0.0003

Instance
johnson32-2-4
johnson8-2-4
johnson8-4-4
keller4
keller5
keller6
MANN a27
MANN a45
MANN a81
MANN a9
p hat1000-1
p hat1000-2
p hat1000-3
p hat1500-1
p hat1500-2
p hat1500-3
p hat300-1
p hat300-2
p hat300-3
p hat500-1
p hat500-2
p hat500-3
p hat700-1
p hat700-2
p hat700-3
san1000
san200 0.7 1
san200 0.7 2
san200 0.9 1
san200 0.9 2
san200 0.9 3
san400 0.5 1
san400 0.7 1
san400 0.7 2
san400 0.7 3
san400 0.9 1
sanr200 0.7
sanr200 0.9
sanr400 0.5
sanr400 0.7

DLS-MC
Success CPU(s)
100
<
100
<
100
<
100
<
100
0.0201
100 170.4829
100
0.0476
100
51.9602
96 264.0094
100
<
100
0.0034
100
0.0024
100
0.0062
100
2.7064
100
0.0061
100
0.0103
100
0.0007
100
0.0002
100
0.0007
100
0.001
100
0.0005
100
0.0023
100
0.0194
100
0.001
100
0.0015
100
8.3636
100
0.0029
100
0.0684
100
0.0003
100
0.0002
100
0.0015
100
0.1641
100
0.1088
100
0.2111
100
0.4249
100
0.0029
100
0.002
100
0.0127
100
0.0393
100
0.023

DAGS
Success CPU(s)
100
0.0042
100
<
100
0.0001
100
0.0009
100
0.079


100
0.1886
94
8.194


100
0.0003
100
0.0353
100
0.0984
81
37.2
69
15.609
100
0.4025
100
6.3255
100
0.0078
100
0.0033
100
0.0609
100
0.0099
100
0.0215
100
0.4236
100
0.1217
100
0.0415
100
0.1086
100
0.967
100
0.0029
92
0.1001
100
0.0023
100
0.0368
100
0.0572
100
0.0336
100
0.0089
100
0.0402
90
0.5333
100
0.0322
100
0.0239
83
0.3745
93
0.231
100
0.1345

Table 5: Success rates average CPU times DLS-MC DAGS (based 100 runs
per instance). 80 DIMACS instances, DLS-MC superior success rate
31 instances and, exception san1000, required less CPU
time DAGS instances. Entries signify runs
terminated excessive CPU time requirements. obtain meaningful
comparison DLS-MC DAGS, MANN a45 MANN a81, 344
1098 respectively used best known results producing table.
DLS-MC DAGS, average CPU time successful runs only.
Using ranking criteria study, DAGS dominant algorithm
san1000 instance, DLS-MC dominant algorithm instances.

171

fiPullan & Hoos

DLS-MC
Instance
Clique size
brock200 2
12
brock200 4
17
brock400 2
29
brock400 4
33
brock800 2
24
brock800 4
26
C1000.9
68
C125.9
34
C2000.5
16
C2000.9
78(77.9,77)
C250.9
44
C4000.5
18
C500.9
57
DSJC1000 5
15
DSJC500 5
13
gen200 p0.9 44
44
gen200 p0.9 55
55
gen400 p0.9 55
55
gen400 p0.9 65
65
gen400 p0.9 75
75
hamming10-4
40
hamming8-4
16
keller4
11
keller5
27
keller6
59
MANN a27
126
MANN a45
344
MANN a81 1098(1097.96,1097)
p hat1500-1
12
p hat1500-2
65
p hat1500-3
94
p hat300-1
8
p hat300-2
25
p hat300-3
36
p hat700-1
11
p hat700-2
44
p hat700-3
62

k-opt

RLS

CPU(s)
Clique size
SCPU(s)
Clique size
SCPU(s)
0.0242
11
0.02184
12
2.01705
0.0468
16
0.01911
17
4.09311
0.4774
25(24.6,24)
0.28028 29(26.063,25)
8.83911
0.0673
25
0.18291 33(32.423,25) 22.81398
15.7335
21(20.8,20)
2.16034
21
0.99519
8.8807
21(20.5,20)
2.50796
21
1.40616
4.44
67
6.3063
68
8.7486
0.0001
34
0.00091
34
0.00084
0.9697
16 13.01846
16
2.09496
193.224
77(75.1,74) 66.14608 78(77.575,77) 172.90518
0.0009
44
0.05642
44
0.00609
181.2339
17 65.27885
18 458.44869
0.1272
57(56.1,56)
0.82264
57
0.65604
0.799
15
5.77941
15
1.35513
0.0138
13
0.12103
13
0.04074
0.001
44
0.06643
44
0.00777
0.0003
55
0.00273
55
0.00336
0.0268
53(52.3,51)
0.56238
55
0.25284
0.001
65
0.24934
65
0.0105
0.0005
75
0.16926
75
0.01071
0.0089
40
0.58422
40
0.01638
<
16
0.00182
16
0.00063
<
11
0.00091
11
0.00042
0.0201
27
0.07371
27
0.03591
170.4829
57(55.5,55) 125.03218
59 39.86094
0.0476
126
0.03276
126
0.65436
51.9602
344(343.6,343)
5.34716 345(343.6,343)
83.7417
264.0094 1099(1098.1,1098)
84.903
1098 594.4722
2.7064
12 15.43997
12
6.35754
0.0061
65
0.42224
65
0.03318
0.0103
94
2.093
94
0.04032
0.0007
8
0.00637
8
0.00378
0.0002
25
0.00546
25
0.00126
0.0007
36
0.0273
36
0.00441
0.0194
11
0.57876
11
0.03906
0.001
44
0.04914
44
0.00588
0.0015
62
0.08008
62
0.00735

GENE
ITER
Avg.
Avg.
Clique size Clique size
10.5
10.5
15.4
15.5
22.5
23.2
23.6
23.1
19.3
19.1
18.9
19
61.6
61.6
33.8
34
14.2
14.2
68.2
68.7
42.8
43
15.4
15.6
52.2
52.7
13.3
13.5
12.2
12.1
39.7
39.5
50.8
48.8
49.7
49.1
53.7
51.2
60.2
62.7
37.7
38.8
16
16
11
11
26
26.3
51.8
52.7
125.6
126
342.4
343.1
1096.3
1097
10.8
10.4
63.8
63.9
92.4
93
8
8
25
25
34.6
35.1
9.8
9.9
43.5
43.6
60.4
61.8

Table 6: Performance DLS-MC, k-opt, RLS, GENE ITER selected DIMACS
instances. SCPU columns contain scaled average run-time CPU seconds
k-opt RLS; DLS-MC RLS results based 100 runs per instance,
k-opt, GENE ITER results based 10 runs per instance. Using
ranking criteria study, RLS dominant algorithm instances
MANN a45 keller6, DLS-MC dominant algorithm
instances.

172

fiDynamic Local Search Max-Clique Problem

DLS-MC
Instance
Clique size CPU(s)
brock400 2 25(24.69,24) 0.1527
brock400 4
25 0.0616
brock800 2 21(20.86,20) 1.7235
brock800 4 21(20.65,20) 1.0058

k-opt
DLS-MC
k-opt
Clique size SCPU(s) Instance Clique size CPU(s) Clique size SCPU(s)
25(24.6,24)
0.280 C1000.9 67(66.07,64) 0.0373
67(66,65)
6.306
25
0.183 C2000.9 77(75.33,74) 0.6317 77(75.1,74)
66.146
21(20.8,20)
2.160 C4000.5
17 1.3005
17
65.279
21(20.5,20)
2.508
keller6 57(55.76,54) 2.6796 57(55.5,55) 125.032

Table 7: Performance DLS-MC k-opt DLS-MC parameter maxSteps
reduced point clique size results comparable
k-opt. CPU(s) values DLS-MC include unsuccessful runs; DLS-MC
results based 100 runs k-opt results 10 runs (per instance).
DLS-MC
QUALEX-MS
DLS-MC
QUALEX-MS
Instance
Clique size CPU(s) Clique size SCPU(s)
Instance
Clique size
CPU(s) Clique size SCPU(s)
brock200 1
21
0.0182
21
0.64 johnson32-2-4
16
<
16
5.12
brock200 2
12
0.0242
12
< 0.64 johnson8-2-4
4
<
4
< 0.64
brock200 3
15
0.0367
15
0.64 johnson8-4-4
14
<
14
< 0.64
brock200 4
17
0.0468
17
< 0.64
keller4
11
<
11
0.64
brock400 1
27
2.2299
27
1.28
keller5
27
0.0201
26
10.24
brock400 2
29
0.4774
29
1.92
keller6
59 170.4829
53
826.24
brock400 3
31
0.1758
31
1.28
MANN a27
126
0.0476
125
0.64
brock400 4
33
0.0673
33
1.28
MANN a45
344 51.9602
342
10.88
brock800 1
23 56.4971
23
11.52
MANN a81 1098(1097.96,1097) 264.0094
1096
305.28
brock800 2
24 15.7335
24
11.52
MANN a9
16
<
16
< 0.64
brock800 3
25 21.9197
25
11.52
p hat1000-1
10
0.0034
10
17.92
brock800 4
26
8.8807
26
11.52
p hat1000-2
46
0.0024
45
21.76
C1000.9
68
4.44
64
17.28
p hat1000-3
68
0.0062
65
20.48
C125.9
34
0.0001
34
< 0.64
p hat1500-1
12
2.7064
12
60.8
C2000.5
16
0.9697
16
177.92
p hat1500-2
65
0.0061
64
71.04
C2000.9 78(77.93,77) 193.224
72
137.6
p hat1500-3
94
0.0103
91
69.12
C250.9
44
0.0009
44
0.64
p hat300-1
8
0.0007
8
0.64
C4000.5
18 181.2339
17
1500.8
p hat300-2
25
0.0002
25
0.64
C500.9
57
0.1272
55
2.56
p hat300-3
36
0.0007
35
0.64
c-fat200-1
12
0.0002
12
< 0.64
p hat500-1
9
0.001
9
1.92
c-fat200-2
24
0.001
24
< 0.64
p hat500-2
36
0.0005
36
2.56
c-fat200-5
58
0.0002
58
< 0.64
p hat500-3
50
0.0023
48
2.56
c-fat500-1
14
0.0004
14
0.64
p hat700-1
11
0.0194
11
6.4
c-fat500-10
126
0.0015
126
1.28
p hat700-2
44
0.001
44
7.68
c-fat500-2
26
0.0004
26
1.28
p hat700-3
62
0.0015
62
7.04
c-fat500-5
64
0.002
64
1.28
san1000
15
8.3636
15
16.0
DSJC1000 5
15
0.799
14
23.04 san200 0.7 1
30
0.0029
30
0.64
DSJC500 5
13
0.0138
13
3.2 san200 0.7 2
18
0.0684
18
< 0.64
gen200 p0.9 44
44
0.001
42
< 0.64 san200 0.9 1
70
0.0003
70
< 0.64
gen200 p0.9 55
55
0.0003
55
0.64 san200 0.9 2
60
0.0002
60
0.64
gen400 p0.9 55
55
0.0268
51
1.28 san200 0.9 3
44
0.0015
40
< 0.64
gen400 p0.9 65
65
0.001
65
1.28 san400 0.5 1
13
0.1641
13
1.28
gen400 p0.9 75
75
0.0005
75
1.28 san400 0.7 1
40
0.1088
40
1.92
hamming10-2
512
0.0008
512
24.32 san400 0.7 2
30
0.2111
30
1.28
hamming10-4
40
0.0089
36
28.8 san400 0.7 3
22
0.4249
18
1.28
hamming6-2
32
<
32
< 0.64 san400 0.9 1
100
0.0029
100
1.28
hamming6-4
4
<
4
< 0.64
sanr200 0.7
18
0.002
18
0.64
hamming8-2
128
0.0003
128
< 0.64
sanr200 0.9
42
0.0127
41
< 0.64
hamming8-4
16
<
16
0.64
sanr400 0.5
13
0.0393
13
1.28
johnson16-2-4
8
<
8
< 0.64
sanr400 0.7
21
0.023
20
1.28

Table 8: Performance DLS-MC QUALEX-MS. SCPU column contains
scaled run-time QUALEX-MS CPU seconds; DLS-MC results based
100 runs per instance. Using ranking criteria study, QUALEX-MS
dominant algorithm instances brock400 1, brock800 1, brock800 2
brock800 3, DLS-MC dominant algorithm instances.
173

fiPullan & Hoos

Overall, results comparative performance evaluations summarised
follows:
QUALEX-MS dominant brock400 1, brock800 1, brock800 2 brock800 3
DIMACS instances.
RLS dominant algorithm MANN a45 keller6 DIMACS instances.
DAGS dominant algorithm san1000 DIMACS instance.
k-opt dominant algorithm MANN a81 DIMACS instance.
DLS-MC dominant algorithm remaining 72 DIMACS instances.
addition, within alotted run-time number runs, DLS-MC obtained current best known results DIMACS instances exceptions MANN a45
MANN a81.

4. Discussion
gain deeper understanding run-time behaviour DLS-MC efficacy
underlying mechanisms, performed additional empirical analyses. Specifically,
studied variability run-time multiple independent runs DLS-MC
problem instance; role vertex penalties general and, particular,
impact penalty delay parameter performance behaviour DLS-MC;
frequency pertubation well role perturbation mechanism.
investigations performed using two DIMACS instances, C1000.9 brock800 1.
instances selected because, firstly, reasonable size difficulty. Secondly, C1000.9 randomly generated instance vertices optimal maximum
clique predominantly higher vertex degree average vertex degree (intuitively
would seem reasonable that, randomly generated problem, vertices optimal
maximum clique would tend higher vertex degrees). brock800 1,
hand, vertices optimal maximum clique predominantly lower-than-average
vertex degree. (Note DIMACS brock instances created attempt defeat
greedy algorithms used vertex degree selecting vertices Brockington & Culberson,
1996).
fundamental difference highlighted results quantitative analysis maximum cliques instances, showed that, C1000.9, averaged
maximal cliques found DLS-MC, average vertex degree vertices maximal cliques 906 (standard deviation 9) compared 900 (9) averaged
vertices; brock800 1, corresponding figures 515 (11) 519 (13) respectively.
4.1 Variability Run-Time
variability run-time multiple independent runs given problem important aspect behaviour SLS algorithms DLS-MC. Following methology Hoos Stutzle (2004), studied aspect based run-time distributions
(RTDs) DLS-MC two reference instances.
174

fiDynamic Local Search Max-Clique Problem

seen empirical RTD graphs shown Figure 3 (each based
100 independent runs reached respective best known clique size), DLS-MC
shows large variability run-time. Closer investigation shows RTDs quite
well approximated exponential distributions (a Kolmogorov-Smirnov goodness-of-fit test
failed reject null hypothesis sampled run-times stem exponential
distributions shown figure standard confidence level = 0.05 p-values
0.16 0.62). observation consistent similar results highperformance SLS algorithms, e.g., SAT (Hoos & Stutzle, 2000) scheduling problems
(Watson, Whitley, & Howe, 2005). consequence, performing multiple independent
runs DLS-MC parallel result close-to-optimal parallelisation speedup (Hoos
& Stutzle, 2004). Similar observation made difficult DIMACS
instances.
1

1

empirical RLD DLS-MC
ed[2.5*105]

0.9

0.8

0.8

0.7

0.7

0.6

0.6

P(solve)

P(solve)

0.9

0.5
0.4

0.5
0.4

0.3

0.3

0.2

0.2

0.1

0.1

0
1000

10000

100000

1e+006

empirical RTD DLS-MC
ed[0.85]

0
0.001

1e+007

0.01

run-time [search steps]
1

1

empirical RLD DLS-MC
ed[0.7*107]

0.9

0.8

0.8

0.7

0.7

0.6

0.6

P(solve)

P(solve)

0.9

0.5
0.4

0.3
0.2

0.1

0.1
1e+006
1e+007
run-time [search steps]

10

1e+008

1e+009

empirical RTD DLS-MC
ed[35]

0.4

0.2

100000

1

0.5

0.3

0
10000

0.1
run-time [CPU sec]

0
0.01

0.1

1
10
run-time [CPU sec]

100

1000

Figure 3: Run-time distributions DLS-MC applied C1000.9 (top) brock800 1
(bottom), measured search steps (left) CPU seconds (right) reference machine (based 100 independent runs reached best
known clique size); empirical RTDs well approximated exponential
distributions, labelled ed[m](x) = 1 2x/m plots.
4.2 Penalty Delay Parameter Vertex Penalties
penalty delay parameter pd specifies number penalty increase iterations must
occur DLS-MC penalty decrease (by 1) vertices currently
175

fiPullan & Hoos

Vertex frequency

penalty. MAX-CLIQUE problem, pd basically provides mechanism focusing
lower degree vertices constructing current cliques. pd = 1 (i.e., penalties),
frequency vertices improving neighbour / level neighbour sets
basically solely dependent degree. Increasing pd overcomes bias towards
higher degree vertices, allows penalty values increase (as often
current clique), inhibits selection current clique. turn
allows lower degree vertices become part current clique. effect penalty
delay parameter illustrated Figure 4, shows correlation degree
vertices frequency included current clique immediately prior
perturbation performed within DLS-MC.

C1000.9 pd = 1
0.4
0.2

Vertex frequency

0
86

0.4

87

88

89
90
Vertex degree

91

92

93

brock800_1 pd = 1

0.3
0.2
0.1
0
58

60

62

64
Vertex degree

66

68

70

62

64
Vertex degree

66

68

70

Vertex frequency

0.25
brock800_1 pd = 45
0.2
0.15
0.1
0.05
58

60

Figure 4: Correlation vertex degree frequency vertices
present clique immediately prior DLS-MC perturbation.
C1000.9 brock800 1, pd = 1, higher degree vertices tend
higher frequency present clique immediately prior DLS-MC
perturbation. brock800 1, pd = 45, frequency present
clique immediately prior DLS-MC perturbation almost independent
vertex degree.
Currently, pd needs tuned family (or, case brock instances,
sub-family) instances. general, could done principled way based RTD
graphs, DLS-MC, reasonably robust regard exact value
parameter (as shown Figures 5 6), actual tuning process simple, almost
interactive process normally require evaluating RTD graphs. Still, fine-tuning
based RTD data could possibly result further, minor performance improvements.

176

fiDynamic Local Search Max-Clique Problem

100

% Success rate

95
90
85
80
75
70

0

10

20

30
Penalty delay

40

50

60

0

10

20

30
Penalty delay

40

50

60

Median processor time

300
250
200
150
100
50
0

Figure 5: Success rate median CPU time DLS-MC function penalty delay
parameter, pd, benchmark instance brock800 1. data point based
100 independent runs.

Cumulative success rate

100
80

pd = 35
pd = 45
pd = 50

60
40
20
0
4
10

5

6

10

7

10
Steps

8

10

10

Cumulative success rate

100
80

pd = 35
pd = 45
pd = 50

60
40
20
0
1
10

0

10

1

10
Processor time (seconds)

2

10

Figure 6: Run-time distributions DLS-MC brock800 1 penalty delays 35, 45
50, measuring run-time search steps (top) CPU seconds (bottom).
performance penalty delay 45 clearly dominates 35 50.

177

fiPullan & Hoos

effect penalty delay parameter vertex penalties clearly illustrated
Figure 7, shows cumulative distributions number penalised vertices
perturbation DLS-MC, representative runs DLS-MC DIMACS brock800 1
instance, varying values parameter pd. Note brock800 1, optimal
pd value 45 corresponds point where, average, 90% vertices
penalised. role pd parameter illustrated Figure 8, shows
(sorted) frequency vertices present current clique immediately
prior perturbation C1000.9 brock800 1. Note instances,
using higher penalty delay settings significanly reduces bias towards including certain
vertices current clique. previously demonstrated, without vertex penalties (i.e.,
pd = 1), DLS-MC prefers include high-degree vertices current clique,
case problem instances like C1000.9, optimal cliques tend consist vertices
higher-than-average degrees, effective strategy. instances brock800 1,
however, optimal clique contains many vertices lower-than-average degree,
heuristic bias towards high-degree vertices misleading needs counteracted, e.g.,
means vertex penalties.
100
pd = 5
pd = 10
pd = 15
pd = 20
pd = 25
pd = 30
pd = 35
pd = 40
pd = 45
pd = 50
pd = 55

90

80

Cumulative frequency

70

60

50

40

30

20

10

0

0

100

200

300

400
500
Penalised vertices

600

700

800

Figure 7: Cumulative distributions number penalised vertices measured
search perturbation representative independent runs DLS-MC DIMACS brock800 1 instance penalty delay parameter pd varied (the left
curve corresponds pd = 5). Note approx. optimal penalty
delay pd = 45 (solid line), average 90% vertices penalised (i.e.,
penalty value greater zero).
Generally, reducing bias cliques visited, vertex penalties help diversify
search DLS-MC. time, penalties appear provide learning
mechanism DLS-MC identifies vertices included
178

fiDynamic Local Search Max-Clique Problem

C1000.9
% frequency vertex clique

0.5
pd = 1
pd = 10

0.4
0.3
0.2
0.1
0

0

100

200

300

400

500
Vertex

600

700

800

900

1000

brock800_1
% frequency vertex clique

0.4
pd = 1
pd = 45
0.3

0.2

0.1

0

0

100

200

300

400
Vertex

500

600

700

800

Figure 8: Sorted frequency vertices present current clique immediately prior DLS-MC perturbation C1000.9 (top) brock800 1
(bottom), based representative run problem instance. Note
using penalty delay values pd > 1, bias towards using certain vertices
frequently others substantially reduced.
current clique. agreement recent results SAPS, high-performance
dynamic local search algorithm SAT (Hoos & Stutzle, 2004).
4.3 Perturbation Mechanism Search Mobility
prevent search stagnation, DLS-MC uses perturbation mechanism executed
whenever plateau search procedure failed lead clique
expanded. Since mechanism causes major changes current clique, relatively
high time complexity. therefore interesting investigate frequently rather
costly disruptive perturbation steps performed. Figure 9 shows distribution
number improving search steps (i.e., clique expansions) plateau steps (i.e., vertex
swaps) successive perturbation phases representative run DLS-MC
C1000.9 instance. Analogous results brock800 1 shown Figure 10. figures
basically show result interactions improving plateau search steps,
perturbation mechanism problem structure.

179

fiPullan & Hoos

c1000.9

Cumulative frequency

100
pd = 1
pd = 2
pd = 10

80
60
40
20
0

0

10

20

30

40

50
60
Improving steps

70

80

90

100

20

30

40

50
60
Plateau swaps

70

80

90

100

Cumulative frequency

100
pd = 1
pd = 2
pd = 10

80
60
40
20
0

0

10

Figure 9: Number improving search steps plateau swaps successive perturbation phases DLS-MC C1000.9. graphs show cumulative distributions measures collected representative independent runs
pd value; solid lines correspond approx. optimal penalty delay
instance, pd = 1.
brock800_1

Cumulative frequency

100
pd = 1
pd = 2
pd = 45

80
60
40
20
0

0

5

10

15

20
Improving steps

25

30

35

40

10

15

20
Plateau swaps

25

30

35

40

Cumulative frequency

100
pd = 1
pd = 2
pd = 45

80
60
40
20
0

0

5

Figure 10: Number improving search steps plateau swaps successive perturbation phases DLS-MC brock800 1. graphs show cumulative
distributions measures collected representative independent runs
pd value; solid lines correspond approx. optimal penalty delay
instance, pd = 45.
180

fiDynamic Local Search Max-Clique Problem

seen data, compared higher penalty delay values, pd = 1
results significantly shorter plateau phases somewhat longer improvement phases.
time, differences behaviour DLS-MC observed various penalty
delay values greater one relatively small. One explanation phenomenon lies
fact pd = 1, effectively vertex penalties used, consequently,
selection improving level neighbours sets search step less constrained.
Intuitively, make easier find exits plateaus underlying search
landscape follow gradients larger number search steps.

Whether renders search efficient clearly depends topology
given search landscape. Instance C1000.9 least 70 optimal solutions (see Table 1),
construction, optimal cliques higher-than-average vertex degree. suggests
respective search landscape relatively high fitness-distance correlation,
would explain problem instance relatively easy solve also using
less radical perturbation mechanism associated pd = 1 (which adds randomly chosen
vertex v current clique removes vertices connected v) provides sufficient
diversification search process. Instance brock800 1, hand, appears
single optimal solution many near-optimal solutions (i.e., large nonoptimal cliques cannot extended), since construction, optimal clique
lower-than-average vertex degree. suggests respective search landscape
relatively low fitness-distance correlation, therefore, radical perturbation
mechanism used pd > 1 (which restarts clique construction recently
added vertex uses vertex penalties diversification) required order obtain
good performance; hypothesis also agreement relatively high cost
solving problem instance.

investigate efficacy perturbation DLS-MC diversification mechanism, measured relative mobility search, defined Hamming distance
current cliques (i.e., number different vertices) consecutive perturbations
divided two times maximum clique size, representative runs DLS-MC instances C1000.9 brock800 1 (this mobility measure closely related used
previous studies (Schuurmans & Southey, 2000)). seen Figure 11,
large difference mobility two variants perturbation mechanism
pd = 1 pd > 1; former restarts search randomly chosen vertex
consequently leads large variability Hamming distance previous clique,
latter restarts recently added vertex, using vertex penalties
increase search diversification, hence shows consistently much higher mobility. Note
vertex penalties used (i.e., pd > 1), pd value significant effect
search mobility. time, previously observed (see Figure 5), performance
DLS-MC significantly depend penalty update delay pd. demonstrates
order achieve peak performance, increased mobility afforded use
vertex penalties needs combined correct amount additional diversification
achieved using specific penalty update delay.
181

fiPullan & Hoos

C1000.9

Cumulative frequency

100
Delay 1
Delay 2
Delay 10

80
60
40
20
0

0

0.05

0.1

0.15

0.2

0.25
0.3
Relative mobility

0.35

0.4

0.45

0.5

0.35

0.4

0.45

0.5

brock800_1

Cumulative frequency

100
Delay 1
Delay 2
Delay 45

80
60
40
20
0

0

0.05

0.1

0.15

0.2

0.25
0.3
Relative mobility

Figure 11: Mobility search consecutive perturbation phases DLS-MC instances C1000.9 (top) brock800 1 (bottom). Mobility measured terms
relative Hamming distance, i.e., number different vertices respective cliques divided two times maximum clique size. graphs
show cumulative distributions relative mobility measurements collected
representative independent runs pd value problem instance;
solid lines correspond respective approx. optimal pd values.

5. Conclusions Future Work
demonstrated applying general paradigm dynamic local search
maximum clique problem, state art MAX-CLIQUE solving improved.
new algorithm, DLS-MC, similarity previous MAX-CLIQUE algorithms,
particular recently introduced DAGS algorithm: algorithms use vertex penalties
guide heuristic selection vertices searching maximum cliques. However,
unlike DAGS, initial phase unweighted greedy construction search, DLS-MC
uses updates vertex penalties throughout entire search process. Furthermore,
weight updates DAGS monotone while, DLS-MC, vertex penalties subject
increases well occasional decreases, effectively allows algorithm
forget vertex penalties time. Furthermore, DLS-MC selects vertex added
current clique step solely based penalty, vertex selection
DAGS based total weight neighbouring vertices hence implicitely uses
vertex degree heuristic guidance. fact DLS-MC, although conceptually slightly
simpler, outperforms DAGS one standard DIMACS benchmark instances
combination excellent performance compared high-performance MAX182

fiDynamic Local Search Max-Clique Problem

CLIQUE algorithms clearly demonstrates value underlying paradigm dynamic
local search non-monotone penalty dynamics.
work presented article extended several directions. particular,
would interesting investigate extent use multiplicative penalty update
mechanisms DLS-MC instead current additive mechanism lead performance improvements. also believe current implementation DLS-MC
optimised. example, selection vertex added current
clique, implementation DLS-MC performs complete scan either improving
plateaus sets build list vertices lowest penalties; would probably
efficient maintain list means incremental update scheme. Another
interesting direction future research develop mechanisms automatically
adjusting DLS-MCs penalty delay parameter search, similar scheme used
dynamically adapting tabu tenure parameter RLS (Battiti & Protasi, 2001)
Reactive Tabu Search (Battiti & Tecchiolli, 1994), mechanism used controlling
noise parameter Adaptive Novelty+ (Hoos, 2002). Finally, given excellent performance DLS-MC standard MAX-CLIQUE instances reported suggests
underlying dynamic local search method substantial potential provide basis
high-performance algorithms combinatorial optimisation problems, particularly
weighted versions MAX-CLIQUE conceptually related clustering problems.

Acknowledgments
authors would like thank Liang Zhao participation performing
initial experiments paper.

References
Balus, E., & Yu, C. (1986). Finding maximum clique arbitary graph. SIAM Journal
Computing, 15 (4), 10541068.
Battiti, R., & Protasi, M. (2001). Reactive local search maximum clique problem.
Algorithmica, 29, 610637.
Battiti, R., & Tecchiolli, G. (1994). reactive tabu search. ORSA Journal Computing,
6 (2), 126140.
Bomze, I., Budinich, M., Pardalos, P., & Pelillo, M. (1999). maximum clique problem.
D.Z. Du, P. P. (Ed.), Handbook Combinatorial Optimization, Vol. A, pp. 174.
Boppana, R., & Halldorsson, M. (1992). Approximating maximum independent sets
excluding subgraphs. Bit, 32, 180196.
Brockington, M., & Culberson, J. (1996). Camouflaging independent sets quasi-random
graphs. D.S. Johnson, M. T. (Ed.), Cliques, Coloring Satisfiability: Second
DIMACS Implementation Challenge, Vol. 26 DIMACS Series. American Mathematical Society.
183

fiPullan & Hoos

Busygin, S. (2002). new trust region technique maximum clique problem. Internal
report, http://www.busygin.dp.ua.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
N P-Completeness. Freeman, San Francisco, CA, USA.
Grosso, A., Locatelli, M., & Croce, F. D. (2004). Combining swaps node weights
adaptive greedy approach maximum clique problem. Journal Heuristics,
10, 135152.
Hastad, J. (1999). Clique hard approximate within n1 . Acta Mathematica, 182,
105142.
Hoos, H. H. (2002). adaptive noise mechanism WalkSAT. Proceedings
Eighteenth National Conference Artificial Intelligence, pp. 655660. AAAI Press /
MIT Press, Menlo Park, CA, USA.
Hoos, H. H., & Stutzle, T. (2004). Stochastic Local Search: Foundations Applications.
Morgan Kaufmann Publishers, USA.
Hoos, H., & Stutzle, T. (2000). Local search algorithms SAT: empirical evaluation.
Gent, I., v.Maaren, H., & Walsh, T. (Eds.), SAT 2000, pp. 4386. IOS Press.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling probabilistic smoothing: Efficient dynamic local search SAT. Hentenryck, P. V. (Ed.), Principles
Practice Constraint Programming CP 2002, Vol. 2470 Lecture Notes
Computer Science, pp. 233248. Springer Verlag, Berlin, Germany.
Ji, Y., Xu, X., & Stormo, G. D. (2004). graph theoretical approach predicting common RNA secondary structure motifs including pseudoknots unaligned sequences.
Bioinformatics, 20 (10), 15911602.
Johnson, D., & Trick, M. (Eds.). (1996). Cliques, Coloring Satisfiability: Second DIMACS Implementation Challenge, Vol. 26 DIMACS Series. American Mathematical
Society.
Katayama, K., Hamamoto, A., & Narihisa, H. (2004). Solving maximum clique problem k-opt local search. Proceedings 2004 ACM Symposium Applied
computing, pp. 10211025.
Marchiori, E. (2002). Genetic, iterated multistart local search maximum clique
problem. Applications Evolutionary Computing, Vol. 2279 Lecture Notes
Computer Science, pp. 112121. Springer Verlag, Berlin, Germany.
Pevzner, P. A., & Sze, S.-H. (2000). Combinatorial approaches finding subtle signals
DNA sequences. Proceedings Eighth International Conference Intelligent
Systems Molecular Biology, pp. 269278. AAAI Press.
Pullan, W., & Zhao, L. (2004). Resolvent clause weighting local search. Tawfik, A. Y.,
& Goodwin, S. D. (Eds.), Advances Artificial Intelligence, 17th Conference
Canadian Society Computational Studies Intelligence, Vol. 3060 Lecture Notes
Computer Science, pp. 233247. Springer Verlag, Berlin, Germany.
184

fiDynamic Local Search Max-Clique Problem

Resende, M., Feo, T., & Smith, S. (1998). Algorithm 786: FORTRAN subroutine approximate solution maximum independent set problem using GRASP. ACM
Transactions Mathematical Software, 24, 386394.
Schuurmans, D., & Southey, F. (2000). Local search characteristics incomplete SAT
procedures. Proceedings Seventeenth National Conference Artificial Intelligence, pp. 297302. AAAI Press / MIT Press, Menlo Park, CA, USA.
Solnon, C., & Fenet, S. (2004). study aco capabilities solving maximum clique
problem. Journal Heuristics, appear.
Thornton, J., Pham, D. N., Bain, S., & Ferreira, V. (2004). Additive versus multiplicative
clause weighting SAT. Proceedings 19th National Conference Artificial
Intelligence (AAAI-04), pp. 191196. AAAI Press / MIT Press, Menlo Park, CA,
USA.
Tompkins, D., & Hoos, H. (2003). Scaling probabilistic smoothing: Dynamic local
search unweighted MAX-SAT. Xiang, Y., & Chaib-draa, B. (Eds.), Advances
Artificial Intelligence, 16th Conference Canadian Society Computational
Studies Intelligence, Vol. 2671 Lecture Notes Computer Science, pp. 145159.
Springer Verlag, Berlin, Germany.
Watson, J., Whitley, L., & Howe, A. (2005). Linking search space structure, run-time
dynamics, problem difficulty: step toward demystifying tabu search. Journal
Artificial Intelligence, 24, 221261.

185

fiJournal Artificial Intelligence Research 25 (2006) 389424

Submitted 09/05; published 03/06

Graphical Modeling Preference Importance
Ronen I. Brafman

brafman@cs.stanford.edu

Department Computer Science
Stanford University
Stanford CA 94305

Carmel Domshlak

dcarmel@ie.technion.ac.il

Faculty Industrial Engineering Management
Technion - Israel Institute Technology
Haifa, Israel 32000

Solomon E. Shimony

shimony@cs.bgu.ac.il

Department Computer Science
Ben-Gurion University
Beer Sheva, Israel 84105

Abstract
recent years, CP-nets emerged useful tool supporting preference elicitation, reasoning, representation. CP-nets capture support reasoning qualitative conditional preference statements, statements relatively natural users
express. paper, extend CP-nets formalism handle another class
natural qualitative statements one often uses expressing preferences daily life statements relative importance attributes. resulting formalism, TCP-nets, maintains
spirit CP-nets, remains focused using simple natural preference
statements, uses ceteris paribus semantics, utilizes graphical representation
information reason consistency perform, possibly constrained, optimization using it. extra expressiveness provides allows us better model tradeoffs
users would like make, faithfully representing preferences.

1. Introduction
ability make decisions assess potential courses action corner-stone
numerous AI applications, including expert systems, autonomous agents, decision-support
systems, recommender systems, configuration software, constrained optimization applications. make good decisions, must able assess compare different alternatives. Sometimes, comparison performed implicitly, many recommender
systems (Burke, 2000; Resnick & Varian, 1997). frequently, explicit information
decision-makers preferences required.
classical decision theory decision analysis utility functions used represent
decision-makers preferences. However, process obtaining type information
required generate good utility function involved, time-consuming requires nonnegligible effort part user (French, 1986). Sometimes effort necessary
possible, many applications user cannot engaged lengthy period
time cannot supported human decision analyst. instance, case
on-line product recommendation systems software decision-support applications.
c
2006
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBrafman, Domshlak, & Shimony

utility function cannot need obtained, one resort other,
qualitative forms preference representation. Ideally, qualitative information
easily obtainable user non-intrusive means. is,
able extract information natural relatively simple statements preference
provided user, elicitation process amenable automation.
addition, automated reasoning qualitative preference information
semantically effective computationally efficient.
One framework preference representation addresses concerns
Conditional Preference Networks (CP-nets) (Boutilier et al. 1999, 2004a). CP-nets
graphical preference representation model grounded notion conditional preferential independence. preference elicitation CP-nets, decision maker (directly
indirectly) describes preference values one variable depends value
variables. example, may state preference dessert depends
main-course well whether alcoholic beverage. turn, preference alcoholic beverage may depend main course time day.
information described graphical structure nodes represent variables
interest edges capture direct preferential dependence relations variables.
node annotated conditional preference table (CPT) describing users
preference alternative values node given different values parent nodes.
CP-nets capture class intuitive useful natural language statements form
prefer value x0 variable X given = y0 Z = z0 . statements
require complex introspection quantitative assessment.
practical perspective, another class preference statements
less intuitive important, yet captured CP-net model. statements
form: important value X better value
better. call relative importance statements. instance, one might say
length journey important choice airline.
refined notion importance, though still intuitive easy communicate, conditional relative importance: length journey important
choice airline need give talk following day. Otherwise, choice airline
important. latter statement form: better assignment X
important better assignment given Z = z0 . Notice information
relative importance different information preferential independence.
instance, example above, users preference airline depend
duration journey because, e.g., compares airlines based service,
security levels, quality frequent flyer program. Informally, using statements
relative importance user expresses preference compromises may
required. information important customized product configuration applications (Sabin & Weigel, 1998; Haag, 1998; Freuder & OSullivan, 2001), production,
supply, constraints posed product space producer,
constraints typically even unknown customer. Indeed, many applications, various resource (e.g., money, time, bandwidth) constraints exist, main computational
task finding solution feasible preferentially dominated
solution.
390

fiTCP-Nets

paper consider enhancing expressive power CP-nets introducing
information importance relations, obtaining preference-representation structure
call TCP-nets (for tradeoffs-enhanced CP-nets). capturing information
conditional preferential independence conditional relative importance, TCP-nets
provide richer framework representing user preferences, allowing stronger conclusions
drawn, yet remaining committed use intuitive, qualitative information. time, show added relative importance information
significant impact consistency specified relation, techniques used
reasoning it. Focusing computational issues, show graphical structure mixed set preference statements captured TCP-net often
exploited order achieve efficiency consistency testing preferential
reasoning.
paper organized follows: Section 2 describes notions underlying TCP-nets:
preference relations, preferential independence, relative importance. Section 3
define TCP-nets, specify semantics, provide number examples. Section 4
characterize class conditionally acyclic TCP-nets whose consistency guaranteed
then, Section 5 discuss complexity identifying members class.
Section 6 present algorithm outcome optimization conditionally acyclic TCPnets, discuss related tasks reasoning preferences given TCP-net.
conclude discussion related future work Section 7.

2. Preference Orders, Independence, Relative Importance
section describe semantic concepts underlying TCP-nets: preference orders,
preferential independence, conditional preferential independence, well relative importance conditional relative importance.
2.1 Preference Independence
model preference relation strict partial order. Thus, use terms preference
order strict partial order interchangeably. strict partial order binary relation
outcomes anti-reflexive, anti-symmetric transitive. Given two outcomes
o, o0 , write o0 denote strictly preferred o0 .
choice implies two outcomes cannot equally preferred. choice
follows fact language preferences use paper allow
statements indifference (as opposed incomparability), thus need
using weak orderings. Incorporating statements indifference pretty straightforward,
explained Boutilier et al. (2004a), introduces much overhead formally
treat throughout paper.
types outcomes concerned consist possible assignments set
variables. formally, assume given set V = {X1 , . . . , Xn } variables
corresponding domains D(X1 ), . . . , D(Xn ). set possible outcomes D(V) =
D(X1 ) D(Xn ), use D() denote domain set variables well.
example, context problem configuring personal computer (PC),
variables may processor type, screen size, operating system etc., screen size
domain {17in, 19in, 21in}, operating system domain {LINUX, Windows98,
391

fiBrafman, Domshlak, & Shimony

WindowsXP}, etc. complete assignment set variables specifies outcome
particular PC configuration. Thus, preference relation outcomes specifies
strict partial order possible PC configurations.
number possible outcomes exponential n, set possible orderings
doubly exponential n. Therefore, explicit specification representation ordering realistic, thus must describe implicitly using
compact representation model. notion preferential independence plays key role
representations. Intuitively, X V preferentially independent = V X
assignments Y, preference X values identical.
Definition 1 Let x1 , x2 D(X) X V, y1 , y2 D(Y), = V X.
say X preferentially independent iff, x1 , x2 , y1 , y2
x1 y1 x2 y1 iff x1 y2 x2 y2

(1)

example, PC configuration example, user may assess screen size preferentially independent processor type operating system. could case
user always prefers larger screen smaller screen, independent selection
processor and/or OS.
Preferential independence strong property, therefore common.
refined notion conditional preferential independence. Intuitively, X conditionally preferentially independent given Z every fixed assignment
Z, ranking X values independent value Y.
Definition 2 Let X, Z partition V let z D(Z). X conditionally
preferentially independent given z iff, x1 , x2 , y1 , y2
x1 y1 z x2 y1 z iff x1 y2 z x2 y2 z,

(2)

X conditionally preferentially independent given Z iff X conditionally preferentially independent given every assignment z D(Z).
Returning PC example, user may assess operating system independent
features given processor type. is, always prefers LINUX given AMD
processor WindowsXP given Intel processor (e.g., might believe
WindowsXP optimized Intel processor, whereas LINUX otherwise better). Note
notions preferential independence conditional preferential independence
among number standard well-known notions independence multi-attribute
utility theory (Keeney & Raiffa, 1976).
2.2 Relative Importance
Although statements preferential independence natural useful, orderings obtained relying alone relatively weak. understand this, consider two
preferentially independent boolean attributes B values a1 , a2 b1 , b2 , respectively. B preferentially independent, specify preference order
values, say a1 a2 , independently value B. Similarly, preference
392

fiTCP-Nets

B values, say b1 b2 , independent value A. deduce a1 b1
preferred outcome a2 b2 least preferred outcome. However,
know relative order a1 b2 a2 b1 . typically case consider
independent variables: rank one given fixed value other, often,
cannot compare outcomes values different. One type information
address (though necessarily all) comparisons information
relative importance. instance, state important B, means
prefer improvement improvement B. case, know
a1 b2 a2 b1 , totally order set outcomes a1 b1 a1 b2 a2 b1 a2 b2 .
One may ask important us order a1 b2 a2 b1 all, know
a1 b1 preferred outcome. However, many typical scenarios,
auxiliary user constraints prevent us providing user preferred
(unconstrained) outcome. simple common example budget constraints,
resource limitations, bandwidth buffer size (as adaptive richmedia systems described Brafman Friedman (2005) also common. cases,
important know attributes user cares strongly, try
maintain good values attributes, compromising others.
Returning PC configuration example, suppose attributes operating system processor type mutually preferentially independent. might say processor type important operating system, e.g, believe effect
processors type system performance significant effect
operating system.
Definition 3 Let pair variables X mutually preferentially independent given
W = V {X, }. say X important , denoted X ,
every assignment w D(W) every xi , xj D(X), ya , yb D(Y ), xi xj
given w, that:
xi ya w xj yb w.
(3)
Note Eq. 3 holds even yb ya given w. instance, X
binary variables, x1 x2 y1 y2 hold given w, X iff
x1 y2 w x2 y1 w w D(W). Notice strict notion importance
reduction preferred reduction X. Clearly, idea refined
providing actual ordering elements D(XY ), discuss extension
Section 3.4. addition, one consider relative importance assessments among
two variables. However, feel benefit capturing statements small:
believe statements relative importance referring two attributes
natural users articulate, inclusion would significantly reduce
computational advantages graphical modeling. Therefore, work focus
relative importance statements referring pairs attributes.
Relative importance information natural enhancement independence information.
such, relative importance retains desirable property - corresponds statements
naive user would find simple clear evaluate articulate. Moreover,
generalized naturally notion conditional relative importance. instance, suppose
relative importance processor type operating system depends primary
usage PC. example, PC used primarily graphical applications,
393

fiBrafman, Domshlak, & Shimony

choice operating system important processor certain
important software packages graphic design available LINUX. However,
applications, processor type important applications
Windows LINUX exist. Thus, say X important given z
always prefer reduce value rather value X, whenever z holds.
Definition 4 Let X pair variables V, let Z W = V {X, }.
say X important given z D(Z) iff, every assignment w0
W0 = V ({X, } Z) have:
xi ya zw0 xj yb zw0

(4)

whenever xi xj given zw0 . denote relation X z . Finally,
z D(Z) either X z , z X, say relative importance
X conditioned Z, write RI(X, |Z).

3. TCP-nets
TCP-net (for Tradeoff-enhanced CP-nets) model extension CP-nets (Boutilier
et al., 2004a) encodes conditional relative importance statements, well conditional preference statements supported CP-nets. primary usage TCP-net
graphical structure consistency analysis provided preference statements,
classification complexity developing efficient algorithms various reasoning tasks
statements. particular, later show, structure acyclic
(for suitable definition notion!), set preference statements represented
TCP-net guaranteed consistent is, strict total order
outcomes satisfies preference statements. follows formally define
TCP-net model. subsumes CP-net model, immediately define
general model rather proceed stages.
3.1 TCP-net Definition
TCP-nets annotated graphs three types edges. nodes TCP-net correspond problem variables V. first type (directed) edges comes
original CP-nets model captures direct preferential dependencies, is, edge
X implies user different preferences values given different
values X. second (directed) edge type captures relative importance relations.
existence edge X implies X important . third
(undirected) edge type captures conditional importance relations: edge
nodes X exists exists non-empty variable subset Z V {X, }
RI(X, |Z) holds. Without loss generality, follows, set Z assumed
minimal set variables upon relative importance X depends.
CP-nets, node X TCP-net annotated conditional preference
table (CPT). table associates preferences D(X) every possible value assignment
parents X (denoted P a(X)). addition, TCP-nets, undirected edge
annotated conditional importance table (CIT). CIT associated edge
394

fiTCP-Nets

(X, ) describes relative importance X given value corresponding
importance-conditioning variables Z.
Definition 5 TCP-net N tuple hV, cp, i, ci, cpt, citi where:
(1) V set nodes, corresponding problem variables {X1 , . . . , Xn }.
(2) cp set directed cp-arcs {1 , . . . , k } (where cp stands conditional preference).

cp-arc hXi , Xj N iff preferences values Xj depend actual

value Xi . X V, let P a(X) = {X 0 |hX 0 , Xi cp}.

(3) set directed i-arcs {1 , . . . , l } (where stands importance). i-arc (Xi , Xj )
N iff Xi Xj .
(4) ci set undirected ci-arcs {1 , . . . , } (where ci stands conditional importance). ci-arc (Xi , Xj ) N iff RI(Xi , Xj |Z) Z V{Xi , Xj }.1
call Z selector set (Xi , Xj ) denote S(Xi , Xj ).

(5) cpt associates CPT every node X V, CP (X) mapping
D(P a(X)) (i.e., assignments Xs parent nodes) strict partial orders D(X).
(6) cit associates every ci-arc = (Xi , Xj ) (possibly partial) mapping CIT ()
(S(Xi , Xj )) orders set {Xi , Xj }.2
TCP-net sets ci (and therefore also cit) empty, also CP-net.
Thus, elements i, ci, cit describe absolute conditional importance
attributes provided TCP-nets, beyond conditional preference information captured
CP-nets.
3.2 TCP-net Semantics
semantics TCP-net defined terms set strict partial orders consistent
set constraints imposed preference importance information captured
TCP-net. intuitive idea rather straightforward: (1) strict partial order
satisfies conditional preferences variable X two complete assignments
differ value X ordered consistently ordering X values
CPT X. Recall ordering depend parent X graph.
(2) strict partial order satisfies assertion X important given
two complete assignments differ value X only, prefers
assignment provides X better value. (3) strict partial order satisfies
assertion X important given assignment z variable set Z
given two complete assignments differ value X only, (both
of) Z assigned z, prefers assignment provides X better value.

1. Observe every i-arc (Xi , Xj ) seen representing RI(Xi , Xj |). However, clear distinction
i-arcs ci-arc simplifies specification many forthcoming notions claims (e.g., Lemma 3
Section 4, well related notion root variables.)
2. is, relative importance relation Xi Xj may specified certain values
selector set.

395

fiBrafman, Domshlak, & Shimony

defined formally below. use X
u denote preference relation
values X given assignment u U P a(X).
Definition 6 Consider TCP-net N = hV, cp, i, ci, cpt, citi.
1. Let W = V ({X} P a(X)) let p D(P a(X)). preference (=strict partial)
order D(V) satisfies X
p iff xi pw xj pw, every w D(W), whenever
X
xi p xj holds.
2. preference order D(V) satisfies CP (X) cpt iff satisfies X
p every
assignment p P a(X).
3. preference order D(V) satisfies X iff every w D(W)
W = V {X, }, xi ya w xj yb w whenever xi X
w xj .
4. preference order D(V) satisfies X z iff every w D(W)
W = V ({X, } Z), xi ya zw xj yb zw whenever xi X
zw xj .
5. preference order D(V) satisfies CIT () ci-arc = (X, ) cit
satisfies X z whenever entry table conditioned z ranks X
important.
preference order D(V) satisfies TCP-net N = hV, cp, i, ci, cpt, citi iff:
(1) every X V, satisfies CP (X),

(2) every i-arc = (Xi , Xj ) i, satisfies X ,
(3) every ci-arc = (Xi , Xj ) ci, satisfies CIT ().

Definition 7 TCP-net satisfiable iff strict partial order D(V)
satisfies it; o0 implied TCP-net N iff holds preference orders
D(V) satisfy N .
Lemma 1 Preferential entailment respect satisfiable TCP-net transitive. is,
N |= o0 N |= o0 o00 , N |= o00 .
Proof: N |= o0 N |= o0 o00 , o0 o0 o00 preference orders
satisfying N . ordering transitive, must o00 satisfying
orderings.
Note that, strictly speaking, use term satisfiable rather consistent respect set preference statements, given provide model theory,
proof theory. However, since corresponding proof theory follows completely straightforward manner semantics combined transitivity, raises
problem.
396

fiTCP-Nets

3.3 TCP-net Examples
provided formal specification TCP-nets model, let us illustrate TCPnets examples. simplicity presentation, following examples
variables binary, although semantics TCP-nets given Definitions 6 7
respect arbitrary finite domains.
Example 1 (Evening Dress) Figure 1(a) presents CP-net consists three variables
J, P , S, standing jacket, pants, shirt, respectively. prefer black white
color jacket pants, preference shirt color (red/white)
conditioned color combination jacket pants: color,
white shirt make dress colorless, therefore, red shirt preferable. Otherwise,
jacket pants different colors, red shirt probably make evening
dress flashy, therefore, white shirt preferable. solid lines Figure 1(c) show
preference relation induced directly information captured CP-net;
top bottom elements worst best outcomes, respectively,
arrows directed less preferred preferred outcomes.
J b Jw



Jw Pw Sw

Pb Pw

?>=<
89:;
@ABC
GFED
J/
P
//

//

//


//

?>=<
89:;

J b Pb
J w Pb
J b Pw
J w Pw

Sr Sw
Sw Sr
Sw Sr
Sr Sw

(a)
?>=<
89:;
GFED
/@ABC

J/
P
//


//

//


//

?>=<
89:;





Jw Pw Sr





Jw Pb Sr _ _ _ _/ iJb Pw Sr



iiii
iiii



iiii
iiii




tii




Jb Pw LSw o_ _ _ _Jw Pb Sw
LLL
LLL
LLL
LLL
LLL
%

Jb Pb Sw
}


Jb Pb Sr

(b)

(c)
Figure 1: Evening Dress CP-net & TCP-net.

Figure 1(b) depicts TCP-net extends CP-net adding i-arc J P ,
i.e., black jacket (unconditionally) important black pants.
397

fiBrafman, Domshlak, & Shimony

induces additional relations among outcomes, captured dashed lines Figure 1(c).
3

reader may rightfully ask whether statement importance Example 1
redundant: According preference, seems always wear black suit
red shirt. However, preferences clear, various constraints may make
outcomes, including preferred one, infeasible. instance, may clean
black jacket, case preferred feasible alternative white jacket, black
pants, white shirt. Alternatively, suppose clean clothes velvet
black jacket white pants, silk white jacket black pants. wife forbids
mix velvet silk, compromise, wear either black (velvet)
jacket white (velvet) pants, white (silk) jacket black (silk) pants.
case, fact prefer wearing preferred jacket wearing preferred pants
determines higher desirability velvet combination. Now, wife prepare
evening dress late work writing paper, information help
choose among available options outfit would like most.
Indeed, noted earlier, many applications involve limited resources, money,
time, bandwidth, memory, etc. many instances, optimal assignment violates
resource constraints, must compromise accept less desirable, feasible assignment. TCP-nets capture information allows us make informed compromises.
Example 2 (Flight USA) Figure 2(a) illustrates complicated CP-net, describing preference flight options conference USA, Israel.
network consists five variables, standing various parameters flight:
Day Flight variable distinguishes flights leaving day (D1d )
two days (D2d ) conference, respectively. Since married,
really busy work, prefer leave day conference.
Airline variable represents airline. prefer fly British Airways (Aba )
KLM (Aklm ).
Departure Time variable distinguishes morning/noon (Tm ) evening/night
(Tn ) flights. Among flights leaving two days conference prefer evening/night
flight, allow work longer day flight. However, among
flights leaving day conference prefer morning/noon flight,
would like hours conference opening order rest
hotel.
Stop-over variable distinguishes direct (S0s ) indirect (S1s ) flights,
respectively. day flights awake time and, smoker, prefer
stop-over Europe (so smoking break). However, night flights
sleep, leading preference direct flights, since shorter.
Ticket Class variable C stands ticket class. night flight, prefer sit
economy class (Ce ) (I dont care sleep, seats significantly
cheaper), day flight prefer pay seat business class (Cb ) (Being
awake, better appreciate good seat, food, wine).
398

fiTCP-Nets

@ABC
GFED


D1d D2d
D1d
D2d

Tm Tn
Tn Tm

Tm
Tn

S1s S0s
S0s S1s

Aba Aklm
Tm
Tn

Cb Ce
Ce Cb

@ABC
GFED


89:;
?>=<
T0
000
00

00

0

?>=<
89:;
@ABC
GFED

C

89:;
?>=<


(a)


89:;
?>=<
?>=<
/ 89:;

T0

00
00

00

00



T,A
@ABC
GFED
89:;
?>=<

C

Tm Aklm
Tm Aba
Tn Aba

SC
C
SC

(b)

Figure 2: Flight USA CP-net & TCP-net Example 2.
CP-net Figure 2(a) captures preference statements, underlying
preferential dependencies, Figure 2(b) presents TCP-net extends CP-net
capture relative importance relations parameters flight. First,
i-arc A, getting suitable flying time important
getting preferred airline. Second, ci-arc C,
relative importance C depends values A:3
1. KLM day flight, intermediate stop Amsterdam important
flying business class (I feel KLMs business class good
cost/performance ratio, visiting casino Amsterdams airport sounds
like good idea.)
2. British Airways night flight, fact flight direct important
getting cheaper economy seat (I ready pay business class,
order spend even one minute Heathrow airport night).
3. British Airways day flight, business class important
short intermediate break (it hard find nice smoking area Heathrow).
CIT ci-arc also shown Figure 2(b). 3
3.4 Relative Importance Non-binary Variables
read far, reader may rightfully ask whether notion relative (conditional)
importance ceteris paribus, specified Section 2.2 (Eq. 3 4), strong
3. clarity, ci-arc Figure 2(b) schematically labeled importance-conditioning variables
A.

399

fiBrafman, Domshlak, & Shimony

D1d D2d

D1d
D2d

Tm Tn
Tn Tm

@ABC
GFED


?>=<
89:;






?>=<
89:;
T6

66

66

66

66
6

SC

Aba Aklm

Tm Aba
Tm Aklm
Tn Aba
Tn Aba

S1s Cb S0s Cb S1s Ce S0s Ce
S1s Cb S1s Ce S0s Cb S0s Ce
S0s Ce S0s Cb S1s Ce S1s Cb
S0s Ce S0s Cb S1s Cb
S0s Ce S1s Ce S1s Cb

Figure 3: network obtained clustering variables C Example 2.

variables binary. example, consider refined notion departure time
(variable ) Example 2, suppose two companies flying
Israel USA (variable A). case, one may prefer better flight time, even
requires compromise airline, long compromise significant.
instance, get better flight time, one may willing compromise accept
airline among ranks top places context.
generally, notion importance, well refined notions it,
really means specifying ordering assignments variable pairs. sense,
one could reduce TCP-nets CP-nets combining variables
importance relation. Thus, instance, Flight USA example, could
combine variables C (see Figure 3). resulting variable, SC
domain Cartesian product domains C. preferences values
SC conditioned , current parent C, well A,
belongs selector set CIT. general, selector set (and parents of) pair
variables viewed conditioning preferences value combinations
pair. Hence, clustering help us already case binary variables certain
orderings assignments two binary variables cannot specified TCP-net.
However, clearly issue case non-binary variables,
number combinations pairs values much larger.
bottom line complex importance relations pairs variables
captured. main questions how. strict importance relation use captures
certain relations compact manner. such, specification (e.g., terms
natural language statements) easy. rule possibility expressing
refined relations. Various linguistic constructs could used express relations.
However, technically, captured clustering relevant variables,
resulting representation would TCP-net, possibly simply CP-net. course,
quite possible relations alternative compact representation could
help make reasoning efficient simply collapsing them,
useful question future research examine.
400

fiTCP-Nets
?>=<
89:;


89:;
?>=<



?>=<
89:;
89:;
/ ?>=<



000
00

00

00


T,A
?>=<
89:;
?>=<
89:;


C
Tm Aklm
Tm Aba
Tn Aba


89:;
?>=<
?>=<
/ 89:;



0

0
0


00 ttt



tt00

0
0
tttt
yt
89:;
?>=<
89:;
?>=<
C


SC
CS
SC

(a)
()*+
/.-,


(b)
()*+
/.-,


()*+
/.-,



/.-,
()*+
/.-,
/ ()*+
T5


55ooooo




5









wo
ooo 5()*+
()*+
/.-,
/ /.-,
C



/.-,
()*+
/.-,
/ ()*+
T5


55ooooo




5









wo
ooo 5/.-,
()*+
/.-,
()*+

C


/.-,
()*+
/.-,
/ ()*+
T5


55ooooo




5









wo
ooo 5/.-,
()*+
/.-,
()*+

C

(Tm Aklm )-directed

(Tn Aklm )-directed

(Tm Aba )-directed

()*+
/.-,



'&%$
!"#
!"#
/ '&%$
T2
qqfiA
2
fi
q
q
2
fi q2 fifi
fi
fi qxfio qqq /.-,
'&%$
!"#
()*+

C
(Tn Aba )-directed

(c)
Figure 4: (a) Flight USA TCP-net. (b) dependency graph. (c), Four w-directed
graphs.

4. Conditionally Acyclic TCP-nets
Returning notion TCP-net satisfiability, observe Definition 7 provides practical tools verifying satisfiability given TCP-net. Tackling issue, section
introduce large class TCP-nets whose members guaranteed satisfiable.
refer class TCP-nets conditionally acyclic.
Let us begin notion dependency graph induced TCP-net.
Definition 8 dependency graph N ? TCP-net N contains nodes edges
N . Additionally, every ci-arc (Xi , Xj ) N every Xk S(Xi , Xj ), N ? contains
pair directed edges (Xk , Xi ) (Xk , Xj ), edges already N .
Figure 4(b) depicts dependency graph TCP-net Flight USA
example, repeated convenience Figure 4(a). next definition, recall
selector set ci-arc set nodes whose value determines direction
arc. Recall also, assign value selector set, are, essence, orienting
401

fiBrafman, Domshlak, & Shimony

conditional importance edges. generally, selector sets assigned,
transform N N ? . motivates following definition.
Definition 9 Let S(N ) union selector sets N . Given assignment w
nodes S(N ), w-directed graph N ? consists nodes directed edges
N ? . addition directed edge Xi Xj edge already
N ? , (Xi , Xj ) ci-arc N CIT (Xi , Xj ) specifies Xi Xj given w.

Figure 4(c) presents four w-directed graphs TCP-net Flight
USA example. Note that, KLM night flights, relative importance C
specified, thus edge C (Tn Aklm )-directed graph
N ? .
Using Definitions 8 9, specify class conditionally acyclic TCP-nets,
show satisfiable4 .
Definition 10 TCP-net N conditionally acyclic if, every assignment w S(N ),
induced w-directed graphs N ? acyclic.
show every conditionally acyclic TCP-net satisfiable, begin
providing two auxiliary lemmas.
Lemma 2 property conditional acyclicity TCP-nets hereditary. is, given two
TCP-nets N = hV, cp, i, ci, cpt, citi N 0 = hV0 , cp0 , i0 , ci0 , cpt0 , cit0 i,
1. N conditionally acyclic,
2. V0 V, cp0 cp, i0 i, ci0 ci, cpt0 cpt, cit0 cit,
N 0 also conditionally acyclic.
Proof:
proof straightforward Definition 10 since removing nodes and/or
edges N , well removing preference importance information CPTs
CITs N , remove cycles w-directed graphs N ? . Hence, N
conditionally acyclic, subnet N .
Lemma 3 Every conditionally acyclic TCP-net N = hV, cp, i, ci, cpt, citi contains least one


variable X V, that, V \ {X}, hY, Xi 6 cp, (Y, X) 6 i,
(X, ) 6 ci.
Proof: prove existence root variable X N , consider dependency
graph N ? . Since N conditionally acyclic, node X 0 N ? neither
incoming directed undirected edges associated it. see latter, observe
(i) every endpoint undirected edge N ? also incoming directed edge,
4. authors would like thank Nic Wilson pointing error original definition
conditionally acyclic TCP-nets (Brafman & Domshlak, 2002).

402

fiTCP-Nets

(ii) least one node N ? incoming directed edges, otherwise
conditional acyclicity N trivially violated. However, node X 0 also
root node N since edge set N ? superset N .
Theorem 1 Every conditionally acyclic TCP-net satisfiable.
Proof: prove constructively building satisfying preference ordering. fact,
inductive hypothesis stronger: conditionally acyclic TCP-net strict
total order satisfies it. proof induction number problem variables.
result trivially holds one variable definition CPTs, since simply use
strict total order consistent CPT (and trivially satisfying Definition 6.)
Assume theorem holds conditionally acyclic TCP-nets fewer
n variables. Let N TCP-net n variables, X one root variables N .
(The existence root X guaranteed Lemma 3.) Let D(X) = {x1 , . . . , xk }
domain chosen root variable X, let x1 . . . xk total ordering D(X)
consistent (possibly partial) preferential ordering dictated CP (X)
N . xi , 1 k, construct TCP-net Ni , n 1 variables V {X}
removing X original network, and:

1. variable , cp-arc hX, N , revise CPT
restricting row X = xi .
2. ci-arc = (Y1 , Y2 ), X S(), revise CIT restricting
row X = xi . If, result restriction, rows new CIT express
relative importance Y1 Y2 , replace Ni corresponding


i-arc, i.e., either (Y1 , Y2 ) (Y2 , Y1 ). Alternatively, CIT becomes empty,
simply removed Ni .

3. Remove variable X, together cp-arcs form hX, i, i-arcs

form (X, ).
Lemma 2 conditional acyclicity N implies conditional acyclicity
reduced TCP-nets Ni . Therefore, inductive hypothesis construct
preference ordering reduced networks Ni . construct
preferential ordering original network N follows. Every outcome X = xj
ranked preferred outcome X = xi , 1 < j k. outcomes
identical X value, xi , ranked according ordering associated Ni (ignoring
value X). Clearly, construction, ordering defined strict total order:
obtained taking set strict total orders ordering them, respectively.
Definition 6, easy see strict total order satisfies N .
close look proof Theorem 1 reveals key property conditionally
acyclic TCP-nets induce ordering nodes network.
ordering fixed, context dependent. Different assignments variables
prefix ordering yield different suffixes. Put differently, ordering depends
403

fiBrafman, Domshlak, & Shimony

values variables, captures relative importance variable
particular context. particular, nodes appear earlier ordering
important particular context.
observation helps explain rationale definition dependency
graph (Definition 8). sense, graph captures constraints ordering
variables. TCP-net conditionally acyclic constraints satisfiable. use
perspective explain choices made definition dependency graph
may seem arbitrary. First, consider direction (unconditional) importance edges
important less important variable. simply goes line
desire use topological ordering important variables appear first.
Second, consider direction CP-net edges parent children. turns
CP-nets, induced importance relationship parents children: parents
important children (see (Boutilier et al., 2004a)). Thus, edges
dependency graph must point parent child.
Finally, order make sense idea context-dependent ordering, must
order variables selector set ci-arc nodes connected arc.
motivation last choice may bit less clear. following example shows
necessity (i.e., Theorem 1 cannot provided stronger notion TCP-net
acyclicity obtained defining w-directed graphs N rather N ? ).

c
c

B
C

89:;
?>=<


AB
BA
aa
bb
: cc
: cc

C


@ABC
GFED
B


@ABC
GFED
C

Consider TCP-net depicted above. TCP-net N defined three boolean

variables V = {A, B, C}, cp = {hA, Ci}, ci = {(A, B)} S(A, B) = {C},
= . Clearly, two possible w-directed graphs N (not N ? ) acyclic.
Now, suppose exists strict partial order 0 D(V) satisfies N .
Definition 6,
(1) abc 0 abc (from CP (C)),
(2) abc 0 abc (from CIT ((A, B)) CP (B)),
(3) abc 0 abc (from CP (C)),
(4) abc 0 abc (from CIT ((A, B)) CP (A)).
However, implies 0 anti-symmetric, contradicting assumption 0
strict partial order.
404

fiTCP-Nets

5. Verifying Conditional Acyclicity
contrast standard acyclicity directed graphs, property conditional acyclicity
cannot easily tested general. Naive verification acyclicity every w-directed
graph require time exponential size S(N ). study complexity
verifying conditional acyclicity, discuss hard polynomial subclasses
problem, provide sufficient and/or necessary conditions conditional acyclicity
easily checked certain subclasses TCP-nets.
Let N TCP-net. cycles undirected graph underlying N ?
(i.e., graph obtained N ? making directed edges undirected edges),
clearly w-directed graphs N ? acyclic, property N ? simple
check. Alternatively, suppose underlying undirected graph N ? contain
cycles. projection cycle back N ? contains directed arcs oriented
different directions cycle (one clockwise another counter-clockwise),
w-directed graphs N ? still guaranteed acyclic. instance, subset (of
size > 2) variables {T, A, S, C} running example Figure 4 forms cycle
undirected graph underlying N ? , yet cycle satisfies aforementioned
criterion. sufficient condition conditional acyclicity also checked (low
order) polynomial time.
remaining cases dependency graph N ? contains define
semi-directed cycles, rest section study cases closely.
Definition 11 Let mixed set directed undirected edges, AU undirected graph underlying (that is, graph obtained dropping orientation
directed edges.) say semi-directed cycle
(1) AU forms simple cycle (that is, AU consists single connected component
vertices degree 2 w.r.t. AU ).
(2) edges directed.
(3) directed edges point direction along AU (i.e., clockwise
counter-clockwise).
assignment w selector sets ci-arcs semi-directed cycle N ?
induces direction ci-arcs. say semi-directed cycle conditionally
acyclic assignment w obtain directed cycle A. Otherwise,
called conditionally directed. Figure 5 illustrates semi-directed cycle (based
variables running example) two possible configurations CITs make
semi-directed cycle conditionally directed conditionally acyclic, respectively.
Using notions, Lemma 4 shows testing conditional acyclicity TCP-nets
naturally decomposable.
Lemma 4 TCP-net N conditionally acyclic every semi-directed cycle N ?
conditionally acyclic.
Proof: proof straightforward: variable assignment makes one
semi-directed cycles N ? conditionally directed, cycle need examined.
405

fiBrafman, Domshlak, & Shimony

?>=<
89:;





D1d

89:;
?>=<



D1d

SC


(a)


?>=<
89:;





89:;
?>=<
C

D1d
D1d

SC


(b)

Figure 5: semi-directed cycle: (a) conditionally directed, (b) conditionally acyclic.
Conversely, consider one semi-directed cycles N ? . assignment S(A)
makes conditionally directed, additional assignments variables selector
sets change property.
decomposition presented Lemma 4 allows us prove first complexity result
testing conditional acyclicity. Theorem 2 shows determining TCP-net
conditionally acyclic coNP-hard.
Theorem 2 Given binary-valued TCP-net N , decision problem: conditionally
directed cycle N ? , NP-complete, even every ci-arc N |S()| = 1.
Proof: proof hardness reduction 3-sat. Given 3-cnf formula F,
construct following TCP-net N . every variable Xi every clause Cj F,
construct boolean variable Xi variable Cj N , respectively (we retain
names, simplicity). addition, every clause Cj , construct three boolean variables
Lj,k , 1 k 3, corresponding literals appearing Cj . Let n number
clauses F. TCP-net N somewhat degenerate, since cp-arcs. However,

i-arc (Cj , Lj,k ) clause Cj every literal Lj,k Cj . addition,
every literal Lj,k Cj , ci-arc (Lj,k , C(j+1) mod n ), whose selector variable
variable Xi represented Lj,k . relative importance Lj,k C(j+1) mod n
selector Xi follows: Lj,k positive literal, variable Lj,k important
C(j+1) mod n Xi true, less important Xi false. negative literals,
dependence selector variable reversed. completes construction - clearly
polynomial-time operation. Figure 6 illustrates subnet N corresponding clause
Cj = (x1 x2 x3 ), Lj,1 , Lj,2 , Lj,3 correspond x1 , x2 , x3 , respectively.
claim N ? , dependency graph network N constructed,
conditionally directed cycle F satisfiable5 . easy see path
Cj C(j+1) mod n values variables participating Cj
Cj satisfied. Thus, assignment creates directed path C0 C0
5. particular construction, directed edges N ? outgoing selector variables Xi
effect existence conditionally directed cycles N ? . Therefore, simply consider
TCP-net N instead dependency graph N ? .

406

fiTCP-Nets

ONML
HIJK
Lj,1
;;
B
;;


;;




X1

;;

;;


;;

X
2
PQRS
HIJK
WVUT
PQRS
/ ONML
Cj+1
Lj,2
Cj
WVUT
::

::

::


::

::
X3

::



HIJK
ONML
Lj,3

GFED
@ABC
ONML
HIJK
@ABC
GFED
@ABC
GFED
Lj,1
X
X
X1
r 2
sy 3
;;
B
r


r


r

;



;;
rr

ssyy
;;
rrr sssysyy

r

r
;; rr

sss yy
;r;rr

sss yyy
r

;

r
r
; sss yyy

ys yy
xrrr
PQRS
WVUT
PQRS
WVUT
HIJK
ONML
/
Cj+1 yyy
Lj,2
Cj
::
yyy
::
yyy

::

::
yyyy

::

::
yy

|yyy
HIJK
ONML
Lj,3

(a)

(b)

Figure 6: (a) TCP-net subnet Cj = (x1 x2 x3 ), (b) dependency graph.
assignment satisfies clauses, problems equivalent - hence decision
problem NP-hard. Deciding existence conditional directed cycle NP: Indeed,
verifying existence semi-directed cycle given assignment S(A) (the union
selector sets ci-arcs A) done polynomial time. Thus, problem
NP-complete.
One reason complexity general problem, emerges proof
Theorem 2, possibility number semi-directed cycles TCP-net dependency graph exponential size network. example, network
reduction 3n semi-directed cycles, due three possible paths generated
subnet depicted Figure 6(a). Thus, natural consider networks
number semi-directed cycles large. follows, call TCP-net N
m-cycle bounded number different semi-directed cycles dependency graph N ?
m.
Lemma 4 follows that, given m-cycle bounded TCP-net N , polynomial
size N , reduce testing conditional acyclicity N ? separate
tests conditional acyclicity every semi-directed cycle N ? . naive check
conditional acyclicity semi-directed cycle requires time exponential size S(A)
S(A) union selector sets ci-arcs A. Thus, S(A) small
semi-directed cycle N ? , conditional acyclicity N ? checked quickly.
fact, often determine semi-directed cycle conditionally directed/acyclic
even efficiently enumerating possible assignments S(A).
Lemma 5 Let semi-directed cycle N ? . conditionally acyclic, contains
pair ci-arcs , j S(i ) S(j ) 6= .
words, selector sets ci-arcs pairwise disjoint,
conditionally directed. Thus, Lemma 5 provides necessary condition conditional
acyclicity checked time polynomial number variables.
407

fiBrafman, Domshlak, & Shimony

Proof (Lemma 5) selector sets ci-arcs pairwise disjoint, trivially
exists assignment S(A) orienting ci-arcs one direction.
developing sufficient conditions conditional acyclicity, let us introduce
useful notation. First, given ci-arc = (X, ), say assignment w subset
0 S() orients rows CIT () consistent w express relative
importance X , any. words, w orients if, given w, relative
importance X independent S() \ 0 . Second, semi-directed cycle
contains directed edges, refer (by definition, unique) direction
direction A.
Lemma 6 semi-directed cycle conditionally acyclic contains pair ci-arcs , j
either:
(a) contains directed edges, every assignment w S(i ) S(j ), either j
oriented w direction opposite direction A.
(b) edges undirected, every assignment w S(i ) S(j ), j
oriented w opposite directions respect A.

Proof:

Follows immediately conditions lemma.

Lemma 6 provides sufficient condition conditional acyclicity
checked time exponential maximal size selector set intersection pair
ci-arcs A. Note size TCP-net least large exponential
term, description CIT exponential size corresponding
selector set. Thus, checking condition linear size network.
Definition 12 Given semi-directed cycle A, let shared(A) denote union pairwise
intersections selector sets ci-arcs A:
[
shared(A) =
S(i ) S(j )
,j

Lemma 7
(a) semi-directed cycle contains directed edges, conditionally acyclic
if, assignment u shared(A), exists ci-arc u oriented
u direction opposite direction A.
(b) semi-directed cycle contains ci-arcs, conditionally acyclic
if, assignment u shared(A), exist two ci-arcs u1 , u2 oriented
u opposite directions respect A.

Proof: sufficiency condition clear, since subsumes condition
Lemma 6. Thus, left proving necessity. start second case
408

fiTCP-Nets

contains ci-arcs. Assume contrary conditionally acyclic,
exists assignment u shared(A) pair ci-arcs oriented u
opposite directions respect A.
ci-arc A, let () = S() \ shared(A). Consider following disjoint
partition = Aiu Aci
u induced u A: ci-arc A, u orients ,
Aiu . Otherwise, direction independent () given u,
Aci
u . make two observations:
1. initial (contradicting) assumption implies (now directed) edges Aiu
agree direction respect A.
2. ci-arc () = , Aiu , since
selectors instantiated u.
Aci
u = , first observation trivially contradicts initial assumption
conditionally acyclic. Alternatively, Aci
u 6= , then, definition shared(A),


(i ) (j ) = pair ci-arcs , j Aci
u . means
assign (non-empty, second observation) (i ) independently,
thus extend u assignment S(A) orient ci-arcs Aci
u either
direction Aiu Aiu 6= , arbitrary joint direction Aiu = . Again,
contradicts assumption conditionally acyclic. Hence, proved
condition necessary second case. proof first case contains
directed edges similar.
general, size shared(A) O(|V|). Since check set assignments
shared(A), implies problem may hard. Theorem 3 shows
indeed case.
Theorem 3 Given binary-valued, 1-cycle bounded TCP-net N , decision problem:
conditionally directed cycle N ? , NP-complete, even every ci-arc N
|S()| 3.
Proof: proof hardness reduction 3-sat. Given 3-cnf formula F,
construct following TCP-net N . every variable Xi every clause Cj F,
construct boolean variables Xi Cj N , respectively. addition, add single dummy

variable C, i-arc (C, C1 ). Let n number clauses F. 1 j n 1,
n 1 ci-arcs Ej = (Cj , Cj+1 ). addition, ci-arc En = (Cn , C).
1 j n, CIT Ej determined clause Cj , follows. selector set Ej
set variables appearing Cj , relative importance variables
Ej determined follows: Cj less important Cj+1 values
variables selector set Cj false. (For j = n, read C instead Cj+1 ).
constructed TCP-net N 1-cycle bounded, one semi-directed
cycle dependency graph N ? , namely C, C1 , . . . , Cn , C. claim semidirected cycle conditionally directed F satisfiable. easy see
directed path C C exists ci-arcs directed Cj Cj+1 ,
occurs exactly variable assignment makes clause Cj satisfiable. Hence,
409

fiBrafman, Domshlak, & Shimony

directed cycle occurs N exactly assignment makes clauses satisfiable,
making two problems equivalent. Thus decision problem NP-hard. Finally,
deciding existence conditional directed cycle NP (see proof Theorem 3),
problem NP-complete.
Observe proof Theorem 3 work size selector
sets bounded 2, 2-sat P. immediate question whether
latter case problem becomes tractable, binary-valued TCP-nets answer
affirmative.
Theorem 4 Given binary-valued, m-cycle bounded TCP-net N , polynomial
size N and, every ci-arc N |S()| 2, decision problem:
conditional directed cycle N ? , P.

Proof: proof uses reduction conditional acyclicity testing satisfiability. Let
semi-directed cycle |S()| k every ci-arc A. reduce conditional
acyclicity testing problem equivalent k-sat problem instance. particular, since 2sat solvable linear time (Even, Itai, & Shamir, 1976), together Lemma 4
proves claim.
First, assume least one directed edge (either i-arc cp-arc). definition
semi-directed cycles, directed edges point direction, specifying
possible cyclic orientation A. ci-arc A, let selector set
S(i ) = {Xi,1 , ..., Xi,k }.6 Clearly, conditionally directed ci-arcs
directed consistently .
Given semi-directed cycle A, create corresponding k-cnf formula F,
F satisfiable conditionally directed. Let us call CIT (i ) entries
consistent term -entries. Since S() = {Xi,1 , ..., Xi,k } N binary
valued, represent non- entries CIT (i ) conjunction disjunctions,
i.e., CNF form. number disjunctions equal number non- entries
CIT (i ), disjunction comprised k literals. Thus, representation
CIT (i ) k-CNF formula, size linear size CIT (i ). (In fact, size
resulting formula sometimes significantly smaller, one frequently simplify
component CNF fragments, property needed here.)
Finally, compose CNF representations CIT (i ), every A, resulting
k-CNF formula size linear combined number table entries. construction
F clearly linear-time operation. Likewise, easy see F satisfiable
assignment S(A) converting directed cycle.
minor unresolved issue semi-directed cycles consisting ci-arcs only. Given
semi-directed cycle A, reduce problem two sub-problems directed
arc. Let A0 A00 cycles created inserting one dummy variable one i-arc
clockwise A0 , counter-clockwise A00 . Now, conditionally directed
either A0 A00 (or both) conditionally directed.
6. |S(i )| < k, impact compact reduction below.

410

fiTCP-Nets

summarize analysis verifying conditional acyclicity, one must first identify
semi-directed cycles dependency graph TCP-net. Next, one must show
given assignment w importance-conditioning variables semi-directed
cycle, w-directed graph acyclic. problem coNP-hard general networks7 ,
interesting classes networks tractable. case
number semi-directed cycles large either size shared(A)
cycle size selector set large. Note practice, one
would expect small selector sets statements X important
= B = b . . . Z = z appear complex
one would expect hear. Thus, Lemma 6, Lemma 7 (for semi-directed cycles small
shared(A)), Theorem 4 theoretical interest. Naturally, extending
toolbox TCP-net subclasses efficiently tested consistency clearly
theoretical practical interest.

6. Reasoning Conditionally Acyclic TCP-nets
automated consistency verification core part preference elicitation stage,
efficiency reasoning user preferences one main desiderata model
preference representation. particular importance task preference-based
optimization constrained optimization, discuss first part section.
Another important task, provides important component algorithm
constrained optimization present, outcome comparison discussed second part
section.
6.1 Generating Optimal Assignments
Following notation Boutilier et al. (2004a), x assignments disjoint
subsets X variable set V, respectively, denote combination x
xy. X = X = V, call xy completion assignment x, denote
Comp(x) set completions x.
One central properties original CP-net model (Boutilier et al., 2004a)
that, given acyclic CP-net N (possibly empty) partial assignment x
variables, simple determine outcome consistent x (a completion x)
preferentially optimal respect N . corresponding linear time forward sweep
procedure follows: Traverse variables topological order induced N ,
set unassigned variable preferred value given parents values.
immediate observation procedure works also conditionally
acyclic TCP-nets: relative importance relations play role case,
network traversed according topological order induced CP-net part
given TCP-net. fact, Corollary 1 holds TCP-net directed cycles
consisting cp-arcs.
Corollary 1 Given conditionally acyclic TCP-net (possibly empty) partial assignment x
variables, forward sweep procedure constructs preferred outcome Comp(x).
7. actually means network large, probably solve reasonable
amount time.

411

fiBrafman, Domshlak, & Shimony

strong computational property outcome optimization respect acyclic CPnets (and conditionally acyclic TCP-nets) hold TCP-net variables
constrained set hard constraints, C. case, determining set preferentially non-dominated8 feasible outcomes trivial. acyclic CP-nets, branch
bound algorithm determining optimal feasible outcomes introduced Boutilier,
Brafman, Domshlak, Hoos, Poole (2004b). algorithm important anytime
property outcome added current set non-dominated outcomes,
never removed. important implication property first generated assignment satisfies set hard constraints also preferentially non-dominated.
words, finding one non-dominated solution algorithm boils solving
underlying CSP certain variable value ordering strategies.
develop extension/modification algorithm Boutilier et al. (2004b)
conditionally acyclic TCP-nets. extended algorithm Search-TCP retains anytime property shown Figure 7. key difference processing acyclic
CP-net conditionally acyclic TCP-net semantics former implicitly induces single partial order importance variables (where node precedes descendants) (Boutilier et al., 2004a), semantics latter induces
hierarchically-structured set partial orders: partial order corresponds
single assignment set selector variables network, or, specifically,
certain w-directed graph.
Formally, problem defined conditionally acyclic TCP-net Norig , set
hard constraints Corig , posed variables Norig . Search-TCP algorithm (depicted
Figure 7) recursive, recursive call receives three parameters:
1. TCP-net N , subnet original conditionally acyclic TCP-net Norig ,
2. set C hard constraints among variables N , subset original
set constraints Corig obtained restricting Corig variables N ,
3. assignment K variables Norig N . follows, refer
assignment K context.
initial call Search-TCP done Norig , Corig , {}, respectively.
Basically, Search-TCP algorithm starts empty set solutions, gradually
extends adding new non-dominated solutions Corig . stage algorithm,
current set solutions serves lower bound future candidates; new candidate
point compared solutions generated point. candidate
dominated member current solution set, added set.
Search-TCP algorithm guided graphical structure Norig . proceeds
assigning values variables top-down manner, assuring outcomes
generated order satisfies (i.e., consistent with) N . recursive call
Search-TCP procedure TCP-net N , eliminated variable X one root
variables N (line 1). Recall that, Lemma 3, conditional acyclicity N guarantees
existence root variable X. values X considered according
8. outcome said non-dominated respect preference order set outcomes
o0 o0 o.

412

fiTCP-Nets

Search-TCP (N , C, K)

Input: Conditionally acyclic TCP-net N ,
Hard constraints C variables N ,
Assignment K variables Norig \ N .
Output: Set all, non-dominated w.r.t. N , solutions C.

1. Choose variable X s.t. cp-arc hY, Xi,

i-arc (Y, X), (X, ) N .
2. Let x1 . . . xk total order D(X) consistent preference
ordering D(X) assignment P a(X) K.
3. Initialize set local results R =
4. (i = 1; k; + +)
5. X = xi
6. Strengthen constraints C X = xi obtain Ci
7. Cj Ci j < Ci inconsistent
8.
continue next iteration
else
9.
Let K0 partial assignment induced X = xi Ci
10.
Ni = Reduce (N ,K0 )
11.
Let Ni1 , . . . , Nim components Ni , connected
either edges Ni constraints Ci .
12.
(j = 1; j m; j + +)
13.
Rji = Search-TCP(Nij , Ci , K K0 )
14.
Rji 6= j
15.
foreach K0 R1i Rm

0
16
K 6 K holds o0 R add R
17. return R

Figure 7: Search-TCP algorithm conditionally acyclic TCP-net based constrained
optimization.

preference ordering induced D(X) assignment provided context K
P a(X) (where P a(X) defined respect Norig ). Note K necessarily contains
assignment P a(X) since X root variable currently considered subnet N
Norig . additional variable assignment X = xi converts current set constraints
C strictly non-weaker constraint set Ci . result propagation X = xi ,
values variables (at least, value X) fixed automatically, partial
assignment K0 extends current context K recursive processing next variable.
Reduce procedure, presented Figure 8, refines TCP-net N respect K0 :
variable assigned K0 , reduce CPTs CITs involving
variable, remove variable network. reduction CITs may remove
conditioning relative importance variables, thus convert ci-arcs
413

fiBrafman, Domshlak, & Shimony

i-arcs, and/or remove ci-arcs completely. main point that, contrast
CP-nets, pair X values xi , xj , variable elimination orderings processing
networks Ni Nj , resulting propagating Ci Cj , respectively, may disagree
ordering variables.
Reduce (N , K0 )
1. foreach {X = xi } K0

2. foreach cp-arc hX, N
3.
Restrict CPT rows dictated X = xi .
4. foreach ci-arc = (Y1 , Y2 ) N s.t. X S()
5.
Restrict CIT rows dictated X = xi .
6.
if, given restricted CIT , relative importance
Y1 Y2 independent S(),
7.
CIT empty
8.
Replace corresponding i-arc.
9.
else Remove .
10. Remove N edges involving X.
11. return N .

Figure 8: Reduce procedure.
partial assignment K0 causes current CP-net become disconnected
respect edges network inter-variable hard constraints,
connected component invokes independent search (lines 11-16).
optimization variables within component independent variables outside
component. addition, strengthening set constraints C X = xi Ci
(line 6), pruning takes place search tree (lines 7-8): set constraints
Ci strictly restrictive set constraints Cj = C {X = xj }
j < i, search X = xi continued. reason pruning
shown feasible outcome involving X = xi dominated (i.e., less
preferable than) feasible outcome b involving X = xj thus cannot
set non-dominated solutions original set constraints9 . Therefore, search
depth-first branch-and-bound, set non-dominated solutions generated far
proper subset required set non-dominated solutions problem,
thus corresponds current lower bound.
potentially non-dominated solutions particular subgraph returned
assignment X = xi , solution compared non-dominated solutions
involving preferred (in current context K) assignments X = xj , j < (line 16).
solution X = xi added set non-dominated solutions current
subgraph context passes non-domination test. semantics
9. pruning introduced Boutilier et al. (2004b) acyclic CP-nets, remains valid
way conditionally acyclic TCP-nets. proof soundness pruning technique
refer reader Lemma 2 (Boutilier et al., 2004b).

414

fiTCP-Nets

TCP-nets, given context K, solution involving X = xi preferred
solution involving X = xj , j < i. Thus, generated global set R never shrinks.
Theorem 5 Given conditionally acyclic TCP-net N set hard constraints C
variables N , outcome belongs set R generated algorithm Search-TCP
consistent C, outcome o0 consistent C
N |= o0 o.
Proof: Let RC desired set preferentially non-dominated solution C.
prove theorem, show that:
1. Completeness: preferentially non-dominated solution C pruned out, is,
R RC ,
2. Soundness: resulting set R contains preferentially dominated solution C,
is, R RC .
(1) solutions C pruned Search-TCP two places, namely search
space pruning lines 7-8, non-dominance test step line 16. first case,
correctness pruning technique used lines 7-8 given Lemma 2 (Boutilier
et al., 2004b), thus pruning violate completeness Search-TCP.
second case, explicitly generated solution rejected due failure nondominance test, 6 RC apparent since rejection based presenting
concrete solution o0 N |= o0 o. Hence, R RC .
(2) show R RC enough prove newly generated solution cannot dominate
existing solution, is, added generated set solutions o0
case N |= o0 . proof induction number problem
variables. First, claim trivially holds one-variable TCP-net, order
solutions examined line 16 coincides total order selected single
variable network line 2. Now, assume claim holds conditionally
acyclic TCP-nets fewer n variables. Let N TCP-net n variables, C
set hard constraints variables, X root variable N selected
line 1. Let R = {o1 , . . . , } output Search-TCP N C,
elements R numbered according order non-dominance examination
line 16. Now, assume exists pair assignments oi , oj R, < j, yet
N |= oj oi .
First, suppose oi oj provide value X, oi = xl o0i
oj = xl o0j , xl D(X). case, however, o0i o0j belong output
recursive call Search-TCP Nl Cl , thus, inductive hypothesis, o0i
o0j preferentially incomparable. Likewise, Nl obtained line 10 reducing N
respect xl , thus variables Nl preferentially independent X. Hence,
preferential incomparability o0i o0j implies preferential incomparability oi oj ,
thus N |= oj oi cannot case.
Alternatively, suppose oi oj provide two different values X, oi = xl o0i
oj = xm o0j , xl , xm D(X), D(X) numbered according total ordering
415

fiBrafman, Domshlak, & Shimony

values selected line 2. Observe that, construction Search-TCP, < j trivially
implies l < m. However, using arguments identical constructive proof
Theorem 6, exists least one preference order complete assignments
variables N oi oj . Hence, cannot case N |= oj oi ,
thus contradiction assumption N |= oj oi complete.
Note that, interested getting one non-dominated solution given set
hard constraints (which often case), output first feasible outcome generated
Search-TCP. comparisons pairs outcomes required
nothing compare first generated solution. However, interested getting
all, even non-dominated solutions, efficiency preferential comparison
pairs outcomes becomes important factor entire complexity
Search-TCP algorithm. Hence, next section consider preferential comparisons
closely.
6.2 Dominance Testing TCP-nets
One fundamental queries preference-representation formalism whether
outcome dominates (i.e., strictly preferred to) outcome o0 . discussed
above, dominance queries required whenever wish generate one
non-dominated solution set hard constrains. Much like CP-nets, dominance
query hN , o, o0 respect TCP-net treated search improving
flipping sequence (purported) less preferred outcome o0 (purported)
preferred outcome sequence successively preferred outcomes,
flip sequence directly sanctioned given TCP-net. Formally,
improving flipping sequence context TCP-nets defined follows:
Definition 13 sequence outcomes
o0 = o0 o1 om1 om =
improving flipping sequence respect TCP-net N if, 0 < m,
either
1. (CP-flips) outcome oi different outcome oi+1 value exactly one
variable Xj , oi [j] oi+1 [j] given (identical) values P a(Xj ) oi oi+1 ,

2. (I-flips) outcome oi different outcome oi+1 value exactly two
variables Xj Xk , oi [j] oi+1 [j] oi [k] oi+1 [k] given (identical) values
P a(Xj ) P a(Xk ) oi oi+1 , Xj Xk given RI(Xj , Xk |Z)
(identical) values Z oi oi+1 .10
Clearly, value flip flipping sequence sanctioned TCP-net N ,
CP-flips exactly flips allowed CP-nets (Boutilier et al., 2004a).
10. implicitly assumed neither node parent other. implicit consequence
standard semantics conditional preferences node important children. Thus,
need specify explicitly.

416

fiTCP-Nets

Theorem 6 Given TCP-net N pair outcomes o0 , N |= o0
improving flipping sequence respect N o0 o.
Proof:
= Given improving flipping sequence F:
o0 = o0 o1 om1 om =
o0 respect N , Definition 13, N |= oi oi+1 improving
flip F. proposition follows transitivity preferential entailment
respect TCP-nets (Lemma 1).
= Let G graph preferential ordering induced N , i.e., nodes G stand
outcomes, directed edge o1 o2 improving
CP-flip I-flip o1 o2 , sanctioned N . Clearly, directed paths G equivalent
improving flipping sequences respect N .
First, show preference ordering respects paths G (that is,
path o1 o2 G, o2 o1 ) satisfies N . Assume
contrary respects paths G, satisfy N . Then, definition
satisfiability (Definition 7), must exist either:
1. variable X, assignment p D(P a(X)), values x, x0 D(X), assignment
w remaining variables W = V (X P a(X)), pxw px0 w,
CP (X) dictates x0 x given p,
2. importance arc pair variables X , assignment z D(S())
(if i-arc, S() = ), values x, x0 D(X), y, 0 D(Y ), assignment w
remaining variables W = V ({X, } S()), pxyw px0 0 w,
(i) CP (X) dictates x0 x, (ii) (possibly empty) CIT dictates
X given z.

However, first case, N specifies x0 x given p, CP-flip px0 w pxw,
contradicting fact extends G. Similarly, second case, N specify x0 x
given w, X given z, I-flip px0 0 w pxyw, contradicting
fact extends G.
Now, construction G, improving flipping sequence o0
o, directed path G o0 o. Therefore, exist preference
ordering respecting paths G o0 o. However, based
observation preference orderings respecting paths G, also satisfies N ,
implies N 6|= o0 .
Various methods used search flipping sequence. particular, believe least techniques, developed task respect CPnets Domshlak Brafman (2002), Domshlak (2002), Boutilier et al. (2004a)
applied TCP-net model issue left future research. However,
general, dominance testing respect CP-nets (and thus TCP-nets) known
417

fiBrafman, Domshlak, & Shimony

NP-hard (Boutilier et al., 2004a), thus practice one may possibly consider performing
approximate constrained optimization, using Search-TCP algorithm dominance
testing based one tractable refinements TCP-nets discussed
Brafman, Domshlak, Kogan (2004a).

7. Discussion
CP-nets (Boutilier et al., 1999, 2004a) relatively new graphical model representation
reasoning preferences. development, however, already stimulated research
several directions (e.g., see (Brafman & Chernyavsky, 2005; Brafman & Dimopoulos, 2004;
Brewka, 2002; Boutilier et al., 2001; Domshlak et al., 2003; Rossi et al., 2004; Lang, 2002;
Wilson, 2004b, 2004a)). paper introduced qualitative notions absolute
conditional relative importance pairs variables extended CP-net
model capture corresponding preference statements. extended model called
TCP-nets. identified wide class TCP-nets satisfiable, notably class
conditionally acyclic TCP-nets, analyzed complexity algorithms testing membership class networks. also studied reasoning TCP-nets, focusing
outcome optimization conditionally acyclic TCP-nets without hard constraints.
work opens several directions future research. First, important open theoretical question precise complexity dominance testing TCP-nets. context
CP-nets problem studied Domshlak (2002), Boutilier et al. (2004a),
Goldsmith et al. (2005). Another question consistency TCP-nets
conditionally acyclic. preliminary study issue context cyclic CP-nets
done Domshlak Brafman (2002) Goldsmith et al. (2005).
growing research preference modeling motivated need preference
elicitation, representation, reasoning techniques diverse areas AI user-centric
information systems. particular, one main application areas mind
automatic personalized product configuration (Sabin & Weigel, 1998). Thus,
remaining part section, first consider process preference elicitation
TCP-nets, listing practical challenges addressed make
process appealing users en-masse. Then, relate work approaches
preference-based optimization.
7.1 Preference Elicitation TCP-nets (and Logical Models
Preference)
process preference elicitation known complex account taken
formal model users preferences also numerous important factors
human-computer interaction (e.g., see (Faltings, Pu, Torrens, & Viappiani, 2004; Pu &
Faltings, 2004)). paper focus formalism structuring analyzing
users preferences, although (probably offline) applications, formalism could
actually used drive input process, much like Bayes network used help
experts express beliefs.
Depending application, schematic process constructing TCP-net would
commence asking decision maker identify variables interest, presenting
user, fixed. example, application CP-net adaptive
418

fiTCP-Nets

document presentation (Domshlak, Brafman, & Shimony, 2001; Brafman, Domshlak, &
Shimony, 2004b), content provider chooses set content elements, correspond
set variables. online shopper-assistant agent, variables likely
fixed (e.g., agent online PC customizer) (Brafman et al., 2004a). Next,
user asked consider variable, value variables influence
preferences values variable. point cp-arcs CPTs introduced.
Next, user asked consider relative importance relations, ci-arcs
added. ci-arc, corresponding CIT filled.
Clearly, one may prefer keep preference elicitation process user-driven, allowing user simply provide us set preference statements. set
statements fits language expressible TCP-nets model, specific TCPnet underlying statements constructed simple analysis referents
conditionals statements. TCP-net extraction statements
simpler statements provided another formal language, obtained
via carefully designed, structured user interface. However, user obviously
natural provide statements natural language. Hence, interesting practical question related elicitation qualitative preferences model acquisition speech
and/or text (Asher & Morreau, 1995; Glass, 1999; Bethard, Yu, Thornton, Hatzivassiloglou,
& Jurafsky, 2004). Observe intuitiveness qualitative preferential statements
closely related fact straightforward representation natural language everyday life. addition, collections typical preferential statements seem
form linguistic domain priori constrained special manner. may
allow us develop specialized techniques tools understanding corresponding
language. offline online language understanding considered, since user
either describe preferences offline, self-contained text, asked online,
part interactive process (possibly mixed) preference elicitation preference-based
constrained optimization.
Yet another possible approach eliciting TCP-nets, well alternative logical
models preferences, would allow user expressing pair-wise comparisons
completely specified choices, construct TCP-net consistent input.
scope quantitative models preference representation, example-based
model generation adopted numerous user-centric optimization systems (e.g.,
see (Linden, Hanks, & Lesh, 1997; Blythe, 2002).) However, devising framework
learning qualitative models preference seems somewhat challenging. theory,
nothing prevents us adopting example-based generation TCP-nets since latter
seen compact representation preference relation space choices.
question, however, whether reasonably small set pair-wise choice comparisons
provide us sufficient basis learning TCP-net consistent
training examples, compact TCP-net generalize justifiable manner
beyond provided examples. best knowledge, far question
studied logical preference-representation models, hence clearly poses
challenging venue future research.11
11. Note that, interested compact modeling pair-wise comparisons choices,
numerous techniques area machine learning found useful. instance, one
learn decision tree classifying ordered pairs choices preferred (first choice second choice)

419

fiBrafman, Domshlak, & Shimony

7.2 Related Work
show Section 6, extending CP-nets TCP-nets appealing mainly scope
decision scenarios space syntactically possible choices (either explicitly
implicitly) constrained hard constraints. review related approaches
preference-based optimization appeared literature.
primary example preference-based optimization soft-constraints formalism
(e.g., see Bistarelli et al. (1997)), developed model constraint satisfaction problems
either over-constrained (and thus unsolvable according standard meaning
satisfaction) (Freuder & Wallace, 1992), suffer imprecise knowledge actual
constraints (Fargier & Lang, 1993). formalism, constrained optimization problem
represented set preference orders assignments subsets variables, together
operator combining preference relations subsets variables
preference relation assignments whole set variables. subset
variables corresponds soft constraint satisfied different extent
different variable assignments. much flexibility local preference
orders specified, combined. Various soft-constraints models,
weighted (Bistarelli et al., 1999), fuzzy (Schiex, 1992), probabilistic (Fargier & Lang,
1993), lexicographic (Fargier et al., 1993) CSPs, discussed literature soft
constraint satisfaction.
conceptual difference approach soft-constraints formalism
latter based tightly coupled representation preferences constraints,
representation two paradigms completely decoupled. Informally, soft
constraints machinery developed optimization partial constraint satisfaction,
dealing optimization face constraint satisfaction. instance,
personalized product configuration, two parties involved typically: manufacturer consumer. manufacturer brings forth product expertise,
set hard constraints possible system configurations operating environment.
user expresses preferences properties final product. Typically numerous
configurations satisfy production constraints, manufacturer strives provide
user maximal satisfaction finding one preferred, feasible product
configuration. naturally leads decoupled approach.
Freuder OSullivan (2001) proposed framework interactive sessions overconstrained problems. session, constraint solver discovers
solution current set constraints available, user asked consider tradeoffs. example, following Freuder OSullivan (2001), suppose set user
requirements photo-camera configuration tool weight camera
less 10 ounces zoom lens least 10X. camera meets
requirements, user may specify tradeoffs increase weight limit 14
ounces, zoom lens least 10X (possibly using suggestions automatically generated tool). turn, tradeoffs used refining current
set requirements, system goes new constraint satisfaction process.
preferred. However, classification guarantee general resulting binary
relation space choices anti-symmetric assumption preference transitivity (a
joint property considered extremely natural literature preference structures (Hansson,
2001).)

420

fiTCP-Nets

tradeoffs exploited Freuder OSullivan (2001) correspond information
captured TCP-nets i-arcs. However, instead treating information
incremental compromising update set hard constraints done Freuder
OSullivan (2001), TCP-net based constrained optimization presented Section 6,
exploit information guide constraint solver preferable feasible solutions.
hand, motivation ideas behind work Freuder OSullivan
well works interactive search (see works, e.g., interactive goal
programming (Dyer, 1972), interactive optimization based example critique (Pu &
Faltings, 2004)) open venue future research interactive preference-based constrained
optimization TCP-nets, elicitation user preferences interleaved
search feasible solution.
notion lexicographic orders/preferences (Fishburn, 1974; Schiex et al., 1995;
Freuder et al., 2003) closely related notion importance. idea lexicographic ordering often used qualitative approaches multi-criteria decision making.
Basically, implies one item better another important (lexicographically earlier) criteria differ, considered better overall, regardless
poorly may criteria. Thus, four criteria (or attributes)
A, B, C, D, thus ordered, well miserably B, C D, whereas o0
slightly worse much better criteria, still deemed better. terms
notion variable importance, lexicographic ordering attributes denotes special
form relative importance attribute versus set attributes. Thus, example
above, important B, C combined; B important C
combined, C important D. note Wilson (2004b) provides
nice language capture statements more. Wilson allows statements
form = preferred = all-else-being-equal, except B C. is, given
two outcomes differ A, B C only, one assigns preferred
one assigns a0 , regardless value B C outcomes. Hence,
richer language particular capture lexicographic preferences.
believe lexicographic ordering attributes typically strong,
think flexibility provided Wilsons language could quite useful. However,
one starts analyzing relationships sets attributes, utility graphical models
analysis power becomes questionable. Indeed, aware graphical
analysis Wilsons approach, except special case covered TCP-nets. Moreover,
intuition relative importance sets notion users much less
comfortable specifying many applications. However, hypothesis requires empirical
verification, well general study exact expressive power TCP-nets, i.e.,
characterizing partial orders expressible using language. believe
important avenue future research.
Acknowledgments
Ronen Brafman Solomon Shimony partly supported Paul Ivanier Center
Robotics Production Management. Ronen Brafman partly supported NSF
grants SES-0527650 IIS-0534662. Ronen Brafmans permanent address is: Department
Computer Science, Ben Gurion University, Israel.
421

fiBrafman, Domshlak, & Shimony

References
Asher, N., & Morreau, M. (1995). generic sentences mean. Carlson, G., &
Pelletier, F. J. (Eds.), Generic Book, pp. 300338. Chicago University Press.
Bethard, S., Yu, H., Thornton, A., Hatzivassiloglou, V., & Jurafsky, D. (2004). Automatic
extraction opinion propositions holders. Proceedings AAAI
Spring Symposium Exploring Attitude Affect Text: Theories Applications.
Bistarelli, S., Fargier, H., Montanari, U., Rossi, F., Schiex, T., & Verfaillie, G. (1999).
Semiring-based CSPs valued CSPs: Frameworks, properties, comparison.
Constraints, 4 (3), 275316.
Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint solving
optimization. Journal ACM, 44 (2), 201236.
Blythe, J. (2002). Visual exploration incremental utility elicitation. Proceedings
National Conference Artificial Intelligence (AAAI), pp. 526532.
Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: directed graphical
representation conditional utilities. Proceedings Seventeenth Conference
Uncertainty Artificial Intelligence, pp. 5664.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004a). CP-nets: tool
representing reasoning conditional ceteris paribus preference statements.
Journal Artificial Intelligence Research (JAIR), 21, 135191.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004b). Preference-based
constrained optimization CP-nets. Computational Intelligence (Special Issue
Preferences AI CP), 20 (2), 137157.
Boutilier, C., Brafman, R., Hoos, H., & Poole, D. (1999). Reasoning conditional ceteris
paribus preference statements. Proceedings Fifteenth Annual Conference
Uncertainty Artificial Intelligence, pp. 7180. Morgan Kaufmann Publishers.
Brafman, R., & Chernyavsky, Y. (2005). Planning goal preferences constraints.
Proceedings International Conference Automated Planning Scheduling,
pp. 182191, Monterey, CA.
Brafman, R., & Domshlak, C. (2002). Introducing variable importance tradeoffs CPnets. Proceedings Eighteenth Annual Conference Uncertainty Artificial
Intelligence, pp. 6976, Edmonton, Canada.
Brafman, R., Domshlak, C., & Kogan, T. (2004a). Compact value-function representations
qualitative preferences. Proceedings Twentieth Annual Conference
Uncertainty Artificial Intelligence, pp. 5158, Banff, Canada.
Brafman, R., Domshlak, C., & Shimony, S. E. (2004b). Qualitative decision making
adaptive presentation structured information. ACM Transactions Information
Systems, 22 (4), 503539.
Brafman, R. I., & Dimopoulos, Y. (2004). Extended semantics optimization algorithms
cp-networks. Computational Intelligence (Special Issue Preferences AI
CP), 20 (2), 218245.
422

fiTCP-Nets

Brafman, R. I., & Friedman, D. (2005). Adaptive rich media presentations via preferencebased constrained optimization. Proceedings IJCAI-05 Workshop Advances Preference Handling, pp. 1924, Edinburgh, Scotland.
Brewka, G. (2002). Logic programming ordered disjunction. Proceedings
Eighteenth National Conference Artificial Intelligence, pp. 100105, Edmonton,
Canada. AAAI Press.
Burke, R. (2000). Knowledge-based recommender systems. Kent, A. (Ed.), Encyclopedia
Library Information Systems, Vol. 69, pp. 180200. Marcel Dekker, New York.
Domshlak, C. (2002). Modeling Reasoning Preferences CP-nets. Ph.D.
thesis, Ben-Gurion University, Israel.
Domshlak, C., & Brafman, R. (2002). CP-nets - reasoning consistency testing.
Proceedings Eighth International Conference Principles Knowledge Representation Reasoning, pp. 121132, Toulouse, France.
Domshlak, C., Brafman, R., & Shimony, S. E. (2001). Preference-based configuration
web page content. Proceedings Seventeenth International Joint Conference
Artificial Intelligence, pp. 14511456, Seattle.
Domshlak, C., Rossi, F., Venable, K. B., & Walsh, T. (2003). Reasoning soft
constraints conditional preferences: Complexity results approximation techniques. Proceedings Eighteenth International Joint Conference Artificial
Intelligence, pp. 215220, Acapulco, Mexico.
Dyer, J. S. (1972). Interactive goal programming. Management Science, 19, 6270.
Even, S., Itai, A., & Shamir, A. (1976). complexity timetable multicommodity
flow problems. SIAM Journal Computing, 5, 691703.
Faltings, B., Pu, P., Torrens, M., & Viappiani, P. (2004). Designing example-critiquing interaction. Proceedings International Conference Intelligent User Interfaces,
pp. 2229, Funchal, Madeira, Portugal.
Fargier, H., & Lang, J. (1993). Uncertainty constraint satisfaction problems: probabilistic approach. Proceedings European Conference Symbolic Qualitative
Approaches Reasoning Uncertainty, Vol. 747 LNCS, pp. 97104.
Fargier, H., Lang, J., & Schiex, T. (1993). Selecting preferred solutions fuzzy constraint
satisfaction problems. Proceedings First European Congress Fuzzy
Intelligent Technologies, pp. 11281134.
Fishburn, P. (1974). Lexicographic orders, utilities, decision rules: survey. Management Science, 20 (11), 14421471.
French, S. (1986). Decision Theory. Halsted Press, New York.
Freuder, E., & OSullivan, B. (2001). Generating tradeoffs interactive constraint-based
configuration. Proceedings 7th International Conference Principles
Practice Constraint Programming, pp. 590594, Paphos, Cyprus.
Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58, 2170.
423

fiBrafman, Domshlak, & Shimony

Freuder, E. C., Wallace, R. J., & Heffernan, R. (2003). Ordinal constraint satisfaction.
Proceedings Fifth International Workshop Soft Constraints.
Glass, J. (1999). Challenges spoken dialogue systems. Proceedings IEEE ASRU
Workshop, Keystone, CO.
Goldsmith, J., Lang, J., Truszczynski, M., & Wilson, N. (2005). computational complexity dominance consistency CP-nets. Proceedings Nineteenth International Joint Conference Artificial Intelligence, pp. 144149, Edinburgh, Scotland.
Haag, A. (1998). Sales configuration business processes. IEEE Intelligent Systems
Applications, 13 (4), 7885.
Hansson, S. O. (2001). Preference logic. Gabbay, D. M., & Guenthner, F. (Eds.),
Handbook Philosophical Logic (2 edition)., Vol. 4, pp. 319394. Kluwer.
Keeney, R. L., & Raiffa, H. (1976). Decision Multiple Objectives: Preferences Value
Tradeoffs. Wiley.
Lang, J. (2002). preference representation combinatorial vote. Proceedings
Eight International Conference Principles Knowledge Representation
Reasoning (KR), pp. 277288.
Linden, G., Hanks, S., & Lesh, N. (1997). Interactive assessment user preference models:
automated travel assistant. Proceedings Sixth International Conference
User Modeling, pp. 6778.
Pu, P., & Faltings, B. (2004). Decision tradeoff using example critiquing constraint
programming. Constraints: International Journal, 9 (4), 289310.
Resnick, P., & Varian, H. R. (Eds.). (1997). Special Issue Recommender Systems, Vol. 40
Communications ACM.
Rossi, F., Venable, K. B., & Walsh, T. (2004). mCP nets: Representing reasoning
preferences multiple agents. Proceedings Nineteenth National Conference
Artificial Intelligence, pp. 729734, San Jose, CL.
Sabin, D., & Weigel, R. (1998). Product conguration frameworks - survey. IEEE Intelligent Systems Applications, 13 (4), 4249.
Schiex, T. (1992). Possibilistic cosntraint satisfaction, handle soft constraints.
Proceedings Eighth Conference Uncertainty Artificial Intelligence, pp. 269
275.
Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued constraint satisfaction problems: Hard
easy problems. Proceedings Fourteenth International Joint Conference
Artificial Intelligence, pp. 631637.
Wilson, N. (2004a). Consistency constrained optimisation conditional preferences.
Proceedings Sixteenth European Conference Artificial Intelligence, pp.
888894, Valencia.
Wilson, N. (2004b). Extending CP-nets stronger conditional preference statements.
Proceedings Nineteenth National Conference Artificial Intelligence, pp.
735741, San Jose, CL.
424

fiJournal Artificial Intelligence Research 25 (2006) 529-576

Submitted 4/05; published 4/06

Asynchronous Partial Overlay: New Algorithm Solving
Distributed Constraint Satisfaction Problems
Roger Mailler

mailler@ai.sri.com

SRI International
333 Ravenswood Dr
Menlo Park, CA 94025 USA

Victor R. Lesser

lesser@cs.umass.edu

University Massachusetts, Department Computer Science
140 Governors Drive
Amherst, 01003 USA

Abstract
Distributed Constraint Satisfaction (DCSP) long considered important
problem multi-agent systems research. many real-world problems
represented constraint satisfaction problems often present
distributed form. article, present new complete, distributed algorithm called
asynchronous partial overlay (APO) solving DCSPs based cooperative mediation process. primary ideas behind algorithm agents, acting
mediator, centralize small, relevant portions DCSP, centralized subproblems overlap, agents increase size subproblems along critical paths
within DCSP problem solving unfolds. present empirical evidence shows
APO outperforms known, complete DCSP techniques.

1. Introduction
Distributed constraint satisfaction problem become useful representation
used describe number problems multi-agent systems including distributed
resource allocation (Conry, Kuwabara, Lesser, & Meyer, 1991) distributed scheduling
(Sycara, Roth, Sadeh, & Fox, 1991). researchers cooperative multi-agent systems
focused developing methods solving problems based one key assumption. Particularly, agents involved problem solving process autonomous.
means agents willing exchange information directly relevant
shared problem retain ability refuse solution obviously conflicts
internal goal.
researchers believe focus agent autonomy precludes use centralization forces agents reveal internal constraints goals
may, reasons privacy pure computational complexity, impossible achieve.
Several algorithms developed explicit purpose allowing agents
retain autonomy even involved shared problem exhibits
interdependencies. Probably best known algorithms fit description
found work Yokoo et al. form distributed breakout (DBA) (Yokoo &
Hirayama, 1996), asynchronous backtracking (ABT) (Yokoo, Durfee, Ishida, & Kuwabara,
1992), asynchronous weak-commitment (AWC) (Yokoo & Hirayama, 2000).
c
2006
AI Access Foundation. rights reserved.

fiMailler & Lesser

Unfortunately, common drawback algorithms effort
provide agents complete privacy, algorithms prevent agents
making informed decisions global effects changing local allocation, schedule, value, etc. example, AWC, agents try value wait another agent
tell work nogood message. this, agents never
learn true reason another agent set agents unable accept value,
learn value combination values doesnt work.
addition, techniques suffer complete distribution control. words, agent makes decisions based incomplete often inaccurate
view world. result leads unnecessary thrashing problem
solving agents trying adapt behavior agents,
turn trying adapt them. Pathologically, behavior counter-productive
convergence protocol(Fernandez, Bejar, Krishnamachari, Gomes, & Selman, 2003).
iterative trial error approach discovering implicit implied constraints
within problem causes agents pass exponential number messages actually reveals great deal information agents constraints domain values
(Yokoo, Suzuki, & Hirayama, 2002). fact, order complete, agents using AWC
willing reveal shared constraints domain values. key thing
note statement AWC still allows agents retain autonomy
even forced reveal information variables constraints form
global constraint network.
paper, present cooperative mediation based DCSP protocol, called Asynchronous Partial Overlay (APO). Cooperative mediation represents new methodology
lies somewhere centralized distributed problem solving uses
dynamically constructed, partial centralization. allows cooperative mediation based
algorithms, like APO, utilize speed current state-of-the-art centralized solvers
taking advantage opportunities parallelism dynamically identifying relevant
problem structure.
APO works agents asynchronously take role mediator. agent
acts mediator, computes solution portion overall problem recommends value changes agents involved mediation session. If, result
recommendations, causes conflicts agents outside session, links
preventing repeating mistake future sessions.
Like AWC, APO provides agents great deal autonomy allowing anyone
take mediator notice undesirable state current
solution shared problem. adding autonomy, agents also ignore
recommendations changing local solution made agents. similar
way AWC, APO sound complete agents willing reveal
domains constraints shared variables allows agents obscure
states, domains, constraints strictly local variables.
rest article, present formalization DCSP problem (section
2). section 3, describe underlying assumptions motivation work.
present APO algorithm (section 4.1) give example protocols
execution simple 3-coloring problem (section 4.2). go give proofs
soundness completeness algorithm (section 4.3). section 5, present
530

fiAsynchronous Partial Overlay: New Algorithm DCSP

results extensive testing compares APO AWC within distributed graph
coloring domain complete compatibility version SensorDCSP domain (Bejar,
Krishnamachari, Gomes, & Selman, 2001) across variety metrics including number
cycles, messages, bytes transmitted, serial runtime. cases, show
APO significantly outperforms AWC (Yokoo, 1995; Hirayama & Yokoo, 2000). Section
6 summarizes article discusses future research directions.

2. Distributed Constraint Satisfaction
Constraint Satisfaction Problem (CSP) consists following:

set n variables V = {x1 , . . . , xn }.

discrete, finite domains variables = {D1 , . . . , Dn }.

set constraints R = {R1 , . . . , Rm } Ri (di1 , . . . , dij ) predicate
Cartesian product Di1 Dij returns true iff value assignments
variables satisfies constraint.

problem find assignment = {d1 , . . . , dn |di Di }
constraints R satisfied. CSP shown NP-complete, making form
search necessity.
distributed case, DCSP, using variable-based decomposition, agent assigned one variables along constraints variables. goal
agent, local perspective, ensure constraints variables
satisfied. Clearly, agents goal independent goals agents
system. fact, simplest cases, goals agents strongly
interrelated. example, order one agent satisfy local constraints, another
agent, potentially directly related constraint, may change value
variable.
article, sake clarity, restrict case agent
assigned single variable given knowledge constraints variable.
Since agent assigned single variable, refer agent name
variable manages. Also, restrict considering binary constraints
form Ri (di1 , di2 ). Since APO uses centralization core, easy
see algorithm would work restrictions removed. point
discussed part algorithm description section 4.1.4.
Throughout article, use term constraint graph refer graph formed
representing variables nodes constraints edges. Also, variables neighbors
variables shares constraints.
531

fiMailler & Lesser

3. Assumptions Motivation
3.1 Assumptions
following assumptions made environments agents
protocol designed:
1. Agents situated, autonomous, computing entities. such, capable
sensing environment, making local decisions based model intentionality, acting decisions. Agents rationally resource bounded.
result, agents must communicate gain information others state,
intentions, decisions, etc.
2. Agents within multi-agent system share one joint goals. paper,
goal Boolean nature stemming DCSP formulation.
3. work focuses cooperative problem solving, agents cooperative.
necessarily imply share state, intentions, etc.
agents, are, degree, willing exchange information solve joint
goals. also imply change intentions, state, decisions
based demands another agent. Agents still maintain autonomy
ability refuse revise decisions agents based local state,
intentions, decisions, etc.
4. agent capability computing solutions joint goal based
potentially limited rationality. follows naturally ability agents
make decisions, i.e., every agent capable computing solution
portion joint goal based desires.
3.2 Motivation Mediation-Based Problem Solving
Websters dictionary defines act mediating follows:
Mediate: 1. act intermediary; especially work opposing sides
order resolve (as dispute) bring (as settlement). 2. bring
about, influence, transmit acting intermediate controlling agent
mechanism. (Merriam-Webster, 1995)
definition, mediation implies degree centralizing shared problem
order group individuals derive conflict free solution. Clearly situations
participants willing (cooperative), mediation powerful paradigm solving
disputes. rather strange, considering this, little done looking
mediation cooperative method solving DCSPs.
Probably, earliest mediation-based approach solving conflicts amongst agents
airspace management application(Cammarata, McArthur, & Steeb, 1983). work
investigates using various conflict resolution strategies deconflict airspace distributed
air traffic control system. author proposes method solving disputes
involved agents elect leader solve problem. elected, leader becomes
532

fiAsynchronous Partial Overlay: New Algorithm DCSP

S1

S2

{1,2}

{1,2}

Figure 1: simple distributed problem two variables.
responsible recognizing dispute, devising plan correct it, acting
plan. Various election schemes tested, unfortunately, leader authority
modify actions order resolve conflicts. obviously leads situations
plan suboptimal.
(Hayden, Carrick, & Yang, 1999), authors describe mediation one number possible coordination mechanisms. work, mediator acts intermediary
agents also act coordinate behavior. intermediary, mediator routes messages, provides directory services, etc. provides loose coupling
agents, since need know mediator. mediator also act
coordinate agents behavior tight interdependencies.
research work mediation-based problem solving involved settling disputes
competitive semi-competitive agents. Probably one best examples
using mediation manner found PERSUADER system(Sycara, 1988).
PERSUADER designed settle conflicts adversarial parties involved
labor dispute. PERSUADER uses case-based reasoning suggest concessions order
converge satisfactory solution. Another example using mediation way
found system called Designer Fabricator Interpreter (DFI) (Werkman, 1990).
DFI, mediation used resolve conflicts one series problem solving steps.
Whenever first step fails, case iterative negotiation, mediator agent steps
tries convince agents relax constraints. fails, mediator mandates
final solution.
may several reasons mediation deeply explored
cooperative problem solving method. First, researchers focused strongly using
distributed computing way exploiting concurrency distribute computation
needed solve hard problems (Rao & Kumar, 1993). this, even partially
and/or temporarily centralizing sections problem viewed contradictory
central goal. Second, researchers often claimed part power
distributed methods lies ability techniques solve problems naturally
distributed. example, supply chain problems generally central monitoring
authority. Again, directly sharing reasons particular choice made form
constraint seem contradict use distributed methods. Lastly, researchers often
claim reasons privacy security problem solved distributed
fashion. Clearly, sharing information solve problem compromises agents ability
private and/or violates security manner.
Although, parallelism, natural distribution, security, privacy, may seem like good
justifications entirely distributed problem solving, actuality, whenever problem
interdependencies distributed problem solvers, degree centralization
information sharing must take place order derive conflict-free solution.
533

fiMailler & Lesser

Consider, simple example, problem figure 1. figure, two problem
solvers, one variable, share common goal different value one
another. agents two allowable values: {1, 2}. Now, order solve
problem, agent must individually decide different value
agent. this, least, one agent must transmit value other.
this, removes half privacy (by revealing one possible values), eliminates
security (because agent could make send values telling
value good), partially centralizes problem solving (agent S2 compute
solutions based solution S1 presented decide problem solved agent
S1 relies S2 solve it.) even simple example, achieving totally distributed
problem solving impossible.
fact, look details current approaches solving DCSPs,
observe significant amount centralization occurring. approaches
perform centralization incrementally problem solving unfolds attempt
restrict amount internal information shared. Unfortunately, problems
interdependencies among problem solvers, revealing agents information (such
potential values variables) unavoidable. fact, solutions derived
one agents conceals information regarding shared constraint
variable based incomplete information therefore may always sound.
follows then, since cannot avoid amount centralization, mediation
natural method solving problems contain interdependencies among distributed
problem solvers.

4. Asynchronous Partial Overlay
cooperative mediation based protocol, key ideas behind creation APO
algorithm
Using mediation, agents solve subproblems DCSP using internal search.
local subproblems overlap allow rapid convergence
problem solving.
Agents should, time, increase size subproblem work along
critical paths within CSP. increases overlap agents ensures
completeness search.
4.1 Algorithm
Figures 2, 3, 4, 5, 6 present basic APO algorithm. algorithm works constructing good list maintaining structure called agent view. agent view
holds names, values, domains, constraints variables agent linked.
good list holds names variables known connected owner
path constraint graph.
problem solving unfolds, agent tries solve subproblem centralized within good list determine unsolvable indicates entire global
problem over-constrained. this, agents take role mediator attempt
534

fiAsynchronous Partial Overlay: New Algorithm DCSP

procedure initialize
di random Di ;
pi sizeof (neighbors) + 1;
mi true;
mediate false;
add xi good list;
send (init, (xi , pi , di , mi , Di , Ci )) neighbors;
initList neighbors;
end initialize;
received (init, (xj , pj , dj , mj , Dj , Cj ))
Add (xj , pj , dj , mj , Dj , Cj ) agent view;
xj neighbor xk good list
add xj good list;
/ good list
add xl agent view xl
connected good list;
pi sizeof (good list);
end if;
xj
/ initList
send (init, (xi , pi , di , mi , Di , Ci )) xj ;
else
remove xj initList;
check agent view;
end do;
Figure 2: APO procedures initialization linking.

change values variables within mediation session achieve satisfied
subsystem. cannot achieved without causing violation agents outside
session, mediator links agents assuming somehow related
mediators variable. process continues one agents finds unsatisfiable
subsystem, conflicts removed.
order facilitate problem solving process, agent dynamic priority
based size good list (if two agents sized good list
tie broken using lexicographical ordering names). Priorities used
agents decide mediates session conflicts arises. Priority ordering
important two reasons. First, priorities ensure agent knowledge
gets make decisions. improves efficiency algorithm decreasing
effects myopic decision making. Second, priorities improve effectiveness
mediation process lower priority agents expect higher priority agents mediate.
improves likelihood lower priority agents available mediation
request sent.
535

fiMailler & Lesser

received (ok?, (xj , pj , dj , mj ))
update agent view (xj , pj , dj , mj );
check agent view;
end do;
procedure check agent view
initList 6= mediate 6=false
return;
m0i hasConf lict(xi );
m0i j (pj > pi mj = = true)
(d0i Di ) (d0i agent view conflict)
di conflicts exclusively lower priority neighbors
di d0i ;
send (ok?, (xi , pi , di , mi )) xj agent view;
else
mediate;
else mi 6= m0i
mi m0i ;
send (ok?, (xi , pi , di , mi )) xj agent view;
end if;
end check agent view;

Figure 3: procedures local resolution, updating agent view
good list.

4.1.1 Initialization (Figure 2)
startup, agents provided value (they pick randomly one isnt
assigned) constraints variable. Initialization proceeds
agents send init message neighbors. initialization message includes
variables name (xi ), priority (pi ), current value(di ), agents desire mediate (mi ),
domain (Di ), constraints (Ci ). array initList records names agents
initialization messages sent to, reason become immediately
apparent.
agent receives initialization message (either initialization
later link request), records information agent view adds
variable good list can. variable added good list
neighbor another variable already good list. ensures graph created
variables good list always remains connected, focuses agents internal
problem solving variables knows interdependency with. initList
checked see message link request response link request.
agent initList, means message response, agent removes
536

fiAsynchronous Partial Overlay: New Algorithm DCSP

procedure mediate
pref erences ;
counter 0;
xj good list
send (evaluate?, (xi , pi )) xj ;
counter ++;
end do;
mediate true;
end mediate;
receive (wait!, (xj , pj ))
update agent view (xj , pj );
counter - -;
counter == 0 choose solution;
end do;
receive (evaluate!, (xj , pj , labeled Dj ))
record (xj , labeled Dj ) preferences;
update agent view (xj , pj );
counter - -;
counter == 0 choose solution;
end do;
Figure 4: procedures mediating APO session.
name initList nothing further. agent initList
means request, response init generated sent.
important note agents contained good list subset
agents contained agent view. done maintain integrity good list
allow links bidirectional. understand point, consider case
single agent repeatedly mediated extended local subproblem long
path constraint graph. so, links agents may limited
view therefore unaware indirect connection mediator. order
link bidirectional, receiver link request store name
requester agent view, cannot add good list path
identified. seen section 4.3, bi-directionality links important ensure
protocols soundness.
4.1.2 Checking agent view (Figure 3)
initialization messages received, agents execute check agent view
procedure (at end figure 2). procedure, current agent view (which contains
assigned, known variable values) checked identify conflicts variable
owned agent neighbors. If, check (called hasConflict
537

fiMailler & Lesser

procedure choose solution
select solution using Branch Bound search that:
1. satisfies constraints agents good list
2. minimizes violations agents outside session
satisfies constraints
broadcast solution;
xj agent view
xj pref erences
d0j violates xk xk
/ agent view
send (init, (xi , pi , di , mi , Di , Ci )) xk ;
add xk initList;
end if;
send (accept!, (d0j , xi , pi , di , mi )) xj ;
update agent view xj
else
send (ok?, (xi , pi , di , mi )) xj ;
end if;
end do;
mediate false;
check agent view;
end choose solution;
Figure 5: procedure choosing solution APO mediation.
figure), agent finds conflict one neighbors told
higher priority agent want mediate, assumes role mediator.
agent tell higher priority agent wants mediate flag
mentioned previous section. Whenever agent checks agent view recomputes
value flag based whether existing conflicts neighbors.
flag set true indicates agent wishes mediate given
opportunity. mechanism acts like two-phase commit protocol, commonly seen
database systems, ensures protocol live-lock dead-lock free.
agent becomes mediator, first attempts rectify conflict(s)
neighbors changing variable. simple, effective technique prevents
mediation sessions occurring unnecessarily, stabilizes system saves messages time. mediator finds value removes conflict, makes change
sends ok? message agents agent view. cannot find nonconflicting value, starts mediation session. ok? message similar init
message, contains information priority, current value, etc. variable.
4.1.3 Mediation (Figures 4, 5, 6)
complex certainly interesting part protocol mediation.
previously mentioned section, agent decides mediate conflict
538

fiAsynchronous Partial Overlay: New Algorithm DCSP

received (evaluate?, (xj , pj ))
mj true;
mediate == true k (pk > pj mk = = true)
send (wait!, (xi , pi ));
else
mediate true;
label Di names agents
would violated setting di d;
send (evaluate!, (xi , pi , labeled Di ));
end if;
end do;
received (accept!, (d, xj , pj , dj , mj ))
di d;
mediate false;
send (ok?, (xi , pi , di , mi )) xj agent view;
update agent view (xj , pj , dj , mj );
check agent view;
end do;
Figure 6: Procedures receiving APO session.

one neighbors expecting session request higher priority
agent. mediation starts mediator sending evaluate? messages
agents good list. purpose message two-fold. First, informs
receiving agent mediation begin tries obtain lock
agent. lock, referred mediate figures, prevents agent engaging
two sessions simultaneously local value change course
session. second purpose message obtain information agent
effects making change local value. key point. obtaining
information, mediator gains information variables constraints outside
local view without directly immediately link agents. allows
mediator understand greater impact decision also used determine
extend view makes final decision.
agent receives mediation request, responds either wait!
evaluate! message. wait message indicates requester agent
currently involved session expecting request agent higher priority
requester, fact could itself. agent available, labels
domain elements names agents would conflict
asked take value. information returned evaluate! message.
size evaluate! message strongly related number variables
size agents domain. cases either extremely large, number
techniques used reduce overall size message. example techniques
539

fiMailler & Lesser

include standard message compression, limiting domain elements returned
ones actually create conflict simply sending relevant value/variable pairs
mediator actually labeling. fact means largest evaluate! message
ever actually needed polynomial number agents (O(V )). implementation,
graph coloring, largest possible evaluate! message O(|D | + |V |).
noted agents need return names
privacy security reasons. effects completeness algorithm,
completeness relies one agents eventually centralizing entire
problem worst case. mentioned section 3.2, whenever agent attempts
completely hide information shared variable constraint distributed problem,
completeness necessarily effected.
mediator received either wait! evaluate! message agents
sent request to, chooses solution. mediator determines received
responses using counter variable set size good list
evaluate? messages first sent. mediator receives either wait!
evaluate! message, decrements counter. reaches 0, agents
replied.
Agents sent wait! message dropped mediation agents
sent evaluate! message labeled domains specified message recorded
used search process. mediator uses current values along
labeled domains received evaluate! messages conduct centralized search.
Currently, solutions generated using Branch Bound search (Freuder & Wallace,
1992) constraints good list must satisfied number outside
conflicts minimized. similar min-conflict heuristic (Minton,
Johnston, Philips, & Laird, 1992). Notice although search takes variables
constraints good list consideration, solution generates may adhere
variable values agents dropped session. variables
actually considered outside session impact able change
values calculated part min-conflict heuristic. causes search consider
current values dropped variables weak-constraints final solution.
addition, domain variables good list ordered
variables current value first element. causes search use current
value assignments first path search tree tendency minimize
changes made current assignments. heuristics, combined together,
form lock key mechanism simultaneously exploits work previously
done mediators acts minimize number changes assignments.
presented section 5, simple feed-forward mechanisms, combined
limited centralization needed solve problems, account considerable improvements
algorithms runtime performance.
satisfying assignments found search, mediator announces
problem unsatisfiable algorithm terminates. solution chosen,
accept! messages sent agents session, who, turn, adopt proposed
answer.
mediator also sends ok messages agents agent view,
whatever reason session. simply keeps agents agent views up540

fiAsynchronous Partial Overlay: New Algorithm DCSP

to-date, important determining solution reached. Lastly, using
information provided evaluate! messages, mediator sends init messages
agent outside agent view, caused conflict choosing
solution. linking step extends mediators view along paths likely
critical solving problem identifying over-constrained condition. step also
ensures completeness protocol.
Although termination detection explicitly part APO protocol, technique
similar (Wellman & Walsh, 1999) could easily added detect quiescence amongst
agents.
4.1.4 Multiple Variables n-ary Constraints
Removing restrictions presented section 2 fairly straightforward process.
APO uses linking part problem solving process, working n-ary constraints
simply involves linking n agents within constraint initialization
post-mediation linking needs occur. Priorities scheme identical
used binary constraints.
Removing single agent per variable restriction also difficult fact
one strengths approach. using spanning tree algorithm initialization,
agents quickly identify interdependencies internal variables
use create separate good lists disconnected components
internal constraint graph. essence, startup, agents would treat
decomposed problems separate problem, using separate flag, priority,
good list, etc. problem solving unfolds, agent discovers connections
internal variables (through external constraints), decomposed problems could
merged together utilize single structure information.
technique advantages able ensure consistency dependent internal variables attempting mediate (because local checking
mediation), allows agent handle independent variables separate problems.
Using situation aware technique one shown yield best results
previous work(Mammen & Lesser, 1998). addition, technique allows agents
hide variables strictly internal. pre-computation decomposed
problems, agents construct constraints encapsulate subproblems
n-ary constraints n number variables external links. derived constraints sent part init message whenever agent receives
link request one external variables.
4.2 Example
Consider 3-coloring problem presented figure 7. problem, 8 agents,
variable 12 edges constraints them. 3-coloring
problem, variable assigned one three available colors {Black, Red,
Blue}. goal find assignment colors variables two
variables, connected edge, color.
example, four constraints violation: (ND0,ND1), (ND1,ND3), (ND2,ND4),
(ND6,ND7). Following algorithm, upon startup agent adds
541

fiMailler & Lesser

Figure 7: example 3-coloring problem 8 nodes 12 edges.
good list sends init message neighbors. Upon receiving messages,
agents add neighbors good list able identify
shared constraint themselves.
startup completed, agents checks agent view.
agents, except ND5, find conflicts. ND0 (priority 3) waits ND1
mediate (priority 5). ND6 ND7, priority 4, wait ND4 (priority 5, tie
ND3 broken using lexicographical ordering). ND1, equal number agents
good list, lower lexicographical order, waits ND4 start mediation. ND3,
knowing highest priority amongst neighbors, first checks see resolve
conflict changing value, case, cannot. ND3 starts session
involves ND1, ND5, ND6, ND7. sends evaluate? message. ND4
highest priority amongst neighbors, unable resolve conflict locally, also
starts session sending evaluate? messages ND1, ND2, ND6, ND7.
agents mediation receives evaluate? message, first
check see expecting mediation higher priority agent. case,
ND1, ND6, ND7 expecting ND4 tell ND3 wait. label
domain elements names variables would conflict
result adopting value. information sent evaluate! message.
following labeled domains agents sent ND4:
ND1 - Black causes conflicts; Red conflicts ND0 ND3; Blue conflicts
ND2 ND4
ND2 - Black causes conflicts; Red conflicts ND0 ND3; Blue conflicts
ND4
ND6 - Black conflicts ND7; Red conflicts ND3; Blue conflicts ND4
ND7 - Black conflicts ND6; Red conflicts ND3; Blue conflicts ND4

542

fiAsynchronous Partial Overlay: New Algorithm DCSP

Figure 8: state sample problem ND3 leads first mediation.
following responses sent ND3:
ND1 - wait!
ND5 - Black causes conflicts; Red conflicts ND3; Blue causes conflicts
ND6 - wait!
ND7 - wait!
responses received, mediators, ND3 ND4, conduct branch
bound searches attempt find satisfying assignment subproblems
minimizes amount conflict would created outside mediation. either
cannot find least one satisfying assignment, broadcasts solution cannot
found.
example, ND3, limited information has, computes satisfying
solution changes color remain consistent would also changed
colors ND6 ND7. Since told ND6 ND7 wait, changes color,
sends accept ! message ND5 ok? messages ND1, ND6 ND7.
information, ND4 finds solution thinks solve subproblem without
creating outside conflicts. changes color red, ND7 blue, ND1 black
leaving problem state shown figure 8.
ND1, ND4, ND5, ND6 ND7 inform agents agent view new
values, check conflicts. time, ND1, ND3, ND6 notice values
conflict. ND3, highest priority, becomes mediator mediates session
ND1, ND5, ND6, ND7. Following protocol, ND3 sends evaluate?
messages receiving agents label respond. following labeled domains
returned:
ND1 - Black conflicts ND3; Red conflicts ND0 ND4; Blue conflicts
ND2
543

fiMailler & Lesser

Figure 9: final solution ND2 leads second mediation.
ND5 - Black conflicts ND3; Red causes conflicts; Blue causes conflicts
ND6 - Black conflicts ND3; Red conflicts ND4; Blue conflicts ND7
ND7 - Black conflicts ND3 ND6; Red conflicts ND4; Blue causes
conflicts
ND3, receiving messages, conducts search finds solution solves
subproblem. chooses change color red. ND1, ND3, ND5, ND6, ND7
check agent view find conflicts. Since, point, none agents
conflict, problem solved (see figure 9).
4.3 Soundness Completeness
section show APO algorithm sound complete.
proofs, assumed communications reliable, meaning message sent
xi xj xj receive message finite amount time. also assume
xi sends message m1 sends message m2 xj , m1 received
m2 . Lastly, assume centralized solver used algorithm sound
complete. prove soundness completeness, helps principal
lemmas established.
Lemma 1 Links bidirectional. i.e. xi xj agent view eventually xj
xi agent view.
Proof:
Assume xi xj agent view xi agent view xj .
order xi xj agent view, xi must received init message
point xj . two cases.
Case 1: xj initList xi . case, xi must sent xj init message
first, meaning xj received init message therefore xi agent view,
contradiction.
544

fiAsynchronous Partial Overlay: New Algorithm DCSP

Case 2: xj initList xi . case, xi receives init message
xj , responds init message. means reliable communication
assumption holds, eventually xj receive xi init message add xi agent view.
Also contradiction.
Lemma 2 agent xi linked xj xj changes value, xi eventually
informed change update agent view.
Proof:
Assume xi value agent view xj incorrect. would mean
point xj altered value without informing xi . two cases:
Case 1: xj know needed send xi update. i.e. xi xj
agent view. Contradicts lemma 1.
Case 2: xj inform agents agent view changes value.
clear code cannot happen. Agents change values
check agent view, choose solution, accept! procedures. cases, informs
agents within agent view either sending ok? accept!
message change value occurred. contradiction.
Lemma 3 xi conflict one neighbors, expect mediation
another higher priority agent agent view, currently session,
act mediator.
Proof:
Directly procedure check agent view.
Lemma 4 xi mediates session solution, constraints
agents involved mediation satisfied.
Proof:
Assume two agents xj xk (either could xi ),
mediated xi mediation conflict xj xk .
two ways could happened.
Case 1: One agents must value xi assign
part mediation.
Assume xj and/or xk value xi assign. know since xi
mediated session including xj xk , xi receive wait! message
either xj xk . means could mediating. also means
must set mediate flags true xi sent evaluate?
message. Since times agent change value mediate flag
false, mediating, told mediator, xj and/or xk could
changed values xi told to, contradicts assumption.
Case 2: xi assigned value caused conflict one another.
Lets assume xi assigned conflicting values. means xi chose
solution take account constraints xj xk . But, know
xi chooses satisfying solutions include constraints
agents good list. leads contradiction.
545

fiMailler & Lesser

lemma important says mediator successfully concluded
session, conflicts exist constraints outside
mediation. viewed mediator pushing constraint violations outside
view. addition, mediators get information violations
pushed establish links agents, time, gain context.
important point considering completeness algorithm.
Theorem 1 APO algorithm sound. i.e. reaches stable state either
found answer solution exists.
Proof:
order sound, agents stop reached answer.
condition would stop without found answer one
agents expecting mediation request higher priority agent
send it. words, protocol deadlocked.
Lets say 3 agents, xi , xj , xk pi < pj pk < pj (i could equal k)
xk conflict xj . two cases xj would mediate session
included xi , xi expecting to:
Case 1: xi mj = true agent view actual value false.
Assume xi mj = true agent view true value mj = false.
would mean point xj changed value mj false without informing
xi . one place xj changes value mj , check agent view
procedure (see figure 3). Note procedure, whenever flag changes value
true false, agent sends ok message agents agent view. Since
lemma 1 know xi agent view xj , xi must received message
saying mj = false, contradicting assumption.
Case 2: xj believes xi mediating xi believe be.
i.e. xj thinks mi = true pi > pj .
previous case, know xj believes mi = true must
case. need show pi < pj . Lets say p0i priority xj believes
xi assume xj believes p0i > pj when, fact pi < pj . means
point xi sent message xj informing current priority p0i . Since
know priorities increase time (the good list gets larger), know
p0i pi (xj always correct value underestimates priority xi ). Since pj > pi
pi p0i pj > p0i contradicts assumption.
also important point considering algorithm behaves. proof
says agents always either know underestimate true value neighbors
priorities. this, agents attempt mediate fact sometimes,
shouldnt. side effect attempt, however, correct priorities
exchanged mistake doesnt get repeated. important thing mention
case priority values become equal. case, tie broken using
alphabetical order names agents. ensures always way
break ties.
Definition 1 Oscillation condition occurs subset V 0 V agents
infinitely cycling allowable values without reaching solution. words,
agents live-locked
546

fiAsynchronous Partial Overlay: New Algorithm DCSP

definition, order considered part oscillation, agent within
subset must changing value (if stable, oscillating) must connected
members subset constraint (otherwise, actually part
oscillation).
Theorem 2 APO algorithm complete. i.e. solution exists, algorithm
find it. solution exist, report fact.
Proof:
solution exist whenever problem over-constrained. problem
over-constrained, algorithm eventually produce good list variables
within associated constraints lead solution. Since subset variables
unsatisfiable, entire problem unsatisfiable, therefore, solution possible.
algorithm terminates failure condition reached.
Since shown Theorem 1 whenever algorithm reaches stable
state, problem solved finds subset variables unsatisfiable terminates, need show reaches one two states finite
time. way agents reach stable state one
agents system oscillation.
two cases consider, easy case single agent oscillating
(|V 0 | = 1) case one agent oscillating (|V 0 | > 1).
Case 1: agent xi caught infinite loop agents
stable.
Lets assume xi infinite processing loop. means matter
changes value to, conflict one neighbors, changed
value something doesnt conflict neighbors, would solution
stop. changes value conflict xj higher priority it,
xj mediate xi , contradicting assumption agents stable.
xi changes value conflict lower priority agent, lemma 3,
act mediator neighbors. Since assumed agents
stable state, agents xi good list participate session
lemma 4, agent xi conflicts removed. means xi
stable state contradicting assumption infinite loop.
Case 2: Two agents oscillation.
Lets say set agents V 0 V oscillation. consider
agent xi within V 0 . know conditions xi changes value
solve conflicts (a contradiction x wouldnt
considered part oscillation), mediator, receiver mediation
agent V 0 . interesting case agent acts mediator.
Consider case xi mediator call set agents mediating
Vi . know according definition 1 mediation, least one
conflict must created remain otherwise oscillation would stop problem
would solved. fact, know remaining conflicts must contain
agent set V 0 Vi lemma 4. also know violated constraint
member Vi , xi link agent part constraints
547

fiMailler & Lesser

member Vi . next time xi mediates, set Vi include members
number agents set V 0 Vi reduced. fact, whenever xi mediates set
V 0 Vi reduced (assuming told wait! one agents.
case, takes longer reduce set, proof still holds). Eventually, O(|V | 2 )
mediations, xi within V 0 must Vi = V 0 (every agent within set must
mediated |V 0 | times order happen). agent mediates push
violations outside set V 0 solve subproblem lemma 4. Either
conditions contradicts oscillation assumption. Therefore, algorithm complete.
QED
fairly clear that, domains exponential, algorithms worsecase runtime exponential. space complexity algorithm is, however, polynomial,
agents retain names, priorities, values, constraints agents.

5. Evaluation
great deal testing evaluation conducted APO algorithm. Almost
exclusively, test done comparing APO algorithm currently fastest
known, complete algorithm solving DCSPs called Asynchronous Weak Commitment
(AWC) protocol. section describe AWC protocol (section 5.1),
describe distributed 3-coloring domain present results extensive testing done
domain (section 5.2). testing compares two algorithms across variety
metrics, including cycle time, number messages, serial runtime.
Next, describe tracking domain (section 5.3) present results testing
domain well. domain, modified core search algorithm APO
take advantage polynomial complexity problem. variant called, APOFlow, also described.
5.1 Asynchronous Weak Commitment (AWC) Protocol
AWC protocol (Yokoo, 1995) one first algorithms used solving DCSPs.
Like APO algorithm, AWC based variable decomposition. Also, like APO, AWC
assigns agent priority value dynamically changes. AWC, however, uses
weak-commitment heuristic (Yokoo, 1994) assign priorities values
gets name.
Upon startup, agents selects value variable sends ok? messages
neighbors (agents shares constraint with). message includes variables
value priority (they start 0).
agent receives ok? message, updates agent view checks
nogood list violated nogoods. nogood composed set nogood pairs
describe combination agents values lead unsatisfiable condition. Initially, nogoods agents nogood list constraints variable.
checking nogood list, agents check violations higher priority nogoods. priority nogood defined priority lowest priority variable
nogood. value greater priority agents variable, nogood
higher priority. Based results check, one three things happen:
548

fiAsynchronous Partial Overlay: New Algorithm DCSP

1. higher priority nogoods violated, agent nothing.
2. higher priority nogoods violated repaired
simply changing agents variable value, agent changes value sends
ok? messages agents agent view. multiple possible
satisfying values, agent chooses one minimizes number violated
lower priority nogoods.
3. violated higher priority nogoods cannot repaired changing
value variable, agent generates new nogood. nogood
previously generated nogood, nothing. Otherwise, sends
new nogood every agent variable contained nogood raises
priority value variable. Finally, changes variable value one causes
least amount conflict sends ok? messages.
Upon receiving nogood message another agent, agent adds nogood
nogood list rechecks nogood violations. new nogood includes names
agents agent view links them. linking step essential
completeness search(Yokoo et al., 1992), causes agents communicate
nogoods ok? messages agents direct neighbors constraint
graph. overall effect increase messages reduction amount
privacy provided agents communicate potential domain values
information constraints exchange ok? nogood messages
larger number agents.
One recent advances AWC protocol addition resolventbased nogood learning (Hirayama & Yokoo, 2000) adaptation classical nogood
learning methods (Ginsberg, 1993; Cha & Iwana, 1996; Frost & Dechter, 1994).
resolvent method used whenever agent finds needs generate new
nogood. Agents generate new nogoods domain values violation
least one higher priority nogood already nogood list. resolvent method
works selecting one higher priority nogoods domain values
aggregating together new nogood. almost identical resolvent
propositional logic referred resolvent-based learning. AWC
protocol used testing incorporates resolvent-based nogood learning.
5.2 Distributed Graph Coloring
Following directly definition CSP, graph coloring problem, also known
k-colorability problem, consists following:
set n variables V = {x1 , . . . , xn }.
set possible colors variables = {D1 , . . . , Dn } Di
exactly k allowable colors.
set constraints R = {R1 , . . . , Rm } Ri (di , dj ) predicate implements equals relationship. predicate returns true iff value assigned
xi differs value assigned xj .
549

fiMailler & Lesser

100

APO
AWC

Cycles

80

60

40

20

0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 10: Comparison number cycles needed solve satisfiable, low-density 3coloring problems various sizes AWC APO.

problem find assignment = {d1 , . . . , dn |di Di }
constraints R satisfied. Like general CSP, graph coloring shown
NP-complete values k > 2.
test APO algorithm, implemented AWC APO algorithms conducted experiments distributed 3-coloring domain. distributed 3-coloring problem 3-coloring problem n variables binary constraints agent
given single variable. conducted 3 sets graph coloring based experiments compare
algorithms computation communication costs.
5.2.1 Satisfiable Graphs
first set experiments, created solvable graph instances = 2.0n (lowdensity), = 2.3n (medium-density), = 2.7n (high-density) according method
presented (Minton et al., 1992). Generating graphs way involves partitioning
variables k equal-sized groups. Edges added selecting two groups
random adding edge random member group. method ensures
resulting graphs satisfiable, also tests limited likely easier
subset possible graphs. tests done traditionally used
researchers DCSPs.
particular values chosen represent three major regions
within phase-transition 3-colorability (Culberson & Gent, 2001). phase transition
CSP defined based order parameter, case average node degree
d. transition occurs point random graphs created order value
yield half satisfiable half unsatisfiable instances. Values order parameter
550

fiAsynchronous Partial Overlay: New Algorithm DCSP

200

APO
AWC

Cycles

150

100

50

0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 11: Comparison number cycles needed solve satisfiable, medium-density
3-coloring problems various sizes AWC APO.

lower transition point (more 50% instance satisfiable) referred
left transition. opposite true values right.
Phase transitions important strongly correlated overall difficulty finding solution graph (Cheeseman, Kanefsky, & Taylor, 1991; Monasson,
Zecchina, Kirkpatrick, Selman, & Troyansky, 1999; Culberson & Gent, 2001). Within
phase transition, randomly created instances typically difficult solve. Interestingly,
problems right left phase transitions tend much easier.
3-colorability, value = 2.0 left phase transition. region,
randomly created graphs likely satisfiable usually easy find solve.
= 2.3, middle phase transition graph 50% chance
satisfiable usually hard solve. = 2.7n, right phase transition,
graphs likely unsatisfiable and, again, also easier solve.
number papers (Yokoo & Hirayama, 2000, 1996; Hirayama & Yokoo, 2000)
reported = 2.7n within critical phase transition 3-colorability. seems
caused misinterpretation previous work area(Cheeseman et al.,
1991). Although Cheeseman, Kanefsky, Taylor reported = 2.7n within
critical region 3-colorability, using reduced graphs analysis.
reduced graph one trivially colorable nodes non-relevant edges
removed. example, one easily remove node two edges
3-coloring problem always trivially colored. Additionally, nodes
possess unique domain element neighbors also easily removed.
later work, Culberson Gent identified critical region approximately
= 2.3 therefore included tests(Culberson & Gent, 2001). One note,
however, phase transitions typically done completely random graphs
551

fiMailler & Lesser



2.0

2.3

2.7

Nodes
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall

APO
Mean
17.82
27.07
39.97
53.24
59.83
80.75

APO
StDev
8.15
17.11
25.79
32.32
35.35
54.30

AWC
Mean
17.38
24.62
43.76
69.96
80.32
82.92

AWC
StDev
12.70
19.23
30.98
49.03
103.76
61.01

15.04
34.01
47.72
92.73
114.02
160.88

5.55
16.81
26.58
72.46
75.84
125.12

20.49
41.30
109.99
135.60
185.84
189.04

11.27
29.58
74.85
146.57
119.94
91.27

13.83
27.28
42.47
52.15
64.54
87.14

3.56
10.10
18.01
23.12
26.26
42.82

20.29
39.99
62.92
86.89
104.09
127.04

10.32
24.08
35.23
43.69
46.62
64.97

p(AW C AP O)
0.77
0.27
0.34
0.01
0.07
0.81
0.01
0.00
0.04
0.00
0.01
0.00
0.07
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 1: Comparison number cycles needed solve satisfiable 3-coloring problems
various sizes densities AWC APO.

552

fiAsynchronous Partial Overlay: New Algorithm DCSP

APO

140

AWC
120

Cycles

100
80
60
40
20
0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 12: Comparison number cycles needed solve satisfiable, high-density
3-coloring problems various sizes AWC APO.

definition involves satisfiable unsatisfiable instances,
hard apply phase-tranisition results graphs created using technique described
beginning section generates satisfiable instances. detailed
phase transition analysis done graph generation technique fact,
believe graphs tend easier randomly created satisfiable ones
size order.
evaluate relative strengths weakness approaches, measured
number cycles number messages used course solving
problems. cycle, incoming messages delivered, agent allowed
process information, messages created processing
added outgoing queue delivered beginning next cycle. actual
execution time given one agent cycle varies according amount work
needed process incoming messages. random seeds used create
graph instance variable instantiation saved used algorithms
fairness.
comparison AWC APO, randomly generated 10 graphs size
n = 15, 30, 45, 60, 75, 90 = 2.0n, 2.3n, 2.7n instance generated 10 initial
variable assignments. Therefore, combination n m, ran 100 trials making
total 1800 trials. results experiment seen figures 10 15
table 1. mention results testing AWC obtained
experiments agree previous results (Hirayama & Yokoo, 2000) verifying correctness
implementation.
first glance, figure 10 appears indicate satisfiable low-density graph instances, AWC APO perform almost identically terms cycles completion. Look553

fiMailler & Lesser

APO

AWC

Nodes
15
30
45
60
75
90
15
30
45
60
75
90

% Links
Mean
32.93
17.24
11.97
9.30
7.42
6.51
55.76
32.89
27.00
26.47
21.99
20.11

% Links
StDev
3.52
2.59
1.84
1.30
0.95
0.96
12.31
8.88
7.89
9.12
7.67
7.89

% Central
Mean
60.53
42.53
33.27
29.00
24.60
24.93
79.73
60.97
56.49
56.98
53.19
50.38

% Central
StDev
9.12
8.06
6.61
7.31
6.73
6.16
11.45
10.98
11.57
14.23
13.29
13.78

Table 2: Link statistics satisfiable, low-density problems.

APO

AWC

Nodes
15
30
45
60
75
90
15
30
45
60
75
90

% Links
Mean
37.46
21.24
14.90
12.85
11.17
10.07
63.19
42.84
52.05
43.69
47.77
44.04

% Links
StDev
3.45
2.85
2.27
2.99
2.45
3.10
12.06
11.83
13.52
14.59
14.10
10.84

% Central
Mean
67.60
51.37
43.67
42.98
43.76
40.47
83.56
72.20
80.67
74.93
79.41
77.98

% Central
StDev
8.26
9.32
11.40
13.06
12.69
14.55
8.96
12.09
11.24
14.56
12.18
10.52

Table 3: Link statistics satisfiable, medium-density problems.

554

fiAsynchronous Partial Overlay: New Algorithm DCSP

APO

AWC

Nodes
15
30
45
60
75
90
15
30
45
60
75
90

% Links
Mean
43.28
24.29
18.07
13.86
11.85
10.86
78.87
58.03
54.06
53.01
49.63
47.72

% Links
StDev
2.86
2.61
2.06
1.77
1.57
1.85
12.29
11.66
13.44
12.77
11.36
13.81

% Central
Mean
74.53
59.30
52.62
47.78
46.37
50.73
93.60
86.27
82.67
83.47
81.91
80.32

% Central
StDev
8.34
9.75
10.21
11.32
11.32
14.48
7.75
9.54
13.34
11.04
9.93
15.06

Table 4: Link statistics satisfiable, high-density problems.
ing associated table (Table 1), however, reveals overall, pairwise T-test
indicates 99% confidence, APO outperforms AWC graphs.
density, average degree, graph increases, difference becomes
apparent. Figures 11 12 show APO begins scale much efficiently
AWC. attributed ability APO rapidly identify strong interdependencies variables derive solutions using centralized search
partial subproblem.
Tables 2 4, partially verify statement. see, average, less
50% possible number links (n (n 1)) used APO solving problems (%
Links column). addition, maximum amount centralization (% Central column)
occuring within single agent (i.e. number agents agent view) remains fairly
low. highest degree centralization occurs small, high-density graphs. Intuitively,
makes lot sense graphs, single node likely high
degree start. Combine fact dynamic priority ordering
result large amounts central problem solving.
profound differences algorithms seen figures 13, 14, 15
table 5. APO uses least order magnitude less messages AWC. Table 6 shows
message savings lead large savings number bytes transmitted
well. Even though APO uses twice many bytes per message AWC (the messages
optimized all), total amount information passed around significantly
less almost every case.
Again, looking linking structure AWC produces gives insights
uses many messages APO. agents communicate
agents linked whenever value changes, large number changes
occur single cycle, AWC tremendous amount thrashing behavior. APO,
hand, avoids problem process mediating implicitly creates
555

fiMailler & Lesser



2.0

2.3

2.7

Nodes
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall

APO
Mean
361.50
1117.15
2078.72
3387.13
4304.22
6742.14

APO
StDev
179.57
844.33
1552.98
2084.69
2651.15
4482.54

AWC
Mean
882.19
2431.71
6926.86
18504.17
21219.01
33125.57

AWC
StDev
967.21
3182.51
7395.22
22281.59
22714.98
39766.56

379.15
1640.08
3299.05
8773.16
14368.87
25826.74

188.69
931.22
2155.45
9613.84
12066.32
29172.66

1205.50
6325.21
44191.89
70104.74
178683.02
201145.37

923.85
6914.79
44693.33
69050.66
173493.21
143236.26

433.64
1623.89
3859.99
5838.36
9507.60
16455.59

164.12
787.59
1921.51
3140.53
4486.04
10679.92

1667.71
9014.02
28964.43
66857.87
116016.71
196239.22

1301.03
8104.34
22900.89
53221.05
82857.63
163722.90

p(AW C AP O)
0.00
0.00
0.00
0.00
0.00
0.00
0.01
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 5: Comparison number messages needed solve satisfiable 3-coloring problems various sizes densities AWC APO.

556

fiAsynchronous Partial Overlay: New Algorithm DCSP



2.0

2.3

2.7

Nodes
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall
15
30
45
60
75
90
Overall

APO
Mean
10560.76
32314.97
59895.10
97126.79
123565.94
192265.35

APO
StDev
4909.09
23138.55
42740.82
57428.70
73493.72
123384.05

AWC
Mean
11590.38
32409.93
95061.77
259529.42
294502.71
466084.60

AWC
StDev
13765.19
45336.65
106391.51
326087.70
328696.59
581963.72

11370.13
47539.20
95098.49
247417.78
401618.24
712035.13

4951.75
25486.74
59312.25
262844.89
327990.65
782835.83

16260.19
88946.59
644007.01
1018059.11
2626178.31
2935211.45

13237.31
101077.04
675192.26
1029273.23
2606377.80
2138087.17

13415.51
48542.24
112541.02
170174.55
272391.95
465571.42

4280.15
21331.96
52729.10
85705.12
122177.07
288265.82

22393.61
125072.76
405535.81
945039.32
1641250.63
2793725.78

18578.25
116518.80
327349.47
773937.60
1204185.34
2397839.47

p(AW C AP O)
0.49
0.98
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 6: Comparison number bytes transmitted APO AWC satisfiable
graph instances various sizes density.

557

fiMailler & Lesser

35000

APO
AWC

30000

Messages

25000
20000
15000
10000
5000
0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 13: Comparison number messages needed solve satisfiable, low-density
3-coloring problems various sizes AWC APO.

250000

APO
AWC

Messages

200000

150000

100000

50000

0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 14: Comparison number messages needed solve satisfiable, mediumdensity 3-coloring problems various sizes AWC APO.

regions stability problem landscape mediator decides solution.
addition, APO uses partial centralization solve problems, avoids use
large number messages discover implied constraints trial error.
558

fiAsynchronous Partial Overlay: New Algorithm DCSP

200000

APO
AWC

Messages

150000

100000

50000

0
10

20

30

40

50

60

70

80

90

100

Variables

Figure 15: Comparison number messages needed solve satisfiable, high-density
3-coloring problems various sizes AWC APO.

% Satisfiable

1
0.8
0.6
0.4
0.2
0
1.8

2

2.2

2.4

2.6

2.8

Density

Figure 16: Phase transition curve 60 node randomly generated graphs used testing.

see next two experiments, high degree centralization caused
unfocused linking degrades AWCs performance even solving randomly
generated, possibly unsatisfiable, graph instances.
559

fiMailler & Lesser

800

APO

700

AWC

Cycles

600
500
400
300
200
100
0
1.8

2

2.2

2.4

2.6

2.8

Density

Figure 17: Number cycles needed solve completely random 60 variable problems
various density using AWC APO.

Density
1.8
2.0
2.1
2.3
2.5
2.7
2.9
Overall

APO
Mean
49.88
88.77
116.79
116.41
56.21
27.62
17.74

APO
StDev
41.98
83.79
107.21
264.65
45.12
25.66
13.69

% APO
Solved
100
100
100
100
100
100
100

AWC
Mean
52.51
189.42
377.54
660.65
640.66
537.99
476.20

AWC
StDev
77.06
237.17
364.55
362.80
335.65
324.87
271.63

% AWC
Solved
100
96
80
55
65
83
92

p(AW C AP O)
0.62
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 7: Number cycles needed solve completely random 60 variable problems
various density using AWC APO.

560

fiAsynchronous Partial Overlay: New Algorithm DCSP

1.4e+06

APO
AWC

1.2e+06

Messages

1e+06
800000
600000
400000
200000
0
1.8

2

2.2

2.4

2.6

2.8

Density

Figure 18: Number messages needed solve completely random 60 variable problems
various density using AWC APO.

5.2.2 Random Graphs
second set experiments, generated completely random 60 node graphs
average degrees = 1.8 2.9. series conducted test completeness
algorithms, verify correctness implementations, study effects
phase transition performance. value d, generated 200 random
graphs single set initial values. Graphs generate randomly choosing
two nodes connecting them. edge already existed, another pair chosen.
phase transition curve instances seen figure 16.
total, 1400 graphs generated tested. Due time constraints, stopped
execution AWC 1000 cycles completed (APO never reached 1000).
results experiments shown figures 17 18 tables 7 8.
graphs, APO significantly outperforms AWC simplest problems
(see figure 17). results directly attributed AWCs poor performance
unsatisfiable problem instances (Fernandez et al., 2003). fact, region
phase transition, AWC unable complete 45% graphs within 1000 cycles.
addition, solve problems, AWC uses least order magnitude
messages APO. results seen figure 18. looking table 9, easy
see occurs. AWC high degree linking centralization. fact,
= 2.9 graphs, AWC reaches average 93% centralization 75% complete
inter-agent linking.
contrast this, APO loose linking throughout entire phase transition
centralizes average around 50% entire problem. results encouraging reinforce idea partial overlays extending along critical paths yields
improvements convergence solutions.
561

fiMailler & Lesser

Density
1.8
2.0
2.1
2.3
2.5
2.7
2.9
Overall

APO
Mean
2822.61
7508.33
12642.68
15614.37
8219.74
4196.58
2736.20

APO
StDev
3040.39
9577.86
16193.56
15761.90
7415.76
4201.80
2286.39

AWC
Mean
12741.58
126658.29
356993.39
882813.45
1080277.25
1047001.18
1000217.83

AWC
StDev
47165.70
269976.18
444899.21
566715.73
661909.63
738367.27
699199.90

p(AW C AP O)
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 8: Number messages needed solve completely random 60 variable problems
various density using AWC APO.

APO

AWC

Density
1.8
2.0
2.1
2.3
2.5
2.7
2.9
1.8
2.0
2.1
2.3
2.5
2.7
2.9

% Links
Mean
8.09
10.93
13.31
15.56
14.37
13.19
13.12
16.28
34.34
46.52
64.13
70.06
72.95
75.19

% Links
StDev
1.50
3.28
4.49
4.94
3.74
3.32
2.70
6.58
13.44
13.95
11.43
10.51
10.65
9.62

% Central
Mean
26.19
36.92
46.68
55.91
53.86
47.33
45.26
41.08
65.00
75.24
86.06
89.46
91.71
92.78

% Central
StDev
6.94
13.77
15.86
18.18
18.25
19.59
17.55
12.42
14.41
10.66
5.61
4.55
3.80
5.63

Table 9: Link statistics 60 node random problems.
5.2.3 Runtime Tests
third set experiments, directly compared serial runtime performance
AWC APO. Serial runtime measured using following formula:

serialtime =

cycles
X

X

i=0 aagents

562

time(a, i)

fiSeconds

Asynchronous Partial Overlay: New Algorithm DCSP

23900
14160
8390
4970
2940
1740
1030
610
360
210
120
70
40
20
10

APO
AWC
Backtracking

15

25

35

45

55

65

Nodes

Seconds

Figure 19: Comparison number seconds needed solve random, low-density 3coloring problems various sizes AWC, APO, centralized Backtracking.

92350
51680
28920
16180
9050
5060
2830
1580
880
490
270
150
80
40
20
10

APO
AWC
Backtracking

15

25

35

45

55

65

Nodes

Figure 20: Comparison number seconds needed solve random, medium-density
3-coloring problems various sizes AWC, APO, centralized Backtracking.

563

fiSeconds

Mailler & Lesser

3620
2360
1540
1000
650
420
270
170
110
70
40
20
10

APO
AWC
Backtracking

15

25

35

45

55

65

Nodes

Figure 21: Comparison number seconds needed solve random, high-density 3coloring problems various sizes AWC, APO, centralized Backtracking.


2.0

2.3

2.7

Nodes
15
30
45
60
15
30
45
60
15
30
45
60

APO
Mean
0.26
0.78
1.27
2.02
0.18
0.98
2.19
10.51
0.09
0.40
0.66
5.47

APO
StDev
0.21
0.66
1.18
1.74
0.23
0.88
2.26
12.97
0.06
0.39
0.63
5.44

AWC
Mean
2.72
10.52
257.88
890.12
3.96
112.95
1236.27
32616.24
3.51
61.43
460.56
4239.16

AWC
StDev
4.40
18.99
1252.25
4288.43
3.23
144.24
1777.51
62111.52
2.90
59.77
690.98
4114.30

BT
Mean
0.02
1.65
245.14
27183.64
0.03
0.86
150.73
92173.71
0.02
0.29
35.58
2997.04

BT
StDev
0.02
4.14
587.31
56053.26
0.01
1.07
241.02
222327.59
0.01
0.36
57.75
4379.97

Table 10: Comparison number seconds needed solve random 3-coloring problems
various sizes densities using AWC, APO, centralized Backtracking.

564

fiAsynchronous Partial Overlay: New Algorithm DCSP

total accumulated runtime needed solve problem one
agent allowed process time.
experiments, generated random graphs, time varying size
density graph. generated 25 graphs values n = 15, 30, 45, 60
densities = 2.0, 2.3, 2.7, total 300 test cases. show performance
difference APO AWC caused speed central solver, ran
centralized backtracking algorithm graph instances. Although, APO uses
branch bound algorithm, backtracking algorithm used test provides best
case lower bound runtime APOs internal solver.
programs used test run identical 2.4GHz Pentium 4
768 Mbytes RAM. machines entirely dedicated tests
minimal amount interference competing processes. addition, computational cost
assigned message passing simulator passes messages cycles.
algorithms were, however, penalized amount time took process messages.
Although realize specific implementation algorithm greatly effect
runtime performance, every possible effort made optimize AWC implementation
used experiments effort fair.
results test series seen figures 19, 20, 21. note
scale used graphs logarithmic. looking results, two
things become apparent. Obviously, first APO outperforms AWC
every case. Second, APO actually outperforms centralized solver graphs larger
45 nodes. indicates two things. First, solver currently APO
poor second APOs runtime performance direct result speed
centralized solver using. fact, tests show improved performance
APO AWC caused APOs ability take advantage problems structure.
replace centralized solver used tests state-of-the-art
solver, would expect two things. first would expect serial runtime
APO algorithm decrease simply speedup caused centralized solver.
second, importantly, centralized solver would always outperform
APO. current CSP solvers take advantage problem structure unlike
solver used tests. way making claim APO improves
centralized solver. simply stating APO outperforms AWC reasons
speed current internal solver.
5.3 Tracking Domain
test APOs adaptability various centralized solvers, created implementation
complete-compatibility version SensorDCSP formulation (Bejar et al., 2001;
Krishnamachari, Bejar, & Wicker, 2002; Fernandez et al., 2003). domain,
number sensors number targets randomly placed within environment.
range restrictions, sensors within distance dist see
target. goal find assignment sensors targets target
three sensors tracking it.
Following directly definition CSP, SensorDCSP problem consists
following:
565

fiMailler & Lesser

Figure 22: example tracking problem. 30 targets (labeled
name) 224 sensors (black dots). Lines connecting sensors targets indicate sensor assigned tracking target.

566

fiAsynchronous Partial Overlay: New Algorithm DCSP

set n targets = {T1 , . . . , Tn }.
set possible sensors see targets = {D 1 , . . . , Dn }.
set constraints R = {R1 , . . . , Rm } Ri (ai , aj ) predicate implements intersects relationship. predicate returns true iff sensors
assigned Ti elements common sensors assigned Tj .
problem find assignment = {a1 , . . . , } constraints
R satisfied ai set |Dci | sensors Di c = min(|Di |, 3).
indicates target requires 3 sensors, enough available, sensors,
less 3.
Since, implementation, sensors compatible one another,
overall complexity problem polynomial, using reduction feasible flow
bipartite graph(Krishnamachari, 2002). this, centralized solver used
APO agents changed modified version Ford-Fulkerson maximum flow
algorithm (Ford & Fulkerson, 1962; Cormen, Leiserson, & Rivest, 1999),
proven run polynomial time.
example tracking problem seen figure 22. example,
224 sensors (black dots) placed ordered pattern environment.
30 targets (labeled names) randomly placed startup. lines
connecting sensors targets indicate sensor assigned target. Note
instance problem satisfiable.
5.3.1 Modifying APO Tracking Domain
tracking domain closely related general CSP formulation,
changes made either AWC APO tests. did, however, decide test
adaptability APO new centralized problem solver. this, changed
centralized problem solver Ford Fulkerson max-flow algorithm figure 23. FordFulkerson works repeatedly finding paths remaining capacity residual
flow network augmenting flows along paths. algorithm terminates
additional paths found. detailed explanation algorithm well proof
optimality found (Cormen et al., 1999).
Like mapping bipartite graphs max-flow, SensorDCSP problem also easily
mapped max-flow. figures 24 25 see mapping simple sensor
allocation problem max-flow problem. Notice capacity flow
sensors targets 1. ensures sensor cannot used
single target. Also, notice capacity targets 3. fact, value
min(|Di |, 3).
use algorithm within APO, mediator simply translates problem
network flow graph G using following rules whenever runs choose solution
procedure figure 5:
1. Add nodes G.
2. Ti add node Ti edge (Ti , t) capacity min(|Di |, 3) G.
567

fiMailler & Lesser

Ford-Fulkerson (G, s, t)
edge (u, v) E[G]
f [u, v] 0;
f [v, u] 0;
end do;
exists path p
residual network Gf
cf (p) min{cf (u, v) : (u, v) p};
edge (u, v) p
f [u, v] f [u, v] + cf (p);
f [v, u] f [u, v];
end do;
end do;
end Ford-Fulkerson;
Figure 23: Ford-Fulkerson maximum flow algorithm.

S1

S2

T1

S3

S4
T2
S5

S6

Figure 24: simple sensor target allocation problem.
3. unique sensor Si domains Ti , add node Si , edge (s, Si )
capacity 1, edge (Si , Ti ) capacity 1 G.

568

fiAsynchronous Partial Overlay: New Algorithm DCSP

S1
1

1
S2

1
1

S3

1
1
1

T1

3




1

1
3

S4
1
S5

1
1

T2

1
S6

Figure 25: flow network simple target allocation problem figure 24.
executes Ford-Fulkerson algorithm. algorithm finishes, mediator
checks residual capacity edges targets t. edges
residual flow, problem unsatisfiable. Otherwise, assignment derived
finding (Si , Ti ) edges flow 1.
One nicest characteristics Ford-Fulkerson algorithm works regardless order paths residual network chosen. implementation,
used breadth-first search which, addition identifying paths residual network, minimized cost path. Cost sense refers amount external
conflict created sensor assigned target. modification maintains
min-conflict heuristic integral part extending mediators local view.
5.3.2 Results
test APO AWC domain, ran test series used 200f 200f
environment 224 sensors placed ordered grid-based pattern. chose place
sensors ordered fashion reduce variance obtained within results. ran
test series varied sensor target ratio 10:1 3.8:1 (22 59 targets)
increments 0.2 across spectrum mostly satisfiable mostly unsatisfiable
instances (see figure 26). conducted 250 trial runs random target placement
values get good statistical sampling.
total, 6750 test cases used. comparison, measured number messages
cycles taken algorithms find solution. random seeds used
place targets saved, APO AWC tested using identical problem
569

fiMailler & Lesser

0.9
0.8

% Satisfiable

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
20

25

30

35

40

45

50

55

60

Targets

Figure 26: Phase transition curve 224 sensor environment used testing.

APO
AWC

Cycles

20

15

10

5

0
20

25

30

35

40

45

50

55

60

Targets

Figure 27: Number cycles needed solve random target configurations field 224
sensors using AWC APO.

instances. correctness algorithms verified cross-checking solutions
(satisfiable/unsatisfiable) obtained tests, matched identically.
seen figure 27 28 tables 11 12, APO outperforms AWC
simplest cases. Part reason minimum 3 cycles takes APO
finish mediation session. problems sparsely connected interdependencies,
cost tends dominate. All-in-all, T-tests indicate, APO significantly better
AWC terms cycles completion number messages used problems
domain.
570

fiAsynchronous Partial Overlay: New Algorithm DCSP

Targets
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
39
40
41
43
45
47
49
51
53
56
59
Overall

APO
Mean
6.36
6.65
7.12
6.55
6.80
7.09
7.38
7.10
7.55
7.18
6.88
7.62
7.47
7.56
8.08
7.48
7.55
6.45
7.45
5.96
4.80
5.15
4.53
3.52
4.12
3.14
3.28

APO
StDev
2.33
3.39
4.72
3.24
4.28
5.02
5.88
4.89
5.99
6.11
6.31
7.59
7.81
7.49
9.89
8.38
10.87
10.54
13.11
7.79
7.25
8.31
5.90
2.00
5.82
0.59
2.26

AWC
Mean
5.88
7.32
8.83
7.15
9.65
10.85
11.05
9.24
13.15
12.42
11.73
12.32
15.88
14.74
15.70
20.70
16.12
15.74
17.56
16.10
17.61
18.52
15.33
14.34
13.13
10.45
7.46

AWC
StDev
6.61
10.55
19.96
10.56
15.78
19.89
15.89
13.76
25.58
22.22
18.21
24.04
28.82
29.01
25.46
39.63
26.43
21.66
30.98
22.91
28.50
30.27
25.28
22.60
22.55
20.81
10.47

p(AW C AP O)
0.29
0.36
0.18
0.39
0.01
0.00
0.00
0.02
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 11: Number cycles needed solve random target configurations field 224
sensors using AWC APO.

571

fiMailler & Lesser

Targets
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
39
40
41
43
45
47
49
51
53
56
59
Overall

APO
Mean
78.28
89.52
105.53
102.92
116.36
128.58
149.23
145.72
167.50
169.30
174.32
212.59
218.74
221.93
258.41
258.95
303.64
293.24
342.33
274.39
277.26
311.91
303.66
269.37
333.42
296.36
339.21

APO
StDev
32.58
54.01
90.36
57.18
86.58
98.63
144.81
93.24
144.27
152.83
152.89
237.60
246.18
230.44
354.13
342.97
501.10
649.42
724.64
267.95
414.19
405.1
299.13
110.57
390.08
45.54
202.98

AWC
Mean
95.68
133.12
184.19
149.18
245.99
263.32
279.49
231.98
378.89
404.40
362.63
410.05
811.58
613.64
671.00
947.95
815.32
884.32
912.65
1279.97
1334.38
1471.82
1487.65
1571.46
1804.47
1895.23
1765.18

AWC
StDev
133.53
237.74
616.36
298.85
566.03
587.30
493.05
335.98
874.99
997.82
570.41
923.25
2243.79
1422.33
1333.50
2116.98
1373.29
1407.99
1517.10
2194.84
2470.13
2172.13
2503.62
2157.52
2815.94
3731.98
3676.16

p(AW C AP O)
0.04
0.00
0.04
0.02
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 12: Number messages needed solve random target configurations targets
field 224 sensors using AWC APO.

572

fiAsynchronous Partial Overlay: New Algorithm DCSP

APO

Messages

2000

AWC

1500

1000

500

0
20

25

30

35

40

45

50

55

60

Targets

Figure 28: Number messages needed solve random target configurations targets
field 224 sensors using AWC APO.

6. Conclusions Future Directions
article, presented new complete, distributed constraint satisfaction protocol
called Asynchronous Partial Overlay (APO). Like AWC, APO allows agents retain
autonomy obscure completely hide internal variables constraints. addition, agents refuse solutions posed mediator, instead taking
mediator reason unhappy proposed solution. also
presented example execution simple problem (section 4.2) proved
soundness completeness algorithm (section 4.3). extensive empirical
testing 10,250 graph instances graph coloring tracking domain, also
showed APO significantly outperforms currently best known distributed constraint
satisfaction algorithm, AWC (Yokoo, 1995). tests shown APO better
AWC terms cycles completion, message usage, runtime performance.
also shown runtime characteristics directly attributed speed
centralized solver.
APOs performance enhancements attributed number things. First,
APO exhibits hill-climbing nature early search becomes focused
controlled time goes on. Like hill-climbing techniques often leads satisfiable
solution early search. Second, using partial overlaying information
agents use decision making, APO exploits work previously done
mediators. forms lock key mechanism promotes solution stability.
Lastly, importantly, APO uses dynamic, partial centralization, agents
work smaller, highly relevant portions overall problem. identifying areas
decomposability, search space greatly reduced which, cases, improves
efficiency centralized search algorithm.
vast number improvements planned APO future. Probably
important improve centralized solver uses. article, inefficient
solver chosen show strengths distributed portions APO. expect
573

fiMailler & Lesser

additional improvements algorithms runtime performance obtained
using faster centralized search engine. addition, modern solvers often use methods
like graph reductions, unit propagation backbone guided search. conceivable
information gained centralized search engine could used prune domains
variables consistency reasons variables centralized subproblem
relevance reasons. expect focus efforts agents additionally
reducing search time communications usage algorithm.
Along improvements selective use memory recording nogoods.
Unlike AWC uses nogoods ensure complete search, APOs completeness relies
one agents centralizing entire problem worst case. key
difference, APO improved simply remembering small, powerful subset
nogoods discovers mediation session session. would allow algorithm
improve future search exploiting work done previously.
clear APO, cooperative mediation methodology
whole, opens new areas future exploration new questions answered
distributed problem solving. believe work shows great deal promise
addressing vast number problems represents bridge centralized
distributed problem solving techniques.

Acknowledgments
Special thanks Bryan Horling design implementation Farm simulation
environment experiment run Shlomo Zilberstein, Bart Selman,
Neil Immerman, Jose Vidal making numerous suggestions development
work. Lastly, authors would like thank JAIR reviewers helpful
feedback suggestions Carlos Ansotegui Jean-Charles Regin lengthy
discussion final revision article.
effort represented paper sponsored Defense Advanced Research Projects Agency (DARPA) Air Force Research Laboratory, Air Force Materiel
Command, USAF, agreement number F30602-99-2-0525. views conclusions
contained herein authors interpreted necessarily representing official policies endorsements, either expressed implied, Defense
Advanced Research Projects Agency (DARPA), Air Force Research Laboratory, U.S.
Government. U.S. Government authorized reproduce distribute reprints
Governmental purposes notwithstanding copyright annotation thereon.

References
Bejar, R., Krishnamachari, B., Gomes, C., & Selman, B. (2001). Distributed constraint
satisfaction wireless sensor tracking system. Workshop Distributed Constraint Reasoning, International Joint Conference Artificial Intelligence, Seattle,
Washington.
Cammarata, S., McArthur, D., & Steeb, R. (1983). Strategies cooperation distributed
problem solving. Proceedings 8th International Joint Conference Artificial
574

fiAsynchronous Partial Overlay: New Algorithm DCSP

Intelligence (IJCAI-83), Vol. 2, pp. 767770.
Cha, B., & Iwana, K. (1996). Adding new clauses faster local search. Proceedings
Thirteenth National Conference Artificial Intelligence (AAAI), pp. 332337.
Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). really hard problems are.
Proceedings 12th International Joint Conference Artificial Intelligence
(IJCAI-91), pp. 331337.
Conry, S. E., Kuwabara, K., Lesser, V. R., & Meyer, R. A. (1991). Multistage negotiation
distributed constraint satisfaction. IEEE Transactions Systems, Man,
Cybernetics, 21 (6).
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1999). Introduction Algorithms.
McGraw-Hill.
Culberson, J., & Gent, I. (2001). Frozen development graph coloring. Theoretical Computer Science, 265 (12), 227264.
Fernandez, C., Bejar, R., Krishnamachari, B., Gomes, C., & Selman, B. (2003). Distributed
Sensor Networks: Multiagent Perspective, chap. Communication Computation
Distributed CSP Algorithms, pp. 299317. Kluwer Academic Publishers.
Ford, L. R., & Fulkerson, D. (1962). Flows Networks. Princeton University Press.
Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58 (13), 2170.
Frost, D., & Dechter, R. (1994). Dead-end driven learning. Proceedings Twelfth
Natioanl Conference Artificial Intelligence, pp. 294300.
Ginsberg, M. L. (1993). Dynamic backtracking. Journal Artificial Intelligence Research,
1, 2546.
Hayden, S., Carrick, C., & Yang, Q. (1999). Architectural design patterns multi-agent
coordination. Proceedings International Conference Agent Systems, Seattle, WA.
Hirayama, K., & Yokoo, M. (2000). effect nogood learning distributed constraint
satisfaction. 20th International Conference Distributed Computing Systems
(ICDCS), pp. 169177.
Krishnamachari, B., Bejar, R., & Wicker, S. (2002). Distributed problem solving
boundaries self-configuration multi-hop wireless networks. Hawaii International Conference System Sciences (HICSS-35).
Krishnamachari, B. (2002). Phase Transitions, Structure, Compleixty Wireless Networks. Ph.D. thesis, Cornell University, Ithaca, NY.
Mammen, D. L., & Lesser, V. R. (1998). Problem Structure Subproblem Sharing
Multi-Agent Systems. Third International Conference Multi-Agent Systems, 174
181.
Merriam-Webster (Ed.). (1995). Merriam-Webster Dictionary (Home Office edition). Springfield, IL.
575

fiMailler & Lesser

Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts:
heuristic repair method constraint satisfaction scheduling problems. Artificial
Intelligence, 58 (1-3), 161205.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computational complexity characteristic phase transitions. Nature, 400,
133137.
Rao, V. N., & Kumar, V. (1993). efficiency parallel backtracking. IEEE Transactions Parallel Distributed Systems, 4 (4), 427437.
Sycara, K., Roth, S., Sadeh, N., & Fox, M. (1991). Distributed constrained heuristic search.
IEEE Transactions Systems, Man, Cybernetics, 21 (6), 14461461.
Sycara, K. (1988). Resolving goal conflicts via negotiation. Proceedings Seventh
National Conference Artificial Intelligence, pp. 245250.
Wellman, M., & Walsh, W. (1999). Distributed quiescence detection multiagent negotiation. AAAI-99 Workshop Negotiation: Settling Conflicts Identifying
Opportunities.
Werkman, K. J. (1990). Knowledge-based model negotiation using shared perspectives.
Proceedings 10th International Workshop Distributed Artificial intelligence,
Bandera, TX.
Yokoo, M. (1994). Weak-commitment search solving constraint satisfaction problems.
Proceedings 12th National Conference Artificial Intelligence (AAAI-94);
Vol. 1, pp. 313318, Seattle, WA, USA. AAAI Press, 1994.
Yokoo, M. (1995). Asynchronous weak-commitment search solving distributed constraint
satisfaction problems.. Proceedings First International Conference Principles Practice Constraint Programming (CP-95), Lecture Notes Computer
Science 976, pp. 88102. Springer-Verlag.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction formalizing distributed problem solving. International Conference
Distributed Computing Systems, pp. 614621.
Yokoo, M., & Hirayama, K. (1996). Distributed breakout algorithm solving distributed
constraint satisfaction problems.. International Conference Multi-Agent Systems
(ICMAS).
Yokoo, M., & Hirayama, K. (2000). Algorithms distributed constraint satisfaction:
review. Autonomous Agents Multi-Agent Systems, 3 (2), 198212.
Yokoo, M., Suzuki, K., & Hirayama, K. (2002). Secure distributed constraint satisfaction: Reaching agreement without revealing private information. Proceeding
Eighth International Conference Principles Practice Constraint Programming (CP).

576

fiJournal Artificial Intelligence Research 25 (2006) 349-387

Submitted 06/05; published 03/06

Representing Conversations Scalable Overhearing
Gery Gutnik
Gal A. Kaminka

gutnikg@cs.biu.ac.il
galk@cs.biu.ac.il

Computer Science Department
Bar Ilan University
Ramat Gan 52900, Israel

Abstract
Open distributed multi-agent systems gaining interest academic community
industry. open settings, agents often coordinated using standardized
agent conversation protocols. representation protocols (for analysis, validation, monitoring, etc) important aspect multi-agent applications. Recently, Petri
nets shown interesting approach representation, radically
different approaches using Petri nets proposed. However, relative strengths
weaknesses examined. Moreover, scalability suitability
different tasks addressed. paper addresses challenges. First,
analyze existing Petri net representations terms scalability appropriateness overhearing, important task monitoring open multi-agent systems. Then,
building insights gained, introduce novel representation using Colored Petri
nets explicitly represent legal joint conversation states messages. representation approach offers significant improvements scalability particularly suitable
overhearing. Furthermore, show new representation offers comprehensive coverage conversation features FIPA conversation standards. also present
procedure transforming AUML conversation protocol diagrams (a standard humanreadable representation), Colored Petri net representation.

1. Introduction
Open distributed multi-agent systems (MAS) composed multiple, independently-built
agents carry mutually-dependent tasks. order allow inter-operability agents
different designs implementation, agents often coordinate using standardized interaction protocols, conversations. Indeed, multi-agent community investing
significant effort developing standardized Agent Communication Languages (ACL) facilitate sophisticated multi-agent systems (Finin, Labrou, & Mayfield, 1997; Kone, Shimazu,
& Nakajima, 2000; ChaibDraa, 2002; FIPA site, 2003). standards define communicative acts, top them, interaction protocols, ranging simple queries
state another agent, complex negotiations auctions bidding contracts.
instance, FIPA Contract Net Interaction Protocol (FIPA Specifications, 2003b) defines
concrete set message sequences allows interacting agents use contract
net protocol negotiations.
Various formalisms proposed describe standards (e.g., Smith & Cohen,
1996; Parunak, 1996; Odell, Parunak, & Bauer, 2000, 2001b; AUML site, 2003). particular, AUMLAgent Unified Modelling Languageis currently used FIPA-ACL standards
c
2006
AI Access Foundation. rights reserved.

fiGutnik & Kaminka

(FIPA Specifications, 2003a, 2003b, 2003c, 2003d; Odell, Parunak, & Bauer, 2001a) 1 . UML
2.0 (AUML site, 2003), new emerging standard influenced AUML, potential
become FIPA-ACL standard (and forthcoming IEEE standard) future. However, moment, large set FIPA specifications remains formalized using AUML.
AUML intended human readability visualization, interaction protocols
ideally represented way amenable automated analysis, validation
verification, online monitoring, etc.
Lately, increasing interest using Petri nets (Petri Nets site, 2003) modelling
multi-agent interaction protocols (Cost, 1999; Cost, Chen, Finin, Labrou, & Peng, 1999,
2000; Lin, Norrie, Shen, & Kremer, 2000; Nowostawski, Purvis, & Cranefield, 2001; Purvis,
Hwang, Purvis, Cranefield, & Schievink, 2002; Cranefield, Purvis, Nowostawski, & Hwang,
2002; Ramos, Frausto, & Camargo, 2002; Mazouzi, Fallah-Seghrouchni, & Haddad, 2002;
Poutakidis, Padgham, & Winikoff, 2002). broad literature using Petri nets
analyze various aspects distributed systems (e.g. deadlock detection shown
Khomenco & Koutny, 2000), recent work specific uses Petri nets
multi-agent systems, e.g., validation testing (Desel, Oberweis, & Zimmer, 1997),
automated debugging monitoring (Poutakidis et al., 2002), dynamic interpretation
interaction protocols (Cranefield et al., 2002; de Silva, Winikoff, & Liu, 2003), modelling
agents behavior induced participation conversation (Ling & Loke, 2003)
interaction protocols refinement allowing modular construction complex conversations
(Hameurlain, 2003).
However, key questions remain open use Petri nets conversation representation. First, radically different approaches representation using Petri nets
proposed, relative strengths weaknesses investigated. Second,
many investigations addressed restricted subsets features needed representing complex conversations standardized FIPA (see detailed discussion
previous work Section 2). Finally, procedures proposed translating
human-readable AUML protocol descriptions corresponding machine-readable Petri
nets.
paper addresses open challenges context scalable overhearing. Here,
overhearing agent passively tracks many concurrent conversations involving multiple participants, based solely exchanged messages, participant
overheard conversations (Novick & Ward, 1993; Busetta, Serafini, Singh, & Zini,
2001; Kaminka, Pynadath, & Tambe, 2002; Poutakidis et al., 2002; Busetta, Dona, & Nori,
2002; Legras, 2002; Gutnik & Kaminka, 2004a; Rossi & Busetta, 2004). Overhearing useful visualization progress monitoring (Kaminka et al., 2002), detecting failures
interactions (Poutakidis et al., 2002), maintaining organizational situational awareness (Novick & Ward, 1993; Legras, 2002; Rossi & Busetta, 2004) non-obtrusively
identifying opportunities offering assistance (Busetta et al., 2001, 2002). instance,
overhearing agent may monitor conversation contractor agent engaged multiple
contract-net protocols different bidders bid callers, order detect failures.
begin analysis Petri net representations, respect scalability
overhearing. classify representation choices along two dimensions affecting scalability:
1. (FIPA Specifications, 2003c) currently deprecated. However, use specification since describes
many important features needed modelling multi-agent interactions.

350

fiRepresenting Conversations Scalable Overhearing

(i) technique used represent multiple concurrent conversations; (ii) choice
representing either individual joint interaction states. show runtime complexity monitoring conversations using different approaches same, choices
along two dimensions significantly different space requirements, thus
choices scalable (in number conversations) others. also argue
representations suitable overhearing require use explicit message places, though
subset previously-explored techniques utilized those.
Building insights gained, paper presents novel representation uses
Colored Petri nets (CP-nets) places explicitly denote messages, valid joint
conversation states. representation particularly suited overhearing number
conversations scaled-up. show representation used represent
essentially features FIPA AUML conversation standards, including simple complex interaction building blocks, communicative act attributes message guards
cardinalities, nesting, temporal aspects deadlines duration.
realize advantages machine-readable representations, debugging
(Poutakidis et al., 2002), existing human-readable protocol descriptions must converted
corresponding Petri net representations. final contribution paper,
provide skeleton semi-automated procedure converting FIPA conversation protocols
AUML Petri nets, demonstrate use complex FIPA protocol.
procedure fully automated, takes first step towards addressing open challenge.
paper organized follows. Section 2 presents motivation work.
Sections 3 6 present proposed representation addressing FIPA conversation features including basic interaction building blocks (Section 3), message attributes
(Section 4), nested & interleaved interactions (Section 5), temporal aspects (Section 6).
Section 7 ties features together: presents skeleton algorithm transforming
AUML protocol diagram Petri net representation, demonstrates use challenging FIPA conversation protocol. Section 8 concludes. paper rounds three
appendixes. first provides quick review Petri nets. Then, complete coverage
FIPA interactions, Appendix B provides additional interaction building blocks. Appendix C
presents Petri net complex conversation protocol, integrates many features
developed representation technique.

2. Representations Scalable Overhearing
Overhearing involves monitoring conversations progress, tracking messages
exchanged participants (Gutnik & Kaminka, 2004a). interested representations facilitate scalable overhearing, tracking many concurrent conversations,
many agents. focus open settings, complex internal state control logic agents known advance, therefore exclude discussions Petri net
representations explicitly model agent internals (e.g., Moldt & Wienberg, 1997; Xu
& Shatz, 2001). Instead, treat agents black boxes, consider representations
commit agents conversation state (i.e., role progress conversation).
suitability representation scalable overhearing affected several facets.
First, since overhearing based tracking messages, representation must able
explicitly represent passing message (communicative act) one agent another
351

fiGutnik & Kaminka

(Section 2.1). Second, representation must facilitate tracking multiple concurrent
conversations. tracking runtime bounded number messages
(since case, messages overheard processed), space requirements may differ
significantly (see Sections 2.22.3).
2.1 Message-monitoring versus state-monitoring
distinguish two settings tracking progress conversations, depending
information available tracking agent. first type setting, refer
state monitoring, tracking agent access internal state conversation
one participants, necessarily messages exchanged.
settings involves message monitoring, tracking agent access
messages exchanged (which externally observable), cannot directly observe
internal state conversation participant. Overhearing form message
monitoring.
Representations support state monitoring use places denote conversation
states participants. Tokens placed places (the net marking) denote
current state. sending receiving message participant explicitly
represented, instead implied moving tokens (through transition firings) new
state places. Thus, representation essentially assumes internal conversation
state participants directly observable monitoring agent. Previous work utilizing
state monitoring includes work Cost (1999), Cost et al. (1999, 2000), Lin et al. (2000),
Mazouzi et al. (2002), Ramos et al. (2002).
representation present paper intended overhearing tasks, cannot
assume conversation states overheard agents observable. Instead, must
support message monitoring, addition using tokens state places (to denote
current conversation state), representation uses message places, tokens placed
corresponding message overheard. conversation-state place message
place connected via transition state place denoting new conversation state.
Tokens placed originating placesindicating message received appropriate
conversation statewill cause transition fire, tokens placed
new conversation state place. Thus new conversation state inferred "observing"
message. Previous investigations, used explicit message places, include work
Cost (1999), Cost et al. (1999, 2000), Nowostawski et al. (2001), Purvis et al. (2002),
Cranefield et al. (2002), Poutakidis et al. (2002)2 . discussed depth below.
2.2 Representing Single Conversation
Two representation variants popular within utilize conversation places (in
addition message places): Individual state representations use separate places tokens
state participant (each role). Thus, overall state conversation
represented different tokens marking multiple places. Joint state representations use
single place joint conversation state participants. placement token
2. Cost (1999), Cost et al. (1999, 2000) present examples state- message- monitoring representations.

352

fiRepresenting Conversations Scalable Overhearing

within place represents overhearing agents belief participants
appropriate joint state.
previous representations use individual states. these, different markings distinguish conversation state one agent sent message, state
agent received it. net conversation role essentially built separately,
merged nets, connected via fusion places similar means.
Cost (1999), Cost et al. (1999, 2000) used CP-nets individual state places
representing KQML FIPA interaction protocols. Transitions represent message events,
CP-net features, token colors arc expressions, used represent AUML
message attributes sequence expressions. authors also point deadlines (a
temporal aspect interaction) modelled, implementation details provided.
Cost (1999) also proposed using hierarchical CP-nets represent hierarchical multi-agent
conversations.
Purvis et al. (2002), Cranefield et al. (2002) represented conversation roles separate
CP-nets, places denote interaction messages states, transitions represent operations performed corresponding communicative acts send, receive,
process. Special in/out places used pass net tokens different CP-nets,
special get/put transitions, simulating actual transmission corresponding
communicative acts.
principle, individual-state representations require two places role, every
message. given message, would two individual places sender (before
sending sending), similarly two receiver (before receiving
receiving). possible conversation statesvalid notcan represented.
single message two roles, two places role (four places total), four
possible conversation states: message sent received, sent received, sent
incorrectly believed received, sent received. states
represented different markings. instance, conversation state message
sent received denoted token after-sending place sender
another token before-receiving place receiver. summarized
following proposition:
Proposition 1 Given conversation R roles total possible messages,
individual state representation space complexity O(M R).
representations represent roles conversation state separately,
many applications overhearing require representation valid conversation states
(message sent received, sent received). Indeed, specifications interaction protocols often assume use underlying synchronization protocols guarantee
delivery messages (Paurobally & Cunningham, 2003; Paurobally, Cunningham, & Jennings, 2003). assumption, every message, two joint states
regardless number roles. example, single message three rolesa
sender two receivers, two places two possible markings: token
sending/receiving place represents conversation state message
yet sent sender (and two receivers waiting it), token
sending/receiving place denotes message sent received
receivers.
353

fiGutnik & Kaminka

Nowostawski et al. (2001) utilize CP-nets places denote joint conversation states.
also utilize places representing communicative acts. Poutakidis et al. (2002) proposed
representation based Place-Transition nets (PT-nets)a restricted representation
Petri nets color. presented several interaction building blocks,
could fit together model additional conversation protocols. general, following
proposition holds respect representations:
Proposition 2 Given conversation R roles total possible messages,
joint state representation represents legal states space complexity O(M ).
condition representing valid states critical complexity analysis.
joint conversation statesvalid invalidare represented, space complexity would
O(M R ). case, individual-state representation would advantage.
would case, instance, assume use synchronization protocols,
e.g., overhearing agent may wish track exact system state even
message underway (i.e., sent yet received).
2.3 Representing Multiple Concurrent Conversations
Propositions 1 2 address space complexity representing single conversation. However, large scale systems overhearing agent may required monitor
multiple conversations parallel. instance, overhearing agent may monitoring
middle agent carrying multiple parallel instances single interaction protocol
multiple partners, e.g., brokering (FIPA Specifications, 2003a).
previous investigations propose duplicate appropriate Petri net representation monitored conversation (Nowostawski et al., 2001; Poutakidis et al., 2002).
approach, every conversation tracked separate Petri-net, thus number
Petri nets (and associated tokens) grows number conversations (Proposition 3). instance, Nowostawski et al. (2001) shows example contract-net
protocol carried three different contractors, using three duplicate CP-nets.
captured following proposition:
Proposition 3 representation creates multiple instances conversation Petri net
represent C conversations, requires O(C) net structures, O(C) bits tokens.
investigations take different approach, single CP-net structure used
monitor conversations protocol. tokens associated conversations
differentiated token color (Cost, 1999; Cost et al., 1999, 2000; Lin et al., 2000;
Mazouzi et al., 2002; Cranefield et al., 2002; Purvis et al., 2002; Ramos et al., 2002).
example, assigning token color tuple type hsender, receiveri, agent
differentiate multiple tokens place thus track conversations different pairs
agents3 . Color tokens use multiple bits per token; log C bits required differentiate C conversations. Therefore, number bits required track C conversations
using C tokens C log C. leads following proposition.
3. See Section 4 distinguish different conversations agents.

354

fiRepresenting Conversations Scalable Overhearing

Proposition 4 representation uses color tokens represent C multiple instances
conversation, requires O(1) net structures, O(C log C) bits tokens.

Due constants involved, space requirements Proposition 3 practice
much expensive Proposition 4. Proposition 3 refers creation
O(C) Petri networks, duplicated place transition data structures. contrast,
Proposition 4 refers bits required representing C color tokens single CP net.
Moreover, practical settings, sufficiently large constant bound number
conversations may found, essentially reduce O(log C) factor O(1).
Based Propositions 14, possible make concrete predictions scalability
different approaches respect number conversations, roles. Table 1 shows
space complexity different approaches modelling C conversations
protocol, maximum R roles, messages, assumption
underlying synchronization protocols. table also cites relevant previous work.

Individual
States
(Proposition 1)
Joint
States
(Proposition 2)

Representing Multiple Conversations (of Protocol)
Multiple CP- PT-nets
Using color tokens, single CP-net
(Proposition 3)
(Proposition 4)
Space: O(M R + C log C)
Cost (1999), Cost et al. (1999, 2000),
Space: O(M RC)
Lin et al. (2000), Cranefield et al. (2002),
Purvis et al. (2002), Ramos et al. (2002),
Mazouzi et al. (2002)
Space: O(M C)
Space: O(M + C log C)
Nowostawski et al. (2001),
paper
Poutakidis et al. (2002)
Table 1: Scalability different representations

Building insights gained Table 1, propose representation using CP-nets
places explicitly represent joint conversation states (corresponding lower-right
cell Table 1), tokens color used distinguish concurrent conversations (as
upper-right cell Table 1). such, related works features,
table demonstrates, novel synthesis.
representation uses similar structures found works Nowostawski
et al. (2001) Poutakidis et al. (2002). However, contrast previous investigations, rely token color CP-nets model concurrent conversations, space
complexity O(M + C log C). also show (Sections 36) used cover
variety conversation features covered investigations. features include
representation full set FIPA interaction building blocks, communicative act attributes
(such message guards, sequence expressions, etc.), compact modelling concurrent conversations, nested interleaved interactions, temporal aspects.
355

fiGutnik & Kaminka

3. Representing Simple & Complex Interaction Building Blocks
section introduces fundamentals representation, demonstrates various simple complex AUML interaction messages, used FIPA conversation standards
(FIPA Specifications, 2003c), implemented using proposed CP-net representation. begin simple conversation, shown Figure 1-a using AUML protocol
diagram. Here, agent1 sends asynchronous message msg agent2 .







ff



fi

fffi


ff
fi





ff
fi

(a) AUML representation




fi !"
# $ %&' (

%)'
* " # $

' ( '
%
+
'

(b) CP-net representation

Figure 1: Asynchronous message interaction.
represent agent conversation protocols, define two types places, corresponding
messages conversation states. first type net places, called message places,
used describe conversation communicative acts. Tokens placed message places indicate
associated communicative act overheard. second type net places,
agent places, associated valid joint conversation states interacting agents.
Tokens placed agent places indicate current joint state conversation within
interaction protocol.
Transitions represent transmission receipt communicative acts agents.
Assuming underlying synchronization protocols, transition always originates within jointstate place message place, targets joint conversation state (more one
possiblesee below). Normally, current conversation state known (marked
token), must wait overhearing matching message (denoted token
connected message place). token marked, transition fires, automatically
marking new conversation state.
Figure 1-b presents CP net representation earlier example Figure 1-a. CPnet Figure 1-b three places one transition connecting them. A1 B1
A2 B2 places agent places, msg place message place. B capital
letters used denote agent1 agent2 individual interaction states respectively
(we indicated individual joint interaction states AUML diagram
Figure 1-a, omit annotations later figures). Thus, A1 B1 place indicates
joint interaction state agent1 ready send msg communicative act agent2
(A1 ) agent2 waiting receive corresponding message (B1 ). msg message
place corresponds msg communicative act sent two agents. Thus,
transmission msg communicative act causes agents transition A2 B2
356

fiRepresenting Conversations Scalable Overhearing

place. place corresponds joint interaction state agent1 already sent
msg communicative act agent2 (A2 ) agent2 received (B2 ).
CP-net implementation Figure 1-b also introduces use token colors
represent additional information interaction states communicative acts.
token color sets defined net declaration, i.e. dashed box Figure 1-b.
syntax follows standard CPN ML notation (Wikstrom, 1987; Milner, Harper, &
Tofte, 1990; Jensen, 1997a). AGEN color identifies agents participating
interaction, used construct two compound color sets.
INTER-STATE color set associated agent places, represents agents
appropriate joint interaction states. record ha1 , a2 i, a1 a2 AGEN
color elements distinguishing interacting agents. apply INTER-STATE color
set model multiple concurrent conversations using CP-net. second color
set SG, describing interaction communicative acts associated message places.
SG color token record , ar i, ar correspond sender
receiver agents associated communicative act. cases, additional elements,
conversation identification, may used. See Section 4 additional details.
Figure 1-b, A1 B1 A2 B2 places associated INTER-STATE
color set, msg place associated SG color set. place color set
written italic capital letters next corresponding place. Furthermore, use
r AGEN color type variables denote net arc expressions. Thus, given
output arc expression A1 B1 msg places hs, ri, r
elements agent place token must correspond r elements message
place token. Consequently, net transition occurs agents message
correspond interacting agents. A2 B2 place input arc expression hr, si following
underlying intuition agent2 going send next interaction communicative
act.
Figure 2-a shows AUML representation another interaction building block, synchronous message passing, denoted filled solid arrowhead. Here, msg communicative act sent synchronously agent1 agent2 , meaning acknowledgement
msg communicative act must always received agent1 interaction may
proceed.
corresponding CP-net representation shown Figure 2-b. interaction starts
A1 B1 place terminates A2 B2 place. A1 B1 place represents joint
interaction state agent1 ready send msg communicative act agent2 (A1 )
agent2 waiting receive corresponding message (B1 ). A2 B2 place denotes
joint interaction state, agent1 already sent msg communicative act
agent2 (A2 ) agent2 received (B2 ). However, since CP-net diagram represents
synchronous message passing, msg communicative act transmission cannot cause
agents transition directly A1 B1 place A2 B2 place. therefore define
intermediate A01 B10 agent place. place represents joint interaction state agent2
received msg communicative act ready send acknowledgement
(B1 ), agent1 waiting acknowledgement (A01 ). Taken together, msg
communicative act causes agents transition A1 B1 place A01 B10 place,
acknowledgement msg message causes agents transition
A01 B10 place A2 B2 place.
357

fiGutnik & Kaminka


ff
fi










fi


ff

fi






ff
fi


fi




!"
#$% & '
( ) ( "
* % & ' ( ) ( "
+ ( "

(a) AUML representation

(b) CP-net representation

Figure 2: Synchronous message interaction.
Transitions typical multi-agent interaction protocols composed interaction
building blocks, two presented above. Additional interaction buildingblocks, fairly straightforward (or appeared previous work, e.g., Poutakidis
et al., 2002) presented Appendix B. remainder section, present two
complex interactions building blocks generally common multi-agent interactions:
XOR-decision OR-parallel.
begin XOR-decision interaction. AUML representation building
block shown Figure 3-a. sender agent agent1 either send message msg1
agent2 message msg2 agent3 , send msg1 msg2 . non-filled
diamond x inside AUML notation constraint.

















ff fi


!"#$ %&

'()' (
"#$#*%&





!




'()' (
)'+(


,$ %&
() (


-' . / (

ff

ff






(a) AUML representation

(b) CP-net representation

Figure 3: XOR-decision messages interaction.
Figure 3-b shows corresponding CP-net. Again, A, B C capital letters
used denote interaction states agent1 , agent2 agent3 , respectively.
358

fiRepresenting Conversations Scalable Overhearing

interaction starts A1 B1 C1 place terminates either A2 B2 place
A2 C2 place. A1 B1 C1 place represents joint interaction state agent1 ready
send either msg1 communicative act agent2 msg2 communicative act agent3
(A1 ); agent2 agent3 waiting receive corresponding msg1 /msg2 message
(B1 /C1 ). represent A1 B1 C1 place color set, extend INTER-STATE color
set denote joint interaction state three interacting agents, i.e. using INTERSTATE-3 color set. msg1 communicative act causes agents transition A2 B2
place. A2 B2 place represents joint interaction state agent1 sent msg1
message (A2 ), agent2 received (B2 ). Similarly, msg2 communicative act causes
agents agent1 agent3 transition A2 C2 place. Exclusiveness achieved since
single agent token A1 B1 C1 place used either activating A1 B1 C1 A2 B2
transition activating A1 B1 C1 A2 C2 transition, both.
similar complex interaction OR-parallel messages interaction. AUML representation presented Figure 4-a. sender agent, agent1 , send message msg1
agent2 message msg2 agent3 , both. non-filled diamond AUML notation
constraint.


fffi

fi



















fffi




fffi






fffi



(a) AUML representation

!"
#$%& '(
)*+) *"
#$%&%, '(
)*+) *
+)-*"
. & '(
*+*"
/) 01 *"


fffi



(b) CP-net representation

Figure 4: OR-parallel messages interaction.
Figure 4-b shows CP-net representation OR-parallel interaction. interaction starts A1 B1 C1 place terminated A2 B2 place,
A2 C2 place, both. represent inclusiveness interaction protocol, define
two intermediate places, A01 B1 place A001 C1 place. A01 B1 place represents
joint interaction state agent1 ready send msg1 communicative act agent2
(A01 ) agent2 waiting receive message (B1 ). A001 C1 place similar meaning, respect agent3 . normally done Petri nets, transition connecting
A1 B1 C1 place intermediate places duplicates single token A1 B1 C1 place
two tokens going A01 B1 A001 C1 places. Consequently, two parts
OR-parallel interaction independently executed.

4. Representing Interaction Attributes
extend representation allow additional interaction aspects, useful describing multi-agent conversation protocols. First, show represent interaction
359

fiGutnik & Kaminka

message attributes, guards, sequence expressions, cardinalities content (FIPA
Specifications, 2003c). explore depth representation multiple concurrent
conversations (on CP net).
Figure 5-a shows simple agent interaction using AUML protocol diagram.
interaction similar one presented Figure 1-a previous section. However,
Figure 5-a uses AUML message guard-conditionmarked [condition]that
following semantics: communicative act sent agent1 agent2
condition true.







ff
fi




fffi fffi


ff
fi

(a) AUML representation

! " #$
!%& " #$

'( ! ! " #$
fi ) !*+,! ! " -

./0 !1 .2 0 !$
3 , " -
0 !1 0 !1

0!%&10'( ! !$
4. 0 !$ 4. 0!%&$

4. 0'( ! !$
(b) CP-net representation

Figure 5: Message guard-condition
guard-condition implementation Petri net representation uses transition
guards (Figure 5-b), native feature CP nets. AUML guard condition mapped
directly CP-net transition guard. CP-net transition guard indicated
net inscription next corresponding transition using square brackets. transition
guard guarantees transition enabled transition guard true.
Figure 5-b, also extend color tokens include information
communicative act used content. extend SG color set definition
record hs, r, t, ci, r elements interpretation previous
section (sender receiver), c elements define message type content,
respectively. element new color P E, determines communicative act
types. c element new color CON EN , represents communicative act
content argument list (e.g. reply-to, reply-by etc).
addition new elements also allows additional potential uses. instance,
facilitate representation multiple concurrent conversations agents
(s r), possible add conversation identification field SG
INTER-STATE colors. simplicity, refrain examples
paper.
Two additional AUML communicative act attributes modelled CP
representation message sequence-expression message cardinality. sequenceexpressions denote constraint message sent sender agent. number
sequence-expressions defined FIPA conversation standards (FIPA Specifications, 2003c):
denotes message sent exactly times; n..m denotes message sent
anywhere n times; denotes message sent arbitrary number
360

fiRepresenting Conversations Scalable Overhearing

times. additional important sequence expression broadcast, i.e. message sent
agents.
explain representation sequence-expressions CP-nets, using broadcast
example (Figure 6-b). sequence expressions easily derived example.
define INTER-STATE-CARD color set. color set tuple (ha1 , a2 i, i) consisting
two elements. first tuple element INTER-STATE color element, denotes
interacting agents previously defined. second tuple element integer
counts number messages already sent agent, i.e. message cardinality.
element initially assigned 0. INTER-STATE-CARD color set applied
S1 R1 place, R capital letters used denote sender
receiver individual interaction states respectively S1 R1 indicates initial joint
interaction state interacting agents. two additional colors, used Figure 6-b,
BROADCAST-LIST ARGET colors. BROADCAST-LIST color defines
sender broadcast list designated receivers, assuming sender must
list carry role. ARGET color defines indexes broadcast list.







fffi




!!

ff
fi

fi ff




'()( * + ,'()( ./ + ,'()( 01 * * + ,'()( 2* 3 + %'(4 56 *7
56 *# $%&
'()( 08 + 9:
'()( 2* 3 30 8 + ; ( 4<':
" "
2* 3 7 08
ff
fi
'()( = + %'(4 6 *7 6 *7
fi
:6./ 7' 601 * *

'()( > 180 2 + * @ :A,3?
B 5 ) $% + , '()( + 94%C > 180 3? 2

ff
@ :A !DDD$%3 B 5 6 *- B 5 6= - B 5 608fi

(a) AUML representation

(b) CP-net representation

Figure 6: Broadcast sequence expression.
According broadcast sequence-expression semantics, sender agent sends
msg1 communicative act receivers broadcast list. CP-net introduced Figure 6-b models behavior.4 interaction starts S1 R1 place,
representing joint interaction state sender ready send msg1 communicative act receiver (S1 ) receiver waiting receive corresponding msg1
message (R1 ). S1 R1 place initial marking single token, set initialization expression (underlined, next corresponding place). initialization expression 1(hs, ARGET (0)i, 0)given standard CPN ML notationdetermines S1 R1
places initial marking multi-set containing single token (hs, ARGET (0)i, 0). Thus,
first designated receiver assigned agent index 0 broadcast list,
message cardinality counter initiated 0.
4. implement broadcast iterative procedure sending corresponding communicative act separately designated recipients.

361

fiGutnik & Kaminka

msg1 message place initially contains multiple tokens. tokens represents msg1 communicative act addressed different designated receiver
broadcast list. Figure 6-b, initialization expression, corresponding msg1 message place, omitted. S1 R1 place token appropriate msg1 place token
together enable corresponding transition. Consequently, transition may fire thus
msg1 communicative act transmission simulated.
msg1 communicative act sent incrementally every designated receiver
broadcast list. incoming arc expression (hs, ri, i) incremented transition
outgoing (hs, ARGET (i + 1)i, + 1) arc expression, causing receiver agent
index + 1 broadcast list selected. transition guard constraint < size,
i.e. < |broadcast list|, ensures msg1 message sent |broadcast list|
times. msg1 communicative act causes agents transition S2 R2 place.
place represents joint interaction state sender already sent msg1
communicative act receiver waiting receive msg2 message (S2 )
receiver received msg1 message ready send msg2 communicative act
sender (R2 ). Finally, msg2 message causes agents transition S3 R3
place. S3 R3 place denotes joint interaction state sender received msg2
communicative act receiver terminated (S3 ), receiver already sent
msg2 message sender terminated well (R3 ).
use Figure 6-b demonstrate use token color represent multiple concurrent
conversations using CP-net. instance, let us assume sender agent
called agent1 broadcast list contains following agents: agent2 , agent3 , agent4 ,
agent5 agent6 . also assume agent1 already sent msg1 communicative act agents broadcast list. However, received msg2
reply message agent3 agent6 . Thus, CP-net current marking complete
interaction protocol described follows: S2 R2 place marked hagent2 , agent1 i,
hagent4 , agent1 i, hagent5 , agent1 i, S3 R3 place contains tokens hagent1 , agent3
hagent1 , agent6 i.
Example. construct CP-net representation FIPA Query Interaction
Protocol (FIPA Specifications, 2003d), shown AUML form Figure 7, demonstrate
building blocks presented Sections 3 4 put together. interaction
protocol, Initiator requests P articipant perform inform action using one two
query communicative acts, query-if query-ref. P articipant processes query
makes decision whether accept ref use query request. Initiator may request
P articipant respond either accept ref use message, simplicity,
assume always case. case query request accepted,
P articipant informs Initiator query results. P articipant fails,
communicates f ailure. successful response, P articipant replies one
two versions inform (inform-t/f inform-result) depending type initial query
request.
CP-net representation FIPA Query Interaction Protocol presented Figure 8. interaction starts I1 P1 place (we use P capital letters
denote Initiator P articipant roles). I1 P1 place represents joint
interaction state (i) Initiator agent ready send either query-if communicative act, query-ref message, P articipant (I1 ); (ii) P articipant wait362

fiRepresenting Conversations Scalable Overhearing

fffi






fffi


fffi fi
fi ff fi
fi ff fi

fifi
fifi
fifi
ff fi



ff fi

fifi

fi ff

fffi fi

Figure 7: FIPA Query Interaction Protocol - AUML representation.

ing receive corresponding message (P1 ). Initiator send either query-if
query-ref communicative act. assume acts belong class,
query communicative act class. Thus, implement messages using single
Query message place, check message type using following transition guard:
[#t msg = query-if #t msg = query-ref]. query communicative act causes
interacting agents transition I2 P2 place. place represents joint interaction
state Initiator sent query communicative act waiting receive
response message (I2 ), P articipant received query communicative act
deciding whether send agree ref use response message Initiator (P2 ).
ref use communicative act causes agents transition I3 P3 place, agree
message causes agents transition I4 P4 place.
P articipant decision whether send agree ref use communicative
act represented using XOR-decision building block introduced earlier (Figure 3-b).
I3 P3 place represents joint interaction state Initiator received ref use
communicative act terminated (I3 ) P articipant sent ref use message
terminated well (P3 ). I4 P4 place represents joint interaction state Initiator
received agree communicative act waiting response
363

fiGutnik & Kaminka

+

+





fffi





ff

3 ? 6 ( 9 6



, B







! "


@Aff

!#

' " (" ) *+, -.


$!


ff fi


ff




%







ff




%




fffi

' " (" 23 +, *+, -.


' " ("



7,/ *8 ' 723 +, * +, .







ff

+, *$4,, * ' "5 6 7 ) *+,8

6 7 ) *+, .
%
' " (" 9 4 ) ' "5 7 ) *+, 87 ) *+,8



: 6 7 ) *+, .


fffi




' " (" ,/ * ! 0 ! 0111.



&


&

: 6 79 4 ) .

: 6 7,/ * .

3 ;,

?!"










>6 (





C ?!" D! 6 ?5 ! E




ff



fffi


fffi
=



=

3 ;,



ff


< <

" C ?! " ( 6 ?5
! E#

3 ;,

Figure 8: FIPA Query Interaction Protocol - CP-net representation.

P articipant (I4 ) P articipant sent agree message deciding
response send Initiator (P4 ). point, P articipant agent may send one
following communicative acts: inform-t/f, inform-result f ailure. choice
represented using another XOR-decision building block, inform-t/f informresult communicative acts represented using single Inf orm message place. f ailure
communicative act causes transition I5 P5 place, inf orm message causes
transition I6 P6 place. I5 P5 place represents joint interaction state
P articipant sent f ailure message terminated (P5 ), Initiator received
f ailure terminated (I5 ). I6 P6 place represents joint interaction state
P articipant sent inf orm message terminated (P6 ), Initiator received
inf orm terminated (I6 ).
implementation [query-if ] [query-ref ] message guard conditions requires detailed discussion. implemented usual manner view fact
depend original request communicative act. Thus, create special intermediate place contains original message type marked "Original essage ype"
figure. case inf orm communicative act sent, transition guard verifies
inf orm message appropriate original query type. Thus, inform-t/f
communicative act sent original query type query-if
inform-result message sent original query type query-ref.
364

fiRepresenting Conversations Scalable Overhearing

5. Representing Nested & Interleaved Interactions
section, extend CP-net representation previous sections model nested
interleaved interaction protocols. focus nested interaction protocols. Nevertheless, discussion also addressed interleaved interaction protocols similar
fashion.
FIPA conversation standards (FIPA Specifications, 2003c) emphasize importance
nested interleaved protocols modelling complex interactions. First, allows reuse interaction protocols different nested interactions. Second, nesting increases
readability interaction protocols.
AUML notation annotates nested interleaved protocols round corner rectangles (Odell et al., 2001a; FIPA Specifications, 2003c). Figure 9-a shows example
nested protocol5 , Figure 9-b illustrates interleaved protocol. Nested protocols
one compartments. first compartment name compartment.
name compartment holds (optional) name nested protocol. nested protocol
name written upper left-hand corner rectangle, i.e. commitment Figure 9a. second compartment, guard compartment, holds (optional) nested protocol
guard. guard compartment written lower left-hand corner rectangle, e.g.
[commit] Figure 9-a. Nested protocols without guards equivalent nested protocols
[true] guard.











fffi






fffifi

ff




fi fi



(a) Nested protocol

(b) Interleave protocol

Figure 9: AUML nested interleaved protocols examples.
Figure 10 describes implementation nested interaction protocol presented
Figure 9-a extending CP-net representation using hierarchies, relying standard CP-net methods (see Appendix A). hierarchical CP-net representation contains
three elements: superpage, subpage page hierarchy graph. CP-net superpage
represents main interaction protocol containing nested interaction, CP-net
subpage models corresponding nested interaction protocol, i.e. Commitment Inter5. Figure 9-a appears FIPA conversation standards (FIPA Specifications, 2003c). Nonetheless, note
request-good request-pay communicative acts part FIPA-ACL standards.

365

fiGutnik & Kaminka

action Protocol. page hierarchy graph describes superpage decomposed
subpages.

# ff ff ffff


ff



fffffi
ff ff ff ffff

ff


fffffi
fffffi



# ff
ffffff



ff

ff ff ffff ff

ff
ff ffff ff




fffffi








!"

Figure 10: Nested protocol implementation using hierarchical CP-nets.
Let us consider detail process modelling nested interaction protocol
Figure 9-a using hierarchical CP-net, resulting net described Figure 10. First,
identify starting ending points nested interaction protocol. starting point
nested interaction protocol Buyer1 sends Request-Good communicative act
Seller1 . ending point Buyer1 receives Request-Pay communicative act
Seller1 . model nested protocol end-points CP-net socket nodes
superpage, i.e. Interaction P rotocol: B11 S11 Request-Good input socket
nodes B13 S13 output socket node.
nested interaction protocol, Commitment Interaction P rotocol, represented
using separate CP-net, following principles outlined Sections 3 4. net
subpage main interaction protocol superpage. nested interaction protocol
starting ending points subpage correspond net port nodes. B1 S1
Request-Good places subpage input port nodes, B3 S3 place output
port node. nodes tagged IN/OUT port type tags correspondingly.
Then, substitution transition, denoted using HS (Hierarchy Substitution), connects corresponding socket places superpage. substitution transition conceals nested interaction protocol implementation net superpage, i.e.
Interaction P rotocol. nested protocol name guard compartments
mapped directly substitution transition name guard respectively. Consequently,
Figure 10 define substitution transition name Commitment substitution
guard determined [commit].
superpage subpage interface provided using hierarchy inscription.
hierarchy inscription indicated using dashed box next substitution transition. first line hierarchy inscription determines subpage identity, i.e.
366

fiRepresenting Conversations Scalable Overhearing

Commitment Interaction P rotocol example. Moreover, indicates substitution transition replaces corresponding subpage detailed implementation superpage. remaining hierarchy inscription lines introduce superpage subpage port
assignment. port assignment relates socket node superpage port node
subpage. substitution transition input socket nodes related IN-tagged
port nodes. Analogously, substitution transition output socket nodes correspond
OUT-tagged port nodes. Therefore, port assignment Figure 10 assigns net socket
port nodes following fashion: B11 S11 B1 S1 , Request-Good Request-Good
B13 S13 B3 S3 .
Finally, page hierarchy graph describes decomposition hierarchy (nesting)
different protocols (pages). CP-net pages, Interaction P rotocol
Commitment Interaction P rotocol, correspond page hierarchy graph nodes
(Figure 10). arc inscription indicates substitution transition, i.e. Commitment.

6. Representing Temporal Aspects Interactions
Two temporal interaction aspects specified FIPA (FIPA Specifications, 2003c).
section, show timed CP-nets (see also Appendix A) applied modelling
agent interactions involve temporal aspects, interaction duration, deadlines
message exchange, etc.
first aspect, duration, interaction activity time period. Two periods
distinguished: transmission time response time. transmission time indicates
time interval communicative act, sent one agent received
designated receiver agent. response time period denotes time interval
corresponding receiver agent performing task response incoming
communicative act.
second temporal aspect deadlines. Deadlines denote time limit
communicative act must sent. Otherwise, corresponding communicative act
considered invalid. issues addressed previous investigations
related agent interactions modelling using Petri nets.6
propose utilize timed CP-nets techniques represent temporal aspects
agent interactions. so, assume global clock.7 begin deadlines. Figure 11-a introduces AUML representation message deadlines. deadline keyword
variation communicative act sequence expressions described Section 4.
sets time constraint start transmission associated communicative act.
Figure 11-a, agent1 must send msg communicative act agent2 defined
deadline. deadline expires, msg communicative act considered invalid.
Figure 11-b shows timed CP-net implementation deadline sequence expression.
timed CP-net Figure 11-b defines additional MSG-TIME color set associated
net message places. MSG-TIME color set extends SG color set, described
Section 4, adding time stamp attribute message token. Thus, communicative
6. Cost et al. (1999, 2000) mention deadlines without presenting implementation details.
7. Implementing it, use private clock overhearing agent global clock Petri
net representation. Thus, time stamp message overhearers time corresponding
message overheard.

367

fiGutnik & Kaminka







ff fi


! ! "#$ % &'
! ! ()# % &'
! ! *+ $#$ % &'
! ! ,$#-./ # % !



ff
01 "#$23 1 "#$'
fiff
fi
! ! 4 /" % !
1 "#$2 1 "#$2

1()#2 1*+ $#$'
! ! 4 /". ,4 #% 4 /" '

5 1 "#$' 5 1()#'

ff
5 1*+ $#$'

fi
5 % &'

(a) AUML representation

(b) CP-net representation

Figure 11: Deadline sequence expression.
act token record hs, r, t, ci@[T ts]. @[..] expression denotes corresponding token
time stamp, whereas token time value indicated starting capital T. Accordingly, described message token ts time stamp. communicative act time limit
defined using val deadline parameter. Therefore, deadline sequence expression
semantics simulated using following transition guard: [T ts < deadline]. transition guard, comparing msg time stamp deadline parameter, guarantees
expired msg communicative act received.
turn representing interaction duration. AUML representation shown
Figure 12-a. AUML time intensive message notation used denote communicative act transmission time. rule communicative act arrows illustrated horizontally.
indicates message transmission time neglected. However, case
message transmission time significant, communicative act drawn slanted downwards.
vertical distance, arrowhead arrow tail, denotes message transmission time. Thus, communicative act msg1 , sent agent1 agent2 , t1
transmission time.

ff

fiff
fi
ff






!"
#$%& '(

)*+ ,)-+
2 2341 1 15 67
$%& % #.


ff

#


ff

fi ff
#$%& / '(
fiff

. & '(
2 234 156 7


+ , + ,

+ , +!"
. &% #.. & / '(

ff
0)
1+ 0) +
fi ff
88
+!"
0)

(a) AUML representation

(b) CP-net representation

Figure 12: Interaction duration.
368

fiRepresenting Conversations Scalable Overhearing

response time Figure 12-a indicated interaction thread length.
incoming msg1 communicative act causes agent2 perform task sending
response msg2 message. corresponding interaction thread duration denoted
t2 time period. Thus, time period specifies agent2 response time incoming
msg1 communicative act.
CP-net implementation interaction duration time periods shown Figure 12-b. communicative act transmission time illustrated using timed CP-nets
@+ operator. net transitions simulate communicative act transmission
agents. Therefore, representing transmission time t1 , CP-net transition adds t1
time period incoming message token time stamp. Accordingly, transition @ + t1
output arc expression denotes t1 delay time stamp outgoing token. Thus,
corresponding transition takes t1 time units consequently msg1 communicative act transmission time.
contrast communicative act transmission time, agent interaction response time
represented implicitly. Previously, defined MSG-TIME color set indicates
message token time stamps. Analogously, Figure 12-b introduce additional INTERSTATE-TIME color set. color set associated net agent places presents
possibility attach time stamps agent tokens well. Now, let us assume A2 B2
msg2 places contain single token each. circled 1 next corresponding place,
together multi-set inscription, indicates place current marking. Thus, agent
message place tokens ts1 ts2 time stamps respectively. ts1 time
stamp denotes time agent2 received msg1 communicative act sent
agent1 . ts2 time stamp indicates time agent2 ready send msg2
response message agent1 . Thus, agent2 response time t2 (Figure 12-a) ts2 ts1 .

7. Algorithm Concluding Example
final contribution paper skeleton procedure transforming AUML
conversation protocol diagram two interacting agents CP-net representation.
procedure semi-automatedit relies human fill detailsbut also
automated aspects. apply procedure complex multi-agent conversation protocol
involves many interaction building blocks already discussed.
procedure shown Algorithm 1. algorithm input AUML protocol
diagram algorithm creates, output, corresponding CP-net representation.
CP-net constructed iterations using queue. algorithm essentially creates
conversation net exploring interaction protocol breadth-first avoiding cycles.
Lines 1-2 create initiate algorithm queue, output CP-net, respectively.
queue, denoted S, holds initiating agent places current iteration.
places correspond interaction states initiate conversation interacting agents. lines 4-5, initial agent place A1 B1 created inserted
queue. A1 B1 place represents joint initial interaction state two agents. Lines
7-23 contain main loop.
enter main loop line 8 set curr variable first initiating agent
place queue. Lines 10-13 create CP-net components corresponding current
iteration follows. First, line 10, message places, associated curr agent place,
369

fiGutnik & Kaminka

Algorithm 1 Create Conversation Net(input:AU L,output:CP N )
1: new queue
2: CP N new CP net
3:
4:
5:

A1 B1 new agent place color information
S.enqueue(A1 B1 )

6:
7:
8:

empty
curr S.dequeue()

9:
10:
11:
12:
13:

P CreateM essageP laces(AU L, curr)
RP CreateResultingAgentP laces(AU L, curr, P )
(T R, AR) CreateT ransitionsAndArcs(AU L, curr, P, RP )
F ixColor(AU L, CP N, P, RP, R, AR)

14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

place p RP
p created current iteration
continue
p terminating place
S.enqueue(p)


CP N.places = CP N.places P SRP
CP.transitions = CP N.transitions
TR

CP N.arcs = CP N.arcs AR

24:
25:

return CP N

created using CreateM essageP laces procedure (which detail here).
procedure extracts communicative acts associated given interaction
state, AUML diagram. places correspond communicative acts,
take agents joint interaction state curr successor(s). line 11,
CreateResultingAgentP laces procedure creates agent places correspond interaction
state changes result communicative acts associated curr agent place (again
based AUML diagram). Then, CreateT ransitionsAndArcs procedure (line 12),
places connected using principles described Sections 36. Thus, CP-net
structure (net places, transitions arcs) created. Finally, line 13, F ixColor procedure adds token color elements CP-net structure, support deadlines, cardinality,
communicative act attributes.
Lines 15-19 determine resulting agent places inserted queue
iteration. non-terminating agent places, i.e. places correspond
interaction states terminate interaction, inserted queue lines 18-19.
However, one exception (lines 16-17): resulting agent place, already
handled algorithm, inserted back queue since inserting cause
infinite loop. Thereafter, completing current iteration, output CP-net, denoted
370

fiRepresenting Conversations Scalable Overhearing

CP N variable, updated according current iteration CP-net components lines
21-23. main loop iterates long queue empty. resulting CP-net
returnedline 25.

fi fi

fifi

ff

fi

fi



ff
















ff fi

fiff fiff
fiff
fiff

Figure 13: FIPA Contract Net Interaction Protocol using AUML.
demonstrate algorithm, use FIPA Contract Net Interaction
Protocol (FIPA Specifications, 2003b) (Figure 13). protocol allows interacting agents
negotiate. Initiator agent issues calls proposals using cf p communicative act.
P articipants may refuse counter-propose given deadline sending either
ref use propose message respectively. ref use message terminates interaction.
contrast, propose message continues corresponding interaction.
deadline expires, Initiator accept P articipant response messages. evaluates received P articipant proposals selects one, several,
agents perform requested task. Accepted proposal result sending
accept-proposal messages, remaining proposals rejected using reject-proposal
message. Reject-proposal terminates interaction corresponding P articipant.
hand, accept-proposal message commits P articipant perform requested task. successful completion, P articipant informs Initiator sending either
inform-done inform-result communicative act. However, case P articipant
failed accomplish task, communicates f ailure message.
371

fiGutnik & Kaminka

use algorithm introduced create CP-net, represents
FIPA Contract Net Interaction Protocol. corresponding CP-net model constructed
four iterations algorithm. Figure 14 shows CP-net representation second
iteration algorithm, Figure 15 shows CP-net representation fourth
final iteration.
Contract Net Interaction Protocol starts I1 P1 place, represents joint interaction state Initiator ready send cf p communicative act (I1 ) P articipant
waiting corresponding cf p message (P1 ). I1 P1 place created inserted
queue iterations main loop begin.
First iteration. curr variable set I1 P1 place. algorithm creates
net places, associated I1 P1 place, i.e. Cf p message place,
I2 P2 resulting agent place. I2 P2 place denotes interaction state Initiator
already sent cf p communicative act P articipant waiting response (I2 ) P articipant received cf p message deciding
appropriate response (P2 ). created using CreateM essageP laces
CreateResultingAgentP laces procedures, respectively.
Then, CreateT ransitionsAndArcs procedure line 12, connects three places
using simple asynchronous message building block shown Figure 1-b (Section 3).
line 13, color sets places determined, algorithm also handles
cardinality cf p communicative act, putting appropriate sequence expression
transition, using principles presented Figure 6-b (Section 4). Accordingly,
color set, associated I1 P1 place, changed INTER-STATE-CARD color set.
Since I2 P2 place terminating place, inserted queue.
Second iteration. curr set I2 P2 place. P articipant agent send,
response, either ref use propose communicative act. Ref use P ropose message
places created CreateM essageP laces (line 10), resulting places I3 P3 I4 P4 ,
corresponding results ref use propose communicative acts, respectively,
created CreateResultingAgentP laces (line 11). I3 P3 place represents joint
interaction state P articipant sent ref use message terminated (P3 ),
Initiator received it, terminated (I3 ). I4 P4 place represents joint state
P articipant sent propose message (P4 ), Initiator received
message considering response (I4 ).
line 12, I2 P2 , Ref use, I3 P3 , P ropose I4 P4 places connected using
XOR-decision building block presented Figure 3-b (Section 3). Then, F ixColor
procedure (line 13), adds appropriate token color attributes, allow deadline sequence
expression (on ref use propose messages) implemented shown
Figure 11-b (Section 6). I3 P3 place denotes terminating state, whereas I4 P4
place continues interaction. Thus, lines 18-19, I4 P4 place inserted
queue, next iteration algorithm. state net end second
iteration algorithm presented Figure 14.
Third iteration. curr set I4 P4 . Here, Initiator response P articipant
proposal either accept-proposal reject-proposal. CreateM essageP laces procedure line 10 thus creates corresponding Accept-Proposal Reject-Proposal message
places. accept-proposal reject-proposal messages cause interacting agents
transition I5 P5 I6 P6 places, respectively. agent places created using
372

fiRepresenting Conversations Scalable Overhearing

3 0 & ( )4
3 0

8


3 0 ; 4
$%

& '( )



**


ff fi ff












fi

3 0



!

./. 01 "





fi ff










fi

4

)'<=

&

)<& ' @ 5
4

!"

!

./.01"


ff
# #

) 5 3 . /

"

- -

!"


ff

5 67
&

& ()4
>

/




ff
fi



)4

)'<=

3 0 & '@ 5 1! 7










+ $+ $
' ,

4

fi

& '( )

fi ff

3 0

5 67

) 5 3 9 , 9:::7

2 2

)'<=

& ()4
->

?
7

.,3!
&

)?& '@7

3 0 A= ( 5 3 . & ( )4 ? & ( ) 4 ?
>
>
!
8 )? 3 ;4 )4 7
>
>

& '() <B =
5 & ( )4
C ! 67
3 0

5 67
E/ 0
3 0

& '()

5 1.F

& '()


C ! * :::
E / & ( ) 4 7 E /
>
E / & '@7
>
E / 0 ./. 015 67



<B =

< $7

A=(7
>

Figure 14: FIPA Contract Net Interaction Protocol using CP-net 2nd iteration.
CreateResultingAgentP laces procedure (line 11). I5 P5 place denotes interaction
state Initiator sent reject-proposal message terminated interaction (I5 ), P articipant received message terminated well (P5 ).
contrast, I6 P6 place represents interaction state Initiator sent acceptproposal message waiting response (I6 ), P articipant received
accept-proposal communicative act performing requested task sending
response (P6 ). Initiator agent sends exclusively either accept-proposal rejectproposal message. Thus, I4 P4 , Reject-Proposal, I5 P5 , Accept-Proposal I6 P6 places
connected using XOR-decision block (in CreateT ransitionsAndArcs procedure,
line 12).
F ixColor procedure line 13 operates follows: According interaction
protocol semantics, Initiator agent evaluates received P articipant proposals
deadline passes. thereafter, appropriate reject-proposal accept-proposal
communicative acts sent. Thus, F ixColor assigns MSG-TIME color set RejectProposal Accept-Proposal message places, creates [T ts >= deadline] transition guard associated transitions. transition guard guarantees Initiator
cannot send response deadline expires, valid P articipant responses
received. resulting I5 P5 agent place denotes terminating interaction state,
whereas I6 P6 agent place continues interaction. Thus, I6 P6 agent place
inserted queue.
Fourth iteration. curr set I6 P6 . place associated three communicative acts: inform-done, inform-result f ailure. inform-done informresult messages instances inf orm communicative act class. Thus, CreateMessagePlaces (line 10) creates two message places, Inf orm F ailure. line 11,
CreateResultingAgentP laces creates I7 P7 I8 P8 agent places. f ailure communicative act causes interacting agents transition I7 P7 agent place, inf orm
messages cause agents transition I8 P8 agent place. I7 P7 place represents
joint interaction state P articipant sent f ailure message terminated (P7 ),
373

fiGutnik & Kaminka

Initiator received f ailure communicative act terminated (I7 ).
hand, I8 P8 place denotes interaction state P articipant sent inf orm
message (either inform-done inform-result) terminated (P8 ), Initiator
received inf orm communicative act terminated (I8 ). inf orm f ailure communicative acts sent exclusively. Thus CreateT ransitionsAndArcs (line 12) connects
I6 P6 , F ailure, I7 P7 , Inf orm I8 P8 places using XOR-decision building block.
Then, F ixColor assigns [#t msg = inform-done #t msg = inform-result] transition
guard transition associated Inf orm message place. Since I7 P7
I8 P8 agent places represent terminating interaction states, inserted
queue, remains empty end current iteration. signifies end
conversion. complete conversation CP-net resulting iteration algorithm
shown Figure 15.
$%

& '( )

* *



fffi ff










, $ , $



.




.


fi

#



#









3

fi

'6 4!5 0 1




3



8 8

fi
+

+


;0 1-



fi

fi

=

4 1
L0 1

& 44!5

)'5D

fi ff

&

)5& 'G 7

5I

/-4!
&

)F& 'G?

FE& () =

@ )F4 EC=

7 & ( )=

)=

F
?

J ! K >?

& '( )

7 2/M

& '()

J !K * BBB 5 $?

?

L 0



5I

EH D( ?

L 0 E& 'G?

!"

L 0 1 /0/ 127 >?

! 7

/0/ 12"

ff
fi


2
fi





9 9

)'5D

7 >?

L 0 E& ()=






& '( )

4 1

0 1





ff

?

!E




F

0 E& ()=
.

4 1 H D( 7 4 / E& ()=

!"




ff

) 7 4 / 0 E& ( ) =

=



! 7

/0/ 12"

7 >?
&

!

!"



4 1

/0/ 12 "



ff



)=

)'5D






=

4 1 & 'G 7 2! ?

fi ff

!


ff





!"

/0/12"

fiff

"


fi







4 1

7 >?

@ ) 7 4 - ABBB?

4 1 C =


ff

fi ff

4 1



& '( )

' -

4 1 & ( )=

fi





:







<!
7 2
5

/ 2 <!
7

2
5- 1!"


ff
:

fi

Figure 15: FIPA Contract Net Interaction Protocol using CP-net 4th (and final)
iteration.
procedure outline guide conversion many 2-agent conversation protocols AUML CP-net equivalents. However, sufficiently developed
address general n-agent case. Appendix C presents complex example 3-agent conversation protocol, successfully converted manually, without guidance
algorithm. example incorporates many advanced features CP-net representation
technique would beyond scope many previous investigations.
374

fiRepresenting Conversations Scalable Overhearing

8. Summary Conclusions
recent years, open distributed MAS applications gained broad acceptance
multi-agent academic community real-world industry. result, increasing attention directed multi-agent conversation representation techniques.
particular, Petri nets recently shown provide viable representation approach
(Cost et al., 1999, 2000; Nowostawski et al., 2001; Mazouzi et al., 2002).
However, radically different approaches proposed using Petri nets modelling multi-agent conversations. Yet, relative strengths weaknesses proposed
techniques examined. work introduces novel classification previous investigations compares investigations addressing scalability
appropriateness overhearing tasks.
Based insights gained analysis, developed novel representation,
uses CP-nets places explicitly represent joint interaction states messages.
representation technique offers significant improvements (compared previous approaches) terms scalability, particularly suitable monitoring via overhearing.
systematically show representation covers essentially features required
model complex multi-agent conversations, defined FIPA conversation standards (FIPA Specifications, 2003c). include simple & complex interaction building
blocks (Section 3 & Appendix B), communicative act attributes multiple concurrent
conversations using CP-net (Section 4), nested & interleaved interactions using
hierarchical CP-nets (Section 5) temporal interaction attributes using timed CP-nets
(Section 6). developed techniques demonstrated, throughout paper,
complex interaction protocols defined FIPA conversation standards (see particular
example presented Appendix C). Previous approaches could handle
examples (though reduced scalability), shown cover
required features.
Finally, paper presented skeleton procedure semi-automatically converting
AUML protocol diagrams (the chosen FIPA representation standard) equivalent CPnet representation. demonstrated use challenging FIPA conversation protocol, difficult represent using previous approaches.
believe work assist motivate continuing research multi-agent
conversations including issues performance analysis, validation verification (Desel et al., 1997), agent conversation visualization, automated monitoring (Kaminka et al.,
2002; Busetta et al., 2001, 2002), deadlock detection (Khomenco & Koutny, 2000), debugging (Poutakidis et al., 2002) dynamic interpretation interaction protocols (Cranefield
et al., 2002; de Silva et al., 2003). Naturally, issues remain open future work.
example, presented procedure addresses AUML protocol diagrams representing two
agent roles. plan investigate n-agent version future.

Acknowledgments
authors would like thank anonymous JAIR reviewers many useful informative comments. Minor subsets work also published LNAI book chapter
(Gutnik & Kaminka, 2004b). K. Ushi deserves many thanks.
375

fiGutnik & Kaminka

Appendix A. Brief Introduction Petri Nets
Petri nets (Petri Nets site, 2003) widespread, established methodology representing
reasoning distributed systems, combining graphical representation comprehensive mathematical theory. One version Petri nets called Place/Transition nets
(PT-nets) (Reisig, 1985). PT-net bipartite directed graph node either
place transition (Figure 16). net places transitions indicated
circles rectangles respectively. PT-net arcs support place transition
transition place connections, never connections two places two
transitions. arc direction determines input/output characteristics place
transition connected. Thus, given arc, P , connecting place P transition ,
say place P input place transition vice versa transition
output transition place P . P arc considered output arc place P
input arc transition .
















(a) firing




(b) firing

Figure 16: PT-net example.
PT-net place may marked small black dots called tokens. arc expression
integer, determines number tokens associated corresponding arc.
convention, arc expression equal 1 omitted. specific transition enabled
input places marking satisfies appropriate arc expressions. example,
consider arc P arc connect place P transition . Thus, given
arc arc expression 2, say transition enabled
place P marked two tokens. case transition enabled, may fire/occur.
transition occurrence removes tokens transition input places puts tokens
transition output places specified arc expressions corresponding
input/output arcs. Thus, Figures 16-a 16-b, demonstrate PT-net marking
transition firing respectively.
Although computationally equivalent, different version Petri nets, called Colored
Petri nets (CP-nets) (Jensen, 1997a, 1997b, 1997c), offers greater flexibility compactly
representing complex systems. Similarly PT-net model, CP-nets consist net places,
net transitions arcs connecting them. However, CP-nets, tokens single
bits, complex, structured, information carriers. type additional information carried token, called token color, may simple (e.g., integer
string), complex (e.g. record tuple). place declared place color set
376

fiRepresenting Conversations Scalable Overhearing

match tokens particular colors. CP-net place marking token multi-set (i.e.,
set member may appear once) corresponding appropriate place
color set. CP-net arcs pass token multi-sets places transitions. CP-net arc
expressions evaluate token multi-sets may involve complex calculation procedures
token variables declared associated corresponding arcs.
CP-net model introduces additional extensions PT-nets. Transition guards
boolean expressions, constrain transition firings. transition guard associated
transition tests tokens pass transition, enable transition
firings guard successfully matched (i.e., test evaluates true). CP-net
transition guards, together places color sets arc expressions, appear part
net inscriptions CP-net.
order visualize manage complexity large CP-nets, hierarchical CP-nets
(Huber, Jensen, & Shapiro, 1991; Jensen, 1997a) allow hierarchical representations CPnets, sub-CP nets re-used higher-level CP nets, abstracted away
them. Hierarchical CP-nets built pages, CP-nets. Superpages
present higher level hierarchy, CP-nets refer subpages, addition
transitions places. subpage may also function superpage subpages.
way, multiple hierarchy levels used hierarchical CP-net structure.
relationship superpage subpage defined substitution transition, substitutes corresponding subpage instance CP-net superpage structure
transition superpage. substitution transition hierarchy inscription supplies
exact mapping superpage places connected substitution transition (called
socket nodes), subpage places (called port nodes). port types determine
characteristics socket node port node mappings. complete CP-net hierarchical
structure presented using page hierarchy graph, directed graph vertices correspond pages, directed edges correspond direct superpage-subpage relationships.
Timed CP-nets (Jensen, 1997b) extend CP-nets support representation temporal aspects using global clock. Timed CP-net tokens additional color attribute
called time stamp, refers earliest time token may used. Time
stamps used arc expression transition guards, enable timed-transition
satisfies two conditions: (i) transition color enabled, i.e. satisfies
constraints defined arc expression transition guards; (ii) tokens ready,
i.e. time global clock equal greater tokens time stamps.
transition fire.

Appendix B. Additional Examples Conversation Representation
Building Blocks
appendix presents additional interaction building blocks already described Section 3. first AND-parallel messages interaction (AUML representation shown Figure 17-a). Here, sender agent1 sends msg1 message
agent2 msg2 message agent3 . However, order two communicative acts
unconstrained.
representation AND-parallel CP-net representation shown Figure 17-b.
A1 B1 C1 , A2 B2 , A2 C2 , msg1 msg2 places defined similarly Figures 3-b
377

fiGutnik & Kaminka




ff fi























01
0



ff fi

ff fi



















ff

ff



!"#$ %& ' ()' (
!"#$#*%& ' ()' ()'+(
,$ %& ()(
-' ./ (

(a) AUML representation

(b) CP-net representation

Figure 17: AND-parallel messages interaction.
4-b Section 3. However, also define two additional intermediate agent places, A01 B2 C1
A001 B1 C2 . A01 B2 C1 place represents joint interaction state agent1 sent
msg1 message agent2 ready send msg2 communicative act agent3
(A1 ), agent2 received msg1 message (B2 ) agent3 waiting receive msg2
communicative act (C1 ). A001 B1 C2 place represents joint interaction state
agent1 ready send msg1 message agent2 already sent msg2 communicative act agent3 (A001 ), agent2 waiting receive msg1 message (B1 ) agent3
received msg2 communicative act (C2 ). places enable agent1 send
communicative acts concurrently. Four transitions connect appropriate places respectively. behavior transitions connecting A01 B2 C1 A2 B2 A001 B1 C2 A2 C2
similar described above. transitions A1 B1 C1 A01 B2 C1 A1 B1 C1 A001 B1 C2
triggered receiving messages msg1 msg2 , respectively. However, transitions consume message token since used triggering transitions
A01 B2 C1 A2 B2 A001 B1 C2 A2 C2 . achieved adding appropriate message
place output place corresponding transition.
second AUML interaction building block, shown Figure 18-a, message
sequence interaction, similar AND-parallel. However, message sequence
interaction defines explicitly order transmitted messages. Using 1/msg1
2/msg2 notation, Figure 18-a specifies msg1 message sent
sending msg2 .
Figure 18-b shows corresponding CP-net representation. A1 B1 C1 , A2 B2 , A2 C2 ,
msg1 msg2 places defined before. However, CP-net implementation presents
additional intermediate agent placeA01 B2 C1which identical corresponding
378

fiRepresenting Conversations Scalable Overhearing



fffi


ff










fffi


ff













fffi


ff
0









fffi


ff







!"
#$%& ' ( ) * + )* "
#$%& %, ' ( ) * +)* +)- * "
. & ' ( * + * "
/ ) * "



(a) AUML representation

(b) CP-net representation

Figure 18: Sequence messages interaction.
intermediate agent place Figure 17-b. A01 B2 C1 defined output place
A1 B1 C1 A2 B2 transition. thus guarantees msg2 communicative act
sent (represented A01 B2 C1 A2 C2 transition) upon completion msg1
transmission (the A1 B1 C1 A2 B2 transition).
last interaction present synchronized messages interaction, shown Figure 19-a. Here, agent3 simultaneously receives msg1 agent1 msg2 agent2 .
AUML, constraint annotated merging two communicative act arrows
horizontal bar single output arrow.


fffi




fffi














fffi

fi1




fffi








/

fffi


.

.



.
.
/0

.
.
/


!"#$ %& '( )'(
!"#$ #* %& '( )'( )'+(
,$ %& ( ) (
- ' .(



(a) AUML representation

(b) CP-net representation

Figure 19: Synchronized messages interaction.
379

fiGutnik & Kaminka

Figure 19-b illustrates CP-net implementation synchronized messages interaction.
previous examples, define A1 C1 , B1 C1 , msg1 , msg2 A2 B2 C2 places.
additionally define two intermediate agent places, A2 C10 B2 C100 . A2 C10 place represents joint interaction state agent1 sent msg1 agent3 (A2 ), agent3
received it, however agent3 also waiting receive msg2 (C10 ). B2 C100 place represents
joint interaction state agent2 sent msg2 agent3 (B2 ), agent3
received it, however agent3 also waiting receive msg1 (C100 ). places guarantee
interaction transition A2 B2 C2 state msg1 msg2
received agent3 .

Appendix C. Example Complex Interaction Protocol
present example complex 3-agent conversation protocol, manually converted CP-net representation using building blocks paper. conversation
protocol addressed FIPA Brokering Interaction Protocol (FIPA Specifications,
2003a). interaction protocol incorporates many advanced conversation features
representation nesting, communicative act sequence expression, message guards
etc. AUML representation shown Figure 20.
Initiator agent begins interaction sending proxy message Broker
agent. proxy communicative act contains requested proxied-communicative-act
part argument list. Broker agent processes request responds either
agree ref use message. Communication ref use message terminates interaction.
Broker agent agreed function proxy, locates agents matching
Initiator request. agent found, Broker agent communicates
failure-no-match message interaction terminates. Otherwise, Broker agent
begins interactions matching agents. agent, Broker informs
Initiator, sending either inform-done-proxy failure-proxy communicative act.
failure-proxy communicative act terminates sub-protocol interaction matching
agent question. inform-done-proxy message continues interaction. subprotocol progresses, Broker forwards received responses Initiator agent using
reply-message-sub-protocol communicative acts. However, failures
explicitly returned sub-protocol interaction (e.g., agent executing
sub-protocol failed). case Broker agent detects failure, communicates
failure-brokering message, terminates sub-protocol interaction.
CP-net representation FIPA Brokering Interaction Protocol shown Figure 21. Brokering Interaction Protocol starts I1 B1 place. I1 B1 place represents joint interaction state Initiator ready send proxy communicative
act (I1 ) Broker waiting receive (B1 ). proxy communicative act causes
interacting agents transition I2 B2 . place denotes interaction state
Initiator already sent proxy message Broker (I2 ) Broker received (B2 ).
Broker agent send, response, either ref use agree communicative act.
CP-net component implemented using XOR-decision building block presented
Section 3. ref use message causes agents transition I3 B3 place thus
terminate interaction. place corresponds Broker sending ref use message
terminating (B3 ), Initiator receiving message terminating (I3 ).
380

fiRepresenting Conversations Scalable Overhearing


fffi ff



























!










!



" ##


!





!
!
!

Figure 20: FIPA Brokering Interaction Protocol - AUML representation.
hand, agree communicative act causes agents transition I4 B4 place,
represents joint interaction state Broker sent agree message
Initiator (and trying locate receivers proxied message),
Initiator received agree message.
Broker agents search suitable receivers may result two alternatives. First,
case matching agents found, interaction terminates I5 B5 agent place.
joint interaction place corresponds interaction state Broker sent
failure-no-match communicative act (B5 ), Initiator received message terminated (I5 ). second alternative suitable agents found. Then, Broker
starts sending proxied-communicative-act messages agents established list
designated receivers, i.e. TARGET-LIST. first proxied-communicative-act message causes interacting agents transition I4 B6 P1 place. I4 B6 P1 place denotes
joint interaction state three agents: Initiator, Broker P articipant (the receiver).
381

fiGutnik & Kaminka






fffi

71/





ff

- 8 7 29 7> /


B * 71/ 8 7 29 7> /





2 9KB



! "#$% & '(


fffi











$% #/0% % # & 1

!

2 3 "# $%4 2 3 " #$% (

$% #/0% % #/5 & 1
2 3 "# $%4 2 3 " #$%4


9






ff


!




fffi



ff

! - .$% #$% & '(







fffi

! %) # & * + +,,,(





ff

2 3 "# $% (

! - . 6 & 789 (




!



$% #/0% % #/5/- 6 &
$% # /0% % #/54

1



- 6 (




$/
:2 9


IJ ff






H2 7!/


9/% *

L

! : 0"& 1 3 "# $%4



=

3 " #$%49 3%) #4

71/



3-. $% # $% (

- 8 7 29 7> /

G



ff



K % "#% K? L ? L

fffi



fffi





>2 ! & '(
! % " #%&781 % " #%/


K * 7L

E fiF


9




ff

G

! % " #%/ ; 0% & " #$% < 79 = ,,(

; 0% < 79 = ? ,,, / @(
>2 3: 0"(






>2 * 3 " #$% (

A7 281 9 &B9 C

K % " #% K7N @L* 7N @L

>2 9 3%) #(
>2 73- 6(

*
H2 7!/




P





ff




*

71/
- 8 7 29 7> /




fffi




8 /6 8/





*


fffi










ff








9

R /0S/9 !

Q0
*

R / 892 9 7 8 /9 !


fffi


ff





X/ XX (
W
71/- 8 7 2 9 7> /


*


P

U78



XY Z/

9 !

H2 7!/


V


fffi


ff fi



ff



ff

*

XY Z/

[
W

(
[
(
W


*




9/R (

XY Z/ Z Z (

R /0S/




*! /
: 2 /

*

0S/
*

*


fffi


fffi



ff



ff








9 !




Figure 21: FIPA Brokering Interaction Protocol - CP-net representation.
Initiator individual state remains unchanged (I4 ) since proxied-communicative-act
message starts interaction Broker P articipant. Broker individual
state (B6 ) denotes designated agents found proxied-communicative382

fiRepresenting Conversations Scalable Overhearing

act messages ready sent, P articipant waiting receive interaction
initiating communicative act (P1 ). proxied-communicative-act message place also
connected output place transition. message place used part
CP-net XOR-decision structure, enables Broker agent send either failure-nomatch proxied-communicative-act, respectively. Thus, token denoting proxiedcommunicative-act message, must consumed transition.
Thus, multiple proxied-communicative-act messages sent P articipants.
implemented similarly broadcast sequence expression implementation (Section 4).
Furthermore, proxied-communicative-act type verified type requested
proxied communicative act, obtained original proxy message content.
use Proxied-Communicative-Act-Type message type place implement CPnet component similarly Figure 8. proxied-communicative-act message causes
interacting agents transition I4 B7 P1 B6 P1 places.
B6 P1 place corresponds interaction Broker P articipant
agents. represents joint interaction state Broker ready send proxiedcommunicative-act message P articipant (B6 ), P articipant waiting message
(P1 ). fact, B6 P1 place initiates nested interaction protocol results B10 P3
place. B10 P3 place represents joint interaction state P articipant sent
reply-message communicative act terminated (P3 ), Broker received
message (B10 ). example, chosen FIPA Query Interaction Protocol (FIPA
Specifications, 2003d) (Figures 78) interaction sub-protocol. CP-net component,
implementing nested interaction sub-protocol, modeled using principles described
Section 5. Consequently, interaction sub-protocol concealed using Query-SubProtocol substitution transition. B6 P1 , proxied-communicative-act B10 P3 places
determine substitution transition socket nodes. socket nodes assigned CPnet port nodes Figure 8 follows. B6 P1 proxied-communicative-act places
assigned I1 P1 query input port nodes, B10 P3 place assigned
I3 P3 , I5 P5 I6 P6 output port nodes.
turn I4 B7 P1 place. contrast B6 P1 place, place corresponds
main interaction protocol. I4 B7 P1 place represents joint interaction state
Initiator waiting Broker respond (I4 ), Broker ready send appropriate response communicative act (B7 ), best Initiators knowledge interaction
P articipant yet begun (P1 ). Broker agent send one two messages,
either failure-proxy inform-done-proxy, depending whether succeeded
send proxied-communicative-act message P articipant. failure-proxy message
causes agents terminate interaction corresponding P articipant agent
transition I6 B8 P1 place. place denotes joint interaction state Initiator
received failure-proxy communicative act terminated (I6 ), Broker sent
failure-proxy message terminated well (B8 ) interaction P articipant
agent never started (P1 ). hand, inform-done-proxy causes agents
transition I7 B9 P2 place. I7 B9 P2 place represents interaction state Broker
sent inform-done-proxy message (B9 ), Initiator received (I7 ), P articipant
begun interaction Broker agent (P2 ). Again, represented using
XOR-decision building block.
383

fiGutnik & Kaminka

Finally, Broker agent either send reply-message-sub-protocol failurebrokering communicative act. failure-brokering message causes interacting agents
transition I8 B11 P2 place. place indicates Broker sent failure-brokering
message terminated (B11 ), Initiator received message terminated (I8 ),
P articipant terminated interaction Broker agent (P2 ). replymessage-sub-protocol communicative act causes agents transition I9 B12 P3 place.
I9 B12 P3 place indicates Broker sent reply-message-sub-protocol message
terminated (B12 ), Initiator received message terminated (I9 ), P articipant
successfully completed nested sub-protocol Broker agent terminated
well (P3 ). Thus, B10 P3 place, denoting successful completion nested sub-protocol,
also corresponding transition input place.

References
AUML site (2003). Agent unified modeling language, www.auml.org..
Busetta, P., Dona, A., & Nori, M. (2002). Channelled multicast group communications.
Proceedings AAMAS-02.
Busetta, P., Serafini, L., Singh, D., & Zini, F. (2001). Extending multi-agent cooperation
overhearing. Proceedings CoopIS-01.
ChaibDraa, B. (2002). Trends agent communication languages. Computational Intelligence, 18 (2), 89101.
Cost, R. S. (1999). framework developing conversational agents. Ph.D. thesis, Department Computer Science, University Maryland.
Cost, R. S., Chen, Y., Finin, T., Labrou, Y., & Peng, Y. (1999). Modeling agent conversations
coloured Petri nets. Proceedings Workshop Specifying Implementing Conversation Policies, Third International Conference Autonomous Agents
(Agents-99), Seattle, Washington.
Cost, R. S., Chen, Y., Finin, T., Labrou, Y., & Peng, Y. (2000). Using coloured petri nets
conversation modeling. Dignum, F., & Greaves, M. (Eds.), Issues Agent
Communications, Lecture notes Computer Science, pp. 178192. Springer-Verlag.
Cranefield, S., Purvis, M., Nowostawski, M., & Hwang, P. (2002). Ontologies interaction protocols. Proceedings Workshop Ontologies Agent Systems,
First International Joint Conference Autonomous Agents & Multi-Agent Systems
(AAMAS-02), Bologna, Italy.
de Silva, L. P., Winikoff, M., & Liu, W. (2003). Extending agents transmitting protocols
open systems. Proceedings Workshop Challenges Open Agent Systems, Second International Joint Conference Autonomous Agents & Multi-Agent
Systems (AAMAS-03), Melbourne, Australia.
Desel, J., Oberweis, A., & Zimmer, T. (1997). Validation information system models: Petri
nets test case generation. Proceedings 1997 IEEE International Conference Systems, Man Cybernetics: Computational Cybernetics Simulation,
pp. 34013406, Orlando, Florida.
384

fiRepresenting Conversations Scalable Overhearing

Finin, T., Labrou, Y., & Mayfield, J. (1997). KQML agent communication language.
Bradshaw, J. (Ed.), Software Agents. MIT Press.
FIPA site (2003). Fipa - Foundation Intelligent Physical Agents, www.fipa.org..
FIPA Specifications (2003a). Fipa Brokering Interaction Protocol Specification, version H,
www.fipa.org/specs/fipa0000033/..
FIPA Specifications (2003b). Fipa Contract Net Interaction Protocol Specification, version
H, www.fipa.org/specs/fipa0000029/..
FIPA Specifications (2003c). Fipa Interaction Protocol Library Specification, version E,
www.fipa.org/specs/fipa0000025/..
FIPA Specifications (2003d). Fipa Query Interaction Protocol Specification, version H,
www.fipa.org/specs/fipa0000027/..
Gutnik, G., & Kaminka, G. (2004a). Towards formal approach overhearing: Algorithms
conversation identification. Proceedings AAMAS-04.
Gutnik, G., & Kaminka, K. A. (2004b). scalable Petri net representation interaction
protocols overhearing.. van Eijk, R. M., Huget, M., & Dignum, F. (Eds.), Agent
Communication LNAI 3396: International Workshop Agent Communication, AC
2004, New York, NY, USA, pp. 5064. Springer-Verlag.
Hameurlain, N. (2003). MIP-Nets: Refinement open protocols modeling analysis
complex interactions multi-agent systems. Proceedings 3rd International
Central Eastern European Conference Multi-Agent Systems (CEEMAS-03), pp.
423434, Prague, Czech Republic.
Huber, P., Jensen, K., & Shapiro, R. M. (1991). Hierarchies Coloured Petri nets.
Jensen, K., & Rozenberg, G. (Eds.), High-level Petri Nets: Theory Application,
pp. 215243. Springer-Verlag.
Jensen, K. (1997a). Coloured Petri Nets. Basic Concepts, Analysis Methods Practical
Use, Vol. 1. Springer-Verlag.
Jensen, K. (1997b). Coloured Petri Nets. Basic Concepts, Analysis Methods Practical
Use, Vol. 2. Springer-Verlag.
Jensen, K. (1997c). Coloured Petri Nets. Basic Concepts, Analysis Methods Practical
Use, Vol. 3. Springer-Verlag.
Kaminka, G., Pynadath, D., & Tambe, M. (2002). Monitoring teams overhearing:
multi-agent plan-recognition approach. JAIR, 17, 83135.
Khomenco, V., & Koutny, M. (2000). LP deadlock checking using partial order dependencies. Proceedings 11th International Conference Concurrency Theory
(CONCUR-00), pp. 410425, Pennsylvania State University, Pennsylvania.
Kone, M. T., Shimazu, A., & Nakajima, T. (2000). state art agent communication languages. Knowledge Information Systems, 2, 258284.
Legras, F. (2002). Using overhearing local group formation. Proceedings AAMAS02.
385

fiGutnik & Kaminka

Lin, F., Norrie, D. H., Shen, W., & Kremer, R. (2000). schema-based approach specifying conversation policies. Dignum, F., & Greaves, M. (Eds.), Issues Agent
Communications, Lecture notes Computer Science, pp. 193204. Springer-Verlag.
Ling, S., & Loke, S. W. (2003). MIP-Nets: compositional model multi-agent interaction.
Proceedings 3rd International Central Eastern European Conference
Multi-Agent Systems (CEEMAS-03), pp. 6172, Prague, Czech Republic.
Mazouzi, H., Fallah-Seghrouchni, A. E., & Haddad, S. (2002). Open protocol design
complex interactions multi-agent systems. Proceedings First International
Joint Conference Autonomous Agents & Multi-Agent Systems (AAMAS-02), pp.
517526, Bologna, Italy.
Milner, R., Harper, R., & Tofte, M. (1990). Definition Standard ML. MIT Press.
Moldt, D., & Wienberg, F. (1997). Multi-agent systems based Coloured Petri nets.
Proceedings 18th International Conference Application Theory Petri
Nets (ICATPN-97), pp. 82101, Toulouse, France.
Novick, D., & Ward, K. (1993). Mutual beliefs multiple conversants: computational
model collaboration air traffic control. Proceedings AAAI-93, pp. 196201.
Nowostawski, M., Purvis, M., & Cranefield, S. (2001). layered approach modeling
agent conversations. Proceedings Second International Workshop Infrastructure Agents, MAS Scalable MAS, Fifth International Conference
Autonomous Agents, pp. 163170, Montreal, Canada.
Odell, J., Parunak, H. V. D., & Bauer, B. (2000). Extending UML design multiagent systems. Proceedings AAAI-2000 Workshop Agent-Oriented Information Systems (AOIS-00).
Odell, J., Parunak, H. V. D., & Bauer, B. (2001a). Agent UML: formalism specifying
multi-agent interactions. Ciancarini, P., & Wooldridge, M. (Eds.), Agent-Oriented
Software Engineering, pp. 91103. Springer-Verlag, Berlin.
Odell, J., Parunak, H. V. D., & Bauer, B. (2001b). Representing agent interaction protocols UML. Ciancarini, P., & Wooldridge, M. (Eds.), Agent-Oriented Software
Engineering, pp. 121140. Springer-Verlag, Berlin.
Parunak, H. V. D. (1996). Visualizing agent conversations: Using enhances Dooley graphs
agent design analysis. Proceedings Second International Conference
Multi-Agent Systems (ICMAS-96).
Paurobally, S., & Cunningham, J. (2003). Achieving common interaction protocols open
agent environments. Proceedings Workshop Challenges Open Agent
Systems, Second International Joint Conference Autonomous Agents & MultiAgent Systems (AAMAS-03), Melbourne, Australia.
Paurobally, S., Cunningham, J., & Jennings, N. R. (2003). Ensuring consistency
joint beliefs interacting agents. Proceedings Second International Joint
Conference Autonomous Agents & Multi-Agent Systems (AAMAS-03), Melbourne,
Australia.
386

fiRepresenting Conversations Scalable Overhearing

Petri Nets site (2003). Petri nets world: Online services international petri nets
community, www.daimi.au.dk/petrinets..
Poutakidis, D., Padgham, L., & Winikoff, M. (2002). Debugging multi-agent systems using
design artifacts: case interaction protocols. Proceedings First International Joint Conference Autonomous Agents & Multi-Agent Systems (AAMAS-02),
pp. 960967, Bologna, Italy.
Purvis, M. K., Hwang, P., Purvis, M. A., Cranefield, S. J., & Schievink, M. (2002). Interaction protocols network environmental problem solvers. Proceedings
2002 iEMSs International Meeting:Integrated Assessment Decision Support
(iEMSs 2002), pp. 318323, Lugano, Switzerland.
Ramos, F., Frausto, J., & Camargo, F. (2002). methodology modeling interactions
cooperative information systems using Coloured Petri nets. International Journal
Software Engineering Knowledge Engineering, 12 (6), 619636.
Reisig, W. (1985). Petri Nets: Introduction. Springer-Verlag.
Rossi, S., & Busetta, P. (2004). Towards monitoring group interactions social roles
via overhearing. Proceedings CIA-04, pp. 4761, Erfurt, Germany.
Smith, I. A., & Cohen, P. R. (1996). Toward semantics agent communications
language based speech-acts. Proceedings AAAI-96.
Wikstrom, A. (1987). Functional Programming using Standard ML. International Series
Computer Science. Prentice-Hall.
Xu, H., & Shatz, S. M. (2001). agent-based Petri net model application
seller/buyer design electronic commerce. Proceedings 5th International
Symposium Autonomous Decentralized Systems (ISAD-01), pp. 1118, Dallas,
Texas, USA.

387

fi ff
fiff ff


!ff#"%$&(')$**+,-$..0/1$+2

34567 8%*9:ff*&<;>=5
"8%*$:ff*+

?@BADCFE(GIH4JDKMLON(PDCQHRSFHT7RVUXWDCFEYPDKYW[Z\N(]^`_INbadceNb^fCFTgW
hjilkmilkgn-opQqsrqut-vYwexzy{n-kg|~} -

r#kjg-y{xeon-kkr#kgjItbrrtk

xYnsfirnqso#
F0 fi ff
F0 fi ggffs

-Yg Q 4

`sF
f{s)0! (40)0)4Q1s%!<40)>>04sff>Q04s0<0! s)> Y>0(

<fiI>ff>040 4<u04) 0 s>0s0 >g%10 sl 004s0<0! s!<>ffff
!ss740f0Y>ff>04b>4ff(0ffs#>44D0s4ffsI>usff 0 s>
>0e40)0)>DF)0 0f1sYs0 0I40)0)0< 00 efi
s40)>7 0 l040!> 7 > <1[s\40)0)D4>>)!l
fi 4<zsF0(s0 >Q10 se ef04) 0 s>0`!<>ffff!ss-0)g>40<!4ff
0 4fiff4ff 0fffi0I1s(s0 0)10 s>!0)> zfiI!<>ffff I004) <
!ssF(04) <z!<>ffff0fi<4>!` 7s0< >!0)> s)sff 0
> ff<>4>0401s00> 1s s4 > <f>Is 0f40)0))
<e7 0D>!0)>40)0)44fi1sV444 s0<efiz04) <e!<>fffffi0<! 0
Ys0fs4ff 0`40)0)>
)`!>Q0Y04) <!<>ffff00I 0 ss040)0))(4 > 0<fi
s>0 zIs0 >04sff>)>4ff4(7 7)fffi<`>f!Y <-> -4 7)ff
!< 7b)>0407)!`)40)4>07 Ffs>0)!sz)s%sz0s> !<e
0
040 0 s>Q)> gs0 0 7)fff>0)>4ff>!0) 0< 74) <
!<>ffffY)F1sY0f(Q<0 sb z!s7>0<!(s> 4sg>g> b> !)F04s<>)
ff>ffs040 < 0 s%>07s> - 7)ff04) <!<>ffff4><0<f0`<0 ss
I040>-<s!0<se0fs0 >>e04) <I!<>ffffe!s<4 I0s> 740
04) <!<>ffff)<0 ss> l!> bff 0<)sff 0 s> ff<>4Y0>
> )>0s%ff 0<7>D!> ff 0<04e0s%!> !4<0!s7ff 0<4





ff







fi



(
!#"%$'&)(+*,*+-.!"/&10243+"%57643,898:3,445:';=<ff6>?&1"%57"%576@AB&"/&10%&CD"FEff6.?G8:3,4'&102H1IKJL9M-N3,4C } -
JL9M-'@PEQ$45:R2$S3,8:H/6T?G3+0%"%5:R157?G3+"/&CU5:V"%$'&D(+*,*(WRX6>?&1"%57"%576@Y5:HZ3["/&>?6,023,8]\^JL_L!9`\W?G8:3,4'&10@
6,?4"%5:>a3,8Bbc:d1cfecV>a3+g,&H/?G3, h"a5:HiG3,H/&CT6T"/&>?6,023,8j0%&1;,0%&H%H%576WH/&3+02R2$WEQ57"%$T3,k3,C4>a5:H%H%57iG87&
$'&l'025:H/"%5:RMR13,8:87&Cm $ JL$'&ff"/&>?6,023,8'm $ $'&l'025:H/"%5:RM5:H 3,n5:4H/"%3,4RX&ff6,o4"%$'&`;,&'&1023,8'm'prqtsvuxw+y2(zy1{1{1{:|
o}3,>a5:87~6,o$'&l'025:H/"%5:R1H1@ffEQ$45:R2$k5:HC'&X '&CTiz~W3D?G3+023,>&1"/&102571&CW0%&8:3'3+"%576k6,oB"%$'&=6,?4"%5:>a3,8QRX6H/"
o}l44RX"%576W6,&10Z"%$'&.H/&3+02R2$SH/?G3,RX& } - 5:H5:C'&"%5:R13,8Q"/6[JL9M-D&X'RX&1?4")"%$43+")57"l4H/&H3D0%&RX&"%87~
C'&1,&876,?&C>&1"%$'6^C@4R13,8:87&C.d%X7z%%+d/X@4"/65:>?40%6,&B"%$'&]m $ $'&l'025:H/"%5:Rnq}Y3,H%8:l4>Z@G(+*,*+-3|
_L&1;,0%&H%H%576?G8:3,4'&102HMR13+0%0%~6l'"ff3KH/&3+02R2$6,&10jH/&1"%H6,o;,63,8:H1@'H/"%3+0%"%5:';ot0%6>"%$'&Q;,63,8G;57,&5:
"%$'&P?G8:3,445:';?40%6,iG87&> JL$'&P0%&8:3'3+"%576"%$43+"L87&3,C4Hff"/6"%$'&m'pr$'&l'025:H/"%5:R1Hff5:Hff"/63,H%H%l4>&P"%$43+"j"%$'&
RX6H/"M6,o3,~nH/&1"M6,o>6,0%&j"%$43,s;,63,8:HM&zl43,8:H "%$'&LRX6H/"M6,o "%$'&L>6H/"MRX6H/"%87~H%l'iGH/&1" 6,oH%571&js JL$'&
$'&l'025:H/"%5:RR13,l44C'&10%&H/"%5:>a3+"/&]"%$'&nRX6H/"B6,o3;,63,8H/&1"1@57o"%$'&10%&3+0%&5:"/&1023,RX"%5764HP5:,687^5:';>6,0%&
G 2 Wf22XXG/2 /`jh]}2/ L%/%F2j2+%F !A2+2j2 !XX!4
`!2F% ! h[! %/Y/%F FP,hhA!2!,+ff 1 Y+h
jh z,/Q +2,h/


G



f

$**+ 8 0

" 8

fi

4

4

"%$43,=s;,63,8:H1@ iGl'"Q57"QR13,N'&1,&10Y6,&10%&H/"%5:>a3+"/& !"%$'&]JL9M-3,4C } - "/&>?6,023,8?G8:3,4'&102H1@4"%$'&
RX6H/"P6,off3H/&1"P6,oM;,63,8:HB5:HQ"%$'&n>a5:45:>]l4>"/6,"%3,8 &X^&R1l'"%576N"%5:>&,@6,0B>a3+g,&H/?G3,@6,o3,~?G8:3,="%$43+"
3,R2$457&1,&HB"%$'&n;,63,8:H1@iGl'"Y"%$'&H%3,>&0%&8:3'3+"%576.R13,.i&nl4H/&CN"/6C'&10257,&n$'&l'025:H/"%5:R1HQot6,0B0%&1;,0%&H%H%576
?G8:3,445:';EQ57"%$=C45&10%&"Q?G8:3,=H/"/02l4RX"%l'0%&HY3,4CNRX6H/"P>&3,H%l'0%&H1@Mc c:@H/&zl'&"%5:3,8A?G8:3,4HLEQ57"%$=H%l4>
6,o3,RX"%576SRX6H/"%Hq}Y3,H%8:l4>Z@jff6'&1"1@LY&X'&10@Y(+*,*|2@Q6,0"/6&H/"%5:>a3+"/&N0%&H/6l'02RX&NRX64H%l4>?4"%576
q}Y3,H%8:l4>Y&X'&10@M(+*,*'w| 6,02>a3,8:87~,@"%$'&)m'p$'&l'025:H/"%5:R+@ot6,0n3,~s@ R13,Di&C'&X '&CD3,HK"%$'&
H/68:l'"%576K"/6B3Y0%&8:3'3+"%576n6,o4"%$'&`6,?4"%5:>a3,8^RX6H/" &zl43+"%576qtg^'6EQn3,H "%$'&jff&8:8:>a3,K&zl43+"%576 |EQ$45:R2$
R2$43+023,RX"/&102571&H"%$'&n6,?4"%5:>a3,8RX6H/"Bo}l44RX"%576N6,&10"%$'&aH/&3+02R2$#H/?G3,RX& RX6>?G87&1"/&aH/68:l'"%576N"/6Z"%$'&
0%&8:3^&C[&zl43+"%576W5:HRX6>?Gl'"/&CT&X^?G8:5:R157"%87~,@iz~[H/687^5:';3.;,&'&1023,8:571&CTH%$'6,0%"/&H/"a?G3+"%$[?40%6,iG87&>Z@
?402576,0K"/6.H/&3+02R2$3,4CH/"/6,0%&C5:D3="%3+iG87&EQ$45:R2$D5:HKl4H/&CD"/6.R13,8:R1l48:3+"/&$'&l'025:H/"%5:R+3,8:l'&H6,oQH/"%3+"/&H
C4l'025:';nH/&3+02R2$
JL$'&]?G3+023,>&1"/&10Ps6+&102HB3"/023,C'&X6+i&1"FEff&1&N"%$'&n3,R1R1l'023,RX~=6,o"%$'&n$'&l'025:H/"%5:R3,4CN57"%HYRX6>n
?Gl'"%3+"%57643,8RX6H/"1I"%$'&)$457;$'&10s@ "%$'&>6,0%&)H%l'i4;,63,85:"/&1023,RX"%5764H]3+0%&"%3+g,&[5:"/6N3,R1RX6l4"3,4C
"%$'&R1876H/&10P"%$'&n$'&l'025:H/"%5:RK5:HQ"/6)"%$'&n"/02l'&nRX6H/"B6,o3,8:8A;,63,8:HB5:N3)H/"%3+"/&,@EQ$45:87&6."%$'&]6,"%$'&10$43,4C@
RX6>?Gl'"%5:';B"%$'&LH/68:l'"%576"/6"%$'&L0%&8:3^&CaRX6H/"M&zl43+"%5765:H?687~^'6>a5:3,8z5:"%$'&QH%571&j6,o"%$'&L?40%6,iG87&>
qt"%$'&=zl4>Ki&10n6,oB3+"/6>aH|]iGl'"n&X^?6'&"%5:3,8j5:[s ff&R13,l4H/&"%$'&R1l'0%0%&"a>&1"%$'6^CTot6,0RX6>?Gl'"!
5:';)"%$'&$'&l'025:H/"%5:R]RX6>?Gl'"/&H3D2+Q :1eFH/68:l'"%576."/6"%$'&0%&8:3^&C#RX6H/"&zl43+"%576@A"%$'&a$'&l'025:H/"%5:R
&X'$457iG57"%Hot6,0>6H/"]?G8:3,445:';?40%6,iG87&>aHK3S/C45:>a5:45:H%$45:';)>a3+0%;5:43,8;3,5:4^Ia64RX&)s;,6z&Hn6,&103
RX&10%"%3,5:="%$'0%&H%$'68:CDqt"F~z?G5:R13,8:87~,@4sur(|j"%$'&]5:>?40%6,&>&"Qi40%6l';$"Liz~Z"%$'&Kl4H/&K6,oMm'p`AY6,&10m'p
i&RX6>&HjH%>a3,8:87&10`ot6,0L5:4RX0%&3,H%5:';]s JL$45:HffRX6>KiG5:'&H`"/6a>a3+g,&B"%$'&B>&1"%$'6^CZRX6H/"j&X&RX"%57,&,@G5:"%$'&
H/&4H/&`"%$43+""%$'&L$'&l'025:H/"%5:RM0%&C4l4RX&HH/&3+02R2$"%5:>&L>6,0%&`"%$43,n"%$'&j"%5:>&`0%&zl4570%&C]"/6RX6>?Gl'"/&j57"1@6487~
ot6,0YH%>a3,8:8+3,8:l'&HL6,osqt"F~z?G5:R13,8:87~,@GsU(| Q6Eff&1,&10@"%$'&]m $ $'&l'025:H/"%5:RP5:HL6,ot"/&="/6z6Eff&3+g JL$'&
zl'&H/"%576)3,C4C'0%&H%H/&C$'&10%&P5:H`57o3n>6,0%&B3,R1R1l'023+"/&Y3,4CRX6H/"j&X&RX"%57,&Y$'&l'025:H/"%5:RLR13,)i&PC'&10257,&C
5:D"%$'&m'pot023,>&1Eff6,0%g JL$'&5:C'&3=6,oQ0%&8:3^&C[H/&3+02R2$T5:HK"/6RX6>?Gl'"/&m'pqtot6,0$457;$'&10]sN|6487~
'+dXe}+t "/63,65:CZ"%$'&B&X^?6'&"%5:3,85:4RX0%&3,H/&5:)RX6>?Gl'"%3+"%57643,8RX6H/" JL$'&3,87"/&10243+"%57,&BEff6l48:C)6,o
RX6l'02H/&Bi&P"/6a3+iG3,4C'6)"%$'&m'prot023,>&1Eff6,0%g3,4CZ876z6,g3+"j6,"%$'&10Q3+?4?40%63,R2$'&Hj"/6aC'&10257^5:';n3,C4>a5:H%H%5
iG87&B$'&l'025:H/"%5:R1H`ot6,0L6,?4"%5:>a3,8"/&>?6,023,8?G8:3,445:';'@ziGl'"j"%$'&10%&3+0%&K'6,"Q>a3,~"/6ai&Bot6l44CI &X'5:H/"%5:';
>a3+g,&H/?G3,^6,?4"%5:>a3,84"/&>?6,023,8'?G8:3,4'&102H &57"%$'&10l4H/&L"%$'&L"/&>?6,023,8Gm $ $'&l'025:H/"%5:RBq%c c:@'<`9JB@,P5:C43,8
Y&X'&10@G(+*,*+-z|ff6,0L6,i4"%3,5:)&H/"%5:>a3+"/&Hjot0%6>3n"/&>?6,023,8?G8:3,445:';K;,023+?G$q%c c:@4JYP9@ \^>a57"%$)
&8:C@jw,,z@M3,4CJL9`\z~^H1@MP3+0%025:C'6'@MP43,5:4C45:3^@`3+0%i&10@(+*,*'w|2@MEQ$45:R2$D3,8:H/6=&4RX6^C'&HK"%$'&)m $
$'&l'025:H/"%5:RY"%$'6l';$=RX6>?Gl'"/&C=5:3aC45&10%&"Lo}3,H%$4576 JL$'&KC'6>a3,5:^F5:4C'&1?&4C'&"`$'&l'025:H/"%5:R1Hjl4H/&C
5:6,"%$'&10`"/&>?6,023,8G?G8:3,4'&102H1@H%l4R2$)3,H`A9`P9q}6'; 6@G(+*,*|6,0`F'JA&Jxq}JA025:4zl43+0%"1@'(+*,*|2@
3+0%&al4H/&C."/6&H/"%5:>a3+"/&"%$'&aC45:H/"%3,4RX&"/6Z"%$'&a'&3+0%&H/"KH/68:l'"%576.5:N"%$'&aH/&3+02R2$DH/?G3,RX&,@A023+"%$'&10B"%$43,
"%$'&KRX6H/"]qhc}c:@4>a3+g,&H/?G3, |ff6,o "%$43+"QH/68:l'"%576
JL$'&)0%&8:3'3+"%576Tl44C'&10287~^5:';="%$'&=m'p$'&l'025:H/"%5:R1HnR13,[i&)&X^?G8:3,5:'&C[5:"/&102>aH6,oY"%$'&ZH/&3+02R2$
H/?G3,RX&,@A023+"%$'&10"%$43,5:."/&102>aHB6,o`H/68:l'"%576.RX6H/"1IK3,~H/&1"B6,o`>6,0%&"%$43,s;,63,8:H5:HZ/H/?G8:57"%)5:"/6
?40%6,iG87&>aHQ6,os;,63,8:HP&3,R2$@EQ$45:R2$.3+0%&H/687,&C.5:4C'&1?&4C'&"%87~#qt"%$'&H/?G8:57"Y5:HP'6,"B3?G3+0%"%57"%57645:';'@
H%5:4RX&+tH%l'iGH/&1"%HY6,o`H%571&ns3+0%&aH/687,&C | JL$'&n0%&8:3^&CRX6H/"B&zl43+"%5765:HB3,8:H/6)"%$'&6,?4"%5:>a3,8 RX6H/"
&zl43+"%576Not6,0B"%$45:HQ0%&8:3^&C.H/&3+02R2$#H/?G3,RX&,@EQ$45:R2$= 8:8R13,8:8"%$'&nshd%,d%2+H/?G3,RX& JL$'&]0%&8:3^&C
H/&3+02R2$>&1"%$'6^C.RX64H%5:H/"%HY5:NH/687^5:';"%$'&n?G8:3,445:';a?40%6,iG87&>qhc}c:@H/&3+02R2$45:';ot0%6>"%$'&n"/6,?87&1,&8
;,63,8:H| 5:]"%$'&js)0%&1;,0%&H%H%576aH/?G3,RX& l'025:';P"%$'&LH/&3+02R2$@?G3+0%"%H 6,o "%$'&LH/68:l'"%576n"/6B"%$'&L0%&8:3^&CRX6H/"
&zl43+"%5763+0%&PC45:H%RX6,&10%&C@'3,4Ca"%$'&H/&P3+0%&PH/"/6,0%&C)5:3"%3+iG87&Yot6,0`8:3+"/&10`l4H/&,@%l4H/"ff3,H`5:a"%$'&Q?40%&1^576l4H
3+?4?40%63,R2$ ff&R13,l4H/&`"%$'&0%&8:3^&C]H/&3+02R2$]6487~P^5:H%57"%H '+dXez6,o4"%$'&s)0%&1;,0%&H%H%576]H/&3+02R2$]H/?G3,RX&,@57"R13,
i&L&X^?&RX"/&C"/6Ki&L3+iG87&L"/6nC'6KH/6]>6,0%&Qzl45:R%g^87~]"%$43,>&1"%$'6^C4HM"%$43+"iGl45:8:Cn3KH/68:l'"%576"/6K"%$'&YRX6H/"
&zl43+"%576not6,0M"%$'&j&"%570%&`s)0%&1;,0%&H%H%576H/?G3,RX& <ff64H/&zl'&"%87~,@57"MR13,ni&j3+?4?G8:57&C]ot6,0M$457;$'&10+3,8:l'&H


>

fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

6,o \^5:4RX&B"%$'&0%&8:3^&C=H/&3+02R2$N5:HLC'6'&ot0%6>"%$'&K;,63,8:HL6,o"%$'&?G8:3,445:';n?40%6,iG87&>Z@'"%$'&K?G3+0%"L6,o
"%$'&)m'pH/68:l'"%576"%$43+"]5:HKRX6>?Gl'"/&C5:HK3,8:H/6N8:57g,&87~Ni&a"%$'&>6H/"K0%&87&1+3,"K?G3+0%" JL$'&RX6>?G87&1"/&
3,4C.?G3+0%"%5:3,8Mm'p$'&l'025:H/"%5:R1HPRX6>?Gl'"/&C.ot6,0KC45&10%&"Psiz~N"%$'&"FEff6=>&1"%$'6^C4HBR13,i&RX6>KiG5:'&C
iz~>a3'5:>a573+"%576@z0%&H%l487"%5:';]5:3n>6,0%&B3,R1R1l'023+"/&B 43,8 $'&l'025:H/"%5:R+@^$'6,?&1o}l48:87~3+"j3nRX6>?Gl'"%3+"%57643,8
RX6H/"L"%$43+"Q5:HL'6,"j;,0%&3+"/&10Y"%$43,57"%Hj+3,8:l'&
JL$45:Hn?G3+?&10>a3+g,&H"FEff6>a3,5:WRX6"/0257iGl'"%5764H1I 5702H/"1@`57"?40%6^5:C'&Ha3DC'&1"%3,5:87&CT?40%&H/&"%3+"%576
6,oP"%$'&Z0%&8:3^&CWH/&3+02R2$k>&1"%$'6^CW3,4CT$'6E"%$45:H>&1"%$'6^CW5:H3+?4?G8:57&C"/6DR18:3,H%H%5:R13,8L3,4CT"/&>?6,023,8
0%&1;,0%&H%H%576[?G8:3,445:'; OD 87"%$'6l';$["%$'&>&1"%$'6^CT5:Hn?40%&H/&"/&CT5:["%$'&RX6"/&X^"a6,oY?G8:3,445:';'@M57"5:H
zl457"/&N;,&'&1023,8P3,4CV>a3~ki&.3+?4?G8:5:R13+iG87&"/6[6,"%$'&10ZH/&3+02R2$S?40%6,iG87&>aH3,H)Eff&8:8 !4C'&1&C@QH%5:>a5:8:3+0
"/&R2$445:zl'&H$43,&Yi&1&3+?4?G8:57&Cn"/6K?G8:3,445:';B3,4Ca6,"%$'&10`H%5:';87&L3+;,&"`H/&3+02R2$?40%6,iG87&>aHQq}9M0257&C457"%5:H1@
w,,z`l4';$43,44HKv\^R2$43+&X&10@(+*,*'w|K3,4C#"/6.RX64H/"/023,5:"K6,?4"%5:>a573+"%576VqP5:4H/i&10%;NY3+0%,&1~,@
w,,(z&10%o}3,5:8:8:57&,@&>a3,57"/0%&,@+S\^R2$457&X@'w,,| JL$'&M0%&8:3+"%576K"/6Y"%$'&H/&ff5:C'&3,H3,4C"/&R2$445:zl'&H5:HA3,8:H/6
C45:H%R1l4H%H/&C \z&RX64C@57"?40%&H/&"%H"%$'&`0%&H%l487"%HA6,o 3,]&X^"/&4C'&Cn3,43,87~^H%5:HA6,oG"%$'&ff0%&8:3+"%57,&`?&10%ot6,02>a3,4RX&
6,oLJL9M-N3,4C } - 5:#"%$'&C'6>a3,5:4H6,oj"%$'&?G8:3,445:';ZRX6>?&1"%57"%576 JL$'&?G5:RX"%l'0%&a"%$43+"K&>&10%;,&H
ot0%6>"%$45:H3,43,87~^H%5:HM5:HH/6>&1EQ$43+"ffC45&10%&"Mot0%6>"%$43+"ff;57,&aiz~"%$'&YRX6>?&1"%57"%576a0%&H%l487"%H !a?G3+0%"
"%$45:HQ5:HYC4l'&K"/6"%$'&]"%5:>&X?&10/?40%6,iG87&>8:5:>a57"L5:>?6H/&C=5:"%$'&nRX6>?&1"%57"%576@ H%5:4RX&K"%$'&n3,C'+3,"%3+;,&
6,o } - 6,&10aJL9M-N5:H]>a3,5:487~N6/$43+02C4?40%6,iG87&>aH1@EQ$45:R2$#0%&zl4570%&3N876,"K6,oj"%5:>&"/6.H/687,&ot6,0
i6,"%$=?G8:3,4'&102H !=?G3+0%"1@57"Y5:HY3,8:H/6i&R13,l4H/&]"%$'&],&102H%576N6,o } - l4H/&C.5:"%$'&nRX6>?&1"%57"%576NE`3,H
iGl';,;,~ JL$'&>a3,5:0%&H%l487"6,oj"%$'&3,43,87~^H%5:H1@$'6Eff&1,&10@M5:HK3NR2$43+023,RX"/&102573+"%5766,oj"%$'&C'6>a3,5:4H5:
EQ$45:R2$n0%&8:3^&CaH/&3+02R2$aR13,ai&`&X^?&RX"/&Ca"/6i&jRX6H/"M&X&RX"%57,&,IM5:H%l4R2$aC'6>a3,5:4H1@,&X^?G3,4C45:';PH%>a3,8:8
H/"%3+"/&HM5:H RX6>?Gl'"%3+"%57643,8:87~KR2$'&3+?&10"%$43,]&X^?G3,4C45:';P8:3+0%;,&`H/"%3+"/&H1@z3,4CnH%>a3,8:8^H/"%3+"/&H"/&4C]"/6$43,&
H%>a3,8:8'H%l4R1RX&H%H/6,0ffH/"%3+"/&H h"5:HM3,8:H/6KH%$'6EQ"%$43+"M"%$'&H/&QRX0257"/&1025:3BR13,ai&KqtEff&3+g^87~4| zl43,"%5G&Ciz~n"FEff6
>&3,H%l'0%&H1@5:,687^5:';Y"%$'&`0%&8:3+"%57,&`0%&1;,0%&H%H%576ni4023,4R2$45:';Qo}3,RX"/6,02HM3,4C]"%$'&LH%571&ff6,oH/"%3+"/&H;,&'&1023+"/&C
iz~0%&1;,0%&H%H%576@45:)"%$'&6,0257;5:43,83,4CZ0%&8:3^&CH/&3+02R2$=H/?G3,RX&H


QA

F



J $'&JL9M-?G8:3,4'&10B 4C4HB"/&>?6,023,8?G8:3,4HBot6,0a\^JL_L!9`\=?40%6,iG87&>aHBEQ57"%$#C4l'023+"%57,&3,RX"%5764H JL$'&
L
?G8:3,4Hot6l44C3+0%&j6,?4"%5:>a3,8bc:d1cfec>a3+g,&H/?G3,@4c}c:@"%$'&j"/6,"%3,84&X^&R1l'"%576"%5:>&j6,o "%$'&j?G8:3, $ @3,4C"%$'&
?G8:3,4'&10M5:HM3,8:H/6K3+iG87&j"/6K&4H%l'0%&j"%$43+"?G8:3,4HC'6K'6,"M^5768:3+"/&QRX&10%"%3,5:ag^5:4C4H 6,o0%&H/6l'02RX&QRX64H/"/023,5:"%H
JL$'&n>a3,5:=Eff6,0%g^5:';?4025:4R157?G87&Hj6,oJL9M-)3+0%&3ot6,02>]l48:3+"%576=6,off30%&1;,0%&H%H%576.H/&3+02R2$.H/?G3,RX&not6,0
"/&>?6,023,8ff?G8:3,445:';=3,4CD"%$'&m'po}3,>a5:87~.6,oY3,C4>a5:H%H%57iG87&$'&l'025:H/"%5:R1H1@i40%6l';$"n"/6,;,&1"%$'&10a"%$'0%6l';$
"%$'&] L
H/&3+02R2$3,87;,6,0257"%$4> JL$'&H/&,@3,4CN"%$'&]6,&1023,8:83+02R2$457"/&RX"%l'0%&]6,o"%$'&]?G8:3,4'&10@ 3+0%&ni40257 & G~
C'&H%RX0257i&C]5:K"%$45:HH/&RX"%576z>6,0%&`C'&1"%3,5:8:H6n"%$'&ff?G8:3,4'&10 R13,ni&ffot6l44C]5:]&3+028:57&10?G3+?&102Hjq}Y3,H%8:l4>
Y&X'&10@M(+*,*,*^@M(+*,*'w+ffY3,H%8:l4>Z@(+*,*+-,i| JA6N?40%6^5:C'&aiG3,R%gz;,0%6l44C#ot6,0n3=R187&3+0%&10nC'&H%RX0257?4"%576
6,o0%&8:3^&CH/&3+02R2$=5:"%$'&K'&X^"LH/&RX"%576@GH/&3+02R2$H/?G3,RX&K3,4C$'&l'025:H/"%5:R1H`3+0%&B&X^?G8:3,5:'&CG02H/"jot6,0L"%$'&
H%5:>?G87&10jR13,H/&K6,oH/&zl'&"%5:3,8?G8:3,445:';'@^ot68:876Eff&CZiz~)"%$'&570L3,C43+?4"%576"/6a"%$'&"/&>?6,023,8R13,H/& O` 8:H/6'@
RX&10%"%3,5:"/&R2$445:R13,8 C'&1"%3,5:8:HM"%$43+"`3+?4?&3+0ff5:>?6,0%"%3,"5:a&X^?G8:3,5:45:';B"%$'&Qi&$43^576l'06,o } - 0%&8:3+"%57,&
"/6JL9M-a5:)"%$'&KRX6>?&1"%57"%576C'6>a3,5:4H`EQ5:8:8Gi&$457;$48:57;$"/&C
1ff
fj ,P2/ !Xh/+%jP 2L2X!+2 L L2 !,!hLKP!L21 !`+%
fi4L2j!2A!j,2F2z 2 FB hAj +%A T+%A,!! +/Y2fi^1 Q7!!
)1 1 %FZz]L2XQ 2j2!!!F2 Pfi^1 P 22 %2L j j!!!%a] hjj
!+2!!j 1ff 2 h/2 ` L2X!+2Y 1 1



>

fi

4

4

-q>qsrtkmxeon-kkr#kg"!#F%$Qgkrn-ofInq>


&Q3,H%H%l4>&`"%$'&LH/"%3,4C43+02Cn?40%6,?6H%57"%57643,8'\^JL_L!9`\K>6^C'&8z6,o ?G8:3,445:'; OA ?G8:3,445:';Q?40%6,iG87&>q'&]|
RX64H%5:H/"%HM6,o3H/&1"6,o3+"/6>aH1@^3KH/&1"M6,o3,RX"%5764H3,4Ca"FEff6]H%l'iGH/&1"%H6,o3+"/6>aH1I "%$'6H/&Q"/02l'&Q5:n"%$'&Y5:457"%5:3,8
H/"%3+"/&.'q (^|B3,4C"%$'6H/&0%&zl4570%&C."/6i&a"/02l'&a5:"%$'&a;,63,8ffH/"%3+"/&.*q )K| ,
+ 3,R2$3,RX"%576.-.5:HC'&H%RX0257i&C
iz~3.H/&1"n6,oQ?40%&RX64C457"%5763+"/6>aH=q /ff01'q -'|/|2@EQ$45:R2$[$43,&Z"/6$'68:C[5:3.H/"%3+"/&Zot6,0"%$'&Z3,RX"%576["/6
i&a&X^&R1l'"%3+iG87&,@ 3,4CDH/&1"%HK6,oj3+"/6>aH]>a3,C'&a"/02l'&N'q -%232 'q -'|/|K3,4Co}3,8:H/&N'q 2%154/'q -'|/|Biz~"%$'&3,RX"%576 OZ
H/68:l'"%576.?G8:3,.5:H3,&X^&R1l'"%3+iG87&aH/&zl'&4RX&a6,0KH%R2$'&C4l487&n6,oj3,RX"%5764HB&4C45:';Z5:.3H/"%3+"/&aEQ$'&10%&a3,8:8
;,63,83+"/6>aH`$'68:C JL$'&Q&X'3,RX"ff?G8:3,aot6,02> C'&1?&4C4H6"%$'&Y>&3,H%l'0%&Q6,?4"%5:>a571&CI 5:a"%$'&PH/&zl'&"%5:3,8
R13,H/&,@3RX6H/"P5:HQ3,H%H/6^R15:3+"/&C="/6&3,R2$.3,RX"%576[q'6 7985:1'q -'<
| ;S*|2@3?G8:3,5:HQ3H/&zl'&4RX&]6,oM3,RX"%5764H1@3,4C
"%$'&KH%l4>6,o "%$'&570LRX6H/"%HQ5:Hj"%$'&RX6H/"Q6,o"%$'&?G8:3,
_L&1;,0%&H%H%5765:HK3=?G8:3,445:';>&1"%$'6^CD5:#EQ$45:R2$#"%$'&)H/&3+02R2$ot6,0n3?G8:3,D5:H]>a3,C'&5:#"%$'&H/?G3,RX&
6,o.!?G8:3,S"%3,5:8:H%^@Y?G3+0%"%5:3,8P?G8:3,4H"%$43+"3,R2$457&1,&#"%$'&;,63,8:HZ?40%6^5:C'&CS"%$43+"Z"%$'&?40%&RX64C457"%5764H6,o
"%$'&)?G3+0%"%5:3,8?G8:3,[3+0%&Z>&1" \z&3+02R2$[&4C4HKEQ$'&[3N?G8:3,D"%3,5:8ffEQ$'6H/&)?40%&RX64C457"%5764HK3+0%&Z3,870%&3,C'~
H%3+"%5:H!G&CDiz~"%$'&)5:457"%5:3,8MH/"%3+"/&Z5:Hot6l44C O= 6,0aH/&zl'&"%5:3,8?G8:3,445:';'@"%$'&?40%&RX64C457"%5764H?40%6^5:C'&
3H%>l =R157&"aH%l4>a>a3+0%~D6,oY"%$'&Z?G8:3,["%3,5:8 JL$zl4H1@3#H/&zl'&"%5:3,8`0%&1;,0%&H%H%576TH/"%3+"/&N5:H3H/&1"1<
@ 8@6,o
3+"/6>aH1@0%&1?40%&H/&"%5:';H%l'i4;,63,8:HQ"/6)i&]3,R2$457&1,&C OP N3,RX"%576?-R13,Ni&]l4H/&C="/60%&1;,0%&H%HP3H/"%3+"/
& 8
5@
2%154/'q -'ff| AB8PD
u Cz@'3,4C"%$'&Y0%&H%l487"6,o0%&1;,0%&H%H%5:';B8Y"%$'0%6l';,
$ -a5:E
H 85FGu q 8HG@-%232 'q -'|/ff| IJ/ff01'q -'| JL$'&
H/&3+02R2$=H/"%3+0%"%HLot0%6>"%$'&H/&1"L6,o ;,63,8:K
H ) 3,4CZ&4C4HjEQ$'&Z3aH/"%3+"/L
& 8MN(5:H`0%&3,R2$'&C

O-q>qsrtkmxeon-kkr#kg"!Dw`bt-n-ofInq>


!D"%$'&)R13,H/&Z6,oL"/&>?6,023,8?G8:3,445:';'@A&3,R2$W3,RX"%576[$43,Hn3NC4l'023+"%576Sq'2Pff04q'-'|;*| JL$'&?G8:3,5:H
3NH%R2$'&C4l487&,@EQ$'&10%&)3,RX"%5764Hn>a3~D&X^&R1l'"/&Z5:D?G3+023,8:87&8Yq}H%l'i^/&RX"]"/6.0%&H/6l'02RX&)3,4C[RX6>?G3+"%57iG5:8:57"F~
RX64H/"/023,5:"%H|2@^3,4C"%$'&Q6,i^/&RX"%57,&Q"/6]>a5:45:>a571&j5:H"%$'&Q"/6,"%3,8G&X^&R1l'"%576a"%5:>&,@6,0ff>a3+g,&H/?G3, S$'&
3,RX"%576=C4l'023+"%5764Hj3+0%&3,8:8&zl43,8"/6Nw+@4"%$'&H/?&R15:3,8R13,H/&K6,o?G3+023,8:87&8?G8:3,445:';K0%&H%l487"%H

QSRTQSR'UWVYX[Z]\S^.Z"_N`JaBaBbcQSRTQedfZ]g h3ikj _mlff^

JL9M-K3,4C } - C'6]'6,"H%l'?4?6,0%"M3,~6,o"%$'&Y'&1Ekot&3+"%l'0%&H5:"/0%6^C4l4RX&Ca5:9 P ( (z@^"%$'&Q?40%6,iG87&>
H/?&R15 R13+"%5768:3,';l43+;,&]ot6,0Q"%$'&n(+*,*+-)RX6>?&1"%57"%576q + C'&87g+3,>?.Q6+>a3,4@(+*,*+-z| JL$'&?G8:3,^
'&102HjH%l'?4?6,0%"`C4l'023+"%57,&B3,RX"%5764H1@'6,iz^576l4H%87~,@'iGl'"`"%$'&H/&3+0%&5:"/&10%?40%&1"/&CZ5:)3>a3,4'&10j"%$43+"LC45&102H
ot0%6>"%$'&N9 P ( w.H/?&R15 R13+"%576q 6U6';'@Q(+*,*| Ox 6,0)?4023,RX"%5:R13,8Q?Gl'0%?6H/&H1@`JL9M-[3,4C
} - 3,R1RX&1?4""%$'&9 P ( waH/~^"%3
n l4>&1025:RnH/"%3+"/&a+3+025:3+iG87&Haq}R13,8:87&C l'&"%H%Z5:9 P ( w|
3+0%&KH%l'?4?6,0%"/&C6487~)5:ZRX&10%"%3,5:Zot6,02>aHj6,o l4H/&
JL$'&KH/&>a3,"%5:R1HQ"%$43+"YJL9M-3,4C } - 3,H%H%l4>&ot6,0YC4l'023+"%57,&K3,RX"%5764HY3+0%&K&H%H/&"%5:3,8:87~"%$'6H/&]5:^
"/0%6^C4l4RX&Caiz~)\^>a57"%$a3,4C&8:C.q!w,,|ot6,0ff"%$'&YJYP9?G8:3,4'&10 6,0j3,3,RX"%576p-n"/6]i&L&X^&R1l'"%3+iG87&
6,&10]3"%5:>&5:"/&10%+3,r
8 q :sy :tN2Pff04'q -'*| u@A3+"/6>aH5:o/ff01'q -'|Y>]l4H/"Pi&]"/02l'&3+f
" :@3,4C.?&102H%5:H/"/&"Y?40%&X
RX64C457"%5764Haq}3+"/6>aHK5:,/1v04'q -'|YD
u /ff01'q -'
| Gw2%154/'q -'|/|B>]l4H/"B0%&>a3,5:."/02l'&6,&10K"%$'&a&"%570%&a5:"/&10%+3,8
+ &RX"%Hn6,oL"%$'&)3,RX"%576"%3+g,&Z?G8:3,RX&3+"H/6>&?65:"]5:D"%$'&)5:"/&102576,0]6,oL"%$'&)5:"/&10%+3,8@3,4CR13,[i&
0%&8:57&C6)"/6$'68:C)3+"j"%$'&B&4C)?65:" JjEff6a3,RX"%5764H1@ -3,4Cx-3Ft@43+0%&3,H%H%l4>&C)"/6i&a2+Q',e}* yX:X@ 5:
"%$'&]H/&4H/&]"%$43+"Q"%$'&1~=R13,=i&K&X^&R1l'"/&C.5:6,&1028:3+?4?G5:';5:"/&10%+3,8:HLEQ57"%$'6l'"Q5:"/&10%ot&1025:';aEQ57"%$&3,R2$
6,"%$'&10`5='&57"%$'&10ff3,RX"%576)C'&87&1"/&Hff3,3+"/6> "%$43+"`5:H3K?40%&RX64C457"%5766,o6,0`3,C4C'&Caiz~a"%$'&Q6,"%$'&10@ c}c:@
5z
2%154/'q -'"
| A{/ff01'q -3F:|M|
u 2%154/'q -'"
| Ap-%232 'q -3Ft|D
u C3,4CZ^5:RX&B,&102H%3
JL$45:HA5:"/&10%?40%&1"%3+"%576K6,o C4l'023+"%57,&ff3,RX"%5764H0%&H/?&RX"%H"%$'&n/'6P>6^5:';Y"%3+0%;,&1"%Y02l487&6,oG9 P ( w+@
iGl'"ff5:3]C45&10%&"E`3~ IM5:4H/"/&3,C6,o0%&zl457025:';B?G8:3,4HM"/6n&X^?G8:5:R157"%87~nH/&1?G3+023+"/&P3,)3,RX"%576C'&1?&4C45:';
6D3=RX64C457"%576ot0%6>"%$'&&X&RX"]"%$43+"K&H/"%3+iG8:5:H%$'&HB"%$'&RX64C457"%576@A"%$'&H/&>a3,"%5:R1H0%&zl4570%&HB"%$43+"

~}

>

fi (^z7G

@5

57;l'0%&aw+I



4 % : 4^-G-Ns4-G^

1 f

p s@ ~
,5 s@5 ~25

5x 5
2 5 5
B5
C'6>a3,5: | JL$'&P?G8:3,
"/&>?6,023,8?G8:3,qt"%$'&H/68:l'"%576"/6?40%6,iG87&>ff%ot0%6>"%$'&K3S]
3,8:H/6.RX6"%3,5:4H]"FEff6#3,RX"%5764He3Z3,4C@EQ$45:R2$3+0%&)'6,"]^5:H%57iG87&ai&R13,l4H/&
"%$'&1~$43,&B1&10%6aC4l'023+"%576 RX"%5764Hos%933,4CNs%9
3+0%&H/&1?G3+023+"/&C)i&R13,l4H/&
6,o3n0%&H/6l'02RX&KRX6> 5:RX" JL$'&>a3+g,&H/?G3,6,o "%$'&B?G8:3,Z5:HQ9, (
5x5

X^+zzaeY 7t[=e}ttGeFXd++ JL$45:HY>a3+g,&HC4l'023+"%57,&n3,RX"%5764HBH/"/025:RX"%87~=87&H%HY&X^?40%&H%H%57,&
"%$43,)5:9 P ( w+@'EQ$'&10%&Y&X&RX"%HjR13,i&PH/?&R15G&Ca"/6n"%3+g,&B?G8:3,RX&Y&X'3,RX"%87~3+"ff"%$'&BH/"%3+0%"ff6,0`&4C6,o
3,3,RX"%576 !a?G3+0%"%5:R1l48:3+0@57"C'6z&H'6,"ffH%l'?4?6,0%"M3,RX"%5764HM"%$43+"`>a3+g,&Y3KRX64C457"%576"/02l'&Q6487~nC4l'025:';
"%$'&570ff&X^&R1l'"%576#qhc}c:@'3,C4C3,Z3+"/6>3+"`"%$'&PH/"%3+0%"`6,oA"%$'&P3,RX"%576Z3,4C)C'&87&1"/&P57"`3+;3,5:)3+"`"%$'&P&4C |2@
EQ$45:R2$?40%&1,&"/&C=JL9M-3,4C } - ot0%6>H/687^5:';n"%$'&BRX6>?G5:87&C),&102H%5764H`6,oA?40%6,iG87&>aHffEQ57"%$)"%5:>&C
5:457"%5:3,88:57"/&1023,8:H
!)?4025:4R157?G87&,@z57"L5:HjRX&10%"%3,5:487~?6H%H%57iG87&Y"/6C'&1^5:H/&B3"/&>?6,023,80%&1;,0%&H%H%576H/&3+02R2$H/?G3,RX&ot6,0L"%$'&
9 P ( wQ5:"/&10%?40%&1"%3+"%576n6,oC4l'023+"%57,&L3,RX"%5764H1@z3,87"%$'6l';$H/"%3+"/&H5:n"%$45:HH/?G3,RX&LEff6l48:Cni&`o}3+0>6,0%&
RX6>?G87&XZH/"/02l4RX"%l'0%&H1@4C4l'&B"/6a"%$'&K'&1&CZ"/6a0%&1"%3,5:>6,0%&6,o "%$'&B?G8:3,)"%3,5:85:)"%$'&KH/"%3+"/&)qt&'6l';$Z"/6
5:4R18:l4C'&B"%$'&K&4C?65:"L6,oM3,8:86^;,65:';3,RX"%5764H| JL$'&KA9`P9 q}6';) 6@(+*,*|Q3,4C=JL9`\z~^H
qP3+0%025:C'6'@ 6@j6';'@Q(+*,*(|?G8:3,4'&102Hi6,"%$kl4H/&"%$'&=9 P ( wNH/&>a3,"%5:R1H1@L3,4Ck3+0%&=i6,"%$
Y023+?G$'?G8:3,.C'&10257+3+"%57,&HB3,4C"%$zl4HR13+0%0%~N6l'"3ZH/&3+02R2$#0%&H/&>KiG8:5:';0%&1;,0%&H%H%576#5:N"%$'&570H/68:l'"%576
&X^"/023,RX"%576?G$43,H/&qt"%$'6l';$Zi6,"%$Z?G8:3,4'&102Hff&>Ki6^C'~)>6^C45 R13+"%5764Hff"/6a"%$'&?Gl'0%&87~aiG3,R%gFR2$43,5:45:';
H/68:l'"%576)&X^"/023,RX"%576Nl4H/&C5:=Y023+?G$'?G8:3, | Q6Eff&1,&10@5:Z"%$'&?G8:3,445:';nC'6>a3,5:4H`"%$43+"Y$43,&Ki&1&
l4H/&CW5:T"%$'&"FEff6?G8:3,445:';.RX6>?&1"%57"%5764HaH%5:4RX&"%$'&=5:"/0%6^C4l4RX"%576[6,oP"/&>?6,023,8L?G8:3,445:';.5:"/6
9 P M@`3,4CV3,8:H/65:k>6H/"6,o"%$'&=&X'3,>?G87&NC'6>a3,5:4H"%$43+")$43,&.3+?4?&3+0%&CV5:W"%$'&.8:57"/&1023+"%l'0%&,@
"%$'&)>a3,5:Dl4H/&6,oL"%$'&H/"/0%6';,&109 P ( w)H/&>a3,"%5:R1H]6,oLC4l'023+"%57,&3,RX"%5764Hn$43,HKi&1&D"/6N&4RX6^C'&
RX&10%"%3,5:=ot&3+"%l'0%&H1@H%l4R2$=3,HQ"%$'&]"%5:>&C=5:457"%5:3,88:57"/&1023,8:HLl4H/&C=5:=H/6>&]C'6>a3,5:,&102H%5764HQ5:"%$'&]8:3,H/"
RX6>?&1"%57"%576@6,0]/'6^F5:'&10%"%Po}3,RX"%HYqhc}c:@o}3,RX"%HM"%$43+"ffC'6'6,"M?&102H%5:H/" 6,&10ff"%5:>&Ll4487&H%H>a3,5:"%3,5:'&C
iz~[3,W3,RX"%576 | h"a>a3~[,&10%~Eff&8:8`i&Z&3,H%57&10a"/6D3,C4CTH/6>&6,oY"%$'&H/&ot&3+"%l'0%&HaC4570%&RX"%87~D"/6#"%$'&
"/&>?6,023,8 0%&1;,0%&H%H%576.ot6,02>]l48:3+"%576l4H/&C.iz~N"%$'&aJL9M-3,4C } - ?G8:3,4'&102H1@"%$'6l';$."%$45:HB$43,HB~,&1"
"/6ai&B?Gl'"j"/6a"%$'&B"/&H/"
n l4>&1025:RH/"%3+"/&ff+3+025:3+iG87&H"%$43+" 3+0%&`l4H/&C)qtiz~3,RX"%5764H|5:KRX&10%"%3,5:]H/?&R15 RME`3~^H 3+0%&`5:"/&10%?40%&1"/&C
3,H)0%&H/6l'02RX&Hqt6,0RX6H/">&3,H%l'0%&H1@Y5:VH/&zl'&"%5:3,8Y?G8:3,445:';z|3,4CSH%l'?4?6,0%"/&CWiz~k"%$'&.?G8:3,4'&102H1@
"%$'6l';$=EQ57"%$NH/6>&]0%&H/"/025:RX"%5764H JL$'&]l4'0%&H/"/025:RX"/&CNl4H/&K6,ozl4>&1025:RH/"%3+"/&n+3+025:3+iG87&HY3,8:876Eff&C=iz~
9 P ( w]5:HQ'6,"YH%l'?4?6,0%"/&C O` >6,0%&KC'&1"%3,5:87&C=C45:H%R1l4H%H%576ZR13,=i&ot6l44C=5:Z"%$'&K?G3+?&10L6NJL9M3,4C } - 5:)"%$'&RX6>?&1"%57"%576Zi6z6,g^87&1"Kq}Y3,H%8:l4>Z@4(+*,*+-,i|

QSRTQSRTQf^ffg "Z "j h@ ^S" ^ svi Z"_

JA&>?6,023,80%&1;,0%&H%H%576@'%l4H/"8:57g,&aH/&zl'&"%5:3,8M0%&1;,0%&H%H%576@5:H3H/&3+02R2$5:."%$'&H/?G3,RX&6,o`?G8:3,"%3,5:8:H
Q6Eff&1,&10@ 5:)"%$'&B"/&>?6,023,8R13,H/&"%$'&H/&1"j6,o ?40%&RX64C457"%576)3+"/6>aHQ5:Hj'6a876';,&10LH%>l =R157&"j"/6H%l4>n
>a3+02571&Y3?G8:3,a"%3,5:8IH/"%3+"/&H`$43,&Y"/6]i&Qi&L&X^"/&4C'&CEQ57"%$3,RX"%5764HffRX64R1l'0%0%&"ffEQ57"%$"%$'&PH%l'i4;,63,8:H
3,4CT"%$'&"%5:>a5:';6,oP"%$'6H/&=3,RX"%5764Ha0%&8:3+"%57,&"/6D"%$'&=H%l'i4;,63,8:H <ff64H%5:C'&10"%$'&&X'3,>?G87&?G8:3,T5:
57;l'0%&nw+@'H/?&R15 R13,8:87~n"%$'&Z/H/"%3+"/&a3+"`"%5:>&B(,+*^IH%5:4RX&Y"%$45:Hff5:Hff"%$'&PH/"%3+0%"%5:';]?65:"ff6,oA3,RX"%576Ns%ff



>

fi

4

4

3@57"%H?40%&RX64C457"%5764H >]l4H/"i&j;,63,8:HM"/6i&L3,R2$457&1,&C3+"M"%$45:H ?65:"

`l'"M"%$'&L3,RX"%5764HYq}5:4R18:l4C45:';
'6+6,?GH|&H/"%3+iG8:5:H%$45:';K"%$'6H/&RX64C457"%5764H`>]l4H/"`i&BRX6>?G3+"%57iG87&PEQ57"%$"%$'&3,RX"%576s%ff@^EQ$45:R2$
H/"%3+0%"%HKwl4457"%H`6,o"%5:>&B&3+028:57&10Q3,4C)EQ$'6H/&&X^&R1l'"%576H/?G3,4HL3,RX0%6H%HL"%$45:H`?65:"
JL$zl4H1@'3]"/&>?6,023,8 0%&1;,0%&H%H%576)H/&3+02R2$H/"%3+"/&5:H`3]?G3,57<
0 8Pu 'q ay ]|2@4EQ$'&10%
& 5:H`3H/&1"`6,o3+"/6>aH
3,4C{x
u z'q - |2y1{1{1{y'q -3 4| `5:HA3YH/&1"A6,o43,RX"%5764H-3'EQ57"%$B"%5:>&ff5:4RX0%&>&"%m
H JL$45:H0%&1?40%&H/&"%H
3n?G3+0%"%5:3,8 ?G8:3,.qt"%3,5:8t|EQ$'&10%&Y"%$'&3+"/6>aHL5:p>]l4H/"j$'68:C)3,4C)&3,R2$3,RX"%576Dq'-3hy F|ff5:,$43,H`i&1&
H/"%3+0%"/&
C A"%5:>&Pl4457"%HM&3+028:57&10 9l'"`3,'6,"%$'&10jE`3~,@43,&X^&R1l'"%3+iG87&P?G8:3,.q}H%R2$'&C4l487&|QX ,ffH/"%3+"/&
8u'q ay ]|L3+"Q"%5:>& :j5"%$'&KO ?G8:3,>a3+g,&HY3,8:8A3+"/6>aHY5: "/02l'&]3+J" :j3,4C=H%R2$'&C4l487&HQ3,RX"%576-33+"
"%5:>f
& :mG ot6,0L&3,R2$['q -3hy Fc
| @
S$'&&X^?G3,4C45:';)3H/"%3+"/,
& 8u'q ay ]|2@ H%l4R1RX&H%H/6,0]H/"%3+"/&L
H 85F u'q F}y fFf|Y3+0%&RX64H/"/02l4RX"/&Ciz~
R2$'6z6H%5:';q}'6^FC'&1"/&102>a5:45:H/"%5:R13,8:87~4|ot6,0P&3,R2$3+"/6O
> /ee3,N&H/"%3+iG8:5:H%$'&10qhc}c:@30%&1;l48:3+0B3,RX"%576
6,0j'6+6,x
? -aEQ57"%f
$ /@@-%232 'q -'|/|2@GH%l4R2$"%$43+"jR2$'6H/&)3,RX"%5764Hj3+0%&PRX6>?G3+"%57iG87&q}3,H`C'&X '&C5:Z\z&RX"%576
( ( w|nEQ57"%$[&3,R2$W6,"%$'&103,4CTEQ57"%$T3,8:8`3,RX"%5764H5:@`3,4CW3,C'+3,4R15:';."%5:>&Z"/6#"%$'&'&X^"?65:"
EQ$'&10%&B3,Z3,RX"%576H/"%3+0%"%HKq}H%5:4RX&P"%$45:H`5:Hj3n0%&1;,0%&H%H%576)H/&3+02R2$@/3,C'+3,4R15:';3,4C[/'&X^"%a3+0%&5:"%$'&
C4570%&RX"%576[6,oY"%$'&i&1;5:445:';=6,oP"%$'&C'&1,&876,?G5:';.?G8:3, | 9M0%&RX64C457"%5764H]6,oB3,8:8j3,RX"%5764Ha3,4CW'6+
6,?GHBH/"%3+0%"%5:';3+"B"%$45:HP?65:"Pi&RX6>
& FEQ$45:87&]0%&>a3,5:45:';)3,RX"%5764HqtEQ57"%$N"%$'&570B"%5:>&5:4RX0%&>&"%H
3,C%l4H/"/&C |i&RX6>{
& fF H/"%3+"/[
& 8Pu 'q ay ]|j5:Hff 43,857m
xD
u C3,4CxMN(
JL$'&L&X'3,RX"ffC'&1"%3,5:8:H6,o"%$'&L"/&>?6,023,8'0%&1;,0%&H%H%576H/&3+02R2$3+0%&Y'6,"5:>?6,0%"%3,"ot6,0"%$'&L0%&H/"M6,o"%$45:H
?G3+?&10L3,4C$43,&Ki&1&ZC'&H%RX0257i&C)&8:H/&1EQ$'&10%&aq}Y3,H%8:l4>Y&X'&10@ (+*,*'w|

QSRTQSR } ff\ }"ik \wdK]\



!n3P"/&>?6,023,8z?G8:3,]"%$'&10%&j5:H l4H%l43,8:87~H/6>&/H%8:3,R%g^^@Gc}c:@H/6>&L3,RX"%5764HR13,ni&jH%$457ot"/&CKot6,0%E`3+02Cn6,0
iG3,R%gzE`3+02C=5:)"%5:>&BEQ57"%$'6l'"LR2$43,';5:';"%$'&KH/"/02l4RX"%l'0%&B6,0Q>a3+g,&H/?G3,6,o "%$'&B?G8:3, OM 0257;$"!FH%$457ot"/&C
?G8:3,5:HL6'&]5:EQ$45:R2$=3,8:8H%l4R2$N>6+3+iG87&]3,RX"%5764HY3+0%&]H%R2$'&C4l487&C=3,HY8:3+"/&]3,HQ?6H%H%57iG87& E
n 6^0257;$"!
H%$457ot"/&C?G8:3,4HKR13,Di&a&X'R18:l4C'&Cot0%6>RX64H%5:C'&1023+"%576EQ57"%$'6l'"&4C43,';,&1025:';6,?4"%5:>a3,8:57"F~ OZ 65:';
"%$45:Hn&8:5:>a5:43+"/&Hn0%&C4l44C43,"ni4023,4R2$'&Ha5:["%$'&H/&3+02R2$kH/?G3,RX&,@ffEQ$45:R2$[6,ot"/&kH/?&1&C4Hl'?W?G8:3,445:';
H%57;45 R13,"%87~ .
JL$45:HaR13,ki&=3,R2$457&1,&Ckiz~W3+?4?G87~^5:';"%$'&ot68:876EQ5:';#02l487&,I=S$'&W&X^?G3,4C45:';#3DH/"%3+"/z
& 85FBu
'q F}y fFf|EQ57"%$?40%&C'&RX&H%H/6,K0 8Pu 'q ay ]|2@G3,3,RX"%576-RX6>?G3+"%57iG87&PEQ57"%$Z3,8:83,RX"%5764HL5:p>a3~N ,e
i&Yl4H/&C"/6]&H/"%3+iG8:5:H%$3,3+"/6>5:,85F'EQ$'&3,8:84"%$'&P3+"/6>aH`5:oFG"%$43+E
" -3,C4C4Hff$43,&Pi&1&6,i4"%3,5:'&C
ot0%6
> 8Biz~'6+6,?GH JL$'&Y0%&3,H/6Z5:Hff"%$43+<
" -RX6l48:C)$43,&Bi&1&)l4H/&C"/6aH%l'?4?6,0%""%$'&BH%3,>&3+"/6>aHj5:
a@G3,4CZ"%$zl4HLRX6l48:CZO $43,&Ki&1&ZH%$457ot"/&C)"/6a"%$'&0257;$"Kq}C'&8:3~,&C |
;3,5:@MC'&1"%3,5:8:H]R13,i&ot6l44CD&8:H/&1EQ$'&10%&3,4C3+0%&)'6,"n5:>?6,0%"%3," S$43+"fK5:>?6,0%"%3,"K"/6
'6,"/&)5:HK"%$43+"]"%$'&0257;$"!FH%$457ot"%5:';Z02l487&a0%&1ot&102HK"/6."%$'&?40%&C'&RX&H%H/6,0n6,oL"%$'&H/"%3+"/&)i&5:';&X^?G3,4C'&C
JL$45:Ha>&3,4Ha"%$43+"EQ$'&T"%$'&=02l487&5:H3+?4?G8:57&C@"%$'&?6H%H%57iG87&ZH%l4R1RX&H%H/6,02H"/6'@L3,4CW"%$'&10%&1ot6,0%&="%$'&
6,?4"%5:>a3,8 RX6H/"P6,oh@30%&1;,0%&H%H%576.H/"%3+"/&a>a3~=i&nC45&10%&"PC'&1?&4C45:';6N"%$'&n?G3+"%$N"%$'0%6l';$NEQ$45:R2$
"%$'&)H/"%3+"/&ZE`3,H]0%&3,R2$'&C JL$zl4H1@"%$'&)876Eff&10ni6l44C#6"%$'&)RX6H/"n6,oQ3.H/"%3+"/&)6,i4"%3,5:'&CEQ$'&D"%$'&
H/"%3+"/&B5:H&X^?G3,4C'&CiGl'"ff'6,"`H/687,&C.q}3,H`57"ffEQ5:8:8'i&Y5:3, L H/&3+02R2$ |>a3~ai&Y5:+3,8:5:C3,H`3]876Eff&10
i6l44Cot6,0L"%$'&KH%3,>&KH/"%3+"/&KEQ$'&)0%&3,R2$'&C^5:33aC45&10%&"j?G3+"%$
[2Qh!! L, 12^ L!j,hhF2 2! 2h h!! L jA L 2
2!! ! L!}F2 /Y2/% Q2, `' ks 1 h f /+4F%h+2Q%A %!}, j
x/%F, 1j2 +%jL%h` `+%`2YL21n, `}hF2 1 2 !1/
, n j`2`22h 4 nj ff! h/Bj/2 1K 1 f B /n }/
24 h f L +%A Bff!2! /%F` h f z h %hff' ks !!,h Y/%F
!%2+Q% ! j! !1A

~

>

fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

TOi|{rq>qsro gfirq rq9!B#F%$Qgkrn-ofInq>
&1"Lm q+8 |MC'&'6,"/&P"%$'&P6,?4"%5:>a3,8 RX6H/"`o}l44RX"%576@ c}c:@z"%$'&Po}l44RX"%576"%$43+"j3,H%H%57;4H"/6n&3,R2$H/"%3+"/&8B5:
"%$'&H/&3+02R2$H/?G3,RX&B"%$'&>a5:45:>a3,8GRX6H/"L6,o 3,~?G3+"%$)ot0%6>B
8 "/6a3] 43,8H/"%3+"/&q}3aH/"%3+"/&{85FMN4( @45:"%$'&
0%&1;,0%&H%H%576#?G8:3,445:';H/?G3,RX&| JL$'&ao}l44RX"%576m q+8 |B5:HKR2$43+023,RX"/&102571&C[iz~"%$'&ff&8:8:>a3,#&zl43+"%576
q}ff&8:8:>a3,@w,, |2I
*
57o8MN(
q8+|Mu
q!w|
>a5:*T9*~* ' , q85:F |"twz q8 y85fF |
EQ$'&10%&r85PS6 ,6 q+8 |A5:H"%$'&`H/&1" 6,o H%l4R1RX&H%H/6,0MH/"%3+"/&H "/6{8 @^c}c:@,"%$'&jH/&1" 6,oGH/"%3+"/&H"%$43+"R13,ni&`RX64H/"/02l4RX"/&C
ot0%6>K
8 iz~)0%&1;,0%&H%H%576@ 3,4Cz q8 y85fF |j5:Hj"%$'&N/C'&87"%3RX6H/"%^@c}c:@G"%$'&K5:4RX0%&3,H/&]5:3,R1R1l4>]l48:3+"/&CNRX6H/"
i&1"FEff&1&p8Q3,4C,85F !"%$'&YH/&zl'&"%5:3,84H/&1"/"%5:';'@z"%$45:H&zl43,8:H"%$'&QRX6H/"ff6,o"%$'&Q3,RX"%576l4H/&C"/6K0%&1;,0%&H%H
ot0%6>P
8 "/685F OH+ zl43+"%576.wPR2$43+023,RX"/&102571&HLm q+8 |M6487~6)H/"%3+"/&HJP8 "%$43+"`3+0%&P0%&3,R2$43+iG87&,I"%$'&BRX6H/"`6,o
3,l4'0%&3,R2$43+iG87&H/"%3+"/&]5:HjC'&X '&C)"/6ai&5:^ 457"/&
ff&R13,l4H/&n3,R2$457&1^5:';3a0%&1;,0%&H%H%576=H/"%3+"/&Zqhc}c:@ H/&1"Q6,o;,63,8:H|<]
8 5:>?G8:57&Hj3,R2$457&1^5:';3,8:8A3+"/6>aHY5:
8 @43,4CZ"%$'&10%&1ot6,0%&K3,~)H%l'iGH/&1"j6,oH8 @'"%$'&6,?4"%5:>a3,8RX6H/"Lo}l44RX"%576ZH%3+"%5:H!G&Hj"%$'&K5:'&zl43,8:57"F~
q+8 |c *TS>a
q(|
3*T q8 F |


p
ot6,0`3,~as H%H%l4>a5:';"%$43+"ff"%$45:H5:'&zl43,8:57"F~5:H3,RX"%l43,8:87~a3,&zl43,8:57"F~a5:HM"%$'&Y0%&8:3'3+"%576"%$43+"ff;57,&H
"%$'&]m'p$'&l'025:H/"%5:R1H1I0%&1EL0257"%5:';n&zl43+"%576Dq!w|jl4H%5:';Zq(|j3,HQ3,Z&zl43,8:57"F~0%&H%l487"%Hj5:
*
p q8+|Mu >a5:
>a3

57o8fMN(
*T9*~* ' , m'pnq85F:|"twzq8y85Ff| 57o< 8>Ws
*TSs *k p m'pnq85F:|

q|

XR 6>?G87&1"/&ffH/68:l'"%576"/6Q"%$45:H&zl43+"%576@+5:"%$'&ot6,02>6,o43,K&X^?G8:5:R157""%3+iG87&6,oGm'pnq8+|ot6,03,8:8H/&1"%HAEQ57"%$
8>A s@R13,#i&aRX6>?Gl'"/&C#iz~H/687^5:';Z3Z;,&'&1023,8:571&CDH%5:';87&XFH/6l'02RX&XF3,8:8"%3+0%;,&1"%HKH%$'6,0%"/&H/"?G3+"%$
?40%6,iG87&> +3+0257&1"F~D6,oQ3,87;,6,0257"%$4>aHq}3,8:8+3+025:3+"%5764HK6,oQC'~^43,>a5:Ra?40%6,;,023,>a>a5:';=6,0];,&'&1023,8:571&C
H%$'6,0%"/&H/"A?G3+"%$ |R13,Ki&l4H/&C"/6YH/687,&"%$45:H?40%6,iG87&>Z@3,HC'&H%RX0257i&CBiz~,@Gc c:@+A5:l1e+7cLq(+*,*(| JL9M3,4C } - l4H/&3+3+025:3+"%576#6,o`"%$'&ZY&'&1023,8:571&CDff&8:8:>a3,^ 6,02CVqP |3,87;,6,0257"%$4> <ff6>?Gl'"%5:';
3RX6>?G87&1"/&]H/68:l'"%576Z"/6&zl43+"%576q|L5:Hj?687~^'6>a5:3,85:"%$'&]zl4>Ki&10L6,oM3+"/6>aHQiGl'"L&X^?6'&"%5:3,8
5:s@H%5:>?G87~i&R13,l4H/&K"%$'&nzl4>Ki&10Q6,oMH%l'iGH/&1"%HQ6,oMH%571&Ks6,0P87&H%HQ;,0%6EQHQ&X^?6'&"%5:3,8:87~)EQ57"%$s
JL$45:Hj8:5:>a57"%Hff"%$'&RX6>?G87&1"/&KH/68:l'"%576Z3+?4?40%63,R2$"/6H%>a3,8:8+3,8:l'&H`6,o sq}5:)?4023,RX"%5:RX&,@4sV(|

QSRR'U[_ b "_ ^.E j"h j \ Z"_ j _"N\ } ^.{^" i# \ le j]ffh ^

JL$'&KH/68:l'"%576Z"/6&zl43+"%576[q|j5:HQH/"/6,0%&C=5:=3a"%3+iG87&qtEQ$45:R2$ZEQ5:8:8i&0%&1ot&10%0%&C"/63,HQ"%$'&a' ^dfXe}
e3 yX:2| JL$'&H/"/6,0%&CH/68:l'"%576@A$'6Eff&1,&10@MRX6>?4025:H/&HB6487~.+3,8:l'&HB6,oQm'pnq 8+|Pot6,0]H/&1"%L
H 8H%l4R2$#"%$43+"
8>zks JA6a6,i4"%3,5:Z"%$'&]$'&l'025:H/"%5:RQ+3,8:l'&6,o3,=3+0%iG57"/023+0%~H/"%3+"/&,@ "%$'&K8:3,H/"LR18:3,l4H/&6,o &zl43+"%576q|
5:HY&1+3,8:l43+"/&CS!6^F8:5:'&^@3,4C.C4l'025:';a"%$45:HQ&1+3,8:l43+"%576."%$'&]+3,8:l'&]6,o`m'pnq 85F:|Lot6,03,~?85FAH%l4R2$N"%$43+"
85F*zWs5:H`6,i4"%3,5:'&CZiz~)876z6,g^5:';57"Ll'?=5:)"%$'&B"%3+iG87&
!o}3,RX"1@"%$'&L$'&l'025:H/"%5:Rff"%3+iG87&L5:>?G87&>&"/&C5:JL9M-3,4C } - 5:H3B;,&'&1023,84>a3+?4?G5:';Pot0%6>H/&1"%H
6,oG3+"/6>aH "/6Y"%$'&570 3,H%H/6^R15:3+"/&Cn+3,8:l'&,@,3,4CK"%$'&`$'&l'025:H/"%5:R+3,8:l'&ff6,oG3YH/"%3+"/<
& 8j5:HA"%$'&`>a3'5:>a3,8+3,8:l'&ff6,o
3,~KH%l'iGH/&1"6,So 8ff"%$43+"5:HH/"/6,0%&C]5:"%$'&ff"%3+iG87& !6,"%$'&10 Eff6,02C4H1@+57>o nq 8+|C'&'6,"/&H"%$'&+3,8:l'&ffH/"/6,0%&CKot6,0
8@^"%$'&B$'&l'025:H/"%5:RL+3,8:l'&Y6,o3nH/"%3+"/{& 8B5:Hff;57,&)iz~)mAq 8+|MuS>a3K vnq 85F: | 85FM8y nq 85F:|&X'5:H/"%vH S$'&

~

>

fi





4

4




qt3iv|

q}3|









q}R|





3v
3v


57;l'0%&K(zI_L&8:3'3+"%576Z6,o"/&>?6,023,80%&1;,0%&H%H%576H/"%3+"/&H
3,8:8'3,4Cn6487~]H/&1"%H6,oH%571&`s6,087&H%H3+0%&LH/"/6,0%&Ca5:n"%$'&j"%3+iG87&Kq}3,HM5:H "%$'&LR13,H/&LEQ$'&am'pV5:HRX6>?Gl'"/&C
RX6>?G87&1"/&87~4|"%$45:HffRX65:4R15:C'&HEQ57"%$&1+3,8:l43+"%5:';]"%$'&P8:3,H/"`R18:3,l4H/&Y6,oA&zl43+"%576q| Q6Eff&1,&10@4"%$'&Pl4H/&
6,o3B;,&'&1023,8G$'&l'025:H/"%5:R"%3+iG87&Q5:>?G8:57&H"%$43+"3,HH/6z6a3,H3B+3,8:l'&jot6,0P+4L3+"/6> H/&1c
" 8Q5:HMH/"/6,0%&Ca5:"%$'&
"%3+iG87&,@ 57"i&RX6>&H]5:>a>&C45:3+"/&87~.5:4R18:l4C'&C5:#3,8:8MH%l'iGH/&zl'&"&1+3,8:l43+"%5764H6,oLH/"%3+"/&H]RX6"%3,5:45:';
8 !Z?G3+0%"%5:R1l48:3+0@^iz~)H/"/6,025:';?G3+0%"%Hj6,o "%$'&KH/68:l'"%576)"/6m'p @'ot6,0YH/6>&K$457;$'&10jx
Ft@G5:)"%$'&ot6,02>6,o
l'?C43+"/&H6,oG"%$'&`+3,8:l'&H 6,o H/6>&jH%571&`x
Fz3+"/6>H/&1"%H1@"%$'&j$'&l'025:H/"%5:RM&1+3,8:l43+"%5765:>?G8:5:R157"%87~PRX6>?Gl'"/&H

"%$'&K>a3'5:>]l4>6,om'px3,4C)"%$'&?G3+0%"%5:3,8:87~RX6>?Gl'"/&CNm'p
JL$'&$'&l'025:H/"%5:RQ"%3+iG87&5:Hj5:>?G87&>&"/&CZ3,HL3JA0257&q}H/&1&)c c $'6'@GQ6,?RX0%6,ot"1@G Y8:8:>a3,@w9 ,|
H/6n"%$43+"`"%$'&P&1+3,8:l43+"%5766,o3,)3+"/6>H/&1<
" 8BR13,)i&PC'6'&P5:"%5:>&B8:5:'&3+0`5:"%$'&Bzl4>Ki&10ff6,oH%l'iGH/&1"%H
6,f
8)"%$43+"&X'5:H/"a5:"%$'&)"%3+iG87v&
,


&
WH
/6'@"%$'&10%&Z5:HH/6>&)6,&102$'&3,CkRX6>?G3+0%&C["/6D3N"%3+iG87&3,4C
+
&1+3,8:l43+"%576Z?40%6^RX&C4l'0%&BC'&H%57;'&C)ot6,0Q3n4^&C>a3'5:>a3,8H%l'iGH/&1"LH%571&







i|{rq>qsr

9!

gfirq r q Iw`bt-n-ofInq>

JA6aC'&X '&Bm'pot6,0j"/&>?6,023,8 0%&1;,0%&H%H%576?G8:3,445:';'@6'&'&1&C4H`6487~a"/6aC'&X '&P3nH%l457"%3+iG87&P>&3,H%l'0%&
6,oYH%571&ot6,0n"/&>?6,023,8ff0%&1;,0%&H%H%576H/"%3+"/&Ha3,4C"%$'&?40%6^RX&1&C[3,H5:D"%$'&ZH/&zl'&"%5:3,8ffR13,H/& _L&R13,8:8
"%$43+"P3"/&>?6,023,8A0%&1;,0%&H%H%576NH/"%3+"/&RX64H%5:H/"%HQ6,oM"FEff6ZRX6>?6'&"%H1"@ 8]u'q ay ]|2@EQ$'&10%L
& 5:HY3H/&1"
6,oL3+"/6>aH]3,4C.3=H/&1"]6,ojH%R2$'&C4l487&CD3,RX"%5764HKEQ57"%$"%5:>&5:4RX0%&>&"%H JL$'&a6,iz^576l4HKR13,4C45:C43+"/&
5:HB"/6.C'&X '& 8>
u p9t , @3,4C#5:4C'&1&C@l4H%5:';Z"%$45:H>&3,H%l'0%&5:&zl43+"%576Vq|3+i6,&0%&H%l487"%H
5:3ZR2$43+023,RX"/&102573+"%576#6,o`3Z876Eff&10i6l44CNo}l44RX"%576N6"%$'&"/&>?6,023,8 0%&1;,0%&H%H%576H/?G3,RX& !."%$45:H
R13,H/&,@'$'6Eff&1,&10@'C4l'&Q"/6]"%$'&Q?40%&H/&4RX&Q6,o3"%5:>&Y5:4RX0%&>&"EB5:&3,R2$#q -'E
| @@z"%$'&YH/&1"ff6,oH/"%3+"/&H
EQ57"%w
$ 8>4s5:HQ?6,"/&"%5:3,8:87~Z5:^ 457"/&,@ 3,4C="%$'&10%&1ot6,0%&n"%$'&nH/68:l'"%576="/6"%$45:HQ&zl43+"%576.R13,.'6,"Pi&
RX6>?Gl'"/&CZ&X^?G8:5:R157"%87~
JA6[6,i4"%3,5:S3[l4H%3+iG87&NRX6H/")&zl43+"%576@Y3o}l'0%"%$'&100%&8:3'3+"%576S5:H'&1&C'&CIH%5:4RX&.3D?G8:3,k"%$43+"
3,R2$457&1,&HK"%$'&H/"%3+"/,
& 8uv'q ay ]|2@ot6,L
0
u z'q - |2y1{1{1{y'q -3 '| @ 3+"K"%5:>B
& :P>]l4H/"K3,R2$457&1,&"%$'&
?40%&RX64C457"%5764H6,oj&3,R2$T3,RX"%576-3L3+"n"%5:>o
& :cGN!@3,4CD"%$'&H/&)>]l4H/"]0%&>a3,5:#"/02l'&)l4"%5:
8 :Kl4487&H%H
`! 2 2 /%M j`!2 +% n K+h j2j ]2AQ!hF2 ]! % t]2!1 !
}2/` `F2 4A %F }h}2! L2 G +h/ff1ff} !2+ff j !j!1F%
24! } F2 M/%%Vh 2 } FK%j 2,/% KE
/P h /2'2Fh/
Z!
2K%VhH
}2/ BMF2 !XhBh+% L P2 /K2j} Q B !Bff}
2 ,}2/ Q2 z,hh2 /2 2 ff !j! } 2 !%!j/
71P!j hff ]2 c
Ds 1+% K2M1P! %//%F,F 1! 2!!1ML !V !
!!XhffBhff ff}2/G2 ffhQ7 ! + PLh!2!,+ B h /2 !F+/X
2 /K,!!]}2/L !Xh/ ff/2 1ff! } `F2 ^ ] M!!1 2P^ M2
} 2 F2, PF2 /Q K

m/%F
1


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

C'&87&1"/&CZiz~p-3!@'"%$'&6,?4"%5:>a3,8RX6H/"Lo}l44RX"%576H%3+"%5:H!G&H
q'ay]|

&

>a3

/ff01q'-3|2yC
wtwfiff
qt-z|
<, ' , '
q'ay]| DI
q|
/ff01q'-3|2yC
K{

' ,
8 uq s/"y zq'- yw|2yq'- $ y2(|+|2@MC'&X
#&X'3,>?G87&>a3~#R18:3+0257ot~N"%$'&?4025:4R157?G87&,IK<ff64H%5:C'&10B"%$'&H/"%3+"/&x)
?G5:RX"/&Cn5: 57;l'0%&j('q}3| OM ?G8:3,]3,R2$457&1^5:';Y"%$45:HH/"%3+"/&L3+" "%5:>&c
: >]l4H/" 3,R2$457&1,&j"%$'&ff?40%&RX64C457"%5764HA6,o
- $ 3+"H:3G(z@H/6Km q8+|>]l4H/"i&j3+"87&3,H/"m q /ff01 q'- $ |2yC |)
( ho3,RX"%576- $ 5:HB/87&1ot"6l'"%^@z3,HM5: 57;l'0%&
('qti|2@57" R13,]i&ffH/&1&]"%$43+" "%$'&`H%3,>&ff?G8:3,]3,8:H/6P3,R2$457&1,&H "%$'&/65:"?40%&RX64C457"%5764H6,oG3,RX"%5764H- 3,4C
- $ 3+":G.w+@^H/6]m q+8 |>]l4H/"Mi&Q3+"ff87&3,H/"ffm q /ff01 q'- |3IE/ff01 q'- $ |2yC |3#
w 5:43,8:87~,@57o i6,"%$3,RX"%5764Hff3+0%&
87&1ot" 6l'"Lq 57;l'0%&Q('q}R|/|2@z57"5:H R187&3+0M"%$43+" "%$'&j?G8:3,n3,8:H/6B3,R2$457&1,&HMH%5:>]l487"%3,'&16l4H%87~B"%$'&j?40%&RX64C457"%5764H
6,o "%$'&B"FEff63,RX"%5764HL3,4C3+"/6>
/ @GH/6m q+8 |`>]l4H/"ji&3+"Q87&3,H/"Ym q s/"<I/ff01 q'- | I{/ff01 q'- $ |2yC |
ff~D"/0%&3+"%5:';.5:'&zl43,8:57"%57&Hqt-z|PVq|K3,Hn&zl43,8:57"%57&H1@3="/&>?6,023,8ff0%&1;,0%&H%H%576[H/"%3+"/&Z5:HK0%&8:3^&C
"/6[3DH/&1"6,oH/"%3+"/&H)5:WEQ$45:R2$
u zC @Bc}c:@jH/"%3+"/&H)RX6"%3,5:45:';D6487~[;,63,8:H3,4Ck'6RX64R1l'0%0%&"
'

3,RX"%5764H JA6]&3,R2$ZH%l4R2$)H/"%3+"/&,@'0%&8:3'3+"%576.q(|R13,i&Y3+?4?G8:57&C@0%&H%l487"%5:';K5:3,&zl43+"%576)C'&X 45:';
"/&>?6,023,8'm'pK@H%5:>a5:8:3+0A"/6aq| JL$45:H&zl43+"%576$43,H 3Q 457"/&ff&X^?G8:5:R157"AH/68:l'"%576@,RX6"%3,5:45:';P3,8:8^H/"%3+"/&H
8Ku 'q ay C|LEQ57"%
$ p4Us OQ ;3,5:@ >6,0%&nC'&1"%3,5:8:HQR13,Ni&ot6l44C&8:H/&1EQ$'&10%&)q}Y3,H%8:l4>v Y&X'&10@
(+*,*'w|


L 5:H3PEff&8:8^g^'6EQa3,C4>a5:H%H%57iG87&ff$'&l'025:H/"%5:RffH/&3+02R2$a3,87;,6,0257"%$4>q}H/&1&nc cB6,0%oh@Gw9,z@Gw,,| JL$'&
3,87;,6,0257"%$4>Eff6,0%g^H`iz~)3H/&10257&H`6,oRX6H/"!i6l44C'&CZC'&1?4"%$^G02H/"jH/&3+02R2$'&H JL$'&BRX6H/"L0%&1"%l'02'&Ciz~"%$'&



:8 3,H/"BRX6>?G87&1"/&C.C'&1?4"%$^G02H/"PH/&3+02R2$#5:HY3Z876Eff&10Bi6l44C=6N"%$'&RX6H/"B6,off3,~NH/68:l'"%576 JL$'&10%&1ot6,0%&,@
"%$'&3,87;,6,0257"%$4>R13,#&3,H%5:87~Ni&a>6^C45G&C."/6"%3+g,&3,Dl'?4?&10B8:5:>a57"P6#H/68:l'"%576RX6H/"1@3,4C"/6&X'57"
EQ57"%$)o}3,5:8:l'0%&P64RX&K57"L$43,Hj?40%6,&Z"%$43+"Q'6H/68:l'"%576)EQ57"%$Z3RX6H/"QEQ57"%$45:"%$45:Hj8:5:>a57"ff&X'5:H/"%H
)&X^"/&4H%576)6,oA"%$'&B L
3,87;,6,0257"%$4> ot6,0LH/&3+02R2$45:'; K
nY
P_;,023+?G$4Hj5:Hff"%$'&>a3,5:"/6z68iz~
EQ$45:R2$="%$'&]0%&8:3^&CNH/&3+02R2$.>&1"%$'6^CN5:HY5:>?G87&>&"/&C JL$'&K&X^"/&4C'&C.3,87;,6,0257"%$4>5:HQ?40%&H/&"/&CN5:
\z&RX"%576= (
L 5:HL3H/6+FR13,8:87&C=8:5:'&3+0QH/?G3,RX&]3,87;,6,0257"%$4>ZIff57"QH/"/6,0%&HQ6487~)"%$'&?G3+"%$"/6"%$'&KR1l'0%0%&"Q'6^C'&
JL$'&3,87;,6,0257"%$4>R13,#i&H/?&1&C'&CDl'?#iz~#l4H%5:';>&>6,0%~5:"%$'&ot6,02>6,oL3Z"/023,4H/?6H%57"%576"%3+iG87&,@
EQ$45:R2$0%&RX6,02C4HKl'?C43+"/&C.&H/"%5:>a3+"/&CRX6H/"%HK6,oj'6^C'&HB"%$43+"]$43,&i&1&&X^?G3,4C'&CiGl'"K'6,"KH/687,&C
q}_L&5:'ot&8:CV
N3+02H%8:3,4C@Bw,-z| JL$'&="%3+iG87&N5:H6,o3#4^&CV8:5:>a57"/&CWH%571&,@LH/6['6,"3,8:8Q&X^?G3,4C'&C
l44H/687,&C#'6^C'&H3+0%&H/"/6,0%&C + S$'&'&1,&10]"%$'&H/&3+02R2$#0%&3,R2$'&H]3='6^C'&"%$43+"K5:H5:"%$'&a"%3+iG87&a"%$'&
l'?C43+"/&CRX6H/"j&H/"%5:>a3+"/&Pot6,0j"%$'&B'6^C'&q}C45:H%RX6,&10%&C)EQ$'&"%$'&P'6^C'&YE`3,H`?40%&1^576l4H%87~&X^?G3,4C'&C |M5:H

12ff+%jh2+%!1!!M+2M,!!n j +/] !K !#"%$M !QY}F%J'&k!#"%$M
Y+2 /' ] ]72hff, !.' !#"%M
$ tQ!^`!jth ]ff 1+2+K L2,2
s1 +2 ! !!+K+%2 ff}F%ffks ,ffh2!!!1M2h ($ffff j +/B2P
Pff M+2 A2'+%!1!!! A/L +2,h/
) M1!#*,+ - !hM!j h!! 2M L2 t2ff/%Fz !`}F%! +%%`2}2/:
! F%ff j Qf h+2+/Y !!1h/B2X2 mt
4fi Q2+./10 2 ,F2 j !j!1/Y2
L! /+21F2 ks }F%!%ff}2/ K% , K!2!,+ QL! +2K2 !`
1P! } P B` +2Ph B!jF% `}F%Ms1 +2 tY!}jXh Y+%
}2/P}F%M +!/Q2j2M,! j 1X/P,Ft
P/224! !hh!!M X!P
}2 `!! hAff122^ /%FY!j4A
3 ! ! 658P7 % 2+z:9;29 1F
1=<


fi

4

4

l4H/&C=5:4H/"/&3,CZ6,oM57"%HQ$'&l'025:H/"%5:RP+3,8:l'&,@G3,8:876EQ5:';"%$'&]3,87;,6,0257"%$4>"/63,65:C=0%&XFH/&3+02R2$45:';'6^C'&Hj"%$43+"
3+0%&0%&3,R2$43+iG87&^5:3H/&1,&1023,8?G3+"%$4HLC4l'025:';]"%$'&KH%3,>&K57"/&1023+"%576

!>Owexzy


JL$'&JL9M-a?G8:3,4'&10j?40%&RX6>?Gl'"/&HL"%$'&"/&>?6,023,8Am $ $'&l'025:H/"%5:RP3,HQC'&H%RX0257i&CZ3+i6,&]3,4Cl4H/&HQ57"L5:
3, L
H/&3+02R2$W5:D"%$'&)"/&>?6,023,80%&1;,0%&H%H%576[H/?G3,RX& _Q57;$"!FH%$457ot"nR1l'"%Hn3+0%&Zl4H/&C"/6.&8:5:>a5:43+"/&
0%&C4l44C43,"?G3+"%$4Hot0%6> "%$'&QH/&3+02R2$H/?G3,RX&,@z3,4C3B"/023,4H/?6H%57"%576]"%3+iG87&Q5:HMl4H/&C"/6]H/?&1&Cal'?aH/&3+02R2$
q}Y3,H%8:l4>xY&X'&10@'(+*,*'w| JL$'&Y>a3,5:H/"/&1?GH6,o"%$'&Q?G8:3,4'&10ff3+0%&Q6l'"%8:5:'&C5: 57;l'0%&Pqt6?G3+;,&
(3- |2@ >a3,5:487~"/65:8:8:l4H/"/023+"/&PH%5:>a5:8:3+0257"F~3,4CC45&10%&4RX&bc:d1cfec"%$'& } - ?G8:3,4'&10

? A@CB DM CEGFGb!s!IHKJbAHLJNMOF=J
6,0`>a3,~?G8:3,445:';P?40%6,iG87&>aH"%$'&Ym $ $'&l'025:H/"%5:R`5:HM"/6z6KEff&3+g >6,0%&Y3,R1R1l'023+"/&P$'&l'025:H/"%5:RjR13,i&
6,i4"%3,5:'&Ciz~RX64H%5:C'&1025:';a$457;$'&10L+3,8:l'&HL6,o"%$'&Ks?G3+023,>&1"/&10@ iGl'"Q3,~>&1"%$'6^Cot6,0PRX6>?Gl'"%5:';
3NRX6>?G87&1"/&)H/68:l'"%576#"/6."%$'&Zm'p&zl43+"%576[H%R13,87&HK&X^?6'&"%5:3,8:87~5:Ds@>a3+g^5:';N57"]5:>?4023,RX"%5:R13,8
ot6,0s ;( O[ RX6>?G87&1"/&H/68:l'"%576[5:Hnl4H/&1o}l48ffi&R13,l4H/&Z57"$'&87?GHnC'&1"/&RX"l4'0%&3,R2$43+iG87&)H/"%3+"/&HNq}5:
?G3+0%"%5:R1l48:3+0@m $ C'&1"/&RX"%H3H%57;45 R13,"?G3+0%"6,o "%$'&LH/"%3+"%5:RL>]l'"/&Xn0%&8:3+"%5764H5:n3B?G8:3,445:';Q?40%6,iG87&>|2@
iGl'"3,8:H/6]E`3,H/"/&1o}l484i&R13,l4H/&Y6,ot"/&>a3,~6,o"%$'&Y3+"/6>H/&1"%H`3+0%&Y'6,"ff0%&87&1+3,"ot6,0ff&1+3,8:l43+"%5:';]H/"%3+"/&H
3,RX"%l43,8:87~]&4RX6l4"/&10%&CaEQ$45:87&jH/&3+02R2$45:';Bot6,03H/68:l'"%576n"/6K"%$'&j?G8:3,445:';Y?40%6,iG87&> 3+"$43,4C _L&R13,8:8
"%$43+"P"%$'&$'&l'025:H/"%5:RB&1+3,8:l43+"%576N6,off3)H/"%3+"/&=q}3)H/&1"P6,oM;,63,8:H|Y>a3+g,&HBl4H/&]6,o"%$'&n&H/"%5:>a3+"/&C.RX6H/"B6,o
3,~.H%l'iGH/&1"P6,off"%$'&aH/"%3+"/&a"%$43+"K5:HPg^'6EQWq}H/"/6,0%&C#5:."%$'&a$'&l'025:H/"%5:R"%3+iG87&| Oa HB8:3+0%;,&10K3+"/6>H/&1"%H
3+0%&RX64H%5:C'&10%&C@Mc}c:@A3,Hs 5:4RX0%&3,H/&H1@"%$'&1~.i&RX6>&ai6,"%$>6,0%&zl4>&10%6l4HB3,4C#>6,0%&H/?&R15 R+@
3,4CZ"%$zl4Hj"%$'&Bot023,RX"%576Z6,o "%$'&KRX6>?G87&1"/&H/68:l'"%576)"%$43+"Q5:HL3,RX"%l43,8:87~)l4H/&1o}l48C'&RX0%&3,H/&H
JA6Dl4H/&=m'pot6,0$457;$'&10s@`R187&3+0287~3E`3~T5:Ha'&1&C'&CT"/6DRX6>?Gl'"/&Z"%$'&=$'&l'025:H/"%5:R3+"3#RX6H/"
?40%6,?6,0%"%57643+"/&Z"/6"%$'&+3,8:l'&)6,oP"%$'&5:>?40%6,&>&" _L&8:3^&CWH/&3+02R2$k3,5:>aH"/6D3,R2$457&1,&"%$45:Hniz~
RX6>?Gl'"%5:';B6487~3?G3+0%"M6,o"%$'&Pm'pSH/68:l'"%576@3,4C3B?G3+0%""%$43+"ff5:HM8:57g,&87~]"/6Ki&L0%&87&1+3,"Mot6,0ffH/687^5:';
"%$'&;57,&Z?G8:3,445:';]?40%6,iG87&>

onQPQ|W#FnfiH!B#F%$Qgkrn-oInq>

HB&X^?G8:3,5:'&CN&3+028:57&10@"%$'&m'p$'&l'025:H/"%5:R]R13,#i&H/&1&#3,HB"%$'&6,?4"%5:>a3,8RX6H/"o}l44RX"%5765:."%$'&s)
0%&1;,0%&H%H%576kH/?G3,RX&,@L3#0%&8:3^&CkH/&3+02R2$VH/?G3,RX&=EQ$'&10%&=H/&1"%H6,o>6,0%&="%$43,Ws ;,63,8:H3+0%&NH/?G8:57"a5:"/6
?40%6,iG87&>aHL6,oMs;,63,8:H1@&3,R2$.6,oMEQ$45:R2$N5:HYH/687,&CN5:4C'&1?&4C'&"%87~ JL$zl4H1@ "%$'&]s)0%&1;,0%&H%H%576NH/?G3,RX&
5:H)3, K
nY
P_;,023+?G$IH/"%3+"/&H)EQ57"%$Vs 6,0)ot&1Eff&103+"/6>aHZ3+0%&DP_jF'6^C'&H)3,4CS3+0%&.&X^?G3,4C'&C
iz~T'6,02>a3,8L0%&1;,0%&H%H%576@ffEQ$45:87&H/"%3+"/&HEQ57"%$W>6,0%&="%$43,Ws 3+"/6>aH3+0%& K
nY F'6^C'&H1@`EQ$45:R2$k3+0%&
&X^?G3,4C'&C=iz~=H/687^5:';a&3,R2$H%l'iGH/&1"Q6,oH%571&]s JL$'&]RX6H/"P6,o3,P_jF'6^C'&]5:HY>a5:45:>a571&CZ6,&103,8:8
57"%HnH%l4R1RX&H%H/6,02H1@EQ$45:87&"%$'&)RX6H/"n6,oY3, K
nY F'6^C'&Z5:H]>a3'5:>a571&C ?
+ '3,>?G87&HK6,onqt?G3+0%"]6,o|"%$45:H
;,023+?G$@ ot6,0P3)(0%&1;,0%&H%H%576.H/?G3,RX&,@3+0%&nH%$'6EQN5: 57;l'0%&HQ-)3,4C.Nqt"%$'&]&X'3,>?G87&]5:HYC'&H%RX0257i&C5:
C'&1"%3,5:85:Z\z&RX"%576 ( w| HjR13,)i&BH/&1&@^"%$'&P;,023+?G$)5:H`'6,"jH/"/025:RX"%87~8:3~,&10%&C@45:"%$43+"YP_jF'6^C'&H
>a3~ZH/6>&1"%5:>&HL$43,&]H%l4R1RX&H%H/6,02Hj"%$43+"Q3+0%&K3,8:H/6)P_jF'6^C'&H
JL$'&LC45&10%&"3,87;,6,0257"%$4>aHl4H/&Cn"/6B6,i4"%3,5:aRX6>?G87&1"/&LH/68:l'"%5764H"/6"%$'&Qm'pk&zl43+"%576aR13,a3,8:8^i&
H/&1&3,Hff+3+025:3+"%5764HM6,o3=!i6,"/"/6>nFl'?G]8:3+i&8:5:';B6,o"%$'&Y'6^C'&H6,o"%$45:HM;,023+?G$@zH/"%3+0%"%5:';Kot0%6> '6^C'&H
EQ57"%$ZRX6H/"j1&10%63,4C)?40%6,?G3+;3+"%5:';aRX6H/"%HL"/6?G3+0%&"L'6^C'&Hj3,R1RX6,02C45:';"/6"%$45:Hj>a5: >a3?4025:4R157?G87&
JL$'&?40%6,?G3+;3+"%576T5:HnRX6>?G87&1"/&,@Qc}c:@M?40%6^RX&1&C4Hnl4"%5:8&1,&10%~Uq}H/687+3+iG87&|K'6^C'&)5:"%$'&);,023+?G$[$43,H
i&1&Z8:3+i&87&CEQ57"%$)57"%H`6,?4"%5:>a3,8RX6H/"]q}3,87"%$'6l';$Z5:ZH/6>&B6,o"%$'&3,87;,6,0257"%$4>aH1@'5:4R18:l4C45:';"%$'&]P
1

fi

fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

5:>?G87&>&"%3+"%576#l4H/&CDiz~JL9M-N3,4C } - @ 6487~."%$'&RX6H/"%HK6,oYP_jF'6^C'&HK3+0%&)3,RX"%l43,8:87~H/"/6,0%&C |
_L&8:3^&C=H/&3+02R2$=&X^?G876,0%&Hj"%$'&Ks)0%&1;,0%&H%H%576H/?G3,RX&]5:3>6,0%&ot6^R1l4H/&Co}3,H%$4576@'EQ57"%$Z"%$'&K3,5:>6,o
C45:H%RX6,&1025:';a"%$'&6,?4"%5:>a3,8ARX6H/"nqt6,0Y3,N5:>?40%6,&C876Eff&10Yi6l44C |M6,oMH/"%3+"/&HQ0%&87&1+3,"Q"/6"%$'&KH/&3+02R2$
ot6,0j3nH/68:l'"%576"/6n"%$'&P;,63,8:H`6,o"%$'&P;57,&?G8:3,445:';?40%6,iG87&> JL$45:H5:H`3,R2$457&1,&C)iz~H/&3+02R2$45:';n"%$'&
s)0%&1;,0%&H%H%576ZH/?G3,RX&Bot6,0Q3,)6,?4"%5:>a3,8H/68:l'"%576"/6a3n?G3+0%"%5:R1l48:3+0jH/"%3+"/&,I"%$'&RX6H/"j6,o"%$45:HjH/68:l'"%576)5:H
"%$'&]m'p$'&l'025:H/"%5:RY+3,8:l'&B6,o "%$43+"QH/"%3+"/& JL$'&K3,87;,6,0257"%$4>C'&H%RX0257i&CZ5:)"%$'&K'&X^"QH/&RX"%576qt L |
R13+0%0257&H6l'"M"%$45:HMH/&3+02R2$D!"/6,?'FC'6EQ4^@^H/"%3+0%"%5:';ot0%6> "%$'&QH/"%3+"/&YRX6,0%0%&H/?64C45:';Y"/6K"%$'&L;,63,8:HM6,o"%$'&
?G8:3,445:';]?40%6,iG87&>
Q&l'025:H/"%5:R1HjC'&10257,&C)iz~)H/&3+02R2$45:';5:Z3,3+iGH/"/023,RX"%576Z6,o"%$'&H/&3+02R2$=H/?G3,RX&$43,&i&1&H/"%l4C457&C
&X^"/&4H%57,&87~5: Lq}H/&1&c c^P3,H%R2$4457;'@w +z'9M0257&C457"%5:H1@4w,,zG<`l487i&102H/6ar\^R2$43+&X&10@w,,| !
?G3+0%"%5:R1l48:3+0@57"P$43,HYi&1&.H%$'6EQN"%$43+"BH%l4R2$.$'&l'025:H/"%5:R1HYR13,.6487~Zi&nRX6H/"B&X&RX"%57,&al44C'&10PRX&10%"%3,5:
RX64C457"%5764H1IP"%$'&;,&'&1023,8:571&C"%$'&16,0%&>6,offM3,87"/6,0%"%3=H/"%3+"/&H"%$43+"K5:."%$'&aRX6l'02H/&6,oj3, H/&3+02R2$
;l45:C'&C)iz~Z3a$'&l'025:H/"%5:RPC'&10257,&CZiz~)H/&3+02R2$45:';aiG8:5:4C487~a5:ZH/6>&K3+iGH/"/023,RX"%5766,o "%$'&KH/&3+02R2$=H/?G3,RX&,@
&1,&10%~[H/"%3+"/&Z"%$43+"Eff6l48:CDi&)&X^?G3,4C'&Ciz~3NiG8:5:4CDH/&3+02R2$W5:"%$'&)6,0257;5:43,8ffH/&3+02R2$WH/?G3,RX&Z>]l4H/"
i&K&X^?G3,4C'&C&57"%$'&10Y5:"%$'&n3+iGH/"/023,RX"PH/?G3,RX&K6,0Piz~Z"%$'&
H/&3+02R2$.5:Z"%$'&]6,0257;5:43,8H/?G3,RX&Zq}Q687"/&,@
9&10%&1,S@ R5:>a>&10@4T
N3,R 643,8:C@Aw,,| JL$45:H`5:>?G8:57&H"%$43+"L57oA"%$'&3+iGH/"/023,RX"%5765:Hj3,Z&>Ki&C4C45:';
qt"%$'&nH/&1"Y6,oMH/"%3+"/&HB5:"%$'&]3+iGH/"/023,RX"BH/?G3,RX&]5:HQ"%$'&]H%3,>&n3,HP5:"%$'&K6,0257;5:43,8AH/&3+02R2$.H/?G3,RX&|2@H%l4R2$N3
$'&l'025:H/"%5:RnR13,'&1,&10Ki&RX6H/"K&X&RX"%57,&qtM3,87"/6,0%"%3^@jw9 -z| JL$'&as)0%&8:3'3+"%576#6,o`"%$'&0%&1;,0%&H%H%576
?G8:3,445:';=H/&3+02R2$WH/?G3,RX&5:Hn3,[&>Ki&C4C45:';'@H%5:4RX&)&1,&10%~[H/"%3+"/&5:"%$'&Z'6,02>a3,8ff0%&1;,0%&H%H%576[H/?G3,RX&
RX6,0%0%&H/?64C4H"/6&X'3,RX"%87~k6'&.H/"%3+"/&Wq}RX6"%3,5:45:';"%$'&.H%3,>&.H/&1")6,oKH%l'i4;,63,8Y3+"/6>aH|5:k"%$'&Ns)
0%&1;,0%&H%H%576#H/?G3,RX& !#H/?G57"/&a6,o`"%$45:H1@A"%$'&10%&3+0%&0%&3,H/64H"/6=i&8:57&1,&"%$43+"K0%&8:3^&C#H/&3+02R2$R13,Di&
RX6H/"`&X&RX"%57,&,IJL$'&P3,87;,6,0257"%$4> l4H/&C"/6H/&3+02R2$"%$'&Ps)0%&1;,0%&H%H%576H/?G3,RX&BC45:H%RX6,&102HBq}3,4C)H/"/6,0%&H`5:
"%$'&j$'&l'025:H/"%5:R"%3+iG87&|"%$'&`"/02l'&jm'pW+3,8:l'&,@+6,0M3P876Eff&10 i6l44CK6]"%$45:H+3,8:l'&ff;,0%&3+"/&10M"%$43,]"%$43+" ;57,&
iz~n"%$'&QR1l'0%0%&"M$'&l'025:H/"%5:Rff"%3+iG87&,@ot6,0M&1,&10%~P_jF'6^C'&j&X^?G3,4C'&CaC4l'025:';P"%$'&QRX6l'02H/&j6,o"%$'&L0%&8:3^&C
H/&3+02R2$ JL$'& K
nY
P_H/"/02l4RX"%l'0%&6,o "%$'&s)0%&1;,0%&H%H%576H/?G3,RX&,@ 3,4C"%$'&o}3,RX"Q"%$43+"L"%$'&=!6^F8:5:'&
$'&l'025:H/"%5:R]>a3+g,&Hl4H/&6,o`3,8:80%&87&1+3,"5:'ot6,02>a3+"%576N?40%&H/&"5:."%$'&a$'&l'025:H/"%5:R"%3+iG87&,@A5:>?G8:57&HQ"%$43+"
3,N5:>?40%6,&>&"Q6,o"%$'&]&H/"%5:>a3+"/&CNRX6H/"Y6,oM3,P_jF'6^C'&K>a3~~^57&8:C5:>a>&C45:3+"/&87~Z3,N5:>?40%6,&C
&H/"%5:>a3+"/&=6,oB"%$'&NRX6H/"6,o>a3,~ K
nY F'6^C'&H.q}3,8:8QH/"%3+"/&H"%$43+")3+0%&NH%l'?&102H/&1"%H6,oB"%$'&N5:>?40%6,&C
H/"%3+"/&|2@'EQ57"%$'6l'"ff3,~3,C4C457"%57643,84H/&3+02R2$)&X6,0%" 5:43,8:87~,@i&R13,l4H/&P_jF'6^C'&Hff5:a"%$'&Ys)0%&1;,0%&H%H%576
H/?G3,RX&Q3+0%&LH/"%3+"/&H6,o8:5:>a57"/&CnH%571&,@&3,R2$'6^C'&j&X^?G3,4H%5765:n"%$'&js)0%&1;,0%&H%H%576aH/?G3,RX&Q5:H8:57g,&87~K"/6i&
RX6>?Gl'"%3+"%57643,8:87~R2$'&3+?&10ff"%$43,a"%$'&Y3,&1023+;,&5:"%$'&Y'6,02>a3,840%&1;,0%&H%H%576H/?G3,RX&,@^H%5:4RX&L"%$'&Yzl4>Ki&10
6,oAH%l4R1RX&H%H/6,02H`;,&'&1023+"/&C)EQ$'&0%&1;,0%&H%H%5:';n3nH/"%3+"/&P;,&'&1023,8:87~5:4RX0%&3,H/&HffEQ57"%$"%$'&Pzl4>Ki&10ff6,o;,63,8
3+"/6>aHQ5:)"%$'&KH/"%3+"/&

iKUI
JA6nH/&3+02R2$)"%$'&Y0%&8:3^&C0%&1;,0%&H%H%576H/?G3,RX&,@ } - l4H/&H`3,)3,87;,6,0257"%$4> R13,8:87&C L Hff"%$'&P43,>&
H%l';,;,&H/"%H1@L57"5:Ha3,k3,C43+?4"%576W6,oP L
"/6DH/&3+02R2$45:'; KnY P_;,023+?G$4H1@Yc}c:@j57"aR13+0%0257&Ha6l'"3
C'&1?4"%$^G02H/"1@'57"/&1023+"%57,&KC'&1&1?&45:';H/&3+02R2$ L 5:Hj3,C4>a5:H%H%57iG87&,@^5:)"%$'&H/&4H/&B"%$43+"Q57oA;l45:C'&C)iz~
3,Z3,C4>a5:H%H%57iG87&L$'&l'025:H/"%5:R+@^57"ff0%&1"%l'024H"%$'&P6,?4"%5:>a3,8H/68:l'"%576RX6H/"j6,o"%$'&BH/"%3+0%"%5:';H/"%3+"/& !o}3,RX"1@457"
4C4H"%$'&L6,?4"%5:>a3,84RX6H/"ff6,o&1,&10%~P_jF'6^C'&L"%$43+"ff5:HMH/687,&C5:"%$'&QRX6l'02H/&Q6,o"%$'&YH/&3+02R2$ Q6Eff&1,&10@
57"]C'6z&HK'6,"]g,&1&1?&'6l';$5:'ot6,02>a3+"%576#ot6,0]"%$'&6,?4"%5:>a3,8H/68:l'"%576#57"%H/&87o`"/6Ni&&X^"/023,RX"/&C@MH/6N57"
R13,Z'6,"ffi&Pl4H/&C"/6n 4CH/68:l'"%5764H"/6 K
nY
P_H/&3+02R2$)?40%6,iG87&>aH h"ffEff6,0%g^Hffot6,0j"%$'&Y?Gl'0%?6H/&Q6,o
5:>?40%6^5:';n"%$'&$'&l'025:H/"%5:R+@'$'6Eff&1,&10@ H%5:4RX&Bot6,0L"%$45:H`6487~"%$'&6,?4"%5:>a3,8M2XeA'&1&C4Hj"/6ai&Pg^'6EQ
1



fi

4

4

V ;WYX:Zv[\ V =]K
W_^
V W ;`fiabv;cedKf5fiavhg
Vji W :kv vlvdnm V Wg
Vpo W qfim;a V :kv vlvsrtlcKlu`@;`fiabv;cAWK^
Vv W
:kv vlvdX:Zv[fiwtZxy V =]{:kv vlvAWg
z
Vj{ W v;kv;l:kv vlvQg
z
Vj| WYX:Zv[fiwtZxy V =]
W_^
Vj} W tfIf~:l~fia V WK^
Vj~ W
;`fiabv;cdo ;k5hg
V :uW
v;kv;lshg
z
V ;W tf V ,`v;c:lyfi`fiabv;cfi5uauWI^
V W
;`fiabv;cdo ;k5hg
ViW
v;kv;l?`v;c@;`fiatkv~;`tlz;`vQg
z
V W tf Vu W_^Yfio;fiZutlu`cv
VvW
f` V vv:m?:k @=`f@,:k:mx;m~ = r;dBW_^
V{W
l5q;`v`f=6dX:Zv[\ V =fi]K
WgfixfiafiaeX:Zv[\q;m;`vav9p
V|W
tf V l5q?;`v`f=
Wn^fix='lu`@;`fiabv;c
V}W
v;kv;lel5q;`v`f@=fig
z
z
V~W
;`fiabv;cd V fiafiax:k ~x;`fiabv;cAWg
V ;uW
l5q;`v`f@ndB`bvfiafiax=4l5q;`v`f@=pQg
V ~;W
tf V ;`fiabv;cAWK^
V W
`v V =]l5q;`v`f@Wo:lsyfi`fiabv;cfi5uahg
z
Vi W
v;kv;lel5q;`v`f@=g
z
V W ave^fiK[tutlu`cv
V v W
f` V vv:m?=f:l:k V WfiWK^
V{ W
tf V cva5 V =]*=WK_m V =W_r;do
W_^
V| W
l5q;`vp;mv`tk;m?=6d_cva5 V =]*=WKeX:Zv[fiwtZxy V =fi]sncva5 V =]*=WfiWg
V} W
tf V ;`fiabv;cAWK^
V~ W
l5q;`v`f_dnl5q;`vp;mv`tk;m?=fig
Vji uW
`v V =]l5q;`v`f@Wo:lsyfi`fiabv;cfi5uahg
Vji ;W
v;kv;lIl5q;`v`f@=g
z
z
Vji W
ave^
Vjifii W
l5q;`vp;mv`tk;m?=6d_cva5 V =]*=WKnm V =Wg
z
z
Vji;o W
l5q;`v`f@ndB9:ls`bvfiafiax=4l5q;`vp;mv`tk;m?=pQg
Vjiv W
`v V =]l5q@;`vs`fW,:lI5kv~ ~ t5uahg
Vjifi{ W
v;kv;lel5q;`v`f@=g
z
z
57;l'0%&KzIJL$'&B L



3,87;,6,0257"%$4>qtEQ57"%$ZH/687,&C"%3+iG87&|
1


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

JL$'&`3,87;,6,0257"%$4>r5:HH/g,&1"%R2$'&Ca5: 57;l'0%&` JL$'&ff>a3,5:]C45&10%&4RX&ffot0%6>r L 5:H5:K"%$'& P \KH%l'i'
0%6l'"%5:'&,IEQ$'&)&X^?G3,4C45:';n3, KnY F'6^C'&,@G57"j0%&R1l'02H%57,&87~5:,6,g,&HL"%$'&>a3,5:)?40%6^RX&C4l'0%&P L @
023+"%$'&10]"%$43,D"%$'& P \o}l44RX"%576 JL$zl4H1@ot6,0]&3,R2$TH%l4R1RX&H%H/6,0]"/6.3, KnY F'6^C'&,@ "%$'&)3,87;,6,0257"%$4>
?&10%ot6,02>aHA3PH/&10257&HA6,oGH/&3+02R2$'&H EQ57"%$]5:4RX0%&3,H%5:';YRX6H/" i6l44C@H/"%3+0%"%5:';Yot0%6>"%$'&`$'&l'025:H/"%5:R&H/"%5:>a3+"/&
6,oj"%$'&)H%l4R1RX&H%H/6,0n'6^C'&NqtEQ$45:R2$#ot6,0nH/6>&)H%l4R1RX&H%H/6,02H]>a3~#i&H%>a3,8:87&10K"%$43,D"%$43+"K6,oL"%$'& K
nY
'6^C'&)57"%H/&87o|3,4CD 45:H%$45:';EQ$'&[3.H/68:l'"%5765:H]ot6l44C6,0n"%$'&ZRX6H/"i6l44CD6,oQ"%$'&)?40%&C'&RX&H%H/6,0
K6, ?4nY"%5: >aF3,'68^RXC'6&BH/"L5:H6,&Xo'"%RX$'&1&P&C'&X&^C ?GO 3,JL4C'$4&5:HCZ&'64H^%C'l'&,0%&@4H3,4C"%$4)3+&"`z"%l4$'3,&P8RX"/66aH/"j"%$'0%&B&1"%6,l'?402"%'&5:>aC3,5:8HffRX3,687H/E`"Q357~^oHj"%$'3n&876'6Eff^&1C'0`&Bi5:Hj6l4H/64C87,&6C "%ff$'~ &

H/"/6,025:';]l'?C43+"/&CRX6H/"%H`6,o P_jF'6^C'&Hff5:"%$'&P$'&l'025:H/"%5:Rj"%3+iG87&,@^"%$'&PH/&3+02R2$ZRX6>?Gl'"/&H`3]?G3+0%"ff6,o"%$'&
m'p$'&l'025:H/"%5:Ra3,H]3NH%5:C'&&X&RX"n3,4C@3,Hn'6,"/&CD&3+028:57&10@ "%$'&+3,8:l'&H]H/"/6,0%&C[5:#"%$'&"%3+iG87&i&RX6>&
5:>a>&C45:3+"/&87~=3+3,5:8:3+iG87&not6,0l4H/&a5:.H%l'iGH/&zl'&"B$'&l'025:H/"%5:RK&1+3,8:l43+"%5764H L H/"/6,?GHH/&3+02R2$45:';
"%$'&H%l4R1RX&H%H/6,02Hn6,oP3, K
nY F'6^C'&3,HaH/6z6T3,H6'&Z5:Hnot6l44C"/6#$43,&=3.RX6H/"a;,0%&3+"/&10"%$43,["%$'&
R1l'0%0%&"`i6l44C@^H%5:4RX&Y"%$45:Hj5:>?G8:57&HM"%$'&BRX6H/"L6,oA"%$'& K
nY F'6^C'&B5:Hj3,8:H/6n;,0%&3+"/&10L"%$43,)"%$'&Bi6l44C
Q6Eff&1,&10@H%5:4RX&"%$'&`3,87;,6,0257"%$4>?&10%ot6,02>aH0%&1?&3+"/&CnC'&1?4"%$^G02H/"AH/&3+02R2$'&H EQ57"%$K5:4RX0%&3,H%5:';Li6l44C4H1@
0%&>a3,5:45:';nH%l4R1RX&H%H/6,02HL6,o"%$'& K
nY F'6^C'&EQ5:8:8G&1,&"%l43,8:87~)3,8:H/6i&H/687,&C S$'&3,Zs)FH/68:l'"%576
$43,H`i&1&ot6l44C@^3,8:8 H%l4R1RX&H%H/6,02H`"/6&1,&10%~ K
nY F'6^C'&B3+?4?&3+025:';]5:"%$'&BH/68:l'"%576"/0%&1&$43,&Bi&1&
H/&3+02R2$'&C@ff3,4C"%$'&570l'?C43+"/&C[RX6H/"%HaH/"/6,0%&C JL$45:HK&4H%l'0%&H]"%$43+""%$'&)0%&H%l487"%5:';N$'&l'025:H/"%5:R+@`c}c:@
"%$43+"QC'&X '&C)iz~"%$'&K$'&l'025:H/"%5:RQ"%3+iG87&K3+ot"/&10Q"%$'&B0%&8:3^&CH/&3+02R2$=5:Hff 45:H%$'&C@'5:HjH/"%5:8:8RX64H%5:H/"/&"
ff&R13,l4H/&"%$'&H%l4R1RX&H%H/6,0]'6^C'&HB6,o K
nY F'6^C'&H]3+0%&H%l'iGH/&1"%H1@A L ot0%&zl'&"%87~.&4RX6l4"/&102H
"%$'&KH%3,>&H/"%3+"/&)q}H/&1"L6,o ;,63,8:H|`>6,0%&"%$43,Z64RX&KC4l'025:';nH/&3+02R2$ JL$'&3,87;,6,0257"%$4>R13,i&H/?&1&C'&C
l'?@H%57;45 R13,"%87~,@iz~.H/"/6,025:';ZH/687,&C'6^C'&Haqti6,"%$ K
nY F'6^C'&H3,4CDP_jF'6^C'&H|L"/6,;,&1"%$'&10KEQ57"%$
"%$'&570n6,?4"%5:>a3,8ffRX6H/"a3,4C[H%$'6,0%"!FR1l'"/"%5:';."%$'&ZH/&3+02R2$[EQ$'&[57"]0%&3,R2$'&Ha3.'6^C'&"%$43+"$43,Hn3,870%&3,C'~
i&1&TH/687,&C 2 !TC45&10%&4RX&Z"/6"%$'&876Eff&10i6l44C4H]H/"/6,0%&Ck5:"%$'&$'&l'025:H/"%5:R"%3+iG87&,@ffEQ$45:R2$T3+0%&
+3,8:5:C3,8:H/65:"%$'&Px
F 0%&1;,0%&H%H%576)H/&3+02R2$ZH/?G3,RX&Bot6,0j3,~x
FS;Ws3,H`Eff&8:8 3,Hj5:"%$'&Y6,0257;5:43,8 H/&3+02R2$
H/?G3,RX&,@ "%$'&]5:'ot6,02>a3+"%5765:"%$'&]H/687,&C"%3+iG87&]5:HL+3,8:5:CZ6487~)ot6,0Q"%$'&]R1l'0%0%&"Qs)0%&1;,0%&H%H%576=H/&3+02R2$
q}H%5:4RX&H/"%3+"/&HK6,ojH%571&ax
Ft@ot6,0Kx
F;s 3+0%&0%&8:3^&C#5:"%$'&as)0%&1;,0%&H%H%576DH/?G3,RX&aiGl'"K'6,"K5:"%$'&
sxF 0%&1;,0%&H%H%576H/?G3,RX&|
n 6,"/&n"%$43+"B3)H/"%3,4C43+02C="/023,4H/?6H%57"%576"%3+iG87&,@EQ$45:R2$0%&RX6,02C4HPl'?C43+"/&CNRX6H/"P&H/"%5:>a3+"/&HY6,offl4^
H/687,&CW'6^C'&H1@5:Hn6,oP'6l4H/&5: L H%5:4RX&Zl'?C43+"/&C&H/"%5:>a3+"/&H6,oP_jF'6^C'&H3+0%&H/"/6,0%&CW5:
"%$'&$'&l'025:H/"%5:R"%3+iG87&,@ffEQ$45:87&"%$'&$'&l'025:H/"%5:R&H/"%5:>a3+"/&Z6,oP3, K
nY F'6^C'&=5:H3,87E`3~^H;57,&Tiz~"%$'&
>a3'5:>]l4>6,o57"%HLH%571&BsH%l4R1RX&H%H/6,02H

RTQSR'UWVL_N j g h ^

6,03,W5:8:8:l4H/"/023+"%576#6,oY"%$'&Zl4H/&)6,oY0%&8:3^&CTH/&3+02R2$T"/6#5:>?40%6,&$'&l'025:H/"%5:Ra+3,8:l'&H1@RX64H%5:C'&10"%$'&
ot68:876EQ5:';)H%5:>?G87&K?40%6,iG87&>ot0%6>"%$'&\^JL_L!9`\Z,&102H%576.6,o"%$'&\^3+"/&8:8:57"/&nC'6>a3,5:@5:"/0%6^C4l4RX&CN5:
"%$'&](+*,*(a?G8:3,445:';nRX6>?&1"%57"%576 JL$'&B?40%6,iG87&>RX64RX&1024HQ3aH%3+"/&8:8:57"/&EQ$'6H/&B;,63,85:Hj"/63,R1zl4570%&
5:>a3+;,&HM6,oC45&10%&"3,H/"/0%6'6>a5:R13,84"%3+0%;,&1"%HPqt0%&1?40%&H/&"/&Ciz~n"%$'&Q?40%&C45:R13+"/p
& fi5,S,| JA6]C'6KH/6'@
57"%H5:4H/"/02l4>&">]l4H/"G02H/"i&?6Eff&10%&CB6q5
",| 3,4CR13,8:57i4023+"/&C5q h,|2@3,4CB"%$'&H%3+"/&8:8:57"/&M>]l4H/"
"%l'02DH/6="%$43+"]57"K5:H?65:"%5:';5:"%$'&C'&H%570%&CDC4570%&RX"%576kq5Qu,S,| !4H/"/02l4>&"KR13,8:57i4023+"%576
0%&zl4570%&H"%$'&`H%3+"/&8:8:57"/&"/6Pi&?65:"%5:';Q3+"3YH/?&R15 RMR13,8:57i4023+"%576"%3+0%;,&1"Qq}5:"%$45:HA&X'3,>?G87&,@,C4570%&RX"%576

1 X/jF2 XAF2, LF2 j !j!1/`2M! /L+21F2
tL/22z! !
! P}2/Y j Q%Xh ! Aj/2+%Aj/%F!AL!Q, !,/%/+
1!2A !hA!2!h!A2^M2 2 P
LL2 ]/] ] Mh2j ` Mj +% j + /Y` !j,2F2 L2 ] ff !/]
!h
1


fi

4

4

{(img d4),(img d5),(img d6)}: 4 (3)

{(img d4),(img d5)}: 4 (3)
(tk_img d4)

{(img d4),(img d6)}: 3

{(point d4),(on),(cal),(img d5)}: 3 + 1

{(img d5),(img d6)}: 3

(tk_img d5)
{(point d5),(on),(cal),(img d4)}: 3 + 1

57;l'0%&B-'IQ9 3+0%"K6,oj"%$'&)(F_L&1;,0%&H%H%576D"/0%&1&qt&X^?G3,4C'&C#"/6N3=RX6H/"]i6l44C6,oL|Bot6,0K"%$'&&X'3,>?G87&
\^3+"/&8:8:57"/&?40%6,iG87&> OjK
nY F'6^C'&HY3+0%&]C'&1?G5:RX"/&Ciz~Z0%&RX"%3,';87&H1@P_jF'6^C'&HLiz~Z&8:8:57?GH/&H
JL$'&QRX6H/"6,o&3,R2$'6^C'&Q5:HEL0257"/"/&a3,HK!&H/"%5:>a3+"/&x
C tU3,R1R1l4>]l48:3+"/&C4 OM 6,0`'6^C'&HEQ$'6H/&
&H/"%5:>a3+"/&CRX6H/"$43,HMi&1&al'?C43+"/&C3+ot"/&10&X^?G3,4H%576@"%$'&KqmX|&H/"%5:>a3+"/&Qi&1ot6,0%&j&X^?G3,^
H%576Z5:Hj;57,&5:)?G3+0%&"%$'&H%5:H

z|

\^5:4RX&"%$45:HQ5:HL"%$'&\^JL_L!9`\,&102H%576=6,o"%$'&]C'6>a3,5:@ 3,8:8A3,RX"%5764HY3+0%&n3,H%H%l4>&C"/6)$43,&nl4457"

RX6H/"
JA6g,&1&1?SH%571&=6,oB"%$'&=&X'3,>?G87&N>a3,43+;,&3+iG87&,@L87&1"1 H3,H%H%l4>&N3DRX6>?G87&1"/&.H/68:l'"%576W$43,Hi&1&
RX6>?Gl'"/&CT6487~Dot6,0m3,4C["%$43+"a0%&8:3^&CWH/&3+02R2$W5:Hl4H/&CT"/6DRX6>?Gl'"/&3.?G3+0%"%5:3,8Lm $ H/68:l'"%576
57;l'0%&HA-B3,4CnYH%$'6Eqt?G3+0%"6,o|"%$'&j(0%&8:3^&CnH/?G3,RX&`&X^?G876,0%&CKiz~"%$'&G02H/" 3,4C]H/&RX64Cn57"/&1023+"%576@
0%&H/?&RX"%57,&87~,@'6,o3,Z L H/&3+02R2$=H/"%3+0%"%5:';ot0%6>"%$'&?40%6,iG87&> ;,63,8:H
!T"%$'&)G02H/"a57"/&1023+"%576q 57;l'0%&Z-z|] L Q P \W5:HR13,8:87&CTEQ57"%$T3RX6H/"i6l44C6,oBz@j3,H"%$45:H
5:Ha"%$'&&H/"%5:>a3+"/&CVRX6H/"6,oP"%$'&NH/"%3+0%"%5:';DH/"%3+"/&N;57,&kiz~["%$'&=?40%&RX6>?Gl'"/&CVm$'&l'025:H/"%5:R JL$'&
0%6z6,"P'6^C'&n5:HY3, K
nY F'6^C'&,@H/6EQ$'&N57"P5:HQ&X^?G3,4C'&CN L 5:HYR13,8:87&CNot6,0P&3,R2$H%571&(H%l'iGH/&1"
q}8:5:'&Haq!w|YTq!5w |B5: 57;l'0%&| JL$'&G02H/"H%l4R2$#H%l'iGH/&1""/6i&;,&'&1023+"/&C5:[
H fi5S3fi5
Q JL$45:HffH/"%3+"/&B3,8:H/6n$43,H`3,&H/"%5:>a3+"/&CZRX6H/"`6,oz@'H/6] L Q P \)5:HffR13,8:87&CEQ57"%$a"%$45:Hi6l44C5:
"%$'&KG02H/"Y57"/&1023+"%576@ iGl'"L"%$'&]"FEff6)?6H%H%57iG87&B0%&1;,0%&H%H%5764HQ6,oM"%$45:HQH/"%3+"/&ni6,"%$N87&3,C="/6)H/"%3+"/&HPEQ57"%$N3
$457;$'&10QRX6H/"Q&H/"%5:>a3+"/&)q}3,&H/"%5:>a3+"/&C=RX6H/"Q6,on?G8:l4Hj3,=3,R1R1l4>]l48:3+"/&CNRX6H/"Q6,ojw| JL$'&'&1ERX6H/"
5:HQ?40%6,?G3+;3+"/&C.iG3,R%g="/6)"%$'&n?G3+0%&"PH/"%3+"/&,@EQ$'&10%&K"%$'&5:>?40%6,&CNRX6H/"B&H/"%5:>a3+"/&qt-z|L6,o"%$'&n3+"/6>
H/&1
" fi5S3fi5Q5:HKH/"/6,0%&C5:"%$'&$'&l'025:H/"%5:Rn"%3+iG87&3,4C#0%&1"%l'02'&CVq}8:5:'&Hq,|YWq,|
5: 57;l'0%&a| \^5:4RX&n"%$45:HP?Gl'"%HP"%$'&&H/"%5:>a3+"/&C#RX6H/"6,o`"%$'&aH/"%3+"/&'6E3+i6,&a"%$'&ai6l44CN6,off"%$'&
L R13,8:8Mq}8:5:'&aqt-z|j5: 57;l'0%&|`'6>6,0%&57"/&1023+"%5764HL3+0%&C'6'& JL$'&'&1ERX6H/"Y5:H`0%&1"%l'02'&C)"/6
"%$'&B L Q P \)?40%6^RX&C4l'0%&P&X^?G3,4C45:';]"%$'&B0%6z6,"L'6^C'&,@'EQ$45:R2$)3,8:H/60%&1"%l'024H`H%5:4RX&B"%$'&B0%6z6,"L'6^C'&
5:Hj3, K
nY F'6^C'&K3,4C57"j'6Er$43,HL3,l44H/687,&CH%l4R1RX&H%H/6,0]q}8:5:'&Hq!~w ,|Mq!5w |j5: 57;l'0%&| JL$45:H
45:H%$'&Hff"%$'&BG02H/"j57"/&1023+"%576
!N"%$'&nH/&RX64C57"/&1023+"%576[q 57;l'0%&a|L L Q P \.5:HYR13,8:87&CNEQ57"%$N3i6l44C6,o- h"Y?40%6^RX&1&C4H
8:57g,&j"%$'&`G02H/"1@iGl'"M'6Ek"%$'&j&H/"%5:>a3+"/&CRX6H/"M6,o"%$'& K
& QuS3
"3 h3fi5
nY F'6^C'f
QQ5:HEQ57"%$45:K"%$'&Li6l44C@H/6B"%$45:HM'6^C'&L5:H &X^?G3,4C'&C JL$'&`G02H/"MH%571&Q(BH%l'iGH/&1"ot6,0MEQ$45:R2$n L
5:H`R13,8:87&C)5:J
H QuS3
"@EQ57"%$)3,Z5:457"%5:3,84&H/"%5:>a3+"/&CZRX6H/"j6,o`w JL$'&YG02H/"`57"/&1023+"%576o}3,5:8:H
"/6B 4C3H/68:l'"%576not6,0"%$45:HH/"%3+"/&,@ziGl'"MH%5:4RX&j"%$'&L'&1EkRX6H/"6,o(B5:HH/"%5:8:8^EQ57"%$45:]"%$'&ji6l44C5:>?6H/&C
iz~"%$'&K?G3+0%&" K
nY F'6^C'&,@3H/&RX64C.57"/&1023+"%576.5:HQC'6'&]EQ$45:R2$Z 4C4HQ3H/68:l'"%576 JL$'&]'&1ExRX6H/"
6,oL"%$'&Z3+"/6>H/&1,
" QuS3
")5:H]H/"/6,0%&C[5:D"%$'&)$'&l'025:H/"%5:Ra"%3+iG87&)3,4C5:3,C4C457"%576@ "%$'&
H/687,&CH/"%3+"/&HBq}3,876';EQ57"%$"%$'&5706,?4"%5:>a3,84H/68:l'"%576aRX6H/"|3+0%&Y3,8:84H/"/6,0%&C5:"%$'&QH/687,&Ca"%3+iG87&]q}8:5:'&H
1}


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

{(img d4),(img d5),(img d6)}: 5 (4)

{(img d4),(img d5)}: 5 (4)
(tk_img d4)

(sw_on)
{(point d4),(off)}: 1 (1) + 1

{(point d0),(img d5)}: 3 + 1

{(point d5),(on),(cal),(img d4)}: 4 (3) + 1

{(point d4),(img d5)}: 4 (3)
(turn d0 d4)

...

(turn d1 d4)

{(point d1),(img d5)}: 3 + 1

{(img d5),(img d6)}: 3

(tk_img d5)

{(point d4),(on),(cal),(img d5)}: 4 (3) + 1

{(point d4),(on)}: 2 (1)

{(img d4),(img d6)}: 3

...
(tk_img d5)

...

{(point d4),(point d5),(on),(cal)}: 3 (2) + 1

(turn d6 d4)
...

{(point d6),(off)}: 0 + 2

57;l'0%&KzIQ9 3+0%"K6,oj"%$'&)(F_L&1;,0%&H%H%576D"/0%&1&qt&X^?G3,4C'&C#"/6N3=RX6H/"]i6l44C6,o`-z|Bot6,0K"%$'&&X'3,>?G87&
\^3+"/&8:8:57"/&?40%6,iG87&> OjK
nY F'6^C'&HY3+0%&]C'&1?G5:RX"/&Ciz~Z0%&RX"%3,';87&H1@P_jF'6^C'&HLiz~Z&8:8:57?GH/&H
JL$'&RX6H/"B6,off&3,R2$D'6^C'&n5:HPEL0257"/"/&.3,H!&H/"%5:>a3+"/&
C 3,R1R1l4>]l48:3+"/&C4 B
n 6,"/&a"%$43+"B"%$'&
3,R1R1l4>]l48:3+"/&C[RX6H/"a5:HK6487~#3,876';N"%$'&?G3+"%$ot0%6>"%$'&Z'&3+0%&H/"n3,4RX&H/"/6,0 K
nY F'6^C'&
6 ,0B'6^C'&HQEQ$'6H/&K&H/"%5:>a3+"/&C.RX6H/"P$43,HQi&1&Nl'?C43+"/&C=3+ot"/&10P&X^?G3,4H%576@G"%$'&K&H/"%5:>a3+"/&
i&1ot6,0%&&X^?G3,4H%5765:Hj;57,&=5:)?G3+0%&"%$'&H%5:H1I"%$45:Hj&H/"%5:>a3+"/&K5:4R18:l4C'&Hjl'?C43+"/&HL>a3,C'&K5:
"%$'&?40%&1^576l4H`57"/&1023+"%576Dq}H%$'6EQ=5: 57;l'0%&P-z|

q (9|L[q^w|P5: 57;l'0%&a| \^5:4RX&n"%$'&nG02H/"BH%l4R1RX&H%H/6,06,off"%$'& KnY F'6^C'&aE`3,HH/687,&C.&X^?G3,4H%576
RX6"%5:zl'&HEQ57"%$."%$'&a'&X^"KH%l'iGH/&1"1@QuS3fi5CQ JL$45:HBH/"%3+"/&$43,HKH/&1,&1023,8?6H%H%57iG87&
0%&1;,0%&H%H%5764H1@^H/6>&Y6,oEQ$45:R2$87&3,C"/6P_jF'6^C'&HiGl'"ffH/6>&P"/6 KnY F'6^C'&H 8:8@z$'6Eff&1,&10@'0%&1"%l'02
3N>a5:45:>]l4>qt&H/"%5:>a3+"/&N
C t3,R1R1l4>]l48:3+"/&C |KRX6H/"n6,oL-'@MH/63,5:>?40%6,&C[RX6H/"Zqtot6,0n"%$'&Z3+"/6>H/&1"
QuS3fi5CQ+|A5:H H/"/6,0%&C5:K"%$'&L$'&l'025:H/"%5:RM"%3+iG87&`3,4Cn"%$'&`?G3+0%&" K nY F'6^C'&`0%&>a3,5:4H
l44H/687,&C H%5:>a5:8:3+0?40%6^RX&H%H$43+?4?&4HEQ$'&]57"%HH%57iG8:5:';j'6^C'&,%@ QuQ3
"3 h3fi5
S@5:H&X^?G3,4C'&C@3,4CB"%$'&RX6H/"A6,o^"%$'&M3+"/6>H/&1" fi5S3fi5Q5:Hl'?C43+"/&CP64RX&ff>6,0%&,@
"/6
JL$'&K?40%6^RX&H%HQRX6"%5:zl'&HQ"%$'0%6l';$N3aot&1Ex>6,0%&]57"/&1023+"%5764H1@ l4"%5:83,8:8"%$'&]H%571&n(aH%l'iGH/&1"%HQ6,o"%$'&
"/6,?'F87&1,&8;,63,8H/&1"P$43,&ni&1&NH/687,&C@"%$'&n>6H/"PRX6H/"%87~=3+"B3RX6H/"P6,E
OB "P"%$45:HL?65:"1@l'?C43+"/&C
&H/"%5:>a3+"/&HB6,oj,)H%571&a()3+"/6>H/&1"%H$43,&i&1&H/"/6,0%&C#5:N"%$'&$'&l'025:H/"%5:RK"%3+iG87&,@AH%8:57;$"%87~87&H%HP"%$43,
$43,87o"%$'&Yzl4>Ki&10ff"%$43+"ffEff6l48:C$43,&Pi&1&H/"/6,0%&C)57o3]RX6>?G87&1"/&Bm $ H/68:l'"%576$43,Ci&1&RX6>?Gl'"/&C

onQPQ|W#FnfiH!Iw`bt-n-ofInq>
H E`3,H"%$'&QR13,H/&YEQ57"%$"%$'&Ym'pS$'&l'025:H/"%5:Rj57"%H/&87oh@3,C43+?4"%5:';0%&8:3^&CH/&3+02R2$a"/6]"%$'&L"/&>?6,023,8GR13,H/&Y5:H

H%5:>?G87&Q5:a?4025:4R157?G87&,@iGl'"ffH/6>&1EQ$43+"`RX6>?G8:5:R13+"/&C5:?4023,RX"%5:RX& OM 5702H/"1@z"%$'&Y0%&8:3'3+"%576)5:"/0%6^C4l4RX&C
iz~K&zl43+"%5764HLqt-z|q| 3+?4?40%6'5:>a3+"/&H3P"/&>?6,023,8z0%&1;,0%&H%H%576H/"%3+"/J
& 8Pu 'q ay ]|iz~]3]1ez6,oH/"%3+"/&H
EQ57"%$'6l'"`3,RX"%5764H1@c}c:@^6,o"%$'&Pot6,02>'q F}y C| JA6g,&1&1?=>a3+"/"/&102HLH%5:>?G87&,@z6487~a&zl43+"%576#q|ff5:Hjl4H/&C
19


fi

4

4

; oQV `uasWI^
u ytA\ V `uasWe^
;`fiabvKm,IxQ]`v@:lK5kv~ ~ t5uahg ; `fiabv_m,exQ]`v:lI5kv~ ~ t5uahg
` edX:Zvu\ V `uasS` fiavWg
sd g
z
qfim; V lu`rv` fi :l?;`tlc~~;`tl WK^
X:Zv[\ V `uasS` fiav=]B:lf~:lfiAWg
tf V
va v;c `uaslu`@;`fiabv;cAWK^
f5v;agfiI`~t~:l~fia `uasskfil;`fiab5ua
z
d[@,g
z
` edeX:Zvu\ V `uasS` fiavWg
z
57;l'0%&KzIJL$'&JL9M-a3,4C

} -

?G8:3,445:';]?40%6^RX&C4l'0%&H

5:)"%$'&0%&8:3^&CH/&3+02R2$IM"%$43+"Q5:H1@'"%$'&H%571&B6,o3aH/"%3+"/&K5:HLC'&X '&CZ3,H

q'ay]|vzu |I /ff01q'-'|A {
' ,


q|

q'ay]|`Vm

q*,|

\z&RX64C@ &1,&.H/63H/"%3+"/&]6,oMH%571&]87&H%HQ"%$43,=s>a3~=H/"%5:8:8$43,&n3'6^&>?4"F
~ RX6>?6'&"1@3,4C
H%l4R2$]3QH/"%3+"/&`R13,]'6,"Ai&H/"/6,0%&C]5:"%$'&ff$'&l'025:H/"%5:R "%3+iG87&PqtEQ$45:R2$K>a3+?GH6487~B3+"/6>xH/&1"%HA"/6Y3,H%H/6^R15:3+"/&C
RX6H/"%H|
n &57"%$'&10LR13,)"%$'&P6,?4"%5:>a3,8RX6H/"`6,0L876Eff&10ji6l44Cot6l44Cot6,0LH%l4R2$)3H/"%3+"/&Bi&PH/"/6,0%&CZ3,H`"%$'&
RX6H/"j6,oA"%$'&BRX6,0%0%&H/?64C45:';]3+"/6>H/&1"qt0257;$"j$43,4CH%5:C'&Y6,oA&zl43+"%576#q|/|2@GH%5:4RX&Y"%$'&P6,?4"%5:>a3,8RX6H/"
6,o3,R2$457&1^5:';]"%$45:Hff3+"/6>H/&1"j>a3~i&P876Eff&10 Q6Eff&1,&10@G3]?G8:3,"%$43+"j3,R2$457&1,&<
H wI_ ' , /ff01'q -'|
3+{
" :B3,8:H/6=3,R2$457&1,&H]"%$'&H/"%3+"/&'q ay ]|3+"]>6H/"]>a3 ' , "%5:>&l4457"%H8:3+"/&10@"%$'0%6l';$D5:'&10%"%5:3^@
c}c:@


|I /ff01q'-'|2ySC G ' >a 3,
' ,

JL$zl4H1@^"/6>a3,5:"%3,5:)"%$'&3,C4>a5:H%H%57iG5:8:57"F~n6,o"%$'&$'&l'025:H/"%5:RQo}l44RX"%576)C'&X '&C)iz~"%$'&RX6"/&"%HQ6,o"%$'&
$'&l'025:H/"%5:RQ"%3+iG87&,@'"%$'&8:3+0%;,&H/K
" K3,>6';3,8:83,RX"%5764HL5:p5:HjH%l'i4"/023,RX"/&CZot0%6>"%$'&RX6H/"Li&1ot6,0%&B57"L5:H
H/"/6,0%&C
Y'ot6,0%"%l443+"/&87~,@,i6,"%$]6,o4"%$'&H/&jH%5:>?G8:5 R13+"%5764HEff&3+g,&"%$'&j$'&l'025:H/"%5:R+3,8:l'&Hot6l44CKiz~0%&8:3^&C
H/&3+02R2$ S$43+"5:HnEff6,02H/&,@`H%5:4RX&Z3#RX6H/"l44C'&10/F3+?4?40%6'5:>a3+"%5765:Ha3+?4?G8:57&CDEQ$'&TH/"/6,025:';#H/"%3+"/&H
RX6"%3,5:45:';nRX64R1l'0%0%&"j3,RX"%5764H1@'iGl'"ff'6,"jC4l'025:';K"%$'&BH/&3+02R2$@'"%$'&P$'&l'025:H/"%5:RQC'&X '&Ciz~a"%$'&P"%3+iG87&
3+ot"/&100%&8:3^&CUH/&3+02R2$R13,Ui&5:4RX64H%5:H/"/&" 8:H/6'@Y0257;$"!FH%$457ot")R1l'"%HR13,U'6,"Zi&l4H/&CU5:V"%$'&
0%&8:3^&CH/&3+02R2$ H]>&"%576'&CD&3+028:57&10@5:#"%$'&H/&3+02R2$[H/?G3,RX&?402l4'&Ciz~0257;$"!FH%$457ot"KR1l'"%H1@"%$'&
?6H%H%57iG87&KH%l4R1RX&H%H/6,02HY"/6Z3)H/"%3+"/&,@3,4CN"%$'&10%&1ot6,0%&n"%$'&nRX6H/"P0%&1"%l'02'&C=EQ$'&="%$'&H/"%3+"/&5:HY&X^?G3,4C'&C
qt0%&1;,0%&H%H/&C |QiGl'"B'6,"H/687,&C@A>a3~.i&C45&10%&"BC'&1?&4C45:';6"%$'&n?G3+"%$"%$'0%6l';$.EQ$45:R2$.57"BE`3,H
0%&3,R2$'&C OM ;3,5:@"%$45:HR13,'6,"Mi&jH/"/6,0%&C5:n"%$'&L$'&l'025:H/"%5:Rff"%3+iG87&,@3,4Ca57"MR13,'6,"i&L57;'6,0%&CaH%5:4RX&
"%$45:HjRX6l48:CZ^5768:3+"/&B"%$'&K3,C4>a5:H%H%57iG5:8:57"F~n6,o "%$'&$'&l'025:H/"%5:R
1


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

yO} -

JL$'& } - ?G8:3,445:';L?40%6^RX&C4l'0%&Bq}H%$'6EQ5: 57;l'0%&L|ARX64H%5:H/"%H 6,oG"%$'0%&1&j>a3,5:nH/"/&1?GH1IA"%$'&ffG02H/" 5:H"/6
?40%&RX6>?Gl'"/&L"%$'&L"/&>?6,023,8Gm $ $'&l'025:H/"%5:R+@,"%$'&QH/&RX64Ca"/6K?&10%ot6,02> 3KH/&10257&H6,os)0%&8:3^&CH/&3+02R2$'&H1@
ot6,0ns uzy1{1{1{f@M5:D6,02C'&10n"/65:>?40%6,&"%$'&)$'&l'025:H/"%5:R+@3,4CD"%$'& 43,8ff5:Hn3, L H/&3+02R2$T5:D"%$'&
"/&>?6,023,80%&1;,0%&H%H%576NH/?G3,RX&,@;l45:C'&C=iz~"%$'&RX6>?Gl'"/&C.$'&l'025:H/"%5:R
n 6,"/&n"%$43+"P"%$'&]G02H/"Y3,4C.8:3,H/"
6,o"%$'&Q"%$'0%&1&YH/"/&1?GH3+0%&P5:C'&"%5:R13,8'"/6]"%$'6H/&Q6,oJL9M-'I "%$'&L6487~C45&10%&4RX&Y5:H"%$'&Y5:"/&102>&C45:3+"/&YH/"/&1?@
"%$'&BH/&10257&H6,oA0%&8:3^&CH/&3+02R2$'&H JL$'&Y?Gl'0%?6H/&Q6,o"%$'&H/&BH/&3+02R2$'&Hj5:Hff"/6C45:H%RX6,&10@'3,4C)H/"/6,0%&B5:"%$'&
$'&l'025:H/"%5:R"%3+iG87&,@5:>?40%6,&C.RX6H/"P&H/"%5:>a3+"/&HP6,offH/"%3+"/&Haqhc}c:@3+"/6>H/&1"%H|L6,offH%571&]s HP5:4C45:R13+"/&C
5: 57;l'0%&z@0%&8:3^&C.H/&3+02R2$'&H3+0%&R13+0%0257&C=6l'"Pot6,0Psu zy1{1{1{f@l4"%5:8H/6>&H/"/6,?4?G5:';RX64C457"%576
5:HjH%3+"%5:H!G&C JL$'&10%&3+0%&KH/&1,&1023,80%&3,H/643+iG87&H/"/6,?4?G5:';RX64C457"%5764H`"%$43+"QR13,i&Bl4H/&CI
q/|)H/"/6,?NEQ$'&"%$'&n8:3,H/"Qs)0%&1;,0%&H%H%576=H/&3+02R2$.C'6z&HQ'6,"Y&4RX6l4"/&10P3,~ KnY F'6^C'&)q}5:EQ$45:R2$
R13,H/&K"%$'&B0%&8:3^&CH/68:l'"%576Z5:HL5:)o}3,RX"Q3aH/68:l'"%576)"/6a"%$'&B6,0257;5:43,8?40%6,iG87&>|2
q y2|)H/"/6,?N3+"Q3n4^&C#B d+d;57,&Zs
q/|)H/"/6,?EQ$'&a"%$'&YRX6H/"6,o"%$'&Qs)FH/68:l'"%576ot6l44Ca5:HM"%$'&YH%3,>&Y3,H"%$43+"6,o"%$'&nqt
Gw|FH/68:l'"%576
qt6,0Y$'&l'025:H/"%5:RQ&H/"%5:>a3+"/&|2
q/|)H/"/6,?N3+ot"/&10Y3RX&10%"%3,5:=3,>6l4"j6,o "%5:>&,@4zl4>Ki&10j6,o &X^?G3,4C'&C'6^C'&H1@^6,0YH%5:>a5:8:3+0
5:>?G87&>&"%H"%$'&aG02H/"K"%$'0%&1& @
+ 3,R2$0%&H%l487"%HK5:D3NC45&10%&"]RX6^G;l'023+"%576D6,oj"%$'&?G8:3,4'&10@
,3 4Cl4H%l43,8:87~3,8:H/65:Z3C45&10%&4RX&5:Z?&10%ot6,02>a3,4RX& !Z"%$'&KRX6>?&1"%57"%576@G3n4^&C8:5:>a57"j3+"Qsvu
E`3,Hl4H/&C OB+ 'RX&1?4"BEQ$'&10%&a57"B5:HP&X^?G8:5:R157"%87~H/"%3+"/&C#6,"%$'&10%EQ5:H/&,@"%$45:HB5:HP"%$'&aRX6^G;l'023+"%576l4H/&C5:
"%$'&&X^?&1025:>&"%H`?40%&H/&"/&C5:)"%$'&K'&X^"LH/&RX"%576=3,HLEff&8:8
} -

F-+Z J(F @CB F
@






JL$45:HH/&RX"%576?40%&H/&"%H`3]RX6>?G3+025:H/66,o"%$'&P0%&8:3+"%57,&Y?&10%ot6,02>a3,4RX&Q6,oAJL9M-n3,4C } - 6"%$'&PC'6+
>a3,5:4HQ3,4C?40%6,iG87&>H/&1"%HQ"%$43+"YEff&10%&]l4H/&C=5:"%$'&n(+*,*+-)?G8:3,445:';RX6>?&1"%57"%576@ 3,4C=3,N3,43,87~^H%5:H
6,o "%$'&0%&H%l487"%H JL$'&B0%&H%l487"%H`?40%&H/&"/&C=$'&10%&K3+0%&ot0%6>0%&102l4445:';]i6,"%$Z?G8:3,4'&102H`6"%$'&KRX6>?&X
"%57"%576?40%6,iG87&>vH/&1"%H1@ '6,"B"%$'&3,RX"%l43,8M0%&H%l487"%HPot0%6>"%$'&RX6>?&1"%57"%576 JL$45:HP5:HBot6,0"FEff6=0%&3,H/64H1I
5702H/"1@3,H3,870%&3,C'~n>&"%576'&C@&10%0%6,02HM5:n"%$'& } - 5:>?G87&>&"%3+"%576a>a3,C'&L57"%H?&10%ot6,02>a3,4RX&L5:n"%$'&
RX6>?&1"%57"%576H/6>&1EQ$43+"Eff6,02H/&L"%$43,EQ$43+"57"5:H3,RX"%l43,8:87~R13+?G3+iG87&L6,o \z&RX64C@"%$'&Q0%&1?&3+"/&C02l44H
Eff&10%&>a3,C'&nEQ57"%$N3)>6,0%&n;,&'&10%6l4HY"%5:>&8:5:>a57"L"%$43,N"%$43+"5:>?6H/&C=C4l'025:';"%$'&nRX6>?&1"%57"%576N"/6
6,i4"%3,5:>6,0%&KC43+"%3a3,4CZ&43+iG87&3ni&1"/"/&10YRX6>?G3+025:H/6 9 8:H/6'@4H/6>&&X^?&1025:>&"%H`Eff&10%&02l4)EQ57"%$
3,87"/&10243+"%57,&LRX6^G;l'023+"%5764H6,o "%$'&`?G8:3,4'&102H &1"%3,5:87&CC'&H%RX0257?4"%5764H6,oG"%$'&LRX6>?&1"%57"%576nC'6>a3,5:4H
3+0%&;57,&Ziz~)Q6+>a3,4@ + C'&87g+3,>?1eff+7cPq(+*,*+-'@ |

~u tn-r#k
JL$'&S~uYC'6>a3,5:]>6^C'&8:HA"/023,4H/?6,0%"%3+"%576K6,o!iG3+"%R2$'&H%Y6,oG?&1"/0%687&l4>?40%6^C4l4RX"%HA"%$'0%6l';$

"


wIg

3?G57?&8:5:'&)'&1"FEff6,0%g JL$'&>a3,5:WC45&10%&4RX&ot0%6> 6,"%$'&10"/023,4H/?6,0%"%3+"%576WC'6>a3,5:4Ha5:H"%$43+""%$'&
9 Qh,h j!1 hYL IfiT jQ j ff,h !!a]2+nB2j/2F !P'
n 1 P h`L2F ff+2n+%/] ]L!j,h 3`41h LX Fn+2L%
1!!2%
2
7Y
`2+a/X%76Nj!j2B K2F27j`ff ff1!!2MhMP21F2ff
2h! !!M%24! ://+A%/QffY!XhF2z }F2!! P+%F2 !7
2! ff %XhF2 AtL2X!+2`2^h,h j!1/
1


fi

14

1000

1000

100



100

Lower Bound H*

10000

HSP* : Time (seconds)

HSP*a: Time (seconds)

10000

4

4

10

1
1

10

100

1000

TP4: Time (seconds)

q}3|

10000

10

1
1

10

100

qti|

1000

TP4: Time (seconds)

10000

13

12

TP4 (Search)
HSP*a (Rel. Search)
HSP* (Final Search)


11

10
10

100

1000

Time (seconds)

q}R|

10000

57;l'0%&LIP\z68:l'"%576)"%5:>&HLot6,0QJL9M-3,4C } - 6?40%6,iG87&>aHjH/687,&C=5:Z"%$'&S~uaC'6>a3,5:
q}3|YEQ57"%$'6l'"P"%3,'g+3+;,&0%&H/"/025:RX"%576@3,4Ckqti|QEQ57"%$."%3,'g+3+;,&0%&H/"/025:RX"%5764H JL$'&,&10%"%5:R13,8
8:5:'&B"/6"%$'&0257;$"Q5:)G;l'0%&qti|`5:4C45:R13+"/&Hj"%$'&K"%5:>&X6l'"Q8:5:>a57"qt"%$zl4H1@G"%$'&K"%$'0%&1&?65:"%H
6]"%$'&`8:5:'&RX6,0%0%&H/?64CK"/6Y?40%6,iG87&>r5:4H/"%3,4RX&H H/687,&CK6487~Biz~ } - | q}R| + ,68:l'"%576K6,o
"%$'&P876Eff&10ffi6l44C6)H/68:l'"%576RX6H/"`C4l'025:';B0%&8:3^&C3,4C'6,02>a3,8q}'6^0%&8:3^&C |H/&3+02R2$
5:?40%6,iG87&
> ffn6,oA"%$'K
& S~unC'6>a3,5:qt,&102H%576)EQ57"%$'6l'"`"%3,'g+3+;,&B0%&H/"/025:RX"%576 |
\z"%3+02H5:4C45:R13+"/&MEQ$'&10%&H/68:l'"%5764HA3+0%&ot6l44C Omn 6,"/&ff"%$43+" 3,8:8"%5:>&H%R13,87&H 3+0%&ff876,;3+0257"%$4>a5:R
?G57?&8:5:'&HM>]l4H/"`i&Q 8:87&C3+"j3,8:8 "%5:>&H1@'H/6nEQ$'&6'&BiG3+"%R2$)&"/&102Hj3]?G57?&nq}5:Hn!?Gl4H%$'&C4|3,'6,"%$'&10
iG3+"%R2$N>]l4H/"Q87&3,&]"%$'&?G57?&B3+"Q"%$'&K6,"%$'&10Q&4Cqti&=!?6,?4?&C4| JL$'&KC'6>a3,5:RX6>&HY5:Z"FEff6,&10/
H%5764H1@^6'&EQ57"%$)0%&H/"/025:RX"%5764H`6W!"%3,'g+3+;,&.q}H/?G3,RX&ot6,0Y5:"/&102>&C45:3+0%~H/"/6,023+;,&|L3,4C)6'&EQ57"%$'6l'"
H%l4R2$Z0%&H/"/025:RX"%5764H
87"%$'6l';$'&57"%$'&10JL9M-Q'6,0 } - 3,R2$457&1,&ff,&10%~P;,6z6^C0%&H%l487"%H5:B"%$45:HC'6>a3,5:@57"A5:H3,&X'3,>?G87&
6,oP3C'6>a3,5:EQ$'&10%& } - ?&10%ot6,02>aH]i&1"/"/&10"%$43,WJL9M- O[ 57;l'0%&o
H ^q}3|n3,4CN^qti|]RX6>?G3+0%&Z"%$'&
02l4"%5:>&H`6,o "%$'&B"FEff6?G8:3,4'&102Hff6Z"%$'&KH/&1"L6,o ?40%6,iG87&>aH`H/687,&CZiz~Z3+"Q87&3,H/"j6'&
& ^q}R|YRX6>?G3+0%&HP"%$'&]i&$43^576l'0Y6,o"%$'&]"FEff6Z?G8:3,4'&102HL6N6'&n&X'3,>?G87&]?40%6,iG87&>Z@ ff
57;l'0%
ot0%6>"%$'&.C'6>a3,5:k,&102H%576kEQ57"%$'6l'""%3,'g+3+;,&.0%&H/"/025:RX"%576@Q5:k>6,0%&.C'&1"%3,5:8 JL$45:Ha?40%6^5:C'&H3,
5:8:8:l4H/"/023+"%57,&&X'3,>?G87&]6,o0%&8:3^&C.H/&3+02R2$.EQ$'&N57"YEff6,0%g^HB3,HP5:"/&4C'&C \^5:4RX&Ki6,"%$N?G8:3,4'&102HQl4H/&
57"/&1023+"%57,&ffC'&1&1?&45:';LH/&3+02R2$'&H1@,"%$'&i&H/"Ag^'6EQK876Eff&10i6l44CB6"%$'&ffRX6H/"6,o'"%$'&?40%6,iG87&>H/68:l'"%576
EQ5:8:8i&5:4RX0%&3,H%5:';'@ H/"%3+0%"%5:';Not0%6>"%$'&)5:457"%5:3,8m $ &H/"%5:>a3+"/&,@"%$'0%6l';$3NH/&10257&HK6,o]qt0%&8:3^&C[3,4C
'6^0%&8:3^&C |H/&3+02R2$'&H`EQ57"%$)5:4RX0%&3,H%5:';Ki6l44C@^l4"%5:8 3nH/68:l'"%576)5:Hffot6l44CI"%$'&P;,023+?G$)?G876,"%Hff"%$45:H
&1,68:l'"%576a6,o"%$'&YH/68:l'"%576aRX6H/"`876Eff&10ffi6l44C3+;3,5:4H/""%5:>& HffR13,i&QH/&1&@'0%&1;,0%&H%H%576H/&3+02R2$
0%&3,R2$'&H3.H/68:l'"%576SqtEQ57"%$RX6H/"w(|o}3,H/"/&10"%$43,"%$'&Z'6,02>a3,80%&1;,0%&H%H%576[H/&3+02R2$TC45:H%RX6,&102H]"%$43+"
"%$'&10%&]5:HL'6H/68:l'"%576ZEQ57"%$45:)"%$'&KH%3,>&]RX6H/"Yi6l44C JL$'&B 43,8q}'6^0%&8:3^&C |ff0%&1;,0%&H%H%576=H/&3+02R2$
5: } - 5:HL3,8:H/6ao}3,H/"/&10Q"%$43,"%$43+"L6,oMJL9M-=q}3,HY5:4C45:R13+"/&C)iz~)"%$'&KH%876,?&B6,o "%$'&KR1l'0%,&|2@ C4l'&B"/6"%$'&
$'&l'025:H/"%5:RY5:>?40%6,&>&"%HQH/"/6,0%&CC4l'025:';n"%$'&B0%&8:3^&CH/&3+02R2$

"



wIg

Qh3

n-kg|

tn-r#kgq

< &10%"%3,5:g^5:4C4HM6,oA>6^C'&8GR2$'&R%g^5:';n?40%6,iG87&>aH1@zH%l4R2$3,Hff"%$'&PC'&1"/&RX"%576)6,oAC'&3,C4876^R%g^Hff3,4C3,H%H/&10%"%576
ff
^5768:3+"%5764H1@ff3+0%&&H%H/&"%5:3,8:87~zl'&H/"%5764H6,oY0%&3,R2$43+iG5:8:57"F~Uqt6,0l4'0%&3,R2$43+iG5:8:57"F~4|K5:TH/"%3+"/&X"/023,4H%57"%576
;,023+?G$4H JL$'r
& Qh3]C'6>a3,5:5:H"%$'&Y0%&H%l487"M6,o"/023,4H%8:3+"%5:';KH%l4R2$)>6^C'&8GR2$'&R%g^5:';]?40%6,iG87&>aH1@ot6,0
H/~^H/"/&>>6^C'&8:H`&X^?40%&H%H/&C5:)"%$'
& `d/+X7KH/?&R15 R13+"%576Z8:3,';l43+;,&,@G5:"/6a9 P JL$'&B?40%6,iG87&>aH

~


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

10000

HSP*a: Time (seconds)

HSP*a: Time (seconds)

10000

1000

100

10

10

100

1000

TP4: Time (seconds)

100

1

1

10000

100

TP4: Time (seconds)

q}3|

10000

qti|

57;l'0%&{zIP\z68:l'"%576)"%5:>&Hjot6,0QJL9M-3,4C } - 6Z?40%6,iG87&>aHjH/687,&C=5:#q}3|`"%$'&KQh3aC'6>a3,5:
Tq S==,SZH%l'iGH/&1"|3,4CUqti|"%$'& DC'6>a3,5:UqvShN5:4H/"%3,4RX&)H%l'iGH/&1"| JL$'&
,&10%"%5:R13,8Q8:5:'&)"/6#"%$'&Z0257;$"5:4C45:R13+"/&Hn"%$'&"%5:>&X6l'"8:5:>a57"Zqt"%$zl4H1@ff?65:"%Hn6W"%$'&=8:5:'&
RX6,0%0%&H/?64C)"/6a?40%6,iG87&>5:4H/"%3,4RX&HLH/687,&CZ6487~iz~ } - |
l4H/&CW5:T"%$'&NRX6>?&1"%57"%576W3+0%&N5:4H/"%3,4RX&H6,oB"FEff6C45&10%&"C'&3,C4876^R%gTC'&1"/&RX"%576k?40%6,iG87&>aHqt"%$'&
/C45:45:';]?G$45:876H/6,?G$'&102H%]3,4CW!6,?4"%5:R13,8"/&87&1;,023+?G$4?40%6,iG87&>aH|M6,o5:4RX0%&3,H%5:';H%571&
JL$'H
& KC'6>a3,5:n>6^C'&8:HA"%$'&`?40%6,iG87&>6,o40%&RX6^G;l'025:';P3Yo}3,l487"F~?6Eff&10M'&1"FEff6,0%g]"/6P0%&H%l'?4?G87~
RX64H%l4>&102Hn3&RX"/&C[iz~#"%$'&)o}3,l487" Y4RX&10%"%3,5:"F~[RX64RX&10245:';="%$'&Z5:457"%5:3,8MH/"%3+"/&Z6,oL"%$'&)?40%6,iG87&>
qt"%$'&zl4>Ki&10K3,4C#876^R13+"%576#6,o`o}3,l487"%H|2@l4'0%&8:5:3+iG87&n3,RX"%5764HK3,4C?G3+0%"%5:3,8@AH/6>&1"%5:>&H&1,&Do}3,8:H/&,@
6,iGH/&10%+3+"%5764H3+0%&Z5:>?6,0%"%3,"]ot&3+"%l'0%&H6,oQ"%$'&Z3+?4?G8:5:R13+"%576@iGl'"]"%$'&H/&3,H/?&RX"%HnEff&10%&H%5:>?G8:5G&C
3E`3~ot0%6>"%$'&]C'6>a3,5:l4H/&C=5:Z"%$'&KRX6>?&1"%57"%576 JL$'&KC'6>a3,5:C45:C$'6Eff&1,&10B>a3+g,&]H%57;45 R13,"
l4H/&=6,o xRX64H/"/02l4RX"%H)3,4Ck"%$'&N'&1EvC'&10257,&Ck?40%&C45:R13+"/&Hot&3+"%l'0%&N6,o9 P ( ( JL$'&
RX64H/"/02l4RX"%H3,4CnC'&10257,&C]?40%&C45:R13+"/&H R13,ni&`RX6>?G5:87&Cn3E`3~,@iGl'"6487~K3+"M3,]&X^?6'&"%5:3,8^5:4RX0%&3,H/&
5:k?40%6,iG87&>H%571& JL$'&10%&1ot6,0%&N6487~T"%$'&.H%>a3,8:87&H/")5:4H/"%3,4RX&HEff&10%&3+3,5:8:3+iG87&.5:k?G8:3,5:S\^JL_L!9`\
ot6,02>]l48:3+"%576@3,4C[i&R13,l4H/&Z6,oY"%$45:H]"%$'&1~Eff&10%&Z"%$'&Z6487~5:4H/"%3,4RX&Hn"%$43+"aJL9M-#3,4C } - RX6l48:C
3+"/"/&>?4"L"/6H/687,&
JL$',
& Qh33,4C. C'6>a3,5:4H3+0%&'6^"/&>?6,023,8@ff5:"%$'&=H/&4H/&Z"%$43+"a3,RX"%576kC4l'023+"%5764H
3+0%&.'6,")RX64H%5:C'&10%&C@jiGl'"'&57"%$'&103+0%&N"%$'&1~kH/"/025:RX"%87~TH/&zl'&"%5:3,8@c}c:@L3,RX"%5764H)R13,V"%3+g,&.?G8:3,RX&
RX64R1l'0%0%&"%87~ ff&R13,l4H/&)6,oj"%$45:H1@ JL9M-N3,4C } - Eff&10%&02l4D5:#?G3+023,8:87&8@A023+"%$'&10]"%$43,#"/&>?6,023,8@
?G8:3,445:';>6^C'&6D?40%6,iG87&>aH]5:#"%$'&H/&ZC'6>a3,5:4HX * JL$'&0%&H%l487"%H6,oL"%$'&"FEff6?G8:3,4'&102H1@ H%$'6EQ
5: 57;l'0%[
& z@ 3+0%&]H%5:>a5:8:3+0j"/6"%$'6H/&K&X'$457iG57"/&C5:Z"%$'f
& S~uC'6>a3,5:I } - 5:Hji&1"/"/&10Y"%$43,
JL9M-P6,&1023,8:8@H/687^5:';P>6,0%&ff?40%6,iG87&>aH5:]i6,"%$nC'6>a3,5:4H3,4CnH/687^5:';Y"%$'&`$43+02C'&105:4H/"%3,4RX&Ho}3,H/"/&10@
EQ$45:87&PJL9M-5:H`o}3,H/"/&10Y3+"QH/687^5:';n&3,H/~Z5:4H/"%3,4RX&H

"TOwIg 9QhhQ tn-r#k
JL$'&L9QhhQKC'6>a3,5:>6^C'&8:HffH%3+"/&8:8:57"/&Hff"%3,H/g,&C)EQ57"%$>a3+g^5:';]3,H/"/0%6'6>a5:R13,8 6,iGH/&10%+3+"%5764H OM


H%5:>?G8:5G&C\^JL_L!9`\,&102H%5766,o"%$'&BC'6>a3,5:E`3,HjC'&H%RX0257i&C5:Z\z&RX"%576 ( w !"%$'&P;,&'&1023,8C'6+
/ fi'%F2 !1 2 4,!! 21/2 2!j,2F2X 2 +%! !2 2h z+/X4 ^F% !
hF2 P j /% 2A /2M% j !j!1/7 !1 /2 + Y,2P 2h!

~,<


fi

4

4

4h

10000

HSP*a: Time

Time (seconds)

1h
1000

100

10

TP4
HSP*

1

CPT
p01

p02

p03

p04

p05

p06

p07

Problem (source parameters)

1m

1s



0.1s
0.1s

p08

q}3|

1s

1m

TP4: Time

1h

4h

qti|

57;l'0%&KzIP\z68:l'"%576"%5:>&HYot6,0PJL9M-)3,4C } 6=?40%6,iG87&>aHYH/687,&C.5:"%$'&,9QhhQC'6>a3,5:
5:8:87&CVqtiG8:3,R%g4|?65:"%H]0%&1?40%&H/&"n5:4H/"%3,4RX&H]i&876';5:';"/6."%$'&ZRX6>?&1"%57"%576?40%6,iG87&>
H/&1"1@EQ$45:87&0%&>a3,5:45:';=?65:"%Hn3+0%&Zot0%6>"%$'&H/&1"n6,oY3,C4C457"%57643,8?40%6,iG87&>aH];,&'&1023+"/&C
"%$'&RX6>?&1"%57"%576
+H/&13,"jR23,$r4C!EQH%5:$'C'6&EQZHffRX"%6$'8:l4&P>aH/6.58:l':"%N576G";l'%5:0%>&=&q}H3ot|Y6,0%0`&1"%?4$'0%&P&H/H/&&1""`%6,HBoA65:'&4H/"%?43,4R0%6,XiG&Hff87&;,>v&'&ot10%0263+>v
"/&C)EQ57"%$a"%$'&BH%3,>&
?G3+023,>&1"/&102H.qt;,0%6l'?&Ck5:"/6H%l'iRX68:l4>a4Hniz~T?G8:3,4'&10X| P487~[H/687,&CV5:4H/"%3,4RX&H3+0%&
H%$'6EQ@4H/6a'6,"Q3,8:8RX68:l4>a4Hj$43,&"%$'&H%3,>&Kzl4>Ki&10j6,o?65:"%H OM 57;l'0%&aqti|`RX6>?G3+0%&H
JL9M-n3,4C } - C4570%&RX"%87~ JL$'&Y,&10%"%5:R13,88:5:'&Q"/6n"%$'&P0257;$"`5:4C45:R13+"/&Hff"%$'&Y"%5:>&X6l'"j8:5:>a57"
qt"%$zl4H1@4?65:"%H`6Z"%$'&K8:5:'&P3+0%&K5:4H/"%3,4RX&HLH/687,&CZiz~ } - iGl'"L'6,"Liz~)JL9M-z|

>a3,5:@^"%$'&10%&R13,Zi&B>6,0%&P"%$43,Z6'&H%3+"/&8:8:57"/&,@^&3,R2$&zl457?4?&CEQ57"%$)>6,0%&B"%$43,)6'&5:4H/"/02l4>&"1@
3,4CNC45&10%&"P5:4H/"/02l4>&"%HQ$43,&C45&10%&"P5:>a3+;5:';R13+?G3+iG5:8:57"%57&H]q}R13,8:87&CV/>6^C'&H%|2@EQ$45:R2$=>a3~
6,&1028:3+?qti&1"FEff&1&=5:4H/"/02l4>&"%Hff3,4C)i&1"FEff&1&=H%3+"/&8:8:57"/&H| H
+ 3,R2$Z;,63,85:H`"/6a$43,&K3,Z5:>a3+;,&B"%3+g,&
6,oA3]H/?&R15 RL"%3+0%;,&1"1@45:3]H/?&R15 RQ>6^C'& H`5:a"%$'&K\^JL_L!9`\n,&102H%576@z"%3+g^5:';n3,5:>a3+;,&Y0%&zl4570%&H
"%$'&a0%&87&1+3,"K5:4H/"/02l4>&"P"/6=i&?6Eff&10%&C#6D3,4C#R13,8:57i4023+"/&C@A3,4C#"/6=R13,8:57i4023+"/&3,#5:4H/"/02l4>&"
"%$'&H%3+"/&8:8:57"/&>]l4H/"i&?65:"/&C"/6E`3+02C4H]3R13,8:57i4023+"%576"%3+0%;,&1" Jl'0245:';)"%5:>&HBi&1"FEff&1&C45&10/
&"C4570%&RX"%5764HY+3+0%~ HB3,#3,C4C457"%57643,8RX6>?G8:5:R13+"%576@3+">6H/"B6'&a5:4H/"/02l4>&"Y6i63+02C.&3,R2$
H%3+"/&8:8:57"/&BR13,Zi&P?6Eff&10%&CZ63+"L3,~"%5:>& JL$zl4H1@^"/6>a5:45:>a571&Q6,&1023,8:8&X^&R1l'"%576Z"%5:>&P0%&zl4570%&H
3R13+0%&1o}l48H/&87&RX"%576=6,oEQ$45:R2$H%3+"/&8:8:57"/&)q}3,4CN5:4H/"/02l4>&"|ff"/6l4H/&Kot6,0Y&3,R2$N6,iGH/&10%+3+"%576@ 3,4C="%$'&
6,02C'&10Q5:)EQ$45:R2$)&3,R2$NH%3+"/&8:8:57"/&BR13+0%0257&Hj6l'"L"%$'&6,iGH/&10%+3+"%5764Hj57"L$43,HLi&1&Z3,H%H%57;'&C
JL$45:HC'6>a3,5:V5:H)$43+02CVot6,0)i6,"%$SJL9M-T3,4C } - @Qot6,0H/&1,&1023,8P0%&3,H/64H1I 5702H/"1@Q3,HZ3,870%&3,C'~
>&"%576'&C@z"%$'&BRX6,0%&P6,o"%$'&BC'6>a3,5:5:Hff3nRX6>KiG5:43+"%5763,)3,H%H%57;4>&"ff?40%6,iG87&> 3,4C)3]JY\^9F8:57g,&
?40%6,iG87&>Z@^i6,"%$)6,o EQ$45:R2$Z3+0%&$43+02C)6,?4"%5:>a573+"%576)?40%6,iG87&>aH 8:H/6'@'"%$'&]m $ $'&l'025:H/"%5:RQ"/&4C4Hj"/6ai&
?G3+0%"%5:R1l48:3+0287~]Eff&3+ga6)JY\^93,4C0%&8:3+"/&C?40%6,iG87&>aHYqt"%$'&PH%3,>&QEff&3+g^'&H%H`$43,Hff3,8:H/6]i&1&'6,"/&Ciz~
\^>a57"%$=q(+*,*+-z|Mot6,0"%$'&Q?G8:3,445:';Y;,023+?G$$'&l'025:H/"%5:R+@EQ$45:R2$a5:HM&H%H/&"%5:3,8:87~]"%$'&YH%3,>&Y3,Hffm $ | \z&RX64C@
3,RX"%576=C4l'023+"%5764H`5:)"%$45:HjC'6>a3,5:ZC45&10jiz~Z8:3+0%;,&3,>6l4"%HL3,4C3+0%&K3+"L"%$'&H%3,>&"%5:>&H/?&R15G&C
EQ57"%$T3.,&10%~[$457;$0%&H/68:l'"%576 O[ 6,0&X'3,>?G87&,@ff5:?40%6,iG87&
> ff>.6'&3,RX"%576k$43,H3C4l'023+"%576[6,o
w+{ (+*+-#3,4C[3,'6,"%$'&103.C4l'023+"%576#6,
,(z{ , S$'&[l4H%5:'; L
EQ57"%$D"/&>?6,023,80%&1;,0%&H%H%576@"%$'&
RX6H/"Bi6l44C="/&4C4HP"/65:4RX0%&3,H/&niz~="%$'&n;R1Ckqt;,0%&3+"/&H/"KRX6>a>6C457^5:H/6,0X|j6,off3,RX"%576#C4l'023+"%5764HY5:

~



fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

&3,R2$D57"/&1023+"%576@&X'RX&1?4"Kot6,0B"%$'&G02H/"Pot&1E57"/&1023+"%5764HX/ !"%$'&x9QhhQZC'6>a3,5:@"%$'&;R1C#6,o
3,RX"%576ZC4l'023+"%5764H`5:Hff"F~z?G5:R13,8:87~,&10%~)H%>a3,8:8 qt6)"%$'&P6,02C'&10j6,o * * | <ff6>KiG5:'&CEQ57"%$"%$'&PEff&3+g^'&H%H
6,oP"%$'&.m $ $'&l'025:H/"%5:R+@EQ$45:R2$W>&3,4Ha"%$'&NC45&10%&4RX&i&1"FEff&1&W"%$'&N5:457"%5:3,8`$'&l'025:H/"%5:R)&H/"%5:>a3+"/&=6,o
"%$'&H/68:l'"%576NRX6H/"aq}>a3+g,&H/?G3, |L6,off3?40%6,iG87&>3,4CN"%$'&3,RX"%l43,8 6,?4"%5:>a3,8RX6H/"B5:HY6,ot"/&8:3+0%;,&,@"%$45:H
0%&H%l487"%H5:K3,n3,8:>6H/" 3,H/"/0%6'6>a5:R13,8zzl4>Ki&106,o4 L
57"/&1023+"%5764HAi&5:';Q0%&zl4570%&Ci&1ot6,0%&ff3YH/68:l'"%576
5:Hffot6l44C JA63,65:C)"%$45:HBq}H/6>&1EQ$43+"j3+0%"%5 R15:3,8t|?40%6,iG87&>Z@^3,RX"%576ZC4l'023+"%5764HEff&10%&P0%6l44C'&Cl'?)"/6
"%$'&P'&3+0%&H/"`5:"/&1;,&10j5:a"%$'&Q&X^?&1025:>&"%HffC'6'&Y5:a"%$45:HffC'6>a3,5: JL$45:H5:4RX0%&3,H/&H"%$'&P>a3+g,&H/?G3,6,o
"%$'&j?G8:3,4Hot6l44C@,iGl'"'6,"M,&10%~n>]l4R2$]K6a3,&1023+;,&Qiz~(z{ @z3,4C3+">6H/"iz~z{ q}RX6>?G3+025:H/6
>a3,C'&B6"%$'&B?40%6,iG87&>aH`"%$43+"QRX6l48:C)i&H/687,&CZEQ57"%$)6,0257;5:43,8C4l'023+"%5764H|% $
l'&"/6Z"%$'&nEff&3+g^'&H%HB6,o"%$'&m $ $'&l'025:H/"%5:RK5:N"%$45:HPC'6>a3,5:@"%$'&&X6,0%"5:,&H/"/&C.iz~ } - 5:
RX6>?Gl'"%5:';B3>6,0%&L3,R1R1l'023+"/&Q$'&l'025:H/"%5:RffR13,ai&`&X^?&RX"/&C"/6?G3~n6+ @0%&H%l487"%5:';P5:3Bi&1"/"/&10M6,&1023,8:8
02l4"%5:>&Yot6,0 } - RX6>?G3+0%&C)"/6aJL9M- JL$45:Hff5:Hj5:4C'&1&C"%$'&R13,H/&,I3,87"%$'6l';$ } - H/687,&H`6487~"%$'&
G,&H%>a3,8:87&H/"P?40%6,iG87&>aHB5:N"%$'&H/&1"q}H%$'6EQ#3,HBiG8:3,R%g.?65:"%HB5: 57;l'0%&a'qti|/|2@ JL9M-H/687,&H6487~
ot6l'0L6,o"%$'6H/&,@G3,4C5:HjH%8:57;$"%87~H%876Eff&10L6>6H/"L6,o"%$'&> JL$'&H/&B0%&H%l487"%H1@'$'6Eff&1,&10@ 3+0%&K'6,"Qzl457"/&
0%&1?40%&H/&"%3+"%57,&
JL$'
& 9QhhQBC'6>a3,5:a$43,H38:3+0%;,&Qzl4>Ki&10M6,o?40%6,iG87&> ?G3+023,>&1"/&102H1I"%$'&Yzl4>Ki&106,o;,63,8:H
3,4C"%$'&Qzl4>Ki&106,oH%3+"/&8:8:57"/&H1@5:4H/"/02l4>&"%HM3,4C"%$'&Q5:4H/"/02l4>&"R13+?G3+iG5:8:57"%57&H1@G1ec:@EQ$45:R2$C'&1"/&10/
>a5:'&B"%$'&Kzl4>Ki&10L6,o E`3~^HQ"/63,R2$457&1,&K&3,R2$=;,63,8 9M0%6,iG87&>5:4H/"%3,4RX&HQl4H/&C5:Z"%$'&KRX6>?&1"%57"%576
Eff&10%&Q;,&'&1023+"/&C023,4C'6>a87~,@EQ57"%$+3+0%~^5:';B?G3+023,>&1"/&10`H/&1"/"%5:';HX . JL$'&LRX6>?&1"%57"%576a?40%6,iG87&> H/&1"1@
EQ$45:R2$D$43,H]"/6N6+&10R2$43,8:87&';5:';=?40%6,iG87&>aH"/6.3=EQ5:C'&+3+0257&1"F~#6,oL?G8:3,4'&102Hqti6,"%$D6,?4"%5:>a3,8ff3,4C
H%l'i6,?4"%5:>a3,8t|EQ$45:87&Bot6,0Y?4023,RX"%5:R13,8A0%&3,H/64HP'6,"Qi&5:';a"/6z6)8:3+0%;,&,@H%R13,87&HYl'?N"%$'&]C45&10%&"Q?G3+023,>n
&1"/&102HPzl457"/&KH/"/&1&1?G87~,@3,4CZ>6,0%&]5:>?6,0%"%3,"%87~ZRX6"%3,5:4HQ6487~Z6'&K?40%6,iG87&>5:4H/"%3,4RX&Kot6,0Y&3,R2$
H/&1"j6,o ?G3+023,>&1"/&102HLl4H/&C Q6Eff&1,&10@G"%$'&$43+02C4'&H%H`6,o 3n?40%6,iG87&>5:4H/"%3,4RX&>a3~)C'&1?&4CZ3,HL>]l4R2$
q}57o'6,">6,0%&|Q6."%$'&n023,4C'6>&87&>&"%HB6,o"%$'&?40%6,iG87&>;,&'&1023+"%576kqtEQ$45:R2$.5:4R18:l4C'&,@Mc c:@"%$'&
"%l'0245:';]"%5:>&Hji&1"FEff&1&"%3+0%;,&1"%HY3,4CZ"%$'&3,RX"%l43,8A3,8:876^R13+"%576)6,oR13+?G3+iG5:8:57"%57&H`3,4CR13,8:57i4023+"%576"%3+0/
;,&1"%H"/65:4H/"/02l4>&"%H|Q3,HB6"%$'&aH/&1"/"%5:';HB6,off"%$'&aRX6"/0%68:8:3+iG87&n?G3+023,>&1"/&102H JA6=5:,&H/"%57;3+"/&a"%$'&
5:>?6,0%"%3,4RX&L6,o"%$'&Q023,4C'6>?40%6,iG87&> &87&>&"%Hffot6,0ff?40%6,iG87&>$43+02C4'&H%H1@3,4C"/6]6,i4"%3,5:3i40%63,C'&10
iG3,H%5:HPot6,0"%$'&aRX6>?G3+025:H/6.i&1"FEff&1&DJL9M-3,4C } - @"/&#3,C4C457"%57643,8?40%6,iG87&>aHYEff&10%&a;,&'&1023+"/&C
q}l4H%5:';"%$'&P3+3,5:8:3+iG87&Q?40%6,iG87&>;,&'&1023+"/6,0X|ot6,0`&3,R2$6,o"%$'&Q?G3+023,>&1"/&10jH/&1"/"%5:';H`RX6,0%0%&H/?64C45:';B"/6
"%$'&n&57;$"H%>a3,8:87&H/"Y?40%6,iG87&>aHY5:N"%$'&RX6>?&1"%57"%576.H/&1" JL$'&nC45:H/"/0257iGl'"%5766,offH/68:l'"%576N"%5:>&HYot6,0
JL9M-'@ } - 3,4CN<`9Jqt"%$'&6487~6,?4"%5:>a3,8"/&>?6,023,8?G8:3,4'&10ji&H%5:C'&HjJL9M-a3,4C } - "/6?G3+0%"%5:R15
fi4]2+_./10
2 /%L2h F% j2ffF% +2 P1nQ/a2A F% +2 '2+Q `j/21
2/%!}F% +2F+%O&ff2+&2 1!h2+'2+% 2h*,+!jM`/K22h F% M2+ff h!j!1 ff!},+``,+ ]/2F
hF% L h/2/j!}2A /2}!} j+% 22h+2+/`M+/ MM!}2,%X
L,+] L! hF% :2ff,h`}F2+%F[

mQ/%F,F]+%ff 1!1 n+2,!M
,j7P2Fh2F/P2 2h F% 27+! F2 % !hA24F2F ` /Y
!hF% /%FP+2!
%1 L2 t/2,Q!}2/1B f}F2B j /% F!j FaYL2X!+22Y~
L24 F2X!2` 2 ,h,+ YF2F~ t2+5 f,+/%F' Y`2 +2
2h F% G7!G 2 P22222^j24hF2 F4 2z/ 4!j,h 2z vafiavv
L2 hffff Y/%F]}F2!!ff /PF2X` ! j+2]Q 2 [

m`/%F] Y2 +2
F% !G f}F2F!j A2 /2 `21YL2 !h X!! Y!hF2 2Y,!
1!}
!T!hF%2/2P, +Y% mv fi tlfil:l 5;m'v!kfiAv;`s ~~;`tlA !~
2 +%F2jhh%ff,h 2'%! !!L% ff#ff,h2^ }j!1 ,h%!
ff,h'2 h!1Gh% `j!!2F2ff,h'2,F%h!2+ff,h42,h% jX2 !

~

>

fi

4

4

?G3+"/&P5:a"%$'&PRX6>?&1"%57"%576 | 6&3,R2$6,o"%$'&Y0%&H%l487"%5:';B?40%6,iG87&>H/&1"%H`5:HffH%$'6EQ5: 57;l'0%&P'q}3| JL$'&
5:4H/"%3,4RX&Hj"%$43+"jEff&10%&?G3+0%"j6,o"%$'&RX6>?&1"%57"%576)?40%6,iG87&>H/&1"L3+0%&KH%$'6EQ)iz~ 8:87&CqtiG8:3,R%g4|ff?65:"%H
<`87&3+0287~,@'"%$'&+3+025:3+"%5765:)?40%6,iG87&>$43+02C4'&H%HL5:HLRX64H%5:C'&1023+iG87&B3,4CZ6,o "%$'&?40%6,iG87&>aHj5:)"%$'&KRX6>n
?&1"%57"%576[H/&1"aH/6>&3+0%&Z,&10%~&3,H/~[3,4CTH/6>&3+0%&Z,&10%~$43+02C@M0%&8:3+"%57,&Z"/6"%$'&H/&1"6,oY?40%6,iG87&>aH
;,&'&1023+"/&CEQ57"%$)"%$'&KH%3,>&?G3+023,>&1"/&102H
57;l'0%&n'qti|LRX6>?G3+0%&HPJL9M-)3,4C } - 6="%$'&]&X^"/&4C'&C=?40%6,iG87&>H/&1" } - H/687,&HB, 6,o
"%$45:H`H/&1"1@'EQ$45:87&YJL9M-H/687,&HL^uw q}3H%l'iGH/&1"ff6,o"%$'6H/&BH/687,&C)iz~ } - | Q6Eff&1,&10@ 3,HjR13,)i&BH/&1&
5:"%$'&YG;l'0%&,@z"%$'&B0%&8:3+"%57,&Y?&10%ot6,02>a3,4RX&P6,oA"%$'&P"FEff6?G8:3,4'&102Hff5:H`3,8:H/6$457;$487~n+3+0257&C@'>]l4R2$Z>6,0%&
H/6a"%$43,Z"%$'&B0%&H%l487"%H`6"%$'&RX6>?&1"%57"%576Z?40%6,iG87&>H/&1"QH%l';,;,&H/"%H

Q % tn-r#k
JL$'&[Q%ZC'6>a3,5:N>6^C'&8:HQ"%$'&n>6,&>&"%HB6,o3,5702RX023+ot"Y6N"%$'&];,0%6l44CN3+"P3,.3,570%?6,0%"

"



wIg

J $'&
L
;,63,85:HB"/6;l45:C'&a3+0%0257^5:';)3,5702RX023+ot""/6?G3+0%g^5:';Z?6H%57"%5764HP3,4C#C'&1?G3+0%"%5:';3,5702RX023+ot"B"/6=3H%l457"%3+iG87&
02l4E`3~]ot6,0M"%3+g,&16+ @^3,876';B"%$'&L3,570%?6,0%"'&1"FEff6,0%gn6,o "%3'57E`3~^H JL$'&L>a3,5:RX6>?G8:5:R13+"%5765:H "/6Bg,&1&1?
"%$'&P3,5702RX023+ot"`H%3+ot&87~aH/&1?G3+023+"/&CIM3+"`>6H/"`6'&P3,5702RX023+ot"ffR13,6^R1R1l'?z~3K02l4E`3~a6,0`"%3'57E`3~H/&1;>&"
3+"Q3,~"%5:>&,@43,4CZC'&1?&4C45:';]6)"%$'&H%571&B6,o"%$'&3,5702RX023+ot"L3,4C)"%$'&K8:3~,6l'"j6,o"%$'&3,570%?6,0%"j'&3+0%iz~
H/&1;>&"%HQ>a3~)i&BiG876^R%g,&C3,HLEff&8:8
JL9M-ZH/687,&HB6487~w6l'"P6,off"%$'&+*)?40%6,iG87&>v5:4H/"%3,4RX&HB5:N"%$45:HPC'6>a3,5: O] 6,0"%$'&5:4H/"%3,4RX&H
H/687,&Cniz~]JL9M-'@"%$'&jzl4>Ki&10 6,o'6^C'&H&X^?G3,4C'&Cn5:nH/&3+02R2$a5:H,&10%~nH%>a3,8:8z0%&8:3+"%57,&`"/6B"%$'&jH/68:l'"%576
C'&1?4"%$qt"%$'6l';$[ot6,0a"%$'&Z8:3+0%;,&105:4H/"%3,4RX&H1@'6^C'&)&X^?G3,4H%576T5:H],&10%~[H%876E@0%&H%l487"%5:';.5:[3.?6z6,0
02l4"%5:>&Z6,&1023,8:8t| JL$45:H5:>?G8:57&Hn"%$43+"aot6,0"%$'&H/&?40%6,iG87&> 5:4H/"%3,4RX&H"%$'&.m $ $'&l'025:H/"%5:R)5:Ha,&10%~
3,R1R1l'023+"/&,@43,4C"%$zl4Hff"%$'&1~3+0%&B5:3]H/&4H/&)!&3,H/~^^4ot6,0jH%l4R2$)5:4H/"%3,4RX&H1@ } - R13,Z'6,"ffi&Y&X^?&RX"/&C
"/6Zi&]i&1"/"/&10@H%5:4RX&n"%$'&H/&3+02R2$.&X6,0%"57"B5:,&H/"%HB5:"/6ZRX6>?Gl'"%5:';)3)>6,0%&3,R1R1l'023+"/&$'&l'025:H/"%5:RK5:H
8:3+0%;,&87~]E`3,H/"/&C@iGl'"57"M3,8:H/65:4C45:R13+"/&H "%$43+"3>6,0%&L3,R1R1l'023+"/&Y$'&l'025:H/"%5:Rff5:HM'&1&C'&Cn"/6KH/687,&/$43+02C4
?40%6,iG87&>5:4H/"%3,4RX&H
Q6Eff&1,&10@ } - H/687,&HP6487z
~ a?40%6,iG87&>aH1@3ZH%l'iGH/&1"P6,o"%$'6H/&aH/687,&C.iz~NJL9M-'@A3,4CN"%3+g,&Ho}3+0
>6,0%&B"%5:>&Pot6,0L&3,R2$ 57;l'0%&w1*4q}3|jH%$'6EQH`"%$'&B"%5:>& } - H/?&4C4H`5:0%&1;,0%&H%H%576H/&3+02R2$3,4CZ5:
"%$'&Q 43,8q}'6^0%&8:3^&C |H/&3+02R2$)ot6,0ff&3,R2$)6,o"%$'f
& Q%5:4H/"%3,4RX&Hff57"ffH/687,&H OM 6,0`0%&1ot&10%&4RX&,@^"%$'&
H/&3+02R2$="%5:>&ot6,0YJL9M-5:HL3,8:H/65:4R18:l4C'&C <`87&3+0287~,@4"%$'&0%&8:3^&C=H/&3+02R2$NRX64H%l4>&HQ3876,"L6,o "%5:>&K5:
"%$45:HjC'6>a3,5:@43,4CZ6+&102Hj,&10%~Z8:57"/"%87&B5:Z"%$'&BE`3~Z6,o $'&l'025:H/"%5:RP5:>?40%6,&>&"L5:Z0%&1"%l'02 JL$43+"L"%$'&
$'&l'025:H/"%5:RB5:>?40%6,&>&"Y5:HQH%>a3,8:8q}R1876H/&K"/6'6^&X'5:H/"/&"|L5:HL&3,H%5:87~&X^?G8:3,5:'&C@GH%5:4RX&,@G3,HY3,870%&3,C'~
6,iGH/&10%,&C@A"%$'&m $ $'&l'025:H/"%5:R]5:HB3,870%&3,C'~N,&10%~3,R1R1l'023+"/&a6"%$'&H/&a?G3+0%"%5:R1l48:3+0B?40%6,iG87&>v5:4H/"%3,4RX&H
JL$'&zl'&H/"%576@'"%$'&@45:H`EQ$~"%$'&0%&8:3^&CH/&3+02R2$=5:HjH/6a"%5:>&RX64H%l4>a5:';
JL$'&)3+?4?G3+0%&"n0%&3,H/6T5:H]"%$43+"a5:D"%$45:HnC'6>a3,5:@MH/&3+02R2$T5:"%$'&0%&1;,0%&H%H%576TH/?G3,RX&Z5:Hn>6,0%&
&X^?&4H%57,&]"%$43,H/&3+02R2$5:="%$'&'6,02>a3,80%&1;,0%&H%H%576.H/?G3,RX& JL$45:HY5:HYRX6"/023+0%~N"/6Z"%$'&n3,H%H%l4>?4"%576
H/"%3+"/&CD5:D\z&RX"%576D w+@A"%$43+""%$'&aRX6H/"6,o`&X^?G3,4C45:';)3H/"%3+"/&H%$'6l48:CNi&aH%>a3,8:87&10B5:."%$'&0%&8:3^&C
0%&1;,0%&H%H%576H/?G3,RX&,@+C4l'&"/6Y3LH%>a3,8:87&10i4023,4R2$45:';`o}3,RX"/6,0 J3+iG87&Lw1*4qti|C45:H/?G8:3~^HH/6>&R2$43+023,RX"/&1025:H/"%5:R1H
6,o"%$'&]'6,02>a3,8A3,4C.0%&1;,0%&H%H%576NH/?G3,RX&HQot6,f
0 Q%)5:4H/"%3,4RXf
& ff.qt"%$'&nH%>a3,8:87&H/"Q5:4H/"%3,4RX&]'6,"
H/687,&Ciz~JL9M-z| OM 3+"%3n5:HRX68:87&RX"/&C)C4l'025:';B"%$'&QG02H/"Pqto}3,5:87&C | 57"/&1023+"%5766,o L

L \z"%3+"/&H
5:#"%$'&)'6,02>a3,80%&1;,0%&H%H%576DH/?G3,RX&ZRX6"%3,5:@ 6[3,&1023+;,&,@ff3N8:3+0%;,&)zl4>Ki&10K6,oQH%l'i4;,63,8:H1@ EQ$45:87&a5:
"%$'&a0%&1;,0%&H%H%576.H/?G3,RX&,@H/"%3+"/&HRX6,0%0%&H/?64C45:';a"/6NP_jF'6^C'&HY3+0%&niz~=C'&X 457"%576N8:5:>a57"/&C=5:NH%571&
<ff64H/&zl'&"%87~,@'"%$'&Pi4023,4R2$45:';no}3,RX"/6,0L6,oMP_jF'6^C'&Hj5:Z0%&1;,0%&H%H%576Z5:HjH%>a3,8:87&10q}H%5:4RX&P"%$'&R2$'65:RX&
6,oL&H/"%3+iG8:5:H%$'&10Kot6,0]&3,R2$TH%l'i4;,63,8ff5:H]3=?6,"/&"%5:3,8i4023,4R2$D?65:"|2@ iGl'"K'6,"]iz~D>]l4R2$I"%$'&)>a3,~
H%l'i4;,63,8:Ha5:T"%$'&='6,02>a3,8`0%&1;,0%&H%H%576W5:"/&1023,RX"1@j0%&H%l487"%5:';5:[0%&8:3+"%57,&87~ot&1ERX64H%5:H/"/&"R2$'65:RX&H

~1


fi (^z7G

10000

4 % : 4^-G-Ns4-G^

1 f

n

HSP*a (Rel. Search)
HSP* (Final Search)


1000

TP4 (Search)

6,02>a3,8

F_L&1;,0%&H%H%576
P_
K nY

8>

9z{

85F 8>

w+{ *

(z{ +

i4023,4R2$45:';
o}3,RX"/6,0

w+{

w+{ *

z{ *

Time (sec.)

100

10

1

0.1

0.01

p01

p02

p03

q}3|

p04

p05

p10

p11

*^{

qti|

57;l'0%&aw1*^I]q}3|JL5:>&WH/?&"D5:0%&1;,0%&H%H%576 H/&3+02R2$3,4C5:x 43,8=q}'6^0%&8:3^&C |NH/&3+02R2$6
Q%N5:4H/"%3,4RX&HH/687,&Ciz~ } - JL$'&aH/&3+02R2$#"%5:>&ot6,0KJL9M-5:HB3,8:H/6H%$'6EQot6,0
RX6>?G3+025:H/6
n 6,"/&L"%$'&L876,;3+0257"%$4>a5:R"%5:>&jH%R13,87&,I H/&3+02R2$"%5:>&H ot6,0 } - 3,4CJL9M-3+0%&
'&3+0287~N5:C'&"%5:R13,8@EQ$45:87&]"%$'&a0%&1;,0%&H%H%576H/&3+02R2$#RX64H%l4>&HBH/&1,&1023,86,02C'&102HP6,off>a3+;+
457"%l4C'&B>6,0%&B"%5:>& qti|j<`$43+023,RX"/&1025:H/"%5:R1H`6,o "%$'&'6,02>a3,83,4C0%&1;,0%&H%H%576H/?G3,RX&Hjot6,0
Q%Z5:4H/"%3,4RX& ff4{I 8>'5:HL"%$'&]3,&1023+;,&aH/"%3+"/&nH%571&,H 85F* 85F '"%$'&]3,&1023+;,&023+"%5766,o
H%l4R1RX&H%H/6,0LH/"%3+"/&H%571&P"/6n"%$'&BH%571&Y6,oA"%$'&P?40%&C'&RX&H%H/6,0jH/"%3+"/& 3+"%3a5:HffRX68:87&RX"/&CZC4l'025:';
"%$'&BG02H/"Kqto}3,5:87&C |57"/&1023+"%576Z6,o L

L

:8 H/6'@ff"%$'&Z0257;$"!FH%$457ot"R1l'"02l487&,@EQ$45:R2$[&8:5:>a5:43+"/&HH/6>&Z0%&C4l44C43,"ni4023,4R2$'&H1@`5:Hl4H/&CW5:["%$'&
'6,02>a3,8P0%&1;,0%&H%H%576UH/?G3,RX&,@PiGl'"Z'6,"EQ$'&V&X^?G3,4C45:';kP_jF'6^C'&HZ5:U0%&1;,0%&H%H%576 Q6Eff&1,&10@
0%&1;,0%&H%H%576"/&4C4H"/6Y>a3+g,&ffH/"%3+"/&HP!;,0%6EQ^@'c}c:@H%l4R1RX&H%H/6,0 H/"%3+"/&HA;,&'&1023,8:87~BRX6"%3,5:K>6,0%&ffH%l'i4;,63,8:H
"%$43,)"%$'&570ff?40%&C'&RX&H%H/6,02H1@43,4CEQ$45:87&Q"%$45:Hff&X&RX"L5:H`zl457"/&P>6^C'&1023+"/&B5:)'6,02>a3,8 0%&1;,0%&H%H%576@zEQ$'&10%&
H%l4R1RX&H%H/6,02H`$43,&,@'6)3,&1023+;,&,@ >6,0%&BH%l'i4;,63,8:H1@^57"ff5:H`>]l4R2$)>6,0%&Y?40%6'6l44RX&Cot6,0`"%$'&PH%>a3,8:87&10
H/"%3+"/&HMRX6,0%0%&H/?64C45:';Y"/6KP_jF'6^C'&H5:]"%$'&L0%&1;,0%&H%H%576H/?G3,RX&,@EQ$'6H/&jH%l4R1RX&H%H/6,02HM3+0%&`63,&1023+;,&
(z{ +e}tj8:3+0%;,&10 OM HL30%&H%l487"1@'H%l4R1RX&H%H/6,02HL"/6)P_jF'6^C'&Hj3+0%&K3,8:8 K
nY F'6^C'&H1@4EQ57"%$Z3,3,&1023+;,&
6,o3+i6l'J
" z{ H%l'i4;,63,8:HL3,4C*^{ H%l4R1RX&H%H/6,02H]q}H%l'iGH/&1"%Hj6,o H%571&]|
JA6WH%l4>a>a3+02571&,@Y&3,R2$U&X^?G3,4C'&CrP_jF'6^C'&5:U0%&1;,0%&H%H%576U0%&H%l487"%Hqt^5:3T3,U5:"/&102>&C45:3+"/&
/8:3~,&102=6,o KnY F'6^C'&H|B5:#3,D3,&1023+;,&)6,oJ9{ (='&1EP_jF'6^C'&H Op+ ,&#"%$'6l';$D>6H/"K6,o`"%$'&>
q*-'{ ( a|3+0%&`ot6l44C5:]"%$'&j L H/687,&Cn"%3+iG87&,@3,4Cn"%$'&10%&1ot6,0%&LC'6 "$43,&L"/6Bi&jH/&3+02R2$'&C@"%$'6H/&
"%$43+"Y0%&>a3,5:~^57&8:C3,=&X&RX"%57,&.2P_j"/6+!P_Qi4023,4R2$45:';ao}3,RX"/6,0P6,oLwz{ Nq(,z{ 6,c
9{ (|2@"/6i&
RX6>?G3+0%&C#EQ57"%$."%$'&ai4023,4R2$45:';)o}3,RX"/6,0K6,oBw+{ )ot6,0K'6,02>a3,80%&1;,0%&H%H%576 Oa ;3,5:@A"%$'&a?40%6,iG87&>5:H
'6,"j"%$'&$457;$)i4023,4R2$45:';]o}3,RX"/6,0Y5:Z57"%H/&87ohI57"L5:H`"%$43+"j"%$'&i4023,4R2$45:';]o}3,RX"/6,0Y5:)"%$'&B0%&8:3^&CZH/&3+02R2$
H/?G3,RX&Y5:HMo}3+0Q7+'Xd"%$43,57"5:HMot6,0`'6,02>a3,840%&1;,0%&H%H%576@z3,4Ca"%$43+"ffH/&3+02R2$)5:"%$'&P0%&1;,0%&H%H%576H/?G3,RX&
5:HMRX64H/&zl'&"%87~n>6,0%&L&X^?&4H%57,&`"%$43,aH/&3+02R2$5:n"%$'&L'6,02>a3,8'0%&1;,0%&H%H%576aH/?G3,RX&,@023+"%$'&10M"%$43,a87&H%H

~~


fi

Type
Type II
Type III

100



100

1000

10

1
1

10

100

1000

TP4: Time (seconds)

q}3|

10000

1000

100



1000

10000

HSP* : Time (seconds)

HSP* : Time (seconds)

10000



HSP* : Time (seconds)

10000

4

4

10

1
1

10

100

1000

TP4: Time (seconds)

qti|

10000

10

1
1

10

100

1000

TP4: Time (seconds)

10000

q}R|

57;l'0%&aw,w+IP\z68:l'"%576."%5:>&HPot6,0KJL9M-3,4C."%$'0%&1&aC45&10%&"RX6^G;l'023+"%5764HP6,o } - 6?40%6,iG87&>aH
H/687,&C5:"%$'{
& 3SaC'6>a3,5:IKq}3| } - EQ57"%$=s)0%&1;,0%&H%H%576.8:5:>a57"/&C"/6)su 6487~
qti|#/l448:5:>a57"/&C4 } - qt?&10%ot6,02>aHas)0%&1;,0%&H%H%5764Hot6,0)5:4RX0%&3,H%5:';#s l4"%5:8L&57"%$'&10)3
'6^0%&8:3^&C.H/68:l'"%576N5:HYot6l44C@ 6,0B"%$'&n&H/"%5:>a3+"/&C.RX6H/"B6,o"%$'&]"/6,?#87&1,&8;,63,8:HBC'6z&H
'6,"5:4RX0%&3,H/&|2`q}R|%- } - q}3,87E`3~^HB?&10%ot6,02>aHBQ3,4CN-+0%&1;,0%&H%H%576 | JL$'&8:5:'&H
"/6"%$'&K0257;$"Y3,4C"/6,?.5:ZG;l'0%&H]qti|j3,4C[q}R|L5:4C45:R13+"/&"%$'&K"%5:>&X6l'"Y8:5:>a57" /J~z?&K
N%%!0%&1ot&102HP"/6"%$'&aR18:3,H%H%5 R13+"%576N6,off"%$'&?40%6,iG87&>v5:4H/"%3,4RX&HBC'&H%RX0257i&C.5:#\z&RX"%576
- =qt?G3+;,&](, ,|

"


wIg

3S tn-r#k

JL$'f
& 3SC'6>a3,5:=>6^C'&8:HL"%$'&[NJY\R13,8:8AH/&1"!Fl'?.?40%6^RX&C4l'0%&ot6,0PC43+"%33+?4?G8:5:R13+"%5764HL5:=>6,iG5:87&
"/&87&1?G$'6'&H JL$'&)C'6>a3,5:[5:Hn3,RX"%l43,8:87~D3.H%R2$'&C4l48:5:';=?40%6,iG87&>Z@H%5:>a5:8:3+0K"/6?G6EQH%$'6,? JL$'&ZR13,8:8
H/&1"!Fl'??40%6^RX&C4l'0%&QRX64H%5:H/"%HM5:a&57;$"C45:H%RX0%&1"/&QH/"/&1?GHot6,0&3,R2$)3+?4?G8:5:R13+"%576@,6,02C'&10%&Caiz~n?40%&RX&C'&4RX&
RX64H/"/023,5:"%H JL$'&KC4l'023+"%576)6,o3H/"/&1?NC'&1?&4C4H`6"%$'&"F~z?&6,oH/"/&1?N3,HLEff&8:83,HL"%$'&K3+?4?G8:5:R13+"%576
S$'&#H/&1,&1023,8M3+?4?G8:5:R13+"%5764HB3+0%&i&5:';ZH/&1"Kl'?@H/"/&1?GHB?&10%"%3,5:45:';"/6=C45&10%&"B3+?4?G8:5:R13+"%5764HBR13,
i&L&X^&R1l'"/&C5:?G3+023,8:87&8@H%l'i^/&RX"M"/6K0%&H/6l'02RX&Q3+3,5:8:3+iG5:8:57"F~ IA"%$'&10%&Y3+0%&Kwzl4>&1025:R`0%&H/6l'02RX&H1@^3,4C
&3,R2$DH/"/&1?#l4H/&H3ZRX&10%"%3,5:3,>6l4"1@EQ$45:R2$C'&1?&4C4HY6."%$'&a3+?4?G8:5:R13+"%576@6,o`H/6>&aH%l'iGH/&1"P6,off0%&X
H/6l'02RX&HMC4l'025:';Q&X^&R1l'"%576=qt6487~nP6,oG"%$'&wP0%&H/6l'02RX&HM3+0%&L3,RX"%l43,8:87~K6,&102H%l'iGH%RX0257i&C | _L&H/6l'02RX&H
3+0%&=!0%&l4H%3+iG87&^@c}c:@'"%$'&K3,>6l4"Ql4H/&C)i&RX6>&HQ3+3,5:8:3+iG87&3+;3,5:Z64RX&"%$'&KH/"/&1?=$43,H` 45:H%$'&C
"LG02H/"L;8:3,4RX&,@G"%$45:HL3+?4?&3+02HL"/6i&3a?&10%ot&RX"QC'6>a3,5:Zot6,0 } - IffJL$'&?40%&H/&4RX&K6,o 0%&l4H%3+iG87&
0%&H/6l'02RX&#8:5:>a57"%3+"%5764H>a3+g,&H57"Z>6,0%&8:57g,&87~W"%$43+"Z"%$'&10%&3+0%&#$457;$'&10/6,02C'&10Z>]l'"%l43,8P&X'R18:l4H%5764H
i&1"FEff&1&T3,RX"%5764Hqhc}c:@M"%$'&10%&Z>a3~i&&'6l';$6,oY3N0%&H/6l'02RX&)"/6#R13+0%0%~#6l'"n"FEff6#3,RX"%5764Hl4H%5:';
"%$'&0%&H/6l'02RX&RX64R1l'0%0%&"%87~,@ffiGl'"a'6,"a"%$'0%&1&,@ff6,0a"%$'0%&1&iGl'"a'6,"ot6l'0@1e| JL$45:HH%l';,;,&H/"%Ha"%$'&
m'p $'&l'025:H/"%5:R1HL3+0%&]>6,0%&n8:57g,&87~"/6)5:>?40%6,&KEQ57"%$=5:4RX0%&3,H%5:';s@H%5:4RX&]m'p RX64H%5:C'&102HQ3+"P>6H/"Qs
H%l'i4;,63,8:Hj3,4CZ"%$'&10%&1ot6,0%&3+"Q>6H/"LsRX64R1l'0%0%&"Q3,RX"%5764H "L"%$'&H%3,>&B"%5:>&,@4C4l'&B"/6a"%$'&H%5:>?G87&
H/"/02l4RX"%l'0%&j6,o?40%&RX&C'&4RX&YRX64H/"/023,5:"%H1@"%$'&!;,0%6EQ5:';H/"%3+"/&Hff3,4Ca"%$'&L0%&H%l487"%5:';Pi4023,4R2$45:';Po}3,RX"/6,0
iG876EjFl'?5:)0%&8:3^&CH/&3+02R2$"%$43+"L6^R1R1l'0%0%&C5:)"%$'{
& Q%C'6>a3,5:@'3+0%&Kl448:57g,&87~
_L&H%l487"%H1@$'6Eff&1,&10@C45:H%3+;,0%&1&,IQJL9M-)3,4C } - H/687,&]"%$'&nH%3,>&nH/&1"P6,oM?40%6,iG87&>5:4H/"%3,4RX&Hq,
6l'"]6,oP+*|2@5:D"%5:>&Hn3,HnH%$'6EQ5: 57;l'0%&.w,wq}3| JL9M-.5:HKo}3,H/"/&10"%$43, } - 5:3.>a3/6,0257"F~#6,o
R13,H/&H1@G"%$'6l';$"%$'&]C45&10%&4RX&5:Hj0%&8:3+"%57,&87~ZH%>a3,8:8@'EQ$45:87&B5:Z"%$'&KR13,H/&HQEQ$'&10%& } - 5:HL"%$'&o}3,H/"/&H/"
6,o"%$'&B"FEff6'@4"%$'&C45&10%&4RX&B5:Hff;,0%&3+"/&10 JA6aH/&1&BEQ$~,@'"%$'&B?40%6,iG87&>5:4H/"%3,4RX&HjR13,Zi&BC457^5:C'&C5:"/6
"%$'&ot68:876EQ5:';]"%$'0%&1&"F~z?&H1I

~5}


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

IT!4H/"%3,4RX&H5:]EQ$45:R2$3,8:8^RX6"/&H/"/&Ca0%&H/6l'02RX&HM3+0%&j3+3,5:8:3+iG87&j5:nH%l>=R157&"zl43,"%57"F~ JL$45:H
wYpQbn
> &3,4Hj"%$'&10%&5:Hj'60%&H/6l'02RX&RX6> 5:RX"j3+"Q3,8:8@'3,4CZ"%$zl4H`"%$43+"Ym uUm JL$'&10%&B3+0%&w5n5:4H/"%3,4RX&Hj6,o

"%$45:H`"F~z?&K3,>6';a"%$'6H/&KH/687,&C@43,4CZ"%$'&1~)3+0%&K5:4C45:R13+"/&C)iz~[5: 57;l'0%&aw,w

u $ iGl'"m ; $ @Mc}c:@"%$'&10%&3+0%&0%&H/6l'02RX&aRX6> 5:RX"%H1@iGl'"

"%$'&H/&N5:,687,&N>6,0%&="%$43,k"%$'0%&1&NRX64R1l'0%0%&"3,RX"%5764H3,4Ck3+0%&N"%$'&10%&1ot6,0%&N'6,"C'&1"/&RX"/&CViz~km .
JL$'&10%&]3+0%&)w1*)5:4H/"%3,4RX&HQ6,oM"%$45:HL"F~z?&n3,>6';"%$'6H/&H/687,&C@3,4C="%$'&1~=3+0%&n5:4C45:R13+"/&Ciz~k 5:
57;l'0%&aw,w
. ;m $
wYpQb8AA I!4H/"%3,4RX&HB5:=EQ$45:R2$.m
JL$'&10%&n3+0%&Zw,wn5:4H/"%3,4RX&HY6,oM"%$45:HY"F~z?&n3,>6';)"%$'6H/&
H/687,&C@43,4CZ"%$'&1~Z3+0%&K5:4C45:R13+"/&C)iz~[; a5: 57;l'0%&aw,w
PB"F~z?&5:4H/"%3,4RX&H1@ } - R187&3+0287~Q?G3~^HA3,B6,&102$'&3,CKot6,0ARX6>?Gl'"%5:';L3,l44'&RX&H%H%3+025:87~YH/"/0%6';
$'&l'025:H/"%5:R+@^"%$'6l';$Z57"L5:H`0%&8:3+"%57,&87~)H%>a3,8:8 6,"/&K"%$43+"j"%$'&H/&K3,R1RX6l4"Qot6,0Q3n"%$45702C)6,o"%$'&5:4H/"%3,4RX&H
5:D"%$'&?40%6,iG87&>H/&1"Zq!5w =6l'"n6,oY+*|2@3,4C'&3+0287~#$43,87oj6,oL"%$'&)H/687,&C[5:4H/"%3,4RX&H P5:4H/"%3,4RX&H
6,o`"F~z?&HB%Y3,4C%%@ } - &X^?G3,4C4HBot&1Eff&10K'6^C'&H5:."%$'&n 43,8Lq}'6^0%&8:3^&C |PH/&3+02R2$#"%$43,#JL9MC'6z&HnC4l'025:';57"%H]H/&3+02R2$@3,Hn>]l4R2$[3,H- ot&1Eff&10n6[3,&1023+;,& JL$45:HKH%$'6EQHK"%$43+"n"%$'&)$'&l'025:H/"%5:R
5:>?40%6,&>&"`0%&H%l487"%5:';ot0%6>0%&1;,0%&H%H%576)5:H`3+"j87&3,H/"ff6,oH/6>&P+3,8:l'&,@z"%$'6l';$)5:0%6l';$487~ . 02C6,o
"%$'&K5:4H/"%3,4RX&Hj'6,"L&'6l';$Z"/6RX6>?&4H%3+"/&ot6,0L"%$'&KRX6H/"L6,o ?&10%ot6,02>a5:';]"%$'&0%&8:3^&CZH/&3+02R2$
_L&R13,8:8 "%$43+"ff"%$'& } - ?G8:3,4'&10ff5:a"%$'&BRX6>?&1"%57"%576@^3,4C5:"%$'&Y&X^?&1025:>&"%H?40%&H/&"/&C)$'&10%&
H/6.o}3+0@ME`3,Hn0%&H/"/025:RX"/&C"/6?&10%ot6,02>a5:';6487~0%&1;,0%&H%H%576TH/&3+02R2$ OD ?6H%H%57iG87&a&X^?G8:3,43+"%576ot6,0
"%$'&0%&8:3+"%57,&87~.Eff&3+g#0%&H%l487"%H"%$'&?G8:3,4'&10?40%6^C4l4RX&H6#"F~z?&%B3,4CD%%P5:4H/"%3,4RX&H]5:H"%$43+"K"%$45:H
0%&H/"/025:RX"%576#?40%&1,&"%HK0%&8:3^&CH/&3+02R2$ot0%6>i&5:';o}l48:87~=&X^?G87657"/&C Q6Eff&1,&10@M"%$45:H"%$'&16,0%~#C'6z&H
'6,"M$'68:C 57;l'0%&HLw,wqti|3,4CZw,wq}R|H%$'6EW0%&H%l487"%Hot6,0M"FEff6K3,87"/&10243+"%57,&LRX6^G;l'023+"%5764H 6,o } - 5:
w,wqti|Q3,V/l448:5:>a57"/&C4RX6^G;l'023+"%576@ EQ$45:R2$=R13+0%0257&HQ6l'"Ys)0%&1;,0%&H%H%576NH/&3+02R2$'&HYot6,0Psuzy1{1{1{
l4"%5:8&57"%$'&103P'6^0%&8:3^&CnH/68:l'"%576]5:Hot6l44C@+6,0 "%$'&`6,?4"%5:>a3,8zs)FH/68:l'"%576]RX6H/"5:Hot6l44CK"/6Bi&ff"%$'&
H%3,>&j3,H "%$'&Bqt
Gw|FH/68:l'"%576RX6H/"1@3,4Cn5:w,wq}R|3)%-RX6^G;l'023+"%576@+EQ$45:R2$n3,87E`3~^H ?&10%ot6,02>aH
`3,4CZ-+0%&1;,0%&H%H%576=H/&3+02R2$'&H O` HQR13,i&KH/&1&=5:)"%$'&G;l'0%&H1@'i6,"%$3,87"/&10243+"%57,&]RX6^G;l'023+"%5764H
5:4R1l'03D8:3+0%;,&106,&102$'&3,CVot6,0"%$'&=0%&8:3^&CVH/&3+02R2$'&H.q}l448:5:>a57"/&C } - &1,&k"%5:>&Ha6l'"6k"FEff6
5:4H/"%3,4RX&HEQ$45:87&C'65:';[0%&1;,0%&H%H%576VH/&3+02R2$ | Ox 8:H/6'@j"%$'&=;3,5:kot0%6>"%$'&.3,C4C457"%57643,8L$'&l'025:H/"%5:R
5:'ot6,02>a3+"%576T5:Hzl457"/&ZH%>a3,8:8IZRX6>?G3+025:';#3+;3,5:T"%$'&=zl4>Ki&106,oP'6^C'&Hn&X^?G3,4C'&CW5:["%$'&) 43,8
q}'6^0%&8:3^&C |P0%&1;,0%&H%H%576H/&3+02R2$"/6N"%$'&)zl4>Ki&10K6,oL'6^C'&HK&X^?G3,4C'&C#iz~#JL9M-'@"%$'&l448:5:>a57"/&C
3,4Ck-[RX6^G;l'023+"%5764H&X^?G3,4CT6k3,&1023+;,&.-4w+{ - 3,4CW-4uw ot&1Eff&10'6^C'&H1@ff0%&H/?&RX"%57,&87~qt"/6
i&KRX6>?G3+0%&C="/6"%$'&n- 3,&1023+;,&aH%3^5:';5:&X^?G3,4C'&C='6^C'&HL6,i4"%3,5:'&C=iz~ } - 0%&H/"/025:RX"/&C"/6
0%&1;,0%&H%H%576H/&3+02R2$6487~4|
'6,"%$'&10Q?6H%H%57iG87&P&X^?G8:3,43+"%5765:Hj"%$43+"Q"%$'&K"/023,4H/?6H%57"%576)"%3+iG87&,@4EQ$45:R2$3,8:H/6H/"/6,0%&HYl'?C43+"/&H
6,oB&H/"%5:>a3+"/&CVRX6H/"1@j"%$'6l';$kot6,0EQ$'687&=H/"%3+"/&H023+"%$'&10"%$43,kH%l'iGH/&1"%Ha6,oB;,63,8:H1@j"/6[H/6>&=&X^"/&"
RX6>?&4H%3+"/&HQot6,0Q"%$'&Eff&3+g,&10P$'&l'025:H/"%5:RBl4H/&CZiz~ZJL9M- O` ;3,5:@ $'6Eff&1,&10@ "%$'&&X^?G8:3,43+"%576Z"%l'024H
6l'"'6,""/6[$'68:CINEQ57"%$W"%$'&="/023,4H/?6H%57"%576T"%3+iG87&=C45:H%3+iG87&CW5:Wi6,"%$W?G8:3,4'&102H1@ff"%$'&NH%3^5:';H5:
zl4>Ki&106,oj'6^C'&HB&X^?G3,4C'&C#5:."%$'& 43,8MH/&3+02R2$#iz~ } - RX6>?G3+0%&C#"/6"%$'&zl4>Ki&106,oj'6^C'&H
&X^?G3,4C'&CDiz~JL9M-N6"F~z?&%K3,4CD%%5:4H/"%3,4RX&Hn5:H]3,RX"%l43,8:87~D87&H%HK"%$43,EQ$'&4CD"/023,4H/?6H%57"%576
"%3+iG87&HB3+0%&l4H/&C@3,&1023+;5:';Z6487~D5w qt"%$'6l';$ } - 5:N"%$45:HY&X^?&1025:>&"YH/687,&HP"FEff6?40%6,iG87&>aH
"%$43+"QJL9M-o}3,5:8:H`"/6H/687,&|2@ 3,4C)"%$'&KC45&10%&4RX&3,8:H/6i&RX6>&HQ>]l4R2$>6,0%&+3+0257&C
_L&R13,8:8"%$43+"Y3azl4>Ki&10j6,oH%5:>?G8:5 R13+"%5764HEff&10%&K5:"/0%6^C4l4RX&CZ5:Z"%$'&Bot6,02>]l48:3+"%576Z6,o "/&>?6,023,8
s)0%&1;,0%&H%H%576@B5:V6,02C'&10"/6W&43+iG87&RX6>?G87&1"/&DH/68:l'"%5764H)"/6W"%$'&0%&8:3^&CRX6H/"&zl43+"%576U"/6Wi&
RX6>?Gl'"/&CZ3,4C)H/"/6,0%&CZ5:"%$'&B$'&l'025:H/"%5:RL"%3+iG87& JL$zl4H1@^3]0%&>a3,5:45:';K?6H%H%57iG87&L&X^?G8:3,43+"%576ot6,0j"%$'&

Ix!4H/"%3,4RX&H5:NEQ$45:R2$m

wYpQb

.

~


fi

4

4

H%>a3,8:8+3,8:l'&]6,oM"%$'&n5:>?40%6,&>&"Y6,oM"%$'&n$'&l'025:H/"%5:R5:="%$'&K 43,8AH/&3+02R2$5:HQ"%$'&H/&nH%5:>?G8:5 R13+"%5764H1@
H%5:4RX&B"%$'&1~)876Eff&10Q"%$'&B&H/"%5:>a3+"/&HQH/"/6,0%&C=5:)"%$'&K$'&l'025:H/"%5:RQ"%3+iG87&
<ff64RX&10245:';`"%$'&M"%5:>&6,&102$'&3,Cot6,00%&8:3^&CH/&3+02R2$@+5:P?G3+0%"%5:R1l48:3+0ot6,0"%$'&M$457;$'&10s)0%&1;,0%&H%H%576
H/&3+02R2$'&H1@"%$'&ff&X^?G8:3,43+"%576]3+?4?&3+02H3+;3,5:n"/6Yi&`3Y$457;$'&10i4023,4R2$45:';Qo}3,RX"/6,0M5:"%$'&`0%&8:3^&C]H/&3+02R2$
H/?G3,RX&,@"%$'6l';$="%$'&H%57"%l43+"%576N5:HYH/6>&1EQ$43+"PC45&10%&"Y"%$43,.5:"%$'
& Q%ZC'6>a3,5: JL$'&]023+"%576
i&1"FEff&1&Z"%$'&H%571&B6,o H%l4R1RX&H%H/6,0QH/"%3+"/&HQ3,4CZ"%$'&570`?40%&C'&RX&H%H/6,02HL5:Hj876E@43,&1023+;5:';*^{ n5:Z'6,02>a3,8
0%&1;,0%&H%H%576#3,4C#*^{ ^waot6,0P_jF'6^C'&H5:#0%&1;,0%&H%H%576Vq}3,4CDH/"%3~^H0%6l';$487~N"%$'&H%3,>&3,8:H/65:"%$'&
-+ff3,4C0%&1;,0%&H%H%576H/?G3,RX&H|`H/6 K
nY F'6^C'&HQ3+0%&B0%&8:3+"%57,&87~)H%R13+02RX& `l'"j"%$'&K3,&1023+;,&Ki4023,4R2$45:';
o}3,RX"/6,0ot6,0`P_jF'6^C'&HM5:a0%&1;,0%&H%H%576a5:H(z{ -q}5:4RX0%&3,H%5:';P"/6-'{ 9 3,4Coz{ ( B5:n-+3,4C0%&1;,0%&H%H%576@
0%&H/?&RX"%57,&87~4|ffRX6>?G3+0%&C"/63,3,&1023+;,&]6,ojw+{ ,a5:Z'6,02>a3,80%&1;,0%&H%H%576 JL$'&B0%&3,H/65:)"%$'
& 3S
C'6>a3,5:k5:Ha"%$'&N0257;$"!FH%$457ot"R1l'"%H1I.0%&R13,8:8Q"%$43+""%$'&H/&=&8:5:>a5:43+"/&0%&C4l44C43,"ai4023,4R2$'&Hot0%6>"%$'&
H/&3+02R2$UH/?G3,RX&,@L"%$zl4H0%&C4l4R15:';#"%$'&Ni4023,4R2$45:';Do}3,RX"/6,0@QiGl'"R13,S'6,"i&.l4H/&CkEQ$'&k0%&1;,0%&H%H%5:';
P_jF'6^C'&Hj5:0%&8:3^&CZH/&3+02R2$=H%5:4RX&P"%$45:Hj>a57;$"jR13,l4H/&B"%$'&RX6>?Gl'"/&C$'&l'025:H/"%5:RL"/6ai&RX6>&5:43,C^
>a5:H%H%57iG87& JL$'&Qi4023,4R2$45:';Ko}3,RX"/6,0jot6,0j'6,02>a3,8G0%&1;,0%&H%H%576H/&3+02R2$)EQ57"%$'6l'"0257;$"!FH%$457ot"R1l'"%H`5:H`(z{ ,
JL$'&C45&10%&4RX&>a3~ZH/&1&>H%>a3,8:8@^iGl'"L57"L$43,HL3;,0%&3+"Q&X&RX"1IffJL9M-EQ57"%$'6l'"j0257;$"!FH%$457ot"jR1l'"%Hjo}3,5:8:H
"/6]H/687,&Q3,8:84iGl'""FEff6]"F~z?&L% 3,4Ca%%5:4H/"%3,4RX&HYq}5:a>a3,~aR13,H/&Hff'6,"M&1,&a 45:H%$45:';Y"%$'&jG02H/" P \
57"/&1023+"%576 |

"!>Oilkgn-opQqsrqun-kg|Itk]o#gqsrtkgq


!]H/&1,&1023,86,o4"%$'&`RX6>?&1"%57"%576KC'6>a3,5:4H1@ } - C'6z&H3,R2$457&1,&`i&1"/"/&100%&H%l487"%HA"%$43,]JL9M-'@,5:4C45:R13+"%5:';
"%$43+"M0%&8:3^&CaH/&3+02R2$R13,ai&L3,& =R157&">&1"%$'6^C6,oRX6>?Gl'"%5:';3>6,0%&L3,R1R1l'023+"/&Y$'&l'025:H/"%5:RffEQ$45:87&
H/"%3~^5:';k5:S"%$'&Dm'pot023,>&1Eff6,0%g !U"%$'&#C'6>a3,5:4H)EQ$'&10%&#57"Zo}3,5:8:Hk"%$'
& Q%k3,4C3S
C'6>a3,5:4H57"LC'6z&HLH/6i&R13,l4H/&B0%&8:3^&CH/&3+02R2$~^57&8:C4H`30%&8:3+"%57,&87~H%>a3,8:85:>?40%6,&>&"j6,&10Y"%$'&
$ $'&l'025:H/"%5:R+@'3+"Q3aC45:H/?40%6,?6,0%"%57643+"/&87~a8:3+0%;,&KRX6>?Gl'"%3+"%57643,8RX6H/"
!"%$'
& Q%aC'6>a3,5:@z"%$'&P"FEff6?40%6,iG87&>aHff3+0%&P"%57;$"%87~RX64'&RX"/&CIM"%$'&B$'&l'025:H/"%5:RQ5:>?40%6,&X
>&"Z5:H'&1;8:57;57iG87&=H%5:>?G87~i&R13,l4H/&.0%&8:3^&CVH/&3+02R2$U5:HH/6[&X^?&4H%57,&="%$43+")"%$'&N6487~W?40%6,iG87&>
5:4H/"%3,4RX&H6EQ$45:R2$57"M 45:H%$'&HEQ57"%$45:"%$'&Q"%5:>&Y8:5:>a57"3+0%&Q"%$'6H/&Y,&10%~H%5:>?G87&L5:4H/"%3,4RX&Qot6,0ffEQ$45:R2$
"%$'&)m $ $'&l'025:H/"%5:R5:HK3,870%&3,C'~R1876H/&"/6=?&10%ot&RX" !"%$'B
& 3SZC'6>a3,5:@A"%$'&0%&3,H/6Dot6,0K"%$'&?6z6,0
$'&l'025:H/"%5:RQ5:>?40%6,&>&"L5:H`H/"%5:8:8 H/6>&1EQ$43+"`6,o3>K~^H/"/&10%~ IM3zl4>Ki&10ff6,o $~z?6,"%$'&H/&HffEff&10%&B"/&H/"/&C@
3,4C0%&1o}l'"/&C OD 0%&>a3,5:45:';=?G8:3,l4H%57iG87&&X^?G8:3,43+"%576[5:H]"%$43+""%$'&ZH%5:>?G8:5 R13+"%5764H5:"/0%6^C4l4RX&C5:
ot6,02>]l48:3+"%576)6,o "/&>?6,023,8s)0%&1;,0%&H%H%5763+0%&?G3+0%"%5:R1l48:3+0287~C43,>a3+;5:';a5:)"%$45:HjC'6>a3,5:
!Zi6,"%$C'6>a3,5:4H1@'$'6Eff&1,&10@G"%$'&&X^?G8:3,43+"%576Zot6,0L"%$'&0%&8:3+"%57,&87~)8:3+0%;,&B6,&102$'&3,C=ot6,0L0%&8:3^&C
H/&3+02R2$]3+?4?&3+02H"/6Qi&"%$43+"57"AH%l^&102Hot0%6>r3L$457;$'&10i4023,4R2$45:';jo}3,RX"/6,0 "%$43,"%$'&'6,02>a3,80%&1;,0%&H%H%576
H/&3+02R2$@ EQ$45:R2$DR13,l4H/&HK&X^?G3,4H%5766,oYP_jF'6^C'&HK5:"%$'&0%&8:3^&CDH/&3+02R2$"/6=i&RX6>?Gl'"%3+"%57643,8:87~
>6,0%&n&X^?&4H%57,&K"%$43,'6^C'&]&X^?G3,4H%576N5:="%$'&'6,02>a3,8 H/&3+02R2$Wqt&1,&57o>a3,~6,o"%$'&n;,&'&1023+"/&C
H%l4R1RX&H%H/6,02H3+0%&Q'6,"H/&3+02R2$'&C | OM 57;l'0%&w(BH%l4>a>a3+02571&HMH/6>&QH/&3+02R2$H/?G3,RX&QR2$43+023,RX"/&1025:H/"%5:R1Hot6,03,8:8
"%$'&YRX6>?&1"%57"%576C'6>a3,5:4H !"%$'&YC'6>a3,5:4HMEQ$'&10%&Q0%&8:3^&CH/&3+02R2$)5:HM&X^?&4H%57,&,@z"%$45:H5:HMi&R13,l4H/&
H/"%3+"/&Ha"/&4C["/6;,0%6EEQ$'&[0%&1;,0%&H%H/&C@Qc}c:
@ 85F 8>5:Hn8:3+0%;,&3,4CT3,Ha3.0%&H%l487"]"%$'&10%&3+0%&>a3,~
F

'6
^
'
C

&

H
Q
E
7
5
%
"
]
$

>
,
3
~
]
%
H
4
l
1
R
X
R

&
%
H
/
H
,
6
2
0
L
H
q

Q









%

KH/?GnY3,RX &P$43,&B3K$457;$'&10ffi4023,4R2$45:';o}3,RX"/6,0`"%$43,54:a"|2@+%6,$'0 &Pi'6&,R1023,>al4H/3,&Q84P0%&1_j;,0%F&'6H%^H%C'576&HH5:/]"?G3,%$'RX&,&ff@'0%3,&4C8:3^&3+Cn0%&Q0%&1"%$';,0%&1&0%H%&1H%ot6,5760%&
RX6>?Gl'"%3+"%57643,8:87~.>6,0%&&X^?&4H%57,&a"/6=&X^?G3,4CVTq 3S| !#"%$'&C'6>a3,5:4HBEQ$'&10%&a0%&8:3^&CDH/&3+02R2$
5:HYH%l4R1RX&H%H/o}l48@G6N"%$'&]6,"%$'&10B$43,4CH
@ 85F* 8>G5:HL"F~z?G5:R13,8:87~R1876H/&]"/6w+@ c}c:@H%>a3,8:8AH/"%3+"/&HH/"%3~=H%>a3,8:8
EQ$'&n0%&1;,0%&H%H/&C@3,4Cn0%&1;,0%&H%H%576n6,oP_jF'6^C'&H5:]"%$'&j0%&8:3^&CH/?G3,RX&Q5:H RX6>?Gl'"%3+"%57643,8:87~]R2$'&3+?&10
"%$43,'6^C'&a&X^?G3,4H%576D5:"%$'&'6,02>a3,8M0%&1;,0%&H%H%576H/?G3,RX&,@3,H]5:4C45:R13+"/&C#iz~3N876Eff&10Zqt6,0]0%6l';$487~

~5


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

n
8>
85F* 8>

i4023,4R2$45:';no}3,RX"/6,0

8>
85F* 8>

6,02>a3,8_L&1;,0%&H%H%576

Q%
9z {
w+{ *
w+{
S~u
z{ +

i4023,4R2$45:';no}3,RX"/6,0

8>
85F* 8>
8>
85F* 8>

i4023,4R2$45:';no}3,RX"/6,0

8>
85F* 8>

i4023,4R2$45:';no}3,RX"/6,0

8>
85F* 8>

z { *
(z{ +
w+{ *

*^{

w+{ (^w
wz{:w

(z{ ,
w+{ ,
z{:w

z{ (

wX-'{
w+{:~w
(^w+{ (

(z{ ,
(z{:~w
z{ +*

+*^{

z{ *
w+{ *
(-'{

(z{ ,
w+{ -(
wz{:w

{ +

(z{ ,
w+{ *
z{ *'w

{ ,

(z{ ,(
*^{ ^w
(z{ -*

z{ ,

Qh3qTS==,S|

i4023,4R2$45:';no}3,RX"/6,0

i4023,4R2$45:';no}3,RX"/6,0

F_L&1;,0%&H%H%576
P_
K nY



9QhhQ
z{ 9
w+{ *+z{ 9
3S
z{:w1*
*^{
w+{ ,

57;l'0%&aw(zIP\z6>&nR2$43+023,RX"/&1025:H/"%5:R1HY6,oM"%$'&]'6,02>a3,8A0%&1;,0%&H%H%576N3,4C.0%&1;,0%&H%H%576NH/&3+02R2$.H/?G3,RX&HP5:
"%$'&ffC'6>a3,5:4HRX64H%5:C'&10%&CI"%$'&ff3,&1023+;,&`H/"%3+"/&`H%571&Yq 8> |2@"%$'&ff3,&1023+;,&ff023+"%576L6,o4H%l4R1RX&H%H/6,0
H/"%3+"/&KH%571&P"/6"%$'&H%571&P6,oA"%$'&B?40%&C'&RX&H%H/6,0LH/"%3+"/&q 85F* 8> |`3,4C"%$'&Bi4023,4R2$45:';]o}3,RX"/6,0
& S~uG5@ Qh34~@ ]3,4Cp9QhhQPC'6>a3,5:4H1@,"%$'&jzl4>Ki&102HH%$'6EQ
6,0"%$'c
3+0%&"%$'&3,&1023+;,&H6,&10 H/687,&CB?40%6,iG87&>5:4H/"%3,4RX&H 6,0A"%$'
& 3S`C'6>a3,5:@"%$'&3,&1023+;,&
5:HL6,&10BH/687,&C"F~z?&K%`3,4C%%`5:4H/"%3,4RX&HL6487~#q}H/&1&\z&RX"%576=- | OL 6,0Y"%$'[
& Q%
C'6>a3,5:@LC43+"%3D5:Hot0%6>3H%5:';87&Dqto}3,5:87&C |57"/&1023+"%576k6k3H%5:';87&?40%6,iG87&> 5:4H/"%3,4RX&
Tq ff^|

~5


fi

4

4

&zl43,8t|i4023,4R2$45:';No}3,RX"/6,0 YoQRX6l'02H/&,@"%$'&H/&Dq}3,&1023+;,&C |nzl4>Ki&102Hn3+0%&'6,"n?&10%ot&RX"n?40%&C45:RX"/6,02H
6,oj?&10%ot6,02>a3,4RX&,I5:#"%$'B
& Qh3=C'6>a3,5:@ot6,0]&X'3,>?G87&,@ "%$'&z 85F* 8>023+"%576.5:HKzl457"/&8:3+0%;,&iGl'"
6l'"/?&10%ot6,02>aHjJL9M-a3,~zE`3~Dq}3,HQH%$'6EQ5: 57;l'0%{
& 'q}3|/|
} -
h"ff5:H5:4H/"/02l4RX"%57,&L"/6n876z6,ga>6,0%&YR1876H/&87~a3+""%$'&PH/"%3+"/&H`5:a"%$'
& Q%nC'6>a3,5:@z3,4CaEQ$~"%$'&1~
;,0%6EEQ$'&[0%&1;,0%&H%H/&C OW 6,0a&X'3,>?G87&,@ff"%$'&W/H/"%3+"/&#6,oP&3,R2$k3,5702RX023+ot"a5:Hq}5:[&3,R2$WEff6,028:CTH/"%3+"/&|
C'&H%RX0257i&Ciz~"%$'0%&1&RX6>?6'&"%H1IM6'&B"/&8:8:H`EQ$'&10%&P"%$'&3,5702RX023+ot"L5:H`?6H%57"%576'&C)5:"%$'&'&1"FEff6,0%g)6,o
3,570%?6,0%"02l4E`3~^HM3,4Cn"%3'57E`3~^H1@6'&jEQ$45:R2$nC4570%&RX"%57657"5:H o}3,R15:';'@3,4Cn6'&jEQ$'&1"%$'&10M57"5:H ?G3+0%g,&C@
i&5:';)?Gl4H%$'&C.6,0K>6^5:';l44C'&1057"%HB6EQ?6Eff&101* Oa 8:>6H/"&1,&10%~.6,?&1023+"/6,0"%$43+"KR2$43,';,&HK6'&
6,o"%$'&H/&n$43,HY3,=&X&RX"P6,0P?40%&RX64C457"%576Z6="%$'&]6,"%$'&10Y"FEff6Z3,HQEff&8:8 OQ 6,0P&X'3,>?G87&,@3,~5:4H/"%3,4RX&
6,o"%$'f
& Qa6,?&1023+"/6,0@EQ$45:R2$R2$43,';,&HY"%$'&]?6H%57"%576Z6,oM3,N3,5702RX023+ot"1@ 0%&zl4570%&Hj"%$'&n3,5702RX023+ot"Q"/6i&
>6^5:';3,4Co}3,R15:';n3]?G3+0%"%5:R1l48:3+0`C4570%&RX"%576@^3,4CZ>a3~3,8:H/6R2$43,';,&P"%$'&Po}3,R15:'; JL$zl4H1@^0%&1;,0%&H%H%5:';
3PH/"%3+"/&LRX6"%3,5:45:';P6487~3P;,63,8'3+"/6> i&876';5:';Q"/6P6'&`6,oG"%$'&jRX6>?6'&"%H5:]>6H/"R13,H/&H0%&H%l487"%H
5:Z3H/"%3+"/&KRX6"%3,5:45:';n;,63,8A3+"/6>aHji&876';5:';]"/63,8:8"%$'0%&1& OM RX64R18:l4H%576)6'&>a3~)C'023E5:H`"%$43+"
H/?G8:57"/"%5:';Q8:3+0%;,&`H/"%3+"/&HLq K
nY F'6^C'&H|A5:"/6PH%>a3,8:87&10H/"%3+"/&HLqP_jF'6^C'&H|iG3,H/&CK6487~B6]"%$'&`zl4>Ki&10
6D3+"/6>aH]5:H'6,"]3,87E`3~^HK"%$'&a0257;$"KR2$'65:RX& OZ D3,87"/&10243+"%57,&aEff6l48:Ci&a"/6NC457^5:C'&3+"/6>aH]5:"%$'&
?G8:3,445:';L?40%6,iG87&>x5:"/6P;,0%6l'?GH6,off!0%&8:3+"/&C4B3+"/6>aHM3,4C]"%3+g,&L"%$'&jzl4>Ki&10 6,oG;,0%6l'?GH0%&1?40%&H/&"/&C
5:Z3aH/"%3+"/&K"/6ai&B57"%HLH%571&
'6,"%$'&10Y6,iGH/&10%+3+"%576"%$43+"YR13,=i&K>a3,C'&]5:HL"%$43+"Q"%$'&KiGl487g6,o"%5:>&]H/?&"Q5:Z0%&8:3^&C=H/&3+02R2$
5:HLH/?&"L5:)"%$'&B 43,8H/&3+02R2$N57"/&1023+"%576@'EQ$'&3aH/68:l'"%576)&X'5:H/"%HLEQ57"%$45:"%$'&KR1l'0%0%&"LRX6H/"Qi6l44C
JL$45:Ha57"/&1023+"%576k5:H'6,"6487~"%$'&N>6H/"&X^?&4H%57,&,@`iGl'"3,8:H/6D"%$'&N87&3,H/"l4H/&1o}l48@`H%5:4RX&0%&8:3+"%57,&87~
ot&1E $'&l'025:H/"%5:R5:>?40%6,&>&"%HP3+0%&nC45:H%RX6,&10%&C5:=57" JL$45:HQ3,8:H/60%&8:3+"/&HY"/6)"%$'&]i4023,4R2$45:';o}3,RX"/6,0@
H/?&R15 R13,8:87~."%$'&o}3,RX"n"%$43+" K
nY F'6^C'&Hn$43,&>a3,~D>6,0%&)H%l4R1RX&H%H/6,02H]"%$43,TP_jF'6^C'&H1Iot6,03,
F

'6
^
'
C
n
&
/
"
Z
6


n
&
/
H

6
7
8
,


&
.
C
,
3
:
8
8 57"%HPH%l4R1RX&H%H/6,02HB>]l4H/"Pi&nH/687,&C@H/6Z5:N"%$'&] 43,857"/&1023+"%576@3,8:8 H%l4R
KRX&nYH%H/ 6,02HQ6,oM&1,&10%~
F

'6
^
'
C
]
&
3+0%&]H/&3+02R2$'&C Q6Eff&1,&10@"%$'&K?Gl'0%?6H/&B6,o0%&8:3^&CNH/&3+02R2$.5:HQ'6,"Q"/6
nY
K
4C3H/68:l'"%5765:Z"%$'&Ks)0%&1;,0%&H%H%576=H/?G3,RX&,@GiGl'"L"/6a 4CH%571&sH/"%3+"/&HnqP_jF'6^C'&H|`EQ$'6H/&KRX6H/"
5:HPl44C'&10%&H/"%5:>a3+"/&C=iz~"%$'&$'&l'025:H/"%5:R+@ 3,4C.>6,0%&3,R1R1l'023+"/&aRX6H/"P&H/"%5:>a3+"/&HPot6,0B"%$'&H/& JL$'&10%&1ot6,0%&
57"`>a3~)'6,"j3,RX"%l43,8:87~ai&P'&RX&H%H%3+0%~"/6H/&3+02R2$3,8:8G"%$'&BH%l4R1RX&H%H/6,02H`"/6&1,&10%~ K
nY F'6^C'& OM Z3,87"/&10/
43+"%57,&YEff6l48:Cai&Y3)qtsy 4|0%&1;,0%&H%H%576)H/&3+02R2$)5:aEQ$45:R2$a6487~"%$'6
& >6H/"n!?40%6>a5:H%5:';H%l4R1RX&H%H/6,02H
"/6Z&1,&10%~ K
F

'6
^
'
C

&
+
3
%
0

&
X
R

6
4H
%
:
5
'
C
1
&
%
0

&

C

@
^
~
7
5

&
:
8
4
C
:
5
';

,
3
'6
,
%
"
'
$
1
&

0
4
C
:
5

>

&
4H
%576ot6,057"/&1023+"%57,&87~=0%&X 45:';"%$'&
nY
$'&l'025:H/"%5:Rnqt"%$45:HL5:HjC45:H%R1l4H%H/&C)o}l'0%"%$'&10L5:)"%$'&K'&X^"LH/&RX"%576 |
5:43,8:87~,@j57"H%$'6l48:CTi&?65:"/&Ck6l'""%$43+"&1,&V"%$'6l';$ } - 5:H1@j6V3,&1023+;,&,@Yi&1"/"/&10"%$43,
JL9M-'@ 0%&H%l487"%HB6,oji6,"%$D?G8:3,4'&102HB6#"%$'&)RX6>?&1"%57"%576DC'6>a3,5:4HH/"%5:8:8M3+?4?&3+0K023+"%$'&10]?6z6,0 JL$'&
6487~Z6,"%$'&10P6,?4"%5:>a3,8"/&>?6,023,8A?G8:3,4'&10Q"/6?G3+0%"%5:R157?G3+"/&]5:"%$'&nRX6>?&1"%57"%576=E`3,H<`9JqtP5:C43,8
Y&X'&10@^(+*,*+-z|2@'EQ$45:R2$@5:>6H/"ffC'6>a3,5:4H1@3,R2$457&1,&C>]l4R2$ai&1"/"/&100%&H%l487"%H"%$43,JL9M-qt"%$'&L0%&H%l487"%H
6"%$'&a&X^"/&4C'&w
C 9QhhQ)?40%6,iG87&>H/&1"1@ 3,4C#3,#5:'ot6,02>a3,8 RX6>?G3+025:H/6i&1"FEff&1&DC43+"%3Zot0%6>
"%$'&KRX6>?&1"%57"%576Z3,4CZ"%$'&0%&H%l487"%H`?40%&H/&"/&C=$'&10%&,@G5:4C45:R13+"/&P"%$43+"Q57"j6l'"/?&10%ot6,02>aH } - 3,HjEff&8:8t|
!["%$'&'6^"/&>?6,023,
8 Qh3.3,4C. C'6>a3,5:4H1@M6,"%$'&10a6,?4"%5:>a3,8`?G8:3,4'&102H]3,8:H/66l'"/?&10%ot6,02>
q}RX6>?&1"%57"%576a0%&H%l487"%HM3+0%&Q?40%&H/&"/&Caiz~aQ6+>a3,43,4C + C'&87g+3,>?h@ | <`9JW3,8:H/6]l4H/&HM"%$'&
} -
"/&>?6,023,8m $ $'&l'025:H/"%5:R+@'iGl'"YH/&3+02R2$'&HP3)<L\^9kot6,02>]l48:3+"%576Z6,o?G3+0%"%5:3,86,02C'&10Y?G8:3,445:';'I$'&l'025:H/"%5:R
&H/"%5:>a3+"/&HY3,4CZ"%$'&KR1l'0%0%&"QRX6H/"Li6l44CZ3+0%&Kot6,02>]l48:3+"/&C3,HQRX64H/"/023,5:"%H1@G3,4C=RX6H/"Li6l44C)^5768:3
"%5764Hj3+0%&B5:'ot&10%0%&Ciz~RX64H/"/023,5:"j?40%6,?G3+;3+"%576@'3,65:C45:';n"%$'&B'&1&C)"/6&X^?G8:5:R157"%87~&1+3,8:l43+"/&H/"%3+"/&H
3,4C&43+iG8:5:';a&3+028:57&10YC'&1"/&RX"%576 JL$'&?G3+0%"%5:3,86,02C'&10Yi4023,4R2$45:';a3,4C="%$'&]l4H/&K6,o & =R157&"Q?40%6,?G3
!!Yt!j,!1%h2z, jA,ht`+%h2h j, Q LAh
L21ff/2F+2 2 `}F%9 ks X % 21!4!1 L21`}F%h2h ` V v;lvv~5f
;lvAW 2A/2F v~5f 2+Q`n /2 !QM2 hF% A2 ,2M ,!! 2

5}


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

;3+"%5763+0%&Qi6,"%$5:>?6,0%"%3,"Mot6,0"%$'&Q& =R157&4RX~6,o"%$'&B<`9JW?G8:3,4'&10@,iGl'"57"Eff6l48:C?40%6,iG3+iG87~n3,8:H/6
i&'&XG"Bot0%6>3Z>6,0%&3,R1R1l'023+"/&$'&l'025:H/"%5:R JL$zl4H1@"%$'&H/&3+02R2$DH%R2$'&>&a6,oQ<`9J 3,4C"%$'&5:C'&3)6,o
5:>?40%6^5:';$'&l'025:H/"%5:R1Hff"%$'0%6l';$H/&3+02R2$=3+0%&RX6>?G87&>&"%3+0%~,@ 3,4CZ>a3~)i&B?6H%H%57iG87&Y"/6RX6>KiG5:'&

F,FG ( FF
JL$'&a5:C'&3)6,o`l4H%5:';ZH/&3+02R2$#"/6=C'&10257,&6,0K5:>?40%6,&a$'&l'025:H/"%5:R1HB5:HB'6,"K'&1E JL$45:HPH/&RX"%576#0%&1^57&1EQH
3=H/&87&RX"%5766,oj0%&8:3+"/&CD>&1"%$'6^C4H S57"%$#"%$'&a&X'RX&1?4"%576#6,oj"%$'&C45:H%R1l4H%H%576N6,oj?G3+"/"/&102DC43+"%3+iG3,H/&
$'&l'025:H/"%5:R1H1@A"%$'&aot6^R1l4H]5:"%$45:HKH/&RX"%5765:HB6$'6E "%$'&H/&.qt6,0nH%5:>a5:8:3+0X|Y>&1"%$'6^C4HKR13,Di&3,C43+?4"/&C
3,4CZ&X^?G87657"/&C)"/65:>?40%6,&0%&8:3^&CZH/&3+02R2$

O#FnfiDnq>|mgfirq rq

1& 0257^5:';$'&l'025:H/"%5:R1HBiz~H/687^5:';3,3+iGH/"/023,RX"/&C@ 6,0]0%&8:3^&C@A,&102H%576#6,oj"%$'&H/&3+02R2$D?40%6,iG87&>5:H
'6,"Q3'&1E5:C'&3^@G3,4C='&57"%$'&10Q5:Hj"%$'&]5:C'&36,ol4H%5:';aH/&3+02R2$="/6H/687,&C"%$'&K3+iGH/"/023,RX"Q?40%6,iG87&>q}H/&1&
c cP3,H%R2$4457;'@w +z9&3+028@w9 -'9M0257&C457"%5:H1@w,,|
0%&RX&"1@'3,4CaH%l4R1RX&H%H/o}l48@+3+025:3,"M6a"%$45:H"%$'&>&Q5:H ',eeFXdN,e3 y%P' ^dfXe}XLq<`l487i&102H/6
\^R2$43+&X&10@Lw,,zjQ&10243,C' 6 87;,~^5`Q687"/&,@ff(+*,*,*| JL$'&H/&3+0%&ZC'&X '&C#iz~D3+iGH/"/023,RX"%5:';.3E`3~
?G3+0%"6,oG"%$'&`?40%6,iG87&>r3,4CnH/687^5:';Q6487~"%$'&`?G3+0%""%$43+" 0%&>a3,5:4H`qt"%$'&!?G3+"/"/&1024| JL$'&`3+iGH/"/023,RX"%576
5:>?G8:5:R157"%87~ZC'&X '&HB3)?40%6/&RX"%576.ot0%6>v"%$'&?40%6,iG87&>vH/&3+02R2$#H/?G3,RX&a5:"/63ZH%>a3,8:87&10BH/&3+02R2$#H/?G3,RX&,I
6,?4"%5:>a3,8H/68:l'"%576DRX6H/"n5:#"%$45:HK3+iGH/"/023,RX"nH/?G3,RX&)5:H]3=876Eff&10ni6l44C6D6,?4"%5:>a3,8H/68:l'"%576DRX6H/"n5:
"%$'&B6,0257;5:43,8 H/&3+02R2$H/?G3,RX&,@43,4C)iz~>a3+g^5:';n"%$45:H`H/?G3,RX&H%>a3,8:8 &'6l';$)"%$'&B6,?4"%5:>a3,8RX6H/"L6,oA&1,&10%~
H/"%3+"/&P5:a"%$'&Y3+iGH/"/023,RX"ff?40%6,iG87&>H/?G3,RX&PR13,ot6l44Caiz~iG8:5:4CH/&3+02R2$@^3,4CH/"/6,0%&C5:3"%3+iG87&YH/6K"%$43+"
H/"%3+"/&K&1+3,8:l43+"%576R13,i&C'6'&Biz~)3aH%5:>?G87&P"%3+iG87&876z6,g^l'?Dq}$'&4RX&"%$'&K43,>&?G3+"/"/&102C43+"%3+iG3,H/&|
Q&l'025:H/"%5:RB&H/"%5:>a3+"/&HQot0%6>>]l487"%57?G87&3+iGH/"/023,RX"%5764HYR13,=i&KRX6>KiG5:'&Ciz~)"%3+g^5:';"%$'&570Y>a3'5:>]l4>
q}5:DH/6>&R13,H/&HK"%$'&570]H%l4>ZH/&1& &8:'&10O
@ B6,0%oh@ Y3,43,@(+*,*+-z| 9 3+"/"/&102C43+"%3+iG3,H/&)$'&l'025:H/"%5:R1H
$43,&i&1&UH%l4R1RX&H%H/o}l48:87~W3+?4?G8:57&Ck"/6W3Tzl4>Ki&106,onH/"%3,4C43+02CSH/&3+02R2$U?40%6,iG87&>aHq<`l487i&102H/6V
\^R2$43+&X&10@Mw,,z &8:'&10P&1" 3,8 @(+*,*+-z|2@A3,4C.3,8:H/6"/6ZH/&zl'&"%5:3,8\^JL_L!9`\)?G8:3,445:';Nq + C'&87g+3,>?@
(+*,*'w| JL$'&a5:C'&3)6,o`?G3+"/"/&102#C43+"%3+iG3,H/&H]>a3~3+?4?&3+0,&10%~H%5:>a5:8:3+0P"/60%&8:3^&CDH/&3+02R2$Vq}5:4C'&1&C@
"/6"%$'&C'&X 457"%5766,o"%$'&Km'pr$'&l'025:H/"%5:R1H`5:;,&'&1023,8t|ff5:"%$43+"L"%$'&P?40%6,iG87&>5:H/H/?G8:57"%n5:"/6H%5:>?G87&10
?40%6,iG87&>aHAEQ$45:R2$n3+0%&LH/687,&Cn5:4C'&1?&4C'&"%87~,@+3,4Cn"%$'&jH/68:l'"%576nRX6H/"%Hot6,0 "%$'&H/&Ll4H/&Cn3,H3B$'&l'025:H/"%5:R
&H/"%5:>a3+"/&Not6,0"%$'&NRX6>?G87&1"/&=?40%6,iG87&> JL$'&10%&N5:H1@j$'6Eff&1,&10@Y3DRX02l4R15:3,8QC45&10%&4RX&,@j5:W"%$43+""%$'&
m'p0%&8:3'3+"%576W?&10%ot6,02>aH"%$45:HH/?G8:57")d%%v ^d2 ,X +@L"/6D&1,&10%~WH/"%3+"/&N6,oH%571&=>6,0%&"%$43,ks@`EQ$45:87&
"%$'&a3+iGH/"/023,RX"%576."%$43+"C'&X '&HP"%$'&n?G3+"/"/&102#5:.3)?G3+"/"/&102#C43+"%3+iG3,H/&a$'&l'025:H/"%5:RK5:HY4^&CWq}3,8:H/6'@"%$'&
3+iGH/"/023,RX"%576n"%$43+"C'&X '&H3P9 5:H 5:K;,&'&1023,8'3j d/1 %e}+4@ c}c:@,3P>a3,~"/6+6'&L>a3+?4?G5:';'@+EQ$45:87&
"%$'&m'p 0%&8:3'3+"%576D5:H3,VXo
y2%+tz,@`c}c:@A&3,R2$H/"%3+"/&)5:."%$'&a6,0257;5:43,8 0%&1;,0%&H%H%576DH/&3+02R2$DH/?G3,RX&
RX6,0%0%&H/?64C4Hj"/63H%5:';87&H/"%3+"/&n5:Z"%$'&K0%&8:3^&C=H/&3+02R2$NH/?G3,RX&| O<+ ,&N57o&H/"%5:>a3+"/&HQot0%6>>]l487"%57?G87&
?G3+"/"/&102.C43+"%3+iG3,H/&Haq}3+iGH/"/023,RX"%5764H|Q3+0%&RX6>KiG5:'&C@ "%$'&nRX6>KiG5:43+"%576N6,oM+3,8:l'&Hqtiz~=>a3'5:>a575:';
6,0H%l4>a>a5:';z|B6^R1R1l'02H6487~3+"a"%$'&Z0%6z6,"1@Yc}c:@"%$'&H/"%3+"/&=i&5:';N&1+3,8:l43+"/&C@`3,4CT'6,"3,876';"%$'&
H/68:l'"%576W?G3+"%$4Hot0%6> "%$45:HH/"%3+"/&5:W"%$'&NC45&10%&"3+iGH/"/023,RX"%5764H JL$45:HC45&10%&4RX&=>&3,4H"%$43+"
5:[H/6>&?40%6,iG87&>aH1@"%$'&=m'p$'&l'025:H/"%5:RR13,Ti&Z>6,0%&3,R1R1l'023+"/&"%$43,W3,~?G3+"/"/&102WC43+"%3+iG3,H/&6,o
0%&3,H/643+iG87&H%571&aq}Y3,H%8:l4>Z@4ff6'&1"YY&X'&10@G(+*,*z@G?40%6^5:C'&B3,Z&X'3,>?G87&| PZ"%$'&B6,"%$'&10Q$43,4C@
"%$'&?6H%H%57iG5:8:57"F~6,o`3,C4>a5:H%H%57iG87~ZH%l4>a>a5:';+3,8:l'&HYot0%6>>]l487"%57?G87&K?G3+"/"/&102#C43+"%3+iG3,H/&H>&3,4HP"%$43+"
5:kH/6>&N?40%6,iG87&>aH1@`3RX68:87&RX"%576k6,o3,C4C457"%57,&?G3+"/"/&102SC43+"%3+iG3,H/&H)R13,kot6,02>3D$'&l'025:H/"%5:R>6,0%&
3,R1R1l'023+"/&a"%$43,Dm'p@ot6,0K3,~N0%&3,H/643+iG87&n+3,8:l'&n6,offs q}3+;3,5:@AY3,H%8:l4>Z@ff6'&1"KY&X'&10@(+*,*z@
;57,&K3,Z&X'3,>?G87&|

5}<


fi

4

4

!H/6>&E`3~^H1@0%&8:3^&C[H/&3+02R2$[$43,H]>6,0%&)5:DRX6>a>6EQ57"%$#"%$'&)5:C'&36,o`',eeFXdW%+d/X'@
C'&1,&876,?&C5:D"%$'&RX6"/&X^"6,oL"%$'&\z6,g,6,iG3,?Gl'187&,@ EQ$45:R2$3+0%&)>6,0%&)C'~^43,>a5:R=ql4';$43,44H]
\^R2$43+&X&10@4(+*,*'w| A57g,&P5:3K?G3+"/"/&102)C43+"%3+iG3,H/&B$'&l'025:H/"%5:R+@z3K?G3+"/"/&102)H/&3+02R2$Z3+iGH/"/023,RX"%H`3E`3~?G3+0%"
6,o'"%$'&?40%6,iG87&>3,4CKH/687,&HA"%$'&M0%&>a3,5:45:';Kq}H%>a3,8:8t| ?40%6,iG87&>U"/6Q6,i4"%3,5:K3,]5:>?40%6,&CK876Eff&10i6l44C@
iGl'"B"%$'&a?G3+"/"/&102kqhc}c:@A"%$'&a?G3+0%"6,off"%$'&a?40%6,iG87&>v"%$43+"K5:HBg,&1?4"iz~."%$'&3+iGH/"/023,RX"%576 |P5:HBH/&87&RX"/&C
EQ$'&'&1,&103?G3+0%"%5:R1l48:3+0aH/"%3+"/&=&X^?G3,4H%576rqX/>6,&|a5:HaRX64H%5:C'&10%&C 9 3+"/"/&1024H"%$43+"$43,&=i&1&
H/&3+02R2$'&C=3+0%&KH/"/6,0%&C@G3,876';EQ57"%$)"%$'&570Ql'?C43+"/&CZRX6H/"1@G3,4CZ"%3+g,&N5:"/6a3,R1RX6l4"Y5:)"%$'&K$'&l'025:H/"%5:R
&1+3,8:l43+"%576=qtiz~a>a3'5:>a573+"%576 |A6,o3,~'&1EVH/"%3+"/&Q"%$43+"Y2+Ge+t'M"%$'&QH%3,>&L?G3+"/"/&102@&4RX6l4"/&10%&C
iz~n"%$'&YH/&3+02R2$ JL$'&L?G3+"/"/&1024H&X^?G876,0%&Ciz~n?G3+"/"/&102H/&3+02R2$'&Hff3+0%&Qot6l44C"%$'0%6l';$3,5:4RX0%&>&"%3,8
?40%6^RX&H%H1IJL$'&LG02H/"M?G3+"/"/&102)RX64H%5:H/"%HM6,o6487~n"%$'&Y?G3+0%"6,o"%$'&Y?40%6,iG87&>qX/H/"/6'&|M"%$43+"`5:HC4570%&RX"%87~
3&RX"/&CViz~["%$'&N>6,&.l44C'&10RX64H%5:C'&1023+"%576 JL$'&='&X^"?G3+"/"/&102W&X^"/&4C4H"%$'&?40%&1^576l4HEQ57"%$
H/"/6'&HK"%$43+"K5:"%$'&R1l'0%0%&"KH/"%3+"/&RX6> 5:RX"EQ57"%$"%$'&H/68:l'"%576.ot6l44C#5:"%$'&?40%&RX&C45:';Z?G3+"/"/&102
H/&3+02R2$@G3,4CZ"%$45:Hj5:Hj0%&1?&3+"/&Cl4"%5:8'6a>6,0%&KRX6> 5:RX"%HL3+0%&Bot6l44C
HY>&"%576'&CN5:"%$'&K?40%&1^576l4HQH/&RX"%576@ "%$'&]$457;$=RX6>?Gl'"%3+"%57643,8RX6H/"Y6,o"%$'&Ks)0%&1;,0%&H%H%576
H/&3+02R2$)5:HM6,ot"/&)C4l'&L"/6]"%$'&Yo}3,RX"ff"%$43+" K
nY F'6^C'&Hff$43,&B>a3,~aH%l4R1RX&H%H/6,02H1@^3,4C>6H/"6,o"%$'&Q"%5:>&
57"5:Hn'6,"3,RX"%l43,8:87~D'&RX&H%H%3+0%~"/6H/&3+02R2$T"%$'&>3,8:8ffot6,0&1,&10%~ K
nY F'6^C'&,IZ3,T3,87"/&10243+"%57,&Z5:H]"/6
H/&3+02R2$#6487~="%$'&a>6H/"=!?40%6>a5:H%5:';H%l4R1RX&H%H/6,02H1@AEQ$'&10%&a3)?40%6>a5:H%5:';)H%l4R1RX&H%H/6,0K5:H3,P_jF'6^C'&
EQ$'6H/&aRX6H/"5:HB8:57g,&87~="/6Zi&l44C'&10%&H/"%5:>a3+"/&C.iz~N"%$'&aR1l'0%0%&"B$'&l'025:H/"%5:R+@3,4C"%$'&10%&1ot6,0%&a8:57g,&87~"/6
5:4RX0%&3,H/&EQ$'&)"%$'&K'6^C'&5:Hj&X^?G3,4C'&C A5:>a57"%5:';]"%$'&Kzl4>Ki&10L6,oH%l4R1RX&H%H/6,02HLH/&3+02R2$'&C=ot6,0L&1,&10%~
l4457ot6,02>a87~#"/63+">6H/I
" [0%&H%l487"%Ha5:W3,qtsy 4|0%&1;,0%&H%H%576VH/?G3,RX&,@j3,4Ck3#H/&10257&Ha6,o
K qtsnYy 4F|'60%^&1C';,&N
0%&H%H%576.H/&3+02R2$'&HYEQ57"%$=5:4RX0%&3,H%5:';as3,4CR13,Ni&K6,0%;3,4571&CN5:=C45&10%&"QE`3~^H1Ijot6,0
&X'3,>?G87&,@ff"%$'&Z?G8:3,4'&10RX6l48:C[?&10%ot6,02> qtsyw|0%&1;,0%&H%H%576kH/&3+02R2$'&Haot6,0as uzy1{1{1{ffl4"%5:8jH/6>&
H%l457"%3+iG87&BH/"/6,?4?G5:';RX64C457"%5765:Hj>&1"1@G"%$'&Dqtsy2(|0%&1;,0%&H%H%576=H/&3+02R2$'&HQot6,0LsvuUzy1{1{1{f@ 1e JL$45:H
5:H`,&10%~)H%5:>a5:8:3+0ff"/6a57"/&1023+"%57,&Bi40%63,C'&45:';nH/&3+02R2$qP5:4H/i&10%;xY3+0%,&1~,@w,,(| O` Z3,87"/&10243+"%57,&5:H
"/6=8:5:>a57"Y"%$'&&X^?G3,4H%576.6,o K
nY F'6^C'&HK'6^Fl4457ot6,02>a87~,@ ot6,0&X'3,>?G87&,@"/6=H/&3+02R2$D3,8:8H%l4R1RX&H%H/6,02H
H%3+"%5:H/ot~^5:';BH/6>&LRX0257"/&102576not6,0i&5:';P?40%6>a5:H%5:';'@,H%5:>a5:8:3+0"/6"%$'&L57"/&1023+"%57,&jEQ5:C'&45:';YH/"/023+"/&1;,~al4H/&C
5:)"%$'&KRX6"/&X^"Q6,o;3,>&X"/0%&1&nH/&3+02R2$q<`3+1&43,&,@(+*,*'w|
JL$'&KRX6> 5:RX"!FC4570%&RX"/&C=H/"/023+"/&1;,~l4H/&C"/6a 4C)?G3+"/"/&1024HY5:)?G3+"/"/&102NH/&3+02R2$'&HYC'&>64H/"/023+"/&HY3
E`3~a"/6nC45:H/"%5:';l45:H%$]?40%6>a5:H%5:';H%l4R1RX&H%H/6,02H <ff64H%5:C'&10H/&zl'&"%5:3,84?G8:3,445:';BEQ$'&10%&Y3, K
nY F'6^C'&
5:HYH%5:>?G87~Z3)H/&1"Y6,off>6,0%&]"%$43,Ns q}H%l'i4;,63,8t|L3+"/6>aH1@3,4CN"%$'&H%l4R1RX&H%H/6,02HP3+0%&n3,8:8 H%571&]sH%l'iGH/&1"%H
6,o"%$45:HQH/&1"1IjH%l'iGH/&1"%HQ>6,0%&]8:57g,&87~"/6)$43,&n3$457;$'&10QRX6H/"Y"%$43,="%$'&K&H/"%5:>a3+"/&K;57,&=iz~=m''
p PR13,
i&Q5:C'&"%5G&Caiz~aH/687^5:';B&3,R2$ZH%571&Q
G#wLH%l'iGH/&1"ff3,4Ca&X'3,>a5:45:';"%$'&YH/68:l'"%576aot6,0ffRX6> 5:RX"%HEQ57"%$
0%&>a3,5:45:';]3+"/6>aHj5:"%$'&PH/"%3+"/&q}3RX6> 5:RX"ffi&5:';]ot6,0`&X'3,>?G87&P3,Z3,RX"%576)"%$43+"jC'&87&1"/&H`"%$'&B3+"/6>Z@
6,0Q3,RX"%5764Hj5:4RX6>?G3+"%57iG87&YEQ57"%$)3,3,RX"%576Z'&1&C'&CZ"/6&H/"%3+iG8:5:H%$"%$'&3+"/6>| !)o}3,RX"1@4"%$45:H`>&1"%$'6^C
$43,Hffi&1&l4H/&C)5:"%$'&PRX6"/&X^"j6,o3]C45&10%&"`>&1"%$'6^Cot6,0j5:>?40%6^5:';"%$'&Bm'p$'&l'025:H/"%5:R1H"%$'0%6l';$
8:5:>a57"/&CH/&3+02R2$Zq}Y3,H%8:l4>Z@,(+*,*+-3| \z6>&`R13+0%&ff>]l4H/"Ai&"%3+g,&]"/6Y&4H%l'0%&M"%$43+""%$'&ffH/&3+02R2$'&H '&1&C'&C
"/6] 4C"%$'&P?40%6>a5:H%5:';KH/&1"%Hj3+0%&B'6,"`>6,0%&P&X^?&4H%57,&Y"%$43,)H/&3+02R2$45:';]&1,&10%~H/&1"1@'iGl'"`57o"%$'&m '
p
+3,8:l'&jE`3,HM3,8:H/6RX6>?Gl'"/&Ciz~]0%&8:3^&CaH/&3+02R2$@"%$'&LH%571&jD
G=wjH%l'iGH/&1"%HLqt6,0ff3+"M87&3,H/"H/6>&j6,o"%$'&>|
$43,&]3,870%&3,C'~i&1&ZH/&3+02R2$'&C@G3,4CRX6> 5:RX"%Hjot6l44CZC4l'025:';]?40%&1^576l4HjH/&3+02R2$'&HYR13,Zi&H%3,&C

iKUIQq9[#nUn wnqw`q n-kg|Ugilo-t-firquv!t-liKsUln
# nfi
F
JL$'&a\4<LY
J KYn P_"/0%&1&H/&3+02R2$3,87;,6,0257"%$4>Z@C'&1,&876,?&C.>a3,5:487~Zot6,0BH/&3+02R2$45:';);3,>&n"/0%&1&H1@
"/0257&HA"/6Y0%&C4l4RX&"%$'&`zl4>Ki&10A6,o4'6^C'&HA&1+3,8:l43+"/&C]iz~PG02H/"A"/&H/"%5:';Qot6,0&3,R2$'6^C'&ff57o'57"R13,n3&RX" "%$'&

5}



fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

+3,8:l'&B6,oM57"%Hj?G3+0%&"Li&1ot6,0%&&1+3,8:l43+"%5:';"%$'&]'6^C'&B&X'3,RX"%87~Dq}9&3+028@Aw9-z| JL$'&"/&H/"Q5:Hj?&10%ot6,02>&C
iz~3N?40%6^RX&C4l'0%&,@MR13,8:87&C[H%5:>?G87~U/JA&H/"%^@EQ$45:R2$"%3+g,&Ha3,H3+0%;l4>&"%H3.'6^C'&)3,4C[3."%$'0%&H%$'68:C
+3,8:l'&,@3,4CNC'&1"/&102>a5:'&HQ57o"%$'&]+3,8:l'&K6,oM"%$'&]'6^C'&]5:HQ;,0%&3+"/&10qt6,0P&zl43,8t|`"%$43,="%$'&]"%$'0%&H%$'68:C@Giz~
0%&R1l'02H%57,&87~]"/&H/"%5:';K"%$'&Q'6^C'&, HH%l4R1RX&H%H/6,02HYqt"/6n3KH/?&R15G&CC'&1?4"%$ |2@iGl'"M6487~nl4"%5:8'"%$'&Y5:'&zl43,8:57"F~
5:H?40%6,& JL$'&j?40%6^RX&C4l'0%&jR13,a&3,H%5:87~]i&L>6^C45G&C]"/60%&1"%l'02a3P;,0%&3+"/&10ff+3,8:l'&jot6,0M"%$'&Q'6^C'&jEQ$'&
H%l4R2$3+3,8:l'&5:Hjot6l44C@'"%$'6l';$Z"%$45:HL>a3~ZH/"%5:8:8 i&K87&H%H`"%$43,"%$'&K'6^C'&HL3,RX"%l43,8+3,8:l'&,@43,4C57"L$43,H
i&1&)H%$'6EQ"%$43+"`"%$'&BJA&H/"`?40%6^RX&C4l'0%&,@z&4$43,4RX&CEQ57"%$)>&>6,0%~5:"%$'&Pot6,02> 6,o3]"/023,4H/?6H%57"%576
"%3+iG87&,@4R13,)i&Bl4H/&C)57"/&1023+"%57,&87~"/6;57,&3,)& =R157&"L3,87;,6,0257"%$4> "%$43+"LC'&1"/&102>a5:'&Hff"%$'&B&X'3,RX"j+3,8:l'&
6,o3'6^C'&q}98:3,3+"1@\^R2$43+&X&10@ 95 %8:H1@' KO @Aw,,|
JL$'& P \aH%l'i40%6l'"%5:'&j6,o L 5:HM,&10%~aH%5:>a5:8:3+0M"/6n3KC'&1?4"%$^Fl4i6l44C'&Cn,&102H%5766,o"%$'&YJA&H/"
?40%6^RX&C4l'0%&,@3,4C"%$zl4H"%$'&j L 3,87;,6,0257"%$4>5:HMH%5:>a5:8:3+0 "/6KH%l4R2$a3,a57"/&1023+"%57,&Q3+?4?G8:5:R13+"%576n6,oJA&H/"
JL$'&B>a3,5:)C45&10%&4RX&B8:57&H`5:"%$43+"j L Q P \Z3+?4?G8:57&Hff57"/&1023+"%57,&C'&1&1?&45:';Zqtiz~R13,8:8:5:';] L |
"/6"%$'&ZH%l4R1RX&H%H/6,02H6,o K
nY F'6^C'&H1@EQ$'&10%&3,HJA&H/"aR13,8:8:Hn57"%H/&87oL0%&R1l'02H%57,&87~#EQ57"%$"%$'&ZH%3,>&RX6H/"
i6l44C HL30%&H%l487"1@^ L 4C4Hff"%$'&B6,?4"%5:>a3,8RX6H/"L6,o 3,~)H/687,&CNP_jF'6^C'&,@^EQ$45:R2$ZJA&H/"LC'6z&H
'6,"]qt"%$'6l';$Z"%$'&$457;$'&10LRX6H/"Q0%&1"%l'02'&C)iz~q}>6^C45G&C |JA&H/"LEQ$'&)"%$'&RX6H/"Q6,o 3a'6^C'&B5:Hj?40%6,&
"/6a&X'RX&1&C="%$'&B"%$'0%&H%$'68:CZ5:HjH/"%5:8:83876Eff&10Qi6l44C6Z"%$'&K'6^C'&Hj6,?4"%5:>a3,8RX6H/"|
_L&RX&"%87~,@Qff6'&1")3,4CSY&X'&10q(+*,*+-z|a?40%&H/&"/&CV3#;,&'&1023,8YC'&1?4"%$^G02H/"H/&3+02R2$V3,87;,6,0257"%$4>
ot6,0 K
nY
P_j;,023+?G$4H1@ R13,8:87&C# P \ @EQ$45:R2$#5:H3,8:H/6,&10%~H%5:>a5:8:3+0B"/6 L A57g,&a L @57"
4C4Hj"%$'&6,?4"%5:>a3,8ARX6H/"Q6,o&1,&10%~H/687,&C='6^C'&,@G3,4C=5:>?40%6,&C=876Eff&10Qi6l44C4Hj6='6^C'&HL"%$43+"Y3+0%&
&X^?G876,0%&C.iGl'"B'6,"H/687,&C P \ @$'6Eff&1,&10@ H/"/6,?GHH/&3+02R2$45:';)"%$'&aH%l4R1RX&H%H/6,02HB6,o`3, K
nY F'6^C'&
3,HH/6z63,HP6'&6,o"%$'&>5:HPot6l44CN"/6$43,&a3ZRX6H/"B;,0%&3+"/&10K"%$43,."%$'&R1l'0%0%&"P&H/"%5:>a3+"/&ot6,0B"%$43+"
'6^C'&,@jEQ$'&10%&3,H L ?&10%ot6,02>aH57"/&1023+"%57,&.C'&1&1?&45:';#H/&3+02R2$'&H)l4"%5:8L"%$'&N'6^C'&N5:HH/687,&Ck6,0
H%$'6EQa"/6n$43,&P3]RX6H/"ff;,0%&3+"/&10`"%$43,"%$'&YR1l'0%0%&"&H/"%5:>a3+"/&CRX6H/"ff6,o"%$'&Q?40%&C'&RX&H%H/6,0 K
nY F'6^C'&
87;,6,0257"%$4>ot6,0s)0%&1;,0%&H%H%576DH/&3+02R2$D$43,&H%$'6EQ#"%$43+"K57"
+5:HK^'6?,&1"K025:>>6,&"0%&%HP & EQ=57R1"%57$#&"3,K#"%$4h"/3,&1#023+"%57,& JA&H/"]3,#&
L O= X^?&1025:>&"%3,8MRX6>?G3+025:H/6#i&1"FEff&1&D L 3,4C#"%$'&
P \)3,87;,6,0257"%$4>0%&>a3,5:4H`"/6ai&C'6'&
L @h"/&1023+"%57,&nJA&H/"Y3,4C= P \3,8:8?&10%ot6,02>"/6,?'FC'6EQ@C'&1?4"%$^G02H/"L57"/&1023+"%57,&nC'&1&1?&45:';
H/&3+02R2$'&H1@BiGl'"'6'&6,on"%$'&H/&DR2$43+023,RX"/&1025:H/"%5:R1H6,on"%$'&#3,87;,6,0257"%$4>aH3+0%&#&H%H/&"%5:3,8Bot6,0"%$'&#"%$'&570
l4H/&a5:RX6>?Gl'"%5:';3,#5:>?40%6,&C#$'&l'025:H/"%5:R ~ K
nY
P_ H/&3+02R2$D3,87;,6,0257"%$4>R13,#i&l4H/&C"/6
R13+0%0%~#6l'"]"%$'&0%&8:3^&CH/&3+02R2$@M3,Hn876';.3,Hn57"]C45:H%RX6,&102H]"%$'&6,?4"%5:>a3,8ffRX6H/"Zqt6,0a3=;,0%&3+"/&10a876Eff&10
i6l44C |`6,o&1,&10%~=&X^?G3,4C'&C#P_jF'6^C'& 6,0B&X'3,>?G87&,@H/"%3,4C43+02C q n 5:8:H%H/6@ w,9 |P3,4CN"%$'&
Y&'&1023,8:571&C %57g^H/"/023Z3,87;,6,0257"%$4>viz
~ N3+0%"/&8:8:5 3,4C8=6"%3,43+025Qq!w +|Pi6,"%$C'6Z"%$45:H1@3,4Ci6,"%$
6+&10Y3?6H%H%57iG5:8:57"F~6,o "/023,C45:';;,0%&3+"/&10P>&>6,0%~0%&zl4570%&>&"%Hjot6,0Q87&H%HLH/&3+02R2$="%5:>&qt"%$'6l';$"%$'&
0%&H%l487"%H`6,off6'&1"Q3,4CNY&X'&10@ (+*,*+-'@ 5:4C45:R13+"/&B"%$43+"j"%$45:HL>a3~Z'6,"ji&B"%$'&KR13,H/&|
H/&3+02R2$k$43,Hi&1&T>6H/"%87~[5:,&H/"%57;3+"/&CW5:["%$'& n3+0%&36,oP;3,>&?G8:3~^5:'; JL$'&
K nYP _ Pqt6,_
O0 N5:^N3G|H/&3+02R2$]H/?G3,RX&HA0%&1?40%&H/&"%5:';L"FEff6+?G8:3~,&10;3,>&H3+0%&ffH/6>&1EQ$43+"C45&10%&"
K nY
ot0%6>"%$'&Ks)0%&1;,0%&H%H%576NH/?G3,RX&,I`"%$'&10%&]5:HQ'6RX64RX&1?4"Y6,oMH/68:l'"%576RX6H/"1@6,"%$'&10Y"%$43,"%$'&]Eff6 876H/"
C45:H/"%5:4RX"%576@M3,4C[ot6,0>6H/";3,>&Ha57"5:H5:'ot&3,H%57iG87&"/6#H/&3+02R2$Tot6,03RX6>?G87&1"/&H/68:l'"%576@M023+"%$'&10
"%$'&H/&3+02R2$3,5:>aHff"/6a5:>?40%6,&B"%$'&B3,R1R1l'023,RX~)6,o3$'&l'025:H/"%5:RQ&H/"%5:>a3+"/&B6,oA"%$'&l4H/&1o}l48:'&H%H6,o 3n>6,&
6,0]?6H%57"%576D5:#"%$'&;3,>& JL$zl4H1@;3,>&X"/0%&1&H/&3+02R2$'&Hn3+0%&)C'&1?4"%$^i6l44C'&C@023+"%$'&10n"%$43,RX6H/"!
i6l44C'&C@3,4C.+3,8:l'&HP3+"B"%$'&a87&3+off'6^C'&HY6,o"%$'&"/0%&1&a3+0%&n;57,&.iz~N3ZH/"%3+"%5:R$'&l'025:H/"%5:Ro}l44RX"%576
s)0%&1;,0%&H%H%576]R13,Ki&Mot6,02>]l48:3+"/&C]5:"%$45:HE`3~,@,iz~B"%3+g^5:';Q"%$'&ffH%l4>6,oG3,R1R1l4>]l48:3+"/&C]3,4CK&H/"%5:>a3+"/&C
RX6H/"]3,HK"%$'&H/"%3+"%5:R$'&l'025:H/"%5:R]o}l44RX"%576 C'&1?4"%$^i6l44C'&C.H/&3+02R2$DEQ57"%$#3=H/"%3,4C43+02C;3,>&X"/0%&1&
H/&3+02R2$W3,87;,6,0257"%$4> q}9&3+028@Qw9 -'Q98:3,3+"n&1"3,8 @Qw,,|nR13,[i&Zl4H/&C"/6#5:>?40%6,&)"%$'&Z3,R1R1l'023,RX~
6,o"%$'&n&H/"%5:>a3+"/&C#RX6H/"B6,o"%$'&n0%6z6,"'6^C'& JL$45:H1@$'6Eff&1,&10@Ao}3,5:8:HY"/6Z3,R2$457&1,&a"%$'&>a3,5:N6,i^/&RX"%57,&

5}



fi

4

4

6,off0%&8:3^&CH/&3+02R2$@AEQ$45:R2$5:HP"/6C45:H%RX6,&10q}3,4C#H/"/6,0%&|P5:>?40%6,&CRX6H/"&H/"%5:>a3+"/&HBot6,0"%$'&aH%571&ns
H/"%3+"/&HP&4RX6l4"/&10%&CC4l'025:';"%$'&H/&3+02R2$@H/6)"%$45:HQ>&1"%$'6^CNEff6l48:CN$43,&n"/6)i&]l4H/&CN5:N3C45&10%&"
E`3~,@c c:@ 3,HP3C'&1?4"%$^i6l44C'&C876z6,gF3,$'&3,C="/6)5:>?40%6,&K"%$'&]3,R1R1l'023,RX~6,oM$'&l'025:H/"%5:RB&1+3,8:l43+"%5764H
6,oMH/"%3+"/&HP5:Z"%$'&]'6,02>a3,80%&1;,0%&H%H%576=H/&3+02R2$ P"%$'&K6,"%$'&10P$43,4C@GH%5:4RX&"%$'&]5:>?40%6,&C=$'&l'025:H/"%5:R
+3,8:l'&j5:H5:]"%$45:H>6^C'&`6,ol4H/&j'6,"MH/"/6,0%&CiGl'" 6487~Kl4H/&Cnot6,0M"%$'&jH/"%3+"/&Lot0%6> EQ$45:R2$]"%$'&L876z6,gF3,$'&3,C
H/&3+02R2$6,0257;5:43+"/&H1@57"P5:HP'6,"B'&RX&H%H%3+0%~N"/6ZH%5:>?G8:57ot~)"%$'&ns)0%&1;,0%&H%H%576.H/?G3,RX&,@A3,4C.3?6,"/&"%5:3,8:87~
>6,0%&?6Eff&10%o}l48 0%&8:3'3+"%576R13,i&l4H/&C

QF(+ 8 F B !AFS
JL$'&j"FEff6K?G8:3,4'&102H &"/&10%&C5:"%$'&Y(+*,*+-K!"/&10243+"%57643,8498:3,445:';K<ff6>?&1"%57"%576@JL9M-K3,4C } - @
3+0%&P,&10%~H%5:>a5:8:3+0IA"%$'&Y6487~C45&10%&4RX&P5:H"%$43+" } - 5:,&H/"%HjH/6>&P&X6,0%"j5:"/6nRX6>?Gl'"%5:';n3n>6,0%&
3,R1R1l'023+"/&j$'&l'025:H/"%5:R+@"%$'0%6l';$]3YH/&10257&HA6,o4H/&3+02R2$'&H5:0%&8:3^&CK0%&1;,0%&H%H%576]H/?G3,RX&Hjqt"%$'&ffs)0%&1;,0%&H%H%576
H/?G3,RX&H|YEQ$45:R2$3+0%&C'&10257,&C.ot0%6>"%$'&aH%3,>&a0%&8:3'3+"%576#3,HB"%$'&m $ $'&l'025:H/"%5:R]l4H/&Ciz~.JL9M- !^
C'&1&C@ "%$'&]>6,"%57+3+"%576=ot6,0Y&"/&1025:';i6,"%$=?G8:3,4'&102HjE`3,HY"/6)>a3+g,&nl4H/&K6,oM"%$'&]RX6>?&1"%57"%576N3,HY3,
&X^?&1025:>&"%3,8 &1+3,8:l43+"%576#6,off"%$'&a0%&8:3^&C#H/&3+02R2$D"/&R2$445:zl'&,@3,HEff&8:83,HKRX6>?G3+025:';Zi6,"%$?G8:3,^
'&102HQ"/66,"%$'&10BH/"%3+"/&X6,of"%$'&XF3+0%"B6,?4"%5:>a3,8A"/&>?6,023,8?G8:3,4'&102H OQ 6,0BH/&1,&1023,80%&3,H/64HY$'6Eff&1,&10@"%$'&
RX6>?&1"%57"%576Z0%&H%l487"%HLC45:CZ'6,"Q?40%6^5:C'&B"%$'&KRX6>?G87&1"/&K?G5:RX"%l'0%&B6,o "%$'&K0%&8:3+"%576Zi&1"FEff&1& } - 3,4C
JL9M-
HPC'&>64H/"/023+"/&C$'&10%&,@ } - R13,N?40%6^C4l4RX&]i&1"/"/&10P0%&H%l487"%HQ"%$43,.JL9M-)5:NH/6>&n6,o"%$'&nRX6>n
?&1"%57"%576TC'6>a3,5:4H1@M"%$'6l';$W5:T'6#C'6>a3,5:TC'6z&H } - RX6>?G87&1"/&87~C'6>a5:43+"/&Z6,&10)JL9M- h"a5:H
>a3,5:487~=6#?40%6,iG87&>aHB"%$43+"]3+0%&$43+02C@Aot6,0Ki6,"%$#?G8:3,4'&102H1@"%$43+""%$'&$'&l'025:H/"%5:Rn5:>?40%6,&>&"K0%&X
H%l487"%5:';jot0%6>r0%&8:3^&CKH/&3+02R2$]~^57&8:C4H3,]3,C'+3,"%3+;,& !]H/6>&ffC'6>a3,5:4H1@"%$'&ff5:>?40%6,&>&"A6,&10"%$'&
$ $'&l'025:H/"%5:RL5:H`'6,"`&'6l';$)"/6RX6>?&4H%3+"/&Bot6,0`"%$'&P"%5:>&BH/?&"`RX6>?Gl'"%5:';n57" OM >6,0%&BC'&1"%3,5:87&C
3,43,87~^H%5:H 0%&H%l487"/&C5:a3qtEff&3+g4|R2$43+023,RX"/&102573+"%576a6,o"%$'&QC'6>a3,5:4H5:nEQ$45:R2$0%&8:3^&CaH/&3+02R2$R13,ai&
&X^?&RX"/&C."/6Zi&RX6H/"B&X&RX"%57,&,IB5:.H%l4R2$C'6>a3,5:4H1@&X^?G3,4C45:';H%>a3,8:8 H/"%3+"/&H5:HPRX6>?Gl'"%3+"%57643,8:87~
R2$'&3+?&10P"%$43,=&X^?G3,4C45:';8:3+0%;,&nH/"%3+"/&H1@3,4CNH%>a3,8:8AH/"%3+"/&HP"/&4C="/6)$43,&H%>a3,8:8AH%l4R1RX&H%H/6,0PH/"%3+"/&H
!"%$'&C'6>a3,5:4HBEQ$'&10%&a57"5:HB"/6z6=&X^?&4H%57,&,@6#"%$'&a6,"%$'&10]$43,4C@"%$'&RX&"/023,8M?40%6,iG87&>5:HP"%$43+"
"%$'&Li4023,4R2$45:';Bo}3,RX"/6,0ff6,os)0%&1;,0%&H%H%5765:H`7+'Xd"%$43,a"%$43+"M6,o'6,02>a3,8'0%&1;,0%&H%H%576aH/&3+02R2$@^H/6"%$43+"
H/&3+02R2$45:';#5:"%$'&Z0%&8:3^&CTH/?G3,RX&=5:HnRX6>?Gl'"%3+"%57643,8:87~ka+d%&X^?&4H%57,&#q}zl457"/&RX6"/023+0%~"/6#"%$'&
5:C'&3)6,off6,i4"%3,5:45:';Z3$'&l'025:H/"%5:RK&H/"%5:>a3+"/&aiz~.H/687^5:';Z3W/H%5:>?G8:5G&C4a?40%6,iG87&>| JjEff6=>&3,H%l'0%&H1@
! cff"%$'&Ki4023,4R2$45:';no}3,RX"/6,0Y6,offP_jF'6^C'&HL5:Z0%&8:3^&CH/&3+02R2$=0%&8:3+"%57,&"/6i4023,4R2$45:';o}3,RX"/6,0P5:Z"%$'&
6,0257;5:43,8H/&3+02R2$NH/?G3,RX&]3,4C"%$'&K0%&8:3+"%57,&]H%571&6,o"%$'&KH%l4R1RX&H%H/6,02HQ6,offP_jF'6^C'&HQ5:Z0%&8:3^&C=H/&3+02R2$@
EQ$'&10%&`ot6l44CK"/6Bi&ff;,6z6^C5:4C45:R13+"/6,02H6,o $'6ETEff&8:8 } - ?&10%ot6,02>&C@,RX6>?G3+0%&Cn"/6BJL9M-'@5:n3Y;57,&
C'6>a3,5: !a"%$'&Q&X^?&1025:>&"%H1@"%$'&H/&P>&3,H%l'0%&HEff&10%&Q"%3+g,&)iz~&X^?G876,025:';B"%$'&YH/&3+02R2$)H/?G3,RX&H1@^iGl'"
57">a3~3,8:H/6i&L?6H%H%57iG87&`"/6]&H/"%5:>a3+"/&Q"%$'&>vq}57o'6,""/6]R13,8:R1l48:3+"/&Y"%$'&>&X'3,RX"%87~4|ot0%6>"%$'&YC'6>a3,5:
C'&H%RX0257?4"%576
JL$'&B3,43,87~^H%5:H`6,oA"%$'&C'6>a3,5:4Hj5:EQ$45:R2$)0%&8:3^&CZH/&3+02R2$Zo}3,5:8:H`"/6i&P&X&RX"%57,&K3,8:H/6?65:"%H`6l'"
?6H%H%57iG5:8:57"%57&Hffot6,0P5:>?40%6,&>&"1@3,4CNH/&1,&1023,8 5:C'&3,HQot6,0P?6,"/&"%5:3,85:>?40%6,&>&"%HQ"/6)"%$'&]>&1"%$'6^C
R13,Ui&Not6l44CS5:V0%&8:3+"/&C/5:4RX0%&>&"%3,8:[H/&3+02R2$H%R2$'&>&H5:V"%$'&8:57"/&1023+"%l'0%& JL$'&H/&5:4R18:l4C'&
8:5:>a57"%5:';ZH/&3+02R2$"/6.3NH%>a3,8:87&10Kot023,RX"%576D6,oj"%$'&0%&8:3^&CH/?G3,RX&,@Ml4H%5:';=RX6> 5:RX"%HK"/6.C4570%&RX"KH/&3+02R2$
"/6a"%$'&KH/"%3+"/&HY>6,0%&K8:57g,&87~"/6i&$43,&K"%$'&570Q$'&l'025:H/"%5:RY+3,8:l'&HL5:>?40%6,&C@43,4C3,87"/&10243+"%57,&KH/&3+02R2$
3,87;,6,0257"%$4>aH`ot6,0 K
nY
P_;,023+?G$4H JL$45:H`5:Hj6'&C4570%&RX"%576)ot6,0Lo}l'"%l'0%&C'&1,&876,?G>&"%H
5:43,8:87~,@0%&H%l487"%HMR187&3+0287~nC'&>64H/"/023+"/&Q"%$43+"3,87"%$'6l';$5:>?40%6^5:';P"%$'&Y$'&l'025:H/"%5:R`5:>?40%6,&HM?&10/
ot6,02>a3,4RX&=5:[H/6>&=C'6>a3,5:4H1@ff3,876'&57"a5:H'6,"&'6l';$T"/6D3,R2$457&1,&=;,6z6^CT?&10%ot6,02>a3,4RX&)0%&8:5:3+iG87~

5}


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

3,RX0%6H%HP3,8:8"%$'&nRX6>?&1"%57"%576=C'6>a3,5:4H <`9JqtP5:C43,8Y&X'&10@(+*,*+-z|2@"%$'&]6487~)6,"%$'&10P6,?4"%5:>a3,8
"/&>?6,023,8?G8:3,4'&10j"/6a?G3+0%"%5:R157?G3+"/&B5:)"%$'&KRX6>?&1"%57"%576@'3+?4?&3+02Hj"/6a~^57&8:Ci&1"/"/&10Q0%&H%l487"%Hj5:Z>6H/"
RX6>?&1"%57"%576ZC'6>a3,5:4HBqt"%$'6l';$'6?40%&R15:H/&BRX6>?G3+025:H/6Z$43,H`i&1&Z>a3,C'&| <`9JB@'8:57g,&BJL9M-'@4l4H/&H
"%$'&]"/&>?6,023,8m $ $'&l'025:H/"%5:R+@GiGl'"Y?&10%ot6,02>aHQ3)'6^FC4570%&RX"%57643,8AH/&3+02R2$@3,4C.l4H/&HPRX64H/"/023,5:"Y0%&1?'
0%&H/&"%3+"%5763,4CN?40%6,?G3+;3+"%576."/&R2$445:zl'&HY"/6Z5:'ot&10PRX6H/"Bi6l44C^5768:3+"%5764H JL$'&n5:'& =R157&4RX~6,o
C4570%&RX"%57643,8`H/&3+02R2$W5:"/&>?6,023,8ff?G8:3,445:';N$43,Hni&1&['6,"/&CTi&1ot6,0%&,I"%$zl4H<`9J3,4C } - R13,
i&]H%3,5:C"/6)5:>?40%6,&K6="FEff6ZC45&10%&"QEff&3+g^'&H%H/&HY6,oMJL9M- <ff6>KiG5:45:';"%$'&H/&]5:>?40%6,&>&"%HP5:H
3,'6,"%$'&10YR2$43,8:87&';,&Bot6,0L"%$'&o}l'"%l'0%&

,Qkgto|ksq
e& RX"/6,0Y&X'&10n$43,HK$43,CD3,D5:>?6,0%"%3,"?G3+0%"K5:"%$'&C'&1,&876,?G>&"K6,oj0%&8:3^&CDH/&3+02R2$H%5:4RX&57"%H
5:4RX&1?4"%576=3,4C"%$'0%6l';$=zl4>&10%6l4HQC45:H%R1l4H%H%5764H ,643,HB
+3+024H/"/0A6 >Z@ 9&10 n ~ziG876>Z@ !_x3,H%H/6+
R15:3+"/&Y&C457"/6,0N
3+025:3 63,4C"%$'&Y3,'6~^>6l4HM0%&1^57&1Eff&102Hff3,8:84?40%6^5:C'&C,&10%~a$'&87?4o}l48'RX6>a>&"%Hff6


C'023+ot"%Hj6,o"%$'&?G3+?&10 OM 8:8o}3,l487"%Hj3+0%&6,o RX6l'02H/&K>a5:'&

F1FF hF-

$'6'@ K
@MQ6,?RX0%6,ot"1@M @YY8:8:>a3,@ q!w9,| OC ,e e}d4e'^d%=+ +d}et] O[ C4C45:H/6^
&H%87&1~
ff&8:8:>a3,@4_ q!w, ,| O' + +n`d/%,d/+nntz 9M025:4RX&1"/6Y457,&102H%57"F~)9M0%&H%H
ff6'&1"1@ @Y&X'&10@ q(+*,*+-z| OK N3,87;,6,0257"%$4>i&1"/"/&10P"%$43, !`d/c(h+etsK,e}+ +
+ Xd%X +s
jh @'?4? w-].w%-
<`3+1&43,&,@+J q(+*,*'w| h"/&1023+"%57,&EQ5:C'&45:'; !`d/c et
2GeFXd ,e}+ + + Xd%X j+
YdXe} ff1+
2GeFXt 7zX K
j
h @4?4? ,(,]Z,9(
<`l487i&102H/6@ff @L\^R2$43+&X&10@L q!w,,| \z&3+02R2$45:';EQ57"%$k?G3+"/"/&102UC43+"%3+iG3,H/&H ! + ++
+ Xd%X +s
2@'68 w1* ^w6,fi
ffL G@'?4? -*(]-4w \z?4025:';,&10
?@ \ q(+*,*'w| 98:3,445:';nEQ57"%$Z?G3+"/"/&102=C43+"%3+iG3,H/&H !`d/
c +et
E^d/24%+ + Xd%X
+ C'&87g+3,+>s
`7+44tz
n @4?4? wKZ(-
6+>a3,4@j q(+*,*+-z| 9 P ( (zI`"%$'&N8:3,';l43+;,&Not6,0"%$'&NR18:3,H%H%5:R13,8L?G3+0%"6,o
+ C'&87g+3,!9`><?@`- \ @`!4Qet

2GeFXd ,e}+ + `7+44tz +Q41e}}e}+
:1e}@ ?4? ()# O= +3,5:8:3+iG87&a3+"
%Oh~t% >
hQ
Sfi19h
&8:'&10@ KO @ B6,0%oh@4_ @ Y3,43,@\ q(+*,*+-z| Oj C4C457"%57,&P?G3+"/"/&102=C43+"%3+iG3,H/&K$'&l'025:H/"%5:R1H ' ^d + 1
"!L%+d/X@ @( +KZ^5w
@ @Mv6';'@ ]O q(+*,*| 9 P ( w+I &X^"/&4H%576["/69 P Uot6,0&X^?40%&H%H%5:';N"/&>?6,023,8
6
?G8:3,445:';nC'6>a3,5:4H ' ^d + 1 6"!L%+d/X
@ h@ ^wPNw(-
P3+0%025:C'6'@ KO @ 6@ @G 6';'@ ]O q(+*,*(| "/&>?6,023,8 ?G8:3,445:';]H/~^H/"/&>ot6,0QC4l'023+"%57,&B3,RX"%5764H
6,o9 P ( w !`d/#1 Net'#
,et$
E^d%%24%+ + Xd%X +C
YdXe} ff1+#
2GeFXt 7zX %
h A&@4?4? 9 ,]),+*
P3+0%025:C'6'@ KO @P43,5:4C45:3^@ +LO @`3+0%i&10@ ffO q(+*,*'w| JL5:>&X6,?4"%5:>a3,8?G8:3,445:';n5:Z"/&>?6,023,8?40%6,i'
87&>aH !`d/fi
c +et
E^d/24%+ + Xd%X +s
`7+44tz
n @4?4? , -*(

5}


fi

4

4

P3,H%R2$4457;'@ q!w+| OT ?40%6,iG87&>H%5:>a5:8:3+0257"F~3+?4?40%63,R2$T"/6#C'&1^5:H%5:';N$'&l'025:H/"%5:R1H1I 5702H/"n0%&H%l487"%H
!`d/c +et_2GeFXd ,e}+ +' '+tGe +Xd%X K+YdXe} ff1+2GeFXt 7zX nj
h )(@^?4?
+*'wPZ+3*
P5:4H/i&10%;'@ @Y3+0%,&1~,@ q!w,,(| h"/&1023+"%57,&i40%63,C'&45:'; YdXe} ff1+ 2GeFXt 7zX X@*q(h|2@
, KZ9 ,
Y3,H%8:l4>Z@9 q(+*,*+-3| !>?40%6^5:';K$'&l'025:H/"%5:R1H "%$'0%6l';$H/&3+02R2$ !I`d/+c E^d/24%+ + Xd%X B+
h ,-@4?4? w1*^wP.w1*,(
Y3,H%8:l4>Z@z9 q(+*,*+-,i| JL9M-' *+-a3,4C } - !4et_
2GeFXd ,e}+ +S `7+44tz +Q41e}}e}+.
:1e}@
?4? 9 K-* +3,5:8:3+iG87&B3+#
" %h~t% >
hQ
,fi19h
Y3,H%8:l4>Z@B9 @ff6'&1"1@K @] Y&X'&10@ q(+*,*| n &1E3,C4>a5:H%H%57iG87&$'&l'025:H/"%5:R1H=ot6,0DC'6>a3,5:^
5:4C'&1?&4C'&"?G8:3,445:'; !I`d/c h+et
K,e}+ + + Xd%X +I
jh @?4? w,w,
.w,w9
Y3,H%8:l4>Z@a9 @) Y&X'&10@ q(+*,*,*| C4>a5:H%H%57iG87&S$'&l'025:H/"%5:R1H[ot6,0k6,?4"%5:>a3,8Z?G8:3,445:'; !
`d//c ,et8
2GeFXd ,e}+ + + Xd%X =+ YdXe} ff1+(
2GeFXt 7zX n
`7+44tz+
X'% ^
tz
j:#
@4?4? wX-*n.wX- YY `9M0%&H%H
Y3,H%8:l4>Z@z9 @4 Y&X'&10@^ q(+*,*'w| Q&l'025:H/"%5:RL?G8:3,445:';BEQ57"%$"%5:>&P3,4C0%&H/6l'02RX&H !`d/
c +et
E^d/24%+ + Xd%X n+
`7+44tz
n @G?4? w(^wP.w,(
Q&10243,C' 6 87;,~^5@ @vQ687"/&,@_ q(+*,*,*| N
+ ^?&1025:>&"%H]EQ57"%$T3,l'"/6>a3+"%5:R13,8:87~RX0%&3+"/&Ck>&>6,0%~
iG3,H/&CV$'&l'025:H/"%5:R1H !2 `d/
c yXe}d/e}+ %0!Lj 1+d[
^7,e}+ %+ G
ff d/,ta,e}+ %04et
2GeFXd ,e}+ +L G+Q' ^ Q1!'Nh
@ ?4? 9( ^wPZ(,+*
Q6+>a3,4@' @' + C'&87g+3,>?@'\ q(+*,*| JL$'&PR18:3,H%H%5:R13,8G?G3+0%"ff6,o!9`<-'I 6,&10%^57&1E JA63+?4?&3+0
5:)"%$'2
& ' ^d + 1 1!L%+d/Xqt"%$45:HY\z?&R15:3,8JA023,R%g4|
Q6+>a3,4@` @ + C'&87g+3,>?@`\ @ + ';87&10%"1@_ @ffA57?6,023,RX&,@ ffO JL$4;5 &1 iG3,l^@`\ @`JA0 l' ;'@`\ q(+*,*+-z|
JA6E`3+02C4HL0%&3,8:5:H/"%5:RQi&4R2$4>a3+0%g^H`ot6,0L?G8:3,445:';'IzJL$'&C'6>a3,5:4H`l4H/&CZ5:)"%$'&R18:3,H%H%5:R13,8?G3+0%"j6,o
!9`<- !#4ets
2GeFXd ,e}+ +
`7+44tz +Q41e}}e}+
:1e}@?4? DwX- +3,5:8:3+iG87&]3+"
%h~t% >
hQ
Sfi19h
Q687"/&,@ff_ @ff9&10%&1,
@
@ R5:>a>&10@M_ @` N3,R 643,8:C@ KO q!w,,| Y57&1023+02R2$45:R13,8

\z&3+02R2$45:';#3+iGH/"/023,RX"%576T$457&1023+02R2$457&Hn & =R157&"%87~ ! `d/3
c 4et
K,e}+ + + Xd%X N+
YdXe} ff1+S 2GeFXt 7zX _
jh (5
@4?4? ,+*nZ,,
l4';$43,44H1@ KO @j\^R2$43+&X&10@j q(+*,*'w| \z6,g,6,iG3,I + 4$43,4R15:';;,&'&1023,8YH%5:';87&XF3+;,&")H/&3+02R2$
>&1"%$'6^C4HLl4H%5:';C'6>a3,5:)g^'6EQ87&C';,& YdXe} ff1+ 2GeFXt 7zX X6
@
(q!wh(|2@(^w])(,^w
B6,0%oh@'_ q!w9 ,| &1?4"%$^G02H/"j57"/&1023+"%57,&XFC'&1&1?&45:';'I )6,?4"%5:>a3,83,C4>a5:H%H%57iG87&L"/0%&1&H/&3+02R2$ YdXe}
1+S 2GeFXt 7zX X
@ 7q!w|2@ .w1*
B6,0%oh@_ q!w,,| 0%"%5 R15:3,8 5:"/&8:8:57;,&4RX&H/&3+02R2$D3,87;,6,0257"%$4>aH !8K+ 3 y% .1 +d}et]+
'e,e}+4@R2$43+? , <`_L<S9M0%&H%H
9 '%+dZ1 +QS
A5:l,@ : A@ B6z&457;'@\ @+ l'02RX~,@ ]O q(+*,*(| \z?&1&C45:';Ll'?"%$'&R13,8:R1l48:3+"%576B6,o4$'&l'025:H/"%5:R1Hot6,0$'&l'025:H/"%5:R
H/&3+02R2$^iG3,H/&C?G8:3,445:'; ! `d/1
c ;et
K,e}+ + + Xd%X +
YdXe} ff1+
2GeFXt 7zX
jh A&@4?4? 3- --^w
6';'@ ]O @A 6L
@ q(+*,*| [
+ ^?G87657"%5:';3;,023+?G$'?G8:3,=ot023,>&1Eff6,0%gN5:="/&>?6,023,8?G8:3,445:'; !
4et
2GeFXd ,e}+ + + Xd%X +
K'e+a,eF%
`7+44tz#+
X'% ^ tz j # -4,@
?4? ^wYZ,(

5}~}


fi (^z7G

4 % : 4^-G-Ns4-G^

1 f

N3+0%"/&8:8:5@ KO @C=6"%3,43+025@ q!w+| C4C457"%57,& KnY P_W;,023+?G$4H !n`d/c+4,d/2GeFXd ,e}+ +
'+tGe + Xd%X +sY
dXe} ff1+2GeFXt 7zX Kj
h <4,@4?4? wQ.w,w
n 5:8:H%H/6H/@68:l'Kn "%O576 4HO q!w!,9 `| d/O \zcL&3+ =O02R2:$4 5:'; )+?4z0%,6,d%iG287&>n@G?4FH/? 687^w5:(,';].3,4Cw+N* ;3,>&X?G8:3~^5:';)"/0%&1&HPot6,0B>a5:45:>a3,8ARX6H/"



9&3+028@M q!w9-z| 8] ^dfXe}X&>2GeFXt 7zXGe%+d/X e}d/,eF,61+d +QS'eFXd_`d/3yX:X + tz

4C C45:H/6^&H%87&1~
98:3,3+"1@ KO @+\^R2$43+&X&10@+ @95 %8:H1@1 @ KO @C q!w,,| ff&H/"!G02H/"4^&C^FC'&1?4"%$>a5:45:>a3P3,87;,6,0257"%$4>aH
YdXe} ff1+S 2GeFXt 7zX X@'; q!wh(|2@(,,]Z(,,
9M0257&C457"%5:H1@ KO+LO q!w,,| N3,R2$45:'&LC45:H%RX6,&10%~]6,oG&X&RX"%57,&Q3,C4>a5:H%H%57iG87&ff$'&l'025:H/"%5:R1H O7? XtffA%+d4
tz,6
@
@Aw,~w K.wX-4w
_L&5:'ot&8:C@ KO @G
N3+02H%8:3,4C@^J q!w,-z|
+ 4$43,4RX&C)57"/&1023+"%57,&XFC'&1&1?&45:';nH/&3+02R2$ fifi 9 d/+'11
e}+'n+s
L,eeFXds
+ f]+ ? Xt6
2GeFXt 7zX X6
@ 5 A*q ,|2"@ *'wY
zw1*
\^>a57"%$@ ]O q(+*,*+-z| <`$'6z6H%5:';6,i^/&RX"%57,&H5:[6,&10/FH%l'iGH%RX0257?4"%576[?G8:3,445:'; !C`d/0
c &4et
24
eFXd ,e}+ + + Xd%X a+
K'e+a,eF%_
`7+44tzA
@X'% ^ tz8
j # ,-@G?4? ,,n
-*'w
\^>a57"%$@ ]O @&8:C@ ]O q!w,,| JA&>?6,023,8ff?G8:3,445:';=EQ57"%$[>]l'"%l43,8ff&X'R18:l4H%5760%&3,H/645:'; !
`d/"c 5 +et
2GeFXd ,e}+ +B
'+tGe + Xd%X Z+G
YdXe} ff1+O
2GeFXt 7zX
j
h ((@A?4?
,(,]Z,,
JA025:4zl43+0%"1@M_ q(+*,*| 43,87~z5:';0%&3,R2$43+iG5:8:57"F~[EQ57"%$45:[?G8:3,WH/?G3,RX& ! # -4 e+d/+
+'1+dXe} ^n@ ?4? w(,(].w(,
M3,87"/6,0%"%3^O
@ q!w9 -z| 0%&H%l487"P6#"%$'&RX6>?Gl'"%3+"%57643,8MRX6>?G87&X'57"F~.6,oj$'&l'025:H/"%5:R]&H/"%5:>a3+"/&HKot6,0
"%$'&
1X C
@ 4<'@43- KZ,
3,87;,6,0257"%$4> 2 1+da,e}+
&10%o}3,5:8:8:57&,@4 @'&>a3,57"/0%&,@ @4\^R2$457&X@'J q!w,,| _Ql4H%H%5:3,C'68:8H/&3+02R2$)ot6,0LH/687^5:';nRX64H/"/023,5:"
6,?4"%5:>a573+"%576?40%6,iG87&>aH !`d//
c 4etG
K,e}+ + + Xd%X =+ YdXe} ff2+
2GeFXt 7zX
jh (5
@4?4? 5w ^wP.5w
P5:C43,8@ff @L Y&X'&10@L q(+*,*+-z| ff023,4R2$45:';[3,4CS?402l445:';'I S6,?4"%5:>a3,8Y"/&>?6,023,8P9`<`
?G8:3,4'&10iG3,H/&CV6SRX64H/"/023,5:"?40%6,;,023,>a>a5:'; !`d/D
c (,et
K,e}+ + + Xd%X #+
YdXe} ff1+S 2GeFXt 7zX _
jh ,-@4?4? *nZ 9

5}


fiJournal Artificial Intelligence Research 25 (2006) 457-502

Submitted 11/05; published 4/06

Continuation Method Nash Equilibria Structured
Games
Ben Blum

bblum@cs.berkeley.edu

University California, Berkeley
Department Electrical Engineering Computer Science
Berkeley, CA 94720

Christian R. Shelton

cshelton@cs.ucr.edu

University California, Riverside
Department Computer Science Engineering
Riverside, CA 92521

Daphne Koller

koller@cs.stanford.edu

Stanford University
Department Computer Science
Stanford, CA 94305

Abstract
Structured game representations recently attracted interest models multiagent artificial intelligence scenarios, rational behavior commonly characterized
Nash equilibria. paper presents efficient, exact algorithms computing Nash equilibria structured game representations, including graphical games multi-agent
influence diagrams (MAIDs). algorithms derived continuation method
normal-form extensive-form games due Govindan Wilson; follow trajectory space perturbed games equilibria, exploiting game structure
fast computation Jacobian payoff function. theoretically
guaranteed find least one equilibrium game, may find more. approach
provides first efficient algorithm computing exact equilibria graphical games
arbitrary topology, first algorithm exploit fine-grained structural properties
MAIDs. Experimental results presented demonstrating effectiveness algorithms comparing predecessors. running time graphical game
algorithm similar to, often better than, running time previous approximate
algorithms. algorithm MAIDs effectively solve games much larger
solvable previous methods.

1. Introduction
attempting reason interactions multiple agents, artificial intelligence
community recently developed interest game theory, tool economics. Game
theory general mathematical formalism representation complex multiagent scenarios, called games, agents choose actions receive payoffs
depend outcome game. number new game representations
introduced past years exploit structure represent games efficiently.
representations inspired graphical models probabilistic reasoning
artificial intelligence literature, include graphical games (Kearns, Littman, & Singh,
c
2006
AI Access Foundation. rights reserved.

fiBlum, Shelton, & Koller

2001), multi-agent influence diagrams (MAIDs) (Koller & Milch, 2001), G nets (La Mura,
2000), action-graph games (Bhat & Leyton-Brown, 2004).
goal describe rational behavior game. game theory, description
behavior agents game referred strategy profile: joint assignment
strategies agent. basic criterion look strategy profile
optimal agent, taken individually: agent able improve utility
changing strategy. fundamental game theoretic notion Nash equilibrium (Nash,
1951) satisfies criterion precisely. Nash equilibrium strategy profile
agent improve payoff deviating unilaterally changing strategy
agents hold fixed. types game theoretic solutions,
Nash equilibrium fundamental often agreed minimum solution
requirement.
Computing equilibria difficult several reasons. First, game representations
grow quite large. However, many games would interested
solving require full generality description leads large representation
size. structured game representations introduced AI exploit structural properties
games represent compactly. Typically, structure involves locality
interaction agents concerned behavior subset agents.
One would hope compact representations might lead efficient computation equilibria would possible standard game-theoretic solution algorithms
(such described McKelvey & McLennan, 1996). Unfortunately, even compact representations, games quite hard solve; present result showing finding
Nash equilibria beyond single trivial one NP-hard types structured games
consider.
paper, describe set algorithms computing equilibria structured
games perform quite well, empirically. algorithms family continuation
methods. begin solution trivial perturbed game, track solution
perturbation incrementally undone, following trajectory space equilibria
perturbed games equilibrium original game found. algorithms
based recent work Govindan Wilson (2002, 2003, 2004) (GW hereafter),
applies standard game representations (normal-form extensive-form).
algorithms GW great interest computational game theory community
right; Nudelman et al. (2004) tested leading algorithms
found them, certain cases, effective available. However,
algorithms unstructured games, infeasible large games.
show game structure exploited perform key computational step
algorithms GW, also give alternative presentation work.
methods address graphical games MAIDs. Several recent papers
presented methods finding equilibria graphical games. Many proposed algorithms (Kearns et al., 2001; Littman, Kearns, & Singh, 2002; Vickrey & Koller, 2002; Ortiz
& Kearns, 2003) focused finding approximate equilibria, agent may
fact small incentive deviate. sorts algorithms problematic:
approximations must crude reasonable running times, guarantee
exact equilibrium neighborhood approximate one. Algorithms find
exact equilibria restricted narrow class games (Kearns et al., 2001).
458

fiA Continuation Method Nash Equilibria Structured Games

present first efficient algorithm finding exact equilibria graphical games arbitrary structure. present experimental results showing running time
algorithm similar to, often better than, running time previous approximate
algorithms. Moreover, algorithm capable using approximate algorithms starting
points finding exact equilibria.
literature MAIDs limited. algorithm Koller Milch (2001)
takes advantage certain coarse-grained structure MAIDs, otherwise falls
back generating solving standard extensive-form games. Methods related types
structured games (La Mura, 2000) also limited coarse-grained structure,
currently unimplemented. Approximate approaches MAIDs (Vickrey, 2002) come
without implementation details timing results. provide first exact algorithm
take advantage fine-grained structure MAIDs. present experimental results
demonstrating algorithm solve MAIDs significantly outside scope
previous methods.
1.1 Outline Guide Background Material
results require background several distinct areas, including game theory, continuation
methods, representations graphical games, representation inference Bayesian
networks. Clearly, outside scope paper provide detailed review
topics. attempted provide, topics, sufficient background
allow results understood.
begin overview game theory Section 2, describing strategy representations payoffs normal-form games (single-move games) extensive-form
games (games multiple moves time). concepts utilized paper
presented section, thorough treatment available standard
text Fudenberg Tirole (1991). Section 3 introduce two structured game
representations addressed paper: graphical games (derived normal-form games)
MAIDs (derived extensive-form games). Section 4 give result complexity computing equilibria graphical games MAIDs, proof deferred
Appendix B. next outline continuation methods, general scheme algorithms
use compute equilibria, Section 5. Continuation methods form broad computational
framework, presentation therefore necessarily limited scope; Watson (2000)
provides thorough grounding. Section 6 describe particulars applying
continuation methods normal-form games extensive-form games. presentation
new, methods exactly GW.
Section 7, present main contribution: exploiting structure perform
algorithms GW efficiently graphical games MAIDs. show Bayesian
network inference MAIDs used perform key computational step GW
algorithm efficiently, taking advantage finer-grained structure previously possible.
algorithm utilizes, subroutine, clique tree inference algorithm Bayesian
networks. Although present clique tree method full, describe
properties method allow used within algorithm; also provide
enough detail allow implementation algorithm using standard clique tree
package black box. comprehensive introduction inference Bayesian
459

fiBlum, Shelton, & Koller

networks, refer reader reference Cowell, Dawid, Lauritzen, Spiegelhalter
(1999). Section 8, present running-time results variety graphical games
MAIDs. conclude Section 9.

2. Game Theory
begin briefly reviewing concepts game theory used paper, referring
text Fudenberg Tirole (1991) good introduction. use notation
employed GW. readers familiar game theory may wish skip directly
table notation Appendix A.
game defines interaction set N = {n1 , n2 , . . . , n|N | } agents. agent
n N set n available strategies, strategy determines agents behavior
game. precise definition set n depends
Q game representation,
discuss below. strategy profile = (n1 , n2 , . . . , n|N | ) nN n defines strategy
n n agent n N . Given strategy profile , game defines expected
payoff Gn () agent n N . use n refer set strategy profiles
agents N \ {n} (agents n) n n refer one profile;
generalize notation n,n0 set strategy profiles two agents.
strategy profile, n0 n strategy agent n, (n0 , n ) new strategy
profile n deviates play n0 , agents act according .
solution game prescription strategy profile agents. paper,
use Nash equilibria solution concept strategy profiles agent
profit deviating unilaterally. agent knew others playing according
equilibrium profile (and would change behavior), would incentive
deviate. Using notation outlined here, define Nash equilibrium
strategy profile that, n N strategies n0 n ,
Gn (n , n ) Gn (n0 , n ).
also define notion approximate equilibrium, agents incentive deviate small. -equilibrium strategy profile agent
improve expected payoff unilaterally deviating .
words, n N strategies n0 n , Gn (n0 , n ) Gn (n , n ) .
Unfortunately, finding -equilibrium necessarily step toward finding exact
equilibrium: fact -equilibrium guarantee existence exact
equilibrium neighborhood .
2.1 Normal-Form Games
normal-form game defines simultaneous-move multi-agent scenario. agent independently selects action receives payoff depends actions selected
agents. precisely, let G normal-form game set N agents.
agent n N hasQ
discrete action set payoff array Gn entries every
action profile = nN is, joint actions = (an1 , an2 , . . . , an|N | )
agents. use refer joint actions agents N \ {n}.
460

fiA Continuation Method Nash Equilibria Structured Games

2.1.1 Strategy Representation
agents restricted choosing actions deterministically, equilibrium guaranteed exist. If, however, agents allowed independently randomize actions,
seminal result game theory (Nash, 1951) guarantees existence mixed strategy
equilibrium. mixed strategy n probability distribution .
strategy set n therefore defined probability simplex mixed
strategies. support mixed strategy set actions non-zero
probability. strategy n agent n said pure strategy single
action support pure strategies correspond
exactly deterministic actions
Q
. set mixed strategy profiles nN n , product simplices. mixed
strategy single agent represented vector probabilities, one
action. notational simplicity later on, concatenate allPthese vectors regard
mixed strategy profile single m-vector, = nN |An |. vector
indexed actions nN , action , probability agent
n plays action a. (Note that, notational convenience, every action associated
particular agent; different agents cannot take action.)
2.1.2 Payoffs
mixed strategy profile induces joint distribution action profiles, compute
expectation payoffs respect distribution. let Gn () represent
expected payoff agent n agents behave according strategy profile .
calculate value
X

Gn () =
Gn (a)
ak .
(1)
aA

kN

general case (a fully mixed strategy profile, every ), sum includes
every entry game array Gn , exponentially large number agents.
2.2 Extensive-Form Games
extensive-form game represented tree. game proceeds sequentially
root. non-leaf node tree corresponds choice either agent
nature; outgoing branches represent possible actions taken node.
natures choice nodes, game definition includes probability distribution
outgoing branches (these points game something happens randomly
world large). leaf z Z tree outcome, associated
vector payoffs G(z), Gn (z) denotes payoff agent n leaf z. choices
agents nature dictate path tree followed.
choice nodes belonging agent partitioned information sets;
information set set states among agent cannot distinguish. Thus, agents
strategy must dictate behavior nodes information set. set
agent ns information sets denoted , set actions available information
set denoted A(i). define agent history Hn (y) node tree
agent n sequence containing pairs (i, a) information sets belonging n
traversed path root (excluding information set
461

fiBlum, Shelton, & Koller

0.7

a2

0.8

0.7

b2

b1

0.6

a1 a2

(0, 2)

0.9

a1

0.3

b1

0.2

0.1

0.4

1.0

a3 a4

(1, 4)

(6, 7)

0.3

b2

0.0

0.5

a5 a6

(6, 0)

(3, 3)

0.5

a7 a8

(1, 7)

(8, 0)

(2, 6)

Figure 1: simple 2-agent extensive-form game.
contained), action selected n one. Since actions unique information
sets (the action cant taken two different information sets), also omit
information sets represent history ordered tuple actions only. Two nodes
agent-n history paths used reach indistinguishable n,
although paths may differ ways, natures decisions decisions
agents. make common assumption perfect recall : agent forget
information known choices made previous decisions. precisely, two nodes
y, 0 information set agent n, Hn (y) = Hn (y 0 ).
Example 1. game tree shown Figure 1, two agents, Alice Bob. Alice
first chooses actions a1 a2 , Bob next chooses b1 b2 , Alice chooses
two set {a01 , a02 , . . . , a08 } (which pair depends Bobs choice). Information
sets indicated nodes connected dashed lines. Bob unaware Alices actions,
nodes information set. Alice aware bottom level
initial action Bobs action, nodes distinct information set.
Edges labeled probability agent whose action follow it;
note actions taken nodes information set must probability
distribution associated them. eight possible outcomes game,
labeled pair payoffs Alice Bob, respectively.

2.2.1 Strategy Representation
Unlike case normal-form games, several quite different choices strategy
representation extensive-form games. One convenient formulation terms behavior
strategies. behavior profile b assigns information set distribution
462

fiA Continuation Method Nash Equilibria Structured Games

actions A(i). probability agent n takes action information set
written b(a|i). node i, also write b(a|y) abbreviation
b(a|i).
methods primarily employ variant sequence form representation (Koller &
Megiddo, 1992; von Stengel, 1996; Romanovskii, 1962), built upon behavior
strategy representation. sequence form, strategy n agent n represented
realization plan, vector real values. value, realization probability,
realization plan corresponds distinct history (or sequence) Hn (y) agent n has,
nodes game tree. sequences may partial records ns
behavior game proper prefixes larger sequences. strategy representation
employed GW (and ourselves) equivalent sequence form representation
restricted terminal sequences: agent-n histories least one leaf node.
shall henceforth refer modified strategy representation simply sequence form,
sake simplicity.
agent n, then, consider realization plan n vector realization
probabilities terminal sequences. outcome z, (Hn (z)), abbreviated n (z),
probability agent ns choices allow realization outcome z
Q words,
product agent ns behavior probabilities along history Hn (z), (i,a)Hn (z) b(a|i).
Several different outcomes may associated terminal sequence,
agent n may fewer realization probabilities leaves tree. set
realization plans agent n therefore subset IR`n , `n , number distinct
terminal sequences agent n, number leaves tree.
Example 2. example above, Alice eight terminal sequences, one
a01 , a02 , . . . , a08 four information sets bottom level. history one
last action (a1 , a03 ). realization probability (a1 , a03 ) equal b(a1 )b(a03 |a1 , b2 ) =
0.1 0.6 = 0.06. Bob two last actions, whose realization probabilities exactly
behavior probabilities.
realization probabilities non-zero, realization plans behavior strategies
one-to-one correspondence. (When probabilities zero, many possible behavior strategy profiles might correspond realization plan, described Koller
& Megiddo, 1992; affect work presented here.)
Q behavior strategy
profile b, easily calculate realization probability n (z) = (i,a)Hn (z) b(a|i).
understand reverse transformation, note also map behavior strategies
full realization plans defined non-terminal sequences
(as originally defined
Q
Koller & Megiddo, 1992) defining n (h) = (i,a)h b(a|i); intuitively, n (h)
probability agent ns choices allow realization partial sequence h. Using
observation, compute behavior strategy extended realization plan: (partial) sequence (h, a) extends sequence h one action, namely action information set
belonging agent n, compute b(a|i) = nn(h,a)
(h) . extended realization
probabilities computed terminal realization probabilities recursive
procedure starting leaves tree working upward: atPinformation set
agent-n history h (determined uniquely perfect recall), n (h) = aA(i) n (h, a).
several different information sets agent-n history h, n (h)
computed multiple ways. order (terminal) realization plan valid,
463

fiBlum, Shelton, & Koller

must satisfy constraint choices information sets agent-n history h must
give rise value n (h). formally, partial sequence h,

P constraints
P pairs information sets i1 i2 Hn (i1 ) = Hn (i2 ) = h,

(h,
a)
=
aA(i1 ) n
aA(i2 ) n (h, a). game tree Example 1, consider Alices
realization probability (a1 ). expressed either (a1 , a01 ) + (a1 , a02 ) =
0.1 0.2 + 0.1 0.8 (a1 , a03 ) + (a1 , a04 ) = 0.1 0.6 + 0.1 0.4, two sums must
same.
recursively defining realization probability sum realization probabilities longer sequences, constraints expressed terms terminal realization
probabilities; fact, constraints linear probabilities. several
constraints: probabilities must nonnegative, and, agent n, n () = 1,
(the empty sequence) agent-n history first information set agent n
encounters. latter constraint simply enforces probabilities sum one. Together,
linear constraints define convex polytope legal terminal realization plans.
2.2.2 Payoffs
agents play according , payoff agent n extensive-form game
X

Gn () =
Gn (z)
k (z) ,
(2)
zZ

kN

augmented N include nature notational convenience.
simply expected sum payoffs leaves. agent
Q k, k (z)
product probabilities controlled n along path z; thus, kN k (z)
multiplication probabilities along path z, precisely probability
z occurring. Importantly, expression similar multi-linear form payoff
normal-form game, using realization plans rather mixed strategies.
Extensive-form games expressed (inefficiently) normal-form games,
guaranteed equilibrium mixed strategies. extensive-form game
satisfying perfect recall, mixed strategy profile represented payoff-equivalent
behavior profile, hence realization plan (Kuhn, 1953).

3. Structured Game Representations
artificial intelligence community recently introduced structured representations
exploit independence relations games order represent compactly.
methods address two representations: graphical games (Kearns et al., 2001),
structured class normal-form games, MAIDs (Koller & Milch, 2001), structured
class extensive-form games.
3.1 Graphical Games
size payoff arrays required describe normal-form game grows exponentially
number agents. order avoid blow-up, Kearns et al. (2001) introduced
framework graphical games, structured representation inspired probabilistic graphical models. Graphical games capture local structure multi-agent interactions,
464

fiA Continuation Method Nash Equilibria Structured Games

allowing compact representation scenarios agents payoff affected
small subset agents. Examples interactions structure occurs include agents interact along organization hierarchies agents interact according
geographic proximity.
graphical game similar definition normal-form game, representation
augmented inclusion interaction graph node agent.
original definition assumed undirected graph, easily generalizes directed graphs.
edge agent n0 agent n graph indicates agent ns payoffs depend
action agent n0 . precisely, define Famn set agents consisting
n parents graph. Agent ns payoff function Gn array indexed
actions agents Famn . Thus, description game exponential
in-degree graph total number agents. case, use
fn Afn refer strategy profiles action profiles, respectively, agents
Famn \ {n}.
Example 3. Suppose 2L landowners along road running north south deciding
whether build factory, residential neighborhood, shopping mall plots.
plots laid along road 2-by-L grid; half agents east side
(e1 , . . . , eL ) half west side (w1 , . . . , wL ). agents payoff depends
builds neighbors north, south, across road build.
example, agent wants build residential neighborhood next factory. agents
payoff matrix indexed actions four agents (fewer ends road)
34 entries, opposed full 32L entries required equivalent normal form
game. (This example due Vickrey & Koller, 2002.)
3.2 Multi-Agent Influence Diagrams
description length extensive-form games also grow exponentially number agents. many situations, large tree represented compactly.
Multi-agent influence diagrams (MAIDs) (Koller & Milch, 2001) allow structured representation games involving time information extending influence diagrams (Howard
& Matheson, 1984) multi-agent case.
MAIDs influence diagrams derive much syntax semantics
Bayesian network framework. MAID compactly represents certain type extensiveform game much way Bayesian network compactly represents joint
probability distribution. thorough treatment Bayesian networks, refer
reader reference Cowell et al. (1999).
3.2.1 MAID Representation
Like Bayesian network, MAID defines directed acyclic graph whose nodes correspond
random variables. random variables partitioned sets: set X chance
variables whose values chosen nature, represented graph ovals;
agent n, set Dn decision variables whose values chosen agent n, represented
rectangles; agent n, set Un utility variables, represented diamonds.
Chance decision variables have, domains, finite sets possible actions.
refer domain random variable V dom(V ). chance decision variable
465

fiBlum, Shelton, & Koller

V , graph defines parent set PaV variables whose values choice V
depend. Utility variables finite sets real payoff values domains,
permitted children graph; represent components agents
payoffs, game state.
game definition supplies chance variable X conditional probability
distribution (CPD) P (X|PaX ), conditioned values parent variables X.
semantics chance variable identical semantics random variable
Bayesian network; CPD specifies probability action dom(X)
selected nature, given actions taken Xs parents. game definition also supplies
utility function utility node U . utility function maps instantiation
pa dom(PaU ) deterministically real value U (pa). notational algorithmic
convenience, regard utility function CPD P (U |PaU ) which,
pa dom(PaU ), value U (pa) probability 1 P (U |pa) values
probability 0 (the domain U simply finite set possible utility values).
end game, agent ns total payoff sum utility received Uni Un
(here index variable). Note component Uni agent ns payoff depends
subset variables MAID; idea compactly decompose payoff
additive pieces.
3.2.2 Strategy Representation
counterpart CPD decision node decision rule. decision rule
decision variable Dni Dn function, specified n, mapping instantiation
pa dom(PaDni ) probability distribution possible actions dom(Dni ).
decision rule identical form conditional probability distribution, refer
using notation P (Dni |PaDni ). semantics chance node, decision
rule specifies probability agent n take particular action dom(Dni ),
seen actions taken Dni parents. assignment decision rules Dni Dn
comprises strategy agent n. agent n chooses strategy, ns behavior Dni
depends actions taken Dni parents. PaDni therefore regarded
set nodes whose values visible n makes choice Dni . Agent ns choice
strategy may well take nodes account; actual game play, nodes
except PaDni invisible n.
Example 4. extensive-form game considered Example 1 represented
MAID shown Figure 2(a). Alice Bob initial decision make without
information previous actions; Alice another decision make
aware Bobs action own. Alice Bob one utility node
(the two condensed single node graph, sake brevity), whose payoff
structure wholly general (dependent every action game) thus whose possible
values exactly values payoff vectors extensive-form game.
Example 5. Figure 2(b) shows complicated MAID somewhat realistic
scenario. Here, three landowners along road deciding whether build store
house. payoff depends happens adjacent along road.
decision proceeds two stages: planning stage building stage. second
466

fiA Continuation Method Nash Equilibria Structured Games


P1

P2

B

P3

E1

E2

C1



C2

B1

AB

C3

B2

R1

L2

(a)

B3

R2

L3

(b)

Figure 2: (a) simple MAID equivalent extensive form game Figure 1. (b)
two-stage road game three agents.

landowner, instance, two decision variables P2 B2 . receives certain
penalty utility node C2 builds opposite planned build.
planning, learns something neighbor left planned.
chance node E1 represents noisy espionage; transmits action taken P1 .
learning value E1 , may second landowners interests deviate
plan, even means incurring penalty. interest start trend
distinguishes previous builders subsequent builders follow: utility
node L2 rewards building opposite built B1 , utility node
R2 rewards third landowner builds thing B3 .
Note MAID exhibits perfect recall, choice made planning stage
visible agent makes next choice building stage.
3.2.3 Payoffs
particular strategy profile is, tuple strategies players
decision nodes CPDs specified. Since chance utility nodes endowed CPDs
already, MAID therefore induces fully-specified Bayesian network B variables
V = X U directed graph MAID. chain rule Bayesian
networks,QB induces joint probability distribution P variables V
P (V) = V V P (V |PaV ), CPDs chance utility variables given MAID
definition CPDs decision variables given . game G represented
MAID, expected payoff agent n receives expectation ns utility
node values respect distribution:
X
Gn () =
EP [Uni ]
Uni Un

=

X

X

Uni Un udom(Uni )

467

u P (u).

fiBlum, Shelton, & Koller

show Section 7 related expectations calculated efficiently
using Bayesian network inference algorithms, giving substantial performance increase
calculation payoffs extensive-form game.
3.2.4 Extensive Form Strategy Representations MAIDs
MAID provides compact definition extensive-form game. note that, although
correspondence MAIDs extensive form games provides intuition
MAIDs, details mapping relevant remainder discussion.
therefore briefly review construction, referring work Koller Milch
(2001) details.
game tree associated MAID full, balanced tree, path corresponding complete assignment chance decision nodes network.
node tree corresponds either chance node decision node one
players, outgoing branch possible action node. nodes
depth tree correspond MAID node. assume nodes
along path tree ordered consistently ordering implied directed
edges MAID, MAID node X parent MAID node , tree
branches X branches . information sets tree nodes associated
decision node Dni correspond assignments parents PaDni : tree nodes
corresponding Dni assignment PaDni single information set.
note that, construction, assignment PaDni determined earlier tree,
partition information sets well-defined. example, simple MAID
Figure 2(a) expands much larger game tree saw earlier Figure 1.
Translating opposite direction, extensive-form games MAIDs,
always natural. game tree unbalanced, cannot simply reverse
process. However, care, possible construct MAID larger
given extensive-form game, may exponentially smaller number agents.
details fairly technical, omit interest brevity.
Despite fact MAID typically much compact equivalent
extensive-form game, strategy representations two turn equivalent
equal size. decision rule decision variable Dni assigns distribution actions
joint assignment PaDni , behavior strategy assigns distribution actions
information set extensive form game discussed above, assignment
parents Dni information set. strategy profile MAID set decision
rules every decision variable therefore equivalent set behavior strategies
every information set, simply behavior profile.
make assumption perfect recall, then, since MAID strategies simply
behavior strategies, represent sequence form. Perfect recall requires
agent forget anything learned course game. MAID
formalism, perfect recall assumption equivalent following constraint: agent
n two decision nodes Dni Dnj , second occurring first,
parents Dni (the information n aware making decision Dni ) Dni must
parents Dnj . implies agent ns final decision node Dnd has, parents, ns
previous decision nodes parents. joint assignment Dnd PaDnd precisely
468

fiA Continuation Method Nash Equilibria Structured Games

determines agent ns sequence information sets actions leading outcome
game agent-n history outcome.
realization probability particular sequence computed multiplying
behavior strategy probabilities actions sequence. MAIDs, sequence corresponds joint assignment Dnd PaDnd , behavior strategy probabilities
sequence entries consistent assignment decision rules agent n.
therefore derive agent ns realization probabilities multiplying
together, conditional probability distributions, decision rules agent ns decision nodes sequence multiplying conditional probability distributions,
entries whose assignments consistent multiplied. Conversely,
given realization plan, derive behavior strategies hence decision rules
according method outlined extensive-form games.
simple MAID example Figure 2(a), terminal sequences
equivalent extensive-form game. road example Figure 2(b), agent 2 8
terminal sequences; one joint assignment final decision node (B2 )
parents (E1 P2 ). associated realization probabilities given multiplying
decision rules P2 B2 .

4. Computational Complexity
developing algorithms compute equilibria efficiently, question naturally arises
well one expect algorithms perform. complexity computing
Nash equilibria studied time. Gilboa Zemel (1989) first showed
NP-hard find one Nash equilibrium normal-form game,
Conitzer Sandholm (2003) recently utilized simpler reduction arrive result
several others vein. recent hardness results pertain restricted
subclasses normal-form games (e.g., Chu & Halpern, 2001; Codenotti & Stefankovic,
2005). However, results apply 2-agent normal-form games. true
proving certain subclass class problems NP-hard also proves entire
class NP-hard (because NP-hardness measure worst-case complexity),
proof might tell us little complexity problems outside subclass.
issue particularly apparent problem computing equilibria, games
grow along two distinct axes: number agents, number actions per agent.
hardness results Conitzer Sandholm (2003) apply number actions
per agent increases. 2-agent normal-form games (fully connected) graphical
games, results apply graphical games.
However, interested hardness graphical games number
agents increases, rather number actions per agent. graphical games
large numbers agents capture structure games
graphical game representation designed. order prove results
asymptotic hardness computing equilibria along interesting (in setting)
axis representation size, require different reduction. proof, like number
previous hardness proofs games (e.g., Chu & Halpern, 2001; Conitzer & Sandholm, 2003;
Codenotti & Stefankovic, 2005), reduces 3SAT equilibrium computation. However,
previous proofs, variables 3SAT instances mapped actions (or sets actions)
469

fiBlum, Shelton, & Koller

game 2 players, whereas reduction mapped agents. Although
differing approach, reduction much spirit reduction appearing
work Conitzer Sandholm (2003), many corollaries main result
also follow (in form adapted graphical games).
Theorem 6. constant 5, k 2, problem deciding whether
graphical game family size k actions per player
one Nash equilibrium NP-hard.
Proof. Deferred Appendix B.
reduction, games one equilibria least one pure
strategy equilibrium. immediately gives us
Corollary 7. NP-hard determine whether graphical game one Nash
equilibrium discretized strategies even coarsest possible granularity.
Finally, graphical games represented (trivial) MAIDs,
agent single parentless decision node single utility node, agents
utility node has, parents, decision nodes graphical game family agent,
obtain following corollary.
Corollary 8. NP-hard determine whether MAID constant family size least
6 one Nash equilibrium.

5. Continuation Methods
Continuation methods form basis algorithms solving structured game representations. begin high-level overview continuation methods,
referring reader work Watson (2000) detailed discussion.
Continuation methods work solving simpler perturbed problem tracing
solution magnitude perturbation decreases, converging solution
original problem. precisely, let scalar parameterizing continuum
perturbed problems. = 0, perturbed problem original one; = 1,
perturbed problem one solution known. Let w represent vector
real values solution. perturbed problem defined , characterize
solutions equation F (w, ) = 0, F real-valued vector function
dimension w (so 0 vector zeros). function F w solution
problem perturbed F (w, ) = 0.
continuation method traces solutions along level set solution pairs (w, )
satisfying F (w, ) = 0. Specifically, solution pair (w, ), would like trace
solution nearby solution. Differential changes w must cancel
F remains equal 0.
(w, ) changes direction unit vector u, F change
direction


F u, F Jacobian F (which also written w F F ).
want find direction u F remains unchanged, i.e., equal 0. Thus, need
solve matrix equation


dw
w F F
=0.
(3)

470

fiA Continuation Method Nash Equilibria Structured Games

Equivalently, changes dw along path must obey w F dw = F d. Rather
inverting matrix w F solving equation, use adjoint adj(w F ),
still defined w F null space rank 1. adjoint matrix cofactors:
element (i, j) (1)i+j times determinant sub-matrix row
column j removed. inverse defined, adj(w F ) = det(w F )[w F ]1 .
practice, therefore set dw = adj(w F ) F = det(w F ). Jacobian
[w F F ] null-space rank 1 everywhere, curve uniquely defined.
function F constructed curve starting = 1 guaranteed
cross = 0, point corresponding value w solution original
problem. continuation method begins known solution = 1 . null-space
Jacobian F current solution (w, ) defines direction, along solution
moved small amount. Jacobian recalculated process repeats,
tracing curve = 0. cost step computation least cubic
size w, due required matrix operations. However, Jacobian may
general much difficult compute. Watson (2000) provides simple examples
continuation methods.

6. Continuation Methods Games
review work GW applying continuation method task finding equilibria games. provide continuation methods normal-form
extensive-form games. algorithms form basis extension structured
games, described next section. continuation methods perturb game giving agents fixed bonuses, scaled , actions, independently whatever
else happens game. bonuses large enough (and unique), dominate
original game structure, agents need consider opponents actions.
thus unique pure-strategy equilibrium easily determined bonuses = 1.
continuation method used follow path space equilibrium
profiles resulting perturbed game, decreasing zero; point,
corresponding strategy profile equilibrium original game.
6.1 Continuation Method Normal-Form Games
make intuition precise, beginning normal-form games.
6.1.1 Perturbations
perturbation vector b vector values chosen random, one action
game. bonus ba given agent n owning action playing a, independently
whatever else happens game. Applying perturbation target game G gives
us new game, denote G b, which, , ,
(G b)n (a, t) = Gn (a, t) + ba . b made sufficiently large, G b unique
equilibrium, agent plays pure strategy ba maximal.
471

fiBlum, Shelton, & Koller

6.1.2 Characterization Equilibria
order apply Equation (3), need characterize equilibria perturbed games
zeros function F . Using structure theorem Kohlberg Mertens (1986), GW
show continuation method path deriving equilibrium characterization
leads convergence perturbation vectors except set measure zero.
present equilibrium characterization here; proofs characterization
methods convergence given Govindan Wilson (2003).
first define auxiliary vector function V G (), indexed actions, payoffs
agent deviating play single action. call V G deviation function.
element VaG () corresponding single action a, owned agent n, payoff
agent n deviates mixed strategy profile playing pure strategy
action a:
X

(4)
VaG () =
Gn (a, t)
tk .
tAn

kN \{n}

also viewed component agent ns payoff derives action a,
strategy profile . Since bonuses given actions independently ,
effect bonuses V G independent . VaG measures payoff deviating
playing a, bonuses given precisely deviation, V Gb () = V G () + b.
also utilize retraction operator R : IRm defined Gul, Pearce, Stachetti (1993), maps arbitrary m-vector w point space mixed
strategies nearest w Euclidean distance. Given operator, equilibrium
characterization follows.
Lemma 9. (Gul et al., 1993) strategy profile G, = R(V G () + ) iff
equilibrium.
Although omit proof, give intuition result true.
Suppose fully-mixed equilibrium; is, every action non-zero probability.
single agent n, VaG () must actions , n
incentive deviate play single one them. Let Vn vector entries
V G () corresponding actions n, let n defined similarly. Vn scalar multiple
1, all-ones vector, simplex n ns mixed strategies defined 1T x = 1,
Vn orthogonal n . V G () therefore orthogonal , retracting + V G () onto
gives precisely . reverse direction, fully-mixed strategy profile satisfying
= R(V G () + ), V G () must orthogonal polytope mixed strategies.
Then, agent, every pure strategy payoff. Therefore, fact
equilibrium. little care must taken dealing actions support.
refer Gul et al. (1993) details.
According Lemma 9, define equilibrium solution equation
= R( + V G ()). hand, = R(w) w IRm ,
equivalent condition w = R(w) + V G (R(w)); equilibrium iff condition
satisfied, easily verified. therefore search point w IRm
satisfies equality, case R(w) guaranteed equilibrium.
form continuation equation

F (w, ) = w R(w) V G (R (w)) + b .
(5)
472

fiA Continuation Method Nash Equilibria Structured Games

V G + b deviation function perturbed game G b, F (w, )
zero R(w) equilibrium G b. = 0 game unperturbed,
F (w, 0) = 0 iff R(w) equilibrium G.
6.1.3 Computation
expensive step continuation method calculation Jacobian w F ,
required computation maintains constraint Equation (3). Here,
w F = (I + V G )R, identity matrix. hard part
calculation V G . pure strategies a0 An0 , n0 6= n, value
location (a, a0 ) V G () equal expected payoff agent n plays
pure strategy a, agent n0 plays pure strategy a0 , agents act according
strategy profile :

X
Gn (a, t)
tk
a0
tAn
kN \{n}
X

0
=
Gn (a, , t)
tk .

G
Va,a
0 () =

tAn,n0

(6)

kN \{n,n0 }

G () = 0.
a0 , Va,a
0

Computing Equation
(6) requires large number multiplications; sum
Q
space An,n0 = kN \{n,n0 } Ai , exponentially large number agents.
6.2 Continuation Method Extensive-Form Games
method applies extensive-form games, using sequence form strategy representation.
6.2.1 Perturbations
normal-form games, game perturbed bonus vector b. Agent n owning
sequence h paid additional bonus bh playing h, independently whatever else
happens game. Applying perturbation gives us new game G b which,
z Z, (G b)n (z) = Gn (z) + bHn (z) .
bonuses large enough unique, GW show perturbed
game unique pure-strategy equilibrium (one realization probabilities
0 1). However, calculating simple case normal-form games.
Behavior strategies must calculated leaves upward recursive procedure,
step agent owns node question chooses action results
sequence largest bonus. Since actions recursively
determined, action node question determines outcome. realization
plans derived behavior profile method outlined Section 2.2.1.
473

fiBlum, Shelton, & Koller

6.2.2 Characterization Equilibria
more, first define vector function capturing benefit deviating given
strategy profile, indexed sequences:
X

k (z),
(7)
VhG () =
Gn (z)
zZh

kN \{n}

Zh set leaves consistent sequence h. interpretation
V G natural case normal-form games, possible agent
play one sequence exclusion others; possible actions partially
determined actions agents. case, VhG () regarded
portion payoff agent n receives playing sequence h, unscaled agent ns
probability playing sequence. normal-form games, vector bonuses
added directly V G , V Gb = V G + b.
retraction operator R realization plans defined way normalform strategies: takes general vector projects onto nearest point valid
region realization plans. constraints defining space linear, discussed
Section 2.2.1 . therefore express constraint matrix C C = 0
valid profiles . addition, probabilities must greater equal zero.
calculate w, must find minimizing (w )T (w ), (squared) Euclidean distance
w , subject C = 0 0. quadratic program (QP),
solved efficiently using standard methods. Jacobian retraction easily
computable set active constraints.
equilibrium characterization realization plans surprisingly similar
mixed strategies normal-form games; GW show that, before, equilibria
characterized = R( + V G ()), R retraction sequence form
V G deviation function. continuation equation F takes exactly form
well.
6.2.3 Computation
key property reduced sequence-form strategy representation deviation function multi-linear function extensive-form parameters, shown
Equation (7). elements Jacobian V G thus also general structure. particular, element corresponding sequence h agent n sequence h0
agent n0

X
Gn (z)
k (z)
h0
zZh
kN \{n}
X

=
Gn (z)
k (z)

G
Vh,h
0 () =

zZh,h0

(8)

kN \{n,n0 }

Zh,h0 set leaves consistent sequences h (for agent n)
h0 (for agent n0 ). Zh,h0 empty set (and hence V G = 0) h h0 incompatible.
Equation (8) precisely analogous Equation (6) normal-form games. sum
outcomes utility outcome multiplied strategy probabilities
474

fiA Continuation Method Nash Equilibria Structured Games


3
2


1

Figure 3: abstract diagram path. horizontal axis represents vertical
axis represents space strategy profiles (actually multidimensional).
algorithm starts right = 1 follows dynamical system
= 0 point 1, found equilibrium original game.
continue trace path find equilibria labeled 2 3.

agents. Note sum leaves tree, may exponentially
numerous number agents.
One additional subtlety, must addressed method equilibrium computation extensive-form games, relates zero-probability actions. actions induce
probability zero entire trajectories tree, possibly leading equilibria based
unrealizable threats. Additionally, information sets occur zero probability,
agents behave arbitrarily without disturbing equilibrium criterion, resulting continuum equilibria possible bifurcation continuation path. prevents
methods converging. therefore constrain realization probabilities greater
equal small > 0. is, fact, requirement GWs equilibrium
characterization hold. algorithm thus looks -perfect equilibrium (Fudenberg
& Tirole, 1991): strategy profile component constrained ,
agents strategy best response among satisfying constraint. Note
entirely different -equilibrium. -perfect equilibrium always exists,
long large make set legal strategies empty. -perfect equilibrium interpreted equilibrium perturbed game agents small
probability choosing unintended action. limit -perfect equilibria approaches
0 perfect equilibrium (Fudenberg & Tirole, 1991): refinement basic notion
Nash equilibrium. approaches 0, equilibria found GWs algorithm therefore
converge exact perfect equilibrium, continuity variation continuation
method path. small enough, perfect equilibrium vicinity
found -perfect equilibrium, easily found local search.
475

fiBlum, Shelton, & Koller

6.3 Path Properties
case normal-form games, GW show, using structure theorem Kohlberg
Mertens (1986), path algorithm one-manifold without boundary
probability one choices b. provide analogous structure theorem
guarantees property extensive-form games. Figure 3(a) shows abstract
representation path followed continuation method. GW show path
must cross = 0 hyperplane least once, yielding equilibrium. fact, path
may cross multiple times, yielding many equilibria single run. path must
eventually continue = side, find odd number equilibria run
completion.
normal-form extensive-form games, path piece-wise polynomial,
piece corresponding different support set strategy profile. pieces
called support cells. path smooth cell boundaries due discontinuities
Jacobian retraction operator, hence w F , support changes. Care
must taken step boundaries exactly following path; point,
Jacobian new support calculated path traced
new support cell.
case two agents, path piece-wise linear and, rather taking steps,
algorithm jump corner corner along path. algorithm applied
two-agent game particular bonus vector used (in single entry nonzero), steps support cell support cell algorithm takes identical
pivots Lemke-Howson algorithm (Lemke & Howson, 1964) two-agent generalsum games, two algorithms find precisely set solutions (Govindan &
Wilson, 2002). Thus, continuation method strict generalization LemkeHowson algorithm allows different perturbation rays games two
agents.
process described detail pseudo-code algorithm, presented
Figure 4.
6.4 Computational Issues
Guarantees convergence apply long stay path defined dynamical system continuation method. However, computational purposes, discrete
steps must taken. result, error inevitably accumulates path traced,
F becomes slightly non-zero. GW use several simple techniques combat problem.
adopt techniques, introduce one own: employ adaptive step
size, taking smaller steps error accumulates quickly larger ones not.
F nearly linear (as is, example, actions support
current strategy profile), technique speeds computation significantly.
GW use two different techniques remove error accumulated. Suppose
point (w, ) wish minimize magnitude F (w, ) = w V G (R(w)) +
b+R(w). two values might change: w, b. change first without
affecting guarantee convergence, every steps run local Newton method
search w minimizing |F (w, )|. search decrease error sufficiently,
perform GW call wobble: change perturbation vector (wobble
476

fiA Continuation Method Nash Equilibria Structured Games

continuation path) make current solution consistent. set b = [w V G (R(w))
R(w)]/, equilibrium characterization equation immediately satisfied. Changing
perturbation vector invalidates theoretical guarantees convergence. However,
nonetheless attractive option immediately reduces error zero.
local Newton method wobbles described detail Govindan
Wilson (2003).
techniques potentially send algorithm cycle, practice
occasionally do. However, necessary keeping algorithm path.
algorithm cycles, random restarts decrease step size improve convergence.
sophisticated path-following algorithms might also used, general could improve
success rate execution time algorithm.
6.5 Iterated Polymatrix Approximation
perturbed games may large number equilibria, path
may wind back forth number them, continuation algorithm
take trace way back solution original game. speed
algorithm using initialization procedure based iterated polymatrix approximation
(IPA) algorithm GW. polymatrix game normal-form game payoffs
agent n equal sum payoffs set two-agent games, involving
n another agent. polymatrix games linear combination two-agent
normal-form games, reduce linear complementarity problem solved
quickly using Lemke-Howson algorithm (Lemke & Howson, 1964).
agent n N polymatrix game, payoff array matrix B n indexed
n
actions agent n agent; actions a0 An0 , Ba,a
0
0
0
0
payoff n receives playing game agent n , n plays . Agent
ns
payoffs receives games agent,
P total
Ppayoff sum
n
n0 6=n
aAn ,a0 An0 a0 Ba,a0 . Given normal-form game G strategy profile ,
construct polymatrix game P whose payoff function Jacobian
Gs setting
G
n
(9)
Ba,a
0 = Va,a0 () .
game P linearization G around : Jacobian everywhere. GW
show equilibrium G equilibrium P . follows
equation V G () = V G () /(|N | 1), holds . see
holds, consider single element indexed :
X
X
X

(V G () )a =
a0
Gn (a, a0 , t)
tk
n0 N \{n} a0 An0

=

X

X

n0 N \{n}

tAn

kN \{n,n0 }

tAn,n0

Gn (a, t)



tk

kN \{n}

G

= (|N | 1)V ()a .
equilibrium characterization equation therefore written

= R + V G () (|N | 1) .
477

fiBlum, Shelton, & Koller

G P value V , thus equilibrium characterization
function. satisfies one satisfies other.
define mapping p : p() equilibrium P (specifically,
first equilibrium found Lemke-Howson algorithm). p() = ,
equilibrium G. IPA procedure Govindan Wilson (2004) aims find
fixed point. begins randomly chosen strategy profile , calculates p()
running Lemke-Howson algorithm; adjusts toward p() using approximate
derivative estimate p built past two iterations. p() sufficiently
close, terminates approximate equilibrium.
IPA guaranteed converge. However, practice, quickly moves near
good solution. possible point calculate perturbed game close
original game (essentially, one differs amount Gs polymatrix
approximation differs G) found approximate equilibrium fact
exact equilibrium. continuation method run starting point
find exact equilibrium original game. continuation method guaranteed
converge starting point. However, practice always found
converge, long IPA configured search high quality equilibrium approximations.
Although theoretical results required quality, IPA refine starting
point continuation method fails. results show IPA quick-start
substantially reduces overall running time algorithm.
fact use approximate algorithm quick-start ours, also
without guarantees convergence. Given approximate equilibrium , inverse
image R defined set linear constraints. let w := V G () + ,
use standard QP methods retract w nearest point w0 satisfying
constraints, let b := w0 w. = R(w0 ) = R(V G () + + b),
continuation method path. Alternatively, choose b wobbling, case
set b := [w V G (R(w)) R(w)]/.

7. Exploiting Structure
algorithms continuation method foundation game representation,
calculation V G Step 2(b)i pseudo-code Figure 4 different
consumes time. normal-form (in worst case) extensiveform games, requires exponential time number agents. However, show
section, using structured representation graphical game MAID,
effectively exploit structure game drastically reduce computational
time required.
7.1 Graphical Games
Since graphical game also normal-form game, definition deviation function
V G Equation (4) same: VaG () payoff agent n deviating
play deterministically. However, due structure graphical game, choice
strategy agent outside family n affect agent n0 payoff.
observation allows us compute payoff locally.
478

fiA Continuation Method Nash Equilibria Structured Games

input game G:
1. Set = 1, choose initial b either quick-start procedure (e.g., IPA) randomizing. Set
w = V G () + b + .
2. greater (negative) threshold (i.e., still good chance picking
another equilibrium):
(a) Initialize current support cell: set steps counter number steps take
crossing cell, depending current amount error. F linear nearly linear (if,
example, strategy profile nearly pure, 2 agents), set steps = 1
cross entire cell.
(b) steps 1:
i. Compute V G ().
ii. Set w F (w, ) = (V G () + I)R(w) (we already know F = b). Set dw =
adj(w F ) b = det(w F ). satisfy Equation (3).
iii. Set equal distance wed go direction dw reach next support
boundary. scale dw /steps.
iv. change signs course step, record equilibrium point
0.
v. Set w := w + dw(/steps) := + d(/steps).
vi. sufficient error accumulated, use local Newton method find w minimizing
|F (w, )|. reduce error enough, increase steps, thereby decreasing step
size. already increased steps, perform wobble reassign b.
vii. Set steps := steps 1.

Figure 4: Pseudo-code cont algorithm.
7.1.1 Jacobian Graphical Games
begin definition V G normal-form games (modified slightly account
local payoff arrays). Recall Afn set action profiles agents Famn
n, let AFamn set action profiles agents Famn .
divide sum full action profiles two sets, switching
normal-form version Gn graphical game version Gn , follows:
X

VaG () =
Gn (a, t)
tk
tAn

=

X
uAfn

kN \{n}

Gn (a, u)



uk

X



v j .

(10)

kFamn \{n} vAFamn jN \Famn

Note latter sum product simply sum probability distribution, hence
always equal 1 due constraints . thus eliminated without
changing value V G takes valid strategy profiles. However, partial derivatives
respect strategies agents Famn non-zero, enter computation V G .
Suppose wish compute row Jacobian matrix corresponding action
agent n. must compute entries action a0 agent n0 N . trivial
G = 0, since appear anywhere expression
case n0 = n Va,a
0

VaG (). next compute entries action a0 agent n0 Famn .
479

fiBlum, Shelton, & Koller

case,
G
Va,a
0 () =

X


Gn (a, u)
a0
f

=

Gn (a, u)

uAfn

=

X


a0





v j

(11)

uk 1

kFamn \{n}



Gn (a, a0 , t)

tAfn,n0

X

vAFamn jN \Famn

kFamn \{n}

uAn

X

uk

kFamn

n0 Famn .

tk ,

(12)

\{n,n0 }

next compute entry single action a0 agent n0
/ Famn . derivative
Equation (11) takes different form case; variable question second
summation, first,
X

X


G
Va,a
v j
Gn (a, u)
uk
0 () =
a0
f
=

X

Gn (a, u)

=

X



uk

Gn (a, u)

uAfn



X

vAFamn

kFamn \{n}

uAfn

vAFamn jN \Famn

kFamn \{n}

uAn

uk 1,


a0



v j

jN \Famn

n0 6 Famn .

(13)

kFamn \{n}

Notice calculation depend a0 ; therefore, action
agent Famn . need compute elements row.
copy value columns actions belonging agents Famn .
7.1.2 Computational Complexity
Due graphical game structure, computation V G () takes time exponential
maximal family size game, hence takes time polynomial number
agents family size constant. particular, methods lead following
theorem complexity continuation method graphical games.
Theorem 10. time complexity computing Jacobian deviation function
V G () graphical game O(f df |N | + d2 |N |2 ), f maximal family size
maximal number actions per agent.
Proof. Consider single row Jacobian, corresponding single action owned
single agent n. d(f 1) entries row actions owned
G
members Famn . one action a0 , computation Jacobian element Va,a
0
f
2
according Equation (12) takes time O(d ). total cost entries therefore
O((f 1)df 1 ). d(|N | f ) entries actions owned non-familyG a0 same. calculated time
members. value Va,a
0
f
1
O(d ), copied across row time d(|N | f ). all, computational cost
row O(f df 1 + d|N |). d|N | rows, total computational
cost O(|N |f df + d2 |N |2 ).
480

fiA Continuation Method Nash Equilibria Structured Games

P1

P2

P3

B1

B2

B3


B

(a)

(b)

Figure 5: strategic relevance graphs MAIDs (a) Figure 2(a) (b) Figure 2(b).

iteration algorithm calculates V G () once; therefore proved
single iteration takes time polynomial |N | f constant (in fact, matrix operations make
complexity cubic |N |). However, normal-form games, theoretical
results many steps continuation method required convergence.
7.2 MAIDs
graphical games, exploitation structure straightforward. turn
difficult problem exploiting structure MAIDs. take advantage two
distinct sets structural properties. first, coarse-grained structural measure known
strategic relevance (Koller & Milch, 2001), used previous computational
methods. decomposing MAID according strategic relevance relations,
exploit finer-grained structure using extensive-form continuation method GW
solve components equivalent extensive-form game. next two sections,
describe two kinds structure.
7.2.1 Strategic Relevance
Intuitively, decision node Dni strategically relevant another decision node Dnj 0 agent
n0 , order optimize decision rule Dnj 0 , needs know agent ns decision rule
Dni . relevance relation induces directed graph known relevance graph,
decision nodes appear edge node Dnj 0 node Dni present iff Dni
strategically relevant Dnj 0 . event relevance graph acyclic, decision
rules optimized sequentially reverse topological order; children
node Dni decision rules set, decision rule Dni optimized
without regard nodes.
cycles exist relevance graph, however, steps must taken. Within
strongly connected component (SCC), set nodes directed path
two nodes exists relevance graph, decision rules cannot optimized sequentially
linear ordering nodes SCC, node must optimized one
481

fiBlum, Shelton, & Koller

children, impossible. Koller Milch (2001) show MAID
decomposed SCCs, solved individually.
example, relevance graph MAID Figure 2(a), shown Figure 5(a),
one SCC consisting B, another consisting A0 . MAID, would
first optimize decision rule A0 , optimal decision rule A0 rely
decision rules B makes decision A0 , Alice already knows
actions taken B, need know decision rules led them.
would turn A0 chance node CPD specified optimized decision
rule optimize decision rules A0 B. relevance graph Figure 2(b),
shown Figure 5(b), forms single strongly connected component.
computational method Koller Milch (2001) stops strategic relevance:
SCC converted equivalent extensive-form game solved using standard
methods. algorithm viewed augmentation method: MAID
decomposed SCCs, solve SCCs using methods, taking
advantage finer-grained MAID structure within find equilibria efficiently.
MAIDs test algorithms (including road MAID Figure 2b)
strongly connected relevance graphs, cannot decomposed (see Figure 5b
Figure 10).
7.2.2 Jacobian MAIDs
MAID equivalent extensive-form game, deviation function V G
one defined Equation (8). Now, however, compute payoffs make
Jacobian V G efficiently. Consider payoff Gn (z) agent n outcome z.
outcome z simply assignment x variables MAID. realization
probability n (z) product
Q probabilities decisions agent n
assignment x, product kN k (z) realization probabilities simply joint
probabilityPof assignment.
expected payoff agent n receive strategy
Q
profile , zZ Gn (z) kN k (z), therefore expectation Gn (z). expectation
respect distribution P defined Bayesian network B whose structure
MAID, decision node CPDs determined .
entries V G strictly expected payoffs, however. Equation (8)
rewritten
Q
X Gn (z)
kN k (z)
G
Vh,h0 () =
.
(14)
n (z)n0 (z)
zZh,h0

expectation quantity Gn (z)/[n (z)n0 (z)]. payoff Gn (z) sum agent
ns utility nodes. Due linearity expectation, perform computation separately
agent ns utility nodes, simply add separate contributions.
therefore restrict attention computing contribution single utility
node Un agent n. Furthermore, value n (z) depends values
set nodes n consisting ns decision nodes parents. Thus, instead
computing probabilities assignments variables, need compute
marginal joint distribution Un , n , n0 . distribution,
compute contribution Un expectation Equation (14) every pair terminal
sequences belonging agents n n0 .
482

fiA Continuation Method Nash Equilibria Structured Games

P1

P2

P3

E1

E2

C1

C2

B1

C3

B2

R1

L2

P1 E1
B1 B2

E1 P2
B1 B2

P2 E2
B2 B3

E2 P3
B2 B3

B3

R2

L3

(a)

(b)

Figure 6: (a) two-stage road MAID three agents shown divided cliques.
four cliques surrounded dashed line, three decision nodes
chance node. (b) resultant clique tree.

7.2.3 Using Bayesian Network Inference
analysis reduces required computations significantly. Rather computing
separate expectation every pair sequences h, h0 , might first seemed
necessary, need compute one marginal joint distribution variables {Un }
n n0 every pair agents n, n0 . marginal joint distribution one defined
Bayesian network B . Naively, computation requires execute Bayesian
network inference |N |2 times: ordered pair agents n, n0 . fact,
exploit structure MAID perform computation much efficiently.
basis method standard clique tree algorithm Lauritzen Spiegelhalter
(1998). clique tree algorithm fairly complex, detailed presentation outside
scope paper. choose treat algorithm black box, describing
properties relevant understanding used within
computation. note details suffice allow method implemented
using one many off-the-shelf implementations clique tree algorithm. reader
wishing understand clique tree algorithm derivation detail referred
reference Cowell et al. (1999) complete description.
clique tree Bayesian network B data structure defined undirected
tree set nodes C. node Ci C corresponds subset variables
B, typically called clique. clique tree satisfies certain important properties.
must family preserving: node X B, exists clique Ci C
(X PaX ) Ci . also satisfies separation requirement: C2 lies unique path
C1 C3 , then, joint distribution defined B, variables C1 must
conditionally independent C3 given C2 .
division 3-agent road MAID cliques shown Figure 7.2.3(a).
MAID 4 cliques. Notice every family contained clique (including families
chance nodes utility nodes). clique tree MAID shown Figure 7.2.3(b).
483

fiBlum, Shelton, & Koller

clique maintains data structure called potential, table entry
joint assignment variables clique. table sort generally called
factor. Inference algorithms typically use two basic operations factors: factor product,
factor marginalization. F G two factors (possibly overlapping) sets
variables X , respectively, define product FG new factor
X . entry FG particular assignment variables X
product entries F G corresponding restriction assignment X
, respectively. notion multiplication corresponds way conditional
probability distributions multiplied. also marginalize, sum, variable X
factor F X way
Pwe would sum variable joint
probability distribution. result factor XP
F variables X\{X}.
entry particular assignment variables X F equal sum entries
F compatible assignment one value X.
factor entry every joint assignment variables, size
potential Ci exponential |Ci |. clique tree inference algorithm proceeds
passing messages, factors, one clique another tree. messages
used update potential receiving clique factor multiplication.
process messages sent directions edge tree,
tree said calibrated ; point, potential every clique Ci contains precisely
joint distribution variables Ci according B (for details, refer
reference Cowell et al., 1999).
use clique tree algorithm perform inference B . Consider final
decision node agent n. Due perfect recall assumption, ns previous decisions
parents also parents decision node. family preservation
property therefore implies n fully contained clique. also implies
family utility node contained clique. expectation Equation (14)
thus requires computation joint distribution three cliques tree: one
containing PaUn , one containing n , one containing n0 . need compute
joint distribution every pair agents n, n0 .
first key insight reduce problem one computing joint marginal distribution pairs cliques tree. Assume computed PB (Ci , Cj )
every pair cliques Ci , Cj . Now, consider triple cliques Ci , Cj , Ck . two
cases: either one cliques path two, not. first
case, assume without loss generality Cj path Ci Ck . case,
separation requirement, PB (Ci , Cj , Ck ) = PB (Ci , Cj )PB (Cj , Ck )/PB (Cj ).
second case, exists unique clique C lies path pair
cliques. Again, separation property, C renders cliques conditionally
independent, compute
PB (Ci , Cj , Ck ) =

X PB (Ci , C )PB (Cj , C )PB (Ck , C )
PB (C )2

C

.

(15)

Thus, reduced problem one computing marginals pairs
cliques calibrated clique-tree. use dynamic programming execute process
efficiently. construct table contains PB (Ci , Cj ) pair cliques Ci , Cj .
construct table order length path Ci Cj . base case Ci
484

fiA Continuation Method Nash Equilibria Structured Games

Cj adjacent tree. case, PB (Ci , Cj ) = PB (Ci )PB (Cj )/PB (Ci
Cj ). probability expressions numerator simply clique potentials
calibrated tree. denominator obtained marginalizing either two
cliques. fact, expression computed byproduct calibration process,
marginalization required. cliques Ci Cj adjacent, let Ck
node adjacent Cj path Ci Cj . clique Ck one step closer
Ci , so, construction, already computed P (Ci , Ck ). apply
separation property again:

PB (Ci , Cj ) =

X PB (Ci , Ck )PB (Ck , Cj )
PB (Ck )

Ck

.

(16)

7.2.4 Computational Complexity
Theorem 11. computation V G () performed time O(`2 d3 + u|N |d4 ),
` number cliques clique tree G, size largest clique
(the number entries potential), |N | number agents, u total
number utility nodes game.
Proof. cost calibrating clique tree B O(`d). cost computing
Equation (16) single pair cliques O(d3 ), must compute factor
variables three cliques summing out. must perform computation O(`2 )
times, pair cliques, total cost O(`2 d3 ). compute marginal
joint probabilities triples cliques PaUni , n , n0 every utility node Uni every
agent n0 n. u(|N | 1) triples. Computing factor
variables three cliques may first require computing factor variables four
cliques, cost O(d4 ). Given factor, computing expected value utility
node takes time O(d3 ), affect asymptotic running time. total cost
computing marginal joint probabilities expected utilities therefore O(u|N |d4 ),
total cost computing V G () O(`2 d3 + u|N |d4 ).

method, shown single iteration continuation method
accomplished time exponential induced width graph number
variables largest clique clique tree. induced width optimal clique
tree one smallest maximal clique called treewidth network.
Although finding optimal clique tree is, itself, NP-hard problem, good heuristic
algorithms known (Cowell et al., 1999). games interactions agents
highly structured (the road MAID, example), size largest clique
constant even number agents grows. case, complexity computing
Jacobian grows quadratically number cliques, hence also number
agents. Note matrix adjoint operation takes time cubic m, least
|N |, single step along path actually cubic computational cost.
485

fiBlum, Shelton, & Koller

1800

400
cont
IPA+cont
VK

1600
1400

300

1200

250
seconds

seconds

cont
IPA+cont
VK

350

1000
800

200
150

600

100

400

50

200
0
0

20

40
60
# agents

80

0

100

10

15

(a)

20

25
30
# agents

35

40

45

(b)

4

7

x 10

0.01
Cumulative

6

0.009

Terminating run

0.008

5

0.007
seconds/iteration

# iterations

cont
cubic fit

4
3

0.006
0.005
0.004
0.003

2

0.002
1
0.001
0

10

15

20
# agents

0
5

25

(c)

10

15
20
# agents

25

(d)

Figure 7: Results 2-by-L road game rock-paper-scissors payoffs: (a) running time.
Results road game random payoffs: (b) running time; (c) number
iterations cont; (d) average time per iteration cont.

8. Results
performed run-time tests algorithms wide variety graphical games
MAIDs. Tests performed Intel Xeon processor running 3 GHz 2
GB RAM, although memory never taxed calculations.
8.1 Graphical Games
graphical games, compared two versions algorithm: cont, simple continuation method, IPA+cont, continuation method IPA initialization. tested
hybrid equilibrium refinement algorithm Vickrey Koller (2002) (VK hereafter)
486

fiA Continuation Method Nash Equilibria Structured Games

4

600

5
cont
IPA+cont
VK

500

Cumulative

4.5

Terminating run

4
3.5
# iterations

400
seconds

x 10

300

200

3
2.5
2
1.5
1

100

0.5
0

5

10

15

20

25
30
# agents

35

40

0

45

5

10

15

(a)

20
25
# agents

30

35

40

(b)
4

250

6

x 10

cont

Cumulative

IPA+cont

5

200

Terminating run

VK

# iterations

seconds

4
150

100

3

2
50

0

1

5

10

15

20
# agents

25

30

35

0

(c)

10

15

20
# agents

25

(d)

Figure 8: Results ring game random payoffs: (a) running time; (b) number
iterations cont. Results L-by-L grid game random payoffs: (c) running
time; (d) number iterations cont.

comparison, parameters used. VK algorithm returns
-equilibria; exact methods exist comparable own.
algorithms run two classes games defined Vickrey Koller (2002)
two additional classes. road game Example 3, denoting situation
agents must build land plots along road, played 2-by-L grid; agent three
actions, payoffs depend actions (grid) neighbors. Following VK,
ran algorithm road games additive rock-paper-scissors payoffs: agents
payoffs sum payoffs independent rock-paper-scissors games
neighbors. game is, fact, polymatrix game, hence easy solve using
methods. order test algorithms typical examples, experimented
487

fiBlum, Shelton, & Koller

road games entries payoff matrix agent chosen uniformly
random [0, 1]. also experimented ring graph three actions per
agent random payoffs. Finally, order test games increasing treewidth,
experimented grid games random payoffs. defined manner
road games, except game graph L-by-L grid.
class games, chose set game sizes run on. each, selected
(randomly cases payoffs random) set 20 test games solve.
solved game using cont, IPA+cont, VK. cont, started different
random perturbation vector time recorded time number iterations
necessary reach first equilibrium. IPA+cont, started different initial
strategy profile IPA time recorded total time IPA cont reach
first equilibrium.
equilibria found algorithm error 1012 , essentially machine
precision. hybrid refinement algorithm VK found -equilibria average error
104 road games rock-paper-scissors payoffs, 0.01 road games grid
games random payoffs, 0.03 ring games random payoffs, although
equilibria error high 0.05 road games 0.1 ring games.
smaller games, algorithms always converged equilibrium. larger
games, cont IPA detected entered cycle terminated without finding
equilibrium. maintaining hash table support cells passed
already, cont IPA able detect entered support cell
second time. Although sure sign entered cycle, strong
indicator. potential cycles detected, algorithms restarted new
random initialization values. Note cycles execution cont never arise
algorithm stray path dictated theory GW, random
restarts reflect failure follow path accurately.
equilibrium eventually found, cumulative time random
restarts recorded. error bars running time graphs show variance due
number random restarts required, choices initialization values, and, random
games, choice game.
Random restarts required 29% games tested. average, 2.2 restarts
necessary games. Note figure skewed larger games,
occasionally required many restarts; largest games sometimes required 8 9 restarts.
large graphical games (10 random road games 8 random ring games), IPA
converge 10 restarts; cases record results IPA+cont. cont
always found equilibrium within 10 restarts. results shown Figures 7(a,b,c,d)
Figures 8(a,b,c).
random roads, also plotted number iterations time per iteration
cont Figures 7(c,d). number iterations varies based game
perturbation vector chosen. However, time per iteration almost exactly cubic,
predicted. note that, IPA used quick-start, cont invariably converged
immediately (within second) time spent IPA algorithm.
road games, methods efficient smaller games, become costly. Due polymatrix nature rock-paper-scissors road games,
IPA+cont algorithm solves immediately Lemke-Howson algorithm,
488

fiA Continuation Method Nash Equilibria Structured Games

therefore significantly less expensive VK. random ring games, algorithms
efficient VK smaller games (up 2030 agents), IPA+cont performing considerably better cont. However, road games, running time
algorithms grows rapidly VK, larger games, become
impractical. Nevertheless, algorithms performed well games 45 agents
3 actions per agent, previously intractable exact algorithms.
L-by-L grid games, algorithm performed much better VK algorithm (see Figures 8(c,d)), without IPA quick-start. reflects fact running-time
complexity algorithms depend treewidth graph.

# equilibria

80
60
40
20
0
20
10

15

8
10

6
5

# players

4
2
# runs

Figure 9: number unique equilibria found function size game
number runs algorithm, averaged ten random ring games.

also examined number equilibria found IPA+cont algorithm. ran
IPA+cont ring graphical game differing numbers agents. number
agents, fixed 10 random games, ran algorithm 10 times game, recorded
cumulative number unique equilibria found. average number equilibria found
10 games number agents plotted figure 9. small games (with
presumably small number equilibria), number equilibria found quickly saturated.
large games, almost linear increase number equilibria found
subsequent random restart, implying run algorithm produced new
set solutions.
8.2 MAIDs
previous computational method MAIDs (Koller & Milch, 2001) stopped strategic
relevance: SCC converted equivalent extensive-form game solved using
standard methods. algorithm takes advantage structure game already decomposed according strategic relevance. test cases therefore
selected relevance graphs consisting single strongly connected component.
489

fiBlum, Shelton, & Koller



NA

B

AB

NB

C

BC
(a)



B

C

(b)

Figure 10: (a) chain game (b) strategic relevance graph case three
agents (A, B, C).

order ascertain much difference enhancements made, compared
results MAID algorithm, MAID cont, achieved converting game
extensive-form running EF cont, extensive-form version cont specified
GW, Gambit (McKelvey, McLennan, & Turocy, 2004), standard game theory software
package. time required conversion extensive form included results.
ran algorithms two classes games, varying sizes. first,
refer chain game, alternates decision chance nodes (see Figure 10).
decision node belongs different agent. agent two utility nodes,
connected decision node neighbors (except end agents,
one utility node single neighbor). three actions decision node.
probability tables payoff matrices chosen uniformly random. second
class two-stage road building game Example 5, shown Figure 2(b).
class, chose payoffs carefully, hand, ensure non-trivial mixed strategy equilibria.
ran chain games sizes 2 21, road games sizes
2 9. size, randomly selected 20 perturbation vectors 20 games (all
20 road games same, since payoffs set hand, 20 chain games
payoffs randomly assigned). tested algorithms games, initialized
perturbation vectors, averaged across test cases. timing results appear
Figures 11(a,b). error bars reflect variance due choice game (in chain
games), choice perturbation vector, number random restarts required.
cases, graphical game tests, MAID cont failed find equilibrium, terminating early detected entered cycle. cases,
restarted new perturbation vector successfully terminated.
equilibrium eventually found, cumulative time random restarts
recorded. course test runs, two chain games required random restart.
size 7. algorithms failed frequently road games; spike
road games size 8 reflects fact games size required, average, 1.2
490

fiA Continuation Method Nash Equilibria Structured Games

200

900
cont
EF cont
gambit

180

800

160

cont
EF cont
gambit

700

140

600
seconds

seconds

120
100

500
400

80
300

60

200

40

100

20
0
2

4

6

8

10

12
14
# agents

16

18

20

0
2

22

3

4

5

(a)

6
# agents

7

8

9

7

8

9

(b)

1000

0.4
cont
cubic fit

900

0.35

800
0.3
seconds/iteration

# iterations

700
600
500
400

0.25
0.2
0.15

300
0.1
200
0.05

100
0
2

3

4

5

6
# agents

7

8

0
2

9

(c)

3

4

5

6
# agents

(d)

Figure 11: Results MAIDs: (a) Running times chain MAID. Results two-stage
road MAID: (b) running time; (c) number iterations; (d) time per iteration.

random restarts equilibrium found. Strangely, MAID cont much
successful road game size 9, succeeding without random restarts two
cases.
tested Gambit EF cont smaller games, time memory
requirements testing larger ones beyond means. results show that,
EF cont faster algorithm Gambit extensive-form games, inadequate
larger MAIDs able solve MAID cont. surprising;
road game size 9 26 decision chance nodes, equivalent extensive-form game
tree 226 67 million outcome nodes. MAIDs size, Bayesian network
inference techniques used become necessary.
491

fiBlum, Shelton, & Koller

MAIDs, realization probabilities constrained least 104 (i.e.,
found -perfect equilibria = 104 ). accuracy equilibria within 1012 ,
machine precision.
graphical games, recorded number iterations convergence well
time per iteration MAID cont. results appear Figures 11(c,d). time
per iteration fit well cubic curve, accordance theoretical predictions.
variance primarily due execution retraction operator, whose running time
depends number strategies support.

9. Discussion Conclusions
described two adaptations continuation method algorithms GW,
purpose accelerated execution structured games. results show
algorithms represent significant advances state art equilibrium computation
graphical games MAIDs.
9.1 Related Work Graphical Games
last years, several papers addressed issue finding equilibria structured games. graphical games, exact algorithms proposed far apply games
interaction structure undirected tree, agent two
possible actions. Kearns et al. (2001) provide exponential-time algorithm compute
exact equilibria game, Littman et al. (2002) provide polynomial-time
algorithm compute single exact equilibrium. limited set games,
algorithms may preferable own, since come running-time guarantees.
However, yet tested whether algorithms are, fact, efficient practice. Moreover, methods applicable fully general games, results indicate
perform well.
effort focused computation -equilibria general graphical
games. number algorithms recently proposed task.
use discretized space mixed strategies: probabilities must selected grid
simplex, made arbitrarily fine. computational reasons, however,
grid must typically quite coarse, number grid points consider grows
exponentially number actions per agent. methods (implicitly
explicitly) define equilibrium set constraints discretized strategy space,
use constraint solving method: Kearns et al. (2001) use tree-propagation
algorithm (KLS); Vickrey Koller (2002) use standard CSP variable elimination methods
(VK1); Ortiz Kearns (2003) use arc-consistency constraint propagation followed
search (OK). Vickrey Koller (2002) also propose gradient ascent algorithm (VK2),
provide hybrid refinement method can, computation, reduce
equilibrium error.
exact methods, KLS algorithm restricted tree-structured games,
comes without experimental running time results (although guaranteed run
polynomial time). Kearns et al. (2001) give suggestion working non-tree graph
constructing junction tree passing messages therein. However, necessary
computations clear potentially expensive.
492

fiA Continuation Method Nash Equilibria Structured Games

VK1 algorithm applicable graphical games arbitrary topology,
number actions per agent. takes time exponential treewidth graph.
treewidth constant, scales linearly number agents; however,
results show quickly becomes infeasible treewidth expands (as grid
game).
methods come complexity guarantees, depend treewidth
graph. others (OK VK2, well algorithm) insensitive treewidth
single iteration takes time polynomial size game representation (and
hence exponential maximum degree graph). However, require
unknown number iterations converge. Corollary 7 shows that, general, computation
equilibria discretized strategies games fixed degree hard. Thus, lack
complexity guarantees methods surprising.
Nonetheless, experimental results OK seem promising indicate that,
average, relatively iterations required convergence. Results indicate OK
capable solving grid games least 100 agents (although cases
large 0.2, much better random fully mixed strategy profile). However,
running time results provided.
VK2 also exhibits strong experimental results. Vickrey Koller (2002) successfully found -equilibria games 400 agents, errors 2% maximal
payoff.
main drawback algorithms compute -equilibria. equilibrium may sufficient certain applications: utility functions
approximate, agent certainly might satisfied -best response; make
assumption slightly costly agents change minds, agent might
need incentive greater deviate. However, -equilibria bring set
problems. primary one guarantee exact equilibrium
neighborhood -equilibrium. make difficult find -equilibria
small values ; attempts refine given -equilibrium may fail. lack nearby
Nash equilibrium also implies certain instability. agent unsatisfied
-equilibrium, play may deviate quite far it. Finally, -equilibria numerous
Nash equilibria (uncountably so, general). exacerbates difficulty agent
faces choosing equilibrium play.
algorithms computing -equilibria frequently faster own, especially
approximations crude games 50 agents. However,
exact equilibria found algorithms satisfying solutions, results
show performance algorithm comparable approximate methods
cases. Surprisingly, many games, running time results show
fastest available, particularly case games large treewidth, grid
game test cases. Furthermore, since use approximate equilibrium
starting point algorithm, advances approximate methods complement
method. hybrid algorithm Vickrey Koller (2002) turns unsuited
purpose, tends remove pure strategies support,
interesting see whether methods (including listed above) might
effective. remains seen small must methods reliably refine
approximate equilibrium.
493

fiBlum, Shelton, & Koller

9.2 Related Work MAIDs
Koller Milch (2001) (KM) define notion dependence agents decisions
(s-relevance), provide algorithm decompose solve MAIDs based
fairly coarse independence structure. algorithm able exploit finer-grained
structure, resolving open problem left KM. general, method automatically exploit structure obtained decomposing game relevance
components, methods best regarded complement KM; decomposition according s-relevance, algorithm applied find equilibria
efficiently decomposed problems. Running time results indicate methods
significantly faster previous standard algorithms extensive-form games.
unsurprising, since game representation test cases exponentially larger
number players converted extensive-form.
Vickrey (2002) proposes approximate hill-climbing algorithm MAIDs takes
advantage sort fine-grained structure do: Bayesian network inference
employed calculate expected utility one component score function single
iteration. constraint-satisfaction approach also proposed. However, proposals
never implemented, hard determine quality equilibria would find
quickly would find them.
La Mura (2000) proposes continuation method finding one equilibria
G net, representation similar MAIDs. proposal exploits
limited set structural properties (a strict subset exploited KM).
proposal also never implemented, several issues regarding non-converging paths
seem unresolved.
algorithm therefore first able exploit finer-grained structure
MAID. Moreover, algorithm, applied conjunction decomposition method
KM, able take advantage full known independence structure MAID.
potential drawback requirement strategies -perturbed. However, decreasing
incurs additional computational cost, although limits imposed machine
precision. Perfect equilibria highly desirable refinement Nash equilibria, defined
limit sequence -perturbed equilibria goes zero therefore
computed effectively algorithm little additional computational cost.
sense, use perturbed strategies advantageous. implemented
local search algorithm find exact perfect equilibrium neighborhood found
-perturbed equilibrium, although straightforward so.
9.3 Conclusion Work
presented two related algorithms computing exact equilibria structured
games. algorithms based methods GW, perform key computational
steps methods much efficiently exploiting game structure. approach
yields first exact algorithm take advantage structure general graphical games
first algorithm take full advantage independence structure MAID.
algorithms capable computing exact equilibria games large numbers
agents, previously intractable exact methods.
494

fiA Continuation Method Nash Equilibria Structured Games

algorithms come without theoretical running time bounds, noticed certain interesting trends. graphical game MAID version algorithm,
iteration executes time polynomial number agents, examined
number iterations required convergence. adaptive step size technique decreases
number random restarts required find equilibrium, increases number
iterations required cross support cell larger games. adaptive step size
disabled, noticed number iterations required, averaged across games
random payoffs, seems grow approximately linearly. Intuitively, makes sense
number iterations least linear: starting pure strategy profile,
linear number actions (in number agents) must enter support order us
reach general strategy profile. support boundary requires least one iteration
algorithm. somewhat surprising, however, number iterations required
grow quickly. interesting open problem analyze number
iterations required convergence.
large games, tendency algorithm cycle increases. phenomenon
attributed, partially, cumulative effect wobbling: great number
wobbles, possible path altered sufficiently pass
equilibrium. noticed games seem intrinsically harder
others, requiring many random restarts convergence. large games,
overall running time algorithm therefore quite unpredictable.
algorithms might improved number ways. importantly, continuation method would profit greatly sophisticated path-following methods;
number cases, cont MAID cont failed find equilibrium strayed
far path. Better path-following techniques might greatly increase reliability
algorithms, particularly obviated need wobbles, negate GWs
theoretical guarantee convergence continuation method.
also number theoretical questions algorithms GW
remain unresolved. Nothing known worst-case average-case running time
IPA, theoretical bounds exist number iterations required cont.
interesting speculate choice perturbation ray might affect execution
algorithm. algorithm directed toward particular equilibria interest
either careful selection perturbation ray change continuation
method? way selecting perturbation rays equilibria found?
way selecting perturbation ray speed execution time?
Several improvements might made MAID cont. adapted IPA use
MAIDs, possible so, making use generalized Lemke algorithm
Koller, Megiddo, von Stengel (1996) solve intermediate linearized MAIDs.
computation V G might also accelerated using variant all-pairs clique tree
algorithm computes potentials pairs sepsets sets variables shared
adjacent cliques rather pairs cliques.
work suggests several interesting avenues research. fact,
initial publication results (Blum, Shelton, & Koller, 2003), least one
application techniques already developed: Bhat Leyton-Brown (2004)
shown adaptation cont used efficiently solve new class structured games called action-graph games (a generalization local effect games presented
495

fiBlum, Shelton, & Koller

Leyton-Brown & Tennenholtz, 2003). believe games, structured
representations, show great promise enablers new applications game theory.
several advantages unstructured counterparts: well-suited games
large number agents, determined fewer parameters, making feasible
human researchers fully specify meaningful way, built-in structure
makes intuitive medium frame structured, real-world scenarios.
However, avoid computational intractability general problem, new class
structured games requires new algorithm equilibrium computation. hypothesize
cont IPA excellent starting point addressing need.

Acknowledgments. work supported ONR MURI Grant N00014-00-1-0637,
Air Force contract F30602-00-2-0598 DARPAs TASK program. Special
thanks Robert Wilson, kindly taking time guide us details
work Srihari Govindan, David Vickrey, aiding us testing algorithms
alongside his. also thank anonymous referees helpful comments.
496

fiA Continuation Method Nash Equilibria Structured Games

Appendix A. Table Notation
Notation games
N
set agents
n
strategy agent n
n
strategy space agent n

strategy profile

space strategy profiles
n
strategy profile restricted agents n
n
space strategy profiles agents n
(n , n ) strategy profile agent n plays strategy n agents act
according n
Gn ()
expected payoff agent n strategy profile
G
V ()
vector deviation function
R
retraction operator mapping points closest valid strategy profile
F
continuation method objective function

scale factor perturbation continuation method
w
free variable continuation method
Notation normal-form games

action agent n

set available actions agent n

action profile

set action profiles

action profile restricted agents n

space action profiles agents n
Notation extensive-form games
z
leaf node game tree (outcome)
Z
set outcomes

information set

set information sets agent n
A(i)
set actions available information set
Hn (y)
sequence (history) agent n determined node
Zh
set outcomes consistent sequence (history) h
b(a|i)
probability behavior profile b agent n choose action
n (z)
realization probability outcome z agent n
Notation graphical games
Famn
set agent n agent ns parents
f
n
strategy profiles agents Famn n
f

space action profiles agents Famn n
Notation MAIDs
Dni
decision node index belonging agent n
Uni
utility node index belonging agent n
PaX
parents node X
dom(S) joint domain variables set
497

fiBlum, Shelton, & Koller

C1

C2



C3


b


b

c

b
c

c

Figure 12: Reduction 3SAT instance (a b c) (a b c) (a b c)
graphical game.

Appendix B. Proof Theorem 6
Proof. proof reduction 3SAT. given 3SAT instance, construct
graphical game whose equilibria encode satisfying assignments variables.
Let C = {c1 , c2 , . . . , cm } clauses 3SAT instance question, let V =
{v1 , v1 , v2 , v2 , . . . , vn , vn } set literals. variable appears one clause,
immediately assigned satisfy clause; therefore, assume variables
appear least two clauses.
construct (undirected) graphical game. clause, ci , create
agent Ci connected Ci1 Ci+1 (except C1 Cm , one clause
neighbor). also create agents Vi` literal ` ci (there 3). If,
example, ci clause (v1 v2 ), agents Viv1 Viv2 . connect
Ci . every variable v, group agents Viv Vjv connect line,
way connected clauses other. order unimportant.
Clause agents 5 neighbors (two clauses either side three
literals) literal agents 3 neighbors (two literals either side
one clause). completely specifies game topology. example, Figure 12 shows
graphical game corresponding 3SAT problem (abc)(abc)(abc).
define actions payoff structure. agent interpreted
Boolean variable, two actions, true false, correspond Boolean
values true false. Intuitively, clause Ci plays true, satisfied. agent
Viv plays true, v non-negated variable, v assigned true. Vjv
plays true, v assigned false.
payoff matrix clause agent Ci designed ensure one clause
unsatisfied, entire 3SAT instance marked unsatisfied. best expressed
pseudo-code, follows:
Ci
clause neighbors play false
1 playing false
payoff
0 playing true
else least one Ci literals plays true (Ci satisfied)
498

fiA Continuation Method Nash Equilibria Structured Games


payoff

2
2

playing false
playing true

else
(Ci unsatisfied)

1 playing false
payoff
0 playing true
end
payoff matrix literal agent Vi` designed encourage agreement
literals along line variable v(`) associated `. described
pseudo-code follows:
parent
clause Ci plays false
1 playing consistently false assignment v(`)
payoff
0 playing opposite
else Vi` sliteral neighbors play consistently single assignment v(`)
2 playing consistently neighbors
payoff
0 playing opposite
else

2 playing consistently false assignment v(`)
payoff
0 playing opposite
end
formula satisfying assignment, pure equilibrium
literal consistent assignment clauses play true; fact,
agents receive higher payoffs case equilibrium, satisfying
assignments correspond equilibria maximum social welfare.
parent clauses play false, clearly equilibrium non-negated literals
must play false negated literals must play true. trivial equilibrium.
remains show trivial equilibrium equilibrium unsatisfiable formulas, i.e. non-trivial equilibrium used construct satisfying assignment.
first prove two simple claims.
Claim 11.1. Nash equilibrium, either clauses play true probability one
clauses play false probability one.
Proof. case advantageous clause choose true false, neighbor
clause takes action false, fact disadvantageous so. Thus, clause
non-zero probability playing false equilibrium, neighbors, consequently
clauses, must play false probability one. Therefore, possible equilibria
clauses playing false clauses playing true.
follows immediately claim every non-trivial equilibrium clauses
playing true probability one.
Claim 11.2. non-trivial Nash equilibrium, line literals variable
v, literals play pure strategies must choose consistently single
assignment v.
499

fiBlum, Shelton, & Koller

Proof. Since equilibrium non-trivial, clauses play true. Suppose one
literals, V ` , employs pure strategy corresponding false assignment v. suffices
show fact literals line must pure strategies corresponding false
0
0
assignment v. Consider neighbor V ` V ` . Either V ` neighbors (one
V ` ) play consistently false assignment v, case V ` must also play
consistently false assignment v, neighbors play inconsistently, case
0
else clause V ` payoff matrix applies V ` must, again, play consistently
false assignment v. may proceed way line manner.
literals line must therefore pure strategies consistent false assignment
v, contradicting literals.

Suppose non-trivial equilibrium. Claim 11.1, clauses must play
true probability 1. literals pure strategies, clear
equilibrium corresponds satisfying assignment: literals must consistent
assignment Claim 11.2, clauses must satisfied. subtleties arise
consider mixed strategy equilibria.
Note first clause, payoff choosing true choosing false case satisfying assignment literals, less case
unsatisfying assignment. Therefore, unsatisfying assignment non-zero
probability, clause must play false.
Consider single clause Ci , assumed choosing true equilibrium. mixed
strategies Ci literals induce distribution joint
Ci plays true,
W actions.
`
joint action non-zero probability must satisfy ` Vi . literal Vi` mixed
strategy, consider happen change strategy either one possible
pure strategies (true false). joint actions non-zero probability

ones remain subset originals, still satisfy
W removed,
` . Essentially, value ` affect satisfiability C , assigned
V

`
arbitrarily.
Thus, literal line certain variable mixed strategy, assign
variable either true false (and give literal line corresponding
pure strategy) without making clauses connected literals unsatisfied.
fact, literals line pure strategies consistent
other: indeed literals pure strategies, assign variable according
them. Claim 11.2, always case.

observe briefly constructed graphical game finite number
equilibria, even peculiarities 3SAT instance give rise equilibria mixed
strategies. clauses play false, one equilibrium. clauses play true,
remove graph trim payoff matrices literals
accordingly. line literals case generic graphical game, finite set
equilibria. equilibria original game must subset direct product
finite sets.
500

fiA Continuation Method Nash Equilibria Structured Games

References
Bhat, N. A. R., & Leyton-Brown, K. (2004). Computing Nash equilibria action-graph
games. Proceedings Twentieth International Conference Uncertainty
Artificial Intelligence.
Blum, B., Shelton, C., & Koller, D. (2003). continuation method Nash equilibria
structured games. Proceedings Eighteenth International Joint Conference
Artificial Intelligence, pp. 757764.
Chu, F., & Halpern, J. (2001). np-completeness finding optimal strategy
games common payoff. International Journal Game Theory, 30, 99106.
Codenotti, B., & Stefankovic, D. (2005). computational complexity nash equilibria
(0,1) bimatrix games. Information Processing Letters, 94, 145150.
Conitzer, V., & Sandholm, T. (2003). Complexity results Nash equilibria. Proceedings Eighteenth International Joint Conference Artificial Intelligence,
pp. 765771.
Cowell, R. G., Dawid, A. P., Lauritzen, S. L., & Spiegelhalter, D. J. (1999). Probabilistic
Networks Expert Systems. Springer-Verlag.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Gilboa, I., & Zemel, E. (1989). Nash correlated equilibria: complexity considerations. Games Economic Behavior, 1, 8093.
Govindan, S., & Wilson, R. (2002). Structure theorems game trees. Proceedings
National Academy Sciences, 99 (13), 90779080.
Govindan, S., & Wilson, R. (2003). global Newton method compute Nash equilibria.
Journal Economic Theory, 110, 6586.
Govindan, S., & Wilson, R. (2004). Computing Nash equilibria iterated polymatrix
approximation. Journal Economic Dynamics Control, 28, 12291241.
Gul, F., Pearce, D., & Stachetti, E. (1993). bound proportion pure strategy
equilibria generic games. Mathematics Operations Research, 18, 548552.
Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. Howard, R. A., & Matheson, J. E. (Eds.), Readings Principles Applications Decision Analysis,
Vol. 2, pp. 719762. Strategic Decision Group. article dated 1981.
Kearns, M., Littman, M. L., & Singh, S. (2001). Graphical models game theory.
Proceedings Seventeenth International Conference Uncertainty Artificial
Intelligence, pp. 253260.
Kohlberg, E., & Mertens, J.-F. (1986). strategic stability equilibria. Econometrica,
54 (5), 10031038.
Koller, D., & Megiddo, N. (1992). complexity two-person zero-sum games extensive
form. Games Economic Bahavior, 4, 528552.
Koller, D., Megiddo, N., & von Stengel, B. (1996). Efficient computation equilibria
extensive two-person games. Games Economic Behavior, 14, 247259.
501

fiBlum, Shelton, & Koller

Koller, D., & Milch, B. (2001). Multi-agent influence diagrams representing solving
games. Proceedings Seventeenth International Joint Conference Artificial
Intelligence, pp. 10271034.
Kuhn, H. W. (1953). Extensive games problem information. Contributions
Theory Games II, eds. H. W. Kuhn A. W. Tucker, Vol. 28, pp. 193216.
Princeton University Press, Princeton, NJ.
La Mura, P. (2000). Game networks. Proceedings Sixteenth International Conference Uncertainty Artificial Intelligence, pp. 335342.
Lauritzen, S. L., & Spiegelhalter, D. J. (1998). Local computations probabilities
graphical structures application expert systems. Journal Royal
Statistical Society, B 50 (2), 157224.
Lemke, C. E., & Howson, Jr., J. T. (1964). Equilibrium points bimatrix games. Journal
Society Applied Mathematics, 12 (2), 413423.
Leyton-Brown, K., & Tennenholtz, M. (2003). Local-effect games. Proceedings
Eighteenth International Joint Conference Artificial Intelligence, pp. 772777.
Littman, M. L., Kearns, M., & Singh, S. (2002). efficient exact algorithm singly
connected graphical games. Advances Neural Information Processing Systems
14, Vol. 2, pp. 817823.
McKelvey, R. D., & McLennan, A. (1996). Computation equilibria finite games.
Handbook Computational Economics, Vol. 1, pp. 87142. Elsevier Science.
McKelvey, R. D., McLennan, A. M., & Turocy, T. L. (2004). Gambit: Software tools
game theory, version 0.97.07.. http://econweb.tamu.edu/gambit.
Nash, J. (1951). Non-cooperative games. Annals Mathematics, 52 (2), 286295.
Nudelman, E., Wortman, J., Shoham, Y., & Leyton-Brown, K. (2004). Run GAMUT:
comprehensive approach evaluating game-theoretic algorithms. Third International Conference Autonomous Agents Multi-Agent Systems.
Ortiz, L. E., & Kearns, M. (2003). Nash propagation loopy graphical games. Advances
Neural Information Processing Systems 15, Vol. 1, pp. 793800.
Romanovskii, I. (1962). Reduction game complete memory matrix game.
Doklady Akademii Nauk, SSSR 144, 6264. [English translation: Soviet Mathematics
3, pages 678681].
Vickrey, D. (2002). Multiagent algorithms solving structured games. Undergraduate
honors thesis, Stanford University.
Vickrey, D., & Koller, D. (2002). Multi-agent algorithms solving graphical games.
Proceedings Eighteenth National Conference Artificial Intelligence (AAAI),
pp. 345351.
von Stengel, B. (1996). Efficient computation behavior strategies. Games Economic
Behavior, 14, 220246.
Watson, L. T. (2000). Theory globally convergent probability-one homotopies nonlinear programming. SIAM Journal Optimization, 11 (3), 761780.

502

fiJournal Artificial Intelligence Research 25 (2006) 1-15

Submitted 08/05; published 01/06

Engineering Note
Engineering Conformant Probabilistic Planner
Nilufer Onder
Garrett C. Whelan
Li Li

nilufer@mtu.edu
gcwhelan@mtu.edu
lili@mtu.edu

Department Computer Science
Michigan Technological University
1400 Townsend Drive
Houghton, MI 49931

Abstract
present partial-order, conformant, probabilistic planner, Probapop competed blind track Probabilistic Planning Competition IPC-4. explain
adapt distance based heuristics use probabilistic domains. Probapop
also incorporates heuristics based probability success. explain successes
difficulties encountered design implementation Probapop.

1. Introduction
Probapop conformant probabilistic planner took part probabilistic track
4th International Planning Competition (IPC-4). conformant planner
competed. conformant probabilistic planning paradigm (Hyafil & Bacchus, 2003)
actions state initialization probabilistic, i.e., several possible
outcomes annotated probability occurrence. addition, planning problem
conformant, i.e., planner construct best plan possible without assuming
results actions performed observed. example conformant
probabilistic planning problem, consider student applying graduate studies. Suppose
application needs include several forms prepared student single
letter recommendation written professor (one letter sufficient one
letter acceptable). assume typical professor students department
80% probability sending letter time. problem student asks
one professor letter, probability complete application 0.8.
student observes professor sent letter due date, way
complete application would late ask another professor. Thus,
observation actions useless way student increase chances
getting letter ask one professor send letter. 9 professors
asked, probability getting letter 0.999997 close 1. Obviously,
asking many people costly, therefore student weigh benefits increased
probability costs asking several people.
conformant probabilistic planners task find best sequence actions
possible results actions predefined probabilities cannot observed.
regard, conformant probabilistic planners classified non-observable Markov
c
2006
AI Access Foundation. rights reserved.

fiOnder, Whelan & Li

decision processes (NOMDPs) (Boutilier, Dean, & Hanks, 1999). Fully-observable MDPs
(FOMDP) extreme MDPs agent complete cost-free
sensors indicate current state. Planners adopt FOMDP framework
generate policies functions states actions. NOMDP based planners
generate unconditional sequences actions based predictive model,
environment cannot observed (Boutilier et al., 1999). middle ground partially
observable MDPs (POMDPs) contingency plans domain observable execution actions may depend results observations (Kaelbling,
Littman, & Cassandra, 1998; Majercik & Littman, 1999; Onder & Pollack, 1999; Hansen
& Feng, 2000; Karlsson, 2001; Hoffmann & Brafman, 2005). also conformant
planners model imperfect actions may multiple possible results
model probability information (Ferraris & Giunchiglia, 2000; Bertoli, Cimatti, & Roveri,
2001; Brafman & Hoffmann, 2004).
work Probapop motivated incentive partial-order planning
viable option conformant probabilistic planning. main reasons threefold.
First, partial-order planners worked well parametric lifted actions,
useful coding large domains. Second, due least commitment strategy step
ordering, partial-order planning (POP) produces plans highly parallelizable. Third,
many planners handle rich temporal constraints based POP
paradigm (Smith, Frank, & Jonsson, 2000). Given advantages, intuition
design Probapop bring together two paradigms model states explicitly:
POP planners represent states search space plans, blind
planners cannot observe state observation actions available.
basic approach form base plans using deterministic partial-order planning
techniques, estimate best way improve plans. Recently, Repop
(Nguyen & Kambhampati, 2001) Vhpop (Younes & Simmons, 2003) planners
demonstrated heuristics speed non-partial-order planners used
scale partial-order planning. show distance-based heuristics (McDermott,
1999; Bonet & Geffner, 2001) implemented using relaxed plan graphs partial-order
planners Repop Vhpop employed probabilistic domains.
heuristics coupled selective plan improvement heuristics incremental planning
techniques result significant advantages. result, Probapop makes partial-order
planning feasible probabilistic domains. work Probapop invaluable
understanding identifying key solutions issues probabilistic conformant
planning.

2. Probapop Partial-Order Planning
partial-order probabilistic planning, implemented Buridan (Kushmerick, Hanks,
& Weld, 1995) probabilistic planning algorithm top Vhpop (Younes & Simmons,
2003), recent partial-order planner. partially ordered plan 6-tuple, <STEPS,
BIND, ORD, LINKS, OPEN, UNSAFE>, representing sets actions, binding constraints, ordering constraints, causal links, open conditions, unsafe links, respectively. binding
constraint constraint action parameters action parameters ground
literals. ordering constraint Si Sj represents fact step Si precedes Sj .
2

fiEngineering Conformant Probabilistic Planner

causal link triple < Si , p, Sj >, Si producer step, Sj consumer step
p represents condition supported Si Sj . open condition pair < p, >,
p condition needed step S. causal link < Si , p, Sj > unsafe plan
contains threatening step Sk Sk p among effects, Sk may intervene
Si Sj . Open conditions unsafe links collectively referred flaws.
planning problem quadruple < D, I, G, >, where, domain theory consisting
(probabilistic) operators, initial state probability distribution states, G
set literals must true end execution, termination criterion
probability threshold time limit. objective planner find
maximal probability plan takes agent G. several plans
probability success, one least number steps cost preferred.
Probapop algorithm shown Figure 1 based classical POP algorithm
(Russell & Norvig, 2003; Younes & Simmons, 2003). first constructs initial plan
converting initial goal dummy initial goal steps, using first
last steps plan empty body. refines plans search queue
meets termination criterion. termination criterion implemented
include time limit (e.g., stop 5 minutes), memory limit (e.g., stop 256MB),
probability threshold (e.g., stop finding plan 0.9 higher probability),
lack significant progress (e.g., stop probability success cannot increased
). possible specify multiple termination criterion use earliest one
becomes true. termination criterion met plan highest probability
returned.
Plan refinement operations involve repairing flaws. open condition closed
adding new step domain theory, reusing step already plan. unsafe
link handled promotion, demotion, separation (when lifted actions used)
operations, confrontation (Penberthy & Weld, 1992). techniques part
Vhpop implementation. Consider step Sk threatening causal link < Si , p, Sj >.
Promotion involves adding extra ordering constraint Sk comes Sj (Sj Sk
added ORD). Demotion involves adding extra ordering constraint k
comes Si (Sk Si added ORD). Separation involves adding extra inequality
constraint BIND Sk threatening effect longer unify p. Finally,
actions multiple effects, confrontation used making commitment
non-threatening effects Sk , i.e., effects Sk contain proposition
unifies p. Note deterministic domains, action multiple
effects due multiple secondary preconditions (when conditions). probabilistic domains,
probabilistic actions always multiple effects.
search conducted using A* algorithm guided ranking function
provides f value. usual plan , f () = g() + h(), g() cost
plan, h() estimated cost completing it. ranking function used
Merge step algorithm order plans search queue. competition
Probapop used distance based heuristic (ADD) explained next section.
flaw selection strategy Select-Flaw method, used Vhpops static, gives
priority static open conditions, i.e., condition whose value altered action
domain theory. flaws plan contain static open conditions
threats handled next; lowest priority given remaining open conditions.
3

fiOnder, Whelan & Li

function Probapop (D, initial, goal, T)
returns solution plan, failure
** plans Make-Minimal-Plan(initial, goal)
** BestPlan null
** loop
**** termination criterion met return BestPlan
**** plans empty return failure
**** plan Remove-Front(plans)
**** Solution?(plan) return plan
**** plans Merge(plans, Refine-Plan(plan))
** end
function Refine-Plan (plan)
returns set plans (possibly null)
** Flaws(plan) empty
**** ProbSuccess (plan) > ProbSuccess (BestPlan)
******* BestPlan plan
**** plan Reopen-Conditions(plan)
** flaw Select-Flaw(plan)
** flaw open condition choose:
****** return Reuse-Step(plan, flaw)
****** return Add-New-Step(plan, flaw)
** flaw threat choose:
****** return Demotion(plan, flaw)
****** return Promotion(plan, flaw)
****** return Separation(plan, flaw)
****** return Confrontation(plan, flaw)

Figure 1: probabilistic POP algorithm.

comment heuristics flaw selection techniques following discussion
competition results.
deterministic POP algorithm, plan considered complete
flaws, i.e., OPEN = UNSAFE = . probabilistic domains, possibility complete
plans insufficient probability success (e.g., 1 ) improved.
Probapop improves plans conducting search reopening conditions
fail explained next section. Probapop viewed first searching
plan complete deterministic sense, searching way improve
plan. current implementation, discard search queue finding
first plan subsequent improvements made first complete plan found.
future, plan implement multiple search queues order able jump
different plans improvements. Figure 2a, show initial plan
corresponds student application domain mentioned first section. open
conditions sending forms (forms-sent) getting letter reference (letter-sent).
Probapop uses Vhpop guided ranking flaw selection heuristics produce
complete plan 80% probability success shown Figure 2b. straight line shows
causal link two actions zigzag line refers causal link plan
4

fiEngineering Conformant Probabilistic Planner

fragment omitted clarity exposition. Probapop reopens condition
letter-sent (Figure 3a) resumes search using heuristics come
improved plan involves asking two professors shown Figure 3b. Assuming
ASK-PROFx action probabilistic effects, probability success
0.8 first complete plan 0.8 + 0.2 0.8 second complete plan. Several
iterations reopen search leads Probapop find plan probability
0.999997. plan cannot improved single precision arithmetic.
INITIAL

INITIAL

ASK PROF1

formssent

lettersent

formssent

lettersent

GOAL

GOAL

(a)

(b)

Figure 2: Starting empty plan finding first plan.

INITIAL

INITIAL

ASK PROF1

formssent

lettersent

ASK PROF1

lettersent

formssent

GOAL

lettersent

ASK PROF2

lettersent

GOAL

(a)

(b)

Figure 3: Starting complete plan finding improved plan.

3. Distance Based Ranking Probapop
Vhpop deterministic partial order-planner described Younes Simmons (2003)
supports distance based heuristics provide estimate total number new actions
needed close open condition. starting search, planner builds planning
graph (Blum & Furst, 1997), literals initial state first level,
continues expand graph reaches level goal literals present.
planning graph different Graphplans planning graph sense
5

fiOnder, Whelan & Li

relaxed, i.e., delete lists ignored thus mutex relationships computed (Bonet
& Geffner, 2001).
order able generate relaxed planning graph multiple probabilistic
effects present, one would need split many plan graphs leaves
probabilistic action. avoid potential blow up, split action domain
theory many deterministic actions number nonempty effect lists. split
action represents different way original action would work. Figure 4, show
action A1, two probabilistic effects b P Q true, one effect c
P true Q false, effect otherwise. split action corresponds one
set non-empty effects. Probapop, plan graph uses split actions, plans
constructed always contain full original action planner correctly assess
probability success. using split actions, compute good estimate
number actions needed complete plan use distance based heuristics.
A1
P
Q

prec: P, Q

prec: P, Q

A11

~P

A12
b



~Q

prec: P, ~Q
0.7

0.3



b

c

A13
c

Figure 4: Probabilistic action A1 split deterministic actions A1-1, A1-2, A1-3.
important distinction deterministic partial-order planning probabilistic
partial-order planning multiple support plan literals. deterministic case,
open condition permanently removed list flaws resolved.
probabilistic case, reopened planner search additional steps
increase probability literal. Buridan system implements technique
reopening previously closed conditions complete plan resuming search
find another complete plan. implementation employs selective reopening (SR)
conditions guaranteed achieved reopened. words,
literals supported probability 1 reopened. Note checking
probability literals costly probabilistic plans, save cost performing
check mandatory assessment complete plans. Obviously, avoiding redundant
searches advantage planner. current implementation reopen
supported literals probability less 1. leave selection among
new set preconditions flaw selection heuristic. implementation
contain probability based heuristics.
important note neither split actions selective reopening technique
change base soundness completeness properties Buridan algorithm. split
actions used relaxed plan graph, reopening technique block
alternatives sought would already covered plan search
queue.
6

fiEngineering Conformant Probabilistic Planner

4. Probapop IPC-4
Probapop among 7 domain-independent planners competed probabilistic
track IPC-4. domain-independent mean planner uses PPDDL description domain solve planning problem employ previously coded
control information. Table 1 show brief description planners (Edelkamp,
Hoffman, Littman, & Younes, 2004; Younes, Littman, Weissman, & Asmuth, 2005; Bonet
& Geffner, 2005; Fern, Yoon, & Givan, 2006; Thiebaux, Gretton, Slaney, Price, & Kabanza,
2006). competition conducted follows: planner given set 24
problems written probabilistic PDDL (PPDDL) allotted 5 minutes solve
problem. this, server simulated possible way executing plan sending
sequence states starting initial state planners responded state
action based solution found. 30 simulations conducted
problem. goal-based problems success measured whether goal reached
end simulation. reward-based problems total reward calculated.
set 24 problems included types.
competition included various domains listed below:
Blocksworld: Includes pick put actions action fail.
6 problems 5, 8, 11, 15, 18 21 blocks given. goal build one
towers blocks.
Colored Blocksworld: actions Blocksworld domain.
block one three colors. goal towers specified using existential
quantifiers, e.g., green block table, red block green
block.
Exploding Blocksworld: similar Blocksworld domain first put-down
action permanently destroy bottom object (block table). Replanning
repetition based approaches fail easily due irreversible nature explosion.
Boxworld: box transportation problem load, unload, drive fly actions.
drive action fail taking truck wrong city.
Fileworld: objective includes actions put papers files matching
type. type paper found using observation action
probabilistics outcomes.
Tireworld: actions include moving several cities tire go flat
trip.
Towers Hanoise: variation Towers Hanoi problem discs
moved singles doubles discs may slip move.
Zeno travel: travel domain includes actions related flying. actions
boarding flying fail.
noted competition domains designed full observability
needed changed incorporate blind planner. instance, PICKUP action
7

fiOnder, Whelan & Li

Planner (code)
UMass (C)
NMRDPP (G1)
Classy (J2)
FF-rePlan (J3)
mGPT (P)
Probapop (Q)
CERT (R)

Description
Symbolic heuristic search based symbolic AO* loops (LAO*)
symbolic real-time dynamic programming (RTDP)
Solving decision problems non-Markovian (and hence Markovian)
rewards
Approximate policy iteration inductive machine learning using
random-walk problems
Deterministic replanner using Fast Forward
Labeled real-time dynamic programming (LRTDP) lower bounds
extracted deterministic relaxations MDP
POP-style plan-space A* search distance based heuristics failure
analysis
Heuristic state space search structured policy iteration algorithm,
factored MDPs, reachability analysis

Table 1: Domain-independent planners listed order competition code.

Blocksworld domain precondition requires block picked
held arm. action two probabilistic effects, one resulting
block held, held. planner assumes
observability, plan involving PICKUP action cannot improved action
cannot executed unless preconditions hold. Thus, Probapop planner cannot
insert second PICKUP action cover case first one fails. help
competition organizers, implemented workaround actions
executed conditions hold effect rather causing error.
Probapop (competition name Q) attempted 4 24 problems. two planners
attempted problems Classy (J2) FF-rePlan (J3).
planners attempted 3 10 problems listed Table 2. Probapop attempted
small number problems due three reasons. First, started building Probapop,
Vhpops version 2.0. performance Vhpop significantly improved better
memory handling techniques version 2.2 time competition
convert implementation newer version. Second, competition Blocksworld
domains included universally quantified preconditions supported Vhpop.
implementation preconditions including FORALL keyword efficient.
Third, implementation disables feature Vhpop allows use multiple
search queues different heuristics. prohibited us constructing several search
queues different heuristic using one finishes earliest.
therefore pick single heuristic run competition problems. result,
picked ADD ranking metric static flaw selection technique ran
problems combination.
competition results announced, observed three domain
independent planners, namely Classy (J2), FF-rePlan (J3), mGPT (P),
able solve largest Blocksworld problems whereas Probapop able solve
5-blocks problem (the competition included domains 5, 8, 11, 15, 18, 21
8

fiEngineering Conformant Probabilistic Planner

Planner
Umass (C)
NMRDPP (G1)
Classy (J2)
FF-rePlan (J3)
mGPT (P)
Probapop (Q)
CERT (R)

#
problems
4
7
18
24
10
4
3

bw-nc-r-5
30
30
30
30
30
11
30

tire-nr
30
9

7
16
7
9

tire-r
30
30

30
30
6
0

zeno
30
30

0
30
1
27

Table 2: number successes 30 trials obtained planners use
domain knowledge. problems attempted Probapop (Q) listed.
dash means planner attempt problem. Bw-nc-r-5
Blocksworld problem 5 blocks. Tire-nr tire-r goal reward
based problems Tireworld domain. Zeno problem using Zeno
travel domain problem.

blocks). Therefore, looked ways improving performance Probapop
problems. first reimplemented Probapop Vhpops newer version 2.2. Second,
brought language competition Blocksworld domain closer STRIPS.
particular, removed FORALL preconditions conditions. example,
replaced PPDDL PICK-UP action shown Figure 5 two actions shown
Figure 6. However, version upgrade language simplification sufficient
enable Probapop solve 8-blocks problem. explained before, Probapops strategy
first find base plan improve plan possible failure points, therefore
finding base plan crucial. next looked heuristics flaw selection
strategies make Blocksworld problems solvable. begin discussing
explaining Vhpops ADD heuristic detail.
(:action pick-up-block-from
* :parameters (?top - block ?bottom)
* :effect (when (and (not (= ?top ?bottom)) (on-top-of ?top ?bottom)
****************** (forall (?b - block) (not (holding ?b)))
****************** (forall (?b - block) (not (on-top-of ?b ?top))))
************ (and (decrease (reward) 1)
************ (probabilistic 0.75 (and (holding ?top) (not (on-top-of ?top ?bottom)))
*************** ******** 0.25 (when (not (= ?bottom table))
*************** ************* (and (not (on-top-of ?top ?bottom)) (on-top-of ?top table)))))))

Figure 5: PPDDLs PICK-UP action
ADD heuristic achieves good performance computing sum step costs
open conditions relaxed planning graph, i.e., heuristic cost plan
computed h() = hadd (OP EN ()). cost achieving literal q level
first action achieves q: hadd (q) = minaGA(q) hadd (a) GA(q) 6= , GA(q)
9

fiOnder, Whelan & Li

(:action pick-up
* :parameters (?x)
* :precondition (and (clear ?x) (ontable ?x) (handempty))
* :effect
* ** (probabilistic 0.75
* **** (and (not (ontable ?x)) (not (clear ?x)) (not (handempty)) (holding ?x))))
(:action unstack
* :parameters (?x ?y)
* :precondition (and (on-top-of ?x ?y) (clear ?x) (handempty))
* :effect
* ** (probabilistic 0.75
* **** (and (holding ?x) (clear ?y) (not (clear ?x)) (not (handempty)) (not (on-top-of ?x ?y)))))

Figure 6: Simplified form PPDDLs PICK-UP action.
action effect q. Note hadd (q) 0 q holds initially,
q never holds. level action first level preconditions become true:
hadd (a) = 1 + hadd (P REC(a)). ADDR heuristic modification ADD heuristic
takes action reuse account, thus addition conditions described above,
heuristic cost literal q 0 plan already contains action achieve q.
observed ADDR effective ADD Blocksworld domain
tested variety flaw selection strategies implemented Vhpop together ADDR.
show flaw selection strategies tried Table 3. adopt notation given
Pollack et al. (1997) revised Younes Simmons (2003). notation,
strategy ordered list selection criteria LR refers least refinements
first, MCadd refers cost computed using ADD, MWadd refers work
using ADD. Open conditions divided three categories use heuristics.
static open condition open condition whose literal provided
initial state, i.e., action literal effect. local open condition refers
open conditions recently added action used maintain focus
achievement single goal. unsafe open condition refers open condition whose
causal link would threatened.
five main strategies prioritize flaws differently. ucpop strategy
gives priority threats, static strategy gives priority static open conditions, lcfr
strategy handles flaws order least expected cost, mc strategy orders open conditions
respect cost extracted relaxed planning graph, mw strategy orders
open conditions respect expected work extracted relaxed planning graph.
strategy loc annotation gives priority local open conditions among open
conditions, strategy conf annotation gives priority unsafe open conditions
among open conditions. refer reader paper Younes Simmons
(2003) thorough description heuristics well experimental results
domains.
depict results experiments Blocksworld problems first
third lines Table 4 (the second fourth lines Tables 4 5 explained later).
10

fiEngineering Conformant Probabilistic Planner

Strategy
ucpop
static
lcfr
lcfr-loc
lcfr-conf
lcfr-loc-conf
mc
mc-dsep
mc-loc
mc-dsep
mw
mw-dsep
mw-loc
mw-loc-dsep

Description
{n,s} LIFO / {o} LIFO
{t} LIFO / {n,s} LIFO / {o} LIFO
{n,s,o} LR
{n,s,l} LR
{n,s,u} LR / {o} LR
{n,s,u} LR / {l} LR
{n,s} LR / {o} MCadd
{n} LR / {o} MCadd / {s} LR
{n,s} LR / {l} MCadd
{n} LR / {l} MCadd / {s} LR
{n,s} LR / {o} MWadd
{n} LR / {o} MWadd / {s} LR
{n,s} LR / {l} MWadd
{n} LR / {l} MWadd / {s} LR

Table 3: description variety flaw selection strategies Vhpop. n nonseparable threat, separable threat, open condition, static
open condition, l local open condition, u unsafe open condition.

seen lcfr mc strategies work problem 8 blocks.
larger problems solvable. actions lifted, tried make search
space smaller delaying separable threats. Peot Smith (1993) explain delaying
separable threats may result decreased branching factor may many
ways add inequality constraints separation. delay might also help
threat disappear variables bound. modified best working strategies,
namely variants mc mw, implemented delay separable threats (in Table 3
shown dsep suffix.) show planning times experiments
without dsep Table 5 (we repeat columns Table 4 comparison).
results show time improvement seen 5-blocks problem. problems
8 blocks show increase time threat must checked see
separable. Delaying threats made 8-blocks problem solvable using mc-loc, mw,
mw-loc strategies. However, larger problems solvable strategy.
results experiments various heuristics strategies show
search time increases dramatically going 5 8 blocks larger problems
solvable. able find heuristic combination solve larger problems.
noticed competition Blocksworld problems list goal towers top bottom
planner spends lot time dead end plans original goal order
preserved. tower built top bottom, initial goals almost always
undone achieve later goals. also concluded interaction cannot
detected heuristics used designed consider subgoals
isolation. Koehler Hoffmann (2000) describe polynomial time algorithm
order goals minimize type undoing. algorithm operates ground
11

fiOnder, Whelan & Li

5
5o
8
8o

ucpop

static

lcfr

80
0



70
0



0
10
55K


lcfrloc
60
50



lcfrconf
570
10



lcfrloc-conf
90
770



mc
10
0
13K
42K

mcloc
50
40



mw
0
0

41K

mwloc
20
30



mwloc-conf
220
120



Table 4: Time (msec) required find base plan Blocksworld problems 5
8 blocks.

5
5o
8
8o

mc
10
0
13K
42K

mc-dsep
0
0
73K
104K

mc-loc
40
50



mc-loc-dsep
20
30
22K


mw
0
0

41K

mw-dsep
0
0
73K
103K

mw-loc
20
30



mw-loc-dsep
30
20
22K


Table 5: Time (msec) required find base plan delaying separable threats
Blocksworld problems 5 8 blocks.

action descriptions generated action schemas implemented
FF planning system (Hoffman & Nebel, 2001). used algorithm order
top-level goals repeated experiments ordering essentially builds
towers bottom top. results ordered goals shown lines 2
4 tables 4 5. Ordering goals mixed results. example, 8 blocks
problem, made lcfr heuristic usable mw heuristic usable. However,
lowest time increased 13K 41K milliseconds larger problems still
solvable.
final strategy combine planning approach used FF planner
POP-style search. particular, ordered top-level goals using FFs ordering algorithm ran Vhpop n times problems n top level goals. first problem
first goal Vhpop returned plan, steps simulated find
resulting state. second problem resulting state initial state goals 1
2 goal 1 would preserved redone goal 2 would achieved.
used strategy default heuristics Vhpop solve problem 21 blocks,
total time 70 milliseconds phases taking 0 milliseconds. Koehler
Hoffmann (2000) explain approach works well invertible planning problems,
i.e., problems Blocksworld actions reversible. case, tradeoff
possibility less optimal plans plan ith goal set working
+ 1st goal. second tradeoff getting several partially-ordered plans
breakpoints problems rather single maximally parallel plan. believe
worthwhile work algorithm combines individual plans preserve
least commitment ordering. Possible strategies causally link action preconditions
12

fiEngineering Conformant Probabilistic Planner

latest producers use approach Edelkamp (2004) parallelize sequential plans
using critical path analysis.

5. Conclusion Future Work
presented design implementation Probapop, partial-order, probabilistic,
conformant planner. described distance-based condition-probability based
heuristics used. discussed advantages disadvantages using incremental algorithm goals first ordered submitted one one. short
term plans involve implementing multiple search queues different base plans reincorporating ADL constructs PPDDL. future work involves three threads. one,
looking improving performance Probapop adding probability information planning graph probability open conditions optimistically
estimated. also considering addition domain specific information (Kuter &
Nau, 2005) probabilistic domains. second thread, exploring middle
ground observability full observability considering POMDP-like problems partial-order setting. Finally, would like incorporate hill climbing techniques
probabilistic framework. current Probapop 2.0 software available
www.cs.mtu.edu/nilufer.

Acknowledgments
work supported Research Excellence Fund grant Nilufer Onder
Michigan Technological University. thank JAIR IPC-4 special track editor David E.
Smith, anonymous reviewers helpful comments.

References
Bertoli, P., Cimatti, A., & Roveri, M. (2001). Heuristic search + symbolic model checking
= efficient conformant planning. Proceedings Eighteenth International Joint
Conference Artificial Intelligence (IJCAI-01), pp. 467472.
Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90, 281300.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (12), 533.
Bonet, B., & Geffner, H. (2005). mGPT: probabilistic planner based heuristic search.
Journal Artificial Intelligence Research, 24, 933944.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Brafman, R. I., & Hoffmann, J. (2004). Conformant planning via heuristic forward search:
new approach. Proceedings Fourteenth International Conference Automated
Planning & Scheduling (ICAPS-04), pp. 355364.
13

fiOnder, Whelan & Li

Edelkamp, S. (2004). Extended critical paths temporal planning. Workshop Integrating Planning Scheduling International Conference Automated Planning
Scheduling (ICAPS-04), pp. 3845.
Edelkamp, S., Hoffman, J., Littman, M., & Younes, H. (2004). International planning competition. Proceedings Fourteenth International Conference Automated Planning
& Scheduling (ICAPS-04).
Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration policy language
bias: Solving relational Markov decision processes. Journal Artificial Intelligence
Research, 25.
Ferraris, P., & Giunchiglia, E. (2000). Planning satisfiability nondeterministic domains. Proceedings Seventeenth National Conference Artificial Intelligence
(AAAI-00), pp. 748754.
Hansen, E. A., & Feng, Z. (2000). Dynamic programming POMDPs using factored
state representation. Proceedings Fifth International Conference Artificial
Intelligence Planning & Scheduling (AIPS-00), pp. 130139.
Hoffman, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Hoffmann, J., & Brafman, R. I. (2005). Contingent planning via heuristic forward search
implicit belief states. Proceedings Fifteenth International Conference
Automated Planning & Scheduling (ICAPS-05), pp. 7180.
Hyafil, N., & Bacchus, F. (2003). Conformant probabilistic planning via CSPs. Proceedings Thirteenth International Conference Automated Planning & Scheduling
(ICAPS-03), pp. 205214.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101, 99134.
Karlsson, L. (2001). Conditional progressive planning uncertainty. Proceedings
Eighteenth International Joint Conference Artificial Intelligence (IJCAI-01), pp.
431436.
Koehler, J., & Hoffmann, J. (2000). reasonable forced goal orderings use
agenda-driven planning algorithm. Journal Artificial Intelligence Research,
12, 339386.
Kushmerick, N., Hanks, S., & Weld, D. S. (1995). algorithm probabilistic planning.
Artificial Intelligence, 76, 239286.
Kuter, U., & Nau, D. (2005). Using domain-configurable search control probabilistic planners. Proceedings Twentieth National Conference Artificial Intelligence
(AAAI-05).
Majercik, S. M., & Littman, M. L. (1999). Contingent planning uncertainty via
stochastic satisfiability. Proceedings Sixteenth National Conference Artificial Intelligence (AAAI-99), pp. 549556.
McDermott, D. (1999). Using regression-match graphs control search planning. Artificial Intelligence, 109 (1-2), 111159.
14

fiEngineering Conformant Probabilistic Planner

Nguyen, X., & Kambhampati, S. (2001). Reviving partial order planning. Proceedings
Eighteenth International Joint Conference Artificial Intelligence (IJCAI-01), pp.
459464.
Onder, N., & Pollack, M. E. (1999). Conditional, probabilistic planning: unifying algorithm effective search control mechanisms. Proceedings Sixteenth
National Conference Artificial Intelligence (AAAI-99), pp. 577584.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: sound, complete, partial order planner
ADL. Proceedings Third International Conference Principles Knowledge
Representation & Reasoning (KR-92), pp. 103114.
Peot, M. A., & Smith, D. E. (1993). Threat-removal strategies partial order planning.
Proceedings Eleventh National Conference Artificial Intelligence (AAAI-93),
pp. 492499.
Pollack, M. E., Joslin, D., & Paolucci, M. (1997). Flaw selection strategies partial-order
planning. Journal Artificial Intelligence Research, 6, 223262.
Russell, S. J., & Norvig, P. (2003). Artificial Intelligence: Modern Approach, Second
Edition. Pearson Education, Upper Saddle River, NJ.
Smith, D. E., Frank, J., & Jonsson, A. K. (2000). Bridging gap planning
scheduling. Knowledge Engineering Review, 15 (1), 4783.
Thiebaux, S., Gretton, C., Slaney, J., Price, D., & Kabanza, F. (2006). Decision-theoretic
planning non-Markovian rewards. Journal Artificial Intelligence Research, 25.
Younes, H. L., Littman, M. L., Weissman, D., & Asmuth, J. (2005). first probabilistic
track international planning competition. Journal Artificial Intelligence
Research, 24, 851887.
Younes, H., & Simmons, R. (2003). VHPOP: Versatile heuristic partial order planner.
Journal Artificial Intelligence Research, 20, 405430.

15

fiJournal Artificial Intelligence Research 25 (2006) 315348

Submitted 08/05; published 03/06

Negotiating Socially Optimal Allocations Resources
Ulle Endriss

ulle@illc.uva.nl

ILLC, University Amsterdam
1018 TV Amsterdam, Netherlands

Nicolas Maudet

maudet@lamsade.dauphine.fr

LAMSADE, Universite Paris-Dauphine
75775 Paris Cedex 16, France

Fariba Sadri

fs@doc.ic.ac.uk

Department Computing, Imperial College London
London SW7 2AZ, UK

Francesca Toni

ft@doc.ic.ac.uk

Department Computing, Imperial College London
London SW7 2AZ, UK

Abstract
multiagent system may thought artificial society autonomous software
agents apply concepts borrowed welfare economics social choice theory
assess social welfare agent society. paper, study abstract
negotiation framework agents agree multilateral deals exchange bundles
indivisible resources. analyse deals affect social welfare different
instances basic framework different interpretations concept social welfare
itself. particular, show certain classes deals sufficient necessary
guarantee socially optimal allocation resources reached eventually.

1. Introduction
multiagent system may thought artificial society autonomous software
agents. Negotiation distribution resources (or tasks) amongst agents inhabiting society important area research artificial intelligence computer
science (Rosenschein & Zlotkin, 1994; Kraus, 2001; Chavez, Moukas, & Maes, 1997; Sandholm, 1999). number variants problem studied literature.
consider case artificial society agents where, begin with, agent
holds bundle indivisible resources assigns certain utility. Agents may
negotiate order agree redistribution resources
benefit either agent society inhabit.
Rather concerned specific strategies negotiation, analyse
redistribution resources means negotiation affects well-being agent society
whole. end, make use formal tools measuring social welfare developed
welfare economics social choice theory (Moulin, 1988; Arrow, Sen, & Suzumura,
2002). multiagent systems literature, utilitarian interpretation concept
social welfare usually taken granted (Rosenschein & Zlotkin, 1994; Sandholm, 1999;
Wooldridge, 2002), i.e. whatever increases average welfare agents inhabiting
society taken beneficial society well. case welfare economics,
c
2006
AI Access Foundation. rights reserved.

fiEndriss, Maudet, Sadri, & Toni

instance, different notions social welfare studied compared
other. Here, concept egalitarian social welfare takes particularly prominent
role (Sen, 1970; Rawls, 1971; Moulin, 1988; Arrow et al., 2002). model, social
welfare tied individual welfare weakest member society, facilitates
incorporation notion fairness resource allocation process.
discussion respective advantages drawbacks different notions social welfare
social sciences tends dominated ethical considerations,1 context
societies artificial software agents choice suitable formal tool modelling
social welfare boils clear-cut (albeit necessarily simple) technical design
decision (Endriss & Maudet, 2004). Indeed, different applications may call different
social criteria. instance, application studied Lematre, Verfaillie, Bataille
(1999), agents need agree access earth observation satellite
funded jointly owners agents, important one
receives fair share common resource. Here, society governed egalitarian
principles may appropriate. electronic commerce application running
Internet agents little commitments towards other,
hand, egalitarian principles seem little relevance. scenario, utilitarian social
welfare would provide appropriate reflection overall profit generated. Besides
utilitarian egalitarian social welfare, also going discuss notions Pareto
Lorenz optimality (Moulin, 1988), well envy-freeness (Brams & Taylor, 1996).
paper, study effect negotiation resources society
number different interpretations concept social welfare. particular, show
certain classes deals regarding exchange resources allow us guarantee
socially optimal allocation resources reached eventually. convergence results
may interpreted emergence particular global behaviour (at level society)
reaction local behaviour governed negotiation strategies individual agents
(which determine kinds deals agents prepared accept). work described
complementary large body literature mechanism design game-theoretical
models negotiation multiagent systems (see e.g. Rosenschein & Zlotkin, 1994; Kraus,
2001; Fatima, Wooldridge, & Jennings, 2004). work typically concerned
negotiation local level (how design mechanisms provide incentive
individual agents adopt certain negotiation strategy?), address negotiation
global level analysing actions taken agents locally affect overall system
social point view.
shall see, truly multilateral deals involving number agents well
number resources may necessary able negotiate socially optimal allocations
resources. certainly true long use arbitrary utility functions model
preferences individual agents. application domains, however, utility
functions may assumed subject certain restrictions (such additive),
able obtain stronger results show also structurally simpler classes deals
(in particular, deals involving single resource time) sufficient negotiate
socially optimal allocations. Nevertheless, seemingly strong restrictions agents
1. famous example Rawls veil ignorance, thought experiment designed establish constitutes
society (Rawls, 1971).

316

fiNegotiating Socially Optimal Allocations Resources

utility functions (such restriction dichotomous preferences) able show
reduction structural complexity negotiation possible.
approach multiagent resource allocation distributed nature. general,
allocation procedure used find suitable allocation resources could either centralised distributed. centralised case, single entity decides final allocation
resources amongst agents, possibly elicited agents preferences alternative allocations. Typical examples combinatorial auctions (Cramton, Shoham, &
Steinberg, 2006). central entity auctioneer reporting preferences
takes form bidding. truly distributed approaches, hand, allocations
emerge result sequence local negotiation steps. approaches
advantages disadvantages. Possibly important argument favour auctionbased mechanisms concerns simplicity communication protocols required implement mechanisms. Another reason popularity centralised mechanisms
recent push design powerful algorithms combinatorial auctions that,
first time, perform reasonably well practice (Fujishima, Leyton-Brown, & Shoham,
1999; Sandholm, 2002). course, techniques are, principle, also applicable
distributed case, research area yet reached level maturity
combinatorial auctions. important argument centralised approaches
may difficult find agent could assume role auctioneer (for
instance, view computational capabilities view trustworthiness).
line research pursued paper inspired Sandholms work
sufficient necessary contract (i.e. deal) types distributed task allocation (Sandholm,
1998). Since then, developed present authors, colleagues,
others context resource allocation problems (Bouveret & Lang, 2005; Chevaleyre,
Endriss, Estivie, & Maudet, 2004; Chevaleyre, Endriss, Lang, & Maudet, 2005a; Chevaleyre,
Endriss, & Maudet, 2005b; Dunne, 2005; Dunne, Laurence, & Wooldridge, 2004; Dunne,
Wooldridge, & Laurence, 2005; Endriss & Maudet, 2004, 2005; Endriss, Maudet, Sadri,
& Toni, 2003a, 2003b). particular, extended Sandholms framework also
addressing negotiation systems without compensatory side payments (Endriss et al., 2003a),
well agent societies concept social welfare given different interpretation
utilitarian programme (Endriss et al., 2003b; Endriss & Maudet, 2004).
present paper provides comprehensive overview fundamental results, mostly
convergence optimal allocation respect different notions social welfare,
active timely area ongoing research.
remainder paper organised follows. Section 2 introduces basic negotiation framework resource reallocation going consider. gives definitions
central notions allocation, deal, utility, discusses possible restrictions
class admissible deals (both structural terms acceptability individual
agents). Section 2 also introduces various concepts social preference going
consider paper. Subsequent sections analyse specific instances basic negotiation framework (characterised, particular, different criteria acceptability
proposed deal) respect specific notions social welfare. first instance,
agents assumed rational (and myopic) sense never accepting deal
would result negative payoff. Section 3 analyses first variant model
rational negotiation, allows monetary side payments increase range
317

fiEndriss, Maudet, Sadri, & Toni

acceptable deals. shall see, model facilitates negotiation processes maximise
utilitarian social welfare. side payments possible, cannot guarantee outcomes
maximal social welfare, still possible negotiate Pareto optimal allocations.
variant rational model studied Section 4. Section 3 4 also investigate restrictions range utility functions agents may use model
preferences affect convergence results.
second part paper apply methodology agent societies
concept social welfare given different kind interpretation commonly
case multiagent systems literature. Firstly, Section 5 analyse framework
resource allocation negotiation context egalitarian agent societies.
Section 6 discusses variant framework combines ideas utilitarian
egalitarian programme enables agents negotiate Lorenz optimal allocations
resources. Finally, Section 7 introduces idea using elitist model social welfare
applications societies agents merely means enabling least one agent
achieve goal. section also reviews concept envy-freeness discusses
ways measuring different degrees envy.
Section 8 summarises results concludes brief discussion concept
welfare engineering, i.e. idea choosing tailor-made definitions social welfare
different applications designing agents behaviour profiles accordingly.

2. Preliminaries
basic scenario resource allocation negotiation studied paper
artificial society inhabited number agents, initially holds certain
number resources. agents typically ascribe different values (utilities) different bundles resources. may engage negotiation agree reallocation
resources, example, order improve respective individual welfare
(i.e. increase utility). Furthermore, assume interest system
designer distributed negotiation processes somehow also result positive
payoff society whole.
2.1 Basic Definitions
instance abstract negotiation framework consists finite set (at least two)
agents finite set resources R. Resources indivisible non-sharable.
allocation resources partitioning R amongst agents A.
Definition 1 (Allocations) allocation
resources function subsets
R A(i) A(j) = { } 6= j iA A(i) = R.
example, given allocation A(i) = {r3 , r7 }, agent would resources r3
r7 . Given particular allocation resources, agents may agree (multilateral) deal
exchange resources currently hold. general case, numbers
agents resources could involved single deal. abstract point view,
deal takes us one allocation resources next. is, may characterise
deal pair allocations.
318

fiNegotiating Socially Optimal Allocations Resources

Definition 2 (Deals) deal pair = (A, A0 ) A0 allocations
resources 6= A0 .
set agents involved deal = (A, A0 ) given = {i | A(i) 6= A0 (i)}.
composition two deals defined follows: 1 = (A, A0 ) 2 = (A0 , A00 ),
1 2 = (A, A00 ). given deal composition two deals concern disjoint sets
agents, deal said independently decomposable.
Definition 3 (Independently decomposable deals) deal called independently
decomposable iff exist deals 1 2 = 1 2 A1 A2 = { }.
Observe = (A, A0 ) independently decomposable exists intermediate
allocation B different A0 intersection {i | A(i) 6= B(i)}
{i | B(i) 6= A0 (i)} empty, i.e. union {i | A(i) = B(i)}
{i | B(i) = A0 (i)} full set agents A. Hence, = (A, A0 ) independently
decomposable implies exists allocation B different A0
either B(i) = A(i) B(i) = A0 (i) agents (we going use fact
proofs necessity theorems later on).
value agent ascribes particular set resources R modelled
means utility function, is, function sets resources real numbers.
going consider general utility functions (without restrictions) several
specific classes functions.
Definition 4 (Utility functions) Every agent equipped utility function
ui : 2R R. going consider following restricted classes utility functions:
ui non-negative iff ui (R) 0 R R.
ui positive iff non-negative ui (R) 6= 0 R R R 6= { }.
ui monotonic iff R1 R2 implies ui (R1 ) ui (R2 ) R1 , R2 R.
P
ui additive iff ui (R) = rR ui ({r}) R R.
ui 0-1 function iff additive ui ({r}) = 0 ui ({r}) = 1 r R.
ui dichotomous iff ui (R) = 0 ui (R) = 1 R R.
Recall that, given allocation A, set A(i) bundle resources held agent
situation. usually going abbreviate ui (A) = ui (A(i)) utility value
assigned agent bundle.
2.2 Deal Types Rationality Criteria
paper investigate kinds negotiation outcomes agents achieve using
different classes deals. class deals may characterised structural constraints
(number agents resources involved, etc.) rationality constraints (relating
changes utility experienced agents involved).
Following Sandholm (1998), distinguish number structurally different types
deals. basic 1-deals, single item passed one agent another.
319

fiEndriss, Maudet, Sadri, & Toni

Definition 5 (1-deals) 1-deal deal involving reallocation exactly one resource.
corresponds classical form contract typically found Contract Net
protocol (Smith, 1980). Deals one agent passes set resources another
agent called cluster deals. Deals one agent gives single item another agent
returns another single item called swap deals. Sometimes also necessary
exchange resources two agents. Sandholms terminology,
multiagent deal deal could involve number agents, agent passes
one resource agents taking part. Finally, deals combine
features cluster multiagent deal type called combined deals Sandholm.
could involve number agents number resources. Therefore, every
deal , sense Definition 2, combined deal. remainder paper,
speaking deals without specifying type, always going refer
combined deals (without structural restrictions).2
agent may may find particular deal acceptable. Whether agent
accept given deal depends rationality criterion applies evaluating deals.
selfish agent may, instance, accept deals = (A, A0 ) strictly improve
personal welfare: ui (A) < ui (A0 ). call criteria this, depend
utilities agent question, personal rationality criteria. want
admit arbitrary rationality criteria, classes deals characterised using
personal rationality criteria alone somewhat narrow purposes. Instead,
going consider rationality criteria local sense depending
utility levels agents involved deal concerned.
Definition 6 (Local rationality criteria) class deals said characterised
local rationality criterion iff possible define predicate 2ARR
deal = (A, A0 ) belongs iff ({(i, ui (A), ui (A0 )) | }) holds true.
is, mapping set triples one agent name two reals (utilities)
truth values. locality aspect comes applying set triples
agents whose bundle changes . Therefore, instance, class deals
increase utility previously poorest agent characterisable local
rationality criterion (because condition checked inspecting utilities
agents system).
2.3 Socially Optimal Allocations Resources
already mentioned introduction, may think multiagent system society
autonomous software agents. agents make local decisions deals
propose accept, also analyse system global societal point
view may thus prefer certain allocations resources others. end, welfare
economics provides formal tools assess distribution resources amongst
members society affects well-being society whole (Sen, 1970; Moulin, 1988;
Arrow et al., 2002).
2. ontology deal types discussed is, course, exhaustive. would, instance, also
interest consider class bilateral deals (involving exactly two agents number items).

320

fiNegotiating Socially Optimal Allocations Resources

Given preference profiles individual agents society (which, framework, represented means utility functions), social welfare ordering
alternative allocations resources formalises notion societys preferences. Next
going formally introduce important social welfare orderings considered
paper (some additional concepts social welfare discussed towards end
paper). cases, social welfare best defined terms collective utility function.
One example notion utilitarian social welfare.
Definition 7 (Utilitarian social welfare) utilitarian social welfare swu (A)
allocation resources defined follows:
X
swu (A) =
ui (A)
iA

Observe maximising collective utility function swu amounts maximising
average utility enjoyed agents system. Asking maximal utilitarian social
welfare strong requirement. somewhat weaker concept Pareto optimality. allocation Pareto optimal iff allocation higher utilitarian
social welfare would worse agents system (i.e. would
strictly better least one agent without worse others).
Definition 8 (Pareto optimality) allocation called Pareto optimal iff
allocation A0 swu (A) < swu (A0 ) ui (A) ui (A0 ) A.
first goal egalitarian society increase welfare weakest
member (Rawls, 1971; Sen, 1970). words, measure social welfare
society measuring welfare agent currently worst off.
Definition 9 (Egalitarian social welfare) egalitarian social welfare swe (A)
allocation resources defined follows:
swe (A) = min{ui (A) | A}
egalitarian collective utility function swe gives rise social welfare ordering
alternative allocations resources: A0 strictly preferred iff swe (A) < swe (A0 ).
ordering sometimes called maximin-ordering. maximin-ordering takes
account welfare currently weakest agent, insensitive utility fluctuations
rest society. allow finer distinction social welfare different
allocations introduce so-called leximin-ordering.
society n agents, let {u1 , . . . , un } set utility functions
society. every allocation determines utility vector hu1 (A), . . . , un (A)i length n.
rearrange elements vector increasing order obtain ordered utility
vector allocation A, going denote ~u(A). number ~ui (A) ith
element vector (for 1 |A|). is, ~u1 (A) instance, utility value
assigned allocation currently weakest agent. declare lexicographic
ordering vectors real numbers (such ~u(A)) usual way: ~x lexicographically
precedes ~y iff ~x (proper) prefix ~y ~x ~y share common (proper) prefix length
k (which may 0) ~xk+1 < ~yk+1 .
321

fiEndriss, Maudet, Sadri, & Toni

Definition 10 (Leximin-ordering) leximin-ordering alternative allocations
resources defined follows:
A0

~u(A) lexicographically precedes ~u(A0 )

iff

write A0 iff either A0 ~u(A) = ~u(A0 ). allocation resources called
leximin-maximal iff allocation A0 A0 .
Finally, introduce concept Lorenz domination, social welfare ordering
combines utilitarian egalitarian aspects social welfare. basic idea endorse
deals result improvement respect utilitarian welfare without causing
loss egalitarian welfare, vice versa.
Definition 11 (Lorenz domination) Let A0 allocations society n
agents. Lorenz dominated A0 iff
k
X

~ui (A)

i=1

k
X

~ui (A0 )

i=1

k 1 k n and, furthermore, inequality strict least one k.
k 1 k n, sum referred definition sum
utility values assigned respective allocation resources k weakest agents.
k = 1, sum equivalent egalitarian social welfare allocation. k = n,
equivalent utilitarian social welfare. allocation resources called Lorenz
optimal iff Lorenz dominated allocation.
illustrate social welfare concepts (and use ordered utility
vectors) means example. Consider society three agents two resources,
agents utility functions given following table:
u1 ({ }) = 0
u1 ({r1 }) = 5
u1 ({r2 }) = 3
u1 ({r1 , r2 }) = 8

u2 ({ }) = 0
u2 ({r1 }) = 4
u2 ({r2 }) = 2
u2 ({r1 , r2 }) = 17

u3 ({ }) = 0
u3 ({r1 }) = 2
u3 ({r2 }) = 6
u3 ({r1 , r2 }) = 7

First all, observe egalitarian social welfare 0 possible allocation
scenario, least one agents would get resources all. Let
allocation agent 2 holds full bundle resources. Observe
allocation maximal utilitarian social welfare. corresponding utility vector
h0, 17, 0i, i.e. ~u(A) = h0, 0, 17i. Furthermore, let A0 allocation agent 1 gets
r1 , agent 2 gets r2 , agent 3 content empty bundle. get
ordered utility vector h0, 2, 5i. initial element either vector 0, 0 < 2, i.e.
~u(A) lexicographically precedes ~u(A0 ). Hence, get A0 , i.e. A0 would socially
preferred allocation respect leximin-ordering. Furthermore, A0
Pareto optimal neither Lorenz-dominated other. Starting allocation A0 ,
agents 1 2 swapping respective bundles would result allocation
ordered utility vector h0, 3, 4i, i.e. move would result Lorenz improvement.
322

fiNegotiating Socially Optimal Allocations Resources

3. Rational Negotiation Side Payments
section, going discuss first instance general framework resource
allocation negotiation set earlier. particular variant, shall refer
model rational negotiation side payments (or simply money), equivalent
framework put forward Sandholm agents negotiate order reallocate
tasks (Sandholm, 1998). variant framework, aim negotiate
allocations maximal utilitarian social welfare.
3.1 Individual Rationality
instance negotiation framework, deal may accompanied number
monetary side payments compensate agents involved accepting loss
utility. Rather specifying pair agents much money former
supposed pay latter, simply say much money agent either pays
receives. modelled using call payment function.
Definition 12 (Payment functions) payment function function p real
numbers satisfying following condition:
X
p(i) = 0
iA

Here, p(i) > 0 means agent pays amount p(i), p(i) < 0 means
receives amount p(i). definition payment function, sum payments
0, i.e. overall amount money present system change.3
rational negotiation model, agents self-interested sense proposing accepting deals strictly increase welfare (for justification
approach refer Sandholm, 1998). myopic notion individual rationality may
formalised follows.
Definition 13 (Individual rationality) deal = (A, A0 ) called individually rational
iff exists payment function p ui (A0 ) ui (A) > p(i) A, except
possibly p(i) = 0 agents A(i) = A0 (i).
is, agent prepared accept deal iff pay less gain
utility get paid loss utility, respectively. agents
affected deal, i.e. case A(i) = A0 (i), may payment all. example,
ui (A) = 8 ui (A0 ) = 5, utility agent would reduced 3 units
accept deal = (A, A0 ). Agent agree deal accompanied
side payment 3 units; is, payment function p satisfies 3 > p(i).
given deal, usually range possible side payments. agents
manage agree particular one matter consideration abstract level
discussing framework here. assume deal go ahead
long exists suitable payment function p. point
3. overall amount money present system stays constant throughout negotiation process,
makes sense take account evaluation social welfare.

323

fiEndriss, Maudet, Sadri, & Toni

assumption may justified circumstances. instance, utility functions
publicly known agents risk-takers, potential deal may identified
such, agents may understate interest deal order
maximise expected payoff (Myerson & Satterthwaite, 1983). Therefore, theoretical
results reachability socially optimal allocations reported apply
assumption strategic considerations prevent agents making
mutually beneficial deals.
3.2 Example
example, consider system two agents, agent 1 agent 2, set two
resources R = {r1 , r2 }. following table specifies values utility functions u1
u2 every subset {r1 , r2 }:
u1 ({ }) = 0
u1 ({r1 }) = 2
u1 ({r2 }) = 3
u1 ({r1 , r2 }) = 7

u2 ({ }) = 0
u2 ({r1 }) = 3
u2 ({r2 }) = 3
u2 ({r1 , r2 }) = 8

Also suppose agent 1 initially holds full set resources {r1 , r2 } agent 2
resources begin with.
utilitarian social welfare initial allocation 7, could 8, namely
agent 2 resources. going see next, simple class 1-deals alone
always sufficient guarantee optimal outcome negotiation process (if agents
abide individual rationality criterion acceptability deal). example,
possible 1-deals would pass either r1 r2 agent 1 agent 2. either
case, loss utility incurred agent 1 (5 4, respectively) would outweigh gain
agent 2 (3 either deal), payment function would make deals
individually rational. cluster deal passing {r1 , r2 } agent 1 2,
hand, would individually rational agent 2 paid agent 1 amount of, say, 7.5 units.
Similarly example above, also construct scenarios swap deals
multiagent deals necessary (i.e. cluster deals alone would sufficient
guarantee maximal social welfare). also follows Theorem 2, going
present later section. Several concrete examples given Sandholm (1998).
3.3 Linking Individual Rationality Social Welfare
following result, first stated form Endriss et al. (2003a), says deal (with
money) individually rational iff increases utilitarian social welfare. mainly going
use lemma give simple proof Sandholms main result sufficient contract
types (Sandholm, 1998), also found useful applications right (Dunne
et al., 2005; Dunne, 2005; Endriss & Maudet, 2005).
Lemma 1 (Individually rational deals utilitarian social welfare) deal =
(A, A0 ) individually rational iff swu (A) < swu (A0 ).
324

fiNegotiating Socially Optimal Allocations Resources

Proof. : definition, = (A, A0 ) individually rational iff exists payment
function p ui (A0 ) ui (A) > p(i) holds A, except possibly p(i) = 0
case A(i) = A0 (i). add inequations agents get:
X

(ui (A0 ) ui (A)) >

iA

X

p(i)

iA

definition payment function, righthand side equates 0 while, definition
utilitarian social welfare, lefthand side equals swu (A0 ) swu (A). Hence, really get
swu (A) < swu (A0 ) claimed.
: let swu (A) < swu (A0 ). show = (A, A0 ) individually
rational deal. done prove exists payment function p
ui (A0 ) ui (A) > p(i) A. define function p : R follows:
p(i) = ui (A0 ) ui (A)

swu (A0 ) swu (A)
|A|

(for A)

P
First, observe p really payment function, get iA p(i) = 0. also
get ui (A0 ) ui (A) > p(i) A, swu (A0 ) swu (A0 ) > 0. Hence,
must indeed individually rational deal.
2
Lemma 1 suggests function swu indeed provide appropriate measure
social well-being societies agents use notion individual rationality (as given
Definition 13) guide behaviour negotiation. also shows individual
rationality indeed local rationality criterion sense Definition 6.
3.4 Maximising Utilitarian Social Welfare
next aim show sequence deals rational negotiation model
side payments converge allocation maximal utilitarian social welfare;
is, class individually rational deals (as given Definition 13) sufficient
guarantee optimal outcomes agent societies measuring welfare according utilitarian
programme (Definition 7). originally shown Sandholm (1998)
context framework rational agents negotiate order reallocate tasks
global aim minimise overall costs carrying tasks.
Theorem 1 (Maximal utilitarian social welfare) sequence individually rational deals eventually result allocation maximal utilitarian social welfare.
Proof. Given set agents well set resources R required
finite, finite number distinct allocations resources. Furthermore, Lemma 1, individually rational deal strictly increase utilitarian social
welfare. Hence, negotiation must terminate finite number deals. sake
contradiction, assume terminal allocation maximal utilitarian
social welfare, i.e. exists another allocation A0 swu (A) < swu (A0 ). then,
Lemma 1, deal = (A, A0 ) would individually rational thereby possible,
contradicts earlier assumption terminal allocation.
2
325

fiEndriss, Maudet, Sadri, & Toni

first sight, result may seem almost trivial. notion multilateral deal without
structural restrictions powerful one. single deal allows number
resources moved number agents. point view,
particularly surprising always reach optimal allocation (even single
step!). Furthermore, finding suitable deal complex task, may always
viable practice. crucial point Theorem 1 sequence deals
result optimal allocation. is, whatever deals agreed early stages
negotiation, system never get stuck local optimum finding allocation
maximal social welfare remains option throughout (provided, course, agents
actually able identify deal theoretically possible). Given restriction
deals individually rational agents involved, social welfare must increase
every single deal. Therefore, negotiation always pays off, even stop early
due computational limitations.
issue complexity still important one. full range deals large
managed practice, important investigate close get finding
optimal allocation restrict set allowed deals certain simple patterns.
Andersson Sandholm (2000), instance, conducted number experiments
sequencing certain contract/deal types reach best possible allocations within
limited amount time. complexity-theoretic analysis problem deciding
whether possible reach optimal allocation means structurally simple types
deals (in particular 1-deals), refer recent work Dunne et al. (2005).
3.5 Necessary Deals
next theorem improves upon Sandholms main result regarding necessary contract
types (Sandholm, 1998), extending cases either utility functions
monotonic utility functions dichotomous.4 Sandholms original result, translated
terminology, states system (consisting set agents
set resources R) (not independently decomposable) deal system,
possible construct utility functions choose initial allocation resources
necessary reach optimal allocation, agents agree individually
rational deals. findings insufficiency certain types contracts reported
Sandholm (1998) may considered corollaries this. instance, fact that,
say, cluster deals alone sufficient guarantee optimal outcomes follows
theorem take particular swap deal system question.
Theorem 2 (Necessary deals side payments) Let sets agents resources fixed. every deal independently decomposable, exist
utility functions initial allocation sequence individually rational
deals leading allocation maximal utilitarian social welfare must include .
continues case even either utility functions required monotonic
utility functions required dichotomous.
4. fact, theorem sharpens also also corrects mistake previous expositions
result (Sandholm, 1998; Endriss et al., 2003a), restriction deals independently
decomposable omitted.

326

fiNegotiating Socially Optimal Allocations Resources

Proof. Given set agents set resources R, let = (A, A0 ) 6= A0
deal system. need show collection utility functions
initial allocation necessary reach allocation maximal social
welfare. would case A0 maximal social welfare, second highest
social welfare, initial allocation resources.
first prove existence collection functions case utility
functions required monotonic. Fix 0 < < 1. 6= A0 ,
must agent j A(j) 6= A0 (j). define utility functions ui
agents sets resources R R follows:

|R| + R = A0 (i) (R = A(i) 6= j)
ui (R) =
|R|
otherwise
Observe ui monotonic utility function every A. get swu (A0 ) = |R|+|A|
swu (A) = swu (A0 ) . = (A, A0 ) individually decomposable,
exists allocation B different A0 either B(i) = A(i) B(i) =
A0 (i) agents A. Hence, swu (B) swu (A) allocation B.
is, A0 (unique) allocation maximal social welfare allocation
higher social welfare A. Therefore, make initial allocation
= (A, A0 ) would deal increasing social welfare. Lemma 1, means
would individually rational (and thereby possible) deal. Hence,
indeed necessary achieve maximal utilitarian social welfare.
proof case dichotomous utility functions similar; need
show suitable collection dichotomous utility functions constructed. Again,
let j agent A(j) 6= A0 (j). use following collection functions:

1 R = A0 (i) (R = A(i) 6= j)
ui (R) =
0 otherwise
get swu (A0 ) = |A|, swu (A) = swu (A0 )1 swu (B) swu (A) allocations
B. Hence, socially optimal and, initial allocation, would
deal individually rational.
2
stress set deals independently decomposable includes
deals involving number agents and/or number resources. Hence, Theorem 2,
negotiation protocol puts restrictions structural complexity deals
may proposed fail guarantee optimal outcomes, even constraints
either time computational resources. emphasises high complexity
negotiation framework (see also Dunne et al., 2005; Chevaleyre et al., 2004; Endriss &
Maudet, 2005; Dunne, 2005). fact necessity (almost) full range deals
persists, even utility functions subject certain restrictions makes result
even striking. true particular case dichotomous functions,
sense simplest class utility functions (as distinguish
good bad bundles).
see restriction deals independently decomposable matters,
consider scenario four agents two resources. deal moving r1
agent 1 agent 2, r2 agent 3 agent 4 individually rational,
327

fiEndriss, Maudet, Sadri, & Toni

either one two subdeals moving either r1 agent 1 agent 2 r2
agent 3 agent 4. Hence, deal (which independently decomposable) cannot
necessary sense Theorem 2 (with reference proof above, case
allocations B either B(i) = A(i) B(i) = A0 (i) agents A,
i.e. could get swu (B) = swu (A0 )).
3.6 Additive Scenarios
Theorem 2 negative result, shows deals complexity may
required guarantee optimal outcomes negotiation. partly consequence
high degree generality framework. Section 2.1, defined utility functions
arbitrary functions sets resources real numbers. many application domains
may unnecessarily general even inappropriate may able obtain
stronger results specific classes utility functions subject certain restrictions.
course, already seen case either monotonicity (possibly
natural restriction) dichotomy (possibly severe restriction).
Here, going consider case additive utility functions, appropriate domains combining resources result synergy effects (in
sense increasing agents welfare). refer systems agents additive
utility functions additive scenarios. following theorem shows additive
scenarios 1-deals sufficient guarantee outcomes maximal social welfare.5
Theorem 3 (Additive scenarios) additive scenarios, sequence individually rational 1-deals eventually result allocation maximal utilitarian social welfare.
Proof. Termination shown Theorem 1. going show that, whenever
current allocation maximal social welfare, still possible 1-deal
individually rational.
additive domains, utilitarian social welfare given allocation may computed
adding appropriate utility values single resources R.
allocation A, let fA function mapping resource r R agent
holds r situation (that is, fA (r) = iff r A(i)). utilitarian social welfare
allocation given following formula:
X
swu (A) =
ufA (r) ({r})
rR

suppose negotiation terminated allocation
individually rational 1-deals possible. Furthermore, sake contradiction, assume
allocation maximal social welfare, i.e. exists another allocation
A0 swu (A) < swu (A0 ). then, characterisation social welfare
additive scenarios, must least one resource r R ufA (r) ({r}) <
ufA0 (r) ({r}). is, 1-deal passing r agent fA (r) agent fA0 (r) would
increase social welfare. Therefore, Lemma 1, must individually rational deal,
i.e. contrary earlier assumption, cannot terminal allocation. Hence, must
allocation maximal utilitarian social welfare.
2
5. also observed T. Sandholm (personal communication, September 2002).

328

fiNegotiating Socially Optimal Allocations Resources

conclude section briefly mentioning two recent results extend,
different ways, result stated Theorem 3 (a detailed discussion, however, would
beyond scope present paper). first results shows rational
deals involving k resources sufficient convergence allocation
maximal social welfare whenever utility functions additively separable
respect common partition R i.e. synergies across different parts partition
possible overall utility defined sum utilities different sets
partition (Fishburn, 1970) set partition k elements
(Chevaleyre et al., 2005a).
second result concerns maximality property utility functions respect
1-deals. Chevaleyre et al. (2005b) show class modular utility functions,
slightly general class additive functions considered (namely,
possible assign non-zero utility empty bundle), maximal sense
class functions strictly including class modular functions would still possible
guarantee agents using utility functions larger class negotiating
individually rational 1-deals eventually reach allocation maximal utilitarian
social welfare cases.

4. Rational Negotiation without Side Payments
implicit assumption made framework presented far every
agent got unlimited amount money available able pay agents
whenever required deal would increase utilitarian social welfare. Concretely,
initial allocation A0 allocation maximal utilitarian social welfare,
agent may require amount money difference ui (A0 ) ui (A)
able get negotiation process. context task contracting,
framework proposed originally (Sandholm, 1998), may justifiable,
resource allocation problems seems questionable make assumptions
unlimited availability one particular resource, namely money. section, therefore
investigate extent theoretical results discussed previous section persist
apply consider negotiation processes without monetary side payments.
4.1 Example
scenario without money, is, allow compensatory payments,
cannot always guarantee outcome maximal utilitarian social welfare. see this,
consider following simple problem system two agents, agent 1 agent 2,
single resource r. agents utility functions defined follows:
u1 ({ }) = 0
u1 ({r}) = 4

u2 ({ }) = 0
u2 ({r}) = 7

suppose agent 1 initially owns resource. passing r agent 1 agent 2
would increase utilitarian social welfare amount 3. framework money,
agent 2 could pay agent 1, say, amount 5.5 units deal would individually
rational them. Without money (i.e. p 0), however, individually rational
deal possible negotiation must terminate non-optimal allocation.
329

fiEndriss, Maudet, Sadri, & Toni

maximising social welfare generally possible, instead going investigate
whether Pareto optimal outcome (see Definition 8) possible framework without
money, types deals sufficient guarantee this.
4.2 Cooperative Rationality
become clear due course, order get desired convergence result, need
relax notion individual rationality little. framework without money,
also want agents agree deal, least maintains utility (that is, strict
increase necessary). However, still going require least one agent strictly
increase utility. could, instance, agent proposing deal question.
call deals conforming criterion cooperatively rational.6
Definition 14 (Cooperative rationality) deal = (A, A0 ) called cooperatively rational iff ui (A) ui (A0 ) agent j uj (A) < uj (A0 ).
analogy Lemma 1, still swu (A) < swu (A0 ) deal = (A, A0 )
cooperatively rational, vice versa. call instance negotiation framework
deals cooperatively rational (and hence include monetary component)
model rational negotiation without side payments.
4.3 Ensuring Pareto Optimal Outcomes
next theorem show, class cooperatively rational deals sufficient
guarantee Pareto optimal outcome money-free negotiation. constitutes analogue
Theorem 1 model rational negotiation without side payments.
Theorem 4 (Pareto optimal outcomes) sequence cooperatively rational deals
eventually result Pareto optimal allocation resources.
Proof. Every cooperatively rational deal strictly increases utilitarian social welfare (this
need condition least one agent behaves truly individually rational
deal). Together fact finitely many different allocations
resources, implies negotiation process eventually terminate.
sake contradiction, assume negotiation ends allocation A, Pareto
optimal. latter means exists another allocation A0 swu (A) < swu (A0 )
ui (A) ui (A0 ) A. ui (A) = ui (A0 ) A, also
swu (A) = swu (A0 ); is, must least one j uj (A) < uj (A0 ).
deal = (A, A0 ) would cooperatively rational, contradicts assumption
terminal allocation.
2
Observe proof would gone deals required strictly
rational (without side payments), would necessitate ui (A) < ui (A0 ) A.
Cooperative rationality means, instance, agents would prepared give away
resources assign utility value 0, without expecting anything return.
6. Note cooperatively rational agents still essentially rational. willingness cooperate
extends cases benefit others without loss utility themselves.

330

fiNegotiating Socially Optimal Allocations Resources

framework money, another agent could always offer agent infinitesimally
small amount money, would accept deal.
Therefore, proposed weakened notion rationality seems indeed reasonable
price pay giving money.
4.4 Necessity Result
next result shows, also framework without side payments, deals
structural complexity may necessary order able guarantee optimal outcome
negotiation.7 Theorem 5 improves upon previous results (Endriss et al., 2003a)
showing necessity property persists also either utility functions belong
class monotonic functions utility functions belong class dichotomous
functions.
Theorem 5 (Necessary deals without side payments) Let sets agents resources fixed. every deal independently decomposable, exist
utility functions initial allocation sequence cooperatively rational
deals leading Pareto optimal allocation would include . continues
case even either utility functions required monotonic utility
functions required dichotomous.
Proof. details proof omitted possible simply reuse construction
used proof Theorem 2. Observe utility functions defined also
guarantee ui (A) ui (A0 ) A, i.e. Pareto optimal, A0 is.
make initial allocation, = (A, A0 ) would cooperatively rational
deal (as every deal would decrease social welfare).
2
4.5 0-1 Scenarios
conclude study rational negotiation framework without side payments
identifying class utility functions able achieve reduction structural
complexity.8 Consider scenario agents use additive utility functions assign
either 0 1 every single resource (this call 0-1 functions).9 may
appropriate simply wish distinguish whether agent needs particular
resource (to execute given plan, example). is, instance, case
agents defined work Sadri, Toni, Torroni (2001). following theorem
shows, 0-1 scenarios (i.e. systems utility functions 0-1 functions),
7. theorem corrects mistake original statement result (Endriss et al., 2003a),
restriction deals independently decomposable omitted.
8. dichotomous functions special case non-negative functions, full range (not independently decomposable) deals also necessary scenarios non-negative functions. Interestingly,
changes restrict positive utility functions. result Theorem 5 would
hold anymore, deal would involve particular agent (with positive utility function)
giving away resources without receiving anything return could never cooperatively rational.
Hence, Theorem 4, deal could never necessary achieve Pareto optimal allocation either.
9. Recall distinction 0-1 functions dichotomous functions. latter assign either 0 1
bundle, former assign either 0 1 individual resource (the utilities bundles
follow fact 0-1 functions additive).

331

fiEndriss, Maudet, Sadri, & Toni

1-deals sufficient guarantee convergence allocation maximal utilitarian
social welfare, even framework without monetary side payments (where deals
required cooperatively rational).
Theorem 6 (0-1 scenarios) 0-1 scenarios, sequence cooperatively rational 1deals eventually result allocation maximal utilitarian social welfare.
Proof. Termination shown proof Theorem 4. allocation
maximal social welfare must case agent holds resource r
ui ({r}) = 0 another agent j system uj ({r}) = 1. Passing r
j would cooperatively rational deal, either negotiation yet terminated
indeed situation maximal utilitarian social welfare.
2
result may interpreted formal justification negotiation strategies
proposed Sadri et al. (2001).

5. Egalitarian Agent Societies
section, going apply methodology used study
optimal outcomes negotiation systems designed according utilitarian principles
first part paper analysis egalitarian agent societies. classical
counterpart utilitarian collective utility function swu egalitarian collective
utility function swe introduced Definition 9 (Moulin, 1988; Sen, 1970; Rawls, 1971).
Therefore, going study design agent societies negotiation
guaranteed converge allocation resources maximal egalitarian social welfare.
first aim identify suitable criterion agents inhabiting egalitarian
agent society may use decide whether accept particular deal. Clearly, cooperatively rational deals, instance, would ideal choice, Pareto optimal
allocations typically optimal egalitarian point view (Moulin, 1988).
5.1 Pigou-Dalton Transfers Equitable Deals
searching economics literature class deals would benefit society
egalitarian system soon encounter Pigou-Dalton transfers. Pigou-Dalton
principle states whenever utility transfer two agents takes place
reduces difference utility two, transfer considered
socially beneficial (Moulin, 1988). context framework, Pigou-Dalton transfer
(between agents j) defined follows.
Definition 15 (Pigou-Dalton transfers) deal = (A, A0 ) called Pigou-Dalton
transfer iff satisfies following criteria:
two agents j involved deal: = {i, j}.
deal mean-preserving: ui (A) + uj (A) = ui (A0 ) + uj (A0 ).
deal reduces inequality: |ui (A0 ) uj (A0 )| < |ui (A) uj (A)|.
332

fiNegotiating Socially Optimal Allocations Resources

second condition definition could relaxed postulate ui (A) + uj (A)
ui (A0 ) + uj (A0 ), also allow inequality-reducing deals increase overall utility.
Pigou-Dalton transfers capture certain egalitarian principles; sufficient
acceptability criteria guarantee negotiation outcomes maximal egalitarian social
welfare? Consider following example:
u1 ({ }) = 0
u1 ({r1 }) = 3
u1 ({r2 }) = 12
u1 ({r1 , r2 }) = 15

u2 ({ }) = 0
u2 ({r1 }) = 5
u2 ({r2 }) = 7
u2 ({r1 , r2 }) = 17

first agent attributes relatively low utility value r1 high one r2 . Furthermore, value resources together simply sum individual utilities, i.e.
agent 1 using additive utility function (no synergy effects). second agent ascribes
medium value either resource high value full set. suppose
initial allocation resources A(1) = {r1 } A(2) = {r2 }. inequality
index allocation |u1 (A) u2 (A)| = 4. easily check inequality
fact minimal allocation (which means inequality-reducing deal,
certainly Pigou-Dalton transfer, given allocation). However, allocation A0
A0 (1) = {r2 } A0 (2) = {r1 } would result higher level egalitarian social welfare
(namely 5 instead 3). Hence, Pigou-Dalton transfers alone sufficient guarantee
optimal outcomes negotiations egalitarian agent societies. need general
acceptability criterion.
Intuitively, agents operating according egalitarian principles help
fellow agents worse (as long afford
without ending even worse). means, purpose exchange
resources improve welfare weakest agent involved respective
deal. formalise idea introducing class equitable deals.
Definition 16 (Equitable deals) deal = (A, A0 ) called equitable iff satisfies
following criterion:
min{ui (A) | } < min{ui (A0 ) | }
Recall = {i | A(i) 6= A0 (i)} denotes set agents involved deal .
Given = (A, A0 ) deal require 6= A0 , never empty set
(i.e. minima referred definition well-defined). Note equitability
local rationality criterion sense Definition 6.
easy see Pigou-Dalton transfer also equitable deal,
always result improvement weaker one two agents concerned.
converse, however, hold (not even restrict deals involving
two agents). fact, equitable deals may even increase inequality agents
concerned, namely cases happier agent gains utility weaker does.
literature multiagent systems, autonomy agent (one central
features distinguishing multiagent systems distributed systems) sometimes
equated pure selfishness. interpretation agent paradigm,
333

fiEndriss, Maudet, Sadri, & Toni

notion equitability would, course, make little sense. believe, however,
useful distinguish different degrees autonomy. agent may well autonomous
decision general, still required follow certain rules imposed society (and
agreed agent entering society).
5.2 Local Actions Global Effects
going prove two lemmas provide connection local
acceptability criterion given notion equitability two egalitarian social
welfare orderings introduced Section 2.3 (i.e. maximin-ordering induced swe
well leximin-ordering).
first lemma shows global changes reflected locally. deal happens
increase (global) egalitarian social welfare, is, results rise respect
maximin-ordering, deal fact equitable deal.
Lemma 2 (Maximin-rise implies equitability) A0 allocations
swe (A) < swe (A0 ), = (A, A0 ) equitable deal.
Proof. Let A0 allocations swe (A) < swe (A0 ) let = (A, A0 ).
agent minimal utility allocation must involved , egalitarian social
welfare, thereby agents individual utility, higher allocation A0 . is,
min{ui (A) | } = swe (A). Furthermore, A, certainly
swe (A0 ) min{ui (A0 ) | }. Given original assumption swe (A) < swe (A0 ),
obtain inequation min{ui (A) | } < min{ui (A0 ) | }. shows
indeed equitable deal.
2
Observe converse hold; every equitable deal necessarily increase
egalitarian social welfare (although equitable deal never decrease egalitarian social
welfare either). is, instance, case agents currently better
involved deal. fact, argued already end Section 2.2,
class deals characterisable local rationality criterion (see Definition 6) would
always result increase egalitarian social welfare.
able detect changes welfare resulting equitable deal require
finer differentiation alternative allocations resources given leximinordering. fact, shall see next, equitable deal shown result strict
improvement respect leximin-ordering.
Lemma 3 (Equitability implies leximin-rise) = (A, A0 ) equitable deal,
A0 .
Proof. Let = (A, A0 ) deal satisfies equitability criterion define =
min{ui (A) | }. value may considered partitioning ordered utility
vector ~u(A) three subvectors: Firstly, ~u(A) got (possibly empty) prefix ~u(A)<
elements strictly lower . middle, got subvector ~u(A)=
(with least one element) elements equal . Finally, ~u(A) got suffix
~u(A)> (which may empty) elements strictly greater .
334

fiNegotiating Socially Optimal Allocations Resources

definition , deal cannot affect agents whose utility values belong ~u(A)< .
Furthermore, definition equitability, < min{ui (A0 ) | }, means
agents involved end utility value strictly
greater , least one agents come ~u(A)= . collect
information ~u(A0 ), ordered utility vector second allocation A0 . Firstly,
prefix ~u(A0 )< identical ~u(A)< . followed (possibly empty)
subvector ~u(A0 )= elements equal must strictly shorter
~u(A)= . remaining elements ~u(A0 ) strictly greater . follows
~u(A) lexicographically precedes ~u(A0 ), i.e. A0 holds claimed.
2
Again, converse hold, i.e. every deal resulting leximin-rise necessarily
equitable. Counterexamples deals utility value weakest agent involved
stays constant, despite improvement respect leximin-ordering
level society.
well-known result welfare economics states every Pigou-Dalton utility transfer
results leximin-rise (Moulin, 1988). Given observed earlier every deal
amounts Pigou-Dalton transfer also equitable deal, result
also regarded simple corollary Lemma 3.
5.3 Maximising Egalitarian Social Welfare
next aim prove convergence result egalitarian framework (in analogy
Theorems 1 4). going show systems agents negotiate equitable
deals always converge towards allocation maximal egalitarian social welfare.
Theorem 7 (Maximal egalitarian social welfare) sequence equitable deals
eventually result allocation resources maximal egalitarian social welfare.
Proof. Lemma 3, equitable deal result strict rise respect leximinordering (which irreflexive transitive). Hence, finite number
distinct allocations, negotiation terminate finite number deals.
suppose negotiation terminated equitable deals possible. Let
corresponding terminal allocation resources. claim allocation
maximal egalitarian social welfare. sake contradiction, assume not, i.e.
assume exists another allocation A0 system swe (A) < swe (A0 ).
then, Lemma 2, deal = (A, A0 ) equitable deal. Hence, still
possible deal, namely , contradicts earlier assumption terminal
allocation. shows allocation maximal egalitarian social welfare,
proves claim.
2
purely practical point view, Theorem 7 may lesser interest
corresponding results utilitarian systems, refer acceptability
criterion depends single agent. course, coincides intuitions egalitarian societies: maximising social welfare possible means
cooperation sharing information agents preferences.
reached allocation maximal egalitarian social welfare, may
case still equitable deals possible, although would increase social
335

fiEndriss, Maudet, Sadri, & Toni

welfare (but would still cause leximin-rise). demonstrated
means simple example. Consider system three agents two resources.
following table fixes utility functions:
u1 ({ }) = 0
u1 ({r1 }) = 5
u1 ({r2 }) = 0
u1 ({r1 , r2 }) = 5

u2 ({ }) = 6
u2 ({r1 }) = 7
u2 ({r2 }) = 6.5
u2 ({r1 , r2 }) = 7.5

u3 ({ }) = 8
u3 ({r1 }) = 9
u3 ({r2 }) = 8.5
u3 ({r1 , r2 }) = 9.5

possible interpretation functions would following. Agent 3 fairly well
case; obtaining either resources r1 r2 great impact
personal welfare. true agent 2, although slightly less well begin
with. Agent 1 poorest agent attaches great value r1 , interest
r2 . Suppose agent 3 initially holds resources. corresponds ordered utility
vector h0, 6, 9.5i. Passing r1 agent 1 would lead new allocation ordered
utility vector h5, 6, 8.5i increase egalitarian social welfare 5, maximum
achievable system. However, still another equitable deal could
implemented latter allocation: agent 3 could offer r2 agent 2. course,
deal affect agent 1. resulting allocation would ordered utility
vector h5, 6.5, 8i, corresponds leximin-maximal allocation.
able detect situations social welfare maximum already reached
equitable deals still possible, able stop negotiation (assuming
interested maximising swe quickly possible), however, would require
non-local rationality criterion. criterion takes welfare agents involved
particular deal account could sharp enough always tell us whether given
deal would increase minimum utility society (see also discussion Lemma 2).
could define class strongly equitable deals like equitable deals top
require (currently) weakest agent involved deal. would
sharper criterion, would also spirit distributivity locality,
every single agent would involved every single deal (in sense everyone
announce utility order able determine weakest).
5.4 Necessity Result
next theorem show, restrict set admissible deals
equitable, every single deal (that independently decomposable) may
necessary guarantee optimal result (that is, sequence equitable deals excluding
could possibly result allocation maximal egalitarian social welfare). Furthermore,
theorem improves upon previous result (Endriss et al., 2003b) showing
holds even utility functions required dichotomous.10
Theorem 8 (Necessary deals egalitarian systems) Let sets agents resources fixed. every deal independently decomposable, exist
10. theorem also corrects mistake original statement result (Endriss et al., 2003b),
restriction deals independently decomposable omitted.

336

fiNegotiating Socially Optimal Allocations Resources

utility functions initial allocation sequence equitable deals leading allocation maximal egalitarian social welfare would include .
continues case even utility functions required dichotomous.
Proof. Given set agents set resources R, let = (A, A0 ) deal
system. 6= A0 , (at least one) agent j A(j) 6= A0 (j).
use particular j fix suitable (dichotomous) utility functions ui agents
sets resources R R follows:

ui (R) =

1 R = A0 (i) (R = A(i) 6= j)
0 otherwise

is, allocation A0 every agent assigns utility value 1 resources holds.
true allocation A, sole exception agent j, assigns
value 0. allocation, agents assign value 0 set resources,
unless set either allocation A0 . independently
decomposable, happen least one agent every allocation different
A0 . Hence, every allocation least one agent assign utility value
0 allocated bundle. get swe (A0 ) = 1, swe (A) = 0, swe (B) = 0 every
allocation B, i.e. A0 allocation maximal egalitarian social welfare.
ordered utility vector A0 form h1, . . . , 1i, form
h0, 1, . . . , 1i, allocation got form h0, . . .i, i.e. A0
B allocations B B 6= B 6= A0 . Therefore, make
initial allocation resources, deal would result strict rise
respect leximin-ordering. Thus, Lemma 3, would also equitable
deal. Hence, set admissible deals restricted equitable deals indeed
necessary reach allocation maximal egalitarian social welfare.
2
result shows, again, structurally simple class deals (such
class deals involving two agents time) would sufficient guarantee
optimal outcome negotiation. case even agents limited
options modelling preferences (as case dichotomous utility functions).11
negative necessity result shared two instances negotiation framework considered, currently positive results sufficiency
1-deals restricted domains egalitarian setting (see Theorems 3 6). instance, difficult construct counterexamples show even agents
using additive 0-1 functions, complex deals involving agents time may
required reach allocation maximal egalitarian social welfare (a concrete example
may found Endriss et al., 2003b).
11. However, observe unlike two variants framework rational negotiation,
necessity result scenarios monotonic utility functions (see Theorems 2 5). Using
collection monotonic utility functions proof Theorem 2 would allow us draw
conclusions regarding respective levels egalitarian social welfare A0 one hand,
allocations B other.

337

fiEndriss, Maudet, Sadri, & Toni

6. Negotiating Lorenz Optimal Allocations
section, going analyse framework resource allocation negotiation
view notion Lorenz optimal allocations introduced Definition 11. begin
somewhat general discussion possible local rationality criteria acceptability
given deal.
6.1 Local Rationality Criteria Separability
far, studied three different variants negotiation framework: (i) rational negotiation side payments (aiming maximising utilitarian social welfare); (ii) rational
negotiation without side payments (aiming Pareto optimal outcomes); (iii) negotiation egalitarian agent societies. first two instances framework, agents
either individually rational cooperatively rational, natural choices
formalise widely made assumptions agents purely self-interested myopic (for scenarios without monetary side payments, respectively).
third variant framework, applies egalitarian agent societies, attractive conceptual technical reasons. Conceptually, egalitarian social welfare
interest, largely neglected multiagent systems literature
despite classical counterpart widely used notion utilitarian social welfare. Technically, analysis egalitarian agent societies interesting,
egalitarian social welfare admit definition local rationality criterion
directly captures class deals resulting increase respect metric.
took detour via leximin-ordering prove termination.
class social welfare orderings captured deals conforming
local rationality criterion closely related class separable social welfare orderings
(Moulin, 1988). nutshell, social welfare ordering separable iff depends
agents changing utility whether given deal result increase social
welfare. Compare notion local rationality criterion (Definition 6);
depends agents changing bundle whether deal acceptable. change
utility presupposes change bundle (but vice versa). Hence, every separable
social welfare ordering corresponds class deals characterised local rationality
criterion (but vice versa). means every separable social welfare ordering gives
rise local rationality criterion proving general convergence theorem becomes
straightforward.12 leximin-ordering, instance, separable (Moulin, 1988),
going discuss social welfare ordering paper.13
Similarly, ordering alternative allocations induced notion Lorenz domination (Definition 11) also separable. Hence, definition appropriate class
deals proof general convergence result would yield significant new
insights either. However, analysis effects previously intro12. Indeed, case framework rational negotiation side payments, central argument
proof Theorem 1 Lemma 1, shows individual rationality fact equivalent
local rationality criterion induced swu .
13. suitable rationality criterion would simply amount lexicographic comparison ordered utility
vectors subsociety agents involved deal question.

338

fiNegotiating Socially Optimal Allocations Resources

duced rationality criteria agent societies social well-being assessed terms
Lorenz condition rather instructive, going see next.
6.2 Lorenz Domination Existing Rationality Criteria
going try establish connections global welfare measure induced
notion Lorenz domination one hand, various local criteria
acceptability proposed deal individual agents may choose apply other.
instance, immediate consequence Definitions 11 14 that, whenever = (A, A0 )
cooperatively rational deal, must Lorenz dominated A0 . may easily
verified, deal amounts Pigou-Dalton transfer (see Definition 15) also result
Lorenz improvement. hand, difficult construct examples
show case class equitable deals anymore (see Definition 16).
is, equitable deals result Lorenz improvement, others not.
next goal check whether possible combine existing rationality criteria
define class deals captures notion Lorenz improvements far
as, two allocations A0 Lorenz dominated A0 , exists
sequence deals (or possibly even single deal) belonging class leading
A0 . Given cooperatively rational deals Pigou-Dalton transfers always
result Lorenz improvement, union two classes deals may seem like
promising candidate. fact, according result reported Moulin (1988, Lemma 2.3),
case Lorenz improvement implemented means sequence
Pareto improvements (i.e. cooperatively rational exchanges) Pigou-Dalton transfers.
important stress seemingly general result apply negotiation
framework. see this, consider following example:
u1 ({ }) = 0
u1 ({r1 }) = 6
u1 ({r2 }) = 1
u1 ({r1 , r2 }) = 7

u2 ({ }) = 0
u2 ({r1 }) = 1
u2 ({r2 }) = 6
u2 ({r1 , r2 }) = 7

u3 ({ }) = 0
u3 ({r1 }) = 1
u3 ({r2 }) = 1
u3 ({r1 , r2 }) = 10

Let allocation agent 3 owns resources, i.e. ~u(A) = h0, 0, 10i
utilitarian social welfare currently 10. Allocation Pareto optimal,
allocation would strictly worse agent 3. Hence, cooperatively rational
deal would applicable situation. also observe deal involving
two agents would best result new allocation utilitarian social welfare 7 (this
would deal consisting either passing resources one agents,
passing preferred resource either agent 1 agent 2, respectively). Hence,
deal involving two agents (and particular Pigou-Dalton transfer) could possibly
result Lorenz improvement. However, allocation Lorenz dominates
A, namely allocation assigning one first two agents respectively
preferred resource. allocation A0 A0 (1) = {r1 }, A0 (2) = {r2 } A0 (3) = { }
got ordered utility vector h0, 6, 6i.
reason general result reported Moulin applicable domain
cannot use Pigou-Dalton transfers implement arbitrary utility transfers here.
Moulin assumes every possible utility vector constitutes feasible agreement.
339

fiEndriss, Maudet, Sadri, & Toni

context resource allocation, would mean allocation every possible
utility vector. framework, agents negotiate finite number indivisible
resources, however, range feasible allocations limited. instance,
example feasible allocation (ordered) utility vector h0, 4, 6i. Moulins
system, agents could first move h0, 0, 10i h0, 4, 6i (a Pigou-Dalton transfer)
h0, 4, 6i h0, 6, 6i (a Pareto improvement). system, hand,
possible.
6.3 Simple Pareto-Pigou-Dalton Deals 0-1 Scenarios
cannot use existing rationality criteria compose criterion captures notion
Lorenz improvement (and would allow us prove general convergence theorem),
going investigate far get scenario restricted utility functions.
Recall definition 0-1 scenarios utility functions used indicate
whether agent need particular resource. shall see next, 0-1
scenarios, aforementioned result Moulin apply. fact, even sharpen
little showing Pigou-Dalton transfers cooperatively rational deals involving
single resource two agents required guarantee negotiation outcomes
Lorenz optimal. first give formal definition class deals.
Definition 17 (Simple Pareto-Pigou-Dalton deals) deal called simple
Pareto-Pigou-Dalton deal iff 1-deal either cooperatively rational PigouDalton transfer.
going show class deals sufficient guarantee Lorenz optimal
outcomes negotiations 0-1 scenarios
Theorem 9 (Lorenz optimal outcomes) 0-1 scenarios, sequence simple
Pareto-Pigou-Dalton deals eventually result Lorenz optimal allocation resources.
Proof. pointed earlier, deal either cooperatively rational PigouDalton transfer result Lorenz improvement (not case 0-1 scenarios).
Hence, given finite number distinct allocations resources,
finite number deals system reached allocation simple
Pareto-Pigou-Dalton deals possible; is, negotiation must terminate. Now,
sake contradiction, let us assume terminal allocation optimal, i.e.
exists another allocation A0 Lorenz dominates A. Amongst things, implies
swu (A) swu (A0 ), i.e. distinguish two cases: either (i) strict
increase utilitarian welfare, (ii) remained constant. 0-1 scenarios, former
possible (at least) one resource r R two agents i, j
ui ({r}) = 0 uj ({r}) = 1 well r A(i) r A0 (j), i.e. r moved
agent (who need it) agent j (who need it). 1-deal moving
r j would cooperatively rational hence also simple Pareto-PigouDalton deal. contradicts assumption terminal allocation.
let us assume utilitarian social welfare remained constant, i.e. swu (A) =
swu (A0 ). Let k smallest index ~uk (A) < ~uk (A0 ). (This first k
inequality Definition 11 strict.) Observe cannot k = |A|,
340

fiNegotiating Socially Optimal Allocations Resources

would contradict swu (A) = swu (A0 ). shall call agents contributing first
k entries ordered utility vector ~u(A) poor agents remaining ones
rich agents. Then, 0-1 scenario, must resource r R owned
rich agent allocation poor agent j allocation A0 needed
agents, i.e. ui ({r}) = 1 uj ({r}) = 1. moving resource
agent agent j would constitute Pigou-Dalton transfer (and hence also simple
Pareto-Pigou-Dalton deal) allocation A, contradicts earlier assumption
terminal.
2
summary, shown (i) allocation resources simple
Pareto-Pigou-Dalton deals possible must Lorenz optimal allocation (ii)
allocation always reached implementing finite number simple ParetoPigou-Dalton deals. earlier convergence results, agents need worry
deals implement, long simple Pareto-Pigou-Dalton deals.
convergence global optimum guaranteed theorem.

7. Variations
section, going briefly consider two notions social preference
discuss context framework resource allocation negotiation. Firstly,
going introduce idea elitist agent societies, social welfare tied
welfare agent currently best off. going discuss societies
envy-free allocations resources desirable.
7.1 Elitist Agent Societies
Earlier discussed maximin-ordering induced egalitarian collective utility
function swe (see Definition 9). ordering actually particular case class social
welfare orderings, sometimes called k-rank dictators (Moulin, 1988), particular
agent (the one corresponding kth element ordered utility vector) chosen
representative society. Amongst class orderings, another particularly
interesting case welfare society evaluated basis happiest
agent (as opposed unhappiest agent, case egalitarian welfare). call
elitist approach measuring social welfare.
Definition 18 (Elitist social welfare) elitist social welfare swel (A) allocation
resources defined follows:
swel (A) = max{ui (A) | A}
elitist agent society, agents would cooperate order support champion (the
currently happiest agent). approach may seem somewhat unethical far
human society concerned, believe could indeed appropriate
certain societies artificial agents. applications, distributed multiagent system
may merely serve means helping single agent system achieve goal.
However, may always known advance agent likely achieve
goal therefore supported peers. typical scenario could
341

fiEndriss, Maudet, Sadri, & Toni

system designer launches different agents goal, aim least one
agent achieves goal matter happens others. egalitarian agent
societies, contradict idea agents autonomous entities. Agents may
physically distributed make autonomous decisions variety issues
whilst also adhering certain social principles, case elitist ones.
technical point view, designing criterion would allow agents inhabiting elitist agent society decide locally whether accept particular deal
similar egalitarian case. analogy case equitable deals defined
earlier, suitable deal would increase maximal individual welfare amongst
agents involved one deal. egalitarian case, class deals
characterised local rationality criterion would exactly capture range deals
resulting increase elitist social welfare (because every agent system would
consulted first determine currently best off). prove convergence,
would resort auxiliary social welfare ordering (similarly use
leximin-ordering proof Theorem 7).
course, many cases much simpler way finding allocation
maximal elitist social welfare. instance, agents use monotonic utility functions,
moving resources agent assigning highest utility value full bundle
R would optimal elitist point view. generally, always find
elitist optimum checking whose utility function got highest peak. is,
highest possible elitist social welfare easily determined centralised manner,
distributed approach still provide useful framework studying process
actually reaching optimal allocation.
7.2 Reducing Envy amongst Agents
final example interesting approach measuring social welfare agent society
issue envy-freeness (Brams & Taylor, 1996). particular allocation resources,
agent may envious another agent would prefer agents set resources
own. Ideally, allocation envy-free.
Definition 19 (Envy-freeness) allocation resources called envy-free iff
ui (A(i)) ui (A(j)) agents i, j A.
Like egalitarian social welfare, related fair division resources amongst agents.
Envy-freeness desirable (though always achievable) societies self-interested agents
cases agents collaborate longer period time.
case, agent believe ripped off, would incentive
leave coalition may disadvantageous agents society
whole. words, envy-freeness plays important role respect stability
group. Unfortunately, envy-free allocations always exist. simple example would
system two agents single resource, valued them.
whichever agent holds single resource envied agent.
Furthermore, aiming agreeing envy-free allocation resources always
compatible with, say, negotiating Pareto optimal outcomes. Consider following example
two agents identical preferences alternative bundles resources:
342

fiNegotiating Socially Optimal Allocations Resources

u1 ({ }) = 0
u1 ({r1 }) = 1
u1 ({r2 }) = 2
u1 ({r1 , r2 }) = 0

u2 ({ }) = 0
u2 ({r1 }) = 1
u2 ({r2 }) = 2
u2 ({r1 , r2 }) = 0

example, either one two allocations one agent owns resources
none would envy-free (as agent would prefer ones bundle
own). However, allocation would Pareto optimal. hand,
allocation agent owns single resource would Pareto optimal,
envy-free (because agent holding r1 would rather r2 ).
stress envy defined sole basis agents private preferences,
i.e. need take agents utility functions account. Still, whether
agent envious depend resources holds, also resources
could hold whether agents currently hold preferred bundle.
somewhat paradoxical situation makes envy-freeness far less amenable methodology
notions social welfare discussed paper.
able measure different degrees envy, could, example, count number
agents envious given allocation. Another option would compute
agent experiences envy difference ui (A(i)) ui (A(j))
agent j envies most. sum differences would also
provide indication degree overall envy (and thereby social welfare).
spirit egalitarianism, third option would identify degree envy society
degree envy experienced agent envious (Lipton, Markakis,
Mossel, & Saberi, 2004). However, possible define local acceptability criterion
terms utility functions agents involved deal (and those)
indicates whether deal question would reduce envy according metric.
simple consequence fact deal may affect degree envy experienced
agent involved deal (because could lead one participating
agents ending bundle preferred non-concerned agent question).

8. Conclusion
studied abstract negotiation framework members agent society
arrange multilateral deals exchange bundles indivisible resources, analysed
resulting changes resource distribution affect society respect different
social welfare orderings.
scenarios agents act rationally sense never accepting deal
would (even temporarily) decrease level welfare, seen systems
side payments possible guarantee outcomes maximal utilitarian social welfare,
systems without side payments allow, least, negotiation Pareto optimal
allocations. also considered two examples special domains restricted utility
functions, namely additive 0-1 scenarios. cases, able prove
convergence socially optimal allocation resources also negotiation protocols
allow deals involving single resources pair agents (so-called
1-deals). case agent societies welfare measured terms egalitarian
collective utility function, put forward class equitable deals shown
343

fiEndriss, Maudet, Sadri, & Toni

negotiation processes agents use equitability acceptability criterion also
converge towards optimal state. Another result states that, relatively simple
0-1 scenarios, Lorenz optimal allocations achieved using one-to-one negotiation
implementing 1-deals either inequality-reducing increase welfare
agents involved. also discussed case elitist agent societies social
welfare tied welfare successful agent. finally, pointed
difficulties associated designing agents would able negotiate
allocations resources degree envy agents society minimal.
Specifically, proved following technical results:
class individually rational deals14 sufficient negotiate allocations
maximal utilitarian social welfare (Theorem 1).
domains additive utility functions, class individually rational 1-deals
sufficient negotiate allocations maximal utilitarian social welfare (Theorem 3).
class cooperatively rational deals sufficient negotiate Pareto optimal allocations (Theorem 4).
domains 0-1 utility functions, class cooperatively rational 1-deals
sufficient negotiate allocations maximal utilitarian social welfare (Theorem 6).
class equitable deals sufficient negotiate allocations maximal egalitarian social welfare (Theorem 7).
domains 0-1 utility functions, class simple Pareto-Pigou-Dalton deals
(which 1-deals) sufficient negotiate Lorenz optimal allocations (Theorem 9).
three convergence results apply deals without structural restrictions
(rather 1-deals), also proved corresponding necessity results (Theorems 2, 5,
8). theorems show given deal (defined pair allocations)
independently decomposable may necessary able negotiate optimal allocation
resources (with respect chosen notion social welfare), deals required
conform rationality criterion question. consequence results,
negotiation protocol allow representation deals involving
number agents number resources could ever enable agents (whose behaviour
constrained various rationality criteria) negotiate socially optimal allocation
cases. Rather surprisingly, three necessity results continue apply even
agents differentiate resource bundles would happy
would happy (using dichotomous utility functions). Theorems 2
5 also apply case agents required use monotonic utility functions.
natural question arises considering convergence results concerns
complexity negotiation framework. difficult agents agree deal
many deals required system converges optimal state? latter
questions recently addressed Endriss Maudet (2005). paper
14. Recall individually rational deals may include monetary side payments.

344

fiNegotiating Socially Optimal Allocations Resources

establishes upper bounds number deals required reach optimal allocations resources referred four convergence theorems model rational
negotiation (i.e. Theorems 1, 3, 4, 6). also discusses different aspects complexity involved general level (such distinction communication
complexity system, i.e. amount information agents need exchange
reach optimal allocation, computational complexity reasoning tasks faced
every single agent). Dunne (2005) addresses related problem studies number
deals meeting certain structural requirements (in particular 1-deals) required
reach given target allocation (whenever possible recall necessity
results show excluding certain deal patterns typically bar agents reaching
optimal allocations).
earlier work, Dunne et al. (2005) studied complexity deciding whether
one-resource-at-a-time trading side payments sufficient reach given allocation
(with improved utilitarian social welfare). problem shown NP-hard.
complexity results concern computational complexity finding socially optimal allocation, independently concrete negotiation mechanism used. mentioned
earlier, results closely related computational complexity winner determination problem combinatorial auctions (Rothkopf, Pekec, & Harstad, 1998; Cramton
et al., 2006). Recently, NP-completeness results optimisation problem
derived respect several different ways representing utility functions (Dunne et al.,
2005; Chevaleyre et al., 2004). Bouveret Lang (2005) also address computational
complexity deciding whether allocation exists envy-free Pareto optimal.
Besides presenting technical results, argued wide spectrum social
welfare orderings (rather induced well-known utilitarian collective
welfare function concept Pareto optimality) interest agent-based
applications. context typical electronic commerce application, participating agents responsibilities towards other, system designer may wish
ensure Pareto optimality guarantee agents get maximal payoff whenever
possible without making agents worse off. applications fair
treatment participants vital (e.g. cases system infrastructure jointly
owned agents), egalitarian approach measuring social welfare may
appropriate. Many applications fact likely warrant mixture utilitarian
egalitarian principles. Here, systems enable Lorenz optimal agreements may turn
technology choice. applications, however, may require social welfare
measured ways foreseen models typically studied social sciences.
proposed notion elitist welfare would example. Elitism little room
human society, ethical considerations paramount, particular computing
application considerations may well dropped changed.
discussion suggests approach multiagent systems design call welfare
engineering (Endriss & Maudet, 2004). involves, firstly, application-driven choice (or
possibly invention) suitable social welfare ordering and, secondly, design agent
behaviour profiles negotiation mechanisms permit (or even guarantee) socially
optimal outcomes interactions agents system. discussed earlier,
designing agent behaviour profiles necessarily contradict idea autonomy
345

fiEndriss, Maudet, Sadri, & Toni

agent, autonomy always understood relative norms
governing society agent operates. stress that,
studying distributed approach multiagent resource allocation paper, general
idea exploring full range social welfare orderings developing agent-based
applications also applies centralised mechanisms (such combinatorial auctions).
hope develop methodology welfare engineering future work.
possible directions future work include identification social welfare
orderings definition corresponding deal acceptability criteria; continuation
complexity-theoretic analysis negotiation framework; design practical
trading mechanisms (including protocols strategies) would allow agents
agree multilateral deals involving two agents time.

Acknowledgments
would like thank Jerome Lang various anonymous referees valuable
comments. research partially supported European Commission
part SOCS project (IST-2001-32530).

References
Andersson, M., & Sandholm, T. W. (2000). Contract type sequencing reallocative negotiation. Proceedings 20th International Conference Distributed Computing
Systems (ICDCS-2000), pp. 154160. IEEE.
Arrow, K. J., Sen, A. K., & Suzumura, K. (Eds.). (2002). Handbook Social Choice
Welfare, Vol. 1. North-Holland.
Bouveret, S., & Lang, J. (2005). Efficiency envy-freeness fair division indivisible
goods: Logical representation complexity. Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI-2005), pp. 935940. Morgan
Kaufmann Publishers.
Brams, S. J., & Taylor, A. D. (1996). Fair Division: Cake-cutting Dispute Resolution. Cambridge University Press.
Chavez, A., Moukas, A., & Maes, P. (1997). Challenger: multi-agent system distributed
resource allocation. Proceedings 1st International Conference Autonomous
Agents (Agents-1997), pp. 323331. ACM Press.
Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2004). Multiagent resource allocation
k-additive utility functions. Proceedings DIMACS-LAMSADE Workshop Computer Science Decision Theory, Vol. 3 Annales du LAMSADE,
pp. 83100.
Chevaleyre, Y., Endriss, U., Lang, J., & Maudet, N. (2005a). Negotiating small bundles
resources. Proceedings 4th International Joint Conference Autonomous
Agents Multiagent Systems (AAMAS-2005), pp. 296302. ACM Press.
Chevaleyre, Y., Endriss, U., & Maudet, N. (2005b). maximal classes utility functions
efficient one-to-one negotiation. Proceedings 19th International Joint
346

fiNegotiating Socially Optimal Allocations Resources

Conference Artificial Intelligence (IJCAI-2005), pp. 941946. Morgan Kaufmann
Publishers.
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2006). Combinatorial Auctions. MIT
Press.
Dunne, P. E. (2005). Extremal behaviour multiagent contract negotiation. Journal
Artificial Intelligence Research, 23, 4178.
Dunne, P. E., Wooldridge, M., & Laurence, M. (2005). complexity contract negotiation. Artificial Intelligence, 164 (12), 2346.
Dunne, P. E., Laurence, M., & Wooldridge, M. (2004). Tractability results automatic
contracting. Proceedings 16th Eureopean Conference Artificial Intelligence
(ECAI-2004), pp. 10031004. IOS Press.
Endriss, U., & Maudet, N. (2004). Welfare engineering multiagent systems. Engineering
Societies Agents World IV, Vol. 3071 LNAI, pp. 93106. Springer-Verlag.
Endriss, U., & Maudet, N. (2005). communication complexity multilateral trading:
Extended report. Journal Autonomous Agents Multiagent Systems, 11 (1), 91
107.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2003a). optimal outcomes negotiations resources. Proceedings 2nd International Joint Conference
Autonomous Agents Multiagent Systems (AAMAS-2003), pp. 177184. ACM
Press.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2003b). Resource allocation egalitarian agent societies. Secondes Journees Francophones sur les Modeles Formels
dInteraction (MFI-2003), pp. 101110. Cepadues-Editions.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2004). agenda-based framework
multi-issues negotiation. Artificial Intelligence, 152 (1), 145.
Fishburn, P. C. (1970). Utility Theory Decision Making. John Wiley Sons.
Fujishima, Y., Leyton-Brown, K., & Shoham, Y. (1999). Taming computational complexity combinatorial auctions: Optimal approximate approaches. Proceedings 16th International Joint Conference Artificial Intelligence (IJCAI1999), pp. 548553. Morgan Kaufmann Publishers.
Kraus, S. (2001). Strategic Negotiation Multiagent Environments. MIT Press.
Lematre, M., Verfaillie, G., & Bataille, N. (1999). Exploiting common property resource
fairness constraint: case study. Proceedings 16th International Joint
Conference Artificial Intelligence (IJCAI-1999), pp. 206211. Morgan Kaufmann
Publishers.
Lipton, R. J., Markakis, E., Mossel, E., & Saberi, A. (2004). approximately fair allocations indivisible goods. Proceedings 5th ACM Conference Electronic
Commerce (EC-2004), pp. 125131. ACM Press.
Moulin, H. (1988). Axioms Cooperative Decision Making. Cambridge University Press.
347

fiEndriss, Maudet, Sadri, & Toni

Myerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms bilateral trading.
Journal Economic Theory, 29 (2), 265281.
Rawls, J. (1971). Theory Justice. Oxford University Press.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules Encounter. MIT Press.
Rothkopf, M. H., Pekec, A., & Harstad, R. M. (1998). Computationally manageable combinational auctions. Management Science, 44 (8), 11311147.
Sadri, F., Toni, F., & Torroni, P. (2001). Dialogues negotiation: Agent varieties dialogue sequences. Proceedings 8th International Workshop Agent Theories,
Architectures, Languages (ATAL-2001), pp. 405421. Springer-Verlag.
Sandholm, T. W. (2002). Algorithm optimal winner determination combinatorial au
ctions. Artificial Intelligence, 135, 154.
Sandholm, T. W. (1998). Contract types satisficing task allocation: Theoretical results.
Proceedings AAAI Spring Symposium: Satisficing Models.
Sandholm, T. W. (1999). Distributed rational decision making. Wei, G. (Ed.), Multiagent Systems: Modern Approach Distributed Artificial Intelligence, pp. 201258.
MIT Press.
Sen, A. K. (1970). Collective Choice Social Welfare. Holden Day.
Smith, R. G. (1980). contract net protocol: High-level communication control
distributed problem solver. IEEE Transactions Computers, C-29 (12), 11041113.
Wooldridge, M. (2002). Introduction MultiAgent Systems. John Wiley Sons.

348

fiJournal Artificial Intelligence Research 25 (2006) 75-118

Submitted 01/05; published 1/06

Approximate Policy Iteration Policy Language Bias:
Solving Relational Markov Decision Processes
Alan Fern

afern@cs.orst.edu
School Electrical Engineering Computer Science, Oregon State University
Sungwook Yoon
sy@purdue.edu
Robert Givan
givan@purdue.edu
School Electrical Computer Engineering, Purdue University

Abstract
study approach policy selection large relational Markov Decision Processes
(MDPs). consider variant approximate policy iteration (API) replaces
usual value-function learning step learning step policy space. advantageous
domains good policies easier represent learn corresponding
value functions, often case relational MDPs interested in.
order apply API problems, introduce relational policy language
corresponding learner. addition, introduce new bootstrapping routine goalbased planning domains, based random walks. bootstrapping necessary
many large relational MDPs, reward extremely sparse, API ineffective
domains initialized uninformed policy. experiments show
resulting system able find good policies number classical planning domains
stochastic variants solving extremely large relational MDPs.
experiments also point limitations approach, suggesting future work.

1. Introduction
Many planning domains naturally represented terms objects relations
among them. Accordingly, AI researchers long studied algorithms planning
learning-to-plan relational state action spaces. include, example, classical
STRIPS domains blocks world logistics.
common criticism domains algorithms assumption idealized,
deterministic world model. This, part, led AI researchers study planning
learning within decision-theoretic framework, explicitly handles stochastic environments generalized reward-based objectives. However, work based
explicit propositional state-space models, far demonstrated scalability
large relational domains commonly addressed classical planning.
Intelligent agents must able simultaneously deal complexity arising
relational structure complexity arising uncertainty. primary goal
research move toward agents bridging gap classical
decision-theoretic techniques.
paper, describe straightforward practical method solving large,
relational MDPs. work viewed form relational reinforcement learning
(RRL) assume strong simulation model environment. is, assume
access black-box simulator, provide (relationally represented)
c
2006
AI Access Foundation. rights reserved.

fiFern, Yoon, & Givan

state/action pair receive sample appropriate next-state reward distributions. goal interact simulator order learn policy achieving high
expected reward. separate challenge, considered here, combine work
methods learning environment simulator avoid dependence provided
simulator.
Dynamic-programming approaches finding optimal control policies MDPs (Bellman, 1957; Howard, 1960), using explicit (flat) state space representations, break
state space becomes extremely large. recent work extends algorithms
use propositional (Boutilier & Dearden, 1996; Dean & Givan, 1997; Dean, Givan, &
Leach, 1997; Boutilier, Dearden, & Goldszmidt, 2000; Givan, Dean, & Greig, 2003; Guestrin,
Koller, Parr, & Venkataraman, 2003b) well relational (Boutilier, Reiter, & Price, 2001;
Guestrin, Koller, Gearhart, & Kanodia, 2003a) state-space representations. extensions significantly expanded set approachable problems, yet shown
capacity solve large classical planning problems benchmark problems
used planning competitions (Bacchus, 2001), let alone stochastic variants. One possible reason methods based calculating representing value
functions. familiar STRIPS planning domains (among others), useful value functions
difficult represent compactly, manipulation becomes bottle-neck.
techniques purely deductivethat is, value function guaranteed certain level accuracy. Rather, work, focus inductive
techniques make guarantees practice. existing inductive forms approximate policy iteration (API) utilize machine learning select compactly represented
approximate value functions iteration dynamic programming (Bertsekas & Tsitsiklis, 1996). machine learning algorithm, selection hypothesis space,
space value functions, critical performance. example space used frequently
space linear combinations human-selected feature set.
knowledge, previous work applies form API
benchmark problems classical planning, stochastic variants.1 Again, one
reason high complexity typical value functions large relational
domains, making difficult specify good value-function spaces facilitate learning.
Comparably, often much easier compactly specify good policies, accordingly
good policy spaces learning. observation basis recent work inductive policy selection relational planning domains, deterministic (Khardon, 1999a;
Martin & Geffner, 2000), probabilistic (Yoon, Fern, & Givan, 2002). techniques
show useful policies learned using policy-space bias described generic
(relational) knowledge representation language. incorporate ideas variant API, achieves significant success without representing learning approximate
value functions. course, natural direction future work combine policy-space
techniques value-function techniques, leverage advantages both.
Given initial policy, approach uses simulation technique policy rollout
(Tesauro & Galperin, 1996) generate trajectories improved policy. trajectories given classification learner, searches classifier, policy,
matches trajectory data, resulting approximately improved policy. two
1. Recent work relational reinforcement learning applied STRIPS problems much simpler
goals typical benchmark planning domains, discussed Section 8.

76

fiAPI Policy Language Bias

steps iterated improvement observed. resulting algorithm
viewed form API iteration carried without inducing approximate
value functions.
avoiding value function learning, algorithm helps address representational
challenge applying API relational planning domains. However, another fundamental
challenge that, non-trivial relational domains, API requires form bootstrapping. particular, STRIPS planning domains reward, corresponds
achieving goal condition, sparsely distributed unlikely reached random exploration. Thus, initializing API random uninformed policy, likely result
reward signal hence guidance policy improvement. One approach bootstrapping rely user provide good initial policy heuristic gives guidance
toward achieving reward. Rather, work develop new automatic bootstrapping
approach goal-based planning domains, require user intervention.
bootstrapping approach based idea random-walk problem distributions.
given planning domain, blocks world, distribution randomly generates
problem (i.e., initial state goal) selecting random initial state
executing sequence n random actions, taking goal condition subset
properties resulting state. problem difficulty typically increases n,
small n (short random walks) even random policies uncover reward. Intuitively,
good policy problems walk length n used bootstrap API problems
slightly longer walk lengths. bootstrapping approach iterates idea, starting
random policy small n, gradually increasing walk length
learn policy long random walks. long-random-walk policies clearly capture
much domain knowledge, used various ways. Here, show empirically
policies often perform well problem distributions relational domains used
recent deterministic probabilistic planning competitions.
implementation bootstrapped API approach took second place 3 competitors hand-tailored track 2004 International Probabilistic Planning Competition.2 knowledge first machine-learning based system entered
planning competition, either deterministic probabilistic.
Here, give evaluation system number probabilistic deterministic
relational planning domains, including AIPS-2000 competition benchmarks, benchmarks hand-tailored track 2004 Probabilistic Planning Competition.
results show system often able learn policies domains perform
well long-random-walk problems. addition, policies often perform well
planning-competition problem distributions, comparing favorably state-of-theart planner FF deterministic domains. experiments also highlight number
limitations current system, point interesting directions future work.
remainder paper proceeds follows. Section 2, introduce problem
setup then, Section 3, present new variant API. Section 4, provide
2. Note, however, approach hand-tailored. Rather, given domain definition, system
learns policy offline, automatically, applied problem domain.
entered hand-tailored track track facilitated use offline learning,
providing domains problem generators competition. entrants humanwritten domain.

77

fiFern, Yoon, & Givan

technical analysis algorithm, giving performance bounds policy-improvement
step. Sections 5 6, describe implemented instantiation API approach
relational planning domains. includes description generic policy language
relational domains, classification learner language, novel bootstrapping
technique goal-based domains. Section 7 presents empirical results, finally
Sections 8 9 discuss related work future directions.

2. Problem Setup
formulate work framework Markov Decision Processes (MDPs).
primary motivation develop algorithms relational planning domains, first
describe problem setup approach general, action-simulatorbased MDP representation. Later, Section 5, describe particular representation planning domains
relational MDPs corresponding relational instantiation approach.
Following adapting Kearns, Mansour, Ng (2002) Bertsekas Tsitsiklis
(1996), represent MDP using generative model hS, A, T, R, Ii, finite
set states, finite, ordered set actions, randomized action-simulation
algorithm that, given state action a, returns next state s0 according unknown
probability distribution PT (s0 |s, a). component R reward function maps
real-numbers, R(s, a) representing reward taking action state s,
randomized initial-state algorithm inputs returns state according
unknown distribution P0 (s). sometimes treat (s, a) random variables
distributions P0 () PT (|s, a) respectively.
MDP = hS, A, T, R, Ii, policy (possibly stochastic) mapping
A. value function , denoted V (s), represents expected, cumulative, discounted
reward following policy starting state s, unique solution
V (s) = E[R(s, (s)) + V (T (s, (s)))]

(1)

0 < 1 discount factor. Q-value function Q (s, a) represents
expected, cumulative, discounted reward taking action state following ,
given
Q (s, a) = R(s, a) + E[V (T (s, a))]

(2)

measure quality policy objective function V () = E[V (I)], giving
expected value obtained policy starting randomly drawn initial
state. common objective MDP planning reinforcement learning find
optimal policy = argmax V (). However, automated technique, including one
present here, date able guarantee finding optimal policy relational
planning domains consider, reasonable running time.
well known fact given current policy , define new improved
policy
PI (s) = argmaxaA Q (s, a)

(3)

value function PI guaranteed 1) worse
state s, 2) strictly improve state optimal. Policy iteration
78

fiAPI Policy Language Bias

algorithm computing optimal policies iterating policy improvement (PI)
initial policy reach fixed point, guaranteed optimal policy.
iteration policy improvement involves two steps: 1) Policy evaluation compute
value function V current policy , 2) Policy selection, where, given V
step 1, select action maximizes Q (s, a) state, defining new improved
policy.
Finite Horizons. Since API variant based simulation, must bound
simulation trajectories horizon h, technical analysis Section 4 use notion
finite-horizon discounted reward. h-horizon value function Vh recursively defined

V0 (s) = 0,

Vh (s) = E[R(s, (s)) + Vh1 (T (s, (s)))]

(4)

giving expected discounted reward obtained following h steps s. also
(T (s, a))], h-horizon
define h-horizon Q-function Qh (s, a) = R(s, a) + E[Vh1

objective function V h () = E[Vh (I)]. well known, effect using finite
horizon made arbitrarily small. particular, states
actions a, approximation error decreases exponentially h,
|V (s) Vh (s)| h Vmax ,
|Q (s, a) Qh (s, a)| h Vmax ,
Rmax
Vmax =
,
1
Rmax maximum absolute value reward action state.
also get |V h () V ()| h Vmax .

3. Approximate Policy Iteration Policy Language Bias
Exact solution techniques, policy iteration, typically intractable large statespace MDPs, arising relational planning domains. section,
introduce new variant approximate policy iteration (API) intended domains.
First, review generic form API used prior work, based learning approximate
value functions. Next, motivated fact value functions often difficult learn
relational domains, describe API variant, avoids learning value functions
instead learns policies directly state-action mappings.
3.1 API Approximate Value Functions
API, described Bertsekas Tsitsiklis (1996), uses combination Monte-Carlo
simulation inductive machine learning heuristically approximate policy iteration
large state-space MDPs. Given current policy , iteration API approximates
policy evaluation policy selection, resulting approximately improved policy .
First, policy-evaluation step constructs training set samples V small
representative set states. sample computed using simulation, estimating V (s)
policy state drawing number sample trajectories starting
79

fiFern, Yoon, & Givan

averaging cumulative, discounted reward along trajectories. Next,
policy-selection step uses function approximator (e.g., neural network) learn
approximation V V based training data. V serves representation
, selects actions using sampled one-step lookahead based V ,
(s) = arg max R(s, a) + E[V (T (s, a))].
aA

common variant procedure learns approximation Q rather V .
API exploits function approximators generalization ability avoid evaluating
state state space, instead directly evaluating small number training states.
Thus, use API assumes states perhaps actions represented factored
form (typically, feature vector) facilitates generalizing properties training data
entire state action spaces. Note case perfect generalization (i.e.,
V (s) = V (s) states s), equal exact policy improvement
PI , thus API simulates exact policy iteration. However, practice, generalization
perfect, typically guarantees policy improvement3 nevertheless,
API often converges usefully (Tesauro, 1992; Tsitsiklis & Van Roy, 1996).
success API procedure depends critically ability represent
learn good value-function approximations. MDPs, arising
relational planning domains, often difficult specify space value functions
learning mechanism facilitate good generalization. example, work relational
reinforcement learning (Dzeroski, DeRaedt, & Driessens, 2001) shown learning
approximate value functions classical domains, blocks world, problematic.4 spite this, often relatively easy compactly specify good policies using
language (relational) state-action mappings. suggests languages may
provide useful policy-space biases learning API. However, prior API methods
based approximating value functions hence leverage biases.
motivation, consider form API directly learns policies without directly
representing approximating value functions.
3.2 Using Policy Language Bias
policy simply classifier, possibly stochastic, maps states actions. API
approach based view, motived recent work casts policy selection
standard classification learning problem. particular, given ability observe
trajectories target policy, use machine learning select policy, classifier,
mimics target closely possible. Khardon (1999b) studied learning setting
provided PAC-like learnability results, showing certain assumptions, small
number trajectories sufficient learn policy whose value close
target. addition, recent empirical work, relational planning domains (Khardon, 1999a;
Martin & Geffner, 2000; Yoon et al., 2002), shown using expressive languages
3. strong assumptions, API shown converge infinite limit near-optimal
value function. See Proposition 6.2 Bertsekas Tsitsiklis (1996).
4. particular, RRL work considered variety value-function representation including relational
regression trees, instance based methods, graph kernels, none generalized well
varying numbers objects.

80

fiAPI Policy Language Bias

specifying state-action mappings, good policies learned sample trajectories
good policies.
results suggest that, given policy , somehow generate trajectories
improved policy, learn approximately improved policy based
trajectories. idea basis approach. Figure 1 gives pseudo-code API
variant, starts initial policy 0 produces sequence approximately
improved policies. iteration involves two primary steps: First, given current
policy , procedure Improved-Trajectories (approximately) generates trajectories
improved policy 0 = PI . Second, trajectories used training data
procedure Learn-Policy, returns approximation 0 . describe
step detail.
Step 1: Generating Improved Trajectories. Given base policy , simulation technique policy rollout (Tesauro & Galperin, 1996; Bertsekas & Tsitsiklis, 1996)
computes approximation improved policy 0 = PI , 0 result
applying one step policy iteration . Furthermore, given state s, policy rollout
computes (s) without need solve 0 states, thus provides
tractable way approximately simulate improved policy 0 large state-space MDPs.
Often 0 significantly better , hence , lead substantially
improved performance small cost. Policy rollout provided significant benefits
number application domains, including example Backgammon (Tesauro & Galperin,
1996), instruction scheduling (McGovern, Moss, & Barto, 2002), network-congestion control
(Wu, Chong, & Givan, 2001), Solitaire (Yan, Diaconis, Rusmevichientong, & Van Roy,
2004).
Policy rollout computes (s), estimate 0 (s), estimating Q (s, a)
action taking maximizing action (s) suggested Equation 3.
Q (s, a) estimated drawing w trajectories length h, trajectory
result starting s, taking action a, following actions selected
h 1 steps. estimate Q (s, a) taken average cumulative
discounted reward along trajectory. sampling width w horizon h specified
user, control trade increased computation time large values,
reduced accuracy small values. Note rollout applies stochastic
deterministic policies due variance Q-value estimates, rollout policy
stochastic even deterministic base policies.
procedure Improved-Trajectories uses rollout generate n length h trajectories
, beginning randomly drawn initial state. Rather recording
states actions along trajectory, store additional information used
policy-learning algorithm. particular, ith element trajectory form
hsi , (si ), Q(si , a1 ), . . . , Q(si , )i, giving ith state si along trajectory, action
selected current (unimproved) policy si , Q-value estimates Q(si , a)
action. Note given Q-value information si learning algorithm
determine approximately improved action (s), maximizing actions, desired.
Step 2: Learn Policy. Intuitively, want Learn-Policy select new policy
closely matches training trajectories. experiments, use relatively simple
learning algorithms based greedy search within space policies specified policylanguage bias. Sections 5.2 5.3 detail policy-language learning bias used
81

fiFern, Yoon, & Givan

technique, associated learning algorithm. Section 4 provide
technical analysis idealized version algorithm, providing guidance regarding
required number training trajectories. note labeling training state
trajectories associated Q-values action, rather simply
best action, enable learner make informed trade-offs, focusing accuracy
states wrong decisions high costs, empirically useful. Also,
inclusion (s) training data enables learner adjust data relative ,
desirede.g., learner uses bias focuses states large improvement
appears possible.
Finally, note API effective, important initial policy
0 provide guidance toward improvement, i.e., 0 must bootstrap API process.
example, goal-based planning domains 0 reach goal sampled
states. Section 6 discuss important issue bootstrapping introduce
new bootstrapping technique.

4. Technical Analysis
section, consider variant policy improvement step main API loop,
learns improved policy given base policy . show select sampling
width w, horizon h, training set size n that, certain assumptions, quality
learned policy close quality 0 policy iteration improvement. Similar
results shown previous forms API based approximate value functions
(Bertsekas & Tsitsiklis, 1996), however, assumptions much different nature.5
analysis divided two parts. First, following Khardon (1999b), consider
sample complexity policy learning. is, consider many trajectories
target policy must observed learner guarantee good approximation
target. Second, show apply result, deterministic policies,
problem learning rollout policies, stochastic. Throughout
assume context MDP = hS, A, T, R, Ii.
4.1 Learning Deterministic Policies
trajectory length h sequence (s0 , a0 , s1 , a1 , . . . , ah1 , sh ) alternating states si
actions ai . say deterministic policy consistent trajectory (s1 , a1 , . . . , sh )
0 < h, (si ) = ai . define Dh distribution set
length h trajectories, Dh (t) probability generates trajectory
= (s0 , a0 , s1 , a1 , . . . , ah1 , sh ) according following process: first draw s0 according
initial state distribution I, draw si+1 (si , (si )) 0 < h. Note
Dh (t) non-zero consistent t.
policy improvement step first generates trajectories rollout policy (see Section 3.2), via procedure Improved-Trajectories, learns approximation
5. particular, Bertsekas Tsitsiklis (1996) assumes bound L norm value function
approximation, i.e., state approximation almost perfect. Rather assume
improved policy 0 comes finite class policies consistent learner.
cases policy improvement guaranteed given additional assumption minimum
Q-advantage MDP (see below).

82

fiAPI Policy Language Bias

API (n, w, h, M, 0 , )
// training set size n, sampling width w, horizon h,
// MDP = hS, {a1 , . . . , }, T, R, Ii, initial policy 0 , discount factor .
0 ;
loop
Improved-Trajectories(n, w, h, M, );
Learn-Policy(T );
satisfied ;
// e.g., change small
Return ;
Improved-Trajectories(n, w, h, M, )
// training set size n, sampling width w,
// horizon h, MDP , current policy
;
repeat n times // generate n trajectories improved policy
nil;
state drawn I; // draw random initial state
= 1 h
hQ(s, a1 ), . . . , Q(s, )i Policy-Rollout(, s, w, h, ); // Q (s, a) estimates
hs, (s), Q(s, a1 ), . . . , Q(s, ))i; // concatenate new sample onto trajectory
action maximizing Q(s, a); // action improved policy state
state sampled (s, a); // simulate action improved policy
t;
Return ;
Policy-Rollout (, s, w, h, )

// Compute Q (s, a) estimates hQ(s, a1 ), . . . , Q(s, )i

// policy , state s, sampling width w, horizon h, MDP
action ai
Q(s, ai ) 0;
repeat w times // Q(s, ai ) average w trajectories
R R(s, ai ); s0 state sampled (s, ai ); // take action ai
= 1 h 1 // take h 1 steps accumulating discounted reward R
R R + R(s0 , (s0 ));
s0 state sampled (s0 , (s0 ))
Q(s, ai ) Q(s, ai ) + R; // include trajectory average
Q(s, ai )

Q(s,ai )
;
w

Return hQ(s, a1 ), . . . , Q(s, )i

Figure 1: Pseudo-code API algorithm. See Section 5.3 instantiation LearnPolicy called Learn-Decision-List.
. Note rollout policy serves stochastic approximation 0 = PI
policy iteration improvement . Thus, Improved-Trajectories viewed at0
tempting draw trajectories Dh , learning step viewed learning
83

fiFern, Yoon, & Givan

0

approximation 0 . Imagining moment draw trajectories Dh ,
fundamental question many trajectories sufficient ensure learned
policy good 0 . Khardon (1999b) studied question case
deterministic policies undiscounted goal-based planning domains (i.e., MDPs
reward received goal states). give straightforward adaptation
main result problem setting general reward functions measure
quality policy V ().
learning-problem formulation similar spirit standard framework Probably Approximately Correct (PAC) learning. particular, assume target
policy comes finite class deterministic policies H. example, H may correspond
set policies described bounded-length decision lists. addition,
assume learner consistenti.e., returns policy H consistent
training trajectories. assumptions, relatively small number
trajectories (logarithmic |H|) sufficient ensure high probability
learned policy good target.
Proposition 1. Let H finite class deterministic policies. H,

set n = 1 ln |H|
trajectories drawn independently Dh , 1 probability
every H consistent trajectories satisfies V () V () 2Vmax ( + h ).
proof proposition Appendix. computational complexity
finding consistent policy depends policy class H. Polynomial-time algorithms
given interesting classes bounded-length decision listshowever,
algorithms typically expensive policy classes consider practice. Rather,
described Section 5.3, use learner based greedy heuristic search, often
works well practice.
assumption target policy comes fixed size class H often
violated. However, pointed Khardon (1999b), straightforward give
extension Proposition 1 setting learner considers increasingly complex
policies consistent one found. case, sample complexity related
encoding size target policy rather size H, thus allowing use
large expressive policy classes without necessarily paying full sample-complexity
price Proposition 1.
4.2 Learning Rollout Policies
proof Proposition 1 relies critically fact policy class H contains
deterministic policies. However, main API loop, target policies computed via
rollout hence stochastic due uncertainty introduced finite sampling. Thus,
cannot directly use Proposition 1 context learning trajectories produced
rollout. deal problem describe variant Improved-Trajectories
reliably generate training trajectories deterministic policy 0 = PI (see
Equation 3), guaranteed improve improvement possible.
Given base policy , first define (s) set actions maximize
Q (s, a). Note 0 (s) = min (s), minimum taken respect action ordering provided MDP. Importantly policy deterministic thus
84

fiAPI Policy Language Bias

generate trajectories it, apply result learn close approximation. order generate trajectories 0 slightly modify Improved-Trajectories.
modification introduced analysis only, experiments based procedures given Figure 1. modification replace action maximization step
Improved-Trajectories (second last statement loop), chooses next
action execute, following two steps
A(, s) {a0 | maxa Q(s, a) Q(s, a0 ) }
min A(, s)
Q(s, a) estimate Qh (s, a) computed policy rollout using sampling width
w, newly introduced parameter.
Note A(, s) = (s), selected action equal 0 (s). condition true every state encountered modified Improved-Trajectories
effectively generate trajectories 0 . Thus, would like bound probability
A(, s) 6= (s) small value appropriately choosing sampling width w,
horizon h, . Unfortunately, choice parameters depends MDP.
is, given particular parameter values, MDP event
A(, s) 6= (s) non-negligible probability state. reason first
define Q-advantage MDP show select appropriate parameter values
given lower-bound .
Given MDP policy , let 0 set states 0 iff
two actions a0 Q (s, a) 6= Q (s, a0 ), i.e., actions distinct
Q-values. Also state 0 define a1 (s) a2 (s) best action second
best action respectively measured Q (s, a). Q-advantage defined =
minsS 0 a1 (s) a2 (s), measures minimum Q-value gap optimal
sub-optimal action state space. Given lower-bound Q-advantage
MDP following proposition indicates select parameter values ensure
A(, s) = (s) high probability.
Proposition 2.


MDP Q-advantage least , 0 < 0 < 1,

h > log


8Vmax

8Vmax



2



w >
=

2

ln

|A|
0

state s, A(, s) = (s) probability least 1 0 .
proof given Appendix. Thus, parameter values satisfying conditions, MDP Q-advantage least guaranteed probability
least 1 0 A(, s) = A(, s). means Improved-Trajectories correctly select action 0 (s) probability least 1 0 . Note proposition
85

fiFern, Yoon, & Givan

agrees intuition h w increase decreasing Q-advantage
increasing Vmax also w increase decreasing 0 .6
order generate n length h trajectories 0 , modified Improved-Trajectories
routine must compute set A(, ) n h states, yielding n h opportunities make
error. ensure error made, modified procedure sets sampling width w

. guarantees error free training set created probability
using 0 = 2nh

least 1 2 .
Combining observation assumption 0 H apply Proposition
0
1 follows. First, generate n = 1 ln 2|H|
trajectories using modified Improved
Trajectories routine (with 0 = 2nh
). Next, learn policy trajectories using
consistent learner. know probability generating imperfect training set
bounded 2 , chosen value n, failure probability learner
also bounded 2 . Thus, get probability least 1 , learned policy
satisfies V () V ( 0 ) 2Vmax ( + h ), giving approximation guarantee relative
improved policy 0 . summarized following proposition.
Proposition 3. Let H finite class deterministic policies, 0 < < 1, 0 < < 1.
MDP Q-advantage
least , policy PI H, set

1
1
n > ln 2|H|
trajectories produced modified Improved-Trajectories using
parameters satisfying,
=


2

h > log


8Vmax

8Vmax 2 2nh|A|
w >
ln


least 1 probability every H consistent trajectories satisfies
V () V (PI ) 2Vmax ( + h ).




One notable aspect result logarithmic dependence
number actions |A| 1 . However, practical utility hindered dependence
typically known practice, exponentially small
planning horizon. Unfortunately, dependence appears unavoidable type
approach try learn trajectories PI produced rollout.
particular setting parameters, always MDP
small enough Q-advantage, value rollout policy arbitrarily worse
PI .

5. API Relational Planning
work motivated goal solving relational MDPs. particular, interested finding policies relational MDPs represent classical planning domains
6. first glance appears lower-bound h decreases increasing Vmax decreasing .
However, opposite true since base logarithm discount factor, strictly less
one. Also note since upper-bounded 2Vmax bound h always positive.

86

fiAPI Policy Language Bias

stochastic variants. policies applied problem instance
planning domain, hence viewed form domain-specific control knowledge.
section, first describe straightforward way view classical planning domains
(not single problem instances) relationally factored MDPs. Next, describe
relational policy space policies compactly represented taxonomic decision
lists. Finally, present heuristic learning algorithm policy space.
5.1 Planning Domains MDPs.
say MDP hS, A, T, R, Ii relational defined giving finite
set objects O, finite set predicates P , finite set action types . fact
predicate applied appropriate number objects, e.g., on(a, b) blocks-world
fact. state set facts, interpreted representing true facts state.
state space contains possible sets facts. action action type applied
appropriate number objects, e.g., putdown(a) blocks-world action, action
space set actions.
classical planning domain describes set problem instances related structure,
problem instance gives initial world state goal. example, blocks
world classical planning domain, problem instance specifies initial block
configuration set goal conditions. Classical planners attempt find solutions
specific problem instances domain. Rather, goal solve entire planning domains
finding policy applied problem instances. described below,
straightforward view classical planning domain relational MDP MDP
state corresponds problem instance.
State Action Spaces. classical planning domain specifies set action
types , world predicates W , possible world objects O. Together define
MDP action space. state MDP corresponds single problem instance (i.e.,
world state goal) planning domain specifying current world
goal. achieve letting set relational MDP predicates P = W G,
G set goal predicates. set goal predicates contains predicate
world predicate W , named prepending g onto corresponding
world predicate name (e.g., goal predicate gclear corresponds world predicate
clear). definition P see MDP states sets goal world
facts, indicating true world facts problem instance goal conditions.
important note, described below, MDP actions change world
facts goal facts. Thus, large relational MDP viewed collection
disconnected sub-MDPs, sub-MDP corresponds distinct goal condition.
Reward Function. Given MDP state objective reach another MDP state
goal facts subset corresponding world factsi.e., reach world state
satisfies goal. call states goal states MDP. example,
MDP state
{on-table(a), on(a, b), clear(b), gclear(b)}
goal state blocks-world MDP, would goal state without world fact
clear(b). represent objective reaching goal state quickly defining R assign
reward zero actions taken goal states negative rewards actions
87

fiFern, Yoon, & Givan

states, representing cost taking actions. Typically, classical planning
domains, action costs uniformly -1, however, framework allows cost vary
across actions.
Transition Function. classical planning domain provides action simulator
(e.g., defined STRIPS rules) that, given world state action, returns new world
state. define MDP transition function simulator modified treat goal
states terminal preserve without change goal predicates MDP state. Since
classical planning domains typically large number actions, action definitions
usually accompanied preconditions indicate legal actions given state,
usually legal actions small subset possible actions. assume
treats actions legal no-ops. simplicity, relational MDP definition
explicitly represent action preconditions, however, assume algorithms
access preconditions thus need consider legal actions. example,
restrict rollout legal actions given state.
Initial State Distribution. Finally, initial state distribution program
generates legal problem instances (MDP states) planning domain. example, problem domains planning competitions commonly distributed problem
generators.
definitions, good policy one reach goal states via low-cost
action sequences initial states drawn I. Note policies mappings
problem instances actions thus sensitive goal conditions.
way, learned policies able generalize across different goals. next describe
language representing generalized policies.
5.2 Taxonomic Decision List Policies.
single argument action types, many useful rules planning domains take form
apply action type object class C (Martin & Geffner, 2000). example,
blocks world, pick clear block belongs table table,
logistics world, unload object destination. Using concept
language describing object classes, Martin Geffner (2000) introduced use
decision lists rules useful learning bias, showing promising experiments
deterministic blocks world. motivation, consider policy space similar
one used originally Martin Geffner, generalized handle multiple action
arguments. Also, historical reasons, concept language based upon taxonomic
syntax (McAllester, 1991; McAllester & Givan, 1993), rather description logic
used Martin Geffner.
Comparison Predicates. relational MDPs world goal predicates,
corresponding classical planning domains, often useful polices compare
current state goal. end, introduce new set predicates, called
comparison predicates, derived world goal predicates.
world predicate p corresponding goal predicate gp, introduce new comparison
predicate cp defined conjunction p gp. is, comparison-predicate
fact true corresponding world goal predicates facts true.
88

fiAPI Policy Language Bias

example, blocks world, comparison-predicate fact con(a, b) indicates
b current state goali.e., on(a, b) gon(a, b) true.
Taxonomic Syntax. Taxonomic syntax provides language writing class expressions represent sets objects properties interest serve fundamental
pieces build policies. Class expressions built MDP predicates
(including comparison predicates applicable) variables. policy representation,
variables used denote action arguments, runtime instantiated
objects. simplicity consider predicates arity one two, call
primitive classes relations, respectively. domain contains predicates arity
three more, automatically convert multiple auxiliary binary predicates. Given
list variables X = (x1 , . . . , xk ), class expressions given by,
C[X] ::= C0 | xi | a-thing | C[X] | (R C[X]) | (min R)
R ::= R0 | R 1 | R
C[X] class expression, R relation expression, C0 primitive class, R0
primitive relation, xi variable X. Note that, classical planning domains,
primitive classes relations world, goal, comparison predicates. define depth d(C[X]) class expression C[X] one C[X] either primitive
class, a-thing, variable, (min R), otherwise define d(C[X]) d(R C[X])
d(C[X]) + 1, R relation expression C[X] class expression. given
relational MDP denote Cd [X] set class expressions C[X] depth
less.
Intuitively class expression (R C[X]) denotes set objects related
relation R object set C[X]. expression (R C[X]) denotes
set objects related R chain object C[X]this
constructor important representing recursive concepts (e.g., blocks a).
expression (min R) denotes set objects minimal relation R.
formally, let MDP state = (o1 , . . . , ok ) variable assignment,
assigns object oi variable xi . interpretation C[X] relative
set objects denoted C[X]s,O . primitive class C0 interpreted set
objects predicate symbol C0 true s. Likewise, primitive relation R0
interpreted set object tuples relation R0 holds s. class
expression a-thing denotes set objects s. class expression xi , xi
variable, interpreted singleton set {oi }. interpretation compound
expressions given by,
(C[X])s,O = {o | 6 C[X]s,O }
(R C[X])s,O = {o | o0 C[X]s,O s.t. (o0 , o) Rs,O }
(min R)s,O = {o | o0 s.t. (o, o0 ) Rs,O , 6 o0 s.t. (o0 , o) Rs,O }
(R )s,O = ID {(o1 , ov ) | o2 , . . . , ov1 s.t. (oi , oi+1 ) Rs,O 1 < v}
(R1 )s,O = {(o, o0 ) | (o0 , o) Rs,O }
C[X] class expression, R relation expression, ID identity relation.
examples useful blocks-world concepts, given primitive classes clear, gclear,
holding, con-table, along primitive relations on, gon, con, are:
89

fiFern, Yoon, & Givan

(gon1 holding) depth two, denotes block want block
held.
(on (on gclear)) depth three, denotes blocks currently blocks
want make clear.
(con con-table) depth two, denotes set blocks well constructed
towers. see note block bv class exists
sequence blocks b1 , . . . , bv b1 table goal
current state (i.e. con-table(b1 )) bi+1 bi goal current state
(i.e. con(bi , bi+1 )) 1 < v.
(gon (con con-table)) depth three, denotes blocks belong top
currently well constructed tower.
Decision List Policies represent policies decision lists action-selection rules.
rule form a(x1 , . . . , xk ) : L1 , L2 , . . . Lm , k-argument action type,
Li literals, xi action-argument variables. denote list
action argument variables X = (x1 , . . . , xk ). literal form x C[X],
C[X] taxonomic syntax class expression x action-argument variable.
Given MDP state list action-argument objects = (o1 , . . . , ok ), say
literal xi C[X] true given iff oi C[X]s,O . say rule
R = a(x1 , . . . , xk ) : L1 , L2 , . . . Lm allows action a(o1 , . . . ok ) iff literal rule
true given O. Note literals rule action type a,
possible actions type allowed rule. rule viewed placing mutual
constraints tuples objects action type applied to. Note
single rule may allow actions many actions one type. Given decision list
rules say action allowed list allowed rule list,
previous rule allows actions. Again, decision list may allow actions
multiple actions one type. decision list L MDP defines deterministic policy
[L] MDP. L allows actions state s, [L](s) least7 legal action
s; otherwise, [L](s) least legal action allowed L. important
note since [L] considers legal actions, specified action preconditions,
rules need encode preconditions, allows simpler rules learning.
words, think rule implicitly containing preconditions
action type.
example taxonomic decision list policy consider simple blocks-world domain
goal condition always clear red blocks. primitive classes
domain red, clear, holding, single relation on. following
policy solve problem domain.
putdown(x1 ) : x1 holding
pickup(x1 ) : x1 clear, x1 (on (on red))
7. action ordering relational MDP defined lexicographically terms orderings action
types objects.

90

fiAPI Policy Language Bias

first rule cause agent putdown block held. Otherwise,
block held, find block x1 clear red block (expressed
(on (on red))) pick up. Appendix B gives examples complex policies
learned system experiments.
5.3 Learning Taxonomic Decision Lists
given relational MDP, define Rd,l set action-selection rules
length l literals whose class expression depth d. Also, let
Hd,l denote policy space defined decision lists whose rules Rd,l . Since
number depth-bounded class expressions finite finite number rules,
hence Hd,l finite, though exponentially large. implementation Learn-Policy,
used main API loop, learns policy Hd,l user specified values l.
use Rivest-style decision-list learning approach (Rivest, 1987)an approach also
taken Martin Geffner (2000) learning class-based policies. primary difference
Martin Geffner (2000) technique method selecting individual
rules decision list. use greedy, heuristic search, previous work used
exhaustive enumeration approach. difference allows us find rules
complex, potential cost failing find good simple rules enumeration
might discover.
Recall Section 3, training set given Learn-Policy contains trajectories
rollout policy. learning algorithm, however, sensitive trajectory
structure (i.e., order trajectory elements) thus, simplify discussion,
take input learner training set contains union
trajectory elements. means trajectory set contains n length h
trajectories, contain total n h training examples. described Section 3,
training example form hs, (s), Q(s, a1 ), . . . , Q(s, )i, state,
(s) action selected previous policy, Q(s, ai ) Q-value estimate
Q (s, ai ). Note experiments training examples contain values
legal actions state.
Given training set D, natural learning goal find decision-list policy
training example selects action maximum estimated Q-value. learning
goal, however, problematic practice often several best (or close
best) actions measured true Q-function. case, due random sampling,
particular action looks best according Q-value estimates training set
arbitrary. Attempting learn concise policy matches arbitrary actions
difficult best likely impossible.
One approach (Lagoudakis & Parr, 2003) avoiding problem use statistical
tests determine actions clearly best (positive examples) ones
clearly best (negative examples). learner asked find
policy consistent positive negative examples. approach
shown empirical success, potential shortcoming throwing away
Q-value information. particular, may always possible find policy
exactly matches training data. cases, would like learner make informed
trade-offs regarding sub-optimal actionsi.e., prefer sub-optimal actions larger
91

fiFern, Yoon, & Givan

Learn-Decision-List (D, d, l, b)
// training set D, concept depth d, rule length l, beam width b
L nil;
(D empty)
R Learn-Rule(D, d, l, b);
{d | R covers d};
L Extend-List(L, R); // add R end list
Return L;
Learn-Rule(D, d, l, b)
// training set D, concept depth d, rule length l, beam width b
action type

// compute rule action type

Ra Beam-Search(D, d, l, b, a);
Return argmaxa Hvalue(Ra , D);
Beam-Search (D, d, l, b, a)
// training set D, concept depth d, rule length l, beam width b, action type
k arity a; X (x1 , . . . , xk );

//

L {(x C) | x X, C Cd [X]}; //

X sequence action-argument variables
construct set depth bounded candidate literals

B0 { a(X) : nil }; 1; // initialize beam single rule literals
loop
G = Bi1 {R Rd,l | R = Add-Literal(R0 , l), R0 Bi1 , l L};
Bi Beam-Select(G, b, D); //

select best b heuristic values

+ 1;
Bi1 = Bi ; //

loop improvement heuristic

Return argmaxRBi Hvalue(R, D) //

return best rule final beam

Figure 2: Pseudo-code learning decision list Hd,l given training data D.
procedure Add-Literal(R, l) simply returns rule literal l added end
rule R. procedure Beam-Select(G, w, D) selects best b rules G different
heuristic values. procedure Hvalue(R, D) returns heuristic value rule R relative
training data described text.

Q-values. motivation, describe cost-sensitive decision-list learner
sensitive full set Q-values D. learning goal roughly find decision
list selects actions large cumulative Q-value training set.
Learning List Rules. say decision list L covers training example
hs, (s), Q(s, a1 ), . . . , Q(s, )i L suggests action state s. Given set training
examples D, search decision list selects actions high Q-value via
iterative set-covering approach carried Learn-Decision-List. Decision-list rules
92

fiAPI Policy Language Bias

constructed one time order list covers training examples.
Pseudo-code algorithm given Figure 2. Initially, decision list null list
cover training examples. iteration, search high quality
rule R quality measured relative set currently uncovered training examples.
selected rule appended current decision-list, training examples newly
covered selected rule removed training set. process repeats
list covers training examples. success approach depends heavily
function Learn-Rule, selects good rule relative uncovered training
examplestypically good rule one selects actions best (or close best)
Q-value also covers significant number examples.
Learning Individual Rules. input rule learner Learn-Rule set
training examples, along depth length parameters l, beam width b.
action type a, rule learner calls routine Beam-Search find good rule
Ra Rd,l action type a. Learn-Rule returns rule Ra highest value
measured heuristic, described later section.
given action type a, procedure Beam-Search generates beam B0 , B1 . . .,
Bi set rules Rd,l action type a. sets evolve specializing
rules previous sets adding literals them, guided heuristic function. Search
begins general rule a(X) : nil, allows action type state.
Search iteration produces set Bi contains b rules highest different heuristic
values among following set8
G = Bi1 {R Rd,l | R = Add-Literal(R0 , l), R0 Bi1 , l L}
L set possible literals depth less. set includes
current best rules (those Bi1 ) also rule Rd,l formed adding
new literal rule Bi1 . search ends improvement heuristic value
occurs, Bi = Bi1 . Beam-Search returns best rule Bi according
heuristic.
Heuristic Function. training instance hs, (s), Q(s, a1 ), . . . , Q(s, )i, define Q-advantage taking action ai instead (s) state (s, ai ) = Q(s, ai )
Q(s, (s)). Likewise, Q-advantage rule R sum Q-advantages actions
allowed R s. Given rule R set training examples D, heuristic function
Hvalue(R, D) equal number training examples rule covers plus
cumulative Q-advantage rule training examples.9 Using Q-advantage rather
Q-value focuses learner toward instances large improvement previous policy possible. Naturally, one could consider using different weights coverage
Q-advantage terms, possibly tuning weight automatically using validation data.
8. Since many rules Rd,l equivalent, must prevent beam filling semantically
equivalent rules. Rather deal problem via expensive equivalence testing take ad-hoc,
practically effective approach. assume rules coincidentally heuristic
value, ones must equivalent. Thus, construct beams whose members
different heuristic values. choose rules value preferring shorter rules,
arbitrarily.
9. coverage term included, covering zero Q-advantage example
covering it. zero Q-advantage good (e.g., previous policy optimal state).

93

fiFern, Yoon, & Givan

6. Random Walk Bootstrapping
two issues critical success API technique. First, API
fundamentally limited expressiveness policy language strength
learner, dictates ability capture improved policy described training
data iteration. Second, API yield improvement Improved-Trajectories
successfully generates training data describes improved policy. large classical
planning domains, initializing API uninformed random policy typically result
essentially random training data, helpful policy improvement.
example, consider MDP corresponding 20-block blocks world initial
problem distribution generates random initial goal states. case, random
policy unlikely reach goal state within practical horizon time. Hence,
rollout trajectories unlikely reach goal, providing guidance toward learning
improved policy (i.e., policy reliably reach goal).
interested solving large domains this, providing guiding inputs
API critical. Fern, Yoon, Givan (2003), showed bootstrapping API
domain-independent heuristic planner FF (Hoffmann & Nebel, 2001), API
able uncover good policies blocks world, simplified logistics world (no planes),
stochastic variants. approach, however, limited heuristics ability
provide useful guidance, vary widely across domains.
describe new bootstrapping procedure goal-based planning domains, based
random walks, guiding API toward good policies. planning system,
evaluated Section 7, based integrating procedure API order find
policies goal-based planning domains. non-goal-based MDPs, bootstrapping
procedure directly applied, bootstrapping mechanisms must used
necessary. might include providing initial non-trivial policy, providing heuristic
function, form reward shaping (Mataric, 1994). Below, first describe
idea random-walk distributions. Next, describe use distributions
context bootstrapping API, giving new algorithm LRW-API.
6.1 Random Walk Distributions
Throughout consider MDP = hS, A, T, R, Ii correspond goal-based planning domains, described Section 5.1. Recall state corresponds
planning problem, specifying world state (via world facts) set goal conditions (via
goal facts). use terms MDP state planning problem interchangeably.
Note that, context, distribution planning problems. convenience
denote MDP states tuples = (w, g), w g sets world facts
goal facts respectively.
Given MDP state = (w, g) set goal predicates G, define s|G
MDP state (w, g 0 ) g 0 contains goal facts g applications predicate
G. Given set goal predicates G, define n-step random-walk problem
distribution RW n (M, G) following stochastic algorithm:
1. Draw random state s0 = (w0 , g0 ) initial state distribution I.
94

fiAPI Policy Language Bias

2. Starting s0 take n uniformly random actions10 , giving state sequence (s0 , . . . , sn ),
sn = (wn , g0 ) (recall actions change goal facts). uniformly
random action selection, assume extra no-op action (that change
state) selected fixed probability, reasons explained below.
3. Let g set goal facts corresponding world facts wn , e.g.,
wn = {on(a, b), clear(a)}, g = {gon(a, b), gclear(a)}. Return planning
problem (MDP state) (s0 , g)|G output.
sometimes abbreviate RW n (M, G) RW n G clear context.
Intuitively, perform well distribution policy must able achieve facts
involving goal predicates typically result n-step random walk
initial state. restricting set goal predicates G specify types facts
interested achievinge.g., blocks world may interested
achieving facts involving predicate.
random-walk distributions provide natural way span range problem difficulties. Since longer random walks tend take us initial state, small
n typically expect planning problems generated RW n become
difficult n grows. However, n becomes large, problems generated require far
fewer n steps solvei.e., direct paths initial state
end state long random walk. Eventually, since finite, problem difficulty
stop increasing n.
question raised idea whether, large n, good performance RW n
ensures good performance problem distributions interest domain.
domains, simple blocks world11 , good random-walk performance
seem yield good performance distributions interest. domains,
grid world (with keys locked doors), intuitively, random walk unlikely
uncover problem requires unlocking sequence doors. Indeed, since RW n
insensitive goal distribution underlying planning domain, random-walk
distribution may quite different.
believe good performance long random walks often useful,
addressing one component difficulty many planning benchmarks. successfully
address problems components difficulty, planner need deploy orthogonal technology landmark extraction setting subgoals (Hoffman, Porteous, &
Sebastia, 2004). example, grid world, could automatically set subgoal
possessing key first door, long random-walk policy could provide useful
macro getting key.
purpose developing bootstrapping technique API, limit focus
finding good policies long random walks. experiments, define long
specifying large walk length N . Theoretically, inclusion no-op action
definition RW ensures induced random-walk Markov chain12 aperiodic,
10. practice, select random actions set applicable actions state si , provided
simulator makes possible identify set.
11. blocks world large n, RW n generates various pairs random block configurations, typically
pairing states far apartclearly, policy performs well distribution captured
significant information blocks world.
12. dont formalize chain here, various formalizations work well.

95

fiFern, Yoon, & Givan

thus distribution states reached increasingly long random walks converges
stationary distribution13 . Thus RW = limn RW n well-defined, take
good performance RW goal.
6.2 Random-Walk Bootstrapping
MDP , define [I 0 ] MDP identical initial state
distribution replaced 0 . also define success ratio SR(, [I]) [I]
probability solves problem drawn I. Also treating random variable,
average length AL(, [I]) [I] conditional expectation solution
length problems drawn given solves I. Typically solution length
problem taken number actions, however, action costs uniform,
length taken sum action costs. Note MDP formulation
classical planning domains, given Section 5.1, policy achieves high V ()
also high success ratio low average cost.
Given MDP set goal predicates G, system attempts find good
policy [RW N ], N selected large enough adequately approximate
RW , still allowing tractable completion learning. Naively, given initial
random policy 0 , could try apply API directly. However, already discussed,
work general, since interested planning domains RW produces
extremely large difficult problems random policies provide ineffective starting
point.
However, small n (e.g., n = 1), RW n typically generates easy problems,
likely API, starting even random initial policy, reliably find good
policy RW n . Furthermore, expect policy n performs well RW n ,
also provide reasonably good, perhaps perfect, guidance problems drawn
RW moderately larger n. Thus, expect able find
good policy RW bootstrapping API initial policy n . suggests natural
iterative bootstrapping technique find good policy large n (in particular, n = N ).
Figure 3 gives pseudo-code procedure LRW-API integrates API
random-walk bootstrapping find policy long-random-walk problem distribution.
Intuitively, algorithm viewed iterating two stages: first, finding
hard enough distribution current policy (by increasing n); and, then, finding good
policy hard distribution using API. algorithm maintains current policy
current walk length n (initially n = 1). long success ratio RWn
success threshold , constant close one, simply iterate steps
approximate policy improvement. achieve success ratio policy ,
if-statement increases n success ratio RW n falls . is,
performs well enough current n-step distribution move distribution
slightly harder. constant determines much harder set small enough
likely used bootstrap policy improvement harder distribution.
(The simpler method increasing n 1 whenever success ratio achieved also
13. Markov chain may irreducible, stationary distribution may reached
initial states; however, considering one initial state, described I.

96

fiAPI Policy Language Bias

LRW-API (N, G, n, w, h, M, 0 , )
// max random-walk length N , goal predicates G
// training set size n, sampling width w, horizon h,
// MDP , initial policy 0 , discount factor .
0 ; n 1;
loop
c (n) >
SR

// Find harder n-step distribution .
c (i) < , N none;
n least [n, N ] s.t. SR
0 = [RW n (M, G)];
Improved-Trajectories(n, w, h, 0 , );
Learn-Policy(T );
satisfied
Return ;

c (n) estimates success ratio planning
Figure 3: Pseudo-code LRW-API. SR
domain problems drawn RW n (M, G) drawing set problems returning
fraction solved . Constants described text.

find good policies whenever method does. take much longer, may run
API repeatedly training set already good policy.)
n becomes equal maximum walk length N , n = N future
iterations. important note even find policy good success ratio
RW N may still possible improve average length policy. Thus,
continue API distribution satisfied success ratio
average length current policy.

7. Relational Planning Experiments
section, evaluate LRW-API technique relational MDPs corresponding
deterministic stochastic classical planning domains. first give results number
deterministic benchmark domains, showing promising results comparison stateof-the-art planner FF (Hoffmann & Nebel, 2001), also highlighting limitations
approach. Next, give results several stochastic planning domains including
domain-specific track 2004 International Probabilistic Planning Competition
(IPPC). domain definitions problem generators used experiments
available upon request.
experiments, use policy learner described Section 5.3 learn
taxonomic decision list policies. cases, number training trajectories 100,
policies restricted rules depth bound length bound l. discount
97

fiFern, Yoon, & Givan

factor always one, LRW-API always initialized policy selects
random actions. utilize maximum-walk-length parameter N = 10, 000 set
equal 0.9 0.1 respectively.
7.1 Deterministic Planning Experiments
perform experiments seven familiar STRIPS planning domains including used
AIPS-2000 planning competition, used evaluate TL-Plan Bacchus
Kabanza (2000), Gripper domain. domain standard problem generator
accepts parameters, control size difficulty randomly generated
problems. list domain parameters associated them. detailed
description domains found Hoffmann Nebel (2001).
Blocks World (n) : standard blocks worlds n blocks.
Freecell (s, c, f, l) : version Solitaire suits, c cards per suit, f freecells,
l columns.
Logistics (a,c,l,p) : logistics transportation domain airplanes, c cities, l
locations, p packages.
Schedule (p) : job shop scheduling domain p parts.
Elevator (f, p) : elevator scheduling f floors p people.
Gripper (b) : robotic gripper domain b balls.
Briefcase (i) : transportation domain items.
LRW Experiments. first set experiments evaluates ability LRW-API
find good policies RW . utilize sampling width one rollout, since
deterministic domains. Recall iteration LRW-API compute
(approximately) improved policy may also increase walk length n find harder
problem distribution. continued iterating LRW-API observed
improvement. training time per iteration approximately five hours.14 Though
initial training period significant, policy learned used solve new
problems quickly, terminating seconds solution one found, even
large problems.

Figure 4 provides data iteration LRW-API seven domains
indicated parameter settings. first column, domain, indicates
iteration number (e.g., Blocks World run 8 iterations). second column
records walk length n used learning corresponding iteration. third
fourth columns record SR AL policy learned corresponding iteration
14. timing information relatively unoptimized Scheme implementation. reimplementation
C would likely result 5-10 fold speed-up.

98

fin

RW n
SR
AL

iter. #

iter. #

API Policy Language Bias

RW
SR
AL

n

Blocks World (20)
1
2
3
4
5
6
7
8

4
14
54
54
54
54
334
334

0.92
0.94
0.56
0.78
0.88
0.98
0.84
0.99
FF

2.0
5.6
15.0
15.0
33.7
25.1
45.6
37.8

0
0.10
0.17
0.32
0.65
0.90
0.87
1
0.96

5
8
30
30
30
30
30
30
30

0.97
0.97
0.65
0.72
0.90
0.81
0.78
0.90
0.93
FF

1.4
2.7
7.0
7.1
6.7
6.7
6.8
6.9
7.7

0.08
0.26
0.78
0.85
0.85
0.89
0.87
0.89
0.93
1

RW
SR
AL

Logistics (1,2,2,6)
0
41.4
42.8
40.2
47.0
43.9
50.1
43.3
49.0

1
2
3
4
5
6
7
8
9
10

43
44
45

Freecell (4,2,2,4)
1
2
3
4
5
6
7
8
9

RW n
SR
AL

3.6
6.3
7.0
7.0
6.3
6.6
6.8
6.6
7.9
5.4

5
45
45
45
45
45
45
45
45
45

45
45
45

0.86
0.86
0.81
0.86
0.76
0.76
0.86
0.76
0.70
0.81

0.74
0.90
0.92
FF

3.1
6.5
6.9
6.8
6.1
5.9
6.2
6.9
6.1
6.1

6.4
6.9
6.6

0.25
0.28
0.31
0.28
0.28
0.32
0.39
0.31
0.19
0.25

0.25
0.39
0.38
1

11.3
7.2
8.4
8.9
7.8
8.4
9.1
11.0
7.8
7.6

9.0
9.3
9.4
13

0.48
1
1

27
34
36

0
0.2
1
1

0
38
30
28

Schedule (20)
1
2

1
4

0.79
1
FF

1
3.45

Briefcase (10)
Elevator (20,10)
1

20

1

4.0

FF

1
1

26
23

1
1

13
13

1
2
3

5
15
15

0.91
0.89
1
FF

1.4
4.2
3.0

Gripper (10)
1

10

1
FF

3.8

Figure 4: Results iteration LRW-API seven deterministic planning domains.
iteration, show walk length n used learning, along success ratio
(SR) average length (AL) learned policy RW n RW . final
policy shown domain performs = 0.9 SR walks length N = 10, 000
(with exception Logistics), iteration improve performance.
benchmark also show SR AL planner FF problems drawn
RW .

measured 100 problems drawn RW n corresponding value n (i.e.,
distribution used learning). SR exceeds , next iteration seeks
increased walk length n. fifth sixth columns record SR AL
99

fiFern, Yoon, & Givan

policy, measured 100 problems drawn LRW target distribution RW ,
experiments approximated RW N N = 10, 000.
So, example, see Blocks World total 8 iterations,
learn first one iteration n = 4, one iteration n = 14, four iterations
n = 54, two iterations n = 334. point see resulting
policy performs well RW . iterations n = N , shown, showed
improvement policy found iteration eight. domains, also observed
improvement iterating n = N , thus show iterations.
note domains except Logistics (see below) achieve policies good performance
RW N learning much shorter RW n distributions, indicating indeed
selected large enough value N capture RW , desired.
General Observations. several domains, learner bootstraps quickly
short random-walk problems, finding policy works well even much longer
random-walk problems. include Schedule, Briefcase, Gripper, Elevator. Typically, large problems domains many somewhat independent subproblems
short solutions, short random walks generate instances different typical
subproblems. domains, best LRW policy found small number
iterations performs comparably FF RW . note FF considered
good domain-independent planner domains, consider successful
result.
two domains, Logistics15 Freecell, planner unable find policy
success ratio one RW . believe result limited knowledge representation allowed policies following reasons. First, cannot write good
policies domains within current policy language. example, logistics, one
important concept set containing packages trucks truck
packages goal city. However, domain defined way concept
cannot expressed within language used experiments. Second, final learned
decision lists Logistics Freecell, Appendix B, contain much larger
number specific rules lists learned domains. indicates
learner difficulty finding general rules, within language restrictions,
applicable large portions training data, resulting poor generalization. Third,
success ratio (not shown) sampling-based rollout policy, i.e., improved policy
simulated Improved-Trajectories, substantially higher resulting
learned policy becomes policy next iteration. indicates LearnDecision-List learning much weaker policy sampling-based policy generating
training data, indicating weakness either policy language learning algorithm. example, logistics domain, iteration eight, training data learning
iteration-nine policy generated sampling rollout policy achieves success ratio
0.97 100 training problems drawn RW 45 distribution, learned
iteration-nine policy achieves success ratio 0.70, shown figure iteration
nine. Extending policy language incorporate expressiveness appears
required domains require sophisticated learning algorithm,
point future work.
15. Logistics, planner generates long sequence policies similar, oscillating success ratio
elided table ellipsis space reasons.

100

fiAPI Policy Language Bias


Domain
Blocks

FF
SR AL
0.81
60
0.28 158

Size
(20)
(50)

SR
1
1

AL
54
151

Freecell

(4,2,2,4)
(4,13,4,8)

0.36
0

15


1
0.47

10
112

Logistics

(1,2,2,6)
(3,10,2,30)

0.87
0

6


1
1

6
158

Elevator

(60,30)

1

112

1

98

Schedule

(50)

1

175

1

212

Briefcase

(10)
(50)

1
1

30
162

1
0

29


Gripper

(50)

1

149

1

149

Figure 5: Results standard problem distributions seven benchmarks. Success ratio
(SR) average length (AL) provided FF policy learned LRW
problem distribution. given domain, learned LRW policy used
problem size shown.

remaining domain, Blocks World, bootstrapping provided increasingly
long random walks appears particularly useful. policies learned walk
lengths 4, 14, 54, 334 increasingly effective target LRW distribution RW .
walks length 54 334, takes multiple iterations master provided level
difficulty beyond previous walk length. Finally, upon mastering walk length 334,
resulting policy appears perform well walk length. learned policy modestly
superior FF RW success ratio average length.
Evaluation Original Problem Distributions. domain denote
best learned LRW policyi.e., policy, domain, highest
performance RW , shown Figure 4. taxonomic decision lists corresponding
domain given Appendix B. Figure 5 shows performance ,
comparison FF, original intended problem distributions domains.
measured success ratio systems giving time limit 100 seconds solve
problem. attempted select largest problem sizes previously used
evaluation domain-specific planners, either AIPS-2000 Bacchus Kabanza
(2000), well show smaller problem size cases one planners
show performed poorly large size. case, use problem generators
provided domains, evaluate 100 problems size.
Overall, results indicate learned, reactive policies competitive
domain-independent planner FF. important remember policies
learned domain-independent fashion, thus LRW-API viewed general
approach generating domain-specific reactive planners. two domains, Blocks World
101

fiFern, Yoon, & Givan

Briefcase, learned policies substantially outperform FF success ratio, especially
large domain sizes. three domains, Elevator, Schedule, Gripper, two approaches perform quite similarly success ratio, approach superior average
length Schedule FF superior average length Elevator.
two domains, Logistics Freecell, FF substantially outperforms learned policies success ratio. believe partly due inadequate policy language,
discussed above. also believe, however, another reason poor performance
long-random-walk distribution RW correspond well standard
problem distributions. seems particularly true Freecell. policy learned
Freecell (4,2,2,4) achieved success ratio 93 percent RW , however, standard distribution achieved 36 percent. suggests RW generates problems
significantly easier standard distribution. supported fact
solutions produced FF standard distribution average twice long
produced RW . One likely reason easy random walks
end dead states Freecell, actions applicable. Thus random walk
distribution typically produce many problems goals correspond dead
states. standard distribution hand treat dead states goals.
7.2 Probabilistic Planning Experiments
present experiments three probabilistic domains described probabilistic planning domain language PPDDL (Younes, 2003).
Ground Logistics (c, p) : probabilistic version logistics airplanes, c
cities p packages. driving action probability failure domain.
Colored Blocks World (n) : probabilistic blocks world n colored blocks,
goals involve constructing towers certain color patterns. probability
moved blocks fall floor.
Boxworld (c, p) : probabilistic version full logistics c cities p packages.
Transportation actions probability going wrong direction.
Ground Logistics domain originally Boutilier et al. (2001), also used
evaluation Yoon et al. (2002). Colored Blocks World Boxworld domains
domains used hand-tailored track IPPC LRW-API technique
entered. hand-tailored track, participants provided problem generators
domain competition allowed incorporate domain knowledge
planner use competition time. provided problem generators LRW-API
learned policies domains, entered competition.
also conducted experiments probabilistic domains Yoon et al.
(2002), including variants blocks world variant Ground Logistics,
appeared Fern et al. (2003). However, show results since
qualitatively identical deterministic blocks world results described
Ground Logistics results show below.
three probabilistic domains, conducted LRW experiments using
procedure above. parameters given LRW-API except
102

fin

SR

RW n
AL

iter. #

iter. #

API Policy Language Bias

RW
SR
AL

Boxworld (10,5)
1
10 0.73
4.3
2
10 0.93
2.3
3
20 0.91
4.4
4
40 0.96
6.1
5 170 0.62
30.8
37.9
6 170 0.49
7 170 0.63
29.3
29.1
8 170 0.63
9 170 0.48
36.4
Standard Distribution (15,15)

n

SR

RW n
AL

SR

RW
AL

Ground Logistics (3,4,4,3)
0.03
0.13
0.17
0.31
0.25
0.17
0.21
0.18
0.17
0

61.5
58.4
55.9
50.4
52.2
55.7
55
55.3
55.3


1
5 0.95
2
10 0.97
3 160
1
Standard Distribution

2.71
2.06
6.41
(5,7,7,20)

0.17
0.84
1
1

168.9
17.5
7.2
20

Colored Blocks World (10)
1
2
3
4
5

2 0.86
1.7
5 0.89
8.4
40 0.92
11.7
100 0.76
37.5
100 0.94
20.0
Standard Distribution (50)

0.19
0.81
0.85
0.77
0.95
0.95

93.6
40.8
32.7
38.5
21.9
123

Figure 6: Results iteration LRW-API three probabilistic planning domains.
iteration, show walk length n used learning, along success ratio
(SR) average length (AL) learned policy RW n RW .
benchmark, show performance standard problem distribution policy whose
performance best RW .
sampling width used rollout set w = 10, set 0.85 order
account stochasticity domains. results experiments shown
Figure 6. tables form Figure 4 last row given
domain gives performance standard distribution, i.e., problems drawn
domains problem generator. Colored Blocks World problem generator
produces problems whose goals specified using existential quantifiers. example,
simple goal may exists blocks x x red, blue x y.
Since policy language cannot directly handle existentially quantified goals preprocess
planning problems produced problem generator remove them. done
assigning particular block names existential variables, ensuring static
properties block (in case color) satisfied static properties variable
assigned to. domain, finding assignment trivial, resulting
assignment taken goal, giving planning problem learned policy
applied. Since blocks world states fully connected, resulting goal always
guaranteed achievable.

Boxworld, LRW-API able find good policy RW standard
distribution. Again, deterministic Logistics Freecell, believe
primarily restricted policy languages currently used learner.
Here, domains, see decision list learned Boxworld contains many
specific rules, indicating learner able generalize well beyond
103

fiFern, Yoon, & Givan

training trajectories. Ground Logistics, see LRW-API quickly finds good
policy RW standard distribution.
Colored Blocks World, also see LRW-API able quickly find good
policy RW standard distribution. However, unlike deterministic
(uncolored) blocks world, success ratio observed less one, solving 95
percent problems. unclear, LRW-API able find perfect policy.
relatively easy hand-code policy Colored Blocks World using language
learner, hence inadequate knowledge representation answer. predicates
action types domain deterministic counterpart
stochastic variants previously considered. difference apparently
interacts badly learners search bias, causing fail find perfect policy.
Nevertheless, two results, along probabilistic planning results shown
here, indicate good policy expressible language, LRW-API
find good policies complex relational MDPs. makes LRW-API one
techniques simultaneously cope complexity resulting stochasticity
relational structure domains these.

8. Related Work
Boutilier et al. (2001) presented first exact solution technique relational MDPs
based structured dynamic programming. However, practical implementation
approach provided, primarily due need simplification first-order
logic formulas. ideas, however, served basis logic-programming-based
system (Kersting, Van Otterlo, & DeRaedt, 2004) successfully applied blocksworld problems involving simple goals simplified logistics world. style approach
inherently limited domains exact value functions and/or policies
compactly represented chosen knowledge representation. Unfortunately,
generally case types domains consider here, particularly planning
horizon grows. Nevertheless, providing techniques directly reason
MDP model important direction. Note API approach essentially ignores
underlying MDP model, simply interacts MDP simulator black box.
interesting research direction consider principled approximations techniques discover good policies difficult domains. considered
Guestrin et al. (2003a), class-based MDP value function representation
used compute approximate value function could generalize across different sets
objects. Promising empirical results shown multi-agent tactical battle domain.
Presently class-based representation support representation features commonly found classical planning domains (e.g., relational facts
on(a, b) change time), thus directly applicable contexts. However, extending work richer representations interesting direction. ability
reason globally domain may give advantages compared API.
approach closely related work relational reinforcement learning (RRL) (Dzeroski et al., 2001), form online API learns relational value-function approximations. Q-value functions learned form relational decision trees (Q-trees)
used learn corresponding policies (P -trees). RRL results clearly demonstrate
104

fiAPI Policy Language Bias

difficulty learning value-function approximations relational domains. Compared P trees, Q-trees tend generalize poorly much larger. RRL yet demonstrated
scalability problems complex considered hereprevious RRL blocks-world
experiments include relatively simple goals16 , lead value functions much
less complex ones here. reason, suspect RRL would difficulty
domains consider, precisely value-function approximation step
avoid; however, needs experimentally tested.
note, however, API approach advantage using unconstrained
simulator, whereas RRL learns irreversible world experience (pure RL). using
simulator, able estimate Q-values actions training state,
providing us rich training data. Without simulator, RRL able directly
estimate Q-value action training statethus, RRL learns Q-tree
provide estimates Q-value information needed learn P -tree. way, valuefunction learning serves critical role simulator unavailable. believe,
many relational planning problems, possible learn model simulator
world experiencein case, API approach incorporated planning
component RRL. Otherwise, finding ways either avoid learning effectively
learn relational value-functions RRL interesting research direction.
Researchers classical planning long studied techniques learning improve
planning performance. collection survey work learning planning domains see Minton (1993) Zimmerman Kambhampati (2003). Two primary approaches learn domain-specific control rules guiding search-based planners e.g.,
Minton, Carbonell, Knoblock, Kuokka, Etzioni, Gil (1989), Veloso, Carbonell, Perez,
Borrajo, Fink, Blythe (1995), Estlin Mooney (1996), Huang, Selman, Kautz
(2000), Ambite, Knoblock, Minton (2000), Aler, Borrajo, Isasi (2002), and,
closely related, learn domain-specific reactive control policies (Khardon, 1999a; Martin
& Geffner, 2000; Yoon et al., 2002).
Regarding latter, work novel using API iteratively improve stand-alone
control policies. Regarding former, theory, search-based planners iteratively
improved continually adding newly learned control knowledgehowever, difficult avoid utility problem (Minton, 1988), i.e., swamped low utility rules.
Critically, policy-language bias confronts issue preferring simpler policies.
learning approach also tied base planner (let alone tied single particular base planner), unlike previous work. Rather, require domain simulator.
ultimate goal systems allow planning large, difficult problems
beyond reach domain-independent planning technology. Clearly, learning
achieve goal requires form bootstrapping almost previous systems
relied human purpose. far, common human-bootstrapping
approach learning small problems. Here, human provides small problem
distribution learner, limiting number objects (e.g., using 2-5 blocks
blocks world), control knowledge learned small problems. approach
work, human must ensure small distribution good control knowledge
small problems also good large target distribution. contrast, long16. complex blocks-world goal RRL achieve on(A, B) n block environment.
consider blocks-world goals involve n blocks.

105

fiFern, Yoon, & Givan

random-walk bootstrapping approach applied without human assistance directly
large planning domains. However, already pointed out, goal performing well
LRW distribution may always correspond well particular target problem
distribution.
bootstrapping approach similar spirit bootstrapping framework learning exercises(Natarajan, 1989; Reddy & Tadepalli, 1997). Here, learner provided planning problems, exercises, order increasing difficulty. learning
easier problems, learner able use new knowledge, skills, order bootstrap learning harder problems. work, however, previously relied
human provide exercises, typically requires insight planning domain
underlying form control knowledge planner. work viewed
automatic instantiation learning exercises, specifically designed learning LRW
policies.
random-walk bootstrapping similar approach used Micro-Hillary
(Finkelstein & Markovitch, 1998), macro-learning system problem solving.
work, instead generating problems via random walks starting initial state, random
walks generated backward goal states. approach assumes actions
invertible given set backward actions. assumptions hold,
backward random-walk approach may preferable provided goal
distribution match well goals generated forward random walks.
course, cases forward random walks may preferable. Micro-Hillary
empirically tested N N sliding-puzzle domain; however, discussed work,
remain challenges applying system complex domains parameterized actions recursive structure, familiar STRIPS domains. best
knowledge, idea learning random walks previously explored
context STRIPS planning domains.
idea searching good policy directly policy space rather value-function
space primary motivation policy-gradient RL algorithms. However, algorithms
largely explored context parametric policy spaces. approach
demonstrated impressive success number domains, appears difficult define
policy spaces types planning problem considered here.
API approach viewed type reduction planning reinforcement
learning classification learning. is, solve MDP generating solving
series cost-sensitive classification problems. Recently, several
proposals reducing reinforcement learning classification. Dietterich Wang (2001)
proposed reinforcement learning approach based batch value function approximation.
One proposed approximations enforced learned approximation assign
best action highest value, type classifier learning. Lagoudakis Parr
(2003) proposed classification-based API approach closely related ours. primary difference form classification problem produced iteration.
generate standard multi-class classification problems, whereas generate cost-sensitive
problems. Bagnell, Kakade, Ng, Schneider (2003) introduced closely related algorithm learning non-stationary policies reinforcement learning. specified horizon
time h, approach learns sequence h policies. iteration, policies
held fixed except one, optimized forming classification problem via policy
106

fiAPI Policy Language Bias

rollout17 . Finally, Langford Zadrozny (2004) provide formal reduction reinforcement learning classification, showing -accurate classification learning implies
near-optimal reinforcement learning. approach uses optimistic variant sparse
sampling generate h classification problems, one horizon time step.

9. Summary Future Work
introduced new variant API learns policies directly, without representing
approximate value functions. allowed us utilize relational policy language
learning compact policy representations. also introduced new API bootstrapping
technique goal-based planning domains. experiments show LRW-API
algorithm, combines techniques, able find good policies variety
relational MDPs corresponding classical planning domains stochastic variants.
know previous MDP technique successfully applied problems
these.
experiments also pointed number weaknesses current approach. First,
bootstrapping technique, based long random walks, always correspond
well problem distribution interest. Investigating automatic bootstrapping
techniques interesting direction, related general problems exploration
reward shaping reinforcement learning. Second, seen limitations
current policy language learner partly responsible failures
system. cases, must either: 1) depend human provide useful features
system, 2) extend policy language develop advanced learning techniques. Policy-language extensions considering include various extensions
knowledge representation used represent sets objects domain (in particular,
route-finding maps/grids), well non-reactive policies incorporate search
decision-making.
consider ever complex planning domains, inevitable brute-force
enumeration approach learning policies trajectories scale. Presently
policy learner, well entire API technique, makes attempt use definition
domain one available. believe developing learner exploit
information bias search good policies important direction future work.
Recently, Gretton Thiebaux (2004) taken step direction using logical
regression (based domain model) generate candidate rules learner. Developing tractable variations approach promising research direction. addition,
exploring ways incorporating domain model approach modelblind approaches critical. Ultimately, scalable AI planning systems need combine
experience stronger forms explicit reasoning.

17. initial state distribution dictated policies previous time steps, held fixed.
Likewise actions selected along rollout trajectories dictated policies future time steps,
also held fixed.

107

fiFern, Yoon, & Givan

Acknowledgments
would like thank Lin Zhu originally suggesting idea using random walks
bootstrapping. would also like thank reviewers editors helping
vastly improve paper. work supported part NSF grants 9977981-IIS
0093100-IIS.

Appendix A. Omitted Proofs
Proposition 1. Let H finite class deterministic policies. H,

set n = 1 ln |H|
trajectories drawn independently Dh , 1 probability
every H consistent trajectories satisfies V () V () 2Vmax ( + h ).
Proof: first introduce basic properties notation used below.
deterministic policy , consistent trajectory t, Dh (t) entirely
determined underlying MDP transition dynamics. implies two deterministic policies 0 consistent trajectory Dh (t) = Dh (t).
denote v(t) cumulative discounted reward accumulated executing trajectory
P
t. policy , V h () = Dh (t) v(t) summation taken
length h trajectories (or simply consistent ). Finally set
P
trajectories let Dh () = t0 Dh (t) giving cumulative probability
generating trajectories .
Consider particular H H consistent n trajectories
. let denote set length h trajectories consistent
denote set trajectories consistent . Following Khardon (1999b)
first give standard argument showing high probability Dh () > 1 . see
consider probability consistent n = 1 ln |H|
trajectories


given Dh () 1 . probability occurs (1 )n < en = |H|
.

Thus probability choosing |H| |H|
= . Thus, probability

least 1 know Dh () > 1 . Note Dh () = Dh ().
given condition Dh () > 1 show V h () V h () 2Vmax
considering difference two value functions.
V h () V h () =

X

Dh (t) v(t)



=

X

=

v(t) +

X

(Dh (t) Dh (t)) v(t)



Dh (t)





Dh (t) v(t)



Dh (t)



X

X

v(t) + 0



Dh (t)



) + Dh ( )]

Dh () + 1 Dh ()]

Vmax [Dh (

= Vmax [1

X

2Vmax
108

X

v(t)

Dh (t) v(t)

fiAPI Policy Language Bias

third lines follows since Dh (t) = Dh (t) consistent t.
last line follows substituting assumption Dh () = Dh () > 1 previous
line. Combining result approximation due using finite horizon,
V () V () V h () V h () + 2 h Vmax
get probability least 1 , V () V () 2Vmax ( + h ), completes
proof. 2
Proposition 2.


MDP Q-advantage least , 0 < 0 < 1,

h > log


8Vmax

8Vmax



2



w >
=

2

ln

|A|
0

state s, A(, s) = (s) probability least 1 0 .
Proof: Given real valued random variable X bounded absolute value Xmax
average X w independently drawn samples X,
q additive Chernoff bound states
probability least 1 , |E[X] X| Xmax wln .
Note Qh (s, a) expectation random variable X(s, a) = R(s, a) +

Vh1 (T (s, a)) Q(s, a) simply average w independent samples X(s, a).
0
Chernoff bound tells us probability least 1 |A|
, |Qh (s, a) Q(s, a)|
q

0


Vmax ln |A|ln
, |A| number actions. Substituting choice w
w

get probability least 1 0 , |Qh (s, a) Q(s, a)| < 8 satisfied actions
simultaneously. also know |Q (s, a) Qh (s, a)| h Vmax , choice

h gives, |Q (s, a) Qh (s, a)| < 8 . Combining relationships get

probability least 1 0 , |Q (s, a) Q(s, a)| < 4 holds actions simultaneously.
use bound show high probability Q-value estimates

actions (s) within 2 range other, actions outside
range. particular, consider action (s) action a0 .
a0 (s) Q (s, a) = Q (s, a0 ). bound get

|Q(s, a) Q(s, a0 )| < 2 . Otherwise a0 6 (s) assumption MDP
Q-advantage get Q (s, a) Q (s, a0 ) . Using bound implies

Q(s, a) Q(s, a0 ) > 2 . relationships definition A(, s) imply
probability least 1 0 A(, s) = (s). 2

Appendix B. Learned Policies
give final taxonomic-decision-list policies learned domain
experiments. Rather write rules form a(x1 , . . . , xk ) : L1 L2 Lm
109

fiFern, Yoon, & Givan

drop variables head simply write, : L1 L2 Lm . addition
use notation R short-hand (R1 ) R relation. interpreting policies, important remember rule action type a,
preconditions action type implicitly included constraints. Thus, rules
often allow actions legal, actions never considered
system.
Gripper
1. MOVE: (X1 (NOT (GAT (CARRY1 GRIPPER)))) (X2 (NOT (GAT (AT1 AT-ROBBY)))) (X2 (GAT (NOT
(CAT1 ROOM)))) (X1 (CAT BALL))
2. DROP: (X1 (GAT1 AT-ROBBY))
3. PICK: (X1 (GAT1 (GAT (CARRY1 GRIPPER)))) (X1 (GAT1 (NOT AT-ROBBY)))
4. PICK: (X2 (AT (NOT (GAT1 ROOM)))) (X1 (GAT1 (NOT AT-ROBBY)))
5. PICK: (X1 (GAT1 (NOT AT-ROBBY)))
Briefcase
1. PUT-IN: (X1 (GAT1 (NOT IS-AT)))
2. MOVE: (X2 (AT (NOT (CAT1 LOCATION)))) (X2 (NOT (AT (GAT1 CIS-AT))))
3. MOVE: (X2 (GAT IN)) (X1 (NOT (CAT IN)))
4. TAKE-OUT: (X1 (CAT1 IS-AT))
5. MOVE: (X2 GIS-AT)
6. MOVE: (X2 (AT (GAT1 CIS-AT)))
7. PUT-IN: (X1 UNIVERSAL)
Schedule
1. DO-IMMERSION-PAINT: (X1 (NOT (PAINTED1 X2 ))) (X1 (GPAINTED1 X2 ))
2. DO-DRILL-PRESS: (X1 (GHAS-HOLEO1 X3 )) (X1 (GHAS-HOLEW1 X2 ))
3. DO-LATHE: (X1 (NOT (SHAPE1 CYLINDRICAL))) (X1 (GSHAPE1 CYLINDRICAL))
4. DO-DRILL-PRESS: (X1 (GHAS-HOLEW1 X2 ))
5. DO-DRILL-PRESS: (X1 (GHAS-HOLEO1 X3 ))
6. DO-GRIND: (X1 (NOT (SURFACE-CONDITION1 SMOOTH))) (X1 (GSURFACE-CONDITION1 SMOOTH))
7. DO-POLISH: (X1 (NOT (SURFACE-CONDITION1 POLISHED))) (X1 (GSURFACE-CONDITION1 POLISHED))
8. DO-TIME-STEP:
Elevator
1. DEPART: (X2 GSERVED)
2. DOWN: (X2 (DESTIN BOARDED)) (X2 (DESTIN GSERVED))
3. UP: (X2 (DESTIN BOARDED)) (X2 (DESTIN GSERVED)) (X2 (ABOVE (ORIGIN BOARDED))) (X1 (NOT
(DESTIN BOARDED)))
4. BOARD: (X2 (NOT CSERVED)) (X2 GSERVED)
5. UP: (X2 (ORIGIN GSERVED)) (X2 (NOT (DESTIN BOARDED))) (X2 (NOT (DESTIN GSERVED))) (X2
(ORIGIN (NOT CSERVED))) (X2 (ABOVE (DESTIN PASSENGER))) (X1 (NOT (DESTIN BOARDED)))
6. DOWN: (X2 (ORIGIN GSERVED)) (X2 (ORIGIN (NOT CSERVED))) (X1 (NOT (DESTIN BOARDED)))

110

fiAPI Policy Language Bias

7. UP: (X2 (NOT (ORIGIN BOARDED))) (X2 (NOT (DESTIN BOARDED)))
FreeCell
1. SENDTOHOME: (X1 (CANSTACK1 (CANSTACK (SUIT1 (SUIT INCELL))))) (X5 (NOT GHOME))
2. MOVE-B: (X2 (NOT (CANSTACK (ON GHOME)))) (X2 (CANSTACK GHOME)) (X2 (VALUE1 (NOT
COLSPACE))) (X1 (CANSTACK1 (SUIT1 (SUIT BOTTOMCOL))))
3. MOVE: (X1 (CANSTACK1 (ON (CANSTACK1 (ON1 GHOME))))) (X3 (CANSTACK (ON (SUIT1 (SUIT BOTTOMCOL))))) (X1 (ON1 BOTTOMCOL)) (X1 (CANSTACK1 (ON GHOME))) (X3 (ON1 (CANSTACK1
(ON1 (NOT (CANSTACK (VALUE1 CELLSPACE))))))) (X1 (NOT (CANSTACK1 (SUIT1 (SUIT INCELL)))))
(X3 (CANSTACK BOTTOMCOL)) (X1 (SUIT1 (SUIT (ON1 (NOT (CANSTACK (VALUE1 CELLSPACE)))))))
(X1 (VALUE1 (NOT COLSPACE))) (0 (ON1 (NOT (CANSTACK1 (SUIT1 (SUIT INCELL)))))) (X1 (NOT
(CANSTACK1 CHOME)))
4. SENDTOHOME-B: (X4 (NOT GHOME))
5. SENDTOHOME: (X1 (ON1 (CANSTACK (CANSTACK1 (SUIT1 (SUIT INCELL)))))) (X5 (NOT GHOME))
6. SENDTOHOME: (X1 (ON1 (ON1 GHOME))) (X1 (CANSTACK1 (NOT GHOME))) (X1 (CANSTACK1 (NOT
(ON1 GHOME)))) (X5 (NOT GHOME))
7. MOVE-B: (X1 (NOT (CANSTACK1 GHOME))) (X2 (VALUE1 (NOT COLSPACE))) (X1 (CANSTACK1
(SUIT1 (SUIT BOTTOMCOL))))
8. SENDTOFREE: (X1 (ON1 (ON1 GHOME))) (X1 (NOT GHOME))
9. SENDTOHOME: (X5 (CANSTACK1 (CANSTACK (ON GHOME)))) (X5 (NOT GHOME))
10. SENDTOHOME: (0 GHOME) (X5 (VALUE1 (NOT COLSPACE))) (X5 (NOT (CANSTACK1 (ON1 (NOT
GHOME))))) (X1 (ON1 (NOT (ON1 GHOME)))) (X5 (NOT GHOME))
11. NEWCOLFROMFREECELL: (X1 GHOME)
12. SENDTOHOME: (X5 (CANSTACK1 (ON GHOME))) (X1 GHOME) (X5 (NOT GHOME))
13. MOVE-B: (X1 (VALUE1 (VALUE HOME))) (X2 (VALUE1 (NOT COLSPACE))) (X1 (CANSTACK1 (SUIT1
(SUIT BOTTOMCOL))))
14. SENDTOHOME: (X1 (CANSTACK1 (ON1 (CANSTACK1 (SUIT1 (SUIT INCELL)))))) (X5 (NOT GHOME))
15. SENDTOHOME: (X1 (ON1 (ON1 (CANSTACK1 (ON1 (NOT GHOME)))))) (X5 (NOT GHOME))
16. SENDTOFREE: (X1 (CANSTACK1 (ON (ON1 GHOME)))) (X1 (SUIT1 (SUIT BOTTOMCOL))) (X1 (ON1
BOTTOMCOL))
17. MOVE: (X3 (ON1 (CANSTACK1 CLEAR))) (X1 (ON1 (CANSTACK (ON1 (NOT (CANSTACK (VALUE1
CELLSPACE))))))) (X3 (NOT GHOME)) (X1 GHOME) (X3 (CANSTACK BOTTOMCOL)) (X3 (ON1
(CANSTACK1 (ON1 (NOT (CANSTACK (VALUE1 CELLSPACE))))))) (X1 (NOT (CANSTACK1 (SUIT1
(SUIT INCELL))))) (X1 (ON1 BOTTOMCOL)) (X1 (SUIT1 (SUIT (ON1 (NOT (CANSTACK (VALUE1
CELLSPACE))))))) (X1 (VALUE1 (NOT COLSPACE))) (X1 (ON1 (NOT (CANSTACK1 (SUIT1 (SUIT
INCELL)))))) (X1 (NOT (CANSTACK1 CHOME)))
18. MOVE: (X1 (SUIT1 (SUIT CHOME))) (X3 (NOT GHOME)) (X3 (NOT (ON1 GHOME))) (X1 (ON1
(CANSTACK1 BOTTOMCOL)))
19. SENDTOHOME: (X1 (CANSTACK (ON (CANSTACK (ON GHOME))))) (X1 GHOME) (X5 (NOT GHOME))
20. SENDTOHOME: (X1 (CANSTACK1 (ON (CANSTACK1 (ON1 GHOME))))) (X1 (NOT (SUIT1 (SUIT BOTTOMCOL)))) (X5 (NOT GHOME))
21. SENDTOFREE: (X1 (CANSTACK (ON (CANSTACK (VALUE1 CELLSPACE))))) (X1 (CANSTACK CHOME))
22. SENDTOHOME: (X1 (CANSTACK1 (SUIT1 (SUIT INCELL)))) (X1 (ON1 (NOT (CANSTACK (VALUE1
CELLSPACE))))) (X5 (NOT GHOME))
23. SENDTONEWCOL: (X1 (CANSTACK (CANSTACK1 (ON1 GHOME))))
24. SENDTOFREE: (X1 (CANSTACK (ON1 (CANSTACK1 (ON1 GHOME))))) (X1 (NOT (CANSTACK GHOME)))
(X1 (NOT (ON1 GHOME))) (X1 (ON1 (NOT (CANSTACK1 (SUIT1 (SUIT INCELL))))))

111

fiFern, Yoon, & Givan

25. SENDTOFREE: (X1 (ON1 (CANSTACK (CANSTACK1 (ON1 GHOME))))) (X1 (NOT (CANSTACK BOTTOMCOL))) (X1 (NOT (CANSTACK1 (CANSTACK (ON GHOME)))))
26. SENDTOFREE: (X1 (CANSTACK (ON1 (CANSTACK1 (ON1 (NOT GHOME)))))) (X1 (NOT (CANSTACK
GHOME))) (X1 (CANSTACK (NOT (SUIT1 (SUIT BOTTOMCOL)))))
27. SENDTOHOME: (X1 (CANSTACK1 (CANSTACK (ON1 GHOME)))) (X1 (ON1 (CANSTACK1 (ON1 (NOT
GHOME))))) (X1 (NOT GHOME)) (X5 (NOT GHOME))
28. SENDTOFREE: (X1 (CANSTACK (ON1 (CANSTACK1 (ON1 (NOT GHOME)))))) (X1 (CANSTACK (CANSTACK1
(ON1 GHOME)))) (X1 (NOT GHOME)) (X1 (ON1 (CANSTACK1 (ON1 (NOT (CANSTACK (VALUE1
CELLSPACE)))))))
29. SENDTOFREE: (X1 (CANSTACK CHOME)) (X1 (SUIT1 (SUIT (CANSTACK1 (ON1 GHOME)))))
30. SENDTOHOME: (X1 GHOME) (X1 (SUIT1 (SUIT BOTTOMCOL))) (X1 (CANSTACK1 (NOT (ON1
GHOME)))) (X5 (NOT GHOME))
31. SENDTOFREE: (X1 (CANSTACK1 (ON1 GHOME))) (X1 (CANSTACK1 (ON1 (NOT GHOME))))
32. SENDTOFREE: (X1 (CANSTACK (ON1 GHOME))) (X1 (NOT GHOME)) (X1 (ON1 (CANSTACK1 (ON1
(NOT GHOME)))))
33. SENDTOHOME: (X1 (ON1 (CANSTACK1 BOTTOMCOL))) (X1 (CANSTACK1 (NOT GHOME))) (X5
(NOT GHOME))
34. SENDTOFREE: (X1 (CANSTACK (ON (CANSTACK1 (ON1 (NOT GHOME)))))) (X1 (NOT (SUIT1 (SUIT
BOTTOMCOL)))) (X1 (NOT GHOME))
35. SENDTOHOME: (X1 (NOT (CANSTACK1 GHOME))) (X1 (NOT (SUIT1 (SUIT BOTTOMCOL)))) (X5
(NOT GHOME))
36. SENDTOFREE: (X1 (NOT (ON1 GHOME))) (X1 (CANSTACK (CANSTACK1 (ON1 (NOT GHOME)))))
37. SENDTOFREE-B: (X1 (NOT GHOME))
38. SENDTOFREE: (X1 UNIVERSAL)
Logistics
1. FLY-AIRPLANE: (X1 (IN (GAT1 AIRPORT))) (X1 (NOT (IN (GAT1 (AT AIRPLANE))))) (X3 (NOT (GAT
(IN1 TRUCK)))) (X1 (NOT (IN (GAT1 (NOT AIRPORT)))))
2. LOAD-TRUCK: (X2 (IN (NOT (GAT1 (NOT AIRPORT))))) (X1 (GAT1 (GAT (IN1 TRUCK)))) (X1 (NOT
(CAT1 LOCATION)))
3. DRIVE-TRUCK: (X3 (AT (AT1 (GAT (IN1 TRUCK))))) (X3 (IN-CITY1 (IN-CITY (AT AIRPLANE)))) (X1
(AT1 (NOT (GAT (IN1 TRUCK)))))
4. UNLOAD-TRUCK: (X1 (GAT1 (AT (IN OBJ)))) (X1 (GAT1 (AT OBJ))) (X1 (NOT (GAT1 (AT AIRPLANE)))) (X2 (AT1 (GAT (IN1 TRUCK)))) (X1 (GAT1 (AT TRUCK)))
5. FLY-AIRPLANE: (X3 (GAT (IN1 AIRPLANE))) (X1 (IN (NOT (GAT1 (AT TRUCK))))) (X1 (AT1 (NOT
(GAT (IN1 TRUCK)))))
6. UNLOAD-AIRPLANE: (X2 (NOT (IN (GAT1 (NOT AIRPORT))))) (X1 (GAT1 (AT AIRPLANE)))
7. LOAD-TRUCK: (X2 (IN (NOT (GAT1 LOCATION)))) (X1 (NOT (GAT1 (AT TRUCK)))) (X1 (GAT1
LOCATION))
8. UNLOAD-TRUCK: (X1 (GAT1 (AT TRUCK))) (X2 (AT1 AIRPORT)) (X2 (NOT (IN (GAT1 (NOT AIRPORT))))) (X1 (GAT1 (AT AIRPLANE)))
9. FLY-AIRPLANE: (X3 (AT (AT1 (GAT (IN1 TRUCK))))) (X1 (AT1 (GAT (GAT1 LOCATION)))) (X1
(NOT (AT1 (CAT OBJ))))
10. DRIVE-TRUCK: (X1 (IN (GAT1 LOCATION))) (X1 (AT1 (NOT (GAT (IN1 TRUCK))))) (X1 (AT1 (NOT
(AT AIRPLANE))))
11. UNLOAD-TRUCK: (X2 (AT1 (GAT (GAT1 (NOT AIRPORT))))) (X1 (NOT (GAT1 AIRPORT)))
12. FLY-AIRPLANE: (X3 (NOT (GAT (GAT1 LOCATION)))) (X1 (AT1 (GAT (AT1 (CAT OBJ))))) (X3 (AT
(NOT (GAT1 (AT AIRPLANE))))) (X3 (AT OBJ)) (X1 (NOT (IN (GAT1 AIRPORT)))) (X3 (NOT (AT
(IN OBJ))))

112

fiAPI Policy Language Bias

13. UNLOAD-TRUCK: (X1 (GAT1 AIRPORT))
14. LOAD-TRUCK: (X1 (AT1 (CAT (GAT1 (AT AIRPLANE))))) (X1 (NOT (GAT1 LOCATION)))
15. LOAD-TRUCK: (X1 (GAT1 (CAT (GAT1 (AT AIRPLANE))))) (X1 (NOT (GAT1 (AT TRUCK)))) (X1
(GAT1 (AT (GAT1 (AT AIRPLANE)))))
16. LOAD-TRUCK: (X1 (GAT1 (NOT AIRPORT))) (X1 (NOT (GAT1 (AT TRUCK))))
17. FLY-AIRPLANE: (X3 (AT (GAT1 (AT AIRPLANE)))) (X1 (AT1 (CAT OBJ)))
18. FLY-AIRPLANE: (X3 (NOT (GAT (AT1 (CAT OBJ))))) (X1 (AT1 (GAT (AT1 (CAT OBJ))))) (X1 (AT1
(GAT (GAT1 (AT TRUCK)))))
19. LOAD-TRUCK: (X1 (GAT1 (AT AIRPLANE))) (X1 (NOT (GAT1 (AT TRUCK)))) (X1 (AT1 (CAT OBJ)))
20. LOAD-AIRPLANE: (X1 (GAT1 AIRPORT)) (X1 (NOT (CAT1 LOCATION))) (X1 (GAT1 (NOT (AT
AIRPLANE)))) (X2 (NOT (IN (GAT1 (NOT AIRPORT)))))
21. FLY-AIRPLANE: (X3 (AT (GAT1 (AT AIRPLANE)))) (X3 (NOT (AT TRUCK)))
22. LOAD-TRUCK: (X1 (AT1 (CAT (GAT1 (NOT AIRPORT))))) (X1 (GAT1 AIRPORT))
23. DRIVE-TRUCK: (X3 (NOT (AT OBJ))) (X1 (NOT (AT1 (CAT OBJ)))) (X1 (AT1 (GAT (GAT1 LOCATION))))
24. LOAD-TRUCK: (X1 (GAT1 (CAT (CAT1 AIRPORT)))) (X1 (NOT (CAT1 LOCATION)))
25. FLY-AIRPLANE: (X3 (AT (GAT1 (AT AIRPLANE)))) (X1 (AT1 (AT OBJ)))
26. DRIVE-TRUCK: (X1 (IN OBJ))
27. DRIVE-TRUCK: (X1 (AT1 (GAT (GAT1 AIRPORT)))) (X3 (AT (GAT1 AIRPORT))) (X1 (AT1 (NOT
(AT AIRPLANE))))
28. FLY-AIRPLANE: (X3 (CAT (GAT1 (AT TRUCK)))) (X1 (AT1 (GAT (GAT1 LOCATION))))
29. LOAD-TRUCK: (X1 (GAT1 (AT OBJ))) (X1 (NOT (CAT1 LOCATION)))
30. DRIVE-TRUCK: (X3 (AT (GAT1 (AT AIRPLANE)))) (X1 (NOT (AT1 (CAT OBJ))))
31. DRIVE-TRUCK: (X3 (AT AIRPLANE)) (X3 (AT (GAT1 (AT TRUCK))))
32. UNLOAD-AIRPLANE: (X2 (NOT (AT1 (CAT OBJ)))) (X1 (GAT1 (NOT AIRPORT)))
33. DRIVE-TRUCK: (X3 (AT (GAT1 (AT TRUCK))))
34. LOAD-TRUCK: (X1 (AT1 (NOT AIRPORT))) (X1 (GAT1 AIRPORT))
35. FLY-AIRPLANE: (X3 (AT (GAT1 LOCATION)))
36. FLY-AIRPLANE: (X1 (IN OBJ)) (X3 (NOT (GAT (GAT1 LOCATION)))) (X1 (NOT (IN (GAT1 AIRPORT))))
(X3 (NOT (AT (IN OBJ)))) (X1 (AT1 (GAT (AT1 (CAT OBJ))))))
37. DRIVE-TRUCK: (X1 (AT1 (AT AIRPLANE)))
38. LOAD-AIRPLANE: (X1 (GAT1 (NOT AIRPORT)))
Blocks World
1. STACK: (X2 (GON HOLDING)) (X2 (CON (MIN GON))) (X1 (GON ON-TABLE))
2. PUTDOWN:
3. UNSTACK: (X1 (ON (ON (MIN GON)))) (X2 (CON (ON (MIN GON))))
4. UNSTACK: (X2 (ON1 (GON CLEAR))) (X2 (GON (ON (MIN GON)))) (X1 (ON (GON ON-TABLE)))
(X1 (GON (NOT CLEAR)))
5. PICKUP: (X1 (GON1 (CON (MIN GON)))) (X1 (GON1 CLEAR)) (X1 (GON1 (CON ON-TABLE)))
6. UNSTACK: (X2 (CON (GON1 CLEAR))) (X1 (GON1 (ON (MIN GON)))) (X1 (GON1 (CON
CLEAR)))

113

fiFern, Yoon, & Givan

7. UNSTACK: (X1 (NOT (GON (MIN GON))))
8. UNSTACK: (X2 (GON ON-TABLE)) (X1 (GON1 (CON (MIN GON)))) (X1 (GON1 CLEAR))
9. UNSTACK: (X1 (NOT (CON (MIN GON)))) (X2 (ON (GON1 ON-TABLE))) (X2 (GON (NOT ONTABLE))) (X1 (GON (GON ON-TABLE))) (X1 (GON (NOT CLEAR)))
10. UNSTACK: (X2 (NOT (CON CLEAR))) (X1 (GON1 (CON ON-TABLE)))
11. UNSTACK: (X1 (GON1 CLEAR)) (X1 (ON (ON (MIN GON)))
Ground Logistics
1. LOAD: (X2 (NOT (IN (GIN1 CITY)))) (X1 (NOT (CIN1 CITY))) (X1 (GIN1 CITY))
2. UNLOAD: (X1 (GIN1 X3 ))
3. DRIVE: (X1 (IN (GIN1 X3 )))
4. DRIVE: (X3 (NOT (GIN BLOCK))) (X3 (IN (GIN1 CITY))) (X1 CAR) (X2 CLEAR)
5. DRIVE: (X3 (IN (GIN1 RAIN))) (X1 TRUCK)
Colored Blocks World
1. PICK-UP-BLOCK-FROM: (X2 (NOT (CON-TOP-OF TABLE))) (X2 (GON-TOP-OF1 (ON-TOP-OF BLOCK)))
2. PUT-DOWN-BLOCK-ON: (X2 (CON-TOP-OF1 (CON-TOP-OF1 BLOCK))) (X2 (GON-TOP-OF HOLDING))
(X2 (CON-TOP-OF TABLE))
3. PICK-UP-BLOCK-FROM: (X2 (NOT (CON-TOP-OF BLOCK))) (X1 (ON-TOP-OF (GON-TOP-OF1 TABLE)))
(X2 (GON-TOP-OF (GON-TOP-OF1 BLOCK))) (X2 (NOT (CON-TOP-OF1 BLOCK))) (X2 (ON-TOPOF1 (GON-TOP-OF BLOCK))) (X1 (GON-TOP-OF (GON-TOP-OF1 BLOCK)))
4. PICK-UP-BLOCK-FROM: (X1 (NOT (CON-TOP-OF TABLE))) (X1 (GON-TOP-OF1 (CON-TOP-OF TABLE))) (X1 (GON-TOP-OF (ON-TOP-OF1 BLOCK)))
5. PUT-DOWN-BLOCK-ON: (X2 (CON-TOP-OF1 (ON-TOP-OF1 TABLE))) (X2 (GON-TOP-OF HOLDING)) (X2
(CON-TOP-OF TABLE))
6. PUT-DOWN-BLOCK-ON: (X2 (CON-TOP-OF (ON-TOP-OF BLOCK))) (X1 (GON-TOP-OF1 (GON-TOP-OF1
BLOCK)))
7. PUT-DOWN-BLOCK-ON: (X2 (GON-TOP-OF HOLDING)) (X2 (CON-TOP-OF TABLE))
8. PUT-DOWN-BLOCK-ON: (X2 TABLE)
9. PICK-UP-BLOCK-FROM: (X2 (NOT (CON-TOP-OF TABLE))) (X2 (GON-TOP-OF1 (CON-TOP-OF TABLE)))
10. PICK-UP-BLOCK-FROM: (X1 (GON-TOP-OF1 (CON-TOP-OF1 TABLE))) (X2 TABLE) (X1 (GON-TOP-OF
(GON-TOP-OF BLOCK))) (X1 (GON-TOP-OF (ON-TOP-OF1 TABLE)))
11. PICK-UP-BLOCK-FROM: (X2 (ON-TOP-OF (CON-TOP-OF BLOCK))) (X1 (GON-TOP-OF1 (CON-TOP-OF1
TABLE)))
12. PICK-UP-BLOCK-FROM: (X2 (ON-TOP-OF1 BLOCK)) (X2 (NOT (CON-TOP-OF TABLE))) (X2 (GONTOP-OF (ON-TOP-OF1 BLOCK))) (X2 (GON-TOP-OF (ON-TOP-OF1 BLOCK)))
13. PICK-UP-BLOCK-FROM: (X1 (GON-TOP-OF1 (GON-TOP-OF1 TABLE)))
Boxworld
1. DRIVE-TRUCK: (X2 (GBOX-AT-CITY (BOX-AT-CITY1 X3 ))) (X3 (NOT (CAN-FLY (TRUCK-AT-CITY (NOT
PREVIOUS))))) (X3 (CAN-DRIVE1 PREVIOUS)) (X2 (NOT (CAN-FLY (TRUCK-AT-CITY (NOT PREVIOUS))))) (X3 (NOT (CAN-FLY (BOX-AT-CITY BOX)))) (X2 (CAN-DRIVE (CAN-DRIVE (BOX-AT-CITY BOX))))
(X3 (NOT (CAN-FLY (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))))
2. UNLOAD-BOX-FROM-TRUCK-IN-CITY: (X1 (GBOX-AT-CITY1 (TRUCK-AT-CITY PREVIOUS))) (X3 (GBOXAT-CITY BOX)) (X3 (NOT (BOX-AT-CITY PREVIOUS))) (X1 (GBOX-AT-CITY1 (CAN-DRIVE1 (CANDRIVE1 (CAN-FLY CITY))))) (X2 (BOX-ON-TRUCK (GBOX-AT-CITY1 PREVIOUS)))
3. DRIVE-TRUCK: (X1 (BOX-ON-TRUCK (GBOX-AT-CITY1 X3 ))) (X2 (NOT (CAN-DRIVE (TRUCK-AT-CITY
(BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))))

114

fiAPI Policy Language Bias

4. DRIVE-TRUCK: (X3 (CAN-DRIVE (BOX-AT-CITY PREVIOUS))) (X2 (CAN-FLY (CAN-DRIVE1 (BOX-AT-CITY
BOX)))) (X3 (CAN-DRIVE (CAN-FLY (TRUCK-AT-CITY TRUCK)))) (X2 (NOT (CAN-DRIVE (TRUCK-AT-CITY
(BOX-ON-TRUCK (GBOX-AT-CITY1 CITY)))))) (X2 PREVIOUS) (X2 (CAN-DRIVE (CAN-DRIVE X3 ))) (X3
(NOT (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))) (X3 (NOT (CAN-FLY PREVIOUS)))
(X3 (CAN-DRIVE (NOT (BOX-AT-CITY BOX)))) (X2 (CAN-DRIVE (CAN-DRIVE1 X3 ))) (X3 (CAN-DRIVE
(NOT (TRUCK-AT-CITY TRUCK))))
5. LOAD-BOX-ON-TRUCK-IN-CITY: (X1 (GBOX-AT-CITY1 (CAN-DRIVE (TRUCK-AT-CITY TRUCK)))) (X3 (NOT
(PLANE-AT-CITY PREVIOUS))) (X3 (CAN-DRIVE (CAN-DRIVE1 (CAN-FLY CITY)))) (X3 (CAN-DRIVE1
(NOT (TRUCK-AT-CITY (NOT PREVIOUS)))))
6. UNLOAD-BOX-FROM-TRUCK-IN-CITY: (X3 (GBOX-AT-CITY (BOX-ON-TRUCK1 TRUCK))) (X3 (NOT (CANFLY (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY)))))) (X1 (GBOX-AT-CITY1 CITY))
7. DRIVE-TRUCK: (X1 (BOX-ON-TRUCK (GBOX-AT-CITY1 PREVIOUS))) (X3 (CAN-DRIVE (GBOX-AT-CITY
(GBOX-AT-CITY1 PREVIOUS)))) (X3 (NOT (PLANE-AT-CITY PLANE))) (X2 (NOT (CAN-FLY (GBOX-ATCITY (GBOX-AT-CITY1 PREVIOUS)))))
8. FLY-PLANE: (X1 (BOX-ON-PLANE (GBOX-AT-CITY1 X3 )))
9. UNLOAD-BOX-FROM-PLANE-IN-CITY: (X1 (GBOX-AT-CITY1 PREVIOUS))
10. FLY-PLANE: (X2 (NOT (CAN-DRIVE (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY)))))) (X2
(GBOX-AT-CITY BOX)) (X3 (NOT (PLANE-AT-CITY PREVIOUS))) (X1 (NOT PREVIOUS))
11. LOAD-BOX-ON-PLANE-IN-CITY: (X1 (GBOX-AT-CITY1 (CAN-FLY PREVIOUS))) (X3 (NOT (TRUCK-AT-CITY
(NOT PREVIOUS)))) (X3 (NOT (CAN-DRIVE (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))))
12. DRIVE-TRUCK: (X1 (BOX-ON-TRUCK (GBOX-AT-CITY1 X3 ))) (X2 (NOT (CAN-DRIVE (CAN-FLY PREVIOUS)))) (X2 (CAN-DRIVE1 (CAN-FLY CITY)))
13. LOAD-BOX-ON-TRUCK-IN-CITY: (X1 (GBOX-AT-CITY1 PREVIOUS))

References
Aler, R., Borrajo, D., & Isasi, P. (2002). Using genetic programming learn improve
control knowledge. Artificial Intelligence, 141 (1-2), 2956.
Ambite, J. L., Knoblock, C. A., & Minton, S. (2000). Learning plan rewriting rules.
Artificial Intelligence Planning Systems, pp. 312.
Bacchus, F. (2001). AIPS 00 planning competition. AI Magazine, 22(3)(3), 5762.
Bacchus, F., & Kabanza, F. (2000). Using temporal logics express search control knowledge planning. Artificial Intelligence, 16, 123191.
Bagnell, J., Kakade, S., Ng, A., & Schneider, J. (2003). Policy search dynamic programming. Proceedings 16th Conference Advances Neural Information
Processing.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Boutilier, C., & Dearden, R. (1996). Approximating value trees structured dynamic
programming. Saitta, L. (Ed.), International Conference Machine Learning.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
factored representations. Artificial Intelligence, 121 (1-2), 49107.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-order
MDPs. International Joint Conference Artificial Intelligence.
115

fiFern, Yoon, & Givan

Dean, T., & Givan, R. (1997). Model minimization markov decision processes. National
Conference Artificial Intelligence, pp. 106111.
Dean, T., Givan, R., & Leach, S. (1997). Model reduction techniques computing approximately optimal solutions Markov decision processes. Conference Uncertainty
Artificial Intelligence, pp. 124131.
Dietterich, T., & Wang, X. (2001). Batch value function approximation via support vectors.
Proceedings Conference Advances Neural Information Processing.
Dzeroski, S., DeRaedt, L., & Driessens, K. (2001). Relational reinforcement learning. Machine Learning, 43, 752.
Estlin, T. A., & Mooney, R. J. (1996). Multi-strategy learning search control partialorder planning. National Conference Artificial Intelligence.
Fern, A., Yoon, S., & Givan, R. (2003). Approximate policy iteration policy language bias. Proceedings 16th Conference Advances Neural Information
Processing.
Finkelstein, L., & Markovitch, S. (1998). selective macro-learning algorithm
application NxN sliding-tile puzzle. Journal Artificial Intelligence Research,
8, 223263.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions model minimization
Markov decision processes. Artificial Intelligence, 147 (1-2), 163223.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression inductive policy
selection. Conference Uncertainty Artificial Intelligence.
Guestrin, C., Koller, D., Gearhart, C., & Kanodia, N. (2003a). Generalizing plans new
environments relational mdps. International Joint Conference Artificial Intelligence.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003b). Efficient solution algorithms
factored mdps. Journal Artificial Intelligence Research, 19, 399468.
Hoffman, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. Journal
Artificial Intelligence Research, 22, 215278.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 263302.
Howard, R. (1960). Dynamic Programming Markov Decision Processes. MIT Press.
Huang, Y.-C., Selman, B., & Kautz, H. (2000). Learning declarative control rules
constraint-based planning. International Conference Machine Learning, pp.
415422.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (2002). sparse sampling algorithm nearoptimal planning large markov decision processes. Machine Learning, 49 (23),
193208.
Kersting, K., Van Otterlo, M., & DeRaedt, L. (2004). Bellman goes relational. Proceedings
Twenty-First International Conference Machine Learning.
116

fiAPI Policy Language Bias

Khardon, R. (1999a). Learning action strategies planning domains. Artificial Intelligence, 113 (1-2), 125148.
Khardon, R. (1999b). Learning take actions. Machine Learning, 35 (1), 5790.
Lagoudakis, M., & Parr, R. (2003). Reinforcement learning classification: Leveraging
modern classifiers. International Conference Machine Learning.
Langford, J., & Zadrozny, B. (2004). Reducing t-step reinforcement learning classification.
http://hunch.net/jl/projects/reductions/RL class/colt submission.ps.
Martin, M., & Geffner, H. (2000). Learning generalized policies planning domains using
concept languages. International Conference Principles Knowledge Representation Reasoning.
Mataric, M. (1994). Reward functions accelarated learning. Proceedings International Conference Machine Learning.
McAllester, D., & Givan, R. (1993). Taxonomic syntax first order inference. Journal
ACM, 40 (2), 246283.
McAllester, D. (1991). Observations cognitive judgements. National Conference
Artificial Intelligence.
McGovern, A., Moss, E., & Barto, A. (2002). Building basic block instruction scheduler
using reinforcement learning rollouts. Machine Learning, 49 (2/3), 141160.
Minton, S. (1988). Quantitative results concerning utility explanation-based learning.
National Conference Artificial Intelligence.
Minton, S. (Ed.). (1993). Machine Learning Methods Planning. Morgan Kaufmann.
Minton, S., Carbonell, J., Knoblock, C. A., Kuokka, D. R., Etzioni, O., & Gil, Y. (1989).
Explanation-based learning: problem solving perspective. Artificial Intelligence, 40,
63118.
Natarajan, B. K. (1989). learning exercises. Annual Workshop Computational
Learning Theory.
Reddy, C., & Tadepalli, P. (1997). Learning goal-decomposition rules using exercises.
International Conference Machine Learning, pp. 278286. Morgan Kaufmann.
Rivest, R. (1987). Learning decision lists. Machine Learning, 2 (3), 229246.
Tesauro, G. (1992). Practical issues temporal difference learning. Machine Learning, 8,
257277.
Tesauro, G., & Galperin, G. (1996). On-line policy improvement using monte-carlo search.
Conference Advances Neural Information Processing.
Tsitsiklis, J., & Van Roy, B. (1996). Feature-based methods large scale DP. Machine
Learning, 22, 5994.
Veloso, M., Carbonell, J., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating
planning learning: PRODIGY architecture. Journal Experimental
Theoretical AI, 7 (1).
Wu, G., Chong, E., & Givan, R. (2001). Congestion control via online sampling. Infocom.
117

fiFern, Yoon, & Givan

Yan, X., Diaconis, P., Rusmevichientong, P., & Van Roy, B. (2004). Solitaire: Man versus
machine. Conference Advances Neural Information Processing.
Yoon, S., Fern, A., & Givan, R. (2002). Inductive policy selection first-order MDPs.
Conference Uncertainty Artificial Intelligence.
Younes, H. (2003). Extending pddl model stochastic decision processes. Proceedings
International Conference Automated Planning Scheduling Workshop
PDDL.
Zimmerman, T., & Kambhampati, S. (2003). Learning-assisted automated planning: Looking back, taking stock, going forward. AI Magazine, 24(2)(2), 7396.

118

fiJournal Artificial Intelligence Research 25 (2006) 187-231

Submitted 03/05; published 02/06

Approach Temporal Planning Scheduling
Domains Predictable Exogenous Events
Alfonso Gerevini
Alessandro Saetti
Ivan Serina

gerevini@ing.unibs.it
saetti@ing.unibs.it
serina@ing.unibs.it

Dipartimento di Elettronica per lAutomazione
Universita degli Studi di Brescia
Via Branze 38, I-25123 Brescia, Italy

Abstract
treatment exogenous events planning practically important many realworld domains preconditions certain plan actions affected events.
paper focus planning temporal domains exogenous events happen
known times, imposing constraint certain actions plan must executed
predefined time windows. actions durations, handling temporal constraints adds extra difficulty planning. propose approach planning
domains integrates constraint-based temporal reasoning graph-based
planning framework using local search. techniques implemented planner
took part 4th International Planning Competition (IPC-4). statistical analysis
results IPC-4 demonstrates effectiveness approach terms
CPU-time plan quality. Additional experiments show good performance
temporal reasoning techniques integrated planner.

1. Introduction
many real-world planning domains, execution certain actions occur
predefined time windows one necessary conditions hold. instance,
car refueled gas station gas station open, space telescope
take picture certain planet region region observable. truth
conditions determined exogenous events happen known times,
cannot influenced actions available planning agent (e.g.,
closing gas station planet movement).
Several frameworks supporting action durations time windows proposed
(e.g., Vere, 1983; Muscettola, 1994; Laborie & Ghallab, 1995; Schwartz & Pollack, 2004;
Kavuluri & U, 2004; Sanchez, Tang, & Mali, 2004). However, domaindependent systems fast enough large-scale problems. paper, propose
new approach planning temporal features, integrating constraint-based
temporal reasoning graph-based planning framework.
last two versions domain definition language International planning competition (IPC) support action durations predictable (deterministic) exogenous
events (Fox & Long, 2003; Edelkamp & Hoffmann, 2004). PDDL2.1, predictable exogenous events implicitly represented (Fox, Long, & Halsey, 2004), PDDL2.2
explicitly represented timed initial literals, one two new PDDL
c
2006
AI Access Foundation. rights reserved.

fiGerevini, Saetti & Serina

features 2004 competition (IPC-4) focused. Timed initial literals specified
description initial state planning problem assertions form
(at L), real number, L ground literal whose predicate
appear effects domain action. obvious meaning (at L) L
true time t. set assertions involving ground predicate defines
sequence disjoint time windows timed predicate holds. example
well-known ZenoTravel domain (Penberthy, 1993; Long & Fox, 2003a)
(at
(at
(at
(at

8 (open-fuelstation city1))
12 (not (open-fuelstation city1)))
15 (open-fuelstation city1))
20 (not (open-fuelstation city1))).

assertions define two time windows (open-fuelstation city1) true,
i.e., 8 12 (excluded) 15 20 (excluded). timed initial literal relevant
planning process precondition domain action, call timed
precondition action. timed precondition action seen temporal
scheduling constraint action, defining feasible time window(s) action
executed. actions plan durations timed preconditions, computing
valid plan requires planning reasoning time integrated, order check
whether execution planned actions satisfy scheduling constraints.
action plan cannot scheduled, plan valid must revised.
main contributions work are: (i) new representation temporal plans
action durations timed preconditions, called Temporally-Disjunctive Action Graph,
(TDA-graph) integrating disjunctive constraint-based temporal reasoning recent
graph-based approach planning; (ii) polynomial method solving disjunctive temporal reasoning problems arise context; (iii) new local search techniques
guide planning process using representation; (iv) experimental analysis
evaluating performance methods implemented planner called lpg-td,
took part IPC-4 showing good performance many benchmark problems.
td extension name planner abbreviation timed initial literals
derived predicates, two main new features PDDL2.2.1 lpg-td, techniques
handling timed initial literals quite different techniques handling derived
predicates. first ones concern representing temporal plans predictable exogenous
events fast temporal reasoning action scheduling planning; second ones
concern incorporating rule-based inference system efficient reasoning derived
predicates planning. timed initial literals derived predicates require
change heuristics guiding search planner, radically different way.
paper, focus timed initial literals, significant useful
extension PDDL2.1. Moreover, analysis results IPC-4 shows lpg-td
top performer benchmark problems involving feature. treatment derived
predicates lpg-td presented another recent paper (Gerevini et al., 2005b).
1. Derived predicates allow us express concise natural way indirect action effects. Informally, predicates appear effect action, truth determined
domain rules specified part domain description.

188

fiAn Approach Temporal Planning Scheduling

paper organized follows. Section 2, necessary background,
introduce TDA-graph representation method solving disjunctive temporal
reasoning problems arise context. Section 3, describe new local
search heuristics planning space TDA-graphs. Section 4, present
experimental analysis illustrating efficiency approach. Section 5, discuss
related work. Finally, Section 6 give conclusions.

2. Temporally Disjunctive Action Graph
Like partial-order causal-link planning, (e.g., Penberthy & Weld, 1992; McAllester &
Rosenblitt, 1991; Nguyen & Kambhampati, 2001), framework search space
partial plans. search state partial temporal plan represent
Temporally-Disjunctive Action Graph (TDA-graph). TDA-graph extension
linear action graph representation (Gerevini, Saetti, & Serina, 2003) integrates disjunctive temporal constraints handling timed initial literals. linear action graph
variant well-known planning graph (Blum & Furst, 1997). section,
necessary background linear action graphs disjunctive temporal constraints,
introduce TDA-graphs, propose techniques temporal reasoning
context representation used next section.
2.1 Background: Linear Action Graph Disjunctive Temporal Constraints
linear action graph (LA-graph) planning problem directed acyclic leveled
graph alternating fact level, action level. Fact levels contain fact nodes,
labeled ground predicate . fact node f level l associated
no-op action node level l representing dummy action predicate f
precondition effect. action level contains one action node labeled
name domain action represents, no-op nodes corresponding
level.
action node labeled level l connected incoming edges fact nodes
level l representing preconditions (precondition nodes), outgoing edges
fact nodes level l + 1 representing effects (effect nodes). initial level
contains special action node astart , last level special action node aend .
effect nodes astart represent positive facts initial state , precondition
nodes aend goals .
pair action nodes (possibly no-op nodes) constrained persistent mutex
relation (Fox & Long, 2003), i.e., mutually exclusive relation holding every level
graph, imposing involved actions never occur parallel valid plan.
relations efficiently precomputed using algorithm proposed previous
work (Gerevini et al., 2003).
LA-graph also contains set ordering constraints actions (partial) plan represented graph. constraints (i) constraints imposed
search deal mutually exclusive actions: action level l mutex
action node b level l, constrained finish start b; (ii)
constraints actions implied causal structure plan: action
189

fiGerevini, Saetti & Serina

used achieve precondition action b, constrained finish start
b.
effects action node automatically propagated next levels
graph corresponding no-ops, interfering (mutex) action
blocking propagation, last level graph reached (Gerevini et al.,
2003). rest paper, assume LA-graph incorporates propagation.
Disjunctive Temporal Problem (DTP) (Stergiou & Koubarakis, 2000; Tsamardinos
& Pollack, 2003) pair hP, Ci, P set time point variables, C set
disjunctive constraints c1 cn , ci form yi xi ki , xi yi P, ki
real number (i = 1...n). C contains unary constraints, DTP called Simple
Temporal Problem (STP) (Dechter, Meiri, & Pearl, 1991).
DTP consistent DTP solution. solution DTP
assignment real values variables DTP consistent every constraint
DTP. Computing solution DTP NP-hard problem (Dechter et al., 1991),
computing solution STP accomplished polynomial time. Given
STP special start time variable preceding others, compute
solution STP variable shortest possible distance O(n c)
time, n variables c constraints STP (Dechter et al., 1991; Gerevini & Cristani,
1997). call solution optimal solution STP. Clearly, DTP consistent
choose constraint DTP disjunct obtaining consistent
STP, solution STP also solution original DTP.
Finally, STP consistent distance graph STP
contain negative cycles (Dechter et al., 1991). distance graph STP hP, Ci
directed labeled graph vertex labeled p p P, edge v P
w P labeled k constraint w v k C.
2.2 Augmenting LA-graph Disjunctive Temporal Constraints
Let p timed precondition set W (p) time windows. following, x x+
indicate start time end time x, respectively, x either time window
action. Moreover, al indicates action node level l LA-graph consideration.
clarity presentation, describe techniques focusing action preconditions
must hold whole execution action (except end point
action), operator effects hold end action execution, i.e., PDDL
conditions type all, PDDL effects type end (Fox & Long, 2003). 2
order represent plans actions durations time windows
execution, augment ordering constraints LA-graph (i) action duration
constraints (ii) action scheduling constraints. Duration constraints form
a+ = Dur(a),
Dur(a) denotes duration action (for special actions start aend ,
+

+
Dur(astart ) = Dur(aend ) = 0, since
start = astart aend = aend ). Duration
constraints supported representation proposed previous work (Gerevini
2. methods planner support types operator condition effect specified
PDDL 2.1 2.2.

190

fiAn Approach Temporal Planning Scheduling

Level 1
(0)
p1

Level 2

Level 3

Goal level

()

p1

p1

p1

p1
p

mutex

p2

(0)

(50)

p5

(50)

p5

(50)

p5

a3

a1
(0)

astart

(0)

p3

p3

p3

(0)

p7

p3
mutex

()

astart

p6

(90)

(70)

a1

aend

(70)

p8

(70)

p8

p

p10

[15]
(75)

[50]
(0)

p

(90)

a2
a3

(70)

p8

aend

a2
(0)

p4

(0)

p4

(0)

p4

[70]
(0)

(70)

p9

(70)

p9

(70)

p9

(70)

0

25

50

75

90

125

p9

Figure 1: example LA-graph nodes labeled -values (in round brackets),
Gantt chart actions labeling nodes LA-graph. Square
nodes action nodes; circle nodes fact nodes. Action nodes also marked
duration represented actions (in square brackets). Unsupported
precondition nodes labeled (). Dashed edges form chains no-ops blocked
mutex actions. Grey areas Gantt chart represent time windows
timed precondition p a3 .

et al., 2003), representation treatment scheduling constraints major
contribution work.
Let plan represented LA-graph A. easy see set C formed
ordering constraints duration constraints actions
encoded STP. instance, ai used support precondition node aj ,

a+
aj 0 C; ai aj two mutex actions , ai ordered aj ,

a+
aj 0 C. Moreover, every action , following STP-constraints
C:
a+ Dur(a), a+ Dur(a),
equivalent a+ = Dur(a). scheduling constraint imposes constraint
execution action must occur time windows associated timed
precondition action. Syntactically, disjunctive constraint c1 cn ,
ci form


(yi x
hi ) (vi ui ki ),

u
, vi , xi , yi action start times action end times, hi , ki R . every action
timed precondition p, following disjunctive constraint added C:

191

fiGerevini, Saetti & Serina

_

wW (p)



+
3


+
a+
a+
.
start w
start w

Definition 1 temporally disjunctive action graph (TDA-graph) 4-tuple hA, , P, Ci

linear action graph;
assignment real values nodes A;
P set time point variables corresponding start times end times
actions labeling action nodes A;
C set ordering constraints, duration constraints scheduling constraints
involving variables P.
TDA-graph hA, , P, Ci represents (partial) plan formed actions labeling
action nodes start times assigned . Figure 1 gives LA-graph
-values simple TDA-graph containing five action nodes (astart , a1 , a2 , a3 , aend )
several fact nodes representing ten facts. ordering constraints duration constraints
C are:4

+

a+
1 a3 0, a2 a3 0,

+
+

+
a1 a1 = 50, a2
2 = 70, a3 a3 = 15.

Assuming p timed precondition a3 windows [25, 50) [75, 125),
scheduling constraint C is:

+
+
+

+
+
((a+
start a3 25) (a3 astart 50)) ((astart a3 75) (a3 astart 125)).

pair hP, Ci defines DTP D.5 Let Ds set scheduling constraints D.
represents set STPs, consists constraints
Ds one disjunct (pair STP-constraints) disjunction subset s0
Ds (Ds0 Ds ). call consistent STP induced STP D. induced
STP contains disjunct every disjunction Ds (i.e., Ds0 = Ds ), say
(consistent) STP complete induced STP D.
values assigned action nodes action start times corresponding optimal solution induced STP. call start times schedule
actions A. value labeling fact node f earliest time = Ta + Dur(a)
3. Note that, p timed condition action a, end time
exogenous event making p false happens, PDDL p required true end
(Fox & Long, 2003).

+

4. brevity, examples omit constraints a+
start ai 0 ai aend 0, action
ai , well duration constraints astart aend , duration zero.
5. disjunctive constraints C exactly DTP-form. However, easy see every
disjunctive constraint C translated equivalent conjunction constraints exact DTPform. use compact notation clarity efficiency reasons.

192

fiAn Approach Temporal Planning Scheduling

supports f A, starts Ta . induced STP derive
schedule incomplete, may violate scheduling constraint action nodes,
say unscheduled current TDA-graph.
following definitions present notions optimality complete induced STP
optimal schedule, used next section.
Definition 2 Given DTP point variable p, complete induced STP
optimal induced STP p iff solution assigning p value less
equal value assigned p every solution every complete induced
STP D.
Definition 3 Given DTP TDA-graph G, optimal schedule actions
G optimal solution optimal induced STP
end .
Note optimal solution minimizes makespan represented (possibly
partial) plan. DTP previous example (Figure 1) two induced STPs: one
time window p (S1 ), one including pair STP-constraints imposing
time window [75, 125) p (S2 ). STP obtained imposing time window [25, 50)
p induced STP DTP, consistent. S1 partial induced
STP D, S2 complete optimal start time aend . temporal values
derived optimal solution S2 assigned action nodes
+



+

TDA-graph are:
start = astart = 0, a1 = 0, a2 = 0, a3 = 75, aend = aend = 90.
2.3 Solving DTP TDA-graph
general, computing complete induced STP DTP (if exists) NP-hard problem
solved backtracking algorithm (Stergiou & Koubarakis, 2000; Tsamardinos
& Pollack, 2003). However, given particular structure temporal constraints
forming TDA-graph, show task accomplished polynomial time
backtrack-free algorithm. Moreover, algorithm computes optimal induced STP

end .
following, assume time window timed precondition shorter
duration action (otherwise, time window removed
available precondition and, time window remains, action cannot
used valid plan). Moreover, without loss generality, assume
action one timed precondition. easy see always replace
set timed conditions action single equivalent timed precondition,
whose time windows obtained intersecting windows forming different original
timed conditions a. Also set start timed conditions set end timed
conditions compiled single equivalent timed preconditions. achieved
translating conditions conditions type all. idea similar
one presented Edelkamp (2004), difference one
time window associated timed condition, Edelkamp assumes timed
condition associated unique time window. Specifically, every start timed
condition p action translated equivalent timed condition p 0 type
replacing scheduling constraint p,
193

fiGerevini, Saetti & Serina

p

p

Dur(a)

Dur(a)

q

Dur(a)

r

r

x
0

35 40

50

60

80

100

120

150

180

Figure 2: example set timed conditions compiled single timed precondition (x). solid boxes represent time windows associated timed
conditions p (of type start), q (of type end), r (of type all)
action a. solid box extended dashed box indicates extension
time window translation corresponding timed condition
timed condition a.

_

wW (p)







+
a+
a+
,
start < w
start < w

forcing occur one time windows,
_

wW (p)



6
+
+


a+
a+
start < w + Dur(a) .
start < w

Similarly, every end timed condition p translated equivalent
timed condition replacing scheduling constraint
_

wW (p)



+

+

+
a+
a+
,
start < w
start < w

forcing a+ occur one time windows,
_

wW (p)



+

+


+
a+
.
start < w + Dur(a) astart < w

Clearly, translation timed conditions domain action single timed
precondition action accomplished preprocessing step polynomial time.
Figure 2 shows example. Assume action duration 20 timed conditions
p type start, q type end r type all. Let [0, 50) [100, 150)
time windows p, [35, 80) time window q, finally [40, 60) [120, 180)
time windows r. compile timed conditions new timed condition x
time window [40, 60).
6. Note timed conditions type start end need use < instead . However,
properties algorithms STPs easily generalized STPs extended <-constraints
(e.g., Gerevini & Cristani, 1997).

194

fiAn Approach Temporal Planning Scheduling

Solve-DTP(X, S)
Input: set X meta-variables meta CSP DTP, partial solution meta CSP;
Output: Either solution meta CSP fail.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

X = stop return S;
x SelectVariable(X); X 0 X {x};
D(x) 6=
SelectValue(D(x));
0 {x d}; D(x) D(x) {d};
D0 (x) D(x); /* Saving domain values */
ForwardCheck-DTP(X 0 , 0 )
Solve-DTP(X 0 , 0 );
D(x) 0 (x); /* Restoring domain values */
return fail; /* backtracking */

ForwardCheck-DTP(X, S)
Input: set X meta-variables, (partial) solution S;
Output: Either true false.
1.
2.
3.
4.
5.
6.

forall x X
forall D(x)
Consistency-STP(S {x d})
D(x) D(x) {d};
D(x) = return false; /* dead-end */
return true.

Figure 3: Basic algorithm solving DTP. D(x) global variable whose value
current domain meta-variable x. Consistency-STP(S) returns true,
STP formed variable values (partial) solution solution, false
otherwise.

observed Stergiou Kourbarakis (2000) Tsamardinos Pollack (2003),
DTP seen meta CSP: variables meta CSP constraints
original CSP, values (meta) variables disjuncts forming
constraints original CSP. constraints meta CSP explicitly
stated. Instead, implicitly defined follows: assignment values
meta-variables satisfies constraints meta CSP iff forms consistent STP (an
induced STP DTP). solution meta CSP complete induced STP
DTP.
Figure 3 shows algorithm solving meta CSP DTP (Tsamardinos &
Pollack, 2003), variant forward-checking backtracking algorithm solving
general CSPs. appropriately choosing next meta-variable instantiate (function
SelectVariable) value (function SelectValue), show algorithm finds
solution backtracking (if one exists). Moreover, simple modification Solve195

fiGerevini, Saetti & Serina

DTP, derive algorithm backtrack free even input meta CSP
solution. achieved exploiting information LA-graph
TDA-graph decompose DTP sequence growing DTPs
D1 D2 ... Dlast =
(i) last number levels A, (ii) variables Vi Di (i = 1..last)
variables corresponding action nodes level i, (iii)
constraints Di constraints involving variables Vi . E.g.,

+

+

+
DTP Figure 1, point variables D3 a+
start , a1 , a1 , a2 , a2 , a3 , a3 , set
constraints D3

+

+

+

+

{ a+
1 a3 0, a2 a3 0, a1 a1 = 50, a2 a2 = 70, a3 a3 = 15,

+
+
+

+
+
((a+
start a3 25) (a3 astart 50)) ((astart a3 75) (a3 astart 125))}.

decomposed DTP, derive ordered partition set metavariables meta CSP original DTP
X = X1 X2 ... Xlast ,
Xi set meta-variables corresponding constraints Di Di1 ,
> 1, D1 otherwise. ordered partition used define order
SelectVariable chooses next variable instantiate, crucial avoid backtracking. Specifically, every variable single domain value (i.e., ordering constraint,
duration constraint, scheduling constraint one time window) selected
every variable one possible value (i.e., scheduling constraint
one time window); moreover, xi Xi , xj Xj < j, xi selected
xj .
order avoid backtracking, order SelectValue chooses value
meta-variable important well: given meta-variable one value
(time window) current domain, choose value corresponding earliest
available time window. E.g., current domain selected meta-variable
possible values
[

i=1..m

+


+
+

a+
,
start wi astart wi

SelectValue chooses j-th value |wj | < |wh |, every h {1, ..., m},
j {1, ..., m}, h 6= j.
following give simple example illustrating order SelectVariable
SelectValue select meta-variables meta-values, respectively. Consider
TDA-graph Figure 1 additional time window [150, 200) timed precondition p a3 . DTP extended TDA-graph six meta-variables (x1 , x2 , . . . , x6 ),
whose domains (the disjuncts corresponding constraints original CSP) are:

x1 : {a+
1 a3 0}
+

x2 : {a2 a3 0}

196

fiAn Approach Temporal Planning Scheduling

x3 :
x4 :
x5 :
x6 :


{a+
1 a1 = 50}
+

{a2 a2 = 70}

{a+
3 a3 = 15}
+
+
+
+

+
+
{(astart
3 25) (a3 astart 50), (astart a3 75) (a3 astart 125),
+

+
+
(astart a3 150) (a3 astart 200)}.

exploiting level structure TDA-graph, derive ordered partition
meta-variables formed following sets:
X1 = {x3 }, X2 = {x4 }, X3 = {x1 , x2 , x5 , x6 }.
Since x3 belongs X1 x4 belongs X2 , SelectVariable selects x3 selecting
x4 . Similarly, function selects x4 meta-variables X3 . algorithm
instantiates x6 , first meta-value x6 (i.e., first time window timed precondition a3 ) removed domain forward checking, SelectValue selects

+
+
+

+
+
(a+
start a3 75) (a3 astart 125) (astart a3 150) (a3 astart 200),
first meta-value corresponds time window starting time 75,
second one corresponds time window starting time 150.
using techniques selecting next meta-variable instantiate
value, prove following theorem.
Theorem 1 Given DTP TDA-graph, meta CSP X solvable,
Solve-DTP finds solution X backtracking. Moreover, solution optimal
induced STP
end .
Proof. proof two key points: way meta-variables selected instantiated
SelectVariable SelectValue, respectively; particular type constraints D,
disjunctive constraints specific form encoding set disjoint time windows,
and, construction D,

j < j |=
j < ai ,

(1)


set ordering constraints duration constraints D,
(aj )
endpoint ai (aj ). property (1), cannot imply restriction
maximum distance endpoint ai endpoint aj (while, course,
lower bound distance). I.e., positive quantity u

j < j |= (a
j ai u).

(2)

Let assume SelectVariable chooses meta-variable x cannot consistently
instantiated value D(x) (and means reached backtracking point).
show cannot case.
SelectVariable chooses meta-variables STP-constraints metavariable scheduling constraint one value (time window). Let X
set meta-variables associated scheduling constraints D.
x must meta-variable X , assuming meta CSP X
solvable. use forward checking subroutine guarantees least one value x
consistent respect meta-variables instantiated current partial
197

fiGerevini, Saetti & Serina

solution S. Hence, case step 7 Solve-DTP ForwardCheck-DTP
returns false every value (time window) D(x), i.e., every D(x)
exists another uninstantiated meta-variable x0 X that, every d0 D(x0 ),
check Consistency-STP(S {x0 d0 }) executed forward checking subroutine returns
false. However, X solution (D consistent), cannot case
(i) value chosen SelectValue instantiate x previously instantiated metavariables (step 4) earliest available time window current domain
meta-variable consideration, least commitment assignment,
(ii) one scheduling constraint (meta-variable X ) level
TDA-graph.
Let a0 action constrained scheduling constraint associated x 0 . Since
SelectVariable selects x x0 , (ii) a0 level following level
action constrained scheduling constraint associated x. Thus, property (2),
x0 could instantiated, would every time window
a0 constrains a0 start early: current partial solution X augmented
possible values x implies start time a0 end
last time window a0 . then, (i) assumption X solvable guarantee
cannot case.
Moreover, since value every instantiated meta-variable propagated forward
checking unassigned variables, first value assigned metavariable value assigned variable solution found CSP (if
any) easy see first value chosen SelectValue(D(x)) feasible
(ForwardCheck-DTP(X 0 , 0 ) returns false), every next value chosen x
feasible.
Finally, since value chosen SelectValue meta-variable corresponds
earliest available window current domain meta-variable, follows
solution computed algorithm complete optimal induced STP
end . 2
consequence previous theorem, Solve-DTP performs backtracking (step 10),
input meta CSP solution. Thus, obtain general backtrack-free
algorithm DTP TDA-graph simply replacing step 10
10. stop return fail.
correctness modified algorithm, called Solve-DTP+ , follows
Theorem 1. next theorem states runtime complexity Solve-DTP + polynomial.
Theorem 2 Given TDA-graph G DTP D, Solve-DTP+ processes meta CSP
corresponding polynomial time respect number action nodes G
maximum number time windows scheduling constraint D. 7
7. noted main goal give complexity bound polynomial. use
improved forward checking techniques (e.g., Tsamardinos & Pollack, 2003) could lead complexity
bound lower one given proof theorem.

198

fiAn Approach Temporal Planning Scheduling

Proof. time complexity depends number times ForwardCheck-DTP executed, time complexity. contains linear number variables respect
number n domain action nodes LA-graph TDA-graph, O(n 2 ) ordering constraints, O(n) duration constraints scheduling constraints. Hence,
meta CSP O(n2 ) meta-variables (one variable constraint original
CSP). Let maximum number time windows scheduling constraint D.
ForwardCheck-DTP executed times meta-variable x, i.e., O( n 2 ) times
total. Consistency-STP decides satisfiability STP involving O(n) variables,
accomplished O(n3 ) time (Dechter et al., 1991; Gerevini & Cristani, 1997). (Note
variables STP processed Consistency-STP variables
original CSP, i.e., starting time end time actions plan.)
Finally, Consistency-STP run O( n2 ) times run ForwardCheck-DTP.
follows runtime complexity Solve-DTP+ O( 2 n7 ). 2
exploiting structure temporal constraints forming DTP TDAgraph, make following additional changes Solve-DTP+ improving efficiency
algorithm.
Instead starting empty assignment (no meta-variable instantiated),
initially every meta-variable associated ordering constraint duration
constraint instantiated value, X contains meta-variables associated
scheduling constraints. observed proof Theorem 1, meta
CSP solvable, values assigned meta-variables initial form
consistent STP.
Forward checking performed meta-variable.
proof Theorem 1 shown that, meta CSP solvable,
first value chosen SelectValue feasible (i.e., ForwardCheck-DTP returns
true). Thus, first value feasible, stop algorithm return fail
meta CSP solvable. Moreover, omit steps 6 9
save restore domain values meta-variables.
Finally, improved algorithm made incremental exploiting particular
way update DTP TDA-graph planning (i.e.,
search solution TDA-graph described next section). described
next section, search step either addition new action node certain
level l, removal action node l. cases, suffices recompute
sub-solution meta-variables subsets Xl , Xl+1 , ..., Xlast . values
assigned meta-variables assignment last solution
computed updating DTP, part input algorithm.
Moreover, order use local search techniques described next section,
need another change basic algorithm: algorithm detects X
solution, instead returning failure, (i) keeps processing remaining meta-variables,
(ii) terminates, returns (partial) induced STP Si formed values
assigned meta-variables. optimal solution Si defines -assignment
TDA-graph.
199

fiGerevini, Saetti & Serina

next section, SG denotes induced STP DTP TDA-graph G computed method.

3. Local Search Techniques TDA-Graphs
TDA-graph hA, , P, Ci contain two types flaw: unsupported precondition nodes
A, called propositional flaws, action nodes scheduled , called
temporal flaws. level contains flaw, say level flawed. example,
time window p TDA-graph Figure 1 [25, 50), level 3 would
flawed, start time a3 would 70, violates scheduling constraint
a3 imposing action must executed [25, 50).
TDA-graph flawed level represents valid plan called solution graph.
section, present new heuristics finding solution graph search space
TDA-graphs. heuristics used guide local search procedure, called Walkplan,
originally proposed Gerevini Serina (1999) heart
search engine planner.
initial TDA-graph contains astart aend . search step identifies
neighborhood N (G) (successor states) current TDA-graph G (search state),
set TDA-graphs obtained G adding helpful action node removing
harmful action node attempt repair earliest flawed level G. 8
following, sake brevity refer action node TDA-graph,
implicitly referring action node LA-graph TDA-graph. Similarly
level TDA-graph. Moreover, remind reader l denotes action
level l, la denotes level action a.
Definition 4 Given flawed level l TDA-graph G, action node helpful l iff
insertion G level l would remove propositional flaw l.
Definition 5 Given flawed level l TDA-graph G, action node level l
harmful l iff removal G would remove propositional flaw l, would
decrease -value al , al unscheduled.
Examples helpful action node harmful action node
action node representing action effect p1 helpful level 3 TDA-graph
Figure 1 added level 2 3 (bear mind insertion action node
level 3 determines expansion TDA-graph postponing a3 level 4; details
given end examples). Action node a3 Figure 1 harmful level 3,
precondition node p1 unsupported; action node a1 harmful level 3,
blocks no-op propagation p1 level 1, would support precondition node p1
level 3. Moreover, assuming W (p) = {[25, 50)}, a3 unscheduled plan represented
LA-graph. Action node a2 harmful level 3, removal a2
8. designed several flaw selection strategies described experimentally evaluated
recent paper (Gerevini, Saetti, & Serina, 2004). strategy preferring flaws earliest level
graph tends perform better others, used default strategy planner.
details discussion strategy given aforementioned paper.

200

fiAn Approach Temporal Planning Scheduling

would decrease temporal value a3 . contrary, a1 harmful level 3,
removal would affect possible scheduling a3 . Notice action
node helpful harmful: a3 harmful level 3, helpful
goal level (because supports precondition node p10 aend ).
add action node level l empty, LA-graph extended
one level, action nodes l shifted forward one level (i.e., moved
next level), new action inserted level l . Similarly, remove
action node level l, graph shrunk removing level l. additional
details process given another paper (Gerevini et al., 2003). Moreover,
pointed previous section, addition (removal) action node requires us
update DTP G adding (removing) appropriate ordering constraints
actions LA-graph G, duration constraint a, scheduling
constraint (if any). updated DTP, use method described
previous section revise , compute possibly new schedule actions G
(i.e., optimal solution SG ).
elements N (G) evaluated using heuristic evaluation function E consisting
two weighted terms, estimating additional search cost temporal cost, i.e.,
number search steps required repair new flaws introduced, contribution
makespan represented plan, respectively. element N (G) lowest
combined cost selected using noise parameter randomizing search escape
local minima (Gerevini et al., 2003). addition, order escape local minima,
new version planner uses short tabu list (Glover & Laguna, 1997). rest
section, focus search cost term E. techniques use
evaluation temporal cost (automatic) setting term weights E
similar introduced previous work (Gerevini et al., 2003).
search cost adding helpful action node repair flawed level l G
estimated constructing relaxed temporal plan achieving
(1) unsupported precondition nodes a, denoted Pre(a)
(2) propositional flaws remaining l adding a, denoted Unsup(l),
(3) supported precondition nodes action nodes G would become
unsupported adding a, denoted Threats(a).
Moreover, estimate number additional temporal flaws addition
G would determine, i.e., count number
(I) action nodes G would become unscheduled adding G,
(II) unsatisfied timed preconditions a, unscheduled TDA-graph extended ,
(III) action nodes scheduling constraint estimate cannot satisfied
context G.
search cost adding G number actions plus (I), (II) (III),
new terms heuristic evaluation. Note action nodes (I)
201

fiGerevini, Saetti & Serina

aend (90)

Goal level
(90)

p9

p8
[15]

a3 (75)

(70)

p8

Action
b1
b2
b3
b4

(70)

(70)

p10
p

(70)

p9

(70)

p9

p5

(70)

(70)

(70)

(50)

p1

p7

Est lower bound
0
0
15
50

(35)
[5]

q

p9

p8

p6

Relaxed Plan

Level 3
()

Action
b1
b2
b3
b4

N um acts
0
0
1
5

anew (30)

(20)

p5 (50)

p1

p3

Level 2
p1
p1

p5
mutex

p1

(0)

[50]

a1 (0)

Level 1
(0)

(50)

(0)

p2

mutex

p6

p3
p3
p3

(0)

(0)

a2 [70]
(0)
()

p4
p4
p4

(30)

q1

q2
[10]
(20)

(0)

b3

(20)

q3

(0)
[15]

p5

(0)

b1 (0)


astart (0)

(0)

p3

[100]

q
p4

b4 (50)

(0)

q4

p5

(50)

[20]

b2 (0)

p4

(0)

(50)

p5

Figure 4: example relaxed temporal plan . Square nodes represent action nodes,
nodes represent fact nodes; solid nodes correspond nodes
{anew }; dotted nodes correspond precondition nodes action nodes
considered construction ; gray dotted nodes
selected inclusion . Action nodes marked duration
represented actions (in square brackets) estimated start time (in
round brackets). meaning Num acts described text; lower
bounds earliest action start times (Est lower bound) computed
algorithm Appendix A.

would ordered (because used achieve one
preconditions, action nodes mutex a) that, given estimated end
time duration a, would excessively increase start time. (II)
consider original formulation timed preconditions (i.e., formulation
possible compilation one merged new precondition, discussed Section 2.3).
Finally, check scheduling constraint action , consider estimated end
time relaxed subplan used achieve preconditions action.
Example relaxed temporal plan additional temporal flaws (IIII)
Figure 4 gives example evaluating addition anew level 2 LAgraph left side figure (the graph one used Figure 1),
202

fiAn Approach Temporal Planning Scheduling

RelaxedTimePlan(G, I, A)
Input: set goal facts (G), initial state relaxed plan (I), set reusable actions (A);
Output: set actions Acts forming relaxed plan G earliest time
facts G achieved.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.


Acts A; F aActs Add (a);
AX {T (g) | g G F g G I};
G G I;
G F 6=
g fact G F ;
b BestAction(g);
hA, t0 RelaxedTimePlan(Pre(b), I, Acts);
(b) ComputeEFT (b, t0 );
AX{t, (b)};
forall f Add(b)

(f ) (f ),ST (b) + Dur(b) ;
Acts {b}; F aActs Add (a);
return hActs, ti.

Figure 5: Algorithm computing relaxed temporal plan. ComputeEFT (b, 0 ) returns
estimated earliest finishing time b consistent scheduling
constraint b (if any), t0 + Dur(b) (for example see
Appendix A). Add (a) denotes set positive effects a.

helpful action node unsupported precondition p6 . goals unsupported
preconditions q1 q2 anew ; initial state formed fact nodes
supported level 2. actions anew , b2 b3. numbers name
actions facts relaxed plan indicate order RelaxedTimePlan
considers them. estimated start time end time b3 20 30, respectively.
Assume timed precondition q anew associated time window [0, 20).
Concerning point (I), action node G would become unscheduled adding
anew G. Concerning point (II), anew unscheduled one timed precondition
unsatisfied (q). Concerning point (III), b3 cannot scheduled
context current TDA-graph G. Finally, since contains three actions,
sum (I), (II) (III) 2, search cost adding anew G level 2
5.
evaluation TDA-graph derived removing harmful action node
flawed level l similar, achieving
precondition nodes supported would become unsupported removing

la < l, unsupported precondition nodes level l become supported removing a.
203

fiGerevini, Saetti & Serina

Regarding second point, note l = la , flaws l eliminated because,
remove action, also (automatically) remove precondition nodes. While,
la < l, removal could leave flaws level l.
Plan relaxed sense derivation ignores possible (negative) interference actions , actions may unscheduled. derivation
takes account actions already current partial plan (the plan represented
TDA-graph G). particular, actions current plan used define
initial state , obtained applying actions G level la 1, ordered
according corresponding levels. Moreover, fact f marked temporal
value, (f ), corresponding time f becomes true (and remains )
current subplan formed actions level la 1.
relaxed plan constructed using backward process, called RelaxedTimePlan (see
Figure 5), extension RelaxedPlan algorithm proposed previous
work (Gerevini et al., 2003). algorithm outputs two values: set actions forming
(sub)relaxed plan, estimated earliest finishing time (used defined temporal
cost term E). set actions Acts forming derived running RelaxedTimePlan
twice: first goals Pre(a), initial state empty set reusable actions;
goals Unsup(l ) hreats(a), initial state Threats(a) Add (a), set reusable
actions formed actions computed first run plus a.
main novelty extended algorithm computing concerns choice
actions forming relaxed plan. action b chosen achieve (sub)goal g action
minimizing sum
estimated minimum number additional actions required support propositional preconditions (Num acts(b, I)),
number supported precondition nodes LA-graph would become
unsupported adding b G (Threats(b)),
number timed preconditions b estimate would unsatisfied G
extended (TimedPre(b));
number action nodes scheduled estimate would become unscheduled adding b G (TimeThreats(b)).
formally, action chosen BestAction(g) step 6 RelaxedTimePlan
achieve (sub)goal g action satisfying


0
0
0
0
ARGMIN Num acts(a , I) + |Threats(a )| + |TimedPre(a )| + |TimeThreats(a )| ,
{a0 Ag }

Ag = {a0 | g Add (a0 ), set domain actions whose preconditions
reachable I}.
Num acts(b, I) computed algorithm given Appendix A; Threats(b) computed previous method deriving (Gerevini et al., 2003), i.e., considering
negative interactions (through mutex relations) b precondition nodes
supported levels al ; TimedPre(b) TimeThreats(b) new components
action selection method, computed follows.
204

fiAn Approach Temporal Planning Scheduling

order compute TimedPre(b), estimate earliest start time b (Est(b))
earliest finishing time b (Ef t(b)). Using values, count number
timed preconditions b cannot satisfied. Ef t(b) defined Est(b) + Dur(b),
Est(b) maximum
lower bound possible earliest start time b (Est lower bound b), computed
reachability analysis algorithm given Appendix A;
-values action nodes ci current TDA-graph G, < la ,

mutex b addition b G would occur addition c+
b 0
DTP G;
maximum estimated lower bound time preconditions
b achieved relaxed plan; estimate computed causal structure
relaxed plan, duration scheduling constraints actions,
-values facts initial state I.
Example TimedPre
example Figure 4, estimated start time b3 maximum 15,
Est lower bound b3, 20, maximum time estimated
times preconditions b3 supported (p4 supported initial state
time 0, q3 supported time 20). Notice a1 mutex b3,
second point definition Est(b3) apply here. Since estimated earliest
start time b3 20 duration b3 10, Ef t(b3) = 20 + 10. Thus, assume
q associated time window [0,20), timed precondition q b3
cannot scheduled, i.e., q TimedPre(b3).
order compute TimeThreats(b), use following notion time slack
action nodes.
Definition 6 Given two action nodes ai aj TDA-graph hA, , P, Ci

C |= a+
< aj , Slack(ai , aj ) maximum time -value ai
consistently increased SG without violating time window chosen scheduling aj .
order estimate whether action b time threat action node k
current TDA-graph extended action node adding repairing level
l (l < k), check
(b , a) > Slack(a, ak )
holds, b portion relaxed plan computed far, (b , a)
estimated delay adding actions b G would cause start time a.
Examples time slack TimeThreat
slack anew a3 TDA-graph Figure 4 extended anew 35,
even anew started 35, a3 could still executed time window
[75, 125) (imposed timed precondition p); anew started 35 + , a3
would finish 125+ (determined summing start time anew , Dur(anew ), Dur(a2 ),
205

fiGerevini, Saetti & Serina

Dur(a3 )), scheduling constraint a3 would violated. Assume
evaluating inclusion b4 relaxed plan Figure 4 achieving q2.
(b4 , anew ) = 150,
i.e. estimated delay portion plan formed b4 would add end
time anew 150. Since slack anew a3 35,
Slack(anew , a3 ) < (b4 , anew ),
a3 TimeThreats(b4). contrary, since
Slack(anew , a3 ) > (b3 , anew ) = 30
a3 6 TimeThreats(b3).
conclude section, observe way consider scheduling constraints
evaluation search neighborhood similarity well-known
technique used scheduling. example, suppose evaluating TDA-graphs
obtained adding helpful action node one among alternative possible levels
graph, current TDA-graph contains another action node c mutex
a. search neighborhood contains two TDA-graphs corresponding (1) adding
level lc (2) adding level lc , (1) violates less scheduling
constraints (2), then, according points (I)(III), (1) preferred (2). similar
heuristic method, called constraint-based analysis, proposed Erschler, Roubellat
Vernhes (1976) decide whether action scheduled another
conflicting action, also used scheduling work guiding search
toward consistent scheduling tasks involved problem (e.g., Smith & Cheng,
1993).

4. Experimental Results
implemented approach planner called lpg-td, obtained 2nd prize
metric-temporal track (satisficing planners) 4th International Planning Competition (IPC-4). lpg-td incremental planner, sense produces sequence
valid plans improves quality previous ones. Plan quality
measured using metric expression specified planning problem description.
incremental process lpg-td described another paper (Gerevini et al., 2003).
Essentially, process iterates search solution graph additional constraint
lower bound plan quality, determined quality previously
generated plans. lpg-td written C available http://lpg.ing.unibs.it.
section, present results experimental study two main goals:
testing efficiency approach temporal planning predictable exogenous
events comparing performance lpg-td recent planners
IPC-4 attempted benchmark problems involving timed initial literals (Edelkamp,
Hoffmann, Littman, & Younes, 2004);
206

fiAn Approach Temporal Planning Scheduling

Planner

lpg-td
sgplan
p-mep
crikey
lpg-ipc3
downward (diag)
downward
marvin
yahsp
macro-ff
fap
roadmapper
tilsapa
optop

Solved
845
1090
98
364
306
380
360
224
255
189
81
52
63
4

Attempted
1074
1415
588
594
594
432
432
432
279
332
193
186
166
50

Success ratio
79%
77%
17%
61%
52%
88%
83%
52%
91%
57%
42%
28%
38%
8%

Planning capabilities IPC-4
Propositional + DP, Metric-Temporal +TIL
Propositional + DP, Metric-Temporal +TIL
Propositional, Metric-Temporal +TIL
Propositional, Metric-Temporal
Propositional, Metric-Temporal
Propositional + DP
Propositional + DP
Propositional + DP
Propositional
Propositional
Propositional
Propositional
TIL
TIL

Table 1: Number problems attempted/solved success ratio (satisficing) planners took part IPC-4. DP means derived predicates; TIL means timed
initial literals; Propositional means STRIPS ADL. planning capabilities PDDL2.2 features test problems attempted planner
IPC-4.

testing effectiveness proposed temporal reasoning techniques integrated
planning process understand, particular, impact overall
performance system, compare existing techniques.
first analysis, consider test problems variant IPC-4 metrictemporal domains involving timed initial literals. comparison lpg-td IPC-4
planners considering variants IPC-4 metric-temporal domains given
Appendix B. Additional results available web site planner.
second experiments, use new domains problems obtained extending
two well-known benchmark domains (and relative problems) IPC-3 timed
initial literals (Long & Fox, 2003a).9
tests conducted Intel Xeon(tm) 3 GHz, 1 Gbytes RAM. ran lpg-td
default settings every problem attempted.
4.1 LPG-td IPC-4 Planners
section, use official results IPC-4 compare performance lpg-td
planners took part competition. performance lpg-td
corresponds single run. CPU-time limit run 30 minutes,
termination forced. lpg-td.s indicates CPU-time required planner derive
first plan; lpg-td.bq indicates best quality plan found within CPU-time limit.
9. description IPC-4 domains relative variants, reader visit
official web site IPC-4 (http://ls5-www.cs.uni-dortmund.de/edelkamp/ipc-4/index.html).
extended versions IPC-3 domains used experiments available
http://zeus.ing.unibs.it/lpg/TestsIPC3-TIL.tgz.

207

fiGerevini, Saetti & Serina

focusing analysis IPC-4 domains involving timed initial literals,
Table 1 give brief overview results IPC-4 (satisficing) planners,
terms planning capabilities problems attempted/solved planner. table
summarizes results domain variants IPC-4. lpg-td sgplan (Chen, Hsu,
& Wah B., 2004) planners supporting major features PDDL2.1
PDDL2.2. planners good success ratio (close 80%). downward (Helmert,
2004) yahsp (Vidal, 2004) success ratio better lpg-td sgplan,
handle propositional domains (downward supports derived predicates,
yahsp not). sgplan attempted problems lpg-td also tested
compiled version variants derived predicates timed initial literals. 10
Moreover, lpg-td attempt numerical variant two versions Promela
domain ADL variant PSR-large, use equality numerical
preconditions conditional effects, currently planner support.
Figure 6 shows performance lpg-td variants three domains involving
predictable exogenous events respect (satisficing) planners IPC-4 supporting timed initial literals: sgplan, p-mep (Sanchez et al., 2004) tilsapa (Kavuluri
& U, 2004). Airport (upper plots figure), lpg-td solves 45 problems 50,
sgplan 43, p-mep 12, tilsapa 7. terms CPU-time, lpg-td performs much better
p-mep tilsapa. lpg-td faster sgplan nearly problems (except
problems 1 43). particular, gap performance problems 2131 nearly
one order magnitude. Regarding plan quality, performance lpg-td similar
performance p-mep tilsapa, while, overall, sgplan finds plan worse quality
(with exception problems 41 43, sgplan performs slightly better,
easiest problems lpg-td sgplan perform similarly).
lpg-td tilsapa planners IPC-4 attempted variant
PipesWorld timed initial literals (central plots Figure 6). lpg-td solves 23 problems 30, tilsapa solves 3 problems. domain variant lpg-td performs
much better tilsapa.
flaw version Umts (bottom plots Figure 6), lpg-td solves 50 problems,
sgplan solves 27 problems (p-mep tilsapa attempt domain variant).
Moreover, lpg-td one order magnitude faster sgplan every problem
solved. Compared IPC-4 benchmark problems, Umts problems generally
easier solve. test problems, main challenge finding plans good quality.
Overall, best quality plans lpg-td much better sgplan plans, except
simplest problems two planners generate plans similar quality. basic
version Umts without flawed actions, sgplan solves problems lpg-td, terms
plan quality lpg-td performs much better.
Figure 7 shows results Wilcoxon sign-rank test, also known Wilcoxon
matched pairs test (Wilcoxon & Wilcox, 1964), comparing performance lpg-td
planners attempted benchmark problems IPC-4 involving timed initial literals. test used Long Fox (2003a) comparing performance
10. versions generated planners support features PDDL2.2.
competition test lpg-td problems compiled domains planner
supports original version domains. lpg-td attempted every problem (uncompiled)
IPC-4 domains could attempt terms planning language supports.

208

fiAn Approach Temporal Planning Scheduling

Airport-Windows

Milliseconds
1e+07

LPG-td.s (45 solved)
P-MEP (12 solved)
SGPlan (43 solved)
TilSapa (7 solved)

1e+06

Airport-Windows

Makespan
1400

LPG-td.bq (45 solved)
P-MEP (12 solved)
SGPlan (43 solved)
TilSapa (7 solved)

1200

1000
100000
800
10000
600
1000
400
100

200

10

0
0

5

10

15

20

25

30

35

40

45

PipesWorldNoTankage-Deadlines

Milliseconds
1e+07

0

5

10

15

20

25

30

35

40

45

PipesWorldNoTankage-Deadlines

Makespan
30

LPG-td.bq (23 solved)
TilSapa (3 solved)

LPG-td.s (23 solved)
TilSapa (3 solved)
1e+06

25

100000

20

10000

15

1000

10

100

5

10

0
0

5

10

15

20

25

30

UmtsFlaw-Windows

Milliseconds
10000

0

5

10

15

20

25

30

UmtsFlaw-Windows

Makespan
1900

LPG-td.s (50 solved)
SGPlan (27 solved)

LPG-td.bq (50 solved)
SGPlan (27 solved)
1850
1800

1000

1750
1700
1650

100

1600
1550
1500

10

1450
0

5

10

15

20

25

30

35

40

45

50

0

5

10

15

20

25

30

35

40

45

Figure 6: CPU-time plan quality lpg-td, p-mep, sgplan, tilsapa three
IPC-4 domains timed initial literals. x-axis problem
names simplified numbers. plots left, y-axis
CPU-milliseconds (logarithmic scale); plots right, y-axis
plan makespan (the lower better).
209

50

fiGerevini, Saetti & Serina

lpg-td.s vs p-mep
5.841
< 0.001
45

lpg-td.bq vs p-mep
< 0.001
12

CPU-Time Analysis
lpg-td.s vs sgplan
3.162
(0.0016)
197

lpg-td.s vs tilsapa
10.118
< 0.001
136

Plan Quality Analysis
lpg-td.bq vs sgplan lpg-td.bq vs tilsapa
9.837
6.901
< 0.001
< 0.001
154
63

Figure 7: Results Wilcoxon test performance lpg-td compared
IPC-4 (satisficing) planners terms CPU-times plan quality benchmark problems timed initial literals.

lpg-td.s

sgplan

tilsapa

p-mep

CPU-Time

lpg-td.bq

sgplan

tilsapa

p-mep



B:

consistently better B



B:

better B
significant number times
(confidence level 99.84%)

Plan Quality

Figure 8: Partial order performance IPC-4 (satisficing) planners according
Wilcoxon test benchmark problems timed initial literals.
dashed arrow indicates performance relationship holds confidence
level slightly less 99.9%.

IPC-3 planners. CPU-time analysis, consider problems attempted
compared planners solved least one (when planner
solve problem, corresponding CPU-time IEEE arithmetic representation
positive infinity). plan quality (makespan) analysis, consider problems
solved compared planners.
210

fiAn Approach Temporal Planning Scheduling

order carry Wilcoxon test, planning problem computed
difference CPU-times two planners compared, defining samples
test CPU-time analysis. Similarly, test concerning plan quality
analysis computed differences makespan plans generated
two planners. absolute values differences ranked increasing numbers
starting lowest value. (The lowest value ranked 1, next lowest value
ranked 2, on.) sum ranks positive differences, sum
ranks negative differences. performance two planners significantly
different, number positive differences approximately equal
number negative differences, sum ranks set positive
differences approximately equal sum ranks set. Intuitively,
test considers weighted sum number times one planner performs better
other. sum weighted test uses performance gap assign rank
performance difference.
cell Figure 7 gives result comparison performance
lpg-td another IPC-4 planner. number samples sufficiently large,
T-distribution used Wilcoxon test approximatively normal distribution.
Therefore, cells figure contain z-value p-value characterizing
normal distribution. higher z-value, significant difference
performance is. p-value represents level significance performance gap.
use confidence level 99.9%; hence, p-value lower 0.001,
performance compared planners statistically different. information
appears left (right) side cell, first (second) planner named title
cell performs better planner.11 analysis comparing CPUtime, value cell number problems solved least one planner;
analysis comparing plan quality, number problems solved
planners.
Figure 8 shows graphical description relative performance IPC-4 satisficing
planners according Wilcoxon test benchmark problems timed initial
literals. solid arrow planner planner B (or cluster planners B)
indicates performance statistically different performance B,
performs better B (every planner B). dashed arrow B
indicates better B significant number times, significant
Wilcoxon relationship B confidence level 99.9% (on
hand, relationship holds confidence level slightly less 99.9%). results
analysis say lpg-td consistently faster tilsapa p-mep,
faster sgplan significant number times. terms plan quality, lpg-td performs
consistently better p-mep, sgplan tilsapa.
Although lpg-td guarantee optimal plans, interesting compare
performance optimal planners took part IPC-4, especially see good
lpg-tds plans are. Figure 9 shows performance lpg-td best results
results optimal IPC-4 planners (AllOthers-Opt) temporal variants
Airport Umts (without flawed actions). plots plan quality (makespan)
11. p-value cell comparing lpg-td p-mep omitted number problems solved
lpg-td p-mep high enough approximate T-distribution normal distribution.

211

fiGerevini, Saetti & Serina

Airport-Time

Milliseconds
1e+07

Airport-Time

Makespan
1000

LPG-td.s (44 solved)
LPG-td.bq (44 solved)
AllOthers-Opt (21 solved)

LPG-td.s (44 solved)
LPG-td.bq (44 solved)
AllOthers-Opt (21 solved)

900

1e+06

800
700

100000

600
10000

500
400

1000

300
200

100

100
10

0
0

5

10

15

20

25

30

35

40

45

UMTS-Time

Milliseconds
1e+07

0

5

10

15

20

30

35

40

45

UMTS-Time

Makespan
900

LPG-td.s (50 solved)
LPG-td.bq (50 solved)
AllOthers-Opt (38 solved)

25

LPG-td.s (50 solved)
LPG-td.bq (50 solved)
AllOthers-Opt (38 solved)

850

1e+06
800
100000
750

10000

700

650
1000
600
100
550

10

500
0

5

10

15

20

25

30

35

40

45

50

0

5

10

15

20

25

30

35

40

45

50

Figure 9: Performance lpg-td best optimal planners IPC-4
(AllOthers-Opt) Airport-Time Umts-Time: CPU-time logarithmic scale
(left plots) plan makespan (right plots). x-axis problem
names simplified numbers.

show that, nearly every problem domains, best quality plan found lpg-td
optimal solution, first plan found lpg-td generally good solution.
plots CPU-time show lpg-td finds plan much quickly
optimal planner, CPU-time required lpg-td find best plan often
lower CPU-time required AllOthers-Opt (except problems 12, 16, 18
20 Airport). noted lpg-td.bq last plan sequence
computed plans increasing quality (and CPU-time). intermediate plans
sequence could already good quality. particular, shown plan quality plot
Airport, first plan (lpg-td.s) solving problem 12 near-optimal quality,
computed much quickly lpg-td.bq plan AllOthers-Opt plan.
212

fiAn Approach Temporal Planning Scheduling

Figure 10: Plan quality distance solutions found lpg-td corresponding optimal solutions. x-axis, classes quality distance
(e.g., 1025% means plan generated lpg-td worse
optimal plan factor 0.1 0.25). y-axis,
percentage solved problems classes.

Finally, Figure 10 gives results general analysis plan quality distance,
considering metric-temporal STRIPS variants IPC-4 domains. 12 analysis
uses problems solved least one IPC-4 optimal planner. also important
note consider plans generated incremental process lpg-td using
CPU-time CPU-time required fastest optimal planner (AllOthersOpt). Overall, results Figure 10 provide significant empirical evidence supporting
claim often incremental local search approach allows us compute plans
good quality using less CPU-time optimal approach. particular,
bars 0%1% class plot metric-temporal problems show
percentage test problems best quality plan lpg-td (lpg-td.bq)
optimal nearly optimal (i.e., plan quality worse optimal factor
0 0.01, 0 meaning difference) 90%. Moreover, often first plan
computed lpg-td (lpg-td.s) good quality: 60% plans quality
optimal nearly optimal, 25% quality worse
optimal factor greater 0.5.
Interestingly, plot right Figure 10 shows similar results concerning good
quality lpg-tds plans also STRIPS problems IPC-4 (with lower percentage
lpg-td.s plans 0%1% class, slightly higher percentage
lpg-td.bqs plans > 50% class).
4.2 Temporal Reasoning LPG-td
conducted two main experiments. first aimed testing performance
lpg-td number windows timed initial literals varies problems
12. STRIPS problems, plan quality metric number actions plan.

213

fiGerevini, Saetti & Serina

initial state goals. second experiment focused temporal
reasoning techniques main goals empirically evaluating performance,
understanding impact overall performance lpg-td.
experiments used two well-known IPC-3 domains, modified
include timed initial literals: Rovers ZenoTravel. version Rovers timed
initial literals obtained IPC-3 temporal version follows. problem
specification, waypoint, added collection pairs timed initial literals
type
(at t1 (in sun waypoint0))
(at t2 (not (in sun waypoint0)))

t1 < t2 . pairs defines time window involved literal.
operator specification file, recharge operator precondition
(over (in sun ?w))

imposes constraint recharging actions applied rover
sun (?w operator parameter representing waypoint recharging action.)
modified version ZenoTravel obtained similarly. problem specification,
city added collection pairs timed initial literals type
(at t1 (open-station city0))
(at t2 (not (open-station city0)))

operator specification file, added timed precondition
(over (open-station ?c))

refuel operator, ?c operator parameter representing city
refuel action executed.
Given planning problem collection time windows W timed literal ,
noted that, general, difficulty solving affected three parameters:
number windows W , size, way distributed time
line.13 considered two methods generating test problems taking account
parameters ( indicates original IPC-3 problem either Rovers ZenoTravel
domain, n indicates number windows W ):
(I) Let best (shortest makespan) plan among generated lpg-td
solving within certain CPU-time limit, makespan . time
interval [0, t] divided 2n 1 sub-intervals equal size. time windows
timed literal extended problem 0 odd sub-intervals [0, t],
i.e.,

nh
h

h
3t

2t
, 2n1
,

.
W = 0, 2n1
, 2n1
, . . . , (2n2)t
2n1
(II) Let maximum duration action timed precondition .
time interval = [0, (2n 1)] divided 2n 1 sub-intervals duration d.
13. general, parameters influence hardness temporal reasoning planning,
also logical part planning process (i.e., selection actions forming plan,
lpg-td done using heuristics taking exogenous events account).

214

fiAn Approach Temporal Planning Scheduling

Rovers-Windows

Milliseconds
10000

ZenoTravel-Windows

Milliseconds
1e+06

1 time window per waypoint
10 time windows per waypoint

1 time window per city
10 time windows per city

100000

1000
10000

1000
100

100

10

10
0

2

4

6

8

10

12

14

16

18

20

0

2

4

6

8

10

12

14

16

18

20

Figure 11: Performance lpg-td Rovers ZenoTravel domains extended
timed initial literals (1 10 time windows timed literal). test
problems generated using method I. x-axis problem
names simplified numbers; y-axis CPU-milliseconds (logarithmic scale).

Similarly method (I), time windows extended problem 0
odd sub-intervals .
Notice use first method number windows relatively
small because, many time windows small size, extended problem
become unsolvable (no window large enough schedule necessary action
timed precondition). second method designed avoid problem,
used test techniques planning problems involving many time windows.
Figures 11 12 give results first experiment. CPU-times plots
median values five runs problem. results Figure 11, use
IPC-3 test problems modified method I, results Figure 12 use
IPC-3 test problems modified method II. cases lpg-td solves problems.
plots Figure 11 indicate performance degradation number
windows increases 1 10 generally moderate, except two cases. plots
Figure 12 indicate that, number windows increases exponentially 1
10,000, approach scales well benchmark problems considered. instance,
consider first ZenoTravel problem. 1 window lpg-td solves problem 10
milliseconds, 10 windows 20 milliseconds, 100 windows 30 milliseconds,
1000 windows 100 milliseconds, 10,000 windows 1 second.
Moreover, observed performance degradation mainly determined heavier
pre-processing phase (parsing instantiation operators).
Tables 2 3 give results concerning experiment temporal reasoning
techniques implemented lpg-td. consider problems 10 time windows
(for timed fluent) used tests Figure 11, examine computational
215

fiGerevini, Saetti & Serina

Performance LPG-td Rovers-TimeWindows

Milliseconds
100000

Performance LPG-td ZenoTravel-TimeWindows

Milliseconds
1e+06

1 time window per waypoint
10 time windows per waypoint
100 time windows per waypoint
1000 time windows per waypoint
10,000 time windows per waypoint

1 time window per city
10 time windows per city
100 time windows per city
1000 time windows per city
10,000 time windows per city

100000

10000

10000
1000
1000

100
100

10

10
0

2

4

6

8

10

12

14

16

18

20

0

2

4

6

8

10

12

14

16

18

20

Figure 12: Performance lpg-td Rovers ZenoTravel domains extended
timed initial literals (110,000 time windows timed literal). test
problems generated using method II. x-axis problem
names simplified numbers; y-axis CPU-milliseconds (logarithmic scale).

cost temporal reasoning planning problems. approach temporal
planning, search step defines set temporal constraints formed ordering
scheduling constraints current TDA-graph. Table 2 gives statistical information
DTPs using compact constraint representation lpg-td classical
DTP representation. action TDA-graph, two temporal variables
(the start/end times action), except astart aend (for which, pointed out,
use one variable). number scheduling constraints number
ordering constraints depend actions current TDA-graph,
actions (causally exclusively) related other, respectively (we
one scheduling constraint action timed precondition TDA-graph).
Notice representation scheduling constraints much compact
classical DTP formulation.14
table also gives information average number DTPs (i.e., search steps)
generated planning, indicating many satisfiable (indicated
Sat. DTPs).
Table 3 gives CPU-time required temporal reasoning techniques implemented
lpg-td (Solve-DTP+ ) tsat++ (Armando, Castellini, Giunchiglia, & Maratea,
2004), state-of-the-art general DTP solver. DTPs considered
Table 2, i.e., sets temporal constraints TDA-graph search
14. classical DTP-translation scheduling constraint contains exponential number disjuncts
respect number time windows scheduling constraint. example, let q
timed precondition Wq = {[25, 50), [75, 125)}. scheduling constraint determined q

+

translated four classical DTP constraints (as abbreviates astart ): (a+
25 75),
+
+

+
+
+
+
+

+
+

50







75),
(a



50



125),
(a





25




(a+
125).






216

fiAn Approach Temporal Planning Scheduling

Problems
Rovers
Problem 1
Problem 5
Problem 10
Problem 15
Problem 20
ZenoTravel
Problem 1
Problem 5
Problem 10
Problem 15
Problem 20

Variables
max mean
28
56
94
98
206

18.4
30.0
65.8
58.8
105.0

8
36
114
172
282

6
20
83.4
122.4
194.6

SC 10 windows (DC)
max
mean
1
2
2
3
4

(1024)
(2048)
(2048)
(3072)
(4096)

1 (1024)
3 (3072)
16 (16,384)
24 (24,576)
42 (43,008)

DTPs
(Sat. DTPs)

0.13 (136.5)
0.33 (341.3)
1.41 (1447)
1.01 (1037)
1.45 (1489)

15 (15)
27 (27)
104 (47)
77 (55)
108 (108)

0.33 (341.3)
0.88 (910.2)
10.5 (10,769)
16.3 (16,673)
24.9 (25,536)

3 (3)
18 (18)
1162 (175)
291 (128)
750 (637)

Table 2: Characteristics DTPs generated planning lpg-td solving
problems Rovers ZenoTravel domains: maximum/mean number variables (2nd/3rd columns); maximum/mean number scheduling constraints (SC) non-unary disjunctions (DC) DTP-form translation (4th/5th columns); number DTPs satisfiable DTPs solved lpg-td
(6th column).

step planning process. noted comparison Solve-DTP +
tsat++ means intended determine one better other. Indeed
tsat++ developed manage much larger class DTPs. However, best
knowledge exists specialized DTP-solver handling scheduling constraints could used. goal comparison experimentally show
existing general DTP solvers, although designed work efficiently general case,
adequate managing class DTPs arise planning framework.
Hence, important develop specialized techniques which, empirically demonstrated results Table 3, much efficient. instance, consider problem
15 Rovers domain. indicated last column Table 2, lpg-td solves
problem 77 search steps, defines 77 DTPs. data Table 3 show
total CPU-time spent lpg-td solving temporal reasoning problems
negligible (< 106 seconds), tsat++ requires 16.8 CPU-seconds total (note
whole temporal planning problem solved lpg-td 0.25 seconds). 15 Overall,
specialized temporal reasoning technique several orders magnitude faster
efficient general DTP, terms CPU-time solving single DTP, CPU-time
solving DTPs generated planning.
15. CPU-time tsat++ includes neither generation explicit (classical) DTPs TDAgraph, parsing time. Moreover, tsat++ decides satisfiability input DTPs,
Solve-DTP+ also finds schedule optimal, DTP satisfiable.

217

fiGerevini, Saetti & Serina

Problems

Rovers
Problem 1
Problem 5
Problem 10
Problem 15
Problem 20
ZenoTravel
Problem 1
Problem 5
Problem 10
Problem 15
Problem 20

CPU-seconds Temporal Reasoning
Solve-DTP+
tsat++
max
mean
total
max
mean
total

Total
CPU-Time
lpg-td

< 106
< 106
< 106
< 106
0.01

< 106
< 106
< 106
< 106
0.0008

< 106
< 106
< 106
< 106
0.03

0.005
0.045
0.54
0.54
3.17

0.002
0.002
0.039
0.028
0.10

0.09
0.14
12.7
16.8
107.1

0.02
0.03
0.30
0.25
3.03

< 106
< 106
0.01
0.01
0.01

< 106
< 106
0.00017
0.00014
0.00065

< 106
< 106
0.2
0.04
0.5

0.001
0.04
2.7
44.6
323.9

0.0003
0.004
9.8
3.9
24.2

0.01
0.21
6018
18,877
177,595

0.02
0.05
22.0
13.9
376.2

Table 3: Performance Solve-DTP+ tsat++ DTPs generated planning
lpg-td solving problems Rovers ZenoTravel domains:
maximum, mean total CPU-seconds. last column gives total CPUtime lpg-td solving planning problem. tsat++ run using default
settings.

Finally, experimentally tested effectiveness improvements Solve-DTP +
making algorithm incremental described end Section 2 (such
improvements included implementation Solve-DTP+ Table 3). particular
observed that, problems Table 3, average CPU-time basic (nonincremental) version Solve-DTP+ one three orders magnitude higher
incremental version. However, basic version still always significantly faster
tsat++ (from one four orders magnitude).

5. Related Work
Several researchers addressed temporal reasoning context DTP framework. general techniques aimed efficiently solving DTP proposed
(e.g., Armando et al., 2004; Tsamardinos & Pollack, 2003), worst-case complexity
remains exponential. Section 4, presented experimental results indicating
simple use state-of-the-art DTP solver adequate solving subclass
DTPs arise context.
Various planning approaches supporting temporal features considered paper
proposed. One first planners capable handling predictable
exogenous events deviser (Vere, 1983), developed nonlin (Tate, 1977).
deviser temporal partial order planner using network activities called plan
network. starting plan generation, plan network contains exogenous events
218

fiAn Approach Temporal Planning Scheduling

explicit nodes network. plan generation, activities added network
ordered respect scheduled events, depending relevance events
activities. similar explicit treatment exogenous events could also adopted
context action-graph representation: initial action graph contains special
action nodes representing predicted exogenous events. However, simple method
disadvantages respect method, treats exogenous events
temporal level representation rather logical (causal) level. particular,
high number timed initial literals, explicit representation
exogenous events action graph could lead large graphs, causing memory
consumption problems possibly heavy CPU-time cost heuristic evaluation
(possibly large) search neighborhood.
late 80s early 90s temporal planners handling exogenous events
developed. general, systems use input descriptions planning problem/domain significantly different PDDL descriptions accepted modern
fully-automated planners. One successful among hsts (Frederking &
Muscettola, 1992; Muscettola, 1994), representation problem solving framework
provides integrated view planning scheduling. hsts represents predictable exogenous events non-controllable state variables. lpg-td hsts manage
temporal constraints, two systems use considerably different approaches temporal
planning (lpg-td adopts classical state-transition view change, hsts adopts
histories view change, Ghallab, Nau, & Traverso, 2003), based
different plan representations search techniques.
zeno (Penberthy, 1993; Penberthy & Weld, 1994) one first domain-independent
planners supports rich class metric-temporal features, including exogenous events.
zeno powerful extension causal-link partial-order planner ucpop (Penberthy &
Weld, 1992). However, terms computational performance, planner competitive recent temporal planners.
IxTeT (Ghallab & Laruelle, 1994; Laborie & Ghallab, 1995) another causal-link planner uses techniques ideas scheduling, temporal constraint reasoning,
graph algorithms. IxTeT supports expressive language temporal description actions, including timed preconditions features cannot
expressed PDDL2.2. expressive power language obtained cost increased semantic complexity (Fox & Long, 2005). observed Ghallab, Nau Traverso
(2003), IxTeT embodies compromise expressiveness complex temporal domains, planning efficiency; however, planner still remains noncompetitive
recent temporal planners.
Smith Weld (1999) studied extension Graphplan-style planning
managing temporal domains. proposed extension tgp planner makes
possible represent predictable exogenous events. tgp supports subclass
durative actions expressible PDDL2.1, prevents cases concurrency
PDDL2.1 admitted. tgp optimal planner (under assumed conservative
model action concurrency), lpg-td near-optimal (satisficing) planner. main
drawback tgp scale adequately.
recently, Edelkamp (2004) proposed method planning timed initial
literals based compiling action timed preconditions time window as219

fiGerevini, Saetti & Serina

sociated action, defining interval action scheduled.
gives efficient, polynomial algorithm based critical path analysis computing
optimal action schedule sequential plans generated using compiled representation. techniques presented Edelkamp assume unique time window
timed precondition. techniques propose general, sense
action representation treats multiple time windows associated timed precondition,
temporal reasoning method computes optimal schedules partially ordered plans
preserving polynomiality.
Cresswell Coddington (2004) proposed extension lpgp planner (Long
& Fox, 2003b) handle timed initial literals, represented special deadline
actions. literal asserted hold time represented deadline action
starting time initial state, duration t. deadline actions
plan construction translated particular linear inequalities that, together
equalities inequalities generated plan representation, managed
general linear programming solver. lpg-td uses different representation
encode timed initial literals special actions, temporal scheduling
constraints associated actions plan managed efficient algorithm
derived specializing general DTP solver.
order handle problems timed initial literals sapa planner (Do & Kambhampati, 2003), Do, Kambhampati Zimmerman (2004) proposed forward search
heuristic based relaxed plans, constructed exploiting technique similar
time slack analysis used scheduling (Smith & Cheng, 1993). Given set candidate
actions choosing action add relaxed plan construction, technique
computes minimum slack candidate action actions currently
relaxed plan. candidate action highest minimum slack preferred. lpg-td
uses different time slack analysis, exploited different way. method
selecting actions forming relaxed plan uses time slacks counting number
scheduling constraints would violated adding candidate action: prefer
candidate actions cause lowest number violations. Moreover, sapa
slack analysis limited actions relaxed plan, method also considers
actions real plan construction.
dt-pop recent planner (Schwartz & Pollack, 2004) extending POP-style
planning action model involving disjunctive temporal constraints. language
dt-pop elegant express rich class temporal features,
indirectly (and less elegantly) expressed PDDL2.2 (Fox et al., 2004). treatment
temporal constraints required manage predictable exogenous events dt-pop
appears less efficient planner, since dt-pop uses general DTP solver
enhanced efficiency techniques, lpg-td uses polynomial solver specialized
subclass DTPs arise representation. dt-pop handles mutex actions
(threats) posting explicit temporal disjunctive constraints imposing disjointness
mutex actions, lpg-td implicitly decides disjunctions search time
choosing level graph actions inserted, asserting appropriate
precedence constraints. Moreover, search procedure heuristics dt-pop lpgtd significantly different.
220

fiAn Approach Temporal Planning Scheduling

IPC-4, planners reasoned timed initial literals tilsapa (Kavuluri &
U, 2004), sgplan (Chen et al., 2004), p-mep (Sanchez et al., 2004) lpg-td. first
two planners, time writing, best knowledge available literature
sufficiently detailed description clearly understand possible similarities
differences lpg-td treatment predictable exogenous events. Regarding
p-mep, planner uses forward state-space search guided relaxed plan heuristic which,
differently relaxed plans lpg-td, constructed without taking account
temporal aspects relaxed plan real plan construction (the makespan
constructed relaxed plans considered comparative evaluation).

6. Conclusions
presented techniques temporal planning domains certain fluents
made true false known times predictable exogenous events cannot
influenced actions available planner. external events present many
realistic domains, planner take account guarantee correctness
synthesized plans, generate plans good optimal quality (makespan),
use effective search heuristics fast planning.
approach, causal structure plan represented graph-based representation called TDA-graph, action ordering scheduling constraints managed
efficient constraint-based reasoning, plan search based stochastic local search
procedure. proposed algorithm managing temporal constraints
TDA-graph specialization general CSP-based method solving DTPs.
algorithm polynomial worst-case complexity and, combined plan
representation, practice efficient. also presented local search
techniques temporal planning using new TDA-graph representation. techniques improve accuracy heuristic methods adopted previous version
lpg, extend consider action scheduling constraints evaluation
search neighborhood, based relaxed temporal plans exploiting (dynamic)
reachability information.
techniques implemented planner lpg-td. experimentally
investigated performance planner statistical analysis IPC-4 results
using Wilcoxons test. results analysis show planner performs well
compared recent temporal planners supporting predictable exogenous events,
terms CPU-time find valid plan quality best plan generated. Moreover,
comparison plans computed lpg-td generated optimal planners
IPC-4 shows often lpg-td generates plans good optimal quality.
Finally, additional experiments indicate temporal reasoning techniques manage
class DTPs arise context efficiently.
directions future work temporal planning within framework are:
extension local search heuristics temporal reasoning techniques explicitly handle action effects limited persistence delays; treatment predictable exogenous
events affecting numerical fluents discrete continuous way; development tech221

fiGerevini, Saetti & Serina

niques supporting controllable exogenous events;16 management actions
variable durations (Fox & Long, 2003), i.e., actions whose durations specified
inequalities constraining lower upper bounds, whose actual duration decided
planner.
Moreover, intend study integration framework techniques
goal partitioning subplan composition successfully used sgplan
(Chen et al., 2004) IPC-4, application approach plan revision.
latter already partially explored, simple strips domains using
less powerful search techniques (Gerevini & Serina, 2000).

Acknowledgments
paper revised extended version paper appearing Proceedings
Nineteenth International Joint Conference Artificial Intelligence (Gerevini, Saetti, &
Serina, 2005a). research supported part MIUR Grant anemone. work
Ivan Serina part carried Department Computer Information
Sciences University Strathclyde (Glasgow, UK), supported Marie Curie
Fellowship N HPMF-CT-2002-02149. would like thank anonymous reviewers
helpful comments, Paolo Toninelli extended parser lpg-td handle
new language features PDDL2.2.

Appendix A: Reachability Information
techniques described paper computing action evaluation function use
heuristic reachability information minimum number actions required reach
preconditions domain action (N um acts) lower bound earliest
finishing time (Ef t) reachable actions (the actions whose preconditions reachable).
following, S(l) denotes state defined facts corresponding fact nodes
supported level l current TDA-graph. l = 1, S(l) represents initial state
planning problem (I).
action a, lpg-td pre-computes N um acts(a, I), i.e., estimated minimum
number actions required reach preconditions I, Ef t(a, I), i.e.,
estimated earliest finishing time (if reachable I). Similarly, fact f
reachable I, lpg-td computes estimated minimum number actions required
reach f (N um acts(f, I)) estimated earliest time f made
true plan starting (Et(f, I)). l > 1, N um acts(a, S(l)) Ef t(a, S(l))
computed search, depend action nodes
current TDA-graph levels preceding l. Since search many action nodes
added removed, operations N um acts(a, S(l)) Ef t(a, S(l))
could change (if operation concerns level preceding l), important
computed efficiently.
16. Consider instance transportation domain shuttle bus train station extra
run airport midnight booked advance. shuttle booking domain action
available planner, event night stop shuttle controlled planner.

222

fiAn Approach Temporal Planning Scheduling

ReachabilityInformation(I, O)
Input: initial state planning problem consideration (I) ground instances
(actions) operators (O);
Output: action a, estimate number actions (N um acts(a, I)) required reach
preconditions I, estimate earliest finishing time (Ef t(a, I)).
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.

forall facts f /* set facts precomputed operator instantiation phase */
f
N um acts(f, I) Et(f, I) 0; Action(f, I) astart ;
else N um acts(f, I) Et(f, I) ;
forall actions N um acts(a, I) Ef t(a, I) Lf t(a) ;
F I; Fnew I; O; Arev ;
( Fnew 6= Arev 6= )
F F Fnew ; Fnew ; Arev ; Arev ;
A0 = {a | P re(a) F } empty
action A0 ;
ComputeEFT(a, AX Et(f, I));
f P re(a)

< Ef t(a, I) Ef t(a, I) t;
Lf t(a) ComputeLFT(a);
Ef t(a, I) Lf t(a) /* scheduled */
ra RequiredActions(I, P re(a));
N um acts(a, I) > ra N um acts(a, I) ra;
forall f Add(a)
Et(f, I) >
Et(f, I) t;
Arev Arev {a0 | f P re(a0 )};
N um acts(f, I) > (ra + 1)
N um acts(f, I) ra + 1; Action(f, I) a;
Fnew Fnew Add(a) F ;
{a};

RequiredActions(I, G)
Input: set facts set action preconditions G;
Output: estimate min number actions required achieve facts G (ACTS).
1.
2.
3.
4.
5.
6.
7.
8.

ACT ;
G G I;
G 6=
g element G;
Action(g, I);
ACT ACT {a};
G G P re(a) bACT Add(b);
return(|ACT S|).

Figure 13: Algorithms computing heuristic information search cost
time reaching set facts G I.

223

fiGerevini, Saetti & Serina

Figure 13 gives ReachabilityInformation, algorithm used lpg-td computing
N um acts(a, I), Ef t(a, I), N um acts(f, I) Et(f, I). ReachabilityInformation similar
reachability algorithm used version lpg took part 2002 planning
competition (lpg-ipc3), significant differences. main differences are:
(i) order estimate earliest finishing time domain actions, ReachabilityInformation takes account scheduling constraints, considered
previous version algorithm;
(ii) algorithm used lpg-ipc3 applies domain action once, ReachabilityInformation apply once.
Notice (i) improves accuracy estimated finishing time actions
(Ef t), important piece information used search neighborhood
evaluation selecting actions forming temporal relaxed plans (see Section 3).
Moreover, (i) allows us identify domain actions cannot scheduled
time windows associated timed preconditions, pruned
away.
Regarding (ii), forward process computing reachability information,
action re-applied whenever estimated earliest time one preconditions
decreased. important two reasons. one hand, reconsidering actions already
applied useful lead better estimate action finishing times;
hand, also necessary guarantee correctness reachability
algorithm. latter because, overestimate earliest finishing time action
scheduling constraint, could incorrectly conclude action cannot
scheduled (and would consider action inapplicable). action necessary
valid plan, incorrect estimate earliest finishing time could lead
incorrect conclusion planning problem unsolvable. words, estimated
finishing time action scheduling constraint lower bound actual
earliest finishing time.
ReachabilityInformation could used update N um acts(a, S(l)) Ef t(a, S(l))
action insertion/removal, l > 1 (when l > 1, instead I, input algorithm
S(l)). However, order make updating process efficient, revision done
selective focused way. Instead revising reachability information
graph modification (search step), evaluating search neighborhood
choosing estimated best modification. Specifically, repairing flawed level l,
update reachability information actions facts levels preceding
l updated yet. (For instance, suppose ith search step add
action level 5, (i + 1)th step add another action level 10.
(i + 1)th step need consider updating reachability information levels
610, since information levels 15 already updated ith step.)
sufficient search neighborhood repairing flawed level consideration
(l) contain graph modifications concerning levels preceding l.
describing steps ReachabilityInformation, need introduce notation. Add (a) denotes set positive effects a; Pre(a) denotes set
(non-timed) preconditions a; Arev denotes set actions already applied whose
224

fiAn Approach Temporal Planning Scheduling

reachability could revised estimated earliest time preconditions revised application. Given action node current
earliest start time computed maximum earliest times preconditions reachable, ComputeEFT (a, t) function computing earliest finishing
time consistent scheduling constraint (if any)
+ Dur(a) .17 ComputeLFT (a) function computing latest finishing time
action a, i.e., returns upper bound last time window
scheduled (if one exists), returns timed precondition.
example, let action preconditions true initial
state (i.e., = 0), duration 50, scheduling constraint imposing
action executed interval [25, 100). ComputeEFT (a, t) returns 75,
ComputeLFT (a, t) returns 100. Thus, scheduling constraint satisfied.
contrary, earliest start time 500, ComputeEFT (a, t) returns 550
cannot scheduled [25, 100).
sake clarity, first describe steps ReachabilityInformation used derive
N um acts, comment computation Ef t. steps 14,
every fact f , algorithm initializes N um acts(f, I) 0, f I, otherwise
(indicating f reachable); while, step 5, N um acts(a, I) initialized
(indicating reachable I). Then, steps 724 algorithm iteratively
constructs set F facts reachable I, starting F = I,
terminating F cannot extended set Arev actions reconsider
empty. set available actions initialized set possible actions
(step 6); reduced application (step 24), augmented set
actions Arev (step 8) action application. modify estimated time
precondition action becomes reachable, added rev (step 20).
internal while-loop (steps 924) applies actions current F , possibly deriving
new set facts Fnew step 23. Fnew Arev empty, F extended
Fnew , extended Arev , internal loop repeated. action A0
(the subset actions currently applicable F ) applied, reachability
information effects revised follows. First estimate minimum number
ra actions required achieve P re(a) using subroutine RequiredActions (step
15). use ra possibly update N um acts(a, I) N um acts(f, I) effect
f (steps 1516, 2122). number actions required achieve preconditions
lower current value N um acts(a, I), N um acts(a, I) set ra.
Moreover, application leads lower estimate f , i.e., ra + 1 less
current value N um acts(f, I), N um acts(f, I) set ra + 1. addition, data
structure indicating current best action achieve f (Action(f, I)) set
(step 22). information used subroutine RequiredActions.
fact f initial state, value Action(f, I) astart (step 3).
subroutine RequiredActions one reachability algorithm lpg-ipc3.
subroutine uses Action derive ra backward process starting input
set action preconditions (G), ending G I. subroutine incrementally
constructs set actions (ACTS) achieving facts G preconditions
17. scheduling constraint associated a, existing scheduling constraints cannot
satisfied starting action t, ComputeEFT (a, t) returns + Dur(a).

225

fiGerevini, Saetti & Serina

actions already selected (using Action). iteration set G revised adding
preconditions last action selected, removing facts belonging
effects actions already selected (step 7). Termination RequiredActions guaranteed
every element G reachable I.
briefly describe computation temporal information. Eft(a, I), computed way similar N um acts(a, I). steps 14, ReachabilityInformation initializes
estimated earliest time (Et(f, I)) fact f becomes reachable 0, f I,
otherwise; moreover, algorithm sets Ef t(a, I) Lf t(a, I) . Then, every
application action forward process described above, estimate earliest
finishing time Ef adding duration (current) maximum estimated earliest
time preconditions a, taking account scheduling constraints
using ComputeEFT (a) (step 11). addition, compute latest finishing time Lf
using ComputeLFT (a) (step 13). earliest finishing time action
greater latest finishing time, timed preconditions cannot satisfied
I, steps 1523 executed (see if-statement step 14). effect f
current temporal value higher earliest finishing time a, steps 1819
set Et(f, I) t, step 20 adds Arev (because decreased estimated
earliestx time f , revision could decrease estimated start time action
precondition f ).

Appendix B: Wilcoxon Test Metric-Temporal Domains IPC-4
appendix, present results Wilcoxon sign-rank test performance
lpg-td satisficing IPC-4 planners attempted metric-temporal
domains. performance evaluated terms CPU-times plan quality.
cell first two tables gives result comparison performance
lpg-td another IPC-4 planner. number samples sufficiently large,
T-distribution used Wilcoxon test approximatively normal distribution. Hence,
cell Figure give z-value p-value characterizing normal
distribution. higher z-value, significant difference performance
is. p-value represents level significance difference performance.
use confidence level 99.9%; therefore, p-value lower 0.001,
performance two planners statistically different. information appears
left (right) side cell, first (second) planner named title cell
performs better other. analysis comparing CPU-time, value
cell number problems solved least one planner; analysis
comparing plan quality, number problems solved planners.
pictures tables show partial order performance compared
planners terms CPU-time plan quality. solid edge planner another
planner B (or cluster planners B) indicates performance statistically
different performance B, performs better B (every planner
B). dashed edge B indicates better B significant number
times, significant Wilcoxon relationship confidence level
99.9%.
226

fiAn Approach Temporal Planning Scheduling

lpg-td.s vs crikey
11.275
< 0.001
169

Analysis CPU-Time
lpg-td.s vs p-mep
lpg-td.s vs sgplan
11.132
0.387
< 0.001
(0.699)
215
513

lpg-td.s vs tilsapa
12.324
< 0.001
136

lpg-td.bq vs crikey
10.500
< 0.001
173

Analysis Plan Quality
lpg-td.bq vs p-mep lpg-td.bq vs sgplan
4.016
16.879
< 0.001
< 0.001
21
452

lpg-td.bq vs tilsapa
6.901
< 0.001
63

lpg-td.s
crikey

p-mep

tilsapa

sgplan

CPU-Time

sgplan
lpg-td.bq
p-mep

crikey



B:

consistently better B

tilsapa



B:

better B significant number times
(confidence level 94.78%)

Plan Quality

227

fiGerevini, Saetti & Serina

References
Armando, A., Castellini, C., Giunchiglia, E., & Maratea, M. (2004). SAT-based decision
procedure boolean combination difference constraints. Proceedings
Seventh International Conference Theory Applications Satisfiability Testing
(SAT-04), Berlin, Heidelberg, New York. Springer-Verlag. SAT 2004 LNCS Volume.
Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90, pp. 281300.
Chen, Y., Hsu, C., & Wah B., W. (2004). SGPlan: Subgoal partitioning resolution
planning. Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.),
Abstract Booklet Competing Planners ICAPS-04, pp. 3032.
Cresswell, S., & Coddington, A. (2004). Adapting LPGP plan deadlines. Proceedings Sixteenth European Conference Artificial Intelligence (ECAI-04),
pp. 983984, Amsterdam, Netherlands. IOS Press.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49, pp. 6195.
Do, M., B., Kambhampati, S., & Zimmerman, T. (2004). Planning - scheduling connections
exogenous events. Proceedings ICAPS-04 Workshop Integrating
Planning Scheduling, pp. 3237.
Do, M., & Kambhampati, S. (2003). SAPA: multi-objective metric temporal planner.
Journal Artificial Intelligence Research (JAIR), 20, pp. 155194.
Edelkamp, S. (2004). Extended critical paths temporal planning. Proceedings
ICAPS-04 Workshop Integrating Planning Scheduling, pp. 3845.
Edelkamp, S., & Hoffmann, J. (2004). PDDL2.2: language classic part
4th international planning competition. Technical report 195, Institut fur Informatik,
Freiburg, Germany.
Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (2004) Abstract Booklet
competing planners ICAPS-04.
Erschler, J., Roubellat, F., & Vernhes, J. P. (1976). Finding essential characteristics
feasible solutions scheduling problem. Operations Research (OR), 24, pp.
772782.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal
planning domains. Journal Artificial Intelligence Research (JAIR), 20, pp. 61124.
Fox, M., & Long, D. (2005). Planning time. Fisher, M., Gabbay, D., & Vila, L. (Eds.),
Handbook Temporal Reasoning Artificial Intelligence, pp. 497536. Elsevier Science Publishers, New York, NY, USA.
Fox, M., Long, D., & Halsey, K. (2004). investigation expressive power
PDDL2.1. Proceedings Sixteenth European Conference Artificial Intelligence (ECAI-04), pp. 338342, Amsterdam, Netherlands. IOS Press.
Frederking, R., E., & Muscettola, N. (1992). Temporal planning transportation planning scheduling. IEEE International Conference Robotics Automation
(ICRA-92), pp. 11251230. IEEE Computer Society Press.
228

fiAn Approach Temporal Planning Scheduling

Gerevini, A., & Cristani, M. (1997). finding solution temporal constraint satisfaction
problems. Proceedings Fifteenth International Joint Conference Artificial
Intelligence (IJCAI-97), Vol. 2, pp. 14601465, San Francisco, CA, USA. Morgan
Kaufmann Publishers.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local search
temporal action graphs. Journal Artificial Intelligence Research (JAIR), 20, pp.
239290.
Gerevini, A., Saetti, A., & Serina, I. (2004). empirical analysis heuristic features
local search LPG. Proceedings Fourteenth International Conference
Automated Planning Scheduling (ICAPS-04), pp. 171180, Menlo Park, CA,
USA. AAAI Press.
Gerevini, A., Saetti, A., & Serina, I. (2005a). Integrating planning temporal reasoning domains durations time windows. Proceedings Nineteenth
International Joint Conference Artificial Intelligence (IJCAI-05), pp. 12261235,
Menlo Park, CA, USA. International Joint Conference Artificial Intelligence Inc.
Gerevini, A., Saetti, A., Serina, I., & Toninelli, P. (2005b). Fast planning domains
derived predicates: approach based rule-action graphs local search.
Proceedings Twentieth National Conference Artificial Intelligence (AAAI05), pp. 11571162, Menlo Park, CA, USA. AAAI Press.
Gerevini, A., & Serina, I. (1999). Fast planning greedy action graphs. Proceedings
Sixteenth National Conference Artificial Intelligence (AAAI-99), pp. 503
510, Menlo Park, CA, USA. AAAI Press/MIT Press.
Gerevini, A., & Serina, I. (2000). Fast plan adaptation planning graphs: Local
systematic search techniques. Proceedings Fifth International Conference
Artificial Intelligence Planning Scheduling (AIPS-00), pp. 112121, Menlo Park,
CA, USA. AAAI Press/MIT Press.
Ghallab, M., & Laruelle, H. (1994). Representation control IxTeT, temporal planner. Proceedings Second International Conference Artificial Intelligence
Planning Systems (AIPS-94), pp. 6167, Menlo Park, CA, USA. AAAI press.
Ghallab, M., Nau, D., & Traverso, P. (2003). Automated Planning: Theory Practice.
Morgan Kaufmann Publishers, San Francisco, CA, USA.
Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer Academic Publishers, Boston, USA.
Helmert, M. (2004). planning heuristic based causal graph analysis. Proceedings
Fourteenth International Conference Automated Planning Scheduling
(ICAPS-04), pp. 161170, Menlo Park, CA, USA. AAAI Press.
Kavuluri, B. R., & U, S. (2004). Tilsapa - timed initial literals using SAPA. Edelkamp, S.,
Hoffmann, J., Littman, M., & Younes, H. (Eds.), Abstract Booklet Competing
Planners ICAPS-04, pp. 4647.
Laborie, P., & Ghallab, M. (1995). Planning sharable resource constraints. Proceedings Fourteenth International Joint Conference Artificial Intelligence
(IJCAI-95), Vol. 2, pp. 16431651, San Francisco, CA, USA. Morgan Kaufmann Publishers.
229

fiGerevini, Saetti & Serina

Long, D., & Fox, M. (2003a). 3rd international planning competition: Results
analysis. Journal Artificial Intelligence Research (JAIR), 20, pp. 159.
Long, D., & Fox, M. (2003b). Exploiting graphplan framework temporal planning.
Proceedings Thirteenth International Conference Automated Planning
Scheduling (ICAPS-03), pp. 5261, Menlo Park, CA, USA. AAAI Press.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. Proceedings
Ninth National Conference Artificial Intelligence (AAAI-91), pp. 634639,
Menlo Park, CA, USA. AAAI Press.
Muscettola, N. (1994). HSTS: Integrating planning scheduling. Zweben, & Fox
(Eds.), Intelligent Scheduling, pp. 169212, San Francisco, CA, USA. Morgan Kaufmann Publishers.
Nguyen, X., & Kambhampati, S. (2001). Reviving partial order planning. Proceedings
Seventeenth International Joint Conference Artificial Intelligence (IJCAI-01),
Vol. 1, pp. 459464, San Francisco, CA, USA. Morgan Kaufmann Publishers.
Penberthy, J., & Weld, D. (1992). UCPOP: sound, complete, partial order planner
ADL. Proceedings Third International Conference Principles Knowledge
Representation Reasoning (KR92), pp. 103114, San Mateo, CA, USA. Morgan
Kaufmann Publishers.
Penberthy, J., & Weld, D. (1994). Temporal planning continuous change. Proceedings
Twelfth National Conference Artificial Intelligence (AAAI-94), pp. 1010
1015, Menlo Park, CA, USA. AAAI Press/MIT Press.
Penberthy, J., S. (1993). Planning Continuous Change. Ph.D. thesis, University
Washington, Seattle, WA, USA. Available technical report UW-CSE-93-12-01.
Sanchez, J., Tang, M., & Mali, A., D. (2004). P-MEP: Parallel expressive planner.
Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.), Abstract Booklet
Competing Planners ICAPS-04, pp. 5355.
Schwartz, P., J., & Pollack, M., E. (2004). Planning disjunctive temporal constraints.
Proceedings ICAPS-04 Workshop Integrating Planning Scheduling,
pp. 6774.
Smith, D., & Weld, D. (1999). Temporal planning mutual exclusive reasoning.
Proceedings Sixteenth International Joint Conference Artificial Intelligence
(IJCAI-99), pp. 326337, San Francisco, CA, USA. Morgan Kaufmann Publishers.
Smith, S., & Cheng, C. (1993). Slack-based heuristics constraint satisfaction scheduling.
Proceedings Eleventh National Conference Artificial Intelligence (AAAI93), pp. 139144, Menlo Park, CA, USA. AAAI Press/The MIT press.
Stergiou, K., & Koubarakis, M. (2000). Backtracking algorithms disjunctions temporal
constraints. Artificial Intelligence, 120 (1), pp. 81117.
Tate, A. (1977). Generating project networks. Proceedings Fifth International
Joint Conference Artificial Intelligence (IJCAI-77), pp. 888889, Cambridge, MA,
USA. MIT, William Kaufmann.
230

fiAn Approach Temporal Planning Scheduling

Tsamardinos, I., & Pollack, M. E. (2003). Efficient solution techniques disjunctive
temporal reasoning problems. Artificial Intelligence, 151 (1-2), pp. 4389.
Vere, S. A. (1983). Planning time: Windows durations activities goals. IEEE
Transactions Pattern Analysis Machine Intelligence, 5 (3), pp. 246267.
Vidal, V. (2004). lookahead strategy heuristic search planning. Proceedings
Fourteenth International Conference Automated Planning Scheduling (ICAPS04), pp. 150159, Menlo Park, CA, USA. AAAI Press.
Wilcoxon, F., & Wilcox, R. A. (1964). Rapid Approximate Statistical Procedures.
American Cyanamid Co., Pearl River, NY, USA.

231

fiJournal Artificial Intelligence Research 25 (2006) 425456

Submitted 12/04; published 4/06

Logical Hidden Markov Models
Kristian Kersting
Luc De Raedt

kersting@informatik.uni-freiburg.de
deraedt@informatik.uni-freiburg.de

Institute Computer Science
Albert-Ludwigs-Universitat Freiburg
Georges-Koehler-Allee 079
D-79110 Freiburg, Germany

Tapani Raiko

tapani.raiko@hut.fi

Laboratory Computer Information Science
Helsinki University Technology
P.O. Box 5400
FIN-02015 HUT, Finland

Abstract
Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov models
deal sequences structured symbols form logical atoms, rather flat
characters.
note formally introduces LOHMMs presents solutions three central inference problems LOHMMs: evaluation, likely hidden state sequence parameter estimation. resulting representation algorithms experimentally evaluated
problems domain bioinformatics.

1. Introduction
Hidden Markov models (HMMs) (Rabiner & Juang, 1986) extremely popular analyzing sequential data. Application areas include computational biology, user modelling,
speech recognition, empirical natural language processing, robotics. Despite successes, HMMs major weakness: handle sequences flat, i.e., unstructured symbols. Yet, many applications symbols occurring sequences structured. Consider, e.g., sequences UNIX commands, may parameters
emacs lohmms.tex, ls, latex lohmms.tex, . . .Thus, commands essentially structured.
Tasks considered UNIX command sequences include prediction
next command sequence (Davison & Hirsh, 1998), classification command
sequence user category (Korvemaker & Greiner, 2000; Jacobs & Blockeel, 2001),
anomaly detection (Lane, 1999). Traditional HMMs cannot easily deal type
structured sequences. Indeed, applying HMMs requires either 1) ignoring structure
commands (i.e., parameters), 2) taking possible parameters explicitly
account. former approach results serious information loss; latter leads
combinatorial explosion number symbols parameters HMM
consequence inhibits generalization.
sketched problem HMMs akin problem dealing structured examples traditional machine learning algorithms studied fields inductive logic programming (Muggleton & De Raedt, 1994) multi-relational learnc
2006
AI Access Foundation. rights reserved.

fiKersting, De Raedt, & Raiko

ing (Dzeroski & Lavrac, 2001). paper, propose (inductive) logic programming
framework, Logical HMMs (LOHMMs), upgrades HMMs deal structure.
key idea underlying LOHMMs employ logical atoms structured (output state)
symbols. Using logical atoms, UNIX command sequence represented
emacs(lohmms.tex), ls, latex(lohmms.tex), . . . two important motivations
using logical atoms symbol level. First, variables atoms allow one make
abstraction specific symbols. E.g., logical atom emacs(X, tex) represents files X
LATEX user tex could edit using emacs. Second, unification allows one share information among states. E.g., sequence emacs(X, tex), latex(X, tex) denotes
file used argument Emacs LATEX.
paper organized follows. reviewing logical preliminaries, introduce
LOHMMs define semantics Section 3; Section 4, upgrade basic
HMM inference algorithms use LOHMMs; investigate benefits LOHMMs
Section 5: show LOHMMs strictly expressive HMMs,
design order magnitude smaller corresponding propositional
instantiations, unification yield models, better fit data. Section 6,
empirically investigate benefits LOHMMs real world data. concluding,
discuss related work Section 7. Proofs theorems found Appendix.

2. Logical Preliminaries
first-order alphabet set relation symbols r arity 0, written r/m,
set functor symbols f arity n 0, written f/n. n = 0 f called constant,
= 0 p called propositional variable. (We assume least one constant
given.) atom r(t1 , . . . , tn ) relation symbol r followed bracketed n-tuple
terms ti . term variable V functor symbol f(t1 , . . . , tk ) immediately followed
bracketed k-tuple terms ti . Variables written upper-case, constant, functor predicate symbols lower-case. symbol denote anonymous variables
read treated distinct, new variables time encountered. iterative
clause formula form H B H (called head) B (called body) logical
atoms. substitution = {V1 /t1 , . . . , Vn /tn }, e.g. {X/tex}, assignment terms ti
variables Vi . Applying substitution term, atom clause e yields instantiated term, atom, clause e occurrences variables V simultaneously
replaced term ti , e.g. ls(X) emacs(F, X){X/tex} yields ls(tex) emacs(F, tex).
substitution called unifier finite set atoms singleton. unifier
called general unifier (MGU) if, unifier S, exists
substitution = . term, atom clause E called ground contains
variables, i.e., vars(E) = . Herbrand base , denoted hb , set
ground atoms constructed predicate functor symbols . set G (A)
atom consists ground atoms belong hb .

3. Logical Hidden Markov Models
logical component traditional HMM corresponds Mealy machine (Hopcroft
& Ullman, 1979), i.e., finite state machine output symbols associated
426

fiLogical Hidden Markov Models

transitions. essentially propositional representation symbols used
represent states output symbols flat, i.e. structured. key idea underlying
LOHMMs replace flat symbols abstract symbols. abstract symbol
definition logical atom. abstract represents set ground, i.e.,
variable-free atoms alphabet , denoted G (A). Ground atoms play
role traditional symbols used HMMs.
Example 1 Consider alphabet 1 constant symbols tex, dvi, hmm1,
lohmm1, relation symbols emacs/2, ls/1, xdvi/1, latex/2. atom
emacs(File, tex) represents set {emacs(hmm1, tex), emacs(lohmm1, tex)}. assume
alphabet typed avoid useless instantiations emacs(tex, tex)).
use atoms instead flat symbols allows us analyze logical structured sequences
emacs(hmm1, tex), latex(hmm1, tex), xdvi(hmm1, dvi).


Definition 1 Abstract transition expressions form p : H
B p [0, 1],
H, B atoms. variables implicitly assumed universally quantified,
i.e., scope variables single abstract transition.
atoms H B represent abstract states represents abstract output symbol.

semantics abstract transition p : H
B one one states
G (B), say BB , one go probability p one states G (HB ), say HB H ,
emitting symbol G (OB H ), say OB H .
latex(File)

Example 2 Consider c 0.8 : xdvi(File, dvi) latex(File, tex). general
H, B share predicate. due nature running example. Assume state latex(hmm1, tex), i.e.
B = {File/hmm1}. c specifies probability 0.8 next state
G1 (xdvi(hmm1, dvi)) = {xdvi(hmm1, dvi)} ( i.e., probability 0.8
next state xdvi(hmm1, dvi)), one symbols G 1 (latex(hmm1)) =
{latex(hmm1)} ( i.e., latex(hmm1)) emitted. Abstract states might also
complex latex(file(FileStem, FileExtension), User)
example simple H empty. situation becomes complicated substitutions empty. Then, resulting
state output symbol sets necessarily singletons. Indeed, transilatex(File)

tion 0.8 : emacs(File0 , dvi) latex(File, tex) resulting state set would
G1 (emacs(File0 , dvi)) = {emacs(hmm1, tex), emacs(lohmm1, tex)}. Thus transition
non-deterministic two possible resulting states. therefore need
mechanism assign probabilities possible alternatives.
Definition 2 selection distribution specifies abstract state observation
symbol alphabet distribution ( | A) G (A).
continue example, let (emacs(hmm1, tex) | emacs(File0 , tex)) = 0.4
(emacs(lohmm1, tex) | emacs(File0 , tex)) = 0.6. would probability 0.4 0.8 = 0.32 next state emacs(hmm1, tex) 0.48
emacs(lohmm1, tex).
427

fiKersting, De Raedt, & Raiko



Taking account, meaning abstract transition p : H
B summarized follows. Let BB G (B), HB H G (HB ) OB H G (OB H ).
model makes transition state BB HB H emits symbol OB H probability
p (HB H | HB ) (OB H | OB H ).

(1)

represent , probabilistic representation - principle - used, e.g. Bayesian
network Markov chain. Throughout remainder present paper, however,
use nave Bayes approach. precisely, associate argument
r/m
r/m
relation r/m finite domain Di
constants probability distribution Pi

r/m
Di . Let vars(A) = {V1 , . . . , Vl } variables occurring atom r/m,
let = {V1 /s1 , . . . Vl /sl } substitution grounding A. Vj considered
r/m
random variable domain Darg(Vj ) argument arg(Vj ) appears first in. Then,
Q
r/m
(A | A) = lj=1 Parg(Vj ) (sj ). E.g. (emacs(hmm1, tex) | emacs(F, E)), computed
emacs/2

emacs/2

product P1
(hmm1) P2
(tex).
Thus far semantics single abstract transition defined. LOHMM
usually consists multiple abstract transitions creates complication.
Example 3 Consider

emacs(File)

0.8 : latex(File, tex) emacs(File, tex)



emacs(File)

0.4 : dvi(File) emacs(File, User).
two abstract transitions make
conflicting statements state resulting emacs(hmm1, tex). Indeed, according
first transition, probability 0.8 resulting state latex(hmm1, tex)
according second one assigns 0.4 xdvi(hmm1).
essentially two ways deal situation. one hand, one might want
combine normalize two transitions assign probability 23 respectively 13 .
hand, one might want one rule firing. paper, chose
latter option allows us consider transitions independently, simplifies
learning, yields locally interpretable models. employ subsumption (or generality) relation among B-parts two abstract transitions. Indeed, B-part
first transition B1 = emacs(File, tex) specific second transition B2 = emacs(File, User) exists substitution = {User/tex}
B2 = B1 , i.e., B2 subsumes B1 . Therefore G1 (B1 ) G1 (B2 ) first transition
regarded informative second one. therefore preferred
second one starting emacs(hmm1, tex). also say first transition specific second one. Remark generality relation imposes
partial order set transitions. considerations lead strategy
considering maximally specific transitions apply state order determine
successor states. implements kind exception handling default reasoning
akin Katzs (1987) back-off n-gram models. back-off n-gram models,
detailed model deemed provide sufficiently reliable information current
context used. is, one encounters n-gram sufficiently reliable,
back-off use (n 1)-gram; reliable either back-off level n 2, etc.
conflict resolution strategy work properly provided bodies maximally specific transitions (matching given state) represent abstract state.
428

fiLogical Hidden Markov Models

start

ls : 0.4

0.45

0.55
emacs(F) : 0.7
ls(U0 )

emacs(F, U)
emacs(F) : 0.3

ls : 0.6
emacs(F) : 0.3

emacs(F0 , U)

latex(F) : 0.2
emacs(F) : 0.1

latex(F) : 0.2
latex(F, tex)

emacs(F, tex)
emacs(F) : 0.6

latex(F) : 0.6

Figure 1: logical hidden Markov model.

enforced requiring generality relation B-parts closed
greatest lower bound (glb) predicate, i.e., pair B1 , B2 bodies,
= mgu(B1 , B2 ) exists, another body B (called lower bound) subsumes B1
(therefore also B2 ) subsumed B1 , B2 , lower bound
subsumed B. E.g., body second abstract transition example
emacs(hmm1, User) set abstract transitions would closed glb.
Finally, order specify prior distribution states, assume finite set
clauses form p : H start using distinguished start symbol p
probability LOHMM start state G (H).
able formally define logical hidden Markov models.
Definition 3 logical hidden Markov model (LOHMM) tuple (, , , )
logical alphabet, selection probability , set abstract transitions,
set abstract transitions encoding prior distribution. Let B set atoms
occur body parts transitions . assume B closed glb require
X
B B :
p = 1.0
(2)

p:H
B

probabilities p clauses sum 1.0 .

HMMs special cases LOHMMs contains relation symbols arity
zero selection probability irrelevant. Thus, LOHMMs directly generalize HMMs.
LOHMMs also represented graphically. Figure 1 contains example. underlying language 2 consists 1 together constant symbol denotes
user employ LATEX. graphical notation, nodes represent abstract states
black tipped arrows denote abstract transitions. White tipped arrows used represent meta knowledge. precisely, white tipped, dashed arrows represent generality
subsumption ordering abstract states. follow transition abstract state
outgoing white tipped, dotted arrow dotted arrow always followed.
Dotted arrows needed abstract state occur different cirlatex(File)
cumstances. Consider transition p : latex(File0 , User0 ) latex(File, User).
429

fiKersting, De Raedt, & Raiko

0.6

start

0.45

em(F, U)



em(F, t)

state

abstract state

abstract state
0.4

ls


ls(t)

em(f1 )

em(f1 , t)

1.0

la(F, t)

la(f1 , t)

abstract state

state

ls(U0 )

em(f2 )

state abstract state

0.6

la(f1 )

0.7

em(f2 , o)
state



em(F, U)
abstract state

em(F0 , U)
abstract state

Figure 2: Generating

observation
sequence
emacs(hmm1), latex(hmm1),
emacs(lohmm1), ls LOHMM Figure 1. command emacs
abbreviated em, f1 denotes filename hmm1, f2 represents lohmm1, denotes
tex user, user. White tipped solid arrows indicate selections.

Even though atoms head body transition syntactically different
represent abstract state. accurately represent meaning transition
cannot use black tipped arrow latex(File, User) itself, would actulatex(File)

ally represent abstract transition p : latex(File, User) latex(File, User).
Furthermore, graphical representation clarifies LOHMMs generative models. Let us explain model Figure 1 would generate observation sequence
emacs(hmm1), latex(hmm1), emacs(lohmm1), ls (cf. Figure 2). chooses initial abstract state, say emacs(F, U). Since variables F U uninstantiated, model
samples state emacs(hmm1, tex) G2 using . indicated dashed arrow, emacs(F, tex) specific emacs(F, U). Moreover, emacs(hmm1, tex) matches
emacs(F, tex). Thus, model enters emacs(F, tex). Since value F already
instantiated previous abstract state, emacs(hmm1, tex) sampled probability
1.0. Now, model goes latex(F, tex), emitting emacs(hmm1) abstract
observation emacs(F) already fully instantiated. Again, since F already instantiated,
latex(hmm1, tex) sampled probability 1.0. Next, move emacs(F 0 , U), emitting latex(hmm1). Variables F0 U emacs(F0 , U) yet bound; so, values, say
lohmm1 others, sampled . dotted arrow brings us back emacs(F, U).
variables implicitly universally quantified abstract transitions, scope
variables restricted single abstract transitions. turn, F treated distinct,
new variable, automatically unified F0 , bound lohmm1. contrast,
variable U already instantiated. Emitting emacs(lohmm1), model makes transition
ls(U0 ). Assume samples tex U0 . Then, remains ls(U0 ) probability
0.4 . Considering possible samples, allows one prove following theorem.
Theorem 1 (Semantics) logical hidden Markov model language defines
discrete time stochastic process, i.e., sequence random variables hX it=1,2,... ,
domain
Xt hb() hb(). induced probability measure Cartesian product
N
hb() hb() exists unique > 0 limit .
concluding section, let us address design choices underlying LOHMMs.
First, LOHMMs introduced Mealy machines, i.e., output symbols
associated transitions. Mealy machines fit logical setting quite intuitively
directly encode conditional probability P (O, S0 |S) making transition S0
430

fiLogical Hidden Markov Models

emitting observation O. Logical hidden Markov models define distribution
X
P (O, S0 |S) =
p (S0 | HB ) (O | O0 B H )
O0
p:HB
O0

sum runs abstract transitions H B B specific S.
Observations correspond (partially) observed proof steps and, hence, provide information
shared among heads bodies abstract transitions. contrast, HMMs usually
introduced Moore machines. Here, output symbols associated states implicitly
assuming S0 independent. Thus, P (O, S0 | S) factorizes P (O | S) P (S0 | S).
makes difficult observe information shared among heads bodies.
turn, Moore-LOHMMs less intuitive harder understand. detailed
discussion issue, refer Appendix B essentially show
propositional case Mealy- Moore-LOHMMs equivalent.
Second, nave Bayes approach selection distribution reduces model complexity expense lower expressivity: functors neglected variables
treated independently. Adapting expressive approaches interesting future line
research. instance, Bayesian networks allow one represent factorial HMMs (Ghahramani & Jordan, 1997). Factorial HMMs viewed LOHMMs, hidden
states summarized 2 k-ary abstract state. first k arguments encode k
state variables, last k arguments serve memory previous joint state.
i-th argument conditioned + k-th argument. Markov chains allow one
sample compound terms finite depth s(s(s(0))) model e.g. misspelled
filenames. akin generalized HMMs (Kulp, Haussler, Reese, & Eeckman, 1996),
node may output finite sequence symbols rather single symbol.
Finally, LOHMMs introduced present paper specify probability distribution sequences given length. Reconsider LOHMM Figure 1. Already probabilities observation sequences length 1, i.e., ls, emacs(hmm1),

P
emacs(lohmm1)) sum 1. precisely, > 0 holds x1 ,...,xt P (X1 =
x1 , . .P
. , Xt P
= xt ) = 1.0 . order model distribution sequences variable length,
i.e., t>0 x1 ,...,xt P (X1 = x1 , . . . , Xt = xt ) = 1.0 may add distinguished end state.
end state absorbing whenever model makes transition state,
terminates observation sequence generated.

4. Three Inference Problems LOHMMs
HMMs, three inference problems interest. Let LOHMM let
= O1 , O2 , . . . , OT , > 0, finite sequence ground observations:
(1) Evaluation: Determine probability P (O | ) sequence generated
model .
(2) likely state sequence: Determine hidden state sequence
likely produced observation sequence O, i.e. = arg maxS P (S | O, ) .
(3) Parameter estimation: Given set = {O1 , . . . , Ok } observation sequences, determine likely parameters abstract transitions selection
| ) .
distribution , i.e. = arg max P (O
431

fiPSfrag replacements
Kersting, De Raedt, & Raiko

sc(1)
abstract selection abstract
transition
transition

selection

abstract selection
sc(2)
transition

sc(Y)
ls(o)
ls(o)

ls(o)

ls(U)
ls(t)

hc(1)

ls(o)
ls(t)

ls(t)

hc(2)

ls(t)
ls(U)

start

hc(X)

...

ls(U)
em(f2,o)

sc(Z)

em(f1,o)

em(F,U)

em(f1,t)

em(F,o)

em(f2,t)

latex(f1,t)

latex(f1,t) latex(f2,t)

em(F,U)

O1

em(F,U)

O2
O2

abstract state

S2

S1

S0

em(F,o)

states

Figure 3: Trellis induced LOHMM Figure 1. sets reachable states time
0, 1, . . . denoted S0 , S1 , . . . contrast HMMs, additional
layer states sampled abstract states.

address problems turn upgrading existing solutions
HMMs. realized computing grounded trellis Figure 3. possible
ground successor states given state computed first selecting applicable
abstract transitions applying selection probabilities (while taking account
substitutions) ground resulting states. two-step factorization coalesced
one step HMMs.
evaluate O, consider probability partial observation sequence 1 , O2 , . . . , Ot
(ground) state time t, 0 < , given model = (, , , )
(S) := P (O1 , O2 , . . . , Ot , qt = | )
qt = denotes system state time t. HMMs, (S) computed using dynamic programming approach. = 0, set 0 (S) = P (q0 = | ) ,
i.e., 0 (S) probability starting state and, > 0, compute (S) based
t1 (S0 ):
1: S0 := {start}
2: = 1, 2, . . . ,
3:
St =
4:
foreach St1
5:
6:
7:
8:
9:

/* initialize set reachable states*/
/* initialize set reachable states clock t*/


foreach maximally specific p : H
B s.t. B = mgu(S, B) exists
foreach S0 = HB H G (HB ) s.t. Ot1 unifies OB H
S0 6 St
St := St {S0 }
(S0 ) := 0.0

(S0 ) := (S0 ) + t1 (S) p
P
11: return P (O | ) = SST (S)

10:

432

(S0 | HB ) (Ot1 | OB H )

fiLogical Hidden Markov Models

assume sake simplicity start abstract transition p : H
start . Furthermore, boxed parts specify differences HMM formula:
unification taken account.
P
Clearly, HMMs P (O | ) = SST (S) holds. computational complexity
forward procedure O(T (|B| + g)) = O(T s2 ) = maxt=1,2,...,T |St | ,
maximal number outgoing abstract transitions regard abstract state,
g maximal number ground instances abstract state. completely
analogous manner, one devise backward procedure compute
(S) = P (Ot+1 , Ot+2 , . . . , OT | qt = S, ) .
useful solving Problem (3).
forward procedure, straightforward adapt Viterbi algorithm
solution Problem (2), i.e., computing likely state sequence. Let (S)
denote highest probability along single path time accounts first
observations ends state S, i.e.,
(S) =

max

S0 ,S1 ,...,St1

P (S0 , S1 , . . . , St1 , St = S, O1 , . . . , Ot1 |M ) .

procedure finding likely state sequence basically follows forward procedure. Instead summing ground transition probabilities line 10, maximize
them. precisely, proceed follows:
1:S0 := {start}
/* initialize set reachable states*/
2: = 1, 2, . . . ,
3:
St =
/* initialize set reachable states clock t*/
foreach St1
4:

5:
foreach maximally specific p : H
B s.t. B = mgu(S, B) exists
foreach S0 = HB H G (HB ) s.t. Ot1 unifies OB H
6:
S0 6 St
7:
8:
St := St {S0 }
(S, S0 ) := 0.0
9:
10:
(S, S0 ) := (S, S0 ) + t1 (S) p (S0 | HB ) (Ot1 | OB H )
11:
foreach S0 St
12:
(S0 ) = maxSSt1 (S, S0 )
13:
(S0 ) = arg maxSSt1 (S, S0 )
Here, (S, S0 ) stores probability making transition S0 (S0 ) (with
1 (S) = start states S) keeps track state maximizing probability along
single path time accounts first observations ends state 0 .
likely hidden state sequence computed
ST +1 = arg max +1 (S)


St

=

SST +1
(St+1 )

= T, 1, . . . , 1 .

One also consider problem (2) abstract level. Instead considering
contributions different abstract transitions single ground transition state
433

fiKersting, De Raedt, & Raiko

state S0 line 10, one might also consider likely abstract transition only.
realized replacing line 10 forward procedure
(S0 ) := max(t (S0 ), t1 (S) p (S0 | HB ) (Ot1 | OB H )) .
solves problem finding (20 ) likely state abstract transition
sequence:
Determine sequence states abstract transitions GT =
S0 , T0 , S1 , T1 , S2 , . . . , ST , TT , ST+1 exists substitutions Si+1
Si Ti likely produced observation sequence O, i.e.
GT = arg maxGT P (GT | O, ) .
Thus, logical hidden Markov models also pose new types inference problems.
parameter estimation, estimate maximum likelihood transition
probabilities selection distributions. estimate former, upgrade well-known
Baum-Welch algorithm (Baum, 1972) estimating maximum likelihood parameters
HMMs probabilistic context-free grammars.
HMMs, Baum-Welch algorithm computes improved estimate p tranO
sition probability (ground) transition p : H
B taking ratio
p= P

(T)
H0

O0

B

(T0 )

(3)

expected number (T) times making transitions time given
model observation sequence O, total number times transitions
made B time given O.
Basically applies abstract transition. However,
little bit careful direct access (T). Let (gcl, T)
GO
probability following abstract transition via ground instance gcl p : GH GB
time t, i.e.,
(gcl, T) =

(GB) p t+1 (GH)
(GH | HB ) (Ot1 | OB H ) ,
P (O | )

(4)

B , H forward procedure (see above) P (O | ) probability
model generated sequence O. Again, boxed terms constitute main
difference corresponding HMM formula. order apply Equation (3) compute
improved estimates probabilities associated abstract transitions, set
(T) =


X
t=1

(T) =

X
X

(gcl, T)

t=1 gcl

inner sum runs ground instances T.
leads following re-estimation method, assume sets
reachable states reused computations - -values:
434

fiLogical Hidden Markov Models

1:
2:
3:
4:
5:
6:
7:
8:
9:

/* initialization expected counts */
foreach
(T) := /* 0 using pseudocounts */
/* compute expected counts */
= 0, 1, . . . ,
foreach St


foreach max. specific p : H
B s.t. B = mgu(S, B) exists
foreach S0 = HB H G (HB ) s.t. S0 St+1 mgu(Ot , OB H ) exists

(T) := (T) + (S) p t+1 (S0 ) P (O | ) (S0 | HB ) (Ot1 | OB H )

Here, equation (4) found line 9. line 3, set pseudocounts small samplesize regularizers. methods avoid biased underestimate probabilities even
zero probabilities m-estimates (see e.g., Mitchell, 1997) easily adapted.
estimate selection probabilities, recall follows nave Bayes scheme. Therefore, estimated probability domain element domain ratio
number times selected number times d0 selected.
procedure computing -values thus reused.
Altogether, Baum-Welch algorithm works follows: converged, (1) estimate abstract transition probabilities, (2) selection probabilities. Since
instance EM algorithm, increases likelihood data every update,
according McLachlan Krishnan (1997), guaranteed reach stationary
point. standard techniques overcome limitations EM algorithms applicable.
computational complexity (per iteration) O(k ( + d)) = O(k s2 + k d)
k number sequences, complexity computing -values (see above),
sum sizes domains associated predicates. Recently, Kersting
Raiko (2005) combined Baum-Welch algorithm structure search model
selection logical hidden Markov models using inductive logic programming (Muggleton
& De Raedt, 1994) refinement operators. refinement operators account different
abstraction levels explored.

5. Advantages LOHMMs
section, investigate benefits LOHMMs: (1) LOHMMs strictly
expressive HMMs, (2), using abstraction, logical variables unification
beneficial. specifically, (2), show
(B1) LOHMMs design smaller propositional instantiations,
(B2) unification yield better log-likelihood estimates.
5.1 Expressivity LOHMMs
Whereas HMMs specify probability distributions regular languages, LOHMMs specify
probability distributions expressive languages.

435

fiKersting, De Raedt, & Raiko

Theorem 2 (consistent) probabilistic context-free grammar (PCFG) G
language L exists LOHMM s.t. PG (w) = PM (w) w L.
proof (see Appendix C) makes use abstract states unbounded depth.
precisely, functors used implement stack. Without functors, LOHMMs cannot
encode PCFGs and, Herbrand base finite, proven always
exists equivalent HMM.
Furthermore, functors allowed, LOHMMs strictly expressive PCFGs.
specify probability distributions languages context-sensitive:
1.0 :
stack(s(0), s(0))

0.8 :
stack(s(X), s(X))


0.2 : unstack(s(X), s(X))

b
1.0 :
unstack(X, Y)

c
1.0 :
unstack(s(0), Y)

end
1.0 :
end

start
stack(X, X)
stack(X, X)
unstack(s(X), Y)
unstack(s(0), s(Y))
unstack(s(0), s(0))

LOHMM defines distribution {an bn cn | n > 0}.
Finally, use logical variables also enables one deal identifiers. Identifiers
special types constants denote objects. Indeed, recall UNIX command
sequence emacs lohmms.tex, ls, latex lohmms.tex, . . . introduction. filename
lohmms.tex identifier. Usually, specific identifiers matter rather
fact object occurs multiple times sequence. LOHMMs easily deal
identifiers setting selection probability constant arguments
identifiers occur. Unification takes care necessary variable bindings.
5.2 Benefits Abstraction Variables Unification
Reconsider domain UNIX command sequences. Unix users oftenly reuse newly created directory subsequent commands mkdir(vt100x), cd(vt100x), ls(vt100x) .
Unification allow us elegantly employ information allows us specify that, observing created directory, model makes transition state
newly created directory used:
p1 : cd(Dir, mkdir) mkdir(Dir, com)



p2 : cd( , mkdir) mkdir(Dir, com)

first transition followed, cd command move newly created directory;
second transition followed, specified directory cd move to. Thus,
LOHMM captures reuse created directories argument future commands.
Moreover, LOHMM encodes simplest possible case show benefits unification. time, observation sequence uniquely determines state sequence,
functors used. Therefore, left abstract output symbols associated
abstract transitions. total, LOHMM U , modelling reuse directories, consists
542 parameters still covers 451000 (ground) states, see Appendix
complete model. compression number parameters supports (B1).
empirically investigate benefits unification, compare U variant N
U variables shared, i.e., unification used instance
436

fiLogical Hidden Markov Models

first transition allowed, see Appendix D. N 164 parameters less U .
computed following zero-one win function
(


1 log PU (O) log PN (O) > 0
f (O) =
0 otherwise
leave-one-out cross-validated Unix shell logs collected Greenberg (1988). Overall,
data consists 168 users four groups: computer scientists, nonprogrammers, novices
others. 300000 commands logged average 110 sessions
per user. present results subset data. considered computer
scientist sessions least single mkdir command appears. yield 283 logical
sequences total 3286 ground atoms. LOO win 81.63%. LOO statistics
also favor U :

U
N

training
O)
O) log PPU (O
log P (O
O)
N (O
11361.0
1795.3
13157.0

test
log P (O) log PPNU (O)
(O)
42.8
7.91
50.7

Thus, although U 164 parameters N , shows better generalization performance. result supports (B2). pattern often found U 1
0.15 : cd(Dir, mkdir) mkdir(Dir, com)



0.08 : cd( , mkdir) mkdir(Dir, com)

favoring changing directory made. knowledge cannot captured N
0.25 : cd( , mkdir) mkdir(Dir, com).
results clearly show abstraction variables unification beneficial
applications, i.e., (B1) (B2) hold.

6. Real World Applications
intentions investigate whether LOHMMs applied real world
domains. precisely, investigate whether benefits (B1) (B2) also
exploited real world application domains. Additionally, investigate whether
(B3) LOHMMs competitive ILP algorithms also utilize unification
abstraction variables,
(B4) LOHMMs handle tree-structured data similar PCFGs.
aim, conducted experiments two bioinformatics application domains: protein
fold recognition (Kersting, Raiko, Kramer, & De Raedt, 2003) mRNA signal structure
detection (Horvath, Wrobel, & Bohnebeck, 2001). application domains multiclass
problems five different classes each.
1. sum probabilities (0.15 + 0.08 = 0.23 6= 0.25) use pseudo counts
subliminal non-determinism (w.r.t. abstract states) U , i.e., case first
transition fires, second one also fires.

437

fiKersting, De Raedt, & Raiko

6.1 Methodology
order tackle multiclass problem LOHMMs, followed plug-in estimate
approach. Let {c1 , c2 , . . . , ck } set possible classes. Given finite set training
examples {(xi , yi )}ni=1 X {c1 , c2 , . . . , cn }, one tries find f : X {c1 , c2 , . . . , ck }
f (x) = arg

max

c{c1 ,c2 ,...,ck }

P (x | M, c ) P (c) .

(5)

low approximation error training data well unseen examples.
Equation (5), denotes model structure classes, c denotes
maximum likelihood parameters class c estimated training examples
yi = c only, P (c) prior class distribution.
implemented Baum-Welch algorithm (with pseudocounts m, see line 3) maximum likelihood parameter estimation using Prolog system Yap-4.4.4. experiments,
set = 1 let Baum-Welch algorithm stop change log-likelihood
less 0.1 one iteration next. experiments ran Pentium-IV
3.2 GHz Linux machine.
6.2 Protein Fold Recognition
Protein fold recognition concerned proteins fold nature, i.e., threedimensional structures. important problem biological functions proteins
depend way fold. common approach use database searches find proteins (of known fold) similar newly discovered protein (of unknown fold). facilitate
protein fold recognition, several expert-based classification schemes proteins
developed group current set known protein structures according similarity
folds. instance, structural classification proteins (Hubbard, Murzin, Brenner, & Chotia, 1997) (SCOP) database hierarchically organizes proteins according
structures evolutionary origin. machine learning perspective, SCOP induces
classification problem: given protein unknown fold, assign best matching group
classification scheme. protein fold classification problem investigated
Turcotte, Muggleton, Sternberg (2001) based inductive logic programming
(ILP) system PROGOL Kersting et al. (2003) based LOHMMs.
secondary structure protein domains2 elegantly represented logical sequences. example, secondary structure Ribosomal protein L4 represented
st(null, 2), he(right, alpha, 6), st(plus, 2), he(right, alpha, 4), st(plus, 2),
he(right, alpha, 4), st(plus, 3), he(right, alpha, 4), st(plus, 1), he(hright, alpha, 6)
Helices certain type, orientation length he(HelixType, HelixOrientation, Length),
strands certain orientation length st(StrandOrientation, Length) atoms
logical predicates. application traditional HMMs sequences requires one
either ignore structure helices strands, results loss information,
take possible combinations (of arguments orientation length) account,
leads combinatorial explosion number parameters
2. domain viewed sub-section protein appears number distantly related
proteins fold independently rest protein.

438

fiLogical Hidden Markov Models

end
Block B length 3

Block s(B) length 2

Dynamics within block

Dynamics within block

block(B, s(P))

block(s(B), s(P))

block(B, P)

block(s(B), P)
Transition next block

Transition next block

block(s(B), s(0))

block(B, s(s(s(0))))

block(B, 0)

block(s(B), 0)

Figure 4: Scheme left-to-right LOHMM block model.
results reported Kersting et al. (2003) indicate LOHMMs well-suited
protein fold classification: number parameters LOHMM order
magnitude smaller number corresponding HMM (120 versus approximately
62000) generalization performance, 74% accuracy, comparable Turcotte
et al.s (2001) result based ILP system Progol, 75% accuracy. Kersting et al.
(2003), however, cross-validate results investigate common
bioinformatics impact primary sequence similarity classification accuracy.
instance, two commonly requested ASTRAL subsets subset sequences
less 95% identity (95 cut) less 40% identity
(40 cut). Motivated this, conducted following new experiments.
data consists logical sequences secondary structure protein domains.
work Kersting et al. (2003), task predict one five populated
SCOP folds alpha beta proteins (a/b): TIM beta/alpha-barrel (fold 1), NAD(P)binding Rossmann-fold domains (fold 2), Ribosomal protein L4 (fold 23), Cysteine hydrolase
(fold 37), Phosphotyrosine protein phosphatases I-like (fold 55). class a/b
proteins consists proteins mainly parallel beta sheets (beta-alpha-beta units).
data extracted automatically ASTRAL dataset version 1.65 (Chandonia,
Hon, Walker, Lo Conte, P.Koehl, & Brenner, 2004) 95 cut 40 cut.
work Kersting et al. (2003), consider strands helices only, i.e., coils
isolated strands discarded. 95 cut, yields 816 logical sequences consisting
total 22210 ground atoms. number sequences classes listed 293,
151, 87, 195, 90. 40 cut, yields 523 logical sequences consisting total
14986 ground atoms. number sequences classes listed 182, 100, 66, 122,
53.
LOHMM structure: used LOHMM structure follows left-to-right block topology,
see Figure 4, model blocks consecutive helices (resp. strands). Block
size s, say 3, model remain block = 3 time steps. similar
idea used model haplotypes (Koivisto, Perola, Varilo, Hennah, Ekelund, Lukk,
Peltonen, Ukkonen, & Mannila, 2002; Koivisto, Kivioja, Mannila, Rastas, & Ukkonen,
2004). contrast common HMM block models (Won, Prugel-Bennett, & Krogh, 2004),
439

fiKersting, De Raedt, & Raiko

transition parameters shared within block one ensure model
makes transition next state s(Block ) end block; example
exactly 3 intra-block transitions. Furthermore, specific abstract transitions
helix types strand orientations model priori distribution, intra-
inter-block transitions. number blocks sizes chosen according
empirical distribution sequence lengths data beginning
ending protein domains likely captured detail. yield following block
structure
1 2

...

19 20

27 28

...

40 41

46 47

61 62

76 77

numbers denote positions within protein domains. Furthermore, note
last block gathers remaining transitions. blocks modelled using
hidden abstract states
hc(HelixType, HelixOrientation, Length, Block ) sc(StrandOrientation, Length, Block ) .
Here, Length denotes number consecutive bases structure element consists of.
length discretized 10 bins original lengths uniformally
distributed. total, LOHMM 295 parameters. corresponding HMM without
parameter sharing 65200 parameters. clearly confirms (B1).
Results: performed 10-fold cross-validation. 95 cut dataset, accuracy
76% took approx. 25 minutes per cross-validation iteration; 40 cut, accuracy
73% took approx. 12 minutes per cross-validation iteration. results validate
Kersting et al.s (2003) results and, turn, clearly show (B3) holds. Moreover,
novel results 40 cut dataset indicate similarities detected LOHMMs
protein domain structures accompanied high sequence similarity.
6.3 mRNA Signal Structure Detection
mRNA sequences consist bases (guanine, adenine, uracil, cytosine) fold intramolecularly form number short base-paired stems (Durbin, Eddy, Krogh, & Mitchison,
1998). base-paired structure called secondary structure, cf. Figures 5 6.
secondary structure contains special subsequences called signal structures responsible special biological functions, RNA-protein interactions cellular transport.
function signal structure class based common characteristic binding
site class elements. elements necessarily identical similar.
vary topology (tree structure), size (number constituting bases), base
sequence.
goal experiments recognize instances signal structures classes
mRNA molecules. first application relational learning recognize signal structure class mRNA molecules described works Bohnebeck, Horvath,
Wrobel (1998) Horvath et al. (2001), relational instance-based learner
RIBL applied. dataset 3 used similar one described Horvath
3. dataset described work Horvath et al. (2001) could obtain
original dataset. compare smaller data set used Horvath et al., consisted

440

fiLogical Hidden Markov Models

et al. (2001). consisted 93 mRNA secondary structure sequences. precisely,
composed 15 5 SECIS (Selenocysteine Insertion Sequence), 27 IRE (Iron Responsive
Element), 36 TAR (Trans Activating Region) 10 histone stem loops constituting five
classes.
secondary structure composed different building blocks stacking region,
hairpin loops, interior loops etc. contrast secondary structure proteins forms
chains, secondary structure mRNA forms tree. trees easily handled
using HMMs, mRNA secondary structure data challenging proteins.
Moreover, Horvath et al. (2001) report making tree structure available RIBL
background knowledge influence classification accuracy. precisely,
using simple chain representation RIBL achieved 77.2% leave-one-out cross-validation
(LOO) accuracy whereas using tree structure background knowledge RIBL achieved
95.4% LOO accuracy.
followed Horvath et al.s experimental setup, is, adapted data representations LOHMMs compared chain model tree model.

Chain Representation: chain representation (see also Figure 5),
signal
structures

described

single(TypeSingle, Position, Acid )

helical(TypeHelical , Position, Acid , Acid ).
Depending type, structure element represented either single/3 helical/4.
first argument
TypeSingle (resp.
TypeHelical ) specifies type structure element, i.e.,
single, bulge3, bulge5, hairpin (resp. stem). argument Position position sequence element within corresponding structure element counted down,
i.e.4 , {n13 (0), n12 (0), . . . , n1 (0)}. maximal position set 13
maximal position observed data. last argument encodes observed nucleotide
(pair).
used LOHMM structure follows left-to-right block structure shown
Figure 4. underlying idea model blocks consecutive helical structure elements. hidden states modelled using single(TypeSingle, Position, Acid , Block )
helical(TypeHelical , Position, Acid , Acid , Block ). Block consecutive helical (resp. single) structure elements, model remain Block transition
single element. transition single (resp. helical) element occurs Position
n(0). positions n(Position), transitions helical (resp. single)
structure elements helical (resp. single) structure elements Position capturing dynamics nucleotide pairs (resp. nucleotides) within structure elements. instance,

66 signal structures close data set. larger data set (with 400 structures) Horvath
et al. report error rate 3.8% .
4. nm (0) shorthand recursive application functor n 0 times, i.e., position m.

441

fiKersting, De Raedt, & Raiko

helical(stem, n(0), c, g).
helical(stem, n(n(0)), c, g).
helical(stem, n(n(n(0))), c, g).
single(bulge5, n(0), a).
single(bulge5, n(n(0)), a).
single(bulge5, n(n(n(0))), g).
helical(stem, n(0), c, g).
helical(stem, n(n(0)), c, g).
single(bulge5, n(0), a).
helical(stem, n(0), a, a).
helical(stem, n(n(0)), u, a).
helical(stem, n(n(n(0))), u, g).
helical(stem, n(n(n(n(0)))), u, a).
helical(stem, n(n(n(n(n(0))))), c, a).
helical(stem, n(n(n(n(n(n(0)))))), u, a).
helical(stem, n(n(n(n(n(n(n(0))))))), a, u).

u


u
c
c
c

g
g
g



g


c
c

single(hairpin, n(n(n(0))), a).
single(hairpin, n(n(0)), u).
single(hairpin, n(0), u).

single(bulge3, n(0), a).

g
g



u
u
u
c
u




g



u

Figure 5: chain representation SECIS signal structure. ground atoms
ordered clockwise starting helical(stem, n(n(n(n(n(n(n(0))))))), a, u)
lower left-hand side corner.

transitions block n(0) position n(n(0))
pa :he(stem,n(0),X,Y)

: he(stem, n(0), X, Y, n(0)) he(stem, n(n(0)), X, Y, n(0)))
pb :he(stem,n(0),X,Y)

b:

he(stem, n(0), Y, X, n(0)) he(stem, n(n(0)), X, Y, n(0)))

c:

he(stem, n(0), X, , n(0)) he(stem, n(n(0)), X, Y, n(0)))

d:

he(stem, n(0), , Y, n(0)) he(stem, n(n(0)), X, Y, n(0)))

e:

he(stem, n(0), , , n(0)) he(stem, n(n(0)), X, Y, n(0)))

pc :he(stem,n(0),X,Y)

pd :he(stem,n(0),X,Y)
pe :he(stem,n(0),X,Y)

total, 5 possible blocks maximal number blocks consecutive
helical structure elements observed data. Overall, LOHMM 702 parameters.
contrast, corresponding HMM 16600 transitions validating (B1).
Results: LOO test log-likelihood 63.7, EM iteration took average
26 seconds.
Without unification-based transitions b-d, i.e., using abstract transitions
pa :he(stem,n(0),X,Y)

: he(stem, n(0), X, Y, n(0)) he(stem, n(n(0)), X, Y, n(0)))
e:

pe :he(stem,n(0),X,Y)

he(stem, n(0), , , n(0)) he(stem, n(n(0)), X, Y, n(0))),

model 506 parameters. LOO test log-likelihood 64.21, EM iteration took average 20 seconds. difference LOO test log-likelihood statistically
significant (paired t-test, p = 0.01).
Omitting even transition a, LOO test log-likelihood dropped 66.06,
average time per EM iteration 18 seconds. model 341 parameters.
difference average LOO log-likelihood statistically significant (paired t-test, p = 0.001).
results clearly show unification yield better LOO test log-likelihoods, i.e.,
(B2) holds.
442

fiLogical Hidden Markov Models

nucleotide pair((c, g)).
nucleotide pair((c, g)).
nucleotide pair((c, g)).
helical(s(s(s(s(s(0))))), s(s(s(0))), [c], stem, n(n(n(0)))).
nucleotide(a).
nucleotide(a).
nucleotide(g).
single(s(s(s(s(0)))), s(s(s(0))), [], bulge5, n(n(n(0)))).
nucleotide pair((c, g)).
nucleotide pair((c, g)).
helical(s(s(s(0))), s(0), [c, c, c], stem, n(n(0))).
nucleotide(a).
single(s(s(0)), s(0), [], bulge5, n(0)).
nucleotide pair((a, a)).
nucleotide pair((u, a)).
nucleotide pair((u, g)).
nucleotide pair((u, a)).
nucleotide pair((c, a)).
nucleotide pair((u, a)).
nucleotide pair((a, u)).
helical(s(0), 0, [c, c], stem, n(n(n(n(n(n(n(0)))))))).

u


u
c
c
c

g
g
g



g


c
c

g
g

single(s(s(s(s(s(s(0)))))), s(s(s(s(s(0))))),
[], hairpin, n(n(n(0)))).
nucleotide(a).
nucleotide(u).
nucleotide(u).

single(s(s(s(s(s(s(s(0))))))), s(s(s(0))),
[], bulge3, n(0)).
nucleotide(a).



u
u
u
c
u




g



u

0
s(0)
s(s(0))s(s(s(0)))
s(s(s(s(0))))
s(s(s(s(s(s(s(0)))))))
s(s(s(s(s(0))))
s(s(s(s(s(s(0))))))

root(0, root, [c]).

Figure 6: tree representation SECIS signal structure. (a) logical sequence,
i.e., sequence ground atoms representing SECIS signal structure.
ground atoms ordered clockwise starting root(0, root, [c]) lower
left-hand side corner. (b) tree formed secondary structure elements.

Tree Representation: tree representation (see Figure 6 (a)), idea capture
tree structure formed secondary structure elements, see Figure 6 (b).
training instance described sequence ground facts
root(0, root, #Children),
helical(ID, ParentID, #Children, Type, Size),
nucleotide pair(BasePair ),
single(ID, ParentID, #Children, Type, Size),
nucleotide(Base) .
Here, ID ParentID natural numbers 0, s(0), s(s(0)), . . . encoding childparent relation, #Children denotes number5 children [], [c], [c, c], . . ., Type
type structure element stem, hairpin, . . ., Size natural number
0, n(0), n(n(0)), . . . Atoms root(0, root, #Children) used root topology.
maximal #Children 9 maximal Size 13 maximal value
observed data.
trees easily handled using HMMs, used LOHMM basically
encodes PCFG. Due Theorem 2, possible. used LOHMM structure
found Appendix E. processes mRNA trees in-order. Unification used
parsing tree. chain representation, used Position argument hidden
states encode dynamics nucleotides (nucleotide pairs) within secondary structure
5. Here, use Prolog short hand notation [] lists. list either constant [] representing
empty list, compound term functor ./2 two arguments, respectively head
tail list. Thus [a, b, c] compound term .(a, .(b, .(c, []))).

443

fiKersting, De Raedt, & Raiko

elements. maximal Position 13. contrast chain representation,
nucleotide pairs (a, u) treated constants. Thus, argument BasePair
consists 16 elements.
Results: LOO test log-likelihood 55.56. Thus, exploiting tree structure
yields better probabilistic models. average, EM iteration took 14 seconds. Overall,
result shows (B4) holds.
Although Baum-Welch algorithm attempts maximize different objective function, namely likelihood data, interesting compare LOHMMs RIBL
terms classification accuracy.
Classification Accuracy: chain representation, LOO accuracies
LOHMMs 99% (92/93). considerable improvement RIBLs 77.2% (51/66)
LOO accuracy representation. tree representation, LOHMM also
achieved LOO accuracy 99% (92/93). comparable RIBLs LOO accuracy
97% (64/66) kind representation.
Thus, already chain LOHMMs show marked increases LOO accuracy compared RIBL (Horvath et al., 2001). order achieve similar LOO accuracies, Horvath
et al. (2001) make tree structure available RIBL background knowledge.
LOHMMs, significant influence LOO test log-likelihood,
LOO accuracies. clearly supports (B3). Moreover, according Horvath et al.,
mRNA application also considered success terms application domain,
although primary goal experiments. exist also alternative
parameter estimation techniques models, covariance models (Eddy &
Durbin, 1994) pair hidden Markov models (Sakakibara, 2003), might
used well basis comparison. However, LOHMMs employ (inductive) logic programming principles, appropriate compare systems within paradigm
RIBL.

7. Related Work
LOHMMs combine two different research directions. one hand, related
several extensions HMMs probabilistic grammars. hand, also
related recent interest combining inductive logic programming principles
probability theory (De Raedt & Kersting, 2003, 2004).
first type approaches, underlying idea upgrade HMMs probabilistic
grammars represent structured state spaces.
Hierarchical HMMs (Fine, Singer, & Tishby, 1998), factorial HMMs (Ghahramani &
Jordan, 1997), HMMs based tree automata (Frasconi, Soda, & Vullo, 2002) decompose state variables smaller units. hierarchical HMMs states
HMMs, factorial HMMs factored k state variables depend one
another observation, tree based HMMs represented probability
distributions defined tree structures. key difference LOHMMs
approaches employ logical concept unification. Unification essential
444

fiLogical Hidden Markov Models

allows us introduce abstract transitions, consist detailed
states. experimental evidence shows, sharing information among abstract states
means unification lead accurate model estimation. holds relational Markov models (RMMs) (Anderson, Domingos, & Weld, 2002) LOHMMs
closely related. RMMs, states different types, type described
different set variables. domain variable hierarchically structured.
main differences LOHMMs RMMs RMMs either support
variable binding unification hidden states.
equivalent HMMs context-free languages probabilistic context-free grammars (PCFGs). Like HMMs, consider sequences logical atoms
employ unification. Nevertheless, formal resemblance Baum-Welch
algorithms LOHMMs PCFGs. case LOHMM encodes PCFG
algorithms identical theoretical point view. re-estimate parameters
ratio expected number times transition (resp. production) used
expected number times transition (resp. production) might used. proof
Theorem 2 assumes PCFG given Greibach normal form6 (GNF) uses
pushdown automaton parse sentences. grammars GNF, pushdown automata
common parsing. contrast, actual computations Baum-Welch algorithm
PCFGs, called Inside-Outside algorithm (Baker, 1979; Lari & Young, 1990),
usually formulated grammars Chomsky normal form7 . Inside-Outside algorithm
make use efficient CYK algorithm (Hopcroft & Ullman, 1979) parsing strings.
alternative learning PCFGs strings learn structured data
skeletons, derivation trees nonterminal nodes removed (Levy &
Joshi, 1978). Skeletons exactly set trees accepted skeletal tree automata (STA).
Informally, STA, given tree input, processes tree bottom up, assigning
state node based states nodes children. STA accepts tree iff
assigns final state root tree. Due automata-based characterization
skeletons derivation trees, learning problem (P)CFGs reduced
problem STA. particular, STA techniques adapted learning tree
grammars (P)CFGs (Sakakibara, 1992; Sakakibara et al., 1994) efficiently.
PCFGs extended several ways. closely related LOHMMs
unification-based grammars extensively studied computational linguistics. Examples (stochastic) attribute-value grammars (Abney, 1997), probabilistic feature grammars (Goodman, 1997), head-driven phrase structure grammars (Pollard & Sag,
1994), lexical-functional grammars (Bresnan, 2001). learning within frameworks, methods undirected graphical models used; see work Johnson (2003)
description recent work. key difference LOHMMs nonterminals replaced structured, complex entities. Thus, observation sequences
flat symbols atoms modelled. Goodmans probabilistic feature grammars
exception. treat terminals nonterminals vectors features. abstraction
made, i.e., feature vectors ground instances, unification employed.
6. grammar GNF iff productions form aV variable, exactly one
terminal V string none variables.
7. grammar CNF iff every production form B, C A, B C variables,
terminal.

445

fiKersting, De Raedt, & Raiko

con

mkdir
con
mkdir

mv

ls

cd

mv
con

vt100x

vt100x

ls
new

vt100x

vt100x

vt100x

new

(a)

cd

vt100x

(b)

vt100x

vt100x

Figure 7: (a) atom logical sequence mkdir(vt100x), mv(new, vt100x),
ls(vt100x), cd(vt100x) forms tree. shaded nodes denote shared labels
among trees. (b) sequence represented single tree. predicate con/2 represents concatenation operator.

Therefore, number parameters needs estimated becomes easily large,
data sparsity serious problem. Goodman applied smoothing overcome problem.
LOHMMs generally related (stochastic) tree automata (see e.g., Carrasco, Oncina, Calera-Rubio, 2001). Reconsider Unix command sequence
mkdir(vt100x), mv(new, vt100x), ls(vt100x), cd(vt100x) . atom forms tree, see
Figure 7 (a), and, indeed, whole sequence atoms also forms (degenerated) tree,
see Figure 7 (b). Tree automata process single trees vertically, e.g., bottom-up. state
automaton assigned every node tree. state depends node label
states associated siblings node. focus sequential
domains. contrast, LOHMMs intended learning sequential domains.
process sequences trees horizontally, i.e., left right. Furthermore, unification
used share information consecutive sequence elements. Figure 7 (b)
illustrates, tree automata employ information allowing higher-order
transitions, i.e., states depend node labels states associated
predecessors 1, 2, . . . levels tree.
second type approaches, attention devoted developing highly
expressive formalisms, e.g. PCUP (Eisele, 1994), PCLP (Riezler, 1998), SLPs (Muggleton, 1996), PLPs (Ngo & Haddawy, 1997), RBNs (Jaeger, 1997), PRMs (Friedman,
Getoor, Koller, & Pfeffer, 1999), PRISM (Sato & Kameya, 2001), BLPs (Kersting & De
Raedt, 2001b, 2001a), DPRMs (Sanghai, Domingos, & Weld, 2003). LOHMMs
seen attempt towards downgrading highly expressive frameworks. Indeed, applying main idea underlying LOHMMs non-regular probabilistic grammar, i.e., replacing
flat symbols atoms, yields principle stochastic logic programs (Muggleton, 1996).
consequence, LOHMMs represent interesting position expressiveness scale.
Whereas retain essential logical features expressive formalisms,
seem easier understand, adapt learn. akin many contemporary consid446

fiLogical Hidden Markov Models

erations inductive logic programming (Muggleton & De Raedt, 1994) multi-relational
data mining (Dzeroski & Lavrac, 2001).

8. Conclusions
Logical hidden Markov models, new formalism representing probability distributions
sequences logical atoms, introduced solutions three central
inference problems (evaluation, likely state sequence parameter estimation)
provided. Experiments demonstrated unification improve generalization
accuracy, number parameters LOHMM order magnitude smaller
number parameters corresponding HMM, solutions presented
perform well practice also LOHMMs possess several advantages traditional
HMMs applications involving structured sequences.
Acknowledgments authors thank Andreas Karwath Johannes Horstmann
interesting collaborations protein data; Ingo Thon interesting collaboration
analyzing Unix command sequences; Saul Greenberg providing Unix command sequence data. authors would also like thank anonymous reviewers comments considerably improved paper. research partly supported
European Union IST programme contract numbers IST-2001-33053 FP6-508861
(Application Probabilistic Inductive Logic Programming (APrIL) II). Tapani Raiko
supported Marie Curie fellowship DAISY, HPMT-CT-2001-00251.

Appendix A. Proof Theorem 1
Let = (, , , ) LOHMM. show specifies time discrete stochastic
process, i.e., sequence random variables hXt it=1,2,... , domains random
variable Xt hb(), Herbrand base , define immediate state operator
TM -operator current emission operator EM -operator.
Definition 4 (TM -Operator, EM -Operator ) operators TM : 2hb 2hb EM :
2hb 2hb


TM (I) = {HB H | (p : H
B) : BB I, HB H G (H)}


EM (I) = {OB H | (p : H
B) : BB I, HB G G (H)
OB H G (O)}
i+1
({start}))
= 1, 2, 3, . . ., set TM
({start}) := TM (TM
1
TM ({start}) := TM ({start}) specifies state set clock forms random varii ({start}) specifies possible symbols emitted transitioning
able Yi . set UM
+ 1. forms variable Ui . Yi (resp. Ui ) extended random
variable Zi (resp. Ui ) hb :

P (Zi = z) =



({start})
0.0 : z 6 TM
P (Yi = z) : otherwise

447

fiKersting, De Raedt, & Raiko

PSfrag replacements

Z1

Z2

Z3

U1

U2

...

U3

Figure 8: Discrete time stochastic process induced LOHMM. nodes Z Ui
represent random variables hb .

Figure 8 depicts influence relation among Zi Ui . Using standard arguments
probability theory noting
P (Ui = Ui | Zi+1 = zi+1 , Zi = zi ) =
P (Zi+1 | Zi ) =

X

P (Zi+1 , ui | Zi )

P (Zi+1 = zi+1 , Ui = ui | Zi )
P
ui P (Zi+1 , ui | Zi )

ui

probability distributions due equation (1), easy show Kolmogorovs extension theorem (see Bauer, 1991; Fristedt
Gray, 1997) holds. Thus,
Nt
specifies unique probability distribution
(Z
Ui ) > 0

i=1
limit .


Appendix B. Moore Representations LOHMMs
HMMs, Moore representations, i.e., output symbols associated states Mealy
representations, i.e., output symbols associated transitions, equivalent.
appendix, investigate extend also holds LOHMMs.
Let L Mealy-LOHMM according definition 3. following, derive
notation equivalent LOHMM L0 Moore representation abstract
transitions abstract emissions (see below). predicate b/n L extended b/n+
1 L0 . domains first n arguments b/n. last argument
store observation emitted. precisely, abstract transition
o(v1 ,...,vk )

p : h(w1 , . . . , wl ) b(u1 , . . . , un )
L, abstract transition
p : h(w1 , . . . , wl , o(v01 , . . . , v0k )) b(u1 , . . . , un , )
L0 . primes o(v01 , . . . , v0k ) denote replaced free 8 variables o(v1 , . . . , vk )
distinguished constant symbol, say #. Due this, holds
(h(w1 , . . . , wl )) = (h(w1 , . . . , wl , o(v01 , . . . , v0k ))) ,
8. variable X vars(o(v1 , . . . , vk )) free iff X 6 vars(h(w1 , . . . , wl )) vars(b(u1 , . . . , un )).

448

(6)

fiLogical Hidden Markov Models

L0 output distribution specified using abstract emissions expressions
form
1.0 : o(v1 , . . . , vk ) h(w1 , . . . , wl , o(v01 , . . . , v0k )) .
(7)
semantics abstract transition L0
state S0t G0 (b(u1 , . . . , un , )) system make transition
S0t+1 G0 (h(w1 , . . . , wl , o(v01 , . . . , v0k ))) probability
p (S0t+1 | h(w1 , . . . , wl , o(v01 , . . . , v0k )) | S0t )


state

(8)

S0t = mgu(S0t , b(u1 , . . . , un , )). Due Equation (6), Equation (8) rewritten

p (S0t+1 | h(w1 , . . . , wl ) | S0t ) .
Due equation (7), system emit output symbol ot+1 G0 (o(v1 , . . . , vk ))
state S0t+1 probability
(ot+1 | o(v1 , . . . , vk )S0t+1 S0t )
S0t+1 = mgu(h(w1 , . . . , wl , o(v01 , . . . , v0k )), S0t+1 ). Due construction L0 ,
exists triple (St , St+1 , Ot+1 ) L triple (S0t , S0t+1 , Ot+1 ), > 0, L0 (and vise
versa). Hence,both LOHMMs assign overall transition probability.
L L0 differ way initialize sequences h(S0t , S0t+1 , Ot+1 it=0,2...,T (resp.
h(St , St+1 , Ot+1 it=0,2...,T ). Whereas L starts state S0 makes transition S1
emitting O1 , Moore-LOHMM L0 supposed emit symbol O0 S00 making
transition S01 . compensate using prior distribution. existence
correct prior distribution L0 seen follows. L, finitely many
states reachable time = 1, i.e, PL (q0 = S) > 0 holds finite set ground
states S. probability PL (q0 = s) computed similar 1 (S). set = 1 line
6, neglecting condition Ot1 line 10, dropping (Ot1 | OB H ) line 14.
Completely listing states S1 together PL (q0 = S), i.e., PL (q0 = S) : start ,
constitutes prior distribution L0 .
argumentation basically followed approach transform Mealy machine
Moore machine (see e.g., Hopcroft Ullman, 1979). Furthermore, mapping
Moore-LOHMM introduced present section Mealy-LOHMM straightforward.

Appendix C. Proof Theorem 2
Let terminal alphabet N nonterminal alphabet. probabilistic context-free
grammar (PCFG) G consists distinguished start symbol N plus finite set

productions
P form p : X , X N , (N ) p [0, 1].
X N , :X p = 1. PCFG defines stochastic process sentential forms states,
leftmost rewriting steps transitions. denote single rewriting operation
grammar single arrow . result one ore rewriting operations
able rewrite (N ) sequence (N ) nonterminals terminals,
write . probability rewriting product probability
449

fiKersting, De Raedt, & Raiko

values associated productions used derivation. assume G consistent, i.e.,
sum probabilities derivations sum 1.0.
assume PCFG G Greibach normal form. follows Abney
et al.s (1999) Theorem 6 G consistent. Thus, every production P G
form p : X aY1 . . . Yn n 0. order encode G LOHMM ,
introduce (1) non-terminal symbol X G constant symbol nX (2)
terminal symbol G constant symbol t. production P G, include

abstract transition form p : stack([nY1 , . . . , nYn |S])
stack([nX|S]), n > 0,

p : stack(S)
stack([nX|S]), n = 0. Furthermore, include 1.0 : stack([s]) start
end
1.0 : end stack([]). straightforward prove induction G
equivalent.


Appendix D. Logical Hidden Markov Model Unix Command
Sequences
LOHMMs described model Unix command sequences triggered mkdir.
aim, transformed original Greenberg data sequence logical atoms
com, mkdir(Dir, LastCom), ls(Dir, LastCom), cd(Dir, Dir, LastCom), cp(Dir, Dir, LastCom)
mv(Dir, Dir, LastCom). domain LastCom {start, com, mkdir, ls, cd, cp, mv}.
domain Dir consisted argument entries mkdir, ls, cd, cp, mv original
dataset. Switches, pipes, etc. neglected, paths made absolute. yields
212 constants domain Dir. original commands, mkdir, ls, cd,
cp, mv, represented com. mkdir appear within 10 time steps
command C {ls, cd, cp,mv}, C represented com. Overall, yields
451000 ground states covered Markov model.
unification LOHMM U basically implements second order Markov model, i.e.,
probability making transition depends upon current state previous
state. 542 parameters following structure:
com start.
mkdir(Dir, start) start.

com com.
mkdir(Dir, com) com.
end com.

Furthermore, C {start, com}
mkdir(Dir, com)
mkdir( , com)
com
end
ls(Dir, mkdir)
ls( , mkdir)
cd(Dir, mkdir)









mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).

cd( , mkdir)
cp( , Dir, mkdir)
cp(Dir, , mkdir)
cp( , , mkdir)
mv( , Dir, mkdir)
mv(Dir, , mkdir)
mv( , , mkdir)

450









mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).

fiLogical Hidden Markov Models

together C {mkdir, ls, cd, cp, mv} C 1 {cd, ls} (resp.
C2 {cp, mv})
mkdir(Dir, com)
mkdir( , com)
com
end
ls(Dir,C1 )
ls( ,C1 )
cd(Dir,C1 )
cd( ,C1 )
cp( , Dir,C1 )
cp(Dir, ,C1 )
cp( , ,C1 )
mv( , Dir,C1 )
mv(Dir, ,C1 )
mv( , ,C1 )
















C1 (Dir,C). mkdir( , com)
com
C1 (Dir,C).
C1 (Dir,C).
end
ls(From,C2 )
C1 (Dir,C).
ls(To,C2 )
C1 (Dir,C).
C1 (Dir,C).
ls( ,C2 )
C1 (Dir,C).
cd(From,C2 )
C1 (Dir,C).
cd(To,C2 )
C1 (Dir,C).
cd( ,C2 )
C1 (Dir,C). cp(From, ,C2 )
C1 (Dir,C).
cp( , To,C2 )
C1 (Dir,C).
cp( , ,C2 )
C1 (Dir,C). mv(From, ,C2 )
mv( , To,C2 )
C1 (Dir,C).
mv( , ,C2 )

C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).

states fully observable, omitted output symbols associated
clauses, and, sake simplicity, omitted associated probability values.
unification LOHMM N variant U variables shared

mkdir( , com) cp(From, To,C).
ls(
com cp(From, To,C).
cd(
end cp(From, To,C). cp( ,
mv( ,

, cp)
, cp)
, cp)
, cp)






cp(From, To,C).
cp(From, To,C).
cp(From, To,C).
cp(From, To,C).

transitions affected, N 164 parameters less U , i.e., 378.

Appendix E. Tree-based LOHMM mRNA Sequences
LOHMM processes nodes mRNA trees in-order. structure LOHMM
shown end section. copies shaded parts. Terms
abbreviated using starting alphanumerical; tr stands tree, helical, si
single, nuc nucleotide, nuc p nucleotide pair.
domain #Children covers maximal branching factor found data, i.e.,
{[c], [c, c], . . . , [c, c, c, c, c, c, c, c, c]}; domain Type consists types occurring
data, i.e., {stem, single, bulge3, bulge5, hairpin}; Size, domain covers
maximal length secondary structure element data, i.e., longest sequence
consecutive bases respectively base pairs constituting secondary structure element.
length encoded {n1 (0), n2 (0), . . . , n13 (0)} nm (0) denotes recursive
application functor n times. Base BasePair , domains 4 bases
respectively 16 base pairs. total, 491 parameters.
451

fimy start
1.0 : root(0, root, X)

Copies tr(Id, [c], [Pa [C]|R]), tr(Id, [c, c], [Pa [C]|R]),
tr(Id, [c, c, c], [Pa [C]|R])
0.25 : he(s(Id), Pa, [], T, L)
tr(Id, , [Pa [C]|R])

tr(0, X, [0 X])

tr(Id, [c, c, c], [Pa [C1, C2|Cs]|R])

0.25 : he(s(Id), Pa, B, T, L)

0.25 : he(s(Id), Pa, [], T, L)
0.25 : he(s(Id), Pa, B, T, L)
tr(Id, , [Pa [C1, C2|Cs]|R])
0.25 : si(s(Id), Pa, B, T, L)
0.25 : si(s(Id), Pa, [], T, L)

0.25 : si(s(Id), Pa, B, T, L)
0.25 : si(s(Id), Pa, [], T, L)
se(T, L, s(Id), B, [s(Id) B|R])

se(T, L, s(Id), [], R)

Copies tr(Id, [c], [Pa [C1, C2|Cs]|R]), tr(Id, [c, c], [Pa [C1, C2|Cs]|R]),

tree
model

se(T, L, s(Id), [], [Pa [C2|Cs]|R]) se(T, L, s(Id), B, [s(Id) B, Pa [C2|Cs]|R])

Copies type single, bulge3, bulge5
Copies n(n(0)) n(n(n(0)))

Copies length sequence n(n(0)), n(n(n(0))), n(n(n(n(0))))
se(stem, n(A), Id, B, S)

se(hairpin, n(A), Id, B, S)

Copies nuc p(a, g), . . . , nuc p(u, u)

Copies nuc(g), nuc(c), nuc(u)
0.25 : nuc(a)

0.0625 : nuc p(a, a)

se(hairpin, A, Id, B, S)

se(hairpin, n(0), Id, B, S)

se(stem, A, Id, B, S)

se(hairpin, n(0), s( ), , [])

0.25 : nuc(a)

se(stem, n(0), s( ), , [])

0.0625 : nuc p(a, a)

0.25 : nuc(a)

se(stem, n(0), Id, B, S)

0.0625 : nuc p(a, a)

Copies nuc p(a, g), . . . , nuc p(u, u)

Copies nuc(g), nuc(c), nuc(u)
end

tr(Id, B, S)

sequence
model

Kersting, De Raedt, & Raiko

Figure 9: mRNA LOHMM structure. symbol denotes anonymous variables
read treated distinct, new variables time encountered.
copies shaded part. Terms abbreviated using starting
alphanumerical; tr stands tree, se structure element, helical,
si single, nuc nucleotide, nuc p nucleotide pair.

References

452

Abney, S. (1997). Stochastic Attribute-Value Grammars. Computational Linguistics, 23 (4),
597618.

1.0

start

fiLogical Hidden Markov Models

Abney, S., McAllester, D., & Pereira, F. (1999). Relating probabilistic grammars automata. Proceedings 37th Annual Meeting Association Computational
Linguistics (ACL-1999), pp. 542549. Morgan Kaufmann.
Anderson, C., Domingos, P., & Weld, D. (2002). Relational Markov Models Application Adaptive Web Navigation. Proceedings Eighth International
Conference Knowledge Discovery Data Mining (KDD-2002), pp. 143152 Edmonton, Canada. ACM Press.
Baker, J. (1979). Trainable grammars speech recognition. Speech communication
paper presented th 97th Meeting Acoustical Society America, pp. 547550
Boston, MA.
Bauer, H. (1991). Wahrscheinlichkeitstheorie (4. edition). Walter de Gruyter, Berlin, New
York.
Baum, L. (1972). inequality associated maximization technique statistical estimation probabilistic functions markov processes. Inequalities, 3, 18.
Bohnebeck, U., Horvath, T., & Wrobel, S. (1998). Term comparison first-order similarity
measures. Proceedings Eigth International Conference Inductive Logic
Programming (ILP-98), Vol. 1446 LNCS, pp. 6579. Springer.
Bresnan, J. (2001). Lexical-Functional Syntax. Blackwell, Malden, MA.
Carrasco, R., Oncina, J., & Calera-Rubio, J. (2001). Stochastic inference regular tree
languages. Machine Learning, 44 (1/2), 185197.
Chandonia, J., Hon, G., Walker, N., Lo Conte, L., P.Koehl, & Brenner, S. (2004).
ASTRAL compendium 2004. Nucleic Acids Research, 32, D189D192.
Davison, B., & Hirsh, H. (1998). Predicting Sequences User Actions. Predicting
Future: AI Approaches Time-Series Analysis, pp. 512. AAAI Press.
De Raedt, L., & Kersting, K. (2003). Probabilistic Logic Learning. ACM-SIGKDD Explorations: Special issue Multi-Relational Data Mining, 5 (1), 3148.
De Raedt, L., & Kersting, K. (2004). Probabilistic Inductive Logic Programming.
Ben-David, S., Case, J., & Maruoka, A. (Eds.), Proceedings 15th International
Conference Algorithmic Learning Theory (ALT-2004), Vol. 3244 LNCS, pp.
1936 Padova, Italy. Springer.
Durbin, R., Eddy, S., Krogh, A., & Mitchison, G. (1998). Biological sequence analysis:
Probabilistic models proteins nucleic acids. Cambridge University Press.
Dzeroski, S., & Lavrac, N. (Eds.). (2001). Relational data mining. Springer-Verlag, Berlin.
Eddy, S., & Durbin, R. (1994). RNA sequence analysis using covariance models. Nucleic
Acids Res., 22 (11), 20792088.
453

fiKersting, De Raedt, & Raiko

Eisele, A. (1994). Towards probabilistic extensions contraint-based grammars.
Dorne, J. (Ed.), Computational Aspects Constraint-Based Linguistics Decription-II.
DYNA-2 deliverable R1.2.B.
Fine, S., Singer, Y., & Tishby, N. (1998). hierarchical hidden markov model: analysis
applications. Machine Learning, 32, 4162.
Frasconi, P., Soda, G., & Vullo, A. (2002). Hidden markov models text categorization
multi-page documents. Journal Intelligent Information Systems, 18, 195217.
Friedman, N., Getoor, L., Koller, D., & Pfeffer, A. (1999). Learning probabilistic relational
models. Proceedings Sixteenth International Joint Conference Artificial Intelligence (IJCAI-1999), pp. 13001307. Morgan Kaufmann.
Fristedt, B., & Gray, L. (1997). Modern Approach Probability Theory. Probability
applications. Birkhauser Boston.
Ghahramani, Z., & Jordan, M. (1997). Factorial hidden Markov models. Machine Learning,
29, 245273.
Goodman, J. (1997). Probabilistic feature grammars. Proceedings Fifth International Workshop Parsing Technologies (IWPT-97) Boston, MA, USA.
Greenberg, S. (1988). Using Unix: collected traces 168 users. Tech. rep., Dept.
Computer Science, University Calgary, Alberta.
Hopcroft, J., & Ullman, J. (1979). Introduction Automata Theory, Languages,
Computation. Addison-Wesley Publishing Company.
Horvath, T., Wrobel, S., & Bohnebeck, U. (2001). Relational Instance-Based learning
Lists Terms. Machine Learning, 43 (1/2), 5380.
Hubbard, T., Murzin, A., Brenner, S., & Chotia, C. (1997). SCOP : structural classification
proteins database. NAR, 27 (1), 236239.
Jacobs, N., & Blockeel, H. (2001). Learning Shell: Automated Macro Construction.
User Modeling 2001, pp. 3443.
Jaeger, M. (1997). Relational Bayesian networks. Proceedings Thirteenth Conference Uncertainty Artificial Intelligence (UAI), pp. 266273. Morgan Kaufmann.
Katz, S. (1987). Estimation probabilities sparse data hte language model component speech recognizer. IEEE Transactions Acoustics, Speech, Signal
Processing (ASSP), 35, 400401.
Kersting, K., & De Raedt, L. (2001a). Adaptive Bayesian Logic Programs. Rouveirol,
C., & Sebag, M. (Eds.), Proceedings 11th International Conference Inductive
Logic Programming (ILP-01), Vol. 2157 LNAI, pp. 118131. Springer.
454

fiLogical Hidden Markov Models

Kersting, K., & De Raedt, L. (2001b). Towards Combining Inductive Logic Programming
Bayesian Networks. Rouveirol, C., & Sebag, M. (Eds.), Proceedings
11th International Conference Inductive Logic Programming (ILP-01), Vol. 2157
LNAI, pp. 118131. Springer.
Kersting, K., & Raiko, T. (2005). Say EM Selecting Probabilistic Models Logical
Sequences. Bacchus, F., & Jaakkola, T. (Eds.), Proceedings 21st Conference
Uncertainty Artificial Intelligence, UAI 2005, pp. 300307 Edinburgh, Scotland.
Kersting, K., Raiko, T., Kramer, S., & De Raedt, L. (2003). Towards discovering structural signatures protein folds based logical hidden markov models. Altman,
R., Dunker, A., Hunter, L., Jung, T., & Klein, T. (Eds.), Proceedings Pacific Symposium Biocomputing (PSB-03), pp. 192203 Kauai, Hawaii, USA. World
Scientific.
Koivisto, M., Kivioja, T., Mannila, H., Rastas, P., & Ukkonen, E. (2004). Hidden Markov
Modelling Techniques Haplotype Analysis. Ben-David, S., Case, J., & Maruoka,
A. (Eds.), Proceedings 15th International Conference Algorithmic Learning Theory (ALT-04), Vol. 3244 LNCS, pp. 3752. Springer.
Koivisto, M., Perola, M., Varilo, T., Hennah, W., Ekelund, J., Lukk, M., Peltonen, L.,
Ukkonen, E., & Mannila, H. (2002). MDL method finding haplotype blocks
estimating strength haplotype block boundaries. Altman, R., Dunker,
A., Hunter, L., Jung, T., & Klein, T. (Eds.), Proceedings Pacific Symposium
Biocomputing (PSB-02), pp. 502513. World Scientific.
Korvemaker, B., & Greiner, R. (2000). Predicting UNIX command files: Adjusting user
patterns. Adaptive User Interfaces: Papers 2000 AAAI Spring Symposium,
pp. 5964.
Kulp, D., Haussler, D., Reese, M., & Eeckman, F. (1996). Generalized Hidden Markov
Model Recognition Human Genes DNA. States, D., Agarwal, P.,
Gaasterland, T., Hunter, L., & Smith, R. (Eds.), Proceedings Fourth International Conference Intelligent Systems Molecular Biology,(ISMB-96), pp. 134
142 St. Louis, MO, USA. AAAI.
Lane, T. (1999). Hidden Markov Models Human/Computer Interface Modeling.
Rudstrom, A. (Ed.), Proceedings IJCAI-99 Workshop Learning Users,
pp. 3544 Stockholm, Sweden.
Lari, K., & Young, S. (1990). estimation stochastic context-free grammars using
inside-outside algorithm. Computer Speech Language, 4, 3556.
Levy, L., & Joshi, A. (1978). Skeletal structural descriptions. Information Control,
2 (2), 192211.
McLachlan, G., & Krishnan, T. (1997). EM Algorithm Extensions. Wiley, New
York.
455

fiKersting, De Raedt, & Raiko

Mitchell, T. M. (1997). Machine Learning. McGraw-Hill Companies, Inc.
Muggleton, S. (1996). Stochastic logic programs. De Raedt, L. (Ed.), Advances
Inductive Logic Programming, pp. 254264. IOS Press.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods.
Journal Logic Programming, 19 (20), 629679.
Ngo, L., & Haddawy, P. (1997). Answering queries context-sensitive probabilistic
knowledge bases. Theoretical Computer Science, 171, 147177.
Pollard, C., & Sag, I. (1994). Head-driven Phrase Structure Grammar. University
Chicago Press, Chicago.
Rabiner, L., & Juang, B. (1986). Introduction Hidden Markov Models. IEEE ASSP
Magazine, 3 (1), 416.
Riezler, S. (1998). Statistical inference probabilistic modelling constraint-based
nlp. Schrder, B., Lenders, W., & und T. Portele, W. H. (Eds.), Proceedings
4th Conference Natural Language Processing (KONVENS-98). Also CoRR
cs.CL/9905010.
Sakakibara, Y. (1992). Efficient learning context-free grammars positive structural
examples. Information Computation, 97 (1), 2360.
Sakakibara, Y. (2003). Pair hidden markov models tree structures. Bioinformatics,
19 (Suppl.1), i232i240.
Sakakibara, Y., Brown, M., Hughey, R., Mian, I., Sjolander, K., & Underwood, R. (1994).
Stochastic context-free grammars tRNA modelling. Nucleic Acids Research,
22 (23), 51125120.
Sanghai, S., Domingos, P., & Weld, D. (2003). Dynamic probabilistic relational models.
Gottlob, G., & Walsh, T. (Eds.), Proceedings Eighteenth International Joint
Conference Artificial Intelligence (IJCAI-03), pp. 992997 Acapulco, Mexico. Morgan Kaufmann.
Sato, T., & Kameya, Y. (2001). Parameter learning logic programs symbolic-statistical
modeling. Journal Artificial Intelligence Research (JAIR), 15, 391454.
Scholkopf, B., & Warmuth, M. (Eds.). (2003). Learning Parsing Stochastic UnificationBased Grammars, Vol. 2777 LNCS. Springer.
Turcotte, M., Muggleton, S., & Sternberg, M. (2001). effect relational background
knowledge learning protein three-dimensional fold signatures. Machine Learning,
43 (1/2), 8195.
Won, K., Prugel-Bennett, A., & Krogh, A. (2004). Block Hidden Markov Model Biological Sequence Analysis. Negoita, M., Howlett, R., & Jain, L. (Eds.), Proceedings
Eighth International Conference Knowledge-Based Intelligent Information
Engineering Systems (KES-04), Vol. 3213 LNCS, pp. 6470. Springer.

456

fiJournal Artificial Intelligence Research 25 (2006) 269-314

Submitted 5/05; published 2/06

Distributed Reasoning Peer-to-Peer Setting:
Application Semantic Web
Philippe Adjiman
Philippe Chatalic
Francois Goasdoue
Marie-Christine Rousset
Laurent Simon

adjiman@lri.fr
chatalic@lri.fr
fg@lri.fr
mcr@lri.fr
simon@lri.fr

LRI-PCRI, Batiment 490
CNRS & Universite Paris-Sud XI INRIA Futurs
91405 Orsay Cedex, France

Abstract
peer-to-peer inference system, peer reason locally also solicit
acquaintances, peers sharing part vocabulary. paper,
consider peer-to-peer inference systems local theory peer set
propositional clauses defined upon local vocabulary. important characteristic peerto-peer inference systems global theory (the union peer theories)
known (as opposed partition-based reasoning systems). main contribution
paper provide first consequence finding algorithm peer-to-peer setting: DeCA.
anytime computes consequences gradually solicited peer peers
distant. exhibit sufficient condition acquaintance graph
peer-to-peer inference system guaranteeing completeness algorithm. Another
important contribution apply general distributed reasoning setting setting
Semantic Web Somewhere semantic peer-to-peer data management
system. last contribution paper provide experimental analysis
scalability peer-to-peer infrastructure propose, large networks 1000
peers.

1. Introduction
Recently peer-to-peer systems received considerable attention underlying infrastructure appropriate scalable flexible distributed applications
Internet. peer-to-peer system, centralized control hierarchical organization: peer equivalent functionality cooperates peers order
solve collective task. Peer-to-peer systems evolved simple keyword-based
peer-to-peer file sharing systems like Napster (http://www.napster.com) Gnutella
(http://gnutella.wego.com) semantic peer-to-peer data management systems like
Edutella (Nejdl, Wolf, Qu, Decker, Sintek, & al., 2002) Piazza (Halevy, Ives, Tatarinov, & Mork, 2003a), handle semantic data description support complex queries
data retrieval. systems, complexity answering queries directly related
expressivity formalism used state semantic mappings peers
schemas (Halevy, Ives, Suciu, & Tatarinov, 2003b).
c
!2006
AI Access Foundation. rights reserved.

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

paper, interested peer-to-peer inference systems peer
answer queries reasoning local (propositional) theory also ask queries
peers semantically related sharing part vocabulary.
framework encompasses several applications like peer-to-peer information integration
systems intelligent agents, peer knowledge (about data
expertise domain) partial knowledge peers. setting,
solicited perform reasoning task, peer, cannot solve completely task locally,
must able distribute appropriate reasoning subtasks among acquainted peers.
leads step step splitting initial task among peers relevant
solve parts it. outputs different splitted tasks must recomposed
construct outputs initial task.
consider peer-to-peer inference systems local theory peer
composed set propositional clauses defined upon set propositional variables
(called local vocabulary). peer may share part vocabulary
peers. investigate reasoning task finding consequences certain form (e.g.,
clauses involving certain variables) given input formula expressed using local
vocabulary peer. Note reasoning tasks like finding implicants certain
form given input formula equivalently reduced consequence finding task.
important emphasize problem distributed reasoning consider
paper quite different problem reasoning partitions obtained
decomposition theory (Dechter & Rish, 1994; Amir & McIlraith, 2000).
problem, centralized large theory given structure exploited compute
best partitioning order optimize use partition-based reasoning algorithm.
problem, whole theory (i.e., union local theories) known
partition imposed peer-to-peer architecture. illustrate example
(Section 2), algorithms based transmitting clauses partitions spirit
work Amir McIlraith (2000), Dechter Rish (1994) del Val (1999)
appropriate consequence finding problem. algorithm splits clauses
share variables several peers. piece splitted clause transmitted
corresponding theory find consequences. consequences found
piece splitted clause must recomposed get consequences clause
splitted.
main contribution paper provide first consequence finding algorithm
peer-to-peer setting: DeCA. anytime computes consequences gradually
solicited peer peers distant. exhibit sufficient condition acquaintance graph peer-to-peer inference system guaranteeing
completeness algorithm.
Another important contribution apply general distributed reasoning setting
setting Semantic Web Somewhere semantic peer-to-peer data
management system. Somewhere based simple data model made taxonomies
atomic classes mappings classes different taxonomies, think
appropriate common semantic support needed future semantic web
applications. Somewhere data model encoded propositional logic
query answering Somewhere equivalently reduced distributed reasoning
logical propositional theories.
270

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

last contribution paper provide experimental analysis scalability approach, deployed large networks. far, scalability system
like Piazza goes 80 peers. Piazza uses expressive language
one used approach, results rely wide range optimizations (mappings
composition, paths pruning Tatarinov & Halevy, 2004) made possible centralized
storage schemas mappings global server. contrast, stuck
fully decentralized approach performed experiments networks 1000 peers.
important point characterizing peer-to-peer systems dynamicity: peers
join leave system moment. Therefore, feasible bring
information single server order reason locally using standard reasoning
algorithms. would costly gather data available system
would useless task changing peers connected network.
dynamicity peer-to-peer inference systems imposes revisit reasoning problem
order address completely decentralized manner.
paper organized follows. Section 2 defines formally peer-to-peer inference
problem address paper. Section 3, describe distributed consequence
finding algorithm state properties. describe Somewhere Section 4.
Section 5 reports experimental study scalability peer-to-peer infrastructure.
Related work summarized Section 6. conclude short discussion Section 7.

2. Consequence Finding Peer-to-peer Inference Systems
peer-to-peer inference system (P2PIS) network peer theories. peer P
finite set propositional formulas language LP . consider case LP
language clauses without duplicated literals built finite set
propositional variables VP , called vocabulary P . Peers semantically related
sharing variables peers. shared variable two peers intersection
vocabularies two peers. impose however variables
common vocabularies two peers shared two peers: two peers may
aware variables common them.
P2PIS, peer knowledge global P2PIS theory. peer knows
local theory variables shares peers P2PIS (its
acquaintances). necessarily knows variables common
peers (including acquaintances). new peer joins P2PIS simply
declares acquaintances P2PIS, i.e., peers knows sharing variables with,
declares corresponding shared variables.
2.1 Syntax Semantics
P2PIS formalized using notion acquaintance graph following
consider P2PIS acquaintance graphs equivalent.
271

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

Definition 1 (Acquaintance graph) Let P = {Pi }i=1..n collection clausal theories respective vocabularies VPi , let V = i=1..n VPi . acquaintance graph V
graph = (P, acq) P set vertices acq V P P set
labelled edges every (v, Pi , Pj ) acq, %= j v VPi VPj .
labelled edge (v, Pi , Pj ) expresses peers Pi Pj know sharing
variable v. peer P literal l, acq(l, P ) denotes set peers sharing
P variable l.
contrast approaches (Ghidini & Serafini, 2000; Calvanese, De Giacomo,
Lenzerini, & Rosati, 2004), adopt epistemic modal semantics interpreting
P2PIS interpret standard semantics propositional logic.
Definition 2 (Semantics P2PIS) Let = (P, acq) P2PIS P = {Pi }i=1..n ,
!
interpretation P assignement variables i=1..n Pi true
f alse. particular, variable common two theories Pi Pj given
P2PIS interpreted value two theories.
model clause c iff one literals c evaluated true I.
model set clauses (i.e., local theory, union local theories,
whole P2PIS) iff model clauses set.
P2PIS satisfiable iff model.
consequence relation P2PIS standard consequence relation |=: given
P2PIS P, clause c, P |= c iff every model P model c.
2.2 Consequence Finding Problem
theory P , consider subset target variables V P VP , supposed represent
variables interest application, (e.g., observable facts model-based diagnosis
application, classes storing data information integration application). goal is,
given clause provided input given peer, find possible consequences
belonging target language input clause union peer theories.
point input clause uses vocabulary queried peer,
expected consequences may involve target variables different peers. target
languages handled algorithm defined terms target variables require
shared variable target status peers sharing it. worth noting
requirement local property: peers sharing variables given peer
acquaintances and, definition, direct neighbours acquaintance
graph.
Definition 3 (Target Language) Let = (P, acq) P2PIS, every peer P ,
let V P set target variables (v, Pi , Pj ) acq v V Pi iff
)
v V Pj . set SP peers P, define target language arget(SP
!
language clauses (including empty clause) involving variables P SP V P .
272

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

reasoning problem interested compute logical consequences
input clause given P2PIS. corresponds notion proper prime implicates
clause w.r.t. clausal (distributed) theory, formally defined Definition 4.
Definition 4 (Proper prime implicate clause w.r.t. clausal theory) Let P
clausal theory q clause. clause said be:
implicate q w.r.t. P iff P {q} |= m.
prime implicate q w.r.t. P iff implicate q w.r.t. P ,
clause m# implicate q w.r.t. P , m# |= m# m.
proper prime implicate q w.r.t. P iff prime implicate q w.r.t. P
P %|= m.
problem finding prime implicates new clause theory, a.k.a. -prime
implicates, corresponds exactly problem computing proper prime implicates
clause w.r.t. clausal theory. extensively studied centralized case
(see work Marquis, 2000, survey). Note deciding whether clause
-prime implicate clausal theory BH2 -complete (Marquis, 2000), i.e., N P
coN P . problem address may viewed refinement, restricting
computation proper prime implicates given target language. corresponds
(L, )-prime implicates work Marquis (2000) complexity.
Definition 5 (The consequence finding problem P2PIS) Let = (P, acq)
P 2P IS, P = {Pi }i=1..n collection clausal theories respective target
variables. consequence finding problem is, given peer P , acquaintances
,
!
clause q LP , find set proper prime implicates q w.r.t. i=1..n Pi
belong arget(P).
algorithmic point view, consequence finding problem P2PIS new
significantly different consequence finding problem single global theory.
According semantics, order complete, peer-to-peer consequence finding
algorithm must obtain results standard consequence finding algorithm
applied union local theories, without global input:
partial local input made theory single peer acquaintances.
reasoning must distributed appropriately among different theories without
global view whole P2PIS. full peer-to-peer setting, consequence finding
algorithm cannot centralized (because would mean super-peer controlling
reasoning). Therefore, must design algorithm running independently peer
possibly distributing part reasoning controls acquainted peers: peer
control whole reasoning.
Among possible consequences distinguish local consequences, involving target
variables solicited peer, remote consequences, involve target variables
single peer distant solicited peer, combined consequences involve target
variables several peers.
273

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

2.3 Example
following example illustrates main characteristics message-based distributed
algorithm running peer, presented detail Section 3.
Let us consider 4 interacting peers. P1 describes tour operator. theory expresses
current F ar destinations either Chile Kenya. far destinations
international destinations (Int) expensive (Exp). peer P2 concerned
police regulations expresses passport required (P ass) international destinations. P3 focuses sanitary conditions travelers. expresses that, Kenya,
yellow fever vaccination (Y ellowF ev) strongly recommended strong protection paludism taken (P alu) accomodation occurs Lodges. P4
describes travel accommodation conditions : Lodge Kenya Hotel Chile. also
expresses anti-paludism protection required, accommodations equipped
appropriate anti-mosquito protections (AntiM ). respective theories peer
described Figure 1 nodes acquaintance graph. Shared variables mentioned edge labels. Target variables defined : V P1 = {Exp}, V P2 = {P ass},
V P3 = {Lodge, ellowF ev, P alu} V P4 = {Lodge, Hotel, P alu, AntiM }.
P1 :
Far Exp
Far Chile Kenya
Int

P2 :
Int Pass

Far Int

Kenya

Kenya,Chile

P3 :
Kenya YellowFev
Lodge Kenya Palu

P4 :
Kenya Lodge
Chile Hotel
Palu AntiM

Lodge,Palu

Figure 1: Acquaintance graph tour operator example
illustrate behavior algorithm input clause Far provided
peer P1 user. Describing precisely behavior distributed algorithm
network peers easy. following present propagation reasoning
tree structure, nodes correspond peers branches
materialize different reasoning paths induced initial input clause. Edges
labelled left side literals propagated along paths and/or right
side consequences transmitted back. downward arrow edge indicates
step literal propagated one peer neighbor. instance,
initial step represented following tree :

F ar
P1 :

Local consequences literal propagated peer explicited within
peer node. Target literals outlined using grey background, well transmitted
274

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

back consequences. Vertical arrows preceding consequences distinguish last returned
consequences earlier ones. Although successive trees presented increasing
depth, reasoning paths explored synchronously parallel, reader
keep mind messages exchanged asynchronous way
order consequents produced cannot predicted.
example, consequences Far derived local reasoning P1 Exp, Int
Chile Kenya. Since Exp arget(P1 ) local consequence Far. Int
target literal shared P2 , therefore transmitted P2 . clause
Chile Kenya also made shared variables. clauses processed algorithm
using split/recombination approach. shared literal processed independently,
transmitted appropriate neighbors. literal associated queue data
structure, transmitted back consequences stored. soon least one consequent obtained literal, respective queued consequents literal
recombined, produce consequences initial clause. recombination process
continues, new consequences literal produced. Note since literal
processed asynchronously, order recombined consequences produced
unpredictable. Here, component Chile transmitted P4 Kenya transmitted
P3 P4 . Let us note peer P4 appears two times tree, two different
literals propagated peer, induces two different reasoning paths.

F ar
P1 :
Exp
Int
P2 :

Chile

Int

Chile Kenya
Kenya Kenya

P4 :

P3 :

P4 :

Exp transmitted back user first (local) consequence Far.
propagation Int P2 produces clause Pass, arget(P2 )
shared therefore, cannot propagated.
clause Chile, transmitted P4 , produces Hotel arget(P4 )
shared cannot propagated.
transmitted P3 , clause Kenya produces YellowFev well clause
Lodge Palu. three variables arget(P3 ). Lodge Palu also shared
variables therefore, splitting second clause, corresponding literals
transmitted (independently) P4 .
transmitted P4 , Kenya produces Lodge, arget(P4 ) also
shared therefore transmitted P3 .

275

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

Exp

F ar
P1 :
Exp

Int
P2 :
Pass

Chile Kenya

Int

Chile
P4 :
Hotel

Kenya

Kenya

P3 :
YellowFev
Lodge
P4 :

P4 :
Lodge Palu
P alu
P4 :

Lodge
Lodge
P3 :

clause Pass, produced P2 , transmitted back P1 consequence Int
user remote consequence Far.

clause Hotel, produced P4 , transmitted back P1 queued
consequent Chile, since combined consequences Kenya.

two local consequences Kenya obtained P3 contain target variables.
transmitted back P1 queued there. may combined
Hotel produce two new combined consequences Far : Hotel YellowFev
Hotel Lodge Palu, transmitted back user.
Similarly P4 , Lodge local target consequent Kenya, transmitted back
P1 consequent Kenya, combined Hotel produce new
consequence Far that, turn, transmitted back user.

Simultaneously, reasoning propagates network peers. propagation Lodge Palu P4 respectively produces Kenya, target literal
shared thus propagated P1 , well AntiM, target literal,
shared. detail propagation Lodge right branch
reasoning tree.
276

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

Exp
Pass

Hotel YellowFev

F ar

Hotel Lodge Palu
Hotel Lodge

P1 :
Exp

Pass
P2 :
Pass

P4 :
Hotel

Chile Kenya

Int

YellowFev

Hotel

Lodge Palu

P3 :

Lodge
P4 :

YellowFev
Lodge
P4 :
Kenya

Lodge Palu
P alu
P4 :
AntiM

Lodge
Lodge
P3 :
...

Kenya
P1 :

Note deepest node P1 asked produce implicates Kenya,
complementary literal Kenya still process. see Section 3
situations handled algorithm mean histories keeping track
reasoning branches ending transmitted literal. history contains
two complementary literals, corresponding reasoning branch closed empty
clause ! returned consequence literals history.

example, consequence produced P1 Kenya thus !, sent
back P4 P3 . combination P3 Palu thus obtain Palu
new consequent Kenya, subsumes previously obtained Lodge Palu.
transmitted back P1 combined Hotel obtain Hotel Palu subsumes
previously obtained consequent Hotel Lodge Palu. Since AntiM shared variable
consequent Palu P4 . transmitted back P3 combination
!, thus obtain AntiM which, turn, returned P1 combination Hotel, thus
giving Hotel AntiM new consequent Far.
277

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

Exp Pass
Hotel YellowFev
Hotel Lodge

Hotel Palu

F ar

Hotel AntiM
Hotel ...

P1 :
Exp

Chile Kenya

Int

YellowFev
Pass
P2 :
Pass

P4 :
Hotel

Lodge

Palu

Hotel

...

AntiM

P3 :

P4 :

YellowFev

Lodge Palu
AntiM

!
P4 :
Kenya

P4 :
AntiM

Lodge
...
P3 :
...

!
P1 :

detailed production consequences Lodge P3 right
branch. reader could check similar way also produces AntiM (which
already produced P3 /P4 third branch). Eventually, whole set
consequences Far {Exp, Pass, HotelLodge, HotelPalu, HotelAntiM, HotelYellowFev}.
Among consequences, important note (e.g., HotelYellowFev)
involve target variables different peers. implicates could obtained
partition-based algorithms like (Amir & McIlraith, 2000). made possible
thanks split/recombination strategy algorithm.

3. Distributed Consequence Finding Algorithm
message passing distributed algorithm implemented described Section
3.2. show terminates computes results recursive
algorithm described Section 3.1. exhibit property acquaintance graph
guarantees completeness recursive algorithm, therefore message passing
distributed algorithm (since algorithms compute results).
algorithms, use following notations :
literal q, Resolvent(q, P ) denotes set clauses obtained resolution
set P {q} P alone. call clauses proper resolvents q
w.r.t. P ,
literal q, q denotes complementary literal,
clause c peer P , S(c) (resp. L(c)) denotes disjunction literals
c whose variables shared (resp. shared) acquaintance P .
278

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

condition S(c) = ! thus expresses c contain variable shared
acquaintance P ,
history hist sequence triples (l, P, c) (where l literal, P peer, c
clause). history [(ln , Pn , cn ), . . . , (l1 , P1 , c1 ), (l0 , P0 , c0 )] represents branch
reasoning initiated propagation literal l0 within peer P0 , either
contains clause l0 c0 (in case c0 may splitted different
literals among l1 propagated P1 ), (in case l0 = c0 l0
propagated P1 , thus l0 = l1 ): every [0..n 1], ci consequence
li Pi , li+1 literal ci , propagated Pi+1 ,
! distribution operator sets clauses: S1 ! ! Sn = {c1 cn
|c1 S1 , . . . , cn Sn }. L = {l1 , . . . , lp }, use notation !lL Sl denote
Sl1 ! ! Slp .

3.1 Recursive Consequence Finding Algorithm

Let = (P, acq) P2PIS, P one peers, q literal whose variable belongs
vocabulary P . RCF (q, P ) computes implicates literal q w.r.t. P, starting
computation local consequences q, i.e., implicates q w.r.t. P ,
recursively following acquaintances visited peers. ensure termination,
necessary keep track literals already processed peers. done
recursive algorithm RCF H(q, SP, hist), hist history reasoning branch
ending propagation literal q SP , set acquaintances
last peer added history.
Algorithm 1: Recursive consequence finding algorithm
RCF (q, P )
(1)return RCF H(q, {P }, )
RCF H(q, SP, hist)
(1)if exists P SP s.t. q P every P SP , (q, P, ) hist return
(2)else (q, , ) hist return {!}
(3)else every P SP local(P ) {q} Resolvent(q, P )
(4)if exists P SP s.t. ! local(P ) return {!}
(5)else every P SP local(P ) {c local(P )|L(c) arget(P
!)}
(6)if every P SP every c local(P ), S(c) = !, return P SP local(P )
(7)else
!
(8) result P SP {c local(P )|S(c) arget(P )}
(9) foreach P SP c local(P ) s.t. S(c) %= !
(10) q c P , P P \{q c}
(11) foreach literal l S(c)
(12)
answer(l) RCF H(l, acq(l, P ), [(q, P, c)|hist])
(13) disjcomb (!lS(c) answer(l)) ! {L(c)}
(14) result result disjcomb
(15) return result
279

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

establish properties algorithm. Theorem 1 states algorithm
guaranteed terminate sound. Theorem 2 exhibits condition
acquaintance graph algorithm complete. properties soundness
completeness, consider topology content P2PIS change
algorithm running. Therefore, properties following meaning
P2PIS: algorithm sound (respectively, complete) iff every P2PIS, results
returned RCF (q, P ), P peer P2PIS q literal whose variable
belongs vocabulary P , implicates (respectively, include proper prime
implicates) q w.r.t. union peers P2PIS, change
P2PIS algorithm running.
sufficient condition exhibited Theorem 2 completeness algorithm
global property acquaintance graph: two peers variable common
must either acquainted (i.e., must share variable) must related path
acquaintances sharing variable. First, important emphasize even
property global, checked running algorithm.
verified, algorithm remains sound complete. Second, worth noticing
modeling/encoding applications general peer-to-peer propositional reasoning
setting may result acquaintance graphs satisfying global property construction.
particular, shown Section 4 (Proposition 2), case propositional
encoding Semantic Web applications deal Somewhere.
Theorem 1 Let P peer P2PIS q literal belonging vocabulary P .
RCF (q, P ) sound terminates.
Proof: Soundness: need prove every result returned RCF (q, P ) belongs
target language implicate q w.r.t. P, P union
peers P2PIS. so, prove induction number rc recursive calls
RCF H(q, SP, hist) every result returned RCF H(q, SP, hist) (where history
hist, empty, form [(ln , Pn , cn ), . . . , (l0 , P0 , c0 )]) implicate q w.r.t.
P {ln , . . . , l0 } belongs target language.
rc = 0: either one conditions Line (1), Line (2), Line (4) Line (6)
satisfied.
- condition Line (1) satisfied, algorithm returns empty result.
- either exists peer P (q, , ) hist ! local(P ):
cases, ! returned algorithm (in respectively Line (2) Line (4))
indeed implicate q w.r.t. P {ln , . . . , l0 } belonging target language.

- Let r result returned algorithm Line (6): exists P SP
r local(P ), obvioulsy implicate q w.r.t. P {ln , . . . , l0 } (as q
resolvent q clause P ), belongs target language.

Suppose induction hypothesis true rc p, let SP set peers
P2PIS q literal (belonging vocabulary peers SP )
RCF H(q, SP, hist) requires p + 1 recursive calls terminate. Let r result
returned RCF H(q, SP, hist).
280

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

- r local(P ) P SP S(r) = ! S(r) arget(P ),
obviously implicate q w.r.t. P {ln , . . . , l0 } belonging target language.

- r % local(P ) P SP , obtained Line (13): exist P SP
clause c P form S(c) L(c) S(c) = ll1 llk r = r1
rk L(c), every ri result returned RCF H(lli , acq(lli , P ), [(q, P, c)|hist])
(Line (12)). According induction hypothesis (the number recursive calls
RCF H(lli , acq(lli , P ), [(q, P, c)|hist]) every lli less equal p), every
ri belongs target language implicate lli w.r.t. P\{q c}
{q, ln , . . . , l0 }, or, equivalently, implicate q w.r.t. P\{q c} {lli , ln , . . . , l0 }.
Therefore, r1 rk belongs target language implicate q w.r.t.
P\{q c} {S(c), ln , . . . , l0 }. Since L(c) belongs target language c =
S(c)L(c), r (i.e, r1 rk L(c)) belongs target language, implicate
q w.r.t. P\{q c} {c, ln , . . . , l0 }, fortiori q w.r.t. P {c, ln , . . . , l0 } Since
c local(P ), c implicate q w.r.t. P, therefore r implicate q
w.r.t. P {ln , . . . , l0 }.

Termination: recursive call, new triple (sl, P, c) added history.
algorithm terminate, history would infinite, possible since
number peers, literals clauses within P2PIS finite.
!
following theorem exhibits sufficient condition algorithm complete.
Theorem 2 Let = (P, acq) P2PIS. every P , P # v VP VP ! exists
path P P # , edges labelled v, every literal
q LP , RCF (q, P ) computes proper prime implicates q w.r.t. P belong
arget(P).

Proof: fact, prove RCF (q, P ) computes least prime proper resolvents
q w.r.t. P , i.e., elements Resolvent(q, P ) strictly subsumed
elements Resolvent(q, P ).
first show proper prime implicates q w.r.t. P prime proper resolvents
q w.r.t. P . Let proper prime implicate q w.r.t. P . definition P {q} |=
P %|= m. completeness resolution w.r.t. prime implicates, obtained
resolution set P {q} P alone, i.e., proper resolvent q w.r.t.
P . Let us suppose strictly subsumed another element m# Resolvent(q, P ).
means P {q} |= m# |= % m# , contradicts prime
implicate q w.r.t. P .
prove induction maximum number rc recursive calls involving
literal triggering RCF H(q, SP, hist) RCF H(q, SP, hist) computes
proper prime implicates belonging target language q w.r.t. P(hist), P(hist)
obtained P replacing li ci li li %= ci . Thus:
P(hist) = P hist empty,
otherwise:
P(hist) = P\{li ci |(li , Pi , ci ) hist s.t. li %= ci } {li |(li , Pi , ci ) hist s.t. li %= ci }
281

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

history hist empty, form [(ln , Pn , cn ), . . . , (l0 , P0 , c0 )]. According
algorithm, RCF H(q, SP, hist) triggered, least n + 1
previous calls algorithm RCF H: RCF H(l0 , SP0 , ) RCF H(li , SPi , histi )
[1..n], histi =[(li1 , Pi , ci1 ) . . . , (l0 , P0 , c0 )]), Pi SPi every [0..n].
Since P2PIS cyclic, may case call RCF H(q, SP, hist),
(q, P, q) hist. case, previous calls RCF H involving q,
i.e., form RCF H(q, SPi , histi ).
rc = 0: either one conditions Line (1), Line (2), Line (4) Line (6)
satisfied.
- first condition satisfied, since rc = 0, cannot case every
P SP , (q, P, ) hist, therefore exists P SP q P :
case, proper prime implicate q w.r.t. P(hist), q P(hist)
prime implicates q w.r.t. theory containing q consequences
theory.
- either (q, , ) hist ! local(P ) given peer P SP : cases,
! prime implicate q w.r.t. P(hist) therefore, proper prime
implicate, one too. returned algorithm (respectively Line (2)
Line (4)).
- every P SP , every resolvent q w.r.t. P shared variable
acquaintance P : P satisfies property stated theorem, means
every prime implicate q w.r.t. P variable common
theory P. According
Lemma 1, set proper resolvents q w.r.t. P(hist)
!
included P SP local(P ), thus particular every proper prime implicate
q w.r.t. P(hist), target language, returned algorithm
(Line(6)).
Suppose induction hypothesis true rc p, let SP set peers
P2PIS satisfying property stated theorem, RCF H(q, SP, hist)
requires atmost p + 1 recursive calls involving q. Since least one recursive
call, condition Line (1) satisfied. Let target language
proper prime implicate q w.r.t. P(hist), P(hist) = P\{li ci |(li , Pi , ci )
hist s.t. li %= ci } {li |(li , Pi , ci ) hist s.t. li %= ci }. Let us show belongs
result returned RCF H(q, SP, hist).
- proper resolvent q w.r.t. given P SP , local(P )
returned algorithm since target language.
-If proper resolvent q w.r.t. given P SP , then, according

!
Lemma 1, either (i) q variable !
common clauses P(hist)\ P SP P ,
(ii) exists
! clause q c P SP P c variables common
P(hist)\ P SP P proper resolvent c w.r.t. P(hist)\{q c}
{q}. addition, prime proper resolvent c w.r.t. P(hist)\{q c} {q}.
Let us suppose case. exists clause m#
Resolvent(c, P(hist)\{q c} {q}), m# |= m# % m. soundness,
P(hist)\{q c} {q} {c} |= m# . Since P(hist)\{q c} {q} {c} P(hist) {q},
282

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

P(hist) {q} |= m# m# |= m# % m, contradicts prime
implicate q w.r.t. P(hist).

(i) first case, according property stated theorem, variable
q shared peers P2PIS SP , therefore q involved
iteration loop Line (9). According induction hypothesis (the
number recursive calls obtain answer(q) Line (12) less equal
p) answer(q) includes set proper prime resolvents q w.r.t. P(hist# ),
target language, hist# = [(q, P, q)|hist], thus P(hist# ) = P(hist).
Therefore, answer(q) includes set proper prime resolvents q w.r.t. P(hist),
particular m.
(ii) second case, according property stated theorem, c shares
variables peers P2PIS SP . addition, since
target language, local variables c target variables. Therefore c
involved iteration loop Line (9). According induction hypothesis
(the number recursive calls obtain answer(l) Line (12) less equal
p), every l S(c), answer(l) includes set proper prime resolvents
l w.r.t. P(hist# ) thus particular, set proper prime implicates
l w.r.t. P(hist# ) target language. Since hist# = [(q, P, c)|hist]
q %= c (because duplicate literals clauses consider),
P(hist# ) = P(hist)\{q c} {q}. apply Lemma 2 infer DisjComp,
computed Line (13), includes set proper prime implicates c w.r.t.
P(hist)\{q c} {q}, target language, particular m.
!
Lemma 1 Let P set clauses q literal. Let P # P contains clauses
variable common q. proper resolvent q w.r.t. P , :
either proper resolvent q w.r.t. P # ,
variable q common clauses P \P # ,
exists clause q c P # c variables common clauses
P \P # proper resolvent c w.r.t. P \{q c} {q}.
Proof: Let proper resolvent q w.r.t. P . different q, exists
clause q c P proper resolvent c w.r.t. P {q}. Since P \{q c}
{q} P {q}, proper resolvent c w.r.t. P \{q c} {q}.
clause exist P # , exists P \P # therefore variable q
common clauses P \P # .
exists clause q c P # proper resolvent c w.r.t.
P \{q c} {q}, proper resolvent q w.r.t. P # , every
proof must exist clause c# P \P # either q q c must
resolved. Therefore, either q c variables common clauses P \P # . !
283

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

Lemma 2 Let P set clauses, let c = l1 ln clause. every proper
prime implicate c w.r.t. P , exists m1 , . . . , mn m1 mn ,
every [1..n], mi proper prime implicate li w.r.t. P .

Proof: Let proper prime implicate c w.r.t. P . every literal li , let od(li )
set models P make li true. od(li ) = , means !
proper prime implicate li w.r.t. P . every od(li ) %= , every model od(li ) model P {c}, model ; therefore,
proper implicate li w.r.t. P , and, definition proper prime implicates, exists
proper prime implicate mi li w.r.t. P mi |= m. Consequently, exists
m1 , . . . , mn m1 mn |= m, every [1..n], mi proper prime
implicate li w.r.t. P (mi may !). Since P {l1 ln } |= m1 mn ,
proper implicate l1 ln w.r.t. P , necessarily get m1 mn . !

3.2 Message-based Consequence Finding Algorithm
section, exhibit result transformation previous recursive algorithm DeCA: message-based Decentralized Consequence finding Algorithm running
locally peer. DeCA composed three procedures, one triggered
reception message. procedure ReceiveForthMessage triggered
reception f orth message m(Sender, Receiver, f orth, hist, l) sent peer Sender
peer Receiver executes procedure: demand Sender,
shares variable l, processes literal l. procedure ReceiveBackMessage
triggered reception back message m(Sender, Receiver, back, hist, r) sent
peer Sender peer Receiver executes procedure: processes consequence r (which clause variables target variables) sent back Sender
literal l (last added history) ; may combine consequences literals clause l. procedure ReceiveFinalMessage
triggered reception f inal message m(Sender, Receiver, f inal, hist, true):
peer Sender notifies peer Receiver computation consequences literal
l (last added history) completed. procedures handle two data structures
stored peer: cons(l, hist) caches consequences l computed reasoning
branch corresponding hist ; final(q, hist) set true propagation q within
reasoning branch history hist completed.
reasoning initiated user (denoted particular peer U ser) sending
given peer P message m(U ser, P, f orth, , q). triggers peer P local execution procedure ReceiveForthMessage(m(U ser, P, f orth, , q)). description
procedures, since locally executed peer receives message,
denote Self receiver peer.
284

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

Algorithm 2: DeCA message passing procedure propagating literals forth
ReceiveForthMessage(m(Sender, Self, f orth, hist, q))
(1) (q, , ) hist
(2) send m(Self, Sender, back, [(q, Self, !)|hist], !)
(3) send m(Self, Sender, f inal, [(q, Self, true)|hist], true)
(4) else q Self (q, Self, ) hist
(5) send m(Self, Sender, f inal, [(q, Self, true)|hist], true)
(6) else
(7) local(Self ) {q} Resolvent(q, Self )
(8) ! local(Self )
(9)
send m(Self, Sender, back, [(q, Self, !)|hist], !)
(10) send m(Self, Sender, f inal, [(q, Self, true)|hist], true)
(11) else
(12) local(Self ) {c local(Self )| L(c) arget(Self )}
(13)
every c local(Self ), S(c) = !
(14)
foreach c local(Self )
(15)
send m(Self, Sender, back, [(q, Self, c)|hist], c)
(16)
send m(Self, Sender, f inal, [(q, Self, true)|hist], true)
(17) else
(18)
foreach c local(Self )
(19)
S(c) = !
(20)
send m(Self, Sender, back, [(q, Self, c)|hist], c)
(21)
else
(22)
foreach literal l S(c)
(23)
l arget(Self )
(24)
cons(l, [(q, Self, c)|hist]) {l}
(25)
else
(26)
cons(l, [(q, Self, c)|hist])
(27)
final(l, [(q, Self, c)|hist]) f alse
(28)
foreach RP acq(l, Self )
(29)
send m(Self, RP, f orth, [(q, Self, c)|hist], l)

Algorithm 3: DeCA message passing procedure processing return consequences
ReceiveBackMessage(m(Sender, Self, back, hist, r))
(1) hist form [(l! , Sender, c! ), (q, Self, c)|hist! ]
(2) cons(l! , hist) cons (l! , hist) {r}
(3) result !lS(c)\{l! } cons(l, hist) ! {L(c) r}
(4) hist! = , U U ser else U first peer P ! hist!
(5) foreach cs result
(6) send m(Self, U, back, [(q, Self, c)|hist!], cs)

Algorithm 4: DeCA message passing procedure notifying termination
ReceiveFinalMessage(m(Sender, Self, f inal, hist, true))
(1) hist form [(l! , Sender, true), (q, Self, c)|hist!]
(2) final(l! , hist) true
(3) every l S(c), final(l, hist) = true
(4) hist! = U U ser else U first peer P ! hist!
(5) send m(Self, U, f inal, [(q, Self, true)|hist!], true)
(6) foreach l S(c)
(7)
cons(l, [(l, Sender, ), (q, Self, c)|hist! ])
285

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

following theorem states two important results: first, message-based distributed
algorithm computes results algorithm Section 3.1, thus, complete
conditions Theorem 2 ; second user notified termination
occurs, crucial anytime algorithm.
Theorem 3 Let r result returned RCF (q, P ). P receives user message m(U ser, P, f orth, , q), message m(P, U ser, back, [(q, P, )], r) produced.
r last result returned RCF (q, P ), user notified termination
message m(P, U ser, f inal, [(q, P, true)], true).
Proof: prove induction number recursive calls RCF H(q, SP, hist) that:
(1) result r returned RCF H(q, SP, hist), exists P SP
P bound send message m(P, S, back, [(q, P, )|hist], r) receiving message
m(S, P, f orth, hist, q),
(2) r last result returned RCF H(q, SP, hist), peers P SP
bound send message m(P, S, f inal, [(q, P, true)|hist], true), first peer
history.
rc = 0: either one conditions Lines (1), (2), (4) (6) algorithm
RCF H(q, SP, hist) satisfied. shown proof Theorem 2
conditions Lines (2) (4) satisfied, ! result returned algorithm. condition Line (2) algorithm RCF H(q, SP, hist) corresponds
condition Line (1) algorithm ReceiveForthMessage(m(S, P, f orth, hist, q))
P SP , triggers sending message m(P, S, back, [(q, P, !)|hist], !)
(Line (2)) message m(P, S, f inal, [(q, P, true)|hist], true) (Line(3)). condition Line (4) algorithm RCF H(q, SP, hist) satisfied, exists P SP
! P . condition corresponds condition Line (8) algorithm
ReceiveForthMessage(m(S, P, f orth, hist, q)), triggers sending message
m(P, S, back, [(q, P, !)|hist], !) (Line (9) message m(P, S, f inal, [(q, P, true)|hist], true)
(Line (10)). condition (1) algorithm RCF H(q, SP, hist), result returned (see proof Theorem 2), corresponds condition Line (4) algorithm
ReceiveForthMessage(m(S, P, f orth, hist, q)), every P SP , triggers
sending final message (Line (5)). Finally, condition Line (6) algorithm RCF H(q, SP, hist) satisfied, exists P SP r local(P ).
condition Line (6) algorithm RCF H(q, SP, hist) corresponds condition
Line (13) ReceiveForthMessage(m(S, P, f orth, hist, q)), triggers sending
messages m(P, S, back, [(q, P, c)|hist], c), c clause local(P ) (Line (15)),
particular message m(P, S, back, [(q, P, r)|hist], r). triggers sending
final message (Line (16)) P . r last result returned RCF H(q, SP, hist),
final messages sent every P SP .
Suppose induction hypothesis true rc p, let = (P, acq) P2PIS
RCF H(q, SP, hist) requires p + 1 recursive calls terminate.
- exists P SP r local(P ), r last result returned
algorithm, r one clauses c involved iteration loop Line
(18) algorithm ReceiveForthMessage(m(S, P, f orth, hist, q)), verifying
condition Line (19), triggers sending message m(P, S, back, [(q, P, r)|
hist], r) (Line (20)).
286

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

-If exists P SP clause c : l1 lk L(c) local(P ) c
involved iteration loop Line (9) algorithm RCF H(q, P, hist), r
element r1 rk L(c) (!lS(c) answer(l)) ! {L(c)} computed Line (12),
answer(l) obtained result RCF H(l, acq(l, P ), [(q, P, c)|hist]) (Line
(13)), requires p less p recursive calls. induction, literal li S(c),
exists RPi acq(li , P ) RPi sends message m(RPi , P, back, [(li , RPi , ),
(q, P, c)|hist], ri ) received message m(P, RPi , f orth, [(q, P, c)|hist], li ). loop
Line (11) algorithm RCF H(q, SP, hist) corresponds loop Line (22)
algorithm ReceiveForthMessage(m(S, P, f orth,hist,q)), triggers sending
messages m(P, RPi , f orth, [(q, P, c)|hist], li ) literal li S(c) (Line (29)).
Therefore, according induction hypothesis, every li S(c), RPi sends message
m(RPi , P, back, [(li , RPi , ), (q, P, c)|hist], ri ). last messages (let us say
m(RPj , P, back, [(lj , RPj , ), (q, P, c)|hist], rj )) processed, r produced Line (3)
ReceiveBackMessage(m(RPj , P , back, [(lj , RPj , ), (q, P, c)|hist], rj )), exists
peer U P bound send message m(P, U, back, [(q, P, c)|hist], r) (Line
(6)).
- r last result returned algorithm RCF H(q, SP , hist), every P
SP , every c local(P ), every l S(c), RCF H(l, acq(l, P ), [(q, P, c)|hist])
finished, and, induction, every peer RP acq(l, P ) sent message m(RP, P, f inal,
[(l, RP, true), (q, P, c)|hist], true). Therefore, condition Line (3) algorithm
ReceiveFinalMessage(m(RP, P, f inal, [(l, RP, ), (q, P, c)|hist], true)) satisfied,
triggers sending message m(P, U, f inal, [(q, P, true)|hist], true) (Line (5)).
!
sake simplicity, recursive distributed algorithms presented
applying literals. mean formulas consider limited
literals. Clauses handled splitting literals using !
operator recompose results obtained literal.
also important notice ! returned algorithm proper prime
implicate lines (1) (3) (8) (10) ReceiveForthMessage.
case, corollary theorems, P2PIS detected unsatisfiable
input clause. Therefore, algorithm exploited checking satisfiability
P2PIS join new peer theory.

4. Application Semantic Web: somewhere Peer-to-peer Data
Management System
Semantic Web (Berners-Lee, Hendler, & Lassila, 2001) envisions world wide distributed architecture data computational resources easily inter-operate based
semantic marking web resources using ontologies. Ontologies formalization
semantics application domains (e.g., tourism, biology, medicine) definition classes relations modeling domain objects properties considered
meaningful application. concepts, tools techniques deployed
far Semantic Web community correspond big beautiful idea high
expressivity needed describing domain ontologies. result, applied,
287

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

current Semantic Web technologies mostly used building thematic portals
scale Web. contrast, Somewhere promotes small beautiful
vision Semantic Web (Rousset, 2004) based simple personalized ontologies (e.g.,
taxonomies atomic classes) distributed large scale. vision
Semantic Web introduced Plu, Bellec, Agosto, van de Velde (2003), user
imposes others ontology logical mappings ontologies make possible
creation web people personalized semantic marking data cohabits
nicely collaborative exchange data. view, Web huge peer-to-peer
data management system based simple distributed ontologies mappings.
Peer-to-peer data management systems proposed recently (Halevy et al.,
2003b; Ooi, Shu, & Tan, 2003; Arenas, Kantere, Kementsietsidis, Kiringa, Miller, & Mylopoulos, 2003; Bernstein, Giunchiglia, Kementsietsidis, Mylopoulos, Serafini, & Zaihraheu,
2002; Calvanese et al., 2004) generalize centralized approach information integration systems based single mediators. peer-to-peer data management system,
central mediator: peer ontology data services, mediate
peers ask answer queries. existing systems vary according
(a) expressive power underlying data model (b) way different peers
semantically connected. characteristics impact allowed queries
distributed processing.
Edutella (Nejdl et al., 2002), peer stores locally data (educational resources)
described RDF relative reference ontologies (e.g., Dmoz - http://dmoz.org).
instance, peer declare data related concept dmoz taxonomy corresponding path Computers/Programming/Languages/Java,
data export author date properties. overlay network underlying Edutella hypercube super-peers peers directly connected.
super-peer mediator data peers connected it. queried,
first task check query matches schema: case, transmits
query peers connected it, likely store data answering
query ; otherwise, routes query neighbour super-peers according
strategy exploiting hypercube topology guaranteeing worst-case logarithmic time
reaching relevant super-peer.
contrast Edutella, Piazza (Halevy et al., 2003b, 2003a) consider
data distributed different peers must described relatively existing
reference schemas. Piazza, peer data schema mediate
peers declaring mappings schema schemas
peers. topology network fixed (as hypercube Edutella)
accounts existence mappings peers: two peers logically connected
exists mapping two schemas. underlying data model first
version Piazza (Halevy et al., 2003b) relational mappings relational
peer schemas inclusion equivalence statements conjunctive queries.
mapping formalism encompasses local-as-views global-as-views (Halevy, 2000)
formalisms used information integration systems based single mediators. price
pay query answering undecidable except restrictions imposed
mappings topology network (Halevy et al., 2003b). currently
implemented version Piazza (Halevy et al., 2003a) relies tree-based data model:
288

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

data XML mappings equivalence inclusion statements XML
queries. Query answering implemented based practical (but complete) algorithms
XML query containment rewriting. scalability Piazza far go
80 peers published experiments relies wide range
optimizations (mappings composition, Madhavan & Halevy, 2003, paths pruning, Tatarinov
& Halevy, 2004), made possible centralized storage schemas mappings
global server.
Somewhere, made choice fully distributed: neither
super-peers (as Edutella) central server global view overlay
network (as Piazza). addition, aim scaling thousands peers.
make possible, chosen simple class-based data model data
set resource identifiers (e.g., URIs), schemas (simple) definitions classes
possibly constrained inclusion, disjunction equivalence statements, mappings
inclusion, disjunction equivalence statements classes different peer schemas.
data model accordance W3C recommendations since captured
propositional fragment OWL ontology language (http://www.w3.org/TR/owlsemantics). Note OWL makes possible, declarative import mechanism,
retrieve ontologies physically distributed. Using transitive mechanism peer
data management systems amounts worst case centralized single peer
whole set peer ontologies, reason locally. feeling large networks
mechanism cannot scale satisfactorily. Moreover, dynamicity
peer-to-peer settings, imports would re-actualized time peer
joins quit network.
Section 4.1 defines Somewhere data model, illustrative example
given Section 4.2. Section 4.3, show query rewriting Somewhere, thus
query answering, reduced propositional encoding distributed reasoning
propositional logic.
4.1 Somewhere Data model
Somewhere, new peer joins network peers knows (its acquaintances) declaring mappings ontology ontologies
acquaintances. Queries posed given peer using local ontology. answers
expected instances local classes possibly instances classes
peers distant queried peer infered peer ontologies
mappings satisfy query. Local ontologies, storage descriptions mappings
defined using fragment OWL DL description logic fragment
Ontology Web Language recommended W3C. call OWL PL fragment OWL
DL consider Somewhere, PL stands propositional logic. OWL PL
fragment OWL DL reduced disjunction, conjunction negation constructors
building class descriptions.
4.1.1 Peer ontologies
peer ontology made set class definitions possibly set equivalence,
inclusion disjointness axioms class descriptions. class description either
289

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

universal class (/), empty class (), atomic class union (1), intersection (2)
complement () class descriptions.
name atomic classes unique peer: use notation P :A identifying atomic class ontology peer P . vocabulary peer P set
names atomic classes.
Class descriptions
Logical notation
universal class
/
empty class

P :A
atomic class
conjunction
D1 2 D2
disjunction
D1 1 D2

negation

OWL notation
hing
N othing
classID
intersectionOf (D1 D2)
unionOf (D1 D2)
complementOf (D)

Axioms class definitions
Logical notation
OWL notation
Complete
P :A
Class(P :A complete D)
Partial
P :A 3
Class(P :A partial D)

Axioms class descriptions
Logical notation
OWL notation
equivalence
D1 D2
EquivalentClasses(D1 D2)
inclusion
D1 3 D2
SubClassOf (D1 D2)
disjointness
D1 2 D2
DisjointClasses(D1 D2)

Taxonomies atomic classes (possibly enriched disjointness statements
atomic classes) particular cases allowed ontologies Somewhere . specification made set inclusion (and disjointness) axioms involving atomic classes only:
class definition using (conjunction, disjunction negation) constructors.
4.1.2 Peer storage descriptions
specification data stored locally peer P done declaration
atomic extensional classes defined terms atomic classes peer ontology,
assertional statements relating data identifiers (e.g., URIs) extensional classes.
restrict axioms defining extensional classes inclusion statements
atomic extensional class description combining atomic classes ontology.
impose restriction order fit Local-as-Views approach open-world
assumption within information integration setting (Halevy, 2000). use
notation P :V iewA denote extensional class V iewA peer P .
Storage description
declaration extensional classes:
OWL notation
Logical notation
P :V iewA 3 C
SubClassOf (P :V iewA C)
assertional statements:
OWL notation
Logical notation
P :V iewA(a)
individual(a type(P :V iewA))
290

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

4.1.3 Mappings
Mappings disjointness, equivalence inclusion statements involving atomic classes
different peers. express semantic correspondence may exist
ontologies different peers.
4.1.4 Schema Somewhere network
Somewhere network, schema centralized distributed union
different peer ontologies mappings. important point peer
partial knowledge schema: knows local ontology mappings
acquaintances.
Let P Somewhere peer-to-peer network made collection peers {Pi }i=1..n .
peer Pi , let Oi , Vi Mi sets axioms defining respectively local
ontology Pi , declaration extensional classes set mappings stated Pi
classes Oi classes
! ontologies acquaintances Pi . schema
P, denoted S(P), union i=1..n Oi Vi Mi ontologies, declaration
extensional classes sets mappings peers.
4.1.5 Semantics
semantics isis standard semantics first order logic defined terms interpretations. interpretation pair (I , .I ) non-empty set, called domain
interpretation, .I interpretation function assigns subset every
class identifier element every data identifier.
interpretation model distributed
! schema Somewhere peer-topeer network P = {Pi }i=1..n iff axiom i=1..n Oi Vi Mi satisfied I.
Interpretations axioms rely interpretations class descriptions inductively
defined follows:
/I = , =
(C1 1 C2 )I = C1I C2I
(C1 2 C2 )I = C1I C2I
(C)I = \C
Axioms satisfied following holds:
C 3 satisfied iff C DI
C satisfied iff C = DI
C 2 satisfied iff C =
Somewhere peer-to-peer network satisfiable iff (distributed) schema
model.
Given Somewhere peer-to-peer network P = {Pi }i=1..n , class description C subsumes class description iff model S(P) DI C .
4.2 Illustrative Example
illustrate Somewhere data model small example four peers modeling four
persons Ann, Bob, Chris Dora, bookmarking URLs restaurants
know like, according taxonomy categorizing restaurants.
291

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

Ann, working restaurant critic, organizes restaurant URLs according
following classes:
class Ann:G restaurants considered offering good cooking, among
distinguishes subclass Ann:R rated: Ann:R 3 Ann:G
class Ann:R union three disjoint classes Ann:S1, Ann:S2, Ann:S3 corresponding respectively restaurants rated 1, 2 3 stars:
Ann:R Ann:S1 1 Ann:S2 1 Ann:S3
Ann:S1 2 Ann:S2 Ann:S1 2 Ann:S3
Ann:S2 2 Ann:S3
classes Ann:I Ann:O, respectively corresponding Indian Oriental
restaurants
classes Ann:C, Ann:T Ann:V subclasses Ann:O denoting Chinese, Ta Vietnamese restaurants respectively: Ann:C 3 Ann:O, Ann:T 3 Ann:O,
Ann:V 3 Ann:O
Suppose data stored Ann accepts make available deals restaurants various specialties, rated 2 stars among rated restaurants. extensional classes declared Ann then:
Ann:V iewS2 3 Ann:S2, Ann:V iewC 3 Ann:C, Ann:V iewV 3 Ann:V ,
Ann:V iewT 3 Ann:T , Ann:V iewI 3 Ann:I
Bob, fond Asian cooking likes high quality, organizes restaurant URLs
according following classes:
class Bob:A Asian restaurants
class Bob:Q high quality restaurants knows
Suppose wants make available every data stored. extensional
classes declares Bob:V iewA Bob:V iewQ (as subclasses Bob:A Bob:Q):
Bob:V iewA 3 Bob:A, Bob:V iewQ 3 Bob:Q
Chris fond fish restaurants recently discovered places serving
nice cantonese cuisine. organizes data respect following classes:
class Chris:F fish restaurants,
class Chris:CA Cantonese restaurants
Suppose declares extensional classes Chris:V iewF Chris:V iewCA subclasses Chris:F Chris:CA respectively:
Chris:V iewF 3 Chris:F , Chris:V iewCA 3 Chris:CA
Dora organizes restaurants URLs around class Dora:DP preferred restaurants, among distinguishes subclass Dora:P pizzerias subclass
Dora:SF seafood restaurants.
Suppose URLs stores concerns pizzerias: extensional class
declare Dora:V iewP subclass Dora:P : Dora:V iewP 3Dora:P
Ann, Bob, Chris Dora express know using mappings
stating properties class inclusion equivalence.
292

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

Ann confident Bobs taste agrees include Bob selection good
restaurants stating Bob:Q 3 Ann:G. Finally, thinks Bobs Asian restaurants
encompass Oriental restaurant concept: Ann:O 3 Bob:A
Bob knows calls Asian cooking corresponds exactly Ann classifies
Oriental cooking. may expressed using equivalence statement : Bob:A
Ann:O (note difference perception Bob Ann regarding mappings
Bob:A Ann:O)
Chris considers calls fish specialties particular case Dora seafood
specialties: Chris:F 3 Dora:SF
Dora counts Ann Bob obtain good Asian restaurants : Bob:A 2 Ann:G
3 Dora:DP
Figure 2 describes resulting overlay network. order alleviate notations,
omit local peer name prefix except mappings. Edges labeled class
identifiers shared mappings peers.
Dora
ontology :
DP 3 /,
P 3 DP , SF 3 DP,
V iewP 3 P
mappings :
Bob:A 2 Ann:G 3 Dora:DP

Ann:G

Bob:A

Dora:SF

Chris
ontology :
F 3 /, CA 3 /,
V iewF 3 F ,V iewCA 3 CA
mappings :
Chris:F 3 Dora:SF

Bob
ontology :
3 /, Q 3 /,
V iewA 3 A,
V iewQ 3 Q
mappings :
Bob:A Ann:O

Bob:Q,
Bob:A,
Ann:O

Ann
ontology :
G 3 /, 3 /, 3 /,
R 3 G,
(S1 1 S2 1 S3) R,
S1 2 S2 ,
S1 2 S3 ,
S2 2 S3 ,
(C 1 V 1 ) 3 O,
V iewC 3 C,
V iewV 3 V ,
V iewT 3 ,
V iewI 3 I,
V iewS2 3 S2
mappings :
Ann:O 3 Bob:A,
Bob:Q 3 Ann:G

Figure 2: restaurants network
4.3 Query Rewriting Somewhere Propositional Encoding
Somewhere, user interrogates peer-to-peer network one peer
choice, uses vocabulary peer express query. Therefore, queries
logical combinations classes given peer ontology.
corresponding answer sets expressed intention terms combinations
extensional classes rewritings query. point extensional classes
293

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

several distant peers participate rewritings, thus answer query
posed given peer.
Definition 6 (Rewritings) Given Somewhere peer-to-peer network P = {Pi }i=1..n ,
logical combination Qe extensional classes rewriting query Q iff Q subsumes Qe
w.r.t. P.
Qe proper rewriting exists model S(P) QIe %= . Qe
conjunctive rewriting rewriting conjunction extensional classes.
Qe maximal (conjunctive) rewriting exist another (conjunctive) rewriting Q#e Q (strictly) subsuming Qe w.r.t. P.
general, finding answers peer data management system critical issue
(Halevy et al., 2003b). setting however, case answers
obtained using rewritings query: shown (Goasdoue & Rousset, 2004)
query finite number maximal conjunctive rewritings, answers
(a.k.a. certain answers) obtained union answer sets rewritings.
query answering point view, notion proper rewriting relevant
guarantees non empty set answers. query proper rewriting,
wont get answer.
Somewhere setting, query rewriting equivalently reduced distributed
reasoning logical propositional theories straighforward propositional encoding
query distributed schema Somewhere network. consists transforming query schema statement propositional formula using class identifiers
propositional variables.
propositional encoding class description D, thus query, propositional formula P rop(D) obtained inductively follows:
P rop(/) = true, P rop() = f alse
P rop(A) = A, atomic class
P rop(D1 2 D2 ) = P rop(D1 ) P rop(D2 )
P rop(D1 1 D2 ) = P rop(D1 ) P rop(D2 )
P rop(D) = (P rop(D))
propositional encoding schema Somewhere peer-to-peer network P
distributed propositional theory P rop(S) made formulas obtained inductively
axioms follows:
P rop(C 3 D) = P rop(C) P rop(D)
P rop(C D) = P rop(C) P rop(D)
P rop(C 2 ) = P rop(C) P rop(D)
on, simplicity, use propositional clausal form notation
queries Somewhere peer-to-peer network schemas. illustration, let us consider
propositional encoding example presented Section 4.2. application
transformation rules, conversion produced formula clausal form suppression
tautologies, obtain (Figure 3) new acquaintance graph peer schema
described propositional theory.
state two propositions showing query rewriting Somewhere
reduced consequence finding P2PIS presented previous sections.
294

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

Dora : :
Dora:V iewP Dora:P
Dora:P Dora:DP
Dora:SF Dora:DP
Bob:A Ann:G Dora:DP

Ann:G

Bob:A

Dora:SF

Bob :
Bob:V iewA Bob:A
Bob:V iewQ Bob:Q
Bob:A Ann:O
Ann:O Bob:A

Chris :
Chris:V iewF Chris:F
Chris:V iewCA Chris:CA
Chris:F Dora:SF

Bob:Q,
Bob:A,
Ann:O

Ann :
Ann:S1 Ann:S2
Ann:S1 Ann:S3
Ann:S2 Ann:S3
Ann:S1 Ann:R
Ann:S2 Ann:R
Ann:S3 Ann:R
Ann:R Ann:S1 Ann:S2 Ann:S3
Ann:V iewS2 S2
Ann:R Ann:G
Bob:Q Ann:G
Ann:O Bob:A
Ann:C Ann:O
Ann:V Ann:O
Ann:T Ann:O
Ann:V iewC Ann:C Ann:V iewV Ann:V
Ann:V iewT Ann:T
Ann:V iewI Ann:I
Chris:CA Ann:C

Chris:CA

Figure 3: propositional encoding restaurant network

Proposition 1 states propositional encoding transfers satisfiability establishes connection proper (maximal) conjunctive rewritings proper (prime)
implicates.
Proposition 2 states P2PIS resulting propositional encoding Somewhere schema fulfills construction property exhibited Theorem 2 sufficient
condition completeness algorithms described Section 3 compute proper
prime implicates clause w.r.t distributed propositional theories.
Proposition 1 Let P Somewhere peer-to-peer network let P rop(S(P))
propositional encoding schema. Let set extensional classes.
S(P) satisfiable iff P rop(S(P)) satisfiable.
qe proper maximal conjunctive rewriting query q iff P rop(qe) proper
prime implicate P rop(q) w.r.t. P rop(S(P)) variables extensional
classes.
Proof: first exhibit properties used proof proposition. Let
P Somewhere network, S(P) schema P rop(S(P)) propositional encoding.
interpretation S(P), element domain interpretation ,
define interpretation po (I) P rop(S(P)) follows: every propositional variable
v P rop(S(P)) (v name atomic class S(P)), v po (I) = true iff v .
interpretation J P rop(S(P)), define interpretation i(J) S(P) follows:
domain i(J) {true}, every atomic class S(P) (A name
propositional variable P rop(S(P))), AJ = true Ai(J) = {true} else Ai(J) = .
easy show following properties, every interpretation S(P) (and every
), every interpretation J P rop(S(P)):
1. every class description C S(P):
295

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

a) C P rop(C)po (I) = true

b) true C i(J) P rop(C)J = true
2. model S(P) po (I) model P rop(S(P))
3. i(J) model S(P) J model P rop(S(P)).
Suppose S(P) satisfiable let model let element
: according Property 2, po (I) model P rop(S(P)), thus
P rop(S(P)) satisfiable. converse way, let J propositional model
P rop(S(P)). According Property 3, i(J) model S(P), thus
S(P) satisfiable.
Suppose P rop(qe ) proper prime implicate P rop(q) w.r.t. P rop(S(P)),
variables extensional classes, let us show qe proper
maximal conjunctive rewriting q.
Let us first show qe rewriting q. Suppose case:
exists interpretation S(P) qeI % q thus element
qeI % q . According Property 2, po (I) model P rop(S(P)),
according Property 1.a, (P rop(qe ))po (I) = true (P rop(q))po (I) %= true, i.e.,
(P rop(qe ))po (I) = f alse (P rop(q))po (I) = true. impossible since
would contradict fact P rop(qe ) proper prime implicate {P rop(q)}
P rop(S(P)). Therefore, qe must rewriting q.
Let us show qe conjunctive rewriting q, i.e., qe conjunction
extensional classes. S(P), extensional classes appear inclusion axioms.
Therefore, propositional encoding S(P) ensures extensional classes
appear variables P rop(S(P)) negative literals. Resolution sound
complete prime implicate computation, P rop(qe ) obtained result
finite chain resolutions, starting clauses P rop(S(P)) P rop(q).
Since P rop(S(P)) extensional classes appear negative literals,
appear negative literals computed implicates, particular P rop(qe).
Therefore, qe conjunction extensional classes.
Let us show qe proper rewriting q, i.e., satisfiable
model S(P). Since P rop(qe) proper prime implicate P rop(q) w.r.t.
P rop(S(P)), exists model J P rop(S(P)) s.t. P rop(qe )J = f alse
thus P rop(qe )J = true. Property 3, i(J) model S(P) according
Property 1.b, true (qe )i(J) . Therefore exists model S(P )
qe satisfiable, thus qe proper rewriting q.
Finally, let us show qe maximal conjunctive rewriting q. Suppose
case. Then, exists conjunctive rewriting qe# q
qe |= qe# qe % qe# . means exists interpretation
element q # Ie , thus q , % qeI . According Property 1.a, P rop(qe# )po (I) = true, P rop(q)po (I) = true, P rop(qe )po (I) = f alse, i.e.,
P rop(qe# )po (I) = f alse, P rop(q)po (I) = f alse, P rop(qe )po (I) = true.
296

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

impossible since contradicts P rop(qe ) prime implicate P rop(q) w.r.t.
P rop(S(P)). Therefore, qe maximal conjunctive rewriting q.
Let us prove converse direction. Suppose qe proper maximal conjunctive rewriting query q let us show hat P rop(qe) proper prime implicate
P rop(q) w.r.t. P rop(S(P)). definition proper rewriting, every model
S(P) qeI q , equivalently (q)I (qe )I exists model # S(P)
!
qeI %= .

Let us first show P rop(qe ) implicate P rop(q) w.r.t P rop(S(P)). Suppose case, i.e., {P rop(q)}P rop(S(P)) %|= P rop(qe ). Then,
exists model J {P rop(q)} P rop(S(P)) P rop(qe )J = f alse.
According Property 3, i(J) model S(P). According
Property 1.b, true (q)i(J) (qe )i(J) = . impossible since contradicts
(q)i(J) (qe )i(J) . Therefore, P rop(qe ) implicate P rop(q) w.r.t.
P rop(S(P)).
Let us show P rop(qe ) proper implicate P rop(q) w.r.t. P rop(S(P)),
i.e., P rop(qe) implicate P rop(S(P)) alone. definition proper
!
!
rewriting, exists model # S(P) qeI %= . Let element qeI .
According Property 2, po (I # ) model P rop(S(P)), according Property
!
!
1.a, (P rop(qe ))po (I ) = true, i.e., (P rop(qe ))po (I ) = f alse. Therefore, P rop(qe )
implicate P rop(S(P)).
Finally, let us show P rop(qe ) prime implicate P rop(q) w.r.t. P rop(S(P)).

Let us show c clause P rop(S(P )) {P rop(q)} |= c c |=
P rop(qe ), c P rop(qe ). Since c |= P rop(qe ) P rop(qe ) disjunction negation extensional classes, c disjunction subset literals P rop(qe ). Let qe# conjunction extensional classes c,
c = P rop(qe# ). proved previously P rop(qe# ) implicate
P rop(q) w.r.t. P rop(S(P )) qe# rewriting q, similarly P rop(qe )
implicate P rop(qe# ) w.r.t. P rop(S(P )) qe# subsumes qe . Therefore, qe#
rewriting qe subsumes qe . Since qe maximal conjunctive rewriting
q, qe# qe , thus qe# qe P rop(qe# ) P rop(qe ), i.e. c P rop(qe ).
!

Proposition 2 Let P Somewhere peer-to-peer network let P rop(S(P))
propositional encoding schema. Let = (P rop(S(P)), acq) corresponding
P2PIS, set labelled edges acq that: (P:A, P, P) acq iff P:A
involved mapping P P (i.e, P = P P = P). every P , P #
v VP VP ! exists path P P # edges labelled
v.
Proof: Let P P # two peers variable v common. Since vocabularies
local ontologies different peers disjoint, v necessarily variable P ## :A involved
297

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

mapping declared P acquaintances P1 (and thus P ## = P P ## =
P1 ), P # acquaintances P1# (in case P ## = P # P ## = P1# ).
- v form P ## :A P ## = P (respectively P ## = P # ), P :A atomic
class P (respectively P # :A atomic class P # ) involved mapping
P P # , therefore, edge (and thus path) P P # labelled
v (P :A P # :A respectively) .
- v form P ## :A P ## distinct P P # , P ## :A atomic
class P ## , involved mapping P ## P mapping
P ## P # . Therefore, exists edge P ## P labelled v edge
P ## P # labelled v, thus path P P # edges
labelled v.
!
two propositions, follows message-based distributed consequence
finding algorithm Section 3.2 used compute maximal conjunctive rewritings
query. algorithm computes set proper prime implicates literal w.r.t.
distributed propositional clausal theory. Therefore, applied distributed theory
resulting propositional encoding schema Somewhere network,
extensional classes symbols target variables, triggered literal q, computes
fact negation maximal conjunctive rewritings atomic query q. result
also holds arbitrary query since, setting, maximal rewritings
query obtained combining maximal rewritings atomic components.
corollary two propositions that, setting, query answering BH2 complete w.r.t. query complexity polynomial w.r.t. data complexity.

5. Experimental Analysis
section devoted experimental study performances distributed
consequence finding algorithm described Section 3, deployed real peer-to-peer
inference systems. Particularly, aim experiments study scalability issues
Somewhere infrastructure Semantic Web. experiments
performed networks 1000 peers. goal thus study practical complexity
reasoning networks size answer questions as: deep
wide reasoning spread network peers? network cope
traffic load? fast results obtained? extent results integrate
information distinct peers? etc.
far, large real corpus distributed clausal theories still missing. Since deploying
new real applications scale requires significant amount time, chosen
perform experiments artificially generated instances peer-to-peer networks.
instances characterized size form local theories corresponding
peer network, well size topology acquaintance
graph.
Since aim use Somewhere infrastructure Semantic Web applications,
focus benchmarking instances suitable characteristics. particular generate local theories supposed encode realistic ontologies mappings, could
298

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

written people categorize data. Acquaintances peers generated
way topology resulting graph looks realistic respect
acquaintances people Web. Therefore focus generation acquaintance graphs so-called small world property, admitted (Newman, 2000)
general property social networks (including Web).
following, first detail Section 5.1 generation process benchmark
instances, involved parameters allowed vary experiments. Section 5.2, first series experiments studies hardness local reasoning
within single peer evaluating number size computed proper prime implicates. allows us realize intrinsic complexity task thus,
reasoning large scale networks theories. also helps us justify choice
parameter values experiments. Finally, Section 5.3 reports experimental results obtained concerning scalability Somewhere large
networks peers.
5.1 Benchmark Generation
Generating satisfactory instances peer-to-peer networks framework means
generating realistic propositional theories peer, well appropriate structure
acquaintance graph. latter induced variables shared peer
theories. setting Semantic Web, correspond names atomic classes
involved mappings peer ontologies.
5.1.1 Generation local theories
make following assumptions ontologies mappings likely
deployed large scale future Semantic Web: ontologies taxonomies
atomic classes (possibly disjointness statements pairs atomic classes) ;
mappings ontologies likely state simple inclusion equivalence
two atomic classes two different peers, want exclude
complex mappings involving logical combination classes.
consequence, propositional encoding taxonomy results set clauses
length 2. mappings result well clauses length 2. complex mappings
might result longer clauses, since set clauses may equivalently rewritten
set clauses length 3, restrict case encoded clauses
length 3. Clauses encoding mappings (called mapping clauses) thus clauses
length 2 3, (2-clauses 3-clauses short). denote %3cnf ratio
3-clauses total number mapping clauses. ratio reflects way degree
complexity mappings. experiments study variations parameter
significant impact hardness reasoning.
local theories generated two steps. first generate set random
2-clauses number n propositional variables randomly select number (t n)
target variables (corresponding names extensional classes). Mapping clauses
generated, according given value %3cnf added theories. Since
mapping clauses induce structure acquaintance graph, way generated
discussed below. Peer theories thus composed 2-clauses 3-clauses.
299

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

literature propositional reasoning, theories correspond so-called 2 + p formulas,
p denotes proportion 3-clauses whole set clauses (note p
%3cnf different ratios).
5.1.2 Generation acquaintance graph
order focus realistic acquaintance graphs, chosen generate random
graphs small worlds properties, proposed studied Watts Strogatz
(1998), well Newman (2000). graphs two properties encountered
social networks: first, short path length pair nodes, observed social
relationship (for instance, widely accepted six-degrees separation humans)
and, second, high clustering ratio, measurement number common neighbors
shared two neighbors (for instance, likely two friends share common subset
friends).
generate graphs, first generate pairs peers connected. Following work Watts Strogatz (1998), start called k-regular ring
structure np nodes, i.e., graph nodes may arranged ring,
node connected k closest neighbors. Edges graph
randomly rewired, given probability pr, replacing one (or both) connected
peers another peer. shown regular graphs (pr = 0)
uniform random graphs (pr = 1), graphs generated pr = 0.1 small world
properties. acquaintance graphs used experiments generated
way, pr = 0.1. Moreover, since aim evaluate scalability approach,
chosen focus networks significant size. instances, fixed
number peers np 1000 number k edges per peer 10.
topology network generated, local theories peer
generated. Portion theories encoding taxonomies first generated previously
described. Then, edge generated graph mapping clauses added.
simplicity, chosen add fixed number q mapping clauses edge.
Mapping clauses randomly generated picking one variable two peers
theories negating probability 0.5. %3cnf probability, third
literal (randomly chosen two peers) added clause produce mapping
clauses length 3. consequence, average number variables shared two
connected peers (2 + %3cnf ) q.
5.2 Hardness Local Reasoning within Single Peer
setting, first part reasoning performed peer consists computing
proper prime implicates received litteral w.r.t. local theory peer.
order grasp hardness local reasoning first conducted experimental
study evaluate number form implicates, also, since local
theories 2 + p clausal theories, evaluate impact ratio p values.
experiments performed using modified version Zres (Simon & del Val,
2001).
Prime implicates already studied 3-CNF random formulas (Schrag &
Crawford, 1996). corresponds case p = 100%. first take
300

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

3CNF m=30 clauses, 1500 experiments

3CNF m=30 clauses, 1500 experiments

1

700

Number prime implicates

600

500

Cumulative Distribution Function (CDF)

n=12 vars
n=14 vars
n=16 vars
n=18 vars
n=20 vars
n=22 vars
n=24 vars
n=26 vars
n=28 vars

400

300

200

100

0.9

0.8

0.7

n=12 vars
n=16 vars
n=20 vars
n=24 vars
n=28 vars

0.6

0.5

0.4

0.3

0.2

0.1

0

0

2

4

6

8

10

0
0
10

12

Size prime implicates

1

10

2

10

3

10

4

10

5

10

6

10

Total number literals

Figure 4: Prime Implicates uniform random 3-CNF theory
reference comparison proper prime implicates. consider 3-CNF theories
= 30 clauses n variables (n ranging 12 28). left part Figure 4 presents
characteristics prime implicates different values n. curve describes
prime implicates distribute according length. curves correspond average
values 1500 experiments. instance, n = 28 variables, average
680 prime implicates length 7. right part Figure 4 describes size
whole set prime implicates means cumulative distribution function
total number L literals. point (x, y) curve must read
N = 1500 runs, y.N led L value smaller x. representation
convenient exhibiting exponential distributions. point stress
small formulas, median value size whole set prime implicates
already reaches ten thousand literals. rare runs (less 5%),
set prime implicates one hundred thousand literals. also observe
augmenting number variables increases difficulty since results longer
longer prime implicates produced final result. Note simple random
3CNF formula 30 clauses may already generate thousands prime implicates. Since
computing implicates random formulas known require lots subsumption checks
(Simon & del Val, 2001), final set prime implicates may hard obtain
practice.
results new (Schrag & Crawford, 1996), interesting compare
obtained similar conditions computing proper prime implicates,
described Figure 5. observe curve shapes similar previously
obtained values one order magnitude smaller. Note n = 28,
median value size whole set proper prime implicates already one
thousand literals. similarly, large values n, majority proper prime implicates long. Intuitively, one may explain phenomenon fact proper prime
implicates prime implicates initial theory augmented additional literal.
literal presumably induces clauses reductions theory consequence
subsumptions. problem thus becomes simplified much version
prime implicates problem. experiments, first conclusion that, even
301

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

3CNF m=30 clauses, 100.n experiments

3CNF m=30 clauses, 100.n experiments
1

90

Number proper prime implicates

80

70

60

Cumulative Distribution Function (CDF)

n=10 vars
n=12 vars
n=14 vars
n=16 vars
n=18 vars
n=20 vars
n=22 vars
n=24 vars
n=26 vars
n=28 vars

50

40

30

20

10

0

0.9

0.8

0.7
n=12 vars
n=16 vars
n=20 vars
n=24 vars
n=28 vars

0.6

0.5

0.4

0.3

0.2

0.1

0

2

4

6

8

10

0
0
10

12

1

10

Size proper prime implicates

2

10

3

10

4

10

5

10

Total number literals

Figure 5: Proper Prime Implicates uniform random 3-CNF theory
2+p CNF m=30 clauses, 1000 experiments

2+p CNF m=30 clauses, 1000 experiments

1

800

Number prime implicates

700

600

500

Cumulative Distribution Function (CDF)

p =0 %
p =10 %
p =20 %
p =30 %
p =40 %
p =50 %
p =60 %
p =70 %
p =80 %
p =90 %
p =100 %

400

300

200

100

0

0.9

0.8

0.7

0.6
p =0 %
p =10 %
p =20 %
p =30 %
p =40 %
p =50 %
p =60 %
p =70 %
p =80 %
p =90 %
p =100 %

0.5

0.4

0.3

0.2

0.1

0

2

4

6

8

10

0
0
10

12

Size prime implicates

1

10

2

10

3

10

4

10

5

10

Size prime implicates

Figure 6: Prime Implicates uniform random 2 + p-CNF theory (m = 30, n = 28)
small 3-CNF theories, number proper prime implicates may quite large,
may quite big.
Let us focus experiments local 2 + p CNF theories, smaller
values p, supposed better correspond encoding applications part
knowledge structured tree dag (which encoded 2-clauses). Figure
6 describes prime implicates 2 + p CNF theory = 30 clauses n = 28
variables, values p ranging 0% 100% (the curve corresponding case
p = 100% Figure 4). Similarly, Figure 7 describes characteristics
proper prime implicates different values p. previously, observe
curves similar shapes. cumulative distribution function (CDF) curves,
appears hardness (measured total size prime/proper prime implicates)
2 + p CNF formula grows exponentially value p. Even small values
p problem may hard. p increases, larger larger clauses quickly appear
result.
Figure 8 studies characteristics proper prime implicates fixed small
value p = 10%, increasing sizes theory number variables n.
302

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

2+p CNF m=30 clauses, 100.n experiments

2+p CNF m=30 clauses, 100.n experiments

1

100

Number proper prime implicates

90

80

70

Cumulative Distribution Function (CDF)

p =0 %
p =10 %
p =20 %
p =30 %
p =40 %
p =50 %
p =60 %
p =70 %
p =80 %
p =90 %
p =100 %

60

50

40

30

20

0.8

0.7

p =0 %
p =10 %
p =20 %
p =30 %
p =40 %
p =50 %
p =60 %
p =70 %
p =80 %
p =90 %
p =100 %

0.6

0.5

0.4

0.3

0.2

0.1

10

0

0.9

0

2

4

6

8

10

0
0
10

12

Size proper prime implicates

1

2

10

3

10

10

4

10

Size proper prime implicates

Figure 7: Proper Prime Implicates uniform random 2+p-CNF theory (m = 30, n = 28)
2+p CNF p=10%, 1000 experiments

2+p CNF p=10%, 100.n experiments
1

0.9

Cumulative Distribution Function (CDF)

Cumulative Distribution Function (CDF)

1

0.8

0.7

0.6

0.5
=30, n =28
=40, n =37
=50, n =46
=60, n =55
=70, n =64
=80, n =73
=90, n =82
=100, n =91

0.4

0.3

0.2

0.1

0
1
10

0.95

0.9

0.85

0.8

=30, n =28
=40, n =37
=50, n =46
=60, n =55
=70, n =64
=80, n =73
=90, n =82
=100, n =91

0.75

0.7

0.65

0.6

0.55

2

10

3

10

4

10

Size prime implicates

0.5
0
10

1

10

2

10

3

10

Size proper prime implicates

Figure 8: Size Prime Proper Prime Implicates uniform random 2+p-CNF theory
fixed p
chosen small value p focus characteristics larger theories (up
= 100 clauses). worth noticing even = 100 p = 10%, problem
seems simpler = 30 p = 100% (note right part Figure 6
y-axis rescaled [0.5 1]). kinds peer theories seem reasonable
behavior integration perspective: half queries short small
part queries still hard (the exponential distribution still observed).
5.3 Scalability Distributed Reasoning
previous section clearly shown local computation performed peer
may really hard, even small simple theories. focusing level
whole Somewhere networks, splitting/recombining strategy followed distributed
consequence finding algorithm clearly adds another source combinatorial explosion.
order evaluate scalability Somewhere networks, performed two kinds
303

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

experiments. first one aims studying behavior Somewhere networks
query processing. consists counting number messages circulating network
number peers solicited processing query. particular,
measured distribution depth query processing well potential width
query. depth processing (depth short) query maximum length
reasoning branches developed distributed algorithm returning answer
query. width query measures average number neighbors solicited
peer reasoning. second kind experiments aims evaluating
processing time number answers obtained query.
experiments, local theories 1000 peers network
generated described Section 5.1, fixed values = n = 70 = 40.
numbers close would obtain encoding taxonomies form
balanced trees, depth 3 5, class 2 3
sub-classes, extensional classes correspond leaves tree.
peer theories contains addition 10 (1 %3cnf ) q mapping clauses length 2
10 q %3cnf mapping clauses length 3. Since seen Section 5.2
proportion 3-clauses local theory strong impact hardness local
computing performed (sub)query peer, studied variations
two related parameters: q %3cnf increasing complexity. Note distributed
theories considered experiments quite large since size corresponding
centralized theories ranges 80 000 130 000 clauses 70 000 variables.
5.3.1 Behavior Distributed Query Processing
Let us study impact number q mapping clauses per edge ratio
%3cnf mapping clauses length 3, depth queries. purpose
measured, pair (q, %3cnf ), depth 1000 random queries1 . Since know (cf.
Section 5.2) local computing may sometimes hard therefore may require
lot time, necessary introduce additional timeout parameter.
query thus tagged remaining time live, which, peer, decreased
local processing time, propagating induced subqueries. experiments,
timeout value set 30 seconds.
Figure 9 shows cumulative distribution functions corresponding pair (q, %3cnf ).
point figure reports run, distinct query. four leftmost curves correspond cases query depth remains relatively small. instance, q = 2
%3cnf = 0 none 1000 queries depth greater 7. Altogether
four leftmost curves none queries depth greater 36. suggests
algorithm behaves well networks.
value %3cnf increases, queries longer depths. instance,
q = 3 %3cnf = 20, observe 22% queries depth greater
100 (the maximum 134). three rightmost curves similar shape,
composed three phases: sharp growth, corresponding small depth queries, followed
1
convenience, since time direct issue (except timeout impact),
experiments performed single computer, running 1000 peers. made building
reports relying peer traces much easier.

304

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

m=70, n=70, t=40, 1000 experiments
Cumulative Distribution Function (CDF)

1

0.9

0.8

0.7

q=2, %3cnf=0
q=2, %3cnf=20
q=3, %3cnf=0
q=3, %3cnf=5
q=3, %3cnf=10
q=3, %3cnf=15
q=3, %3cnf=20

0.6

0.5

0.4

0

20

40

60

80

100

120

140

Depth queries

Figure 9: Cumulative distribution function depth 1000 queries. q number
mapping clauses per edge, %3cnf ratio 3-clauses mappings.
scale re-centered [0.4 1.0].

plateau, slower growth. small depth query processing plateau
characteristics exponential distribution values: processing easy,
little remaining hard. slow growth observed due timeout, sideeffect bound query depth. Without timeout, previous experiments
suggest would exist queries requiring long reasoning branches.
point outlined curve corresponding hardest cases (q = 3 %3cnf = 20)
query depth 20 60. suggests hard processing
appears, hard. One may notice last case corresponds local theories
close one cases studied section 5.2. matter fact, q = 3
%3cnf = 20 local theories close 2 + p theories = 100 clauses
p = 6%.
experiments detail here, checked exponential
distribution values cannot observed acquaintance graphs strong structure ring (this corresponds rewiring probability p = 0 network generator).
exponential distribution observed random graphs (corresponding p = 1), suspect behavior due short path length
two peers, property shared random small world graphs. However, two
types graphs differ clustering coefficient, property direct impact
algorithm behavior.
depth query length history handled algorithm,
given peer appear several times since processing subqueries (resulting several
splitting clauses entailed initial query) solicit peer several times.
305

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

m=70, n=70, t=40, 20 000 experiments
Cumulative Distribution Function (CDF)

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6
q=2, %3cnf=0
q=3, %3cnf=20
q=3, %3cnf=100
q=5, %3cnf=100

0.55

0.5

0

5

10

15

20

25

Width queries

Figure 10: Cumulative Distribution Function queries width without timeouts.
curve summarise 20000 runs. scale re-centered [0.5 1.0],
X axis [0 25].

also measured integration degree queries, number distinct
peers involved query processing. observed kind exponential
distributions values depth queries, 20% smaller values.
means one fifth history peers repeated ones. phenomenon
observed random acquaintance graphs seems closely related small
world topology. important point difference small world
random graphs could observed large experimental data, large number
peers.
also studied wide reasoning propagates network peers
query processing. purpose evaluated average number neighbors
peers solicited peer solving query. estimated value
generating 20000 random queries random peers, counting them,
number induced subqueries neighbor peers. Figure 10 shows, different pairs
(q, %3cnf ), corresponding cumulative distribution functions. instance, q = 2
%3cnf = 0, 75% queries solved locally 15% remaining
ones solved asking one neighbor. q = 5 %3cnf = 100, 25%
queries solicit least 10 neighbors. course, 25% subqueries may also solicit
least 10 peers on.
summarize, experiments pointed direct impact %3cnf value
hardness query processing, surprising considering hardness
clauses length 3 prime implicates computation. experiments also suggest
exponential distribution query depths, due short path length two peers
306

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

acquaintance graphs, important repetition solicited peers, due
large clustering coefficient small world acquaintance graphs.
5.3.2 Time Number Answers
report time performance study algorithm deployed real
cluster 75 heterogeneous computers2 . Based observations previous sections,
chosen focus 5 differents kinds acquaintance graphs, denoted Easy,
Easy, Medium, Hard Hard (see Table 1). One main goals section
estimate limits processing algorithm faces hard
(and even hard) Somewhere networks. Again, experiments, set
timeout value 30s.
Network
1st ans.
10th ans.
100th ans.
1000thans.

% timeout
#answers
%unsat

Easy
q=2
%3cnf = 0
0.04s (100%)
0.06s (14.3%)


0.07s
0%
5.17
4.4%

Easy
q=3
%3cnf = 20
1.26s (99.6%)
1.37s (25.6%)
2.11s (12.7%)
4.17s (6.80%)
5.56s
13.9%
364
3.64%

Medium
q=3
%3cnf = 100
1.58s (95.6%)
0.99s (33.3%)
0.84s (27.0%)
4.59s (21.2%)
14.6s
37.5%
1006
3.76%

Hard
q=5
%3cnf = 100
1.39s (89.3%)
1.13s (12.0%)
4.09s (10.7%)
11.35s (7.15%)
21.23s
66.9%
1004
1.84%

Hard
q = 10
%3cnf = 100
2.66s (49.7%)
5.38s (29.9%)
11.0s (9.0%)
16.6s (1.80%)
27.74s
86.9%
65
1.81%

Table 1: Characteristics query processing ranging easy hard cases.

values reported Table 1 mean values three hundred different
random queries. column indicates time needed produce respectively 1st ,
10th , 100th 1000th answer query. mean time (in seconds) followed
percentage initial queries taken account average. instance,
medium case q = 3, 12.7% queries produced 100 answers,
100th answer given average 2.11 seconds (the average take
account queries produce least 100 answers). row corresponds
mean time needed produce answers, including queries lead timeout,
percentage reported %timeout row. last two rows report mean
number answers ratio proved unsatisfiable queries w.r.t. Somewhere
network (some unsatisfiable queries w.r.t. network may counted since
inconsistency might found timeout).
Unsurprisingly, timeout occurs Easy case. known satisfiability
checking prime implicates computation tractable sets clauses length 2.
Moreover, high partitioning global theory induced low value q (number
mappings per peer) often witness easy cases reasoning centralized theories.
2
computers Linux Athlon units, 1GB RAM. 26 ran 1.4GHz, 9 1.8GHz
40 2 GHz. computer running around 14 randomly assigned peers

307

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

point outline case 5 answers average,
produced quickly algorithm (in less 0.1 seconds).
Let us point that, even Medium Hard instances, algorithm produces
lot answers. instance, obtained average 1006 answers q = 3,
1004 answers q = 5. addition, hard instances, 90% runs produced
least one answer. noticeable Hard case (q = 10), half queries
produce least one answer, even 13% complete without timeout. Let
us note yet checking satisfiability corresponding centralized theories
also hard. matter fact formula corresponding centralized version
distributed theories n=70 000 variables m=120 000 clauses, 50 000
length 3. ratio 3-clauses 2 + p theories thus p = 0.416.
shown (Monasson, Zecchina, Kirkpatrick, Selman, & Troyansky, 1999) that,
2 + p random formulas, one restrict locality variables, SAT/UNSAT
transition continuous p < p0 (p0 = 0.41) discontinuous p > p0 , like 3-SAT
instances. Intuitively, p > p0 , random 2 + p-SAT problem shares characteristics
random 3-SAT problems, well known canonical NP-Complete problem.
Let us recall generation model induces high clustering variables inside
peer. Therefore cannot claim corresponding centralized theories exactly
cararacteristics uniform 2+ p-SAT random formulas. However, one focus
values parameters n, characteristics Hard network,
transition phase SAT UNSAT instance occurs m/n=1.69. Here,
m/n=1.71, close enough transition phase suspect
hard instances may found.
summarize, deployed real cluster heterogeneous computers, algorithm scales well. Even Hard instances share characteristics
large 2 + p-SAT formula crossover 2-SAT/3-SAT SAT/UNSAT
transitions, algorithm able return many answers reasonable time.

6. Related work
Section 6.1, situate work w.r.t. existing work related distributed reasoning
distributed logics, Section 6.2 summarize distinguishing points Somewhere among existing peer data management systems.
6.1 Related Work Distributed Reasoning
message passing distributed algorithm described Section 3 proceeds
splitting clauses distributing work corresponding piece clause
appropriate neighbor peers network. idea splitting formulas several
parts may found back called splitting rule natural deduction calculus,
introduced middle 1930s two independent works Gentzen (1935, 1969)
Jaskowski (1934). algorithm may viewed distributed version Ordered
Linear Deduction (Chang & Lee, 1973) produce new target clauses. principle
extended Siegel (1987) order produce implicates given clause belonging
target language, extended first order case Inoue (1992).
308

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

already pointed differences work approach Amir
McIlraith (2000). peer-to-peer setting, tree decomposition acquaintance
graph possible. addition, shown introductory example,
algorithm Amir McIlraith (2000) complete general case proper
prime implicate computation. However, Goasdoue Rousset (2003) shown
completeness guaranteed family P2PIS peer/super-peers topology.
describes encode P2PIS peer/super-peers partitioned propositional
theory order use consequence finding algorithm Amir McIlraith (2000).
model-based diagnosis framework distributed embedded systems (Provan, 2002)
based work Amir McIlraith (2000). think benefit approach
apply real peer-to-peer setting global knowledge shared.
forms distributed reasoning procedures may found multiagent frameworks,
several agents try cooperate solve complex problems. Problems addressed
way generally decomposed several interacting subproblems,
addressed one agent. case Distributed Constraint Satisfaction Problems
(DCSP, Yokoo, Durfee, Ishida, & Kuwabara, 1998). Given set variables,
associated given domain, set constraints theses variables, problem assign variable one value respective domain, way
constraints satisfied. distributed case, variable associated agent.
Constraints may either concern set variables relevant agent variables relevant different agents. first case, may considered characterizing
local theory agent, second case may assimilated mapping constraints agents. mapping constraint assigned one agent. problem
addressed DCSP easier problem consequence finding since satisfiability problem, NP-complete. centralized CSP solved using combination
backtrack search consistency techniques, algorithms used solve DCSP use asynchronous versions backtracking (Yokoo, Durfee, Ishida, & Kuwabara, 1992; Yokoo et al.,
1998) consistency techniques (Silaghi, Sam-Haroud, & Faltings, 2000). Basically, agents
proceed exchanging invalid partial affectations, converging globally consistent
solution. Similar ideas may also found distributed ATMS (Mason & Johnson, 1989),
agents exchange nogood sets order converge globally consistent set justifications. Let us note contrast peer-to-peer vision, methods aim
sharing global knowledge among agents.
Probabilistic reasoning bayesian networks (Pearl, 1988) also given rise several
adaptations suited distributed reasoning (e.g., message passing algorithm Pfeffer
& Tai, 2005). However problem addressed context different, since consists
updating set posteriori beliefs, according observed evidence set conditional
probabilities. conditional probabilities form P (x|u1 , ..., un ) describe
probability event x u1 . . . un observed. describe value
interactions viewed mappings conjunction literals single
literal, oriented nature conditioning.
Another work using oriented mappings framework distributed first order logic
(Ghidini & Serafini, 2000), collection first order theories communicate
bridge rules. approach adopts epistemic semantics connections
peer theories reflected mappings respective domains inter309

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

pretation involved peers. Based work, distributed description logics
introduced Borgida Serafini (2003) distributed tableau method described
reasoning distributed description logics proposed Serafini Tamilin
(2004b). reasoning algorithm implemented Drago (Serafini & Tamilin,
2004a) bridge rules restricted inclusion statements atomic concepts.
6.2 Related Work Peer Data Management Systems
pointed Section 4, Somewhere peer data management system
distinguishes Edutella (Nejdl et al., 2002) fact need
super-peers, Piazza (Halevy et al., 2003b, 2003a) require
central server global view overlay network.
semantic point view, Somewhereuses propositional language mappings correspond unrestricted formulas. semantics standard propositional semantics query answering always decidable. peer data management systems
investigated Halevy et al. (2003b) use relational language mappings correspond inclusion statements conjonctive queries. semantics used work
standard FOL semantics, query answering shown undecidable
general case. Restricting acyclic mappings renders query answering decidable Piazza,
checking property requires global knowledge network topology
performed central server.
peer data management system considered work Calvanese et al. (2004)
similar Halevy et al. (2003b) proposes alternative semantics based
epistemic logic. semantics shown query answering always decidable
(even cyclic mappings). Answers obtained according semantics correspond
subset would obtained according standard FOL semantics. However,
best knowledge, results implemented.
systems point view, recent work around coDB peer data management system (Franconi, Kuper, Lopatenko, & Zaihrayeu, 2004) supports dynamic networks
first step distributed algorithm let node know network topology.
contrast, Somewhere node know topology network.
KadoP system (Abiteboul, Manolescu, & Preda, 2004) infastructure based
distributed hash tables constructing querying peer-to-peer warehouses XML
resources semantically enriched taxonomies mappings. mappings considered simple inclusion statements atomic classes. Compared KadoP (and
also Drago, Serafini & Tamilin, 2004a), mapping language dealt
Somewhere expressive simple inclusion statements atomic classes.
important difference makes Somewhere able combine elements answers
coming different sources answering query, KadoP Drago cannot do.
Somewhere implements simpler setting (not implemented) vision peer
peer data management systems proposed Bernstein et al. (2002) relational model.
310

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

7. Conclusion
contributions paper theoretical practical. provided first
distributed consequence finding algorithm peer-to-peer setting, exhibited
sufficient condition completeness. developed P2PIS architecture
implements algorithm first experimental results look promising.
architecture used joint project France Telecom, aims enriching peerto-peer web applications reasoning services (e.g., Someone, Plu et al., 2003).
far, restricted algorithm deal vocabulary-based target language.
However, adapted sophisticated target languages (e.g., implicates
given, maximal, length, language based literals variables).
done adding simple tag messages encode desired target language.
Another possible extension algorithm allow compact representation
implicates, proposed Simon del Val (2001). work relies efficient
clause-distribution operator. adapted extending messages algorithm
order send compressed sets clauses instead one clause case right now,
without changing deep architecture algorithm.
Semantic Web direction, plan deal distributed RDF(S) resources
shared large scale. RDF(S) (Antoniou & van Harmelen, 2004) W3C standard
annotating web resources, think encoded propositonal setting.
distributed reasoning direction, plan consider sophisticated reasoning
order deal real multi-agent setting, possible inconsistencies
agents must handled.

Acknowledgnents
paper revised extended version ECAI short paper (Adjiman, Chatalic,
Goasdoue, Rousset, & Simon, 2004) IJCAI paper (Adjiman, Chatalic, Goasdoue,
Rousset, & Simon, 2005).

References
Abiteboul, S., Manolescu, I., & Preda, N. (2004). Constructing querying peer-to-peer
warehouses xml resources. Proceedings the2nd International VLDB Workshop
Semantic Web Databases.
Adjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2004). Distributed
reasoning peer-to-peer setting. de Mantaras, R. L., & Saitta, L. (Eds.), Proceedings ECAI 2004 (16th European Conference Artificial Intelligence), pp. 945946.
ECCAI, IOS Press.
Adjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2005). Scalability
study peer-to-peer consequence finding. Proceedings IJCAI05 (19th International Joint Conference Artificial Intelligence), pp. 351356. IJCAI.
Amir, E., & McIlraith, S. (2000). Partition-based logical reasoning. Proceedings KR00
(7th International Conference Principles Knowledge Representation Reasoning), pp. 389400, Breckenridge, Colorado, USA. Morgan Kaufmann Publishers.
311

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

Antoniou, G., & van Harmelen, F. (2004). Semantic Web Primer. MIT Press.
Arenas, M., Kantere, V., Kementsietsidis, A., Kiringa, I., Miller, R., & Mylopoulos, J.
(2003). hyperion project: data integration data coordination. ACM
SIGMOD Record, 32 (3), 5358.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). semantic web. Scientific American,
284 (5), 3543.
Bernstein, P., Giunchiglia, F., Kementsietsidis, A., Mylopoulos, J., Serafini, L., & Zaihraheu,
I. (2002). Data management peer-to-peer computing: vision. Proceedings
WebDB 2002 (5th International Workshop Web Databases).
Borgida, A., & Serafini, L. (2003). Distributed description logics: Assimilating information
peer sources. Journal Data Semantics, 1, 153184.
Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2004). Logical foundations
peer-to-peer data integration. Deutsch, A. (Ed.), Proceedings PODS 2004 (20th
Symposium Principles Database Systems), pp. 241251, Paris, France. ACM.
Chang, C. L., & Lee, R. C. (1973). Symbolic Logic Mechanical Theorem Proving.
Computer Science Classics. Academic Press.
Dechter, R., & Rish, I. (1994). Directed resolution: davis-putnam procedure revisited.
Proceedings KR94 (4th International Conference Principles Knowledge
Representation Reasoning), pp. 134145, Bonn, Germany. Morgan Kaufmann.
del Val, A. (1999). new method consequence finding compilation restricted
languages. Proceedings AAAI 1999 (16th National Conference Artificial
Intelligence), pp. 259264, Orlando, FL, USA. AAAI Press / MIT Press.
Franconi, E., Kuper, G., Lopatenko, A., & Zaihrayeu, I. (2004). Queries updates
coDB peer peer database system. Proceedings VLDB04 (30th International Conference Large Databases), pp. 12771280, Toronto, Canada. Morgan
Kaufmann.
Gentzen, G. (1935). Untersuchungen uber das logisches Schlieen.
Zeitschrift, 1, 176210.

Mathematische

Gentzen, G. (1969). Collected Works, edited M.E. Szabo. North-Holland, Amsterdam.
Ghidini, C., & Serafini, L. (2000). Frontiers Combining Systems 2, chap. Distributed
First Order Logics, pp. 121139. No. 7 Studies Logic Computation. Research
Studies Press Ltd.
Goasdoue, F., & Rousset, M.-C. (2003). Querying distributed data distributed
ontologies: simple scalable approach. IEEE Intelligent Systems, pp. 6065.
Goasdoue, F., & Rousset, M.-C. (2004). Answering queries using views: krdb perspective
semantic web. ACM Transactions Internet Technology (TOIT), 4 (3), 255
288.
Halevy, A. Y. (2000). Logic-based artificial intelligence, chap. Logic-based techniques
data integration, pp. 575595. Kluwer Academic Publishers.
312

fiDistributed Reasoning Peer-to-Peer Setting: Application Semantic Web

Halevy, A. Y., Ives, Z., Tatarinov, I., & Mork, P. (2003a). Piazza: data management infrastructure semantic web applications. Proceedings WWW 2003 (12th International World Wide Web Conference), pp. 556567. ACM Press.
Halevy, A. Y., Ives, Z. G., Suciu, D., & Tatarinov, I. (2003b). Schema mediation peer
data management systems. Dayal, U., Ramamritham, K., & Vijayaraman, T. M.
(Eds.), Proceedings ICDE03 (International Conference Data Engineering), pp.
505518, Bangalore, India. IEEE Computer Society.
Inoue, K. (1992). Linear resolution consequence finding. Artificial Intelligence, 2-3 (56),
301353.
Jaskowski, S. (1934). rules suppositions formal logic. Studia Logica, 1, 532.
Reprinted in: S. McCall (ed.), Polish Logic 19201939, Clarendon Press, Oxford, pp.
232258.
Madhavan, J., & Halevy, A. Y. (2003). Composing mappings among data sources.
Proceedings VLDB03 (29th International Conference Large Databases),
pp. 572583, Berlin, Germany. Morgan Kaufman.
Marquis, P. (2000). Handbook Defeasible Reasoning Uncertainty Management Systems, vol 5: Algorithms Defeasible Uncertain Reasoning, Vol. 5, chap. Consequence Finding Algorithms, pp. 41145. Kluwer Academic Publisher.
Mason, C., & Johnson, R. (1989). Distributed Artificial Intelligence II, chap. DATMS:
framework distributed assumption based reasoning, pp. 293317. Pitman.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). 2+p-sat:
Relation typical-case complexity nature phase transition. Random
Structure Algorithms, 15, 414435.
Nejdl, W., Wolf, B., Qu, C., Decker, S., Sintek, M., & al. (2002). Edutella: p2p networking
infrastructure based rdf. Proceedings WWW 2002 (11th International World
Wide Web Conference), pp. 604615. ACM.
Newman, M. E. J. (2000). Models small world. Journal Statistical Physics, 101,
819841.
Ooi, B., Shu, Y., & Tan, K.-L. (2003). Relational data sharing peer data management
systems. ACM SIGMOD Record, 23 (2).
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Pfeffer, A., & Tai, T. (2005). Asynchronous dynamic bayesian networks. Bacchus, F.,
& Jaakkola, T. (Eds.), Proceedings UAI05 (21th Conference Uncertainty
Artificial Intelligence).
Plu, M., Bellec, P., Agosto, L., & van de Velde, W. (2003). web people: dual view
www. Proceedings WWW 2003 (12th International World Wide Web
Conference), Vol. Alternate Paper Tracks, p. 379, Budapest, Hungary.
Provan, G. (2002). model-based diagnosis framework distributed embedded systems.
Fensel, D., Giunchiglia, F., McGuinness, D. L., & Williams, M.-A. (Eds.), Proceedings KR02 (8th International Conference Principles Knowledge Representation Reasoning), pp. 341352. Morgan Kaufmann.
313

fiAdjiman, Chatalic, Goasdoue, Rousset, & Simon

Rousset, M.-C. (2004). Small beautiful semantic web. McIlaith, S., Plexousakis, D., & van Harmelen, F. (Eds.), Proceedings ISWC 2004 (3rd International
Semantic Web Conference), Vol. 3298 Lectures Notes Computer Science, pp.
616, Hiroshima, Japan. Springer Verlag.
Schrag, R., & Crawford, J. (1996). Implicates prime implicates random 3-sat. Artificial Intelligence, 81 (1-2), 199222.
Serafini, L., & Tamilin, A. (2004a). Drago: Distributed reasoning architecture semantic web. Tech. rep., ITC-IRST.
Serafini, L., & Tamilin, A. (2004b). Local tableaux reasoning distributed description
logics.. Haarslev, V., & Moller, R. (Eds.), Proceedings DL04 (International
Workshop Description Logics), Vol. 104 CEUR Workshop Proceedings, Whistler,
British Columbia, Canada.
Siegel, P. (1987). Representation et utilisation de la connaissance en calcul propositionnel.
Ph.D. thesis, Universite dAix-Marseille II, Marseille, France.
Silaghi, M.-C., Sam-Haroud, D., & Faltings, B. (2000). Asynchronous search aggregations. Proceedings AAAI 2000 (17th National Conference Artificial
Intelligence), pp. 917922. AAAI Press / MIT Press.
Simon, L., & del Val, A. (2001). Efficient consequence finding. Nebel, B. (Ed.), Proceedings IJCAI01 (17th International Joint Conference Artificial Intelligence), pp.
359365, Seattle, Washington, USA. Morgan Kaufmann.
Tatarinov, I., & Halevy, A. Y. (2004). Efficient query reformulation peer data management
systems. Proceedings SIGMOD04 (International Conference Management
Data), pp. 539550, New York, NY, USA. ACM Press.
Watts, D. J., & Strogatz, S. H. (1998). Collective dynamics small-world networks.
Nature, 393, 440442.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction formalizing distributed problem solving. Proceedings ICDS92 (12th
IEEE International Conference Distributed Computing Systems), pp. 614621.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1998). distributed constraint
satisfaction problem: Formalization algorithms. IEEE Transactions Knowledge
Data Engineering, 10 (5), 673685.

314

fiJournal Artificial Intelligence Research 25 (2006) 119-157

Submitted 04/05; published 02/06

Learning Real-Time Search: Unifying Framework
Vadim Bulitko
Greg Lee

BULITKO @ UALBERTA . CA
GREGLEE @ CS . UALBERTA . CA

Department Computing Science
University Alberta
Edmonton, Alberta T6G 2E8, CANADA

Abstract
Real-time search methods suited tasks agent interacting initially
unknown environment real time. simultaneous planning learning problems, agent
select actions limited amount time, sensing local part environment centered agents current location. Real-time heuristic search agents select actions using
limited lookahead search evaluating frontier states heuristic function. repeated experiences, refine heuristic values states avoid infinite loops converge
better solutions. wide spread settings autonomous software hardware agents
led explosion real-time search algorithms last two decades. potential
user confronted hodgepodge algorithms, also faces choice control parameters
use. paper address problems. first contribution introduction simple three-parameter framework (named LRTS) extracts core ideas behind many existing
algorithms. prove LRTA*, -LRTA* , SLA*, -Trap algorithms special cases
framework. Thus, unified extended additional features. Second, prove
completeness convergence algorithm covered LRTS framework. Third, prove
several upper-bounds relating control parameters solution quality. Finally, analyze
influence three control parameters empirically realistic scalable domains real-time
navigation initially unknown maps commercial role-playing game well routing
ad hoc sensor networks.

1. Motivation
paper, consider simultaneous planning learning problem. One motivating application lies navigation initially unknown map real-time constraints. example,
consider robot driving work every morning. Imagine robot newcomer town.
first route robot finds may optimal traffic jams, road conditions,
factors initially unknown. passage time, robot continues learn eventually
converges nearly optimal commute. Note planning learning happen robot
driving therefore subject time constraints.
Present-day mobile robots often plagued localization problems power limitations,
simulation counter-parts already allow researchers focus planning learning
problem. instance, RoboCup Rescue simulation league (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjou, & Shimada, 1999) requires real-time planning learning multiple
agents mapping unknown terrain. Pathfinding done real time various crises, involving
fire spread human victims trapped rubble, progress agents plan.
Similarly, many current-generation real-time strategy games employ priori known maps. Full
knowledge maps enables complete search methods A* (Hart, Nilsson, & Raphael,
c
2006
AI Access Foundation. rights reserved.

fiB ULITKO & L EE

1968) Dijkstras algorithm (Dijkstra, 1959). Prior availability maps allows pathfinding
engines pre-compute various data speed on-line navigation. Examples data include
visibility graphs (Woodcock, 2000), influence maps (Pottinger, 2000), space triangulation (Kallmann, Bieri, & Thalmann, 2003), state abstraction hierarchies (Holte, Drummond, Perez, Zimmer,
& MacDonald, 1994; Holte, 1996; Botea, Muller, & Schaeffer, 2004) route waypoints (Reece,
Krauss, & Dumanoir, 2000). However, forthcoming generations commercial academic
games (Buro, 2002) require agent cope initially unknown maps via exploration
learning game, therefore greatly limit applicability complete search
algorithms pre-computation techniques.
Incremental search methods dynamic A* (D*) (Stenz, 1995) D* Lite (Koenig &
Likhachev, 2002) deal initially unknown maps widely used robotics, including
DARPAs Unmanned Ground Vehicle program, Mars rover, mobile robot prototypes (Herbert, McLachlan, & Chang, 1999; Thayer, Digney, Diaz, Stentz, Nabbe, & Hebert, 2000).
work well robots movements slow respect planning speed (Koenig, 2004).
real-time strategy games, however, AI engine responsible hundreds thousands
agents traversing map simultaneously planning cost becomes major factor. illustrate: even smaller scale six-year old Age Empires 2 (Ensemble-Studios, 1999),
60-70% simulation time spent pathfinding (Pottinger, 2000). gives rise following
questions:
1. planning time per move, particularly first-move delay, minimized
agent moves smoothly responds user requests nearly instantly?
2. Given real-time execution, local sensory information, initially unknown terrain,
agent learn near-optimal path and, time, minimize learning time
memory required?
rest paper organized follows. first introduce family real-time search algorithms designed address questions. make first contribution defining simple
parameterized framework unifies extends several popular real-time search algorithms.
second contribution lies theoretical analysis resulting framework wherein prove
convergence completeness well several performance bounds. Finally, evaluate
influence control parameters two different domains: single agent navigation unknown
maps routing ad hoc sensor networks. Detailed pseudocode needed reimplement algorithms well follow theorem proofs presented Appendix A. Theorem proofs
found Appendix B.

2. Real-time Heuristic Search Related Work
situate survey real-time heuristic search literature framework agent-centered
search (Koenig, 2001). traditional off-line search methods first plan path start
goal state move agent along path, agent-centered search interleaves planning
execution. Furthermore, planning restricted areas search space around current
state agent physical location mobile robot current board position
game. agent-centered search methods thus satisfy two requirements: (i) times,
agent single state changed via taking actions and, therefore, incurring
execution cost; (ii) agent see states around current state.
120

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

Real-time heuristic search methods subset agent-centered search methods.
distinguished following two additional properties: (i) planning time per move
upper-bounded user-supplied constant (hence real-time); (ii) associate heuristic function
state (hence heuristic). former requirement motivated need act quickly
control tasks flying helicopter (Ng, Coates, Diel, Ganapathi, Schulte, Tse, Berger, & Liang,
2004) playing real-time strategy computer game (Buro, 2002). latter property allows
agent avoid infinite cycles map learning proper distances state goal
state. Unlike learning map using planning (Stenz, 1995; Koenig & Likhachev, 2002;
Chimura & Tokoro, 1994; Sturtevant, 2005), acquiring high-quality heuristic values lets agent
select right action quickly thereby improving reaction time. heuristic values
also allows agent pick next best action primary choice become unavailable
(e.g., due road block map). Remarkably, learning state state-action values
prevailing technique Reinforcement Learning (Sutton, 2005).
Learning real-time search algorithms, LRTA* (Korf, 1990), interleave planning
execution on-line decision-making setting. planning time per action executed
agent bounded, algorithms used control policies autonomous agents, even
unknown and/or non-stationary environment (Koenig & Simmons, 1998; Koenig, 1999; Ishida
& Korf, 1991, 1995; Ishida, 1997). particular, ability make decisions smallscale local search makes algorithms well suited routing large-scale wireless networks
simple sensors actuators node aware nearby neighbors global
map exists. scenarios, limited amount computation per node suit
low computing power energy requirements, also network collectively learns
topology time (Yu, Govindan, & Estrin, 2001; Shang, Fromherz, Zhang, & Crawford, 2003).
Since pioneering LRTA* (Korf, 1990), research field learning real-time heuristic
search developed several major directions. Ishida Korf (1991) investigated modifications LRTA* non-stationary environments. Shimbo Ishida (2003) studied convergence
suboptimal solutions well mechanisms bounding amount state space exploration.
Furcy Koenig (2000) considered different learning mechanism speeding convergence.
Russell Wefald (1991) researched decision-theoretic approach balancing partial planning
execution. Shue Zamani (1993) Bulitko (2004) proposed backtracking component
suggesting yet another way control exploration environment.
Note original LRTA* algorithm viewed special case real-time dynamic programming (Barto, Bradtke, & Singh, 1995), general, real-time heuristic search methods
several notable differences reinforcement learning methods. First, usually assume
deterministic environment thus take advantage aggressive value update rules. Second, employ non-trivial initial heuristics converge even faster heuristic
admissible (Pearl, 1984) never decreasing heuristic values states. Third, real-time heuristic
search methods often use extensive sophisticated local search methods -greedy
control policy commonly used reinforcement learning. Examples include dynamically selected
lookahead search space DTA* (Russell & Wefald, 1991), additional heuristic tie-breaking
FALCONS (Furcy & Koenig, 2000), heuristic upper-bounds safe space exploration
bounded LRTA* (Shimbo & Ishida, 2003). improves quality decision-making compensating inaccuracies heuristic function speeds learning process. Finally,
backtracking extensions (Shue & Zamani, 1993; Bulitko, 2004) give agent another mechanism
maintain consistency heuristic.
121

fiB ULITKO & L EE

multitude learning real-time search algorithms (LRTA*, -LRTA* , -LRTA*, FALCONS, eFALCONS, SLA*, -Trap, SLA*T, DTA*, etc.) available user disorienting
since decide algorithm use also algorithm parameters
major impact performance. problem complicated fact
empirical studies done different test beds making results incomparable directly.
illustrate: Furcy Koenig (2000) Shimbo Ishida (2003) appear use testbed
(the 8-puzzle). Yet, results algorithm (LRTA*) differ substantially. closer inspection reveals two different goal states used leading major discrepancies
performance. compound problems, performance metrics measured
myopic lookahead search depth one. contrast game-playing practice
competitive systems gain substantial benefit deeper search horizon (Schaeffer, Culberson,
Treloar, Knight, Lu, & Szafron, 1992; Hsu, Campbell, & Hoane, 1995; Buro, 1995).
take step towards unified view learning real-time search make four contributions. First, introduce simple three-parameter framework (named LRTS) includes LRTA*,
-LRTA* , SLA* -Trap special cases. by-product generalization extension
first three algorithms variable depth lookahead. Second, prove completeness convergence combination parameters. Third, prove non-trivial theoretical bounds
influence parameters. Fourth, present large-scale empirical evaluation practically
relevant domains.

3. Problem Formulation
section, formally introduce learning real-time search problem settings metrics
using.
Definition 3.1 search space defined tuple (S, A, c, s0 , Sg , h0 ) finite set
states, finite set actions, c : R+ cost function c(s, a) incurred
cost taking action state s, s0 initial state, Sg set goal states, h0
initial heuristic (e.g., Manhattan distance).
adopt assumptions Shue Zamani (1993), Shimbo Ishida (2003) (i)
every action every state exists reverse action (possibly different cost), (ii) every
applicable action leads another state (i.e., self-loops), (iii) least one goal state Sg
reachable s0 . Assumption (i) needed backtracking mechanism used SLA* (Shue
& Zamani, 1993). holds many combinatorial path-planning problems. applicability
domains reversing action may require non-trivial sequence actions subject
future research. Assumption (ii) adopted sake proof simplicity. words,
results presented paper hold even self-loops (of positive cost) present.
real-time search algorithms unifying, assume environment stationary
deterministic. Extension algorithms dynamic/stochastic domains subject current
research.
Definition 3.2 execution cost traveling state s1 state s2 denoted dist(s1 , s2 )
defined minimal cumulative execution cost agent incur traveling s1
s2 . Throughout paper, assume dist satisfies standard triangle inequality:
s1 , s2 , s3 [dist(s1 , s3 ) dist(s1 , s2 ) + dist(s2 , s3 )]. Then, state true execution
122

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

cost h defined minimal execution cost nearest goal: h (s) = minsg Sg dist(s, sg ).
heuristic approximation h true execution cost called admissible iff [h(s) h (s)].
value h state referred heuristic value state s. assume
heuristic value initial heuristic function 0 goal state. latter assumption
holds trivially initial heuristic function admissible. depth child state state s0
reachable minimum actions (denoted ks, s0 k = d). depth neighborhood
state defined S(s, d) = {sd | ks, sd k = d}.
Definition 3.3 Search agents operate starting fixed start state s0 executing actions suggested control policies. trial defined sequence states algorithm visits
start state first goal state encounters. trial completed, agent reset
start state next trial begins. Final trial defined first trial learning (i.e.,
updates heuristic function) occurs.1 convergence run sequence trials first
trial final trial.

4. Performance Measures
problem instance fully specified search space includes start goal states.
agent run convergence following statistics collected:
execution convergence cost sum execution costs actions taken agent
convergence process (i.e., convergence run);
planning cost average cost planning per action convergence process. Planning
cost action number states agent considered decide taking action;
total convergence cost total planning convergence cost actions convergence
plus execution convergence cost scaled factor called planning speed. scaling
factor represents amount planning (measured number nodes considered)
agent would able time takes execute unit travel. instance,
computer on-board AIBO robodog may able plan 10,000 states time
takes AIBO traverse 1 foot distance ground. Correspondingly, planning
speed 10,000 states per foot. commonly used way combining execution
planning costs single metric (Russell & Wefald, 1991; Koenig, 2004);2
memory total amount memory (measured number state values) agent used
store heuristic function convergence run. Note initial heuristic
usually represented compact algorithmic form opposed table. Therefore, memory
required store heuristic values modified learning;
first-move delay (lag) amount planning time (measured milliseconds3 ) agent takes
deciding first move. important metric real-time strategy games,
1. Note random tie-breaking used, learning actually take place learning-free trial. use fixed
tie-breaking throughout paper simplicity.
2. Note unlike total planning cost, total convergence cost allows us model domains actions
expensive disproportionally running time (e.g., taking damage running wall). Additionally, use
standard real-time search framework assume planning execution simultaneous interleaved.
3. timings taken PowerMac G5 running 2GHz. Apple gcc 3.3 compiler used.

123

fiB ULITKO & L EE

wherein hundreds thousands units tasked simultaneously yet react
users commands quickly possible;
suboptimality final solution defined percentage execution cost
final trial exceeds best possible solution. instance, pathfinding agent
execution cost 120 final trial shortest path cost 100,
suboptimality 20%.

5. Application Domains
paper, illustrate algorithms two realistic testbeds: real-time navigation unknown
terrain routing ad hoc wireless sensor networks. former domain standard
single-agent search use throughout paper illustrate points. latter
domain relative newcomer field real-time heuristic search. use later
paper (Section 8) demonstrate learning single-search methods handle practically
important yet different task.
Note domains desired attributes listed introduction. Indeed,
domains, state space initially unknown, agent single current state changed via
executing actions, agent sense local part state space centered agents
current state, planning time per action minimized, repeated trials.
5.1 Real-time navigation next-generation video games
agent tasked travel start state (xs , ys ) single goal state (xg , yg ).
coordinates two-dimensional rectangular grid. state, eight moves available
leading eight immediate neighbors. straight move (i.e.,
north, south, west, east)
travel cost 1 diagonal move travel cost 2. state map
passable occupied wall. latter case, agent unable move it. Initially,
map entirety unknown agent. state (x, y) agent see status
(occupied/free) neighborhood visibility radius v: {(x0 , 0 ) | |x0 x| v & |y 0 y|
v}. initial heuristic use so-called octile distance defined length shortest
path two locations cells passable. translation Manhattan distance onto
case eight moves easily computed closed form.
Note classical A* search inapplicable due initially unknown map. Specifically,
impossible agent plan path state (x, y) unless either positioned within
visibility radius state visited state prior trial. simple solution problem
generate initial path assumption unknown areas map contain
occupied states (the free space assumption Koenig, Tovey, & Smirnov, 2003). octile
distance heuristic, initial path close straight line since map assumed
empty. agent follows existing path runs occupied state. travel,
updates explored portion map memory. current path blocked, A*
invoked generate new complete path current position goal. process
repeats agent arrives goal. reset start state new trial begins.
convergence run ends new states seen.4
4. Note framework easily extended case multiple start goal states.

124

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

increase planning efficiency, several methods re-using information subsequent planning episodes suggested. Two popular versions D* (Stenz, 1995) D* Lite (Koenig
& Likhachev, 2002). Powerful are, enhancements reduce first-move lag time.
Specifically, agent given destination coordinates, conduct A* search
position destination move. Even small maps, delay substantial.
contrast LRTA* (Korf, 1990), performs small local search select
first move. result, several orders magnitude agents respond users request
time takes one A* agent.
present-day games circumvent problem using fully observable maps precomputing auxiliary data structures beforehand, conduct experiments research prototype, called Hierarchical Open Graph (HOG), developed University Alberta Nathan
Sturtevant. allows one load maps commercial role-playing real-time strategy games
Baldurs Gate (BioWare, 1999) WarCraft III (Blizzard, 2002). use five maps ranging
size 214 192 states (of 2765 traversable) 235 204 (with 16142 traversable
states). largest map shown Figure 1. total 1000 problem instances five maps
randomly chosen shortest path lengths fall ten bins: 1-10, 11-20, . . . , 91-100
100 problems per bin. use suite 1000 problems throughout paper.

Figure 1: Left: one five test maps Baldurs Gate commercial role-playing game
BioWare. Right: close-up showing agent dot middle.

6. LRTS: Unifying Framework
section introduce primary contribution paper unifying framework
real-time heuristic search incremental fashion. Namely, start base
algorithm, LRTA*, analyze three extensions: deeper lookahead, optimality weight,
backtracking. extension illustrated hand-traced micro-example empirical results
real-time pathfinding domain. unifying algorithm constitutes sections finale.
6.1 Learning Real-Time A* (LRTA*)
LRTA* introduced Korf (1990), first best known learning real-time heuristic search
algorithm . key idea lies interleaving acting backing heuristic values (Figure 2).5
5. clarity, pseudocode paper describes single trial only.

125

fiB ULITKO & L EE

Specifically, current state s, LRTA* lookahead one considers immediate neighbors (lines 4-5 Figure 2). neighbor state, two values computed: execution cost
getting current state (henceforth denoted g) estimated execution cost
getting closest goal state neighbor state (henceforth denoted h). g known
precisely, h heuristic estimate. LRTA* travels state lowest f = g + h value
(line 7). Additionally, updates heuristic value current state minimum f -value
greater heuristic value current state (line 6).6
LRTA*
1 initialize heuristic: h h0
2 reset current state: sstart
3 6 Sg
4
generate children one move away state
5
find state s0 lowest f = g + h
6
update h(s) f (s0 ) f (s0 ) greater
7
execute action get s0
8 end
Figure 2: LRTA* algorithm lookahead one.
Korf (1990) showed finite domains goal state reachable state
action costs non-zero, LRTA* finds goal every trial. Additionally, initial heuristic
function admissible LRTA* converge optimal solution finite number
trials. Compared A* search, LRTA* lookahead one considerably shorter firstmove lag finds suboptimal solutions much faster. However, converging optimal (e.g.,
lowest execution cost) solution expensive terms number trials total
execution cost. Table 1 lists measurements averaged 1000 convergence runs five game
maps pathfinding domain (details Section 5.1). differences become pronounced
larger problems (Figure 3).
Table 1: LRTA* lookahead one vs. A* pathfinding.
Algorithm
A*
LRTA*

First-move lag
3.09 ms
0.05 ms

Convergence execution cost
159
9346

6.2 Extension 1: Deeper Lookahead Search
LRTA* follows heuristic landscape local minimum. avoids getting stuck raising
heuristic values eventually eliminating local minimum. Thus, local heuristic minima,
caused inaccuracies initial heuristic values, eliminated process learning.
Ishida (1997) referred inaccuracies heuristic heuristic depressions. studied
basic case LRTA* lookahead one. Heuristic depressions later generalized case weighted LRTA* arbitrary lookahead (Bulitko, 2004) name
-traps.
6. condition necessary heuristic consistent f -values non-decreasing along lookahead
branches. general, need decrement h-value state initial heuristic admissible.

126

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

4

8

3.5

LRTA*
A*

6
5
4
3
2

A*
2.5
2
1.5
1
0.5

1
0

LRTA*

3
Convergence execution cost

First move lag (ms)

7

x 10

0

10

20

30

40
50
60
70
Shortest solution length

80

90

100

0

0

10

20

30

40
50
60
70
Shortest solution length

80

90

100

Figure 3: Differences LRTA* A* pathfinding problems scale up.
process filling heuristic depressions, agent incur substantial execution cost
moving back forth inside depression. happens due fact LRTA*
lookahead one myopic conducts minimum amount planning per move. planning
cheaper execution, natural solution increase amount planning per move
hope eliminating heuristic local minima lower execution cost.
LRTA*, additional planning per move implemented deeper lookahead search.
heuristic update action selection rules introduced previous section extended
arbitrary lookahead depth manner inspired standard mini-max search games (Korf,
1990). Korf called new rule mini-min empirically observed deeper lookahead decreases execution cost increases planning cost per move. phenomenon illustrated hand-traceable example Figure 4. six states shown circle
actions shown edges. action cost 1. initial heuristic h0 admissible
inaccurate. Heuristic values first trial shown numbers state
(circle).
goal

start

goal

h0:

0

1

2

1

1

3

h1:

0

1

2

3

3

3

5 moves

start

h0:

0

1

2

1

1

3

h1:

0

1

2

3

1

3

3 moves

Figure 4: Initial final heuristics LRTA* lookahead one (left) two (right).
LRTA* lookahead one two converge final heuristics one trial. However,
additional ply lookahead takes advantage two extra heuristic values and, result,
reduces execution cost 5 3 moves. hand, planning cost move
increases 2 4 nodes.
general, planning per action compensates inaccuracies heuristic function
allows one select better moves. Table 2 demonstrates effect averaged 1000
pathfinding problems. reduction execution cost due deeper lookahead becomes
pronounced larger problems (Figure 5).
Lookahead search LRTA*-like algorithms received considerable attention literature.
Russell Wefald (1991) Koenig (2004) analyzed selection lookahead depth optimal
terms total cost (i.e., weighted combination planning execution costs). Bulitko,
127

fiB ULITKO & L EE

Table 2: Effects deeper lookahead LRTA*.
Lookahead
1
3
5
7
9

First move lag
0.05 ms
0.26 ms
0.38 ms
0.68 ms
0.92 ms

Execution convergence cost
9346
7795
6559
5405
4423

4

Convergence execution cost

3.5

x 10

d=1

3

d=3
d=5
d=7

2.5
2

d=9

1.5
1
0.5
0

0

20

40

60
Solution length

80

100

Figure 5: Convergence LRTA* different lookaheads problems scale up.
Li, Greiner, Levner (2003) Bulitko (2003) examined pathological cases deeper lookahead
increasing execution planning costs.
6.3 Extension 2: Heuristic Weight
Scalability LRTA* large problems determined two attributes. First, uses
initial optimistic heuristic function unknown areas search space informed,
realistic function explored areas. results so-called optimism face uncertainty agent eagerly explores novel parts space. Consequently, convergence
time lengthened solution quality oscillates significantly consecutive trials (Shimbo
& Ishida, 2003). Second, complete convergence process intractable terms running
time amount memory required store heuristic optimal solutions sought.
Indeed, even simple problems intractable terms finding optimal solutions (e.g.,
generalized sliding tile puzzle Ratner & Warmuth, 1986).
weighted version LRTA* proposed Shimbo Ishida (2003) systematic way
giving optimality converged solution. authors argued optimal solutions
rarely needed practice, achieved large domains. Consequently, user must
settle suboptimal satisficing solutions. Weighted LRTA* algorithm (LRTA*)
run inadmissible initial heuristic. inadmissibility bounded every state h(s)
value required upper-bounded (1 + )h (s) h (s) true distance goal.
resulting -admissibility leads -optimality solution -LRTA* converges.
giving optimality -controlled fashion, algorithm gains faster convergence, smaller
memory requirements, higher stability. Note 0-LRTA* equivalent original LRTA*.
Shimbo Ishida (2003) used Manhattan distance scaled (1 + ) initial heuristic
h0 . underlying idea illustrated five-state domain (Figure 6). left, initial
128

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

heuristic listed immediately five states (shown circles). action (shown
edge) execution cost 1. Every trial LRTA* involve four moves update
heuristic values. values trial listed successive rows. 16 moves (4
trials), LRTA* converge perfect heuristic. right, start heuristic
multiplied (1+) = 2. shown there, 1-LRTA* takes one trial (i.e., 4 moves) converge.
goal

initial heuristic:

final heuristic:

start

0

0

1

1

2

0

1

1

2

2

goal

4 moves

start

initial heuristic:

0

0

2

2

4

final heuristic:

0

1

2

3

4

4 moves

4 moves

0

1

2

2

3

0

1

2

3

3

4 moves

0

1

2

3

4

4 moves

Figure 6: Heuristic values successive trials of: LRTA* (left) 1-LRTA* (right).
general, scaling initial (admissible) heuristic (1 +), amount underestimation
admissible initial heuristic relative perfect heuristic reduced many states. Therefore,
number updates (line 6 LRTA*, Figure 2) needed increase initial heuristic value
state final value reduced. Correspondingly, execution convergence cost lowered
learning sped up. Figure 7 illustrates correlation pathfinding domain. discrepancy
amount underestimation initial heuristic (1 + )h0 averaged states map
hand (s S):
avg
sgoal , sS

h (s, sgoal )(1 + )h0 (s, sgoal )
.
h (s, sgoal )

(6.1)

denotes non-negative subtraction: ab equals b result positive 0 otherwise.
usual, h perfect heuristic h0 octile distance. heuristics taken
respect 1000 random goal states (sgoal ) defined Section 5.1.

Convergence execution cost

10000

8000

6000

4000

2000

0

0

2

4

6

8
10
12
Discrepancy (%)

14

16

18

20

Figure 7: Convergence cost -LRTA* vs. weighted initial heuristic discrepancy. points
graph correspond values (1 + ): 3.3, 2.0, 1.5, 1.1, 1.0.
negative side, scaling uniform states sometimes result scaled
values exceeding perfect heuristic values. inadmissibility lead suboptimal solutions
129

fiB ULITKO & L EE

illustrated Figure 8. Specifically, initial heuristic admissible (the values five states
shown circles left part figure). actions (shown arrows)
execution cost 1. Thus, LRTA* converges optimal path (i.e., top state).
hand, scaling heuristic (1 + ) = 3 (shown right) leads 2-LRTA* converge
longer suboptimal path (through bottom two states).
3

1
2

6

0

0

Start

Start

0

0

Goal

0

Goal

0

Figure 8: 2-LRTA* converges suboptimal path.
summary, scaling initial (admissible) heuristic 1 + tends reduce underestimation error heuristic thereby speeding convergence. weighted heuristic becomes
progressively overestimating, suboptimality final solutions increases and, eventually,
overall convergence slows (due less informative weighted heuristic). Table 3 illustrates
trends pathfinding domain. again, effect pronounced larger problems
(Figure 9).
Note similar effect observed weighted A* search wherein weight w
put heuristic h. w exceeds 1, fewer nodes expanded cost suboptimal
solutions (Korf, 1993). Table 4 lists results running weighted A* pathfinding problems
used Table 3.
x 10

6

1+ = 1.25
1.5

1+ = 1.25
1+ = 5

5

1+ = 5

Suboptimality (%)

Convergence execution cost

4

2

1

0.5

4
3
2
1

0

0

20

40
60
Solution length

80

100

0

0

20

40
60
Solution length

80

100

Figure 9: Impact heuristic weight pathfinding problems scale up.
6.4 Extension 3: Backtracking
LRTA* -LRTA* learn updating heuristic function advancing state
space. SLA* (Shue & Zamani, 1993) introduced backtracking mechanism. is, upon making
update heuristic value current state, agent backtracks previous state.
provides agent two opportunities: (i) update heuristic value previous state
(ii) possibly select different action previous state. alternative scheme would update
130

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

Table 3: Effects scaling initial heuristic -LRTA* .
Heuristic weight (1 + )
10.0
5.0
3.3
2.0
1.5
1.1
1.0 (LRTA*)

Execution convergence cost
2832
2002
1908
2271
3639
6520
9346

Suboptimality
3.17%
2.76%
2.20%
1.52%
0.97%
0.25%
0.00%

Table 4: Effects scaling initial heuristic weighted A*.
Heuristic weight
10.0
5.0
3.3
2.0
1.5
1.1
1.0 (A*)

Planning convergence cost
1443
1568
1732
2074
3725
5220
10590

Suboptimality
3.48%
3.08%
2.37%
1.69%
1.16%
0.30%
0.00%

heuristic values previously visited states memory (i.e., without physically moving agent
there). approach exploited, example, temporal difference methods eligibility
traces (Watkins, 1989; Sutton & Barto, 1998). give agent opportunity (ii) above.
Naturally, backtracking moves counted execution cost.
goal

initial heuristic:

final heuristic:

start

0

1

1

2

3

0

1

2

2

3

0

1

2

3

3

0

1

2

3

4

goal

4 moves

start

initial heuristic:

0

1

1

2

3

final heuristic:

0

1

2

3

4

8 moves

4 moves
4 moves

Figure 10: Heuristic values successive trials of: LRTA* (left) SLA* (right).
underlying intuition illustrated simple example Figure 10. again,
consider one-dimensional five-state domain. action execution cost one. initial
heuristic accurate left two states one lower right three states. trial,
LRTA* raise heuristic value single state. Therefore, three trials (12 moves) needed
make heuristic perfect. SLA*, hand, gets middle state 2 moves, updates
value 1 2, backtracks second state right, increases value 2 3,
backtracks right state, increases value 3 4. agent take 4
moves towards goal, following perfect heuristic. result, first trial longer (8
vs. 4 moves) overall number moves convergence reduced (from 12 8).
SLA* algorithm nearly identical LRTA* lookahead one (Figure 2).
difference upon increasing state value line 6, agent takes action leading
131

fiB ULITKO & L EE

previous state state s0 line 7 LRTA*. previous state exist (i.e.,
algorithm reached start state) action taken heuristic value still updated.
SLA* backtracks every time heuristic updated (i.e., learning took place). causes
substantial execution cost first trial. fact, prove Theorem 7.8, learning
occurs first trial. larger problems, user may want suboptimal solution
convergence process completes. address problem, SLA*T, introduced (Shue, Li, &
Zamani, 2001), gives user control amount learning done per trial. Namely,
SLA*T backtracks exhausts user-specified learning quota.
two primary effects backtracking metrics consider paper. First,
larger values learning quota decrease execution cost trial less backtracking occurs.
extreme, learning quota set infinity, transforming SLA*T backtracking-free
LRTA* lookahead one. end spectrum (with learning quota zero),
SLA*T always backtracks upon updating heuristic value becomes equivalent SLA*.
backtracking tends speed convergence process (i.e., decrease convergence execution
cost) fewer trials needed converge (recall example Figure 10). Empirically,
first effect backtracking clearly seen 8-puzzle (Table 5) well pathfinding
task small maps (Table 6). still open question trend observed larger
pathfinding tasks (Table 7).
Table 5: Effects backtracking 8-puzzle.
Learning quota
0 (SLA*)
7
13
(LRTA*)

First trial execution cost
1846
424
375
371

Convergence execution cost
2321
52766
57816
58002

Memory required
728
18751
20594
25206

Table 6: Effects backtracking pathfinding domain small maps.
Learning quota
0 (SLA*)
10
50
1000
(LRTA*)

First trial execution cost
434
413
398
390
235

Convergence execution cost
457
487
592
810
935

second effect backtracking reduction amount memory required store
learned heuristic values.7 backtracking, agent tends update heuristic values previously visited states. updates require additional memory allocated. memorysaving effect seen across problem sizes Table 5 Table 7.
6.5 Combining Extensions
previous sections introduced three extensions base algorithm LRTA*. Table 8 lists six
algorithms use extensions. first entry summarizes arbitrary-depth LRTA*
7. Recall initial heuristic values stored table come effectively computable function
Manhattan distance.

132

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

Table 7: Effects backtracking pathfinding domain large maps.
Learning quota
0 (SLA*)
10
100
1000
10000
(LRTA*)

Convergence execution cost
9741
10315
11137
10221
9471
9346

Memory required
174
178
207
258
299
303

Table 8: Real-time heuristic methods compared along several dimensions.
Algorithm
LRTA*
-LRTA*
SLA*
SLA*T
-Trap
LRTS

Lookahead
arbitrary
arbitrary
one ply
one ply
arbitrary
arbitrary

Learning rule
mini-min, frontier
mini-min, frontier
mini-min, frontier
mini-min, frontier
max min, states
max min, states

Execution
one action
one action
one action
one action
actions
actions

Optimality
optimal
-optimal
optimal
optimal
-optimal
-optimal

Backtracking
none
none
always
controlled
always
controlled

uses mini-min backup rule propagate heuristic values search frontier interior states lookahead tree. executes one action converges optimal solution.
backtracking used. Weighted LRTA* (-LRTA* ) converges -optimal solution identical LRTA* columns table. SLA* SLA*T myopic versions LRTA*
use backtracking: unlimited controlled learning quota respectively. Backtracking
independently introduced another algorithm, called -Trap (Bulitko, 2004), also used
heuristic weight parameter = 1/(1 + ) effect weighted LRTA*. Additionally,
-Trap employed aggressive heuristic update rule, denoted max min table,
used heuristic values states (as opposed frontier lookahead search only). Instead
taking single action lookahead search episodes, applied actions amortize
planning cost.
combine three extensions single algorithm called Learning Real-Time
Search LRTS. shown last row table and, simplified fashion, Figure 11.
detailed pseudo-code necessary re-implement algorithm well follow theorem
proofs found Figure 20, Appendix A. step operation LRTS
explain control parameters.
line 1, trial initialized setting current state agent start state sstart
resetting amount learning done trial (u). long goal state sgoal reached,
LRTS agent interleaves planning (lines 3 4), learning (line 5), execution (lines 6-11).
planning state, LRTS uses model environment (implicitly updated new
states comes agents radius visibility) generate child states current state
moves away. lookahead level (i = 1, 2, . . . , d), LRTS finds promising state
(si ) minimizes weighted estimate total distance:
si = arg min

s0 S(s,i)


g(s0 ) + h(s0 ) ,
133

(6.2)

fiB ULITKO & L EE

g(s0 ) shortest distance current state child s0 h(s0 ) heuristic
estimate distance child state s0 goal state sgoal . Throughout paper
breaking ties systematic fashion detailed later paper. Note distance g(s0 )
current state child state s0 weighted control parameter (0, 1].
behavior obtained multiplying initial heuristic 1+ = 1/ -LRTA* except
makes initial heuristic inadmissible (Theorem 7.2).
LRTS(d, , )
1 initialize: sstart , u 0
2 6 Sg
3
generate children moves away, = 1 . . .
4
level i, find state si lowest f = g + h
5
update h(s) max f (si ) greater
1id

6
increase amount learning u |h|
7
u
8
execute moves get state sd
9
else
10
execute moves backtrack previous state, set u =
11 end
12 end
Figure 11: LRTS algorithm combines three extensions LRTA*.
lookahead search conducted line 5 complexity O(bd ) child state reached
b branching factor. maps, however, complexity much reduced
O(dn ) n dimensionality map. Thus, path-planning sensor
network routing task, lookahead complexity O(d2 ).
Like -Trap , LRTS uses so-called max min heuristic update (or learning) rule line 5.
Specifically, initial heuristic admissible, agent want increase aggressively possible keeping admissible. Lemma 7.1 prove setting heuristic value
h current state maximum minima found ply preserves admissibility
h times.
line 6 amount increase heuristic value current state (h(s)) added
cumulative amount learning done far trial (u). new amount learning
exceed learning quota agent execute moves promising frontier
state sd seen lookahead (line 8). moves added path
undone later backtracking. new amount learning exceeds learning quota
agent backtrack previous state (line 10). so, undo last
actions. last actions thus removed path traveled far trial. Note
backtracking actions count execution cost.
encourage reader revisit hand-traceable examples found Sections 6.2,
6.3, 6.4 show LRTS operation simple domains. weighting example (Section 6.3),
set 1/(1+) enable LRTS behave -LRTA* . concludes presentation
LRTS algorithm highlight properties theoretical empirical results.
134

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

7. Theoretical Results
LRTS framework includes LRTA*, weighted LRTA*, SLA*, -Trap special cases (Table 8). fact presented formally following theorems. proofs found Appendix B. Note following assumptions mandatory real-time search,
needed proofs. Specifically, search space assumed deterministic stationary. Additionally, agents current state affected agents actions. Finally, agent
possesses sufficient knowledge environment conduct local-search (i.e., lookahead)
depth current state.
Theorem 7.1 LRTS(d = 1, = 1, = ) equivalent LRTA* lookahead one.
1
, = ) initialized admissible heuristic h0 equivTheorem 7.2 LRTS(d = 1, = 1+
alent -LRTA* initialized (1 + )h0 .

Theorem 7.3 LRTS(d = 1, = 1, = 0) equivalent SLA*.
Theorem 7.4 LRTS(d, , = 0) equivalent -Trap(d, ).
past, convergence completeness proven special cases (LRTA*, LRTA* , SLA*). prove properties valid values lookahead
N, 1, heuristic weight R, (0, 1], learning quota R {}, 0.
Definition 7.1 tie-breaking scheme employed two states equal f = g + h
values. Among candidate states equal f -values, random tie-breaking selects one random.
scheme used (Shimbo & Ishida, 2003). Systematic tie-breaking used Furcy
Koenig (2000) uses fixed action ordering per current state. orderings are, however, generated
randomly search problem. Finally, fixed tie-breaking always uses action ordering.
use latter scheme experiments simplest.
Lemma 7.1 (Admissibility) valid T, , admissible initial heuristic h0 , heuristic function h maintained admissible times LRTS(d, , ) search.
Theorem 7.5 (Completeness) valid T, , admissible initial heuristic h0 ,
LRTS(d, , ) arrives goal state every trial.
Theorem 7.6 (Convergence) valid T, , admissible initial heuristic h0 ,
LRTS(d, , ) systematic fixed tie-breaking converges final solution finite
number trials. makes zero updates heuristic function final trial. subsequent
trial identical final trial.
Theorem 7.7 (Suboptimality) valid T, , admissible initial heuristic h0 , ex
0)
ecution cost final trial LRTS(d, , ) upper-bounded h (s
. words,
suboptimality final solution worse 1 1.


non-trivial upper-bound h (s0 )+T imposed solution produced LRTS
first trial. requires, however, minor extension LRTS algorithm. Thus, list
extension theorem Figure 21, Appendix Theorem B.1, Appendix B respectively.
135

fiB ULITKO & L EE

Theorem 7.8 (One trial convergence) valid , admissible initial heuristic h0 ,
second trial LRTS(T = 0) systematic fixed tie-breaking guaranteed final. Thus,
learning exploration done first trial.

8. Combinations Parameters
introduced three-parameter algorithm unifying several previously proposed extensions
base LRTA*. also demonstrated theoretically empirically influence
parameter independently two parameters Sections 6.2, 6.3, 6.4. summary
influences found Table 9. up-arrow/down-arrow means increase/decrease
metric. Notation p b indicates parameter p increases b. illustrate:
intersection row labeled convergence execution cost column labeled 1
states convergence execution cost decreases goes up.
Table 9: Summary influences LRTS parameters pathfinding domain.
metric / parameter
convergence execution cost
planning cost per move
convergence memory
first-move lag
suboptimality final solution

1d




small

01
small , large
influence

influence


0T
small , large
influence

influence
influence

8.1 Interaction among parameters
section consider effects parameter combinations. Figure 12 shows impact
lookahead depth function heuristic weight learning quota . Specifically,
value , left plot shows difference convergence execution costs
lookahead values = 1 = 10:
impact-of-d(, ) = execution-cost(d = 1, , ) execution-cost(d = 10, , ).
Reduction conv. execution cost d=1 d=10
1

Reduction conv. memory d=1 d=10

4

x 10
3

1

2.5

Heuristic weight ()

0.7
2
0.6
1.5

0.5
0.4

1
0.3
0.2

0.5

Heuristic weight ()

0.9
0.8

0.9

160

0.8

140

0.7

120

0.6

100

0.5

80

0.4
60
0.3
40

0.2

20

0.1

0.1
0

5

10

50 100 500 1000 5000 100001e+13
Learning quota (T)

(8.1)

0

5

10

50

100 500 1000 5000 10000 1e+13
Learning quota (T)

Figure 12: Impact lookahead different values heuristic weight learning quota .
Brighter shades indicate higher impact increasing 1 10. Likewise, right plot
demonstrates impact memory required convergence. Brighter shades indicate
136

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

stronger impact (i.e., larger differences). point averaged 1000 convergence
runs pathfinding domain. observe larger values learning quota higher values
heuristic weight magnify reduction memory due increasing lookahead depth. Note
plot show memory requirements different values rather reduction
memory requirements increases 1 10.
Figure 13 shows impact function conditions. Learning
quota seem affect impact suboptimality final solution. However, deeper
lookahead make less effective respect. believe deeper lookahead
compensates inaccuracies heuristic function. Therefore, weighting heuristic
makes less difference.
Reduction suboptimality gamma=0.3 gamma=1.0

Reduction conv. execution cost gamma=1.0 gamma=0.3

10

6000

9

5000

9

8

4000

8

7

3000

6

2000

Lookahead depth (d)

Lookahead depth (d)

10

1000

5

0

4

2.1
2
1.9

7
1.8
6
1.7
5
1.6

4

1000

1.5

3

3
2000
2

1.4

2

3000

1.3

1

1
0

5

10

50

0

100 500 1000 5000 10000 1e+13
Learning quota (T)

5

10

50 100 500 1000 5000 100001e+13
Learning quota (T)

Figure 13: Impact heuristic weight different values lookahead learning quota .
Figure 14 shows impact function 1000 pathfinding problems.
Reducing learning quota 500 0 results increased amount backtracking.
discussed previously, backtracking tends save memory speed convergence.
figure shows trends demonstrates affect impact backtracking. left
plot shows backtracking effective speeding convergence lower heuristic
weights ( = 0.1). contrary, right plot indicates higher heuristic weights (close
= 1) make backtracking effective reducing memory required convergence.
Reduction conv. execution cost T=500 T=0
10

Reduction conv. memory T=500 T=0

4

x 10
1

10

0.5

7
0

6
5

0.5

4

Lookahead depth (d)

Lookahead depth (d)

8

50

8
7

40

6
30

5
4

20

3

3
1

2
1
0.1

60

9

9

0.2

0.3

0.4 0.5 0.6 0.7
Heuristic weight ()

0.8

0.9

1

10

2
1
0.1

0.2

0.3

0.4
0.5 0.6 0.7
Heuristic weight ()

0.8

0.9

1

Figure 14: Impact learning quota different values heuristic weight lookahead d.
137

fiB ULITKO & L EE

Overall, influences non-trivial and, according experiments, domain-specific.
explained readily, others appear result interaction three
parameters structure problem space. Consequently, selection optimal parameters
specific performance metric concrete domain presently matter trial error.
manual tuning control parameters typical scenario Artificial Intelligence, future
research investigate automatic parameter adjustment.
8.2 Minimizing performance measures
section, list parameter combinations optimizing performance measures
explain underlying rationale. best combinations parameters sought following
parameter space: lookahead {1, 2, . . . , 10}, heuristic weight {0.1, 0.2, . . . , 1.0},
learning quota {0, 5, 10, 50, 100, 500, 1000, 5000, 10000, }. one thousand
combinations parameters, ran LRTS convergence test suite 1000 pathfinding
problems described Section 5.1. results shown Table 10 explained below.
Table 10: LRTS control parameters minimizing performance metrics.
Metric
execution convergence cost
planning convergence cost
planning cost per move
total convergence cost (speed < 185.638)
total convergence cost (speed 185.638)
suboptimality
memory

Lookahead
10
1
1
1
10

10

Heuristic weight
0.3
0.3

0.3
0.3
1.0
0.3

Learning quota






0

Execution convergence cost lowest quality individual moves highest.
Thus, deeper lookahead leading planning per move favored. High values undesirable LRTS seeks near-optimal solutions takes longer converge. low values
also undesirable weighted initial heuristic becomes less less informative (Table 3).
Hence, best value lays somewhere middle (0.3). Finally, backtracking preferred
convergence fastest larger maps used experiment (Table 7).
Planning convergence cost defined total planning cost moves convergence
achieved. Deeper lookahead leads faster convergence quadratically increases planning
cost every move (cf., Section 6.5). Hence, overall, shallow lookahead preferred. also
explains optimal parameter combination minimizing planning cost per move LRTS
depends only.
Total convergence cost. minimize weighted sum planning execution
costs. common objective previously studied (Russell & Wefald, 1991; Koenig, 2004).
adopt settings latter bring two metrics (the execution cost planning
cost) single scalar expressed number states. Namely, total convergence cost
defined convergence execution cost multiplied planning speed plus total planning
cost (Section 4). instance, agent converged 1000 units execution cost, considered
total 5000 states planning (i.e., average five states per unit execution cost),
planning speed 200 (i.e., 200 states considered planning one unit travel
executed), total convergence cost 1000200+5000 = 205000, clearly dominated
138

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

execution component. planning per move tends decrease convergence execution cost
thus preferred planning speed high (i.e., planning cheap relative execution
execution cost dominates total cost). Conversely, less planning per move preferred
planning speed low. Russell Wefald (1991) Koenig (2004) used
reasoning select optimal lookahead depth LRTA*-based real-time search algorithms.
pathfinding domain, best values fastest convergence 0.3
discussed above. best value lookahead depends planning speed: deeper lookahead (10
vs. 1) preferred planning speed sufficiently high planning per move
afforded.
Suboptimality zero = 1 regardless .
Memory. optimal parameter combination (10, 0.3, 0) consistent previous analysis: backtracking (T = 0) reduces memory consumption, lower heuristic weight leads
less exploration (since LRTS satisfied suboptimal solutions), since heuristic values
stored actions apart, higher lookahead preferred.
Adjusting LRTS control parameters allows agent achieve convergence
previously impossible due memory limitations. following, adopt settings Shimbo
Ishida (2003) Bulitko (2004) consider one hundred 15-puzzles first published Korf
(1985). known FALCONS LRTA* lookahead one unable converge
optimal solutions 100 puzzles memory limited 40 million states (Shimbo &
Ishida, 2003). hand, convergence suboptimal solutions achieved
-LRTA* (Shimbo & Ishida, 2003) -Trap (Bulitko, 2004).
Table 11: Convergence Korfs one hundred 15-puzzles four million states memory.
Algorithm
FALCONS
LRTA*
LRTS
LRTS
LRTS


1
2
4



0
0

max
0.29
0.6
0.7

Suboptimality
convergence problem
convergence problem
50%
10%
4%

make task challenging reducing memory limit ten fold forty four
million states. unable find set d, T, parameters allowed LRTS
converge optimal solutions 100 instances, algorithm converge solutions
were, average, 4% worse optimal 100 puzzles (Table 11). Lower values
higher values increase memory cost efficiency allow increase leads
higher quality solutions.
summary, adjusting control parameters LRTS significantly affects performance.
analysis individual combined influence parameters several practically relevant
performance metrics gives practitioner better intuition application-specific parameter tuning.
Perhaps importantly, opens interesting line research automatic selection optimal
parameters basis effectively measurable properties domain target performance
metric.
139

fiB ULITKO & L EE

8.3 Application Domain #2: Routing Sensor Networks
second application LRTS considered routing ad hoc wireless sensor networks. Sensor
nodes generally limited computational power energy, thus simple, energy efficient routing
routines typically used within sensor networks. Various applications exist, including military
(Shang et al., 2003), environmental monitoring (Braginsky & Estrin, 2002) reconnaissance
missions (Yan, He, & Stankovic, 2003). such, routing sensor networks interest large
community users.
Distance Vector Routing (DVR) algorithm widely used network routing (Royer & Toh,
1999). particular, DVR employed within Border Gateway Protocol used route traffic
entire Internet. DVR, network node maintains heuristic estimate distance
destination/goal node. observe DVR conceptually similar LRTA*
lookahead one. Thus, applying LRTS network routing domain essentially equivalent
extending DVR heuristic weight backtracking mechanism controlled
learning quota . Note analogue deeper lookahead DVR since network
node learn explicit model network and, consequently, aware network
nodes beyond immediate neighbors. Hence, limited one.
order demonstrate impact two DVR extensions, chose setup used
recent application real-time heuristic search network routing (Shang et al., 2003). Specifically,
network sensor nodes assumed dropped sky monitor territory, landing
random dispersion, thus necessitating ad hoc routing order allow data transmissions.
consider case fixed destination transmission, simulating source node
providing information central information hub (e.g., satellite up-link).
Note LRTA* pathfinding domain access heuristic values neighbors
extra cost since heuristic values stored centralized memory. case
ad hoc networks, wherein node maintains estimate number hops goal node
(the h heuristic). naive application LRTA* would query neighbor nodes order retrieve
h values. would, however, substantially increase amount network traffic, thereby
depleting nodes energy sources faster making network easily detectable. adopted
standard solution node stores heuristic value also last known
heuristic values immediate neighbors. Then, whenever node updates heuristic value,
notifies neighbors change broadcasting short control message. last major
difference pathfinding nodes know either geographical position
coordinates destination node. Thus, Euclidian distance available initial
heuristic initialized one nodes except destination node set zero.
experiments reported below, N sensor nodes randomly distributed X
two-dimensional grid two nodes occupy location. node fixed transmission radius determines number neighbors node broadcast messages (and
whose heuristic values store). specify single problem instance, distinct start (source)
node goal (destination hub) nodes randomly selected N nodes. start node
needs transmit data message goal node. so, send message one
neighbors re-transmit message another node, etc. transmission
two neighboring nodes incurs execution cost one hop. message reaches destination/goal node, new trial begins start goal nodes. Convergence occurs
heuristic values updated trial.
140

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

objective find shortest route start node goal node learning better
heuristic values repeated trials. take measurements four metrics paralleling used
domain pathfinding:
convergence execution cost total number hops traveled messages convergence;
first trial execution cost number hops traveled first trial;
suboptimality final solution excess execution cost final trial
shortest path possible. instance, network converged route 120 hops
shortest route 100 hops, suboptimality 20%;
convergence traffic defined total volume information transmitted throughout network convergence. data message counts one. Short control messages (used
notify nodes neighbors update nodes heuristic function) count one third.
Interference modeled simulation. assume scheduling performed independently
routing strategy, different schedules severely affect performance
routing algorithms. Note that, since transmission delay timings modeled, would
difficult model interference accurately.
Four batches networks 50, 200, 400, 800 nodes considered. nodes
randomly positioned square grid dimensions 20 20, 30 30, 50 50,
80 80 respectively. batch consisted 100 randomly generated networks. network,
25 convergence runs performed parameter values running = {0.1, 0.3, 0.5, 0.7, 1}
= {0, 10, 100, 1000, 100000}. following, present results batch
800-node networks. trends observed smaller networks well.
start demonstrating influence two control parameters isolation. Table 12
shows influence heuristic weight backtracking. expected, smaller values
speed convergence increase suboptimality final solution. results
parallel ones reported Table 3 pathfinding domain.
Table 12: Effects heuristic weighting network routing.
Heuristic weight
1.0 (DVR)
0.7
0.5
0.3
0.1

Execution convergence cost
8188
8188
8106
7972
7957

Suboptimality
0%
0%
0%
0.29%
0.33%

Table 13 demonstrates influence learning quota heuristic weight set
one. Smaller values increase amount backtracking speed convergence cost
lengthen first trial. parallels results 8-puzzle (Table 5) well pathfinding
small maps (Table 6).
consider impact parameter combination. fixed lookahead depth one
makes two-dimensional parameter space easier visualize contour plots. Thus, instead
plotting impact parameter metric function two control parameters,
141

fiB ULITKO & L EE

Table 13: Effects backtracking network routing.
Learning quota
105 (DVR)
103
102
10
0

First trial execution cost
549
1188
4958
5956
6117

Convergence execution cost
8188
8032
6455
6230
6138

First trial execution cost (hops)

Convergence execution cost (hops)
1

1

Heuristic weight ()

Heuristic weight ()

5000
0.7
4000
0.5

3000
2000

0.3

0.7

7500

0.5

7000

0.3
6500

1000
0.1
0

10

100
1000
Learning quota (T)

0.1

100000

0

10

100
1000
Learning quota (T)

100000

Figure 15: First-trial convergence execution costs network routing.
plot metric itself. Figure 15 shows first trial convergence execution costs
combinations . Brighter areas indicates higher costs.
Figure 16 shows suboptimality final solution total traffic convergence run,
averaged 100 networks 800 nodes each. Again, brighter areas indicate higher values.
Suboptimality final solution (%)

Convergence traffic

1

4

x 10

1

0.2
0.5

0.15
0.1

0.3

Heuristic weight ()

Heuristic weight ()

0.25
0.7

5
0.7
4.5

0.5

4

0.3

0.05

3.5

0.1

0.1
0

10

100
1000
Learning quota (T)

0

100000

10

100
1000
Learning quota (T)

100000

Figure 16: Suboptimality final solution total traffic network routing.
summary, extending DVR algorithm heuristic weight backtracking mechanisms LRTS, reduction convergence execution cost total network traffic
achieved. result, near-optimal routes found faster, lower energy consumption.
promising result raises several questions research. First, additional routing
constraints, ones investigated (Shang et al., 2003), affect performance LRTS?
Second, benefits LRTS scale case several messages destina142

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

tion passing network simultaneously? latter question also applies path-planning
multiple memory-sharing units common situation squad-based games.

9. Summary Future Research
paper, considered simultaneous planning learning problems real-time
pathfinding initially unknown environments well routing ad hoc wireless sensor networks. Learning real-time search methods plan conducting limited local search learn
maintaining refining state-based heuristic function. Thus, react quickly users
commands yet converge better solutions repeated experiences.
analyzed three complementary extensions base LRTA* algorithm showed
effects hand-traceable examples, theoretical bounds, extensive empirical evaluation two
different domains. proposed simple algorithm, called LRTS, unifying extensions
studied problem choosing optimal parameter combinations.
Current future work includes extending LRTS automated state space abstraction mechanisms, function approximation methods heuristic, automated parameter selection techniques. also interest investigate convergence process team LRTS-based
agents shared memory extend LRTS moving targets stochastic/dynamic environments video games, robotics, networking.

Acknowledgements
Input Valeriy Bulitko, Sven Koenig, Rich Korf, David Furcy, Nathan Sturtevant, Masashi
Shimbo, Rob Holte, Douglas Aberdeen, Reza Zamani, Stuart Russell, Maryia Kazakevich
greatly appreciated. Nathan Sturtevant kindly provided supported Hierarchical Open Graph
simulator. Additionally, Valeriy Bulitko, David Furcy, Sven Koenig, Rich Korf, Scott Thiessen,
Ilya Levner taken time proof read early draft paper. Great thanks JAIR reviewers
working us improving manuscript. grateful support NSERC,
University Alberta, Alberta Ingenuity Centre Machine Learning, Jonathan Schaeffer.

143

fiB ULITKO & L EE

Appendix A. Detailed Pseudocode
section list pseudocode algorithms level detail necessary implement
well follow proofs listed Appendix B.
LRTA*
UTPUT: series actions leading s0 goal state
1
2
3
4
5
6
7

h undefined set h h0
reset current state: s0
6 Sg
generate depth 1 neighborhood S(s, 1) = {s0 | ks, s0 k = 1}
compute h0 (s) = 0 min (dist(s, s0 ) + h(s0 ))
S(s,1)

h0 (s) > h(s) update h(s) h0 (s)
update current state arg 0 min (dist(s, s0 ) + h(s0 ))
S(s,1)

Figure 17: LRTA* algorithm.
SLA*
UTPUT: series actions leading s0 goal state
1
2
3
4
5
6
7
8
9
10
11
12

h undefined set h h0
reset current state: s0
reset path traveled far: path
6 Sg
generate depth 1 neighborhood S(s, 1) = {s0 | ks, s0 k = 1}
compute h0 (s) = 0 min (dist(s, s0 ) + h(s0 ))
S(s,1)

h0 (s) > h(s)
update h(s) h0 (s)
update current state pop(path) [or remain state path = ]
else
push onto stack path
update current state arg 0 min (dist(s, s0 ) + h(s0 ))
S(s,1)

Figure 18: SLA* algorithm.

144

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

-Trap(d, )
NPUT:
d: lookahead depth
: optimality weight
UTPUT: path: series actions leading s0 goal state
1
2
3
4
5
6
7
8
9
10
11
12
13
14

h undefined set h h0
reset current state: s0
reset stack: path
reset cumulative learning amount u 0
6 Sg
set min{d, min{k | S(s, k) = }}
reset set G
k=1
generate depth k neighborhood S(s, k) = {s0 | ks, s0 k = k}
S(s, k) Sg 6= update G G {k}
compute fmin (s, k) = 0 min ( dist(s, s0 ) + h(s0 ))
S(s,k)

compute smin (s, k) = arg

min

s0 S(s,k)

end
compute h0 (s)


max fmin (s, k)

G = ,

1kd



max

1kmin{G}

15
16

( dist(s, s0 ) + h(s0 ))

fmin (s, k)

otherwise.

h0 (s) h(s)
push onto stack path

17

(
G = ,
smin (s, d)
update current state
arg min fmin (s, k) otherwise.

18
19
20
21
22

else
update h(s) h0 (s)
backtrack: pop(path) [or remain state path = ].
end
end

kG

Figure 19: -Trap algorithm.

145

fiB ULITKO & L EE

LRTS(d, , )
NPUT:
d0 : lookahead depth
: optimality weight
: learning threshold
UTPUT: path: series actions leading s0 goal state
1
2
3
4
5
6
7
8
9
10
11
12
13
14

h undefined set h h0
reset current state: s0
reset stack: path
reset cumulative learning amount u 0
6 Sg
set min{d0 , min{k | S(s, k) = }}
reset set G
k=1
generate depth k neighborhood S(s, k) = {s0 | ks, s0 k = k}
S(s, k) Sg 6= update G G {k}
compute fmin (s, k) = 0 min ( dist(s, s0 ) + h(s0 ))
S(s,k)

compute smin (s, k) = arg

min

s0 S(s,k)

end
compute h0 (s)


max fmin (s, k)
max

1kmin{G}

23

G = ,

1kd


15
16
17
18
19
20
21
22

( dist(s, s0 ) + h(s0 ))

fmin (s, k)

otherwise.

h0 (s) > h(s)
compute amount learning move: ` h0 (s) h(s)
update h(s) h0 (s)
else
reset ` = 0
end
u + `
push onto stack path
(
G = ,
smin (s, d)
update current state:
arg min fmin (s, k) otherwise.
kG

24
25
26
27
28

accumulate learning amount u u + `
else
backtrack: pop(path) [or remain state path = ].
end
end

Figure 20: LRTS algorithm.
23a
23b
23c

s0 [s = s0 & s0 path]
remove s0 following states path
end

Figure 21: Additional lines facilitating incremental pruning LRTS. inserted lines 23 24 Figure 20.
146

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

Appendix B. Theorem Proofs
Theorem 7.1 LRTS(d = 1, = 1, = ) equivalent LRTA* lookahead one.
Proof. show state LRTS(d = 1, = 1, = ) (i) takes action
LRTA* (ii) updates h function way LRTA* does.
(i) state s, LRTS(d = 1, = 1, = ) goes state smin (s, 1) (line 23, Figure 20)
equivalent state arg min (dist(s, s1 ) + h(s1 )) LRTA* move (line
s1 S(s,1)

7, Figure 17).8
(ii) LRTS(d = 1, = 1, = ) updates h(s) h0 (s) line 17 Figure 20 h0 (s) >
h(s) (line 15) equivalent LRTA*s update condition line 6, Figure 17.

1
, = ) initialized admissible heuristic h0 equivaTheorem 7.2 LRTS(d = 1, = 1+
lent -LRTA* initialized (1 + )h0 .

Proof. two algorithms begin maintain different heuristic functions. Let hLRTS


1
heuristic function LRTS(d = 1, = 1+
, = ) maintains iteration t.9 Likewise, let h-LRTA*

denote heuristic function -LRTA* maintains iteration t. also denote sLRTS


1
, = ) iteration t. Likewise, s-LRTA*


current
current state LRTS(d = 1, = 1+

state -LRTA* iteration t. show induction iteration number t:
h-LRTA*
= (1 + )hLRTS
.



(B.1)

1
Base step: since -LRTA* initialized (1 + )h0 LRTS(d = 1, = 1+
, = )
initialized h0 , equation B.1 trivially holds = 0.
Inductive step: suppose equation B.1 holds iteration t. show holds iteration
+ 1.
First, show algorithms bound state st . Suppose not: sLRTS
6=

-LRTA*
st
. since start state s0 must earliest t0 <
-LRTA* . means state different actions taken
sLRTS
= s-LRTA*
sLRTS
t0
t0
t0
t0 +1 6= st0 +1
LRTS -LRTA* . LRTS takes action line 23 Figure 20 would move state:


arg

min

s0 S(st0 ,1)


1
0
LRTS 0
dist(st0 , ) + ht0 (s ) .
1+

(B.2)

-LRTA* moves following state (line 7 Figure 17):
arg

min

s0 S(s

t0 ,1)


dist(st0 , s0 ) + h-LRTA*
(s0 ) .
t0

(B.3)

Since h-LRTA*
= (1 + )hLRTS
(as t0 < equation B.1 holds time inductive
t0
t0
hypothesis), action B.2 B.3.
8. Throughout paper assume tie-breaking done consistent way among algorithms.
9. One measure iterations counting number times condition executed (line 5 Figure 20
line 3 Figure 17).

147

fiB ULITKO & L EE

update h LRTS(d = 1, =
expression h0 (s) arrive at:
hLRTS
t+1 (st )

1
1+ ,


=
=
=
=

min

s0 S(st ,1)

= ) occurs line 17, Figure 20. Substituting


1
0
LRTS 0
dist(st , ) + ht
(s )
1+


1
min
dist(st , s0 ) + (1 + )hLRTS
(s0 )

1 + s0 S(st ,1)

1
min
dist(st , s0 ) + h-LRTA*
(s0 )

0
1 + S(st ,1)
1 -LRTA*
h
(st )
1 + t+1

(B.4)
(B.5)
(B.6)
(B.7)

concludes inductive proof.
Theorem 7.3 LRTS(d = 1, = 1, = 0) equivalent SLA*.
Proof. = 0, condition u + ` line 21 Figure 20 hold h(s)
updated (i.e., h0 (s) h(s) line 15). case forward move executed line
23 equivalent SLA*s forward move lines 11, 12 Figure 18. h(s) indeed updated
LRTS SLA* backtrack executing line 26 Figure 20 line 9 Figure 18
respectively. proof concluded observation h0 (s) computed LRTS(d = 1, =
1, = 0) equivalent h0 (s) computed SLA* line 6 Figure 18.
Theorem 7.4 LRTS(d, , = 0) equivalent -Trap(d, ).
Proof. Substituting = 0 line 21 Figure 20, effectively obtain -Trap (Figure 19).
Lemma 7.1 (Admissibility) valid T, , admissible initial heuristic h0 , heuristic
function h maintained admissible times LRTS(d, , ) search.
Proof. prove lemma induction iteration number t. = 0 statement
trivially holds since initial heuristic h0 admissible. Suppose ht admissible
show ht+1 admissible. Heuristic ht+1 different ht state st updated
lines 14 17 Figure 20. Combining expressions together, obtain:
ht+1 (st ) = max

min ( dist(st , s0 ) + ht (s0 )),

1kd s0 S(st ,k)

(B.8)

goal states discovered lookahead (G = ) and:
ht+1 (st ) = min

min ( dist(st , s0 ) + ht (s0 )),

kG s0 S(st ,k)

(B.9)

otherwise. Suppose max min min min reached state :
ht+1 (st ) = dist(st , ) + ht (s ), S(st , m), 1 d.

(B.10)

shortest path state st closest goal state intersects neighborhood S(st , m) depth
m. Let us denote state S(st , m) belonging shortest path (Figure 22).
148

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

S(st,d)
S(st,m)

st

s*



goal

Figure 22: proof admissibility.


Since ht admissible states,
arg min ( dist(st , s) + ht (s)) conclude that:

1,







chosen



sS(st ,m)

ht+1 (st ) = dist(st , ) + ht (s )


(B.11)



dist(st , ) + ht (s )

(B.12)



dist(st , ) + h (s )

(B.13)







= h (st ).

(B.14)

Thus, ht+1 admissible heuristic concludes induction.
Theorem 7.5 (Completeness) valid T, , admissible initial heuristic h0 ,
LRTS(d, , ) arrives goal state every trial.
Proof. Since heuristic h maintained admissible times update increases value
positive amount lower-bounded constant, finite number updates made
trial. Let last update made state su1 , u 0 sequence states s0 , s1 , . . . traversed
trial. show sequence states su , su+1 , . . . guaranteed terminate
goal state. words, LRTS cannot loop finite state space forever avoiding goal
states. updates made su , su+1 , . . . , must hold that10 j u:
h(sj )

max

min ( dist(sj , s0 ) + h(s0 )),

1kd s0 S(sj ,k)

h(sj ) dist(sj , sj+1 ) + h(sj+1 ),
h(sj ) h(sj+1 ) dist(sj , sj+1 ).

(B.15)
(B.16)
(B.17)

Since > 0 states 6= b [dist(a, b) > 0], follows h(su ) > h(su+1 ) > h(su+2 ) >
. . . . state space finite, series states must terminate goal state h(sn ) = 0.
Thus, valid parameter values, LRTS ends goal state every trial.
10. Similar reasoning applies case G 6= min min used instead max min.

149

fiB ULITKO & L EE

Theorem 7.6 (Convergence) valid T, , admissible initial heuristic h0 ,
LRTS(d, , ) systematic fixed tie-breaking converges final solution finite
number trials. makes zero updates heuristic function final trial. subsequent
trial identical final trial.
Proof. proof Theorem 7.5 clear trial zero updates
heuristic function h. call trial final trial demonstrate subsequent
trial identical final trial. Suppose not. final trial s0 , s1 , . . . , sn
earliest state sj next trial different final trial state sj+1 . implies
subsequent trial LRTS took different action state sj action took
state final trial. updates heuristic function, action must
selected line 23 Figure 20. Expanding expression, get:11
sj+1 = smin (sj , d) = arg

min ( dist(sj , s0 ) + h(s0 )).

s0 S(sj ,d)

(B.18)

Since ties broken systematic fixed fashion, choice state sj+1 unique
contradicts assumption existence subsequent trial different final trial.
Theorem 7.7 valid T, , admissible initial heuristic h0 , converged solution

0)
cost12 LRTS(d, , ) upper-bounded h (s
.
Proof. First, let us denote converged solution cost (i.e., execution cost final trial)
\ theorem 7.6 follows final trial updates heuristic
solution.
function. Suppose s0 , . . . , sn states traversed final trial. Since updates
made heuristic function, condition h0 (si ) h(si ) line 15 Figure 20 must held
si , = 0, . . . , n. Substituting expression h0 (si ) line 14, arrive at:13
max

1kd

min

s0 S(s

,k)


dist(si , s0 ) + h(s0 ) h(si ).

(B.19)


dist(si , s0 ) + h(s0 )

(B.20)

Since:
si+1 =

min

s0 S(s

,d)

conclude that:
dist(si , si+1 ) + h(si+1 ) h(si )
dist(si , si+1 ) h(si ) h(si+1 ).

(B.21)
(B.22)

Summing telescoping inequalities = 0, . . . , n 1, derive:
n1
X
i=0

dist(si , si+1 )

h(s0 ) h(sn )
.


11. case G 6= handled similarly.
12. Henceforth converged solution cost defined execution cost final trial.
13. again, case G 6= handled similarly.

150

(B.23)

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

Since sn Sg , h(sn ) = 0. sum distances travelled travel cost final trial
converged solution cost definition.14 Thus, conclude converged solution cost

0)
upper bounded h(s 0 ) h (s
.
Theorem 7.8 valid , admissible initial heuristic h0 , second trial LRTS(T =
0) systematic fixed tie-breaking guaranteed final. Thus, learning exploration
done first trial.
Proof. Consider sequence states [s0 , s1 , . . . , sn ] traversed first trial. s0
start state sn goal state found. transition st st+1 , = 0, . . . , n 1
carried via forward move (line 23 Figure 20) backtracking move (line 26). move
= 0, . . . , n 1, heuristic function may changed (line 17) state st . use
ht denote heuristic function update line 17. update results ht+1 . Note
update changes h state st ht (s) = ht+1 (s) 6= st . particular, since
st+1 6= st 15 ht+1 (st+1 ) = ht (st+1 ).
Since = 0, n moves divided two groups: forward moves change
h function backtracking moves do. prune sequence states
[s0 , s1 , . . . , sn ] using backtracking stack. Namely, beginning stack contains s0 . Every
forward move 0 n1 add st+1 stack. Every backtracking move 0 n1
remove st+1 stack. Carrying procedure = 0, . . . , n 1, derive sequence
states: p0 = [s0 , . . . , sm ] leading initial state s0 goal state sm . Clearly, n.
Observe since backtracking moves removed original move sequence,
moves left forward moves updates h. Thus, let us define b
h-value state
visited trial as:
(
h(s)
recent backtracking move state s;
b
h(s) =
initial h(s) backtracking move taken trial.
Since = 0, state si , = 0, . . . , updates h last
backtracking
h move fromi state (if any). words, h final heuristic =
0, . . . , h(si ) = b
h(si ) . also means every k-neighborhood (1 k d) state
0
si p state s0 b
h(s0 ) + dist(si , s0 ) b
h(si ).
show updates h done second trial. Suppose
case. must exist earliest state sj p0 , 0 j < LRTS second
trial updated h(sj ) greater b
h(sj ). reasoned above, final value h(sj )
b
first trial h(sj ) need increase it. assumption, LRTS increase
h(sj ) second trial. Consequently, least one state one sj neighborhoods updated
first trial (as updates second trial happened yet).
Formally, h(s0 ), s0 S(sj , k) 1 k d, increased first trial
state sj visited p0 . Additionally, means result update must
hold that:
hnew (s0 ) + k > h(sj ).

(B.24)

\ =
14. P
easy check sequence states final trial repetitions. Therefore, solution
n1
dist(s
,

).
i+1
i=0
15. Except possibly case backtracking st = s0 affect proof.

151

fiB ULITKO & L EE

s0

S(sj,k)

p'

sj

sm
s'

Figure 23: proof Theorem 7.8.
Thus, first trial, state s0 must arrived via sequence forward moves starting
state sj (shown dashed curve Figure 23). forward moves h-updates,
h(scurrent ) > h(snext ) holds. means sequence states traversed forward
moves started sj ended s0 . Since hnew (s0 ) + k > h(sj ), LRTS must
backtrack s0 least sj . would updated h(sj ) hnew (sj ) hnew (s0 ) + k
first trial. Consequently, state s0 cannot cause update h(sj ) second trial
inequality B.24 longer hold, leading contradiction. Thus, updates h carried
second trial.
Let us define solution(i) execution cost path p start goal states
that: (i) p lies fully set states traversed agent trial (ii) p minimizes
execution cost.
Theorem B.1 valid T, , admissible initial heuristic h0 , first trial
LRTS(d, , ) systematic fixed tie-breaking incremental pruning (see Figure 21)
sults solution(1) h (s0 )+T .
Proof. prove theorem three steps.
Notation. Consider sequence states [s0 , s1 , . . . , sn ] traversed first trial.
state s0 start state state si current state (s) i-th iteration line 27 LRTS
(Figure 20). State sn goal state reached end first trial. Within iteration (0
n1), two states computed follows. beginning iteration (line 6), si current
state s. forward move (line 23), state set si+1 = smin (si , d).16 Let us denote
heuristic value state si beginning iteration hi (si ). possible update line
17, new heuristic value hi+1 (si ). Also, let us denote amount learning ` iteration
`i . precisely:
(
hi+1 (si ) hi (si ) h0 (s) > h(s) line 15,
`i =
(B.25)
0
otherwise.
notation, easy see following inequality holds:
dist(si , si+1 ) + hi (si+1 ) hi (si ) + `i
16. case seeing goal state, i.e., G 6= , dealt similar fashion.

152

(B.26)

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

forward move (line 23) happens. Note iteration i, state si state
heuristic value updated. Thus, sj 6= si [hi+1 (sj ) = hi (sj )]. particular17 ,
hi+1 (si+1 ) = hi (si+1 ) allows transform inequality B.26 into:
dist(si , si+1 ) + hi+1 (si+1 ) hi (si ) + `i .

(B.27)

Forward moves. consider case revisiting states forward moves.
Namely, suppose sequence states [s0 , s1 , . . . , sn ], state si+k = si k > 0
smallest k equation holds. also assume backtracking (line 26) took
place states si si+k . pruning module (lines 23a 23c Figure 21)
purge states si si+k . Therefore, move si+k si+k+1 immediately follow
move si1 si making new sequence: si1 si (= si+k ) si+k+1 .
state si1 , inequality B.27 is:
dist(si1 , si ) + hi (si ) hi1 (si1 ) + `i1 .

(B.28)

state si+k , inequality B.27 is:
dist(si+k , si+k+1 ) + hi+k+1 (si+k+1 ) hi+k (si+k ) + `i+k .

(B.29)

Remembering si = si+k increase h(si ) possible algorithm left
state si , conclude hi (si )+`i = hi+k (si+k ). consistent pruned move sequence
si1 si+k si+k+1 , re-write inequality B.28 as:
dist(si1 , si+k ) + hi+k (si+k ) `i hi1 (si1 ) + `i1 .

(B.30)

Adding B.29 B.30 re-grouping terms, arrive at:
[dist(si1 , si+k ) + dist(si+k , si+k+1 )]
hi1 (si1 ) hi+k+1 (si+k+1 ) + `i1 + `i + `i+k .

(B.31)

means weighted sum distances sequence states [si1 , si+k , si+k+1 ]
upper bounded differences h first last states plus sum learning amounts
three states. easy show absence backtracking, inequality B.31
generalizes entire pruned sequence [s0 , s1 , . . . , sn ] sequence. Note pruning done
sequence manifest gaps enumeration states. similar
subsequence si1 , si , si+1 , . . . , si+k , si+k+1 used earlier proof becoming si1 , si+k , si+k+1 .
Let us denote pruned sequence indices I. inequality B.31 becomes:


X

dist(si , si+1 ) h0 (s0 ) hn (sn ) +

n1
X

`i .

(B.32)

i=0

iI

Noticing h0 (s0 ) h (s0 ) hn (sn ) = 0 sn goal state, arrive at:


X

dist(si , si+1 ) h (s0 ) +

n1
X
i=0

iI

17. special case si = s0 dealt simple fashion.

153

`i .

(B.33)

fiB ULITKO & L EE

Finally, observing total amount learning upper bounded learning quota
sum distances along pruned state sequence exactly solution(1), derive
desired upper bound:
solution(1)

h (s0 ) +
.


(B.34)

Backtracking. finale proof lies showing backtracking (line 26 LRTS)
affect derivation previous section. Suppose, LRTS traversed states si si+1
si+2 . . . backtracked state si iteration i+k (i.e., si+k = si ). pruned solution
sequence states si1 si+k removed path (line 26) creating gap
enumeration: . . . , si1 , si+k , si+k+1 , . . . . Observing states si1 , si = si+k , si+k+1 (i)
backtracked (ii) among states removed path backtracking,18
conclude backtracking affect heuristic value three states. Therefore,
previous derivation still holds concludes proof theorem.

References
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72(1), 81138.
BioWare (1999). Baldurs Gate. http://www.bioware.com/games/baldur_gate.
Blizzard (2002). Warcraft 3: Reign chaos. http://www.blizzard.com/war3.
Botea, A., Muller, M., & Schaeffer, J. (2004). Near Optimal Hierarchical Path-Finding. Journal
Game Development, 1(1), 728.
Braginsky, D., & Estrin, D. (2002). Rumor routing algorithm sensor networks. Proceedings
First ACM Workshop Sensor Networks Applications, pp. 2231.
Bulitko, V. (2003). Lookahead pathologies meta-level control real-time heuristic search.
Proceedings 15th Euromicro Conference Real-Time Systems, pp. 1316, Porto,
Portugal.
Bulitko, V., Li, L., Greiner, R., & Levner, I. (2003). Lookahead pathologies single agent search.
Proceedings International Joint Conference Artificial Intelligence, pp. 15311533,
Acapulco, Mexico.
Bulitko, V. (2004). Learning adaptive real-time search. Tech. rep. http: // arxiv. org / abs / cs.AI
/ 0407016, Computer Science Research Repository (CoRR).
Buro, M. (1995). Probcut: effective selective extension alpha-beta algorithm. ICCA
Journal, 18, 7176.
Buro, M. (2002). ORTS: hack-free RTS game environment. Proceedings International
Computers Games Conference, p. 12, Edmonton, Canada.
Chimura, F., & Tokoro, M. (1994). Trailblazer search: new method searching capturing moving targets. Proceedings National Conference Artificial Intelligence, pp.
13471352.
18. fact three states cannot among states removed backtracking due incremental pruning
mechanism lines 23a, 23b, 23c Figure 21. pruning mechanism clearly separates updates heuristic
function made forward backward moves.

154

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

Dijkstra, E. W. (1959). note two problems connexion graphs.. Numerische Mathematik,
1, 269271.
Ensemble-Studios (1999). Age empires II: Age kings. http: // www.microsoft.com / games /
age2.
Furcy, D., & Koenig, S. (2000). Speeding convergence real-time search. Proceedings
National Conference Artificial Intelligence, pp. 891897.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Transactions Systems Science Cybernetics, 4(2), 100107.
Herbert, M., McLachlan, R., & Chang, P. (1999). Experiments driving modes urban robots.
Proceedings SPIE Mobile Robots.
Holte, R., Drummond, C., Perez, M., Zimmer, R., & MacDonald, A. (1994). Searching abstractions: unifying framework new high-performance algorithm. Proceedings
Canadian Artificial Intelligence Conference, pp. 263270.
Holte, R. (1996). Hierarchical A*: Searching abstraction hierarchies efficiently. Proceedings
National Conference Artificial Intelligence, pp. 530 535.
Hsu, F., Campbell, M., & Hoane, A. (1995). Deep Blue system overview. Proceedings 9th
ACM International Conference Supercomputing, pp. 240244.
Ishida, T. (1997). Real-time Search Learning Autonomous Agents. Kluwer Academic Publishers.
Ishida, T., & Korf, R. (1991). Moving target search. Proceedings International Joint
Conference Artificial Intelligence, pp. 204210.
Ishida, T., & Korf, R. (1995). realtime search changing goals. IEEE Transactions Pattern
Analysis Machine Intelligence, 17(6), 609619.
Kallmann, M., Bieri, H., & Thalmann, D. (2003). Fully dynamic constrained Delaunay triangulations. Brunnett, G., Hamann, B., Mueller, H., & Linsen, L. (Eds.), Geometric Modelling
Scientific Visualization, pp. 241 257. Springer-Verlag, Heidelberg, Germany.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjou, A., & Shimada, S. (1999).
Robocup rescue: Search rescue large-scale disasters domain autonomous agents
research. Proceedings IEEE Conference Man, Systems, Cybernetics.
Koenig, S. (1999). Exploring unknown environments real-time search reinforcement learning. Proceedings Neural Information Processing Systems, pp. 10031009.
Koenig, S., & Likhachev, M. (2002). D* Lite. Proceedings National Conference
Artificial Intelligence, pp. 476483.
Koenig, S., Tovey, C., & Smirnov, Y. (2003). Performance bounds planning unknown terrain.
Artificial Intelligence, 147, 253279.
Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22(4), 109132.
Koenig, S. (2004). comparison fast search methods real-time situated agents. Proceedings Third International Joint Conference Autonomous Agents Multiagent
Systems - Volume 2, pp. 864 871.
155

fiB ULITKO & L EE

Koenig, S., & Simmons, R. (1998). Solving robot navigation problems initial pose uncertainty
using real-time heuristic search. Proceedings International Conference Artificial
Intelligence Planning Systems, pp. 144 153.
Korf, R. (1985). Depth-first iterative deepening : optimal admissible tree search. Artificial
Intelligence, 27(3), 97109.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189211.
Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.
Ng, A. Y., Coates, A., Diel, M., Ganapathi, V., Schulte, J., Tse, B., Berger, E., & Liang, E. (2004).
Inverted autonomous helicopter flight via reinforcement learning. Proceedings International Symposium Experimental Robotics.
Pearl, J. (1984). Heuristics. Addison-Wesley.
Pottinger, D. C. (2000). Terrain analysis realtime strategy games. Proceedings Computer
Game Developers Conference.
Ratner, D., & Warmuth, M. (1986). Finding shortest solution N N extension
15-puzzle intractable. Proceedings National Conference Artificial Intelligence,
pp. 168172.
Reece, D., Krauss, M., & Dumanoir, P. (2000). Tactical movement planning individual combatants. Proceedings 9th Conference Computer Generated Forces Behavioral
Representation.
Royer, E., & Toh, C. (1999). review current routing protocols ad hoc mobile wireless
networks. IEEE Personal Communications, Vol. 6, pp. 4655.
Russell, S., & Wefald, E. (1991). right thing: Studies limited rationality. MIT Press.
Schaeffer, J., Culberson, J., Treloar, N., Knight, B., Lu, P., & Szafron, D. (1992). world championship caliber checkers program. Artificial Intelligence, 53(2-3), 273290.
Shang, Y., Fromherz, M. P., Zhang, Y., & Crawford, L. S. (2003). Constraint-based routing
ad-hoc networks. Proceedings IEEE International Conference Information Technology: Research Education, pp. 306310, Newark, NJ, USA.
Shimbo, M., & Ishida, T. (2003). Controlling learning process real-time heuristic search.
Artificial Intelligence, 146(1), 141.
Shue, L.-Y., Li, S.-T., & Zamani, R. (2001). intelligent heuristic algorithm project scheduling
problems. Proceedings Thirty Second Annual Meeting Decision Sciences
Institute, San Francisco.
Shue, L.-Y., & Zamani, R. (1993). admissible heuristic search algorithm. Proceedings
7th International Symposium Methodologies Intelligent Systems, Vol. 689 LNAI, pp.
6975. Springer Verlag.
Stenz, A. (1995). focussed D* algorithm real-time replanning. Proceedings
International Conference Artificial Intelligence, pp. 16521659.
Sturtevant, N. (2005). Path refinement A* search. Tech. rep., University Alberta.
Sutton, R. (2005). value function hypothesis. http:// rlai.cs.ualberta.ca/ RLAI/ valuefunctionhypothesis. html.
156

fiL EARNING R EAL -T IME EARCH : U NIFYING F RAMEWORK

Sutton, R., & Barto, A. (1998). Reinforcement Learning: Introduction. MIT Press.
Thayer, S., Digney, B., Diaz, M., Stentz, A., Nabbe, B., & Hebert, M. (2000). Distributed robotic
mapping extreme environments. Proceedings SPIE: Mobile Robots XV Telemanipulator Telepresence Technologies VII.
Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Kings College, Cambridge
University.
Woodcock, S. (2000). AI SDKs help?. Game Developer magazine.
Yan, T., He, T., & Stankovic, J. A. (2003). Differentiated surveillance sensor networks.
Proceedings 1st international conference Embedded networked sensor systems, pp.
5162.
Yu, Y., Govindan, R., & Estrin, D. (2001). Geographical energy aware routing: recursive data
dissemination protocol wireless sensor networks. Tech. rep. UCLA/CSD-TR-01-0023,
UCLA Computer Science Department.

157

fiJournal Artificial Intelligence Research 25 (2006) 17-74

Submitted 12/04; published 01/06

Decision-Theoretic Planning non-Markovian Rewards
Sylvie Thiebaux
Charles Gretton
John Slaney
David Price

Sylvie.Thiebaux@anu.edu.au
Charles.Gretton@anu.edu.au
John.Slaney@anu.edu.au
David.Price@anu.edu.au

National ICT Australia &
Australian National University
Canberra, ACT 0200, Australia

Froduald Kabanza

kabanza@usherbrooke.ca

Departement dInformatique
Universite de Sherbrooke
Sherbrooke, Quebec J1K 2R1, Canada

Abstract
decision process rewards depend history rather merely current state called decision process non-Markovian rewards (NMRDP). decisiontheoretic planning, many desirable behaviours naturally expressed properties execution sequences rather properties states, NMRDPs form
natural model commonly adopted fully Markovian decision process (MDP) model.
tractable solution methods developed MDPs directly apply
presence non-Markovian rewards, number solution methods NMRDPs
proposed literature. exploit compact specification non-Markovian
reward function temporal logic, automatically translate NMRDP equivalent MDP solved using efficient MDP solution methods. paper presents
nmrdpp(Non-Markovian Reward Decision Process Planner), software platform
development experimentation methods decision-theoretic planning nonMarkovian rewards. current version nmrdpp implements, single interface,
family methods based existing well new approaches describe detail. include dynamic programming, heuristic search, structured methods. Using
nmrdpp, compare methods identify certain problem features affect
performance. nmrdpps treatment non-Markovian rewards inspired treatment
domain-specific search control knowledge TLPlan planner, incorporates
special case. First International Probabilistic Planning Competition, nmrdpp
able compete perform well domain-independent hand-coded
tracks, using search control knowledge latter.

c
2006
AI Access Foundation. rights reserved.

fiThiebaux, Gretton, Slaney, Price & Kabanza

1. Introduction
1.1 Problem
Markov decision processes (MDPs) widely accepted preferred model
decision-theoretic planning problems (Boutilier, Dean, & Hanks, 1999). fundamental
assumption behind MDP formulation system dynamics also
reward function Markovian. Therefore, information needed determine reward
given state must encoded state itself.
requirement always easy meet planning problems, many desirable
behaviours naturally expressed properties execution sequences (see e.g., Drummond, 1989; Haddawy & Hanks, 1992; Bacchus & Kabanza, 1998; Pistore & Traverso,
2001). Typical cases include rewards maintenance property, periodic
achievement goal, achievement goal within given number steps
request made, even simply first achievement goal
becomes irrelevant afterwards.
instance, consider health care robot assists ederly disabled people
achieving simple goals reminding important tasks (e.g. taking pill),
entertaining them, checking transporting objects (e.g. checking stoves
temperature bringing coffee), escorting them, searching (e.g. glasses
nurse) (Cesta et al., 2003). domain, might want reward robot making
sure given patient takes pill exactly every 8 hours (and penalise fails
prevent patient within time frame!), may
reward repeatedly visiting rooms ward given order reporting
problem detects, may also receive reward patients request answered
within appropriate time-frame, etc. Another example elevator control domain
(Koehler & Schuster, 2000), elevator must get passengers origin
destination efficiently possible, attempting satisfying range
conditions providing priority services critical customers. domain,
trajectories elevator desirable others, makes natural encode
problem assigning rewards trajectories.
decision process rewards depend sequence states passed
rather merely current state called decision process non-Markovian
rewards (NMRDP) (Bacchus, Boutilier, & Grove, 1996). difficulty NMRDPs
efficient MDP solution methods directly apply them. traditional way
circumvent problem formulate NMRDP equivalent MDP, whose states
result augmenting original NMRDP extra information capturing
enough history make reward Markovian. Hand crafting MDP however
difficult general. exacerbated fact size MDP
impacts effectiveness many solution methods. Therefore, interest
automating translation MDP, starting natural specification nonMarkovian rewards systems dynamics (Bacchus et al., 1996; Bacchus, Boutilier,
& Grove, 1997). problem focus on.

18

fiDecision-Theoretic Planning non-Markovian Rewards

1.2 Existing Approaches
solving NMRDPs setting, central issue define non-Markovian reward
specification language translation MDP adapted class MDP solution
methods representations would like use type problems hand.
precisely, tradeoff effort spent translation, e.g. producing
small equivalent MDP without many irrelevant history distinctions, effort required
solve it. Appropriate resolution tradeoff depends type representations
solution methods envisioned MDP. instance, structured representations
solution methods ability ignore irrelevant information may cope
crude translation, state-based (flat) representations methods require
sophisticated translation producing MDP small feasible.
two previous proposals within line research rely past linear temporal
logic (PLTL) formulae specify behaviours rewarded (Bacchus et al., 1996, 1997).
nice feature PLTL yields straightforward semantics non-Markovian
rewards, lends range translations crudest finest. two
proposals adopt different translations adapted two different types solution
methods representations. first (Bacchus et al., 1996) targets classical state-based
solution methods policy iteration (Howard, 1960) generate complete policies
cost enumerating states entire MDP. Consequently, adopts expensive
translation attempts produce minimal MDP. contrast, second translation
(Bacchus et al., 1997) efficient crude, targets structured solution methods
representations (see e.g., Hoey, St-Aubin, Hu, & Boutilier, 1999; Boutilier, Dearden, &
Goldszmidt, 2000; Feng & Hansen, 2002), require explicit state enumeration.
1.3 New Approach
first contribution paper provide language translation adapted
another class solution methods proven quite effective dealing large
MDPs, namely anytime state-based heuristic search methods LAO* (Hansen &
Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003), ancestors (Barto, Bardtke, &
Singh, 1995; Dean, Kaelbling, Kirman, & Nicholson, 1995; Thiebaux, Hertzberg, Shoaff,
& Schneider, 1995). methods typically start compact representation
MDP based probabilistic planning operators, search forward initial state,
constructing new states expanding envelope policy time permits. may
produce approximate even incomplete policy, explicitly construct explore
fraction MDP. Neither two previous proposals well-suited
solution methods, first cost translation (most performed
prior solution phase) annihilates benefits anytime algorithms, second
size MDP obtained obstacle applicability state-based
methods. Since cost translation size MDP results
severely impact quality policy obtainable deadline, need
appropriate resolution tradeoff two.
approach following main features. translation entirely embedded
anytime solution method, full control given parts MDP
explicitly constructed explored. MDP obtained minimal,
19

fiThiebaux, Gretton, Slaney, Price & Kabanza

minimal size achievable without stepping outside anytime framework, i.e.,
without enumerating parts state space solution method would necessarily
explore. formalise relaxed notion minimality, call blind minimality
reference fact require lookahead (beyond fringe).
appropriate context anytime state-based solution methods, want
minimal MDP achievable without expensive pre-processing.
rewarding behaviours specified PLTL, appear
way achieving relaxed notion minimality powerful blind minimality without
prohibitive translation. Therefore instead PLTL, adopt variant future linear
temporal logic (FLTL) specification language, extend handle rewards.
language complex semantics PLTL, enables natural translation blind-minimal MDP simple progression reward formulae. Moreover,
search control knowledge expressed FLTL (Bacchus & Kabanza, 2000) fits particularly
nicely framework, used dramatically reduce fraction search
space explored solution method.
1.4 New System
second contribution nmrdpp, first reported implementation NMRDP solution
methods. nmrdpp designed software platform development experimentation common interface. Given description actions domain, nmrdpp
lets user play compare various encoding styles non-Markovian rewards
search control knowledge, various translations resulting NMRDP MDP,
various MDP solution methods. solving problem, made record
range statistics space time behaviour algorithms. also supports
graphical display MDPs policies generated.
nmrdpps primary interest treatment non-Markovian rewards,
also competitive platform decision-theoretic planning purely Markovian rewards.
First International Probabilistic Planning Competition, nmrdpp able enrol
domain-independent hand-coded tracks, attempting problems featuring
contest. Thanks use search control-knowledge, scored second place
hand-coded track featured probabilistic variants blocks world logistics
problems. surprisingly, also scored second domain-independent subtrack consisting problems taken blocks world logistic domains.
latter problems released participants prior competition.
1.5 New Experimental Analysis
third contribution experimental analysis factors affect performance
NMRDP solution methods. Using nmrdpp, compared behaviours
influence parameters structure degree uncertainty dynamics,
type rewards syntax used described them, reachability conditions
tracked, relevance rewards optimal policy. able identify number
general trends behaviours methods provide advice concerning
best suited certain circumstances. experiments also lead us rule one

20

fiDecision-Theoretic Planning non-Markovian Rewards

methods systematically underperforming, identify issues claim
minimality made one PLTL approaches.
1.6 Organisation Paper
paper organised follows. Section 2 begins background material MDPs,
NMRDPs, existing approaches. Section 3 describes new approach Section 4
presents nmrdpp. Sections 5 6 report experimental analysis various approaches. Section 7 explains used nmrdpp competition. Section 8 concludes
remarks related future work. Appendix B gives proofs theorems.
material presented compiled series recent conference workshop
papers (Thiebaux, Kabanza, & Slaney, 2002a, 2002b; Gretton, Price, & Thiebaux, 2003a,
2003b). Details logic use represent rewards may found 2005 paper
(Slaney, 2005).

2. Background
2.1 MDPs, NMRDPs, Equivalence
start notation definitions. Given finite set states, write
set finite sequences states S, set possibly infinite
state sequences. stands possibly infinite state sequence
natural number, mean state index , (i) mean prefix
h0 , . . . , . ; 0 denotes concatenation 0 .
2.1.1 MDPs
Markov decision process type consider 5-tuple hS, s0 , A, Pr, Ri,
finite set fully observable states, s0 initial state, finite set actions
(A(s) denotes subset actions applicable S), {Pr(s, a, ) | S, A(s)}
family probability distributions S, Pr(s, a, s0 ) probability
state s0 performing action state s, R : 7 IR reward function
R(s) immediate reward state s. well known MDP
compactly represented using dynamic Bayesian networks (Dean & Kanazawa, 1989;
Boutilier et al., 1999) probabilistic extensions traditional planning languages (see e.g.,
Kushmerick, Hanks, & Weld, 1995; Thiebaux et al., 1995; Younes & Littman, 2004).
stationary policy MDP function : 7 A, (s) A(s)
action executed state s. value V policy s0 , seek
maximise, sum expected future rewards infinite horizon, discounted
far future occur:
V (s0 ) = lim E
n

X
n





R(i ) | , 0 = s0

i=0

0 < 1 discount factor controlling contribution distant rewards.

21

fiThiebaux, Gretton, Slaney, Price & Kabanza

.............................
........
...
....
...
....
.

.....................
...
.... 0 ....
.
....
...................

@
R



..................................
....
.......
...
...
..

initial state s0 , p false two actions
possible: causes transition s1 probability
0.1, change probability 0.9, b
transition probabilities 0.5. state s1 , p true,
actions c (stay go) lead s1 s0
respectively probability 1.
reward received first time p true,
subsequently. is, rewarded state sequences are:
hs0 , s1
hs0 , s0 , s1
hs0 , s0 , s0 , s1
hs0 , s0 , s0 , s0 , s1
etc.



..
..
...
...
...
..
....
...
.
.
.
.
.
.......
.
.
.
.
.
.
.
.
.
.
..................................
................................
...
...
...
...
...
...
....
.
..
..
..
..
...
..
...
.
..
...
.
....
..
.... .................. .......
....... ...
... .......
.....
.
... 1 .....
.
....
...................



0.9

b
6@

1.0

0.1

0.5

0.5

~s =
I.....
@
...
..
c

.....
...
....
............

...
..

1.0 ..................

Figure 1: Simple NMRDP
2.1.2 NMRDPs
decision process non-Markovian rewards identical MDP except
domain reward function . idea process passed
state sequence (i) stage i, reward R((i)) received stage i. Figure 1
gives example. Like reward function, policy NMRDP depends history,
mapping A. before, value policy expectation
discounted cumulative reward infinite horizon:
X

n

V (s0 ) = lim E
R((i)) | , 0 = s0
n

i=0

e
decision process = hS, s0 , A, Pr, Ri state S, let D(s)
stand
set state sequences rooted feasible actions D, is:
e
D(s)
= { | 0 = A(i ) Pr(i , a, i+1 ) > 0}. Note definition
e
D(s) depend R therefore applies MDPs NMRDPs.
2.1.3 Equivalence
clever algorithms developed solve MDPs cannot directly applied NMRDPs.
One way dealing problem translate NMRDP equivalent MDP
expanded state space (Bacchus et al., 1996). expanded states MDP
(e-states, short) augment states NMRDP encoding additional information
sufficient make reward history-independent. instance, want reward
first achievement goal g NMRDP, states equivalent MDP would
carry one extra bit information recording whether g already true. e-state
seen labelled state NMRDP (via function Definition 1 below)
history information. dynamics NMRDPs Markovian, actions
probabilistic effects MDP exactly NMRDP. following definition,
adapted given Bacchus et al. (1996), makes concept equivalent MDP
precise. Figure 2 gives example.

22

fiDecision-Theoretic Planning non-Markovian Rewards


..............................
.......
..
....
...
....
.



.................................
....
.......
...
...
..

...................
....
.
..... 0 ....
.... 2 ....
..
..
................
...
..
...
...
....
...
.
.
.
.
.
.......
.
.
.
.
.
.
.
.
.
.
.
.
.
..............................
................................
.
...
...
...
...
...
..
....
.
..
..
..
..
...
.
..
...
.
.
...
.
.
....
..
.... ................. .......
.
....... ...
..... 0 ..............
...
.
.
.
.... 3 ...
...............

@
R ?




0.9

0.1

b
6@

1.0

~s =

c....

@
....
.
.
.....
...
....
...........

1.0

0.5

..... ................
...........
......
...
...
...
....
.
.
..
.............................. ....
.
.
.
.
.
..
.
.
.
.
.
.
..........
..
.....
.
.
.
.
.
.
.
.
.
.
.
.
...
.
...
.
.
.
.
...
....................
....................
...
...
.
.
..... 0 ....
..... 0 ....
.... 1 ....
.... 0 ....
................
...............
...
....
......
.
....
......
........
...
............... ..........................
...
..........
.
..
...
...
..
..
...
.
.
.
....
.
.
.
.
.......
............. ...................

0.5

0.5

0.5...........



b

/

c

@







...
..
..
....
............

0.1





@


0.9

Figure 2: MDP Equivalent NMRDP Figure 1. (s00 ) = (s02 ) = s0 . (s01 ) =
(s03 ) = s1 . initial state s00 . State s01 rewarded; states not.

Definition 1 MDP D0=hS 0 , s00 , A0 , Pr0, R0 equivalent NMRDP = hS, s0 , A, Pr, Ri
exists mapping : 0 7 that:1
1. (s00 ) = s0 .
2. s0 0 , A0 (s0 ) = A( (s0 )).
3. s1 , s2 S, A(s1 ) Pr(s1 , a, s2 ) > 0, s01 0
(s01 ) = s1 , exists unique s02 0 , (s02 ) = s2 ,
a0 A0 (s01 ), Pr0 (s01 , a0 , s02 ) = Pr(s1 , a0 , s2 ).
e 0 ) 0
e 0 (s0 ) (0 ) =
4. feasible state sequence D(s
0

i, have: R0 (0i ) = R((i)) i.
Items 13 ensure bijection feasible state sequences NMRDP
feasible e-state sequences MDP. Therefore, stationary policy MDP
reinterpreted non-stationary policy NMRDP. Furthermore, item 4 ensures
two policies identical values, consequently, solving NMRDP optimally
reduces producing equivalent MDP solving optimally (Bacchus et al., 1996):
Proposition 1 Let NMRDP, D0 equivalent MDP it, 0 policy
e 0 ) ((i)) = 0 (0 ),
D0 . Let function defined sequence prefixes (i) D(s

0
j (j ) = j . policy V (s0 ) = V0 (s00 ).
1. Technically, definition allows sets actions A0 different, action
differ must inapplicable reachable states NMRDP e-states equivalent
MDP. practical purposes, A0 seen identical.

23

fiThiebaux, Gretton, Slaney, Price & Kabanza

2.2 Existing Approaches
existing approaches NMRDPs (Bacchus et al., 1996, 1997) use temporal logic
past (PLTL) compactly represent non-Markovian rewards exploit compact
representation translate NMRDP MDP amenable off-the-shelf solution
methods. However, target different classes MDP representations solution methods, consequently, adopt different styles translations.
Bacchus et al. (1996) target state-based MDP representations. equivalent MDP
first generated entirely involves enumeration e-states transitions
them. Then, solved using traditional dynamic programming methods
value policy iteration. methods extremely sensitive number
states, attention paid producing minimal equivalent MDP (with least number
states). first simple translation call pltlsim produces large MDP
post-processed minimisation solved. Another, call pltlmin,
directly results minimal MDP, relies expensive pre-processing phase.
second approach (Bacchus et al., 1997), call pltlstr, targets structured
MDP representations: transition model, policies, reward value functions represented compact form, e.g. trees algebraic decision diagrams (ADDs) (Hoey et al.,
1999; Boutilier et al., 2000). instance, probability given proposition (state
variable) true execution action specified tree whose internal
nodes labelled state variables whose previous values given variable depends, whose arcs labelled possible previous values (> ) variables,
whose leaves labelled probabilities. translation amounts augmenting
structured MDP new temporal variables tracking relevant properties state
sequences, together compact representation (1) dynamics, e.g. trees
previous values relevant variables, (2) non-Markovian reward function
terms variables current values. Then, structured solution methods structured
policy iteration SPUDD algorithm run resulting structured MDP. Neither
translation solution methods explicitly enumerates states.
review approaches detail. reader referred respective
papers additional information.
2.2.1 Representing Rewards PLTL
syntax PLTL, language chosen represent rewarding behaviours,
propositional logic, augmented operators (previously) (since) (see Emerson, 1990). Whereas classical propositional logic formula denotes set states (a subset
S), PLTL formula denotes set finite sequences states (a subset ). formula
without temporal modality expresses property must true current state, i.e.,
last state finite sequence. f specifies f holds previous state (the
state one last). f1 f2 , requires f2 true point sequence, and, unless point present, f1 held ever since. formally,
modelling relation |= stating whether formula f holds finite sequence (i) defined
recursively follows:
(i) |= p iff p , p P, set atomic propositions

24

fiDecision-Theoretic Planning non-Markovian Rewards

(i) |= f iff (i) 6|= f
(i) |= f1 f2 iff (i) |= f1 (i) |= f2
(i) |= f iff > 0 (i 1) |= f
(i) |= f1 f2 iff j i, (j) |= f2 k, j < k i, (k) |= f1
- f > f meaning f true
S, one define useful operators
- f meaning f always true. E.g, g
- g denotes
point, fif
set finite sequences ending state g true first time sequence.
- k f
useful abbreviation k (k times ago), k iterations modality,
ki=1 f (f true k last steps), fik f ki=1 f (f true
k last steps).
Non-Markovian reward functions described set pairs (fi : ri ) fi
PLTL reward formula ri real, semantics reward assigned
sequence sum ri sequence model fi . Below, let
F denote set reward formulae fi description reward function. Bacchus
et al. (1996) give list behaviours might useful reward, together
expression PLTL. instance, f atemporal formula, (f : r) rewards
r units achievement f whenever happens. Markovian reward.
- f : r) rewards every state following (and including) achievement f ,
contrast (
- f : r) rewards first occurrence f . (f fik f : r) rewards occurrence
(f
f every k steps. (n : r) rewards nth state, independently
properties. (2 f1 f2 f3 : r) rewards occurrence f1 immediately followed
f2 f3 . reactive planning, so-called response formulae describe
achievement f triggered condition (or command) c particularly useful.
- c : r) every state f true following first issue
written (f
command rewarded. Alternatively, written (f (f c) : r)
first occurrence f rewarded command. common
- k c : r)
reward achievement f within k steps trigger; write example (f
reward states f holds.
theoretical point view, known (Lichtenstein, Pnueli, & Zuck, 1985)
behaviours representable PLTL exactly corresponding star-free regular
languages. Non star-free behaviours (pp) (reward even number states
containing p) therefore representable. Nor, course, non-regular behaviours
pn q n (e.g. reward taking equal numbers steps left right). shall
speculate severe restriction purposes planning.
2.2.2 Principles Behind Translations
three translations MDP (pltlsim, pltlmin, pltlstr) rely equivalence f1 f2 f2 (f1 (f1 f2 )), decompose temporal modalities
requirement last state sequence (i), requirement
prefix (i 1) sequence. precisely, given state formula f , one com-

25

fiThiebaux, Gretton, Slaney, Price & Kabanza

pute in2 O(||f ||) new formula Reg(f, s) called regression f s. Regression
property that, > 0, f true finite sequence (i) ending = iff
Reg(f, s) true prefix (i 1). is, Reg(f, s) represents must
true previously f true now. Reg defined follows:
Reg(p, s) = > iff p otherwise, p P
Reg(f, s) = Reg(f, s)
Reg(f1 f2 , s) = Reg(f1 , s) Reg(f2 , s)
Reg(f, s) = f
Reg(f1 f2 , s) = Reg(f2 , s) (Reg(f1 , s) (f1 f2 ))
instance, take state p holds q not, take f = (q) (p q),
meaning q must false 1 step ago, must held point
past p must held since q last did. Reg(f, s) = q (p q), is,
f hold now, previous stage, q false p q requirement
still hold. p q false s, Reg(f, s) = , indicating f
cannot satisfied, regardless came earlier sequence.
notational convenience, X set formulae write X X{x | x X}.
translations exploit PLTL representation rewards follows. expanded
state (e-state) generated MDP seen labelled set Sub(F )
subformulae reward formulae F (and negations). subformulae must
(1) true paths leading e-state, (2) sufficient determine current
truth reward formulae F , needed compute current reward. Ideally
also (3) small enough enable that, i.e. contain
subformulae draw history distinctions irrelevant determining reward
one point another. Note however worst-case, number distinctions
needed, even minimal equivalent MDP, may exponential ||F ||. happens
instance formula k f , requires k additional bits information memorising
truth f last k steps.
2.2.3 pltlsim
choice s, Bacchus et al. (1996) consider two cases. simple case,
call pltlsim, MDP obeying properties (1) (2) produced simply labelling
e-state set subformulae Sub(F ) true sequence leading
e-state. MDP generated forward, starting initial e-state labelled
s0 set 0 Sub(F ) subformulae true sequence
hs0 i. successors e-state labelled NMRDP state subformula set
generated follows: labelled successor s0 NMRDP
set subformulae { 0 Sub(F ) | |= Reg( 0 , s0 )}.
instance, consider NMRDP shown Figure 3. set F = {qp} consists
single reward formula. set Sub(F ) consists subformulae reward formula,
2. size ||f || reward formula measured length size ||F || set reward formulae
F measured sum lengths formulae F .

26

fiDecision-Theoretic Planning non-Markovian Rewards

start_state
a(0.16)
p

a(1) b(1)

a(0.04) b(0.2)
a(0.16)

a(0.64)

b(0.8)

q

a(0.2) b(1)

a(0.8)
p, q

a(1) b(1)

initial state, p q false.
p false, action independently sets
p q true probability 0.8.
p q false, action b sets q true
probability 0.8. actions
effect otherwise. reward obtained
whenever q p. optimal policy
apply b q gets produced, making
sure avoid state left-hand side,
apply p gets produced,
apply b indifferently forever.

Figure 3: Another Simple NMRDP
negations, Sub(F ) = {p, q, p, p, q p, p, q, p, p, (q
p)}. equivalent MDP produced pltlsim shown Figure 4.
2.2.4 pltlmin
Unfortunately, MDPs produced pltlsim far minimal. Although could
postprocessed minimisation invoking MDP solution method,
expansion may still constitute serious bottleneck. Therefore, Bacchus et al. (1996) consider
complex two-phase translation, call pltlmin, capable producing
MDP also satisfying property (3). Here, preprocessing phase iterates states
S, computes, state s, set l(s) subformulae, function l
solution fixpoint equation l(s) = F {Reg( 0 , s0 ) | 0 l(s0 ), s0 successor s}.
subformulae l(s) candidates inclusion sets labelling respective
e-states labelled s. is, subsequent expansion phase above, taking
0 l(s0 ) 0 l(s0 ) instead 0 Sub(F ) 0 Sub(F ). subformulae
l(s) exactly relevant way feasible execution sequences starting
e-states labelled rewarded, leads expansion phase produce minimal
equivalent MDP.
Figure 5 shows equivalent MDP produced pltlmin NMRDP example
Figure 3, together function l labels built. Observe
MDP smaller pltlsim MDP: reach state left-hand side
p true q false, point tracking values subformulae,
q cannot become true reward formula cannot either. reflected fact
l({p}) contains reward formula.
worst case, computing l requires space, number iterations S,
exponential ||F ||. Hence question arises whether gain expansion
phase worth extra complexity preprocessing phase. one questions
experimental analysis Section 5 try answer.
2.2.5 pltlstr
pltlstr translation seen symbolic version pltlsim. set
added temporal variables contains purely temporal subformulae PTSub(F ) reward
formulae F , modality prepended (unless already there): = { |

27

fiThiebaux, Gretton, Slaney, Price & Kabanza

start_state
f6,f7,f8,f9,f10
Reward=0
a(0.16)

a(0.04) b(0.2)

a(0.16)

p
f1,f7,f8,f9,f10
Reward=0

a(0.64)

q
f2,f6,f8,f9,f10
Reward=0

a(1) b(1)

p
f1,f3,f4,f7,f10
Reward=0

following subformulae Sub(F ) label
e-states:
f1 : p
f2 : q
f3 : p
f4 : p
f5 : q p
f6 : p
f7 : q
f8 : p
f9 : p
f10 : (q p)

a(0.2) b(1)

a(0.8)

p
f1,f3,f7,f9,f10
Reward=0
a(1)

b(0.8)

p, q
f1,f2,f8,f9,f10
Reward=0
b(1)

a(1)

a(1) b(1)

b(1)

p, q
f1,f2,f3,f9,f10
Reward=0
a(1) b(1)
p, q
f1,f2,f3,f4,f5
Reward=1

a(1) b(1)

Figure 4: Equivalent MDP Produced pltlsim

start_state
f4,f5,f6
Reward=0
a(0.16)
p
f4
Reward=0

a(1) b(1)

a(0.04) b(0.2)

a(0.16)

b(0.8)

q
f4,f5,f6
Reward=0

a(0.64)

function l given by:
l({}) = {q p, p, p}
l({p}) = {q p}
l({q}) = {q p, p, p}
l({p, q}) = {q p, p, p}

a(0.2) b(1)

a(0.8)

following formulae label e-states:
f1 : q p
f2 : p
f3 : p
f4 : (q p)
f5 : p
f6 : p

p, q
f3,f4,f5
Reward=0
a(1)

b(1)

p, q
f2,f3,f4
Reward=0
a(1)

b(1)

p, q
f1,f2,f3
Reward=1

a(1) b(1)

Figure 5: Equivalent MDP Produced pltlmin

28

fiDecision-Theoretic Planning non-Markovian Rewards

q

p

1.00

prv prv p

prv p

0.00

1. dynamics p

1.00

0.00

2. dynamics p

0.00

1.00

3. reward

Figure 6: ADDs Produced pltlstr. prv (previously) stands
PTSub(F ), 6= 0 } { | PTSub(F )}. repeatedly applying equivalence
f1 f2 f2 (f1 (f1 f2 )) subformula PTSub(F ), express current
value, hence reward formulae, function current values formulae
state variables, required compact representation transition
reward models.
NMRDP example Figure 3, set purely temporal variables PTSub(F ) =
{p, p}, identical PTSub(F ). Figure 6 shows ADDs forming
part symbolic MDP produced pltlstr: ADDs describing dynamics
temporal variables, i.e., ADDs describing effects actions b
respective values, ADD describing reward.
complex illustration, consider example (Bacchus et al., 1997)
- (p (q r))} {> (p (q r))}
F = {

PTSub(F ) = {> (p (q r)), p (q r), r}
set temporal variables used
= {t1 : (> (p (q r))), t2 : (p (q r)), t3 : r}
Using equivalences, reward decomposed expressed means
propositions p, q temporal variables t1 , t2 , t3 follows:
> (p (q r))

(p (q r)) (> (p (q r)))
(q r) (p (p (q r))) t1

(q t3 ) (p t2 ) t1
pltlsim, underlying MDP produced pltlstr far minimal
encoded history features even vary one state next. However, size
problematic state-based approaches, structured solution methods
enumerate states able dynamically ignore variables become
irrelevant policy construction. instance, solving MDP, may
29

fiThiebaux, Gretton, Slaney, Price & Kabanza

able determine temporal variables become irrelevant situation
track, although possible principle, costly realised good policy.
dynamic analysis rewards contrast pltlmins static analysis (Bacchus et al.,
1996) must encode enough history determine reward reachable future
states policy.
One question arises circumstances analysis irrelevance structured solution methods, especially dynamic aspects, really effective.
another question experimental analysis try address.

3. fltl: Forward-Looking Approach
noted Section 1 above, two key issues facing approaches NMRDPs
specify reward functions compactly exploit compact representation
automatically translate NMRDP equivalent MDP amenable chosen
solution method. Accordingly, goals provide reward function specification
language translation adapted anytime state-based solution methods.
brief reminder relevant features methods, consider two goals
turn. describe syntax semantics language, notion formula
progression language form basis translation, translation
itself, properties, embedding solution method. call approach
fltl. finish section discussion features distinguish fltl
existing approaches.
3.1 Anytime State-Based Solution Methods
main drawback traditional dynamic programming algorithms policy iteration
(Howard, 1960) explicitly enumerate states reachable s0
entire MDP. interest state-based solution methods, may
produce incomplete policies, enumerate fraction states policy iteration
requires.
Let E() denote envelope policy , set states reachable
(with non-zero probability) initial state s0 policy. defined
E(), say policy complete, incomplete otherwise.
set states E() undefined called fringe policy.
fringe states taken absorbing, value heuristic. common feature
anytime state-based algorithms perform forward search, starting s0
repeatedly expanding envelope current policy one step forward adding one
fringe states. provided admissible heuristic values fringe states,
eventually converge optimal policy without necessarily needing explore
entire state space. fact, since planning operators used compactly represent
state space, may even need construct small subset MDP
returning optimal policy. interrupted convergence, return
possibly incomplete often useful policy.
methods include envelope expansion algorithm (Dean et al., 1995),
deploys policy iteration judiciously chosen larger larger envelopes, using successive policy seed calculation next. recent LAO algorithm (Hansen
30

fiDecision-Theoretic Planning non-Markovian Rewards

& Zilberstein, 2001) combines dynamic programming heuristic search
viewed clever implementation particular case envelope expansion algorithm,
fringe states given admissible heuristic values, policy iteration run
convergence envelope expansions, clever implementation runs
policy iteration states whose optimal value actually affected new fringe
state added envelope. Another example backtracking forward search
space (possibly incomplete) policies rooted s0 (Thiebaux et al., 1995), performed interrupted, point best policy found far returned. Real-time
dynamic programming (RTDP) (Barto et al., 1995) another popular anytime algorithm
MDPs learning real-time (Korf, 1990) deterministic domains,
asymptotic convergence guarantees. RTDP envelope made sample
paths visited frequency determined current greedy policy
transition probabilities domain. RTDP run on-line, off-line given number
steps interrupted. variant called LRTDP (Bonet & Geffner, 2003) incorporates
mechanisms focus search states whose value yet converged, resulting
convergence speed finite time convergence guarantees.
fltl translation present targets anytime algorithms, although
could also used traditional methods value policy iteration.
3.2 Language Semantics
Compactly representing non-Markovian reward functions reduces compactly representing
behaviours interest, behaviour mean set finite sequences states
(a subset ), e.g. {hs0 , s1 i, hs0 , s0 , s1 i, hs0 , s0 , s0 , s1 . . .} Figure 1. Recall
reward issued end prefix (i) set. behaviours compactly
represented, straightforward represent non-Markovian reward functions mappings
behaviours real numbers shall defer looking Section 3.6.
represent behaviours compactly, adopt version future linear temporal logic
(FLTL) (see Emerson, 1990), augmented propositional constant $, intended
read behaviour want reward happened reward received now.
language $FLTL begins set basic propositions P giving rise literals:
L ::= P | P | > | | $
> stand true false, respectively. connectives classical
, temporal modalities (next) U (weak until), giving formulae:
F ::= L | F F | F F | F | F U F
weak: f1 U f2 means f1 true f2 is, ever. Unlike
commonly used strong until, imply f2 eventually true.
allows us define useful operator (always): f f U (f always true
k f k iterations modality (f
on). also adopt notations
Wk
true exactly k steps), k f i=1 f (f true within next k steps),
V
k f ki=1 f (f true throughout next k steps).
Although negation officially occurs literals, i.e., formulae negation
normal form (NNF), allow write formulae involving usual way,
31

fiThiebaux, Gretton, Slaney, Price & Kabanza

provided equivalent NNF. every formula equivalent,
literal $ eventualities (f true time)
expressible. restrictions deliberate. use notation
logic theorise allocation rewards, would indeed need means say
rewards received express features liveness (always,
reward eventually), fact using mechanism ensuring
rewards given be, restricted purpose eventualities
negated dollar needed. fact, including would create technical difficulties
relating formulae behaviours represent.
semantics language similar FLTL, important difference:
interpretation constant $ depends behaviour B want reward
(whatever is), modelling relation |= must indexed B. therefore write
(, i) |=B f mean formula f holds i-th stage arbitrary sequence ,
relative behaviour B. Defining |=B first step description semantics:
(, i) |=B $ iff (i) B
(, i) |=B >
(, i) 6|=B
(, i) |=B p, p P, iff p
(, i) |=B p, p P, iff p 6
(, i) |=B f1 f2 iff (, i) |=B f1 (, i) |=B f2
(, i) |=B f1 f2 iff (, i) |=B f1 (, i) |=B f2
(, i) |=B

f

iff (, + 1) |=B f

(, i) |=B f1 U f2 iff k (j, j k (, j) 6|=B f2 ) (, k) |=B f1
Note except subscript B first rule, standard FLTL
semantics, therefore $-free formulae keep FLTL meaning. FLTL,
say |=B f iff (, 0) |=B f , |=B f iff |=B f .
modelling relation |=B seen specifying formula holds,
reading takes B input. next final step use |=B relation define,
formula f , behaviour Bf represents, must rather assume
f holds, solve B. instance, let f (p $), i.e., get rewarded
every time p true. would like Bf set finite sequences ending
state containing p. arbitrary f , take Bf set prefixes
rewarded f hold sequences:

Definition 2 Bf {B | |=B f }
understand Definition 2, recall B contains prefixes end get
reward $ evaluates true. Since f supposed describe way rewards
received arbitrary sequence, interested behaviours B make $
true way make f hold without imposing constraints evolution
world. However, may many behaviours property, take

32

fiDecision-Theoretic Planning non-Markovian Rewards

intersection,3 ensuring Bf reward prefix prefix
every behaviour satisfying f . pathological cases (see Section 3.4), makes Bf
coincide (set-inclusion) minimal behaviour B |=B f . reason
stingy semantics, making rewards minimal, f actually say rewards
allocated prefixes required truth. instance, (p $) says
reward given every time p true, even though generous distribution
rewards would consistent it.
3.3 Examples
intuitively clear many behaviours specified means $FLTL formulae.
simple way general translate past future tense expressions,4 examples used illustrate PLTL Section 2.2 expressible
naturally $FLTL, follows.
classical goal formula g saying goal p rewarded whenever happens
easily expressed: (p $). already noted, Bg set finite sequences states
p holds last state. care p achieved get rewarded
state on, write (p $). behaviour formula represents
set finite state sequences least one state p holds. contrast,
formula p U (p $) stipulates first occurrence p rewarded (i.e.
specifies behaviour Figure 1). reward occurrence p every k
steps, write (( k+1 p k p) k+1 $).
response formulae, achievement p triggered command c,
write (c (p $)) reward every state p true following first
issue command. reward first occurrence p command, write
(c (p U (p$))). bounded variants reward goal achievement
within k steps trigger command, write example (c k (p $)) reward
states p holds.
also worth noting express simple behaviours involving past tense operators.
stipulate reward p always true, write $ U p. say rewarded
p true since q was, write (q ($ U p)).
Finally, often find useful reward holding p occurrence q.
neatest expression q U ((p q) (q $)).
3.4 Reward Normality
$FLTL therefore quite expressive. Unfortunately, rather expressive,
contains formulae describe unnatural allocations rewards. instance,
may make rewards depend future behaviours rather past, may
3.
B |=B f , case $-free f logical theorem,
Bf i.e. following normal set-theoretic conventions. limiting case harm, since
$-free formulae describe attribution rewards.
4. open question whether set representable behaviours $FLTL PLTL,
star-free regular languages. Even behaviours same, little hope
practical translation one exists.

33

fiThiebaux, Gretton, Slaney, Price & Kabanza

leave open choice several behaviours rewarded.5 example
dependence future p $, stipulates reward p going hold
next. call formula reward-unstable. reward-stable f amounts
whether particular prefix needs rewarded order make f true depend
future sequence. example open choice behavior reward
(p $) (p $) says either reward achievements goal p
reward achievements p determine which. call formula rewardindeterminate. reward-determinate f amounts set behaviours
modelling f , i.e. {B | |=B f }, unique minimum. not, Bf insufficient (too
small) make f true.
investigating $FLTL (Slaney, 2005), examine notions reward-stability
reward-determinacy depth, motivate claim formulae rewardstable reward-determinate call reward-normal precisely
capture notion funny business. intuition ask reader
note, needed rest paper. reference then, define:
Definition 3 f reward-normal iff every every B , |=B f iff
every i, (i) Bf (i) B.
property reward-normality decidable (Slaney, 2005). Appendix give
simple syntactic constructions guaranteed result reward-normal formulae.
reward-abnormal formulae may interesting, present purposes restrict attention
reward-normal ones. Indeed, stipulate part method reward-normal
formulae used represent behaviours. Naturally, formulae Section 3.3
normal.
3.5 $FLTL Formula Progression
defined language represent behaviours rewarded, turn
problem computing, given reward formula, minimum allocation rewards states
actually encountered execution sequence, way satisfy formula.
ultimately wish use anytime solution methods generate state sequences
incrementally via forward search, computation best done fly, sequence
generated. therefore devise incremental algorithm based model-checking
technique normally used check whether state sequence model FLTL formula
(Bacchus & Kabanza, 1998). technique known formula progression
progresses pushes formula sequence.
progression technique shown Algorithm 1. essence, computes modelling relation |=B given Section 3.2. However,unlike definition |=B , designed
useful states sequence become available one time, defers
evaluation part formula refers future point next
state becomes available. Let state, say , last state sequence prefix (i)
5. difficulties inherent use linear-time formalisms contexts principle
directionality must enforced. shared instance formalisms developed reasoning
actions Event Calculus LTL action theories (see e.g., Calvanese, De Giacomo, &
Vardi, 2002).

34

fiDecision-Theoretic Planning non-Markovian Rewards

generated far, let b boolean true iff (i) behaviour B
rewarded. Let $FLTL formula f describe allocation rewards possible
futures. progression f given b, written Prog(b, s, f ), new formula
describe allocation rewards possible futures next state, given
passed s. Crucially, function Prog Markovian, depending
current state single boolean value b. Note Prog computable
linear time length f , $-free formulae, collapses FLTL formula
progression (Bacchus & Kabanza, 1998), regardless value b. assume Prog
incorporates usual simplification sentential constants >: f simplifies
, f > simplifies f , etc.
Algorithm 1 $FLTL Progression
Prog(true, s, $)
= >
Prog(false, s, $)
=
Prog(b, s, >)
= >
Prog(b, s, )
=
Prog(b, s, p)
= > iff p otherwise
Prog(b, s, p)
= > iff p 6 otherwise
Prog(b, s, f1 f2 ) = Prog(b, s, f1 ) Prog(b, s, f2 )
Prog(b, s, f1 f2 ) = Prog(b, s, f1 ) Prog(b, s, f2 )
Prog(b, s, f )
= f
Prog(b, s, f1 U f2 ) = Prog(b, s, f2 ) (Prog(b, s, f1 ) f1 U f2 )
Rew(s, f )
$Prog(s, f )

= true iff Prog(false, s, f ) =
= Prog(Rew(s, f ), s, f )

fundamental property Prog following. b ((i) B):
Property 1 (, i) |=B f iff (, + 1) |=B Prog(b, , f )
Proof:

See Appendix B.



Like |=B , function Prog seems require B (or least b) input, course
progression applied practice f one new state time ,
really want compute appropriate B, namely represented
f . So, similarly Section 3.2, turn second step, use Prog
decide fly whether newly generated sequence prefix (i) Bf
allocated reward. purpose functions $Prog Rew, also given
Algorithm 1. Given f , function $Prog Algorithm 1 defines infinite sequence
formulae hf0 , f1 , . . .i obvious way:
f0 = f
fi+1 = $Prog(i , fi )
decide whether prefix (i) rewarded, Rew first tries progressing
formula fi boolean flag set false. gives consistent result,
need reward prefix continue without rewarding (i), result
35

fiThiebaux, Gretton, Slaney, Price & Kabanza

know (i) must rewarded order satisfy f . case,
obtain fi+1 must progress fi again, time boolean flag set
value true. sum up, behaviour corresponding f {(i)|Rew(i , fi )}.
illustrate behaviour $FLTL progression, consider formula f = p U (p $)
stating reward received first time p true. Let state p
holds, Prog(false, s, f ) = ( p U (p $)) . Therefore, since formula
progressed , Rew(s, f ) true reward received. $Prog(s, f ) = Prog(true, s, f ) =
> ( p U (p $)) >, reward formula fades away affect subsequent
progression steps. If, hand, p false s, Prog(false, s, f ) = (>
p U (p$)) p U (p$)). Therefore, since formula progressed , Rew(s, f )
false reward received. $Prog(s, f ) = Prog(false, s, f ) = p U (p$), reward
formula persists subsequent progression steps.
following theorem states weak assumptions, rewards correctly allocated progression:
Theorem 1 Let f reward-normal, let hf0 , f1 , . . .i result progressing
successive states sequence using function $Prog. Then, provided
fi , Rew(i , fi ) iff (i) Bf .
Proof: See Appendix B



premise theorem f never progresses . Indeed fi =
i, means even rewarding (i) suffice make f true, something must
gone wrong: earlier stage, boolean Rew made false
made true. usual explanation original f reward-normal.
instance p $, reward unstable, progresses next state p
true there: regardless 0 , f0 = p $ = p $, Rew(0 , f0 ) = false, f1 = p,
p 1 f2 = . However, (admittedly bizarre) possibilities exist:
example, although p $ reward-unstable, substitution instance > $,
also progresses steps, logically equivalent $ reward-normal.
progression method deliver correct minimal behaviour cases
(even reward-normal cases) would backtrack choice values
boolean flags. interest efficiency, choose allow backtracking. Instead,
algorithm raises exception whenever reward formula progresses , informs
user sequence caused problem. onus thus placed domain
modeller select sensible reward formulae avoid possible progression .
noted worst case, detecting reward-normality cannot easier
decision problem $FLTL expected simple
syntactic criterion reward-normality. practice, however, commonsense precautions
avoiding making rewards depend explicitly future tense expressions suffice
keep things normal routine cases. generous class syntactically recognisable
reward-normal formulae, see Appendix A.
3.6 Reward Functions
language defined far, able compactly represent behaviours.
extension non-Markovian reward function straightforward. represent
36

fiDecision-Theoretic Planning non-Markovian Rewards

function set6 $FLTL IR formulae associated real valued rewards.
call reward function specification. formula f associated reward r ,
write (f : r) . rewards assumed independent additive,
reward function R represented given by:
X
{r | (i) Bf }
Definition 4 R ((i)) =
(f :r)

E.g, {p U (p $) : 5.2, (q $) : 7.3}, get reward 5.2 first time p
holds, reward 7.3 first time q holds onwards, reward 12.5
conditions met, 0 otherwise.
Again, progress reward function specification compute reward
stages . before, progression defines sequence h0 , 1 , . . .i reward function
specifications, i+1 = RProg(i , ), RProg function applies Prog
formulae reward function specification:
RProg(s, ) = {(Prog(s, f ) : r) | (f : r) }
Then, total reward received stage simply sum real-valued rewards
granted progression function behaviours represented formulae :
X
{r | Rew(i , f )}
(f :r)i

proceeding way, get expected analog Theorem 1, states progression
correctly computes non-Markovian reward functions:
Theorem 2 Let reward-normal7 reward function specification, let h0 , 1 . . .i
result progressing successive states
X sequence using function
RProg. Then, provided ( : r) 6 i,
{r | Rew(i , f )} = R ((i)).
(f :r)i

Proof:

Immediate Theorem 1.



3.7 Translation MDP
exploit compact representation non-Markovian reward function reward
function specification translate NMRDP equivalent MDP amenable statebased anytime solution methods. Recall Section 2 e-state MDP
labelled state NMRDP history information sufficient determine
immediate reward. case compact representation reward function specification
0 , additional information summarised progression 0
sequence states passed through. e-state form hs, i,
6. Strictly speaking, multiset, convenience represent set, rewards multiple
occurrences formula multiset summed.
7. extend definition reward-normality reward specification functions obvious way,
requiring reward formulae involved reward normal.

37

fiThiebaux, Gretton, Slaney, Price & Kabanza

state, $FLTL IR reward function specification (obtained progression).
Two e-states hs, ht, equal = t, immediate rewards same,
results progressing semantically equivalent.8
Definition 5 Let = hS, s0 , A, Pr, Ri NMRDP, 0 reward function specification representing R (i.e., R0 = R, see Definition 4). translate MDP
D0 = hS 0 , s00 , A0 , Pr0 , R0 defined follows:
1. 0 2$FLTL IR
2. s00 = hs0 , 0
3. A0 (hs, i) = A(s)
Pr(s, a, s0 ) 0 = RProg(s, )
0
otherwise
0
0
6 (hs, i), Pr (hs, i, a, ) undefined
X
5. R0 (hs, i) =
{r | Rew(s, f )}
4. A0 (hs, i), Pr0 (hs, i, a, hs0 , 0 i) =



(f :r)

6. s0 0 , s0 reachable A0 s00 .
Item 1 says e-states labelled state reward function specification. Item
2 says initial e-state labelled initial state original reward
function specification. Item 3 says action applicable e-state applicable
state labelling it. Item 4 explains successor e-states probabilities
computed. Given action applicable e-state hs, i, successor e-state
labelled successor state s0 via NMRDP progression
s. probability e-state Pr(s, a, s0 ) NMRDP. Note
cost computing Pr0 linear computing Pr sum lengths
formulae . Item 5 motivated (see Section 3.6). Finally, since items 15
leave open choice many MDPs differing unreachable states contain,
item 6 excludes irrelevant extensions. easy show translation leads
equivalent MDP, defined Definition 1. Obviously, function required
Definition1 given (hs, i) = s, proof matter checking conditions.
practical implementation, labelling one step ahead definition:
label initial e-state RProg(s0 , 0 ) compute current reward current reward specification label progression predecessor reward specifications
current state rather predecessor states. apparent below,
potential reduce number states generated MDP.
Figure 7 shows equivalent MDP produced $FLTL version NMRDP
example Figure 3. Recall example, PLTL reward formula q p.
$FLTL, allocation rewards described ((p q) $). figure also
8. Care needed notion semantic equivalence. rewards additive, determining
equivalence may involve arithmetic well theorem proving. example, reward function specification {(p $ : 3), (q $ : 2)} equivalent {((p q) $ : 5), ((p q) $ : 3), ((p q) $ : 2)}
although one-one correspondence formulae two sets.

38

fiDecision-Theoretic Planning non-Markovian Rewards

start_state
f1
Reward=0
a(0.16)
p
f1,f2
Reward=0
a(1)
p
f1,f2,f3
Reward=0

a(0.04) b(0.2)

a(0.16)

a(0.64)

q
f1
Reward=0

b(1)

a(1) b(1)

b(0.8)

a(0.2) b(1)

a(0.8)

following formulae label e-states:
f1 : ((p q) $)
f2 : q $
f3 : q $

p, q
f1,f2
Reward=0
a(1) b(1)
p, q
f1,f2,f3
Reward=0
a(1) b(1)
p, q
f1,f2,f3
Reward=1

a(1) b(1)

Figure 7: Equivalent MDP Produced fltl
shows relevant formulae labelling e-states, obtained progression reward
formula. Note without progressing one step ahead, would 3 e-states state
{p} left-hand side, labelled {f1 }, {f1 , f2 }, {f1 , f2 , f3 }, respectively.
3.8 Blind Minimality
size MDP obtained, i.e. number e-states contains key issue
us, amenable state-based solution methods. Ideally, would like
MDP minimal size. However, know method building minimal
equivalent MDP incrementally, adding parts required solution method. since
worst case even minimal equivalent MDP larger NMRDP
factor exponential length reward formulae (Bacchus et al., 1996), constructing
entirely would nullify interest anytime solution methods.
However, explain, Definition 5 leads equivalent MDP exhibiting relaxed
notion minimality, amenable incremental construction. inspection,
may observe wherever e-state hs, successor hs0 , 0 via action a,
means order succeed rewarding behaviours described means
execution sequences start going s0 via a, necessary future
starting s0 succeeds rewarding behaviours described 0 . hs,
minimal equivalent MDP, really execution sequences succeeding
rewarding behaviours described , hs0 , 0 must also minimal MDP.
is, construction progression introduce e-states priori needed.
Note e-state priori needed may really needed: may fact
execution sequence using available actions exhibits given behaviour.

39

fiThiebaux, Gretton, Slaney, Price & Kabanza

instance, consider response formula (p ( k q k $)), i.e., every time trigger p
true, rewarded k steps later provided q true then. Obviously, whether p
true stage affects way future states rewarded. However,
transition relation happens property k steps state satisfying p,
state satisfying q reached, posteriori p irrelevant, need
label e-states differently according whether p true observe occurrence
example Figure 7, leads fltl produce extra state
bottom left Figure. detect cases, would look perhaps quite deep
feasible futures, cannot constructing e-states fly. Hence
relaxed notion call blind minimality always coincide absolute
minimality.
formalise difference true blind minimality. purpose,
convenient define functions mapping e-states e functions
IR intuitively assigning rewards sequences NMRDP starting (e). Recall
Definition 1 maps e-state MDP underlying NMRDP state.
Definition 6 Let NMRDP. Let 0 set e-states equivalent MDP D0
f0 (s0 )
D. Let e reachable e-state 0 . Let 0 (i) sequence e-states
0
e 0 ) obtained
0 (i) = e. Let (i) corresponding sequence D(s
sense that, j i, (j) = (0j ). , define

(e) : 7



(e) : 7

R((i 1); ) 0 =
0
otherwise

e i)
R((i 1); ) D(
0
otherwise

unreachable e-state e, define (e)() (e)() 0 .
Note carefully difference . former describes rewards assigned
continuations given state sequence, latter confines rewards feasible
continuations. Note also well-defined despite indeterminacy
choice 0 (i), since clause 4 Definition 1, choices lead values
R.
Theorem 3 Let 0 set e-states equivalent MDP D0 = hS, s0 , A, Pr, Ri.
D0 minimal iff every e-state 0 reachable 0 contains two distinct e-states s01
s02 (s01 ) = (s02 ) (s01 ) = (s02 ).
Proof:

See Appendix B.



Blind minimality similar, except that, since looking ahead, distinction
drawn feasible trajectories others future s:
Definition 7 Let 0 set e-states equivalent MDP D0 = hS, s0 , A, Pr, Ri.
D0 blind minimal iff every e-state 0 reachable 0 contains two distinct estates s01 s02 (s01 ) = (s02 ) (s01 ) = (s02 ).
40

fiDecision-Theoretic Planning non-Markovian Rewards

Theorem 4 Let D0 translation Definition 5. D0 blind minimal
equivalent MDP D.
Proof:

See Appendix B.



size difference blind-minimal minimal MDPs depend
precise interaction rewards dynamics problem hand, making theoretical analyses difficult experimental results rather anecdotal. However, experiments
Section 5 6 show computation time point view, often preferable work blind-minimal MDP invest overhead computing
truly minimal one.
Finally, recall syntactically different semantically equivalent reward function
specifications define e-state. Therefore, neither minimality blind minimality
achieved general without equivalence check least complex theorem
proving LTL. pratical implementations, avoid theorem proving favour embedding (fast) formula simplification progression regression algorithms.
means principle approximate minimality blind minimality,
appears enough practical purposes.
3.9 Embedded Solution/Construction
Blind minimality essentially best achievable anytime state-based solution methods typically extend envelope one step forward without looking deeper
future. translation blind-minimal MDP trivially embedded
solution methods. results on-line construction MDP: method entirely
drives construction parts MDP feels need explore,
leave others implicit. time short, suboptimal even incomplete policy may
returned, fraction state expanded state spaces might constructed.
Note solution method raise exception soon one reward formulae progresses , i.e., soon expanded state hs, built ( : r) ,
since acts detector unsuitable reward function specifications.
extent enabled blind minimality, approach allows dynamic analysis
reward formulae, much pltlstr (Bacchus et al., 1997). Indeed, execution
sequences feasible particular policy actually explored solution method contribute analysis rewards policy. Specifically, reward formulae generated
progression given policy determined prefixes execution sequences
feasible policy. dynamic analysis particularly useful, since relevance
reward formulae particular policies (e.g. optimal policy) cannot detected priori.
forward-chaining planner TLPlan (Bacchus & Kabanza, 2000) introduced idea
using FLTL specify domain-specific search control knowledge formula progression
prune unpromising sequential plans (plans violating knowledge) deterministic
search spaces. shown provide enormous time gains, leading TLPlan
win 2002 planning competition hand-tailored track. approach based
progression, provides elegant way exploit search control knowledge, yet
context decision-theoretic planning. results dramatic reduction

41

fiThiebaux, Gretton, Slaney, Price & Kabanza

fraction MDP constructed explored, therefore substantially better
policies deadline.
achieve follows. specify, via $-free formula c0 , properties know
must verified paths feasible promising policies. simply progress c0
alongside reward function specification, making e-states triples hs, , ci c $-free
formula obtained progression. prevent solution method applying action
leads control knowledge violated, action applicability condition (item
3 Definition 5) becomes: A0 (hs, , ci) iff A(s) c 6= (the changes
straightforward). instance, effect control knowledge formula (p q)
remove consideration feasible path p followed q. detected
soon violation occurs, formula progresses . Although paper focuses
non-Markovian rewards rather dynamics, noted $-free formulae
also used express non-Markovian constraints systems dynamics,
incorporated approach exactly control knowledge.
3.10 Discussion
Existing approaches (Bacchus et al., 1996, 1997) advocate use PLTL finite
past specify non-Markovian rewards. PLTL style specification, describe
past conditions get rewarded now, $FLTL describe
conditions present future future states rewarded.
behaviours rewards may scheme, naturalness thinking one
style depends case. Letting kids strawberry dessert
good day fits naturally past-oriented account rewards, whereas
promising may watch movie tidy room (indeed, making sense
whole notion promising) goes naturally $FLTL. One advantage PLTL
formulation trivially enforces principle present rewards depend
future states. $FLTL, responsibility placed domain modeller. best
offer exception mechanism recognise mistakes effects appear,
syntactic restrictions. hand, greater expressive power $FLTL opens
possibility considering richer class decision processes, e.g. uncertainty
rewards received (the dessert movie) (some time next week,
rains).
rate, believe $FLTL better suited PLTL solving NMRDPs
using anytime state-based solution methods. pltlsim translation could easily embedded solution method, loses structure original formulae
considering subformulae individually. Consequently, expanded state space easily
becomes exponentially bigger blind-minimal one. problematic
solution methods consider, size severely affects performance solution
quality. pre-processing phase pltlmin uses PLTL formula regression find sets
subformulae potential labels possible predecessor states, subsequent
generation phase builds MDP representing histories make difference way actually feasible execution sequences rewarded.
recover structure original formula, best case, MDP produced
exponentially smaller blind-minimal one. However, prohibitive cost

42

fiDecision-Theoretic Planning non-Markovian Rewards

pre-processing phase makes unsuitable anytime solution methods. consider method based PLTL regression achieve meaningful relaxed
notion minimality without costly pre-processing phase. fltl approach based
$FLTL progression precisely that, letting solution method resolve
tradeoff quality cost principled way intermediate two extreme
suggestions above.
structured representation solution methods targeted Bacchus et al. (1997)
differ anytime state-based solution methods fltl primarily aims at, particular
require explicit state enumeration all. Here, non-minimality
problematic state-based approaches. virtue size MDP produced,
pltlstr translation is, pltlsim, clearly unsuitable anytime state-based methods.9
another sense, too, fltl represents middle way, combining advantages conferred
state-based structured approaches, e.g. pltlmin one side, pltlstr
other. former fltl inherits meaningful notion minimality. latter,
approximate solution methods used perform restricted dynamic analysis
reward formulae. particular, formula progression enables even state-based methods
exploit structure $FLTL space. However, gap blind
true minimality indicates progression alone insufficient always fully exploit
structure. hope pltlstr able take advantage full structure
reward function, also possibility fail exploit even much structure
fltl, efficiently. empirical comparison three approaches needed answer
question identify domain features favoring one other.

4. NMRDPP
first step towards decent comparison different approaches framework
includes all. Non-Markovian Reward Decision Process Planner, nmrdpp,
platform development experimentation approaches NMRDPs.
provides implementation approaches described common framework,
within single system, common input language. nmrdpp available on-line,
see http://rsise.anu.edu.au/~charlesg/nmrdpp. worth noting Bacchus et al.
(1996, 1997) report implementation approaches.
4.1 Input language
input language enables specification actions, initial states, rewards, search
control-knowledge. format action specification essentially
SPUDD system (Hoey et al., 1999). reward specification one formulae,
associated name real number. formulae either PLTL $FLTL.
Control knowledge given language chosen reward. Control
knowledge formulae verified sequence states feasible
generated policies. Initial states simply specified part control knowledge
explicit assignments propositions.
9. would interesting, hand, use pltlstr conjunction symbolic versions
methods, e.g. Symbolic LAO* (Feng & Hansen, 2002) Symbolic RTDP (Feng, Hansen, & Zilberstein,
2003).

43

fiThiebaux, Gretton, Slaney, Price & Kabanza

action flip
heads (0.5)
endaction
action tilt
heads (heads (0.9) (0.1))
endaction
heads = ff
[first, 5.0]? heads ~prv (pdi heads)
[seq, 1.0]? (prv^2 heads) (prv heads) ~heads
Figure 8: Input Coin Example. prv (previously) stands
-.
pdi (past diamond) stands
instance, consider simple example consisting coin showing either heads
tails (heads). two actions performed. flip action changes
coin show heads tails 50% probability. tilt action changes 10%
probability, otherwise leaving is. initial state tails. get reward 5.0
- heads PLTL) reward 1.0
first head (this written heads
time achieve sequence heads, heads, tails (2 heads heads heads PLTL).
input language, NMRDP described shown Figure 8.
4.2 Common framework
common framework underlying nmrdpp takes advantage fact NMRDP
solution methods can, general, divided distinct phases preprocessing,
expansion, solving. first two optional.
pltlsim, preprocessing simply computes set Sub(F ) subformulae reward
formulae. pltlmin, also includes computing labels l(s) state s.
pltlstr, preprocessing involves computing set temporal variables well
ADDs dynamics rewards. fltl require preprocessing.
Expansion optional generation entire equivalent MDP prior solving.
Whether off-line expansion sensible depends MDP solution method used.
state-based value policy iteration used, MDP needs expanded anyway.
If, hand, anytime search algorithm structured method used,
definitely bad idea. experiments, often used expansion solely purpose
measuring size generated MDP.
Solving MDP done using number methods. Currently, nmrdpp provides
implementations classical dynamic programming methods, namely state-based value
policy iteration (Howard, 1960), heuristic search methods: state-based LAO* (Hansen &
Zilberstein, 2001) using either value policy iteration subroutine, one structured
method, namely SPUDD (Hoey et al., 1999). Prime candidates future developments
(L)RTDP (Bonet & Geffner, 2003), symbolic LAO* (Feng & Hansen, 2002), symbolic
RTDP (Feng et al., 2003).
44

fiDecision-Theoretic Planning non-Markovian Rewards

load coin NMRDP
pltlstr preprocessing

> loadWorld(coin)
> preprocess(sPltl)
> startCPUtimer
> spudd(0.99, 0.0001)
> stopCPUtimer
> readCPUtimer
1.22000
> iterationCount
1277
> displayDot(valueToDot)
Expected value

18.87

18.62

report number iterations
display ADD value function

(prv heads)

(prv (prv pdi heads))

23.87

report solving time

heads

(prv heads)

(prv (prv pdi heads))

solve MDP SPUDD(, )

23.62

(prv^2 heads)

(prv pdi heads)

18.25

23.15

(prv pdi heads)

19.25

24.15

display policy

> displayDot(policyToDot)
Optimal policy

heads

(prv heads)

flip

>
>
>
6
>

tilt

pltlmin preprocessing
completely expand MDP
report MDP size

preprocess(mPltl)
expand
domainStateSize
printDomain ("") | show-domain.rb
Reward=0
flip(0.5)

flip(0.5)

display postcript rendering MDP

tilt(0.9)

tilt(0.1)

heads
Reward=5
flip(0.5)
heads
Reward=0
tilt(0.1)

flip(0.5)

tilt(0.9)

tilt(0.9)

flip(0.5)

tilt(0.1)

flip(0.5)

Reward=1
tilt(0.9)

flip(0.5)

tilt(0.9)

flip(0.5)

tilt(0.1)

tilt(0.1)

flip(0.5)
Reward=0
flip(0.5)

flip(0.5)

flip(0.5)

tilt(0.9)

tilt(0.1)

heads
Reward=0

solve MDP VI(, )
report number iterations

> valIt(0.99, 0.0001)
> iterationCount
1277
> getPolicy
...

output policy (textual)

Figure 9: Sample Session
45

fiThiebaux, Gretton, Slaney, Price & Kabanza

4.3 Approaches covered
Altogether, various types preprocessing, choice whether expand,
MDP solution methods, give rise quite number NMRDP approaches, including,
limited previously mentioned (see e.g. pltlstr(a) below). combinations possible. E.g., state-based processing variants incompatible structured
solution methods (the converse possible principle, however). Also, present
structured form preprocessing $FLTL formulae.
pltlstr(a) example interesting variant pltlstr, obtain
considering additional preprocessing, whereby state space explored (without explicitly
enumerating it) produce BDD representation e-states reachable start
state. done starting BDD representing start e-state, repeatedly
applying action. Non-zero probabilities converted ones result or-ed
last result. action adds reachable e-states BDD,
sure represents reachable e-state space. used additional control
knowledge restrict search. noted without phase pltlstr makes
assumptions start state, thus left possible disadvantage. Similar
structured reachability analysis techniques used symbolic implementation
LAO* (Feng & Hansen, 2002). However, important aspect
temporal variables also included BDD.
4.4 nmrdpp System
nmrdpp controlled command language, read either file interactively. command language provides commands different phases (preprocessing,
expansion, solution) methods, commands inspect resulting policy value
functions, e.g. rendering via DOT (AT&T Labs-Research, 2000), well supporting
commands timing memory usage. sample session, coin NMRDP
successively solved pltlstr pltlmin shown Figure 9.
nmrdpp implemented C++, makes use number supporting libraries.
particular, relies heavily CUDD package manipulating ADDs (Somenzi,
2001): action specification trees converted stored ADDs system,
moreover structured algorithms rely heavily CUDD ADD computations.
state-based algorithms make use MTL Matrix Template Library matrix
operations. MTL takes advantage modern processor features MMX SSE
provides efficient sparse matrix operations. believe implementations
MDP solution methods comparable state art. instance, found
implementation SPUDD comparable performance (within factor 2)
reference implementation (Hoey et al., 1999). hand, believe data
structures used regression progression temporal formulae could optimised.

5. Experimental Analysis
faced three substantially different approaches easy compare,
performance depend domain features varied structure
transition model, type, syntax, length temporal reward formula, presence

46

fiDecision-Theoretic Planning non-Markovian Rewards

rewards unreachable irrelevant optimal policy, availability good heuristics
control-knowledge, etc, interactions factors. section,
report experimental investigation influence factors try
answer questions raised previously:10
1. dynamics domain predominant factor affecting performance?
2. type reward major factor?
3. syntax used describe rewards major factor?
4. overall best method?
5. overall worst method?
6. preprocessing phase pltlmin pay, compared pltlsim?
7. simplicity fltl translation compensate blind-minimality,
benefit true minimality outweigh cost pltlmin preprocessing?
8. dynamic analyses rewards pltlstr fltl effective?
9. one analyses powerful, rather complementary?
cases all, able identify systematic patterns. results
section obtained using Pentium4 2.6GHz GNU/Linux 2.4.20 machine 500MB
ram.
5.1 Preliminary Remarks
Clearly, fltl pltlstr(a) great potential exploiting domain-specific heuristics control-knowledge; pltlmin less so. avoid obscuring results, therefore
refrained incorporating features experiments. running LAO*,
heuristic value state crudest possible (the sum reward values
problem). Performance results interpreted light necessarily
reflect practical abilities methods able exploit features.
begin general observations. One question raised whether
gain PLTL expansion phase worth expensive preprocessing performed
pltlmin, i.e. whether pltlmin typically outperforms pltlsim. definitively answer
question: pathological exceptions, preprocessing pays. found expansion
bottleneck, post-hoc minimisation MDP produced pltlsim
help much. pltlsim therefore little practical interest, decided
report results performance, often order magnitude worse
pltlmin. Unsurprisingly, also found pltlstr would typically scale larger state
spaces, inevitably leading outperform state-based methods. However, effect
uniform: structured solution methods sometimes impose excessive memory requirements
makes uncompetitive certain cases, example n f , large n,
features reward formula.
10. executive summary answers executive reader. 1. no, 2. yes, 3. yes, 4. pltlstr
fltl, 5. pltlsim, 6. yes, 7. yes no, respectively, 8. yes, 9. yes, respectively.

47

fiThiebaux, Gretton, Slaney, Price & Kabanza

5.2 Domains
Experiments performed four hand-coded domains (propositions + dynamics)
random domains. hand-coded domain n propositions pi , dynamics
makes every state possible eventually reachable initial state
propositions false. first two domains, spudd-linear spudd-expon
discussed Hoey et al. (1999); two others own.
intention spudd-linear take advantage best case behaviour
SPUDD. proposition pi , action ai sets pi true propositions
pj , 1 j < false. spudd-expon, used Hoey et al. (1999) demonstrate
worst case behaviour SPUDD. proposition pi , action ai sets pi
true propositions pj , 1 j < true (and sets pi false otherwise),
sets latter propositions false. third domain, called on/off, one turn-on
one turn-off action per proposition. turn-on-pi action probabilistically
succeeds setting pi true pi false. turn-off action similar. fourth
domain, called complete, fully connected reflexive domain. proposition pi
action ai sets pi true probability i/(n + 1) (and false otherwise)
pj , j 6= true false probability 0.5. Note ai cause transition
2n states.
Random domains size n also involve n propositions. method generating
dynamics detailed appendix C. Let us summarise saying able
generate random dynamics exhibiting given degree structure given degree
uncertainty. Lack structure essentially measures bushiness internal part
ADDs representing actions, uncertainty measures bushiness leaves.
5.3 Influence Dynamics
interaction dynamics reward certainly affects performance
different approaches, though strikingly factors reward type (see
below). found reward scheme, varying degree structure
uncertainty generally change relative success different approaches.
instance, Figures 10 11 show average run time methods function
degree structure, resp. degree uncertainty, random problems size n = 6
reward n > (the state encountered stage n rewarded, regardless properties11 ).
Run-time increases slightly degrees, significant change relative
performance. typical graphs obtain rewards.
Clearly, counterexamples observation exist. notable cases
extreme dynamics, instance spudd-expon domain. Although small values
n, n = 6, pltlstr approaches faster others handling reward
n > virtually type dynamics encountered, perform poorly
reward spudd-expon. explained fact small fraction
spudd-expon states reachable first n steps. n steps, fltl immediately
recognises reward consequence, formula progressed >.
pltlmin discovers fact expensive preprocessing. pltlstr,
hand, remains concerned prospect reward, pltlsim would.
11.

n $

$FLTL

48

fiAverage CPU time (sec)

Decision-Theoretic Planning non-Markovian Rewards

30
25
20
15
10
5

0.1

0.3

0.5

0.7

0.9

1.1

Structure (0:Structured, ... 1:Unstructured)
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 10: Changing Degree Structure

Average CPU time (sec)

35
25
20
15
10
5

0

0.2

0.4

0.6

0.8

1

1.2

Uncertainty (0:Certain, ... 1:Uncertain)
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 11: Changing Degree Uncertainty
5.4 Influence Reward Types
type reward appears stronger influence performance dynamics.
unsurprising, reward type significantly affects size generated MDP:
certain rewards make size minimal equivalent MDP increase constant
number states constant factor, others make increase factor exponential
length formula. Table 1 illustrates this. third column reports size
minimal equivalent MDP induced formulae left hand side.12
legitimate question whether direct correlation size increase
(in)appropriateness different methods. instance, might expect state-based
methods particularly well conjunction reward types inducing small MDP
12. figures necessarily valid non-completely connected NMRDPs. Unfortunately, even
completely connected domains, appear much cheaper way determine MDP
size generate count states.

49

fiThiebaux, Gretton, Slaney, Price & Kabanza

type
first time pi
pi sequence start state
two consecutive pi
pi n times ago

formula
- ni=1 pi )
(ni=1 pi ) (
(ni=1 pi ) n >
n1
i=1
(pi pi+1 )
n ni=1 pi

size
O(1)||S||
O(n)||S||
O(nk )||S||
O(2n )||S||

fastest
pltlstr(a)
fltl
pltlstr
pltlstr

slowest
pltlmin
pltlstr
fltl
pltlmin

Table 1: Influence Reward Type MDP Size Method Performance

Average CPU time (sec)

1000
600
400
200

2

2.5

3

3.5

4

4.5

5

5.5

n
APPROACHES prvIn
APPROACHES prvOut

Figure 12: Changing Syntax
otherwise badly comparison structured methods. Interestingly, always
case. instance, Table 1 whose last two columns report fastest slowest
methods range hand-coded domains 1 n 12, first row contradicts
expectation. Moreover, although pltlstr fastest last row, larger values
n (not represented table), aborts lack memory, unlike
methods.
obvious observations arising experiments pltlstr nearly
always fastest runs memory. Perhaps interesting results
second row, expose inability methods based PLTL deal
rewards specified long sequences events. converting reward formula
set subformulae, lose information order events,
recovered laboriously reasoning. $FLTL progression contrast takes events one
time, preserving relevant structure step. experimentation led us
observe PLTL based algorithms perform poorly reward specified using
- k f , fik f (f true k steps ago, within last k
formulae form k f ,
steps, last k steps).
5.5 Influence Syntax
Unsurprisingly, find syntax used express rewards, affects length
formula, major influence run time. typical example effect
captured Figure 12. graph demonstrates re-expressing prvOut n (ni=1 pi )
50

fiDecision-Theoretic Planning non-Markovian Rewards

State count/(2^n)

11
9
7
5
3
1
0

2

4

6

8

10

12

14

n
PLTLMIN
FLTL

Figure 13: Effect Multiple Rewards MDP size

Total CPU time (sec)

1500
1000
500

0

2

4

6

8

10

12

14

n
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 14: Effect Multiple Rewards Run Time
prvIn ni=1 n pi , thereby creating n times temporal subformulae, alters
running time PLTL methods. fltl affected $FLTL progression requires two
iterations reward formula. graph represents averages running
times methods, complete domain.
serious concern relation PLTL approaches handling reward
specifications containing multiple reward elements. notably found pltlmin
necessarily produce minimal equivalent MDP situation. demonstrate, consider set reward formulae {f1 , f2 , . . . , fn }, associated
real value r. Given this, PLTL approaches distinguish unnecessarily past
behaviours lead identical future rewards. may occur reward
e-state determined truth value f1 f2 . formula necessarily require
e-states distinguish cases {f1 >, f2 } {f1 , f2 >}
hold; however, given specification, pltlmin makes distinction. example,

51

fiThiebaux, Gretton, Slaney, Price & Kabanza

taking fi = pi , Figure 13 shows fltl leads MDP whose size 3 times
NMRDP. contrast, relative size MDP produced pltlmin
linear n, number rewards propositions. results obtained
hand-coded domains except spudd-expon. Figure 14 shows run-times function
n complete. fltl dominates overtaken pltlstr(A) large values
n, MDP becomes large explicit exploration practical. obtain
minimal equivalent MDP using pltlmin, bloated reward specification form
{( ni=1 (pi nj=1,j6=i pj ) : r), . . . , ( ni=1 pi : n r)} necessary, which, virtue
exponential length, adequate solution.
5.6 Influence Reachability
approaches claim ability ignore variables irrelevant
condition track unreachable:13 pltlmin detects preprocessing,
pltlstr exploits ability structured solution methods ignore them, fltl ignores progression never exposes them. However, given mechanisms
avoiding irrelevance different, expect corresponding differences effects.
experimental investigation, found differences performance best illustrated looking response formulae, assert trigger condition c reached
reward received upon achievement goal g in, resp. within, k steps.
- k c, $FLTL, (c k (g $)), resp.
PLTL, written g k c, resp. g
(c k (g $))
goal unreachable, PLTL approaches perform well. always false,
goal g lead behavioural distinctions. hand, constructing
MDP, fltl considers successive progressions k g without able detect
unreachable actually fails happen. exactly blindness blind
minimality amounts to. Figure 15 illustrates difference performance function
number n propositions involved spudd-linear domain, reward
form g n c, g unreachable.
fltl shines trigger unreachable. Since c never happens, formula
always progress itself, goal, however complicated, never tracked generated MDP. situation PLTL approaches still consider k c subformulae,
discover, expensive preprocessing pltlmin, reachability analysis pltlstr(a), never pltlstr, irrelevant. illustrated Figure 16,
spudd-linear reward form g n c, c unreachable.
5.7 Dynamic Irrelevance
Earlier claimed one advantage pltlstr fltl pltlmin pltlsim
former perform dynamic analysis rewards capable detecting irrelevance
variables particular policies, e.g. optimal policy. experiments confirm
claim. However, reachability, whether goal triggering condition
response formula becomes irrelevant plays important role determining whether
13. sometimes speak conditions goals reachable achievable rather feasible,
although may temporally extended. keep line conventional vocabulary
phrase reachability analysis.

52

fiDecision-Theoretic Planning non-Markovian Rewards

Total CPU time (sec)

350
250
150
100
50

2

4

6

8

10

12

14

n
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 15: Response Formula Unachievable Goal

Total CPU time (sec)

350
250
150
100
50

1

3

5

7

9

11

n
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 16: Response Formula Unachievable Trigger
pltlstr fltl approach taken: pltlstr able dynamically ignore goal,
fltl able dynamically ignore trigger.
illustrated Figures 17 18. figures, domain considered
on/off n = 6 propositions, response formula g n c before,
g c achievable. response formula assigned fixed reward. study effect
dynamic irrelevance goal, Figure 17, achievement g rewarded value
r (i.e. (g : r) PLTL). Figure 18, hand, study effect
dynamic irrelevance trigger achievement c rewarded value r.
figures show runtime methods r increases.
Achieving goal, resp. trigger, made less attractive r increases
point response formula becomes irrelevant optimal policy.
happens, run-time pltlstr resp. fltl, exhibits abrupt durable improvement.
figures show fltl able pick irrelevance trigger, pltlstr able
exploit irrelevance goal. expected, pltlmin whose analysis static pick
53

fiThiebaux, Gretton, Slaney, Price & Kabanza

Total CPU time (sec)

200
150
100
50

0

50

100

150

200

250

300

350

r
PLTLMIN

PLTLSTRUCT

FLTL

PLTLSTRUCT (A)

Average CPU time (sec)

Figure 17: Response Formula Unrewarding Goal
200
150
100
50

0

50

100

150

200

250

300

350

r
PLTLMIN

FLTL

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 18: Response Formula Unrewarding Trigger
either performs consistently badly. Note figures, pltlstr progressively
takes longer compute r increases value iteration requires additional iterations
converge.
5.8 Summary
experiments artificial domains, found pltlstr fltl preferable statebased PLTL approaches cases. one insists using latter, strongly
recommend preprocessing. fltl technique choice reward requires tracking
long sequence events desired behaviour composed many elements
identical rewards. response formulae, advise use pltlstr probability
reaching goal low achieving goal costly, conversely, advise
use fltl probability reaching triggering condition low reaching
costly. cases, attention paid syntax reward formulae
54

fiDecision-Theoretic Planning non-Markovian Rewards

particular minimising length. Indeed, could expected, found syntax
formulae type non-Markovian reward encode predominant
factor determining difficulty problem, much features
Markovian dynamics domain.

6. Concrete Example
experiments far focused artificial problems aimed characterising
strengths weaknesses various approaches. look concrete example
order give sense size interesting problems techniques
solve. example derived Miconic elevator classical planning benchmark
(Koehler & Schuster, 2000). elevator must get number passengers origin
floor destination. Initially, elevator arbitrary floor passenger
served boarded elevator. version problem, one single
action causes elevator service given floor, effect unserved
passengers whose origin serviced floor board elevator, boarded passengers
whose destination serviced floor unboard become served. task plan
elevator movement passengers eventually served.14
two variants Miconic. simple variant, reward received
time passenger becomes served. hard variant, elevator also attempts
provide range priority services passengers special requirements: many passengers
prefer travelling single direction (either down) destination, certain
passengers might offered non-stop travel destination, finally, passengers
disabilities young children supervised inside elevator
passenger (the supervisor) assigned them. omit VIP conflicting group
services present original hard Miconic problem, reward formulae
create additional difficulties.
formulation problem makes use propositions PDDL description Miconic used 2000 International Planning Competition: dynamic propositions
record floor elevator currently whether passengers served boarded,
static propositions record origin destination floors passengers, well
categories (non-stop, direct-travel, supervisor, supervised) passengers fall in. However,
formulation differs PDDL description two interesting ways. Firstly, since
use rewards instead goals, able find preferred solution even
goals cannot simultaneously satisfied. Secondly, priority services naturally
described terms non-Markovian rewards, able use action description simple hard versions, whereas PDDL description hard miconic
requires additional actions (up, down) complex preconditions monitor satisfaction priority service constraints. reward schemes Miconic encapsulated
four different types reward formula.
1. simple variant, reward received first time passenger Pi served:
14. experimented stochastic variants Miconic passengers small probability
desembarking wrong floor. However, find useful present results deterministic
version since closer Miconic deterministic planning benchmark since, shown
before, rewards far crucial impact dynamics relative performance methods.

55

fiThiebaux, Gretton, Slaney, Price & Kabanza

PLTL:

ServedPi fi ServedPi

$FLTL:

ServedPi U (ServedPi $)

2. Next, reward received time non-stop passenger Pi served one step
boarding elevator:
PLTL:

N onStopPi BoardedPi ServedPi ServedPi

$FLTL:

((N onStopPi BoardedPi ServedPi ServedPi ) $)

3. Then, reward received time supervised passenger Pi served
accompanied times inside elevator supervisor15 Pj :
PLTL:
$FLTL:

SupervisedPi SupervisorPj Pi ServedPi
fi ServedPi fi(BoardedPi BoardedPj )
ServedPi U ((BoardedPi SupervisedPi (BoardedPj SupervisorPj Pi )
ServedPi ) (ServededPi $))

4. Finally, reward received time direct travel passenger Pi served
travelled one direction since boarding, e.g., case going up:
DirectP
W W ServedPi ServedPi
(( j k>j (AtF loork AtF loorj )) (BoardedPi BoardedPi ))
W W
$FLTL: ((DirectPi BoardedPi ) (ServedPi U ((( j k>i AtF loorj AtF loork )
ServedPi ) (servedPi $))))

PLTL:

similarly case going down.
Experiments section run Dual Pentium4 3.4GHz GNU/Linux 2.6.11
machine 1GB ram. first experimented simple variant, giving reward
50 time passenger first served. Figure 19 shows CPU time taken
various approaches solve random problems increasing number n floors
passengers, Figure 20 shows number states expanded so. data
point corresponds one random problem. fair structured approach,
ran pltlstr(a) able exploit reachability start state. first observation
although pltlstr(a) best small values n, quickly runs memory.
pltlstr(a) pltlsim need track formulae form fi ServedPi
pltlsim not, conjecture run memory earlier.
second observation attempts PLTL minimisation pay much here.
pltlmin reduced memory tracks fewer subformulae, size
MDP produces identical size pltlsim MDP larger
fltl MDP. size increase due fact PLTL approaches label differently
e-states passengers served, depending become served
(for passengers, reward formula true e-state). contrast, fltl
implementation progression one step ahead labels e-states reward
15. understand $FLTL formula, observe get reward iff (BoardedPi SupervisedPi )
(BoardedPj SupervisorPj Pi ) holds ServedPi becomes true, recall formula q U ((p
q) (q $)) rewards holding p occurrence q.

56

fiDecision-Theoretic Planning non-Markovian Rewards

Total CPU time (sec)

7000
4000
2000
1000

2

4

6

8

10

12

14

n
FLTL
PLTLSIM
PLTLMIN
PLTLSTR(A)

Figure 19: Simple Miconic - Run Time

45

State count/(2^n)

40
35
30
25
20
15
10
5
0
2

4

6

8

10

12

14

n
FLTL
PLTLSIM, PLTLMIN

Figure 20: Simple Miconic - Number Expanded States
formulae relevant passengers still need served, formulae
progressed >. gain number expanded states materialises run time gains,
resulting fltl eventually taking lead.
second experiment illustrates benefits using even extremely simple admissible heuristic conjunction fltl. heuristic applicable discounted stochastic
shortest path problems, discounts rewards shortest time future
possible. simply amounts assigning fringe state value 50 times
number still unserved passengers (discounted once), results avoiding floors
passenger waiting destination boarded passenger.
Figures 21 22 compare run time number states expanded fltl used
conjunction value iteration (valIt) used conjunction LAO*
57

fiThiebaux, Gretton, Slaney, Price & Kabanza

Total CPU time (sec)

35000
20000
10000
5000

2

4

6

8

10

12

14

n
FLTLLAO(h)
FLTLLAO(u)
FLTLvalIt

Figure 21: Effect Simple Heuristic Run Time

State count/(2^n)

50
40
30
20
10
0
2

4

6

8

10

12

14

n
FLTLLAO(h)
FLTLvalIt,FLTLLAO(u)

Figure 22: Effect Simple Heuristic Number Expanded States
search informed heuristic (LAO(h)). Uninformed LAO* (LAO*(u), i.e. LAO*
heuristic 50 n node) also included reference point show
overhead induced heuristic search. seen graphs, heuristic search
generates significantly fewer states eventually pays terms run time.
final experiment, considered hard variant, giving reward 50
service (1), reward 2 non-stop travel (2), reward 5 appropriate supervision
(3), reward 10 direct travel (2). Regardless number n floors
passengers, problems feature single non-stop traveller, third passengers require
supervision, half passengers care traveling direct. CPU time number
states expanded shown Figures 23 24, respectively. simple case,
pltlsim pltlstr quickly run memory. Formulae type (2) (3) create
many additional variables track approaches, problem seem
58

fiDecision-Theoretic Planning non-Markovian Rewards

Total CPU time (sec)

14000
8000
4000
2000

2

3

4

5

6

7

n
FLTL
PLTLSIM
PLTLMIN
PLTLSTRUCT(A)

Figure 23: Hard Miconic - Run Time

State count/(2^n)

100
80
60
40
20
0
2

3

4

5

6

7

n
FLTL
PLTLSIM
PLTLMIN

Figure 24: Hard Miconic - Number Expanded States
exhibit enough structure help pltlstr. fltl remains fastest. Here,
seem much due size generated MDP slightly
pltlmin MDP, rather overhead incurred minimisation. Another
observation arising experiment small instances handled
comparison classical planning version problem solved state art
optimal classical planners. example, 2000 International Planning Competition,
PropPlan planner (Fourman, 2000) optimally solved instances hard Miconic
20 passengers 40 floors 1000 seconds much less powerful machine.

59

fiThiebaux, Gretton, Slaney, Price & Kabanza

7. nmrdpp Probabilistic Planning Competition
report behaviour nmrdpp probabilistic track 4th International Planning Competition (IPC-4). Since competition feature non-Markovian
rewards, original motivation taking part compare solution methods
implemented nmrdpp Markovian setting. objective largely underestimated
challenges raised merely getting planner ready competition, especially
competition first kind. end, decided successfully preparing nmrdpp attempt problems competition using one solution method (and possibly
search control knowledge), would honorable result.
crucial problem encountered translation PPDDL (Younes &
Littman, 2004), probabilistic variant PDDL used input language competition, nmrdpps ADD-based input language. translating PPDDL ADDs
possible theory, devising translation practical enough need
competition (small number variables, small, quickly generated, easily manipulable
ADDs) another matter. mtbdd, translator kindly made available participants
competition organisers, always able achieve required efficiency.
times, translation quick nmrdpp unable use generated ADDs efficiently. Consequently, implemented state-based translator top PDDL parser
backup, opted state-based solution method since rely ADDs
could operate translators.
version nmrdpp entered competition following:
1. Attempt get translation ADDs using mtbdd, proves infeasible,
abort rely state-based translator instead.
2. Run fltl expansion state space, taking search control knowledge account
available. Break 10mn complete.
3. Run value iteration convergence. Failing achieve useful result (e.g.
expansion complete enough even reach goal state), go back step 2.
4. Run many 30 trials possible remaining time,16 following generated policy defined, falling back non-deterministic search control
policy available.
Step 1 trying maximise instances original ADD-based
nmrdpp version could run intact. Step 3, decided use LAO*
run good heuristic, often incurs significant overhead compared value
iteration.
problems featured competition classified goal-based rewardbased problems. goal-based problems, (positive) reward received goal
state reached. reward-based problems, action performance may also incur (usually
negative) reward. Another orthogonal distinction made problems
16. given problem, planners 15mn run whatever computation saw appropriate (including parsing, pre-processing, policy generation any), execute 30 trial runs generated
policy initial state goal state.

60

fiDecision-Theoretic Planning non-Markovian Rewards

domains communicated advance participants domains
were. latter consisted variants blocks world logistics (or box world)
problems, gave participating planners opportunity exploit knowledge
domain, much hand-coded deterministic planning track.
decided enroll nmrdpp control-knowledge mode domain-independent
mode. difference two modes first uses FLTL search
control knowledge written known domains additional input. main concern
writing control knowledge achieve reasonable compromise size
effectiveness formulae. blocks world domain, two actions
pickup-from putdown-to 25% chance dropping block onto table,
control knowledge used encoded variant well-known GN1 near-optimal strategy
deterministic blocks world planning (Slaney & Thiebaux, 2001): whenever possible,
try putting clear block goal position, otherwise put arbitrary clear block
table. blocks get dropped table whenever action fails,
success probabilities rewards identical across actions, optimal policies
problem essentially made optimal sequences actions deterministic blocks
world little need sophisticated strategy.17 colored blocks
world domain, several blocks share color goal refers
color blocks, control knowledge selected arbitrary goal state non-colored
blocks world consistent colored goal specification, used strategy
non-colored blocks world. performance strategy depends entirely
goal-state selected therefore arbitrarily bad.
Logistics problems IPC-2 distinguish airports locations within
city; trucks drive two locations city planes fly
two airports. contrast, box world features cities,
airport, accessible truck. priori, map truck
plane connections arbitrary. goal get packages city origin
city destination. Moving truck 20% chance resulting reaching one
three cities closest departure city rather intended one. size box
world search space turned quite challenging nmrdpp. Therefore, writing
search control knowledge, gave optimality consideration favored maximal
pruning. helped fact box world generator produces problems
following structure. Cities divided clusters, composed
least one airport city. Furthermore cluster least one hamiltonian circuit
trucks follow. control knowledge used forced planes one, trucks
one cluster idle. cluster, truck allowed move could
attempt driving along chosen hamiltonian circuit, picking dropping parcels
went.
planners participating competition shown Table 2. Planners E, G2,
J1, J2 domain-specific: either tuned blocks box worlds, use
domain-specific search control knowledge, learn examples. participating
planners domain-independent.
17. sophisticated near-optimal strategies deterministic blocks world exist (see Slaney & Thiebaux,
2001), much complex encode might caused time performance problems.

61

fiThiebaux, Gretton, Slaney, Price & Kabanza

Part.
C
E*
G1
G2*
J1*
J2*
J3
P
Q
R

Description
symbolic LAO*
first-order heuristic search fluent calculus
nmrdpp without control knowledge
nmrdpp control knowledge
interpreter hand written classy policies
learns classy policies random walks
version ff replanning upon failure
mgpt: lrtdp automatically extracted heuristics
ProbaProp: conformant probabilistic planner
structured reachability analysis structured PI

Reference
(Feng & Hansen, 2002)
(Karabaev & Skvortsova, 2005)
paper
paper
(Fern et al., 2004)
(Fern et al., 2004)
(Hoffmann & Nebel, 2001)
(Bonet & Geffner, 2005)
(Onder et al., 2006)
(Teichteil-Konigsbuch & Fabiani, 2005)

Table 2: Competition Participants. Domain-specific planners starred
dom
prob
G2*
J1*
J2*
E*
J3
G1
R
P
C
Q

5
100
100
100
100
100

bw-c-nr
8
11
100 100
100 100
100 100
100 100
100 100

bw-nc-nr
8
100
100
100
100
100

bx-nr
5-10 10-10
100 100
100
100
100
67
100

expl-bw
11

hanoise
5-3

zeno
1-2-3-7

tire-nr
30-4

9



50
57


100
90
100
100
3

23
30
30
53
?
23

100

3

total
600
600
567
400
632
180
177
153
100
26

Table 3: Results Goal-Based Problems. Domain-specific planners starred. Entries
percentage runs goal reached. blank indicates
planner unable attempt problem. indicates planner
attempted problem never able achieve goal. ? indicates
result unavailable (due bug evaluation software, couple
results initially announced found invalid).
dom
prob
J1*
G2*
E*
J2*
J3
P
C
G1
R
Q

5
497
495
496
497
496

bw-c-r
8
11
487 481
486 480
492 486
486 482
487 482

5
494
495
495
495
494
494
495
495
494
180

8
489
490
490
490
490
488

bw-nc-r
11 15 18
21
480 470 462 458
480 468 352 286
480 468
481
466 397

455
459

bx-r
5-10 10-10 10-15
419
317
129
438 376

376
425
184


346



279

file
30-4

tire-r
30-4

36
58



?




11

total
5183
4846
2459
4229
4475
2087
495
495
494
191

Table 4: Results Reward-Based Problems. Domain-specific planners starred. Entries
average reward achieved 30 runs. blank indicates
planner unable attempt problem. indicates planner
attempted problem achieve strictly positive reward. ? indicates
result unavailable.
62

fiDecision-Theoretic Planning non-Markovian Rewards

Tables 3 4 show results competition, extracted competition overview paper (Younes, Littman, Weissmann, & Asmuth, 2005)
competition web site http://www.cs.rutgers.edu/~mlittman/topics/ipc04-pt/.
first tables concerns goal-based problems second reward-based problems. entries tables represent goal-achievement percentage average reward achieved various planner versions (left-column) various problems (top
two rows). Planners top part tables domain-specific. Problems
known domains lie left-hand side tables. colored blocks world problems
bw-c-nr (goal-based version) bw-c-r (reward version) 5, 8, 11 blocks.
non-colored blocks world problems bw-nc-nr (goal-based version) 8 blocks, bwnc-r (reward-based version) 5, 8, 11, 15, 18, 21 blocks. box world problems
bx-nr (goal-based) bx-r (reward-based), 5 10 cities 10 15 boxes. Problems unknown domains lie right hand side tables. comprise:
expl-bw, exploding version 11 block blocks world problem putting
block may destroy object put on, zeno, probabilistic variant zeno travel
domain problem IPC-3 1 plane, 2 persons, 3 cities 7 fuel levels, hanoise,
probabilistic variant tower hanoi problem 5 disks 3 rods, file, problem
putting 30 files 5 randomly chosen folders, tire, variant tire world problem
30 cities spare tires 4 them, tire may go flat driving.
planner nmrdpp G1 G2 version, able attempt problems, achieving strictly positive reward 4 them. even ff (J3), competition overall
winner, able successfully attempt many problems. nmrdpp performed particularly well goal-based problems, achieving goal 100% runs except expl-bw,
hanoise, tire-nr (note three problems, goal achievement probability
optimal policy exceed 65%). planner outperformed nmrdpp
scale. pointed before, ff behaves well probabilistic version blocks box
world optimal policies close deterministic problem
Hoffmann (2002) analyses reasons ff heuristic works well traditional planning benchmarks blocks world logistics. hand, ff unable
solve unknown problems different structure require substantial
probabilistic reasoning, although problems easily solved number participating planners. expected, large discrepancy version nmrdpp
allowed use search control (G2) domain-independent version (G1).
latter performs okay unknown goal-based domains, able solve
known ones. fact, except ff, none participating domain-independent
planners able solve problems.
reward-based case, nmrdpp control knoweldge behaves well known
problems. human-encoded policies (J1) performed better. Without control knowledge nmrdpp unable scale problems, participants ff
mgpt are. Furthermore nmrdpp appears perform poorly two unknown problems.
cases, might due fact fails generate optimal policy: suboptimal policies easily high negative score domains (see Younes et al., 2005).
r-tire, know nmrdpp indeed generate suboptimal policy. Additionally,
could nmrdpp unlucky sampling-based policy evaluation process:

63

fiThiebaux, Gretton, Slaney, Price & Kabanza

tire-r particular, high variance costs various trajectories
optimal policy.
Alltogether, competition results suggest control knowledge likely essential solving larger problems (Markovian not) nmrdpp, that,
observed deterministic planners, approaches making use control knowledge
quite powerful.

8. Conclusion, Related, Future Work
paper, examined problem solving decision processes nonMarkovian rewards. described existing approaches exploit compact representation reward function automatically translate NMRDP equivalent
process amenable MDP solution methods. computational model underlying
framework traced back work relationship linear temporal logic
automata areas automated verification model-checking (Vardi, 2003;
Wolper, 1987). remaining framework, proposed new representation
non-Markovian reward functions translation MDPs aimed making best
possible use state-based anytime heuristic search solution method. representation extends future linear temporal logic express rewards. translation
effect embedding model-checking solution method. results MDP
minimal size achievable without stepping outside anytime framework, consequently
better policies deadline. described nmrdpp, software platform
implements approaches common interface, proved useful tool
experimental analysis. system analysis first kind.
able identify number general trends behaviours methods
provide advice best suited certain circumstances. obvious
reasons, analysis focused artificial domains. Additional work examine
wider range domains practical interest, see form results take
context. Ultimately, would like analysis help nmrdpp automatically select
appropriate method. Unfortunately, difficulty translating
PLTL $FLTL, likely nmrdpp would still maintain PLTL
$FLTL version reward formulae.
detailed comparison approach solving NMRDPs existing methods (Bacchus et al., 1996, 1997) found Sections 3.10 5. Two important aspects future
work would help take comparison further. One settle question appropriateness translation structured solution methods. Symbolic implementations
solution methods consider, e.g. symbolic LAO* (Feng & Hansen, 2002), well
formula progression context symbolic state representations (Pistore & Traverso,
2001) could investigated purpose. take advantage greater
expressive power $FLTL consider richer class decision processes, instance
uncertainty rewards received when. Many extensions language
possible: adding eventualities, unrestricted negation, first-class reward propositions,
quantitative time, etc. course, dealing via progression without backtracking
another matter.

64

fiDecision-Theoretic Planning non-Markovian Rewards

investigate precise relationship line work recent work
planning temporally extended goals non-deterministic domains. particular
interest weak temporally extended goals expressible Eagle language
(Dal Lago et al., 2002), temporally extended goals expressible -CTL* (Baral &
Zhao, 2004). Eagle enables expression attempted reachability maintenance goals
form try-reach p try-maintain p, add goals do-reach p
do-maintain p already expressible CTL. idea generated policy
make every attempt satisfying proposition p. Furthermore, Eagle includes recovery goals
form g1 fail g2 , meaning goal g2 must achieved whenever goal g1 fails,
cyclic goals form repeat g, meaning g achieved cyclically
fails. semantics goals given terms variants Buchi tree automata
preferred transitions. Dal Lago et al. (2002) present planning algorithm based
symbolic model-checking generates policies achieving goals. Baral Zhao
(2004) describe -CTL*, alternative framework expressing subset Eagle goals
variety others. -CTL* variant CTL* allows formulae involving
two types path quantifiers: quantifiers tied paths feasible generated
policy, usual, also quantifiers generally tied paths feasible
domain actions. Baral Zhao (2004) present planning algorithm.
would interesting know whether Eagle -CTL* goals encoded nonMarkovian rewards framework. immediate consequence would nmrdpp
could used plan them. generally, would like examine respective
merits non-deterministic planning temporally extended goals decision-theoretic
planning non-Markovian rewards.
pure probabilistic setting (no rewards), recent related research includes work
planning controller synthesis probabilistic temporally extended goals expressible
probabilistic temporal logics CSL PCTL (Younes & Simmons, 2004; Baier et al.,
2004). logics enable expressing statements probability policy satisfying given temporal goal exceeding given threshold. instance, Younes Simmons
(2004) describe general probabilistic planning framework, involving concurrency, continuous time, temporally extended goals, rich enough model generalised semi-Markov
processes. solution algorithms directly comparable presented here.
Another exciting future work area investigation temporal logic formalisms
specifying heuristic functions NMRDPs generally search problems
temporally extended goals. Good heuristics important solution methods
targeting, surely value ought depend history. methods
described could applicable description processing heuristics. Related
problem extending search control knowledge fully operate
presence temporally extended goals, rewards, stochastic actions. first issue
branching probabilistic logics CTL PCTL variants preferred
FLTL describing search control knowledge, stochastic actions
involved, search control often needs refer possible futures even
probabilities.18 Another major problem GOALP modality,
key specification reusable search control knowledge interpreted respect
18. would argue, hand, CTL necessary representing non-Markovian rewards.

65

fiThiebaux, Gretton, Slaney, Price & Kabanza

fixed reachability goal19 (Bacchus & Kabanza, 2000), such, applicable
domains temporally extended goals, let alone rewards. Kabanza Thiebaux (2005)
present first approach search control presence temporally extended goals
deterministic domains, much remains done system like nmrdpp able
support meaningful extension GOALP.
Finally, let us mention related work area databases uses similar approach
pltlstr extend database auxiliary relations containing sufficient information
check temporal integrity constraints (Chomicki, 1995). issues somewhat different
raised NMRDPs: ever one sequence databases, matters
size auxiliary relations avoiding making redundant distinctions.

Acknowledgements
Many thanks Fahiem Bacchus, Rajeev Gore, Marco Pistore, Ron van der Meyden, Moshe
Vardi, Lenore Zuck useful discussions comments, well anonymous
reviewers David Smith thorough reading paper excellent
suggestions. Sylvie Thiebaux, Charles Gretton, John Slaney, David Price thank National ICT Australia support. NICTA funded Australian Governments
Backing Australias Ability initiative, part Australian Research Council. Froduald Kabanza supported Canadian Natural Sciences Engineering Research
Council (NSERC).

Appendix A. Class Reward-Normal Formulae
existing decision procedure (Slaney, 2005) determining whether formula rewardnormal guaranteed terminate finitely, involves construction comparison
automata rather intricate practice. therefore useful give simple syntactic
characterisation set constructors obtaining reward-normal formulae even though
formulae constructible.
say formula material iff contains $ temporal operators
is, material formulae boolean combinations atoms.
consider four operations behaviours representable formulae $FLTL. Firstly,
behaviour may delayed specified number timesteps. Secondly, may made
conditional material trigger. Thirdly, may started repeatedly material
termination condition met. Fourthly, two behaviours may combined form
union. operations easily realised syntactically corresponding operations
formulae. material formula:
delay[f ] =

f

cond[m, f ] = f
loop[m, f ] = f U
union[f1 , f2 ] = f1 f2
19. f atemporal formula, GOALP(f ) true iff f true goal states.

66

fiDecision-Theoretic Planning non-Markovian Rewards

shown (Slaney, 2005) set reward-normal formulae closed delay,
cond (for material m), loop (for material m) union, also closure
{$} operations represents class behaviours closed intersection
concatenation well union.
Many familiar reward-normal formulae obtainable $ applying four operations. example, (p $) loop[, cond[p, $]]. Sometimes paraphrase necessary.
example, ((p q) $) required form antecedent
conditional, equivalent (p (q $)) loop[, cond[p, delay[cond[q, $]]]].
cases easy. example formula p U (p$) stipulates reward
first time p happens form suggested. capture
behaviour using operations requires formula like (p $) ( (p $) U p).

Appendix B. Proofs Theorems
Property 1 b ((i) B), (, i) |=B f iff (, + 1) |=B Prog(b, , f ).
Proof:
Induction structure f . several base cases, fairly trivial.
f = > f = nothing prove, progress hold
everywhere nowhere respectively. f = p f holds progresses >
holds i+1 f hold progresses
hold i+1 . case f = p similar. last base case, f = $. following
equivalent:
(, i) |=B f
(i) B
b
Prog(b, , f ) = >
(, + 1) |=B Prog(b, , f )
Induction case 1: f = g h. following equivalent:
(, i) |=B f
(, i) |=B g (, i) |=B h
(, + 1) |=B Prog(b, , g) (, + 1) |=B Prog(b, , h) (by induction hypothesis)
(, + 1) |=B Prog(b, , g) Prog(b, , h)
(, + 1) |=B Prog(b, , f )
Induction case 2: f = g h. Analogous case 1.
Induction case 3: f = g. Trivial inspection definitions.
Induction case 4: f = g U h. f logically equivalent h (g (g U h)
cases 1, 2 3 holds stage behaviour B iff Prog(b, , f ) holds stage i+1.

Theorem 1 Let f reward-normal, let hf0 , f1 , . . .i result progressing
successive states sequence . Then, provided fi , Rew(i , fi )
iff (i) Bf .

67

fiThiebaux, Gretton, Slaney, Price & Kabanza

Proof: First, definition reward-normality, f reward-normal |=B f iff
i, (i) Bf (i) B. Next, |=B f progressing f according
B (that is, letting bi true iff (i) B) cannot lead contradiction
Property 1, progression truth-preserving.
remains, then, show 6|=B f progressing f according B
must lead eventually . proof induction structure f
usual base case f literal (an atom, negated atom >, $) trivial.
Case f = g h. Suppose 6|=B f . either 6|=B g 6|=B h, induction
hypothesis either g h progresses eventually , hence conjunction.
Case f = g h. Suppose 6|=B f . 6|=B g 6|=B h, induction
hypothesis g h progresses eventually . Suppose without loss generality
g progress h does. point g progressed
formula g 0 f progressed g 0 simplifies g 0 . Since g 0 also progresses
eventually, f .
Case f = g. Suppose 6|=B f . Let = 0 ; let B 0 = {|0 ; B}.
6|=B 0 g, induction hypothesis g progressed according B 0 eventually
reaches . progression f according B exactly
first step, leads .
Case f = g U h. Suppose 6|=B f . j (, j) 6|=B g
j, (, i) 6|=B h. proceed induction j. base case j = 0, 6|=B g
6|=B h whence main induction hypothesis g h eventually progress
. Thus h (g f 0 ) progresses eventually f 0 , particular f 0 = f ,
establishing base case. induction case, suppose |=B g (and course 6|=B h).
Since f equivalent h (g f ) 6|=B f , 6|=B h |=B g, clearly 6|=B f .
B 0 previous case, therefore, 6|=B 0 f failure occurs stage j 1
. Therefore hypothesis induction j applies, f progressed
according B 0 goes eventually , f progressed according B goes
similarly .

Theorem 3 Let 0 set e-states equivalent MDP D0 = hS, s0 , A, Pr, Ri.
D0 minimal iff every e-state 0 reachable 0 contains two distinct e-states s01
s02 (s01 ) = (s02 ) (s01 ) = (s02 ).
Proof: Proof construction canonical equivalent MDP Dc . Let set
e 0 ) partitioned equivalence classes,
finite prefixes state sequences D(s
e 0 ), R(1(i); ) =
1(i) 2(j) iff 1i = 2j 1(i); D(s
R(2(j); ). Let [(i)] denote equivalence class (i). Let E set
equivalence classes. Let function takes [(i)] E A(i ).
(i) (j) A([(i)]), let ([(i)], a, [(j)]) Pr(i , a, s) [(j)] =
[(i); hsi]. Otherwise let ([(i)], a, [(j)]) = 0. Let R([(i)]) R((i)). note
following four facts:
1. functions A, R well-defined.
2. Dc = hE, [hs0 i], A, , Ri equivalent MDP ([(i)]) = .

68

fiDecision-Theoretic Planning non-Markovian Rewards

3. equivalent MDP D00 mapping subset states
D00 onto E.
4. D0 satisfies condition every e-state 0 reachable 0 contains two
distinct e-states s01 s02 (s01 ) = (s02 ) (s01 ) = (s02 ) iff Dc isomorphic
D0 .
fact 1 amounts 1(i) 2(j) matter
two sequences used define A, R equivalence class. cases
simply 1i = 2j . case R, special case = h1i
equality rewards extensions.
Fact 2 matter checking four conditions Definition 1 hold. these,
conditions 1 ( ([s0 ]) = s0 ) 2 (A([(i)]) = A(i )) hold trivially construction.
e 0 ), R([(i)]) = R((i))
Condition 4 says feasible state sequence D(s
i. also given construction. Condition 3 states:
s1 , s2 S, A(s1 ) Pr(s1 , a, s2 ) > 0,
e 0 ) = s1 , exists unique [(j)] E, j = s2 ,
(i) D(s
A([(i)]), ([(i)], a, [[j]]) = Pr(s1 , a, s2 ).
e 0 ) = s1 . required (j) (i); hs2 i,
Suppose Pr(s1 , , s2 ) > 0, (i) D(s
course A([(i)]) = A(i ), required condition reads:
[(i); hs2 i] unique element X E (X) = s2
A(i ), ([(i)], a, X) = Pr(s1 , a, s2 ).
establish existence, need A(i ) ([(i)], a, [(i); hs2 i]) = Pr(i , a, s2 ),
immediate definition above. establish uniqueness, suppose
(X) = s2 ([(i)], a, X) = Pr(s1 , a, s2 ) actions A(i ). Since Pr(s1 , , s2 ) >
0, transition probability [(i)] X nonzero action, definition
, X [(i); hs2 i].
Fact 3 readily observed. Let equivalent MDP D. states s1
s2 D, state X (X) = s1 one state
(Y ) = s2 action A(s1 ) gives nonzero probability
transition X . follows uniqueness part condition 3 Definition 1
together fact transition function probability distribution (sums 1).
Therefore given finite state sequence (i) one state reached
start state following (i). Therefore induces equivalence relation
: (i) (j) iff lead state (the sequences
feasible may regarded equivalent ). reachable state
associated nonempty equivalence class finite sequences states D. Working
definitions, may observe sub-relation (if (i) (j)
(i) (j)). Hence function takes equivalence class
feasible sequence (i) [(i)] induces mapping h (an epimorphism fact)
reachable subset states onto E.
establish Fact 4, must shown case D0 mapping
reversed, equivalence class [(i)] Dc corresponds exactly one element
69

fiThiebaux, Gretton, Slaney, Price & Kabanza

e 0)
D0 . Suppose (for contradiction). exist sequences 1(i) 2(j) D(s
0
1(i) 2(j) following two sequences s0 arrive two different
elements s01 s02 D0 (s01 ) = 1i = 2j = (s02 ) (s01 ) 6= (s02 ). Therefore
e
exists sequence (k) D(s)
R(1(i 1); (k)) 6= R(2(j 1); (k)).
contradicts condition 1(i) 2(j).

Theorem 3 follows immediately facts 14.
Theorem 4 Let D0 translation Definition 5. D0 blind minimal
equivalent MDP D.
Proof: Reachability e-states obvious, constructed
reached. e-state pair hs, state reward function
specification. fact, = (hs, i) determines distribution rewards
continuations sequences
reach hs, i. is, 0 = s,
P
reward (f :r) {r | Bf }. D0 blind minimal, exist
distinct e-states hs, hs, 0 sum . makes
0 semantically equivalent, contradicting supposition distinct.


Appendix C. Random Problem Domains
Random problem domains produced first creating random action specification
defining domain dynamics. experiments conducted20 also involved
producing, second step, random reward specification desired properties
relation generated dynamics.
random generation domain dynamics takes parameters number n
propositions domain number actions produced, starts
assigning effects action proposition affected exactly one
action. example, 5 actions 14 propositions, first 4 actions may affect
3 propositions each, 5th one 2, affected propositions different.
action initial effects, continue add effects one time,
sufficient proportion state space reachable see proportion reachable parameter
below. additional effect generated picking random action random
proposition, producing random decision diagram according uncertainty
structure parameters below:
Uncertainty parameter probability non zero/one value leaf node.
uncertainty 1 result leaf nodes random values uniform
distribution. uncertainty 0 result leaf nodes values 0 1
equal probability.
Structure (or influence) parameter probability decision diagram containing
particular proposition. influence 1 result decision diagrams
20. None included paper, however.

70

fiDecision-Theoretic Planning non-Markovian Rewards

including propositions (and unlikely significant structure), 0
result decision diagrams depend values propositions.
Proportion Reachable parameter lower bound proportion entire 2n
state space reachable start state. algorithm adds behaviour
lower bound reached. value 1 result algorithm running
actions sufficient allow entire state space reachable.
reward specification produced regard generated dynamics
specified number rewards reachable specified number unreachable.
First, decision diagram produced represent states reachable
not, given domain dynamics. Next, random path taken root
decision diagram true terminal generating attainable reward, false
terminal producing unattainable reward. propositions encountered
path, negated not, form conjunction reward formula. process
repeated desired number reachable unreachable rewards obtained.

References
AT&T Labs-Research (2000). Graphviz. Available http://www.research.att.com/
sw/tools/graphviz/.
Bacchus, F., Boutilier, C., & Grove, A. (1996). Rewarding behaviors. Proc. American
National Conference Artificial Intelligence (AAAI), pp. 11601167.
Bacchus, F., Boutilier, C., & Grove, A. (1997). Structured solution methods nonMarkovian decision processes. Proc. American National Conference Artificial
Intelligence (AAAI), pp. 112117.
Bacchus, F., & Kabanza, F. (1998). Planning temporally extended goals. Annals
Mathematics Artificial Intelligence, 22, 527.
Bacchus, F., & Kabanza, F. (2000). Using temporal logic express search control knowledge
planning. Artificial Intelligence, 116 (1-2).
Baier, C., Groer, M., Leucker, M., Bollig, B., & Ciesinski, F. (2004). Controller synthesis
probabilistic systems (extended abstract). Proc. IFIP International Conference
Theoretical Computer Science (IFIP TCS).
Baral, C., & Zhao, J. (2004). Goal specification presence nondeterministic actions.
Proc. European Conference Artificial Intelligence (ECAI), pp. 273277.
Barto, A., Bardtke, S., & Singh, S. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72, 81138.
Bonet, B., & Geffner, H. (2003). Labeled RTDP: Improving convergence real-time
dynamic programming. Proc. International Conference Automated Planning
Scheduling (ICAPS), pp. 1221.

71

fiThiebaux, Gretton, Slaney, Price & Kabanza

Bonet, B., & Geffner, H. (2005). mGPT: probabilistic planner based heuristic search.
Journal Artificial Intelligence Research, 24, 933944.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
Vol. 11, pp. 194.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
factored representations. Artificial Intelligence, 121 (1-2), 49107.
Calvanese, D., De Giacomo, G., & Vardi, M. (2002). Reasoning actions planning LTL action theories. Proc. International Conference Principles
Knowledge Representation Reasoning (KR), pp. 493602.
Cesta, A., Bahadori, S., G, C., Grisetti, G., Giuliani, M., Loochi, L., Leone, G., Nardi, D.,
Oddi, A., Pecora, F., Rasconi, R., Saggase, A., & Scopelliti, M. (2003). RoboCare
project. Cognitive systems care elderly. Proc. International Conference
Aging, Disability Independence (ICADI).
Chomicki, J. (1995). Efficient checking temporal integrity constraints using bounded
history encoding. ACM Transactions Database Systems, 20 (2), 149186.
Dal Lago, U., Pistore, M., & Traverso, P. (2002). Planning language extended
goals. Proc. American National Conference Artificial Intelligence (AAAI), pp.
447454.
Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1995). Planning time constraints stochastic domains. Artificial Intelligence, 76, 3574.
Dean, T., & Kanazawa, K. (1989). model reasoning persistance causation.
Computational Intelligence, 5, 142150.
Drummond, M. (1989). Situated control rules. Proc. International Conference
Principles Knowledge Representation Reasoning (KR), pp. 103113.
Emerson, E. A. (1990). Temporal modal logic. Handbook Theoretical Computer
Science, Vol. B, pp. 9971072. Elsevier MIT Press.
Feng, Z., & Hansen, E. (2002). Symbolic LAO search factored Markov decision processes. Proc. American National Conference Artificial Intelligence (AAAI), pp.
455460.
Feng, Z., Hansen, E., & Zilberstein, S. (2003). Symbolic generalization on-line planning.
Proc. Conference Uncertainty Artificial Intelligence (UAI), pp. 209216.
Fern, A., Yoon, S., & Givan, R. (2004). Learning domain-specific knowledge random
walks. Proc. International Conference Automated Planning Scheduling
(ICAPS), pp. 191198.
Fourman, M. (2000). Propositional planning. Proc. AIPS Workshop Model-Theoretic
Approaches Planning, pp. 1017.
72

fiDecision-Theoretic Planning non-Markovian Rewards

Gretton, C., Price, D., & Thiebaux, S. (2003a). Implementation comparison solution
methods decision processes non-Markovian rewards. Proc. Conference
Uncertainty Artificial Intelligence (UAI), pp. 289296.
Gretton, C., Price, D., & Thiebaux, S. (2003b). NMRDPP: system decision-theoretic
planning non-Markovian rewards. Proc. ICAPS Workshop Planning
Uncertainty Incomplete Information, pp. 4856.
Haddawy, P., & Hanks, S. (1992). Representations decision-theoretic planning: Utility
functions deadline goals. Proc. International Conference Principles
Knowledge Representation Reasoning (KR), pp. 7182.
Hansen, E., & Zilberstein, S. (2001). LAO : heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129, 3562.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: stochastic planning using
decision diagrams. Proc. Conference Uncertainty Artificial Intelligence (UAI),
pp. 279288.
Hoffmann, J. (2002). Local search topology planning benchmarks: theoretical analysis.
Proc. International Conference AI Planning Scheduling (AIPS), pp. 92100.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Howard, R. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge,
MA.
Kabanza, F., & Thiebaux, S. (2005). Search control planning temporally extended
goals. Proc. International Conference Automated Planning Scheduling
(ICAPS), pp. 130139.
Karabaev, E., & Skvortsova, O. (2005). Heuristic Search Algorithm Solving FirstOrder MDPs. Proc. Conference Uncertainty Artificial Intelligence (UAI),
pp. 292299.
Koehler, J., & Schuster, K. (2000). Elevator control planning problem. Proc.
International Conference AI Planning Scheduling (AIPS), pp. 331338.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189211.
Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning.
Artificial Intelligence, 76, 239286.
Lichtenstein, O., Pnueli, A., & Zuck, L. (1985). glory past. Proc. Conference
Logics Programs, pp. 196218. LNCS, volume 193.
Onder, N., Whelan, G. C., & Li, L. (2006). Engineering conformant probabilistic planner.
Journal Artificial Intelligence Research, 25, 115.

73

fiThiebaux, Gretton, Slaney, Price & Kabanza

Pistore, M., & Traverso, P. (2001). Planning model-checking extended goals
non-deterministic domains. Proc. International Joint Conference Artificial Intelligence (IJCAI-01), pp. 479484.
Slaney, J. (2005). Semi-positive LTL uninterpreted past operator. Logic Journal
IGPL, 13, 211229.
Slaney, J., & Thiebaux, S. (2001). Blocks world revisited. Artificial Intelligence, 125,
119153.
Somenzi, F. (2001).
CUDD: CU Decision Diagram Package.
ftp://vlsi.colorado.edu/pub/.

Available

Teichteil-Konigsbuch, F., & Fabiani, P. (2005). Symbolic heuristic policy iteration algorithms structured decision-theoretic exploration problems. Proc. ICAPS workshop Planning Uncertainty Autonomous Systems.
Thiebaux, S., Hertzberg, J., Shoaff, W., & Schneider, M. (1995). stochastic model
actions plans anytime planning uncertainty. International Journal
Intelligent Systems, 10 (2), 155183.
Thiebaux, S., Kabanza, F., & Slaney, J. (2002a). Anytime state-based solution methods
decision processes non-Markovian rewards. Proc. Conference Uncertainty
Artificial Intelligence (UAI), pp. 501510.
Thiebaux, S., Kabanza, F., & Slaney, J. (2002b). model-checking approach decisiontheoretic planning non-Markovian rewards. Proc. ECAI Workshop ModelChecking Artificial Intelligence (MoChArt-02), pp. 101108.
Vardi, M. (2003). Automated verification = graph, logic, automata. Proc. International Joint Conference Artificial Intelligence (IJCAI), pp. 603606. Invited
paper.
Wolper, P. (1987). relation programs computations models temporal
logic. Proc. Temporal Logic Specification, LNCS 398, pp. 75123.
Younes, H. L. S., & Littman, M. (2004). PPDDL1.0: extension PDDL expressing
planning domains probabilistic effects. Tech. rep. CMU-CS-04-167, School
Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania.
Younes, H. L. S., Littman, M., Weissmann, D., & Asmuth, J. (2005). first probabilistic
track International Planning Competition. Journal Artificial Intelligence
Research, Vol. 24, pp. 851887.
Younes, H., & Simmons, R. G. (2004). Policy generation continuous-time stochastic
domains concurrency. Proc. International Conference Automated Planning
Scheduling (ICAPS), pp. 325333.

74

fi
